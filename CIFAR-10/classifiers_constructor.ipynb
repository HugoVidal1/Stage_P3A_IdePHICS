{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 18\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10]) torch.Size([10000, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAEkCAYAAABQRik9AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs/UmsbGmSJobZmXz2O78xIjIyI7Mqq6rJKlY3KU6LBihoQS20IbTUSoDW0pY7QSAEQQtJK0ECtJG2lBYiBIECAUEAQbBZ1c2uqSsrx8gY3nxHn88ofJ+ZnXP8xsuseM9fdKlLv0V63vvu9et+/D//YPbZZ59FTdM0EixYsGDBggX7/2uL/64vIFiwYMGCBQv2d2/BIQgWLFiwYMGCBYcgWLBgwYIFCxYcgmDBggULFixYcAiCBQsWLFiwYLDgEAQLFixYsGDBgkMQLFiwYMGCBQsOQbBgwYIFCxZMRNJv86S6ruXZs2cyn88liqLv/qr+FTLoOi0WC3n69KnE8bf3r8KY/mYLY/rhLYzph7cwph/ewpj+HY9p8y3syy+/hJphePyWB8boXSyMaRjTMKZ/Px5hTMOYyt+TMf1WCAG8Ltj/4//5f5fpdMrv4YWpJxZJHOtX/Rl/u/c1iuL2ue698bm9f7/d7HdR0/dg9n7VPhOv0zT85LS6FvwL3hEe+H0cJ/YaNceott+pejO+Svu1U3Tu/7t7vv5MZLVayX//P/wftGP0bc2f/x/8R/9IhoMRRlGKspSyqCWOa4mSWrJ0ILPZjONXlpVUVS3b1VaKXSlV0eijrCTPc45zlsX0AIejTOI0kaZsMAySDjIZjUaC2xQPIj7v9HQm2SCVbJRKksaSF7nk5U6kiaSpEyl2hVy9uJX1Opdf//JSVqtCvvfZj+Xhk08kjbaS4RHXMk4KXvOLN1vZbEt58/qFrJZLKepKilLvQWtVI+PhUP74xz+Wk6OZ/Orr5/Lq6kaWq6XcLhb08qu6lrpuZLPevveY/s//p/+hjEeZRLFwbAfZkPcMrx9FjWAaxFEko3QoSRzLIEslSRKJ8YgxVzGfdE5lSaJzNda5jLHM0kzSRL8mSSqDIcY25vc+r/HkprHPj/lvcy+yeYXf+Lzl3OUj1klV63xN8DdRJFVV8bXKsuT3nPt2ff4czG38uC5LqatKyrKQoig4xzGmy/VW/vH/8D9+7zH91a9+LicnZ3zP57drebXayV/+zVfy//4nP5FGEslGM5mOB/IHP3wkx/ORTFKRQSQyyBIZDlJZrHby8vJWqrqRotbxn08GMsoSPoZJIsPRUIbjCe/TbpNLJI2kuFdxJFmKexRLnGCcI9nsclmu11IUtWzWBa/r5BhzOpPVcivbXc71UhQlX/fkZMZtcbXeSlVXHBf84NHFkTw8n8kwjeRogHvRSCw1x7eWRJoI/8KV8K7qw7bYu7s7+cH3f/DeY/p//E/+JxI1kVQl1nwsSYx7XUpR4LML1yW+QVCH6YFx0K8iCS6i3a8a3m9cMceLcwPzDtMp4vUzOxwP+AnKJuPPKs4NkZLzpZZ8V8hmk/NlMd/xtaowj0UGo0TXSIw9JtX1ZHNrV2x5zwZpJCnmYoQZIRIliaRpxj0oz/Wzp0Pcw0gS7lUixa6SYot5jfcvZbst5H/9f/i/vfeY/q/+T/9Lmc0mtt/rfij+FeOBocDQcU01kpcF1xX3nqrm0dDUOqZca701mvo+gddL9EzDuuddsPmEvUv3mkqaupKI91VvIL9iTPm7hvt9XeF9sVfi/nXnjp+L2FPwfv2zrG6wR9bc9/V19LNIkurkMOPf1CK77U7+9//x/+Zbjem3cgj84vCCOKD2f64D3x+4/t/4z3Covf3nf5tT4O+1f0jfvzb+tNeWAQOGmYyv7hDgZvrTu4N9/9F/ne4rHvVb/85f711hKn/+0elMxqOxxJJIXuBwLaVpsGEV3OyxCUZRIoPhkGM9nYy54deF0CHAZN5ut9xMxmMcUIkM7GtTR1yMo9FQxuMxz5s4bSSFQ3A2lSxLpagqLurFaiH5ciORpBInKTfaomp42BdFw4fEqSSDocR487qSNI1kPh1KWTay3EYSJ6UsF0Mpi51IGUsT6YLiQsRCwRaLTSKJJcVni/0A1sXl88nv7/uO6XiYyXQChyCS8Wgi4+GU46SOUyNxjEcs09FIUl5PIgmuIdGDH4eVLnw4BHZtiR7ag+FA0iyTLBnIIB3y+9FowgOZi7dHy+EnxhzBYW0HtjuqWNQ9V2nfw4VDYJsB3rusSt4jbPr4HHQGeDiYI8DNRp9b5blUeF4RS5Hopl/axn3QPJ3N5OgIcGws62Qo5TiX8fM7adIRD846GUmVjKTOJlKnQ1nkO5GqlOlkILNsKIuilFd3leRlJTt4BJHIRYF7lcp8GPHrOE6lHo64BlbbigtvlOlBOJJUUtE5g6+7upHFLpI8b2SxwphEEo9jGcWJ3OzgqNshV9YyaRpJZ3qI3e1EihKbsgYFx5jvo6kMh7HMJolg642l2nMIKjgEmB96xupDh/OgMcVGXZdYYxXXBNY67i8dgkgdAn6lz4e9Rv+dJnCUGq75jOdcI0mEA6WRlK6L7pcSY55FUjexOaWYc3A6a3UIilrqquGhhAcOsLrembepRwN/3wjXOD60vwOdARxmjTpd/FsEM3gHXDMdF6wJvKbIdqOzPRvHEqcRthKeXfm2lHxd0Ckq8kp2eXHQmE4mIxmPR61DgAO55+HzIIc1di7Eux0PZgRWJRwAC/TUISj5XN2jIhkMBnRw1NHYP9fcMeufF3jg/bGf9E3PpFqGCH7wPubo988dd0YQpOB9fC/x3zO4SWL9Ozj+GGQ8FwPbGw/8nL/7lmP6rRwCt9bj6jkDPiiOBPTfuEMJus3+/gH6NofgvlPR2W9xCHyw/FoZoenGf39AHMnQiLE75PVl3vKVXpkjEI00kSME6tkfYq9eLGSQbi3+0ENEP0VtUSf+ichBP8N0NpThMCVigImMMR9NJvS6h2NMVvXsS3iOuSIOm20uN7crHmpJpp7q1dWS1141hUWylT6aUqTJpcxLiZJSkmElg6F6+Mu7a3n+VSRNsZImv6MzIE+OGH0gksBg8qBCJNc0kpgjpmPdSGGfQT1rvT/umePzOEIA5+EQ08WokRw3zTSSQTaQ2WTYji+uaZgm3LwwvnhuGqdEADA+iEgxluq04HNhc4kkGw4kgUMQq1OgyAIOY2y0uGt6/fwMHsnDacZzcHEJNlT8TDckOHfYnNoNhQABtmu9j/ha1hrBuJNroR/HFBsp5zU2WESzvtHFMZ0VLPABECbzzd7fMM8tAMDHwOabJXI+H0rdJJJlA8nSWNa3KynXW8m3a6nKXB4/PJPJdCw1rkES2RaVvLq8o2M0Sk4kkZGU2UjqKJXn1xu5+nIheV7IYrHhGoBDgD18Mh7SgR0MM6IAQD822w3HDhsq5lMZ3fA56w0QAqAGisxUeSHllSJQi8WKc83j5pPZWI7HI4nnI7kYDzW6xgHKPQTHm6IDvdtJlOdDZKlx1+uokRKHPVBBTKGEk0h3gKbU2N6WQ8V50EiuQa85ArpXNI05BHR28bpYz+oA4JiOolTiFEFFLFVU8Wc5Aopa1yAPfjw78z1SJwzmD9+hLmS7hXNaSF7oQcsAi082FC2i69Q6N5gpMR2LRrKh7pv4ZxPXUlaNYKvBmYv5AzQBKETDBfn+tstzrkE4OHZx5jzbNafqODu6ggPV95zakDk9x/B9YutMn5tjP23RHHUS4LQ7WteFMnam4DXw2u25aPedT9J55X/Rodhx+z0dA3caar1OPUaB4pqDQMdLnR68z/2AlUhCYfDMt7D3cgi6SK5LB+w7CG871Psphd988P82R8GGuvfc+7/qOQU2oP3Bvf+68GAd6nHHoHtNG1RGtlhWdoM5l3zA+x7i+9nV66XCvjiQUmziBi23DpZO1i2gvKaRR09OuNiKqiTchY14NB7xbwcjTHaR3Q4eN7xtpBMAHZd0DvRAwy3HCa0HTiOYLJWMx3idgXrINTYAOASFJFktaYZ0RCSr5a1sdjspNysp1ndyejKV4xlSDplUjSIYfnBy0SiazQMVC1S3N78HOm5+0MEp8O8PbsBJ3A8fJJIEUF8C7z6T4XCgiDxXJGBN3cCAtBBxw6aUDYkYDLJMYUKL0hFZ4HOkg4HEOGjjlE4BI4A2FWVQIyA8LF7Fbnmw8NMSotFNlNuvzy+LJtTx7EUC0j8Ieo4S/07nKw83vAeiWQZ/Pp/t2unMpDLKD/QIemseHyuNRMZZLCdTOARAUnRebRYbWTeNrDdLyYudTGcTecTjFwdTInnVyNX1ko7Qw5OJTAaYOzxG5M3tSv7qV294mN8tNhyj8UDTBtPJSIa4h0gr8D5izCrC08NEnYZtVdEBZOqtqrk2BkMgb5UsiyU3czgEOARwtYhib+/WcjsdyxSHW61zl84P9loPXvbKsd6esnwfwzkDP7qO8RX3rJYGpz8uDFE8Pl9vKQAa5uHV3g9i33yuruNasH1EMVz7Up1TwzyiOJMk08MbTggOo6JCSqBzRDVN4R9Mo+UU49GIbHaNIHhfb0WWa10rSENivIGuII2ja71huhJrSCck9gKk2nAwNtLEcFTUka2Q0kRKQhS5xd5XHbj0iQSWOR3Gbs9XZwDRNuaErhd66vcsUofXAlxLOiuawHQAdjBNkftZmMIfuI8+23/8noe2Oe1vyXzbBbbf9gNu/C0QXHwmoIRlDeSqfy7qu/Cz0Y/U9I4jDLqvVnSYvxOHwC/Uv+6nDO7xA34jn2D/8P9NB/9vcwje5lTcRwh0sXTwij7Ff9veOT0UezwC/Z85EvgJYDZ7nb3neRR/IEKwWe+4eLjuQOkg+qAwnl6pOi042PF1s0YEawdo1UixrWS1RDSERafQIg84ROQFJkPFxQEnYTBM5TgFJyGS3VbzWHGCgwtjmqsHar4rPNI83xHG8+h9t15KVa2kyndS7dbcjJ+/uibXIUknimQUBdGBzBaJjqV6ubqJ2zj2IbbejdPf9/DY97DRcChj5PWxYAkR4vSsmOZo0wFRLMPBmP8mklLVMhqNZTSetAgBNhI4a+4QaISRcMPDEU+Ivq5lvdxJXpTy8vUVc9cYa4yZIguRTMdjOTs94nWdnZ/yoHanpQTch1yijw8dAN1AMttseCBhgwD606InOC4VQWA6GXnJFv3wqMjRO4XaD7K9daubIRzBbDDEsPI9cI1ZZlwGQs9AqEq5uVnIdlvyd6PhQObzCRGCs5O5XJxMZTodyXiEVAxQhJoHZApoAJHlQPPio+mQf8sIL4kJteOgh8M3shTU6dFMRkOkwRRpQhRalApt55jHyNUC3q5qmUwyGWWpTCdDmU9HMhikTK3o1DN4qbZDg4e17xqODuwHN+9jJTZsbPpwhEmxiaQGdF0UbboThtQc3quEo+5JcN5nJgAsgtc9OK90DQORwQGsqJ0eGNjL+LlidQjqGuvTokrsBYZYtB+r/03SEBnJhkjfaP4/TbH/4UDyeQuekzrXiqgA5dSBo6ODvcn2AHxuZxjhd5rigEdx2JjySuzsoHPNw7ym5xUVJXkSvqZa9IAHvO4JRJTNC3MYn1dpqQQGEoRtPTWgr8dUXs8aplc6BpX+qe13FmS2XDYc3sYN4nnSdO/NNDLRAUcIsBdZ0Oif2XlJjSJadAJ6KYh3sXdGCLro2hGBLge8nx7oezsdSUv/3X3tv6aOWRfV95/rr7P/9TetSKcA7R/kvsB45PXuVD9l0Hce9PFNGIYbt6UN3jXXdd/ubhcyMpi6aQY8nHFA5Tsj99jz3Mu7u2mk2G0JxSFvvNkWcnu3YTRRxQobJvC46QPrvUDufLPZyvxoIpP5gBN/tVwzYhqOUqILcBpkrVBkmqjDsV7lTDkAKsO/lzdLWa827QQGZJvXJXNrp0fHPOjqfKcOAfgGUSJFXcoOqY0WllPngAegr5G+Y9ZuE+9v0+lE5lPP6zOB0oZjgO6HyP2nqczmR8wJbrZbRpXkGyD9QkfAIE+bn4T8+b2FUnbxZV7wwLu5W8p/9d/+tVxdL2Sx3souLxm1YjwfnJ/Ijz59Kuenx/KHo4FMkOM08lFhOX8SvMxJwthgI6Xz4muIkQs2ZhsnSx+0GcwKKAKuU+85+QyEdM1pt03kva07Ec0ZSHnfwZ9AfrkudazmM/AysL52Ekel5NudvHp5Sf7JaDDkS5yfHhHqfvroVB6dzXkPcL3DYQIXQupEJJsBvm9kmGnUO4PjMB7xkIfzhg1vk1cywOdFGiHN5NHFqZwcjdvD5vpmKa/f3ElVVLJZb+ik5RtE0o2MTyZyNBvK6dGESBc4DHxd26t4iFnYnMa13ssWKNH941CQIK/B76glx35X4sDFOi+k3OU8UMF14NCTS2AOCL5y0eAwBo9Ad7uyAmfI0g2Wu8bBDzQR6xv7NVADncuFwc5IoWXd/shFgjfFexm8R/4EoH7l3gyySNKRRrsNMT9LWeDzFLU0uD91LQnQmgREPL1uRR4UGSAE7kkbHr5ARuDwHq6MgyvhZ08S7jV0unBfQXBmis0dZzUnCpIonCk/IE0UHemfF3QwDL1UhBHryc4K5yT0rgNjxp3MCYb2ev5v/MAdTPUs9s8dIAIM6ow/5KOsyKsFKUaAbu/TPYeAV/6O59N7IwT9w7/vDNy/iC6FcP/vO8gTRhgGkKrnY97yGvq9v3f727d66i2o0uMC+GsC6tdvuufcd0R8MqiH2ZFQugoDy7kdiBDoxKO/zq/KOq1JNILnmqYGMdv74BBHBApSUZNIO7k4hu21K5TIcSPLGItEc9O73c6Iahr11xU8YqxDg7bhECAqBhELe0LTMIoaYkNoYhkPQBospMgLGeBwm0xkOBzK/OhYYfYGbG7kfvE3A1lt1nKzuGHkvNlhQ3JgTiOODm35cDbKMhkC2idMCDgZnyWVDFA/qgIGyP2nrBRglcBgyK+Z/ZwHsafCzCHwSILzgzdEwyCM4+1iKVe3C3n+6kpeX97KalsQ3lWCM6DZmu+5zUv55OlDOndjvBfvqeUOjTjE+YpozXKpfaewdVz3kCp71t66xMbQOVmEnzFPDrBtUcv1YscD+c1qJ9ebXJ6/WcnlHX4G8EU3qiZLySW429ay3jayLApJ1rXE2HBHqKSpZJkr+ez17ZYRLx2COOXrA5bm9AX0HIkUip7LZoN5XvD9EYXmu0q2u1rKKJLbppBd3siLyxXJiBp3idwutnJ1u2PqbAGSYVXzebCrZSF5Fcmzy5Vk2VBGg0TmI8DJvQG3r8iyYS2Mh4lMhphDMeeYgXjvbfyc9r3GjEAMKqkade6AGNCZM04JUgqaesJxatUmiK4R9CPqR5CC39EhQCmH8hJ0tWFf0L1Stz/l9CAdhR1C9wvMI3UImDLwEgcjEerVgguDUgYkgHC44k8VWUEKAPeG3Ahz+IHm8O0M2XIGf1PHdgbqteGigHIgEj5oTOGMuBNP4jL2VuB5ihQpebIj9Ub4t4WQFTlHsdRE5tQJJ0ZrxGKgURgGXZu6dj3yhxO6F3y2oU0PeSYq6GPgfCBcj6VKzTnwc5GVRc4dsvRjf6v0s1BvjaK69Antb/wcfRek4CBS4Te5A/ukwj5C0EKZTqywiYHIFd4MNnBu2GlKr+3+++rrfJMH0HEZfJCs7Osth8w+C/SbDsE3HAHP5/aqFfp5bhy2JJUcZEoYxDpI8pKTjeU/25wQK3L7hLEqZUkD+txsEJUnMhwW+neETGNJkUyNGlltwJzFpmFEsKSRwTjlZnF5dWUH0EAXSqmeJQ7vDKV0USqT4YATKwXs1UQyOB4KQHKwiAG3rtZrWSyWkoHNfzwjh+HJ049lOBrroZqmcjGfy9l0Ls9fPJe/+dnfyM3tjSxWG/rVSJGyLIkbkS4MdXTffQK/zY6PZnI000gfETPgfjg5A1QTpJmMR3NuFmmmaYXhaGLVBSBBmQPg88oXojkCGnkhwaib4Xq3kV9++UyevbqU//qf/0Sev76RskIqwcrEkliOpkP5+Zcv5eNH53IyG8iDs2P53pPHMpuMNXrKMLrYZh2+1dDQ844tl8CQmc4nUUcSBofCowavUyMSgxwkUkZg/R9g1+tc3rzK5Xqxla9e3cmLq5U8v1zJL76+Uag7AtKUyMMNoshE7u5y2W4K2e7WdAZRmjk5RgUNSK47HgPX6xcySjH3QNJM5HpdyGLVZl+JEBSApSOk1taSxhsealiidApynTuv6i0Pvs9fbbhmsFk3QMaKknwEjAPy5ZhWeog38vJmyUP+Vy+2cnZ0RScGZF3OP7/3ximajlAaGcvTh0fy6ZNjOZ4M5JOLVHYH0jLIpycZDPcKjkwhZZPLrtlyTdY1c4BGzsW6KUVi3H9wJdS5VgQMfgHmDj4aSMEIADQ1oHwIXGiiiAPmCSAYpCvhuMJhSxBggJiJv9GDidC/IUtYA3ThuDbBFQIGWckg3XH5brfYnyLZbDDmiLpL4x3xLvJaYqQymNvCa0TSVHAI7HvOcSXFefnd+xrWCNY4VlNGLgP2OfWCsA6WlldHKgtOARBQVETp3Yi4lzJgiFEui0AB+4bObQ2b9GzpOEDK1ICzidcDgY8ROsbQyL0slyXv15x8lh3iOnANltIFQmvuEcYDKYg2XUEkA2cjPhecFkMI7LxlcGdpZez7fiY6kvAu9k6nWf8gvv+9ftWP5F+7n3UwnEMvWkJVye3trex2ucxmU5mMxzIcjcgifvt7378Oc0a6t9vLD913CjqEoO8QKDlQf94jINqfY1PQfwNi60ICj9QOJRXuVZsZa9y9WCeOtTl3fw7Z+F6frrlt8gD6ZBWDPD3/7BMTByMOw0E25lfktbFwJtlYxgOMfSrj6ZD14usUkYoIuOpYCshv1tQWgCcPzkJKJ2AwGEmSoiRnIKPpjCWSJ6dn8vDkhJDXqzeveUHUVEA6YYDJ3Z+sb7tP7+8UgOAGtEIRAlQB6OfGoQOEQEuHNK+upEAlk3VuQO86+gSeBtUWJXO+3AjKUm6XW7lZrOUWZLptTkeOETPyzzyX4XY1zG0vpmvWzqNeH4u42ZvXWhqmN71PkvG51stB9glv99djb13er1s+xG7Xuby8LuXNzYaOwKurhVzdbmW1yZUUiIgxTWSx2tJZxc9325KoEPQpBnUhdbbjnMRBjXlwW1eyiSMZJCVTTEuQYLFB23jTH6rVIUCJnPJs1IHFgQW0TPk2WopZNjvOcXfgsEmDR6MRqx48TsEocaiiXj5CQIJrhx6FsbUNt/bs0GQUyzBDWSqc8IzvezYfEfE5xAjRM8Aou0eDg6DUawBsz03Tax3wcyAD2PQVIXAmScsK1E9r3zuaaRU9xm73yBdrGA+OC8YAe0iqP8D6V4DCUEaS7Dq+Ae53CoKg/32N8QYaZmkAlvfiPZTW2PIwPAhr92HTf3Gy9odAC9uXMHSa1Q6R1KwOUF5T0qSaxuTVaRVJY2OFa8an1dJUdaz7a25PE6BNgRo52tMUhhq4M8fXwS3tPnj7Oq6H068xI9rTnW5tynCfxL//mff0c3rVc98dQsDykvuaA/3KAruWe//u/l5ra8ttIa9evZa7u4X8yX/zz/j9Zz/8TD56+lQ++vip/OhHP7DXNtoU31PZqs6+bwmN9ruOJ+gD6TOjI8l0Q9s9tz959AbZb9oBVlIhIFcnqTjjm0S7AzcFMFCZuwK5DZoCOysRYQlZI6tVrqid+R28hgj5RxzOyDEDHlf0AOkAvV6Mi+a4AC0OEhCohsyRP7h4SLGes+NzGQ5Hcnp8KuPxRC5Oz+ScwjMaFcNJe/Hqml8Xd3ckGC5WK+bbk3Eh2exUy/JA8opjuVuWst7t5JMHH8vs/Fw+/eH35ceffiKPX7yQ0/MHcnV1Jcenp1LmO3kwGVPAxLUVNEp6t4n72wy6DkezY0OcgFhgfGNJjW0Mh0Bzo+6xA/nQ+6uQoObIbcCVMGWpnJ9/fSVfvb7jQQMo+vL6Vv7qpy/k9m4p2xI504HUFQRDVLAJB3/SVLLKYrkdDeTZy0tuvh9/9JEcMfrSSe0bBcsN/WD3CU/nwevebDu3++RptlbPwZwcF0ZhFAQyndVUv6/9l//ipfzZr+7k2aulbFdL2SEnj/kPaJOkMc1nbpavOVXJK4BjhMO7iWSXb2S52dki1NB6Ba4GImA6nGCYI5J3ZndHnLM40xx/h2v5DzvkNUcbLbc9VNWqaFoHuQsGYNyopZHF3UbTb1xjOsaExLGGDGFjDBjVcn46lYcXM/ns8bHkxSdS5SiNfH/Li41UJcSVdu2cIQEXgjYYlVRz/shpYxxYEmwQN4N8kgy9VFVfE1G+Vs7YXlEndJ50rICFgxSLr3DqWZIgq/VSNutcEIdNR54es0oVEGhjVDWh/DWiPgn2hCwpZTLItWxXUjpcm10s610kaaGcgRFSFOQv4Bk6P3iG2AGshzDuXcX7q37wYY4reEscD5Ir7XzifVUy8Xgy1n3GUiGK5mHvrU3gR8cfvkBZ4vNxcssg05K/HpWGawt7NiF+Ox9YFWCljHQOQDxG+pDVJOqMUdLEXTOsgTQVVHt2aWshSqFz0R0AJT2S70DoRVMgihLiHoEvYnPDkAt8FqAQ4CN8RwhBRwDcdwj4k99CIHSo39MgDcV0lsuVPH/+Qr766muZTmc8mE7PTtrn9pN59z0i99jcu7Rwaw8Z8Pe2LFUvL6Rf973Rfai6DcoIa9nnRW7OYW1Mskon2aHm+WkoCzo7nSWRDhNzYXUpJAaSOCdIKlN+AEsFDbbSxCEOCx0NpGFGw7FMxzM5np/JaDiR89MH/NnZ2YVMJ9joLuTh+UULg603O8nz1FI6qJbbyBaLBhs2lMpQw9TqCQg3CkQ32HCSbCiT2VxOz885aVeLJfP0V5evZbfdyFgTjl253gc2fF4sKFQ/pCgjTEGk9HKiTrHSST/mA3b5zV6ErV91bPFZbhdrefH6RnY5xqiUm7uFvLlZUwGPjhiVxQp9baIIWvbDcqiiMC6FVnRYoqQtIeum8T7y1s1Kj3h6iJulNL7pqN9Hlw5ztl5er+Sr10v56vVCZLeGooxWURKa9vWBqgndIMlNITkFE1eJa4y8rDSNEb0Vfqq/2+Vj9bnOjTHPvHWEOqKnonldzTeRNbtedwruI4j+b49M8wrVNHrg18i7t0EGmPlaSrmDeiCqblD+W1VMHyB10hTbg8YU+XscOnAI6AxA8IsXr7l/1vXzIwAN4CQ0QjMidp8tzjD3j6fzCE4Aon+W+1bmQNkG7Okl6glGCX+/y3VzQbZWhbs0ANN0LRAZQNNY55q2bKCkCk4Dg7zaHD9FLlnSjJJGIhA6YfFVU+/KKcCBy7Rxb51h/jhj/hDr5r2dBhgcA1u8Is61Q5DyBeIEAaU4Bn8DBzruBQIurGPdg0s635627nRsuI575cLOC/F/R84vwLgzUPPx91tiPzP9AUepXQnR077tV+xhPWe3AwR6nD7XyjGn5V3QwXdLgH9j47FLaYWJvBbVhSr6kLCyP50PAAGJzWYjr1+9ka+/fi7j0ZQTDWmD3/mdH1FOFtCjvqe+hg8Ay+FAbEOOkFGH5vJxOB+dHFNNDpGHrxnfEFzzovUc94hYPpm6z9pWIDgZpmWNOmoASPKwnJeSeVTBD1511qiELgRWvPwQkxA5TuTDEYFiMiPPjEkLjxiMdSrgIQ9JCEs3ttEYNduZPP3od+Wzz/6hDEczmR8/UEKdRc3lMJNVmsplNZZyic+D+7Kkw/bi6loPsTKWMhpLkUFarpEB8msYb+TfBwPJdzt58cWXslus5Oarr0VWG6k+/Vimo7GkDx/KfDyW9WotTx48kvVyKV/99Cdye3VJdAPBC3KHiFoiREZMgUKz4P0jhen0WMbjOT1vyKwqWqL3Gf9ry3SsVI8qcPgP5WzmWFIIyup59YDChljL189eyF/+1c+YKnhzvWTebrne8DmoFhlmqaziWPJsgCSt5qwjra/2QlIYkBKI98B50TLIrv7ZipvbdeWbRee59GVM7QC1qDs2cRUeHCAWcZMfyCA7LOH9iy8u5eY2l3xbyINRJGfzgYzSQmZZzvKzyQQbqtXSN418/eJObm53EqdTidMZxZwGoynFsdKBohgst41jOqjQf+DGyzLZnPLXfU0KssHx3NGIaUV1DAwhaBnVeq10jD1XbCWdcMowLkhXcZwJCfS0SnDtMT5DLINU+TVRBBXGWF7eruVmvZN1kcubyxuZD0R+9fWVNOVhDkFdbulUbjea907SoeWWNW1EwR5KkoPr4ntZIw1QSRMiQhrBZoX+G04Y5GqhVFk4uVRr55O0kCguZTAuuJegJh979XiC98O4IJhQYloJZVLmp/Vva0HqBVUNlUxGWl3AOU0VT92jjo8Smc4IaLFEEumbqlaZZBI9ETjkTmZ2XQ3lFShpD87GYQ4BVVuhi6ILz/i29hmwX6Naiyq/0BvJ5OOnn8jp6QM6R00E0TDgQUBqt/Ly+Vey221ls17IerXjHozKiT4M72kV7GWqzpjSScC5gOourlV8VlQdYL72UXQCNIqyJnjvPrxvZ5enZKhWWmmZe1HscwOckAgHs1/a7by9d0m/viOpsJMp9g+lX/tpgs5T6Z5rUGwPVVANfiiSLeXm+lbevLlkJHdzc0cGOwQvEBa3bOteeSAGe73eyHa7k8XdkheAvwWUMp5CkhfyZu689LfhPs9hPwq7/70uvV6kZbWp/aoFeLiEdg4yXUx46GSzXH+GshkvlXQOgMpnYiFXRU4P1meOch0UJgKqgZkHHf7JcCTn50/k+z/6NyQbTmUwBgITkcTChSmV7KSWZZVIvW1kt8U9ARFsI29Wa0JgjQx1sXCjBGEIgi9jwomT0Ui2qxXz6flmLevrG8mwIW93MgTbP8vkaD6XYpfL8eRIlre3snz+UnaLNdXTuPeZoBFLJmNs5ofxMobDiWRD5UjsswK6WuKWMGq5ap8XXtet0KGxfXEfOGdrubm5la+fv5DL64U8e3XD11QHK5GTk7nJz2LMQLCqKOcMT1Tz2B6zas8B3EOVjbEqEp+ze9EB/AmHwuxvewTX+2VUcAqVRGUcFyxycCcORGPAGVivAexUMkkjeTBLZZaVcjosZTio5GQOchpEaYzlvLgVWd1Jkp5ImjUyGKHkdUwNB8rXUklRnd/ZVNNZ2KxRhrnd1nKT6oHu+yPUMlE+N5sNZDaFDLfmhdVp9koljT7pBOC1KFetTt1up5vpeKywqzD6VwdQ9ybV5NAKgqGiSPFUqgaOOhySSJ5f5bJcgS8ykNc3K5EDHQI49Dggt7tGBnDs04FCzUQFgV4oIjIQLUvWPDLmzo7Mfk29WCmsY0w476pItjsIMlm9uuphSTbEfBSJsT0K9hIcRhBwaqQeGevf9jUvedS1acgDHSZF252gqfSXhofbONXrANqwKxS5xPhFFEHTe7XewknTk44Oa2+FYi28g4bOW40IlRG1e+CSBgLU/9c9M4O0eoYy2WN58vAjiZOhKjlSPKmW5WrBtNhyuZANerOgQgplXSRvWtRtmgCuY6DnHtAVdXgQrGnqzrkBFog4MtBWQyhC3M6LnsPvCIQ7y3g+vvfyQxjnC+Wo9b9+BZKnvb+tvTdFvn/wd86A5i5BFKS6kjWNAJlsPj+yGk+Ue2ED1YdDI8vlWl6/vpSry2u5vV1IVU1kNBl2xK66kbu7W1mt1vLll1/KF198RS3wxXLFKcXSofFI/uiP/0guLi7k/PxU5nP0XTA8vd00v0XvBMtTdk7EN9MJMERE+AyHmLKiVTNcURTlFVSArUBKsUY8QAxA2kL0goiviRKF7W3SV0kJtgChVhLWmkiOHjyRi0efyPTR96Uan0gZJbLYQLNAnaqW0V7XcgV2LL3ajWxWCymrXLa7Ba9tejSTwWAsx+dTmY6gLz+QyWhMdOBkPpXNaiVZvpG7mxtZLJfy4s1L+fyLz+Wv/+YnMhlP5Oj42IhIWlOtCXPk1SpJSezSEjNtuPIBihAxD21DcFTA9SP0Jwa5GSyt+W8vf+qeq8ROO5QsOvv+0wv5d/7od8kdePbqinnxl29uiR5sNmuNgrireZ04BIaApKRU2suGQ6od4l4mezrnXo5lqmbcxPZFuNqP13NmNUr2+dmxZ5xc2OcnHmKMTqKY2vmTTOR4iEctDyaljIelnB1tJEsgXKP13y+zO7mrLqUuc6m3G0nqYxlPhpR7HiMiwuEB3YyolEmUykhwKFacx4NkJ3VWSB1rPTg+C5pVAXQ5mkYymyuiws2xrGSz2pJYCI0B7D+Z/Q7l8tFQyYU7ux+YvzjgtSyu08RgaSqIcglKa+Gw1BJlCvUu86FUcSS3RCOB9sSyxaF24OE1GasaYTYsJRvEPJhZK8/0VMOeD7i3qALoyGQoJFAxK85YF67iNoJoVNFMkAPjHJGr9bwBe36oqQDICMN5S9MtkVTJEB9rYx8sCaYbqBegzY0ilJRmiqLh0CbfPkGjIkVW0swqX8gPiNnnAU4AHQVLabEnBOf3TjkSVlLpkDmCHYiB5UhdHDRPwWkBGmB8MMugslKilf9WEiyegPJiVGgRPdiCIAu+TS7L5VIW1ws6BOu7ney2udTDjBU0LFmE400+knIUmtI5P7r/gLSI5m/gVoBrQ7NDPqfGQC0xxpZVUOjtsN8sSZ9vCAKrUDyp5mXEeg0wncuYx4ri7KfcO+fgO6syeFu1gXs8uCHX19eyXq+ZEsDB89FHH8t0ik5L8EittBClRg6XxjH5BOh09frNlVxf3/ETnT84sTpwzZff3NwQSfizP/tz+af/9J+R3AKBHSwWRM1ovjSZTgjVoMHF0RF09u+xtv82OZG9Cob9z77/9/pVCWrvb4OBaeCzTg2TwyBzhzhB6iG8pw4UNPnhEERQDrN6ViywCnW9McqWSomgVIjGLY8/kkef/ZHMzr4n1eRUdkUu18tLLXcxed0YXmdVMfLFo0TXw91ayYgZpJEzmZwMJBvP5dFHM3l8MZbpYChH47FMR0N5eHoq69VSohwO3Wv5J3/yz+T56xfyi89/JafnJ3J+fiGffO97vNfjbEimtjZJqbgJpRC0NUaywmOHlx1iU+KGQB1x9QmVFKolOW0DIi8x9Y6dzoBuHQEn53g+spHPPnkoF6czeX19I89evZYXb27kn/5lTnb966sFSaaAt1VXA5uu3kPK7g4HdAjoFMAp7olyeQoK892dAqJke5/Lvtrf9POO3Of6z23ziV2K5BBD/h+sFkzXWSZyMhI5G9fy5KiQ8aCQB0drGaSNjCcDjv0vs1t5Xb2Wbb6S7e4O7Yhkcnoiw6aW42ys5GIpeCiPgOqAQY95kTQyRDlbljOnr/trLLNxI8NRJMezWI7mFo2hWCwv5WazoUxyubplcyeQVdGTAmsFVTRwCLYgriWxQCQRUxBbp6oDKiMczsDQHDgoeBP1mcBZjOSuHEqZpPLyZs0tE2ttC0gdTsEBhs+UDCoZ5pVkSL1QEEeri3DN621h9xVQtpau0bkFYRMRuBOrqVdicROJZvBboP2gpX+DoR64IA2SH2AQP7UMmpqSxuAXshlaCWQB2S6tIFBZANXz4OGllEs6f9OJpogilOzRqbfSPM08sERStRHwTqqcSIcApbOCFIVF14aIDod6OB5i6PuRVJBKNwEhO1Txupqq0D1ARZzgEEREz8AJKnZAlbRHBhyB28uFrFYLWd9uVSkWApIDI0Kag4kFoa+viq9w9NkzBqlfpkThENlewpQu0iJamoj0qDacc7QMCqkqme7mFVAgZbKmxMoVdW/qPjdRBF6Xljo64qp76b8EhODtKEHUst3RFvjly5eyWABq3FBDHBH748cPjQ3fcQrwlR2f0DUPIi+XV9xI6/qxvQ82ThXVgeOAtqNwDrRlpx6c3nTixYsXHNizszM5PT3lAu823h4967egBAoz9aKt/d/2WYxyqFFHwCasOc0mw+9OlnUOIyMb3mxKvQBX+cQEL9C2OEokRR7QNAXANp/PzmU6fyDZYEKiDJjMEYh/tXZUo4gHHAKWgVorTkq+qjIbcls43ADVYkMB+rBFJzZTdoXl+DuJ5OjkhH+HlAaclOura/n8l7+W7WpjiMJIkuNjqXZbUskylkkadMhoXDdDai4ceH6pB98r47HyzZaoT7YvsX77t3UU23tfxUWpMmwwKr6iyQ699gR165oOAenz5m4lf/E3vybpEMx7rw1uPXT7nN5ZzsoEOi6O33uHfCw1oGiBeyoGn5j331Ye6LFvZbA9qT0zvseBTWPaz4EDILGOdZTvg5sAnQPkOAEdq5MChweSzU2DRkTq5EBLAL+bFnP9rGzAo5A/FPpY0UFpilpL6I0EvKdqa04auS2bjazXK3kG/go28cWSkDH2GXT3RIAwQBtwQtPKp6Cj5yerATFMF/hYWdpFHVOVlyGPg1CdJnj8cej65xyxgwsReUtsZWmxCg7xeogeImCwaBfXnfQroTSqZ8GkddFGJlMRTKQ78Bmxpl0YzBu2OU9GK0LoEOAwrfF3Gt3rvK/VITCWMucTER5tpOSOK66ZjbmYDrLywxgcDhzAONTgRNQyYirEnT2NsNlvhCjEYUZuCUtPlU/DQ5PvofOHiLRxxvC53qAkGg7etuKD6ejNhufXq+cvGdTmm41C9ttCe81YMzjsj6haQFA7HENSG6kak4C3FvbKo1R9GHI5oprOJc3KM2EqHNZItUeMh6Onbb+x/7ZtpxH89ZUPbb+DI+bjr+l6Lan3tPu3sUNVdd7KGbhb3Mmby0v5i7/4S/nyyy/k9PRczk4v5KOPnsq/8cd/RKgUUB++sh1umrDsZbddswTxl7/8pVR1Ib/3+z+USCBhqh27wDd48+aNvHj+Ur7+6pmKy+DwMwIWPLu//Mt/IV8/ey6np2dycX5BmdjBbGKbsp/2v+UD+UFCfLv9lPZZ2yfJhzI0ohpMVF60U7ZyOM02wlgkY9mcyu6OsgkdJvwNIvrNBge5q96JzKZzGQ7H8vjhZ/Lg8e+ybj7foctbLhFbF6NzGZq9lJJh82VnxK2y4aH3XQIdiGRAAteAgi2ILrZVTvGYpWzlVhI5nc/leDqnl/34o09kfnQi4//mTzgpf/35r+Xy+Wv59JOPpNls5fT4SEY/+FSiupS0KWQEBUHmuyrtTU6YHQcNVBIP4xDQgzZ1Lz/IaTZPvA+BC3cwb99rhgU40Tuc+Y6r+exaToczOWdpn5YCbXZb+Yf/4Ify6s21lMVWvnj2Wr5+fWfRnbHDEY+ixryCcIi3MAZciGjCdnizPtOewUWvxlw7I1b6dxRKcTSrSzd0csX2kZ1/cmhFRysrrQ5BAlgd5MA0o5jO7TIn4UwFsUSG2VDOT0750YoCFTSVXF9ekuMzOwFyNyRTHeNTgntVaeMYQMkkAPZkYFTLwVahlYgu7+7k2bOv5Qr7zD//5ySrgqeCe/Tw0SM5PTmRp0+fUvxJV5Ruzg1ScSaVS0TOuBpekdPqGvDIx3OhPwDCH+55JnWUGa51uM4uqhew1hDMIE3AVFbbrlZLh+nEUHYXYbndT1wronr2K1E0DMqmsPEIQUMkg4lKDXfkb6wJJUCD8e/aAdQPQHtziDwxz69fhwNFzEq2YjceAYW2cGpYbwM4DRA9M3ni1VZ7R8QZUmSaGozinRJrlV8nM/JHMPaqp8DqBDSV4stnkmNOHGAsL/ay4rZIxQ5NIMmGLCV0+Cr5+c/+Rn5W/VQ261LW64KicKvlhlE8UqA4pxC1cw1ZhYtC9RX3xuPTE+V1ReDHxHJ3vZTVeiXpMJZsGLNhHCTj1Zlywh/uMXpnoIpB0YZ+czddbg79g88Cgn1GlFj9BeuEyJQ8dD00uKOK51u0Sup3SGu/l0PgpRFvM/wcdeDQk5/Opm2qAJH9NWDWZ8+1ExbqkFerXi5bBwOsztu7OyILm82urbvEJooUAYiE+BuNnno9yCyKw83A4r27vSOSgPswp0PwPlF9Hw3oVSDoJ23f9xBr88V9wqPlBv1dtP4UkrDQHECTF7CfC7JKARP59WAzxZgMh1MZj2dEBuIkk6jO26qELgLdZ6t3XQQULoc/D30E8ATAnh8PB5RwzQBXQCWuKChL/PrqkpsqCHKI1vBzRDiIfraI4tCmdreTbL2W12/esJIAWv9rCPwQCdGa6G+Whx04pn0H0H/+tyXUeyGXl6qqUI2/Zh8N040N1TAguaFq5tGDMx6Il3dbkYVGw50WRi+co+OmokmtEmJ7F/3/e6mr+5LFruzYG6+eSKoJ+PgQ9KWODzGLZjRBrG11PbqEw7nFIV5KZV0VQeKjk8o665iQMZwqOERY44iO0HkTm6giJZo6a8e4X8bsZVx1Ldvdjt8jvfX69Ru5ubkmb2W32dAxbXuj2H3rd47Ul+8rERppzt5P//MiTS377a9Do3W03x86pJsNiIOx5Ohn7IQz1MVrU7v2Ppa5deNkSWBXvg1/E/XznBJ19zPs/3HScwhUGqgTEbKSS23pDhImhJ+iPSVHRZRQX69kWFX8bKRgmwBtswxyIPzZ3UBJ0ZCqBhkxS5WLwbnCRmwgGtp8ZNoCOXNFMLQ3he6zcMAO13Xp1k1bcusImxEstapBr4edMXN059zJagl+g44ZpctVjk33VRevI8dDUVk8N8s2Ug1rmc6QTxBWvW3WW9Jjkh3GdsimXTjzwK9xwSZ+Xht/r8BQQmevj4ITCLHlojqHKLIF3qZXoAGjnoNKI/EGfWrsa/G3ceY+BEKgAIgun34jIRzen376qTx69IgH2Pc++Z78+vOv5PNffSG3N7fyi1/8kp7O0fGcXtHN9bU6A9buE/yDn/3sZ/z+Hzz7PfIAptMpB+j66pakQzgGienQuzdoFyVvXr3h+/zsZz+X0WgoP/zsM7k4P9+riPh2RIu/LTXwYVIGJKXwDlobW6/pdsVFQG1RJtPxhI7WxdmFzKfH8vrmuby+emlEHxXjQDYAh8z89GM5Pnsk4/mFxMlAmjyXEgiAidXgnjEnZf2+kUctILtJLfVSompH4aCj2ZzO1EcXp3I8m0k20mYli7tbuV5esXTwb37611bepgxxqBJSCIebSCLrupbb7Vbu1iv5+S9/Tq82X28Yzd0WiVTDuTRUoUO7W9txD0Rg1OHsKjS8Rpcvz1vXg+T5u06JUv/GlNXavzDCEEvBtEKAhCtCtJHMplNyY/7xv/vH8ub6Vq4X/x958frSyKx4IGrHwsQ4gVCFVEQqKZr99NF9E9XYd3SVcOpQMr6nwp7CB61uv1WbMb3kf6jlTgrBe8+Q9zbmgrV9rebeVXFwhXZEm1wWb5ZSFSD3wfmsZV0MJC9TqWQko+mEvRDW21Kq3UZevXnFNsYPnzxiwyLqRaQopbUyWxLBmKux9zaho6aQy8sbWa2W8uL5C/nrn/y17LZbWdze8D589PQJCcwnp6cym88lHY6k8DJMokJoEQ1oCvfD25pj01dhpCiGGIx9Rna94+mlBwQ3y0ayppYUEHp7b9/fvnqRCSrTUA2gLW41hQCAgqvAHQJVI5aEbXRtwzfVb6pcY9+wBkZA8ngWA7LWRL5mn12xsKdZwFQCzyg0RVGdgc3OyCht5011HoYjqJvCCex098HCwIE0BNSPQ6vU9TGfILWmKUB15ER2pR5+mnlpZDwWXutuB0Ejb7AAkabDxhQoAAeA4kcqyrOXRcPhTicb6zkm52CH9tirjVxdLeRofiRPnzyRElygBs5aroRvekW6uLabLXluOIhvrxfWfl7b0F+9uqJMe17tpKh2Mj+eyoPH5zKZjOTh41Mlh3MKQkJaHTA4ZkBtGOVTx0PFxPQ+l1KkiVRlJlWhDcuwVvo8PlSTsZkTnViXgta1VJS5ijX9y0oZ3Deye5EvThLm8OFF4SBHAxxEBozaUZqFnHbdUGO9X2+M6J8Mz7s7IgpwGtiytIFq34o5HS+/2Bdh0YEA+QM5HCAM+Hv8DfWtmVfZ5xF8W3t7dNVBtYcYS05sE4/32tbajmBCKYB82YiHxELvjmgiFvY9IEY0iUF5IUiAOKQ8SiY5zoRNqNFugKeL83QfyfL5OCyxmTIgVHYR6QegEAGpQf4WCMHlpbKGUWcLpUXcK6R4uKHGdAzWuCdFQUeNXRsNIkXVQ4wSL3IiVFJU25ceNKQdhN7CbhotuJCVoksG+PphQZaulkK5VKzzOPQl/as1oeLzAZnivoA0OGA3QxzmaOVLqdQ96K8TIGGEhDx8y0pucwU9ZKDPZfnmkHQRak9c6W1j0KIHHyDNZc6aq2GyogViVUUl2w3KKNUpxHzIGzBFEhXQsTDfldNQjQF0C88tS9WNb2qtTnAdjX4tta7rnK+BdX17cyPXN9f8iv2CURQqcNh9cSTZEIJUGTfPFh9om/r0BtwFx1otFetpb7Cs/8Puhq4bqzaxOpWDhhNqgmURSwlCWwkeFHL4cAD0Hb2/PSJoVhx1xSdOGdHDStHoFiFgTwKDMvT5TolVU60NEUhTEHWwWJj9C1COx49rmvqmuZIWkVTQLcF+bLwHfMV7blWjTJscMZp2VErTGl5CCaM8Mlj5cKgHIls4BFt3vQ93XPurpX+HWiGgzsU3Hok+tD9GrfwXQCzUUUBFnGoIdJlc3Y+9o6A3DUPFG9NjDL4KyctcduWOKBg61zIljLRACvS14w5wre9VsHmPBJu5Vnqu79cTybNUoE5RF6Iz2W3nR5vy6bvYOzoEXTjTtte91/IYX3H4Y3F+//s/YIXBxcUj+f73fygvX7yQv/oXf0UP/8svv+KhgtaxiPgxLQfDVHb5Vl6+esmyt+Q/x+Y6ooIhPvmzr78mmRBwoed/ydbswYA8aKpSfv3rX8t2uyGk+/1PP2XVwfHR3NiXB0ZLHxAhGE1wuGs+GIc9dPi1Ea627MWhBFIkIkKk3bIG/cgjOc5GUs1O9RJA5AIsl2KBj2Ry+kRG508lSgZSbNdS7nZS7lQrPSp2hO2Taqc66gabsq98FMkOTtpuJzuUIn71UtbDgeR3CxmNMmlEe9xDdwDCMWhS8/pmoRsMkAg0lGlG0hw9kGY05OOqbOSvnr3hQbBcojkNIDBFRcr5sZyiD0KWkh2OUsuSUNxhUcJ6tWIUjbFh3g/d/zx3R7nRgvMWfRiQn2u7GPYKl0lMskYi3Cy1not/j4NQS46sSoaNoRq5ODujIuPJfC7z8UgWmx317sFHSYdDvh8Ib3ig2Q/KD10xr4P6rLUxhUY0SuAaixNl3Ruq0QllGZnQ2iFrbaeT03pd0g40dY7UQcF1oadDKlvZIKrbbCXfWtkqvtaV7BoAwBVltisQqQgNV4y8Lq+ec8yLfC3zozm5J5PpvNsI27IqzedjztxY5dLlmzdydfmG6anV3TXH/xj8lNFIzi+O5ejomIgixItGI0RUfSIxNnfr0Ged52BO2NWmMTiwrMeF1qvZruf9BKyboLmvh9hFUss2LmUH4l0CQhnkESJpTIpcHQFDiIj+6K47ySIZ43k83BHlNnJX1OQKbS3lAFEj4IDqXNWGHlg6xQgTCeZSLsz3jwapjJJYjk9QkhdLnoAh38hys6UWyTBLJEtiWWwiuVlUPOCLDbOHsnTRLUIakdwtG64HnTieqjHxKOM5ofoBNBj2ZDFnBtmjQ7sdKrHRuRNwusH27zrBasMqoK4DIiNoHof3XS9RUaUl2ZDUh3MLATysobPTE6ZpVVIcL1LL7R04Q+BioY38Tl48+5KCeNhb0LcE3Jq4alhdRBVaqWWzW0tRYy9w5WRzVE1LwwM8/Me+GtagCBgPuB4FRJsiaFd4B177LNa3JaOuBCAf/bxMPyH4egfU5d3KDp3z29di7h2K7r14u0gV7aF6swwH2Hxj+eLLL8hwx6YNJACHD7zCJEHrWUwOkDtWnLOf/+pzlhOOxugKp1A1lKjgQHh/A+9H0Hm/utDBQ8D9A0qAjaTrK/92v/7b5Vn2Cru+Zerhtxs7YZlDAOUs9B0gP8AgO+yoqqymrn2MSAHlinEqk2ykyBfbIGu9cJyOqQiXjmbKAMeGar21sfKoBohD2RnU9h82myyOhMEJFlVRyg4H+A7EqpLqcnW1lrrOpdhtybxF45qb241GMtmY7wdILB4OpBlk0qSpbNENbaks3TVeC2VO3GxFMvAhsky2ywm/R+dFpB6Ulvb+BtgNDoiXrHJBenkm0hmQ3aWYFXaleykFJ+IQcen6aFgT6VbQiAVWnrOGwwbnbjRiBMXOndDYiBAt61yhkibU+lAOZzoEmoYwONKqHFhK6p3UbC7fd75hLarT8h56A7BX3aDvf7j5a8AZ0fIpIgS4X+gch42HyI+OMX4OVkGFDn6o3beUR1nsZHl3w3WLhmYgWmq+d39lOsGvKnOiSjfXV1z/b16/IjMcPyt2O4lHQ2o8AJUZj4YyHg9lgHrxAbQOXLRnP13Ig7wVenJhqj4/RDkeHZVxn1vi6MChozpBQyhqH2gJpKaUu4iVKDvWor0RHQJwogaRHA31SkBuhBPfbGp2XyTJzK6atesmGc1ue1aBYOq9pojcgKsoGUo7s1gejNkvXDbpgDILIIrmRUSHALwAkOxWaFqECzKJ4hyIgQUWeOGcJZEeBLtYT5/Lo4rWnP52z9nvgPvYYftp62D7e7UIkP3bglnuqThErdMozpkU7dsR1CA4MQ0dPB/lwpPpWLs9cp0Puc8hwMJaAKET5Yl5Dp0RbdxWQZQoVsEyZ/mzpwD2C/j5RCitc2p73RbR9+aj9g9SR8uJzkocVL0K3Q/AMUMwqOdDu09Y1cp3J128B7b4YnHik5c2dXCnGwbzIcr/kN9KG5YVDkcZS9N+8pO/kauraynKLSee6tBnshSRFy9fqg4/IzlINmr9Jj4g2uz2B9KlZr1/PUpHsGF9+fXX8pO/+Rt59OiBHJ/M+dr9s78TcPjtANSHqyvYt1SGLeEMYhaJqOeq3Uu8zCjmIkNO7Op2yTphOAwSzzgR0oFOnMHRTNJsLHOIBmGSowZ+EMkoGcpsAI8xl2LL5vXSTBJVfaxSCq3ks5Hk+VTy7VRWZyMukiEIiSA/lTuyoZEjrKpYinos22Qk1aiRCWqn6ZJqIXRNTXakFxAVbm1CmkQwo2yT24X+YQaiZCKfPH0ov/vkTDXcucBy+b/+Z//5e48pCFCKICmZRxEYzBPUkJs+PA9pXWyuhdFCxW2GQA8//tuy9X7UKjELvQrgvOkc3KFt9WbL0qNtXrEZDHLh6KcAlAs9I05PjuX0eE6HwV64/4V5WfA4NJ+u4wV2u6uikenN1JJ+JkQhpMCxrKlDGXRX8QqGPlHhENPXAX8nyyJJ61ziCghJIikIgtC3Tyca3ecJ4Wdo329LEIEr2exUznW9WJMj9OLr53I9vJLk2Uv2v/A+BXC0yAMBT2EHZ7KU66tLCmDtdhvJtxs+Z5Si6iZrkTWmF/OC+wXuORw+dOL0/UEjNe1u18Li/FjejQ+RtOZ3qf2Be8R0FtTvrHHMB9wL/q0nO5EqVwcW/jqvp9OpB2/CCYPKIdCDdphibeq9oJ/fRLKGoFATMXLHV7wW9gsSP60xHJo4ESCgdEokO5AAi0Ze327l9fVWyjH2dD3EHgxzzrUHc/BTgNSiLTAO+4wEwV2BDpjgBjRytQLJDgeVpTjY7U9RCVQ8UAqZDHghYgYHpfD8OfcHdVgQFB+qmQFxoR2JEd2N8sCeZwV1CFCarQHYg/OHMpucyMXZQ/n46ZKo9eL2junPMeTjUS67Xct6vSRSgnQ0zpA//KN/wNTX61cviSSsFnfUbxlkU0mjRIbTsSTDOcX1ZiczytKDf0XuLFNnyMNougbzVHsq3FcV9FRWR7j1MlAilabiBscEIm8gmMbJriXW6nqAQGD13SIEbf6ldQZ8v/H6Tv+qECM8KjygVDafT+Xy8orSkNApAGJweXXZlt6gbpkEFdMhV7lRrfV0r4mliq71bx4QytaYRyfsh2YdOR8Qy/n1F7/m7Pi9H//YYN6+Atxv2Cjv/fibTsGH2RbQQIUHJX39RGLQcAn3KfNYEV/I4WpEtlhtlMk7GFE+mFAgBUgGcnJ8IelgzBwqoglsGoDmOLHHqZRFInmypUOQQlIUsGSDVseJFBUEXFCHO5bNbqSIAhwPVAtcXUmxQ/c+/DuRHDIz6BGQoqzGo1cdk912xYoDyLoiBYAIsKo0/wtsDl/II8C9SsdsSQxxo08uTljPPIhRE7w7yCHQOlw95NkjoldyF1OBZV/0w9sk9zSMVbamjdLvNQkiP0CbzTCGoJOq/ciBeAFOR18O0hFiRKoQJULqC10YpyRqkoXd00pwc8UxndumzeBdRnGtIL95kzFrv+1Mao8wPMqiyJF+995j2V2Y8RsYjeCwjSWpMHdVfhiHCNQzMSfoHGH6WDc+jkteyHoFgZdctmvMDXAIgOIk4ho/vGfM26qQEwZwtwEki1IwbLhbHjAYOvwe6pnQGYDmATk1pqcB4zVaZQ73CJBlW/a5i195rN85BESOOCfMKWETHNPK6FVzfIgx/f2LnQwa8GuApqjwYdvmXKcXrxXQr7heAvU7+nuwXgtKIbUpl36mkoJLDOIVzrc0HTl3A0VWofq+2TZSrXN5vgT3KpYGEW6dyDFaUkNNczKSeJjKZIoUG6B26O4nsitFbreRbPJGnl/nFGoq7ZDfVbXk0DJJQBxUwhy6IMIpuF1HDC5WdCpU/wD6M5ipkC4oQYY4wFg2TQT5m/L0INkC5uf6gZ5AGsnx0bE8fvhUzk5yWT/csYz1cwSeZSnpeMSvz589l9VyRWE3dA393vc+kR/9zmdMfePzAeleL2/Zn0QaOKNQoRzKZD6V4Xgo0/mE0vdVVGgTLQpCIZo33QRPT7WBRj/NZa2/3BOlRLFCDJ5mwB4NQ4UTHNn+Z/e+Ed8hqbCbhL/p+/5POwNZI2E5IsiFk8mYGyQuvCUT4YZRjFs3a/zMvR1C/vYhAeEgV6svq7lS3DiOl+WM8F7YFJarlfz611/wgPv0k0/k6Ggujx5dMN/TJ/S97Yr7P/5N8gWHbgsJnADmq7EhQRbT63cN8sEqA3+UzGYoYJWSNLkMQV6DstVgIJMJcqYTOXn0mJrxo7NHko6Rpwa0RYUSa6eMCG3GSQWmNLcFOB/I91rbTggNbdA8Ki/l7nZDsoxsUOolUlRbslZRkYDa8wjRKXZ9J8TBwQAHACSvOue1UhnNDi0lMCo8iXHDV1QzjLOBHDGqs1pfICSHjCk2ccNFufgpRmJ3Cxu+KLqkuWKNvFuEwGqxlRjV5fb7TgSPEqtjp5mIDYlJJipDYRlCkoistGxzYrA28pHOa2g5lOZwEADE76gPr0z7tqGYVRa08Fu7cXhqo0d+auuRFZHR3u6HmUOR7BkyGkhW7CRtcBBZ/4ZKZYP1elwdDsqXihBoX4i6g1FtzL16Qv8OcwtaAwrHblaql4GNXMXMNBhQB05r8IHK4CsE0OggQVjHJNIxF5wz1FZ9tt87+XSfOOwHPiI39BdALwaW8vX6uPiIHGJFkUkEohnaZNtVtsmItkK1x2CIlaC3h/c4wukttK03iuoGKcmPtTYmpkUwz/u7ZI1MokYezSK5PoaSYCSv1pWMAYxAMRJqp8h5a4JMRYsbsO+h66/COlP0qZhrhQKZ8kQntCSVSJ126pUC3S+bRjZlSs4DHAo6jOQrW9oAjmRRyn/z5+8/pngtLb9WRVIVIbPDkWMEDhMQjwm7vTpBG3yzq8trotAXF2dEnR6cnhKJevH0McvmL6+v5HaxkJOTo7YMHj+HswBHhCJx251sGZiaxkVTEw1H+mowBp8ATo+qNnoqoL3NbYqllzoxpUFShLxDpFV++Jpvy5tZrNZ1H7VXeydi4XukDPpf73/f/WwfidcLRt4FeVZ4NFARg9CQyh2rpKRH+2AOs++BfeUB4x3fOEYzLna+hUVK3tWMFIwEjGPAmhn7Kvz1X/+EKmYgfV1cnMv8aMquZ45kfKtPvk8G/WAGh4BlI6T8Z3QIGAxC1xxjwlWjncAo9dsUXOwjSMimDdXyMJbD+Yk8+N6nMpzN5ej8IR2DDB24cOghn53htVXeEveCBSp4TebJAOEBLWgkL2tZ52jusZVff/lKZLkWIQcAHn3FSE/Z5QVFhqoSbX9x36w2qpVVQEkSZIlRl6ybAZwNDGHmnRJxn+JY5oOBXExndIgwIgM5rM88IThDBZhCssYwHG+iAWmvoZKql7WFyR4Je3GwCxSZcZGbat1+/k+dAXAICI9STx69J5DfHspsMqJIDkoUpxN1hJ0U1O4FlqNm62uOpaEZ98/y1lFpT/994mCL2Fk6Dbr0H4C97UQtOOMg7sVgT9dDqXYpSxCB0iXsJuiVMRrFbLZbq0bQkirkarlmPX3UE2ThpwEyYAJOgGKBJgBRwevhvsLJ1cMNojYN+6AgXTOeLg1ix16DgxzljCDDYvGqKE/nELjkc0+Lo+Vi6IHCw2IAgjRez9QKP6BtdgPWFNaA0a2ktZUjtMHgjLCDnJ/D+DDOaXHvQKNNRNpKflWp/q7ZOyWHgdDBQafAv8gcmtHQgzmOJL9I5MWylp9cFjLKaokHA5lJIlMQ73hMUGRcOQd1IcOmkRmU+JJGPmZs5nr7eo8YwMFng3hRr013g7Vn7ZuVA9FpI2CPWW0L+d/9p+8/pipXrM4AkDVveY7/2NoYAZWkMp8eMYU3Aqk5islpe/HiORHsjz56JCfHx/KHv//75AO9fPGSKMAvfvW5fP38OXkU0C9AxRsI7khlAZUAlwDKrNU2V6jEZOFBjI9HAxnNJlT4zAU8MY30VQZZZdu7Q5MlG235IAwoivKGO4XKjhuzz3NRUTNrNW5CbH9nZYe+GznAeP/AJamryMnkxGENL8vbCXt5WEuGsDIQ5AVdehX/j4hja4iBVxowgsCGz458lptCU5PtTso8p77BV18/Iwt0sfihCu7gRrF5wD264G8CCvo/9/zUBxgxFzuhXDAdJ6Ag2qI1HSqDdDyacXODfsOIB8uEUFQ2msvw5JFk45mMT84lG00kzkZUV1PmjkmutiQVj2q1lhhoAGRjl+stGcXbXSG3qy1FoZ49fy0byE7fXslmvWLFAlq2EqoilVmhVIW1dYAcdUHkn5Lhr5r8YMEndm/BVvZcsff0wxKBc4Be8xvjT7z3eBoRsCWZUujFGofwOjvxGvsDy9PpRuk5W24sLnPcQrm9MFP/WGF6QvZ7QbulIqBPnsh4mMkIZDfoD1j6q1+S6EiXvVXn1TPpu6c00kb9/us2OjTGsj2tLTvSUsfDVPUUwdLXcf2PyLrpteVlhGQVmi9K1NZrem8yQX8R1IR7qsBa4BpJq02atFUGKmGNcXKiFw9AQzq0eZfr1Mc8/Py+sMc9OAVs7WvVJa1aZZfe3OdgdpUeHp33F7iX9v52vtF7WIqWTobSWUVBp1/eeS/sFtjngjjnwdUDsd4sbY4OohxR0+rv3T1WIKAHwqbQRkNOy97mkQwRwaYgK6sywwbEwKSWXR3JCPuQqXvyb0wPwastvEwX6SONzinSq5/BKEaKbnXokVeGacGvPhVpzgN7Gxlh2crwrJmeDYBF5Xpu4LxQKfwlrxvt2XOWwSryjPnJEveiYGUbSOpvLt9QDAuHP1ApkFwhgIfzSBECm/8kz2pKbLfJ6SRgz8zGmSRIVWQmLkSAQB08csL0bqs2jM9xHx8jjHY31LAkK+dthcus5TErktjEqSth/FZT8rDhf7sb0Bcr4kfsHdx5vmWL41/84lfy1VdfcUDZ7rjNA6pQDjbHwsqucpS7tYdNRAQBuuj9AVSIKmE5F5XzAOlQsndnOudospLL06dP5LMf/IAL7+HDC6IIHVXsHcxYnIca64ZR8xqJTAe1jLNa5qNIzqcZGfhn8zkZ+JPzJ5KNp3Ly5COZnJzIeH4mo6NzqbKR5KMTaSCrmkCmlQoh6nHTEbDaaofwGDEZyaeq5dXdLVtIf/HVc/ny6xfsNPni5Wsy8TeLG2WB18iNQWtOBVl0v9RNCnAxnRnAwBFyhliE6Iqnylp8HkU3cCrDyWlkhPwzfk9EIJJBU8uszuV4GMnHR6mskT8+wPzgUmIWSEwQslKomURDyv52kZiOUd9R0uYoPFwylcZWmFVL55izcwVDSyO4NgGiXmZ92HAGmu7Q9B/IxclMzo8UHUDEwC5z7kHgtRBRwAHuOZ6UNe4p6RFF87yhaXGwzMj/hoCGE9H8gNTGWCi/PWhMvQQKyA6qQ0ZjqYqBlMZ+LotaKkS6zKM2sspT2VVAE+by5MkF2wZXzRWFhJa3qCwqJYbAit0nDr31euB7DIEioFJDN0cQuLA5A4nwHg+tk0N1zqGmo7JMRiOopM75HOcmkYCFg9MP3Xbtds6IqtRpcNLvQ8G5at0p94SkDrXpqUg1Re5A8XOQCDmYhg+XlroCORXPZ++ISCKQ04ygRr8ffQCGXRqGjisEosBq5/U2stlCtz+nmuCvXkIkSpkHePbxWPg4Kmo5G4jkTSRvNiV1Ck4hhlMPQXVWxWI08Ems06IltiEYpU6Iincp74H5Wz7MDVDHAAe0lXPyp6x68LbfiQwPbBg1yIaKDhKZ1IoenikcU+WF4FrgCEBgaLuuZJC+kevrW/4MFSvkTW238vXXX3Ou/eSvfyKXby7lZ7/4JUnqQLavr260xfZuR/+MrRswOZJKSqKTqKGMWGZb5pUMxgOih+B3zU7n/AoUGAjLrgDvaNfuJ/1utGodUR9jRd0OQ4Sca6T9YLRDK4IQIM8wih29Q0/pw6WLFaHpnAH73f0Po3kRFZ7Ah0XOBl6Wevu9yKaFbR0F8YiHXSo67YHea/eN4g9ew4x8JPuiA+rWfA9u+so6MSoZw+EWC9V7sOLfSjz8AHYEretpRklgHByIJGejTE5nmYwGQzk9PpFsOJLJ+WNJ0Ur44qGMj45lMD3ho0qHEg3nUiOPi5SDiw1hkiDH6pfOQwS5O2iLI++KvG4pry+v6RBcXl2z6gOTfXF3Q2JgsQEEW0kSsVdZFwlYdM35bxEKGzShtSp185WYBZQA+ULcczawAaGHB13W6tSTL4LDGockW0Fj4R4oX9pPr9+zNrPmZ0KnHN0jixn3wDaqrnqm2Q/WW3xAf4voVRdnN4E4JpmWxY0gmGPQ9/61OtRnOua9selPPY0mv9kfWteby93inuu90gDj7d07D7E+mZfNeeDIGFvcu0OixKosYxlPUxnP50zTbHelrJNEFjepdZ3zz66vqxEdeBeZzGdT/iwfDVvuwcbSW2wew5awXqal16K9TbQrKJyXlifQwqp+NLluyX0uxj63gI6Hd7v8xnw6XOypyYbSkIyZsn8GezsAKUDlDp0TpFj0vXSAFGrnAcrDX/tdMBCvzWW1e85eLFi1DJZENoXIzbqR5baWq1Uj652mDQnyc/3iOUoMZOqQpw+hH4mB7iFooeiRqyX2yi59GNqWw71OfC71bE6tEl9dw0HlexnJEmGQw8fUX4P5dtUMgbPdrk0iTVoCyMMzAuep07OpjJOCOVchPVqW8uo1tC+gQAinYaUCeWjURTK7SguT9G7ZPVeGdGlv7LMY4C1aKEP1kQ35FMnwLq89cLCrJG6Pwt78xHMth9ilujohr/uj966iZO8vXdyuECVxtOlWmolQ2InqPdsBG65Waw7qrfUagAelhO1WSf8bG56/l3aJ06+FwXc+2UgM80ii1PSB143jz/OiklsK7Izl669fcPNAR0QImew7Bd96AD7IJvsf/OEn8vh8Ts90hIN+NpdkciTZ0YUkw7GMjh6wJGswO5YYudPhWKJsoPW/OO2x+aGGH1dD8mrDshv24fZJhcVQ1rJabeSr5694DyDchPtw8+aK+bPteiW7zYrEw7raKBmGkYARH43dzoOchz+KBmIZZ9Yx0Eqi3CFw2fstyDiXaLmsvAN46JMH53QAIK+KJw5PT+XoR78rsrqTZ6++oujRIealqd4KVNELXHOynzaxaLpbxjYFYkS2SsaE6FCbXjAxFz+Y3StHyoXjvt2S2IYNAJDsEFUTg1SO5hP5+OkDeXxxpk4BDoHevWkFeBApOpzadkHsEAIVI9KfYQNSh8NSZ7ZGtPY+pkPH6/IPdSCapVEnNtaGWu13t5E0q5XU6y03OkjOQgCFuhcgA25BIBN58v3fld/9B/860bmb26VcX18RRUADNPS4gMPuvUzQuvzs5EROT0/kRz/6IccJKT8483/5F38hz54/5/qGM6/LVQmryPXjM0+mU65nlHiC56DBhzZNYoqjnQOdE9V3B33rpGJfpDniCE40qgMJ2XY73IegE1UnZ1Khth9BDJzmFhHQByJQHq7sBImx1VwWtAHgQHhqFNHv5m6ryCqbBtXsO4IUQF7UsikqeXFTyp/+bCvLHRyCiiqDI2v289WNqiCihHCBvgmYX8NYpujuWazlFJyvVSWzHZwuMNltHFxTv1aBLa3m6Hqi0KHIrJU3iL3WstK4dBxD7ZYonVLhgWJPebmTBrWXJCiuW4Jte2CatkCBklnoY7A8u2aIPz2ak0P1059/wbX85tUrliG+efWaWjbQxwFywM6ctt+1KUWqIQO5QXvJVBpwtpJMSrhcOaoucsnrGzqru02p2ganExlNR4oqGpFaQUtNwaIigXLWdA51zpY2HzjKrB6w3gju79I78cYGlsrr9XL42+wgHFGJT17+9M0+9t2/raSCWs26SPFg3sURAFNe2yf9f/PA9ef5e/JpvQoEdkozONdnHJnHhGFKbi5ACHAYKslpn2G8X07w3SEDbo/PxvL4fCajwUCGJ2eSHZ1KND2V+PixxIOJpEcPVAlvMlMyHOAiEDF3pTRwpqy8TgEO835Nt5wbDfJT6KiWV7Jab+Ty6pbyr8+evZTlYiF31zeyhbfL6gAskqqFBNl3xjqD0RGw3LtK7+rhP2gZ/Bq9ARXQn1iLVaRuDI3BQkLnLV+UCjtqG2IZjqXebWQHsg486gOtLQ6zChU/VFvnz5xFe3LXYtijGf8bhzN9LqKsp89ON/YyvuM4m6BJX1YaTgFTBeNhq4/QyvL2EIKuDNGjKa7wFm7tLtBoYi1S0/1Sc92oUgBSYGz1vbV4yJjq+tPyVOjuluSSsFyYh0G/fa/C9Pi848mEqRfEo4D94WwpdLt/bXQWJyMqOT54cMHNE4cynAJUJKmGu6YYPIInyc5Y9jj0SSaEkxxDHMYOl77jtMcD6MatyyD4waGVE4jSVWf+m/vbweM5GEqNkmBKBmsky+3HuRQYX8wllPHidyCGEh3BoY+ORHrAwW9YbWopUQaLrjpRLdUgljoFF6CW1a6W21Utr5eVrLa1LKgFIFKhTbKRW7k/woEHCoByQSgnQugGJcp1IWmJUmUlEbZ5LVMmJJqhMIU6yXuljkYmNBIuU0Qs29X7RqTHqyT87w6wuofSaeTsqJvvObwwNoHAQYleBugAG0UqTATNjMVyJevlSp6h/fF6LXc34LoBNVCZbPadsfdzsK8t9cP6s9QOP6chJuwBsc2lTCoZDsDDaiQlpyCVihoiHUXEEUpbcG1aUfcsl/Y2iXnnTLVzu+O9tO3R3+EYOwghUF9AcwbOauxbp6qGD6CesDoCOBxsI2lH4puLrSWfmGfeilb0SqqckMWyw1g3HDoLzHdqDTrYxiAaoSYcOcVnz17QG//RD38oZ6dnkg20rvrvws6fjCQZjSSHlObskcjxJzI9OpOjBx/RAWAO22RytdeB5TGzRBocTiSuGUSljQhlWTaS17Xc3d3IarmQq5uFPH99LW+u7+SvfglJ553slkuK6iTlTkbYYFinhPFGSRdbH1EohIIh1gJG9tIGKpSSGmEFpWLUPuQk9Q59uN9biYqNNCDxoDwMjMn6sZZYYgNrann95kp++tNfyUxyeRinLEM8xBAlA3bmIWHNQBw+hIEh3ONsaW4UjGSS71TfwhehHroayXcw8X6vBCIQRUzhndVqR8Y75jh4ApNxJidHM3n6+IGcHc9ZCorxRXkmiUOW/4OxpXELWVsxOu6NVUFQEbEnyarma8Kuj7Ah9BE6OHlP6+QA8zGEM4ComQo1OaoBAMXC0cF8tSgMXQnrUl68eCn1n/25wdoiyzt0MUUTm4LVB3DQlUuQshT54YMHcnFxIQ8vLvieXyyWjM7wPCrIGYeiFbSJtcQQjgDKb8EdQB65k61VHpIHJW0J5/3NytME1KpAVGVOD+rqt5FsN0j1H+6o9i0fz6QZDFoVR592XtgCchr3MqhuOkIAUR8KfJWyvl7K9YtXJK7dXK5JYluCGFfXMh0kMs5i2aJqqKzldlPJ7RLtlhuKFzFwSFxGXEcH0ecyV0GhMYnBjSzWhdxlIvMj8GnSVhlVk/96fACh4Nhx+nb9ASI0XBrouoLSqYqqKLLVOo4U4rLDmumuw/ZhbZIXfcMh8A6ybRkiD+FadgUE8UoZj45kMJyiaQZLH3dwdEmDALEbirCJ1ECfa3AGnNCnCOS+fPCQZw02ZdwHGh1LdHzWYIooD9Cvciuj5UiSQSLJUMvkByNFspznRFI0UxumbGqp99ZpuBfP8H2oPKuVXUDr3yWsfSeHwEG1vfy9sTd/G4amLGTN7SvBxxrE3GM/3r/w1uHx5jTtdfyGlAJzmtBPV0a5s0qVsIhNRyVUb2/uZJBm3GiwyaAjIG/AO0ArH8omR1hkmZRVJtVgJtXoXKLpuUyADOBz2GLRTmeKBDE9AGfAoixy93AMoE7Z+tbDm0d74vr2RtavL+XVl8/l1fWdfPXrX3Njm4BgiMnD3KtJ5HolglUA6BqOBDRE7QtlioN2E+JeTh1cA5UvdZK2adKD2IQe9PCud1u+n5bbgYinssqo40Werh7E8mTm/czf37S0UCNlLE7WorP3us1BHsLdPEqwkxlXwJnprfPpTm2vx4CjDw4mMfqENn8B0qwekCorik5wyh84PppRqhekTiXSGRnIKjXc2eN6MnKhOgQK/7VypCzhUnEgPsV7GDgaYJGzCyh9yFpZZ+xjjSEqxUFFhMA2KIwhNi7FPdE7U9ikrJCv1TlPB1qtYrlZHLh4ZCxRBCFswLLMOUszJ5r3JREZJV0anXWwfYfSIF2I+wzkAeRClYXutanmZO2a++h97ZwCz736HsP9xD4newWA8wcRHY517wUORBBLiNojteH5Y0ec+I9ul01MsVIXGFA3cHJKWd3V8npVy3YFJUc4aYXcrHeyK2uZQc45VVnjdVXTMdjugCJo0MBdAocLyxCtcyZgdqjdgf9DcSEoC0JhElUJWs7Nrpr3HQIrkeOOYERjHIjU/4Czh6oiQOl0CEwsiwqbmPtAaTU1yZ4hB3MI3FF3Make8uf8p/YMAdqFALKWIdZgYhw19isB0qHIBkq2U2BybLBl6QLuyxqVayUVAiktacYD74azxh1Nfr62kk6Js/gBqt4gXjRqhmhUQ04JxitRZjI5FnBwWC3EsTOHybs2tmQj+4ze+8Sqt4jCvMOQvh+p0PbJPrmsZWexNEjzHiAOYoNE3gXfIzJFVcHz5y+Yx1aP35jV995DP2dHTNTDEJPNcuJekiP3FhM2K4ridR6Vl39hE8X7PXv+Qparpfzyl58zevzoo8fy+MlDI+J8iMZH394+v7qVKeuyR1LlX0v9Mpej42O5e/nSmPFW122Ql9fV9iNUPQgUMsTYX6OiIs9lcXOrPSM2W4nXGxkiHzhopEygYqgtNFUO2priWE8Dd+aBEHiFQku06qV48CuKyFDfwFwB7x9gcwR6EGhFi1wcCJ2A3dAeFFE6ZNMHeNvFtTSXqSTHMxmfPezl+N/P6GjyMG2UGCSapybURgfKoA/nuPDz4BC/R241Zn87xu02Ylr3Pv8puSqyWK7l+m5limGxzKcTeXB2ImdHcxmjvwG6fKIbYK9TJTUN6Ob5uGqai3eDfABzFPp8Ajq7jtgYgmHX7kpnSvL7QMlubjRe263Oi3WD5gMaFosdehoUUhdoCFPJYovOl7Xky7Us8jc2h7QbHNUsS61cYaqFHJdSrq6v5Ve/+lzevHlDjgEO4C+/AqdkxaZG2/XGyGAd0Q/OHubXycmxnJyd8jFAYy1zklX50drJuvPmwuteRmefsXVmbW8DRwbpLEj+AkBkN7kPmEVkR0ikJloRml57Yt5Wdfo4G9gAQA8Y9ARZLzZy/WYpX7y4lhI6wlXJFEkF4mEcUymSlTLSULMElQhlBtlxketc0zt4dbjeIBCuIRZEZU2FuqE0CDXBn17m8npda7Oy6VCGw5QN2VStU3kYUFpUYS7T46AYkpIFkdlIaogcOWlPgwoXTNK9TZn/+reHTVjW/KdI8Vg62iXGbc4Q9eG91wonlQfHeizZ+fHk7EhOz85Zbj2bj63seqGlhqs5z7IcpPTFnQYIhUb/2ilTFQXJQTF+kZ9jWj3s6T2tgkLXWCXQKXm1QhUU02yxyHionAvcQ46rOt50VHy+tPoDtifZ5GQKih0lvfz0X0LKoO1o0LKw1Sihap68MzLBXr+6uqJzgKgcCx4EDfaa3vO63+5zO5zkv+Tn67VM7mjfWkpH6gCYsf63JmXLvGJdy7MXL+TqeiC//NXnWhY2m8jjp4+t1vnt6Yvvyr64upYH47Egw7p89UxWuzcUx7g9/4rpguF4ZrXWNrlMdVBZsyY8Y/rWRa1iLpCGRu4VEw6tOLUuO5NRmdMhAPII+A+vs24Kye0QwvaBA3pCRwBQrG6oBVAd87Y1Au2RFf0fNmYx8o7WvAibLcrdotm8TQsVVSG3t9d0EB+MBzJCb4m7WJq0kjh7KKPBJy0U9r7mal7MpbJUr8slM7q22nbvaNhGhEA5gJb00l1dROq10/pR6OAbQY2bI7qkrTZyc6ea5xiDKXp4nJ/I6dFMJlDXTBKTTtVX9Ly25SQU3nRpWpRHskzSyW8WlZkTopU9NshtuWEXNVtwYld+uEETg1AknQL1p6Coi/ct0BVvV0mOXgVr5YtgTuFgKPKVlM2GiAC6bvJzsvW5tZo2Eig+A+q90acATuRXX33J5/BncG7vtBFa+4FsU4TTPD+ayREcglMQEk9V+MifZ9AwG8swPaRjylbCppPRzpteubQ6BAgqEslQaof74s6XfBjLUZKHVKrl4J2l7ggBRlsvT5s0eQroNl/K1fVaLi8X8sWLG5Kpj8da9luZ6A+loysTMLPGQc0gZgOkW3ZGVIdgEKkzsOQa79Q3c9bSV/LTq1yGaS3zB5GM64GcpCPJIMVLrpKpxKZFqxZLtT6TMOYF44AzZjzJfCCeav2cpkKTrhQQlA9C5AcYulzihYBgsaG6dxK0PL6mfXAvNbXZVHAG1CGIk4YdcZ88/R7ltSGohiDm8jV0WDYk04LPcnd1JXWRSw31zWpn0bt3JcRVKI/BgwkPjsDFUocepOJaih3xf5V5LNCfBw6BqexivUEG3MqduV5AIemf7vQXuwZ/rVH90Z73jkFB+u75GX0jRPtsykHVui63hoGAI5AX2kcABDZ3CJg33GxZXYAqA/ZFh8ePydI5yb2ov3WVu5yu/9sV3npwn76/HlwJIz7tekftfCrSqcet7OtGnr14LnEWy/x4JrP5lC2Sj45mFgH9Ldso3/zwrfZIxpKxrVnFutlxql0KEW2iUUW6UV1u3wDZ1MbL4Awp0F/pgaZkLp30gE8hk8s+8eOxxMulJHd3fJ4gQrPQEupllCSGoh0auyD3174lDnGF2z3v16/jdqTIN1IXBXEomR3wmAOLZQ6FPhF5cn4is8lEnhzP5HQ8lvP5VC5wD46OZN1AQEUOdghwADjoquxmTSEQSm5FZuxw72nAt4JhZm3b1LZFXLSnieGvg/dcrNZyu1gxGkWDHJSQHs0m1DVXwRQ/+BVVaMsbLbJQBTrdlPulgn7Q+7X3iYQkoJmDrlU1HXrWFkXSh/wAHJmWeNE5gD4HoUK5y0vZbDUdEA2GrDkfRKlkMcoMSynRppuNr7Bx6sERA/7GBgmHHaWC5iCAX4C1DGcA+4ZHePfpRizrHEDKHOqFLvikDoByHZSrsUW9OFx+KikCAvceFQarEuIGTwdOA7oQJmzggxbii0Us63VMdOND8goRK5OvQDDLyyF7vRLafjCOXOgcZaOo9VbWm0I2uUaXOFhR3UOY2vgA1GZBhRXXckeoJfAM9IxOiaUQSJrshtYjdlQpNHUpr+52MnuzkWI4ltlIHXzC++ABoGaeB5YiBYhyveueoo4qooQ9Phloy3XtJKrpTpbhmm+LKPqgMYUz3jLsTRKfiLWeW+QWtUgRzgLVScH+t1ze0jnJtytG5CcnM5lOhnJ2ckzO2Q20ChYreRaLLNB/B3uvS8zjqDZPvE2RW3k81z3uga1bEFSJTphcP8iNFatGKskTVAdpf5DEOBjaj0W5G0piVnlioD6cIzw8NfXjzkdL8u8LsH1wDkGv5hEqgxAYQiSKQ99Z01iEr169IhLw7NkzevhX5hDgoNIca8m/d41yVVjyN7kf0phuc0ui0MH0RbInO8oJgVKdSGKDhogMJFBIM7IYFlRVyLaI5C9/8tfyxddf6uZRlvLR0yfyez/+HdNa/9s20HvM7/e0JzKXIo9kV5UyOprIZP5QlsuFvLq6cvHq/Z7oJuaC6BHpDi3WMTZ92qu0EJHZfEaiFZTiIG88ePNGfvniOTeUZodOhMhz6ya6y7eyWK/ZLKZCcySUzFkFgebcrRmHhcgOt/rhBRi+v1eTOGokMGwquHcXJ0cyylL5g0+fyunxkfzgyUN5iF7jo7GMxlNuYtelyFJv3XsbHBFAfDh8AB+ng0HbOKff+MPZdt7LQEl+XUKXi9kIfV6yyIODuTwnLNmcKit5fXkjz19d8hADSRXlho8vTokQAB1ABQacK4UQ9VqU+Ibyp4697S1PPT2oSIw6VbrgjTVPidL9SdivmmlRNSU6HDSmvBpXU7SWziw1xSHUCGHszSaXxQJlv6UcnY6oBIpug9DRwGFRgVxKOW5VLMTODTnsKgLT2pixAueicwggHsOSVdwbG48WYSFfBD3tJ5znTK8l2gMFKQigUNiHNBBBn4OIzwUkW+falRNrAY6wQj5MSgruBsZ6OF5KFA/k9XYk18WAHVTdI/gQumSY75QeYDW1O9UuU+ypg47Eqz+PWMJ5e7OU27uN3GxwAMPpVu2PAXuHNCTFoclQDRGwFnEzEbGoJmEQzgA6ciJVoA2fOnTZ4ediV8q2EfnFi6VcbyvZHp/L+eyhrgsruEerdMwLBGEUO0KLc+sHQilufo/f4fMS9yDHiU6DyTDrwYlUL2D09zes+2yo0t+YMyxBt4OXFS9jqNNq/T8GlIJaFRpwreXVKygK5nLGrrhDefr0glwzdERE34Ovvnomr1+9kb/MInn+1ReS4yDHPmpEXgqaASVhRK/BE9NMVprogQCIsCTZ4+D3scG/0dOiRDChcxj7++x4xkCRBM0MqULN7pLvNACCZdleCw6wp1E8jSXMVqPUa+72wR0CZfnWdAIUAYBq01UrmYjNACkBIAFoFrFYLlhXzoVtDGWvPeZr3iuJehsPyj1nel/AlSxnspdH92t0P9jIVd41kYKeNXJf3uUOhFItQcT1QjURAjKffPwRa0TTKfTmDY/tXZtfkWPH/ZKv97GbRS5b8C2qSObjWqasm4eyXKYyz5WmVVzGmekQi4S6yNY9RY18AXVic4M883g45GtpG2CNUkneMUiZJWTIH65Xsr69lQolYRVKvFQsSFmx/YCw301rv6uWpi+AJiBqyFnF4D9H6uBkds4GPw8fXDCvPp8fyQCtrVGSZiEEU04HOlrt9fk1WmR//161oh/9KdRHpdq/Nx6FL7oWXfD8nRGyWNYKTQKdY5AshuIevrbqgwSWDNmx3CGJVv5vxG/G1PT64U56t8etsXDO84StuFHb7GTfaT60rawNjiFRRrgjl8flq7soVh1WdHgcssKHaqC4vWCjEeUYSF0nrPmO4koKEPcIt2opVX/9dtkSVbm8fz24FuR1kZoEz8CbouHv4QTg51j/4LDgNeCkYB0wqi3RzMuaLjkRjo4vujCiXPSOCMFlfSyLZiIb5NJ76bEDl75WvXCPciqITxJDhhwF6hO5zSnw+a2cEYX4lYOK9IY+h2Q3Ruj2KsqtbKNzOhx7Ajj+1h0vDKx37gGpdqUsoCfT6xHAe8ADqSZ3QccPCoiK+nrD6Zh8kR6q6g4Bn9eVBBZASw8wvfdeSeKYSP81+993VUN6VjSy3a3l5vaKlSrltJBhNpQ5+qyQIHsrl5ev+RWdN8G/8hJHr2JodRisEVG/9JJCWixh9fdUjQAn21PPB/9GFUJetJ1C2ZgKAQSJ5BpEeAMjToMWUWQnCd2PsJdQnM/apn8nHIIGEB4U7gr5xS9/IX/x538lb16/kS+++JID7S2JoQvN6J+ysdpngDWc5hAw2rSIxdMNnUKh5Wp7i7/FD9oNYh9KbW91b9NnPqslH2mzE5bxYZM2LxFRCNIWf/bnfyGff/65/PG/8UdyfnYmJ5AG/t7H9NQAHe1bD2v+AEzjf/LXL2XRjGTXJPKvDz+S332o7Z4RqQPmfHN9zaYxSoxT+U2mmqFLbsdV28DDpC3HozGdm9OjE5kfHbX+CzYHtBvOk5SbMV4V6R2oFr559iUfo8lMtufnrB0fPHnMJlA6tpoOYE2/aePzvQE5VrUsUZJTFfyalwUXOx7oZzCIE5kfH8t/5w9/X86OZvI73/uEhDuSztD0pgS0q/oRLBc9sImMs31Vo0GZJLjaLgvUiZTw4EEFh4t4tChbl98n2cmQA8B0yspuupwv87Uo0dqxDwRmKcZ/OhnJCVCfCUSOzJO3yJ6pMp/vNl+9D0BbAev8DOcaaDtFg+lt/lvtt+ZJcUBX2oueeeeEB6wiYAfCLjpsSgxkxF0jHOPc1FJTPWgoFds0FAg6Pj21jpKJVMVOCikJH08GE77QYLCiA7UqlAXPKg3IG7gzYN0NY++L4eqIzLnrxglk79mz5+xVAgcfTZe0mZXQIUAO2IMCyjgDLQK5DfwlUzElLE6SHA40HFS6J0XxEF3GpD7+gTSTB5IPz6UePvhgaQNE+glJhUauczYu3xyOkyKACvRgbZhPEEOifchyTYw8iHhLVAdEIiN0ekTFBkTD0kymicgRm4sJeQX4ezRTQzxLFT2k1yy/jhfA73hc4Z9JLGePT2Q4HjCix893USqLWh0CV5IgbE7ui116p56hOjkgF7c9QQy1cjTBIXVDQ9doz36A6brqxKcoY2wNhGBKMOzOF606whjoOrm82sr11WuWss6nx2x+dHN1JdPJTP7Zn/638pO//qlcvXkjb16/UATAUiOa+tvngBB14WfTz4xY1g9xoARQgCXCq7xC84gUdQOKjfHHNWXDjH1rxs1YMiiejtXBxv4tLinu/WEopgbnQVESDSK/o5SBDrguLkBxgP2vLSUAw2JjPpF5O6+L1pK0fiTTMqz7kZynDfqyyGZ7zsE3FuM3axRa8ldb6ggYpmuI4hwBLAj8Hp8FOU50rgK3QXuo90iL7Xt37/MhiFowwONQFiwgX2q18CS52E3uRCd8s3APuy+napGktTv1H7XiLS1krBBW3D/oXKITTaZw3+AsbLdcED5e/XHtxmJfIhSHgpaTGtHQiI7KRtfrB1yH9AUiR0By2yq3OmTL/dmEPjSa1fJBzQ2apNJbn9cNXxdhv+2zdmOLxaoRCDEr0xJnu2OmRkrt+kf2uzbnATKCr/eusJfq6hpEKQLRH+O++9vl8H2deGmnf7/3ms5A9tf5AGIEGtd0yBucEyAcbflwL1/pVT1ehoUIEXMa5DwSTY1wioMoRc6ZB8M9foIdFvwtoyBf6+YsWL4WjgD+DafAUwN4AJVcraG+qRLm2tdBCaU1/ga6KEDIUEfPBlXqrMU1nCds5kAUUbJ8LpFMpYnm0uxJZBwezRIhoJ9vFQwtSMD2p/pZMI/hgCL3bGkg5ZN0vBc4BdQyMi/Vsodvv8Y2/WAanZircLhAZhuiC6z2WEHwBMh6NBshdKdcI7ogUsjcUgzKa+gu3DE1EE7dCffcue87+iRtJtZ2ESfy1kgB3YgDzNeSR+bOY2irpFjybCkYJyD7mhEtBYaqLfa/AWThJeJ5liUDdsy9ubkm4g2nuMUPe8JX3Vo0B8RPKKNeMcXCVIkhbZzH6Hto2gh2nwlY1bEqbUZIwSoXLqnsoGfvFAtS7gfGbXpdCbXvgmK/m0MQxS08B23nly9fkmihfchVmrh7c//qEKx3nyL300RFFN7DV6AIrFl3mGzv8PGNrf3JN/6tm4DWyytUah4gogBrM6u1WXEbaaPu2cvqQJJEOeKf/tN/Lp98/FQ++uiJHMtc0kwjlP1xkA9m1dkn8tnTj0iy++jxhcwmQ1muEbkDkioIEYEZjWZB/qmsh50uNL8eziddyTtwOqSRq/WS0O3x0ZGcn58ToUFXM5QdbhrTB9gupVovZRjXcjqfMpu3u7uRFGNVPQGmoLl37k9635AOAEfAW1V73S8u4wgpgGgiWyAFKHekPoEiS9e3iNawWFMZs/5aUZxuTLW2eb0xNvl7GvXwoWXA4Fqbvmj3tW5uanc25WewPTDdd0M+2o5p6qgoYci6OnJT7kraEKGuQJSFc3x1K9fXC86ZyXgk8+lYTmZTGQ9RQ9JDtKwMiciASf1iAWfZsG0dzKu0Tm2aBrD5YvhylzpQBrUKHEH+FusIcnO4Z7rRYcP5Jtz+btY6G6i0oqjQTuJiK7Fxgnzd4ndMdeWqMTAbQnnwSMo8k22E1rONoGoNL7TFxytRJ2/OKO6D5aPZSCvWVtl66OiYIKJXhVF1wmCQmdUmZtueGJr+nshIj1fhXQsR3SoHBhs5+gno3CN8jbWBHHQNwnMi2XYryXAn0UgVPA/V23cDERPNcEEydxgfRscHczHRUr2WM+GkSaiJjiACpeWpPBCwtUVQf4ikMHgazeEYXICTYgexwv6qZko3KKplfn4s0/NjGc+mcnJxSuekKCGek8rTz57K5GgqVb7l3EKkWrFcEoJJLmHcIRt+thNBN1cBvy66Apkujdf2OrSfxJFsWAj5/qYNfhxJ7VJuSAupBoo6U96oin1WiKzphUIDZTgaM2VwfvxAxsOxHB+fymg44RxcQ+dhh1QojnCUeGr12ng6oxMBVBd7AvdJapF0suet0J6lbJGu4ji2KRrb3aFfUkI/RTU06jzjOOEsG7CVeE6EoEJzCTS2KiG8l0iWaNt3goqePsG1fFfNjVhHarKlYO9SVhTtc9tGJQbTtGzf+y9gfbEN2vbUgSMGv/VGf8MpaH/TZoq+QUF2roHBqQqfdEkzJZNpZ0TcPEj7vnr9moppFEOpKpLDvCObkza7tPM3OQzvasn4SE7OLuT8eEYSGqBmbA6IXKB0RXGdnnIuPVofTIfBek4YERpsytCTh+QpJjnSB82pab9rm1E9ECGXij72BQlvOLgY5WJzLoYtsc4hdEd6sHHDYwWcBQU5PAclOozAoAyJ8jqJqZaom7kqSa63Wx7QWbaQdYZD3whFpiqJTRmH74b1uYciBErJpcyyoyxtRNMfP/foVUTNqyT6Wg8q9mTxsVUJOPrFMiuo6O1yQsBo551mE8nIA9EHyEEW3Ld5yz6XgOsguS+x3HOKbYftUAqNZLvkrwGVngZhak0fbQb1Q+Dctp7YghjdCoG8ObRvT2G/g96D0s3o7AaVepD+pJYMTDqgKCjzp8gLcukgA5scq5fYmsiUwudwJTxVomkq/V7RClwDSsI67kGXdtibGzau2nMDh26m+jqcHJoywINpHfSgx5oBW5x8j1YWphuQA4wKfdYfQp17R0Ewh9Up4Pce0TOy1Twz88ktDN9dCkoH8R9V8pAmiBqImrZlwOhOicPIK4hgg9FApqczmR8fyfkTKERGAlV0lCafP30k06OplPmGgQCDOLxrr5ytB0bppZgDxss23kJbc2AHX5cdMdE5rNMmJp/kEGtbAfdBtBYd4Agp0kLBRM3neyfMyPLtCBTR0XMyHst4OOH3gxTQEDhyWj3lVCNXPwQ6niYZHVZ0kt3Ts+k20b042asSfO36XkT5C2ZRECgrQbzMEyl4EDRKLkxE6hR7J6rCtLKGSt1I3dg+0u4H7rh9aIeARCkM1nBI+Bf5OmwMo9Gwze1hCZHFbspuXirkrRr34f9O7x0bt0e87QbT8/Z/88Hb0o5s8+uU9FrSGG48IF0rQWILSbZKHrIRhTL0M3p+L16+Yg4f7S3hbWVnJ/QaW03sPfLN4fbf+7f+Nfn06ROZon52MKCyF9qPNldXXJDH4yE93O3G0jA2FvBGtde3wk8wbizgS4BaAr5GiSgN0Vsiw9FU4nUu6zqVdYWyK1UUQ0lciiWOsK2ays3dQpYv39ChYN03dfH1EHXHydMOIMvNhiDcaOkRxnh9c83oEJUSQJJG46FM5zOmlv70n/85xx0LDGPr+hVUE0SaxljPcEoOsQJIU4JFpRUZToxsZ5BLbju8YnwIJ+eQoQyv2sqHlDCrf+qlrCp2lXADQKnh7VLba+PfIKUezacym074GEJroX1vSze0l9KvsFGHAzl01qH3HD2mF6HsB9gQ0QDFrAiBdQ5NmzNVx4YyWxYpOwfi/a3bzLry0y737ZsQSap5QR4JxgIDh2tNmqHEo6EI4Ph8RTQwroEY1Jx/eOA/ZLexQcYsRzZMzBxYrfVWFr3rL5C5bV39OiRAN39cl4s9uoCa8m3gwKatQwAkhqqf+J6lgKoYifkArIKy4b35+uEMhWZa9tdWTziHhee5EizdSWCqitF4QyngdIBr09QYHvok9R548EOJTxqigao4quOBtvCYF3S+0I58PpGLjx5aRQ7GA2RY5SjgvbBvaqdAhdSr3lzVQ7+X6+DvetuyoWGpo2JGnrQiWRsGddbRJsF1wN7X4Jj7XOwcAU/Qd5Lfuhw6tVxeUxPJbDiRJw8ey2Q8lSfnT2SQgYSK800FzahdgfkCsiw4GpMpz8Sj03PuYzukoQudy6oFYmulO5C4HqgGydeKW4Ku9yfgNKsrCxDgmBpCSyKoCkiw6mDMpIxUOG9ZywkAvNkT4YNoFB7fjUNgGsv4QO4UIIUAIRHdCDSXV/aER/TQt5yXbX99NKDjEihMu5dP3XMIfvOGtsf+1z/oJ4jbyA4qcbimIk3YZAc3UCFizddjs72+uZXTm1tZLJd0DKCA5tFkV87Vi/YODLz+0Y8/lYfnDwj3LBsIBUVys1wSEsKEQAc4bSgjUucl4UF8JKQC2OVN69KsGQrKV4zsVid0BthBDbDncCxxupa8TmRnrVLxRNy7jGVIA5YiFRjnl695gDBlYSVxTuDCJEPUi0MOh/jReMT33MIBqSq5Xi5J5GKb6dVKqqO5DCcjOhcgfym3pNPmp1BQmkmaDrgxouOak9Te1yhCg4iD7H73lq0ta7sJ9VjcliroW1fV0cF4MFXhRAmTOhuY90hx4IEqA0QQJBSOR1wf6G6I++QCGy2/havXnGWfuRbdeBqFlcWsClG5V7wX0B9srnq9uoO2ZWnScwrateBtmQ9zCDwN16r/oWTOIz13CnDAVVr2hHHwUkEiJE0q8XAgNVrsboFMFYzE4RDggAdSABVEfOVBY3K4HCs/SZg+17wz861EGfEZMT66ISpnRhVHWbPRGxcdGx1TqoDSCVDSI4lYeLA0zrYQBsJKclUdh/4cOTxxoPhYL9i28mpGiUw6q/x1V3aoanRMeSBdQPKs6+N3R6wHno2pEDI9C9Eeu36MjCta4qTLJkM5ujhpS2Ah3DU8mvJzMy3GEm4ndHd8lbbaAn5fOyw9qVyWxer192+jX6n+vJP/VbLtYaOKPcpLzvUS2lC+PWc49nttrXX8IolknI3k4ekFSYSPHzySLB2wP8l2g8ZvGryqZgUaEw1lNJnzLJzMj7gnxDcLYJztvdU0rvXisRRCkvTWDeSgvVqH0TzujjoEEJkiGIj9BqXwILljDuSQNE64d1ON1RwCoAW4xQlIlabCCI4YzoDviEMgFLrBB/nkk09kjTaod3fy+skblkesVgtOHvwM/6ZankHvLDU0lnBbLrUH6ylsBA2BTh633/e5f/DfJ/u58+A1u/0qhQ4y5UEDUShTSGTkQl4DYF0l06CV63KFUsQrbhoPLs4ph3mf2PihEIK62Mjt4pLCIZt4IttoyLrss7OHfH+o3WHgh3OVGOYBh3IgwNHcaLXmmnl6y60S8hS0bUXjjpzeK1pxRoORTI7OpIGGerVlJIoHc7KUPIWwiUHsgGBJtqwlgZhLksgIEUOq9c4gLdX5Tq7WWtYFmWSM6+3dnbWs3UlRl7LJdY7gdbS2HBNX88IadSl7nHK/QGRwHw4cW5CBgIr4gmzj7LZE0lruOrxoZX/e6MWfS0jReAZk8dsG5hsn9QOqRm4Wa7m5Q727yqXCIRhPRjIeQeoVkQQcT9+UZG9OtyQoT23pb22Ka3qCKpG2cWGukmjHUEo3mw7t0LwlBWl6XAQnax5uXb8EHTt1ptyxc9Ekj7k68mO990AUhLUIBBR8j/EAzaeGkhWlJHEpJWrnnZDszo31i0iSTAa9zDMdVeOzIDok6dn0GRxtxBrXqEwjJ27GLmlrKolIo5EE2ettj3lBvVNWrHRImd2hDzCauBY9dO8Pc5dwb5OTXUUsxoEBja6fbp4aXEyCpB5yKhWg8wky4/r6OjZwxDEvIVo2QnWG/w3HwVqdG1KlnL/onuPiL+dj7VPX/u0pVf69CZa1A6iukDqT4EOxPVYrrPS+ppoCrkbYlUcz1QFn1arGtNGZr0HV+EiiWBarlXzx1ddMETx/9oppADQ9wn1Kx0P56PufSL4rZbfNGTA+OH+gJbaDMV9nud6y4git5HebtXajrEo6n5DYRin4x598QnQdar3Yo9GADhwkyu4TtUFKTp3BorDeB8j7JLivIvEgJR8CY6YpA0WO2EUUxFA0VkoV4XxXe+cqg/FkLMPRUH70ox/JyckZI8Gr6yuKgKj+wIaCRPiwUChElLhaQSBEm8rAY+kTgvoZcc1PdbLE/YffvI58cZ9k2IetvBNjm2XfyzUWBn/DswNSoHXTA/5+DYLYYikvXrzgjfk+SuTmsw9WanTf6t1CLvOlrFFylZ1JkR4T6n74+GMtkZyMlMPAQcKBAJEdOAMxtfGVwKUODg5/fE6U+JDIUyrhq5JUCrTnGE1ldvZI4uFCqs2NRHUpKyABEBBCSSa68FV2kFSNrJdLyfJcZsfHvDcj5NIgecw8bynb3ZZaFECJcO+RP3eSDtP2mMBb6M+rB0unENsgUiNEZbSsjDByo8qKx6Osgz/f04bDCZ0Oj7NZG20qhVoV4PBc10WzzW32WcMGj8NKtviFp67wsbLnEy7+N9dLubxZUqkPrw0nAKkCEAvhTLIXfFu33SfLdiQoHpImr6vQtLel1RyzVh/GRBu6g6yrMvA2LupgG/rhKZkPUMrZ0i3sgHGnXZ0NfX0XceqcAodMraMjDy2kL5RISZXBNJYpytpQZpuXkqa56Yb451CHgDoFOKwo0oIGUdrQCA4QUjVM8cCx7CnUcdXwgI9V456OgW552s5Wr4NoEGW3UxIbVDcCZX64CkUTQR5DFcK7lHD9rUPKKgbgv+4UanVIp6GhA++te/UpumaQTsRDS8wqaYh+uuAVu06oAoCz3nkw6XwnLwrlwDhABpmMZzOZHs81n87OfIrW0iWAQ8CzW9Fb7xTYNV/qEAHdps0B8Pa7tjejFLDPodE29uYQeCViDYfgMCSLxNRaEeq20sU4YIpgWV8TSwV6oEcyXixye7uQ26uFzquVcp4++uT7MpsfSToZyg9+/CNN0ZSQJp/K0ycf8fUh24397/puyRJaFzori1yK3U7R1JMTErz/4T/6R/LgwQP54te/ltevX8nrV6/YZhkVIqqcqOX4XDOQYUZlAYI1kz9KIHFcoE8E0sHWkh3tD8YVUQNv0ubp2X7l0gfWIdAvuIkQ+Dg+PiZhCA+wyEHGw0IDVArHwB0CwO+LO+1dQI2CquyEiky8RglFqoet0bwe4N/QJGg37w66b9MO9y7XCUROvvHIxks9SIyi1n8hSQIinjXAyAu5vL7m5gHhovl2RwSBTSfujcWhoUJSb2XIzn+VLG9fyfX2kgS/IRjWg4HUx3OtwrCURZGjvA8cAiAbmsfG82B+oNRYCJhcVpdeQvgFsOpmLVm5lrTcUJ6TbWhBkmm9fxO6sLJM1CRjc6S8KKDD3UbKLYoT0CNhR4cADqGrT7Zd9nz029SZ0uioDeCwXdvGF5v1kAJK89lUnjx6yNf51c9/+t5jqg09ulZEzBVamSCBeGOtM2gxoqFdcWs+7/QgR8MRdKXUhd66nSbstELKYJtrZM5W1V25oXOp2+iaBKL+u3V8GVVB3L8eL09qnWDrWOSscd2bbVM2x0cl6TthIodLD7F7aWHd0Ft9/Y586Q9XcmTTKDD9uc6xtjGGgFxVYhubK+ax5luxmSdaOmdIB5r/0Im0SoBkMJQ4AwKED5nQAWU767oTf+omnn5PgR6UPII3BGeU7HLlvLTX20M9vIrHo1c4CehApwjBfpndYWaQecvPsGZs/YZxunL06fa2qAxwCJnETBwILBcEyuTzzO4BKxaUtDocDTSFhPwj9p4skXSoqF8JtoFJ6FJkyOAIKAu2KTebvy5E5WOwP79NPrs9LJDiQVmjrn1NB2uznlbjhfl53W/Qj+UQowgWv1P+BTkCRCN6zMe2Pt9RQ3PARYMDlmKXleyQnkuFB//Z2SkF1MA5wTRGk0TfJz0YY+fOwUCms6nUJSqxdhKNx3J0dCSz2Ux+9MPPGFwCKcBeiHmLCj0N5PZ0oy2V6ETTTmStrZ7gmrd9gmkakONrKUT1CxTlir7b5kYqx6gb1jG8neNTq9/UOnTkiQkfLxY8/NG9jM2Nrm9I0sPh8fr1GzoDr9+85vP9uUWO3gjoI1+00F+LDuyxWHtEq1Za/r5D0OnM9/XfXZQistfGdQDm1Tp4RLWaH1quVvLzn/+SbZJ/7/d+n3KnR/O5pCng+/169UNtkF/JDOSUupbPP/9afvr5CxlFsUwlltFkLOePHzO3CR0YNBl68fVLub255WaI3eD49Ew+/vT7zNNmgI9I4lJuRGIP8ALQgKbabGWyvpFytZTFm5dyu1xQLhMtXq2VipU7KVdkOoJewECGVv5yg/sJsuB6LYvFHcfMS8D6HI9OOUtzihhzTFKQb3AftKOZkhJx3Y8fXsj3njyWxw8fyB/+/o95yP4X/8V/8d5jWuUly9ccRvfARbMDFgGyq5tpZVhE+w2HkgphuhmjlBDGvJwJBpHZvivk9fWdXCJ3COg5RbtjEG7HMhikWtHhlQNU+FOHVysfbGp7R068R39fsEQwyswomoT3BrOfzpr3QveKA633VhUz3E+H6PegtAPMCu4s2mQUTodAv6ewCx9onKNd2bAnYFMstivcFGly9JKvVOwKPBa2nq1kyFRRLINU5bJx/0eUb4X6o/a3j7MRoftsPJFsNGFaIIewjvE2pEHPD96hDnVpUQzAqRkPC2zu+Dcdglr/Fu2EqwYOLQ5Fa10JiBi7I1BLCK5BYwUImhb6yYexjs+iDsh9MrpzXPRpnnYyv5rNcNJMV248BU+lke0CTaS01BcpAk7fFKjZQM5Oj9gJ7+7NHcuGB5OBjGZjkWEjW9moc5cYX4gkQiX68R0ipKoSKZtCyia3eWUIlHUrVHnvmOqqZa1NvOgQo6Q2gSPmeK2pebGPAvbfgv0uxtlQduVhFUbT6aztY8G9nb0ysOd3PAb96q3eDUGwnH9R17JBlRbULXc7GUWRPPnosfzgBz+UJ0+fyvnFA8l3lew2pbx+fSl//md/IZvtpk2Tg/P1OH1MiXZ0y0SQ8/DiQh48uJB/99/7t5lmwN8hMEZq+tWr1yzR9tbeft91f/BKOKuIMWYhwEGOOFM7xsVCOem2ECi7oKqMpGP4f5mjT99JcyOLVpiXNflcbECIcC1HlKb6wXCQaNle2vZJ9Z/xIC5z1v7zYN5uZYeDyz44y2ZYb201pAbztOIx99AAdwpgbY6rjbT6EG03MNyITRIYDPnCFNWSFF2+0JURoiYb6pcjjQCtdKOnf5gEohmIfNgE0QkLtf8N2sfawdzksVTYRKmihXGopco3UqMmmHm3WPLxWvLtWhpAoYCx4bGCwQ2vnOpVxmSGRjyUGVcL2ayW2saTDV+MYW/1wm1JsTHtNVLWiQpEgaWm+FtIa5ogj94DX3Bd9K/ojB5Q3qlPJ7eWe85nMx6eD8/P5dHDC7k4P6XjhblxiKmc6H5U0HGJ/f89YNmPnluZ7J51xFedM10gqukHzGPA1jCF6lRvgyx3lo4i92fUMYdwtUFsz8/tiU/1DnGPIY03tleW6I6vB2s9cKG1LiF3YETbJ5E75Bp/M7W3/+j4ApRottgaa5yfxRtOtblk5w72P2FHqlOCZXcPjULX5s/7fKSWAGz3j+Wn5CD05maPtOwOnqYbDH0hxG5NvfaEwD7MmOr7upDO/kC7g9qB7jYlrITPn4mmRtDBz8YoExYpt+gXoWkFKBYmqEYYJzIaDGR2jMi1ktX1kgclmuckQAigekfNBS2FJBLpVQUk26mMNgifBbujaqMoJ+JpJ0PrIMnul9B+0XLklsvSEvo8Ste/L5uSzgPQiaxOpfqGMuy7me4/TlpWUaf+ePoY9+dAHyVI0JIdUupszJXJdDyRk6NjOTk6kuP5XI5nM9mmUNzMSaxm1QWruRQhYEUFUr2jIZ1PoOgPHz1kF04GRBJR7h8ieAg8UbbtUv4tr6hdx3Z9nr5kQKEl3145gc+YRhmbQinpUNMf2nFWH79JmO1ghwBeE5vGtMxNO4jt/XxhgvgDEsbxkbB8A4zN09MzksoePHhIj+riwQX1xZGrB4qwXCw0j4L38KY4yI37V0Qe1l5zb9PxmmNf0AY19oEgZeJ7HtaYtGR3YgOAF7mU9WpFr2owHJG5fXl1w+jrq69fSZIMZTSayNzb+La1h3KwHQ2HhJDgUEHW92w6FuhjTeAgoH6bpSYNddfxWWfjoQyiGRuToCkJgq1iB3njTOp0wgMezoMSZVR3gApkWcreEn/1Nz8hcebVm0tCVcfTqYwHA7Z4xSO1v0HuC7ktWHsfKDoD4lyHQrVdIZ0xT9IWoN2RJCl6FHT5beZpo0genp3KdDKR3/vdH8pHhgx89PiRQrpAQ9BE5gArip3UJbgWyr3gfccBbSWmfr0dx0oZ2/YvJZx5pUNPVIQOjWls6KFT0cl6/epSLi8xX7RqY2QVOHCS8nzbOglsH+wlUVbx4Js7on7X5GjnlymaeW8aRGKMYPWXNsEVuQBzGabrQCPkrtzqQ5hjxspfoNQyCIDY0NHJkFLl2vKVcsomiAJnG+klUDwhrgXof4gQF9yIHO2QlRlO7rupXno1BSVljSpBHYAYaT6oaiIFprK7kD7GPoLnA3FsW6q3pDJNS3kuuUUO+3kf4zRhzgMt2hlMTrg9LiSebCUeFlJnfTr94cZqLJQGml5DK0FuB1SXVFJrHRiOsaZCpiNwoAYye/yAB+Gb5qVsFxuZXhzL+Hgqs9O5HD864WENB2Fzt5bb17c8vAbTkYxPp1RfXOdLEu5yD77sPZ1YiEMHKMGu3sqmXvPqUvCSKCuNaqdYsmbI56zLtWzLFREF3G/M/CyGsp/luIiIDcmF2lU5nzuswSuKJa8OCwY8LQgj699Z/C7oZXMCXBzVwnHiKeZHSpng8emEfKnHZxdyNJvLP/yDf02ePn5COXfwnO6KmqWcdYXqg6Vs1gtZLW5I6sY8A9p1fnYiF6cn8vHHT+WP/vAPOb9evkIfhDv5r/6r/1pevHjJ4GxHB87ONTgXVgraCmgBRcVnoGQ4kI+SQarq0yh3Z36MZkwDmZ2MZTRBWsOCEqYuB98dh4BtZY0U2Io/9KIpJxx1cJ2pE0L9aajPLKaa45hNp1pWN5noJmB17L5wXcrWlQy1rSVY3Vaz2SMe1r2mS56z9sXzzdxpx0HwqMC9YrKNAQtyc1EEA17cAu2c2bSmq/nvf95DTJnW2C7hxWcyGQ9k2KD3eCSR9YaAk4MNj13ZQCpB/pB11rr5o9FGlaAkUPXX2MaYClha/IIyIsC0gPpRBbBcr5n/1+5+VrbWE8fQaKS2+uOaiAA5An7fvfTGCHieA9cv3utASV/qBKh2AQ6EIcg1sxkfQAaePnzASo7zs1MlPtEhPAySpWPaHmD70bSjH33koONvuRvZc3h9W+6RvfD5tFpGeS6YK3jAXJ+ARELLp6rMqMJWbbOo1px05Ve599P2K1+bndO6Ch3zVIyMbvnm+1offN8PBHH3SYVYfz1tkfbhT7VyK6+iUBVRJZCRi0PNKidaOvqo81b15f17nZGuFEnRIjgaLKe0g9yqmPq9S/rR32/aC7p/q1OgaA7iAU291ZFxHIAUWJqowwX6rIr3M3feWuSI48kr8wJtu1Af097lWt0wKn4iko/RdwHlxamUW6Qbh1QYhENw8uBUBZeQomHJrHU9JIcAfTYgZIRIHaXD2hq+lfx1NIZ6/JGlhcBoMv6AReSszrDOhYjygRIk3Ct0DkaVzVNzCBIQKm3vhSMCdBTfd+2s3s98Dt5H0/w88ojb0SkPbbBfpUnKKrrj2Vwm44k8eviQDgH/jf4wqOxohRk1cAJBk43gUMoOZByVayhpZQo3Zdn6yemxrFfaV+Pq+oZpc6QKgN2qCJZ3tbW5a+lK5ZK02s7t/OZawF8zsE1Zdlgn2qGzq6axqpl3rDB6J4cAkZv3J8dXfl9azh+TKe9PJtuMTNzFeyAgL4MDF1492O0Ke6m2gZZNaP6j3XTsb7F4fNH7w0WNeGhZZEDHwn7Pw54NTPTvVGJV69QJYVoZjffLdl15/D65GfD5P//Zz4heXJweyekRPLGU8JtOssPTsxsKtSwI6z94NJF/c/opFbuSOpG71U5+/uxabpcb+YtffiV3q61cnJzIbDImpNWkiVyvFrL44istRUwHzHmTQ5DElEbl2Jlc5u3dQp49+5pR1QaiQ2BbI7fKvg6VlAkOt4LNTkA2hDytb9B7qRnL1Wr0pdrwWgJFiKDlByAqAdv+9OSIql9PHz3gQvvBJx+TXPPg/IxpAywmsHS9AkCVvt7f4AAxOjBZWm9XTHIb56Q2FdFrdwayM/47REnrhHXj4CFGgSvnEShnBqQgCBOtjJGMEiAQClE+Sb2GTPtisNKiFfAxYmKrt97BgbpuzAyJIOplqpUdk9BTNL7WTF2PwkkWcTKdZ2NyaNmhseBdkIZrBu1VmoIRK+v3TQUTaAc3OpQWWjMdPEZQBSTuhr81Geyq5pzDWELHASqVIEdBT55OAhvRgPRWSJRDpL8SSUvjEFScy0D33GHF+GLMHQ3QgARljbqPOFG5TSO0iJGlNyyPrFA4yh5raYpcUrTRZWVHT6LvYEtsC4Z3xKOmdUrdNXcJH/xMfRLA4OCaYI9ERA49kFRG8xGRwOnRSJKmIjdgAB7LdCIZtExQjbSEZj49MokztNady+mDc5GJsvt5IGOCmzKpLnWP8AeSRUD8gOVYyWvvkKVEDvYtVia4I2oQPsfSHHBz7FShEk5ELANiDcpEOFQ+a7VZEAkk/yHtysnb8NWFiNxJNDLq0fSIqQFowvz4s9/lfnV+ckbED/MHXDeWeiaJ5NBiSRJ5eHEu//6/928zGACvi5wFC1jxvnAgMB9v7q7k1cvX8id/+k/k8s0VOyaWxVZTLKY3oXtSoyWEVhlDnhPFsfYdfCcZY51QkXazkqou5KyZk4eBVBD2VE8hvstsfSeHAIetOwNoCMTc/26nAjTgBmy1N3R/MfarBNwpcPljzz+3BLh7Hk2LQngk7y1XvazIFjvf21QIvQTPn0tRDfudQ6gdCa7zHruqBtJHyXPAhnp5ecn3Wdwt+PmSeEyyRnuDDvQIkKuLQcJpaplOM5nNBmRMN3Uq9dVC8i9eymK9kq9evJKr26VuIsgZDXV8F5udvHh1RQ9ziEMYJVbjkZZxAenAJMUBjzJCkAHvFiowZLLCOCSrStnHzpp3rgjr6jnOBmHej7D4fyYuxL4QVj9kBCNEL0hHXBwfyfHRXD773idk23726ffoCHiNPuaEkhO9UctBQ6qHuKMY9gCvosHhZRu/7lYgSunftMiWOz/8P1fh65T4vCRS0TJ1NrEhAMFhnTzvgbaa9q9e9oX2sEqS9bjL3tuQCxXV+WbGr0Vm9tAti7bc7ufukVO8pzF/kLUBcT9VZ49e+1qXyHXBLG3Hq04Cy93wOdzxMdGcstcYCg8VNzL9B0cl6MzbuAE+BbO77Zmg5a7uxGHj5mbcHvxde2ltbmNj4ukD/4jWTRXHJk4+dBjlPfW9oyXOHo4O8FV4WLqjpt0p7cpa7oeOrB2qHFLP3eNaMH8VOgbBEIc82hXX21RSEFrR2RECYmhIhJ4j1FXo9ubBaCSj6USKrGRE3yJPvbnljXIA/adxJmlTShprUypUMPlzmAboT8kWnDNnxn9GR8EOZjZJVOIuHMm9v39PQ8oB5Go6NFaF1XYutYCjnVh2ebhCpALm07mcn57L9z/+Hsl/2KMwTojske72IKMxhAFVdd/75OOWSI/cPlBlls2iLDRN2U0WJdk3dzfy1VdfsgfQZgM9lorAHY7t/tpm3xWmeQ0965Xw9g/3Filg8AYBOf05UAnnMOmzgRx8R6RCVarTyMjV6PBAlzEcxPi+36muhTz2PoRuJs4HoHgRNlUS1ZSf0BKC+qQf+4qfAUnolxH57zishgx09ae+EShC4NrVRByYp1SnoW3KgjwQPO7xhAcrIhtENfg9VRlB3osme9d5iK12G8lj7/iH1Egsu20l61VJB+Dl1a28QW3s4lZul0v55ReVPHv5QiajAWWHTycj+Xc+fczPK+weJ3Kz2dIRWEJS93YhG0gJA/ZHBLbe6BiZahuXM+pV8XlHE+0Pjw3Z1a32GPim9oYcVQrHBOVjWjngI4GmRdiMgQY8uriQh+dndARAHjw9OyGkNh2jARKi7VpysrvBVlcNCj0ADotm/TDvasa7jc7LyfYcVmoMMDnbHgo8XLjwsUl5OVi8F62ng5Gk8MizAR+D2KISLkrrGU+iEqBII1XaQdXNabtCK/20XaGNZlTZU0V52lyoE3t7REQnbCFv63LwJIWas324ddAmKoK2spYoyWWQKSPbL1s7GTZs3w1SK9RB725uZJgl0owgBIa0kavFKUKAuaZtiEvO2xrVA1Ya54I2yk1xgmCsXRZ5UFsXU1aCaA0+O2pa59W9edGmDlS6OJKhKqz2SF26iSpC0DUyUrSjU975UAgByicHbWlhiwYYg7DtIunuVisI1NMtgPOCXD1EhviZbL+kvgGi40yKCqlHjOFUkqxmLhxO2WA0lnQwliLeMMLUjomd08UIvq141Cok7IZpo+Q/OgQ9pUqS2HBvGOy6SJxxW4zXwQ59dPz1P82QoKgZkHsr6vfehnGEmJAS6rrgz1MFmBe8HqwjKjh2aFFVl3J7dys//eXPOT8cqYbsOs48IgQsmY1Z+YXfYf7g3ECVAXk0prmCQBVBGdCv1WotN1fXMp6O5KQ5ltlsYvsIUlIq1e0dGEl6taoYdw489e6yy47C63qAngRQokRmR7ifSpzFcdCmINAX4zshFRrBz1EBDAJK0dD1ED+7ublpywX5Md6Sv7vvIHjk7sjBXiXAb/h7l07GV3hyjhQ4uoDn+O/dcfD3UohSIwoc8PCu4NXh+vFvODUsTcugMjdUpr6hI/g9xGbsQj7IxrDOc0L26D6228XMny8X6Mm9kuvFSl7dLuT6biF3q4XcLe/k6uaGG+YpnIHpSM4+/Vj++I9+xL4MRTaQXVXLT5+9kqu7lZRbbMYLuVuv5Wq50LyhL26MlzX4gBeNDl+zo2PyJbBBM0K7x9L3+4G/pWwmKwiGykpmBC6Ea6Er8OlHT+X3fvSZPH30SH782WfKuk/VQUO6ghUTeA+KIOhpQuKx5YwPsZaD3YZ/HQCrt65XY85NQxvqKCZrHBnCiRE1HShshNQUCJKWVoCnADEr1MWrQwAJaJXppUOA9IHBpVTDRPdFXfG8hrZNtBNejUegPRUUVSA0SxKGXgMbKeHwdT5hW16Le+LManWnHB6H1LUrFh5mLpGNuZHLrtpIAmcg0i6lzKliI0N3UGmYMoIzDYdgcVdLQXW1EX8+mGANaWMZF8PpnAItsXQRHUdSMGx7nACiNZ4a1HHE2MP5wp4AB7SPJnrFjL8m9wtzDLDGCZlzzvc09ltCst+nroSrWxmHGA4nkE/3nRWdrUpa7faY/powZ4VcGYWaVbnQO0PCtN4QXSLKKtU0Ax35iiqaUVVKhs6k2ZjESeVkdE2P2PzTI1LTRiApVhIC/DA2APa5aEhap6Hg1WgGc7POqZMOV2dAHQLQE3G91jX9IKOjzkhed7qu6gR8CxVjokCRdWlNWkcGlW0l99hfffE5X4tCelXFnD8CV84vzBvMGaQCUc5tgaQ7BM41wn7HFvIsF0Z5ck7CX4rGRGi3XYNwjGAYDaO0mRWrYNCjAI7LCKTodseyua1pWd+7vImg9wKazhCgKE8nZqdT02F4h+30nR0CfyirWJnF/r3n/DuI027S/i1rJ0W/VKiD9OUb6EAnEuNwrS5yd1D6h77/bavUNNSD3Z2GtvaAC0CjU9xUXL/mhNeaA0fziiyTp0+eEOZGHSnySpgIffLUoXZ9V0h5h8gITXKAspSyXG7k5nqhRML1WqKikBG0CoZjWTe55HUp6WBAyVFJM1nnpWzqRq7v1rIpSvnyxRu5ulvKJRoVbXfWZtWXppprMkBmGDlYLCSMxWq11DwiBWTUhXA5VK+JpcY5c/OAxFVO93x6xDKcj588lpP5XD779BN5+uQxS3XwnpRNBdkR424wuYprWK8+mzeuhX+IvXxzzfchH4X153CXLQZzrX9sXnrKqFNCPYJO6Aqm/AJTeWNJUddSVVO9mudTSDyWhrk75W+o5rluknukwd7EUREhaxJk5ZhaxOIk1+7w2WPLt5Il9lvrhKffarrG2dMts/6gEfUjCGa92J1kZ6I/2q+hYYVFieu0hmKEotGutUplEDdSATlANES1QCVjYn5qlzjr/GeHs0ZGvp6VpIr1BwcMv2ATKDrv3nG132/EDv0eQdl/xvvbpJSBBd8Gj4ibtPa4bxPZ/UH7hvN/+JiquBM+q6V8jCKi8xDRet8h6Kc2VOiJzgD2NFx/MqS+PrT3UakDnQpVu9V5rx9BoXTcoxiwNlA+ii0BStc2353j3KUqWjKpQf+OJHixGbkbLkrVU4htC0Qd/dLEmP4NEjJNQTYJHV8IUoFdciBEgHbF3O+NG9T5U1bhw2/18MWW5A6M84mLqpC79UIRGiPyQesV3QVZTlmgKVcicaPKkM5vQ8fB1Oaoi5epMJClAeJMLi7OtMwdxHwEpvla8hL6GfWeQ6BrqoeOQw4dlWDGjXHVUU9vtlyYGPcAgZm+Lz82nIPku+IQ2MHvELsTC0EqRC7PhVNcPrVl/fbK/vZ4AV4v7Lmlt6AC/e+dW+AOiTsGTkTsQ8H+M8CHUIqiLO7xsf4Mvavt94Ta7Xq8rTMHxmBf/A1gJkhOIvLA36jXdXBWlvb1y1yuFhtZrLfy/OWVvLq85aF8c3sj00Eqn5wd4T7LHD0kCKstuWkiop8enUg8HMoVkI6qkV++uZO79Vb+5tdf0SGAeh5IV/uLXNnh7nwtV0suAuS17m7viJAAAtbDRD1QbMKsHKD2OeA99eQ18k3YyOfH3/9ETo6P5A//4PepNohxw5gpSdBQICJAmKQmDWuRKwhP8M6xWFhag3K0A+znv/5C8nIrk9GIpEZN/QxkmI2N76DyntRI4AWYI0tmea8u2dIaMAiosBzIG93Ygc9DGjLSqPyQjGMFNECfp8/tNqaGEXvLfu513/NmOy3vxf/IlTjbPgZdfXHHF1A0Qdtbd6mKlq3cF4p6T9Ogry+zrEgKu7YlifJWcP1TKLQVdJJQToXeIFBtI0JQFXTO2NgI6bLViqgbEDo6vwUQEI/otUxswBSIzsEM6ajRSDIgd+D74DDfZbLZapfJflDhKCEezm+CeVSVIIKqIaKjfBrV1biHWbf3YP9Q/jAr31o447hhCrOWmFK1dgabI9p/TyeToV05iI4IaFh3PBhJlE2ppDccjKQZDmU0gJMmTGOlDXRi8E5aFjyfT9kvIkOFAeTFKzgE+C3e3/c2IzlaGguHDA57+M5wxLyFMp5egngJfx8blSF8+h+QLe3V4NRXD0zgBqDoEoECKhfqKCXBEA3SDrH57EhGLPlVonTXKEznBMiMHFPj17TONekctWzzgiiplm07wbMRyeDAYp5qC2g0GOLr2npm4MHU4JDr3/l2SDfiLdNkKI8ePuCeV/KsRG+XpeTFVgXSIpXyhgCankXgwml5L+bGaARHR7lhCFaVTAjOHlghBoZi3WCfsupYlkBiryu/w14GbvsEpm7zeuty8dRcDznQMqKeg/BbYm5HCLyywEmJzhHww9wjovsoQr/qQG+gRxAoD/EyDWt+MZm0kA8iNtTLewWEbyY2ACbbepg9e3Up14sNm2LcIN+/WjFfOR8P5Xg8kkfnZ7zmW3AdcWiDAwCGdVHIcrORy7tEPn95SYW4F9crWSFnRQ6BVn603foc5OwYVUo5MWIWJz6gWouETeaxJ/TR1XCTQY96+8FATo7mrHr4yNCAE3eceuxelaW1ueHUV9/gehwPXKuyaw+DuAGrk2yG6HCElIbske30/nVRn/Javumc9sfJf+QOJzqR7cne9jz17uztcWF6FQXti2GTxj1ijX3XVMqdZ80haz5YM1T+XsoBcW5E+x8jZqu9t2v3L+58va9pttWi7CSWITposoRS4VjCzZaKcidSywuxrjSyZOOWUkgAxt+hMgjzmHlYQ4jaVI/zW1xkxQhinhbEMQO430XQoqgU1CwpaXC/RJSKjz38u6XLEXXR0i1IIySpcwWMoNjxzr6xq304+WJ9Nf4/IjvrislD02+/pyg8iGJfAaSwIDs84KOJIa0NHtFQIjTaYR8GJSXq4a2vB6GhZjBSYqLdK23s41UOSnSM0ROhPdTNIfB1bIJEKk4HpwB/a5UPbLKMaicEMNCbwKNruKaImK0wU2jU6hTdb2tKJ7+/oWss28Kb+me/JFbfskPkYPppu0ofsdQX1/M9ASvlTNj8ivfPJIo8WbmfxQttiTF9LBtalmcaCjBAWoCgNSqQFJHF32trcawJq8oA18EqEhzhYBDBa8D7mmga8s493mu3xUXfUftj70Jhm5Pn5JXc0yqA76UF+rxcPx06lv++Q9ChO15fbVHOPXIiczV2PUreUIKHVxz0I3/8DPkf3zjwAPyCTZiNINDBEVr64zFhT3hiKg2LxaK76V5Out+xq/1g72//r//yT2Vtyn9oSoTH47NT+eGn35OLk2P5o9/5IcsAs5/+Sp5f38nNciWXN7d83CyW8nkcyZ/97Fd8rYITCB0MPQ+uTgu3ciOntKQlO+hAesF7J5tdK9YEJIDPNUi9Vf6ysTg9msmTRw/k4uxU/uB3fkeOplP53kdPGSU6ZIbXRuTGFrLuwmJM23RQxPwyhY7McaNHC0a09WR4X0OjLThWUVnIBEJTY52PZYSeFaqXT8KRRQ+qgaEIgM9YbzKDBjd93TxsWswfetrMWv3i+tlwKlOn0Z1kLG4cZpqu6ioGvKJhtVnJhpoQmkMfpqkcjdUp9fbMehBaRzvUkeMaemVcujHpPWfzK1wz2lHiPpB8V2iHyQMMpC92o0xiOcqG8iCeSCw5I2oEdZscuVKRwXjK2BLjgHW0g6ONwy0SWUNOGt1Q10teF+6RVrwgQsW8RZmVHnZRaumIoTri6XAog2HGZjwsq0U2G/KwWSp5CQe5lOUKOVnolXjTICA0qsRH5U9zTjmm1NZvJM1whCUSsZxxKA1ywhRfcqKpl1OqNLTKXLMJ8+ESxrGKLal2BKJ4J42ZYI4TXN2NpHQ8Hjh8Y6mSqcgEZYMjqdIhmz41s3OJmpGUg6Eezk0ssemSVAUi+0Sq44dKqhvOOA5ZPJSxQIm1x1eATgAukQcSxg0S6OpQGF5FiFy/g/hNIlE1E4mGkjaJZDLW8kO2JsC91IoTHHzqFCj3gWqKWDeULh5JWh8oXTyZs+LCgws89JzSXD8QbRg7kGJ+9ZC0Gmx9aWQ41n4XykVQUjCcx3GOnj0myYz+b+h3YIjycKidcyH7jgOb9885Uez4rpgIHWumFiMSueN4Yl6fdV9kx9KSHACIcGGmlaWRM7HGMQ+hUcO1rugiCYhRxCZKvo0pEmuO43fpEHi+vnuY597W9/b+wKKYdz00XZyhXQzGuu3/vhNxiPachL6gUb/2eA8xwMM4B3u8A0IsIIQBAu4xdtsot0t3+AUeWmmAQzGuK7YTng4zmQwyHrg4bM+Pj+RofsQSKwg5zaCVPR4RlvKyE9TsF+Vu3xt8a/1OJ3PTlf+YtK79bVsh4qQ7kxzmPWa3Q62tPz2ey4OzU7k4O5MHZ2dEUcCzQGoF+cW2m1lLljMm/L37p0I01ra1Xxt+IEIAdEJL/rTkqEuX9JCq9tm/SZZWv3Z/b6V1Fml6xYwT4/rIU/vK9hmZP/UqmL66zD0kC3OYuXcr9exXW+xd3lsqXL6x5D0lQ+2VTgDofU05H9IhBBAJK6FiCR2Sgu1gEUGR3Ih7CC/QoOY2NGLEBJ6AVhIwcnUSaQ+5chDJSz2VSOWpE0O8XMWR0Ggsaa0pFwwLN2ZHa9oI33cUQ4GsJLKN9S19wwZGRuLs6td9jC1saafSgQhBm5vvOQG+Xhwx6L27zjOfDHBOwHhH+suaG7CKBW3O4RxYy2b2ElHmu+tssLGXH1rIcTegClq1lr0Xjkp+1xIq4Vxqe3oc/urjG4bQYK9M1CkgogAXy5FjT3t4lYESNRtA23CrItcvULpiRYnv97d+makKBGH9KorFUlybxzo/NHztOBZNixB0JYBdyk0DUF2TvDOejnOxIidT2mf1UsJu++nutvJjtGKrDULUHbCoP2ZApl0YrdLIzRq3+XW5FDzPNGrq+P7QI0d8Fw7BdDJlHmOQDaVkz2l9q+16o4x9kvOUoNLHWd0v6IJThxk6Ru03eLvtxt1t1u0m6N28eugCPDVHB/AHuBZMiPu8Bxi/RhG9RScaOSmR/wYkDAKHl9G0evEtVvDB7B89fiTnZ3OySs8ePJBjHLDTuZyeXnCygKgX5bk8eXgho9GAfRWQq8dXPNzrxVcICbGM0ssrcViRiaz9DN5qvg8DDXCHz3pqjyHKkWibTxz2P/j0E8oMP338WH7wySdau4taXYu+qHnQSkR7V8PeWrB7BqhYSWP6b2Xma75dyXiHOQQ/+vQHcnE6J8ud1SZWP9wuHJMx7aD7jkBY90orAVcDsVAnoGto5DC3zyseikUpo4kRFg1Bw88gwEVNhkxZ7hCPsSfwjeGANhHK32pJs1odAtkfQ21frNE+CSV+4/qEKYSw3PTgFINEajwNbHDmtB9iSD6xbjoVGQ0TmQ8yefP6Sr788gsiXK+vb3h98/FAx3085diRZxKP+P1sPuWGtVmiZruQuMTBUthmGkmMtJ+tUQTLXIvm38YN+iGU0hS1RXvKM4CjjqOM1UDjEZ20ETqwgssBR4G8ikbQBJAOnLWVbdBsCaVzyOeyw6oOLUZ/MMTGiv0B5cC4B1oipslZm7dyuJWeSmGGwqo4rBkYTHPYTOwZmVPbNTNahI+FfPJwwmqXuFJm+WAy0RJEE7Yh42ez0OlbNNKAp4GmZqjoQH65ziTKZjLOTvZFyLzcMMJcMsQEh2wzkhRaxyTU+lw2pxmvhXtWj0UaPBwJ6BwyJjFbkSs4BHDgsAcAVUjZy+UQQ/UAOj9i/aL0FesrF+yJOOg1mvax5SnUTyfWpiaKQ5oBuadSNdh1oS2sL1Sn4fOxuIfESa1U0u6KWqHg5cm67yjUT5fIZNFBToQDpSifo7q6z0ByGOkPaDSQZO3nKbsaQsNAg1bsLdjjtJFdJCU+L1R2d9ZLgh0xvyOlQi1V04MT7Y/Ho7FsR2N+jw+Fm0qlPyy6PdGU7rxvKwZ6aYVOqLPHZL7vHNxzCPosXUQv96N/TWN0ZUd9WBqHZnIPQWgf2BysdhueGa/PuiXq9RtFpksDH2SP51N5+uBcJtORnD95KicPHrKb23h6zMMVbaVxGbPxmLD2xSl+LrJYrUl+UQ0HFcPAEGPyYLI7mQ/tY9uo6K1X0CbH25wrCVwgig1VbWs+1Xv86OJcPnn6hA7Bx0+ftogRxnG92ZnSmbl32EjpPfccOlPZuq8Y5+jMoVGs2xFajE5m2h+gl3d2R7StN79/83iP9/kT+ne6QTDnzzlmyIZBuC6zq4hCN6pe7oY+A239vGmUu/kYWii/B0K30bFdv+4JXVipa6xLaeH57XF1r0rhYLO8JV5MK0uwjipKe8MxXTkRqy65EVfgGACVhkM1wOkOeH/MzTBOc0aFUVKwtNOjK/iIMcmE9z5/r+EW2Nl125Ss5OfFsU8CIvcfVUZE0x/s66pRaeqv1giJuQ22Xi5JcITokUb7Gk0yamMZmgkB9VyAPQ7IgW4BL4Vr1e+X56XtjtphYyvH2gVrNYULBsVeYkeCQK2HGYMZj+DxWSFZ7GcwujsWUqO8kw+gUVC3A8TfkSpdQ4ByQdhjGyWNg52CSF6RAK3a6ta358whrW66g5YG0ZbjHt26vD0cApQ4WgRcp3qwHjKmhhopIOXRfXevnL3fBur8sPQWRZHfnmCYkzhbErnrKnTIR6t02SKNfW6d6VnwPnZv2LZANwSIFV9W8uotoVUASduuKzrkZcrKLdL76zLxhtLX0HtsJCoNxfAmc9+VQwCdZ2xe/EqlpqlGj6MB2eloDAGkAO2OcZAhaqdH7k0Cdc0pKcU4B6pg1XMQfssa63Tlpefl6STEgGI5tyWRvQjOCYjajSrSr5Y7JkHMNBDag8pUwRQdBKRlb9srqeFlfwBS4b/1b/5DObo4I0M4nUwkHaHFciRb5rp04iE3/fGjB1Kcncj5+bmsdrls8pLlhuypTXSmlLvFUh0Ei8CdrMWxsFy9dtbyiERTLToZ9fNMpxM5Oztjcw+kLCgkNAOxciCPHlxQ3hPtoHXMQYozURce/picnePHMepptntXL1ibcrqvTMmPfNioIoUBXkifANo355e57kXrmDCaVaKUVgui3FA97VapsMcNQHUEHU9zErQcT9njRGkoiLJf9ufvpcQh7UxGdCIxVKAtj7JOoi5KQoh4n0joDk7nYHTAuN4gHW9ch8o2v78lUSk7NGuSSG7KWF42iVzlA1k1U8mRr89wuEMLX/sQbMqVSLylGiUidrDHx9MJP9twOpcBoiewscGYNx4EymsF1TSG2onfI6xV1HSj4oB6DMivomwLOVnt78DILlap2MoOAa90h9tQEiVoJAcqgEAAeXWUfhVAcowfwCqOSJpUtUZqpm0SacYnIqO5RCDwEeauJQN1+13o22+xqsrQ8smib00R8M75+rGDTOeP6go0hOVtDaGuPgMBsJZ8cSsFOBtbNGtDHjwl0oEDl42vGpEcHIICI5IpIZGGSBcVIa4B42lFTTco4genBRwH76fRguzdWqDPqH/r8w+IBTgamo/X+c2xRqm3q1TaQYw5Awct3x42pqhITRINNuio5ztbi4ZoG1nV+UPYsLjagBJHfkB7Ca8/vL+LrjvsiYjKSYg2fRAoGeKrr2+m1chtA0hizlUrrKWywt4YziX9XTbf+Qt0km1t6F7eOUtwdPGR4Hhh/uueoSPP8sSp7qtAGVTy7TtwCBAtImXAPH0UaxkESFRxI+PJWLb5ViWNd1tGFDk2IVUB7XrMtFuWeVz7AEK3ob0temvTDf6jXuzTY6v3qwva3Kw1RqIAEh5Wn9wiCVSY8uYRXiLpjozplrXUgQ/HIfjhZz+QwcmplHFK0k7h/RfQ9pj68IClIh7OeMuLi3PGCDvrdgghF1QVAMa+uVuYnkJuToA6BOwEh6oDa8SjNeQGa2+hHa/90/E4OT6mfgA0Fx6fX5CEOZrgPqcyGU20NaiVHGkpU8+ztnxan/LhzgcdE9TfcqNwtro6Bd0tdIfgsDziCFUh0ItgFOaNtrxEqPPsnMPQtQq1VAGfbTlPO0hjwKuE9+yD6QDwgQOqX8WizXlsgzUypzuXvRHSPCDKBFs1uJ4eQS/zrRevjGwis14e5sviHrLScl8MBiWx0Dg272tRpP0H8CrrKpKbKpZllcquGXAORMlImgiO+JYwcVEBMWpkZH3pccBDBRMHGEpmeWADbbDeIzwQ0kJqq/qB0BY+AwIM/p5KlnDUte47rxrZFJb6ManaARXaYpJYSyMCuiQOrhHvpyVzKHXD3qCvk2OPwuHPiBv3Q9NENZ2QVJrBRCIK+GC7RAYeqR1PH7y/1ThkIyBsurfsq7y2A2/7nJYBK3lbNyZvcY77XKIFOtJxOyV5otMo5XPJetfAi/MUnVCJqLiENuaplrj1+QqoGGBkWqlgEFInIBSqdKGtEXMgvGLMo4q2VDxRmQQ4Blnqa0MxDt83tNJTnYoI92J3GEJAAVVGcKppgfUJiB+Rd9RXAW25YzrY3gW3sYZlMG3nbObPhxOPVARIm94lEw4HOvUCgfYqmEg7t7rqpqcnyHkxBVR3+LBXoJwRX11AawBHyiaB6kRoeWY7P5iXQLAKR1q1QIiKmS6Itl9XpLuq0u/GIWhhT6vxZUMbI19MplOOLhjegAnRovHq6spqjXeEtcnMBtO43Q671ED/6286YjuqVxdptZHo3mZsLVp7IkoYJEcIfND9566S6F9hWrKHz4ZDQ/a5BHqh+1/f0+4GKdsdY/NkPbJBonx3eIhW2QFngddl5Uj0rCmsAZLxkFwDPMjW5gHvJDaTCTVY2zUGKNeL9wRsaJ8Zf4HIGuWDSBuMJ1PNX0P4BNKe2IgdXmW0fC+baoefogL2OYhQWI7WdFA1KrZ713IL+l8PM9bkm1zpfUeyK9/ze3q/Ja7vbBq12cDwvmCj0br+VCIiAhplYk5jHDvnUCM6tAHHxsGqANMk6ORHHZXR+MMDQkW8un4HHRVn/9Dv1eTs/caJXiDuaatqUwNsdfjfz+ajodytlSdyKyMeMKvkodQnUF7LJZ6uNDdaIwfcle1BiAiVBovJRF4mZ2STT5IR5+0u20oVGekX8x7CRYk6jNqUBoiEOrCMmKxdrXOEiuFOP59dY84yuo5zoZCxlrIVkZYlF4kiEuVIHREgDVh32oYRDkDEa9b5wtNFZPJYmsGxxOlU2zenqZxOBlxDhxiFfK0cUNNORknn3G2IGmkKY18wSJU1UXqNXLHqjKSWZiDvAWjpVtGsMkukGhoBFFWfPJC1HHG7Q8DQSDyMJR6YmJXOIkliqhYwiUWEhloFiGZNg6PHM3CEUQmIyhSg+1pEst7pugKrXhFJRV9ZmcM8uyKCLJ1LVC3wUNNtqCPy6kW5I2/9KqxrrzuTXqVTkZuiY6p9OTTaV7KzkSi59+l+yjMD3AJLeTKvT1Eok+5u0zw9584WrIqx6RxgMAVlVK98wl4BR7F1GnQ/ahEGchn0GjyAQDIHThily1zjhe1RvyNSIczJd/yKEg1o6k/HJLYhfbDbbSkGBKQAeeebm1u5u72V2+aOteGtQ9DTLvg2FJ2WjPgbShHdIcDARRYZ+yHfTxnAvPTkPvEQz+mqFbQbIjYpeLgeSd63QxGCG9Tzk6+iSn582AQiRAwyGb43NTMXJ8GqQoACAiTIhpr7t8jGZhyjHW+0QzJb2yzAyFWav3PFuQ7k1yWdN+AKI6pSB8OhcCXKtC5u/wa1P2MUxs57nWYE0AaPgNUZ8Jr7LoK3bw4aU4XZMAp+2PdZ9h1xSvNz8o33bp1VTxca0oBvWNKKRc/GTxYhFCUfzp9whj8OtcFA02usjW6voJcn5obbMev9GfuOkj6znzL7TUMEEpJqdSj5iYeNickcYvPJWOSqll1ZyHU9liVqzpOJ1GcPlGxVKeRZ16h4AelJnRyWFOLnaSpFMqRjNIkhNw4CVCF1osgRIHztlaeHuhNVy4SxpFaOeNUBnHkj06qQEwiCPn7gjXT33Hs8NJklWkZd4MD9wku32zwRnPB75Od0Lk0yYrkYHIJRlsrZfEAJ2kMM7ZUryfSQNrQSsLlQMbGROkZUCz/FG99o1AfeA9JBQBHXW01/otQO82OX2+sADSlq/lwP4Uh20KxhFSGQCZTnAk1sJEGEOVIBLU9BaUkb0ijqsBYl5hTSBppGcfZ9p5dh1nO48VyUKyoT3js5+kGFvViFyvCAv5A00cHCRH4Ne/90oM7LfY14q5oVQAa8b0Rt7deV2Og8hIjl6YogeNdTneu6x6k4nmoxMFDGnktnouuuyOvoB7Cu/GvkQ9U26YSU/Dzal3ruHAJ1XiywNZn5VrCM60TTEv1U1Lexg4qT1VvSGmtcIFrb4vuTk1PyDOAADAZDPtJ0QGdhcZcqw3WHnItD9RovOfTcW93tYdW9571UwT3rIwW/kVT4llLE/X9rG1C8C2rWYV7K8rb3O8gw/03hi5/PXe+21bAd5NSsgdKdsd0Jizn8bo1ReqgArw3CoDikKNOpTNU2B9569d37KgLSCe24rBEnIyIWk8jVwP5eft4JUTyMtUaXSmxwwg2+cqGPdty6Gqp79++wIf361ZUMn2YyBY/Ac+wO3beDrFFZ751NYMWt5x7Z4utLblMrwbr8YfNkqsE/mzXtcbKh11/38f8ug4Y1pJUJjlB4LlehTiMyQosiVRVE1THwG9c5If7eMJfj1lSOyfUeYJ88PJa7LTa6WCpU9CB/35aNAi1RCNWSSKr6qIOltfxe0osNFZ/BCG8xIpykljhVB46SSm0fe5wUGh17pznfVNGBMEvwnjVLntt1aOWDvn76PJI2P2zwvKJnGrH6HEFkzta8nXCvSDpiE6/5ZCQns5F8/PBEHp3PpS6Usf6+djGHfHIqyHxgOlUohWhzzppOUpndrixS2fz0JmQHZNYq3BKkbsmL0nbb2QCVLaokOhii6VkkJSLOGjr+dvibFgAkB2JWHTonRdN5SiXQiBbIAK6PJYveEMicNkdtHMjq1rA6Iip77qk487Ya1H+oQ8D5EUcyQsv27WHVMM7n8W2U5HCTSOfeZsiop/Ac9te5E7VkwDZIATpb4jzxcw6BsLVC5/pWJJnthr3xGz+rrXHjJuHlHKFmZ0rrxqgFDbpvekM+mCMOup8a4dQChE7i3++X7q0U2EJlgpHiWTwK9CNtvhuHwA/bvjk8p9EQRGUqmYyn/PBwDJAyuLq6ljeXlyQevn71imTD2+trkvpYLgj4BpG918T336B/gHyba+ylC7p+C+U3SYX29T5C4IqHWFiA7NQRsJrct+RqDz69AKlhW3LYyJrUUHHOHC6XufUSGSIF3jnMQkv1OD2y1WtCVEgUoBeBe76v/STWMMOp3p7I4fNtstL5gNwxoSmVLm6Pcv/4ViPtJXmdrDQkVIFgdPoCvIZ2OnfEEYfzD83D/Mlf/ULmJLuiX7zmAvsFwRQh4UFrcrVOxkmtsZDVJ3t+lNB1rmWtaZETbUFJIcJwbInDgebH/foR8SJvXhY6D5nvtWtzbS9+bFvELN9qCaqW82fkq7D/arvhvJzN5jJjrbFJCPfutX8+OhGEwrUtMNflCMpxh5Vy/tv/AB0rb+SrV3dyebWSm7t115ei7WOveXiFtfXg2Lf9+4pGLBx3Y2675M9bnUXXOHEZZ7QGchSlddT679HnGvX/7akYb9LsR78ic/obaxhk69Lh3+89OpXPPjqT7z0+lj/80VPJt+h78v72e49GEqOEG3B8haoNfG5UPsDR8AOlv0bsIEOwQqKoSPng2HhO6rCzukdMwDB1LXuvVrGUSqx8GE0X4u9UKc8Z6Vr149oGFhSA7wANgpbE6uhLhwo6B8IRNt9n9HDu/a19llbSnvC8tuxFN9ZDDOXwZapEYKz/AUTWKOlrfQkszeNBlJ4VqhQaIUpHHt+c/codcwgDkayHfgLWQIqBmiK2bEaEYBhOa9KRhnmmYX+lZgfks/WzTeOpZJmmF+A8+HnUJ9O6FL93GO5Qly6V2TrHCcT5wJsayRAlqHZuKOkUFSfZh3UIfNNBz+dvHMwW1WiuUg8CfHDdFJXhj5rNPoufN4EDD6iwY8C37Pe3vHf7dnv9DdofavRsEqfMFULwxya3pzncYcFgwwHAze1KOWoyor03AjZzjWqglGZ5ybd0iVguFm+9zm87puv1RlESZ40bOc9h51aVvj2FnWNu5DK7B61aeB9+v7exOnKyd+tcm99yW1454cRJlrZB952l+FBIM/ERSpX2pN/sXXbmELRtqOEQlNab3kkyTtDpiYLozzWihrLkIWN6h5JMPsZtr3XfvFqGAPUaNMp1S1P0UreeGPYzJ1wC3ULZJz4P5hi786GpFLUfuo6dSupEZ8yc9xalobgG6izscRXUYmzO99IJWoZUyW4LolIp6w2aoIArj3JaQ4L6fUJsNOkUWp8BOAaoLIjTUkaVyArJ3APGtNxtpSm3+EakxPdIEfhc0YNVqyDcEdhX8mtLuPbef78kDPX+BmO1ZV29p3YQfs8576M2v/Ez+Ev04Nq+U9BRmdURAIOHwkSeuiOZFFyJrTQgSxYb2eGe7NYHjWmOlrrs7NiVrioP45t5dCcS+gMOAQJ8lsP7Z/KD2r4nxwi+JviPLMHVQ1+5CVDmU/6EsjCUkKROAYSlOkRL14s6ef42Opb9Ut7evuEjbiWA/RRv/35344b9BTCEdsc8ZEx3223L6OcB6h16wWEzzgiNyIEhHAbPRdQosZRBOwcdE8FeAL6Qcllc+RIPsvnjVKq0E+fTKiKcQ1jH2l0XX5keYVl8JnVqMuh2jd4XyAcdr4PAo+pLe/eQL+f0pXEqdYrdATm6bl+lUBc7Lb7D2m++hX355Zf9oDA83vLAGL2LhTENYxrG9O/HI4xpGFP5ezKmEf7vb3Ma4Fk9e/ZM5vP5BxOP+ftiGD4gJ09NqOfbWhjT32xhTD+8hTH98BbG9MNbGNO/2zH9Vg5BsGDBggULFuzvtx3YritYsGDBggUL9vfBgkMQLFiwYMGCBQsOQbBgwYIFCxYsOATBggULFixYsOAQBAsWLFiwYMFgwSEIFixYsGDBggWHIFiwYMGCBQsWHIJgwYIFCxYsWHAIggULFixYsGCw4BAECxYsWLBgwYJDECxYsGDBggULDkGwYMGCBQsWLDgEwYIFCxYsWDBYcAiCBQsWLFiwYMEhCBYsWLBgwYIFhyBYsGDBggULFhyCYMGCBQsWLBgsOATBggULFixYsOAQBAsWLFiwYMGCQxAsWLBgwYIFCw5BsGDBggULFgwWHIJgwYIFCxYsWHAIggULFixYsGDBIQgWLFiwYMGCBYcgWLBgwYIFCwYLDkGwYMGCBQsWLDgEwYIFCxYsWLDgEAQLFixYsGDBgkMQLFiwYMGCBYMFhyBYsGDBggULFhyCYMGCBQsWLFhwCIIFCxYsWLBgwSEIFixYsGDBgsGCQxAsWLBgwYIFCw5BsGDBggULFiw4BMGCBQsWLFiw4BAECxYsWLBgwWDBIQgWLFiwYMGCBYcgWLBgwYIFCxYcgmDBggULFixYcAiCBQsWLFiwYLDgEAQLFixYsGDBgkMQLFiwYMGCBQsOQbBgwYIFCxYsOATBggULFixYMFhwCIIFCxYsWLBgwSEIFixYsGDBggWHIFiwYMGCBQsWHIJgwYIFCxYsGCw4BMGCBQsWLFiw4BAECxYsWLBgwYJDECxYsGDBggULDkGwYMGCBQsWDBYcgmDBggULFixYcAiCBQsWLFiwYMEhCBYsWLBgwYIFhyBYsGDBggULBgsOQbBgwYIFCxYsOATBggULFixYsOAQBAsWLFiwYMGCQxAsWLBgwYIFgwWHIFiwYMGCBQsWHIJgwYIFCxYsWHAIggULFixYsGDBIQgWLFiwYMGCwYJDECxYsGDBggULDkGwYMGCBQsWLDgEwYIFCxYsWLDgEAQLFixYsGDBYMEhCBYsWLBgwYIFhyBYsGDBggULFhyCYMGCBQsWLFhwCIIFCxYsWLBgsOAQBAsWLFiwYMGCQxAsWLBgwYIFCw5BsGDBggULFiw4BMGCBQsWLFgwWHAIggULFixYsGDBIQgWLFiwYMGCBYcgWLBgwYIFCxYcgmDBggULFiwYLDgEwYIFCxYsWLDgEAQLFixYsGDBgkMQLFiwYMGCBQsOQbBgwYIFCxYMFhyCYMGCBQsWLFhwCIIFCxYsWLBgwSEIFixYsGDBggWHIFiwYMGCBQsGCw5BsGDBggULFiw4BMGCBQsWLFiw4BAECxYsWLBgwYJDECxYsGDBggWDBYcgWLBgwYIFCxYcgmDBggULFixYcAiCBQsWLFiwYMEhCBYsWLBgwYLBgkMQLFiwYMGCBQsOQbBgwYIFCxYsOATBggULFixYsOAQBAsWLFiwYMFgwSEIFixYsGDBggWHIFiwYMGCBQsWHIJgwYIFCxYsWHAIggULFixYsGCw4BAECxYsWLBgwYJDECxYsGDBggULDkGwYMGCBQsWLDgEwYIFCxYsWDBYcAiCBQsWLFiwYMEhCBYsWLBgwYIFhyBYsGDBggULFhyCYMGCBQsWLBgsOATBggULFixYsOAQBAsWLFiwYMGCQxAsWLBgwYIFCw5BsGDBggULFgwWHIJgwYIFCxYsWHAIggULFixYsGDBIQgWLFiwYMGCBYcgWLBgwYIFCwYLDkGwYMGCBQsWLDgEwYIFCxYsWLDgEAQLFixYsGDBgkMQLFiwYMGCBYMFhyBYsGDBggULFhyCYMGCBQsWLJhI+m2eVNe1PHv2TObzuURR9N1f1b9C1jSNLBYLefr0qcTxt/evwpj+Zgtj+uEtjOn/74xpsGD/SjsE2BA++eST7/5q/hW2L7/8Uj7++ONv/fwwpn+7hTH98BbG9O9+TIMF+1faIUB0APuTf/bPZTKb0jNerday3uRSFqXkeSFFUchicSv5LpfLN5ey225kubiW7WYt+W4rRb6VQRrLLEulLEu5u11KXpRyvVzLrizl8aPHcn5+LpPZTI5OTmQ8HsvZ+bkMBplMxjPJskxOT0/5c/xskKUiUSNNI4xc4KHHcSRpmvLfuJ6yquDGS43cSBxJlia89t1uJ1VZyQbXX1YicSJRFEsELz+JJYljGaaZIB7C8/GIolqiqNEsS5RIXhSy3m5luVjIf/cf//vtGH1b8+f/u783k/FwKHGciCRjkWQg66KSu20pdSNSCd5TP1ecRDIdT2UwzESaSJomlrpupCpxbfr54ijC5XFscM1xVEtVF1KWG9ltK7m93UpTi2TpSJI4lcFoIFmS8jOnUSTpQGQ4wtdEJvOJjCZj+b0f/7EcH1/w3pZFxft8e3Mp48lUHj5+yvfmWFalLFYb3tff/wd/IL/7uz+W66sref7VV/L6zQv50z/5L2W9uhNpNiJNKVLtpKkL2exKWW9zaapUpBxIVTbylz999t5j+j/63/6nko2nHLdYIomaCJOED4xmwzsrEmMg8CxGvrzb/E/Nv3bf8ku09xv+lb4LX4g/iOx1901fnxO2N69wv/h3fIZF4Py3/l6/6rt3EXr31X+mz+1fLL7WnEO1RJJvVvJ/+Z/9R+89pv/J//k/k/F0op8PwbAPKZYDRgDzlyNhkTLnHxanjjbmomAN2VrEf2kT897EDe5RNwL+//oxY0x1kbgUW8l6H5tEKsl4v4oEv8PY2n2tU4lqXEcldVTu39fa5gDXj3At+L2obLyBiuhzan2O/V4f+FkjddXIdr2S/8X/+N3HNFiwf6UdAt90BqOZDIZjqetKFi/fyIvnL2WzXsvi7k5Wq5U8f/FcyqKQpiyBNUq5W0lV7GS5XMpiuZDT2Uzmjx5I2ohkdSNN3XD7wOvv8p3cLe7k5u5Ovvz6azoA09lMsjSV2XQuw8GA0NzxyTEdh7PzMxmNRnJ0NJckSSTL4AjEkqbYmEQ2263keW6bih6W4/GQiznfFVJVlaxWeE7JjaCqG4ngDCQJH+Ns0DoW+JvBIJE0jXWDrSPZ7nIpKpEky/fG6NuaP386nsgwG0oUJ5LXiZR1JHUVCX0ZHO5pxs12MpnSKcgGmaQJHJKSTklV1VJgvHEz65ivm2buHEU86KMolSweSxRjoxvaRmgbeIINF5skttuGnw9beBQlMhzBGRv9f9v7zy45tvRKDN7hI9KWgb2mm002F0fvev//v5DW0geNNEPT9hoAhapKF95o7f2ck5m43ZzpRl5JbCIPWV24QFVmZJjzuG2wXi9wd7dEeajR1C32+2dd034YkRUbxHGCNMsRpwmSvNDvv3n9lb6SKENbtdpk725fIksz9O0Ow9Cg3D2jbRp03YRhcJFWCdx40TlNiiXiYm6pFIMNv4cRIgUtiwI6Bz4x0DliULKAYLHDgoj9lQvidiu50OKCv0tG7Qf87zPw6SweY7deX4HTjoeJHH/+p8mFvrvXY9Cy7/azSlr5OhZhXR5xShp8UnAMXOOgMznwf3zS8ZnndLleKlnXWbOYrK/weCzufLjPF2FkKEc4DojGEcE0IBp6nZeQn0Xfx7NzFCieH8/HWW6k1wuYPE1AmGCKYvRhhDpJMIYBuji1Z2WKgClE1CdKCqazhEAh3l9zn5fxWdblsntu5DlTQjD8+YRgHHXdlBCMI5LYrtN1lHJdX1RC4Nc4QkGAwXS33ePDhwcc9jtsnh41S/vd73+Hoe9RxKw4A6BvgLHDZrvD0/MGcd9jWK/s4e96TMPgHrJRVTs31qpusC9LBeUsz5UQLOcLZFmmB7UsD3qaGRj5YC4Wc9tktTGfNj1l/MNgG5QCHrsA/udCPejWVQjVsRi4cXGv4Gb8kyqOx8eyW8FzOP0bOxDsNFx0AUJW/ux2RMDAZAAYBh47D5DbZQQEMaI4RZQkCgrc1JjAtO5aMDHgPtcz+AUB0sC6CVMQaaOM9BXq+JM01scbevuYqiNdwFE1NzKABBgn6zZYYsFEi8mS/Tffs6pqjFOA/f6ANLVji8MQSZLq8/B6JUmGNHXfkxxZVihhDMbWBUUmXBP6ngmZDkTBVMdzwbJw4ypAV3Fq61ZQdjE0sMpZ96L1EfR3PLsWpk9RwycOOmb/Bx8IfJmsxXLz/AfOgoW793hf238yYJ8lBGef2d+nOhoXsPxr6Uv3tI/xVrH6Xz9VsvoXS3LOj+MzF59ne878R7aER59Gh+rex32PpxHRxKduQDQNCIcBUd/qeKJxUEcg4Oc6Xi93Lx4P017HCgYgDZhkTJgY/KcUQTyh56sHAQYltewMMClghy9CqJSEz7idE13XsyaK/7MCfMBEOACfZOsG+nNoe4E7TIxKyCwx0Ge/4gau60tOCDoGgucNqqrEv/3mX/F//h//B8a+Q982CtSbxweM/YA+jRGHAXIGkihA09TYlaWq9O8eH1WtKbGYgFZVDAPCoEqXWXoUWYUbMmsfOhx2z6gPIcKpx9PDOzx//IB3P36Hm5tbfPXNN5jP5/j666+QZgw8rKhD9BxjNJ2CICvsiBuw9hjblBlYWa2GYYyhqlVx99OAfhwxRLHa6JZIMDBb8sCNZhh6JRAMwk3boumsQ/C5KytixEmi4D2EFhyTMEEWxgCThSTXsXYDExDrVvD/Bh4nkyl+oiS18Yl2WaBl5dUD3dgj6Owix64KYwXFDxSlsTV4mRRxx9UmPSCMrFcQhDFmxRzL+RLr1RK3NyukcYo6a7HdbFAUMyRphojjBp6vONV/L5ZrJXKLxVLJwXy2wMsXr7Wx3qzv9XN9l2PoGzRVg7psEUa9ri2OQfDPtdz/8qXP4z6uGxrYSEQJFoOUtYRbjlzUDTn97rH17/7eV90+oDJgHRvQDB68L1yQ9xU9RwaB63Icy373H+xGHf/KJSj+ta2LoBaBez37MV1X/b/LcFwya8HZ3lttbp8MHN/NJSG8f49Jy+ctVuhR6Ds31hlgkI30Lkwk+bl437G1NSKoawR8nusaU10jZBeLST8/VN8pMZhUvdu1GN1n1PVTh8GCeMpgPk1KNpnYDkmOISmQpClGdoHiGNksw8RxWbbUyE3XKAwwBqr7rfPgEl4/WbBbzAV/913HpL6CLZ/88WMrh3TJkB4hy9ev67q+5IRgxG63x267wR//8Af8yz//N22yWRyirmvsnp/UpkTKFnWIeD5TG7lrWxyqSpvyu+cNIs75VRUHGFit8ztHDEOvYMCf0yxcbboBVV0pK+cclG3z58ePeP9uhfuXL9H2nbAFt7c3bvOzjY+VO/EN4cgRADDyf1xlyASBGwA3lTAc1f73nQAePiuyXv/m5pdq9Vrw1H42DMekgInEJSvJIgVnbq2qpoJRm59PCIJkpvfkOfTJiDoWDG76ChFwTKLjtORqHO08oteAVFVcomo/QqIECYjixI1HYgWLaeiAkZ9lwDh1CjR5PtOoYrmYY7WcIwpipHGH+Wyuap+jAiZJ/Ioj4jpSLJcrjXqYTMRRrMTh9vYOfdditbrVz45DhmFo8fjA5HCLcOJx8Fy7Ku6yfOBYSXt8iRKfgAHFgk0S2vtoSqEL7Gf//vftmp+/Fs+1/1t2MGycYHP9E8JcL3iagbt1DNFnAdsnAj4J8GOL89/jZfbYgeM44NivsN/neTv9vDtxPpPgsbnv/vg/d/E5UcveJQO+O8A7V6OBiQnliKBjV3DAuN9bMrA/6M/sCIYV8Su8L1vrBIWNkgJddwexMNiBvSY7DCk7C/wzq36eo3SOIJ0R6IJpvgDYNVstEKQpsA6BdESXJOgiYoCsB6Qz7qp+u0SWPB+7Abr2dl79tfLXSB0au6zHZhA7WzwJ14Tgur7ohGCzP2DzvFWFyDEA58jcZOvQAhaDFQN4yYfYPUR1mqBue/XuuAFX/QDWpikDER+qiBshW80JMlbKcXJMCrKUD/WEiSA6B/TRI9s1KLfPbhY8YXN3pyp2tVrh9es3yItCmwdb2Xxtvje/sz142uz5QDMy+g3djRnYDuTPusBg3QQrDRQI3HyWS8HwbEP+nEXcQsjPHcaWEIQjot5an2xjErCp8YTOrQsHTE543njcLjGwF7NeKAO/At1k1Tbn6JNv0WNQNTfCRjRRmOg8MFFIogRZmmM2T7BczvHixWuslktV9Xxvft40AdbrtVDVAjSqWmXnhGOfDpvNRviNLJsJV8D6XCDQNFXC0MQx9lWnrlFD/AZfQ19Wfquiv7S9rREHwwhPD2tYAiYtEeDfZhNnxKzWT0Ic/G8bxtj9oT+dIACuaA9cu/g0TvBDB6uTT2A7/kEJph/H+GUxSedcn/Y4HvDdCA8mPI10lCH5bMV1IjhS4vVUW9tjEs4gkToCdxy8ny+lxVlXwIInj1kz/YmVPJghY6ordQuH/RYjE+XdDn1dYyhLDOUBAfcEjYtGhENnZyviqMsdt14vQETAKzELGjtMyPoOoTuPStqTGmNaMpNGWPB7jGi/A5gQdBMwm2NcR5iILwgC9G4f8ufziLc4jgXc+f5kSmTJpDulNsnyoxt/L/DwrxnBdX3JCcH7j894fPiI56cnfHh4xNPTM4JxULvXI2/5vW47PUFl0wnsV/U9gigCIUX7rhM2OMxzmznHsUByRNoXOdvjNqtkgFrkmbLwmGX7OKI8HAQUPNQlnp9rPH98wB//8HuxEbIksu95hjuBA9nyTk6AQQYtlxBwns4VMtiHDKA2omC3syNSkAmBEODsQLIlbijy0bfp3Uw3TlgZ/1Wn8E9WkCYI8wxTRLDgiLQfMTTWJu07fub6CGTSBWNrlB2OiJgG1xk47vXc/C1pUNLigWbj6doQt8EuQKDOBgOSdQhWizmKfI7Fco43b+6VYH37zd9hPjfQIzdDXqcoi/H61SvM5iscDhXef3iwjkk36NwdykodH2EiwgCrxQqvXrwUOyTPC9R1hbJshD2p6wZdz86QJQRWdRrY8ZKVxpExMY4JQaQ5djLx3hsxF3fDmvodN3/HPuA6zpaPFbydV78swLukwJ16JhLnycORzcAuhM73EdPnr5La34bRsATId6h43Y6VvusY/HSE4scDXMPggW02QvBBy/7OStrwZ0gIIj0nlnAwuRIuhX/P4+bYcLvFUNcof3yHrip1fck0IsOobSoXbQdLnvidz09sz5jHdcRTgGQECjIQmNBPI4q2QTiMGBsbQQxxipFUGIJt0wJhHCGaFQiyFGhbTMuVkoNwnqMPYyXads78J/Fdmk8TAiY3dqmY+POcW0J3nDTwGWTy7XAfvL0vrAWu67r+w62/KpqVZYXdvhSQjOh8vwV6xO5xJqr2PFH7DAoExAVIQrY37e3UzWbFCwYZNzJgO7zrDD8gIB07j702Mv4MK+Ixy7RRCP3LIOQwCH1TY0NsArsFT0/GEsjnArRNAtNZi8/qONus9d2Br/zmrSpIrUDXytUIwnUBuClxV3EbMTdZof5Jf7xkRSnYMxnGCE3P8cWAupsMbMcg7oONh1sfUV2nl7Czfpoc+6rQUoOzfyNi0KPS3UuMxw2SnQYG0gLr9R1WqwUWi7VYBhwN8PP2PNfdiK7r0bU2vrC2K8GKmY6r5mhjZIcn0HVjgtIzYPSdobe1M7tzyFFDHCs8T0Q5eiCgm8V/7lKg/eQlDKHOqpZXK9W5sQDkJxQehGg4c9/eP/3++Wvpt9yI4QjuO5vzK4FzuBgDpBrbwV8dHwE9mO5Ebz3hAKw1f+r8WAJwxlrwIDyH1Dcg3E8TGP9gXr7Cn9x6PL96Bsl0qStUZJ1UNaryoM5A1TSGsxl6YVpcCLZj10hw0nVgYmH/R1AsU1X73K06KAQO9jq1Smg7YnxIJwYCUm3FGA6RsChpU2CzteSX44r5ChPvSY3jjsSRs67L2fH4kcDZqMCPRo73gkCojkrpwJoaoVzXdf0nWn9VNPvh3XtRDckqOJQ1oih1c1PbUv0mzPm06G+kohHkN43K9hVwlRCM2jgY7JNgjjCZ0FYTxrYxUGCWoQ9DtGOHKYpUrbLqm7MKADUQchzySq3pzWGPqTzgt//9/0KxWGhc8OLVa7z9+mvc3t8jTjIkWWEMgdBAZD6EstpTW9dV4OLyE//AqkMgNLbIE1Xl3VBLs0DVBP8tiTELcnH8L1lRukI1pmj6AJsdKZq1AIQt5//cEokjOKsk/TLon40DjFrnWgWiZ7EqNqqVAgyTIXK+1Xrm8fr2Mqs7Q/ezmR5GKVarO/zqV/+E1XKBr96+tusBm/kemi32hxK7fYXnzUEg0I4joDjBze2dvj9tHnVdilmG5XKmkcHhsMOh3KFpa3REmjM4xyHyIkM/zDEeDiibxmFGfKV2wTkVIt0negSFcBQzIY+APAyw1DghQDOEaFgJajxDECbvB6Ob6RDOkfA+ovgxwRm2wGEKNTbhD/K8CPQ5MCFgNWzjHd/MUUcgcvNsItxdYOfPe9aHqHYO1S86HE+MymkXQDUOYrLFz2fvz2SWyTDvFb7vp/S5y4IXc2Ky7BgY/fntugb1Zot6u8PHP/xeiUC72WLoOtRDq2S9mQY0TI7IguGLSCfDxiWLIELCpEDqBSHCYULEez8MMCO1cBqQdRz1tGhaJhwt6qBDEzSGn2HHKggw5+iLOh3sRM7niJIZewwYl7cYb3MVHKQnWsfGUTh1KOcJlKWG4px4UKfrAPBeUF53liAS69NdCna5ruv6W04IGj7wbAEKWT8pKKp6gVWKg6tW2MoWiIv0Q2IFHDXKZv4uMfikjcnN0LQL+ICPBB1yg+TPeKEQ0uCcoEkSJ8hTEx1i5akHvuvQVTX2243Agsv1SslBVrBJwfYxOxXu7RzgTDPws3Y8N5mYx63jdy3dP5nx+pdwQMML0dv9EKAdJjT9hLYb0YqCZ4HptGl5qtkZUfu8Aj77O1+1nsncOI63B6z9pMQ88utNeyBKUuTFTGDAPCuUHLV1o/NcVZVwI4d9Ld0JJVSs3CLrRNj5MmEnfUUEEA6oa27oFCGyDZRsEL5rMZupiqYwVVhWjnN2NvT97PVpZXwU9nGV3Qk86DADvvr+5GTan70GgOek6x5096E/wwzYJz6jf83zs/0THIF/7fNjPAcLeobAMfmwatTT8kxo6SfX/d9tqhgq7tKE4Kji495QGAxW/+wEEA9CPYm2QTd06uy144iOzydBuHzWAmBwEZbJDO8V0vjYwQui1PaJYUIw2P7AfyNGgpTCcAzRsmuAANUEVBrs84UoTWSNr5gjIbIZOBo8lAh3JYYwR581KirgMCW8H5X8kzap8+oAqAIUu3JBokrnJ9jdO2fn+dohuC586QnBx/ffoT5sMLZ75HmMuxd3GNsWXVNhYMCoOSuEgGl8wNPU2s3Gzx4Nmd824rSzJc2ZNNm/fLj4GnXXY8xz8ZSVGEQB+jjCPk7QsFJ3gTpPEixuZ0LLE2DfNC3ePTyJpfCHf/4X/PD7P+Kw3eLpq7e4ub3H/SsCDWe4uXt5AuNx3NAPEinqXTXOajgvqOAXIhUKnwkJldTOgqef9QqDwIrssvb29x9KHLoANRMBjSWYZLHLYgmKxwl82h1wM2MflFyr+riFjacWsrXhTzNtl/qcRhAu0QhYYRH8lxdYrdbqytzcrHVtfvvjH/D89Ijf/Pb3+PHdB4FEy6pDmufqDDApqPYlemoQjAGyyACEBBM+PR3ww3ffSdFxGDqkaYy//9Wv1XV5/+EdNttn/PG779FwFNGMaMrLWBv6+AKhqcFr10vt+0k0tB7UbWgMcBbH6PldQlAB+olBzGdivEdIo6OC4oCusVFINp8jKQoX+8mOGVC1jc5pRoCm5t/UZAjU3iabwicU4uC7TpRm1Hou7L1YPCeRC1ruKpHLbxRG9sZHBUUet66/15xwI6sTQcE6XqJXRpyDc9plz98laxpaDD3b746hMUHJ98d3P6AuSzxWOxsNRRQD4kjQzql9EZpvHTcljoMxPtIsQhoH2keWN2sF67HvkfQdkrpE0LXsTaFrJzznEaohxdMU4Fm9rgFD0COZRtwNA9IxxN12j6weMPzmewyPHQ43G+zuK0R5hmy1NvCtOka8xlIdsGecXzrXg/YXUqX1nWJnGlm6AsGdaFOu6JHgsu7gdV3X33aHoDpgpNjQ1OmhybKUvQFR1tTS9Nmzo+h51T/j7bD97aRLg1ABQ0jwvrHWpksYKGzETUGUKcqaUpqUrUBl8dauzxlskkTV6yyPUcU1np+3BBSg2u0xHSpsn5+RzwvESYrZYqU2Ot/nyChwyGyb91qA9YqHqm4FxvrTzsCfnyt//qrqHoyB7Az42b9VIk4ExlWjx3c8o8ed6k+XDBy7A24Of6J2n47/+JtnXHv3vgKgRZHGPNwM2YnhTKGpGwE6t5stNk8b1N0gueFZP2I2X+rcMmAyvtlM2/ZcBiImgPv9TjoVDJ78XLP5HHmWoawr/UyePyMmWIzX2RroF51TH7Q+AY5JfMYSBWEf1EYedU/6kYGnE55NkgWa5TkYOwZES1iPZ9CdTGpXWDIW63VtOGDAQQldMYGUzoNOipuy2XjnpOxzEtbS3wjoZnoGNuga0TuJQGM5eOS/7yWcDcG9qA/fX12Qn+FeFUjV4T+cyujAgC1F0MbwJUyoWN2T5itxqxCTklOTHA41tiKTwM4Nu3HEBDEJX6xXOr8sLKK2QTQ1uicHIhjZRYsi1FGEkmJYU6g2PvEF6QSNI9lzWvSDAnW3r9BjjwMyPEdzxMWAWVSYEikTAp0t3gOjAWWV+FNIiQkBMMYGMhbzxAkwCQ/lRz76zp+/jgyu6wtOCOrnj8izEEUaYteyvulRtZWqPKL/qVbIlSalwGJ3t3fisfvWqzYNsRLsgWNAT7KFoy4maEPTKpgoXBJFpoBHCl3bHYWMuAaqGe73mOUpbpczbU63y6Xm2Y9lIwW/ZrfH5v2D5o7VrsRyfSsFPormzFYrdTD4clEcIQ/oj0BhnRh5kdr27DZW0ik1F2ZA0IyXGzorr0mUwIaMigvWrukxBoYTsKDs1eBcwFYF+efyj7Mk4RPe9PmowP2bF31RL9am6/ZzpmNg7Vsi7gd0Y4eu5yYfoS63GPsWHz/8iHc//qhKUNiLfrDxUdvJu4Dnbve0l9ATzyevHccAP7x/j932GQ8ffhT4jNrvAnwWc7T5gPLAMQQrWnZkciDp0ScnsZ3PXUeAnhDjpjRHDY2Kn6+rgN0HwxDM2CXIMUSZsBp2niwhYMgJxg5xu8PYNKjfmUx3336DdnyFLM5QpLm6R0ctfrWjBwEux35EWiRYUDSnPQDtM+qqxod3H5QTLF+/kd9CulggzgsXaCy0x8KGDAibnTj9iKlJQbxNTK1shxlxv+HGbibN65KCI03QknNeD/lEXLDmUYS51w4hkHBihRwI5Z/mGW5e3ivR2h0oad0jn6dIiPDnh+1HdeLKA++fyTA6UYCbmzmKIse3v/w7vP3l32Ga+Lotxs0j2n+p0B9GHJ4ztFOAbRpjNw3YTSNKyTI3QM/PPuKQZxovjim7hpklLIcKz8MG/1aFiGdLFG0qRtM6MdE0dhemgKwHS1IZ3MlmUtcxdXidiHocAWY5NTYizLII85w+CT3ivkY0sDN0Xdf1n2f9VbtEW+2xTOcyKapJQ3JKghwVcIZYlqV5BSTsINCDYCG/AWtXe765cYolDxAFyLNUWTqNAUgv0gbIaspV6aqy1EpnJ8I2PfoLcLcbhxyz1BQEC7YFxwl7+hRQlKipUbJNzbFA2wsZv7i5Q97OEUlW19G/RFGUXI1GGZ41YLLKpP71amWyuvVzZFbtPBYCukRTvGCRWRDxWLye/k9EcaycdD98rpp3Pvb2/3z2Yyf1O58t+HbB2Wu6mb+CClvnxG2MgzE+hhZdW0lQaL/dYrfZqAvAc8aWPM8Lf6budjZeGULklCnmyCeOBSr7uGUQ3GO7eTxWmOw6VBW7TJF0CIibYLVMYSPmKkzUThSxz1tHyKRHlTuTKM61iTWJyp06Vl24xpBEmHLq459+32rDUWI7Ud8g6Er0249odjsE8yWmYoEop0JkrtPKhMApVqm50VeNlDKzZIlUtDf+W4Wu2mH/4QexR9JipsAuwGt+tEdSmGeqqkDV19KYnoICU5C4ZoJLBPy9oUrWQQXOEPKekGIo/hC9Rz5+5sqiAIVGfJzl9+hGAxdK2podJXYLhx41RYfaAPPZDAVR/j0/x4gyLNFRdpxPWkQ9oUDeIvPFTHTh12+/4hXCFLRosgDb72I0XYRdEKPDhJqmXylQT73eP6BWB5U4gxEkGEg0LM4QBKklBEOLcqjxUO8QtQGKtJYfSpSHSCNqoowaIXk2CcczYitQmiTl5uSVCgNRY/OUAMYUcyYL/LmxQ8hx0HVd1xfbIagq1EmIkU/0OCGVZ4HpfRsq2rvvMYiSQlcjrVMT36FCHYO9YxmwihIeQGp3IWazAgvqELBNOg6mTUCxEQEGTXg/kP6uqQfKtEXuegf9bJ6xlRuI4pgiVhuz3Q5IRX3iXDPA7OEBxbxGTMBcMWmDkJeAdNqNwuTV5AzZfXKc8xuwiZEEkmhl14KV4CVLWgg/QYSdOgSfjis+/Zk/D3b89372/M/nwjfazONY53+5WOhcPG+eMLQVioCtciZ8Ju3M4MOEiVREnk92RzqOaIJRLIK2rRC3mbwM+t2EnlLMHUFne11zts2Z5O22FZI4k2YBaYqHw1aARVIazzn2n7uO549cd6cU4KmIwUDxK6tcg/qAkMJPHF2lsdOsd3RTUinjCDdzakQM2FIxB52qwrCvMBxalOVOct57YgimCQXZHdOEDz9+wGF3wNe//AZF9A2SdoOs3mBodkirZ0z9hG5HYS0gWy5tjKVxi9ovCIYD0DUYdh+VEISrl1L21GCCCZwElRzI8OjM50Yd5/eST4hc4LtkzRcFFvNC770n3bBpkQasmq0Vn87IuDEmBEHHd4sVZiwGmCT1PdLdDs3Uakx4/+JOeiEv7+4xm81w++IFcnaNmgl1vVcHcCxrjFUt50t5a4QxRtFYfW+ESWgiKqmMDhzrgNc56SmlPJLMqyS0azvU2wOypMNs4J4TS9EQlOl2NFdhTDi24ynmPsWtQIky8RItyppFApVPO2Rhj1V08ji4ruv6QjEEFao4wJhQ8CNQQkCuuU8IGJgHDAoUEiiqaykQ2kw6OQZZtds70s+AIRsxxZAfwYyCIpqdkiZG5bxY1fl+t1WSYej/0GxKWb3TRnlvmwy3R7EbYrafQ+zIhT70SNsGedei4wYxW2G+rDFf32lz5fF6W2HjzPuK0jMbTqqEVnG5atrN3NWJuNDciMHTSyL/OaT4T5kNf26dB/mf/r19tz/zM50nEfwjrw+7ODz/y6UlBE9Pj2irDBlrs4EAQksITG8gEQAxylLJUe+rg0Yp+3KrSpQsBfogbKs9dgSGsdLl65CGR2yIFN7eSyyIDR/zMyJLhT/DTdcQ35esI1/e60iw1e7HMUOiFjzfL6j25pA351ircOffgg4TvyyMcBdlCKIBzzHvBbo2MkhVGJoWh7JB0/XYlpVx5ZnAjhN++O//io8fPiIPOry6mSMdNpg1zxjqHdLyGQPZJBuCYCcsX71211/efBhJY233mJoS7fZBzqFJvkSUkUpnCbGh4C3IH1X0jpx6Z9R01Jdw/LkL592r1QzLvBCivytr1CM9N0gFLtQdWNzfGAYjoj5Qixc3N1jMZsKFMKmMZyl23QF5nuGbX/9S99uL9T1m+Qzru9coigXGvsZQtxiqBkNZYSgbBWkCE5UQcJTkaI8CTfaUF5cut1McNQIjhc+ingwEZlnEsXSyWyduaRWkGFJTrAxTJzgkd2ViIkyFSNREVTjGk66aViOksY91/yyzCfP55SZc13Vdf9MJwf5wQJ6w0h8RMpBSNc+ZBLGimqg6F7LlbA/LT7nzwVHMJ5HxDdvHrC5Ma500RfKSA6Qhp5NGJ+IjRxVDJiHG5TbYmQ/W1O0XCtgZFvnRhAVyVqidZr80D0oeHzTamC2XmB0WmPp7KfFxjkntA0/fOm6uUgaMTezEKcjJZtXTzSQf+/MAi04ys/6d/7Jk4H/0715b4VM/9z+/ibHF++rlC9wu57i7uVFSxUgjbr2uD+WQrerkWIFGS+zQeNtYOw10f3QgMiHgU9kcc6zCxK6rbebKypuULa/3x86B2fUyGTjT5P/MZbNzB9TzAjOuw85KO8jmQNACh535PbQHBC1ZJSkQ8V52dFNRU9mxsuDrNfsjGjExgZkadVKax0e970x+DwGyqcEsGhC1B3S7RwxBiTAYkQQT5lS+ZOckpi7GyUGQYAQxEPhdYEez49a7MuElnebo3GlCOabBf7wD3H1wGhOdXBM+QcN+1irZTVFbbkJ52KM67JUQc1zEZ4TPIHEo9LFIySahymhMSetOuCH2Mfh8R2KyZMiKHPlsJr8M4U740jQL2+/RHQ5KuJhI8Pd95yZkvyfopQHAzcHwEQ54q+eSAEyeE/ofBEimDBk7BByDkQHBcUfbKhlIU+oeGP3YmDgGvvTy5aa2aSwQr0ZJ0bAdr8sIrBJ2yK6gwuv6kqWLP3zQw8E2X56nNpMLIqwWC831mRRo5s6xAbXLXTvcZt7c6Cjok+v3aH5DyhRFjMAqLiHAMMYsYQaeCbjWVjQ14myaPuzci6zF573l2SGQbXLERIOe6EBPOVzO/J3mQVOX0lWP9wfsy1q0urIqhXRvf/V3MkW6vbtHHN8cCd1H9LbrUnCzoqAOnQiPND6JnBCxfunA+y8L+n/Jv/25YP9pt2P6059zQ/YX93f4L//4j1jNC7y+XQpMeHj8oOsp6+c4MQ63XBd53is0HdHlPYbJkkC9rJgClDhOdY6rkvLLnRIzYjq4isIwAyrvmMQpISBmg9iM0JKQC5booA4lLhEdOl0yAWGgTzIEy1cI6hLT+3eYiCfYczQ1IJ7dIJ1bYqgRg4z5KPPMefGImN2Nic6MDYKxRDTu0B+esf/dvyoCr796KTOvJfZIsg5J9RGHHyJk/LzrEEUQ4OU8QzMCuzxGl1L62qpQm8fTQGiQTfgYj6hTggEDTFmKMePojUnxJ7YGzgzo05GQ/eGMceC0GC9Zz08PqOM9xm7A9nGL3fMBWZZjVrB7QdBdIvWieP7CmA0cycjWfELd1aJ7Jtwzigyz5QLz5RLL2xszwSIQECO6ssT+3Xv0Hx/RHUoxGMQ8oj5B2CNmUi+RI3YiRkwJpcfNu0KJH4GYUy+AYDhOKJBgjoVcP/u+1p6yTzL0XYI5BcuY5Lozw5+XMyIxSryXcRR9cGwPTnMIjB3RzEPM4hhdfaGC1nVd13+w9VdDj/280rfWZYxDxzE2h1PSdkIkVJ07s6H1aHY/hxfjYGAL2kBJav93DBoReoIFR7Z0XXtbm6Xj4h+DmgNgidpor+vR5AL69VaRqmwVJ9uqUNo08xPs2a5tG2xulgJGWpVmVra0Q+brklIp4SE3lzx/3yNIkonHpbJ6f3J2/+fdgZ/+23kX4H/2Oz9NGuTP4ACUpi8fCSPQNwEIAbO26FGf10xhcOqY8DoygKuqcvbSujasbXnNHQrecBlO3pfgPrW9z/jznjUX/Fzn0f/JMQ5Ev3NGP0wek14V7MRKva0wHUJnN50pQaUQxEQdASorctTh7mc5cVIRUPflhISVvzPMzWIgTwOs5gmKaMI8C5AGHeKAyQ9HKcTG2MiLwTLgM8Bu25HGRrDa4P5+UBImkgmFuuTQ6ZghZwo5AiMez9k5fsDOrzMlPskgf+ZqDgdMIamXo7BExIbwuSdbQM/HwNGQOSLqGvKciWHA7wQWEhvRy/q4ryt0lKzuWowJGQKtnvOhMTMkuiTqd3iOnP8BEzGCLZmUOTUEAUXNusocPeWKyPMIJga8LiNS/Y4BWoVd6g3HQv8Ndgi8FrPtH6ZZYfRTp8DqbBglo0wVRdqCdRwjcD+5XDPjuq7rbzYhIL2MM29VdaJ0GShtGS0FIiSIiJ0CBoqGrTnORp12ANvHavFTxayb8LzZ6t+LjKODCOHA9muGjsBC4gXkwOdAQk721WfzkoWlqmFkIDceR0Od82HCbs+5rnNTY1fC8eqVLJRbdAfg8PSgQF9tP2C5XuPm7h4rSe9mSHN2P3K8evVKGx6pjUebYH4eF2QYRGnOQ47+z7/+XDXnuxPOjOXISvgUXOj/7U9++9/pHrDDws9SlnuUhx3uVjN1C0gx/Pjjd1b5OyKed6Yzmd8RdddiW5WWFDmVQdH1iCXIMuQ8b06TQswMd3zcjMkiJwfd2xMLwOVplpT1veTsaXM3AyPKEbOxw8SR3Z6QokGkwsYZBiaESYfm6T36HytE928Q3R+AJMc4W2qeTyEuNNXRLpvPQExPjaFGHI9oE+A2N7ng13cFZvMC6X2uICQqYzRIwGnMlpimGBm7ZgQO1qVm1mHbIh4mJGOPdKxFUez3G6PnzW40xgjzBQJ2NsjjV04lG04nguSNt04eDPblEjElcZFEmC5Zj7/7rTEzeibBTNrpYhkjzXhfjBhqpo+8vhynjMI+MCEPagKLK/SknO5J4azx+Nvfopqxdo8QrBt1COi6WT+8Q/3DjxrlRG0rKeJ4ahTYi6mTxHE8Ek5IEGCATloh1CEIFfipipIyQePPjj3mQYtV0KMWsoC05QGHmvLZPaKCGhHMASnKxfNK1U2TelYHQvoq9tjZ4MaShl6CFR2SsUbflBed0+u6rv9o66/aJY7gJ+n/O9c/LZPxVcLAAM7sXxXDyTKY69zRTRL6asnbhmaGOSG6KLKkQsmADYRZaKjC8a50PrF3FbzRjMZPQIum8EYdf5sB6jedbCnbkAxQ1X5nrUYyIJTcZMiyGg0lj93mP7Q5kiSS0iHNkuxVDaDHz2jB7edc/6NgeMIBeOlkE1OiZbRjR5wBB/05Pz//n4IKneUzP0dv5lJyNXRg0WM36BN+o/99AwTaOTDJajvL9udPjHh9EftJG9tfxzOzqU80GH6+9RNTYAMYsuKmPXaXqgpFWwF1CVQ7UWkH3uscjXBs1ZqPhTFoiKoj2tzuBCnuMaEl0j6JkKUR5sRPkA5H90xW/JRn7giqZELMwEODqAYtbcOrSi6B4UhXS75XIxEkAuhCjglIbWQSzsBPkR/XhVGa5gWsjvRSd53PMCliJTiWxSWLlEGOykjBQ0DNBo4z6GrK+19OZNaV8H/X9eoahFQm7TuNQWKOERlwqefA1vzhgC7JMIYdojBGX5WY6GrIZICFBBN6Vf2TkgIDXloyQqqnWv1ToOSLGxk7Nvp52qfr7/jlsBmuQCD+paMqJbta2lgomCSwyFGYyr6fi3iZ34RTr9I5qJsObXPtEFzXF5wQcE85sJKsAocjyLW5MbPO0gS3q6VmndS5J8iItsfUOveiROwOUOBHNMOikFbBwOBDo5CWmTxbqfZlbVmrfjnFl0wrNQvi2KkUkjJoKHIG5fFQI+wHdSn43qxwyZv3GgJC1AvEFeKmWBrugOOF7Q7PZY3Nu/fSWqfWGelLcZ7pvW7WSyGjf/2P/4Q3b9+qpRxlucBJrK6p0f+zrT+jLXD8J8dC8AHeBH4oM7zC27dvFdB//PFHHRPPv7Qa/gJ8gddWoA49wWJ1RSdLAgGpST84dT/XepYRkHULOFqhgNBAyhaFcthm189Ygse2+FFC2G2wai2Tty4gGummBgRlqcat/hOz+QsWsSlMSr0EsHdP1CSdwbmtrEpf3uo6h2WNjKOPZoP6+0d0QYIymony1u5KjC3vkz2GtsewHTDMnnGfT3gzo95diOX9jebYBMtx5DKPR8xCYLsr0ewrPDY7fHd4VuerqPe6Vv/2+BH7bsKuBm7fPyALWszCVtOKOGGHZYbidoGoWFpSwH/wlsf+mkjEz9I1UXo9q8CNNvhlzJgIIxWfLlj9u/fqjAj+RzAwmRplhMNHO9fzjB2MiZaoGhPQ+ZCJTepkk8n0yampMPaInh4R7RIcEKJjt8ZhSbr3HxBvHhF2LbKRLf8Jq5hJFZBz1DBOKGmMRuwOLbcmMoRC5GEKNipm4YCMSYGom53GNbOwQyb1Tw0DREFmTtM0nYqBZIrocqKCYmIS47Q4dL863QUn96D9i0niQEQEkxrZOl/XdX2pCQFbsKxe2K5XVWQPkiiBTvObFTkrVsnDEhfgqnHNCJkQUJbUqRB6VoBkUAXQA7qwRyOFMLdP6FdtdkpQWKJgGCN13QGxAMheiKy6NWGhSbrv4r4TlBVYe1p2uyFFSjj7jNT2ZtCTec8waeOhpwC3jp7vG0W4v1uLo//ixQus12skBSvCxFXGlFr9fwJYdCYqdLbOZZPluBjHSgru7+8VwKneyL8np/+nDIN/b51oluwScGZOTEB/tO71Vr6+YnJ2O0epYy+M662l/fFbB+FPNQXU9FEiZ8Hq+JeeBvAzLDPO8tLNnrlhLV/53ms2PSJKWX0HUgpEmqEuNxjKDbopQj3uWeRiv2lFExzKVpz6pgvRlT2SVYJbzr/pgVEwEXCa+ExgQyCN+F4dxrZCuW/w7kOlqvdmMmnuzeMW27pDtn6nY83jHkPcKegWqyXiaMSM44I4U/LlP8QnvRfPHPEJnySavZevc1h0JmJ/boz016zxYO1xeYEY+1FaCX21R+SwBKzE0dR67pkYkDXAy6D5Pjn8wp1MGAnGizp0z08Yea9ynMQX3T4jbKgAyFGBVeWpOmFKNeV+yK4DwYGRUhOmphHyMUKmvcE6AuoOEljIDoHrFHjHKTGQZMlNcS3iNKi2yL3CvTZxMbIAtfPrk1idA+0nA7qpl5qq5Lqv67q+1ISAGw+DDStIqtFlWaYHxoI7JF3MwMJ5dN20Ah9Vkvd1ErkMBKoIuW80GCIigicBjDAQSW2zfvLY2bJm+1VzUg+cY+UxjigJ7lGGb5swK82MfPokQZrF2vBnWYK6muPQ1NiWB7VuiRJmNVOkhdqo1EmgwZECm3TfqWIWoe17lNudYQXqrQCGL+/v1LG4efkGd0kuXj75zfz6f6RB8Im+jCVNmt87xoA6H/2ga/D1269shv3qldQi/9f/7X/FO3Y8jva7Nu45t3r1zD6ejziJsFwulfSQglkdShz2ezw+PSvJ4GfVIIDXRa9Fyh2TOW8dLNV3nT9T7fNJg7/uBogTndBR+bwtsOOQOpqoUR0v7RAo9DmqoD+ZJ38DfXrnXkdMTITg9i2CfIls9x7xLkFYtqg/HtDse3z/+4P8Jqg8yASzuB2RrUYcghgfIxp1Rchmhe6NshqVXKZpjTHqcNhXaMoG20e+Djn1wI/yzxjxwO4BA9t3P0gB9Pa2QPpygWg2R3L3tXAOY0S4Ik+YXUNdP+JrJwbfo6/iT9gENtJS14yJDzsy7KL1l92nQV1hPs8MkyPdjgDx0CI9bJDy+atLSwiYOJAlUTd6Xg3wZ06oM6ku8vYgHShEsI+odoaONEzSLasK8diq3Z8wc3Ofk539lsVF0yIbRhTEhaBHG/QySqLJkLqFzhEypIYWYnUWOWog0JA4B6YIljxRK8GxEfi+fBOBPAlGZIfBWBn8PY3mHDCZoyV2OEg5VcJ89TK4rv9k669GGjGIVqLtLRRwqEegqpwa4+VBHQH+DEGFDW1RZZVs25UU8ViNycGsFYZAM0X5mtvrsw0dx5z1Q0Fe8rps9UtO2Ob2DIQNfdAcRJ1GOcv7wjbngI1GU1Fs8g7j8zOe99SEt5mhsAb8zhkvFcza1uhcPAYGR9KJuhE1Ndk58ijZIY3w4d0PCpZRWmB1/8bJFlO+92ecI54Uhf9keU/7c6wAzwXBmgzkHN/w2MvqgH/913/Bx48fnWmUxxv8FAdgwVLaD3EiHYKb9VqaEvQpYGKx3e1w2B8EJPMGSBbkTVxXHHlRMA0nb3hve30fivW3+r1TN0idG/3ZrG6PZkY/QzKgd2dSaYpHp/Pn1C2trc7Kz5sRxcDqBcL5WkI1SdxgnLaI+2eMVYOHdztsD71sffl7L6MEN0mEKoywjUIU8xzzWxo8BajLVkFlRrnjiAlxg65ucdhVePhxJwMr0ioNBMugMiJOPmKoN0iCe9zd0lMhQrx+iSgvMHGkps/jgpQzKGBCoBn6Ed9xOm9+Bi7IjOh0LNFZGV94n1LgiyZCFGwimJcwDLbmK/M0SKtSwTSoKpNwJrBXOCID5bm01il8WXdpKA/CCiEcZDKVNB1ySgITz+NuVxYDvG70xuibBskIZAQI03sjYPeRe4ipnsqlQ7/HAoUalcQXOOtoz45xTpPsDrBDSAllnhrhCGSNPR4TAmGPjuZQ/LfhLCmwsc11XdeXK11MdzoB81gEsEI2bfuMSmGScjcanpTmvOSvYwV4FzwJgygBONHR9HpUL1Q7sUHdd6r2ubOzYo+zxNqxZ2wDp9tqwbELFMAkfEJZZW4iLCsmA8gx2JnfuQEdS9KaAmBfMZeSB+cAAGJvSURBVGHpECaJEhuam1DPnO3D5axQEOZhMNHgzLnab7F5fkLy7gOetzs8Pj7icNhfdAEsUJ2EkFRJn0nQGmlMoHJ3LrkXuTa4OOiTNk2er/v7W7TtHF+9foXqsFOFr66NA1PhJy6AfEV6FXBz+/jxEb//3R+xmOXYPa30ud59eFZy18u5jhr0NveXZLMDFZ5cAS0SC/h2GiyYi2RIMR4i/JlImKCM+/RnHPmzc3BhUqBEVZSxE8ju/H48nXvrDCFKxQiIizWisUYx5bhrRgRFjTePEeb7VueB98+rFwXuXmQoshCLwuipCYmHw4SnPTEyHaasRRk16Cr+N9ERxBckCChZPDgb3cy8BRb3Kyxv51i/fYX1198gWd0BaY4pSsxFUZRHA8d6uuFJqtjjI1yixet8LrnNoCUb8UjP3SVrf9hhaFcIySwYQsx4PAQJUqeBgdSxCpgIsKMh0KVGM0ax5L2h9EXJKfUDQgVr0jvpL9CzdU9jId79THY5v+PzTXfVcUDWDchGMgoMU9AHEzoqJRHUO5kbKmXIPaslZEJAYKab/xs942T+pMRQRpYDSQOWODuqsg/4ZCXw/rRnkPc8uy69Gy90AoZe13V9sQnBjipiAt6YwRDHB6r+ZhYwmHVLvc7bpDqaGkVnDKU9nlDsCgrGCKCpTTtM0njv+0p+9fOcVCSb989izv79eIDob2Mq8MEk0IcP7m7PWSDdCnNhDJic8L0IQCTwzhvykHK02e70vew6iZAQAR0RiEamhN4rwt1qYchxArwjSt522G8+Yowy+bJv9we8+/G9RI4uWQz+x+Ldz7qdWqLhJ6yK5lzatPjN4/4oRcxAjRCLLMMvv36rrsDf//IbTH2taqatSrR9h1YW1aYf4N+Mr0u6ZteH+PHHd0gCM4mi0FTbtHj/zoSJYipRhhHo/JslRjl0jCwf9hXXbd9lZWUVrAiFASmbnDGTg0/5WXLVPVXS87ztfrF492nQ/qxzKh0KdoHOGQY/tQH2IJVQwZffKWBEcGxW7LHIExTbGo91hv2+weF5h75t8ebrGe5fZGqZkyETBTnSgPfEgPebTklmnVVYxI1D1hvwsJhliJxWM4M4E10GxpuvvsLtq3vc//JrvPj132FKcgz53AK52vx83qjj78yJXCZ4RAQ4oyuT3CAWh8Jg9mwwUOpTh7HYEpesp80TfvGSIExqLIRYMkFvO8SUGe46tPu9iUsdeTjOc4AHJiYO73Pey8S+JPrsCTUYaGpFz4MkxcjJISF7o+F6xFLoaokcFV2v7gAppPxi4tMSj8FxBDsvEp0qbMxACWOaQfGa+uvuugTHZpS1iyS0NFBxkMeiYodvwmeFI0YCovl3vA42fmOnhc8Qu5TDNSG4ri97ZHDWsnaUOyHRZbDihH2I2iUKmWZCYgYMqPhw17U6AxkVwli1z+fSBygytroTBWQGctMWCFXxekoatzXNJ11Ak4ocZ7+cG0YGSOpddcIAL7VB5SLm4Cfa1ZmQEb/IKKBjmjf/M9YWDU4GjBETkxDRyGAhjoNR04YOTblHHz3gULViGMge+WdYR/CdA9hZ4eeoZc4ngrK42sRY1Do3SAUGJ7xiYkAB7m/XKF+/UlDnz5B7vT2UtpGRIigau831rZoKMZ/NFbxpMLTbHcx3QAqFVuXzHCupkn+DfRdtyw/mz0ha/l7xy0YNHktwcnU8/oYfF3lL5gszgjiJkabJeaP6E18KE+052U1b2hViDBKMYYYkG1DcrpQI/fIb2jQ32BSxzud6HWJeBMjiBHnKzkCEtuowcHzk7L3rnk6JI9ZpivkswW0EfEszqGHErjUp3jgxfYv71y+xenmP+c0Nwnyu5ISjNJ0ClzT7c3Y8WWfeBb65ojRKn8kJGIXsGjhr5P8JsPQvWZ1AfgbWO34d00H/AJ2uvlNjNuyLtCzUsHfzHDImTMxIapJSNTSOv45T4D0i+s062ap23wXxVFhSBk921fp/dQsYuhtRE4eQwk7uGeKYRTgXJ9Xk1UapiErpSPeeYkpwDCjNFEsMAtkkO1oi1TQ9TsZpPVzXdX2xoEKv6MUKmxQ3zvmpE85NgZucnOQkQjPiltLGcYyH52c8PD6JfVDkhWb+L1++NJGX+CT4w3dg1ZXTt4AbDjcMzmYdbTHmRsv2OJHYaaaqmXQjzrift+ws9NjsDARHWiPn4ZyxUz+A20BC5HcAyaeGPZHkVs0oweDbS3nPql8qGRqAammOjrLBHbB7aLD98b2Q6Icx+1lohz/drP0YRZUWbaaJQqf3QxRqDk2kdBywPRoZ4JLnn19DiySN8V9+/St89eoO3371FR4+Pun8f/f+QQGc142fl9dBZlFONCggtpI69WWJD+8/KmlaLJaWPKkjY8jsoW8kyFTXrYBZpzzGjwk8PdJ37C1IcWQgHQdVhV51z2MmvIqBgRANQPn5a0nb7cXC9myPwnfdJK9T4V3uLDzI6gZhmEtDeL7I8Yv1HOHQ49dfv0bfdPjxj48CCfbTBv20x2I2x3qxxnbT4Df/8hFt2aLtazRTh03doUKPu5sX+OrrV3gb5PincIm6G/Buu1XySuYKE+L89TdIb18gLAqEi4VzPLQASlgc109FqE56HjZCssGSqW0w6Yp8MuASZQ8wvGQdHEA1ZL+DQdI9O9JHIPNAniR+Dk+GhT1T6gB2zquCAVvuqGx3MbFtMMU90oZU4hgtPS+cgiOrbybnYUvVRr4GPwdFpkZ0DNaSMDZ6oOEDeC5qAwfTRnmK0aYBhmzlaLqxYyZQWptCRPQoCVCTHllWVv1rHzvpFtDMSncHGRQugSQ2QeMFnt8riOC6vmwdAm7YJD//RCPfm6q4mTEDPzcGAt0IWMtT2t06ExQ65fHLBSP4JMMJFmlOzQ1G1eLJU0AUQ7omMmHIcxR5fqQVjQQXxa02KRPLGZUUWPHKbgJpW9aSNgvjSGMKGS2NxunXpiv53rMyxwdnJ3xklKfRNriAHQzCqS7cFI4dbadEeHpjO2YB5Dw/0FryOk553Whrs1GCfsc26YwWxXQwnBWmptj1WC1IraQgTq/PyoTJFBjts9N4qK3MxpeJHrs4voPgK1Lrujhxo7OGxukPnwopHN32jtWrMR2sovQYibPf81XmhS0Cf9yWjzibYKtFTzbBuu9clahKz0R8zBeDdtoRon5EmlhAnSUTJo5LplDJYBZHSGkHzbl3xNFUgOU804w9IoJ9CGSctbphQCIeIEXcdNhUDNIB8pTOkRGyjGp/seyXlUwzmHK0oEO05+M8GbCEwA9ALLHx4xZ/36gTckzGjIN/6Tml54jhPw2YOxEIyDHfT7oPhl04JbXGKHEV/sDOCSEBdnyyE2ZgJ4ul6WRzbh1H17p3fiT8XSU0pCcrGbCEQO9xnvRJa4DnxSUKXixJz8250Zodo3Q1ZFxm78nvNvJyP/OJPNGZxbQ+jx3bdV3XF5sQzGYL93BbW9638LxboZC+ArjNNPMnmE9qf/2Equ6UTHhwICtVL3HM9jQxCUTs392skL98IdliqgNK6GU+UwXx+uUdlrRJpkFKkaGqK2w2W80yF1EiKiKrMCYP+0OJqmlUCXM8QXlkyhBzU5AKoY47Rpd5vr0bRRwH+kb1YjVOO2c50KkFG2FGQR4lKBFSYZsvWA4Rf4S/uXxENtDH+SdllwmFNnlmbmJJTkEYWlCHyLMEGSMTZ8YDkdmTkOCzNEFLKeibGHG6ELjzmBDMZkeRJyYFHz+8x8f3H7CbRuy2z6IzUsJYSpLs2PDaTibUwn22DBtEIa8pU5KzjdM577BqO+IMzH7QtcD/h/pLP4sSgeHpPKLdEkt2KJRFie1i7+Stg1t5MUzI4hCLMMIq63Ef04Z4h+37f8a4LxE/tYibAcWyQDRb2jU77JCNA97cp3iBHG9//Y3wAk8fHlEfDvj//dOv8P//L7/E7nGHD7//EdFQItx8MJ1+LBCMGaJuhXgoAPodELCi5NgpSLouwE+HMVwCdbKj7oygcna/iManuRfv14TXnLN6U1Jk8L1kvbi9Q87kgmJfPUkHBA5OoviZhoV3rLT2O0GW/PepaTG1re0V/aA9QD4CvusW0aSsQbNLRQ0mFZNdjYnVf2heGRwb9C29BywR8BgbmZ05GqmbENp5ihIxC6RBIIYGEDsBIlMNMTEtD8ikRTfxI6obRFV0wFhnJe2ZBjyHtGPWuLSjPsUVQ3BdX3BC4Nv7vkL0hjXioTujI/4bq0sGdAabjK1uKQuaJYnX4ze2wqQZPBkL/N5R7KMtzCnxE2EViiDFCu7LJROCHLMZOeQhKtEGI+QdOcUmeKSKg06HFEZh4O6odD4hG6hsRl0DPfk6RiHtBXoKTgmBC2q+XvfdEG6Exuu2DYMqccOlY0RvHvRTAR//3/pnJ81MtL+qJYsSXo3OG+58giUwLSh9EX+QZ7k+QxwaVYv/zevJ5Ij/vhPQyzZIJmgj7XYdKtwjt9lyDSeTS9Y1OeIBTjRGd+JOc/sjSONMeOiTqH9m2HPMFC7tupzmyvYWXujppI/gzy1/VE53Zg+AJGKSFWgUJX5As0dQ7zUuinoCODOkpAPK+IgUuQl5ZliTxTLHGCYU68B+GrFazrBaLzBWNZ6orz80CKhux/uNyoE9wZdswRMlb+33Yz9Dx8Ye2E/OxZm2gsf0aOxFMJ1AcqQ6MlDlSgpMtdA6QZesPM1tPKXndsAYEm1vSP2TfbUTsnLMAiVlx/m/055gEFa3nQwKglHduI40XlfbqxPvbhd9ft3XHqvk//6sM+ATUtcBZEdEhmV+XzriU04qmL62P8qau6Dvnxkl4+5elJ23w0CoM0CmQU8V1Kt08XV9wQkBqyzJDVORUNamjYRr+OAIzT9fYCTCn+CxaEQXtdbmIxXLIaCJeJfCoTfocXM476nO5ODDw0epAdIAhZV93TTIsxSv72+Aca4KuCAfejnHerlQdXI41KqO0jHAdrOTxgDlSfm4c87P9+Y2wPeUO2MQoKUCnavOuR3UFBvqWhWz9EtnwrCYZdI04HuaHwK/OK8HIm5mlwoTuZGE/dEDoE4UvHOKviojt+EykN+sV1gs5mo5s1vhzaNcr1ZeDY8fPqDsJmwb26wNVBhgl1hyR4wFg8Vuu5HMNK8XMRr8exM/YnfET/gtvPPaLGYzNW553TvGN1ERbRM9EQnd7wlMKJkXN+s2aujxM7pl++/lc1keM8FiHpR6+vKfw8n7cu4tbImpC84p0JRMWOaTOihEmdOYl+c8d6A2quFRLoh4jbGmRHeI+TwVHa39+L0g8Pm+AU9K3O4wthsMhwf0D39AvykxfngwhPssk/xvMvTIGQZ5bamA6dgwx86A63D4++OYaDm1xTzoJaCzmlpJ9n744ff48ccHrF9/hZuvfiFr4iTLlTxfsl5QwhsNop7zeVLyWo0M+DzovpPc7+nasbaWtohzvPQJgX0OowmaUAIwtvR14EOXAGQXxZTlzjWeY5eP9ywreKZH9gIW7JmMHXUX/DnSaNDuw6OfwUQvBKEfzAuBPyfrY7O4pvqpsAX8TAInU6mTSRlHaLRxt+YWWR8sNtTRbBrtVdd1XV8whsBVhqEBzVhZMylgAGUQmXHjUUt50EMn2WLGNKF/rcqm4hjpPVaNUxkwOXGH41DzxQO90DmzpHGJ+ruBM+DpjhUwhUtitv6LuYL6PDew2+bdI6Zu0PHwd7nRioLI7L/zs2WrEju2/xiMnUYyg1rZEEzI0QbtjwmUJDOCfCjjVXPOKFWzsTPkcn8Zy8AqvDNgoU8GHNvANSssETgCDiclZtw0mShZxW4dguNMmdCqukG52+PQjTg0rpvjkg+TDiajg52BEG1TaaPja1NoiqMa2+gNJ+KPQ3I+UpFMkXbEdNAYysu9ejyJpQ4nUKGdX0NnO6W6I5DQ/Yy99FFJ7pIlyiGP25lxGRj2lBDY+XWCV0ejLMoNh8gpbpVM1mliIujEbRKHgmeAESeBojw9xyYxoiS1+/OwxVB3SHoGQKLnqV1QYWz3GA5PGPcVpv3eettMJDlTZ9eG1DbGR51LO04f5Hzy4u+VU1VsVWzC1jhG5FODeGzQbh6wffcdUvpcvHqjZ+rcEfNz1yzLkLDjxhGFEks7qQzaerZ57X0CcJZ82TjR8CEKtq4DoI4IFzsxjnXC7l0UpHIfjGe5gu8UUbLcnnkxCww0YOfmKCZ0SkX0WTnRFMbRAIJmkGR4m2NPQMqkluQqVRVzgvc/95nGtBCGyjQOJG/MP3bWGegd6JFGTtd1XV8sy+AYrE7tSlMmtIBBHIASArepSTOe81LhAXJEHR89ew2fWLDCZOAW0l2teNtHGHDmC2q6h05sKMShrPG02WsT19931BCwdjqDX9d0aucuixnu1jeqBgwFbcFNtslDj83eVAg9ay4h+DHL0PHoeAysUPIceWr0yFmWUvsMCZOGrpUfA9HOpJHFl+KKjna1x7/4SVVoRkFUapPmQJYIT0HEfpplmrc+PD2h6VvMF6zsY1WgZj7kAokEXqyaUZ3oWBWmH8BN0Ko5/m5RpFguFroGlKhmx0FKlDqHvRtbsONjm+dJOZF5klVf3GTpCEf2h1zkGED0ZX8WPO1cNMYnEfbjFy8lSI6W6nECPDCznRAe3dEoXTLozoWn5w1tJ9Or7nmLzcMW1WaPGrkkttV4ZpeA9s639xLIKaNB57fdb9DtGwwhhY4iBN0eUb9DPpS4c66HqUY/nJ9TijtAXzZotntM6QHTslIDyqZIwuif0exsVi+qnhMdSoIR99mAdOqwLj8ibHdYlt8j33+PtHmBaKBrICl+5zLOn3lOsxjhSDGiAD2lnAmsdaqfPI8dGRwOqMlLOuP4jlLglMf2Cb+72B4XYwBTf01MmIgmUexqpItCnZKAnS9Si8mcOGpJ8L49PTeWaFhi4SYSTj2b70NmxGCiYywChG05USWp1zF0e4DJW7c3rwLSpNldiEzaWM8bvU+GFkFQIwi4U9QiRF/XdX2xCYFp1h+p0FoywhFamMZHPSK1hk0oxNrxBPmFCrCisGneyN90jARWmkoI5GByTAw4414sFtpEpLIXAoeqwdNmp2RAlS29EDgWYJu1ZuY+mOBLUeD2Zg3KDFKsiK1Z4hMOZYmyqrA9vENF5TlXxcwCYJZQJnbCKOAjW5ZUo0uxmM8wyzMsyJag4ltdoZHYT4uyrkxs5pJ14ued/+WxkjUaGn1kSL+iB09uyUCa6tz5hICjmPu7pYSF2AYV5/+odW+capOBNqDUKSGwBEQmNEmEgjLQy7kSJgpP9QSHLuZmuOvkCU3JjV0f67xY58EEk/xnYbeFoxz/+fy4QsmJu/a85Fah29zX8CiXZwS85pqZOzjIMSk4ujsaYHRikqP5MGvIo7utRlxdu0PzvMPmYYeKVNZ1hpGjGXU/JiRphsWiQD22qNqNEqRuv0WzKTHkMwntBN0BUbdD0Ve4paANEwImSmqfMRcL0FUtot0BU1ECTYmR+BY+lj4zZgBTPPdIe8PH8CiScMRdRm3/Dsv9RwTNIxbVD8gOPyCpv0U0tghJjfBCYRcs4hGC3hIZCoH1qpCNccAOXE13TCV9DoifpCiiWNcic1RkbzjlWREmDXySlRboOIkQpQmSxQxB2ykhYLbqpYdPemdnF5ZjC40BLCHw2hOGC2BCYF0YpnOGpDixeSYmAuzgtAcM1cYM2Khkyvuo4PGHSIIcWURmTweElhD0QXVy6Lyu6/oihYm4cUvYJ5KYyECaj55J51UuYRbO4TkqsBIgPnYCWBGmav0xWHBeby/pqG0u0xB4z5XuG1cpE2msfx56bDe55Hifnp+P80k5zLmWYMFp4RRIMe5pu1V3QF+udclgL3c2dggorkNes9DNtmF7nX0PnjzUrWkTpHRhjMz3XS36DDlb0oQmX7hO44Cz945du1ub7iRzHSUG/YCEG3DbYk+55iTC83avYPq42WLWZJqXsl1KuqFAXto43fzW8dMNcX8C+/nkwXdfCEUgpkCjXm72TK5cpcmEYBg4KpnM4IriTTErxRPUO6cOBKUN+Tvkr5PZMJ/rM1EJ0QCn1Hng5zRb3KZuse/Ki+lcHC0xATyJ+HCMdUaR43lWMmsVracc9kGPDi2qZofH7Q8oHx7ww/tHVIcKYVSYL0AeIuxCDEGi+lCtfb6QEPAWlAanvy+KHFX3ul5CVruqxVPb4zBGKKoGSVKjL/aoownxco2UmhbS9T+m20dqobdu1rgoIDh0wDIacZe0yLsKwfY9huf3yMoNlkONnCZBrIWVuP25pPOvWzWfayfty46VGCQM5OzC8KkmulazextzeXni47iD/3OmrWQqlvZnJQO6BNbNY4dO91REtsEZFddPfBxo1DonblTBV2TSwv0mTxGR/qkvMz1KQ2ogMg8bpHRIUzNqm0zVVg6XY1diaMzzRE6JTvsjTWN1CovMdAzYceH553eaMl3Xdf1nWn9lNONMn5WngZ+IBbCAYwIkpBJyQ6ha6t8zcNoDzjkz1Qe1GRcZmr7Dh83eXsN1BLi0SdO4qK1RTRN2zxslCNKHn0b8+EMioN9slmNO0JHaf5M0Du5Wa1W3//D2GyyKGR42G/zx3Y/OEIa/M8OrFy+UDNC3nr/XHCq9diAntdo5nQ0YqZpI//kgxNOu1KYzz+imSOpipvcm02DJz1ZfhiHQiMUlIeZOaNbSPGYmAl1r2gcm5Tohm+dq3+7oKqdRAauoWI6O5LTz+GYECroRi418GRSIv/CtVJ4TA1aSn23tadvheSnYsaFMb3WwROPu9hZDyka/VabTSEAVK6UR88VCbe584EjB4QamAHmeSAY4cAkBg7QPAPe3d0okFurCJOjaGm1XYbfZoSkNgHrJqkkHqxO7Yx0T4ig/oD84Pw6V6q6qDCLUQ4M6rDB9fI/uX/8rtg+P+K//8gdRZm+YAN6yRRMgStjdoIdBIAU/jgtIqRtp2BNG6NgdY3va3btl3eBhU+LdtsHvDx0O44Cb5wMyshaCDnG3xXI2x015kJRvFHvTJ7bhfaPc6Q6wBR8MuEla3MU9fplXSKYNHt79C8of/oDFZocXXYVkKJGMBADyXr+8Q3BoW5QE1UncybXomQyIKBBiksGZAW2FrWFAdy2go1i2C+jqUWmad65YyQo8QkiqpP9ylGb/+e1FHItFHQNPNzRKa0iKLUeQdGXMOYo0W2pSjGfE/DAl4EiAWCZiZtgJ2r7HuPsRU0+sx07YGo7i4ijBoiCzKcNiniLLE7RRjyxsMQwBuoTmZlelwuv6kjEEZ7QtVnyc/bOaZ+7Nv/MgQvL2mSAQqc6KnBKveRabQx83FRcg2FJukvY0h5ZHAhXwOMMzAZ7pmBCYrDDRyGIr0C3RYEbuOCYlBDfzlRTPNoe9gqaPAwQzUtiI711Rb55ujG2rv+dMMXLWp6qCx1GKh8Zw4GcZUYrrz5Z6qk0iobZ/mjgmw+cviQ9zc3PSsP78CqzHz2fsPwchOxnX6Fzy3PSDjjGKWyUA7AZEhdk4G4DQ8B2aqzo/em2ybK9yaw6d0JG6MUwIqBZp1EpjIZJ33R/ZBh4gxiWDGvo8SPwpMTtkV4mzMyTgImfK2vVHUU+9XTYxIgImSsSKiRBf39Faj+ZHn7e8roSOUVXruZnS2TfXvp4C1vqsH2tMKDH2tRKiXqMDamRwZk4LZNPOsArXNA6VDRNYx88dxKId1vWAehpQHTq0FdvpQBmlOIQTdkONfT8hKBuwX1YUTMBGJHWnBEWaif6p9IZMXrXSBcM8HLGKOyzDBmm7QVQ/Y9hv0e72mJrGjIW82t5Rbvyyrosl7Seg6PFg/LngfSqbYF5zN/o7+5kjxvBP8hL/ly7wB3Lv0LjRqzF+IsRwZlDkOwMaCzKZyFIlBFMc0l1ZmKCOIxZwlNkjJiuo5/mhG+MBI/UR+kqMEYIHxWRQh8y++Cx4JdPYdQMniaOZ6mkXXzEE1/VFSxczePHJ5wyfssPJscXPTedwOJjoUNtp8xe7inzsxQxvXr5QoD/stwrou91WHgd0kWOFz82XowHzSLDEgjblxhV3SPcj2M7J3rrApgDKyj9JsdnWAgJ+/+4HfHx+NO19JgO0Xv7uO/2sUYpGJQXS5Hczbmfoq0C147ghCLDZ7tX5mNP0JnFzeYfOp/wsq/pLVkopVIYWBWYNSJDFqV6bHYE2GCTK0h2cmZNzmvQAvabt8bQtUTcDsvijOgThy0BdjOHMVIYANHm4d60DqNlcl7Nh72dgwK8RcR4jGGN1RfjZu7pCSfoVhab4+hoDsYUaInWCP0ma675oOWKhqY+MfxjcbUJPwaopn6kqpLQwMSXsDnCU5LUP4tg0KzQyumDJ2padH4FXTTRLiY+AaQQvmpa+ATT4VbHWB6KtvsbuUYkpk4GhHjBUPZp9JUWjaSDQ1eSBw4BUvw4Bu0TNgCFZokly/PDdD3h83OA2n+Ht8h4P+xg/rF7gu7bEb7sS26pD/sNHJZz37RKrcoZxtcPs0CEpYkQZ7zPqI4jkL6lgzcFjagoEeJW2+KfZHrN+g9nTf0f//IT9736Lpz98QNulCKm3IT8Fh5mQQ99lwSvNZgiqvQFWidl3HiJckln26oNMdlxl75OB8xzCcP6nCG9XwP6GnQbCkkVKKhv0FEFiF2fwSZElD/pMZCewE0FxrfncHEtXK1EXD2ODauqxnVpsuz1qCXUtlOi39ZOsnKf9TnbKUfOMqCdQmVbuE+IkQDEzRVV2FOhqGAczeVck4YQ8YbJOk69JxcR1XdeXSzt02nmGCDYikK8sZXzDgCPtdNt8fAvcixWx0mTg4RYgMSJWYHGnqlZKhXIZs+ruyFL3YLAzkSD7g5s5nlEGGdyfdzu0Q6/5et2wyuP7mOWyOPguQHCpO6AEZDwGdm68/PN2T15BICAijzUcM0wDuxwmn8pkiKpqDMyXLAm1yKTIqiv9Hytv8aR5hVhxsRKPhbb2FEVfBfNYGbgYfKuqUWXIz0WPCT+KsM/lJKJdkqCW68SqR9JuR5Uek4tmAGf845iHO69ZR/vrfaSTOXiidQrIaggxsmqieJFDlUvkRR2Ck4iSAqrDdZyWEz+i5vzF1A1/z/gi20v5/qnIj5LAkDTYHknYI4049jJq3fHLAdlOfgs8dkO36xxTPZJF5xShnSbU3Yiy5uiL9zmliiM0UYImitEFETqCGgeFG/1M27KFPeo6BhHvR0tkiPew9+TzQJbLhCycMAt7LMIG2VQhqDaYymd0ZSXpaSPYMZgds+njvP2SJb0KMldUFFgny3FMT98/WX8q6v0nmhPO9Mq6CQ7rIUXDHmNdY2hI8+O18L/jPpXrKHjMAceAYRJj5P0amYlUSdAvFTXHAA2Bzkml6xR0peii6CpTSiTwksBD0jfZGRNg2bAtR0qqv3U8U1KgZ+t2Xdd1fbnCRHSQYw/bqZ+RN+yFdfgg950JDjEhkPMbAwkFPThHZKsfAVpWtf0ocxwFMNqhUsxI80nTJ2AVpFleZPrp3ueA1CYGtWO7OU5kcsSgfCgrff/uu++cBoKNJvzxDe49jdHlKhf3kHswoakAMmloxGgQTqBIsUgjvF7TzGauv99VtYBJeyeactFqzeyHm2EWpqq48zjBjJscqXl5plZy0NJtr5ONMDEFBBnWFTEQA7ohQF2TFtaJIrnkeKar0TQHYOKG10tYiep6bbkzjjVBntyIxxxBymAun2e7ltw4xwHLWWEdFMcmMVk/4woax982cPHhE7tmBF6NdBOW655nfhMfEWDq2I0wESACvQharCujgpKlEMUZ1nd3aJoLcRlMsJh4ekS6yweU7jDBGWmeRfpjo0Tgdtlhnvd4mUZ4mc1QtikeKb3b8bNZN2CezzCfLTArlkgzmhAN2JdUBexQNyEOTYTvyx7PJfARGQ7ZDF2+QjC7w9BuUEU79IlROgmWQ7RAENLZMEXVxThsA2y/+4hk3qHtF2p9B6zKwwDrtEYR9XiZV3hT1HgVPOPt+D2Gw3vsfv9fcfj4jP3HDQ7bFn1Bj2onHc3nj0wQsUIu6xC8ffUWKefupGR2nL/zWTQrYtkuO1wGcwUlnyHdI11ScHRqPOkq8P6J85k8EqQ1EoaowwkHduTGAeV33ysZSHY1QnYJzh4zviZxNkmSI+JesF5ijEJ8GFqUbYXfP73H+90GD2OCH4cUbTTDId3as9+XEpGK2xZp3yOZWsThqCQgLzLEZBatV0pMw4BfoT6fhM7qElW1w3pZ4MX6DpEYIdd1XV9qQkCVQlX4DrHvOdzcY1kpq3IdJPhj9DIvxOroXHItNEdBUpX05fjxR0c2BRWHUyAo0UVuJ0BqmwFlkdMURZpKc0BBizzwrsN2t0dDPQR3fL6qkyiSuNI2QzcuvjOLcf4M1t2gqgnR/GxhR7iZpRoVzIsMK7bxGcMdqryiO9uFYC0zL/ICLmYhzQSJM0tWZEmUYWCrMmvVKRin2qRjNaqhRWEvZTWeo7pigB9Q1xXqhEwBs2/1pkf8PnStm69bRT4kfB9rpxMAKL0AaTsMOgbytHh+Vbkd/QpcgXhWdYu2KBFCV886W2E/bzefeqOeuWm06844ZoervEmrPIIcP3NZvncq7U7AdOO4nYyzRiHGZ+mE5WzCKg2xzmIS5/HI+4AdJQdMZIfL3DkpWEWHT3OO7DomqoG+Dn2APf/MQJ9kmBKqEWaYQjISLAiSMpoPE4Z4CdBdkWJc9MboInSHFgg6RLWTpRTmgpa99J+YMIsn3CY9VlODYqSr5w7d7hHtdqvuQNeOGHPr4nig3ukJvOw+LTjuIZWSe4CEwpikO6MlL0zkLY/PVDfd2T+9kJIBTzFMEaTZEQMQdA2Glq6lLQ4UcBpGLDqqmZ7orEe/Aj4fpJeSMcRCJQzQdLU6A89VhY+HA56GGE9DgiHq0aTmY5KisY7L0CMiqJnS0XI0jKW3wZEVwbAyRJN3CRNf63ayi8lktS/YLWE34soyuK4vOCF49eo1nh8DVIcIfVdJpZDBmZslM3YCxIQB4Ixbsrf0KGjw/Pysiozc/+enJxkPSVpWlaeRsn1L05uWcDNoBPZySofififIw0x4hDcv7rFeLfDVqxfaLPjaRHP/7//tX/HxeaMHlwmCdS/M8CjOEm1NSj68GJC86UmJPG2iDJR5ngpA+L/8wze4Xc1xO19gnuegdXrNr37EpmJbuMW//dtvP/sCFIulsxUe3IZE34EYc26WlIoOY2HNOW9mVT9MBjakQFHGjYkVVsoqliBAA71tNx/R11ukDhzIjUwVDr+7qpHjFJ63p91GvPY45CaX6FjqhtdvQkVwqBzljEJmrdlI53Uc2J7mhkkwISVmKwUv46fzs2RIGDxdx4eMkI6qb4OxAMyZztDh3qFOrBSyWLQRf/5KuLmbg9ERf8IkQyBKp4rH5tYqo6HRhK/vYry4CVHUHdKqwrjd4emHB2yfD7of0jTCar3Eze2tulJjM6AsK2y2G7E42j5AOUUY0gwRMvzdPy2RJQFevrlFEwxougbt8w5xDfz93Uv0U472/tcYsxWyWYwki3BfJHg9y9AVS+yyW9E8x5iqfcAq7/EiG/E2q/F1/Ii8eg9s/oj+8QG791vsHg+o6hENNQJ0Tu16yZZYCdjlre2Krfa2RNeUAuEJ08COUsBnilfQvEpiKjjKUNoBZH2PiJAN0naZMNJkLE0Qv/0WwXyFgHvBMCHePSN/6syCSFKDZtRl44RJIlC++yNhqNlMDpG0Medz+TgMeKZk93KFiZTXpkdUcqwXYuh4LSlXbfUAFQzJQ+G+5HFQTIRpYja0g4kh2R0kYCrFpEhl5bPDDmhVk21wdTu8ri84IVjf3EiUh6Atynsa15s0KArBhDIy4oMVl6WruHu17neHg4mX1DWeFKyb4+zdMANnb6IWPlvTjt/tFNr4eOZxhjxNcXezxtdvXuPl/Q1+/XdfmwxxR+GhSnRDbh4EA3JZB4JjACYEmYIDg6GAhc4RT+1HF0C4WTAwL+czLOYFfvWLr/Hybq1uBOl4U5xijHNU3YinA1UaL3M8y9jircwNjoGMMskUciGIyUtFs3LnMUa9ibDyJEVphITcaHVLTPWPeuwEHpb7Dfo6xELSxgQGUmehU4ekdy56Xc8NrZcXRdu0iMIMUZhqBFGWlgh0rAK5iSbUl4+xuqkxazM3umH3wVdQxC3wPARC51vnwiyr1YlQlWugRh4731tgw4hJj+DphnI4BrHLEgKeE+IUjLliZlrWLnCyNAw04YQinTBPgRfLCG/WRvOcti3GssT+cYNyV+nY+NlJW10ul0rQhm5AU7bYPu8x8r+TAjW7N7F1XV5/c4/72xnW8SSku3QRDiWiIcXbJQWzVqi+/QcMixfICiYEMe7GFq+nBvtsjkOywEjMjbAWE2ZZjHXe4TbpcB/tEQzPwP4Bw/YR1XOJctMYFoHiULwmovsZtuZo6HRhUsAkjhV839VIh15yyTyfTArkseGMN2Kxdcyt00KqMxSSEFSAiYltkSEoCowvX2Fa3yHg6LDpEPcd0udHjcqER1EiILmwY6/DgK/sDiSiBw5hiAMTUtKUhxHbcUJbzDAtF8C+QjCVGssNNa9lgCHMrDPo5Ki8OqUZNdm4TB0y4k5cJ5FJsjlLGuWSxQ61Ja4JwXV90QkBlQOr1Vp/7vsKZblTJUman7WZrepm203yseRl8+EhvYdCOU2jdiBbrUfXsTPgm1qdR89355rmKI5MOO7ubnGzXOLmZo1MGv6hXo/VM5UF+fByRrtaVma2A2v900mRm8dyfaN3Fd7A+SvwPahrkJPD7LjRfF12A+QTkNB6JpZBUMXWPJOChK/NLsJMFsqXrPlyhb57VlBWR0U1Xo+a9q9qzZbS0OdnENaBNsystqkkWNcaKzBBE82MlCoG3NgMe/rVQokNRXE2+1oJXFmVRvukfSuThwNpnq21USnM0zFxs46Nt4oNwl7B/f6wkoqhKeWZoozGEgrm1h4WMp9y1SFNYQw8SuQG36+sam3EeUa7XyrAETzJUZKpXUr9kSqQ7WUYAnfUR6MoT2rzrXNq1vM8LbIQyxyYx0ARTNjta2y/f8DuwxPK/UFjgRcvbpEUM6xuligWhYk0NSV1upFOtIbOEL+4xxClyL9eYghi3GQ9imiU2RDn91lW4M2rX2CibFbxK0zpEruvv0I3v0EuC+sAy7FDPnTosxwznuOIYl4dsmjE26LFV2mFFfaIBvolbNBvntBstqi3A+o9q1tD3TOQUi1UXxwJuS8D837+6sYGcTRhIiuEFtodO0dCBZ2ohM7A7Hh/CEvAEQGTM/4zJbBjRGQFzOYIb24Q3N6i/fiMsemUrDLQsu0ij4hpMPEzJpQGCZGlc8rnktihNAbvlD2pxEzs1ysUYYhez20MbA/okw3KQ4Om3hqTyGEgna+ViSzxeANnveVotxxXeo9Ddd9IQ8xipHGGIk9kO25izdd1Xf951l8VzW7Wa1XirJgO5VZB0dRpjSbYtVQFoyhNpqCqLZit/6FxyYCbwTkTHS7PF+fyqHPNts+qgYzt5zTFq1ev8NXrl1iwXZjnzpSnRhhkmM/WokIyWSibDp0sbel0yGo0kC/Cmzdv9Pqb3c46B+4Ybm6WWC5NJlmVqwM0MtGI0zm6KUFLPjoDXUyzE27yOZarJcL4suC1vr3BgdK1DOSc7/dErTNKGiuirAmOhAKNQFouIaBU8XjgBhYiCUmHIz3uIBGgHRHT3Oxe9BhvJjztK7x/OmizpueBsBL6bpoMvKbjyP820yp+Vo8T8dxzVu0vX1RYr2gpTQAgZWwn9FUrb3lJ5stZkvRRAiUHxFQvdBoG/Gy7PccKUCJlmAHbaIexVcDhDHhfbpUcXbZcgvnJlwkSmXJfrfOzmqW4mSVYpiNmwYjH5wM+/uZHPDEpeN5Knvirty8wW69x9+IG+WqO/eMjunIvWV0mBHQSXL59jWS2wOrFt4iTFO3Db9EfHnEomYD1mM3m+PabeyBaIlr/I8ZsgY9vvkYzm2E2DchoTjQMmDGYJgnms6Xon7fhDvOwwy/nNb5K9ljUW9Hk+NrN4wfUH0uUjx2qHe8dU10MyTAIaBhGcS2KA8USS7q069INVGscQV/ooAsQtOwCmBWyQUmsU0TjH7E31LKy51gBnd8p/8sEarlCuFwienmP4PYlmqpFvaVQGX0kqBcwYiYxBo54Rpkbsbsgum+RI5sXSsyHLEFDzZCqRUPszd0t5kWBaDbHjHbNTxsgKbB93mG3sc6mxzcQ1KpDZJIj63ZLCpgQMNgzIaCAkWGWzHgpTg3bM8uJKyL759ohuK4vOCG4u71Ru5kt9f3+GVV5sHZoQ0EcxwIIAmTUT2egUqcgdPQ4Z0PqrIfF8w2cx7nHD7iOwPn3c7qU2fta21lz6XHAoaolQ8qgxE4FQYXsALCVr+DE7gUD6mAgsKNEr6PAmQgQN1Buml49zYxu+mHCoW4EIiTNkIAwgsTCOFPyIQnnC33mDZzl5GklqWuKguyXCoTZWaeD8ipsoQr/EIyg3RLlk73Ko8x76ENMsJSQigFaIuDLFnXVS6fARgXWJrU2KDfuBHFClgffx6lO+sTMHaMqPrFIejEdCLrSCEOzYvtZyUB7qqkUFjvUUWVuRbxuMmeyWt2SBOObm0x1p+6E/yIn/JJlFtHjJ1bB3iNQjAt9EYwfIufxNxyl0IvgIKOhUXbbsTAay9sFZus5klmCKAuRL2ZGnSx6xHmHeLVAMSsQ5xkKAjSZ4Dhn33yxRLK+R1PFCPIMYzjHMLsBkhlSziryGAnV/cSG4P3MQBSjoBpiCKyiEXOCHqMWWdggGiugPWBsKnRkOFTUSqCsNcdrhvKUSqACKe9nU9s0YORlIwPZ/zoFRkkz6+2czC8RBFIOtJ9Rsvepj6X92VE3KS0u8CHBqhQoo/0xtQGUmJp+goZVTntALX7nQcxOXzov0AchWgVtoE8T/TdFg2gh3Q2TGEV8VjhyM5wTGUvsuNlxcv9hkOc92JPhIjAvwMZCQr0HdgmcPoK6A4Q/hBw1saMTIndjuuu6ri82Ifj1P/49bl7cYbvdCYGfpgU2z094/+P36Cj7WtEOljr2qehn8/kMRVEYwK9tpcHPwGoVqSkQCk/gHdzOdAb83NdJz+u7fr6tMeUpgeAKOu/KvYJq3fZKAH7zhx/wvDtgfzio+jUt9FACPvu9CSexwmYFkOeFsAPS4z9LCIQzYHCdBnkZ8He++eor3K5vFISZEAikKK7/ZcFLWgnUb5D+AZXTavMdGCl6wnk6wVqA9nwe11BLfW25WhuY09lGMyB3nP2zhS8wFrALa0xdhE3VYbNvjR7mKZydBUmOgQie5PhhIB5i6jCqFXzKCJiEyRuhqnHYlIg4lqEqHKy67SbaRpvnA8cbrPRKavjXpRI3qjoS2+AV95hA8vNxU2bCSBh5FA6II45/OkzySfj8xfPJbo7uIyfuY3gCOt71SIMKBTsE6LCm1O6mkklR+f0H7P/4A8bdAXfrArP1Cm9/9QrF7RpYk84XY758hSgwKii1A4IsQbBeCClvU6sBTTCimSbcfvUNlt/+PfaHEA8fY7Rjis20VhI2m7HtTCieIRsYwFsGcQS4mUJkwYivkwGLoMVduMcy2ADtg2R2u+cHlB92KD92qLcjmnLC2DGAhggnPnlE1KdAnCGIBJ3DpStKQ/RRiIZ04yMwc0Q0cUzFziAFqkJ0vF95HEex4VNiQCErJiwRE1F2wgjgTPeod1vsN88IKd3s6K1skhkOgWBZCl9xTJBgdrNC8eIW+6HHc9viMI6o4wgtsQxFgSkrsN+WeKYmQ8+EleeZHcSZ9o+6oSnRhNWKY8IUk7Awpppqo8UJM5I/4sCZXwn6LBwDja04BpsXIW6WmY03ruu6vlgMwbywtnkYCHFdHipVX7vNxhQKm0qIXE9FksYAW3tH4Z8AY2wBnxQuBWdnoexFjGx5ENhJVMWL8PhWv4SGpMJmhj/b/UHBtWoMo8DqQGwFp5surwXREQ157elvpqfAKsI48qwazDTFyaYKZWzKjGnKMUWiL0mlyuToslYsA7neT10JGseobNaXr7X4vwxy6hhIpfDUhqem0MBa3WvMn5v36Ge42VEB0rsJWvVsoM5TB8Ym7B665f/X073s9aj4yKSANsBdnds5pm8Fuy8NQYuUijWWyEBBHkrA8hqz9OJ79q2qKsOikpEQo6PwUmjt2mnk9SEj5GeYzWpY7DtNlogwSLGSnUUTinBExo4Vma4UqhlaURDpBxHMMky3CxSrOfIiQZZFmJiBUqWO97Tvi/Acsg1upEKEIyltZC1OailnKeWtycgB8oRIeuIEKnRBi37KEEx8HTPn8fx8O8YeedAhD1vkQYt4ahGMrKBrdQcGjt8aYmMMu+PkNhx4z3AEJtjljawuBxWqw0ZGBe0cyDHgc8MOgaP2+ismRcojjNHOuaSxvfAVgbxdj6ltMRB8zEDO701DYJLJXByvoeOQ8vcoFMRnVmW64IA0XhQ+glLFfB3pObKTRSxOw3PmNJRktmQ4JFb+XPN5ri/5GwSDNDGq0hRImaixI0D10POngU2KhMJF/GLCIGXR67quLzQh+PqbN3jZ0Ra3xv1qjV+8/Rp//O475GmG3W6D7wIPMuNDSfpeJN620X04Ex8Qd6b2R2AZq+uy2juGwEEsBCUPMq73HQIL5NwUykOJfZoAt7fK+CN6EYxs63f43ffvNT7Ys40qAxNuGV5JL1LQ2mx2BhqkOE4UYZ7PxfkvskIWv2otsmfIDVC5iSUITBruX7zC7c2d4NRC13vU9IVKhVVJIygT/OFYoKZ6mtPJF1CQyOeRHQtjZkibndSq3U6KbjIiynIzZgo6gCp/8pKgTCsrsgGEBJhYH9vnpFzyHJvne9vysxA1TWwHgYa9wGLmYeA+G42RMGG/eQIohrQtMGyf1GY/9DXaccADRys8PucvwX3X1A4j5HSh00Y/2Kinz/X3/RAhrbnR816h0RWvRSqhmEuWKJJql7ugzURvHARSW8UT/m42Yhn3uO1LzCkL3NYK1verFPn/8q0Tfhokh5vdZghTIJmHCLJQNtLsmCiJGKl6FyGqbXQTtKa+eJezTTZDnnXIukdEXY2h2+qZmDcUSY7x0L5CFc5Qx7doI7IX6MhHQFuLRbJDETS4CT9gFlSID0+Y2mf0Tx/Rv3+Hw8Mznp4a7DakiBKIx/zRkkp2lNgZQJxY/5v8fly+tocaVdmhKgcEY6LnJIo7JFShIti1ZaVt11zXwLEMknEAawD5hPD+I6j48Qnjfo+Ko5osR/e0Qbyj2VOLQuZpvJe9uqSJDwRMyOIII++bKEIzDthTe4AZcVaow9IQ4Ny3OGxLHDZ73QPsoJl0uY0CBbqMA/zDr97i1et7BXbiJOmg+vHDg9GRideZyKwgfsccU/jsFEmKeZFgMeN9yk7JFVR4XV9wQsDNmnN0tqqb21Zz66qqcHNzqw3/aTaX7sA0UmDFVyusxo0+JTtkR/FhoB7HCOnATgHn/wY2pKKaQETHZ+20nXkzHyYNqnTVTrQ5n7oDdaNkQAwDJ6DuNwFVzBLBIY3O5v7mwpioHcmOhddUMAc8g6JpVk6DpqzQz1BRT8HVid6xs3DJ0vhEFCezyxXi+ajubnNUo2YOn9gzq1vS0u9+xCBdeSA1nySVRWa8w+SBWgzOLc5LyQkMZV+idckamdWe/b6aHvpRV9X50QFppF2NtpnQRKZO10+WqEiXgLN7cra9na3kfWmyFB8BXCY0Rcg4xwhmm+um/eKAM0H7lIf61y977zNTHefiyOELg+4injCPRqRTj3hgNO2EvaDVbbAyi2adEp6IlHa8lN62OXLvujhj2KEPyM4IdU9TO2EavM9FIbOnNKBVdYV0KJFNzwLLBUpWI+FshrBDN+UIRucPEY1IpgZ5XCFHjXSqkIBJB1kNlXUHqJJZt2hrSh1zPGPdr6Oit3r5Nm+3e99LTF+WFrCl3vYTWlpxM7VjZe/uNTvJvqPl8QonUy2fHEjBkMfqRoZDtEfPMVVZKblnjc8Z/fH6qdkl0wJlGuwOCMcgTI31ZQZdWMMscdtRQUHcELNg3m9kBVFNUwN/oxZQ62OxyHGzpg03ry3ZLhP6urRuF1+DnQbDoR47nsTRpgm7A9ZpYHJ7Xdf15SoVBgFmRY5ZnmCWZnj78iXu79ZYrZZ4fn7C3d0a5WGPj+9/RKPxQaQgzWoiy3MFM0oOUySnLFkxsUVo1sjrFR/ShboPB9rummuuRg7r9Q1SqYelqir++MMHlPsK81mOu9uV5uVsTTM56BiwOoLTbA4oTnpMGWDTKWdQf/PqBWZFgW++/Rar9VoJAY9RM0ZS+aIY8/nyqIhIVsWSALGMhkFE6FPZLBSG4NIaYb95RDsyGerQqdquTf/ftV3ZkDVgk0NkOXl9a5CyBU9KYGcdjyyzli1hCD2QpQSAsn1KbrhZU1PjgDt1UfiEpxMjIM0TIDLVx641gSmOgLgbkm3BTm0xhsi4yY+swLZI8gg3txmCJMLLxY25zFF4hkmgOgIG/jTVxUCbqX03yijH28JkCkTIeazpx1/KOiRHPnbAREn3OuzALBqwSjq8zErMQ1IDHZ+eGQ9BrskcyZK6v878ULxFm41LMEliPOyB9MjzAXOeFI2ZdFKklqckmH4OTBanZ4SHHYp6i7j7UcZSfbszEFvzHSossIveogxvEUWU8E0xSzu8CkqkYYtl/4BkOiB+/u8Iygf07/+A6t0jdu9rPD9OIKmEzTPJQrOOZQwlroUsHyrzubEW72clWhcsMib2LdU5gYJ6GcQKkMkged8eYUr8wiTmhfUkXELrck8vECYcS9sImBhSjIjFAp+nkf4gofA8UpeWe/UkiWQF5KyQBkhLB9K2x4G4ockgoojZ+mf3hkqHg8CKFDtios2OGlPXjOwIUPhrhiyN8fJugZf3c3P2jIB5FoiGykSAoEQ+588EmRIcW3NPabGc5VgtCu1hEvy65gPX9WWbGwUSzqE4Dmk9Xv6WWTX56WW5l4shA0l5ID7ADIcYbBnYNa92zoIeD5CkJiLDn/FIaCKOmRCw4OLv0b1QwD+phI3Y8EGtagkGvbhdqE1pCmZEsJuSGFvpXlOA7Wn+G6v/nBbJNzfSK3jx4oX+bLLBodTnaHVLRDJHEkwkmMgoMchyiaHQSlWVjwMVJj1HIp+/mqbWa46s5gnoY2nqbF2djZR5yGscbN0JzURFmzKaFGfwNAQiP5sblcR4OO9kq5sObmOPsG1UhSapVWC2vFHNKA13BhGOa9qEBXOPmizvaUCeUYAmQNZPSAhvqMksOSBOTCgqyQPkLwqEaYyeUrwyNfJGSW58wN9PCYLkdSAOg8fhHAgnbtrBMQm8dJ8ly0JKefqgxp5gCzqNRmQhA3mLGZMBOtz50RRR5jxhAe2YTWNBxlND+Amegja6YFIgAIprgxAy7xsfGiVxTEEQZYegGxB2z0jGj8BI/YtHUXXrgd2AuX4uCg4IqeqYZBphrLJWo4NsfEA0lYiqD0D5HsPhGe2uREtBrBIgNlNMDSe6qUMk3VejAt8h4DNqQj+XLCbB7BBQEbBlR4+wYSL9pUTJCttK6aMZEJfMO0/yySYOagZb1lxopJdA/L+wGXxeJf5FCojrHAnTI3cldQJ6BmJSY8kusCHUMWmbJqpkmuS0HhR3bniP0biIz+wsJ5iYz3eKxYzqnMYeMIiIY+DQqIrHGJiImZkfjWKe0KGTCbKAlRfep9d1XX/b9seaXzvan2ulr9c0enmD25ulOgdlecCbF7c4HHZ4//69ZnNekpcPKythJgSsEvlIiX7onfGkGc+AbFgCdhQ4onj56jUW87l0+HkM1W6L7X6H9WqGgjrm8wJvX95juZgjm8/QdL3GG3lq1sJsVvL92IHIsxxv37wR+2E+X7ggYBt7HPeq1MQtX90o4NvkIEBeFEpKyIogIl/JQpZezDJ4+5IVJTsNEZp+iY4jFMaYiAGfyRINpSYcKCzUUkiIFK0Od3dL3N3TaAco5qzEQ+SybIW5xDHgMBGLIiyHAXek/XHTdYmXU4ZVxOb3mLztJHegLFOZbNUh4OuQBTAhamj4A+wfD9i+j1DMM9ze3SAtEqxezhFnkUSInC/iEdCpbodGwN6bwpIQJTNOmlboSKlUnoHKPnPlwYB1YfcnRxbsEFBnYBUOuAkqpN0HRMS6OLEbe0dn7S3nR/M74JJy4rEZ7v0QXLojpUav18D/tODHcQLCXr4SDCYTSL8smUkhHCtVw7fjOyzGFMv+Cc2UYxgdpoJeCm2GMRrRxQcMU41o8yNQfkC12WK7abHfjqgPTCZJQyWrAuiZDKstkWJIC0xUgZT3BQPqUUrws1dG45+8RNiGqIYOH5sOTIU3bhRTiEUQYMZ76UwAIo0m/ZzZOVuQJwPDNAzN7XCg1HQYoUtjhAUlmwMMORH+nOYYAKaWSxp1OWochhptFKNM5OKkhEeiWI76LKMlCRrZ9WARw32Kip73t9bhXMwJjaS9sVGiKTiFoVOgl9UxKYmLOYos0ziJ1zJlV0ujEKM/n2uoXNd1fYEJAWfYw7FiZZBfLmfIixxdd4P725UC5ou7Nfa7Hf6v//bf8MMPPxznyTIgaljFmx4BF2etmp+6pCCJWwVBBgwCFBl47+9Zya/R1pUU75qqwk7VfCfAGoPh6/tbLCl/WsxR9wNW8wyzLLGZYD+KYnh3/1JB/f72TqOANC+sInEYgzBm+50mNhkWlKlNThRJCiExKRAFkMZHiZnd8DNdsl6+CNQlkfAf9fDH3HjPMYM8E6lC3OmPH3aoqw7PcSA09OtXK/zil6+RpJMSAgZWVkZibTgPB0NVm4ogvQSOhlGiTxkATrNVzb1p3JNZLSeyggEPWWZxJEE6HaoJUzPh4ftYOMPZPMf6Zo18FuPFfY44C9EMvTZmBidOlY+jfN+WcDLHvI+UEAi9bZu6+2djS1ywipBVtuFDhKUgBTbosMaA+dAgaT6KLge2vZUZJQbGY+gKLFH1+BdWlyfMvF8i2rnv3g5aUFoFISo7MiCxlW4mx9TpINWSyQCTgwEJWQNjiJYAxoHV/qSqn61xDC8lg9wlrIFbpLsPCKv3qHcl9tsO5R5grkYwoQpiJ0Mg5Ai7L2kuamPEGYJm8JfXskoIKJWdBtK2aBoTI2MSkIQhFklEEgZqUV6dSiRlwDl399ZkDv9CmipXygSVzx6tvTnKSlPOuTDSmGuW2GFTxIyBngJYXY9N3WBTtwYkpNx2EiAjfkBMHI4IhlNC4HAyLDpWqxlmswxfv73BrEj0JRgvixwGdz7HYrlEoil6tgb3D9Jn+7Y+SwgMcHspoPi6rutvOiFQ8HN0QpP9NU67KX0BWZaoNffm9SvU65VkaBmAiQmgZj5dx7SJRDG+/vorJxZEkCCFRGjXa6MCAvckbkNlvabBh4cHVeV0HdRMOo6xoMYB2/nSNY/w5uWNtAP6cCPtAG5OROGTAcG5X17McHd7qyBhSormA2C6BATlcX6ZY3UToChmyIpCyQjHBNzqZ4sFZrNCWwxNgnxnwxB4n7/uX86VEKiIc+eTwYXIe75+EuXqDOyfD2TGicZG+15uaEVBsSRKL9scQRRGzWLNcMg+n6teHcjTCy+Jfnn239bFcfNytV+pVWAqlAY8HPm2Jo+bRQhyOvpFmqMyNHYSxWEi4IGKDhgafGo1rd6BCwg+UZBrnoCP/hxcdEqP4yhjlHiZWmIbErRDgk2fgV19D7hTMsokCS0ohsvkiroIOnR/eZWwOLCpAG/8b0ti7Ef4uZhAMTSRleG0JIYO02GLbvsREwG3u61m3EqMe+BQERAb4NAAuypAtBxRhCtEaYxknqgt3zQLxHWNTTXKcrlsqEfBtrbRAJkQkAZI1j3ojpkQdR8jcKg4il3xHrpkkX55c7sQ3qd63KMZD9JiKFujClYjg3uAGYOmEk+7r9IJSDVBmRDpJucUxradjPiGMBKuRcZoEWf43GdGuTeqvndZT1NWsvw+kJHU9lLnnOvZJQ3UILLt0ImNw+/t1Dn8ih2LHlWNx3hvhsa2EZbWUXUdBdcSYfJiDZPE/WWWZ5iGhd1LuidsVHop5fi6rutvOiGQoMy0cuI9bLV1pvxFdbEAAvlxY31NB0JAhjAfHj7id7/7HX77298qoWALmwGLlSUDz+PjFlXVqJNQkoLXE19AQ5FJ3GCOGP7tN79Rhf72zWusl0uBAOlrQLlhCQvlKV69eSEwEK1knzYlqsNeWIbFao03X71FXsxxe//qiHkQnoFAK+fNPgYhiizHOrlTErNYrJRoaAYZADe3NxJa4s5CCebwLKhesr75FbsVZsRzmr4aQ8CaqikadgYeNuqOkPKUpwHWy0xfElJZ2IyWAlDnAVVUOKf46KLzMUHwpjfsIGjjTmNTIOTIJMk0CmlabtB0SjTfg471LtH2swRYZBiTWBUhN1O6KDKr6R2IT0wQbrbuPHnFQP/e/ku/PfF9vAmR6TJdskg9IwiMAYVjI2khBBH6IMM45fixmSMYYnkasNpju1qS26hQEMQXdkii2qRsORsXQ4L3SoQxpOIgcQZMBjiiYaAjaJNKnJWBN/tKugYYqG/QYNgd0LynA2WH+rHC0I447OjxMeHjU4jtLsRTG+JDE2H2ssWbaCk1vgWWAtrFh3uEhxCbXYfddofNYcS2M2ov47yRVNlWSjCkc0zFGiPFk5pJM4UgZlftMhOu1brAYp6grRd4Fwb4OLQaXz2UTOI5e7d+ie/Y8dlhNyqbqFwamLvgeNJI4L2ZUhpcDB5j+bSc2QvURzqys+lmYsD/psVzXUt7gCnqKjZgasL9hPflNOFA6eumRNlTaKpBHiXIEoI1mRRaks3nigkoRcCGnsZhDq3jGDUaazKxYzKQEDcUYcViIM+lqMg9kDRmClElFGa4ruv6UhMCudepOjrR+bhp+mU2sz7o0NOA4h8zrNdr3N/fG5K+bY7MAXGCo1Qywz3ndwSe5Zlei5axNs4l0MhoRQT81ewiNI2qLTISdmUpql46y0wLndxhCvZQRc1VBsr+GdBa2xTJfJAwj3rzIdJ8hsR5I7BLoHGGSkN2I6yL4IGRx64CW589AYyXRa/Ii6BYeXyih7m5L9usnKFqFu8qeZNZFYnrpN3iDFs4MzYhmvN1PgP3Rbtv57sq2X9XVWQ4cdOb45/ZETIBGgVHqSNGRgNzmAH7cloHEnM6UQ/98UnHxX9G40cehYPs+AzUZZTRz1+SsZYUs4k+DUOIgdbQpGN2IcJhjmCMEcuDY0JCZDyDhKy3qb4XISVFMDDdBH2OkXgSOhqy+0IlQIah2Hn5GVYgYPuZ55csF4kdEfVXo29qtLzHqx7lnoDMAZvdJIfCD/sUz2WExz7Bhz7DvJ0haGbIkwJdVyCZIuTjTN2LEjOUEwmJA9qJHhKkxpm0tbos+gy8N4nHCE2Yh8cuGu5lwUuqgQ5AaJpPHizoSBnS/aAOgBUHcjxUosAk0XopFIFSV0rdFKL0LVHgWI5DQqZw7AvwM1VU7eSIgfoG/aAxIPU6CFgdlcBbt8aaDo6GzFGVElJnjOacL9kNVAeM96xuM8MAcMzCDswxQXVjK98l89RVEzIz906OGM/mYJfdqNd1XX/LCUGWF8dZL0VkiBZnxk1etmbX4meHSKUhHmKxnOsBIsbg5asXjjtPeeMI88VCDxx9ByhB/P7hAc/PG/z47gN+97vvNEJ43jyLpUDqD6sFVg0K5uUefbXH2DbSfV8uZviF81/fPT9iv90jn6WYzVNRirq2VneCFTQ3H3YtVEFScTCM8PabX+D126+REBy3utWxcyMLxkBzclIeSVOkoyIrGSYGPCaa9RwO5cUJASlrhIt7TUBPO1QS0/CcdYqfrHSLWWqyy+wqUFyIgitUvKOhkVRhTp0Lm3MawE9fki42F0UmS9oE+TkFomP4p+wtAV6Zrmk4cVPmbk/agev5c+ROL/sZrYsndATPBaOa7ef8AFH4jxutNSgENlQMcdx1baiWeLl8SB0nfl2ymjHGc8UOSy+2gdKaYBTFMJhihONb/f0s7KROGA1W3SaoxP0n8C8enkTtzCN2g0zRkjRaZDMgMcXKOEoFoCtYfU4dku4RwVABhwcE9RbBeBBFs9rU2D7u0OwHPPyBEt8TftiFOHQBflPN8a6d4zm8wYfwHvN6ia+2X2M25Pg6W2OWhliPPfJwhm3YYx8OqFBhO27UxRkmhlA2AgJpOgw9KaMpij7GrOc9zNxrkIvlJSvUOGpS659APCbC4ubrnpvQtqT5EdfA7M7a9ErmHdpCwFUPMqU6pRJ9FgFASBdMPnNKBowXQmVOKZRKdcm8MHgvSmwoijASd+CkhemWyf1Bltwe5zQaNZgJPgHEZBVlKccafNxGDJJTJw7KfEnIVCAu6BjofSbrOm3CaxKQG518ToQpva7r+mI7BK4F/alC+XmW7FvAtvEzMEmpLsswLpa2gfU29yfIT89cFKnyJ16A7UMi3A+HWl0DD+rb7elUxtY12QdmWiLNAGqZ140CZElPdQZBSv5Km4WIegPUmSyyjQvZ7q/rylwQY7rymXCJKn9nVuRR41K809+Fp787a3WbDPBlF+CsdvdD9uOZFWJdoj/299KDF42S+AV+Vo4HbFey6t5pFng5WW1s/gB/YjZz7A54eeKT/uxJyoavY6A0P3UwSWpujqaQQIVBSUOz6mJyYeX+n7ynvydOoxEnsexGGhJ88t70F55U5TCuzcBrJPEmg/y5T0S9Brb4GYTMnEf9FgUj+9TRSI1+ekwwIZgQjQbC5IweQSZef4xMgjaqRAUIzVSNB0qgYkRjgnBKBKLrkaFVZQ9VwXUQogpC1FGBOpmhiWbo4jm6dI46LOSX0IU5Oo7nEo6qekzZAkO2QJ8G6JMGI8cZDIo8m0mBkaMefoWJRmfGLrDre+k5ZRLgX8tjPzyExoQlPaXY3SdiELjnxJ9fJzjEpMI6BRTDkuWQPbvuegkO6P0nHC7Gt/RN7Mg/NXKFOspre68MT3JQt4vPixMckwARbalNPuKsy+kArf5BdF4NR6rmUejK6MmSYuaHvnYIrutLTggYoFU9+56u29tF73Kbjh8XaHNgm68jIj/Fkpm4jIRSBeBGWgMjMlrhTqP0ABi4f/HLA/7h1/8oat1uVwpU+MMP71CWJf7wx9/h+ekRz22DckfJ4hbvHjfYHirU3Wh2xVmM29laQjtsE3If25V74+DHuRs7sFPQY54vxDRYrla4vbtTB8SEZiyJ0YwyYaLAmePgrJslYacOSZrnSmQuWVM1IqJzGvHLLph6dUcipomjaFvO4+1824w/1Z9pU8xrAjVcuTm778ekzRr5onS666YWvVr6Lnlz0ZuJlI1tojNqqYkY8RzKzKenLDT5+yFul9Sh4KZI8N4onX2OVNlncP56x41VYPcRaFoHQnXvPLCi04EaYt+CyiB3ycuWG0W4VrrsmRU77DxHnLVLNIeCVRTHMcGkFnN1EaapxRisVVmHA9UILZtUkOlOHhaxLIYDFLTLZWJBLNrQIO5ahMMaSdAJj9AXHZoXNdp5j3o66HoOJUGFATIssZoK5MkS62SNJM8wv1uL737zYol5FuFNEmMZNgjGHFW4AD5uUcUfFJxmxUxYGCUNlAn/5tcI1q8QFkthYxReqS7oZag/c9WHyrREBhvFkcvPoJ3RfpwS2KF5bRC1b1oZLoE6JgVmzqWr4x6ZIxvS0RD1b/6+dH+ve1EX0s6/9B0Gc8y0ToA8NnWvEyfAjgW9BoY4Esh5NsuxWBRYr5bqEMxTiktRn8MAg/IlCWIVGY1wJ65fJRqy7XccLxITcxyXSt+E1uhX6eLr+qI7BI53fW5g5DZfv3wFel5RilYo++BUQEN1CpxZEQHHXqqUGXmaFUoSGLB9p4CbOlkKdFasK25MRP6TZjShdhrq8a4UMO42p6KgWfpGVM5rOevvDSEcOTChWhMmREPBocxRCvnf5nhofH3bMFz7faSGgrUmbV8zj4NLpYvlreLn8372SYQzm62sWOlofDSvscAuy2WKqEiy1pG//pxQim8GOGAf+8ea/P6ZfexoFXz8Xdv0/DXm+wg8NnBGHInbbdu8GyMQCahN03H0/Xd3ICrwHIPET5/lIedm017u1t87P9dyIcg6JzyfOk9WY7I9bYRTtp5PAkRjEDs8BMWFKM40Hl0TiTUgnc8j0JkQ9JRmZoXaZ+Lax+PSZuagQuKAMRow5A069BiWe1PTIw1moCDPHFlADYg50nSJiP4JRapglmbUuohRFBNmcYZ0tUa0poxxgGHFzhillm/0bIXJHBPpusu1xhrUthBg1o9jLjyPHLnx89rzbIly0rHi5n1BNL/LD48DKjvPzlnqSCf1Ak/+mE7MEgd69ZW4b274loMb2x8Bsp4ZYDKI+m8b7dg4jBgbfnn2DPeGNImQSlODOkfmc6BBBq/3GY7FE009k8DjCcw51ZlQOWDkdV3Xl0s7VOudX2yxG4qcD6hmes7O2FrALuayrRqnmrc1NatyZ7srUx1WXpTQZefg1GFgZS6hkQm4vWFl2stlsSpLqe7dv1vjj7PUJEdjKieyWqPyGyV6R8S7Ck3duWplQpxy7l+gWM7w9qtvVVW9/PqXCnCrmzvREV++eYP7ly/PBIpI6ytMq1wt98lUDNsOm+1eWAdiDyJK8Hog0meuJFyhSIqjO5+qIhcsyWcPWH3GHeKUn4nf5/pMWR7aV0GE9kKJjFDZx53fqQBSz4BVE8cnx3EOK0ZPDbQEgXgQS5AKZLO1859gh6UX1qJuaCd9MGXFng5+VILkDNb5I4S1ExvK1IL12hKeyqVrkRDh7TfxCb1AeGwXM6GhkiXHDsQkXNh18QmpY8p7uILAYUftBeYvNjIgqtO8KyyJGqMJfURGAqtedw842zyvUGfTGJomAbvWBZCBYkAR0uAeUbSQ6yEDpUEuW/TJgCokgHZE1IfIpwDrKUU2UQ+BUsBUmqRDYirO+800IWMHqu3VNZnyBNGLpbpgCVUfKaCzvFVCgGRhCcH6JTC7QZzPEBfE+JjkZzgZWPezl54LYkyA5ZJeDRNm8wzFPEHTdNhsSoFsD4fOuhFn4zR+VweKdD69lhd0MnRBJ2aRdQB830pB2I0P+H+iMkqnhCJjvDeJmaAAlwmm8U1yWlEj179TrXS9XGC9IjuCe4xJFFuQd9gBObGyc8YEhM+DZ7p4SrVn5pANEaMbDctko0Uycq4dguv6ghMCzbNdEmCb/PgJhew0A3ZZtJNNJSCIoCbiAygSZKh/8pddpeUSCn55ND+DBFvj3EjyPBbI8HDYakNo6wP2u63ay4GjubV9I7BQVbfouwF106PtB8wXAdY3M0Rxhpu7eySU9nNiSMv1rcYEi9UK8yUVFw3MxJkjqYw8PlIr5RzYtAIQHg4HJQekPtLP4VLaYRxST33m5pU2MhA4k58t7MwaWOjmAhHn0kmBKMkQpxESBYZM1SWPgx2M81mx79gycWJC4FHTqvpbo3X5a8bxAANMnBRImHRItpmOlKO6MCHFWoiqD1L0HStbbopATme/gIY/JiYVhjkCCf1Ya93bVmuWHxtQzGRsJ0RDa1LNsARCP6tN1wWOz1zHAtLOgrPoPd2nvrKTs50fclAxz7EjaHtLrvw4hugkOGTmUydZ3jNhBQ9OEX4idgiFBeJoJg68RHpitoE6jAnVB9nqJneDTIAAs5FcfWZzlmDwFQoCdkNgRitkAkP53BAsx4C8opgW7wee/xSREgJqVC+kUhjM1wizuUkhE1VPYSJx9Wyc9PnrJGpV8NmIgJTiTxQqangdmeSb7wCfdRmJne0HRie2UZC5MjPQmu6F6Y7oylky4GWgHKX1KIEtl1KqmZImyktAfIFpCkgVkSMuKWsSVzFpVDCbpchysoO8NoJhoSh+xnvc3sHug1PzgXgE16JwIzjfKbTPY0kCO2XXdV3/mdZf3e/mQ61Wv6sgfQDyYjAe2MdFAJ9J/TYo6WrmyGx+VswZ/XzGCv50GDLV6e31fbUgwaM4wJs3r5ArUPPvLGCzZckNoalLPcjsxPL3dodS78nj2ux3GhF898P3mM3mePXmG40JFssVZvO5sACcxXNTC9n+pFqhVO6cXPNkwkm73f74JSnjguOPy2azXVigAc1Z+DlsAxXnf6Tin5v3h5RTtnk9udHkxEtZz6nrjRMTCqMv6owdhQjcKEbjAgOf+XNKQyN1EMRC4H+70chEXAQFmGIEXW7g0IhaCcR6UOq5wdCHGEjfk1+C50bQkYgJYmYitsIteMCgtYr9uRKdjHXzYCMYI6VxYyd6nXP3yzjzCisOEMZNm8nJUeHhmL95eWWbQAs46f0fxwAdxWuIPwhMmOqkz+8SqyNQj8BIBiQGEWcbHdD0ZzQPBx0Lr12imcWUeGV/U9cLp0hjiFBfhsRnNcyxXBd3okrS/In0vCktkAcJlnGHF9Fc50xBjcmMG2tEQ49woCgPUw6zfg4oCdxfNtoSVkVa/zweXmeqjfJc0BjMzi+T/VnRqyPYdeT5n5ICju3qiiLHHqznjKMmAitDtKLRugGP7h2dOQyD3dSs/lkskFFEUTI9v7PMqnVnl02xrixzttcBsFwUuFlTfphiZBR54p0p/cxjF+lIcxXrIXbAZ2P8SK/E0sQzNVXTu2YXkl3O67quL1e6WJk+q/3eqeGdUOE+UeBmKR8Cgd5aBVJW1PtD5UyNnHp8YN0AyhwfX1yv0+v3DClv80qOFRhovv76De7vbrQh3N3dac4vBsMw4LDbac7ZNaW+f3x+xu5wwIcP7/Hhh+/lb8CSdr2+xau336r6pdPhcrW2Wat2fHYprAKhSA8ffGoX8LirssZ2u8N2u8d2u9VGsiQQ8sKEoA9mSgi4d8tGWF8cy4TaDBXYwhFJHqvC5mdW8HZCOVNAiuAMEbn0zrrZWp4n8SEGRNMT8K10E2rRlYj8z5pC4jglaLsCEWUJg+Ikc5wNiBJ2FajwZsfG7kUYsb2v7dNVc6y6uHla0PDHcnYXOQtnJn+GSlcwZvtegkYmCnTJkoG0R5CH8TG4+HawwG0Ok2HQSwtQnlmphEAeCHLGOaFn9ePOSMd1B9QpozKhftYsgDsKGp3InvbrPB9ylzQevyUp+hueLT2ICZMCdgXYHcKINqglr0sgJ4F7SOeY5RGifECU96b0yepcVr2kA9JuuUPEL7EgjIVAU6DABdbPXXHqAKYC0bITNEk0iaqfvI4UF+JxSFiQI8KmsRGiklwbER5KX0C48877m12RKEATnzqMR5yMKzS4VouZioC7uxvc3q4taXAMC3b1uNgNUGIs7RDSkQvc3CwkgMSuhq5Fb3bMdvnOdC/cqJBvN/SWnJvypx2PJQQ0+YolgEaAMaXEr+u6vlz7Y0fL8ywD3+b3X3oYz3zoOXMkkDB3/uJKCCSnapAyzSQdYttHDWvL2cyZrocMfhLvET2JG2okCWEqB1JAiLN+At3oTiixkd6C1my1wr48iDtcUV0MgRITzv4O5eHYDlcVewa6shmxKZepupXOuTkoqg0q7X3fPnRSwxes5+0zstbmu/wcBhRktde7c0Ct9Un4BVP/M8+HrgvAuFnXqWyTpcHuDKNUj58FPHO8cxQqP9f9yahDSHop87EbUxorg+Yx2rhNAVGOjLrW7AJZQhAzIVDFqDr7pPEvLf+Tw935ex9BZfLrNSYAS3eNbCg/W1+WEJgTpAe4WSD21bw/EN+S1s/4MZcT21F84P2mrsGntE0/N3C6Ssc8wViUpzvJ/+nI+pTew5lcsrANJr3L88UjY6jlO7LatwzVPBOSMEVInQh1O4iOHREn7PCwO8Gqlm8ulQUDGDpfEJ+QWcpz2X16nvQbENQ+i4SbmMw4uIIBUc3nhIm50WZH9FIjtITAKnMm23xN54SZWYGhy+Lkyfm6+mys9uc0F0uxXi2wWs7t+kgvhGJIBiyWEZVjAPA7uwpSQ/RJ2BlKQZ/BMgFLLpiYHfcgSwKPz6JPrsUw4AhmkMU6O6DXdV1fbEJA0x8zxbEA48cEflTAh9hmbfzpSW09AtU45yZ4z9wODcxGsBpRvgzqfj4nTXxtIuYkVml0YIHXVwz8eVIESRXM0kyOhVzM5o0qZ3PC3X6Hqq7wz//8LwI3Pj894Tf/9hvJJD98eK+N55tvf6FjUJWvhIabk2mxy6KVQMm2E/VP39kipASrYzBY4nIZRe73f/ydkORWOVtQ8XxqVTFursmP57Eb/EvO2tVeT+KjRXORFsJkCDHtv44WPJYQHMOChHbON0lvBhOeWAtHIX8XKN3rmcgRgwG7PI6nHzlQmEtExC1nheqAYe5lXFDUEFl1Ma+6Ic9Ny4DJBzUiLlkUn6GVrX02u44K/OKs25udB2edFeExT8cqVc4/0W84AT/5QY84RIfLMOzHeCYEdUI4GlDerKmJWUhIEVWHx5ICmk+xs2D3lDl0YmSUjZFTaQ+TzIASXs1kAgt2BTVqJbCqlo4DgymZMiagc0wGxKa4DKip9vlIYDDNx5j4m8ofpYMJgeCYzaSws2NAVfXe9fqyhNJ3E61A2O9rS3iFHTnpVxjdl3TAk/bHrCBYMMF6vRKF0F8OgmPpc6JRhtt7TFOEoNtU3UVvwa1HynXN+Hm4BxFrITr1aJiXvg/1nBv2xScDHivF1+aIjnimGofyssT1uq7rbzIh8DNpUv98IuABWp5dYCJETAiC48NsQCL+O8cA1uI0Hj8fNloNk06Yqk3sF0F7fB9P8zP+vDmW+WOxKtUyeE+FlA+6Zp2mjMeHml8UNPKJi5gNYYiS4MBiL4tm2jP7Vr1jIyopSDkSGUf9O+WSDUxYmoiSzE8ClAdD3Z+fo790+Z9nNWxzUl+lWkvzWHU6Lr1ps1iG4Gft/dAi6mMlDaKE9TYzl8DOGbDO9Ql8H+AsIThRFU9EMJ8guArVHcgptnkUAs/ViJSVqlQSHU/cB30R9xza/ExwScfj5WKVEJzhR0RH5XXrLjqnTVUqWJ2ciY4/8QkBT2TPoxqNn+2fv+eJNqkz6AfP7idNDIffrTXeszvlxZbsgM46B/Y6kRgVIXo6GTq3RVIhvcgOJZSY7FlDw8Yw/FueEXofEtmi/ppTfuyUEDhqqs4tOweUlWZSYPc7gyPPySXnlJofGO158qZRAqv2xhSwj8iiwNrr/nd9weClju3vzCmQYzw+T1aJn95Tr6WR0+n8J1RaDBisO9SNPX9cZKoYqJFZjzPuigyDZMdktFbdZTwGvo4SJwOShhIncxLXZEJ0nRRR+We/p5yLkfEbRdD4VdWfd59e13X9R13B9BfczX/84x/x7bff/r9zRH+j6w9/+AO++eabv/jnr+f0f76u5/TnX9dz+v/9Ob2u6/qbTghYPX///fdYLpcX0+z+sy2ePnYRvvrqq7/KDvV6Tv/9dT2nP/+6ntP/OOf0uq7rbzohuK7ruq7ruq7ruq7/3Oua1l7XdV3XdV3XdV3XNSG4ruu6ruu6ruu6rmtCcF3XdV3XdV3XdV3XhOC6ruu6ruu6ruu6uK4JwXVd13Vd13Vd13VdE4Lruq7ruq7ruq7ruiYE13Vd13Vd13VduC7g/wb3ITbavCr6tAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 32, 32, 3]) torch.Size([10000, 1]) tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) torch.Size([2000, 32, 32, 3]) torch.Size([2000, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1024]) torch.Size([2000, 1024])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "transform_data = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) # Apply the (x - mean)/var operation on the components of the data # if x is in [0,1] then Normalise(x) is in [-1,1] # is applied on the three channels RGB\n",
    "])\n",
    "\n",
    "# Data import\n",
    "dtype = torch.float32\n",
    "trainset = torchvision.datasets.CIFAR10(root = './datas', train= True, download = True, transform = transform_data)\n",
    "validset = torchvision.datasets.CIFAR10(root = './datas', train = False, download = True, transform = transform_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 1)\n",
    "\n",
    "x_train, y_train_raw, x_valid, y_valid_raw = torch.tensor(trainset.data), torch.tensor(trainset.targets), torch.tensor(validset.data), torch.tensor(validset.targets)\n",
    "\n",
    "# Modification du format des données shape (n_data,1) -> (n_data, n_classes)\n",
    "y_train = torch.zeros((y_train_raw.shape[0], torch.max(y_train_raw)+1))\n",
    "for i, y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros((y_valid_raw.shape[0], torch.max(y_valid_raw)+1))\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "print(y_train.shape, y_valid.shape)\n",
    "\n",
    "# Binary reduction of the classes # To avoid using softmax, we regroup classes in two classes\n",
    "class_binary_reduction = True\n",
    "determination_des_classes = True\n",
    "black_and_white_images = True\n",
    "\n",
    "if class_binary_reduction :\n",
    "    if determination_des_classes :\n",
    "        # Determination des classes\n",
    "        class_list = []\n",
    "        class_index = 0\n",
    "        for i in range (x_train.shape[0]):\n",
    "            if y_train[i, class_index] == 1:\n",
    "                class_list.append(x_train[i])\n",
    "                class_index += 1\n",
    "            if len(class_list) == len(y_train[0]):\n",
    "                break\n",
    "        for i, image in enumerate(class_list):\n",
    "            plt.subplot(2, int(len(class_list)/2+1),i+1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "                \n",
    "    # classe1 = [0, 1, 8, 9]  # vehicles\n",
    "    # classe2 = [2, 3, 4, 5]  # animals\n",
    "    \n",
    "    classe1 = [4]  # elk\n",
    "    classe2 = [7]  # horse\n",
    "\n",
    "    # Création des masques pour les échantillons appartenant à ces classes\n",
    "    mask_classe1_train = y_train[:, classe1].sum(dim=1) > 0  # True si appartient à classe1\n",
    "    mask_classe2_train = y_train[:, classe2].sum(dim=1) > 0  # True si appartient à classe2\n",
    "    \n",
    "    mask_classe1_valid = y_valid[:, classe1].sum(dim=1) > 0\n",
    "    mask_classe2_valid = y_valid[:, classe2].sum(dim=1) > 0\n",
    "    \n",
    "    # Filtrage des exemples concernés\n",
    "    mask_train = torch.logical_or(mask_classe1_train, mask_classe2_train)\n",
    "    mask_valid = torch.logical_or(mask_classe1_valid, mask_classe2_valid)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid]\n",
    "\n",
    "    # Création du vecteur de labels binaires (1 pour classe1, 0 pour classe2)\n",
    "    y_train = (y_train[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    y_valid = (y_valid[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    \n",
    "    # Avec tanh\n",
    "    \n",
    "    # x_train = 2*(x_train-0.5)\n",
    "    # y_train = 2*(y_train-0.5)\n",
    "    # x_valid = 2*(x_valid-0.5)\n",
    "    # y_valid = 2*(y_valid-0.5)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_train[0:10], x_valid.shape, y_valid.shape)\n",
    "\n",
    "if black_and_white_images :\n",
    "    x_train = 0.299*x_train[:,:,:,0] + 0.587*x_train[:,:,:,1] + 0.114*x_train[:,:,:,2]\n",
    "    x_valid = 0.299*x_valid[:,:,:,0] + 0.587*x_valid[:,:,:,1] + 0.114*x_valid[:,:,:,2]\n",
    "    for i, image in enumerate(x_train[0:10]):\n",
    "        plt.subplot(2, int(len(x_train[0:10])/2+1),i+1)\n",
    "        plt.imshow(image, cmap = 'grey')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    x_train, x_valid = (x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])).to(dtype), x_valid.reshape(x_valid.shape[0], x_valid.shape[1]*x_valid.shape[2]).to(dtype)\n",
    "    print(x_train.shape, x_valid.shape)\n",
    "\n",
    "else :    \n",
    "    x_train, x_valid = (x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]*x_train.shape[3])).to(dtype), x_valid.reshape(x_valid.shape[0], x_valid.shape[1]*x_valid.shape[2]*x_valid.shape[3]).to(dtype)\n",
    "    print(x_train.shape, x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45060724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on :  mps\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Computing on : \", device)\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0).to(device),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).type(dtype).to(device)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    s = s.to(device)\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return torch.exp(-x)/((1 + torch.exp(-x))**2)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (torch.tanh(x)**2)).to(device)\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output); grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = (torch.einsum('no,noz->nz',grad_output,softmax_derivative(output))).to(dtype) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "\n",
    "class binary_classification_two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(1, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = torch.sigmoid(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            #print(output[0:5], z2[0:5], h1[0:5], z1[0:5])\n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                print(\"Output\", output[20:22])\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = grad_output*sigmoid_derivative(z2); grad_z2  = grad_z2.to(dtype) # shape(n_data, 1)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # if i == 250:\n",
    "            #     break\n",
    "\n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "class binary_classification_three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(1, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, 1)\n",
    "        output = sigmoid(z3) # output layer # shape (n_data, 1)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = grad_output*sigmoid_derivative(z3).to(dtype) # shape (n_data, 1) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "            \n",
    "            # if i == 12000:\n",
    "            #     break\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36584c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2_layer = binary_classification_two_layer_NN(1024, 512, eps_init = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57236d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 1.6, the number of datas used for the training is 6553600.000000005 and the number of iterations is 65536.\n",
      "Output tensor([[0.4966],\n",
      "        [0.5048]], device='mps:0')\n",
      "Iteration 0 Training loss 0.12480664998292923 Validation loss 0.12459351122379303 Accuracy 0.5505000352859497\n",
      "Output tensor([[0.4999],\n",
      "        [0.5108]], device='mps:0')\n",
      "Iteration 10 Training loss 0.1248476505279541 Validation loss 0.12446843832731247 Accuracy 0.5135000348091125\n",
      "Output tensor([[0.4985],\n",
      "        [0.4959]], device='mps:0')\n",
      "Iteration 20 Training loss 0.12377745658159256 Validation loss 0.12441219389438629 Accuracy 0.5220000147819519\n",
      "Output tensor([[0.4746],\n",
      "        [0.4827]], device='mps:0')\n",
      "Iteration 30 Training loss 0.12355556339025497 Validation loss 0.12427321821451187 Accuracy 0.5015000104904175\n",
      "Output tensor([[0.4725],\n",
      "        [0.4792]], device='mps:0')\n",
      "Iteration 40 Training loss 0.1246996819972992 Validation loss 0.12418034672737122 Accuracy 0.5\n",
      "Output tensor([[0.5054],\n",
      "        [0.4892]], device='mps:0')\n",
      "Iteration 50 Training loss 0.12509547173976898 Validation loss 0.1241317167878151 Accuracy 0.5015000104904175\n",
      "Output tensor([[0.4760],\n",
      "        [0.4914]], device='mps:0')\n",
      "Iteration 60 Training loss 0.12303023785352707 Validation loss 0.12408021092414856 Accuracy 0.5035000443458557\n",
      "Output tensor([[0.4767],\n",
      "        [0.4947]], device='mps:0')\n",
      "Iteration 70 Training loss 0.12424616515636444 Validation loss 0.12400887161493301 Accuracy 0.5060000419616699\n",
      "Output tensor([[0.4786],\n",
      "        [0.4675]], device='mps:0')\n",
      "Iteration 80 Training loss 0.12592525780200958 Validation loss 0.1239558756351471 Accuracy 0.5045000314712524\n",
      "Output tensor([[0.4638],\n",
      "        [0.4748]], device='mps:0')\n",
      "Iteration 90 Training loss 0.12300696969032288 Validation loss 0.12388566881418228 Accuracy 0.5045000314712524\n",
      "Output tensor([[0.4793],\n",
      "        [0.4808]], device='mps:0')\n",
      "Iteration 100 Training loss 0.12310101091861725 Validation loss 0.1238410547375679 Accuracy 0.5024999976158142\n",
      "Output tensor([[0.4913],\n",
      "        [0.4896]], device='mps:0')\n",
      "Iteration 110 Training loss 0.1234351322054863 Validation loss 0.12379501760005951 Accuracy 0.5085000395774841\n",
      "Output tensor([[0.4785],\n",
      "        [0.4762]], device='mps:0')\n",
      "Iteration 120 Training loss 0.12366527318954468 Validation loss 0.12374848127365112 Accuracy 0.5010000467300415\n",
      "Output tensor([[0.4443],\n",
      "        [0.5187]], device='mps:0')\n",
      "Iteration 130 Training loss 0.1247667670249939 Validation loss 0.12370244413614273 Accuracy 0.5125000476837158\n",
      "Output tensor([[0.4882],\n",
      "        [0.4843]], device='mps:0')\n",
      "Iteration 140 Training loss 0.12458731234073639 Validation loss 0.12364806979894638 Accuracy 0.5130000114440918\n",
      "Output tensor([[0.4981],\n",
      "        [0.5108]], device='mps:0')\n",
      "Iteration 150 Training loss 0.12362290173768997 Validation loss 0.12363661825656891 Accuracy 0.5295000076293945\n",
      "Output tensor([[0.4986],\n",
      "        [0.4763]], device='mps:0')\n",
      "Iteration 160 Training loss 0.1238134354352951 Validation loss 0.12360364198684692 Accuracy 0.5385000109672546\n",
      "Output tensor([[0.5106],\n",
      "        [0.4720]], device='mps:0')\n",
      "Iteration 170 Training loss 0.12292958796024323 Validation loss 0.12356320023536682 Accuracy 0.5385000109672546\n",
      "Output tensor([[0.5024],\n",
      "        [0.4987]], device='mps:0')\n",
      "Iteration 180 Training loss 0.12355756759643555 Validation loss 0.12359891086816788 Accuracy 0.6050000190734863\n",
      "Output tensor([[0.4967],\n",
      "        [0.4864]], device='mps:0')\n",
      "Iteration 190 Training loss 0.12424415349960327 Validation loss 0.12351811677217484 Accuracy 0.5890000462532043\n",
      "Output tensor([[0.4895],\n",
      "        [0.4852]], device='mps:0')\n",
      "Iteration 200 Training loss 0.12352767586708069 Validation loss 0.12343631684780121 Accuracy 0.5580000281333923\n",
      "Output tensor([[0.4707],\n",
      "        [0.4814]], device='mps:0')\n",
      "Iteration 210 Training loss 0.12311931699514389 Validation loss 0.12334641069173813 Accuracy 0.5365000367164612\n",
      "Output tensor([[0.4868],\n",
      "        [0.5008]], device='mps:0')\n",
      "Iteration 220 Training loss 0.12345998734235764 Validation loss 0.12326843291521072 Accuracy 0.5160000324249268\n",
      "Output tensor([[0.4348],\n",
      "        [0.4992]], device='mps:0')\n",
      "Iteration 230 Training loss 0.12416146695613861 Validation loss 0.12321440875530243 Accuracy 0.5200000405311584\n",
      "Output tensor([[0.4959],\n",
      "        [0.4677]], device='mps:0')\n",
      "Iteration 240 Training loss 0.12396541237831116 Validation loss 0.1231592446565628 Accuracy 0.518500030040741\n",
      "Output tensor([[0.4938],\n",
      "        [0.4905]], device='mps:0')\n",
      "Iteration 250 Training loss 0.12231510132551193 Validation loss 0.1230921745300293 Accuracy 0.5145000219345093\n",
      "Output tensor([[0.4936],\n",
      "        [0.4660]], device='mps:0')\n",
      "Iteration 260 Training loss 0.12089696526527405 Validation loss 0.12304554134607315 Accuracy 0.515500009059906\n",
      "Output tensor([[0.4948],\n",
      "        [0.4467]], device='mps:0')\n",
      "Iteration 270 Training loss 0.12357345223426819 Validation loss 0.12299120426177979 Accuracy 0.5130000114440918\n",
      "Output tensor([[0.4852],\n",
      "        [0.4929]], device='mps:0')\n",
      "Iteration 280 Training loss 0.12468608468770981 Validation loss 0.12295017391443253 Accuracy 0.5190000534057617\n",
      "Output tensor([[0.4868],\n",
      "        [0.5259]], device='mps:0')\n",
      "Iteration 290 Training loss 0.12329085916280746 Validation loss 0.12290811538696289 Accuracy 0.5205000042915344\n",
      "Output tensor([[0.4861],\n",
      "        [0.4640]], device='mps:0')\n",
      "Iteration 300 Training loss 0.1228514239192009 Validation loss 0.12292426079511642 Accuracy 0.5725000500679016\n",
      "Output tensor([[0.4721],\n",
      "        [0.4836]], device='mps:0')\n",
      "Iteration 310 Training loss 0.12388665974140167 Validation loss 0.1228707805275917 Accuracy 0.5605000257492065\n",
      "Output tensor([[0.4914],\n",
      "        [0.4891]], device='mps:0')\n",
      "Iteration 320 Training loss 0.12293323129415512 Validation loss 0.1227869763970375 Accuracy 0.5290000438690186\n",
      "Output tensor([[0.5085],\n",
      "        [0.5177]], device='mps:0')\n",
      "Iteration 330 Training loss 0.12439988553524017 Validation loss 0.12275389581918716 Accuracy 0.5375000238418579\n",
      "Output tensor([[0.5070],\n",
      "        [0.4741]], device='mps:0')\n",
      "Iteration 340 Training loss 0.12361647933721542 Validation loss 0.1227223202586174 Accuracy 0.5545000433921814\n",
      "Output tensor([[0.5049],\n",
      "        [0.4873]], device='mps:0')\n",
      "Iteration 350 Training loss 0.12382199615240097 Validation loss 0.12268497049808502 Accuracy 0.5575000047683716\n",
      "Output tensor([[0.4739],\n",
      "        [0.4858]], device='mps:0')\n",
      "Iteration 360 Training loss 0.1238396018743515 Validation loss 0.12263555079698563 Accuracy 0.5520000457763672\n",
      "Output tensor([[0.4744],\n",
      "        [0.5120]], device='mps:0')\n",
      "Iteration 370 Training loss 0.12300188094377518 Validation loss 0.12257170677185059 Accuracy 0.5335000157356262\n",
      "Output tensor([[0.4663],\n",
      "        [0.4530]], device='mps:0')\n",
      "Iteration 380 Training loss 0.12093892693519592 Validation loss 0.12256776541471481 Accuracy 0.5670000314712524\n",
      "Output tensor([[0.4776],\n",
      "        [0.5222]], device='mps:0')\n",
      "Iteration 390 Training loss 0.12045947462320328 Validation loss 0.12248483300209045 Accuracy 0.5275000333786011\n",
      "Output tensor([[0.4815],\n",
      "        [0.4701]], device='mps:0')\n",
      "Iteration 400 Training loss 0.12254954874515533 Validation loss 0.12243732064962387 Accuracy 0.5320000052452087\n",
      "Output tensor([[0.4990],\n",
      "        [0.5085]], device='mps:0')\n",
      "Iteration 410 Training loss 0.12403640151023865 Validation loss 0.12240607291460037 Accuracy 0.5560000538825989\n",
      "Output tensor([[0.4538],\n",
      "        [0.5473]], device='mps:0')\n",
      "Iteration 420 Training loss 0.1226116418838501 Validation loss 0.1224464476108551 Accuracy 0.6030000448226929\n",
      "Output tensor([[0.4320],\n",
      "        [0.5024]], device='mps:0')\n",
      "Iteration 430 Training loss 0.12399052083492279 Validation loss 0.12238523364067078 Accuracy 0.6030000448226929\n",
      "Output tensor([[0.4775],\n",
      "        [0.4998]], device='mps:0')\n",
      "Iteration 440 Training loss 0.12266822904348373 Validation loss 0.12231262773275375 Accuracy 0.5795000195503235\n",
      "Output tensor([[0.5145],\n",
      "        [0.4426]], device='mps:0')\n",
      "Iteration 450 Training loss 0.12335746735334396 Validation loss 0.12224023044109344 Accuracy 0.5430000424385071\n",
      "Output tensor([[0.4617],\n",
      "        [0.5060]], device='mps:0')\n",
      "Iteration 460 Training loss 0.12346827983856201 Validation loss 0.12218588590621948 Accuracy 0.5405000448226929\n",
      "Output tensor([[0.5079],\n",
      "        [0.4670]], device='mps:0')\n",
      "Iteration 470 Training loss 0.1217028871178627 Validation loss 0.12214229255914688 Accuracy 0.5420000553131104\n",
      "Output tensor([[0.5252],\n",
      "        [0.4657]], device='mps:0')\n",
      "Iteration 480 Training loss 0.12323123961687088 Validation loss 0.12209893763065338 Accuracy 0.5460000038146973\n",
      "Output tensor([[0.4845],\n",
      "        [0.4863]], device='mps:0')\n",
      "Iteration 490 Training loss 0.12307185679674149 Validation loss 0.12205860018730164 Accuracy 0.550000011920929\n",
      "Output tensor([[0.5058],\n",
      "        [0.4961]], device='mps:0')\n",
      "Iteration 500 Training loss 0.12410048395395279 Validation loss 0.12206844240427017 Accuracy 0.6000000238418579\n",
      "Output tensor([[0.4288],\n",
      "        [0.4918]], device='mps:0')\n",
      "Iteration 510 Training loss 0.12233461439609528 Validation loss 0.12213527411222458 Accuracy 0.6265000104904175\n",
      "Output tensor([[0.4771],\n",
      "        [0.5540]], device='mps:0')\n",
      "Iteration 520 Training loss 0.12028027325868607 Validation loss 0.12206806242465973 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5149],\n",
      "        [0.4996]], device='mps:0')\n",
      "Iteration 530 Training loss 0.12355420738458633 Validation loss 0.12200818955898285 Accuracy 0.6155000329017639\n",
      "Output tensor([[0.5251],\n",
      "        [0.5145]], device='mps:0')\n",
      "Iteration 540 Training loss 0.12242180854082108 Validation loss 0.12199937552213669 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.5169],\n",
      "        [0.5362]], device='mps:0')\n",
      "Iteration 550 Training loss 0.12201344966888428 Validation loss 0.12194977700710297 Accuracy 0.624500036239624\n",
      "Output tensor([[0.4803],\n",
      "        [0.5144]], device='mps:0')\n",
      "Iteration 560 Training loss 0.12404082715511322 Validation loss 0.12190207093954086 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5137],\n",
      "        [0.4948]], device='mps:0')\n",
      "Iteration 570 Training loss 0.12200409919023514 Validation loss 0.12187333405017853 Accuracy 0.625\n",
      "Output tensor([[0.4986],\n",
      "        [0.4985]], device='mps:0')\n",
      "Iteration 580 Training loss 0.12277867645025253 Validation loss 0.12174367904663086 Accuracy 0.6020000576972961\n",
      "Output tensor([[0.4722],\n",
      "        [0.4906]], device='mps:0')\n",
      "Iteration 590 Training loss 0.12371939420700073 Validation loss 0.12168397754430771 Accuracy 0.5825000405311584\n",
      "Output tensor([[0.5075],\n",
      "        [0.4436]], device='mps:0')\n",
      "Iteration 600 Training loss 0.12200284004211426 Validation loss 0.12164155393838882 Accuracy 0.5770000219345093\n",
      "Output tensor([[0.5308],\n",
      "        [0.5091]], device='mps:0')\n",
      "Iteration 610 Training loss 0.12357711791992188 Validation loss 0.12161092460155487 Accuracy 0.5785000324249268\n",
      "Output tensor([[0.4706],\n",
      "        [0.4740]], device='mps:0')\n",
      "Iteration 620 Training loss 0.1233476772904396 Validation loss 0.12158312648534775 Accuracy 0.578000009059906\n",
      "Output tensor([[0.5112],\n",
      "        [0.4418]], device='mps:0')\n",
      "Iteration 630 Training loss 0.12450093030929565 Validation loss 0.12157974392175674 Accuracy 0.6055000424385071\n",
      "Output tensor([[0.5289],\n",
      "        [0.5222]], device='mps:0')\n",
      "Iteration 640 Training loss 0.12103650718927383 Validation loss 0.12152449786663055 Accuracy 0.5825000405311584\n",
      "Output tensor([[0.5064],\n",
      "        [0.4737]], device='mps:0')\n",
      "Iteration 650 Training loss 0.12293092161417007 Validation loss 0.12147831171751022 Accuracy 0.5750000476837158\n",
      "Output tensor([[0.4631],\n",
      "        [0.4821]], device='mps:0')\n",
      "Iteration 660 Training loss 0.12035218626260757 Validation loss 0.12145131826400757 Accuracy 0.5645000338554382\n",
      "Output tensor([[0.5043],\n",
      "        [0.4633]], device='mps:0')\n",
      "Iteration 670 Training loss 0.12028399854898453 Validation loss 0.12141778320074081 Accuracy 0.5649999976158142\n",
      "Output tensor([[0.4385],\n",
      "        [0.5312]], device='mps:0')\n",
      "Iteration 680 Training loss 0.11820594221353531 Validation loss 0.12138760834932327 Accuracy 0.5720000267028809\n",
      "Output tensor([[0.5114],\n",
      "        [0.4109]], device='mps:0')\n",
      "Iteration 690 Training loss 0.11976569890975952 Validation loss 0.12135958671569824 Accuracy 0.5845000147819519\n",
      "Output tensor([[0.4960],\n",
      "        [0.4928]], device='mps:0')\n",
      "Iteration 700 Training loss 0.12266350537538528 Validation loss 0.12133679538965225 Accuracy 0.5925000309944153\n",
      "Output tensor([[0.4888],\n",
      "        [0.4878]], device='mps:0')\n",
      "Iteration 710 Training loss 0.12321782112121582 Validation loss 0.12130466103553772 Accuracy 0.5885000228881836\n",
      "Output tensor([[0.4897],\n",
      "        [0.4932]], device='mps:0')\n",
      "Iteration 720 Training loss 0.12173344939947128 Validation loss 0.12126877158880234 Accuracy 0.5835000276565552\n",
      "Output tensor([[0.5198],\n",
      "        [0.4991]], device='mps:0')\n",
      "Iteration 730 Training loss 0.11962636560201645 Validation loss 0.12127687782049179 Accuracy 0.612000048160553\n",
      "Output tensor([[0.4675],\n",
      "        [0.5339]], device='mps:0')\n",
      "Iteration 740 Training loss 0.12301690131425858 Validation loss 0.12120269238948822 Accuracy 0.5850000381469727\n",
      "Output tensor([[0.4990],\n",
      "        [0.4897]], device='mps:0')\n",
      "Iteration 750 Training loss 0.1228034719824791 Validation loss 0.1211913526058197 Accuracy 0.6015000343322754\n",
      "Output tensor([[0.5023],\n",
      "        [0.4825]], device='mps:0')\n",
      "Iteration 760 Training loss 0.12225223332643509 Validation loss 0.1211332455277443 Accuracy 0.5720000267028809\n",
      "Output tensor([[0.4882],\n",
      "        [0.4667]], device='mps:0')\n",
      "Iteration 770 Training loss 0.12065524607896805 Validation loss 0.12110132724046707 Accuracy 0.5680000185966492\n",
      "Output tensor([[0.5239],\n",
      "        [0.4910]], device='mps:0')\n",
      "Iteration 780 Training loss 0.11731546372175217 Validation loss 0.1210620328783989 Accuracy 0.5700000524520874\n",
      "Output tensor([[0.4534],\n",
      "        [0.5235]], device='mps:0')\n",
      "Iteration 790 Training loss 0.12238143384456635 Validation loss 0.12102757394313812 Accuracy 0.5560000538825989\n",
      "Output tensor([[0.4310],\n",
      "        [0.5074]], device='mps:0')\n",
      "Iteration 800 Training loss 0.12048936635255814 Validation loss 0.12100444734096527 Accuracy 0.5515000224113464\n",
      "Output tensor([[0.4852],\n",
      "        [0.5297]], device='mps:0')\n",
      "Iteration 810 Training loss 0.12177569419145584 Validation loss 0.1209593266248703 Accuracy 0.5575000047683716\n",
      "Output tensor([[0.4796],\n",
      "        [0.5116]], device='mps:0')\n",
      "Iteration 820 Training loss 0.12228838354349136 Validation loss 0.12092258036136627 Accuracy 0.5800000429153442\n",
      "Output tensor([[0.4968],\n",
      "        [0.5440]], device='mps:0')\n",
      "Iteration 830 Training loss 0.12266232073307037 Validation loss 0.12092483788728714 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5004],\n",
      "        [0.4311]], device='mps:0')\n",
      "Iteration 840 Training loss 0.12053034454584122 Validation loss 0.12087950855493546 Accuracy 0.6050000190734863\n",
      "Output tensor([[0.4975],\n",
      "        [0.4400]], device='mps:0')\n",
      "Iteration 850 Training loss 0.12197764962911606 Validation loss 0.12085225433111191 Accuracy 0.6105000376701355\n",
      "Output tensor([[0.4892],\n",
      "        [0.5123]], device='mps:0')\n",
      "Iteration 860 Training loss 0.12227392941713333 Validation loss 0.12087324261665344 Accuracy 0.6255000233650208\n",
      "Output tensor([[0.4964],\n",
      "        [0.4882]], device='mps:0')\n",
      "Iteration 870 Training loss 0.1212499812245369 Validation loss 0.12079210579395294 Accuracy 0.6145000457763672\n",
      "Output tensor([[0.5013],\n",
      "        [0.4839]], device='mps:0')\n",
      "Iteration 880 Training loss 0.12394416332244873 Validation loss 0.12076106667518616 Accuracy 0.6170000433921814\n",
      "Output tensor([[0.5295],\n",
      "        [0.4565]], device='mps:0')\n",
      "Iteration 890 Training loss 0.12065833806991577 Validation loss 0.12072743475437164 Accuracy 0.6130000352859497\n",
      "Output tensor([[0.5010],\n",
      "        [0.5565]], device='mps:0')\n",
      "Iteration 900 Training loss 0.11819305270910263 Validation loss 0.12070859223604202 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5043],\n",
      "        [0.5029]], device='mps:0')\n",
      "Iteration 910 Training loss 0.12137486785650253 Validation loss 0.1206619143486023 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4796],\n",
      "        [0.5039]], device='mps:0')\n",
      "Iteration 920 Training loss 0.12147002667188644 Validation loss 0.12063252180814743 Accuracy 0.6185000538825989\n",
      "Output tensor([[0.4764],\n",
      "        [0.4638]], device='mps:0')\n",
      "Iteration 930 Training loss 0.12036238610744476 Validation loss 0.12061523646116257 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4912],\n",
      "        [0.5288]], device='mps:0')\n",
      "Iteration 940 Training loss 0.12364606559276581 Validation loss 0.12056761980056763 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5135],\n",
      "        [0.5090]], device='mps:0')\n",
      "Iteration 950 Training loss 0.12143943458795547 Validation loss 0.1206326112151146 Accuracy 0.6295000314712524\n",
      "Output tensor([[0.5075],\n",
      "        [0.5153]], device='mps:0')\n",
      "Iteration 960 Training loss 0.1193353459239006 Validation loss 0.12053214013576508 Accuracy 0.6255000233650208\n",
      "Output tensor([[0.3971],\n",
      "        [0.4366]], device='mps:0')\n",
      "Iteration 970 Training loss 0.1203804761171341 Validation loss 0.12051437795162201 Accuracy 0.627500057220459\n",
      "Output tensor([[0.4872],\n",
      "        [0.5509]], device='mps:0')\n",
      "Iteration 980 Training loss 0.12035367637872696 Validation loss 0.12044662982225418 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.5371],\n",
      "        [0.5094]], device='mps:0')\n",
      "Iteration 990 Training loss 0.1192970797419548 Validation loss 0.12038107961416245 Accuracy 0.6010000109672546\n",
      "Output tensor([[0.4840],\n",
      "        [0.4725]], device='mps:0')\n",
      "Iteration 1000 Training loss 0.11833741515874863 Validation loss 0.12034031003713608 Accuracy 0.5840000510215759\n",
      "Output tensor([[0.4115],\n",
      "        [0.5102]], device='mps:0')\n",
      "Iteration 1010 Training loss 0.11782599985599518 Validation loss 0.12031250447034836 Accuracy 0.6100000143051147\n",
      "Output tensor([[0.4998],\n",
      "        [0.5179]], device='mps:0')\n",
      "Iteration 1020 Training loss 0.1220984235405922 Validation loss 0.12027944624423981 Accuracy 0.6080000400543213\n",
      "Output tensor([[0.4063],\n",
      "        [0.5156]], device='mps:0')\n",
      "Iteration 1030 Training loss 0.12113931030035019 Validation loss 0.12024983763694763 Accuracy 0.609000027179718\n",
      "Output tensor([[0.4224],\n",
      "        [0.5027]], device='mps:0')\n",
      "Iteration 1040 Training loss 0.12070981413125992 Validation loss 0.12022261321544647 Accuracy 0.5705000162124634\n",
      "Output tensor([[0.4808],\n",
      "        [0.5013]], device='mps:0')\n",
      "Iteration 1050 Training loss 0.1208452358841896 Validation loss 0.12018602341413498 Accuracy 0.5755000114440918\n",
      "Output tensor([[0.4496],\n",
      "        [0.4791]], device='mps:0')\n",
      "Iteration 1060 Training loss 0.12339477241039276 Validation loss 0.12014647573232651 Accuracy 0.5870000123977661\n",
      "Output tensor([[0.4940],\n",
      "        [0.4299]], device='mps:0')\n",
      "Iteration 1070 Training loss 0.11702071130275726 Validation loss 0.12013481557369232 Accuracy 0.5695000290870667\n",
      "Output tensor([[0.4920],\n",
      "        [0.4710]], device='mps:0')\n",
      "Iteration 1080 Training loss 0.11902567744255066 Validation loss 0.12010776251554489 Accuracy 0.5665000081062317\n",
      "Output tensor([[0.4442],\n",
      "        [0.5107]], device='mps:0')\n",
      "Iteration 1090 Training loss 0.11941935867071152 Validation loss 0.12005294114351273 Accuracy 0.5835000276565552\n",
      "Output tensor([[0.4728],\n",
      "        [0.4552]], device='mps:0')\n",
      "Iteration 1100 Training loss 0.12023252248764038 Validation loss 0.12002579122781754 Accuracy 0.5850000381469727\n",
      "Output tensor([[0.4855],\n",
      "        [0.5512]], device='mps:0')\n",
      "Iteration 1110 Training loss 0.11920156329870224 Validation loss 0.11999604105949402 Accuracy 0.5825000405311584\n",
      "Output tensor([[0.4142],\n",
      "        [0.5143]], device='mps:0')\n",
      "Iteration 1120 Training loss 0.1252802312374115 Validation loss 0.11997334659099579 Accuracy 0.5800000429153442\n",
      "Output tensor([[0.4297],\n",
      "        [0.5204]], device='mps:0')\n",
      "Iteration 1130 Training loss 0.11933576315641403 Validation loss 0.11993605643510818 Accuracy 0.5920000076293945\n",
      "Output tensor([[0.5425],\n",
      "        [0.4048]], device='mps:0')\n",
      "Iteration 1140 Training loss 0.12136794626712799 Validation loss 0.11990955471992493 Accuracy 0.5940000414848328\n",
      "Output tensor([[0.4491],\n",
      "        [0.4517]], device='mps:0')\n",
      "Iteration 1150 Training loss 0.1195085197687149 Validation loss 0.11987859010696411 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.4721],\n",
      "        [0.5124]], device='mps:0')\n",
      "Iteration 1160 Training loss 0.1244269534945488 Validation loss 0.11985120177268982 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.3802],\n",
      "        [0.5192]], device='mps:0')\n",
      "Iteration 1170 Training loss 0.12129838764667511 Validation loss 0.1198335811495781 Accuracy 0.6210000514984131\n",
      "Output tensor([[0.5511],\n",
      "        [0.5103]], device='mps:0')\n",
      "Iteration 1180 Training loss 0.12015172094106674 Validation loss 0.11982603371143341 Accuracy 0.627500057220459\n",
      "Output tensor([[0.5067],\n",
      "        [0.4392]], device='mps:0')\n",
      "Iteration 1190 Training loss 0.11975984275341034 Validation loss 0.11978185921907425 Accuracy 0.6185000538825989\n",
      "Output tensor([[0.4608],\n",
      "        [0.4767]], device='mps:0')\n",
      "Iteration 1200 Training loss 0.120541512966156 Validation loss 0.11975540965795517 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.5152],\n",
      "        [0.5182]], device='mps:0')\n",
      "Iteration 1210 Training loss 0.1208229586482048 Validation loss 0.11973290145397186 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4483],\n",
      "        [0.5767]], device='mps:0')\n",
      "Iteration 1220 Training loss 0.11959580332040787 Validation loss 0.119719959795475 Accuracy 0.625\n",
      "Output tensor([[0.4394],\n",
      "        [0.5122]], device='mps:0')\n",
      "Iteration 1230 Training loss 0.12293490022420883 Validation loss 0.1197362095117569 Accuracy 0.6305000185966492\n",
      "Output tensor([[0.5082],\n",
      "        [0.5658]], device='mps:0')\n",
      "Iteration 1240 Training loss 0.12169518321752548 Validation loss 0.11969985067844391 Accuracy 0.6285000443458557\n",
      "Output tensor([[0.4829],\n",
      "        [0.5139]], device='mps:0')\n",
      "Iteration 1250 Training loss 0.11784423887729645 Validation loss 0.11970165371894836 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.5562],\n",
      "        [0.5095]], device='mps:0')\n",
      "Iteration 1260 Training loss 0.11983225494623184 Validation loss 0.11962620913982391 Accuracy 0.627500057220459\n",
      "Output tensor([[0.4833],\n",
      "        [0.5112]], device='mps:0')\n",
      "Iteration 1270 Training loss 0.12184463441371918 Validation loss 0.11959519237279892 Accuracy 0.6285000443458557\n",
      "Output tensor([[0.5096],\n",
      "        [0.4885]], device='mps:0')\n",
      "Iteration 1280 Training loss 0.11972221732139587 Validation loss 0.11959049850702286 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.5364],\n",
      "        [0.4614]], device='mps:0')\n",
      "Iteration 1290 Training loss 0.11941017955541611 Validation loss 0.11963344365358353 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.5364],\n",
      "        [0.5116]], device='mps:0')\n",
      "Iteration 1300 Training loss 0.1185210794210434 Validation loss 0.11976651847362518 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.5458],\n",
      "        [0.4159]], device='mps:0')\n",
      "Iteration 1310 Training loss 0.11877535283565521 Validation loss 0.11973974853754044 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.4431],\n",
      "        [0.4394]], device='mps:0')\n",
      "Iteration 1320 Training loss 0.12375911325216293 Validation loss 0.11959889531135559 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.5282],\n",
      "        [0.5002]], device='mps:0')\n",
      "Iteration 1330 Training loss 0.11991985142230988 Validation loss 0.11961304396390915 Accuracy 0.6365000009536743\n",
      "Output tensor([[0.4337],\n",
      "        [0.5432]], device='mps:0')\n",
      "Iteration 1340 Training loss 0.1162756159901619 Validation loss 0.11953174322843552 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.4730],\n",
      "        [0.4054]], device='mps:0')\n",
      "Iteration 1350 Training loss 0.12352988123893738 Validation loss 0.11949317157268524 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.5097],\n",
      "        [0.4940]], device='mps:0')\n",
      "Iteration 1360 Training loss 0.11826827377080917 Validation loss 0.11944127827882767 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.4002],\n",
      "        [0.5717]], device='mps:0')\n",
      "Iteration 1370 Training loss 0.1192246600985527 Validation loss 0.11943280696868896 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.5594],\n",
      "        [0.5475]], device='mps:0')\n",
      "Iteration 1380 Training loss 0.11852073669433594 Validation loss 0.11938890814781189 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.5343],\n",
      "        [0.4730]], device='mps:0')\n",
      "Iteration 1390 Training loss 0.11924334615468979 Validation loss 0.11937453597784042 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.3975],\n",
      "        [0.5231]], device='mps:0')\n",
      "Iteration 1400 Training loss 0.12144424021244049 Validation loss 0.11933496594429016 Accuracy 0.6310000419616699\n",
      "Output tensor([[0.4682],\n",
      "        [0.5185]], device='mps:0')\n",
      "Iteration 1410 Training loss 0.11648488789796829 Validation loss 0.11933685094118118 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.4937],\n",
      "        [0.4367]], device='mps:0')\n",
      "Iteration 1420 Training loss 0.11866550147533417 Validation loss 0.11932191997766495 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.4610],\n",
      "        [0.5304]], device='mps:0')\n",
      "Iteration 1430 Training loss 0.12125957012176514 Validation loss 0.11926542967557907 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.4031],\n",
      "        [0.4732]], device='mps:0')\n",
      "Iteration 1440 Training loss 0.11690018326044083 Validation loss 0.11915941536426544 Accuracy 0.6295000314712524\n",
      "Output tensor([[0.5117],\n",
      "        [0.4887]], device='mps:0')\n",
      "Iteration 1450 Training loss 0.11525321751832962 Validation loss 0.11913008242845535 Accuracy 0.628000020980835\n",
      "Output tensor([[0.4856],\n",
      "        [0.5200]], device='mps:0')\n",
      "Iteration 1460 Training loss 0.11911103129386902 Validation loss 0.11911116540431976 Accuracy 0.6305000185966492\n",
      "Output tensor([[0.5314],\n",
      "        [0.4578]], device='mps:0')\n",
      "Iteration 1470 Training loss 0.12024202942848206 Validation loss 0.11907348036766052 Accuracy 0.625\n",
      "Output tensor([[0.4023],\n",
      "        [0.5164]], device='mps:0')\n",
      "Iteration 1480 Training loss 0.11921880394220352 Validation loss 0.11906713992357254 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.5094],\n",
      "        [0.4815]], device='mps:0')\n",
      "Iteration 1490 Training loss 0.11808899790048599 Validation loss 0.11905711144208908 Accuracy 0.6310000419616699\n",
      "Output tensor([[0.4998],\n",
      "        [0.4254]], device='mps:0')\n",
      "Iteration 1500 Training loss 0.12011245638132095 Validation loss 0.11900609731674194 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.4991],\n",
      "        [0.5126]], device='mps:0')\n",
      "Iteration 1510 Training loss 0.11919093132019043 Validation loss 0.11898352205753326 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.5542],\n",
      "        [0.4896]], device='mps:0')\n",
      "Iteration 1520 Training loss 0.12153901904821396 Validation loss 0.1189856007695198 Accuracy 0.6315000057220459\n",
      "Output tensor([[0.4863],\n",
      "        [0.4823]], device='mps:0')\n",
      "Iteration 1530 Training loss 0.11849748343229294 Validation loss 0.118944451212883 Accuracy 0.6305000185966492\n",
      "Output tensor([[0.4461],\n",
      "        [0.5108]], device='mps:0')\n",
      "Iteration 1540 Training loss 0.1171504408121109 Validation loss 0.11889594793319702 Accuracy 0.6295000314712524\n",
      "Output tensor([[0.5449],\n",
      "        [0.4574]], device='mps:0')\n",
      "Iteration 1550 Training loss 0.11860990524291992 Validation loss 0.11888193339109421 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.4403],\n",
      "        [0.3580]], device='mps:0')\n",
      "Iteration 1560 Training loss 0.11942987143993378 Validation loss 0.11884702742099762 Accuracy 0.6270000338554382\n",
      "Output tensor([[0.5215],\n",
      "        [0.4222]], device='mps:0')\n",
      "Iteration 1570 Training loss 0.11683166027069092 Validation loss 0.11883261799812317 Accuracy 0.6305000185966492\n",
      "Output tensor([[0.3329],\n",
      "        [0.4846]], device='mps:0')\n",
      "Iteration 1580 Training loss 0.12278105318546295 Validation loss 0.1188124269247055 Accuracy 0.6315000057220459\n",
      "Output tensor([[0.5471],\n",
      "        [0.5404]], device='mps:0')\n",
      "Iteration 1590 Training loss 0.12127445638179779 Validation loss 0.1187930628657341 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.4411],\n",
      "        [0.4540]], device='mps:0')\n",
      "Iteration 1600 Training loss 0.11944686621427536 Validation loss 0.11876993626356125 Accuracy 0.6305000185966492\n",
      "Output tensor([[0.4327],\n",
      "        [0.5190]], device='mps:0')\n",
      "Iteration 1610 Training loss 0.12097430974245071 Validation loss 0.11874675750732422 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.5024],\n",
      "        [0.4738]], device='mps:0')\n",
      "Iteration 1620 Training loss 0.12023530900478363 Validation loss 0.11872303485870361 Accuracy 0.6270000338554382\n",
      "Output tensor([[0.4898],\n",
      "        [0.4837]], device='mps:0')\n",
      "Iteration 1630 Training loss 0.11780041456222534 Validation loss 0.1187010258436203 Accuracy 0.6235000491142273\n",
      "Output tensor([[0.5811],\n",
      "        [0.5325]], device='mps:0')\n",
      "Iteration 1640 Training loss 0.11691956967115402 Validation loss 0.11867132782936096 Accuracy 0.6285000443458557\n",
      "Output tensor([[0.4038],\n",
      "        [0.4723]], device='mps:0')\n",
      "Iteration 1650 Training loss 0.1188654899597168 Validation loss 0.11865808069705963 Accuracy 0.6135000586509705\n",
      "Output tensor([[0.3920],\n",
      "        [0.5408]], device='mps:0')\n",
      "Iteration 1660 Training loss 0.11660471558570862 Validation loss 0.11863787472248077 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5034],\n",
      "        [0.4563]], device='mps:0')\n",
      "Iteration 1670 Training loss 0.12192752212285995 Validation loss 0.11860848218202591 Accuracy 0.628000020980835\n",
      "Output tensor([[0.3597],\n",
      "        [0.5193]], device='mps:0')\n",
      "Iteration 1680 Training loss 0.11745621263980865 Validation loss 0.11858759075403214 Accuracy 0.6220000386238098\n",
      "Output tensor([[0.4462],\n",
      "        [0.5415]], device='mps:0')\n",
      "Iteration 1690 Training loss 0.11999566853046417 Validation loss 0.11857850849628448 Accuracy 0.6070000529289246\n",
      "Output tensor([[0.5196],\n",
      "        [0.3188]], device='mps:0')\n",
      "Iteration 1700 Training loss 0.11551868915557861 Validation loss 0.11854184418916702 Accuracy 0.6270000338554382\n",
      "Output tensor([[0.5121],\n",
      "        [0.3951]], device='mps:0')\n",
      "Iteration 1710 Training loss 0.11781815439462662 Validation loss 0.11852768808603287 Accuracy 0.6185000538825989\n",
      "Output tensor([[0.5275],\n",
      "        [0.5550]], device='mps:0')\n",
      "Iteration 1720 Training loss 0.11612453311681747 Validation loss 0.11850152164697647 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.5465],\n",
      "        [0.4822]], device='mps:0')\n",
      "Iteration 1730 Training loss 0.12000879645347595 Validation loss 0.11849324405193329 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4909],\n",
      "        [0.5057]], device='mps:0')\n",
      "Iteration 1740 Training loss 0.12260476499795914 Validation loss 0.11845893412828445 Accuracy 0.6265000104904175\n",
      "Output tensor([[0.3590],\n",
      "        [0.5256]], device='mps:0')\n",
      "Iteration 1750 Training loss 0.11913885921239853 Validation loss 0.11843651533126831 Accuracy 0.6305000185966492\n",
      "Output tensor([[0.5588],\n",
      "        [0.4825]], device='mps:0')\n",
      "Iteration 1760 Training loss 0.12100028991699219 Validation loss 0.1184111088514328 Accuracy 0.6285000443458557\n",
      "Output tensor([[0.4264],\n",
      "        [0.5393]], device='mps:0')\n",
      "Iteration 1770 Training loss 0.11875743418931961 Validation loss 0.11839272826910019 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.5006],\n",
      "        [0.4226]], device='mps:0')\n",
      "Iteration 1780 Training loss 0.12057428061962128 Validation loss 0.11837462335824966 Accuracy 0.6315000057220459\n",
      "Output tensor([[0.4838],\n",
      "        [0.5421]], device='mps:0')\n",
      "Iteration 1790 Training loss 0.11672969907522202 Validation loss 0.11835826933383942 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.4672],\n",
      "        [0.5841]], device='mps:0')\n",
      "Iteration 1800 Training loss 0.11605677008628845 Validation loss 0.11833806335926056 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.5040],\n",
      "        [0.4660]], device='mps:0')\n",
      "Iteration 1810 Training loss 0.12310325354337692 Validation loss 0.11831897497177124 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.5331],\n",
      "        [0.4938]], device='mps:0')\n",
      "Iteration 1820 Training loss 0.11497219651937485 Validation loss 0.11829818040132523 Accuracy 0.6315000057220459\n",
      "Output tensor([[0.4476],\n",
      "        [0.5169]], device='mps:0')\n",
      "Iteration 1830 Training loss 0.11946303397417068 Validation loss 0.11828621476888657 Accuracy 0.6360000371932983\n",
      "Output tensor([[0.4909],\n",
      "        [0.5566]], device='mps:0')\n",
      "Iteration 1840 Training loss 0.11975986510515213 Validation loss 0.11826347559690475 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.5114],\n",
      "        [0.5702]], device='mps:0')\n",
      "Iteration 1850 Training loss 0.11624683439731598 Validation loss 0.11823225766420364 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.4803],\n",
      "        [0.4697]], device='mps:0')\n",
      "Iteration 1860 Training loss 0.1260855793952942 Validation loss 0.11821208894252777 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.4565],\n",
      "        [0.5436]], device='mps:0')\n",
      "Iteration 1870 Training loss 0.12273786962032318 Validation loss 0.11819666624069214 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.4421],\n",
      "        [0.5160]], device='mps:0')\n",
      "Iteration 1880 Training loss 0.1191568523645401 Validation loss 0.1181735098361969 Accuracy 0.6355000138282776\n",
      "Output tensor([[0.5335],\n",
      "        [0.3612]], device='mps:0')\n",
      "Iteration 1890 Training loss 0.11591262370347977 Validation loss 0.11820502579212189 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.4606],\n",
      "        [0.5433]], device='mps:0')\n",
      "Iteration 1900 Training loss 0.1170608177781105 Validation loss 0.11822040379047394 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.4966],\n",
      "        [0.3591]], device='mps:0')\n",
      "Iteration 1910 Training loss 0.1177000179886818 Validation loss 0.11813246458768845 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.4693],\n",
      "        [0.5835]], device='mps:0')\n",
      "Iteration 1920 Training loss 0.10930777341127396 Validation loss 0.11811933666467667 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.5339],\n",
      "        [0.5188]], device='mps:0')\n",
      "Iteration 1930 Training loss 0.11900351196527481 Validation loss 0.11814350634813309 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.5202],\n",
      "        [0.5512]], device='mps:0')\n",
      "Iteration 1940 Training loss 0.11967729032039642 Validation loss 0.11810486763715744 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.5498],\n",
      "        [0.5445]], device='mps:0')\n",
      "Iteration 1950 Training loss 0.11795619875192642 Validation loss 0.11805953830480576 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5171],\n",
      "        [0.4805]], device='mps:0')\n",
      "Iteration 1960 Training loss 0.11566148698329926 Validation loss 0.11804216355085373 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5301],\n",
      "        [0.4319]], device='mps:0')\n",
      "Iteration 1970 Training loss 0.11816207319498062 Validation loss 0.11799175292253494 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.5306],\n",
      "        [0.4013]], device='mps:0')\n",
      "Iteration 1980 Training loss 0.11287043243646622 Validation loss 0.11797407269477844 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.3944],\n",
      "        [0.5002]], device='mps:0')\n",
      "Iteration 1990 Training loss 0.12019185721874237 Validation loss 0.11795490235090256 Accuracy 0.6285000443458557\n",
      "Output tensor([[0.4354],\n",
      "        [0.5730]], device='mps:0')\n",
      "Iteration 2000 Training loss 0.12265903502702713 Validation loss 0.11793337017297745 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.4187],\n",
      "        [0.4149]], device='mps:0')\n",
      "Iteration 2010 Training loss 0.11620699614286423 Validation loss 0.11791595816612244 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.5319],\n",
      "        [0.5030]], device='mps:0')\n",
      "Iteration 2020 Training loss 0.11956802010536194 Validation loss 0.11789468675851822 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.4876],\n",
      "        [0.4799]], device='mps:0')\n",
      "Iteration 2030 Training loss 0.11947275698184967 Validation loss 0.1178746372461319 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.6004],\n",
      "        [0.4943]], device='mps:0')\n",
      "Iteration 2040 Training loss 0.11948852986097336 Validation loss 0.11786859482526779 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.5449],\n",
      "        [0.3973]], device='mps:0')\n",
      "Iteration 2050 Training loss 0.11993062496185303 Validation loss 0.11784955114126205 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.4968],\n",
      "        [0.5467]], device='mps:0')\n",
      "Iteration 2060 Training loss 0.11976304650306702 Validation loss 0.11783960461616516 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5345],\n",
      "        [0.4889]], device='mps:0')\n",
      "Iteration 2070 Training loss 0.11666751652956009 Validation loss 0.117859847843647 Accuracy 0.6360000371932983\n",
      "Output tensor([[0.5760],\n",
      "        [0.5581]], device='mps:0')\n",
      "Iteration 2080 Training loss 0.1223980113863945 Validation loss 0.11778567731380463 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.3931],\n",
      "        [0.5448]], device='mps:0')\n",
      "Iteration 2090 Training loss 0.113130122423172 Validation loss 0.11778148263692856 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5375],\n",
      "        [0.4740]], device='mps:0')\n",
      "Iteration 2100 Training loss 0.11901239305734634 Validation loss 0.11776886880397797 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.5095],\n",
      "        [0.4783]], device='mps:0')\n",
      "Iteration 2110 Training loss 0.11699450016021729 Validation loss 0.11775966733694077 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.4002],\n",
      "        [0.4901]], device='mps:0')\n",
      "Iteration 2120 Training loss 0.1166914850473404 Validation loss 0.1177210882306099 Accuracy 0.64000004529953\n",
      "Output tensor([[0.5024],\n",
      "        [0.4225]], device='mps:0')\n",
      "Iteration 2130 Training loss 0.11396777629852295 Validation loss 0.1177108958363533 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.4462],\n",
      "        [0.5420]], device='mps:0')\n",
      "Iteration 2140 Training loss 0.1217293068766594 Validation loss 0.11770464479923248 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.6168],\n",
      "        [0.4653]], device='mps:0')\n",
      "Iteration 2150 Training loss 0.11462315171957016 Validation loss 0.11768198013305664 Accuracy 0.6355000138282776\n",
      "Output tensor([[0.4631],\n",
      "        [0.5172]], device='mps:0')\n",
      "Iteration 2160 Training loss 0.11489495635032654 Validation loss 0.11764177680015564 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5731],\n",
      "        [0.5008]], device='mps:0')\n",
      "Iteration 2170 Training loss 0.11644290387630463 Validation loss 0.1176498755812645 Accuracy 0.6360000371932983\n",
      "Output tensor([[0.5514],\n",
      "        [0.4716]], device='mps:0')\n",
      "Iteration 2180 Training loss 0.1215747743844986 Validation loss 0.11762163043022156 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.3690],\n",
      "        [0.4133]], device='mps:0')\n",
      "Iteration 2190 Training loss 0.12082985788583755 Validation loss 0.11761502921581268 Accuracy 0.6365000009536743\n",
      "Output tensor([[0.4287],\n",
      "        [0.5038]], device='mps:0')\n",
      "Iteration 2200 Training loss 0.11590400338172913 Validation loss 0.11756483465433121 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.4764],\n",
      "        [0.5313]], device='mps:0')\n",
      "Iteration 2210 Training loss 0.11624526977539062 Validation loss 0.11757749319076538 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5021],\n",
      "        [0.3422]], device='mps:0')\n",
      "Iteration 2220 Training loss 0.11514640599489212 Validation loss 0.11751088500022888 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.5120],\n",
      "        [0.5386]], device='mps:0')\n",
      "Iteration 2230 Training loss 0.11796266585588455 Validation loss 0.11749181151390076 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.4942],\n",
      "        [0.5176]], device='mps:0')\n",
      "Iteration 2240 Training loss 0.12085799127817154 Validation loss 0.11747381836175919 Accuracy 0.6355000138282776\n",
      "Output tensor([[0.5837],\n",
      "        [0.5564]], device='mps:0')\n",
      "Iteration 2250 Training loss 0.11766292154788971 Validation loss 0.11746539175510406 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.4310],\n",
      "        [0.3957]], device='mps:0')\n",
      "Iteration 2260 Training loss 0.1145528182387352 Validation loss 0.11744264513254166 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5176],\n",
      "        [0.4494]], device='mps:0')\n",
      "Iteration 2270 Training loss 0.11800257116556168 Validation loss 0.11742603033781052 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.3209],\n",
      "        [0.5058]], device='mps:0')\n",
      "Iteration 2280 Training loss 0.11391331255435944 Validation loss 0.1174154281616211 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.5375],\n",
      "        [0.4563]], device='mps:0')\n",
      "Iteration 2290 Training loss 0.11520417779684067 Validation loss 0.11741551756858826 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.3675],\n",
      "        [0.4435]], device='mps:0')\n",
      "Iteration 2300 Training loss 0.11616486310958862 Validation loss 0.11742439866065979 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5045],\n",
      "        [0.5367]], device='mps:0')\n",
      "Iteration 2310 Training loss 0.11384446918964386 Validation loss 0.11736166477203369 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.5654],\n",
      "        [0.4767]], device='mps:0')\n",
      "Iteration 2320 Training loss 0.1245143860578537 Validation loss 0.11732938885688782 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.3975],\n",
      "        [0.5245]], device='mps:0')\n",
      "Iteration 2330 Training loss 0.11747828871011734 Validation loss 0.11733116954565048 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5011],\n",
      "        [0.5273]], device='mps:0')\n",
      "Iteration 2340 Training loss 0.11488134413957596 Validation loss 0.11729561537504196 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.5003],\n",
      "        [0.5467]], device='mps:0')\n",
      "Iteration 2350 Training loss 0.11740083247423172 Validation loss 0.11727989464998245 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.4283],\n",
      "        [0.3558]], device='mps:0')\n",
      "Iteration 2360 Training loss 0.12204059958457947 Validation loss 0.11725365370512009 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5300],\n",
      "        [0.4769]], device='mps:0')\n",
      "Iteration 2370 Training loss 0.11880525201559067 Validation loss 0.11724956333637238 Accuracy 0.6315000057220459\n",
      "Output tensor([[0.4885],\n",
      "        [0.4896]], device='mps:0')\n",
      "Iteration 2380 Training loss 0.11260043829679489 Validation loss 0.11722978949546814 Accuracy 0.6355000138282776\n",
      "Output tensor([[0.4796],\n",
      "        [0.5476]], device='mps:0')\n",
      "Iteration 2390 Training loss 0.12105047702789307 Validation loss 0.11721020191907883 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.5568],\n",
      "        [0.5195]], device='mps:0')\n",
      "Iteration 2400 Training loss 0.12042995542287827 Validation loss 0.11721775680780411 Accuracy 0.6365000009536743\n",
      "Output tensor([[0.5119],\n",
      "        [0.2804]], device='mps:0')\n",
      "Iteration 2410 Training loss 0.11641097813844681 Validation loss 0.11720575392246246 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.4781],\n",
      "        [0.5250]], device='mps:0')\n",
      "Iteration 2420 Training loss 0.11709075421094894 Validation loss 0.11720431596040726 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5622],\n",
      "        [0.5731]], device='mps:0')\n",
      "Iteration 2430 Training loss 0.1145726665854454 Validation loss 0.11719789355993271 Accuracy 0.640500009059906\n",
      "Output tensor([[0.5075],\n",
      "        [0.5036]], device='mps:0')\n",
      "Iteration 2440 Training loss 0.11878327280282974 Validation loss 0.1171417161822319 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.4541],\n",
      "        [0.4803]], device='mps:0')\n",
      "Iteration 2450 Training loss 0.11873805522918701 Validation loss 0.11711549758911133 Accuracy 0.640500009059906\n",
      "Output tensor([[0.5564],\n",
      "        [0.3911]], device='mps:0')\n",
      "Iteration 2460 Training loss 0.11853045225143433 Validation loss 0.11710629612207413 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.4312],\n",
      "        [0.5386]], device='mps:0')\n",
      "Iteration 2470 Training loss 0.11983060836791992 Validation loss 0.11710351705551147 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5366],\n",
      "        [0.5080]], device='mps:0')\n",
      "Iteration 2480 Training loss 0.11852157115936279 Validation loss 0.11707896739244461 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.4781],\n",
      "        [0.4999]], device='mps:0')\n",
      "Iteration 2490 Training loss 0.1158209964632988 Validation loss 0.11707888543605804 Accuracy 0.64000004529953\n",
      "Output tensor([[0.4651],\n",
      "        [0.5775]], device='mps:0')\n",
      "Iteration 2500 Training loss 0.11102929711341858 Validation loss 0.11704850196838379 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5051],\n",
      "        [0.5222]], device='mps:0')\n",
      "Iteration 2510 Training loss 0.11286915838718414 Validation loss 0.1170416995882988 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.4922],\n",
      "        [0.4221]], device='mps:0')\n",
      "Iteration 2520 Training loss 0.11757807433605194 Validation loss 0.11701669543981552 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.5507],\n",
      "        [0.4622]], device='mps:0')\n",
      "Iteration 2530 Training loss 0.11425997316837311 Validation loss 0.11700080335140228 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5447],\n",
      "        [0.4939]], device='mps:0')\n",
      "Iteration 2540 Training loss 0.11425229907035828 Validation loss 0.11701650172472 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.3046],\n",
      "        [0.5512]], device='mps:0')\n",
      "Iteration 2550 Training loss 0.11340620368719101 Validation loss 0.11700168251991272 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5456],\n",
      "        [0.5364]], device='mps:0')\n",
      "Iteration 2560 Training loss 0.11602668464183807 Validation loss 0.11695760488510132 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5380],\n",
      "        [0.5673]], device='mps:0')\n",
      "Iteration 2570 Training loss 0.11969798058271408 Validation loss 0.11693738400936127 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5102],\n",
      "        [0.4276]], device='mps:0')\n",
      "Iteration 2580 Training loss 0.11893443018198013 Validation loss 0.11692317575216293 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5195],\n",
      "        [0.4463]], device='mps:0')\n",
      "Iteration 2590 Training loss 0.1199810728430748 Validation loss 0.11693332344293594 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5370],\n",
      "        [0.5461]], device='mps:0')\n",
      "Iteration 2600 Training loss 0.11783662438392639 Validation loss 0.11695204675197601 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.4965],\n",
      "        [0.4057]], device='mps:0')\n",
      "Iteration 2610 Training loss 0.11895132064819336 Validation loss 0.11692769080400467 Accuracy 0.64000004529953\n",
      "Output tensor([[0.5468],\n",
      "        [0.5178]], device='mps:0')\n",
      "Iteration 2620 Training loss 0.1207464411854744 Validation loss 0.11687330901622772 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5330],\n",
      "        [0.6294]], device='mps:0')\n",
      "Iteration 2630 Training loss 0.11742677539587021 Validation loss 0.11682385951280594 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.6174],\n",
      "        [0.5276]], device='mps:0')\n",
      "Iteration 2640 Training loss 0.11389593034982681 Validation loss 0.11682429909706116 Accuracy 0.64000004529953\n",
      "Output tensor([[0.5350],\n",
      "        [0.5444]], device='mps:0')\n",
      "Iteration 2650 Training loss 0.11274249851703644 Validation loss 0.11682935059070587 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.5946],\n",
      "        [0.5416]], device='mps:0')\n",
      "Iteration 2660 Training loss 0.11969272047281265 Validation loss 0.11680614203214645 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5712],\n",
      "        [0.3880]], device='mps:0')\n",
      "Iteration 2670 Training loss 0.11876197904348373 Validation loss 0.11675203591585159 Accuracy 0.64000004529953\n",
      "Output tensor([[0.5033],\n",
      "        [0.3489]], device='mps:0')\n",
      "Iteration 2680 Training loss 0.11657385528087616 Validation loss 0.11675481498241425 Accuracy 0.64000004529953\n",
      "Output tensor([[0.4605],\n",
      "        [0.4936]], device='mps:0')\n",
      "Iteration 2690 Training loss 0.11405185610055923 Validation loss 0.11673014611005783 Accuracy 0.640500009059906\n",
      "Output tensor([[0.4182],\n",
      "        [0.5534]], device='mps:0')\n",
      "Iteration 2700 Training loss 0.11487632244825363 Validation loss 0.1166938841342926 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.4345],\n",
      "        [0.4634]], device='mps:0')\n",
      "Iteration 2710 Training loss 0.11989275366067886 Validation loss 0.1166815385222435 Accuracy 0.64000004529953\n",
      "Output tensor([[0.5225],\n",
      "        [0.5848]], device='mps:0')\n",
      "Iteration 2720 Training loss 0.11483155190944672 Validation loss 0.11667428910732269 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.5049],\n",
      "        [0.5118]], device='mps:0')\n",
      "Iteration 2730 Training loss 0.11176816374063492 Validation loss 0.11665112525224686 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.4701],\n",
      "        [0.5024]], device='mps:0')\n",
      "Iteration 2740 Training loss 0.1156880334019661 Validation loss 0.11662986874580383 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.4561],\n",
      "        [0.4880]], device='mps:0')\n",
      "Iteration 2750 Training loss 0.12156109511852264 Validation loss 0.11661142855882645 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.3750],\n",
      "        [0.5280]], device='mps:0')\n",
      "Iteration 2760 Training loss 0.11717072874307632 Validation loss 0.1166093498468399 Accuracy 0.6395000219345093\n",
      "Output tensor([[0.4981],\n",
      "        [0.3523]], device='mps:0')\n",
      "Iteration 2770 Training loss 0.11376580595970154 Validation loss 0.11660456657409668 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.5091],\n",
      "        [0.5084]], device='mps:0')\n",
      "Iteration 2780 Training loss 0.12169075012207031 Validation loss 0.11660251766443253 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.5383],\n",
      "        [0.5239]], device='mps:0')\n",
      "Iteration 2790 Training loss 0.11668064445257187 Validation loss 0.11657015979290009 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5250],\n",
      "        [0.3083]], device='mps:0')\n",
      "Iteration 2800 Training loss 0.12113012373447418 Validation loss 0.11654777824878693 Accuracy 0.643500030040741\n",
      "Output tensor([[0.5610],\n",
      "        [0.5420]], device='mps:0')\n",
      "Iteration 2810 Training loss 0.12250935286283493 Validation loss 0.11655901372432709 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.4706],\n",
      "        [0.5493]], device='mps:0')\n",
      "Iteration 2820 Training loss 0.11874303221702576 Validation loss 0.11654925346374512 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.6030],\n",
      "        [0.5804]], device='mps:0')\n",
      "Iteration 2830 Training loss 0.11367141455411911 Validation loss 0.11650659888982773 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.6006],\n",
      "        [0.4838]], device='mps:0')\n",
      "Iteration 2840 Training loss 0.11665298789739609 Validation loss 0.11648401618003845 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.3848],\n",
      "        [0.5183]], device='mps:0')\n",
      "Iteration 2850 Training loss 0.11759140342473984 Validation loss 0.11648216843605042 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5100],\n",
      "        [0.4949]], device='mps:0')\n",
      "Iteration 2860 Training loss 0.11145582050085068 Validation loss 0.11645888537168503 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5498],\n",
      "        [0.5107]], device='mps:0')\n",
      "Iteration 2870 Training loss 0.11688445508480072 Validation loss 0.11644436419010162 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5241],\n",
      "        [0.3851]], device='mps:0')\n",
      "Iteration 2880 Training loss 0.11246209591627121 Validation loss 0.11643088608980179 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.4328],\n",
      "        [0.3957]], device='mps:0')\n",
      "Iteration 2890 Training loss 0.11547085642814636 Validation loss 0.1164102852344513 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5253],\n",
      "        [0.4040]], device='mps:0')\n",
      "Iteration 2900 Training loss 0.11579068750143051 Validation loss 0.11639818549156189 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5079],\n",
      "        [0.6090]], device='mps:0')\n",
      "Iteration 2910 Training loss 0.12209592759609222 Validation loss 0.11638211458921432 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.5150],\n",
      "        [0.5087]], device='mps:0')\n",
      "Iteration 2920 Training loss 0.11645197868347168 Validation loss 0.116376593708992 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.5140],\n",
      "        [0.3744]], device='mps:0')\n",
      "Iteration 2930 Training loss 0.1149899810552597 Validation loss 0.11638176441192627 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5555],\n",
      "        [0.5117]], device='mps:0')\n",
      "Iteration 2940 Training loss 0.11815076321363449 Validation loss 0.11634880304336548 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5677],\n",
      "        [0.5058]], device='mps:0')\n",
      "Iteration 2950 Training loss 0.12075819820165634 Validation loss 0.11633167415857315 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.4085],\n",
      "        [0.5527]], device='mps:0')\n",
      "Iteration 2960 Training loss 0.1104617640376091 Validation loss 0.11632362008094788 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5461],\n",
      "        [0.5261]], device='mps:0')\n",
      "Iteration 2970 Training loss 0.10899882018566132 Validation loss 0.11631517857313156 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.5292],\n",
      "        [0.5246]], device='mps:0')\n",
      "Iteration 2980 Training loss 0.11744711548089981 Validation loss 0.11630471795797348 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.5964],\n",
      "        [0.5406]], device='mps:0')\n",
      "Iteration 2990 Training loss 0.11589742451906204 Validation loss 0.11627620458602905 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.4468],\n",
      "        [0.5689]], device='mps:0')\n",
      "Iteration 3000 Training loss 0.11614852398633957 Validation loss 0.11625780910253525 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4922],\n",
      "        [0.6027]], device='mps:0')\n",
      "Iteration 3010 Training loss 0.11759376525878906 Validation loss 0.11624341458082199 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.5690],\n",
      "        [0.5807]], device='mps:0')\n",
      "Iteration 3020 Training loss 0.11658450961112976 Validation loss 0.116244837641716 Accuracy 0.640500009059906\n",
      "Output tensor([[0.5981],\n",
      "        [0.3583]], device='mps:0')\n",
      "Iteration 3030 Training loss 0.11768463999032974 Validation loss 0.11624353379011154 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.4476],\n",
      "        [0.4919]], device='mps:0')\n",
      "Iteration 3040 Training loss 0.11739150434732437 Validation loss 0.11619877815246582 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5341],\n",
      "        [0.4972]], device='mps:0')\n",
      "Iteration 3050 Training loss 0.12487412989139557 Validation loss 0.11620232462882996 Accuracy 0.640500009059906\n",
      "Output tensor([[0.6055],\n",
      "        [0.4553]], device='mps:0')\n",
      "Iteration 3060 Training loss 0.12212274223566055 Validation loss 0.1161731630563736 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.5572],\n",
      "        [0.4789]], device='mps:0')\n",
      "Iteration 3070 Training loss 0.11547979712486267 Validation loss 0.11616133898496628 Accuracy 0.643500030040741\n",
      "Output tensor([[0.5421],\n",
      "        [0.4011]], device='mps:0')\n",
      "Iteration 3080 Training loss 0.12537802755832672 Validation loss 0.11614758521318436 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.5883],\n",
      "        [0.4032]], device='mps:0')\n",
      "Iteration 3090 Training loss 0.11390256881713867 Validation loss 0.1161336675286293 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.4350],\n",
      "        [0.4870]], device='mps:0')\n",
      "Iteration 3100 Training loss 0.11477808654308319 Validation loss 0.11612699180841446 Accuracy 0.643500030040741\n",
      "Output tensor([[0.4771],\n",
      "        [0.5344]], device='mps:0')\n",
      "Iteration 3110 Training loss 0.11655280739068985 Validation loss 0.11612657457590103 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.5341],\n",
      "        [0.4831]], device='mps:0')\n",
      "Iteration 3120 Training loss 0.11631442606449127 Validation loss 0.11610588431358337 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5554],\n",
      "        [0.4415]], device='mps:0')\n",
      "Iteration 3130 Training loss 0.11695925146341324 Validation loss 0.11609625071287155 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.3536],\n",
      "        [0.6091]], device='mps:0')\n",
      "Iteration 3140 Training loss 0.11914216727018356 Validation loss 0.11607663333415985 Accuracy 0.643500030040741\n",
      "Output tensor([[0.4847],\n",
      "        [0.5374]], device='mps:0')\n",
      "Iteration 3150 Training loss 0.11905613541603088 Validation loss 0.1160648837685585 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5263],\n",
      "        [0.4200]], device='mps:0')\n",
      "Iteration 3160 Training loss 0.1174505427479744 Validation loss 0.11604257673025131 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.6085],\n",
      "        [0.4587]], device='mps:0')\n",
      "Iteration 3170 Training loss 0.11477865278720856 Validation loss 0.11602207273244858 Accuracy 0.643500030040741\n",
      "Output tensor([[0.5262],\n",
      "        [0.4124]], device='mps:0')\n",
      "Iteration 3180 Training loss 0.11327379196882248 Validation loss 0.11600569635629654 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.4932],\n",
      "        [0.4767]], device='mps:0')\n",
      "Iteration 3190 Training loss 0.11817184090614319 Validation loss 0.11599161475896835 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.5555],\n",
      "        [0.5063]], device='mps:0')\n",
      "Iteration 3200 Training loss 0.11563076823949814 Validation loss 0.11597660183906555 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.3248],\n",
      "        [0.4684]], device='mps:0')\n",
      "Iteration 3210 Training loss 0.10888965427875519 Validation loss 0.11596127599477768 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.4993],\n",
      "        [0.4876]], device='mps:0')\n",
      "Iteration 3220 Training loss 0.11335329711437225 Validation loss 0.11594872921705246 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5210],\n",
      "        [0.5410]], device='mps:0')\n",
      "Iteration 3230 Training loss 0.11939205229282379 Validation loss 0.11593389511108398 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.4620],\n",
      "        [0.3574]], device='mps:0')\n",
      "Iteration 3240 Training loss 0.11741742491722107 Validation loss 0.11596662551164627 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.3696],\n",
      "        [0.5928]], device='mps:0')\n",
      "Iteration 3250 Training loss 0.11669780313968658 Validation loss 0.11594143509864807 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.5944],\n",
      "        [0.5320]], device='mps:0')\n",
      "Iteration 3260 Training loss 0.11728185415267944 Validation loss 0.1159377321600914 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5238],\n",
      "        [0.3463]], device='mps:0')\n",
      "Iteration 3270 Training loss 0.11448139697313309 Validation loss 0.11592982709407806 Accuracy 0.640500009059906\n",
      "Output tensor([[0.5123],\n",
      "        [0.4920]], device='mps:0')\n",
      "Iteration 3280 Training loss 0.11152955889701843 Validation loss 0.11593350768089294 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.4993],\n",
      "        [0.4440]], device='mps:0')\n",
      "Iteration 3290 Training loss 0.11321716010570526 Validation loss 0.11593577265739441 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.5180],\n",
      "        [0.4058]], device='mps:0')\n",
      "Iteration 3300 Training loss 0.11581625044345856 Validation loss 0.11594744771718979 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5650],\n",
      "        [0.3751]], device='mps:0')\n",
      "Iteration 3310 Training loss 0.11882074177265167 Validation loss 0.11588378250598907 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5579],\n",
      "        [0.5394]], device='mps:0')\n",
      "Iteration 3320 Training loss 0.11433649063110352 Validation loss 0.11582816392183304 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.4861],\n",
      "        [0.4872]], device='mps:0')\n",
      "Iteration 3330 Training loss 0.11655845493078232 Validation loss 0.11579600721597672 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.4968],\n",
      "        [0.3502]], device='mps:0')\n",
      "Iteration 3340 Training loss 0.11253398656845093 Validation loss 0.11578056961297989 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.6142],\n",
      "        [0.5217]], device='mps:0')\n",
      "Iteration 3350 Training loss 0.11644037067890167 Validation loss 0.11577365547418594 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.4502],\n",
      "        [0.5328]], device='mps:0')\n",
      "Iteration 3360 Training loss 0.11469270288944244 Validation loss 0.11575737595558167 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.4640],\n",
      "        [0.4455]], device='mps:0')\n",
      "Iteration 3370 Training loss 0.11104623228311539 Validation loss 0.11574453115463257 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5088],\n",
      "        [0.4811]], device='mps:0')\n",
      "Iteration 3380 Training loss 0.11822741478681564 Validation loss 0.11573320627212524 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5492],\n",
      "        [0.2650]], device='mps:0')\n",
      "Iteration 3390 Training loss 0.11422471702098846 Validation loss 0.11572431027889252 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5514],\n",
      "        [0.2806]], device='mps:0')\n",
      "Iteration 3400 Training loss 0.12369804084300995 Validation loss 0.11572737246751785 Accuracy 0.643500030040741\n",
      "Output tensor([[0.5823],\n",
      "        [0.4816]], device='mps:0')\n",
      "Iteration 3410 Training loss 0.11434533447027206 Validation loss 0.11573526263237 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.4759],\n",
      "        [0.4566]], device='mps:0')\n",
      "Iteration 3420 Training loss 0.11924901604652405 Validation loss 0.11572571098804474 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.4779],\n",
      "        [0.4858]], device='mps:0')\n",
      "Iteration 3430 Training loss 0.11171798408031464 Validation loss 0.11571860313415527 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.6119],\n",
      "        [0.4453]], device='mps:0')\n",
      "Iteration 3440 Training loss 0.11511742323637009 Validation loss 0.11569850891828537 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.4354],\n",
      "        [0.6515]], device='mps:0')\n",
      "Iteration 3450 Training loss 0.11061231046915054 Validation loss 0.11569807678461075 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.4292],\n",
      "        [0.5731]], device='mps:0')\n",
      "Iteration 3460 Training loss 0.11548352241516113 Validation loss 0.11566326767206192 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.4461],\n",
      "        [0.4547]], device='mps:0')\n",
      "Iteration 3470 Training loss 0.114894799888134 Validation loss 0.11562243103981018 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.4544],\n",
      "        [0.4218]], device='mps:0')\n",
      "Iteration 3480 Training loss 0.11481046676635742 Validation loss 0.11560267955064774 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.3625],\n",
      "        [0.4664]], device='mps:0')\n",
      "Iteration 3490 Training loss 0.11658569425344467 Validation loss 0.11558788269758224 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5117],\n",
      "        [0.5283]], device='mps:0')\n",
      "Iteration 3500 Training loss 0.11403657495975494 Validation loss 0.11557887494564056 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.4635],\n",
      "        [0.2727]], device='mps:0')\n",
      "Iteration 3510 Training loss 0.11968667805194855 Validation loss 0.11556148529052734 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.4492],\n",
      "        [0.5257]], device='mps:0')\n",
      "Iteration 3520 Training loss 0.11600295454263687 Validation loss 0.11554621160030365 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4823],\n",
      "        [0.5212]], device='mps:0')\n",
      "Iteration 3530 Training loss 0.11829938739538193 Validation loss 0.11554586887359619 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.4734],\n",
      "        [0.5456]], device='mps:0')\n",
      "Iteration 3540 Training loss 0.11274763196706772 Validation loss 0.11551810801029205 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.5182],\n",
      "        [0.5517]], device='mps:0')\n",
      "Iteration 3550 Training loss 0.11767898499965668 Validation loss 0.11551330238580704 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.5695],\n",
      "        [0.4485]], device='mps:0')\n",
      "Iteration 3560 Training loss 0.10753877460956573 Validation loss 0.11549150198698044 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.6097],\n",
      "        [0.5966]], device='mps:0')\n",
      "Iteration 3570 Training loss 0.11159059405326843 Validation loss 0.11548930406570435 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.4575],\n",
      "        [0.5005]], device='mps:0')\n",
      "Iteration 3580 Training loss 0.11833923310041428 Validation loss 0.11546581238508224 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.3491],\n",
      "        [0.5148]], device='mps:0')\n",
      "Iteration 3590 Training loss 0.11615592986345291 Validation loss 0.11545217037200928 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4960],\n",
      "        [0.5973]], device='mps:0')\n",
      "Iteration 3600 Training loss 0.12187422811985016 Validation loss 0.1154499277472496 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.4631],\n",
      "        [0.4083]], device='mps:0')\n",
      "Iteration 3610 Training loss 0.10719486325979233 Validation loss 0.11543421447277069 Accuracy 0.643500030040741\n",
      "Output tensor([[0.5775],\n",
      "        [0.5354]], device='mps:0')\n",
      "Iteration 3620 Training loss 0.11633642017841339 Validation loss 0.11541444808244705 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4948],\n",
      "        [0.5116]], device='mps:0')\n",
      "Iteration 3630 Training loss 0.11436545103788376 Validation loss 0.11542213708162308 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.4421],\n",
      "        [0.4846]], device='mps:0')\n",
      "Iteration 3640 Training loss 0.11732327938079834 Validation loss 0.11539623886346817 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5502],\n",
      "        [0.4931]], device='mps:0')\n",
      "Iteration 3650 Training loss 0.11647889763116837 Validation loss 0.11539126932621002 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.4442],\n",
      "        [0.5219]], device='mps:0')\n",
      "Iteration 3660 Training loss 0.1162601038813591 Validation loss 0.11537560075521469 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5761],\n",
      "        [0.5548]], device='mps:0')\n",
      "Iteration 3670 Training loss 0.11551627516746521 Validation loss 0.11538411676883698 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.2893],\n",
      "        [0.5323]], device='mps:0')\n",
      "Iteration 3680 Training loss 0.11395251005887985 Validation loss 0.11534543335437775 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.4988],\n",
      "        [0.4695]], device='mps:0')\n",
      "Iteration 3690 Training loss 0.11525620520114899 Validation loss 0.11533413082361221 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4128],\n",
      "        [0.5437]], device='mps:0')\n",
      "Iteration 3700 Training loss 0.11020386219024658 Validation loss 0.11533382534980774 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5533],\n",
      "        [0.5613]], device='mps:0')\n",
      "Iteration 3710 Training loss 0.11595077067613602 Validation loss 0.1153293028473854 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5122],\n",
      "        [0.5492]], device='mps:0')\n",
      "Iteration 3720 Training loss 0.11604354530572891 Validation loss 0.11530843377113342 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.4063],\n",
      "        [0.4754]], device='mps:0')\n",
      "Iteration 3730 Training loss 0.11623571068048477 Validation loss 0.11528156697750092 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5317],\n",
      "        [0.5467]], device='mps:0')\n",
      "Iteration 3740 Training loss 0.11235713958740234 Validation loss 0.11528270691633224 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.4159],\n",
      "        [0.5746]], device='mps:0')\n",
      "Iteration 3750 Training loss 0.1154010146856308 Validation loss 0.11525815725326538 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.5236],\n",
      "        [0.4901]], device='mps:0')\n",
      "Iteration 3760 Training loss 0.12042407691478729 Validation loss 0.11524839699268341 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5590],\n",
      "        [0.4900]], device='mps:0')\n",
      "Iteration 3770 Training loss 0.11487167328596115 Validation loss 0.11523991823196411 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5842],\n",
      "        [0.5554]], device='mps:0')\n",
      "Iteration 3780 Training loss 0.11424409598112106 Validation loss 0.11522234231233597 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.3875],\n",
      "        [0.5465]], device='mps:0')\n",
      "Iteration 3790 Training loss 0.11608556658029556 Validation loss 0.1152084618806839 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5047],\n",
      "        [0.4459]], device='mps:0')\n",
      "Iteration 3800 Training loss 0.11587493866682053 Validation loss 0.11520610004663467 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.5391],\n",
      "        [0.5579]], device='mps:0')\n",
      "Iteration 3810 Training loss 0.1168120950460434 Validation loss 0.11520207673311234 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5704],\n",
      "        [0.4550]], device='mps:0')\n",
      "Iteration 3820 Training loss 0.12016607820987701 Validation loss 0.11523427069187164 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5181],\n",
      "        [0.4752]], device='mps:0')\n",
      "Iteration 3830 Training loss 0.11563649773597717 Validation loss 0.11521992087364197 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5069],\n",
      "        [0.5321]], device='mps:0')\n",
      "Iteration 3840 Training loss 0.11509565263986588 Validation loss 0.11523821204900742 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.5249],\n",
      "        [0.5599]], device='mps:0')\n",
      "Iteration 3850 Training loss 0.1102433055639267 Validation loss 0.11529286950826645 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5564],\n",
      "        [0.2414]], device='mps:0')\n",
      "Iteration 3860 Training loss 0.1120193675160408 Validation loss 0.11529833823442459 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.6234],\n",
      "        [0.4052]], device='mps:0')\n",
      "Iteration 3870 Training loss 0.11773417890071869 Validation loss 0.11513661593198776 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.6126],\n",
      "        [0.3894]], device='mps:0')\n",
      "Iteration 3880 Training loss 0.1146000474691391 Validation loss 0.11513388156890869 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.4512],\n",
      "        [0.5787]], device='mps:0')\n",
      "Iteration 3890 Training loss 0.10927915573120117 Validation loss 0.11516852676868439 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.6288],\n",
      "        [0.5911]], device='mps:0')\n",
      "Iteration 3900 Training loss 0.11412116885185242 Validation loss 0.11517573893070221 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.6245],\n",
      "        [0.6400]], device='mps:0')\n",
      "Iteration 3910 Training loss 0.11655853688716888 Validation loss 0.11511804908514023 Accuracy 0.643500030040741\n",
      "Output tensor([[0.4668],\n",
      "        [0.5589]], device='mps:0')\n",
      "Iteration 3920 Training loss 0.11838719993829727 Validation loss 0.11511795222759247 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5378],\n",
      "        [0.3746]], device='mps:0')\n",
      "Iteration 3930 Training loss 0.11245973408222198 Validation loss 0.11508059501647949 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.2755],\n",
      "        [0.3805]], device='mps:0')\n",
      "Iteration 3940 Training loss 0.12365710735321045 Validation loss 0.1150418370962143 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.4751],\n",
      "        [0.5054]], device='mps:0')\n",
      "Iteration 3950 Training loss 0.11546725779771805 Validation loss 0.11504162847995758 Accuracy 0.643500030040741\n",
      "Output tensor([[0.6056],\n",
      "        [0.4460]], device='mps:0')\n",
      "Iteration 3960 Training loss 0.11430574953556061 Validation loss 0.11502246558666229 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.5058],\n",
      "        [0.5527]], device='mps:0')\n",
      "Iteration 3970 Training loss 0.11632173508405685 Validation loss 0.11499295383691788 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5032],\n",
      "        [0.6166]], device='mps:0')\n",
      "Iteration 3980 Training loss 0.1172638013958931 Validation loss 0.11499485373497009 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.5788],\n",
      "        [0.5473]], device='mps:0')\n",
      "Iteration 3990 Training loss 0.11165831983089447 Validation loss 0.11496035009622574 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5173],\n",
      "        [0.4975]], device='mps:0')\n",
      "Iteration 4000 Training loss 0.1174338087439537 Validation loss 0.11495713144540787 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5767],\n",
      "        [0.5087]], device='mps:0')\n",
      "Iteration 4010 Training loss 0.11570145934820175 Validation loss 0.11496042460203171 Accuracy 0.643500030040741\n",
      "Output tensor([[0.4349],\n",
      "        [0.5481]], device='mps:0')\n",
      "Iteration 4020 Training loss 0.11418166756629944 Validation loss 0.11495732516050339 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5560],\n",
      "        [0.5430]], device='mps:0')\n",
      "Iteration 4030 Training loss 0.1167212650179863 Validation loss 0.11493794620037079 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.4748],\n",
      "        [0.6158]], device='mps:0')\n",
      "Iteration 4040 Training loss 0.11315441131591797 Validation loss 0.11493371427059174 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5467],\n",
      "        [0.5303]], device='mps:0')\n",
      "Iteration 4050 Training loss 0.11254049837589264 Validation loss 0.11492694914340973 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.6252],\n",
      "        [0.3734]], device='mps:0')\n",
      "Iteration 4060 Training loss 0.12201259285211563 Validation loss 0.11493115872144699 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.5694],\n",
      "        [0.3471]], device='mps:0')\n",
      "Iteration 4070 Training loss 0.11846223473548889 Validation loss 0.11488334834575653 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5254],\n",
      "        [0.6250]], device='mps:0')\n",
      "Iteration 4080 Training loss 0.11766554415225983 Validation loss 0.11486722528934479 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.4047],\n",
      "        [0.3406]], device='mps:0')\n",
      "Iteration 4090 Training loss 0.11123944818973541 Validation loss 0.11487025767564774 Accuracy 0.643500030040741\n",
      "Output tensor([[0.4641],\n",
      "        [0.5628]], device='mps:0')\n",
      "Iteration 4100 Training loss 0.11607035994529724 Validation loss 0.11483664065599442 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.4942],\n",
      "        [0.4706]], device='mps:0')\n",
      "Iteration 4110 Training loss 0.11897793412208557 Validation loss 0.11485153436660767 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.6040],\n",
      "        [0.6482]], device='mps:0')\n",
      "Iteration 4120 Training loss 0.11173147708177567 Validation loss 0.11483809351921082 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5834],\n",
      "        [0.6303]], device='mps:0')\n",
      "Iteration 4130 Training loss 0.11644913256168365 Validation loss 0.11482911556959152 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5120],\n",
      "        [0.3814]], device='mps:0')\n",
      "Iteration 4140 Training loss 0.10896891355514526 Validation loss 0.11480323225259781 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.4530],\n",
      "        [0.4545]], device='mps:0')\n",
      "Iteration 4150 Training loss 0.1209869235754013 Validation loss 0.11480112373828888 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.4451],\n",
      "        [0.5273]], device='mps:0')\n",
      "Iteration 4160 Training loss 0.11319917440414429 Validation loss 0.11477574706077576 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.3936],\n",
      "        [0.3573]], device='mps:0')\n",
      "Iteration 4170 Training loss 0.11496561020612717 Validation loss 0.1147552877664566 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.4970],\n",
      "        [0.4551]], device='mps:0')\n",
      "Iteration 4180 Training loss 0.11775068193674088 Validation loss 0.11476372927427292 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5752],\n",
      "        [0.5223]], device='mps:0')\n",
      "Iteration 4190 Training loss 0.11689189821481705 Validation loss 0.11474042385816574 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5287],\n",
      "        [0.5047]], device='mps:0')\n",
      "Iteration 4200 Training loss 0.12111030519008636 Validation loss 0.114736407995224 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5287],\n",
      "        [0.3246]], device='mps:0')\n",
      "Iteration 4210 Training loss 0.11356135457754135 Validation loss 0.11472216993570328 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.6090],\n",
      "        [0.4573]], device='mps:0')\n",
      "Iteration 4220 Training loss 0.11158604174852371 Validation loss 0.114735908806324 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5037],\n",
      "        [0.4461]], device='mps:0')\n",
      "Iteration 4230 Training loss 0.10891958326101303 Validation loss 0.11472281068563461 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4689],\n",
      "        [0.5399]], device='mps:0')\n",
      "Iteration 4240 Training loss 0.1125277653336525 Validation loss 0.11468052864074707 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5404],\n",
      "        [0.4431]], device='mps:0')\n",
      "Iteration 4250 Training loss 0.11886269599199295 Validation loss 0.11466499418020248 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5550],\n",
      "        [0.4802]], device='mps:0')\n",
      "Iteration 4260 Training loss 0.1139034554362297 Validation loss 0.11465971916913986 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5194],\n",
      "        [0.3877]], device='mps:0')\n",
      "Iteration 4270 Training loss 0.117611825466156 Validation loss 0.11465684324502945 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.6304],\n",
      "        [0.4912]], device='mps:0')\n",
      "Iteration 4280 Training loss 0.11168242990970612 Validation loss 0.11469727009534836 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.4514],\n",
      "        [0.6834]], device='mps:0')\n",
      "Iteration 4290 Training loss 0.10971055924892426 Validation loss 0.11467528343200684 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5867],\n",
      "        [0.4982]], device='mps:0')\n",
      "Iteration 4300 Training loss 0.11156173795461655 Validation loss 0.11465547978878021 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.4096],\n",
      "        [0.4975]], device='mps:0')\n",
      "Iteration 4310 Training loss 0.11682358384132385 Validation loss 0.11467501521110535 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.5484],\n",
      "        [0.5201]], device='mps:0')\n",
      "Iteration 4320 Training loss 0.11102067679166794 Validation loss 0.11461929231882095 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4591],\n",
      "        [0.4584]], device='mps:0')\n",
      "Iteration 4330 Training loss 0.11124547570943832 Validation loss 0.1145997866988182 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5244],\n",
      "        [0.2430]], device='mps:0')\n",
      "Iteration 4340 Training loss 0.11153937876224518 Validation loss 0.114562027156353 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4963],\n",
      "        [0.5359]], device='mps:0')\n",
      "Iteration 4350 Training loss 0.11696670204401016 Validation loss 0.11455049365758896 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5292],\n",
      "        [0.5638]], device='mps:0')\n",
      "Iteration 4360 Training loss 0.11706157773733139 Validation loss 0.1145419329404831 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4074],\n",
      "        [0.3745]], device='mps:0')\n",
      "Iteration 4370 Training loss 0.10906554758548737 Validation loss 0.11452925205230713 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.3045],\n",
      "        [0.4839]], device='mps:0')\n",
      "Iteration 4380 Training loss 0.1146683320403099 Validation loss 0.11451557278633118 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5145],\n",
      "        [0.5263]], device='mps:0')\n",
      "Iteration 4390 Training loss 0.11724305897951126 Validation loss 0.11452019959688187 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.2993],\n",
      "        [0.5198]], device='mps:0')\n",
      "Iteration 4400 Training loss 0.11229383200407028 Validation loss 0.11452513933181763 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.5459],\n",
      "        [0.5043]], device='mps:0')\n",
      "Iteration 4410 Training loss 0.1132671907544136 Validation loss 0.11454267054796219 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.4172],\n",
      "        [0.3858]], device='mps:0')\n",
      "Iteration 4420 Training loss 0.11467240005731583 Validation loss 0.11447693407535553 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5706],\n",
      "        [0.5141]], device='mps:0')\n",
      "Iteration 4430 Training loss 0.11309526115655899 Validation loss 0.1144673302769661 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.3637],\n",
      "        [0.4293]], device='mps:0')\n",
      "Iteration 4440 Training loss 0.10902246087789536 Validation loss 0.11445567011833191 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4433],\n",
      "        [0.5884]], device='mps:0')\n",
      "Iteration 4450 Training loss 0.11773453652858734 Validation loss 0.1144430935382843 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4174],\n",
      "        [0.5057]], device='mps:0')\n",
      "Iteration 4460 Training loss 0.11102107167243958 Validation loss 0.11443382501602173 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5542],\n",
      "        [0.5789]], device='mps:0')\n",
      "Iteration 4470 Training loss 0.11977314949035645 Validation loss 0.11443313956260681 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4415],\n",
      "        [0.5053]], device='mps:0')\n",
      "Iteration 4480 Training loss 0.11388681083917618 Validation loss 0.11443593353033066 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.5178],\n",
      "        [0.5334]], device='mps:0')\n",
      "Iteration 4490 Training loss 0.1145872101187706 Validation loss 0.11443071067333221 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4685],\n",
      "        [0.6337]], device='mps:0')\n",
      "Iteration 4500 Training loss 0.1167500764131546 Validation loss 0.11438962817192078 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4837],\n",
      "        [0.5491]], device='mps:0')\n",
      "Iteration 4510 Training loss 0.11531562358140945 Validation loss 0.11438411474227905 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5513],\n",
      "        [0.5579]], device='mps:0')\n",
      "Iteration 4520 Training loss 0.12135475128889084 Validation loss 0.11440205574035645 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.4478],\n",
      "        [0.4312]], device='mps:0')\n",
      "Iteration 4530 Training loss 0.11237149685621262 Validation loss 0.11440352350473404 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.4023],\n",
      "        [0.5270]], device='mps:0')\n",
      "Iteration 4540 Training loss 0.1191655620932579 Validation loss 0.11438947916030884 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4594],\n",
      "        [0.5252]], device='mps:0')\n",
      "Iteration 4550 Training loss 0.11298199743032455 Validation loss 0.1143733412027359 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.6136],\n",
      "        [0.6025]], device='mps:0')\n",
      "Iteration 4560 Training loss 0.11400701850652695 Validation loss 0.11442909389734268 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5484],\n",
      "        [0.3037]], device='mps:0')\n",
      "Iteration 4570 Training loss 0.10685066133737564 Validation loss 0.1143651008605957 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4761],\n",
      "        [0.6032]], device='mps:0')\n",
      "Iteration 4580 Training loss 0.10915794223546982 Validation loss 0.11436518281698227 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.2419],\n",
      "        [0.5054]], device='mps:0')\n",
      "Iteration 4590 Training loss 0.11234426498413086 Validation loss 0.11434145271778107 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.6300],\n",
      "        [0.5982]], device='mps:0')\n",
      "Iteration 4600 Training loss 0.1170734390616417 Validation loss 0.1143621876835823 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5541],\n",
      "        [0.5320]], device='mps:0')\n",
      "Iteration 4610 Training loss 0.10856345295906067 Validation loss 0.11432491987943649 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.4946],\n",
      "        [0.4296]], device='mps:0')\n",
      "Iteration 4620 Training loss 0.11319338530302048 Validation loss 0.11430899798870087 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.5677],\n",
      "        [0.5624]], device='mps:0')\n",
      "Iteration 4630 Training loss 0.11627492308616638 Validation loss 0.11433110386133194 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5525],\n",
      "        [0.5364]], device='mps:0')\n",
      "Iteration 4640 Training loss 0.11410533636808395 Validation loss 0.11431021988391876 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5792],\n",
      "        [0.5115]], device='mps:0')\n",
      "Iteration 4650 Training loss 0.11590446531772614 Validation loss 0.11425168067216873 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5681],\n",
      "        [0.4458]], device='mps:0')\n",
      "Iteration 4660 Training loss 0.10414961725473404 Validation loss 0.11421538144350052 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4192],\n",
      "        [0.4064]], device='mps:0')\n",
      "Iteration 4670 Training loss 0.11776093393564224 Validation loss 0.11419986188411713 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4402],\n",
      "        [0.5180]], device='mps:0')\n",
      "Iteration 4680 Training loss 0.11652631312608719 Validation loss 0.11418522894382477 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4427],\n",
      "        [0.3995]], device='mps:0')\n",
      "Iteration 4690 Training loss 0.11270904541015625 Validation loss 0.1141752079129219 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.2862],\n",
      "        [0.5355]], device='mps:0')\n",
      "Iteration 4700 Training loss 0.11297540366649628 Validation loss 0.11416731774806976 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5592],\n",
      "        [0.4731]], device='mps:0')\n",
      "Iteration 4710 Training loss 0.11520104855298996 Validation loss 0.1141594722867012 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5252],\n",
      "        [0.5100]], device='mps:0')\n",
      "Iteration 4720 Training loss 0.11624889075756073 Validation loss 0.1141488254070282 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5666],\n",
      "        [0.6207]], device='mps:0')\n",
      "Iteration 4730 Training loss 0.11094409972429276 Validation loss 0.11415089666843414 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5387],\n",
      "        [0.5233]], device='mps:0')\n",
      "Iteration 4740 Training loss 0.11434711515903473 Validation loss 0.11413992941379547 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5640],\n",
      "        [0.4753]], device='mps:0')\n",
      "Iteration 4750 Training loss 0.11938733607530594 Validation loss 0.11412592232227325 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.4093],\n",
      "        [0.4471]], device='mps:0')\n",
      "Iteration 4760 Training loss 0.11312856525182724 Validation loss 0.11410728842020035 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4441],\n",
      "        [0.5593]], device='mps:0')\n",
      "Iteration 4770 Training loss 0.11360051482915878 Validation loss 0.11410536617040634 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5333],\n",
      "        [0.4031]], device='mps:0')\n",
      "Iteration 4780 Training loss 0.11692919582128525 Validation loss 0.11410211771726608 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5031],\n",
      "        [0.4050]], device='mps:0')\n",
      "Iteration 4790 Training loss 0.10842231661081314 Validation loss 0.11408229917287827 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5446],\n",
      "        [0.3678]], device='mps:0')\n",
      "Iteration 4800 Training loss 0.11744369566440582 Validation loss 0.11406810581684113 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4894],\n",
      "        [0.5380]], device='mps:0')\n",
      "Iteration 4810 Training loss 0.11141300946474075 Validation loss 0.11407531052827835 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.3809],\n",
      "        [0.4984]], device='mps:0')\n",
      "Iteration 4820 Training loss 0.1130036786198616 Validation loss 0.11405100673437119 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5558],\n",
      "        [0.3403]], device='mps:0')\n",
      "Iteration 4830 Training loss 0.11728093773126602 Validation loss 0.11404457688331604 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5002],\n",
      "        [0.3747]], device='mps:0')\n",
      "Iteration 4840 Training loss 0.11219556629657745 Validation loss 0.11402574926614761 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4526],\n",
      "        [0.4253]], device='mps:0')\n",
      "Iteration 4850 Training loss 0.11960746347904205 Validation loss 0.11402015388011932 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5921],\n",
      "        [0.5422]], device='mps:0')\n",
      "Iteration 4860 Training loss 0.11843594163656235 Validation loss 0.11402641981840134 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5818],\n",
      "        [0.4843]], device='mps:0')\n",
      "Iteration 4870 Training loss 0.10269863903522491 Validation loss 0.11400872468948364 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5339],\n",
      "        [0.4752]], device='mps:0')\n",
      "Iteration 4880 Training loss 0.11606695502996445 Validation loss 0.1139783039689064 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.6151],\n",
      "        [0.6043]], device='mps:0')\n",
      "Iteration 4890 Training loss 0.10631061345338821 Validation loss 0.11396785825490952 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5346],\n",
      "        [0.4952]], device='mps:0')\n",
      "Iteration 4900 Training loss 0.12578190863132477 Validation loss 0.11395830661058426 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4041],\n",
      "        [0.4017]], device='mps:0')\n",
      "Iteration 4910 Training loss 0.11894121021032333 Validation loss 0.11395839601755142 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.3976],\n",
      "        [0.4780]], device='mps:0')\n",
      "Iteration 4920 Training loss 0.11149448901414871 Validation loss 0.11393900960683823 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.4733],\n",
      "        [0.3521]], device='mps:0')\n",
      "Iteration 4930 Training loss 0.10531831532716751 Validation loss 0.11393845826387405 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.6344],\n",
      "        [0.5041]], device='mps:0')\n",
      "Iteration 4940 Training loss 0.11335691064596176 Validation loss 0.11392150074243546 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.3945],\n",
      "        [0.5438]], device='mps:0')\n",
      "Iteration 4950 Training loss 0.10529693216085434 Validation loss 0.11391309648752213 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.3397],\n",
      "        [0.5071]], device='mps:0')\n",
      "Iteration 4960 Training loss 0.11477541923522949 Validation loss 0.1139097660779953 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.3839],\n",
      "        [0.4988]], device='mps:0')\n",
      "Iteration 4970 Training loss 0.11075459420681 Validation loss 0.11389992386102676 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5573],\n",
      "        [0.3519]], device='mps:0')\n",
      "Iteration 4980 Training loss 0.10931967198848724 Validation loss 0.11388243734836578 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.2987],\n",
      "        [0.4292]], device='mps:0')\n",
      "Iteration 4990 Training loss 0.11540450900793076 Validation loss 0.11388286203145981 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5811],\n",
      "        [0.6056]], device='mps:0')\n",
      "Iteration 5000 Training loss 0.10659260302782059 Validation loss 0.11387554556131363 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5648],\n",
      "        [0.5312]], device='mps:0')\n",
      "Iteration 5010 Training loss 0.11266788840293884 Validation loss 0.11388016492128372 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5721],\n",
      "        [0.7099]], device='mps:0')\n",
      "Iteration 5020 Training loss 0.11616411805152893 Validation loss 0.11384926736354828 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.3724],\n",
      "        [0.5466]], device='mps:0')\n",
      "Iteration 5030 Training loss 0.11030231416225433 Validation loss 0.11383713036775589 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.5178],\n",
      "        [0.5425]], device='mps:0')\n",
      "Iteration 5040 Training loss 0.10785228759050369 Validation loss 0.11382698267698288 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5637],\n",
      "        [0.5137]], device='mps:0')\n",
      "Iteration 5050 Training loss 0.11574210226535797 Validation loss 0.11385693401098251 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4611],\n",
      "        [0.4998]], device='mps:0')\n",
      "Iteration 5060 Training loss 0.10919567197561264 Validation loss 0.11383754760026932 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5892],\n",
      "        [0.5624]], device='mps:0')\n",
      "Iteration 5070 Training loss 0.11190885305404663 Validation loss 0.11383721977472305 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5975],\n",
      "        [0.5280]], device='mps:0')\n",
      "Iteration 5080 Training loss 0.10898192971944809 Validation loss 0.11384283006191254 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.3869],\n",
      "        [0.5876]], device='mps:0')\n",
      "Iteration 5090 Training loss 0.1075473353266716 Validation loss 0.11387008428573608 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.5711],\n",
      "        [0.5844]], device='mps:0')\n",
      "Iteration 5100 Training loss 0.12121699005365372 Validation loss 0.11380159854888916 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.6011],\n",
      "        [0.5757]], device='mps:0')\n",
      "Iteration 5110 Training loss 0.12065879255533218 Validation loss 0.11376798897981644 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5271],\n",
      "        [0.5108]], device='mps:0')\n",
      "Iteration 5120 Training loss 0.1150006651878357 Validation loss 0.1137903481721878 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5190],\n",
      "        [0.5122]], device='mps:0')\n",
      "Iteration 5130 Training loss 0.1065329909324646 Validation loss 0.11379790306091309 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5030],\n",
      "        [0.3717]], device='mps:0')\n",
      "Iteration 5140 Training loss 0.1107562780380249 Validation loss 0.11375715583562851 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5144],\n",
      "        [0.5789]], device='mps:0')\n",
      "Iteration 5150 Training loss 0.12663401663303375 Validation loss 0.11374527215957642 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5965],\n",
      "        [0.5402]], device='mps:0')\n",
      "Iteration 5160 Training loss 0.1103663221001625 Validation loss 0.11373662203550339 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.6120],\n",
      "        [0.5494]], device='mps:0')\n",
      "Iteration 5170 Training loss 0.11120551079511642 Validation loss 0.1137436032295227 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5657],\n",
      "        [0.4843]], device='mps:0')\n",
      "Iteration 5180 Training loss 0.10965791344642639 Validation loss 0.1137160062789917 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.4844],\n",
      "        [0.5214]], device='mps:0')\n",
      "Iteration 5190 Training loss 0.10872398316860199 Validation loss 0.11368778347969055 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5145],\n",
      "        [0.5633]], device='mps:0')\n",
      "Iteration 5200 Training loss 0.1088133156299591 Validation loss 0.11368349939584732 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.4646],\n",
      "        [0.3901]], device='mps:0')\n",
      "Iteration 5210 Training loss 0.10339980572462082 Validation loss 0.11364828795194626 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5588],\n",
      "        [0.4655]], device='mps:0')\n",
      "Iteration 5220 Training loss 0.11759830266237259 Validation loss 0.11368777602910995 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.3323],\n",
      "        [0.4409]], device='mps:0')\n",
      "Iteration 5230 Training loss 0.11777466535568237 Validation loss 0.11363491415977478 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.4681],\n",
      "        [0.5164]], device='mps:0')\n",
      "Iteration 5240 Training loss 0.11785842478275299 Validation loss 0.11362366378307343 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.3865],\n",
      "        [0.5371]], device='mps:0')\n",
      "Iteration 5250 Training loss 0.1159057766199112 Validation loss 0.11362170428037643 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.3285],\n",
      "        [0.5875]], device='mps:0')\n",
      "Iteration 5260 Training loss 0.11044759303331375 Validation loss 0.11360049992799759 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5230],\n",
      "        [0.5649]], device='mps:0')\n",
      "Iteration 5270 Training loss 0.1110241711139679 Validation loss 0.11358381062746048 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5340],\n",
      "        [0.5522]], device='mps:0')\n",
      "Iteration 5280 Training loss 0.11215640604496002 Validation loss 0.11357563734054565 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.3331],\n",
      "        [0.3394]], device='mps:0')\n",
      "Iteration 5290 Training loss 0.11404649168252945 Validation loss 0.11357223242521286 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5901],\n",
      "        [0.3087]], device='mps:0')\n",
      "Iteration 5300 Training loss 0.11970708519220352 Validation loss 0.11356574296951294 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.6016],\n",
      "        [0.4167]], device='mps:0')\n",
      "Iteration 5310 Training loss 0.12439104169607162 Validation loss 0.11355191469192505 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5648],\n",
      "        [0.2558]], device='mps:0')\n",
      "Iteration 5320 Training loss 0.11479552090167999 Validation loss 0.11354116350412369 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5965],\n",
      "        [0.6713]], device='mps:0')\n",
      "Iteration 5330 Training loss 0.11005578935146332 Validation loss 0.11353030055761337 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.3568],\n",
      "        [0.3969]], device='mps:0')\n",
      "Iteration 5340 Training loss 0.1212272047996521 Validation loss 0.1135164201259613 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4688],\n",
      "        [0.5671]], device='mps:0')\n",
      "Iteration 5350 Training loss 0.10963740944862366 Validation loss 0.11350900679826736 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4003],\n",
      "        [0.4836]], device='mps:0')\n",
      "Iteration 5360 Training loss 0.11325114965438843 Validation loss 0.11349868774414062 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.3991],\n",
      "        [0.5827]], device='mps:0')\n",
      "Iteration 5370 Training loss 0.11770519614219666 Validation loss 0.11348860710859299 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.3574],\n",
      "        [0.4085]], device='mps:0')\n",
      "Iteration 5380 Training loss 0.11849372833967209 Validation loss 0.11347901076078415 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5053],\n",
      "        [0.5593]], device='mps:0')\n",
      "Iteration 5390 Training loss 0.11268815398216248 Validation loss 0.11347132921218872 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4418],\n",
      "        [0.5799]], device='mps:0')\n",
      "Iteration 5400 Training loss 0.11505887657403946 Validation loss 0.11345819383859634 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4271],\n",
      "        [0.5898]], device='mps:0')\n",
      "Iteration 5410 Training loss 0.1114962249994278 Validation loss 0.11345473676919937 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.2201],\n",
      "        [0.5396]], device='mps:0')\n",
      "Iteration 5420 Training loss 0.10681618750095367 Validation loss 0.11346190422773361 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.6022],\n",
      "        [0.5249]], device='mps:0')\n",
      "Iteration 5430 Training loss 0.11531605571508408 Validation loss 0.11344357579946518 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.3818],\n",
      "        [0.4869]], device='mps:0')\n",
      "Iteration 5440 Training loss 0.10784272849559784 Validation loss 0.11343960464000702 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.4126],\n",
      "        [0.2805]], device='mps:0')\n",
      "Iteration 5450 Training loss 0.11909282207489014 Validation loss 0.11341499537229538 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5502],\n",
      "        [0.4255]], device='mps:0')\n",
      "Iteration 5460 Training loss 0.11468660831451416 Validation loss 0.11340247094631195 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5500],\n",
      "        [0.5806]], device='mps:0')\n",
      "Iteration 5470 Training loss 0.11848796904087067 Validation loss 0.11340628564357758 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5411],\n",
      "        [0.5681]], device='mps:0')\n",
      "Iteration 5480 Training loss 0.10649215430021286 Validation loss 0.11341696977615356 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.2155],\n",
      "        [0.4206]], device='mps:0')\n",
      "Iteration 5490 Training loss 0.11908476799726486 Validation loss 0.11337351053953171 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5032],\n",
      "        [0.5657]], device='mps:0')\n",
      "Iteration 5500 Training loss 0.11487238854169846 Validation loss 0.11337267607450485 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5051],\n",
      "        [0.5297]], device='mps:0')\n",
      "Iteration 5510 Training loss 0.12006096541881561 Validation loss 0.11337024718523026 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5470],\n",
      "        [0.3752]], device='mps:0')\n",
      "Iteration 5520 Training loss 0.11531184613704681 Validation loss 0.11338160932064056 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.3560],\n",
      "        [0.2653]], device='mps:0')\n",
      "Iteration 5530 Training loss 0.11187894642353058 Validation loss 0.11336534470319748 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5746],\n",
      "        [0.4332]], device='mps:0')\n",
      "Iteration 5540 Training loss 0.11213292926549911 Validation loss 0.11335267871618271 Accuracy 0.655500054359436\n",
      "Output tensor([[0.5405],\n",
      "        [0.6875]], device='mps:0')\n",
      "Iteration 5550 Training loss 0.11172576993703842 Validation loss 0.11334716528654099 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.4201],\n",
      "        [0.3215]], device='mps:0')\n",
      "Iteration 5560 Training loss 0.11444681137800217 Validation loss 0.11333253979682922 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.4544],\n",
      "        [0.5531]], device='mps:0')\n",
      "Iteration 5570 Training loss 0.1134839728474617 Validation loss 0.11332585662603378 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.4362],\n",
      "        [0.3645]], device='mps:0')\n",
      "Iteration 5580 Training loss 0.1155121847987175 Validation loss 0.11335352808237076 Accuracy 0.655500054359436\n",
      "Output tensor([[0.4787],\n",
      "        [0.5592]], device='mps:0')\n",
      "Iteration 5590 Training loss 0.1146005392074585 Validation loss 0.1133294403553009 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5592],\n",
      "        [0.5604]], device='mps:0')\n",
      "Iteration 5600 Training loss 0.10749729722738266 Validation loss 0.11330409348011017 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.6351],\n",
      "        [0.5781]], device='mps:0')\n",
      "Iteration 5610 Training loss 0.11362778395414352 Validation loss 0.1133052259683609 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5782],\n",
      "        [0.5900]], device='mps:0')\n",
      "Iteration 5620 Training loss 0.11642052978277206 Validation loss 0.11328750848770142 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.4162],\n",
      "        [0.4482]], device='mps:0')\n",
      "Iteration 5630 Training loss 0.11734668165445328 Validation loss 0.11326495558023453 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5350],\n",
      "        [0.5755]], device='mps:0')\n",
      "Iteration 5640 Training loss 0.1163172498345375 Validation loss 0.11325433850288391 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.3287],\n",
      "        [0.5389]], device='mps:0')\n",
      "Iteration 5650 Training loss 0.11703215539455414 Validation loss 0.113233283162117 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.4817],\n",
      "        [0.3164]], device='mps:0')\n",
      "Iteration 5660 Training loss 0.11558032035827637 Validation loss 0.11322630196809769 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5804],\n",
      "        [0.6696]], device='mps:0')\n",
      "Iteration 5670 Training loss 0.11566617339849472 Validation loss 0.11320900171995163 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.6083],\n",
      "        [0.5453]], device='mps:0')\n",
      "Iteration 5680 Training loss 0.1081918478012085 Validation loss 0.11319905519485474 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5158],\n",
      "        [0.3487]], device='mps:0')\n",
      "Iteration 5690 Training loss 0.1107013002038002 Validation loss 0.11319433152675629 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.5254],\n",
      "        [0.3805]], device='mps:0')\n",
      "Iteration 5700 Training loss 0.11589227616786957 Validation loss 0.1131805032491684 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5518],\n",
      "        [0.2821]], device='mps:0')\n",
      "Iteration 5710 Training loss 0.1123039573431015 Validation loss 0.1131710410118103 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5474],\n",
      "        [0.4887]], device='mps:0')\n",
      "Iteration 5720 Training loss 0.11751735210418701 Validation loss 0.11316300928592682 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.6265],\n",
      "        [0.6659]], device='mps:0')\n",
      "Iteration 5730 Training loss 0.11562356352806091 Validation loss 0.11315523087978363 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.6076],\n",
      "        [0.3708]], device='mps:0')\n",
      "Iteration 5740 Training loss 0.11662163585424423 Validation loss 0.11314224451780319 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.6237],\n",
      "        [0.4831]], device='mps:0')\n",
      "Iteration 5750 Training loss 0.11326902359724045 Validation loss 0.11313704401254654 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5404],\n",
      "        [0.6442]], device='mps:0')\n",
      "Iteration 5760 Training loss 0.11779745668172836 Validation loss 0.11315060406923294 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.4411],\n",
      "        [0.5392]], device='mps:0')\n",
      "Iteration 5770 Training loss 0.11293429881334305 Validation loss 0.11315000057220459 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5544],\n",
      "        [0.5534]], device='mps:0')\n",
      "Iteration 5780 Training loss 0.11025489866733551 Validation loss 0.11313174664974213 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.3934],\n",
      "        [0.4772]], device='mps:0')\n",
      "Iteration 5790 Training loss 0.11082687228918076 Validation loss 0.11310601979494095 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5119],\n",
      "        [0.4715]], device='mps:0')\n",
      "Iteration 5800 Training loss 0.11330097168684006 Validation loss 0.11311210691928864 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.6082],\n",
      "        [0.5065]], device='mps:0')\n",
      "Iteration 5810 Training loss 0.10947488248348236 Validation loss 0.11309061199426651 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5444],\n",
      "        [0.5599]], device='mps:0')\n",
      "Iteration 5820 Training loss 0.11632569134235382 Validation loss 0.11307702958583832 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5146],\n",
      "        [0.5494]], device='mps:0')\n",
      "Iteration 5830 Training loss 0.10969119518995285 Validation loss 0.11306992173194885 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.4258],\n",
      "        [0.4885]], device='mps:0')\n",
      "Iteration 5840 Training loss 0.10998071730136871 Validation loss 0.11305156350135803 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4486],\n",
      "        [0.5364]], device='mps:0')\n",
      "Iteration 5850 Training loss 0.1165148988366127 Validation loss 0.1130475178360939 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5834],\n",
      "        [0.5916]], device='mps:0')\n",
      "Iteration 5860 Training loss 0.11718883365392685 Validation loss 0.113038569688797 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5294],\n",
      "        [0.4881]], device='mps:0')\n",
      "Iteration 5870 Training loss 0.11123988032341003 Validation loss 0.11302991211414337 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5573],\n",
      "        [0.4907]], device='mps:0')\n",
      "Iteration 5880 Training loss 0.11401773244142532 Validation loss 0.11302237212657928 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.4481],\n",
      "        [0.4875]], device='mps:0')\n",
      "Iteration 5890 Training loss 0.11027852445840836 Validation loss 0.11301030963659286 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4768],\n",
      "        [0.3088]], device='mps:0')\n",
      "Iteration 5900 Training loss 0.1149318739771843 Validation loss 0.11299610137939453 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4597],\n",
      "        [0.5076]], device='mps:0')\n",
      "Iteration 5910 Training loss 0.10910215228796005 Validation loss 0.11300226300954819 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.4156],\n",
      "        [0.5408]], device='mps:0')\n",
      "Iteration 5920 Training loss 0.1208072230219841 Validation loss 0.11298845708370209 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5370],\n",
      "        [0.4757]], device='mps:0')\n",
      "Iteration 5930 Training loss 0.1126241385936737 Validation loss 0.1129821240901947 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.5893],\n",
      "        [0.4432]], device='mps:0')\n",
      "Iteration 5940 Training loss 0.11299587786197662 Validation loss 0.11297111958265305 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5451],\n",
      "        [0.5929]], device='mps:0')\n",
      "Iteration 5950 Training loss 0.11329751461744308 Validation loss 0.1129612922668457 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5668],\n",
      "        [0.4244]], device='mps:0')\n",
      "Iteration 5960 Training loss 0.11848064512014389 Validation loss 0.11298748105764389 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.4648],\n",
      "        [0.4866]], device='mps:0')\n",
      "Iteration 5970 Training loss 0.11871253699064255 Validation loss 0.11299554258584976 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.5128],\n",
      "        [0.4701]], device='mps:0')\n",
      "Iteration 5980 Training loss 0.11340124160051346 Validation loss 0.11300087720155716 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.5210],\n",
      "        [0.4980]], device='mps:0')\n",
      "Iteration 5990 Training loss 0.11042378842830658 Validation loss 0.11294496059417725 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.5474],\n",
      "        [0.3029]], device='mps:0')\n",
      "Iteration 6000 Training loss 0.11414827406406403 Validation loss 0.11293742805719376 Accuracy 0.655500054359436\n",
      "Output tensor([[0.4230],\n",
      "        [0.5847]], device='mps:0')\n",
      "Iteration 6010 Training loss 0.10828445106744766 Validation loss 0.11295433342456818 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.4909],\n",
      "        [0.4148]], device='mps:0')\n",
      "Iteration 6020 Training loss 0.11482017487287521 Validation loss 0.11296287924051285 Accuracy 0.659000039100647\n",
      "Output tensor([[0.4422],\n",
      "        [0.4062]], device='mps:0')\n",
      "Iteration 6030 Training loss 0.10762409120798111 Validation loss 0.11288420855998993 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.3720],\n",
      "        [0.4383]], device='mps:0')\n",
      "Iteration 6040 Training loss 0.11656048148870468 Validation loss 0.11288058757781982 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.3725],\n",
      "        [0.5149]], device='mps:0')\n",
      "Iteration 6050 Training loss 0.11246631294488907 Validation loss 0.11285784095525742 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5125],\n",
      "        [0.6314]], device='mps:0')\n",
      "Iteration 6060 Training loss 0.11381348967552185 Validation loss 0.11284938454627991 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5396],\n",
      "        [0.3576]], device='mps:0')\n",
      "Iteration 6070 Training loss 0.11200662702322006 Validation loss 0.11283736675977707 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.3493],\n",
      "        [0.2395]], device='mps:0')\n",
      "Iteration 6080 Training loss 0.1149371862411499 Validation loss 0.11283604800701141 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.3337],\n",
      "        [0.5473]], device='mps:0')\n",
      "Iteration 6090 Training loss 0.11510733515024185 Validation loss 0.11282067000865936 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5302],\n",
      "        [0.4880]], device='mps:0')\n",
      "Iteration 6100 Training loss 0.10713308304548264 Validation loss 0.11280826479196548 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.3454],\n",
      "        [0.6623]], device='mps:0')\n",
      "Iteration 6110 Training loss 0.1063520610332489 Validation loss 0.11281298100948334 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.6742],\n",
      "        [0.5114]], device='mps:0')\n",
      "Iteration 6120 Training loss 0.11589697748422623 Validation loss 0.1127898320555687 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4412],\n",
      "        [0.4024]], device='mps:0')\n",
      "Iteration 6130 Training loss 0.10064384341239929 Validation loss 0.11280430853366852 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.4536],\n",
      "        [0.3617]], device='mps:0')\n",
      "Iteration 6140 Training loss 0.12111414968967438 Validation loss 0.11277120560407639 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5574],\n",
      "        [0.5858]], device='mps:0')\n",
      "Iteration 6150 Training loss 0.10923206061124802 Validation loss 0.11275914311408997 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.6000],\n",
      "        [0.6085]], device='mps:0')\n",
      "Iteration 6160 Training loss 0.11060085147619247 Validation loss 0.1127495989203453 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5705],\n",
      "        [0.5074]], device='mps:0')\n",
      "Iteration 6170 Training loss 0.10321765393018723 Validation loss 0.1127556636929512 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.2373],\n",
      "        [0.3675]], device='mps:0')\n",
      "Iteration 6180 Training loss 0.12218005955219269 Validation loss 0.11274633556604385 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5949],\n",
      "        [0.5395]], device='mps:0')\n",
      "Iteration 6190 Training loss 0.11105632781982422 Validation loss 0.11276648938655853 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.4955],\n",
      "        [0.3118]], device='mps:0')\n",
      "Iteration 6200 Training loss 0.11975762248039246 Validation loss 0.11274347454309464 Accuracy 0.656000018119812\n",
      "Output tensor([[0.5163],\n",
      "        [0.4981]], device='mps:0')\n",
      "Iteration 6210 Training loss 0.11268198490142822 Validation loss 0.11270613223314285 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5832],\n",
      "        [0.5487]], device='mps:0')\n",
      "Iteration 6220 Training loss 0.10483899712562561 Validation loss 0.11269468814134598 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5483],\n",
      "        [0.5678]], device='mps:0')\n",
      "Iteration 6230 Training loss 0.11108952015638351 Validation loss 0.11268815398216248 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5059],\n",
      "        [0.3851]], device='mps:0')\n",
      "Iteration 6240 Training loss 0.11050030589103699 Validation loss 0.11267877370119095 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.6384],\n",
      "        [0.5895]], device='mps:0')\n",
      "Iteration 6250 Training loss 0.11818444728851318 Validation loss 0.11267173290252686 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4220],\n",
      "        [0.4413]], device='mps:0')\n",
      "Iteration 6260 Training loss 0.1160283163189888 Validation loss 0.11266054213047028 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5347],\n",
      "        [0.3635]], device='mps:0')\n",
      "Iteration 6270 Training loss 0.10987622290849686 Validation loss 0.1126519963145256 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.3573],\n",
      "        [0.3057]], device='mps:0')\n",
      "Iteration 6280 Training loss 0.11083593219518661 Validation loss 0.11264334619045258 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.4939],\n",
      "        [0.5238]], device='mps:0')\n",
      "Iteration 6290 Training loss 0.12044747918844223 Validation loss 0.11265069991350174 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4967],\n",
      "        [0.4317]], device='mps:0')\n",
      "Iteration 6300 Training loss 0.11454017460346222 Validation loss 0.11263773590326309 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5478],\n",
      "        [0.3329]], device='mps:0')\n",
      "Iteration 6310 Training loss 0.11405561864376068 Validation loss 0.11262376606464386 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5816],\n",
      "        [0.5358]], device='mps:0')\n",
      "Iteration 6320 Training loss 0.11771176010370255 Validation loss 0.11260572820901871 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.2301],\n",
      "        [0.4554]], device='mps:0')\n",
      "Iteration 6330 Training loss 0.11272985488176346 Validation loss 0.11260143667459488 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5940],\n",
      "        [0.5499]], device='mps:0')\n",
      "Iteration 6340 Training loss 0.11588089913129807 Validation loss 0.11259181797504425 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5150],\n",
      "        [0.4851]], device='mps:0')\n",
      "Iteration 6350 Training loss 0.11150022596120834 Validation loss 0.11258180439472198 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4038],\n",
      "        [0.5055]], device='mps:0')\n",
      "Iteration 6360 Training loss 0.11373122781515121 Validation loss 0.11257229745388031 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.3941],\n",
      "        [0.3278]], device='mps:0')\n",
      "Iteration 6370 Training loss 0.11031270772218704 Validation loss 0.11256318539381027 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5993],\n",
      "        [0.3700]], device='mps:0')\n",
      "Iteration 6380 Training loss 0.11501502990722656 Validation loss 0.11255510151386261 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.5863],\n",
      "        [0.5544]], device='mps:0')\n",
      "Iteration 6390 Training loss 0.11669391393661499 Validation loss 0.11254512518644333 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5344],\n",
      "        [0.4983]], device='mps:0')\n",
      "Iteration 6400 Training loss 0.11348670721054077 Validation loss 0.11254992336034775 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.4810],\n",
      "        [0.5236]], device='mps:0')\n",
      "Iteration 6410 Training loss 0.11560861021280289 Validation loss 0.1125333309173584 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5045],\n",
      "        [0.2669]], device='mps:0')\n",
      "Iteration 6420 Training loss 0.11311513930559158 Validation loss 0.11253015697002411 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5298],\n",
      "        [0.5938]], device='mps:0')\n",
      "Iteration 6430 Training loss 0.1163383424282074 Validation loss 0.11252696812152863 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.4245],\n",
      "        [0.4581]], device='mps:0')\n",
      "Iteration 6440 Training loss 0.10313089191913605 Validation loss 0.11252269148826599 Accuracy 0.656000018119812\n",
      "Output tensor([[0.5202],\n",
      "        [0.6570]], device='mps:0')\n",
      "Iteration 6450 Training loss 0.11084186285734177 Validation loss 0.1125291958451271 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.7059],\n",
      "        [0.3792]], device='mps:0')\n",
      "Iteration 6460 Training loss 0.11316157877445221 Validation loss 0.11250068992376328 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5042],\n",
      "        [0.4721]], device='mps:0')\n",
      "Iteration 6470 Training loss 0.11834248900413513 Validation loss 0.11251457780599594 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.5593],\n",
      "        [0.5016]], device='mps:0')\n",
      "Iteration 6480 Training loss 0.11547262966632843 Validation loss 0.11256247758865356 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.4392],\n",
      "        [0.5483]], device='mps:0')\n",
      "Iteration 6490 Training loss 0.11030670255422592 Validation loss 0.11252720654010773 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.6008],\n",
      "        [0.5953]], device='mps:0')\n",
      "Iteration 6500 Training loss 0.11192507296800613 Validation loss 0.1124906912446022 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.6009],\n",
      "        [0.6275]], device='mps:0')\n",
      "Iteration 6510 Training loss 0.1110953688621521 Validation loss 0.11246707290410995 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.3084],\n",
      "        [0.5321]], device='mps:0')\n",
      "Iteration 6520 Training loss 0.111948162317276 Validation loss 0.11244146525859833 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4735],\n",
      "        [0.6356]], device='mps:0')\n",
      "Iteration 6530 Training loss 0.11031472682952881 Validation loss 0.11243568360805511 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5359],\n",
      "        [0.4178]], device='mps:0')\n",
      "Iteration 6540 Training loss 0.10332418233156204 Validation loss 0.11242221295833588 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5853],\n",
      "        [0.5037]], device='mps:0')\n",
      "Iteration 6550 Training loss 0.11336559057235718 Validation loss 0.1124197468161583 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4506],\n",
      "        [0.5363]], device='mps:0')\n",
      "Iteration 6560 Training loss 0.11538948118686676 Validation loss 0.11241067200899124 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.6160],\n",
      "        [0.4591]], device='mps:0')\n",
      "Iteration 6570 Training loss 0.10732400417327881 Validation loss 0.11242200434207916 Accuracy 0.656000018119812\n",
      "Output tensor([[0.3325],\n",
      "        [0.5228]], device='mps:0')\n",
      "Iteration 6580 Training loss 0.11925466358661652 Validation loss 0.11241348832845688 Accuracy 0.655500054359436\n",
      "Output tensor([[0.4353],\n",
      "        [0.3671]], device='mps:0')\n",
      "Iteration 6590 Training loss 0.10884147137403488 Validation loss 0.11241354793310165 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.4091],\n",
      "        [0.5728]], device='mps:0')\n",
      "Iteration 6600 Training loss 0.10934305191040039 Validation loss 0.11239448189735413 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.6114],\n",
      "        [0.4646]], device='mps:0')\n",
      "Iteration 6610 Training loss 0.12152522802352905 Validation loss 0.1124148815870285 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.4942],\n",
      "        [0.3214]], device='mps:0')\n",
      "Iteration 6620 Training loss 0.11412801593542099 Validation loss 0.11245670914649963 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5442],\n",
      "        [0.4499]], device='mps:0')\n",
      "Iteration 6630 Training loss 0.12042894214391708 Validation loss 0.11242592334747314 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.5064],\n",
      "        [0.3225]], device='mps:0')\n",
      "Iteration 6640 Training loss 0.11482707411050797 Validation loss 0.1123974546790123 Accuracy 0.659500002861023\n",
      "Output tensor([[0.5478],\n",
      "        [0.4334]], device='mps:0')\n",
      "Iteration 6650 Training loss 0.11062410473823547 Validation loss 0.11236125230789185 Accuracy 0.659000039100647\n",
      "Output tensor([[0.3447],\n",
      "        [0.4778]], device='mps:0')\n",
      "Iteration 6660 Training loss 0.11348163336515427 Validation loss 0.11236339807510376 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.6009],\n",
      "        [0.6046]], device='mps:0')\n",
      "Iteration 6670 Training loss 0.10912228375673294 Validation loss 0.11238084733486176 Accuracy 0.659500002861023\n",
      "Output tensor([[0.4960],\n",
      "        [0.5801]], device='mps:0')\n",
      "Iteration 6680 Training loss 0.11879999190568924 Validation loss 0.11234019696712494 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.3385],\n",
      "        [0.3287]], device='mps:0')\n",
      "Iteration 6690 Training loss 0.10987526923418045 Validation loss 0.11237943917512894 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.5053],\n",
      "        [0.3123]], device='mps:0')\n",
      "Iteration 6700 Training loss 0.11115127056837082 Validation loss 0.11237604171037674 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.5449],\n",
      "        [0.5496]], device='mps:0')\n",
      "Iteration 6710 Training loss 0.11206644028425217 Validation loss 0.11239241808652878 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5374],\n",
      "        [0.5713]], device='mps:0')\n",
      "Iteration 6720 Training loss 0.11450506001710892 Validation loss 0.1123894676566124 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.5959],\n",
      "        [0.5543]], device='mps:0')\n",
      "Iteration 6730 Training loss 0.11404889076948166 Validation loss 0.112383633852005 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.5316],\n",
      "        [0.2223]], device='mps:0')\n",
      "Iteration 6740 Training loss 0.11438242346048355 Validation loss 0.11237864196300507 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.4525],\n",
      "        [0.4070]], device='mps:0')\n",
      "Iteration 6750 Training loss 0.11347051709890366 Validation loss 0.11230508983135223 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.6237],\n",
      "        [0.5184]], device='mps:0')\n",
      "Iteration 6760 Training loss 0.11337747424840927 Validation loss 0.11228175461292267 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.5157],\n",
      "        [0.5529]], device='mps:0')\n",
      "Iteration 6770 Training loss 0.11073669046163559 Validation loss 0.1122608408331871 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5225],\n",
      "        [0.4762]], device='mps:0')\n",
      "Iteration 6780 Training loss 0.11568229645490646 Validation loss 0.11223523318767548 Accuracy 0.656000018119812\n",
      "Output tensor([[0.6066],\n",
      "        [0.4434]], device='mps:0')\n",
      "Iteration 6790 Training loss 0.1120147854089737 Validation loss 0.11223900318145752 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.5501],\n",
      "        [0.4264]], device='mps:0')\n",
      "Iteration 6800 Training loss 0.11589810252189636 Validation loss 0.11221931874752045 Accuracy 0.656000018119812\n",
      "Output tensor([[0.5442],\n",
      "        [0.5360]], device='mps:0')\n",
      "Iteration 6810 Training loss 0.1142376959323883 Validation loss 0.11221201717853546 Accuracy 0.656000018119812\n",
      "Output tensor([[0.5799],\n",
      "        [0.5423]], device='mps:0')\n",
      "Iteration 6820 Training loss 0.10885410010814667 Validation loss 0.11218661814928055 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.5203],\n",
      "        [0.3736]], device='mps:0')\n",
      "Iteration 6830 Training loss 0.11312393099069595 Validation loss 0.11218401044607162 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.3331],\n",
      "        [0.4506]], device='mps:0')\n",
      "Iteration 6840 Training loss 0.11385085433721542 Validation loss 0.11218199133872986 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.4292],\n",
      "        [0.5523]], device='mps:0')\n",
      "Iteration 6850 Training loss 0.11065643280744553 Validation loss 0.11217378824949265 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.3835],\n",
      "        [0.5469]], device='mps:0')\n",
      "Iteration 6860 Training loss 0.1120639219880104 Validation loss 0.11216187477111816 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5204],\n",
      "        [0.3639]], device='mps:0')\n",
      "Iteration 6870 Training loss 0.12069661170244217 Validation loss 0.11214009672403336 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5754],\n",
      "        [0.3388]], device='mps:0')\n",
      "Iteration 6880 Training loss 0.11616864055395126 Validation loss 0.11213415861129761 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.4292],\n",
      "        [0.6225]], device='mps:0')\n",
      "Iteration 6890 Training loss 0.11147479712963104 Validation loss 0.11212817579507828 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.2872],\n",
      "        [0.5787]], device='mps:0')\n",
      "Iteration 6900 Training loss 0.11482236534357071 Validation loss 0.11212785542011261 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5508],\n",
      "        [0.4811]], device='mps:0')\n",
      "Iteration 6910 Training loss 0.11013894528150558 Validation loss 0.11210805177688599 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.3250],\n",
      "        [0.5559]], device='mps:0')\n",
      "Iteration 6920 Training loss 0.10939963907003403 Validation loss 0.11210048198699951 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4042],\n",
      "        [0.5635]], device='mps:0')\n",
      "Iteration 6930 Training loss 0.11760809272527695 Validation loss 0.11209432780742645 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4787],\n",
      "        [0.5297]], device='mps:0')\n",
      "Iteration 6940 Training loss 0.12039857357740402 Validation loss 0.11208636313676834 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5833],\n",
      "        [0.4992]], device='mps:0')\n",
      "Iteration 6950 Training loss 0.11103340238332748 Validation loss 0.11207945644855499 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.3154],\n",
      "        [0.3153]], device='mps:0')\n",
      "Iteration 6960 Training loss 0.1078592911362648 Validation loss 0.11210992932319641 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.3281],\n",
      "        [0.5798]], device='mps:0')\n",
      "Iteration 6970 Training loss 0.11313915997743607 Validation loss 0.11210989952087402 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.6520],\n",
      "        [0.5497]], device='mps:0')\n",
      "Iteration 6980 Training loss 0.10999268293380737 Validation loss 0.11207795888185501 Accuracy 0.659500002861023\n",
      "Output tensor([[0.6717],\n",
      "        [0.3689]], device='mps:0')\n",
      "Iteration 6990 Training loss 0.11909592151641846 Validation loss 0.11208612471818924 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.3708],\n",
      "        [0.2044]], device='mps:0')\n",
      "Iteration 7000 Training loss 0.10984326153993607 Validation loss 0.11209796369075775 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.5433],\n",
      "        [0.4280]], device='mps:0')\n",
      "Iteration 7010 Training loss 0.1177414208650589 Validation loss 0.11208020150661469 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.4620],\n",
      "        [0.4259]], device='mps:0')\n",
      "Iteration 7020 Training loss 0.11444453150033951 Validation loss 0.11205579340457916 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.2558],\n",
      "        [0.5860]], device='mps:0')\n",
      "Iteration 7030 Training loss 0.11597617715597153 Validation loss 0.11202457547187805 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5388],\n",
      "        [0.6028]], device='mps:0')\n",
      "Iteration 7040 Training loss 0.11154043674468994 Validation loss 0.11204515397548676 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.4569],\n",
      "        [0.6250]], device='mps:0')\n",
      "Iteration 7050 Training loss 0.1121164932847023 Validation loss 0.11208862066268921 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.4751],\n",
      "        [0.5514]], device='mps:0')\n",
      "Iteration 7060 Training loss 0.11143188178539276 Validation loss 0.11208726465702057 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.5163],\n",
      "        [0.5794]], device='mps:0')\n",
      "Iteration 7070 Training loss 0.11453910917043686 Validation loss 0.11206164956092834 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5876],\n",
      "        [0.6322]], device='mps:0')\n",
      "Iteration 7080 Training loss 0.10440673679113388 Validation loss 0.112045519053936 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.4589],\n",
      "        [0.5040]], device='mps:0')\n",
      "Iteration 7090 Training loss 0.11042812466621399 Validation loss 0.11198015511035919 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.4630],\n",
      "        [0.5599]], device='mps:0')\n",
      "Iteration 7100 Training loss 0.11853542923927307 Validation loss 0.1119685173034668 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.5870],\n",
      "        [0.3634]], device='mps:0')\n",
      "Iteration 7110 Training loss 0.1227508932352066 Validation loss 0.11198361963033676 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.5321],\n",
      "        [0.4175]], device='mps:0')\n",
      "Iteration 7120 Training loss 0.12041967362165451 Validation loss 0.11200444400310516 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.4997],\n",
      "        [0.5927]], device='mps:0')\n",
      "Iteration 7130 Training loss 0.10925464332103729 Validation loss 0.11197195947170258 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.4325],\n",
      "        [0.5914]], device='mps:0')\n",
      "Iteration 7140 Training loss 0.10898078978061676 Validation loss 0.11192680895328522 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5030],\n",
      "        [0.4711]], device='mps:0')\n",
      "Iteration 7150 Training loss 0.11653833091259003 Validation loss 0.11190950870513916 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.4578],\n",
      "        [0.3808]], device='mps:0')\n",
      "Iteration 7160 Training loss 0.11171239614486694 Validation loss 0.11190322041511536 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5473],\n",
      "        [0.5677]], device='mps:0')\n",
      "Iteration 7170 Training loss 0.10748939961194992 Validation loss 0.11189647763967514 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5518],\n",
      "        [0.6119]], device='mps:0')\n",
      "Iteration 7180 Training loss 0.11284974217414856 Validation loss 0.11188741773366928 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5546],\n",
      "        [0.3875]], device='mps:0')\n",
      "Iteration 7190 Training loss 0.11888889968395233 Validation loss 0.11188896745443344 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5817],\n",
      "        [0.4412]], device='mps:0')\n",
      "Iteration 7200 Training loss 0.11464391648769379 Validation loss 0.11188364028930664 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.3415],\n",
      "        [0.4735]], device='mps:0')\n",
      "Iteration 7210 Training loss 0.10321532934904099 Validation loss 0.11186785995960236 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.2870],\n",
      "        [0.5383]], device='mps:0')\n",
      "Iteration 7220 Training loss 0.10703334212303162 Validation loss 0.1118619441986084 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.4038],\n",
      "        [0.5950]], device='mps:0')\n",
      "Iteration 7230 Training loss 0.10843299329280853 Validation loss 0.11185136437416077 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5790],\n",
      "        [0.5605]], device='mps:0')\n",
      "Iteration 7240 Training loss 0.1142285093665123 Validation loss 0.11184021830558777 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5752],\n",
      "        [0.3278]], device='mps:0')\n",
      "Iteration 7250 Training loss 0.11508111655712128 Validation loss 0.11183105409145355 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.4936],\n",
      "        [0.4113]], device='mps:0')\n",
      "Iteration 7260 Training loss 0.11112366616725922 Validation loss 0.11182887852191925 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5688],\n",
      "        [0.4137]], device='mps:0')\n",
      "Iteration 7270 Training loss 0.10504983365535736 Validation loss 0.11182848364114761 Accuracy 0.656000018119812\n",
      "Output tensor([[0.5702],\n",
      "        [0.4567]], device='mps:0')\n",
      "Iteration 7280 Training loss 0.11917741596698761 Validation loss 0.11180797964334488 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5430],\n",
      "        [0.5311]], device='mps:0')\n",
      "Iteration 7290 Training loss 0.1101808026432991 Validation loss 0.1118011549115181 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.6371],\n",
      "        [0.4698]], device='mps:0')\n",
      "Iteration 7300 Training loss 0.11004837602376938 Validation loss 0.11179780960083008 Accuracy 0.655500054359436\n",
      "Output tensor([[0.4333],\n",
      "        [0.4513]], device='mps:0')\n",
      "Iteration 7310 Training loss 0.11093924939632416 Validation loss 0.11184026300907135 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.4707],\n",
      "        [0.4700]], device='mps:0')\n",
      "Iteration 7320 Training loss 0.11330707371234894 Validation loss 0.11182213574647903 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.6055],\n",
      "        [0.4610]], device='mps:0')\n",
      "Iteration 7330 Training loss 0.11492983251810074 Validation loss 0.1118011623620987 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.4653],\n",
      "        [0.2990]], device='mps:0')\n",
      "Iteration 7340 Training loss 0.10783933103084564 Validation loss 0.11178047955036163 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.5885],\n",
      "        [0.3726]], device='mps:0')\n",
      "Iteration 7350 Training loss 0.11299457401037216 Validation loss 0.11175954341888428 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.6037],\n",
      "        [0.3472]], device='mps:0')\n",
      "Iteration 7360 Training loss 0.10960998386144638 Validation loss 0.11179114133119583 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.3040],\n",
      "        [0.4005]], device='mps:0')\n",
      "Iteration 7370 Training loss 0.11441254615783691 Validation loss 0.11176642775535583 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5881],\n",
      "        [0.5411]], device='mps:0')\n",
      "Iteration 7380 Training loss 0.10963651537895203 Validation loss 0.11173854768276215 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.5534],\n",
      "        [0.3485]], device='mps:0')\n",
      "Iteration 7390 Training loss 0.12065155059099197 Validation loss 0.11175772547721863 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5671],\n",
      "        [0.5060]], device='mps:0')\n",
      "Iteration 7400 Training loss 0.11246275901794434 Validation loss 0.11179617792367935 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.3539],\n",
      "        [0.5198]], device='mps:0')\n",
      "Iteration 7410 Training loss 0.11394613236188889 Validation loss 0.11172951757907867 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.6200],\n",
      "        [0.5241]], device='mps:0')\n",
      "Iteration 7420 Training loss 0.11966151744127274 Validation loss 0.11172332614660263 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.5031],\n",
      "        [0.3333]], device='mps:0')\n",
      "Iteration 7430 Training loss 0.10989806801080704 Validation loss 0.11173322051763535 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.3041],\n",
      "        [0.4824]], device='mps:0')\n",
      "Iteration 7440 Training loss 0.11553570628166199 Validation loss 0.11167730391025543 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.2726],\n",
      "        [0.4679]], device='mps:0')\n",
      "Iteration 7450 Training loss 0.12066913396120071 Validation loss 0.11170855909585953 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.6424],\n",
      "        [0.5835]], device='mps:0')\n",
      "Iteration 7460 Training loss 0.11099852621555328 Validation loss 0.1116814985871315 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.2713],\n",
      "        [0.3488]], device='mps:0')\n",
      "Iteration 7470 Training loss 0.1140269860625267 Validation loss 0.11167211830615997 Accuracy 0.659500002861023\n",
      "Output tensor([[0.6110],\n",
      "        [0.4435]], device='mps:0')\n",
      "Iteration 7480 Training loss 0.11102835088968277 Validation loss 0.11166414618492126 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.5203],\n",
      "        [0.6903]], device='mps:0')\n",
      "Iteration 7490 Training loss 0.11521365493535995 Validation loss 0.11163778603076935 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.4384],\n",
      "        [0.3271]], device='mps:0')\n",
      "Iteration 7500 Training loss 0.11271078139543533 Validation loss 0.11162984371185303 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.6175],\n",
      "        [0.1745]], device='mps:0')\n",
      "Iteration 7510 Training loss 0.11461765319108963 Validation loss 0.11162234842777252 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.6278],\n",
      "        [0.5103]], device='mps:0')\n",
      "Iteration 7520 Training loss 0.11918163299560547 Validation loss 0.11161315441131592 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.5728],\n",
      "        [0.4407]], device='mps:0')\n",
      "Iteration 7530 Training loss 0.12290693074464798 Validation loss 0.1116020679473877 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.4579],\n",
      "        [0.6018]], device='mps:0')\n",
      "Iteration 7540 Training loss 0.11214437335729599 Validation loss 0.11159978061914444 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.3703],\n",
      "        [0.3176]], device='mps:0')\n",
      "Iteration 7550 Training loss 0.12412677705287933 Validation loss 0.11159224808216095 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.3657],\n",
      "        [0.5591]], device='mps:0')\n",
      "Iteration 7560 Training loss 0.11061311513185501 Validation loss 0.11158192902803421 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5404],\n",
      "        [0.4881]], device='mps:0')\n",
      "Iteration 7570 Training loss 0.11081300675868988 Validation loss 0.11157397925853729 Accuracy 0.6550000309944153\n"
     ]
    }
   ],
   "source": [
    "binary_model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 1.6, 1e-9, 0, 0, 1e-1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08bfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer = binary_classification_three_layer_NN(3072, 2048, 2048, eps_init = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c674f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 1.6, the number of datas used for the training is 38007905.54899443 and the number of iterations is 95019.\n",
      "Iteration 0 Training loss 0.12499987334012985 Validation loss 0.12499989569187164 Accuracy 0.5012500286102295\n",
      "Iteration 10 Training loss 0.12495912611484528 Validation loss 0.1249452456831932 Accuracy 0.5\n",
      "Iteration 20 Training loss 0.124824658036232 Validation loss 0.12487567961215973 Accuracy 0.5\n",
      "Iteration 30 Training loss 0.1246841624379158 Validation loss 0.12475623935461044 Accuracy 0.5\n",
      "Iteration 40 Training loss 0.12463304400444031 Validation loss 0.12460537999868393 Accuracy 0.5\n",
      "Iteration 50 Training loss 0.1240801215171814 Validation loss 0.12428715825080872 Accuracy 0.5\n",
      "Iteration 60 Training loss 0.12385658174753189 Validation loss 0.1237538680434227 Accuracy 0.5\n",
      "Iteration 70 Training loss 0.12375392764806747 Validation loss 0.12308808416128159 Accuracy 0.5\n",
      "Iteration 80 Training loss 0.12164410948753357 Validation loss 0.12210513651371002 Accuracy 0.5033750534057617\n",
      "Iteration 90 Training loss 0.12039179354906082 Validation loss 0.12067040055990219 Accuracy 0.5215000510215759\n",
      "Iteration 100 Training loss 0.12058631330728531 Validation loss 0.11901064217090607 Accuracy 0.6151250004768372\n",
      "Iteration 110 Training loss 0.11775729805231094 Validation loss 0.11658082902431488 Accuracy 0.6151250004768372\n",
      "Iteration 120 Training loss 0.11382438242435455 Validation loss 0.1136026680469513 Accuracy 0.6427500247955322\n",
      "Iteration 130 Training loss 0.1139482855796814 Validation loss 0.1100473627448082 Accuracy 0.6617500185966492\n",
      "Iteration 140 Training loss 0.10652171820402145 Validation loss 0.10595228523015976 Accuracy 0.7437500357627869\n",
      "Iteration 150 Training loss 0.10127490758895874 Validation loss 0.10114031285047531 Accuracy 0.7455000281333923\n",
      "Iteration 160 Training loss 0.09542390704154968 Validation loss 0.09711791574954987 Accuracy 0.7508750557899475\n",
      "Iteration 170 Training loss 0.0916648730635643 Validation loss 0.0936761200428009 Accuracy 0.7507500648498535\n",
      "Iteration 180 Training loss 0.09658931195735931 Validation loss 0.09106772392988205 Accuracy 0.752875030040741\n",
      "Iteration 190 Training loss 0.09682386368513107 Validation loss 0.08850450068712234 Accuracy 0.7618750333786011\n",
      "Iteration 200 Training loss 0.08168473839759827 Validation loss 0.08691210299730301 Accuracy 0.7621250152587891\n",
      "Iteration 210 Training loss 0.08336688578128815 Validation loss 0.08512650430202484 Accuracy 0.7640000581741333\n",
      "Iteration 220 Training loss 0.09333572536706924 Validation loss 0.08405623584985733 Accuracy 0.7643750309944153\n",
      "Iteration 230 Training loss 0.07767755538225174 Validation loss 0.08364494889974594 Accuracy 0.7640000581741333\n",
      "Iteration 240 Training loss 0.08370590955018997 Validation loss 0.0840478241443634 Accuracy 0.7578750252723694\n",
      "Iteration 250 Training loss 0.07358691096305847 Validation loss 0.0807872787117958 Accuracy 0.7758750319480896\n",
      "Iteration 260 Training loss 0.0802534744143486 Validation loss 0.08006877452135086 Accuracy 0.7771250605583191\n",
      "Iteration 270 Training loss 0.07300497591495514 Validation loss 0.08028343319892883 Accuracy 0.7718750238418579\n",
      "Iteration 280 Training loss 0.08498015999794006 Validation loss 0.07911031693220139 Accuracy 0.7785000205039978\n",
      "Iteration 290 Training loss 0.07712056487798691 Validation loss 0.07897266000509262 Accuracy 0.7788750529289246\n",
      "Iteration 300 Training loss 0.07880005240440369 Validation loss 0.07777340710163116 Accuracy 0.7846250534057617\n",
      "Iteration 310 Training loss 0.07279841601848602 Validation loss 0.07736963778734207 Accuracy 0.7858750224113464\n",
      "Iteration 320 Training loss 0.07539085298776627 Validation loss 0.07703111320734024 Accuracy 0.7866250276565552\n",
      "Iteration 330 Training loss 0.07558947056531906 Validation loss 0.07798916846513748 Accuracy 0.7813750505447388\n",
      "Iteration 340 Training loss 0.08237570524215698 Validation loss 0.07657144218683243 Accuracy 0.7867500185966492\n",
      "Iteration 350 Training loss 0.07675536721944809 Validation loss 0.07631480693817139 Accuracy 0.7881250381469727\n",
      "Iteration 360 Training loss 0.07842345535755157 Validation loss 0.07656953483819962 Accuracy 0.7857500314712524\n",
      "Iteration 370 Training loss 0.07669422030448914 Validation loss 0.07574319839477539 Accuracy 0.7887500524520874\n",
      "Iteration 380 Training loss 0.06487959623336792 Validation loss 0.0758211612701416 Accuracy 0.7892500162124634\n",
      "Iteration 390 Training loss 0.0711124986410141 Validation loss 0.07538703829050064 Accuracy 0.7913750410079956\n",
      "Iteration 400 Training loss 0.08093754947185516 Validation loss 0.0778522714972496 Accuracy 0.7800000309944153\n",
      "Iteration 410 Training loss 0.0776059478521347 Validation loss 0.07484093308448792 Accuracy 0.7930000424385071\n",
      "Iteration 420 Training loss 0.079445481300354 Validation loss 0.07512678951025009 Accuracy 0.7901250123977661\n",
      "Iteration 430 Training loss 0.07816886901855469 Validation loss 0.07444366812705994 Accuracy 0.7945000529289246\n",
      "Iteration 440 Training loss 0.08099681884050369 Validation loss 0.07466533780097961 Accuracy 0.7926250100135803\n",
      "Iteration 450 Training loss 0.0795138031244278 Validation loss 0.07416617125272751 Accuracy 0.7937500476837158\n",
      "Iteration 460 Training loss 0.07616807520389557 Validation loss 0.08033204078674316 Accuracy 0.7730000615119934\n",
      "Iteration 470 Training loss 0.07491051405668259 Validation loss 0.07385498285293579 Accuracy 0.7943750619888306\n",
      "Iteration 480 Training loss 0.07527320832014084 Validation loss 0.07433745265007019 Accuracy 0.7916250228881836\n",
      "Iteration 490 Training loss 0.08281440287828445 Validation loss 0.07463207095861435 Accuracy 0.7921250462532043\n",
      "Iteration 500 Training loss 0.08256098628044128 Validation loss 0.0757608711719513 Accuracy 0.7873750329017639\n",
      "Iteration 510 Training loss 0.06706518679857254 Validation loss 0.07347755879163742 Accuracy 0.7982500195503235\n",
      "Iteration 520 Training loss 0.06438831984996796 Validation loss 0.07350340485572815 Accuracy 0.7976250648498535\n",
      "Iteration 530 Training loss 0.0790524110198021 Validation loss 0.07591918110847473 Accuracy 0.7863750457763672\n",
      "Iteration 540 Training loss 0.06785134226083755 Validation loss 0.0737728476524353 Accuracy 0.7955000400543213\n",
      "Iteration 550 Training loss 0.07426617294549942 Validation loss 0.07349162548780441 Accuracy 0.7951250672340393\n",
      "Iteration 560 Training loss 0.06947101652622223 Validation loss 0.07313346862792969 Accuracy 0.7956250309944153\n",
      "Iteration 570 Training loss 0.06735289841890335 Validation loss 0.07393144816160202 Accuracy 0.7921250462532043\n",
      "Iteration 580 Training loss 0.07322733104228973 Validation loss 0.0739501565694809 Accuracy 0.7926250100135803\n",
      "Iteration 590 Training loss 0.06704182177782059 Validation loss 0.07328985631465912 Accuracy 0.7956250309944153\n",
      "Iteration 600 Training loss 0.059996575117111206 Validation loss 0.0746784508228302 Accuracy 0.7912500500679016\n",
      "Iteration 610 Training loss 0.0779077485203743 Validation loss 0.07268011569976807 Accuracy 0.799375057220459\n",
      "Iteration 620 Training loss 0.07300085574388504 Validation loss 0.07240135967731476 Accuracy 0.800000011920929\n",
      "Iteration 630 Training loss 0.06591321527957916 Validation loss 0.07256626337766647 Accuracy 0.7980000376701355\n",
      "Iteration 640 Training loss 0.06849443912506104 Validation loss 0.07244501262903214 Accuracy 0.7987500429153442\n",
      "Iteration 650 Training loss 0.07152976095676422 Validation loss 0.07193226367235184 Accuracy 0.8018750548362732\n",
      "Iteration 660 Training loss 0.08671338856220245 Validation loss 0.07722822576761246 Accuracy 0.7796250581741333\n",
      "Iteration 670 Training loss 0.06945855915546417 Validation loss 0.07196548581123352 Accuracy 0.8011250495910645\n",
      "Iteration 680 Training loss 0.07540249824523926 Validation loss 0.07246822118759155 Accuracy 0.7978750467300415\n",
      "Iteration 690 Training loss 0.06203562766313553 Validation loss 0.07159389555454254 Accuracy 0.8028750419616699\n",
      "Iteration 700 Training loss 0.07683751732110977 Validation loss 0.07410886138677597 Accuracy 0.7925000190734863\n",
      "Iteration 710 Training loss 0.07060876488685608 Validation loss 0.07158879190683365 Accuracy 0.8001250624656677\n",
      "Iteration 720 Training loss 0.06814760714769363 Validation loss 0.07160598784685135 Accuracy 0.8011250495910645\n",
      "Iteration 730 Training loss 0.07242885231971741 Validation loss 0.07160396128892899 Accuracy 0.8021250367164612\n",
      "Iteration 740 Training loss 0.07154219597578049 Validation loss 0.07099658995866776 Accuracy 0.8058750629425049\n",
      "Iteration 750 Training loss 0.0745331421494484 Validation loss 0.07357381284236908 Accuracy 0.7942500114440918\n",
      "Iteration 760 Training loss 0.06977970153093338 Validation loss 0.07864343374967575 Accuracy 0.7801250219345093\n",
      "Iteration 770 Training loss 0.06322120875120163 Validation loss 0.07099059969186783 Accuracy 0.8045000433921814\n",
      "Iteration 780 Training loss 0.07118348777294159 Validation loss 0.07089588046073914 Accuracy 0.8045000433921814\n",
      "Iteration 790 Training loss 0.07197071611881256 Validation loss 0.0709991455078125 Accuracy 0.8043750524520874\n",
      "Iteration 800 Training loss 0.07117536664009094 Validation loss 0.07113737612962723 Accuracy 0.8022500276565552\n",
      "Iteration 810 Training loss 0.08064354956150055 Validation loss 0.0705365315079689 Accuracy 0.8045000433921814\n",
      "Iteration 820 Training loss 0.07386286556720734 Validation loss 0.0703127533197403 Accuracy 0.8067500591278076\n",
      "Iteration 830 Training loss 0.07025466114282608 Validation loss 0.07093480974435806 Accuracy 0.8023750185966492\n",
      "Iteration 840 Training loss 0.0862659439444542 Validation loss 0.08636005222797394 Accuracy 0.7518750429153442\n",
      "Iteration 850 Training loss 0.06446003168821335 Validation loss 0.07003108412027359 Accuracy 0.8076250553131104\n",
      "Iteration 860 Training loss 0.07362934201955795 Validation loss 0.06995640695095062 Accuracy 0.8068750500679016\n",
      "Iteration 870 Training loss 0.06667828559875488 Validation loss 0.07218021154403687 Accuracy 0.796125054359436\n",
      "Iteration 880 Training loss 0.07844360917806625 Validation loss 0.07794856280088425 Accuracy 0.7768750190734863\n",
      "Iteration 890 Training loss 0.06573113799095154 Validation loss 0.06990201026201248 Accuracy 0.8060000538825989\n",
      "Iteration 900 Training loss 0.0643780454993248 Validation loss 0.07153066992759705 Accuracy 0.7990000247955322\n",
      "Iteration 910 Training loss 0.05879702419042587 Validation loss 0.06956207007169724 Accuracy 0.8086250424385071\n",
      "Iteration 920 Training loss 0.07547055184841156 Validation loss 0.07216185331344604 Accuracy 0.7970000505447388\n",
      "Iteration 930 Training loss 0.06836238503456116 Validation loss 0.0709589272737503 Accuracy 0.799375057220459\n",
      "Iteration 940 Training loss 0.06496651470661163 Validation loss 0.06927205622196198 Accuracy 0.8096250295639038\n",
      "Iteration 950 Training loss 0.061265796422958374 Validation loss 0.07015819102525711 Accuracy 0.8040000200271606\n",
      "Iteration 960 Training loss 0.06381699442863464 Validation loss 0.06917165219783783 Accuracy 0.8098750114440918\n",
      "Iteration 970 Training loss 0.08180543780326843 Validation loss 0.06933305412530899 Accuracy 0.8072500228881836\n",
      "Iteration 980 Training loss 0.07020856440067291 Validation loss 0.07478776574134827 Accuracy 0.7896250486373901\n",
      "Iteration 990 Training loss 0.07052188366651535 Validation loss 0.07339352369308472 Accuracy 0.7938750386238098\n",
      "Iteration 1000 Training loss 0.06352481991052628 Validation loss 0.06903059035539627 Accuracy 0.8085000514984131\n",
      "Iteration 1010 Training loss 0.07591020315885544 Validation loss 0.07513698190450668 Accuracy 0.7910000085830688\n",
      "Iteration 1020 Training loss 0.06447605043649673 Validation loss 0.06986137479543686 Accuracy 0.8045000433921814\n",
      "Iteration 1030 Training loss 0.07341030240058899 Validation loss 0.07044550031423569 Accuracy 0.8027500510215759\n",
      "Iteration 1040 Training loss 0.07044967263936996 Validation loss 0.07444564998149872 Accuracy 0.7933750152587891\n",
      "Iteration 1050 Training loss 0.06716763973236084 Validation loss 0.06904763728380203 Accuracy 0.8085000514984131\n",
      "Iteration 1060 Training loss 0.07604794949293137 Validation loss 0.0723065510392189 Accuracy 0.7952500581741333\n",
      "Iteration 1070 Training loss 0.07457689940929413 Validation loss 0.07680867612361908 Accuracy 0.7813750505447388\n",
      "Iteration 1080 Training loss 0.0718487873673439 Validation loss 0.06939471513032913 Accuracy 0.8086250424385071\n",
      "Iteration 1090 Training loss 0.07376608997583389 Validation loss 0.06812667846679688 Accuracy 0.8107500672340393\n",
      "Iteration 1100 Training loss 0.07183618098497391 Validation loss 0.06990525126457214 Accuracy 0.8041250109672546\n",
      "Iteration 1110 Training loss 0.0914720967411995 Validation loss 0.07714927941560745 Accuracy 0.7800000309944153\n",
      "Iteration 1120 Training loss 0.0672791376709938 Validation loss 0.067942775785923 Accuracy 0.8115000128746033\n",
      "Iteration 1130 Training loss 0.07372982800006866 Validation loss 0.06782837212085724 Accuracy 0.8112500309944153\n",
      "Iteration 1140 Training loss 0.065157949924469 Validation loss 0.07151543349027634 Accuracy 0.8001250624656677\n",
      "Iteration 1150 Training loss 0.0646921694278717 Validation loss 0.06948988139629364 Accuracy 0.8082500100135803\n",
      "Iteration 1160 Training loss 0.07804657518863678 Validation loss 0.0748884454369545 Accuracy 0.7916250228881836\n",
      "Iteration 1170 Training loss 0.06826049089431763 Validation loss 0.06764258444309235 Accuracy 0.8115000128746033\n",
      "Iteration 1180 Training loss 0.06982377171516418 Validation loss 0.06783101707696915 Accuracy 0.8111250400543213\n",
      "Iteration 1190 Training loss 0.06414156407117844 Validation loss 0.06896989047527313 Accuracy 0.8067500591278076\n",
      "Iteration 1200 Training loss 0.07970193028450012 Validation loss 0.06752892583608627 Accuracy 0.812125027179718\n",
      "Iteration 1210 Training loss 0.06597460806369781 Validation loss 0.06933019310235977 Accuracy 0.8050000667572021\n",
      "Iteration 1220 Training loss 0.06735436618328094 Validation loss 0.0677420124411583 Accuracy 0.8143750429153442\n",
      "Iteration 1230 Training loss 0.07436370849609375 Validation loss 0.07180063426494598 Accuracy 0.8018750548362732\n",
      "Iteration 1240 Training loss 0.06393343955278397 Validation loss 0.06870817393064499 Accuracy 0.8087500333786011\n",
      "Iteration 1250 Training loss 0.07620219141244888 Validation loss 0.06722739338874817 Accuracy 0.8135000467300415\n",
      "Iteration 1260 Training loss 0.0634637176990509 Validation loss 0.06701605767011642 Accuracy 0.8141250610351562\n",
      "Iteration 1270 Training loss 0.06512615084648132 Validation loss 0.0668940618634224 Accuracy 0.815125048160553\n",
      "Iteration 1280 Training loss 0.07360737770795822 Validation loss 0.07485724240541458 Accuracy 0.7906250357627869\n",
      "Iteration 1290 Training loss 0.06529559195041656 Validation loss 0.07435692846775055 Accuracy 0.7921250462532043\n",
      "Iteration 1300 Training loss 0.07371301203966141 Validation loss 0.06869304180145264 Accuracy 0.8097500205039978\n",
      "Iteration 1310 Training loss 0.06944902241230011 Validation loss 0.06885980814695358 Accuracy 0.8063750267028809\n",
      "Iteration 1320 Training loss 0.06635818630456924 Validation loss 0.06880947202444077 Accuracy 0.8073750138282776\n",
      "Iteration 1330 Training loss 0.07669036090373993 Validation loss 0.07046495378017426 Accuracy 0.8037500381469727\n",
      "Iteration 1340 Training loss 0.06911492347717285 Validation loss 0.07286925613880157 Accuracy 0.7955000400543213\n",
      "Iteration 1350 Training loss 0.06356468051671982 Validation loss 0.06663831323385239 Accuracy 0.8165000677108765\n",
      "Iteration 1360 Training loss 0.06723745167255402 Validation loss 0.06665689498186111 Accuracy 0.8170000314712524\n",
      "Iteration 1370 Training loss 0.07426811009645462 Validation loss 0.07014106214046478 Accuracy 0.8021250367164612\n",
      "Iteration 1380 Training loss 0.06682603806257248 Validation loss 0.07053319364786148 Accuracy 0.8001250624656677\n",
      "Iteration 1390 Training loss 0.0598786324262619 Validation loss 0.06715239584445953 Accuracy 0.815500020980835\n",
      "Iteration 1400 Training loss 0.07898513972759247 Validation loss 0.07404544949531555 Accuracy 0.7925000190734863\n",
      "Iteration 1410 Training loss 0.0909278392791748 Validation loss 0.08154816180467606 Accuracy 0.7701250314712524\n",
      "Iteration 1420 Training loss 0.0772184431552887 Validation loss 0.0686945840716362 Accuracy 0.8105000257492065\n",
      "Iteration 1430 Training loss 0.06874095648527145 Validation loss 0.07637477666139603 Accuracy 0.7827500104904175\n",
      "Iteration 1440 Training loss 0.06701164692640305 Validation loss 0.06660783290863037 Accuracy 0.8147500157356262\n",
      "Iteration 1450 Training loss 0.056594762951135635 Validation loss 0.0659206211566925 Accuracy 0.8172500133514404\n",
      "Iteration 1460 Training loss 0.07622510939836502 Validation loss 0.06769156455993652 Accuracy 0.8131250143051147\n",
      "Iteration 1470 Training loss 0.06287923455238342 Validation loss 0.0674864873290062 Accuracy 0.8130000233650208\n",
      "Iteration 1480 Training loss 0.07486677169799805 Validation loss 0.06611290574073792 Accuracy 0.8160000443458557\n",
      "Iteration 1490 Training loss 0.07368721067905426 Validation loss 0.07025297731161118 Accuracy 0.8011250495910645\n",
      "Iteration 1500 Training loss 0.059620704501867294 Validation loss 0.07306325435638428 Accuracy 0.7943750619888306\n",
      "Iteration 1510 Training loss 0.0788663923740387 Validation loss 0.07059209793806076 Accuracy 0.8022500276565552\n",
      "Iteration 1520 Training loss 0.06286048144102097 Validation loss 0.06761287152767181 Accuracy 0.8112500309944153\n",
      "Iteration 1530 Training loss 0.06450245529413223 Validation loss 0.06566070765256882 Accuracy 0.8185000419616699\n",
      "Iteration 1540 Training loss 0.06240047141909599 Validation loss 0.06596684455871582 Accuracy 0.8167500495910645\n",
      "Iteration 1550 Training loss 0.05952721834182739 Validation loss 0.0659668892621994 Accuracy 0.8163750171661377\n",
      "Iteration 1560 Training loss 0.07463117688894272 Validation loss 0.06736710667610168 Accuracy 0.8110000491142273\n",
      "Iteration 1570 Training loss 0.07534368336200714 Validation loss 0.06530702859163284 Accuracy 0.8202500343322754\n",
      "Iteration 1580 Training loss 0.06674909591674805 Validation loss 0.06566920876502991 Accuracy 0.8178750276565552\n",
      "Iteration 1590 Training loss 0.07101432979106903 Validation loss 0.06674741953611374 Accuracy 0.8166250586509705\n",
      "Iteration 1600 Training loss 0.06573981791734695 Validation loss 0.06827995926141739 Accuracy 0.8111250400543213\n",
      "Iteration 1610 Training loss 0.06509436666965485 Validation loss 0.06933699548244476 Accuracy 0.8080000281333923\n",
      "Iteration 1620 Training loss 0.0589890331029892 Validation loss 0.06645381450653076 Accuracy 0.815250039100647\n",
      "Iteration 1630 Training loss 0.07082398980855942 Validation loss 0.06493404507637024 Accuracy 0.8208750486373901\n",
      "Iteration 1640 Training loss 0.062138430774211884 Validation loss 0.06500613689422607 Accuracy 0.8198750615119934\n",
      "Iteration 1650 Training loss 0.07550165057182312 Validation loss 0.07904179394245148 Accuracy 0.7757500410079956\n",
      "Iteration 1660 Training loss 0.06873483210802078 Validation loss 0.06669259816408157 Accuracy 0.8143750429153442\n",
      "Iteration 1670 Training loss 0.06267649680376053 Validation loss 0.06695977598428726 Accuracy 0.8126250505447388\n",
      "Iteration 1680 Training loss 0.06198449432849884 Validation loss 0.06466306000947952 Accuracy 0.8231250643730164\n",
      "Iteration 1690 Training loss 0.06644698232412338 Validation loss 0.06465067714452744 Accuracy 0.8220000267028809\n",
      "Iteration 1700 Training loss 0.06785620748996735 Validation loss 0.06939836591482162 Accuracy 0.8052500486373901\n",
      "Iteration 1710 Training loss 0.06552227586507797 Validation loss 0.06507723033428192 Accuracy 0.8218750357627869\n",
      "Iteration 1720 Training loss 0.06966891884803772 Validation loss 0.06476926058530807 Accuracy 0.8223750591278076\n",
      "Iteration 1730 Training loss 0.06237442418932915 Validation loss 0.06482949107885361 Accuracy 0.8225000500679016\n",
      "Iteration 1740 Training loss 0.06429100036621094 Validation loss 0.06549917161464691 Accuracy 0.8191250562667847\n",
      "Iteration 1750 Training loss 0.07706304639577866 Validation loss 0.07142036408185959 Accuracy 0.799750030040741\n",
      "Iteration 1760 Training loss 0.07538428157567978 Validation loss 0.06631682068109512 Accuracy 0.8141250610351562\n",
      "Iteration 1770 Training loss 0.06065042316913605 Validation loss 0.0642399936914444 Accuracy 0.8231250643730164\n",
      "Iteration 1780 Training loss 0.06774985045194626 Validation loss 0.06542797386646271 Accuracy 0.8183750510215759\n",
      "Iteration 1790 Training loss 0.058104123920202255 Validation loss 0.06401968747377396 Accuracy 0.8236250281333923\n",
      "Iteration 1800 Training loss 0.0700356662273407 Validation loss 0.06977727264165878 Accuracy 0.8045000433921814\n",
      "Iteration 1810 Training loss 0.05354801565408707 Validation loss 0.06414177268743515 Accuracy 0.8223750591278076\n",
      "Iteration 1820 Training loss 0.06736184656620026 Validation loss 0.06425709277391434 Accuracy 0.8251250386238098\n",
      "Iteration 1830 Training loss 0.06320326775312424 Validation loss 0.06858967989683151 Accuracy 0.8088750243186951\n",
      "Iteration 1840 Training loss 0.06934420019388199 Validation loss 0.06701910495758057 Accuracy 0.8137500286102295\n",
      "Iteration 1850 Training loss 0.06682199984788895 Validation loss 0.06365921348333359 Accuracy 0.8253750205039978\n",
      "Iteration 1860 Training loss 0.07076054066419601 Validation loss 0.06400943547487259 Accuracy 0.8211250305175781\n",
      "Iteration 1870 Training loss 0.06400244683027267 Validation loss 0.06384781748056412 Accuracy 0.8223750591278076\n",
      "Iteration 1880 Training loss 0.07631462812423706 Validation loss 0.07856646925210953 Accuracy 0.7775000333786011\n",
      "Iteration 1890 Training loss 0.06595955044031143 Validation loss 0.06509634107351303 Accuracy 0.8201250433921814\n",
      "Iteration 1900 Training loss 0.07490857690572739 Validation loss 0.07208342105150223 Accuracy 0.7968750596046448\n",
      "Iteration 1910 Training loss 0.06558404117822647 Validation loss 0.06340460479259491 Accuracy 0.8266250491142273\n",
      "Iteration 1920 Training loss 0.0713127851486206 Validation loss 0.06350897252559662 Accuracy 0.8282500505447388\n",
      "Iteration 1930 Training loss 0.0646144449710846 Validation loss 0.06722018122673035 Accuracy 0.8133750557899475\n",
      "Iteration 1940 Training loss 0.06287179887294769 Validation loss 0.06506258249282837 Accuracy 0.8222500681877136\n",
      "Iteration 1950 Training loss 0.07614526897668839 Validation loss 0.06659800559282303 Accuracy 0.8167500495910645\n",
      "Iteration 1960 Training loss 0.06050112470984459 Validation loss 0.06843063980340958 Accuracy 0.8091250658035278\n",
      "Iteration 1970 Training loss 0.0584167055785656 Validation loss 0.0638444721698761 Accuracy 0.8271250128746033\n",
      "Iteration 1980 Training loss 0.05825962498784065 Validation loss 0.0635729730129242 Accuracy 0.8235000371932983\n",
      "Iteration 1990 Training loss 0.06593775004148483 Validation loss 0.07243423908948898 Accuracy 0.796750009059906\n",
      "Iteration 2000 Training loss 0.0711500495672226 Validation loss 0.06576259434223175 Accuracy 0.8170000314712524\n",
      "Iteration 2010 Training loss 0.06290079653263092 Validation loss 0.06494171172380447 Accuracy 0.8215000629425049\n",
      "Iteration 2020 Training loss 0.05843367800116539 Validation loss 0.06300333142280579 Accuracy 0.8265000581741333\n",
      "Iteration 2030 Training loss 0.062765933573246 Validation loss 0.06593582034111023 Accuracy 0.8185000419616699\n",
      "Iteration 2040 Training loss 0.0639781653881073 Validation loss 0.06324493139982224 Accuracy 0.8245000243186951\n",
      "Iteration 2050 Training loss 0.060702621936798096 Validation loss 0.0640011802315712 Accuracy 0.8240000605583191\n",
      "Iteration 2060 Training loss 0.0753621757030487 Validation loss 0.06718200445175171 Accuracy 0.8136250376701355\n",
      "Iteration 2070 Training loss 0.0614076592028141 Validation loss 0.06643273681402206 Accuracy 0.8141250610351562\n",
      "Iteration 2080 Training loss 0.06202766299247742 Validation loss 0.06382874399423599 Accuracy 0.8230000138282776\n",
      "Iteration 2090 Training loss 0.06428476423025131 Validation loss 0.0632672905921936 Accuracy 0.8241250514984131\n",
      "Iteration 2100 Training loss 0.0687430277466774 Validation loss 0.06832464784383774 Accuracy 0.8090000152587891\n",
      "Iteration 2110 Training loss 0.06135539710521698 Validation loss 0.0693250298500061 Accuracy 0.8072500228881836\n",
      "Iteration 2120 Training loss 0.06769797950983047 Validation loss 0.0667296051979065 Accuracy 0.815500020980835\n",
      "Iteration 2130 Training loss 0.06802022457122803 Validation loss 0.06345971673727036 Accuracy 0.8251250386238098\n",
      "Iteration 2140 Training loss 0.07466714084148407 Validation loss 0.06661726534366608 Accuracy 0.8133750557899475\n",
      "Iteration 2150 Training loss 0.060140155255794525 Validation loss 0.06307000666856766 Accuracy 0.8262500166893005\n",
      "Iteration 2160 Training loss 0.06928752362728119 Validation loss 0.06564278900623322 Accuracy 0.8202500343322754\n",
      "Iteration 2170 Training loss 0.07072132080793381 Validation loss 0.06759165227413177 Accuracy 0.811625063419342\n",
      "Iteration 2180 Training loss 0.06074139475822449 Validation loss 0.06377176940441132 Accuracy 0.8262500166893005\n",
      "Iteration 2190 Training loss 0.07033015042543411 Validation loss 0.06347362697124481 Accuracy 0.8231250643730164\n",
      "Iteration 2200 Training loss 0.06551970541477203 Validation loss 0.06497539579868317 Accuracy 0.8206250667572021\n",
      "Iteration 2210 Training loss 0.05974781513214111 Validation loss 0.06775987893342972 Accuracy 0.8097500205039978\n",
      "Iteration 2220 Training loss 0.061636436730623245 Validation loss 0.06366124749183655 Accuracy 0.8250000476837158\n",
      "Iteration 2230 Training loss 0.0602228119969368 Validation loss 0.06456299871206284 Accuracy 0.8223750591278076\n",
      "Iteration 2240 Training loss 0.06703365594148636 Validation loss 0.06202461197972298 Accuracy 0.8315000534057617\n",
      "Iteration 2250 Training loss 0.06199679896235466 Validation loss 0.06232106313109398 Accuracy 0.8291250467300415\n",
      "Iteration 2260 Training loss 0.06934277713298798 Validation loss 0.062023330479860306 Accuracy 0.8315000534057617\n",
      "Iteration 2270 Training loss 0.07002954930067062 Validation loss 0.06595832109451294 Accuracy 0.815375030040741\n",
      "Iteration 2280 Training loss 0.07192698866128922 Validation loss 0.06480865925550461 Accuracy 0.8220000267028809\n",
      "Iteration 2290 Training loss 0.061407383531332016 Validation loss 0.06243445724248886 Accuracy 0.827875018119812\n",
      "Iteration 2300 Training loss 0.06926432251930237 Validation loss 0.06484810262918472 Accuracy 0.8228750228881836\n",
      "Iteration 2310 Training loss 0.05724705383181572 Validation loss 0.06262606382369995 Accuracy 0.830500066280365\n",
      "Iteration 2320 Training loss 0.06396356970071793 Validation loss 0.06561438739299774 Accuracy 0.8167500495910645\n",
      "Iteration 2330 Training loss 0.0583760067820549 Validation loss 0.0702076330780983 Accuracy 0.8036250472068787\n",
      "Iteration 2340 Training loss 0.0785110741853714 Validation loss 0.07260981947183609 Accuracy 0.796625018119812\n",
      "Iteration 2350 Training loss 0.07049495726823807 Validation loss 0.06934679299592972 Accuracy 0.8050000667572021\n",
      "Iteration 2360 Training loss 0.07570799440145493 Validation loss 0.06823998689651489 Accuracy 0.8080000281333923\n",
      "Iteration 2370 Training loss 0.060089435428380966 Validation loss 0.06203070282936096 Accuracy 0.8298750519752502\n",
      "Iteration 2380 Training loss 0.05493251606822014 Validation loss 0.06921824812889099 Accuracy 0.8072500228881836\n",
      "Iteration 2390 Training loss 0.06545762717723846 Validation loss 0.06975492835044861 Accuracy 0.8057500123977661\n",
      "Iteration 2400 Training loss 0.05730689689517021 Validation loss 0.06290960311889648 Accuracy 0.8251250386238098\n",
      "Iteration 2410 Training loss 0.07375623285770416 Validation loss 0.06847302615642548 Accuracy 0.8072500228881836\n",
      "Iteration 2420 Training loss 0.06613217294216156 Validation loss 0.06275863200426102 Accuracy 0.8287500143051147\n",
      "Iteration 2430 Training loss 0.0731348842382431 Validation loss 0.07143121212720871 Accuracy 0.8010000586509705\n",
      "Iteration 2440 Training loss 0.0601123683154583 Validation loss 0.06144803762435913 Accuracy 0.8322500586509705\n",
      "Iteration 2450 Training loss 0.05913205072283745 Validation loss 0.06202857196331024 Accuracy 0.8303750157356262\n",
      "Iteration 2460 Training loss 0.07610586285591125 Validation loss 0.0647369846701622 Accuracy 0.8175000548362732\n",
      "Iteration 2470 Training loss 0.06241458281874657 Validation loss 0.06728755682706833 Accuracy 0.8095000386238098\n",
      "Iteration 2480 Training loss 0.06963609158992767 Validation loss 0.06300390511751175 Accuracy 0.8256250619888306\n",
      "Iteration 2490 Training loss 0.06436862796545029 Validation loss 0.0631035715341568 Accuracy 0.8252500295639038\n",
      "Iteration 2500 Training loss 0.06089593842625618 Validation loss 0.06139690801501274 Accuracy 0.8317500352859497\n",
      "Iteration 2510 Training loss 0.056630782783031464 Validation loss 0.061280153691768646 Accuracy 0.8331250548362732\n",
      "Iteration 2520 Training loss 0.0607764907181263 Validation loss 0.061260323971509933 Accuracy 0.8335000276565552\n",
      "Iteration 2530 Training loss 0.06225745007395744 Validation loss 0.06258704513311386 Accuracy 0.8281250596046448\n",
      "Iteration 2540 Training loss 0.05577728524804115 Validation loss 0.061960622668266296 Accuracy 0.8321250677108765\n",
      "Iteration 2550 Training loss 0.06575292348861694 Validation loss 0.06481273472309113 Accuracy 0.8191250562667847\n",
      "Iteration 2560 Training loss 0.05728081241250038 Validation loss 0.06445922702550888 Accuracy 0.8195000290870667\n",
      "Iteration 2570 Training loss 0.06312067061662674 Validation loss 0.06253471225500107 Accuracy 0.8290000557899475\n",
      "Iteration 2580 Training loss 0.0656549260020256 Validation loss 0.06258450448513031 Accuracy 0.8268750309944153\n",
      "Iteration 2590 Training loss 0.05967217683792114 Validation loss 0.060840457677841187 Accuracy 0.8335000276565552\n",
      "Iteration 2600 Training loss 0.06498770415782928 Validation loss 0.06502940505743027 Accuracy 0.8201250433921814\n",
      "Iteration 2610 Training loss 0.06515214592218399 Validation loss 0.06076829880475998 Accuracy 0.8353750109672546\n",
      "Iteration 2620 Training loss 0.059190090745687485 Validation loss 0.06075049191713333 Accuracy 0.8351250290870667\n",
      "Iteration 2630 Training loss 0.06175265088677406 Validation loss 0.061932094395160675 Accuracy 0.8315000534057617\n",
      "Iteration 2640 Training loss 0.06873487681150436 Validation loss 0.060710567981004715 Accuracy 0.8345000147819519\n",
      "Iteration 2650 Training loss 0.05692979693412781 Validation loss 0.060627344995737076 Accuracy 0.8351250290870667\n",
      "Iteration 2660 Training loss 0.06276532262563705 Validation loss 0.061421480029821396 Accuracy 0.8328750133514404\n",
      "Iteration 2670 Training loss 0.06621216982603073 Validation loss 0.06111739203333855 Accuracy 0.8326250314712524\n",
      "Iteration 2680 Training loss 0.05986030027270317 Validation loss 0.06059093773365021 Accuracy 0.8345000147819519\n",
      "Iteration 2690 Training loss 0.05720129981637001 Validation loss 0.06038444861769676 Accuracy 0.8362500667572021\n",
      "Iteration 2700 Training loss 0.06275907903909683 Validation loss 0.06646684557199478 Accuracy 0.815250039100647\n",
      "Iteration 2710 Training loss 0.06426383554935455 Validation loss 0.06860773265361786 Accuracy 0.8095000386238098\n",
      "Iteration 2720 Training loss 0.05174951255321503 Validation loss 0.06174695864319801 Accuracy 0.8326250314712524\n",
      "Iteration 2730 Training loss 0.0593714602291584 Validation loss 0.06091131269931793 Accuracy 0.8326250314712524\n",
      "Iteration 2740 Training loss 0.06131623312830925 Validation loss 0.06192707270383835 Accuracy 0.8316250443458557\n",
      "Iteration 2750 Training loss 0.07100389152765274 Validation loss 0.07209576666355133 Accuracy 0.7990000247955322\n",
      "Iteration 2760 Training loss 0.05693478509783745 Validation loss 0.06154608353972435 Accuracy 0.830625057220459\n",
      "Iteration 2770 Training loss 0.06679337471723557 Validation loss 0.060514308512210846 Accuracy 0.8367500305175781\n",
      "Iteration 2780 Training loss 0.05625172331929207 Validation loss 0.06061359494924545 Accuracy 0.8361250162124634\n",
      "Iteration 2790 Training loss 0.05340813845396042 Validation loss 0.06009143963456154 Accuracy 0.8380000591278076\n",
      "Iteration 2800 Training loss 0.06459276378154755 Validation loss 0.06004606559872627 Accuracy 0.8383750319480896\n",
      "Iteration 2810 Training loss 0.054282572120428085 Validation loss 0.06099097803235054 Accuracy 0.8326250314712524\n",
      "Iteration 2820 Training loss 0.06195642426609993 Validation loss 0.06019454449415207 Accuracy 0.8366250395774841\n",
      "Iteration 2830 Training loss 0.06800982356071472 Validation loss 0.06984802335500717 Accuracy 0.8050000667572021\n",
      "Iteration 2840 Training loss 0.060443658381700516 Validation loss 0.06043890491127968 Accuracy 0.8352500200271606\n",
      "Iteration 2850 Training loss 0.06575925648212433 Validation loss 0.06116250157356262 Accuracy 0.8325000405311584\n",
      "Iteration 2860 Training loss 0.0732513964176178 Validation loss 0.0676063671708107 Accuracy 0.8131250143051147\n",
      "Iteration 2870 Training loss 0.056067321449518204 Validation loss 0.061173684895038605 Accuracy 0.8325000405311584\n",
      "Iteration 2880 Training loss 0.06472795456647873 Validation loss 0.06955220550298691 Accuracy 0.8045000433921814\n",
      "Iteration 2890 Training loss 0.054664548486471176 Validation loss 0.06066391244530678 Accuracy 0.8360000252723694\n",
      "Iteration 2900 Training loss 0.05671604350209236 Validation loss 0.05991280823945999 Accuracy 0.8376250267028809\n",
      "Iteration 2910 Training loss 0.06907162815332413 Validation loss 0.07034065574407578 Accuracy 0.8060000538825989\n",
      "Iteration 2920 Training loss 0.0647798404097557 Validation loss 0.06252255290746689 Accuracy 0.8268750309944153\n",
      "Iteration 2930 Training loss 0.06338345259428024 Validation loss 0.060634974390268326 Accuracy 0.8358750343322754\n",
      "Iteration 2940 Training loss 0.06464863568544388 Validation loss 0.0602993443608284 Accuracy 0.8340000510215759\n",
      "Iteration 2950 Training loss 0.07182604819536209 Validation loss 0.06354381144046783 Accuracy 0.8270000219345093\n",
      "Iteration 2960 Training loss 0.05396126210689545 Validation loss 0.06408409774303436 Accuracy 0.8247500658035278\n",
      "Iteration 2970 Training loss 0.074587382376194 Validation loss 0.06890548765659332 Accuracy 0.8085000514984131\n",
      "Iteration 2980 Training loss 0.05289455130696297 Validation loss 0.060411129146814346 Accuracy 0.8356250524520874\n",
      "Iteration 2990 Training loss 0.06368399411439896 Validation loss 0.06912816315889359 Accuracy 0.8068750500679016\n",
      "Iteration 3000 Training loss 0.06885543465614319 Validation loss 0.06023607775568962 Accuracy 0.8352500200271606\n",
      "Iteration 3010 Training loss 0.06272030621767044 Validation loss 0.060910116881132126 Accuracy 0.8333750367164612\n",
      "Iteration 3020 Training loss 0.061222758144140244 Validation loss 0.06524331867694855 Accuracy 0.8208750486373901\n",
      "Iteration 3030 Training loss 0.0622810535132885 Validation loss 0.060861337929964066 Accuracy 0.8315000534057617\n",
      "Iteration 3040 Training loss 0.06870093941688538 Validation loss 0.060258571058511734 Accuracy 0.8341250419616699\n",
      "Iteration 3050 Training loss 0.06461601704359055 Validation loss 0.059506095945835114 Accuracy 0.8393750190734863\n",
      "Iteration 3060 Training loss 0.07282897084951401 Validation loss 0.06988061219453812 Accuracy 0.8036250472068787\n",
      "Iteration 3070 Training loss 0.06657695025205612 Validation loss 0.05954213812947273 Accuracy 0.8386250138282776\n",
      "Iteration 3080 Training loss 0.06465272605419159 Validation loss 0.0643780529499054 Accuracy 0.8227500319480896\n",
      "Iteration 3090 Training loss 0.062229499220848083 Validation loss 0.05952401086688042 Accuracy 0.8392500281333923\n",
      "Iteration 3100 Training loss 0.07111863046884537 Validation loss 0.06276553124189377 Accuracy 0.8240000605583191\n",
      "Iteration 3110 Training loss 0.05945262312889099 Validation loss 0.05926533788442612 Accuracy 0.8403750658035278\n",
      "Iteration 3120 Training loss 0.055486857891082764 Validation loss 0.060802631080150604 Accuracy 0.8316250443458557\n",
      "Iteration 3130 Training loss 0.0621100477874279 Validation loss 0.05928551033139229 Accuracy 0.8393750190734863\n",
      "Iteration 3140 Training loss 0.06056888401508331 Validation loss 0.05911838263273239 Accuracy 0.8395000696182251\n",
      "Iteration 3150 Training loss 0.0708363801240921 Validation loss 0.06270074844360352 Accuracy 0.8245000243186951\n",
      "Iteration 3160 Training loss 0.05520884320139885 Validation loss 0.061773378401994705 Accuracy 0.827625036239624\n",
      "Iteration 3170 Training loss 0.06519490480422974 Validation loss 0.05953346937894821 Accuracy 0.8388750553131104\n",
      "Iteration 3180 Training loss 0.06578543782234192 Validation loss 0.06237002834677696 Accuracy 0.8290000557899475\n",
      "Iteration 3190 Training loss 0.06691867858171463 Validation loss 0.0624086856842041 Accuracy 0.8248750567436218\n",
      "Iteration 3200 Training loss 0.07004442811012268 Validation loss 0.07101791352033615 Accuracy 0.7991250157356262\n",
      "Iteration 3210 Training loss 0.05924241989850998 Validation loss 0.05895920842885971 Accuracy 0.8406250476837158\n",
      "Iteration 3220 Training loss 0.06564821302890778 Validation loss 0.061083871871232986 Accuracy 0.8303750157356262\n",
      "Iteration 3230 Training loss 0.0602564662694931 Validation loss 0.05910414457321167 Accuracy 0.8406250476837158\n",
      "Iteration 3240 Training loss 0.04811831936240196 Validation loss 0.05879979208111763 Accuracy 0.8402500152587891\n",
      "Iteration 3250 Training loss 0.06389228999614716 Validation loss 0.05926785618066788 Accuracy 0.8396250605583191\n",
      "Iteration 3260 Training loss 0.05445783585309982 Validation loss 0.06025371700525284 Accuracy 0.8352500200271606\n",
      "Iteration 3270 Training loss 0.05904914811253548 Validation loss 0.05877864360809326 Accuracy 0.8411250114440918\n",
      "Iteration 3280 Training loss 0.05841406062245369 Validation loss 0.05871029943227768 Accuracy 0.8413750529289246\n",
      "Iteration 3290 Training loss 0.05536140501499176 Validation loss 0.0588286854326725 Accuracy 0.8398750424385071\n",
      "Iteration 3300 Training loss 0.058598507195711136 Validation loss 0.05864126607775688 Accuracy 0.8396250605583191\n",
      "Iteration 3310 Training loss 0.05931779742240906 Validation loss 0.07361502945423126 Accuracy 0.796000063419342\n",
      "Iteration 3320 Training loss 0.06513115018606186 Validation loss 0.0652632862329483 Accuracy 0.8218750357627869\n",
      "Iteration 3330 Training loss 0.05773455649614334 Validation loss 0.059239648282527924 Accuracy 0.8373750448226929\n",
      "Iteration 3340 Training loss 0.061982955783605576 Validation loss 0.06438074260950089 Accuracy 0.8191250562667847\n",
      "Iteration 3350 Training loss 0.054476652294397354 Validation loss 0.058707963675260544 Accuracy 0.8406250476837158\n",
      "Iteration 3360 Training loss 0.06428618729114532 Validation loss 0.05919932946562767 Accuracy 0.8386250138282776\n",
      "Iteration 3370 Training loss 0.05947555974125862 Validation loss 0.05883535370230675 Accuracy 0.8395000696182251\n",
      "Iteration 3380 Training loss 0.06144806370139122 Validation loss 0.060484878718853 Accuracy 0.8356250524520874\n",
      "Iteration 3390 Training loss 0.05747547373175621 Validation loss 0.06005411967635155 Accuracy 0.8365000486373901\n",
      "Iteration 3400 Training loss 0.05660241097211838 Validation loss 0.058429889380931854 Accuracy 0.8418750166893005\n",
      "Iteration 3410 Training loss 0.06132072955369949 Validation loss 0.05979873612523079 Accuracy 0.8370000123977661\n",
      "Iteration 3420 Training loss 0.058525826781988144 Validation loss 0.06015046685934067 Accuracy 0.8361250162124634\n",
      "Iteration 3430 Training loss 0.05145130679011345 Validation loss 0.05873754248023033 Accuracy 0.8416250348091125\n",
      "Iteration 3440 Training loss 0.07649286836385727 Validation loss 0.07144270837306976 Accuracy 0.8015000224113464\n",
      "Iteration 3450 Training loss 0.06578845530748367 Validation loss 0.05935797467827797 Accuracy 0.8382500410079956\n",
      "Iteration 3460 Training loss 0.06749839335680008 Validation loss 0.058901455253362656 Accuracy 0.8393750190734863\n",
      "Iteration 3470 Training loss 0.06083345413208008 Validation loss 0.061611682176589966 Accuracy 0.8330000638961792\n",
      "Iteration 3480 Training loss 0.06463976204395294 Validation loss 0.059485916048288345 Accuracy 0.8376250267028809\n",
      "Iteration 3490 Training loss 0.06103037670254707 Validation loss 0.05895587429404259 Accuracy 0.8395000696182251\n",
      "Iteration 3500 Training loss 0.06891058385372162 Validation loss 0.06917069852352142 Accuracy 0.8075000643730164\n",
      "Iteration 3510 Training loss 0.06211955100297928 Validation loss 0.06067058444023132 Accuracy 0.8360000252723694\n",
      "Iteration 3520 Training loss 0.06633801013231277 Validation loss 0.06579753011465073 Accuracy 0.8188750147819519\n",
      "Iteration 3530 Training loss 0.05970337614417076 Validation loss 0.05803217738866806 Accuracy 0.8415000438690186\n",
      "Iteration 3540 Training loss 0.06658853590488434 Validation loss 0.06703945994377136 Accuracy 0.8157500624656677\n",
      "Iteration 3550 Training loss 0.07205479592084885 Validation loss 0.07542387396097183 Accuracy 0.7908750176429749\n",
      "Iteration 3560 Training loss 0.06501096487045288 Validation loss 0.060933854430913925 Accuracy 0.8331250548362732\n",
      "Iteration 3570 Training loss 0.060719430446624756 Validation loss 0.060877274721860886 Accuracy 0.8336250185966492\n",
      "Iteration 3580 Training loss 0.0686187669634819 Validation loss 0.06228287145495415 Accuracy 0.8317500352859497\n",
      "Iteration 3590 Training loss 0.061712510883808136 Validation loss 0.06126578152179718 Accuracy 0.8297500610351562\n",
      "Iteration 3600 Training loss 0.055649422109127045 Validation loss 0.05805633217096329 Accuracy 0.8442500233650208\n",
      "Iteration 3610 Training loss 0.05621272325515747 Validation loss 0.057871539145708084 Accuracy 0.8426250219345093\n",
      "Iteration 3620 Training loss 0.05803397670388222 Validation loss 0.06071092560887337 Accuracy 0.8325000405311584\n",
      "Iteration 3630 Training loss 0.06138552352786064 Validation loss 0.06350582093000412 Accuracy 0.827875018119812\n",
      "Iteration 3640 Training loss 0.08040318638086319 Validation loss 0.07526213675737381 Accuracy 0.7895000576972961\n",
      "Iteration 3650 Training loss 0.058705661445856094 Validation loss 0.06153532490134239 Accuracy 0.8288750648498535\n",
      "Iteration 3660 Training loss 0.06001904234290123 Validation loss 0.059387385845184326 Accuracy 0.8372500538825989\n",
      "Iteration 3670 Training loss 0.05846274271607399 Validation loss 0.0580526702105999 Accuracy 0.843375027179718\n",
      "Iteration 3680 Training loss 0.06476271897554398 Validation loss 0.06436385214328766 Accuracy 0.8257500529289246\n",
      "Iteration 3690 Training loss 0.06278982013463974 Validation loss 0.059039752930402756 Accuracy 0.8381250500679016\n",
      "Iteration 3700 Training loss 0.06554177403450012 Validation loss 0.057891231030225754 Accuracy 0.8447500467300415\n",
      "Iteration 3710 Training loss 0.07428594678640366 Validation loss 0.06972845643758774 Accuracy 0.8038750290870667\n",
      "Iteration 3720 Training loss 0.054190054535865784 Validation loss 0.0580822192132473 Accuracy 0.84312504529953\n",
      "Iteration 3730 Training loss 0.05639011785387993 Validation loss 0.06111882999539375 Accuracy 0.8328750133514404\n",
      "Iteration 3740 Training loss 0.053606174886226654 Validation loss 0.05956365168094635 Accuracy 0.8360000252723694\n",
      "Iteration 3750 Training loss 0.053783051669597626 Validation loss 0.05768094211816788 Accuracy 0.8436250686645508\n",
      "Iteration 3760 Training loss 0.062062475830316544 Validation loss 0.05982847884297371 Accuracy 0.8375000357627869\n",
      "Iteration 3770 Training loss 0.04792698845267296 Validation loss 0.058459095656871796 Accuracy 0.8411250114440918\n",
      "Iteration 3780 Training loss 0.054398439824581146 Validation loss 0.05940713733434677 Accuracy 0.8386250138282776\n",
      "Iteration 3790 Training loss 0.06006888300180435 Validation loss 0.06374258548021317 Accuracy 0.8265000581741333\n",
      "Iteration 3800 Training loss 0.053582314401865005 Validation loss 0.062428683042526245 Accuracy 0.8300000429153442\n",
      "Iteration 3810 Training loss 0.05952944979071617 Validation loss 0.058100778609514236 Accuracy 0.8442500233650208\n",
      "Iteration 3820 Training loss 0.053409211337566376 Validation loss 0.058084968477487564 Accuracy 0.8442500233650208\n",
      "Iteration 3830 Training loss 0.0635569617152214 Validation loss 0.059171129018068314 Accuracy 0.8388750553131104\n",
      "Iteration 3840 Training loss 0.056816596537828445 Validation loss 0.05818752944469452 Accuracy 0.8423750400543213\n",
      "Iteration 3850 Training loss 0.061007216572761536 Validation loss 0.06092850863933563 Accuracy 0.8303750157356262\n",
      "Iteration 3860 Training loss 0.0658857449889183 Validation loss 0.06218313053250313 Accuracy 0.8268750309944153\n",
      "Iteration 3870 Training loss 0.06402204185724258 Validation loss 0.060372453182935715 Accuracy 0.8356250524520874\n",
      "Iteration 3880 Training loss 0.06204695627093315 Validation loss 0.062176331877708435 Accuracy 0.8301250338554382\n",
      "Iteration 3890 Training loss 0.05564003437757492 Validation loss 0.05925580859184265 Accuracy 0.8357500433921814\n",
      "Iteration 3900 Training loss 0.05309286341071129 Validation loss 0.057373762130737305 Accuracy 0.8458750247955322\n",
      "Iteration 3910 Training loss 0.05807478725910187 Validation loss 0.05754996836185455 Accuracy 0.8453750610351562\n",
      "Iteration 3920 Training loss 0.056214626878499985 Validation loss 0.059178538620471954 Accuracy 0.8391250371932983\n",
      "Iteration 3930 Training loss 0.0682976171374321 Validation loss 0.061783552169799805 Accuracy 0.827875018119812\n",
      "Iteration 3940 Training loss 0.07083998620510101 Validation loss 0.0666365772485733 Accuracy 0.8138750195503235\n",
      "Iteration 3950 Training loss 0.0682172030210495 Validation loss 0.05810536444187164 Accuracy 0.8412500619888306\n",
      "Iteration 3960 Training loss 0.055421702563762665 Validation loss 0.06352761387825012 Accuracy 0.827375054359436\n",
      "Iteration 3970 Training loss 0.058069247752428055 Validation loss 0.05739975348114967 Accuracy 0.8460000157356262\n",
      "Iteration 3980 Training loss 0.06835640966892242 Validation loss 0.06242690607905388 Accuracy 0.8247500658035278\n",
      "Iteration 3990 Training loss 0.06150036305189133 Validation loss 0.06026605889201164 Accuracy 0.8357500433921814\n",
      "Iteration 4000 Training loss 0.050153300166130066 Validation loss 0.057649172842502594 Accuracy 0.8445000648498535\n",
      "Iteration 4010 Training loss 0.06396190077066422 Validation loss 0.05714106932282448 Accuracy 0.846125066280365\n",
      "Iteration 4020 Training loss 0.0549875907599926 Validation loss 0.061377011239528656 Accuracy 0.8336250185966492\n",
      "Iteration 4030 Training loss 0.04919179901480675 Validation loss 0.05729761719703674 Accuracy 0.8455000519752502\n",
      "Iteration 4040 Training loss 0.06704617291688919 Validation loss 0.06150487810373306 Accuracy 0.8332500457763672\n",
      "Iteration 4050 Training loss 0.05964462086558342 Validation loss 0.057414691895246506 Accuracy 0.8440000414848328\n",
      "Iteration 4060 Training loss 0.06437728554010391 Validation loss 0.05721075460314751 Accuracy 0.8456250429153442\n",
      "Iteration 4070 Training loss 0.05927027389407158 Validation loss 0.060662515461444855 Accuracy 0.8350000381469727\n",
      "Iteration 4080 Training loss 0.05536844953894615 Validation loss 0.0578017495572567 Accuracy 0.8420000672340393\n",
      "Iteration 4090 Training loss 0.055997561663389206 Validation loss 0.056838296353816986 Accuracy 0.8472500443458557\n",
      "Iteration 4100 Training loss 0.0683654248714447 Validation loss 0.062264975160360336 Accuracy 0.8260000348091125\n",
      "Iteration 4110 Training loss 0.05789117515087128 Validation loss 0.0572027750313282 Accuracy 0.8441250324249268\n",
      "Iteration 4120 Training loss 0.061104029417037964 Validation loss 0.05819018930196762 Accuracy 0.8413750529289246\n",
      "Iteration 4130 Training loss 0.06365583091974258 Validation loss 0.059171564877033234 Accuracy 0.8391250371932983\n",
      "Iteration 4140 Training loss 0.05156863108277321 Validation loss 0.056746430695056915 Accuracy 0.8471250534057617\n",
      "Iteration 4150 Training loss 0.05372431501746178 Validation loss 0.05717483162879944 Accuracy 0.8456250429153442\n",
      "Iteration 4160 Training loss 0.047911081463098526 Validation loss 0.05712636560201645 Accuracy 0.8460000157356262\n",
      "Iteration 4170 Training loss 0.047083012759685516 Validation loss 0.05694227293133736 Accuracy 0.8471250534057617\n",
      "Iteration 4180 Training loss 0.051369454711675644 Validation loss 0.057252831757068634 Accuracy 0.8440000414848328\n",
      "Iteration 4190 Training loss 0.060212392359972 Validation loss 0.06164979189634323 Accuracy 0.8291250467300415\n",
      "Iteration 4200 Training loss 0.05804897099733353 Validation loss 0.05708380788564682 Accuracy 0.8460000157356262\n",
      "Iteration 4210 Training loss 0.06708579510450363 Validation loss 0.06567779183387756 Accuracy 0.8202500343322754\n",
      "Iteration 4220 Training loss 0.06068407744169235 Validation loss 0.05713895335793495 Accuracy 0.8438750505447388\n",
      "Iteration 4230 Training loss 0.052465811371803284 Validation loss 0.05752221867442131 Accuracy 0.8422500491142273\n",
      "Iteration 4240 Training loss 0.053966913372278214 Validation loss 0.05656618997454643 Accuracy 0.8481250405311584\n",
      "Iteration 4250 Training loss 0.05862680450081825 Validation loss 0.05777846276760101 Accuracy 0.8423750400543213\n",
      "Iteration 4260 Training loss 0.05340442433953285 Validation loss 0.05669349804520607 Accuracy 0.8481250405311584\n",
      "Iteration 4270 Training loss 0.05871940031647682 Validation loss 0.05704285576939583 Accuracy 0.8437500596046448\n",
      "Iteration 4280 Training loss 0.056441351771354675 Validation loss 0.060579944401979446 Accuracy 0.8321250677108765\n",
      "Iteration 4290 Training loss 0.05872446298599243 Validation loss 0.056958600878715515 Accuracy 0.8450000286102295\n",
      "Iteration 4300 Training loss 0.05456339567899704 Validation loss 0.05944887176156044 Accuracy 0.8353750109672546\n",
      "Iteration 4310 Training loss 0.05522415041923523 Validation loss 0.06552047282457352 Accuracy 0.8206250667572021\n",
      "Iteration 4320 Training loss 0.06334253400564194 Validation loss 0.06286279112100601 Accuracy 0.8283750414848328\n",
      "Iteration 4330 Training loss 0.05708965286612511 Validation loss 0.05632904917001724 Accuracy 0.8491250276565552\n",
      "Iteration 4340 Training loss 0.06163248419761658 Validation loss 0.06398423761129379 Accuracy 0.8248750567436218\n",
      "Iteration 4350 Training loss 0.05238962173461914 Validation loss 0.05665825307369232 Accuracy 0.8472500443458557\n",
      "Iteration 4360 Training loss 0.05882985517382622 Validation loss 0.058487553149461746 Accuracy 0.8386250138282776\n",
      "Iteration 4370 Training loss 0.05571296438574791 Validation loss 0.05647343769669533 Accuracy 0.8486250638961792\n",
      "Iteration 4380 Training loss 0.05607818439602852 Validation loss 0.05794253945350647 Accuracy 0.8403750658035278\n",
      "Iteration 4390 Training loss 0.05801227316260338 Validation loss 0.060157325118780136 Accuracy 0.8327500224113464\n",
      "Iteration 4400 Training loss 0.0574277900159359 Validation loss 0.0563669428229332 Accuracy 0.8483750224113464\n",
      "Iteration 4410 Training loss 0.059629764407873154 Validation loss 0.062419816851615906 Accuracy 0.8252500295639038\n",
      "Iteration 4420 Training loss 0.05140160396695137 Validation loss 0.056963320821523666 Accuracy 0.8456250429153442\n",
      "Iteration 4430 Training loss 0.049845580011606216 Validation loss 0.0652034804224968 Accuracy 0.8166250586509705\n",
      "Iteration 4440 Training loss 0.05304849147796631 Validation loss 0.059456270188093185 Accuracy 0.8392500281333923\n",
      "Iteration 4450 Training loss 0.05986234173178673 Validation loss 0.05639283359050751 Accuracy 0.8493750691413879\n",
      "Iteration 4460 Training loss 0.05776027962565422 Validation loss 0.05726911872625351 Accuracy 0.8436250686645508\n",
      "Iteration 4470 Training loss 0.05725971981883049 Validation loss 0.05720196291804314 Accuracy 0.8440000414848328\n",
      "Iteration 4480 Training loss 0.0475330613553524 Validation loss 0.056224722415208817 Accuracy 0.8491250276565552\n",
      "Iteration 4490 Training loss 0.051911745220422745 Validation loss 0.05628281831741333 Accuracy 0.8487500548362732\n",
      "Iteration 4500 Training loss 0.06302327662706375 Validation loss 0.05810680612921715 Accuracy 0.8417500257492065\n",
      "Iteration 4510 Training loss 0.045621346682310104 Validation loss 0.05624966323375702 Accuracy 0.8476250171661377\n",
      "Iteration 4520 Training loss 0.05662602186203003 Validation loss 0.06339908391237259 Accuracy 0.8271250128746033\n",
      "Iteration 4530 Training loss 0.059310004115104675 Validation loss 0.055947642773389816 Accuracy 0.8500000238418579\n",
      "Iteration 4540 Training loss 0.055334508419036865 Validation loss 0.05603265389800072 Accuracy 0.8491250276565552\n",
      "Iteration 4550 Training loss 0.05347200855612755 Validation loss 0.0568254292011261 Accuracy 0.8457500338554382\n",
      "Iteration 4560 Training loss 0.06722770631313324 Validation loss 0.05636197328567505 Accuracy 0.8471250534057617\n",
      "Iteration 4570 Training loss 0.07235782593488693 Validation loss 0.06821144372224808 Accuracy 0.812125027179718\n",
      "Iteration 4580 Training loss 0.06493273377418518 Validation loss 0.06500399112701416 Accuracy 0.8225000500679016\n",
      "Iteration 4590 Training loss 0.060781434178352356 Validation loss 0.057185590267181396 Accuracy 0.8458750247955322\n",
      "Iteration 4600 Training loss 0.05092340335249901 Validation loss 0.05705210566520691 Accuracy 0.8460000157356262\n",
      "Iteration 4610 Training loss 0.06487090140581131 Validation loss 0.056019220501184464 Accuracy 0.8501250147819519\n",
      "Iteration 4620 Training loss 0.06523040682077408 Validation loss 0.06023382022976875 Accuracy 0.8343750238418579\n",
      "Iteration 4630 Training loss 0.051556870341300964 Validation loss 0.055934976786375046 Accuracy 0.8511250615119934\n",
      "Iteration 4640 Training loss 0.04914540797472 Validation loss 0.055877458304166794 Accuracy 0.8498750329017639\n",
      "Iteration 4650 Training loss 0.05374501645565033 Validation loss 0.057273995131254196 Accuracy 0.8438750505447388\n",
      "Iteration 4660 Training loss 0.06420896202325821 Validation loss 0.06028950959444046 Accuracy 0.8365000486373901\n",
      "Iteration 4670 Training loss 0.05822175741195679 Validation loss 0.05643327534198761 Accuracy 0.846875011920929\n",
      "Iteration 4680 Training loss 0.0605463832616806 Validation loss 0.05579117685556412 Accuracy 0.8495000600814819\n",
      "Iteration 4690 Training loss 0.07140807062387466 Validation loss 0.0667819008231163 Accuracy 0.8097500205039978\n",
      "Iteration 4700 Training loss 0.06056968867778778 Validation loss 0.05595451593399048 Accuracy 0.8480000495910645\n",
      "Iteration 4710 Training loss 0.054352495819330215 Validation loss 0.05568494647741318 Accuracy 0.8502500653266907\n",
      "Iteration 4720 Training loss 0.056821439415216446 Validation loss 0.055722519755363464 Accuracy 0.8508750200271606\n",
      "Iteration 4730 Training loss 0.05738801881670952 Validation loss 0.057358067482709885 Accuracy 0.8422500491142273\n",
      "Iteration 4740 Training loss 0.05518163740634918 Validation loss 0.05789679288864136 Accuracy 0.8415000438690186\n",
      "Iteration 4750 Training loss 0.060033440589904785 Validation loss 0.056221675127744675 Accuracy 0.8487500548362732\n",
      "Iteration 4760 Training loss 0.053201548755168915 Validation loss 0.05549774318933487 Accuracy 0.8523750305175781\n",
      "Iteration 4770 Training loss 0.054928891360759735 Validation loss 0.05822683125734329 Accuracy 0.8426250219345093\n",
      "Iteration 4780 Training loss 0.046607475727796555 Validation loss 0.05578019469976425 Accuracy 0.8501250147819519\n",
      "Iteration 4790 Training loss 0.05106591433286667 Validation loss 0.055920664221048355 Accuracy 0.8485000133514404\n",
      "Iteration 4800 Training loss 0.05867315083742142 Validation loss 0.057051729410886765 Accuracy 0.8450000286102295\n",
      "Iteration 4810 Training loss 0.05267256498336792 Validation loss 0.05617665499448776 Accuracy 0.8480000495910645\n",
      "Iteration 4820 Training loss 0.06288137286901474 Validation loss 0.07033916562795639 Accuracy 0.8050000667572021\n",
      "Iteration 4830 Training loss 0.05548844113945961 Validation loss 0.05624460056424141 Accuracy 0.8476250171661377\n",
      "Iteration 4840 Training loss 0.05457177013158798 Validation loss 0.056949809193611145 Accuracy 0.8457500338554382\n",
      "Iteration 4850 Training loss 0.06008051708340645 Validation loss 0.05625687167048454 Accuracy 0.8483750224113464\n",
      "Iteration 4860 Training loss 0.05555998533964157 Validation loss 0.062388986349105835 Accuracy 0.8323750495910645\n",
      "Iteration 4870 Training loss 0.05272943899035454 Validation loss 0.0557636097073555 Accuracy 0.8493750691413879\n",
      "Iteration 4880 Training loss 0.04420913755893707 Validation loss 0.05579100176692009 Accuracy 0.8497500419616699\n",
      "Iteration 4890 Training loss 0.05451521277427673 Validation loss 0.05554543063044548 Accuracy 0.8502500653266907\n",
      "Iteration 4900 Training loss 0.05155069753527641 Validation loss 0.05575009062886238 Accuracy 0.8502500653266907\n",
      "Iteration 4910 Training loss 0.05722001940011978 Validation loss 0.05934511497616768 Accuracy 0.8366250395774841\n",
      "Iteration 4920 Training loss 0.07261120527982712 Validation loss 0.06485119462013245 Accuracy 0.8221250176429749\n",
      "Iteration 4930 Training loss 0.06978192925453186 Validation loss 0.061159901320934296 Accuracy 0.8332500457763672\n",
      "Iteration 4940 Training loss 0.06395316869020462 Validation loss 0.057840198278427124 Accuracy 0.842875063419342\n",
      "Iteration 4950 Training loss 0.06491641700267792 Validation loss 0.057885121554136276 Accuracy 0.8425000309944153\n",
      "Iteration 4960 Training loss 0.06153750419616699 Validation loss 0.06043349951505661 Accuracy 0.8371250629425049\n",
      "Iteration 4970 Training loss 0.05866992473602295 Validation loss 0.0586562417447567 Accuracy 0.8396250605583191\n",
      "Iteration 4980 Training loss 0.06163357198238373 Validation loss 0.06017511710524559 Accuracy 0.8331250548362732\n",
      "Iteration 4990 Training loss 0.05124017596244812 Validation loss 0.06368757039308548 Accuracy 0.8222500681877136\n",
      "Iteration 5000 Training loss 0.05626222863793373 Validation loss 0.057495154440402985 Accuracy 0.8438750505447388\n",
      "Iteration 5010 Training loss 0.055113401263952255 Validation loss 0.05623823031783104 Accuracy 0.8476250171661377\n",
      "Iteration 5020 Training loss 0.05689609423279762 Validation loss 0.055179789662361145 Accuracy 0.8531250357627869\n",
      "Iteration 5030 Training loss 0.056264881044626236 Validation loss 0.05542439594864845 Accuracy 0.8501250147819519\n",
      "Iteration 5040 Training loss 0.05600189417600632 Validation loss 0.056564174592494965 Accuracy 0.8475000262260437\n",
      "Iteration 5050 Training loss 0.057166337966918945 Validation loss 0.06250763684511185 Accuracy 0.8260000348091125\n",
      "Iteration 5060 Training loss 0.05489658564329147 Validation loss 0.05997171252965927 Accuracy 0.8357500433921814\n",
      "Iteration 5070 Training loss 0.05387629568576813 Validation loss 0.056536998599767685 Accuracy 0.846500039100647\n",
      "Iteration 5080 Training loss 0.05409466475248337 Validation loss 0.056931231170892715 Accuracy 0.846625030040741\n",
      "Iteration 5090 Training loss 0.049798086285591125 Validation loss 0.0552644282579422 Accuracy 0.8515000343322754\n",
      "Iteration 5100 Training loss 0.04565926268696785 Validation loss 0.0552355982363224 Accuracy 0.8530000448226929\n",
      "Iteration 5110 Training loss 0.04767323285341263 Validation loss 0.05509801208972931 Accuracy 0.8513750433921814\n",
      "Iteration 5120 Training loss 0.04882543534040451 Validation loss 0.05487591773271561 Accuracy 0.8542500138282776\n",
      "Iteration 5130 Training loss 0.055238619446754456 Validation loss 0.05668728053569794 Accuracy 0.8452500104904175\n",
      "Iteration 5140 Training loss 0.05770934745669365 Validation loss 0.054977256804704666 Accuracy 0.8527500629425049\n",
      "Iteration 5150 Training loss 0.06311440467834473 Validation loss 0.05823514983057976 Accuracy 0.8410000205039978\n",
      "Iteration 5160 Training loss 0.054968759417533875 Validation loss 0.05669645965099335 Accuracy 0.8448750376701355\n",
      "Iteration 5170 Training loss 0.057822924107313156 Validation loss 0.05557483807206154 Accuracy 0.8491250276565552\n",
      "Iteration 5180 Training loss 0.05607234686613083 Validation loss 0.06175726652145386 Accuracy 0.8323750495910645\n",
      "Iteration 5190 Training loss 0.062285661697387695 Validation loss 0.057480502873659134 Accuracy 0.8445000648498535\n",
      "Iteration 5200 Training loss 0.04964560270309448 Validation loss 0.05669538676738739 Accuracy 0.8460000157356262\n",
      "Iteration 5210 Training loss 0.04666880518198013 Validation loss 0.05523424968123436 Accuracy 0.8506250381469727\n",
      "Iteration 5220 Training loss 0.06885756552219391 Validation loss 0.062200888991355896 Accuracy 0.8260000348091125\n",
      "Iteration 5230 Training loss 0.06349113583564758 Validation loss 0.05681735649704933 Accuracy 0.8443750143051147\n",
      "Iteration 5240 Training loss 0.046490658074617386 Validation loss 0.05697508156299591 Accuracy 0.8458750247955322\n",
      "Iteration 5250 Training loss 0.06743220239877701 Validation loss 0.06022803485393524 Accuracy 0.8340000510215759\n",
      "Iteration 5260 Training loss 0.04927739128470421 Validation loss 0.06071966141462326 Accuracy 0.831250011920929\n",
      "Iteration 5270 Training loss 0.05838390067219734 Validation loss 0.05833696201443672 Accuracy 0.8393750190734863\n",
      "Iteration 5280 Training loss 0.05614083260297775 Validation loss 0.057281799614429474 Accuracy 0.843000054359436\n",
      "Iteration 5290 Training loss 0.05324115604162216 Validation loss 0.055993348360061646 Accuracy 0.8457500338554382\n",
      "Iteration 5300 Training loss 0.056814178824424744 Validation loss 0.05461112782359123 Accuracy 0.8540000319480896\n",
      "Iteration 5310 Training loss 0.05454656109213829 Validation loss 0.05527573078870773 Accuracy 0.8503750562667847\n",
      "Iteration 5320 Training loss 0.05009220540523529 Validation loss 0.05900540575385094 Accuracy 0.8393750190734863\n",
      "Iteration 5330 Training loss 0.052936580032110214 Validation loss 0.05546959862112999 Accuracy 0.8502500653266907\n",
      "Iteration 5340 Training loss 0.049439333379268646 Validation loss 0.05774698778986931 Accuracy 0.8401250243186951\n",
      "Iteration 5350 Training loss 0.054637011140584946 Validation loss 0.058207590132951736 Accuracy 0.8420000672340393\n",
      "Iteration 5360 Training loss 0.05431241914629936 Validation loss 0.05547800660133362 Accuracy 0.8493750691413879\n",
      "Iteration 5370 Training loss 0.05046886205673218 Validation loss 0.05463652312755585 Accuracy 0.8552500605583191\n",
      "Iteration 5380 Training loss 0.05059768259525299 Validation loss 0.05759537220001221 Accuracy 0.8411250114440918\n",
      "Iteration 5390 Training loss 0.04922904074192047 Validation loss 0.054991524666547775 Accuracy 0.8522500395774841\n",
      "Iteration 5400 Training loss 0.06286858767271042 Validation loss 0.06214974820613861 Accuracy 0.8271250128746033\n",
      "Iteration 5410 Training loss 0.05929064750671387 Validation loss 0.05510589852929115 Accuracy 0.8507500290870667\n",
      "Iteration 5420 Training loss 0.05700548365712166 Validation loss 0.05788165703415871 Accuracy 0.84312504529953\n",
      "Iteration 5430 Training loss 0.04868684709072113 Validation loss 0.054864488542079926 Accuracy 0.8520000576972961\n",
      "Iteration 5440 Training loss 0.05262472480535507 Validation loss 0.05721982195973396 Accuracy 0.84312504529953\n",
      "Iteration 5450 Training loss 0.05746214836835861 Validation loss 0.05444201081991196 Accuracy 0.8546250462532043\n",
      "Iteration 5460 Training loss 0.05093400925397873 Validation loss 0.055715300142765045 Accuracy 0.8483750224113464\n",
      "Iteration 5470 Training loss 0.05362183600664139 Validation loss 0.05459808558225632 Accuracy 0.8536250591278076\n",
      "Iteration 5480 Training loss 0.057925377041101456 Validation loss 0.06014519929885864 Accuracy 0.8368750214576721\n",
      "Iteration 5490 Training loss 0.061863306909799576 Validation loss 0.05623495578765869 Accuracy 0.8475000262260437\n",
      "Iteration 5500 Training loss 0.06223636493086815 Validation loss 0.05496896803379059 Accuracy 0.8518750667572021\n",
      "Iteration 5510 Training loss 0.0547296404838562 Validation loss 0.056799646466970444 Accuracy 0.8457500338554382\n",
      "Iteration 5520 Training loss 0.05388466268777847 Validation loss 0.05484361574053764 Accuracy 0.8518750667572021\n",
      "Iteration 5530 Training loss 0.054167021065950394 Validation loss 0.054258380085229874 Accuracy 0.8548750281333923\n",
      "Iteration 5540 Training loss 0.050230178982019424 Validation loss 0.05465008318424225 Accuracy 0.8518750667572021\n",
      "Iteration 5550 Training loss 0.049027469009160995 Validation loss 0.0572567917406559 Accuracy 0.8460000157356262\n",
      "Iteration 5560 Training loss 0.0633818507194519 Validation loss 0.06275267899036407 Accuracy 0.8255000114440918\n",
      "Iteration 5570 Training loss 0.05534547567367554 Validation loss 0.0610627606511116 Accuracy 0.8355000615119934\n",
      "Iteration 5580 Training loss 0.05962895601987839 Validation loss 0.05422639846801758 Accuracy 0.8541250228881836\n",
      "Iteration 5590 Training loss 0.05798035487532616 Validation loss 0.05500298738479614 Accuracy 0.8490000367164612\n",
      "Iteration 5600 Training loss 0.048141248524188995 Validation loss 0.05557285621762276 Accuracy 0.8511250615119934\n",
      "Iteration 5610 Training loss 0.05146823450922966 Validation loss 0.054164111614227295 Accuracy 0.8528750538825989\n",
      "Iteration 5620 Training loss 0.058480825275182724 Validation loss 0.05464315041899681 Accuracy 0.8530000448226929\n",
      "Iteration 5630 Training loss 0.051637619733810425 Validation loss 0.054609186947345734 Accuracy 0.8523750305175781\n",
      "Iteration 5640 Training loss 0.06450528651475906 Validation loss 0.05974947661161423 Accuracy 0.8386250138282776\n",
      "Iteration 5650 Training loss 0.05731406435370445 Validation loss 0.05761091783642769 Accuracy 0.8441250324249268\n",
      "Iteration 5660 Training loss 0.05249438062310219 Validation loss 0.05528127774596214 Accuracy 0.8497500419616699\n",
      "Iteration 5670 Training loss 0.05495511740446091 Validation loss 0.05521758273243904 Accuracy 0.8513750433921814\n",
      "Iteration 5680 Training loss 0.059056930243968964 Validation loss 0.055789921432733536 Accuracy 0.8491250276565552\n",
      "Iteration 5690 Training loss 0.04928574338555336 Validation loss 0.05701588839292526 Accuracy 0.8460000157356262\n",
      "Iteration 5700 Training loss 0.058652982115745544 Validation loss 0.054488491266965866 Accuracy 0.8546250462532043\n",
      "Iteration 5710 Training loss 0.052416808903217316 Validation loss 0.05475085601210594 Accuracy 0.8508750200271606\n",
      "Iteration 5720 Training loss 0.051374852657318115 Validation loss 0.061407603323459625 Accuracy 0.8336250185966492\n",
      "Iteration 5730 Training loss 0.050759248435497284 Validation loss 0.053938742727041245 Accuracy 0.8546250462532043\n",
      "Iteration 5740 Training loss 0.05781855434179306 Validation loss 0.05512666702270508 Accuracy 0.8502500653266907\n",
      "Iteration 5750 Training loss 0.05583507940173149 Validation loss 0.054608941078186035 Accuracy 0.8521250486373901\n",
      "Iteration 5760 Training loss 0.054060474038124084 Validation loss 0.05891626328229904 Accuracy 0.8378750681877136\n",
      "Iteration 5770 Training loss 0.06051013991236687 Validation loss 0.05864904448390007 Accuracy 0.8417500257492065\n",
      "Iteration 5780 Training loss 0.04688240587711334 Validation loss 0.05493804067373276 Accuracy 0.8513750433921814\n",
      "Iteration 5790 Training loss 0.04524951055645943 Validation loss 0.054269347339868546 Accuracy 0.8527500629425049\n",
      "Iteration 5800 Training loss 0.06125740706920624 Validation loss 0.055653396993875504 Accuracy 0.8488750457763672\n",
      "Iteration 5810 Training loss 0.052406445145606995 Validation loss 0.054945625364780426 Accuracy 0.8492500185966492\n",
      "Iteration 5820 Training loss 0.05603310838341713 Validation loss 0.053657807409763336 Accuracy 0.8576250672340393\n",
      "Iteration 5830 Training loss 0.06047183275222778 Validation loss 0.05396714434027672 Accuracy 0.8532500267028809\n",
      "Iteration 5840 Training loss 0.055480651557445526 Validation loss 0.05365704372525215 Accuracy 0.8560000658035278\n",
      "Iteration 5850 Training loss 0.06112894043326378 Validation loss 0.0608426034450531 Accuracy 0.830500066280365\n",
      "Iteration 5860 Training loss 0.05653093382716179 Validation loss 0.06063569709658623 Accuracy 0.8332500457763672\n",
      "Iteration 5870 Training loss 0.05520792677998543 Validation loss 0.06031128764152527 Accuracy 0.8343750238418579\n",
      "Iteration 5880 Training loss 0.04682920500636101 Validation loss 0.05703345686197281 Accuracy 0.8457500338554382\n",
      "Iteration 5890 Training loss 0.06385387480258942 Validation loss 0.05399145185947418 Accuracy 0.8548750281333923\n",
      "Iteration 5900 Training loss 0.049805354326963425 Validation loss 0.055967845022678375 Accuracy 0.8486250638961792\n",
      "Iteration 5910 Training loss 0.049916934221982956 Validation loss 0.05357948690652847 Accuracy 0.8560000658035278\n",
      "Iteration 5920 Training loss 0.05928215757012367 Validation loss 0.05919690057635307 Accuracy 0.8381250500679016\n",
      "Iteration 5930 Training loss 0.05047007277607918 Validation loss 0.053505685180425644 Accuracy 0.8567500114440918\n",
      "Iteration 5940 Training loss 0.05308430641889572 Validation loss 0.055672723799943924 Accuracy 0.8478750586509705\n",
      "Iteration 5950 Training loss 0.05258854851126671 Validation loss 0.05373166874051094 Accuracy 0.8541250228881836\n",
      "Iteration 5960 Training loss 0.058009691536426544 Validation loss 0.05487488582730293 Accuracy 0.8528750538825989\n",
      "Iteration 5970 Training loss 0.03937278687953949 Validation loss 0.05359034985303879 Accuracy 0.8576250672340393\n",
      "Iteration 5980 Training loss 0.04671933129429817 Validation loss 0.05411307141184807 Accuracy 0.8546250462532043\n",
      "Iteration 5990 Training loss 0.05937128886580467 Validation loss 0.06271330267190933 Accuracy 0.8288750648498535\n",
      "Iteration 6000 Training loss 0.059717100113630295 Validation loss 0.05934397876262665 Accuracy 0.8385000228881836\n",
      "Iteration 6010 Training loss 0.0666218250989914 Validation loss 0.06365589052438736 Accuracy 0.8196250200271606\n",
      "Iteration 6020 Training loss 0.055263739079236984 Validation loss 0.05614906921982765 Accuracy 0.8485000133514404\n",
      "Iteration 6030 Training loss 0.04922240227460861 Validation loss 0.055232755839824677 Accuracy 0.8516250252723694\n",
      "Iteration 6040 Training loss 0.05443063750863075 Validation loss 0.05339362472295761 Accuracy 0.8580000400543213\n",
      "Iteration 6050 Training loss 0.05432048812508583 Validation loss 0.05874304473400116 Accuracy 0.8390000462532043\n",
      "Iteration 6060 Training loss 0.05766904354095459 Validation loss 0.053916819393634796 Accuracy 0.8538750410079956\n",
      "Iteration 6070 Training loss 0.06385812908411026 Validation loss 0.06287089735269547 Accuracy 0.8240000605583191\n",
      "Iteration 6080 Training loss 0.048767607659101486 Validation loss 0.053379107266664505 Accuracy 0.8576250672340393\n",
      "Iteration 6090 Training loss 0.05406658351421356 Validation loss 0.05521228164434433 Accuracy 0.8485000133514404\n",
      "Iteration 6100 Training loss 0.049627598375082016 Validation loss 0.05414010211825371 Accuracy 0.8547500371932983\n",
      "Iteration 6110 Training loss 0.05875100940465927 Validation loss 0.056005097925662994 Accuracy 0.8501250147819519\n",
      "Iteration 6120 Training loss 0.05366164445877075 Validation loss 0.05447086691856384 Accuracy 0.8535000681877136\n",
      "Iteration 6130 Training loss 0.0582052581012249 Validation loss 0.05895169451832771 Accuracy 0.8376250267028809\n",
      "Iteration 6140 Training loss 0.046647436916828156 Validation loss 0.054379209876060486 Accuracy 0.8533750176429749\n",
      "Iteration 6150 Training loss 0.05230329558253288 Validation loss 0.05323855206370354 Accuracy 0.8576250672340393\n",
      "Iteration 6160 Training loss 0.05050881206989288 Validation loss 0.053391385823488235 Accuracy 0.8557500243186951\n",
      "Iteration 6170 Training loss 0.05309103801846504 Validation loss 0.05644240230321884 Accuracy 0.8451250195503235\n",
      "Iteration 6180 Training loss 0.05631355196237564 Validation loss 0.053123246878385544 Accuracy 0.8578750491142273\n",
      "Iteration 6190 Training loss 0.052587904036045074 Validation loss 0.053617212921381 Accuracy 0.8546250462532043\n",
      "Iteration 6200 Training loss 0.054181262850761414 Validation loss 0.05419111251831055 Accuracy 0.8541250228881836\n",
      "Iteration 6210 Training loss 0.048057716339826584 Validation loss 0.056409094482660294 Accuracy 0.8470000624656677\n",
      "Iteration 6220 Training loss 0.05424442142248154 Validation loss 0.0566035732626915 Accuracy 0.8457500338554382\n",
      "Iteration 6230 Training loss 0.06652285158634186 Validation loss 0.05573171004652977 Accuracy 0.8505000472068787\n",
      "Iteration 6240 Training loss 0.05089886486530304 Validation loss 0.055213555693626404 Accuracy 0.8513750433921814\n",
      "Iteration 6250 Training loss 0.06066529080271721 Validation loss 0.056917060166597366 Accuracy 0.843500018119812\n",
      "Iteration 6260 Training loss 0.053675804287195206 Validation loss 0.053310930728912354 Accuracy 0.8550000190734863\n",
      "Iteration 6270 Training loss 0.052433665841817856 Validation loss 0.05751083418726921 Accuracy 0.8455000519752502\n",
      "Iteration 6280 Training loss 0.05268152058124542 Validation loss 0.05974128842353821 Accuracy 0.8350000381469727\n",
      "Iteration 6290 Training loss 0.046395864337682724 Validation loss 0.054472845047712326 Accuracy 0.8515000343322754\n",
      "Iteration 6300 Training loss 0.05918310955166817 Validation loss 0.05744298920035362 Accuracy 0.843250036239624\n",
      "Iteration 6310 Training loss 0.0437810979783535 Validation loss 0.05292186141014099 Accuracy 0.8576250672340393\n",
      "Iteration 6320 Training loss 0.05129062384366989 Validation loss 0.05394855886697769 Accuracy 0.8541250228881836\n",
      "Iteration 6330 Training loss 0.04443276301026344 Validation loss 0.05364622920751572 Accuracy 0.8546250462532043\n",
      "Iteration 6340 Training loss 0.05236682668328285 Validation loss 0.05581028386950493 Accuracy 0.8500000238418579\n",
      "Iteration 6350 Training loss 0.04983282461762428 Validation loss 0.05341604724526405 Accuracy 0.8540000319480896\n",
      "Iteration 6360 Training loss 0.05312887206673622 Validation loss 0.052811380475759506 Accuracy 0.8580000400543213\n",
      "Iteration 6370 Training loss 0.05784666910767555 Validation loss 0.05328655242919922 Accuracy 0.8545000553131104\n",
      "Iteration 6380 Training loss 0.04792112112045288 Validation loss 0.053841687738895416 Accuracy 0.8527500629425049\n",
      "Iteration 6390 Training loss 0.04958835616707802 Validation loss 0.05271683260798454 Accuracy 0.858625054359436\n",
      "Iteration 6400 Training loss 0.05348477140069008 Validation loss 0.05397125333547592 Accuracy 0.8528750538825989\n",
      "Iteration 6410 Training loss 0.05813393369317055 Validation loss 0.05907163396477699 Accuracy 0.8385000228881836\n",
      "Iteration 6420 Training loss 0.054282017052173615 Validation loss 0.05324738472700119 Accuracy 0.8553750514984131\n",
      "Iteration 6430 Training loss 0.05975734442472458 Validation loss 0.0603049173951149 Accuracy 0.8328750133514404\n",
      "Iteration 6440 Training loss 0.04200058802962303 Validation loss 0.052678149193525314 Accuracy 0.859125018119812\n",
      "Iteration 6450 Training loss 0.05619734153151512 Validation loss 0.05360676720738411 Accuracy 0.8557500243186951\n",
      "Iteration 6460 Training loss 0.05857625976204872 Validation loss 0.054769761860370636 Accuracy 0.8537500500679016\n",
      "Iteration 6470 Training loss 0.0501449778676033 Validation loss 0.05301273241639137 Accuracy 0.8571250438690186\n",
      "Iteration 6480 Training loss 0.05416594073176384 Validation loss 0.05344795063138008 Accuracy 0.8556250333786011\n",
      "Iteration 6490 Training loss 0.050334855914115906 Validation loss 0.05287003517150879 Accuracy 0.8572500348091125\n",
      "Iteration 6500 Training loss 0.05100879445672035 Validation loss 0.05557303503155708 Accuracy 0.8478750586509705\n",
      "Iteration 6510 Training loss 0.045738864690065384 Validation loss 0.053895559161901474 Accuracy 0.8543750643730164\n",
      "Iteration 6520 Training loss 0.04520188644528389 Validation loss 0.052574846893548965 Accuracy 0.8576250672340393\n",
      "Iteration 6530 Training loss 0.049740590155124664 Validation loss 0.05345083028078079 Accuracy 0.8540000319480896\n",
      "Iteration 6540 Training loss 0.05509587749838829 Validation loss 0.055632539093494415 Accuracy 0.8513750433921814\n",
      "Iteration 6550 Training loss 0.050995320081710815 Validation loss 0.052476197481155396 Accuracy 0.8575000166893005\n",
      "Iteration 6560 Training loss 0.04928288981318474 Validation loss 0.052779849618673325 Accuracy 0.8570000529289246\n",
      "Iteration 6570 Training loss 0.05099805071949959 Validation loss 0.05252636969089508 Accuracy 0.8581250309944153\n",
      "Iteration 6580 Training loss 0.07610167562961578 Validation loss 0.06998700648546219 Accuracy 0.8015000224113464\n",
      "Iteration 6590 Training loss 0.05547250807285309 Validation loss 0.055314913392066956 Accuracy 0.8481250405311584\n",
      "Iteration 6600 Training loss 0.05168214067816734 Validation loss 0.060540821403265 Accuracy 0.8323750495910645\n",
      "Iteration 6610 Training loss 0.05662349611520767 Validation loss 0.05737272650003433 Accuracy 0.8442500233650208\n",
      "Iteration 6620 Training loss 0.05968911573290825 Validation loss 0.05326167494058609 Accuracy 0.8543750643730164\n",
      "Iteration 6630 Training loss 0.06097179651260376 Validation loss 0.05250359699130058 Accuracy 0.8577500581741333\n",
      "Iteration 6640 Training loss 0.04777618125081062 Validation loss 0.05274055153131485 Accuracy 0.8572500348091125\n",
      "Iteration 6650 Training loss 0.06393641978502274 Validation loss 0.052482470870018005 Accuracy 0.85875004529953\n",
      "Iteration 6660 Training loss 0.04915058612823486 Validation loss 0.05394454300403595 Accuracy 0.8561250567436218\n",
      "Iteration 6670 Training loss 0.05616015940904617 Validation loss 0.05669403076171875 Accuracy 0.846875011920929\n",
      "Iteration 6680 Training loss 0.0489385724067688 Validation loss 0.05234282836318016 Accuracy 0.8582500219345093\n",
      "Iteration 6690 Training loss 0.060954052954912186 Validation loss 0.05743696168065071 Accuracy 0.843000054359436\n",
      "Iteration 6700 Training loss 0.040475714951753616 Validation loss 0.05247541517019272 Accuracy 0.8578750491142273\n",
      "Iteration 6710 Training loss 0.04864704981446266 Validation loss 0.05710660666227341 Accuracy 0.8422500491142273\n",
      "Iteration 6720 Training loss 0.054448410868644714 Validation loss 0.057432908564805984 Accuracy 0.8453750610351562\n",
      "Iteration 6730 Training loss 0.04965110495686531 Validation loss 0.052469827234745026 Accuracy 0.8576250672340393\n",
      "Iteration 6740 Training loss 0.04807400703430176 Validation loss 0.05434392765164375 Accuracy 0.8545000553131104\n",
      "Iteration 6750 Training loss 0.03844210132956505 Validation loss 0.052389148622751236 Accuracy 0.8583750128746033\n",
      "Iteration 6760 Training loss 0.04013951122760773 Validation loss 0.052790530025959015 Accuracy 0.8557500243186951\n",
      "Iteration 6770 Training loss 0.06078466400504112 Validation loss 0.05648934468626976 Accuracy 0.8440000414848328\n",
      "Iteration 6780 Training loss 0.05113622546195984 Validation loss 0.056742679327726364 Accuracy 0.8438750505447388\n",
      "Iteration 6790 Training loss 0.048887524753808975 Validation loss 0.05215877667069435 Accuracy 0.8583750128746033\n",
      "Iteration 6800 Training loss 0.054950207471847534 Validation loss 0.05330090969800949 Accuracy 0.8552500605583191\n",
      "Iteration 6810 Training loss 0.04528314620256424 Validation loss 0.05255765840411186 Accuracy 0.8562500476837158\n",
      "Iteration 6820 Training loss 0.05982893332839012 Validation loss 0.058426517993211746 Accuracy 0.8388750553131104\n",
      "Iteration 6830 Training loss 0.04929331690073013 Validation loss 0.051924288272857666 Accuracy 0.8601250648498535\n",
      "Iteration 6840 Training loss 0.06075367331504822 Validation loss 0.0548260323703289 Accuracy 0.8478750586509705\n",
      "Iteration 6850 Training loss 0.052813585847616196 Validation loss 0.05816076323390007 Accuracy 0.8400000333786011\n",
      "Iteration 6860 Training loss 0.0447976291179657 Validation loss 0.05213044211268425 Accuracy 0.8600000143051147\n",
      "Iteration 6870 Training loss 0.04833466559648514 Validation loss 0.05303362011909485 Accuracy 0.8553750514984131\n",
      "Iteration 6880 Training loss 0.04877599701285362 Validation loss 0.05295233428478241 Accuracy 0.8582500219345093\n",
      "Iteration 6890 Training loss 0.04917747899889946 Validation loss 0.05499817803502083 Accuracy 0.8528750538825989\n",
      "Iteration 6900 Training loss 0.048502665013074875 Validation loss 0.05317331850528717 Accuracy 0.8551250696182251\n",
      "Iteration 6910 Training loss 0.049042727798223495 Validation loss 0.05425461754202843 Accuracy 0.8506250381469727\n",
      "Iteration 6920 Training loss 0.05010026693344116 Validation loss 0.05218978971242905 Accuracy 0.858500063419342\n",
      "Iteration 6930 Training loss 0.05731327459216118 Validation loss 0.05332183465361595 Accuracy 0.8535000681877136\n",
      "Iteration 6940 Training loss 0.051534127444028854 Validation loss 0.05364224314689636 Accuracy 0.8570000529289246\n",
      "Iteration 6950 Training loss 0.04634746536612511 Validation loss 0.05245983973145485 Accuracy 0.8595000505447388\n",
      "Iteration 6960 Training loss 0.05344619229435921 Validation loss 0.05196739360690117 Accuracy 0.8597500324249268\n",
      "Iteration 6970 Training loss 0.052052758634090424 Validation loss 0.054240841418504715 Accuracy 0.8500000238418579\n",
      "Iteration 6980 Training loss 0.04418705031275749 Validation loss 0.05291000008583069 Accuracy 0.858625054359436\n",
      "Iteration 6990 Training loss 0.05685776099562645 Validation loss 0.05649634078145027 Accuracy 0.8455000519752502\n",
      "Iteration 7000 Training loss 0.06273195892572403 Validation loss 0.06005674973130226 Accuracy 0.8352500200271606\n",
      "Iteration 7010 Training loss 0.06032903119921684 Validation loss 0.056551165878772736 Accuracy 0.8482500314712524\n",
      "Iteration 7020 Training loss 0.05226770415902138 Validation loss 0.05242551490664482 Accuracy 0.858625054359436\n",
      "Iteration 7030 Training loss 0.04998103901743889 Validation loss 0.05383910611271858 Accuracy 0.8522500395774841\n",
      "Iteration 7040 Training loss 0.05494808033108711 Validation loss 0.05598427727818489 Accuracy 0.8505000472068787\n",
      "Iteration 7050 Training loss 0.045045774430036545 Validation loss 0.055746112018823624 Accuracy 0.8515000343322754\n",
      "Iteration 7060 Training loss 0.0590115450322628 Validation loss 0.051901478320360184 Accuracy 0.8598750233650208\n",
      "Iteration 7070 Training loss 0.045968394726514816 Validation loss 0.051612697541713715 Accuracy 0.8605000376701355\n",
      "Iteration 7080 Training loss 0.05197450518608093 Validation loss 0.05644742771983147 Accuracy 0.846500039100647\n",
      "Iteration 7090 Training loss 0.06143440678715706 Validation loss 0.062449291348457336 Accuracy 0.8288750648498535\n",
      "Iteration 7100 Training loss 0.04960602521896362 Validation loss 0.05382765457034111 Accuracy 0.8513750433921814\n",
      "Iteration 7110 Training loss 0.048336420208215714 Validation loss 0.05415109172463417 Accuracy 0.8565000295639038\n",
      "Iteration 7120 Training loss 0.05096272751688957 Validation loss 0.05261548236012459 Accuracy 0.858500063419342\n",
      "Iteration 7130 Training loss 0.07167278975248337 Validation loss 0.0548512227833271 Accuracy 0.8476250171661377\n",
      "Iteration 7140 Training loss 0.04833516478538513 Validation loss 0.052198626101017 Accuracy 0.8571250438690186\n",
      "Iteration 7150 Training loss 0.051135677844285965 Validation loss 0.05158828943967819 Accuracy 0.8602500557899475\n",
      "Iteration 7160 Training loss 0.0526471883058548 Validation loss 0.05470097437500954 Accuracy 0.8530000448226929\n",
      "Iteration 7170 Training loss 0.057325076311826706 Validation loss 0.05739501491189003 Accuracy 0.8458750247955322\n",
      "Iteration 7180 Training loss 0.048789169639348984 Validation loss 0.05351083725690842 Accuracy 0.8565000295639038\n",
      "Iteration 7190 Training loss 0.05758592113852501 Validation loss 0.061217013746500015 Accuracy 0.8342500329017639\n",
      "Iteration 7200 Training loss 0.04616423323750496 Validation loss 0.05206124484539032 Accuracy 0.8597500324249268\n",
      "Iteration 7210 Training loss 0.04770287498831749 Validation loss 0.05379289388656616 Accuracy 0.8530000448226929\n",
      "Iteration 7220 Training loss 0.04688937962055206 Validation loss 0.05392805114388466 Accuracy 0.8521250486373901\n",
      "Iteration 7230 Training loss 0.05779990181326866 Validation loss 0.05230693891644478 Accuracy 0.858875036239624\n",
      "Iteration 7240 Training loss 0.0586504265666008 Validation loss 0.06088997423648834 Accuracy 0.8348750472068787\n",
      "Iteration 7250 Training loss 0.050221432000398636 Validation loss 0.05820724740624428 Accuracy 0.8423750400543213\n",
      "Iteration 7260 Training loss 0.052644889801740646 Validation loss 0.05173485726118088 Accuracy 0.8592500686645508\n",
      "Iteration 7270 Training loss 0.05469583347439766 Validation loss 0.053545404225587845 Accuracy 0.8538750410079956\n",
      "Iteration 7280 Training loss 0.051990117877721786 Validation loss 0.052694130688905716 Accuracy 0.8597500324249268\n",
      "Iteration 7290 Training loss 0.052952494472265244 Validation loss 0.05150914937257767 Accuracy 0.8607500195503235\n",
      "Iteration 7300 Training loss 0.05505722761154175 Validation loss 0.05434615537524223 Accuracy 0.8548750281333923\n",
      "Iteration 7310 Training loss 0.04000511020421982 Validation loss 0.05130939185619354 Accuracy 0.861875057220459\n",
      "Iteration 7320 Training loss 0.04046379774808884 Validation loss 0.05196862295269966 Accuracy 0.8606250286102295\n",
      "Iteration 7330 Training loss 0.05293884500861168 Validation loss 0.0541565865278244 Accuracy 0.8558750152587891\n",
      "Iteration 7340 Training loss 0.04998749494552612 Validation loss 0.056705087423324585 Accuracy 0.8442500233650208\n",
      "Iteration 7350 Training loss 0.04825511947274208 Validation loss 0.05358133837580681 Accuracy 0.8523750305175781\n",
      "Iteration 7360 Training loss 0.04837929457426071 Validation loss 0.05268003046512604 Accuracy 0.8565000295639038\n",
      "Iteration 7370 Training loss 0.05129792541265488 Validation loss 0.05272676423192024 Accuracy 0.858625054359436\n",
      "Iteration 7380 Training loss 0.051151376217603683 Validation loss 0.05621426925063133 Accuracy 0.8482500314712524\n",
      "Iteration 7390 Training loss 0.06381134688854218 Validation loss 0.05664762482047081 Accuracy 0.8442500233650208\n",
      "Iteration 7400 Training loss 0.04549696296453476 Validation loss 0.05511197820305824 Accuracy 0.8473750352859497\n",
      "Iteration 7410 Training loss 0.04741188883781433 Validation loss 0.0532868392765522 Accuracy 0.8576250672340393\n",
      "Iteration 7420 Training loss 0.0519435778260231 Validation loss 0.05193705856800079 Accuracy 0.8615000247955322\n",
      "Iteration 7430 Training loss 0.05888262018561363 Validation loss 0.058265428990125656 Accuracy 0.8392500281333923\n",
      "Iteration 7440 Training loss 0.05812489241361618 Validation loss 0.05594777315855026 Accuracy 0.8476250171661377\n",
      "Iteration 7450 Training loss 0.04560409486293793 Validation loss 0.05129021778702736 Accuracy 0.8608750700950623\n",
      "Iteration 7460 Training loss 0.04802434518933296 Validation loss 0.052471060305833817 Accuracy 0.8560000658035278\n",
      "Iteration 7470 Training loss 0.05209630727767944 Validation loss 0.051380619406700134 Accuracy 0.8606250286102295\n",
      "Iteration 7480 Training loss 0.04722493141889572 Validation loss 0.05349695309996605 Accuracy 0.8577500581741333\n",
      "Iteration 7490 Training loss 0.039095595479011536 Validation loss 0.051473964005708694 Accuracy 0.8607500195503235\n",
      "Iteration 7500 Training loss 0.048921842128038406 Validation loss 0.05114535242319107 Accuracy 0.861875057220459\n",
      "Iteration 7510 Training loss 0.05646945536136627 Validation loss 0.05402511730790138 Accuracy 0.8505000472068787\n",
      "Iteration 7520 Training loss 0.04674100875854492 Validation loss 0.051245640963315964 Accuracy 0.8610000610351562\n",
      "Iteration 7530 Training loss 0.05470779165625572 Validation loss 0.0547177791595459 Accuracy 0.8536250591278076\n",
      "Iteration 7540 Training loss 0.054168786853551865 Validation loss 0.05150464177131653 Accuracy 0.8607500195503235\n",
      "Iteration 7550 Training loss 0.05117594450712204 Validation loss 0.051642417907714844 Accuracy 0.8583750128746033\n",
      "Iteration 7560 Training loss 0.0516340471804142 Validation loss 0.055164437741041183 Accuracy 0.8523750305175781\n",
      "Iteration 7570 Training loss 0.04264130815863609 Validation loss 0.0510820634663105 Accuracy 0.8603750467300415\n",
      "Iteration 7580 Training loss 0.04234220087528229 Validation loss 0.051714736968278885 Accuracy 0.8612500429153442\n",
      "Iteration 7590 Training loss 0.040855906903743744 Validation loss 0.05108832195401192 Accuracy 0.8616250157356262\n",
      "Iteration 7600 Training loss 0.06904232501983643 Validation loss 0.06422127038240433 Accuracy 0.8252500295639038\n",
      "Iteration 7610 Training loss 0.046231936663389206 Validation loss 0.05120640993118286 Accuracy 0.8608750700950623\n",
      "Iteration 7620 Training loss 0.04979630187153816 Validation loss 0.05158381536602974 Accuracy 0.8592500686645508\n",
      "Iteration 7630 Training loss 0.04309963062405586 Validation loss 0.051027875393629074 Accuracy 0.862250030040741\n",
      "Iteration 7640 Training loss 0.046251848340034485 Validation loss 0.050958726555109024 Accuracy 0.8615000247955322\n",
      "Iteration 7650 Training loss 0.05881902575492859 Validation loss 0.05633606016635895 Accuracy 0.8478750586509705\n",
      "Iteration 7660 Training loss 0.05028287693858147 Validation loss 0.051455192267894745 Accuracy 0.8602500557899475\n",
      "Iteration 7670 Training loss 0.053612422198057175 Validation loss 0.051299843937158585 Accuracy 0.8603750467300415\n",
      "Iteration 7680 Training loss 0.0484207347035408 Validation loss 0.05081524699926376 Accuracy 0.8626250624656677\n",
      "Iteration 7690 Training loss 0.05467510595917702 Validation loss 0.050952401012182236 Accuracy 0.862000048160553\n",
      "Iteration 7700 Training loss 0.06589195132255554 Validation loss 0.0639519914984703 Accuracy 0.8263750672340393\n",
      "Iteration 7710 Training loss 0.04020405188202858 Validation loss 0.05219510197639465 Accuracy 0.8577500581741333\n",
      "Iteration 7720 Training loss 0.05331886559724808 Validation loss 0.056626565754413605 Accuracy 0.843375027179718\n",
      "Iteration 7730 Training loss 0.053427066653966904 Validation loss 0.05132301524281502 Accuracy 0.8608750700950623\n",
      "Iteration 7740 Training loss 0.055050481110811234 Validation loss 0.05096854642033577 Accuracy 0.862500011920929\n",
      "Iteration 7750 Training loss 0.04592004045844078 Validation loss 0.052208803594112396 Accuracy 0.8572500348091125\n",
      "Iteration 7760 Training loss 0.051746949553489685 Validation loss 0.051211338490247726 Accuracy 0.861875057220459\n",
      "Iteration 7770 Training loss 0.05972517654299736 Validation loss 0.05556078627705574 Accuracy 0.8510000109672546\n",
      "Iteration 7780 Training loss 0.05514952540397644 Validation loss 0.05882618576288223 Accuracy 0.8398750424385071\n",
      "Iteration 7790 Training loss 0.043510571122169495 Validation loss 0.053395457565784454 Accuracy 0.8561250567436218\n",
      "Iteration 7800 Training loss 0.05286766588687897 Validation loss 0.051528267562389374 Accuracy 0.8603750467300415\n",
      "Iteration 7810 Training loss 0.05007876455783844 Validation loss 0.052117303013801575 Accuracy 0.8571250438690186\n",
      "Iteration 7820 Training loss 0.040475767105817795 Validation loss 0.051334407180547714 Accuracy 0.8603750467300415\n",
      "Iteration 7830 Training loss 0.04405815899372101 Validation loss 0.05098900571465492 Accuracy 0.862500011920929\n",
      "Iteration 7840 Training loss 0.04941897466778755 Validation loss 0.053071703761816025 Accuracy 0.8580000400543213\n",
      "Iteration 7850 Training loss 0.05222379416227341 Validation loss 0.05058899149298668 Accuracy 0.8642500638961792\n",
      "Iteration 7860 Training loss 0.04914907366037369 Validation loss 0.05128542706370354 Accuracy 0.8607500195503235\n",
      "Iteration 7870 Training loss 0.0442127026617527 Validation loss 0.052053485065698624 Accuracy 0.8608750700950623\n",
      "Iteration 7880 Training loss 0.043981797993183136 Validation loss 0.05131848528981209 Accuracy 0.8601250648498535\n",
      "Iteration 7890 Training loss 0.041210874915122986 Validation loss 0.05298878625035286 Accuracy 0.8537500500679016\n",
      "Iteration 7900 Training loss 0.039714422076940536 Validation loss 0.05076858028769493 Accuracy 0.8635000586509705\n",
      "Iteration 7910 Training loss 0.049346376210451126 Validation loss 0.05288787931203842 Accuracy 0.8578750491142273\n",
      "Iteration 7920 Training loss 0.04929874464869499 Validation loss 0.05350050330162048 Accuracy 0.8567500114440918\n",
      "Iteration 7930 Training loss 0.047174666076898575 Validation loss 0.05067449435591698 Accuracy 0.8635000586509705\n",
      "Iteration 7940 Training loss 0.04971159249544144 Validation loss 0.05079227313399315 Accuracy 0.8630000352859497\n",
      "Iteration 7950 Training loss 0.05305630713701248 Validation loss 0.05587127059698105 Accuracy 0.8498750329017639\n",
      "Iteration 7960 Training loss 0.04007963091135025 Validation loss 0.051939185708761215 Accuracy 0.8613750338554382\n",
      "Iteration 7970 Training loss 0.048818040639162064 Validation loss 0.051231499761343 Accuracy 0.8627500534057617\n",
      "Iteration 7980 Training loss 0.049834270030260086 Validation loss 0.05816396698355675 Accuracy 0.8392500281333923\n",
      "Iteration 7990 Training loss 0.04927555471658707 Validation loss 0.051658760756254196 Accuracy 0.8571250438690186\n",
      "Iteration 8000 Training loss 0.05141743645071983 Validation loss 0.05066072195768356 Accuracy 0.8636250495910645\n",
      "Iteration 8010 Training loss 0.044127002358436584 Validation loss 0.05178811028599739 Accuracy 0.859000027179718\n",
      "Iteration 8020 Training loss 0.04010201245546341 Validation loss 0.052142899483442307 Accuracy 0.8576250672340393\n",
      "Iteration 8030 Training loss 0.045088861137628555 Validation loss 0.05063318833708763 Accuracy 0.862125039100647\n",
      "Iteration 8040 Training loss 0.04319654405117035 Validation loss 0.050419460982084274 Accuracy 0.8628750443458557\n",
      "Iteration 8050 Training loss 0.050345078110694885 Validation loss 0.051761042326688766 Accuracy 0.8571250438690186\n",
      "Iteration 8060 Training loss 0.059778813272714615 Validation loss 0.057552535086870193 Accuracy 0.8412500619888306\n",
      "Iteration 8070 Training loss 0.05320334434509277 Validation loss 0.0544404610991478 Accuracy 0.8501250147819519\n",
      "Iteration 8080 Training loss 0.040436454117298126 Validation loss 0.051053546369075775 Accuracy 0.8610000610351562\n",
      "Iteration 8090 Training loss 0.05737852305173874 Validation loss 0.056954625993967056 Accuracy 0.8455000519752502\n",
      "Iteration 8100 Training loss 0.05760890245437622 Validation loss 0.05031844601035118 Accuracy 0.8648750185966492\n",
      "Iteration 8110 Training loss 0.04280318692326546 Validation loss 0.05050858110189438 Accuracy 0.8652500510215759\n",
      "Iteration 8120 Training loss 0.04098159819841385 Validation loss 0.051734041422605515 Accuracy 0.862500011920929\n",
      "Iteration 8130 Training loss 0.050506964325904846 Validation loss 0.052371133118867874 Accuracy 0.8612500429153442\n",
      "Iteration 8140 Training loss 0.06087106093764305 Validation loss 0.052231088280677795 Accuracy 0.8552500605583191\n",
      "Iteration 8150 Training loss 0.05875677615404129 Validation loss 0.056115515530109406 Accuracy 0.8493750691413879\n",
      "Iteration 8160 Training loss 0.049363989382982254 Validation loss 0.05118335783481598 Accuracy 0.8606250286102295\n",
      "Iteration 8170 Training loss 0.04389595985412598 Validation loss 0.051004767417907715 Accuracy 0.8635000586509705\n",
      "Iteration 8180 Training loss 0.04937293007969856 Validation loss 0.050193775445222855 Accuracy 0.8652500510215759\n",
      "Iteration 8190 Training loss 0.04475212097167969 Validation loss 0.05041416361927986 Accuracy 0.862125039100647\n",
      "Iteration 8200 Training loss 0.06041807681322098 Validation loss 0.06293094158172607 Accuracy 0.8258750438690186\n",
      "Iteration 8210 Training loss 0.04972996562719345 Validation loss 0.052887555211782455 Accuracy 0.859125018119812\n",
      "Iteration 8220 Training loss 0.05248119682073593 Validation loss 0.051496002823114395 Accuracy 0.8582500219345093\n",
      "Iteration 8230 Training loss 0.05234813690185547 Validation loss 0.050341393798589706 Accuracy 0.8645000457763672\n",
      "Iteration 8240 Training loss 0.046647027134895325 Validation loss 0.05025969818234444 Accuracy 0.862125039100647\n",
      "Iteration 8250 Training loss 0.05883101001381874 Validation loss 0.05697266757488251 Accuracy 0.8412500619888306\n",
      "Iteration 8260 Training loss 0.04220346733927727 Validation loss 0.05298348516225815 Accuracy 0.8547500371932983\n",
      "Iteration 8270 Training loss 0.04546193405985832 Validation loss 0.05067196115851402 Accuracy 0.8657500147819519\n",
      "Iteration 8280 Training loss 0.039509329944849014 Validation loss 0.05045266076922417 Accuracy 0.8611250519752502\n",
      "Iteration 8290 Training loss 0.03883574530482292 Validation loss 0.050237465649843216 Accuracy 0.8636250495910645\n",
      "Iteration 8300 Training loss 0.07140562683343887 Validation loss 0.06360001116991043 Accuracy 0.8243750333786011\n",
      "Iteration 8310 Training loss 0.04801582545042038 Validation loss 0.05021091550588608 Accuracy 0.8642500638961792\n",
      "Iteration 8320 Training loss 0.046491339802742004 Validation loss 0.05699664726853371 Accuracy 0.8470000624656677\n",
      "Iteration 8330 Training loss 0.05310444533824921 Validation loss 0.056640625 Accuracy 0.8475000262260437\n",
      "Iteration 8340 Training loss 0.0717049315571785 Validation loss 0.0632975623011589 Accuracy 0.8245000243186951\n",
      "Iteration 8350 Training loss 0.056908607482910156 Validation loss 0.0602569617331028 Accuracy 0.8330000638961792\n",
      "Iteration 8360 Training loss 0.042827997356653214 Validation loss 0.05095231160521507 Accuracy 0.8598750233650208\n",
      "Iteration 8370 Training loss 0.03963140770792961 Validation loss 0.05036609247326851 Accuracy 0.8655000329017639\n",
      "Iteration 8380 Training loss 0.05815699324011803 Validation loss 0.050222814083099365 Accuracy 0.8626250624656677\n",
      "Iteration 8390 Training loss 0.04897480085492134 Validation loss 0.05016040429472923 Accuracy 0.8655000329017639\n",
      "Iteration 8400 Training loss 0.0479581281542778 Validation loss 0.052154671400785446 Accuracy 0.8571250438690186\n",
      "Iteration 8410 Training loss 0.042057104408741 Validation loss 0.049957338720560074 Accuracy 0.8667500615119934\n",
      "Iteration 8420 Training loss 0.04380447790026665 Validation loss 0.051203999668359756 Accuracy 0.8636250495910645\n",
      "Iteration 8430 Training loss 0.06356944143772125 Validation loss 0.060601625591516495 Accuracy 0.8350000381469727\n",
      "Iteration 8440 Training loss 0.04740628972649574 Validation loss 0.05060279369354248 Accuracy 0.8636250495910645\n",
      "Iteration 8450 Training loss 0.05166777968406677 Validation loss 0.057226743549108505 Accuracy 0.8448750376701355\n",
      "Iteration 8460 Training loss 0.04674961045384407 Validation loss 0.05132662504911423 Accuracy 0.859125018119812\n",
      "Iteration 8470 Training loss 0.05336933583021164 Validation loss 0.05420627444982529 Accuracy 0.8508750200271606\n",
      "Iteration 8480 Training loss 0.0436285100877285 Validation loss 0.050375062972307205 Accuracy 0.8643750548362732\n",
      "Iteration 8490 Training loss 0.04145125672221184 Validation loss 0.04999246075749397 Accuracy 0.8637500405311584\n",
      "Iteration 8500 Training loss 0.051517512649297714 Validation loss 0.052245065569877625 Accuracy 0.8571250438690186\n",
      "Iteration 8510 Training loss 0.04680421203374863 Validation loss 0.05257158726453781 Accuracy 0.8562500476837158\n",
      "Iteration 8520 Training loss 0.06078055873513222 Validation loss 0.06285630166530609 Accuracy 0.8300000429153442\n",
      "Iteration 8530 Training loss 0.04309337958693504 Validation loss 0.050209857523441315 Accuracy 0.8641250133514404\n",
      "Iteration 8540 Training loss 0.04470653086900711 Validation loss 0.05019122734665871 Accuracy 0.8642500638961792\n",
      "Iteration 8550 Training loss 0.05637441575527191 Validation loss 0.050486426800489426 Accuracy 0.8653750419616699\n",
      "Iteration 8560 Training loss 0.05286280810832977 Validation loss 0.05632501095533371 Accuracy 0.8471250534057617\n",
      "Iteration 8570 Training loss 0.043147992342710495 Validation loss 0.052188728004693985 Accuracy 0.8602500557899475\n",
      "Iteration 8580 Training loss 0.03860004246234894 Validation loss 0.05045130103826523 Accuracy 0.8658750653266907\n",
      "Iteration 8590 Training loss 0.04538429155945778 Validation loss 0.05621481314301491 Accuracy 0.8452500104904175\n",
      "Iteration 8600 Training loss 0.040739506483078 Validation loss 0.050073955208063126 Accuracy 0.8626250624656677\n",
      "Iteration 8610 Training loss 0.06238305941224098 Validation loss 0.05638206750154495 Accuracy 0.8453750610351562\n",
      "Iteration 8620 Training loss 0.04381437972187996 Validation loss 0.05073212459683418 Accuracy 0.8608750700950623\n",
      "Iteration 8630 Training loss 0.03768735006451607 Validation loss 0.05116063356399536 Accuracy 0.858625054359436\n",
      "Iteration 8640 Training loss 0.05373428389430046 Validation loss 0.052237216383218765 Accuracy 0.85875004529953\n",
      "Iteration 8650 Training loss 0.05839155241847038 Validation loss 0.055810458958148956 Accuracy 0.8451250195503235\n",
      "Iteration 8660 Training loss 0.04980073869228363 Validation loss 0.04977436736226082 Accuracy 0.8657500147819519\n",
      "Iteration 8670 Training loss 0.046764325350522995 Validation loss 0.05241890996694565 Accuracy 0.8556250333786011\n",
      "Iteration 8680 Training loss 0.047625165432691574 Validation loss 0.049923308193683624 Accuracy 0.8642500638961792\n",
      "Iteration 8690 Training loss 0.04576367884874344 Validation loss 0.05341997742652893 Accuracy 0.8523750305175781\n",
      "Iteration 8700 Training loss 0.05325351655483246 Validation loss 0.05030044540762901 Accuracy 0.862375020980835\n",
      "Iteration 8710 Training loss 0.047799523919820786 Validation loss 0.0542941614985466 Accuracy 0.8505000472068787\n",
      "Iteration 8720 Training loss 0.04401616007089615 Validation loss 0.049975261092185974 Accuracy 0.8640000224113464\n",
      "Iteration 8730 Training loss 0.041158199310302734 Validation loss 0.04967654496431351 Accuracy 0.8661250472068787\n",
      "Iteration 8740 Training loss 0.04627117142081261 Validation loss 0.05276219919323921 Accuracy 0.858500063419342\n",
      "Iteration 8750 Training loss 0.042077742516994476 Validation loss 0.0495561920106411 Accuracy 0.8656250238418579\n",
      "Iteration 8760 Training loss 0.04388326033949852 Validation loss 0.05242312699556351 Accuracy 0.8550000190734863\n",
      "Iteration 8770 Training loss 0.044175777584314346 Validation loss 0.049695394933223724 Accuracy 0.8640000224113464\n",
      "Iteration 8780 Training loss 0.04989529401063919 Validation loss 0.050384435802698135 Accuracy 0.8668750524520874\n",
      "Iteration 8790 Training loss 0.046604108065366745 Validation loss 0.049359921365976334 Accuracy 0.8645000457763672\n",
      "Iteration 8800 Training loss 0.047120027244091034 Validation loss 0.049606963992118835 Accuracy 0.8632500171661377\n",
      "Iteration 8810 Training loss 0.049288559705019 Validation loss 0.052380092442035675 Accuracy 0.8603750467300415\n",
      "Iteration 8820 Training loss 0.04685858637094498 Validation loss 0.050996068865060806 Accuracy 0.8633750677108765\n",
      "Iteration 8830 Training loss 0.04787258058786392 Validation loss 0.05127475783228874 Accuracy 0.8592500686645508\n",
      "Iteration 8840 Training loss 0.04287813976407051 Validation loss 0.049662210047245026 Accuracy 0.8670000433921814\n",
      "Iteration 8850 Training loss 0.051015567034482956 Validation loss 0.049547020345926285 Accuracy 0.8657500147819519\n",
      "Iteration 8860 Training loss 0.05328463762998581 Validation loss 0.054838716983795166 Accuracy 0.846625030040741\n",
      "Iteration 8870 Training loss 0.04309971630573273 Validation loss 0.05080706998705864 Accuracy 0.8640000224113464\n",
      "Iteration 8880 Training loss 0.04585736244916916 Validation loss 0.0530678853392601 Accuracy 0.8570000529289246\n",
      "Iteration 8890 Training loss 0.05146624147891998 Validation loss 0.049858614802360535 Accuracy 0.8660000562667847\n",
      "Iteration 8900 Training loss 0.04042806103825569 Validation loss 0.05007120594382286 Accuracy 0.8630000352859497\n",
      "Iteration 8910 Training loss 0.04936642572283745 Validation loss 0.05187389254570007 Accuracy 0.8615000247955322\n",
      "Iteration 8920 Training loss 0.05423135682940483 Validation loss 0.04943979158997536 Accuracy 0.8672500252723694\n",
      "Iteration 8930 Training loss 0.05207747220993042 Validation loss 0.0527261421084404 Accuracy 0.8546250462532043\n",
      "Iteration 8940 Training loss 0.03785742446780205 Validation loss 0.05128074809908867 Accuracy 0.859000027179718\n",
      "Iteration 8950 Training loss 0.049236346036195755 Validation loss 0.05157562717795372 Accuracy 0.8610000610351562\n",
      "Iteration 8960 Training loss 0.04990075156092644 Validation loss 0.04982313886284828 Accuracy 0.8632500171661377\n",
      "Iteration 8970 Training loss 0.05143037810921669 Validation loss 0.05440104007720947 Accuracy 0.8495000600814819\n",
      "Iteration 8980 Training loss 0.05481680855154991 Validation loss 0.059212375432252884 Accuracy 0.8388750553131104\n",
      "Iteration 8990 Training loss 0.056302472949028015 Validation loss 0.05178583785891533 Accuracy 0.8626250624656677\n",
      "Iteration 9000 Training loss 0.03953448310494423 Validation loss 0.0494731143116951 Accuracy 0.8673750162124634\n",
      "Iteration 9010 Training loss 0.04071834310889244 Validation loss 0.052279744297266006 Accuracy 0.8565000295639038\n",
      "Iteration 9020 Training loss 0.043827131390571594 Validation loss 0.05081701651215553 Accuracy 0.8651250600814819\n",
      "Iteration 9030 Training loss 0.03818543255329132 Validation loss 0.04922816902399063 Accuracy 0.8682500123977661\n",
      "Iteration 9040 Training loss 0.04495953395962715 Validation loss 0.049364082515239716 Accuracy 0.8693750500679016\n",
      "Iteration 9050 Training loss 0.041233282536268234 Validation loss 0.04911224916577339 Accuracy 0.8678750395774841\n",
      "Iteration 9060 Training loss 0.038130249828100204 Validation loss 0.05174989625811577 Accuracy 0.8608750700950623\n",
      "Iteration 9070 Training loss 0.05405024439096451 Validation loss 0.05130942165851593 Accuracy 0.8628750443458557\n",
      "Iteration 9080 Training loss 0.05005783587694168 Validation loss 0.05280076339840889 Accuracy 0.8532500267028809\n",
      "Iteration 9090 Training loss 0.0585126057267189 Validation loss 0.055868782103061676 Accuracy 0.8455000519752502\n",
      "Iteration 9100 Training loss 0.038179364055395126 Validation loss 0.04923120513558388 Accuracy 0.8673750162124634\n",
      "Iteration 9110 Training loss 0.04591749608516693 Validation loss 0.05057486146688461 Accuracy 0.8613750338554382\n",
      "Iteration 9120 Training loss 0.046343110501766205 Validation loss 0.05106162279844284 Accuracy 0.8637500405311584\n",
      "Iteration 9130 Training loss 0.05430453270673752 Validation loss 0.04922018200159073 Accuracy 0.8691250681877136\n",
      "Iteration 9140 Training loss 0.04655545577406883 Validation loss 0.05698298662900925 Accuracy 0.8423750400543213\n",
      "Iteration 9150 Training loss 0.04036320745944977 Validation loss 0.0506361648440361 Accuracy 0.8615000247955322\n",
      "Iteration 9160 Training loss 0.05323271080851555 Validation loss 0.052137382328510284 Accuracy 0.8573750257492065\n",
      "Iteration 9170 Training loss 0.0508853942155838 Validation loss 0.05025220289826393 Accuracy 0.8675000667572021\n",
      "Iteration 9180 Training loss 0.04881338030099869 Validation loss 0.05398917198181152 Accuracy 0.8542500138282776\n",
      "Iteration 9190 Training loss 0.041766345500946045 Validation loss 0.04891110211610794 Accuracy 0.8698750138282776\n",
      "Iteration 9200 Training loss 0.046372946351766586 Validation loss 0.04897308722138405 Accuracy 0.8705000281333923\n",
      "Iteration 9210 Training loss 0.04438764601945877 Validation loss 0.04967469349503517 Accuracy 0.8632500171661377\n",
      "Iteration 9220 Training loss 0.053281497210264206 Validation loss 0.05116209760308266 Accuracy 0.859125018119812\n",
      "Iteration 9230 Training loss 0.05944204330444336 Validation loss 0.060259439051151276 Accuracy 0.8330000638961792\n",
      "Iteration 9240 Training loss 0.046743955463171005 Validation loss 0.05364910513162613 Accuracy 0.8556250333786011\n",
      "Iteration 9250 Training loss 0.05670903995633125 Validation loss 0.053298380225896835 Accuracy 0.8572500348091125\n",
      "Iteration 9260 Training loss 0.04830562323331833 Validation loss 0.04927565157413483 Accuracy 0.8647500276565552\n",
      "Iteration 9270 Training loss 0.040324561297893524 Validation loss 0.049390602856874466 Accuracy 0.8688750267028809\n",
      "Iteration 9280 Training loss 0.04887792468070984 Validation loss 0.048857592046260834 Accuracy 0.8667500615119934\n",
      "Iteration 9290 Training loss 0.04633302241563797 Validation loss 0.04992564022541046 Accuracy 0.8685000538825989\n",
      "Iteration 9300 Training loss 0.04322967305779457 Validation loss 0.050112634897232056 Accuracy 0.8606250286102295\n",
      "Iteration 9310 Training loss 0.05696472153067589 Validation loss 0.057600636035203934 Accuracy 0.84312504529953\n",
      "Iteration 9320 Training loss 0.05060599371790886 Validation loss 0.05159797891974449 Accuracy 0.8600000143051147\n",
      "Iteration 9330 Training loss 0.0446772463619709 Validation loss 0.04914894327521324 Accuracy 0.8697500228881836\n",
      "Iteration 9340 Training loss 0.04322997108101845 Validation loss 0.050039149820804596 Accuracy 0.861750066280365\n",
      "Iteration 9350 Training loss 0.0403355248272419 Validation loss 0.04898246005177498 Accuracy 0.8680000305175781\n",
      "Iteration 9360 Training loss 0.0406331941485405 Validation loss 0.049995556473731995 Accuracy 0.8662500381469727\n",
      "Iteration 9370 Training loss 0.04641248658299446 Validation loss 0.05034118890762329 Accuracy 0.8658750653266907\n",
      "Iteration 9380 Training loss 0.046511948108673096 Validation loss 0.0492989681661129 Accuracy 0.8680000305175781\n",
      "Iteration 9390 Training loss 0.04350088909268379 Validation loss 0.04902936890721321 Accuracy 0.8682500123977661\n",
      "Iteration 9400 Training loss 0.0488937683403492 Validation loss 0.04899783059954643 Accuracy 0.8672500252723694\n",
      "Iteration 9410 Training loss 0.047324199229478836 Validation loss 0.04883190989494324 Accuracy 0.8693750500679016\n",
      "Iteration 9420 Training loss 0.043958716094493866 Validation loss 0.049128029495477676 Accuracy 0.8665000200271606\n",
      "Iteration 9430 Training loss 0.047427885234355927 Validation loss 0.05024450272321701 Accuracy 0.861750066280365\n",
      "Iteration 9440 Training loss 0.038618460297584534 Validation loss 0.04971810430288315 Accuracy 0.8635000586509705\n",
      "Iteration 9450 Training loss 0.04599742218852043 Validation loss 0.050159476697444916 Accuracy 0.862000048160553\n",
      "Iteration 9460 Training loss 0.04109485074877739 Validation loss 0.04936249554157257 Accuracy 0.8661250472068787\n",
      "Iteration 9470 Training loss 0.043836697936058044 Validation loss 0.05337255448102951 Accuracy 0.8521250486373901\n",
      "Iteration 9480 Training loss 0.043914299458265305 Validation loss 0.053877342492341995 Accuracy 0.8516250252723694\n",
      "Iteration 9490 Training loss 0.03795503452420235 Validation loss 0.04988910257816315 Accuracy 0.862125039100647\n",
      "Iteration 9500 Training loss 0.05237341672182083 Validation loss 0.05063902586698532 Accuracy 0.8601250648498535\n",
      "Iteration 9510 Training loss 0.039562489837408066 Validation loss 0.05481864511966705 Accuracy 0.8492500185966492\n",
      "Iteration 9520 Training loss 0.040115468204021454 Validation loss 0.048982810229063034 Accuracy 0.8681250214576721\n",
      "Iteration 9530 Training loss 0.046158354729413986 Validation loss 0.049762576818466187 Accuracy 0.8685000538825989\n",
      "Iteration 9540 Training loss 0.04166402667760849 Validation loss 0.05018464848399162 Accuracy 0.8633750677108765\n",
      "Iteration 9550 Training loss 0.043220404535532 Validation loss 0.05361435189843178 Accuracy 0.8563750386238098\n",
      "Iteration 9560 Training loss 0.04469582065939903 Validation loss 0.048824649304151535 Accuracy 0.8697500228881836\n",
      "Iteration 9570 Training loss 0.03332995995879173 Validation loss 0.0496341735124588 Accuracy 0.8633750677108765\n",
      "Iteration 9580 Training loss 0.05758362635970116 Validation loss 0.05316014587879181 Accuracy 0.8555000424385071\n",
      "Iteration 9590 Training loss 0.04116441681981087 Validation loss 0.048821672797203064 Accuracy 0.8712500333786011\n",
      "Iteration 9600 Training loss 0.048170797526836395 Validation loss 0.04929865524172783 Accuracy 0.8631250262260437\n",
      "Iteration 9610 Training loss 0.047993674874305725 Validation loss 0.049725376069545746 Accuracy 0.8676250576972961\n",
      "Iteration 9620 Training loss 0.036208897829055786 Validation loss 0.0489104725420475 Accuracy 0.8671250343322754\n",
      "Iteration 9630 Training loss 0.04948386177420616 Validation loss 0.04963389039039612 Accuracy 0.8677500486373901\n",
      "Iteration 9640 Training loss 0.0436905100941658 Validation loss 0.04879232123494148 Accuracy 0.8690000176429749\n",
      "Iteration 9650 Training loss 0.06608487665653229 Validation loss 0.06577225029468536 Accuracy 0.8200000524520874\n",
      "Iteration 9660 Training loss 0.059462614357471466 Validation loss 0.055287592113018036 Accuracy 0.8502500653266907\n",
      "Iteration 9670 Training loss 0.050342246890068054 Validation loss 0.052992187440395355 Accuracy 0.8576250672340393\n",
      "Iteration 9680 Training loss 0.04027668014168739 Validation loss 0.04927397519350052 Accuracy 0.8692500591278076\n",
      "Iteration 9690 Training loss 0.0504239983856678 Validation loss 0.05731986463069916 Accuracy 0.843375027179718\n",
      "Iteration 9700 Training loss 0.04535685479640961 Validation loss 0.055840566754341125 Accuracy 0.8476250171661377\n",
      "Iteration 9710 Training loss 0.0481160432100296 Validation loss 0.04894672706723213 Accuracy 0.8701250553131104\n",
      "Iteration 9720 Training loss 0.03716488555073738 Validation loss 0.048663780093193054 Accuracy 0.8690000176429749\n",
      "Iteration 9730 Training loss 0.046775951981544495 Validation loss 0.050441913306713104 Accuracy 0.8672500252723694\n",
      "Iteration 9740 Training loss 0.04400017485022545 Validation loss 0.04907891899347305 Accuracy 0.8645000457763672\n",
      "Iteration 9750 Training loss 0.04236840084195137 Validation loss 0.04876215010881424 Accuracy 0.8667500615119934\n",
      "Iteration 9760 Training loss 0.04246313124895096 Validation loss 0.048320747911930084 Accuracy 0.8715000152587891\n",
      "Iteration 9770 Training loss 0.04433049261569977 Validation loss 0.05351557955145836 Accuracy 0.8545000553131104\n",
      "Iteration 9780 Training loss 0.05122748389840126 Validation loss 0.05053285136818886 Accuracy 0.8650000691413879\n",
      "Iteration 9790 Training loss 0.04471876099705696 Validation loss 0.05180727690458298 Accuracy 0.8607500195503235\n",
      "Iteration 9800 Training loss 0.03820383548736572 Validation loss 0.05039731785655022 Accuracy 0.8616250157356262\n",
      "Iteration 9810 Training loss 0.040446147322654724 Validation loss 0.050565239042043686 Accuracy 0.8612500429153442\n",
      "Iteration 9820 Training loss 0.04337847977876663 Validation loss 0.04850085824728012 Accuracy 0.8722500205039978\n",
      "Iteration 9830 Training loss 0.04830344021320343 Validation loss 0.054472342133522034 Accuracy 0.8515000343322754\n",
      "Iteration 9840 Training loss 0.04865718632936478 Validation loss 0.05174712836742401 Accuracy 0.8581250309944153\n",
      "Iteration 9850 Training loss 0.043998945504426956 Validation loss 0.0534052774310112 Accuracy 0.8522500395774841\n",
      "Iteration 9860 Training loss 0.05255845561623573 Validation loss 0.048543162643909454 Accuracy 0.8697500228881836\n",
      "Iteration 9870 Training loss 0.050812866538763046 Validation loss 0.048350777477025986 Accuracy 0.8707500696182251\n",
      "Iteration 9880 Training loss 0.046746425330638885 Validation loss 0.05044311657547951 Accuracy 0.8667500615119934\n",
      "Iteration 9890 Training loss 0.04443101957440376 Validation loss 0.05165962874889374 Accuracy 0.8606250286102295\n",
      "Iteration 9900 Training loss 0.05305396392941475 Validation loss 0.05706284940242767 Accuracy 0.8451250195503235\n",
      "Iteration 9910 Training loss 0.053159359842538834 Validation loss 0.05021747201681137 Accuracy 0.8670000433921814\n",
      "Iteration 9920 Training loss 0.04255654290318489 Validation loss 0.05157934129238129 Accuracy 0.8615000247955322\n",
      "Iteration 9930 Training loss 0.05592280253767967 Validation loss 0.05444859340786934 Accuracy 0.8518750667572021\n",
      "Iteration 9940 Training loss 0.031771209090948105 Validation loss 0.04823457449674606 Accuracy 0.8708750605583191\n",
      "Iteration 9950 Training loss 0.05222490057349205 Validation loss 0.05150390788912773 Accuracy 0.85875004529953\n",
      "Iteration 9960 Training loss 0.048284292221069336 Validation loss 0.04852275177836418 Accuracy 0.8691250681877136\n",
      "Iteration 9970 Training loss 0.04449206590652466 Validation loss 0.048090070486068726 Accuracy 0.8718750476837158\n",
      "Iteration 9980 Training loss 0.05248875543475151 Validation loss 0.04881659150123596 Accuracy 0.8707500696182251\n",
      "Iteration 9990 Training loss 0.0471298024058342 Validation loss 0.048168718814849854 Accuracy 0.8691250681877136\n",
      "Iteration 10000 Training loss 0.04893549904227257 Validation loss 0.054860129952430725 Accuracy 0.8513750433921814\n",
      "Iteration 10010 Training loss 0.039765432476997375 Validation loss 0.049045100808143616 Accuracy 0.8697500228881836\n",
      "Iteration 10020 Training loss 0.044790878891944885 Validation loss 0.04816659912467003 Accuracy 0.8712500333786011\n",
      "Iteration 10030 Training loss 0.03683336079120636 Validation loss 0.048226479440927505 Accuracy 0.8710000514984131\n",
      "Iteration 10040 Training loss 0.04262852668762207 Validation loss 0.052238620817661285 Accuracy 0.8566250205039978\n",
      "Iteration 10050 Training loss 0.045605458319187164 Validation loss 0.04799523949623108 Accuracy 0.8701250553131104\n",
      "Iteration 10060 Training loss 0.04594441130757332 Validation loss 0.04821852594614029 Accuracy 0.8683750629425049\n",
      "Iteration 10070 Training loss 0.05239998549222946 Validation loss 0.049574680626392365 Accuracy 0.8691250681877136\n",
      "Iteration 10080 Training loss 0.047241415828466415 Validation loss 0.048708148300647736 Accuracy 0.8700000643730164\n",
      "Iteration 10090 Training loss 0.042924538254737854 Validation loss 0.049826107919216156 Accuracy 0.8666250705718994\n",
      "Iteration 10100 Training loss 0.046869248151779175 Validation loss 0.05153132230043411 Accuracy 0.8601250648498535\n",
      "Iteration 10110 Training loss 0.045254386961460114 Validation loss 0.04897800087928772 Accuracy 0.8627500534057617\n",
      "Iteration 10120 Training loss 0.044572558254003525 Validation loss 0.05329421907663345 Accuracy 0.8563750386238098\n",
      "Iteration 10130 Training loss 0.04867726191878319 Validation loss 0.04899853095412254 Accuracy 0.8641250133514404\n",
      "Iteration 10140 Training loss 0.04618946835398674 Validation loss 0.04881782829761505 Accuracy 0.8708750605583191\n",
      "Iteration 10150 Training loss 0.04319979250431061 Validation loss 0.048253484070301056 Accuracy 0.8717500567436218\n",
      "Iteration 10160 Training loss 0.04129653051495552 Validation loss 0.04825606569647789 Accuracy 0.8670000433921814\n",
      "Iteration 10170 Training loss 0.04320532828569412 Validation loss 0.04859280586242676 Accuracy 0.8703750371932983\n",
      "Iteration 10180 Training loss 0.04712006449699402 Validation loss 0.05552888661623001 Accuracy 0.8511250615119934\n",
      "Iteration 10190 Training loss 0.03842122480273247 Validation loss 0.04884093627333641 Accuracy 0.8640000224113464\n",
      "Iteration 10200 Training loss 0.045363809913396835 Validation loss 0.05088699609041214 Accuracy 0.8597500324249268\n",
      "Iteration 10210 Training loss 0.049533456563949585 Validation loss 0.053894948214292526 Accuracy 0.8512500524520874\n",
      "Iteration 10220 Training loss 0.04730867221951485 Validation loss 0.0482916422188282 Accuracy 0.8696250319480896\n",
      "Iteration 10230 Training loss 0.05506141483783722 Validation loss 0.05376124382019043 Accuracy 0.8516250252723694\n",
      "Iteration 10240 Training loss 0.04123184084892273 Validation loss 0.04817519709467888 Accuracy 0.8691250681877136\n",
      "Iteration 10250 Training loss 0.0485399104654789 Validation loss 0.05578403174877167 Accuracy 0.8505000472068787\n",
      "Iteration 10260 Training loss 0.04544760659337044 Validation loss 0.050780344754457474 Accuracy 0.8596250414848328\n",
      "Iteration 10270 Training loss 0.052127670496702194 Validation loss 0.05232248827815056 Accuracy 0.8557500243186951\n",
      "Iteration 10280 Training loss 0.05444302037358284 Validation loss 0.053555358201265335 Accuracy 0.8550000190734863\n",
      "Iteration 10290 Training loss 0.04191594570875168 Validation loss 0.04842864349484444 Accuracy 0.8718750476837158\n",
      "Iteration 10300 Training loss 0.050612691789865494 Validation loss 0.04885021969676018 Accuracy 0.8655000329017639\n",
      "Iteration 10310 Training loss 0.047896161675453186 Validation loss 0.04830538481473923 Accuracy 0.8675000667572021\n",
      "Iteration 10320 Training loss 0.035506196320056915 Validation loss 0.04876106232404709 Accuracy 0.8657500147819519\n",
      "Iteration 10330 Training loss 0.04404861852526665 Validation loss 0.04985147714614868 Accuracy 0.862125039100647\n",
      "Iteration 10340 Training loss 0.04304361343383789 Validation loss 0.051273126155138016 Accuracy 0.8615000247955322\n",
      "Iteration 10350 Training loss 0.04040185362100601 Validation loss 0.047650355845689774 Accuracy 0.8723750710487366\n",
      "Iteration 10360 Training loss 0.046541254967451096 Validation loss 0.04860338196158409 Accuracy 0.8715000152587891\n",
      "Iteration 10370 Training loss 0.04663970693945885 Validation loss 0.05245140194892883 Accuracy 0.8565000295639038\n",
      "Iteration 10380 Training loss 0.04165072366595268 Validation loss 0.04962305352091789 Accuracy 0.8635000586509705\n",
      "Iteration 10390 Training loss 0.03954724594950676 Validation loss 0.049228236079216 Accuracy 0.8707500696182251\n",
      "Iteration 10400 Training loss 0.044007495045661926 Validation loss 0.049383364617824554 Accuracy 0.8695000410079956\n",
      "Iteration 10410 Training loss 0.042537227272987366 Validation loss 0.04762954264879227 Accuracy 0.8726250529289246\n",
      "Iteration 10420 Training loss 0.05436987802386284 Validation loss 0.06046832725405693 Accuracy 0.8373750448226929\n",
      "Iteration 10430 Training loss 0.054137904196977615 Validation loss 0.05699629336595535 Accuracy 0.8458750247955322\n",
      "Iteration 10440 Training loss 0.045197468250989914 Validation loss 0.05370417982339859 Accuracy 0.8543750643730164\n",
      "Iteration 10450 Training loss 0.05259953811764717 Validation loss 0.053241271525621414 Accuracy 0.8563750386238098\n",
      "Iteration 10460 Training loss 0.03668713942170143 Validation loss 0.04847327247262001 Accuracy 0.8677500486373901\n",
      "Iteration 10470 Training loss 0.045924533158540726 Validation loss 0.05442851409316063 Accuracy 0.8528750538825989\n",
      "Iteration 10480 Training loss 0.047864578664302826 Validation loss 0.04912694916129112 Accuracy 0.8696250319480896\n",
      "Iteration 10490 Training loss 0.035867124795913696 Validation loss 0.04803983494639397 Accuracy 0.8716250658035278\n",
      "Iteration 10500 Training loss 0.037552449852228165 Validation loss 0.04810134321451187 Accuracy 0.8678750395774841\n",
      "Iteration 10510 Training loss 0.0423087403178215 Validation loss 0.04785279929637909 Accuracy 0.8705000281333923\n",
      "Iteration 10520 Training loss 0.05548033490777016 Validation loss 0.054779354482889175 Accuracy 0.8495000600814819\n",
      "Iteration 10530 Training loss 0.052025992423295975 Validation loss 0.05084008350968361 Accuracy 0.8641250133514404\n",
      "Iteration 10540 Training loss 0.051562752574682236 Validation loss 0.05417174845933914 Accuracy 0.8507500290870667\n",
      "Iteration 10550 Training loss 0.04560622200369835 Validation loss 0.048263221979141235 Accuracy 0.8676250576972961\n",
      "Iteration 10560 Training loss 0.04765452444553375 Validation loss 0.04868703708052635 Accuracy 0.8661250472068787\n",
      "Iteration 10570 Training loss 0.0393993966281414 Validation loss 0.047700271010398865 Accuracy 0.8705000281333923\n",
      "Iteration 10580 Training loss 0.04226788505911827 Validation loss 0.049151644110679626 Accuracy 0.8716250658035278\n",
      "Iteration 10590 Training loss 0.04743919149041176 Validation loss 0.0479983314871788 Accuracy 0.8707500696182251\n",
      "Iteration 10600 Training loss 0.04013783857226372 Validation loss 0.04801094904541969 Accuracy 0.8686250448226929\n",
      "Iteration 10610 Training loss 0.04410259798169136 Validation loss 0.05188208073377609 Accuracy 0.8592500686645508\n",
      "Iteration 10620 Training loss 0.04688667505979538 Validation loss 0.052229754626750946 Accuracy 0.8576250672340393\n",
      "Iteration 10630 Training loss 0.050907909870147705 Validation loss 0.053939640522003174 Accuracy 0.8542500138282776\n",
      "Iteration 10640 Training loss 0.04124828428030014 Validation loss 0.047956690192222595 Accuracy 0.8701250553131104\n",
      "Iteration 10650 Training loss 0.046116575598716736 Validation loss 0.051206670701503754 Accuracy 0.862125039100647\n",
      "Iteration 10660 Training loss 0.037686824798583984 Validation loss 0.04786625877022743 Accuracy 0.8696250319480896\n",
      "Iteration 10670 Training loss 0.044759802520275116 Validation loss 0.04775354266166687 Accuracy 0.8718750476837158\n",
      "Iteration 10680 Training loss 0.05104933679103851 Validation loss 0.054280657321214676 Accuracy 0.8513750433921814\n",
      "Iteration 10690 Training loss 0.04692188277840614 Validation loss 0.04796559363603592 Accuracy 0.8697500228881836\n",
      "Iteration 10700 Training loss 0.047299984842538834 Validation loss 0.04807421192526817 Accuracy 0.8713750243186951\n",
      "Iteration 10710 Training loss 0.046595267951488495 Validation loss 0.04872458800673485 Accuracy 0.8660000562667847\n",
      "Iteration 10720 Training loss 0.05976057052612305 Validation loss 0.05012516304850578 Accuracy 0.8608750700950623\n",
      "Iteration 10730 Training loss 0.04447178915143013 Validation loss 0.048210032284259796 Accuracy 0.8706250190734863\n",
      "Iteration 10740 Training loss 0.044790830463171005 Validation loss 0.048255760222673416 Accuracy 0.8665000200271606\n",
      "Iteration 10750 Training loss 0.034257419407367706 Validation loss 0.04767888784408569 Accuracy 0.8695000410079956\n",
      "Iteration 10760 Training loss 0.038231246173381805 Validation loss 0.04769040271639824 Accuracy 0.8708750605583191\n",
      "Iteration 10770 Training loss 0.04687345027923584 Validation loss 0.052512697875499725 Accuracy 0.8595000505447388\n",
      "Iteration 10780 Training loss 0.042618293315172195 Validation loss 0.04753031209111214 Accuracy 0.8725000619888306\n",
      "Iteration 10790 Training loss 0.04786447435617447 Validation loss 0.047610919922590256 Accuracy 0.8726250529289246\n",
      "Iteration 10800 Training loss 0.039649542421102524 Validation loss 0.050357721745967865 Accuracy 0.8616250157356262\n",
      "Iteration 10810 Training loss 0.03982652351260185 Validation loss 0.049698606133461 Accuracy 0.8637500405311584\n",
      "Iteration 10820 Training loss 0.05067320540547371 Validation loss 0.04893853887915611 Accuracy 0.8706250190734863\n",
      "Iteration 10830 Training loss 0.05119987204670906 Validation loss 0.0532769113779068 Accuracy 0.8566250205039978\n",
      "Iteration 10840 Training loss 0.050036486238241196 Validation loss 0.04753740131855011 Accuracy 0.8712500333786011\n",
      "Iteration 10850 Training loss 0.03491329774260521 Validation loss 0.048569321632385254 Accuracy 0.8672500252723694\n",
      "Iteration 10860 Training loss 0.03750333935022354 Validation loss 0.04833222180604935 Accuracy 0.8713750243186951\n",
      "Iteration 10870 Training loss 0.0414409264922142 Validation loss 0.05122906714677811 Accuracy 0.8596250414848328\n",
      "Iteration 10880 Training loss 0.041815850883722305 Validation loss 0.04751782491803169 Accuracy 0.8716250658035278\n",
      "Iteration 10890 Training loss 0.05181043595075607 Validation loss 0.05014782026410103 Accuracy 0.8660000562667847\n",
      "Iteration 10900 Training loss 0.03882076218724251 Validation loss 0.04962211474776268 Accuracy 0.8630000352859497\n",
      "Iteration 10910 Training loss 0.043196387588977814 Validation loss 0.04744904860854149 Accuracy 0.8718750476837158\n",
      "Iteration 10920 Training loss 0.04522291570901871 Validation loss 0.05000626668334007 Accuracy 0.862500011920929\n",
      "Iteration 10930 Training loss 0.04990449175238609 Validation loss 0.06003303825855255 Accuracy 0.8348750472068787\n",
      "Iteration 10940 Training loss 0.04291922599077225 Validation loss 0.0502031110227108 Accuracy 0.8665000200271606\n",
      "Iteration 10950 Training loss 0.0326441191136837 Validation loss 0.047656502574682236 Accuracy 0.8728750348091125\n",
      "Iteration 10960 Training loss 0.04336327314376831 Validation loss 0.04889712855219841 Accuracy 0.8711250424385071\n",
      "Iteration 10970 Training loss 0.03893201798200607 Validation loss 0.0494731143116951 Accuracy 0.8686250448226929\n",
      "Iteration 10980 Training loss 0.043885231018066406 Validation loss 0.047196969389915466 Accuracy 0.8723750710487366\n",
      "Iteration 10990 Training loss 0.04568057879805565 Validation loss 0.04745623096823692 Accuracy 0.8715000152587891\n",
      "Iteration 11000 Training loss 0.04279797524213791 Validation loss 0.048867061734199524 Accuracy 0.8705000281333923\n",
      "Iteration 11010 Training loss 0.04238421469926834 Validation loss 0.04743046686053276 Accuracy 0.8713750243186951\n",
      "Iteration 11020 Training loss 0.03846891596913338 Validation loss 0.04918189346790314 Accuracy 0.8693750500679016\n",
      "Iteration 11030 Training loss 0.0360841266810894 Validation loss 0.04734516888856888 Accuracy 0.8723750710487366\n",
      "Iteration 11040 Training loss 0.0388314463198185 Validation loss 0.04782360419631004 Accuracy 0.8692500591278076\n",
      "Iteration 11050 Training loss 0.041493795812129974 Validation loss 0.048279207199811935 Accuracy 0.8677500486373901\n",
      "Iteration 11060 Training loss 0.04088287800550461 Validation loss 0.05201974883675575 Accuracy 0.8582500219345093\n",
      "Iteration 11070 Training loss 0.0391722247004509 Validation loss 0.04791184514760971 Accuracy 0.8710000514984131\n",
      "Iteration 11080 Training loss 0.041286010295152664 Validation loss 0.04756420850753784 Accuracy 0.8710000514984131\n",
      "Iteration 11090 Training loss 0.04732392728328705 Validation loss 0.05354638397693634 Accuracy 0.8532500267028809\n",
      "Iteration 11100 Training loss 0.029558029025793076 Validation loss 0.047388020902872086 Accuracy 0.8716250658035278\n",
      "Iteration 11110 Training loss 0.049281977117061615 Validation loss 0.050323374569416046 Accuracy 0.8661250472068787\n",
      "Iteration 11120 Training loss 0.0467069149017334 Validation loss 0.04802963510155678 Accuracy 0.8695000410079956\n",
      "Iteration 11130 Training loss 0.04125949740409851 Validation loss 0.05008777230978012 Accuracy 0.8666250705718994\n",
      "Iteration 11140 Training loss 0.04960675910115242 Validation loss 0.04830782487988472 Accuracy 0.8682500123977661\n",
      "Iteration 11150 Training loss 0.042786598205566406 Validation loss 0.05028442665934563 Accuracy 0.861875057220459\n",
      "Iteration 11160 Training loss 0.034538719803094864 Validation loss 0.048465829342603683 Accuracy 0.8668750524520874\n",
      "Iteration 11170 Training loss 0.05273757502436638 Validation loss 0.04935393109917641 Accuracy 0.8690000176429749\n",
      "Iteration 11180 Training loss 0.047236330807209015 Validation loss 0.04878709465265274 Accuracy 0.8651250600814819\n",
      "Iteration 11190 Training loss 0.03845565393567085 Validation loss 0.04728713631629944 Accuracy 0.8735000491142273\n",
      "Iteration 11200 Training loss 0.04453713446855545 Validation loss 0.05095028877258301 Accuracy 0.862375020980835\n",
      "Iteration 11210 Training loss 0.04130604490637779 Validation loss 0.04936670511960983 Accuracy 0.8647500276565552\n",
      "Iteration 11220 Training loss 0.044889628887176514 Validation loss 0.047206614166498184 Accuracy 0.8727500438690186\n",
      "Iteration 11230 Training loss 0.05192384496331215 Validation loss 0.05085175111889839 Accuracy 0.8607500195503235\n",
      "Iteration 11240 Training loss 0.04470367357134819 Validation loss 0.049068182706832886 Accuracy 0.8653750419616699\n",
      "Iteration 11250 Training loss 0.04690707102417946 Validation loss 0.04727865383028984 Accuracy 0.8702500462532043\n",
      "Iteration 11260 Training loss 0.04050604626536369 Validation loss 0.04912162944674492 Accuracy 0.8688750267028809\n",
      "Iteration 11270 Training loss 0.05050799250602722 Validation loss 0.050961751490831375 Accuracy 0.861875057220459\n",
      "Iteration 11280 Training loss 0.037060804665088654 Validation loss 0.04812297224998474 Accuracy 0.8680000305175781\n",
      "Iteration 11290 Training loss 0.045011185109615326 Validation loss 0.04823704808950424 Accuracy 0.8715000152587891\n",
      "Iteration 11300 Training loss 0.03980018198490143 Validation loss 0.04745973274111748 Accuracy 0.8707500696182251\n",
      "Iteration 11310 Training loss 0.04553550109267235 Validation loss 0.047192174941301346 Accuracy 0.8735000491142273\n",
      "Iteration 11320 Training loss 0.049759410321712494 Validation loss 0.04970390722155571 Accuracy 0.8676250576972961\n",
      "Iteration 11330 Training loss 0.04742325842380524 Validation loss 0.04803020879626274 Accuracy 0.8713750243186951\n",
      "Iteration 11340 Training loss 0.04849718138575554 Validation loss 0.049134884029626846 Accuracy 0.8705000281333923\n",
      "Iteration 11350 Training loss 0.049334943294525146 Validation loss 0.05422583594918251 Accuracy 0.8493750691413879\n",
      "Iteration 11360 Training loss 0.03512829169631004 Validation loss 0.04759250953793526 Accuracy 0.8695000410079956\n",
      "Iteration 11370 Training loss 0.038174819201231 Validation loss 0.04741518571972847 Accuracy 0.8721250295639038\n",
      "Iteration 11380 Training loss 0.04298838600516319 Validation loss 0.04703134298324585 Accuracy 0.8721250295639038\n",
      "Iteration 11390 Training loss 0.04590258374810219 Validation loss 0.046940241008996964 Accuracy 0.8730000257492065\n",
      "Iteration 11400 Training loss 0.0412554070353508 Validation loss 0.04881903529167175 Accuracy 0.8642500638961792\n",
      "Iteration 11410 Training loss 0.045510996133089066 Validation loss 0.047250960022211075 Accuracy 0.8712500333786011\n",
      "Iteration 11420 Training loss 0.044119276106357574 Validation loss 0.04758838191628456 Accuracy 0.8695000410079956\n",
      "Iteration 11430 Training loss 0.044631633907556534 Validation loss 0.04690610617399216 Accuracy 0.8732500672340393\n",
      "Iteration 11440 Training loss 0.0329802967607975 Validation loss 0.04684619978070259 Accuracy 0.8726250529289246\n",
      "Iteration 11450 Training loss 0.04161565378308296 Validation loss 0.04721624776721001 Accuracy 0.8715000152587891\n",
      "Iteration 11460 Training loss 0.0445374958217144 Validation loss 0.04828654229640961 Accuracy 0.8675000667572021\n",
      "Iteration 11470 Training loss 0.05066164955496788 Validation loss 0.05058613419532776 Accuracy 0.8605000376701355\n",
      "Iteration 11480 Training loss 0.04504591226577759 Validation loss 0.04924636334180832 Accuracy 0.8686250448226929\n",
      "Iteration 11490 Training loss 0.04245016351342201 Validation loss 0.04945351555943489 Accuracy 0.8680000305175781\n",
      "Iteration 11500 Training loss 0.04679547995328903 Validation loss 0.0518609918653965 Accuracy 0.8610000610351562\n",
      "Iteration 11510 Training loss 0.04708932712674141 Validation loss 0.04741034656763077 Accuracy 0.8695000410079956\n",
      "Iteration 11520 Training loss 0.05187498778104782 Validation loss 0.052443116903305054 Accuracy 0.8576250672340393\n",
      "Iteration 11530 Training loss 0.04166894033551216 Validation loss 0.04873274639248848 Accuracy 0.8667500615119934\n",
      "Iteration 11540 Training loss 0.042027685791254044 Validation loss 0.05425594002008438 Accuracy 0.8528750538825989\n",
      "Iteration 11550 Training loss 0.05441722646355629 Validation loss 0.06154952943325043 Accuracy 0.8327500224113464\n",
      "Iteration 11560 Training loss 0.04285191372036934 Validation loss 0.05042017623782158 Accuracy 0.8641250133514404\n",
      "Iteration 11570 Training loss 0.05199915170669556 Validation loss 0.05676731467247009 Accuracy 0.8427500128746033\n",
      "Iteration 11580 Training loss 0.03435056656599045 Validation loss 0.0500807911157608 Accuracy 0.8641250133514404\n",
      "Iteration 11590 Training loss 0.04463198035955429 Validation loss 0.052033308893442154 Accuracy 0.859125018119812\n",
      "Iteration 11600 Training loss 0.039680927991867065 Validation loss 0.05040999501943588 Accuracy 0.8641250133514404\n",
      "Iteration 11610 Training loss 0.04539738968014717 Validation loss 0.04736750200390816 Accuracy 0.8723750710487366\n",
      "Iteration 11620 Training loss 0.03804212808609009 Validation loss 0.047434642910957336 Accuracy 0.8716250658035278\n",
      "Iteration 11630 Training loss 0.03575776889920235 Validation loss 0.047030191868543625 Accuracy 0.8722500205039978\n",
      "Iteration 11640 Training loss 0.03687514364719391 Validation loss 0.05013616755604744 Accuracy 0.8635000586509705\n",
      "Iteration 11650 Training loss 0.04705379530787468 Validation loss 0.047470953315496445 Accuracy 0.8716250658035278\n",
      "Iteration 11660 Training loss 0.05526408925652504 Validation loss 0.05512412637472153 Accuracy 0.8518750667572021\n",
      "Iteration 11670 Training loss 0.038540180772542953 Validation loss 0.047124624252319336 Accuracy 0.8712500333786011\n",
      "Iteration 11680 Training loss 0.041525207459926605 Validation loss 0.050272274762392044 Accuracy 0.8616250157356262\n",
      "Iteration 11690 Training loss 0.03584128990769386 Validation loss 0.04876813665032387 Accuracy 0.8652500510215759\n",
      "Iteration 11700 Training loss 0.04364572837948799 Validation loss 0.04889087378978729 Accuracy 0.8655000329017639\n",
      "Iteration 11710 Training loss 0.041362397372722626 Validation loss 0.04746481403708458 Accuracy 0.8706250190734863\n",
      "Iteration 11720 Training loss 0.05279023200273514 Validation loss 0.047136981040239334 Accuracy 0.8733750581741333\n",
      "Iteration 11730 Training loss 0.039065491408109665 Validation loss 0.051128871738910675 Accuracy 0.8631250262260437\n",
      "Iteration 11740 Training loss 0.04301970452070236 Validation loss 0.04975719377398491 Accuracy 0.8682500123977661\n",
      "Iteration 11750 Training loss 0.04496702551841736 Validation loss 0.04701017588376999 Accuracy 0.8715000152587891\n",
      "Iteration 11760 Training loss 0.03566708788275719 Validation loss 0.04771715775132179 Accuracy 0.8725000619888306\n",
      "Iteration 11770 Training loss 0.03671044111251831 Validation loss 0.04686146602034569 Accuracy 0.8736250400543213\n",
      "Iteration 11780 Training loss 0.04090036451816559 Validation loss 0.048215579241514206 Accuracy 0.8725000619888306\n",
      "Iteration 11790 Training loss 0.05129794031381607 Validation loss 0.05034725368022919 Accuracy 0.8663750290870667\n",
      "Iteration 11800 Training loss 0.04087052121758461 Validation loss 0.048161476850509644 Accuracy 0.8673750162124634\n",
      "Iteration 11810 Training loss 0.042970456182956696 Validation loss 0.047691769897937775 Accuracy 0.8725000619888306\n",
      "Iteration 11820 Training loss 0.048276059329509735 Validation loss 0.05131043121218681 Accuracy 0.8626250624656677\n",
      "Iteration 11830 Training loss 0.048198964446783066 Validation loss 0.048491403460502625 Accuracy 0.8712500333786011\n",
      "Iteration 11840 Training loss 0.038800399750471115 Validation loss 0.04718882218003273 Accuracy 0.8717500567436218\n",
      "Iteration 11850 Training loss 0.044697199016809464 Validation loss 0.04829292371869087 Accuracy 0.8675000667572021\n",
      "Iteration 11860 Training loss 0.04297713562846184 Validation loss 0.0550076887011528 Accuracy 0.8488750457763672\n",
      "Iteration 11870 Training loss 0.0431678332388401 Validation loss 0.0482906699180603 Accuracy 0.8708750605583191\n",
      "Iteration 11880 Training loss 0.03799571096897125 Validation loss 0.04790642857551575 Accuracy 0.8716250658035278\n",
      "Iteration 11890 Training loss 0.03775877505540848 Validation loss 0.047748129814863205 Accuracy 0.8687500357627869\n",
      "Iteration 11900 Training loss 0.04636617377400398 Validation loss 0.04745835065841675 Accuracy 0.8722500205039978\n",
      "Iteration 11910 Training loss 0.038114454597234726 Validation loss 0.047863513231277466 Accuracy 0.8691250681877136\n",
      "Iteration 11920 Training loss 0.04129902645945549 Validation loss 0.04699947312474251 Accuracy 0.874250054359436\n",
      "Iteration 11930 Training loss 0.031182536855340004 Validation loss 0.04836489260196686 Accuracy 0.8712500333786011\n",
      "Iteration 11940 Training loss 0.048532094806432724 Validation loss 0.05460544675588608 Accuracy 0.8516250252723694\n",
      "Iteration 11950 Training loss 0.040673695504665375 Validation loss 0.04810451716184616 Accuracy 0.8702500462532043\n",
      "Iteration 11960 Training loss 0.043375372886657715 Validation loss 0.052534595131874084 Accuracy 0.8566250205039978\n",
      "Iteration 11970 Training loss 0.04190002381801605 Validation loss 0.04879474267363548 Accuracy 0.8656250238418579\n",
      "Iteration 11980 Training loss 0.04168452322483063 Validation loss 0.049342118203639984 Accuracy 0.8643750548362732\n",
      "Iteration 11990 Training loss 0.04576099291443825 Validation loss 0.04761643335223198 Accuracy 0.8733750581741333\n",
      "Iteration 12000 Training loss 0.03924918174743652 Validation loss 0.05001727491617203 Accuracy 0.8637500405311584\n",
      "Iteration 12010 Training loss 0.04086943715810776 Validation loss 0.04715278744697571 Accuracy 0.874750018119812\n",
      "Iteration 12020 Training loss 0.05023369565606117 Validation loss 0.055376943200826645 Accuracy 0.8482500314712524\n",
      "Iteration 12030 Training loss 0.04325747862458229 Validation loss 0.04851648956537247 Accuracy 0.8662500381469727\n",
      "Iteration 12040 Training loss 0.03920444846153259 Validation loss 0.048687852919101715 Accuracy 0.8710000514984131\n",
      "Iteration 12050 Training loss 0.04140926152467728 Validation loss 0.0472966767847538 Accuracy 0.8726250529289246\n",
      "Iteration 12060 Training loss 0.03706718981266022 Validation loss 0.04759657010436058 Accuracy 0.8697500228881836\n",
      "Iteration 12070 Training loss 0.04122978076338768 Validation loss 0.04753096401691437 Accuracy 0.8706250190734863\n",
      "Iteration 12080 Training loss 0.0427514910697937 Validation loss 0.052505820989608765 Accuracy 0.8572500348091125\n",
      "Iteration 12090 Training loss 0.043906647711992264 Validation loss 0.04721751809120178 Accuracy 0.8695000410079956\n",
      "Iteration 12100 Training loss 0.038576655089855194 Validation loss 0.04683982953429222 Accuracy 0.874125063419342\n",
      "Iteration 12110 Training loss 0.03913265839219093 Validation loss 0.05028041452169418 Accuracy 0.861875057220459\n",
      "Iteration 12120 Training loss 0.03747793287038803 Validation loss 0.04702141880989075 Accuracy 0.8720000386238098\n",
      "Iteration 12130 Training loss 0.03251909837126732 Validation loss 0.04696694761514664 Accuracy 0.8730000257492065\n",
      "Iteration 12140 Training loss 0.039187949150800705 Validation loss 0.05224311351776123 Accuracy 0.858625054359436\n",
      "Iteration 12150 Training loss 0.03665463998913765 Validation loss 0.04658202454447746 Accuracy 0.8736250400543213\n",
      "Iteration 12160 Training loss 0.03772186115384102 Validation loss 0.046694155782461166 Accuracy 0.8731250166893005\n",
      "Iteration 12170 Training loss 0.05589834600687027 Validation loss 0.05354200676083565 Accuracy 0.8532500267028809\n",
      "Iteration 12180 Training loss 0.03661377727985382 Validation loss 0.04699629172682762 Accuracy 0.8725000619888306\n",
      "Iteration 12190 Training loss 0.05387074872851372 Validation loss 0.057354748249053955 Accuracy 0.8458750247955322\n",
      "Iteration 12200 Training loss 0.04613881930708885 Validation loss 0.04739922285079956 Accuracy 0.874125063419342\n",
      "Iteration 12210 Training loss 0.05291067808866501 Validation loss 0.05090595409274101 Accuracy 0.8628750443458557\n",
      "Iteration 12220 Training loss 0.041978493332862854 Validation loss 0.04695577546954155 Accuracy 0.8728750348091125\n",
      "Iteration 12230 Training loss 0.047906987369060516 Validation loss 0.04824969172477722 Accuracy 0.8660000562667847\n",
      "Iteration 12240 Training loss 0.040031347423791885 Validation loss 0.046484265476465225 Accuracy 0.8737500309944153\n",
      "Iteration 12250 Training loss 0.03899078816175461 Validation loss 0.04640420153737068 Accuracy 0.87437504529953\n",
      "Iteration 12260 Training loss 0.043034885078668594 Validation loss 0.046689677983522415 Accuracy 0.8740000128746033\n",
      "Iteration 12270 Training loss 0.04058018699288368 Validation loss 0.052255406975746155 Accuracy 0.8553750514984131\n",
      "Iteration 12280 Training loss 0.040106192231178284 Validation loss 0.04776391014456749 Accuracy 0.8691250681877136\n",
      "Iteration 12290 Training loss 0.04935075342655182 Validation loss 0.04704983904957771 Accuracy 0.8726250529289246\n",
      "Iteration 12300 Training loss 0.04791189730167389 Validation loss 0.04997555911540985 Accuracy 0.8655000329017639\n",
      "Iteration 12310 Training loss 0.039461880922317505 Validation loss 0.04774933680891991 Accuracy 0.8717500567436218\n",
      "Iteration 12320 Training loss 0.0526774637401104 Validation loss 0.05885879695415497 Accuracy 0.8385000228881836\n",
      "Iteration 12330 Training loss 0.041798777878284454 Validation loss 0.048742637038230896 Accuracy 0.8707500696182251\n",
      "Iteration 12340 Training loss 0.03843836858868599 Validation loss 0.050524983555078506 Accuracy 0.8606250286102295\n",
      "Iteration 12350 Training loss 0.03912074863910675 Validation loss 0.04678119719028473 Accuracy 0.8718750476837158\n",
      "Iteration 12360 Training loss 0.033165182918310165 Validation loss 0.047823671251535416 Accuracy 0.8696250319480896\n",
      "Iteration 12370 Training loss 0.03966415673494339 Validation loss 0.05151747912168503 Accuracy 0.8561250567436218\n",
      "Iteration 12380 Training loss 0.04782358184456825 Validation loss 0.056580837815999985 Accuracy 0.8447500467300415\n",
      "Iteration 12390 Training loss 0.045350100845098495 Validation loss 0.046829767525196075 Accuracy 0.8735000491142273\n",
      "Iteration 12400 Training loss 0.03897674381732941 Validation loss 0.047642093151807785 Accuracy 0.8726250529289246\n",
      "Iteration 12410 Training loss 0.04742278903722763 Validation loss 0.05317071080207825 Accuracy 0.8571250438690186\n",
      "Iteration 12420 Training loss 0.04016559198498726 Validation loss 0.04655301198363304 Accuracy 0.874125063419342\n",
      "Iteration 12430 Training loss 0.035173457115888596 Validation loss 0.04772345349192619 Accuracy 0.8732500672340393\n",
      "Iteration 12440 Training loss 0.042077746242284775 Validation loss 0.0468495674431324 Accuracy 0.8730000257492065\n",
      "Iteration 12450 Training loss 0.053023360669612885 Validation loss 0.05040828883647919 Accuracy 0.8605000376701355\n",
      "Iteration 12460 Training loss 0.04674217104911804 Validation loss 0.05464945733547211 Accuracy 0.8540000319480896\n",
      "Iteration 12470 Training loss 0.05178368464112282 Validation loss 0.04968549683690071 Accuracy 0.8665000200271606\n",
      "Iteration 12480 Training loss 0.044518060982227325 Validation loss 0.04657641798257828 Accuracy 0.874250054359436\n",
      "Iteration 12490 Training loss 0.03865749388933182 Validation loss 0.04744569584727287 Accuracy 0.8708750605583191\n",
      "Iteration 12500 Training loss 0.04257683828473091 Validation loss 0.04753488302230835 Accuracy 0.8737500309944153\n",
      "Iteration 12510 Training loss 0.04341348633170128 Validation loss 0.04747901111841202 Accuracy 0.8686250448226929\n",
      "Iteration 12520 Training loss 0.04773935303092003 Validation loss 0.05220865458250046 Accuracy 0.8551250696182251\n",
      "Iteration 12530 Training loss 0.04717467352747917 Validation loss 0.05469287186861038 Accuracy 0.8483750224113464\n",
      "Iteration 12540 Training loss 0.04283382371068001 Validation loss 0.04831773787736893 Accuracy 0.8676250576972961\n",
      "Iteration 12550 Training loss 0.04618183150887489 Validation loss 0.04915282130241394 Accuracy 0.8645000457763672\n",
      "Iteration 12560 Training loss 0.03571408987045288 Validation loss 0.04718858376145363 Accuracy 0.8701250553131104\n",
      "Iteration 12570 Training loss 0.039883099496364594 Validation loss 0.04629037529230118 Accuracy 0.8740000128746033\n",
      "Iteration 12580 Training loss 0.04019372537732124 Validation loss 0.04681806266307831 Accuracy 0.8716250658035278\n",
      "Iteration 12590 Training loss 0.04470740258693695 Validation loss 0.046602778136730194 Accuracy 0.8726250529289246\n",
      "Iteration 12600 Training loss 0.04604003205895424 Validation loss 0.04787410795688629 Accuracy 0.8688750267028809\n",
      "Iteration 12610 Training loss 0.051635611802339554 Validation loss 0.05951627343893051 Accuracy 0.8363750576972961\n",
      "Iteration 12620 Training loss 0.04462728276848793 Validation loss 0.05065460130572319 Accuracy 0.8597500324249268\n",
      "Iteration 12630 Training loss 0.034598466008901596 Validation loss 0.05011625587940216 Accuracy 0.8615000247955322\n",
      "Iteration 12640 Training loss 0.057990722358226776 Validation loss 0.061520542949438095 Accuracy 0.8303750157356262\n",
      "Iteration 12650 Training loss 0.03929993137717247 Validation loss 0.04649317264556885 Accuracy 0.8730000257492065\n",
      "Iteration 12660 Training loss 0.04499566927552223 Validation loss 0.04649096354842186 Accuracy 0.874125063419342\n",
      "Iteration 12670 Training loss 0.04041462019085884 Validation loss 0.049779269844293594 Accuracy 0.8666250705718994\n",
      "Iteration 12680 Training loss 0.042300138622522354 Validation loss 0.048418913036584854 Accuracy 0.8671250343322754\n",
      "Iteration 12690 Training loss 0.03386661037802696 Validation loss 0.05191076174378395 Accuracy 0.8547500371932983\n",
      "Iteration 12700 Training loss 0.037633564323186874 Validation loss 0.046569596976041794 Accuracy 0.8740000128746033\n",
      "Iteration 12710 Training loss 0.04063147306442261 Validation loss 0.047196656465530396 Accuracy 0.8738750219345093\n",
      "Iteration 12720 Training loss 0.04198038950562477 Validation loss 0.046610359102487564 Accuracy 0.8737500309944153\n",
      "Iteration 12730 Training loss 0.04732644930481911 Validation loss 0.05071326345205307 Accuracy 0.8598750233650208\n",
      "Iteration 12740 Training loss 0.03735613077878952 Validation loss 0.04757877439260483 Accuracy 0.8687500357627869\n",
      "Iteration 12750 Training loss 0.040261559188365936 Validation loss 0.04631869122385979 Accuracy 0.874125063419342\n",
      "Iteration 12760 Training loss 0.03642949089407921 Validation loss 0.04655018821358681 Accuracy 0.874750018119812\n",
      "Iteration 12770 Training loss 0.04904311150312424 Validation loss 0.05094625800848007 Accuracy 0.862375020980835\n",
      "Iteration 12780 Training loss 0.03967549651861191 Validation loss 0.04846617951989174 Accuracy 0.8677500486373901\n",
      "Iteration 12790 Training loss 0.04088684171438217 Validation loss 0.049142125993967056 Accuracy 0.8642500638961792\n",
      "Iteration 12800 Training loss 0.03025062195956707 Validation loss 0.046143531799316406 Accuracy 0.8752500414848328\n",
      "Iteration 12810 Training loss 0.047560155391693115 Validation loss 0.05351829156279564 Accuracy 0.8556250333786011\n",
      "Iteration 12820 Training loss 0.037132132798433304 Validation loss 0.04628279432654381 Accuracy 0.87437504529953\n",
      "Iteration 12830 Training loss 0.04274751618504524 Validation loss 0.047932207584381104 Accuracy 0.8725000619888306\n",
      "Iteration 12840 Training loss 0.042295102030038834 Validation loss 0.046850282698869705 Accuracy 0.8737500309944153\n",
      "Iteration 12850 Training loss 0.033969759941101074 Validation loss 0.04615257307887077 Accuracy 0.874625027179718\n",
      "Iteration 12860 Training loss 0.036321766674518585 Validation loss 0.046199727803468704 Accuracy 0.8737500309944153\n",
      "Iteration 12870 Training loss 0.03987767547369003 Validation loss 0.05141240358352661 Accuracy 0.8616250157356262\n",
      "Iteration 12880 Training loss 0.04144098982214928 Validation loss 0.04676852002739906 Accuracy 0.8721250295639038\n",
      "Iteration 12890 Training loss 0.04959801584482193 Validation loss 0.049366459250450134 Accuracy 0.8645000457763672\n",
      "Iteration 12900 Training loss 0.0346868634223938 Validation loss 0.04924954101443291 Accuracy 0.8670000433921814\n",
      "Iteration 12910 Training loss 0.03860490024089813 Validation loss 0.04836723953485489 Accuracy 0.8675000667572021\n",
      "Iteration 12920 Training loss 0.040120311081409454 Validation loss 0.04685599356889725 Accuracy 0.8718750476837158\n",
      "Iteration 12930 Training loss 0.039549682289361954 Validation loss 0.048140913248062134 Accuracy 0.8681250214576721\n",
      "Iteration 12940 Training loss 0.03944456949830055 Validation loss 0.04788915440440178 Accuracy 0.8675000667572021\n",
      "Iteration 12950 Training loss 0.03970690071582794 Validation loss 0.04608162119984627 Accuracy 0.8750000596046448\n",
      "Iteration 12960 Training loss 0.03652501851320267 Validation loss 0.046735040843486786 Accuracy 0.874750018119812\n",
      "Iteration 12970 Training loss 0.04398574307560921 Validation loss 0.05417429655790329 Accuracy 0.8497500419616699\n",
      "Iteration 12980 Training loss 0.03725384920835495 Validation loss 0.047445788979530334 Accuracy 0.8698750138282776\n",
      "Iteration 12990 Training loss 0.04832741618156433 Validation loss 0.046942662447690964 Accuracy 0.8711250424385071\n",
      "Iteration 13000 Training loss 0.04307993873953819 Validation loss 0.05006588250398636 Accuracy 0.8631250262260437\n",
      "Iteration 13010 Training loss 0.04265788942575455 Validation loss 0.0500224344432354 Accuracy 0.8613750338554382\n",
      "Iteration 13020 Training loss 0.044840533286333084 Validation loss 0.04755131155252457 Accuracy 0.8686250448226929\n",
      "Iteration 13030 Training loss 0.04877128452062607 Validation loss 0.05087640881538391 Accuracy 0.862375020980835\n",
      "Iteration 13040 Training loss 0.038748376071453094 Validation loss 0.048490606248378754 Accuracy 0.8715000152587891\n",
      "Iteration 13050 Training loss 0.04024003446102142 Validation loss 0.050533708184957504 Accuracy 0.8603750467300415\n",
      "Iteration 13060 Training loss 0.038019247353076935 Validation loss 0.04858001321554184 Accuracy 0.8706250190734863\n",
      "Iteration 13070 Training loss 0.036933984607458115 Validation loss 0.04642949253320694 Accuracy 0.8725000619888306\n",
      "Iteration 13080 Training loss 0.04369699954986572 Validation loss 0.0485135018825531 Accuracy 0.8655000329017639\n",
      "Iteration 13090 Training loss 0.044763486832380295 Validation loss 0.05157109349966049 Accuracy 0.8608750700950623\n",
      "Iteration 13100 Training loss 0.04018218070268631 Validation loss 0.04618220031261444 Accuracy 0.87437504529953\n",
      "Iteration 13110 Training loss 0.04382434859871864 Validation loss 0.046192556619644165 Accuracy 0.8753750324249268\n",
      "Iteration 13120 Training loss 0.04483683407306671 Validation loss 0.04639887064695358 Accuracy 0.8720000386238098\n",
      "Iteration 13130 Training loss 0.040015269070863724 Validation loss 0.0482720285654068 Accuracy 0.8681250214576721\n",
      "Iteration 13140 Training loss 0.041677430272102356 Validation loss 0.04795490577816963 Accuracy 0.8691250681877136\n",
      "Iteration 13150 Training loss 0.045128773897886276 Validation loss 0.04708568751811981 Accuracy 0.8713750243186951\n",
      "Iteration 13160 Training loss 0.03744728863239288 Validation loss 0.04651113599538803 Accuracy 0.8738750219345093\n",
      "Iteration 13170 Training loss 0.04200226813554764 Validation loss 0.048827577382326126 Accuracy 0.8670000433921814\n",
      "Iteration 13180 Training loss 0.03803124651312828 Validation loss 0.04692208766937256 Accuracy 0.8715000152587891\n",
      "Iteration 13190 Training loss 0.04099772870540619 Validation loss 0.047238122671842575 Accuracy 0.8701250553131104\n",
      "Iteration 13200 Training loss 0.04467618837952614 Validation loss 0.052138470113277435 Accuracy 0.8553750514984131\n",
      "Iteration 13210 Training loss 0.03880571201443672 Validation loss 0.04923079162836075 Accuracy 0.8646250367164612\n",
      "Iteration 13220 Training loss 0.04543093591928482 Validation loss 0.051957253366708755 Accuracy 0.8596250414848328\n",
      "Iteration 13230 Training loss 0.047220442444086075 Validation loss 0.050956498831510544 Accuracy 0.861750066280365\n",
      "Iteration 13240 Training loss 0.03988945484161377 Validation loss 0.05389826372265816 Accuracy 0.8525000214576721\n",
      "Iteration 13250 Training loss 0.03825509920716286 Validation loss 0.046977076679468155 Accuracy 0.8723750710487366\n",
      "Iteration 13260 Training loss 0.029901117086410522 Validation loss 0.046449724584817886 Accuracy 0.8755000233650208\n",
      "Iteration 13270 Training loss 0.04130014777183533 Validation loss 0.045943841338157654 Accuracy 0.8738750219345093\n",
      "Iteration 13280 Training loss 0.02540758065879345 Validation loss 0.045910000801086426 Accuracy 0.874625027179718\n",
      "Iteration 13290 Training loss 0.043202679604291916 Validation loss 0.04862188547849655 Accuracy 0.8678750395774841\n",
      "Iteration 13300 Training loss 0.0516732782125473 Validation loss 0.0618574395775795 Accuracy 0.8338750600814819\n",
      "Iteration 13310 Training loss 0.05011656507849693 Validation loss 0.050090692937374115 Accuracy 0.862250030040741\n",
      "Iteration 13320 Training loss 0.03551842272281647 Validation loss 0.04632643237709999 Accuracy 0.8733750581741333\n",
      "Iteration 13330 Training loss 0.04027467966079712 Validation loss 0.046386025846004486 Accuracy 0.874250054359436\n",
      "Iteration 13340 Training loss 0.05019586905837059 Validation loss 0.058512378484010696 Accuracy 0.8407500386238098\n",
      "Iteration 13350 Training loss 0.043812647461891174 Validation loss 0.04634759575128555 Accuracy 0.8732500672340393\n",
      "Iteration 13360 Training loss 0.04162706434726715 Validation loss 0.05319566652178764 Accuracy 0.8532500267028809\n",
      "Iteration 13370 Training loss 0.04399177432060242 Validation loss 0.047992508858442307 Accuracy 0.8721250295639038\n",
      "Iteration 13380 Training loss 0.042027149349451065 Validation loss 0.048525553196668625 Accuracy 0.8701250553131104\n",
      "Iteration 13390 Training loss 0.03665713593363762 Validation loss 0.046807657927274704 Accuracy 0.8738750219345093\n",
      "Iteration 13400 Training loss 0.04742567986249924 Validation loss 0.04998967796564102 Accuracy 0.8658750653266907\n",
      "Iteration 13410 Training loss 0.03568711131811142 Validation loss 0.04707280173897743 Accuracy 0.8723750710487366\n",
      "Iteration 13420 Training loss 0.03655610233545303 Validation loss 0.0524040162563324 Accuracy 0.8576250672340393\n",
      "Iteration 13430 Training loss 0.040449149906635284 Validation loss 0.04621032625436783 Accuracy 0.874625027179718\n",
      "Iteration 13440 Training loss 0.04557136073708534 Validation loss 0.05011730641126633 Accuracy 0.8671250343322754\n",
      "Iteration 13450 Training loss 0.03605562448501587 Validation loss 0.04804440960288048 Accuracy 0.8726250529289246\n",
      "Iteration 13460 Training loss 0.04067002609372139 Validation loss 0.048308875411748886 Accuracy 0.8720000386238098\n",
      "Iteration 13470 Training loss 0.039862267673015594 Validation loss 0.04741596803069115 Accuracy 0.8718750476837158\n",
      "Iteration 13480 Training loss 0.04460645467042923 Validation loss 0.04582244157791138 Accuracy 0.8753750324249268\n",
      "Iteration 13490 Training loss 0.044145479798316956 Validation loss 0.048879802227020264 Accuracy 0.8660000562667847\n",
      "Iteration 13500 Training loss 0.04388447478413582 Validation loss 0.04829581081867218 Accuracy 0.8687500357627869\n",
      "Iteration 13510 Training loss 0.05039070174098015 Validation loss 0.04631677642464638 Accuracy 0.874125063419342\n",
      "Iteration 13520 Training loss 0.03930559381842613 Validation loss 0.04814174771308899 Accuracy 0.8710000514984131\n",
      "Iteration 13530 Training loss 0.03331165388226509 Validation loss 0.04631834477186203 Accuracy 0.874250054359436\n",
      "Iteration 13540 Training loss 0.0349239818751812 Validation loss 0.0486772283911705 Accuracy 0.8695000410079956\n",
      "Iteration 13550 Training loss 0.03688809275627136 Validation loss 0.04728231579065323 Accuracy 0.8730000257492065\n",
      "Iteration 13560 Training loss 0.03498978540301323 Validation loss 0.046997904777526855 Accuracy 0.874125063419342\n",
      "Iteration 13570 Training loss 0.035318512469530106 Validation loss 0.05084816366434097 Accuracy 0.8610000610351562\n",
      "Iteration 13580 Training loss 0.037724316120147705 Validation loss 0.04587472602725029 Accuracy 0.8763750195503235\n",
      "Iteration 13590 Training loss 0.0370846726000309 Validation loss 0.046145595610141754 Accuracy 0.8757500648498535\n",
      "Iteration 13600 Training loss 0.037453994154930115 Validation loss 0.04601484537124634 Accuracy 0.8738750219345093\n",
      "Iteration 13610 Training loss 0.041181351989507675 Validation loss 0.04600594565272331 Accuracy 0.874750018119812\n",
      "Iteration 13620 Training loss 0.029732028022408485 Validation loss 0.04668628051877022 Accuracy 0.8736250400543213\n",
      "Iteration 13630 Training loss 0.04038989916443825 Validation loss 0.04811784252524376 Accuracy 0.8718750476837158\n",
      "Iteration 13640 Training loss 0.038415540009737015 Validation loss 0.04624456539750099 Accuracy 0.8727500438690186\n",
      "Iteration 13650 Training loss 0.0321105532348156 Validation loss 0.04581962153315544 Accuracy 0.8752500414848328\n",
      "Iteration 13660 Training loss 0.03974875062704086 Validation loss 0.05070609971880913 Accuracy 0.8631250262260437\n",
      "Iteration 13670 Training loss 0.04378106817603111 Validation loss 0.0514984056353569 Accuracy 0.861750066280365\n",
      "Iteration 13680 Training loss 0.036627352237701416 Validation loss 0.047672778367996216 Accuracy 0.8695000410079956\n",
      "Iteration 13690 Training loss 0.03825169801712036 Validation loss 0.047833580523729324 Accuracy 0.8722500205039978\n",
      "Iteration 13700 Training loss 0.04927946999669075 Validation loss 0.04925785958766937 Accuracy 0.8695000410079956\n",
      "Iteration 13710 Training loss 0.029331916943192482 Validation loss 0.04651515558362007 Accuracy 0.8751250505447388\n",
      "Iteration 13720 Training loss 0.044374361634254456 Validation loss 0.05495588481426239 Accuracy 0.8535000681877136\n",
      "Iteration 13730 Training loss 0.04707570746541023 Validation loss 0.04855401813983917 Accuracy 0.8708750605583191\n",
      "Iteration 13740 Training loss 0.04926089197397232 Validation loss 0.05335889756679535 Accuracy 0.8550000190734863\n",
      "Iteration 13750 Training loss 0.05016984045505524 Validation loss 0.049196526408195496 Accuracy 0.8666250705718994\n",
      "Iteration 13760 Training loss 0.038799948990345 Validation loss 0.04590873420238495 Accuracy 0.8752500414848328\n",
      "Iteration 13770 Training loss 0.0564824678003788 Validation loss 0.06239771470427513 Accuracy 0.8290000557899475\n",
      "Iteration 13780 Training loss 0.04475175589323044 Validation loss 0.04678603634238243 Accuracy 0.8713750243186951\n",
      "Iteration 13790 Training loss 0.03377914801239967 Validation loss 0.04589761421084404 Accuracy 0.87437504529953\n",
      "Iteration 13800 Training loss 0.04001866653561592 Validation loss 0.0459241047501564 Accuracy 0.8758750557899475\n",
      "Iteration 13810 Training loss 0.03479214757680893 Validation loss 0.04596196860074997 Accuracy 0.8738750219345093\n",
      "Iteration 13820 Training loss 0.03422379493713379 Validation loss 0.04640137776732445 Accuracy 0.8748750686645508\n",
      "Iteration 13830 Training loss 0.06165923923254013 Validation loss 0.060492295771837234 Accuracy 0.8370000123977661\n",
      "Iteration 13840 Training loss 0.039248403161764145 Validation loss 0.04599056392908096 Accuracy 0.8750000596046448\n",
      "Iteration 13850 Training loss 0.039611756801605225 Validation loss 0.04601861163973808 Accuracy 0.8737500309944153\n",
      "Iteration 13860 Training loss 0.038303446024656296 Validation loss 0.04613150283694267 Accuracy 0.8731250166893005\n",
      "Iteration 13870 Training loss 0.03447962924838066 Validation loss 0.04576179385185242 Accuracy 0.8738750219345093\n",
      "Iteration 13880 Training loss 0.03335878625512123 Validation loss 0.045758455991744995 Accuracy 0.874625027179718\n",
      "Iteration 13890 Training loss 0.04452009126543999 Validation loss 0.0483437143266201 Accuracy 0.8670000433921814\n",
      "Iteration 13900 Training loss 0.04072189703583717 Validation loss 0.045773476362228394 Accuracy 0.8757500648498535\n",
      "Iteration 13910 Training loss 0.03482021763920784 Validation loss 0.04602450877428055 Accuracy 0.8761250376701355\n",
      "Iteration 13920 Training loss 0.029148254543542862 Validation loss 0.04621153697371483 Accuracy 0.8753750324249268\n",
      "Iteration 13930 Training loss 0.03955893963575363 Validation loss 0.049281831830739975 Accuracy 0.8651250600814819\n",
      "Iteration 13940 Training loss 0.04346505180001259 Validation loss 0.05095834657549858 Accuracy 0.859000027179718\n",
      "Iteration 13950 Training loss 0.03890378400683403 Validation loss 0.04937465488910675 Accuracy 0.8646250367164612\n",
      "Iteration 13960 Training loss 0.036164622753858566 Validation loss 0.04653773084282875 Accuracy 0.8731250166893005\n",
      "Iteration 13970 Training loss 0.03329235315322876 Validation loss 0.046441689133644104 Accuracy 0.8756250143051147\n",
      "Iteration 13980 Training loss 0.034492071717977524 Validation loss 0.046260204166173935 Accuracy 0.8716250658035278\n",
      "Iteration 13990 Training loss 0.03902602568268776 Validation loss 0.04633830487728119 Accuracy 0.8703750371932983\n",
      "Iteration 14000 Training loss 0.03702414408326149 Validation loss 0.04655040055513382 Accuracy 0.8731250166893005\n",
      "Iteration 14010 Training loss 0.0438668318092823 Validation loss 0.0480826199054718 Accuracy 0.8687500357627869\n",
      "Iteration 14020 Training loss 0.048929475247859955 Validation loss 0.04869621619582176 Accuracy 0.8685000538825989\n",
      "Iteration 14030 Training loss 0.04111621901392937 Validation loss 0.05087600648403168 Accuracy 0.8611250519752502\n",
      "Iteration 14040 Training loss 0.03861208260059357 Validation loss 0.04785214364528656 Accuracy 0.8692500591278076\n",
      "Iteration 14050 Training loss 0.04623235762119293 Validation loss 0.05206558108329773 Accuracy 0.8556250333786011\n",
      "Iteration 14060 Training loss 0.042668648064136505 Validation loss 0.05196083337068558 Accuracy 0.8568750619888306\n",
      "Iteration 14070 Training loss 0.03964696824550629 Validation loss 0.06073334068059921 Accuracy 0.8337500691413879\n",
      "Iteration 14080 Training loss 0.04628067463636398 Validation loss 0.05111983045935631 Accuracy 0.85875004529953\n",
      "Iteration 14090 Training loss 0.040916405618190765 Validation loss 0.047975488007068634 Accuracy 0.8691250681877136\n",
      "Iteration 14100 Training loss 0.02936655469238758 Validation loss 0.04705077409744263 Accuracy 0.8700000643730164\n",
      "Iteration 14110 Training loss 0.039275702089071274 Validation loss 0.048045359551906586 Accuracy 0.8690000176429749\n",
      "Iteration 14120 Training loss 0.03711717575788498 Validation loss 0.045957304537296295 Accuracy 0.8737500309944153\n",
      "Iteration 14130 Training loss 0.03388143330812454 Validation loss 0.04607047513127327 Accuracy 0.8737500309944153\n",
      "Iteration 14140 Training loss 0.04663386568427086 Validation loss 0.05458490550518036 Accuracy 0.8503750562667847\n",
      "Iteration 14150 Training loss 0.03879633918404579 Validation loss 0.046154946088790894 Accuracy 0.8728750348091125\n",
      "Iteration 14160 Training loss 0.03783789277076721 Validation loss 0.04827352985739708 Accuracy 0.8698750138282776\n",
      "Iteration 14170 Training loss 0.0356307178735733 Validation loss 0.047442611306905746 Accuracy 0.8723750710487366\n",
      "Iteration 14180 Training loss 0.03877653554081917 Validation loss 0.048179131001234055 Accuracy 0.8673750162124634\n",
      "Iteration 14190 Training loss 0.03804120048880577 Validation loss 0.04966984689235687 Accuracy 0.8651250600814819\n",
      "Iteration 14200 Training loss 0.040033359080553055 Validation loss 0.048609115183353424 Accuracy 0.8658750653266907\n",
      "Iteration 14210 Training loss 0.035278841853141785 Validation loss 0.045760367065668106 Accuracy 0.8761250376701355\n",
      "Iteration 14220 Training loss 0.03344913199543953 Validation loss 0.04821494221687317 Accuracy 0.8702500462532043\n",
      "Iteration 14230 Training loss 0.06995221227407455 Validation loss 0.0689210370182991 Accuracy 0.8140000104904175\n",
      "Iteration 14240 Training loss 0.0393168143928051 Validation loss 0.046220339834690094 Accuracy 0.8733750581741333\n",
      "Iteration 14250 Training loss 0.04315169155597687 Validation loss 0.04543103277683258 Accuracy 0.8771250247955322\n",
      "Iteration 14260 Training loss 0.04492723196744919 Validation loss 0.04566202312707901 Accuracy 0.8768750429153442\n",
      "Iteration 14270 Training loss 0.03877868875861168 Validation loss 0.04587327688932419 Accuracy 0.874750018119812\n",
      "Iteration 14280 Training loss 0.04114723950624466 Validation loss 0.04679010435938835 Accuracy 0.8712500333786011\n",
      "Iteration 14290 Training loss 0.034931741654872894 Validation loss 0.045221563428640366 Accuracy 0.878000020980835\n",
      "Iteration 14300 Training loss 0.045352447777986526 Validation loss 0.05013912543654442 Accuracy 0.8665000200271606\n",
      "Iteration 14310 Training loss 0.02855140157043934 Validation loss 0.046936750411987305 Accuracy 0.8728750348091125\n",
      "Iteration 14320 Training loss 0.04313068464398384 Validation loss 0.05267482250928879 Accuracy 0.8560000658035278\n",
      "Iteration 14330 Training loss 0.041803739964962006 Validation loss 0.04731142148375511 Accuracy 0.8697500228881836\n",
      "Iteration 14340 Training loss 0.045682597905397415 Validation loss 0.04818624258041382 Accuracy 0.8675000667572021\n",
      "Iteration 14350 Training loss 0.04118578881025314 Validation loss 0.049880608916282654 Accuracy 0.8612500429153442\n",
      "Iteration 14360 Training loss 0.04837198182940483 Validation loss 0.045515887439250946 Accuracy 0.8756250143051147\n",
      "Iteration 14370 Training loss 0.038094744086265564 Validation loss 0.045613184571266174 Accuracy 0.8760000467300415\n",
      "Iteration 14380 Training loss 0.046364497393369675 Validation loss 0.052242960780858994 Accuracy 0.8592500686645508\n",
      "Iteration 14390 Training loss 0.04268113151192665 Validation loss 0.0458483062684536 Accuracy 0.874125063419342\n",
      "Iteration 14400 Training loss 0.043180037289857864 Validation loss 0.05202748253941536 Accuracy 0.8563750386238098\n",
      "Iteration 14410 Training loss 0.04934348911046982 Validation loss 0.05144776403903961 Accuracy 0.8577500581741333\n",
      "Iteration 14420 Training loss 0.035705942660570145 Validation loss 0.047539085149765015 Accuracy 0.8670000433921814\n",
      "Iteration 14430 Training loss 0.031748879700899124 Validation loss 0.04566335678100586 Accuracy 0.8760000467300415\n",
      "Iteration 14440 Training loss 0.03853882849216461 Validation loss 0.04682771489024162 Accuracy 0.8716250658035278\n",
      "Iteration 14450 Training loss 0.04289499670267105 Validation loss 0.049824364483356476 Accuracy 0.862500011920929\n",
      "Iteration 14460 Training loss 0.045304231345653534 Validation loss 0.05330038443207741 Accuracy 0.8581250309944153\n",
      "Iteration 14470 Training loss 0.04237739369273186 Validation loss 0.0461672842502594 Accuracy 0.874500036239624\n",
      "Iteration 14480 Training loss 0.03857323154807091 Validation loss 0.049935489892959595 Accuracy 0.8653750419616699\n",
      "Iteration 14490 Training loss 0.035375118255615234 Validation loss 0.048629194498062134 Accuracy 0.8670000433921814\n",
      "Iteration 14500 Training loss 0.03904401510953903 Validation loss 0.04615461453795433 Accuracy 0.8718750476837158\n",
      "Iteration 14510 Training loss 0.04213300719857216 Validation loss 0.048003312200307846 Accuracy 0.8667500615119934\n",
      "Iteration 14520 Training loss 0.03531539440155029 Validation loss 0.04640114679932594 Accuracy 0.8755000233650208\n",
      "Iteration 14530 Training loss 0.03353577479720116 Validation loss 0.04560522735118866 Accuracy 0.8750000596046448\n",
      "Iteration 14540 Training loss 0.03337220102548599 Validation loss 0.045354802161455154 Accuracy 0.8765000700950623\n",
      "Iteration 14550 Training loss 0.03247855231165886 Validation loss 0.04706871882081032 Accuracy 0.8717500567436218\n",
      "Iteration 14560 Training loss 0.0423191636800766 Validation loss 0.05201081559062004 Accuracy 0.8563750386238098\n",
      "Iteration 14570 Training loss 0.03075881488621235 Validation loss 0.046083901077508926 Accuracy 0.8730000257492065\n",
      "Iteration 14580 Training loss 0.03525539115071297 Validation loss 0.04567341506481171 Accuracy 0.8748750686645508\n",
      "Iteration 14590 Training loss 0.03544766828417778 Validation loss 0.04856189712882042 Accuracy 0.8660000562667847\n",
      "Iteration 14600 Training loss 0.03704771772027016 Validation loss 0.04605332389473915 Accuracy 0.8736250400543213\n",
      "Iteration 14610 Training loss 0.03613410145044327 Validation loss 0.048987261950969696 Accuracy 0.8653750419616699\n",
      "Iteration 14620 Training loss 0.03758949786424637 Validation loss 0.04568435996770859 Accuracy 0.8748750686645508\n",
      "Iteration 14630 Training loss 0.04220317676663399 Validation loss 0.04627317935228348 Accuracy 0.8750000596046448\n",
      "Iteration 14640 Training loss 0.03983159735798836 Validation loss 0.04745690897107124 Accuracy 0.8700000643730164\n",
      "Iteration 14650 Training loss 0.04906811565160751 Validation loss 0.056440696120262146 Accuracy 0.8495000600814819\n",
      "Iteration 14660 Training loss 0.03631142899394035 Validation loss 0.046914391219615936 Accuracy 0.8717500567436218\n",
      "Iteration 14670 Training loss 0.03428957611322403 Validation loss 0.046686802059412 Accuracy 0.8720000386238098\n",
      "Iteration 14680 Training loss 0.03592037037014961 Validation loss 0.0456107072532177 Accuracy 0.877625048160553\n",
      "Iteration 14690 Training loss 0.040136124938726425 Validation loss 0.05272560194134712 Accuracy 0.8581250309944153\n",
      "Iteration 14700 Training loss 0.02879629097878933 Validation loss 0.04576212912797928 Accuracy 0.8761250376701355\n",
      "Iteration 14710 Training loss 0.04111485555768013 Validation loss 0.05175668001174927 Accuracy 0.8571250438690186\n",
      "Iteration 14720 Training loss 0.034162167459726334 Validation loss 0.0490066297352314 Accuracy 0.8652500510215759\n",
      "Iteration 14730 Training loss 0.03926028311252594 Validation loss 0.04789168760180473 Accuracy 0.8723750710487366\n",
      "Iteration 14740 Training loss 0.04321720823645592 Validation loss 0.04914155974984169 Accuracy 0.8641250133514404\n",
      "Iteration 14750 Training loss 0.04346412420272827 Validation loss 0.05832715705037117 Accuracy 0.8417500257492065\n",
      "Iteration 14760 Training loss 0.032073818147182465 Validation loss 0.04622000828385353 Accuracy 0.8765000700950623\n",
      "Iteration 14770 Training loss 0.04427146911621094 Validation loss 0.053255535662174225 Accuracy 0.8547500371932983\n",
      "Iteration 14780 Training loss 0.034500546753406525 Validation loss 0.045525774359703064 Accuracy 0.8767500519752502\n",
      "Iteration 14790 Training loss 0.04378865286707878 Validation loss 0.050763025879859924 Accuracy 0.8602500557899475\n",
      "Iteration 14800 Training loss 0.03929876163601875 Validation loss 0.04729100316762924 Accuracy 0.8735000491142273\n",
      "Iteration 14810 Training loss 0.03159424662590027 Validation loss 0.04941994696855545 Accuracy 0.8675000667572021\n",
      "Iteration 14820 Training loss 0.03663396090269089 Validation loss 0.046012431383132935 Accuracy 0.8732500672340393\n",
      "Iteration 14830 Training loss 0.037812940776348114 Validation loss 0.055929798632860184 Accuracy 0.8520000576972961\n",
      "Iteration 14840 Training loss 0.04364944249391556 Validation loss 0.05479724332690239 Accuracy 0.8538750410079956\n",
      "Iteration 14850 Training loss 0.04824643209576607 Validation loss 0.050836231559515 Accuracy 0.861875057220459\n",
      "Iteration 14860 Training loss 0.048806332051754 Validation loss 0.04594365879893303 Accuracy 0.8737500309944153\n",
      "Iteration 14870 Training loss 0.035156283527612686 Validation loss 0.04566600173711777 Accuracy 0.877625048160553\n",
      "Iteration 14880 Training loss 0.04118192568421364 Validation loss 0.053736329078674316 Accuracy 0.8540000319480896\n",
      "Iteration 14890 Training loss 0.04196777567267418 Validation loss 0.04872290417551994 Accuracy 0.8691250681877136\n",
      "Iteration 14900 Training loss 0.039585769176483154 Validation loss 0.05386575683951378 Accuracy 0.8565000295639038\n",
      "Iteration 14910 Training loss 0.04095511510968208 Validation loss 0.04675908014178276 Accuracy 0.8710000514984131\n",
      "Iteration 14920 Training loss 0.03916379436850548 Validation loss 0.04615020006895065 Accuracy 0.8717500567436218\n",
      "Iteration 14930 Training loss 0.035593897104263306 Validation loss 0.04586758092045784 Accuracy 0.8748750686645508\n",
      "Iteration 14940 Training loss 0.0435369499027729 Validation loss 0.05035188049077988 Accuracy 0.8626250624656677\n",
      "Iteration 14950 Training loss 0.034573279321193695 Validation loss 0.048645518720149994 Accuracy 0.8687500357627869\n",
      "Iteration 14960 Training loss 0.04152866452932358 Validation loss 0.04656706377863884 Accuracy 0.8731250166893005\n",
      "Iteration 14970 Training loss 0.03569408506155014 Validation loss 0.04497850686311722 Accuracy 0.877500057220459\n",
      "Iteration 14980 Training loss 0.03547058627009392 Validation loss 0.045198407024145126 Accuracy 0.8782500624656677\n",
      "Iteration 14990 Training loss 0.047810204327106476 Validation loss 0.049133770167827606 Accuracy 0.8653750419616699\n",
      "Iteration 15000 Training loss 0.038356199860572815 Validation loss 0.04523330554366112 Accuracy 0.8771250247955322\n",
      "Iteration 15010 Training loss 0.03981390967965126 Validation loss 0.045581817626953125 Accuracy 0.878000020980835\n",
      "Iteration 15020 Training loss 0.035724688321352005 Validation loss 0.04700689762830734 Accuracy 0.8711250424385071\n",
      "Iteration 15030 Training loss 0.02915235422551632 Validation loss 0.045453689992427826 Accuracy 0.8767500519752502\n",
      "Iteration 15040 Training loss 0.04539082571864128 Validation loss 0.04863358661532402 Accuracy 0.8691250681877136\n",
      "Iteration 15050 Training loss 0.034943629056215286 Validation loss 0.04691053926944733 Accuracy 0.874250054359436\n",
      "Iteration 15060 Training loss 0.043133143335580826 Validation loss 0.05000559613108635 Accuracy 0.8636250495910645\n",
      "Iteration 15070 Training loss 0.03449658304452896 Validation loss 0.04541714861989021 Accuracy 0.8752500414848328\n",
      "Iteration 15080 Training loss 0.045166753232479095 Validation loss 0.05711232125759125 Accuracy 0.843500018119812\n",
      "Iteration 15090 Training loss 0.04030889272689819 Validation loss 0.049166128039360046 Accuracy 0.8662500381469727\n",
      "Iteration 15100 Training loss 0.03451668098568916 Validation loss 0.04673762619495392 Accuracy 0.8718750476837158\n",
      "Iteration 15110 Training loss 0.040430039167404175 Validation loss 0.058254074305295944 Accuracy 0.843000054359436\n",
      "Iteration 15120 Training loss 0.04061231389641762 Validation loss 0.0503043606877327 Accuracy 0.8632500171661377\n",
      "Iteration 15130 Training loss 0.03161519020795822 Validation loss 0.048529982566833496 Accuracy 0.8696250319480896\n",
      "Iteration 15140 Training loss 0.035083889961242676 Validation loss 0.04769294336438179 Accuracy 0.8711250424385071\n",
      "Iteration 15150 Training loss 0.036251235753297806 Validation loss 0.04545244947075844 Accuracy 0.8772500157356262\n",
      "Iteration 15160 Training loss 0.03750554099678993 Validation loss 0.045272376388311386 Accuracy 0.8770000338554382\n",
      "Iteration 15170 Training loss 0.03278153017163277 Validation loss 0.04529610648751259 Accuracy 0.8766250610351562\n",
      "Iteration 15180 Training loss 0.037481021136045456 Validation loss 0.049942243844270706 Accuracy 0.862500011920929\n",
      "Iteration 15190 Training loss 0.03527132794260979 Validation loss 0.045724187046289444 Accuracy 0.8735000491142273\n",
      "Iteration 15200 Training loss 0.04074009880423546 Validation loss 0.049152955412864685 Accuracy 0.8635000586509705\n",
      "Iteration 15210 Training loss 0.045992057770490646 Validation loss 0.052035607397556305 Accuracy 0.8576250672340393\n",
      "Iteration 15220 Training loss 0.04297966882586479 Validation loss 0.05226050317287445 Accuracy 0.8576250672340393\n",
      "Iteration 15230 Training loss 0.0424455925822258 Validation loss 0.04943440482020378 Accuracy 0.8637500405311584\n",
      "Iteration 15240 Training loss 0.03499479219317436 Validation loss 0.04565310850739479 Accuracy 0.877500057220459\n",
      "Iteration 15250 Training loss 0.03440640866756439 Validation loss 0.0491148978471756 Accuracy 0.8681250214576721\n",
      "Iteration 15260 Training loss 0.03813459351658821 Validation loss 0.045889291912317276 Accuracy 0.877500057220459\n",
      "Iteration 15270 Training loss 0.03731480613350868 Validation loss 0.04726064205169678 Accuracy 0.8716250658035278\n",
      "Iteration 15280 Training loss 0.04746808484196663 Validation loss 0.0479980930685997 Accuracy 0.8707500696182251\n",
      "Iteration 15290 Training loss 0.03548470139503479 Validation loss 0.04540793225169182 Accuracy 0.877500057220459\n",
      "Iteration 15300 Training loss 0.03844018280506134 Validation loss 0.04571687430143356 Accuracy 0.8752500414848328\n",
      "Iteration 15310 Training loss 0.03572889417409897 Validation loss 0.04785120487213135 Accuracy 0.8687500357627869\n",
      "Iteration 15320 Training loss 0.040787000209093094 Validation loss 0.049031928181648254 Accuracy 0.8641250133514404\n",
      "Iteration 15330 Training loss 0.04200774058699608 Validation loss 0.05010489746928215 Accuracy 0.8662500381469727\n",
      "Iteration 15340 Training loss 0.03348635509610176 Validation loss 0.04672392085194588 Accuracy 0.874750018119812\n",
      "Iteration 15350 Training loss 0.03442215546965599 Validation loss 0.04756338894367218 Accuracy 0.8705000281333923\n",
      "Iteration 15360 Training loss 0.03534192964434624 Validation loss 0.04533911868929863 Accuracy 0.8763750195503235\n",
      "Iteration 15370 Training loss 0.030844945460557938 Validation loss 0.04666408151388168 Accuracy 0.8740000128746033\n",
      "Iteration 15380 Training loss 0.027954690158367157 Validation loss 0.04616871848702431 Accuracy 0.8757500648498535\n",
      "Iteration 15390 Training loss 0.039747800678014755 Validation loss 0.045012008398771286 Accuracy 0.8763750195503235\n",
      "Iteration 15400 Training loss 0.03538581356406212 Validation loss 0.04672473669052124 Accuracy 0.8721250295639038\n",
      "Iteration 15410 Training loss 0.031007694080471992 Validation loss 0.04826999083161354 Accuracy 0.8677500486373901\n",
      "Iteration 15420 Training loss 0.03836752846837044 Validation loss 0.048181917518377304 Accuracy 0.8687500357627869\n",
      "Iteration 15430 Training loss 0.029042068868875504 Validation loss 0.04492785781621933 Accuracy 0.877625048160553\n",
      "Iteration 15440 Training loss 0.031720127910375595 Validation loss 0.047660693526268005 Accuracy 0.8740000128746033\n",
      "Iteration 15450 Training loss 0.05244962126016617 Validation loss 0.05332934856414795 Accuracy 0.8560000658035278\n",
      "Iteration 15460 Training loss 0.040261294692754745 Validation loss 0.045373160392045975 Accuracy 0.8751250505447388\n",
      "Iteration 15470 Training loss 0.03731023147702217 Validation loss 0.048162251710891724 Accuracy 0.8687500357627869\n",
      "Iteration 15480 Training loss 0.03817927464842796 Validation loss 0.05034700036048889 Accuracy 0.8607500195503235\n",
      "Iteration 15490 Training loss 0.033210285007953644 Validation loss 0.045058853924274445 Accuracy 0.8765000700950623\n",
      "Iteration 15500 Training loss 0.06733911484479904 Validation loss 0.06820439547300339 Accuracy 0.815250039100647\n",
      "Iteration 15510 Training loss 0.02835088223218918 Validation loss 0.04785418510437012 Accuracy 0.8698750138282776\n",
      "Iteration 15520 Training loss 0.03574475646018982 Validation loss 0.046655431389808655 Accuracy 0.8727500438690186\n",
      "Iteration 15530 Training loss 0.03856974095106125 Validation loss 0.04575507715344429 Accuracy 0.8767500519752502\n",
      "Iteration 15540 Training loss 0.02984943799674511 Validation loss 0.04499194771051407 Accuracy 0.878000020980835\n",
      "Iteration 15550 Training loss 0.033991359174251556 Validation loss 0.0455927811563015 Accuracy 0.8765000700950623\n",
      "Iteration 15560 Training loss 0.04124787077307701 Validation loss 0.049550123512744904 Accuracy 0.8665000200271606\n",
      "Iteration 15570 Training loss 0.032730650156736374 Validation loss 0.04959417134523392 Accuracy 0.8667500615119934\n",
      "Iteration 15580 Training loss 0.03432950749993324 Validation loss 0.046842072159051895 Accuracy 0.8730000257492065\n",
      "Iteration 15590 Training loss 0.03501411899924278 Validation loss 0.04581979289650917 Accuracy 0.877500057220459\n",
      "Iteration 15600 Training loss 0.031100640073418617 Validation loss 0.04611407220363617 Accuracy 0.8755000233650208\n",
      "Iteration 15610 Training loss 0.038537025451660156 Validation loss 0.05243014544248581 Accuracy 0.8570000529289246\n",
      "Iteration 15620 Training loss 0.04550561308860779 Validation loss 0.046486034989356995 Accuracy 0.8728750348091125\n",
      "Iteration 15630 Training loss 0.026140620931982994 Validation loss 0.045728616416454315 Accuracy 0.877500057220459\n",
      "Iteration 15640 Training loss 0.03622596338391304 Validation loss 0.047613468021154404 Accuracy 0.8717500567436218\n",
      "Iteration 15650 Training loss 0.042327880859375 Validation loss 0.04519513249397278 Accuracy 0.8761250376701355\n",
      "Iteration 15660 Training loss 0.0369642972946167 Validation loss 0.04660652205348015 Accuracy 0.8737500309944153\n",
      "Iteration 15670 Training loss 0.03849285840988159 Validation loss 0.04925614222884178 Accuracy 0.8668750524520874\n",
      "Iteration 15680 Training loss 0.031478505581617355 Validation loss 0.05436684563755989 Accuracy 0.8536250591278076\n",
      "Iteration 15690 Training loss 0.03452127054333687 Validation loss 0.0484800785779953 Accuracy 0.8683750629425049\n",
      "Iteration 15700 Training loss 0.03553866967558861 Validation loss 0.04540941119194031 Accuracy 0.874625027179718\n",
      "Iteration 15710 Training loss 0.039527129381895065 Validation loss 0.05115651711821556 Accuracy 0.8582500219345093\n",
      "Iteration 15720 Training loss 0.04448902979493141 Validation loss 0.04725171998143196 Accuracy 0.8723750710487366\n",
      "Iteration 15730 Training loss 0.032054148614406586 Validation loss 0.04551195353269577 Accuracy 0.8770000338554382\n",
      "Iteration 15740 Training loss 0.03182809427380562 Validation loss 0.045115262269973755 Accuracy 0.8763750195503235\n",
      "Iteration 15750 Training loss 0.03430384770035744 Validation loss 0.04572776332497597 Accuracy 0.8733750581741333\n",
      "Iteration 15760 Training loss 0.039051733911037445 Validation loss 0.044823095202445984 Accuracy 0.877750039100647\n",
      "Iteration 15770 Training loss 0.032795898616313934 Validation loss 0.04614204540848732 Accuracy 0.8770000338554382\n",
      "Iteration 15780 Training loss 0.04368051514029503 Validation loss 0.049600981175899506 Accuracy 0.862250030040741\n",
      "Iteration 15790 Training loss 0.03709050267934799 Validation loss 0.045319974422454834 Accuracy 0.87437504529953\n",
      "Iteration 15800 Training loss 0.02967856265604496 Validation loss 0.045242808759212494 Accuracy 0.8791250586509705\n",
      "Iteration 15810 Training loss 0.05151167884469032 Validation loss 0.058076635003089905 Accuracy 0.8447500467300415\n",
      "Iteration 15820 Training loss 0.03358713909983635 Validation loss 0.045246049761772156 Accuracy 0.877375066280365\n",
      "Iteration 15830 Training loss 0.03615086153149605 Validation loss 0.0487980917096138 Accuracy 0.8681250214576721\n",
      "Iteration 15840 Training loss 0.036648306995630264 Validation loss 0.046333931386470795 Accuracy 0.8731250166893005\n",
      "Iteration 15850 Training loss 0.04529637098312378 Validation loss 0.04935750365257263 Accuracy 0.8665000200271606\n",
      "Iteration 15860 Training loss 0.03614160045981407 Validation loss 0.04498707130551338 Accuracy 0.8770000338554382\n",
      "Iteration 15870 Training loss 0.036460526287555695 Validation loss 0.04829349368810654 Accuracy 0.8667500615119934\n",
      "Iteration 15880 Training loss 0.033122796565294266 Validation loss 0.048813242465257645 Accuracy 0.8650000691413879\n",
      "Iteration 15890 Training loss 0.03533143550157547 Validation loss 0.04557689651846886 Accuracy 0.8786250352859497\n",
      "Iteration 15900 Training loss 0.034822702407836914 Validation loss 0.051535241305828094 Accuracy 0.858875036239624\n",
      "Iteration 15910 Training loss 0.05239187926054001 Validation loss 0.0560930073261261 Accuracy 0.846750020980835\n",
      "Iteration 15920 Training loss 0.035020556300878525 Validation loss 0.04529445245862007 Accuracy 0.8760000467300415\n",
      "Iteration 15930 Training loss 0.036849286407232285 Validation loss 0.044893331825733185 Accuracy 0.8767500519752502\n",
      "Iteration 15940 Training loss 0.032915856689214706 Validation loss 0.0466373935341835 Accuracy 0.8716250658035278\n",
      "Iteration 15950 Training loss 0.0472312867641449 Validation loss 0.05351749062538147 Accuracy 0.8552500605583191\n",
      "Iteration 15960 Training loss 0.04241354763507843 Validation loss 0.04599977284669876 Accuracy 0.8732500672340393\n",
      "Iteration 15970 Training loss 0.04124043881893158 Validation loss 0.050816696137189865 Accuracy 0.8616250157356262\n",
      "Iteration 15980 Training loss 0.03424932435154915 Validation loss 0.046837471425533295 Accuracy 0.8720000386238098\n",
      "Iteration 15990 Training loss 0.03252485767006874 Validation loss 0.04862160608172417 Accuracy 0.8708750605583191\n",
      "Iteration 16000 Training loss 0.045815400779247284 Validation loss 0.050599437206983566 Accuracy 0.8645000457763672\n",
      "Iteration 16010 Training loss 0.03275773301720619 Validation loss 0.045073408633470535 Accuracy 0.878125011920929\n",
      "Iteration 16020 Training loss 0.0340048149228096 Validation loss 0.047014687210321426 Accuracy 0.8757500648498535\n",
      "Iteration 16030 Training loss 0.0343870110809803 Validation loss 0.045453671365976334 Accuracy 0.8767500519752502\n",
      "Iteration 16040 Training loss 0.039921656250953674 Validation loss 0.04798121750354767 Accuracy 0.8685000538825989\n",
      "Iteration 16050 Training loss 0.03797324001789093 Validation loss 0.048708826303482056 Accuracy 0.8667500615119934\n",
      "Iteration 16060 Training loss 0.03873671218752861 Validation loss 0.051379863172769547 Accuracy 0.8631250262260437\n",
      "Iteration 16070 Training loss 0.04039553552865982 Validation loss 0.045015331357717514 Accuracy 0.8761250376701355\n",
      "Iteration 16080 Training loss 0.027847599238157272 Validation loss 0.04598471149802208 Accuracy 0.877375066280365\n",
      "Iteration 16090 Training loss 0.04630264267325401 Validation loss 0.04687529802322388 Accuracy 0.8726250529289246\n",
      "Iteration 16100 Training loss 0.029178189113736153 Validation loss 0.04526602476835251 Accuracy 0.8765000700950623\n",
      "Iteration 16110 Training loss 0.04086875915527344 Validation loss 0.04520707204937935 Accuracy 0.8756250143051147\n",
      "Iteration 16120 Training loss 0.03281201794743538 Validation loss 0.047988757491111755 Accuracy 0.8701250553131104\n",
      "Iteration 16130 Training loss 0.03426368162035942 Validation loss 0.047653790563344955 Accuracy 0.8697500228881836\n",
      "Iteration 16140 Training loss 0.04300310090184212 Validation loss 0.048053544014692307 Accuracy 0.8682500123977661\n",
      "Iteration 16150 Training loss 0.032923176884651184 Validation loss 0.04748936742544174 Accuracy 0.8723750710487366\n",
      "Iteration 16160 Training loss 0.03802996873855591 Validation loss 0.04507721588015556 Accuracy 0.8767500519752502\n",
      "Iteration 16170 Training loss 0.04377662390470505 Validation loss 0.05272502452135086 Accuracy 0.8552500605583191\n",
      "Iteration 16180 Training loss 0.02973940782248974 Validation loss 0.04544086009263992 Accuracy 0.874750018119812\n",
      "Iteration 16190 Training loss 0.03564058616757393 Validation loss 0.04924248158931732 Accuracy 0.8686250448226929\n",
      "Iteration 16200 Training loss 0.04402025043964386 Validation loss 0.04760714992880821 Accuracy 0.8697500228881836\n",
      "Iteration 16210 Training loss 0.051288262009620667 Validation loss 0.05524660646915436 Accuracy 0.8498750329017639\n",
      "Iteration 16220 Training loss 0.04062148183584213 Validation loss 0.04755976051092148 Accuracy 0.8691250681877136\n",
      "Iteration 16230 Training loss 0.047927550971508026 Validation loss 0.04816592484712601 Accuracy 0.8660000562667847\n",
      "Iteration 16240 Training loss 0.03394228219985962 Validation loss 0.04474327713251114 Accuracy 0.8785000443458557\n",
      "Iteration 16250 Training loss 0.03863055258989334 Validation loss 0.049720343202352524 Accuracy 0.8630000352859497\n",
      "Iteration 16260 Training loss 0.04119380936026573 Validation loss 0.05587472766637802 Accuracy 0.8475000262260437\n",
      "Iteration 16270 Training loss 0.042703013867139816 Validation loss 0.04507790505886078 Accuracy 0.8762500286102295\n",
      "Iteration 16280 Training loss 0.03403785452246666 Validation loss 0.046387676149606705 Accuracy 0.8721250295639038\n",
      "Iteration 16290 Training loss 0.03271447494626045 Validation loss 0.050537239760160446 Accuracy 0.862250030040741\n",
      "Iteration 16300 Training loss 0.04690270498394966 Validation loss 0.060478243976831436 Accuracy 0.8340000510215759\n",
      "Iteration 16310 Training loss 0.02650303766131401 Validation loss 0.04622361809015274 Accuracy 0.8758750557899475\n",
      "Iteration 16320 Training loss 0.02940545417368412 Validation loss 0.04495924711227417 Accuracy 0.8766250610351562\n",
      "Iteration 16330 Training loss 0.033758483827114105 Validation loss 0.04762646555900574 Accuracy 0.8690000176429749\n",
      "Iteration 16340 Training loss 0.036809176206588745 Validation loss 0.05144941061735153 Accuracy 0.862375020980835\n",
      "Iteration 16350 Training loss 0.03436829522252083 Validation loss 0.045018937438726425 Accuracy 0.8763750195503235\n",
      "Iteration 16360 Training loss 0.029477467760443687 Validation loss 0.044993456453084946 Accuracy 0.877750039100647\n",
      "Iteration 16370 Training loss 0.0407881885766983 Validation loss 0.045294709503650665 Accuracy 0.8753750324249268\n",
      "Iteration 16380 Training loss 0.0377935916185379 Validation loss 0.045830659568309784 Accuracy 0.8740000128746033\n",
      "Iteration 16390 Training loss 0.038697127252817154 Validation loss 0.04593158885836601 Accuracy 0.8748750686645508\n",
      "Iteration 16400 Training loss 0.036903269588947296 Validation loss 0.04785744845867157 Accuracy 0.8710000514984131\n",
      "Iteration 16410 Training loss 0.047574955970048904 Validation loss 0.05338696390390396 Accuracy 0.8571250438690186\n",
      "Iteration 16420 Training loss 0.02995242550969124 Validation loss 0.045819640159606934 Accuracy 0.8760000467300415\n",
      "Iteration 16430 Training loss 0.052474744617938995 Validation loss 0.05131565406918526 Accuracy 0.8596250414848328\n",
      "Iteration 16440 Training loss 0.03593321889638901 Validation loss 0.045902058482170105 Accuracy 0.8736250400543213\n",
      "Iteration 16450 Training loss 0.030451809987425804 Validation loss 0.0453236848115921 Accuracy 0.8782500624656677\n",
      "Iteration 16460 Training loss 0.0365084744989872 Validation loss 0.048203181475400925 Accuracy 0.8693750500679016\n",
      "Iteration 16470 Training loss 0.0380621999502182 Validation loss 0.0450742281973362 Accuracy 0.8771250247955322\n",
      "Iteration 16480 Training loss 0.04312220960855484 Validation loss 0.050039827823638916 Accuracy 0.8628750443458557\n",
      "Iteration 16490 Training loss 0.031165868043899536 Validation loss 0.04660586640238762 Accuracy 0.874500036239624\n",
      "Iteration 16500 Training loss 0.034061986953020096 Validation loss 0.044698458164930344 Accuracy 0.8786250352859497\n",
      "Iteration 16510 Training loss 0.04516330733895302 Validation loss 0.047582272440195084 Accuracy 0.8712500333786011\n",
      "Iteration 16520 Training loss 0.03968106210231781 Validation loss 0.045113589614629745 Accuracy 0.8796250224113464\n",
      "Iteration 16530 Training loss 0.040910616517066956 Validation loss 0.04443124681711197 Accuracy 0.8797500133514404\n",
      "Iteration 16540 Training loss 0.03331638127565384 Validation loss 0.04770487919449806 Accuracy 0.8702500462532043\n",
      "Iteration 16550 Training loss 0.03771816939115524 Validation loss 0.04613371193408966 Accuracy 0.8735000491142273\n",
      "Iteration 16560 Training loss 0.030169889330863953 Validation loss 0.046973247081041336 Accuracy 0.8716250658035278\n",
      "Iteration 16570 Training loss 0.04732806235551834 Validation loss 0.049441706389188766 Accuracy 0.8661250472068787\n",
      "Iteration 16580 Training loss 0.03221745043992996 Validation loss 0.047112055122852325 Accuracy 0.8723750710487366\n",
      "Iteration 16590 Training loss 0.03435206040740013 Validation loss 0.04552916809916496 Accuracy 0.8756250143051147\n",
      "Iteration 16600 Training loss 0.037040140479803085 Validation loss 0.04509221017360687 Accuracy 0.8755000233650208\n",
      "Iteration 16610 Training loss 0.0371021069586277 Validation loss 0.045301035046577454 Accuracy 0.877750039100647\n",
      "Iteration 16620 Training loss 0.041848257184028625 Validation loss 0.050311680883169174 Accuracy 0.8626250624656677\n",
      "Iteration 16630 Training loss 0.040847163647413254 Validation loss 0.05043322220444679 Accuracy 0.8633750677108765\n",
      "Iteration 16640 Training loss 0.03764432668685913 Validation loss 0.047865767031908035 Accuracy 0.8698750138282776\n",
      "Iteration 16650 Training loss 0.03552006557583809 Validation loss 0.048511214554309845 Accuracy 0.8690000176429749\n",
      "Iteration 16660 Training loss 0.039560068398714066 Validation loss 0.04890421777963638 Accuracy 0.8691250681877136\n",
      "Iteration 16670 Training loss 0.0361592173576355 Validation loss 0.0446166954934597 Accuracy 0.8787500262260437\n",
      "Iteration 16680 Training loss 0.04252907633781433 Validation loss 0.04444842413067818 Accuracy 0.8793750405311584\n",
      "Iteration 16690 Training loss 0.031207919120788574 Validation loss 0.04587212949991226 Accuracy 0.8768750429153442\n",
      "Iteration 16700 Training loss 0.04665954411029816 Validation loss 0.049233272671699524 Accuracy 0.8670000433921814\n",
      "Iteration 16710 Training loss 0.03898431733250618 Validation loss 0.04508258029818535 Accuracy 0.8766250610351562\n",
      "Iteration 16720 Training loss 0.03453477472066879 Validation loss 0.045955099165439606 Accuracy 0.8752500414848328\n",
      "Iteration 16730 Training loss 0.05586250498890877 Validation loss 0.054233428090810776 Accuracy 0.8517500162124634\n",
      "Iteration 16740 Training loss 0.029608629643917084 Validation loss 0.04493625462055206 Accuracy 0.8750000596046448\n",
      "Iteration 16750 Training loss 0.03690151125192642 Validation loss 0.04518213868141174 Accuracy 0.8783750534057617\n",
      "Iteration 16760 Training loss 0.03872346132993698 Validation loss 0.05195339396595955 Accuracy 0.8608750700950623\n",
      "Iteration 16770 Training loss 0.032238900661468506 Validation loss 0.04736270755529404 Accuracy 0.8725000619888306\n",
      "Iteration 16780 Training loss 0.031534649431705475 Validation loss 0.045810312032699585 Accuracy 0.8758750557899475\n",
      "Iteration 16790 Training loss 0.04090723395347595 Validation loss 0.045230682939291 Accuracy 0.87437504529953\n",
      "Iteration 16800 Training loss 0.0319066047668457 Validation loss 0.044943131506443024 Accuracy 0.8806250691413879\n",
      "Iteration 16810 Training loss 0.032457996159791946 Validation loss 0.04435988888144493 Accuracy 0.8793750405311584\n",
      "Iteration 16820 Training loss 0.037699759006500244 Validation loss 0.047606345266103745 Accuracy 0.8703750371932983\n",
      "Iteration 16830 Training loss 0.03616548329591751 Validation loss 0.05148584395647049 Accuracy 0.8570000529289246\n",
      "Iteration 16840 Training loss 0.03345831483602524 Validation loss 0.05080006644129753 Accuracy 0.862000048160553\n",
      "Iteration 16850 Training loss 0.041329726576805115 Validation loss 0.053686242550611496 Accuracy 0.8522500395774841\n",
      "Iteration 16860 Training loss 0.03829235956072807 Validation loss 0.04501882568001747 Accuracy 0.877375066280365\n",
      "Iteration 16870 Training loss 0.042891643941402435 Validation loss 0.048814430832862854 Accuracy 0.8685000538825989\n",
      "Iteration 16880 Training loss 0.03117072582244873 Validation loss 0.046173784881830215 Accuracy 0.8737500309944153\n",
      "Iteration 16890 Training loss 0.031512532383203506 Validation loss 0.04473193734884262 Accuracy 0.8765000700950623\n",
      "Iteration 16900 Training loss 0.03377370536327362 Validation loss 0.04452117905020714 Accuracy 0.8767500519752502\n",
      "Iteration 16910 Training loss 0.03775123879313469 Validation loss 0.050098855048418045 Accuracy 0.8646250367164612\n",
      "Iteration 16920 Training loss 0.05388488993048668 Validation loss 0.058369118720293045 Accuracy 0.84312504529953\n",
      "Iteration 16930 Training loss 0.034646593034267426 Validation loss 0.04578869789838791 Accuracy 0.8765000700950623\n",
      "Iteration 16940 Training loss 0.03441048040986061 Validation loss 0.04476076737046242 Accuracy 0.877375066280365\n",
      "Iteration 16950 Training loss 0.02611391805112362 Validation loss 0.04455587640404701 Accuracy 0.8782500624656677\n",
      "Iteration 16960 Training loss 0.03073476254940033 Validation loss 0.04467272013425827 Accuracy 0.8797500133514404\n",
      "Iteration 16970 Training loss 0.03267431631684303 Validation loss 0.04532519355416298 Accuracy 0.8762500286102295\n",
      "Iteration 16980 Training loss 0.0388413742184639 Validation loss 0.04683559760451317 Accuracy 0.8752500414848328\n",
      "Iteration 16990 Training loss 0.03726360946893692 Validation loss 0.044629380106925964 Accuracy 0.8782500624656677\n",
      "Iteration 17000 Training loss 0.026635903865098953 Validation loss 0.04674432799220085 Accuracy 0.8722500205039978\n",
      "Iteration 17010 Training loss 0.044771067798137665 Validation loss 0.0564681738615036 Accuracy 0.8485000133514404\n",
      "Iteration 17020 Training loss 0.0470808781683445 Validation loss 0.05724148824810982 Accuracy 0.8441250324249268\n",
      "Iteration 17030 Training loss 0.03403899073600769 Validation loss 0.04566697031259537 Accuracy 0.8766250610351562\n",
      "Iteration 17040 Training loss 0.03562763333320618 Validation loss 0.04885847494006157 Accuracy 0.8696250319480896\n",
      "Iteration 17050 Training loss 0.04209009185433388 Validation loss 0.04920465871691704 Accuracy 0.8656250238418579\n",
      "Iteration 17060 Training loss 0.03252784535288811 Validation loss 0.04486162215471268 Accuracy 0.8752500414848328\n",
      "Iteration 17070 Training loss 0.029757291078567505 Validation loss 0.0446573831140995 Accuracy 0.8760000467300415\n",
      "Iteration 17080 Training loss 0.02807093970477581 Validation loss 0.0442170649766922 Accuracy 0.877500057220459\n",
      "Iteration 17090 Training loss 0.041682448238134384 Validation loss 0.0460781566798687 Accuracy 0.8762500286102295\n",
      "Iteration 17100 Training loss 0.03138573467731476 Validation loss 0.044465068727731705 Accuracy 0.8782500624656677\n",
      "Iteration 17110 Training loss 0.034404899924993515 Validation loss 0.045741043984889984 Accuracy 0.8748750686645508\n",
      "Iteration 17120 Training loss 0.033082250505685806 Validation loss 0.04549849405884743 Accuracy 0.8738750219345093\n",
      "Iteration 17130 Training loss 0.032527897506952286 Validation loss 0.04913262277841568 Accuracy 0.8677500486373901\n",
      "Iteration 17140 Training loss 0.038325682282447815 Validation loss 0.04924766719341278 Accuracy 0.8670000433921814\n",
      "Iteration 17150 Training loss 0.03950970619916916 Validation loss 0.05311160534620285 Accuracy 0.8566250205039978\n",
      "Iteration 17160 Training loss 0.043726060539484024 Validation loss 0.05155796930193901 Accuracy 0.859125018119812\n",
      "Iteration 17170 Training loss 0.034297313541173935 Validation loss 0.04422851651906967 Accuracy 0.8793750405311584\n",
      "Iteration 17180 Training loss 0.040779970586299896 Validation loss 0.050878625363111496 Accuracy 0.8611250519752502\n",
      "Iteration 17190 Training loss 0.047104090452194214 Validation loss 0.0535690002143383 Accuracy 0.8528750538825989\n",
      "Iteration 17200 Training loss 0.04965195432305336 Validation loss 0.05109821632504463 Accuracy 0.862000048160553\n",
      "Iteration 17210 Training loss 0.033093955367803574 Validation loss 0.045378077775239944 Accuracy 0.877875030040741\n",
      "Iteration 17220 Training loss 0.03228112310171127 Validation loss 0.04493023082613945 Accuracy 0.8797500133514404\n",
      "Iteration 17230 Training loss 0.0360146127641201 Validation loss 0.04889566823840141 Accuracy 0.8667500615119934\n",
      "Iteration 17240 Training loss 0.044076986610889435 Validation loss 0.04668029025197029 Accuracy 0.8723750710487366\n",
      "Iteration 17250 Training loss 0.03290357068181038 Validation loss 0.04476413130760193 Accuracy 0.8767500519752502\n",
      "Iteration 17260 Training loss 0.027625173330307007 Validation loss 0.04475348815321922 Accuracy 0.877625048160553\n",
      "Iteration 17270 Training loss 0.02577662654221058 Validation loss 0.04657416790723801 Accuracy 0.8732500672340393\n",
      "Iteration 17280 Training loss 0.02881033346056938 Validation loss 0.04399767890572548 Accuracy 0.8790000677108765\n",
      "Iteration 17290 Training loss 0.02631714567542076 Validation loss 0.04486599937081337 Accuracy 0.8760000467300415\n",
      "Iteration 17300 Training loss 0.02691403403878212 Validation loss 0.04411690682172775 Accuracy 0.8782500624656677\n",
      "Iteration 17310 Training loss 0.04238664358854294 Validation loss 0.053500719368457794 Accuracy 0.8566250205039978\n",
      "Iteration 17320 Training loss 0.032657478004693985 Validation loss 0.046145230531692505 Accuracy 0.8750000596046448\n",
      "Iteration 17330 Training loss 0.03960787132382393 Validation loss 0.04527905583381653 Accuracy 0.8758750557899475\n",
      "Iteration 17340 Training loss 0.030866103246808052 Validation loss 0.05011816695332527 Accuracy 0.8660000562667847\n",
      "Iteration 17350 Training loss 0.0394473671913147 Validation loss 0.04499518498778343 Accuracy 0.878125011920929\n",
      "Iteration 17360 Training loss 0.027087630704045296 Validation loss 0.04431130364537239 Accuracy 0.878000020980835\n",
      "Iteration 17370 Training loss 0.02384200133383274 Validation loss 0.04502861574292183 Accuracy 0.877875030040741\n",
      "Iteration 17380 Training loss 0.03342786058783531 Validation loss 0.045020315796136856 Accuracy 0.877625048160553\n",
      "Iteration 17390 Training loss 0.03214770928025246 Validation loss 0.04720306769013405 Accuracy 0.8716250658035278\n",
      "Iteration 17400 Training loss 0.03149614483118057 Validation loss 0.045160938054323196 Accuracy 0.8786250352859497\n",
      "Iteration 17410 Training loss 0.033184491097927094 Validation loss 0.05113688111305237 Accuracy 0.8633750677108765\n",
      "Iteration 17420 Training loss 0.03181004151701927 Validation loss 0.04842372238636017 Accuracy 0.8708750605583191\n",
      "Iteration 17430 Training loss 0.03640389442443848 Validation loss 0.046129029244184494 Accuracy 0.8763750195503235\n",
      "Iteration 17440 Training loss 0.03941207379102707 Validation loss 0.044939957559108734 Accuracy 0.878000020980835\n",
      "Iteration 17450 Training loss 0.03007475845515728 Validation loss 0.04796377196907997 Accuracy 0.8701250553131104\n",
      "Iteration 17460 Training loss 0.036127928644418716 Validation loss 0.04423227906227112 Accuracy 0.8810000419616699\n",
      "Iteration 17470 Training loss 0.032793086022138596 Validation loss 0.044719088822603226 Accuracy 0.8766250610351562\n",
      "Iteration 17480 Training loss 0.025952624157071114 Validation loss 0.044189196079969406 Accuracy 0.8791250586509705\n",
      "Iteration 17490 Training loss 0.03994639962911606 Validation loss 0.045750536024570465 Accuracy 0.8732500672340393\n",
      "Iteration 17500 Training loss 0.037775129079818726 Validation loss 0.04403942450881004 Accuracy 0.8786250352859497\n",
      "Iteration 17510 Training loss 0.031819041818380356 Validation loss 0.05003298074007034 Accuracy 0.8655000329017639\n",
      "Iteration 17520 Training loss 0.043809808790683746 Validation loss 0.05242898687720299 Accuracy 0.8595000505447388\n",
      "Iteration 17530 Training loss 0.03207811340689659 Validation loss 0.0444609671831131 Accuracy 0.8786250352859497\n",
      "Iteration 17540 Training loss 0.04080042615532875 Validation loss 0.04996970668435097 Accuracy 0.8653750419616699\n",
      "Iteration 17550 Training loss 0.042399611324071884 Validation loss 0.05449061468243599 Accuracy 0.8540000319480896\n",
      "Iteration 17560 Training loss 0.04225023090839386 Validation loss 0.049331191927194595 Accuracy 0.8697500228881836\n",
      "Iteration 17570 Training loss 0.034810591489076614 Validation loss 0.04947168380022049 Accuracy 0.8673750162124634\n",
      "Iteration 17580 Training loss 0.03626579791307449 Validation loss 0.04445425793528557 Accuracy 0.8770000338554382\n",
      "Iteration 17590 Training loss 0.026936722919344902 Validation loss 0.04456479102373123 Accuracy 0.8785000443458557\n",
      "Iteration 17600 Training loss 0.031866565346717834 Validation loss 0.044393010437488556 Accuracy 0.8788750171661377\n",
      "Iteration 17610 Training loss 0.0422343909740448 Validation loss 0.04880017414689064 Accuracy 0.8680000305175781\n",
      "Iteration 17620 Training loss 0.034560512751340866 Validation loss 0.04528282582759857 Accuracy 0.8763750195503235\n",
      "Iteration 17630 Training loss 0.03497282788157463 Validation loss 0.04896315559744835 Accuracy 0.8687500357627869\n",
      "Iteration 17640 Training loss 0.04023394361138344 Validation loss 0.052789356559515 Accuracy 0.8558750152587891\n",
      "Iteration 17650 Training loss 0.03419908136129379 Validation loss 0.044469576328992844 Accuracy 0.8791250586509705\n",
      "Iteration 17660 Training loss 0.03359517827630043 Validation loss 0.04402024298906326 Accuracy 0.8802500367164612\n",
      "Iteration 17670 Training loss 0.04837453365325928 Validation loss 0.056120507419109344 Accuracy 0.8496250510215759\n",
      "Iteration 17680 Training loss 0.03152481094002724 Validation loss 0.044508278369903564 Accuracy 0.877375066280365\n",
      "Iteration 17690 Training loss 0.03932072967290878 Validation loss 0.052365876734256744 Accuracy 0.8595000505447388\n",
      "Iteration 17700 Training loss 0.03288785368204117 Validation loss 0.04528450593352318 Accuracy 0.8791250586509705\n",
      "Iteration 17710 Training loss 0.023894796147942543 Validation loss 0.0442451648414135 Accuracy 0.877625048160553\n",
      "Iteration 17720 Training loss 0.06386198848485947 Validation loss 0.06680021435022354 Accuracy 0.8202500343322754\n",
      "Iteration 17730 Training loss 0.04299112781882286 Validation loss 0.04881209135055542 Accuracy 0.8696250319480896\n",
      "Iteration 17740 Training loss 0.029496321454644203 Validation loss 0.04460271820425987 Accuracy 0.8783750534057617\n",
      "Iteration 17750 Training loss 0.03677675873041153 Validation loss 0.046207450330257416 Accuracy 0.87437504529953\n",
      "Iteration 17760 Training loss 0.035126592963933945 Validation loss 0.04791351407766342 Accuracy 0.8676250576972961\n",
      "Iteration 17770 Training loss 0.028221098706126213 Validation loss 0.04530073702335358 Accuracy 0.8765000700950623\n",
      "Iteration 17780 Training loss 0.05069206282496452 Validation loss 0.0538247711956501 Accuracy 0.8528750538825989\n",
      "Iteration 17790 Training loss 0.039368886500597 Validation loss 0.04522119462490082 Accuracy 0.8761250376701355\n",
      "Iteration 17800 Training loss 0.035921331495046616 Validation loss 0.04423565790057182 Accuracy 0.8783750534057617\n",
      "Iteration 17810 Training loss 0.03481011465191841 Validation loss 0.047601036727428436 Accuracy 0.8735000491142273\n",
      "Iteration 17820 Training loss 0.03924432024359703 Validation loss 0.044565338641405106 Accuracy 0.877875030040741\n",
      "Iteration 17830 Training loss 0.03534109145402908 Validation loss 0.044172629714012146 Accuracy 0.8797500133514404\n",
      "Iteration 17840 Training loss 0.02776745706796646 Validation loss 0.044291481375694275 Accuracy 0.8772500157356262\n",
      "Iteration 17850 Training loss 0.035355210304260254 Validation loss 0.04423585906624794 Accuracy 0.8788750171661377\n",
      "Iteration 17860 Training loss 0.03102400153875351 Validation loss 0.049794431775808334 Accuracy 0.8640000224113464\n",
      "Iteration 17870 Training loss 0.038907166570425034 Validation loss 0.04610949754714966 Accuracy 0.8740000128746033\n",
      "Iteration 17880 Training loss 0.03574131801724434 Validation loss 0.045631010085344315 Accuracy 0.874625027179718\n",
      "Iteration 17890 Training loss 0.038415975868701935 Validation loss 0.04854166880249977 Accuracy 0.8676250576972961\n",
      "Iteration 17900 Training loss 0.034305669367313385 Validation loss 0.04687671363353729 Accuracy 0.8717500567436218\n",
      "Iteration 17910 Training loss 0.03971783444285393 Validation loss 0.05366266891360283 Accuracy 0.8547500371932983\n",
      "Iteration 17920 Training loss 0.029114441946148872 Validation loss 0.04448612406849861 Accuracy 0.877500057220459\n",
      "Iteration 17930 Training loss 0.0299560334533453 Validation loss 0.04539310559630394 Accuracy 0.8768750429153442\n",
      "Iteration 17940 Training loss 0.03606563061475754 Validation loss 0.04584618657827377 Accuracy 0.8750000596046448\n",
      "Iteration 17950 Training loss 0.03282754868268967 Validation loss 0.0485224649310112 Accuracy 0.8698750138282776\n",
      "Iteration 17960 Training loss 0.027157563716173172 Validation loss 0.04561159387230873 Accuracy 0.8795000314712524\n",
      "Iteration 17970 Training loss 0.02898109331727028 Validation loss 0.04429776966571808 Accuracy 0.8783750534057617\n",
      "Iteration 17980 Training loss 0.03330094367265701 Validation loss 0.044453542679548264 Accuracy 0.8786250352859497\n",
      "Iteration 17990 Training loss 0.041246261447668076 Validation loss 0.04926251992583275 Accuracy 0.8670000433921814\n",
      "Iteration 18000 Training loss 0.031023763120174408 Validation loss 0.046989671885967255 Accuracy 0.8751250505447388\n",
      "Iteration 18010 Training loss 0.04363055154681206 Validation loss 0.048869531601667404 Accuracy 0.8680000305175781\n",
      "Iteration 18020 Training loss 0.03572923317551613 Validation loss 0.04728260636329651 Accuracy 0.8728750348091125\n",
      "Iteration 18030 Training loss 0.03158428147435188 Validation loss 0.04437462240457535 Accuracy 0.8783750534057617\n",
      "Iteration 18040 Training loss 0.040580786764621735 Validation loss 0.05466257780790329 Accuracy 0.8530000448226929\n",
      "Iteration 18050 Training loss 0.03438877314329147 Validation loss 0.04701646789908409 Accuracy 0.874125063419342\n",
      "Iteration 18060 Training loss 0.03913290426135063 Validation loss 0.048255063593387604 Accuracy 0.8697500228881836\n",
      "Iteration 18070 Training loss 0.02612943761050701 Validation loss 0.044532205909490585 Accuracy 0.8793750405311584\n",
      "Iteration 18080 Training loss 0.04258967190980911 Validation loss 0.051456935703754425 Accuracy 0.8611250519752502\n",
      "Iteration 18090 Training loss 0.03203016147017479 Validation loss 0.04632409289479256 Accuracy 0.8748750686645508\n",
      "Iteration 18100 Training loss 0.030793040990829468 Validation loss 0.04446401074528694 Accuracy 0.8790000677108765\n",
      "Iteration 18110 Training loss 0.03798849508166313 Validation loss 0.04603176936507225 Accuracy 0.8738750219345093\n",
      "Iteration 18120 Training loss 0.03955646976828575 Validation loss 0.04481072351336479 Accuracy 0.8792500495910645\n",
      "Iteration 18130 Training loss 0.034398894757032394 Validation loss 0.04453869163990021 Accuracy 0.8782500624656677\n",
      "Iteration 18140 Training loss 0.03243786841630936 Validation loss 0.04444633796811104 Accuracy 0.878125011920929\n",
      "Iteration 18150 Training loss 0.03634707257151604 Validation loss 0.04423600435256958 Accuracy 0.8782500624656677\n",
      "Iteration 18160 Training loss 0.0395948551595211 Validation loss 0.05500185489654541 Accuracy 0.8496250510215759\n",
      "Iteration 18170 Training loss 0.039253510534763336 Validation loss 0.044477883726358414 Accuracy 0.8783750534057617\n",
      "Iteration 18180 Training loss 0.03833872824907303 Validation loss 0.04444912075996399 Accuracy 0.8788750171661377\n",
      "Iteration 18190 Training loss 0.03863544017076492 Validation loss 0.05344545096158981 Accuracy 0.8536250591278076\n",
      "Iteration 18200 Training loss 0.037203408777713776 Validation loss 0.056576091796159744 Accuracy 0.8472500443458557\n",
      "Iteration 18210 Training loss 0.03583839535713196 Validation loss 0.0443144328892231 Accuracy 0.877375066280365\n",
      "Iteration 18220 Training loss 0.044087208807468414 Validation loss 0.048555564135313034 Accuracy 0.8697500228881836\n",
      "Iteration 18230 Training loss 0.030812829732894897 Validation loss 0.04768359661102295 Accuracy 0.8717500567436218\n",
      "Iteration 18240 Training loss 0.030360732227563858 Validation loss 0.0441955141723156 Accuracy 0.8791250586509705\n",
      "Iteration 18250 Training loss 0.034335289150476456 Validation loss 0.04711998999118805 Accuracy 0.874250054359436\n",
      "Iteration 18260 Training loss 0.031037693843245506 Validation loss 0.050284069031476974 Accuracy 0.8648750185966492\n",
      "Iteration 18270 Training loss 0.033643316477537155 Validation loss 0.044202063232660294 Accuracy 0.8791250586509705\n",
      "Iteration 18280 Training loss 0.03974596783518791 Validation loss 0.05003628134727478 Accuracy 0.8648750185966492\n",
      "Iteration 18290 Training loss 0.035971302539110184 Validation loss 0.04546140506863594 Accuracy 0.8763750195503235\n",
      "Iteration 18300 Training loss 0.03478533774614334 Validation loss 0.04519813507795334 Accuracy 0.8792500495910645\n",
      "Iteration 18310 Training loss 0.0406305268406868 Validation loss 0.0519527941942215 Accuracy 0.8607500195503235\n",
      "Iteration 18320 Training loss 0.032977521419525146 Validation loss 0.046168506145477295 Accuracy 0.8750000596046448\n",
      "Iteration 18330 Training loss 0.02842043526470661 Validation loss 0.04409932717680931 Accuracy 0.877625048160553\n",
      "Iteration 18340 Training loss 0.05333345755934715 Validation loss 0.053237102925777435 Accuracy 0.8565000295639038\n",
      "Iteration 18350 Training loss 0.037223152816295624 Validation loss 0.04550786688923836 Accuracy 0.8758750557899475\n",
      "Iteration 18360 Training loss 0.033009424805641174 Validation loss 0.04573444649577141 Accuracy 0.877750039100647\n",
      "Iteration 18370 Training loss 0.046143434941768646 Validation loss 0.052276961505413055 Accuracy 0.8597500324249268\n",
      "Iteration 18380 Training loss 0.028181394562125206 Validation loss 0.04463610425591469 Accuracy 0.8792500495910645\n",
      "Iteration 18390 Training loss 0.039681050926446915 Validation loss 0.047354210168123245 Accuracy 0.8731250166893005\n",
      "Iteration 18400 Training loss 0.036871712654829025 Validation loss 0.04656016081571579 Accuracy 0.8733750581741333\n",
      "Iteration 18410 Training loss 0.0500032901763916 Validation loss 0.06093024089932442 Accuracy 0.8343750238418579\n",
      "Iteration 18420 Training loss 0.027806226164102554 Validation loss 0.04438824579119682 Accuracy 0.8802500367164612\n",
      "Iteration 18430 Training loss 0.03733991086483002 Validation loss 0.04610058292746544 Accuracy 0.8768750429153442\n",
      "Iteration 18440 Training loss 0.0395435094833374 Validation loss 0.04655158519744873 Accuracy 0.8750000596046448\n",
      "Iteration 18450 Training loss 0.05065329745411873 Validation loss 0.06251020729541779 Accuracy 0.8281250596046448\n",
      "Iteration 18460 Training loss 0.034141287207603455 Validation loss 0.04423201084136963 Accuracy 0.8807500600814819\n",
      "Iteration 18470 Training loss 0.03270469978451729 Validation loss 0.04371984302997589 Accuracy 0.8811250329017639\n",
      "Iteration 18480 Training loss 0.0656600147485733 Validation loss 0.07020184397697449 Accuracy 0.811625063419342\n",
      "Iteration 18490 Training loss 0.03704560175538063 Validation loss 0.04422793537378311 Accuracy 0.8800000548362732\n",
      "Iteration 18500 Training loss 0.03355410322546959 Validation loss 0.048656415194272995 Accuracy 0.8710000514984131\n",
      "Iteration 18510 Training loss 0.03555746376514435 Validation loss 0.051441483199596405 Accuracy 0.8607500195503235\n",
      "Iteration 18520 Training loss 0.03277719020843506 Validation loss 0.0453951433300972 Accuracy 0.8795000314712524\n",
      "Iteration 18530 Training loss 0.037311580032110214 Validation loss 0.044557541608810425 Accuracy 0.878125011920929\n",
      "Iteration 18540 Training loss 0.034305065870285034 Validation loss 0.04561847820878029 Accuracy 0.8782500624656677\n",
      "Iteration 18550 Training loss 0.03358694911003113 Validation loss 0.045763108879327774 Accuracy 0.8787500262260437\n",
      "Iteration 18560 Training loss 0.031077789142727852 Validation loss 0.049556002020835876 Accuracy 0.8666250705718994\n",
      "Iteration 18570 Training loss 0.03262191638350487 Validation loss 0.044919535517692566 Accuracy 0.8772500157356262\n",
      "Iteration 18580 Training loss 0.03819222375750542 Validation loss 0.05096152424812317 Accuracy 0.8613750338554382\n",
      "Iteration 18590 Training loss 0.024184545502066612 Validation loss 0.04391445964574814 Accuracy 0.8796250224113464\n",
      "Iteration 18600 Training loss 0.03395135700702667 Validation loss 0.04839484021067619 Accuracy 0.8702500462532043\n",
      "Iteration 18610 Training loss 0.03604201227426529 Validation loss 0.05217890813946724 Accuracy 0.858875036239624\n",
      "Iteration 18620 Training loss 0.03489108011126518 Validation loss 0.04952467605471611 Accuracy 0.8657500147819519\n",
      "Iteration 18630 Training loss 0.04111093282699585 Validation loss 0.06153629720211029 Accuracy 0.8350000381469727\n",
      "Iteration 18640 Training loss 0.032277096062898636 Validation loss 0.044207729399204254 Accuracy 0.877750039100647\n",
      "Iteration 18650 Training loss 0.032667893916368484 Validation loss 0.04804748669266701 Accuracy 0.8708750605583191\n",
      "Iteration 18660 Training loss 0.02488434687256813 Validation loss 0.04388847574591637 Accuracy 0.8792500495910645\n",
      "Iteration 18670 Training loss 0.049836691468954086 Validation loss 0.055484622716903687 Accuracy 0.8518750667572021\n",
      "Iteration 18680 Training loss 0.028391236439347267 Validation loss 0.04625173285603523 Accuracy 0.877375066280365\n",
      "Iteration 18690 Training loss 0.03023684397339821 Validation loss 0.04615173861384392 Accuracy 0.8770000338554382\n",
      "Iteration 18700 Training loss 0.02475420944392681 Validation loss 0.0451815165579319 Accuracy 0.877750039100647\n",
      "Iteration 18710 Training loss 0.025784634053707123 Validation loss 0.04678093269467354 Accuracy 0.874250054359436\n",
      "Iteration 18720 Training loss 0.029875682666897774 Validation loss 0.043713074177503586 Accuracy 0.8795000314712524\n",
      "Iteration 18730 Training loss 0.026588620617985725 Validation loss 0.04539269208908081 Accuracy 0.8786250352859497\n",
      "Iteration 18740 Training loss 0.02861102484166622 Validation loss 0.044632717967033386 Accuracy 0.877875030040741\n",
      "Iteration 18750 Training loss 0.027648724615573883 Validation loss 0.04457531496882439 Accuracy 0.877375066280365\n",
      "Iteration 18760 Training loss 0.036956872791051865 Validation loss 0.048800110816955566 Accuracy 0.8670000433921814\n",
      "Iteration 18770 Training loss 0.035589106380939484 Validation loss 0.0448111928999424 Accuracy 0.8772500157356262\n",
      "Iteration 18780 Training loss 0.03079168312251568 Validation loss 0.04498978704214096 Accuracy 0.8768750429153442\n",
      "Iteration 18790 Training loss 0.02556183561682701 Validation loss 0.04410567507147789 Accuracy 0.8797500133514404\n",
      "Iteration 18800 Training loss 0.04004814103245735 Validation loss 0.05449304357171059 Accuracy 0.8540000319480896\n",
      "Iteration 18810 Training loss 0.032124113291502 Validation loss 0.05277585983276367 Accuracy 0.8603750467300415\n",
      "Iteration 18820 Training loss 0.031383760273456573 Validation loss 0.04424337297677994 Accuracy 0.878125011920929\n",
      "Iteration 18830 Training loss 0.031470317393541336 Validation loss 0.04600704833865166 Accuracy 0.8763750195503235\n",
      "Iteration 18840 Training loss 0.032047852873802185 Validation loss 0.05049526318907738 Accuracy 0.8645000457763672\n",
      "Iteration 18850 Training loss 0.03018861636519432 Validation loss 0.04431704431772232 Accuracy 0.8783750534057617\n",
      "Iteration 18860 Training loss 0.028705153614282608 Validation loss 0.04404380917549133 Accuracy 0.8791250586509705\n",
      "Iteration 18870 Training loss 0.03649643436074257 Validation loss 0.04407940432429314 Accuracy 0.8796250224113464\n",
      "Iteration 18880 Training loss 0.03970486670732498 Validation loss 0.04408901557326317 Accuracy 0.8783750534057617\n",
      "Iteration 18890 Training loss 0.033747315406799316 Validation loss 0.04684196785092354 Accuracy 0.8727500438690186\n",
      "Iteration 18900 Training loss 0.045317552983760834 Validation loss 0.06074190139770508 Accuracy 0.8368750214576721\n",
      "Iteration 18910 Training loss 0.03254527971148491 Validation loss 0.048548631370067596 Accuracy 0.8705000281333923\n",
      "Iteration 18920 Training loss 0.02420034445822239 Validation loss 0.0451245978474617 Accuracy 0.8796250224113464\n",
      "Iteration 18930 Training loss 0.025933796539902687 Validation loss 0.044549647718667984 Accuracy 0.8795000314712524\n",
      "Iteration 18940 Training loss 0.04471007362008095 Validation loss 0.0497281514108181 Accuracy 0.8635000586509705\n",
      "Iteration 18950 Training loss 0.034532032907009125 Validation loss 0.04627063125371933 Accuracy 0.8748750686645508\n",
      "Iteration 18960 Training loss 0.025788001716136932 Validation loss 0.04449231177568436 Accuracy 0.8782500624656677\n",
      "Iteration 18970 Training loss 0.03227842599153519 Validation loss 0.04612484946846962 Accuracy 0.8765000700950623\n",
      "Iteration 18980 Training loss 0.028134679421782494 Validation loss 0.04590850695967674 Accuracy 0.8766250610351562\n",
      "Iteration 18990 Training loss 0.03497214615345001 Validation loss 0.044526804238557816 Accuracy 0.8800000548362732\n",
      "Iteration 19000 Training loss 0.04015817493200302 Validation loss 0.050318680703639984 Accuracy 0.8615000247955322\n",
      "Iteration 19010 Training loss 0.03525501489639282 Validation loss 0.04617754742503166 Accuracy 0.8757500648498535\n",
      "Iteration 19020 Training loss 0.044409021735191345 Validation loss 0.050830576568841934 Accuracy 0.8613750338554382\n",
      "Iteration 19030 Training loss 0.026054495945572853 Validation loss 0.04559187963604927 Accuracy 0.8761250376701355\n",
      "Iteration 19040 Training loss 0.01985444687306881 Validation loss 0.043894778937101364 Accuracy 0.8803750276565552\n",
      "Iteration 19050 Training loss 0.03663843125104904 Validation loss 0.049616288393735886 Accuracy 0.8665000200271606\n",
      "Iteration 19060 Training loss 0.02951650321483612 Validation loss 0.04831454157829285 Accuracy 0.8683750629425049\n",
      "Iteration 19070 Training loss 0.03282548859715462 Validation loss 0.04463515430688858 Accuracy 0.8792500495910645\n",
      "Iteration 19080 Training loss 0.05671445280313492 Validation loss 0.06498578190803528 Accuracy 0.8238750100135803\n",
      "Iteration 19090 Training loss 0.02881780080497265 Validation loss 0.04392709583044052 Accuracy 0.8816250562667847\n",
      "Iteration 19100 Training loss 0.02832471951842308 Validation loss 0.044251374900341034 Accuracy 0.8796250224113464\n",
      "Iteration 19110 Training loss 0.030390111729502678 Validation loss 0.04361886903643608 Accuracy 0.8802500367164612\n",
      "Iteration 19120 Training loss 0.026401348412036896 Validation loss 0.0441490039229393 Accuracy 0.8795000314712524\n",
      "Iteration 19130 Training loss 0.051515910774469376 Validation loss 0.06168842688202858 Accuracy 0.8327500224113464\n",
      "Iteration 19140 Training loss 0.03781835362315178 Validation loss 0.044217851012945175 Accuracy 0.878125011920929\n",
      "Iteration 19150 Training loss 0.02418399788439274 Validation loss 0.04719468951225281 Accuracy 0.874125063419342\n",
      "Iteration 19160 Training loss 0.03248682990670204 Validation loss 0.04487314075231552 Accuracy 0.8806250691413879\n",
      "Iteration 19170 Training loss 0.038447827100753784 Validation loss 0.04693603515625 Accuracy 0.8726250529289246\n",
      "Iteration 19180 Training loss 0.030929816886782646 Validation loss 0.046238504350185394 Accuracy 0.874625027179718\n",
      "Iteration 19190 Training loss 0.0314454548060894 Validation loss 0.044377390295267105 Accuracy 0.8797500133514404\n",
      "Iteration 19200 Training loss 0.02956325002014637 Validation loss 0.04495016857981682 Accuracy 0.8792500495910645\n",
      "Iteration 19210 Training loss 0.03342496231198311 Validation loss 0.04391571879386902 Accuracy 0.8818750381469727\n",
      "Iteration 19220 Training loss 0.033039357513189316 Validation loss 0.04952353239059448 Accuracy 0.8638750314712524\n",
      "Iteration 19230 Training loss 0.03784019500017166 Validation loss 0.05033336207270622 Accuracy 0.8645000457763672\n",
      "Iteration 19240 Training loss 0.03230136260390282 Validation loss 0.0460423044860363 Accuracy 0.8752500414848328\n",
      "Iteration 19250 Training loss 0.035600658506155014 Validation loss 0.044506631791591644 Accuracy 0.877625048160553\n",
      "Iteration 19260 Training loss 0.02559557370841503 Validation loss 0.05061071738600731 Accuracy 0.8632500171661377\n",
      "Iteration 19270 Training loss 0.034874580800533295 Validation loss 0.0441494844853878 Accuracy 0.8808750510215759\n",
      "Iteration 19280 Training loss 0.040792111307382584 Validation loss 0.05129167437553406 Accuracy 0.8615000247955322\n",
      "Iteration 19290 Training loss 0.02974010445177555 Validation loss 0.04749332368373871 Accuracy 0.8716250658035278\n",
      "Iteration 19300 Training loss 0.04290400445461273 Validation loss 0.05726413056254387 Accuracy 0.8453750610351562\n",
      "Iteration 19310 Training loss 0.03479618579149246 Validation loss 0.044336672872304916 Accuracy 0.878000020980835\n",
      "Iteration 19320 Training loss 0.027267493307590485 Validation loss 0.04385923221707344 Accuracy 0.8805000185966492\n",
      "Iteration 19330 Training loss 0.039064668118953705 Validation loss 0.046212535351514816 Accuracy 0.8733750581741333\n",
      "Iteration 19340 Training loss 0.02948213741183281 Validation loss 0.044123273342847824 Accuracy 0.8815000653266907\n",
      "Iteration 19350 Training loss 0.038693659007549286 Validation loss 0.04700541868805885 Accuracy 0.8731250166893005\n",
      "Iteration 19360 Training loss 0.0261921975761652 Validation loss 0.043853357434272766 Accuracy 0.8803750276565552\n",
      "Iteration 19370 Training loss 0.03323725238442421 Validation loss 0.045755136758089066 Accuracy 0.8770000338554382\n",
      "Iteration 19380 Training loss 0.03333250433206558 Validation loss 0.04463222622871399 Accuracy 0.8803750276565552\n",
      "Iteration 19390 Training loss 0.03706040233373642 Validation loss 0.04392864555120468 Accuracy 0.8801250457763672\n",
      "Iteration 19400 Training loss 0.035907045006752014 Validation loss 0.05199887230992317 Accuracy 0.8595000505447388\n",
      "Iteration 19410 Training loss 0.024804791435599327 Validation loss 0.04383175075054169 Accuracy 0.8821250200271606\n",
      "Iteration 19420 Training loss 0.025507284328341484 Validation loss 0.044340021908283234 Accuracy 0.8793750405311584\n",
      "Iteration 19430 Training loss 0.028487753123044968 Validation loss 0.044109005481004715 Accuracy 0.8807500600814819\n",
      "Iteration 19440 Training loss 0.03635549545288086 Validation loss 0.04966073855757713 Accuracy 0.8660000562667847\n",
      "Iteration 19450 Training loss 0.03510720655322075 Validation loss 0.049036476761102676 Accuracy 0.8683750629425049\n",
      "Iteration 19460 Training loss 0.033576902002096176 Validation loss 0.04630456864833832 Accuracy 0.874125063419342\n",
      "Iteration 19470 Training loss 0.026095077395439148 Validation loss 0.043727558106184006 Accuracy 0.8802500367164612\n",
      "Iteration 19480 Training loss 0.02909555658698082 Validation loss 0.04561883583664894 Accuracy 0.8761250376701355\n",
      "Iteration 19490 Training loss 0.04024248942732811 Validation loss 0.05296996608376503 Accuracy 0.858875036239624\n",
      "Iteration 19500 Training loss 0.03867606073617935 Validation loss 0.052591241896152496 Accuracy 0.8572500348091125\n",
      "Iteration 19510 Training loss 0.0301878210157156 Validation loss 0.04396909847855568 Accuracy 0.8798750638961792\n",
      "Iteration 19520 Training loss 0.037597186863422394 Validation loss 0.04892188310623169 Accuracy 0.8682500123977661\n",
      "Iteration 19530 Training loss 0.025180114433169365 Validation loss 0.04531616345047951 Accuracy 0.877375066280365\n",
      "Iteration 19540 Training loss 0.02767368219792843 Validation loss 0.044251568615436554 Accuracy 0.8805000185966492\n",
      "Iteration 19550 Training loss 0.030945615842938423 Validation loss 0.04576858878135681 Accuracy 0.8768750429153442\n",
      "Iteration 19560 Training loss 0.027654210105538368 Validation loss 0.04496503621339798 Accuracy 0.8790000677108765\n",
      "Iteration 19570 Training loss 0.03250271826982498 Validation loss 0.04521928355097771 Accuracy 0.878000020980835\n",
      "Iteration 19580 Training loss 0.05872032046318054 Validation loss 0.06201458349823952 Accuracy 0.8342500329017639\n",
      "Iteration 19590 Training loss 0.03010549768805504 Validation loss 0.044176794588565826 Accuracy 0.8805000185966492\n",
      "Iteration 19600 Training loss 0.03608165681362152 Validation loss 0.05417994037270546 Accuracy 0.8547500371932983\n",
      "Iteration 19610 Training loss 0.033449895679950714 Validation loss 0.04449762776494026 Accuracy 0.8798750638961792\n",
      "Iteration 19620 Training loss 0.02669859677553177 Validation loss 0.04426131024956703 Accuracy 0.8805000185966492\n",
      "Iteration 19630 Training loss 0.025187291204929352 Validation loss 0.04496309161186218 Accuracy 0.8806250691413879\n",
      "Iteration 19640 Training loss 0.03491036966443062 Validation loss 0.048493362963199615 Accuracy 0.8698750138282776\n",
      "Iteration 19650 Training loss 0.031343717128038406 Validation loss 0.043697938323020935 Accuracy 0.8808750510215759\n",
      "Iteration 19660 Training loss 0.04737377166748047 Validation loss 0.05829830467700958 Accuracy 0.8441250324249268\n",
      "Iteration 19670 Training loss 0.03629491850733757 Validation loss 0.05655350536108017 Accuracy 0.8456250429153442\n",
      "Iteration 19680 Training loss 0.036309387534856796 Validation loss 0.04841352626681328 Accuracy 0.8690000176429749\n",
      "Iteration 19690 Training loss 0.033258650451898575 Validation loss 0.047253236174583435 Accuracy 0.874125063419342\n",
      "Iteration 19700 Training loss 0.03479393944144249 Validation loss 0.04387633502483368 Accuracy 0.8802500367164612\n",
      "Iteration 19710 Training loss 0.0323161706328392 Validation loss 0.04981272667646408 Accuracy 0.8645000457763672\n",
      "Iteration 19720 Training loss 0.029879311099648476 Validation loss 0.046267129480838776 Accuracy 0.8748750686645508\n",
      "Iteration 19730 Training loss 0.03026348538696766 Validation loss 0.045323628932237625 Accuracy 0.8788750171661377\n",
      "Iteration 19740 Training loss 0.02819957211613655 Validation loss 0.04578912630677223 Accuracy 0.8760000467300415\n",
      "Iteration 19750 Training loss 0.03901245445013046 Validation loss 0.04846850037574768 Accuracy 0.8697500228881836\n",
      "Iteration 19760 Training loss 0.036158692091703415 Validation loss 0.04495382681488991 Accuracy 0.8795000314712524\n",
      "Iteration 19770 Training loss 0.023975227028131485 Validation loss 0.04380767419934273 Accuracy 0.8797500133514404\n",
      "Iteration 19780 Training loss 0.031317003071308136 Validation loss 0.0444423146545887 Accuracy 0.8795000314712524\n",
      "Iteration 19790 Training loss 0.038254514336586 Validation loss 0.04465948045253754 Accuracy 0.8793750405311584\n",
      "Iteration 19800 Training loss 0.048623695969581604 Validation loss 0.06149917468428612 Accuracy 0.8332500457763672\n",
      "Iteration 19810 Training loss 0.03459978848695755 Validation loss 0.04450327157974243 Accuracy 0.8800000548362732\n",
      "Iteration 19820 Training loss 0.05954135209321976 Validation loss 0.0713217630982399 Accuracy 0.8095000386238098\n",
      "Iteration 19830 Training loss 0.02740032598376274 Validation loss 0.04410054162144661 Accuracy 0.8795000314712524\n",
      "Iteration 19840 Training loss 0.025686020031571388 Validation loss 0.04427359998226166 Accuracy 0.8801250457763672\n",
      "Iteration 19850 Training loss 0.029500242322683334 Validation loss 0.04422372579574585 Accuracy 0.8791250586509705\n",
      "Iteration 19860 Training loss 0.031966377049684525 Validation loss 0.047523461282253265 Accuracy 0.8727500438690186\n",
      "Iteration 19870 Training loss 0.03477073088288307 Validation loss 0.044743429869413376 Accuracy 0.8791250586509705\n",
      "Iteration 19880 Training loss 0.03511321544647217 Validation loss 0.04576486349105835 Accuracy 0.8782500624656677\n",
      "Iteration 19890 Training loss 0.051070477813482285 Validation loss 0.06013151630759239 Accuracy 0.8381250500679016\n",
      "Iteration 19900 Training loss 0.03419692814350128 Validation loss 0.048998598009347916 Accuracy 0.8686250448226929\n",
      "Iteration 19910 Training loss 0.04310750961303711 Validation loss 0.04994010552763939 Accuracy 0.8662500381469727\n",
      "Iteration 19920 Training loss 0.028453687205910683 Validation loss 0.04446568712592125 Accuracy 0.8802500367164612\n",
      "Iteration 19930 Training loss 0.03650819882750511 Validation loss 0.044154293835163116 Accuracy 0.8810000419616699\n",
      "Iteration 19940 Training loss 0.027891051024198532 Validation loss 0.04449404776096344 Accuracy 0.8810000419616699\n",
      "Iteration 19950 Training loss 0.028111286461353302 Validation loss 0.04389370232820511 Accuracy 0.8795000314712524\n",
      "Iteration 19960 Training loss 0.025074951350688934 Validation loss 0.048406511545181274 Accuracy 0.8687500357627869\n",
      "Iteration 19970 Training loss 0.042469024658203125 Validation loss 0.0543026328086853 Accuracy 0.8528750538825989\n",
      "Iteration 19980 Training loss 0.031677428632974625 Validation loss 0.046181514859199524 Accuracy 0.8738750219345093\n",
      "Iteration 19990 Training loss 0.02496715448796749 Validation loss 0.04366721957921982 Accuracy 0.8800000548362732\n",
      "Iteration 20000 Training loss 0.036805298179388046 Validation loss 0.04764023423194885 Accuracy 0.8710000514984131\n",
      "Iteration 20010 Training loss 0.02651476487517357 Validation loss 0.04480178281664848 Accuracy 0.8792500495910645\n",
      "Iteration 20020 Training loss 0.029260214418172836 Validation loss 0.04886647313833237 Accuracy 0.8695000410079956\n",
      "Iteration 20030 Training loss 0.02654789201915264 Validation loss 0.043616555631160736 Accuracy 0.8811250329017639\n",
      "Iteration 20040 Training loss 0.03735706955194473 Validation loss 0.0453193373978138 Accuracy 0.877625048160553\n",
      "Iteration 20050 Training loss 0.032101698219776154 Validation loss 0.047737255692481995 Accuracy 0.8702500462532043\n",
      "Iteration 20060 Training loss 0.027141112834215164 Validation loss 0.04373122751712799 Accuracy 0.8811250329017639\n",
      "Iteration 20070 Training loss 0.04550663009285927 Validation loss 0.05755814537405968 Accuracy 0.8446250557899475\n",
      "Iteration 20080 Training loss 0.029005197808146477 Validation loss 0.0437234602868557 Accuracy 0.8818750381469727\n",
      "Iteration 20090 Training loss 0.03235473483800888 Validation loss 0.04867617413401604 Accuracy 0.8700000643730164\n",
      "Iteration 20100 Training loss 0.03259715810418129 Validation loss 0.05115872621536255 Accuracy 0.862375020980835\n",
      "Iteration 20110 Training loss 0.027798060327768326 Validation loss 0.04386342689394951 Accuracy 0.8807500600814819\n",
      "Iteration 20120 Training loss 0.04445013776421547 Validation loss 0.05224087834358215 Accuracy 0.858625054359436\n",
      "Iteration 20130 Training loss 0.03185899183154106 Validation loss 0.04601316154003143 Accuracy 0.8761250376701355\n",
      "Iteration 20140 Training loss 0.03094095177948475 Validation loss 0.046476125717163086 Accuracy 0.8757500648498535\n",
      "Iteration 20150 Training loss 0.03159674257040024 Validation loss 0.044721607118844986 Accuracy 0.877500057220459\n",
      "Iteration 20160 Training loss 0.03740234673023224 Validation loss 0.04846823960542679 Accuracy 0.8687500357627869\n",
      "Iteration 20170 Training loss 0.03051227517426014 Validation loss 0.04677446559071541 Accuracy 0.874625027179718\n",
      "Iteration 20180 Training loss 0.03121885657310486 Validation loss 0.04451151192188263 Accuracy 0.8800000548362732\n",
      "Iteration 20190 Training loss 0.026837846264243126 Validation loss 0.045342233031988144 Accuracy 0.8786250352859497\n",
      "Iteration 20200 Training loss 0.03789630904793739 Validation loss 0.04769989848136902 Accuracy 0.8723750710487366\n",
      "Iteration 20210 Training loss 0.028552670031785965 Validation loss 0.047383036464452744 Accuracy 0.8717500567436218\n",
      "Iteration 20220 Training loss 0.04328189790248871 Validation loss 0.050843678414821625 Accuracy 0.8636250495910645\n",
      "Iteration 20230 Training loss 0.028332889080047607 Validation loss 0.044284701347351074 Accuracy 0.8791250586509705\n",
      "Iteration 20240 Training loss 0.0255325548350811 Validation loss 0.04479352384805679 Accuracy 0.8797500133514404\n",
      "Iteration 20250 Training loss 0.033036526292562485 Validation loss 0.04811987653374672 Accuracy 0.8701250553131104\n",
      "Iteration 20260 Training loss 0.028140220791101456 Validation loss 0.04350968450307846 Accuracy 0.8821250200271606\n",
      "Iteration 20270 Training loss 0.031132405623793602 Validation loss 0.04400469362735748 Accuracy 0.8812500238418579\n",
      "Iteration 20280 Training loss 0.02212192490696907 Validation loss 0.04346511885523796 Accuracy 0.8826250433921814\n",
      "Iteration 20290 Training loss 0.03716930001974106 Validation loss 0.04643683880567551 Accuracy 0.874750018119812\n",
      "Iteration 20300 Training loss 0.04329456388950348 Validation loss 0.04781300574541092 Accuracy 0.8708750605583191\n",
      "Iteration 20310 Training loss 0.04270520061254501 Validation loss 0.051506612449884415 Accuracy 0.8616250157356262\n",
      "Iteration 20320 Training loss 0.03837650269269943 Validation loss 0.04661823436617851 Accuracy 0.8736250400543213\n",
      "Iteration 20330 Training loss 0.03274345397949219 Validation loss 0.052755940705537796 Accuracy 0.8603750467300415\n",
      "Iteration 20340 Training loss 0.048639927059412 Validation loss 0.05305632948875427 Accuracy 0.8578750491142273\n",
      "Iteration 20350 Training loss 0.028700925409793854 Validation loss 0.04495115950703621 Accuracy 0.8795000314712524\n",
      "Iteration 20360 Training loss 0.03445911034941673 Validation loss 0.04948164522647858 Accuracy 0.8675000667572021\n",
      "Iteration 20370 Training loss 0.03247978538274765 Validation loss 0.04487712308764458 Accuracy 0.8792500495910645\n",
      "Iteration 20380 Training loss 0.035887278616428375 Validation loss 0.047935329377651215 Accuracy 0.8693750500679016\n",
      "Iteration 20390 Training loss 0.03443393483757973 Validation loss 0.051409054547548294 Accuracy 0.8596250414848328\n",
      "Iteration 20400 Training loss 0.027034414932131767 Validation loss 0.044392336159944534 Accuracy 0.8796250224113464\n",
      "Iteration 20410 Training loss 0.02610141970217228 Validation loss 0.04342104494571686 Accuracy 0.8817500472068787\n",
      "Iteration 20420 Training loss 0.026431545615196228 Validation loss 0.04504653438925743 Accuracy 0.8795000314712524\n",
      "Iteration 20430 Training loss 0.02328203059732914 Validation loss 0.04438727721571922 Accuracy 0.8808750510215759\n",
      "Iteration 20440 Training loss 0.03885515779256821 Validation loss 0.05024048686027527 Accuracy 0.862250030040741\n",
      "Iteration 20450 Training loss 0.03521513566374779 Validation loss 0.04912637546658516 Accuracy 0.8667500615119934\n",
      "Iteration 20460 Training loss 0.03496487811207771 Validation loss 0.04464738443493843 Accuracy 0.8782500624656677\n",
      "Iteration 20470 Training loss 0.028215507045388222 Validation loss 0.04426457732915878 Accuracy 0.8810000419616699\n",
      "Iteration 20480 Training loss 0.03146078810095787 Validation loss 0.04420328140258789 Accuracy 0.8812500238418579\n",
      "Iteration 20490 Training loss 0.039409298449754715 Validation loss 0.044656720012426376 Accuracy 0.8793750405311584\n",
      "Iteration 20500 Training loss 0.03366255387663841 Validation loss 0.048522233963012695 Accuracy 0.8683750629425049\n",
      "Iteration 20510 Training loss 0.027478903532028198 Validation loss 0.04338161647319794 Accuracy 0.8820000290870667\n",
      "Iteration 20520 Training loss 0.03192486986517906 Validation loss 0.044561244547367096 Accuracy 0.8798750638961792\n",
      "Iteration 20530 Training loss 0.028998540714383125 Validation loss 0.043718818575143814 Accuracy 0.8813750147819519\n",
      "Iteration 20540 Training loss 0.029257757589221 Validation loss 0.045161545276641846 Accuracy 0.877625048160553\n",
      "Iteration 20550 Training loss 0.04935593530535698 Validation loss 0.05868931859731674 Accuracy 0.843250036239624\n",
      "Iteration 20560 Training loss 0.035990290343761444 Validation loss 0.04519444331526756 Accuracy 0.8788750171661377\n",
      "Iteration 20570 Training loss 0.0362604558467865 Validation loss 0.049979280680418015 Accuracy 0.8643750548362732\n",
      "Iteration 20580 Training loss 0.027648398652672768 Validation loss 0.04492349177598953 Accuracy 0.878125011920929\n",
      "Iteration 20590 Training loss 0.028808843344449997 Validation loss 0.043936289846897125 Accuracy 0.8816250562667847\n",
      "Iteration 20600 Training loss 0.0346844382584095 Validation loss 0.04513772204518318 Accuracy 0.878000020980835\n",
      "Iteration 20610 Training loss 0.023803450167179108 Validation loss 0.043590571731328964 Accuracy 0.8808750510215759\n",
      "Iteration 20620 Training loss 0.03388935327529907 Validation loss 0.04414869099855423 Accuracy 0.8806250691413879\n",
      "Iteration 20630 Training loss 0.030371202155947685 Validation loss 0.045688509941101074 Accuracy 0.8748750686645508\n",
      "Iteration 20640 Training loss 0.03028927743434906 Validation loss 0.04515501484274864 Accuracy 0.8802500367164612\n",
      "Iteration 20650 Training loss 0.026067454367876053 Validation loss 0.04621242731809616 Accuracy 0.878000020980835\n",
      "Iteration 20660 Training loss 0.04920949786901474 Validation loss 0.06234428286552429 Accuracy 0.8320000171661377\n",
      "Iteration 20670 Training loss 0.02993340976536274 Validation loss 0.04467571899294853 Accuracy 0.8806250691413879\n",
      "Iteration 20680 Training loss 0.026055824011564255 Validation loss 0.04316405951976776 Accuracy 0.8818750381469727\n",
      "Iteration 20690 Training loss 0.026314152404665947 Validation loss 0.043314576148986816 Accuracy 0.8820000290870667\n",
      "Iteration 20700 Training loss 0.021427558735013008 Validation loss 0.0452347956597805 Accuracy 0.8792500495910645\n",
      "Iteration 20710 Training loss 0.04547068104147911 Validation loss 0.055855341255664825 Accuracy 0.8481250405311584\n",
      "Iteration 20720 Training loss 0.03137090429663658 Validation loss 0.043528780341148376 Accuracy 0.8827500343322754\n",
      "Iteration 20730 Training loss 0.02694135159254074 Validation loss 0.04699546471238136 Accuracy 0.8750000596046448\n",
      "Iteration 20740 Training loss 0.03284464403986931 Validation loss 0.04546758532524109 Accuracy 0.877875030040741\n",
      "Iteration 20750 Training loss 0.032809820026159286 Validation loss 0.05045964941382408 Accuracy 0.8642500638961792\n",
      "Iteration 20760 Training loss 0.04025493189692497 Validation loss 0.05056014657020569 Accuracy 0.8666250705718994\n",
      "Iteration 20770 Training loss 0.05695918947458267 Validation loss 0.06629393249750137 Accuracy 0.8226250410079956\n",
      "Iteration 20780 Training loss 0.02751068025827408 Validation loss 0.045052699744701385 Accuracy 0.877625048160553\n",
      "Iteration 20790 Training loss 0.026214566081762314 Validation loss 0.0447869673371315 Accuracy 0.8785000443458557\n",
      "Iteration 20800 Training loss 0.026354677975177765 Validation loss 0.044108834117650986 Accuracy 0.8813750147819519\n",
      "Iteration 20810 Training loss 0.027687154710292816 Validation loss 0.044690050184726715 Accuracy 0.8808750510215759\n",
      "Iteration 20820 Training loss 0.029650084674358368 Validation loss 0.04722951725125313 Accuracy 0.8712500333786011\n",
      "Iteration 20830 Training loss 0.043852318078279495 Validation loss 0.05571579933166504 Accuracy 0.8515000343322754\n",
      "Iteration 20840 Training loss 0.02909712679684162 Validation loss 0.04362516477704048 Accuracy 0.8812500238418579\n",
      "Iteration 20850 Training loss 0.0429675318300724 Validation loss 0.054215800017118454 Accuracy 0.8525000214576721\n",
      "Iteration 20860 Training loss 0.03631316497921944 Validation loss 0.048510752618312836 Accuracy 0.8686250448226929\n",
      "Iteration 20870 Training loss 0.02515019290149212 Validation loss 0.04391375556588173 Accuracy 0.8815000653266907\n",
      "Iteration 20880 Training loss 0.03648380562663078 Validation loss 0.04353727027773857 Accuracy 0.8837500214576721\n",
      "Iteration 20890 Training loss 0.030689828097820282 Validation loss 0.04588519409298897 Accuracy 0.8758750557899475\n",
      "Iteration 20900 Training loss 0.03955811262130737 Validation loss 0.05312824621796608 Accuracy 0.8567500114440918\n",
      "Iteration 20910 Training loss 0.0245783943682909 Validation loss 0.04398368299007416 Accuracy 0.8810000419616699\n",
      "Iteration 20920 Training loss 0.04223331809043884 Validation loss 0.05034243315458298 Accuracy 0.8642500638961792\n",
      "Iteration 20930 Training loss 0.025861604139208794 Validation loss 0.04361632093787193 Accuracy 0.8828750252723694\n",
      "Iteration 20940 Training loss 0.02638271264731884 Validation loss 0.0448436364531517 Accuracy 0.8813750147819519\n",
      "Iteration 20950 Training loss 0.035226400941610336 Validation loss 0.04959160089492798 Accuracy 0.8631250262260437\n",
      "Iteration 20960 Training loss 0.025855349376797676 Validation loss 0.043911416083574295 Accuracy 0.8806250691413879\n",
      "Iteration 20970 Training loss 0.04438536986708641 Validation loss 0.06072884798049927 Accuracy 0.8410000205039978\n",
      "Iteration 20980 Training loss 0.02840268984436989 Validation loss 0.0435776449739933 Accuracy 0.8810000419616699\n",
      "Iteration 20990 Training loss 0.028536543250083923 Validation loss 0.043460581451654434 Accuracy 0.8820000290870667\n",
      "Iteration 21000 Training loss 0.026201879605650902 Validation loss 0.04455706477165222 Accuracy 0.8812500238418579\n",
      "Iteration 21010 Training loss 0.038846831768751144 Validation loss 0.05425048992037773 Accuracy 0.8556250333786011\n",
      "Iteration 21020 Training loss 0.03060183860361576 Validation loss 0.046615276485681534 Accuracy 0.8758750557899475\n",
      "Iteration 21030 Training loss 0.027943162247538567 Validation loss 0.04432448744773865 Accuracy 0.8815000653266907\n",
      "Iteration 21040 Training loss 0.030431052669882774 Validation loss 0.04338546097278595 Accuracy 0.8821250200271606\n",
      "Iteration 21050 Training loss 0.035454656928777695 Validation loss 0.04630817845463753 Accuracy 0.8758750557899475\n",
      "Iteration 21060 Training loss 0.02861749194562435 Validation loss 0.04327006638050079 Accuracy 0.8836250305175781\n",
      "Iteration 21070 Training loss 0.03961176797747612 Validation loss 0.0470447912812233 Accuracy 0.8736250400543213\n",
      "Iteration 21080 Training loss 0.029579531401395798 Validation loss 0.048877786844968796 Accuracy 0.8680000305175781\n",
      "Iteration 21090 Training loss 0.034167978912591934 Validation loss 0.04483327642083168 Accuracy 0.8806250691413879\n",
      "Iteration 21100 Training loss 0.03956303000450134 Validation loss 0.04420394077897072 Accuracy 0.8810000419616699\n",
      "Iteration 21110 Training loss 0.03632673621177673 Validation loss 0.0503244586288929 Accuracy 0.8647500276565552\n",
      "Iteration 21120 Training loss 0.028308136388659477 Validation loss 0.04441140219569206 Accuracy 0.8820000290870667\n",
      "Iteration 21130 Training loss 0.02712238021194935 Validation loss 0.04775368049740791 Accuracy 0.8685000538825989\n",
      "Iteration 21140 Training loss 0.03981760889291763 Validation loss 0.04484650492668152 Accuracy 0.8790000677108765\n",
      "Iteration 21150 Training loss 0.028231987729668617 Validation loss 0.044352516531944275 Accuracy 0.8820000290870667\n",
      "Iteration 21160 Training loss 0.04012013226747513 Validation loss 0.04966914653778076 Accuracy 0.8637500405311584\n",
      "Iteration 21170 Training loss 0.028789428994059563 Validation loss 0.04475557059049606 Accuracy 0.8806250691413879\n",
      "Iteration 21180 Training loss 0.03033568523824215 Validation loss 0.043532416224479675 Accuracy 0.8827500343322754\n",
      "Iteration 21190 Training loss 0.031946804374456406 Validation loss 0.04789765924215317 Accuracy 0.8717500567436218\n",
      "Iteration 21200 Training loss 0.02228189818561077 Validation loss 0.04318311810493469 Accuracy 0.8841250538825989\n",
      "Iteration 21210 Training loss 0.02316582389175892 Validation loss 0.04365362226963043 Accuracy 0.8821250200271606\n",
      "Iteration 21220 Training loss 0.03105488419532776 Validation loss 0.04383217915892601 Accuracy 0.8823750615119934\n",
      "Iteration 21230 Training loss 0.02645142562687397 Validation loss 0.04627251252532005 Accuracy 0.8762500286102295\n",
      "Iteration 21240 Training loss 0.025945795699954033 Validation loss 0.044755496084690094 Accuracy 0.8806250691413879\n",
      "Iteration 21250 Training loss 0.03859531879425049 Validation loss 0.05969472602009773 Accuracy 0.8383750319480896\n",
      "Iteration 21260 Training loss 0.026590904220938683 Validation loss 0.04463917016983032 Accuracy 0.8795000314712524\n",
      "Iteration 21270 Training loss 0.039544180035591125 Validation loss 0.04835471510887146 Accuracy 0.8693750500679016\n",
      "Iteration 21280 Training loss 0.027988599613308907 Validation loss 0.04517720267176628 Accuracy 0.8765000700950623\n",
      "Iteration 21290 Training loss 0.024060793220996857 Validation loss 0.04381300508975983 Accuracy 0.8820000290870667\n",
      "Iteration 21300 Training loss 0.02783259004354477 Validation loss 0.043543990701436996 Accuracy 0.8838750123977661\n",
      "Iteration 21310 Training loss 0.027868537232279778 Validation loss 0.051133785396814346 Accuracy 0.861875057220459\n",
      "Iteration 21320 Training loss 0.03581348434090614 Validation loss 0.05305001139640808 Accuracy 0.8565000295639038\n",
      "Iteration 21330 Training loss 0.02340318076312542 Validation loss 0.044634852558374405 Accuracy 0.8802500367164612\n",
      "Iteration 21340 Training loss 0.02446553111076355 Validation loss 0.04588055983185768 Accuracy 0.8771250247955322\n",
      "Iteration 21350 Training loss 0.026336779817938805 Validation loss 0.043244995176792145 Accuracy 0.8838750123977661\n",
      "Iteration 21360 Training loss 0.032081473618745804 Validation loss 0.04379896819591522 Accuracy 0.8802500367164612\n",
      "Iteration 21370 Training loss 0.04375453665852547 Validation loss 0.05937837064266205 Accuracy 0.8401250243186951\n",
      "Iteration 21380 Training loss 0.02980673313140869 Validation loss 0.045480113476514816 Accuracy 0.878000020980835\n",
      "Iteration 21390 Training loss 0.028199711814522743 Validation loss 0.0445680171251297 Accuracy 0.8805000185966492\n",
      "Iteration 21400 Training loss 0.040423743426799774 Validation loss 0.052465829998254776 Accuracy 0.858625054359436\n",
      "Iteration 21410 Training loss 0.03697073459625244 Validation loss 0.04698652774095535 Accuracy 0.8735000491142273\n",
      "Iteration 21420 Training loss 0.03062095306813717 Validation loss 0.0436268113553524 Accuracy 0.8817500472068787\n",
      "Iteration 21430 Training loss 0.04815340042114258 Validation loss 0.05868469551205635 Accuracy 0.843500018119812\n",
      "Iteration 21440 Training loss 0.02257464826107025 Validation loss 0.043931994587183 Accuracy 0.8798750638961792\n",
      "Iteration 21450 Training loss 0.03603002429008484 Validation loss 0.04692526534199715 Accuracy 0.8735000491142273\n",
      "Iteration 21460 Training loss 0.031029654666781425 Validation loss 0.04557379335165024 Accuracy 0.877375066280365\n",
      "Iteration 21470 Training loss 0.035942334681749344 Validation loss 0.047604747116565704 Accuracy 0.8723750710487366\n",
      "Iteration 21480 Training loss 0.02783042937517166 Validation loss 0.04344772547483444 Accuracy 0.8813750147819519\n",
      "Iteration 21490 Training loss 0.037905801087617874 Validation loss 0.05042929947376251 Accuracy 0.8627500534057617\n",
      "Iteration 21500 Training loss 0.02628023363649845 Validation loss 0.04460116848349571 Accuracy 0.8810000419616699\n",
      "Iteration 21510 Training loss 0.030947662889957428 Validation loss 0.043349526822566986 Accuracy 0.8828750252723694\n",
      "Iteration 21520 Training loss 0.03913295269012451 Validation loss 0.051867321133613586 Accuracy 0.8608750700950623\n",
      "Iteration 21530 Training loss 0.04431263729929924 Validation loss 0.05405690521001816 Accuracy 0.8545000553131104\n",
      "Iteration 21540 Training loss 0.026634735986590385 Validation loss 0.04392152652144432 Accuracy 0.8823750615119934\n",
      "Iteration 21550 Training loss 0.03767035901546478 Validation loss 0.05035591125488281 Accuracy 0.8647500276565552\n",
      "Iteration 21560 Training loss 0.031025337055325508 Validation loss 0.0469212532043457 Accuracy 0.8752500414848328\n",
      "Iteration 21570 Training loss 0.028217729181051254 Validation loss 0.04560772702097893 Accuracy 0.8766250610351562\n",
      "Iteration 21580 Training loss 0.03457029536366463 Validation loss 0.05226382613182068 Accuracy 0.862250030040741\n",
      "Iteration 21590 Training loss 0.02916860766708851 Validation loss 0.04404193162918091 Accuracy 0.8798750638961792\n",
      "Iteration 21600 Training loss 0.04357615485787392 Validation loss 0.04934464022517204 Accuracy 0.8672500252723694\n",
      "Iteration 21610 Training loss 0.02733572944998741 Validation loss 0.04684153199195862 Accuracy 0.8748750686645508\n",
      "Iteration 21620 Training loss 0.024854082614183426 Validation loss 0.04490826651453972 Accuracy 0.8796250224113464\n",
      "Iteration 21630 Training loss 0.029377566650509834 Validation loss 0.04926138371229172 Accuracy 0.8676250576972961\n",
      "Iteration 21640 Training loss 0.025969326496124268 Validation loss 0.04401220753788948 Accuracy 0.8807500600814819\n",
      "Iteration 21650 Training loss 0.03451992943882942 Validation loss 0.056605320423841476 Accuracy 0.8491250276565552\n",
      "Iteration 21660 Training loss 0.04384045675396919 Validation loss 0.04841075465083122 Accuracy 0.8713750243186951\n",
      "Iteration 21670 Training loss 0.02930101566016674 Validation loss 0.04390048608183861 Accuracy 0.8808750510215759\n",
      "Iteration 21680 Training loss 0.037549860775470734 Validation loss 0.04950625076889992 Accuracy 0.8647500276565552\n",
      "Iteration 21690 Training loss 0.028278669342398643 Validation loss 0.04350026324391365 Accuracy 0.8833750486373901\n",
      "Iteration 21700 Training loss 0.030210798606276512 Validation loss 0.04453267157077789 Accuracy 0.8800000548362732\n",
      "Iteration 21710 Training loss 0.022397048771381378 Validation loss 0.04313329607248306 Accuracy 0.8851250410079956\n",
      "Iteration 21720 Training loss 0.05213659256696701 Validation loss 0.06358563899993896 Accuracy 0.8295000195503235\n",
      "Iteration 21730 Training loss 0.029551252722740173 Validation loss 0.0453544445335865 Accuracy 0.8791250586509705\n",
      "Iteration 21740 Training loss 0.029729174450039864 Validation loss 0.0439055860042572 Accuracy 0.8810000419616699\n",
      "Iteration 21750 Training loss 0.023809870705008507 Validation loss 0.04381990805268288 Accuracy 0.8808750510215759\n",
      "Iteration 21760 Training loss 0.02577786147594452 Validation loss 0.043857648968696594 Accuracy 0.8815000653266907\n",
      "Iteration 21770 Training loss 0.029898500069975853 Validation loss 0.047531723976135254 Accuracy 0.8708750605583191\n",
      "Iteration 21780 Training loss 0.03282235935330391 Validation loss 0.046239111572504044 Accuracy 0.874250054359436\n",
      "Iteration 21790 Training loss 0.032577432692050934 Validation loss 0.04461420699954033 Accuracy 0.8792500495910645\n",
      "Iteration 21800 Training loss 0.04385031759738922 Validation loss 0.054647527635097504 Accuracy 0.8552500605583191\n",
      "Iteration 21810 Training loss 0.019874216988682747 Validation loss 0.04515815153717995 Accuracy 0.8785000443458557\n",
      "Iteration 21820 Training loss 0.030697070062160492 Validation loss 0.0446031279861927 Accuracy 0.8788750171661377\n",
      "Iteration 21830 Training loss 0.026576830074191093 Validation loss 0.043317604809999466 Accuracy 0.8830000162124634\n",
      "Iteration 21840 Training loss 0.04033338278532028 Validation loss 0.0561162605881691 Accuracy 0.8496250510215759\n",
      "Iteration 21850 Training loss 0.031158117577433586 Validation loss 0.045771580189466476 Accuracy 0.8763750195503235\n",
      "Iteration 21860 Training loss 0.027596918866038322 Validation loss 0.04435550794005394 Accuracy 0.8806250691413879\n",
      "Iteration 21870 Training loss 0.03043108619749546 Validation loss 0.04444678872823715 Accuracy 0.8813750147819519\n",
      "Iteration 21880 Training loss 0.02171936444938183 Validation loss 0.043573763221502304 Accuracy 0.8832500576972961\n",
      "Iteration 21890 Training loss 0.02539915032684803 Validation loss 0.044215358793735504 Accuracy 0.8805000185966492\n",
      "Iteration 21900 Training loss 0.02309201844036579 Validation loss 0.043767936527729034 Accuracy 0.8816250562667847\n",
      "Iteration 21910 Training loss 0.031749412417411804 Validation loss 0.04914995655417442 Accuracy 0.8673750162124634\n",
      "Iteration 21920 Training loss 0.02872609533369541 Validation loss 0.0434642918407917 Accuracy 0.8832500576972961\n",
      "Iteration 21930 Training loss 0.04700712114572525 Validation loss 0.0621170848608017 Accuracy 0.8328750133514404\n",
      "Iteration 21940 Training loss 0.036925308406353 Validation loss 0.05306617543101311 Accuracy 0.8560000658035278\n",
      "Iteration 21950 Training loss 0.029958071187138557 Validation loss 0.048635683953762054 Accuracy 0.8700000643730164\n",
      "Iteration 21960 Training loss 0.01870420202612877 Validation loss 0.043560996651649475 Accuracy 0.8810000419616699\n",
      "Iteration 21970 Training loss 0.048293616622686386 Validation loss 0.05839874595403671 Accuracy 0.8408750295639038\n",
      "Iteration 21980 Training loss 0.026345832273364067 Validation loss 0.04374231398105621 Accuracy 0.8841250538825989\n",
      "Iteration 21990 Training loss 0.04897494241595268 Validation loss 0.06355278939008713 Accuracy 0.8287500143051147\n",
      "Iteration 22000 Training loss 0.035765018314123154 Validation loss 0.04440298676490784 Accuracy 0.8795000314712524\n",
      "Iteration 22010 Training loss 0.02935892529785633 Validation loss 0.04560297727584839 Accuracy 0.8771250247955322\n",
      "Iteration 22020 Training loss 0.036473993211984634 Validation loss 0.04961061477661133 Accuracy 0.8680000305175781\n",
      "Iteration 22030 Training loss 0.02434569224715233 Validation loss 0.04595454782247543 Accuracy 0.8757500648498535\n",
      "Iteration 22040 Training loss 0.02575729973614216 Validation loss 0.04509378969669342 Accuracy 0.8790000677108765\n",
      "Iteration 22050 Training loss 0.02613677829504013 Validation loss 0.04553795978426933 Accuracy 0.8771250247955322\n",
      "Iteration 22060 Training loss 0.018502917140722275 Validation loss 0.045569147914648056 Accuracy 0.878000020980835\n",
      "Iteration 22070 Training loss 0.027290601283311844 Validation loss 0.04334743693470955 Accuracy 0.8826250433921814\n",
      "Iteration 22080 Training loss 0.01915365643799305 Validation loss 0.04337528347969055 Accuracy 0.8805000185966492\n",
      "Iteration 22090 Training loss 0.021100949496030807 Validation loss 0.04375595971941948 Accuracy 0.8822500705718994\n",
      "Iteration 22100 Training loss 0.05741095542907715 Validation loss 0.05971703678369522 Accuracy 0.8388750553131104\n",
      "Iteration 22110 Training loss 0.03234915807843208 Validation loss 0.04583808034658432 Accuracy 0.8768750429153442\n",
      "Iteration 22120 Training loss 0.02928607165813446 Validation loss 0.04357614368200302 Accuracy 0.8831250667572021\n",
      "Iteration 22130 Training loss 0.03029312938451767 Validation loss 0.04689681902527809 Accuracy 0.8733750581741333\n",
      "Iteration 22140 Training loss 0.03792361542582512 Validation loss 0.05213132128119469 Accuracy 0.8593750596046448\n",
      "Iteration 22150 Training loss 0.029830319806933403 Validation loss 0.046121746301651 Accuracy 0.874625027179718\n",
      "Iteration 22160 Training loss 0.02625933662056923 Validation loss 0.04431409388780594 Accuracy 0.8806250691413879\n",
      "Iteration 22170 Training loss 0.030963057652115822 Validation loss 0.04929281771183014 Accuracy 0.8675000667572021\n",
      "Iteration 22180 Training loss 0.029080938547849655 Validation loss 0.04838138073682785 Accuracy 0.8695000410079956\n",
      "Iteration 22190 Training loss 0.03578409552574158 Validation loss 0.04358096048235893 Accuracy 0.8835000395774841\n",
      "Iteration 22200 Training loss 0.0237155482172966 Validation loss 0.04432627931237221 Accuracy 0.8818750381469727\n",
      "Iteration 22210 Training loss 0.02407950349152088 Validation loss 0.04543242231011391 Accuracy 0.8791250586509705\n",
      "Iteration 22220 Training loss 0.028287198394536972 Validation loss 0.043472159653902054 Accuracy 0.8848750591278076\n",
      "Iteration 22230 Training loss 0.027233049273490906 Validation loss 0.044771257787942886 Accuracy 0.8802500367164612\n",
      "Iteration 22240 Training loss 0.029233602806925774 Validation loss 0.04330659657716751 Accuracy 0.8836250305175781\n",
      "Iteration 22250 Training loss 0.032744068652391434 Validation loss 0.05186900496482849 Accuracy 0.8606250286102295\n",
      "Iteration 22260 Training loss 0.023841729387640953 Validation loss 0.04402964562177658 Accuracy 0.8807500600814819\n",
      "Iteration 22270 Training loss 0.022869037464261055 Validation loss 0.04527072235941887 Accuracy 0.8801250457763672\n",
      "Iteration 22280 Training loss 0.034622564911842346 Validation loss 0.04936102032661438 Accuracy 0.8661250472068787\n",
      "Iteration 22290 Training loss 0.035014934837818146 Validation loss 0.04550300911068916 Accuracy 0.877500057220459\n",
      "Iteration 22300 Training loss 0.03463498130440712 Validation loss 0.04819633066654205 Accuracy 0.8692500591278076\n",
      "Iteration 22310 Training loss 0.0348559245467186 Validation loss 0.04846099764108658 Accuracy 0.8716250658035278\n",
      "Iteration 22320 Training loss 0.027781132608652115 Validation loss 0.049038998782634735 Accuracy 0.8687500357627869\n",
      "Iteration 22330 Training loss 0.025097450241446495 Validation loss 0.04323761537671089 Accuracy 0.8840000629425049\n",
      "Iteration 22340 Training loss 0.027478033676743507 Validation loss 0.04414401575922966 Accuracy 0.8815000653266907\n",
      "Iteration 22350 Training loss 0.0423366017639637 Validation loss 0.05889291316270828 Accuracy 0.8423750400543213\n",
      "Iteration 22360 Training loss 0.022146956995129585 Validation loss 0.0434473380446434 Accuracy 0.8836250305175781\n",
      "Iteration 22370 Training loss 0.0361165814101696 Validation loss 0.05421033874154091 Accuracy 0.8530000448226929\n",
      "Iteration 22380 Training loss 0.031473558396101 Validation loss 0.04348123446106911 Accuracy 0.8842500448226929\n",
      "Iteration 22390 Training loss 0.026254434138536453 Validation loss 0.04320753365755081 Accuracy 0.8840000629425049\n",
      "Iteration 22400 Training loss 0.02481091022491455 Validation loss 0.04337690770626068 Accuracy 0.8832500576972961\n",
      "Iteration 22410 Training loss 0.023783566430211067 Validation loss 0.044636666774749756 Accuracy 0.8792500495910645\n",
      "Iteration 22420 Training loss 0.03577851504087448 Validation loss 0.053624268621206284 Accuracy 0.8582500219345093\n",
      "Iteration 22430 Training loss 0.03629183769226074 Validation loss 0.0534597709774971 Accuracy 0.8566250205039978\n",
      "Iteration 22440 Training loss 0.025141283869743347 Validation loss 0.04467964172363281 Accuracy 0.8802500367164612\n",
      "Iteration 22450 Training loss 0.035451605916023254 Validation loss 0.04675384983420372 Accuracy 0.8736250400543213\n",
      "Iteration 22460 Training loss 0.02265167236328125 Validation loss 0.044668953865766525 Accuracy 0.8806250691413879\n",
      "Iteration 22470 Training loss 0.03584469109773636 Validation loss 0.0510319247841835 Accuracy 0.8643750548362732\n",
      "Iteration 22480 Training loss 0.0280989408493042 Validation loss 0.047147106379270554 Accuracy 0.874500036239624\n",
      "Iteration 22490 Training loss 0.03719349205493927 Validation loss 0.047688744962215424 Accuracy 0.8736250400543213\n",
      "Iteration 22500 Training loss 0.023745855316519737 Validation loss 0.04483867064118385 Accuracy 0.8793750405311584\n",
      "Iteration 22510 Training loss 0.030073091387748718 Validation loss 0.04396440461277962 Accuracy 0.8827500343322754\n",
      "Iteration 22520 Training loss 0.04003647714853287 Validation loss 0.049742747098207474 Accuracy 0.8663750290870667\n",
      "Iteration 22530 Training loss 0.03590604290366173 Validation loss 0.05345974117517471 Accuracy 0.8576250672340393\n",
      "Iteration 22540 Training loss 0.03383611515164375 Validation loss 0.047118477523326874 Accuracy 0.8735000491142273\n",
      "Iteration 22550 Training loss 0.03445544093847275 Validation loss 0.04633626714348793 Accuracy 0.8772500157356262\n",
      "Iteration 22560 Training loss 0.017724838107824326 Validation loss 0.04318332299590111 Accuracy 0.8825000524520874\n",
      "Iteration 22570 Training loss 0.025301406159996986 Validation loss 0.04596161097288132 Accuracy 0.8768750429153442\n",
      "Iteration 22580 Training loss 0.027083180844783783 Validation loss 0.04730924591422081 Accuracy 0.8725000619888306\n",
      "Iteration 22590 Training loss 0.03601221367716789 Validation loss 0.052254773676395416 Accuracy 0.8597500324249268\n",
      "Iteration 22600 Training loss 0.028950361534953117 Validation loss 0.04338614270091057 Accuracy 0.8832500576972961\n",
      "Iteration 22610 Training loss 0.027840472757816315 Validation loss 0.04534154757857323 Accuracy 0.8783750534057617\n",
      "Iteration 22620 Training loss 0.034192152321338654 Validation loss 0.048735253512859344 Accuracy 0.8697500228881836\n",
      "Iteration 22630 Training loss 0.024242127314209938 Validation loss 0.04621996358036995 Accuracy 0.874750018119812\n",
      "Iteration 22640 Training loss 0.025979427620768547 Validation loss 0.043560296297073364 Accuracy 0.8827500343322754\n",
      "Iteration 22650 Training loss 0.021609291434288025 Validation loss 0.04685099795460701 Accuracy 0.8753750324249268\n",
      "Iteration 22660 Training loss 0.01575743593275547 Validation loss 0.043257877230644226 Accuracy 0.8832500576972961\n",
      "Iteration 22670 Training loss 0.03937847167253494 Validation loss 0.052063632756471634 Accuracy 0.8600000143051147\n",
      "Iteration 22680 Training loss 0.025257879868149757 Validation loss 0.04335681349039078 Accuracy 0.8830000162124634\n",
      "Iteration 22690 Training loss 0.03008609637618065 Validation loss 0.04760425165295601 Accuracy 0.8708750605583191\n",
      "Iteration 22700 Training loss 0.02700505405664444 Validation loss 0.044949036091566086 Accuracy 0.8800000548362732\n",
      "Iteration 22710 Training loss 0.020189080387353897 Validation loss 0.04349870979785919 Accuracy 0.8825000524520874\n",
      "Iteration 22720 Training loss 0.04173700138926506 Validation loss 0.05930057540535927 Accuracy 0.8426250219345093\n",
      "Iteration 22730 Training loss 0.03170902281999588 Validation loss 0.04528925195336342 Accuracy 0.8796250224113464\n",
      "Iteration 22740 Training loss 0.03413095697760582 Validation loss 0.058617521077394485 Accuracy 0.843500018119812\n",
      "Iteration 22750 Training loss 0.026617029681801796 Validation loss 0.04362468048930168 Accuracy 0.8812500238418579\n",
      "Iteration 22760 Training loss 0.031446218490600586 Validation loss 0.04387588053941727 Accuracy 0.8810000419616699\n",
      "Iteration 22770 Training loss 0.03377533704042435 Validation loss 0.043138537555933 Accuracy 0.8857500553131104\n",
      "Iteration 22780 Training loss 0.023977389559149742 Validation loss 0.044218696653842926 Accuracy 0.8806250691413879\n",
      "Iteration 22790 Training loss 0.03202424943447113 Validation loss 0.044310715049505234 Accuracy 0.8816250562667847\n",
      "Iteration 22800 Training loss 0.02384563349187374 Validation loss 0.04491046071052551 Accuracy 0.878000020980835\n",
      "Iteration 22810 Training loss 0.034461162984371185 Validation loss 0.04714440926909447 Accuracy 0.8740000128746033\n",
      "Iteration 22820 Training loss 0.03715572878718376 Validation loss 0.04964965954422951 Accuracy 0.8672500252723694\n",
      "Iteration 22830 Training loss 0.021938709542155266 Validation loss 0.043047983199357986 Accuracy 0.8840000629425049\n",
      "Iteration 22840 Training loss 0.03377879410982132 Validation loss 0.050351642072200775 Accuracy 0.862000048160553\n",
      "Iteration 22850 Training loss 0.03733038157224655 Validation loss 0.04912792518734932 Accuracy 0.8672500252723694\n",
      "Iteration 22860 Training loss 0.027439869940280914 Validation loss 0.04466162249445915 Accuracy 0.8793750405311584\n",
      "Iteration 22870 Training loss 0.028069166466593742 Validation loss 0.04347812756896019 Accuracy 0.8817500472068787\n",
      "Iteration 22880 Training loss 0.030749019235372543 Validation loss 0.047086846083402634 Accuracy 0.8752500414848328\n",
      "Iteration 22890 Training loss 0.029511956498026848 Validation loss 0.04776506870985031 Accuracy 0.8738750219345093\n",
      "Iteration 22900 Training loss 0.02787538431584835 Validation loss 0.04711378365755081 Accuracy 0.874500036239624\n",
      "Iteration 22910 Training loss 0.022091740742325783 Validation loss 0.04311792552471161 Accuracy 0.8835000395774841\n",
      "Iteration 22920 Training loss 0.029547346755862236 Validation loss 0.045628078281879425 Accuracy 0.8797500133514404\n",
      "Iteration 22930 Training loss 0.025895850732922554 Validation loss 0.05190319940447807 Accuracy 0.862000048160553\n",
      "Iteration 22940 Training loss 0.03012053482234478 Validation loss 0.04292780160903931 Accuracy 0.8851250410079956\n",
      "Iteration 22950 Training loss 0.033043745905160904 Validation loss 0.04885042831301689 Accuracy 0.8681250214576721\n",
      "Iteration 22960 Training loss 0.025668004527688026 Validation loss 0.04471350833773613 Accuracy 0.8801250457763672\n",
      "Iteration 22970 Training loss 0.02615274302661419 Validation loss 0.044659145176410675 Accuracy 0.8786250352859497\n",
      "Iteration 22980 Training loss 0.031023850664496422 Validation loss 0.0448409840464592 Accuracy 0.8807500600814819\n",
      "Iteration 22990 Training loss 0.035468533635139465 Validation loss 0.047019537538290024 Accuracy 0.87437504529953\n",
      "Iteration 23000 Training loss 0.034945257008075714 Validation loss 0.047460950911045074 Accuracy 0.8730000257492065\n",
      "Iteration 23010 Training loss 0.030760902911424637 Validation loss 0.04312581941485405 Accuracy 0.8841250538825989\n",
      "Iteration 23020 Training loss 0.021627450361847878 Validation loss 0.045215874910354614 Accuracy 0.8772500157356262\n",
      "Iteration 23030 Training loss 0.028818663209676743 Validation loss 0.04717030003666878 Accuracy 0.8726250529289246\n",
      "Iteration 23040 Training loss 0.0258928295224905 Validation loss 0.043802179396152496 Accuracy 0.8830000162124634\n",
      "Iteration 23050 Training loss 0.022222649306058884 Validation loss 0.043615423142910004 Accuracy 0.8832500576972961\n",
      "Iteration 23060 Training loss 0.02784213423728943 Validation loss 0.04803035408258438 Accuracy 0.8696250319480896\n",
      "Iteration 23070 Training loss 0.030828384682536125 Validation loss 0.047782860696315765 Accuracy 0.8715000152587891\n",
      "Iteration 23080 Training loss 0.02616873010993004 Validation loss 0.045426033437252045 Accuracy 0.877500057220459\n",
      "Iteration 23090 Training loss 0.024148719385266304 Validation loss 0.04735623300075531 Accuracy 0.8727500438690186\n",
      "Iteration 23100 Training loss 0.03376182168722153 Validation loss 0.04955120384693146 Accuracy 0.8657500147819519\n",
      "Iteration 23110 Training loss 0.022959312424063683 Validation loss 0.04446368291974068 Accuracy 0.8808750510215759\n",
      "Iteration 23120 Training loss 0.038025520741939545 Validation loss 0.049327146261930466 Accuracy 0.8696250319480896\n",
      "Iteration 23130 Training loss 0.031573113054037094 Validation loss 0.045289330184459686 Accuracy 0.877625048160553\n",
      "Iteration 23140 Training loss 0.02771332673728466 Validation loss 0.04856973513960838 Accuracy 0.8690000176429749\n",
      "Iteration 23150 Training loss 0.026256583631038666 Validation loss 0.04303068667650223 Accuracy 0.8845000267028809\n",
      "Iteration 23160 Training loss 0.03026149608194828 Validation loss 0.0450129434466362 Accuracy 0.8772500157356262\n",
      "Iteration 23170 Training loss 0.02703140676021576 Validation loss 0.04419928044080734 Accuracy 0.8813750147819519\n",
      "Iteration 23180 Training loss 0.05125909671187401 Validation loss 0.06015137583017349 Accuracy 0.8382500410079956\n",
      "Iteration 23190 Training loss 0.030494842678308487 Validation loss 0.04459967091679573 Accuracy 0.8806250691413879\n",
      "Iteration 23200 Training loss 0.02258540876209736 Validation loss 0.04606040194630623 Accuracy 0.8758750557899475\n",
      "Iteration 23210 Training loss 0.031336233019828796 Validation loss 0.04575579985976219 Accuracy 0.8772500157356262\n",
      "Iteration 23220 Training loss 0.027744939550757408 Validation loss 0.04501600190997124 Accuracy 0.8793750405311584\n",
      "Iteration 23230 Training loss 0.03295355290174484 Validation loss 0.04565265029668808 Accuracy 0.8808750510215759\n",
      "Iteration 23240 Training loss 0.027921566739678383 Validation loss 0.049729298800230026 Accuracy 0.8672500252723694\n",
      "Iteration 23250 Training loss 0.02692604996263981 Validation loss 0.045896418392658234 Accuracy 0.874750018119812\n",
      "Iteration 23260 Training loss 0.01865723915398121 Validation loss 0.04317795857787132 Accuracy 0.8856250643730164\n",
      "Iteration 23270 Training loss 0.03441968187689781 Validation loss 0.0533091202378273 Accuracy 0.8561250567436218\n",
      "Iteration 23280 Training loss 0.029054032638669014 Validation loss 0.04748856648802757 Accuracy 0.8712500333786011\n",
      "Iteration 23290 Training loss 0.030288204550743103 Validation loss 0.04401040077209473 Accuracy 0.8823750615119934\n",
      "Iteration 23300 Training loss 0.034285951405763626 Validation loss 0.050020910799503326 Accuracy 0.8632500171661377\n",
      "Iteration 23310 Training loss 0.03675481304526329 Validation loss 0.055613674223423004 Accuracy 0.8490000367164612\n",
      "Iteration 23320 Training loss 0.04846290498971939 Validation loss 0.055043768137693405 Accuracy 0.8518750667572021\n",
      "Iteration 23330 Training loss 0.02705366350710392 Validation loss 0.0434931218624115 Accuracy 0.8833750486373901\n",
      "Iteration 23340 Training loss 0.03237513080239296 Validation loss 0.04355311766266823 Accuracy 0.8835000395774841\n",
      "Iteration 23350 Training loss 0.026000306010246277 Validation loss 0.04333715885877609 Accuracy 0.8826250433921814\n",
      "Iteration 23360 Training loss 0.041883885860443115 Validation loss 0.05500065162777901 Accuracy 0.8542500138282776\n",
      "Iteration 23370 Training loss 0.02697502262890339 Validation loss 0.04334582760930061 Accuracy 0.8836250305175781\n",
      "Iteration 23380 Training loss 0.04303557425737381 Validation loss 0.05485280975699425 Accuracy 0.8536250591278076\n",
      "Iteration 23390 Training loss 0.023593615740537643 Validation loss 0.04607471823692322 Accuracy 0.8767500519752502\n",
      "Iteration 23400 Training loss 0.02728142961859703 Validation loss 0.04391907528042793 Accuracy 0.8818750381469727\n",
      "Iteration 23410 Training loss 0.027728039771318436 Validation loss 0.04621702805161476 Accuracy 0.8737500309944153\n",
      "Iteration 23420 Training loss 0.023303521797060966 Validation loss 0.04722484201192856 Accuracy 0.8711250424385071\n",
      "Iteration 23430 Training loss 0.022411933168768883 Validation loss 0.04287344589829445 Accuracy 0.8856250643730164\n",
      "Iteration 23440 Training loss 0.031012360006570816 Validation loss 0.04815130680799484 Accuracy 0.8691250681877136\n",
      "Iteration 23450 Training loss 0.04723583161830902 Validation loss 0.05978769063949585 Accuracy 0.8378750681877136\n",
      "Iteration 23460 Training loss 0.024909671396017075 Validation loss 0.045151032507419586 Accuracy 0.8798750638961792\n",
      "Iteration 23470 Training loss 0.029200224205851555 Validation loss 0.04291239753365517 Accuracy 0.8863750696182251\n",
      "Iteration 23480 Training loss 0.02906433679163456 Validation loss 0.049287717789411545 Accuracy 0.8676250576972961\n",
      "Iteration 23490 Training loss 0.03217135742306709 Validation loss 0.044229522347450256 Accuracy 0.8825000524520874\n",
      "Iteration 23500 Training loss 0.034572482109069824 Validation loss 0.05022325739264488 Accuracy 0.8655000329017639\n",
      "Iteration 23510 Training loss 0.030339695513248444 Validation loss 0.04337512329220772 Accuracy 0.8828750252723694\n",
      "Iteration 23520 Training loss 0.02037033811211586 Validation loss 0.04321257025003433 Accuracy 0.8833750486373901\n",
      "Iteration 23530 Training loss 0.021554499864578247 Validation loss 0.04557844623923302 Accuracy 0.8790000677108765\n",
      "Iteration 23540 Training loss 0.032410506159067154 Validation loss 0.04367458447813988 Accuracy 0.8818750381469727\n",
      "Iteration 23550 Training loss 0.040584687143564224 Validation loss 0.0560755580663681 Accuracy 0.8515000343322754\n",
      "Iteration 23560 Training loss 0.026680031791329384 Validation loss 0.04545877128839493 Accuracy 0.8792500495910645\n",
      "Iteration 23570 Training loss 0.03492527827620506 Validation loss 0.05318592116236687 Accuracy 0.8581250309944153\n",
      "Iteration 23580 Training loss 0.022467149421572685 Validation loss 0.043388888239860535 Accuracy 0.8857500553131104\n",
      "Iteration 23590 Training loss 0.0280638188123703 Validation loss 0.04442661628127098 Accuracy 0.8806250691413879\n",
      "Iteration 23600 Training loss 0.025364074856042862 Validation loss 0.043349433690309525 Accuracy 0.8840000629425049\n",
      "Iteration 23610 Training loss 0.0424204096198082 Validation loss 0.05196729302406311 Accuracy 0.8605000376701355\n",
      "Iteration 23620 Training loss 0.029351472854614258 Validation loss 0.04425809904932976 Accuracy 0.8835000395774841\n",
      "Iteration 23630 Training loss 0.018686337396502495 Validation loss 0.04296175763010979 Accuracy 0.8853750228881836\n",
      "Iteration 23640 Training loss 0.028408564627170563 Validation loss 0.04644625261425972 Accuracy 0.8760000467300415\n",
      "Iteration 23650 Training loss 0.023766329512000084 Validation loss 0.04655538499355316 Accuracy 0.8752500414848328\n",
      "Iteration 23660 Training loss 0.027742484584450722 Validation loss 0.04580758884549141 Accuracy 0.8756250143051147\n",
      "Iteration 23670 Training loss 0.02360115759074688 Validation loss 0.04627268761396408 Accuracy 0.874625027179718\n",
      "Iteration 23680 Training loss 0.026992149651050568 Validation loss 0.04778175428509712 Accuracy 0.8730000257492065\n",
      "Iteration 23690 Training loss 0.03482740744948387 Validation loss 0.051878832280635834 Accuracy 0.862250030040741\n",
      "Iteration 23700 Training loss 0.033024273812770844 Validation loss 0.04562755674123764 Accuracy 0.877875030040741\n",
      "Iteration 23710 Training loss 0.02729794941842556 Validation loss 0.044153403490781784 Accuracy 0.8802500367164612\n",
      "Iteration 23720 Training loss 0.018682751804590225 Validation loss 0.04360662400722504 Accuracy 0.8850000500679016\n",
      "Iteration 23730 Training loss 0.031764086335897446 Validation loss 0.0479818694293499 Accuracy 0.8707500696182251\n",
      "Iteration 23740 Training loss 0.020290512591600418 Validation loss 0.04373187571763992 Accuracy 0.8817500472068787\n",
      "Iteration 23750 Training loss 0.0332455113530159 Validation loss 0.04835735633969307 Accuracy 0.8700000643730164\n",
      "Iteration 23760 Training loss 0.021512597799301147 Validation loss 0.0458805114030838 Accuracy 0.8770000338554382\n",
      "Iteration 23770 Training loss 0.024514364078640938 Validation loss 0.04278066009283066 Accuracy 0.8848750591278076\n",
      "Iteration 23780 Training loss 0.028665117919445038 Validation loss 0.04551459848880768 Accuracy 0.8765000700950623\n",
      "Iteration 23790 Training loss 0.038051050156354904 Validation loss 0.05513189360499382 Accuracy 0.8516250252723694\n",
      "Iteration 23800 Training loss 0.027215516194701195 Validation loss 0.04401050880551338 Accuracy 0.8810000419616699\n",
      "Iteration 23810 Training loss 0.02757122367620468 Validation loss 0.043806400150060654 Accuracy 0.8835000395774841\n",
      "Iteration 23820 Training loss 0.031733375042676926 Validation loss 0.04532211273908615 Accuracy 0.8788750171661377\n",
      "Iteration 23830 Training loss 0.017424948513507843 Validation loss 0.043214116245508194 Accuracy 0.8852500319480896\n",
      "Iteration 23840 Training loss 0.019602729007601738 Validation loss 0.04311123862862587 Accuracy 0.8856250643730164\n",
      "Iteration 23850 Training loss 0.044500138610601425 Validation loss 0.060464244335889816 Accuracy 0.8395000696182251\n",
      "Iteration 23860 Training loss 0.016048425808548927 Validation loss 0.04354139789938927 Accuracy 0.8823750615119934\n",
      "Iteration 23870 Training loss 0.020637627691030502 Validation loss 0.0436076745390892 Accuracy 0.8823750615119934\n",
      "Iteration 23880 Training loss 0.03464910760521889 Validation loss 0.05559060722589493 Accuracy 0.8531250357627869\n",
      "Iteration 23890 Training loss 0.024760980159044266 Validation loss 0.04413742944598198 Accuracy 0.8835000395774841\n",
      "Iteration 23900 Training loss 0.02875022031366825 Validation loss 0.0427674800157547 Accuracy 0.8837500214576721\n",
      "Iteration 23910 Training loss 0.03341272100806236 Validation loss 0.04572085663676262 Accuracy 0.8795000314712524\n",
      "Iteration 23920 Training loss 0.03968598693609238 Validation loss 0.06025448441505432 Accuracy 0.8436250686645508\n",
      "Iteration 23930 Training loss 0.027798909693956375 Validation loss 0.044474683701992035 Accuracy 0.8806250691413879\n",
      "Iteration 23940 Training loss 0.02588958479464054 Validation loss 0.04475381597876549 Accuracy 0.8801250457763672\n",
      "Iteration 23950 Training loss 0.022021615877747536 Validation loss 0.04294929653406143 Accuracy 0.8841250538825989\n",
      "Iteration 23960 Training loss 0.027032148092985153 Validation loss 0.046904463320970535 Accuracy 0.8730000257492065\n",
      "Iteration 23970 Training loss 0.02218765765428543 Validation loss 0.04294746741652489 Accuracy 0.8861250281333923\n",
      "Iteration 23980 Training loss 0.02296171523630619 Validation loss 0.0445776991546154 Accuracy 0.8796250224113464\n",
      "Iteration 23990 Training loss 0.024613594636321068 Validation loss 0.04314565658569336 Accuracy 0.8856250643730164\n",
      "Iteration 24000 Training loss 0.03275895491242409 Validation loss 0.05359998717904091 Accuracy 0.8548750281333923\n",
      "Iteration 24010 Training loss 0.02562757395207882 Validation loss 0.046499911695718765 Accuracy 0.8763750195503235\n",
      "Iteration 24020 Training loss 0.022273458540439606 Validation loss 0.04334431514143944 Accuracy 0.8840000629425049\n",
      "Iteration 24030 Training loss 0.042239703238010406 Validation loss 0.0494733601808548 Accuracy 0.8668750524520874\n",
      "Iteration 24040 Training loss 0.027977408841252327 Validation loss 0.04440262168645859 Accuracy 0.8810000419616699\n",
      "Iteration 24050 Training loss 0.02925601415336132 Validation loss 0.05075208470225334 Accuracy 0.8646250367164612\n",
      "Iteration 24060 Training loss 0.03412606939673424 Validation loss 0.04669547080993652 Accuracy 0.8762500286102295\n",
      "Iteration 24070 Training loss 0.02853231318295002 Validation loss 0.04320446401834488 Accuracy 0.8853750228881836\n",
      "Iteration 24080 Training loss 0.02144789882004261 Validation loss 0.04289105162024498 Accuracy 0.8855000138282776\n",
      "Iteration 24090 Training loss 0.023724880069494247 Validation loss 0.04435007646679878 Accuracy 0.8822500705718994\n",
      "Iteration 24100 Training loss 0.02439049631357193 Validation loss 0.04854121804237366 Accuracy 0.8688750267028809\n",
      "Iteration 24110 Training loss 0.046635378152132034 Validation loss 0.05725502222776413 Accuracy 0.8476250171661377\n",
      "Iteration 24120 Training loss 0.025018075481057167 Validation loss 0.04664471745491028 Accuracy 0.8750000596046448\n",
      "Iteration 24130 Training loss 0.03223777189850807 Validation loss 0.04817478358745575 Accuracy 0.8727500438690186\n",
      "Iteration 24140 Training loss 0.02973809652030468 Validation loss 0.0430346317589283 Accuracy 0.8847500681877136\n",
      "Iteration 24150 Training loss 0.021070774644613266 Validation loss 0.04416525363922119 Accuracy 0.8831250667572021\n",
      "Iteration 24160 Training loss 0.028396304696798325 Validation loss 0.04669184982776642 Accuracy 0.87437504529953\n",
      "Iteration 24170 Training loss 0.030721651390194893 Validation loss 0.04609966650605202 Accuracy 0.8783750534057617\n",
      "Iteration 24180 Training loss 0.016246061772108078 Validation loss 0.043232712894678116 Accuracy 0.8826250433921814\n",
      "Iteration 24190 Training loss 0.029242966324090958 Validation loss 0.04374079778790474 Accuracy 0.8840000629425049\n",
      "Iteration 24200 Training loss 0.028607293963432312 Validation loss 0.04281815513968468 Accuracy 0.8850000500679016\n",
      "Iteration 24210 Training loss 0.032896265387535095 Validation loss 0.04735693708062172 Accuracy 0.874125063419342\n",
      "Iteration 24220 Training loss 0.02513064071536064 Validation loss 0.044180672615766525 Accuracy 0.8800000548362732\n",
      "Iteration 24230 Training loss 0.04229307919740677 Validation loss 0.056076765060424805 Accuracy 0.8496250510215759\n",
      "Iteration 24240 Training loss 0.03720807284116745 Validation loss 0.046847227960824966 Accuracy 0.8728750348091125\n",
      "Iteration 24250 Training loss 0.02182774804532528 Validation loss 0.04274660721421242 Accuracy 0.8861250281333923\n",
      "Iteration 24260 Training loss 0.03990449756383896 Validation loss 0.04847653582692146 Accuracy 0.8696250319480896\n",
      "Iteration 24270 Training loss 0.02400367148220539 Validation loss 0.04401011019945145 Accuracy 0.8828750252723694\n",
      "Iteration 24280 Training loss 0.01959698460996151 Validation loss 0.042992003262043 Accuracy 0.8858750462532043\n",
      "Iteration 24290 Training loss 0.02816818654537201 Validation loss 0.048953186720609665 Accuracy 0.8685000538825989\n",
      "Iteration 24300 Training loss 0.028267141431570053 Validation loss 0.0446116179227829 Accuracy 0.8818750381469727\n",
      "Iteration 24310 Training loss 0.021340694278478622 Validation loss 0.04488036409020424 Accuracy 0.8805000185966492\n",
      "Iteration 24320 Training loss 0.027168162167072296 Validation loss 0.043114155530929565 Accuracy 0.8850000500679016\n",
      "Iteration 24330 Training loss 0.02730974368751049 Validation loss 0.043814752250909805 Accuracy 0.8825000524520874\n",
      "Iteration 24340 Training loss 0.022806501016020775 Validation loss 0.04360879212617874 Accuracy 0.8845000267028809\n",
      "Iteration 24350 Training loss 0.023223189637064934 Validation loss 0.043851159512996674 Accuracy 0.8833750486373901\n",
      "Iteration 24360 Training loss 0.02873428538441658 Validation loss 0.047794584184885025 Accuracy 0.8720000386238098\n",
      "Iteration 24370 Training loss 0.03273000195622444 Validation loss 0.046593256294727325 Accuracy 0.8766250610351562\n",
      "Iteration 24380 Training loss 0.022915029898285866 Validation loss 0.04376230761408806 Accuracy 0.8811250329017639\n",
      "Iteration 24390 Training loss 0.03351942077279091 Validation loss 0.05074587091803551 Accuracy 0.8635000586509705\n",
      "Iteration 24400 Training loss 0.02612602710723877 Validation loss 0.04324930161237717 Accuracy 0.8836250305175781\n",
      "Iteration 24410 Training loss 0.02613096497952938 Validation loss 0.049832750111818314 Accuracy 0.8660000562667847\n",
      "Iteration 24420 Training loss 0.02841760963201523 Validation loss 0.04551359638571739 Accuracy 0.878125011920929\n",
      "Iteration 24430 Training loss 0.023516248911619186 Validation loss 0.04281235113739967 Accuracy 0.8875000476837158\n",
      "Iteration 24440 Training loss 0.023990070447325706 Validation loss 0.0449339933693409 Accuracy 0.8811250329017639\n",
      "Iteration 24450 Training loss 0.04463784024119377 Validation loss 0.05130045861005783 Accuracy 0.8635000586509705\n",
      "Iteration 24460 Training loss 0.02639918215572834 Validation loss 0.04428559169173241 Accuracy 0.8821250200271606\n",
      "Iteration 24470 Training loss 0.0472150519490242 Validation loss 0.05479271337389946 Accuracy 0.8528750538825989\n",
      "Iteration 24480 Training loss 0.02481985278427601 Validation loss 0.043396227061748505 Accuracy 0.8836250305175781\n",
      "Iteration 24490 Training loss 0.02586360089480877 Validation loss 0.04799732565879822 Accuracy 0.8711250424385071\n",
      "Iteration 24500 Training loss 0.027364904060959816 Validation loss 0.04520154744386673 Accuracy 0.8798750638961792\n",
      "Iteration 24510 Training loss 0.029434213414788246 Validation loss 0.05020955950021744 Accuracy 0.8662500381469727\n",
      "Iteration 24520 Training loss 0.02807575650513172 Validation loss 0.043690379709005356 Accuracy 0.8836250305175781\n",
      "Iteration 24530 Training loss 0.023202888667583466 Validation loss 0.043319426476955414 Accuracy 0.8843750357627869\n",
      "Iteration 24540 Training loss 0.03428024798631668 Validation loss 0.049814704805612564 Accuracy 0.8652500510215759\n",
      "Iteration 24550 Training loss 0.044196829199790955 Validation loss 0.06391501426696777 Accuracy 0.8313750624656677\n",
      "Iteration 24560 Training loss 0.031648021191358566 Validation loss 0.049271587282419205 Accuracy 0.8688750267028809\n",
      "Iteration 24570 Training loss 0.02147713303565979 Validation loss 0.04305773973464966 Accuracy 0.8850000500679016\n",
      "Iteration 24580 Training loss 0.05080585926771164 Validation loss 0.0592823252081871 Accuracy 0.8407500386238098\n",
      "Iteration 24590 Training loss 0.01880643516778946 Validation loss 0.04371446743607521 Accuracy 0.8840000629425049\n",
      "Iteration 24600 Training loss 0.02764844335615635 Validation loss 0.0455329604446888 Accuracy 0.8783750534057617\n",
      "Iteration 24610 Training loss 0.02861883118748665 Validation loss 0.04434904828667641 Accuracy 0.8821250200271606\n",
      "Iteration 24620 Training loss 0.0372609905898571 Validation loss 0.04399515688419342 Accuracy 0.8831250667572021\n",
      "Iteration 24630 Training loss 0.028693517670035362 Validation loss 0.04291351139545441 Accuracy 0.8863750696182251\n",
      "Iteration 24640 Training loss 0.03164849430322647 Validation loss 0.04493967071175575 Accuracy 0.8793750405311584\n",
      "Iteration 24650 Training loss 0.030800823122262955 Validation loss 0.04354562610387802 Accuracy 0.8828750252723694\n",
      "Iteration 24660 Training loss 0.02706148475408554 Validation loss 0.05194942653179169 Accuracy 0.8603750467300415\n",
      "Iteration 24670 Training loss 0.024839000776410103 Validation loss 0.04397981986403465 Accuracy 0.8828750252723694\n",
      "Iteration 24680 Training loss 0.026840396225452423 Validation loss 0.045142922550439835 Accuracy 0.8795000314712524\n",
      "Iteration 24690 Training loss 0.027582161128520966 Validation loss 0.043401528149843216 Accuracy 0.8860000371932983\n",
      "Iteration 24700 Training loss 0.022483639419078827 Validation loss 0.047914233058691025 Accuracy 0.8697500228881836\n",
      "Iteration 24710 Training loss 0.027379317209124565 Validation loss 0.04955432191491127 Accuracy 0.8652500510215759\n",
      "Iteration 24720 Training loss 0.027468914166092873 Validation loss 0.04357219859957695 Accuracy 0.8826250433921814\n",
      "Iteration 24730 Training loss 0.023087767884135246 Validation loss 0.044570960104465485 Accuracy 0.8797500133514404\n",
      "Iteration 24740 Training loss 0.03828402981162071 Validation loss 0.05084718391299248 Accuracy 0.8626250624656677\n",
      "Iteration 24750 Training loss 0.02171037532389164 Validation loss 0.04333715885877609 Accuracy 0.8845000267028809\n",
      "Iteration 24760 Training loss 0.02814578264951706 Validation loss 0.053487177938222885 Accuracy 0.8550000190734863\n",
      "Iteration 24770 Training loss 0.022996459156274796 Validation loss 0.04548151046037674 Accuracy 0.8805000185966492\n",
      "Iteration 24780 Training loss 0.02506588213145733 Validation loss 0.04375351592898369 Accuracy 0.8832500576972961\n",
      "Iteration 24790 Training loss 0.021910347044467926 Validation loss 0.04583103582262993 Accuracy 0.877625048160553\n",
      "Iteration 24800 Training loss 0.0266575887799263 Validation loss 0.04697619006037712 Accuracy 0.8756250143051147\n",
      "Iteration 24810 Training loss 0.022356228902935982 Validation loss 0.04287105053663254 Accuracy 0.8856250643730164\n",
      "Iteration 24820 Training loss 0.025431908667087555 Validation loss 0.04297555238008499 Accuracy 0.8850000500679016\n",
      "Iteration 24830 Training loss 0.024873537942767143 Validation loss 0.043101608753204346 Accuracy 0.8843750357627869\n",
      "Iteration 24840 Training loss 0.02690444327890873 Validation loss 0.046430185437202454 Accuracy 0.8761250376701355\n",
      "Iteration 24850 Training loss 0.02122870832681656 Validation loss 0.043186888098716736 Accuracy 0.8836250305175781\n",
      "Iteration 24860 Training loss 0.022583017125725746 Validation loss 0.04309932887554169 Accuracy 0.8853750228881836\n",
      "Iteration 24870 Training loss 0.026256702840328217 Validation loss 0.04344963654875755 Accuracy 0.8848750591278076\n",
      "Iteration 24880 Training loss 0.03358086571097374 Validation loss 0.051923274993896484 Accuracy 0.861750066280365\n",
      "Iteration 24890 Training loss 0.0275360569357872 Validation loss 0.04565086588263512 Accuracy 0.8788750171661377\n",
      "Iteration 24900 Training loss 0.03378407284617424 Validation loss 0.04782335087656975 Accuracy 0.8727500438690186\n",
      "Iteration 24910 Training loss 0.021092083305120468 Validation loss 0.04310845211148262 Accuracy 0.8847500681877136\n",
      "Iteration 24920 Training loss 0.027187902480363846 Validation loss 0.0430133119225502 Accuracy 0.8847500681877136\n",
      "Iteration 24930 Training loss 0.02815139666199684 Validation loss 0.045973844826221466 Accuracy 0.8785000443458557\n",
      "Iteration 24940 Training loss 0.027044067159295082 Validation loss 0.04381067305803299 Accuracy 0.8838750123977661\n",
      "Iteration 24950 Training loss 0.026466859504580498 Validation loss 0.04755040258169174 Accuracy 0.8721250295639038\n",
      "Iteration 24960 Training loss 0.024105383083224297 Validation loss 0.04740526154637337 Accuracy 0.8737500309944153\n",
      "Iteration 24970 Training loss 0.02539835311472416 Validation loss 0.0442420095205307 Accuracy 0.8821250200271606\n",
      "Iteration 24980 Training loss 0.03465403616428375 Validation loss 0.05285999923944473 Accuracy 0.8597500324249268\n",
      "Iteration 24990 Training loss 0.024035591632127762 Validation loss 0.043400928378105164 Accuracy 0.8835000395774841\n",
      "Iteration 25000 Training loss 0.03681030496954918 Validation loss 0.05171055719256401 Accuracy 0.8593750596046448\n",
      "Iteration 25010 Training loss 0.021071305498480797 Validation loss 0.043697573244571686 Accuracy 0.8831250667572021\n",
      "Iteration 25020 Training loss 0.02998768351972103 Validation loss 0.05214734748005867 Accuracy 0.8570000529289246\n",
      "Iteration 25030 Training loss 0.027821391820907593 Validation loss 0.043109193444252014 Accuracy 0.8852500319480896\n",
      "Iteration 25040 Training loss 0.022329119965434074 Validation loss 0.04806913807988167 Accuracy 0.8708750605583191\n",
      "Iteration 25050 Training loss 0.024385085329413414 Validation loss 0.045606277883052826 Accuracy 0.878000020980835\n",
      "Iteration 25060 Training loss 0.027850594371557236 Validation loss 0.045015107840299606 Accuracy 0.8785000443458557\n",
      "Iteration 25070 Training loss 0.02869495004415512 Validation loss 0.05071625858545303 Accuracy 0.8641250133514404\n",
      "Iteration 25080 Training loss 0.02724521979689598 Validation loss 0.04347698390483856 Accuracy 0.8823750615119934\n",
      "Iteration 25090 Training loss 0.03743917495012283 Validation loss 0.04960408806800842 Accuracy 0.8660000562667847\n",
      "Iteration 25100 Training loss 0.05222215875983238 Validation loss 0.059833794832229614 Accuracy 0.8400000333786011\n",
      "Iteration 25110 Training loss 0.026074372231960297 Validation loss 0.043802566826343536 Accuracy 0.8823750615119934\n",
      "Iteration 25120 Training loss 0.022179869934916496 Validation loss 0.043003980070352554 Accuracy 0.8858750462532043\n",
      "Iteration 25130 Training loss 0.02143576182425022 Validation loss 0.044068217277526855 Accuracy 0.8821250200271606\n",
      "Iteration 25140 Training loss 0.03720412030816078 Validation loss 0.04801587015390396 Accuracy 0.8720000386238098\n",
      "Iteration 25150 Training loss 0.025657707825303078 Validation loss 0.044967856258153915 Accuracy 0.8803750276565552\n",
      "Iteration 25160 Training loss 0.028123142197728157 Validation loss 0.0451035238802433 Accuracy 0.8802500367164612\n",
      "Iteration 25170 Training loss 0.02133384346961975 Validation loss 0.04294944182038307 Accuracy 0.8848750591278076\n",
      "Iteration 25180 Training loss 0.024987434968352318 Validation loss 0.04517010599374771 Accuracy 0.8786250352859497\n",
      "Iteration 25190 Training loss 0.049009378999471664 Validation loss 0.0606219545006752 Accuracy 0.8412500619888306\n",
      "Iteration 25200 Training loss 0.023142943158745766 Validation loss 0.04271204024553299 Accuracy 0.8865000605583191\n",
      "Iteration 25210 Training loss 0.02584853023290634 Validation loss 0.04383907839655876 Accuracy 0.8820000290870667\n",
      "Iteration 25220 Training loss 0.02190399542450905 Validation loss 0.04411914944648743 Accuracy 0.8820000290870667\n",
      "Iteration 25230 Training loss 0.02633335068821907 Validation loss 0.04287773370742798 Accuracy 0.8848750591278076\n",
      "Iteration 25240 Training loss 0.02014349400997162 Validation loss 0.0426635779440403 Accuracy 0.8868750333786011\n",
      "Iteration 25250 Training loss 0.01674267277121544 Validation loss 0.042952410876750946 Accuracy 0.8870000243186951\n",
      "Iteration 25260 Training loss 0.0389048270881176 Validation loss 0.058414384722709656 Accuracy 0.8437500596046448\n",
      "Iteration 25270 Training loss 0.026676161214709282 Validation loss 0.04317879676818848 Accuracy 0.8845000267028809\n",
      "Iteration 25280 Training loss 0.02888435684144497 Validation loss 0.04866235703229904 Accuracy 0.8698750138282776\n",
      "Iteration 25290 Training loss 0.03277868777513504 Validation loss 0.05154762417078018 Accuracy 0.8598750233650208\n",
      "Iteration 25300 Training loss 0.02335629239678383 Validation loss 0.04390754923224449 Accuracy 0.8815000653266907\n",
      "Iteration 25310 Training loss 0.04195322096347809 Validation loss 0.0600462481379509 Accuracy 0.8402500152587891\n",
      "Iteration 25320 Training loss 0.027022255584597588 Validation loss 0.04506855830550194 Accuracy 0.8808750510215759\n",
      "Iteration 25330 Training loss 0.022429637610912323 Validation loss 0.04693051055073738 Accuracy 0.8738750219345093\n",
      "Iteration 25340 Training loss 0.02546602673828602 Validation loss 0.045224521309137344 Accuracy 0.8783750534057617\n",
      "Iteration 25350 Training loss 0.02745564468204975 Validation loss 0.04562972113490105 Accuracy 0.8790000677108765\n",
      "Iteration 25360 Training loss 0.024300966411828995 Validation loss 0.04277496039867401 Accuracy 0.8858750462532043\n",
      "Iteration 25370 Training loss 0.03093324601650238 Validation loss 0.049002524465322495 Accuracy 0.8680000305175781\n",
      "Iteration 25380 Training loss 0.024679280817508698 Validation loss 0.043521732091903687 Accuracy 0.8827500343322754\n",
      "Iteration 25390 Training loss 0.021795744076371193 Validation loss 0.04251908138394356 Accuracy 0.8867500424385071\n",
      "Iteration 25400 Training loss 0.02475547231733799 Validation loss 0.042670320719480515 Accuracy 0.8858750462532043\n",
      "Iteration 25410 Training loss 0.03881080821156502 Validation loss 0.05879422277212143 Accuracy 0.8451250195503235\n",
      "Iteration 25420 Training loss 0.021043984219431877 Validation loss 0.04311833903193474 Accuracy 0.8853750228881836\n",
      "Iteration 25430 Training loss 0.025798477232456207 Validation loss 0.04330706223845482 Accuracy 0.8848750591278076\n",
      "Iteration 25440 Training loss 0.029055766761302948 Validation loss 0.04532620310783386 Accuracy 0.8802500367164612\n",
      "Iteration 25450 Training loss 0.02630416862666607 Validation loss 0.04363745450973511 Accuracy 0.8850000500679016\n",
      "Iteration 25460 Training loss 0.025393445044755936 Validation loss 0.042760588228702545 Accuracy 0.8870000243186951\n",
      "Iteration 25470 Training loss 0.02278154157102108 Validation loss 0.043682750314474106 Accuracy 0.8821250200271606\n",
      "Iteration 25480 Training loss 0.022699343040585518 Validation loss 0.04408402740955353 Accuracy 0.8821250200271606\n",
      "Iteration 25490 Training loss 0.02362086810171604 Validation loss 0.043578896671533585 Accuracy 0.8831250667572021\n",
      "Iteration 25500 Training loss 0.02170085906982422 Validation loss 0.04507506638765335 Accuracy 0.8798750638961792\n",
      "Iteration 25510 Training loss 0.014909088611602783 Validation loss 0.04274854063987732 Accuracy 0.8872500658035278\n",
      "Iteration 25520 Training loss 0.044557664543390274 Validation loss 0.04846461862325668 Accuracy 0.8688750267028809\n",
      "Iteration 25530 Training loss 0.02581438235938549 Validation loss 0.04367627203464508 Accuracy 0.8832500576972961\n",
      "Iteration 25540 Training loss 0.02882338874042034 Validation loss 0.04702363908290863 Accuracy 0.874250054359436\n",
      "Iteration 25550 Training loss 0.027904709801077843 Validation loss 0.04478774592280388 Accuracy 0.8792500495910645\n",
      "Iteration 25560 Training loss 0.02840365283191204 Validation loss 0.04705610126256943 Accuracy 0.8727500438690186\n",
      "Iteration 25570 Training loss 0.023334451019763947 Validation loss 0.0435965433716774 Accuracy 0.8840000629425049\n",
      "Iteration 25580 Training loss 0.017789864912629128 Validation loss 0.0425577349960804 Accuracy 0.8873750567436218\n",
      "Iteration 25590 Training loss 0.037552736699581146 Validation loss 0.05155716836452484 Accuracy 0.8607500195503235\n",
      "Iteration 25600 Training loss 0.03405516594648361 Validation loss 0.049634817987680435 Accuracy 0.8676250576972961\n",
      "Iteration 25610 Training loss 0.018974410369992256 Validation loss 0.04397205263376236 Accuracy 0.8841250538825989\n",
      "Iteration 25620 Training loss 0.021938344463706017 Validation loss 0.04427173733711243 Accuracy 0.8805000185966492\n",
      "Iteration 25630 Training loss 0.021960746496915817 Validation loss 0.04349241033196449 Accuracy 0.8850000500679016\n",
      "Iteration 25640 Training loss 0.020246991887688637 Validation loss 0.04250489920377731 Accuracy 0.8878750205039978\n",
      "Iteration 25650 Training loss 0.03340641409158707 Validation loss 0.051950786262750626 Accuracy 0.8595000505447388\n",
      "Iteration 25660 Training loss 0.027019774541258812 Validation loss 0.043598707765340805 Accuracy 0.8832500576972961\n",
      "Iteration 25670 Training loss 0.021327797323465347 Validation loss 0.042702604085206985 Accuracy 0.8866250514984131\n",
      "Iteration 25680 Training loss 0.021308068186044693 Validation loss 0.043166421353816986 Accuracy 0.8847500681877136\n",
      "Iteration 25690 Training loss 0.01537566352635622 Validation loss 0.043233659118413925 Accuracy 0.8857500553131104\n",
      "Iteration 25700 Training loss 0.023026851937174797 Validation loss 0.04734324291348457 Accuracy 0.8727500438690186\n",
      "Iteration 25710 Training loss 0.023417210206389427 Validation loss 0.04359330236911774 Accuracy 0.8843750357627869\n",
      "Iteration 25720 Training loss 0.02523880824446678 Validation loss 0.04402708634734154 Accuracy 0.8831250667572021\n",
      "Iteration 25730 Training loss 0.017629485577344894 Validation loss 0.04280896112322807 Accuracy 0.8865000605583191\n",
      "Iteration 25740 Training loss 0.024056386202573776 Validation loss 0.0430963933467865 Accuracy 0.8857500553131104\n",
      "Iteration 25750 Training loss 0.018658800050616264 Validation loss 0.0431169793009758 Accuracy 0.8862500190734863\n",
      "Iteration 25760 Training loss 0.028158215805888176 Validation loss 0.04454141482710838 Accuracy 0.8801250457763672\n",
      "Iteration 25770 Training loss 0.020938578993082047 Validation loss 0.04318876564502716 Accuracy 0.8870000243186951\n",
      "Iteration 25780 Training loss 0.028068987652659416 Validation loss 0.045717477798461914 Accuracy 0.877500057220459\n",
      "Iteration 25790 Training loss 0.024994267150759697 Validation loss 0.04315010458230972 Accuracy 0.8861250281333923\n",
      "Iteration 25800 Training loss 0.03337845578789711 Validation loss 0.04874176159501076 Accuracy 0.8692500591278076\n",
      "Iteration 25810 Training loss 0.02243622951209545 Validation loss 0.04583064839243889 Accuracy 0.878000020980835\n",
      "Iteration 25820 Training loss 0.022605981677770615 Validation loss 0.04299413785338402 Accuracy 0.8855000138282776\n",
      "Iteration 25830 Training loss 0.022709760814905167 Validation loss 0.043188631534576416 Accuracy 0.8837500214576721\n",
      "Iteration 25840 Training loss 0.04319231957197189 Validation loss 0.05967579036951065 Accuracy 0.8411250114440918\n",
      "Iteration 25850 Training loss 0.02804444171488285 Validation loss 0.046232759952545166 Accuracy 0.8761250376701355\n",
      "Iteration 25860 Training loss 0.024803370237350464 Validation loss 0.04863305762410164 Accuracy 0.8688750267028809\n",
      "Iteration 25870 Training loss 0.03161608800292015 Validation loss 0.05205149203538895 Accuracy 0.8616250157356262\n",
      "Iteration 25880 Training loss 0.019402947276830673 Validation loss 0.04286157339811325 Accuracy 0.8847500681877136\n",
      "Iteration 25890 Training loss 0.017746714875102043 Validation loss 0.04333451762795448 Accuracy 0.8836250305175781\n",
      "Iteration 25900 Training loss 0.028067436069250107 Validation loss 0.04736998304724693 Accuracy 0.8730000257492065\n",
      "Iteration 25910 Training loss 0.023658355697989464 Validation loss 0.045054566115140915 Accuracy 0.8790000677108765\n",
      "Iteration 25920 Training loss 0.03556395322084427 Validation loss 0.046115290373563766 Accuracy 0.8770000338554382\n",
      "Iteration 25930 Training loss 0.02616087906062603 Validation loss 0.04267260804772377 Accuracy 0.8858750462532043\n",
      "Iteration 25940 Training loss 0.0309084914624691 Validation loss 0.04694094508886337 Accuracy 0.8727500438690186\n",
      "Iteration 25950 Training loss 0.04680551588535309 Validation loss 0.06113709509372711 Accuracy 0.8372500538825989\n",
      "Iteration 25960 Training loss 0.021112065762281418 Validation loss 0.04345989227294922 Accuracy 0.8836250305175781\n",
      "Iteration 25970 Training loss 0.025494780391454697 Validation loss 0.043246202170848846 Accuracy 0.8851250410079956\n",
      "Iteration 25980 Training loss 0.02672654576599598 Validation loss 0.044367142021656036 Accuracy 0.8818750381469727\n",
      "Iteration 25990 Training loss 0.02897634170949459 Validation loss 0.043799180537462234 Accuracy 0.8833750486373901\n",
      "Iteration 26000 Training loss 0.024872049689292908 Validation loss 0.04262812063097954 Accuracy 0.8878750205039978\n",
      "Iteration 26010 Training loss 0.02405882254242897 Validation loss 0.0462479405105114 Accuracy 0.8755000233650208\n",
      "Iteration 26020 Training loss 0.018917640671133995 Validation loss 0.043505601584911346 Accuracy 0.8840000629425049\n",
      "Iteration 26030 Training loss 0.01652616076171398 Validation loss 0.042965278029441833 Accuracy 0.8861250281333923\n",
      "Iteration 26040 Training loss 0.023256812244653702 Validation loss 0.042720384895801544 Accuracy 0.8856250643730164\n",
      "Iteration 26050 Training loss 0.020363926887512207 Validation loss 0.042730800807476044 Accuracy 0.8855000138282776\n",
      "Iteration 26060 Training loss 0.05069117248058319 Validation loss 0.06443552672863007 Accuracy 0.8297500610351562\n",
      "Iteration 26070 Training loss 0.022782517597079277 Validation loss 0.04964202269911766 Accuracy 0.8678750395774841\n",
      "Iteration 26080 Training loss 0.020091308280825615 Validation loss 0.044038064777851105 Accuracy 0.8826250433921814\n",
      "Iteration 26090 Training loss 0.02327141724526882 Validation loss 0.04612591862678528 Accuracy 0.8772500157356262\n",
      "Iteration 26100 Training loss 0.023711008951067924 Validation loss 0.04611836373806 Accuracy 0.8767500519752502\n",
      "Iteration 26110 Training loss 0.02776285633444786 Validation loss 0.04389404505491257 Accuracy 0.8812500238418579\n",
      "Iteration 26120 Training loss 0.03472083806991577 Validation loss 0.050991885364055634 Accuracy 0.8645000457763672\n",
      "Iteration 26130 Training loss 0.018985582515597343 Validation loss 0.04287842661142349 Accuracy 0.8851250410079956\n",
      "Iteration 26140 Training loss 0.04065372049808502 Validation loss 0.05705241486430168 Accuracy 0.8502500653266907\n",
      "Iteration 26150 Training loss 0.020349282771348953 Validation loss 0.04329143464565277 Accuracy 0.8850000500679016\n",
      "Iteration 26160 Training loss 0.023380151018500328 Validation loss 0.04344233497977257 Accuracy 0.8850000500679016\n",
      "Iteration 26170 Training loss 0.032956261187791824 Validation loss 0.042816340923309326 Accuracy 0.8862500190734863\n",
      "Iteration 26180 Training loss 0.01576278731226921 Validation loss 0.04446464031934738 Accuracy 0.8810000419616699\n",
      "Iteration 26190 Training loss 0.04509458318352699 Validation loss 0.057143811136484146 Accuracy 0.8498750329017639\n",
      "Iteration 26200 Training loss 0.015705298632383347 Validation loss 0.04318150505423546 Accuracy 0.8843750357627869\n",
      "Iteration 26210 Training loss 0.01988080143928528 Validation loss 0.04625207558274269 Accuracy 0.8768750429153442\n",
      "Iteration 26220 Training loss 0.02068360336124897 Validation loss 0.043403442949056625 Accuracy 0.8841250538825989\n",
      "Iteration 26230 Training loss 0.018299220129847527 Validation loss 0.04284750297665596 Accuracy 0.8856250643730164\n",
      "Iteration 26240 Training loss 0.028068723157048225 Validation loss 0.04582793638110161 Accuracy 0.877625048160553\n",
      "Iteration 26250 Training loss 0.021714994683861732 Validation loss 0.04325654357671738 Accuracy 0.8852500319480896\n",
      "Iteration 26260 Training loss 0.041029851883649826 Validation loss 0.05882580578327179 Accuracy 0.8420000672340393\n",
      "Iteration 26270 Training loss 0.027820629999041557 Validation loss 0.04293111711740494 Accuracy 0.8856250643730164\n",
      "Iteration 26280 Training loss 0.033368904143571854 Validation loss 0.05052385851740837 Accuracy 0.8647500276565552\n",
      "Iteration 26290 Training loss 0.026520008221268654 Validation loss 0.04499144107103348 Accuracy 0.8790000677108765\n",
      "Iteration 26300 Training loss 0.026296257972717285 Validation loss 0.049166277050971985 Accuracy 0.8692500591278076\n",
      "Iteration 26310 Training loss 0.019289961084723473 Validation loss 0.04364572837948799 Accuracy 0.8847500681877136\n",
      "Iteration 26320 Training loss 0.023815345019102097 Validation loss 0.04359591007232666 Accuracy 0.8840000629425049\n",
      "Iteration 26330 Training loss 0.023334307596087456 Validation loss 0.0434609018266201 Accuracy 0.8848750591278076\n",
      "Iteration 26340 Training loss 0.03128950670361519 Validation loss 0.052017759531736374 Accuracy 0.8612500429153442\n",
      "Iteration 26350 Training loss 0.02993391454219818 Validation loss 0.04659242182970047 Accuracy 0.8748750686645508\n",
      "Iteration 26360 Training loss 0.019723879173398018 Validation loss 0.044021982699632645 Accuracy 0.8841250538825989\n",
      "Iteration 26370 Training loss 0.023578356951475143 Validation loss 0.043359801173210144 Accuracy 0.8853750228881836\n",
      "Iteration 26380 Training loss 0.043456725776195526 Validation loss 0.056640803813934326 Accuracy 0.8515000343322754\n",
      "Iteration 26390 Training loss 0.030698804184794426 Validation loss 0.049873627722263336 Accuracy 0.8670000433921814\n",
      "Iteration 26400 Training loss 0.027936844155192375 Validation loss 0.042909856885671616 Accuracy 0.8852500319480896\n",
      "Iteration 26410 Training loss 0.025767583400011063 Validation loss 0.045749928802251816 Accuracy 0.8782500624656677\n",
      "Iteration 26420 Training loss 0.02429155446588993 Validation loss 0.04301183670759201 Accuracy 0.8847500681877136\n",
      "Iteration 26430 Training loss 0.04524483531713486 Validation loss 0.05658971145749092 Accuracy 0.8516250252723694\n",
      "Iteration 26440 Training loss 0.01943204551935196 Validation loss 0.04364101588726044 Accuracy 0.8852500319480896\n",
      "Iteration 26450 Training loss 0.019633810967206955 Validation loss 0.046776268631219864 Accuracy 0.8750000596046448\n",
      "Iteration 26460 Training loss 0.024869663640856743 Validation loss 0.04313359782099724 Accuracy 0.8861250281333923\n",
      "Iteration 26470 Training loss 0.033072348684072495 Validation loss 0.05056708678603172 Accuracy 0.8651250600814819\n",
      "Iteration 26480 Training loss 0.015605108812451363 Validation loss 0.042478449642658234 Accuracy 0.8867500424385071\n",
      "Iteration 26490 Training loss 0.029688533395528793 Validation loss 0.04480358213186264 Accuracy 0.8807500600814819\n",
      "Iteration 26500 Training loss 0.02532460168004036 Validation loss 0.05254068970680237 Accuracy 0.8611250519752502\n",
      "Iteration 26510 Training loss 0.018881281837821007 Validation loss 0.04297040030360222 Accuracy 0.8860000371932983\n",
      "Iteration 26520 Training loss 0.03352971747517586 Validation loss 0.05458575114607811 Accuracy 0.8556250333786011\n",
      "Iteration 26530 Training loss 0.0342121496796608 Validation loss 0.04327007383108139 Accuracy 0.8861250281333923\n",
      "Iteration 26540 Training loss 0.025759024545550346 Validation loss 0.049209654331207275 Accuracy 0.8691250681877136\n",
      "Iteration 26550 Training loss 0.021365398541092873 Validation loss 0.04591507837176323 Accuracy 0.877375066280365\n",
      "Iteration 26560 Training loss 0.022528452798724174 Validation loss 0.04351283237338066 Accuracy 0.8840000629425049\n",
      "Iteration 26570 Training loss 0.03032524511218071 Validation loss 0.052882224321365356 Accuracy 0.8571250438690186\n",
      "Iteration 26580 Training loss 0.022478489205241203 Validation loss 0.043257251381874084 Accuracy 0.8861250281333923\n",
      "Iteration 26590 Training loss 0.018264081329107285 Validation loss 0.04255411773920059 Accuracy 0.8875000476837158\n",
      "Iteration 26600 Training loss 0.02160194143652916 Validation loss 0.04558802396059036 Accuracy 0.8791250586509705\n",
      "Iteration 26610 Training loss 0.02198588103055954 Validation loss 0.047703616321086884 Accuracy 0.874125063419342\n",
      "Iteration 26620 Training loss 0.013543269596993923 Validation loss 0.04308600351214409 Accuracy 0.8846250176429749\n",
      "Iteration 26630 Training loss 0.03971301391720772 Validation loss 0.05085432529449463 Accuracy 0.8642500638961792\n",
      "Iteration 26640 Training loss 0.02532859519124031 Validation loss 0.04346605762839317 Accuracy 0.8857500553131104\n",
      "Iteration 26650 Training loss 0.028493361547589302 Validation loss 0.05287665128707886 Accuracy 0.8593750596046448\n",
      "Iteration 26660 Training loss 0.0305235143750906 Validation loss 0.04749898612499237 Accuracy 0.8731250166893005\n",
      "Iteration 26670 Training loss 0.021589871495962143 Validation loss 0.04461950063705444 Accuracy 0.8806250691413879\n",
      "Iteration 26680 Training loss 0.019433211535215378 Validation loss 0.046402186155319214 Accuracy 0.8760000467300415\n",
      "Iteration 26690 Training loss 0.022289881482720375 Validation loss 0.04576139897108078 Accuracy 0.8768750429153442\n",
      "Iteration 26700 Training loss 0.030769648030400276 Validation loss 0.04694372043013573 Accuracy 0.8768750429153442\n",
      "Iteration 26710 Training loss 0.02595231495797634 Validation loss 0.04892122372984886 Accuracy 0.8701250553131104\n",
      "Iteration 26720 Training loss 0.041427601128816605 Validation loss 0.05939794331789017 Accuracy 0.8417500257492065\n",
      "Iteration 26730 Training loss 0.016333669424057007 Validation loss 0.042696431279182434 Accuracy 0.8862500190734863\n",
      "Iteration 26740 Training loss 0.017224997282028198 Validation loss 0.04284321144223213 Accuracy 0.8860000371932983\n",
      "Iteration 26750 Training loss 0.028180796653032303 Validation loss 0.04578911140561104 Accuracy 0.877375066280365\n",
      "Iteration 26760 Training loss 0.020073827356100082 Validation loss 0.044790931046009064 Accuracy 0.8807500600814819\n",
      "Iteration 26770 Training loss 0.02040347456932068 Validation loss 0.04566330090165138 Accuracy 0.8790000677108765\n",
      "Iteration 26780 Training loss 0.019804812967777252 Validation loss 0.04483197256922722 Accuracy 0.8793750405311584\n",
      "Iteration 26790 Training loss 0.03910662978887558 Validation loss 0.05570080131292343 Accuracy 0.8493750691413879\n",
      "Iteration 26800 Training loss 0.02534743770956993 Validation loss 0.043203286826610565 Accuracy 0.8848750591278076\n",
      "Iteration 26810 Training loss 0.01981808803975582 Validation loss 0.04263053834438324 Accuracy 0.8862500190734863\n",
      "Iteration 26820 Training loss 0.024622254073619843 Validation loss 0.04601571708917618 Accuracy 0.877625048160553\n",
      "Iteration 26830 Training loss 0.02896650694310665 Validation loss 0.043944984674453735 Accuracy 0.8850000500679016\n",
      "Iteration 26840 Training loss 0.01926763914525509 Validation loss 0.04300926998257637 Accuracy 0.8850000500679016\n",
      "Iteration 26850 Training loss 0.021027134731411934 Validation loss 0.04599078372120857 Accuracy 0.8766250610351562\n",
      "Iteration 26860 Training loss 0.040944430977106094 Validation loss 0.05240853875875473 Accuracy 0.862500011920929\n",
      "Iteration 26870 Training loss 0.03315945714712143 Validation loss 0.050196241587400436 Accuracy 0.8652500510215759\n",
      "Iteration 26880 Training loss 0.024813491851091385 Validation loss 0.042964886873960495 Accuracy 0.8860000371932983\n",
      "Iteration 26890 Training loss 0.021178731694817543 Validation loss 0.042893026024103165 Accuracy 0.8852500319480896\n",
      "Iteration 26900 Training loss 0.021946193650364876 Validation loss 0.045469172298908234 Accuracy 0.878125011920929\n",
      "Iteration 26910 Training loss 0.0239227507263422 Validation loss 0.04292385280132294 Accuracy 0.8852500319480896\n",
      "Iteration 26920 Training loss 0.017875641584396362 Validation loss 0.04276832193136215 Accuracy 0.8876250386238098\n",
      "Iteration 26930 Training loss 0.03591765835881233 Validation loss 0.05404433235526085 Accuracy 0.8566250205039978\n",
      "Iteration 26940 Training loss 0.024935182183980942 Validation loss 0.04518994316458702 Accuracy 0.8807500600814819\n",
      "Iteration 26950 Training loss 0.020467396825551987 Validation loss 0.04343584552407265 Accuracy 0.8857500553131104\n",
      "Iteration 26960 Training loss 0.028028586879372597 Validation loss 0.04787040129303932 Accuracy 0.8716250658035278\n",
      "Iteration 26970 Training loss 0.033821381628513336 Validation loss 0.04901685565710068 Accuracy 0.8697500228881836\n",
      "Iteration 26980 Training loss 0.03667547553777695 Validation loss 0.05160794034600258 Accuracy 0.8635000586509705\n",
      "Iteration 26990 Training loss 0.03135300055146217 Validation loss 0.050276435911655426 Accuracy 0.8673750162124634\n",
      "Iteration 27000 Training loss 0.01586388237774372 Validation loss 0.04278508946299553 Accuracy 0.8860000371932983\n",
      "Iteration 27010 Training loss 0.026444310322403908 Validation loss 0.050577882677316666 Accuracy 0.8653750419616699\n",
      "Iteration 27020 Training loss 0.024539807811379433 Validation loss 0.04303901642560959 Accuracy 0.8856250643730164\n",
      "Iteration 27030 Training loss 0.022251419723033905 Validation loss 0.0467376671731472 Accuracy 0.8761250376701355\n",
      "Iteration 27040 Training loss 0.040996503084897995 Validation loss 0.06312056630849838 Accuracy 0.8335000276565552\n",
      "Iteration 27050 Training loss 0.01247200183570385 Validation loss 0.042785510420799255 Accuracy 0.8845000267028809\n",
      "Iteration 27060 Training loss 0.018386483192443848 Validation loss 0.04244822636246681 Accuracy 0.8865000605583191\n",
      "Iteration 27070 Training loss 0.016615109518170357 Validation loss 0.04470556229352951 Accuracy 0.8808750510215759\n",
      "Iteration 27080 Training loss 0.03315655142068863 Validation loss 0.055115602910518646 Accuracy 0.8522500395774841\n",
      "Iteration 27090 Training loss 0.04854045435786247 Validation loss 0.06326849013566971 Accuracy 0.8360000252723694\n",
      "Iteration 27100 Training loss 0.020907729864120483 Validation loss 0.044402267783880234 Accuracy 0.8808750510215759\n",
      "Iteration 27110 Training loss 0.020793037489056587 Validation loss 0.04275836795568466 Accuracy 0.8876250386238098\n",
      "Iteration 27120 Training loss 0.018203942105174065 Validation loss 0.042583268135786057 Accuracy 0.8873750567436218\n",
      "Iteration 27130 Training loss 0.03394049406051636 Validation loss 0.044803667813539505 Accuracy 0.8792500495910645\n",
      "Iteration 27140 Training loss 0.032489512115716934 Validation loss 0.050718579441308975 Accuracy 0.8650000691413879\n",
      "Iteration 27150 Training loss 0.028532445430755615 Validation loss 0.044467534869909286 Accuracy 0.8822500705718994\n",
      "Iteration 27160 Training loss 0.017258677631616592 Validation loss 0.0441802479326725 Accuracy 0.8848750591278076\n",
      "Iteration 27170 Training loss 0.016941452398896217 Validation loss 0.04266810417175293 Accuracy 0.8873750567436218\n",
      "Iteration 27180 Training loss 0.024972494691610336 Validation loss 0.04296911880373955 Accuracy 0.8856250643730164\n",
      "Iteration 27190 Training loss 0.019413959234952927 Validation loss 0.043480034917593 Accuracy 0.8848750591278076\n",
      "Iteration 27200 Training loss 0.024761775508522987 Validation loss 0.04455562308430672 Accuracy 0.8811250329017639\n",
      "Iteration 27210 Training loss 0.02096785232424736 Validation loss 0.045273005962371826 Accuracy 0.8782500624656677\n",
      "Iteration 27220 Training loss 0.02223094180226326 Validation loss 0.04344824701547623 Accuracy 0.8850000500679016\n",
      "Iteration 27230 Training loss 0.0242963545024395 Validation loss 0.04274908825755119 Accuracy 0.8862500190734863\n",
      "Iteration 27240 Training loss 0.028743984177708626 Validation loss 0.04678460955619812 Accuracy 0.874750018119812\n",
      "Iteration 27250 Training loss 0.021661851555109024 Validation loss 0.043452873826026917 Accuracy 0.8846250176429749\n",
      "Iteration 27260 Training loss 0.027580389752984047 Validation loss 0.04797159135341644 Accuracy 0.8703750371932983\n",
      "Iteration 27270 Training loss 0.03098028525710106 Validation loss 0.052921898663043976 Accuracy 0.858625054359436\n",
      "Iteration 27280 Training loss 0.02258911542594433 Validation loss 0.04401613026857376 Accuracy 0.8836250305175781\n",
      "Iteration 27290 Training loss 0.024258989840745926 Validation loss 0.044495441019535065 Accuracy 0.8826250433921814\n",
      "Iteration 27300 Training loss 0.03292113542556763 Validation loss 0.051267530769109726 Accuracy 0.862250030040741\n",
      "Iteration 27310 Training loss 0.024601589888334274 Validation loss 0.04275454208254814 Accuracy 0.8873750567436218\n",
      "Iteration 27320 Training loss 0.021721886470913887 Validation loss 0.042898859828710556 Accuracy 0.8872500658035278\n",
      "Iteration 27330 Training loss 0.029199115931987762 Validation loss 0.05273462459445 Accuracy 0.8577500581741333\n",
      "Iteration 27340 Training loss 0.026361331343650818 Validation loss 0.04365120828151703 Accuracy 0.8831250667572021\n",
      "Iteration 27350 Training loss 0.017064720392227173 Validation loss 0.04278146103024483 Accuracy 0.8866250514984131\n",
      "Iteration 27360 Training loss 0.03205224499106407 Validation loss 0.05269719287753105 Accuracy 0.859125018119812\n",
      "Iteration 27370 Training loss 0.019454633817076683 Validation loss 0.04281900078058243 Accuracy 0.8853750228881836\n",
      "Iteration 27380 Training loss 0.017689814791083336 Validation loss 0.04277006536722183 Accuracy 0.8867500424385071\n",
      "Iteration 27390 Training loss 0.03354135528206825 Validation loss 0.05263786017894745 Accuracy 0.8571250438690186\n",
      "Iteration 27400 Training loss 0.02106429822742939 Validation loss 0.0428190641105175 Accuracy 0.8862500190734863\n",
      "Iteration 27410 Training loss 0.026437921449542046 Validation loss 0.044413331896066666 Accuracy 0.8816250562667847\n",
      "Iteration 27420 Training loss 0.025639427825808525 Validation loss 0.04757903516292572 Accuracy 0.8732500672340393\n",
      "Iteration 27430 Training loss 0.026834189891815186 Validation loss 0.04393286257982254 Accuracy 0.8815000653266907\n",
      "Iteration 27440 Training loss 0.021657995879650116 Validation loss 0.04254542291164398 Accuracy 0.8887500166893005\n",
      "Iteration 27450 Training loss 0.018538039177656174 Validation loss 0.04252827540040016 Accuracy 0.8878750205039978\n",
      "Iteration 27460 Training loss 0.016348447650671005 Validation loss 0.04235880449414253 Accuracy 0.8887500166893005\n",
      "Iteration 27470 Training loss 0.032282669097185135 Validation loss 0.046977460384368896 Accuracy 0.8752500414848328\n",
      "Iteration 27480 Training loss 0.024308757856488228 Validation loss 0.04434441030025482 Accuracy 0.8815000653266907\n",
      "Iteration 27490 Training loss 0.023535296320915222 Validation loss 0.04790740832686424 Accuracy 0.8726250529289246\n",
      "Iteration 27500 Training loss 0.03109883703291416 Validation loss 0.04868611693382263 Accuracy 0.8681250214576721\n",
      "Iteration 27510 Training loss 0.01921907812356949 Validation loss 0.044809069484472275 Accuracy 0.8806250691413879\n",
      "Iteration 27520 Training loss 0.04458924010396004 Validation loss 0.052166737616062164 Accuracy 0.8615000247955322\n",
      "Iteration 27530 Training loss 0.015211007557809353 Validation loss 0.04280565306544304 Accuracy 0.8875000476837158\n",
      "Iteration 27540 Training loss 0.022845642641186714 Validation loss 0.045971330255270004 Accuracy 0.877500057220459\n",
      "Iteration 27550 Training loss 0.019707759842276573 Validation loss 0.04588799923658371 Accuracy 0.878125011920929\n",
      "Iteration 27560 Training loss 0.022358281537890434 Validation loss 0.0425226092338562 Accuracy 0.8880000710487366\n",
      "Iteration 27570 Training loss 0.022049035876989365 Validation loss 0.04652222618460655 Accuracy 0.8760000467300415\n",
      "Iteration 27580 Training loss 0.03505951166152954 Validation loss 0.05372446030378342 Accuracy 0.8570000529289246\n",
      "Iteration 27590 Training loss 0.02166973054409027 Validation loss 0.04273252189159393 Accuracy 0.8880000710487366\n",
      "Iteration 27600 Training loss 0.01588786393404007 Validation loss 0.0422668494284153 Accuracy 0.8885000348091125\n",
      "Iteration 27610 Training loss 0.027873696759343147 Validation loss 0.044982463121414185 Accuracy 0.8820000290870667\n",
      "Iteration 27620 Training loss 0.019225014373660088 Validation loss 0.04294532537460327 Accuracy 0.8858750462532043\n",
      "Iteration 27630 Training loss 0.018009942024946213 Validation loss 0.04242309555411339 Accuracy 0.8875000476837158\n",
      "Iteration 27640 Training loss 0.0172833651304245 Validation loss 0.042369745671749115 Accuracy 0.8883750438690186\n",
      "Iteration 27650 Training loss 0.039813362061977386 Validation loss 0.05341332405805588 Accuracy 0.8565000295639038\n",
      "Iteration 27660 Training loss 0.022979754954576492 Validation loss 0.042828064411878586 Accuracy 0.8880000710487366\n",
      "Iteration 27670 Training loss 0.03589735925197601 Validation loss 0.05185086652636528 Accuracy 0.8602500557899475\n",
      "Iteration 27680 Training loss 0.025946805253624916 Validation loss 0.04561644047498703 Accuracy 0.8787500262260437\n",
      "Iteration 27690 Training loss 0.018492039293050766 Validation loss 0.044773608446121216 Accuracy 0.8800000548362732\n",
      "Iteration 27700 Training loss 0.025885777547955513 Validation loss 0.04276091977953911 Accuracy 0.8878750205039978\n",
      "Iteration 27710 Training loss 0.023152507841587067 Validation loss 0.04992281645536423 Accuracy 0.8676250576972961\n",
      "Iteration 27720 Training loss 0.021002506837248802 Validation loss 0.043312132358551025 Accuracy 0.8850000500679016\n",
      "Iteration 27730 Training loss 0.01788802444934845 Validation loss 0.04378579184412956 Accuracy 0.8827500343322754\n",
      "Iteration 27740 Training loss 0.023848358541727066 Validation loss 0.04761534929275513 Accuracy 0.8716250658035278\n",
      "Iteration 27750 Training loss 0.04084049165248871 Validation loss 0.0618269182741642 Accuracy 0.8382500410079956\n",
      "Iteration 27760 Training loss 0.020473388954997063 Validation loss 0.046172838658094406 Accuracy 0.8761250376701355\n",
      "Iteration 27770 Training loss 0.022517094388604164 Validation loss 0.04378041997551918 Accuracy 0.8831250667572021\n",
      "Iteration 27780 Training loss 0.032637495547533035 Validation loss 0.04472411051392555 Accuracy 0.8803750276565552\n",
      "Iteration 27790 Training loss 0.01773163303732872 Validation loss 0.043762121349573135 Accuracy 0.8832500576972961\n",
      "Iteration 27800 Training loss 0.021021489053964615 Validation loss 0.04358097165822983 Accuracy 0.8838750123977661\n",
      "Iteration 27810 Training loss 0.018404001370072365 Validation loss 0.04393111914396286 Accuracy 0.8838750123977661\n",
      "Iteration 27820 Training loss 0.028702611103653908 Validation loss 0.04443882778286934 Accuracy 0.8816250562667847\n",
      "Iteration 27830 Training loss 0.021708853542804718 Validation loss 0.04550324007868767 Accuracy 0.8798750638961792\n",
      "Iteration 27840 Training loss 0.021056469529867172 Validation loss 0.042852699756622314 Accuracy 0.8866250514984131\n",
      "Iteration 27850 Training loss 0.05378551036119461 Validation loss 0.06757481396198273 Accuracy 0.8227500319480896\n",
      "Iteration 27860 Training loss 0.02612186409533024 Validation loss 0.042806901037693024 Accuracy 0.8862500190734863\n",
      "Iteration 27870 Training loss 0.030790481716394424 Validation loss 0.04355107992887497 Accuracy 0.8828750252723694\n",
      "Iteration 27880 Training loss 0.06131386756896973 Validation loss 0.07907209545373917 Accuracy 0.7943750619888306\n",
      "Iteration 27890 Training loss 0.0286838598549366 Validation loss 0.050249382853507996 Accuracy 0.8673750162124634\n",
      "Iteration 27900 Training loss 0.014766358770430088 Validation loss 0.04316241666674614 Accuracy 0.8831250667572021\n",
      "Iteration 27910 Training loss 0.019098592922091484 Validation loss 0.04310750961303711 Accuracy 0.8867500424385071\n",
      "Iteration 27920 Training loss 0.046855125576257706 Validation loss 0.056554701179265976 Accuracy 0.8471250534057617\n",
      "Iteration 27930 Training loss 0.014846314676105976 Validation loss 0.043073639273643494 Accuracy 0.8848750591278076\n",
      "Iteration 27940 Training loss 0.019798381254076958 Validation loss 0.04422834888100624 Accuracy 0.8818750381469727\n",
      "Iteration 27950 Training loss 0.01812603510916233 Validation loss 0.04382302239537239 Accuracy 0.8848750591278076\n",
      "Iteration 27960 Training loss 0.017453519627451897 Validation loss 0.04272107779979706 Accuracy 0.8876250386238098\n",
      "Iteration 27970 Training loss 0.0184625331312418 Validation loss 0.042630910873413086 Accuracy 0.8867500424385071\n",
      "Iteration 27980 Training loss 0.03535819798707962 Validation loss 0.056043338030576706 Accuracy 0.8528750538825989\n",
      "Iteration 27990 Training loss 0.027228444814682007 Validation loss 0.05216852203011513 Accuracy 0.8597500324249268\n",
      "Iteration 28000 Training loss 0.01784982904791832 Validation loss 0.04448029026389122 Accuracy 0.8825000524520874\n",
      "Iteration 28010 Training loss 0.020539438351988792 Validation loss 0.04601985216140747 Accuracy 0.877500057220459\n",
      "Iteration 28020 Training loss 0.019571945071220398 Validation loss 0.04848343878984451 Accuracy 0.8733750581741333\n",
      "Iteration 28030 Training loss 0.017570698633790016 Validation loss 0.04321296513080597 Accuracy 0.8868750333786011\n",
      "Iteration 28040 Training loss 0.02453226037323475 Validation loss 0.04369716718792915 Accuracy 0.8853750228881836\n",
      "Iteration 28050 Training loss 0.024809984490275383 Validation loss 0.04518045112490654 Accuracy 0.8803750276565552\n",
      "Iteration 28060 Training loss 0.026255091652274132 Validation loss 0.04464517533779144 Accuracy 0.8816250562667847\n",
      "Iteration 28070 Training loss 0.023138368502259254 Validation loss 0.04229570925235748 Accuracy 0.890250027179718\n",
      "Iteration 28080 Training loss 0.018578678369522095 Validation loss 0.04350460320711136 Accuracy 0.8851250410079956\n",
      "Iteration 28090 Training loss 0.0193574670702219 Validation loss 0.04307113215327263 Accuracy 0.8865000605583191\n",
      "Iteration 28100 Training loss 0.020129617303609848 Validation loss 0.04552213475108147 Accuracy 0.8801250457763672\n",
      "Iteration 28110 Training loss 0.03473696485161781 Validation loss 0.04674278572201729 Accuracy 0.8750000596046448\n",
      "Iteration 28120 Training loss 0.03793775290250778 Validation loss 0.05535005405545235 Accuracy 0.8537500500679016\n",
      "Iteration 28130 Training loss 0.02623457834124565 Validation loss 0.04310309886932373 Accuracy 0.8856250643730164\n",
      "Iteration 28140 Training loss 0.021905262023210526 Validation loss 0.04641348496079445 Accuracy 0.8748750686645508\n",
      "Iteration 28150 Training loss 0.020597290247678757 Validation loss 0.043712206184864044 Accuracy 0.8857500553131104\n",
      "Iteration 28160 Training loss 0.01652563363313675 Validation loss 0.04375437647104263 Accuracy 0.8837500214576721\n",
      "Iteration 28170 Training loss 0.02744048833847046 Validation loss 0.04250655695796013 Accuracy 0.8885000348091125\n",
      "Iteration 28180 Training loss 0.022981146350502968 Validation loss 0.04677996411919594 Accuracy 0.8751250505447388\n",
      "Iteration 28190 Training loss 0.023341702297329903 Validation loss 0.04267481341958046 Accuracy 0.8860000371932983\n",
      "Iteration 28200 Training loss 0.020779931917786598 Validation loss 0.045729633420705795 Accuracy 0.878125011920929\n",
      "Iteration 28210 Training loss 0.020681682974100113 Validation loss 0.04244932904839516 Accuracy 0.8860000371932983\n",
      "Iteration 28220 Training loss 0.039537642151117325 Validation loss 0.06391509622335434 Accuracy 0.8325000405311584\n",
      "Iteration 28230 Training loss 0.023889243602752686 Validation loss 0.04446633905172348 Accuracy 0.8830000162124634\n",
      "Iteration 28240 Training loss 0.030283324420452118 Validation loss 0.04563168063759804 Accuracy 0.8786250352859497\n",
      "Iteration 28250 Training loss 0.019183918833732605 Validation loss 0.04551266133785248 Accuracy 0.877750039100647\n",
      "Iteration 28260 Training loss 0.017185742035508156 Validation loss 0.042716480791568756 Accuracy 0.8865000605583191\n",
      "Iteration 28270 Training loss 0.019482893869280815 Validation loss 0.04248040169477463 Accuracy 0.8893750309944153\n",
      "Iteration 28280 Training loss 0.013994824141263962 Validation loss 0.042621757835149765 Accuracy 0.8877500295639038\n",
      "Iteration 28290 Training loss 0.02082217112183571 Validation loss 0.04797105863690376 Accuracy 0.8723750710487366\n",
      "Iteration 28300 Training loss 0.04342447966337204 Validation loss 0.0598827563226223 Accuracy 0.8415000438690186\n",
      "Iteration 28310 Training loss 0.026784084737300873 Validation loss 0.04253455996513367 Accuracy 0.8873750567436218\n",
      "Iteration 28320 Training loss 0.017194874584674835 Validation loss 0.044554997235536575 Accuracy 0.8811250329017639\n",
      "Iteration 28330 Training loss 0.02895279973745346 Validation loss 0.047962725162506104 Accuracy 0.8718750476837158\n",
      "Iteration 28340 Training loss 0.02338610403239727 Validation loss 0.049480684101581573 Accuracy 0.8705000281333923\n",
      "Iteration 28350 Training loss 0.018351983278989792 Validation loss 0.04332295432686806 Accuracy 0.8858750462532043\n",
      "Iteration 28360 Training loss 0.025969073176383972 Validation loss 0.044319480657577515 Accuracy 0.8835000395774841\n",
      "Iteration 28370 Training loss 0.03756013885140419 Validation loss 0.048939600586891174 Accuracy 0.8715000152587891\n",
      "Iteration 28380 Training loss 0.025299973785877228 Validation loss 0.04252825304865837 Accuracy 0.8876250386238098\n",
      "Iteration 28390 Training loss 0.02473301813006401 Validation loss 0.044385168701410294 Accuracy 0.8827500343322754\n",
      "Iteration 28400 Training loss 0.02511673793196678 Validation loss 0.048600275069475174 Accuracy 0.8698750138282776\n",
      "Iteration 28410 Training loss 0.041138630360364914 Validation loss 0.0651082694530487 Accuracy 0.830500066280365\n",
      "Iteration 28420 Training loss 0.015925444662570953 Validation loss 0.04326066002249718 Accuracy 0.8862500190734863\n",
      "Iteration 28430 Training loss 0.020819667726755142 Validation loss 0.04418112337589264 Accuracy 0.8817500472068787\n",
      "Iteration 28440 Training loss 0.016115309670567513 Validation loss 0.04281662777066231 Accuracy 0.8868750333786011\n",
      "Iteration 28450 Training loss 0.026899952441453934 Validation loss 0.050875574350357056 Accuracy 0.8640000224113464\n",
      "Iteration 28460 Training loss 0.024374309927225113 Validation loss 0.045473501086235046 Accuracy 0.877875030040741\n",
      "Iteration 28470 Training loss 0.020899195224046707 Validation loss 0.04706970974802971 Accuracy 0.8760000467300415\n",
      "Iteration 28480 Training loss 0.029593078419566154 Validation loss 0.05254361778497696 Accuracy 0.8582500219345093\n",
      "Iteration 28490 Training loss 0.027366945520043373 Validation loss 0.049901220947504044 Accuracy 0.8677500486373901\n",
      "Iteration 28500 Training loss 0.0231686532497406 Validation loss 0.0426965206861496 Accuracy 0.8867500424385071\n",
      "Iteration 28510 Training loss 0.022720936685800552 Validation loss 0.04725600779056549 Accuracy 0.8748750686645508\n",
      "Iteration 28520 Training loss 0.020374583080410957 Validation loss 0.042320240288972855 Accuracy 0.8888750672340393\n",
      "Iteration 28530 Training loss 0.022387156262993813 Validation loss 0.043607160449028015 Accuracy 0.8840000629425049\n",
      "Iteration 28540 Training loss 0.023465419188141823 Validation loss 0.04513007402420044 Accuracy 0.8786250352859497\n",
      "Iteration 28550 Training loss 0.03316156193614006 Validation loss 0.05032658949494362 Accuracy 0.8676250576972961\n",
      "Iteration 28560 Training loss 0.020941581577062607 Validation loss 0.0427771620452404 Accuracy 0.8860000371932983\n",
      "Iteration 28570 Training loss 0.030323125422000885 Validation loss 0.04588787630200386 Accuracy 0.8765000700950623\n",
      "Iteration 28580 Training loss 0.023206109181046486 Validation loss 0.04615481570363045 Accuracy 0.877750039100647\n",
      "Iteration 28590 Training loss 0.033838000148534775 Validation loss 0.054320674389600754 Accuracy 0.8550000190734863\n",
      "Iteration 28600 Training loss 0.024613022804260254 Validation loss 0.04431133344769478 Accuracy 0.8830000162124634\n",
      "Iteration 28610 Training loss 0.028253845870494843 Validation loss 0.05102790892124176 Accuracy 0.8655000329017639\n",
      "Iteration 28620 Training loss 0.016247503459453583 Validation loss 0.043118882924318314 Accuracy 0.8856250643730164\n",
      "Iteration 28630 Training loss 0.023070644587278366 Validation loss 0.0431605763733387 Accuracy 0.8868750333786011\n",
      "Iteration 28640 Training loss 0.02506924979388714 Validation loss 0.047743700444698334 Accuracy 0.8720000386238098\n",
      "Iteration 28650 Training loss 0.017288781702518463 Validation loss 0.04318782314658165 Accuracy 0.8867500424385071\n",
      "Iteration 28660 Training loss 0.024565882980823517 Validation loss 0.042815808206796646 Accuracy 0.8870000243186951\n",
      "Iteration 28670 Training loss 0.019802028313279152 Validation loss 0.04245457798242569 Accuracy 0.8877500295639038\n",
      "Iteration 28680 Training loss 0.0186244435608387 Validation loss 0.04278232157230377 Accuracy 0.8868750333786011\n",
      "Iteration 28690 Training loss 0.02048669010400772 Validation loss 0.04277675226330757 Accuracy 0.8885000348091125\n",
      "Iteration 28700 Training loss 0.02057994157075882 Validation loss 0.05008671060204506 Accuracy 0.8653750419616699\n",
      "Iteration 28710 Training loss 0.03144049644470215 Validation loss 0.05110150948166847 Accuracy 0.8635000586509705\n",
      "Iteration 28720 Training loss 0.021622102707624435 Validation loss 0.043548375368118286 Accuracy 0.8835000395774841\n",
      "Iteration 28730 Training loss 0.025747332721948624 Validation loss 0.043036337941884995 Accuracy 0.8857500553131104\n",
      "Iteration 28740 Training loss 0.02194514311850071 Validation loss 0.043021414428949356 Accuracy 0.8857500553131104\n",
      "Iteration 28750 Training loss 0.015978451818227768 Validation loss 0.04290251433849335 Accuracy 0.8871250152587891\n",
      "Iteration 28760 Training loss 0.037862036377191544 Validation loss 0.05282076820731163 Accuracy 0.8565000295639038\n",
      "Iteration 28770 Training loss 0.02088668756186962 Validation loss 0.04239160940051079 Accuracy 0.8881250619888306\n",
      "Iteration 28780 Training loss 0.01582890935242176 Validation loss 0.045982614159584045 Accuracy 0.8788750171661377\n",
      "Iteration 28790 Training loss 0.01851288601756096 Validation loss 0.04322927072644234 Accuracy 0.8850000500679016\n",
      "Iteration 28800 Training loss 0.01943742297589779 Validation loss 0.0438263937830925 Accuracy 0.8848750591278076\n",
      "Iteration 28810 Training loss 0.02076050452888012 Validation loss 0.04313270002603531 Accuracy 0.8862500190734863\n",
      "Iteration 28820 Training loss 0.03345479816198349 Validation loss 0.05157148838043213 Accuracy 0.8611250519752502\n",
      "Iteration 28830 Training loss 0.03598151355981827 Validation loss 0.054041482508182526 Accuracy 0.8561250567436218\n",
      "Iteration 28840 Training loss 0.014784526079893112 Validation loss 0.04272398725152016 Accuracy 0.8861250281333923\n",
      "Iteration 28850 Training loss 0.026622265577316284 Validation loss 0.047624245285987854 Accuracy 0.8735000491142273\n",
      "Iteration 28860 Training loss 0.01780105009675026 Validation loss 0.04596702754497528 Accuracy 0.8766250610351562\n",
      "Iteration 28870 Training loss 0.014137395657598972 Validation loss 0.04307704046368599 Accuracy 0.8852500319480896\n",
      "Iteration 28880 Training loss 0.02169978618621826 Validation loss 0.043856896460056305 Accuracy 0.8820000290870667\n",
      "Iteration 28890 Training loss 0.059561219066381454 Validation loss 0.0736246183514595 Accuracy 0.8091250658035278\n",
      "Iteration 28900 Training loss 0.02484002523124218 Validation loss 0.04591511934995651 Accuracy 0.8765000700950623\n",
      "Iteration 28910 Training loss 0.026868633925914764 Validation loss 0.04931919276714325 Accuracy 0.8685000538825989\n",
      "Iteration 28920 Training loss 0.021304253488779068 Validation loss 0.04243135824799538 Accuracy 0.8883750438690186\n",
      "Iteration 28930 Training loss 0.03148621320724487 Validation loss 0.04907749593257904 Accuracy 0.8691250681877136\n",
      "Iteration 28940 Training loss 0.020340003073215485 Validation loss 0.04689007252454758 Accuracy 0.8755000233650208\n",
      "Iteration 28950 Training loss 0.03471873328089714 Validation loss 0.056834276765584946 Accuracy 0.8495000600814819\n",
      "Iteration 28960 Training loss 0.019841518253087997 Validation loss 0.042702239006757736 Accuracy 0.8862500190734863\n",
      "Iteration 28970 Training loss 0.016472959890961647 Validation loss 0.04269814118742943 Accuracy 0.8866250514984131\n",
      "Iteration 28980 Training loss 0.013428266160190105 Validation loss 0.04262664169073105 Accuracy 0.8886250257492065\n",
      "Iteration 28990 Training loss 0.03011285699903965 Validation loss 0.04865049943327904 Accuracy 0.8696250319480896\n",
      "Iteration 29000 Training loss 0.01614110916852951 Validation loss 0.04245147109031677 Accuracy 0.8883750438690186\n",
      "Iteration 29010 Training loss 0.03570675104856491 Validation loss 0.053524889051914215 Accuracy 0.8568750619888306\n",
      "Iteration 29020 Training loss 0.022759493440389633 Validation loss 0.04818455129861832 Accuracy 0.8713750243186951\n",
      "Iteration 29030 Training loss 0.014952040277421474 Validation loss 0.042435817420482635 Accuracy 0.8885000348091125\n",
      "Iteration 29040 Training loss 0.024295873939990997 Validation loss 0.05007181689143181 Accuracy 0.8667500615119934\n",
      "Iteration 29050 Training loss 0.026402754709124565 Validation loss 0.044925421476364136 Accuracy 0.8806250691413879\n",
      "Iteration 29060 Training loss 0.017480162903666496 Validation loss 0.04353105649352074 Accuracy 0.8840000629425049\n",
      "Iteration 29070 Training loss 0.020286664366722107 Validation loss 0.04235699027776718 Accuracy 0.890250027179718\n",
      "Iteration 29080 Training loss 0.018432701006531715 Validation loss 0.043313272297382355 Accuracy 0.8847500681877136\n",
      "Iteration 29090 Training loss 0.04119978845119476 Validation loss 0.06095191836357117 Accuracy 0.8382500410079956\n",
      "Iteration 29100 Training loss 0.019972991198301315 Validation loss 0.04385346546769142 Accuracy 0.8845000267028809\n",
      "Iteration 29110 Training loss 0.02328866347670555 Validation loss 0.044792626053094864 Accuracy 0.8797500133514404\n",
      "Iteration 29120 Training loss 0.02134091965854168 Validation loss 0.04249925538897514 Accuracy 0.8876250386238098\n",
      "Iteration 29130 Training loss 0.02378806471824646 Validation loss 0.04575582593679428 Accuracy 0.8788750171661377\n",
      "Iteration 29140 Training loss 0.0178158488124609 Validation loss 0.042779043316841125 Accuracy 0.8870000243186951\n",
      "Iteration 29150 Training loss 0.027630817145109177 Validation loss 0.04868405684828758 Accuracy 0.8703750371932983\n",
      "Iteration 29160 Training loss 0.020847303792834282 Validation loss 0.04569095000624657 Accuracy 0.8791250586509705\n",
      "Iteration 29170 Training loss 0.020006243139505386 Validation loss 0.04648644104599953 Accuracy 0.8758750557899475\n",
      "Iteration 29180 Training loss 0.02069965749979019 Validation loss 0.04377754405140877 Accuracy 0.8832500576972961\n",
      "Iteration 29190 Training loss 0.02123655192553997 Validation loss 0.04405345395207405 Accuracy 0.8840000629425049\n",
      "Iteration 29200 Training loss 0.018952781334519386 Validation loss 0.042578812688589096 Accuracy 0.8883750438690186\n",
      "Iteration 29210 Training loss 0.02475038543343544 Validation loss 0.04571860656142235 Accuracy 0.8796250224113464\n",
      "Iteration 29220 Training loss 0.01745874248445034 Validation loss 0.04237796738743782 Accuracy 0.8882500529289246\n",
      "Iteration 29230 Training loss 0.021771535277366638 Validation loss 0.04789892956614494 Accuracy 0.8723750710487366\n",
      "Iteration 29240 Training loss 0.02716205082833767 Validation loss 0.04264532029628754 Accuracy 0.8880000710487366\n",
      "Iteration 29250 Training loss 0.016209052875638008 Validation loss 0.04409611597657204 Accuracy 0.8827500343322754\n",
      "Iteration 29260 Training loss 0.019463330507278442 Validation loss 0.04416719079017639 Accuracy 0.8831250667572021\n",
      "Iteration 29270 Training loss 0.022497005760669708 Validation loss 0.04426635801792145 Accuracy 0.8817500472068787\n",
      "Iteration 29280 Training loss 0.0566854290664196 Validation loss 0.0695764496922493 Accuracy 0.8226250410079956\n",
      "Iteration 29290 Training loss 0.016373345628380775 Validation loss 0.04426772519946098 Accuracy 0.8812500238418579\n",
      "Iteration 29300 Training loss 0.028414711356163025 Validation loss 0.05096900090575218 Accuracy 0.8630000352859497\n",
      "Iteration 29310 Training loss 0.019263168796896935 Validation loss 0.043887972831726074 Accuracy 0.8826250433921814\n",
      "Iteration 29320 Training loss 0.018020140007138252 Validation loss 0.042714666575193405 Accuracy 0.8861250281333923\n",
      "Iteration 29330 Training loss 0.014626206830143929 Validation loss 0.0423678383231163 Accuracy 0.8890000581741333\n",
      "Iteration 29340 Training loss 0.017693977802991867 Validation loss 0.042376305907964706 Accuracy 0.8887500166893005\n",
      "Iteration 29350 Training loss 0.02627556212246418 Validation loss 0.048227548599243164 Accuracy 0.8722500205039978\n",
      "Iteration 29360 Training loss 0.03232872486114502 Validation loss 0.0515887625515461 Accuracy 0.8641250133514404\n",
      "Iteration 29370 Training loss 0.019335603341460228 Validation loss 0.0455772764980793 Accuracy 0.8798750638961792\n",
      "Iteration 29380 Training loss 0.03682940825819969 Validation loss 0.05126455798745155 Accuracy 0.8640000224113464\n",
      "Iteration 29390 Training loss 0.018092812970280647 Validation loss 0.04310069978237152 Accuracy 0.8866250514984131\n",
      "Iteration 29400 Training loss 0.020932968705892563 Validation loss 0.04363524541258812 Accuracy 0.8828750252723694\n",
      "Iteration 29410 Training loss 0.018036074936389923 Validation loss 0.04441710188984871 Accuracy 0.8821250200271606\n",
      "Iteration 29420 Training loss 0.022449860349297523 Validation loss 0.0450911819934845 Accuracy 0.8803750276565552\n",
      "Iteration 29430 Training loss 0.02104097232222557 Validation loss 0.04329714551568031 Accuracy 0.8850000500679016\n",
      "Iteration 29440 Training loss 0.012329732067883015 Validation loss 0.04257506877183914 Accuracy 0.8882500529289246\n",
      "Iteration 29450 Training loss 0.02447541244328022 Validation loss 0.04721254110336304 Accuracy 0.8757500648498535\n",
      "Iteration 29460 Training loss 0.02098880521953106 Validation loss 0.04252214357256889 Accuracy 0.8881250619888306\n",
      "Iteration 29470 Training loss 0.029388107359409332 Validation loss 0.05002940446138382 Accuracy 0.8666250705718994\n",
      "Iteration 29480 Training loss 0.023890074342489243 Validation loss 0.043045345693826675 Accuracy 0.8878750205039978\n",
      "Iteration 29490 Training loss 0.02298717014491558 Validation loss 0.048327457159757614 Accuracy 0.8711250424385071\n",
      "Iteration 29500 Training loss 0.020017441362142563 Validation loss 0.046762436628341675 Accuracy 0.8765000700950623\n",
      "Iteration 29510 Training loss 0.022124487906694412 Validation loss 0.0454753078520298 Accuracy 0.8795000314712524\n",
      "Iteration 29520 Training loss 0.021950816735625267 Validation loss 0.04336956515908241 Accuracy 0.8855000138282776\n",
      "Iteration 29530 Training loss 0.022325662896037102 Validation loss 0.049067314714193344 Accuracy 0.8692500591278076\n",
      "Iteration 29540 Training loss 0.02503363974392414 Validation loss 0.04239945486187935 Accuracy 0.8886250257492065\n",
      "Iteration 29550 Training loss 0.014200751669704914 Validation loss 0.042981430888175964 Accuracy 0.8861250281333923\n",
      "Iteration 29560 Training loss 0.014117913320660591 Validation loss 0.04297037050127983 Accuracy 0.8875000476837158\n",
      "Iteration 29570 Training loss 0.017680902034044266 Validation loss 0.04299196973443031 Accuracy 0.8862500190734863\n",
      "Iteration 29580 Training loss 0.04670146852731705 Validation loss 0.07299629598855972 Accuracy 0.8131250143051147\n",
      "Iteration 29590 Training loss 0.012090879492461681 Validation loss 0.042925700545310974 Accuracy 0.8873750567436218\n",
      "Iteration 29600 Training loss 0.021941065788269043 Validation loss 0.04615791514515877 Accuracy 0.877875030040741\n",
      "Iteration 29610 Training loss 0.026626452803611755 Validation loss 0.04517008736729622 Accuracy 0.8822500705718994\n",
      "Iteration 29620 Training loss 0.02622162736952305 Validation loss 0.04641703888773918 Accuracy 0.8792500495910645\n",
      "Iteration 29630 Training loss 0.022358885034918785 Validation loss 0.042393758893013 Accuracy 0.8885000348091125\n",
      "Iteration 29640 Training loss 0.024668382480740547 Validation loss 0.04230961203575134 Accuracy 0.8876250386238098\n",
      "Iteration 29650 Training loss 0.021385557949543 Validation loss 0.043758951127529144 Accuracy 0.8851250410079956\n",
      "Iteration 29660 Training loss 0.019231131300330162 Validation loss 0.04244135320186615 Accuracy 0.8877500295639038\n",
      "Iteration 29670 Training loss 0.015815189108252525 Validation loss 0.0428897961974144 Accuracy 0.8853750228881836\n",
      "Iteration 29680 Training loss 0.022318843752145767 Validation loss 0.04381977766752243 Accuracy 0.8843750357627869\n",
      "Iteration 29690 Training loss 0.023237742483615875 Validation loss 0.042556557804346085 Accuracy 0.8893750309944153\n",
      "Iteration 29700 Training loss 0.018531806766986847 Validation loss 0.04439448192715645 Accuracy 0.8833750486373901\n",
      "Iteration 29710 Training loss 0.01946447230875492 Validation loss 0.04280179738998413 Accuracy 0.8863750696182251\n",
      "Iteration 29720 Training loss 0.029599683359265327 Validation loss 0.053522989153862 Accuracy 0.8572500348091125\n",
      "Iteration 29730 Training loss 0.018917886540293694 Validation loss 0.04701914265751839 Accuracy 0.874750018119812\n",
      "Iteration 29740 Training loss 0.02293112687766552 Validation loss 0.04499160498380661 Accuracy 0.8808750510215759\n",
      "Iteration 29750 Training loss 0.013104051351547241 Validation loss 0.04315611347556114 Accuracy 0.8870000243186951\n",
      "Iteration 29760 Training loss 0.04131949320435524 Validation loss 0.06595456600189209 Accuracy 0.8287500143051147\n",
      "Iteration 29770 Training loss 0.021326621994376183 Validation loss 0.04303889721632004 Accuracy 0.8875000476837158\n",
      "Iteration 29780 Training loss 0.017912954092025757 Validation loss 0.04278657212853432 Accuracy 0.8882500529289246\n",
      "Iteration 29790 Training loss 0.016434570774435997 Validation loss 0.044334154576063156 Accuracy 0.8805000185966492\n",
      "Iteration 29800 Training loss 0.01984252966940403 Validation loss 0.04283644258975983 Accuracy 0.8871250152587891\n",
      "Iteration 29810 Training loss 0.021007131785154343 Validation loss 0.04376476630568504 Accuracy 0.8836250305175781\n",
      "Iteration 29820 Training loss 0.026870382949709892 Validation loss 0.04574580118060112 Accuracy 0.8802500367164612\n",
      "Iteration 29830 Training loss 0.022298110648989677 Validation loss 0.04235098510980606 Accuracy 0.890250027179718\n",
      "Iteration 29840 Training loss 0.03278005123138428 Validation loss 0.05471569299697876 Accuracy 0.8551250696182251\n",
      "Iteration 29850 Training loss 0.015159153379499912 Validation loss 0.04335605725646019 Accuracy 0.8852500319480896\n",
      "Iteration 29860 Training loss 0.016133392229676247 Validation loss 0.042942728847265244 Accuracy 0.8868750333786011\n",
      "Iteration 29870 Training loss 0.01897474192082882 Validation loss 0.04274483770132065 Accuracy 0.8892500400543213\n",
      "Iteration 29880 Training loss 0.02216784842312336 Validation loss 0.04599368944764137 Accuracy 0.8795000314712524\n",
      "Iteration 29890 Training loss 0.02381441555917263 Validation loss 0.042503565549850464 Accuracy 0.8877500295639038\n",
      "Iteration 29900 Training loss 0.02018151432275772 Validation loss 0.04604540020227432 Accuracy 0.8767500519752502\n",
      "Iteration 29910 Training loss 0.02018909715116024 Validation loss 0.042913768440485 Accuracy 0.8873750567436218\n",
      "Iteration 29920 Training loss 0.01927359588444233 Validation loss 0.04624532163143158 Accuracy 0.877875030040741\n",
      "Iteration 29930 Training loss 0.018236253410577774 Validation loss 0.047750622034072876 Accuracy 0.8733750581741333\n",
      "Iteration 29940 Training loss 0.018080491572618484 Validation loss 0.04460636526346207 Accuracy 0.8823750615119934\n",
      "Iteration 29950 Training loss 0.01940944604575634 Validation loss 0.04242534190416336 Accuracy 0.8878750205039978\n",
      "Iteration 29960 Training loss 0.02071523107588291 Validation loss 0.042988911271095276 Accuracy 0.8860000371932983\n",
      "Iteration 29970 Training loss 0.03599094599485397 Validation loss 0.05477195978164673 Accuracy 0.8508750200271606\n",
      "Iteration 29980 Training loss 0.040309857577085495 Validation loss 0.05679723620414734 Accuracy 0.8511250615119934\n",
      "Iteration 29990 Training loss 0.015648849308490753 Validation loss 0.0461796298623085 Accuracy 0.877875030040741\n",
      "Iteration 30000 Training loss 0.021092791110277176 Validation loss 0.04313715919852257 Accuracy 0.8851250410079956\n",
      "Iteration 30010 Training loss 0.019622284919023514 Validation loss 0.04246826469898224 Accuracy 0.8890000581741333\n",
      "Iteration 30020 Training loss 0.021872488781809807 Validation loss 0.05025745928287506 Accuracy 0.8643750548362732\n",
      "Iteration 30030 Training loss 0.020164407789707184 Validation loss 0.046192679554224014 Accuracy 0.877375066280365\n",
      "Iteration 30040 Training loss 0.015799233689904213 Validation loss 0.04228644818067551 Accuracy 0.8905000686645508\n",
      "Iteration 30050 Training loss 0.021488023921847343 Validation loss 0.042279619723558426 Accuracy 0.8895000219345093\n",
      "Iteration 30060 Training loss 0.021281380206346512 Validation loss 0.04329840466380119 Accuracy 0.8840000629425049\n",
      "Iteration 30070 Training loss 0.02065451629459858 Validation loss 0.04485879838466644 Accuracy 0.8813750147819519\n",
      "Iteration 30080 Training loss 0.016660258173942566 Validation loss 0.04325214773416519 Accuracy 0.8855000138282776\n",
      "Iteration 30090 Training loss 0.01777188666164875 Validation loss 0.04446498304605484 Accuracy 0.8818750381469727\n",
      "Iteration 30100 Training loss 0.02229355275630951 Validation loss 0.04454493150115013 Accuracy 0.8797500133514404\n",
      "Iteration 30110 Training loss 0.018547529354691505 Validation loss 0.042733706533908844 Accuracy 0.8883750438690186\n",
      "Iteration 30120 Training loss 0.026604551821947098 Validation loss 0.04403890669345856 Accuracy 0.8847500681877136\n",
      "Iteration 30130 Training loss 0.02524646930396557 Validation loss 0.048086121678352356 Accuracy 0.8698750138282776\n",
      "Iteration 30140 Training loss 0.015500488691031933 Validation loss 0.04284112527966499 Accuracy 0.8877500295639038\n",
      "Iteration 30150 Training loss 0.03701234608888626 Validation loss 0.05861253663897514 Accuracy 0.846875011920929\n",
      "Iteration 30160 Training loss 0.015066113322973251 Validation loss 0.04295546934008598 Accuracy 0.8877500295639038\n",
      "Iteration 30170 Training loss 0.02007354609668255 Validation loss 0.04285292327404022 Accuracy 0.8886250257492065\n",
      "Iteration 30180 Training loss 0.029285423457622528 Validation loss 0.044298894703388214 Accuracy 0.8826250433921814\n",
      "Iteration 30190 Training loss 0.02639404870569706 Validation loss 0.04454881697893143 Accuracy 0.8811250329017639\n",
      "Iteration 30200 Training loss 0.025970546528697014 Validation loss 0.04803634434938431 Accuracy 0.874500036239624\n",
      "Iteration 30210 Training loss 0.020985597744584084 Validation loss 0.04233832284808159 Accuracy 0.8888750672340393\n",
      "Iteration 30220 Training loss 0.01803138479590416 Validation loss 0.042350634932518005 Accuracy 0.8881250619888306\n",
      "Iteration 30230 Training loss 0.023940982297062874 Validation loss 0.04234659671783447 Accuracy 0.8885000348091125\n",
      "Iteration 30240 Training loss 0.04791925475001335 Validation loss 0.05916967615485191 Accuracy 0.8441250324249268\n",
      "Iteration 30250 Training loss 0.016801368445158005 Validation loss 0.04376375675201416 Accuracy 0.8845000267028809\n",
      "Iteration 30260 Training loss 0.022285962477326393 Validation loss 0.045283056795597076 Accuracy 0.8797500133514404\n",
      "Iteration 30270 Training loss 0.018062330782413483 Validation loss 0.04267843812704086 Accuracy 0.8885000348091125\n",
      "Iteration 30280 Training loss 0.026462770998477936 Validation loss 0.05249231681227684 Accuracy 0.8597500324249268\n",
      "Iteration 30290 Training loss 0.0316176563501358 Validation loss 0.04814103990793228 Accuracy 0.8735000491142273\n",
      "Iteration 30300 Training loss 0.01971038244664669 Validation loss 0.04256587103009224 Accuracy 0.8892500400543213\n",
      "Iteration 30310 Training loss 0.032185573130846024 Validation loss 0.05677128955721855 Accuracy 0.8481250405311584\n",
      "Iteration 30320 Training loss 0.02292684093117714 Validation loss 0.043927207589149475 Accuracy 0.8835000395774841\n",
      "Iteration 30330 Training loss 0.0336323007941246 Validation loss 0.05318856239318848 Accuracy 0.859000027179718\n",
      "Iteration 30340 Training loss 0.015417544171214104 Validation loss 0.042332131415605545 Accuracy 0.8895000219345093\n",
      "Iteration 30350 Training loss 0.015061881393194199 Validation loss 0.04370863735675812 Accuracy 0.8841250538825989\n",
      "Iteration 30360 Training loss 0.019390543922781944 Validation loss 0.04293898865580559 Accuracy 0.8865000605583191\n",
      "Iteration 30370 Training loss 0.017240267246961594 Validation loss 0.04235274717211723 Accuracy 0.8893750309944153\n",
      "Iteration 30380 Training loss 0.017907053232192993 Validation loss 0.04335077479481697 Accuracy 0.8856250643730164\n",
      "Iteration 30390 Training loss 0.01715201325714588 Validation loss 0.04236866161227226 Accuracy 0.8885000348091125\n",
      "Iteration 30400 Training loss 0.03983113914728165 Validation loss 0.07120693475008011 Accuracy 0.8157500624656677\n",
      "Iteration 30410 Training loss 0.01217526663094759 Validation loss 0.04496874660253525 Accuracy 0.8805000185966492\n",
      "Iteration 30420 Training loss 0.025373883545398712 Validation loss 0.049417972564697266 Accuracy 0.8673750162124634\n",
      "Iteration 30430 Training loss 0.019991815090179443 Validation loss 0.04434499517083168 Accuracy 0.8825000524520874\n",
      "Iteration 30440 Training loss 0.02850770391523838 Validation loss 0.050319090485572815 Accuracy 0.8680000305175781\n",
      "Iteration 30450 Training loss 0.013813194818794727 Validation loss 0.04303682595491409 Accuracy 0.8875000476837158\n",
      "Iteration 30460 Training loss 0.02027827501296997 Validation loss 0.05083301290869713 Accuracy 0.8641250133514404\n",
      "Iteration 30470 Training loss 0.012227964587509632 Validation loss 0.04276430606842041 Accuracy 0.8867500424385071\n",
      "Iteration 30480 Training loss 0.05073956772685051 Validation loss 0.0661957710981369 Accuracy 0.8287500143051147\n",
      "Iteration 30490 Training loss 0.02133958600461483 Validation loss 0.046987589448690414 Accuracy 0.874500036239624\n",
      "Iteration 30500 Training loss 0.01889076828956604 Validation loss 0.044721949845552444 Accuracy 0.8815000653266907\n",
      "Iteration 30510 Training loss 0.01629868522286415 Validation loss 0.043922413140535355 Accuracy 0.8838750123977661\n",
      "Iteration 30520 Training loss 0.019424574449658394 Validation loss 0.04323698207736015 Accuracy 0.8867500424385071\n",
      "Iteration 30530 Training loss 0.018974680453538895 Validation loss 0.04578219726681709 Accuracy 0.8800000548362732\n",
      "Iteration 30540 Training loss 0.017722750082612038 Validation loss 0.042554352432489395 Accuracy 0.8883750438690186\n",
      "Iteration 30550 Training loss 0.018631193786859512 Validation loss 0.04642130434513092 Accuracy 0.8758750557899475\n",
      "Iteration 30560 Training loss 0.022917840629816055 Validation loss 0.045740410685539246 Accuracy 0.8798750638961792\n",
      "Iteration 30570 Training loss 0.018813222646713257 Validation loss 0.04314754903316498 Accuracy 0.8860000371932983\n",
      "Iteration 30580 Training loss 0.014282247982919216 Validation loss 0.04250597581267357 Accuracy 0.8875000476837158\n",
      "Iteration 30590 Training loss 0.02786668762564659 Validation loss 0.04813149571418762 Accuracy 0.8716250658035278\n",
      "Iteration 30600 Training loss 0.04106205329298973 Validation loss 0.06328219175338745 Accuracy 0.8357500433921814\n",
      "Iteration 30610 Training loss 0.023274006322026253 Validation loss 0.043421268463134766 Accuracy 0.8861250281333923\n",
      "Iteration 30620 Training loss 0.019164999946951866 Validation loss 0.046284209936857224 Accuracy 0.8785000443458557\n",
      "Iteration 30630 Training loss 0.01482229121029377 Validation loss 0.042506422847509384 Accuracy 0.8877500295639038\n",
      "Iteration 30640 Training loss 0.029314197599887848 Validation loss 0.04984317347407341 Accuracy 0.8661250472068787\n",
      "Iteration 30650 Training loss 0.018742486834526062 Validation loss 0.0424373634159565 Accuracy 0.8872500658035278\n",
      "Iteration 30660 Training loss 0.023172451183199883 Validation loss 0.044086046516895294 Accuracy 0.8831250667572021\n",
      "Iteration 30670 Training loss 0.04611166939139366 Validation loss 0.06531722098588943 Accuracy 0.8295000195503235\n",
      "Iteration 30680 Training loss 0.023576034232974052 Validation loss 0.04834265261888504 Accuracy 0.8711250424385071\n",
      "Iteration 30690 Training loss 0.016252068802714348 Validation loss 0.042589668184518814 Accuracy 0.8875000476837158\n",
      "Iteration 30700 Training loss 0.025981416925787926 Validation loss 0.04809880256652832 Accuracy 0.8716250658035278\n",
      "Iteration 30710 Training loss 0.019347963854670525 Validation loss 0.04289398342370987 Accuracy 0.8863750696182251\n",
      "Iteration 30720 Training loss 0.01772444322705269 Validation loss 0.044154610484838486 Accuracy 0.8823750615119934\n",
      "Iteration 30730 Training loss 0.017931299284100533 Validation loss 0.04297947883605957 Accuracy 0.8870000243186951\n",
      "Iteration 30740 Training loss 0.02783392369747162 Validation loss 0.048994820564985275 Accuracy 0.8692500591278076\n",
      "Iteration 30750 Training loss 0.02625814639031887 Validation loss 0.0432174876332283 Accuracy 0.8868750333786011\n",
      "Iteration 30760 Training loss 0.019506530836224556 Validation loss 0.042461927980184555 Accuracy 0.8882500529289246\n",
      "Iteration 30770 Training loss 0.0397174246609211 Validation loss 0.056560151278972626 Accuracy 0.8520000576972961\n",
      "Iteration 30780 Training loss 0.03451334312558174 Validation loss 0.05697739124298096 Accuracy 0.8520000576972961\n",
      "Iteration 30790 Training loss 0.017444835975766182 Validation loss 0.04389028623700142 Accuracy 0.8837500214576721\n",
      "Iteration 30800 Training loss 0.030963744968175888 Validation loss 0.05837255343794823 Accuracy 0.8460000157356262\n",
      "Iteration 30810 Training loss 0.01713418960571289 Validation loss 0.042721427977085114 Accuracy 0.8878750205039978\n",
      "Iteration 30820 Training loss 0.03586401045322418 Validation loss 0.05979333817958832 Accuracy 0.8455000519752502\n",
      "Iteration 30830 Training loss 0.013859644532203674 Validation loss 0.042969271540641785 Accuracy 0.8880000710487366\n",
      "Iteration 30840 Training loss 0.022617250680923462 Validation loss 0.04523903876543045 Accuracy 0.8791250586509705\n",
      "Iteration 30850 Training loss 0.01904488354921341 Validation loss 0.043367981910705566 Accuracy 0.8867500424385071\n",
      "Iteration 30860 Training loss 0.023872876539826393 Validation loss 0.04937974736094475 Accuracy 0.8677500486373901\n",
      "Iteration 30870 Training loss 0.016513077542185783 Validation loss 0.042201172560453415 Accuracy 0.8895000219345093\n",
      "Iteration 30880 Training loss 0.018039166927337646 Validation loss 0.04227767139673233 Accuracy 0.8893750309944153\n",
      "Iteration 30890 Training loss 0.035409167408943176 Validation loss 0.05394434556365013 Accuracy 0.859000027179718\n",
      "Iteration 30900 Training loss 0.013945810496807098 Validation loss 0.04557071253657341 Accuracy 0.8807500600814819\n",
      "Iteration 30910 Training loss 0.025345318019390106 Validation loss 0.0434466116130352 Accuracy 0.8863750696182251\n",
      "Iteration 30920 Training loss 0.026214228942990303 Validation loss 0.045040782541036606 Accuracy 0.8796250224113464\n",
      "Iteration 30930 Training loss 0.020486950874328613 Validation loss 0.043527889996767044 Accuracy 0.8875000476837158\n",
      "Iteration 30940 Training loss 0.022005071863532066 Validation loss 0.04562317952513695 Accuracy 0.878125011920929\n",
      "Iteration 30950 Training loss 0.02349875308573246 Validation loss 0.0464458204805851 Accuracy 0.8750000596046448\n",
      "Iteration 30960 Training loss 0.03823234885931015 Validation loss 0.060921020805835724 Accuracy 0.8416250348091125\n",
      "Iteration 30970 Training loss 0.02203565277159214 Validation loss 0.04517820104956627 Accuracy 0.8803750276565552\n",
      "Iteration 30980 Training loss 0.01740339770913124 Validation loss 0.04449496418237686 Accuracy 0.8820000290870667\n",
      "Iteration 30990 Training loss 0.023341583088040352 Validation loss 0.043521180748939514 Accuracy 0.8860000371932983\n",
      "Iteration 31000 Training loss 0.019620290026068687 Validation loss 0.0426337830722332 Accuracy 0.8880000710487366\n",
      "Iteration 31010 Training loss 0.01967860944569111 Validation loss 0.0446629598736763 Accuracy 0.8818750381469727\n",
      "Iteration 31020 Training loss 0.0177350752055645 Validation loss 0.042602021247148514 Accuracy 0.8880000710487366\n",
      "Iteration 31030 Training loss 0.026547245681285858 Validation loss 0.05235142260789871 Accuracy 0.8607500195503235\n",
      "Iteration 31040 Training loss 0.023926522582769394 Validation loss 0.047631822526454926 Accuracy 0.8733750581741333\n",
      "Iteration 31050 Training loss 0.016420254483819008 Validation loss 0.044078752398490906 Accuracy 0.8831250667572021\n",
      "Iteration 31060 Training loss 0.017321672290563583 Validation loss 0.043852128088474274 Accuracy 0.8845000267028809\n",
      "Iteration 31070 Training loss 0.016454605385661125 Validation loss 0.04285619407892227 Accuracy 0.8871250152587891\n",
      "Iteration 31080 Training loss 0.028833841904997826 Validation loss 0.05239785835146904 Accuracy 0.858500063419342\n",
      "Iteration 31090 Training loss 0.02249007113277912 Validation loss 0.04314001649618149 Accuracy 0.8857500553131104\n",
      "Iteration 31100 Training loss 0.018507501110434532 Validation loss 0.04233464226126671 Accuracy 0.8893750309944153\n",
      "Iteration 31110 Training loss 0.021365471184253693 Validation loss 0.04396793246269226 Accuracy 0.8840000629425049\n",
      "Iteration 31120 Training loss 0.04479389265179634 Validation loss 0.06448372453451157 Accuracy 0.8322500586509705\n",
      "Iteration 31130 Training loss 0.019546734169125557 Validation loss 0.04283083602786064 Accuracy 0.8865000605583191\n",
      "Iteration 31140 Training loss 0.018670327961444855 Validation loss 0.04217534884810448 Accuracy 0.8895000219345093\n",
      "Iteration 31150 Training loss 0.020151758566498756 Validation loss 0.04373551532626152 Accuracy 0.8838750123977661\n",
      "Iteration 31160 Training loss 0.01992839016020298 Validation loss 0.04426873102784157 Accuracy 0.8826250433921814\n",
      "Iteration 31170 Training loss 0.02160831354558468 Validation loss 0.04258999228477478 Accuracy 0.889750063419342\n",
      "Iteration 31180 Training loss 0.01202927902340889 Validation loss 0.043711163103580475 Accuracy 0.8841250538825989\n",
      "Iteration 31190 Training loss 0.024501223117113113 Validation loss 0.0498586967587471 Accuracy 0.8663750290870667\n",
      "Iteration 31200 Training loss 0.01549153309315443 Validation loss 0.043057940900325775 Accuracy 0.8887500166893005\n",
      "Iteration 31210 Training loss 0.018050147220492363 Validation loss 0.042566318064928055 Accuracy 0.8883750438690186\n",
      "Iteration 31220 Training loss 0.022168908268213272 Validation loss 0.049710724502801895 Accuracy 0.8676250576972961\n",
      "Iteration 31230 Training loss 0.020164363086223602 Validation loss 0.04533756524324417 Accuracy 0.8812500238418579\n",
      "Iteration 31240 Training loss 0.018152641132473946 Validation loss 0.04258272796869278 Accuracy 0.8882500529289246\n",
      "Iteration 31250 Training loss 0.013990122824907303 Validation loss 0.04259302839636803 Accuracy 0.8887500166893005\n",
      "Iteration 31260 Training loss 0.023076729848980904 Validation loss 0.050576966255903244 Accuracy 0.8663750290870667\n",
      "Iteration 31270 Training loss 0.030390825122594833 Validation loss 0.053436148911714554 Accuracy 0.8610000610351562\n",
      "Iteration 31280 Training loss 0.016201311722397804 Validation loss 0.042485084384679794 Accuracy 0.8873750567436218\n",
      "Iteration 31290 Training loss 0.025143012404441833 Validation loss 0.04316176101565361 Accuracy 0.8865000605583191\n",
      "Iteration 31300 Training loss 0.027921417728066444 Validation loss 0.05282317101955414 Accuracy 0.861750066280365\n",
      "Iteration 31310 Training loss 0.031620997935533524 Validation loss 0.05186238884925842 Accuracy 0.862250030040741\n",
      "Iteration 31320 Training loss 0.02172425016760826 Validation loss 0.05360421910881996 Accuracy 0.8581250309944153\n",
      "Iteration 31330 Training loss 0.01596864126622677 Validation loss 0.04230882227420807 Accuracy 0.8890000581741333\n",
      "Iteration 31340 Training loss 0.025986088439822197 Validation loss 0.044212762266397476 Accuracy 0.8843750357627869\n",
      "Iteration 31350 Training loss 0.021048879250884056 Validation loss 0.04423060640692711 Accuracy 0.8835000395774841\n",
      "Iteration 31360 Training loss 0.017345931380987167 Validation loss 0.04266482591629028 Accuracy 0.8862500190734863\n",
      "Iteration 31370 Training loss 0.01647270657122135 Validation loss 0.04228776693344116 Accuracy 0.8888750672340393\n",
      "Iteration 31380 Training loss 0.021204723045229912 Validation loss 0.04350867122411728 Accuracy 0.8857500553131104\n",
      "Iteration 31390 Training loss 0.02348310872912407 Validation loss 0.04744197428226471 Accuracy 0.8740000128746033\n",
      "Iteration 31400 Training loss 0.032662283629179 Validation loss 0.05215094983577728 Accuracy 0.8648750185966492\n",
      "Iteration 31410 Training loss 0.020933637395501137 Validation loss 0.04725521057844162 Accuracy 0.8762500286102295\n",
      "Iteration 31420 Training loss 0.016185631975531578 Validation loss 0.04244165122509003 Accuracy 0.8883750438690186\n",
      "Iteration 31430 Training loss 0.01934640482068062 Validation loss 0.047379568219184875 Accuracy 0.8748750686645508\n",
      "Iteration 31440 Training loss 0.018123004585504532 Validation loss 0.04257228597998619 Accuracy 0.8877500295639038\n",
      "Iteration 31450 Training loss 0.018041549250483513 Validation loss 0.045310862362384796 Accuracy 0.8797500133514404\n",
      "Iteration 31460 Training loss 0.017805950716137886 Validation loss 0.04698099568486214 Accuracy 0.8766250610351562\n",
      "Iteration 31470 Training loss 0.017514199018478394 Validation loss 0.04372222349047661 Accuracy 0.8866250514984131\n",
      "Iteration 31480 Training loss 0.01969883218407631 Validation loss 0.042934633791446686 Accuracy 0.8880000710487366\n",
      "Iteration 31490 Training loss 0.013275585137307644 Validation loss 0.044662706553936005 Accuracy 0.8825000524520874\n",
      "Iteration 31500 Training loss 0.012904872186481953 Validation loss 0.04307660087943077 Accuracy 0.8873750567436218\n",
      "Iteration 31510 Training loss 0.020509008318185806 Validation loss 0.043518487364053726 Accuracy 0.8875000476837158\n",
      "Iteration 31520 Training loss 0.02167520485818386 Validation loss 0.04226648807525635 Accuracy 0.8886250257492065\n",
      "Iteration 31530 Training loss 0.01937132701277733 Validation loss 0.04276265576481819 Accuracy 0.8882500529289246\n",
      "Iteration 31540 Training loss 0.014070175588130951 Validation loss 0.04232805222272873 Accuracy 0.8876250386238098\n",
      "Iteration 31550 Training loss 0.016924120485782623 Validation loss 0.04540811479091644 Accuracy 0.8811250329017639\n",
      "Iteration 31560 Training loss 0.018063893541693687 Validation loss 0.04237775132060051 Accuracy 0.8887500166893005\n",
      "Iteration 31570 Training loss 0.05082230269908905 Validation loss 0.06726463884115219 Accuracy 0.8268750309944153\n",
      "Iteration 31580 Training loss 0.026487762108445168 Validation loss 0.05021156370639801 Accuracy 0.8696250319480896\n",
      "Iteration 31590 Training loss 0.02401076816022396 Validation loss 0.04900378733873367 Accuracy 0.8701250553131104\n",
      "Iteration 31600 Training loss 0.0222382303327322 Validation loss 0.04535524919629097 Accuracy 0.8813750147819519\n",
      "Iteration 31610 Training loss 0.01959344558417797 Validation loss 0.0422629751265049 Accuracy 0.8882500529289246\n",
      "Iteration 31620 Training loss 0.02897108532488346 Validation loss 0.05233977735042572 Accuracy 0.8606250286102295\n",
      "Iteration 31630 Training loss 0.018921930342912674 Validation loss 0.042608655989170074 Accuracy 0.8871250152587891\n",
      "Iteration 31640 Training loss 0.023281356319785118 Validation loss 0.04664245992898941 Accuracy 0.8770000338554382\n",
      "Iteration 31650 Training loss 0.018103156238794327 Validation loss 0.0423629954457283 Accuracy 0.8876250386238098\n",
      "Iteration 31660 Training loss 0.023519951850175858 Validation loss 0.047799549996852875 Accuracy 0.8735000491142273\n",
      "Iteration 31670 Training loss 0.019810819998383522 Validation loss 0.04277120530605316 Accuracy 0.8863750696182251\n",
      "Iteration 31680 Training loss 0.017200591042637825 Validation loss 0.044365182518959045 Accuracy 0.8811250329017639\n",
      "Iteration 31690 Training loss 0.020536188036203384 Validation loss 0.0451938658952713 Accuracy 0.8797500133514404\n",
      "Iteration 31700 Training loss 0.019367165863513947 Validation loss 0.04495712369680405 Accuracy 0.8803750276565552\n",
      "Iteration 31710 Training loss 0.03351515531539917 Validation loss 0.06216559186577797 Accuracy 0.8386250138282776\n",
      "Iteration 31720 Training loss 0.0183574166148901 Validation loss 0.04337267577648163 Accuracy 0.8856250643730164\n",
      "Iteration 31730 Training loss 0.0166231207549572 Validation loss 0.045112669467926025 Accuracy 0.8812500238418579\n",
      "Iteration 31740 Training loss 0.015689946711063385 Validation loss 0.04482012242078781 Accuracy 0.8797500133514404\n",
      "Iteration 31750 Training loss 0.012206725776195526 Validation loss 0.04291795566678047 Accuracy 0.8865000605583191\n",
      "Iteration 31760 Training loss 0.016212305054068565 Validation loss 0.04744105786085129 Accuracy 0.8756250143051147\n",
      "Iteration 31770 Training loss 0.014368083328008652 Validation loss 0.0425364188849926 Accuracy 0.8877500295639038\n",
      "Iteration 31780 Training loss 0.017250657081604004 Validation loss 0.04238656908273697 Accuracy 0.8896250128746033\n",
      "Iteration 31790 Training loss 0.017373399809002876 Validation loss 0.043543312698602676 Accuracy 0.8863750696182251\n",
      "Iteration 31800 Training loss 0.017256468534469604 Validation loss 0.04533753916621208 Accuracy 0.8801250457763672\n",
      "Iteration 31810 Training loss 0.017401468008756638 Validation loss 0.04368414729833603 Accuracy 0.8840000629425049\n",
      "Iteration 31820 Training loss 0.04493822902441025 Validation loss 0.061810873448848724 Accuracy 0.8392500281333923\n",
      "Iteration 31830 Training loss 0.019239241257309914 Validation loss 0.04272178187966347 Accuracy 0.8881250619888306\n",
      "Iteration 31840 Training loss 0.021530432626605034 Validation loss 0.04343143850564957 Accuracy 0.8861250281333923\n",
      "Iteration 31850 Training loss 0.019217578694224358 Validation loss 0.04632507637143135 Accuracy 0.877375066280365\n",
      "Iteration 31860 Training loss 0.018322259187698364 Validation loss 0.04534536227583885 Accuracy 0.8808750510215759\n",
      "Iteration 31870 Training loss 0.018554111942648888 Validation loss 0.04226355254650116 Accuracy 0.8895000219345093\n",
      "Iteration 31880 Training loss 0.03318730369210243 Validation loss 0.048809681087732315 Accuracy 0.8687500357627869\n",
      "Iteration 31890 Training loss 0.021274203434586525 Validation loss 0.04314088821411133 Accuracy 0.8887500166893005\n",
      "Iteration 31900 Training loss 0.021048054099082947 Validation loss 0.042347878217697144 Accuracy 0.8886250257492065\n",
      "Iteration 31910 Training loss 0.016298789530992508 Validation loss 0.04444258287549019 Accuracy 0.8831250667572021\n",
      "Iteration 31920 Training loss 0.01485531311482191 Validation loss 0.042422059923410416 Accuracy 0.8895000219345093\n",
      "Iteration 31930 Training loss 0.023314718157052994 Validation loss 0.051254164427518845 Accuracy 0.8662500381469727\n",
      "Iteration 31940 Training loss 0.01424109935760498 Validation loss 0.043369557708501816 Accuracy 0.8871250152587891\n",
      "Iteration 31950 Training loss 0.021698150783777237 Validation loss 0.04460737481713295 Accuracy 0.8822500705718994\n",
      "Iteration 31960 Training loss 0.015368659049272537 Validation loss 0.04323451220989227 Accuracy 0.8867500424385071\n",
      "Iteration 31970 Training loss 0.01809748448431492 Validation loss 0.04289687052369118 Accuracy 0.8881250619888306\n",
      "Iteration 31980 Training loss 0.026695966720581055 Validation loss 0.048455432057380676 Accuracy 0.8716250658035278\n",
      "Iteration 31990 Training loss 0.0214132871478796 Validation loss 0.042937982827425 Accuracy 0.8875000476837158\n",
      "Iteration 32000 Training loss 0.020123835653066635 Validation loss 0.045607950538396835 Accuracy 0.878125011920929\n",
      "Iteration 32010 Training loss 0.015740418806672096 Validation loss 0.04386892542243004 Accuracy 0.8851250410079956\n",
      "Iteration 32020 Training loss 0.02045496366918087 Validation loss 0.05023077502846718 Accuracy 0.8690000176429749\n",
      "Iteration 32030 Training loss 0.024139808490872383 Validation loss 0.04622570425271988 Accuracy 0.877875030040741\n",
      "Iteration 32040 Training loss 0.015780672430992126 Validation loss 0.04352131858468056 Accuracy 0.8837500214576721\n",
      "Iteration 32050 Training loss 0.05079676955938339 Validation loss 0.06740076094865799 Accuracy 0.8267500400543213\n",
      "Iteration 32060 Training loss 0.02243986539542675 Validation loss 0.04258163645863533 Accuracy 0.8882500529289246\n",
      "Iteration 32070 Training loss 0.028683943673968315 Validation loss 0.04652781784534454 Accuracy 0.8766250610351562\n",
      "Iteration 32080 Training loss 0.018276719376444817 Validation loss 0.042528338730335236 Accuracy 0.8880000710487366\n",
      "Iteration 32090 Training loss 0.03496163710951805 Validation loss 0.05274108052253723 Accuracy 0.861750066280365\n",
      "Iteration 32100 Training loss 0.02562578208744526 Validation loss 0.04369256645441055 Accuracy 0.8842500448226929\n",
      "Iteration 32110 Training loss 0.016472503542900085 Validation loss 0.04254065081477165 Accuracy 0.8878750205039978\n",
      "Iteration 32120 Training loss 0.01791810430586338 Validation loss 0.04843147099018097 Accuracy 0.8710000514984131\n",
      "Iteration 32130 Training loss 0.014186497777700424 Validation loss 0.0423872172832489 Accuracy 0.8885000348091125\n",
      "Iteration 32140 Training loss 0.025019869208335876 Validation loss 0.05053501948714256 Accuracy 0.8655000329017639\n",
      "Iteration 32150 Training loss 0.017350422218441963 Validation loss 0.042611703276634216 Accuracy 0.8876250386238098\n",
      "Iteration 32160 Training loss 0.01424340344965458 Validation loss 0.043907612562179565 Accuracy 0.8845000267028809\n",
      "Iteration 32170 Training loss 0.02301817759871483 Validation loss 0.04223301634192467 Accuracy 0.8882500529289246\n",
      "Iteration 32180 Training loss 0.02297484315931797 Validation loss 0.044548340141773224 Accuracy 0.8816250562667847\n",
      "Iteration 32190 Training loss 0.0189561415463686 Validation loss 0.04545104131102562 Accuracy 0.8791250586509705\n",
      "Iteration 32200 Training loss 0.02141955867409706 Validation loss 0.04388798400759697 Accuracy 0.8861250281333923\n",
      "Iteration 32210 Training loss 0.02352246269583702 Validation loss 0.04255172610282898 Accuracy 0.8905000686645508\n",
      "Iteration 32220 Training loss 0.013641168363392353 Validation loss 0.04243944585323334 Accuracy 0.8868750333786011\n",
      "Iteration 32230 Training loss 0.03810928389430046 Validation loss 0.05519670620560646 Accuracy 0.8555000424385071\n",
      "Iteration 32240 Training loss 0.019173983484506607 Validation loss 0.04538685455918312 Accuracy 0.8800000548362732\n",
      "Iteration 32250 Training loss 0.01403108797967434 Validation loss 0.04250863194465637 Accuracy 0.8880000710487366\n",
      "Iteration 32260 Training loss 0.00930547434836626 Validation loss 0.04227321967482567 Accuracy 0.8883750438690186\n",
      "Iteration 32270 Training loss 0.02279004082083702 Validation loss 0.045215219259262085 Accuracy 0.8808750510215759\n",
      "Iteration 32280 Training loss 0.016636140644550323 Validation loss 0.04284151643514633 Accuracy 0.8873750567436218\n",
      "Iteration 32290 Training loss 0.02235914207994938 Validation loss 0.04725112393498421 Accuracy 0.874625027179718\n",
      "Iteration 32300 Training loss 0.019776422530412674 Validation loss 0.04301616549491882 Accuracy 0.8877500295639038\n",
      "Iteration 32310 Training loss 0.013900465331971645 Validation loss 0.04245590418577194 Accuracy 0.8891250491142273\n",
      "Iteration 32320 Training loss 0.015541746281087399 Validation loss 0.04726506397128105 Accuracy 0.874250054359436\n",
      "Iteration 32330 Training loss 0.018436679616570473 Validation loss 0.04256654530763626 Accuracy 0.8870000243186951\n",
      "Iteration 32340 Training loss 0.015987634658813477 Validation loss 0.04272202402353287 Accuracy 0.8866250514984131\n",
      "Iteration 32350 Training loss 0.038618169724941254 Validation loss 0.06540481746196747 Accuracy 0.830625057220459\n",
      "Iteration 32360 Training loss 0.021179575473070145 Validation loss 0.04932928830385208 Accuracy 0.8701250553131104\n",
      "Iteration 32370 Training loss 0.015320463106036186 Validation loss 0.042875491082668304 Accuracy 0.8875000476837158\n",
      "Iteration 32380 Training loss 0.01795976422727108 Validation loss 0.04605785757303238 Accuracy 0.8793750405311584\n",
      "Iteration 32390 Training loss 0.020243357867002487 Validation loss 0.04277467727661133 Accuracy 0.8891250491142273\n",
      "Iteration 32400 Training loss 0.026549657806754112 Validation loss 0.05742959305644035 Accuracy 0.8498750329017639\n",
      "Iteration 32410 Training loss 0.016558896750211716 Validation loss 0.0434998981654644 Accuracy 0.8876250386238098\n",
      "Iteration 32420 Training loss 0.01776432804763317 Validation loss 0.042649585753679276 Accuracy 0.8880000710487366\n",
      "Iteration 32430 Training loss 0.01806560531258583 Validation loss 0.042157988995313644 Accuracy 0.890125036239624\n",
      "Iteration 32440 Training loss 0.012055986560881138 Validation loss 0.04254556819796562 Accuracy 0.8887500166893005\n",
      "Iteration 32450 Training loss 0.0227607823908329 Validation loss 0.049996159970760345 Accuracy 0.8675000667572021\n",
      "Iteration 32460 Training loss 0.016889093443751335 Validation loss 0.04424525052309036 Accuracy 0.8830000162124634\n",
      "Iteration 32470 Training loss 0.02667694352567196 Validation loss 0.047815609723329544 Accuracy 0.8738750219345093\n",
      "Iteration 32480 Training loss 0.022059040144085884 Validation loss 0.0482957623898983 Accuracy 0.8727500438690186\n",
      "Iteration 32490 Training loss 0.02210639975965023 Validation loss 0.043299008160829544 Accuracy 0.8883750438690186\n",
      "Iteration 32500 Training loss 0.019391050562262535 Validation loss 0.045570604503154755 Accuracy 0.8808750510215759\n",
      "Iteration 32510 Training loss 0.014958121813833714 Validation loss 0.042054034769535065 Accuracy 0.89000004529953\n",
      "Iteration 32520 Training loss 0.016092855483293533 Validation loss 0.0424327552318573 Accuracy 0.8890000581741333\n",
      "Iteration 32530 Training loss 0.02323460951447487 Validation loss 0.05359724536538124 Accuracy 0.8571250438690186\n",
      "Iteration 32540 Training loss 0.009544802829623222 Validation loss 0.042954523116350174 Accuracy 0.8868750333786011\n",
      "Iteration 32550 Training loss 0.019485093653202057 Validation loss 0.04602813348174095 Accuracy 0.877625048160553\n",
      "Iteration 32560 Training loss 0.014787032268941402 Validation loss 0.0451599583029747 Accuracy 0.8827500343322754\n",
      "Iteration 32570 Training loss 0.015369105152785778 Validation loss 0.04265059158205986 Accuracy 0.8862500190734863\n",
      "Iteration 32580 Training loss 0.015189909376204014 Validation loss 0.042414482682943344 Accuracy 0.8886250257492065\n",
      "Iteration 32590 Training loss 0.024697797372937202 Validation loss 0.049742236733436584 Accuracy 0.8682500123977661\n",
      "Iteration 32600 Training loss 0.019991759210824966 Validation loss 0.04344907030463219 Accuracy 0.8865000605583191\n",
      "Iteration 32610 Training loss 0.023323258385062218 Validation loss 0.043504685163497925 Accuracy 0.8876250386238098\n",
      "Iteration 32620 Training loss 0.014589488506317139 Validation loss 0.04282236099243164 Accuracy 0.8858750462532043\n",
      "Iteration 32630 Training loss 0.018338041380047798 Validation loss 0.04600593447685242 Accuracy 0.8772500157356262\n",
      "Iteration 32640 Training loss 0.02513318881392479 Validation loss 0.051785096526145935 Accuracy 0.8656250238418579\n",
      "Iteration 32650 Training loss 0.011134151369333267 Validation loss 0.0425894632935524 Accuracy 0.8887500166893005\n",
      "Iteration 32660 Training loss 0.025516632944345474 Validation loss 0.04853205010294914 Accuracy 0.8698750138282776\n",
      "Iteration 32670 Training loss 0.02331853285431862 Validation loss 0.0501902811229229 Accuracy 0.8655000329017639\n",
      "Iteration 32680 Training loss 0.017341289669275284 Validation loss 0.04336051270365715 Accuracy 0.8862500190734863\n",
      "Iteration 32690 Training loss 0.012616675347089767 Validation loss 0.043301649391651154 Accuracy 0.8853750228881836\n",
      "Iteration 32700 Training loss 0.030422205105423927 Validation loss 0.06025156006217003 Accuracy 0.843000054359436\n",
      "Iteration 32710 Training loss 0.010822713375091553 Validation loss 0.042390961199998856 Accuracy 0.8875000476837158\n",
      "Iteration 32720 Training loss 0.015446583740413189 Validation loss 0.042427826672792435 Accuracy 0.8890000581741333\n",
      "Iteration 32730 Training loss 0.05332307517528534 Validation loss 0.06960532814264297 Accuracy 0.8231250643730164\n",
      "Iteration 32740 Training loss 0.014157865196466446 Validation loss 0.04234103113412857 Accuracy 0.8887500166893005\n",
      "Iteration 32750 Training loss 0.016064943745732307 Validation loss 0.042191606014966965 Accuracy 0.8890000581741333\n",
      "Iteration 32760 Training loss 0.01917348802089691 Validation loss 0.04830622673034668 Accuracy 0.8728750348091125\n",
      "Iteration 32770 Training loss 0.01362719014286995 Validation loss 0.04411764815449715 Accuracy 0.8841250538825989\n",
      "Iteration 32780 Training loss 0.011135018430650234 Validation loss 0.04266911745071411 Accuracy 0.8871250152587891\n",
      "Iteration 32790 Training loss 0.014564622193574905 Validation loss 0.04300669953227043 Accuracy 0.8865000605583191\n",
      "Iteration 32800 Training loss 0.02430487982928753 Validation loss 0.0422905758023262 Accuracy 0.8911250233650208\n",
      "Iteration 32810 Training loss 0.011999930255115032 Validation loss 0.04225537180900574 Accuracy 0.8885000348091125\n",
      "Iteration 32820 Training loss 0.01740441285073757 Validation loss 0.04213550314307213 Accuracy 0.890375018119812\n",
      "Iteration 32830 Training loss 0.04313932731747627 Validation loss 0.06176338344812393 Accuracy 0.8395000696182251\n",
      "Iteration 32840 Training loss 0.02168632298707962 Validation loss 0.04450308531522751 Accuracy 0.8836250305175781\n",
      "Iteration 32850 Training loss 0.017632052302360535 Validation loss 0.04209645465016365 Accuracy 0.89000004529953\n",
      "Iteration 32860 Training loss 0.02081112377345562 Validation loss 0.04735010862350464 Accuracy 0.874125063419342\n",
      "Iteration 32870 Training loss 0.02102883718907833 Validation loss 0.04456871375441551 Accuracy 0.8810000419616699\n",
      "Iteration 32880 Training loss 0.01277408841997385 Validation loss 0.043591003865003586 Accuracy 0.8862500190734863\n",
      "Iteration 32890 Training loss 0.04289441928267479 Validation loss 0.059308961033821106 Accuracy 0.8453750610351562\n",
      "Iteration 32900 Training loss 0.018515268340706825 Validation loss 0.04419854283332825 Accuracy 0.8837500214576721\n",
      "Iteration 32910 Training loss 0.015532910823822021 Validation loss 0.04295488819479942 Accuracy 0.8878750205039978\n",
      "Iteration 32920 Training loss 0.01922953501343727 Validation loss 0.04299640282988548 Accuracy 0.8862500190734863\n",
      "Iteration 32930 Training loss 0.01713663339614868 Validation loss 0.045619167387485504 Accuracy 0.8791250586509705\n",
      "Iteration 32940 Training loss 0.028822144493460655 Validation loss 0.050921715795993805 Accuracy 0.8643750548362732\n",
      "Iteration 32950 Training loss 0.02340894564986229 Validation loss 0.04897141456604004 Accuracy 0.8725000619888306\n",
      "Iteration 32960 Training loss 0.02050032652914524 Validation loss 0.042687274515628815 Accuracy 0.8876250386238098\n",
      "Iteration 32970 Training loss 0.019048824906349182 Validation loss 0.04236196354031563 Accuracy 0.8885000348091125\n",
      "Iteration 32980 Training loss 0.02258411794900894 Validation loss 0.046088237315416336 Accuracy 0.8783750534057617\n",
      "Iteration 32990 Training loss 0.026111798360943794 Validation loss 0.047148264944553375 Accuracy 0.8750000596046448\n",
      "Iteration 33000 Training loss 0.015598958358168602 Validation loss 0.04821639508008957 Accuracy 0.8730000257492065\n",
      "Iteration 33010 Training loss 0.015788555145263672 Validation loss 0.042709507048130035 Accuracy 0.8885000348091125\n",
      "Iteration 33020 Training loss 0.019739098846912384 Validation loss 0.04422999173402786 Accuracy 0.8837500214576721\n",
      "Iteration 33030 Training loss 0.01971224695444107 Validation loss 0.0482405349612236 Accuracy 0.8721250295639038\n",
      "Iteration 33040 Training loss 0.020480114966630936 Validation loss 0.04438938945531845 Accuracy 0.8831250667572021\n",
      "Iteration 33050 Training loss 0.01834515482187271 Validation loss 0.042737122625112534 Accuracy 0.8880000710487366\n",
      "Iteration 33060 Training loss 0.01737643964588642 Validation loss 0.04333648830652237 Accuracy 0.8863750696182251\n",
      "Iteration 33070 Training loss 0.019461192190647125 Validation loss 0.04829200357198715 Accuracy 0.8697500228881836\n",
      "Iteration 33080 Training loss 0.03497743606567383 Validation loss 0.053604334592819214 Accuracy 0.859000027179718\n",
      "Iteration 33090 Training loss 0.017842557281255722 Validation loss 0.04231707379221916 Accuracy 0.8896250128746033\n",
      "Iteration 33100 Training loss 0.02153646945953369 Validation loss 0.043125949800014496 Accuracy 0.8866250514984131\n",
      "Iteration 33110 Training loss 0.023573368787765503 Validation loss 0.051470063626766205 Accuracy 0.8628750443458557\n",
      "Iteration 33120 Training loss 0.017046751454472542 Validation loss 0.04396148771047592 Accuracy 0.8833750486373901\n",
      "Iteration 33130 Training loss 0.017218202352523804 Validation loss 0.04434526339173317 Accuracy 0.8826250433921814\n",
      "Iteration 33140 Training loss 0.01675320416688919 Validation loss 0.04223642870783806 Accuracy 0.89000004529953\n",
      "Iteration 33150 Training loss 0.015990493819117546 Validation loss 0.04274391382932663 Accuracy 0.8883750438690186\n",
      "Iteration 33160 Training loss 0.017206287011504173 Validation loss 0.04762354493141174 Accuracy 0.874125063419342\n",
      "Iteration 33170 Training loss 0.024312172085046768 Validation loss 0.04973578080534935 Accuracy 0.8687500357627869\n",
      "Iteration 33180 Training loss 0.011689577251672745 Validation loss 0.04330197349190712 Accuracy 0.8872500658035278\n",
      "Iteration 33190 Training loss 0.013725433498620987 Validation loss 0.042438752949237823 Accuracy 0.8886250257492065\n",
      "Iteration 33200 Training loss 0.018382662907242775 Validation loss 0.04668024182319641 Accuracy 0.8756250143051147\n",
      "Iteration 33210 Training loss 0.027989694848656654 Validation loss 0.048992034047842026 Accuracy 0.8713750243186951\n",
      "Iteration 33220 Training loss 0.02219238691031933 Validation loss 0.04244882985949516 Accuracy 0.8888750672340393\n",
      "Iteration 33230 Training loss 0.011816170066595078 Validation loss 0.042356789112091064 Accuracy 0.89000004529953\n",
      "Iteration 33240 Training loss 0.04371500760316849 Validation loss 0.06214122846722603 Accuracy 0.8402500152587891\n",
      "Iteration 33250 Training loss 0.020050300285220146 Validation loss 0.044683974236249924 Accuracy 0.8806250691413879\n",
      "Iteration 33260 Training loss 0.012955131009221077 Validation loss 0.043134622275829315 Accuracy 0.8873750567436218\n",
      "Iteration 33270 Training loss 0.02513185702264309 Validation loss 0.04312576353549957 Accuracy 0.8868750333786011\n",
      "Iteration 33280 Training loss 0.04224676638841629 Validation loss 0.058598581701517105 Accuracy 0.846250057220459\n",
      "Iteration 33290 Training loss 0.0205796267837286 Validation loss 0.04323919862508774 Accuracy 0.8855000138282776\n",
      "Iteration 33300 Training loss 0.02072405442595482 Validation loss 0.04682467132806778 Accuracy 0.8766250610351562\n",
      "Iteration 33310 Training loss 0.03787347301840782 Validation loss 0.06859193742275238 Accuracy 0.8251250386238098\n",
      "Iteration 33320 Training loss 0.021591586992144585 Validation loss 0.04260970652103424 Accuracy 0.8885000348091125\n",
      "Iteration 33330 Training loss 0.01994619518518448 Validation loss 0.052183397114276886 Accuracy 0.8613750338554382\n",
      "Iteration 33340 Training loss 0.01915380358695984 Validation loss 0.0429295152425766 Accuracy 0.8876250386238098\n",
      "Iteration 33350 Training loss 0.0120957987383008 Validation loss 0.042553652077913284 Accuracy 0.8893750309944153\n",
      "Iteration 33360 Training loss 0.040178488940000534 Validation loss 0.06325142085552216 Accuracy 0.8371250629425049\n",
      "Iteration 33370 Training loss 0.01800566166639328 Validation loss 0.04636550322175026 Accuracy 0.8788750171661377\n",
      "Iteration 33380 Training loss 0.01822677068412304 Validation loss 0.04759976267814636 Accuracy 0.8748750686645508\n",
      "Iteration 33390 Training loss 0.025947140529751778 Validation loss 0.043794017285108566 Accuracy 0.8857500553131104\n",
      "Iteration 33400 Training loss 0.016807781532406807 Validation loss 0.04246855527162552 Accuracy 0.8887500166893005\n",
      "Iteration 33410 Training loss 0.019353032112121582 Validation loss 0.04241577535867691 Accuracy 0.8891250491142273\n",
      "Iteration 33420 Training loss 0.0184527225792408 Validation loss 0.04844771698117256 Accuracy 0.8720000386238098\n",
      "Iteration 33430 Training loss 0.01681903749704361 Validation loss 0.04379705339670181 Accuracy 0.8843750357627869\n",
      "Iteration 33440 Training loss 0.0230485200881958 Validation loss 0.050288308411836624 Accuracy 0.8711250424385071\n",
      "Iteration 33450 Training loss 0.014884483069181442 Validation loss 0.043234843760728836 Accuracy 0.8862500190734863\n",
      "Iteration 33460 Training loss 0.02965403161942959 Validation loss 0.04898443818092346 Accuracy 0.8708750605583191\n",
      "Iteration 33470 Training loss 0.018023712560534477 Validation loss 0.04299722984433174 Accuracy 0.8881250619888306\n",
      "Iteration 33480 Training loss 0.01713465340435505 Validation loss 0.042455583810806274 Accuracy 0.8887500166893005\n",
      "Iteration 33490 Training loss 0.018849318847060204 Validation loss 0.049084946513175964 Accuracy 0.8707500696182251\n",
      "Iteration 33500 Training loss 0.02489357441663742 Validation loss 0.04629294201731682 Accuracy 0.877750039100647\n",
      "Iteration 33510 Training loss 0.02655147574841976 Validation loss 0.05156337097287178 Accuracy 0.8630000352859497\n",
      "Iteration 33520 Training loss 0.013796036131680012 Validation loss 0.04227238893508911 Accuracy 0.8907500505447388\n",
      "Iteration 33530 Training loss 0.018571797758340836 Validation loss 0.042579811066389084 Accuracy 0.890250027179718\n",
      "Iteration 33540 Training loss 0.017585203051567078 Validation loss 0.04378663748502731 Accuracy 0.8843750357627869\n",
      "Iteration 33550 Training loss 0.03225468471646309 Validation loss 0.055879686027765274 Accuracy 0.8548750281333923\n",
      "Iteration 33560 Training loss 0.013157175853848457 Validation loss 0.043134987354278564 Accuracy 0.8855000138282776\n",
      "Iteration 33570 Training loss 0.014246703125536442 Validation loss 0.04291678965091705 Accuracy 0.8871250152587891\n",
      "Iteration 33580 Training loss 0.016116272658109665 Validation loss 0.04377751424908638 Accuracy 0.8843750357627869\n",
      "Iteration 33590 Training loss 0.013678614981472492 Validation loss 0.04340893030166626 Accuracy 0.8862500190734863\n",
      "Iteration 33600 Training loss 0.024701392278075218 Validation loss 0.05101232975721359 Accuracy 0.8653750419616699\n",
      "Iteration 33610 Training loss 0.02062384970486164 Validation loss 0.046084389090538025 Accuracy 0.8815000653266907\n",
      "Iteration 33620 Training loss 0.018425889313220978 Validation loss 0.04318220540881157 Accuracy 0.8872500658035278\n",
      "Iteration 33630 Training loss 0.015542417764663696 Validation loss 0.04438804090023041 Accuracy 0.8821250200271606\n",
      "Iteration 33640 Training loss 0.018123287707567215 Validation loss 0.042930543422698975 Accuracy 0.8860000371932983\n",
      "Iteration 33650 Training loss 0.016849078238010406 Validation loss 0.04474406689405441 Accuracy 0.8825000524520874\n",
      "Iteration 33660 Training loss 0.0158059261739254 Validation loss 0.04300966113805771 Accuracy 0.8872500658035278\n",
      "Iteration 33670 Training loss 0.017849184572696686 Validation loss 0.04386609047651291 Accuracy 0.8845000267028809\n",
      "Iteration 33680 Training loss 0.019096195697784424 Validation loss 0.0426948107779026 Accuracy 0.8888750672340393\n",
      "Iteration 33690 Training loss 0.02261759154498577 Validation loss 0.04798947274684906 Accuracy 0.8750000596046448\n",
      "Iteration 33700 Training loss 0.010350389406085014 Validation loss 0.04235871508717537 Accuracy 0.8896250128746033\n",
      "Iteration 33710 Training loss 0.026505839079618454 Validation loss 0.0516948327422142 Accuracy 0.8635000586509705\n",
      "Iteration 33720 Training loss 0.022591928020119667 Validation loss 0.04363842308521271 Accuracy 0.8842500448226929\n",
      "Iteration 33730 Training loss 0.026011858135461807 Validation loss 0.045700833201408386 Accuracy 0.8793750405311584\n",
      "Iteration 33740 Training loss 0.015368998982012272 Validation loss 0.04362023249268532 Accuracy 0.8845000267028809\n",
      "Iteration 33750 Training loss 0.02200201526284218 Validation loss 0.0479440875351429 Accuracy 0.8727500438690186\n",
      "Iteration 33760 Training loss 0.019239872694015503 Validation loss 0.046107638627290726 Accuracy 0.877875030040741\n",
      "Iteration 33770 Training loss 0.02115771919488907 Validation loss 0.044155169278383255 Accuracy 0.8837500214576721\n",
      "Iteration 33780 Training loss 0.032526422291994095 Validation loss 0.058857034891843796 Accuracy 0.8441250324249268\n",
      "Iteration 33790 Training loss 0.01747559942305088 Validation loss 0.04352011904120445 Accuracy 0.8855000138282776\n",
      "Iteration 33800 Training loss 0.021843990311026573 Validation loss 0.047759875655174255 Accuracy 0.8755000233650208\n",
      "Iteration 33810 Training loss 0.02046259306371212 Validation loss 0.04475589841604233 Accuracy 0.8827500343322754\n",
      "Iteration 33820 Training loss 0.02299448475241661 Validation loss 0.044798970222473145 Accuracy 0.8822500705718994\n",
      "Iteration 33830 Training loss 0.012387384660542011 Validation loss 0.042900606989860535 Accuracy 0.8875000476837158\n",
      "Iteration 33840 Training loss 0.022651957347989082 Validation loss 0.04280532896518707 Accuracy 0.8891250491142273\n",
      "Iteration 33850 Training loss 0.01740926504135132 Validation loss 0.04340817406773567 Accuracy 0.8861250281333923\n",
      "Iteration 33860 Training loss 0.01697755604982376 Validation loss 0.042345356196165085 Accuracy 0.8906250596046448\n",
      "Iteration 33870 Training loss 0.020292574539780617 Validation loss 0.0471503809094429 Accuracy 0.8755000233650208\n",
      "Iteration 33880 Training loss 0.017399080097675323 Validation loss 0.044731006026268005 Accuracy 0.8818750381469727\n",
      "Iteration 33890 Training loss 0.026314005255699158 Validation loss 0.05015238747000694 Accuracy 0.8701250553131104\n",
      "Iteration 33900 Training loss 0.014749664813280106 Validation loss 0.04267193749547005 Accuracy 0.8878750205039978\n",
      "Iteration 33910 Training loss 0.012709843926131725 Validation loss 0.04277429357171059 Accuracy 0.8881250619888306\n",
      "Iteration 33920 Training loss 0.020212123170495033 Validation loss 0.04694075137376785 Accuracy 0.8761250376701355\n",
      "Iteration 33930 Training loss 0.020851509645581245 Validation loss 0.04743378609418869 Accuracy 0.874500036239624\n",
      "Iteration 33940 Training loss 0.02118738181889057 Validation loss 0.051662877202034 Accuracy 0.8660000562667847\n",
      "Iteration 33950 Training loss 0.012049948796629906 Validation loss 0.043052103370428085 Accuracy 0.8890000581741333\n",
      "Iteration 33960 Training loss 0.02194872871041298 Validation loss 0.048856470733881 Accuracy 0.8691250681877136\n",
      "Iteration 33970 Training loss 0.024976694956421852 Validation loss 0.044994957745075226 Accuracy 0.8826250433921814\n",
      "Iteration 33980 Training loss 0.020233983173966408 Validation loss 0.04690098762512207 Accuracy 0.8763750195503235\n",
      "Iteration 33990 Training loss 0.02232350781559944 Validation loss 0.04846363514661789 Accuracy 0.8698750138282776\n",
      "Iteration 34000 Training loss 0.020083989948034286 Validation loss 0.04386882111430168 Accuracy 0.8845000267028809\n",
      "Iteration 34010 Training loss 0.013733206316828728 Validation loss 0.0426548570394516 Accuracy 0.8885000348091125\n",
      "Iteration 34020 Training loss 0.018511973321437836 Validation loss 0.04424053430557251 Accuracy 0.8837500214576721\n",
      "Iteration 34030 Training loss 0.015205733478069305 Validation loss 0.04258771985769272 Accuracy 0.8891250491142273\n",
      "Iteration 34040 Training loss 0.014213459566235542 Validation loss 0.04512845352292061 Accuracy 0.8827500343322754\n",
      "Iteration 34050 Training loss 0.018453577533364296 Validation loss 0.04335475340485573 Accuracy 0.8870000243186951\n",
      "Iteration 34060 Training loss 0.009491232223808765 Validation loss 0.042691849172115326 Accuracy 0.8888750672340393\n",
      "Iteration 34070 Training loss 0.016847074031829834 Validation loss 0.043090879917144775 Accuracy 0.8867500424385071\n",
      "Iteration 34080 Training loss 0.01631173864006996 Validation loss 0.04323801398277283 Accuracy 0.8866250514984131\n",
      "Iteration 34090 Training loss 0.02361331321299076 Validation loss 0.04717479273676872 Accuracy 0.8757500648498535\n",
      "Iteration 34100 Training loss 0.037946783006191254 Validation loss 0.06686606258153915 Accuracy 0.8257500529289246\n",
      "Iteration 34110 Training loss 0.012879328802227974 Validation loss 0.042625702917575836 Accuracy 0.8878750205039978\n",
      "Iteration 34120 Training loss 0.01251886785030365 Validation loss 0.04260609671473503 Accuracy 0.8883750438690186\n",
      "Iteration 34130 Training loss 0.016371069476008415 Validation loss 0.04519883170723915 Accuracy 0.8818750381469727\n",
      "Iteration 34140 Training loss 0.04437584429979324 Validation loss 0.07028436660766602 Accuracy 0.8200000524520874\n",
      "Iteration 34150 Training loss 0.014529176987707615 Validation loss 0.04394586384296417 Accuracy 0.8851250410079956\n",
      "Iteration 34160 Training loss 0.01157014723867178 Validation loss 0.043540194630622864 Accuracy 0.8851250410079956\n",
      "Iteration 34170 Training loss 0.009576473385095596 Validation loss 0.04270833730697632 Accuracy 0.8868750333786011\n",
      "Iteration 34180 Training loss 0.015148813836276531 Validation loss 0.04279005900025368 Accuracy 0.8890000581741333\n",
      "Iteration 34190 Training loss 0.02513991855084896 Validation loss 0.04293329641222954 Accuracy 0.8866250514984131\n",
      "Iteration 34200 Training loss 0.051852747797966 Validation loss 0.07295549660921097 Accuracy 0.8146250247955322\n",
      "Iteration 34210 Training loss 0.012434825301170349 Validation loss 0.04293719679117203 Accuracy 0.8881250619888306\n",
      "Iteration 34220 Training loss 0.02028755471110344 Validation loss 0.04318930208683014 Accuracy 0.8860000371932983\n",
      "Iteration 34230 Training loss 0.027080459520220757 Validation loss 0.0502450130879879 Accuracy 0.8678750395774841\n",
      "Iteration 34240 Training loss 0.026412080973386765 Validation loss 0.04254259541630745 Accuracy 0.8888750672340393\n",
      "Iteration 34250 Training loss 0.014968888834118843 Validation loss 0.04278681427240372 Accuracy 0.8880000710487366\n",
      "Iteration 34260 Training loss 0.017211155965924263 Validation loss 0.04674072563648224 Accuracy 0.8755000233650208\n",
      "Iteration 34270 Training loss 0.016951274126768112 Validation loss 0.042632605880498886 Accuracy 0.8871250152587891\n",
      "Iteration 34280 Training loss 0.020954446867108345 Validation loss 0.047096431255340576 Accuracy 0.8767500519752502\n",
      "Iteration 34290 Training loss 0.020740045234560966 Validation loss 0.04466267675161362 Accuracy 0.8842500448226929\n",
      "Iteration 34300 Training loss 0.022533992305397987 Validation loss 0.045110926032066345 Accuracy 0.8810000419616699\n",
      "Iteration 34310 Training loss 0.015366503968834877 Validation loss 0.04395028576254845 Accuracy 0.8851250410079956\n",
      "Iteration 34320 Training loss 0.01805894635617733 Validation loss 0.04390823468565941 Accuracy 0.8840000629425049\n",
      "Iteration 34330 Training loss 0.06234126538038254 Validation loss 0.07870722562074661 Accuracy 0.8026250600814819\n",
      "Iteration 34340 Training loss 0.023077992722392082 Validation loss 0.048586953431367874 Accuracy 0.8717500567436218\n",
      "Iteration 34350 Training loss 0.03420789912343025 Validation loss 0.049086444079875946 Accuracy 0.8718750476837158\n",
      "Iteration 34360 Training loss 0.015128286555409431 Validation loss 0.04311663284897804 Accuracy 0.8855000138282776\n",
      "Iteration 34370 Training loss 0.019516319036483765 Validation loss 0.043622080236673355 Accuracy 0.8857500553131104\n",
      "Iteration 34380 Training loss 0.019708354026079178 Validation loss 0.04680555313825607 Accuracy 0.8801250457763672\n",
      "Iteration 34390 Training loss 0.02016988769173622 Validation loss 0.04508315026760101 Accuracy 0.8808750510215759\n",
      "Iteration 34400 Training loss 0.015970248728990555 Validation loss 0.04299516603350639 Accuracy 0.8878750205039978\n",
      "Iteration 34410 Training loss 0.010113515891134739 Validation loss 0.04247249290347099 Accuracy 0.889750063419342\n",
      "Iteration 34420 Training loss 0.016257338225841522 Validation loss 0.042387790977954865 Accuracy 0.8893750309944153\n",
      "Iteration 34430 Training loss 0.026631204411387444 Validation loss 0.049520500004291534 Accuracy 0.8690000176429749\n",
      "Iteration 34440 Training loss 0.013157756999135017 Validation loss 0.04435383528470993 Accuracy 0.8843750357627869\n",
      "Iteration 34450 Training loss 0.016324948519468307 Validation loss 0.042369578033685684 Accuracy 0.8882500529289246\n",
      "Iteration 34460 Training loss 0.015361743047833443 Validation loss 0.04370385780930519 Accuracy 0.8860000371932983\n",
      "Iteration 34470 Training loss 0.012671038508415222 Validation loss 0.043216679245233536 Accuracy 0.8873750567436218\n",
      "Iteration 34480 Training loss 0.011168192140758038 Validation loss 0.042327042669057846 Accuracy 0.889750063419342\n",
      "Iteration 34490 Training loss 0.01879669539630413 Validation loss 0.04275882989168167 Accuracy 0.8882500529289246\n",
      "Iteration 34500 Training loss 0.015840651467442513 Validation loss 0.04254179075360298 Accuracy 0.8886250257492065\n",
      "Iteration 34510 Training loss 0.013523750007152557 Validation loss 0.04339222237467766 Accuracy 0.8871250152587891\n",
      "Iteration 34520 Training loss 0.01569826528429985 Validation loss 0.04364261031150818 Accuracy 0.8853750228881836\n",
      "Iteration 34530 Training loss 0.01579565741121769 Validation loss 0.04324658587574959 Accuracy 0.8865000605583191\n",
      "Iteration 34540 Training loss 0.012532238848507404 Validation loss 0.04553060978651047 Accuracy 0.8796250224113464\n",
      "Iteration 34550 Training loss 0.01257697120308876 Validation loss 0.044432755559682846 Accuracy 0.8841250538825989\n",
      "Iteration 34560 Training loss 0.01918288692831993 Validation loss 0.042515940964221954 Accuracy 0.8906250596046448\n",
      "Iteration 34570 Training loss 0.009329701773822308 Validation loss 0.04239886626601219 Accuracy 0.8893750309944153\n",
      "Iteration 34580 Training loss 0.016446193680167198 Validation loss 0.04324817657470703 Accuracy 0.8862500190734863\n",
      "Iteration 34590 Training loss 0.021264582872390747 Validation loss 0.04526893049478531 Accuracy 0.8838750123977661\n",
      "Iteration 34600 Training loss 0.012765116058290005 Validation loss 0.042611442506313324 Accuracy 0.8895000219345093\n",
      "Iteration 34610 Training loss 0.06307071447372437 Validation loss 0.07208838313817978 Accuracy 0.8162500262260437\n",
      "Iteration 34620 Training loss 0.013246526010334492 Validation loss 0.04280225560069084 Accuracy 0.8883750438690186\n",
      "Iteration 34630 Training loss 0.015048546716570854 Validation loss 0.04605985805392265 Accuracy 0.8790000677108765\n",
      "Iteration 34640 Training loss 0.02395208738744259 Validation loss 0.050083745270967484 Accuracy 0.8692500591278076\n",
      "Iteration 34650 Training loss 0.012604951858520508 Validation loss 0.04348169267177582 Accuracy 0.8870000243186951\n",
      "Iteration 34660 Training loss 0.026461409404873848 Validation loss 0.043434109538793564 Accuracy 0.8855000138282776\n",
      "Iteration 34670 Training loss 0.015522615984082222 Validation loss 0.04325682297348976 Accuracy 0.8865000605583191\n",
      "Iteration 34680 Training loss 0.023400263860821724 Validation loss 0.050773654133081436 Accuracy 0.8661250472068787\n",
      "Iteration 34690 Training loss 0.0243357103317976 Validation loss 0.048397213220596313 Accuracy 0.8720000386238098\n",
      "Iteration 34700 Training loss 0.01117328368127346 Validation loss 0.04263816401362419 Accuracy 0.8886250257492065\n",
      "Iteration 34710 Training loss 0.017697976902127266 Validation loss 0.048128772526979446 Accuracy 0.8738750219345093\n",
      "Iteration 34720 Training loss 0.015507005155086517 Validation loss 0.043628569692373276 Accuracy 0.8863750696182251\n",
      "Iteration 34730 Training loss 0.010752956382930279 Validation loss 0.043221428990364075 Accuracy 0.8867500424385071\n",
      "Iteration 34740 Training loss 0.016846010461449623 Validation loss 0.04312076047062874 Accuracy 0.8870000243186951\n",
      "Iteration 34750 Training loss 0.01075468398630619 Validation loss 0.04338585212826729 Accuracy 0.8865000605583191\n",
      "Iteration 34760 Training loss 0.011708552949130535 Validation loss 0.042831726372241974 Accuracy 0.8893750309944153\n",
      "Iteration 34770 Training loss 0.013344276696443558 Validation loss 0.04689112305641174 Accuracy 0.8753750324249268\n",
      "Iteration 34780 Training loss 0.0167300533503294 Validation loss 0.04341532289981842 Accuracy 0.8861250281333923\n",
      "Iteration 34790 Training loss 0.012651264667510986 Validation loss 0.04256162792444229 Accuracy 0.8895000219345093\n",
      "Iteration 34800 Training loss 0.04821281507611275 Validation loss 0.06325820833444595 Accuracy 0.8402500152587891\n",
      "Iteration 34810 Training loss 0.017946016043424606 Validation loss 0.04322946444153786 Accuracy 0.8876250386238098\n",
      "Iteration 34820 Training loss 0.012103728018701077 Validation loss 0.04257611185312271 Accuracy 0.8906250596046448\n",
      "Iteration 34830 Training loss 0.0136763546615839 Validation loss 0.04275117814540863 Accuracy 0.8885000348091125\n",
      "Iteration 34840 Training loss 0.02881801128387451 Validation loss 0.05096830055117607 Accuracy 0.8675000667572021\n",
      "Iteration 34850 Training loss 0.013975434936583042 Validation loss 0.04417257755994797 Accuracy 0.8831250667572021\n",
      "Iteration 34860 Training loss 0.015117799863219261 Validation loss 0.04294831305742264 Accuracy 0.8887500166893005\n",
      "Iteration 34870 Training loss 0.007859727367758751 Validation loss 0.04269692301750183 Accuracy 0.8872500658035278\n",
      "Iteration 34880 Training loss 0.013853030279278755 Validation loss 0.04250174015760422 Accuracy 0.8890000581741333\n",
      "Iteration 34890 Training loss 0.04214278236031532 Validation loss 0.06771451979875565 Accuracy 0.8290000557899475\n",
      "Iteration 34900 Training loss 0.00958123430609703 Validation loss 0.042726028710603714 Accuracy 0.8883750438690186\n",
      "Iteration 34910 Training loss 0.012595225125551224 Validation loss 0.04285803809762001 Accuracy 0.8876250386238098\n",
      "Iteration 34920 Training loss 0.015968162566423416 Validation loss 0.04312833771109581 Accuracy 0.8860000371932983\n",
      "Iteration 34930 Training loss 0.017774470150470734 Validation loss 0.044813744723796844 Accuracy 0.8837500214576721\n",
      "Iteration 34940 Training loss 0.016549810767173767 Validation loss 0.04242188483476639 Accuracy 0.8895000219345093\n",
      "Iteration 34950 Training loss 0.01769397221505642 Validation loss 0.046142589300870895 Accuracy 0.877375066280365\n",
      "Iteration 34960 Training loss 0.010191327892243862 Validation loss 0.04262187331914902 Accuracy 0.8892500400543213\n",
      "Iteration 34970 Training loss 0.01606765016913414 Validation loss 0.04393057897686958 Accuracy 0.8840000629425049\n",
      "Iteration 34980 Training loss 0.016767974942922592 Validation loss 0.04515683278441429 Accuracy 0.8822500705718994\n",
      "Iteration 34990 Training loss 0.020172810181975365 Validation loss 0.0456673689186573 Accuracy 0.8797500133514404\n",
      "Iteration 35000 Training loss 0.01794700138270855 Validation loss 0.04328061640262604 Accuracy 0.8847500681877136\n",
      "Iteration 35010 Training loss 0.017706483602523804 Validation loss 0.04400092363357544 Accuracy 0.8852500319480896\n",
      "Iteration 35020 Training loss 0.013486900366842747 Validation loss 0.04507802799344063 Accuracy 0.8815000653266907\n",
      "Iteration 35030 Training loss 0.018561651930212975 Validation loss 0.04768773913383484 Accuracy 0.874625027179718\n",
      "Iteration 35040 Training loss 0.018245894461870193 Validation loss 0.044562533497810364 Accuracy 0.8835000395774841\n",
      "Iteration 35050 Training loss 0.0180999506264925 Validation loss 0.04682786762714386 Accuracy 0.877500057220459\n",
      "Iteration 35060 Training loss 0.017383072525262833 Validation loss 0.04347795248031616 Accuracy 0.8852500319480896\n",
      "Iteration 35070 Training loss 0.05182486027479172 Validation loss 0.069403275847435 Accuracy 0.8215000629425049\n",
      "Iteration 35080 Training loss 0.012865413911640644 Validation loss 0.04684063792228699 Accuracy 0.8770000338554382\n",
      "Iteration 35090 Training loss 0.01144468318670988 Validation loss 0.04260531812906265 Accuracy 0.890125036239624\n",
      "Iteration 35100 Training loss 0.03193093091249466 Validation loss 0.0585576631128788 Accuracy 0.8446250557899475\n",
      "Iteration 35110 Training loss 0.010178081691265106 Validation loss 0.04444461315870285 Accuracy 0.8843750357627869\n",
      "Iteration 35120 Training loss 0.01989869959652424 Validation loss 0.045034486800432205 Accuracy 0.8790000677108765\n",
      "Iteration 35130 Training loss 0.012818705290555954 Validation loss 0.04366389662027359 Accuracy 0.8853750228881836\n",
      "Iteration 35140 Training loss 0.02058737725019455 Validation loss 0.05099155381321907 Accuracy 0.8682500123977661\n",
      "Iteration 35150 Training loss 0.007191094104200602 Validation loss 0.04258378595113754 Accuracy 0.8885000348091125\n",
      "Iteration 35160 Training loss 0.0359712578356266 Validation loss 0.05486920475959778 Accuracy 0.8576250672340393\n",
      "Iteration 35170 Training loss 0.015475478023290634 Validation loss 0.0425291545689106 Accuracy 0.889750063419342\n",
      "Iteration 35180 Training loss 0.013729684054851532 Validation loss 0.04237836226820946 Accuracy 0.889875054359436\n",
      "Iteration 35190 Training loss 0.015621245838701725 Validation loss 0.046871770173311234 Accuracy 0.8751250505447388\n",
      "Iteration 35200 Training loss 0.018828149884939194 Validation loss 0.04712231084704399 Accuracy 0.8753750324249268\n",
      "Iteration 35210 Training loss 0.009831228293478489 Validation loss 0.04295014217495918 Accuracy 0.8873750567436218\n",
      "Iteration 35220 Training loss 0.011959380470216274 Validation loss 0.042804054915905 Accuracy 0.8866250514984131\n",
      "Iteration 35230 Training loss 0.02043396420776844 Validation loss 0.04969389736652374 Accuracy 0.8708750605583191\n",
      "Iteration 35240 Training loss 0.016827547922730446 Validation loss 0.04535619169473648 Accuracy 0.8836250305175781\n",
      "Iteration 35250 Training loss 0.021571435034275055 Validation loss 0.04937461391091347 Accuracy 0.8711250424385071\n",
      "Iteration 35260 Training loss 0.016053268685936928 Validation loss 0.043322786688804626 Accuracy 0.8857500553131104\n",
      "Iteration 35270 Training loss 0.01959029585123062 Validation loss 0.04285094887018204 Accuracy 0.8875000476837158\n",
      "Iteration 35280 Training loss 0.01084726769477129 Validation loss 0.042262498289346695 Accuracy 0.8891250491142273\n",
      "Iteration 35290 Training loss 0.023062588647007942 Validation loss 0.05035286396741867 Accuracy 0.8667500615119934\n",
      "Iteration 35300 Training loss 0.022591346874833107 Validation loss 0.05231594294309616 Accuracy 0.8646250367164612\n",
      "Iteration 35310 Training loss 0.037309542298316956 Validation loss 0.04883440211415291 Accuracy 0.8736250400543213\n",
      "Iteration 35320 Training loss 0.022056663408875465 Validation loss 0.042415834963321686 Accuracy 0.89000004529953\n",
      "Iteration 35330 Training loss 0.01592395454645157 Validation loss 0.04245087876915932 Accuracy 0.889875054359436\n",
      "Iteration 35340 Training loss 0.047228146344423294 Validation loss 0.06159408763051033 Accuracy 0.8421250581741333\n",
      "Iteration 35350 Training loss 0.01157286111265421 Validation loss 0.04478510469198227 Accuracy 0.8827500343322754\n",
      "Iteration 35360 Training loss 0.014823067933321 Validation loss 0.04325408115983009 Accuracy 0.8867500424385071\n",
      "Iteration 35370 Training loss 0.011402287520468235 Validation loss 0.04241662845015526 Accuracy 0.8906250596046448\n",
      "Iteration 35380 Training loss 0.03565659746527672 Validation loss 0.05982320010662079 Accuracy 0.846250057220459\n",
      "Iteration 35390 Training loss 0.024507960304617882 Validation loss 0.04568709805607796 Accuracy 0.8803750276565552\n",
      "Iteration 35400 Training loss 0.013519336469471455 Validation loss 0.04245329648256302 Accuracy 0.890250027179718\n",
      "Iteration 35410 Training loss 0.015485342592000961 Validation loss 0.04570828005671501 Accuracy 0.8785000443458557\n",
      "Iteration 35420 Training loss 0.015133400447666645 Validation loss 0.0424840971827507 Accuracy 0.8888750672340393\n",
      "Iteration 35430 Training loss 0.016972608864307404 Validation loss 0.04448617249727249 Accuracy 0.8838750123977661\n",
      "Iteration 35440 Training loss 0.02882343716919422 Validation loss 0.050775714218616486 Accuracy 0.8647500276565552\n",
      "Iteration 35450 Training loss 0.03755801171064377 Validation loss 0.05883250758051872 Accuracy 0.8478750586509705\n",
      "Iteration 35460 Training loss 0.02230345830321312 Validation loss 0.046283770352602005 Accuracy 0.8792500495910645\n",
      "Iteration 35470 Training loss 0.013731158338487148 Validation loss 0.04491141438484192 Accuracy 0.8816250562667847\n",
      "Iteration 35480 Training loss 0.014911291189491749 Validation loss 0.04557281732559204 Accuracy 0.8815000653266907\n",
      "Iteration 35490 Training loss 0.014072190970182419 Validation loss 0.042447760701179504 Accuracy 0.8890000581741333\n",
      "Iteration 35500 Training loss 0.015567424707114697 Validation loss 0.043339915573596954 Accuracy 0.8867500424385071\n",
      "Iteration 35510 Training loss 0.012726099230349064 Validation loss 0.04263897240161896 Accuracy 0.8895000219345093\n",
      "Iteration 35520 Training loss 0.01116855163127184 Validation loss 0.04285162314772606 Accuracy 0.8865000605583191\n",
      "Iteration 35530 Training loss 0.024925626814365387 Validation loss 0.04784927889704704 Accuracy 0.8748750686645508\n",
      "Iteration 35540 Training loss 0.027888700366020203 Validation loss 0.054413165897130966 Accuracy 0.8580000400543213\n",
      "Iteration 35550 Training loss 0.02044004388153553 Validation loss 0.04478271305561066 Accuracy 0.8846250176429749\n",
      "Iteration 35560 Training loss 0.021616730839014053 Validation loss 0.04551073536276817 Accuracy 0.8835000395774841\n",
      "Iteration 35570 Training loss 0.014482353813946247 Validation loss 0.04454769939184189 Accuracy 0.8821250200271606\n",
      "Iteration 35580 Training loss 0.029166212305426598 Validation loss 0.057319268584251404 Accuracy 0.8490000367164612\n",
      "Iteration 35590 Training loss 0.017131734639406204 Validation loss 0.043071944266557693 Accuracy 0.8867500424385071\n",
      "Iteration 35600 Training loss 0.01610478200018406 Validation loss 0.0448770746588707 Accuracy 0.8825000524520874\n",
      "Iteration 35610 Training loss 0.013999849557876587 Validation loss 0.04316823184490204 Accuracy 0.8870000243186951\n",
      "Iteration 35620 Training loss 0.017898719757795334 Validation loss 0.0426403284072876 Accuracy 0.890375018119812\n",
      "Iteration 35630 Training loss 0.015917465090751648 Validation loss 0.04239903762936592 Accuracy 0.889750063419342\n",
      "Iteration 35640 Training loss 0.013482813723385334 Validation loss 0.04319699481129646 Accuracy 0.8866250514984131\n",
      "Iteration 35650 Training loss 0.010847982950508595 Validation loss 0.04369412362575531 Accuracy 0.8840000629425049\n",
      "Iteration 35660 Training loss 0.017122512683272362 Validation loss 0.04255286604166031 Accuracy 0.8911250233650208\n",
      "Iteration 35670 Training loss 0.018235374242067337 Validation loss 0.04528467729687691 Accuracy 0.8813750147819519\n",
      "Iteration 35680 Training loss 0.01252121850848198 Validation loss 0.0441245473921299 Accuracy 0.8841250538825989\n",
      "Iteration 35690 Training loss 0.012010829523205757 Validation loss 0.042771052569150925 Accuracy 0.8891250491142273\n",
      "Iteration 35700 Training loss 0.012882615439593792 Validation loss 0.044318895787000656 Accuracy 0.8820000290870667\n",
      "Iteration 35710 Training loss 0.05304644629359245 Validation loss 0.067527174949646 Accuracy 0.827625036239624\n",
      "Iteration 35720 Training loss 0.011700496077537537 Validation loss 0.04250692203640938 Accuracy 0.8906250596046448\n",
      "Iteration 35730 Training loss 0.010730723850429058 Validation loss 0.0444248802959919 Accuracy 0.8835000395774841\n",
      "Iteration 35740 Training loss 0.010049199685454369 Validation loss 0.04250948131084442 Accuracy 0.8886250257492065\n",
      "Iteration 35750 Training loss 0.03629010170698166 Validation loss 0.06318061798810959 Accuracy 0.8366250395774841\n",
      "Iteration 35760 Training loss 0.016741642728447914 Validation loss 0.04274751618504524 Accuracy 0.8888750672340393\n",
      "Iteration 35770 Training loss 0.019395142793655396 Validation loss 0.044984787702560425 Accuracy 0.8810000419616699\n",
      "Iteration 35780 Training loss 0.016424192115664482 Validation loss 0.0427272692322731 Accuracy 0.8873750567436218\n",
      "Iteration 35790 Training loss 0.04587272182106972 Validation loss 0.06702832132577896 Accuracy 0.827750027179718\n",
      "Iteration 35800 Training loss 0.006928553339093924 Validation loss 0.04278906434774399 Accuracy 0.8880000710487366\n",
      "Iteration 35810 Training loss 0.01617518439888954 Validation loss 0.04296111315488815 Accuracy 0.8877500295639038\n",
      "Iteration 35820 Training loss 0.015067190863192081 Validation loss 0.04545344412326813 Accuracy 0.8813750147819519\n",
      "Iteration 35830 Training loss 0.018084468320012093 Validation loss 0.04288541525602341 Accuracy 0.8876250386238098\n",
      "Iteration 35840 Training loss 0.014061788097023964 Validation loss 0.04227103292942047 Accuracy 0.8895000219345093\n",
      "Iteration 35850 Training loss 0.019987842068076134 Validation loss 0.04229290038347244 Accuracy 0.89000004529953\n",
      "Iteration 35860 Training loss 0.015399052761495113 Validation loss 0.04272214695811272 Accuracy 0.8873750567436218\n",
      "Iteration 35870 Training loss 0.02084990404546261 Validation loss 0.047918371856212616 Accuracy 0.8752500414848328\n",
      "Iteration 35880 Training loss 0.013954303227365017 Validation loss 0.043799906969070435 Accuracy 0.8856250643730164\n",
      "Iteration 35890 Training loss 0.025020524859428406 Validation loss 0.05181647464632988 Accuracy 0.8636250495910645\n",
      "Iteration 35900 Training loss 0.02401883155107498 Validation loss 0.04852842539548874 Accuracy 0.8731250166893005\n",
      "Iteration 35910 Training loss 0.014242174103856087 Validation loss 0.04310699924826622 Accuracy 0.8865000605583191\n",
      "Iteration 35920 Training loss 0.012608794495463371 Validation loss 0.04251103475689888 Accuracy 0.8877500295639038\n",
      "Iteration 35930 Training loss 0.043575771152973175 Validation loss 0.06701253354549408 Accuracy 0.827625036239624\n",
      "Iteration 35940 Training loss 0.010924024507403374 Validation loss 0.042310427874326706 Accuracy 0.889875054359436\n",
      "Iteration 35950 Training loss 0.01472456380724907 Validation loss 0.042153339833021164 Accuracy 0.890125036239624\n",
      "Iteration 35960 Training loss 0.03002476692199707 Validation loss 0.0529712550342083 Accuracy 0.858875036239624\n",
      "Iteration 35970 Training loss 0.01747238077223301 Validation loss 0.04524039849638939 Accuracy 0.8820000290870667\n",
      "Iteration 35980 Training loss 0.013529544696211815 Validation loss 0.043618910014629364 Accuracy 0.8860000371932983\n",
      "Iteration 35990 Training loss 0.0201741810888052 Validation loss 0.04715433344244957 Accuracy 0.877750039100647\n",
      "Iteration 36000 Training loss 0.05471685156226158 Validation loss 0.07175175845623016 Accuracy 0.8172500133514404\n",
      "Iteration 36010 Training loss 0.01108455378562212 Validation loss 0.042670316994190216 Accuracy 0.8892500400543213\n",
      "Iteration 36020 Training loss 0.012623924762010574 Validation loss 0.04245278611779213 Accuracy 0.889875054359436\n",
      "Iteration 36030 Training loss 0.010565906763076782 Validation loss 0.04254516214132309 Accuracy 0.8877500295639038\n",
      "Iteration 36040 Training loss 0.018561601638793945 Validation loss 0.043074168264865875 Accuracy 0.8855000138282776\n",
      "Iteration 36050 Training loss 0.01197188813239336 Validation loss 0.042644720524549484 Accuracy 0.8887500166893005\n",
      "Iteration 36060 Training loss 0.017996955662965775 Validation loss 0.046451371163129807 Accuracy 0.8785000443458557\n",
      "Iteration 36070 Training loss 0.01474741380661726 Validation loss 0.044353023171424866 Accuracy 0.8838750123977661\n",
      "Iteration 36080 Training loss 0.012779573909938335 Validation loss 0.043316565454006195 Accuracy 0.8873750567436218\n",
      "Iteration 36090 Training loss 0.030563585460186005 Validation loss 0.054376475512981415 Accuracy 0.8598750233650208\n",
      "Iteration 36100 Training loss 0.018321799114346504 Validation loss 0.04307631030678749 Accuracy 0.8875000476837158\n",
      "Iteration 36110 Training loss 0.01226431131362915 Validation loss 0.04295843467116356 Accuracy 0.8861250281333923\n",
      "Iteration 36120 Training loss 0.010789894498884678 Validation loss 0.044174324721097946 Accuracy 0.8840000629425049\n",
      "Iteration 36130 Training loss 0.01272241584956646 Validation loss 0.0426267571747303 Accuracy 0.8890000581741333\n",
      "Iteration 36140 Training loss 0.011633538641035557 Validation loss 0.04273369908332825 Accuracy 0.8878750205039978\n",
      "Iteration 36150 Training loss 0.014905084855854511 Validation loss 0.042440012097358704 Accuracy 0.8892500400543213\n",
      "Iteration 36160 Training loss 0.01146590057760477 Validation loss 0.04432696849107742 Accuracy 0.8830000162124634\n",
      "Iteration 36170 Training loss 0.01282066572457552 Validation loss 0.04522239789366722 Accuracy 0.8801250457763672\n",
      "Iteration 36180 Training loss 0.009988845326006413 Validation loss 0.043156854808330536 Accuracy 0.8876250386238098\n",
      "Iteration 36190 Training loss 0.029642654582858086 Validation loss 0.048541247844696045 Accuracy 0.8738750219345093\n",
      "Iteration 36200 Training loss 0.022489352151751518 Validation loss 0.04881363734602928 Accuracy 0.8723750710487366\n",
      "Iteration 36210 Training loss 0.01299653947353363 Validation loss 0.044191330671310425 Accuracy 0.8846250176429749\n",
      "Iteration 36220 Training loss 0.019467659294605255 Validation loss 0.04494883120059967 Accuracy 0.8828750252723694\n",
      "Iteration 36230 Training loss 0.027947096154093742 Validation loss 0.0573643259704113 Accuracy 0.8521250486373901\n",
      "Iteration 36240 Training loss 0.012707769870758057 Validation loss 0.04332483559846878 Accuracy 0.8871250152587891\n",
      "Iteration 36250 Training loss 0.01991417445242405 Validation loss 0.04424753040075302 Accuracy 0.8846250176429749\n",
      "Iteration 36260 Training loss 0.019981609657406807 Validation loss 0.050477489829063416 Accuracy 0.8693750500679016\n",
      "Iteration 36270 Training loss 0.01853279024362564 Validation loss 0.04380347579717636 Accuracy 0.8868750333786011\n",
      "Iteration 36280 Training loss 0.011112839914858341 Validation loss 0.04342561960220337 Accuracy 0.8865000605583191\n",
      "Iteration 36290 Training loss 0.013515817001461983 Validation loss 0.043222371488809586 Accuracy 0.8871250152587891\n",
      "Iteration 36300 Training loss 0.01051709521561861 Validation loss 0.043128255754709244 Accuracy 0.8867500424385071\n",
      "Iteration 36310 Training loss 0.019491972401738167 Validation loss 0.04557207599282265 Accuracy 0.8805000185966492\n",
      "Iteration 36320 Training loss 0.05294894427061081 Validation loss 0.0708727017045021 Accuracy 0.8180000185966492\n",
      "Iteration 36330 Training loss 0.017548035830259323 Validation loss 0.04296307638287544 Accuracy 0.8882500529289246\n",
      "Iteration 36340 Training loss 0.013415608555078506 Validation loss 0.04443582892417908 Accuracy 0.8828750252723694\n",
      "Iteration 36350 Training loss 0.013556400313973427 Validation loss 0.043241508305072784 Accuracy 0.8875000476837158\n",
      "Iteration 36360 Training loss 0.021333791315555573 Validation loss 0.04703308269381523 Accuracy 0.8762500286102295\n",
      "Iteration 36370 Training loss 0.021030964329838753 Validation loss 0.04897803068161011 Accuracy 0.8723750710487366\n",
      "Iteration 36380 Training loss 0.016834424808621407 Validation loss 0.045916248112916946 Accuracy 0.8800000548362732\n",
      "Iteration 36390 Training loss 0.009741069748997688 Validation loss 0.043145209550857544 Accuracy 0.8883750438690186\n",
      "Iteration 36400 Training loss 0.01218633633106947 Validation loss 0.042707208544015884 Accuracy 0.8891250491142273\n",
      "Iteration 36410 Training loss 0.01421322301030159 Validation loss 0.043172456324100494 Accuracy 0.8886250257492065\n",
      "Iteration 36420 Training loss 0.015471703372895718 Validation loss 0.04526751860976219 Accuracy 0.8808750510215759\n",
      "Iteration 36430 Training loss 0.014058942906558514 Validation loss 0.04499337077140808 Accuracy 0.8817500472068787\n",
      "Iteration 36440 Training loss 0.012563805095851421 Validation loss 0.046289391815662384 Accuracy 0.878000020980835\n",
      "Iteration 36450 Training loss 0.013760032132267952 Validation loss 0.0430285781621933 Accuracy 0.8871250152587891\n",
      "Iteration 36460 Training loss 0.012892486527562141 Validation loss 0.042963504791259766 Accuracy 0.8875000476837158\n",
      "Iteration 36470 Training loss 0.03812507912516594 Validation loss 0.06755819916725159 Accuracy 0.8287500143051147\n",
      "Iteration 36480 Training loss 0.013725876808166504 Validation loss 0.044390466064214706 Accuracy 0.8836250305175781\n",
      "Iteration 36490 Training loss 0.020343346521258354 Validation loss 0.04550673067569733 Accuracy 0.8813750147819519\n",
      "Iteration 36500 Training loss 0.015560547821223736 Validation loss 0.04443901404738426 Accuracy 0.8826250433921814\n",
      "Iteration 36510 Training loss 0.01982825994491577 Validation loss 0.05134861171245575 Accuracy 0.8691250681877136\n",
      "Iteration 36520 Training loss 0.014545357786118984 Validation loss 0.04263598099350929 Accuracy 0.8875000476837158\n",
      "Iteration 36530 Training loss 0.011295848526060581 Validation loss 0.04342257231473923 Accuracy 0.8861250281333923\n",
      "Iteration 36540 Training loss 0.018052976578474045 Validation loss 0.049308035522699356 Accuracy 0.8712500333786011\n",
      "Iteration 36550 Training loss 0.013814101926982403 Validation loss 0.04543723911046982 Accuracy 0.8817500472068787\n",
      "Iteration 36560 Training loss 0.012603964656591415 Validation loss 0.04441876336932182 Accuracy 0.8836250305175781\n",
      "Iteration 36570 Training loss 0.03274710476398468 Validation loss 0.052626967430114746 Accuracy 0.8670000433921814\n",
      "Iteration 36580 Training loss 0.018571488559246063 Validation loss 0.047798581421375275 Accuracy 0.8770000338554382\n",
      "Iteration 36590 Training loss 0.017570018768310547 Validation loss 0.0441308319568634 Accuracy 0.8831250667572021\n",
      "Iteration 36600 Training loss 0.01733442023396492 Validation loss 0.045277711004018784 Accuracy 0.8803750276565552\n",
      "Iteration 36610 Training loss 0.012638182379305363 Validation loss 0.04269678518176079 Accuracy 0.8893750309944153\n",
      "Iteration 36620 Training loss 0.021048689261078835 Validation loss 0.05122196301817894 Accuracy 0.8658750653266907\n",
      "Iteration 36630 Training loss 0.011304094456136227 Validation loss 0.0430079810321331 Accuracy 0.8876250386238098\n",
      "Iteration 36640 Training loss 0.015243308618664742 Validation loss 0.04281381517648697 Accuracy 0.8867500424385071\n",
      "Iteration 36650 Training loss 0.014102238230407238 Validation loss 0.044507402926683426 Accuracy 0.8842500448226929\n",
      "Iteration 36660 Training loss 0.04966625198721886 Validation loss 0.06884350627660751 Accuracy 0.8252500295639038\n",
      "Iteration 36670 Training loss 0.012157726101577282 Validation loss 0.042842332273721695 Accuracy 0.8867500424385071\n",
      "Iteration 36680 Training loss 0.014985048212110996 Validation loss 0.043644387274980545 Accuracy 0.8860000371932983\n",
      "Iteration 36690 Training loss 0.021736254915595055 Validation loss 0.042417269200086594 Accuracy 0.8896250128746033\n",
      "Iteration 36700 Training loss 0.016992930322885513 Validation loss 0.04411435127258301 Accuracy 0.8847500681877136\n",
      "Iteration 36710 Training loss 0.016168568283319473 Validation loss 0.042784661054611206 Accuracy 0.8876250386238098\n",
      "Iteration 36720 Training loss 0.015163479372859001 Validation loss 0.04329035058617592 Accuracy 0.8867500424385071\n",
      "Iteration 36730 Training loss 0.036203861236572266 Validation loss 0.05640457570552826 Accuracy 0.8547500371932983\n",
      "Iteration 36740 Training loss 0.015891827642917633 Validation loss 0.04352908208966255 Accuracy 0.8853750228881836\n",
      "Iteration 36750 Training loss 0.015773501247167587 Validation loss 0.0439019501209259 Accuracy 0.8845000267028809\n",
      "Iteration 36760 Training loss 0.020128482952713966 Validation loss 0.04697059839963913 Accuracy 0.8762500286102295\n",
      "Iteration 36770 Training loss 0.018505007028579712 Validation loss 0.04322987422347069 Accuracy 0.8877500295639038\n",
      "Iteration 36780 Training loss 0.007585213519632816 Validation loss 0.04289380460977554 Accuracy 0.8868750333786011\n",
      "Iteration 36790 Training loss 0.015546788461506367 Validation loss 0.04509042575955391 Accuracy 0.8816250562667847\n",
      "Iteration 36800 Training loss 0.01454959623515606 Validation loss 0.04495086893439293 Accuracy 0.8823750615119934\n",
      "Iteration 36810 Training loss 0.016326328739523888 Validation loss 0.04250858724117279 Accuracy 0.8895000219345093\n",
      "Iteration 36820 Training loss 0.011330115608870983 Validation loss 0.042671043425798416 Accuracy 0.8892500400543213\n",
      "Iteration 36830 Training loss 0.010580642148852348 Validation loss 0.044615488499403 Accuracy 0.8821250200271606\n",
      "Iteration 36840 Training loss 0.0117634367197752 Validation loss 0.0425381176173687 Accuracy 0.8883750438690186\n",
      "Iteration 36850 Training loss 0.015577561222016811 Validation loss 0.04419909790158272 Accuracy 0.8857500553131104\n",
      "Iteration 36860 Training loss 0.014090499840676785 Validation loss 0.04271845519542694 Accuracy 0.8881250619888306\n",
      "Iteration 36870 Training loss 0.01841786317527294 Validation loss 0.043819211423397064 Accuracy 0.8863750696182251\n",
      "Iteration 36880 Training loss 0.01183185912668705 Validation loss 0.04326271638274193 Accuracy 0.8875000476837158\n",
      "Iteration 36890 Training loss 0.014574580825865269 Validation loss 0.04334840178489685 Accuracy 0.8870000243186951\n",
      "Iteration 36900 Training loss 0.03932293877005577 Validation loss 0.06274160742759705 Accuracy 0.8368750214576721\n",
      "Iteration 36910 Training loss 0.011494483798742294 Validation loss 0.04385649785399437 Accuracy 0.8858750462532043\n",
      "Iteration 36920 Training loss 0.011858094483613968 Validation loss 0.04310669004917145 Accuracy 0.8862500190734863\n",
      "Iteration 36930 Training loss 0.015071436762809753 Validation loss 0.04412614926695824 Accuracy 0.8853750228881836\n",
      "Iteration 36940 Training loss 0.01873268373310566 Validation loss 0.0431535430252552 Accuracy 0.8873750567436218\n",
      "Iteration 36950 Training loss 0.00932878628373146 Validation loss 0.0437508262693882 Accuracy 0.8847500681877136\n",
      "Iteration 36960 Training loss 0.011674112640321255 Validation loss 0.043138593435287476 Accuracy 0.8860000371932983\n",
      "Iteration 36970 Training loss 0.011478549800813198 Validation loss 0.042757607996463776 Accuracy 0.889750063419342\n",
      "Iteration 36980 Training loss 0.013617743737995625 Validation loss 0.04285319894552231 Accuracy 0.8881250619888306\n",
      "Iteration 36990 Training loss 0.02031818777322769 Validation loss 0.0482872799038887 Accuracy 0.8732500672340393\n",
      "Iteration 37000 Training loss 0.016269320622086525 Validation loss 0.04338951036334038 Accuracy 0.8886250257492065\n",
      "Iteration 37010 Training loss 0.011634218506515026 Validation loss 0.04285994544625282 Accuracy 0.8881250619888306\n",
      "Iteration 37020 Training loss 0.016597270965576172 Validation loss 0.04340729862451553 Accuracy 0.8872500658035278\n",
      "Iteration 37030 Training loss 0.011486195027828217 Validation loss 0.04267781600356102 Accuracy 0.8885000348091125\n",
      "Iteration 37040 Training loss 0.014439779333770275 Validation loss 0.04262224957346916 Accuracy 0.8887500166893005\n",
      "Iteration 37050 Training loss 0.010694649070501328 Validation loss 0.04368678852915764 Accuracy 0.8860000371932983\n",
      "Iteration 37060 Training loss 0.012200964614748955 Validation loss 0.04324888810515404 Accuracy 0.8863750696182251\n",
      "Iteration 37070 Training loss 0.011485655792057514 Validation loss 0.043323833495378494 Accuracy 0.8866250514984131\n",
      "Iteration 37080 Training loss 0.03744816035032272 Validation loss 0.05989190191030502 Accuracy 0.8470000624656677\n",
      "Iteration 37090 Training loss 0.012152259238064289 Validation loss 0.0431014820933342 Accuracy 0.8876250386238098\n",
      "Iteration 37100 Training loss 0.011938066221773624 Validation loss 0.04264942556619644 Accuracy 0.8893750309944153\n",
      "Iteration 37110 Training loss 0.011304076761007309 Validation loss 0.04285760968923569 Accuracy 0.8878750205039978\n",
      "Iteration 37120 Training loss 0.013203860260546207 Validation loss 0.042710646986961365 Accuracy 0.889875054359436\n",
      "Iteration 37130 Training loss 0.01888180524110794 Validation loss 0.042771827429533005 Accuracy 0.8883750438690186\n",
      "Iteration 37140 Training loss 0.010023921728134155 Validation loss 0.04288269951939583 Accuracy 0.8871250152587891\n",
      "Iteration 37150 Training loss 0.022840024903416634 Validation loss 0.05154795944690704 Accuracy 0.8660000562667847\n",
      "Iteration 37160 Training loss 0.017076091840863228 Validation loss 0.04517494514584541 Accuracy 0.8815000653266907\n",
      "Iteration 37170 Training loss 0.032077983021736145 Validation loss 0.053862351924180984 Accuracy 0.862250030040741\n",
      "Iteration 37180 Training loss 0.016213690862059593 Validation loss 0.04308332875370979 Accuracy 0.8892500400543213\n",
      "Iteration 37190 Training loss 0.010713676922023296 Validation loss 0.04259635508060455 Accuracy 0.890375018119812\n",
      "Iteration 37200 Training loss 0.018392395228147507 Validation loss 0.049057893455028534 Accuracy 0.8697500228881836\n",
      "Iteration 37210 Training loss 0.014450669288635254 Validation loss 0.044334642589092255 Accuracy 0.8856250643730164\n",
      "Iteration 37220 Training loss 0.01727631315588951 Validation loss 0.04377739131450653 Accuracy 0.8846250176429749\n",
      "Iteration 37230 Training loss 0.021017398685216904 Validation loss 0.04962179809808731 Accuracy 0.8736250400543213\n",
      "Iteration 37240 Training loss 0.012254439294338226 Validation loss 0.04305592551827431 Accuracy 0.8887500166893005\n",
      "Iteration 37250 Training loss 0.014953847974538803 Validation loss 0.04275177791714668 Accuracy 0.8890000581741333\n",
      "Iteration 37260 Training loss 0.016753792762756348 Validation loss 0.046121083199977875 Accuracy 0.8793750405311584\n",
      "Iteration 37270 Training loss 0.009215924888849258 Validation loss 0.04449200630187988 Accuracy 0.8835000395774841\n",
      "Iteration 37280 Training loss 0.020154986530542374 Validation loss 0.043155916035175323 Accuracy 0.8867500424385071\n",
      "Iteration 37290 Training loss 0.016173211857676506 Validation loss 0.0506812185049057 Accuracy 0.8671250343322754\n",
      "Iteration 37300 Training loss 0.015077978372573853 Validation loss 0.04528556019067764 Accuracy 0.8806250691413879\n",
      "Iteration 37310 Training loss 0.014876880683004856 Validation loss 0.04334232583642006 Accuracy 0.8867500424385071\n",
      "Iteration 37320 Training loss 0.015939174219965935 Validation loss 0.04359731078147888 Accuracy 0.8871250152587891\n",
      "Iteration 37330 Training loss 0.012613002210855484 Validation loss 0.042835380882024765 Accuracy 0.8892500400543213\n",
      "Iteration 37340 Training loss 0.05104019120335579 Validation loss 0.07630103081464767 Accuracy 0.8071250319480896\n",
      "Iteration 37350 Training loss 0.015499440021812916 Validation loss 0.046405892819166183 Accuracy 0.8806250691413879\n",
      "Iteration 37360 Training loss 0.016846684738993645 Validation loss 0.0427476242184639 Accuracy 0.8882500529289246\n",
      "Iteration 37370 Training loss 0.013859998434782028 Validation loss 0.04347915202379227 Accuracy 0.8858750462532043\n",
      "Iteration 37380 Training loss 0.011536032892763615 Validation loss 0.04282338172197342 Accuracy 0.8878750205039978\n",
      "Iteration 37390 Training loss 0.0282894317060709 Validation loss 0.059654682874679565 Accuracy 0.8476250171661377\n",
      "Iteration 37400 Training loss 0.010942813940346241 Validation loss 0.04316910728812218 Accuracy 0.8880000710487366\n",
      "Iteration 37410 Training loss 0.01785266026854515 Validation loss 0.0434604175388813 Accuracy 0.8872500658035278\n",
      "Iteration 37420 Training loss 0.012603591196238995 Validation loss 0.043724581599235535 Accuracy 0.8861250281333923\n",
      "Iteration 37430 Training loss 0.05171374976634979 Validation loss 0.075087770819664 Accuracy 0.812250018119812\n",
      "Iteration 37440 Training loss 0.01586281508207321 Validation loss 0.0431828610599041 Accuracy 0.8866250514984131\n",
      "Iteration 37450 Training loss 0.02713661640882492 Validation loss 0.05265580117702484 Accuracy 0.8638750314712524\n",
      "Iteration 37460 Training loss 0.01731131412088871 Validation loss 0.04378323256969452 Accuracy 0.8870000243186951\n",
      "Iteration 37470 Training loss 0.013405587524175644 Validation loss 0.043128062039613724 Accuracy 0.8878750205039978\n",
      "Iteration 37480 Training loss 0.017095869407057762 Validation loss 0.04508247971534729 Accuracy 0.8805000185966492\n",
      "Iteration 37490 Training loss 0.01806776411831379 Validation loss 0.04410320520401001 Accuracy 0.8875000476837158\n",
      "Iteration 37500 Training loss 0.014761492609977722 Validation loss 0.04409640654921532 Accuracy 0.8837500214576721\n",
      "Iteration 37510 Training loss 0.01377763319760561 Validation loss 0.04321799799799919 Accuracy 0.8883750438690186\n",
      "Iteration 37520 Training loss 0.0181532371789217 Validation loss 0.04301111027598381 Accuracy 0.8876250386238098\n",
      "Iteration 37530 Training loss 0.012735191732645035 Validation loss 0.04350099712610245 Accuracy 0.8861250281333923\n",
      "Iteration 37540 Training loss 0.022793926298618317 Validation loss 0.0482855923473835 Accuracy 0.8757500648498535\n",
      "Iteration 37550 Training loss 0.015898365527391434 Validation loss 0.042847637087106705 Accuracy 0.8882500529289246\n",
      "Iteration 37560 Training loss 0.017796969041228294 Validation loss 0.04397339001297951 Accuracy 0.8853750228881836\n",
      "Iteration 37570 Training loss 0.02548382617533207 Validation loss 0.05158691480755806 Accuracy 0.8656250238418579\n",
      "Iteration 37580 Training loss 0.013873059302568436 Validation loss 0.044034261256456375 Accuracy 0.8873750567436218\n",
      "Iteration 37590 Training loss 0.01748577132821083 Validation loss 0.043486759066581726 Accuracy 0.8883750438690186\n",
      "Iteration 37600 Training loss 0.014134183526039124 Validation loss 0.043507494032382965 Accuracy 0.8853750228881836\n",
      "Iteration 37610 Training loss 0.010704992339015007 Validation loss 0.04349267855286598 Accuracy 0.8851250410079956\n",
      "Iteration 37620 Training loss 0.013732612133026123 Validation loss 0.04267597198486328 Accuracy 0.8883750438690186\n",
      "Iteration 37630 Training loss 0.028719523921608925 Validation loss 0.0537961982190609 Accuracy 0.8626250624656677\n",
      "Iteration 37640 Training loss 0.01502902340143919 Validation loss 0.04547689110040665 Accuracy 0.8795000314712524\n",
      "Iteration 37650 Training loss 0.015332193113863468 Validation loss 0.042993709444999695 Accuracy 0.8875000476837158\n",
      "Iteration 37660 Training loss 0.015492348000407219 Validation loss 0.04556484520435333 Accuracy 0.8801250457763672\n",
      "Iteration 37670 Training loss 0.013900792226195335 Validation loss 0.04482019320130348 Accuracy 0.8856250643730164\n",
      "Iteration 37680 Training loss 0.012774243950843811 Validation loss 0.04267213121056557 Accuracy 0.8891250491142273\n",
      "Iteration 37690 Training loss 0.014770573936402798 Validation loss 0.04296191409230232 Accuracy 0.8873750567436218\n",
      "Iteration 37700 Training loss 0.017663154751062393 Validation loss 0.043452005833387375 Accuracy 0.8863750696182251\n",
      "Iteration 37710 Training loss 0.01934763789176941 Validation loss 0.0467461496591568 Accuracy 0.877750039100647\n",
      "Iteration 37720 Training loss 0.01662914641201496 Validation loss 0.04604032263159752 Accuracy 0.8788750171661377\n",
      "Iteration 37730 Training loss 0.016972387209534645 Validation loss 0.04531717300415039 Accuracy 0.8811250329017639\n",
      "Iteration 37740 Training loss 0.03299238160252571 Validation loss 0.062189847230911255 Accuracy 0.8395000696182251\n",
      "Iteration 37750 Training loss 0.03144509345293045 Validation loss 0.05483974888920784 Accuracy 0.8595000505447388\n",
      "Iteration 37760 Training loss 0.009883960708975792 Validation loss 0.04269077628850937 Accuracy 0.8882500529289246\n",
      "Iteration 37770 Training loss 0.014504866674542427 Validation loss 0.04234789311885834 Accuracy 0.890375018119812\n",
      "Iteration 37780 Training loss 0.012943418696522713 Validation loss 0.04247147962450981 Accuracy 0.8893750309944153\n",
      "Iteration 37790 Training loss 0.008399163372814655 Validation loss 0.042690210044384 Accuracy 0.8878750205039978\n",
      "Iteration 37800 Training loss 0.03557319939136505 Validation loss 0.06186280772089958 Accuracy 0.8405000567436218\n",
      "Iteration 37810 Training loss 0.028990209102630615 Validation loss 0.05306132882833481 Accuracy 0.862250030040741\n",
      "Iteration 37820 Training loss 0.01224544458091259 Validation loss 0.043347787111997604 Accuracy 0.8870000243186951\n",
      "Iteration 37830 Training loss 0.021047409623861313 Validation loss 0.04510821774601936 Accuracy 0.8806250691413879\n",
      "Iteration 37840 Training loss 0.0062061334028840065 Validation loss 0.04337208345532417 Accuracy 0.8886250257492065\n",
      "Iteration 37850 Training loss 0.017740651965141296 Validation loss 0.0476158931851387 Accuracy 0.8757500648498535\n",
      "Iteration 37860 Training loss 0.014027259312570095 Validation loss 0.04429517686367035 Accuracy 0.8830000162124634\n",
      "Iteration 37870 Training loss 0.012389618903398514 Validation loss 0.042639970779418945 Accuracy 0.8895000219345093\n",
      "Iteration 37880 Training loss 0.04371114447712898 Validation loss 0.07005025446414948 Accuracy 0.8241250514984131\n",
      "Iteration 37890 Training loss 0.014580994844436646 Validation loss 0.0437798947095871 Accuracy 0.8863750696182251\n",
      "Iteration 37900 Training loss 0.011975831352174282 Validation loss 0.043292123824357986 Accuracy 0.8868750333786011\n",
      "Iteration 37910 Training loss 0.03144648298621178 Validation loss 0.05335858464241028 Accuracy 0.8636250495910645\n",
      "Iteration 37920 Training loss 0.015664177015423775 Validation loss 0.043199267238378525 Accuracy 0.8867500424385071\n",
      "Iteration 37930 Training loss 0.021921509876847267 Validation loss 0.052353885024785995 Accuracy 0.8647500276565552\n",
      "Iteration 37940 Training loss 0.018528107553720474 Validation loss 0.04530351608991623 Accuracy 0.8820000290870667\n",
      "Iteration 37950 Training loss 0.007766896393150091 Validation loss 0.04375460371375084 Accuracy 0.8858750462532043\n",
      "Iteration 37960 Training loss 0.016705460846424103 Validation loss 0.04416847974061966 Accuracy 0.8833750486373901\n",
      "Iteration 37970 Training loss 0.018833255395293236 Validation loss 0.049187589436769485 Accuracy 0.8711250424385071\n",
      "Iteration 37980 Training loss 0.01869110018014908 Validation loss 0.048576757311820984 Accuracy 0.8738750219345093\n",
      "Iteration 37990 Training loss 0.009102931246161461 Validation loss 0.0430731363594532 Accuracy 0.8886250257492065\n",
      "Iteration 38000 Training loss 0.01015836838632822 Validation loss 0.04333021491765976 Accuracy 0.8880000710487366\n",
      "Iteration 38010 Training loss 0.017503531649708748 Validation loss 0.04285123944282532 Accuracy 0.8891250491142273\n",
      "Iteration 38020 Training loss 0.01858515664935112 Validation loss 0.04797567054629326 Accuracy 0.874125063419342\n",
      "Iteration 38030 Training loss 0.018602585420012474 Validation loss 0.048192668706178665 Accuracy 0.8736250400543213\n",
      "Iteration 38040 Training loss 0.016466833651065826 Validation loss 0.05081210285425186 Accuracy 0.8682500123977661\n",
      "Iteration 38050 Training loss 0.012342817150056362 Validation loss 0.043104883283376694 Accuracy 0.8891250491142273\n",
      "Iteration 38060 Training loss 0.017617514356970787 Validation loss 0.043942712247371674 Accuracy 0.8847500681877136\n",
      "Iteration 38070 Training loss 0.008198555558919907 Validation loss 0.04248503968119621 Accuracy 0.8887500166893005\n",
      "Iteration 38080 Training loss 0.010918155312538147 Validation loss 0.04264386370778084 Accuracy 0.8881250619888306\n",
      "Iteration 38090 Training loss 0.0125166866928339 Validation loss 0.043569616973400116 Accuracy 0.8861250281333923\n",
      "Iteration 38100 Training loss 0.016660651192069054 Validation loss 0.04449733346700668 Accuracy 0.8841250538825989\n",
      "Iteration 38110 Training loss 0.02027018554508686 Validation loss 0.04971565306186676 Accuracy 0.8738750219345093\n",
      "Iteration 38120 Training loss 0.01621730998158455 Validation loss 0.04291556030511856 Accuracy 0.8877500295639038\n",
      "Iteration 38130 Training loss 0.012368199415504932 Validation loss 0.04259691759943962 Accuracy 0.8886250257492065\n",
      "Iteration 38140 Training loss 0.017359105870127678 Validation loss 0.04308857023715973 Accuracy 0.8865000605583191\n",
      "Iteration 38150 Training loss 0.016732212156057358 Validation loss 0.04319065436720848 Accuracy 0.8877500295639038\n",
      "Iteration 38160 Training loss 0.011971535161137581 Validation loss 0.042983077466487885 Accuracy 0.8878750205039978\n",
      "Iteration 38170 Training loss 0.006348694209009409 Validation loss 0.043017465621232986 Accuracy 0.8878750205039978\n",
      "Iteration 38180 Training loss 0.012712905183434486 Validation loss 0.04300682991743088 Accuracy 0.8867500424385071\n",
      "Iteration 38190 Training loss 0.06347737461328506 Validation loss 0.09095098078250885 Accuracy 0.7732500433921814\n",
      "Iteration 38200 Training loss 0.012672631070017815 Validation loss 0.04364423453807831 Accuracy 0.8867500424385071\n",
      "Iteration 38210 Training loss 0.015766704455018044 Validation loss 0.04448026046156883 Accuracy 0.8845000267028809\n",
      "Iteration 38220 Training loss 0.0127014871686697 Validation loss 0.043269380927085876 Accuracy 0.8872500658035278\n",
      "Iteration 38230 Training loss 0.04921877384185791 Validation loss 0.073796346783638 Accuracy 0.8183750510215759\n",
      "Iteration 38240 Training loss 0.01251487247645855 Validation loss 0.043323930352926254 Accuracy 0.8866250514984131\n",
      "Iteration 38250 Training loss 0.014037387445569038 Validation loss 0.043727513402700424 Accuracy 0.8865000605583191\n",
      "Iteration 38260 Training loss 0.017293546348810196 Validation loss 0.04349047690629959 Accuracy 0.8867500424385071\n",
      "Iteration 38270 Training loss 0.011495612561702728 Validation loss 0.04301539808511734 Accuracy 0.8885000348091125\n",
      "Iteration 38280 Training loss 0.009760653600096703 Validation loss 0.04323431849479675 Accuracy 0.8885000348091125\n",
      "Iteration 38290 Training loss 0.017807411029934883 Validation loss 0.0437721386551857 Accuracy 0.8858750462532043\n",
      "Iteration 38300 Training loss 0.03076363541185856 Validation loss 0.05132810398936272 Accuracy 0.8668750524520874\n",
      "Iteration 38310 Training loss 0.017192838713526726 Validation loss 0.04995351657271385 Accuracy 0.8722500205039978\n",
      "Iteration 38320 Training loss 0.013894923962652683 Validation loss 0.04327283054590225 Accuracy 0.8872500658035278\n",
      "Iteration 38330 Training loss 0.0086672343313694 Validation loss 0.04402659088373184 Accuracy 0.8856250643730164\n",
      "Iteration 38340 Training loss 0.012973985634744167 Validation loss 0.042926982045173645 Accuracy 0.8891250491142273\n",
      "Iteration 38350 Training loss 0.012690417468547821 Validation loss 0.04576191306114197 Accuracy 0.8807500600814819\n",
      "Iteration 38360 Training loss 0.01263272762298584 Validation loss 0.04274819418787956 Accuracy 0.8871250152587891\n",
      "Iteration 38370 Training loss 0.005468225572258234 Validation loss 0.04271586984395981 Accuracy 0.8895000219345093\n",
      "Iteration 38380 Training loss 0.020637886598706245 Validation loss 0.0453413724899292 Accuracy 0.8817500472068787\n",
      "Iteration 38390 Training loss 0.014851409941911697 Validation loss 0.04289974272251129 Accuracy 0.890125036239624\n",
      "Iteration 38400 Training loss 0.015696842223405838 Validation loss 0.04345260560512543 Accuracy 0.8875000476837158\n",
      "Iteration 38410 Training loss 0.02016199380159378 Validation loss 0.049696460366249084 Accuracy 0.8702500462532043\n",
      "Iteration 38420 Training loss 0.00596796628087759 Validation loss 0.04398520290851593 Accuracy 0.8856250643730164\n",
      "Iteration 38430 Training loss 0.01679345592856407 Validation loss 0.043766915798187256 Accuracy 0.8855000138282776\n",
      "Iteration 38440 Training loss 0.013043600134551525 Validation loss 0.04255514591932297 Accuracy 0.890375018119812\n",
      "Iteration 38450 Training loss 0.015591499395668507 Validation loss 0.04365498572587967 Accuracy 0.8862500190734863\n",
      "Iteration 38460 Training loss 0.0173232089728117 Validation loss 0.04578974470496178 Accuracy 0.8782500624656677\n",
      "Iteration 38470 Training loss 0.024825753644108772 Validation loss 0.04699654132127762 Accuracy 0.8772500157356262\n",
      "Iteration 38480 Training loss 0.014136724174022675 Validation loss 0.04297883063554764 Accuracy 0.8882500529289246\n",
      "Iteration 38490 Training loss 0.020949427038431168 Validation loss 0.04817979037761688 Accuracy 0.8740000128746033\n",
      "Iteration 38500 Training loss 0.016669537872076035 Validation loss 0.04898650571703911 Accuracy 0.8740000128746033\n",
      "Iteration 38510 Training loss 0.016914162784814835 Validation loss 0.043607328087091446 Accuracy 0.8870000243186951\n",
      "Iteration 38520 Training loss 0.02233046293258667 Validation loss 0.04395885020494461 Accuracy 0.8843750357627869\n",
      "Iteration 38530 Training loss 0.02739587426185608 Validation loss 0.05649913474917412 Accuracy 0.8543750643730164\n",
      "Iteration 38540 Training loss 0.01833740621805191 Validation loss 0.050595544278621674 Accuracy 0.8683750629425049\n",
      "Iteration 38550 Training loss 0.011106564663350582 Validation loss 0.04325783625245094 Accuracy 0.8873750567436218\n",
      "Iteration 38560 Training loss 0.009679125621914864 Validation loss 0.042863812297582626 Accuracy 0.890250027179718\n",
      "Iteration 38570 Training loss 0.01035347394645214 Validation loss 0.046010445803403854 Accuracy 0.8795000314712524\n",
      "Iteration 38580 Training loss 0.02341889962553978 Validation loss 0.051237475126981735 Accuracy 0.8697500228881836\n",
      "Iteration 38590 Training loss 0.014803447760641575 Validation loss 0.04266730695962906 Accuracy 0.8895000219345093\n",
      "Iteration 38600 Training loss 0.010187091305851936 Validation loss 0.042556144297122955 Accuracy 0.8890000581741333\n",
      "Iteration 38610 Training loss 0.012469012290239334 Validation loss 0.04263551905751228 Accuracy 0.8893750309944153\n",
      "Iteration 38620 Training loss 0.008799144066870213 Validation loss 0.04269741103053093 Accuracy 0.8892500400543213\n",
      "Iteration 38630 Training loss 0.01098229642957449 Validation loss 0.04554927721619606 Accuracy 0.8798750638961792\n",
      "Iteration 38640 Training loss 0.011952086351811886 Validation loss 0.04284701123833656 Accuracy 0.8880000710487366\n",
      "Iteration 38650 Training loss 0.01569143310189247 Validation loss 0.04337066039443016 Accuracy 0.8860000371932983\n",
      "Iteration 38660 Training loss 0.03537237271666527 Validation loss 0.05785248801112175 Accuracy 0.8542500138282776\n",
      "Iteration 38670 Training loss 0.01050880178809166 Validation loss 0.04317483678460121 Accuracy 0.8887500166893005\n",
      "Iteration 38680 Training loss 0.008325420320034027 Validation loss 0.04287439212203026 Accuracy 0.8888750672340393\n",
      "Iteration 38690 Training loss 0.011608490720391273 Validation loss 0.04262029379606247 Accuracy 0.8895000219345093\n",
      "Iteration 38700 Training loss 0.010435436852276325 Validation loss 0.042579665780067444 Accuracy 0.8895000219345093\n",
      "Iteration 38710 Training loss 0.010109583847224712 Validation loss 0.04322165995836258 Accuracy 0.8882500529289246\n",
      "Iteration 38720 Training loss 0.006824408657848835 Validation loss 0.04393294081091881 Accuracy 0.8856250643730164\n",
      "Iteration 38730 Training loss 0.047635167837142944 Validation loss 0.0709494948387146 Accuracy 0.8222500681877136\n",
      "Iteration 38740 Training loss 0.011927329003810883 Validation loss 0.04376373440027237 Accuracy 0.8872500658035278\n",
      "Iteration 38750 Training loss 0.008226662874221802 Validation loss 0.042925212532281876 Accuracy 0.8888750672340393\n",
      "Iteration 38760 Training loss 0.012145865708589554 Validation loss 0.04295811057090759 Accuracy 0.8878750205039978\n",
      "Iteration 38770 Training loss 0.017680464312434196 Validation loss 0.04911884665489197 Accuracy 0.8725000619888306\n",
      "Iteration 38780 Training loss 0.01857691816985607 Validation loss 0.04485289007425308 Accuracy 0.8831250667572021\n",
      "Iteration 38790 Training loss 0.02015296369791031 Validation loss 0.04741254821419716 Accuracy 0.8768750429153442\n",
      "Iteration 38800 Training loss 0.01197256799787283 Validation loss 0.04367711767554283 Accuracy 0.8882500529289246\n",
      "Iteration 38810 Training loss 0.010288304649293423 Validation loss 0.042757585644721985 Accuracy 0.8885000348091125\n",
      "Iteration 38820 Training loss 0.010785102844238281 Validation loss 0.043105702847242355 Accuracy 0.8875000476837158\n",
      "Iteration 38830 Training loss 0.014495979994535446 Validation loss 0.042926423251628876 Accuracy 0.8893750309944153\n",
      "Iteration 38840 Training loss 0.03878888487815857 Validation loss 0.06202495098114014 Accuracy 0.843500018119812\n",
      "Iteration 38850 Training loss 0.016286121681332588 Validation loss 0.043860726058483124 Accuracy 0.8853750228881836\n",
      "Iteration 38860 Training loss 0.018453143537044525 Validation loss 0.04336211457848549 Accuracy 0.8877500295639038\n",
      "Iteration 38870 Training loss 0.01876160316169262 Validation loss 0.04376564919948578 Accuracy 0.8868750333786011\n",
      "Iteration 38880 Training loss 0.011621758341789246 Validation loss 0.04289335757493973 Accuracy 0.889875054359436\n",
      "Iteration 38890 Training loss 0.037668626755476 Validation loss 0.06296873092651367 Accuracy 0.8380000591278076\n",
      "Iteration 38900 Training loss 0.015505177900195122 Validation loss 0.04534144699573517 Accuracy 0.8822500705718994\n",
      "Iteration 38910 Training loss 0.013078110292553902 Validation loss 0.04274049401283264 Accuracy 0.8890000581741333\n",
      "Iteration 38920 Training loss 0.014480797573924065 Validation loss 0.0425635427236557 Accuracy 0.89000004529953\n",
      "Iteration 38930 Training loss 0.04674755781888962 Validation loss 0.07552746683359146 Accuracy 0.8127500414848328\n",
      "Iteration 38940 Training loss 0.012258581817150116 Validation loss 0.043032266199588776 Accuracy 0.8892500400543213\n",
      "Iteration 38950 Training loss 0.012090752832591534 Validation loss 0.042457111179828644 Accuracy 0.890250027179718\n",
      "Iteration 38960 Training loss 0.020205901935696602 Validation loss 0.046871405094861984 Accuracy 0.8765000700950623\n",
      "Iteration 38970 Training loss 0.009896861389279366 Validation loss 0.042765550315380096 Accuracy 0.8891250491142273\n",
      "Iteration 38980 Training loss 0.01137387566268444 Validation loss 0.04484545439481735 Accuracy 0.8818750381469727\n",
      "Iteration 38990 Training loss 0.009272413328289986 Validation loss 0.04295779764652252 Accuracy 0.8880000710487366\n",
      "Iteration 39000 Training loss 0.011845909990370274 Validation loss 0.04255667328834534 Accuracy 0.8887500166893005\n",
      "Iteration 39010 Training loss 0.016352005302906036 Validation loss 0.044760216027498245 Accuracy 0.8837500214576721\n",
      "Iteration 39020 Training loss 0.06109616532921791 Validation loss 0.07945418357849121 Accuracy 0.8058750629425049\n",
      "Iteration 39030 Training loss 0.015692321583628654 Validation loss 0.04378485307097435 Accuracy 0.8883750438690186\n",
      "Iteration 39040 Training loss 0.009817499667406082 Validation loss 0.04324968904256821 Accuracy 0.8878750205039978\n",
      "Iteration 39050 Training loss 0.02055170014500618 Validation loss 0.052512895315885544 Accuracy 0.862500011920929\n",
      "Iteration 39060 Training loss 0.011056484654545784 Validation loss 0.04321699216961861 Accuracy 0.8888750672340393\n",
      "Iteration 39070 Training loss 0.01143035851418972 Validation loss 0.042979646474123 Accuracy 0.8892500400543213\n",
      "Iteration 39080 Training loss 0.008925809524953365 Validation loss 0.04297944903373718 Accuracy 0.8878750205039978\n",
      "Iteration 39090 Training loss 0.010855144821107388 Validation loss 0.04272895306348801 Accuracy 0.8883750438690186\n",
      "Iteration 39100 Training loss 0.0460817813873291 Validation loss 0.06367310881614685 Accuracy 0.8371250629425049\n",
      "Iteration 39110 Training loss 0.014970691874623299 Validation loss 0.04297681525349617 Accuracy 0.8886250257492065\n",
      "Iteration 39120 Training loss 0.012823522090911865 Validation loss 0.04281817004084587 Accuracy 0.8892500400543213\n",
      "Iteration 39130 Training loss 0.011971741914749146 Validation loss 0.04266662895679474 Accuracy 0.8890000581741333\n",
      "Iteration 39140 Training loss 0.008041324093937874 Validation loss 0.04267779365181923 Accuracy 0.890250027179718\n",
      "Iteration 39150 Training loss 0.009514877572655678 Validation loss 0.04265590384602547 Accuracy 0.89000004529953\n",
      "Iteration 39160 Training loss 0.009909001179039478 Validation loss 0.04401160776615143 Accuracy 0.8847500681877136\n",
      "Iteration 39170 Training loss 0.08751584589481354 Validation loss 0.09683889895677567 Accuracy 0.7608750462532043\n",
      "Iteration 39180 Training loss 0.018538737669587135 Validation loss 0.04602980613708496 Accuracy 0.8811250329017639\n",
      "Iteration 39190 Training loss 0.012428311631083488 Validation loss 0.0431516207754612 Accuracy 0.8882500529289246\n",
      "Iteration 39200 Training loss 0.011140360496938229 Validation loss 0.042657576501369476 Accuracy 0.8892500400543213\n",
      "Iteration 39210 Training loss 0.015166821889579296 Validation loss 0.04281010851264 Accuracy 0.889750063419342\n",
      "Iteration 39220 Training loss 0.019118716940283775 Validation loss 0.04836726561188698 Accuracy 0.874750018119812\n",
      "Iteration 39230 Training loss 0.02496200054883957 Validation loss 0.04971027001738548 Accuracy 0.874125063419342\n",
      "Iteration 39240 Training loss 0.011269092559814453 Validation loss 0.044652070850133896 Accuracy 0.8842500448226929\n",
      "Iteration 39250 Training loss 0.014442392624914646 Validation loss 0.04354940354824066 Accuracy 0.8852500319480896\n",
      "Iteration 39260 Training loss 0.011783507652580738 Validation loss 0.042544521391391754 Accuracy 0.8888750672340393\n",
      "Iteration 39270 Training loss 0.00973566249012947 Validation loss 0.04288876801729202 Accuracy 0.8887500166893005\n",
      "Iteration 39280 Training loss 0.013546735048294067 Validation loss 0.045032527297735214 Accuracy 0.8840000629425049\n",
      "Iteration 39290 Training loss 0.011973841115832329 Validation loss 0.04310060665011406 Accuracy 0.8885000348091125\n",
      "Iteration 39300 Training loss 0.01185486651957035 Validation loss 0.04432714357972145 Accuracy 0.8862500190734863\n",
      "Iteration 39310 Training loss 0.011608781293034554 Validation loss 0.04346233233809471 Accuracy 0.8865000605583191\n",
      "Iteration 39320 Training loss 0.010185113176703453 Validation loss 0.043538112193346024 Accuracy 0.8871250152587891\n",
      "Iteration 39330 Training loss 0.01962955668568611 Validation loss 0.044872816652059555 Accuracy 0.8847500681877136\n",
      "Iteration 39340 Training loss 0.015918102115392685 Validation loss 0.045244112610816956 Accuracy 0.8816250562667847\n",
      "Iteration 39350 Training loss 0.01139518991112709 Validation loss 0.04265141114592552 Accuracy 0.8878750205039978\n",
      "Iteration 39360 Training loss 0.014473473653197289 Validation loss 0.0436575785279274 Accuracy 0.8861250281333923\n",
      "Iteration 39370 Training loss 0.014841925352811813 Validation loss 0.043698228895664215 Accuracy 0.8865000605583191\n",
      "Iteration 39380 Training loss 0.016893530264496803 Validation loss 0.04281247779726982 Accuracy 0.8881250619888306\n",
      "Iteration 39390 Training loss 0.010203800164163113 Validation loss 0.04269513487815857 Accuracy 0.8868750333786011\n",
      "Iteration 39400 Training loss 0.008755616843700409 Validation loss 0.044308751821517944 Accuracy 0.8835000395774841\n",
      "Iteration 39410 Training loss 0.01586032845079899 Validation loss 0.04333377629518509 Accuracy 0.8860000371932983\n",
      "Iteration 39420 Training loss 0.014567201025784016 Validation loss 0.045437417924404144 Accuracy 0.8818750381469727\n",
      "Iteration 39430 Training loss 0.038137562572956085 Validation loss 0.06048807129263878 Accuracy 0.8481250405311584\n",
      "Iteration 39440 Training loss 0.01105994451791048 Validation loss 0.04323117807507515 Accuracy 0.8878750205039978\n",
      "Iteration 39450 Training loss 0.019444609060883522 Validation loss 0.04817766696214676 Accuracy 0.8755000233650208\n",
      "Iteration 39460 Training loss 0.018279125913977623 Validation loss 0.045312151312828064 Accuracy 0.8836250305175781\n",
      "Iteration 39470 Training loss 0.015391318127512932 Validation loss 0.042747754603624344 Accuracy 0.8880000710487366\n",
      "Iteration 39480 Training loss 0.010479006916284561 Validation loss 0.04358821362257004 Accuracy 0.8877500295639038\n",
      "Iteration 39490 Training loss 0.010384561493992805 Validation loss 0.0434434749186039 Accuracy 0.8856250643730164\n",
      "Iteration 39500 Training loss 0.015360326506197453 Validation loss 0.04670042544603348 Accuracy 0.8802500367164612\n",
      "Iteration 39510 Training loss 0.010831445455551147 Validation loss 0.04361378028988838 Accuracy 0.8880000710487366\n",
      "Iteration 39520 Training loss 0.00901517178863287 Validation loss 0.04592585936188698 Accuracy 0.8803750276565552\n",
      "Iteration 39530 Training loss 0.012651822529733181 Validation loss 0.04292597249150276 Accuracy 0.8887500166893005\n",
      "Iteration 39540 Training loss 0.016756972298026085 Validation loss 0.04312363639473915 Accuracy 0.8871250152587891\n",
      "Iteration 39550 Training loss 0.039117734879255295 Validation loss 0.06600217521190643 Accuracy 0.8296250104904175\n",
      "Iteration 39560 Training loss 0.014617828652262688 Validation loss 0.047188468277454376 Accuracy 0.877500057220459\n",
      "Iteration 39570 Training loss 0.015776388347148895 Validation loss 0.043284524232149124 Accuracy 0.8880000710487366\n",
      "Iteration 39580 Training loss 0.011081905104219913 Validation loss 0.043363675475120544 Accuracy 0.8868750333786011\n",
      "Iteration 39590 Training loss 0.0127094192430377 Validation loss 0.04350985214114189 Accuracy 0.8847500681877136\n",
      "Iteration 39600 Training loss 0.016710424795746803 Validation loss 0.04532625153660774 Accuracy 0.8802500367164612\n",
      "Iteration 39610 Training loss 0.04412555694580078 Validation loss 0.06744150817394257 Accuracy 0.8331250548362732\n",
      "Iteration 39620 Training loss 0.011726426891982555 Validation loss 0.04318251088261604 Accuracy 0.8875000476837158\n",
      "Iteration 39630 Training loss 0.01326209120452404 Validation loss 0.04297405853867531 Accuracy 0.8891250491142273\n",
      "Iteration 39640 Training loss 0.010768432170152664 Validation loss 0.042744025588035583 Accuracy 0.8890000581741333\n",
      "Iteration 39650 Training loss 0.013867254368960857 Validation loss 0.04323642700910568 Accuracy 0.8868750333786011\n",
      "Iteration 39660 Training loss 0.04064197465777397 Validation loss 0.06808969378471375 Accuracy 0.8292500376701355\n",
      "Iteration 39670 Training loss 0.012821625918149948 Validation loss 0.04364704713225365 Accuracy 0.8868750333786011\n",
      "Iteration 39680 Training loss 0.014686865732073784 Validation loss 0.043742116540670395 Accuracy 0.8855000138282776\n",
      "Iteration 39690 Training loss 0.039467424154281616 Validation loss 0.06612763553857803 Accuracy 0.8335000276565552\n",
      "Iteration 39700 Training loss 0.005281769670546055 Validation loss 0.04324736446142197 Accuracy 0.8886250257492065\n",
      "Iteration 39710 Training loss 0.013042970560491085 Validation loss 0.04346579313278198 Accuracy 0.8865000605583191\n",
      "Iteration 39720 Training loss 0.01493905484676361 Validation loss 0.04254981502890587 Accuracy 0.8893750309944153\n",
      "Iteration 39730 Training loss 0.013994816690683365 Validation loss 0.04266008362174034 Accuracy 0.8912500143051147\n",
      "Iteration 39740 Training loss 0.03412961959838867 Validation loss 0.061017490923404694 Accuracy 0.846125066280365\n",
      "Iteration 39750 Training loss 0.014341683126986027 Validation loss 0.04571960121393204 Accuracy 0.8805000185966492\n",
      "Iteration 39760 Training loss 0.010188615880906582 Validation loss 0.04354811832308769 Accuracy 0.8868750333786011\n",
      "Iteration 39770 Training loss 0.00713500427082181 Validation loss 0.04268236085772514 Accuracy 0.8915000557899475\n",
      "Iteration 39780 Training loss 0.011914963833987713 Validation loss 0.04331284761428833 Accuracy 0.8882500529289246\n",
      "Iteration 39790 Training loss 0.016130302101373672 Validation loss 0.04598609358072281 Accuracy 0.8792500495910645\n",
      "Iteration 39800 Training loss 0.02373920939862728 Validation loss 0.05330774933099747 Accuracy 0.861875057220459\n",
      "Iteration 39810 Training loss 0.008793479762971401 Validation loss 0.042769066989421844 Accuracy 0.890125036239624\n",
      "Iteration 39820 Training loss 0.014638491906225681 Validation loss 0.04412062466144562 Accuracy 0.8853750228881836\n",
      "Iteration 39830 Training loss 0.022151503711938858 Validation loss 0.04784775897860527 Accuracy 0.8755000233650208\n",
      "Iteration 39840 Training loss 0.016722476109862328 Validation loss 0.044354282319545746 Accuracy 0.8873750567436218\n",
      "Iteration 39850 Training loss 0.004818327259272337 Validation loss 0.04359304904937744 Accuracy 0.8873750567436218\n",
      "Iteration 39860 Training loss 0.008352396078407764 Validation loss 0.04311734065413475 Accuracy 0.8885000348091125\n",
      "Iteration 39870 Training loss 0.014700776897370815 Validation loss 0.042859870940446854 Accuracy 0.8891250491142273\n",
      "Iteration 39880 Training loss 0.034762900322675705 Validation loss 0.059334978461265564 Accuracy 0.8438750505447388\n",
      "Iteration 39890 Training loss 0.012919231317937374 Validation loss 0.04379789158701897 Accuracy 0.8870000243186951\n",
      "Iteration 39900 Training loss 0.018998827785253525 Validation loss 0.04267233610153198 Accuracy 0.8888750672340393\n",
      "Iteration 39910 Training loss 0.01459359098225832 Validation loss 0.04319731891155243 Accuracy 0.8876250386238098\n",
      "Iteration 39920 Training loss 0.010682718828320503 Validation loss 0.04264019802212715 Accuracy 0.8887500166893005\n",
      "Iteration 39930 Training loss 0.006901055574417114 Validation loss 0.04290003702044487 Accuracy 0.8882500529289246\n",
      "Iteration 39940 Training loss 0.01649615541100502 Validation loss 0.04280044138431549 Accuracy 0.8887500166893005\n",
      "Iteration 39950 Training loss 0.00945433508604765 Validation loss 0.04488740116357803 Accuracy 0.8836250305175781\n",
      "Iteration 39960 Training loss 0.08082542568445206 Validation loss 0.0983823612332344 Accuracy 0.7551250457763672\n",
      "Iteration 39970 Training loss 0.014994408003985882 Validation loss 0.04302328824996948 Accuracy 0.8885000348091125\n",
      "Iteration 39980 Training loss 0.01136518083512783 Validation loss 0.04287384822964668 Accuracy 0.8882500529289246\n",
      "Iteration 39990 Training loss 0.006466468330472708 Validation loss 0.04318319261074066 Accuracy 0.8876250386238098\n",
      "Iteration 40000 Training loss 0.04042530059814453 Validation loss 0.06544903665781021 Accuracy 0.8323750495910645\n",
      "Iteration 40010 Training loss 0.00920493621379137 Validation loss 0.04306792840361595 Accuracy 0.8893750309944153\n",
      "Iteration 40020 Training loss 0.009272829629480839 Validation loss 0.043069902807474136 Accuracy 0.8882500529289246\n",
      "Iteration 40030 Training loss 0.013358565047383308 Validation loss 0.043838389217853546 Accuracy 0.8863750696182251\n",
      "Iteration 40040 Training loss 0.013787741772830486 Validation loss 0.042607445269823074 Accuracy 0.8906250596046448\n",
      "Iteration 40050 Training loss 0.011399964801967144 Validation loss 0.04318062961101532 Accuracy 0.8872500658035278\n",
      "Iteration 40060 Training loss 0.018071426078677177 Validation loss 0.046202402561903 Accuracy 0.8808750510215759\n",
      "Iteration 40070 Training loss 0.04217606410384178 Validation loss 0.06876159459352493 Accuracy 0.8271250128746033\n",
      "Iteration 40080 Training loss 0.016119031235575676 Validation loss 0.04408964142203331 Accuracy 0.8848750591278076\n",
      "Iteration 40090 Training loss 0.011518006213009357 Validation loss 0.04300519824028015 Accuracy 0.8893750309944153\n",
      "Iteration 40100 Training loss 0.011112019419670105 Validation loss 0.04675282537937164 Accuracy 0.8792500495910645\n",
      "Iteration 40110 Training loss 0.0936947837471962 Validation loss 0.11062012612819672 Accuracy 0.7291250228881836\n",
      "Iteration 40120 Training loss 0.009830553084611893 Validation loss 0.04385845363140106 Accuracy 0.8867500424385071\n",
      "Iteration 40130 Training loss 0.015778880566358566 Validation loss 0.04623691737651825 Accuracy 0.8798750638961792\n",
      "Iteration 40140 Training loss 0.010668606497347355 Validation loss 0.043709468096494675 Accuracy 0.8863750696182251\n",
      "Iteration 40150 Training loss 0.012204665690660477 Validation loss 0.04343269392848015 Accuracy 0.8866250514984131\n",
      "Iteration 40160 Training loss 0.012132096104323864 Validation loss 0.04289409890770912 Accuracy 0.8888750672340393\n",
      "Iteration 40170 Training loss 0.011334449984133244 Validation loss 0.04386254400014877 Accuracy 0.8858750462532043\n",
      "Iteration 40180 Training loss 0.010044294409453869 Validation loss 0.043518006801605225 Accuracy 0.8870000243186951\n",
      "Iteration 40190 Training loss 0.013420956209301949 Validation loss 0.04480578005313873 Accuracy 0.8827500343322754\n",
      "Iteration 40200 Training loss 0.008633645251393318 Validation loss 0.04266775771975517 Accuracy 0.889750063419342\n",
      "Iteration 40210 Training loss 0.03802664205431938 Validation loss 0.06426030397415161 Accuracy 0.8345000147819519\n",
      "Iteration 40220 Training loss 0.0114643145352602 Validation loss 0.04358861967921257 Accuracy 0.8865000605583191\n",
      "Iteration 40230 Training loss 0.016950806602835655 Validation loss 0.042743831872940063 Accuracy 0.889875054359436\n",
      "Iteration 40240 Training loss 0.014256094582378864 Validation loss 0.04269148036837578 Accuracy 0.8878750205039978\n",
      "Iteration 40250 Training loss 0.009620456025004387 Validation loss 0.04315096512436867 Accuracy 0.8873750567436218\n",
      "Iteration 40260 Training loss 0.013552350923418999 Validation loss 0.04270661249756813 Accuracy 0.889750063419342\n",
      "Iteration 40270 Training loss 0.03945675864815712 Validation loss 0.05801795423030853 Accuracy 0.8546250462532043\n",
      "Iteration 40280 Training loss 0.008964566513895988 Validation loss 0.04289335757493973 Accuracy 0.8881250619888306\n",
      "Iteration 40290 Training loss 0.009424922056496143 Validation loss 0.04301805421710014 Accuracy 0.8873750567436218\n",
      "Iteration 40300 Training loss 0.014204771257936954 Validation loss 0.04775742441415787 Accuracy 0.8758750557899475\n",
      "Iteration 40310 Training loss 0.015893755480647087 Validation loss 0.046396587044000626 Accuracy 0.8808750510215759\n",
      "Iteration 40320 Training loss 0.012949138879776001 Validation loss 0.04288725554943085 Accuracy 0.8896250128746033\n",
      "Iteration 40330 Training loss 0.020723693072795868 Validation loss 0.04773346334695816 Accuracy 0.8761250376701355\n",
      "Iteration 40340 Training loss 0.01231972873210907 Validation loss 0.042682718485593796 Accuracy 0.8895000219345093\n",
      "Iteration 40350 Training loss 0.008876467123627663 Validation loss 0.04565795511007309 Accuracy 0.8802500367164612\n",
      "Iteration 40360 Training loss 0.009819173254072666 Validation loss 0.04280060902237892 Accuracy 0.8887500166893005\n",
      "Iteration 40370 Training loss 0.0065987855195999146 Validation loss 0.04294366016983986 Accuracy 0.8890000581741333\n",
      "Iteration 40380 Training loss 0.0182630755007267 Validation loss 0.04353543370962143 Accuracy 0.8852500319480896\n",
      "Iteration 40390 Training loss 0.015808703377842903 Validation loss 0.0426560640335083 Accuracy 0.89000004529953\n",
      "Iteration 40400 Training loss 0.00736351078376174 Validation loss 0.042980682104825974 Accuracy 0.8895000219345093\n",
      "Iteration 40410 Training loss 0.014358491636812687 Validation loss 0.04383480176329613 Accuracy 0.8867500424385071\n",
      "Iteration 40420 Training loss 0.009413178078830242 Validation loss 0.042707618325948715 Accuracy 0.889750063419342\n",
      "Iteration 40430 Training loss 0.010534517467021942 Validation loss 0.042625922709703445 Accuracy 0.8892500400543213\n",
      "Iteration 40440 Training loss 0.0496329739689827 Validation loss 0.07742257416248322 Accuracy 0.8067500591278076\n",
      "Iteration 40450 Training loss 0.009667477570474148 Validation loss 0.04419464245438576 Accuracy 0.8866250514984131\n",
      "Iteration 40460 Training loss 0.01061293575912714 Validation loss 0.04317856952548027 Accuracy 0.8872500658035278\n",
      "Iteration 40470 Training loss 0.007485538721084595 Validation loss 0.042829737067222595 Accuracy 0.8888750672340393\n",
      "Iteration 40480 Training loss 0.027631428092718124 Validation loss 0.054434023797512054 Accuracy 0.858500063419342\n",
      "Iteration 40490 Training loss 0.023227965459227562 Validation loss 0.049632396548986435 Accuracy 0.8715000152587891\n",
      "Iteration 40500 Training loss 0.015859603881835938 Validation loss 0.04419081285595894 Accuracy 0.8866250514984131\n",
      "Iteration 40510 Training loss 0.011169386096298695 Validation loss 0.043070871382951736 Accuracy 0.8882500529289246\n",
      "Iteration 40520 Training loss 0.012327206321060658 Validation loss 0.04280541092157364 Accuracy 0.8891250491142273\n",
      "Iteration 40530 Training loss 0.010813621804118156 Validation loss 0.043684251606464386 Accuracy 0.8873750567436218\n",
      "Iteration 40540 Training loss 0.01325349323451519 Validation loss 0.04367668181657791 Accuracy 0.8882500529289246\n",
      "Iteration 40550 Training loss 0.014105737209320068 Validation loss 0.04298939183354378 Accuracy 0.8885000348091125\n",
      "Iteration 40560 Training loss 0.010150063782930374 Validation loss 0.043262653052806854 Accuracy 0.8868750333786011\n",
      "Iteration 40570 Training loss 0.014870231039822102 Validation loss 0.043973829597234726 Accuracy 0.8865000605583191\n",
      "Iteration 40580 Training loss 0.03657149896025658 Validation loss 0.06069456785917282 Accuracy 0.8485000133514404\n",
      "Iteration 40590 Training loss 0.00684184767305851 Validation loss 0.04424230754375458 Accuracy 0.8861250281333923\n",
      "Iteration 40600 Training loss 0.007651457563042641 Validation loss 0.04365124553442001 Accuracy 0.8865000605583191\n",
      "Iteration 40610 Training loss 0.013137638568878174 Validation loss 0.04356636106967926 Accuracy 0.8863750696182251\n",
      "Iteration 40620 Training loss 0.014052917249500751 Validation loss 0.04276349022984505 Accuracy 0.8891250491142273\n",
      "Iteration 40630 Training loss 0.010127279907464981 Validation loss 0.042566150426864624 Accuracy 0.8881250619888306\n",
      "Iteration 40640 Training loss 0.012692496180534363 Validation loss 0.042843714356422424 Accuracy 0.8868750333786011\n",
      "Iteration 40650 Training loss 0.012576973997056484 Validation loss 0.042501579970121384 Accuracy 0.8881250619888306\n",
      "Iteration 40660 Training loss 0.012603342533111572 Validation loss 0.0435866080224514 Accuracy 0.8877500295639038\n",
      "Iteration 40670 Training loss 0.008350974880158901 Validation loss 0.04378509894013405 Accuracy 0.8858750462532043\n",
      "Iteration 40680 Training loss 0.008907577954232693 Validation loss 0.042943112552165985 Accuracy 0.8890000581741333\n",
      "Iteration 40690 Training loss 0.014432082884013653 Validation loss 0.045639146119356155 Accuracy 0.8807500600814819\n",
      "Iteration 40700 Training loss 0.01812652125954628 Validation loss 0.042676232755184174 Accuracy 0.8891250491142273\n",
      "Iteration 40710 Training loss 0.008964981883764267 Validation loss 0.04282303899526596 Accuracy 0.8881250619888306\n",
      "Iteration 40720 Training loss 0.010864797979593277 Validation loss 0.04263181611895561 Accuracy 0.8886250257492065\n",
      "Iteration 40730 Training loss 0.016265220940113068 Validation loss 0.0430678054690361 Accuracy 0.8881250619888306\n",
      "Iteration 40740 Training loss 0.05000291392207146 Validation loss 0.0737723559141159 Accuracy 0.8178750276565552\n",
      "Iteration 40750 Training loss 0.012240350246429443 Validation loss 0.04334172606468201 Accuracy 0.8882500529289246\n",
      "Iteration 40760 Training loss 0.02527117170393467 Validation loss 0.05648684874176979 Accuracy 0.8538750410079956\n",
      "Iteration 40770 Training loss 0.014353650622069836 Validation loss 0.043440572917461395 Accuracy 0.8891250491142273\n",
      "Iteration 40780 Training loss 0.01667562872171402 Validation loss 0.04273771122097969 Accuracy 0.8896250128746033\n",
      "Iteration 40790 Training loss 0.009096263907849789 Validation loss 0.043518878519535065 Accuracy 0.8860000371932983\n",
      "Iteration 40800 Training loss 0.01234900951385498 Validation loss 0.04709823802113533 Accuracy 0.8758750557899475\n",
      "Iteration 40810 Training loss 0.01315612718462944 Validation loss 0.04290831834077835 Accuracy 0.8891250491142273\n",
      "Iteration 40820 Training loss 0.012015821412205696 Validation loss 0.04364790394902229 Accuracy 0.8887500166893005\n",
      "Iteration 40830 Training loss 0.005271053407341242 Validation loss 0.04263811558485031 Accuracy 0.889875054359436\n",
      "Iteration 40840 Training loss 0.015804728493094444 Validation loss 0.0427323542535305 Accuracy 0.8885000348091125\n",
      "Iteration 40850 Training loss 0.013370509259402752 Validation loss 0.043827783316373825 Accuracy 0.8840000629425049\n",
      "Iteration 40860 Training loss 0.019380735233426094 Validation loss 0.055797845125198364 Accuracy 0.8593750596046448\n",
      "Iteration 40870 Training loss 0.008254662156105042 Validation loss 0.04464776813983917 Accuracy 0.8858750462532043\n",
      "Iteration 40880 Training loss 0.015603105537593365 Validation loss 0.04257894307374954 Accuracy 0.8895000219345093\n",
      "Iteration 40890 Training loss 0.009372085332870483 Validation loss 0.042536698281764984 Accuracy 0.8891250491142273\n",
      "Iteration 40900 Training loss 0.013163125142455101 Validation loss 0.04527572914958 Accuracy 0.8846250176429749\n",
      "Iteration 40910 Training loss 0.012364184483885765 Validation loss 0.04296853765845299 Accuracy 0.8886250257492065\n",
      "Iteration 40920 Training loss 0.009639746509492397 Validation loss 0.04293570667505264 Accuracy 0.8880000710487366\n",
      "Iteration 40930 Training loss 0.02109222300350666 Validation loss 0.047880787402391434 Accuracy 0.8757500648498535\n",
      "Iteration 40940 Training loss 0.01073472946882248 Validation loss 0.04294418543577194 Accuracy 0.8880000710487366\n",
      "Iteration 40950 Training loss 0.014122601598501205 Validation loss 0.04288338124752045 Accuracy 0.8893750309944153\n",
      "Iteration 40960 Training loss 0.01651843450963497 Validation loss 0.04321010038256645 Accuracy 0.8870000243186951\n",
      "Iteration 40970 Training loss 0.010749952867627144 Validation loss 0.04297103360295296 Accuracy 0.8886250257492065\n",
      "Iteration 40980 Training loss 0.019316455349326134 Validation loss 0.05233512818813324 Accuracy 0.8666250705718994\n",
      "Iteration 40990 Training loss 0.030917154625058174 Validation loss 0.06142164766788483 Accuracy 0.8442500233650208\n",
      "Iteration 41000 Training loss 0.023058900609612465 Validation loss 0.050736431032419205 Accuracy 0.8721250295639038\n",
      "Iteration 41010 Training loss 0.006705371662974358 Validation loss 0.042781054973602295 Accuracy 0.8896250128746033\n",
      "Iteration 41020 Training loss 0.017100555822253227 Validation loss 0.04265092685818672 Accuracy 0.8895000219345093\n",
      "Iteration 41030 Training loss 0.023235434666275978 Validation loss 0.05095304548740387 Accuracy 0.8676250576972961\n",
      "Iteration 41040 Training loss 0.01891500875353813 Validation loss 0.04641745239496231 Accuracy 0.8808750510215759\n",
      "Iteration 41050 Training loss 0.014477817341685295 Validation loss 0.042657434940338135 Accuracy 0.8893750309944153\n",
      "Iteration 41060 Training loss 0.009193094447255135 Validation loss 0.04297683760523796 Accuracy 0.8887500166893005\n",
      "Iteration 41070 Training loss 0.014836592599749565 Validation loss 0.043938834220170975 Accuracy 0.8857500553131104\n",
      "Iteration 41080 Training loss 0.013313619419932365 Validation loss 0.043433479964733124 Accuracy 0.8871250152587891\n",
      "Iteration 41090 Training loss 0.01133180595934391 Validation loss 0.042821213603019714 Accuracy 0.8891250491142273\n",
      "Iteration 41100 Training loss 0.012395079247653484 Validation loss 0.04263954982161522 Accuracy 0.8913750648498535\n",
      "Iteration 41110 Training loss 0.01730749011039734 Validation loss 0.04506034404039383 Accuracy 0.8833750486373901\n",
      "Iteration 41120 Training loss 0.0559525303542614 Validation loss 0.06344710290431976 Accuracy 0.843375027179718\n",
      "Iteration 41130 Training loss 0.012350214645266533 Validation loss 0.0433424711227417 Accuracy 0.8887500166893005\n",
      "Iteration 41140 Training loss 0.010353317484259605 Validation loss 0.04298552870750427 Accuracy 0.8888750672340393\n",
      "Iteration 41150 Training loss 0.015936169773340225 Validation loss 0.04403085634112358 Accuracy 0.8856250643730164\n",
      "Iteration 41160 Training loss 0.027022847905755043 Validation loss 0.05525248870253563 Accuracy 0.8598750233650208\n",
      "Iteration 41170 Training loss 0.009013577364385128 Validation loss 0.04415213316679001 Accuracy 0.8860000371932983\n",
      "Iteration 41180 Training loss 0.009356847032904625 Validation loss 0.0432731956243515 Accuracy 0.8890000581741333\n",
      "Iteration 41190 Training loss 0.013029664754867554 Validation loss 0.042887017130851746 Accuracy 0.8886250257492065\n",
      "Iteration 41200 Training loss 0.017170144245028496 Validation loss 0.048441242426633835 Accuracy 0.8750000596046448\n",
      "Iteration 41210 Training loss 0.012514111585915089 Validation loss 0.044133491814136505 Accuracy 0.8862500190734863\n",
      "Iteration 41220 Training loss 0.014388703741133213 Validation loss 0.04283858463168144 Accuracy 0.8887500166893005\n",
      "Iteration 41230 Training loss 0.011530612595379353 Validation loss 0.04288902506232262 Accuracy 0.8896250128746033\n",
      "Iteration 41240 Training loss 0.009389697574079037 Validation loss 0.04260946437716484 Accuracy 0.890250027179718\n",
      "Iteration 41250 Training loss 0.011089280247688293 Validation loss 0.042606499046087265 Accuracy 0.890250027179718\n",
      "Iteration 41260 Training loss 0.010304699651896954 Validation loss 0.04298464581370354 Accuracy 0.8885000348091125\n",
      "Iteration 41270 Training loss 0.009883548133075237 Validation loss 0.04340008646249771 Accuracy 0.8862500190734863\n",
      "Iteration 41280 Training loss 0.01336932834237814 Validation loss 0.04323989525437355 Accuracy 0.8870000243186951\n",
      "Iteration 41290 Training loss 0.010650516487658024 Validation loss 0.04306187480688095 Accuracy 0.8876250386238098\n",
      "Iteration 41300 Training loss 0.012093305587768555 Validation loss 0.043513476848602295 Accuracy 0.8858750462532043\n",
      "Iteration 41310 Training loss 0.01207191776484251 Validation loss 0.04326852038502693 Accuracy 0.8866250514984131\n",
      "Iteration 41320 Training loss 0.0077516152523458 Validation loss 0.042912255972623825 Accuracy 0.8880000710487366\n",
      "Iteration 41330 Training loss 0.011906015686690807 Validation loss 0.04284452274441719 Accuracy 0.8873750567436218\n",
      "Iteration 41340 Training loss 0.010232198052108288 Validation loss 0.04327959194779396 Accuracy 0.8881250619888306\n",
      "Iteration 41350 Training loss 0.013235237449407578 Validation loss 0.04612976312637329 Accuracy 0.8817500472068787\n",
      "Iteration 41360 Training loss 0.007800327148288488 Validation loss 0.04291005805134773 Accuracy 0.8873750567436218\n",
      "Iteration 41370 Training loss 0.011578531935811043 Validation loss 0.04446040093898773 Accuracy 0.8845000267028809\n",
      "Iteration 41380 Training loss 0.026546524837613106 Validation loss 0.05533630773425102 Accuracy 0.8592500686645508\n",
      "Iteration 41390 Training loss 0.012993206270039082 Validation loss 0.04466015100479126 Accuracy 0.8858750462532043\n",
      "Iteration 41400 Training loss 0.006415958050638437 Validation loss 0.043742310255765915 Accuracy 0.8873750567436218\n",
      "Iteration 41410 Training loss 0.007930879481136799 Validation loss 0.04388722404837608 Accuracy 0.8857500553131104\n",
      "Iteration 41420 Training loss 0.05633953958749771 Validation loss 0.07661750912666321 Accuracy 0.8092500567436218\n",
      "Iteration 41430 Training loss 0.015209739096462727 Validation loss 0.046399470418691635 Accuracy 0.8772500157356262\n",
      "Iteration 41440 Training loss 0.011192426085472107 Validation loss 0.04322340711951256 Accuracy 0.8881250619888306\n",
      "Iteration 41450 Training loss 0.0089793112128973 Validation loss 0.04329638555645943 Accuracy 0.8876250386238098\n",
      "Iteration 41460 Training loss 0.01453658752143383 Validation loss 0.04635417461395264 Accuracy 0.8807500600814819\n",
      "Iteration 41470 Training loss 0.009761840105056763 Validation loss 0.04371098428964615 Accuracy 0.8867500424385071\n",
      "Iteration 41480 Training loss 0.01759481057524681 Validation loss 0.04582834616303444 Accuracy 0.8801250457763672\n",
      "Iteration 41490 Training loss 0.041393451392650604 Validation loss 0.07037242501974106 Accuracy 0.8280000686645508\n",
      "Iteration 41500 Training loss 0.010739642195403576 Validation loss 0.04339689016342163 Accuracy 0.8875000476837158\n",
      "Iteration 41510 Training loss 0.011604880914092064 Validation loss 0.042546868324279785 Accuracy 0.889750063419342\n",
      "Iteration 41520 Training loss 0.006239224690943956 Validation loss 0.042752742767333984 Accuracy 0.8892500400543213\n",
      "Iteration 41530 Training loss 0.023174477741122246 Validation loss 0.05582181364297867 Accuracy 0.858875036239624\n",
      "Iteration 41540 Training loss 0.008737423457205296 Validation loss 0.04285429045557976 Accuracy 0.8891250491142273\n",
      "Iteration 41550 Training loss 0.006312709301710129 Validation loss 0.04307596758008003 Accuracy 0.8883750438690186\n",
      "Iteration 41560 Training loss 0.012676394544541836 Validation loss 0.04287859424948692 Accuracy 0.8876250386238098\n",
      "Iteration 41570 Training loss 0.015369231812655926 Validation loss 0.04305829107761383 Accuracy 0.8871250152587891\n",
      "Iteration 41580 Training loss 0.010778466239571571 Validation loss 0.04520593583583832 Accuracy 0.8831250667572021\n",
      "Iteration 41590 Training loss 0.013432085514068604 Validation loss 0.04469844698905945 Accuracy 0.8837500214576721\n",
      "Iteration 41600 Training loss 0.014742206782102585 Validation loss 0.04814981296658516 Accuracy 0.877875030040741\n",
      "Iteration 41610 Training loss 0.0197626780718565 Validation loss 0.04335591942071915 Accuracy 0.8875000476837158\n",
      "Iteration 41620 Training loss 0.012810109183192253 Validation loss 0.04307401180267334 Accuracy 0.8872500658035278\n",
      "Iteration 41630 Training loss 0.014939293265342712 Validation loss 0.04530426859855652 Accuracy 0.8818750381469727\n",
      "Iteration 41640 Training loss 0.007754713762551546 Validation loss 0.04430984705686569 Accuracy 0.8845000267028809\n",
      "Iteration 41650 Training loss 0.011376888491213322 Validation loss 0.044500283896923065 Accuracy 0.8853750228881836\n",
      "Iteration 41660 Training loss 0.009524947963654995 Validation loss 0.0435158871114254 Accuracy 0.8870000243186951\n",
      "Iteration 41670 Training loss 0.007812089752405882 Validation loss 0.046926554292440414 Accuracy 0.8783750534057617\n",
      "Iteration 41680 Training loss 0.017735807225108147 Validation loss 0.04665076360106468 Accuracy 0.8815000653266907\n",
      "Iteration 41690 Training loss 0.013420764356851578 Validation loss 0.04560675472021103 Accuracy 0.8815000653266907\n",
      "Iteration 41700 Training loss 0.011598131619393826 Validation loss 0.047221384942531586 Accuracy 0.8770000338554382\n",
      "Iteration 41710 Training loss 0.02783949300646782 Validation loss 0.05545628443360329 Accuracy 0.8583750128746033\n",
      "Iteration 41720 Training loss 0.00981331430375576 Validation loss 0.043201349675655365 Accuracy 0.8887500166893005\n",
      "Iteration 41730 Training loss 0.009635190479457378 Validation loss 0.04347836598753929 Accuracy 0.8866250514984131\n",
      "Iteration 41740 Training loss 0.010833067819476128 Validation loss 0.04307148978114128 Accuracy 0.8876250386238098\n",
      "Iteration 41750 Training loss 0.03656302019953728 Validation loss 0.061325568705797195 Accuracy 0.8455000519752502\n",
      "Iteration 41760 Training loss 0.01563623920083046 Validation loss 0.0440492108464241 Accuracy 0.8872500658035278\n",
      "Iteration 41770 Training loss 0.008956007659435272 Validation loss 0.04257265850901604 Accuracy 0.8895000219345093\n",
      "Iteration 41780 Training loss 0.0087820366024971 Validation loss 0.04585310071706772 Accuracy 0.8797500133514404\n",
      "Iteration 41790 Training loss 0.010497626848518848 Validation loss 0.04268261045217514 Accuracy 0.8896250128746033\n",
      "Iteration 41800 Training loss 0.02712668664753437 Validation loss 0.05658630654215813 Accuracy 0.8555000424385071\n",
      "Iteration 41810 Training loss 0.014344800263643265 Validation loss 0.04602457210421562 Accuracy 0.8821250200271606\n",
      "Iteration 41820 Training loss 0.007552841678261757 Validation loss 0.043281253427267075 Accuracy 0.889750063419342\n",
      "Iteration 41830 Training loss 0.01186731830239296 Validation loss 0.043913520872592926 Accuracy 0.8856250643730164\n",
      "Iteration 41840 Training loss 0.0093652019277215 Validation loss 0.04298343509435654 Accuracy 0.8896250128746033\n",
      "Iteration 41850 Training loss 0.013751339167356491 Validation loss 0.04406067728996277 Accuracy 0.8843750357627869\n",
      "Iteration 41860 Training loss 0.01584845595061779 Validation loss 0.04565351456403732 Accuracy 0.8840000629425049\n",
      "Iteration 41870 Training loss 0.007268713321536779 Validation loss 0.043495722115039825 Accuracy 0.8876250386238098\n",
      "Iteration 41880 Training loss 0.01365418266505003 Validation loss 0.04358859360218048 Accuracy 0.8852500319480896\n",
      "Iteration 41890 Training loss 0.009995978325605392 Validation loss 0.04259040579199791 Accuracy 0.8892500400543213\n",
      "Iteration 41900 Training loss 0.022758692502975464 Validation loss 0.05103747174143791 Accuracy 0.8685000538825989\n",
      "Iteration 41910 Training loss 0.010466069914400578 Validation loss 0.04354812204837799 Accuracy 0.8880000710487366\n",
      "Iteration 41920 Training loss 0.008449262008070946 Validation loss 0.04356667399406433 Accuracy 0.8875000476837158\n",
      "Iteration 41930 Training loss 0.00787787139415741 Validation loss 0.043493300676345825 Accuracy 0.8870000243186951\n",
      "Iteration 41940 Training loss 0.012553769163787365 Validation loss 0.04270893335342407 Accuracy 0.8890000581741333\n",
      "Iteration 41950 Training loss 0.014196163974702358 Validation loss 0.04270724579691887 Accuracy 0.8882500529289246\n",
      "Iteration 41960 Training loss 0.007615511771291494 Validation loss 0.04263485223054886 Accuracy 0.8890000581741333\n",
      "Iteration 41970 Training loss 0.01119453925639391 Validation loss 0.043037548661231995 Accuracy 0.8875000476837158\n",
      "Iteration 41980 Training loss 0.017404258251190186 Validation loss 0.04339354857802391 Accuracy 0.8870000243186951\n",
      "Iteration 41990 Training loss 0.00905145425349474 Validation loss 0.04290584847331047 Accuracy 0.8883750438690186\n",
      "Iteration 42000 Training loss 0.011735158041119576 Validation loss 0.04301767796278 Accuracy 0.8876250386238098\n",
      "Iteration 42010 Training loss 0.01054096594452858 Validation loss 0.04304623603820801 Accuracy 0.8875000476837158\n",
      "Iteration 42020 Training loss 0.01474897563457489 Validation loss 0.04750680923461914 Accuracy 0.8792500495910645\n",
      "Iteration 42030 Training loss 0.010989870876073837 Validation loss 0.042889222502708435 Accuracy 0.8888750672340393\n",
      "Iteration 42040 Training loss 0.030028993263840675 Validation loss 0.056172728538513184 Accuracy 0.858625054359436\n",
      "Iteration 42050 Training loss 0.03886864706873894 Validation loss 0.06321799755096436 Accuracy 0.8382500410079956\n",
      "Iteration 42060 Training loss 0.009958513081073761 Validation loss 0.0452379435300827 Accuracy 0.8853750228881836\n",
      "Iteration 42070 Training loss 0.007553196977823973 Validation loss 0.04299071803689003 Accuracy 0.8885000348091125\n",
      "Iteration 42080 Training loss 0.011701376177370548 Validation loss 0.0433354526758194 Accuracy 0.8895000219345093\n",
      "Iteration 42090 Training loss 0.007861574180424213 Validation loss 0.04280092939734459 Accuracy 0.8895000219345093\n",
      "Iteration 42100 Training loss 0.013342044316232204 Validation loss 0.04328793287277222 Accuracy 0.8875000476837158\n",
      "Iteration 42110 Training loss 0.00935995951294899 Validation loss 0.043043602257966995 Accuracy 0.8877500295639038\n",
      "Iteration 42120 Training loss 0.00859032291918993 Validation loss 0.04300004616379738 Accuracy 0.8880000710487366\n",
      "Iteration 42130 Training loss 0.010868559591472149 Validation loss 0.04306131601333618 Accuracy 0.8875000476837158\n",
      "Iteration 42140 Training loss 0.06298887729644775 Validation loss 0.08207106590270996 Accuracy 0.7991250157356262\n",
      "Iteration 42150 Training loss 0.00540488725528121 Validation loss 0.04315253719687462 Accuracy 0.8886250257492065\n",
      "Iteration 42160 Training loss 0.009010926820337772 Validation loss 0.044219255447387695 Accuracy 0.8858750462532043\n",
      "Iteration 42170 Training loss 0.00945581030100584 Validation loss 0.04418851062655449 Accuracy 0.8845000267028809\n",
      "Iteration 42180 Training loss 0.016096701845526695 Validation loss 0.043540894985198975 Accuracy 0.8888750672340393\n",
      "Iteration 42190 Training loss 0.01321022491902113 Validation loss 0.04316481947898865 Accuracy 0.8872500658035278\n",
      "Iteration 42200 Training loss 0.013773975893855095 Validation loss 0.04304121807217598 Accuracy 0.8888750672340393\n",
      "Iteration 42210 Training loss 0.011665468104183674 Validation loss 0.043000493198633194 Accuracy 0.8878750205039978\n",
      "Iteration 42220 Training loss 0.009433852508664131 Validation loss 0.04619823023676872 Accuracy 0.8800000548362732\n",
      "Iteration 42230 Training loss 0.011920244432985783 Validation loss 0.0462707094848156 Accuracy 0.8810000419616699\n",
      "Iteration 42240 Training loss 0.01148943044245243 Validation loss 0.046817123889923096 Accuracy 0.8817500472068787\n",
      "Iteration 42250 Training loss 0.0033814709167927504 Validation loss 0.0437026247382164 Accuracy 0.8868750333786011\n",
      "Iteration 42260 Training loss 0.008853803388774395 Validation loss 0.04334867373108864 Accuracy 0.8877500295639038\n",
      "Iteration 42270 Training loss 0.006871738936752081 Validation loss 0.04323987662792206 Accuracy 0.8881250619888306\n",
      "Iteration 42280 Training loss 0.007043185643851757 Validation loss 0.04321269318461418 Accuracy 0.8883750438690186\n",
      "Iteration 42290 Training loss 0.013482151553034782 Validation loss 0.0430728904902935 Accuracy 0.8865000605583191\n",
      "Iteration 42300 Training loss 0.01066957414150238 Validation loss 0.04319841414690018 Accuracy 0.8877500295639038\n",
      "Iteration 42310 Training loss 0.009157347492873669 Validation loss 0.04410318285226822 Accuracy 0.8857500553131104\n",
      "Iteration 42320 Training loss 0.008332034572958946 Validation loss 0.04330998286604881 Accuracy 0.8870000243186951\n",
      "Iteration 42330 Training loss 0.014341399073600769 Validation loss 0.043248437345027924 Accuracy 0.8880000710487366\n",
      "Iteration 42340 Training loss 0.00849577784538269 Validation loss 0.04362965002655983 Accuracy 0.8881250619888306\n",
      "Iteration 42350 Training loss 0.0108011644333601 Validation loss 0.04445024952292442 Accuracy 0.8841250538825989\n",
      "Iteration 42360 Training loss 0.011392292566597462 Validation loss 0.04395240917801857 Accuracy 0.8867500424385071\n",
      "Iteration 42370 Training loss 0.009625537320971489 Validation loss 0.04285486042499542 Accuracy 0.8881250619888306\n",
      "Iteration 42380 Training loss 0.01381378248333931 Validation loss 0.04302068427205086 Accuracy 0.8877500295639038\n",
      "Iteration 42390 Training loss 0.009873970411717892 Validation loss 0.04469777271151543 Accuracy 0.8835000395774841\n",
      "Iteration 42400 Training loss 0.014203152619302273 Validation loss 0.04445735365152359 Accuracy 0.8845000267028809\n",
      "Iteration 42410 Training loss 0.010663281194865704 Validation loss 0.04632004722952843 Accuracy 0.8842500448226929\n",
      "Iteration 42420 Training loss 0.012725009582936764 Validation loss 0.04327869042754173 Accuracy 0.8880000710487366\n",
      "Iteration 42430 Training loss 0.01817394606769085 Validation loss 0.04790636524558067 Accuracy 0.8787500262260437\n",
      "Iteration 42440 Training loss 0.010347634553909302 Validation loss 0.046237923204898834 Accuracy 0.8808750510215759\n",
      "Iteration 42450 Training loss 0.01333678886294365 Validation loss 0.0431019589304924 Accuracy 0.8886250257492065\n",
      "Iteration 42460 Training loss 0.005472277291119099 Validation loss 0.043180011212825775 Accuracy 0.8881250619888306\n",
      "Iteration 42470 Training loss 0.009660603478550911 Validation loss 0.043011169880628586 Accuracy 0.8878750205039978\n",
      "Iteration 42480 Training loss 0.010722740553319454 Validation loss 0.0450519435107708 Accuracy 0.8823750615119934\n",
      "Iteration 42490 Training loss 0.009838947094976902 Validation loss 0.04391084983944893 Accuracy 0.8873750567436218\n",
      "Iteration 42500 Training loss 0.004007281735539436 Validation loss 0.043347567319869995 Accuracy 0.8873750567436218\n",
      "Iteration 42510 Training loss 0.011221927590668201 Validation loss 0.044183336198329926 Accuracy 0.8855000138282776\n",
      "Iteration 42520 Training loss 0.017573121935129166 Validation loss 0.04859533905982971 Accuracy 0.8740000128746033\n",
      "Iteration 42530 Training loss 0.025262832641601562 Validation loss 0.05391634255647659 Accuracy 0.8645000457763672\n",
      "Iteration 42540 Training loss 0.010575666092336178 Validation loss 0.043599970638751984 Accuracy 0.8887500166893005\n",
      "Iteration 42550 Training loss 0.009120426140725613 Validation loss 0.04333217069506645 Accuracy 0.8886250257492065\n",
      "Iteration 42560 Training loss 0.014983179047703743 Validation loss 0.04412426799535751 Accuracy 0.8865000605583191\n",
      "Iteration 42570 Training loss 0.008415364660322666 Validation loss 0.04295656085014343 Accuracy 0.8881250619888306\n",
      "Iteration 42580 Training loss 0.00898187980055809 Validation loss 0.04370071366429329 Accuracy 0.8856250643730164\n",
      "Iteration 42590 Training loss 0.012239508330821991 Validation loss 0.04294309765100479 Accuracy 0.8892500400543213\n",
      "Iteration 42600 Training loss 0.007296410854905844 Validation loss 0.04357239603996277 Accuracy 0.8863750696182251\n",
      "Iteration 42610 Training loss 0.030527610331773758 Validation loss 0.056020550429821014 Accuracy 0.858625054359436\n",
      "Iteration 42620 Training loss 0.02181900478899479 Validation loss 0.05017612874507904 Accuracy 0.8727500438690186\n",
      "Iteration 42630 Training loss 0.010554288513958454 Validation loss 0.043258536607027054 Accuracy 0.889875054359436\n",
      "Iteration 42640 Training loss 0.014517092145979404 Validation loss 0.04324454814195633 Accuracy 0.8877500295639038\n",
      "Iteration 42650 Training loss 0.008117481134831905 Validation loss 0.043052468448877335 Accuracy 0.8887500166893005\n",
      "Iteration 42660 Training loss 0.00848885253071785 Validation loss 0.04313134774565697 Accuracy 0.8885000348091125\n",
      "Iteration 42670 Training loss 0.015376828610897064 Validation loss 0.04417894035577774 Accuracy 0.8862500190734863\n",
      "Iteration 42680 Training loss 0.014422750100493431 Validation loss 0.043069012463092804 Accuracy 0.8882500529289246\n",
      "Iteration 42690 Training loss 0.009161156602203846 Validation loss 0.045236799865961075 Accuracy 0.8818750381469727\n",
      "Iteration 42700 Training loss 0.023683475330471992 Validation loss 0.05503831431269646 Accuracy 0.8612500429153442\n",
      "Iteration 42710 Training loss 0.012027239426970482 Validation loss 0.043148063123226166 Accuracy 0.8868750333786011\n",
      "Iteration 42720 Training loss 0.009481092914938927 Validation loss 0.043286919593811035 Accuracy 0.8870000243186951\n",
      "Iteration 42730 Training loss 0.014311135746538639 Validation loss 0.043056827038526535 Accuracy 0.8873750567436218\n",
      "Iteration 42740 Training loss 0.01147921197116375 Validation loss 0.04315587133169174 Accuracy 0.8880000710487366\n",
      "Iteration 42750 Training loss 0.010114943608641624 Validation loss 0.04301191866397858 Accuracy 0.8880000710487366\n",
      "Iteration 42760 Training loss 0.008736060932278633 Validation loss 0.043049752712249756 Accuracy 0.8877500295639038\n",
      "Iteration 42770 Training loss 0.01136845164000988 Validation loss 0.04309464246034622 Accuracy 0.8867500424385071\n",
      "Iteration 42780 Training loss 0.012594015337526798 Validation loss 0.04319803789258003 Accuracy 0.8875000476837158\n",
      "Iteration 42790 Training loss 0.008351312950253487 Validation loss 0.04271669313311577 Accuracy 0.8886250257492065\n",
      "Iteration 42800 Training loss 0.036053918302059174 Validation loss 0.0689135417342186 Accuracy 0.831125020980835\n",
      "Iteration 42810 Training loss 0.010443096980452538 Validation loss 0.04352598264813423 Accuracy 0.8873750567436218\n",
      "Iteration 42820 Training loss 0.011350265704095364 Validation loss 0.04882791265845299 Accuracy 0.8762500286102295\n",
      "Iteration 42830 Training loss 0.009753190912306309 Validation loss 0.045305054634809494 Accuracy 0.8843750357627869\n",
      "Iteration 42840 Training loss 0.032026324421167374 Validation loss 0.061930034309625626 Accuracy 0.8401250243186951\n",
      "Iteration 42850 Training loss 0.022104645147919655 Validation loss 0.0475039929151535 Accuracy 0.8783750534057617\n",
      "Iteration 42860 Training loss 0.011102582328021526 Validation loss 0.04283219948410988 Accuracy 0.8893750309944153\n",
      "Iteration 42870 Training loss 0.009167969226837158 Validation loss 0.04311187192797661 Accuracy 0.8895000219345093\n",
      "Iteration 42880 Training loss 0.009661993011832237 Validation loss 0.04309588670730591 Accuracy 0.889750063419342\n",
      "Iteration 42890 Training loss 0.011447709985077381 Validation loss 0.042646050453186035 Accuracy 0.8893750309944153\n",
      "Iteration 42900 Training loss 0.013620223850011826 Validation loss 0.04308968409895897 Accuracy 0.8881250619888306\n",
      "Iteration 42910 Training loss 0.014609768986701965 Validation loss 0.04295870289206505 Accuracy 0.8882500529289246\n",
      "Iteration 42920 Training loss 0.009343653917312622 Validation loss 0.04342162609100342 Accuracy 0.8872500658035278\n",
      "Iteration 42930 Training loss 0.006141813471913338 Validation loss 0.04311184957623482 Accuracy 0.8887500166893005\n",
      "Iteration 42940 Training loss 0.014762254431843758 Validation loss 0.043274130672216415 Accuracy 0.8867500424385071\n",
      "Iteration 42950 Training loss 0.006393156014382839 Validation loss 0.04334424436092377 Accuracy 0.8875000476837158\n",
      "Iteration 42960 Training loss 0.010526297613978386 Validation loss 0.045252542942762375 Accuracy 0.8826250433921814\n",
      "Iteration 42970 Training loss 0.01571500115096569 Validation loss 0.042802851647138596 Accuracy 0.889750063419342\n",
      "Iteration 42980 Training loss 0.009515304118394852 Validation loss 0.042703159153461456 Accuracy 0.8890000581741333\n",
      "Iteration 42990 Training loss 0.014450561255216599 Validation loss 0.04305978864431381 Accuracy 0.8866250514984131\n",
      "Iteration 43000 Training loss 0.06295697391033173 Validation loss 0.08826048672199249 Accuracy 0.7858750224113464\n",
      "Iteration 43010 Training loss 0.008399481885135174 Validation loss 0.04439753666520119 Accuracy 0.8855000138282776\n",
      "Iteration 43020 Training loss 0.009397141635417938 Validation loss 0.04349048063158989 Accuracy 0.8868750333786011\n",
      "Iteration 43030 Training loss 0.010429209098219872 Validation loss 0.043047647923231125 Accuracy 0.8885000348091125\n",
      "Iteration 43040 Training loss 0.00708191329613328 Validation loss 0.04271157458424568 Accuracy 0.8891250491142273\n",
      "Iteration 43050 Training loss 0.009916773997247219 Validation loss 0.04296846315264702 Accuracy 0.8875000476837158\n",
      "Iteration 43060 Training loss 0.017448948696255684 Validation loss 0.049125202000141144 Accuracy 0.8717500567436218\n",
      "Iteration 43070 Training loss 0.058948010206222534 Validation loss 0.0776011124253273 Accuracy 0.8110000491142273\n",
      "Iteration 43080 Training loss 0.005550080444663763 Validation loss 0.0431530699133873 Accuracy 0.8888750672340393\n",
      "Iteration 43090 Training loss 0.012228745967149734 Validation loss 0.04607703164219856 Accuracy 0.8787500262260437\n",
      "Iteration 43100 Training loss 0.014362089335918427 Validation loss 0.04386831074953079 Accuracy 0.8856250643730164\n",
      "Iteration 43110 Training loss 0.01373725663870573 Validation loss 0.04300256446003914 Accuracy 0.890375018119812\n",
      "Iteration 43120 Training loss 0.006944494787603617 Validation loss 0.04281426593661308 Accuracy 0.8888750672340393\n",
      "Iteration 43130 Training loss 0.008015778847038746 Validation loss 0.044464271515607834 Accuracy 0.8848750591278076\n",
      "Iteration 43140 Training loss 0.010400903411209583 Validation loss 0.04296543821692467 Accuracy 0.8882500529289246\n",
      "Iteration 43150 Training loss 0.009047298692166805 Validation loss 0.04283982887864113 Accuracy 0.8888750672340393\n",
      "Iteration 43160 Training loss 0.0074423751793801785 Validation loss 0.04443173483014107 Accuracy 0.8845000267028809\n",
      "Iteration 43170 Training loss 0.009778526611626148 Validation loss 0.04285379871726036 Accuracy 0.8882500529289246\n",
      "Iteration 43180 Training loss 0.010632085613906384 Validation loss 0.0430232472717762 Accuracy 0.8863750696182251\n",
      "Iteration 43190 Training loss 0.010579278692603111 Validation loss 0.04388687014579773 Accuracy 0.8857500553131104\n",
      "Iteration 43200 Training loss 0.007705669850111008 Validation loss 0.04289766028523445 Accuracy 0.8892500400543213\n",
      "Iteration 43210 Training loss 0.009067398495972157 Validation loss 0.0431048758327961 Accuracy 0.8873750567436218\n",
      "Iteration 43220 Training loss 0.0260907169431448 Validation loss 0.054097939282655716 Accuracy 0.8608750700950623\n",
      "Iteration 43230 Training loss 0.023925689980387688 Validation loss 0.05330575630068779 Accuracy 0.8662500381469727\n",
      "Iteration 43240 Training loss 0.05827242136001587 Validation loss 0.0763460248708725 Accuracy 0.8137500286102295\n",
      "Iteration 43250 Training loss 0.014230572618544102 Validation loss 0.04315438121557236 Accuracy 0.8878750205039978\n",
      "Iteration 43260 Training loss 0.010813849046826363 Validation loss 0.04309084638953209 Accuracy 0.8880000710487366\n",
      "Iteration 43270 Training loss 0.012761252000927925 Validation loss 0.043141480535268784 Accuracy 0.8886250257492065\n",
      "Iteration 43280 Training loss 0.01581992581486702 Validation loss 0.045665573328733444 Accuracy 0.8816250562667847\n",
      "Iteration 43290 Training loss 0.016928812488913536 Validation loss 0.04884485527873039 Accuracy 0.878000020980835\n",
      "Iteration 43300 Training loss 0.01245568972080946 Validation loss 0.043492671102285385 Accuracy 0.8870000243186951\n",
      "Iteration 43310 Training loss 0.009858183562755585 Validation loss 0.04343860596418381 Accuracy 0.8887500166893005\n",
      "Iteration 43320 Training loss 0.014091349206864834 Validation loss 0.04308723285794258 Accuracy 0.8883750438690186\n",
      "Iteration 43330 Training loss 0.007797052152454853 Validation loss 0.04332813248038292 Accuracy 0.8872500658035278\n",
      "Iteration 43340 Training loss 0.007167081814259291 Validation loss 0.043077800422906876 Accuracy 0.8891250491142273\n",
      "Iteration 43350 Training loss 0.0074323806911706924 Validation loss 0.04459184780716896 Accuracy 0.8852500319480896\n",
      "Iteration 43360 Training loss 0.0069123427383601665 Validation loss 0.04317818582057953 Accuracy 0.8866250514984131\n",
      "Iteration 43370 Training loss 0.01041165366768837 Validation loss 0.04314657300710678 Accuracy 0.8872500658035278\n",
      "Iteration 43380 Training loss 0.010106334462761879 Validation loss 0.044507402926683426 Accuracy 0.8840000629425049\n",
      "Iteration 43390 Training loss 0.04944717884063721 Validation loss 0.06100425869226456 Accuracy 0.8472500443458557\n",
      "Iteration 43400 Training loss 0.012112260796129704 Validation loss 0.044148653745651245 Accuracy 0.8868750333786011\n",
      "Iteration 43410 Training loss 0.01035502552986145 Validation loss 0.04488237202167511 Accuracy 0.8840000629425049\n",
      "Iteration 43420 Training loss 0.047550857067108154 Validation loss 0.07426275312900543 Accuracy 0.8168750405311584\n",
      "Iteration 43430 Training loss 0.009814934805035591 Validation loss 0.0431118868291378 Accuracy 0.8893750309944153\n",
      "Iteration 43440 Training loss 0.011557238176465034 Validation loss 0.043446239084005356 Accuracy 0.8858750462532043\n",
      "Iteration 43450 Training loss 0.005974347237497568 Validation loss 0.04348602518439293 Accuracy 0.8860000371932983\n",
      "Iteration 43460 Training loss 0.011766073293983936 Validation loss 0.04311809688806534 Accuracy 0.8880000710487366\n",
      "Iteration 43470 Training loss 0.01903136819601059 Validation loss 0.04723171144723892 Accuracy 0.8792500495910645\n",
      "Iteration 43480 Training loss 0.005152830854058266 Validation loss 0.04306454211473465 Accuracy 0.8878750205039978\n",
      "Iteration 43490 Training loss 0.012708687223494053 Validation loss 0.04272856563329697 Accuracy 0.8895000219345093\n",
      "Iteration 43500 Training loss 0.007015541195869446 Validation loss 0.04282669350504875 Accuracy 0.8882500529289246\n",
      "Iteration 43510 Training loss 0.007926925085484982 Validation loss 0.04282746836543083 Accuracy 0.8883750438690186\n",
      "Iteration 43520 Training loss 0.00928801391273737 Validation loss 0.04430346190929413 Accuracy 0.8843750357627869\n",
      "Iteration 43530 Training loss 0.006950349546968937 Validation loss 0.04289524257183075 Accuracy 0.8872500658035278\n",
      "Iteration 43540 Training loss 0.007730929646641016 Validation loss 0.043269895017147064 Accuracy 0.8882500529289246\n",
      "Iteration 43550 Training loss 0.014101330190896988 Validation loss 0.04550173878669739 Accuracy 0.8821250200271606\n",
      "Iteration 43560 Training loss 0.0063111144118011 Validation loss 0.04332505911588669 Accuracy 0.8876250386238098\n",
      "Iteration 43570 Training loss 0.028078097850084305 Validation loss 0.05672389641404152 Accuracy 0.8520000576972961\n",
      "Iteration 43580 Training loss 0.009075096808373928 Validation loss 0.044513460248708725 Accuracy 0.8870000243186951\n",
      "Iteration 43590 Training loss 0.013563480228185654 Validation loss 0.04408997669816017 Accuracy 0.8857500553131104\n",
      "Iteration 43600 Training loss 0.008982800878584385 Validation loss 0.04311233013868332 Accuracy 0.8866250514984131\n",
      "Iteration 43610 Training loss 0.007569358218461275 Validation loss 0.04303237423300743 Accuracy 0.8872500658035278\n",
      "Iteration 43620 Training loss 0.012905455194413662 Validation loss 0.04636102542281151 Accuracy 0.8853750228881836\n",
      "Iteration 43630 Training loss 0.008535990491509438 Validation loss 0.04300941526889801 Accuracy 0.8887500166893005\n",
      "Iteration 43640 Training loss 0.0088307224214077 Validation loss 0.04301753267645836 Accuracy 0.8876250386238098\n",
      "Iteration 43650 Training loss 0.007375070359557867 Validation loss 0.04311934486031532 Accuracy 0.8872500658035278\n",
      "Iteration 43660 Training loss 0.012786569073796272 Validation loss 0.04304587095975876 Accuracy 0.889875054359436\n",
      "Iteration 43670 Training loss 0.010508173145353794 Validation loss 0.043509338051080704 Accuracy 0.8885000348091125\n",
      "Iteration 43680 Training loss 0.013395961374044418 Validation loss 0.045883290469646454 Accuracy 0.8831250667572021\n",
      "Iteration 43690 Training loss 0.01277384627610445 Validation loss 0.04459797590970993 Accuracy 0.8851250410079956\n",
      "Iteration 43700 Training loss 0.012115266174077988 Validation loss 0.04875696077942848 Accuracy 0.8757500648498535\n",
      "Iteration 43710 Training loss 0.0123462388291955 Validation loss 0.0437164269387722 Accuracy 0.8860000371932983\n",
      "Iteration 43720 Training loss 0.0059843542985618114 Validation loss 0.04306410253047943 Accuracy 0.8878750205039978\n",
      "Iteration 43730 Training loss 0.012065351009368896 Validation loss 0.04425521567463875 Accuracy 0.8843750357627869\n",
      "Iteration 43740 Training loss 0.014674325473606586 Validation loss 0.049382809549570084 Accuracy 0.8732500672340393\n",
      "Iteration 43750 Training loss 0.010810835286974907 Validation loss 0.04292432591319084 Accuracy 0.8891250491142273\n",
      "Iteration 43760 Training loss 0.012921962887048721 Validation loss 0.043199922889471054 Accuracy 0.8862500190734863\n",
      "Iteration 43770 Training loss 0.007778949104249477 Validation loss 0.043291956186294556 Accuracy 0.8868750333786011\n",
      "Iteration 43780 Training loss 0.012613286264240742 Validation loss 0.04380648955702782 Accuracy 0.8880000710487366\n",
      "Iteration 43790 Training loss 0.010486791841685772 Validation loss 0.04360811412334442 Accuracy 0.8867500424385071\n",
      "Iteration 43800 Training loss 0.013050435110926628 Validation loss 0.04554813727736473 Accuracy 0.8821250200271606\n",
      "Iteration 43810 Training loss 0.009892283007502556 Validation loss 0.0435350276529789 Accuracy 0.8866250514984131\n",
      "Iteration 43820 Training loss 0.008644611574709415 Validation loss 0.04322275519371033 Accuracy 0.8873750567436218\n",
      "Iteration 43830 Training loss 0.012540052644908428 Validation loss 0.04733932390809059 Accuracy 0.8790000677108765\n",
      "Iteration 43840 Training loss 0.006850913632661104 Validation loss 0.04462380334734917 Accuracy 0.8851250410079956\n",
      "Iteration 43850 Training loss 0.009200655855238438 Validation loss 0.04394375905394554 Accuracy 0.8847500681877136\n",
      "Iteration 43860 Training loss 0.011178672313690186 Validation loss 0.04496889188885689 Accuracy 0.8821250200271606\n",
      "Iteration 43870 Training loss 0.008422242477536201 Validation loss 0.04545363038778305 Accuracy 0.8810000419616699\n",
      "Iteration 43880 Training loss 0.013095754198729992 Validation loss 0.04640383645892143 Accuracy 0.8797500133514404\n",
      "Iteration 43890 Training loss 0.0126694580540061 Validation loss 0.04370427131652832 Accuracy 0.8865000605583191\n",
      "Iteration 43900 Training loss 0.005117082968354225 Validation loss 0.04306993633508682 Accuracy 0.889875054359436\n",
      "Iteration 43910 Training loss 0.01019109133630991 Validation loss 0.04296308755874634 Accuracy 0.8888750672340393\n",
      "Iteration 43920 Training loss 0.009198320098221302 Validation loss 0.04395058751106262 Accuracy 0.8855000138282776\n",
      "Iteration 43930 Training loss 0.008771899156272411 Validation loss 0.043877169489860535 Accuracy 0.8858750462532043\n",
      "Iteration 43940 Training loss 0.010999433696269989 Validation loss 0.04739637300372124 Accuracy 0.8785000443458557\n",
      "Iteration 43950 Training loss 0.014185931533575058 Validation loss 0.04513345658779144 Accuracy 0.8847500681877136\n",
      "Iteration 43960 Training loss 0.013841783627867699 Validation loss 0.04320569336414337 Accuracy 0.8880000710487366\n",
      "Iteration 43970 Training loss 0.00874367356300354 Validation loss 0.04311603680253029 Accuracy 0.8873750567436218\n",
      "Iteration 43980 Training loss 0.03107624500989914 Validation loss 0.05565841123461723 Accuracy 0.8605000376701355\n",
      "Iteration 43990 Training loss 0.010701439343392849 Validation loss 0.04398094490170479 Accuracy 0.8875000476837158\n",
      "Iteration 44000 Training loss 0.004861753433942795 Validation loss 0.043448854237794876 Accuracy 0.8877500295639038\n",
      "Iteration 44010 Training loss 0.008671006187796593 Validation loss 0.04428406059741974 Accuracy 0.8865000605583191\n",
      "Iteration 44020 Training loss 0.005625996273010969 Validation loss 0.042971350252628326 Accuracy 0.8885000348091125\n",
      "Iteration 44030 Training loss 0.011341380886733532 Validation loss 0.04300544038414955 Accuracy 0.8883750438690186\n",
      "Iteration 44040 Training loss 0.013966941274702549 Validation loss 0.04669259488582611 Accuracy 0.8805000185966492\n",
      "Iteration 44050 Training loss 0.007296865805983543 Validation loss 0.043903812766075134 Accuracy 0.8848750591278076\n",
      "Iteration 44060 Training loss 0.008719841949641705 Validation loss 0.04416169971227646 Accuracy 0.8841250538825989\n",
      "Iteration 44070 Training loss 0.03921299800276756 Validation loss 0.06372509151697159 Accuracy 0.8400000333786011\n",
      "Iteration 44080 Training loss 0.05926719680428505 Validation loss 0.06856881827116013 Accuracy 0.831125020980835\n",
      "Iteration 44090 Training loss 0.009184537455439568 Validation loss 0.04353328049182892 Accuracy 0.8876250386238098\n",
      "Iteration 44100 Training loss 0.012300841510295868 Validation loss 0.04303247109055519 Accuracy 0.8876250386238098\n",
      "Iteration 44110 Training loss 0.008501322008669376 Validation loss 0.04307849332690239 Accuracy 0.8880000710487366\n",
      "Iteration 44120 Training loss 0.010670511052012444 Validation loss 0.04308284446597099 Accuracy 0.8880000710487366\n",
      "Iteration 44130 Training loss 0.007384073454886675 Validation loss 0.04311249032616615 Accuracy 0.8876250386238098\n",
      "Iteration 44140 Training loss 0.008048641495406628 Validation loss 0.04382150620222092 Accuracy 0.8857500553131104\n",
      "Iteration 44150 Training loss 0.010623716749250889 Validation loss 0.0428699292242527 Accuracy 0.8883750438690186\n",
      "Iteration 44160 Training loss 0.006595280487090349 Validation loss 0.04306456074118614 Accuracy 0.8887500166893005\n",
      "Iteration 44170 Training loss 0.01071906741708517 Validation loss 0.04317089170217514 Accuracy 0.8881250619888306\n",
      "Iteration 44180 Training loss 0.008613607846200466 Validation loss 0.043158840388059616 Accuracy 0.8896250128746033\n",
      "Iteration 44190 Training loss 0.012555613182485104 Validation loss 0.04467329755425453 Accuracy 0.8837500214576721\n",
      "Iteration 44200 Training loss 0.008114789612591267 Validation loss 0.04322250559926033 Accuracy 0.8868750333786011\n",
      "Iteration 44210 Training loss 0.01039907056838274 Validation loss 0.04369785264134407 Accuracy 0.8865000605583191\n",
      "Iteration 44220 Training loss 0.008983558043837547 Validation loss 0.04351883754134178 Accuracy 0.8861250281333923\n",
      "Iteration 44230 Training loss 0.010264410637319088 Validation loss 0.04378087818622589 Accuracy 0.8851250410079956\n",
      "Iteration 44240 Training loss 0.010867645032703876 Validation loss 0.043207231909036636 Accuracy 0.8868750333786011\n",
      "Iteration 44250 Training loss 0.01190623827278614 Validation loss 0.045589715242385864 Accuracy 0.8826250433921814\n",
      "Iteration 44260 Training loss 0.02372599020600319 Validation loss 0.05690141022205353 Accuracy 0.8543750643730164\n",
      "Iteration 44270 Training loss 0.008924293331801891 Validation loss 0.04326711595058441 Accuracy 0.8890000581741333\n",
      "Iteration 44280 Training loss 0.016539614647626877 Validation loss 0.046696390956640244 Accuracy 0.8792500495910645\n",
      "Iteration 44290 Training loss 0.017917295917868614 Validation loss 0.04885222390294075 Accuracy 0.8763750195503235\n",
      "Iteration 44300 Training loss 0.009029228240251541 Validation loss 0.04349594935774803 Accuracy 0.8880000710487366\n",
      "Iteration 44310 Training loss 0.0050245970487594604 Validation loss 0.043583162128925323 Accuracy 0.8867500424385071\n",
      "Iteration 44320 Training loss 0.006351956631988287 Validation loss 0.04303031414747238 Accuracy 0.889750063419342\n",
      "Iteration 44330 Training loss 0.007190241478383541 Validation loss 0.04331592097878456 Accuracy 0.889875054359436\n",
      "Iteration 44340 Training loss 0.009336075745522976 Validation loss 0.04295746982097626 Accuracy 0.8890000581741333\n",
      "Iteration 44350 Training loss 0.009519333951175213 Validation loss 0.043186552822589874 Accuracy 0.8877500295639038\n",
      "Iteration 44360 Training loss 0.00894130952656269 Validation loss 0.043444421142339706 Accuracy 0.8858750462532043\n",
      "Iteration 44370 Training loss 0.010981593281030655 Validation loss 0.0434764102101326 Accuracy 0.8882500529289246\n",
      "Iteration 44380 Training loss 0.009376203641295433 Validation loss 0.044075023382902145 Accuracy 0.8841250538825989\n",
      "Iteration 44390 Training loss 0.006226427387446165 Validation loss 0.04307565465569496 Accuracy 0.8873750567436218\n",
      "Iteration 44400 Training loss 0.01413652766495943 Validation loss 0.044975459575653076 Accuracy 0.8831250667572021\n",
      "Iteration 44410 Training loss 0.005204801447689533 Validation loss 0.04343872889876366 Accuracy 0.8888750672340393\n",
      "Iteration 44420 Training loss 0.010395601391792297 Validation loss 0.04299992695450783 Accuracy 0.8876250386238098\n",
      "Iteration 44430 Training loss 0.010894473642110825 Validation loss 0.04546572268009186 Accuracy 0.8807500600814819\n",
      "Iteration 44440 Training loss 0.007189334370195866 Validation loss 0.04379913583397865 Accuracy 0.8856250643730164\n",
      "Iteration 44450 Training loss 0.00880203302949667 Validation loss 0.04640074446797371 Accuracy 0.8790000677108765\n",
      "Iteration 44460 Training loss 0.0119849294424057 Validation loss 0.04593264311552048 Accuracy 0.8841250538825989\n",
      "Iteration 44470 Training loss 0.007291116751730442 Validation loss 0.043159037828445435 Accuracy 0.8890000581741333\n",
      "Iteration 44480 Training loss 0.007286368403583765 Validation loss 0.043060366064310074 Accuracy 0.8893750309944153\n",
      "Iteration 44490 Training loss 0.011301589198410511 Validation loss 0.04353880509734154 Accuracy 0.8872500658035278\n",
      "Iteration 44500 Training loss 0.007876095362007618 Validation loss 0.04360247030854225 Accuracy 0.8860000371932983\n",
      "Iteration 44510 Training loss 0.010779534466564655 Validation loss 0.04387974739074707 Accuracy 0.8863750696182251\n",
      "Iteration 44520 Training loss 0.012045052833855152 Validation loss 0.04307207837700844 Accuracy 0.889875054359436\n",
      "Iteration 44530 Training loss 0.012964440509676933 Validation loss 0.04294124245643616 Accuracy 0.8893750309944153\n",
      "Iteration 44540 Training loss 0.006449372507631779 Validation loss 0.04375508427619934 Accuracy 0.8862500190734863\n",
      "Iteration 44550 Training loss 0.01928752288222313 Validation loss 0.050654612481594086 Accuracy 0.8702500462532043\n",
      "Iteration 44560 Training loss 0.007218868471682072 Validation loss 0.046181000769138336 Accuracy 0.8835000395774841\n",
      "Iteration 44570 Training loss 0.005162996705621481 Validation loss 0.04321858659386635 Accuracy 0.8875000476837158\n",
      "Iteration 44580 Training loss 0.021433234214782715 Validation loss 0.05286036431789398 Accuracy 0.8633750677108765\n",
      "Iteration 44590 Training loss 0.007639218587428331 Validation loss 0.043198153376579285 Accuracy 0.8886250257492065\n",
      "Iteration 44600 Training loss 0.00819994043558836 Validation loss 0.04296271130442619 Accuracy 0.8882500529289246\n",
      "Iteration 44610 Training loss 0.010355274192988873 Validation loss 0.042676106095314026 Accuracy 0.889750063419342\n",
      "Iteration 44620 Training loss 0.010376326739788055 Validation loss 0.043662987649440765 Accuracy 0.8880000710487366\n",
      "Iteration 44630 Training loss 0.01171813253313303 Validation loss 0.042930711060762405 Accuracy 0.8885000348091125\n",
      "Iteration 44640 Training loss 0.008627439849078655 Validation loss 0.04308708384633064 Accuracy 0.8892500400543213\n",
      "Iteration 44650 Training loss 0.007651591207832098 Validation loss 0.043313849717378616 Accuracy 0.8875000476837158\n",
      "Iteration 44660 Training loss 0.010660827159881592 Validation loss 0.04329845309257507 Accuracy 0.8878750205039978\n",
      "Iteration 44670 Training loss 0.009733669459819794 Validation loss 0.04538612812757492 Accuracy 0.8837500214576721\n",
      "Iteration 44680 Training loss 0.02824542485177517 Validation loss 0.059891100972890854 Accuracy 0.8507500290870667\n",
      "Iteration 44690 Training loss 0.00679326755926013 Validation loss 0.04390319436788559 Accuracy 0.8870000243186951\n",
      "Iteration 44700 Training loss 0.010091015137732029 Validation loss 0.04319099709391594 Accuracy 0.8875000476837158\n",
      "Iteration 44710 Training loss 0.012776249088346958 Validation loss 0.044586922973394394 Accuracy 0.8836250305175781\n",
      "Iteration 44720 Training loss 0.08351176977157593 Validation loss 0.09246693551540375 Accuracy 0.7755000591278076\n",
      "Iteration 44730 Training loss 0.005689548794180155 Validation loss 0.0431513749063015 Accuracy 0.8886250257492065\n",
      "Iteration 44740 Training loss 0.021238382905721664 Validation loss 0.05520201474428177 Accuracy 0.862250030040741\n",
      "Iteration 44750 Training loss 0.005484590772539377 Validation loss 0.045069679617881775 Accuracy 0.8856250643730164\n",
      "Iteration 44760 Training loss 0.012886974960565567 Validation loss 0.04323806241154671 Accuracy 0.8876250386238098\n",
      "Iteration 44770 Training loss 0.012692236341536045 Validation loss 0.04321041703224182 Accuracy 0.8877500295639038\n",
      "Iteration 44780 Training loss 0.00688295578584075 Validation loss 0.04286906495690346 Accuracy 0.8881250619888306\n",
      "Iteration 44790 Training loss 0.010111021809279919 Validation loss 0.04563628509640694 Accuracy 0.8820000290870667\n",
      "Iteration 44800 Training loss 0.008061995729804039 Validation loss 0.04306399077177048 Accuracy 0.8856250643730164\n",
      "Iteration 44810 Training loss 0.005780765786767006 Validation loss 0.04324162378907204 Accuracy 0.8887500166893005\n",
      "Iteration 44820 Training loss 0.005638513248413801 Validation loss 0.04333725944161415 Accuracy 0.8877500295639038\n",
      "Iteration 44830 Training loss 0.008885628543794155 Validation loss 0.04351580888032913 Accuracy 0.8871250152587891\n",
      "Iteration 44840 Training loss 0.01155821979045868 Validation loss 0.043138936161994934 Accuracy 0.8882500529289246\n",
      "Iteration 44850 Training loss 0.01644011214375496 Validation loss 0.046025797724723816 Accuracy 0.8863750696182251\n",
      "Iteration 44860 Training loss 0.009546980261802673 Validation loss 0.04334234818816185 Accuracy 0.8881250619888306\n",
      "Iteration 44870 Training loss 0.00892358273267746 Validation loss 0.04357228800654411 Accuracy 0.8867500424385071\n",
      "Iteration 44880 Training loss 0.012406323105096817 Validation loss 0.04409855976700783 Accuracy 0.8861250281333923\n",
      "Iteration 44890 Training loss 0.013087877072393894 Validation loss 0.04319451376795769 Accuracy 0.8878750205039978\n",
      "Iteration 44900 Training loss 0.010409662500023842 Validation loss 0.04322361201047897 Accuracy 0.8868750333786011\n",
      "Iteration 44910 Training loss 0.0085422582924366 Validation loss 0.04334520176053047 Accuracy 0.8882500529289246\n",
      "Iteration 44920 Training loss 0.06260215491056442 Validation loss 0.08684057742357254 Accuracy 0.7916250228881836\n",
      "Iteration 44930 Training loss 0.009674069471657276 Validation loss 0.04390227794647217 Accuracy 0.889875054359436\n",
      "Iteration 44940 Training loss 0.00734314089640975 Validation loss 0.0432775504887104 Accuracy 0.8893750309944153\n",
      "Iteration 44950 Training loss 0.021927515044808388 Validation loss 0.05811084061861038 Accuracy 0.8541250228881836\n",
      "Iteration 44960 Training loss 0.04297580569982529 Validation loss 0.06845206022262573 Accuracy 0.8293750286102295\n",
      "Iteration 44970 Training loss 0.015391618944704533 Validation loss 0.04388357698917389 Accuracy 0.8853750228881836\n",
      "Iteration 44980 Training loss 0.006500148680061102 Validation loss 0.04348795488476753 Accuracy 0.8876250386238098\n",
      "Iteration 44990 Training loss 0.004681876394897699 Validation loss 0.04315889999270439 Accuracy 0.8876250386238098\n",
      "Iteration 45000 Training loss 0.007201250642538071 Validation loss 0.04337210953235626 Accuracy 0.8865000605583191\n",
      "Iteration 45010 Training loss 0.010354817844927311 Validation loss 0.043582670390605927 Accuracy 0.8876250386238098\n",
      "Iteration 45020 Training loss 0.007441429886966944 Validation loss 0.04304495453834534 Accuracy 0.8895000219345093\n",
      "Iteration 45030 Training loss 0.009148407727479935 Validation loss 0.04321238026022911 Accuracy 0.8878750205039978\n",
      "Iteration 45040 Training loss 0.005693668499588966 Validation loss 0.043112803250551224 Accuracy 0.8878750205039978\n",
      "Iteration 45050 Training loss 0.006217815913259983 Validation loss 0.043098725378513336 Accuracy 0.8878750205039978\n",
      "Iteration 45060 Training loss 0.00869693886488676 Validation loss 0.043998029083013535 Accuracy 0.8851250410079956\n",
      "Iteration 45070 Training loss 0.011852900497615337 Validation loss 0.045631762593984604 Accuracy 0.8823750615119934\n",
      "Iteration 45080 Training loss 0.046163175255060196 Validation loss 0.07308383285999298 Accuracy 0.8231250643730164\n",
      "Iteration 45090 Training loss 0.011247310787439346 Validation loss 0.04353475570678711 Accuracy 0.8888750672340393\n",
      "Iteration 45100 Training loss 0.008146132342517376 Validation loss 0.04437483102083206 Accuracy 0.8860000371932983\n",
      "Iteration 45110 Training loss 0.011642835102975368 Validation loss 0.04393204674124718 Accuracy 0.8853750228881836\n",
      "Iteration 45120 Training loss 0.00998583436012268 Validation loss 0.042930059134960175 Accuracy 0.8883750438690186\n",
      "Iteration 45130 Training loss 0.007664728444069624 Validation loss 0.04330948367714882 Accuracy 0.8863750696182251\n",
      "Iteration 45140 Training loss 0.01163607556372881 Validation loss 0.04295352101325989 Accuracy 0.8886250257492065\n",
      "Iteration 45150 Training loss 0.010933130979537964 Validation loss 0.043409064412117004 Accuracy 0.8882500529289246\n",
      "Iteration 45160 Training loss 0.018862277269363403 Validation loss 0.05088271573185921 Accuracy 0.874125063419342\n",
      "Iteration 45170 Training loss 0.009624349884688854 Validation loss 0.043415866792201996 Accuracy 0.8880000710487366\n",
      "Iteration 45180 Training loss 0.008737246505916119 Validation loss 0.04382890462875366 Accuracy 0.8862500190734863\n",
      "Iteration 45190 Training loss 0.006708171218633652 Validation loss 0.04328920319676399 Accuracy 0.8875000476837158\n",
      "Iteration 45200 Training loss 0.005615066736936569 Validation loss 0.04308633506298065 Accuracy 0.8885000348091125\n",
      "Iteration 45210 Training loss 0.00782153382897377 Validation loss 0.04484717547893524 Accuracy 0.8836250305175781\n",
      "Iteration 45220 Training loss 0.008387385867536068 Validation loss 0.04363195225596428 Accuracy 0.8873750567436218\n",
      "Iteration 45230 Training loss 0.011419428512454033 Validation loss 0.04333791509270668 Accuracy 0.8871250152587891\n",
      "Iteration 45240 Training loss 0.007576291449368 Validation loss 0.04342411831021309 Accuracy 0.8875000476837158\n",
      "Iteration 45250 Training loss 0.007332324981689453 Validation loss 0.04322536662220955 Accuracy 0.8878750205039978\n",
      "Iteration 45260 Training loss 0.027253346517682076 Validation loss 0.05332263559103012 Accuracy 0.8672500252723694\n",
      "Iteration 45270 Training loss 0.010911009274423122 Validation loss 0.0442219115793705 Accuracy 0.8877500295639038\n",
      "Iteration 45280 Training loss 0.009234941564500332 Validation loss 0.0431315116584301 Accuracy 0.8872500658035278\n",
      "Iteration 45290 Training loss 0.0062127686105668545 Validation loss 0.043374184519052505 Accuracy 0.8873750567436218\n",
      "Iteration 45300 Training loss 0.008837971836328506 Validation loss 0.043186720460653305 Accuracy 0.8866250514984131\n",
      "Iteration 45310 Training loss 0.010523459874093533 Validation loss 0.04355024918913841 Accuracy 0.8857500553131104\n",
      "Iteration 45320 Training loss 0.00951158907264471 Validation loss 0.04522065445780754 Accuracy 0.8823750615119934\n",
      "Iteration 45330 Training loss 0.05658457800745964 Validation loss 0.07461565732955933 Accuracy 0.8157500624656677\n",
      "Iteration 45340 Training loss 0.010594135150313377 Validation loss 0.044050268828868866 Accuracy 0.8866250514984131\n",
      "Iteration 45350 Training loss 0.009929535910487175 Validation loss 0.04316352307796478 Accuracy 0.8882500529289246\n",
      "Iteration 45360 Training loss 0.04251113161444664 Validation loss 0.05730399489402771 Accuracy 0.859125018119812\n",
      "Iteration 45370 Training loss 0.007414964959025383 Validation loss 0.04385640472173691 Accuracy 0.8887500166893005\n",
      "Iteration 45380 Training loss 0.01812707632780075 Validation loss 0.052148349583148956 Accuracy 0.8665000200271606\n",
      "Iteration 45390 Training loss 0.012685039080679417 Validation loss 0.044564276933670044 Accuracy 0.8871250152587891\n",
      "Iteration 45400 Training loss 0.008096810430288315 Validation loss 0.0436994731426239 Accuracy 0.8877500295639038\n",
      "Iteration 45410 Training loss 0.009233695454895496 Validation loss 0.04547706991434097 Accuracy 0.8832500576972961\n",
      "Iteration 45420 Training loss 0.007905438542366028 Validation loss 0.04323683679103851 Accuracy 0.8885000348091125\n",
      "Iteration 45430 Training loss 0.011782500892877579 Validation loss 0.04304524511098862 Accuracy 0.8878750205039978\n",
      "Iteration 45440 Training loss 0.010439473204314709 Validation loss 0.043356072157621384 Accuracy 0.8882500529289246\n",
      "Iteration 45450 Training loss 0.0051302239298820496 Validation loss 0.0432584211230278 Accuracy 0.8880000710487366\n",
      "Iteration 45460 Training loss 0.007474551443010569 Validation loss 0.04379425197839737 Accuracy 0.8878750205039978\n",
      "Iteration 45470 Training loss 0.01632709614932537 Validation loss 0.044721364974975586 Accuracy 0.8842500448226929\n",
      "Iteration 45480 Training loss 0.009284626692533493 Validation loss 0.04409387335181236 Accuracy 0.8868750333786011\n",
      "Iteration 45490 Training loss 0.010788904502987862 Validation loss 0.04319620504975319 Accuracy 0.8875000476837158\n",
      "Iteration 45500 Training loss 0.008073083125054836 Validation loss 0.04315708950161934 Accuracy 0.8882500529289246\n",
      "Iteration 45510 Training loss 0.005868690088391304 Validation loss 0.043667420744895935 Accuracy 0.8873750567436218\n",
      "Iteration 45520 Training loss 0.012307560071349144 Validation loss 0.04313259571790695 Accuracy 0.8885000348091125\n",
      "Iteration 45530 Training loss 0.009678099304437637 Validation loss 0.04461350291967392 Accuracy 0.8832500576972961\n",
      "Iteration 45540 Training loss 0.00734216533601284 Validation loss 0.043741725385189056 Accuracy 0.8850000500679016\n",
      "Iteration 45550 Training loss 0.010357075370848179 Validation loss 0.044021204113960266 Accuracy 0.8860000371932983\n",
      "Iteration 45560 Training loss 0.010987862013280392 Validation loss 0.04606283828616142 Accuracy 0.8847500681877136\n",
      "Iteration 45570 Training loss 0.013966815546154976 Validation loss 0.04499411582946777 Accuracy 0.8850000500679016\n",
      "Iteration 45580 Training loss 0.01061287708580494 Validation loss 0.04363377392292023 Accuracy 0.8881250619888306\n",
      "Iteration 45590 Training loss 0.010633754543960094 Validation loss 0.043193090707063675 Accuracy 0.8878750205039978\n",
      "Iteration 45600 Training loss 0.00772384786978364 Validation loss 0.04301317781209946 Accuracy 0.8877500295639038\n",
      "Iteration 45610 Training loss 0.01268483605235815 Validation loss 0.04473016411066055 Accuracy 0.8851250410079956\n",
      "Iteration 45620 Training loss 0.005748395342379808 Validation loss 0.04344545677304268 Accuracy 0.8877500295639038\n",
      "Iteration 45630 Training loss 0.00962609238922596 Validation loss 0.043439943343400955 Accuracy 0.8867500424385071\n",
      "Iteration 45640 Training loss 0.005601259879767895 Validation loss 0.04318287968635559 Accuracy 0.8872500658035278\n",
      "Iteration 45650 Training loss 0.004420039709657431 Validation loss 0.04327978566288948 Accuracy 0.8865000605583191\n",
      "Iteration 45660 Training loss 0.007001082878559828 Validation loss 0.04313473775982857 Accuracy 0.8881250619888306\n",
      "Iteration 45670 Training loss 0.004022911190986633 Validation loss 0.04332158342003822 Accuracy 0.8863750696182251\n",
      "Iteration 45680 Training loss 0.015248572453856468 Validation loss 0.052855417132377625 Accuracy 0.8656250238418579\n",
      "Iteration 45690 Training loss 0.01311464048922062 Validation loss 0.04610755294561386 Accuracy 0.8843750357627869\n",
      "Iteration 45700 Training loss 0.017255915328860283 Validation loss 0.04353489726781845 Accuracy 0.8872500658035278\n",
      "Iteration 45710 Training loss 0.009874505922198296 Validation loss 0.04362194985151291 Accuracy 0.8871250152587891\n",
      "Iteration 45720 Training loss 0.007252086419612169 Validation loss 0.04324059560894966 Accuracy 0.8872500658035278\n",
      "Iteration 45730 Training loss 0.009617060422897339 Validation loss 0.043967925012111664 Accuracy 0.8871250152587891\n",
      "Iteration 45740 Training loss 0.013792605139315128 Validation loss 0.04337397217750549 Accuracy 0.8881250619888306\n",
      "Iteration 45750 Training loss 0.005667399149388075 Validation loss 0.043596163392066956 Accuracy 0.8867500424385071\n",
      "Iteration 45760 Training loss 0.011250904761254787 Validation loss 0.0455501414835453 Accuracy 0.8823750615119934\n",
      "Iteration 45770 Training loss 0.0083566689863801 Validation loss 0.04334981366991997 Accuracy 0.8872500658035278\n",
      "Iteration 45780 Training loss 0.009773564524948597 Validation loss 0.04406634345650673 Accuracy 0.8871250152587891\n",
      "Iteration 45790 Training loss 0.0131555600091815 Validation loss 0.04735567420721054 Accuracy 0.8782500624656677\n",
      "Iteration 45800 Training loss 0.014980904757976532 Validation loss 0.04638415575027466 Accuracy 0.8822500705718994\n",
      "Iteration 45810 Training loss 0.018329564481973648 Validation loss 0.04839861020445824 Accuracy 0.8771250247955322\n",
      "Iteration 45820 Training loss 0.015755075961351395 Validation loss 0.04468556493520737 Accuracy 0.8835000395774841\n",
      "Iteration 45830 Training loss 0.00830788817256689 Validation loss 0.04402517154812813 Accuracy 0.8863750696182251\n",
      "Iteration 45840 Training loss 0.005093854386359453 Validation loss 0.0447736456990242 Accuracy 0.8835000395774841\n",
      "Iteration 45850 Training loss 0.010652342811226845 Validation loss 0.045550387352705 Accuracy 0.8811250329017639\n",
      "Iteration 45860 Training loss 0.020790278911590576 Validation loss 0.053939320147037506 Accuracy 0.862125039100647\n",
      "Iteration 45870 Training loss 0.01128609012812376 Validation loss 0.04475242644548416 Accuracy 0.8862500190734863\n",
      "Iteration 45880 Training loss 0.009893557988107204 Validation loss 0.04362785443663597 Accuracy 0.8888750672340393\n",
      "Iteration 45890 Training loss 0.004698121454566717 Validation loss 0.04330999776721001 Accuracy 0.8885000348091125\n",
      "Iteration 45900 Training loss 0.010449404828250408 Validation loss 0.043005600571632385 Accuracy 0.8888750672340393\n",
      "Iteration 45910 Training loss 0.005752662196755409 Validation loss 0.04332618787884712 Accuracy 0.8872500658035278\n",
      "Iteration 45920 Training loss 0.008165824227035046 Validation loss 0.043286751955747604 Accuracy 0.8880000710487366\n",
      "Iteration 45930 Training loss 0.0088194003328681 Validation loss 0.04374690353870392 Accuracy 0.8861250281333923\n",
      "Iteration 45940 Training loss 0.008081767708063126 Validation loss 0.04346069321036339 Accuracy 0.8857500553131104\n",
      "Iteration 45950 Training loss 0.003775493474677205 Validation loss 0.043167732656002045 Accuracy 0.8871250152587891\n",
      "Iteration 45960 Training loss 0.007571820635348558 Validation loss 0.044134799391031265 Accuracy 0.8880000710487366\n",
      "Iteration 45970 Training loss 0.010793290100991726 Validation loss 0.0443357452750206 Accuracy 0.8862500190734863\n",
      "Iteration 45980 Training loss 0.004962975159287453 Validation loss 0.04353567957878113 Accuracy 0.8882500529289246\n",
      "Iteration 45990 Training loss 0.01136658526957035 Validation loss 0.043172381818294525 Accuracy 0.8882500529289246\n",
      "Iteration 46000 Training loss 0.008865325711667538 Validation loss 0.043257568031549454 Accuracy 0.8881250619888306\n",
      "Iteration 46010 Training loss 0.014251839369535446 Validation loss 0.04807348921895027 Accuracy 0.8772500157356262\n",
      "Iteration 46020 Training loss 0.017627498134970665 Validation loss 0.04887542873620987 Accuracy 0.8763750195503235\n",
      "Iteration 46030 Training loss 0.00925364252179861 Validation loss 0.0434199683368206 Accuracy 0.8883750438690186\n",
      "Iteration 46040 Training loss 0.010889902710914612 Validation loss 0.04335187003016472 Accuracy 0.8882500529289246\n",
      "Iteration 46050 Training loss 0.008651933632791042 Validation loss 0.04420098662376404 Accuracy 0.8843750357627869\n",
      "Iteration 46060 Training loss 0.010984119027853012 Validation loss 0.04442279785871506 Accuracy 0.8855000138282776\n",
      "Iteration 46070 Training loss 0.008235915564000607 Validation loss 0.04353604093194008 Accuracy 0.8873750567436218\n",
      "Iteration 46080 Training loss 0.006146715953946114 Validation loss 0.04321857541799545 Accuracy 0.8887500166893005\n",
      "Iteration 46090 Training loss 0.05635158345103264 Validation loss 0.08111093938350677 Accuracy 0.7978750467300415\n",
      "Iteration 46100 Training loss 0.006480928044766188 Validation loss 0.04426800459623337 Accuracy 0.8872500658035278\n",
      "Iteration 46110 Training loss 0.009737932123243809 Validation loss 0.04357895255088806 Accuracy 0.8892500400543213\n",
      "Iteration 46120 Training loss 0.009534443728625774 Validation loss 0.043199360370635986 Accuracy 0.8881250619888306\n",
      "Iteration 46130 Training loss 0.010571703314781189 Validation loss 0.0434313602745533 Accuracy 0.8871250152587891\n",
      "Iteration 46140 Training loss 0.013333866372704506 Validation loss 0.04921252280473709 Accuracy 0.8722500205039978\n",
      "Iteration 46150 Training loss 0.009199479594826698 Validation loss 0.04445553198456764 Accuracy 0.8852500319480896\n",
      "Iteration 46160 Training loss 0.008684137836098671 Validation loss 0.04446723312139511 Accuracy 0.8865000605583191\n",
      "Iteration 46170 Training loss 0.007328615989536047 Validation loss 0.043299295008182526 Accuracy 0.8880000710487366\n",
      "Iteration 46180 Training loss 0.007885278202593327 Validation loss 0.04339395463466644 Accuracy 0.8875000476837158\n",
      "Iteration 46190 Training loss 0.008898171596229076 Validation loss 0.043685946613550186 Accuracy 0.8886250257492065\n",
      "Iteration 46200 Training loss 0.005400820169597864 Validation loss 0.043304506689310074 Accuracy 0.8877500295639038\n",
      "Iteration 46210 Training loss 0.006431281566619873 Validation loss 0.044501785188913345 Accuracy 0.8851250410079956\n",
      "Iteration 46220 Training loss 0.012095448561012745 Validation loss 0.04324691742658615 Accuracy 0.8876250386238098\n",
      "Iteration 46230 Training loss 0.010050687938928604 Validation loss 0.04625869542360306 Accuracy 0.8812500238418579\n",
      "Iteration 46240 Training loss 0.03632175922393799 Validation loss 0.059565357863903046 Accuracy 0.8543750643730164\n",
      "Iteration 46250 Training loss 0.03181635960936546 Validation loss 0.055846843868494034 Accuracy 0.859125018119812\n",
      "Iteration 46260 Training loss 0.008317721076309681 Validation loss 0.04639964550733566 Accuracy 0.8801250457763672\n",
      "Iteration 46270 Training loss 0.010020794346928596 Validation loss 0.0430736243724823 Accuracy 0.8912500143051147\n",
      "Iteration 46280 Training loss 0.013264494016766548 Validation loss 0.04362814128398895 Accuracy 0.8860000371932983\n",
      "Iteration 46290 Training loss 0.009889760054647923 Validation loss 0.04330780729651451 Accuracy 0.8880000710487366\n",
      "Iteration 46300 Training loss 0.016401352360844612 Validation loss 0.048773668706417084 Accuracy 0.8755000233650208\n",
      "Iteration 46310 Training loss 0.007652678526937962 Validation loss 0.044001899659633636 Accuracy 0.8870000243186951\n",
      "Iteration 46320 Training loss 0.00761629780754447 Validation loss 0.04338681325316429 Accuracy 0.8877500295639038\n",
      "Iteration 46330 Training loss 0.010957838967442513 Validation loss 0.04352467879652977 Accuracy 0.8880000710487366\n",
      "Iteration 46340 Training loss 0.008973434567451477 Validation loss 0.043056290596723557 Accuracy 0.890125036239624\n",
      "Iteration 46350 Training loss 0.003991123754531145 Validation loss 0.043425362557172775 Accuracy 0.8875000476837158\n",
      "Iteration 46360 Training loss 0.007754131220281124 Validation loss 0.04356435686349869 Accuracy 0.8867500424385071\n",
      "Iteration 46370 Training loss 0.008666659705340862 Validation loss 0.04312540590763092 Accuracy 0.8888750672340393\n",
      "Iteration 46380 Training loss 0.0114534180611372 Validation loss 0.0433213971555233 Accuracy 0.8880000710487366\n",
      "Iteration 46390 Training loss 0.008333703503012657 Validation loss 0.04343992844223976 Accuracy 0.8873750567436218\n",
      "Iteration 46400 Training loss 0.016088509932160378 Validation loss 0.051148414611816406 Accuracy 0.8722500205039978\n",
      "Iteration 46410 Training loss 0.012655884958803654 Validation loss 0.04404424875974655 Accuracy 0.8880000710487366\n",
      "Iteration 46420 Training loss 0.025776950642466545 Validation loss 0.05392186716198921 Accuracy 0.8636250495910645\n",
      "Iteration 46430 Training loss 0.01076139323413372 Validation loss 0.043752945959568024 Accuracy 0.8862500190734863\n",
      "Iteration 46440 Training loss 0.00934214610606432 Validation loss 0.043259792029857635 Accuracy 0.8882500529289246\n",
      "Iteration 46450 Training loss 0.009004934690892696 Validation loss 0.04346230626106262 Accuracy 0.8870000243186951\n",
      "Iteration 46460 Training loss 0.0069995056837797165 Validation loss 0.04338211193680763 Accuracy 0.8863750696182251\n",
      "Iteration 46470 Training loss 0.006294153165072203 Validation loss 0.04420765861868858 Accuracy 0.8851250410079956\n",
      "Iteration 46480 Training loss 0.012830552645027637 Validation loss 0.04375462606549263 Accuracy 0.8861250281333923\n",
      "Iteration 46490 Training loss 0.005343534052371979 Validation loss 0.04451660439372063 Accuracy 0.8852500319480896\n",
      "Iteration 46500 Training loss 0.008173663169145584 Validation loss 0.04375142976641655 Accuracy 0.8853750228881836\n",
      "Iteration 46510 Training loss 0.007766833063215017 Validation loss 0.04343017190694809 Accuracy 0.8871250152587891\n",
      "Iteration 46520 Training loss 0.00866162683814764 Validation loss 0.04461030662059784 Accuracy 0.8840000629425049\n",
      "Iteration 46530 Training loss 0.008939552120864391 Validation loss 0.04354529082775116 Accuracy 0.8867500424385071\n",
      "Iteration 46540 Training loss 0.004146974999457598 Validation loss 0.04422159492969513 Accuracy 0.8851250410079956\n",
      "Iteration 46550 Training loss 0.013071900233626366 Validation loss 0.04354654252529144 Accuracy 0.8873750567436218\n",
      "Iteration 46560 Training loss 0.008057838305830956 Validation loss 0.043922483921051025 Accuracy 0.8876250386238098\n",
      "Iteration 46570 Training loss 0.005952501203864813 Validation loss 0.043338969349861145 Accuracy 0.8865000605583191\n",
      "Iteration 46580 Training loss 0.00987223256379366 Validation loss 0.04383173957467079 Accuracy 0.8858750462532043\n",
      "Iteration 46590 Training loss 0.02161632478237152 Validation loss 0.049737535417079926 Accuracy 0.8760000467300415\n",
      "Iteration 46600 Training loss 0.01064429059624672 Validation loss 0.043893616646528244 Accuracy 0.8876250386238098\n",
      "Iteration 46610 Training loss 0.011886313557624817 Validation loss 0.046589285135269165 Accuracy 0.8802500367164612\n",
      "Iteration 46620 Training loss 0.007027324289083481 Validation loss 0.043263960629701614 Accuracy 0.8892500400543213\n",
      "Iteration 46630 Training loss 0.011374231427907944 Validation loss 0.04352286830544472 Accuracy 0.8877500295639038\n",
      "Iteration 46640 Training loss 0.007509042043238878 Validation loss 0.04362209886312485 Accuracy 0.8881250619888306\n",
      "Iteration 46650 Training loss 0.015043462626636028 Validation loss 0.04598986357450485 Accuracy 0.8818750381469727\n",
      "Iteration 46660 Training loss 0.013155488297343254 Validation loss 0.04689883813261986 Accuracy 0.8832500576972961\n",
      "Iteration 46670 Training loss 0.007604129146784544 Validation loss 0.04459889233112335 Accuracy 0.8848750591278076\n",
      "Iteration 46680 Training loss 0.011629164218902588 Validation loss 0.04379599913954735 Accuracy 0.8877500295639038\n",
      "Iteration 46690 Training loss 0.00866983737796545 Validation loss 0.04351028800010681 Accuracy 0.8885000348091125\n",
      "Iteration 46700 Training loss 0.007704898715019226 Validation loss 0.04383618384599686 Accuracy 0.8871250152587891\n",
      "Iteration 46710 Training loss 0.011174089275300503 Validation loss 0.04360407963395119 Accuracy 0.8876250386238098\n",
      "Iteration 46720 Training loss 0.008620262145996094 Validation loss 0.04367101192474365 Accuracy 0.8883750438690186\n",
      "Iteration 46730 Training loss 0.008821862749755383 Validation loss 0.043919939547777176 Accuracy 0.8863750696182251\n",
      "Iteration 46740 Training loss 0.012353458441793919 Validation loss 0.043478142470121384 Accuracy 0.8867500424385071\n",
      "Iteration 46750 Training loss 0.013261184096336365 Validation loss 0.04628878831863403 Accuracy 0.8815000653266907\n",
      "Iteration 46760 Training loss 0.027211513370275497 Validation loss 0.055852100253105164 Accuracy 0.8628750443458557\n",
      "Iteration 46770 Training loss 0.007880233228206635 Validation loss 0.04386498034000397 Accuracy 0.8873750567436218\n",
      "Iteration 46780 Training loss 0.009263589046895504 Validation loss 0.04341593757271767 Accuracy 0.8886250257492065\n",
      "Iteration 46790 Training loss 0.009955343790352345 Validation loss 0.04369387775659561 Accuracy 0.8875000476837158\n",
      "Iteration 46800 Training loss 0.006806973833590746 Validation loss 0.04329204186797142 Accuracy 0.8876250386238098\n",
      "Iteration 46810 Training loss 0.010540038347244263 Validation loss 0.04491759091615677 Accuracy 0.8851250410079956\n",
      "Iteration 46820 Training loss 0.006119503173977137 Validation loss 0.043911758810281754 Accuracy 0.8861250281333923\n",
      "Iteration 46830 Training loss 0.009722095914185047 Validation loss 0.043480463325977325 Accuracy 0.8880000710487366\n",
      "Iteration 46840 Training loss 0.005229766480624676 Validation loss 0.04332965239882469 Accuracy 0.8890000581741333\n",
      "Iteration 46850 Training loss 0.006307278294116259 Validation loss 0.044665753841400146 Accuracy 0.8847500681877136\n",
      "Iteration 46860 Training loss 0.01014510728418827 Validation loss 0.04605263099074364 Accuracy 0.8835000395774841\n",
      "Iteration 46870 Training loss 0.007382808718830347 Validation loss 0.043497100472450256 Accuracy 0.8871250152587891\n",
      "Iteration 46880 Training loss 0.010648757219314575 Validation loss 0.04355378448963165 Accuracy 0.8867500424385071\n",
      "Iteration 46890 Training loss 0.01749100349843502 Validation loss 0.04675138369202614 Accuracy 0.8793750405311584\n",
      "Iteration 46900 Training loss 0.013173653744161129 Validation loss 0.04731103777885437 Accuracy 0.8803750276565552\n",
      "Iteration 46910 Training loss 0.007992029190063477 Validation loss 0.04382007196545601 Accuracy 0.8877500295639038\n",
      "Iteration 46920 Training loss 0.010229282081127167 Validation loss 0.04340382665395737 Accuracy 0.8882500529289246\n",
      "Iteration 46930 Training loss 0.010521454736590385 Validation loss 0.04346524551510811 Accuracy 0.8867500424385071\n",
      "Iteration 46940 Training loss 0.010992209427058697 Validation loss 0.04428503289818764 Accuracy 0.8838750123977661\n",
      "Iteration 46950 Training loss 0.008378865197300911 Validation loss 0.04880034923553467 Accuracy 0.8756250143051147\n",
      "Iteration 46960 Training loss 0.011995989829301834 Validation loss 0.047122880816459656 Accuracy 0.8816250562667847\n",
      "Iteration 46970 Training loss 0.009109281934797764 Validation loss 0.04356067255139351 Accuracy 0.8878750205039978\n",
      "Iteration 46980 Training loss 0.006178137380629778 Validation loss 0.04325215891003609 Accuracy 0.8885000348091125\n",
      "Iteration 46990 Training loss 0.006769869010895491 Validation loss 0.0433642603456974 Accuracy 0.8883750438690186\n",
      "Iteration 47000 Training loss 0.007859041914343834 Validation loss 0.04338010773062706 Accuracy 0.8886250257492065\n",
      "Iteration 47010 Training loss 0.0060364180244505405 Validation loss 0.04342854768037796 Accuracy 0.8876250386238098\n",
      "Iteration 47020 Training loss 0.007339944131672382 Validation loss 0.044591065496206284 Accuracy 0.8836250305175781\n",
      "Iteration 47030 Training loss 0.006104918196797371 Validation loss 0.04357817769050598 Accuracy 0.8868750333786011\n",
      "Iteration 47040 Training loss 0.009493246674537659 Validation loss 0.043355487287044525 Accuracy 0.8888750672340393\n",
      "Iteration 47050 Training loss 0.004870929289609194 Validation loss 0.04345099255442619 Accuracy 0.8875000476837158\n",
      "Iteration 47060 Training loss 0.007775039412081242 Validation loss 0.0433528907597065 Accuracy 0.8891250491142273\n",
      "Iteration 47070 Training loss 0.007902838289737701 Validation loss 0.04383735731244087 Accuracy 0.8860000371932983\n",
      "Iteration 47080 Training loss 0.010415676981210709 Validation loss 0.04471299797296524 Accuracy 0.8840000629425049\n",
      "Iteration 47090 Training loss 0.01642593927681446 Validation loss 0.04778138920664787 Accuracy 0.8836250305175781\n",
      "Iteration 47100 Training loss 0.013340499252080917 Validation loss 0.0451253205537796 Accuracy 0.8845000267028809\n",
      "Iteration 47110 Training loss 0.007128878030925989 Validation loss 0.04430394619703293 Accuracy 0.8838750123977661\n",
      "Iteration 47120 Training loss 0.00865799281746149 Validation loss 0.04355809837579727 Accuracy 0.8876250386238098\n",
      "Iteration 47130 Training loss 0.008267818950116634 Validation loss 0.04359755292534828 Accuracy 0.8878750205039978\n",
      "Iteration 47140 Training loss 0.005819498561322689 Validation loss 0.04349825531244278 Accuracy 0.8870000243186951\n",
      "Iteration 47150 Training loss 0.00959515105932951 Validation loss 0.04417979717254639 Accuracy 0.8863750696182251\n",
      "Iteration 47160 Training loss 0.048461031168699265 Validation loss 0.07542774081230164 Accuracy 0.8138750195503235\n",
      "Iteration 47170 Training loss 0.014405869878828526 Validation loss 0.04968474060297012 Accuracy 0.8756250143051147\n",
      "Iteration 47180 Training loss 0.013216037303209305 Validation loss 0.04358069971203804 Accuracy 0.8875000476837158\n",
      "Iteration 47190 Training loss 0.010465010069310665 Validation loss 0.04428139701485634 Accuracy 0.8848750591278076\n",
      "Iteration 47200 Training loss 0.009393414482474327 Validation loss 0.043377578258514404 Accuracy 0.8886250257492065\n",
      "Iteration 47210 Training loss 0.029872292652726173 Validation loss 0.059837184846401215 Accuracy 0.8526250123977661\n",
      "Iteration 47220 Training loss 0.01212791632860899 Validation loss 0.04374391585588455 Accuracy 0.8882500529289246\n",
      "Iteration 47230 Training loss 0.004992734640836716 Validation loss 0.043202146887779236 Accuracy 0.8885000348091125\n",
      "Iteration 47240 Training loss 0.004746376536786556 Validation loss 0.04412228614091873 Accuracy 0.8836250305175781\n",
      "Iteration 47250 Training loss 0.005660392343997955 Validation loss 0.04323459044098854 Accuracy 0.8872500658035278\n",
      "Iteration 47260 Training loss 0.006509128492325544 Validation loss 0.04315214976668358 Accuracy 0.8881250619888306\n",
      "Iteration 47270 Training loss 0.006165989674627781 Validation loss 0.043347276747226715 Accuracy 0.8881250619888306\n",
      "Iteration 47280 Training loss 0.008397014811635017 Validation loss 0.04358869418501854 Accuracy 0.8863750696182251\n",
      "Iteration 47290 Training loss 0.012336828745901585 Validation loss 0.044780053198337555 Accuracy 0.8838750123977661\n",
      "Iteration 47300 Training loss 0.007401361130177975 Validation loss 0.044164493680000305 Accuracy 0.8845000267028809\n",
      "Iteration 47310 Training loss 0.008449001237750053 Validation loss 0.04336792975664139 Accuracy 0.8880000710487366\n",
      "Iteration 47320 Training loss 0.05248553305864334 Validation loss 0.074721559882164 Accuracy 0.8195000290870667\n",
      "Iteration 47330 Training loss 0.009074649773538113 Validation loss 0.04442524164915085 Accuracy 0.8872500658035278\n",
      "Iteration 47340 Training loss 0.009460318833589554 Validation loss 0.04317300021648407 Accuracy 0.8890000581741333\n",
      "Iteration 47350 Training loss 0.007217884063720703 Validation loss 0.04322177171707153 Accuracy 0.8886250257492065\n",
      "Iteration 47360 Training loss 0.008929204195737839 Validation loss 0.04560073837637901 Accuracy 0.8825000524520874\n",
      "Iteration 47370 Training loss 0.009402425028383732 Validation loss 0.0439763143658638 Accuracy 0.8870000243186951\n",
      "Iteration 47380 Training loss 0.009300129488110542 Validation loss 0.04340783879160881 Accuracy 0.8877500295639038\n",
      "Iteration 47390 Training loss 0.010015624575316906 Validation loss 0.0432957299053669 Accuracy 0.8881250619888306\n",
      "Iteration 47400 Training loss 0.009067734703421593 Validation loss 0.04313959553837776 Accuracy 0.8882500529289246\n",
      "Iteration 47410 Training loss 0.03290208801627159 Validation loss 0.05856893211603165 Accuracy 0.8510000109672546\n",
      "Iteration 47420 Training loss 0.013800324872136116 Validation loss 0.05043161287903786 Accuracy 0.8727500438690186\n",
      "Iteration 47430 Training loss 0.00816245749592781 Validation loss 0.04365963116288185 Accuracy 0.8877500295639038\n",
      "Iteration 47440 Training loss 0.00696418946608901 Validation loss 0.043371766805648804 Accuracy 0.8895000219345093\n",
      "Iteration 47450 Training loss 0.007593005895614624 Validation loss 0.043632622808218 Accuracy 0.8863750696182251\n",
      "Iteration 47460 Training loss 0.004721120931208134 Validation loss 0.04329030588269234 Accuracy 0.8891250491142273\n",
      "Iteration 47470 Training loss 0.010211784392595291 Validation loss 0.04338214918971062 Accuracy 0.8890000581741333\n",
      "Iteration 47480 Training loss 0.014837431721389294 Validation loss 0.05055506154894829 Accuracy 0.8755000233650208\n",
      "Iteration 47490 Training loss 0.008804187178611755 Validation loss 0.044170282781124115 Accuracy 0.8873750567436218\n",
      "Iteration 47500 Training loss 0.0036243204958736897 Validation loss 0.043571993708610535 Accuracy 0.8870000243186951\n",
      "Iteration 47510 Training loss 0.008519953116774559 Validation loss 0.04449671506881714 Accuracy 0.8865000605583191\n",
      "Iteration 47520 Training loss 0.013219882734119892 Validation loss 0.04358231648802757 Accuracy 0.8871250152587891\n",
      "Iteration 47530 Training loss 0.007983020506799221 Validation loss 0.045717574656009674 Accuracy 0.8813750147819519\n",
      "Iteration 47540 Training loss 0.007496001664549112 Validation loss 0.0431424118578434 Accuracy 0.8880000710487366\n",
      "Iteration 47550 Training loss 0.006827723700553179 Validation loss 0.0431542806327343 Accuracy 0.8880000710487366\n",
      "Iteration 47560 Training loss 0.010712556540966034 Validation loss 0.043277326971292496 Accuracy 0.8876250386238098\n",
      "Iteration 47570 Training loss 0.009229308925569057 Validation loss 0.04349754750728607 Accuracy 0.8866250514984131\n",
      "Iteration 47580 Training loss 0.012152590788900852 Validation loss 0.047772958874702454 Accuracy 0.8787500262260437\n",
      "Iteration 47590 Training loss 0.011863265186548233 Validation loss 0.0514150969684124 Accuracy 0.8712500333786011\n",
      "Iteration 47600 Training loss 0.006301140412688255 Validation loss 0.0437157079577446 Accuracy 0.8870000243186951\n",
      "Iteration 47610 Training loss 0.013449760153889656 Validation loss 0.04371603950858116 Accuracy 0.8876250386238098\n",
      "Iteration 47620 Training loss 0.008157051168382168 Validation loss 0.04393557086586952 Accuracy 0.8861250281333923\n",
      "Iteration 47630 Training loss 0.010511836968362331 Validation loss 0.047831423580646515 Accuracy 0.8790000677108765\n",
      "Iteration 47640 Training loss 0.009202560409903526 Validation loss 0.043732140213251114 Accuracy 0.8872500658035278\n",
      "Iteration 47650 Training loss 0.007606695871800184 Validation loss 0.04362194985151291 Accuracy 0.8860000371932983\n",
      "Iteration 47660 Training loss 0.009686574339866638 Validation loss 0.04411635175347328 Accuracy 0.8871250152587891\n",
      "Iteration 47670 Training loss 0.00757930614054203 Validation loss 0.044114693999290466 Accuracy 0.8856250643730164\n",
      "Iteration 47680 Training loss 0.007794838864356279 Validation loss 0.043732333928346634 Accuracy 0.8868750333786011\n",
      "Iteration 47690 Training loss 0.009492041543126106 Validation loss 0.044589076191186905 Accuracy 0.8855000138282776\n",
      "Iteration 47700 Training loss 0.011923904530704021 Validation loss 0.047077178955078125 Accuracy 0.8800000548362732\n",
      "Iteration 47710 Training loss 0.008245197124779224 Validation loss 0.04361295327544212 Accuracy 0.8878750205039978\n",
      "Iteration 47720 Training loss 0.007055874448269606 Validation loss 0.04407825693488121 Accuracy 0.8848750591278076\n",
      "Iteration 47730 Training loss 0.007658676709979773 Validation loss 0.043502077460289 Accuracy 0.8882500529289246\n",
      "Iteration 47740 Training loss 0.02090405859053135 Validation loss 0.052825678139925 Accuracy 0.8688750267028809\n",
      "Iteration 47750 Training loss 0.0705755203962326 Validation loss 0.08376195281744003 Accuracy 0.7956250309944153\n",
      "Iteration 47760 Training loss 0.0066938516683876514 Validation loss 0.04394873231649399 Accuracy 0.8891250491142273\n",
      "Iteration 47770 Training loss 0.008095095865428448 Validation loss 0.04420214518904686 Accuracy 0.8871250152587891\n",
      "Iteration 47780 Training loss 0.005163697991520166 Validation loss 0.04343767091631889 Accuracy 0.8861250281333923\n",
      "Iteration 47790 Training loss 0.011253712698817253 Validation loss 0.04353107511997223 Accuracy 0.8878750205039978\n",
      "Iteration 47800 Training loss 0.008381909690797329 Validation loss 0.043657489120960236 Accuracy 0.8870000243186951\n",
      "Iteration 47810 Training loss 0.011250129900872707 Validation loss 0.043491873890161514 Accuracy 0.8882500529289246\n",
      "Iteration 47820 Training loss 0.010427135974168777 Validation loss 0.04357371851801872 Accuracy 0.8866250514984131\n",
      "Iteration 47830 Training loss 0.008984046056866646 Validation loss 0.04359547421336174 Accuracy 0.8878750205039978\n",
      "Iteration 47840 Training loss 0.008058788254857063 Validation loss 0.043623607605695724 Accuracy 0.8873750567436218\n",
      "Iteration 47850 Training loss 0.008546746335923672 Validation loss 0.04480775445699692 Accuracy 0.8832500576972961\n",
      "Iteration 47860 Training loss 0.007611711975187063 Validation loss 0.0436076745390892 Accuracy 0.8882500529289246\n",
      "Iteration 47870 Training loss 0.007761118467897177 Validation loss 0.044354524463415146 Accuracy 0.8856250643730164\n",
      "Iteration 47880 Training loss 0.007938748225569725 Validation loss 0.04360389709472656 Accuracy 0.8867500424385071\n",
      "Iteration 47890 Training loss 0.010650291107594967 Validation loss 0.04357965663075447 Accuracy 0.8872500658035278\n",
      "Iteration 47900 Training loss 0.033650707453489304 Validation loss 0.061687275767326355 Accuracy 0.8445000648498535\n",
      "Iteration 47910 Training loss 0.00946506205946207 Validation loss 0.0497221015393734 Accuracy 0.8726250529289246\n",
      "Iteration 47920 Training loss 0.010966458357870579 Validation loss 0.04640913009643555 Accuracy 0.8812500238418579\n",
      "Iteration 47930 Training loss 0.007026091683655977 Validation loss 0.045080896466970444 Accuracy 0.8850000500679016\n",
      "Iteration 47940 Training loss 0.004916027188301086 Validation loss 0.04354769363999367 Accuracy 0.8880000710487366\n",
      "Iteration 47950 Training loss 0.008037353865802288 Validation loss 0.043570734560489655 Accuracy 0.8876250386238098\n",
      "Iteration 47960 Training loss 0.006828736048191786 Validation loss 0.044360194355249405 Accuracy 0.8848750591278076\n",
      "Iteration 47970 Training loss 0.006991301663219929 Validation loss 0.04356970638036728 Accuracy 0.8876250386238098\n",
      "Iteration 47980 Training loss 0.006459541153162718 Validation loss 0.04358654469251633 Accuracy 0.8886250257492065\n",
      "Iteration 47990 Training loss 0.010974428616464138 Validation loss 0.04361860081553459 Accuracy 0.8868750333786011\n",
      "Iteration 48000 Training loss 0.010336115956306458 Validation loss 0.043775781989097595 Accuracy 0.8865000605583191\n",
      "Iteration 48010 Training loss 0.008065035566687584 Validation loss 0.043797094374895096 Accuracy 0.8857500553131104\n",
      "Iteration 48020 Training loss 0.04835338145494461 Validation loss 0.07005239278078079 Accuracy 0.8321250677108765\n",
      "Iteration 48030 Training loss 0.009076683782041073 Validation loss 0.04571041092276573 Accuracy 0.8826250433921814\n",
      "Iteration 48040 Training loss 0.04713980481028557 Validation loss 0.06928911060094833 Accuracy 0.8313750624656677\n",
      "Iteration 48050 Training loss 0.00879436731338501 Validation loss 0.044226329773664474 Accuracy 0.8877500295639038\n",
      "Iteration 48060 Training loss 0.012826886028051376 Validation loss 0.043780338019132614 Accuracy 0.8885000348091125\n",
      "Iteration 48070 Training loss 0.008579269051551819 Validation loss 0.04408349469304085 Accuracy 0.8865000605583191\n",
      "Iteration 48080 Training loss 0.00714886886999011 Validation loss 0.04342765361070633 Accuracy 0.8881250619888306\n",
      "Iteration 48090 Training loss 0.005126155447214842 Validation loss 0.043423984199762344 Accuracy 0.8880000710487366\n",
      "Iteration 48100 Training loss 0.008608640171587467 Validation loss 0.04347965866327286 Accuracy 0.8885000348091125\n",
      "Iteration 48110 Training loss 0.007169917691498995 Validation loss 0.04334734380245209 Accuracy 0.8886250257492065\n",
      "Iteration 48120 Training loss 0.01116294227540493 Validation loss 0.043557070195674896 Accuracy 0.8871250152587891\n",
      "Iteration 48130 Training loss 0.008034929633140564 Validation loss 0.043766144663095474 Accuracy 0.8885000348091125\n",
      "Iteration 48140 Training loss 0.009307066909968853 Validation loss 0.043611735105514526 Accuracy 0.8870000243186951\n",
      "Iteration 48150 Training loss 0.009046670980751514 Validation loss 0.043576423078775406 Accuracy 0.8875000476837158\n",
      "Iteration 48160 Training loss 0.00845149252563715 Validation loss 0.04677772894501686 Accuracy 0.8797500133514404\n",
      "Iteration 48170 Training loss 0.006943694315850735 Validation loss 0.0435701459646225 Accuracy 0.8886250257492065\n",
      "Iteration 48180 Training loss 0.04924829304218292 Validation loss 0.08777103573083878 Accuracy 0.7913750410079956\n",
      "Iteration 48190 Training loss 0.11153846234083176 Validation loss 0.11794263124465942 Accuracy 0.7236250638961792\n",
      "Iteration 48200 Training loss 0.007086921017616987 Validation loss 0.04390154778957367 Accuracy 0.8873750567436218\n",
      "Iteration 48210 Training loss 0.009384476579725742 Validation loss 0.04428824409842491 Accuracy 0.8857500553131104\n",
      "Iteration 48220 Training loss 0.00807426031678915 Validation loss 0.043561868369579315 Accuracy 0.8873750567436218\n",
      "Iteration 48230 Training loss 0.013453688472509384 Validation loss 0.04375394806265831 Accuracy 0.8866250514984131\n",
      "Iteration 48240 Training loss 0.007044077385216951 Validation loss 0.04354299232363701 Accuracy 0.8865000605583191\n",
      "Iteration 48250 Training loss 0.006428279913961887 Validation loss 0.04370535537600517 Accuracy 0.8858750462532043\n",
      "Iteration 48260 Training loss 0.010320297442376614 Validation loss 0.043255820870399475 Accuracy 0.8876250386238098\n",
      "Iteration 48270 Training loss 0.005965104792267084 Validation loss 0.04345570504665375 Accuracy 0.8875000476837158\n",
      "Iteration 48280 Training loss 0.0033270956482738256 Validation loss 0.04373897239565849 Accuracy 0.8871250152587891\n",
      "Iteration 48290 Training loss 0.008555066771805286 Validation loss 0.0437200628221035 Accuracy 0.8882500529289246\n",
      "Iteration 48300 Training loss 0.008351333439350128 Validation loss 0.04383073374629021 Accuracy 0.8867500424385071\n",
      "Iteration 48310 Training loss 0.011334795504808426 Validation loss 0.04386269301176071 Accuracy 0.8866250514984131\n",
      "Iteration 48320 Training loss 0.00921088457107544 Validation loss 0.043523967266082764 Accuracy 0.8880000710487366\n",
      "Iteration 48330 Training loss 0.005183161236345768 Validation loss 0.04459374397993088 Accuracy 0.8860000371932983\n",
      "Iteration 48340 Training loss 0.004519538953900337 Validation loss 0.043620843440294266 Accuracy 0.8881250619888306\n",
      "Iteration 48350 Training loss 0.0091529730707407 Validation loss 0.04357082396745682 Accuracy 0.8875000476837158\n",
      "Iteration 48360 Training loss 0.03417060151696205 Validation loss 0.0663594976067543 Accuracy 0.8400000333786011\n",
      "Iteration 48370 Training loss 0.01081334799528122 Validation loss 0.04366350173950195 Accuracy 0.8896250128746033\n",
      "Iteration 48380 Training loss 0.010806122794747353 Validation loss 0.04353565350174904 Accuracy 0.8885000348091125\n",
      "Iteration 48390 Training loss 0.008357043378055096 Validation loss 0.04357394576072693 Accuracy 0.8888750672340393\n",
      "Iteration 48400 Training loss 0.0066751656122505665 Validation loss 0.04330960288643837 Accuracy 0.8893750309944153\n",
      "Iteration 48410 Training loss 0.012157195247709751 Validation loss 0.04461140185594559 Accuracy 0.8846250176429749\n",
      "Iteration 48420 Training loss 0.005168175790458918 Validation loss 0.04362897947430611 Accuracy 0.8876250386238098\n",
      "Iteration 48430 Training loss 0.046076592057943344 Validation loss 0.07332364469766617 Accuracy 0.8198750615119934\n",
      "Iteration 48440 Training loss 0.010664671659469604 Validation loss 0.04501703381538391 Accuracy 0.8861250281333923\n",
      "Iteration 48450 Training loss 0.004655058961361647 Validation loss 0.04371335729956627 Accuracy 0.8881250619888306\n",
      "Iteration 48460 Training loss 0.012758034281432629 Validation loss 0.04321000725030899 Accuracy 0.8883750438690186\n",
      "Iteration 48470 Training loss 0.006794608663767576 Validation loss 0.043333932757377625 Accuracy 0.8886250257492065\n",
      "Iteration 48480 Training loss 0.006597286090254784 Validation loss 0.0435514971613884 Accuracy 0.8872500658035278\n",
      "Iteration 48490 Training loss 0.005769827403128147 Validation loss 0.04323358088731766 Accuracy 0.8886250257492065\n",
      "Iteration 48500 Training loss 0.002386616077274084 Validation loss 0.04342509061098099 Accuracy 0.8872500658035278\n",
      "Iteration 48510 Training loss 0.008701879531145096 Validation loss 0.04440176486968994 Accuracy 0.8862500190734863\n",
      "Iteration 48520 Training loss 0.0075733778066933155 Validation loss 0.04324443265795708 Accuracy 0.8888750672340393\n",
      "Iteration 48530 Training loss 0.010721900500357151 Validation loss 0.046420734375715256 Accuracy 0.8812500238418579\n",
      "Iteration 48540 Training loss 0.008816135115921497 Validation loss 0.047666996717453 Accuracy 0.8818750381469727\n",
      "Iteration 48550 Training loss 0.012059771455824375 Validation loss 0.04554590955376625 Accuracy 0.8827500343322754\n",
      "Iteration 48560 Training loss 0.007149567361921072 Validation loss 0.04339790344238281 Accuracy 0.8887500166893005\n",
      "Iteration 48570 Training loss 0.0084663862362504 Validation loss 0.044375211000442505 Accuracy 0.8846250176429749\n",
      "Iteration 48580 Training loss 0.00822201743721962 Validation loss 0.04345889389514923 Accuracy 0.8882500529289246\n",
      "Iteration 48590 Training loss 0.006927964743226767 Validation loss 0.04358035698533058 Accuracy 0.8868750333786011\n",
      "Iteration 48600 Training loss 0.008295141160488129 Validation loss 0.043714847415685654 Accuracy 0.8873750567436218\n",
      "Iteration 48610 Training loss 0.006757027469575405 Validation loss 0.04391610249876976 Accuracy 0.8858750462532043\n",
      "Iteration 48620 Training loss 0.003259934950619936 Validation loss 0.04348137974739075 Accuracy 0.8878750205039978\n",
      "Iteration 48630 Training loss 0.010161506943404675 Validation loss 0.04371173307299614 Accuracy 0.8873750567436218\n",
      "Iteration 48640 Training loss 0.012168705463409424 Validation loss 0.04364055022597313 Accuracy 0.8882500529289246\n",
      "Iteration 48650 Training loss 0.010078659281134605 Validation loss 0.04345065355300903 Accuracy 0.8880000710487366\n",
      "Iteration 48660 Training loss 0.005372760351747274 Validation loss 0.04444094002246857 Accuracy 0.8850000500679016\n",
      "Iteration 48670 Training loss 0.003943641670048237 Validation loss 0.04390658810734749 Accuracy 0.8880000710487366\n",
      "Iteration 48680 Training loss 0.0038243597373366356 Validation loss 0.04360846057534218 Accuracy 0.8880000710487366\n",
      "Iteration 48690 Training loss 0.009272809140384197 Validation loss 0.04412547126412392 Accuracy 0.8857500553131104\n",
      "Iteration 48700 Training loss 0.006655714940279722 Validation loss 0.043800149112939835 Accuracy 0.8872500658035278\n",
      "Iteration 48710 Training loss 0.006832953076809645 Validation loss 0.04389336705207825 Accuracy 0.8863750696182251\n",
      "Iteration 48720 Training loss 0.0049194395542144775 Validation loss 0.043796833604574203 Accuracy 0.8870000243186951\n",
      "Iteration 48730 Training loss 0.005909426603466272 Validation loss 0.04359334707260132 Accuracy 0.8883750438690186\n",
      "Iteration 48740 Training loss 0.0031016350258141756 Validation loss 0.0435815192759037 Accuracy 0.8873750567436218\n",
      "Iteration 48750 Training loss 0.006952574010938406 Validation loss 0.04400734230875969 Accuracy 0.8861250281333923\n",
      "Iteration 48760 Training loss 0.011142926290631294 Validation loss 0.047132521867752075 Accuracy 0.8787500262260437\n",
      "Iteration 48770 Training loss 0.052036188542842865 Validation loss 0.0720457136631012 Accuracy 0.8221250176429749\n",
      "Iteration 48780 Training loss 0.01323838159441948 Validation loss 0.04784189164638519 Accuracy 0.8798750638961792\n",
      "Iteration 48790 Training loss 0.009648660197854042 Validation loss 0.043552592396736145 Accuracy 0.8885000348091125\n",
      "Iteration 48800 Training loss 0.0038258552085608244 Validation loss 0.043581631034612656 Accuracy 0.8887500166893005\n",
      "Iteration 48810 Training loss 0.012468023225665092 Validation loss 0.04373643547296524 Accuracy 0.8866250514984131\n",
      "Iteration 48820 Training loss 0.00624286150559783 Validation loss 0.04362965375185013 Accuracy 0.8872500658035278\n",
      "Iteration 48830 Training loss 0.007567753084003925 Validation loss 0.04342295974493027 Accuracy 0.8871250152587891\n",
      "Iteration 48840 Training loss 0.005412302445620298 Validation loss 0.04339379444718361 Accuracy 0.8878750205039978\n",
      "Iteration 48850 Training loss 0.010257062502205372 Validation loss 0.04572688415646553 Accuracy 0.8820000290870667\n",
      "Iteration 48860 Training loss 0.005764568690210581 Validation loss 0.04407404735684395 Accuracy 0.8886250257492065\n",
      "Iteration 48870 Training loss 0.0024839958641678095 Validation loss 0.043485376983881 Accuracy 0.8885000348091125\n",
      "Iteration 48880 Training loss 0.009750519879162312 Validation loss 0.0436166375875473 Accuracy 0.8881250619888306\n",
      "Iteration 48890 Training loss 0.0046316515654325485 Validation loss 0.04453180730342865 Accuracy 0.8841250538825989\n",
      "Iteration 48900 Training loss 0.006622493267059326 Validation loss 0.04387471079826355 Accuracy 0.8866250514984131\n",
      "Iteration 48910 Training loss 0.009342106059193611 Validation loss 0.046002425253391266 Accuracy 0.8806250691413879\n",
      "Iteration 48920 Training loss 0.0068611521273851395 Validation loss 0.043783996254205704 Accuracy 0.8870000243186951\n",
      "Iteration 48930 Training loss 0.0730433240532875 Validation loss 0.09612489491701126 Accuracy 0.7750000357627869\n",
      "Iteration 48940 Training loss 0.006924749352037907 Validation loss 0.04450054466724396 Accuracy 0.8848750591278076\n",
      "Iteration 48950 Training loss 0.008208717219531536 Validation loss 0.04400001838803291 Accuracy 0.8872500658035278\n",
      "Iteration 48960 Training loss 0.00685357628390193 Validation loss 0.043636005371809006 Accuracy 0.8871250152587891\n",
      "Iteration 48970 Training loss 0.014771969057619572 Validation loss 0.04470527544617653 Accuracy 0.8865000605583191\n",
      "Iteration 48980 Training loss 0.00679286802187562 Validation loss 0.0434928722679615 Accuracy 0.8876250386238098\n",
      "Iteration 48990 Training loss 0.009198935702443123 Validation loss 0.04428335651755333 Accuracy 0.8863750696182251\n",
      "Iteration 49000 Training loss 0.003709007054567337 Validation loss 0.04397042840719223 Accuracy 0.8876250386238098\n",
      "Iteration 49010 Training loss 0.007172676268965006 Validation loss 0.04354157671332359 Accuracy 0.8875000476837158\n",
      "Iteration 49020 Training loss 0.009597782976925373 Validation loss 0.04406154900789261 Accuracy 0.8870000243186951\n",
      "Iteration 49030 Training loss 0.005848734173923731 Validation loss 0.043684616684913635 Accuracy 0.8878750205039978\n",
      "Iteration 49040 Training loss 0.00682801054790616 Validation loss 0.04360927641391754 Accuracy 0.8875000476837158\n",
      "Iteration 49050 Training loss 0.005703453905880451 Validation loss 0.04388486593961716 Accuracy 0.8865000605583191\n",
      "Iteration 49060 Training loss 0.008947581984102726 Validation loss 0.04437065124511719 Accuracy 0.8878750205039978\n",
      "Iteration 49070 Training loss 0.009520606137812138 Validation loss 0.04513144493103027 Accuracy 0.8848750591278076\n",
      "Iteration 49080 Training loss 0.007882339879870415 Validation loss 0.04374073073267937 Accuracy 0.8877500295639038\n",
      "Iteration 49090 Training loss 0.007138542830944061 Validation loss 0.043813809752464294 Accuracy 0.8878750205039978\n",
      "Iteration 49100 Training loss 0.09876149892807007 Validation loss 0.10047037154436111 Accuracy 0.7573750615119934\n",
      "Iteration 49110 Training loss 0.007310027256608009 Validation loss 0.04691363126039505 Accuracy 0.8831250667572021\n",
      "Iteration 49120 Training loss 0.0037814010865986347 Validation loss 0.04361031576991081 Accuracy 0.8881250619888306\n",
      "Iteration 49130 Training loss 0.008635658770799637 Validation loss 0.04399650916457176 Accuracy 0.8868750333786011\n",
      "Iteration 49140 Training loss 0.005010585300624371 Validation loss 0.04396882653236389 Accuracy 0.8868750333786011\n",
      "Iteration 49150 Training loss 0.006012118887156248 Validation loss 0.04397643357515335 Accuracy 0.8871250152587891\n",
      "Iteration 49160 Training loss 0.005906444508582354 Validation loss 0.04366818442940712 Accuracy 0.8877500295639038\n",
      "Iteration 49170 Training loss 0.006166134960949421 Validation loss 0.04361649230122566 Accuracy 0.8873750567436218\n",
      "Iteration 49180 Training loss 0.006246484350413084 Validation loss 0.04372550547122955 Accuracy 0.8873750567436218\n",
      "Iteration 49190 Training loss 0.007616596762090921 Validation loss 0.04378582164645195 Accuracy 0.8866250514984131\n",
      "Iteration 49200 Training loss 0.005532033275812864 Validation loss 0.044160109013319016 Accuracy 0.8865000605583191\n",
      "Iteration 49210 Training loss 0.00988610740751028 Validation loss 0.043856583535671234 Accuracy 0.8878750205039978\n",
      "Iteration 49220 Training loss 0.011513126082718372 Validation loss 0.04387155920267105 Accuracy 0.8876250386238098\n",
      "Iteration 49230 Training loss 0.018884675577282906 Validation loss 0.044480182230472565 Accuracy 0.8857500553131104\n",
      "Iteration 49240 Training loss 0.006427571643143892 Validation loss 0.044518861919641495 Accuracy 0.8850000500679016\n",
      "Iteration 49250 Training loss 0.008841853588819504 Validation loss 0.04374004527926445 Accuracy 0.8851250410079956\n",
      "Iteration 49260 Training loss 0.006347982678562403 Validation loss 0.044085122644901276 Accuracy 0.8861250281333923\n",
      "Iteration 49270 Training loss 0.0035120141692459583 Validation loss 0.043536100536584854 Accuracy 0.8883750438690186\n",
      "Iteration 49280 Training loss 0.09724447876214981 Validation loss 0.10443509370088577 Accuracy 0.7523750066757202\n",
      "Iteration 49290 Training loss 0.00846081878989935 Validation loss 0.044119127094745636 Accuracy 0.8888750672340393\n",
      "Iteration 49300 Training loss 0.006164809223264456 Validation loss 0.04445667564868927 Accuracy 0.8850000500679016\n",
      "Iteration 49310 Training loss 0.004116865806281567 Validation loss 0.043942272663116455 Accuracy 0.8865000605583191\n",
      "Iteration 49320 Training loss 0.009834064170718193 Validation loss 0.04374855384230614 Accuracy 0.8871250152587891\n",
      "Iteration 49330 Training loss 0.007870337925851345 Validation loss 0.04357241094112396 Accuracy 0.8883750438690186\n",
      "Iteration 49340 Training loss 0.018717437982559204 Validation loss 0.053450364619493484 Accuracy 0.8653750419616699\n",
      "Iteration 49350 Training loss 0.011282439343631268 Validation loss 0.046274591237306595 Accuracy 0.8842500448226929\n",
      "Iteration 49360 Training loss 0.007989645935595036 Validation loss 0.0437963642179966 Accuracy 0.8873750567436218\n",
      "Iteration 49370 Training loss 0.005601875018328428 Validation loss 0.04361554607748985 Accuracy 0.8872500658035278\n",
      "Iteration 49380 Training loss 0.005257473327219486 Validation loss 0.043639443814754486 Accuracy 0.8862500190734863\n",
      "Iteration 49390 Training loss 0.005377715919166803 Validation loss 0.04365789145231247 Accuracy 0.8867500424385071\n",
      "Iteration 49400 Training loss 0.007664656266570091 Validation loss 0.04453448951244354 Accuracy 0.8855000138282776\n",
      "Iteration 49410 Training loss 0.0055213007144629955 Validation loss 0.04378506913781166 Accuracy 0.8876250386238098\n",
      "Iteration 49420 Training loss 0.0032744344789534807 Validation loss 0.04348491132259369 Accuracy 0.8881250619888306\n",
      "Iteration 49430 Training loss 0.0065963612869381905 Validation loss 0.04335857555270195 Accuracy 0.8886250257492065\n",
      "Iteration 49440 Training loss 0.006946012377738953 Validation loss 0.043550118803977966 Accuracy 0.8865000605583191\n",
      "Iteration 49450 Training loss 0.00916482787579298 Validation loss 0.04667425900697708 Accuracy 0.8808750510215759\n",
      "Iteration 49460 Training loss 0.0057579586282372475 Validation loss 0.04455344006419182 Accuracy 0.8855000138282776\n",
      "Iteration 49470 Training loss 0.006837508641183376 Validation loss 0.04344334080815315 Accuracy 0.8895000219345093\n",
      "Iteration 49480 Training loss 0.006819767411798239 Validation loss 0.043881163001060486 Accuracy 0.8863750696182251\n",
      "Iteration 49490 Training loss 0.005803866311907768 Validation loss 0.043630633503198624 Accuracy 0.8881250619888306\n",
      "Iteration 49500 Training loss 0.0044646114110946655 Validation loss 0.04346364736557007 Accuracy 0.8881250619888306\n",
      "Iteration 49510 Training loss 0.0067041656002402306 Validation loss 0.04361598938703537 Accuracy 0.8872500658035278\n",
      "Iteration 49520 Training loss 0.008209438063204288 Validation loss 0.044215284287929535 Accuracy 0.8872500658035278\n",
      "Iteration 49530 Training loss 0.011031131260097027 Validation loss 0.044020771980285645 Accuracy 0.8877500295639038\n",
      "Iteration 49540 Training loss 0.003874911228194833 Validation loss 0.04379842430353165 Accuracy 0.8878750205039978\n",
      "Iteration 49550 Training loss 0.015387183986604214 Validation loss 0.04769989475607872 Accuracy 0.8837500214576721\n",
      "Iteration 49560 Training loss 0.00943366065621376 Validation loss 0.044221919029951096 Accuracy 0.8873750567436218\n",
      "Iteration 49570 Training loss 0.005621931981295347 Validation loss 0.04398445039987564 Accuracy 0.8881250619888306\n",
      "Iteration 49580 Training loss 0.0069404286332428455 Validation loss 0.04501008987426758 Accuracy 0.8847500681877136\n",
      "Iteration 49590 Training loss 0.009297301061451435 Validation loss 0.04393996670842171 Accuracy 0.8863750696182251\n",
      "Iteration 49600 Training loss 0.00532660773023963 Validation loss 0.044189244508743286 Accuracy 0.8860000371932983\n",
      "Iteration 49610 Training loss 0.0055309743620455265 Validation loss 0.043725572526454926 Accuracy 0.8880000710487366\n",
      "Iteration 49620 Training loss 0.014570027589797974 Validation loss 0.048942118883132935 Accuracy 0.8753750324249268\n",
      "Iteration 49630 Training loss 0.018283551558852196 Validation loss 0.05476001277565956 Accuracy 0.8652500510215759\n",
      "Iteration 49640 Training loss 0.004501768387854099 Validation loss 0.043822191655635834 Accuracy 0.8871250152587891\n",
      "Iteration 49650 Training loss 0.009419274516403675 Validation loss 0.04474269971251488 Accuracy 0.8838750123977661\n",
      "Iteration 49660 Training loss 0.012517737224698067 Validation loss 0.04382949694991112 Accuracy 0.8876250386238098\n",
      "Iteration 49670 Training loss 0.008045661263167858 Validation loss 0.045017220079898834 Accuracy 0.8842500448226929\n",
      "Iteration 49680 Training loss 0.004297308158129454 Validation loss 0.04384731501340866 Accuracy 0.8867500424385071\n",
      "Iteration 49690 Training loss 0.004526257980614901 Validation loss 0.045379847288131714 Accuracy 0.8821250200271606\n",
      "Iteration 49700 Training loss 0.005740002728998661 Validation loss 0.04376222938299179 Accuracy 0.8876250386238098\n",
      "Iteration 49710 Training loss 0.03861334174871445 Validation loss 0.07133646309375763 Accuracy 0.8248750567436218\n",
      "Iteration 49720 Training loss 0.008349666371941566 Validation loss 0.04397871717810631 Accuracy 0.8885000348091125\n",
      "Iteration 49730 Training loss 0.014070839621126652 Validation loss 0.04349520429968834 Accuracy 0.8888750672340393\n",
      "Iteration 49740 Training loss 0.010070056654512882 Validation loss 0.04372400790452957 Accuracy 0.8871250152587891\n",
      "Iteration 49750 Training loss 0.011015817523002625 Validation loss 0.044209275394678116 Accuracy 0.8876250386238098\n",
      "Iteration 49760 Training loss 0.010756347328424454 Validation loss 0.04354984313249588 Accuracy 0.8888750672340393\n",
      "Iteration 49770 Training loss 0.00580477574840188 Validation loss 0.04348504915833473 Accuracy 0.8890000581741333\n",
      "Iteration 49780 Training loss 0.007864641956984997 Validation loss 0.04407871142029762 Accuracy 0.8858750462532043\n",
      "Iteration 49790 Training loss 0.005120010115206242 Validation loss 0.04490051791071892 Accuracy 0.8832500576972961\n",
      "Iteration 49800 Training loss 0.009532885625958443 Validation loss 0.043665699660778046 Accuracy 0.8867500424385071\n",
      "Iteration 49810 Training loss 0.008925535716116428 Validation loss 0.04493820294737816 Accuracy 0.8838750123977661\n",
      "Iteration 49820 Training loss 0.009958986192941666 Validation loss 0.04499254748225212 Accuracy 0.8841250538825989\n",
      "Iteration 49830 Training loss 0.00993817113339901 Validation loss 0.04367414861917496 Accuracy 0.8873750567436218\n",
      "Iteration 49840 Training loss 0.0061796437948942184 Validation loss 0.04421105608344078 Accuracy 0.8862500190734863\n",
      "Iteration 49850 Training loss 0.0064794993959367275 Validation loss 0.043672800064086914 Accuracy 0.8887500166893005\n",
      "Iteration 49860 Training loss 0.011123837903141975 Validation loss 0.04639007896184921 Accuracy 0.8808750510215759\n",
      "Iteration 49870 Training loss 0.0096967164427042 Validation loss 0.043952543288469315 Accuracy 0.8866250514984131\n",
      "Iteration 49880 Training loss 0.007999305613338947 Validation loss 0.04575315862894058 Accuracy 0.8815000653266907\n",
      "Iteration 49890 Training loss 0.009882455691695213 Validation loss 0.04576082155108452 Accuracy 0.8816250562667847\n",
      "Iteration 49900 Training loss 0.008025233633816242 Validation loss 0.04448990151286125 Accuracy 0.8858750462532043\n",
      "Iteration 49910 Training loss 0.00791932176798582 Validation loss 0.04388180747628212 Accuracy 0.8875000476837158\n",
      "Iteration 49920 Training loss 0.005290385335683823 Validation loss 0.04373382776975632 Accuracy 0.8872500658035278\n",
      "Iteration 49930 Training loss 0.007187926676124334 Validation loss 0.043868646025657654 Accuracy 0.8872500658035278\n",
      "Iteration 49940 Training loss 0.006656449753791094 Validation loss 0.04414178058505058 Accuracy 0.8868750333786011\n",
      "Iteration 49950 Training loss 0.009227721020579338 Validation loss 0.04560109227895737 Accuracy 0.8821250200271606\n",
      "Iteration 49960 Training loss 0.008411059156060219 Validation loss 0.04371199011802673 Accuracy 0.8885000348091125\n",
      "Iteration 49970 Training loss 0.007913267239928246 Validation loss 0.0453772209584713 Accuracy 0.8828750252723694\n",
      "Iteration 49980 Training loss 0.007454928010702133 Validation loss 0.04354596883058548 Accuracy 0.8883750438690186\n",
      "Iteration 49990 Training loss 0.009464914910495281 Validation loss 0.04366424307227135 Accuracy 0.8881250619888306\n",
      "Iteration 50000 Training loss 0.010118791833519936 Validation loss 0.04410669207572937 Accuracy 0.8878750205039978\n",
      "Iteration 50010 Training loss 0.005626276601105928 Validation loss 0.043834179639816284 Accuracy 0.8880000710487366\n",
      "Iteration 50020 Training loss 0.026535939425230026 Validation loss 0.06277190893888474 Accuracy 0.8427500128746033\n",
      "Iteration 50030 Training loss 0.006060372572392225 Validation loss 0.048011694103479385 Accuracy 0.8802500367164612\n",
      "Iteration 50040 Training loss 0.008519071154296398 Validation loss 0.044174257665872574 Accuracy 0.8895000219345093\n",
      "Iteration 50050 Training loss 0.004760551732033491 Validation loss 0.0440133698284626 Accuracy 0.8873750567436218\n",
      "Iteration 50060 Training loss 0.007823994383215904 Validation loss 0.04375222325325012 Accuracy 0.8862500190734863\n",
      "Iteration 50070 Training loss 0.003454359481111169 Validation loss 0.04363872855901718 Accuracy 0.8881250619888306\n",
      "Iteration 50080 Training loss 0.00957360491156578 Validation loss 0.04353155940771103 Accuracy 0.8892500400543213\n",
      "Iteration 50090 Training loss 0.007286672480404377 Validation loss 0.04392094910144806 Accuracy 0.8880000710487366\n",
      "Iteration 50100 Training loss 0.0030989658553153276 Validation loss 0.04355685040354729 Accuracy 0.8892500400543213\n",
      "Iteration 50110 Training loss 0.006213416811078787 Validation loss 0.043497033417224884 Accuracy 0.8887500166893005\n",
      "Iteration 50120 Training loss 0.007503310684114695 Validation loss 0.04394250363111496 Accuracy 0.8876250386238098\n",
      "Iteration 50130 Training loss 0.015119578689336777 Validation loss 0.05413929373025894 Accuracy 0.8663750290870667\n",
      "Iteration 50140 Training loss 0.010585030540823936 Validation loss 0.04615236818790436 Accuracy 0.8830000162124634\n",
      "Iteration 50150 Training loss 0.009037978015840054 Validation loss 0.044362977147102356 Accuracy 0.8867500424385071\n",
      "Iteration 50160 Training loss 0.004917343612760305 Validation loss 0.043616827577352524 Accuracy 0.8868750333786011\n",
      "Iteration 50170 Training loss 0.007070519030094147 Validation loss 0.04368682950735092 Accuracy 0.8885000348091125\n",
      "Iteration 50180 Training loss 0.006182962562888861 Validation loss 0.04387209564447403 Accuracy 0.8881250619888306\n",
      "Iteration 50190 Training loss 0.011763932183384895 Validation loss 0.04366360604763031 Accuracy 0.8888750672340393\n",
      "Iteration 50200 Training loss 0.006896574515849352 Validation loss 0.0433729887008667 Accuracy 0.8893750309944153\n",
      "Iteration 50210 Training loss 0.0035549120511859655 Validation loss 0.0449446439743042 Accuracy 0.8830000162124634\n",
      "Iteration 50220 Training loss 0.005070310086011887 Validation loss 0.043583452701568604 Accuracy 0.8893750309944153\n",
      "Iteration 50230 Training loss 0.007592742331326008 Validation loss 0.044145338237285614 Accuracy 0.8865000605583191\n",
      "Iteration 50240 Training loss 0.0069817909970879555 Validation loss 0.04392695799469948 Accuracy 0.8873750567436218\n",
      "Iteration 50250 Training loss 0.006008263677358627 Validation loss 0.0437319315969944 Accuracy 0.8890000581741333\n",
      "Iteration 50260 Training loss 0.007995556108653545 Validation loss 0.043856095522642136 Accuracy 0.8878750205039978\n",
      "Iteration 50270 Training loss 0.00444819824770093 Validation loss 0.043950892984867096 Accuracy 0.8865000605583191\n",
      "Iteration 50280 Training loss 0.008656834252178669 Validation loss 0.04398094862699509 Accuracy 0.8871250152587891\n",
      "Iteration 50290 Training loss 0.0050794524140655994 Validation loss 0.0447613000869751 Accuracy 0.8841250538825989\n",
      "Iteration 50300 Training loss 0.00855894573032856 Validation loss 0.04398031905293465 Accuracy 0.8862500190734863\n",
      "Iteration 50310 Training loss 0.0077048251405358315 Validation loss 0.045071836560964584 Accuracy 0.8818750381469727\n",
      "Iteration 50320 Training loss 0.009235427714884281 Validation loss 0.0436762273311615 Accuracy 0.8882500529289246\n",
      "Iteration 50330 Training loss 0.008879320695996284 Validation loss 0.04500472918152809 Accuracy 0.8831250667572021\n",
      "Iteration 50340 Training loss 0.004859310574829578 Validation loss 0.04384849593043327 Accuracy 0.8873750567436218\n",
      "Iteration 50350 Training loss 0.005034035537391901 Validation loss 0.044021960347890854 Accuracy 0.8885000348091125\n",
      "Iteration 50360 Training loss 0.007479872554540634 Validation loss 0.04398953914642334 Accuracy 0.8871250152587891\n",
      "Iteration 50370 Training loss 0.007165858522057533 Validation loss 0.04378221184015274 Accuracy 0.8871250152587891\n",
      "Iteration 50380 Training loss 0.004751003347337246 Validation loss 0.04381541162729263 Accuracy 0.8876250386238098\n",
      "Iteration 50390 Training loss 0.0065561034716665745 Validation loss 0.0436287485063076 Accuracy 0.8887500166893005\n",
      "Iteration 50400 Training loss 0.06844859570264816 Validation loss 0.08186856657266617 Accuracy 0.7983750104904175\n",
      "Iteration 50410 Training loss 0.007022649981081486 Validation loss 0.04439141973853111 Accuracy 0.8873750567436218\n",
      "Iteration 50420 Training loss 0.0051630339585244656 Validation loss 0.044297151267528534 Accuracy 0.8886250257492065\n",
      "Iteration 50430 Training loss 0.007555842399597168 Validation loss 0.04549476131796837 Accuracy 0.8837500214576721\n",
      "Iteration 50440 Training loss 0.02173466421663761 Validation loss 0.05324770510196686 Accuracy 0.8692500591278076\n",
      "Iteration 50450 Training loss 0.009169378317892551 Validation loss 0.04522169753909111 Accuracy 0.8876250386238098\n",
      "Iteration 50460 Training loss 0.008352364413440228 Validation loss 0.04404577985405922 Accuracy 0.8877500295639038\n",
      "Iteration 50470 Training loss 0.007209813222289085 Validation loss 0.04362131655216217 Accuracy 0.8886250257492065\n",
      "Iteration 50480 Training loss 0.004988702945411205 Validation loss 0.04349391534924507 Accuracy 0.889750063419342\n",
      "Iteration 50490 Training loss 0.014141564257442951 Validation loss 0.04385386034846306 Accuracy 0.8873750567436218\n",
      "Iteration 50500 Training loss 0.004938605707138777 Validation loss 0.04382037743926048 Accuracy 0.8878750205039978\n",
      "Iteration 50510 Training loss 0.009247589856386185 Validation loss 0.043932102620601654 Accuracy 0.8875000476837158\n",
      "Iteration 50520 Training loss 0.01601197011768818 Validation loss 0.049791522324085236 Accuracy 0.8792500495910645\n",
      "Iteration 50530 Training loss 0.009757861495018005 Validation loss 0.04747511446475983 Accuracy 0.8788750171661377\n",
      "Iteration 50540 Training loss 0.028291603550314903 Validation loss 0.05921686813235283 Accuracy 0.8533750176429749\n",
      "Iteration 50550 Training loss 0.046559303998947144 Validation loss 0.07707301527261734 Accuracy 0.812375009059906\n",
      "Iteration 50560 Training loss 0.00624146917834878 Validation loss 0.0438893660902977 Accuracy 0.8891250491142273\n",
      "Iteration 50570 Training loss 0.007930171675980091 Validation loss 0.04340105503797531 Accuracy 0.8880000710487366\n",
      "Iteration 50580 Training loss 0.00832295697182417 Validation loss 0.04394707456231117 Accuracy 0.8871250152587891\n",
      "Iteration 50590 Training loss 0.007075658068060875 Validation loss 0.04381748288869858 Accuracy 0.8881250619888306\n",
      "Iteration 50600 Training loss 0.0036188047379255295 Validation loss 0.04362659528851509 Accuracy 0.8877500295639038\n",
      "Iteration 50610 Training loss 0.004218054469674826 Validation loss 0.04384837672114372 Accuracy 0.8882500529289246\n",
      "Iteration 50620 Training loss 0.005243391264230013 Validation loss 0.043355606496334076 Accuracy 0.8881250619888306\n",
      "Iteration 50630 Training loss 0.007159380242228508 Validation loss 0.04401939734816551 Accuracy 0.8887500166893005\n",
      "Iteration 50640 Training loss 0.005643027368932962 Validation loss 0.045468978583812714 Accuracy 0.8832500576972961\n",
      "Iteration 50650 Training loss 0.00672645540907979 Validation loss 0.04409965127706528 Accuracy 0.8866250514984131\n",
      "Iteration 50660 Training loss 0.007302276324480772 Validation loss 0.04339183494448662 Accuracy 0.8883750438690186\n",
      "Iteration 50670 Training loss 0.009284092113375664 Validation loss 0.04362749308347702 Accuracy 0.8883750438690186\n",
      "Iteration 50680 Training loss 0.00961278099566698 Validation loss 0.04362902790307999 Accuracy 0.8883750438690186\n",
      "Iteration 50690 Training loss 0.002605902496725321 Validation loss 0.043733902275562286 Accuracy 0.8871250152587891\n",
      "Iteration 50700 Training loss 0.008310656063258648 Validation loss 0.043769288808107376 Accuracy 0.8887500166893005\n",
      "Iteration 50710 Training loss 0.004380229394882917 Validation loss 0.04351401701569557 Accuracy 0.8886250257492065\n",
      "Iteration 50720 Training loss 0.007043107412755489 Validation loss 0.04378846287727356 Accuracy 0.8882500529289246\n",
      "Iteration 50730 Training loss 0.007997438311576843 Validation loss 0.04426869377493858 Accuracy 0.8863750696182251\n",
      "Iteration 50740 Training loss 0.009123893454670906 Validation loss 0.04434860870242119 Accuracy 0.8858750462532043\n",
      "Iteration 50750 Training loss 0.00622537499293685 Validation loss 0.04363930970430374 Accuracy 0.8881250619888306\n",
      "Iteration 50760 Training loss 0.008461480028927326 Validation loss 0.043859515339136124 Accuracy 0.8868750333786011\n",
      "Iteration 50770 Training loss 0.005723894573748112 Validation loss 0.04371810331940651 Accuracy 0.8872500658035278\n",
      "Iteration 50780 Training loss 0.00924601312726736 Validation loss 0.04517890512943268 Accuracy 0.8836250305175781\n",
      "Iteration 50790 Training loss 0.0042196279391646385 Validation loss 0.04400033876299858 Accuracy 0.8876250386238098\n",
      "Iteration 50800 Training loss 0.006124630570411682 Validation loss 0.043717995285987854 Accuracy 0.8877500295639038\n",
      "Iteration 50810 Training loss 0.006972310598939657 Validation loss 0.043675996363162994 Accuracy 0.8881250619888306\n",
      "Iteration 50820 Training loss 0.0075584459118545055 Validation loss 0.044092532247304916 Accuracy 0.8870000243186951\n",
      "Iteration 50830 Training loss 0.0038921956438571215 Validation loss 0.04481269419193268 Accuracy 0.8841250538825989\n",
      "Iteration 50840 Training loss 0.006900462321937084 Validation loss 0.043682802468538284 Accuracy 0.8885000348091125\n",
      "Iteration 50850 Training loss 0.006708512082695961 Validation loss 0.043755922466516495 Accuracy 0.8871250152587891\n",
      "Iteration 50860 Training loss 0.09032375365495682 Validation loss 0.10120929777622223 Accuracy 0.7642500400543213\n",
      "Iteration 50870 Training loss 0.007519533392041922 Validation loss 0.044829558581113815 Accuracy 0.8875000476837158\n",
      "Iteration 50880 Training loss 0.006813917774707079 Validation loss 0.043482501059770584 Accuracy 0.8887500166893005\n",
      "Iteration 50890 Training loss 0.047179918736219406 Validation loss 0.08047569543123245 Accuracy 0.8046250343322754\n",
      "Iteration 50900 Training loss 0.006809631362557411 Validation loss 0.04446078836917877 Accuracy 0.8860000371932983\n",
      "Iteration 50910 Training loss 0.004965025465935469 Validation loss 0.04384075477719307 Accuracy 0.8878750205039978\n",
      "Iteration 50920 Training loss 0.005923343822360039 Validation loss 0.043551888316869736 Accuracy 0.8881250619888306\n",
      "Iteration 50930 Training loss 0.006470378488302231 Validation loss 0.044081397354602814 Accuracy 0.8872500658035278\n",
      "Iteration 50940 Training loss 0.005990246310830116 Validation loss 0.044528666883707047 Accuracy 0.8858750462532043\n",
      "Iteration 50950 Training loss 0.007653933484107256 Validation loss 0.04356086999177933 Accuracy 0.8875000476837158\n",
      "Iteration 50960 Training loss 0.007837108336389065 Validation loss 0.04424048960208893 Accuracy 0.8865000605583191\n",
      "Iteration 50970 Training loss 0.01005113311111927 Validation loss 0.04393410682678223 Accuracy 0.8881250619888306\n",
      "Iteration 50980 Training loss 0.007256315555423498 Validation loss 0.04419104382395744 Accuracy 0.8862500190734863\n",
      "Iteration 50990 Training loss 0.005288059823215008 Validation loss 0.04357845336198807 Accuracy 0.8875000476837158\n",
      "Iteration 51000 Training loss 0.005070671904832125 Validation loss 0.043725818395614624 Accuracy 0.8875000476837158\n",
      "Iteration 51010 Training loss 0.002770085819065571 Validation loss 0.043668847531080246 Accuracy 0.8890000581741333\n",
      "Iteration 51020 Training loss 0.004863054491579533 Validation loss 0.04342781752347946 Accuracy 0.8885000348091125\n",
      "Iteration 51030 Training loss 0.007904437370598316 Validation loss 0.04341917112469673 Accuracy 0.8882500529289246\n",
      "Iteration 51040 Training loss 0.0068921721540391445 Validation loss 0.04519681632518768 Accuracy 0.8837500214576721\n",
      "Iteration 51050 Training loss 0.005624407436698675 Validation loss 0.04434429481625557 Accuracy 0.8865000605583191\n",
      "Iteration 51060 Training loss 0.00509520573541522 Validation loss 0.043773751705884933 Accuracy 0.8877500295639038\n",
      "Iteration 51070 Training loss 0.007627974729984999 Validation loss 0.04395671188831329 Accuracy 0.8866250514984131\n",
      "Iteration 51080 Training loss 0.009264905005693436 Validation loss 0.047233279794454575 Accuracy 0.8807500600814819\n",
      "Iteration 51090 Training loss 0.012597617693245411 Validation loss 0.047158487141132355 Accuracy 0.8817500472068787\n",
      "Iteration 51100 Training loss 0.008376767858862877 Validation loss 0.043850116431713104 Accuracy 0.8867500424385071\n",
      "Iteration 51110 Training loss 0.008604533039033413 Validation loss 0.045024484395980835 Accuracy 0.8842500448226929\n",
      "Iteration 51120 Training loss 0.014384321868419647 Validation loss 0.04574340209364891 Accuracy 0.8867500424385071\n",
      "Iteration 51130 Training loss 0.007146318908780813 Validation loss 0.04449168220162392 Accuracy 0.8855000138282776\n",
      "Iteration 51140 Training loss 0.009834271855652332 Validation loss 0.04348121955990791 Accuracy 0.8895000219345093\n",
      "Iteration 51150 Training loss 0.008221336640417576 Validation loss 0.04359067603945732 Accuracy 0.8891250491142273\n",
      "Iteration 51160 Training loss 0.006383208092302084 Validation loss 0.0436471551656723 Accuracy 0.8887500166893005\n",
      "Iteration 51170 Training loss 0.00866621918976307 Validation loss 0.04368700087070465 Accuracy 0.8882500529289246\n",
      "Iteration 51180 Training loss 0.009798328392207623 Validation loss 0.04413648694753647 Accuracy 0.8868750333786011\n",
      "Iteration 51190 Training loss 0.004411621484905481 Validation loss 0.043706417083740234 Accuracy 0.8890000581741333\n",
      "Iteration 51200 Training loss 0.007384712342172861 Validation loss 0.043526362627744675 Accuracy 0.8885000348091125\n",
      "Iteration 51210 Training loss 0.008339032530784607 Validation loss 0.043924592435359955 Accuracy 0.8888750672340393\n",
      "Iteration 51220 Training loss 0.010481944307684898 Validation loss 0.043719325214624405 Accuracy 0.8885000348091125\n",
      "Iteration 51230 Training loss 0.006556335370987654 Validation loss 0.04383565112948418 Accuracy 0.8875000476837158\n",
      "Iteration 51240 Training loss 0.014172363094985485 Validation loss 0.05197133868932724 Accuracy 0.8686250448226929\n",
      "Iteration 51250 Training loss 0.05580734461545944 Validation loss 0.07192368060350418 Accuracy 0.8235000371932983\n",
      "Iteration 51260 Training loss 0.009038478136062622 Validation loss 0.04515931010246277 Accuracy 0.8831250667572021\n",
      "Iteration 51270 Training loss 0.0038031612057238817 Validation loss 0.04405989870429039 Accuracy 0.8870000243186951\n",
      "Iteration 51280 Training loss 0.0064314063638448715 Validation loss 0.04355546832084656 Accuracy 0.8896250128746033\n",
      "Iteration 51290 Training loss 0.008814634755253792 Validation loss 0.04532603919506073 Accuracy 0.8823750615119934\n",
      "Iteration 51300 Training loss 0.005158145446330309 Validation loss 0.04469437152147293 Accuracy 0.8842500448226929\n",
      "Iteration 51310 Training loss 0.005980884190648794 Validation loss 0.043855346739292145 Accuracy 0.8878750205039978\n",
      "Iteration 51320 Training loss 0.003891073865815997 Validation loss 0.04357118532061577 Accuracy 0.8886250257492065\n",
      "Iteration 51330 Training loss 0.002042815089225769 Validation loss 0.04384146258234978 Accuracy 0.8887500166893005\n",
      "Iteration 51340 Training loss 0.005628128536045551 Validation loss 0.043773677200078964 Accuracy 0.8886250257492065\n",
      "Iteration 51350 Training loss 0.009387298487126827 Validation loss 0.04366149753332138 Accuracy 0.8878750205039978\n",
      "Iteration 51360 Training loss 0.009977186098694801 Validation loss 0.04374069347977638 Accuracy 0.8866250514984131\n",
      "Iteration 51370 Training loss 0.006245889700949192 Validation loss 0.04376975819468498 Accuracy 0.8876250386238098\n",
      "Iteration 51380 Training loss 0.010778028517961502 Validation loss 0.04369349777698517 Accuracy 0.8866250514984131\n",
      "Iteration 51390 Training loss 0.007144173141568899 Validation loss 0.044881317764520645 Accuracy 0.8832500576972961\n",
      "Iteration 51400 Training loss 0.006043388042598963 Validation loss 0.04366932809352875 Accuracy 0.8882500529289246\n",
      "Iteration 51410 Training loss 0.006995729170739651 Validation loss 0.04403707757592201 Accuracy 0.8875000476837158\n",
      "Iteration 51420 Training loss 0.005546267610043287 Validation loss 0.0438230037689209 Accuracy 0.8892500400543213\n",
      "Iteration 51430 Training loss 0.005126493517309427 Validation loss 0.044391930103302 Accuracy 0.8847500681877136\n",
      "Iteration 51440 Training loss 0.011039899662137032 Validation loss 0.04417586699128151 Accuracy 0.8878750205039978\n",
      "Iteration 51450 Training loss 0.00660211406648159 Validation loss 0.04573211073875427 Accuracy 0.8812500238418579\n",
      "Iteration 51460 Training loss 0.010157478973269463 Validation loss 0.04378530755639076 Accuracy 0.8862500190734863\n",
      "Iteration 51470 Training loss 0.004680457524955273 Validation loss 0.04405221343040466 Accuracy 0.8866250514984131\n",
      "Iteration 51480 Training loss 0.015135492198169231 Validation loss 0.04371626675128937 Accuracy 0.8885000348091125\n",
      "Iteration 51490 Training loss 0.032316334545612335 Validation loss 0.06398998200893402 Accuracy 0.8450000286102295\n",
      "Iteration 51500 Training loss 0.004988945089280605 Validation loss 0.044017039239406586 Accuracy 0.8873750567436218\n",
      "Iteration 51510 Training loss 0.005261566955596209 Validation loss 0.04398554563522339 Accuracy 0.8873750567436218\n",
      "Iteration 51520 Training loss 0.005035309121012688 Validation loss 0.04377342388033867 Accuracy 0.8872500658035278\n",
      "Iteration 51530 Training loss 0.010082408785820007 Validation loss 0.04379163682460785 Accuracy 0.8877500295639038\n",
      "Iteration 51540 Training loss 0.01069854386150837 Validation loss 0.04670117422938347 Accuracy 0.8800000548362732\n",
      "Iteration 51550 Training loss 0.006374468095600605 Validation loss 0.044151078909635544 Accuracy 0.8872500658035278\n",
      "Iteration 51560 Training loss 0.003026918973773718 Validation loss 0.04509208723902702 Accuracy 0.8831250667572021\n",
      "Iteration 51570 Training loss 0.006174366921186447 Validation loss 0.04369831085205078 Accuracy 0.8887500166893005\n",
      "Iteration 51580 Training loss 0.004775201436132193 Validation loss 0.04455317184329033 Accuracy 0.8855000138282776\n",
      "Iteration 51590 Training loss 0.012646982446312904 Validation loss 0.04499397426843643 Accuracy 0.8836250305175781\n",
      "Iteration 51600 Training loss 0.005128295160830021 Validation loss 0.04389030486345291 Accuracy 0.8891250491142273\n",
      "Iteration 51610 Training loss 0.010189802385866642 Validation loss 0.04376620799303055 Accuracy 0.8878750205039978\n",
      "Iteration 51620 Training loss 0.004849092103540897 Validation loss 0.04385798051953316 Accuracy 0.8876250386238098\n",
      "Iteration 51630 Training loss 0.03688391298055649 Validation loss 0.0676286444067955 Accuracy 0.8325000405311584\n",
      "Iteration 51640 Training loss 0.011798975057899952 Validation loss 0.04870394617319107 Accuracy 0.8800000548362732\n",
      "Iteration 51650 Training loss 0.009570220485329628 Validation loss 0.046014122664928436 Accuracy 0.8842500448226929\n",
      "Iteration 51660 Training loss 0.007385688833892345 Validation loss 0.04395008087158203 Accuracy 0.8873750567436218\n",
      "Iteration 51670 Training loss 0.00513956043869257 Validation loss 0.04402453824877739 Accuracy 0.8875000476837158\n",
      "Iteration 51680 Training loss 0.007310658693313599 Validation loss 0.043723758310079575 Accuracy 0.8887500166893005\n",
      "Iteration 51690 Training loss 0.0032637198455631733 Validation loss 0.04376737400889397 Accuracy 0.8881250619888306\n",
      "Iteration 51700 Training loss 0.007593147456645966 Validation loss 0.04365023970603943 Accuracy 0.8888750672340393\n",
      "Iteration 51710 Training loss 0.009316706098616123 Validation loss 0.045588914304971695 Accuracy 0.8848750591278076\n",
      "Iteration 51720 Training loss 0.006596231367439032 Validation loss 0.043631523847579956 Accuracy 0.8891250491142273\n",
      "Iteration 51730 Training loss 0.010050127282738686 Validation loss 0.043771859258413315 Accuracy 0.8882500529289246\n",
      "Iteration 51740 Training loss 0.004104727879166603 Validation loss 0.04383903741836548 Accuracy 0.8871250152587891\n",
      "Iteration 51750 Training loss 0.006860160734504461 Validation loss 0.04430592060089111 Accuracy 0.8873750567436218\n",
      "Iteration 51760 Training loss 0.007610907778143883 Validation loss 0.04477107524871826 Accuracy 0.8845000267028809\n",
      "Iteration 51770 Training loss 0.006433708593249321 Validation loss 0.043739013373851776 Accuracy 0.8885000348091125\n",
      "Iteration 51780 Training loss 0.006599156651645899 Validation loss 0.04421734809875488 Accuracy 0.8870000243186951\n",
      "Iteration 51790 Training loss 0.008160785771906376 Validation loss 0.04540971294045448 Accuracy 0.8821250200271606\n",
      "Iteration 51800 Training loss 0.003466624766588211 Validation loss 0.04420095309615135 Accuracy 0.8873750567436218\n",
      "Iteration 51810 Training loss 0.0034232442267239094 Validation loss 0.04404712840914726 Accuracy 0.8883750438690186\n",
      "Iteration 51820 Training loss 0.004936050157994032 Validation loss 0.044183239340782166 Accuracy 0.8866250514984131\n",
      "Iteration 51830 Training loss 0.008087574504315853 Validation loss 0.044666457921266556 Accuracy 0.8842500448226929\n",
      "Iteration 51840 Training loss 0.009823361411690712 Validation loss 0.04391627758741379 Accuracy 0.8885000348091125\n",
      "Iteration 51850 Training loss 0.0015250204596668482 Validation loss 0.043650541454553604 Accuracy 0.8883750438690186\n",
      "Iteration 51860 Training loss 0.00282861664891243 Validation loss 0.04466139152646065 Accuracy 0.8852500319480896\n",
      "Iteration 51870 Training loss 0.009349925443530083 Validation loss 0.04382653161883354 Accuracy 0.8881250619888306\n",
      "Iteration 51880 Training loss 0.0071881781332194805 Validation loss 0.045067328959703445 Accuracy 0.8823750615119934\n",
      "Iteration 51890 Training loss 0.007196249905973673 Validation loss 0.04381202533841133 Accuracy 0.8880000710487366\n",
      "Iteration 51900 Training loss 0.004007580690085888 Validation loss 0.044037867337465286 Accuracy 0.8875000476837158\n",
      "Iteration 51910 Training loss 0.0033258148469030857 Validation loss 0.04375261068344116 Accuracy 0.8885000348091125\n",
      "Iteration 51920 Training loss 0.003086681244894862 Validation loss 0.044165559113025665 Accuracy 0.8868750333786011\n",
      "Iteration 51930 Training loss 0.007761616725474596 Validation loss 0.04458313435316086 Accuracy 0.8850000500679016\n",
      "Iteration 51940 Training loss 0.00615851441398263 Validation loss 0.04431657865643501 Accuracy 0.8862500190734863\n",
      "Iteration 51950 Training loss 0.010950877331197262 Validation loss 0.04720664396882057 Accuracy 0.877375066280365\n",
      "Iteration 51960 Training loss 0.0055275848135352135 Validation loss 0.043900441378355026 Accuracy 0.8877500295639038\n",
      "Iteration 51970 Training loss 0.007944533601403236 Validation loss 0.04368600621819496 Accuracy 0.8873750567436218\n",
      "Iteration 51980 Training loss 0.006544927600771189 Validation loss 0.04459134861826897 Accuracy 0.8840000629425049\n",
      "Iteration 51990 Training loss 0.00957954116165638 Validation loss 0.04537593200802803 Accuracy 0.8833750486373901\n",
      "Iteration 52000 Training loss 0.0041773151606321335 Validation loss 0.04407482221722603 Accuracy 0.8871250152587891\n",
      "Iteration 52010 Training loss 0.005398438312113285 Validation loss 0.04449351504445076 Accuracy 0.8857500553131104\n",
      "Iteration 52020 Training loss 0.004964801482856274 Validation loss 0.045168325304985046 Accuracy 0.8842500448226929\n",
      "Iteration 52030 Training loss 0.004150754772126675 Validation loss 0.044678181409835815 Accuracy 0.8853750228881836\n",
      "Iteration 52040 Training loss 0.011966006830334663 Validation loss 0.044592540711164474 Accuracy 0.8843750357627869\n",
      "Iteration 52050 Training loss 0.006315487436950207 Validation loss 0.04400595650076866 Accuracy 0.8872500658035278\n",
      "Iteration 52060 Training loss 0.008116764947772026 Validation loss 0.044322073459625244 Accuracy 0.8892500400543213\n",
      "Iteration 52070 Training loss 0.009945179335772991 Validation loss 0.04377496987581253 Accuracy 0.8882500529289246\n",
      "Iteration 52080 Training loss 0.004379140213131905 Validation loss 0.043938715010881424 Accuracy 0.8872500658035278\n",
      "Iteration 52090 Training loss 0.004673692863434553 Validation loss 0.04404963552951813 Accuracy 0.8871250152587891\n",
      "Iteration 52100 Training loss 0.0021517521236091852 Validation loss 0.04418352618813515 Accuracy 0.8872500658035278\n",
      "Iteration 52110 Training loss 0.03756032511591911 Validation loss 0.06892872601747513 Accuracy 0.8300000429153442\n",
      "Iteration 52120 Training loss 0.010166749358177185 Validation loss 0.047242749482393265 Accuracy 0.8847500681877136\n",
      "Iteration 52130 Training loss 0.004017841536551714 Validation loss 0.04430985450744629 Accuracy 0.8882500529289246\n",
      "Iteration 52140 Training loss 0.0039022406563162804 Validation loss 0.04385499283671379 Accuracy 0.8881250619888306\n",
      "Iteration 52150 Training loss 0.010173793882131577 Validation loss 0.0440138503909111 Accuracy 0.8875000476837158\n",
      "Iteration 52160 Training loss 0.004151006229221821 Validation loss 0.04399768263101578 Accuracy 0.8877500295639038\n",
      "Iteration 52170 Training loss 0.029552750289440155 Validation loss 0.05375751107931137 Accuracy 0.8683750629425049\n",
      "Iteration 52180 Training loss 0.007245189975947142 Validation loss 0.04468216747045517 Accuracy 0.8862500190734863\n",
      "Iteration 52190 Training loss 0.009489629417657852 Validation loss 0.04381755366921425 Accuracy 0.8887500166893005\n",
      "Iteration 52200 Training loss 0.006880576256662607 Validation loss 0.044468630105257034 Accuracy 0.8856250643730164\n",
      "Iteration 52210 Training loss 0.032935645431280136 Validation loss 0.06459379196166992 Accuracy 0.8421250581741333\n",
      "Iteration 52220 Training loss 0.00878836028277874 Validation loss 0.04428286850452423 Accuracy 0.8877500295639038\n",
      "Iteration 52230 Training loss 0.005239303223788738 Validation loss 0.043740786612033844 Accuracy 0.8891250491142273\n",
      "Iteration 52240 Training loss 0.005395274143666029 Validation loss 0.04380127415060997 Accuracy 0.890125036239624\n",
      "Iteration 52250 Training loss 0.008685414679348469 Validation loss 0.044021543115377426 Accuracy 0.8888750672340393\n",
      "Iteration 52260 Training loss 0.011683332733809948 Validation loss 0.043835144490003586 Accuracy 0.8880000710487366\n",
      "Iteration 52270 Training loss 0.01312571857124567 Validation loss 0.05159473791718483 Accuracy 0.8728750348091125\n",
      "Iteration 52280 Training loss 0.03513101488351822 Validation loss 0.065841443836689 Accuracy 0.8362500667572021\n",
      "Iteration 52290 Training loss 0.03422214090824127 Validation loss 0.06319115310907364 Accuracy 0.8450000286102295\n",
      "Iteration 52300 Training loss 0.00747915031388402 Validation loss 0.0439557321369648 Accuracy 0.8892500400543213\n",
      "Iteration 52310 Training loss 0.00352725968696177 Validation loss 0.04378635436296463 Accuracy 0.8890000581741333\n",
      "Iteration 52320 Training loss 0.00969434343278408 Validation loss 0.04399716109037399 Accuracy 0.8873750567436218\n",
      "Iteration 52330 Training loss 0.005353120621293783 Validation loss 0.043727610260248184 Accuracy 0.8882500529289246\n",
      "Iteration 52340 Training loss 0.012603625655174255 Validation loss 0.04388156160712242 Accuracy 0.8875000476837158\n",
      "Iteration 52350 Training loss 0.008673807606101036 Validation loss 0.04360414296388626 Accuracy 0.8893750309944153\n",
      "Iteration 52360 Training loss 0.007143175229430199 Validation loss 0.043837618082761765 Accuracy 0.8885000348091125\n",
      "Iteration 52370 Training loss 0.004827173892408609 Validation loss 0.04364322870969772 Accuracy 0.889750063419342\n",
      "Iteration 52380 Training loss 0.006988582666963339 Validation loss 0.04418736323714256 Accuracy 0.8867500424385071\n",
      "Iteration 52390 Training loss 0.006745986174792051 Validation loss 0.043604783713817596 Accuracy 0.8896250128746033\n",
      "Iteration 52400 Training loss 0.004090052098035812 Validation loss 0.04405704885721207 Accuracy 0.8882500529289246\n",
      "Iteration 52410 Training loss 0.006858942564576864 Validation loss 0.043758146464824677 Accuracy 0.8888750672340393\n",
      "Iteration 52420 Training loss 0.004160800948739052 Validation loss 0.04365184158086777 Accuracy 0.8886250257492065\n",
      "Iteration 52430 Training loss 0.003799768630415201 Validation loss 0.043869614601135254 Accuracy 0.8886250257492065\n",
      "Iteration 52440 Training loss 0.011032668873667717 Validation loss 0.04381716623902321 Accuracy 0.8888750672340393\n",
      "Iteration 52450 Training loss 0.007298621349036694 Validation loss 0.04396338015794754 Accuracy 0.8875000476837158\n",
      "Iteration 52460 Training loss 0.00630881218239665 Validation loss 0.04384080693125725 Accuracy 0.8876250386238098\n",
      "Iteration 52470 Training loss 0.005029791034758091 Validation loss 0.043726321309804916 Accuracy 0.8877500295639038\n",
      "Iteration 52480 Training loss 0.001399681787006557 Validation loss 0.04381158575415611 Accuracy 0.8870000243186951\n",
      "Iteration 52490 Training loss 0.009868534281849861 Validation loss 0.04380765184760094 Accuracy 0.8886250257492065\n",
      "Iteration 52500 Training loss 0.009503690525889397 Validation loss 0.04377209395170212 Accuracy 0.8873750567436218\n",
      "Iteration 52510 Training loss 0.005631588399410248 Validation loss 0.04371035844087601 Accuracy 0.8882500529289246\n",
      "Iteration 52520 Training loss 0.004071371629834175 Validation loss 0.0436946339905262 Accuracy 0.8877500295639038\n",
      "Iteration 52530 Training loss 0.006132837850600481 Validation loss 0.044187501072883606 Accuracy 0.8881250619888306\n",
      "Iteration 52540 Training loss 0.007880410179495811 Validation loss 0.043599460273981094 Accuracy 0.8875000476837158\n",
      "Iteration 52550 Training loss 0.005860709585249424 Validation loss 0.04373665899038315 Accuracy 0.8872500658035278\n",
      "Iteration 52560 Training loss 0.005047985352575779 Validation loss 0.04600265994668007 Accuracy 0.8825000524520874\n",
      "Iteration 52570 Training loss 0.02568517066538334 Validation loss 0.05631113797426224 Accuracy 0.861875057220459\n",
      "Iteration 52580 Training loss 0.005828950088471174 Validation loss 0.04395117983222008 Accuracy 0.8896250128746033\n",
      "Iteration 52590 Training loss 0.004419947974383831 Validation loss 0.04359642416238785 Accuracy 0.8895000219345093\n",
      "Iteration 52600 Training loss 0.009468864649534225 Validation loss 0.04753773659467697 Accuracy 0.8837500214576721\n",
      "Iteration 52610 Training loss 0.005501271225512028 Validation loss 0.04430320858955383 Accuracy 0.8868750333786011\n",
      "Iteration 52620 Training loss 0.008853189647197723 Validation loss 0.04345814883708954 Accuracy 0.8910000324249268\n",
      "Iteration 52630 Training loss 0.00882161594927311 Validation loss 0.04354247450828552 Accuracy 0.8876250386238098\n",
      "Iteration 52640 Training loss 0.006266388110816479 Validation loss 0.043555665761232376 Accuracy 0.8888750672340393\n",
      "Iteration 52650 Training loss 0.009654860012233257 Validation loss 0.043852441012859344 Accuracy 0.8881250619888306\n",
      "Iteration 52660 Training loss 0.005881618708372116 Validation loss 0.04367287829518318 Accuracy 0.8886250257492065\n",
      "Iteration 52670 Training loss 0.0046766274608671665 Validation loss 0.043647486716508865 Accuracy 0.8883750438690186\n",
      "Iteration 52680 Training loss 0.0057116542011499405 Validation loss 0.044187068939208984 Accuracy 0.8880000710487366\n",
      "Iteration 52690 Training loss 0.007531879935413599 Validation loss 0.0441262386739254 Accuracy 0.8875000476837158\n",
      "Iteration 52700 Training loss 0.007986708544194698 Validation loss 0.04497460648417473 Accuracy 0.8843750357627869\n",
      "Iteration 52710 Training loss 0.005404143128544092 Validation loss 0.043709564954042435 Accuracy 0.8887500166893005\n",
      "Iteration 52720 Training loss 0.008773872628808022 Validation loss 0.0444304533302784 Accuracy 0.8853750228881836\n",
      "Iteration 52730 Training loss 0.005185819696635008 Validation loss 0.043596528470516205 Accuracy 0.8887500166893005\n",
      "Iteration 52740 Training loss 0.006703204941004515 Validation loss 0.04359257221221924 Accuracy 0.8905000686645508\n",
      "Iteration 52750 Training loss 0.0035201050341129303 Validation loss 0.043940555304288864 Accuracy 0.8871250152587891\n",
      "Iteration 52760 Training loss 0.007371317129582167 Validation loss 0.045857466757297516 Accuracy 0.8797500133514404\n",
      "Iteration 52770 Training loss 0.007799285463988781 Validation loss 0.04785537347197533 Accuracy 0.878125011920929\n",
      "Iteration 52780 Training loss 0.005545776803046465 Validation loss 0.04588375613093376 Accuracy 0.8836250305175781\n",
      "Iteration 52790 Training loss 0.009152442216873169 Validation loss 0.0440395288169384 Accuracy 0.8875000476837158\n",
      "Iteration 52800 Training loss 0.00527515122666955 Validation loss 0.04406570643186569 Accuracy 0.8866250514984131\n",
      "Iteration 52810 Training loss 0.0035798102617263794 Validation loss 0.04407339543104172 Accuracy 0.8881250619888306\n",
      "Iteration 52820 Training loss 0.008650525473058224 Validation loss 0.044565118849277496 Accuracy 0.8843750357627869\n",
      "Iteration 52830 Training loss 0.010309730656445026 Validation loss 0.04376821964979172 Accuracy 0.8886250257492065\n",
      "Iteration 52840 Training loss 0.0076913186348974705 Validation loss 0.04522294923663139 Accuracy 0.8816250562667847\n",
      "Iteration 52850 Training loss 0.00744126969948411 Validation loss 0.043897733092308044 Accuracy 0.8881250619888306\n",
      "Iteration 52860 Training loss 0.008474372327327728 Validation loss 0.0447021946310997 Accuracy 0.8846250176429749\n",
      "Iteration 52870 Training loss 0.0037644654512405396 Validation loss 0.04387453570961952 Accuracy 0.8871250152587891\n",
      "Iteration 52880 Training loss 0.005199373699724674 Validation loss 0.04397374764084816 Accuracy 0.8870000243186951\n",
      "Iteration 52890 Training loss 0.09532565623521805 Validation loss 0.11230312287807465 Accuracy 0.7351250052452087\n",
      "Iteration 52900 Training loss 0.006561704911291599 Validation loss 0.04440607503056526 Accuracy 0.8870000243186951\n",
      "Iteration 52910 Training loss 0.005511862691491842 Validation loss 0.04354962706565857 Accuracy 0.8892500400543213\n",
      "Iteration 52920 Training loss 0.005221574101597071 Validation loss 0.04370368272066116 Accuracy 0.8881250619888306\n",
      "Iteration 52930 Training loss 0.010480535216629505 Validation loss 0.04510834068059921 Accuracy 0.8841250538825989\n",
      "Iteration 52940 Training loss 0.0075049614533782005 Validation loss 0.04364294558763504 Accuracy 0.8891250491142273\n",
      "Iteration 52950 Training loss 0.014071439392864704 Validation loss 0.045893311500549316 Accuracy 0.8837500214576721\n",
      "Iteration 52960 Training loss 0.00791871827095747 Validation loss 0.04390794783830643 Accuracy 0.8883750438690186\n",
      "Iteration 52970 Training loss 0.004422906320542097 Validation loss 0.04391252249479294 Accuracy 0.8888750672340393\n",
      "Iteration 52980 Training loss 0.004701701458543539 Validation loss 0.04363277554512024 Accuracy 0.889750063419342\n",
      "Iteration 52990 Training loss 0.00571681372821331 Validation loss 0.04393623024225235 Accuracy 0.8885000348091125\n",
      "Iteration 53000 Training loss 0.0066086421720683575 Validation loss 0.044404901564121246 Accuracy 0.8856250643730164\n",
      "Iteration 53010 Training loss 0.007547051180154085 Validation loss 0.04388022795319557 Accuracy 0.8877500295639038\n",
      "Iteration 53020 Training loss 0.0075761303305625916 Validation loss 0.04377339407801628 Accuracy 0.8880000710487366\n",
      "Iteration 53030 Training loss 0.006640217732638121 Validation loss 0.04403052479028702 Accuracy 0.8883750438690186\n",
      "Iteration 53040 Training loss 0.005861736834049225 Validation loss 0.044607747346162796 Accuracy 0.8847500681877136\n",
      "Iteration 53050 Training loss 0.0080400500446558 Validation loss 0.04490814357995987 Accuracy 0.8847500681877136\n",
      "Iteration 53060 Training loss 0.005374222993850708 Validation loss 0.04403334856033325 Accuracy 0.8878750205039978\n",
      "Iteration 53070 Training loss 0.0038881879299879074 Validation loss 0.04435489699244499 Accuracy 0.8885000348091125\n",
      "Iteration 53080 Training loss 0.004451108630746603 Validation loss 0.043993640691041946 Accuracy 0.8875000476837158\n",
      "Iteration 53090 Training loss 0.004380181431770325 Validation loss 0.043951746076345444 Accuracy 0.8876250386238098\n",
      "Iteration 53100 Training loss 0.006659480277448893 Validation loss 0.04395265877246857 Accuracy 0.8880000710487366\n",
      "Iteration 53110 Training loss 0.007000811398029327 Validation loss 0.04408080130815506 Accuracy 0.8866250514984131\n",
      "Iteration 53120 Training loss 0.004671577829867601 Validation loss 0.044255562126636505 Accuracy 0.8872500658035278\n",
      "Iteration 53130 Training loss 0.0030002326238900423 Validation loss 0.0442633181810379 Accuracy 0.8856250643730164\n",
      "Iteration 53140 Training loss 0.006917349062860012 Validation loss 0.043854959309101105 Accuracy 0.8880000710487366\n",
      "Iteration 53150 Training loss 0.007881557568907738 Validation loss 0.043954141438007355 Accuracy 0.8872500658035278\n",
      "Iteration 53160 Training loss 0.008056211285293102 Validation loss 0.04403921961784363 Accuracy 0.8873750567436218\n",
      "Iteration 53170 Training loss 0.006770827807486057 Validation loss 0.04401351511478424 Accuracy 0.8876250386238098\n",
      "Iteration 53180 Training loss 0.006720614153891802 Validation loss 0.0442572757601738 Accuracy 0.8873750567436218\n",
      "Iteration 53190 Training loss 0.00957215204834938 Validation loss 0.044171810150146484 Accuracy 0.8872500658035278\n",
      "Iteration 53200 Training loss 0.009267290122807026 Validation loss 0.04454349726438522 Accuracy 0.8857500553131104\n",
      "Iteration 53210 Training loss 0.025954870507121086 Validation loss 0.05230606719851494 Accuracy 0.8750000596046448\n",
      "Iteration 53220 Training loss 0.0047479430213570595 Validation loss 0.044134728610515594 Accuracy 0.8871250152587891\n",
      "Iteration 53230 Training loss 0.012093770317733288 Validation loss 0.043820980936288834 Accuracy 0.8887500166893005\n",
      "Iteration 53240 Training loss 0.009642920456826687 Validation loss 0.04382866248488426 Accuracy 0.8875000476837158\n",
      "Iteration 53250 Training loss 0.006952672265470028 Validation loss 0.04381488263607025 Accuracy 0.8887500166893005\n",
      "Iteration 53260 Training loss 0.01210134569555521 Validation loss 0.04404284805059433 Accuracy 0.8865000605583191\n",
      "Iteration 53270 Training loss 0.12206035107374191 Validation loss 0.12082981318235397 Accuracy 0.7201250195503235\n",
      "Iteration 53280 Training loss 0.00989985466003418 Validation loss 0.04658005014061928 Accuracy 0.8863750696182251\n",
      "Iteration 53290 Training loss 0.008331610821187496 Validation loss 0.04419499263167381 Accuracy 0.8867500424385071\n",
      "Iteration 53300 Training loss 0.003887834493070841 Validation loss 0.04472522810101509 Accuracy 0.8868750333786011\n",
      "Iteration 53310 Training loss 0.006778108887374401 Validation loss 0.043926700949668884 Accuracy 0.8877500295639038\n",
      "Iteration 53320 Training loss 0.006780379451811314 Validation loss 0.043800920248031616 Accuracy 0.8895000219345093\n",
      "Iteration 53330 Training loss 0.007904707454144955 Validation loss 0.044167857617139816 Accuracy 0.8873750567436218\n",
      "Iteration 53340 Training loss 0.0014076563529670238 Validation loss 0.04373292997479439 Accuracy 0.8895000219345093\n",
      "Iteration 53350 Training loss 0.006663183216005564 Validation loss 0.04401832073926926 Accuracy 0.8877500295639038\n",
      "Iteration 53360 Training loss 0.004264639690518379 Validation loss 0.0450613833963871 Accuracy 0.8843750357627869\n",
      "Iteration 53370 Training loss 0.007381049916148186 Validation loss 0.04382796585559845 Accuracy 0.8882500529289246\n",
      "Iteration 53380 Training loss 0.005551083013415337 Validation loss 0.0446992963552475 Accuracy 0.8862500190734863\n",
      "Iteration 53390 Training loss 0.006379910744726658 Validation loss 0.04398978501558304 Accuracy 0.8881250619888306\n",
      "Iteration 53400 Training loss 0.006757860537618399 Validation loss 0.04370877891778946 Accuracy 0.8896250128746033\n",
      "Iteration 53410 Training loss 0.0038807587698101997 Validation loss 0.044672269374132156 Accuracy 0.8852500319480896\n",
      "Iteration 53420 Training loss 0.003196562174707651 Validation loss 0.044721126556396484 Accuracy 0.8858750462532043\n",
      "Iteration 53430 Training loss 0.005134099163115025 Validation loss 0.04390271380543709 Accuracy 0.8877500295639038\n",
      "Iteration 53440 Training loss 0.013032320886850357 Validation loss 0.04846850782632828 Accuracy 0.877500057220459\n",
      "Iteration 53450 Training loss 0.008677498437464237 Validation loss 0.04580164700746536 Accuracy 0.8821250200271606\n",
      "Iteration 53460 Training loss 0.010814685374498367 Validation loss 0.05015208572149277 Accuracy 0.8787500262260437\n",
      "Iteration 53470 Training loss 0.008167549036443233 Validation loss 0.04476340115070343 Accuracy 0.8853750228881836\n",
      "Iteration 53480 Training loss 0.005268940236419439 Validation loss 0.043908871710300446 Accuracy 0.8873750567436218\n",
      "Iteration 53490 Training loss 0.005457145627588034 Validation loss 0.044662751257419586 Accuracy 0.8850000500679016\n",
      "Iteration 53500 Training loss 0.004587796051055193 Validation loss 0.04504534974694252 Accuracy 0.8837500214576721\n",
      "Iteration 53510 Training loss 0.006620828993618488 Validation loss 0.04431788995862007 Accuracy 0.8870000243186951\n",
      "Iteration 53520 Training loss 0.0036126466002315283 Validation loss 0.04444616287946701 Accuracy 0.8865000605583191\n",
      "Iteration 53530 Training loss 0.007287026382982731 Validation loss 0.04426905885338783 Accuracy 0.8873750567436218\n",
      "Iteration 53540 Training loss 0.0037393851671367884 Validation loss 0.044053636491298676 Accuracy 0.8862500190734863\n",
      "Iteration 53550 Training loss 0.005640367045998573 Validation loss 0.04405513033270836 Accuracy 0.8872500658035278\n",
      "Iteration 53560 Training loss 0.002737671136856079 Validation loss 0.04388713464140892 Accuracy 0.8871250152587891\n",
      "Iteration 53570 Training loss 0.007623353041708469 Validation loss 0.044043902307748795 Accuracy 0.8877500295639038\n",
      "Iteration 53580 Training loss 0.005544166546314955 Validation loss 0.04486697539687157 Accuracy 0.8862500190734863\n",
      "Iteration 53590 Training loss 0.05886348709464073 Validation loss 0.09205134958028793 Accuracy 0.781000018119812\n",
      "Iteration 53600 Training loss 0.0071631502360105515 Validation loss 0.045327670872211456 Accuracy 0.8876250386238098\n",
      "Iteration 53610 Training loss 0.0022852998226881027 Validation loss 0.04419327527284622 Accuracy 0.8885000348091125\n",
      "Iteration 53620 Training loss 0.00295281526632607 Validation loss 0.04418947175145149 Accuracy 0.8885000348091125\n",
      "Iteration 53630 Training loss 0.0052652377635240555 Validation loss 0.0439719557762146 Accuracy 0.8886250257492065\n",
      "Iteration 53640 Training loss 0.006207089405506849 Validation loss 0.04385879635810852 Accuracy 0.8888750672340393\n",
      "Iteration 53650 Training loss 0.004197034519165754 Validation loss 0.04385920614004135 Accuracy 0.8877500295639038\n",
      "Iteration 53660 Training loss 0.007003073580563068 Validation loss 0.04408968612551689 Accuracy 0.8866250514984131\n",
      "Iteration 53670 Training loss 0.006077178753912449 Validation loss 0.0444943942129612 Accuracy 0.8857500553131104\n",
      "Iteration 53680 Training loss 0.022113170474767685 Validation loss 0.05412277579307556 Accuracy 0.8647500276565552\n",
      "Iteration 53690 Training loss 0.008166123181581497 Validation loss 0.04466764256358147 Accuracy 0.889875054359436\n",
      "Iteration 53700 Training loss 0.007148854434490204 Validation loss 0.04360643029212952 Accuracy 0.8883750438690186\n",
      "Iteration 53710 Training loss 0.008294221945106983 Validation loss 0.04360736906528473 Accuracy 0.8887500166893005\n",
      "Iteration 53720 Training loss 0.006072408519685268 Validation loss 0.04387092962861061 Accuracy 0.8880000710487366\n",
      "Iteration 53730 Training loss 0.006394446827471256 Validation loss 0.04373374208807945 Accuracy 0.8888750672340393\n",
      "Iteration 53740 Training loss 0.006357057951390743 Validation loss 0.04481262341141701 Accuracy 0.8848750591278076\n",
      "Iteration 53750 Training loss 0.012244249694049358 Validation loss 0.0437355600297451 Accuracy 0.8882500529289246\n",
      "Iteration 53760 Training loss 0.011390064842998981 Validation loss 0.043855614960193634 Accuracy 0.8882500529289246\n",
      "Iteration 53770 Training loss 0.004590850789099932 Validation loss 0.045188553631305695 Accuracy 0.8842500448226929\n",
      "Iteration 53780 Training loss 0.011425876058638096 Validation loss 0.04407013580203056 Accuracy 0.8895000219345093\n",
      "Iteration 53790 Training loss 0.005696208216249943 Validation loss 0.04397917911410332 Accuracy 0.8885000348091125\n",
      "Iteration 53800 Training loss 0.010842042043805122 Validation loss 0.043980203568935394 Accuracy 0.8881250619888306\n",
      "Iteration 53810 Training loss 0.00787270162254572 Validation loss 0.04531549662351608 Accuracy 0.8830000162124634\n",
      "Iteration 53820 Training loss 0.00408126087859273 Validation loss 0.044089075177907944 Accuracy 0.8875000476837158\n",
      "Iteration 53830 Training loss 0.011129488237202168 Validation loss 0.04554814472794533 Accuracy 0.8833750486373901\n",
      "Iteration 53840 Training loss 0.007559380494058132 Validation loss 0.04424193128943443 Accuracy 0.8877500295639038\n",
      "Iteration 53850 Training loss 0.010706926696002483 Validation loss 0.044117413461208344 Accuracy 0.8878750205039978\n",
      "Iteration 53860 Training loss 0.07568176835775375 Validation loss 0.09650268405675888 Accuracy 0.7698750495910645\n",
      "Iteration 53870 Training loss 0.009706414304673672 Validation loss 0.04575362429022789 Accuracy 0.8863750696182251\n",
      "Iteration 53880 Training loss 0.003558606607839465 Validation loss 0.04481583461165428 Accuracy 0.8852500319480896\n",
      "Iteration 53890 Training loss 0.008820107206702232 Validation loss 0.044277969747781754 Accuracy 0.8863750696182251\n",
      "Iteration 53900 Training loss 0.005410273093730211 Validation loss 0.04409278184175491 Accuracy 0.8865000605583191\n",
      "Iteration 53910 Training loss 0.003323172451928258 Validation loss 0.04389480873942375 Accuracy 0.8890000581741333\n",
      "Iteration 53920 Training loss 0.007585109211504459 Validation loss 0.04404347017407417 Accuracy 0.8887500166893005\n",
      "Iteration 53930 Training loss 0.005397669970989227 Validation loss 0.0440688282251358 Accuracy 0.8871250152587891\n",
      "Iteration 53940 Training loss 0.008172355592250824 Validation loss 0.04419684782624245 Accuracy 0.8865000605583191\n",
      "Iteration 53950 Training loss 0.00448980275541544 Validation loss 0.04599546268582344 Accuracy 0.8811250329017639\n",
      "Iteration 53960 Training loss 0.010590308345854282 Validation loss 0.04774590581655502 Accuracy 0.8808750510215759\n",
      "Iteration 53970 Training loss 0.007880083285272121 Validation loss 0.044216614216566086 Accuracy 0.8880000710487366\n",
      "Iteration 53980 Training loss 0.007935641333460808 Validation loss 0.044046662747859955 Accuracy 0.8867500424385071\n",
      "Iteration 53990 Training loss 0.00609798775985837 Validation loss 0.04416154697537422 Accuracy 0.8865000605583191\n",
      "Iteration 54000 Training loss 0.0026391767896711826 Validation loss 0.044054608792066574 Accuracy 0.8867500424385071\n",
      "Iteration 54010 Training loss 0.00730486772954464 Validation loss 0.04423172026872635 Accuracy 0.8873750567436218\n",
      "Iteration 54020 Training loss 0.00624607689678669 Validation loss 0.04435170814394951 Accuracy 0.8863750696182251\n",
      "Iteration 54030 Training loss 0.006553139537572861 Validation loss 0.04405524954199791 Accuracy 0.8883750438690186\n",
      "Iteration 54040 Training loss 0.005611563567072153 Validation loss 0.04412980005145073 Accuracy 0.8876250386238098\n",
      "Iteration 54050 Training loss 0.09359308332204819 Validation loss 0.11598610877990723 Accuracy 0.7262500524520874\n",
      "Iteration 54060 Training loss 0.013069932349026203 Validation loss 0.04472217336297035 Accuracy 0.8885000348091125\n",
      "Iteration 54070 Training loss 0.006555738393217325 Validation loss 0.04396820440888405 Accuracy 0.8895000219345093\n",
      "Iteration 54080 Training loss 0.004592408891767263 Validation loss 0.04495445638895035 Accuracy 0.8860000371932983\n",
      "Iteration 54090 Training loss 0.004848345182836056 Validation loss 0.04387566074728966 Accuracy 0.8891250491142273\n",
      "Iteration 54100 Training loss 0.00501840328797698 Validation loss 0.04412687197327614 Accuracy 0.8873750567436218\n",
      "Iteration 54110 Training loss 0.009641611948609352 Validation loss 0.043940454721450806 Accuracy 0.8891250491142273\n",
      "Iteration 54120 Training loss 0.008015794679522514 Validation loss 0.043895021080970764 Accuracy 0.8886250257492065\n",
      "Iteration 54130 Training loss 0.0065284851007163525 Validation loss 0.04399791359901428 Accuracy 0.8875000476837158\n",
      "Iteration 54140 Training loss 0.0049192169681191444 Validation loss 0.04398208111524582 Accuracy 0.8878750205039978\n",
      "Iteration 54150 Training loss 0.005798376630991697 Validation loss 0.04413064941763878 Accuracy 0.8868750333786011\n",
      "Iteration 54160 Training loss 0.008387369103729725 Validation loss 0.044037699699401855 Accuracy 0.8887500166893005\n",
      "Iteration 54170 Training loss 0.003788318019360304 Validation loss 0.04406438767910004 Accuracy 0.8875000476837158\n",
      "Iteration 54180 Training loss 0.008179127238690853 Validation loss 0.044224418699741364 Accuracy 0.8878750205039978\n",
      "Iteration 54190 Training loss 0.004136099014431238 Validation loss 0.044448018074035645 Accuracy 0.8866250514984131\n",
      "Iteration 54200 Training loss 0.007623220328241587 Validation loss 0.04412946477532387 Accuracy 0.8872500658035278\n",
      "Iteration 54210 Training loss 0.0025026884395629168 Validation loss 0.04414697363972664 Accuracy 0.8877500295639038\n",
      "Iteration 54220 Training loss 0.012070455588400364 Validation loss 0.04439947009086609 Accuracy 0.8860000371932983\n",
      "Iteration 54230 Training loss 0.0067002978175878525 Validation loss 0.044120438396930695 Accuracy 0.8877500295639038\n",
      "Iteration 54240 Training loss 0.004907447844743729 Validation loss 0.04412213712930679 Accuracy 0.8866250514984131\n",
      "Iteration 54250 Training loss 0.004641715902835131 Validation loss 0.04418077692389488 Accuracy 0.8867500424385071\n",
      "Iteration 54260 Training loss 0.003935573156923056 Validation loss 0.04408721998333931 Accuracy 0.8875000476837158\n",
      "Iteration 54270 Training loss 0.006686615291982889 Validation loss 0.04451454430818558 Accuracy 0.8860000371932983\n",
      "Iteration 54280 Training loss 0.008917816914618015 Validation loss 0.04412161558866501 Accuracy 0.8882500529289246\n",
      "Iteration 54290 Training loss 0.00417397590354085 Validation loss 0.04416016489267349 Accuracy 0.8883750438690186\n",
      "Iteration 54300 Training loss 0.005687034223228693 Validation loss 0.04710504412651062 Accuracy 0.8805000185966492\n",
      "Iteration 54310 Training loss 0.010466091334819794 Validation loss 0.04849793389439583 Accuracy 0.8805000185966492\n",
      "Iteration 54320 Training loss 0.007523111999034882 Validation loss 0.04473860561847687 Accuracy 0.8872500658035278\n",
      "Iteration 54330 Training loss 0.003355225780978799 Validation loss 0.0442693755030632 Accuracy 0.8873750567436218\n",
      "Iteration 54340 Training loss 0.0026066601276397705 Validation loss 0.04419117048382759 Accuracy 0.8882500529289246\n",
      "Iteration 54350 Training loss 0.004179021809250116 Validation loss 0.044142939150333405 Accuracy 0.8887500166893005\n",
      "Iteration 54360 Training loss 0.007720530964434147 Validation loss 0.043842725455760956 Accuracy 0.8887500166893005\n",
      "Iteration 54370 Training loss 0.0058529735542833805 Validation loss 0.04512302577495575 Accuracy 0.8857500553131104\n",
      "Iteration 54380 Training loss 0.010667159222066402 Validation loss 0.047134775668382645 Accuracy 0.8851250410079956\n",
      "Iteration 54390 Training loss 0.005617023445665836 Validation loss 0.04422425106167793 Accuracy 0.8882500529289246\n",
      "Iteration 54400 Training loss 0.009689642116427422 Validation loss 0.04432864487171173 Accuracy 0.8882500529289246\n",
      "Iteration 54410 Training loss 0.004637992475181818 Validation loss 0.04399767518043518 Accuracy 0.8878750205039978\n",
      "Iteration 54420 Training loss 0.005235770251601934 Validation loss 0.044150836765766144 Accuracy 0.8875000476837158\n",
      "Iteration 54430 Training loss 0.005838281475007534 Validation loss 0.04410431906580925 Accuracy 0.8872500658035278\n",
      "Iteration 54440 Training loss 0.007459376938641071 Validation loss 0.04407210275530815 Accuracy 0.8876250386238098\n",
      "Iteration 54450 Training loss 0.004551757592707872 Validation loss 0.04400663077831268 Accuracy 0.8876250386238098\n",
      "Iteration 54460 Training loss 0.004721292294561863 Validation loss 0.044201646000146866 Accuracy 0.8880000710487366\n",
      "Iteration 54470 Training loss 0.005455654580146074 Validation loss 0.04421417415142059 Accuracy 0.8863750696182251\n",
      "Iteration 54480 Training loss 0.007673128042370081 Validation loss 0.04499261453747749 Accuracy 0.8837500214576721\n",
      "Iteration 54490 Training loss 0.004323138389736414 Validation loss 0.044136110693216324 Accuracy 0.8876250386238098\n",
      "Iteration 54500 Training loss 0.004787483252584934 Validation loss 0.04429491609334946 Accuracy 0.8867500424385071\n",
      "Iteration 54510 Training loss 0.004986669402569532 Validation loss 0.04426350072026253 Accuracy 0.8858750462532043\n",
      "Iteration 54520 Training loss 0.010567281395196915 Validation loss 0.04441968351602554 Accuracy 0.8870000243186951\n",
      "Iteration 54530 Training loss 0.003153864061459899 Validation loss 0.043917249888181686 Accuracy 0.8891250491142273\n",
      "Iteration 54540 Training loss 0.004975149407982826 Validation loss 0.04439130052924156 Accuracy 0.8871250152587891\n",
      "Iteration 54550 Training loss 0.004947151057422161 Validation loss 0.044437844306230545 Accuracy 0.8872500658035278\n",
      "Iteration 54560 Training loss 0.0057228864170610905 Validation loss 0.04403472691774368 Accuracy 0.8878750205039978\n",
      "Iteration 54570 Training loss 0.005163026507943869 Validation loss 0.04430294409394264 Accuracy 0.8867500424385071\n",
      "Iteration 54580 Training loss 0.004445439204573631 Validation loss 0.044288188219070435 Accuracy 0.8860000371932983\n",
      "Iteration 54590 Training loss 0.0021749266888946295 Validation loss 0.04526850953698158 Accuracy 0.8840000629425049\n",
      "Iteration 54600 Training loss 0.004846935626119375 Validation loss 0.044427234679460526 Accuracy 0.8856250643730164\n",
      "Iteration 54610 Training loss 0.004821827169507742 Validation loss 0.044552262872457504 Accuracy 0.8852500319480896\n",
      "Iteration 54620 Training loss 0.006855761632323265 Validation loss 0.04660071060061455 Accuracy 0.8793750405311584\n",
      "Iteration 54630 Training loss 0.004514235071837902 Validation loss 0.045058198273181915 Accuracy 0.8885000348091125\n",
      "Iteration 54640 Training loss 0.0037346414756029844 Validation loss 0.044025491923093796 Accuracy 0.8890000581741333\n",
      "Iteration 54650 Training loss 0.00808036234229803 Validation loss 0.04411228373646736 Accuracy 0.8877500295639038\n",
      "Iteration 54660 Training loss 0.006778343580663204 Validation loss 0.04408537596464157 Accuracy 0.8872500658035278\n",
      "Iteration 54670 Training loss 0.007088474463671446 Validation loss 0.044914644211530685 Accuracy 0.8850000500679016\n",
      "Iteration 54680 Training loss 0.006182619370520115 Validation loss 0.04404943436384201 Accuracy 0.8890000581741333\n",
      "Iteration 54690 Training loss 0.00433339923620224 Validation loss 0.0441654808819294 Accuracy 0.8872500658035278\n",
      "Iteration 54700 Training loss 0.006354219280183315 Validation loss 0.04422421008348465 Accuracy 0.8873750567436218\n",
      "Iteration 54710 Training loss 0.0024615328293293715 Validation loss 0.044052425771951675 Accuracy 0.8880000710487366\n",
      "Iteration 54720 Training loss 0.004019204061478376 Validation loss 0.0448896549642086 Accuracy 0.8846250176429749\n",
      "Iteration 54730 Training loss 0.005431742407381535 Validation loss 0.045033786445856094 Accuracy 0.8838750123977661\n",
      "Iteration 54740 Training loss 0.0038024510722607374 Validation loss 0.044262416660785675 Accuracy 0.8872500658035278\n",
      "Iteration 54750 Training loss 0.008553732186555862 Validation loss 0.04423091933131218 Accuracy 0.8871250152587891\n",
      "Iteration 54760 Training loss 0.002996375085785985 Validation loss 0.04416482895612717 Accuracy 0.8883750438690186\n",
      "Iteration 54770 Training loss 0.007211081683635712 Validation loss 0.04588732123374939 Accuracy 0.8846250176429749\n",
      "Iteration 54780 Training loss 0.0065327417105436325 Validation loss 0.04433506354689598 Accuracy 0.8866250514984131\n",
      "Iteration 54790 Training loss 0.003444335423409939 Validation loss 0.04413793236017227 Accuracy 0.8872500658035278\n",
      "Iteration 54800 Training loss 0.005146298091858625 Validation loss 0.04423986375331879 Accuracy 0.8868750333786011\n",
      "Iteration 54810 Training loss 0.00697700958698988 Validation loss 0.0452481247484684 Accuracy 0.8856250643730164\n",
      "Iteration 54820 Training loss 0.014462020248174667 Validation loss 0.05169636011123657 Accuracy 0.8708750605583191\n",
      "Iteration 54830 Training loss 0.01706920564174652 Validation loss 0.04930434748530388 Accuracy 0.8792500495910645\n",
      "Iteration 54840 Training loss 0.037256401032209396 Validation loss 0.061056092381477356 Accuracy 0.8502500653266907\n",
      "Iteration 54850 Training loss 0.005612213630229235 Validation loss 0.044968705624341965 Accuracy 0.8878750205039978\n",
      "Iteration 54860 Training loss 0.008089832961559296 Validation loss 0.04456596449017525 Accuracy 0.8890000581741333\n",
      "Iteration 54870 Training loss 0.009865417145192623 Validation loss 0.04490244761109352 Accuracy 0.8857500553131104\n",
      "Iteration 54880 Training loss 0.008526772260665894 Validation loss 0.04411947354674339 Accuracy 0.8873750567436218\n",
      "Iteration 54890 Training loss 0.003910680767148733 Validation loss 0.04407289996743202 Accuracy 0.8876250386238098\n",
      "Iteration 54900 Training loss 0.004970093257725239 Validation loss 0.04415285587310791 Accuracy 0.8873750567436218\n",
      "Iteration 54910 Training loss 0.004623685497790575 Validation loss 0.04400306195020676 Accuracy 0.8876250386238098\n",
      "Iteration 54920 Training loss 0.0100264186039567 Validation loss 0.04418843239545822 Accuracy 0.8863750696182251\n",
      "Iteration 54930 Training loss 0.004499021451920271 Validation loss 0.04461551830172539 Accuracy 0.8853750228881836\n",
      "Iteration 54940 Training loss 0.003816444193944335 Validation loss 0.04430218040943146 Accuracy 0.8861250281333923\n",
      "Iteration"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbinary_model_3_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 535\u001b[39m, in \u001b[36mbinary_classification_three_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, reg1, reg2, reg3, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2, train_layer_3)\u001b[39m\n\u001b[32m    533\u001b[39m accuracy = torch.mean(((\u001b[38;5;28mself\u001b[39m.forward(x_valid)[\u001b[32m0\u001b[39m] > \u001b[32m0.5\u001b[39m).to(dtype) == y_valid).to(dtype))\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m.accuracy_trajectory.append(accuracy.item())\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIteration\u001b[39m\u001b[33m\"\u001b[39m, i, \u001b[33m\"\u001b[39m\u001b[33mTraining loss\u001b[39m\u001b[33m\"\u001b[39m, training_loss.item(), \u001b[33m\"\u001b[39m\u001b[33mValidation loss\u001b[39m\u001b[33m\"\u001b[39m, validation_loss.item(), \u001b[33m\"\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m\"\u001b[39m, accuracy.item())\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# Soustraction du temps de sauvegarde\u001b[39;00m\n\u001b[32m    537\u001b[39m unwanted_time += time.time() - unwanted_time_begin \n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "binary_model_3_layer.train_layers(x_train, y_train, x_valid, y_valid, 1.6, 1e-4, 0, 0, 0, 1, 0.01, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "347eb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer_1_untrained = binary_classification_three_layer_NN(3072, 2048, 2048, eps_init = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3149bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 1.5, the number of datas used for the training is 17026752.25872509 and the number of iterations is 42566.\n",
      "Iteration 0 Training loss 0.12500081956386566 Validation loss 0.1250009983778 Accuracy 0.5\n",
      "Iteration 10 Training loss 0.12426601350307465 Validation loss 0.12450466305017471 Accuracy 0.5\n",
      "Iteration 20 Training loss 0.12398973107337952 Validation loss 0.12418626993894577 Accuracy 0.5\n",
      "Iteration 30 Training loss 0.1239936351776123 Validation loss 0.12389950454235077 Accuracy 0.5003750324249268\n",
      "Iteration 40 Training loss 0.12299794703722 Validation loss 0.12358492612838745 Accuracy 0.5003750324249268\n",
      "Iteration 50 Training loss 0.1238676980137825 Validation loss 0.12329433858394623 Accuracy 0.5015000104904175\n",
      "Iteration 60 Training loss 0.12382584065198898 Validation loss 0.12299849838018417 Accuracy 0.5056250095367432\n",
      "Iteration 70 Training loss 0.12160582095384598 Validation loss 0.12270954251289368 Accuracy 0.5121250152587891\n",
      "Iteration 80 Training loss 0.12226653844118118 Validation loss 0.12241272628307343 Accuracy 0.5383750200271606\n",
      "Iteration 90 Training loss 0.12233316153287888 Validation loss 0.12208987772464752 Accuracy 0.530750036239624\n",
      "Iteration 100 Training loss 0.12150275707244873 Validation loss 0.12177664786577225 Accuracy 0.5545000433921814\n",
      "Iteration 110 Training loss 0.12068451941013336 Validation loss 0.12143169343471527 Accuracy 0.5478750467300415\n",
      "Iteration 120 Training loss 0.12114730477333069 Validation loss 0.12109870463609695 Accuracy 0.5532500147819519\n",
      "Iteration 130 Training loss 0.12187673151493073 Validation loss 0.12075270712375641 Accuracy 0.5758750438690186\n",
      "Iteration 140 Training loss 0.12150336802005768 Validation loss 0.1203947365283966 Accuracy 0.5901250243186951\n",
      "Iteration 150 Training loss 0.11914005130529404 Validation loss 0.12002328038215637 Accuracy 0.6076250076293945\n",
      "Iteration 160 Training loss 0.11922638863325119 Validation loss 0.11964116990566254 Accuracy 0.6163750290870667\n",
      "Iteration 170 Training loss 0.11833209544420242 Validation loss 0.11923659592866898 Accuracy 0.643125057220459\n",
      "Iteration 180 Training loss 0.11932384222745895 Validation loss 0.11882791668176651 Accuracy 0.6541250348091125\n",
      "Iteration 190 Training loss 0.11824792623519897 Validation loss 0.1184067577123642 Accuracy 0.6630000472068787\n",
      "Iteration 200 Training loss 0.1171611025929451 Validation loss 0.11796920001506805 Accuracy 0.6808750033378601\n",
      "Iteration 210 Training loss 0.11763301491737366 Validation loss 0.11749573051929474 Accuracy 0.6797500252723694\n",
      "Iteration 220 Training loss 0.1177871897816658 Validation loss 0.11701357364654541 Accuracy 0.6805000305175781\n",
      "Iteration 230 Training loss 0.1171034574508667 Validation loss 0.11656138300895691 Accuracy 0.6876250505447388\n",
      "Iteration 240 Training loss 0.11567439883947372 Validation loss 0.11603683978319168 Accuracy 0.6953750252723694\n",
      "Iteration 250 Training loss 0.11566507071256638 Validation loss 0.11552048474550247 Accuracy 0.6852500438690186\n",
      "Iteration 260 Training loss 0.11386704444885254 Validation loss 0.11498410999774933 Accuracy 0.6816250085830688\n",
      "Iteration 270 Training loss 0.11195962876081467 Validation loss 0.11439892649650574 Accuracy 0.6955000162124634\n",
      "Iteration 280 Training loss 0.11295853555202484 Validation loss 0.11378650367259979 Accuracy 0.7035000324249268\n",
      "Iteration 290 Training loss 0.11095702648162842 Validation loss 0.11316952109336853 Accuracy 0.7051250338554382\n",
      "Iteration 300 Training loss 0.11122428625822067 Validation loss 0.11253130435943604 Accuracy 0.7175000309944153\n",
      "Iteration 310 Training loss 0.11130460351705551 Validation loss 0.1119128167629242 Accuracy 0.7251250147819519\n",
      "Iteration 320 Training loss 0.10985273122787476 Validation loss 0.11126255989074707 Accuracy 0.721875011920929\n",
      "Iteration 330 Training loss 0.11197946965694427 Validation loss 0.11063212901353836 Accuracy 0.7240000367164612\n",
      "Iteration 340 Training loss 0.11040907353162766 Validation loss 0.10995689779520035 Accuracy 0.7357500195503235\n",
      "Iteration 350 Training loss 0.10559430718421936 Validation loss 0.109294094145298 Accuracy 0.7305000424385071\n",
      "Iteration 360 Training loss 0.10954295843839645 Validation loss 0.10865043848752975 Accuracy 0.7255000472068787\n",
      "Iteration 370 Training loss 0.1082962229847908 Validation loss 0.1079115942120552 Accuracy 0.7361250519752502\n",
      "Iteration 380 Training loss 0.10929186642169952 Validation loss 0.10722336918115616 Accuracy 0.7378750443458557\n",
      "Iteration 390 Training loss 0.10715131461620331 Validation loss 0.10651329904794693 Accuracy 0.7392500638961792\n",
      "Iteration 400 Training loss 0.10558973252773285 Validation loss 0.10580719262361526 Accuracy 0.7402500510215759\n",
      "Iteration 410 Training loss 0.10467255115509033 Validation loss 0.10512010008096695 Accuracy 0.7395000457763672\n",
      "Iteration 420 Training loss 0.10309846699237823 Validation loss 0.10440416634082794 Accuracy 0.7416250109672546\n",
      "Iteration 430 Training loss 0.10294951498508453 Validation loss 0.10370064526796341 Accuracy 0.7435000538825989\n",
      "Iteration 440 Training loss 0.10257615149021149 Validation loss 0.103031225502491 Accuracy 0.7450000643730164\n",
      "Iteration 450 Training loss 0.1024819165468216 Validation loss 0.10238690674304962 Accuracy 0.7455000281333923\n",
      "Iteration 460 Training loss 0.10018076002597809 Validation loss 0.10169459879398346 Accuracy 0.7457500100135803\n",
      "Iteration 470 Training loss 0.09848402440547943 Validation loss 0.10101952403783798 Accuracy 0.7457500100135803\n",
      "Iteration 480 Training loss 0.10178877413272858 Validation loss 0.10037302225828171 Accuracy 0.7453750371932983\n",
      "Iteration 490 Training loss 0.09751712530851364 Validation loss 0.09975741803646088 Accuracy 0.7455000281333923\n",
      "Iteration 500 Training loss 0.10108835995197296 Validation loss 0.09909859299659729 Accuracy 0.7473750114440918\n",
      "Iteration 510 Training loss 0.09647805988788605 Validation loss 0.09849175810813904 Accuracy 0.7472500205039978\n",
      "Iteration 520 Training loss 0.09702835232019424 Validation loss 0.09790407866239548 Accuracy 0.7490000128746033\n",
      "Iteration 530 Training loss 0.09752210229635239 Validation loss 0.097310870885849 Accuracy 0.7486250400543213\n",
      "Iteration 540 Training loss 0.09715737402439117 Validation loss 0.0967499241232872 Accuracy 0.7487500309944153\n",
      "Iteration 550 Training loss 0.09880515187978745 Validation loss 0.09619378298521042 Accuracy 0.7485000491142273\n",
      "Iteration 560 Training loss 0.09609565883874893 Validation loss 0.09567249566316605 Accuracy 0.749125063419342\n",
      "Iteration 570 Training loss 0.09655209630727768 Validation loss 0.09515262395143509 Accuracy 0.749750018119812\n",
      "Iteration 580 Training loss 0.0950784683227539 Validation loss 0.09468814730644226 Accuracy 0.7508750557899475\n",
      "Iteration 590 Training loss 0.09321288019418716 Validation loss 0.0942227765917778 Accuracy 0.7503750324249268\n",
      "Iteration 600 Training loss 0.09369157254695892 Validation loss 0.09377056360244751 Accuracy 0.7535000443458557\n",
      "Iteration 610 Training loss 0.10002002120018005 Validation loss 0.09334088861942291 Accuracy 0.7532500624656677\n",
      "Iteration 620 Training loss 0.09706538915634155 Validation loss 0.0928640365600586 Accuracy 0.752875030040741\n",
      "Iteration 630 Training loss 0.0896449089050293 Validation loss 0.09247201681137085 Accuracy 0.7538750171661377\n",
      "Iteration 640 Training loss 0.08701477199792862 Validation loss 0.09204709529876709 Accuracy 0.7533750534057617\n",
      "Iteration 650 Training loss 0.0869930163025856 Validation loss 0.0916605293750763 Accuracy 0.7541250586509705\n",
      "Iteration 660 Training loss 0.09268945455551147 Validation loss 0.09128878265619278 Accuracy 0.7548750638961792\n",
      "Iteration 670 Training loss 0.09121432900428772 Validation loss 0.09091450273990631 Accuracy 0.7562500238418579\n",
      "Iteration 680 Training loss 0.09003812819719315 Validation loss 0.09053632616996765 Accuracy 0.7557500600814819\n",
      "Iteration 690 Training loss 0.0978630781173706 Validation loss 0.09019993990659714 Accuracy 0.7561250329017639\n",
      "Iteration 700 Training loss 0.08488410711288452 Validation loss 0.08983557671308517 Accuracy 0.7571250200271606\n",
      "Iteration 710 Training loss 0.08829464018344879 Validation loss 0.0894998162984848 Accuracy 0.7576250433921814\n",
      "Iteration 720 Training loss 0.0869097113609314 Validation loss 0.089225172996521 Accuracy 0.7572500109672546\n",
      "Iteration 730 Training loss 0.09241233021020889 Validation loss 0.08887903392314911 Accuracy 0.7592500448226929\n",
      "Iteration 740 Training loss 0.08989213407039642 Validation loss 0.08862049132585526 Accuracy 0.7592500448226929\n",
      "Iteration 750 Training loss 0.08999384939670563 Validation loss 0.08833251148462296 Accuracy 0.7597500085830688\n",
      "Iteration 760 Training loss 0.09232745319604874 Validation loss 0.08807039260864258 Accuracy 0.7608750462532043\n",
      "Iteration 770 Training loss 0.08918840438127518 Validation loss 0.08774840086698532 Accuracy 0.7612500190734863\n",
      "Iteration 780 Training loss 0.08537967503070831 Validation loss 0.0875166729092598 Accuracy 0.7601250410079956\n",
      "Iteration 790 Training loss 0.083948515355587 Validation loss 0.0871991440653801 Accuracy 0.7617500424385071\n",
      "Iteration 800 Training loss 0.08654917776584625 Validation loss 0.08696941286325455 Accuracy 0.7617500424385071\n",
      "Iteration 810 Training loss 0.08588296920061111 Validation loss 0.08670075982809067 Accuracy 0.7632500529289246\n",
      "Iteration 820 Training loss 0.08182067424058914 Validation loss 0.08645156025886536 Accuracy 0.7635000348091125\n",
      "Iteration 830 Training loss 0.0886494591832161 Validation loss 0.08622951060533524 Accuracy 0.7633750438690186\n",
      "Iteration 840 Training loss 0.0856594443321228 Validation loss 0.08598906546831131 Accuracy 0.7645000219345093\n",
      "Iteration 850 Training loss 0.08595116436481476 Validation loss 0.08575630187988281 Accuracy 0.764875054359436\n",
      "Iteration 860 Training loss 0.08998819440603256 Validation loss 0.0856635794043541 Accuracy 0.7636250257492065\n",
      "Iteration 870 Training loss 0.08833852410316467 Validation loss 0.08532962203025818 Accuracy 0.7657500505447388\n",
      "Iteration 880 Training loss 0.08810758590698242 Validation loss 0.08513540029525757 Accuracy 0.7667500376701355\n",
      "Iteration 890 Training loss 0.09075355529785156 Validation loss 0.08493229746818542 Accuracy 0.7662500143051147\n",
      "Iteration 900 Training loss 0.08255898952484131 Validation loss 0.08474907279014587 Accuracy 0.7671250104904175\n",
      "Iteration 910 Training loss 0.08788169920444489 Validation loss 0.08456875383853912 Accuracy 0.7678750157356262\n",
      "Iteration 920 Training loss 0.07703004032373428 Validation loss 0.08435263484716415 Accuracy 0.768375039100647\n",
      "Iteration 930 Training loss 0.0860258936882019 Validation loss 0.08413396030664444 Accuracy 0.768375039100647\n",
      "Iteration 940 Training loss 0.07857412099838257 Validation loss 0.08395860344171524 Accuracy 0.7688750624656677\n",
      "Iteration 950 Training loss 0.08394858986139297 Validation loss 0.08378169685602188 Accuracy 0.768375039100647\n",
      "Iteration 960 Training loss 0.0870414525270462 Validation loss 0.0835854709148407 Accuracy 0.7690000534057617\n",
      "Iteration 970 Training loss 0.08496209979057312 Validation loss 0.08340293914079666 Accuracy 0.7688750624656677\n",
      "Iteration 980 Training loss 0.08516985923051834 Validation loss 0.08323801308870316 Accuracy 0.7688750624656677\n",
      "Iteration 990 Training loss 0.08830976486206055 Validation loss 0.08306573331356049 Accuracy 0.7702500224113464\n",
      "Iteration 1000 Training loss 0.0902993381023407 Validation loss 0.08292009681463242 Accuracy 0.7712500095367432\n",
      "Iteration 1010 Training loss 0.08626513928174973 Validation loss 0.08276107907295227 Accuracy 0.7701250314712524\n",
      "Iteration 1020 Training loss 0.08271272480487823 Validation loss 0.08262576907873154 Accuracy 0.7712500095367432\n",
      "Iteration 1030 Training loss 0.07569755613803864 Validation loss 0.08250627666711807 Accuracy 0.7711250185966492\n",
      "Iteration 1040 Training loss 0.0806470438838005 Validation loss 0.08233862370252609 Accuracy 0.7716250419616699\n",
      "Iteration 1050 Training loss 0.08433479070663452 Validation loss 0.08217282593250275 Accuracy 0.7726250290870667\n",
      "Iteration 1060 Training loss 0.0827551856637001 Validation loss 0.08209371566772461 Accuracy 0.7731250524520874\n",
      "Iteration 1070 Training loss 0.08452484011650085 Validation loss 0.08189354836940765 Accuracy 0.7723750472068787\n",
      "Iteration 1080 Training loss 0.08294137567281723 Validation loss 0.0817965641617775 Accuracy 0.7726250290870667\n",
      "Iteration 1090 Training loss 0.0835752859711647 Validation loss 0.0816134512424469 Accuracy 0.7733750343322754\n",
      "Iteration 1100 Training loss 0.0804288238286972 Validation loss 0.08149406313896179 Accuracy 0.7738750576972961\n",
      "Iteration 1110 Training loss 0.08161729574203491 Validation loss 0.08136319369077682 Accuracy 0.7740000486373901\n",
      "Iteration 1120 Training loss 0.07789608836174011 Validation loss 0.08124019205570221 Accuracy 0.7740000486373901\n",
      "Iteration 1130 Training loss 0.08513131737709045 Validation loss 0.0810912474989891 Accuracy 0.7751250267028809\n",
      "Iteration 1140 Training loss 0.07651800662279129 Validation loss 0.08101620525121689 Accuracy 0.7750000357627869\n",
      "Iteration 1150 Training loss 0.08178745210170746 Validation loss 0.08085707575082779 Accuracy 0.7753750085830688\n",
      "Iteration 1160 Training loss 0.0875249058008194 Validation loss 0.08072426170110703 Accuracy 0.7766250371932983\n",
      "Iteration 1170 Training loss 0.08908329159021378 Validation loss 0.08061575889587402 Accuracy 0.7768750190734863\n",
      "Iteration 1180 Training loss 0.0788378119468689 Validation loss 0.0805370956659317 Accuracy 0.7760000228881836\n",
      "Iteration 1190 Training loss 0.0844629630446434 Validation loss 0.08041328936815262 Accuracy 0.7763750553131104\n",
      "Iteration 1200 Training loss 0.07579416781663895 Validation loss 0.08026114851236343 Accuracy 0.7773750424385071\n",
      "Iteration 1210 Training loss 0.0815722718834877 Validation loss 0.08021536469459534 Accuracy 0.7767500281333923\n",
      "Iteration 1220 Training loss 0.08250720053911209 Validation loss 0.08006010949611664 Accuracy 0.7777500152587891\n",
      "Iteration 1230 Training loss 0.07611890882253647 Validation loss 0.07994731515645981 Accuracy 0.7776250243186951\n",
      "Iteration 1240 Training loss 0.08220675587654114 Validation loss 0.07987350225448608 Accuracy 0.7788750529289246\n",
      "Iteration 1250 Training loss 0.07933767884969711 Validation loss 0.07974812388420105 Accuracy 0.7790000438690186\n",
      "Iteration 1260 Training loss 0.08456609398126602 Validation loss 0.07965018600225449 Accuracy 0.7791250348091125\n",
      "Iteration 1270 Training loss 0.07414901256561279 Validation loss 0.07953554391860962 Accuracy 0.7796250581741333\n",
      "Iteration 1280 Training loss 0.07269716262817383 Validation loss 0.07946469634771347 Accuracy 0.7797500491142273\n",
      "Iteration 1290 Training loss 0.07197772711515427 Validation loss 0.07934992760419846 Accuracy 0.780500054359436\n",
      "Iteration 1300 Training loss 0.0796135812997818 Validation loss 0.07923373579978943 Accuracy 0.78062504529953\n",
      "Iteration 1310 Training loss 0.08395934849977493 Validation loss 0.0791463553905487 Accuracy 0.7816250324249268\n",
      "Iteration 1320 Training loss 0.08301802724599838 Validation loss 0.07906699180603027 Accuracy 0.7816250324249268\n",
      "Iteration 1330 Training loss 0.07675515115261078 Validation loss 0.07921020686626434 Accuracy 0.7795000076293945\n",
      "Iteration 1340 Training loss 0.07745920121669769 Validation loss 0.07888954877853394 Accuracy 0.7821250557899475\n",
      "Iteration 1350 Training loss 0.08142798393964767 Validation loss 0.07879985123872757 Accuracy 0.7826250195503235\n",
      "Iteration 1360 Training loss 0.07890051603317261 Validation loss 0.07872089743614197 Accuracy 0.7827500104904175\n",
      "Iteration 1370 Training loss 0.07304943352937698 Validation loss 0.07866278290748596 Accuracy 0.7830000519752502\n",
      "Iteration 1380 Training loss 0.07631705701351166 Validation loss 0.07855980098247528 Accuracy 0.7832500338554382\n",
      "Iteration 1390 Training loss 0.06942962855100632 Validation loss 0.07847470790147781 Accuracy 0.784250020980835\n",
      "Iteration 1400 Training loss 0.08236724883317947 Validation loss 0.0784090980887413 Accuracy 0.7832500338554382\n",
      "Iteration 1410 Training loss 0.07567863911390305 Validation loss 0.07834114134311676 Accuracy 0.783625066280365\n",
      "Iteration 1420 Training loss 0.07866965979337692 Validation loss 0.07826743274927139 Accuracy 0.783625066280365\n",
      "Iteration 1430 Training loss 0.07591080665588379 Validation loss 0.0781686007976532 Accuracy 0.784250020980835\n",
      "Iteration 1440 Training loss 0.07879028469324112 Validation loss 0.07806336134672165 Accuracy 0.7860000133514404\n",
      "Iteration 1450 Training loss 0.08202651143074036 Validation loss 0.07808768004179001 Accuracy 0.784000039100647\n",
      "Iteration 1460 Training loss 0.08068237453699112 Validation loss 0.07797626405954361 Accuracy 0.7853750586509705\n",
      "Iteration 1470 Training loss 0.080562524497509 Validation loss 0.07784796506166458 Accuracy 0.7863750457763672\n",
      "Iteration 1480 Training loss 0.07605735957622528 Validation loss 0.0777961015701294 Accuracy 0.7853750586509705\n",
      "Iteration 1490 Training loss 0.07448433339595795 Validation loss 0.07785169780254364 Accuracy 0.784375011920929\n",
      "Iteration 1500 Training loss 0.07990968227386475 Validation loss 0.07765179127454758 Accuracy 0.7855000495910645\n",
      "Iteration 1510 Training loss 0.0838869959115982 Validation loss 0.07757455855607986 Accuracy 0.7866250276565552\n",
      "Iteration 1520 Training loss 0.07133245468139648 Validation loss 0.07752121984958649 Accuracy 0.7860000133514404\n",
      "Iteration 1530 Training loss 0.07873836159706116 Validation loss 0.07742434740066528 Accuracy 0.7871250510215759\n",
      "Iteration 1540 Training loss 0.08019859343767166 Validation loss 0.07738204300403595 Accuracy 0.7861250638961792\n",
      "Iteration 1550 Training loss 0.07601220160722733 Validation loss 0.07729317992925644 Accuracy 0.7870000600814819\n",
      "Iteration 1560 Training loss 0.08470995724201202 Validation loss 0.07731877267360687 Accuracy 0.7871250510215759\n",
      "Iteration 1570 Training loss 0.08084079623222351 Validation loss 0.07716485112905502 Accuracy 0.7878750562667847\n",
      "Iteration 1580 Training loss 0.06936134397983551 Validation loss 0.07710935920476913 Accuracy 0.7873750329017639\n",
      "Iteration 1590 Training loss 0.07760916650295258 Validation loss 0.07705733925104141 Accuracy 0.7875000238418579\n",
      "Iteration 1600 Training loss 0.08148814737796783 Validation loss 0.07698780298233032 Accuracy 0.7875000238418579\n",
      "Iteration 1610 Training loss 0.07906291633844376 Validation loss 0.07693081349134445 Accuracy 0.7880000472068787\n",
      "Iteration 1620 Training loss 0.07367800176143646 Validation loss 0.07686685770750046 Accuracy 0.7876250147819519\n",
      "Iteration 1630 Training loss 0.07657156139612198 Validation loss 0.07684598118066788 Accuracy 0.7883750200271606\n",
      "Iteration 1640 Training loss 0.0811157375574112 Validation loss 0.07676874846220016 Accuracy 0.7881250381469727\n",
      "Iteration 1650 Training loss 0.07620014995336533 Validation loss 0.07669717073440552 Accuracy 0.7880000472068787\n",
      "Iteration 1660 Training loss 0.07851108908653259 Validation loss 0.07666098326444626 Accuracy 0.7891250252723694\n",
      "Iteration 1670 Training loss 0.07520639151334763 Validation loss 0.0765959694981575 Accuracy 0.7901250123977661\n",
      "Iteration 1680 Training loss 0.07305534183979034 Validation loss 0.07652217894792557 Accuracy 0.7891250252723694\n",
      "Iteration 1690 Training loss 0.07734179496765137 Validation loss 0.07653726637363434 Accuracy 0.7887500524520874\n",
      "Iteration 1700 Training loss 0.07776688039302826 Validation loss 0.07647203654050827 Accuracy 0.7892500162124634\n",
      "Iteration 1710 Training loss 0.07652387768030167 Validation loss 0.0764005184173584 Accuracy 0.7897500395774841\n",
      "Iteration 1720 Training loss 0.08166800439357758 Validation loss 0.07633833587169647 Accuracy 0.7891250252723694\n",
      "Iteration 1730 Training loss 0.06782712042331696 Validation loss 0.07631752640008926 Accuracy 0.7900000214576721\n",
      "Iteration 1740 Training loss 0.07304937392473221 Validation loss 0.0762556940317154 Accuracy 0.7902500629425049\n",
      "Iteration 1750 Training loss 0.07613351941108704 Validation loss 0.07616215199232101 Accuracy 0.7906250357627869\n",
      "Iteration 1760 Training loss 0.07892165333032608 Validation loss 0.0761197954416275 Accuracy 0.7905000448226929\n",
      "Iteration 1770 Training loss 0.07501106709241867 Validation loss 0.07606765627861023 Accuracy 0.7912500500679016\n",
      "Iteration 1780 Training loss 0.06460000574588776 Validation loss 0.07601669430732727 Accuracy 0.7911250591278076\n",
      "Iteration 1790 Training loss 0.07084934413433075 Validation loss 0.07605762034654617 Accuracy 0.7915000319480896\n",
      "Iteration 1800 Training loss 0.07434269040822983 Validation loss 0.0759955644607544 Accuracy 0.7915000319480896\n",
      "Iteration 1810 Training loss 0.07568871974945068 Validation loss 0.07592558860778809 Accuracy 0.7916250228881836\n",
      "Iteration 1820 Training loss 0.07313165813684464 Validation loss 0.07583063095808029 Accuracy 0.7912500500679016\n",
      "Iteration 1830 Training loss 0.07097706943750381 Validation loss 0.07577585428953171 Accuracy 0.7908750176429749\n",
      "Iteration 1840 Training loss 0.07496620714664459 Validation loss 0.07577577978372574 Accuracy 0.7916250228881836\n",
      "Iteration 1850 Training loss 0.07472603023052216 Validation loss 0.07571398466825485 Accuracy 0.7913750410079956\n",
      "Iteration 1860 Training loss 0.08507903665304184 Validation loss 0.07570895552635193 Accuracy 0.7927500605583191\n",
      "Iteration 1870 Training loss 0.0747726783156395 Validation loss 0.07560334354639053 Accuracy 0.7920000553131104\n",
      "Iteration 1880 Training loss 0.06905543804168701 Validation loss 0.0755612850189209 Accuracy 0.7917500138282776\n",
      "Iteration 1890 Training loss 0.07380829751491547 Validation loss 0.07550891488790512 Accuracy 0.7928750514984131\n",
      "Iteration 1900 Training loss 0.07556455582380295 Validation loss 0.07547669112682343 Accuracy 0.7925000190734863\n",
      "Iteration 1910 Training loss 0.07837358862161636 Validation loss 0.07547091692686081 Accuracy 0.7918750643730164\n",
      "Iteration 1920 Training loss 0.06604398787021637 Validation loss 0.07540234923362732 Accuracy 0.7930000424385071\n",
      "Iteration 1930 Training loss 0.0748293474316597 Validation loss 0.0753852128982544 Accuracy 0.7917500138282776\n",
      "Iteration 1940 Training loss 0.08128052204847336 Validation loss 0.07533309608697891 Accuracy 0.7927500605583191\n",
      "Iteration 1950 Training loss 0.07277613133192062 Validation loss 0.07529819011688232 Accuracy 0.7923750281333923\n",
      "Iteration 1960 Training loss 0.0684836208820343 Validation loss 0.07537209987640381 Accuracy 0.7928750514984131\n",
      "Iteration 1970 Training loss 0.07772310078144073 Validation loss 0.07524769753217697 Accuracy 0.7936250567436218\n",
      "Iteration 1980 Training loss 0.07132722437381744 Validation loss 0.07517683506011963 Accuracy 0.7938750386238098\n",
      "Iteration 1990 Training loss 0.07373211532831192 Validation loss 0.07515423744916916 Accuracy 0.7937500476837158\n",
      "Iteration 2000 Training loss 0.06784453988075256 Validation loss 0.07510066777467728 Accuracy 0.7930000424385071\n",
      "Iteration 2010 Training loss 0.08176133036613464 Validation loss 0.07506067305803299 Accuracy 0.7933750152587891\n",
      "Iteration 2020 Training loss 0.07753005623817444 Validation loss 0.07508715987205505 Accuracy 0.7932500243186951\n",
      "Iteration 2030 Training loss 0.07543663680553436 Validation loss 0.07499469071626663 Accuracy 0.7932500243186951\n",
      "Iteration 2040 Training loss 0.0752001702785492 Validation loss 0.07495597004890442 Accuracy 0.7938750386238098\n",
      "Iteration 2050 Training loss 0.0747697651386261 Validation loss 0.07492338120937347 Accuracy 0.7941250205039978\n",
      "Iteration 2060 Training loss 0.0718640387058258 Validation loss 0.07493016868829727 Accuracy 0.7940000295639038\n",
      "Iteration 2070 Training loss 0.076322041451931 Validation loss 0.0748581662774086 Accuracy 0.7937500476837158\n",
      "Iteration 2080 Training loss 0.07322431355714798 Validation loss 0.07482489943504333 Accuracy 0.7952500581741333\n",
      "Iteration 2090 Training loss 0.07038703560829163 Validation loss 0.07478520274162292 Accuracy 0.7945000529289246\n",
      "Iteration 2100 Training loss 0.07230309396982193 Validation loss 0.07480881363153458 Accuracy 0.7947500348091125\n",
      "Iteration 2110 Training loss 0.07040201872587204 Validation loss 0.07473766058683395 Accuracy 0.7946250438690186\n",
      "Iteration 2120 Training loss 0.07287418097257614 Validation loss 0.07471123337745667 Accuracy 0.7947500348091125\n",
      "Iteration 2130 Training loss 0.07190269231796265 Validation loss 0.0746530145406723 Accuracy 0.7951250672340393\n",
      "Iteration 2140 Training loss 0.06641519069671631 Validation loss 0.0747576430439949 Accuracy 0.7948750257492065\n",
      "Iteration 2150 Training loss 0.07893022149801254 Validation loss 0.07469239830970764 Accuracy 0.7948750257492065\n",
      "Iteration 2160 Training loss 0.0824098214507103 Validation loss 0.07456795126199722 Accuracy 0.7946250438690186\n",
      "Iteration 2170 Training loss 0.07512049376964569 Validation loss 0.0745367631316185 Accuracy 0.7951250672340393\n",
      "Iteration 2180 Training loss 0.07608155161142349 Validation loss 0.07454229891300201 Accuracy 0.7941250205039978\n",
      "Iteration 2190 Training loss 0.07665325701236725 Validation loss 0.07448773086071014 Accuracy 0.7945000529289246\n",
      "Iteration 2200 Training loss 0.08410421013832092 Validation loss 0.07443573325872421 Accuracy 0.7956250309944153\n",
      "Iteration 2210 Training loss 0.07432086765766144 Validation loss 0.07439981400966644 Accuracy 0.7958750128746033\n",
      "Iteration 2220 Training loss 0.07341577112674713 Validation loss 0.07440076768398285 Accuracy 0.7951250672340393\n",
      "Iteration 2230 Training loss 0.07147843390703201 Validation loss 0.07439509779214859 Accuracy 0.7955000400543213\n",
      "Iteration 2240 Training loss 0.07534076273441315 Validation loss 0.07439205795526505 Accuracy 0.7950000166893005\n",
      "Iteration 2250 Training loss 0.07949277758598328 Validation loss 0.07429081946611404 Accuracy 0.796000063419342\n",
      "Iteration 2260 Training loss 0.07523565739393234 Validation loss 0.07424736022949219 Accuracy 0.7956250309944153\n",
      "Iteration 2270 Training loss 0.07175001502037048 Validation loss 0.07422690093517303 Accuracy 0.7958750128746033\n",
      "Iteration 2280 Training loss 0.07212706655263901 Validation loss 0.07421012967824936 Accuracy 0.7957500219345093\n",
      "Iteration 2290 Training loss 0.06996498256921768 Validation loss 0.07420067489147186 Accuracy 0.7946250438690186\n",
      "Iteration 2300 Training loss 0.07471217960119247 Validation loss 0.07413876801729202 Accuracy 0.796375036239624\n",
      "Iteration 2310 Training loss 0.06969838589429855 Validation loss 0.0740988478064537 Accuracy 0.79625004529953\n",
      "Iteration 2320 Training loss 0.08004437386989594 Validation loss 0.07411834597587585 Accuracy 0.7957500219345093\n",
      "Iteration 2330 Training loss 0.08049610257148743 Validation loss 0.07405346632003784 Accuracy 0.796000063419342\n",
      "Iteration 2340 Training loss 0.07445331662893295 Validation loss 0.0741269588470459 Accuracy 0.7951250672340393\n",
      "Iteration 2350 Training loss 0.08056192100048065 Validation loss 0.07400095462799072 Accuracy 0.796500027179718\n",
      "Iteration 2360 Training loss 0.07664041221141815 Validation loss 0.07396408170461655 Accuracy 0.796375036239624\n",
      "Iteration 2370 Training loss 0.06798074394464493 Validation loss 0.0739496573805809 Accuracy 0.796625018119812\n",
      "Iteration 2380 Training loss 0.08278919756412506 Validation loss 0.07390126585960388 Accuracy 0.7970000505447388\n",
      "Iteration 2390 Training loss 0.07392948865890503 Validation loss 0.07387613505125046 Accuracy 0.7970000505447388\n",
      "Iteration 2400 Training loss 0.0665614902973175 Validation loss 0.07388699799776077 Accuracy 0.79625004529953\n",
      "Iteration 2410 Training loss 0.0744505226612091 Validation loss 0.07393117249011993 Accuracy 0.7951250672340393\n",
      "Iteration 2420 Training loss 0.07603314518928528 Validation loss 0.07379617542028427 Accuracy 0.7972500324249268\n",
      "Iteration 2430 Training loss 0.08418529480695724 Validation loss 0.07376988232135773 Accuracy 0.7971250414848328\n",
      "Iteration 2440 Training loss 0.06890098750591278 Validation loss 0.07374103367328644 Accuracy 0.7968750596046448\n",
      "Iteration 2450 Training loss 0.07786723226308823 Validation loss 0.07370993494987488 Accuracy 0.796625018119812\n",
      "Iteration 2460 Training loss 0.07351399213075638 Validation loss 0.0738418772816658 Accuracy 0.79625004529953\n",
      "Iteration 2470 Training loss 0.0671362355351448 Validation loss 0.07367382943630219 Accuracy 0.796750009059906\n",
      "Iteration 2480 Training loss 0.07665778696537018 Validation loss 0.07368507236242294 Accuracy 0.796500027179718\n",
      "Iteration 2490 Training loss 0.06935188174247742 Validation loss 0.07361213862895966 Accuracy 0.7972500324249268\n",
      "Iteration 2500 Training loss 0.07708201557397842 Validation loss 0.07358492165803909 Accuracy 0.7976250648498535\n",
      "Iteration 2510 Training loss 0.07594063878059387 Validation loss 0.07357074320316315 Accuracy 0.796500027179718\n",
      "Iteration 2520 Training loss 0.07247944921255112 Validation loss 0.07353894412517548 Accuracy 0.7983750104904175\n",
      "Iteration 2530 Training loss 0.07226338982582092 Validation loss 0.07351043820381165 Accuracy 0.7976250648498535\n",
      "Iteration 2540 Training loss 0.07405360043048859 Validation loss 0.07350537180900574 Accuracy 0.796750009059906\n",
      "Iteration 2550 Training loss 0.06517679244279861 Validation loss 0.07346460223197937 Accuracy 0.7977500557899475\n",
      "Iteration 2560 Training loss 0.07659529894590378 Validation loss 0.07344119250774384 Accuracy 0.7977500557899475\n",
      "Iteration 2570 Training loss 0.07652585953474045 Validation loss 0.07344365119934082 Accuracy 0.7980000376701355\n",
      "Iteration 2580 Training loss 0.07323537021875381 Validation loss 0.07340479642152786 Accuracy 0.7975000143051147\n",
      "Iteration 2590 Training loss 0.07621943950653076 Validation loss 0.07342185080051422 Accuracy 0.7968750596046448\n",
      "Iteration 2600 Training loss 0.06671798974275589 Validation loss 0.07333491742610931 Accuracy 0.7968750596046448\n",
      "Iteration 2610 Training loss 0.08007659018039703 Validation loss 0.07335580885410309 Accuracy 0.7976250648498535\n",
      "Iteration 2620 Training loss 0.0667579248547554 Validation loss 0.07329860329627991 Accuracy 0.7977500557899475\n",
      "Iteration 2630 Training loss 0.07311851531267166 Validation loss 0.07326474785804749 Accuracy 0.7982500195503235\n",
      "Iteration 2640 Training loss 0.073794424533844 Validation loss 0.0732751339673996 Accuracy 0.7973750233650208\n",
      "Iteration 2650 Training loss 0.08104858547449112 Validation loss 0.07328470051288605 Accuracy 0.7976250648498535\n",
      "Iteration 2660 Training loss 0.06798277050256729 Validation loss 0.07321249693632126 Accuracy 0.7980000376701355\n",
      "Iteration 2670 Training loss 0.07401538640260696 Validation loss 0.07318928837776184 Accuracy 0.7978750467300415\n",
      "Iteration 2680 Training loss 0.06321489065885544 Validation loss 0.07317650318145752 Accuracy 0.7978750467300415\n",
      "Iteration 2690 Training loss 0.0681290552020073 Validation loss 0.07315851002931595 Accuracy 0.7986250519752502\n",
      "Iteration 2700 Training loss 0.07777782529592514 Validation loss 0.07314177602529526 Accuracy 0.7987500429153442\n",
      "Iteration 2710 Training loss 0.0777454823255539 Validation loss 0.07310072332620621 Accuracy 0.799250066280365\n",
      "Iteration 2720 Training loss 0.07757590711116791 Validation loss 0.07307213544845581 Accuracy 0.799375057220459\n",
      "Iteration 2730 Training loss 0.06805232167243958 Validation loss 0.07302254438400269 Accuracy 0.7981250286102295\n",
      "Iteration 2740 Training loss 0.06987344473600388 Validation loss 0.07314115017652512 Accuracy 0.7977500557899475\n",
      "Iteration 2750 Training loss 0.07721387594938278 Validation loss 0.07318083196878433 Accuracy 0.796750009059906\n",
      "Iteration 2760 Training loss 0.0830935463309288 Validation loss 0.07295485585927963 Accuracy 0.7981250286102295\n",
      "Iteration 2770 Training loss 0.07425507158041 Validation loss 0.07293600589036942 Accuracy 0.7986250519752502\n",
      "Iteration 2780 Training loss 0.07418768107891083 Validation loss 0.0729382336139679 Accuracy 0.7987500429153442\n",
      "Iteration 2790 Training loss 0.08010537922382355 Validation loss 0.07295316457748413 Accuracy 0.7991250157356262\n",
      "Iteration 2800 Training loss 0.06773335486650467 Validation loss 0.07287026196718216 Accuracy 0.7980000376701355\n",
      "Iteration 2810 Training loss 0.07761643081903458 Validation loss 0.07284707576036453 Accuracy 0.7985000610351562\n",
      "Iteration 2820 Training loss 0.0810185968875885 Validation loss 0.0728253573179245 Accuracy 0.7982500195503235\n",
      "Iteration 2830 Training loss 0.07200945913791656 Validation loss 0.07294786721467972 Accuracy 0.7983750104904175\n",
      "Iteration 2840 Training loss 0.07330203056335449 Validation loss 0.07284219563007355 Accuracy 0.7987500429153442\n",
      "Iteration 2850 Training loss 0.07288617640733719 Validation loss 0.07290605455636978 Accuracy 0.7983750104904175\n",
      "Iteration 2860 Training loss 0.07183211296796799 Validation loss 0.07273608446121216 Accuracy 0.7988750338554382\n",
      "Iteration 2870 Training loss 0.07036435604095459 Validation loss 0.07275611907243729 Accuracy 0.7983750104904175\n",
      "Iteration 2880 Training loss 0.0693356916308403 Validation loss 0.07275790721178055 Accuracy 0.799375057220459\n",
      "Iteration 2890 Training loss 0.06160421296954155 Validation loss 0.07277558743953705 Accuracy 0.7982500195503235\n",
      "Iteration 2900 Training loss 0.07903008908033371 Validation loss 0.07265381515026093 Accuracy 0.7988750338554382\n",
      "Iteration 2910 Training loss 0.07936158776283264 Validation loss 0.07266044616699219 Accuracy 0.7990000247955322\n",
      "Iteration 2920 Training loss 0.07377505302429199 Validation loss 0.07260335236787796 Accuracy 0.799250066280365\n",
      "Iteration 2930 Training loss 0.07184424251317978 Validation loss 0.0725816935300827 Accuracy 0.799750030040741\n",
      "Iteration 2940 Training loss 0.06748507171869278 Validation loss 0.07257507741451263 Accuracy 0.799625039100647\n",
      "Iteration 2950 Training loss 0.07010187208652496 Validation loss 0.07254014909267426 Accuracy 0.799750030040741\n",
      "Iteration 2960 Training loss 0.0748240277171135 Validation loss 0.07260973751544952 Accuracy 0.7988750338554382\n",
      "Iteration 2970 Training loss 0.08080741763114929 Validation loss 0.07251520454883575 Accuracy 0.799500048160553\n",
      "Iteration 2980 Training loss 0.06905092298984528 Validation loss 0.07249002158641815 Accuracy 0.800000011920929\n",
      "Iteration 2990 Training loss 0.06816404312849045 Validation loss 0.07245989143848419 Accuracy 0.8008750677108765\n",
      "Iteration 3000 Training loss 0.06646289676427841 Validation loss 0.07271667569875717 Accuracy 0.7976250648498535\n",
      "Iteration 3010 Training loss 0.0635305643081665 Validation loss 0.07241691648960114 Accuracy 0.8003750443458557\n",
      "Iteration 3020 Training loss 0.07410811632871628 Validation loss 0.07239628583192825 Accuracy 0.8003750443458557\n",
      "Iteration 3030 Training loss 0.07020808011293411 Validation loss 0.07245334982872009 Accuracy 0.799625039100647\n",
      "Iteration 3040 Training loss 0.06367260217666626 Validation loss 0.07235564291477203 Accuracy 0.8010000586509705\n",
      "Iteration 3050 Training loss 0.06599414348602295 Validation loss 0.07234388589859009 Accuracy 0.799875020980835\n",
      "Iteration 3060 Training loss 0.07245224714279175 Validation loss 0.07241240888834 Accuracy 0.8002500534057617\n",
      "Iteration 3070 Training loss 0.06819896399974823 Validation loss 0.0723203495144844 Accuracy 0.799750030040741\n",
      "Iteration 3080 Training loss 0.07162871211767197 Validation loss 0.07227475196123123 Accuracy 0.8011250495910645\n",
      "Iteration 3090 Training loss 0.06790843605995178 Validation loss 0.07231926172971725 Accuracy 0.799875020980835\n",
      "Iteration 3100 Training loss 0.07166637480258942 Validation loss 0.07223203033208847 Accuracy 0.8010000586509705\n",
      "Iteration 3110 Training loss 0.08292218297719955 Validation loss 0.0724823847413063 Accuracy 0.7977500557899475\n",
      "Iteration 3120 Training loss 0.06740809231996536 Validation loss 0.07224304974079132 Accuracy 0.8002500534057617\n",
      "Iteration 3130 Training loss 0.07655785232782364 Validation loss 0.07217229157686234 Accuracy 0.8012500405311584\n",
      "Iteration 3140 Training loss 0.06691630184650421 Validation loss 0.07216601818799973 Accuracy 0.8003750443458557\n",
      "Iteration 3150 Training loss 0.06835442036390305 Validation loss 0.07213626801967621 Accuracy 0.8015000224113464\n",
      "Iteration 3160 Training loss 0.06792858242988586 Validation loss 0.0721987932920456 Accuracy 0.8007500171661377\n",
      "Iteration 3170 Training loss 0.06714006513357162 Validation loss 0.07218316942453384 Accuracy 0.8010000586509705\n",
      "Iteration 3180 Training loss 0.07053600251674652 Validation loss 0.07214803248643875 Accuracy 0.8012500405311584\n",
      "Iteration 3190 Training loss 0.06826357543468475 Validation loss 0.0722007304430008 Accuracy 0.8005000352859497\n",
      "Iteration 3200 Training loss 0.06902074068784714 Validation loss 0.07203204929828644 Accuracy 0.8008750677108765\n",
      "Iteration 3210 Training loss 0.0675649493932724 Validation loss 0.07201743870973587 Accuracy 0.8007500171661377\n",
      "Iteration 3220 Training loss 0.06949525326490402 Validation loss 0.07212666422128677 Accuracy 0.8006250262260437\n",
      "Iteration 3230 Training loss 0.07956857979297638 Validation loss 0.0719914585351944 Accuracy 0.8001250624656677\n",
      "Iteration 3240 Training loss 0.06908312439918518 Validation loss 0.07219959795475006 Accuracy 0.7988750338554382\n",
      "Iteration 3250 Training loss 0.06450214982032776 Validation loss 0.07201758027076721 Accuracy 0.8006250262260437\n",
      "Iteration 3260 Training loss 0.07913318276405334 Validation loss 0.07198917865753174 Accuracy 0.8008750677108765\n",
      "Iteration 3270 Training loss 0.07065603882074356 Validation loss 0.07190076261758804 Accuracy 0.8015000224113464\n",
      "Iteration 3280 Training loss 0.07214716821908951 Validation loss 0.0719069316983223 Accuracy 0.8010000586509705\n",
      "Iteration 3290 Training loss 0.07585066556930542 Validation loss 0.07187128067016602 Accuracy 0.8006250262260437\n",
      "Iteration 3300 Training loss 0.07491893321275711 Validation loss 0.07187323272228241 Accuracy 0.8010000586509705\n",
      "Iteration 3310 Training loss 0.07561793923377991 Validation loss 0.07182042300701141 Accuracy 0.8016250133514404\n",
      "Iteration 3320 Training loss 0.06969986110925674 Validation loss 0.07182435691356659 Accuracy 0.8007500171661377\n",
      "Iteration 3330 Training loss 0.06826253980398178 Validation loss 0.07178961485624313 Accuracy 0.8011250495910645\n",
      "Iteration 3340 Training loss 0.06758937984704971 Validation loss 0.0717688500881195 Accuracy 0.8016250133514404\n",
      "Iteration 3350 Training loss 0.07804305106401443 Validation loss 0.07175830751657486 Accuracy 0.8013750314712524\n",
      "Iteration 3360 Training loss 0.07600688934326172 Validation loss 0.07172349840402603 Accuracy 0.8023750185966492\n",
      "Iteration 3370 Training loss 0.07542187720537186 Validation loss 0.07201462239027023 Accuracy 0.799875020980835\n",
      "Iteration 3380 Training loss 0.06393507122993469 Validation loss 0.07175282388925552 Accuracy 0.8013750314712524\n",
      "Iteration 3390 Training loss 0.06652601808309555 Validation loss 0.07177285850048065 Accuracy 0.8010000586509705\n",
      "Iteration 3400 Training loss 0.06993720680475235 Validation loss 0.07177326083183289 Accuracy 0.8012500405311584\n",
      "Iteration 3410 Training loss 0.0810505673289299 Validation loss 0.0716598704457283 Accuracy 0.8022500276565552\n",
      "Iteration 3420 Training loss 0.06376972794532776 Validation loss 0.07163985073566437 Accuracy 0.8025000095367432\n",
      "Iteration 3430 Training loss 0.0766749233007431 Validation loss 0.07162924110889435 Accuracy 0.8017500638961792\n",
      "Iteration 3440 Training loss 0.0735900029540062 Validation loss 0.07157022505998611 Accuracy 0.8020000457763672\n",
      "Iteration 3450 Training loss 0.07012838870286942 Validation loss 0.07155757397413254 Accuracy 0.8015000224113464\n",
      "Iteration 3460 Training loss 0.07474269717931747 Validation loss 0.0715525671839714 Accuracy 0.8021250367164612\n",
      "Iteration 3470 Training loss 0.06787716597318649 Validation loss 0.07151184976100922 Accuracy 0.8026250600814819\n",
      "Iteration 3480 Training loss 0.07139566540718079 Validation loss 0.07154211401939392 Accuracy 0.8016250133514404\n",
      "Iteration 3490 Training loss 0.07222888618707657 Validation loss 0.07154755294322968 Accuracy 0.8017500638961792\n",
      "Iteration 3500 Training loss 0.06294973939657211 Validation loss 0.07144627720117569 Accuracy 0.8025000095367432\n",
      "Iteration 3510 Training loss 0.0728740319609642 Validation loss 0.07142693549394608 Accuracy 0.8025000095367432\n",
      "Iteration 3520 Training loss 0.07374553382396698 Validation loss 0.07141807675361633 Accuracy 0.8030000329017639\n",
      "Iteration 3530 Training loss 0.05924046412110329 Validation loss 0.07140010595321655 Accuracy 0.8022500276565552\n",
      "Iteration 3540 Training loss 0.07696978747844696 Validation loss 0.07137741893529892 Accuracy 0.8030000329017639\n",
      "Iteration 3550 Training loss 0.0710626170039177 Validation loss 0.07136940211057663 Accuracy 0.8022500276565552\n",
      "Iteration 3560 Training loss 0.08694321662187576 Validation loss 0.07133767753839493 Accuracy 0.8031250238418579\n",
      "Iteration 3570 Training loss 0.06885123252868652 Validation loss 0.07133247703313828 Accuracy 0.8022500276565552\n",
      "Iteration 3580 Training loss 0.06793428957462311 Validation loss 0.07130545377731323 Accuracy 0.8028750419616699\n",
      "Iteration 3590 Training loss 0.06855963915586472 Validation loss 0.07135222852230072 Accuracy 0.8018750548362732\n",
      "Iteration 3600 Training loss 0.07102790474891663 Validation loss 0.07130034267902374 Accuracy 0.8028750419616699\n",
      "Iteration 3610 Training loss 0.07044090330600739 Validation loss 0.0712505504488945 Accuracy 0.8035000562667847\n",
      "Iteration 3620 Training loss 0.07808980345726013 Validation loss 0.07124673575162888 Accuracy 0.8038750290870667\n",
      "Iteration 3630 Training loss 0.07485304772853851 Validation loss 0.0712398812174797 Accuracy 0.8036250472068787\n",
      "Iteration 3640 Training loss 0.07483182102441788 Validation loss 0.07119931280612946 Accuracy 0.8036250472068787\n",
      "Iteration 3650 Training loss 0.06283745914697647 Validation loss 0.07119546085596085 Accuracy 0.8040000200271606\n",
      "Iteration 3660 Training loss 0.08067134767770767 Validation loss 0.07117345184087753 Accuracy 0.8037500381469727\n",
      "Iteration 3670 Training loss 0.07517638802528381 Validation loss 0.0711456760764122 Accuracy 0.8031250238418579\n",
      "Iteration 3680 Training loss 0.07413721829652786 Validation loss 0.07128606736660004 Accuracy 0.8022500276565552\n",
      "Iteration 3690 Training loss 0.06837095320224762 Validation loss 0.07110269367694855 Accuracy 0.8033750653266907\n",
      "Iteration 3700 Training loss 0.07507756352424622 Validation loss 0.07110258936882019 Accuracy 0.8026250600814819\n",
      "Iteration 3710 Training loss 0.07259897887706757 Validation loss 0.07118728756904602 Accuracy 0.8042500615119934\n",
      "Iteration 3720 Training loss 0.06690250337123871 Validation loss 0.07105691730976105 Accuracy 0.8035000562667847\n",
      "Iteration 3730 Training loss 0.06680051982402802 Validation loss 0.07114484161138535 Accuracy 0.8023750185966492\n",
      "Iteration 3740 Training loss 0.06831053644418716 Validation loss 0.07101564854383469 Accuracy 0.8035000562667847\n",
      "Iteration 3750 Training loss 0.0671226903796196 Validation loss 0.07102087885141373 Accuracy 0.8035000562667847\n",
      "Iteration 3760 Training loss 0.07300592213869095 Validation loss 0.07099299132823944 Accuracy 0.8040000200271606\n",
      "Iteration 3770 Training loss 0.07020771503448486 Validation loss 0.07105197012424469 Accuracy 0.8027500510215759\n",
      "Iteration 3780 Training loss 0.07413887232542038 Validation loss 0.070942722260952 Accuracy 0.8043750524520874\n",
      "Iteration 3790 Training loss 0.07013528048992157 Validation loss 0.07098214328289032 Accuracy 0.8032500147819519\n",
      "Iteration 3800 Training loss 0.07206670939922333 Validation loss 0.07091034203767776 Accuracy 0.8050000667572021\n",
      "Iteration 3810 Training loss 0.0753210112452507 Validation loss 0.07091527432203293 Accuracy 0.8037500381469727\n",
      "Iteration 3820 Training loss 0.07017415016889572 Validation loss 0.07091449201107025 Accuracy 0.8035000562667847\n",
      "Iteration 3830 Training loss 0.07375708967447281 Validation loss 0.07112288475036621 Accuracy 0.8020000457763672\n",
      "Iteration 3840 Training loss 0.06951861083507538 Validation loss 0.07082030177116394 Accuracy 0.8047500252723694\n",
      "Iteration 3850 Training loss 0.07362071424722672 Validation loss 0.07081124186515808 Accuracy 0.8042500615119934\n",
      "Iteration 3860 Training loss 0.07044078409671783 Validation loss 0.07080009579658508 Accuracy 0.8036250472068787\n",
      "Iteration 3870 Training loss 0.0711682140827179 Validation loss 0.07076672464609146 Accuracy 0.8043750524520874\n",
      "Iteration 3880 Training loss 0.07046423852443695 Validation loss 0.07074493914842606 Accuracy 0.8043750524520874\n",
      "Iteration 3890 Training loss 0.07432675361633301 Validation loss 0.07072463631629944 Accuracy 0.8045000433921814\n",
      "Iteration 3900 Training loss 0.07795611023902893 Validation loss 0.07070963829755783 Accuracy 0.8050000667572021\n",
      "Iteration 3910 Training loss 0.06380810588598251 Validation loss 0.07080995291471481 Accuracy 0.8032500147819519\n",
      "Iteration 3920 Training loss 0.06873860210180283 Validation loss 0.07070338726043701 Accuracy 0.8035000562667847\n",
      "Iteration 3930 Training loss 0.07232432812452316 Validation loss 0.07068213075399399 Accuracy 0.8040000200271606\n",
      "Iteration 3940 Training loss 0.06635141372680664 Validation loss 0.07063914090394974 Accuracy 0.8050000667572021\n",
      "Iteration 3950 Training loss 0.07281038165092468 Validation loss 0.07071256637573242 Accuracy 0.8037500381469727\n",
      "Iteration 3960 Training loss 0.06840934604406357 Validation loss 0.07077590376138687 Accuracy 0.8040000200271606\n",
      "Iteration 3970 Training loss 0.07174690067768097 Validation loss 0.07063945382833481 Accuracy 0.8041250109672546\n",
      "Iteration 3980 Training loss 0.06857209652662277 Validation loss 0.07058035582304001 Accuracy 0.8048750162124634\n",
      "Iteration 3990 Training loss 0.07111220806837082 Validation loss 0.07054620236158371 Accuracy 0.8055000305175781\n",
      "Iteration 4000 Training loss 0.0728607252240181 Validation loss 0.07055456191301346 Accuracy 0.8048750162124634\n",
      "Iteration 4010 Training loss 0.07305591553449631 Validation loss 0.07050725817680359 Accuracy 0.8048750162124634\n",
      "Iteration 4020 Training loss 0.07473781704902649 Validation loss 0.07052989304065704 Accuracy 0.8045000433921814\n",
      "Iteration 4030 Training loss 0.07077868282794952 Validation loss 0.07055310159921646 Accuracy 0.8036250472068787\n",
      "Iteration 4040 Training loss 0.06346543878316879 Validation loss 0.0704461857676506 Accuracy 0.8050000667572021\n",
      "Iteration 4050 Training loss 0.07824940979480743 Validation loss 0.07042615115642548 Accuracy 0.8052500486373901\n",
      "Iteration 4060 Training loss 0.07174387574195862 Validation loss 0.07045600563287735 Accuracy 0.8048750162124634\n",
      "Iteration 4070 Training loss 0.06589169055223465 Validation loss 0.07041531056165695 Accuracy 0.8052500486373901\n",
      "Iteration 4080 Training loss 0.06842599809169769 Validation loss 0.07041043788194656 Accuracy 0.8050000667572021\n",
      "Iteration 4090 Training loss 0.07280638813972473 Validation loss 0.0704447403550148 Accuracy 0.8040000200271606\n",
      "Iteration 4100 Training loss 0.06912905722856522 Validation loss 0.07042339444160461 Accuracy 0.8037500381469727\n",
      "Iteration 4110 Training loss 0.0735563263297081 Validation loss 0.07039929181337357 Accuracy 0.8041250109672546\n",
      "Iteration 4120 Training loss 0.06082690879702568 Validation loss 0.07035132497549057 Accuracy 0.8045000433921814\n",
      "Iteration 4130 Training loss 0.07423931360244751 Validation loss 0.07034362852573395 Accuracy 0.8042500615119934\n",
      "Iteration 4140 Training loss 0.06079094856977463 Validation loss 0.07030320912599564 Accuracy 0.8050000667572021\n",
      "Iteration 4150 Training loss 0.06263775378465652 Validation loss 0.07023900747299194 Accuracy 0.8058750629425049\n",
      "Iteration 4160 Training loss 0.0676441639661789 Validation loss 0.07021704316139221 Accuracy 0.8058750629425049\n",
      "Iteration 4170 Training loss 0.0668819472193718 Validation loss 0.07021971046924591 Accuracy 0.8050000667572021\n",
      "Iteration 4180 Training loss 0.07239525765180588 Validation loss 0.07022395730018616 Accuracy 0.8050000667572021\n",
      "Iteration 4190 Training loss 0.07188630849123001 Validation loss 0.07016925513744354 Accuracy 0.8052500486373901\n",
      "Iteration 4200 Training loss 0.06977832317352295 Validation loss 0.07015345990657806 Accuracy 0.8055000305175781\n",
      "Iteration 4210 Training loss 0.06527801603078842 Validation loss 0.07018397748470306 Accuracy 0.8051250576972961\n",
      "Iteration 4220 Training loss 0.0666932687163353 Validation loss 0.07013580203056335 Accuracy 0.8051250576972961\n",
      "Iteration 4230 Training loss 0.0750841349363327 Validation loss 0.07009028643369675 Accuracy 0.8058750629425049\n",
      "Iteration 4240 Training loss 0.0703980103135109 Validation loss 0.07011764496564865 Accuracy 0.8062500357627869\n",
      "Iteration 4250 Training loss 0.0717308446764946 Validation loss 0.07007016241550446 Accuracy 0.8067500591278076\n",
      "Iteration 4260 Training loss 0.06941141188144684 Validation loss 0.07003741711378098 Accuracy 0.8057500123977661\n",
      "Iteration 4270 Training loss 0.06596838682889938 Validation loss 0.07001042366027832 Accuracy 0.8063750267028809\n",
      "Iteration 4280 Training loss 0.07276669144630432 Validation loss 0.07002885639667511 Accuracy 0.8065000176429749\n",
      "Iteration 4290 Training loss 0.06939446926116943 Validation loss 0.07006236165761948 Accuracy 0.8047500252723694\n",
      "Iteration 4300 Training loss 0.0704224556684494 Validation loss 0.07006068527698517 Accuracy 0.8060000538825989\n",
      "Iteration 4310 Training loss 0.06813626736402512 Validation loss 0.06995537132024765 Accuracy 0.8057500123977661\n",
      "Iteration 4320 Training loss 0.07472784072160721 Validation loss 0.06992397457361221 Accuracy 0.8066250085830688\n",
      "Iteration 4330 Training loss 0.07230310142040253 Validation loss 0.06995682418346405 Accuracy 0.8067500591278076\n",
      "Iteration 4340 Training loss 0.0761503204703331 Validation loss 0.06993617862462997 Accuracy 0.8056250214576721\n",
      "Iteration 4350 Training loss 0.06747172772884369 Validation loss 0.06987523287534714 Accuracy 0.8066250085830688\n",
      "Iteration 4360 Training loss 0.0638599544763565 Validation loss 0.06993753463029861 Accuracy 0.8055000305175781\n",
      "Iteration 4370 Training loss 0.0675746500492096 Validation loss 0.06984753161668777 Accuracy 0.8075000643730164\n",
      "Iteration 4380 Training loss 0.07025221735239029 Validation loss 0.06984969228506088 Accuracy 0.8063750267028809\n",
      "Iteration 4390 Training loss 0.06253715604543686 Validation loss 0.06982719898223877 Accuracy 0.8066250085830688\n",
      "Iteration 4400 Training loss 0.06914077699184418 Validation loss 0.06994496285915375 Accuracy 0.8068750500679016\n",
      "Iteration 4410 Training loss 0.07156899571418762 Validation loss 0.0697813406586647 Accuracy 0.8062500357627869\n",
      "Iteration 4420 Training loss 0.0704060047864914 Validation loss 0.06978371739387512 Accuracy 0.8063750267028809\n",
      "Iteration 4430 Training loss 0.0720221996307373 Validation loss 0.06973081827163696 Accuracy 0.8067500591278076\n",
      "Iteration 4440 Training loss 0.06248336285352707 Validation loss 0.06977079063653946 Accuracy 0.8070000410079956\n",
      "Iteration 4450 Training loss 0.07056073844432831 Validation loss 0.06972838193178177 Accuracy 0.8068750500679016\n",
      "Iteration 4460 Training loss 0.06569996476173401 Validation loss 0.06967476010322571 Accuracy 0.8075000643730164\n",
      "Iteration 4470 Training loss 0.07035113126039505 Validation loss 0.06969550251960754 Accuracy 0.8067500591278076\n",
      "Iteration 4480 Training loss 0.0620996467769146 Validation loss 0.06963786482810974 Accuracy 0.8068750500679016\n",
      "Iteration 4490 Training loss 0.06380527466535568 Validation loss 0.06963192671537399 Accuracy 0.8058750629425049\n",
      "Iteration 4500 Training loss 0.06722178310155869 Validation loss 0.06960739195346832 Accuracy 0.8068750500679016\n",
      "Iteration 4510 Training loss 0.06817004829645157 Validation loss 0.06959381699562073 Accuracy 0.8065000176429749\n",
      "Iteration 4520 Training loss 0.06363127380609512 Validation loss 0.06958264857530594 Accuracy 0.8068750500679016\n",
      "Iteration 4530 Training loss 0.06276129931211472 Validation loss 0.06965992599725723 Accuracy 0.8063750267028809\n",
      "Iteration 4540 Training loss 0.06318415701389313 Validation loss 0.06960809975862503 Accuracy 0.8065000176429749\n",
      "Iteration 4550 Training loss 0.07300708442926407 Validation loss 0.06952162832021713 Accuracy 0.8085000514984131\n",
      "Iteration 4560 Training loss 0.07337844371795654 Validation loss 0.06955328583717346 Accuracy 0.8070000410079956\n",
      "Iteration 4570 Training loss 0.06913930922746658 Validation loss 0.0695687010884285 Accuracy 0.8068750500679016\n",
      "Iteration 4580 Training loss 0.06757329404354095 Validation loss 0.0696350634098053 Accuracy 0.8062500357627869\n",
      "Iteration 4590 Training loss 0.07166654616594315 Validation loss 0.06956791877746582 Accuracy 0.8066250085830688\n",
      "Iteration 4600 Training loss 0.0696079358458519 Validation loss 0.0695377066731453 Accuracy 0.8070000410079956\n",
      "Iteration 4610 Training loss 0.06423448771238327 Validation loss 0.0694504976272583 Accuracy 0.8075000643730164\n",
      "Iteration 4620 Training loss 0.06534391641616821 Validation loss 0.06952480971813202 Accuracy 0.8071250319480896\n",
      "Iteration 4630 Training loss 0.06607230007648468 Validation loss 0.06939452141523361 Accuracy 0.8091250658035278\n",
      "Iteration 4640 Training loss 0.0713273286819458 Validation loss 0.06946435570716858 Accuracy 0.8072500228881836\n",
      "Iteration 4650 Training loss 0.06579814106225967 Validation loss 0.06935615092515945 Accuracy 0.8095000386238098\n",
      "Iteration 4660 Training loss 0.07248109579086304 Validation loss 0.0693328008055687 Accuracy 0.8088750243186951\n",
      "Iteration 4670 Training loss 0.06966856867074966 Validation loss 0.06938852369785309 Accuracy 0.8080000281333923\n",
      "Iteration 4680 Training loss 0.06465445458889008 Validation loss 0.0693042203783989 Accuracy 0.8080000281333923\n",
      "Iteration 4690 Training loss 0.07390394061803818 Validation loss 0.06929074227809906 Accuracy 0.8078750371932983\n",
      "Iteration 4700 Training loss 0.06467465311288834 Validation loss 0.06930246204137802 Accuracy 0.8086250424385071\n",
      "Iteration 4710 Training loss 0.06227733567357063 Validation loss 0.06926851719617844 Accuracy 0.8080000281333923\n",
      "Iteration 4720 Training loss 0.07141076773405075 Validation loss 0.06925106793642044 Accuracy 0.8080000281333923\n",
      "Iteration 4730 Training loss 0.06927764415740967 Validation loss 0.06920473277568817 Accuracy 0.8095000386238098\n",
      "Iteration 4740 Training loss 0.06735017895698547 Validation loss 0.06926828622817993 Accuracy 0.8082500100135803\n",
      "Iteration 4750 Training loss 0.07463537901639938 Validation loss 0.06925973296165466 Accuracy 0.8082500100135803\n",
      "Iteration 4760 Training loss 0.07601236552000046 Validation loss 0.06924581527709961 Accuracy 0.8081250190734863\n",
      "Iteration 4770 Training loss 0.07097075134515762 Validation loss 0.06912713497877121 Accuracy 0.8090000152587891\n",
      "Iteration 4780 Training loss 0.0671858936548233 Validation loss 0.06911734491586685 Accuracy 0.8095000386238098\n",
      "Iteration 4790 Training loss 0.06821039319038391 Validation loss 0.06911627948284149 Accuracy 0.8091250658035278\n",
      "Iteration 4800 Training loss 0.07075590640306473 Validation loss 0.06914499402046204 Accuracy 0.8086250424385071\n",
      "Iteration 4810 Training loss 0.07128152996301651 Validation loss 0.06906753033399582 Accuracy 0.8090000152587891\n",
      "Iteration 4820 Training loss 0.06410770118236542 Validation loss 0.06905001401901245 Accuracy 0.8092500567436218\n",
      "Iteration 4830 Training loss 0.07062831521034241 Validation loss 0.06905572861433029 Accuracy 0.8090000152587891\n",
      "Iteration 4840 Training loss 0.06536655128002167 Validation loss 0.06912684440612793 Accuracy 0.8088750243186951\n",
      "Iteration 4850 Training loss 0.062318313866853714 Validation loss 0.06904605776071548 Accuracy 0.8093750476837158\n",
      "Iteration 4860 Training loss 0.07205020636320114 Validation loss 0.06897902488708496 Accuracy 0.8097500205039978\n",
      "Iteration 4870 Training loss 0.06202457845211029 Validation loss 0.06895472854375839 Accuracy 0.8101250529289246\n",
      "Iteration 4880 Training loss 0.07891467213630676 Validation loss 0.06892988830804825 Accuracy 0.8093750476837158\n",
      "Iteration 4890 Training loss 0.0771636813879013 Validation loss 0.06908141821622849 Accuracy 0.8090000152587891\n",
      "Iteration 4900 Training loss 0.07399594038724899 Validation loss 0.06890730559825897 Accuracy 0.8090000152587891\n",
      "Iteration 4910 Training loss 0.06922633945941925 Validation loss 0.06889785826206207 Accuracy 0.8103750348091125\n",
      "Iteration 4920 Training loss 0.07402683794498444 Validation loss 0.06893371790647507 Accuracy 0.8095000386238098\n",
      "Iteration 4930 Training loss 0.07560610771179199 Validation loss 0.06886932998895645 Accuracy 0.8098750114440918\n",
      "Iteration 4940 Training loss 0.07733318209648132 Validation loss 0.06882762908935547 Accuracy 0.8095000386238098\n",
      "Iteration 4950 Training loss 0.07058291137218475 Validation loss 0.06881909817457199 Accuracy 0.8103750348091125\n",
      "Iteration 4960 Training loss 0.0634264424443245 Validation loss 0.06879822164773941 Accuracy 0.8097500205039978\n",
      "Iteration 4970 Training loss 0.07671688497066498 Validation loss 0.06877990067005157 Accuracy 0.8102500438690186\n",
      "Iteration 4980 Training loss 0.06384564936161041 Validation loss 0.06886526197195053 Accuracy 0.8103750348091125\n",
      "Iteration 4990 Training loss 0.06347727030515671 Validation loss 0.06886731088161469 Accuracy 0.8096250295639038\n",
      "Iteration 5000 Training loss 0.06813990324735641 Validation loss 0.068762868642807 Accuracy 0.8100000619888306\n",
      "Iteration 5010 Training loss 0.0676223635673523 Validation loss 0.06872769445180893 Accuracy 0.8103750348091125\n",
      "Iteration 5020 Training loss 0.06656154245138168 Validation loss 0.06872157752513885 Accuracy 0.8105000257492065\n",
      "Iteration 5030 Training loss 0.06161394342780113 Validation loss 0.06868526339530945 Accuracy 0.8107500672340393\n",
      "Iteration 5040 Training loss 0.06993299722671509 Validation loss 0.06868334114551544 Accuracy 0.8095000386238098\n",
      "Iteration 5050 Training loss 0.06212587654590607 Validation loss 0.06865768134593964 Accuracy 0.8088750243186951\n",
      "Iteration 5060 Training loss 0.07530692219734192 Validation loss 0.06863418966531754 Accuracy 0.8096250295639038\n",
      "Iteration 5070 Training loss 0.06481023877859116 Validation loss 0.06871747225522995 Accuracy 0.8100000619888306\n",
      "Iteration 5080 Training loss 0.07173888385295868 Validation loss 0.06863075494766235 Accuracy 0.8105000257492065\n",
      "Iteration 5090 Training loss 0.06902382522821426 Validation loss 0.06858514994382858 Accuracy 0.8107500672340393\n",
      "Iteration 5100 Training loss 0.07141461968421936 Validation loss 0.06858143955469131 Accuracy 0.811625063419342\n",
      "Iteration 5110 Training loss 0.06766408681869507 Validation loss 0.0685431957244873 Accuracy 0.8102500438690186\n",
      "Iteration 5120 Training loss 0.07488507777452469 Validation loss 0.06853432953357697 Accuracy 0.8110000491142273\n",
      "Iteration 5130 Training loss 0.0709400400519371 Validation loss 0.06903249025344849 Accuracy 0.8071250319480896\n",
      "Iteration 5140 Training loss 0.06664574146270752 Validation loss 0.06852646172046661 Accuracy 0.8112500309944153\n",
      "Iteration 5150 Training loss 0.06909778714179993 Validation loss 0.06849633902311325 Accuracy 0.811625063419342\n",
      "Iteration 5160 Training loss 0.06328463554382324 Validation loss 0.0684666857123375 Accuracy 0.8098750114440918\n",
      "Iteration 5170 Training loss 0.06741790473461151 Validation loss 0.06845347583293915 Accuracy 0.8097500205039978\n",
      "Iteration 5180 Training loss 0.06865940988063812 Validation loss 0.06846454739570618 Accuracy 0.81187504529953\n",
      "Iteration 5190 Training loss 0.060323309153318405 Validation loss 0.06844642758369446 Accuracy 0.812000036239624\n",
      "Iteration 5200 Training loss 0.07353369891643524 Validation loss 0.06846814602613449 Accuracy 0.8115000128746033\n",
      "Iteration 5210 Training loss 0.07299325615167618 Validation loss 0.06846136599779129 Accuracy 0.8111250400543213\n",
      "Iteration 5220 Training loss 0.06084202602505684 Validation loss 0.06852369755506516 Accuracy 0.8096250295639038\n",
      "Iteration 5230 Training loss 0.05677320808172226 Validation loss 0.06836771219968796 Accuracy 0.8111250400543213\n",
      "Iteration 5240 Training loss 0.06538720428943634 Validation loss 0.06845082342624664 Accuracy 0.8105000257492065\n",
      "Iteration 5250 Training loss 0.0715891569852829 Validation loss 0.06835642457008362 Accuracy 0.8125000596046448\n",
      "Iteration 5260 Training loss 0.06391393393278122 Validation loss 0.06843792647123337 Accuracy 0.8101250529289246\n",
      "Iteration 5270 Training loss 0.06934122741222382 Validation loss 0.06835326552391052 Accuracy 0.8113750219345093\n",
      "Iteration 5280 Training loss 0.06957779824733734 Validation loss 0.0682913213968277 Accuracy 0.81187504529953\n",
      "Iteration 5290 Training loss 0.06957679986953735 Validation loss 0.06829030066728592 Accuracy 0.811750054359436\n",
      "Iteration 5300 Training loss 0.08151497691869736 Validation loss 0.06828390806913376 Accuracy 0.812250018119812\n",
      "Iteration 5310 Training loss 0.0741339847445488 Validation loss 0.06822627782821655 Accuracy 0.8103750348091125\n",
      "Iteration 5320 Training loss 0.07742121815681458 Validation loss 0.06820158660411835 Accuracy 0.8107500672340393\n",
      "Iteration 5330 Training loss 0.06332281231880188 Validation loss 0.06828337907791138 Accuracy 0.8108750581741333\n",
      "Iteration 5340 Training loss 0.0747729167342186 Validation loss 0.06817898899316788 Accuracy 0.8112500309944153\n",
      "Iteration 5350 Training loss 0.06549639254808426 Validation loss 0.0683104395866394 Accuracy 0.8101250529289246\n",
      "Iteration 5360 Training loss 0.07491707801818848 Validation loss 0.06815484911203384 Accuracy 0.8108750581741333\n",
      "Iteration 5370 Training loss 0.06354418396949768 Validation loss 0.06815026700496674 Accuracy 0.812125027179718\n",
      "Iteration 5380 Training loss 0.06997144222259521 Validation loss 0.06821473687887192 Accuracy 0.8107500672340393\n",
      "Iteration 5390 Training loss 0.0738506093621254 Validation loss 0.06809442490339279 Accuracy 0.8107500672340393\n",
      "Iteration 5400 Training loss 0.06715433299541473 Validation loss 0.06809848546981812 Accuracy 0.812125027179718\n",
      "Iteration 5410 Training loss 0.06910700350999832 Validation loss 0.0682380422949791 Accuracy 0.8102500438690186\n",
      "Iteration 5420 Training loss 0.0657014325261116 Validation loss 0.06803032755851746 Accuracy 0.8111250400543213\n",
      "Iteration 5430 Training loss 0.07112831622362137 Validation loss 0.06827376782894135 Accuracy 0.8101250529289246\n",
      "Iteration 5440 Training loss 0.06428921222686768 Validation loss 0.06803019344806671 Accuracy 0.812250018119812\n",
      "Iteration 5450 Training loss 0.0745135024189949 Validation loss 0.06805814057588577 Accuracy 0.812125027179718\n",
      "Iteration 5460 Training loss 0.06127610057592392 Validation loss 0.06812437623739243 Accuracy 0.8110000491142273\n",
      "Iteration 5470 Training loss 0.0655306726694107 Validation loss 0.06802229583263397 Accuracy 0.812125027179718\n",
      "Iteration 5480 Training loss 0.07158353924751282 Validation loss 0.06793513894081116 Accuracy 0.8110000491142273\n",
      "Iteration 5490 Training loss 0.06500132381916046 Validation loss 0.06804244965314865 Accuracy 0.8113750219345093\n",
      "Iteration 5500 Training loss 0.07164785265922546 Validation loss 0.06801297515630722 Accuracy 0.812125027179718\n",
      "Iteration 5510 Training loss 0.07379264384508133 Validation loss 0.06821222603321075 Accuracy 0.8106250166893005\n",
      "Iteration 5520 Training loss 0.062062423676252365 Validation loss 0.06792964786291122 Accuracy 0.812250018119812\n",
      "Iteration 5530 Training loss 0.058985065668821335 Validation loss 0.0678604319691658 Accuracy 0.8113750219345093\n",
      "Iteration 5540 Training loss 0.06891385465860367 Validation loss 0.06782405823469162 Accuracy 0.812000036239624\n",
      "Iteration 5550 Training loss 0.07414619624614716 Validation loss 0.0678061693906784 Accuracy 0.812000036239624\n",
      "Iteration 5560 Training loss 0.06799854338169098 Validation loss 0.0678112804889679 Accuracy 0.8115000128746033\n",
      "Iteration 5570 Training loss 0.07395044714212418 Validation loss 0.06778611242771149 Accuracy 0.811750054359436\n",
      "Iteration 5580 Training loss 0.05888330191373825 Validation loss 0.06778064370155334 Accuracy 0.811625063419342\n",
      "Iteration 5590 Training loss 0.06269029527902603 Validation loss 0.06776396930217743 Accuracy 0.8115000128746033\n",
      "Iteration 5600 Training loss 0.067670077085495 Validation loss 0.06782568246126175 Accuracy 0.8125000596046448\n",
      "Iteration 5610 Training loss 0.07756190747022629 Validation loss 0.06771358102560043 Accuracy 0.812250018119812\n",
      "Iteration 5620 Training loss 0.0718170553445816 Validation loss 0.06768383830785751 Accuracy 0.812000036239624\n",
      "Iteration 5630 Training loss 0.060091469436883926 Validation loss 0.06769368052482605 Accuracy 0.812375009059906\n",
      "Iteration 5640 Training loss 0.06318109482526779 Validation loss 0.06770037859678268 Accuracy 0.81187504529953\n",
      "Iteration 5650 Training loss 0.06420603394508362 Validation loss 0.06787123531103134 Accuracy 0.8112500309944153\n",
      "Iteration 5660 Training loss 0.07081302255392075 Validation loss 0.06762554496526718 Accuracy 0.812125027179718\n",
      "Iteration 5670 Training loss 0.07489240169525146 Validation loss 0.06760311871767044 Accuracy 0.8127500414848328\n",
      "Iteration 5680 Training loss 0.06801357120275497 Validation loss 0.06773500144481659 Accuracy 0.812250018119812\n",
      "Iteration 5690 Training loss 0.06921539455652237 Validation loss 0.06759871542453766 Accuracy 0.81187504529953\n",
      "Iteration 5700 Training loss 0.07083968073129654 Validation loss 0.06755435466766357 Accuracy 0.8130000233650208\n",
      "Iteration 5710 Training loss 0.061791323125362396 Validation loss 0.06763888895511627 Accuracy 0.8125000596046448\n",
      "Iteration 5720 Training loss 0.07171528041362762 Validation loss 0.06753165274858475 Accuracy 0.812000036239624\n",
      "Iteration 5730 Training loss 0.06295082718133926 Validation loss 0.06753633916378021 Accuracy 0.8125000596046448\n",
      "Iteration 5740 Training loss 0.06696009635925293 Validation loss 0.06747746467590332 Accuracy 0.8126250505447388\n",
      "Iteration 5750 Training loss 0.06800226867198944 Validation loss 0.06748190522193909 Accuracy 0.8128750324249268\n",
      "Iteration 5760 Training loss 0.07028143852949142 Validation loss 0.06749800592660904 Accuracy 0.8125000596046448\n",
      "Iteration 5770 Training loss 0.07751080393791199 Validation loss 0.06741175800561905 Accuracy 0.8130000233650208\n",
      "Iteration 5780 Training loss 0.060945603996515274 Validation loss 0.06749621778726578 Accuracy 0.8133750557899475\n",
      "Iteration 5790 Training loss 0.06140337511897087 Validation loss 0.06744915246963501 Accuracy 0.8132500648498535\n",
      "Iteration 5800 Training loss 0.053180232644081116 Validation loss 0.06746207922697067 Accuracy 0.8135000467300415\n",
      "Iteration 5810 Training loss 0.0611632876098156 Validation loss 0.06745556741952896 Accuracy 0.8133750557899475\n",
      "Iteration 5820 Training loss 0.06909959018230438 Validation loss 0.06737016141414642 Accuracy 0.8133750557899475\n",
      "Iteration 5830 Training loss 0.06353221833705902 Validation loss 0.06731695681810379 Accuracy 0.8130000233650208\n",
      "Iteration 5840 Training loss 0.06993842869997025 Validation loss 0.06751477718353271 Accuracy 0.81187504529953\n",
      "Iteration 5850 Training loss 0.06844541430473328 Validation loss 0.06732930988073349 Accuracy 0.8137500286102295\n",
      "Iteration 5860 Training loss 0.0693671852350235 Validation loss 0.06732885539531708 Accuracy 0.8136250376701355\n",
      "Iteration 5870 Training loss 0.07209201902151108 Validation loss 0.06732385605573654 Accuracy 0.8137500286102295\n",
      "Iteration 5880 Training loss 0.06787533313035965 Validation loss 0.06729882210493088 Accuracy 0.8142500519752502\n",
      "Iteration 5890 Training loss 0.06990060210227966 Validation loss 0.06723140925168991 Accuracy 0.8132500648498535\n",
      "Iteration 5900 Training loss 0.06515707820653915 Validation loss 0.06728368252515793 Accuracy 0.8136250376701355\n",
      "Iteration 5910 Training loss 0.0707579180598259 Validation loss 0.0671984925866127 Accuracy 0.8131250143051147\n",
      "Iteration 5920 Training loss 0.07884246110916138 Validation loss 0.06720919907093048 Accuracy 0.8133750557899475\n",
      "Iteration 5930 Training loss 0.06905848532915115 Validation loss 0.0671568289399147 Accuracy 0.8135000467300415\n",
      "Iteration 5940 Training loss 0.059458520263433456 Validation loss 0.06728573888540268 Accuracy 0.8138750195503235\n",
      "Iteration 5950 Training loss 0.06127655506134033 Validation loss 0.06714073568582535 Accuracy 0.8141250610351562\n",
      "Iteration 5960 Training loss 0.06962031871080399 Validation loss 0.06751534342765808 Accuracy 0.8131250143051147\n",
      "Iteration 5970 Training loss 0.062109917402267456 Validation loss 0.06714053452014923 Accuracy 0.8136250376701355\n",
      "Iteration 5980 Training loss 0.06201697140932083 Validation loss 0.06706871837377548 Accuracy 0.8140000104904175\n",
      "Iteration 5990 Training loss 0.07180998474359512 Validation loss 0.06710544973611832 Accuracy 0.8136250376701355\n",
      "Iteration 6000 Training loss 0.0696384459733963 Validation loss 0.0670362263917923 Accuracy 0.8140000104904175\n",
      "Iteration 6010 Training loss 0.06365544348955154 Validation loss 0.06714435666799545 Accuracy 0.8133750557899475\n",
      "Iteration 6020 Training loss 0.06122339144349098 Validation loss 0.06699086725711823 Accuracy 0.8141250610351562\n",
      "Iteration 6030 Training loss 0.07146988809108734 Validation loss 0.06697886437177658 Accuracy 0.8143750429153442\n",
      "Iteration 6040 Training loss 0.06444161385297775 Validation loss 0.06696029007434845 Accuracy 0.8140000104904175\n",
      "Iteration 6050 Training loss 0.06955195963382721 Validation loss 0.06746967136859894 Accuracy 0.8125000596046448\n",
      "Iteration 6060 Training loss 0.06885996460914612 Validation loss 0.0669315904378891 Accuracy 0.8143750429153442\n",
      "Iteration 6070 Training loss 0.06951221823692322 Validation loss 0.06698374450206757 Accuracy 0.8138750195503235\n",
      "Iteration 6080 Training loss 0.06464351713657379 Validation loss 0.06714721769094467 Accuracy 0.815250039100647\n",
      "Iteration 6090 Training loss 0.06484196335077286 Validation loss 0.06690876185894012 Accuracy 0.8138750195503235\n",
      "Iteration 6100 Training loss 0.06134750694036484 Validation loss 0.06688965857028961 Accuracy 0.8141250610351562\n",
      "Iteration 6110 Training loss 0.06604740768671036 Validation loss 0.06688820570707321 Accuracy 0.8147500157356262\n",
      "Iteration 6120 Training loss 0.07030268758535385 Validation loss 0.06684661656618118 Accuracy 0.8141250610351562\n",
      "Iteration 6130 Training loss 0.0694299191236496 Validation loss 0.06681974232196808 Accuracy 0.8147500157356262\n",
      "Iteration 6140 Training loss 0.05934039130806923 Validation loss 0.06684854626655579 Accuracy 0.8143750429153442\n",
      "Iteration 6150 Training loss 0.06355643272399902 Validation loss 0.06680586189031601 Accuracy 0.8138750195503235\n",
      "Iteration 6160 Training loss 0.062158968299627304 Validation loss 0.06701521575450897 Accuracy 0.814875066280365\n",
      "Iteration 6170 Training loss 0.06529438495635986 Validation loss 0.06683012843132019 Accuracy 0.814875066280365\n",
      "Iteration 6180 Training loss 0.06011557579040527 Validation loss 0.06683474034070969 Accuracy 0.8145000338554382\n",
      "Iteration 6190 Training loss 0.06511890143156052 Validation loss 0.06688551604747772 Accuracy 0.815125048160553\n",
      "Iteration 6200 Training loss 0.05802155286073685 Validation loss 0.06676550209522247 Accuracy 0.8145000338554382\n",
      "Iteration 6210 Training loss 0.05804198607802391 Validation loss 0.06668265163898468 Accuracy 0.8147500157356262\n",
      "Iteration 6220 Training loss 0.06416597217321396 Validation loss 0.06717368960380554 Accuracy 0.8136250376701355\n",
      "Iteration 6230 Training loss 0.06478268653154373 Validation loss 0.06695225834846497 Accuracy 0.8145000338554382\n",
      "Iteration 6240 Training loss 0.06392757594585419 Validation loss 0.06664230674505234 Accuracy 0.8147500157356262\n",
      "Iteration 6250 Training loss 0.06463950872421265 Validation loss 0.06665302067995071 Accuracy 0.815125048160553\n",
      "Iteration 6260 Training loss 0.07338415086269379 Validation loss 0.06668159365653992 Accuracy 0.815250039100647\n",
      "Iteration 6270 Training loss 0.07666267454624176 Validation loss 0.0665920227766037 Accuracy 0.815125048160553\n",
      "Iteration 6280 Training loss 0.057215455919504166 Validation loss 0.06662444025278091 Accuracy 0.815250039100647\n",
      "Iteration 6290 Training loss 0.06484883278608322 Validation loss 0.06656254082918167 Accuracy 0.815250039100647\n",
      "Iteration 6300 Training loss 0.06753669679164886 Validation loss 0.06654231250286102 Accuracy 0.815250039100647\n",
      "Iteration 6310 Training loss 0.06204644963145256 Validation loss 0.06655927747488022 Accuracy 0.815625011920929\n",
      "Iteration 6320 Training loss 0.06569284200668335 Validation loss 0.06678388267755508 Accuracy 0.8146250247955322\n",
      "Iteration 6330 Training loss 0.06575359404087067 Validation loss 0.06649381667375565 Accuracy 0.815500020980835\n",
      "Iteration 6340 Training loss 0.06954080611467361 Validation loss 0.06648688018321991 Accuracy 0.815375030040741\n",
      "Iteration 6350 Training loss 0.06352047622203827 Validation loss 0.066510871052742 Accuracy 0.8161250352859497\n",
      "Iteration 6360 Training loss 0.06885291635990143 Validation loss 0.06647554039955139 Accuracy 0.815500020980835\n",
      "Iteration 6370 Training loss 0.06693705171346664 Validation loss 0.06648856401443481 Accuracy 0.815125048160553\n",
      "Iteration 6380 Training loss 0.0676521360874176 Validation loss 0.06650693714618683 Accuracy 0.8161250352859497\n",
      "Iteration 6390 Training loss 0.06498828530311584 Validation loss 0.06646312028169632 Accuracy 0.815625011920929\n",
      "Iteration 6400 Training loss 0.07558368146419525 Validation loss 0.06637006253004074 Accuracy 0.8157500624656677\n",
      "Iteration 6410 Training loss 0.06242396682500839 Validation loss 0.06636635214090347 Accuracy 0.815375030040741\n",
      "Iteration 6420 Training loss 0.06085114926099777 Validation loss 0.06640893220901489 Accuracy 0.8158750534057617\n",
      "Iteration 6430 Training loss 0.05742424353957176 Validation loss 0.06634160131216049 Accuracy 0.815500020980835\n",
      "Iteration 6440 Training loss 0.07654811441898346 Validation loss 0.06637532263994217 Accuracy 0.8160000443458557\n",
      "Iteration 6450 Training loss 0.07264188677072525 Validation loss 0.06646301597356796 Accuracy 0.8166250586509705\n",
      "Iteration 6460 Training loss 0.06627220660448074 Validation loss 0.06630758941173553 Accuracy 0.8160000443458557\n",
      "Iteration 6470 Training loss 0.07352431863546371 Validation loss 0.06627347320318222 Accuracy 0.8160000443458557\n",
      "Iteration 6480 Training loss 0.06849682331085205 Validation loss 0.0663248598575592 Accuracy 0.8168750405311584\n",
      "Iteration 6490 Training loss 0.06786877661943436 Validation loss 0.06623814254999161 Accuracy 0.815375030040741\n",
      "Iteration 6500 Training loss 0.07306573539972305 Validation loss 0.06623640656471252 Accuracy 0.8168750405311584\n",
      "Iteration 6510 Training loss 0.0679226666688919 Validation loss 0.06618831306695938 Accuracy 0.8160000443458557\n",
      "Iteration 6520 Training loss 0.07867877930402756 Validation loss 0.06619240343570709 Accuracy 0.8170000314712524\n",
      "Iteration 6530 Training loss 0.06963752955198288 Validation loss 0.0661531388759613 Accuracy 0.8162500262260437\n",
      "Iteration 6540 Training loss 0.06919605284929276 Validation loss 0.06617972254753113 Accuracy 0.8163750171661377\n",
      "Iteration 6550 Training loss 0.06868761032819748 Validation loss 0.06612777709960938 Accuracy 0.8160000443458557\n",
      "Iteration 6560 Training loss 0.06656426191329956 Validation loss 0.06610679626464844 Accuracy 0.8166250586509705\n",
      "Iteration 6570 Training loss 0.06588927656412125 Validation loss 0.06611263006925583 Accuracy 0.8175000548362732\n",
      "Iteration 6580 Training loss 0.06273451447486877 Validation loss 0.06612387299537659 Accuracy 0.8167500495910645\n",
      "Iteration 6590 Training loss 0.07128636538982391 Validation loss 0.06607582420110703 Accuracy 0.8172500133514404\n",
      "Iteration 6600 Training loss 0.06122797355055809 Validation loss 0.06617046147584915 Accuracy 0.8175000548362732\n",
      "Iteration 6610 Training loss 0.06364850699901581 Validation loss 0.06618800759315491 Accuracy 0.8172500133514404\n",
      "Iteration 6620 Training loss 0.061800550669431686 Validation loss 0.06606514751911163 Accuracy 0.8170000314712524\n",
      "Iteration 6630 Training loss 0.05932433903217316 Validation loss 0.06601110100746155 Accuracy 0.8166250586509705\n",
      "Iteration 6640 Training loss 0.0663241297006607 Validation loss 0.06603292375802994 Accuracy 0.8176250457763672\n",
      "Iteration 6650 Training loss 0.06500868499279022 Validation loss 0.06598573923110962 Accuracy 0.8165000677108765\n",
      "Iteration 6660 Training loss 0.0659283846616745 Validation loss 0.06595534086227417 Accuracy 0.8171250224113464\n",
      "Iteration 6670 Training loss 0.06111398711800575 Validation loss 0.06593203544616699 Accuracy 0.8170000314712524\n",
      "Iteration 6680 Training loss 0.06576399505138397 Validation loss 0.06593814492225647 Accuracy 0.8166250586509705\n",
      "Iteration 6690 Training loss 0.06674408912658691 Validation loss 0.06589721143245697 Accuracy 0.8173750638961792\n",
      "Iteration 6700 Training loss 0.05981559306383133 Validation loss 0.06587304919958115 Accuracy 0.8171250224113464\n",
      "Iteration 6710 Training loss 0.06411616504192352 Validation loss 0.06594458222389221 Accuracy 0.8185000419616699\n",
      "Iteration 6720 Training loss 0.07587116956710815 Validation loss 0.06595205515623093 Accuracy 0.8183750510215759\n",
      "Iteration 6730 Training loss 0.0621701218187809 Validation loss 0.06584685295820236 Accuracy 0.8180000185966492\n",
      "Iteration 6740 Training loss 0.06147259473800659 Validation loss 0.06593780964612961 Accuracy 0.8181250095367432\n",
      "Iteration 6750 Training loss 0.05532769858837128 Validation loss 0.06580080837011337 Accuracy 0.8180000185966492\n",
      "Iteration 6760 Training loss 0.061986491084098816 Validation loss 0.06584802269935608 Accuracy 0.8185000419616699\n",
      "Iteration 6770 Training loss 0.06526541709899902 Validation loss 0.06578461825847626 Accuracy 0.8181250095367432\n",
      "Iteration 6780 Training loss 0.06275837868452072 Validation loss 0.06592737883329391 Accuracy 0.8186250329017639\n",
      "Iteration 6790 Training loss 0.0628361701965332 Validation loss 0.06585020571947098 Accuracy 0.8178750276565552\n",
      "Iteration 6800 Training loss 0.05680691450834274 Validation loss 0.06579364091157913 Accuracy 0.8185000419616699\n",
      "Iteration 6810 Training loss 0.06343197077512741 Validation loss 0.06598860770463943 Accuracy 0.8191250562667847\n",
      "Iteration 6820 Training loss 0.07937970757484436 Validation loss 0.06572052091360092 Accuracy 0.8183750510215759\n",
      "Iteration 6830 Training loss 0.05972816422581673 Validation loss 0.06571338325738907 Accuracy 0.8183750510215759\n",
      "Iteration 6840 Training loss 0.06153246387839317 Validation loss 0.06567111611366272 Accuracy 0.8176250457763672\n",
      "Iteration 6850 Training loss 0.07239577174186707 Validation loss 0.06567619740962982 Accuracy 0.8180000185966492\n",
      "Iteration 6860 Training loss 0.06378145515918732 Validation loss 0.06563730537891388 Accuracy 0.8175000548362732\n",
      "Iteration 6870 Training loss 0.0645333006978035 Validation loss 0.06560754776000977 Accuracy 0.8178750276565552\n",
      "Iteration 6880 Training loss 0.06605759263038635 Validation loss 0.06559233367443085 Accuracy 0.8178750276565552\n",
      "Iteration 6890 Training loss 0.06375133991241455 Validation loss 0.0655781552195549 Accuracy 0.8183750510215759\n",
      "Iteration 6900 Training loss 0.056093666702508926 Validation loss 0.06555455178022385 Accuracy 0.8177500367164612\n",
      "Iteration 6910 Training loss 0.06245870515704155 Validation loss 0.06554847955703735 Accuracy 0.8177500367164612\n",
      "Iteration 6920 Training loss 0.06143127381801605 Validation loss 0.06561457365751266 Accuracy 0.8188750147819519\n",
      "Iteration 6930 Training loss 0.06488392502069473 Validation loss 0.06553332507610321 Accuracy 0.8182500600814819\n",
      "Iteration 6940 Training loss 0.07253771275281906 Validation loss 0.06552177667617798 Accuracy 0.8183750510215759\n",
      "Iteration 6950 Training loss 0.058245182037353516 Validation loss 0.06549205631017685 Accuracy 0.8190000653266907\n",
      "Iteration 6960 Training loss 0.07019993662834167 Validation loss 0.06547053158283234 Accuracy 0.8185000419616699\n",
      "Iteration 6970 Training loss 0.06535323709249496 Validation loss 0.06553738564252853 Accuracy 0.8182500600814819\n",
      "Iteration 6980 Training loss 0.06164005026221275 Validation loss 0.06542995572090149 Accuracy 0.8188750147819519\n",
      "Iteration 6990 Training loss 0.0704704150557518 Validation loss 0.06541652232408524 Accuracy 0.8190000653266907\n",
      "Iteration 7000 Training loss 0.059174373745918274 Validation loss 0.06538428366184235 Accuracy 0.8188750147819519\n",
      "Iteration 7010 Training loss 0.060783594846725464 Validation loss 0.06550928205251694 Accuracy 0.8201250433921814\n",
      "Iteration 7020 Training loss 0.06283921748399734 Validation loss 0.06540346145629883 Accuracy 0.8196250200271606\n",
      "Iteration 7030 Training loss 0.06568188965320587 Validation loss 0.06535187363624573 Accuracy 0.8197500109672546\n",
      "Iteration 7040 Training loss 0.07636140286922455 Validation loss 0.06532035768032074 Accuracy 0.8196250200271606\n",
      "Iteration 7050 Training loss 0.06487905234098434 Validation loss 0.06531520932912827 Accuracy 0.8196250200271606\n",
      "Iteration 7060 Training loss 0.05892190337181091 Validation loss 0.06536171585321426 Accuracy 0.8198750615119934\n",
      "Iteration 7070 Training loss 0.06221131980419159 Validation loss 0.06535772234201431 Accuracy 0.8181250095367432\n",
      "Iteration 7080 Training loss 0.06644958257675171 Validation loss 0.06542645394802094 Accuracy 0.8197500109672546\n",
      "Iteration 7090 Training loss 0.0686681941151619 Validation loss 0.06526752561330795 Accuracy 0.8198750615119934\n",
      "Iteration 7100 Training loss 0.06249213218688965 Validation loss 0.06530024856328964 Accuracy 0.8188750147819519\n",
      "Iteration 7110 Training loss 0.06513381004333496 Validation loss 0.06521221995353699 Accuracy 0.8191250562667847\n",
      "Iteration 7120 Training loss 0.06606797873973846 Validation loss 0.06522060185670853 Accuracy 0.8197500109672546\n",
      "Iteration 7130 Training loss 0.05679646506905556 Validation loss 0.06518199294805527 Accuracy 0.8196250200271606\n",
      "Iteration 7140 Training loss 0.068801648914814 Validation loss 0.06527859717607498 Accuracy 0.8197500109672546\n",
      "Iteration 7150 Training loss 0.06098879128694534 Validation loss 0.06515397876501083 Accuracy 0.8193750381469727\n",
      "Iteration 7160 Training loss 0.065358467400074 Validation loss 0.0651698186993599 Accuracy 0.8202500343322754\n",
      "Iteration 7170 Training loss 0.0592239685356617 Validation loss 0.06512140482664108 Accuracy 0.8200000524520874\n",
      "Iteration 7180 Training loss 0.06453320384025574 Validation loss 0.06510630995035172 Accuracy 0.8200000524520874\n",
      "Iteration 7190 Training loss 0.06213752552866936 Validation loss 0.06509480625391006 Accuracy 0.8203750252723694\n",
      "Iteration 7200 Training loss 0.06841178983449936 Validation loss 0.06506866216659546 Accuracy 0.8200000524520874\n",
      "Iteration 7210 Training loss 0.05824866145849228 Validation loss 0.0650637075304985 Accuracy 0.8207500576972961\n",
      "Iteration 7220 Training loss 0.06995488703250885 Validation loss 0.06508273631334305 Accuracy 0.8210000395774841\n",
      "Iteration 7230 Training loss 0.06352919340133667 Validation loss 0.06506050378084183 Accuracy 0.8212500214576721\n",
      "Iteration 7240 Training loss 0.06504819542169571 Validation loss 0.06501471251249313 Accuracy 0.8203750252723694\n",
      "Iteration 7250 Training loss 0.06101822853088379 Validation loss 0.06499026715755463 Accuracy 0.8202500343322754\n",
      "Iteration 7260 Training loss 0.0573812760412693 Validation loss 0.06501699984073639 Accuracy 0.8211250305175781\n",
      "Iteration 7270 Training loss 0.07067101448774338 Validation loss 0.06495317816734314 Accuracy 0.8205000162124634\n",
      "Iteration 7280 Training loss 0.06803537905216217 Validation loss 0.06495924293994904 Accuracy 0.8205000162124634\n",
      "Iteration 7290 Training loss 0.0682692676782608 Validation loss 0.06492208689451218 Accuracy 0.8212500214576721\n",
      "Iteration 7300 Training loss 0.06732482463121414 Validation loss 0.06489667296409607 Accuracy 0.8205000162124634\n",
      "Iteration 7310 Training loss 0.06475352495908737 Validation loss 0.06488486379384995 Accuracy 0.8215000629425049\n",
      "Iteration 7320 Training loss 0.06423351168632507 Validation loss 0.06487151235342026 Accuracy 0.8205000162124634\n",
      "Iteration 7330 Training loss 0.05664129927754402 Validation loss 0.06485437601804733 Accuracy 0.8208750486373901\n",
      "Iteration 7340 Training loss 0.0659378245472908 Validation loss 0.064947709441185 Accuracy 0.8211250305175781\n",
      "Iteration 7350 Training loss 0.05988754704594612 Validation loss 0.06495656073093414 Accuracy 0.8208750486373901\n",
      "Iteration 7360 Training loss 0.07373979687690735 Validation loss 0.06491722911596298 Accuracy 0.8208750486373901\n",
      "Iteration 7370 Training loss 0.06114506721496582 Validation loss 0.06486544758081436 Accuracy 0.8216250538825989\n",
      "Iteration 7380 Training loss 0.05167557671666145 Validation loss 0.06482452154159546 Accuracy 0.8213750123977661\n",
      "Iteration 7390 Training loss 0.06665558367967606 Validation loss 0.06476671248674393 Accuracy 0.8211250305175781\n",
      "Iteration 7400 Training loss 0.06468239426612854 Validation loss 0.06487250328063965 Accuracy 0.8213750123977661\n",
      "Iteration 7410 Training loss 0.06984629482030869 Validation loss 0.0647544264793396 Accuracy 0.8213750123977661\n",
      "Iteration 7420 Training loss 0.06132214888930321 Validation loss 0.06471320241689682 Accuracy 0.8211250305175781\n",
      "Iteration 7430 Training loss 0.06295667588710785 Validation loss 0.0649951696395874 Accuracy 0.8220000267028809\n",
      "Iteration 7440 Training loss 0.05657610669732094 Validation loss 0.06468679010868073 Accuracy 0.8213750123977661\n",
      "Iteration 7450 Training loss 0.05879834666848183 Validation loss 0.06475677341222763 Accuracy 0.8210000395774841\n",
      "Iteration 7460 Training loss 0.06640230864286423 Validation loss 0.06472204625606537 Accuracy 0.8225000500679016\n",
      "Iteration 7470 Training loss 0.06576027721166611 Validation loss 0.06463424116373062 Accuracy 0.8215000629425049\n",
      "Iteration 7480 Training loss 0.0679720938205719 Validation loss 0.0646134614944458 Accuracy 0.8213750123977661\n",
      "Iteration 7490 Training loss 0.06747990101575851 Validation loss 0.06460929661989212 Accuracy 0.8222500681877136\n",
      "Iteration 7500 Training loss 0.06675033271312714 Validation loss 0.06459496915340424 Accuracy 0.8221250176429749\n",
      "Iteration 7510 Training loss 0.07028558850288391 Validation loss 0.06464210897684097 Accuracy 0.8222500681877136\n",
      "Iteration 7520 Training loss 0.06208143010735512 Validation loss 0.06454013288021088 Accuracy 0.8222500681877136\n",
      "Iteration 7530 Training loss 0.06676601618528366 Validation loss 0.06464597582817078 Accuracy 0.8227500319480896\n",
      "Iteration 7540 Training loss 0.06740011274814606 Validation loss 0.06460293382406235 Accuracy 0.8225000500679016\n",
      "Iteration 7550 Training loss 0.06251496076583862 Validation loss 0.06450843811035156 Accuracy 0.8231250643730164\n",
      "Iteration 7560 Training loss 0.05575350672006607 Validation loss 0.06448228657245636 Accuracy 0.8213750123977661\n",
      "Iteration 7570 Training loss 0.06300414353609085 Validation loss 0.06446444988250732 Accuracy 0.8212500214576721\n",
      "Iteration 7580 Training loss 0.057811055332422256 Validation loss 0.06445436179637909 Accuracy 0.8236250281333923\n",
      "Iteration 7590 Training loss 0.058705948293209076 Validation loss 0.06445199251174927 Accuracy 0.8233750462532043\n",
      "Iteration 7600 Training loss 0.06362738460302353 Validation loss 0.06463358551263809 Accuracy 0.8228750228881836\n",
      "Iteration 7610 Training loss 0.07020117342472076 Validation loss 0.0645533949136734 Accuracy 0.8226250410079956\n",
      "Iteration 7620 Training loss 0.06250862777233124 Validation loss 0.06442099064588547 Accuracy 0.8235000371932983\n",
      "Iteration 7630 Training loss 0.06060314178466797 Validation loss 0.06439575552940369 Accuracy 0.8217500448226929\n",
      "Iteration 7640 Training loss 0.058280814439058304 Validation loss 0.06442642211914062 Accuracy 0.8223750591278076\n",
      "Iteration 7650 Training loss 0.057554781436920166 Validation loss 0.06434386223554611 Accuracy 0.8227500319480896\n",
      "Iteration 7660 Training loss 0.06690476089715958 Validation loss 0.06451370567083359 Accuracy 0.8236250281333923\n",
      "Iteration 7670 Training loss 0.06534121185541153 Validation loss 0.06430882960557938 Accuracy 0.8231250643730164\n",
      "Iteration 7680 Training loss 0.06893236935138702 Validation loss 0.06434354186058044 Accuracy 0.8230000138282776\n",
      "Iteration 7690 Training loss 0.05858687311410904 Validation loss 0.06429849565029144 Accuracy 0.8230000138282776\n",
      "Iteration 7700 Training loss 0.06713304668664932 Validation loss 0.06426837295293808 Accuracy 0.8226250410079956\n",
      "Iteration 7710 Training loss 0.07071702927350998 Validation loss 0.06449083983898163 Accuracy 0.8245000243186951\n",
      "Iteration 7720 Training loss 0.06458428502082825 Validation loss 0.06422484666109085 Accuracy 0.8227500319480896\n",
      "Iteration 7730 Training loss 0.06383545696735382 Validation loss 0.06422499567270279 Accuracy 0.8235000371932983\n",
      "Iteration 7740 Training loss 0.0666830837726593 Validation loss 0.06419548392295837 Accuracy 0.8233750462532043\n",
      "Iteration 7750 Training loss 0.06117631122469902 Validation loss 0.06433077901601791 Accuracy 0.8235000371932983\n",
      "Iteration 7760 Training loss 0.05795083940029144 Validation loss 0.0641668364405632 Accuracy 0.8236250281333923\n",
      "Iteration 7770 Training loss 0.06332764029502869 Validation loss 0.0642726942896843 Accuracy 0.8236250281333923\n",
      "Iteration 7780 Training loss 0.0647461861371994 Validation loss 0.06439542025327682 Accuracy 0.8248750567436218\n",
      "Iteration 7790 Training loss 0.05940411612391472 Validation loss 0.0642106831073761 Accuracy 0.8241250514984131\n",
      "Iteration 7800 Training loss 0.05950479209423065 Validation loss 0.06425195932388306 Accuracy 0.8232500553131104\n",
      "Iteration 7810 Training loss 0.059675201773643494 Validation loss 0.06409170478582382 Accuracy 0.8235000371932983\n",
      "Iteration 7820 Training loss 0.06121901795268059 Validation loss 0.06415107101202011 Accuracy 0.8236250281333923\n",
      "Iteration 7830 Training loss 0.06377773731946945 Validation loss 0.06409305334091187 Accuracy 0.8241250514984131\n",
      "Iteration 7840 Training loss 0.0704035684466362 Validation loss 0.06405504047870636 Accuracy 0.8238750100135803\n",
      "Iteration 7850 Training loss 0.0673663541674614 Validation loss 0.0640232190489769 Accuracy 0.8235000371932983\n",
      "Iteration 7860 Training loss 0.06666581332683563 Validation loss 0.0640380010008812 Accuracy 0.8247500658035278\n",
      "Iteration 7870 Training loss 0.0593809150159359 Validation loss 0.06401482224464417 Accuracy 0.8238750100135803\n",
      "Iteration 7880 Training loss 0.06596700102090836 Validation loss 0.06406188011169434 Accuracy 0.8243750333786011\n",
      "Iteration 7890 Training loss 0.06085066497325897 Validation loss 0.0640425831079483 Accuracy 0.8238750100135803\n",
      "Iteration 7900 Training loss 0.060429323464632034 Validation loss 0.06395772099494934 Accuracy 0.8237500190734863\n",
      "Iteration 7910 Training loss 0.059117019176483154 Validation loss 0.06448879092931747 Accuracy 0.8243750333786011\n",
      "Iteration 7920 Training loss 0.06078754737973213 Validation loss 0.06390845775604248 Accuracy 0.8250000476837158\n",
      "Iteration 7930 Training loss 0.06546617299318314 Validation loss 0.06389353424310684 Accuracy 0.8247500658035278\n",
      "Iteration 7940 Training loss 0.0631253570318222 Validation loss 0.06388434022665024 Accuracy 0.8247500658035278\n",
      "Iteration 7950 Training loss 0.06551475077867508 Validation loss 0.06398577988147736 Accuracy 0.8238750100135803\n",
      "Iteration 7960 Training loss 0.06627746671438217 Validation loss 0.06385333091020584 Accuracy 0.8238750100135803\n",
      "Iteration 7970 Training loss 0.06158829480409622 Validation loss 0.06413561850786209 Accuracy 0.8252500295639038\n",
      "Iteration 7980 Training loss 0.07449058443307877 Validation loss 0.06383226066827774 Accuracy 0.8245000243186951\n",
      "Iteration 7990 Training loss 0.06676703691482544 Validation loss 0.06384895741939545 Accuracy 0.8246250152587891\n",
      "Iteration 8000 Training loss 0.06579456478357315 Validation loss 0.06380448490381241 Accuracy 0.8247500658035278\n",
      "Iteration 8010 Training loss 0.06273773312568665 Validation loss 0.06378350406885147 Accuracy 0.8248750567436218\n",
      "Iteration 8020 Training loss 0.06536094099283218 Validation loss 0.063778817653656 Accuracy 0.8245000243186951\n",
      "Iteration 8030 Training loss 0.05883318930864334 Validation loss 0.06375737488269806 Accuracy 0.8255000114440918\n",
      "Iteration 8040 Training loss 0.06286898255348206 Validation loss 0.06397468596696854 Accuracy 0.8246250152587891\n",
      "Iteration 8050 Training loss 0.05775174871087074 Validation loss 0.06404710561037064 Accuracy 0.8255000114440918\n",
      "Iteration 8060 Training loss 0.07355529814958572 Validation loss 0.0637185350060463 Accuracy 0.8242500424385071\n",
      "Iteration 8070 Training loss 0.07035409659147263 Validation loss 0.0637255311012268 Accuracy 0.8242500424385071\n",
      "Iteration 8080 Training loss 0.05761409550905228 Validation loss 0.06378041952848434 Accuracy 0.8245000243186951\n",
      "Iteration 8090 Training loss 0.06804726272821426 Validation loss 0.06370091438293457 Accuracy 0.8245000243186951\n",
      "Iteration 8100 Training loss 0.06062045320868492 Validation loss 0.06364040821790695 Accuracy 0.8252500295639038\n",
      "Iteration 8110 Training loss 0.05697614699602127 Validation loss 0.06389676779508591 Accuracy 0.8246250152587891\n",
      "Iteration 8120 Training loss 0.05813146382570267 Validation loss 0.06383495777845383 Accuracy 0.8243750333786011\n",
      "Iteration 8130 Training loss 0.06266575306653976 Validation loss 0.06359969824552536 Accuracy 0.8251250386238098\n",
      "Iteration 8140 Training loss 0.06772144883871078 Validation loss 0.06371352076530457 Accuracy 0.8262500166893005\n",
      "Iteration 8150 Training loss 0.06479518115520477 Validation loss 0.06356547027826309 Accuracy 0.8255000114440918\n",
      "Iteration 8160 Training loss 0.0630655363202095 Validation loss 0.06362144649028778 Accuracy 0.8258750438690186\n",
      "Iteration 8170 Training loss 0.058712128549814224 Validation loss 0.06355379521846771 Accuracy 0.8253750205039978\n",
      "Iteration 8180 Training loss 0.06086951121687889 Validation loss 0.0636327713727951 Accuracy 0.8263750672340393\n",
      "Iteration 8190 Training loss 0.061709463596343994 Validation loss 0.06364304572343826 Accuracy 0.8256250619888306\n",
      "Iteration 8200 Training loss 0.06007910892367363 Validation loss 0.06352023035287857 Accuracy 0.8267500400543213\n",
      "Iteration 8210 Training loss 0.055049389600753784 Validation loss 0.0634731724858284 Accuracy 0.8260000348091125\n",
      "Iteration 8220 Training loss 0.06462343782186508 Validation loss 0.06361540406942368 Accuracy 0.8255000114440918\n",
      "Iteration 8230 Training loss 0.06589678674936295 Validation loss 0.06380491703748703 Accuracy 0.8271250128746033\n",
      "Iteration 8240 Training loss 0.06446638703346252 Validation loss 0.06347116827964783 Accuracy 0.8263750672340393\n",
      "Iteration 8250 Training loss 0.05815356969833374 Validation loss 0.06341861188411713 Accuracy 0.8266250491142273\n",
      "Iteration 8260 Training loss 0.057723596692085266 Validation loss 0.06340299546718597 Accuracy 0.8268750309944153\n",
      "Iteration 8270 Training loss 0.06554900109767914 Validation loss 0.0634191632270813 Accuracy 0.8262500166893005\n",
      "Iteration 8280 Training loss 0.06899344176054001 Validation loss 0.06339739263057709 Accuracy 0.8270000219345093\n",
      "Iteration 8290 Training loss 0.05962804704904556 Validation loss 0.0633796751499176 Accuracy 0.8266250491142273\n",
      "Iteration 8300 Training loss 0.06243416294455528 Validation loss 0.0634177029132843 Accuracy 0.8270000219345093\n",
      "Iteration 8310 Training loss 0.06766363978385925 Validation loss 0.06333498656749725 Accuracy 0.827875018119812\n",
      "Iteration 8320 Training loss 0.06187113747000694 Validation loss 0.06345817446708679 Accuracy 0.827250063419342\n",
      "Iteration 8330 Training loss 0.06309868395328522 Validation loss 0.06361176818609238 Accuracy 0.8270000219345093\n",
      "Iteration 8340 Training loss 0.06934244185686111 Validation loss 0.06336937099695206 Accuracy 0.827625036239624\n",
      "Iteration 8350 Training loss 0.06250031292438507 Validation loss 0.06328681111335754 Accuracy 0.827750027179718\n",
      "Iteration 8360 Training loss 0.05867442488670349 Validation loss 0.06326641887426376 Accuracy 0.827750027179718\n",
      "Iteration 8370 Training loss 0.05699126049876213 Validation loss 0.06338091939687729 Accuracy 0.8268750309944153\n",
      "Iteration 8380 Training loss 0.06391111016273499 Validation loss 0.06325899809598923 Accuracy 0.827250063419342\n",
      "Iteration 8390 Training loss 0.05970574542880058 Validation loss 0.06331667304039001 Accuracy 0.827875018119812\n",
      "Iteration 8400 Training loss 0.05195623263716698 Validation loss 0.06322073936462402 Accuracy 0.8280000686645508\n",
      "Iteration 8410 Training loss 0.06607142090797424 Validation loss 0.06322398036718369 Accuracy 0.827875018119812\n",
      "Iteration 8420 Training loss 0.061275601387023926 Validation loss 0.06328698247671127 Accuracy 0.8281250596046448\n",
      "Iteration 8430 Training loss 0.07757452130317688 Validation loss 0.06321229040622711 Accuracy 0.8286250233650208\n",
      "Iteration 8440 Training loss 0.06550489366054535 Validation loss 0.06319081038236618 Accuracy 0.82750004529953\n",
      "Iteration 8450 Training loss 0.06027007848024368 Validation loss 0.06317513436079025 Accuracy 0.827375054359436\n",
      "Iteration 8460 Training loss 0.06219421699643135 Validation loss 0.06314347684383392 Accuracy 0.8287500143051147\n",
      "Iteration 8470 Training loss 0.05830104649066925 Validation loss 0.06318280100822449 Accuracy 0.8280000686645508\n",
      "Iteration 8480 Training loss 0.06643068790435791 Validation loss 0.06332395225763321 Accuracy 0.827750027179718\n",
      "Iteration 8490 Training loss 0.055936772376298904 Validation loss 0.06318441033363342 Accuracy 0.827375054359436\n",
      "Iteration 8500 Training loss 0.060718778520822525 Validation loss 0.06323815882205963 Accuracy 0.8270000219345093\n",
      "Iteration 8510 Training loss 0.05892210826277733 Validation loss 0.06306347250938416 Accuracy 0.8286250233650208\n",
      "Iteration 8520 Training loss 0.06316913664340973 Validation loss 0.06304727494716644 Accuracy 0.8286250233650208\n",
      "Iteration 8530 Training loss 0.06742783635854721 Validation loss 0.06305672973394394 Accuracy 0.8285000324249268\n",
      "Iteration 8540 Training loss 0.055141057819128036 Validation loss 0.06310997903347015 Accuracy 0.8268750309944153\n",
      "Iteration 8550 Training loss 0.06478102505207062 Validation loss 0.06309791654348373 Accuracy 0.8268750309944153\n",
      "Iteration 8560 Training loss 0.06336572021245956 Validation loss 0.0629761815071106 Accuracy 0.8280000686645508\n",
      "Iteration 8570 Training loss 0.06242198869585991 Validation loss 0.06295983493328094 Accuracy 0.827750027179718\n",
      "Iteration 8580 Training loss 0.06100780516862869 Validation loss 0.06294353306293488 Accuracy 0.8281250596046448\n",
      "Iteration 8590 Training loss 0.06455179303884506 Validation loss 0.0629744604229927 Accuracy 0.8281250596046448\n",
      "Iteration 8600 Training loss 0.06169450655579567 Validation loss 0.06311909854412079 Accuracy 0.827750027179718\n",
      "Iteration 8610 Training loss 0.0569692887365818 Validation loss 0.0629458948969841 Accuracy 0.8283750414848328\n",
      "Iteration 8620 Training loss 0.06120028346776962 Validation loss 0.0629521980881691 Accuracy 0.8283750414848328\n",
      "Iteration 8630 Training loss 0.057939350605010986 Validation loss 0.0629049688577652 Accuracy 0.827750027179718\n",
      "Iteration 8640 Training loss 0.06366004049777985 Validation loss 0.0628744587302208 Accuracy 0.8286250233650208\n",
      "Iteration 8650 Training loss 0.05588121712207794 Validation loss 0.0629473477602005 Accuracy 0.8270000219345093\n",
      "Iteration 8660 Training loss 0.06396863609552383 Validation loss 0.06284452974796295 Accuracy 0.8282500505447388\n",
      "Iteration 8670 Training loss 0.06972324848175049 Validation loss 0.06285765022039413 Accuracy 0.8283750414848328\n",
      "Iteration 8680 Training loss 0.06543035060167313 Validation loss 0.06281612068414688 Accuracy 0.8283750414848328\n",
      "Iteration 8690 Training loss 0.06104384735226631 Validation loss 0.06279504299163818 Accuracy 0.8283750414848328\n",
      "Iteration 8700 Training loss 0.05459421128034592 Validation loss 0.06278718262910843 Accuracy 0.827875018119812\n",
      "Iteration 8710 Training loss 0.061413366347551346 Validation loss 0.06280215084552765 Accuracy 0.8283750414848328\n",
      "Iteration 8720 Training loss 0.06371971219778061 Validation loss 0.06275486946105957 Accuracy 0.8287500143051147\n",
      "Iteration 8730 Training loss 0.06068936362862587 Validation loss 0.06275871396064758 Accuracy 0.8288750648498535\n",
      "Iteration 8740 Training loss 0.0623430535197258 Validation loss 0.06282125413417816 Accuracy 0.82750004529953\n",
      "Iteration 8750 Training loss 0.055310625582933426 Validation loss 0.0627286434173584 Accuracy 0.8286250233650208\n",
      "Iteration 8760 Training loss 0.07338967174291611 Validation loss 0.0628606528043747 Accuracy 0.8283750414848328\n",
      "Iteration 8770 Training loss 0.06634344160556793 Validation loss 0.06301600486040115 Accuracy 0.827875018119812\n",
      "Iteration 8780 Training loss 0.05508169159293175 Validation loss 0.06270846724510193 Accuracy 0.8286250233650208\n",
      "Iteration 8790 Training loss 0.06142221763730049 Validation loss 0.06275387853384018 Accuracy 0.8282500505447388\n",
      "Iteration 8800 Training loss 0.06059122830629349 Validation loss 0.06267163157463074 Accuracy 0.8288750648498535\n",
      "Iteration 8810 Training loss 0.059424757957458496 Validation loss 0.06289251148700714 Accuracy 0.827750027179718\n",
      "Iteration 8820 Training loss 0.060651443898677826 Validation loss 0.06264347583055496 Accuracy 0.8283750414848328\n",
      "Iteration 8830 Training loss 0.06353291869163513 Validation loss 0.06286728382110596 Accuracy 0.8281250596046448\n",
      "Iteration 8840 Training loss 0.06247570365667343 Validation loss 0.06263428926467896 Accuracy 0.8293750286102295\n",
      "Iteration 8850 Training loss 0.06022094935178757 Validation loss 0.06275512278079987 Accuracy 0.8288750648498535\n",
      "Iteration 8860 Training loss 0.074077308177948 Validation loss 0.06257563829421997 Accuracy 0.8285000324249268\n",
      "Iteration 8870 Training loss 0.06029944866895676 Validation loss 0.06258858740329742 Accuracy 0.8295000195503235\n",
      "Iteration 8880 Training loss 0.05660772696137428 Validation loss 0.06263484805822372 Accuracy 0.8297500610351562\n",
      "Iteration 8890 Training loss 0.05531863868236542 Validation loss 0.06252165883779526 Accuracy 0.8297500610351562\n",
      "Iteration 8900 Training loss 0.0636223554611206 Validation loss 0.0625220537185669 Accuracy 0.8295000195503235\n",
      "Iteration 8910 Training loss 0.05915731191635132 Validation loss 0.06251490116119385 Accuracy 0.8286250233650208\n",
      "Iteration 8920 Training loss 0.0700436532497406 Validation loss 0.06249425187706947 Accuracy 0.830625057220459\n",
      "Iteration 8930 Training loss 0.062014978379011154 Validation loss 0.06247835233807564 Accuracy 0.8301250338554382\n",
      "Iteration 8940 Training loss 0.06304941326379776 Validation loss 0.062459927052259445 Accuracy 0.8301250338554382\n",
      "Iteration 8950 Training loss 0.061479661613702774 Validation loss 0.06243712082505226 Accuracy 0.8300000429153442\n",
      "Iteration 8960 Training loss 0.04882488027215004 Validation loss 0.062436703592538834 Accuracy 0.8292500376701355\n",
      "Iteration 8970 Training loss 0.05826449394226074 Validation loss 0.06243481487035751 Accuracy 0.830625057220459\n",
      "Iteration 8980 Training loss 0.05678556486964226 Validation loss 0.062477219849824905 Accuracy 0.8295000195503235\n",
      "Iteration 8990 Training loss 0.06313569098711014 Validation loss 0.06256032735109329 Accuracy 0.8302500247955322\n",
      "Iteration 9000 Training loss 0.07253398001194 Validation loss 0.06250855326652527 Accuracy 0.8283750414848328\n",
      "Iteration 9010 Training loss 0.06410873681306839 Validation loss 0.06238720938563347 Accuracy 0.830875039100647\n",
      "Iteration 9020 Training loss 0.060641635209321976 Validation loss 0.06234614923596382 Accuracy 0.830500066280365\n",
      "Iteration 9030 Training loss 0.060803402215242386 Validation loss 0.06248365342617035 Accuracy 0.830500066280365\n",
      "Iteration 9040 Training loss 0.06263427436351776 Validation loss 0.062426015734672546 Accuracy 0.8298750519752502\n",
      "Iteration 9050 Training loss 0.06058548018336296 Validation loss 0.062305301427841187 Accuracy 0.830625057220459\n",
      "Iteration 9060 Training loss 0.059764061123132706 Validation loss 0.0622808113694191 Accuracy 0.8300000429153442\n",
      "Iteration 9070 Training loss 0.06757082045078278 Validation loss 0.06232551857829094 Accuracy 0.830500066280365\n",
      "Iteration 9080 Training loss 0.05957592651247978 Validation loss 0.062546007335186 Accuracy 0.8302500247955322\n",
      "Iteration 9090 Training loss 0.06231047585606575 Validation loss 0.062260277569293976 Accuracy 0.8293750286102295\n",
      "Iteration 9100 Training loss 0.0597890168428421 Validation loss 0.062275055795907974 Accuracy 0.8302500247955322\n",
      "Iteration 9110 Training loss 0.05433391407132149 Validation loss 0.06222475692629814 Accuracy 0.830875039100647\n",
      "Iteration 9120 Training loss 0.06483665108680725 Validation loss 0.06222722679376602 Accuracy 0.831000030040741\n",
      "Iteration 9130 Training loss 0.06041062995791435 Validation loss 0.06233394145965576 Accuracy 0.830875039100647\n",
      "Iteration 9140 Training loss 0.06913720816373825 Validation loss 0.0621831975877285 Accuracy 0.8303750157356262\n",
      "Iteration 9150 Training loss 0.06390277296304703 Validation loss 0.06226220354437828 Accuracy 0.8302500247955322\n",
      "Iteration 9160 Training loss 0.05781654641032219 Validation loss 0.06219852715730667 Accuracy 0.8301250338554382\n",
      "Iteration 9170 Training loss 0.059800803661346436 Validation loss 0.06217190995812416 Accuracy 0.8298750519752502\n",
      "Iteration 9180 Training loss 0.05999474227428436 Validation loss 0.06212804839015007 Accuracy 0.8303750157356262\n",
      "Iteration 9190 Training loss 0.0600135512650013 Validation loss 0.06211366876959801 Accuracy 0.8301250338554382\n",
      "Iteration 9200 Training loss 0.06132279708981514 Validation loss 0.06210160255432129 Accuracy 0.8302500247955322\n",
      "Iteration 9210 Training loss 0.061099063605070114 Validation loss 0.06218720227479935 Accuracy 0.8303750157356262\n",
      "Iteration 9220 Training loss 0.06217128783464432 Validation loss 0.062375590205192566 Accuracy 0.8301250338554382\n",
      "Iteration 9230 Training loss 0.0601874478161335 Validation loss 0.06219496950507164 Accuracy 0.831000030040741\n",
      "Iteration 9240 Training loss 0.06172236427664757 Validation loss 0.06210112199187279 Accuracy 0.8303750157356262\n",
      "Iteration 9250 Training loss 0.05777747556567192 Validation loss 0.06216387450695038 Accuracy 0.830625057220459\n",
      "Iteration 9260 Training loss 0.07030071318149567 Validation loss 0.06202363967895508 Accuracy 0.831000030040741\n",
      "Iteration 9270 Training loss 0.06262604147195816 Validation loss 0.06200169771909714 Accuracy 0.831000030040741\n",
      "Iteration 9280 Training loss 0.057538993656635284 Validation loss 0.062049586325883865 Accuracy 0.830625057220459\n",
      "Iteration 9290 Training loss 0.05816974490880966 Validation loss 0.06197896972298622 Accuracy 0.831125020980835\n",
      "Iteration 9300 Training loss 0.07465141266584396 Validation loss 0.061989642679691315 Accuracy 0.831250011920929\n",
      "Iteration 9310 Training loss 0.05701766535639763 Validation loss 0.06201820820569992 Accuracy 0.830750048160553\n",
      "Iteration 9320 Training loss 0.05747956037521362 Validation loss 0.06195361912250519 Accuracy 0.8313750624656677\n",
      "Iteration 9330 Training loss 0.057163432240486145 Validation loss 0.06214643269777298 Accuracy 0.8316250443458557\n",
      "Iteration 9340 Training loss 0.05546308308839798 Validation loss 0.06201031059026718 Accuracy 0.8316250443458557\n",
      "Iteration 9350 Training loss 0.0608053021132946 Validation loss 0.06191359832882881 Accuracy 0.8313750624656677\n",
      "Iteration 9360 Training loss 0.06652727723121643 Validation loss 0.061919569969177246 Accuracy 0.8313750624656677\n",
      "Iteration 9370 Training loss 0.058009564876556396 Validation loss 0.06191880255937576 Accuracy 0.8316250443458557\n",
      "Iteration 9380 Training loss 0.060131337493658066 Validation loss 0.06189605966210365 Accuracy 0.831000030040741\n",
      "Iteration 9390 Training loss 0.05797988176345825 Validation loss 0.061887226998806 Accuracy 0.831250011920929\n",
      "Iteration 9400 Training loss 0.055684298276901245 Validation loss 0.06185819208621979 Accuracy 0.831000030040741\n",
      "Iteration 9410 Training loss 0.05995018407702446 Validation loss 0.06185014545917511 Accuracy 0.8320000171661377\n",
      "Iteration 9420 Training loss 0.06564249098300934 Validation loss 0.06184917688369751 Accuracy 0.8316250443458557\n",
      "Iteration 9430 Training loss 0.0671771764755249 Validation loss 0.06185649335384369 Accuracy 0.831125020980835\n",
      "Iteration 9440 Training loss 0.0687437504529953 Validation loss 0.06181354448199272 Accuracy 0.8320000171661377\n",
      "Iteration 9450 Training loss 0.057040341198444366 Validation loss 0.06190013885498047 Accuracy 0.830875039100647\n",
      "Iteration 9460 Training loss 0.05315463989973068 Validation loss 0.061782438308000565 Accuracy 0.8320000171661377\n",
      "Iteration 9470 Training loss 0.06005672365427017 Validation loss 0.061760660260915756 Accuracy 0.8320000171661377\n",
      "Iteration 9480 Training loss 0.0637679249048233 Validation loss 0.06198808550834656 Accuracy 0.8317500352859497\n",
      "Iteration 9490 Training loss 0.06515093892812729 Validation loss 0.061747871339321136 Accuracy 0.8321250677108765\n",
      "Iteration 9500 Training loss 0.05848204344511032 Validation loss 0.06175821274518967 Accuracy 0.8320000171661377\n",
      "Iteration 9510 Training loss 0.06445484608411789 Validation loss 0.0617254413664341 Accuracy 0.8317500352859497\n",
      "Iteration 9520 Training loss 0.06492580473423004 Validation loss 0.061708372086286545 Accuracy 0.8317500352859497\n",
      "Iteration 9530 Training loss 0.05499007925391197 Validation loss 0.061696916818618774 Accuracy 0.8323750495910645\n",
      "Iteration 9540 Training loss 0.06681739538908005 Validation loss 0.061688750982284546 Accuracy 0.8317500352859497\n",
      "Iteration 9550 Training loss 0.05900375172495842 Validation loss 0.061901479959487915 Accuracy 0.8315000534057617\n",
      "Iteration 9560 Training loss 0.05866876617074013 Validation loss 0.06204397231340408 Accuracy 0.8313750624656677\n",
      "Iteration 9570 Training loss 0.06342672556638718 Validation loss 0.061638232320547104 Accuracy 0.8320000171661377\n",
      "Iteration 9580 Training loss 0.07109182327985764 Validation loss 0.06161094829440117 Accuracy 0.8327500224113464\n",
      "Iteration 9590 Training loss 0.06709613651037216 Validation loss 0.0616043396294117 Accuracy 0.8328750133514404\n",
      "Iteration 9600 Training loss 0.05865989252924919 Validation loss 0.061580635607242584 Accuracy 0.8327500224113464\n",
      "Iteration 9610 Training loss 0.05933103337883949 Validation loss 0.06162498891353607 Accuracy 0.8326250314712524\n",
      "Iteration 9620 Training loss 0.04635726660490036 Validation loss 0.061571210622787476 Accuracy 0.8322500586509705\n",
      "Iteration 9630 Training loss 0.06075919419527054 Validation loss 0.06160390377044678 Accuracy 0.8321250677108765\n",
      "Iteration 9640 Training loss 0.059752922505140305 Validation loss 0.061589889228343964 Accuracy 0.8318750262260437\n",
      "Iteration 9650 Training loss 0.07100678980350494 Validation loss 0.0615789070725441 Accuracy 0.8330000638961792\n",
      "Iteration 9660 Training loss 0.05616699159145355 Validation loss 0.061545755714178085 Accuracy 0.8326250314712524\n",
      "Iteration 9670 Training loss 0.05681871250271797 Validation loss 0.06152000278234482 Accuracy 0.8321250677108765\n",
      "Iteration 9680 Training loss 0.05907059088349342 Validation loss 0.06151900440454483 Accuracy 0.8322500586509705\n",
      "Iteration 9690 Training loss 0.05971716716885567 Validation loss 0.06168586015701294 Accuracy 0.8327500224113464\n",
      "Iteration 9700 Training loss 0.05804628133773804 Validation loss 0.06165319308638573 Accuracy 0.8325000405311584\n",
      "Iteration 9710 Training loss 0.054661162197589874 Validation loss 0.06145570054650307 Accuracy 0.8325000405311584\n",
      "Iteration 9720 Training loss 0.06323660910129547 Validation loss 0.06161251664161682 Accuracy 0.8323750495910645\n",
      "Iteration 9730 Training loss 0.06582736223936081 Validation loss 0.061432577669620514 Accuracy 0.8318750262260437\n",
      "Iteration 9740 Training loss 0.051516760140657425 Validation loss 0.06144721806049347 Accuracy 0.8325000405311584\n",
      "Iteration 9750 Training loss 0.0702643096446991 Validation loss 0.061439089477062225 Accuracy 0.8326250314712524\n",
      "Iteration 9760 Training loss 0.0590965636074543 Validation loss 0.06149857118725777 Accuracy 0.8313750624656677\n",
      "Iteration 9770 Training loss 0.06441880762577057 Validation loss 0.06143394857645035 Accuracy 0.8325000405311584\n",
      "Iteration 9780 Training loss 0.05804551765322685 Validation loss 0.06143629923462868 Accuracy 0.8328750133514404\n",
      "Iteration 9790 Training loss 0.057424016296863556 Validation loss 0.061430059373378754 Accuracy 0.8315000534057617\n",
      "Iteration 9800 Training loss 0.05983257666230202 Validation loss 0.06141671538352966 Accuracy 0.8322500586509705\n",
      "Iteration 9810 Training loss 0.061636026948690414 Validation loss 0.06147322803735733 Accuracy 0.8336250185966492\n",
      "Iteration 9820 Training loss 0.06487969309091568 Validation loss 0.06143219769001007 Accuracy 0.8325000405311584\n",
      "Iteration 9830 Training loss 0.06521597504615784 Validation loss 0.06143178418278694 Accuracy 0.8320000171661377\n",
      "Iteration 9840 Training loss 0.06052454933524132 Validation loss 0.06140538677573204 Accuracy 0.8337500691413879\n",
      "Iteration 9850 Training loss 0.06086629629135132 Validation loss 0.06139146536588669 Accuracy 0.8326250314712524\n",
      "Iteration 9860 Training loss 0.06165337562561035 Validation loss 0.06134482100605965 Accuracy 0.8337500691413879\n",
      "Iteration 9870 Training loss 0.06427982449531555 Validation loss 0.062084559351205826 Accuracy 0.830625057220459\n",
      "Iteration 9880 Training loss 0.05523554980754852 Validation loss 0.06128415837883949 Accuracy 0.8333750367164612\n",
      "Iteration 9890 Training loss 0.06337251514196396 Validation loss 0.06125548109412193 Accuracy 0.8335000276565552\n",
      "Iteration 9900 Training loss 0.06293679028749466 Validation loss 0.061254385858774185 Accuracy 0.8326250314712524\n",
      "Iteration 9910 Training loss 0.05446687713265419 Validation loss 0.06124520301818848 Accuracy 0.8343750238418579\n",
      "Iteration 9920 Training loss 0.05852711945772171 Validation loss 0.06124630942940712 Accuracy 0.8336250185966492\n",
      "Iteration 9930 Training loss 0.05779675021767616 Validation loss 0.06119069457054138 Accuracy 0.8343750238418579\n",
      "Iteration 9940 Training loss 0.06815511733293533 Validation loss 0.06119289994239807 Accuracy 0.8342500329017639\n",
      "Iteration 9950 Training loss 0.061049871146678925 Validation loss 0.0611807145178318 Accuracy 0.8341250419616699\n",
      "Iteration 9960 Training loss 0.06361058354377747 Validation loss 0.06131015345454216 Accuracy 0.8323750495910645\n",
      "Iteration 9970 Training loss 0.06943662464618683 Validation loss 0.06115225329995155 Accuracy 0.8338750600814819\n",
      "Iteration 9980 Training loss 0.06458202004432678 Validation loss 0.061169691383838654 Accuracy 0.8335000276565552\n",
      "Iteration 9990 Training loss 0.062369346618652344 Validation loss 0.061143215745687485 Accuracy 0.8335000276565552\n",
      "Iteration 10000 Training loss 0.061643704771995544 Validation loss 0.06122385337948799 Accuracy 0.8330000638961792\n",
      "Iteration 10010 Training loss 0.058652620762586594 Validation loss 0.0612952895462513 Accuracy 0.8323750495910645\n",
      "Iteration 10020 Training loss 0.056075166910886765 Validation loss 0.06113538518548012 Accuracy 0.8340000510215759\n",
      "Iteration 10030 Training loss 0.06136776879429817 Validation loss 0.06121569871902466 Accuracy 0.8330000638961792\n",
      "Iteration 10040 Training loss 0.05965045839548111 Validation loss 0.06110880523920059 Accuracy 0.8332500457763672\n",
      "Iteration 10050 Training loss 0.06535176932811737 Validation loss 0.06133938208222389 Accuracy 0.8328750133514404\n",
      "Iteration 10060 Training loss 0.056208934634923935 Validation loss 0.061106692999601364 Accuracy 0.8356250524520874\n",
      "Iteration 10070 Training loss 0.06431347876787186 Validation loss 0.061044562608003616 Accuracy 0.8346250653266907\n",
      "Iteration 10080 Training loss 0.06349523365497589 Validation loss 0.06105339899659157 Accuracy 0.8348750472068787\n",
      "Iteration 10090 Training loss 0.060988254845142365 Validation loss 0.06129058822989464 Accuracy 0.8331250548362732\n",
      "Iteration 10100 Training loss 0.05600178986787796 Validation loss 0.06118191406130791 Accuracy 0.8341250419616699\n",
      "Iteration 10110 Training loss 0.05896935239434242 Validation loss 0.06099987402558327 Accuracy 0.8342500329017639\n",
      "Iteration 10120 Training loss 0.06019657850265503 Validation loss 0.06100098416209221 Accuracy 0.8348750472068787\n",
      "Iteration 10130 Training loss 0.05652981624007225 Validation loss 0.06103162840008736 Accuracy 0.8343750238418579\n",
      "Iteration 10140 Training loss 0.05822256952524185 Validation loss 0.06098911538720131 Accuracy 0.8348750472068787\n",
      "Iteration 10150 Training loss 0.05656985193490982 Validation loss 0.060967978090047836 Accuracy 0.8346250653266907\n",
      "Iteration 10160 Training loss 0.06556518375873566 Validation loss 0.061100978404283524 Accuracy 0.8337500691413879\n",
      "Iteration 10170 Training loss 0.05884891375899315 Validation loss 0.06096462160348892 Accuracy 0.8342500329017639\n",
      "Iteration 10180 Training loss 0.05850359424948692 Validation loss 0.06114189326763153 Accuracy 0.8333750367164612\n",
      "Iteration 10190 Training loss 0.05812946334481239 Validation loss 0.06092504411935806 Accuracy 0.8340000510215759\n",
      "Iteration 10200 Training loss 0.05818253383040428 Validation loss 0.060913898050785065 Accuracy 0.8352500200271606\n",
      "Iteration 10210 Training loss 0.06108352541923523 Validation loss 0.060947444289922714 Accuracy 0.8347500562667847\n",
      "Iteration 10220 Training loss 0.0662495344877243 Validation loss 0.060878511518239975 Accuracy 0.8347500562667847\n",
      "Iteration 10230 Training loss 0.05902595445513725 Validation loss 0.06089315935969353 Accuracy 0.8348750472068787\n",
      "Iteration 10240 Training loss 0.062352489680051804 Validation loss 0.06087141111493111 Accuracy 0.8352500200271606\n",
      "Iteration 10250 Training loss 0.05497507005929947 Validation loss 0.06097578629851341 Accuracy 0.8342500329017639\n",
      "Iteration 10260 Training loss 0.05873136222362518 Validation loss 0.060843102633953094 Accuracy 0.8353750109672546\n",
      "Iteration 10270 Training loss 0.06453724950551987 Validation loss 0.06105458363890648 Accuracy 0.8330000638961792\n",
      "Iteration 10280 Training loss 0.06124653294682503 Validation loss 0.060817912220954895 Accuracy 0.8348750472068787\n",
      "Iteration 10290 Training loss 0.05601595342159271 Validation loss 0.06081249192357063 Accuracy 0.8352500200271606\n",
      "Iteration 10300 Training loss 0.05570853129029274 Validation loss 0.06080411747097969 Accuracy 0.8348750472068787\n",
      "Iteration 10310 Training loss 0.0604938343167305 Validation loss 0.0609799288213253 Accuracy 0.8338750600814819\n",
      "Iteration 10320 Training loss 0.058933552354574203 Validation loss 0.060748107731342316 Accuracy 0.8355000615119934\n",
      "Iteration 10330 Training loss 0.06566210836172104 Validation loss 0.06084209308028221 Accuracy 0.8341250419616699\n",
      "Iteration 10340 Training loss 0.05880890414118767 Validation loss 0.06073339283466339 Accuracy 0.8352500200271606\n",
      "Iteration 10350 Training loss 0.052128035575151443 Validation loss 0.06090154871344566 Accuracy 0.8341250419616699\n",
      "Iteration 10360 Training loss 0.057407986372709274 Validation loss 0.060707759112119675 Accuracy 0.8347500562667847\n",
      "Iteration 10370 Training loss 0.053926169872283936 Validation loss 0.06076979637145996 Accuracy 0.8343750238418579\n",
      "Iteration 10380 Training loss 0.061534710228443146 Validation loss 0.06070423871278763 Accuracy 0.8351250290870667\n",
      "Iteration 10390 Training loss 0.05219841003417969 Validation loss 0.06080794706940651 Accuracy 0.8346250653266907\n",
      "Iteration 10400 Training loss 0.05815291404724121 Validation loss 0.060695718973875046 Accuracy 0.8345000147819519\n",
      "Iteration 10410 Training loss 0.059124905616045 Validation loss 0.0606951005756855 Accuracy 0.8352500200271606\n",
      "Iteration 10420 Training loss 0.06105823069810867 Validation loss 0.060695260763168335 Accuracy 0.8342500329017639\n",
      "Iteration 10430 Training loss 0.05952335521578789 Validation loss 0.06070869415998459 Accuracy 0.8332500457763672\n",
      "Iteration 10440 Training loss 0.06851956248283386 Validation loss 0.06065927818417549 Accuracy 0.8343750238418579\n",
      "Iteration 10450 Training loss 0.05334091931581497 Validation loss 0.06066097691655159 Accuracy 0.8356250524520874\n",
      "Iteration 10460 Training loss 0.05567879602313042 Validation loss 0.060600221157073975 Accuracy 0.8351250290870667\n",
      "Iteration 10470 Training loss 0.06534167379140854 Validation loss 0.06059951335191727 Accuracy 0.8346250653266907\n",
      "Iteration 10480 Training loss 0.059125788509845734 Validation loss 0.06059729680418968 Accuracy 0.8351250290870667\n",
      "Iteration 10490 Training loss 0.06316080689430237 Validation loss 0.06061631813645363 Accuracy 0.8365000486373901\n",
      "Iteration 10500 Training loss 0.060882844030857086 Validation loss 0.060591623187065125 Accuracy 0.8366250395774841\n",
      "Iteration 10510 Training loss 0.05601062998175621 Validation loss 0.06056267023086548 Accuracy 0.8365000486373901\n",
      "Iteration 10520 Training loss 0.06282287836074829 Validation loss 0.060541871935129166 Accuracy 0.8350000381469727\n",
      "Iteration 10530 Training loss 0.0663829818367958 Validation loss 0.06057615205645561 Accuracy 0.8352500200271606\n",
      "Iteration 10540 Training loss 0.05995020642876625 Validation loss 0.06056603416800499 Accuracy 0.8348750472068787\n",
      "Iteration 10550 Training loss 0.058601949363946915 Validation loss 0.06072394177317619 Accuracy 0.8345000147819519\n",
      "Iteration 10560 Training loss 0.04603057727217674 Validation loss 0.06058184430003166 Accuracy 0.8337500691413879\n",
      "Iteration 10570 Training loss 0.055008336901664734 Validation loss 0.06052376702427864 Accuracy 0.8345000147819519\n",
      "Iteration 10580 Training loss 0.0570051483809948 Validation loss 0.060519758611917496 Accuracy 0.8350000381469727\n",
      "Iteration 10590 Training loss 0.05827292427420616 Validation loss 0.06046878919005394 Accuracy 0.8353750109672546\n",
      "Iteration 10600 Training loss 0.06094168499112129 Validation loss 0.06046684458851814 Accuracy 0.8360000252723694\n",
      "Iteration 10610 Training loss 0.05440815910696983 Validation loss 0.060453422367572784 Accuracy 0.8355000615119934\n",
      "Iteration 10620 Training loss 0.05755364149808884 Validation loss 0.060500793159008026 Accuracy 0.8357500433921814\n",
      "Iteration 10630 Training loss 0.06722918152809143 Validation loss 0.06083778664469719 Accuracy 0.8345000147819519\n",
      "Iteration 10640 Training loss 0.06641658395528793 Validation loss 0.060506902635097504 Accuracy 0.8362500667572021\n",
      "Iteration 10650 Training loss 0.06473975628614426 Validation loss 0.06042545288801193 Accuracy 0.8355000615119934\n",
      "Iteration 10660 Training loss 0.05072722211480141 Validation loss 0.06049840897321701 Accuracy 0.8363750576972961\n",
      "Iteration 10670 Training loss 0.06288579106330872 Validation loss 0.06045141816139221 Accuracy 0.8368750214576721\n",
      "Iteration 10680 Training loss 0.04885728284716606 Validation loss 0.0603933222591877 Accuracy 0.8367500305175781\n",
      "Iteration 10690 Training loss 0.0545099712908268 Validation loss 0.060415126383304596 Accuracy 0.8357500433921814\n",
      "Iteration 10700 Training loss 0.05546797811985016 Validation loss 0.06042959913611412 Accuracy 0.8350000381469727\n",
      "Iteration 10710 Training loss 0.06478247046470642 Validation loss 0.06051965802907944 Accuracy 0.8351250290870667\n",
      "Iteration 10720 Training loss 0.0549435093998909 Validation loss 0.06037333980202675 Accuracy 0.8366250395774841\n",
      "Iteration 10730 Training loss 0.0634787306189537 Validation loss 0.06045187637209892 Accuracy 0.8347500562667847\n",
      "Iteration 10740 Training loss 0.059151165187358856 Validation loss 0.06033630296587944 Accuracy 0.8356250524520874\n",
      "Iteration 10750 Training loss 0.06841747462749481 Validation loss 0.0603281706571579 Accuracy 0.8356250524520874\n",
      "Iteration 10760 Training loss 0.07167518138885498 Validation loss 0.06032685190439224 Accuracy 0.8353750109672546\n",
      "Iteration 10770 Training loss 0.05453085899353027 Validation loss 0.06033497303724289 Accuracy 0.8377500176429749\n",
      "Iteration 10780 Training loss 0.061920490115880966 Validation loss 0.06029285863041878 Accuracy 0.8356250524520874\n",
      "Iteration 10790 Training loss 0.04721067473292351 Validation loss 0.06035307049751282 Accuracy 0.8357500433921814\n",
      "Iteration 10800 Training loss 0.06375111639499664 Validation loss 0.060267023742198944 Accuracy 0.8363750576972961\n",
      "Iteration 10810 Training loss 0.05681310594081879 Validation loss 0.06029706820845604 Accuracy 0.8367500305175781\n",
      "Iteration 10820 Training loss 0.060860030353069305 Validation loss 0.06022702530026436 Accuracy 0.8353750109672546\n",
      "Iteration 10830 Training loss 0.061237651854753494 Validation loss 0.06021760776638985 Accuracy 0.8358750343322754\n",
      "Iteration 10840 Training loss 0.05757461115717888 Validation loss 0.060207974165678024 Accuracy 0.8351250290870667\n",
      "Iteration 10850 Training loss 0.05353357270359993 Validation loss 0.060312096029520035 Accuracy 0.8360000252723694\n",
      "Iteration 10860 Training loss 0.056788451969623566 Validation loss 0.060190487653017044 Accuracy 0.8375000357627869\n",
      "Iteration 10870 Training loss 0.06087913736701012 Validation loss 0.06024814769625664 Accuracy 0.8362500667572021\n",
      "Iteration 10880 Training loss 0.05948806554079056 Validation loss 0.06017450615763664 Accuracy 0.8358750343322754\n",
      "Iteration 10890 Training loss 0.05185327306389809 Validation loss 0.06015222147107124 Accuracy 0.8371250629425049\n",
      "Iteration 10900 Training loss 0.061734285205602646 Validation loss 0.06017325073480606 Accuracy 0.8377500176429749\n",
      "Iteration 10910 Training loss 0.056460823863744736 Validation loss 0.060376740992069244 Accuracy 0.8363750576972961\n",
      "Iteration 10920 Training loss 0.05267434939742088 Validation loss 0.060112178325653076 Accuracy 0.8382500410079956\n",
      "Iteration 10930 Training loss 0.04974210634827614 Validation loss 0.06009872630238533 Accuracy 0.8381250500679016\n",
      "Iteration 10940 Training loss 0.057614028453826904 Validation loss 0.060124147683382034 Accuracy 0.8382500410079956\n",
      "Iteration 10950 Training loss 0.06185255944728851 Validation loss 0.060076866298913956 Accuracy 0.8373750448226929\n",
      "Iteration 10960 Training loss 0.05325544625520706 Validation loss 0.06006874516606331 Accuracy 0.8375000357627869\n",
      "Iteration 10970 Training loss 0.06480344384908676 Validation loss 0.06026564911007881 Accuracy 0.8368750214576721\n",
      "Iteration 10980 Training loss 0.05287845432758331 Validation loss 0.06007431447505951 Accuracy 0.8363750576972961\n",
      "Iteration 10990 Training loss 0.05375208705663681 Validation loss 0.06018882617354393 Accuracy 0.8391250371932983\n",
      "Iteration 11000 Training loss 0.058369528502225876 Validation loss 0.060145314782857895 Accuracy 0.8368750214576721\n",
      "Iteration 11010 Training loss 0.056760337203741074 Validation loss 0.06009475514292717 Accuracy 0.8370000123977661\n",
      "Iteration 11020 Training loss 0.05929994583129883 Validation loss 0.06035660579800606 Accuracy 0.8363750576972961\n",
      "Iteration 11030 Training loss 0.05856533721089363 Validation loss 0.06003083288669586 Accuracy 0.8372500538825989\n",
      "Iteration 11040 Training loss 0.06963049620389938 Validation loss 0.06002048775553703 Accuracy 0.8373750448226929\n",
      "Iteration 11050 Training loss 0.06278582662343979 Validation loss 0.06000648811459541 Accuracy 0.8383750319480896\n",
      "Iteration 11060 Training loss 0.05433505401015282 Validation loss 0.05998857319355011 Accuracy 0.8376250267028809\n",
      "Iteration 11070 Training loss 0.06159108877182007 Validation loss 0.059990327805280685 Accuracy 0.8388750553131104\n",
      "Iteration 11080 Training loss 0.06852876394987106 Validation loss 0.060026880353689194 Accuracy 0.8383750319480896\n",
      "Iteration 11090 Training loss 0.06363645941019058 Validation loss 0.060005344450473785 Accuracy 0.8365000486373901\n",
      "Iteration 11100 Training loss 0.0641007348895073 Validation loss 0.0599675215780735 Accuracy 0.8365000486373901\n",
      "Iteration 11110 Training loss 0.051067184656858444 Validation loss 0.06014971807599068 Accuracy 0.8361250162124634\n",
      "Iteration 11120 Training loss 0.05433327704668045 Validation loss 0.05991186946630478 Accuracy 0.8385000228881836\n",
      "Iteration 11130 Training loss 0.061045531183481216 Validation loss 0.05996455252170563 Accuracy 0.8385000228881836\n",
      "Iteration 11140 Training loss 0.06023307517170906 Validation loss 0.059911374002695084 Accuracy 0.8373750448226929\n",
      "Iteration 11150 Training loss 0.06265823543071747 Validation loss 0.059893377125263214 Accuracy 0.8368750214576721\n",
      "Iteration 11160 Training loss 0.0567815862596035 Validation loss 0.06002827361226082 Accuracy 0.8362500667572021\n",
      "Iteration 11170 Training loss 0.05600695312023163 Validation loss 0.05994652211666107 Accuracy 0.8391250371932983\n",
      "Iteration 11180 Training loss 0.05386223644018173 Validation loss 0.060014285147190094 Accuracy 0.8358750343322754\n",
      "Iteration 11190 Training loss 0.058414820581674576 Validation loss 0.059981394559144974 Accuracy 0.8362500667572021\n",
      "Iteration 11200 Training loss 0.06071190536022186 Validation loss 0.059853266924619675 Accuracy 0.8386250138282776\n",
      "Iteration 11210 Training loss 0.05902830883860588 Validation loss 0.05981598049402237 Accuracy 0.8385000228881836\n",
      "Iteration 11220 Training loss 0.05764656886458397 Validation loss 0.05983014777302742 Accuracy 0.8397500514984131\n",
      "Iteration 11230 Training loss 0.04555526748299599 Validation loss 0.059815824031829834 Accuracy 0.8378750681877136\n",
      "Iteration 11240 Training loss 0.06388945132493973 Validation loss 0.05980739742517471 Accuracy 0.8382500410079956\n",
      "Iteration 11250 Training loss 0.061285827308893204 Validation loss 0.0598333403468132 Accuracy 0.8383750319480896\n",
      "Iteration 11260 Training loss 0.06343626230955124 Validation loss 0.059785082936286926 Accuracy 0.8381250500679016\n",
      "Iteration 11270 Training loss 0.05837155133485794 Validation loss 0.0597890205681324 Accuracy 0.8380000591278076\n",
      "Iteration 11280 Training loss 0.06387314200401306 Validation loss 0.059858813881874084 Accuracy 0.8370000123977661\n",
      "Iteration 11290 Training loss 0.059055015444755554 Validation loss 0.059823065996170044 Accuracy 0.8401250243186951\n",
      "Iteration 11300 Training loss 0.06987511366605759 Validation loss 0.05994885414838791 Accuracy 0.8377500176429749\n",
      "Iteration 11310 Training loss 0.058882780373096466 Validation loss 0.05973165109753609 Accuracy 0.8387500643730164\n",
      "Iteration 11320 Training loss 0.062393318861722946 Validation loss 0.05973416194319725 Accuracy 0.8385000228881836\n",
      "Iteration 11330 Training loss 0.06000332161784172 Validation loss 0.059961531311273575 Accuracy 0.8366250395774841\n",
      "Iteration 11340 Training loss 0.06211785227060318 Validation loss 0.059718403965234756 Accuracy 0.8386250138282776\n",
      "Iteration 11350 Training loss 0.05110052600502968 Validation loss 0.05969887971878052 Accuracy 0.8390000462532043\n",
      "Iteration 11360 Training loss 0.058828458189964294 Validation loss 0.05970189347863197 Accuracy 0.8397500514984131\n",
      "Iteration 11370 Training loss 0.05564024671912193 Validation loss 0.05978076532483101 Accuracy 0.8380000591278076\n",
      "Iteration 11380 Training loss 0.06311263144016266 Validation loss 0.05968224257230759 Accuracy 0.8390000462532043\n",
      "Iteration 11390 Training loss 0.07036788016557693 Validation loss 0.05973906069993973 Accuracy 0.8382500410079956\n",
      "Iteration 11400 Training loss 0.05842607468366623 Validation loss 0.05979928746819496 Accuracy 0.8381250500679016\n",
      "Iteration 11410 Training loss 0.06158897280693054 Validation loss 0.05963031202554703 Accuracy 0.8395000696182251\n",
      "Iteration 11420 Training loss 0.05912217125296593 Validation loss 0.059601668268442154 Accuracy 0.8400000333786011\n",
      "Iteration 11430 Training loss 0.05485108122229576 Validation loss 0.05963800475001335 Accuracy 0.8385000228881836\n",
      "Iteration 11440 Training loss 0.05709671974182129 Validation loss 0.05985979735851288 Accuracy 0.8381250500679016\n",
      "Iteration 11450 Training loss 0.05438348650932312 Validation loss 0.05965278297662735 Accuracy 0.8382500410079956\n",
      "Iteration 11460 Training loss 0.05236617848277092 Validation loss 0.05960124731063843 Accuracy 0.8390000462532043\n",
      "Iteration 11470 Training loss 0.053878918290138245 Validation loss 0.059703368693590164 Accuracy 0.8383750319480896\n",
      "Iteration 11480 Training loss 0.06745821982622147 Validation loss 0.0596066415309906 Accuracy 0.8386250138282776\n",
      "Iteration 11490 Training loss 0.06428265571594238 Validation loss 0.059607889503240585 Accuracy 0.8381250500679016\n",
      "Iteration 11500 Training loss 0.058691829442977905 Validation loss 0.0595984049141407 Accuracy 0.8375000357627869\n",
      "Iteration 11510 Training loss 0.05300190672278404 Validation loss 0.05961740389466286 Accuracy 0.8381250500679016\n",
      "Iteration 11520 Training loss 0.058357495814561844 Validation loss 0.05954832211136818 Accuracy 0.8378750681877136\n",
      "Iteration 11530 Training loss 0.060204897075891495 Validation loss 0.05951443687081337 Accuracy 0.8402500152587891\n",
      "Iteration 11540 Training loss 0.0645410344004631 Validation loss 0.05967294052243233 Accuracy 0.8376250267028809\n",
      "Iteration 11550 Training loss 0.05662795528769493 Validation loss 0.05969288945198059 Accuracy 0.8387500643730164\n",
      "Iteration 11560 Training loss 0.05784473195672035 Validation loss 0.05967729538679123 Accuracy 0.8383750319480896\n",
      "Iteration 11570 Training loss 0.05931558459997177 Validation loss 0.05966153368353844 Accuracy 0.8386250138282776\n",
      "Iteration 11580 Training loss 0.05617864429950714 Validation loss 0.05949879065155983 Accuracy 0.8390000462532043\n",
      "Iteration 11590 Training loss 0.06785035878419876 Validation loss 0.059457872062921524 Accuracy 0.8401250243186951\n",
      "Iteration 11600 Training loss 0.059974391013383865 Validation loss 0.059551090002059937 Accuracy 0.8385000228881836\n",
      "Iteration 11610 Training loss 0.0593731589615345 Validation loss 0.059429366141557693 Accuracy 0.8397500514984131\n",
      "Iteration 11620 Training loss 0.055637769401073456 Validation loss 0.05941310524940491 Accuracy 0.8408750295639038\n",
      "Iteration 11630 Training loss 0.058284785598516464 Validation loss 0.059411562979221344 Accuracy 0.8402500152587891\n",
      "Iteration 11640 Training loss 0.054420214146375656 Validation loss 0.05942271649837494 Accuracy 0.8397500514984131\n",
      "Iteration 11650 Training loss 0.0506298802793026 Validation loss 0.05943923443555832 Accuracy 0.8402500152587891\n",
      "Iteration 11660 Training loss 0.05993793532252312 Validation loss 0.05940695479512215 Accuracy 0.8395000696182251\n",
      "Iteration 11670 Training loss 0.05801244080066681 Validation loss 0.05939009413123131 Accuracy 0.8400000333786011\n",
      "Iteration 11680 Training loss 0.06071499362587929 Validation loss 0.0593838207423687 Accuracy 0.8402500152587891\n",
      "Iteration 11690 Training loss 0.054928481578826904 Validation loss 0.059401508420705795 Accuracy 0.8417500257492065\n",
      "Iteration 11700 Training loss 0.05783338472247124 Validation loss 0.05934590846300125 Accuracy 0.8402500152587891\n",
      "Iteration 11710 Training loss 0.06293056905269623 Validation loss 0.05942322686314583 Accuracy 0.8392500281333923\n",
      "Iteration 11720 Training loss 0.0548536479473114 Validation loss 0.05943769961595535 Accuracy 0.8393750190734863\n",
      "Iteration 11730 Training loss 0.070744089782238 Validation loss 0.05934032052755356 Accuracy 0.8418750166893005\n",
      "Iteration 11740 Training loss 0.05605945363640785 Validation loss 0.059692200273275375 Accuracy 0.8387500643730164\n",
      "Iteration 11750 Training loss 0.06179344654083252 Validation loss 0.05930172652006149 Accuracy 0.8403750658035278\n",
      "Iteration 11760 Training loss 0.0629647970199585 Validation loss 0.05935866758227348 Accuracy 0.8406250476837158\n",
      "Iteration 11770 Training loss 0.06253200024366379 Validation loss 0.05929557979106903 Accuracy 0.8406250476837158\n",
      "Iteration 11780 Training loss 0.06233308091759682 Validation loss 0.05933170020580292 Accuracy 0.8403750658035278\n",
      "Iteration 11790 Training loss 0.06556510180234909 Validation loss 0.059321023523807526 Accuracy 0.8406250476837158\n",
      "Iteration 11800 Training loss 0.059859514236450195 Validation loss 0.05926302447915077 Accuracy 0.8412500619888306\n",
      "Iteration 11810 Training loss 0.05641559511423111 Validation loss 0.059324439615011215 Accuracy 0.8408750295639038\n",
      "Iteration 11820 Training loss 0.06314655393362045 Validation loss 0.05926729738712311 Accuracy 0.8401250243186951\n",
      "Iteration 11830 Training loss 0.06223849207162857 Validation loss 0.059262972325086594 Accuracy 0.8403750658035278\n",
      "Iteration 11840 Training loss 0.056808970868587494 Validation loss 0.05925417318940163 Accuracy 0.8402500152587891\n",
      "Iteration 11850 Training loss 0.06101999059319496 Validation loss 0.05930561572313309 Accuracy 0.8391250371932983\n",
      "Iteration 11860 Training loss 0.060802459716796875 Validation loss 0.05923129618167877 Accuracy 0.8397500514984131\n",
      "Iteration 11870 Training loss 0.05158855393528938 Validation loss 0.05961492285132408 Accuracy 0.8396250605583191\n",
      "Iteration 11880 Training loss 0.051695480942726135 Validation loss 0.05957664176821709 Accuracy 0.8398750424385071\n",
      "Iteration 11890 Training loss 0.051883332431316376 Validation loss 0.059304285794496536 Accuracy 0.8397500514984131\n",
      "Iteration 11900 Training loss 0.054043129086494446 Validation loss 0.059248026460409164 Accuracy 0.8415000438690186\n",
      "Iteration 11910 Training loss 0.0577378086745739 Validation loss 0.05917147174477577 Accuracy 0.8418750166893005\n",
      "Iteration 11920 Training loss 0.060419850051403046 Validation loss 0.05918336659669876 Accuracy 0.8408750295639038\n",
      "Iteration 11930 Training loss 0.05796882510185242 Validation loss 0.0592324323952198 Accuracy 0.8401250243186951\n",
      "Iteration 11940 Training loss 0.05265704169869423 Validation loss 0.05913757532835007 Accuracy 0.8416250348091125\n",
      "Iteration 11950 Training loss 0.06272220611572266 Validation loss 0.05932608246803284 Accuracy 0.8402500152587891\n",
      "Iteration 11960 Training loss 0.05664999037981033 Validation loss 0.059134770184755325 Accuracy 0.8417500257492065\n",
      "Iteration 11970 Training loss 0.061749376356601715 Validation loss 0.05925510451197624 Accuracy 0.8410000205039978\n",
      "Iteration 11980 Training loss 0.056647151708602905 Validation loss 0.059193942695856094 Accuracy 0.8408750295639038\n",
      "Iteration 11990 Training loss 0.0576435923576355 Validation loss 0.0592113733291626 Accuracy 0.8410000205039978\n",
      "Iteration 12000 Training loss 0.054660335183143616 Validation loss 0.05930382385849953 Accuracy 0.8406250476837158\n",
      "Iteration 12010 Training loss 0.06293127685785294 Validation loss 0.05910452455282211 Accuracy 0.8410000205039978\n",
      "Iteration 12020 Training loss 0.05145423859357834 Validation loss 0.0590890534222126 Accuracy 0.8411250114440918\n",
      "Iteration 12030 Training loss 0.057985857129096985 Validation loss 0.059071846306324005 Accuracy 0.8417500257492065\n",
      "Iteration 12040 Training loss 0.05320802703499794 Validation loss 0.05981588363647461 Accuracy 0.8387500643730164\n",
      "Iteration 12050 Training loss 0.06279351562261581 Validation loss 0.059067387133836746 Accuracy 0.8422500491142273\n",
      "Iteration 12060 Training loss 0.05833040550351143 Validation loss 0.05912427976727486 Accuracy 0.8413750529289246\n",
      "Iteration 12070 Training loss 0.05900602042675018 Validation loss 0.059092357754707336 Accuracy 0.8415000438690186\n",
      "Iteration 12080 Training loss 0.0630417987704277 Validation loss 0.05910155922174454 Accuracy 0.8408750295639038\n",
      "Iteration 12090 Training loss 0.05773531273007393 Validation loss 0.05910491198301315 Accuracy 0.8413750529289246\n",
      "Iteration 12100 Training loss 0.051226403564214706 Validation loss 0.05906647816300392 Accuracy 0.8403750658035278\n",
      "Iteration 12110 Training loss 0.06076812744140625 Validation loss 0.05913807824254036 Accuracy 0.8415000438690186\n",
      "Iteration 12120 Training loss 0.05887981131672859 Validation loss 0.05907182767987251 Accuracy 0.8412500619888306\n",
      "Iteration 12130 Training loss 0.06197870150208473 Validation loss 0.05897746607661247 Accuracy 0.8423750400543213\n",
      "Iteration 12140 Training loss 0.05659956857562065 Validation loss 0.05908830091357231 Accuracy 0.8412500619888306\n",
      "Iteration 12150 Training loss 0.05826393514871597 Validation loss 0.059070881456136703 Accuracy 0.8417500257492065\n",
      "Iteration 12160 Training loss 0.06361042708158493 Validation loss 0.059011392295360565 Accuracy 0.8403750658035278\n",
      "Iteration 12170 Training loss 0.05611870810389519 Validation loss 0.05897551402449608 Accuracy 0.8418750166893005\n",
      "Iteration 12180 Training loss 0.0573270209133625 Validation loss 0.05906854569911957 Accuracy 0.8415000438690186\n",
      "Iteration 12190 Training loss 0.056516073644161224 Validation loss 0.058914944529533386 Accuracy 0.843250036239624\n",
      "Iteration 12200 Training loss 0.06810074299573898 Validation loss 0.058894503861665726 Accuracy 0.84312504529953\n",
      "Iteration 12210 Training loss 0.05387384444475174 Validation loss 0.058916639536619186 Accuracy 0.8417500257492065\n",
      "Iteration 12220 Training loss 0.05452428385615349 Validation loss 0.058939266949892044 Accuracy 0.8423750400543213\n",
      "Iteration 12230 Training loss 0.06493140012025833 Validation loss 0.05900095775723457 Accuracy 0.8416250348091125\n",
      "Iteration 12240 Training loss 0.05524349957704544 Validation loss 0.05884460732340813 Accuracy 0.8426250219345093\n",
      "Iteration 12250 Training loss 0.05683834105730057 Validation loss 0.05898968130350113 Accuracy 0.8413750529289246\n",
      "Iteration 12260 Training loss 0.05416315048933029 Validation loss 0.05891871079802513 Accuracy 0.8408750295639038\n",
      "Iteration 12270 Training loss 0.06632063537836075 Validation loss 0.058836184442043304 Accuracy 0.8427500128746033\n",
      "Iteration 12280 Training loss 0.06191160902380943 Validation loss 0.05885941535234451 Accuracy 0.8423750400543213\n",
      "Iteration 12290 Training loss 0.05867614597082138 Validation loss 0.05883662402629852 Accuracy 0.8423750400543213\n",
      "Iteration 12300 Training loss 0.06423872709274292 Validation loss 0.05884828791022301 Accuracy 0.8426250219345093\n",
      "Iteration 12310 Training loss 0.05277642980217934 Validation loss 0.05880998820066452 Accuracy 0.84312504529953\n",
      "Iteration 12320 Training loss 0.061959464102983475 Validation loss 0.05947897210717201 Accuracy 0.8391250371932983\n",
      "Iteration 12330 Training loss 0.05264019966125488 Validation loss 0.05881347879767418 Accuracy 0.8421250581741333\n",
      "Iteration 12340 Training loss 0.059841081500053406 Validation loss 0.058813340961933136 Accuracy 0.84312504529953\n",
      "Iteration 12350 Training loss 0.06190372258424759 Validation loss 0.05881837010383606 Accuracy 0.8421250581741333\n",
      "Iteration 12360 Training loss 0.06534549593925476 Validation loss 0.05884727090597153 Accuracy 0.8417500257492065\n",
      "Iteration 12370 Training loss 0.05983920022845268 Validation loss 0.05878531187772751 Accuracy 0.8418750166893005\n",
      "Iteration 12380 Training loss 0.057099368423223495 Validation loss 0.058805957436561584 Accuracy 0.8425000309944153\n",
      "Iteration 12390 Training loss 0.05400539189577103 Validation loss 0.059319622814655304 Accuracy 0.8403750658035278\n",
      "Iteration 12400 Training loss 0.05452634394168854 Validation loss 0.058870408684015274 Accuracy 0.8425000309944153\n",
      "Iteration 12410 Training loss 0.054678935557603836 Validation loss 0.05881287530064583 Accuracy 0.8425000309944153\n",
      "Iteration 12420 Training loss 0.05372314155101776 Validation loss 0.05878390744328499 Accuracy 0.8411250114440918\n",
      "Iteration 12430 Training loss 0.06112349033355713 Validation loss 0.05875670909881592 Accuracy 0.8408750295639038\n",
      "Iteration 12440 Training loss 0.05174078047275543 Validation loss 0.05874289199709892 Accuracy 0.8411250114440918\n",
      "Iteration 12450 Training loss 0.06354443728923798 Validation loss 0.05872802436351776 Accuracy 0.8423750400543213\n",
      "Iteration 12460 Training loss 0.05714985728263855 Validation loss 0.05873144790530205 Accuracy 0.8418750166893005\n",
      "Iteration 12470 Training loss 0.05971904471516609 Validation loss 0.05870119109749794 Accuracy 0.8437500596046448\n",
      "Iteration 12480 Training loss 0.06120891124010086 Validation loss 0.05871954560279846 Accuracy 0.8426250219345093\n",
      "Iteration 12490 Training loss 0.05285384878516197 Validation loss 0.05871303007006645 Accuracy 0.8426250219345093\n",
      "Iteration 12500 Training loss 0.06501125544309616 Validation loss 0.05869986116886139 Accuracy 0.843375027179718\n",
      "Iteration 12510 Training loss 0.04981962963938713 Validation loss 0.05871450901031494 Accuracy 0.8421250581741333\n",
      "Iteration 12520 Training loss 0.0639273077249527 Validation loss 0.05867641791701317 Accuracy 0.843375027179718\n",
      "Iteration 12530 Training loss 0.057947855442762375 Validation loss 0.05868217721581459 Accuracy 0.8437500596046448\n",
      "Iteration 12540 Training loss 0.060375671833753586 Validation loss 0.05868929624557495 Accuracy 0.843250036239624\n",
      "Iteration 12550 Training loss 0.05297744646668434 Validation loss 0.05874057114124298 Accuracy 0.842875063419342\n",
      "Iteration 12560 Training loss 0.05233514681458473 Validation loss 0.0586777999997139 Accuracy 0.843250036239624\n",
      "Iteration 12570 Training loss 0.060817621648311615 Validation loss 0.05868563801050186 Accuracy 0.8415000438690186\n",
      "Iteration 12580 Training loss 0.06293249130249023 Validation loss 0.058655306696891785 Accuracy 0.8421250581741333\n",
      "Iteration 12590 Training loss 0.056792326271533966 Validation loss 0.058726731687784195 Accuracy 0.8420000672340393\n",
      "Iteration 12600 Training loss 0.05822159722447395 Validation loss 0.0586218424141407 Accuracy 0.8420000672340393\n",
      "Iteration 12610 Training loss 0.054389819502830505 Validation loss 0.05862896144390106 Accuracy 0.8423750400543213\n",
      "Iteration 12620 Training loss 0.055246833711862564 Validation loss 0.05860046669840813 Accuracy 0.8441250324249268\n",
      "Iteration 12630 Training loss 0.054292354732751846 Validation loss 0.058952972292900085 Accuracy 0.8417500257492065\n",
      "Iteration 12640 Training loss 0.05075245350599289 Validation loss 0.05863356217741966 Accuracy 0.8422500491142273\n",
      "Iteration 12650 Training loss 0.06165735051035881 Validation loss 0.05858331173658371 Accuracy 0.84312504529953\n",
      "Iteration 12660 Training loss 0.06583625823259354 Validation loss 0.058553729206323624 Accuracy 0.8422500491142273\n",
      "Iteration 12670 Training loss 0.052788011729717255 Validation loss 0.05855758115649223 Accuracy 0.8426250219345093\n",
      "Iteration 12680 Training loss 0.057397861033678055 Validation loss 0.05857254937291145 Accuracy 0.8437500596046448\n",
      "Iteration 12690 Training loss 0.050326645374298096 Validation loss 0.058510832488536835 Accuracy 0.84312504529953\n",
      "Iteration 12700 Training loss 0.05380234122276306 Validation loss 0.05851226672530174 Accuracy 0.843500018119812\n",
      "Iteration 12710 Training loss 0.06400751322507858 Validation loss 0.058691974729299545 Accuracy 0.8425000309944153\n",
      "Iteration 12720 Training loss 0.0516887903213501 Validation loss 0.058476291596889496 Accuracy 0.84312504529953\n",
      "Iteration 12730 Training loss 0.06174628064036369 Validation loss 0.05844337120652199 Accuracy 0.8438750505447388\n",
      "Iteration 12740 Training loss 0.05140332132577896 Validation loss 0.05846097320318222 Accuracy 0.8436250686645508\n",
      "Iteration 12750 Training loss 0.05404318869113922 Validation loss 0.05843222141265869 Accuracy 0.8437500596046448\n",
      "Iteration 12760 Training loss 0.057802267372608185 Validation loss 0.058588866144418716 Accuracy 0.8427500128746033\n",
      "Iteration 12770 Training loss 0.06176012009382248 Validation loss 0.05842247232794762 Accuracy 0.8448750376701355\n",
      "Iteration 12780 Training loss 0.05482171103358269 Validation loss 0.058433715254068375 Accuracy 0.8446250557899475\n",
      "Iteration 12790 Training loss 0.05075351521372795 Validation loss 0.058420274406671524 Accuracy 0.8438750505447388\n",
      "Iteration 12800 Training loss 0.05870573967695236 Validation loss 0.058462101966142654 Accuracy 0.8437500596046448\n",
      "Iteration 12810 Training loss 0.05434601753950119 Validation loss 0.058397937566041946 Accuracy 0.8445000648498535\n",
      "Iteration 12820 Training loss 0.05911651626229286 Validation loss 0.05841876566410065 Accuracy 0.8443750143051147\n",
      "Iteration 12830 Training loss 0.05463787913322449 Validation loss 0.05845414847135544 Accuracy 0.842875063419342\n",
      "Iteration 12840 Training loss 0.04982564225792885 Validation loss 0.05844280868768692 Accuracy 0.8436250686645508\n",
      "Iteration 12850 Training loss 0.05691905319690704 Validation loss 0.058490149676799774 Accuracy 0.8436250686645508\n",
      "Iteration 12860 Training loss 0.06308627128601074 Validation loss 0.05868377164006233 Accuracy 0.84312504529953\n",
      "Iteration 12870 Training loss 0.06213495135307312 Validation loss 0.05886618420481682 Accuracy 0.8420000672340393\n",
      "Iteration 12880 Training loss 0.05737645551562309 Validation loss 0.058370865881443024 Accuracy 0.8436250686645508\n",
      "Iteration 12890 Training loss 0.06023643910884857 Validation loss 0.058672647923231125 Accuracy 0.842875063419342\n",
      "Iteration 12900 Training loss 0.0546838752925396 Validation loss 0.05854913219809532 Accuracy 0.8423750400543213\n",
      "Iteration 12910 Training loss 0.06183210760354996 Validation loss 0.05834213271737099 Accuracy 0.8438750505447388\n",
      "Iteration 12920 Training loss 0.05525066331028938 Validation loss 0.05834977328777313 Accuracy 0.8442500233650208\n",
      "Iteration 12930 Training loss 0.05807885527610779 Validation loss 0.05838114395737648 Accuracy 0.843500018119812\n",
      "Iteration 12940 Training loss 0.05826446786522865 Validation loss 0.058402273803949356 Accuracy 0.8442500233650208\n",
      "Iteration 12950 Training loss 0.0673767626285553 Validation loss 0.05827809125185013 Accuracy 0.8452500104904175\n",
      "Iteration 12960 Training loss 0.053074758499860764 Validation loss 0.058287136256694794 Accuracy 0.8440000414848328\n",
      "Iteration 12970 Training loss 0.05424336716532707 Validation loss 0.05828757584095001 Accuracy 0.8442500233650208\n",
      "Iteration 12980 Training loss 0.06347381323575974 Validation loss 0.058398425579071045 Accuracy 0.8440000414848328\n",
      "Iteration 12990 Training loss 0.06243934482336044 Validation loss 0.05828333646059036 Accuracy 0.8448750376701355\n",
      "Iteration 13000 Training loss 0.05686004459857941 Validation loss 0.05840613320469856 Accuracy 0.842875063419342\n",
      "Iteration 13010 Training loss 0.0587199330329895 Validation loss 0.05829516798257828 Accuracy 0.8448750376701355\n",
      "Iteration 13020 Training loss 0.057624444365501404 Validation loss 0.05829555541276932 Accuracy 0.8447500467300415\n",
      "Iteration 13030 Training loss 0.053140923380851746 Validation loss 0.05835741013288498 Accuracy 0.843500018119812\n",
      "Iteration 13040 Training loss 0.05092909187078476 Validation loss 0.058246638625860214 Accuracy 0.8448750376701355\n",
      "Iteration 13050 Training loss 0.06033682823181152 Validation loss 0.058299869298934937 Accuracy 0.8443750143051147\n",
      "Iteration 13060 Training loss 0.06879988312721252 Validation loss 0.058209337294101715 Accuracy 0.8440000414848328\n",
      "Iteration 13070 Training loss 0.05913669615983963 Validation loss 0.05819357931613922 Accuracy 0.8442500233650208\n",
      "Iteration 13080 Training loss 0.053538549691438675 Validation loss 0.05818483233451843 Accuracy 0.843250036239624\n",
      "Iteration 13090 Training loss 0.0544683113694191 Validation loss 0.058173585683107376 Accuracy 0.8442500233650208\n",
      "Iteration 13100 Training loss 0.06274347007274628 Validation loss 0.05817464366555214 Accuracy 0.8441250324249268\n",
      "Iteration 13110 Training loss 0.05648795887827873 Validation loss 0.05818569287657738 Accuracy 0.8442500233650208\n",
      "Iteration 13120 Training loss 0.056250713765621185 Validation loss 0.05830719321966171 Accuracy 0.843000054359436\n",
      "Iteration 13130 Training loss 0.06123415753245354 Validation loss 0.05819287523627281 Accuracy 0.8442500233650208\n",
      "Iteration 13140 Training loss 0.06021309643983841 Validation loss 0.05825043469667435 Accuracy 0.843000054359436\n",
      "Iteration 13150 Training loss 0.048743836581707 Validation loss 0.058262646198272705 Accuracy 0.8445000648498535\n",
      "Iteration 13160 Training loss 0.05852777510881424 Validation loss 0.058126144111156464 Accuracy 0.8442500233650208\n",
      "Iteration 13170 Training loss 0.05882059782743454 Validation loss 0.058129481971263885 Accuracy 0.843500018119812\n",
      "Iteration 13180 Training loss 0.06103420630097389 Validation loss 0.058111026883125305 Accuracy 0.8436250686645508\n",
      "Iteration 13190 Training loss 0.05898793414235115 Validation loss 0.05815569683909416 Accuracy 0.8438750505447388\n",
      "Iteration 13200 Training loss 0.0615302175283432 Validation loss 0.05817048251628876 Accuracy 0.8443750143051147\n",
      "Iteration 13210 Training loss 0.05550527572631836 Validation loss 0.05860484391450882 Accuracy 0.8423750400543213\n",
      "Iteration 13220 Training loss 0.056223392486572266 Validation loss 0.058161988854408264 Accuracy 0.8450000286102295\n",
      "Iteration 13230 Training loss 0.06698858737945557 Validation loss 0.05805889144539833 Accuracy 0.8456250429153442\n",
      "Iteration 13240 Training loss 0.0587894581258297 Validation loss 0.05806193873286247 Accuracy 0.8451250195503235\n",
      "Iteration 13250 Training loss 0.056541964411735535 Validation loss 0.05805679038167 Accuracy 0.8455000519752502\n",
      "Iteration 13260 Training loss 0.05113939940929413 Validation loss 0.05806327238678932 Accuracy 0.8450000286102295\n",
      "Iteration 13270 Training loss 0.061316099017858505 Validation loss 0.05802975967526436 Accuracy 0.8457500338554382\n",
      "Iteration 13280 Training loss 0.062058404088020325 Validation loss 0.058019112795591354 Accuracy 0.8448750376701355\n",
      "Iteration 13290 Training loss 0.06448782980442047 Validation loss 0.05815090611577034 Accuracy 0.8451250195503235\n",
      "Iteration 13300 Training loss 0.056448668241500854 Validation loss 0.05815541371703148 Accuracy 0.8445000648498535\n",
      "Iteration 13310 Training loss 0.07011017948389053 Validation loss 0.057994257658720016 Accuracy 0.8458750247955322\n",
      "Iteration 13320 Training loss 0.06023078411817551 Validation loss 0.05798742547631264 Accuracy 0.8452500104904175\n",
      "Iteration 13330 Training loss 0.0532526895403862 Validation loss 0.05801256746053696 Accuracy 0.846375048160553\n",
      "Iteration 13340 Training loss 0.05922871083021164 Validation loss 0.0582856684923172 Accuracy 0.8437500596046448\n",
      "Iteration 13350 Training loss 0.057277850806713104 Validation loss 0.057991378009319305 Accuracy 0.846375048160553\n",
      "Iteration 13360 Training loss 0.0700841024518013 Validation loss 0.057967912405729294 Accuracy 0.8451250195503235\n",
      "Iteration 13370 Training loss 0.05264822021126747 Validation loss 0.05800959840416908 Accuracy 0.8470000624656677\n",
      "Iteration 13380 Training loss 0.0521199069917202 Validation loss 0.057999175041913986 Accuracy 0.8476250171661377\n",
      "Iteration 13390 Training loss 0.0614243745803833 Validation loss 0.05804121494293213 Accuracy 0.846500039100647\n",
      "Iteration 13400 Training loss 0.059696536511182785 Validation loss 0.05790097266435623 Accuracy 0.8448750376701355\n",
      "Iteration 13410 Training loss 0.05688988417387009 Validation loss 0.058135997503995895 Accuracy 0.8445000648498535\n",
      "Iteration 13420 Training loss 0.06302334368228912 Validation loss 0.05788552388548851 Accuracy 0.8456250429153442\n",
      "Iteration 13430 Training loss 0.053768184036016464 Validation loss 0.05789712443947792 Accuracy 0.8452500104904175\n",
      "Iteration 13440 Training loss 0.05979907512664795 Validation loss 0.05809516832232475 Accuracy 0.8450000286102295\n",
      "Iteration 13450 Training loss 0.057068441063165665 Validation loss 0.05784318968653679 Accuracy 0.8448750376701355\n",
      "Iteration 13460 Training loss 0.05975469574332237 Validation loss 0.05786348506808281 Accuracy 0.8471250534057617\n",
      "Iteration 13470 Training loss 0.055989041924476624 Validation loss 0.05802244320511818 Accuracy 0.846125066280365\n",
      "Iteration 13480 Training loss 0.06399247795343399 Validation loss 0.05783698707818985 Accuracy 0.8450000286102295\n",
      "Iteration 13490 Training loss 0.06549648195505142 Validation loss 0.05784925818443298 Accuracy 0.8453750610351562\n",
      "Iteration 13500 Training loss 0.05868053436279297 Validation loss 0.0579078234732151 Accuracy 0.846125066280365\n",
      "Iteration 13510 Training loss 0.06374157965183258 Validation loss 0.05787232145667076 Accuracy 0.846125066280365\n",
      "Iteration 13520 Training loss 0.059746403247117996 Validation loss 0.057930175215005875 Accuracy 0.8470000624656677\n",
      "Iteration 13530 Training loss 0.06876780837774277 Validation loss 0.057901062071323395 Accuracy 0.8448750376701355\n",
      "Iteration 13540 Training loss 0.05276670679450035 Validation loss 0.05785476043820381 Accuracy 0.846125066280365\n",
      "Iteration 13550 Training loss 0.06262433528900146 Validation loss 0.057812776416540146 Accuracy 0.8460000157356262\n",
      "Iteration 13560 Training loss 0.0506092831492424 Validation loss 0.05780620127916336 Accuracy 0.8456250429153442\n",
      "Iteration 13570 Training loss 0.055864255875349045 Validation loss 0.05781731754541397 Accuracy 0.8457500338554382\n",
      "Iteration 13580 Training loss 0.0502786710858345 Validation loss 0.05780170112848282 Accuracy 0.8457500338554382\n",
      "Iteration 13590 Training loss 0.05802503973245621 Validation loss 0.05785596743226051 Accuracy 0.8456250429153442\n",
      "Iteration 13600 Training loss 0.06311824917793274 Validation loss 0.05850227549672127 Accuracy 0.8437500596046448\n",
      "Iteration 13610 Training loss 0.05086454376578331 Validation loss 0.05783892050385475 Accuracy 0.8460000157356262\n",
      "Iteration 13620 Training loss 0.053161878138780594 Validation loss 0.057768840342760086 Accuracy 0.846125066280365\n",
      "Iteration 13630 Training loss 0.055009875446558 Validation loss 0.057758670300245285 Accuracy 0.846125066280365\n",
      "Iteration 13640 Training loss 0.06127851456403732 Validation loss 0.05784233286976814 Accuracy 0.8460000157356262\n",
      "Iteration 13650 Training loss 0.0558026023209095 Validation loss 0.05798321217298508 Accuracy 0.8453750610351562\n",
      "Iteration 13660 Training loss 0.05008362606167793 Validation loss 0.05805572122335434 Accuracy 0.8456250429153442\n",
      "Iteration 13670 Training loss 0.06610589474439621 Validation loss 0.05772506445646286 Accuracy 0.8471250534057617\n",
      "Iteration 13680 Training loss 0.05137541517615318 Validation loss 0.05771743878722191 Accuracy 0.8481250405311584\n",
      "Iteration 13690 Training loss 0.05534294620156288 Validation loss 0.057710979133844376 Accuracy 0.8472500443458557\n",
      "Iteration 13700 Training loss 0.06366821378469467 Validation loss 0.05767546221613884 Accuracy 0.8457500338554382\n",
      "Iteration 13710 Training loss 0.0483214370906353 Validation loss 0.0576859787106514 Accuracy 0.846375048160553\n",
      "Iteration 13720 Training loss 0.0560297854244709 Validation loss 0.05770540609955788 Accuracy 0.846750020980835\n",
      "Iteration 13730 Training loss 0.049154751002788544 Validation loss 0.05799395218491554 Accuracy 0.8455000519752502\n",
      "Iteration 13740 Training loss 0.05831734463572502 Validation loss 0.0576942078769207 Accuracy 0.8470000624656677\n",
      "Iteration 13750 Training loss 0.05505950003862381 Validation loss 0.05767657980322838 Accuracy 0.8470000624656677\n",
      "Iteration 13760 Training loss 0.055355824530124664 Validation loss 0.05766401067376137 Accuracy 0.846250057220459\n",
      "Iteration 13770 Training loss 0.053236979991197586 Validation loss 0.057842448353767395 Accuracy 0.8458750247955322\n",
      "Iteration 13780 Training loss 0.05143672972917557 Validation loss 0.05767720192670822 Accuracy 0.8471250534057617\n",
      "Iteration 13790 Training loss 0.06303542852401733 Validation loss 0.057647354900836945 Accuracy 0.846250057220459\n",
      "Iteration 13800 Training loss 0.0639912411570549 Validation loss 0.057681821286678314 Accuracy 0.8472500443458557\n",
      "Iteration 13810 Training loss 0.056310221552848816 Validation loss 0.05771177262067795 Accuracy 0.8450000286102295\n",
      "Iteration 13820 Training loss 0.04965488240122795 Validation loss 0.057650964707136154 Accuracy 0.8477500677108765\n",
      "Iteration 13830 Training loss 0.0554688423871994 Validation loss 0.057772960513830185 Accuracy 0.846375048160553\n",
      "Iteration 13840 Training loss 0.06412137299776077 Validation loss 0.057693205773830414 Accuracy 0.8476250171661377\n",
      "Iteration 13850 Training loss 0.05145459994673729 Validation loss 0.05785255879163742 Accuracy 0.846375048160553\n",
      "Iteration 13860 Training loss 0.05502127483487129 Validation loss 0.05759444087743759 Accuracy 0.8455000519752502\n",
      "Iteration 13870 Training loss 0.05816440284252167 Validation loss 0.05762776359915733 Accuracy 0.8456250429153442\n",
      "Iteration 13880 Training loss 0.05950517579913139 Validation loss 0.05755623057484627 Accuracy 0.846125066280365\n",
      "Iteration 13890 Training loss 0.05637533217668533 Validation loss 0.057678475975990295 Accuracy 0.846875011920929\n",
      "Iteration 13900 Training loss 0.05881844460964203 Validation loss 0.05755995213985443 Accuracy 0.8456250429153442\n",
      "Iteration 13910 Training loss 0.05190311372280121 Validation loss 0.05756603181362152 Accuracy 0.8460000157356262\n",
      "Iteration 13920 Training loss 0.05226646363735199 Validation loss 0.05781600624322891 Accuracy 0.846500039100647\n",
      "Iteration 13930 Training loss 0.05126720294356346 Validation loss 0.057590335607528687 Accuracy 0.8470000624656677\n",
      "Iteration 13940 Training loss 0.053040388971567154 Validation loss 0.05755436047911644 Accuracy 0.8473750352859497\n",
      "Iteration 13950 Training loss 0.054317593574523926 Validation loss 0.057521045207977295 Accuracy 0.8457500338554382\n",
      "Iteration 13960 Training loss 0.04888003319501877 Validation loss 0.057603754103183746 Accuracy 0.8477500677108765\n",
      "Iteration 13970 Training loss 0.05606365576386452 Validation loss 0.057525698095560074 Accuracy 0.8472500443458557\n",
      "Iteration 13980 Training loss 0.061003487557172775 Validation loss 0.057498835027217865 Accuracy 0.8480000495910645\n",
      "Iteration 13990 Training loss 0.05835377424955368 Validation loss 0.05765678733587265 Accuracy 0.846500039100647\n",
      "Iteration 14000 Training loss 0.06077735871076584 Validation loss 0.05763007700443268 Accuracy 0.846500039100647\n",
      "Iteration 14010 Training loss 0.061044786125421524 Validation loss 0.057468995451927185 Accuracy 0.8471250534057617\n",
      "Iteration 14020 Training loss 0.058159392327070236 Validation loss 0.057469531893730164 Accuracy 0.8470000624656677\n",
      "Iteration 14030 Training loss 0.05228089168667793 Validation loss 0.057466812431812286 Accuracy 0.8470000624656677\n",
      "Iteration 14040 Training loss 0.059133775532245636 Validation loss 0.057457469403743744 Accuracy 0.8472500443458557\n",
      "Iteration 14050 Training loss 0.04864847660064697 Validation loss 0.05754098668694496 Accuracy 0.8471250534057617\n",
      "Iteration 14060 Training loss 0.053932350128889084 Validation loss 0.05741517245769501 Accuracy 0.846500039100647\n",
      "Iteration 14070 Training loss 0.05771635100245476 Validation loss 0.057415109127759933 Accuracy 0.846125066280365\n",
      "Iteration 14080 Training loss 0.06222159415483475 Validation loss 0.057637378573417664 Accuracy 0.846125066280365\n",
      "Iteration 14090 Training loss 0.05355292558670044 Validation loss 0.05745069310069084 Accuracy 0.8476250171661377\n",
      "Iteration 14100 Training loss 0.05954771861433983 Validation loss 0.05740859732031822 Accuracy 0.8470000624656677\n",
      "Iteration 14110 Training loss 0.05548526719212532 Validation loss 0.05740763992071152 Accuracy 0.8481250405311584\n",
      "Iteration 14120 Training loss 0.05840618908405304 Validation loss 0.057416364550590515 Accuracy 0.8471250534057617\n",
      "Iteration 14130 Training loss 0.06424427777528763 Validation loss 0.057406261563301086 Accuracy 0.8475000262260437\n",
      "Iteration 14140 Training loss 0.05621837452054024 Validation loss 0.05739757418632507 Accuracy 0.8472500443458557\n",
      "Iteration 14150 Training loss 0.06307999789714813 Validation loss 0.05738942325115204 Accuracy 0.8483750224113464\n",
      "Iteration 14160 Training loss 0.06194823980331421 Validation loss 0.057489026337862015 Accuracy 0.8475000262260437\n",
      "Iteration 14170 Training loss 0.06119373068213463 Validation loss 0.05740071088075638 Accuracy 0.8478750586509705\n",
      "Iteration 14180 Training loss 0.05641733482480049 Validation loss 0.0573992021381855 Accuracy 0.846625030040741\n",
      "Iteration 14190 Training loss 0.052630890160799026 Validation loss 0.05742907151579857 Accuracy 0.8477500677108765\n",
      "Iteration 14200 Training loss 0.06485334783792496 Validation loss 0.0574239082634449 Accuracy 0.8478750586509705\n",
      "Iteration 14210 Training loss 0.0499577522277832 Validation loss 0.057323817163705826 Accuracy 0.8475000262260437\n",
      "Iteration 14220 Training loss 0.054477594792842865 Validation loss 0.057482436299324036 Accuracy 0.846875011920929\n",
      "Iteration 14230 Training loss 0.0602039135992527 Validation loss 0.057695962488651276 Accuracy 0.846375048160553\n",
      "Iteration 14240 Training loss 0.0567605160176754 Validation loss 0.05737972632050514 Accuracy 0.8476250171661377\n",
      "Iteration 14250 Training loss 0.05905338004231453 Validation loss 0.05733712762594223 Accuracy 0.8471250534057617\n",
      "Iteration 14260 Training loss 0.06588367372751236 Validation loss 0.057283058762550354 Accuracy 0.8476250171661377\n",
      "Iteration 14270 Training loss 0.05252368003129959 Validation loss 0.057255037128925323 Accuracy 0.8481250405311584\n",
      "Iteration 14280 Training loss 0.060832131654024124 Validation loss 0.0573749765753746 Accuracy 0.8477500677108765\n",
      "Iteration 14290 Training loss 0.05622829496860504 Validation loss 0.057319026440382004 Accuracy 0.8482500314712524\n",
      "Iteration 14300 Training loss 0.04943879321217537 Validation loss 0.05768775939941406 Accuracy 0.846375048160553\n",
      "Iteration 14310 Training loss 0.05361798033118248 Validation loss 0.057606350630521774 Accuracy 0.846125066280365\n",
      "Iteration 14320 Training loss 0.058507613837718964 Validation loss 0.05726010724902153 Accuracy 0.8481250405311584\n",
      "Iteration 14330 Training loss 0.0573585070669651 Validation loss 0.05724390596151352 Accuracy 0.8477500677108765\n",
      "Iteration 14340 Training loss 0.05924595147371292 Validation loss 0.05735957622528076 Accuracy 0.8472500443458557\n",
      "Iteration 14350 Training loss 0.062302712351083755 Validation loss 0.05722098425030708 Accuracy 0.8483750224113464\n",
      "Iteration 14360 Training loss 0.0585189089179039 Validation loss 0.05728987976908684 Accuracy 0.8481250405311584\n",
      "Iteration 14370 Training loss 0.05127481371164322 Validation loss 0.057260408997535706 Accuracy 0.8478750586509705\n",
      "Iteration 14380 Training loss 0.05602960288524628 Validation loss 0.05723007768392563 Accuracy 0.8480000495910645\n",
      "Iteration 14390 Training loss 0.058769892901182175 Validation loss 0.0572098046541214 Accuracy 0.8488750457763672\n",
      "Iteration 14400 Training loss 0.06183589622378349 Validation loss 0.05729062855243683 Accuracy 0.8480000495910645\n",
      "Iteration 14410 Training loss 0.05988899990916252 Validation loss 0.05725519731640816 Accuracy 0.8490000367164612\n",
      "Iteration 14420 Training loss 0.05552653595805168 Validation loss 0.05718143656849861 Accuracy 0.8488750457763672\n",
      "Iteration 14430 Training loss 0.05419856309890747 Validation loss 0.057253215461969376 Accuracy 0.8487500548362732\n",
      "Iteration 14440 Training loss 0.05328434705734253 Validation loss 0.05717219039797783 Accuracy 0.8483750224113464\n",
      "Iteration 14450 Training loss 0.05995851755142212 Validation loss 0.057222988456487656 Accuracy 0.8483750224113464\n",
      "Iteration 14460 Training loss 0.054388854652643204 Validation loss 0.05720486491918564 Accuracy 0.8486250638961792\n",
      "Iteration 14470 Training loss 0.053926363587379456 Validation loss 0.057205941528081894 Accuracy 0.8488750457763672\n",
      "Iteration 14480 Training loss 0.056084975600242615 Validation loss 0.057603150606155396 Accuracy 0.8471250534057617\n",
      "Iteration 14490 Training loss 0.06110748276114464 Validation loss 0.05712103471159935 Accuracy 0.8492500185966492\n",
      "Iteration 14500 Training loss 0.061098143458366394 Validation loss 0.05730608478188515 Accuracy 0.8476250171661377\n",
      "Iteration 14510 Training loss 0.05579468980431557 Validation loss 0.057311952114105225 Accuracy 0.8475000262260437\n",
      "Iteration 14520 Training loss 0.06054425984621048 Validation loss 0.0572122223675251 Accuracy 0.8471250534057617\n",
      "Iteration 14530 Training loss 0.05747617036104202 Validation loss 0.05716119706630707 Accuracy 0.8485000133514404\n",
      "Iteration 14540 Training loss 0.04813329502940178 Validation loss 0.057103481143713 Accuracy 0.8491250276565552\n",
      "Iteration 14550 Training loss 0.05864565819501877 Validation loss 0.05709254741668701 Accuracy 0.8496250510215759\n",
      "Iteration 14560 Training loss 0.05731956660747528 Validation loss 0.05709515139460564 Accuracy 0.8487500548362732\n",
      "Iteration 14570 Training loss 0.0529182106256485 Validation loss 0.057155970484018326 Accuracy 0.8487500548362732\n",
      "Iteration 14580 Training loss 0.07215237617492676 Validation loss 0.0574851855635643 Accuracy 0.846125066280365\n",
      "Iteration 14590 Training loss 0.051724448800086975 Validation loss 0.057079415768384933 Accuracy 0.8485000133514404\n",
      "Iteration 14600 Training loss 0.06268635392189026 Validation loss 0.05708475038409233 Accuracy 0.8490000367164612\n",
      "Iteration 14610 Training loss 0.05277689918875694 Validation loss 0.05711711570620537 Accuracy 0.8485000133514404\n",
      "Iteration 14620 Training loss 0.06419043242931366 Validation loss 0.05711021646857262 Accuracy 0.8488750457763672\n",
      "Iteration 14630 Training loss 0.057626981288194656 Validation loss 0.05708855018019676 Accuracy 0.8486250638961792\n",
      "Iteration 14640 Training loss 0.05750510096549988 Validation loss 0.05715049430727959 Accuracy 0.8486250638961792\n",
      "Iteration 14650 Training loss 0.05513615533709526 Validation loss 0.05707141011953354 Accuracy 0.8478750586509705\n",
      "Iteration 14660 Training loss 0.05371912941336632 Validation loss 0.057093482464551926 Accuracy 0.8487500548362732\n",
      "Iteration 14670 Training loss 0.05802494287490845 Validation loss 0.05704960227012634 Accuracy 0.8490000367164612\n",
      "Iteration 14680 Training loss 0.060278329998254776 Validation loss 0.05702170729637146 Accuracy 0.8493750691413879\n",
      "Iteration 14690 Training loss 0.05675909295678139 Validation loss 0.0570223331451416 Accuracy 0.8495000600814819\n",
      "Iteration 14700 Training loss 0.05289310961961746 Validation loss 0.05703219771385193 Accuracy 0.8493750691413879\n",
      "Iteration 14710 Training loss 0.06832891702651978 Validation loss 0.05732933431863785 Accuracy 0.8470000624656677\n",
      "Iteration 14720 Training loss 0.05792330577969551 Validation loss 0.0570165291428566 Accuracy 0.8492500185966492\n",
      "Iteration 14730 Training loss 0.055289898067712784 Validation loss 0.0569966584444046 Accuracy 0.8492500185966492\n",
      "Iteration 14740 Training loss 0.06733982264995575 Validation loss 0.057147614657878876 Accuracy 0.8473750352859497\n",
      "Iteration 14750 Training loss 0.057320959866046906 Validation loss 0.05700288340449333 Accuracy 0.8492500185966492\n",
      "Iteration 14760 Training loss 0.052733954042196274 Validation loss 0.05698586627840996 Accuracy 0.8497500419616699\n",
      "Iteration 14770 Training loss 0.06167672947049141 Validation loss 0.0570426769554615 Accuracy 0.8486250638961792\n",
      "Iteration 14780 Training loss 0.05182238295674324 Validation loss 0.05698193982243538 Accuracy 0.8500000238418579\n",
      "Iteration 14790 Training loss 0.04967811331152916 Validation loss 0.056963931769132614 Accuracy 0.8506250381469727\n",
      "Iteration 14800 Training loss 0.063311867415905 Validation loss 0.05697900429368019 Accuracy 0.8496250510215759\n",
      "Iteration 14810 Training loss 0.06335554271936417 Validation loss 0.05711217597126961 Accuracy 0.8483750224113464\n",
      "Iteration 14820 Training loss 0.06211361661553383 Validation loss 0.05693250149488449 Accuracy 0.8498750329017639\n",
      "Iteration 14830 Training loss 0.050543367862701416 Validation loss 0.05696115270256996 Accuracy 0.8490000367164612\n",
      "Iteration 14840 Training loss 0.05933045223355293 Validation loss 0.05744984373450279 Accuracy 0.846750020980835\n",
      "Iteration 14850 Training loss 0.056300610303878784 Validation loss 0.056929104030132294 Accuracy 0.8506250381469727\n",
      "Iteration 14860 Training loss 0.06128522753715515 Validation loss 0.05717269703745842 Accuracy 0.8473750352859497\n",
      "Iteration 14870 Training loss 0.049153245985507965 Validation loss 0.05693308636546135 Accuracy 0.8496250510215759\n",
      "Iteration 14880 Training loss 0.053041234612464905 Validation loss 0.05691125616431236 Accuracy 0.8495000600814819\n",
      "Iteration 14890 Training loss 0.055537812411785126 Validation loss 0.05700300261378288 Accuracy 0.8492500185966492\n",
      "Iteration 14900 Training loss 0.05838390067219734 Validation loss 0.05690276622772217 Accuracy 0.8497500419616699\n",
      "Iteration 14910 Training loss 0.05723021551966667 Validation loss 0.05728738009929657 Accuracy 0.846750020980835\n",
      "Iteration 14920 Training loss 0.06153169646859169 Validation loss 0.056886255741119385 Accuracy 0.8497500419616699\n",
      "Iteration 14930 Training loss 0.05599317327141762 Validation loss 0.05691349506378174 Accuracy 0.8492500185966492\n",
      "Iteration 14940 Training loss 0.05348212271928787 Validation loss 0.05694034695625305 Accuracy 0.8495000600814819\n",
      "Iteration 14950 Training loss 0.0503363311290741 Validation loss 0.05690451338887215 Accuracy 0.8501250147819519\n",
      "Iteration 14960 Training loss 0.056245267391204834 Validation loss 0.056878700852394104 Accuracy 0.8508750200271606\n",
      "Iteration 14970 Training loss 0.05565037950873375 Validation loss 0.056934088468551636 Accuracy 0.8491250276565552\n",
      "Iteration 14980 Training loss 0.05755399540066719 Validation loss 0.05727877467870712 Accuracy 0.846750020980835\n",
      "Iteration 14990 Training loss 0.06202219799160957 Validation loss 0.05710689723491669 Accuracy 0.8481250405311584\n",
      "Iteration 15000 Training loss 0.05810694023966789 Validation loss 0.056851595640182495 Accuracy 0.8508750200271606\n",
      "Iteration 15010 Training loss 0.06031918525695801 Validation loss 0.056830745190382004 Accuracy 0.8508750200271606\n",
      "Iteration 15020 Training loss 0.05086357891559601 Validation loss 0.05688222870230675 Accuracy 0.8497500419616699\n",
      "Iteration 15030 Training loss 0.05835637450218201 Validation loss 0.05682393163442612 Accuracy 0.8505000472068787\n",
      "Iteration 15040 Training loss 0.052328143268823624 Validation loss 0.05697821453213692 Accuracy 0.8480000495910645\n",
      "Iteration 15050 Training loss 0.06061772629618645 Validation loss 0.056833431124687195 Accuracy 0.8507500290870667\n",
      "Iteration 15060 Training loss 0.05317375063896179 Validation loss 0.056907761842012405 Accuracy 0.8480000495910645\n",
      "Iteration 15070 Training loss 0.0590413473546505 Validation loss 0.05678891763091087 Accuracy 0.8515000343322754\n",
      "Iteration 15080 Training loss 0.05919872224330902 Validation loss 0.056760042905807495 Accuracy 0.8506250381469727\n",
      "Iteration 15090 Training loss 0.05612370744347572 Validation loss 0.056767258793115616 Accuracy 0.8502500653266907\n",
      "Iteration 15100 Training loss 0.0526127964258194 Validation loss 0.05673038586974144 Accuracy 0.8521250486373901\n",
      "Iteration 15110 Training loss 0.053870439529418945 Validation loss 0.05678068473935127 Accuracy 0.8488750457763672\n",
      "Iteration 15120 Training loss 0.05432213842868805 Validation loss 0.056859422475099564 Accuracy 0.8481250405311584\n",
      "Iteration 15130 Training loss 0.04693056643009186 Validation loss 0.0567716620862484 Accuracy 0.8500000238418579\n",
      "Iteration 15140 Training loss 0.05235231667757034 Validation loss 0.05671520531177521 Accuracy 0.8507500290870667\n",
      "Iteration 15150 Training loss 0.05187758430838585 Validation loss 0.056696366518735886 Accuracy 0.8506250381469727\n",
      "Iteration 15160 Training loss 0.041409868746995926 Validation loss 0.05668853223323822 Accuracy 0.8515000343322754\n",
      "Iteration 15170 Training loss 0.05714564025402069 Validation loss 0.056829821318387985 Accuracy 0.8490000367164612\n",
      "Iteration 15180 Training loss 0.060369424521923065 Validation loss 0.05666612833738327 Accuracy 0.8511250615119934\n",
      "Iteration 15190 Training loss 0.06349515169858932 Validation loss 0.05665060877799988 Accuracy 0.8516250252723694\n",
      "Iteration 15200 Training loss 0.05913815274834633 Validation loss 0.05667455494403839 Accuracy 0.8510000109672546\n",
      "Iteration 15210 Training loss 0.0601937472820282 Validation loss 0.056724268943071365 Accuracy 0.8497500419616699\n",
      "Iteration 15220 Training loss 0.06162368506193161 Validation loss 0.05677545815706253 Accuracy 0.8488750457763672\n",
      "Iteration 15230 Training loss 0.053943932056427 Validation loss 0.05740689858794212 Accuracy 0.8453750610351562\n",
      "Iteration 15240 Training loss 0.05483634024858475 Validation loss 0.056777290999889374 Accuracy 0.8498750329017639\n",
      "Iteration 15250 Training loss 0.04669015482068062 Validation loss 0.05684804916381836 Accuracy 0.8498750329017639\n",
      "Iteration 15260 Training loss 0.051240697503089905 Validation loss 0.05665970966219902 Accuracy 0.8517500162124634\n",
      "Iteration 15270 Training loss 0.05833104997873306 Validation loss 0.056926179677248 Accuracy 0.8491250276565552\n",
      "Iteration 15280 Training loss 0.053576644510030746 Validation loss 0.0567016564309597 Accuracy 0.8513750433921814\n",
      "Iteration 15290 Training loss 0.057853784412145615 Validation loss 0.056662265211343765 Accuracy 0.8512500524520874\n",
      "Iteration 15300 Training loss 0.05413365364074707 Validation loss 0.056589219719171524 Accuracy 0.8527500629425049\n",
      "Iteration 15310 Training loss 0.05607239529490471 Validation loss 0.05666666850447655 Accuracy 0.8501250147819519\n",
      "Iteration 15320 Training loss 0.053084611892700195 Validation loss 0.05656467005610466 Accuracy 0.8531250357627869\n",
      "Iteration 15330 Training loss 0.05978722497820854 Validation loss 0.056559037417173386 Accuracy 0.8526250123977661\n",
      "Iteration 15340 Training loss 0.05342685431241989 Validation loss 0.05655167996883392 Accuracy 0.8516250252723694\n",
      "Iteration 15350 Training loss 0.04895061254501343 Validation loss 0.05658228322863579 Accuracy 0.8502500653266907\n",
      "Iteration 15360 Training loss 0.062310151755809784 Validation loss 0.056542035192251205 Accuracy 0.8525000214576721\n",
      "Iteration 15370 Training loss 0.05418505519628525 Validation loss 0.056546494364738464 Accuracy 0.8513750433921814\n",
      "Iteration 15380 Training loss 0.05269230902194977 Validation loss 0.05668451264500618 Accuracy 0.8498750329017639\n",
      "Iteration 15390 Training loss 0.05502992868423462 Validation loss 0.05655103549361229 Accuracy 0.8520000576972961\n",
      "Iteration 15400 Training loss 0.05178726464509964 Validation loss 0.05658290162682533 Accuracy 0.8505000472068787\n",
      "Iteration 15410 Training loss 0.06025455892086029 Validation loss 0.05669450759887695 Accuracy 0.8498750329017639\n",
      "Iteration 15420 Training loss 0.04412991553544998 Validation loss 0.05656185373663902 Accuracy 0.8507500290870667\n",
      "Iteration 15430 Training loss 0.051082003861665726 Validation loss 0.056551042944192886 Accuracy 0.8517500162124634\n",
      "Iteration 15440 Training loss 0.046261295676231384 Validation loss 0.05661172419786453 Accuracy 0.8502500653266907\n",
      "Iteration 15450 Training loss 0.05451742187142372 Validation loss 0.05650106444954872 Accuracy 0.8525000214576721\n",
      "Iteration 15460 Training loss 0.05111785605549812 Validation loss 0.0564817450940609 Accuracy 0.8525000214576721\n",
      "Iteration 15470 Training loss 0.049245938658714294 Validation loss 0.056491222232580185 Accuracy 0.8515000343322754\n",
      "Iteration 15480 Training loss 0.05674992501735687 Validation loss 0.056474871933460236 Accuracy 0.8518750667572021\n",
      "Iteration 15490 Training loss 0.04917558655142784 Validation loss 0.05648002400994301 Accuracy 0.8521250486373901\n",
      "Iteration 15500 Training loss 0.05702344700694084 Validation loss 0.0564572811126709 Accuracy 0.8517500162124634\n",
      "Iteration 15510 Training loss 0.05333733558654785 Validation loss 0.056511636823415756 Accuracy 0.8511250615119934\n",
      "Iteration 15520 Training loss 0.054824844002723694 Validation loss 0.056454189121723175 Accuracy 0.8511250615119934\n",
      "Iteration 15530 Training loss 0.06538869440555573 Validation loss 0.056690096855163574 Accuracy 0.8506250381469727\n",
      "Iteration 15540 Training loss 0.05747606232762337 Validation loss 0.05647418648004532 Accuracy 0.8510000109672546\n",
      "Iteration 15550 Training loss 0.05816446244716644 Validation loss 0.05650961771607399 Accuracy 0.8507500290870667\n",
      "Iteration 15560 Training loss 0.04932991415262222 Validation loss 0.056461550295352936 Accuracy 0.8511250615119934\n",
      "Iteration 15570 Training loss 0.04996080324053764 Validation loss 0.0565343014895916 Accuracy 0.8495000600814819\n",
      "Iteration 15580 Training loss 0.06303846091032028 Validation loss 0.05647847428917885 Accuracy 0.8508750200271606\n",
      "Iteration 15590 Training loss 0.055342793464660645 Validation loss 0.056725986301898956 Accuracy 0.8492500185966492\n",
      "Iteration 15600 Training loss 0.049353837966918945 Validation loss 0.05665719881653786 Accuracy 0.8498750329017639\n",
      "Iteration 15610 Training loss 0.052260883152484894 Validation loss 0.056844860315322876 Accuracy 0.8492500185966492\n",
      "Iteration 15620 Training loss 0.06465581059455872 Validation loss 0.0565510131418705 Accuracy 0.8507500290870667\n",
      "Iteration 15630 Training loss 0.05341588705778122 Validation loss 0.05641510337591171 Accuracy 0.8511250615119934\n",
      "Iteration 15640 Training loss 0.05663025751709938 Validation loss 0.05653312802314758 Accuracy 0.8496250510215759\n",
      "Iteration 15650 Training loss 0.05113779753446579 Validation loss 0.05638786405324936 Accuracy 0.8517500162124634\n",
      "Iteration 15660 Training loss 0.055856846272945404 Validation loss 0.05640324577689171 Accuracy 0.8516250252723694\n",
      "Iteration 15670 Training loss 0.05734649673104286 Validation loss 0.05638619139790535 Accuracy 0.8517500162124634\n",
      "Iteration 15680 Training loss 0.05181765556335449 Validation loss 0.05667915567755699 Accuracy 0.8487500548362732\n",
      "Iteration 15690 Training loss 0.05782795697450638 Validation loss 0.05636502429842949 Accuracy 0.8530000448226929\n",
      "Iteration 15700 Training loss 0.05300593376159668 Validation loss 0.056459102779626846 Accuracy 0.8501250147819519\n",
      "Iteration 15710 Training loss 0.05336688831448555 Validation loss 0.05637332424521446 Accuracy 0.8523750305175781\n",
      "Iteration 15720 Training loss 0.05802193656563759 Validation loss 0.05635682865977287 Accuracy 0.8526250123977661\n",
      "Iteration 15730 Training loss 0.05440584197640419 Validation loss 0.056373462080955505 Accuracy 0.8520000576972961\n",
      "Iteration 15740 Training loss 0.06202490255236626 Validation loss 0.056386128067970276 Accuracy 0.8516250252723694\n",
      "Iteration 15750 Training loss 0.0500907264649868 Validation loss 0.05681460723280907 Accuracy 0.8485000133514404\n",
      "Iteration 15760 Training loss 0.05679130554199219 Validation loss 0.0563628114759922 Accuracy 0.8531250357627869\n",
      "Iteration 15770 Training loss 0.04904981330037117 Validation loss 0.056362736970186234 Accuracy 0.8506250381469727\n",
      "Iteration 15780 Training loss 0.06291746348142624 Validation loss 0.0565650649368763 Accuracy 0.8491250276565552\n",
      "Iteration 15790 Training loss 0.05370044708251953 Validation loss 0.056369632482528687 Accuracy 0.8517500162124634\n",
      "Iteration 15800 Training loss 0.05276806280016899 Validation loss 0.056469082832336426 Accuracy 0.8497500419616699\n",
      "Iteration 15810 Training loss 0.05056662857532501 Validation loss 0.05632631853222847 Accuracy 0.8516250252723694\n",
      "Iteration 15820 Training loss 0.054551463574171066 Validation loss 0.056345537304878235 Accuracy 0.8525000214576721\n",
      "Iteration 15830 Training loss 0.0538809560239315 Validation loss 0.056318849325180054 Accuracy 0.8528750538825989\n",
      "Iteration 15840 Training loss 0.06359194964170456 Validation loss 0.056362733244895935 Accuracy 0.8508750200271606\n",
      "Iteration 15850 Training loss 0.05973859503865242 Validation loss 0.05630286782979965 Accuracy 0.8521250486373901\n",
      "Iteration 15860 Training loss 0.05441899970173836 Validation loss 0.05636262893676758 Accuracy 0.8502500653266907\n",
      "Iteration 15870 Training loss 0.057233672589063644 Validation loss 0.056668445467948914 Accuracy 0.8486250638961792\n",
      "Iteration 15880 Training loss 0.0542779341340065 Validation loss 0.056222591549158096 Accuracy 0.8537500500679016\n",
      "Iteration 15890 Training loss 0.05233708396553993 Validation loss 0.05626029521226883 Accuracy 0.8520000576972961\n",
      "Iteration 15900 Training loss 0.055114924907684326 Validation loss 0.056225892156362534 Accuracy 0.8533750176429749\n",
      "Iteration 15910 Training loss 0.051819004118442535 Validation loss 0.05627445504069328 Accuracy 0.8511250615119934\n",
      "Iteration 15920 Training loss 0.058569762855768204 Validation loss 0.056258391588926315 Accuracy 0.8516250252723694\n",
      "Iteration 15930 Training loss 0.046569664031267166 Validation loss 0.056211650371551514 Accuracy 0.8525000214576721\n",
      "Iteration 15940 Training loss 0.0461277961730957 Validation loss 0.05618049576878548 Accuracy 0.8541250228881836\n",
      "Iteration 15950 Training loss 0.05118929594755173 Validation loss 0.05622788146138191 Accuracy 0.8525000214576721\n",
      "Iteration 15960 Training loss 0.05602974817156792 Validation loss 0.056187935173511505 Accuracy 0.8537500500679016\n",
      "Iteration 15970 Training loss 0.05870857089757919 Validation loss 0.05618753656744957 Accuracy 0.8536250591278076\n",
      "Iteration 15980 Training loss 0.05375251546502113 Validation loss 0.05676316097378731 Accuracy 0.8485000133514404\n",
      "Iteration 15990 Training loss 0.04835887625813484 Validation loss 0.056250255554914474 Accuracy 0.8513750433921814\n",
      "Iteration 16000 Training loss 0.05214225500822067 Validation loss 0.056248780339956284 Accuracy 0.8511250615119934\n",
      "Iteration 16010 Training loss 0.04967173561453819 Validation loss 0.05614418536424637 Accuracy 0.8536250591278076\n",
      "Iteration 16020 Training loss 0.04946610704064369 Validation loss 0.056229785084724426 Accuracy 0.8518750667572021\n",
      "Iteration 16030 Training loss 0.060709305107593536 Validation loss 0.05645168945193291 Accuracy 0.8498750329017639\n",
      "Iteration 16040 Training loss 0.06762981414794922 Validation loss 0.056279972195625305 Accuracy 0.8515000343322754\n",
      "Iteration 16050 Training loss 0.052556008100509644 Validation loss 0.05615769326686859 Accuracy 0.8535000681877136\n",
      "Iteration 16060 Training loss 0.04889510199427605 Validation loss 0.0562509261071682 Accuracy 0.8515000343322754\n",
      "Iteration 16070 Training loss 0.052267007529735565 Validation loss 0.05618860945105553 Accuracy 0.8516250252723694\n",
      "Iteration 16080 Training loss 0.05045692250132561 Validation loss 0.05615353211760521 Accuracy 0.8531250357627869\n",
      "Iteration 16090 Training loss 0.054498203098773956 Validation loss 0.05620463192462921 Accuracy 0.8515000343322754\n",
      "Iteration 16100 Training loss 0.05116927996277809 Validation loss 0.05625661835074425 Accuracy 0.8510000109672546\n",
      "Iteration 16110 Training loss 0.053924258798360825 Validation loss 0.056208569556474686 Accuracy 0.8516250252723694\n",
      "Iteration 16120 Training loss 0.05811182036995888 Validation loss 0.05612267553806305 Accuracy 0.8517500162124634\n",
      "Iteration 16130 Training loss 0.05235745757818222 Validation loss 0.05614330992102623 Accuracy 0.8512500524520874\n",
      "Iteration 16140 Training loss 0.05300459638237953 Validation loss 0.05612841993570328 Accuracy 0.8517500162124634\n",
      "Iteration 16150 Training loss 0.05311107635498047 Validation loss 0.05676387995481491 Accuracy 0.8480000495910645\n",
      "Iteration 16160 Training loss 0.0539029985666275 Validation loss 0.05617543309926987 Accuracy 0.8511250615119934\n",
      "Iteration 16170 Training loss 0.05697911977767944 Validation loss 0.05606972053647041 Accuracy 0.8518750667572021\n",
      "Iteration 16180 Training loss 0.0546734519302845 Validation loss 0.056084971874952316 Accuracy 0.8531250357627869\n",
      "Iteration 16190 Training loss 0.06023469939827919 Validation loss 0.05604809895157814 Accuracy 0.8532500267028809\n",
      "Iteration 16200 Training loss 0.048308875411748886 Validation loss 0.05606187880039215 Accuracy 0.8528750538825989\n",
      "Iteration 16210 Training loss 0.05921591445803642 Validation loss 0.056109603494405746 Accuracy 0.8526250123977661\n",
      "Iteration 16220 Training loss 0.05407604202628136 Validation loss 0.05604218319058418 Accuracy 0.8536250591278076\n",
      "Iteration 16230 Training loss 0.04904822260141373 Validation loss 0.056098852306604385 Accuracy 0.8522500395774841\n",
      "Iteration 16240 Training loss 0.059308987110853195 Validation loss 0.05627761781215668 Accuracy 0.8511250615119934\n",
      "Iteration 16250 Training loss 0.05541706830263138 Validation loss 0.05605296418070793 Accuracy 0.8533750176429749\n",
      "Iteration 16260 Training loss 0.05298725515604019 Validation loss 0.05607366934418678 Accuracy 0.8530000448226929\n",
      "Iteration 16270 Training loss 0.06454894691705704 Validation loss 0.056245967745780945 Accuracy 0.8496250510215759\n",
      "Iteration 16280 Training loss 0.05294905602931976 Validation loss 0.05601000413298607 Accuracy 0.8536250591278076\n",
      "Iteration 16290 Training loss 0.048510607331991196 Validation loss 0.05599364638328552 Accuracy 0.8532500267028809\n",
      "Iteration 16300 Training loss 0.05219988524913788 Validation loss 0.056105442345142365 Accuracy 0.8513750433921814\n",
      "Iteration 16310 Training loss 0.050322797149419785 Validation loss 0.055991534143686295 Accuracy 0.8533750176429749\n",
      "Iteration 16320 Training loss 0.05146535113453865 Validation loss 0.05608534440398216 Accuracy 0.8517500162124634\n",
      "Iteration 16330 Training loss 0.05724409967660904 Validation loss 0.055976495146751404 Accuracy 0.8538750410079956\n",
      "Iteration 16340 Training loss 0.06004810333251953 Validation loss 0.05599427968263626 Accuracy 0.8537500500679016\n",
      "Iteration 16350 Training loss 0.04861989989876747 Validation loss 0.056033551692962646 Accuracy 0.8526250123977661\n",
      "Iteration 16360 Training loss 0.04927133396267891 Validation loss 0.055975157767534256 Accuracy 0.8527500629425049\n",
      "Iteration 16370 Training loss 0.058516938239336014 Validation loss 0.056041110306978226 Accuracy 0.8520000576972961\n",
      "Iteration 16380 Training loss 0.05143364891409874 Validation loss 0.05593985319137573 Accuracy 0.8530000448226929\n",
      "Iteration 16390 Training loss 0.054960183799266815 Validation loss 0.05604451149702072 Accuracy 0.8518750667572021\n",
      "Iteration 16400 Training loss 0.05965317413210869 Validation loss 0.056002285331487656 Accuracy 0.8525000214576721\n",
      "Iteration 16410 Training loss 0.04765785112977028 Validation loss 0.05593664199113846 Accuracy 0.8528750538825989\n",
      "Iteration 16420 Training loss 0.05257507041096687 Validation loss 0.05611632019281387 Accuracy 0.8510000109672546\n",
      "Iteration 16430 Training loss 0.050087690353393555 Validation loss 0.05610604211688042 Accuracy 0.8512500524520874\n",
      "Iteration 16440 Training loss 0.052957210689783096 Validation loss 0.05592940375208855 Accuracy 0.8535000681877136\n",
      "Iteration 16450 Training loss 0.055414337664842606 Validation loss 0.05595950409770012 Accuracy 0.8518750667572021\n",
      "Iteration 16460 Training loss 0.044933538883924484 Validation loss 0.0559152327477932 Accuracy 0.8530000448226929\n",
      "Iteration 16470 Training loss 0.05609128996729851 Validation loss 0.056078217923641205 Accuracy 0.8503750562667847\n",
      "Iteration 16480 Training loss 0.05765479430556297 Validation loss 0.0559120737016201 Accuracy 0.8545000553131104\n",
      "Iteration 16490 Training loss 0.052871912717819214 Validation loss 0.05603303015232086 Accuracy 0.8523750305175781\n",
      "Iteration 16500 Training loss 0.05248846486210823 Validation loss 0.05626391991972923 Accuracy 0.8518750667572021\n",
      "Iteration 16510 Training loss 0.05656302347779274 Validation loss 0.05644436925649643 Accuracy 0.8498750329017639\n",
      "Iteration 16520 Training loss 0.05727056413888931 Validation loss 0.05586889013648033 Accuracy 0.8538750410079956\n",
      "Iteration 16530 Training loss 0.04455440491437912 Validation loss 0.05637520179152489 Accuracy 0.8496250510215759\n",
      "Iteration 16540 Training loss 0.05882192403078079 Validation loss 0.05585088953375816 Accuracy 0.8537500500679016\n",
      "Iteration 16550 Training loss 0.05996266379952431 Validation loss 0.05586248263716698 Accuracy 0.8540000319480896\n",
      "Iteration 16560 Training loss 0.05277664214372635 Validation loss 0.055901192128658295 Accuracy 0.8527500629425049\n",
      "Iteration 16570 Training loss 0.0633918046951294 Validation loss 0.05580637603998184 Accuracy 0.8537500500679016\n",
      "Iteration 16580 Training loss 0.06572610139846802 Validation loss 0.05619525909423828 Accuracy 0.8508750200271606\n",
      "Iteration 16590 Training loss 0.059724483639001846 Validation loss 0.05579296499490738 Accuracy 0.8542500138282776\n",
      "Iteration 16600 Training loss 0.05332329124212265 Validation loss 0.05582153797149658 Accuracy 0.8538750410079956\n",
      "Iteration 16610 Training loss 0.05765558406710625 Validation loss 0.05581643059849739 Accuracy 0.8543750643730164\n",
      "Iteration 16620 Training loss 0.05110381916165352 Validation loss 0.05577964335680008 Accuracy 0.8545000553131104\n",
      "Iteration 16630 Training loss 0.05882362276315689 Validation loss 0.055766962468624115 Accuracy 0.8541250228881836\n",
      "Iteration 16640 Training loss 0.06977315992116928 Validation loss 0.05577797815203667 Accuracy 0.8541250228881836\n",
      "Iteration 16650 Training loss 0.05732119455933571 Validation loss 0.055768635123968124 Accuracy 0.8545000553131104\n",
      "Iteration 16660 Training loss 0.04978556931018829 Validation loss 0.05613341182470322 Accuracy 0.8500000238418579\n",
      "Iteration 16670 Training loss 0.04838040843605995 Validation loss 0.05583500117063522 Accuracy 0.8535000681877136\n",
      "Iteration 16680 Training loss 0.053500037640333176 Validation loss 0.05577469244599342 Accuracy 0.8541250228881836\n",
      "Iteration 16690 Training loss 0.05466608703136444 Validation loss 0.055762879550457 Accuracy 0.8541250228881836\n",
      "Iteration 16700 Training loss 0.049244172871112823 Validation loss 0.05610235035419464 Accuracy 0.8493750691413879\n",
      "Iteration 16710 Training loss 0.05547718703746796 Validation loss 0.05577385798096657 Accuracy 0.8543750643730164\n",
      "Iteration 16720 Training loss 0.05852997303009033 Validation loss 0.05571579560637474 Accuracy 0.8543750643730164\n",
      "Iteration 16730 Training loss 0.05342923104763031 Validation loss 0.05591011419892311 Accuracy 0.8521250486373901\n",
      "Iteration 16740 Training loss 0.048819731920957565 Validation loss 0.05611962825059891 Accuracy 0.8511250615119934\n",
      "Iteration 16750 Training loss 0.05548249930143356 Validation loss 0.055697694420814514 Accuracy 0.8545000553131104\n",
      "Iteration 16760 Training loss 0.06116986274719238 Validation loss 0.055740609765052795 Accuracy 0.8538750410079956\n",
      "Iteration 16770 Training loss 0.04961534962058067 Validation loss 0.05580548942089081 Accuracy 0.8532500267028809\n",
      "Iteration 16780 Training loss 0.054253432899713516 Validation loss 0.05571252852678299 Accuracy 0.8547500371932983\n",
      "Iteration 16790 Training loss 0.05661344900727272 Validation loss 0.05594749376177788 Accuracy 0.8517500162124634\n",
      "Iteration 16800 Training loss 0.048558760434389114 Validation loss 0.0557672455906868 Accuracy 0.8536250591278076\n",
      "Iteration 16810 Training loss 0.05342831835150719 Validation loss 0.055678024888038635 Accuracy 0.8541250228881836\n",
      "Iteration 16820 Training loss 0.057984787970781326 Validation loss 0.055801909416913986 Accuracy 0.8521250486373901\n",
      "Iteration 16830 Training loss 0.06380724161863327 Validation loss 0.0556814968585968 Accuracy 0.8533750176429749\n",
      "Iteration 16840 Training loss 0.057683832943439484 Validation loss 0.05565302446484566 Accuracy 0.8540000319480896\n",
      "Iteration 16850 Training loss 0.05353529006242752 Validation loss 0.05563036724925041 Accuracy 0.8542500138282776\n",
      "Iteration 16860 Training loss 0.05320069193840027 Validation loss 0.05570080131292343 Accuracy 0.8540000319480896\n",
      "Iteration 16870 Training loss 0.04632050544023514 Validation loss 0.05561821162700653 Accuracy 0.8545000553131104\n",
      "Iteration 16880 Training loss 0.054050855338573456 Validation loss 0.05568506941199303 Accuracy 0.8540000319480896\n",
      "Iteration 16890 Training loss 0.0459870770573616 Validation loss 0.05565890297293663 Accuracy 0.8538750410079956\n",
      "Iteration 16900 Training loss 0.05161772668361664 Validation loss 0.05567988008260727 Accuracy 0.8538750410079956\n",
      "Iteration 16910 Training loss 0.04992513731122017 Validation loss 0.05558416247367859 Accuracy 0.8543750643730164\n",
      "Iteration 16920 Training loss 0.05681465566158295 Validation loss 0.05558108538389206 Accuracy 0.8551250696182251\n",
      "Iteration 16930 Training loss 0.04334833845496178 Validation loss 0.055574771016836166 Accuracy 0.8550000190734863\n",
      "Iteration 16940 Training loss 0.05299248546361923 Validation loss 0.055632904171943665 Accuracy 0.8527500629425049\n",
      "Iteration 16950 Training loss 0.05733388662338257 Validation loss 0.055634837597608566 Accuracy 0.8536250591278076\n",
      "Iteration 16960 Training loss 0.053123630583286285 Validation loss 0.05560992285609245 Accuracy 0.8541250228881836\n",
      "Iteration 16970 Training loss 0.0604349784553051 Validation loss 0.05554790794849396 Accuracy 0.8541250228881836\n",
      "Iteration 16980 Training loss 0.052649516612291336 Validation loss 0.05553973838686943 Accuracy 0.8543750643730164\n",
      "Iteration 16990 Training loss 0.05876313894987106 Validation loss 0.05556720867753029 Accuracy 0.8543750643730164\n",
      "Iteration 17000 Training loss 0.050004616379737854 Validation loss 0.055714964866638184 Accuracy 0.8521250486373901\n",
      "Iteration 17010 Training loss 0.05426415428519249 Validation loss 0.05556991696357727 Accuracy 0.8550000190734863\n",
      "Iteration 17020 Training loss 0.04870321601629257 Validation loss 0.05561244860291481 Accuracy 0.8541250228881836\n",
      "Iteration 17030 Training loss 0.04743476212024689 Validation loss 0.05563966929912567 Accuracy 0.8533750176429749\n",
      "Iteration 17040 Training loss 0.055708497762680054 Validation loss 0.05556269735097885 Accuracy 0.8547500371932983\n",
      "Iteration 17050 Training loss 0.05423632636666298 Validation loss 0.055538762360811234 Accuracy 0.8543750643730164\n",
      "Iteration 17060 Training loss 0.0586700513958931 Validation loss 0.055832233279943466 Accuracy 0.8508750200271606\n",
      "Iteration 17070 Training loss 0.05988175794482231 Validation loss 0.05551595613360405 Accuracy 0.8545000553131104\n",
      "Iteration 17080 Training loss 0.0546836256980896 Validation loss 0.055538471788167953 Accuracy 0.8548750281333923\n",
      "Iteration 17090 Training loss 0.06315702944993973 Validation loss 0.05552350729703903 Accuracy 0.8552500605583191\n",
      "Iteration 17100 Training loss 0.053154680877923965 Validation loss 0.05549550801515579 Accuracy 0.8546250462532043\n",
      "Iteration 17110 Training loss 0.04984678328037262 Validation loss 0.055530160665512085 Accuracy 0.8543750643730164\n",
      "Iteration 17120 Training loss 0.05589773878455162 Validation loss 0.055948853492736816 Accuracy 0.8522500395774841\n",
      "Iteration 17130 Training loss 0.04946041852235794 Validation loss 0.055507052689790726 Accuracy 0.8538750410079956\n",
      "Iteration 17140 Training loss 0.0610772967338562 Validation loss 0.055482737720012665 Accuracy 0.8537500500679016\n",
      "Iteration 17150 Training loss 0.05515842139720917 Validation loss 0.05552499741315842 Accuracy 0.8538750410079956\n",
      "Iteration 17160 Training loss 0.05909609794616699 Validation loss 0.05552660673856735 Accuracy 0.8541250228881836\n",
      "Iteration 17170 Training loss 0.05933982878923416 Validation loss 0.055619075894355774 Accuracy 0.8525000214576721\n",
      "Iteration 17180 Training loss 0.05885617434978485 Validation loss 0.05548561364412308 Accuracy 0.8542500138282776\n",
      "Iteration 17190 Training loss 0.05297017842531204 Validation loss 0.05562926456332207 Accuracy 0.8527500629425049\n",
      "Iteration 17200 Training loss 0.04719769209623337 Validation loss 0.055574946105480194 Accuracy 0.8533750176429749\n",
      "Iteration 17210 Training loss 0.057447269558906555 Validation loss 0.05543695390224457 Accuracy 0.8543750643730164\n",
      "Iteration 17220 Training loss 0.05091268569231033 Validation loss 0.05544394254684448 Accuracy 0.8542500138282776\n",
      "Iteration 17230 Training loss 0.058737415820360184 Validation loss 0.05541650205850601 Accuracy 0.8548750281333923\n",
      "Iteration 17240 Training loss 0.05046224594116211 Validation loss 0.05541887879371643 Accuracy 0.8541250228881836\n",
      "Iteration 17250 Training loss 0.051602672785520554 Validation loss 0.055428918451070786 Accuracy 0.8542500138282776\n",
      "Iteration 17260 Training loss 0.05661085620522499 Validation loss 0.05541227385401726 Accuracy 0.8548750281333923\n",
      "Iteration 17270 Training loss 0.05553159490227699 Validation loss 0.05542288348078728 Accuracy 0.8545000553131104\n",
      "Iteration 17280 Training loss 0.05536237731575966 Validation loss 0.055403932929039 Accuracy 0.8550000190734863\n",
      "Iteration 17290 Training loss 0.054655950516462326 Validation loss 0.05538869649171829 Accuracy 0.8547500371932983\n",
      "Iteration 17300 Training loss 0.05640431120991707 Validation loss 0.05540198087692261 Accuracy 0.8547500371932983\n",
      "Iteration 17310 Training loss 0.06194661185145378 Validation loss 0.05550337955355644 Accuracy 0.8528750538825989\n",
      "Iteration 17320 Training loss 0.04821933060884476 Validation loss 0.05544283241033554 Accuracy 0.8527500629425049\n",
      "Iteration 17330 Training loss 0.05885454639792442 Validation loss 0.05544167757034302 Accuracy 0.8540000319480896\n",
      "Iteration 17340 Training loss 0.043565452098846436 Validation loss 0.055348701775074005 Accuracy 0.8555000424385071\n",
      "Iteration 17350 Training loss 0.0493382029235363 Validation loss 0.05538806691765785 Accuracy 0.8546250462532043\n",
      "Iteration 17360 Training loss 0.05683385580778122 Validation loss 0.055567704141139984 Accuracy 0.8531250357627869\n",
      "Iteration 17370 Training loss 0.04911818355321884 Validation loss 0.05542684346437454 Accuracy 0.8537500500679016\n",
      "Iteration 17380 Training loss 0.05330396071076393 Validation loss 0.05563434585928917 Accuracy 0.8516250252723694\n",
      "Iteration 17390 Training loss 0.05315537378191948 Validation loss 0.05536169558763504 Accuracy 0.8545000553131104\n",
      "Iteration 17400 Training loss 0.0488680936396122 Validation loss 0.05535626411437988 Accuracy 0.8540000319480896\n",
      "Iteration 17410 Training loss 0.05905551835894585 Validation loss 0.055528368800878525 Accuracy 0.8523750305175781\n",
      "Iteration 17420 Training loss 0.05809426307678223 Validation loss 0.0554562471807003 Accuracy 0.8527500629425049\n",
      "Iteration 17430 Training loss 0.05673269182443619 Validation loss 0.05569871887564659 Accuracy 0.8516250252723694\n",
      "Iteration 17440 Training loss 0.05592725798487663 Validation loss 0.05529032275080681 Accuracy 0.8547500371932983\n",
      "Iteration 17450 Training loss 0.06310810148715973 Validation loss 0.055286165326833725 Accuracy 0.8553750514984131\n",
      "Iteration 17460 Training loss 0.05884392559528351 Validation loss 0.055415909737348557 Accuracy 0.8540000319480896\n",
      "Iteration 17470 Training loss 0.0546085387468338 Validation loss 0.055292438715696335 Accuracy 0.8561250567436218\n",
      "Iteration 17480 Training loss 0.06155714392662048 Validation loss 0.05533652380108833 Accuracy 0.8551250696182251\n",
      "Iteration 17490 Training loss 0.05900534614920616 Validation loss 0.05562527850270271 Accuracy 0.8512500524520874\n",
      "Iteration 17500 Training loss 0.05169422924518585 Validation loss 0.05528324097394943 Accuracy 0.8557500243186951\n",
      "Iteration 17510 Training loss 0.04853259027004242 Validation loss 0.055322062224149704 Accuracy 0.8553750514984131\n",
      "Iteration 17520 Training loss 0.05472668632864952 Validation loss 0.055270250886678696 Accuracy 0.8550000190734863\n",
      "Iteration 17530 Training loss 0.05705254524946213 Validation loss 0.05526584014296532 Accuracy 0.8556250333786011\n",
      "Iteration 17540 Training loss 0.056716859340667725 Validation loss 0.05527632683515549 Accuracy 0.8562500476837158\n",
      "Iteration 17550 Training loss 0.04834609106183052 Validation loss 0.05532281845808029 Accuracy 0.8542500138282776\n",
      "Iteration 17560 Training loss 0.04474997892975807 Validation loss 0.05527693033218384 Accuracy 0.8547500371932983\n",
      "Iteration 17570 Training loss 0.053882844746112823 Validation loss 0.05537218973040581 Accuracy 0.8532500267028809\n",
      "Iteration 17580 Training loss 0.05229899659752846 Validation loss 0.05525275692343712 Accuracy 0.8545000553131104\n",
      "Iteration 17590 Training loss 0.05907301604747772 Validation loss 0.05526388809084892 Accuracy 0.8548750281333923\n",
      "Iteration 17600 Training loss 0.049014363437891006 Validation loss 0.05527959764003754 Accuracy 0.8537500500679016\n",
      "Iteration 17610 Training loss 0.05027416720986366 Validation loss 0.05542130768299103 Accuracy 0.8528750538825989\n",
      "Iteration 17620 Training loss 0.05257626250386238 Validation loss 0.055285822600126266 Accuracy 0.8536250591278076\n",
      "Iteration 17630 Training loss 0.056361522525548935 Validation loss 0.055329445749521255 Accuracy 0.8537500500679016\n",
      "Iteration 17640 Training loss 0.05968104302883148 Validation loss 0.05544590204954147 Accuracy 0.8523750305175781\n",
      "Iteration 17650 Training loss 0.052561063319444656 Validation loss 0.055199358612298965 Accuracy 0.8546250462532043\n",
      "Iteration 17660 Training loss 0.05366681516170502 Validation loss 0.05565924197435379 Accuracy 0.8517500162124634\n",
      "Iteration 17670 Training loss 0.05859231948852539 Validation loss 0.05529889091849327 Accuracy 0.8532500267028809\n",
      "Iteration 17680 Training loss 0.044172219932079315 Validation loss 0.05524969846010208 Accuracy 0.8550000190734863\n",
      "Iteration 17690 Training loss 0.04344560578465462 Validation loss 0.055220142006874084 Accuracy 0.8538750410079956\n",
      "Iteration 17700 Training loss 0.055657919496297836 Validation loss 0.05538361147046089 Accuracy 0.8532500267028809\n",
      "Iteration 17710 Training loss 0.05749332904815674 Validation loss 0.05518519878387451 Accuracy 0.8546250462532043\n",
      "Iteration 17720 Training loss 0.05822042375802994 Validation loss 0.05518811196088791 Accuracy 0.8542500138282776\n",
      "Iteration 17730 Training loss 0.05212012305855751 Validation loss 0.05523372069001198 Accuracy 0.8532500267028809\n",
      "Iteration 17740 Training loss 0.04928514361381531 Validation loss 0.055168747901916504 Accuracy 0.8546250462532043\n",
      "Iteration 17750 Training loss 0.04333190992474556 Validation loss 0.05518689379096031 Accuracy 0.8538750410079956\n",
      "Iteration 17760 Training loss 0.05559141933917999 Validation loss 0.05517854169011116 Accuracy 0.8546250462532043\n",
      "Iteration 17770 Training loss 0.05117868259549141 Validation loss 0.055239517241716385 Accuracy 0.8533750176429749\n",
      "Iteration 17780 Training loss 0.051032159477472305 Validation loss 0.05518997460603714 Accuracy 0.8540000319480896\n",
      "Iteration 17790 Training loss 0.05427783355116844 Validation loss 0.055134519934654236 Accuracy 0.8551250696182251\n",
      "Iteration 17800 Training loss 0.04345834255218506 Validation loss 0.0555880144238472 Accuracy 0.8516250252723694\n",
      "Iteration 17810 Training loss 0.05608726292848587 Validation loss 0.05513886734843254 Accuracy 0.8545000553131104\n",
      "Iteration 17820 Training loss 0.05259397253394127 Validation loss 0.0554409883916378 Accuracy 0.8520000576972961\n",
      "Iteration 17830 Training loss 0.0544821061193943 Validation loss 0.05514201521873474 Accuracy 0.8553750514984131\n",
      "Iteration 17840 Training loss 0.0556262731552124 Validation loss 0.055117372423410416 Accuracy 0.8560000658035278\n",
      "Iteration 17850 Training loss 0.05192266404628754 Validation loss 0.05546318739652634 Accuracy 0.8530000448226929\n",
      "Iteration 17860 Training loss 0.05139211565256119 Validation loss 0.05536308512091637 Accuracy 0.8531250357627869\n",
      "Iteration 17870 Training loss 0.05442716181278229 Validation loss 0.055195439606904984 Accuracy 0.8536250591278076\n",
      "Iteration 17880 Training loss 0.056876812130212784 Validation loss 0.05507863312959671 Accuracy 0.8552500605583191\n",
      "Iteration 17890 Training loss 0.054629307240247726 Validation loss 0.055094435811042786 Accuracy 0.8561250567436218\n",
      "Iteration 17900 Training loss 0.054916009306907654 Validation loss 0.05507584661245346 Accuracy 0.8560000658035278\n",
      "Iteration 17910 Training loss 0.05320396646857262 Validation loss 0.05531534552574158 Accuracy 0.8535000681877136\n",
      "Iteration 17920 Training loss 0.05619223043322563 Validation loss 0.0551149807870388 Accuracy 0.8551250696182251\n",
      "Iteration 17930 Training loss 0.04667448252439499 Validation loss 0.0550728477537632 Accuracy 0.8566250205039978\n",
      "Iteration 17940 Training loss 0.04916653409600258 Validation loss 0.05509514361619949 Accuracy 0.8555000424385071\n",
      "Iteration 17950 Training loss 0.05403786525130272 Validation loss 0.055074021220207214 Accuracy 0.8550000190734863\n",
      "Iteration 17960 Training loss 0.05276230722665787 Validation loss 0.055423084646463394 Accuracy 0.8535000681877136\n",
      "Iteration 17970 Training loss 0.053640563040971756 Validation loss 0.05510224401950836 Accuracy 0.8542500138282776\n",
      "Iteration 17980 Training loss 0.059545304626226425 Validation loss 0.05515160411596298 Accuracy 0.8530000448226929\n",
      "Iteration 17990 Training loss 0.04384644329547882 Validation loss 0.05506076291203499 Accuracy 0.8548750281333923\n",
      "Iteration 18000 Training loss 0.050521593540906906 Validation loss 0.05505188927054405 Accuracy 0.8553750514984131\n",
      "Iteration 18010 Training loss 0.055049359798431396 Validation loss 0.05501273646950722 Accuracy 0.8548750281333923\n",
      "Iteration 18020 Training loss 0.05064908042550087 Validation loss 0.055304765701293945 Accuracy 0.8517500162124634\n",
      "Iteration 18030 Training loss 0.0544544979929924 Validation loss 0.0550566092133522 Accuracy 0.8565000295639038\n",
      "Iteration 18040 Training loss 0.05126186087727547 Validation loss 0.05501072108745575 Accuracy 0.8562500476837158\n",
      "Iteration 18050 Training loss 0.04822699353098869 Validation loss 0.05499701201915741 Accuracy 0.8553750514984131\n",
      "Iteration 18060 Training loss 0.059048864990472794 Validation loss 0.05500102415680885 Accuracy 0.8563750386238098\n",
      "Iteration 18070 Training loss 0.05607728660106659 Validation loss 0.05500207468867302 Accuracy 0.8562500476837158\n",
      "Iteration 18080 Training loss 0.05529998615384102 Validation loss 0.055045682936906815 Accuracy 0.8552500605583191\n",
      "Iteration 18090 Training loss 0.05892748758196831 Validation loss 0.055220723152160645 Accuracy 0.8528750538825989\n",
      "Iteration 18100 Training loss 0.06300762295722961 Validation loss 0.054993655532598495 Accuracy 0.8548750281333923\n",
      "Iteration 18110 Training loss 0.05370130389928818 Validation loss 0.055187828838825226 Accuracy 0.8527500629425049\n",
      "Iteration 18120 Training loss 0.049563661217689514 Validation loss 0.05493674799799919 Accuracy 0.8565000295639038\n",
      "Iteration 18130 Training loss 0.061448171734809875 Validation loss 0.05496245250105858 Accuracy 0.8553750514984131\n",
      "Iteration 18140 Training loss 0.05786534771323204 Validation loss 0.055006857961416245 Accuracy 0.8553750514984131\n",
      "Iteration 18150 Training loss 0.05593622103333473 Validation loss 0.05502846837043762 Accuracy 0.8543750643730164\n",
      "Iteration 18160 Training loss 0.05207715928554535 Validation loss 0.05505741387605667 Accuracy 0.8538750410079956\n",
      "Iteration 18170 Training loss 0.05627094954252243 Validation loss 0.05504447966814041 Accuracy 0.8547500371932983\n",
      "Iteration 18180 Training loss 0.05262397602200508 Validation loss 0.05524508282542229 Accuracy 0.8527500629425049\n",
      "Iteration 18190 Training loss 0.04874080419540405 Validation loss 0.055119216442108154 Accuracy 0.8527500629425049\n",
      "Iteration 18200 Training loss 0.04816234111785889 Validation loss 0.054953716695308685 Accuracy 0.8556250333786011\n",
      "Iteration 18210 Training loss 0.05527739226818085 Validation loss 0.05489934980869293 Accuracy 0.8561250567436218\n",
      "Iteration 18220 Training loss 0.04994581267237663 Validation loss 0.05499432235956192 Accuracy 0.8552500605583191\n",
      "Iteration 18230 Training loss 0.04898778721690178 Validation loss 0.0549527071416378 Accuracy 0.8556250333786011\n",
      "Iteration 18240 Training loss 0.053558871150016785 Validation loss 0.05503333359956741 Accuracy 0.8552500605583191\n",
      "Iteration 18250 Training loss 0.046290624886751175 Validation loss 0.05498133599758148 Accuracy 0.8551250696182251\n",
      "Iteration 18260 Training loss 0.050139836966991425 Validation loss 0.05492567643523216 Accuracy 0.8566250205039978\n",
      "Iteration 18270 Training loss 0.048213012516498566 Validation loss 0.05488638952374458 Accuracy 0.8562500476837158\n",
      "Iteration 18280 Training loss 0.05493983253836632 Validation loss 0.05505108833312988 Accuracy 0.8546250462532043\n",
      "Iteration 18290 Training loss 0.059656210243701935 Validation loss 0.05491382256150246 Accuracy 0.8561250567436218\n",
      "Iteration 18300 Training loss 0.049990177154541016 Validation loss 0.05490851029753685 Accuracy 0.8557500243186951\n",
      "Iteration 18310 Training loss 0.05821283534169197 Validation loss 0.05487661063671112 Accuracy 0.8552500605583191\n",
      "Iteration 18320 Training loss 0.05862832069396973 Validation loss 0.05488194525241852 Accuracy 0.8558750152587891\n",
      "Iteration 18330 Training loss 0.0584695041179657 Validation loss 0.05505330488085747 Accuracy 0.8555000424385071\n",
      "Iteration 18340 Training loss 0.04941609129309654 Validation loss 0.05487874150276184 Accuracy 0.8560000658035278\n",
      "Iteration 18350 Training loss 0.05007654055953026 Validation loss 0.054867006838321686 Accuracy 0.8563750386238098\n",
      "Iteration 18360 Training loss 0.0522853285074234 Validation loss 0.05510291829705238 Accuracy 0.8532500267028809\n",
      "Iteration 18370 Training loss 0.05834057554602623 Validation loss 0.05485772341489792 Accuracy 0.8557500243186951\n",
      "Iteration 18380 Training loss 0.056689534336328506 Validation loss 0.05494685471057892 Accuracy 0.8545000553131104\n",
      "Iteration 18390 Training loss 0.04824502021074295 Validation loss 0.054863475263118744 Accuracy 0.8551250696182251\n",
      "Iteration 18400 Training loss 0.04812314361333847 Validation loss 0.054807670414447784 Accuracy 0.8553750514984131\n",
      "Iteration 18410 Training loss 0.05205968767404556 Validation loss 0.054861605167388916 Accuracy 0.8550000190734863\n",
      "Iteration 18420 Training loss 0.04975137487053871 Validation loss 0.05479168891906738 Accuracy 0.8560000658035278\n",
      "Iteration 18430 Training loss 0.057046279311180115 Validation loss 0.054805878549814224 Accuracy 0.8556250333786011\n",
      "Iteration 18440 Training loss 0.04958946257829666 Validation loss 0.054809603840112686 Accuracy 0.8556250333786011\n",
      "Iteration 18450 Training loss 0.049733731895685196 Validation loss 0.054947905242443085 Accuracy 0.8543750643730164\n",
      "Iteration 18460 Training loss 0.05920984223484993 Validation loss 0.05484114959836006 Accuracy 0.8552500605583191\n",
      "Iteration 18470 Training loss 0.05183232203125954 Validation loss 0.05487750470638275 Accuracy 0.8545000553131104\n",
      "Iteration 18480 Training loss 0.04890424758195877 Validation loss 0.05475720390677452 Accuracy 0.8557500243186951\n",
      "Iteration 18490 Training loss 0.045667391270399094 Validation loss 0.054824039340019226 Accuracy 0.8557500243186951\n",
      "Iteration 18500 Training loss 0.051700275391340256 Validation loss 0.05476411059498787 Accuracy 0.8568750619888306\n",
      "Iteration 18510 Training loss 0.04551494121551514 Validation loss 0.05480343848466873 Accuracy 0.8553750514984131\n",
      "Iteration 18520 Training loss 0.05559851974248886 Validation loss 0.05496366322040558 Accuracy 0.8537500500679016\n",
      "Iteration 18530 Training loss 0.054968204349279404 Validation loss 0.05490263178944588 Accuracy 0.8540000319480896\n",
      "Iteration 18540 Training loss 0.06090758368372917 Validation loss 0.05475245416164398 Accuracy 0.8561250567436218\n",
      "Iteration 18550 Training loss 0.044504277408123016 Validation loss 0.05473149195313454 Accuracy 0.8568750619888306\n",
      "Iteration 18560 Training loss 0.04925492778420448 Validation loss 0.05484161525964737 Accuracy 0.8543750643730164\n",
      "Iteration 18570 Training loss 0.04943355545401573 Validation loss 0.05505792424082756 Accuracy 0.8531250357627869\n",
      "Iteration 18580 Training loss 0.053860366344451904 Validation loss 0.05481322109699249 Accuracy 0.8557500243186951\n",
      "Iteration 18590 Training loss 0.05826018378138542 Validation loss 0.05491786077618599 Accuracy 0.8550000190734863\n",
      "Iteration 18600 Training loss 0.051808807998895645 Validation loss 0.05507450923323631 Accuracy 0.8525000214576721\n",
      "Iteration 18610 Training loss 0.05416105315089226 Validation loss 0.05476754158735275 Accuracy 0.8555000424385071\n",
      "Iteration 18620 Training loss 0.05490938946604729 Validation loss 0.05470113083720207 Accuracy 0.8567500114440918\n",
      "Iteration 18630 Training loss 0.05322163552045822 Validation loss 0.054891061037778854 Accuracy 0.8548750281333923\n",
      "Iteration 18640 Training loss 0.053941503167152405 Validation loss 0.054769936949014664 Accuracy 0.8560000658035278\n",
      "Iteration 18650 Training loss 0.0517287440598011 Validation loss 0.05486728623509407 Accuracy 0.8552500605583191\n",
      "Iteration 18660 Training loss 0.05529789626598358 Validation loss 0.05473433807492256 Accuracy 0.8558750152587891\n",
      "Iteration 18670 Training loss 0.0597846582531929 Validation loss 0.054939426481723785 Accuracy 0.8541250228881836\n",
      "Iteration 18680 Training loss 0.061526086181402206 Validation loss 0.05473502725362778 Accuracy 0.8557500243186951\n",
      "Iteration 18690 Training loss 0.05786263942718506 Validation loss 0.05473601445555687 Accuracy 0.8560000658035278\n",
      "Iteration 18700 Training loss 0.04896588996052742 Validation loss 0.054842036217451096 Accuracy 0.8541250228881836\n",
      "Iteration 18710 Training loss 0.0517784021794796 Validation loss 0.05483699217438698 Accuracy 0.8547500371932983\n",
      "Iteration 18720 Training loss 0.05478915944695473 Validation loss 0.054701387882232666 Accuracy 0.8557500243186951\n",
      "Iteration 18730 Training loss 0.05787905678153038 Validation loss 0.05476414039731026 Accuracy 0.8552500605583191\n",
      "Iteration 18740 Training loss 0.05902685597538948 Validation loss 0.054673925042152405 Accuracy 0.8561250567436218\n",
      "Iteration 18750 Training loss 0.05208395794034004 Validation loss 0.05479533597826958 Accuracy 0.8540000319480896\n",
      "Iteration 18760 Training loss 0.05452654883265495 Validation loss 0.05462133139371872 Accuracy 0.8565000295639038\n",
      "Iteration 18770 Training loss 0.04707113653421402 Validation loss 0.05488544702529907 Accuracy 0.8550000190734863\n",
      "Iteration 18780 Training loss 0.060587406158447266 Validation loss 0.05484180152416229 Accuracy 0.8535000681877136\n",
      "Iteration 18790 Training loss 0.053681179881095886 Validation loss 0.05503229796886444 Accuracy 0.8536250591278076\n",
      "Iteration 18800 Training loss 0.05304818972945213 Validation loss 0.05482775717973709 Accuracy 0.8542500138282776\n",
      "Iteration 18810 Training loss 0.06042138487100601 Validation loss 0.054662469774484634 Accuracy 0.8548750281333923\n",
      "Iteration 18820 Training loss 0.05816160514950752 Validation loss 0.054679617285728455 Accuracy 0.8567500114440918\n",
      "Iteration 18830 Training loss 0.05756736546754837 Validation loss 0.054716698825359344 Accuracy 0.8555000424385071\n",
      "Iteration 18840 Training loss 0.04921702295541763 Validation loss 0.054609060287475586 Accuracy 0.8568750619888306\n",
      "Iteration 18850 Training loss 0.057270314544439316 Validation loss 0.054632242769002914 Accuracy 0.8557500243186951\n",
      "Iteration 18860 Training loss 0.05169958993792534 Validation loss 0.054570186883211136 Accuracy 0.8565000295639038\n",
      "Iteration 18870 Training loss 0.059892747551202774 Validation loss 0.05477435886859894 Accuracy 0.8545000553131104\n",
      "Iteration 18880 Training loss 0.05768144130706787 Validation loss 0.05457903444766998 Accuracy 0.8568750619888306\n",
      "Iteration 18890 Training loss 0.053981442004442215 Validation loss 0.0546734444797039 Accuracy 0.8561250567436218\n",
      "Iteration 18900 Training loss 0.052595410495996475 Validation loss 0.0546366386115551 Accuracy 0.8562500476837158\n",
      "Iteration 18910 Training loss 0.054713085293769836 Validation loss 0.054682303220033646 Accuracy 0.8556250333786011\n",
      "Iteration 18920 Training loss 0.05828644707798958 Validation loss 0.0546812079846859 Accuracy 0.8550000190734863\n",
      "Iteration 18930 Training loss 0.058212537318468094 Validation loss 0.05453268811106682 Accuracy 0.8567500114440918\n",
      "Iteration 18940 Training loss 0.05823889747262001 Validation loss 0.05457136034965515 Accuracy 0.8568750619888306\n",
      "Iteration 18950 Training loss 0.055859245359897614 Validation loss 0.05456965044140816 Accuracy 0.8578750491142273\n",
      "Iteration 18960 Training loss 0.04729669541120529 Validation loss 0.0546746701002121 Accuracy 0.8552500605583191\n",
      "Iteration 18970 Training loss 0.04184355586767197 Validation loss 0.0544770210981369 Accuracy 0.8567500114440918\n",
      "Iteration 18980 Training loss 0.0483151413500309 Validation loss 0.054672449827194214 Accuracy 0.8550000190734863\n",
      "Iteration 18990 Training loss 0.05084070935845375 Validation loss 0.054485950618982315 Accuracy 0.8557500243186951\n",
      "Iteration 19000 Training loss 0.0563732385635376 Validation loss 0.054505605250597 Accuracy 0.8562500476837158\n",
      "Iteration 19010 Training loss 0.047773294150829315 Validation loss 0.054625254124403 Accuracy 0.8545000553131104\n",
      "Iteration 19020 Training loss 0.05157534405589104 Validation loss 0.05463738739490509 Accuracy 0.8552500605583191\n",
      "Iteration 19030 Training loss 0.05661192908883095 Validation loss 0.054769985377788544 Accuracy 0.8541250228881836\n",
      "Iteration 19040 Training loss 0.056086309254169464 Validation loss 0.054496634751558304 Accuracy 0.8561250567436218\n",
      "Iteration 19050 Training loss 0.056236766278743744 Validation loss 0.05449041351675987 Accuracy 0.8562500476837158\n",
      "Iteration 19060 Training loss 0.053160276263952255 Validation loss 0.05464041605591774 Accuracy 0.8546250462532043\n",
      "Iteration 19070 Training loss 0.049692943692207336 Validation loss 0.0549168586730957 Accuracy 0.8541250228881836\n",
      "Iteration 19080 Training loss 0.051702830940485 Validation loss 0.05449112877249718 Accuracy 0.8566250205039978\n",
      "Iteration 19090 Training loss 0.05706257373094559 Validation loss 0.054471757262945175 Accuracy 0.8560000658035278\n",
      "Iteration 19100 Training loss 0.05286167562007904 Validation loss 0.05453099310398102 Accuracy 0.8562500476837158\n",
      "Iteration 19110 Training loss 0.05914707109332085 Validation loss 0.054526813328266144 Accuracy 0.8560000658035278\n",
      "Iteration 19120 Training loss 0.051931727677583694 Validation loss 0.054735612124204636 Accuracy 0.8551250696182251\n",
      "Iteration 19130 Training loss 0.05221056938171387 Validation loss 0.0544237457215786 Accuracy 0.8567500114440918\n",
      "Iteration 19140 Training loss 0.05009615421295166 Validation loss 0.05445289611816406 Accuracy 0.8560000658035278\n",
      "Iteration 19150 Training loss 0.049857307225465775 Validation loss 0.05455996096134186 Accuracy 0.8547500371932983\n",
      "Iteration 19160 Training loss 0.046197984367609024 Validation loss 0.054809484630823135 Accuracy 0.8532500267028809\n",
      "Iteration 19170 Training loss 0.05731409043073654 Validation loss 0.05478730425238609 Accuracy 0.8530000448226929\n",
      "Iteration 19180 Training loss 0.057270441204309464 Validation loss 0.054426100105047226 Accuracy 0.8556250333786011\n",
      "Iteration 19190 Training loss 0.05099670588970184 Validation loss 0.054649654775857925 Accuracy 0.8546250462532043\n",
      "Iteration 19200 Training loss 0.06743662804365158 Validation loss 0.0545133501291275 Accuracy 0.8553750514984131\n",
      "Iteration 19210 Training loss 0.04774361476302147 Validation loss 0.05448780208826065 Accuracy 0.8567500114440918\n",
      "Iteration 19220 Training loss 0.053985510021448135 Validation loss 0.054466333240270615 Accuracy 0.8568750619888306\n",
      "Iteration 19230 Training loss 0.05022154375910759 Validation loss 0.05443186312913895 Accuracy 0.8570000529289246\n",
      "Iteration 19240 Training loss 0.052974846214056015 Validation loss 0.05437532439827919 Accuracy 0.8572500348091125\n",
      "Iteration 19250 Training loss 0.04942991957068443 Validation loss 0.05438198894262314 Accuracy 0.8573750257492065\n",
      "Iteration 19260 Training loss 0.06007295474410057 Validation loss 0.05445929989218712 Accuracy 0.8566250205039978\n",
      "Iteration 19270 Training loss 0.05227508395910263 Validation loss 0.05435977876186371 Accuracy 0.8570000529289246\n",
      "Iteration 19280 Training loss 0.058922383934259415 Validation loss 0.05446314811706543 Accuracy 0.8567500114440918\n",
      "Iteration 19290 Training loss 0.056075457483530045 Validation loss 0.054345957934856415 Accuracy 0.8572500348091125\n",
      "Iteration 19300 Training loss 0.056116268038749695 Validation loss 0.05437067151069641 Accuracy 0.8573750257492065\n",
      "Iteration 19310 Training loss 0.052400216460227966 Validation loss 0.05442899093031883 Accuracy 0.8567500114440918\n",
      "Iteration 19320 Training loss 0.049418170005083084 Validation loss 0.054331324994564056 Accuracy 0.8575000166893005\n",
      "Iteration 19330 Training loss 0.044299058616161346 Validation loss 0.05439671501517296 Accuracy 0.8563750386238098\n",
      "Iteration 19340 Training loss 0.04330980032682419 Validation loss 0.054490476846694946 Accuracy 0.8556250333786011\n",
      "Iteration 19350 Training loss 0.058342669159173965 Validation loss 0.05459293723106384 Accuracy 0.8547500371932983\n",
      "Iteration 19360 Training loss 0.046898867934942245 Validation loss 0.05465436726808548 Accuracy 0.8545000553131104\n",
      "Iteration 19370 Training loss 0.03841526806354523 Validation loss 0.05434991791844368 Accuracy 0.8567500114440918\n",
      "Iteration 19380 Training loss 0.0436277873814106 Validation loss 0.054340943694114685 Accuracy 0.8570000529289246\n",
      "Iteration 19390 Training loss 0.055647529661655426 Validation loss 0.05436577647924423 Accuracy 0.8566250205039978\n",
      "Iteration 19400 Training loss 0.05379649996757507 Validation loss 0.054773952811956406 Accuracy 0.8546250462532043\n",
      "Iteration 19410 Training loss 0.05254342034459114 Validation loss 0.05436041206121445 Accuracy 0.8563750386238098\n",
      "Iteration 19420 Training loss 0.05339035391807556 Validation loss 0.0543401725590229 Accuracy 0.8567500114440918\n",
      "Iteration 19430 Training loss 0.056697528809309006 Validation loss 0.054369356483221054 Accuracy 0.8561250567436218\n",
      "Iteration 19440 Training loss 0.05226842686533928 Validation loss 0.054438550025224686 Accuracy 0.8557500243186951\n",
      "Iteration 19450 Training loss 0.04486515745520592 Validation loss 0.0542890727519989 Accuracy 0.8575000166893005\n",
      "Iteration 19460 Training loss 0.047869663685560226 Validation loss 0.05427732318639755 Accuracy 0.8572500348091125\n",
      "Iteration 19470 Training loss 0.048322685062885284 Validation loss 0.05444673076272011 Accuracy 0.8553750514984131\n",
      "Iteration 19480 Training loss 0.057420339435338974 Validation loss 0.054302629083395004 Accuracy 0.8561250567436218\n",
      "Iteration 19490 Training loss 0.0529097355902195 Validation loss 0.054317768663167953 Accuracy 0.8556250333786011\n",
      "Iteration 19500 Training loss 0.06139400228857994 Validation loss 0.054290495812892914 Accuracy 0.8577500581741333\n",
      "Iteration 19510 Training loss 0.054375533014535904 Validation loss 0.05445919930934906 Accuracy 0.8550000190734863\n",
      "Iteration 19520 Training loss 0.04905920475721359 Validation loss 0.0542825423181057 Accuracy 0.8572500348091125\n",
      "Iteration 19530 Training loss 0.0483882911503315 Validation loss 0.05431396886706352 Accuracy 0.8572500348091125\n",
      "Iteration 19540 Training loss 0.047950249165296555 Validation loss 0.054298579692840576 Accuracy 0.8575000166893005\n",
      "Iteration 19550 Training loss 0.04745396599173546 Validation loss 0.05425693094730377 Accuracy 0.8566250205039978\n",
      "Iteration 19560 Training loss 0.06359567493200302 Validation loss 0.054491449147462845 Accuracy 0.8553750514984131\n",
      "Iteration 19570 Training loss 0.04754982516169548 Validation loss 0.054285887628793716 Accuracy 0.8575000166893005\n",
      "Iteration 19580 Training loss 0.05361134931445122 Validation loss 0.05424584448337555 Accuracy 0.8573750257492065\n",
      "Iteration 19590 Training loss 0.04999544098973274 Validation loss 0.05426627770066261 Accuracy 0.8568750619888306\n",
      "Iteration 19600 Training loss 0.05317180976271629 Validation loss 0.054203473031520844 Accuracy 0.8573750257492065\n",
      "Iteration 19610 Training loss 0.05217524990439415 Validation loss 0.05419398844242096 Accuracy 0.8581250309944153\n",
      "Iteration 19620 Training loss 0.04933202639222145 Validation loss 0.05423697829246521 Accuracy 0.8573750257492065\n",
      "Iteration 19630 Training loss 0.047302570194005966 Validation loss 0.054179735481739044 Accuracy 0.8577500581741333\n",
      "Iteration 19640 Training loss 0.04748404026031494 Validation loss 0.05416273698210716 Accuracy 0.8580000400543213\n",
      "Iteration 19650 Training loss 0.051360998302698135 Validation loss 0.05417228862643242 Accuracy 0.8577500581741333\n",
      "Iteration 19660 Training loss 0.05064013972878456 Validation loss 0.054495371878147125 Accuracy 0.8558750152587891\n",
      "Iteration 19670 Training loss 0.04934873431921005 Validation loss 0.05415603145956993 Accuracy 0.8578750491142273\n",
      "Iteration 19680 Training loss 0.06130647659301758 Validation loss 0.054368432611227036 Accuracy 0.8573750257492065\n",
      "Iteration 19690 Training loss 0.051703937351703644 Validation loss 0.05426590144634247 Accuracy 0.8576250672340393\n",
      "Iteration 19700 Training loss 0.045259226113557816 Validation loss 0.05424075573682785 Accuracy 0.8571250438690186\n",
      "Iteration 19710 Training loss 0.053101442754268646 Validation loss 0.05413595959544182 Accuracy 0.8578750491142273\n",
      "Iteration 19720 Training loss 0.05209442600607872 Validation loss 0.05415085703134537 Accuracy 0.85875004529953\n",
      "Iteration 19730 Training loss 0.04626357927918434 Validation loss 0.05414096266031265 Accuracy 0.8581250309944153\n",
      "Iteration 19740 Training loss 0.04978908598423004 Validation loss 0.0544426366686821 Accuracy 0.8556250333786011\n",
      "Iteration 19750 Training loss 0.05085405334830284 Validation loss 0.05413505807518959 Accuracy 0.8581250309944153\n",
      "Iteration 19760 Training loss 0.04888853803277016 Validation loss 0.05415746569633484 Accuracy 0.8577500581741333\n",
      "Iteration 19770 Training loss 0.05283641070127487 Validation loss 0.05415375903248787 Accuracy 0.8573750257492065\n",
      "Iteration 19780 Training loss 0.051332369446754456 Validation loss 0.05406970530748367 Accuracy 0.8581250309944153\n",
      "Iteration 19790 Training loss 0.054656729102134705 Validation loss 0.05407901853322983 Accuracy 0.8573750257492065\n",
      "Iteration 19800 Training loss 0.0532221682369709 Validation loss 0.05408426746726036 Accuracy 0.8567500114440918\n",
      "Iteration 19810 Training loss 0.04901132360100746 Validation loss 0.05407130718231201 Accuracy 0.8575000166893005\n",
      "Iteration 19820 Training loss 0.04957609996199608 Validation loss 0.054109301418066025 Accuracy 0.8581250309944153\n",
      "Iteration 19830 Training loss 0.05600878596305847 Validation loss 0.05409032851457596 Accuracy 0.8573750257492065\n",
      "Iteration 19840 Training loss 0.0583939291536808 Validation loss 0.05447882041335106 Accuracy 0.8536250591278076\n",
      "Iteration 19850 Training loss 0.050648316740989685 Validation loss 0.05409420654177666 Accuracy 0.8580000400543213\n",
      "Iteration 19860 Training loss 0.0573929026722908 Validation loss 0.05411948636174202 Accuracy 0.8568750619888306\n",
      "Iteration 19870 Training loss 0.054061003029346466 Validation loss 0.054255999624729156 Accuracy 0.8570000529289246\n",
      "Iteration 19880 Training loss 0.057475823909044266 Validation loss 0.05409570410847664 Accuracy 0.8568750619888306\n",
      "Iteration 19890 Training loss 0.05550113692879677 Validation loss 0.05407819524407387 Accuracy 0.8581250309944153\n",
      "Iteration 19900 Training loss 0.04950545355677605 Validation loss 0.05448456108570099 Accuracy 0.8546250462532043\n",
      "Iteration 19910 Training loss 0.05030517280101776 Validation loss 0.05404911935329437 Accuracy 0.8576250672340393\n",
      "Iteration 19920 Training loss 0.057717278599739075 Validation loss 0.05402781069278717 Accuracy 0.8580000400543213\n",
      "Iteration 19930 Training loss 0.045665282756090164 Validation loss 0.054262202233076096 Accuracy 0.8563750386238098\n",
      "Iteration 19940 Training loss 0.05552133917808533 Validation loss 0.05406477674841881 Accuracy 0.8581250309944153\n",
      "Iteration 19950 Training loss 0.04345669597387314 Validation loss 0.054018307477235794 Accuracy 0.858500063419342\n",
      "Iteration 19960 Training loss 0.045964181423187256 Validation loss 0.05399806797504425 Accuracy 0.8582500219345093\n",
      "Iteration 19970 Training loss 0.04828186333179474 Validation loss 0.05408253148198128 Accuracy 0.8565000295639038\n",
      "Iteration 19980 Training loss 0.05015743151307106 Validation loss 0.054259639233350754 Accuracy 0.8552500605583191\n",
      "Iteration 19990 Training loss 0.05693656951189041 Validation loss 0.05407308042049408 Accuracy 0.8576250672340393\n",
      "Iteration 20000 Training loss 0.05164346098899841 Validation loss 0.05397561937570572 Accuracy 0.8582500219345093\n",
      "Iteration 20010 Training loss 0.05185523256659508 Validation loss 0.05417748540639877 Accuracy 0.8568750619888306\n",
      "Iteration 20020 Training loss 0.053598612546920776 Validation loss 0.053987544029951096 Accuracy 0.8578750491142273\n",
      "Iteration 20030 Training loss 0.04961394518613815 Validation loss 0.05397039279341698 Accuracy 0.8582500219345093\n",
      "Iteration 20040 Training loss 0.054861828684806824 Validation loss 0.05399404466152191 Accuracy 0.8581250309944153\n",
      "Iteration 20050 Training loss 0.048021964728832245 Validation loss 0.05398216098546982 Accuracy 0.858500063419342\n",
      "Iteration 20060 Training loss 0.04685652628540993 Validation loss 0.05402374640107155 Accuracy 0.8572500348091125\n",
      "Iteration 20070 Training loss 0.0525972917675972 Validation loss 0.053948841989040375 Accuracy 0.858500063419342\n",
      "Iteration 20080 Training loss 0.06323235481977463 Validation loss 0.053981270641088486 Accuracy 0.8580000400543213\n",
      "Iteration 20090 Training loss 0.053911320865154266 Validation loss 0.05402752012014389 Accuracy 0.8572500348091125\n",
      "Iteration 20100 Training loss 0.053672030568122864 Validation loss 0.05395039543509483 Accuracy 0.858500063419342\n",
      "Iteration 20110 Training loss 0.04859739542007446 Validation loss 0.0539398230612278 Accuracy 0.8572500348091125\n",
      "Iteration 20120 Training loss 0.0465359166264534 Validation loss 0.053995102643966675 Accuracy 0.8581250309944153\n",
      "Iteration 20130 Training loss 0.05102990195155144 Validation loss 0.05431443825364113 Accuracy 0.8548750281333923\n",
      "Iteration 20140 Training loss 0.04599588364362717 Validation loss 0.05395227670669556 Accuracy 0.8581250309944153\n",
      "Iteration 20150 Training loss 0.04741948843002319 Validation loss 0.05404900014400482 Accuracy 0.8563750386238098\n",
      "Iteration 20160 Training loss 0.05711040273308754 Validation loss 0.05402654781937599 Accuracy 0.8561250567436218\n",
      "Iteration 20170 Training loss 0.04272209107875824 Validation loss 0.05391153320670128 Accuracy 0.858625054359436\n",
      "Iteration 20180 Training loss 0.049279697239398956 Validation loss 0.053904514759778976 Accuracy 0.8570000529289246\n",
      "Iteration 20190 Training loss 0.048966262489557266 Validation loss 0.053915392607450485 Accuracy 0.8577500581741333\n",
      "Iteration 20200 Training loss 0.04486888274550438 Validation loss 0.0539047047495842 Accuracy 0.858625054359436\n",
      "Iteration 20210 Training loss 0.053894560784101486 Validation loss 0.05390576273202896 Accuracy 0.8566250205039978\n",
      "Iteration 20220 Training loss 0.0563676543533802 Validation loss 0.05434153974056244 Accuracy 0.8550000190734863\n",
      "Iteration 20230 Training loss 0.056925367563962936 Validation loss 0.053858134895563126 Accuracy 0.8581250309944153\n",
      "Iteration 20240 Training loss 0.06367369741201401 Validation loss 0.05393484607338905 Accuracy 0.8573750257492065\n",
      "Iteration 20250 Training loss 0.05802473425865173 Validation loss 0.05394510179758072 Accuracy 0.8567500114440918\n",
      "Iteration 20260 Training loss 0.05742422863841057 Validation loss 0.05388180539011955 Accuracy 0.8578750491142273\n",
      "Iteration 20270 Training loss 0.04813528060913086 Validation loss 0.05384819954633713 Accuracy 0.858875036239624\n",
      "Iteration 20280 Training loss 0.048189472407102585 Validation loss 0.05392962321639061 Accuracy 0.8568750619888306\n",
      "Iteration 20290 Training loss 0.061909016221761703 Validation loss 0.05386214703321457 Accuracy 0.858625054359436\n",
      "Iteration 20300 Training loss 0.054939381778240204 Validation loss 0.05387125536799431 Accuracy 0.858500063419342\n",
      "Iteration 20310 Training loss 0.055327873677015305 Validation loss 0.053861912339925766 Accuracy 0.8592500686645508\n",
      "Iteration 20320 Training loss 0.05335156247019768 Validation loss 0.05389193817973137 Accuracy 0.858875036239624\n",
      "Iteration 20330 Training loss 0.0469239316880703 Validation loss 0.05382908135652542 Accuracy 0.8573750257492065\n",
      "Iteration 20340 Training loss 0.05189572274684906 Validation loss 0.053952399641275406 Accuracy 0.8571250438690186\n",
      "Iteration 20350 Training loss 0.05800895392894745 Validation loss 0.05389268696308136 Accuracy 0.8592500686645508\n",
      "Iteration 20360 Training loss 0.05696900188922882 Validation loss 0.05381399765610695 Accuracy 0.8583750128746033\n",
      "Iteration 20370 Training loss 0.06063395366072655 Validation loss 0.05465216934680939 Accuracy 0.8536250591278076\n",
      "Iteration 20380 Training loss 0.05715445429086685 Validation loss 0.05396804213523865 Accuracy 0.8571250438690186\n",
      "Iteration 20390 Training loss 0.049607548862695694 Validation loss 0.05389031395316124 Accuracy 0.8571250438690186\n",
      "Iteration 20400 Training loss 0.05308283865451813 Validation loss 0.05452630668878555 Accuracy 0.8538750410079956\n",
      "Iteration 20410 Training loss 0.05141519755125046 Validation loss 0.05378314480185509 Accuracy 0.858625054359436\n",
      "Iteration 20420 Training loss 0.04913834482431412 Validation loss 0.053794343024492264 Accuracy 0.8582500219345093\n",
      "Iteration 20430 Training loss 0.04272777587175369 Validation loss 0.053913991898298264 Accuracy 0.8570000529289246\n",
      "Iteration 20440 Training loss 0.06159557029604912 Validation loss 0.053804680705070496 Accuracy 0.8578750491142273\n",
      "Iteration 20450 Training loss 0.04742254316806793 Validation loss 0.05406095087528229 Accuracy 0.8571250438690186\n",
      "Iteration 20460 Training loss 0.04887806624174118 Validation loss 0.054005034267902374 Accuracy 0.8562500476837158\n",
      "Iteration 20470 Training loss 0.05640678107738495 Validation loss 0.05375105142593384 Accuracy 0.858625054359436\n",
      "Iteration 20480 Training loss 0.05479482188820839 Validation loss 0.0538129061460495 Accuracy 0.859000027179718\n",
      "Iteration 20490 Training loss 0.04310058429837227 Validation loss 0.053880371153354645 Accuracy 0.8576250672340393\n",
      "Iteration 20500 Training loss 0.057194989174604416 Validation loss 0.05378960818052292 Accuracy 0.859125018119812\n",
      "Iteration 20510 Training loss 0.05129304900765419 Validation loss 0.053761519491672516 Accuracy 0.858625054359436\n",
      "Iteration 20520 Training loss 0.05337200686335564 Validation loss 0.05379069596529007 Accuracy 0.85875004529953\n",
      "Iteration 20530 Training loss 0.050942562520504 Validation loss 0.05377158522605896 Accuracy 0.858500063419342\n",
      "Iteration 20540 Training loss 0.05243108794093132 Validation loss 0.05376821756362915 Accuracy 0.8583750128746033\n",
      "Iteration 20550 Training loss 0.04987340420484543 Validation loss 0.05380591005086899 Accuracy 0.8583750128746033\n",
      "Iteration 20560 Training loss 0.0444827638566494 Validation loss 0.05409083515405655 Accuracy 0.8570000529289246\n",
      "Iteration 20570 Training loss 0.038934968411922455 Validation loss 0.05381312966346741 Accuracy 0.8583750128746033\n",
      "Iteration 20580 Training loss 0.05619592219591141 Validation loss 0.05380594730377197 Accuracy 0.8578750491142273\n",
      "Iteration 20590 Training loss 0.048189934343099594 Validation loss 0.05386151745915413 Accuracy 0.8573750257492065\n",
      "Iteration 20600 Training loss 0.05218419432640076 Validation loss 0.05400773882865906 Accuracy 0.8567500114440918\n",
      "Iteration 20610 Training loss 0.05133690685033798 Validation loss 0.05375547334551811 Accuracy 0.859000027179718\n",
      "Iteration 20620 Training loss 0.05514305830001831 Validation loss 0.05378160998225212 Accuracy 0.8575000166893005\n",
      "Iteration 20630 Training loss 0.0582914724946022 Validation loss 0.05374503880739212 Accuracy 0.8576250672340393\n",
      "Iteration 20640 Training loss 0.05684052035212517 Validation loss 0.0544218048453331 Accuracy 0.8552500605583191\n",
      "Iteration 20650 Training loss 0.057617537677288055 Validation loss 0.05374032258987427 Accuracy 0.8580000400543213\n",
      "Iteration 20660 Training loss 0.054508913308382034 Validation loss 0.05369485914707184 Accuracy 0.859125018119812\n",
      "Iteration 20670 Training loss 0.05543901026248932 Validation loss 0.05473397299647331 Accuracy 0.8531250357627869\n",
      "Iteration 20680 Training loss 0.05376853048801422 Validation loss 0.05369612202048302 Accuracy 0.8582500219345093\n",
      "Iteration 20690 Training loss 0.052021510899066925 Validation loss 0.05368395894765854 Accuracy 0.858875036239624\n",
      "Iteration 20700 Training loss 0.04690876230597496 Validation loss 0.05368823558092117 Accuracy 0.8582500219345093\n",
      "Iteration 20710 Training loss 0.05370094254612923 Validation loss 0.05381013825535774 Accuracy 0.8573750257492065\n",
      "Iteration 20720 Training loss 0.044662922620773315 Validation loss 0.05368475615978241 Accuracy 0.859000027179718\n",
      "Iteration 20730 Training loss 0.047355689108371735 Validation loss 0.053722865879535675 Accuracy 0.858875036239624\n",
      "Iteration 20740 Training loss 0.05146351829171181 Validation loss 0.05372887849807739 Accuracy 0.858625054359436\n",
      "Iteration 20750 Training loss 0.047857679426670074 Validation loss 0.05365080386400223 Accuracy 0.8581250309944153\n",
      "Iteration 20760 Training loss 0.06214861571788788 Validation loss 0.05382130295038223 Accuracy 0.8573750257492065\n",
      "Iteration 20770 Training loss 0.044715337455272675 Validation loss 0.05370645225048065 Accuracy 0.859000027179718\n",
      "Iteration 20780 Training loss 0.05902905389666557 Validation loss 0.053958795964717865 Accuracy 0.8571250438690186\n",
      "Iteration 20790 Training loss 0.055815935134887695 Validation loss 0.05362075939774513 Accuracy 0.858875036239624\n",
      "Iteration 20800 Training loss 0.05465809628367424 Validation loss 0.05367779731750488 Accuracy 0.8597500324249268\n",
      "Iteration 20810 Training loss 0.057610392570495605 Validation loss 0.053625378757715225 Accuracy 0.859125018119812\n",
      "Iteration 20820 Training loss 0.04858306795358658 Validation loss 0.05381776764988899 Accuracy 0.8566250205039978\n",
      "Iteration 20830 Training loss 0.050527047365903854 Validation loss 0.053711749613285065 Accuracy 0.8593750596046448\n",
      "Iteration 20840 Training loss 0.06259440630674362 Validation loss 0.05359845981001854 Accuracy 0.8582500219345093\n",
      "Iteration 20850 Training loss 0.048280972987413406 Validation loss 0.0537281408905983 Accuracy 0.859125018119812\n",
      "Iteration 20860 Training loss 0.05579298734664917 Validation loss 0.0537673719227314 Accuracy 0.8575000166893005\n",
      "Iteration 20870 Training loss 0.0557851605117321 Validation loss 0.05358516797423363 Accuracy 0.858625054359436\n",
      "Iteration 20880 Training loss 0.056907568126916885 Validation loss 0.05366438627243042 Accuracy 0.8578750491142273\n",
      "Iteration 20890 Training loss 0.057723693549633026 Validation loss 0.053549814969301224 Accuracy 0.8580000400543213\n",
      "Iteration 20900 Training loss 0.05230192840099335 Validation loss 0.053557030856609344 Accuracy 0.8583750128746033\n",
      "Iteration 20910 Training loss 0.05867207422852516 Validation loss 0.0536997988820076 Accuracy 0.8575000166893005\n",
      "Iteration 20920 Training loss 0.049653418362140656 Validation loss 0.05361229181289673 Accuracy 0.8582500219345093\n",
      "Iteration 20930 Training loss 0.05503334477543831 Validation loss 0.05364016443490982 Accuracy 0.85875004529953\n",
      "Iteration 20940 Training loss 0.04922928661108017 Validation loss 0.05360826477408409 Accuracy 0.8593750596046448\n",
      "Iteration 20950 Training loss 0.0503140352666378 Validation loss 0.053604550659656525 Accuracy 0.8595000505447388\n",
      "Iteration 20960 Training loss 0.04781248793005943 Validation loss 0.053584735840559006 Accuracy 0.859000027179718\n",
      "Iteration 20970 Training loss 0.04752287641167641 Validation loss 0.053612083196640015 Accuracy 0.8593750596046448\n",
      "Iteration 20980 Training loss 0.04722914472222328 Validation loss 0.05368812382221222 Accuracy 0.8570000529289246\n",
      "Iteration 20990 Training loss 0.059252459555864334 Validation loss 0.053586672991514206 Accuracy 0.8575000166893005\n",
      "Iteration 21000 Training loss 0.054420918226242065 Validation loss 0.05382074415683746 Accuracy 0.8568750619888306\n",
      "Iteration 21010 Training loss 0.056444015353918076 Validation loss 0.05355796962976456 Accuracy 0.859125018119812\n",
      "Iteration 21020 Training loss 0.05105224624276161 Validation loss 0.05351852625608444 Accuracy 0.858500063419342\n",
      "Iteration 21030 Training loss 0.05104542151093483 Validation loss 0.05356930196285248 Accuracy 0.8580000400543213\n",
      "Iteration 21040 Training loss 0.048859670758247375 Validation loss 0.053755469620227814 Accuracy 0.8577500581741333\n",
      "Iteration 21050 Training loss 0.046366527676582336 Validation loss 0.053512390702962875 Accuracy 0.858875036239624\n",
      "Iteration 21060 Training loss 0.04545803740620613 Validation loss 0.05348354950547218 Accuracy 0.8597500324249268\n",
      "Iteration 21070 Training loss 0.056556835770606995 Validation loss 0.053551677614450455 Accuracy 0.85875004529953\n",
      "Iteration 21080 Training loss 0.055655911564826965 Validation loss 0.053494058549404144 Accuracy 0.8592500686645508\n",
      "Iteration 21090 Training loss 0.05033795163035393 Validation loss 0.05350124463438988 Accuracy 0.85875004529953\n",
      "Iteration 21100 Training loss 0.04951351881027222 Validation loss 0.05346473678946495 Accuracy 0.85875004529953\n",
      "Iteration 21110 Training loss 0.057467687875032425 Validation loss 0.05406339839100838 Accuracy 0.8566250205039978\n",
      "Iteration 21120 Training loss 0.05588438734412193 Validation loss 0.053465183824300766 Accuracy 0.8592500686645508\n",
      "Iteration 21130 Training loss 0.04846625775098801 Validation loss 0.053463712334632874 Accuracy 0.859125018119812\n",
      "Iteration 21140 Training loss 0.056375425308942795 Validation loss 0.053486548364162445 Accuracy 0.858500063419342\n",
      "Iteration 21150 Training loss 0.05158571898937225 Validation loss 0.053452666848897934 Accuracy 0.859000027179718\n",
      "Iteration 21160 Training loss 0.05974173545837402 Validation loss 0.05352174863219261 Accuracy 0.8577500581741333\n",
      "Iteration 21170 Training loss 0.04685003682971001 Validation loss 0.05359938368201256 Accuracy 0.8582500219345093\n",
      "Iteration 21180 Training loss 0.056147802621126175 Validation loss 0.05345718562602997 Accuracy 0.859125018119812\n",
      "Iteration 21190 Training loss 0.04970858618617058 Validation loss 0.05356426537036896 Accuracy 0.859000027179718\n",
      "Iteration 21200 Training loss 0.04732143133878708 Validation loss 0.05346682295203209 Accuracy 0.858500063419342\n",
      "Iteration 21210 Training loss 0.05548717826604843 Validation loss 0.053487833589315414 Accuracy 0.858500063419342\n",
      "Iteration 21220 Training loss 0.04206039384007454 Validation loss 0.05346555635333061 Accuracy 0.8582500219345093\n",
      "Iteration 21230 Training loss 0.05916152894496918 Validation loss 0.05343521758913994 Accuracy 0.8582500219345093\n",
      "Iteration 21240 Training loss 0.05352579429745674 Validation loss 0.05342301353812218 Accuracy 0.85875004529953\n",
      "Iteration 21250 Training loss 0.04840188845992088 Validation loss 0.05352574586868286 Accuracy 0.8580000400543213\n",
      "Iteration 21260 Training loss 0.04706597328186035 Validation loss 0.05340193957090378 Accuracy 0.8593750596046448\n",
      "Iteration 21270 Training loss 0.04691663011908531 Validation loss 0.053384799510240555 Accuracy 0.8595000505447388\n",
      "Iteration 21280 Training loss 0.05088723078370094 Validation loss 0.05372919887304306 Accuracy 0.8576250672340393\n",
      "Iteration 21290 Training loss 0.04741918295621872 Validation loss 0.05364632233977318 Accuracy 0.8570000529289246\n",
      "Iteration 21300 Training loss 0.051831815391778946 Validation loss 0.05388595908880234 Accuracy 0.8563750386238098\n",
      "Iteration 21310 Training loss 0.05633044242858887 Validation loss 0.053697094321250916 Accuracy 0.8557500243186951\n",
      "Iteration 21320 Training loss 0.047796983271837234 Validation loss 0.053433775901794434 Accuracy 0.859000027179718\n",
      "Iteration 21330 Training loss 0.05349399521946907 Validation loss 0.05337819084525108 Accuracy 0.8598750233650208\n",
      "Iteration 21340 Training loss 0.04508739337325096 Validation loss 0.053458116948604584 Accuracy 0.8595000505447388\n",
      "Iteration 21350 Training loss 0.05042115971446037 Validation loss 0.05337351933121681 Accuracy 0.8593750596046448\n",
      "Iteration 21360 Training loss 0.0507059171795845 Validation loss 0.053368281573057175 Accuracy 0.8593750596046448\n",
      "Iteration 21370 Training loss 0.04325602948665619 Validation loss 0.05336013436317444 Accuracy 0.8592500686645508\n",
      "Iteration 21380 Training loss 0.05781973525881767 Validation loss 0.05334587022662163 Accuracy 0.8595000505447388\n",
      "Iteration 21390 Training loss 0.056900832802057266 Validation loss 0.053682129830121994 Accuracy 0.8566250205039978\n",
      "Iteration 21400 Training loss 0.05755668878555298 Validation loss 0.053511034697294235 Accuracy 0.8578750491142273\n",
      "Iteration 21410 Training loss 0.045849330723285675 Validation loss 0.05332987755537033 Accuracy 0.859000027179718\n",
      "Iteration 21420 Training loss 0.050998058170080185 Validation loss 0.053312018513679504 Accuracy 0.8593750596046448\n",
      "Iteration 21430 Training loss 0.04886867478489876 Validation loss 0.05329067260026932 Accuracy 0.8595000505447388\n",
      "Iteration 21440 Training loss 0.05985800549387932 Validation loss 0.053549036383628845 Accuracy 0.858625054359436\n",
      "Iteration 21450 Training loss 0.04782227426767349 Validation loss 0.05333966761827469 Accuracy 0.8598750233650208\n",
      "Iteration 21460 Training loss 0.048660438507795334 Validation loss 0.0532960407435894 Accuracy 0.8592500686645508\n",
      "Iteration 21470 Training loss 0.05663979426026344 Validation loss 0.054173074662685394 Accuracy 0.8550000190734863\n",
      "Iteration 21480 Training loss 0.05243920162320137 Validation loss 0.05423488840460777 Accuracy 0.8551250696182251\n",
      "Iteration 21490 Training loss 0.04935064539313316 Validation loss 0.053306058049201965 Accuracy 0.85875004529953\n",
      "Iteration 21500 Training loss 0.05105867236852646 Validation loss 0.05326222628355026 Accuracy 0.8601250648498535\n",
      "Iteration 21510 Training loss 0.052149638533592224 Validation loss 0.05329854041337967 Accuracy 0.8583750128746033\n",
      "Iteration 21520 Training loss 0.050668392330408096 Validation loss 0.053322773426771164 Accuracy 0.85875004529953\n",
      "Iteration 21530 Training loss 0.04770524799823761 Validation loss 0.05361150577664375 Accuracy 0.8580000400543213\n",
      "Iteration 21540 Training loss 0.04475288838148117 Validation loss 0.053312044590711594 Accuracy 0.85875004529953\n",
      "Iteration 21550 Training loss 0.046080321073532104 Validation loss 0.05330482870340347 Accuracy 0.8603750467300415\n",
      "Iteration 21560 Training loss 0.045812368392944336 Validation loss 0.053262658417224884 Accuracy 0.8601250648498535\n",
      "Iteration 21570 Training loss 0.05386470630764961 Validation loss 0.05342046916484833 Accuracy 0.858500063419342\n",
      "Iteration 21580 Training loss 0.05301174893975258 Validation loss 0.05325159803032875 Accuracy 0.8596250414848328\n",
      "Iteration 21590 Training loss 0.05463719367980957 Validation loss 0.05338563397526741 Accuracy 0.8600000143051147\n",
      "Iteration 21600 Training loss 0.06134415417909622 Validation loss 0.05336443707346916 Accuracy 0.858625054359436\n",
      "Iteration 21610 Training loss 0.057146959006786346 Validation loss 0.05353296920657158 Accuracy 0.85875004529953\n",
      "Iteration 21620 Training loss 0.061402447521686554 Validation loss 0.053252387791872025 Accuracy 0.8593750596046448\n",
      "Iteration 21630 Training loss 0.04836975410580635 Validation loss 0.05326392501592636 Accuracy 0.8600000143051147\n",
      "Iteration 21640 Training loss 0.05368147790431976 Validation loss 0.05330204218626022 Accuracy 0.8603750467300415\n",
      "Iteration 21650 Training loss 0.053460147231817245 Validation loss 0.05366964265704155 Accuracy 0.8583750128746033\n",
      "Iteration 21660 Training loss 0.05089174583554268 Validation loss 0.05322747677564621 Accuracy 0.8601250648498535\n",
      "Iteration 21670 Training loss 0.05471653863787651 Validation loss 0.05321536958217621 Accuracy 0.8600000143051147\n",
      "Iteration 21680 Training loss 0.05737147107720375 Validation loss 0.05325063318014145 Accuracy 0.8598750233650208\n",
      "Iteration 21690 Training loss 0.05358068272471428 Validation loss 0.05375731736421585 Accuracy 0.8563750386238098\n",
      "Iteration 21700 Training loss 0.05461834371089935 Validation loss 0.05316399037837982 Accuracy 0.8601250648498535\n",
      "Iteration 21710 Training loss 0.05016982927918434 Validation loss 0.05327465385198593 Accuracy 0.858875036239624\n",
      "Iteration 21720 Training loss 0.05226578563451767 Validation loss 0.05316707119345665 Accuracy 0.8598750233650208\n",
      "Iteration 21730 Training loss 0.04759930446743965 Validation loss 0.05352533608675003 Accuracy 0.8566250205039978\n",
      "Iteration 21740 Training loss 0.05890313908457756 Validation loss 0.05350661650300026 Accuracy 0.8572500348091125\n",
      "Iteration 21750 Training loss 0.055015504360198975 Validation loss 0.053258027881383896 Accuracy 0.8593750596046448\n",
      "Iteration 21760 Training loss 0.05324818938970566 Validation loss 0.05326303467154503 Accuracy 0.858875036239624\n",
      "Iteration 21770 Training loss 0.04949101433157921 Validation loss 0.053148139268159866 Accuracy 0.8600000143051147\n",
      "Iteration 21780 Training loss 0.056144509464502335 Validation loss 0.05313696712255478 Accuracy 0.8606250286102295\n",
      "Iteration 21790 Training loss 0.06038070470094681 Validation loss 0.053171563893556595 Accuracy 0.8606250286102295\n",
      "Iteration 21800 Training loss 0.06040486320853233 Validation loss 0.053252264857292175 Accuracy 0.8593750596046448\n",
      "Iteration 21810 Training loss 0.0558706559240818 Validation loss 0.05318109691143036 Accuracy 0.8592500686645508\n",
      "Iteration 21820 Training loss 0.052336301654577255 Validation loss 0.05311973765492439 Accuracy 0.8600000143051147\n",
      "Iteration 21830 Training loss 0.04589937999844551 Validation loss 0.05313177779316902 Accuracy 0.8592500686645508\n",
      "Iteration 21840 Training loss 0.039751384407281876 Validation loss 0.05351141467690468 Accuracy 0.8573750257492065\n",
      "Iteration 21850 Training loss 0.04610937461256981 Validation loss 0.053174939006567 Accuracy 0.8596250414848328\n",
      "Iteration 21860 Training loss 0.05021487921476364 Validation loss 0.05314399302005768 Accuracy 0.8603750467300415\n",
      "Iteration 21870 Training loss 0.049846142530441284 Validation loss 0.053504932671785355 Accuracy 0.8578750491142273\n",
      "Iteration 21880 Training loss 0.05573035031557083 Validation loss 0.05313658341765404 Accuracy 0.8596250414848328\n",
      "Iteration 21890 Training loss 0.04690557345747948 Validation loss 0.053195346146821976 Accuracy 0.8593750596046448\n",
      "Iteration 21900 Training loss 0.0475921630859375 Validation loss 0.053080156445503235 Accuracy 0.8603750467300415\n",
      "Iteration 21910 Training loss 0.0532422810792923 Validation loss 0.053096704185009 Accuracy 0.8600000143051147\n",
      "Iteration 21920 Training loss 0.05590376630425453 Validation loss 0.05315500125288963 Accuracy 0.8592500686645508\n",
      "Iteration 21930 Training loss 0.05397367477416992 Validation loss 0.05311516672372818 Accuracy 0.8605000376701355\n",
      "Iteration 21940 Training loss 0.052074041217565536 Validation loss 0.05310547351837158 Accuracy 0.859125018119812\n",
      "Iteration 21950 Training loss 0.05594315752387047 Validation loss 0.05314192548394203 Accuracy 0.8593750596046448\n",
      "Iteration 21960 Training loss 0.050743844360113144 Validation loss 0.053075242787599564 Accuracy 0.8602500557899475\n",
      "Iteration 21970 Training loss 0.04825008660554886 Validation loss 0.05305136367678642 Accuracy 0.8598750233650208\n",
      "Iteration 21980 Training loss 0.05750851333141327 Validation loss 0.05303794518113136 Accuracy 0.8593750596046448\n",
      "Iteration 21990 Training loss 0.05585165694355965 Validation loss 0.05304410681128502 Accuracy 0.8598750233650208\n",
      "Iteration 22000 Training loss 0.04214686155319214 Validation loss 0.05337578058242798 Accuracy 0.8577500581741333\n",
      "Iteration 22010 Training loss 0.047440607100725174 Validation loss 0.053366221487522125 Accuracy 0.8571250438690186\n",
      "Iteration 22020 Training loss 0.05562898516654968 Validation loss 0.05307154729962349 Accuracy 0.859000027179718\n",
      "Iteration 22030 Training loss 0.047315988689661026 Validation loss 0.05310169234871864 Accuracy 0.859125018119812\n",
      "Iteration 22040 Training loss 0.055014800280332565 Validation loss 0.053046900779008865 Accuracy 0.8597500324249268\n",
      "Iteration 22050 Training loss 0.052522141486406326 Validation loss 0.05306105688214302 Accuracy 0.85875004529953\n",
      "Iteration 22060 Training loss 0.05107958987355232 Validation loss 0.053158555179834366 Accuracy 0.858875036239624\n",
      "Iteration 22070 Training loss 0.04971519485116005 Validation loss 0.05303436517715454 Accuracy 0.8597500324249268\n",
      "Iteration 22080 Training loss 0.044549550861120224 Validation loss 0.05305151268839836 Accuracy 0.8601250648498535\n",
      "Iteration 22090 Training loss 0.053639911115169525 Validation loss 0.053251590579748154 Accuracy 0.859125018119812\n",
      "Iteration 22100 Training loss 0.04963255673646927 Validation loss 0.053395338356494904 Accuracy 0.859000027179718\n",
      "Iteration 22110 Training loss 0.04906203970313072 Validation loss 0.05304689705371857 Accuracy 0.8595000505447388\n",
      "Iteration 22120 Training loss 0.050704434514045715 Validation loss 0.05315038189291954 Accuracy 0.858875036239624\n",
      "Iteration 22130 Training loss 0.058268554508686066 Validation loss 0.053264107555150986 Accuracy 0.858625054359436\n",
      "Iteration 22140 Training loss 0.046914514154195786 Validation loss 0.05303115025162697 Accuracy 0.8598750233650208\n",
      "Iteration 22150 Training loss 0.0502249151468277 Validation loss 0.05303201824426651 Accuracy 0.8596250414848328\n",
      "Iteration 22160 Training loss 0.05154746025800705 Validation loss 0.05305415391921997 Accuracy 0.861750066280365\n",
      "Iteration 22170 Training loss 0.0534890815615654 Validation loss 0.05309341102838516 Accuracy 0.8595000505447388\n",
      "Iteration 22180 Training loss 0.06122330576181412 Validation loss 0.05300590768456459 Accuracy 0.8598750233650208\n",
      "Iteration 22190 Training loss 0.05736459419131279 Validation loss 0.053036052733659744 Accuracy 0.8600000143051147\n",
      "Iteration 22200 Training loss 0.045783694833517075 Validation loss 0.05302208289504051 Accuracy 0.8598750233650208\n",
      "Iteration 22210 Training loss 0.04455243796110153 Validation loss 0.05300401151180267 Accuracy 0.8598750233650208\n",
      "Iteration 22220 Training loss 0.04999314621090889 Validation loss 0.0530763640999794 Accuracy 0.8612500429153442\n",
      "Iteration 22230 Training loss 0.04814288020133972 Validation loss 0.05300891026854515 Accuracy 0.8603750467300415\n",
      "Iteration 22240 Training loss 0.04729384183883667 Validation loss 0.05353790521621704 Accuracy 0.8572500348091125\n",
      "Iteration 22250 Training loss 0.05369609221816063 Validation loss 0.05318053811788559 Accuracy 0.85875004529953\n",
      "Iteration 22260 Training loss 0.0543660931289196 Validation loss 0.053136326372623444 Accuracy 0.858625054359436\n",
      "Iteration 22270 Training loss 0.0553806908428669 Validation loss 0.053017374128103256 Accuracy 0.8593750596046448\n",
      "Iteration 22280 Training loss 0.05897200480103493 Validation loss 0.05317404493689537 Accuracy 0.8582500219345093\n",
      "Iteration 22290 Training loss 0.05067696422338486 Validation loss 0.05320582166314125 Accuracy 0.8580000400543213\n",
      "Iteration 22300 Training loss 0.05755942314863205 Validation loss 0.053032286465168 Accuracy 0.8605000376701355\n",
      "Iteration 22310 Training loss 0.04949541762471199 Validation loss 0.053020838648080826 Accuracy 0.8582500219345093\n",
      "Iteration 22320 Training loss 0.05711809918284416 Validation loss 0.0529770702123642 Accuracy 0.8612500429153442\n",
      "Iteration 22330 Training loss 0.05503835529088974 Validation loss 0.05293348804116249 Accuracy 0.8602500557899475\n",
      "Iteration 22340 Training loss 0.0513598807156086 Validation loss 0.05296522006392479 Accuracy 0.8592500686645508\n",
      "Iteration 22350 Training loss 0.0539994053542614 Validation loss 0.05297540873289108 Accuracy 0.8598750233650208\n",
      "Iteration 22360 Training loss 0.05308626964688301 Validation loss 0.05292985960841179 Accuracy 0.8603750467300415\n",
      "Iteration 22370 Training loss 0.045488934963941574 Validation loss 0.05297643318772316 Accuracy 0.8592500686645508\n",
      "Iteration 22380 Training loss 0.04739517718553543 Validation loss 0.052931081503629684 Accuracy 0.8598750233650208\n",
      "Iteration 22390 Training loss 0.05606709420681 Validation loss 0.05319027230143547 Accuracy 0.8578750491142273\n",
      "Iteration 22400 Training loss 0.04966964200139046 Validation loss 0.052947480231523514 Accuracy 0.8607500195503235\n",
      "Iteration 22410 Training loss 0.05420372262597084 Validation loss 0.05291792377829552 Accuracy 0.8595000505447388\n",
      "Iteration 22420 Training loss 0.054873302578926086 Validation loss 0.05301615595817566 Accuracy 0.858500063419342\n",
      "Iteration 22430 Training loss 0.052659738808870316 Validation loss 0.052933815866708755 Accuracy 0.8608750700950623\n",
      "Iteration 22440 Training loss 0.048777129501104355 Validation loss 0.05321604013442993 Accuracy 0.8582500219345093\n",
      "Iteration 22450 Training loss 0.0512831024825573 Validation loss 0.05305492877960205 Accuracy 0.8608750700950623\n",
      "Iteration 22460 Training loss 0.050598811358213425 Validation loss 0.05300119146704674 Accuracy 0.8583750128746033\n",
      "Iteration 22470 Training loss 0.04699493944644928 Validation loss 0.05322634056210518 Accuracy 0.858625054359436\n",
      "Iteration 22480 Training loss 0.04779265820980072 Validation loss 0.05344977229833603 Accuracy 0.8581250309944153\n",
      "Iteration 22490 Training loss 0.05504826083779335 Validation loss 0.05397099629044533 Accuracy 0.8565000295639038\n",
      "Iteration 22500 Training loss 0.04824160411953926 Validation loss 0.05301378294825554 Accuracy 0.8580000400543213\n",
      "Iteration 22510 Training loss 0.05744398385286331 Validation loss 0.053244635462760925 Accuracy 0.859125018119812\n",
      "Iteration 22520 Training loss 0.05097511038184166 Validation loss 0.0528901070356369 Accuracy 0.8597500324249268\n",
      "Iteration 22530 Training loss 0.04857126623392105 Validation loss 0.05297957733273506 Accuracy 0.8602500557899475\n",
      "Iteration 22540 Training loss 0.0452326275408268 Validation loss 0.0532374382019043 Accuracy 0.858625054359436\n",
      "Iteration 22550 Training loss 0.04978189244866371 Validation loss 0.05294782668352127 Accuracy 0.8592500686645508\n",
      "Iteration 22560 Training loss 0.05578373000025749 Validation loss 0.05328942462801933 Accuracy 0.8577500581741333\n",
      "Iteration 22570 Training loss 0.055233754217624664 Validation loss 0.05282670259475708 Accuracy 0.8606250286102295\n",
      "Iteration 22580 Training loss 0.05422001704573631 Validation loss 0.05294262245297432 Accuracy 0.8592500686645508\n",
      "Iteration 22590 Training loss 0.04957493022084236 Validation loss 0.052820101380348206 Accuracy 0.8610000610351562\n",
      "Iteration 22600 Training loss 0.05038914084434509 Validation loss 0.05298784002661705 Accuracy 0.858625054359436\n",
      "Iteration 22610 Training loss 0.05251162871718407 Validation loss 0.05279083177447319 Accuracy 0.8603750467300415\n",
      "Iteration 22620 Training loss 0.04401664063334465 Validation loss 0.05281383916735649 Accuracy 0.8602500557899475\n",
      "Iteration 22630 Training loss 0.05067694932222366 Validation loss 0.05292695015668869 Accuracy 0.8593750596046448\n",
      "Iteration 22640 Training loss 0.05699579045176506 Validation loss 0.0529371052980423 Accuracy 0.8593750596046448\n",
      "Iteration 22650 Training loss 0.04614618793129921 Validation loss 0.05280618742108345 Accuracy 0.8607500195503235\n",
      "Iteration 22660 Training loss 0.04582207649946213 Validation loss 0.053091861307621 Accuracy 0.858875036239624\n",
      "Iteration 22670 Training loss 0.05080382525920868 Validation loss 0.05289221182465553 Accuracy 0.858875036239624\n",
      "Iteration 22680 Training loss 0.052753522992134094 Validation loss 0.05282573029398918 Accuracy 0.8602500557899475\n",
      "Iteration 22690 Training loss 0.05103149265050888 Validation loss 0.05293514207005501 Accuracy 0.8592500686645508\n",
      "Iteration 22700 Training loss 0.046628646552562714 Validation loss 0.052817631512880325 Accuracy 0.8592500686645508\n",
      "Iteration 22710 Training loss 0.04843434318900108 Validation loss 0.0527673065662384 Accuracy 0.8600000143051147\n",
      "Iteration 22720 Training loss 0.05831003561615944 Validation loss 0.052809007465839386 Accuracy 0.8613750338554382\n",
      "Iteration 22730 Training loss 0.051159895956516266 Validation loss 0.052770864218473434 Accuracy 0.8600000143051147\n",
      "Iteration 22740 Training loss 0.04385417327284813 Validation loss 0.052747342735528946 Accuracy 0.8602500557899475\n",
      "Iteration 22750 Training loss 0.05604398250579834 Validation loss 0.05285672843456268 Accuracy 0.8602500557899475\n",
      "Iteration 22760 Training loss 0.05133616924285889 Validation loss 0.05282067507505417 Accuracy 0.8593750596046448\n",
      "Iteration 22770 Training loss 0.061290841549634933 Validation loss 0.05309082940220833 Accuracy 0.858500063419342\n",
      "Iteration 22780 Training loss 0.055882856249809265 Validation loss 0.052816640585660934 Accuracy 0.8606250286102295\n",
      "Iteration 22790 Training loss 0.04509802535176277 Validation loss 0.052980199456214905 Accuracy 0.8596250414848328\n",
      "Iteration 22800 Training loss 0.049623358994722366 Validation loss 0.05308028310537338 Accuracy 0.859125018119812\n",
      "Iteration 22810 Training loss 0.046988144516944885 Validation loss 0.0527108795940876 Accuracy 0.8606250286102295\n",
      "Iteration 22820 Training loss 0.053351230919361115 Validation loss 0.05272787809371948 Accuracy 0.8605000376701355\n",
      "Iteration 22830 Training loss 0.05935315787792206 Validation loss 0.052831728011369705 Accuracy 0.8612500429153442\n",
      "Iteration 22840 Training loss 0.04861746355891228 Validation loss 0.052701257169246674 Accuracy 0.8600000143051147\n",
      "Iteration 22850 Training loss 0.049702245742082596 Validation loss 0.052706606686115265 Accuracy 0.8615000247955322\n",
      "Iteration 22860 Training loss 0.053020376712083817 Validation loss 0.052726976573467255 Accuracy 0.862250030040741\n",
      "Iteration 22870 Training loss 0.052635423839092255 Validation loss 0.052657805383205414 Accuracy 0.8606250286102295\n",
      "Iteration 22880 Training loss 0.05712959170341492 Validation loss 0.05282192677259445 Accuracy 0.8596250414848328\n",
      "Iteration 22890 Training loss 0.04796494543552399 Validation loss 0.05281253531575203 Accuracy 0.8608750700950623\n",
      "Iteration 22900 Training loss 0.05375375598669052 Validation loss 0.052609965205192566 Accuracy 0.8616250157356262\n",
      "Iteration 22910 Training loss 0.048077523708343506 Validation loss 0.052865467965602875 Accuracy 0.8595000505447388\n",
      "Iteration 22920 Training loss 0.05260370299220085 Validation loss 0.052687402814626694 Accuracy 0.8600000143051147\n",
      "Iteration 22930 Training loss 0.04730067029595375 Validation loss 0.052631087601184845 Accuracy 0.8615000247955322\n",
      "Iteration 22940 Training loss 0.0624031238257885 Validation loss 0.05266817286610603 Accuracy 0.8611250519752502\n",
      "Iteration 22950 Training loss 0.050961095839738846 Validation loss 0.052668772637844086 Accuracy 0.8612500429153442\n",
      "Iteration 22960 Training loss 0.05320687219500542 Validation loss 0.0526731051504612 Accuracy 0.8612500429153442\n",
      "Iteration 22970 Training loss 0.05211641266942024 Validation loss 0.052628111094236374 Accuracy 0.8611250519752502\n",
      "Iteration 22980 Training loss 0.04239194840192795 Validation loss 0.052640609443187714 Accuracy 0.8606250286102295\n",
      "Iteration 22990 Training loss 0.05519821122288704 Validation loss 0.05266088247299194 Accuracy 0.8611250519752502\n",
      "Iteration 23000 Training loss 0.04376494139432907 Validation loss 0.05294566601514816 Accuracy 0.858875036239624\n",
      "Iteration 23010 Training loss 0.052124567329883575 Validation loss 0.05264627933502197 Accuracy 0.8610000610351562\n",
      "Iteration 23020 Training loss 0.04449564963579178 Validation loss 0.052596643567085266 Accuracy 0.862000048160553\n",
      "Iteration 23030 Training loss 0.05285176634788513 Validation loss 0.05274302884936333 Accuracy 0.859125018119812\n",
      "Iteration 23040 Training loss 0.04647218436002731 Validation loss 0.052629563957452774 Accuracy 0.8616250157356262\n",
      "Iteration 23050 Training loss 0.05373712256550789 Validation loss 0.05262311175465584 Accuracy 0.8607500195503235\n",
      "Iteration 23060 Training loss 0.04902331158518791 Validation loss 0.052599627524614334 Accuracy 0.861750066280365\n",
      "Iteration 23070 Training loss 0.04596288502216339 Validation loss 0.052614886313676834 Accuracy 0.8608750700950623\n",
      "Iteration 23080 Training loss 0.05461631342768669 Validation loss 0.052791059017181396 Accuracy 0.8605000376701355\n",
      "Iteration 23090 Training loss 0.05186441168189049 Validation loss 0.052842944860458374 Accuracy 0.8598750233650208\n",
      "Iteration 23100 Training loss 0.050204962491989136 Validation loss 0.05260080099105835 Accuracy 0.8603750467300415\n",
      "Iteration 23110 Training loss 0.048418886959552765 Validation loss 0.05286233127117157 Accuracy 0.8597500324249268\n",
      "Iteration 23120 Training loss 0.04737634211778641 Validation loss 0.05272836610674858 Accuracy 0.8607500195503235\n",
      "Iteration 23130 Training loss 0.05935192480683327 Validation loss 0.052626390010118484 Accuracy 0.8603750467300415\n",
      "Iteration 23140 Training loss 0.05297383293509483 Validation loss 0.052658166736364365 Accuracy 0.8600000143051147\n",
      "Iteration 23150 Training loss 0.04887143895030022 Validation loss 0.052607759833335876 Accuracy 0.8598750233650208\n",
      "Iteration 23160 Training loss 0.05577215179800987 Validation loss 0.05261721834540367 Accuracy 0.8612500429153442\n",
      "Iteration 23170 Training loss 0.04932709410786629 Validation loss 0.052555833011865616 Accuracy 0.8600000143051147\n",
      "Iteration 23180 Training loss 0.04576978459954262 Validation loss 0.05257940664887428 Accuracy 0.8612500429153442\n",
      "Iteration 23190 Training loss 0.05335104465484619 Validation loss 0.052511170506477356 Accuracy 0.8602500557899475\n",
      "Iteration 23200 Training loss 0.05138318985700607 Validation loss 0.05259628966450691 Accuracy 0.8615000247955322\n",
      "Iteration 23210 Training loss 0.05123679339885712 Validation loss 0.05250123143196106 Accuracy 0.8606250286102295\n",
      "Iteration 23220 Training loss 0.051582079380750656 Validation loss 0.052583396434783936 Accuracy 0.8597500324249268\n",
      "Iteration 23230 Training loss 0.04659871384501457 Validation loss 0.05269499495625496 Accuracy 0.8595000505447388\n",
      "Iteration 23240 Training loss 0.04625346139073372 Validation loss 0.052499208599328995 Accuracy 0.8603750467300415\n",
      "Iteration 23250 Training loss 0.047408681362867355 Validation loss 0.052550703287124634 Accuracy 0.8597500324249268\n",
      "Iteration 23260 Training loss 0.047898709774017334 Validation loss 0.0525590218603611 Accuracy 0.8601250648498535\n",
      "Iteration 23270 Training loss 0.050857409834861755 Validation loss 0.05254150554537773 Accuracy 0.8596250414848328\n",
      "Iteration 23280 Training loss 0.04373377561569214 Validation loss 0.05249727517366409 Accuracy 0.8595000505447388\n",
      "Iteration 23290 Training loss 0.043182339519262314 Validation loss 0.05246018245816231 Accuracy 0.8615000247955322\n",
      "Iteration 23300 Training loss 0.05374075844883919 Validation loss 0.052492495626211166 Accuracy 0.8612500429153442\n",
      "Iteration 23310 Training loss 0.051058877259492874 Validation loss 0.05262856185436249 Accuracy 0.8613750338554382\n",
      "Iteration 23320 Training loss 0.04454483464360237 Validation loss 0.0526193343102932 Accuracy 0.8610000610351562\n",
      "Iteration 23330 Training loss 0.05122250318527222 Validation loss 0.05250032991170883 Accuracy 0.8605000376701355\n",
      "Iteration 23340 Training loss 0.05562416836619377 Validation loss 0.05265388637781143 Accuracy 0.8600000143051147\n",
      "Iteration 23350 Training loss 0.06023591011762619 Validation loss 0.052547745406627655 Accuracy 0.8611250519752502\n",
      "Iteration 23360 Training loss 0.0497514009475708 Validation loss 0.05243847146630287 Accuracy 0.8613750338554382\n",
      "Iteration 23370 Training loss 0.050774574279785156 Validation loss 0.05271249637007713 Accuracy 0.8597500324249268\n",
      "Iteration 23380 Training loss 0.05286189913749695 Validation loss 0.05256626009941101 Accuracy 0.8601250648498535\n",
      "Iteration 23390 Training loss 0.04899827018380165 Validation loss 0.052529893815517426 Accuracy 0.8612500429153442\n",
      "Iteration 23400 Training loss 0.05453891679644585 Validation loss 0.05247221514582634 Accuracy 0.8607500195503235\n",
      "Iteration 23410 Training loss 0.05004826933145523 Validation loss 0.05244998633861542 Accuracy 0.8607500195503235\n",
      "Iteration 23420 Training loss 0.04621557518839836 Validation loss 0.052422575652599335 Accuracy 0.8613750338554382\n",
      "Iteration 23430 Training loss 0.050088070333004 Validation loss 0.052419502288103104 Accuracy 0.8615000247955322\n",
      "Iteration 23440 Training loss 0.05174138769507408 Validation loss 0.05238521099090576 Accuracy 0.8612500429153442\n",
      "Iteration 23450 Training loss 0.04649689048528671 Validation loss 0.052402157336473465 Accuracy 0.8611250519752502\n",
      "Iteration 23460 Training loss 0.04211312532424927 Validation loss 0.052521366626024246 Accuracy 0.8595000505447388\n",
      "Iteration 23470 Training loss 0.054585978388786316 Validation loss 0.05280059203505516 Accuracy 0.8597500324249268\n",
      "Iteration 23480 Training loss 0.05729000270366669 Validation loss 0.05243009328842163 Accuracy 0.8605000376701355\n",
      "Iteration 23490 Training loss 0.040492042899131775 Validation loss 0.0525222010910511 Accuracy 0.8613750338554382\n",
      "Iteration 23500 Training loss 0.03682830557227135 Validation loss 0.05279621109366417 Accuracy 0.8595000505447388\n",
      "Iteration 23510 Training loss 0.04591437056660652 Validation loss 0.05243752524256706 Accuracy 0.8598750233650208\n",
      "Iteration 23520 Training loss 0.050318386405706406 Validation loss 0.05244000628590584 Accuracy 0.8603750467300415\n",
      "Iteration 23530 Training loss 0.0410119965672493 Validation loss 0.05242370069026947 Accuracy 0.8612500429153442\n",
      "Iteration 23540 Training loss 0.04893732815980911 Validation loss 0.05259961262345314 Accuracy 0.8598750233650208\n",
      "Iteration 23550 Training loss 0.05151870474219322 Validation loss 0.05242792144417763 Accuracy 0.8612500429153442\n",
      "Iteration 23560 Training loss 0.050463028252124786 Validation loss 0.052655357867479324 Accuracy 0.8596250414848328\n",
      "Iteration 23570 Training loss 0.05056329444050789 Validation loss 0.05236727371811867 Accuracy 0.8613750338554382\n",
      "Iteration 23580 Training loss 0.058752257376909256 Validation loss 0.05237041413784027 Accuracy 0.8615000247955322\n",
      "Iteration 23590 Training loss 0.0552484393119812 Validation loss 0.05313733220100403 Accuracy 0.8592500686645508\n",
      "Iteration 23600 Training loss 0.043935880064964294 Validation loss 0.05243357643485069 Accuracy 0.8610000610351562\n",
      "Iteration 23610 Training loss 0.059658922255039215 Validation loss 0.05236139893531799 Accuracy 0.8607500195503235\n",
      "Iteration 23620 Training loss 0.04676620289683342 Validation loss 0.052316684275865555 Accuracy 0.861750066280365\n",
      "Iteration 23630 Training loss 0.04643094539642334 Validation loss 0.05236915498971939 Accuracy 0.8611250519752502\n",
      "Iteration 23640 Training loss 0.04200267791748047 Validation loss 0.052295587956905365 Accuracy 0.862000048160553\n",
      "Iteration 23650 Training loss 0.052272114902734756 Validation loss 0.05264822021126747 Accuracy 0.8606250286102295\n",
      "Iteration 23660 Training loss 0.055684249848127365 Validation loss 0.052604567259550095 Accuracy 0.8610000610351562\n",
      "Iteration 23670 Training loss 0.04568958654999733 Validation loss 0.05238042026758194 Accuracy 0.8615000247955322\n",
      "Iteration 23680 Training loss 0.04853347688913345 Validation loss 0.05242454260587692 Accuracy 0.8606250286102295\n",
      "Iteration 23690 Training loss 0.0441334992647171 Validation loss 0.05235333368182182 Accuracy 0.8612500429153442\n",
      "Iteration 23700 Training loss 0.06175926700234413 Validation loss 0.05250868573784828 Accuracy 0.8597500324249268\n",
      "Iteration 23710 Training loss 0.04497213661670685 Validation loss 0.052349191159009933 Accuracy 0.8610000610351562\n",
      "Iteration 23720 Training loss 0.05785280093550682 Validation loss 0.052750878036022186 Accuracy 0.8597500324249268\n",
      "Iteration 23730 Training loss 0.047027792781591415 Validation loss 0.05282211676239967 Accuracy 0.8597500324249268\n",
      "Iteration 23740 Training loss 0.04252723604440689 Validation loss 0.052420858293771744 Accuracy 0.8602500557899475\n",
      "Iteration 23750 Training loss 0.04661306366324425 Validation loss 0.05257406830787659 Accuracy 0.8596250414848328\n",
      "Iteration 23760 Training loss 0.04670866206288338 Validation loss 0.05249427631497383 Accuracy 0.8600000143051147\n",
      "Iteration 23770 Training loss 0.05662935972213745 Validation loss 0.052348434925079346 Accuracy 0.8608750700950623\n",
      "Iteration 23780 Training loss 0.046379994601011276 Validation loss 0.052372902631759644 Accuracy 0.8607500195503235\n",
      "Iteration 23790 Training loss 0.054854631423950195 Validation loss 0.0523497499525547 Accuracy 0.861875057220459\n",
      "Iteration 23800 Training loss 0.04880750924348831 Validation loss 0.05230396240949631 Accuracy 0.862125039100647\n",
      "Iteration 23810 Training loss 0.05055107921361923 Validation loss 0.0522882342338562 Accuracy 0.861875057220459\n",
      "Iteration 23820 Training loss 0.045578908175230026 Validation loss 0.05228744074702263 Accuracy 0.8627500534057617\n",
      "Iteration 23830 Training loss 0.048574332147836685 Validation loss 0.05230507627129555 Accuracy 0.862500011920929\n",
      "Iteration 23840 Training loss 0.057206492871046066 Validation loss 0.05249441787600517 Accuracy 0.8603750467300415\n",
      "Iteration 23850 Training loss 0.04642771929502487 Validation loss 0.05235471576452255 Accuracy 0.8615000247955322\n",
      "Iteration 23860 Training loss 0.05092328414320946 Validation loss 0.05229444429278374 Accuracy 0.862125039100647\n",
      "Iteration 23870 Training loss 0.04754564166069031 Validation loss 0.05231352150440216 Accuracy 0.862500011920929\n",
      "Iteration 23880 Training loss 0.05523200333118439 Validation loss 0.052266791462898254 Accuracy 0.862000048160553\n",
      "Iteration 23890 Training loss 0.05082398280501366 Validation loss 0.052272532135248184 Accuracy 0.862500011920929\n",
      "Iteration 23900 Training loss 0.057137381285429 Validation loss 0.05243051052093506 Accuracy 0.8608750700950623\n",
      "Iteration 23910 Training loss 0.05505027994513512 Validation loss 0.052305109798908234 Accuracy 0.861750066280365\n",
      "Iteration 23920 Training loss 0.044469527900218964 Validation loss 0.052314117550849915 Accuracy 0.862125039100647\n",
      "Iteration 23930 Training loss 0.05594389885663986 Validation loss 0.05235607549548149 Accuracy 0.8608750700950623\n",
      "Iteration 23940 Training loss 0.04622916132211685 Validation loss 0.05227915197610855 Accuracy 0.8615000247955322\n",
      "Iteration 23950 Training loss 0.05537163093686104 Validation loss 0.05232942849397659 Accuracy 0.8610000610351562\n",
      "Iteration 23960 Training loss 0.048895079642534256 Validation loss 0.05223246291279793 Accuracy 0.8616250157356262\n",
      "Iteration 23970 Training loss 0.05262542515993118 Validation loss 0.05236465111374855 Accuracy 0.8608750700950623\n",
      "Iteration 23980 Training loss 0.05075132101774216 Validation loss 0.052191972732543945 Accuracy 0.862125039100647\n",
      "Iteration 23990 Training loss 0.05379718542098999 Validation loss 0.05218721181154251 Accuracy 0.862500011920929\n",
      "Iteration 24000 Training loss 0.05349091812968254 Validation loss 0.05217508226633072 Accuracy 0.8628750443458557\n",
      "Iteration 24010 Training loss 0.04207061976194382 Validation loss 0.052145425230264664 Accuracy 0.862000048160553\n",
      "Iteration 24020 Training loss 0.043209295719861984 Validation loss 0.0521387904882431 Accuracy 0.8626250624656677\n",
      "Iteration 24030 Training loss 0.054464854300022125 Validation loss 0.052156079560518265 Accuracy 0.8616250157356262\n",
      "Iteration 24040 Training loss 0.05057825893163681 Validation loss 0.05217086896300316 Accuracy 0.8607500195503235\n",
      "Iteration 24050 Training loss 0.049300555139780045 Validation loss 0.05224334076046944 Accuracy 0.8610000610351562\n",
      "Iteration 24060 Training loss 0.05287037789821625 Validation loss 0.052132852375507355 Accuracy 0.862000048160553\n",
      "Iteration 24070 Training loss 0.05100155621767044 Validation loss 0.05214519798755646 Accuracy 0.862500011920929\n",
      "Iteration 24080 Training loss 0.05585001781582832 Validation loss 0.052525829523801804 Accuracy 0.8601250648498535\n",
      "Iteration 24090 Training loss 0.04379772022366524 Validation loss 0.05210217460989952 Accuracy 0.862500011920929\n",
      "Iteration 24100 Training loss 0.05315553396940231 Validation loss 0.05211392790079117 Accuracy 0.8630000352859497\n",
      "Iteration 24110 Training loss 0.048659056425094604 Validation loss 0.052141353487968445 Accuracy 0.862250030040741\n",
      "Iteration 24120 Training loss 0.04929029196500778 Validation loss 0.05222323164343834 Accuracy 0.8610000610351562\n",
      "Iteration 24130 Training loss 0.04854053258895874 Validation loss 0.052144855260849 Accuracy 0.862250030040741\n",
      "Iteration 24140 Training loss 0.05177152156829834 Validation loss 0.05214247480034828 Accuracy 0.8616250157356262\n",
      "Iteration 24150 Training loss 0.04389726370573044 Validation loss 0.05211569741368294 Accuracy 0.8631250262260437\n",
      "Iteration 24160 Training loss 0.04816292226314545 Validation loss 0.052146486937999725 Accuracy 0.8615000247955322\n",
      "Iteration 24170 Training loss 0.05641881003975868 Validation loss 0.05213623121380806 Accuracy 0.862125039100647\n",
      "Iteration 24180 Training loss 0.0501587837934494 Validation loss 0.052276596426963806 Accuracy 0.8610000610351562\n",
      "Iteration 24190 Training loss 0.045350152999162674 Validation loss 0.052167970687150955 Accuracy 0.8608750700950623\n",
      "Iteration 24200 Training loss 0.04992508888244629 Validation loss 0.05208829045295715 Accuracy 0.861875057220459\n",
      "Iteration 24210 Training loss 0.052330732345581055 Validation loss 0.05206657573580742 Accuracy 0.862125039100647\n",
      "Iteration 24220 Training loss 0.05356524884700775 Validation loss 0.05273247882723808 Accuracy 0.858875036239624\n",
      "Iteration 24230 Training loss 0.052582625299692154 Validation loss 0.05207853019237518 Accuracy 0.8612500429153442\n",
      "Iteration 24240 Training loss 0.049865543842315674 Validation loss 0.052141934633255005 Accuracy 0.862250030040741\n",
      "Iteration 24250 Training loss 0.0376121886074543 Validation loss 0.05210155248641968 Accuracy 0.8608750700950623\n",
      "Iteration 24260 Training loss 0.049935441464185715 Validation loss 0.05211448669433594 Accuracy 0.8606250286102295\n",
      "Iteration 24270 Training loss 0.046911418437957764 Validation loss 0.052069757133722305 Accuracy 0.8606250286102295\n",
      "Iteration 24280 Training loss 0.04678267240524292 Validation loss 0.05207282677292824 Accuracy 0.8612500429153442\n",
      "Iteration 24290 Training loss 0.047414615750312805 Validation loss 0.052090417593717575 Accuracy 0.862375020980835\n",
      "Iteration 24300 Training loss 0.05599330738186836 Validation loss 0.05220013111829758 Accuracy 0.8613750338554382\n",
      "Iteration 24310 Training loss 0.055883120745420456 Validation loss 0.05208173021674156 Accuracy 0.861750066280365\n",
      "Iteration 24320 Training loss 0.0555843822658062 Validation loss 0.05216628313064575 Accuracy 0.8610000610351562\n",
      "Iteration 24330 Training loss 0.04704222455620766 Validation loss 0.052740197628736496 Accuracy 0.8610000610351562\n",
      "Iteration 24340 Training loss 0.046506576240062714 Validation loss 0.052035827189683914 Accuracy 0.862250030040741\n",
      "Iteration 24350 Training loss 0.05093011632561684 Validation loss 0.05199860781431198 Accuracy 0.8627500534057617\n",
      "Iteration 24360 Training loss 0.05759384110569954 Validation loss 0.05200311169028282 Accuracy 0.8631250262260437\n",
      "Iteration 24370 Training loss 0.05459900572896004 Validation loss 0.05202812701463699 Accuracy 0.8630000352859497\n",
      "Iteration 24380 Training loss 0.05657466873526573 Validation loss 0.0519905723631382 Accuracy 0.862250030040741\n",
      "Iteration 24390 Training loss 0.04890363663434982 Validation loss 0.05202076584100723 Accuracy 0.861750066280365\n",
      "Iteration 24400 Training loss 0.0387054942548275 Validation loss 0.052019741386175156 Accuracy 0.862000048160553\n",
      "Iteration 24410 Training loss 0.04675712808966637 Validation loss 0.05200609192252159 Accuracy 0.8633750677108765\n",
      "Iteration 24420 Training loss 0.053258076310157776 Validation loss 0.05251681059598923 Accuracy 0.8605000376701355\n",
      "Iteration 24430 Training loss 0.04686153307557106 Validation loss 0.05204144865274429 Accuracy 0.862125039100647\n",
      "Iteration 24440 Training loss 0.045037344098091125 Validation loss 0.05201086774468422 Accuracy 0.861875057220459\n",
      "Iteration 24450 Training loss 0.04866013303399086 Validation loss 0.05199841037392616 Accuracy 0.862375020980835\n",
      "Iteration 24460 Training loss 0.05414220690727234 Validation loss 0.052250076085329056 Accuracy 0.8611250519752502\n",
      "Iteration 24470 Training loss 0.0459422767162323 Validation loss 0.05253397673368454 Accuracy 0.8602500557899475\n",
      "Iteration 24480 Training loss 0.048113156110048294 Validation loss 0.05219557136297226 Accuracy 0.8608750700950623\n",
      "Iteration 24490 Training loss 0.04968541860580444 Validation loss 0.052055757492780685 Accuracy 0.8627500534057617\n",
      "Iteration 24500 Training loss 0.05013667047023773 Validation loss 0.052215807139873505 Accuracy 0.8607500195503235\n",
      "Iteration 24510 Training loss 0.046813711524009705 Validation loss 0.05203421413898468 Accuracy 0.862250030040741\n",
      "Iteration 24520 Training loss 0.04992111027240753 Validation loss 0.05200326070189476 Accuracy 0.8633750677108765\n",
      "Iteration 24530 Training loss 0.05842253565788269 Validation loss 0.05219158157706261 Accuracy 0.8605000376701355\n",
      "Iteration 24540 Training loss 0.04463433101773262 Validation loss 0.05208215489983559 Accuracy 0.861750066280365\n",
      "Iteration 24550 Training loss 0.050570353865623474 Validation loss 0.05252508446574211 Accuracy 0.8593750596046448\n",
      "Iteration 24560 Training loss 0.052547868341207504 Validation loss 0.05200217664241791 Accuracy 0.862250030040741\n",
      "Iteration 24570 Training loss 0.05179747939109802 Validation loss 0.05194877088069916 Accuracy 0.8637500405311584\n",
      "Iteration 24580 Training loss 0.04106699302792549 Validation loss 0.05200734734535217 Accuracy 0.8616250157356262\n",
      "Iteration 24590 Training loss 0.04728111997246742 Validation loss 0.051926422864198685 Accuracy 0.862000048160553\n",
      "Iteration 24600 Training loss 0.05430231988430023 Validation loss 0.05191446840763092 Accuracy 0.8630000352859497\n",
      "Iteration 24610 Training loss 0.047070443630218506 Validation loss 0.052034251391887665 Accuracy 0.862250030040741\n",
      "Iteration 24620 Training loss 0.05376593768596649 Validation loss 0.05191487818956375 Accuracy 0.8626250624656677\n",
      "Iteration 24630 Training loss 0.05232809856534004 Validation loss 0.051939066499471664 Accuracy 0.8628750443458557\n",
      "Iteration 24640 Training loss 0.048597678542137146 Validation loss 0.05186847969889641 Accuracy 0.862375020980835\n",
      "Iteration 24650 Training loss 0.04569659009575844 Validation loss 0.05201065167784691 Accuracy 0.862375020980835\n",
      "Iteration 24660 Training loss 0.0500461719930172 Validation loss 0.05187160149216652 Accuracy 0.861750066280365\n",
      "Iteration 24670 Training loss 0.04881201684474945 Validation loss 0.051920175552368164 Accuracy 0.862375020980835\n",
      "Iteration 24680 Training loss 0.05216808244585991 Validation loss 0.051894526928663254 Accuracy 0.862500011920929\n",
      "Iteration 24690 Training loss 0.03409130126237869 Validation loss 0.051883161067962646 Accuracy 0.862375020980835\n",
      "Iteration 24700 Training loss 0.04309086874127388 Validation loss 0.051872678101062775 Accuracy 0.862250030040741\n",
      "Iteration 24710 Training loss 0.05620031803846359 Validation loss 0.05195636302232742 Accuracy 0.8616250157356262\n",
      "Iteration 24720 Training loss 0.05886329337954521 Validation loss 0.051968272775411606 Accuracy 0.8615000247955322\n",
      "Iteration 24730 Training loss 0.04737585410475731 Validation loss 0.0519152507185936 Accuracy 0.8635000586509705\n",
      "Iteration 24740 Training loss 0.04323050007224083 Validation loss 0.051895394921302795 Accuracy 0.8628750443458557\n",
      "Iteration 24750 Training loss 0.0489453487098217 Validation loss 0.05190618708729744 Accuracy 0.8628750443458557\n",
      "Iteration 24760 Training loss 0.041968945413827896 Validation loss 0.05204791575670242 Accuracy 0.8612500429153442\n",
      "Iteration 24770 Training loss 0.05148673802614212 Validation loss 0.05184795334935188 Accuracy 0.862125039100647\n",
      "Iteration 24780 Training loss 0.044995177537202835 Validation loss 0.051882579922676086 Accuracy 0.862250030040741\n",
      "Iteration 24790 Training loss 0.057816751301288605 Validation loss 0.051861170679330826 Accuracy 0.8627500534057617\n",
      "Iteration 24800 Training loss 0.048012588173151016 Validation loss 0.051840610802173615 Accuracy 0.8626250624656677\n",
      "Iteration 24810 Training loss 0.04207421839237213 Validation loss 0.05183637514710426 Accuracy 0.8626250624656677\n",
      "Iteration 24820 Training loss 0.04483281075954437 Validation loss 0.0519099086523056 Accuracy 0.8626250624656677\n",
      "Iteration 24830 Training loss 0.0443403534591198 Validation loss 0.05185903236269951 Accuracy 0.8627500534057617\n",
      "Iteration 24840 Training loss 0.049420394003391266 Validation loss 0.052119165658950806 Accuracy 0.8610000610351562\n",
      "Iteration 24850 Training loss 0.05237575247883797 Validation loss 0.05209683999419212 Accuracy 0.8613750338554382\n",
      "Iteration 24860 Training loss 0.0553598627448082 Validation loss 0.05213603749871254 Accuracy 0.8612500429153442\n",
      "Iteration 24870 Training loss 0.049995459616184235 Validation loss 0.05185447260737419 Accuracy 0.8630000352859497\n",
      "Iteration 24880 Training loss 0.05613311752676964 Validation loss 0.051878198981285095 Accuracy 0.8632500171661377\n",
      "Iteration 24890 Training loss 0.0434318482875824 Validation loss 0.0519549623131752 Accuracy 0.8627500534057617\n",
      "Iteration 24900 Training loss 0.05366257578134537 Validation loss 0.05185861885547638 Accuracy 0.862500011920929\n",
      "Iteration 24910 Training loss 0.05328739061951637 Validation loss 0.051892999559640884 Accuracy 0.862500011920929\n",
      "Iteration 24920 Training loss 0.04929820075631142 Validation loss 0.052174508571624756 Accuracy 0.862375020980835\n",
      "Iteration 24930 Training loss 0.060726601630449295 Validation loss 0.05182330682873726 Accuracy 0.862250030040741\n",
      "Iteration 24940 Training loss 0.04934006556868553 Validation loss 0.05187729001045227 Accuracy 0.861875057220459\n",
      "Iteration 24950 Training loss 0.04961128532886505 Validation loss 0.05199456959962845 Accuracy 0.862500011920929\n",
      "Iteration 24960 Training loss 0.04409581050276756 Validation loss 0.051822975277900696 Accuracy 0.8631250262260437\n",
      "Iteration 24970 Training loss 0.056741952896118164 Validation loss 0.051807016134262085 Accuracy 0.8640000224113464\n",
      "Iteration 24980 Training loss 0.0496084950864315 Validation loss 0.0519227534532547 Accuracy 0.8626250624656677\n",
      "Iteration 24990 Training loss 0.04998660832643509 Validation loss 0.051826778799295425 Accuracy 0.8635000586509705\n",
      "Iteration 25000 Training loss 0.051230695098638535 Validation loss 0.052046578377485275 Accuracy 0.861875057220459\n",
      "Iteration 25010 Training loss 0.04149812459945679 Validation loss 0.051814302802085876 Accuracy 0.862500011920929\n",
      "Iteration 25020 Training loss 0.04850177839398384 Validation loss 0.05177536979317665 Accuracy 0.8632500171661377\n",
      "Iteration 25030 Training loss 0.04929402098059654 Validation loss 0.051918551325798035 Accuracy 0.862250030040741\n",
      "Iteration 25040 Training loss 0.05549544095993042 Validation loss 0.05193132907152176 Accuracy 0.862000048160553\n",
      "Iteration 25050 Training loss 0.05585285648703575 Validation loss 0.05183761194348335 Accuracy 0.8628750443458557\n",
      "Iteration 25060 Training loss 0.04851745441555977 Validation loss 0.05181334912776947 Accuracy 0.8637500405311584\n",
      "Iteration 25070 Training loss 0.04311665892601013 Validation loss 0.051772698760032654 Accuracy 0.8633750677108765\n",
      "Iteration 25080 Training loss 0.051321204751729965 Validation loss 0.05232271924614906 Accuracy 0.8615000247955322\n",
      "Iteration 25090 Training loss 0.05034820735454559 Validation loss 0.05177430808544159 Accuracy 0.8635000586509705\n",
      "Iteration 25100 Training loss 0.05038389191031456 Validation loss 0.05182769522070885 Accuracy 0.8632500171661377\n",
      "Iteration 25110 Training loss 0.043076351284980774 Validation loss 0.051791299134492874 Accuracy 0.8638750314712524\n",
      "Iteration 25120 Training loss 0.04904583469033241 Validation loss 0.05179634317755699 Accuracy 0.8635000586509705\n",
      "Iteration 25130 Training loss 0.05096571147441864 Validation loss 0.05178247392177582 Accuracy 0.8636250495910645\n",
      "Iteration 25140 Training loss 0.05525492504239082 Validation loss 0.05212899670004845 Accuracy 0.8610000610351562\n",
      "Iteration 25150 Training loss 0.042785391211509705 Validation loss 0.05188428610563278 Accuracy 0.862500011920929\n",
      "Iteration 25160 Training loss 0.049048393964767456 Validation loss 0.05180928483605385 Accuracy 0.8635000586509705\n",
      "Iteration 25170 Training loss 0.04990389198064804 Validation loss 0.05174039304256439 Accuracy 0.862375020980835\n",
      "Iteration 25180 Training loss 0.05610664561390877 Validation loss 0.051713116466999054 Accuracy 0.8626250624656677\n",
      "Iteration 25190 Training loss 0.04091845452785492 Validation loss 0.0517193041741848 Accuracy 0.8631250262260437\n",
      "Iteration 25200 Training loss 0.04035717993974686 Validation loss 0.05183248966932297 Accuracy 0.8633750677108765\n",
      "Iteration 25210 Training loss 0.05053965374827385 Validation loss 0.0516827292740345 Accuracy 0.8635000586509705\n",
      "Iteration 25220 Training loss 0.04632196202874184 Validation loss 0.05174203962087631 Accuracy 0.8641250133514404\n",
      "Iteration 25230 Training loss 0.04224403202533722 Validation loss 0.05168452113866806 Accuracy 0.8627500534057617\n",
      "Iteration 25240 Training loss 0.046751100569963455 Validation loss 0.05177650600671768 Accuracy 0.8632500171661377\n",
      "Iteration 25250 Training loss 0.048765409737825394 Validation loss 0.0517544150352478 Accuracy 0.8632500171661377\n",
      "Iteration 25260 Training loss 0.05180354043841362 Validation loss 0.05177881196141243 Accuracy 0.8633750677108765\n",
      "Iteration 25270 Training loss 0.05199269577860832 Validation loss 0.051686063408851624 Accuracy 0.8633750677108765\n",
      "Iteration 25280 Training loss 0.046974554657936096 Validation loss 0.05188027024269104 Accuracy 0.862125039100647\n",
      "Iteration 25290 Training loss 0.03727535530924797 Validation loss 0.0517345666885376 Accuracy 0.8643750548362732\n",
      "Iteration 25300 Training loss 0.044433627277612686 Validation loss 0.05167991295456886 Accuracy 0.8626250624656677\n",
      "Iteration 25310 Training loss 0.05450555309653282 Validation loss 0.05167306587100029 Accuracy 0.8641250133514404\n",
      "Iteration 25320 Training loss 0.057407770305871964 Validation loss 0.05171993747353554 Accuracy 0.8643750548362732\n",
      "Iteration 25330 Training loss 0.0503314808011055 Validation loss 0.051825735718011856 Accuracy 0.8635000586509705\n",
      "Iteration 25340 Training loss 0.061699628829956055 Validation loss 0.051728472113609314 Accuracy 0.8632500171661377\n",
      "Iteration 25350 Training loss 0.05607186257839203 Validation loss 0.051664918661117554 Accuracy 0.8628750443458557\n",
      "Iteration 25360 Training loss 0.04399718716740608 Validation loss 0.051659274846315384 Accuracy 0.8630000352859497\n",
      "Iteration 25370 Training loss 0.05998166278004646 Validation loss 0.05178265646100044 Accuracy 0.8632500171661377\n",
      "Iteration 25380 Training loss 0.04753375053405762 Validation loss 0.05178473889827728 Accuracy 0.8636250495910645\n",
      "Iteration 25390 Training loss 0.05871514230966568 Validation loss 0.0516466461122036 Accuracy 0.862500011920929\n",
      "Iteration 25400 Training loss 0.05348780378699303 Validation loss 0.05165177956223488 Accuracy 0.8630000352859497\n",
      "Iteration 25410 Training loss 0.04342634230852127 Validation loss 0.05169939622282982 Accuracy 0.8633750677108765\n",
      "Iteration 25420 Training loss 0.046261586248874664 Validation loss 0.051670048385858536 Accuracy 0.8636250495910645\n",
      "Iteration 25430 Training loss 0.04967649281024933 Validation loss 0.05162468180060387 Accuracy 0.8638750314712524\n",
      "Iteration 25440 Training loss 0.051051631569862366 Validation loss 0.05163130536675453 Accuracy 0.8633750677108765\n",
      "Iteration 25450 Training loss 0.03512585908174515 Validation loss 0.05176004022359848 Accuracy 0.862500011920929\n",
      "Iteration 25460 Training loss 0.055575210601091385 Validation loss 0.05177348479628563 Accuracy 0.8633750677108765\n",
      "Iteration 25470 Training loss 0.05851438269019127 Validation loss 0.05177254602313042 Accuracy 0.8631250262260437\n",
      "Iteration 25480 Training loss 0.047006674110889435 Validation loss 0.05186384171247482 Accuracy 0.862500011920929\n",
      "Iteration 25490 Training loss 0.049583643674850464 Validation loss 0.051670655608177185 Accuracy 0.8628750443458557\n",
      "Iteration 25500 Training loss 0.03888934105634689 Validation loss 0.05163697898387909 Accuracy 0.8638750314712524\n",
      "Iteration 25510 Training loss 0.05183662474155426 Validation loss 0.05159858986735344 Accuracy 0.8637500405311584\n",
      "Iteration 25520 Training loss 0.06198790296912193 Validation loss 0.05159062147140503 Accuracy 0.8628750443458557\n",
      "Iteration 25530 Training loss 0.05278979241847992 Validation loss 0.05177726224064827 Accuracy 0.8632500171661377\n",
      "Iteration 25540 Training loss 0.05122818052768707 Validation loss 0.05159810557961464 Accuracy 0.8637500405311584\n",
      "Iteration 25550 Training loss 0.040838997811079025 Validation loss 0.051633045077323914 Accuracy 0.8640000224113464\n",
      "Iteration 25560 Training loss 0.047118425369262695 Validation loss 0.051626577973365784 Accuracy 0.8627500534057617\n",
      "Iteration 25570 Training loss 0.048650551587343216 Validation loss 0.05160069465637207 Accuracy 0.8638750314712524\n",
      "Iteration 25580 Training loss 0.054425809532403946 Validation loss 0.051683541387319565 Accuracy 0.8628750443458557\n",
      "Iteration 25590 Training loss 0.04300137609243393 Validation loss 0.05165323242545128 Accuracy 0.8633750677108765\n",
      "Iteration 25600 Training loss 0.05185411497950554 Validation loss 0.05253882333636284 Accuracy 0.8611250519752502\n",
      "Iteration 25610 Training loss 0.054100941866636276 Validation loss 0.051578883081674576 Accuracy 0.8631250262260437\n",
      "Iteration 25620 Training loss 0.04600496590137482 Validation loss 0.051949914544820786 Accuracy 0.8607500195503235\n",
      "Iteration 25630 Training loss 0.05482421815395355 Validation loss 0.05163601413369179 Accuracy 0.8632500171661377\n",
      "Iteration 25640 Training loss 0.04813617467880249 Validation loss 0.051588986068964005 Accuracy 0.8631250262260437\n",
      "Iteration 25650 Training loss 0.05381790176033974 Validation loss 0.051565904170274734 Accuracy 0.8633750677108765\n",
      "Iteration 25660 Training loss 0.047107961028814316 Validation loss 0.05150596797466278 Accuracy 0.8630000352859497\n",
      "Iteration 25670 Training loss 0.05043407902121544 Validation loss 0.05149739980697632 Accuracy 0.8628750443458557\n",
      "Iteration 25680 Training loss 0.051613181829452515 Validation loss 0.05167161673307419 Accuracy 0.8635000586509705\n",
      "Iteration 25690 Training loss 0.04660015180706978 Validation loss 0.0516246072947979 Accuracy 0.862375020980835\n",
      "Iteration 25700 Training loss 0.041605137288570404 Validation loss 0.051594726741313934 Accuracy 0.8626250624656677\n",
      "Iteration 25710 Training loss 0.05216364562511444 Validation loss 0.05182502046227455 Accuracy 0.862500011920929\n",
      "Iteration 25720 Training loss 0.05186837911605835 Validation loss 0.052144601941108704 Accuracy 0.862125039100647\n",
      "Iteration 25730 Training loss 0.04611041769385338 Validation loss 0.05150974541902542 Accuracy 0.8633750677108765\n",
      "Iteration 25740 Training loss 0.05042770132422447 Validation loss 0.051590751856565475 Accuracy 0.8643750548362732\n",
      "Iteration 25750 Training loss 0.04836173728108406 Validation loss 0.05188753083348274 Accuracy 0.8633750677108765\n",
      "Iteration 25760 Training loss 0.04297070577740669 Validation loss 0.05185144394636154 Accuracy 0.8633750677108765\n",
      "Iteration 25770 Training loss 0.0522732138633728 Validation loss 0.051533713936805725 Accuracy 0.8638750314712524\n",
      "Iteration 25780 Training loss 0.05077485740184784 Validation loss 0.051514819264411926 Accuracy 0.8637500405311584\n",
      "Iteration 25790 Training loss 0.054609593003988266 Validation loss 0.05149116739630699 Accuracy 0.8633750677108765\n",
      "Iteration 25800 Training loss 0.04960053041577339 Validation loss 0.05151698365807533 Accuracy 0.8633750677108765\n",
      "Iteration 25810 Training loss 0.04827279970049858 Validation loss 0.05169132351875305 Accuracy 0.862375020980835\n",
      "Iteration 25820 Training loss 0.05239621922373772 Validation loss 0.051523324102163315 Accuracy 0.8640000224113464\n",
      "Iteration 25830 Training loss 0.04997977241873741 Validation loss 0.05147989094257355 Accuracy 0.8636250495910645\n",
      "Iteration 25840 Training loss 0.05261041224002838 Validation loss 0.051453761756420135 Accuracy 0.8641250133514404\n",
      "Iteration 25850 Training loss 0.050967223942279816 Validation loss 0.051545821130275726 Accuracy 0.8636250495910645\n",
      "Iteration 25860 Training loss 0.05116714909672737 Validation loss 0.051569536328315735 Accuracy 0.8636250495910645\n",
      "Iteration 25870 Training loss 0.055643558502197266 Validation loss 0.05150386691093445 Accuracy 0.8633750677108765\n",
      "Iteration 25880 Training loss 0.04314977675676346 Validation loss 0.05145128443837166 Accuracy 0.8641250133514404\n",
      "Iteration 25890 Training loss 0.057230617851018906 Validation loss 0.05145663022994995 Accuracy 0.8632500171661377\n",
      "Iteration 25900 Training loss 0.047186899930238724 Validation loss 0.05166332796216011 Accuracy 0.862500011920929\n",
      "Iteration 25910 Training loss 0.052962664514780045 Validation loss 0.05149015411734581 Accuracy 0.8640000224113464\n",
      "Iteration 25920 Training loss 0.04455361142754555 Validation loss 0.0513874776661396 Accuracy 0.8641250133514404\n",
      "Iteration 25930 Training loss 0.04824718460440636 Validation loss 0.05152727663516998 Accuracy 0.8630000352859497\n",
      "Iteration 25940 Training loss 0.05415807664394379 Validation loss 0.05138924717903137 Accuracy 0.8646250367164612\n",
      "Iteration 25950 Training loss 0.043574023991823196 Validation loss 0.05144783854484558 Accuracy 0.8636250495910645\n",
      "Iteration 25960 Training loss 0.05176771059632301 Validation loss 0.05159398168325424 Accuracy 0.8638750314712524\n",
      "Iteration 25970 Training loss 0.048873331397771835 Validation loss 0.051364459097385406 Accuracy 0.8647500276565552\n",
      "Iteration 25980 Training loss 0.04518960416316986 Validation loss 0.05140748247504234 Accuracy 0.8646250367164612\n",
      "Iteration 25990 Training loss 0.04610147327184677 Validation loss 0.051373548805713654 Accuracy 0.8653750419616699\n",
      "Iteration 26000 Training loss 0.045343805104494095 Validation loss 0.05137254670262337 Accuracy 0.8648750185966492\n",
      "Iteration 26010 Training loss 0.05416406691074371 Validation loss 0.05137782543897629 Accuracy 0.8650000691413879\n",
      "Iteration 26020 Training loss 0.049117643386125565 Validation loss 0.051480960100889206 Accuracy 0.8648750185966492\n",
      "Iteration 26030 Training loss 0.051487404853105545 Validation loss 0.05138842388987541 Accuracy 0.8636250495910645\n",
      "Iteration 26040 Training loss 0.05507602170109749 Validation loss 0.05138318985700607 Accuracy 0.8646250367164612\n",
      "Iteration 26050 Training loss 0.052697353065013885 Validation loss 0.051526278257369995 Accuracy 0.8633750677108765\n",
      "Iteration 26060 Training loss 0.04527679830789566 Validation loss 0.05178079381585121 Accuracy 0.8633750677108765\n",
      "Iteration 26070 Training loss 0.04970358684659004 Validation loss 0.05135329067707062 Accuracy 0.8643750548362732\n",
      "Iteration 26080 Training loss 0.047006066888570786 Validation loss 0.051476601511240005 Accuracy 0.8636250495910645\n",
      "Iteration 26090 Training loss 0.05770609900355339 Validation loss 0.051363978534936905 Accuracy 0.8643750548362732\n",
      "Iteration 26100 Training loss 0.052244871854782104 Validation loss 0.051443278789520264 Accuracy 0.8631250262260437\n",
      "Iteration 26110 Training loss 0.0463499017059803 Validation loss 0.05139783024787903 Accuracy 0.8647500276565552\n",
      "Iteration 26120 Training loss 0.05060224235057831 Validation loss 0.05136191099882126 Accuracy 0.8642500638961792\n",
      "Iteration 26130 Training loss 0.05212463438510895 Validation loss 0.05163273215293884 Accuracy 0.862125039100647\n",
      "Iteration 26140 Training loss 0.04596978425979614 Validation loss 0.0513305738568306 Accuracy 0.8642500638961792\n",
      "Iteration 26150 Training loss 0.049417443573474884 Validation loss 0.05166082829236984 Accuracy 0.8631250262260437\n",
      "Iteration 26160 Training loss 0.05401177331805229 Validation loss 0.05139520391821861 Accuracy 0.8645000457763672\n",
      "Iteration 26170 Training loss 0.04579212889075279 Validation loss 0.05138900876045227 Accuracy 0.8638750314712524\n",
      "Iteration 26180 Training loss 0.04723236337304115 Validation loss 0.05133098363876343 Accuracy 0.8643750548362732\n",
      "Iteration 26190 Training loss 0.04773911461234093 Validation loss 0.051361117511987686 Accuracy 0.8643750548362732\n",
      "Iteration 26200 Training loss 0.04945850372314453 Validation loss 0.05141036584973335 Accuracy 0.8636250495910645\n",
      "Iteration 26210 Training loss 0.04817147180438042 Validation loss 0.05139525979757309 Accuracy 0.8643750548362732\n",
      "Iteration 26220 Training loss 0.04715095832943916 Validation loss 0.05135186016559601 Accuracy 0.8645000457763672\n",
      "Iteration 26230 Training loss 0.0430181510746479 Validation loss 0.051374442875385284 Accuracy 0.8635000586509705\n",
      "Iteration 26240 Training loss 0.04806745424866676 Validation loss 0.05139267072081566 Accuracy 0.8638750314712524\n",
      "Iteration 26250 Training loss 0.04469090327620506 Validation loss 0.05184777081012726 Accuracy 0.8635000586509705\n",
      "Iteration 26260 Training loss 0.05452185496687889 Validation loss 0.051291316747665405 Accuracy 0.8645000457763672\n",
      "Iteration 26270 Training loss 0.04561327025294304 Validation loss 0.05128052085638046 Accuracy 0.8643750548362732\n",
      "Iteration 26280 Training loss 0.043900325894355774 Validation loss 0.05132025480270386 Accuracy 0.8646250367164612\n",
      "Iteration 26290 Training loss 0.04635104909539223 Validation loss 0.05150336027145386 Accuracy 0.8632500171661377\n",
      "Iteration 26300 Training loss 0.04781913757324219 Validation loss 0.0512956865131855 Accuracy 0.8643750548362732\n",
      "Iteration 26310 Training loss 0.04978618025779724 Validation loss 0.05142694339156151 Accuracy 0.8641250133514404\n",
      "Iteration 26320 Training loss 0.051841139793395996 Validation loss 0.051314182579517365 Accuracy 0.8642500638961792\n",
      "Iteration 26330 Training loss 0.05059577524662018 Validation loss 0.05139901116490364 Accuracy 0.8643750548362732\n",
      "Iteration 26340 Training loss 0.05868426337838173 Validation loss 0.051292650401592255 Accuracy 0.8651250600814819\n",
      "Iteration 26350 Training loss 0.04783111438155174 Validation loss 0.05178016051650047 Accuracy 0.8633750677108765\n",
      "Iteration 26360 Training loss 0.0467468686401844 Validation loss 0.051342010498046875 Accuracy 0.8636250495910645\n",
      "Iteration 26370 Training loss 0.048419076949357986 Validation loss 0.051402315497398376 Accuracy 0.8632500171661377\n",
      "Iteration 26380 Training loss 0.04620923846960068 Validation loss 0.05154198035597801 Accuracy 0.8628750443458557\n",
      "Iteration 26390 Training loss 0.04600696638226509 Validation loss 0.051491379737854004 Accuracy 0.8636250495910645\n",
      "Iteration 26400 Training loss 0.04784165695309639 Validation loss 0.051263730973005295 Accuracy 0.8636250495910645\n",
      "Iteration 26410 Training loss 0.05195900797843933 Validation loss 0.05136507749557495 Accuracy 0.8636250495910645\n",
      "Iteration 26420 Training loss 0.04120378941297531 Validation loss 0.051252543926239014 Accuracy 0.8655000329017639\n",
      "Iteration 26430 Training loss 0.04503778740763664 Validation loss 0.05131242051720619 Accuracy 0.8636250495910645\n",
      "Iteration 26440 Training loss 0.05394134297966957 Validation loss 0.05178016424179077 Accuracy 0.862125039100647\n",
      "Iteration 26450 Training loss 0.051851775497198105 Validation loss 0.05168250575661659 Accuracy 0.8628750443458557\n",
      "Iteration 26460 Training loss 0.04920167848467827 Validation loss 0.05120787397027016 Accuracy 0.8647500276565552\n",
      "Iteration 26470 Training loss 0.05176883563399315 Validation loss 0.051305703818798065 Accuracy 0.8638750314712524\n",
      "Iteration 26480 Training loss 0.046840064227581024 Validation loss 0.05145048350095749 Accuracy 0.8632500171661377\n",
      "Iteration 26490 Training loss 0.04786459729075432 Validation loss 0.05117672309279442 Accuracy 0.8650000691413879\n",
      "Iteration 26500 Training loss 0.04940690100193024 Validation loss 0.05122605338692665 Accuracy 0.8638750314712524\n",
      "Iteration 26510 Training loss 0.050127509981393814 Validation loss 0.05129776895046234 Accuracy 0.8637500405311584\n",
      "Iteration 26520 Training loss 0.04445495456457138 Validation loss 0.0512160062789917 Accuracy 0.8647500276565552\n",
      "Iteration 26530 Training loss 0.04988409951329231 Validation loss 0.05127207562327385 Accuracy 0.8636250495910645\n",
      "Iteration 26540 Training loss 0.04959266632795334 Validation loss 0.05139991268515587 Accuracy 0.8637500405311584\n",
      "Iteration 26550 Training loss 0.05991274118423462 Validation loss 0.051277559250593185 Accuracy 0.8630000352859497\n",
      "Iteration 26560 Training loss 0.049564164131879807 Validation loss 0.051467131823301315 Accuracy 0.862500011920929\n",
      "Iteration 26570 Training loss 0.04987950995564461 Validation loss 0.05115819722414017 Accuracy 0.8648750185966492\n",
      "Iteration 26580 Training loss 0.04927223548293114 Validation loss 0.05119284987449646 Accuracy 0.8657500147819519\n",
      "Iteration 26590 Training loss 0.03967426344752312 Validation loss 0.0512176975607872 Accuracy 0.8641250133514404\n",
      "Iteration 26600 Training loss 0.04801124334335327 Validation loss 0.05112380534410477 Accuracy 0.8655000329017639\n",
      "Iteration 26610 Training loss 0.03723037987947464 Validation loss 0.051120005548000336 Accuracy 0.8633750677108765\n",
      "Iteration 26620 Training loss 0.04135308042168617 Validation loss 0.051082223653793335 Accuracy 0.8653750419616699\n",
      "Iteration 26630 Training loss 0.052867479622364044 Validation loss 0.05133510008454323 Accuracy 0.8642500638961792\n",
      "Iteration 26640 Training loss 0.04758589714765549 Validation loss 0.051107294857501984 Accuracy 0.8643750548362732\n",
      "Iteration 26650 Training loss 0.05097854137420654 Validation loss 0.051106344908475876 Accuracy 0.8647500276565552\n",
      "Iteration 26660 Training loss 0.05197596922516823 Validation loss 0.05109965056180954 Accuracy 0.8650000691413879\n",
      "Iteration 26670 Training loss 0.053795523941516876 Validation loss 0.05109759047627449 Accuracy 0.8657500147819519\n",
      "Iteration 26680 Training loss 0.04300906881690025 Validation loss 0.05134090781211853 Accuracy 0.8637500405311584\n",
      "Iteration 26690 Training loss 0.04806462302803993 Validation loss 0.05122694373130798 Accuracy 0.8653750419616699\n",
      "Iteration 26700 Training loss 0.04768700525164604 Validation loss 0.05127275362610817 Accuracy 0.8633750677108765\n",
      "Iteration 26710 Training loss 0.050190214067697525 Validation loss 0.05113444849848747 Accuracy 0.8636250495910645\n",
      "Iteration 26720 Training loss 0.05183641240000725 Validation loss 0.05122129246592522 Accuracy 0.8650000691413879\n",
      "Iteration 26730 Training loss 0.04927925020456314 Validation loss 0.051505107432603836 Accuracy 0.8636250495910645\n",
      "Iteration 26740 Training loss 0.051209867000579834 Validation loss 0.051203928887844086 Accuracy 0.8650000691413879\n",
      "Iteration 26750 Training loss 0.043434351682662964 Validation loss 0.05150756984949112 Accuracy 0.8628750443458557\n",
      "Iteration 26760 Training loss 0.04290899261832237 Validation loss 0.05104943364858627 Accuracy 0.8655000329017639\n",
      "Iteration 26770 Training loss 0.039986565709114075 Validation loss 0.051067616790533066 Accuracy 0.8651250600814819\n",
      "Iteration 26780 Training loss 0.04792490974068642 Validation loss 0.05115356296300888 Accuracy 0.8642500638961792\n",
      "Iteration 26790 Training loss 0.049498338252305984 Validation loss 0.05107911676168442 Accuracy 0.8655000329017639\n",
      "Iteration 26800 Training loss 0.04400761425495148 Validation loss 0.05105903372168541 Accuracy 0.8652500510215759\n",
      "Iteration 26810 Training loss 0.05514506623148918 Validation loss 0.05132920295000076 Accuracy 0.8635000586509705\n",
      "Iteration 26820 Training loss 0.04891326278448105 Validation loss 0.05110689252614975 Accuracy 0.8650000691413879\n",
      "Iteration 26830 Training loss 0.05281955748796463 Validation loss 0.05126626417040825 Accuracy 0.8636250495910645\n",
      "Iteration 26840 Training loss 0.045717451721429825 Validation loss 0.051117055118083954 Accuracy 0.8645000457763672\n",
      "Iteration 26850 Training loss 0.04346897825598717 Validation loss 0.05112744867801666 Accuracy 0.8657500147819519\n",
      "Iteration 26860 Training loss 0.05156494677066803 Validation loss 0.05152653902769089 Accuracy 0.862000048160553\n",
      "Iteration 26870 Training loss 0.04836464673280716 Validation loss 0.05108892172574997 Accuracy 0.8647500276565552\n",
      "Iteration 26880 Training loss 0.04204428568482399 Validation loss 0.05121023580431938 Accuracy 0.8648750185966492\n",
      "Iteration 26890 Training loss 0.05132102966308594 Validation loss 0.05132623389363289 Accuracy 0.8637500405311584\n",
      "Iteration 26900 Training loss 0.0494198352098465 Validation loss 0.05112415552139282 Accuracy 0.8650000691413879\n",
      "Iteration 26910 Training loss 0.05653444305062294 Validation loss 0.05110423266887665 Accuracy 0.8636250495910645\n",
      "Iteration 26920 Training loss 0.058215294033288956 Validation loss 0.05102655664086342 Accuracy 0.8646250367164612\n",
      "Iteration 26930 Training loss 0.04371168091893196 Validation loss 0.05107344686985016 Accuracy 0.8652500510215759\n",
      "Iteration 26940 Training loss 0.04373668506741524 Validation loss 0.05116057023406029 Accuracy 0.8645000457763672\n",
      "Iteration 26950 Training loss 0.048675019294023514 Validation loss 0.05223472788929939 Accuracy 0.8611250519752502\n",
      "Iteration 26960 Training loss 0.05456492304801941 Validation loss 0.05101429671049118 Accuracy 0.8651250600814819\n",
      "Iteration 26970 Training loss 0.05897710099816322 Validation loss 0.051001887768507004 Accuracy 0.8657500147819519\n",
      "Iteration 26980 Training loss 0.0496007464826107 Validation loss 0.05133463069796562 Accuracy 0.8626250624656677\n",
      "Iteration 26990 Training loss 0.040539950132369995 Validation loss 0.051023218780756 Accuracy 0.8638750314712524\n",
      "Iteration 27000 Training loss 0.04867887496948242 Validation loss 0.050966035574674606 Accuracy 0.8645000457763672\n",
      "Iteration 27010 Training loss 0.039785269647836685 Validation loss 0.05105945095419884 Accuracy 0.8657500147819519\n",
      "Iteration 27020 Training loss 0.05219954997301102 Validation loss 0.05138954520225525 Accuracy 0.8637500405311584\n",
      "Iteration 27030 Training loss 0.05480263754725456 Validation loss 0.050986986607313156 Accuracy 0.8652500510215759\n",
      "Iteration 27040 Training loss 0.04493863880634308 Validation loss 0.05098474398255348 Accuracy 0.8651250600814819\n",
      "Iteration 27050 Training loss 0.040306758135557175 Validation loss 0.05123814567923546 Accuracy 0.8643750548362732\n",
      "Iteration 27060 Training loss 0.04626268148422241 Validation loss 0.051012035459280014 Accuracy 0.8651250600814819\n",
      "Iteration 27070 Training loss 0.04909973964095116 Validation loss 0.05094185471534729 Accuracy 0.8656250238418579\n",
      "Iteration 27080 Training loss 0.048506371676921844 Validation loss 0.050944436341524124 Accuracy 0.8648750185966492\n",
      "Iteration 27090 Training loss 0.04666849970817566 Validation loss 0.05093935877084732 Accuracy 0.8653750419616699\n",
      "Iteration 27100 Training loss 0.043816667050123215 Validation loss 0.050973981618881226 Accuracy 0.8657500147819519\n",
      "Iteration 27110 Training loss 0.039309825748205185 Validation loss 0.050988491624593735 Accuracy 0.8651250600814819\n",
      "Iteration 27120 Training loss 0.047078292816877365 Validation loss 0.05110006779432297 Accuracy 0.8646250367164612\n",
      "Iteration 27130 Training loss 0.04997127875685692 Validation loss 0.05098381265997887 Accuracy 0.8651250600814819\n",
      "Iteration 27140 Training loss 0.042390499264001846 Validation loss 0.050972238183021545 Accuracy 0.8648750185966492\n",
      "Iteration 27150 Training loss 0.04460156336426735 Validation loss 0.051048021763563156 Accuracy 0.8647500276565552\n",
      "Iteration 27160 Training loss 0.038988154381513596 Validation loss 0.050955288112163544 Accuracy 0.8662500381469727\n",
      "Iteration 27170 Training loss 0.046518225222826004 Validation loss 0.05093434453010559 Accuracy 0.8651250600814819\n",
      "Iteration 27180 Training loss 0.04755103960633278 Validation loss 0.05113675072789192 Accuracy 0.8638750314712524\n",
      "Iteration 27190 Training loss 0.0464203916490078 Validation loss 0.05127827078104019 Accuracy 0.8650000691413879\n",
      "Iteration 27200 Training loss 0.0450056828558445 Validation loss 0.05088364705443382 Accuracy 0.8653750419616699\n",
      "Iteration 27210 Training loss 0.046224191784858704 Validation loss 0.050894610583782196 Accuracy 0.8652500510215759\n",
      "Iteration 27220 Training loss 0.049836166203022 Validation loss 0.05107957869768143 Accuracy 0.8640000224113464\n",
      "Iteration 27230 Training loss 0.05024653300642967 Validation loss 0.0509321466088295 Accuracy 0.8650000691413879\n",
      "Iteration 27240 Training loss 0.0451851487159729 Validation loss 0.05119236186146736 Accuracy 0.8638750314712524\n",
      "Iteration 27250 Training loss 0.05065349489450455 Validation loss 0.050970714539289474 Accuracy 0.8652500510215759\n",
      "Iteration 27260 Training loss 0.052313435822725296 Validation loss 0.05114298313856125 Accuracy 0.8638750314712524\n",
      "Iteration 27270 Training loss 0.04020330682396889 Validation loss 0.0511639304459095 Accuracy 0.8638750314712524\n",
      "Iteration 27280 Training loss 0.05575643852353096 Validation loss 0.05098024383187294 Accuracy 0.8660000562667847\n",
      "Iteration 27290 Training loss 0.05662931501865387 Validation loss 0.05098489299416542 Accuracy 0.8653750419616699\n",
      "Iteration 27300 Training loss 0.05616849660873413 Validation loss 0.05106198042631149 Accuracy 0.8646250367164612\n",
      "Iteration 27310 Training loss 0.03662968426942825 Validation loss 0.05129576474428177 Accuracy 0.8636250495910645\n",
      "Iteration 27320 Training loss 0.05168784037232399 Validation loss 0.05102251097559929 Accuracy 0.8651250600814819\n",
      "Iteration 27330 Training loss 0.05742586776614189 Validation loss 0.05132192373275757 Accuracy 0.8637500405311584\n",
      "Iteration 27340 Training loss 0.057527653872966766 Validation loss 0.051402054727077484 Accuracy 0.862000048160553\n",
      "Iteration 27350 Training loss 0.04527639225125313 Validation loss 0.05119717866182327 Accuracy 0.8636250495910645\n",
      "Iteration 27360 Training loss 0.04958796873688698 Validation loss 0.050889261066913605 Accuracy 0.8656250238418579\n",
      "Iteration 27370 Training loss 0.04660545289516449 Validation loss 0.05088600516319275 Accuracy 0.8658750653266907\n",
      "Iteration 27380 Training loss 0.04944726452231407 Validation loss 0.051377132534980774 Accuracy 0.861750066280365\n",
      "Iteration 27390 Training loss 0.04802697151899338 Validation loss 0.051272373646497726 Accuracy 0.8632500171661377\n",
      "Iteration 27400 Training loss 0.049758508801460266 Validation loss 0.05098939687013626 Accuracy 0.8652500510215759\n",
      "Iteration 27410 Training loss 0.05246683955192566 Validation loss 0.05107143148779869 Accuracy 0.8640000224113464\n",
      "Iteration 27420 Training loss 0.04845808818936348 Validation loss 0.05092926323413849 Accuracy 0.8648750185966492\n",
      "Iteration 27430 Training loss 0.043589625507593155 Validation loss 0.05121244117617607 Accuracy 0.8631250262260437\n",
      "Iteration 27440 Training loss 0.05452960729598999 Validation loss 0.050838273018598557 Accuracy 0.8658750653266907\n",
      "Iteration 27450 Training loss 0.050430964678525925 Validation loss 0.05115462839603424 Accuracy 0.8638750314712524\n",
      "Iteration 27460 Training loss 0.04431818053126335 Validation loss 0.05101752653717995 Accuracy 0.8655000329017639\n",
      "Iteration 27470 Training loss 0.043878406286239624 Validation loss 0.05108683928847313 Accuracy 0.8642500638961792\n",
      "Iteration 27480 Training loss 0.04162224754691124 Validation loss 0.05084966495633125 Accuracy 0.8655000329017639\n",
      "Iteration 27490 Training loss 0.05258768051862717 Validation loss 0.051238518208265305 Accuracy 0.8635000586509705\n",
      "Iteration 27500 Training loss 0.047507550567388535 Validation loss 0.050853244960308075 Accuracy 0.8650000691413879\n",
      "Iteration 27510 Training loss 0.04509410634636879 Validation loss 0.05095742642879486 Accuracy 0.8646250367164612\n",
      "Iteration 27520 Training loss 0.04216720536351204 Validation loss 0.05083784833550453 Accuracy 0.8657500147819519\n",
      "Iteration 27530 Training loss 0.041345272213220596 Validation loss 0.050927773118019104 Accuracy 0.8651250600814819\n",
      "Iteration 27540 Training loss 0.04179198294878006 Validation loss 0.05096405744552612 Accuracy 0.8652500510215759\n",
      "Iteration 27550 Training loss 0.04283018037676811 Validation loss 0.0509687140583992 Accuracy 0.8643750548362732\n",
      "Iteration 27560 Training loss 0.04367241635918617 Validation loss 0.05092143639922142 Accuracy 0.8640000224113464\n",
      "Iteration 27570 Training loss 0.04696713015437126 Validation loss 0.05086560919880867 Accuracy 0.8645000457763672\n",
      "Iteration 27580 Training loss 0.044880617409944534 Validation loss 0.05090566724538803 Accuracy 0.8661250472068787\n",
      "Iteration 27590 Training loss 0.04882313683629036 Validation loss 0.05089876800775528 Accuracy 0.8656250238418579\n",
      "Iteration 27600 Training loss 0.04583078250288963 Validation loss 0.0509532131254673 Accuracy 0.8640000224113464\n",
      "Iteration 27610 Training loss 0.06350652873516083 Validation loss 0.050823431462049484 Accuracy 0.8660000562667847\n",
      "Iteration 27620 Training loss 0.0463610403239727 Validation loss 0.050857819616794586 Accuracy 0.8660000562667847\n",
      "Iteration 27630 Training loss 0.04484172537922859 Validation loss 0.050924986600875854 Accuracy 0.8662500381469727\n",
      "Iteration 27640 Training loss 0.04486404359340668 Validation loss 0.05094463750720024 Accuracy 0.8655000329017639\n",
      "Iteration 27650 Training loss 0.045839957892894745 Validation loss 0.05089754983782768 Accuracy 0.8640000224113464\n",
      "Iteration 27660 Training loss 0.046512868255376816 Validation loss 0.05132215470075607 Accuracy 0.8640000224113464\n",
      "Iteration 27670 Training loss 0.05701381713151932 Validation loss 0.05083771049976349 Accuracy 0.8661250472068787\n",
      "Iteration 27680 Training loss 0.050317343324422836 Validation loss 0.051007404923439026 Accuracy 0.8645000457763672\n",
      "Iteration 27690 Training loss 0.05366509407758713 Validation loss 0.05081494525074959 Accuracy 0.8657500147819519\n",
      "Iteration 27700 Training loss 0.04614397883415222 Validation loss 0.05161410942673683 Accuracy 0.8637500405311584\n",
      "Iteration 27710 Training loss 0.0604875274002552 Validation loss 0.050753988325595856 Accuracy 0.8660000562667847\n",
      "Iteration 27720 Training loss 0.049099188297986984 Validation loss 0.05076710134744644 Accuracy 0.8660000562667847\n",
      "Iteration 27730 Training loss 0.045753199607133865 Validation loss 0.051772940903902054 Accuracy 0.8627500534057617\n",
      "Iteration 27740 Training loss 0.047103289514780045 Validation loss 0.05076080933213234 Accuracy 0.8651250600814819\n",
      "Iteration 27750 Training loss 0.05118582770228386 Validation loss 0.05078161507844925 Accuracy 0.8663750290870667\n",
      "Iteration 27760 Training loss 0.041784659028053284 Validation loss 0.05092090740799904 Accuracy 0.8641250133514404\n",
      "Iteration 27770 Training loss 0.05371715873479843 Validation loss 0.05085103213787079 Accuracy 0.8648750185966492\n",
      "Iteration 27780 Training loss 0.04202971234917641 Validation loss 0.05082391947507858 Accuracy 0.8648750185966492\n",
      "Iteration 27790 Training loss 0.04510252922773361 Validation loss 0.05078783631324768 Accuracy 0.8658750653266907\n",
      "Iteration 27800 Training loss 0.06290259957313538 Validation loss 0.050846587866544724 Accuracy 0.8653750419616699\n",
      "Iteration 27810 Training loss 0.04728478193283081 Validation loss 0.0513073094189167 Accuracy 0.8646250367164612\n",
      "Iteration 27820 Training loss 0.04278144612908363 Validation loss 0.05082283169031143 Accuracy 0.8650000691413879\n",
      "Iteration 27830 Training loss 0.044604651629924774 Validation loss 0.050859756767749786 Accuracy 0.8660000562667847\n",
      "Iteration 27840 Training loss 0.04881076142191887 Validation loss 0.050704989582300186 Accuracy 0.8653750419616699\n",
      "Iteration 27850 Training loss 0.055501971393823624 Validation loss 0.05152016878128052 Accuracy 0.8642500638961792\n",
      "Iteration 27860 Training loss 0.05546801909804344 Validation loss 0.05073757842183113 Accuracy 0.8656250238418579\n",
      "Iteration 27870 Training loss 0.051760587841272354 Validation loss 0.05067775771021843 Accuracy 0.8663750290870667\n",
      "Iteration 27880 Training loss 0.05131153762340546 Validation loss 0.05072567239403725 Accuracy 0.8660000562667847\n",
      "Iteration 27890 Training loss 0.043533310294151306 Validation loss 0.05069343000650406 Accuracy 0.8665000200271606\n",
      "Iteration 27900 Training loss 0.04953176900744438 Validation loss 0.05070456117391586 Accuracy 0.8658750653266907\n",
      "Iteration 27910 Training loss 0.041776444762945175 Validation loss 0.050859563052654266 Accuracy 0.8651250600814819\n",
      "Iteration 27920 Training loss 0.04853232204914093 Validation loss 0.05065513774752617 Accuracy 0.8662500381469727\n",
      "Iteration 27930 Training loss 0.05373593047261238 Validation loss 0.050748832523822784 Accuracy 0.8646250367164612\n",
      "Iteration 27940 Training loss 0.04078100621700287 Validation loss 0.05066759139299393 Accuracy 0.8658750653266907\n",
      "Iteration 27950 Training loss 0.03978320583701134 Validation loss 0.050660088658332825 Accuracy 0.8665000200271606\n",
      "Iteration 27960 Training loss 0.047804106026887894 Validation loss 0.050815608352422714 Accuracy 0.8661250472068787\n",
      "Iteration 27970 Training loss 0.05075082555413246 Validation loss 0.05069070681929588 Accuracy 0.8666250705718994\n",
      "Iteration 27980 Training loss 0.05352235585451126 Validation loss 0.050636228173971176 Accuracy 0.8661250472068787\n",
      "Iteration 27990 Training loss 0.04229726269841194 Validation loss 0.050924722105264664 Accuracy 0.8652500510215759\n",
      "Iteration 28000 Training loss 0.04706476256251335 Validation loss 0.05064288526773453 Accuracy 0.8667500615119934\n",
      "Iteration 28010 Training loss 0.05050479620695114 Validation loss 0.05067458376288414 Accuracy 0.8671250343322754\n",
      "Iteration 28020 Training loss 0.05107670649886131 Validation loss 0.05112878978252411 Accuracy 0.8637500405311584\n",
      "Iteration 28030 Training loss 0.04602811858057976 Validation loss 0.05085653066635132 Accuracy 0.8662500381469727\n",
      "Iteration 28040 Training loss 0.03738142549991608 Validation loss 0.050851717591285706 Accuracy 0.8657500147819519\n",
      "Iteration 28050 Training loss 0.04121715575456619 Validation loss 0.05069303885102272 Accuracy 0.8652500510215759\n",
      "Iteration 28060 Training loss 0.046905163675546646 Validation loss 0.050708286464214325 Accuracy 0.8666250705718994\n",
      "Iteration 28070 Training loss 0.048839833587408066 Validation loss 0.05079608038067818 Accuracy 0.8661250472068787\n",
      "Iteration 28080 Training loss 0.05247541144490242 Validation loss 0.050696924328804016 Accuracy 0.8658750653266907\n",
      "Iteration 28090 Training loss 0.04650930315256119 Validation loss 0.05061287805438042 Accuracy 0.8672500252723694\n",
      "Iteration 28100 Training loss 0.05084481090307236 Validation loss 0.050604045391082764 Accuracy 0.8665000200271606\n",
      "Iteration 28110 Training loss 0.04061258211731911 Validation loss 0.0505998395383358 Accuracy 0.8661250472068787\n",
      "Iteration 28120 Training loss 0.05113692954182625 Validation loss 0.05075482279062271 Accuracy 0.8647500276565552\n",
      "Iteration 28130 Training loss 0.052333611994981766 Validation loss 0.05059904232621193 Accuracy 0.8672500252723694\n",
      "Iteration 28140 Training loss 0.03989898040890694 Validation loss 0.05066458880901337 Accuracy 0.8675000667572021\n",
      "Iteration 28150 Training loss 0.04637187719345093 Validation loss 0.0506972074508667 Accuracy 0.8670000433921814\n",
      "Iteration 28160 Training loss 0.04120596498250961 Validation loss 0.05100373178720474 Accuracy 0.8661250472068787\n",
      "Iteration 28170 Training loss 0.04546409845352173 Validation loss 0.050693903118371964 Accuracy 0.8668750524520874\n",
      "Iteration 28180 Training loss 0.051928263157606125 Validation loss 0.05074309557676315 Accuracy 0.8665000200271606\n",
      "Iteration 28190 Training loss 0.049626629799604416 Validation loss 0.05064884573221207 Accuracy 0.8653750419616699\n",
      "Iteration 28200 Training loss 0.054156165570020676 Validation loss 0.05081569030880928 Accuracy 0.8661250472068787\n",
      "Iteration 28210 Training loss 0.04047868773341179 Validation loss 0.05060160905122757 Accuracy 0.8656250238418579\n",
      "Iteration 28220 Training loss 0.047226179391145706 Validation loss 0.050694726407527924 Accuracy 0.8652500510215759\n",
      "Iteration 28230 Training loss 0.04180426523089409 Validation loss 0.05065307393670082 Accuracy 0.8672500252723694\n",
      "Iteration 28240 Training loss 0.05123616009950638 Validation loss 0.05099012330174446 Accuracy 0.8628750443458557\n",
      "Iteration 28250 Training loss 0.04233879595994949 Validation loss 0.050572264939546585 Accuracy 0.8660000562667847\n",
      "Iteration 28260 Training loss 0.04023444652557373 Validation loss 0.05056067928671837 Accuracy 0.8667500615119934\n",
      "Iteration 28270 Training loss 0.047279518097639084 Validation loss 0.0506129115819931 Accuracy 0.8665000200271606\n",
      "Iteration 28280 Training loss 0.04873758554458618 Validation loss 0.050653908401727676 Accuracy 0.8658750653266907\n",
      "Iteration 28290 Training loss 0.053399305790662766 Validation loss 0.05063130706548691 Accuracy 0.8673750162124634\n",
      "Iteration 28300 Training loss 0.058294426649808884 Validation loss 0.05081108957529068 Accuracy 0.8655000329017639\n",
      "Iteration 28310 Training loss 0.04622148349881172 Validation loss 0.050572190433740616 Accuracy 0.8673750162124634\n",
      "Iteration 28320 Training loss 0.04650646075606346 Validation loss 0.050520624965429306 Accuracy 0.8665000200271606\n",
      "Iteration 28330 Training loss 0.04771241173148155 Validation loss 0.0507720448076725 Accuracy 0.8663750290870667\n",
      "Iteration 28340 Training loss 0.049731653183698654 Validation loss 0.0506526380777359 Accuracy 0.8657500147819519\n",
      "Iteration 28350 Training loss 0.05552470684051514 Validation loss 0.05088163912296295 Accuracy 0.8666250705718994\n",
      "Iteration 28360 Training loss 0.045768335461616516 Validation loss 0.05051460862159729 Accuracy 0.8687500357627869\n",
      "Iteration 28370 Training loss 0.04253308102488518 Validation loss 0.05094693973660469 Accuracy 0.8656250238418579\n",
      "Iteration 28380 Training loss 0.04388531669974327 Validation loss 0.05047913268208504 Accuracy 0.8680000305175781\n",
      "Iteration 28390 Training loss 0.05742167681455612 Validation loss 0.05053222179412842 Accuracy 0.8671250343322754\n",
      "Iteration 28400 Training loss 0.04488882049918175 Validation loss 0.05056034028530121 Accuracy 0.8672500252723694\n",
      "Iteration 28410 Training loss 0.05442512780427933 Validation loss 0.05046199634671211 Accuracy 0.8673750162124634\n",
      "Iteration 28420 Training loss 0.03963566944003105 Validation loss 0.05051148310303688 Accuracy 0.8671250343322754\n",
      "Iteration 28430 Training loss 0.04366043955087662 Validation loss 0.050492435693740845 Accuracy 0.8672500252723694\n",
      "Iteration 28440 Training loss 0.04370943456888199 Validation loss 0.05059892684221268 Accuracy 0.8670000433921814\n",
      "Iteration 28450 Training loss 0.03588264808058739 Validation loss 0.050465814769268036 Accuracy 0.8668750524520874\n",
      "Iteration 28460 Training loss 0.04547892510890961 Validation loss 0.050523050129413605 Accuracy 0.8668750524520874\n",
      "Iteration 28470 Training loss 0.04416915774345398 Validation loss 0.0505690723657608 Accuracy 0.8672500252723694\n",
      "Iteration 28480 Training loss 0.056571029126644135 Validation loss 0.050512537360191345 Accuracy 0.8675000667572021\n",
      "Iteration 28490 Training loss 0.04403848573565483 Validation loss 0.05056074261665344 Accuracy 0.8658750653266907\n",
      "Iteration 28500 Training loss 0.040203265845775604 Validation loss 0.050461556762456894 Accuracy 0.8670000433921814\n",
      "Iteration 28510 Training loss 0.05392194166779518 Validation loss 0.05070074647665024 Accuracy 0.8638750314712524\n",
      "Iteration 28520 Training loss 0.0518643856048584 Validation loss 0.050617121160030365 Accuracy 0.8648750185966492\n",
      "Iteration 28530 Training loss 0.05076289921998978 Validation loss 0.050422873347997665 Accuracy 0.8670000433921814\n",
      "Iteration 28540 Training loss 0.047768499702215195 Validation loss 0.050511304289102554 Accuracy 0.8660000562667847\n",
      "Iteration 28550 Training loss 0.0565938875079155 Validation loss 0.050390321761369705 Accuracy 0.8678750395774841\n",
      "Iteration 28560 Training loss 0.050323743373155594 Validation loss 0.05041130259633064 Accuracy 0.8677500486373901\n",
      "Iteration 28570 Training loss 0.0523005872964859 Validation loss 0.0505281463265419 Accuracy 0.8668750524520874\n",
      "Iteration 28580 Training loss 0.05069582164287567 Validation loss 0.050431422889232635 Accuracy 0.8665000200271606\n",
      "Iteration 28590 Training loss 0.048455409705638885 Validation loss 0.050546687096357346 Accuracy 0.8655000329017639\n",
      "Iteration 28600 Training loss 0.04721830412745476 Validation loss 0.050578538328409195 Accuracy 0.8658750653266907\n",
      "Iteration 28610 Training loss 0.0448756106197834 Validation loss 0.05056087672710419 Accuracy 0.8668750524520874\n",
      "Iteration 28620 Training loss 0.0445474311709404 Validation loss 0.05038050562143326 Accuracy 0.8668750524520874\n",
      "Iteration 28630 Training loss 0.043070316314697266 Validation loss 0.05040310323238373 Accuracy 0.8670000433921814\n",
      "Iteration 28640 Training loss 0.043781936168670654 Validation loss 0.05050364136695862 Accuracy 0.8673750162124634\n",
      "Iteration 28650 Training loss 0.0465841181576252 Validation loss 0.05041857063770294 Accuracy 0.8675000667572021\n",
      "Iteration 28660 Training loss 0.04860846325755119 Validation loss 0.050402186810970306 Accuracy 0.8681250214576721\n",
      "Iteration 28670 Training loss 0.04868033155798912 Validation loss 0.050394438207149506 Accuracy 0.8667500615119934\n",
      "Iteration 28680 Training loss 0.048679009079933167 Validation loss 0.050571221858263016 Accuracy 0.8658750653266907\n",
      "Iteration 28690 Training loss 0.050978031009435654 Validation loss 0.05041133239865303 Accuracy 0.8678750395774841\n",
      "Iteration 28700 Training loss 0.05076197534799576 Validation loss 0.050448060035705566 Accuracy 0.8665000200271606\n",
      "Iteration 28710 Training loss 0.048347968608140945 Validation loss 0.05037938058376312 Accuracy 0.8678750395774841\n",
      "Iteration 28720 Training loss 0.04448819160461426 Validation loss 0.050471000373363495 Accuracy 0.8682500123977661\n",
      "Iteration 28730 Training loss 0.05311640724539757 Validation loss 0.05044789984822273 Accuracy 0.8676250576972961\n",
      "Iteration 28740 Training loss 0.046673670411109924 Validation loss 0.050473157316446304 Accuracy 0.8656250238418579\n",
      "Iteration 28750 Training loss 0.045663654804229736 Validation loss 0.050376035273075104 Accuracy 0.8677500486373901\n",
      "Iteration 28760 Training loss 0.047014884650707245 Validation loss 0.0509641058743 Accuracy 0.8647500276565552\n",
      "Iteration 28770 Training loss 0.05107437074184418 Validation loss 0.050573088228702545 Accuracy 0.8666250705718994\n",
      "Iteration 28780 Training loss 0.05308568850159645 Validation loss 0.050326310098171234 Accuracy 0.8671250343322754\n",
      "Iteration 28790 Training loss 0.04151124507188797 Validation loss 0.05035887286067009 Accuracy 0.8673750162124634\n",
      "Iteration 28800 Training loss 0.0569915771484375 Validation loss 0.05035590007901192 Accuracy 0.8680000305175781\n",
      "Iteration 28810 Training loss 0.05392534285783768 Validation loss 0.050344325602054596 Accuracy 0.8678750395774841\n",
      "Iteration 28820 Training loss 0.03860343247652054 Validation loss 0.05050346627831459 Accuracy 0.8675000667572021\n",
      "Iteration 28830 Training loss 0.04533139243721962 Validation loss 0.05127142742276192 Accuracy 0.8647500276565552\n",
      "Iteration 28840 Training loss 0.044208768755197525 Validation loss 0.05035989359021187 Accuracy 0.8682500123977661\n",
      "Iteration 28850 Training loss 0.04003698751330376 Validation loss 0.05045349895954132 Accuracy 0.8671250343322754\n",
      "Iteration 28860 Training loss 0.04254809394478798 Validation loss 0.05028171092271805 Accuracy 0.8680000305175781\n",
      "Iteration 28870 Training loss 0.040714800357818604 Validation loss 0.05027400329709053 Accuracy 0.8685000538825989\n",
      "Iteration 28880 Training loss 0.04900599271059036 Validation loss 0.05024202913045883 Accuracy 0.8690000176429749\n",
      "Iteration 28890 Training loss 0.04189961776137352 Validation loss 0.05030366778373718 Accuracy 0.8678750395774841\n",
      "Iteration 28900 Training loss 0.0474916435778141 Validation loss 0.05107078328728676 Accuracy 0.8658750653266907\n",
      "Iteration 28910 Training loss 0.052137039601802826 Validation loss 0.0502978079020977 Accuracy 0.8678750395774841\n",
      "Iteration 28920 Training loss 0.045664120465517044 Validation loss 0.050301164388656616 Accuracy 0.8675000667572021\n",
      "Iteration 28930 Training loss 0.05273885279893875 Validation loss 0.05030816048383713 Accuracy 0.8676250576972961\n",
      "Iteration 28940 Training loss 0.04448306933045387 Validation loss 0.05039402097463608 Accuracy 0.8671250343322754\n",
      "Iteration 28950 Training loss 0.05951869860291481 Validation loss 0.05034814029932022 Accuracy 0.8667500615119934\n",
      "Iteration 28960 Training loss 0.04879390075802803 Validation loss 0.05031084641814232 Accuracy 0.8662500381469727\n",
      "Iteration 28970 Training loss 0.04538591206073761 Validation loss 0.05035433545708656 Accuracy 0.8671250343322754\n",
      "Iteration 28980 Training loss 0.043192308396101 Validation loss 0.0503205768764019 Accuracy 0.8666250705718994\n",
      "Iteration 28990 Training loss 0.04743044823408127 Validation loss 0.05030052363872528 Accuracy 0.8676250576972961\n",
      "Iteration 29000 Training loss 0.0478285476565361 Validation loss 0.05028562620282173 Accuracy 0.8680000305175781\n",
      "Iteration 29010 Training loss 0.059460777789354324 Validation loss 0.05031198263168335 Accuracy 0.8671250343322754\n",
      "Iteration 29020 Training loss 0.05562613904476166 Validation loss 0.050456706434488297 Accuracy 0.8672500252723694\n",
      "Iteration 29030 Training loss 0.04334506019949913 Validation loss 0.05028262734413147 Accuracy 0.8680000305175781\n",
      "Iteration 29040 Training loss 0.046756647527217865 Validation loss 0.0506444051861763 Accuracy 0.8662500381469727\n",
      "Iteration 29050 Training loss 0.05423319339752197 Validation loss 0.05036517605185509 Accuracy 0.8675000667572021\n",
      "Iteration 29060 Training loss 0.047956470400094986 Validation loss 0.050384730100631714 Accuracy 0.8672500252723694\n",
      "Iteration 29070 Training loss 0.05598278343677521 Validation loss 0.05057704821228981 Accuracy 0.8666250705718994\n",
      "Iteration 29080 Training loss 0.041417840868234634 Validation loss 0.05034251883625984 Accuracy 0.8673750162124634\n",
      "Iteration 29090 Training loss 0.03935901075601578 Validation loss 0.05033735930919647 Accuracy 0.8668750524520874\n",
      "Iteration 29100 Training loss 0.04138088971376419 Validation loss 0.05031171068549156 Accuracy 0.8671250343322754\n",
      "Iteration 29110 Training loss 0.04955591633915901 Validation loss 0.050465237349271774 Accuracy 0.8676250576972961\n",
      "Iteration 29120 Training loss 0.048585906624794006 Validation loss 0.05045527219772339 Accuracy 0.8660000562667847\n",
      "Iteration 29130 Training loss 0.04671677574515343 Validation loss 0.05027342587709427 Accuracy 0.8680000305175781\n",
      "Iteration 29140 Training loss 0.0423285998404026 Validation loss 0.050235699862241745 Accuracy 0.8682500123977661\n",
      "Iteration 29150 Training loss 0.04360764101147652 Validation loss 0.05042245611548424 Accuracy 0.8668750524520874\n",
      "Iteration 29160 Training loss 0.0499972440302372 Validation loss 0.05060858279466629 Accuracy 0.8660000562667847\n",
      "Iteration 29170 Training loss 0.05477086827158928 Validation loss 0.05047304928302765 Accuracy 0.8668750524520874\n",
      "Iteration 29180 Training loss 0.03740890324115753 Validation loss 0.050499483942985535 Accuracy 0.8670000433921814\n",
      "Iteration 29190 Training loss 0.04399007558822632 Validation loss 0.05025028437376022 Accuracy 0.8686250448226929\n",
      "Iteration 29200 Training loss 0.044172100722789764 Validation loss 0.05022953823208809 Accuracy 0.8687500357627869\n",
      "Iteration 29210 Training loss 0.04107336327433586 Validation loss 0.05028267949819565 Accuracy 0.8665000200271606\n",
      "Iteration 29220 Training loss 0.04440625384449959 Validation loss 0.050276871770620346 Accuracy 0.8671250343322754\n",
      "Iteration 29230 Training loss 0.04044385999441147 Validation loss 0.0502653494477272 Accuracy 0.8678750395774841\n",
      "Iteration 29240 Training loss 0.04656078293919563 Validation loss 0.050324007868766785 Accuracy 0.8660000562667847\n",
      "Iteration 29250 Training loss 0.05142466351389885 Validation loss 0.050298430025577545 Accuracy 0.8668750524520874\n",
      "Iteration 29260 Training loss 0.047426749020814896 Validation loss 0.05025414749979973 Accuracy 0.8678750395774841\n",
      "Iteration 29270 Training loss 0.04891632869839668 Validation loss 0.05026562139391899 Accuracy 0.8665000200271606\n",
      "Iteration 29280 Training loss 0.04643673822283745 Validation loss 0.05020378902554512 Accuracy 0.8686250448226929\n",
      "Iteration 29290 Training loss 0.04682965949177742 Validation loss 0.05046658590435982 Accuracy 0.8661250472068787\n",
      "Iteration 29300 Training loss 0.05236268788576126 Validation loss 0.050212617963552475 Accuracy 0.8668750524520874\n",
      "Iteration 29310 Training loss 0.048515308648347855 Validation loss 0.05019429326057434 Accuracy 0.8673750162124634\n",
      "Iteration 29320 Training loss 0.05518653616309166 Validation loss 0.050226885825395584 Accuracy 0.8672500252723694\n",
      "Iteration 29330 Training loss 0.04891530051827431 Validation loss 0.05017685890197754 Accuracy 0.8685000538825989\n",
      "Iteration 29340 Training loss 0.04574356973171234 Validation loss 0.050179820507764816 Accuracy 0.8692500591278076\n",
      "Iteration 29350 Training loss 0.04693220183253288 Validation loss 0.050163302570581436 Accuracy 0.8686250448226929\n",
      "Iteration 29360 Training loss 0.051037974655628204 Validation loss 0.05034423992037773 Accuracy 0.8663750290870667\n",
      "Iteration 29370 Training loss 0.045782286673784256 Validation loss 0.050163935869932175 Accuracy 0.8680000305175781\n",
      "Iteration 29380 Training loss 0.05193376541137695 Validation loss 0.050366420298814774 Accuracy 0.8682500123977661\n",
      "Iteration 29390 Training loss 0.03817848488688469 Validation loss 0.050130587071180344 Accuracy 0.8685000538825989\n",
      "Iteration 29400 Training loss 0.04269370064139366 Validation loss 0.0501512810587883 Accuracy 0.8682500123977661\n",
      "Iteration 29410 Training loss 0.04879068583250046 Validation loss 0.05016764625906944 Accuracy 0.8673750162124634\n",
      "Iteration 29420 Training loss 0.045578110963106155 Validation loss 0.05010191723704338 Accuracy 0.8675000667572021\n",
      "Iteration 29430 Training loss 0.04530256241559982 Validation loss 0.050287097692489624 Accuracy 0.8662500381469727\n",
      "Iteration 29440 Training loss 0.05210479721426964 Validation loss 0.05007254332304001 Accuracy 0.8685000538825989\n",
      "Iteration 29450 Training loss 0.0403897799551487 Validation loss 0.05008343234658241 Accuracy 0.8682500123977661\n",
      "Iteration 29460 Training loss 0.050699956715106964 Validation loss 0.05021427571773529 Accuracy 0.8673750162124634\n",
      "Iteration 29470 Training loss 0.04927199333906174 Validation loss 0.05021991208195686 Accuracy 0.8670000433921814\n",
      "Iteration 29480 Training loss 0.04128658026456833 Validation loss 0.050114694982767105 Accuracy 0.8685000538825989\n",
      "Iteration 29490 Training loss 0.05338194593787193 Validation loss 0.05051812157034874 Accuracy 0.8658750653266907\n",
      "Iteration 29500 Training loss 0.05888766050338745 Validation loss 0.050615016371011734 Accuracy 0.8632500171661377\n",
      "Iteration 29510 Training loss 0.04876463860273361 Validation loss 0.05009178817272186 Accuracy 0.8672500252723694\n",
      "Iteration 29520 Training loss 0.05032970383763313 Validation loss 0.050080880522727966 Accuracy 0.8687500357627869\n",
      "Iteration 29530 Training loss 0.042808130383491516 Validation loss 0.05064735934138298 Accuracy 0.8668750524520874\n",
      "Iteration 29540 Training loss 0.04474479705095291 Validation loss 0.05033290386199951 Accuracy 0.8675000667572021\n",
      "Iteration 29550 Training loss 0.042153749614953995 Validation loss 0.0501258485019207 Accuracy 0.8670000433921814\n",
      "Iteration 29560 Training loss 0.04948972165584564 Validation loss 0.050272975116968155 Accuracy 0.8662500381469727\n",
      "Iteration 29570 Training loss 0.04507274180650711 Validation loss 0.050257351249456406 Accuracy 0.8673750162124634\n",
      "Iteration 29580 Training loss 0.04020193964242935 Validation loss 0.05058746412396431 Accuracy 0.8662500381469727\n",
      "Iteration 29590 Training loss 0.04471760615706444 Validation loss 0.05005183070898056 Accuracy 0.8682500123977661\n",
      "Iteration 29600 Training loss 0.0458843894302845 Validation loss 0.050059858709573746 Accuracy 0.8687500357627869\n",
      "Iteration 29610 Training loss 0.045628275722265244 Validation loss 0.05010102316737175 Accuracy 0.8677500486373901\n",
      "Iteration 29620 Training loss 0.036040689796209335 Validation loss 0.05021478608250618 Accuracy 0.8658750653266907\n",
      "Iteration 29630 Training loss 0.05131494998931885 Validation loss 0.05009216442704201 Accuracy 0.8680000305175781\n",
      "Iteration 29640 Training loss 0.04870140925049782 Validation loss 0.05012356862425804 Accuracy 0.8675000667572021\n",
      "Iteration 29650 Training loss 0.044053979218006134 Validation loss 0.05009147897362709 Accuracy 0.8682500123977661\n",
      "Iteration 29660 Training loss 0.05095761641860008 Validation loss 0.050038307905197144 Accuracy 0.8695000410079956\n",
      "Iteration 29670 Training loss 0.04435458034276962 Validation loss 0.05006934329867363 Accuracy 0.8687500357627869\n",
      "Iteration 29680 Training loss 0.04171547666192055 Validation loss 0.05025535821914673 Accuracy 0.8678750395774841\n",
      "Iteration 29690 Training loss 0.04595368355512619 Validation loss 0.05008435249328613 Accuracy 0.8682500123977661\n",
      "Iteration 29700 Training loss 0.055454835295677185 Validation loss 0.04999595135450363 Accuracy 0.8697500228881836\n",
      "Iteration 29710 Training loss 0.046813104301691055 Validation loss 0.0499904491007328 Accuracy 0.8688750267028809\n",
      "Iteration 29720 Training loss 0.046766314655542374 Validation loss 0.05004534497857094 Accuracy 0.8691250681877136\n",
      "Iteration 29730 Training loss 0.0488450787961483 Validation loss 0.05026772990822792 Accuracy 0.8663750290870667\n",
      "Iteration 29740 Training loss 0.05746918171644211 Validation loss 0.04996079206466675 Accuracy 0.8695000410079956\n",
      "Iteration 29750 Training loss 0.049354199320077896 Validation loss 0.050128206610679626 Accuracy 0.8681250214576721\n",
      "Iteration 29760 Training loss 0.051235608756542206 Validation loss 0.05026108771562576 Accuracy 0.8660000562667847\n",
      "Iteration 29770 Training loss 0.052719488739967346 Validation loss 0.04996013268828392 Accuracy 0.8696250319480896\n",
      "Iteration 29780 Training loss 0.038337551057338715 Validation loss 0.05034342408180237 Accuracy 0.8673750162124634\n",
      "Iteration 29790 Training loss 0.04529157653450966 Validation loss 0.04994082823395729 Accuracy 0.8681250214576721\n",
      "Iteration 29800 Training loss 0.05429317057132721 Validation loss 0.05025564879179001 Accuracy 0.8682500123977661\n",
      "Iteration 29810 Training loss 0.04712922126054764 Validation loss 0.04995999112725258 Accuracy 0.8697500228881836\n",
      "Iteration 29820 Training loss 0.04155413433909416 Validation loss 0.049979519098997116 Accuracy 0.8692500591278076\n",
      "Iteration 29830 Training loss 0.05748153105378151 Validation loss 0.05009876564145088 Accuracy 0.8683750629425049\n",
      "Iteration 29840 Training loss 0.045766446739435196 Validation loss 0.04997529461979866 Accuracy 0.8691250681877136\n",
      "Iteration 29850 Training loss 0.05191687494516373 Validation loss 0.04996873810887337 Accuracy 0.8681250214576721\n",
      "Iteration 29860 Training loss 0.05594668164849281 Validation loss 0.04992283135652542 Accuracy 0.8693750500679016\n",
      "Iteration 29870 Training loss 0.0428566075861454 Validation loss 0.049912240356206894 Accuracy 0.8692500591278076\n",
      "Iteration 29880 Training loss 0.055435530841350555 Validation loss 0.049920082092285156 Accuracy 0.8692500591278076\n",
      "Iteration 29890 Training loss 0.040729276835918427 Validation loss 0.05001053214073181 Accuracy 0.8680000305175781\n",
      "Iteration 29900 Training loss 0.04392743110656738 Validation loss 0.05010027810931206 Accuracy 0.8670000433921814\n",
      "Iteration 29910 Training loss 0.04507480561733246 Validation loss 0.04994874820113182 Accuracy 0.8690000176429749\n",
      "Iteration 29920 Training loss 0.050802797079086304 Validation loss 0.04997361823916435 Accuracy 0.8686250448226929\n",
      "Iteration 29930 Training loss 0.04517132788896561 Validation loss 0.04995564743876457 Accuracy 0.8685000538825989\n",
      "Iteration 29940 Training loss 0.04714201018214226 Validation loss 0.050116755068302155 Accuracy 0.8673750162124634\n",
      "Iteration 29950 Training loss 0.04164566844701767 Validation loss 0.05002974346280098 Accuracy 0.8672500252723694\n",
      "Iteration 29960 Training loss 0.05292782559990883 Validation loss 0.049941424280405045 Accuracy 0.8688750267028809\n",
      "Iteration 29970 Training loss 0.046926967799663544 Validation loss 0.04991902410984039 Accuracy 0.8696250319480896\n",
      "Iteration 29980 Training loss 0.0507417768239975 Validation loss 0.0510089211165905 Accuracy 0.8646250367164612\n",
      "Iteration 29990 Training loss 0.04757130891084671 Validation loss 0.049943070858716965 Accuracy 0.8688750267028809\n",
      "Iteration 30000 Training loss 0.0443638414144516 Validation loss 0.05000186711549759 Accuracy 0.8680000305175781\n",
      "Iteration 30010 Training loss 0.04520739987492561 Validation loss 0.050026100128889084 Accuracy 0.8678750395774841\n",
      "Iteration 30020 Training loss 0.05194399878382683 Validation loss 0.050095949321985245 Accuracy 0.8682500123977661\n",
      "Iteration 30030 Training loss 0.0502854660153389 Validation loss 0.049960389733314514 Accuracy 0.8687500357627869\n",
      "Iteration 30040 Training loss 0.04712678864598274 Validation loss 0.05027179792523384 Accuracy 0.8682500123977661\n",
      "Iteration 30050 Training loss 0.04327134042978287 Validation loss 0.049953754991292953 Accuracy 0.8672500252723694\n",
      "Iteration 30060 Training loss 0.051584843546152115 Validation loss 0.04988939315080643 Accuracy 0.8698750138282776\n",
      "Iteration 30070 Training loss 0.04594840854406357 Validation loss 0.05038653686642647 Accuracy 0.8663750290870667\n",
      "Iteration 30080 Training loss 0.04515208303928375 Validation loss 0.05002908408641815 Accuracy 0.8673750162124634\n",
      "Iteration 30090 Training loss 0.04286831617355347 Validation loss 0.049885209649801254 Accuracy 0.8690000176429749\n",
      "Iteration 30100 Training loss 0.04791492223739624 Validation loss 0.04992825537919998 Accuracy 0.8693750500679016\n",
      "Iteration 30110 Training loss 0.041013214737176895 Validation loss 0.049860090017318726 Accuracy 0.8690000176429749\n",
      "Iteration 30120 Training loss 0.04674225673079491 Validation loss 0.04989178106188774 Accuracy 0.8683750629425049\n",
      "Iteration 30130 Training loss 0.0383375883102417 Validation loss 0.05019370838999748 Accuracy 0.8678750395774841\n",
      "Iteration 30140 Training loss 0.05126523971557617 Validation loss 0.04986311495304108 Accuracy 0.8687500357627869\n",
      "Iteration 30150 Training loss 0.04892222210764885 Validation loss 0.04987654462456703 Accuracy 0.8683750629425049\n",
      "Iteration 30160 Training loss 0.05491182208061218 Validation loss 0.0498475506901741 Accuracy 0.8687500357627869\n",
      "Iteration 30170 Training loss 0.04587018862366676 Validation loss 0.04983644559979439 Accuracy 0.8696250319480896\n",
      "Iteration 30180 Training loss 0.04289579391479492 Validation loss 0.049903180450201035 Accuracy 0.8682500123977661\n",
      "Iteration 30190 Training loss 0.05006112903356552 Validation loss 0.04996350407600403 Accuracy 0.8675000667572021\n",
      "Iteration 30200 Training loss 0.052186835557222366 Validation loss 0.04988778382539749 Accuracy 0.8687500357627869\n",
      "Iteration 30210 Training loss 0.0444486029446125 Validation loss 0.04991587996482849 Accuracy 0.8673750162124634\n",
      "Iteration 30220 Training loss 0.048225633800029755 Validation loss 0.049825262278318405 Accuracy 0.8688750267028809\n",
      "Iteration 30230 Training loss 0.045867402106523514 Validation loss 0.0498243048787117 Accuracy 0.8695000410079956\n",
      "Iteration 30240 Training loss 0.05220832675695419 Validation loss 0.04984322562813759 Accuracy 0.8683750629425049\n",
      "Iteration 30250 Training loss 0.050890058279037476 Validation loss 0.04983418807387352 Accuracy 0.8681250214576721\n",
      "Iteration 30260 Training loss 0.049984175711870193 Validation loss 0.04983391985297203 Accuracy 0.8693750500679016\n",
      "Iteration 30270 Training loss 0.04845232889056206 Validation loss 0.0498703233897686 Accuracy 0.8683750629425049\n",
      "Iteration 30280 Training loss 0.04176460579037666 Validation loss 0.04982319846749306 Accuracy 0.8691250681877136\n",
      "Iteration 30290 Training loss 0.043913524597883224 Validation loss 0.05041949823498726 Accuracy 0.8637500405311584\n",
      "Iteration 30300 Training loss 0.04760563746094704 Validation loss 0.0498533733189106 Accuracy 0.8690000176429749\n",
      "Iteration 30310 Training loss 0.04648219048976898 Validation loss 0.04977898299694061 Accuracy 0.8700000643730164\n",
      "Iteration 30320 Training loss 0.0514749139547348 Validation loss 0.049837496131658554 Accuracy 0.8687500357627869\n",
      "Iteration 30330 Training loss 0.04084373265504837 Validation loss 0.0497405044734478 Accuracy 0.8708750605583191\n",
      "Iteration 30340 Training loss 0.04662076011300087 Validation loss 0.04976517707109451 Accuracy 0.8698750138282776\n",
      "Iteration 30350 Training loss 0.04725608602166176 Validation loss 0.0497860424220562 Accuracy 0.8697500228881836\n",
      "Iteration 30360 Training loss 0.05364900827407837 Validation loss 0.050058793276548386 Accuracy 0.8671250343322754\n",
      "Iteration 30370 Training loss 0.03626814857125282 Validation loss 0.049761056900024414 Accuracy 0.8693750500679016\n",
      "Iteration 30380 Training loss 0.04535285010933876 Validation loss 0.049775224179029465 Accuracy 0.8697500228881836\n",
      "Iteration 30390 Training loss 0.044530630111694336 Validation loss 0.050091877579689026 Accuracy 0.8672500252723694\n",
      "Iteration 30400 Training loss 0.04195624217391014 Validation loss 0.05028460547327995 Accuracy 0.8670000433921814\n",
      "Iteration 30410 Training loss 0.047889575362205505 Validation loss 0.04978073388338089 Accuracy 0.8692500591278076\n",
      "Iteration 30420 Training loss 0.04047404229640961 Validation loss 0.049840979278087616 Accuracy 0.8696250319480896\n",
      "Iteration 30430 Training loss 0.05154816433787346 Validation loss 0.050116166472435 Accuracy 0.8677500486373901\n",
      "Iteration 30440 Training loss 0.04686445742845535 Validation loss 0.0510173961520195 Accuracy 0.8651250600814819\n",
      "Iteration 30450 Training loss 0.0471080020070076 Validation loss 0.049779925495386124 Accuracy 0.8700000643730164\n",
      "Iteration 30460 Training loss 0.04763970896601677 Validation loss 0.04980430752038956 Accuracy 0.8695000410079956\n",
      "Iteration 30470 Training loss 0.04476993903517723 Validation loss 0.04976610094308853 Accuracy 0.8691250681877136\n",
      "Iteration 30480 Training loss 0.044003553688526154 Validation loss 0.049882326275110245 Accuracy 0.8683750629425049\n",
      "Iteration 30490 Training loss 0.05825695022940636 Validation loss 0.04975458234548569 Accuracy 0.8695000410079956\n",
      "Iteration 30500 Training loss 0.04014232009649277 Validation loss 0.049830660223960876 Accuracy 0.8682500123977661\n",
      "Iteration 30510 Training loss 0.04791660234332085 Validation loss 0.04988561570644379 Accuracy 0.8677500486373901\n",
      "Iteration 30520 Training loss 0.05106644332408905 Validation loss 0.04970052093267441 Accuracy 0.8683750629425049\n",
      "Iteration 30530 Training loss 0.040643032640218735 Validation loss 0.04971856623888016 Accuracy 0.8691250681877136\n",
      "Iteration 30540 Training loss 0.044021595269441605 Validation loss 0.04969443753361702 Accuracy 0.8690000176429749\n",
      "Iteration 30550 Training loss 0.04550129175186157 Validation loss 0.04968808591365814 Accuracy 0.8695000410079956\n",
      "Iteration 30560 Training loss 0.043783169239759445 Validation loss 0.049697209149599075 Accuracy 0.8692500591278076\n",
      "Iteration 30570 Training loss 0.049566201865673065 Validation loss 0.049817804247140884 Accuracy 0.8687500357627869\n",
      "Iteration 30580 Training loss 0.046894997358322144 Validation loss 0.04969625174999237 Accuracy 0.8688750267028809\n",
      "Iteration 30590 Training loss 0.04841145873069763 Validation loss 0.04994689300656319 Accuracy 0.8681250214576721\n",
      "Iteration 30600 Training loss 0.04236570745706558 Validation loss 0.049692410975694656 Accuracy 0.8697500228881836\n",
      "Iteration 30610 Training loss 0.05146478861570358 Validation loss 0.050861749798059464 Accuracy 0.8656250238418579\n",
      "Iteration 30620 Training loss 0.04222644865512848 Validation loss 0.04970209300518036 Accuracy 0.8695000410079956\n",
      "Iteration 30630 Training loss 0.050085656344890594 Validation loss 0.04970535635948181 Accuracy 0.8687500357627869\n",
      "Iteration 30640 Training loss 0.04780076816678047 Validation loss 0.049804456532001495 Accuracy 0.8675000667572021\n",
      "Iteration 30650 Training loss 0.04217773303389549 Validation loss 0.04965902864933014 Accuracy 0.8698750138282776\n",
      "Iteration 30660 Training loss 0.04883529618382454 Validation loss 0.049644920974969864 Accuracy 0.8695000410079956\n",
      "Iteration 30670 Training loss 0.0466967336833477 Validation loss 0.04988919198513031 Accuracy 0.8695000410079956\n",
      "Iteration 30680 Training loss 0.043575841933488846 Validation loss 0.04984476789832115 Accuracy 0.8697500228881836\n",
      "Iteration 30690 Training loss 0.05399496108293533 Validation loss 0.04987962543964386 Accuracy 0.8666250705718994\n",
      "Iteration 30700 Training loss 0.04781213775277138 Validation loss 0.04978165030479431 Accuracy 0.8681250214576721\n",
      "Iteration 30710 Training loss 0.04809480533003807 Validation loss 0.04962779954075813 Accuracy 0.8698750138282776\n",
      "Iteration 30720 Training loss 0.038346707820892334 Validation loss 0.049666497856378555 Accuracy 0.8698750138282776\n",
      "Iteration 30730 Training loss 0.050604939460754395 Validation loss 0.04991549253463745 Accuracy 0.8676250576972961\n",
      "Iteration 30740 Training loss 0.04571264982223511 Validation loss 0.04972356557846069 Accuracy 0.8691250681877136\n",
      "Iteration 30750 Training loss 0.03911516070365906 Validation loss 0.049897920340299606 Accuracy 0.8680000305175781\n",
      "Iteration 30760 Training loss 0.051302745938301086 Validation loss 0.05061180517077446 Accuracy 0.8671250343322754\n",
      "Iteration 30770 Training loss 0.05244771018624306 Validation loss 0.04968178644776344 Accuracy 0.8690000176429749\n",
      "Iteration 30780 Training loss 0.045688655227422714 Validation loss 0.049748294055461884 Accuracy 0.8690000176429749\n",
      "Iteration 30790 Training loss 0.0430402047932148 Validation loss 0.04976867884397507 Accuracy 0.8683750629425049\n",
      "Iteration 30800 Training loss 0.0424656867980957 Validation loss 0.04974709078669548 Accuracy 0.8686250448226929\n",
      "Iteration 30810 Training loss 0.05030612647533417 Validation loss 0.05057016387581825 Accuracy 0.8676250576972961\n",
      "Iteration 30820 Training loss 0.05268361419439316 Validation loss 0.05064794421195984 Accuracy 0.8671250343322754\n",
      "Iteration 30830 Training loss 0.04870746657252312 Validation loss 0.05030272528529167 Accuracy 0.8645000457763672\n",
      "Iteration 30840 Training loss 0.0478210486471653 Validation loss 0.04979509860277176 Accuracy 0.8680000305175781\n",
      "Iteration 30850 Training loss 0.03942018747329712 Validation loss 0.049760423600673676 Accuracy 0.8675000667572021\n",
      "Iteration 30860 Training loss 0.04407653212547302 Validation loss 0.049962326884269714 Accuracy 0.8686250448226929\n",
      "Iteration 30870 Training loss 0.04682275652885437 Validation loss 0.04979486018419266 Accuracy 0.8665000200271606\n",
      "Iteration 30880 Training loss 0.04762348160147667 Validation loss 0.04961834475398064 Accuracy 0.8683750629425049\n",
      "Iteration 30890 Training loss 0.04746195673942566 Validation loss 0.04976683109998703 Accuracy 0.8687500357627869\n",
      "Iteration 30900 Training loss 0.046951692551374435 Validation loss 0.049605999141931534 Accuracy 0.8696250319480896\n",
      "Iteration 30910 Training loss 0.049842946231365204 Validation loss 0.049720555543899536 Accuracy 0.8695000410079956\n",
      "Iteration 30920 Training loss 0.051956452429294586 Validation loss 0.049631524831056595 Accuracy 0.8688750267028809\n",
      "Iteration 30930 Training loss 0.03787469118833542 Validation loss 0.04986310005187988 Accuracy 0.8678750395774841\n",
      "Iteration 30940 Training loss 0.04042496904730797 Validation loss 0.04964316263794899 Accuracy 0.8690000176429749\n",
      "Iteration 30950 Training loss 0.0485830195248127 Validation loss 0.04961412772536278 Accuracy 0.8686250448226929\n",
      "Iteration 30960 Training loss 0.04587429389357567 Validation loss 0.049601249396800995 Accuracy 0.8698750138282776\n",
      "Iteration 30970 Training loss 0.05100206658244133 Validation loss 0.04967794194817543 Accuracy 0.8678750395774841\n",
      "Iteration 30980 Training loss 0.042549170553684235 Validation loss 0.05030696466565132 Accuracy 0.8635000586509705\n",
      "Iteration 30990 Training loss 0.04523032531142235 Validation loss 0.04990572854876518 Accuracy 0.8697500228881836\n",
      "Iteration 31000 Training loss 0.053299058228731155 Validation loss 0.0498160719871521 Accuracy 0.8661250472068787\n",
      "Iteration 31010 Training loss 0.05014592036604881 Validation loss 0.049646131694316864 Accuracy 0.8690000176429749\n",
      "Iteration 31020 Training loss 0.04130150377750397 Validation loss 0.04962312430143356 Accuracy 0.8698750138282776\n",
      "Iteration 31030 Training loss 0.0448082871735096 Validation loss 0.04972398281097412 Accuracy 0.8683750629425049\n",
      "Iteration 31040 Training loss 0.03825486823916435 Validation loss 0.04979073628783226 Accuracy 0.8690000176429749\n",
      "Iteration 31050 Training loss 0.04189542680978775 Validation loss 0.0496172197163105 Accuracy 0.8692500591278076\n",
      "Iteration 31060 Training loss 0.04690984636545181 Validation loss 0.0496259480714798 Accuracy 0.8690000176429749\n",
      "Iteration 31070 Training loss 0.054995451122522354 Validation loss 0.04964837804436684 Accuracy 0.8696250319480896\n",
      "Iteration 31080 Training loss 0.04141532629728317 Validation loss 0.05027453228831291 Accuracy 0.8680000305175781\n",
      "Iteration 31090 Training loss 0.04330039769411087 Validation loss 0.05002265423536301 Accuracy 0.8682500123977661\n",
      "Iteration 31100 Training loss 0.03992095962166786 Validation loss 0.0502639040350914 Accuracy 0.8673750162124634\n",
      "Iteration 31110 Training loss 0.04622700437903404 Validation loss 0.049915410578250885 Accuracy 0.8693750500679016\n",
      "Iteration 31120 Training loss 0.043799202889204025 Validation loss 0.04969320073723793 Accuracy 0.8691250681877136\n",
      "Iteration 31130 Training loss 0.044437993317842484 Validation loss 0.04969024285674095 Accuracy 0.8700000643730164\n",
      "Iteration 31140 Training loss 0.04252638667821884 Validation loss 0.049955353140830994 Accuracy 0.8693750500679016\n",
      "Iteration 31150 Training loss 0.0468897745013237 Validation loss 0.04953301325440407 Accuracy 0.8692500591278076\n",
      "Iteration 31160 Training loss 0.04702000692486763 Validation loss 0.04954008013010025 Accuracy 0.8701250553131104\n",
      "Iteration 31170 Training loss 0.04302624240517616 Validation loss 0.04999049752950668 Accuracy 0.8660000562667847\n",
      "Iteration 31180 Training loss 0.04493814706802368 Validation loss 0.049703359603881836 Accuracy 0.8678750395774841\n",
      "Iteration 31190 Training loss 0.04275600239634514 Validation loss 0.04953474923968315 Accuracy 0.8700000643730164\n",
      "Iteration 31200 Training loss 0.04161151871085167 Validation loss 0.04968754202127457 Accuracy 0.8672500252723694\n",
      "Iteration 31210 Training loss 0.051792919635772705 Validation loss 0.04951161891222 Accuracy 0.8688750267028809\n",
      "Iteration 31220 Training loss 0.050275132060050964 Validation loss 0.0494876392185688 Accuracy 0.8700000643730164\n",
      "Iteration 31230 Training loss 0.04477132856845856 Validation loss 0.049577876925468445 Accuracy 0.8692500591278076\n",
      "Iteration 31240 Training loss 0.0383487306535244 Validation loss 0.04950094223022461 Accuracy 0.8702500462532043\n",
      "Iteration 31250 Training loss 0.03652083873748779 Validation loss 0.04958455264568329 Accuracy 0.8690000176429749\n",
      "Iteration 31260 Training loss 0.05331110954284668 Validation loss 0.04972383379936218 Accuracy 0.8670000433921814\n",
      "Iteration 31270 Training loss 0.05428424850106239 Validation loss 0.04947866126894951 Accuracy 0.8707500696182251\n",
      "Iteration 31280 Training loss 0.048365868628025055 Validation loss 0.049454912543296814 Accuracy 0.8703750371932983\n",
      "Iteration 31290 Training loss 0.04574103280901909 Validation loss 0.049463823437690735 Accuracy 0.8706250190734863\n",
      "Iteration 31300 Training loss 0.037287387996912 Validation loss 0.04954046383500099 Accuracy 0.8692500591278076\n",
      "Iteration 31310 Training loss 0.04812576249241829 Validation loss 0.04943850636482239 Accuracy 0.8701250553131104\n",
      "Iteration 31320 Training loss 0.04664578288793564 Validation loss 0.04947778210043907 Accuracy 0.8700000643730164\n",
      "Iteration 31330 Training loss 0.055436328053474426 Validation loss 0.04942474514245987 Accuracy 0.8696250319480896\n",
      "Iteration 31340 Training loss 0.04686063155531883 Validation loss 0.049485884606838226 Accuracy 0.8698750138282776\n",
      "Iteration 31350 Training loss 0.05168847739696503 Validation loss 0.04970892518758774 Accuracy 0.8681250214576721\n",
      "Iteration 31360 Training loss 0.04369441047310829 Validation loss 0.04950815066695213 Accuracy 0.8696250319480896\n",
      "Iteration 31370 Training loss 0.043986670672893524 Validation loss 0.04950053244829178 Accuracy 0.8695000410079956\n",
      "Iteration 31380 Training loss 0.04473455622792244 Validation loss 0.04949158802628517 Accuracy 0.8693750500679016\n",
      "Iteration 31390 Training loss 0.045408643782138824 Validation loss 0.0496547631919384 Accuracy 0.8688750267028809\n",
      "Iteration 31400 Training loss 0.04003497213125229 Validation loss 0.04954368993639946 Accuracy 0.8686250448226929\n",
      "Iteration 31410 Training loss 0.04600710794329643 Validation loss 0.04947312921285629 Accuracy 0.8702500462532043\n",
      "Iteration 31420 Training loss 0.046662092208862305 Validation loss 0.049502428621053696 Accuracy 0.8706250190734863\n",
      "Iteration 31430 Training loss 0.04055047780275345 Validation loss 0.04944920539855957 Accuracy 0.8712500333786011\n",
      "Iteration 31440 Training loss 0.04642626643180847 Validation loss 0.04960658401250839 Accuracy 0.8693750500679016\n",
      "Iteration 31450 Training loss 0.04192645847797394 Validation loss 0.04947346821427345 Accuracy 0.8695000410079956\n",
      "Iteration 31460 Training loss 0.04642830416560173 Validation loss 0.04942961037158966 Accuracy 0.8700000643730164\n",
      "Iteration 31470 Training loss 0.050100214779376984 Validation loss 0.049453120678663254 Accuracy 0.8701250553131104\n",
      "Iteration 31480 Training loss 0.04260769113898277 Validation loss 0.049649689346551895 Accuracy 0.8682500123977661\n",
      "Iteration 31490 Training loss 0.04805576801300049 Validation loss 0.04966093599796295 Accuracy 0.8673750162124634\n",
      "Iteration 31500 Training loss 0.049822527915239334 Validation loss 0.049621399492025375 Accuracy 0.8678750395774841\n",
      "Iteration 31510 Training loss 0.04857302084565163 Validation loss 0.04945986345410347 Accuracy 0.8693750500679016\n",
      "Iteration 31520 Training loss 0.050573185086250305 Validation loss 0.04940063878893852 Accuracy 0.8695000410079956\n",
      "Iteration 31530 Training loss 0.04739425703883171 Validation loss 0.049491528421640396 Accuracy 0.8687500357627869\n",
      "Iteration 31540 Training loss 0.03662396967411041 Validation loss 0.04955922067165375 Accuracy 0.8676250576972961\n",
      "Iteration 31550 Training loss 0.05215878412127495 Validation loss 0.04964849725365639 Accuracy 0.8667500615119934\n",
      "Iteration 31560 Training loss 0.037969160825014114 Validation loss 0.04950911924242973 Accuracy 0.8676250576972961\n",
      "Iteration 31570 Training loss 0.039878711104393005 Validation loss 0.049402181059122086 Accuracy 0.8698750138282776\n",
      "Iteration 31580 Training loss 0.04013899713754654 Validation loss 0.049396153539419174 Accuracy 0.8703750371932983\n",
      "Iteration 31590 Training loss 0.043691229075193405 Validation loss 0.04946941137313843 Accuracy 0.8707500696182251\n",
      "Iteration 31600 Training loss 0.04526380077004433 Validation loss 0.04945871978998184 Accuracy 0.8710000514984131\n",
      "Iteration 31610 Training loss 0.04322507232427597 Validation loss 0.049469783902168274 Accuracy 0.8701250553131104\n",
      "Iteration 31620 Training loss 0.04829464480280876 Validation loss 0.04939817264676094 Accuracy 0.8695000410079956\n",
      "Iteration 31630 Training loss 0.0483323372900486 Validation loss 0.04948623105883598 Accuracy 0.8675000667572021\n",
      "Iteration 31640 Training loss 0.0498264878988266 Validation loss 0.049435924738645554 Accuracy 0.8702500462532043\n",
      "Iteration 31650 Training loss 0.041957635432481766 Validation loss 0.049409326165914536 Accuracy 0.8696250319480896\n",
      "Iteration 31660 Training loss 0.04248903691768646 Validation loss 0.049559444189071655 Accuracy 0.8692500591278076\n",
      "Iteration 31670 Training loss 0.047698725014925 Validation loss 0.049479890614748 Accuracy 0.8691250681877136\n",
      "Iteration 31680 Training loss 0.039557721465826035 Validation loss 0.049331631511449814 Accuracy 0.8701250553131104\n",
      "Iteration 31690 Training loss 0.05113472789525986 Validation loss 0.04936540499329567 Accuracy 0.8702500462532043\n",
      "Iteration 31700 Training loss 0.05073001608252525 Validation loss 0.049342937767505646 Accuracy 0.8706250190734863\n",
      "Iteration 31710 Training loss 0.04746472090482712 Validation loss 0.0498865470290184 Accuracy 0.8698750138282776\n",
      "Iteration 31720 Training loss 0.04221009090542793 Validation loss 0.04946988821029663 Accuracy 0.8695000410079956\n",
      "Iteration 31730 Training loss 0.04541133716702461 Validation loss 0.04933498799800873 Accuracy 0.8697500228881836\n",
      "Iteration 31740 Training loss 0.04811001569032669 Validation loss 0.04970037192106247 Accuracy 0.8673750162124634\n",
      "Iteration 31750 Training loss 0.04769256338477135 Validation loss 0.04934097081422806 Accuracy 0.8703750371932983\n",
      "Iteration 31760 Training loss 0.04872683435678482 Validation loss 0.04931369423866272 Accuracy 0.8700000643730164\n",
      "Iteration 31770 Training loss 0.041824452579021454 Validation loss 0.049370720982551575 Accuracy 0.8691250681877136\n",
      "Iteration 31780 Training loss 0.04971687123179436 Validation loss 0.04932216554880142 Accuracy 0.8696250319480896\n",
      "Iteration 31790 Training loss 0.04280632734298706 Validation loss 0.049330610781908035 Accuracy 0.8712500333786011\n",
      "Iteration 31800 Training loss 0.04361676424741745 Validation loss 0.04958249256014824 Accuracy 0.8701250553131104\n",
      "Iteration 31810 Training loss 0.0400245264172554 Validation loss 0.049453433603048325 Accuracy 0.8701250553131104\n",
      "Iteration 31820 Training loss 0.04706908017396927 Validation loss 0.049573492258787155 Accuracy 0.8696250319480896\n",
      "Iteration 31830 Training loss 0.043115586042404175 Validation loss 0.04933631420135498 Accuracy 0.8697500228881836\n",
      "Iteration 31840 Training loss 0.04457099735736847 Validation loss 0.049379702657461166 Accuracy 0.8685000538825989\n",
      "Iteration 31850 Training loss 0.03999362885951996 Validation loss 0.04939519613981247 Accuracy 0.8702500462532043\n",
      "Iteration 31860 Training loss 0.03972681984305382 Validation loss 0.04926681146025658 Accuracy 0.8696250319480896\n",
      "Iteration 31870 Training loss 0.044513098895549774 Validation loss 0.049483537673950195 Accuracy 0.8688750267028809\n",
      "Iteration 31880 Training loss 0.05103231221437454 Validation loss 0.04941904544830322 Accuracy 0.8698750138282776\n",
      "Iteration 31890 Training loss 0.04680384695529938 Validation loss 0.04940569028258324 Accuracy 0.8700000643730164\n",
      "Iteration 31900 Training loss 0.043203286826610565 Validation loss 0.049345195293426514 Accuracy 0.8693750500679016\n",
      "Iteration 31910 Training loss 0.04857160523533821 Validation loss 0.04936375841498375 Accuracy 0.8696250319480896\n",
      "Iteration 31920 Training loss 0.04968925192952156 Validation loss 0.049944184720516205 Accuracy 0.8685000538825989\n",
      "Iteration 31930 Training loss 0.05109822750091553 Validation loss 0.04942695051431656 Accuracy 0.8675000667572021\n",
      "Iteration 31940 Training loss 0.04539280757308006 Validation loss 0.049715351313352585 Accuracy 0.8665000200271606\n",
      "Iteration 31950 Training loss 0.05002216622233391 Validation loss 0.04943525791168213 Accuracy 0.8686250448226929\n",
      "Iteration 31960 Training loss 0.05085909739136696 Validation loss 0.04937680438160896 Accuracy 0.8688750267028809\n",
      "Iteration 31970 Training loss 0.05478818342089653 Validation loss 0.04940958321094513 Accuracy 0.8702500462532043\n",
      "Iteration 31980 Training loss 0.05790195241570473 Validation loss 0.04945804178714752 Accuracy 0.8687500357627869\n",
      "Iteration 31990 Training loss 0.044065237045288086 Validation loss 0.04928434267640114 Accuracy 0.8705000281333923\n",
      "Iteration 32000 Training loss 0.04259559512138367 Validation loss 0.04919654130935669 Accuracy 0.8715000152587891\n",
      "Iteration 32010 Training loss 0.05026652291417122 Validation loss 0.04923143982887268 Accuracy 0.8706250190734863\n",
      "Iteration 32020 Training loss 0.04727062210440636 Validation loss 0.04949387162923813 Accuracy 0.8698750138282776\n",
      "Iteration 32030 Training loss 0.04261814057826996 Validation loss 0.0494469553232193 Accuracy 0.8680000305175781\n",
      "Iteration 32040 Training loss 0.04292536526918411 Validation loss 0.049696844071149826 Accuracy 0.8668750524520874\n",
      "Iteration 32050 Training loss 0.040524229407310486 Validation loss 0.04917917028069496 Accuracy 0.8712500333786011\n",
      "Iteration 32060 Training loss 0.04509764164686203 Validation loss 0.04922407120466232 Accuracy 0.8707500696182251\n",
      "Iteration 32070 Training loss 0.04443443566560745 Validation loss 0.049226149916648865 Accuracy 0.8712500333786011\n",
      "Iteration 32080 Training loss 0.04710669070482254 Validation loss 0.04927690327167511 Accuracy 0.8706250190734863\n",
      "Iteration 32090 Training loss 0.04637965187430382 Validation loss 0.04941707104444504 Accuracy 0.8683750629425049\n",
      "Iteration 32100 Training loss 0.048136577010154724 Validation loss 0.04919067397713661 Accuracy 0.8702500462532043\n",
      "Iteration 32110 Training loss 0.042792461812496185 Validation loss 0.049172546714544296 Accuracy 0.8710000514984131\n",
      "Iteration 32120 Training loss 0.043125562369823456 Validation loss 0.0492367185652256 Accuracy 0.8706250190734863\n",
      "Iteration 32130 Training loss 0.045544013381004333 Validation loss 0.0492505244910717 Accuracy 0.8700000643730164\n",
      "Iteration 32140 Training loss 0.037224724888801575 Validation loss 0.04924697428941727 Accuracy 0.8702500462532043\n",
      "Iteration 32150 Training loss 0.04595726355910301 Validation loss 0.049281440675258636 Accuracy 0.8702500462532043\n",
      "Iteration 32160 Training loss 0.049175094813108444 Validation loss 0.049230802804231644 Accuracy 0.8711250424385071\n",
      "Iteration 32170 Training loss 0.04363495111465454 Validation loss 0.04957021772861481 Accuracy 0.8672500252723694\n",
      "Iteration 32180 Training loss 0.049818843603134155 Validation loss 0.04922033101320267 Accuracy 0.8705000281333923\n",
      "Iteration 32190 Training loss 0.045151472091674805 Validation loss 0.04938558489084244 Accuracy 0.8690000176429749\n",
      "Iteration 32200 Training loss 0.04583654925227165 Validation loss 0.04920166730880737 Accuracy 0.8700000643730164\n",
      "Iteration 32210 Training loss 0.04118838161230087 Validation loss 0.04916909709572792 Accuracy 0.8705000281333923\n",
      "Iteration 32220 Training loss 0.04067637398838997 Validation loss 0.049186863005161285 Accuracy 0.8703750371932983\n",
      "Iteration 32230 Training loss 0.049257174134254456 Validation loss 0.04977545514702797 Accuracy 0.8682500123977661\n",
      "Iteration 32240 Training loss 0.04443199932575226 Validation loss 0.04909785836935043 Accuracy 0.8703750371932983\n",
      "Iteration 32250 Training loss 0.0421573743224144 Validation loss 0.04910161346197128 Accuracy 0.8710000514984131\n",
      "Iteration 32260 Training loss 0.037553559988737106 Validation loss 0.049167849123477936 Accuracy 0.8698750138282776\n",
      "Iteration 32270 Training loss 0.04440778121352196 Validation loss 0.04917966201901436 Accuracy 0.8703750371932983\n",
      "Iteration 32280 Training loss 0.047550834715366364 Validation loss 0.04929307848215103 Accuracy 0.8701250553131104\n",
      "Iteration 32290 Training loss 0.053160410374403 Validation loss 0.04915826767683029 Accuracy 0.8711250424385071\n",
      "Iteration 32300 Training loss 0.04703177511692047 Validation loss 0.04914749786257744 Accuracy 0.8715000152587891\n",
      "Iteration 32310 Training loss 0.04173508659005165 Validation loss 0.04912082105875015 Accuracy 0.8701250553131104\n",
      "Iteration 32320 Training loss 0.04437969624996185 Validation loss 0.04964180290699005 Accuracy 0.8697500228881836\n",
      "Iteration 32330 Training loss 0.045076511800289154 Validation loss 0.04914895072579384 Accuracy 0.8712500333786011\n",
      "Iteration 32340 Training loss 0.05004628375172615 Validation loss 0.049139536917209625 Accuracy 0.8706250190734863\n",
      "Iteration 32350 Training loss 0.04668360576033592 Validation loss 0.04927707836031914 Accuracy 0.8690000176429749\n",
      "Iteration 32360 Training loss 0.0449172779917717 Validation loss 0.049159035086631775 Accuracy 0.8715000152587891\n",
      "Iteration 32370 Training loss 0.04413488879799843 Validation loss 0.04945606365799904 Accuracy 0.8670000433921814\n",
      "Iteration 32380 Training loss 0.039547257125377655 Validation loss 0.04913359507918358 Accuracy 0.8698750138282776\n",
      "Iteration 32390 Training loss 0.045382097363471985 Validation loss 0.049263596534729004 Accuracy 0.8686250448226929\n",
      "Iteration 32400 Training loss 0.04176332429051399 Validation loss 0.049178194254636765 Accuracy 0.8688750267028809\n",
      "Iteration 32410 Training loss 0.033423617482185364 Validation loss 0.049191005527973175 Accuracy 0.8702500462532043\n",
      "Iteration 32420 Training loss 0.04785468429327011 Validation loss 0.04917578771710396 Accuracy 0.8706250190734863\n",
      "Iteration 32430 Training loss 0.05407532677054405 Validation loss 0.04958001896739006 Accuracy 0.8701250553131104\n",
      "Iteration 32440 Training loss 0.0484066940844059 Validation loss 0.04908649995923042 Accuracy 0.8707500696182251\n",
      "Iteration 32450 Training loss 0.04289165511727333 Validation loss 0.04961341246962547 Accuracy 0.8708750605583191\n",
      "Iteration 32460 Training loss 0.04881800711154938 Validation loss 0.04918928071856499 Accuracy 0.8710000514984131\n",
      "Iteration 32470 Training loss 0.043630607426166534 Validation loss 0.04910924285650253 Accuracy 0.8710000514984131\n",
      "Iteration 32480 Training loss 0.045045290142297745 Validation loss 0.049069181084632874 Accuracy 0.8701250553131104\n",
      "Iteration 32490 Training loss 0.05455901846289635 Validation loss 0.04905252531170845 Accuracy 0.8713750243186951\n",
      "Iteration 32500 Training loss 0.04769362509250641 Validation loss 0.04909645393490791 Accuracy 0.8703750371932983\n",
      "Iteration 32510 Training loss 0.04024917632341385 Validation loss 0.04904238507151604 Accuracy 0.8711250424385071\n",
      "Iteration 32520 Training loss 0.05040182173252106 Validation loss 0.0490436889231205 Accuracy 0.8701250553131104\n",
      "Iteration 32530 Training loss 0.04531949758529663 Validation loss 0.04904945567250252 Accuracy 0.8720000386238098\n",
      "Iteration 32540 Training loss 0.04788563773036003 Validation loss 0.04931027442216873 Accuracy 0.8686250448226929\n",
      "Iteration 32550 Training loss 0.04776381701231003 Validation loss 0.049118731170892715 Accuracy 0.8708750605583191\n",
      "Iteration 32560 Training loss 0.0501658134162426 Validation loss 0.049050942063331604 Accuracy 0.8715000152587891\n",
      "Iteration 32570 Training loss 0.048898305743932724 Validation loss 0.04902211204171181 Accuracy 0.8710000514984131\n",
      "Iteration 32580 Training loss 0.04373099282383919 Validation loss 0.04915944114327431 Accuracy 0.8710000514984131\n",
      "Iteration 32590 Training loss 0.03902733698487282 Validation loss 0.04909123107790947 Accuracy 0.8708750605583191\n",
      "Iteration 32600 Training loss 0.04750822111964226 Validation loss 0.049218371510505676 Accuracy 0.8703750371932983\n",
      "Iteration 32610 Training loss 0.04667384549975395 Validation loss 0.04910169541835785 Accuracy 0.8712500333786011\n",
      "Iteration 32620 Training loss 0.041859474033117294 Validation loss 0.049089450389146805 Accuracy 0.8708750605583191\n",
      "Iteration 32630 Training loss 0.03989503160119057 Validation loss 0.04932329058647156 Accuracy 0.8705000281333923\n",
      "Iteration 32640 Training loss 0.040745023638010025 Validation loss 0.04906383901834488 Accuracy 0.8701250553131104\n",
      "Iteration 32650 Training loss 0.042126283049583435 Validation loss 0.04903849959373474 Accuracy 0.8717500567436218\n",
      "Iteration 32660 Training loss 0.0435919463634491 Validation loss 0.0491066500544548 Accuracy 0.8703750371932983\n",
      "Iteration 32670 Training loss 0.05047076940536499 Validation loss 0.049082208424806595 Accuracy 0.8712500333786011\n",
      "Iteration 32680 Training loss 0.042403072118759155 Validation loss 0.04914884269237518 Accuracy 0.8696250319480896\n",
      "Iteration 32690 Training loss 0.04796759411692619 Validation loss 0.04932539910078049 Accuracy 0.8706250190734863\n",
      "Iteration 32700 Training loss 0.04153810068964958 Validation loss 0.04981747269630432 Accuracy 0.8652500510215759\n",
      "Iteration 32710 Training loss 0.03829857334494591 Validation loss 0.04905546084046364 Accuracy 0.8715000152587891\n",
      "Iteration 32720 Training loss 0.052482202649116516 Validation loss 0.049208298325538635 Accuracy 0.8712500333786011\n",
      "Iteration 32730 Training loss 0.04474920034408569 Validation loss 0.04894925653934479 Accuracy 0.8711250424385071\n",
      "Iteration 32740 Training loss 0.045460257679224014 Validation loss 0.049805909395217896 Accuracy 0.8687500357627869\n",
      "Iteration 32750 Training loss 0.04994329437613487 Validation loss 0.04918334633111954 Accuracy 0.8691250681877136\n",
      "Iteration 32760 Training loss 0.03973877429962158 Validation loss 0.04906124994158745 Accuracy 0.8711250424385071\n",
      "Iteration 32770 Training loss 0.05040867626667023 Validation loss 0.04899272695183754 Accuracy 0.8710000514984131\n",
      "Iteration 32780 Training loss 0.05092252790927887 Validation loss 0.049036070704460144 Accuracy 0.8705000281333923\n",
      "Iteration 32790 Training loss 0.045215606689453125 Validation loss 0.049158334732055664 Accuracy 0.8712500333786011\n",
      "Iteration 32800 Training loss 0.04695666581392288 Validation loss 0.04909820854663849 Accuracy 0.8701250553131104\n",
      "Iteration 32810 Training loss 0.049129389226436615 Validation loss 0.048995569348335266 Accuracy 0.8710000514984131\n",
      "Iteration 32820 Training loss 0.05686641484498978 Validation loss 0.04894133284687996 Accuracy 0.8717500567436218\n",
      "Iteration 32830 Training loss 0.03893977403640747 Validation loss 0.048968493938446045 Accuracy 0.8715000152587891\n",
      "Iteration 32840 Training loss 0.04881502687931061 Validation loss 0.04893568903207779 Accuracy 0.8711250424385071\n",
      "Iteration 32850 Training loss 0.04615537449717522 Validation loss 0.04911272972822189 Accuracy 0.8698750138282776\n",
      "Iteration 32860 Training loss 0.044698379933834076 Validation loss 0.04930077865719795 Accuracy 0.8692500591278076\n",
      "Iteration 32870 Training loss 0.045148514211177826 Validation loss 0.0490877702832222 Accuracy 0.8715000152587891\n",
      "Iteration 32880 Training loss 0.050161972641944885 Validation loss 0.048951730132102966 Accuracy 0.8715000152587891\n",
      "Iteration 32890 Training loss 0.04707883298397064 Validation loss 0.048902906477451324 Accuracy 0.8717500567436218\n",
      "Iteration 32900 Training loss 0.04589464142918587 Validation loss 0.04915817454457283 Accuracy 0.8713750243186951\n",
      "Iteration 32910 Training loss 0.0356692336499691 Validation loss 0.048926182091236115 Accuracy 0.8720000386238098\n",
      "Iteration 32920 Training loss 0.041740067303180695 Validation loss 0.04906882718205452 Accuracy 0.8707500696182251\n",
      "Iteration 32930 Training loss 0.04656614363193512 Validation loss 0.04888666421175003 Accuracy 0.8722500205039978\n",
      "Iteration 32940 Training loss 0.052704665809869766 Validation loss 0.048904795199632645 Accuracy 0.8712500333786011\n",
      "Iteration 32950 Training loss 0.04066799208521843 Validation loss 0.048916809260845184 Accuracy 0.8708750605583191\n",
      "Iteration 32960 Training loss 0.052230510860681534 Validation loss 0.04974942281842232 Accuracy 0.8685000538825989\n",
      "Iteration 32970 Training loss 0.0480438657104969 Validation loss 0.05028148740530014 Accuracy 0.8663750290870667\n",
      "Iteration 32980 Training loss 0.04639584571123123 Validation loss 0.049363140016794205 Accuracy 0.8702500462532043\n",
      "Iteration 32990 Training loss 0.05168521776795387 Validation loss 0.04931849241256714 Accuracy 0.8710000514984131\n",
      "Iteration 33000 Training loss 0.04200316220521927 Validation loss 0.048906825482845306 Accuracy 0.8701250553131104\n",
      "Iteration 33010 Training loss 0.05041983351111412 Validation loss 0.04890289530158043 Accuracy 0.8702500462532043\n",
      "Iteration 33020 Training loss 0.04896407946944237 Validation loss 0.04899607226252556 Accuracy 0.8706250190734863\n",
      "Iteration 33030 Training loss 0.04344099760055542 Validation loss 0.04939195513725281 Accuracy 0.8706250190734863\n",
      "Iteration 33040 Training loss 0.05039570853114128 Validation loss 0.04923529922962189 Accuracy 0.8686250448226929\n",
      "Iteration 33050 Training loss 0.043207380920648575 Validation loss 0.0488879419863224 Accuracy 0.8713750243186951\n",
      "Iteration 33060 Training loss 0.040571101009845734 Validation loss 0.048892028629779816 Accuracy 0.8720000386238098\n",
      "Iteration 33070 Training loss 0.043404631316661835 Validation loss 0.04948744550347328 Accuracy 0.8718750476837158\n",
      "Iteration 33080 Training loss 0.04442324489355087 Validation loss 0.04911387711763382 Accuracy 0.8703750371932983\n",
      "Iteration 33090 Training loss 0.04412126913666725 Validation loss 0.04891284555196762 Accuracy 0.8717500567436218\n",
      "Iteration 33100 Training loss 0.05162570998072624 Validation loss 0.048913005739450455 Accuracy 0.8716250658035278\n",
      "Iteration 33110 Training loss 0.04672779142856598 Validation loss 0.048944659531116486 Accuracy 0.8701250553131104\n",
      "Iteration 33120 Training loss 0.04031888023018837 Validation loss 0.04889901354908943 Accuracy 0.8710000514984131\n",
      "Iteration 33130 Training loss 0.04530857130885124 Validation loss 0.048952218145132065 Accuracy 0.8718750476837158\n",
      "Iteration 33140 Training loss 0.04590413346886635 Validation loss 0.04890965297818184 Accuracy 0.8700000643730164\n",
      "Iteration 33150 Training loss 0.04593544453382492 Validation loss 0.04890747740864754 Accuracy 0.8701250553131104\n",
      "Iteration 33160 Training loss 0.04318510368466377 Validation loss 0.04892774298787117 Accuracy 0.8712500333786011\n",
      "Iteration 33170 Training loss 0.036129359155893326 Validation loss 0.04903656989336014 Accuracy 0.8681250214576721\n",
      "Iteration 33180 Training loss 0.04747088626027107 Validation loss 0.04884021729230881 Accuracy 0.8717500567436218\n",
      "Iteration 33190 Training loss 0.04756684973835945 Validation loss 0.048980455845594406 Accuracy 0.8700000643730164\n",
      "Iteration 33200 Training loss 0.05020812526345253 Validation loss 0.049124088138341904 Accuracy 0.8718750476837158\n",
      "Iteration 33210 Training loss 0.0489315539598465 Validation loss 0.04929261654615402 Accuracy 0.8717500567436218\n",
      "Iteration 33220 Training loss 0.04486100748181343 Validation loss 0.049038439989089966 Accuracy 0.8716250658035278\n",
      "Iteration 33230 Training loss 0.0472651869058609 Validation loss 0.048809122294187546 Accuracy 0.8721250295639038\n",
      "Iteration 33240 Training loss 0.05439719930291176 Validation loss 0.04907281696796417 Accuracy 0.8711250424385071\n",
      "Iteration 33250 Training loss 0.04367758706212044 Validation loss 0.04936223477125168 Accuracy 0.8711250424385071\n",
      "Iteration 33260 Training loss 0.043022263795137405 Validation loss 0.04884892329573631 Accuracy 0.8710000514984131\n",
      "Iteration 33270 Training loss 0.0463985875248909 Validation loss 0.048962634056806564 Accuracy 0.8713750243186951\n",
      "Iteration 33280 Training loss 0.04537727311253548 Validation loss 0.0500430203974247 Accuracy 0.8676250576972961\n",
      "Iteration 33290 Training loss 0.049321968108415604 Validation loss 0.04895123094320297 Accuracy 0.8710000514984131\n",
      "Iteration 33300 Training loss 0.04411550983786583 Validation loss 0.04888175055384636 Accuracy 0.8705000281333923\n",
      "Iteration 33310 Training loss 0.05267318710684776 Validation loss 0.04889430105686188 Accuracy 0.8698750138282776\n",
      "Iteration 33320 Training loss 0.04442736506462097 Validation loss 0.04893847927451134 Accuracy 0.8720000386238098\n",
      "Iteration 33330 Training loss 0.051407866179943085 Validation loss 0.04914766922593117 Accuracy 0.8720000386238098\n",
      "Iteration 33340 Training loss 0.04879537969827652 Validation loss 0.0492975190281868 Accuracy 0.8715000152587891\n",
      "Iteration 33350 Training loss 0.04439827799797058 Validation loss 0.048834558576345444 Accuracy 0.8720000386238098\n",
      "Iteration 33360 Training loss 0.039460863918066025 Validation loss 0.04876480624079704 Accuracy 0.8715000152587891\n",
      "Iteration 33370 Training loss 0.04251755028963089 Validation loss 0.04889470338821411 Accuracy 0.8697500228881836\n",
      "Iteration 33380 Training loss 0.05283114314079285 Validation loss 0.04893431439995766 Accuracy 0.8718750476837158\n",
      "Iteration 33390 Training loss 0.045732803642749786 Validation loss 0.048753708600997925 Accuracy 0.8730000257492065\n",
      "Iteration 33400 Training loss 0.051689162850379944 Validation loss 0.04892893508076668 Accuracy 0.8701250553131104\n",
      "Iteration 33410 Training loss 0.04969193786382675 Validation loss 0.049326688051223755 Accuracy 0.8672500252723694\n",
      "Iteration 33420 Training loss 0.05406804010272026 Validation loss 0.04879559949040413 Accuracy 0.8721250295639038\n",
      "Iteration 33430 Training loss 0.0510094054043293 Validation loss 0.04948262870311737 Accuracy 0.8713750243186951\n",
      "Iteration 33440 Training loss 0.04110186547040939 Validation loss 0.04878045246005058 Accuracy 0.8721250295639038\n",
      "Iteration 33450 Training loss 0.039672065526247025 Validation loss 0.04976717755198479 Accuracy 0.8663750290870667\n",
      "Iteration 33460 Training loss 0.04534490779042244 Validation loss 0.04878830537199974 Accuracy 0.8726250529289246\n",
      "Iteration 33470 Training loss 0.04322631657123566 Validation loss 0.04878642410039902 Accuracy 0.8713750243186951\n",
      "Iteration 33480 Training loss 0.04881640151143074 Validation loss 0.04877045750617981 Accuracy 0.8718750476837158\n",
      "Iteration 33490 Training loss 0.050746120512485504 Validation loss 0.04875612631440163 Accuracy 0.8725000619888306\n",
      "Iteration 33500 Training loss 0.05227968096733093 Validation loss 0.04882150515913963 Accuracy 0.8706250190734863\n",
      "Iteration 33510 Training loss 0.04849814251065254 Validation loss 0.048714231699705124 Accuracy 0.8723750710487366\n",
      "Iteration 33520 Training loss 0.04138942062854767 Validation loss 0.04873514175415039 Accuracy 0.8728750348091125\n",
      "Iteration 33530 Training loss 0.03745577484369278 Validation loss 0.04875338822603226 Accuracy 0.8708750605583191\n",
      "Iteration 33540 Training loss 0.051061466336250305 Validation loss 0.049395617097616196 Accuracy 0.8696250319480896\n",
      "Iteration 33550 Training loss 0.03811971843242645 Validation loss 0.049348361790180206 Accuracy 0.8700000643730164\n",
      "Iteration 33560 Training loss 0.04667382314801216 Validation loss 0.04876984655857086 Accuracy 0.8723750710487366\n",
      "Iteration 33570 Training loss 0.04878357797861099 Validation loss 0.04901451617479324 Accuracy 0.8706250190734863\n",
      "Iteration 33580 Training loss 0.04453303664922714 Validation loss 0.04874609783291817 Accuracy 0.8723750710487366\n",
      "Iteration 33590 Training loss 0.04819324240088463 Validation loss 0.04881894215941429 Accuracy 0.8721250295639038\n",
      "Iteration 33600 Training loss 0.052648548036813736 Validation loss 0.04877243936061859 Accuracy 0.8722500205039978\n",
      "Iteration 33610 Training loss 0.037671659141778946 Validation loss 0.048756130039691925 Accuracy 0.8715000152587891\n",
      "Iteration 33620 Training loss 0.04625605046749115 Validation loss 0.04923178255558014 Accuracy 0.8708750605583191\n",
      "Iteration 33630 Training loss 0.04406481608748436 Validation loss 0.04870268329977989 Accuracy 0.8727500438690186\n",
      "Iteration 33640 Training loss 0.044048357754945755 Validation loss 0.04884755238890648 Accuracy 0.8706250190734863\n",
      "Iteration 33650 Training loss 0.04108479246497154 Validation loss 0.04894118756055832 Accuracy 0.8723750710487366\n",
      "Iteration 33660 Training loss 0.04472685232758522 Validation loss 0.04879426211118698 Accuracy 0.8692500591278076\n",
      "Iteration 33670 Training loss 0.04546770080924034 Validation loss 0.04956910014152527 Accuracy 0.8701250553131104\n",
      "Iteration 33680 Training loss 0.04191751405596733 Validation loss 0.04868645966053009 Accuracy 0.8726250529289246\n",
      "Iteration 33690 Training loss 0.04141481965780258 Validation loss 0.04960353299975395 Accuracy 0.8701250553131104\n",
      "Iteration 33700 Training loss 0.04858298972249031 Validation loss 0.048719700425863266 Accuracy 0.8728750348091125\n",
      "Iteration 33710 Training loss 0.055819071829319 Validation loss 0.04933660477399826 Accuracy 0.8710000514984131\n",
      "Iteration 33720 Training loss 0.043378766626119614 Validation loss 0.048982713371515274 Accuracy 0.8723750710487366\n",
      "Iteration 33730 Training loss 0.04604965075850487 Validation loss 0.04869328439235687 Accuracy 0.8720000386238098\n",
      "Iteration 33740 Training loss 0.04751555249094963 Validation loss 0.04950416460633278 Accuracy 0.8691250681877136\n",
      "Iteration 33750 Training loss 0.04846055060625076 Validation loss 0.049015406519174576 Accuracy 0.8723750710487366\n",
      "Iteration 33760 Training loss 0.045033980160951614 Validation loss 0.048690877854824066 Accuracy 0.8725000619888306\n",
      "Iteration 33770 Training loss 0.04152379930019379 Validation loss 0.04878227412700653 Accuracy 0.8730000257492065\n",
      "Iteration 33780 Training loss 0.049712855368852615 Validation loss 0.04873711243271828 Accuracy 0.8726250529289246\n",
      "Iteration 33790 Training loss 0.042836859822273254 Validation loss 0.04920095205307007 Accuracy 0.8721250295639038\n",
      "Iteration 33800 Training loss 0.04257657378911972 Validation loss 0.048853132873773575 Accuracy 0.8728750348091125\n",
      "Iteration 33810 Training loss 0.04148345813155174 Validation loss 0.04872617498040199 Accuracy 0.8732500672340393\n",
      "Iteration 33820 Training loss 0.03679841384291649 Validation loss 0.048662446439266205 Accuracy 0.8721250295639038\n",
      "Iteration 33830 Training loss 0.04372360184788704 Validation loss 0.048784948885440826 Accuracy 0.8705000281333923\n",
      "Iteration 33840 Training loss 0.04170632362365723 Validation loss 0.048667605966329575 Accuracy 0.8722500205039978\n",
      "Iteration 33850 Training loss 0.04184798523783684 Validation loss 0.04865478724241257 Accuracy 0.8727500438690186\n",
      "Iteration 33860 Training loss 0.048985280096530914 Validation loss 0.04865667596459389 Accuracy 0.8718750476837158\n",
      "Iteration 33870 Training loss 0.05091526731848717 Validation loss 0.05010475218296051 Accuracy 0.8681250214576721\n",
      "Iteration 33880 Training loss 0.03653515875339508 Validation loss 0.048928312957286835 Accuracy 0.8716250658035278\n",
      "Iteration 33890 Training loss 0.05210365355014801 Validation loss 0.049197085201740265 Accuracy 0.8680000305175781\n",
      "Iteration 33900 Training loss 0.04345858842134476 Validation loss 0.04863642528653145 Accuracy 0.8720000386238098\n",
      "Iteration 33910 Training loss 0.051052626222372055 Validation loss 0.048771344125270844 Accuracy 0.8731250166893005\n",
      "Iteration 33920 Training loss 0.053251102566719055 Validation loss 0.0486554279923439 Accuracy 0.8716250658035278\n",
      "Iteration 33930 Training loss 0.04018731042742729 Validation loss 0.04924910143017769 Accuracy 0.8706250190734863\n",
      "Iteration 33940 Training loss 0.04507288709282875 Validation loss 0.04881511256098747 Accuracy 0.8723750710487366\n",
      "Iteration 33950 Training loss 0.046080078929662704 Validation loss 0.049256425350904465 Accuracy 0.8702500462532043\n",
      "Iteration 33960 Training loss 0.04261961951851845 Validation loss 0.04874607175588608 Accuracy 0.8711250424385071\n",
      "Iteration 33970 Training loss 0.05076243355870247 Validation loss 0.04868470877408981 Accuracy 0.8716250658035278\n",
      "Iteration 33980 Training loss 0.04367862269282341 Validation loss 0.048701196908950806 Accuracy 0.8717500567436218\n",
      "Iteration 33990 Training loss 0.04888061806559563 Validation loss 0.04871644079685211 Accuracy 0.8716250658035278\n",
      "Iteration 34000 Training loss 0.04719424992799759 Validation loss 0.04871759191155434 Accuracy 0.8715000152587891\n",
      "Iteration 34010 Training loss 0.043347716331481934 Validation loss 0.04884854704141617 Accuracy 0.8691250681877136\n",
      "Iteration 34020 Training loss 0.04529215767979622 Validation loss 0.04920302703976631 Accuracy 0.8712500333786011\n",
      "Iteration 34030 Training loss 0.046649713069200516 Validation loss 0.049336694180965424 Accuracy 0.8718750476837158\n",
      "Iteration 34040 Training loss 0.05309046804904938 Validation loss 0.04864763095974922 Accuracy 0.8715000152587891\n",
      "Iteration 34050 Training loss 0.047231778502464294 Validation loss 0.04883913695812225 Accuracy 0.8723750710487366\n",
      "Iteration 34060 Training loss 0.0506853312253952 Validation loss 0.04901178926229477 Accuracy 0.8686250448226929\n",
      "Iteration 34070 Training loss 0.04299900308251381 Validation loss 0.04862276464700699 Accuracy 0.8702500462532043\n",
      "Iteration 34080 Training loss 0.044628772884607315 Validation loss 0.04869098216295242 Accuracy 0.8711250424385071\n",
      "Iteration 34090 Training loss 0.04557601734995842 Validation loss 0.04868590086698532 Accuracy 0.8713750243186951\n",
      "Iteration 34100 Training loss 0.035404570400714874 Validation loss 0.048753298819065094 Accuracy 0.8732500672340393\n",
      "Iteration 34110 Training loss 0.04658612981438637 Validation loss 0.04887253791093826 Accuracy 0.8691250681877136\n",
      "Iteration 34120 Training loss 0.042558372020721436 Validation loss 0.04884391650557518 Accuracy 0.8718750476837158\n",
      "Iteration 34130 Training loss 0.04395231604576111 Validation loss 0.04859369993209839 Accuracy 0.8712500333786011\n",
      "Iteration 34140 Training loss 0.04900356009602547 Validation loss 0.04860265552997589 Accuracy 0.8715000152587891\n",
      "Iteration 34150 Training loss 0.050689201802015305 Validation loss 0.04860911890864372 Accuracy 0.8722500205039978\n",
      "Iteration 34160 Training loss 0.038859523832798004 Validation loss 0.04862538352608681 Accuracy 0.8716250658035278\n",
      "Iteration 34170 Training loss 0.044400881975889206 Validation loss 0.04884718731045723 Accuracy 0.8726250529289246\n",
      "Iteration 34180 Training loss 0.03352133557200432 Validation loss 0.04869496449828148 Accuracy 0.8708750605583191\n",
      "Iteration 34190 Training loss 0.050962552428245544 Validation loss 0.048558786511421204 Accuracy 0.8730000257492065\n",
      "Iteration 34200 Training loss 0.05036255717277527 Validation loss 0.04876153543591499 Accuracy 0.8718750476837158\n",
      "Iteration 34210 Training loss 0.03971722349524498 Validation loss 0.04856906458735466 Accuracy 0.8718750476837158\n",
      "Iteration 34220 Training loss 0.043287504464387894 Validation loss 0.04860835149884224 Accuracy 0.8713750243186951\n",
      "Iteration 34230 Training loss 0.044370636343955994 Validation loss 0.04875731095671654 Accuracy 0.8696250319480896\n",
      "Iteration 34240 Training loss 0.03982006385922432 Validation loss 0.04865699261426926 Accuracy 0.8707500696182251\n",
      "Iteration 34250 Training loss 0.04226244240999222 Validation loss 0.048644281923770905 Accuracy 0.8736250400543213\n",
      "Iteration 34260 Training loss 0.041702527552843094 Validation loss 0.0487433485686779 Accuracy 0.8726250529289246\n",
      "Iteration 34270 Training loss 0.04763789102435112 Validation loss 0.048824869096279144 Accuracy 0.8728750348091125\n",
      "Iteration 34280 Training loss 0.0419604629278183 Validation loss 0.04871218651533127 Accuracy 0.8725000619888306\n",
      "Iteration 34290 Training loss 0.04404473677277565 Validation loss 0.04861980676651001 Accuracy 0.8716250658035278\n",
      "Iteration 34300 Training loss 0.045239705592393875 Validation loss 0.048630405217409134 Accuracy 0.8717500567436218\n",
      "Iteration 34310 Training loss 0.04689165949821472 Validation loss 0.04861835017800331 Accuracy 0.8715000152587891\n",
      "Iteration 34320 Training loss 0.04563407972455025 Validation loss 0.04908749461174011 Accuracy 0.8675000667572021\n",
      "Iteration 34330 Training loss 0.044654734432697296 Validation loss 0.04864373803138733 Accuracy 0.8721250295639038\n",
      "Iteration 34340 Training loss 0.04728727415204048 Validation loss 0.04859491437673569 Accuracy 0.8718750476837158\n",
      "Iteration 34350 Training loss 0.04188638553023338 Validation loss 0.04883386939764023 Accuracy 0.8695000410079956\n",
      "Iteration 34360 Training loss 0.045966871082782745 Validation loss 0.048662129789590836 Accuracy 0.8715000152587891\n",
      "Iteration 34370 Training loss 0.04212773218750954 Validation loss 0.04851877689361572 Accuracy 0.8728750348091125\n",
      "Iteration 34380 Training loss 0.047785766422748566 Validation loss 0.048659708350896835 Accuracy 0.8712500333786011\n",
      "Iteration 34390 Training loss 0.04256588965654373 Validation loss 0.05060041323304176 Accuracy 0.8627500534057617\n",
      "Iteration 34400 Training loss 0.04498966038227081 Validation loss 0.04853155463933945 Accuracy 0.8716250658035278\n",
      "Iteration 34410 Training loss 0.05429946631193161 Validation loss 0.048485126346349716 Accuracy 0.8712500333786011\n",
      "Iteration 34420 Training loss 0.046395719051361084 Validation loss 0.04851019009947777 Accuracy 0.8718750476837158\n",
      "Iteration 34430 Training loss 0.04350120574235916 Validation loss 0.04871268570423126 Accuracy 0.8718750476837158\n",
      "Iteration 34440 Training loss 0.046450477093458176 Validation loss 0.048479046672582626 Accuracy 0.8717500567436218\n",
      "Iteration 34450 Training loss 0.052976761013269424 Validation loss 0.04894234240055084 Accuracy 0.8682500123977661\n",
      "Iteration 34460 Training loss 0.034171316772699356 Validation loss 0.04850928112864494 Accuracy 0.8721250295639038\n",
      "Iteration 34470 Training loss 0.04133063182234764 Validation loss 0.048568956553936005 Accuracy 0.8732500672340393\n",
      "Iteration 34480 Training loss 0.0415569469332695 Validation loss 0.04855223745107651 Accuracy 0.8720000386238098\n",
      "Iteration 34490 Training loss 0.045321960002183914 Validation loss 0.04861827194690704 Accuracy 0.8723750710487366\n",
      "Iteration 34500 Training loss 0.051033057272434235 Validation loss 0.048511914908885956 Accuracy 0.8716250658035278\n",
      "Iteration 34510 Training loss 0.05001489818096161 Validation loss 0.04848726838827133 Accuracy 0.8722500205039978\n",
      "Iteration 34520 Training loss 0.046596042811870575 Validation loss 0.04884970188140869 Accuracy 0.8717500567436218\n",
      "Iteration 34530 Training loss 0.04492779076099396 Validation loss 0.04864075407385826 Accuracy 0.8728750348091125\n",
      "Iteration 34540 Training loss 0.037458695471286774 Validation loss 0.04880482330918312 Accuracy 0.8728750348091125\n",
      "Iteration 34550 Training loss 0.04808177053928375 Validation loss 0.048530999571084976 Accuracy 0.8732500672340393\n",
      "Iteration 34560 Training loss 0.040457285940647125 Validation loss 0.04850217327475548 Accuracy 0.8720000386238098\n",
      "Iteration 34570 Training loss 0.04533444344997406 Validation loss 0.048479821532964706 Accuracy 0.8730000257492065\n",
      "Iteration 34580 Training loss 0.03688017651438713 Validation loss 0.048747073858976364 Accuracy 0.8728750348091125\n",
      "Iteration 34590 Training loss 0.04293128103017807 Validation loss 0.04847768694162369 Accuracy 0.8725000619888306\n",
      "Iteration 34600 Training loss 0.03908933699131012 Validation loss 0.048616185784339905 Accuracy 0.8712500333786011\n",
      "Iteration 34610 Training loss 0.043503254652023315 Validation loss 0.04850982502102852 Accuracy 0.8728750348091125\n",
      "Iteration 34620 Training loss 0.043491773307323456 Validation loss 0.048851508647203445 Accuracy 0.8688750267028809\n",
      "Iteration 34630 Training loss 0.047574520111083984 Validation loss 0.048471033573150635 Accuracy 0.8725000619888306\n",
      "Iteration 34640 Training loss 0.04642808809876442 Validation loss 0.048479169607162476 Accuracy 0.8727500438690186\n",
      "Iteration 34650 Training loss 0.0439237542450428 Validation loss 0.04848159849643707 Accuracy 0.8721250295639038\n",
      "Iteration 34660 Training loss 0.043851662427186966 Validation loss 0.048480987548828125 Accuracy 0.874250054359436\n",
      "Iteration 34670 Training loss 0.04202040657401085 Validation loss 0.048633843660354614 Accuracy 0.8728750348091125\n",
      "Iteration 34680 Training loss 0.05146924778819084 Validation loss 0.04896249249577522 Accuracy 0.8731250166893005\n",
      "Iteration 34690 Training loss 0.04903515800833702 Validation loss 0.04852453991770744 Accuracy 0.8731250166893005\n",
      "Iteration 34700 Training loss 0.042672932147979736 Validation loss 0.048490218818187714 Accuracy 0.8720000386238098\n",
      "Iteration 34710 Training loss 0.04285191372036934 Validation loss 0.04852686822414398 Accuracy 0.8737500309944153\n",
      "Iteration 34720 Training loss 0.04265850782394409 Validation loss 0.04844294860959053 Accuracy 0.8728750348091125\n",
      "Iteration 34730 Training loss 0.043652743101119995 Validation loss 0.048455122858285904 Accuracy 0.8731250166893005\n",
      "Iteration 34740 Training loss 0.04189109802246094 Validation loss 0.048753321170806885 Accuracy 0.8727500438690186\n",
      "Iteration 34750 Training loss 0.04134088754653931 Validation loss 0.048489343374967575 Accuracy 0.8720000386238098\n",
      "Iteration 34760 Training loss 0.04776914417743683 Validation loss 0.049220941960811615 Accuracy 0.8723750710487366\n",
      "Iteration 34770 Training loss 0.04360797628760338 Validation loss 0.04854602739214897 Accuracy 0.8711250424385071\n",
      "Iteration 34780 Training loss 0.04713486507534981 Validation loss 0.04974496364593506 Accuracy 0.8696250319480896\n",
      "Iteration 34790 Training loss 0.05352930352091789 Validation loss 0.048458244651556015 Accuracy 0.8722500205039978\n",
      "Iteration 34800 Training loss 0.04594655707478523 Validation loss 0.04843809828162193 Accuracy 0.8722500205039978\n",
      "Iteration 34810 Training loss 0.05309172347187996 Validation loss 0.04972830042243004 Accuracy 0.8701250553131104\n",
      "Iteration 34820 Training loss 0.04918039217591286 Validation loss 0.04835541546344757 Accuracy 0.8730000257492065\n",
      "Iteration 34830 Training loss 0.05450303480029106 Validation loss 0.048668283969163895 Accuracy 0.8737500309944153\n",
      "Iteration 34840 Training loss 0.04464014992117882 Validation loss 0.04836030304431915 Accuracy 0.8732500672340393\n",
      "Iteration 34850 Training loss 0.04228091984987259 Validation loss 0.04848857969045639 Accuracy 0.8738750219345093\n",
      "Iteration 34860 Training loss 0.04083189368247986 Validation loss 0.04835400730371475 Accuracy 0.8740000128746033\n",
      "Iteration 34870 Training loss 0.04710569232702255 Validation loss 0.04851846396923065 Accuracy 0.8731250166893005\n",
      "Iteration 34880 Training loss 0.042694032192230225 Validation loss 0.04838643595576286 Accuracy 0.8735000491142273\n",
      "Iteration 34890 Training loss 0.03697468340396881 Validation loss 0.04843728989362717 Accuracy 0.8732500672340393\n",
      "Iteration 34900 Training loss 0.04278995469212532 Validation loss 0.04839315637946129 Accuracy 0.874250054359436\n",
      "Iteration 34910 Training loss 0.04535401239991188 Validation loss 0.04851919040083885 Accuracy 0.8710000514984131\n",
      "Iteration 34920 Training loss 0.04192621260881424 Validation loss 0.04832955449819565 Accuracy 0.8730000257492065\n",
      "Iteration 34930 Training loss 0.04945982247591019 Validation loss 0.04875697195529938 Accuracy 0.8733750581741333\n",
      "Iteration 34940 Training loss 0.04842056334018707 Validation loss 0.04844946041703224 Accuracy 0.8738750219345093\n",
      "Iteration 34950 Training loss 0.04743234068155289 Validation loss 0.04833868891000748 Accuracy 0.8730000257492065\n",
      "Iteration 34960 Training loss 0.051940687000751495 Validation loss 0.04832989722490311 Accuracy 0.8728750348091125\n",
      "Iteration 34970 Training loss 0.05058100074529648 Validation loss 0.0483771450817585 Accuracy 0.8726250529289246\n",
      "Iteration 34980 Training loss 0.04609532654285431 Validation loss 0.048423875123262405 Accuracy 0.8715000152587891\n",
      "Iteration 34990 Training loss 0.04710612818598747 Validation loss 0.04837287962436676 Accuracy 0.8728750348091125\n",
      "Iteration 35000 Training loss 0.035876836627721786 Validation loss 0.04920302703976631 Accuracy 0.8723750710487366\n",
      "Iteration 35010 Training loss 0.04276444390416145 Validation loss 0.048943471163511276 Accuracy 0.8737500309944153\n",
      "Iteration 35020 Training loss 0.04810502380132675 Validation loss 0.048505496233701706 Accuracy 0.8711250424385071\n",
      "Iteration 35030 Training loss 0.03884308412671089 Validation loss 0.04840809106826782 Accuracy 0.8717500567436218\n",
      "Iteration 35040 Training loss 0.04733530804514885 Validation loss 0.04830553010106087 Accuracy 0.8731250166893005\n",
      "Iteration 35050 Training loss 0.04251492768526077 Validation loss 0.048432838171720505 Accuracy 0.8713750243186951\n",
      "Iteration 35060 Training loss 0.04625007137656212 Validation loss 0.04832521453499794 Accuracy 0.8728750348091125\n",
      "Iteration 35070 Training loss 0.04061213880777359 Validation loss 0.0483207032084465 Accuracy 0.8721250295639038\n",
      "Iteration 35080 Training loss 0.05400065332651138 Validation loss 0.049424998462200165 Accuracy 0.8715000152587891\n",
      "Iteration 35090 Training loss 0.04259761422872543 Validation loss 0.04837796092033386 Accuracy 0.8726250529289246\n",
      "Iteration 35100 Training loss 0.053131844848394394 Validation loss 0.048805829137563705 Accuracy 0.8737500309944153\n",
      "Iteration 35110 Training loss 0.04641616716980934 Validation loss 0.04856988415122032 Accuracy 0.8737500309944153\n",
      "Iteration 35120 Training loss 0.04374169185757637 Validation loss 0.04921144247055054 Accuracy 0.8730000257492065\n",
      "Iteration 35130 Training loss 0.051725950092077255 Validation loss 0.048558518290519714 Accuracy 0.874125063419342\n",
      "Iteration 35140 Training loss 0.04329530522227287 Validation loss 0.048279888927936554 Accuracy 0.8733750581741333\n",
      "Iteration 35150 Training loss 0.03614044561982155 Validation loss 0.04877530783414841 Accuracy 0.8687500357627869\n",
      "Iteration 35160 Training loss 0.0424480065703392 Validation loss 0.04841471463441849 Accuracy 0.8737500309944153\n",
      "Iteration 35170 Training loss 0.040717270225286484 Validation loss 0.04925493523478508 Accuracy 0.8723750710487366\n",
      "Iteration 35180 Training loss 0.03903651982545853 Validation loss 0.04829110950231552 Accuracy 0.8738750219345093\n",
      "Iteration 35190 Training loss 0.04347463324666023 Validation loss 0.04832448810338974 Accuracy 0.8740000128746033\n",
      "Iteration 35200 Training loss 0.041379936039447784 Validation loss 0.04825698956847191 Accuracy 0.8732500672340393\n",
      "Iteration 35210 Training loss 0.042843759059906006 Validation loss 0.04841116443276405 Accuracy 0.8715000152587891\n",
      "Iteration 35220 Training loss 0.04311937093734741 Validation loss 0.04836716875433922 Accuracy 0.8737500309944153\n",
      "Iteration 35230 Training loss 0.04840777441859245 Validation loss 0.04843924939632416 Accuracy 0.8728750348091125\n",
      "Iteration 35240 Training loss 0.046997539699077606 Validation loss 0.04838438704609871 Accuracy 0.8720000386238098\n",
      "Iteration 35250 Training loss 0.03932361677289009 Validation loss 0.04822716489434242 Accuracy 0.874125063419342\n",
      "Iteration 35260 Training loss 0.037463512271642685 Validation loss 0.048250921070575714 Accuracy 0.8738750219345093\n",
      "Iteration 35270 Training loss 0.043622493743896484 Validation loss 0.04828757047653198 Accuracy 0.8721250295639038\n",
      "Iteration 35280 Training loss 0.043728332966566086 Validation loss 0.04845649003982544 Accuracy 0.8725000619888306\n",
      "Iteration 35290 Training loss 0.04182305186986923 Validation loss 0.04829200729727745 Accuracy 0.8735000491142273\n",
      "Iteration 35300 Training loss 0.032692741602659225 Validation loss 0.04871708154678345 Accuracy 0.8701250553131104\n",
      "Iteration 35310 Training loss 0.0453779511153698 Validation loss 0.04833752661943436 Accuracy 0.8721250295639038\n",
      "Iteration 35320 Training loss 0.04681488871574402 Validation loss 0.04824979975819588 Accuracy 0.8736250400543213\n",
      "Iteration 35330 Training loss 0.046863969415426254 Validation loss 0.04864051192998886 Accuracy 0.8692500591278076\n",
      "Iteration 35340 Training loss 0.037463799118995667 Validation loss 0.04837585985660553 Accuracy 0.8733750581741333\n",
      "Iteration 35350 Training loss 0.05689852684736252 Validation loss 0.048262789845466614 Accuracy 0.8717500567436218\n",
      "Iteration 35360 Training loss 0.047736603766679764 Validation loss 0.04818335920572281 Accuracy 0.8732500672340393\n",
      "Iteration 35370 Training loss 0.04307398572564125 Validation loss 0.048233479261398315 Accuracy 0.8723750710487366\n",
      "Iteration 35380 Training loss 0.0388752743601799 Validation loss 0.0482928492128849 Accuracy 0.8736250400543213\n",
      "Iteration 35390 Training loss 0.04117245599627495 Validation loss 0.048199594020843506 Accuracy 0.8722500205039978\n",
      "Iteration 35400 Training loss 0.04825971648097038 Validation loss 0.04861224442720413 Accuracy 0.8730000257492065\n",
      "Iteration 35410 Training loss 0.044087089598178864 Validation loss 0.04819956421852112 Accuracy 0.8727500438690186\n",
      "Iteration 35420 Training loss 0.04567062109708786 Validation loss 0.04843854904174805 Accuracy 0.8732500672340393\n",
      "Iteration 35430 Training loss 0.047890499234199524 Validation loss 0.04816827550530434 Accuracy 0.8731250166893005\n",
      "Iteration 35440 Training loss 0.04440813139081001 Validation loss 0.04836270213127136 Accuracy 0.8731250166893005\n",
      "Iteration 35450 Training loss 0.04946878179907799 Validation loss 0.04826214164495468 Accuracy 0.8740000128746033\n",
      "Iteration 35460 Training loss 0.05012302100658417 Validation loss 0.04859634116292 Accuracy 0.8732500672340393\n",
      "Iteration 35470 Training loss 0.04294518381357193 Validation loss 0.04840471222996712 Accuracy 0.8717500567436218\n",
      "Iteration 35480 Training loss 0.05063575506210327 Validation loss 0.04838944599032402 Accuracy 0.8730000257492065\n",
      "Iteration 35490 Training loss 0.0459345243871212 Validation loss 0.04817604646086693 Accuracy 0.8740000128746033\n",
      "Iteration 35500 Training loss 0.04436517506837845 Validation loss 0.048227302730083466 Accuracy 0.874250054359436\n",
      "Iteration 35510 Training loss 0.043507665395736694 Validation loss 0.04820416495203972 Accuracy 0.8735000491142273\n",
      "Iteration 35520 Training loss 0.0381958894431591 Validation loss 0.048231080174446106 Accuracy 0.8740000128746033\n",
      "Iteration 35530 Training loss 0.05589132755994797 Validation loss 0.048565879464149475 Accuracy 0.874250054359436\n",
      "Iteration 35540 Training loss 0.05025840550661087 Validation loss 0.04824608191847801 Accuracy 0.8731250166893005\n",
      "Iteration 35550 Training loss 0.04661703109741211 Validation loss 0.048602160066366196 Accuracy 0.8740000128746033\n",
      "Iteration 35560 Training loss 0.040597692131996155 Validation loss 0.048257749527692795 Accuracy 0.8725000619888306\n",
      "Iteration 35570 Training loss 0.04799085110425949 Validation loss 0.0483759306371212 Accuracy 0.8735000491142273\n",
      "Iteration 35580 Training loss 0.032020583748817444 Validation loss 0.04840175062417984 Accuracy 0.8736250400543213\n",
      "Iteration 35590 Training loss 0.04074554517865181 Validation loss 0.04821149632334709 Accuracy 0.8730000257492065\n",
      "Iteration 35600 Training loss 0.050075359642505646 Validation loss 0.048652805387973785 Accuracy 0.8686250448226929\n",
      "Iteration 35610 Training loss 0.0400780625641346 Validation loss 0.048174869269132614 Accuracy 0.8731250166893005\n",
      "Iteration 35620 Training loss 0.05233879014849663 Validation loss 0.04822228476405144 Accuracy 0.8725000619888306\n",
      "Iteration 35630 Training loss 0.036872748285532 Validation loss 0.04813951626420021 Accuracy 0.8740000128746033\n",
      "Iteration 35640 Training loss 0.04738827049732208 Validation loss 0.04857344925403595 Accuracy 0.8708750605583191\n",
      "Iteration 35650 Training loss 0.04695451632142067 Validation loss 0.0481996163725853 Accuracy 0.874625027179718\n",
      "Iteration 35660 Training loss 0.046023719012737274 Validation loss 0.04850737005472183 Accuracy 0.8712500333786011\n",
      "Iteration 35670 Training loss 0.05086031183600426 Validation loss 0.048243213444948196 Accuracy 0.8726250529289246\n",
      "Iteration 35680 Training loss 0.040440578013658524 Validation loss 0.04814369976520538 Accuracy 0.8738750219345093\n",
      "Iteration 35690 Training loss 0.045382242649793625 Validation loss 0.04827330261468887 Accuracy 0.8732500672340393\n",
      "Iteration 35700 Training loss 0.04574938118457794 Validation loss 0.048474010080099106 Accuracy 0.8700000643730164\n",
      "Iteration 35710 Training loss 0.04013654589653015 Validation loss 0.048122555017471313 Accuracy 0.8732500672340393\n",
      "Iteration 35720 Training loss 0.05155225470662117 Validation loss 0.04815342277288437 Accuracy 0.874125063419342\n",
      "Iteration 35730 Training loss 0.05050908029079437 Validation loss 0.04863811284303665 Accuracy 0.8692500591278076\n",
      "Iteration 35740 Training loss 0.044751692563295364 Validation loss 0.0480462983250618 Accuracy 0.874125063419342\n",
      "Iteration 35750 Training loss 0.04194003343582153 Validation loss 0.04810108616948128 Accuracy 0.874125063419342\n",
      "Iteration 35760 Training loss 0.034551337361335754 Validation loss 0.048061590641736984 Accuracy 0.874750018119812\n",
      "Iteration 35770 Training loss 0.04313346743583679 Validation loss 0.0481022372841835 Accuracy 0.8727500438690186\n",
      "Iteration 35780 Training loss 0.04740336909890175 Validation loss 0.048241883516311646 Accuracy 0.874250054359436\n",
      "Iteration 35790 Training loss 0.04896637797355652 Validation loss 0.04868273437023163 Accuracy 0.8758750557899475\n",
      "Iteration 35800 Training loss 0.044235486537218094 Validation loss 0.04861655831336975 Accuracy 0.874750018119812\n",
      "Iteration 35810 Training loss 0.0422406904399395 Validation loss 0.04815289378166199 Accuracy 0.8740000128746033\n",
      "Iteration 35820 Training loss 0.04634946584701538 Validation loss 0.04846354201436043 Accuracy 0.8696250319480896\n",
      "Iteration 35830 Training loss 0.04687714949250221 Validation loss 0.048132821917533875 Accuracy 0.87437504529953\n",
      "Iteration 35840 Training loss 0.04525305703282356 Validation loss 0.0482737235724926 Accuracy 0.8706250190734863\n",
      "Iteration 35850 Training loss 0.03953256830573082 Validation loss 0.048297811299562454 Accuracy 0.8703750371932983\n",
      "Iteration 35860 Training loss 0.04261576384305954 Validation loss 0.048216018825769424 Accuracy 0.874250054359436\n",
      "Iteration 35870 Training loss 0.04094887524843216 Validation loss 0.048163384199142456 Accuracy 0.8736250400543213\n",
      "Iteration 35880 Training loss 0.043808963149785995 Validation loss 0.048129819333553314 Accuracy 0.8732500672340393\n",
      "Iteration 35890 Training loss 0.04493892565369606 Validation loss 0.048133641481399536 Accuracy 0.8736250400543213\n",
      "Iteration 35900 Training loss 0.037597425282001495 Validation loss 0.04823144152760506 Accuracy 0.8756250143051147\n",
      "Iteration 35910 Training loss 0.043902553617954254 Validation loss 0.0483408086001873 Accuracy 0.8716250658035278\n",
      "Iteration 35920 Training loss 0.054286591708660126 Validation loss 0.04840845614671707 Accuracy 0.874625027179718\n",
      "Iteration 35930 Training loss 0.0469573549926281 Validation loss 0.048607610166072845 Accuracy 0.874750018119812\n",
      "Iteration 35940 Training loss 0.0546896830201149 Validation loss 0.04804420843720436 Accuracy 0.87437504529953\n",
      "Iteration 35950 Training loss 0.04283000901341438 Validation loss 0.04810322821140289 Accuracy 0.8736250400543213\n",
      "Iteration 35960 Training loss 0.04179263859987259 Validation loss 0.04941849038004875 Accuracy 0.8716250658035278\n",
      "Iteration 35970 Training loss 0.03791940212249756 Validation loss 0.04815281182527542 Accuracy 0.8726250529289246\n",
      "Iteration 35980 Training loss 0.03953007981181145 Validation loss 0.048068538308143616 Accuracy 0.874125063419342\n",
      "Iteration 35990 Training loss 0.045561302453279495 Validation loss 0.04804759472608566 Accuracy 0.874250054359436\n",
      "Iteration 36000 Training loss 0.04885035380721092 Validation loss 0.04864615201950073 Accuracy 0.8737500309944153\n",
      "Iteration 36010 Training loss 0.03788397088646889 Validation loss 0.04813527315855026 Accuracy 0.8727500438690186\n",
      "Iteration 36020 Training loss 0.048572372645139694 Validation loss 0.04810795933008194 Accuracy 0.8718750476837158\n",
      "Iteration 36030 Training loss 0.03797416761517525 Validation loss 0.048073943704366684 Accuracy 0.8733750581741333\n",
      "Iteration 36040 Training loss 0.04717544466257095 Validation loss 0.04811432957649231 Accuracy 0.8733750581741333\n",
      "Iteration 36050 Training loss 0.03610476478934288 Validation loss 0.04829884693026543 Accuracy 0.8710000514984131\n",
      "Iteration 36060 Training loss 0.04211254045367241 Validation loss 0.04862223193049431 Accuracy 0.874125063419342\n",
      "Iteration 36070 Training loss 0.046477556228637695 Validation loss 0.04824460670351982 Accuracy 0.8751250505447388\n",
      "Iteration 36080 Training loss 0.05210115388035774 Validation loss 0.048157285898923874 Accuracy 0.8757500648498535\n",
      "Iteration 36090 Training loss 0.0497773177921772 Validation loss 0.04804525524377823 Accuracy 0.8725000619888306\n",
      "Iteration 36100 Training loss 0.043918807059526443 Validation loss 0.04819450527429581 Accuracy 0.8740000128746033\n",
      "Iteration 36110 Training loss 0.045216891914606094 Validation loss 0.04816116392612457 Accuracy 0.8726250529289246\n",
      "Iteration 36120 Training loss 0.04474280774593353 Validation loss 0.04882451891899109 Accuracy 0.8730000257492065\n",
      "Iteration 36130 Training loss 0.04666777327656746 Validation loss 0.048057373613119125 Accuracy 0.874250054359436\n",
      "Iteration 36140 Training loss 0.04020005092024803 Validation loss 0.04809951409697533 Accuracy 0.8738750219345093\n",
      "Iteration 36150 Training loss 0.04028327390551567 Validation loss 0.04803873598575592 Accuracy 0.8755000233650208\n",
      "Iteration 36160 Training loss 0.0476202629506588 Validation loss 0.04804325848817825 Accuracy 0.874625027179718\n",
      "Iteration 36170 Training loss 0.04439561814069748 Validation loss 0.04812920093536377 Accuracy 0.874500036239624\n",
      "Iteration 36180 Training loss 0.03646238148212433 Validation loss 0.04819754138588905 Accuracy 0.8723750710487366\n",
      "Iteration 36190 Training loss 0.03921915963292122 Validation loss 0.04828713461756706 Accuracy 0.8718750476837158\n",
      "Iteration 36200 Training loss 0.041844237595796585 Validation loss 0.0480615608394146 Accuracy 0.8733750581741333\n",
      "Iteration 36210 Training loss 0.051619499921798706 Validation loss 0.048219479620456696 Accuracy 0.8718750476837158\n",
      "Iteration 36220 Training loss 0.05151677504181862 Validation loss 0.048379942774772644 Accuracy 0.874625027179718\n",
      "Iteration 36230 Training loss 0.05003225803375244 Validation loss 0.0484333299100399 Accuracy 0.8701250553131104\n",
      "Iteration 36240 Training loss 0.047411464154720306 Validation loss 0.049470994621515274 Accuracy 0.8651250600814819\n",
      "Iteration 36250 Training loss 0.040337055921554565 Validation loss 0.04795905202627182 Accuracy 0.874625027179718\n",
      "Iteration 36260 Training loss 0.038573674857616425 Validation loss 0.048169028013944626 Accuracy 0.8750000596046448\n",
      "Iteration 36270 Training loss 0.04704848304390907 Validation loss 0.04849569872021675 Accuracy 0.8697500228881836\n",
      "Iteration 36280 Training loss 0.039513859897851944 Validation loss 0.04796183481812477 Accuracy 0.8736250400543213\n",
      "Iteration 36290 Training loss 0.044970978051424026 Validation loss 0.048521559685468674 Accuracy 0.8686250448226929\n",
      "Iteration 36300 Training loss 0.04321473091840744 Validation loss 0.04811881110072136 Accuracy 0.8720000386238098\n",
      "Iteration 36310 Training loss 0.05131510645151138 Validation loss 0.04807395115494728 Accuracy 0.8728750348091125\n",
      "Iteration 36320 Training loss 0.038160186260938644 Validation loss 0.04870471730828285 Accuracy 0.87437504529953\n",
      "Iteration 36330 Training loss 0.04188952222466469 Validation loss 0.04795095697045326 Accuracy 0.8751250505447388\n",
      "Iteration 36340 Training loss 0.04089302569627762 Validation loss 0.048000916838645935 Accuracy 0.8751250505447388\n",
      "Iteration 36350 Training loss 0.04801357164978981 Validation loss 0.04821775481104851 Accuracy 0.8751250505447388\n",
      "Iteration 36360 Training loss 0.042941369116306305 Validation loss 0.04801298305392265 Accuracy 0.8727500438690186\n",
      "Iteration 36370 Training loss 0.04078907519578934 Validation loss 0.04795489087700844 Accuracy 0.8750000596046448\n",
      "Iteration 36380 Training loss 0.046044740825891495 Validation loss 0.048249825835227966 Accuracy 0.8710000514984131\n",
      "Iteration 36390 Training loss 0.048701465129852295 Validation loss 0.047954075038433075 Accuracy 0.8733750581741333\n",
      "Iteration 36400 Training loss 0.04275359958410263 Validation loss 0.0481126643717289 Accuracy 0.8750000596046448\n",
      "Iteration 36410 Training loss 0.04636773094534874 Validation loss 0.048100199550390244 Accuracy 0.8725000619888306\n",
      "Iteration 36420 Training loss 0.0398377887904644 Validation loss 0.04799484461545944 Accuracy 0.8726250529289246\n",
      "Iteration 36430 Training loss 0.04800482466816902 Validation loss 0.04828023165464401 Accuracy 0.8711250424385071\n",
      "Iteration 36440 Training loss 0.03990209102630615 Validation loss 0.04797022417187691 Accuracy 0.8732500672340393\n",
      "Iteration 36450 Training loss 0.053954146802425385 Validation loss 0.047975990921258926 Accuracy 0.8732500672340393\n",
      "Iteration 36460 Training loss 0.03476230427622795 Validation loss 0.048683930188417435 Accuracy 0.8692500591278076\n",
      "Iteration 36470 Training loss 0.04577895998954773 Validation loss 0.04841925576329231 Accuracy 0.8702500462532043\n",
      "Iteration 36480 Training loss 0.048498377203941345 Validation loss 0.04795289784669876 Accuracy 0.8732500672340393\n",
      "Iteration 36490 Training loss 0.04482660070061684 Validation loss 0.048929911106824875 Accuracy 0.8738750219345093\n",
      "Iteration 36500 Training loss 0.042653921991586685 Validation loss 0.0481736920773983 Accuracy 0.874500036239624\n",
      "Iteration 36510 Training loss 0.05002773180603981 Validation loss 0.04802972823381424 Accuracy 0.8748750686645508\n",
      "Iteration 36520 Training loss 0.04176412522792816 Validation loss 0.0480104424059391 Accuracy 0.8728750348091125\n",
      "Iteration 36530 Training loss 0.0435965433716774 Validation loss 0.04792100936174393 Accuracy 0.8735000491142273\n",
      "Iteration 36540 Training loss 0.0442081019282341 Validation loss 0.04804059863090515 Accuracy 0.8722500205039978\n",
      "Iteration 36550 Training loss 0.04644004628062248 Validation loss 0.04788675904273987 Accuracy 0.874250054359436\n",
      "Iteration 36560 Training loss 0.04841810092329979 Validation loss 0.047977082431316376 Accuracy 0.8727500438690186\n",
      "Iteration 36570 Training loss 0.04130024462938309 Validation loss 0.04791263863444328 Accuracy 0.8736250400543213\n",
      "Iteration 36580 Training loss 0.04587097838521004 Validation loss 0.04788484424352646 Accuracy 0.874250054359436\n",
      "Iteration 36590 Training loss 0.03580455854535103 Validation loss 0.04800458997488022 Accuracy 0.8752500414848328\n",
      "Iteration 36600 Training loss 0.05069040134549141 Validation loss 0.048061054199934006 Accuracy 0.8725000619888306\n",
      "Iteration 36610 Training loss 0.04550553113222122 Validation loss 0.04899794980883598 Accuracy 0.8681250214576721\n",
      "Iteration 36620 Training loss 0.04060100018978119 Validation loss 0.04817988723516464 Accuracy 0.8722500205039978\n",
      "Iteration 36630 Training loss 0.03889157623052597 Validation loss 0.047986820340156555 Accuracy 0.8727500438690186\n",
      "Iteration 36640 Training loss 0.04306598752737045 Validation loss 0.04813261330127716 Accuracy 0.8728750348091125\n",
      "Iteration 36650 Training loss 0.04054315388202667 Validation loss 0.04790177196264267 Accuracy 0.8732500672340393\n",
      "Iteration 36660 Training loss 0.04681622236967087 Validation loss 0.0484081506729126 Accuracy 0.8756250143051147\n",
      "Iteration 36670 Training loss 0.03833156079053879 Validation loss 0.04792480543255806 Accuracy 0.874500036239624\n",
      "Iteration 36680 Training loss 0.044933442026376724 Validation loss 0.04803432896733284 Accuracy 0.8755000233650208\n",
      "Iteration 36690 Training loss 0.0407090000808239 Validation loss 0.04830685630440712 Accuracy 0.8755000233650208\n",
      "Iteration 36700 Training loss 0.044710416346788406 Validation loss 0.048059601336717606 Accuracy 0.8717500567436218\n",
      "Iteration 36710 Training loss 0.04636510834097862 Validation loss 0.048150040209293365 Accuracy 0.8727500438690186\n",
      "Iteration 36720 Training loss 0.04074995964765549 Validation loss 0.048019733279943466 Accuracy 0.8738750219345093\n",
      "Iteration 36730 Training loss 0.04951843246817589 Validation loss 0.04791073128581047 Accuracy 0.874125063419342\n",
      "Iteration 36740 Training loss 0.04278356581926346 Validation loss 0.047965601086616516 Accuracy 0.8732500672340393\n",
      "Iteration 36750 Training loss 0.043116550892591476 Validation loss 0.04790081828832626 Accuracy 0.8756250143051147\n",
      "Iteration 36760 Training loss 0.03974108770489693 Validation loss 0.04864704981446266 Accuracy 0.8686250448226929\n",
      "Iteration 36770 Training loss 0.03784204646945 Validation loss 0.04788100719451904 Accuracy 0.8753750324249268\n",
      "Iteration 36780 Training loss 0.04759841039776802 Validation loss 0.04786323755979538 Accuracy 0.8755000233650208\n",
      "Iteration 36790 Training loss 0.04202146455645561 Validation loss 0.04816356301307678 Accuracy 0.8711250424385071\n",
      "Iteration 36800 Training loss 0.04730570316314697 Validation loss 0.04824744910001755 Accuracy 0.87437504529953\n",
      "Iteration 36810 Training loss 0.03649463877081871 Validation loss 0.04790686443448067 Accuracy 0.8740000128746033\n",
      "Iteration 36820 Training loss 0.04464351385831833 Validation loss 0.04790967330336571 Accuracy 0.8738750219345093\n",
      "Iteration 36830 Training loss 0.041298069059848785 Validation loss 0.04794731363654137 Accuracy 0.8723750710487366\n",
      "Iteration 36840 Training loss 0.053433410823345184 Validation loss 0.04827747493982315 Accuracy 0.8750000596046448\n",
      "Iteration 36850 Training loss 0.03938277065753937 Validation loss 0.04783935472369194 Accuracy 0.874125063419342\n",
      "Iteration 36860 Training loss 0.03610670194029808 Validation loss 0.048046499490737915 Accuracy 0.8755000233650208\n",
      "Iteration 36870 Training loss 0.04425252974033356 Validation loss 0.04794381558895111 Accuracy 0.8752500414848328\n",
      "Iteration 36880 Training loss 0.05045582726597786 Validation loss 0.048324666917324066 Accuracy 0.8707500696182251\n",
      "Iteration 36890 Training loss 0.04458397626876831 Validation loss 0.048105355352163315 Accuracy 0.8760000467300415\n",
      "Iteration 36900 Training loss 0.04751387611031532 Validation loss 0.04788830876350403 Accuracy 0.8736250400543213\n",
      "Iteration 36910 Training loss 0.041894759982824326 Validation loss 0.047869741916656494 Accuracy 0.8735000491142273\n",
      "Iteration 36920 Training loss 0.04298629239201546 Validation loss 0.04791657626628876 Accuracy 0.8732500672340393\n",
      "Iteration 36930 Training loss 0.04682280868291855 Validation loss 0.047832027077674866 Accuracy 0.87437504529953\n",
      "Iteration 36940 Training loss 0.044359106570482254 Validation loss 0.04808855801820755 Accuracy 0.8721250295639038\n",
      "Iteration 36950 Training loss 0.04138496145606041 Validation loss 0.04799198359251022 Accuracy 0.8756250143051147\n",
      "Iteration 36960 Training loss 0.04307284206151962 Validation loss 0.0482320562005043 Accuracy 0.8748750686645508\n",
      "Iteration 36970 Training loss 0.043411023914813995 Validation loss 0.0483739972114563 Accuracy 0.8701250553131104\n",
      "Iteration 36980 Training loss 0.042009901255369186 Validation loss 0.04781191051006317 Accuracy 0.8758750557899475\n",
      "Iteration 36990 Training loss 0.04163427650928497 Validation loss 0.04802490398287773 Accuracy 0.8713750243186951\n",
      "Iteration 37000 Training loss 0.033975329250097275 Validation loss 0.04781954362988472 Accuracy 0.8748750686645508\n",
      "Iteration 37010 Training loss 0.05761552229523659 Validation loss 0.04800422117114067 Accuracy 0.8763750195503235\n",
      "Iteration 37020 Training loss 0.04379703477025032 Validation loss 0.04790131747722626 Accuracy 0.8725000619888306\n",
      "Iteration 37030 Training loss 0.04610283672809601 Validation loss 0.04812459275126457 Accuracy 0.8750000596046448\n",
      "Iteration 37040 Training loss 0.0441744364798069 Validation loss 0.04799098148941994 Accuracy 0.8721250295639038\n",
      "Iteration 37050 Training loss 0.048424091190099716 Validation loss 0.04789004847407341 Accuracy 0.8721250295639038\n",
      "Iteration 37060 Training loss 0.03891780227422714 Validation loss 0.04777878150343895 Accuracy 0.87437504529953\n",
      "Iteration 37070 Training loss 0.038105811923742294 Validation loss 0.04791347682476044 Accuracy 0.8765000700950623\n",
      "Iteration 37080 Training loss 0.0446174219250679 Validation loss 0.04778926447033882 Accuracy 0.8756250143051147\n",
      "Iteration 37090 Training loss 0.047467298805713654 Validation loss 0.047825828194618225 Accuracy 0.8755000233650208\n",
      "Iteration 37100 Training loss 0.0456848181784153 Validation loss 0.04783846437931061 Accuracy 0.8765000700950623\n",
      "Iteration 37110 Training loss 0.042195308953523636 Validation loss 0.04781582951545715 Accuracy 0.8730000257492065\n",
      "Iteration 37120 Training loss 0.03977065905928612 Validation loss 0.04879399761557579 Accuracy 0.8688750267028809\n",
      "Iteration 37130 Training loss 0.04131140559911728 Validation loss 0.04777604341506958 Accuracy 0.8728750348091125\n",
      "Iteration 37140 Training loss 0.049850624054670334 Validation loss 0.04792667180299759 Accuracy 0.8757500648498535\n",
      "Iteration 37150 Training loss 0.04051627218723297 Validation loss 0.04805247485637665 Accuracy 0.8723750710487366\n",
      "Iteration 37160 Training loss 0.04708530753850937 Validation loss 0.04771897941827774 Accuracy 0.874250054359436\n",
      "Iteration 37170 Training loss 0.05192567780613899 Validation loss 0.048793110996484756 Accuracy 0.8678750395774841\n",
      "Iteration 37180 Training loss 0.042293183505535126 Validation loss 0.04802507162094116 Accuracy 0.8722500205039978\n",
      "Iteration 37190 Training loss 0.047013059258461 Validation loss 0.047884501516819 Accuracy 0.8763750195503235\n",
      "Iteration 37200 Training loss 0.040834978222846985 Validation loss 0.04848852753639221 Accuracy 0.8695000410079956\n",
      "Iteration 37210 Training loss 0.045582160353660583 Validation loss 0.04785225912928581 Accuracy 0.8762500286102295\n",
      "Iteration 37220 Training loss 0.035536739975214005 Validation loss 0.04794991388916969 Accuracy 0.8753750324249268\n",
      "Iteration 37230 Training loss 0.04217500612139702 Validation loss 0.04800430312752724 Accuracy 0.8716250658035278\n",
      "Iteration 37240 Training loss 0.048699770122766495 Validation loss 0.047745898365974426 Accuracy 0.8751250505447388\n",
      "Iteration 37250 Training loss 0.03975164145231247 Validation loss 0.04772893711924553 Accuracy 0.8748750686645508\n",
      "Iteration 37260 Training loss 0.042559996247291565 Validation loss 0.04849519208073616 Accuracy 0.8691250681877136\n",
      "Iteration 37270 Training loss 0.04227262735366821 Validation loss 0.047759365290403366 Accuracy 0.874750018119812\n",
      "Iteration 37280 Training loss 0.04522157460451126 Validation loss 0.047771576792001724 Accuracy 0.874125063419342\n",
      "Iteration 37290 Training loss 0.04708665609359741 Validation loss 0.047987308353185654 Accuracy 0.8762500286102295\n",
      "Iteration 37300 Training loss 0.04458681121468544 Validation loss 0.04768206179141998 Accuracy 0.874625027179718\n",
      "Iteration 37310 Training loss 0.03524841368198395 Validation loss 0.0477370023727417 Accuracy 0.8736250400543213\n",
      "Iteration 37320 Training loss 0.04718076437711716 Validation loss 0.047775719314813614 Accuracy 0.8757500648498535\n",
      "Iteration 37330 Training loss 0.042109787464141846 Validation loss 0.0476691871881485 Accuracy 0.874125063419342\n",
      "Iteration 37340 Training loss 0.04669184610247612 Validation loss 0.04768580570816994 Accuracy 0.8740000128746033\n",
      "Iteration 37350 Training loss 0.04223174601793289 Validation loss 0.04779938608407974 Accuracy 0.8737500309944153\n",
      "Iteration 37360 Training loss 0.045613136142492294 Validation loss 0.04769672080874443 Accuracy 0.8752500414848328\n",
      "Iteration 37370 Training loss 0.036532897502183914 Validation loss 0.047749750316143036 Accuracy 0.8766250610351562\n",
      "Iteration 37380 Training loss 0.04705062136054039 Validation loss 0.047781433910131454 Accuracy 0.8731250166893005\n",
      "Iteration 37390 Training loss 0.046175260096788406 Validation loss 0.04767458513379097 Accuracy 0.8770000338554382\n",
      "Iteration 37400 Training loss 0.03704606741666794 Validation loss 0.047812510281801224 Accuracy 0.8717500567436218\n",
      "Iteration 37410 Training loss 0.04398474469780922 Validation loss 0.04765511676669121 Accuracy 0.874250054359436\n",
      "Iteration 37420 Training loss 0.03818060830235481 Validation loss 0.04773704707622528 Accuracy 0.8757500648498535\n",
      "Iteration 37430 Training loss 0.04634629935026169 Validation loss 0.04778563976287842 Accuracy 0.8765000700950623\n",
      "Iteration 37440 Training loss 0.039690032601356506 Validation loss 0.04785001277923584 Accuracy 0.8767500519752502\n",
      "Iteration 37450 Training loss 0.05056963860988617 Validation loss 0.04766179621219635 Accuracy 0.8761250376701355\n",
      "Iteration 37460 Training loss 0.04005681350827217 Validation loss 0.04766836762428284 Accuracy 0.8753750324249268\n",
      "Iteration 37470 Training loss 0.04365156963467598 Validation loss 0.047685861587524414 Accuracy 0.8750000596046448\n",
      "Iteration 37480 Training loss 0.03970777615904808 Validation loss 0.0477815605700016 Accuracy 0.8758750557899475\n",
      "Iteration 37490 Training loss 0.04844525083899498 Validation loss 0.04773341491818428 Accuracy 0.8758750557899475\n",
      "Iteration 37500 Training loss 0.039176538586616516 Validation loss 0.04819268360733986 Accuracy 0.8763750195503235\n",
      "Iteration 37510 Training loss 0.03499271348118782 Validation loss 0.0477299690246582 Accuracy 0.8748750686645508\n",
      "Iteration 37520 Training loss 0.048569392412900925 Validation loss 0.047744762152433395 Accuracy 0.874750018119812\n",
      "Iteration 37530 Training loss 0.04067515209317207 Validation loss 0.047754984349012375 Accuracy 0.8730000257492065\n",
      "Iteration 37540 Training loss 0.04821246117353439 Validation loss 0.048061493784189224 Accuracy 0.8758750557899475\n",
      "Iteration 37550 Training loss 0.03697853907942772 Validation loss 0.047727108001708984 Accuracy 0.8755000233650208\n",
      "Iteration 37560 Training loss 0.047781363129615784 Validation loss 0.047952134162187576 Accuracy 0.8748750686645508\n",
      "Iteration 37570 Training loss 0.04293859750032425 Validation loss 0.04797149822115898 Accuracy 0.8748750686645508\n",
      "Iteration 37580 Training loss 0.05080051347613335 Validation loss 0.047921184450387955 Accuracy 0.8752500414848328\n",
      "Iteration 37590 Training loss 0.04595223814249039 Validation loss 0.04772436246275902 Accuracy 0.874250054359436\n",
      "Iteration 37600 Training loss 0.043428801000118256 Validation loss 0.04768945276737213 Accuracy 0.8756250143051147\n",
      "Iteration 37610 Training loss 0.041782692074775696 Validation loss 0.0477164164185524 Accuracy 0.8738750219345093\n",
      "Iteration 37620 Training loss 0.05196765437722206 Validation loss 0.04778607562184334 Accuracy 0.8755000233650208\n",
      "Iteration 37630 Training loss 0.037963591516017914 Validation loss 0.048065897077322006 Accuracy 0.8707500696182251\n",
      "Iteration 37640 Training loss 0.04996814578771591 Validation loss 0.04794909805059433 Accuracy 0.8712500333786011\n",
      "Iteration 37650 Training loss 0.04685395956039429 Validation loss 0.04810614138841629 Accuracy 0.8758750557899475\n",
      "Iteration 37660 Training loss 0.05296855792403221 Validation loss 0.04769448563456535 Accuracy 0.8748750686645508\n",
      "Iteration 37670 Training loss 0.04522523656487465 Validation loss 0.04766349121928215 Accuracy 0.8751250505447388\n",
      "Iteration 37680 Training loss 0.05058074742555618 Validation loss 0.047837067395448685 Accuracy 0.8730000257492065\n",
      "Iteration 37690 Training loss 0.04697096347808838 Validation loss 0.04771106317639351 Accuracy 0.8727500438690186\n",
      "Iteration 37700 Training loss 0.05275589972734451 Validation loss 0.047720301896333694 Accuracy 0.8730000257492065\n",
      "Iteration 37710 Training loss 0.04240458086133003 Validation loss 0.04766390472650528 Accuracy 0.874125063419342\n",
      "Iteration 37720 Training loss 0.04320507124066353 Validation loss 0.04764951393008232 Accuracy 0.874250054359436\n",
      "Iteration 37730 Training loss 0.03996796905994415 Validation loss 0.047656938433647156 Accuracy 0.8752500414848328\n",
      "Iteration 37740 Training loss 0.04888853430747986 Validation loss 0.04772185906767845 Accuracy 0.8725000619888306\n",
      "Iteration 37750 Training loss 0.044677067548036575 Validation loss 0.04767274856567383 Accuracy 0.874500036239624\n",
      "Iteration 37760 Training loss 0.04344712197780609 Validation loss 0.047676753252744675 Accuracy 0.8730000257492065\n",
      "Iteration 37770 Training loss 0.0461677722632885 Validation loss 0.047730639576911926 Accuracy 0.8738750219345093\n",
      "Iteration 37780 Training loss 0.039825115352869034 Validation loss 0.047722116112709045 Accuracy 0.8731250166893005\n",
      "Iteration 37790 Training loss 0.04583091661334038 Validation loss 0.04769868403673172 Accuracy 0.8738750219345093\n",
      "Iteration 37800 Training loss 0.04555448517203331 Validation loss 0.04771692305803299 Accuracy 0.874125063419342\n",
      "Iteration 37810 Training loss 0.04280926659703255 Validation loss 0.04781029745936394 Accuracy 0.8763750195503235\n",
      "Iteration 37820 Training loss 0.04091736674308777 Validation loss 0.047655001282691956 Accuracy 0.8758750557899475\n",
      "Iteration 37830 Training loss 0.03653364256024361 Validation loss 0.04804941639304161 Accuracy 0.8762500286102295\n",
      "Iteration 37840 Training loss 0.03955874219536781 Validation loss 0.04767674580216408 Accuracy 0.8762500286102295\n",
      "Iteration 37850 Training loss 0.04113054648041725 Validation loss 0.047593455761671066 Accuracy 0.8755000233650208\n",
      "Iteration 37860 Training loss 0.04113953188061714 Validation loss 0.04785744845867157 Accuracy 0.8767500519752502\n",
      "Iteration 37870 Training loss 0.04556116461753845 Validation loss 0.0478774793446064 Accuracy 0.8771250247955322\n",
      "Iteration 37880 Training loss 0.04590579867362976 Validation loss 0.047861017286777496 Accuracy 0.8721250295639038\n",
      "Iteration 37890 Training loss 0.042285773903131485 Validation loss 0.04779369756579399 Accuracy 0.8723750710487366\n",
      "Iteration 37900 Training loss 0.04534919187426567 Validation loss 0.04784252494573593 Accuracy 0.8757500648498535\n",
      "Iteration 37910 Training loss 0.052813902497291565 Validation loss 0.04755319654941559 Accuracy 0.8753750324249268\n",
      "Iteration 37920 Training loss 0.048757534474134445 Validation loss 0.047509729862213135 Accuracy 0.8750000596046448\n",
      "Iteration 37930 Training loss 0.03684240207076073 Validation loss 0.047581929713487625 Accuracy 0.874125063419342\n",
      "Iteration 37940 Training loss 0.04206821322441101 Validation loss 0.04751673713326454 Accuracy 0.8755000233650208\n",
      "Iteration 37950 Training loss 0.03770381957292557 Validation loss 0.04795512557029724 Accuracy 0.8757500648498535\n",
      "Iteration 37960 Training loss 0.04690876975655556 Validation loss 0.04832247644662857 Accuracy 0.874625027179718\n",
      "Iteration 37970 Training loss 0.04534419998526573 Validation loss 0.04787641763687134 Accuracy 0.8765000700950623\n",
      "Iteration 37980 Training loss 0.038516681641340256 Validation loss 0.04753546416759491 Accuracy 0.8757500648498535\n",
      "Iteration 37990 Training loss 0.04178696498274803 Validation loss 0.04753110185265541 Accuracy 0.8757500648498535\n",
      "Iteration 38000 Training loss 0.04251653701066971 Validation loss 0.047797758132219315 Accuracy 0.8735000491142273\n",
      "Iteration 38010 Training loss 0.046586453914642334 Validation loss 0.04754631966352463 Accuracy 0.8750000596046448\n",
      "Iteration 38020 Training loss 0.04463978484272957 Validation loss 0.04753970727324486 Accuracy 0.8757500648498535\n",
      "Iteration 38030 Training loss 0.0447208397090435 Validation loss 0.047583866864442825 Accuracy 0.8760000467300415\n",
      "Iteration 38040 Training loss 0.04316491633653641 Validation loss 0.04751713201403618 Accuracy 0.8765000700950623\n",
      "Iteration 38050 Training loss 0.04435097053647041 Validation loss 0.04778456687927246 Accuracy 0.8763750195503235\n",
      "Iteration 38060 Training loss 0.04008400812745094 Validation loss 0.04756901040673256 Accuracy 0.8738750219345093\n",
      "Iteration 38070 Training loss 0.0390881709754467 Validation loss 0.04749784618616104 Accuracy 0.874500036239624\n",
      "Iteration 38080 Training loss 0.045378707349300385 Validation loss 0.04746640846133232 Accuracy 0.8757500648498535\n",
      "Iteration 38090 Training loss 0.0525643490254879 Validation loss 0.04764704406261444 Accuracy 0.8771250247955322\n",
      "Iteration 38100 Training loss 0.03945256769657135 Validation loss 0.04747268185019493 Accuracy 0.8753750324249268\n",
      "Iteration 38110 Training loss 0.044412530958652496 Validation loss 0.047681454569101334 Accuracy 0.8732500672340393\n",
      "Iteration 38120 Training loss 0.03711109235882759 Validation loss 0.047464121133089066 Accuracy 0.8753750324249268\n",
      "Iteration 38130 Training loss 0.043022945523262024 Validation loss 0.04746543988585472 Accuracy 0.8755000233650208\n",
      "Iteration 38140 Training loss 0.04493051394820213 Validation loss 0.04746830835938454 Accuracy 0.874625027179718\n",
      "Iteration 38150 Training loss 0.04009707272052765 Validation loss 0.047635145485401154 Accuracy 0.8732500672340393\n",
      "Iteration 38160 Training loss 0.04978847876191139 Validation loss 0.04755949601531029 Accuracy 0.8748750686645508\n",
      "Iteration 38170 Training loss 0.04540649428963661 Validation loss 0.047536451369524 Accuracy 0.8757500648498535\n",
      "Iteration 38180 Training loss 0.04849576950073242 Validation loss 0.04821465164422989 Accuracy 0.8711250424385071\n",
      "Iteration 38190 Training loss 0.03710184618830681 Validation loss 0.04752142354846001 Accuracy 0.874250054359436\n",
      "Iteration 38200 Training loss 0.04673415422439575 Validation loss 0.04745187610387802 Accuracy 0.8740000128746033\n",
      "Iteration 38210 Training loss 0.041970670223236084 Validation loss 0.04750669375061989 Accuracy 0.8760000467300415\n",
      "Iteration 38220 Training loss 0.0474361926317215 Validation loss 0.04744129627943039 Accuracy 0.8760000467300415\n",
      "Iteration 38230 Training loss 0.04146556183695793 Validation loss 0.04738321900367737 Accuracy 0.8755000233650208\n",
      "Iteration 38240 Training loss 0.04688353091478348 Validation loss 0.0473523810505867 Accuracy 0.8756250143051147\n",
      "Iteration 38250 Training loss 0.04612268507480621 Validation loss 0.04747272655367851 Accuracy 0.8740000128746033\n",
      "Iteration 38260 Training loss 0.0498378612101078 Validation loss 0.04740174859762192 Accuracy 0.8765000700950623\n",
      "Iteration 38270 Training loss 0.04227558895945549 Validation loss 0.04743306338787079 Accuracy 0.8755000233650208\n",
      "Iteration 38280 Training loss 0.04308508709073067 Validation loss 0.04749229922890663 Accuracy 0.878125011920929\n",
      "Iteration 38290 Training loss 0.05078844726085663 Validation loss 0.04807785525918007 Accuracy 0.8698750138282776\n",
      "Iteration 38300 Training loss 0.04152197390794754 Validation loss 0.04741751030087471 Accuracy 0.8758750557899475\n",
      "Iteration 38310 Training loss 0.04457346349954605 Validation loss 0.04751159995794296 Accuracy 0.8768750429153442\n",
      "Iteration 38320 Training loss 0.040833283215761185 Validation loss 0.047418542206287384 Accuracy 0.8751250505447388\n",
      "Iteration 38330 Training loss 0.04140990972518921 Validation loss 0.04740674048662186 Accuracy 0.8757500648498535\n",
      "Iteration 38340 Training loss 0.0450013242661953 Validation loss 0.04758705198764801 Accuracy 0.874125063419342\n",
      "Iteration 38350 Training loss 0.042154617607593536 Validation loss 0.04755330830812454 Accuracy 0.8750000596046448\n",
      "Iteration 38360 Training loss 0.03865881264209747 Validation loss 0.04746489226818085 Accuracy 0.8751250505447388\n",
      "Iteration 38370 Training loss 0.049301885068416595 Validation loss 0.047416023910045624 Accuracy 0.8763750195503235\n",
      "Iteration 38380 Training loss 0.03770184889435768 Validation loss 0.04745103046298027 Accuracy 0.8760000467300415\n",
      "Iteration 38390 Training loss 0.04350399971008301 Validation loss 0.047796960920095444 Accuracy 0.8717500567436218\n",
      "Iteration 38400 Training loss 0.03935014456510544 Validation loss 0.04756128415465355 Accuracy 0.8735000491142273\n",
      "Iteration 38410 Training loss 0.04944156855344772 Validation loss 0.04741806164383888 Accuracy 0.8748750686645508\n",
      "Iteration 38420 Training loss 0.03638177737593651 Validation loss 0.04745710268616676 Accuracy 0.8765000700950623\n",
      "Iteration 38430 Training loss 0.0462564080953598 Validation loss 0.04738537594676018 Accuracy 0.8748750686645508\n",
      "Iteration 38440 Training loss 0.046490032225847244 Validation loss 0.048154812306165695 Accuracy 0.874250054359436\n",
      "Iteration 38450 Training loss 0.04253266379237175 Validation loss 0.04736483842134476 Accuracy 0.8761250376701355\n",
      "Iteration 38460 Training loss 0.04343916475772858 Validation loss 0.04750702530145645 Accuracy 0.8735000491142273\n",
      "Iteration 38470 Training loss 0.04265991970896721 Validation loss 0.047346919775009155 Accuracy 0.877875030040741\n",
      "Iteration 38480 Training loss 0.04317444562911987 Validation loss 0.04732125997543335 Accuracy 0.8767500519752502\n",
      "Iteration 38490 Training loss 0.04294019937515259 Validation loss 0.04734395816922188 Accuracy 0.8748750686645508\n",
      "Iteration 38500 Training loss 0.0395687110722065 Validation loss 0.04753411188721657 Accuracy 0.8735000491142273\n",
      "Iteration 38510 Training loss 0.041133277118206024 Validation loss 0.04776870086789131 Accuracy 0.8753750324249268\n",
      "Iteration 38520 Training loss 0.027296820655465126 Validation loss 0.04739793390035629 Accuracy 0.874625027179718\n",
      "Iteration 38530 Training loss 0.04532033950090408 Validation loss 0.047393932938575745 Accuracy 0.8758750557899475\n",
      "Iteration 38540 Training loss 0.040953416377305984 Validation loss 0.04744809865951538 Accuracy 0.8756250143051147\n",
      "Iteration 38550 Training loss 0.042688921093940735 Validation loss 0.047831278294324875 Accuracy 0.8713750243186951\n",
      "Iteration 38560 Training loss 0.0479663647711277 Validation loss 0.047518666833639145 Accuracy 0.874250054359436\n",
      "Iteration 38570 Training loss 0.04209528863430023 Validation loss 0.04747111722826958 Accuracy 0.874500036239624\n",
      "Iteration 38580 Training loss 0.04200439527630806 Validation loss 0.04749320447444916 Accuracy 0.8737500309944153\n",
      "Iteration 38590 Training loss 0.04674135893583298 Validation loss 0.04740729182958603 Accuracy 0.8762500286102295\n",
      "Iteration 38600 Training loss 0.03693216294050217 Validation loss 0.047481801360845566 Accuracy 0.8757500648498535\n",
      "Iteration 38610 Training loss 0.046522367745637894 Validation loss 0.048554033041000366 Accuracy 0.8737500309944153\n",
      "Iteration 38620 Training loss 0.04717704653739929 Validation loss 0.0481865368783474 Accuracy 0.8753750324249268\n",
      "Iteration 38630 Training loss 0.04018193483352661 Validation loss 0.04742985963821411 Accuracy 0.8762500286102295\n",
      "Iteration 38640 Training loss 0.041670091450214386 Validation loss 0.047391586005687714 Accuracy 0.8753750324249268\n",
      "Iteration 38650 Training loss 0.03625495731830597 Validation loss 0.04743635281920433 Accuracy 0.874625027179718\n",
      "Iteration 38660 Training loss 0.037341874092817307 Validation loss 0.04738472029566765 Accuracy 0.8760000467300415\n",
      "Iteration 38670 Training loss 0.048746515065431595 Validation loss 0.04737016558647156 Accuracy 0.8760000467300415\n",
      "Iteration 38680 Training loss 0.04916567727923393 Validation loss 0.04769011214375496 Accuracy 0.8761250376701355\n",
      "Iteration 38690 Training loss 0.046953897923231125 Validation loss 0.04762818291783333 Accuracy 0.8765000700950623\n",
      "Iteration 38700 Training loss 0.03813505172729492 Validation loss 0.04733999818563461 Accuracy 0.8757500648498535\n",
      "Iteration 38710 Training loss 0.03737189993262291 Validation loss 0.04759644344449043 Accuracy 0.8768750429153442\n",
      "Iteration 38720 Training loss 0.03979610279202461 Validation loss 0.04767946898937225 Accuracy 0.8772500157356262\n",
      "Iteration 38730 Training loss 0.042378973215818405 Validation loss 0.04790642112493515 Accuracy 0.8756250143051147\n",
      "Iteration 38740 Training loss 0.04563334956765175 Validation loss 0.047382332384586334 Accuracy 0.8752500414848328\n",
      "Iteration 38750 Training loss 0.03835509717464447 Validation loss 0.04758547618985176 Accuracy 0.874250054359436\n",
      "Iteration 38760 Training loss 0.038684677332639694 Validation loss 0.04735179990530014 Accuracy 0.8757500648498535\n",
      "Iteration 38770 Training loss 0.04592609778046608 Validation loss 0.047395579516887665 Accuracy 0.8752500414848328\n",
      "Iteration 38780 Training loss 0.04532342404127121 Validation loss 0.04737840220332146 Accuracy 0.8753750324249268\n",
      "Iteration 38790 Training loss 0.04697732999920845 Validation loss 0.04776841029524803 Accuracy 0.8756250143051147\n",
      "Iteration 38800 Training loss 0.03846634551882744 Validation loss 0.04742736369371414 Accuracy 0.874125063419342\n",
      "Iteration 38810 Training loss 0.04888516291975975 Validation loss 0.047310229390859604 Accuracy 0.8765000700950623\n",
      "Iteration 38820 Training loss 0.03801896423101425 Validation loss 0.04747011512517929 Accuracy 0.8770000338554382\n",
      "Iteration 38830 Training loss 0.04549328610301018 Validation loss 0.047335948795080185 Accuracy 0.8760000467300415\n",
      "Iteration 38840 Training loss 0.04704727977514267 Validation loss 0.04751037806272507 Accuracy 0.8738750219345093\n",
      "Iteration 38850 Training loss 0.04610048234462738 Validation loss 0.04745648428797722 Accuracy 0.8772500157356262\n",
      "Iteration 38860 Training loss 0.04382618889212608 Validation loss 0.047375619411468506 Accuracy 0.874500036239624\n",
      "Iteration 38870 Training loss 0.03868973255157471 Validation loss 0.04746615141630173 Accuracy 0.8733750581741333\n",
      "Iteration 38880 Training loss 0.05248153954744339 Validation loss 0.047460611909627914 Accuracy 0.8758750557899475\n",
      "Iteration 38890 Training loss 0.044900137931108475 Validation loss 0.047323424369096756 Accuracy 0.8755000233650208\n",
      "Iteration 38900 Training loss 0.04510298743844032 Validation loss 0.047368135303258896 Accuracy 0.8753750324249268\n",
      "Iteration 38910 Training loss 0.04002321511507034 Validation loss 0.047374892979860306 Accuracy 0.8753750324249268\n",
      "Iteration 38920 Training loss 0.042957477271556854 Validation loss 0.04773001745343208 Accuracy 0.8765000700950623\n",
      "Iteration 38930 Training loss 0.041494958102703094 Validation loss 0.04730115085840225 Accuracy 0.8748750686645508\n",
      "Iteration 38940 Training loss 0.03761223331093788 Validation loss 0.047391556203365326 Accuracy 0.8757500648498535\n",
      "Iteration 38950 Training loss 0.041957926005125046 Validation loss 0.047336146235466 Accuracy 0.8756250143051147\n",
      "Iteration 38960 Training loss 0.04314853996038437 Validation loss 0.047391701489686966 Accuracy 0.8753750324249268\n",
      "Iteration 38970 Training loss 0.04445264860987663 Validation loss 0.047333572059869766 Accuracy 0.8740000128746033\n",
      "Iteration 38980 Training loss 0.0444338321685791 Validation loss 0.04729372635483742 Accuracy 0.8761250376701355\n",
      "Iteration 38990 Training loss 0.05107685923576355 Validation loss 0.047354958951473236 Accuracy 0.8756250143051147\n",
      "Iteration 39000 Training loss 0.044507306069135666 Validation loss 0.04730620235204697 Accuracy 0.8751250505447388\n",
      "Iteration 39010 Training loss 0.0442221574485302 Validation loss 0.047815170139074326 Accuracy 0.8711250424385071\n",
      "Iteration 39020 Training loss 0.041691310703754425 Validation loss 0.04742686450481415 Accuracy 0.87437504529953\n",
      "Iteration 39030 Training loss 0.045927371829748154 Validation loss 0.04749332740902901 Accuracy 0.8768750429153442\n",
      "Iteration 39040 Training loss 0.041852276772260666 Validation loss 0.04733327776193619 Accuracy 0.8757500648498535\n",
      "Iteration 39050 Training loss 0.05176340416073799 Validation loss 0.047265443950891495 Accuracy 0.8751250505447388\n",
      "Iteration 39060 Training loss 0.04177853465080261 Validation loss 0.047330088913440704 Accuracy 0.8761250376701355\n",
      "Iteration 39070 Training loss 0.043493397533893585 Validation loss 0.047278665006160736 Accuracy 0.8756250143051147\n",
      "Iteration 39080 Training loss 0.04254193231463432 Validation loss 0.0473184660077095 Accuracy 0.8748750686645508\n",
      "Iteration 39090 Training loss 0.044274814426898956 Validation loss 0.04729282483458519 Accuracy 0.8760000467300415\n",
      "Iteration 39100 Training loss 0.041752465069293976 Validation loss 0.04733729735016823 Accuracy 0.8752500414848328\n",
      "Iteration 39110 Training loss 0.03677409142255783 Validation loss 0.04752035811543465 Accuracy 0.8763750195503235\n",
      "Iteration 39120 Training loss 0.04933174327015877 Validation loss 0.04725213348865509 Accuracy 0.8752500414848328\n",
      "Iteration 39130 Training loss 0.04446813091635704 Validation loss 0.0473676398396492 Accuracy 0.8751250505447388\n",
      "Iteration 39140 Training loss 0.042966216802597046 Validation loss 0.0479300431907177 Accuracy 0.8748750686645508\n",
      "Iteration 39150 Training loss 0.04357270523905754 Validation loss 0.04768846556544304 Accuracy 0.8723750710487366\n",
      "Iteration 39160 Training loss 0.041037850081920624 Validation loss 0.04724503681063652 Accuracy 0.8760000467300415\n",
      "Iteration 39170 Training loss 0.04597261920571327 Validation loss 0.04738626256585121 Accuracy 0.8770000338554382\n",
      "Iteration 39180 Training loss 0.03689657896757126 Validation loss 0.047291070222854614 Accuracy 0.877625048160553\n",
      "Iteration 39190 Training loss 0.03815080597996712 Validation loss 0.047557588666677475 Accuracy 0.8760000467300415\n",
      "Iteration 39200 Training loss 0.0427689254283905 Validation loss 0.047213755548000336 Accuracy 0.874250054359436\n",
      "Iteration 39210 Training loss 0.041265662759542465 Validation loss 0.047672729939222336 Accuracy 0.8763750195503235\n",
      "Iteration 39220 Training loss 0.04174747318029404 Validation loss 0.04746421426534653 Accuracy 0.8730000257492065\n",
      "Iteration 39230 Training loss 0.04108728468418121 Validation loss 0.04725221171975136 Accuracy 0.8738750219345093\n",
      "Iteration 39240 Training loss 0.03869267553091049 Validation loss 0.04732483997941017 Accuracy 0.8768750429153442\n",
      "Iteration 39250 Training loss 0.04024519771337509 Validation loss 0.04715694859623909 Accuracy 0.8771250247955322\n",
      "Iteration 39260 Training loss 0.03756439685821533 Validation loss 0.047318655997514725 Accuracy 0.87437504529953\n",
      "Iteration 39270 Training loss 0.04952072352170944 Validation loss 0.04750913009047508 Accuracy 0.8740000128746033\n",
      "Iteration 39280 Training loss 0.037002500146627426 Validation loss 0.04717564582824707 Accuracy 0.8752500414848328\n",
      "Iteration 39290 Training loss 0.03963695839047432 Validation loss 0.04766707867383957 Accuracy 0.8725000619888306\n",
      "Iteration 39300 Training loss 0.04260651394724846 Validation loss 0.04806945100426674 Accuracy 0.8707500696182251\n",
      "Iteration 39310 Training loss 0.04411191865801811 Validation loss 0.04749758541584015 Accuracy 0.8758750557899475\n",
      "Iteration 39320 Training loss 0.04184465482831001 Validation loss 0.04727064073085785 Accuracy 0.8765000700950623\n",
      "Iteration 39330 Training loss 0.05726027861237526 Validation loss 0.047229379415512085 Accuracy 0.8758750557899475\n",
      "Iteration 39340 Training loss 0.04492154344916344 Validation loss 0.04753679037094116 Accuracy 0.8761250376701355\n",
      "Iteration 39350 Training loss 0.03689524903893471 Validation loss 0.04726305603981018 Accuracy 0.8758750557899475\n",
      "Iteration 39360 Training loss 0.043159570544958115 Validation loss 0.04727058857679367 Accuracy 0.8763750195503235\n",
      "Iteration 39370 Training loss 0.04693486541509628 Validation loss 0.047345101833343506 Accuracy 0.8758750557899475\n",
      "Iteration 39380 Training loss 0.04302534461021423 Validation loss 0.04755420237779617 Accuracy 0.8755000233650208\n",
      "Iteration 39390 Training loss 0.046591050922870636 Validation loss 0.047193657606840134 Accuracy 0.8762500286102295\n",
      "Iteration 39400 Training loss 0.034531887620687485 Validation loss 0.04756768047809601 Accuracy 0.8765000700950623\n",
      "Iteration 39410 Training loss 0.04561540484428406 Validation loss 0.0472569465637207 Accuracy 0.8760000467300415\n",
      "Iteration 39420 Training loss 0.04314766824245453 Validation loss 0.047555599361658096 Accuracy 0.8770000338554382\n",
      "Iteration 39430 Training loss 0.040544040501117706 Validation loss 0.04725397005677223 Accuracy 0.874625027179718\n",
      "Iteration 39440 Training loss 0.04241640865802765 Validation loss 0.04737083986401558 Accuracy 0.8738750219345093\n",
      "Iteration 39450 Training loss 0.042436279356479645 Validation loss 0.04759618639945984 Accuracy 0.8756250143051147\n",
      "Iteration 39460 Training loss 0.03962172567844391 Validation loss 0.04735476151108742 Accuracy 0.874500036239624\n",
      "Iteration 39470 Training loss 0.03636068105697632 Validation loss 0.04763098806142807 Accuracy 0.8722500205039978\n",
      "Iteration 39480 Training loss 0.04743752256035805 Validation loss 0.047376323491334915 Accuracy 0.8762500286102295\n",
      "Iteration 39490 Training loss 0.04693274572491646 Validation loss 0.047255244106054306 Accuracy 0.8748750686645508\n",
      "Iteration 39500 Training loss 0.041870005428791046 Validation loss 0.04726159945130348 Accuracy 0.878125011920929\n",
      "Iteration 39510 Training loss 0.040433380752801895 Validation loss 0.04728111997246742 Accuracy 0.874500036239624\n",
      "Iteration 39520 Training loss 0.03653347119688988 Validation loss 0.047270677983760834 Accuracy 0.877750039100647\n",
      "Iteration 39530 Training loss 0.045810602605342865 Validation loss 0.04805813357234001 Accuracy 0.874250054359436\n",
      "Iteration 39540 Training loss 0.051669079810380936 Validation loss 0.04759141802787781 Accuracy 0.8735000491142273\n",
      "Iteration 39550 Training loss 0.03974241763353348 Validation loss 0.04728736728429794 Accuracy 0.877375066280365\n",
      "Iteration 39560 Training loss 0.039662089198827744 Validation loss 0.04729054495692253 Accuracy 0.877625048160553\n",
      "Iteration 39570 Training loss 0.04132131487131119 Validation loss 0.04750499129295349 Accuracy 0.877750039100647\n",
      "Iteration 39580 Training loss 0.04415152966976166 Validation loss 0.04801472648978233 Accuracy 0.8707500696182251\n",
      "Iteration 39590 Training loss 0.045516613870859146 Validation loss 0.047860536724328995 Accuracy 0.8757500648498535\n",
      "Iteration 39600 Training loss 0.04342033341526985 Validation loss 0.047187838703393936 Accuracy 0.8762500286102295\n",
      "Iteration 39610 Training loss 0.04190867021679878 Validation loss 0.0471983402967453 Accuracy 0.8770000338554382\n",
      "Iteration 39620 Training loss 0.04642757400870323 Validation loss 0.047203026711940765 Accuracy 0.8763750195503235\n",
      "Iteration 39630 Training loss 0.039928682148456573 Validation loss 0.04728611931204796 Accuracy 0.8765000700950623\n",
      "Iteration 39640 Training loss 0.04102756083011627 Validation loss 0.047099411487579346 Accuracy 0.877500057220459\n",
      "Iteration 39650 Training loss 0.03753454238176346 Validation loss 0.04770423844456673 Accuracy 0.8757500648498535\n",
      "Iteration 39660 Training loss 0.040924519300460815 Validation loss 0.04716802015900612 Accuracy 0.8765000700950623\n",
      "Iteration 39670 Training loss 0.03902069106698036 Validation loss 0.04711555317044258 Accuracy 0.8761250376701355\n",
      "Iteration 39680 Training loss 0.045570969581604004 Validation loss 0.047166720032691956 Accuracy 0.8768750429153442\n",
      "Iteration 39690 Training loss 0.04787883162498474 Validation loss 0.04710659384727478 Accuracy 0.8772500157356262\n",
      "Iteration 39700 Training loss 0.04157336801290512 Validation loss 0.047908201813697815 Accuracy 0.8712500333786011\n",
      "Iteration 39710 Training loss 0.04254596680402756 Validation loss 0.04707745835185051 Accuracy 0.8766250610351562\n",
      "Iteration 39720 Training loss 0.043997038155794144 Validation loss 0.0471770353615284 Accuracy 0.8762500286102295\n",
      "Iteration 39730 Training loss 0.04393540322780609 Validation loss 0.04732630401849747 Accuracy 0.8740000128746033\n",
      "Iteration 39740 Training loss 0.041796669363975525 Validation loss 0.04705347493290901 Accuracy 0.8756250143051147\n",
      "Iteration 39750 Training loss 0.04780927672982216 Validation loss 0.04717577248811722 Accuracy 0.8762500286102295\n",
      "Iteration 39760 Training loss 0.041694268584251404 Validation loss 0.047047801315784454 Accuracy 0.8768750429153442\n",
      "Iteration 39770 Training loss 0.0421457476913929 Validation loss 0.04702780395746231 Accuracy 0.8762500286102295\n",
      "Iteration 39780 Training loss 0.03841651231050491 Validation loss 0.047046251595020294 Accuracy 0.8760000467300415\n",
      "Iteration 39790 Training loss 0.04025131091475487 Validation loss 0.04718811437487602 Accuracy 0.8765000700950623\n",
      "Iteration 39800 Training loss 0.03782213479280472 Validation loss 0.04701252654194832 Accuracy 0.8772500157356262\n",
      "Iteration 39810 Training loss 0.052154283970594406 Validation loss 0.047362037003040314 Accuracy 0.8770000338554382\n",
      "Iteration 39820 Training loss 0.04544495791196823 Validation loss 0.04705327749252319 Accuracy 0.8762500286102295\n",
      "Iteration 39830 Training loss 0.039009712636470795 Validation loss 0.04725617542862892 Accuracy 0.8765000700950623\n",
      "Iteration 39840 Training loss 0.041341960430145264 Validation loss 0.04706161096692085 Accuracy 0.8771250247955322\n",
      "Iteration 39850 Training loss 0.038764167577028275 Validation loss 0.04803475737571716 Accuracy 0.87437504529953\n",
      "Iteration 39860 Training loss 0.039728231728076935 Validation loss 0.04704941064119339 Accuracy 0.8755000233650208\n",
      "Iteration 39870 Training loss 0.045592937618494034 Validation loss 0.047007966786623 Accuracy 0.8756250143051147\n",
      "Iteration 39880 Training loss 0.04114792123436928 Validation loss 0.04704718664288521 Accuracy 0.874125063419342\n",
      "Iteration 39890 Training loss 0.03830597177147865 Validation loss 0.047201789915561676 Accuracy 0.8762500286102295\n",
      "Iteration 39900 Training loss 0.049951568245887756 Validation loss 0.048353683203458786 Accuracy 0.8688750267028809\n",
      "Iteration 39910 Training loss 0.045290060341358185 Validation loss 0.04727144539356232 Accuracy 0.8736250400543213\n",
      "Iteration 39920 Training loss 0.0420033223927021 Validation loss 0.047184642404317856 Accuracy 0.8737500309944153\n",
      "Iteration 39930 Training loss 0.04039433225989342 Validation loss 0.04700639098882675 Accuracy 0.8756250143051147\n",
      "Iteration 39940 Training loss 0.0411858931183815 Validation loss 0.04802361875772476 Accuracy 0.874125063419342\n",
      "Iteration 39950 Training loss 0.04316221922636032 Validation loss 0.047248583287000656 Accuracy 0.87437504529953\n",
      "Iteration 39960 Training loss 0.05332232266664505 Validation loss 0.04698667675256729 Accuracy 0.8760000467300415\n",
      "Iteration 39970 Training loss 0.0463155061006546 Validation loss 0.04698958992958069 Accuracy 0.877875030040741\n",
      "Iteration 39980 Training loss 0.03822150453925133 Validation loss 0.04697521775960922 Accuracy 0.877750039100647\n",
      "Iteration 39990 Training loss 0.038025952875614166 Validation loss 0.04695839807391167 Accuracy 0.8772500157356262\n",
      "Iteration 40000 Training loss 0.04539550095796585 Validation loss 0.0470607690513134 Accuracy 0.878125011920929\n",
      "Iteration 40010 Training loss 0.04766395315527916 Validation loss 0.046935904771089554 Accuracy 0.8772500157356262\n",
      "Iteration 40020 Training loss 0.04468638449907303 Validation loss 0.047079045325517654 Accuracy 0.8762500286102295\n",
      "Iteration 40030 Training loss 0.04518403112888336 Validation loss 0.04706651344895363 Accuracy 0.8762500286102295\n",
      "Iteration 40040 Training loss 0.04188302904367447 Validation loss 0.047345805913209915 Accuracy 0.8728750348091125\n",
      "Iteration 40050 Training loss 0.03624729812145233 Validation loss 0.04701899364590645 Accuracy 0.8751250505447388\n",
      "Iteration 40060 Training loss 0.04823892191052437 Validation loss 0.04774124547839165 Accuracy 0.8760000467300415\n",
      "Iteration 40070 Training loss 0.037770770490169525 Validation loss 0.046940822154283524 Accuracy 0.8767500519752502\n",
      "Iteration 40080 Training loss 0.04417344555258751 Validation loss 0.046938683837652206 Accuracy 0.8756250143051147\n",
      "Iteration 40090 Training loss 0.052151069045066833 Validation loss 0.047088541090488434 Accuracy 0.8757500648498535\n",
      "Iteration 40100 Training loss 0.046106968075037 Validation loss 0.04697731137275696 Accuracy 0.8760000467300415\n",
      "Iteration 40110 Training loss 0.04239000380039215 Validation loss 0.046984970569610596 Accuracy 0.877750039100647\n",
      "Iteration 40120 Training loss 0.04717318341135979 Validation loss 0.047013841569423676 Accuracy 0.8766250610351562\n",
      "Iteration 40130 Training loss 0.04088042303919792 Validation loss 0.04832702502608299 Accuracy 0.8685000538825989\n",
      "Iteration 40140 Training loss 0.05096138268709183 Validation loss 0.046994853764772415 Accuracy 0.8763750195503235\n",
      "Iteration 40150 Training loss 0.038565635681152344 Validation loss 0.04703393951058388 Accuracy 0.8768750429153442\n",
      "Iteration 40160 Training loss 0.04376503452658653 Validation loss 0.04740118980407715 Accuracy 0.8772500157356262\n",
      "Iteration 40170 Training loss 0.043636396527290344 Validation loss 0.04708375036716461 Accuracy 0.877500057220459\n",
      "Iteration 40180 Training loss 0.04224953427910805 Validation loss 0.04699259251356125 Accuracy 0.8765000700950623\n",
      "Iteration 40190 Training loss 0.043050508946180344 Validation loss 0.04713861271739006 Accuracy 0.8750000596046448\n",
      "Iteration 40200 Training loss 0.04259902983903885 Validation loss 0.046986937522888184 Accuracy 0.8768750429153442\n",
      "Iteration 40210 Training loss 0.038906827569007874 Validation loss 0.046939052641391754 Accuracy 0.8765000700950623\n",
      "Iteration 40220 Training loss 0.04029424116015434 Validation loss 0.04697871580719948 Accuracy 0.8760000467300415\n",
      "Iteration 40230 Training loss 0.03743651509284973 Validation loss 0.04692431166768074 Accuracy 0.8762500286102295\n",
      "Iteration 40240 Training loss 0.03652257099747658 Validation loss 0.046957939863204956 Accuracy 0.8761250376701355\n",
      "Iteration 40250 Training loss 0.04691895470023155 Validation loss 0.04699673131108284 Accuracy 0.8758750557899475\n",
      "Iteration 40260 Training loss 0.03645511716604233 Validation loss 0.04694391041994095 Accuracy 0.8756250143051147\n",
      "Iteration 40270 Training loss 0.03660096973180771 Validation loss 0.04744182154536247 Accuracy 0.8765000700950623\n",
      "Iteration 40280 Training loss 0.04045726731419563 Validation loss 0.04713309556245804 Accuracy 0.8751250505447388\n",
      "Iteration 40290 Training loss 0.04759825021028519 Validation loss 0.0481710322201252 Accuracy 0.8731250166893005\n",
      "Iteration 40300 Training loss 0.04637691751122475 Validation loss 0.04739190265536308 Accuracy 0.8736250400543213\n",
      "Iteration 40310 Training loss 0.04649882763624191 Validation loss 0.04710019379854202 Accuracy 0.877375066280365\n",
      "Iteration 40320 Training loss 0.04531047120690346 Validation loss 0.04698437079787254 Accuracy 0.8771250247955322\n",
      "Iteration 40330 Training loss 0.04930600896477699 Validation loss 0.047826554626226425 Accuracy 0.8711250424385071\n",
      "Iteration 40340 Training loss 0.04219019412994385 Validation loss 0.046969544142484665 Accuracy 0.8768750429153442\n",
      "Iteration 40350 Training loss 0.040815237909555435 Validation loss 0.04697326198220253 Accuracy 0.8758750557899475\n",
      "Iteration 40360 Training loss 0.04270397871732712 Validation loss 0.04697741940617561 Accuracy 0.8760000467300415\n",
      "Iteration 40370 Training loss 0.051076795905828476 Validation loss 0.0469825305044651 Accuracy 0.8761250376701355\n",
      "Iteration 40380 Training loss 0.04634576663374901 Validation loss 0.0469227209687233 Accuracy 0.8760000467300415\n",
      "Iteration 40390 Training loss 0.0435919463634491 Validation loss 0.047173768281936646 Accuracy 0.877500057220459\n",
      "Iteration 40400 Training loss 0.0384407564997673 Validation loss 0.0470007061958313 Accuracy 0.877375066280365\n",
      "Iteration 40410 Training loss 0.04299160838127136 Validation loss 0.04697958752512932 Accuracy 0.8765000700950623\n",
      "Iteration 40420 Training loss 0.039966095238924026 Validation loss 0.04691798612475395 Accuracy 0.8758750557899475\n",
      "Iteration 40430 Training loss 0.051546573638916016 Validation loss 0.04741066321730614 Accuracy 0.8767500519752502\n",
      "Iteration 40440 Training loss 0.039844922721385956 Validation loss 0.046934425830841064 Accuracy 0.8760000467300415\n",
      "Iteration 40450 Training loss 0.04462622106075287 Validation loss 0.04694626107811928 Accuracy 0.8765000700950623\n",
      "Iteration 40460 Training loss 0.04169687256217003 Validation loss 0.04696207493543625 Accuracy 0.8765000700950623\n",
      "Iteration 40470 Training loss 0.05043565481901169 Validation loss 0.04693274199962616 Accuracy 0.8762500286102295\n",
      "Iteration 40480 Training loss 0.04693808779120445 Validation loss 0.046917401254177094 Accuracy 0.8762500286102295\n",
      "Iteration 40490 Training loss 0.03368678316473961 Validation loss 0.04746273159980774 Accuracy 0.8763750195503235\n",
      "Iteration 40500 Training loss 0.04581528156995773 Validation loss 0.047240063548088074 Accuracy 0.8732500672340393\n",
      "Iteration 40510 Training loss 0.05681554228067398 Validation loss 0.047112978994846344 Accuracy 0.8758750557899475\n",
      "Iteration 40520 Training loss 0.04364996775984764 Validation loss 0.046924982219934464 Accuracy 0.8766250610351562\n",
      "Iteration 40530 Training loss 0.04349781945347786 Validation loss 0.04689701274037361 Accuracy 0.8771250247955322\n",
      "Iteration 40540 Training loss 0.04649600014090538 Validation loss 0.04705851525068283 Accuracy 0.877500057220459\n",
      "Iteration 40550 Training loss 0.041491370648145676 Validation loss 0.04714887589216232 Accuracy 0.874625027179718\n",
      "Iteration 40560 Training loss 0.03543645888566971 Validation loss 0.046975672245025635 Accuracy 0.877500057220459\n",
      "Iteration 40570 Training loss 0.04299319162964821 Validation loss 0.04825938120484352 Accuracy 0.8728750348091125\n",
      "Iteration 40580 Training loss 0.04590805992484093 Validation loss 0.046926744282245636 Accuracy 0.877875030040741\n",
      "Iteration 40590 Training loss 0.041273653507232666 Validation loss 0.04691322520375252 Accuracy 0.8767500519752502\n",
      "Iteration 40600 Training loss 0.04272341728210449 Validation loss 0.04716448485851288 Accuracy 0.877875030040741\n",
      "Iteration 40610 Training loss 0.039856962859630585 Validation loss 0.04695454612374306 Accuracy 0.8755000233650208\n",
      "Iteration 40620 Training loss 0.03971691429615021 Validation loss 0.04690568521618843 Accuracy 0.8762500286102295\n",
      "Iteration 40630 Training loss 0.04276387020945549 Validation loss 0.04688045009970665 Accuracy 0.8772500157356262\n",
      "Iteration 40640 Training loss 0.04179265722632408 Validation loss 0.04687400907278061 Accuracy 0.8772500157356262\n",
      "Iteration 40650 Training loss 0.04193601384758949 Validation loss 0.046876732259988785 Accuracy 0.878125011920929\n",
      "Iteration 40660 Training loss 0.04102964326739311 Validation loss 0.04699542373418808 Accuracy 0.8763750195503235\n",
      "Iteration 40670 Training loss 0.03396012634038925 Validation loss 0.04690602049231529 Accuracy 0.877625048160553\n",
      "Iteration 40680 Training loss 0.04159180447459221 Validation loss 0.047054536640644073 Accuracy 0.8782500624656677\n",
      "Iteration 40690 Training loss 0.04467124864459038 Validation loss 0.04694412276148796 Accuracy 0.8788750171661377\n",
      "Iteration 40700 Training loss 0.05057873949408531 Validation loss 0.046940527856349945 Accuracy 0.8768750429153442\n",
      "Iteration 40710 Training loss 0.042595818638801575 Validation loss 0.04748833552002907 Accuracy 0.8733750581741333\n",
      "Iteration 40720 Training loss 0.03472956269979477 Validation loss 0.04710086062550545 Accuracy 0.8786250352859497\n",
      "Iteration 40730 Training loss 0.03655650466680527 Validation loss 0.046927232295274734 Accuracy 0.8767500519752502\n",
      "Iteration 40740 Training loss 0.04535144567489624 Validation loss 0.04692689701914787 Accuracy 0.877625048160553\n",
      "Iteration 40750 Training loss 0.041773781180381775 Validation loss 0.046897053718566895 Accuracy 0.8766250610351562\n",
      "Iteration 40760 Training loss 0.04157339781522751 Validation loss 0.04706763103604317 Accuracy 0.877375066280365\n",
      "Iteration 40770 Training loss 0.038517165929079056 Validation loss 0.04693066328763962 Accuracy 0.8762500286102295\n",
      "Iteration 40780 Training loss 0.04652153328061104 Validation loss 0.046877432614564896 Accuracy 0.8768750429153442\n",
      "Iteration 40790 Training loss 0.04427637159824371 Validation loss 0.047008875757455826 Accuracy 0.8787500262260437\n",
      "Iteration 40800 Training loss 0.04986787214875221 Validation loss 0.0468982569873333 Accuracy 0.8771250247955322\n",
      "Iteration 40810 Training loss 0.04421404376626015 Validation loss 0.04725975915789604 Accuracy 0.8768750429153442\n",
      "Iteration 40820 Training loss 0.04428533464670181 Validation loss 0.04770646244287491 Accuracy 0.8752500414848328\n",
      "Iteration 40830 Training loss 0.04193919897079468 Validation loss 0.04850464314222336 Accuracy 0.8703750371932983\n",
      "Iteration 40840 Training loss 0.04311827942728996 Validation loss 0.04691930115222931 Accuracy 0.878125011920929\n",
      "Iteration 40850 Training loss 0.03735140711069107 Validation loss 0.04682708904147148 Accuracy 0.8783750534057617\n",
      "Iteration 40860 Training loss 0.046402521431446075 Validation loss 0.04682019352912903 Accuracy 0.8782500624656677\n",
      "Iteration 40870 Training loss 0.04098769277334213 Validation loss 0.046914976090192795 Accuracy 0.8767500519752502\n",
      "Iteration 40880 Training loss 0.042940668761730194 Validation loss 0.04720963537693024 Accuracy 0.874250054359436\n",
      "Iteration 40890 Training loss 0.043803948909044266 Validation loss 0.046953216195106506 Accuracy 0.877750039100647\n",
      "Iteration 40900 Training loss 0.04318591207265854 Validation loss 0.0477716438472271 Accuracy 0.8758750557899475\n",
      "Iteration 40910 Training loss 0.045894868671894073 Validation loss 0.0467904694378376 Accuracy 0.877500057220459\n",
      "Iteration 40920 Training loss 0.03843450918793678 Validation loss 0.04693117365241051 Accuracy 0.8785000443458557\n",
      "Iteration 40930 Training loss 0.04455462470650673 Validation loss 0.04675428196787834 Accuracy 0.878000020980835\n",
      "Iteration 40940 Training loss 0.03901052102446556 Validation loss 0.046740952879190445 Accuracy 0.8772500157356262\n",
      "Iteration 40950 Training loss 0.047060687094926834 Validation loss 0.048619769513607025 Accuracy 0.8706250190734863\n",
      "Iteration 40960 Training loss 0.04116593301296234 Validation loss 0.04681849479675293 Accuracy 0.877875030040741\n",
      "Iteration 40970 Training loss 0.04331362247467041 Validation loss 0.04681381210684776 Accuracy 0.877625048160553\n",
      "Iteration 40980 Training loss 0.037700504064559937 Validation loss 0.047346506267786026 Accuracy 0.874250054359436\n",
      "Iteration 40990 Training loss 0.04634712263941765 Validation loss 0.046820979565382004 Accuracy 0.878000020980835\n",
      "Iteration 41000 Training loss 0.0375664159655571 Validation loss 0.04688810184597969 Accuracy 0.8786250352859497\n",
      "Iteration 41010 Training loss 0.054459162056446075 Validation loss 0.04678720235824585 Accuracy 0.8790000677108765\n",
      "Iteration 41020 Training loss 0.04458102583885193 Validation loss 0.04692428559064865 Accuracy 0.8755000233650208\n",
      "Iteration 41030 Training loss 0.04376881569623947 Validation loss 0.04694009944796562 Accuracy 0.8783750534057617\n",
      "Iteration 41040 Training loss 0.041712190955877304 Validation loss 0.046768367290496826 Accuracy 0.877500057220459\n",
      "Iteration 41050 Training loss 0.04159869998693466 Validation loss 0.046848539263010025 Accuracy 0.877500057220459\n",
      "Iteration 41060 Training loss 0.04960217326879501 Validation loss 0.04678205028176308 Accuracy 0.8770000338554382\n",
      "Iteration 41070 Training loss 0.04265567660331726 Validation loss 0.04684208706021309 Accuracy 0.877875030040741\n",
      "Iteration 41080 Training loss 0.04278038442134857 Validation loss 0.046759456396102905 Accuracy 0.878125011920929\n",
      "Iteration 41090 Training loss 0.0366983599960804 Validation loss 0.04685255140066147 Accuracy 0.8787500262260437\n",
      "Iteration 41100 Training loss 0.04164549708366394 Validation loss 0.0469360388815403 Accuracy 0.8771250247955322\n",
      "Iteration 41110 Training loss 0.04113046079874039 Validation loss 0.04754406958818436 Accuracy 0.8726250529289246\n",
      "Iteration 41120 Training loss 0.03226489573717117 Validation loss 0.04729144647717476 Accuracy 0.874625027179718\n",
      "Iteration 41130 Training loss 0.04983179643750191 Validation loss 0.04684378206729889 Accuracy 0.8767500519752502\n",
      "Iteration 41140 Training loss 0.037818919867277145 Validation loss 0.04674097150564194 Accuracy 0.8787500262260437\n",
      "Iteration 41150 Training loss 0.042802099138498306 Validation loss 0.04742977395653725 Accuracy 0.8735000491142273\n",
      "Iteration 41160 Training loss 0.0452071838080883 Validation loss 0.04705677926540375 Accuracy 0.874500036239624\n",
      "Iteration 41170 Training loss 0.03949807956814766 Validation loss 0.04677872732281685 Accuracy 0.8768750429153442\n",
      "Iteration 41180 Training loss 0.042486608028411865 Validation loss 0.046858806163072586 Accuracy 0.8770000338554382\n",
      "Iteration 41190 Training loss 0.04033937305212021 Validation loss 0.046719394624233246 Accuracy 0.8782500624656677\n",
      "Iteration 41200 Training loss 0.039670638740062714 Validation loss 0.047044090926647186 Accuracy 0.8768750429153442\n",
      "Iteration 41210 Training loss 0.04494413733482361 Validation loss 0.046724844723939896 Accuracy 0.877750039100647\n",
      "Iteration 41220 Training loss 0.03289675712585449 Validation loss 0.04673673212528229 Accuracy 0.8771250247955322\n",
      "Iteration 41230 Training loss 0.038603831082582474 Validation loss 0.04676812142133713 Accuracy 0.877625048160553\n",
      "Iteration 41240 Training loss 0.04327801614999771 Validation loss 0.046876490116119385 Accuracy 0.8782500624656677\n",
      "Iteration 41250 Training loss 0.04569718614220619 Validation loss 0.046832673251628876 Accuracy 0.8771250247955322\n",
      "Iteration 41260 Training loss 0.03898429125547409 Validation loss 0.04692970588803291 Accuracy 0.8765000700950623\n",
      "Iteration 41270 Training loss 0.042184896767139435 Validation loss 0.04672667011618614 Accuracy 0.878125011920929\n",
      "Iteration 41280 Training loss 0.036594703793525696 Validation loss 0.0467383973300457 Accuracy 0.878125011920929\n",
      "Iteration 41290 Training loss 0.04451339691877365 Validation loss 0.046855758875608444 Accuracy 0.877750039100647\n",
      "Iteration 41300 Training loss 0.039971914142370224 Validation loss 0.04688210412859917 Accuracy 0.8757500648498535\n",
      "Iteration 41310 Training loss 0.04166889190673828 Validation loss 0.047442734241485596 Accuracy 0.8723750710487366\n",
      "Iteration 41320 Training loss 0.04223398491740227 Validation loss 0.04705765098333359 Accuracy 0.8771250247955322\n",
      "Iteration 41330 Training loss 0.041905730962753296 Validation loss 0.046804167330265045 Accuracy 0.8768750429153442\n",
      "Iteration 41340 Training loss 0.04735102877020836 Validation loss 0.04696744307875633 Accuracy 0.8763750195503235\n",
      "Iteration 41350 Training loss 0.0392770878970623 Validation loss 0.04725146293640137 Accuracy 0.8767500519752502\n",
      "Iteration 41360 Training loss 0.04548131674528122 Validation loss 0.04711133614182472 Accuracy 0.8770000338554382\n",
      "Iteration 41370 Training loss 0.04577012360095978 Validation loss 0.047100234776735306 Accuracy 0.8750000596046448\n",
      "Iteration 41380 Training loss 0.041142839938402176 Validation loss 0.04664216190576553 Accuracy 0.8783750534057617\n",
      "Iteration 41390 Training loss 0.04499812796711922 Validation loss 0.046679604798555374 Accuracy 0.8786250352859497\n",
      "Iteration 41400 Training loss 0.05316166579723358 Validation loss 0.046693965792655945 Accuracy 0.8788750171661377\n",
      "Iteration 41410 Training loss 0.05024697631597519 Validation loss 0.04667118191719055 Accuracy 0.878000020980835\n",
      "Iteration 41420 Training loss 0.04263676702976227 Validation loss 0.046697281301021576 Accuracy 0.8771250247955322\n",
      "Iteration 41430 Training loss 0.04388487711548805 Validation loss 0.0471169538795948 Accuracy 0.8767500519752502\n",
      "Iteration 41440 Training loss 0.04420770704746246 Validation loss 0.046730026602745056 Accuracy 0.8782500624656677\n",
      "Iteration 41450 Training loss 0.04291342571377754 Validation loss 0.047202449291944504 Accuracy 0.877500057220459\n",
      "Iteration 41460 Training loss 0.03937229514122009 Validation loss 0.04671298339962959 Accuracy 0.878125011920929\n",
      "Iteration 41470 Training loss 0.04194449260830879 Validation loss 0.04665958508849144 Accuracy 0.878125011920929\n",
      "Iteration 41480 Training loss 0.04151111841201782 Validation loss 0.047456782311201096 Accuracy 0.8750000596046448\n",
      "Iteration 41490 Training loss 0.03746086359024048 Validation loss 0.046739306300878525 Accuracy 0.877625048160553\n",
      "Iteration 41500 Training loss 0.035189468413591385 Validation loss 0.046755243092775345 Accuracy 0.8768750429153442\n",
      "Iteration 41510 Training loss 0.038995396345853806 Validation loss 0.04673057049512863 Accuracy 0.8767500519752502\n",
      "Iteration 41520 Training loss 0.04203370586037636 Validation loss 0.047483012080192566 Accuracy 0.8728750348091125\n",
      "Iteration 41530 Training loss 0.03834587335586548 Validation loss 0.04666108265519142 Accuracy 0.8786250352859497\n",
      "Iteration 41540 Training loss 0.0449204258620739 Validation loss 0.04680660739541054 Accuracy 0.877500057220459\n",
      "Iteration 41550 Training loss 0.048134759068489075 Validation loss 0.0470455102622509 Accuracy 0.877500057220459\n",
      "Iteration 41560 Training loss 0.0401545986533165 Validation loss 0.046711213886737823 Accuracy 0.878000020980835\n",
      "Iteration 41570 Training loss 0.04460589960217476 Validation loss 0.04671993479132652 Accuracy 0.877625048160553\n",
      "Iteration 41580 Training loss 0.04367892071604729 Validation loss 0.04699935391545296 Accuracy 0.8788750171661377\n",
      "Iteration 41590 Training loss 0.04753856733441353 Validation loss 0.04684150964021683 Accuracy 0.8792500495910645\n",
      "Iteration 41600 Training loss 0.04454965889453888 Validation loss 0.04673153534531593 Accuracy 0.8786250352859497\n",
      "Iteration 41610 Training loss 0.037919677793979645 Validation loss 0.04670697823166847 Accuracy 0.8782500624656677\n",
      "Iteration 41620 Training loss 0.042725466191768646 Validation loss 0.046827174723148346 Accuracy 0.8786250352859497\n",
      "Iteration 41630 Training loss 0.04873524233698845 Validation loss 0.04744832217693329 Accuracy 0.874625027179718\n",
      "Iteration 41640 Training loss 0.0345979668200016 Validation loss 0.04661885276436806 Accuracy 0.878000020980835\n",
      "Iteration 41650 Training loss 0.04042435437440872 Validation loss 0.046715203672647476 Accuracy 0.8783750534057617\n",
      "Iteration 41660 Training loss 0.042830657213926315 Validation loss 0.04670850560069084 Accuracy 0.8786250352859497\n",
      "Iteration 41670 Training loss 0.04226303845643997 Validation loss 0.04668941721320152 Accuracy 0.8783750534057617\n",
      "Iteration 41680 Training loss 0.03951798751950264 Validation loss 0.04759080708026886 Accuracy 0.8710000514984131\n",
      "Iteration 41690 Training loss 0.03423086926341057 Validation loss 0.04676574096083641 Accuracy 0.8763750195503235\n",
      "Iteration 41700 Training loss 0.047875355929136276 Validation loss 0.04708171263337135 Accuracy 0.8750000596046448\n",
      "Iteration 41710 Training loss 0.03660273551940918 Validation loss 0.04726152867078781 Accuracy 0.8771250247955322\n",
      "Iteration 41720 Training loss 0.03637994825839996 Validation loss 0.04674476012587547 Accuracy 0.877875030040741\n",
      "Iteration 41730 Training loss 0.04430929198861122 Validation loss 0.04672696813941002 Accuracy 0.878000020980835\n",
      "Iteration 41740 Training loss 0.041698601096868515 Validation loss 0.04681392014026642 Accuracy 0.8753750324249268\n",
      "Iteration 41750 Training loss 0.03850916400551796 Validation loss 0.046654071658849716 Accuracy 0.877375066280365\n",
      "Iteration 41760 Training loss 0.04739544913172722 Validation loss 0.0466546006500721 Accuracy 0.878000020980835\n",
      "Iteration 41770 Training loss 0.048226673156023026 Validation loss 0.04689362645149231 Accuracy 0.8782500624656677\n",
      "Iteration 41780 Training loss 0.03880270570516586 Validation loss 0.046708617359399796 Accuracy 0.8767500519752502\n",
      "Iteration 41790 Training loss 0.045520417392253876 Validation loss 0.04656291380524635 Accuracy 0.8785000443458557\n",
      "Iteration 41800 Training loss 0.03685217350721359 Validation loss 0.046621453016996384 Accuracy 0.8783750534057617\n",
      "Iteration 41810 Training loss 0.043265704065561295 Validation loss 0.04686698317527771 Accuracy 0.8748750686645508\n",
      "Iteration 41820 Training loss 0.042327649891376495 Validation loss 0.04672388732433319 Accuracy 0.8783750534057617\n",
      "Iteration 41830 Training loss 0.03873366862535477 Validation loss 0.0473349392414093 Accuracy 0.8750000596046448\n",
      "Iteration 41840 Training loss 0.04307999089360237 Validation loss 0.04726245999336243 Accuracy 0.8756250143051147\n",
      "Iteration 41850 Training loss 0.04867270216345787 Validation loss 0.047908611595630646 Accuracy 0.874125063419342\n",
      "Iteration 41860 Training loss 0.04186958819627762 Validation loss 0.047891926020383835 Accuracy 0.8740000128746033\n",
      "Iteration 41870 Training loss 0.04251553863286972 Validation loss 0.04662424698472023 Accuracy 0.877625048160553\n",
      "Iteration 41880 Training loss 0.048114076256752014 Validation loss 0.04664885252714157 Accuracy 0.878000020980835\n",
      "Iteration 41890 Training loss 0.04305819794535637 Validation loss 0.046729471534490585 Accuracy 0.8786250352859497\n",
      "Iteration 41900 Training loss 0.04657566919922829 Validation loss 0.04711920768022537 Accuracy 0.877375066280365\n",
      "Iteration 41910 Training loss 0.042502760887145996 Validation loss 0.047352295368909836 Accuracy 0.8737500309944153\n",
      "Iteration 41920 Training loss 0.040624819695949554 Validation loss 0.04660218954086304 Accuracy 0.8783750534057617\n",
      "Iteration 41930 Training loss 0.04119977727532387 Validation loss 0.047083932906389236 Accuracy 0.8765000700950623\n",
      "Iteration 41940 Training loss 0.0368184968829155 Validation loss 0.04665244370698929 Accuracy 0.878000020980835\n",
      "Iteration 41950 Training loss 0.0433964729309082 Validation loss 0.04701191931962967 Accuracy 0.8765000700950623\n",
      "Iteration 41960 Training loss 0.04381348937749863 Validation loss 0.046646736562252045 Accuracy 0.8770000338554382\n",
      "Iteration 41970 Training loss 0.03913380205631256 Validation loss 0.046574387699365616 Accuracy 0.878000020980835\n",
      "Iteration 41980 Training loss 0.04620080068707466 Validation loss 0.04680737107992172 Accuracy 0.8770000338554382\n",
      "Iteration 41990 Training loss 0.041320111602544785 Validation loss 0.04657459259033203 Accuracy 0.877500057220459\n",
      "Iteration 42000 Training loss 0.03729825094342232 Validation loss 0.048108138144016266 Accuracy 0.8727500438690186\n",
      "Iteration 42010 Training loss 0.04173006862401962 Validation loss 0.046916358172893524 Accuracy 0.8765000700950623\n",
      "Iteration 42020 Training loss 0.039565376937389374 Validation loss 0.04660630226135254 Accuracy 0.877625048160553\n",
      "Iteration 42030 Training loss 0.04114986211061478 Validation loss 0.046761516481637955 Accuracy 0.8765000700950623\n",
      "Iteration 42040 Training loss 0.04487433284521103 Validation loss 0.04663146287202835 Accuracy 0.877375066280365\n",
      "Iteration 42050 Training loss 0.03578557074069977 Validation loss 0.04675210639834404 Accuracy 0.877625048160553\n",
      "Iteration 42060 Training loss 0.05282365158200264 Validation loss 0.04657093435525894 Accuracy 0.878125011920929\n",
      "Iteration 42070 Training loss 0.03633498400449753 Validation loss 0.046701692044734955 Accuracy 0.8765000700950623\n",
      "Iteration 42080 Training loss 0.03843941539525986 Validation loss 0.047373779118061066 Accuracy 0.8728750348091125\n",
      "Iteration 42090 Training loss 0.039720457047224045 Validation loss 0.046588946133852005 Accuracy 0.8785000443458557\n",
      "Iteration 42100 Training loss 0.04790776968002319 Validation loss 0.04686933383345604 Accuracy 0.877500057220459\n",
      "Iteration 42110 Training loss 0.04359113425016403 Validation loss 0.04654966667294502 Accuracy 0.877500057220459\n",
      "Iteration 42120 Training loss 0.04188917949795723 Validation loss 0.04688788205385208 Accuracy 0.878125011920929\n",
      "Iteration 42130 Training loss 0.04912981763482094 Validation loss 0.04657289385795593 Accuracy 0.878000020980835\n",
      "Iteration 42140 Training loss 0.0324777252972126 Validation loss 0.04658733680844307 Accuracy 0.8787500262260437\n",
      "Iteration 42150 Training loss 0.036740440875291824 Validation loss 0.046625103801488876 Accuracy 0.8782500624656677\n",
      "Iteration 42160 Training loss 0.03860168531537056 Validation loss 0.046719301491975784 Accuracy 0.8792500495910645\n",
      "Iteration 42170 Training loss 0.04670887440443039 Validation loss 0.046569596976041794 Accuracy 0.877875030040741\n",
      "Iteration 42180 Training loss 0.04876387491822243 Validation loss 0.04676401987671852 Accuracy 0.8785000443458557\n",
      "Iteration 42190 Training loss 0.03896903991699219 Validation loss 0.04662670940160751 Accuracy 0.8791250586509705\n",
      "Iteration 42200 Training loss 0.04301758110523224 Validation loss 0.046518586575984955 Accuracy 0.8772500157356262\n",
      "Iteration 42210 Training loss 0.04256075248122215 Validation loss 0.046828873455524445 Accuracy 0.8765000700950623\n",
      "Iteration 42220 Training loss 0.03523290902376175 Validation loss 0.046719636768102646 Accuracy 0.877750039100647\n",
      "Iteration 42230 Training loss 0.04161932319402695 Validation loss 0.04695073515176773 Accuracy 0.877875030040741\n",
      "Iteration 42240 Training loss 0.037073198705911636 Validation loss 0.04649991914629936 Accuracy 0.8770000338554382\n",
      "Iteration 42250 Training loss 0.03863589093089104 Validation loss 0.04648267477750778 Accuracy 0.8765000700950623\n",
      "Iteration 42260 Training loss 0.04641987755894661 Validation loss 0.046536583453416824 Accuracy 0.8786250352859497\n",
      "Iteration 42270 Training loss 0.04526730254292488 Validation loss 0.047020476311445236 Accuracy 0.877375066280365\n",
      "Iteration 42280 Training loss 0.036507751792669296 Validation loss 0.04702452942728996 Accuracy 0.87437504529953\n",
      "Iteration 42290 Training loss 0.03509807959198952 Validation loss 0.04663991183042526 Accuracy 0.8795000314712524\n",
      "Iteration 42300 Training loss 0.03984331712126732 Validation loss 0.047022294253110886 Accuracy 0.8783750534057617\n",
      "Iteration 42310 Training loss 0.03885302320122719 Validation loss 0.04685036465525627 Accuracy 0.8762500286102295\n",
      "Iteration 42320 Training loss 0.03917285427451134 Validation loss 0.047564439475536346 Accuracy 0.8716250658035278\n",
      "Iteration 42330 Training loss 0.0382281169295311 Validation loss 0.047226432710886 Accuracy 0.8771250247955322\n",
      "Iteration 42340 Training loss 0.04229661822319031 Validation loss 0.046503931283950806 Accuracy 0.8783750534057617\n",
      "Iteration 42350 Training loss 0.03837358206510544 Validation loss 0.04646565392613411 Accuracy 0.878125011920929\n",
      "Iteration 42360 Training loss 0.045251186937093735 Validation loss 0.0464923121035099 Accuracy 0.8786250352859497\n",
      "Iteration 42370 Training loss 0.037725914269685745 Validation loss 0.04669896140694618 Accuracy 0.877375066280365\n",
      "Iteration 42380 Training loss 0.03704821318387985 Validation loss 0.04691701754927635 Accuracy 0.8751250505447388\n",
      "Iteration 42390 Training loss 0.04352520778775215 Validation loss 0.04786036163568497 Accuracy 0.8733750581741333\n",
      "Iteration 42400 Training loss 0.04434698447585106 Validation loss 0.0466325506567955 Accuracy 0.8771250247955322\n",
      "Iteration 42410 Training loss 0.04557659104466438 Validation loss 0.04763910174369812 Accuracy 0.8705000281333923\n",
      "Iteration 42420 Training loss 0.04233168065547943 Validation loss 0.04653850197792053 Accuracy 0.8782500624656677\n",
      "Iteration 42430 Training loss 0.04227332025766373 Validation loss 0.04647206515073776 Accuracy 0.877750039100647\n",
      "Iteration 42440 Training loss 0.04257091507315636 Validation loss 0.04647946357727051 Accuracy 0.8790000677108765\n",
      "Iteration 42450 Training loss 0.04003534093499184 Validation loss 0.046839211136102676 Accuracy 0.8756250143051147\n",
      "Iteration 42460 Training loss 0.03621673211455345 Validation loss 0.047043222934007645 Accuracy 0.8772500157356262\n",
      "Iteration 42470 Training loss 0.0445575937628746 Validation loss 0.04686363413929939 Accuracy 0.8783750534057617\n",
      "Iteration 42480 Training loss 0.04771290719509125 Validation loss 0.04670363664627075 Accuracy 0.877375066280365\n",
      "Iteration 42490 Training loss 0.04499567300081253 Validation loss 0.04652577266097069 Accuracy 0.8787500262260437\n",
      "Iteration 42500 Training loss 0.037032317370176315 Validation loss 0.046588290482759476 Accuracy 0.8798750638961792\n",
      "Iteration 42510 Training loss 0.046189308166503906 Validation loss 0.04670002683997154 Accuracy 0.8791250586509705\n",
      "Iteration 42520 Training loss 0.04064662754535675 Validation loss 0.04646201804280281 Accuracy 0.8788750171661377\n",
      "Iteration 42530 Training loss 0.03935462236404419 Validation loss 0.04676548019051552 Accuracy 0.8785000443458557\n",
      "Iteration 42540 Training loss 0.046368587762117386 Validation loss 0.04789271950721741 Accuracy 0.8727500438690186\n",
      "Iteration 42550 Training loss 0.044055551290512085 Validation loss 0.046862222254276276 Accuracy 0.878000020980835\n",
      "Iteration 42560 Training loss 0.04215564206242561 Validation loss 0.04738985747098923 Accuracy 0.8728750348091125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 1.5, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(3072,2048,10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-5, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 2048, 2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB4AAAIJCAYAAAACpf4qAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAuq5JREFUeJzs3QecU1Xax/EHht47SBUQEURAQUBUsOBie200sSMsYl+xIKIiuiusgh3LWlAUUVHRRVwrKIooqIAIFkQQpPdeZ/J+/kdvzGSSTGYmySSZ3/djzJDc3Nyc3HtzznPPeU4xn8/nMwAAAAAAgDgoHo+VAgAAAAAACIEHAAAAAAAQNwQeAAAAAABA3BB4AAAAAAAAcUPgAQAAAAAAxA2BBwAAAAAAEDcEHgAAAAAAQNwQeAAAAAAAAHFD4AEAAAAAAMQNgQcAQNrJysoq7E0AECcc3wCQegg8IO7WrVtnt99+u7Vs2dLKly9vjRs3tiuvvNJWrlxZoPXOmDHD+vbta6VLl47ZtgJF2aZNm+zBBx+05s2b21133RWzdT7wwAMxXWckO3bscOebl156Ke7vBaBwXHjhhTZr1qzC3gwUcfv27bNXXnnFunbtaieeeKIVRTt37rSnn37a2rVrZ5dddlnIZd566y2rVq2adevWzZVZXtf/zDPPRFx/PBRkm5HAwEOPHj3s/fffj/VqkaK+/fZbO/roo+2II46w2bNnuxP0xo0b7cknn7SjjjrKVq9ened1/u9//7MOHTq4E73Wl6wnBF2RmTJlil166aXWokULq1ChgpUqVcrq169vZ599to0bN852797tlu3Xr587ycmXX35pxYoVi3hTo2ru3Ll21VVXWevWrXNdPvgWTQNw/fr1duihh9qBAwei+rxbt2616667zk455ZSQ75mRkWFly5a1gw46yDp27GjXXHONLVq0KM/lun37dhs9erTVq1fPli1bFvXrpk2b5ratatWqVrNmTbvgggvs559/zvP7pyMdQ3//+9/t4IMPtsGDB8ekXLT/Dxw40A455BC78cYbE1LWeg99x506dbJLLrkkx/PvvvuunXzyyVa5cmW3L+q89O9//zvqc0he9yGVgT5/NMekgrKhvP322/73LFOmjDVr1sz+8Y9/2KpVqyzefD6fPf/883bcccdZpUqVrFy5ctaqVSu78847bfPmzQk/7v773/+6ssqt8rlmzRq76aab3HlX37PKtk2bNu68t23bNou3gn5n33zzjZ1zzjmuvLSOs846y+bMmZOvbdG69LtzwgknRFxO5TJixAhXTvqtUrkddthh7thVeSbK3r17XSNGvz2ffPJJ2OVUh7j77rvtjjvucPspEkvHv/aNJk2auP1L++p5551n8+bNs6JizJgxbj/VBTBdCCuK++Ftt93mykC/9arvh6PAgfaZjz/+2BYsWBD1+m+55Ra3j6l+Emn98ZDfbUYufDH066+/+ooXL+477bTTYrlapKitW7f6GjVq5Dv99NOzPf7WW2/5ihUrpjO0b+rUqbmu54MPPsj27127drn7yy67zK0jxrtxTHz00Ue+ww47zG1b165dfa+99prv559/9u3cudO3cuVK3+TJk91xUq1aNd9JJ53kykOPeXbv3u2bOXOm75BDDvF/xurVq7vy2r59uy8rK8u/bGZmpu/444/3L/fAAw/4VqxYke32448/+v73v//5zjzzTLfM8OHDc/0Md911l1v21VdfzfPnv+WWW/zbc9555/k+/PBD38KFC31z5szxjR492le7dm33nM4Xd955Z1TrXLdune/222/3ValSxb/upUuX5ml7Bg8e7Fu1apVvyZIlrizKlSvne++99/L8+dKRjqtffvnFf2xGs4/kZs+ePW5/j+U6w/n+++99NWrU8L3zzjshn3/wwQf9+03wTeeowGMqVvuQtiXcewbf+vTpk+P111xzTdjlde747LPP8lRGb7/9tjsXRGP//v2+c845J+z769y+aNGihB13a9as8dWsWdOt79JLLw273Ndff+2Wa9CggTt36XU6Bz7++OO+ihUr+po0aRL1eUOmTZvmzlvRKuh39uijj/oyMjJ8ffv29S1btswdP5dffrmvRIkSvueee86X12M68HcoHJWHykXb95///MeVl8rt5Zdf9tWqVcsdV998803U76vyUrnlxbZt23z333+/r27duv7ymj59esTX6HdSv30XXHBBrscvYke/xc2aNQu5j5cpU8b3ySef+NLxM8+dOzfH8bV3717foYcemusxlq70G696bdWqVSOem1W/1TInn3yyK7NoqYw3bdrkq1SpUq7n/vwIbl/EYpuLig8ilF0kMW2xXXfddW7HUCXzp59+iuWqkYJUgdL+oP0i2Keffup79tlnc60szJgxI+yJ5rHHHkvKwMM999zj/wF+4403Ii47YcIEX6lSpdzygYEHz7///W//Zxw4cGDY9dx0003+5caNGxd2OZX3WWedlWsDUD8mqnBqfZ07d/bl1ZQpU/zbM3LkyBzPqzKthkukZYK3Z8SIEb7XX3/dd8wxx+Qp8DBmzBi37IUXXpjtcf1YahvKli3r++GHH/L8GdOVAlyxDhJ4DcZ4BR7UqFWD5eabbw75/Pvvv+876KCD3Dln9erVrlE1fvx4/3bppn/Heh869dRTfe3bt3cNuNmzZ7tlgm9qlGvdb775ZrbXPv300+63tHfv3r7//ve/rtKr/b9Tp07+bVYQTsdStFQxjnR+CDRs2DBfyZIlfVdccYULWur9X3zxRV+LFi3873/wwQf7duzYEdMyC8cLmkaqfKqSqoCDAhuh6iAvvPCCe33Hjh2jfl+9V7T7bUG/My2rZdSYVkDZo7+1zQrUfvzxx1Fv+9VXX+1/33CNIv0mHH300W679XsbTI1IL9Ck83A0VF55bSDce++97vcy8HvOLfAgv//+u2scKFCO+NP+ctxxx/nOP/98F+TTBaZ58+a5fd773hSUCNx/08Hdd98d9tzZs2fPIht48OgcEo/AgKddu3YxX78u4rVp0yZm6ytKtheg7GLWYtuyZYuvQoUK/hOPfvBQtJ1xxhluX7jtttvyvQ5FGsOdaJ555pmkCzzcd999/m1SYyPaymq4wEPgZ7zjjjvCrkO9BqIJPIgqw7lVpNVAC7yKkZcrfqKrXbkFFXT11VtGDQWdQ6IxceLEqAMPer506dJu2VBXetX7Qs8pmIE/1K9fP+ZBgnisM5B6D+n3Z8OGDTmeUwW4e/fuvuXLl+d47quvvnJXl7VtqjjHch9SD6ezzz471yslapTqSryu3gZusxp6L730UsieCOpF5B0D//jHPyKuPz+BBx2Luvod6qq1Ag2Bwb+HHnoo7sfdE0884QIKquhEqnyqh4OeV8AnFH0X6jkQbrsKEngo6HemBpx6Fuh5BcrCBSXUM8Hr9ReJgkW6StilS5eIjSIdA3pePSPCUdlrmWh7qeQn8OCZNWtWngIPXpBLgZNoelCiYHSMqedhKP/3f//n/+5y6w2VSnTFXcdmuHPnRRddVOQDDwpGxTPwEI/1K5ikczYSW3Yxy/GgMXka/6lxXvLCCy+4Md8oulasWOHuS5Qoka/XP/roo25sVTjKGZBMNAZ36NCh7u/u3bu7cX/RGDBggB177LG5fsbixcMfrhr3HK22bdu6nBKRKMGgxnJ7Hn744ajXH+32nHbaaW78s+zatcu++OKLqNatZD/R0vh9jRnWWGUlNwx27rnnunslKZs+fXrU601n8Tiu4nmsvvzyyy7vi3I6VK9ePcfzyt+g/bdBgwY5nlOuGN0k3Pjc/O5DSp773HPPufHPkc6RX331lcv54h0L3rlE5xAl0Aum8+l//vMf93srn376qcXae++953IkhEqWpnwJGvvqCfX+sTzulA/i5ptvtvHjx1uVKlUiLvvrr79G/M3Rd+HtI9q+WCrod/bEE0/Yhg0bXH6Fk046KeT5UomU9RmV2ygSrUfneP2GKplzQcpM6tatG5cyK+j53TNo0CBXtsp5tH///rhsF/6gvCnKrRGK8jZ50iXfgT6H8gvomEqVumhhiHcZxHr9X3/9tf3zn/+M6TqLiq8LWHYxCTwo+Zx+4K6++mq74oor/JnFn3322VisHinKS+KVl0axRxV2JeNKJUrKl5mZ6f87L5RAJ5EaNWoU9rkPPvjAfvjhB5s8ebJLNiivvfZazBOMlSxZMlslM9pkdXpdNNTgnDhxovtb2ZBDUdIiryHC+Sr16HhTQjw588wzQy6jBn2oxq/HC0icccYZMd2HlMwvt0bU66+/7iq2ffr0ybHNmpkjHL2fF6yMR3LdOnXqZGtEBNMMRUqYGOr9Y3ncqW5x8cUXuwZlbskRAxvICmYoOW4w1UvUgGjYsKFrQMVSQb8zXawRJXcMFQRQw/rwww+P6lylRG9dunQJmWA1XJktXLjQ3YJp//ztt99cQCRcgDyWoj2/B5eNjn9tJ+fx+FJC3nB1Ou9c2rRp04jn3FSh84/aNG+88UZhbwpi3HA+/fTTkzYxfbqXXUwCD6o8acpEVQ50867wPPbYY3maa1nLqsGp2Qq8bNCqHKhi6WX/D0WVDM2mocqS3lsnP11F9q64i7JyB2YQ9xpUHmWQjvS8N9uAZihQtmfR+vVjV7FiRXfFKvBqwHfffWcXXXSR2xZtkyqg7du3t/vuuy/iF6Z16Gqzsv4ri7je68gjj3RX7LxGrejKT6jM6N7MCJ7zzz8/2/PhKueRpsIbNWqU2wZtj74XbZuy+e7ZsyfH8soa7r2XN+OAvr/AbcjN5Zdfbv379/fvO6qQea/V1fpwVEHSVSXN8qCKiO6DyyPYZ599Zj179vTvO5pxQhnTf/rpJ8sLZdv9/PPP3d8qJ2XOzwtdzfIq8vESbTBEUx/qeFA2fq/xoX1WV+RiSft64FUENQaiEW0gS7OoeL2uIl31U0Zmb1+IFV3tViOkVq1a/szsyi6v408VeM3sce2117qGkEc9Pv7v//7PHWNqpOhcs2XLloiVIl0F1lVpvY+uROt8qUzj0cwW8+qrr7r9VO+l88wxxxzjrnRHM6vIPffc445FfRbddE5Qlvm8nO9jNXuArojrSnA0DdNQFGTTzDuabSHR+9CkSZPcufxvf/tbtsfV+AzVQyNQjRo1ct2u/NJvsH7X8vP+sSwzXVnVFWztb9HQ8aNzv6ZfUx0geH9U7xg9pt/T/DRwIynId6bzhfbDUM8F8hpz6l0RrveB6lB6XsdjNDp37uy2W7+f6iURXNf68MMPXdD5X//6V8geRbGWnwsVXh1P7r333mx1pVhQjyqtX/uW6nKaZUT1W50zQ836pNlXTj31VPd9az+rXbu2m+VEx3sgzU4Vqh6nc2qgjz76KMcywTM66d+aplyNftWd9V2pXqHvL1G8fVizThXkCrWOB82WoF6X+l1TOer8rp7V4b5b9RzTb2Zgz7EJEya44Ke+NwVLdQElWvrtVW84vadHx4dX/pEujOl3/dZbb3V1Gn2Xmo7x+++/D7u8zkmq5+q3XPuXPoN6i6kMItUBgilgGWp/UhstkOq54WY50/lW50f9pus3QL+tOj/o91GNzvzQd/bOO++487PqlZHowpeW0zGjclA9Q99jJHnZZrURdCx7gWkFKwPLwivvvGyz2hlqjCuIq7qUjkH1wgo3e9O2bdvc+UP7pFf2v//+u/vN0udWHVA9ptX+yivtSw899JCrC3rloP1Wn9vraRiuV6hmA9OMXzpe9Humtl9gOy/assuVLwY6dOiQLfGdN94p3Lj1UNavX+/G8Bx++OFuXJ/GkX733Xfu31rPUUcd5bIeBzpw4IDLJVG5cmWXGEzjUpVoyEtyozGqWocoKZLGtnsJ7YLHpmh87fz58/2ZegOf17jGI488MtuYd40lDs7q6yUHUyZzJQzUWMwvv/zSbde7777rq1OnjluuV69eYWcFUeIuJZZS0h6VgcY7eq/TOGaNFfXGD5944on+927atKnLRB2crHHfvn2+f/3rX26Zv//9766co6Vt0NhObY8SoymZiLbHG9+rTL7Kuh08zlXbqJtX1spN4D3mbX8k+l61nDc29ZJLLvG/Vs95NN7O+/x6TlnAlbCsYcOG/iz6Gr/9+eefh3yfIUOG+Fq1auW+382bN7us+D169PDnHMhLxlaNefS2Rd9LrAR+xkjjjPVcbjkeNJY8N5p5Quv44osvcuRuUbLJaJOL6RjOLceDEmt6y9SrV8/tq3ldd6QcD15yU92eeuqpsMtpHH7gcV0Qv/32m5udwBtH7o1TVlI+jXnX8RT4nMZ9i7ZP+6rKQfue97xynITLsK3jQ8sriZ3GhyuZnrLf63Ua2x1uPLbKWfu5tkPfzdq1a11yRh2nSiaoW7j9TeN2dV679dZb3QwFOm6UsE/nWm92iFDHuHcuiHWOBy+pV36THOkYb9y4sfveEr0P6Xyt81S/fv3yte0nnHCCez/liIlHcsncKLGk3l8z1sSjzPRbo9wXgWPFtf25jfMdO3asf716Dy/5pdajXCOaYSgv8pJcMr/fWWAy3qFDh4Z9/fXXX+9fTr/PwXRMqj4UmJtD25/b+HOdQ5S40su7ofOLKBGrZlZ6+OGH8/Q5C5LjQef0vOZ4CM4Nkd9s6+FyCunc/Mgjj7j9VHUenTe9pNDB5zslZfXyjKhOpzqX9kkvl4z+9ui1SrTtHUtePS04h4fqdVpP8+bNXS6O4PwkSsqp3wLtV0qcq3qwzufed6o6YLx5s2tp7HdB6HdLCVjPPfdcVydTvVPnGG92Fu2fyrkQOIOYl0/Mu+k3Tklt9Zvr5RfSTefbvMy44dU7vdcr95X3WGDyzMBjTL+lRxxxhEt46s3epZv+1u90MLVrunXr5pJ+f/vtt24Z7fdah16n3yf9VkRDeYz0ub33VMJlPRbcLlA9TucZ7R+qh3n1OuXA8doVel77u34bBwwY4B7TPh88s0du52bVD7zvLlTbK9QEBUq4rO3W+yuHkOr1Xs6g4PXndZu9NoqOD297gtsn0W6zfltUB/HaoKoP6b1VP9K+pm1+/vnn/ctrv7zppptcouvAer2S+iqHiNp6gbkSNeNdXg0aNMjV1dXm1L6kOv3FF1/s1nfsscfmWF77a9u2bX39+/d3v5F6jfKvqR2l1yipp7ffRlN20Shw4EGNOhVwYHZq/SB6Bacf2txox1GjSAeqCiGQpo/y1qUTeqAbbrjBPa4kSoF0svdeE1zQXvb/cDvSjTfemON57chqgAVOLaYvSScH/cDrx0AnN50cVPjeyUazLoRKIqhbcFZr7bCq0OsWnCU8MHFgYIVl48aN/h1YmdPD0fSKyt4ebaNR9D2oMaEfxOAfQW1fy5Yt/QGP4IBQrBobuVUyAxvlOrCUsMzbVlVCvBOVGlmhptZT2XkVLI8OLJWlXqfPH23CQyWv87ZFJ7xkCjwoWKMp0vRcbrTtwQGKwOnhAk+iBQk8qOLkzZqhHz9NsRqtaAMP3rGsmxKzhaOAlbecTtIFoWNDx46mhQs8Vygg4DVutU/pB8V7Xse3GuzeOVTfl/cDHCqxp/ZRndcUOPACq4G8SpBmVQk1BZ43Da0S9kXaj4L3N52jdD7QrC2REoWGej4egQeVkxekCZcYMhLNDqAfe82WEhjQTNQ+5E3vmZ/pXHWeU2VM2x/u/BvPwIPXOFSgPDh7fSzKTI0N/bYENtKiDTx4MyR469ZFA33XmnJRAfu8ilXgIdJ3Fhis0e9YOKpYe8sFJ1LUPqwZiIJndokm8CCqOHtBUTV2vDLLa2Lhwgo86Lfce921117riwU12FS3U9LEYN53Fljh1vnY2wY1mgN5DUJdsIoUNNFMVqHoOFNDMjjQp4tCqusogBHMqyPrFmrGkljR754uOKr+unjx4nyvR+WnhqLqIMGNZQVUvDqDgg/eOVvnCrnyyiv9n1W/cSpH7zm1SfR7qOcUpMircHWr4GNMwQJdLFB9xtv+wGmctc8EU7vilFNOyXEeVdBP54tIFyBCUbl49XOdn8PRhbfgGYe8fVrnqEDaNi/BrAJjoYQ7N6vdobJQcCVS28ubvl3bFakeHLz+/G6zV9cJtT3RbrNmdtHzauSHW7/qtwose7Zv3+6+Wy8oqP1R379XV9P35603VP0vEl18UntcyXaDqR0U3B5WIESzkaiOGkwX4gPrsKE+W36TSxY48KArdqEOZC8DqW76AJF4026FivR7BRn84fUleSegUBT99RrGeSmwSM9rHnDvMwV/sd5JRlcPvWUUSQykKYe859QTIpB+KMNdIVL0Plyl3mtQhvqh86jiFekqSijelX81oEIJ3CZVNAs78KCod7jPoJNPcANKUUVFVUNRxc1br8o3Gl7PnHAnzkQGHnS1W8Ev7+Zdmckt8KCKm36cJ02alO1xVSS8Y1D7Un4DDzpG1LhWQ09XMvWcKhLB0wjGKvCgHxtvuVA/DB4vGqzbzJkzfbEQeNIOdQzpx8R7XtMVBley9OPkXSHTeSfUD224CpQCkl75KlodSEFaPa5gaaipbHVseD+IwfubAr/aJi0TTFMkep9H595EBB4CK/nRnt9UodAVb11d8vZpr8IYHPSO9z6k3y5NXZqXKwXBs84EB7d1lUjrDHdTw1LnvnDPK8gYDe+Kh3r3xaPM9Fsfav+ONvDgNaQDrx6pchccaPZEKjM16tQACPe89qWCfGfi9UoMdbyHKvdQ9QvVDXTlKngWlWgDD6LZNBRw984B6mmqXhSh6LcgXJl4VyjDPR/pdyS/gQfxvu9of6dyowaC1qdyDaZKu36/Ao9fb1aVUBeXdOVWj+s3NhQ1CiL13lLjpXXr1iFfp+8pFAWnvO1RECnWFDDUfhd4BVfnGDW288O76BNcBwlV3w1uxAf2dAq13+g49eoc8Qo8KDAU3HtOv7PetM065wVSEEmPq8dRpCkqdctL0DTwImeoHgrad/WdBV+Y8II3oYJj3oXXcDMG5XZuDnVR17NgwQK336ini+oSwdQY98oweP353eZoGs+Rttnrpaa6fyg6DyuA6+0XwRd+a/75efS7FHzOVs+XaH4Pws34FmrWJNVJgwMP3n4S6gJW4DaqDRF4QbyggYf8TTfwp6VLl7qxLaHGkGn8kzfmXWNHIiX8efzxx929xsCFGgc6Y8YMmzdvnstXEM1rvKzR7777bsis3Pml8TKe6667LuS4RI21VpLNxYsX5xi3GzhmNnB8pv72yifU59FjU6dOdTkllDcikMa0afyrntMYTI1hDaSs4co3kVuug0CrVq3yL3/88ceHXEZj1pQgTBmxNe5H4yojZW+Pt1A5FbyxssHZiJUoSGPwlJk91NixwHH3CxYsiOr9A/N25HcWj1jRuOjAZHX6PBpbqvGCkeiYUu6B4HFgGt+mxHsa7zZ37lw3JjvcfhGK9kvlBNG4Nq+cNH5RYyB79+7tz5kSa4FZtQOP3WCBWdDzO744mMalepTbJZg+v0fj6oLfV+NCNUZXuXOCk26OHTvW3Yf7DjRGVHlLxo0b5/JK6BygscjedyEatxjqsyrfQL169bLlx/FoDKrKNHDbw43P1djE/GSnz4vAYzO32Q48GrepMYwa+6yxixpP6q1L57T58+dnO37jtQ9pPKdyBilbel7PF/q90PlW37/GdAfSGNNIY4nPO+88d24ITmbpieZY1PlUyaQ1e0OohJwFLTPl7ZgyZUrU595w41yXL1/uzvXal1VOqqcol4fWrQR5gSKV2fXXX+/GyWqWj1Ci+d2L9J3Fosw0jvn+++93+1RBfof1e/7II4+440ljjJW7SGWmcgzOofL++++HHW+vMf4q93CzIcUrA75yNf3yyy95ztEUjsZaazYp1T+Vk0c5urxjRLkbdB4Nrqspt4PG9ntJO4Prf+Fyc2g/mzlzpjsHqc6r5KDB533VLQMtWbLEvUZj4fXZI9VLCnI8RaJx6hpLr3wVa9eudTkvbrjhBlcXDpU3JxzlaPDG44f7bdM5R7mqVKfROSgwAW7gcRMq34/G3UtecibklfJ0Beeq0nGqZN4aFx9cF/USyiqvWahjInCMv76/aPOAqZxUv9q4caM7npX3JdCbb77p8tgdddRR2R7XdmhfV7siWG77b268GX1C0SxI2m/0GxxqOZWN6h2hEgYX1jbnVg/TeVj54oYPH+7aVCrzwJnuSv+5vyrHTvA529tX87q/ejmEVJfXb5zKxqOcd8G5Krz9L1w7WvuPdx7ROTV4f8mvArWQ9KOiDxNq6iclp9OOrYQ3agxrx/IKJbiSqpOnl/AkFCWz8BIHebzpqMK9Rl+cfihiKXA6w0iVRSUNCaTPpwQ1gZlxA5Ne6WSr6QQjfR4lLglFO6ymGlMgRMnilKjES9rl7YCqHEabuM+r+HkVCiU6CUUnUyUgU0VFyeZUQVEDKpl4J43gJJhqiMkdd9yRI5ATbh25CWz4eAdrYVHDKrgSomk+IyVx04lZySO1XKgfQAUSFXjwjvu8BB7U2NHxr8aWzhXaX5SURslr4hV0CA70RUroGrh/KDFoLORWsQ5OHhaKVzaB267kXT/++GPEY9OreCnw4J0rFXhQgjgvGBx4jggWatpWfXe66T31I5+baAMBBRFYiYumPL3j2QsIK/mTGmoKaGt/XLRokb344ovZppqN1z4UbjaLaCjQrGNIUyoGf1fa70I1QAJ/L0KdH/JC5wK9PlzywoKUmRouCsaooqyGS37oPfWdqsHsfdeq+Cugqu9ZDTpN0xxYiYpUHjoOtX8VpMwifWcFLTMlg9TvmAIb+Z2pQ/ui6ksKbDz11FPuMf1eqBGt84buNcuRAnYeb+r0UFReKreClFl+eL/Xqk8pYKvAQUGoceA1qlWvU91IDTpV6NXYD5xWVvR+SkQZSOdcLacLYZGmmdT+qQCXAjZqLAYGHhRMUdLW4NkVvLqMvhfvYlw48bggoqCDbvqN1z4+bNgwlxxdlFxZx2GkacADqWHm1S3D7Vv6fnVcK6G8Evdp3/T2sdx+c73jpTBmMsitLqr9ShfyYvWbqmNPM9uMHDnSzTCkgFlg+0uN5lDtI10kCZzWXOcDHfeqS3h1h/wmkA63HyjgoM+fn3pJYW2zAl/eRYvc6mEe1cMCAw8ZEfbXwDpEXoImOmeo7as2pxL066KwjkO1AXVcPf/88/5l1cZTEmLRfW7JlmOZWDjfs1ooa7UqBzr4deAH33TVzJt+Tweb92MWKvDgycv8y97rknnOZu30+sK1s+nkrEzHsSwDjypqKnMdXIFzq6pirivdwVHy3OhHzhPpRyPwymc0WfQTzbsiFPxD7+2XOuGF2ncDb9E2RAOnjlLlORkFX+ULpJ4f2m5VHkKVQ+Dc9OoNo6uJ0fIqoPqB0InQqyyrspKfrL3RCgy2qdIfTuA25JaVvrAV5NhUb5W8BtSCjxn1WtEPbW7HTbSVzVhM1yuBmczzQsFSXQH3KgG60piIfUjnZZWjgrd5MW3aNNco0W9J8BXVRNAVElW41AMvXLCnIGWmBp0qa8pk7gW7Am9eJUwNS+8xL2jv0ZUvVfI0u4tHV9K03eqJpCtIauQF9myLp2i+s4KUmT6nehdpZq1QZeaVj8rOeyzw2BH1hlMgSb3lPAoM63jQ8azXqhGp1yazwPOaZjaJBZWLd+VeQQHVp3RRTQ39cD0+VBfTb6pmA1NvP/1+KvieW2DAq6vpNzaw15kuCujqafB52zsv6zvO7Zwc6sJfLOl3XjNi9erVy98jOlTPudx+21RvS+V6Z6S6aHAD2Pv+9Hlz+/7y+hunGQa1T6n9FTgzh3pOqEe2ekWGo3OQAplqtygAqR5MmjkwHrQt3jkvr/WSwtpm7dved5lM+2qpUqXczGTeNNbqRaReWTqXB49MUGDW+z1V0CG3/S+Ws0Dlu3aoHVmNMu00ugIW6qYuY97BopN0qEZ1YKNQV8+j5b0uL69JFP2oa/5sRaH1g6VItf4d7upufsvAozL2KlnqXeL1INHwDVVownWjCSfwBzu4a1i4CGxBrywkkrcfav+MlcDIpr7vcFc1ClOkio+uUqirp7ochzue1ZVXVNkK7tUTLVVeva7GOnnr+IjXFIw62YYK7gXzAkWqTMaqx0O8FOTYDGy05LW7qXfMKGAUboqoRAv8IYw03XI0ATld0Q1VOYjHPqT16EqXKn556XKu34aLL77YXcHStGGJpvPaLbfc4q7cRro6l98yU28ErVtBGTWqQ93UQ0UClwmcIk/TDqpXQffu3XN0X1WvHz2nRoACp5GGf8ZKtN9ZXstMV+K9Sq0apdqfwpWZN4Wjys57TA1Ej3pQ6dyuqQODr96pa7eCTKpjqF6jRnQyCzyeYtWbTsEqBREUQFK3aO97UJBA543gAIfOjwpoKoijQLuuvqoOFs0QLF1E0nbrN9brwaBzmwJ+oS4gxaMuU1A6R+SnseWVo+oDkS5IpGq9M5R4fn/qPa2LO945wpv2Vb0drrjiirANSfXYUc8DXThV4FFDWg4//HCLl4LUSwprm5O5jXTIIYe4gIN6WamnnxdsUu8/9Wj2BLbHE33+yFfgQTuwN75KEfxwERLtCN6VUo1xCZ5LVgK7U+Y237B2Kq+R4r0ut9doqEdg1DVWY7jD0Y+EvmB12VUlJ/BKcTh5KYPgK3IeNebUFUY/WBrHrXJSLxM9ntfPHHjFLtLY18DGdbRjz5KBF/lXZDC3BktgF65IFOX3rgCq4qix9alCVwJV+VTwKlLEU13T1U1bdFILvtIYLQU5vIisxglryEs8qJLodWH25hcPpuPFmw891JCxZFOQYzOwQZzX8b6BV8tUkY5EQ8fyO6YyLwJzSBT06qYXOAxueMVjH1JDMK/DLDS2VYFsXcUJlVch3tSYUqBEw/lC5SyJRZnFIljr5TgKN7TwzDPPdD0DRN214ykv35nyEXnHabgyE++iwrHHHhuznEoK3Oj7CFdmGpKic38iyqygAnuLxHq4l/KFKZ+CAjFel3AFujS8wKMLcdr/NR5aZaVhAXk9p3nDP3VxT1ertU8rkBEq2Oedl3Xl3AvKhaNtTwSdH7wcR5G6oRf0t00BmnBDk1OF9/3l9puqAI4u1OSVLiaJ2kB6D/VU1/GuwEMoGsqjc5WCweqhpZxf8VaQeklhbXOyt5EyMjLcUAudj9Tbzqu3jxo1yp+/T+car7dGbvuf2gex7J2cr8CDvmhFeTSGKDeBSRhDJRrSD67X8B4/fnzYD6dxWYq2ewXlXT1Q4sRIP4bKeRDYRcn7sc5tnFd+r8Jq+ImiTfpSo60g6kTtRerD9QzxcgcEjyn06ESvhD6ioIfWo6h84Hjl/CRqDB6vGMhL9KLvQkNrUoWu7HjR1cArP8EUmNB3Ge3JU7k2PBpblxdqOIVKOpYICgSoUZFbF3EFVnQy87pp6XjND12t04+fVzFUWeUl+Wle3sfr9hmuUqYfDa+RrCuTyU7nCu+HWkGbcI0179hUGXjnIeXj8ai8czsHBnYjVhTda+g/9NBDEX+E7rzzzph2ywsnMFGSKlQF4QUNgwMH8diHFHjQ+TI4b1E4OtZ0ZVWBwcAxoominghqsOuqWaikbcHyW2aqOP4501bYmzc0RcMpvMfUBT34CmukfdtLOO1dAYyH/HxnXjlovG2oLvz6vfICD4H7WW5l5iVdU9l5j6lelGxlFsvAg/alWCSwVJkH/y4r35Z6AHrHgi4wBQ5ZUT1NQ3vyOxTKqzNrPeppEW48fmBdxjvvhqP6em45IGJJ51PV7b0E3/God5511lmFnsi7oLzvTxd/vHwEoajtlJ/eAAqCeYFiNUDVc0a9wUI1ztXmUa8anR/UEzVRZasgntdGU1tOCbUjCTw3FtY26/fbG16tgF7w0DVPYDLMeA37CKR6nS4OeFQP07lD5ysvCOidr/Q77dUJVZcPHMYbTEOwY9kzOV+BByUq0Y9pNFnL9cG8g0uNuFAHl9crQAeWghnBH1A7lRrVgQntApMC6jWhEvrpC1AFIDBRjbfNapQHV1Y15tNLwJfb1dxwX4IX/dIPYPA6AgMKgQePrg7pJCr68gO7qgW+VuNflbQzHPVAUWRLlQNFOnUg5ifqr+i6d7JSJCxcdznvqn64HBJeRSa/lRXvJBI4Flfl4JVdXq6QBS6rSrEXgBoxYoQ/sU0g7Tf6HgJnUsmNujF5ATElSvK6uOZG5TNgwAA3Ji/SfhZuPGnwc3kpF3WxUoAl2jwg2s7AgEW44yC37dHVGy/5oZ7XUKRoo92B68vtsyoRmE6+ClCGGsbk5V3RFcRoGlXRiuVQm+CM917AV73IwkWqvWNT51bvHKBud97+qfOfkg5Fer/A4069prwGlF6rQFWoXgaq3CogEDzu0VtnLMtFV2K99/GunueXejbpim+o4z2W+5A3zELnoGh6ouk8pEaMvsdIgX41OiKdHwoSdND7qxdduEC6zl/K3J0Mx51XkYrUU82bJSa3nhv5ld/vTBVEHat6vZc8O5Bykej4UWMump6UeS0z1c/C7UPxLrP8nt+DX+eNmY9V9nXvQltwDy5dbfdmCAo8D3r1v1B5nsLV/4K1atXK3whXIEPrD54hLXCIjpb3esvquAtVLqobJqLhIwqO6fOHqsdGomEB3gUsXcAL1xvV+20Lri/lpWGU198hL4gV+JsYuH3Rri94Oe841uP67QnV+NPn1TAf5QvJD6/Xg9pe2mcDZwIJbiR7+22k/Tfcvpvbb3y451Wn8Xqhqc2g82CodYSqlxRkm0O1L/Qegcd6pM/kBQO1zeEuCHv7qi5oBM8GlhXl/prXfTXUTH0KxHoBzcDzlbf/aV9W3qNQs4YoqbzKKLDXazRlF1Fe59/U3NF62eTJk6N+zZ133umfk/T444/PMX+85tauU6dOtmW0fs1lqnmRO3fu7Dv00EN9u3fv9r9G6+jWrZv/NQcffLCbk1Sv0dzhl1xyiZsvWfMMB5o1a5b/NVpmw4YNbk7bt99+280hfPrpp/uf19yygfN+az5V77nguXo99957r3+Zfv36ublbtf6XX37Z16JFC/9zzz//vJuX94knnnCv++GHH3zlypXzP3/22Wf7/ve//7l5dl944QVfy5YtfV26dMlRdsFuv/12/zpUFvn19ddfu3m4tZ7zzjsvx/tqbm89r+8gFH1m7/XRzLkeSq9evfxz4G7dutXN73vhhRf6du3alW1ebN0C55gNnmtWty1btoSdD11zlmvu+Y8//th9bs0Vrf3prrvuyvM2b9q0yc0h7s1nHWrO9kCbN2/29ejRw33Xue1Pmq84HG2/t9z9998f9fb+7W9/81WuXNmXFw0bNvS/l465UF566SX/Mueee27Ydd1www3+5TSvtI653HjzJ+v2/fff57r8P//5T7fsVVddleO7qlevnjvuollPXsybN8+/jaHm6Na5zHs+3D7inRNVRoG2bdvma9Kkif+8F7xv69jT3NKar3zjxo3ZnpsxY4YvIyPD/96a71n7oOh+0KBB7njQczrn6Hjw5gHXnPZap/faQw45xH/O/eCDD3yXXXaZr3bt2tnOmR7tY6E+S0F585x36tQp7DKaA1y/CeH2rS+++MJXqVIl3+effx73fcg7Z+l3KDcqR51L+vfv734fAm+LFi1y38ubb77pO+2006I+x44ZM8Z93mj88ssvbv/S73eo99dvk37XjjnmGHeuTcRxl9tc8cuXL/dVrFjRLfPOO+/keH779u2+pk2b+qpWrepbu3ZtVO+p32qdc6JR0O/MO2+qHhJI9QitV8fuhx9+6MsLvY/WqbILRWVSv379sOciHT+a/11zuUf7fam8VG75oePUO8eE+g7DWbZsmf91Y8eO9cWCzola34ABA3I89+mnn7rnzjnnnGy/p942qB7hreNf//qXr0aNGv7ntK0qo+nTp4d8X/1meMs+/PDDEbdR+4N3ztbtpJNOcvuY9rU33njD1adPPPFEX6wsXrzYN3Xq1JB1rszMTN///d//uTLJra4a7nMXK1Ys7G/FzJkz3XM6voLdd999/jLw6ojR1gdzU7NmTfc6fTZ9Ln1/V1xxhf95fV49rzZEpPNW27Ztc5TXCSec4N8u/Q7dc889vi+//NL9HqkOqt9O1U3za+/evf66hM4h4Whb1GbSclWqVPF99dVX/rq+2jJe+0R1Dy2rbdO5w9OmTZuI9b3rrrvOv+5gP/30k698+fL+cjj//PNdfUNU79d3p/q0ntN3od9PlVFBtlnnCO/95syZ475XtQv0dzTbrPOi6h3e86HahDoflC1b1rX1AmVlZfk/7y233BKyvALraNFSu1mv0e9IsBEjRrjnVAfxqGwPO+ww/3tpP9HzKgOdm/Te2v/02xUomrKLJOrAgw5kncxUwHqzCy64wPfrr79GPLnoi1m6dKk76Xkb6TXIdfLS8x5tcOCJOfCmyo+WD7Z+/Xpfu3btQr5GO1yoCr8cd9xx2ZbVj7kqIjqpBZ6cdBK4+uqrXUVeB4YqWN5zffv2dTuaduZAeizwANJBoR2vY8eO7gfVO6mWLFnSfeH6DB6dzLVsqM+jE0bgsuEokFKhQgW3rQWlYIz3WRQEUBBHPzb6oVOjo3v37jlO4NofVLEMbFCqHN977z1XeQour0iefPLJbOvQAfDss8+6/UaVYu+g123UqFH+BpS33+kk7z2vg04NNo+Wufzyy0OWtW59+vTJ07YG0olNB6zXwFPgbNy4cW4f1nGk71H7+2233eYqCWrcBdNy+uHxGpi66fhQZUUnC5WzbnovVc6qVavmX04VazUwAwN1wVauXOm7/vrr3fKqtOhElduPsd5XP4CB+6j2NVUwvX1TP3JqkKjhG7iMtlv7TvD5QsdW4HGl41Zlp2Mx8EfNW/eKFSt8PXv29C9/7bXXuh+o/fv3h91ufY9qFOvYU1BG+6fKX8ek9is1mmNJ69c5ztvGM844w7dmzRq3Hd7x4TXKdFOlXvuzykL0uVWm3vONGjXyfffdd9k+o/YlnRf1vH7wP/nkE/c6nWNOPfVUt98EB109OoYCgw9qUGhdChRqu/R+3nNqAHk/5jJ79mx/JSz4puNT31vwfvziiy/6l2nQoIHbRu+zFtTo0aP97x3ut0jBXe/9TznlFHcuUkBGjUQFk1u3bp3tM8ZzH9J3rfLNjRrQzZs3D3t+Cr5NmzbNF0vz58/PdjEg0k1loopeIo673AIPovUq+KBzySOPPOKOPX3f+t1Sw6B69eoRg0z5FavvzLt4MHjwYLePKmih41DH6fjx4/O8XbkFHkSNVH3fOi+ocq7PomC/glQ6ZlSWasTGk84J+q68yr5uOtdrW3Tuz42Oa68+F21QKdrAg25nnnmmKw/9Tup80apVKxewUV3D8/rrr2f7jrWfq8Gk34PXXnst2++cGmjh6hh6XL/j+u3U95AbBYADz+mBN/0Wx6o8RMFlrVfHkRrJalTpt131GO2nqjNH+j2Opt7nNTK1LrUzVHfT/qfAt/bnwP1B5/3g+p5+x7xyU11P6/AaxropmBpYH8yNGsLea/X5Vd/SOVJ1Wu0Taj949X3th6oriZ5XI1nfo57X51IQXI97dIzrvBTu3Proo4/6CsprdOq3P5Jrrrkm2/vrc6leoODXTTfd5H9c7YIHHnjAvUbfxUcffeT/zrTPf/bZZ/7PqPL/8ccf/cFNL8AZXL9TuQTWLbU/q16iY2XgwIHuwqv3nNpw77//fr63WVQ/8gJ2Orfqe7344ovztM2q93oXGvW7rnqu9jvV57Qu1dlVFoF27tzpe+qpp/zrVV1N7+XViXQsBV7k1oU+NfyjOaa8wIO+ixtvvNEFinW+Uttd34va4sHnUm1rYDsj8KZy0WuDRSq7mAYedNINtWH6YsNRZSPSD68aY4HUgNDVEX3Z+jAqjFtvvdXfoAxFO/fIkSPd1TntbDoxqQAUKAhHlZCLLrrIBVG0k+qkohOTKPCg3hXaab0Tl06u0X4G0YlIjWIdMPoMumrt7TT6fDoAFCH1InqB1KDQ9uuLVBmol4Q+X6RGZCAdMLqaFCrilR8qF11p1+dQ+eqqtCoi6okSGDgKVckPdQsVqY70WdQ41gGj93/uuefc4zoRh1u/GjXqFRHu+WA6qNT413vopKdAlk4K+Q06BB/Q+u51klSDS+WnHyZVKBTYUCUl1PsE9soJd1NjbuLEibkuF+rKqoJT4ZaPdEUrMJgQ6qagQG7bE+p4UcVSlZjgZfU9BgoXZNQtmiu+asxrHToutR/r6v7vv//uiyU1EsJt44MPPugq+OGe1xWUwIpu8G3YsGHZ3ks/gnfffbdrOKtio3OZegDoxzW3yqoagOpVpdeoPHRVzOt1o2Otd+/ertdGKKrEKrDYuHFj/4+Oeo+FanxqXw/1WdTLJxZUXl4wPNy+q/1dn0e/DarM6PPqx1wVf51T8hIEKcg+pOVUkYz0m+nR+SK3YymwYpKfq4uRfh+9Mo3mpvNboo67aAIP3gUABSWbNWvmD/7r/KWrS6F+d2Mhlt+ZLpromFQARedGnQt1bsmPaAIPXiVawXA1qPVd6fdK5ac6i37L4k3nhHDlldu2y5AhQ/zBilgJdT5WhVt1VO1foS4GqXGnAJT2O+33+p33grAKuunY0jkgt/OOrjxG6uUYTOdr1WV1ntN5WefeoUOH5qmBHQ3VLbWP6JjS+VT751FHHeUaOuGC3Xmlz6LfFK/epPuzzjorZM/QSHVO1Qe9/T/ULbghGY5+89RLSceFvkOv11i4fVZlIqqLR7M/qzGo3mgKnqhcFUjX1fJYBZS1/eo5HKonSCC1M7Rvalm1U/SZvXqAylKfR+euwN5MJ598csjPqH1fVG8JV/7B7Rq123Su0z7sHT9eW0Y9Q3SxUz2NCrrNgceqXqcAgdbhNcrzss06jnWsdujQwbUjdM5WkEsB3FABv9p/Bu5C1YkUQAv3vjq+og08BN50kVvtY12kCRe80DlCwThdDNfxpv1X9aNIvebDlV00iul/eRk/guSm3ALKQK35tmOV9RoAkpWmrVOOAeUeCkzwCqDo0LS4CxcudHlFvLwHAIDkQuAhzShztRJ23XvvvYW9KQAQd8oo3aJFCzdPthKUxnvKZADJRUmS27Zt6xIOahYIAEByIvCQRjS/dO/evV1m3ETMZQsAyUAzRWh2hOeff75QppwEUHg0K9gPP/zgpo3T1OIAgOSUr+k0Ufg0JYqmxtN8yZrmTF2MVeHWvOEEHQAUJcccc4yb61xTh3lT6gFIf5pSePbs2fbuu+8SdACAJEePhxQ1d+7cHHNVax7k1157zT/nMAAUJZpP+6mnnnJz2lepUqWwNwdAHCng0K9fP5s4caK1bt26sDcHAJALAg8pSl/b1VdfbS+++KLVq1fPrrzySvfvEiVKFPamASlNAb3ly5fn67XqdfToo4/GfJvSUY0aNfL92ltuucXdQvn888/tgQcesJEjR1rz5s0LsIUAktUTTzxhM2fOdD2dqlevHnIZzuXZKQGvbvm1YcOGmG4PgKKHwAMABFi/fr1lZmbm67Vly5a1ypUrx3yb0lFBhkRoiJlu4ezfv98WLVpkbdq0yfd7AEheX3zxhXXu3DniMpzLs9uxY4e75VedOnViuj0Aih4CDwAAAAAAIG5ILgkAAAAAAOKGwAMAAAAAAIgbAg8AAAAAACBuCDwAAAAAAIC4IfAAAAAAAADihsADAAAAAACIGwIPAAAAAAAgbgg8AAAAAACAuCHwAAAAAAAA4obAAwAAAAAAiBsCDwAAAAAAIG4IPABpZOrUqda5c2d7/vnn8/X6NWvW2BVXXGFNmjSxxo0bW58+fWz58uUx304AAAAARQeBByANvPbaa9axY0c788wzbdasWflax9KlS619+/a2ZcsWW7hwof3yyy9Wt25d99hPP/0U820GAAAAUDQQeADSgIIDM2bMsGbNmuXr9ZmZmdarVy/bt2+fPffcc1a2bFnLyMiw0aNHW5kyZax37962f//+mG83AAAAgPRH4AFIAxoaUbp0aTvyyCPz9fqJEyfaN99844IP5cuX9z+u4EPfvn3tu+++s2effTaGWwwAAACgqCDwAKQR9U7IjwkTJrh75YcI1qlTJ3f/9NNPF3DrAAAAABRFBB6ANFKsWLE8v2bXrl32ySef+HtOBDviiCPc/dy5c23r1q0x2EoAAAAARQmBB6CI++GHH2zPnj3u7/r16+d4vkqVKu7e5/PZ/PnzE759AAAAAFJbicLeAACFa/369TmCDIEqV67s/3vDhg0h17F3715382RlZdmmTZusevXq+eqFAQAAEk8XGbZv3+5mtSpenOuTAGKHwANQxG3cuNH/d7ly5XI8H1jx8HpGBBs5cqSNGDEiTlsIAAASacWKFSF7QQJAfhF4AIq4UqVKZbvSEUxTbHqqVasWch1Dhw61wYMH+/+tXBANGza0pUuXhuxFgdhR7xL1RKlRowZXpxKA8k4cyjpxKOvESfay3rZtmzVq1MgqVqxY2JsCIM0QeACKuDp16vj/3rlzZ7ahFbJlyxb/36oohaKpPHULpqADgYf4V2IVHFI5J2MlNt1Q3olDWScOZZ04yV7W3jYxTBJArCXfGQ9AQrVq1cpfwVi1alWO59euXevvGdGiRYuEbx8AAACA1EbgASjiqlatah06dHB/L1y4MMfzv/zyi7vv0qWLlS9fPuHbBwAAACC1EXgAYAMHDnT3M2bMyPHcrFmz3P0FF1yQ8O0CAAAAkPoIPABp5MCBA+4+MzMz5PPTp0+3jh072iOPPJLt8YsvvtiOOOIIe+2117LNXKFxqK+88oobjnHRRRfFeesBAAAApCMCD0Ca2L17t3333Xfu7y+//DLkMmPGjLHZs2fbsGHDsj1esmRJe/nll13gQrNT6H7Xrl12+eWXu0RYr7/+ulsGAAAAAPKKwAOQBs4//3w348SCBQvcv5955hmrXr26Pfnkk9mW69u3r5si69JLL82xDvVq0LAKJZNs1qyZtW3b1mXdnj9/vjVv3jxhnwUAAABAeinm8/l8hb0RANKL5gHXtJybN29mOs04U4+UdevWWa1atZJyarZ0Q3knDmWdOJR14iR7WXu/31u3brVKlSoV9uYASCPJd8YDAAAAAABpo0RhbwAAhKLOWPv373dXhxCeykflpKSgyXj1LN1Q3qlR1lpeeWmKFSsWt+0DAADRI/AAIKloRo4NGzbY9u3bXaMDuQdo1EBTedHIij/KO3XKWoEH5bRR/puMjIy4bCMAAIgOgQcASRV0WLFihe3du9eNMa1QoYJrMNDAi9w40ywkJUqUoJwSgPJO/rLW63Qu2bFjh23ZssXN+NOgQQOCDwAAFCICDwCShno6KOjQsGFDK1u2bGFvTkqgIZxYlHfqlLUClwpgLl++3J1bateuHZftBAAAuWOAKoCkaWSoS7UaCgQdAMSCziXKzK9zC5N4AQBQeAg8AEgKyuegm65SAkCsKM+Dd34BAACFg8ADgKTgzV7BOGwAseSdU5ghBwCAwkPgAUBSYdw8gFjinAIAQOEj8AAAAAAAAOKGwAMAAAAAAIgbAg8AAAAAACBuCDwAAAAAAIC4IfAAAGninXfeseuvv97Kly/vEurpVrx4catTp441btzYatWqZQ0bNrTTTjvNnnnmGduxY0dhbzKS3C+//GLnn3++23+aNGliV1xxhW3atCnP69m4caNdd911bj3aB+vWrWu9e/e2RYsWhX3Nf//7X+vWrZs1bdrUatSo4fbb2bNnF/ATAQCAwkDgAQDSxJlnnmkPP/yw3Xffff7H1Ehcs2aNLV261N2/8sortmfPHvv73/9ubdq0idjwizVNZzhz5syEvR8KZs6cOda+fXs76KCDXABC+4r2p06dOtnatWujXs/69eutY8eO9s0339gnn3xiy5cvt59//tlKlChhRx99dMhgwvDhw+3yyy+3W2+91ZYsWeL232rVqtlxxx1nkydPjvEnBQAA8UbgAQDSzKGHHur/u0KFCv6/1fuhc+fO9uGHH9opp5xiv/76q51zzjm2b9++hGzXG2+84d4byW/79u3Wo0cPa9CggY0ZM8YyMjKsTJky9vTTT9vKlStdUCBad999t9vXXnvtNWvUqJF/v9S6FHy49tprsy0/depU95p77rnH9XiQihUr2vPPP2/NmjWzSy+91AUiAABA6iDwAABppmTJkhGfV2Nv1KhR7u/Fixfbu+++G/dtUm+LG2+8Me7vg9hQz5kVK1bYJZdc4gJWnipVqrieNdpn3nvvvajWNW3aNKtZs6bVq1cv2+MaEqQg2YIFC7I9/q9//cvdn3766Tn266uuusoFRbxlAABAaiDwAABF0OGHH+7/W93o42nDhg2usaqGLFLDhAkT3L16yATTUAtRj4VoKMCg4RaheikoiNC2bVv/v3fu3GlfffWV+1t5IIJ5PSAmTZpkmZmZUX8eAABQuAg8AEARpHH2HiX7C7Zr1y7X3b1du3ZWu3ZtN85fV5uDEwv6fD574oknrHXr1q5bvq6OK6ml15hctmyZG86hrvbyyCOP2CGHHOJu6gURzfAMjes/4ogj3NV25aXQ1Xi9byjqzn/88ce7LvlquEZKSPjxxx9b9+7d3bJKwNmlSxf74IMP3HNq1Or1XpLOgw8+2P+66dOnu23xnrvrrrv8z2nYyrhx46xVq1ZuaICCLV27dnXLv/zyy3H7XOpR4G2PbpUqVXLbGZh4tFy5cv6Eo4HPBdN39eOPP7q/lVAymLZZlK8hGmeddZb7XFdffbXL8+FRzgi9l9f7RrZs2eJfZuvWrTnW5Q3V2LZtm8sTAQAAUoQPQMrbu3evb+TIkb5DDz3U16RJE1+XLl18n376aZ7X89xzz/k6dOjga9y4sa9mzZq+nj17+n788cc8r2fr1q1qPfk2b94c9Wt2797tW7RokbsPJysry7dz7/60uukzFYRev2/fvmzrmT59uit/3fbv3x/ydVdddZV7vmHDhr49e/Zke07f21FHHeUbPny4e73WP3ToULd8q1atfDt27PAv+9BDD/latGjhW7Vqlfv3smXLfJ06dfK1adMm2zq1Lr1e99G699573WteffVV9+8NGzb4jj76aPfYf/7znxzLX3HFFb6WLVv6fv75Z/fvJUuW+CpUqOArWbKk76OPPsqx7nr16vm++uor9+/169f7GjRo4Nb9/PPP+5fT++ixRo0aZStv3bp3757tM02bNs3Xtm1bf9k/+eSTvnbt2vnKlCnj/n3cccfF7XPpmDv22GPdOvT4li1bcqxnypQpvoyMDN8XX3wRsdxff/11t54SJUr4MjMzczw/Z84c/2f87bfffLnZuXOnv1x69OjhjnE9dsopp/jefffdbMtqXyxVqpRbdurUqTn27V27dvnfe8aMGb5YnVuKOn3Pq1evDvl9o2iVtff7rXsAiKUShR34AFAwe/fudVc/lWVeift09VrdkNUlWd2le/Xqles6dDWyX79+7grmm2++aUcddZStW7fOLrzwQpd1XmO5Q3W5TrTd+zOt5Z3vWzpZdHd3K1cqfqdidV2vXLmy/3vWleyxY8fa448/bocddpi9/fbbVrp06Wyv0ZScmnoz8Eq+xtRresPvv//ezZoxYsQIfw+Gc8891/WI8K5Ia7/TVIkF5c3O4e3D1atXd1fNL7vsMpdjQDNzePR5nnrqKZs1a5brFeBdrT/22GPt/fffd70JTj75ZH/ywttuu80mTpxoHTp0cI9pukblFNA6lExRCQzFe00wb0YGrdtz4okn2ty5c92xou3QunTTdqu8vM8Rj8+lHg6aIlW9Efbv3+96BHjfu0c9BPQZjznmmIjlrmERonUG5nfwBK5Xw2hC9ZgJpJ4W6kly6qmnup4eOlfpszz77LOul0wg7YtnnHGGm7lC+6mSoAZSYktPqVKlIr4vAABIHgy1AFLckCFDXLdpde/2GgBq0PTs2dMFE6LJ/q7GzQsvvOC6zCvoIGp4vv76665y36dPH9cFGqlHCf0aN27s9g0l52vZsqVr0GlowbBhw/xd1z2rV692gQPNaBBIXfS9LvbaLzwKUOnfgdMrqmF80kknFXjbNRxDQxD03p769evn6IavYREaFqK8FV7+gcDjQw3z888/P9tUjZolQft1IDX+NfRCx40nVMPbo5keQvGGJyj4p+CE/q3jS3ku4vm5FEjyPpOCFcFeeumlqGaj2Lhxoz9gEEpgmWhq1mhoKIgCoipjDb/R+UqfIdTrH3roIbffKghz0003uSCKhl989tlnNnDgQP9ywfsuAABIXvR4AFKYKvBqRKox6V259Vx88cXuiu7QoUPtlVdeCbsOXQW/99573RXcv/3tbzmubKoRNnr0aHvsscfs9ttvt8JUtmSG6yGQTvSZ4kkBI323okbeF1984RqlyhmgfUTj66dMmeKCEzJjxgx/g1ffe6AdO3a4q/NqCAZe5dfrleNBPST69+/vglXeVf2C8JIMiq7iqzfOk08+6f4dmCvgm2++cYGPUFfytX26BQZKtLxyUAQ2/EWBlWhnaojEK28dl4n6XJ6bb77ZBY4URNSxr8SOop4Y6i3gBT8i8XoShMs3ETj9arVq1Swa8+bNs//85z8uyKntUk8GnZ9WrVrlAgyBQQ4Fyb7++mvXS0S9U/S8vhv11lDATJRzw/sbAAAkP3o8ACns1VdftQMHDoQcBtGxY0d3ry7L3hXMUNT1XpV/9XAIdQXXyyIfKXiRKGooalhCOt2CG7/xVKZMGdcTQfuNFxhYuHChXXDBBTmSTio4oX0j8Pb777+7rvWBs1NoOV2NV4NeySd1NV9X98M1WvNCV9YVLPn3v//tut9rBoRQU3IqACfRzHKQl2XjJR6fy6OeFAogKgmohl54NKxBU2N6QZFIvAa9humEEtj7SUNUcqMpW9U747rrrnP/Vm+GTz/91PXQ0P0NN9wQchsUpPjhhx/cEBEFZzQUxUv+GdxbBQAAJDcCD0AK09XAcJnndSVSFXxdnZw5c2bYdXizFARexQ7kdWdWBnrNdID0oIau18vhyy+/tCVLlri/FciSaGcMUG4H7V9q2OpKtYISaiBqCEDg1fv80FV6NaRFPREGDBhgFSpUyLGc9z7eZ4jEW1ZDkGIRHEmWzxXc60EefPBB930qyKEZNaIZZiHqveIFGEId896wGp1fogk8XHvttW7mDgUaAode6PylPBLad6KZalWzhCiIqgCaF8QAAACpgcADkMLUgAkcHx5MlX2vm3M4ajx43eh1dTGY1zjTfaSeE0gtuurevn37HAEoL0mkhmKEM23atBzrUqNWwQoloVQuCb0+cPrIvNJUm7pyr0awcgFEyrXg7f8Kjin5ZbhGqxry3rLa39WFPxQl3PT29Vj3SInX5wrupaShJL/99pvr3aJeT2r0t2jRIqpt1LZ5+4HeO9gvv/zi7pXUNjcqZ/VSUI+qYAqYar9Rjw4NK4lEgVHlJBEN6dGUogAAIHUQeABSlK5iqlIfGGAIl31e3ePD0VjpI4880v2tPA7Boskir5k11DAIvIkaRHm5KbjBLW83CbwPfDz4seCbhkeIAgWaZUCPHXfcce4x5YLQjBWhXvPcc8/5/60ZGLy/tX9o/L6XG0I9IfKyPYE3JSL0ZkwI9ZkD9xclRPV6DNxzzz05ltfVdDV+FURQoK1p06ZuWQVJ1CMgcFkld3z66addjyH9W1fXZfPmzTnK2xtyoP0/2rKP1+cKfk5JGeX+++93PQqUqyUv+5RydYiGQgQ/rxk2pG/fvrmuSz2udK9hOqGe19Acbx8Mt297+5n2vbPPPtt9tvwcK3k9HxW1G2VEWXs3AIgHkksCKSqw90Fu2edzyzyvhskJJ5zgEtxpaIW6RqshoOn67rjjDv97hOtWPXLkSP/0isHT8gUmootESfZU4VFD0Ovuj9ypAuvlAPCuzgd2jw9XlgoKaJYA0dAIJSHUsvr+lYDwnXfecdNq6kq7hgLou58/f75LMOo12L3eAYMHD/Y3IMULXuiqubecN2Wnti2a79dbRlfsr7nmGtdQVw8fJb309i3t10psqek8NdvBAw884HpaKGig3gRVq1Z1DWcNK1EQxVunPpe66qsBrWk/lVxVvQuU0FDLal3esgrqqWwUTFPuCuXD0N+actNLRKkcBl4AQ9+B91oFBoM/azw/V6DzzjvPrVvfmbZfr8/LcaVzgM4LL774YrZhDQqaqKeIkkMef/zx2dapoJPKVAlvNROHaPaQLl26uKSl2mZNAxpIy2vf0uPB2+cFLrQt2v7/+7//c9uT18aR1qvldc7UeQ05qXwUdFOZR+qFg/Qva+WcAYC48AFISevWrdMlQXf78MMPQy7ToUMH9/yQIUNyXd/PP//su+iii3yNGjXyHX744b7evXv7Jk6c6OvSpYtbxwknnBD2tXv27PFt3brVf1uxYoV7zcaNG32ZmZlR3Xbu3OlbuHChb9euXb6srCxuebjt3bs3278fe+wx/77x22+/ZXtu8+bNvrFjx/oqVarknj/xxBNd2Qcus3r1al+zZs386wi8XXnlldmWLV++vFt2xowZ7t/79u3zDRo0yFe/fn3f2rVr/ctNmTLFvV77k77vN954wzd//vywn2nBggW+jIwM95pSpUr56tWr5zv44IN9L730kn9bateu7fv000/d8tpvjj322JDbfNNNN2Vbt96/Z8+eIZft06ePez5weR0X3vPajrJly/qGDRvmu/POO/2Pn3zyyb4ffvjBt2PHDt8RRxzhfyx4f47n5wq+PfDAA265yy67LF/71UcffeQ+6z//+U9XJuvXr/d169bNd9hhh/nWrFmTbdnA89HVV1+d7TmVS61atXxNmjTxffXVV+6x/fv3+x5++GFf6dKlff/9739zvPfu3bt9r732mu/oo4/2lSlTxjdy5Mgc30u0N5Whzi3az6M9HxW1m76PVatWufvC3pZ0v8WirFdt3unbtWef78CBA+62YuMOd+89r+fGf7HUt2TdNt/vm3Zke+3efft9P6/emm35P7brgG/zjj3uN0LHsX7LASCWiul/8QlpAIgnXeVWLwRdEXzrrbdcF+RgzZs3d+Pu1d3a63qdF7q6qezyei9Nzzdo0KCoXqcrwhrmoe7p4YaBBNNVXiX8U8JDr3s7cqdTuK7oaraCjz76yKZPn+6GzARetdJVZ10l15U2DQ/QFeZWrVq5BJC6gh8qj4FyPugq/BtvvOG6uOt7ufLKK93V78DlNRTAm/1A37X2F129/uc//5ltukNt59VXX+16DWjq11tuuSXXHAEvvfSS3Xnnne79NfuDPpeu+utKu4YZ6Mr6WWed5V9+9+7drvfC+PHjXS4F5TVQosWLLroox7q1Tz/88MNuVg7NHqEhR5qVQ1fXg69C6uqken3873//s9q1a7ueBioL9fJRr4fbbrvNXY1XWV166aXZepyot4GGVwR+1nh+rkDqcaH1KieH1wslr9QLRJ9PvTrUa6VXr17uu9M+FUjf7znnnGOff/65vf766zmm+tTn0nAR9ZZQ2Wt/1cw7ytvgDfXynHrqqa5HjnJAKB+GeoZ4SW7zg3NL7nRu0P6oXBzJeBW+qJS1VyUfN3OZ3f3OosLZvr27bMVDvd15T8lfASBWCDwAKUwVdiWO1LRzaggFU0NQlQeNA1fX6LzyutWrsaWKe9myZaN6HYGHwgk8JHJqzqIqlcpbjXclb/zpp5+sKJc155bcEXhInF1799t3S1ba/xbvsAlfLbcDWclVDSfwACBeyPEApLDu3bu7wMPChQtD9lZQxUFXXLt27ZrndesqsMaxi64MRxt0AJAcnnnmmain0AQQe5lZPvvu9y127uNfFPamAEChI/AApDBlntcwCiVuC+Zlnu/Ro0fY2SjC0fANdePWlUJ1c+7Tp0/MthlA/Gk2mjfffDNlezsAyW7n3gP209rtllGsmC1et8PObH2QPfnpEnt1zgpbvTVyQmcAKIoIPAApTFMgKuu9ZqNQz4e2bdv6n9NYevVSGD58uP8xjf+/9dZb7cILL8yWqT6QxpNr3L+6aWv4hno7AEhua9euddNbqoeTZiXRlKDKNxGYZwNA3ixeu900EmLByq22PzPLPl+8waYuWB1y2ZsmzU/49gFAKiHwAKS40aNH25w5c1ziRyVuUxLBRx991E3HN2HCBJegzaOhE7Nnz7ZFixblCDwoGaGS440aNcolsBs3bpybZhFA8tPUqAosiqZCPeKII1wyRwDhKZiw70CWlS9dwuYu32yf/rzevl+51T76YV1hbxoApB0CD0CK0xVONTjuuOMOa9++vUsMphkLFIxo3bp1tmV1RVTDMi655JJsj7ds2dJ1zdYsGBpioSCGZj4AkDr5XjRzxffff+9mxFCQUQlegaJq4469LilpiYxiNvGr5XZyi9p277s/2LQfCSoAQGFgVgsAMcesFomTSrMspAPKO3GY1SJx0mFWiwOZWTZ+1m8u38LE2csLe3NSFrNaAIgXejwAAAAg5Wzeuc/KlsqwxWt32P899nlhbw4AIAICDwAAAEgJS9bvcFNU3vAqyRwBIJUQeAAAAEBS2rM/095fuMYlfJwyf1Vhbw4AIJ8IPAAAAKDQc3r8d/4qO7JBVfv4x7U2Ysqiwt4kAEAMEXgAkFTIdwsgljinJKdnPvvV/jn1h8LeDABAghB4AJAUvEzqmZmZhb0pANKId05J1dkaUt2cZZvss5/X26Zd++ylL5ltAgCKKgIPAJJCyZIl3W3Hjh1WoUKFwt4cAGli+/bt/vML4t+7RLkYhrzxnR3ZoIod1aiq3f/+T4W9WQCAJEDgAUBSKFasmFWsWNG2bNlilStXtrJlyxb2JgFIcbt377Zt27ZZlSpV3DkGsff2vJV2/Svzcjz+8Y/r3A0AACHwACBp1KhRwzUUli9fbpUqVXKBiIyMDBoMuVxhPHDggJUoUYJySgDKO/nLWq/T8Ar1dFDQoXTp0u7cgoI5kJllJTKK2xdLNtitbyyw5Zt2FfYmAQBSCIEHAElDQYYGDRrYhg0bXKNBvR+QeyMrKyvLjV+nIRx/lHfqlLWGVqing4IOOrcg7+WvctdME9dNnFvYmwMASHEEHgAkFTUQateubbVq1bL9+/e7hgfCU/ls3LjRqlevTvK8BKC8U6OstbwCDwSHorN3f6a9u2ij7flhu933/s+WTmpXKm1rt+0t7M0AgCKPwAOApKQGQ6lSpQp7M1KicaYGVpkyZWgIJwDlnTiUdXxkZvnsnncW2UGVy1idymWsTf0qdsLoTywd/evcVm4mjXCBh9b1K9t3v2/N9/r7dmhoC1Zuse9XbivAVgJA0UDgAQAAII3t2Z9p/5y6qMhNZ1kqo7gdXL2c/bA6dGCgZoXSVq18Kdu0c1/E9fxw96k28MWv7bPFG7I9/q9zWtmIKQsJPABAFLiEAAAAkAY27NhrG3f8cXV/f2aWm3Gi91Oz7LA73ku5oEOnJtXCPnf+0Q1yPDZ/+N9C9py7++xWdt5R9eyiTg1Drmv6TSfkui1lS2WYz2cxc/9ZTe3cI+vGboUAkALo8QAAAJDiVmzaZcffN939Xb18KatctqT9umGnpar/XNLeWt/1QcjnQuXu0OcNpWbF0vZA77ZuiImWOfrgajbopW9sz/4s63JozRyvq1GhtAvg5BYUqVK2lBUvnr8cIsc3qWI9Oh1qM3/ZaOu2k38CQNFAjwcAAIAUsG77Hjvz0c+s+4MzbOLs5bZ47Xb3uBrKj037xb/cxp37kj7oULFM9mtfr11xjNWtXMb/70pl/goIPHnRURHXderhddz9F7eeZO9ce1zIZTKKF7Obux9mJzSvZZ/cdKKNveAou6hTI/fcCc1ruvtbTm1uc4adnOu2vzLwGHvy4nZWUM9c2t4a1yhv/wlY1w3dDi3wegEgGdHjAQAAIMnoCr0ay1lZPnvi0yV2//s/ZXt+6JsL3P0R9SrbgpX5T5BYWB674Ci79LnZ/n+r80DX5rVcQCWwET5vxWbr1qJ2tteWzMje02B07zbuvm6Vsu7mCdcfQUk1z2h9kP/fT17Uzhat3mZt61dxvSkUlLj65W+zvcZnBRtroeEhr8xZYc9c8leQoXX9Kv6hHu//o4t9/ssGu+SYRvbgR+k1swgACIEHAACAJLHvQJbNXrrJLnr2q6iWT5WgQ4uDKtk5bevaqPd+tJIZxe3og6vaIbUq2C/rdoQdKnF9t2b+v49pUt1m/brRurWoZdeceIiNn/Wb/7kKpQtWnS1TMsOOaljV/28FJX5e28we/nixxcqIsw+3UT1au9la1q1bl+P55nUquhsApCsCDwAAAIU868Skr1fYI9N+sS279tn+zBhmMkwC7/3jeDusTiX394Djm7jEl2rs+wIyNjarHbnR/WL/DrZhxz7XW0HevKqz3fbmAnuxf0dLNuVKZdiufZnZHisZgylhzzjiILvu5GYuKLWe3BAAUgw5HoA0sG/fPhs1apQ1b97cmjZtal27drUZM2bkeT3jxo2zDh062EEHHeRuHTt2tPHjx8dlmwGgqFBDWzf1Zrjz7e+t37jZLhnkkNe/s84jP7arJnxrd7y90DUmUyXoULF0CRvT648hDpEoQOAFHUTDRxR0kMuObezuleQxNyUyivuDDqIeCu/9o4tLHhnssD97DnT9M3dDfgR/C+Fmtbiia1MrX+qPz+O57fQW/r+/H9HdTceZ30SUgZRTU70iZt+Wex4KAEg29HgAUtzevXvttNNOs7Vr19qHH35oDRs2tEmTJlm3bt1swoQJ1qtXr6jWc91119lzzz3nXnP22We7K1Faz4UXXmjfffedjR49Ou6fBQDSybiZS23czGW2e3+mS6b4z3Na+YcIeDNQyKqte+K6HQ2qlbVnLjnauj+U94B0OFk+n/VoV9/OObKezfh5vfV7fk7I5QKHMAS7qGNDO7JBFWtWu8Kfj8Qm6KIEk3sOZBV4CEag8mHWpZwS393V3fo+/aUbIiOBk27EchvqVS0bdlYPAEh29HgAUtyQIUNs+vTprreCgg6iYEPPnj2tX79+tnTp0lzX8c0339ijjz5qw4YNc0EHr2LTu3dvu+SSS2zMmDG2aNGiuH8WAEh2BzKzbPPOfTke/3rZJrvrvwut//Nz7F9TF9nBt061EVMW2fJNu1xPhl/X77Qlf+YzSLRz2taLWf6Ajo2ruXsFHbweDJoV4oXLO+R5XfqdaVWvspUukb3HQEGpd0RBG/ydm1bP9u+7zjrcWtWrFLKXh8og3o47pIZde1KzbP/2LBzR3f+9AECyoscDkMKWLVtmY8eOtZYtW7ohEoEuvvhimzhxog0dOtReeeWViOuZNm2au2/btm2O54466ijXE+L777937wMARcW3yzdbrYqlrV6Vsjbyfz/aJz+ts5/X/hE8uKzzwa5xe2qrOvbIx4vtg0Vr/a/7+MfQ69NwinirXr6Um04zmmEC+fH0pe3ti182uGkpAwMIXQ+taT2Oqm9vfPu7pYNOTaq7KT4Prl7O/Vv7wDvXHh92ee0P6vGggECxsPNp5M/gUw51uR0CHdeshpsFI1JvDABIJpypgBT26quv2oEDB6xz5845nlN+Bpk8ebJt3LjRqlfPfvUmUPny5d39V1995YZtBNq+fburVLZpk/tYXgBINb9t3GlPfrrErux6iO09kGkVypRwPRfeX/hXICGU579Y5u4fm/5LzLZFQY51+UwaqKvxavAqABC8TZd0buTuH+zTxm54dX6BtrFSmZJ2aqu/pqIMNKZ3G+vUpJrd/Pp3lg465KEXwelHHOSmxqxftazL5TF2+i927CHhf3ejVaJ4Mbv2pENyXU4zhQBAMuMsBaSwqVOnuvsmTZrkeK5atWpWr149l3hy5syZEddzxhlnWEZGhj3wwAP288/Z5w9X4GLAgAEucSUApLJVW3ZbZpbPFq/dbo9/8otL+Ki8BxNnr7Au90+3Ux6cYceMnJZr0CFeZtxyomu4R6tKuZJ2Rdcm1rNdfZtyzXE25drjrHSJnFW7WhX/SMp47pF/DI+Ip17tG7jgR37EsmdGYWhco7wLAKgHwme3nGj39SxYwP7Q2hVccspQOR2Cy+rusw8v0HsBQLwReABS2Ny5c919/fqhK5NVqlRx9/PmzYu4nkaNGtndd9/tejeceOKJNn/+H1fE7r//fjv66KPtiSeeiPm2A0AifLFkgwsyPPzRYus8apo1ve1dF2C4772frNmw/9me/VmWLDTbQ78/Z3oIVq18qRyPFS9WzIae1sJG92rjb5zuy8zb51EDOVp9O/yRRyg39/dq7ZZVkse8SESuhESJxSwW5UqV8M8AEirIEahJzQp29MHhE3kCQGFjqAWQovbs2WM7duzIFmAIVrlyZXe/YcMf40Ajue2229w677nnHuvSpYv179/fDa+4+eabo5pZQzfPtm3b3H1WVpa7IX5UvpqBhHJODMo7ucr61w073dCBulXK2G2nHWYNqv0xHt/z5a8b7YJnZufpfU9sXtNWbt5tP8cgEeRzl7a3sZ8ssW9+2xzV8vqsge3VlgdVtEWrt7tcEtec2NTufueHbMtr0eDyUS+OQMPPbBGyDF/od7Qd3+yPBIXfDDvJlvy+xnq/EDmJ8L/OOTyqfb9G+VJuWe8zRevaE5vaJz+tt/OPrp+2x1heziGRljulRU2744wWdkT9yv5lslK8xwiA9EbgAUhRytvgKVcue2XbU7z4H52aFFCIxogRI1wwY8WKFfbggw+6nhBHHnmktW7dOuLrRo4c6V4bbP369W6oB+JHFc6tW7e6Cqr3fSN+KO/ElvWWLVts+uLNbkjBwdXKWJkSxW3B6p1WuWwJO6xWORv32e+2YOVWd9PwiGY1yror/h0aVbJyJTPshTlrcn2fnm1q2uvz1/v/PbBDTVu9bZ8Nfjt07oZDapS1XzbsjuozNCp3wMae28SOeeibqCaKXLdune3cttX/76En1bf3ftxkZx1ew7787Y+AbqCGVUq51wQqmfXXOffdga2tWrmS2ZY5/8hatmTjbjukYqb/cZV1RdtjZ7asZu8s+mNKSDm+SWX77Ne/tif4veLhjctaJuy9kv0csn///ojlcEYz/fb/tUyo39sBnQ6yZ75cHYMtB4CCIfAApKhSpf7qdqsKTCheJUT5HnKj4MSgQYNcAEHTcg4ePNgeeughO/744+29996zY445JuxrNXOGlg/s8dCgQQOrWbNm2N4YiF0lVl2sVdY0hOOP8o6vHXsP2PwVW1zPhf4vfG3bdu+z9Tv2+59XI3rL7v3uyu5Jh9W0aT/+FTCQxX8GBH7bnP1xXRkukVHMhv835xX9Pp2a2n8XbnQJAaVxvdqWWWpnjuXqVCpt/9emrg3q2sTa/fPjqD5Po3p1XJf7aIIO6rVRq1Ytq7r1ry4PLQ6ua8cefrD7e+GmP5JZyvjLj7bXvv7dbj21udWqUjbbegadXN0Wb9pvf2tZ2w47uF6O97m311+zUQTv1w9d0Mx6Ldlo178y30ae18re/HZltuW0fUjcOaRNo+p5KvNSJZfkeOy2s9rmCDzc0v1Qu+/97PmcACDeCDwAKUrBBAUfFFzYuTNnJVl0tVBq1Phrvu9QFLjo3bu3my5TvRxEPR6UcHLMmDF29tln2+LFi/1DN4KVLl3a3YKpUkXjLP5UiaWsE4fyjg2ddzbt3OdyF2iKyuk/rbMXZ/1mK7fstkbVy9lvG3fleM2mXX8FIYKDDp429Svb/N//ukovpx1xkEsq6QUe2jSoYgt+3+ICAq0bVHEzNWzY8cdwsSrlS1vZrTl7iR3ZsKoNOyNvUwqXKPHH+PwaFUr71z/z1pPsjre+t2k//nGVumzJDDfrRJ/2Ddw+VazYX/tVqZIZAfvZXwGJLofWcrdQypcpbk9e3D5P2xm4X3dtXtvm3nmK+/eHi7JfbWefT8w55H/XH29T5q+yK09omqcyDxXgCvX6nu0bEHgAkHAEHoAUpaCAAgVKHLlq1aqQy6xd+0dm9tymwtS0nFOmTMmRz0HJJTXLhZ4bO3asywMBAHkNMCxctc3lOVi9dY/1O/ZgW7R6m/37fz/aj2u2W4eDq9ncFZttf+ZfzabAoMNRDavYqB6t7W8Pzsj1vepWLmOvX9nZJs9daXUqlbHPf9lg5UplWN0qZW3XvgP+5WpWKGWf3nyiS2ZYukSGVSidYRv+TOmgZH6lMnIm9DsQMIC+XaOq/rwNY3q1sRsnZZ+i8poTD7EWB1Xy//umvx1qD370sz18/pFWr0pZN0WiR4EXJYj0l1fAekoWUkPfS1Q55LTD7M252Xs9IP607wTuP7HmzXICAIlE4AFIYd27d3eBh4ULF+Z4TgklNY60fPny1rVr14jrefPNN919cJdOVT6VbFKBh9mz85agDQC+Xb7Zrn9lrq3Y9FdOhBk/r7dfN+zwzyYxe9kfOQU6Nq5mB1cvb69+vcK/7ANnH2JntG9qpUuWsCMbVrG5y//oxXVhx4b2t8Pr2OXPz7HzjqxnMxavt7Xb9tr/ta3rpjPs3b6BW65LwLSOmiEgUGAiSk1/GKh0yZwN/sCkjU9cdJT7TApAKLASHHi49uRDXEDDc36Hhtbn6Ab+Br2GfXiygobKBQ6dC1wuxIyKcVe7UhlbNuoM953Vr5p9SAeSD7klASQzAg9ACtPME+qVMGNGziuBs2bNcvc9evTIlg8iUi6I33//3Zo3b57tuWbNmrn73NYBAIG27NpnV730ra3ZtscNJ+jUpJp9/dtm19tBjmlS3W7qfqg9Pn2JtapX2a4/uZlrXKuXgoZblC5R3I5uWNEFEqRB1XL+wEO9qmWt66E17YtbT7Kq5Uq55d9fuMYuDzMVZU7ZW/Hlg4ISpf58z3CBB10x9q4ae8GE3F4fuFyJgJ4MGuYRTmDPiMIUGMBB8gqT7sluObW5PfTh4jxPtQoAscRgPSCFKSgwcOBAW7Bggev5EOiFF16wsmXL2vDhw/2PTZ8+3Tp27GiPPPJItmXPOeccdz9x4sQc7/Hll1/6AxgAEIryJxz4s1GjIMBd/11oFzz9lQs6NK5R3mYPO9nG9etg9557hFumYukSNrp3G2vXqJo9e9nRdsMph7okjGqcdzm0hj9Xgxd0kIYBPRTqVy3nvyJfqkRx9x6DujZ1f0cjuEEffDU/1Hq85JPRCBWMCPf+D/dtG9V6wjUqgdxcdcIhdutphxX2ZgAo4ujxAKS40aNH25w5c9yMFO+++65VrVrVHn30UTc8YsKECdakSRP/skoUqSETixYtsuuuu87/+CWXXGL//e9/7fnnn7dWrVrZ1VdfbSVLlrRvv/3WBTYuvPBCl3wSAAIpz8Fj0xbb7KWbrGKZkja+fwe7esK3tnjdHwkTSmYUs0fOP9I9J5oVolLZkla7UmmX6yCUS4452L5ettnlggjUoNpfy+e3279yLzw3c6nd+LdDsz2uRtmS9Tvswo5/JNdVb4tggTkogn12y4n2/cqtNu6LZda8dsVctyNwCMVhdbKP5Q/3LuVL58w7AQQiNgUgmRF4AFKccjioJ8Mdd9xh7du3dxmsFTxQMKJ169bZlu3bt68blqFAQyC9ZtKkSS6B5Lhx49yUmhUrVrQ6derYkCFDbMCAAblewQNQ9Nz59vcucaTs3JdpZz76uesZUKNCKbu+26HWuWl1a1qzQrbXaIhEJEqq9+Hgrm7awXXr/ppV4aDKAYGHMEGL3NzUvbld361Ztp4UUqtSGXv7muPC9njQv4f/X/gZLZQvQjfNnhGNjAhJIxuECaqcc2Q9e3fBGjv2kOpRvQcQKLDHEAAUBgIPQBpQkOChhx5yt0jUc0G3cLNkqBdEYE8IAAhn/fa9/qDDuH5H2+2Tv3fDLERDKpT8MZY0xaZH01PmV3DQIZTAxJADjmtsN5/aPNtjBaWeIOEcUquiPX7hUVarYvbPqPd/4fIOMdsGpJ+/taxt81f8kQcl2MktatnQ0w6zI+qHnhYbAOKNwAMAAMizzxavd/eH161kJzavZS9cfrQNfm2+dWlWM+ZBB2lUvbw9dsGRVq1cKZcPIp4CAwPqvh7LoIPkNlXi6VH2nAACDezSxKZ+t9qfwDWQei1e0bVpoWwXAAiBBwAAENKe/Zm2cNVW27Rzv+3ad8DN/lCnchkXbNAUi4FDJ3Sl/r8BwxXi4czWdS0R4p3UUdN9btm13830AcSKevOcfkSdkIEHAChsBB4AAIDzxZINNm/FFtuzL9N+WrvdZvy8wXbvz8yxXO/29d20l0VhqkVfHFL2ZRQvZleewNVnAEDRQeABAADYlPmr7NqJc3M8rlwDB1UuY+VLl7Cdew/Y96u22Wtf/+6eK18qw45qWNXSGdNYIpWQCBpAsiLwAABAEbd84y677c0F7u/jm9WwxjXKuwSOJx1Wyw2rCGzMvDhrmd3x9kL39zFNa+SYAQJA4bmoYyO7//2f8vSatg2quJ5OABBPBB4AACjCNP3ltRO/te17D1j7RlVt3GVHW4kIMz9cfMzB9vuW3fbUp79aj6PqJXRbAURWuVzJPL9GgUYCDwDijcADAABFxJL1O2zu8i1WongxUyeGb3/bbB/9sM5Ng1m5bEl7uO+REYMOnqGntbB/nHyolS0V29kekpGPsRZIMYO6NrUnP10S9fJVy5WK6/YAgBB4AAAgze3PzLLHpy+xR6cttgNZORvSytXwUJ+2Vq9K2ajXme5BhxoVStmGHfvslJaxnxoUiKdW9SJP1xrsgo4N7LmZS+O2PQAgBB4AAEhTW3fvd12ox3zwk333+1b/eO4KpUvYgawsO7h6eevWorYde0iNtA8k5NVHg7vaso27XHkBqaSYRZ9g8qJODd1UuAAQbwQeAABIM89+vtRe+vI3W7php/8xDaW4++zD7aw2dcl8H4Uq5UpZW7qgI81VKJ33nBAAkB8EHgAASCNZWT7793s/uqSR0qBaWevUuLrd1L251a5UprA3D0ASIQYJIFEIPAAAkEY27Nzrgg5qUMwZ1s1NiwkAAFCYmHwbAIA0smbrHndfq2Jpgg5AEUQvBgDJiMADAABpZPWfgYc6laOfoQJA0USMAkCiEHgAACANezzUrUw+BwDROe+oeoW9CQDSHIEHAADSyKqtu919HQIPQJGkGWzyOiyjZkWGZQGIL5JLAgCQhj0eDiLwABRJnZtWt8s6H2yH1alY2JsCAH4EHgAASCPkeACKtmLFitldZx0e1bJVy5WK+/YAgBB4AAAgjZDjAUBuHj6/rU37cZ1d1KlRYW8KgCKCHA9AGti3b5+NGjXKmjdvbk2bNrWuXbvajBkz8vT6mjVruqskkW7r16+P6+cAUDA+n88feCDHA4Bwzm5bzx4+/0grUzLD/btf58ZWtmSG9WxXv7A3DUCaoscDkOL27t1rp512mq1du9Y+/PBDa9iwoU2aNMm6detmEyZMsF69euW6jsmTJ9uGDRsiLtOxY0cXnACQvDbu3Gf7MrNcwrhaFQk8AIiOApUL7vqb7dq5wx4s7I0BkJbo8QCkuCFDhtj06dNt3LhxLuggCjb07NnT+vXrZ0uXLs11Hc8884xdf/31Nn/+fFuzZo3r2eDdVq1aZRUrVowqgAGgcHm9HWpUKG2lSvATDyB6JTI4ZwCIH84wQApbtmyZjR071lq2bGkdOnTI9tzFF19sO3futKFDh0Zcx6+//monnXSSPfTQQ9a6dWurXbu21ahRw3+bN2+ebd++ncADkEKJJcnvAAAAkgmBByCFvfrqq3bgwAHr3LlzyKER3jCKjRs3hl1HvXr1XK+JcDRsQ+vyelMASF5rtu529+R3AAAAyYTAA5DCpk6d6u6bNGmS47lq1aq5oIISR86cOTPsOkqXLm3Fi4c+Fezfv9/eeust6927dwy3GkC8rPqzx8NBTKUJAACSCMklgRQ2d+5cd1+/fugs1FWqVLGVK1e64RJnnXVWntf/8ccf25YtW1y+iNwSXOrm2bZtm7vPyspyN8SPylczGVDOiZHs5b16yx89HmpXKp2025guZZ1OKOvESfayTtbtApD6CDwAKWrPnj22Y8cOf4AhlMqVK7v73GasKOgwi5EjR9qIESNyPK7klOpxgfhWErdu3eoqsuF6rqDolPfyDX8E/crbPlu3bp2lsmQv63RCWSdOspe1cjoBQDwQeABSVGDehnLlyoVcxqvUKEiRV8odoWEWw4YNy3VZJbAcPHhwth4PDRo0cNNvhguKIHaV2GLFirmyTsZKbLpJ9vLeuPsHd39og1pWq1Y1S2XJXtbphLJOnGQv6zJlyA8DID4IPAApqlSpUv6/deUkFK+3gfI95GeYxebNm6OazUJ5InQLpkpVMlas0o0qsZR14iRrees84E2nWa9quaTbvnQq63REWSdOMpd1Mm4TgPTA2QVIUQomeMEHTZsZivIziKbFzO8wC/VcAJD8Nu/ab3sP/DE+u1alnIFAAACAwkLgAUhRGRkZ1rJlS/f3qlWrQi6zdu1ad9+mTZt8DbNgNgsgdaz+cyrNGhVKW+kSGYW9OQAAAH4EHoAU1r17d3e/cOHCHM8poaQSWJUvX966du2ap/VOmzbNNm3alOtsFgCShzfM4qDKjNEGAADJhcADkML69+/vxmPOmDEjx3OzZs1y9z169MiWDyLaYRadOnVimAWQQlb/GXioQ+ABAAAkGQIPQApr1qyZDRw40BYsWGDz5s3L9twLL7xgZcuWteHDh/sfmz59usvb8Mgjj0QcZjF58uSokkoCSL6hFvR4AAAAyYbAA5DiRo8ebe3atbNBgwa54RHKbK/AwpQpU2z8+PHWpEkT/7Jjxoyx2bNnR5wiU8EJrYfAA5CaPR4Oqly2sDcFAAAgGwIPQIpTDgcFCzQ0on379q4XhHI0zJkzJ0eOhr59+1rFihXt0ksvzXWYRf369ROw9QBihRwPAAAgWZUo7A0AUHAKJjz00EPuFsmFF17obpH85z//ifHWAUhk4IEcDwAAINnQ4wEAgBSnIVZ/DbUg8AAAAJILgQcAAFLc1t37bff+TPd37UoEHgAAQHIh8AAAQIrzejtUL1/KypTMKOzNAQAAyIbAAwAAKe6737e4+4Oq0NsBAAAkHwIPAACksP2ZWfbY9F/c3//Xum5hbw4AAEAOBB4AAEhhk77+3VZs2m01KpS2S445uLA3BwAAIAcCDwAApKg9+zPt0WmL3d9Xn9jUypYivwMAAEg+BB4AAEhRr8xe7hJLagrNvh0aFvbmAAAAhETgAQCAFLR7X6Y9Nn2J+/uakw5hNgsAAJC0CDwAAJCCXvxymW3YsdcaVCtrvdo1KOzNAQAACIvAAwAAKTiTxX9mLHV/X3dSMytVgp9zAACQvKipAACQYqb9uM71dqhZsbSdc2S9wt4cAACAiAg8AACQYiZ9vcLdn3dUPSuZwU85AABIbtRWAABIIeu27bHpP613f/duT24HAACQ/Ag8AACQQt74dqVlZvmsfaOq1rRmhcLeHAAAgFwReAAAIEX4fD7/MAt6OwAAgFRB4AEAgBTxzW+b7dcNO61cqQw7vfVBhb05AAAAUSHwAABAinjtz94OZxxxkFUoXaKwNwcAACAqBB6AOOjfv39hbwKANLNj7wF757vV7u8+RzPMAgAApA4ulwBxMG7cOKtYsaLdfvvtVqNGjbi/3759++yBBx5w73vgwAGrX7++3XPPPdalS5d8r3Pz5s1ufTNmzLBatWpZ3bp1bdiwYVayZMmYbjuA0Lbs2mcrNu32/3vG4vW2a1+mNalR3to1qlqo2wYAAJAXBB6AOHnllVfsqaeestNOO80uv/xyO/3006148dh3Mtq7d697j7Vr19qHH35oDRs2tEmTJlm3bt1swoQJ1qtXrzyv8+WXX7Z//OMfNnDgQHvppZesQgUy5wOJ7t3Q9f5PbOvu/Tme69W+gRUrVqxQtgsAACA/GGoBxIF6HKxevdp+//13O/HEE13PhwYNGtjQoUNt8eLFMX2vIUOG2PTp013vBAUdRMGGnj17Wr9+/Wzp0qV5Wt9tt93mhoq8+OKL9s9//pOgA1AIfl673QUdShQvZnUrl/HfjmpYxc5nmAUAAEgxBB6AOFi+fLm7Ilm9enW79tprbd68efb222/btm3brGPHjm4IxPjx42337r+6UefHsmXLbOzYsdayZUvr0KFDtucuvvhi27lzpwt2RGvUqFE2cuRIF3To3r17gbYNQP4t37jL3WtIxRdDT/bf3rzqWKtavlRhbx4AAECeEHgAEqR9+/YuSLBq1SoXJFBvhDp16rjhDF9++WW+1vnqq6+6nA6dO3fO8ZwCHDJ58mTbuHFjrut6//33XW+HPn36uN4SAArPb38GHhpVL1fYmwIAAFBgBB6ABPr444/t7LPPtgcffNB8Pp9LCrljxw679NJL7fDDD7dHHnnE9uzZE/X6pk6d6u6bNGmS47lq1apZvXr13HvMnDkz4nr2799v119/vdum4cOH5+OTAYil3zbtdPeNqpcv7E0BAAAoMJJLAnFwzTXX2GOPPeb+zsrKcske77//fps7d65r3FetWtWuvPJKu+6669yMEfLpp5/a6NGj3XAHDcNQcsjcaH1eTolQqlSpYitXrnRDPc4666yw63nttdfsp59+cj0xlIPi7rvvdv9WT4njjjvOzZARKrgRmOBSN4+GlHifXTfEj8pX+xTlnF7l7fV4aFC1bJH9btm3E4eyTpxkL+tk3S4AqY/AAxAHms3iiCOOcA33Z555xn777TdX0WjUqJHdcMMNLnlj+fLZr2R27drV3dTz4IwzzrCPPvrIjj/++LDvoZ4R6i3hBRhCqVy5srvfsGFDxO1VYETWr1/v1vncc89ZRkaGPfzww3bLLbe4YRiaVlO5JEJRXogRI0bkeFzrU48LxLeSuHXrVrd/xWPWFBROeS9bv93dVyy2x9atW2dFEft24lDWiZPsZb19+x/nHgCINQIPQBxkZmbaVVdd5f5W5eLII4+0m2++2c02oQZ9JOoBoaEPWj5S7ofAvA3lyoUeB+5VanIbvqHeFvLAAw/YOeec439c2zB//nw3LadyUnz11VchX68EloMHD87W40GzeNSsWTNsUASxq8QqkanKOhkrsekmEeW9a98B27jrgPu77SH1rXLZklYUsW8nDmWdOMle1mXKlCnsTQCQpgg8AHGigIN6LNx555128sknR/26KVOmuPsff/wx4nKlSpXK9l6heL0NlO8hHM18sWXLFve3ckIEUwBFgYfZs2fbwoULXS6KYKVLl3a3YKpUJWPFKt2oEktZp095/77lj0ChAg5Vy+c8rooS9u3EoawTJ5nLOhm3CUB64OwCxIl6D6gnQV6CDnLSSSe5YRiaYSISBRO84IOCB6F4AYUaNWqEXY+Xj0EqVaqU43nNmOH1Wli0aFGUnwJAfjGjBQAASDcEHoA4uOmmm+wf//hHvl577733ujGWyq0QiYZseDkXNEVnKGvXrnX3bdq0CbseBSV09SU4CBHIS14ZrmcFgNhZ/mfgoWE1Ag8AACA9EHgA4uC+++5z95s3bw45pWa4QEFede/e3d1rCEQwJZRUAiv1nlDSynBKlixprVu3DruewDGfhx56aEy2G0B4yzZ6U2kSeAAAAOmBwAMQp+RRmrlCvQkGDBiQ7bnmzZu7pI2XXHJJyMBEXug9NB5TM04EmzVrlrvv0aNHtnwQoZx//vnu/t133w35/LJly6xp06YRe04AiI3lm/4calEt+8w3AAAAqYrAAxAHTz75pI0bN84NTdi9e3eOYQtK1njgwAHr0qVLgaauatasmQ0cONAWLFhg8+bNy/bcCy+8YGXLlrXhw4f7H5s+fbp17NjRHnnkkWzLXnvttW67Jk+ebL/88ku259555x3Xe+Jf//qXf0gGgPjneGhIjwcAAJAmCDwAcQo8nH322TZx4kT3dyh33XWXG9qgWS8KYvTo0dauXTsbNGiQbdq0yQU7FFjQ7Bjjx4+3Jk2a+JcdM2aMm51i2LBh2dah4RhaXoEK9ZBYvny5e1zbp6CEclb06dOnQNsJIHf7M7Ns5ZY/gpUMtQAAAOmC6TSBOFAAYO7cuS4BZDheQOC1116zBx98MN/vpaCBejLccccd1r59ezf0olWrVjZnzhx/7gZP37593bAMDfMI1rZtW/vyyy/dbBoaUlGrVi03VGTUqFEEHYAEWbVlt2Vm+axUieJWu+IfuVUAAABSHYEHIA4UDIgUdBAFBgKnvCyIihUr2kMPPeRukVx44YXuFo5myXjrrbcKvD0ACjjMolo5K16coU0AACA9MNQCiINOnTq5PA7hrFu3zq644gqXM0E9DQBAfvMnlmSYBQAASB/0eADi4Pbbb3dJHL/44gs384SSQGZmZtqSJUvc0Iqnn37aTXUpwfkWABRdy/+cSpPEkgAAIJ0QeADiQIEGBRh69+4dMrmkEkCWKFHCHnjgATv99NMLZRsBJO9QC3o8AACAdMJQCyBOunXrZt9//73dcMMNdthhh1mZMmWsVKlSLqmkekF88803ds011xT2ZgJIIsu9oRbVyxf2pgAAAMQMPR6AOKpbt66b7lK3YHv27CmUbQKQnNQTygs8MNQCAACkE3o8AIXk448/tquvvtqysrIKe1MAJIH1O/barn2ZVqyYWf2qZQt7cwAAAGKGHg9AHG3fvt0lkQwOLujftWrVsldeecWKFy9ujz76aKFtI4DksPzP/A51K5e10iUiT8cLAACQSgg8AHGwdu1a69mzp5vVIreu1S+++CKBBwB/JZZkmAUAAEgzBB6AOLjtttts5syZLpmkejZs2LDBateunW2Z1atXu6STl19+eaFtJ4Dk8Zs/sSSBBwAAkF4IPABx8MEHH9g999xjt9xyi5UsWdKuvfZau/766+2QQw7xL3P77be75JNXXXVVoW4rgOSwfONOd9+wGjNaAACA9EJySSAODhw4YMOGDXNBBxkwYIA9/fTT2Za56aabXGBi+vTphbSVAJIJPR4AAEC6IvAAxIGGV2RmZvr/3aZNG1u0aJGtW7fO/1iVKlXc7cYbbyykrQSQjMklG1Yj8AAAANILgQcgDlq3bm29e/e2F154wb755hv3mIZbnH/++bZlyxb372effdZWrVplixcvLuStBVDYduw9YBt37nN/0+MBAACkG3I8AHFw1113Wbt27eytt95ywy127txpf/vb32z8+PF20EEHWfny5W3z5s1u2Y4dOxb25gIoZL/9md+hWvlSVrHMH0O0AAAA0gWBByAOmjZtal999ZU99dRTLqFkRkaGe/yZZ56xYsWK2csvv+ym0uzUqVOO3A8Aih6GWQAAgHRG4AGIE02l+cADD2R7rEyZMvbiiy/a448/7v5dsWLFQto6AMlk2Z+BB4ZZAACAdESOByAOevbs6Xo63HzzzSGfV8CBoAMAz/JNfwy1aESPBwAAkIYIPABx8PHHH7v7atWqFfamAEgBv3lDLaqXL+xNAQAAiDkCD0AcXHHFFVa5cmW75ZZbcl22f//+CdkmAMlp7vLN9vWyP5LNHlKrQmFvDgAAQMwReADiYNSoUW6YhWa32L9/f9jlFi5c6Ga6KKh9+/a592zevLlLbNm1a1ebMWNGvtZ1/fXXuwSYwTcvLwWA2Fm/fa9d+dK3ti8zy7ofXtva1K9c2JsEAAAQcySXBOJAU2ceOHDAVqxY4ZJJNmnSJMcyu3btsu+++86ysrIK9F579+610047zdauXWsffvihNWzY0CZNmmTdunWzCRMmWK9evaJe14YNG9zMG8GqV69ul112WYG2E0B2BzKz7JqXv7U12/ZY05rlbXSvNi7IBwAAkG4IPABxUKVKFXvjjTfclJmyfPnysMsWtKExZMgQmz59upu+U0EHUbBh8uTJ1q9fP2vfvr01btw4qnU99NBDNmjQIPv73/+e7fEKFSpYuXIkvQNiaeT/frSvlm6yCqVL2FMXt7eKZUoW9iYBAADEBYEHIA40zOKtt96ysWPHut4OJUrkPNTU02HmzJk2fPjwfL/PsmXL3Hu0bNnSOnTokO25iy++2CZOnGhDhw61V155Jdd1bd++3Z5//nmbP3++6+EAIH7enrfSnv18qftbPR3I7QAAANIZgQcgDo4++mg755xzcvQcCHbiiSfaY489lu/3efXVV92Qjs6dO+d4rmPHju5ePR82btyYazBBORwqVapkH3zwgZ100klWu3btfG8XgPB+WL3Nhrzxnfv7qhOa2qmt6hT2JgEAAMQVgQcgTpTbITfjxo2zNWvW5Ps9pk6d6u5D5ZDQVJ716tWzlStXup4VZ511Vtj17Nmzxx588EGXJ+KCCy5wPTTOPPNMf8LK/FqzdbftKVbGNJrEDShx98X8/3aJK939n3+HevzP5S3o38WDXgskqz37M23R6m323Yot9t3vW+3Tn9fbnv1ZdnyzGnbj3/J/fAEAAKQKAg9AnJQuXTri89u2bbN77rnH5WNQDoX8mDt3rruvX79+2FwTCjzMmzcvYuDhiy++cPkhypQpY7/99pvrRaGhIu+9954999xz1rdv31wTXOoW+Nnkbw99bsVLJy43ROjAxR9/6N4FK/zLuCfCvsYfCAnx+F/vlX19fy0bYl1/vF2O57RNOQMy2f/tbX9wwOWPpXxu5pRSpZZZ8eJBz7nlcwZ7wpeV94YIy+dz+3rp0iu9go6wqM9+27jLflyz3Q5k/ZHvxdOoejl7qE8b9/1lBT2Hv4ajqQwLmoAXuaOsEyfZyzpZtwtA6iPwAMRBqB4IwdNfagYJNRgfffRRl4chr9RLYceOHf4AQyiVK/8xNZ/eKxINrZg9e7b7WzNxPP3003b//fe791CuiBo1atgpp5wS9vUjR460ESNG5Hi8hBrCmrTX5/5Tm83dx4t//X8m9Qx4Jo7vCuSuatkS1qJ2OWtRu7y1rFPejqpfwfbv2GLr/jiEEaYBtHXrVtdIK16c2b/jibJOnGQva+V7AoB4KObz0u4DiJm8VCZq1qzphjjklXoyeD0dPvroIzv55JNzLHP88cfb559/7nJN/Oc//8nT+hctWuQCEtq2Zs2a2U8//RR2SEOoHg8NGjRwuSXCBUV06vECBe7qj/vbFxSkyL7MH0EFs6w//3bPBfztX2/A693F5DDrCgxU+ILX++dyfz4d9PrAdeR8TfC2ZOXYzqB1BW9bmL//2pa/ykyV2G3bt1mFCpXc9xOxzEJ8/uAyQ2QqKwX81EspmiE+tSuVttb1K1u9KmUZEpRH2rfXr1/vzpHJ2EBLJ5R14iR7Wev3u2rVqi44orxPABAr9HgA4uSll16yTp06WUZGRo7ntmzZYjfeeKONHj3a/cDnR6lSpfx/h4sfqmeFl+8hrzRTxrvvvusSZS5evNi++eYbNzVnuGEloYaWqFKVjBWrdKvErlu3zmrVqkVZJwDlnVgK1nAeSQzKOnGSuayTcZsApAcCD0AcHH744S5JYziNGjWym266yfr162effPJJvt5DwQQFHxRc2LlzZ8hlFOAQDZXIj6OOOsrld5gwYYItWbIkbOABAAAAAMIhrAnEwYIFC3Jd5tRTT3UzWgwePDhf76GeFOqVIKtWrQq5jDeEo02bNpZf3bp1c/f5TYAJAAAAoGgj8AAUEuVE2LVrl7355pv5Xkf37t3d/cKFC3M8p4SSGqNZvnx569q1a77f46CDDnJBDg25AAAAAIC8YqgFEAczZsyI+PymTZts3LhxLnt03bp18/0+/fv3d7NPhHq/WbNmufsePXpkyweRV99//7316dPHjWkHAAAAgLwi8ADEwQknnJBrBnsvIeTtt9+e7/fRbBMDBw60J5980ubNm2dt27b1P/fCCy9Y2bJlbfjw4f7Hpk+fbrfeeqtdeOGFdt111/kfV88Lba+WD6QeE2+99Za9/vrr+d5GAAAAAEUbgQcgTqpXr24tWrTIkSHaa+ArwWTPnj3dlJUFoZkx5syZY4MGDXKzUGiWjEcffdSmTJnikkI2adLEv+yYMWNs9uzZbqpML/CQmZnppuVUtv6RI0fagAEDrGTJkm74xoMPPugCGLVr1y7QNgIAAAAougg8AHGgoQ3fffed1alTJ+7vpRwO6slwxx13uFknFOho1aqVC0a0bt0627KaoULDMi655BL/Y8rfcM8999hDDz1kN9xwgws+dOnSxQVE1JOiRAlOEwAAAADyr5jP6+8NIGbUgFdvgaJq27ZtVrlyZdu8ebNVqVKlsDcnramnyrp161wODuZfjz/KO3Eo68ShrBMn2cva+/3WUMtKlSoV9uYASCPJd8YD0oAXdFDehZ07d2Z7btKkSfbVV18V0pYBAAAAQGIReADiYM+ePXbKKadYu3bt7LLLLsv23Omnn26TJ092wxmWLVtWaNsIAAAAAIlA4AGIAyVx/Pjjj93MFTVq1MiRk2HUqFF21FFH2bHHHmtr1qwptO0EAAAAgHgj8ADEwUsvvWRXX321zZo1y8aOHRtymcGDB9vq1att2LBhCd8+AAAAAEgU0tUDcbBr1y43pWUkBx10kLv/73//m6CtAgAAAIDEo8cDEAdly5Z1masjmTZtmj8fBAAAAACkKwIPQBx069bNHnjggbDPL1q0yAYOHGjFihWzY445JqHbBgAAAACJxFALIA7uuOMOa9OmjU2fPt369+9vzZo1s8zMTFuyZIm99tprblaLAwcOWMmSJe3uu+8u7M0FAAAAgLgh8ADEQe3ate3999+3c845x3r16pXjec12UalSJXv++eetU6dOhbKNAAAAAJAIBB6AOFGPBw2pePbZZ+1///ufLVu2zOV9qF+/vp1wwgk2YMAAF6AAAAAAgHRG4AGIc5LJa665xt0AAAAAoCgiuSQQR5s3b87x2Mcff2yrVq0qlO0BAAAAgEQj8ADEgYZUaChFjRo13H2g5s2b280332yXXHJJyMAEAAAAAKQTAg9AHDz55JP23HPPuSSSu3fvzvaccjxMmDDBzWrRpUsX2759e6FtJwAAAADEG4EHIE6Bh7PPPtsmTpzo/g7lrrvusoULF9qdd96Z8O0DAAAAgEQhuSQQB5s2bbK5c+daRkZG2GWaNGni7l977TV78MEHE7h1AAAAAJA49HgA4qB8+fIRgw4yZ84cd79ly5YEbRUAAAAAJB6BByAOOnXq5PI4hLNu3Tq74oorrFixYta2bduEbhsAAAAAJBJDLYA4uP32261jx472xRdfWP/+/a1Zs2aWmZlpS5YscUMrnn76adu6datbdtiwYYW9uQAAAAAQN/R4AOJAgQYFGJRc8uijj7YqVapY9erVrUOHDjZ69Gg3vEJDMR5++GE7/fTTC/x++/bts1GjRrmpOps2bWpdu3a1GTNmFHi9N910k+uVsWzZsgKvCwAAAEDRROABiJNu3brZ999/bzfccIMddthhVqZMGStVqpRLKqleEN98841dc801BX6fvXv32qmnnmovvviiffjhh65Xhdar9580aVK+16vABUkvAQAAABQUQy2AOKpbt67r4aBbOGeeeaa98847+X6PIUOG2PTp0+2rr76yhg0busd69eplkydPtn79+ln79u2tcePGeVrnjh07XHCkdOnStnv37nxvGwAAAADQ4wEoRN9++6299957+X69hkCMHTvWWrZs6YZxBLr44ott586dNnTo0DyvV700+vTpY7Vq1cr3tgEAAACAEHgACsGBAwfsmWeese7du5vP58v3el599VW3rs6dO+d4TsktRT0fNm7cGPU63333XRcQGT58eL63CwAAAAA8BB6ABFq/fr3dc889dvDBB7vpNPMSEAhl6tSp7l55I4JVq1bN6tWr5xJPzpw5M6r1aXuUH0L5IkqWLFmgbQMAAAAAIccDkABz5syxRx991CV7VCCgIL0cAs2dO9fd169fP+Tzmk1j5cqVNm/ePDvrrLNyXd9VV11l1113nRu6kdcEl7p5tm3b5u6zsrLcDfGj8tX+RDknBuWdOJR14lDWiZPsZZ2s2wUg9RF4AOJEQyA0FEIBBwUeRJUNJZy8/PLLrUePHi6J4wknnJCv9e/Zs8e93gswhFK5cmV3v2HDhlzXp6k/tdz111+f520ZOXKkjRgxImQPDwVaEN9K4tatW92+Vbw4ndjijfJOHMo6cSjrxEn2st6+fXthbwKANEXgAYix1atX25NPPmn/+c9/bN26da5yUaxYMStfvrwbwqBZLDIyMvzLn3vuufl6n8BhGuXKlQu5jFepUZAiklWrVtmwYcPs008/dduaV0pgOXjw4Gw9Hho0aGA1a9YMGxRB7Cqx+s5U1slYiU03lHfiUNaJQ1knTrKXtab+BoB4IPAAxMgXX3zheje8+eabrreDAg4KNlx22WVu+MLZZ5/tbsFee+21fL1fqVKl/H+HG7rh9TZQvodINHWmeiwoWJAfmnZTt2CqVCVjxSrdqBJLWScO5Z04lHXiUNaJk8xlnYzbBCA9EHgACmjcuHH22GOPuTwKXhBADXglafz73/8etyv+CiYo+KDggqbNDGXLli3uvkaNGmHXo94ZCpBo+k0AAAAAiDUCD0ABaTiFchko4FCpUiV74oknrHfv3tmGU8SD1q8kkAp4aKhEKGvXrnX3bdq0Cbue+++/33799deIQywaN27sD7KoBwcAAAAARIvAA1BAQ4YMsZtuuslef/11e/jhh92/f//9dxs4cKA/uWO8dO/e3QUeFi5cmOM5JYpUAiv1ZujatWvYdWhqz3BTZy5ZssQNG9F0nVom3p8HAAAAQPoh8ADEqPdBnz593G327NkuAKHG+kUXXWQ33HCDa9zHg3IzqMfCjBkzcjw3a9Ysd6/ZMwLzQQT7+OOPwz6n7f7tt9/cMvH6DAAAAADSGxlkgBjr0KGDTZgwwb7//nurWLGiderUyXr16mW7d+8Oufwrr7yS7/dq1qyZ61mxYMECf44JzwsvvGBly5a14cOH+x+bPn26dezY0R555JF8vycAAAAA5AWBByBODjroIPvnP//pegxoSISCEO3bt3d5ErzpLTWMYdCgQQV6n9GjR1u7du3cejZt2uRyTSiwMGXKFBs/frzreeEZM2aM65GhqTMBAAAAIBEIPABxpmkmBwwYYN999539+9//tsmTJ1v9+vWtX79+bmjG9u3bC7R+5XBQTwb1rFBgQ70gpk2bZnPmzLGePXtmW7Zv374uAHLppZcW8FMBAAAAQHSK+XR5FEBCLV682AUh1PtBMjMzLZ1s27bNJaLcvHlz3KYTxR+ysrLczCq1atVi/vUEoLwTh7JOHMo6cZK9rL3fbyWn1kxdABAryXfGA4oA9Up45plnbNKkSYW9KQAAAAAQVwQegEJ03nnn2SmnnFLYmwEAAAAAcUPgAShk7733XmFvAgAAAADEDYEHAAAAAAAQNwQeAAAAAABA3BB4AAAAAAAAcUPgAQAAAAAAxA2BBwAAAAAAEDcEHgAAAAAAQNwQeAAAAAAAAHFD4AEAAAAAAMQNgQcAAAAAABA3BB4AAAAAAEDcEHgAAAAAAABxQ+ABAAAAAADEDYEHAAAAAAAQNwQeAAAAAABA3BB4AAAAAAAAcUPgAQAAAAAAxA2BByAN7Nu3z0aNGmXNmze3pk2bWteuXW3GjBl5WkdmZqY98sgjdvjhh1vZsmWtUaNGNnToUNu7d2/cthsAAABA+iPwAKQ4BQZOPfVUe/HFF+3DDz+0JUuW2DXXXGPdunWzSZMmRb2eAQMG2ODBg2379u0uCLF8+XIXzLj00kvjuv0AAAAA0huBByDFDRkyxKZPn27jxo2zhg0busd69eplPXv2tH79+tnSpUtzXcerr75qO3futN9//90FHDZv3myXX365/7nvvvsu7p8DAAAAQHoi8ACksGXLltnYsWOtZcuW1qFDh2zPXXzxxS6YoOESuVGw4ZVXXrE6deq4f5cvX96eeuopa9Kkifv3Tz/9FKdPAAAAACDdEXgAUph6Ixw4cMA6d+6c47mOHTu6+8mTJ9vGjRsjrufmm2+24sWznw5KlChh7dq1c3+3adMmptsNAAAAoOgg8ACksKlTp7p7r2dCoGrVqlm9evVc4smZM2fma/1r1qyxCy64wA499NACbysAAACAoqlEYW8AgPybO3euu69fv37I56tUqWIrV660efPm2VlnnZWndX/77be2f/9+e+KJJ6JKcBk4+8W2bdvcfVZWlrshflS+Pp+Pck4QyjtxKOvEoawTJ9nLOlm3C0DqI/AApKg9e/bYjh07/AGGUCpXruzuN2zYkKd1v/feey4x5d/+9jeXJ6JSpUoRlx85cqSNGDEix+Pr1693PS4Q30ri1q1bXUU2eLgMYo/yThzKOnEo68RJ9rLWzFYAEA8EHoAUFZi3oVy5ciGX8So1ClJEY9GiRXbPPffY66+/7nJHjB8/3j744AObNm2atWjRIuzrlMBSU3EG9nho0KCB1axZM2xQBLGrxBYrVsyVdTJWYtMN5Z04lHXiUNaJk+xlXaZMmcLeBABpisADkKJKlSrl/1tXTkLxehso30M0NDvGxIkT7fHHH3c3BSGU52HAgAER80SULl3a3YKpUpWMFat0o0osZZ04lHfiUNaJQ1knTjKXdTJuE4D0wNkFSFEKJnjBBw2HCGXLli3uvkaNGnlad9WqVW3YsGGu54N88cUXbspNAAAAAMgrAg9AisrIyHA9FGTVqlUhl1m7dm2BpsM888wzrUOHDhHfAwAAAAAiIfAApLDu3bu7+4ULF+Z4TgkllcCqfPny1rVr13y/x3HHHefuDzrooAJsKQAAAICiisADkML69+/vxmPOmDEjx3OzZs1y9z169MiWDyKvFLxQj4lGjRoVaFsBAAAAFE0EHoAU1qxZMxs4cKAtWLDA5s2bl+25F154wcqWLWvDhw/3PzZ9+nTr2LGjPfLII1Gtf9OmTfbuu+/amDFjYr7tAAAAAIoGAg9Aihs9erS1a9fOBg0a5AIFmuFCgYUpU6a46TCbNGniX1YBhNmzZ7vEkYFDMjT15RFHHGHjxo2zvXv3useXLFlivXv3dq85+eSTC+WzAQAAAEh9BB6AFKccDurJ0KlTJ2vfvr3rBTFt2jSbM2eO9ezZM9uyffv2tYoVK9qll17qf6xKlSp2yimn2OrVq920mQpCKHfEk08+6QIReg0AAAAA5Fcxny6PAkAMbdu2zSpXrmybN292gQ3ET1ZWlq1bt85q1arF/OsJQHknDmWdOJR14iR7WXu/38rvVKlSpcLeHABpJPnOeAAAAAAAIG0QeAAAAAAAAHFD4AEAAAAAAMQNgQcAAAAAABA3BB4AAAAAAEDcEHgAAAAAAABxQ+ABAAAAAADEDYEHAAAAAAAQNwQeAAAAAABA3BB4AAAAAAAAcUPgAQAAAAAAxA2BBwAAAAAAEDcEHgAAAAAAQNwQeAAAAAAAAHFD4AEAAAAAAMQNgQcAAAAAABA3BB4AAAAAAEDcEHgAAAAAAABxQ+ABAAAAAADEDYEHIA3s27fPRo0aZc2bN7emTZta165dbcaMGXlax44dO+yWW26xxo0bW6lSpax+/fo2aNAgW716ddy2GwAAAED6K1HYGwCgYPbu3WunnXaarV271j788ENr2LChTZo0ybp162YTJkywXr16RRV06NKli82dO9cyMjIsKyvLVq5caU899ZS9/fbbLojRrFmzhHweAAAAAOmFHg9AihsyZIhNnz7dxo0b54IOomBDz549rV+/frZ06dJc13HPPfeYz+ezadOm2a5du2zbtm123333WYkSJWzNmjV26aWXJuCTAAAAAEhHBB6AFLZs2TIbO3astWzZ0jp06JDtuYsvvth27txpQ4cOjbiOzMxM16NBwYsTTzzRDbOoUKGC3Xzzzf7Xzpo1y3799de4fhYAAAAA6YnAA5DCXn31VTtw4IB17tw5x3MdO3Z095MnT7aNGzeGXYd6NKjXRJUqVXI8d+ONN/r/Xr9+fcy2GwAAAEDRQeABSGFTp051902aNMnxXLVq1axevXou8eTMmTPDrkPLnHPOOSGfq1y5stWqVcv97Q3jAAAAAIC8ILkkkMKUDFI0A0Uo6sWgJJHz5s2zs846K8/rV2+KLVu2uGEcBx10UMQEl7p5lCNClKRSN8SPylf5OSjnxKC8E4eyThzKOnGSvayTdbsApD4CD0CK2rNnj5uNQkINk/B6LMiGDRvy9R6fffaZ6zGhfA+RjBw50kaMGJHjcQ3P0OsR30ri1q1bXUW2eHE6scUb5Z04lHXiUNaJk+xlvX379sLeBABpisADkKIC8zaUK1cu5DJepUZBivx49NFH3bScmiEjEiWhHDx4cLYeDw0aNLCaNWuGDYogdpXYYsWKubJOxkpsuqG8E4eyThzKOnGSvazLlClT2JsAIE0ReABSlGaf8OjKSShebwPle8irTz75xD7//HP/cI5ISpcu7W7BVKlKxopVulEllrJOHMo7cSjrxKGsEyeZyzoZtwlAeuDsAqQoBRO84IOmzQxF+RmkRo0aeVr35s2b7aqrrrI333zTJZ8EAAAAgPwi8ACkqIyMDGvZsqX7e9WqVSGXWbt2rbtv06ZN1OvNzMy0Sy65xO655x477rjjYrS1AAAAAIoqAg9ACuvevbu7X7hwYY7nlFBSCazKly9vXbt2jXqdV155pZ199tnWo0ePmG4rAAAAgKKJwAOQwvr37+/GY86YMSPHc7NmzXL3CiAE5oOI5MYbb7RDDz3UBgwYEDKZpTdNJgAAAABEi8ADkMKaNWtmAwcOtAULFti8efOyPffCCy9Y2bJlbfjw4f7Hpk+fbh07drRHHnkkx7o0ZaZmoLjppptyPKf1n3vuuW54BwAAAADkBYEHIMWNHj3a2rVrZ4MGDbJNmza5GS4UWJgyZYqNHz/emjRp4l92zJgxNnv2bBs2bJj/MS2vRJJ67uGHH3aJKL1b9erV3VSdrVu3toYNG7phGwAAAACQF0ynCaQ4BQPUk+GOO+6w9u3bu6EXrVq1sjlz5riAQaC+ffu6YRlKHum59dZb7YknnvAPpwjnwgsvjOOnAAAAAJCuivl0uRMAYki5ICpXruym5dTwDcRPVlaWrVu3zmrVqsX86wlAeScOZZ04lHXiJHtZe7/fSk5dqVKlwt4cAGkk+c54AAAAAAAgbRB4AAAAAAAAcUPgAQAAAAAAxA2BBwAAAAAAEDcEHgAAAAAAQNwQeAAAAAAAAHFD4AEAAAAAAMQNgQcAAAAAABA3BB4AAAAAAEDcEHgAAAAAAABxQ+ABAAAAAADEDYEHAAAAAAAQNwQeAAAAAABA3BB4AAAAAAAAcUPgAQAAAAAAxA2BBwAAAAAAEDcEHgAAAAAAQNwQeAAAAAAAAHFD4AEAAAAAAMQNgQcgDezbt89GjRplzZs3t6ZNm1rXrl1txowZ+VrXnj177PHHH7eDDz7Yli1bFvNtBQAAAFC0lCjsDQBQMHv37rXTTjvN1q5dax9++KE1bNjQJk2aZN26dbMJEyZYr169olrPrl277IknnrCHH37YVqxYEfftBgAAAFA00OMBSHFDhgyx6dOn27hx41zQQRRs6Nmzp/Xr18+WLl0a1XoyMzPtkksucesqXpxTAwAAAIDYoHUBpDANhRg7dqy1bNnSOnTokO25iy++2Hbu3GlDhw6Nal0VK1a0mjVruqEaNWrUiNMWAwAAAChqCDwAKezVV1+1AwcOWOfOnXM817FjR3c/efJk27hxY57WW6ZMmZhtIwAAAICijcADkMKmTp3q7ps0aZLjuWrVqlm9evVc4smZM2fmab3FihWL2TYCAAAAKNpILgmksLlz57r7+vXrh3y+SpUqtnLlSps3b56dddZZcU1wqZtn27Zt7j4rK8vdED8qX5/PRzknCOWdOJR14lDWiZPsZZ2s2wUg9RF4AFKUpr3csWOHP8AQSuXKld39hg0b4rotI0eOtBEjRuR4fP369a7HBeJbSdy6dauryJIUNP4o78ShrBOHsk6cZC/r7du3F/YmAEhTBB6AFBWYt6FcuXIhl/EqNQpSxJMSWA4ePDhbj4cGDRq4ZJXhgiKIXSVWQ2NU1slYiU03lHfiUNaJQ1knTrKXNTmeAMQLgQcgRZUqVcr/t66chOL1NlC+h3gqXbq0uwVTpSoZK1bpRpVYyjpxKO/EoawTh7JOnGQu62TcJgDpgbMLkKIUTPCCD5o2M5QtW7a4e6bHBAAAAFBYCDwAKSojI8Natmzp/l61alXIZdauXevu27Rpk9BtAwAAAAAPgQcghXXv3t3dL1y4MMdzSiipBFbly5e3rl27FsLWAQAAAACBByCl9e/f343HnDFjRo7nZs2a5e579OiRLR8EAAAAACQSgQcghTVr1swGDhxoCxYssHnz5mV77oUXXrCyZcva8OHD/Y9Nnz7dOnbsaI888kjE9R44cMDdZ2ZmxmnLAQAAABQVBB6AFDd69Ghr166dDRo0yDZt2uRmuFBgYcqUKTZ+/Hhr0qSJf9kxY8bY7NmzbdiwYWHXt3TpUlu3bp37+8svv0zIZwAAAACQvgg8AClOORzUk6FTp07Wvn171wti2rRpNmfOHOvZs2e2Zfv27WsVK1a0Sy+9NOS6GjVqZIceeqjt37/f/fuiiy6yunXr5uhNAQAAAADRKubT5VEAiKFt27ZZ5cqVbfPmzValSpXC3py0lpWV5Xqo1KpVi/nXE4DyThzKOnEo68RJ9rL2fr+VnLpSpUqFvTkA0kjynfEAAAAAAEDaIPAAAAAAAADihsADAAAAAACIGwIPAAAAAAAgbgg8AAAAAACAuCHwAAAAAAAA4obAAwAAAAAAiBsCDwAAAAAAIG4IPAAAAAAAgLgh8AAAAAAAAOKGwAMAAAAAAIgbAg8AAAAAACBuCDwAAAAAAIC4IfAAAAAAAADihsADAAAAAACIGwIPAAAAAAAgbgg8AAAAAACAuCHwAAAAAAAA4obAA5AG9u3bZ6NGjbLmzZtb06ZNrWvXrjZjxow8r2fNmjV2xRVXWJMmTaxx48bWp08fW758eVy2GQAAAEDRQOABSHF79+61U0891V588UX78MMPbcmSJXbNNddYt27dbNKkSVGvZ+nSpda+fXvbsmWLLVy40H755RerW7eue+ynn36K62cAAAAAkL4IPAApbsiQITZ9+nQbN26cNWzY0D3Wq1cv69mzp/Xr188FFHKTmZnpXqOeE88995yVLVvWMjIybPTo0VamTBnr3bu37d+/PwGfBgAAAEC6IfAApLBly5bZ2LFjrWXLltahQ4dsz1188cW2c+dOGzp0aK7rmThxon3zzTcu+FC+fHn/4wo+9O3b17777jt79tln4/IZAAAAAKQ3Ag9ACnv11VftwIED1rlz5xzPdezY0d1PnjzZNm7cGHE9EyZMcPeh1tOpUyd3//TTT8doqwEAAAAUJQQegBQ2depUd69kkMGqVatm9erVc8MnZs6cGXYdu3btsk8++STseo444gh3P3fuXNu6dWsMtx4AAABAUUDgAUhhCgZI/fr1Qz5fpUoVdz9v3ryw6/jhhx9sz549YdfjrcPn89n8+fNjst0AAAAAio4Shb0BAPJHwYIdO3ZkCw4Eq1y5srvfsGFD2PWsX7/e/3eo9XjriLQezayhm8frGaEZMhBfWVlZtm3bNitVqpQVL04sOd4o78ShrBOHsk6cZC9rbZt3sQEAYonAA5CiAvM2lCtXLuQyXqXG69GQn/UEVozCrWfkyJE2YsSIHI83btw47PsCAIDktH379mwXHgCgoAg8AClKV0s84a5MKL+Dl+8hv+vx1hFpPZo5Y/Dgwf5/q6dDo0aNbPny5VRcEnB1qkGDBrZixQqrVKlSYW9O2qO8E4eyThzKOnGSvaxVD1DQoW7duoW9KQDSDIEHIEUpCKCggQIDmjYzFG+oQ40aNcKup06dOv6/tZ7gQEHgcIlw6yldurS7BdO6krFilY5UzpR14lDeiUNZJw5lnTjJXNZcMAAQD8k3uAxAVDIyMqxly5bu71WrVoVcZu3ate6+TZs2YdfTqlUrK1asWNj1eOtQkKNFixYx2XYAAAAARQeBByCFde/e3d0vXLgwx3NKBKkkj+XLl7euXbuGXUfVqlWtQ4cOYdfzyy+/uPsuXbq4dQEAAABAXhB4AFJY//79XfLHGTNm5Hhu1qxZ7r5Hjx7Z8jiEMnDgQHcfaT0XXHBB1NulYRfDhw8POfwCsUVZJxblnTiUdeJQ1olDWQMoqor5mC8HSGlXXnmlPfnkkzZ37lxr27at//GePXvau+++a99//701adLEPTZ9+nS79dZb7cILL7TrrrvOv+z+/futXbt2tm7dOlu2bJmVKVPGPa78EZqZQvkkvv32WytZsmQhfEIAAAAAqYweD0CKGz16tAsaDBo0yDZt2uQyUj/yyCM2ZcoUGz9+vD/oIGPGjLHZs2fbsGHDsq1DAYWXX37ZDhw44Gan0P2uXbvs8ssvd3OOv/766wQdAAAAAOQLgQcgxSnvgnoydOrUydq3b2/NmjWzadOm2Zw5c1yvh0B9+/a1ihUr2qWXXhoyyaSGVSiZpNah3hNVqlSx+fPnW/PmzRP4iQAAAACkE4ZaAAAAAACAuKHHAwAAAAAAiBsCDwCyUULJUaNGueEVTZs2dVNxhprtIjdr1qyxK664wuWYUILKPn362PLlyyO+Rrkkjj76aPea1q1b2zPPPGPpLBZlvWPHDrvllltcGWv2kvr167t8H6tXr871+1ES0WLFimW76fVKNpqOYrVvy/XXX5+j7HR7/PHHQy7Pvp23stbra9asGbKMA2/r16/P8dqiuG/L1KlTrXPnzvb888/n6/WcsxNT1pyzARRZGmoBALJnzx7fiSee6GvZsqXvt99+c4+99tprvpIlS7r7aP3666++evXq+Xr37u3btWuX78CBA75//OMfvpo1a/p+/PHHkK8ZOnSor0KFCr5PPvnE/fuHH35wy1977bW+dBSLst6+fbvvyCOP1HA5X0ZGhq9YsWLub93q1Knj+/nnn8O+9uabb/YvG3j797//7UtHsdq3Zf369b5y5crlKLvq1av7du7cmWN59u28l/Urr7wScv8MvHXs2DHka4vavv3qq6/6OnTo4P+c48aNy/M6OGcnpqw5ZwMoygg8APC7/vrrXUXmq6++yvZ43759feXLl3eV09yowtquXTtXAd2xY0e2xxs0aOBr3bq1b9++fdleM3ny5JAVqKeeeso9rspeuolFWd9yyy2+tm3b+qZNm+bbu3evq9Ted999vhIlSrh1H3PMMSFft2nTJtfImDdvnmssBN52797tS0exKG/PsGHDfIMHD85RditWrMixLPt2/sq6W7dubj3z58/3rVmzxgV7vNuqVat8FStW9I0ePTrH64rivr1kyRIX7GnWrFm+GsOcsxNX1pyzARRlBB4AOEuXLnWVH12lDPbuu++6SlGfPn1yXc+LL77olr3qqqtCVrr03BNPPOF/LDMz01XidOVn7dq12ZZXpUxXhVThUiU4XcSirFUenTp18m3evDnHc3fccYf/apgqysHuvvtu35AhQ3xFRaz2bdm2bZvbHzds2JDrsuzb+Str7bP33ntv2Oe99Xi9KYryvh1IvRXy0xjmnJ2YsuacDaCoI8cDAOfVV1+1AwcOuHGrwTp27OjuJ0+ebBs3boy4ngkTJrj7UOvRlJ/y9NNP+x/TtJ+LFy9248Br1aqVbfkKFSrY4YcfbitXrrR3333X0kUsylrjfYcMGeKmPA124403+v8OHgO/c+dOe+SRRywrK8tmzpzp7tNdrPZtUQ6HSpUq2QcffOCmno2EfTt/ZV2vXj23b4czadIkt66GDRtaUd+3A2n8f35wzk5MWXPOBlDUEXgA4E+WJUoSFqxatWquMaCEb6r4hLNr1y775JNPwq7niCOOcPdz5861rVu35vq+ga+ZPn26pYtYlLWWOeecc0I+V7lyZX+DILhxpgbEhg0b7P7777fjjjvODj74YBs7dqxlZmZauopFecuePXvswQcftB9++MEuuOACl9Tt3HPPtZ9++inP7yvs26GVLl3aihcPXT1REr233nrLevfuneO5orhvB1KiwbzinJ24suacDaCoI/AAwF+xFDWmQvGu0sybNy/sOtQgU+Ms3Hq8dWiY1/z582P2vqkm3p9ZV5y3bNliHTp0sIMOOijbc19//bW1atXKypcv7/69YsUKu+aaa6xbt262efNmS0exKu8vvvjCNQoaNWrkL2c1gtu2bWsTJ06M2/umknh/5o8//tjt2z179szxXFHctwuKc3Zy4JwNoCgg8ADAVTw1xZeE6gbqXZERXXkJJ7CLaKj1eOsIXI/3moK8b1Es60g+++wzd1X55ptvzvHcSy+9ZAsWLLBNmza54QLt27d3j+uq53nnnZd23XhjWd4nnXSSzZ4925YtW+amGbzjjjtcl2u9x8UXX2wffvhhtuXZt2P/mcMNsyiK+3YscM5ODpyzARQFBB4AZBtvXa5cuZDLeF2fvatj+VlPYPdpbz3eawryvkWxrCN59NFH3dWwUFeFPZo//pRTTrGvvvrKrr/+en9F9uWXX7Z0Eq/ybtCggd199932zTffWO3atV2356uvvtpdGQ5+b/bt2Hxmr4dJr169Ii5XVPbtWOCcnRw4ZwMoCgg8AHAVGk9gwymQrsZ447Tzux5vHYHr8V5TkPctimUdjiqin3/+uT3//PNRLa9GwkMPPeRyFcgrr7xi6STe5d2yZUuXRE/lqIR7CkQEvzf7dmw+s4ZZqGt5boGHorJvxwLn7MLHORtAUUHgAYCrIHqVSWXQDkXjT6VGjRph11OnTh3/36HW460jcD3eawryvkWxrENRo+yqq66yN9980yUyy4tRo0a5hGlLliyxdBLP8vYcddRR1rdvX/d3YPmxb8f2M3vDLNTbJC/Sdd+OBc7ZhYtzNoCihMADAMvIyHBXbmXVqlUhl/GmDmzTpk3Y9SgBlpftO9R6vHWocdKiRQv3d+vWrQv8vkWxrIOpq/8ll1xi99xzj8t8nleHHnqoGzev6fDSSbzKO5i6SUtg+bFvx+4ze8MsQs1mUVT37VjgnF14OGcDKGoIPABwunfv7u4XLlyY4zklCdNUasqq3bVr17DrqFq1qsvKHW49v/zyi7vv0qWLP0N3pPcNfM3pp59u6SIWZR3syiuvtLPPPtt69OiR7+1SNvVOnTpZuolHeYcqOzW8jz766KjeV9i3ozdt2jSXXC/SGPiiuG8XFOfswsM5G0BRQ+ABgNO/f383dnTGjBk5nps1a5a7VwUpcExwKAMHDnT3kdZzwQUXZLtS3LhxYzetW2CGda/Lrh7X8+lUuYpVWXtuvPFGd/VrwIABOZ5TIrht27ZFdUX5119/dZXhdBPr8g7l+++/tz59+litWrX8j7Fvx66sNcxC5ZTXYRbpvm/HAufsxOOcDaBI8gHAnwYNGqRsYb65c+dme7xHjx6+smXL+pYsWeJ/bNq0ab4OHTr4Hn744WzL7tu3z3fEEUf4ateu7du9e7f/8b179/rq1q3ra9WqlVsm0CuvvOLe98EHH8z2+KOPPuoef/nll33pJhZlLTfddJPv7rvvDvke3333ne/444/37dixw//Y+vXrQy77wAMP+P71r3/50lUsynvnzp2+Xbt25Vj3li1bfMcdd5xvzZo1OZ5j387/vu3Zv3+/r3r16m4fjaSo7tueCy+80JX7M888E/J5ztmFX9bCORtAUUXgAYCfKjvt2rXzdezY0bdx40ZfVlaWqziVKlXKN2nSpGzLnnHGGa7iVaFChRzrWbBggWsoXHnlla7RoAabKmp16tTx/fjjjyHf+4orrnCvmT9/vvv3jBkzfJUqVfLdcMMNvnRU0LLW8irfYsWKuXILvFWrVs018PQalbtnzJgx7rFTTz3V98MPP7jH9uzZ4973/vvv96Wzgpb3gQMHfFWrVvVVrlzZ9/jjj/sbYt9//72vf//+2RrTwdi383ce8XzwwQduP1+xYkXYZYryvi0KiCl4oDIYMGBAyGU4ZxduWXPOBlDUEXgAkM22bdt8119/va9x48a+pk2b+s4++2x/xTLQSy+95KtYsaLv6quvDrmen3/+2Xfeeef5Dj74YF+zZs3ccmvXrg37vqqU6WpZixYtfE2aNPEdc8wxvrfeesuXzgpS1rfccourkOZ2e/fdd/2vWb58uasQV6lSxVemTBl3ZW3IkCH+Cm26K+i+/dhjj/kOOeQQX+nSpX0NGjRwDYRnn33WNdQiYd/O/3lE/v73v7syi6Qo79t9+vTxlStXLttxr4bsE088kW05ztmFW9acswEUdcX0v8Ie7gEAAAAAANITySUBAAAAAEDcEHgAAAAAAABxQ+ABAAAAAADEDYEHAAAAAAAQNwQeAAAAAABA3BB4AAAAAAAAcUPgAQAAAAAAxA2BBwAAAAAAEDcEHgAAAAAAQNwQeAAAAAAAAHFD4AEAAAAAAMRNifitGgAApLPXX3/d5s2bZ9u3b7eHH364sDcHAAAkKXo8AACAfDnzzDNt0qRJtnfv3ny9ftq0afbTTz/lutz3339v33zzTb7eAwAAFD4CDwAAIF+KFStmy5cvtxNOOCHPr73//vtt8eLF1rx581yXbdWqlc2fP9/Gjx+fzy0FAACFicADAADIly+//NL27NmT58DDk08+aT///LNdccUVUb/m8ssvt88++8xmzZqVjy0FAACFqZjP5/MV6hYAAICUNGLE/7d3ZyFVtV8cx5flUJllo0azhV5ZNhFlFpaVSAR1Y3TRRTRcZBMoEkn0VmJRZBjYhA0WJGSFNEpOmTQaRhZUZjTQSGU2D+L7Zz1wNvo2nuM5Jzf/7+dms4dnD3fHn+tZzz9y8OBBuXXr1h+PefDggQwZMsRUO/Ts2dOp592/f18mTJhgnte+fXsX3hgAAPwNVDwAAACXlJWVWdUOb968kdTUVImJiZHk5GT59OmTLF68WIKDg2XNmjXWmMzMTBk2bJgVOvzpODVgwADp3Lmz5OTkePlLAQBASxA8AAAAp2lDSZ1qoRUISoOCpUuXSkVFhYwfP17S09MlLS1NRo0aZZpDOhQUFEhkZKS1/6fjHLRaIi8vz0tfCQAA3IHgAQAAuKW/Q1VVlXTo0MH0b1i0aJGEhISY5pMjRoww53XZTZ0uoceb+t24pkJDQ80KF8wUBQDAPggeAACAS9MswsPDpVevXtaxoqIi6dGjh3X8yZMnJkyYMmWKOV9fX2+2/v7+ze71u3FNaUChgYeGGAAAwB4IHgAAQIv6OzQNEMLCwmT69Olm/8yZM6aXQ1RUlNnv2LGj2f43NPjduKYaGhrMNiAgwENfBgAA3M3X7XcEAAD/F/0dFixYIM+fPzdVCB8/fjQ9GY4cOWJdpwFCXFycNDY2mioF7eegUyXq6uqsa3T878YFBgZa53Rs3759CR4AALARKh4AAIBTrl27ZgKB0aNHy7FjxyQoKMhULegSl/Hx8dZ12jAyOjpatm3bZkIElZCQ0Kxp5J+Oc6itrZXJkyd75TsBAIB7EDwAAACn6JKWGjZs2LBBEhMTzbHi4mKJjY01IYKDNoncv3+/TJ061VyvkpKS5NKlS2bZTGfGKQ07tNJC7wEAAOzD51/aQgMAAC9KSUmR3r17y7Jly5wal5WVJXfv3jVbAABgH1Q8AAAAr8rIyJDS0lKprq52anqHVkps3rzZo+8GAADcj+ABAAAbrSSxatUqmTFjhgwcOLBZk8by8nIZM2aMdOrUSfLz86U18/X1Ne9YWFgoNTU1v73+5s2bUlJSIrm5uWYsAACwF6ZaAABgE9++fZMTJ06Y4EGXmqyqqjLH09PTZf369dK2bVupr6+X1NRUs++gy16ePXvWpWd6+meC3t/Hx6fF1wAAgNaLfxsAAGATfn5+1h/g2pBRLVmyRF69eiWPHz821QDnz5+XcePGNRvXr18/iYiIkNboTwIFQgcAAOyNigcAAGxk+fLlsmXLFikoKDAhg1Y4ZGdn88c5AABotah4AADARk6fPi1t2rSR2tpauXLlihQVFRE6AACAVo2KBwAAbOLhw4fSv39/6dq1q7x//178/f1Nc8bQ0NC//WoAAAA/RcUDAAA2qnZQ8+bNk4qKCjPVYuXKlZKTk/PLcXPmzJHLly+79Mxbt265NA4AAMCBigcAAGxi5syZcvToUbNCRZcuXWT48OHS2NgolZWVMmzYsJ+Oc/eqFq1tagc/ZQAAaN0IHgAAsIGGhgbp1q2b+aP/5cuXZgWLlJQU2bRpk0yYMEHKysr+9isCAAD8UJsfHwYAAK3JhQsX5O3btxIXF2dCB7V69WoJCwsz1Qy5ublef6f8/HxJS0uTpUuXuv3e2jhzzZo1Mnv2bLNUKAAAsC+CBwAAbKCwsNBs4+PjrWOBgYFy8uRJGTt2rCQlJcmKFSvkyZMnXnunadOmyaFDh+TLly8ujS8pKZHbt2//8NzIkSPl8+fPcurUKXn16pVcvXq1hW8LAAD+FqZaAAAAl2jgEBwcLHv27JFZs2Y5NXbjxo3SqVMnWbhw4U+vmT9/vjx9+lSOHz8uu3fvNpUe2igTAADYCxUPAADAJRcvXjRVCdq80hnbt2+XO3fu/DJ0UOXl5da9586dK+fOnTNTTgAAgL2wnCYAAHCJNrSMiIiQ0NDQPx7z4MEDSU1NlZqaml9e9+zZMxNOaE8LB106VBtp6hKf7du3b9G7AwAA76HiAQAAuBw8OCoS3rx5YwKFmJgYSU5Olk+fPsnixYvNVAxtEumQmZlplv7s2bPnd/fbtWuXGbNhwwbTtLJ79+4ydOhQ6/yAAQOkc+fOkpOT46UvBAAA7kDwAAAAXOrvoFMttAJBacCgq1tUVFTI+PHjJT093YQHo0aNkhs3bljjCgoKJDIystm9tN3UvHnzpLKyUrZu3WoCjGvXrsnEiRPN8qFNDRkyRPLy8rz0lQAAwB0IHgAAgFv6O1RVVUmHDh3MFIlFixZJSEiIPHz4UEaMGGHOv3v3Tu7fv2+ON7Vu3ToTWGjo4PDixQuZNGnSd8/VaR26wgW9sQEAsA96PAAAAJemWYSHh0uvXr2sY0VFRdKjRw/ruC7tqSHElClTzPn6+nqz9ff3t8bcu3dP1q5dK/v27bOOazjx6NEjU/HwXxpsaOChIYauigEAAFo/Kh4AAECL+js0DR7CwsJk+vTpZv/MmTOml0NUVJTZ79ixo9lqaOCwd+9e8fPzkxkzZljHtIdDnz59ZPDgwd89t6GhwWwDAgI89GUAAMDdCB4AAIBL/R00eHj+/LkJEnSrvRy0OaSDBg+6KkVjY6N8+PDB9IHQqRJ1dXXWNdevX5dBgwZJu3btrAqInTt3SmxsrNnXJpVN6di+ffsSPAAAYCMEDwAAwCna+FGnO4wePVqOHTsmQUFBptpBl7iMj4+3rtO+DdHR0bJt2zYTPqiEhIRmzSYDAwPNlIzXr1+bEOLAgQPi6+tr+kLs2LFDvn792uzZtbW1MnnyZC9+LQAAaCmCBwAA4BRd0lLDBl32MjEx0RwrLi42VQoaPjhoE8n9+/fL1KlTzfUqKSlJLl26ZFUy6NKbWgmh0yqys7PNihbaJ+Lw4cMmxNBnOWjYoZUWeg8AAGAfPv/SFhoAAHhRSkqK9O7dW5YtW+bUuKysLLl7967ZAgAA+6DiAQAAeFVGRoaUlpZKdXW1U9M7tFJi8+bNHn03AADgfgQPAADAq7SHQ35+vhQWFkpNTc1vr79586aUlJRIbm6uGQsAAOyFqRYAAOCv0Z8hPj4+Lb4GAAC0XgQPAAAAAADAY5hqAQAAAAAAPIbgAQAAAAAAeAzBAwAAAAAA8BiCBwAAAAAA4DEEDwAAAAAAwGMIHgAAAAAAgMcQPAAAAAAAAI8heAAAAAAAAB5D8AAAAAAAADyG4AEAAAAAAHgMwQMAAAAAABBP+R/4ZtjkMbke0QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = True\n",
    "date = \"02_05_25_\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = binary_model_3_layer\n",
    "    model_name = \"CIFAR10_model_(3072+2048+2048+1)_save_3\"\n",
    "    save_path = \"Classifiers/\" + date + model_name + \"/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"datas\\models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "tensor(0.8692)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "model_name = \"model_2_layer_trained_save_1\"\n",
    "assessed_model = torch.load(\"datas/models/model_2_layer_save_1.pt\", weights_only=False)\n",
    "\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plots of performances\n",
    "accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(accuracy)\n",
    "kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_accuracy)\n",
    "loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(loss)\n",
    "kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_loss)\n",
    "plt.show()\n",
    "\n",
    "# Import datas\n",
    "accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).to(dtype)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "\n",
    "\n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, esp_init = 1, fraction_batch=0.01):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = esp_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = esp_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # # hidden neurons layer 2\n",
    "        z3 = torch.mm(self.W3, h2.t() + self.b3)\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, coef_iter = 1, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=0.2, fraction_batch=1e-2, train_layer1=True, train_layer2=True, train_layer3=True):\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        kappa_max = 1 + coef_iter\n",
    "        max_iter = self.input_dimension**(kappa_max)\n",
    "        print(\"max_iter\", max_iter)\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_minibatches = int(max_iter / self.fraction_batch) # Nombre de minibatches utilisés pour l'apprentissage de la première couche\n",
    "        for i in range(N_minibatches):\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            # Suivi de l'apprentissage\n",
    "            if i % 100 == 0:\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2, dim=0) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                overall_training_loss = torch.mean(training_loss,dim=0)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2, dim=0)\n",
    "                overall_validation_loss = torch.mean(validation_loss,dim=0)\n",
    "                self.training_loss_trajectory.append(overall_training_loss.item())\n",
    "                self.validation_loss_trajectory.append(overall_validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", overall_training_loss.item(), \"Validation loss\", overall_validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0] # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = torch.mean(grad_z1, dim=0).unsqueeze(1) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0] # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = torch.mean(grad_z2, dim=0).unsqueeze(1)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            self.W1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/self.eps_init**2 + self.reg1*self.W1) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "            self.b1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/self.eps_init**2 + self.reg1*self.b1)\n",
    "            self.W2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/self.eps_init**2 +self.reg2*self.W2)\n",
    "            self.b2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/self.eps_init**2 + self.reg2*self.b2)\n",
    "        return \"Training done\"\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
