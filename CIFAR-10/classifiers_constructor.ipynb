{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import scipy\n",
    "from scipy.signal import butter, filtfilt, argrelextrema\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 22\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10]) torch.Size([10000, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 32, 32, 3]) torch.Size([10000, 1]) tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]]) torch.Size([2000, 32, 32, 3]) torch.Size([2000, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1024]) torch.Size([2000, 1024])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "transform_data = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) # Apply the (x - mean)/var operation on the components of the data # if x is in [0,1] then Normalise(x) is in [-1,1] # is applied on the three channels RGB\n",
    "])\n",
    "\n",
    "# Data import\n",
    "dtype = torch.float32\n",
    "trainset = torchvision.datasets.CIFAR10(root = './datas', train= True, download = True, transform = transform_data)\n",
    "validset = torchvision.datasets.CIFAR10(root = './datas', train = False, download = True, transform = transform_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 1)\n",
    "\n",
    "x_train, y_train_raw, x_valid, y_valid_raw = torch.tensor(trainset.data), torch.tensor(trainset.targets), torch.tensor(validset.data), torch.tensor(validset.targets)\n",
    "\n",
    "# Modification du format des donnes shape (n_data,1) -> (n_data, n_classes)\n",
    "y_train = torch.zeros((y_train_raw.shape[0], torch.max(y_train_raw)+1))\n",
    "for i, y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros((y_valid_raw.shape[0], torch.max(y_valid_raw)+1))\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "print(y_train.shape, y_valid.shape)\n",
    "\n",
    "# Binary reduction of the classes # To avoid using softmax, we regroup classes in two classes\n",
    "class_binary_reduction = True\n",
    "determination_des_classes = True\n",
    "black_and_white_images = True\n",
    "\n",
    "if class_binary_reduction :\n",
    "    if determination_des_classes :\n",
    "        # Determination des classes\n",
    "        class_list = []\n",
    "        class_index = 0\n",
    "        for i in range (x_train.shape[0]):\n",
    "            if y_train[i, class_index] == 1:\n",
    "                class_list.append(x_train[i])\n",
    "                class_index += 1\n",
    "            if len(class_list) == len(y_train[0]):\n",
    "                break\n",
    "        for i, image in enumerate(class_list):\n",
    "            plt.subplot(2, int(len(class_list)/2+1),i+1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "                \n",
    "    # classe1 = [0, 1, 8, 9]  # vehicles\n",
    "    # classe2 = [2, 3, 4, 5]  # animals\n",
    "    \n",
    "    # classe1 = [1, 3, 4]  # elk, truck, dog\n",
    "    # classe2 = [5, 7, 9]  # horse, car, cat\n",
    "\n",
    "    classe1 = [1]  # elk\n",
    "    classe2 = [0]  # horse\n",
    "    \n",
    "    # Cration des masques pour les chantillons appartenant  ces classes\n",
    "    mask_classe1_train = y_train[:, classe1].sum(dim=1) > 0  # True si appartient  classe1\n",
    "    mask_classe2_train = y_train[:, classe2].sum(dim=1) > 0  # True si appartient  classe2\n",
    "    \n",
    "    mask_classe1_valid = y_valid[:, classe1].sum(dim=1) > 0\n",
    "    mask_classe2_valid = y_valid[:, classe2].sum(dim=1) > 0\n",
    "    \n",
    "    # Filtrage des exemples concerns\n",
    "    mask_train = torch.logical_or(mask_classe1_train, mask_classe2_train)\n",
    "    mask_valid = torch.logical_or(mask_classe1_valid, mask_classe2_valid)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid]\n",
    "\n",
    "    # Cration du vecteur de labels binaires (1 pour classe1, 0 pour classe2)\n",
    "    y_train = (y_train[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    y_valid = (y_valid[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    \n",
    "    # Avec tanh\n",
    "    \n",
    "    # x_train = 2*(x_train-0.5)\n",
    "    # y_train = 2*(y_train-0.5)\n",
    "    # x_valid = 2*(x_valid-0.5)\n",
    "    # y_valid = 2*(y_valid-0.5)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_train[0:10], x_valid.shape, y_valid.shape)\n",
    "\n",
    "if black_and_white_images :\n",
    "    x_train = 0.299*x_train[:,:,:,0] + 0.587*x_train[:,:,:,1] + 0.114*x_train[:,:,:,2]\n",
    "    x_valid = 0.299*x_valid[:,:,:,0] + 0.587*x_valid[:,:,:,1] + 0.114*x_valid[:,:,:,2]\n",
    "    for i, image in enumerate(x_train[0:10]):\n",
    "        plt.subplot(2, int(len(x_train[0:10])/2+1),i+1)\n",
    "        plt.imshow(image, cmap = 'grey')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    x_train, x_valid = (x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])).to(dtype), x_valid.reshape(x_valid.shape[0], x_valid.shape[1]*x_valid.shape[2]).to(dtype)\n",
    "    print(x_train.shape, x_valid.shape)\n",
    "\n",
    "else :    \n",
    "    x_train, x_valid = (x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]*x_train.shape[3])).to(dtype), x_valid.reshape(x_valid.shape[0], x_valid.shape[1]*x_valid.shape[2]*x_valid.shape[3]).to(dtype)\n",
    "    print(x_train.shape, x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45060724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on :  cpu\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# else :\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Computing on : \", device)\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0).to(device),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).type(dtype).to(device)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), dj softmax\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque chantillon\n",
    "    \"\"\"\n",
    "    s = s.to(device)\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = drive de softmax au logit j du ime batch par rapport au logit k du mme batch\n",
    "    for i in range(n):  # Pour chaque chantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-me donne du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des drives croises) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return torch.exp(-x)/((1 + torch.exp(-x))**2)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (torch.tanh(x)**2)).to(device)\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajust en fonction du seuil kappa_eff d'apprentissage des donnes.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prdiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output); grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la dure de l'entranement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        # Initialisation des proprits du rseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la proprit dure d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du rseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage alatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = (torch.einsum('no,noz->nz',grad_output,softmax_derivative(output))).to(dtype) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la dure d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "\n",
    "class binary_classification_two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size, lr=1e-3, lr_decay_rate = 1e9, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.lr = lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(1, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x, dropout_rate=0):\n",
    "        if dropout_rate != 0 :    \n",
    "            dropout_mask_1 = ((torch.rand((x.shape[0], self.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n",
    "        else : \n",
    "            dropout_mask_1 = 1\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)*dropout_mask_1  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = torch.sigmoid(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    \n",
    "    # coef_iter est a ajust en fonction du seuil kappa_eff d'apprentissage des donnes.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, lr_decay_rate = 1e9, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True, dropout_rate = 0):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - lr_decay_rate = \" + str(lr_decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) + ' - observation rate = ' + str(observation_rate) + ' - Train layer 1 = ' + str(train_layer_1) + ' - Train layer 2 = ' + str(train_layer_2) + ' - Dropout rate = ' + str(dropout_rate)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.lr_decay_rate = torch.tensor(lr_decay_rate)\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch, dropout_rate)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                print(\"Output\", output[20:22])\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = grad_output*sigmoid_derivative(z2); grad_z2  = grad_z2.to(dtype) # shape(n_data, 1)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # if i == 250:\n",
    "            #     break\n",
    "\n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n",
    "        \n",
    "        # Calcul de la dure de l'entranement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "class binary_classification_three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, lr=0.01, lr_decay_rate = 1e9, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, dropout_rate = 0):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_three_layer_NN,self).__init__()\n",
    "        # Initialisation des proprits du rseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.lr = lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la proprit dure d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du rseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(1, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x, dropout_rate=0):\n",
    "        if dropout_rate != 0 :    \n",
    "            dropout_mask_1 = ((torch.rand((x.shape[0], self.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_2 = ((torch.rand((x.shape[0], self.hidden_2_size)) > dropout_rate).to(dtype)).to(device)\n",
    "        else : \n",
    "            dropout_mask_1, dropout_mask_2 = 1, 1\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)*dropout_mask_1  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2)*dropout_mask_2 # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, 1)\n",
    "        output = sigmoid(z3) # output layer # shape (n_data, 1)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, lr_decay_rate = 1e9, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True, dropout_rate = 0):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - lr_decay_rate \" + str(lr_decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.decay_rate = lr_decay_rate\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage alatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            \n",
    "            # Dropout\n",
    "            # dropout_mask = ((torch.rand((minibatch_size, x_train.shape[1])) > dropout_rate).to(dtype)).to(device)\n",
    "            # x_minibatch = x_minibatch*dropout_mask\n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch, dropout_rate)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = grad_output*sigmoid_derivative(z3).to(dtype) # shape (n_data, 1) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n",
    "            \n",
    "            # if i == 12000:\n",
    "            #     break\n",
    "        \n",
    "        # Calcul de la dure d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\" \n",
    "\n",
    "class binary_classification_four_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, hidden_3_size, lr=0.01, lr_decay_rate = 1e9, reg1 =0, reg2 = 0, reg3=0, eps_init=1, fraction_batch=0.01, observation_rate = 10, dropout_rate = 0):\n",
    "        \"\"\"\n",
    "        Constructor of the four-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_four_layer_NN,self).__init__()\n",
    "        # Initialisation des proprits du rseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.hidden_3_size = hidden_3_size\n",
    "        self.lr = lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la proprit dure d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du rseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(hidden_3_size, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.W4 = eps_init*torch.randn(1, hidden_3_size, dtype = dtype) / np.sqrt(hidden_3_size)\n",
    "        \n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(hidden_3_size, 1, dtype=dtype)-1)\n",
    "        self.b4 = eps_init*(2*torch.rand(1, 1, dtype = dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.W4 = self.W4.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        self.b4 = self.b4.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x, dropout_rate=0):\n",
    "        if dropout_rate != 0 :    \n",
    "            dropout_mask_1 = ((torch.rand((x.shape[0], self.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_2 = ((torch.rand((x.shape[0], self.hidden_2_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_3 = ((torch.rand((x.shape[0], self.hidden_3_size)) > dropout_rate).to(dtype)).to(device)\n",
    "        else : \n",
    "            dropout_mask_1, dropout_mask_2, dropout_mask_3 = 1, 1, 1\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)*dropout_mask_1  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2)*dropout_mask_2 # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, hidden_3_size)\n",
    "        h3 = ReLU(z3)*dropout_mask_3 # hidden neurons layer_3\n",
    "        z4 = (torch.mm(self.W4, z3.t()) + self.b4).t() # shape (n_data,1)\n",
    "        output = sigmoid(z4) # output layer # shape (n_data, 1)\n",
    "        return output, z4, h3, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, lr_decay_rate = 1e9, reg1 = 0, reg2 = 0, reg3 = 0, reg4 =0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True, train_layer_4 = True, dropout_rate = 0):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - Training fourth layer = \" + str(train_layer_4) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - lr_decay_rate \" + str(lr_decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) + \" - dropout rate = \" + str(dropout_rate)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.decay_rate = lr_decay_rate\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.reg4 = reg4\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage alatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            \n",
    "            # Dropout\n",
    "            # dropout_mask = ((torch.rand((minibatch_size, x_train.shape[1])) > dropout_rate).to(dtype)).to(device)\n",
    "            # x_minibatch = x_minibatch*dropout_mask\n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z4, h3, z3, h2, z2, h1, z1 = self.forward(x_minibatch, dropout_rate)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)  # shape (n_data, 1)\n",
    "            grad_z4 = grad_output*sigmoid_derivative(z4).to(dtype)  # shape (n_data, 1)\n",
    "            grad_h3 = (torch.mm(grad_z4, self.W4)).to(dtype)  # shape (n_data, hidden_3_size)\n",
    "            grad_z3 = grad_h3*sigmoid_derivative(z3).to(dtype) # shape (n_data, hidden_3_size) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            grad_W4 = (torch.mm(grad_z4.t(),h3)/x_minibatch.shape[0]).to(dtype)\n",
    "            \n",
    "            grad_b4 = (torch.mean(grad_z4, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            if train_layer_4:\n",
    "                self.W4 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_3_size))*grad_W4/(eps_init**2) + self.reg4*self.W4)).to(dtype)\n",
    "                self.b4 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_3_size))*grad_b4/(eps_init**2) + self.reg4*self.b4)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z4, h3, z3, h2, z2, h1, z1, grad_output, grad_z4, grad_h3, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3, grad_W4, grad_b4\n",
    "            gc.collect()\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n",
    "        \n",
    "        # Calcul de la dure d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\" \n",
    "    \n",
    "    \n",
    "class binary_classification_five_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, hidden_3_size, hidden_4_size, lr=0.01, lr_decay_rate = 1e9, reg1=0, reg2=0, reg3=0, reg4=0, reg5=0, eps_init=1, fraction_batch=0.01, observation_rate = 10, dropout_rate = 0):\n",
    "        \"\"\"\n",
    "        Constructor of the five-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_five_layer_NN,self).__init__()\n",
    "        # Initialisation des proprits du rseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.hidden_3_size = hidden_3_size\n",
    "        self.hidden_4_size = hidden_4_size\n",
    "        self.lr = lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.reg4 = reg4\n",
    "        self.reg5 = reg5\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la proprit dure d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du rseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(hidden_3_size, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.W4 = eps_init*torch.randn(hidden_4_size, hidden_3_size, dtype = dtype) / np.sqrt(hidden_3_size)\n",
    "        self.W5 = eps_init*torch.randn(1, hidden_4_size, dtype = dtype) / np.sqrt(hidden_4_size)\n",
    "        \n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(hidden_3_size, 1, dtype=dtype)-1)\n",
    "        self.b4 = eps_init*(2*torch.rand(hidden_4_size, 1, dtype = dtype)-1)\n",
    "        self.b5 = eps_init*(2*torch.rand(1, 1, dtype = dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.W4 = self.W4.to(device)\n",
    "        self.W5 = self.W5.to(device)\n",
    "\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        self.b4 = self.b4.to(device)\n",
    "        self.b5 = self.b5.to(device)\n",
    "\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x, dropout_rate=0):\n",
    "        if dropout_rate != 0 :    \n",
    "            dropout_mask_1 = ((torch.rand((x.shape[0], self.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_2 = ((torch.rand((x.shape[0], self.hidden_2_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_3 = ((torch.rand((x.shape[0], self.hidden_3_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_4 = ((torch.rand((x.shape[0], self.hidden_4_size)) > dropout_rate).to(dtype)).to(device)\n",
    "        else : \n",
    "            dropout_mask_1, dropout_mask_2, dropout_mask_3, dropout_mask_4 = 1, 1, 1, 1\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)*dropout_mask_1  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2)*dropout_mask_2 # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, hidden_3_size)\n",
    "        h3 = ReLU(z3)*dropout_mask_3 # hidden neurons layer_3\n",
    "        z4 = (torch.mm(self.W4, z3.t()) + self.b4).t() # shape (n_data,1)\n",
    "        h4 = ReLU(z4)*dropout_mask_4\n",
    "        z5 = torch.mm(self.W5, h4.t() + self.b5).t()\n",
    "        output = sigmoid(z5) # output layer # shape (n_data, 1)\n",
    "        return output, z5, h4, z4, h3, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, lr_decay_rate = 1e9, reg1 = 0, reg2 = 0, reg3 = 0, reg4 =0, reg5=0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True, train_layer_4 = True, train_layer_5=True, dropout_rate = 0):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - Training fourth layer = \" + str(train_layer_4) + \" - Training fifth layer = \" + str(train_layer_5) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - lr_decay_rate \" + str(lr_decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) + \" - dropout rate = \" + str(dropout_rate)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.decay_rate = lr_decay_rate\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.reg4 = reg4\n",
    "        self.reg5 = reg5\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage alatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            \n",
    "            # Dropout\n",
    "            # dropout_mask = ((torch.rand((minibatch_size, x_train.shape[1])) > dropout_rate).to(dtype)).to(device)\n",
    "            # x_minibatch = x_minibatch*dropout_mask\n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z5, h4, z4, h3, z3, h2, z2, h1, z1 = self.forward(x_minibatch, dropout_rate)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss_tot = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)  # shape (n_data, 1)\n",
    "            grad_z5 = grad_output*sigmoid_derivative(z5).to(dtype)  # shape (n_data, 1)\n",
    "            grad_h4 = torch.mm(grad_z5, self.W5)    # shape (n_data, hidden_4_size)\n",
    "            grad_z4 = grad_h4*ReLU_derivative(z4)   # shape (n_data, hidden_4_size)\n",
    "            grad_h3 = (torch.mm(grad_z4, self.W4)).to(dtype)  # shape (n_data, hidden_3_size)\n",
    "            grad_z3 = grad_h3*sigmoid_derivative(z3).to(dtype) # shape (n_data, hidden_3_size) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_W4 = (torch.mm(grad_z4.t(),h3)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b4 = (torch.mean(grad_z4, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation            \n",
    "            grad_W5 = (torch.mm(grad_z5.t(), h4)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b5 = (torch.mean(grad_z5, dim=0)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            if train_layer_4:\n",
    "                self.W4 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_3_size))*grad_W4/(eps_init**2) + self.reg4*self.W4)).to(dtype)\n",
    "                self.b4 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_3_size))*grad_b4/(eps_init**2) + self.reg4*self.b4)).to(dtype)\n",
    "            if train_layer_5:\n",
    "                self.W5 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_4_size))*grad_W5/(eps_init**2) +  self.reg5*self.W5)).to(dtype)\n",
    "                self.b5 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_4_size))*grad_b5/(eps_init**2) + self.reg5*self.b5)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z5, h4, z4, h3, z3, h2, z2, h1, z1, grad_output, grad_z5, grad_h4, grad_z4, grad_h3, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3, grad_W4, grad_b4, grad_W5, grad_b5\n",
    "            gc.collect()\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n",
    "        \n",
    "        # Calcul de la dure d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\" \n",
    "    \n",
    "    \n",
    "class torch_network(nn.Module):\n",
    "    def __init__(self, list_of_layer_size, trained_layer_list, dropout = 0):\n",
    "        super(torch_network,self).__init__()\n",
    "        \n",
    "        # Classification type \n",
    "        self.binary = (list_of_layer_size[-1] == 1)\n",
    "    \n",
    "        # Last layer activation function\n",
    "        if self.binary:\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else : \n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Building the model\n",
    "        self.layers = []\n",
    "        self.dropout = dropout\n",
    "        if trained_layer_list == []:  \n",
    "            self.trained_list = [True for i in range(len(list_of_layer_size)-1)]\n",
    "        else : \n",
    "            self.trained_list = trained_layer_list\n",
    "        for i in range(1,len(list_of_layer_size)) : \n",
    "            # Adding layer mentionning if it should be trained\n",
    "            self.layers.append(nn.Linear(list_of_layer_size[i-1], list_of_layer_size[i]))\n",
    "            for params in self.layers[-1].parameters() :\n",
    "                params.requires_grad = self.trained_list[i-1]\n",
    "            self.layers.append(nn.Dropout(self.dropout))\n",
    "            if i < len(list_of_layer_size)-2:\n",
    "                self.layers.append(nn.ReLU())\n",
    "            else :\n",
    "                self.layers.append(self.activation)\n",
    "        self.model = nn.Sequential(*self.layers) \n",
    "\n",
    "        # Training arguments\n",
    "        self.fraction_minibatch = None\n",
    "        self.kappa = 0\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.trainable_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        self.optimizer = None\n",
    "        self.lr = None\n",
    "        self.lr_decay_rate = None\n",
    "        self.lr_decay_norm = None\n",
    "        self.lr_scheduler = None\n",
    "        \n",
    "        # Architecture\n",
    "        self.architecture = ''\n",
    "        \n",
    "        # Recording parameters\n",
    "        self.observation_interval = 0\n",
    "        self.training_loss_trajectory = []\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        \n",
    "    def training_layers(self, training_inputs, training_targets, validation_inputs, validation_targets, kappa=2, observation_interval=10, lr=1e-2, lr_decay_rate=1e8, lr_decay_norm=0, trained_layer_list=[], fraction_minibatch=0.1):\n",
    "        \n",
    "        # Formating datas\n",
    "        training_inputs, training_targets = torch.tensor(training_inputs).to(device), torch.tensor(training_targets).to(device)\n",
    "        validation_inputs, validation_targets = torch.tensor(validation_inputs).to(device), torch.tensor(validation_targets).to(device)\n",
    "        \n",
    "        # Updating the trained list\n",
    "        if trained_layer_list != [] :\n",
    "            self.trained_list = trained_layer_list\n",
    "            \n",
    "        # Initializing optimizer\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.trainable_parameters, lr=self.lr)\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.lr_decay_norm = lr_decay_norm\n",
    "        self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, self.lr_decay_rate, self.lr_decay_norm)\n",
    "        \n",
    "        # Minibatching \n",
    "        self.fraction_minibatch = fraction_minibatch\n",
    "        random_indices = torch.randperm(int(training_inputs.shape[0]*self.fraction_minibatch))\n",
    "        minibatch_inputs = training_inputs[random_indices]\n",
    "        \n",
    "        # Determining number of iterations\n",
    "        self.kappa = kappa\n",
    "        n_iterations = training_inputs.shape[1]**self.kappa\n",
    "        n_datas = n_iterations*training_inputs.shape[0]*self.fraction_minibatch\n",
    "        \n",
    "        # Recording the observation interval\n",
    "        self.observation_interval = observation_interval\n",
    "        \n",
    "        # Training !\n",
    "        for epoch in range(n_iterations):\n",
    "            \n",
    "            # Make prediction\n",
    "            training_pred = self.model(minibatch_inputs)\n",
    "            validation_pred = self.model(validation_inputs)\n",
    "            \n",
    "            # Computing estimated loss\n",
    "            training_loss = torch.mean((training_pred - training_targets[random_indices])**2, dim=0)\n",
    "            \n",
    "            # Computing loss gradient by backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            training_loss.backward()\n",
    "            \n",
    "            # Updating parameters\n",
    "            self.optimizer.step()\n",
    "            self.lr_scheduler.step()\n",
    "                        \n",
    "            # Record performances\n",
    "            if epoch % self.observation_interval == 0 :\n",
    "                \n",
    "                # Computing generalisation loss and generalisation accuracy\n",
    "                validation_loss = torch.mean((validation_pred - validation_targets)**2, dim=0)\n",
    "                if self.binary: \n",
    "                    accuracy = torch.mean(((validation_pred > 0.5)&(validation_targets > 0.5)).to(dtype))\n",
    "                else :\n",
    "                    accuracy = torch.mean((torch.argmax(validation_pred, dim=1) == torch.argmax(validation_targets, dim=1)).to(dtype))\n",
    "                \n",
    "                # Record \n",
    "                self.training_loss_trajectory.append(training_loss)\n",
    "                self.validation_loss_trajectory.append(validation_loss)\n",
    "                self.accuracy_trajectory.append(accuracy)\n",
    "                \n",
    "                # Display informations about training\n",
    "                print(f\"Iteration {epoch} - Accuracy : {accuracy.item()} - Training loss : {training_loss.item()} - Validation loss : {validation_loss.item()}\")\n",
    "            \n",
    "        return 'Training done'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36584c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2_layer = binary_classification_two_layer_NN(1024, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57236d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2.4, the number of datas used for the training is 83886079.99999994 and the number of iterations is 41943.\n",
      "Output tensor([[0.6995],\n",
      "        [0.5169]])\n",
      "Iteration 0 Training loss 0.12678751349449158 Validation loss 0.12859347462654114 Accuracy 0.49300000071525574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9t/xvrp6p712_b4g4ttdrxvysvr0000gv/T/ipykernel_64813/757459398.py:427: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.5837],\n",
      "        [0.5544]])\n",
      "Iteration 10 Training loss 0.140453040599823 Validation loss 0.14142367243766785 Accuracy 0.4959999918937683\n",
      "Output tensor([[0.8067],\n",
      "        [0.6067]])\n",
      "Iteration 20 Training loss 0.12316486984491348 Validation loss 0.12196558713912964 Accuracy 0.5325000286102295\n",
      "Output tensor([[0.4844],\n",
      "        [0.8020]])\n",
      "Iteration 30 Training loss 0.11809499561786652 Validation loss 0.1169099360704422 Accuracy 0.5734999775886536\n",
      "Output tensor([[0.6002],\n",
      "        [0.6816]])\n",
      "Iteration 40 Training loss 0.11234699934720993 Validation loss 0.11221875250339508 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4366],\n",
      "        [0.7330]])\n",
      "Iteration 50 Training loss 0.11648080497980118 Validation loss 0.1161593645811081 Accuracy 0.593999981880188\n",
      "Output tensor([[0.6754],\n",
      "        [0.6890]])\n",
      "Iteration 60 Training loss 0.10626330226659775 Validation loss 0.108177550137043 Accuracy 0.6439999938011169\n",
      "Output tensor([[0.6005],\n",
      "        [0.8161]])\n",
      "Iteration 70 Training loss 0.10216909646987915 Validation loss 0.10169409960508347 Accuracy 0.6869999766349792\n",
      "Output tensor([[0.9394],\n",
      "        [0.8013]])\n",
      "Iteration 80 Training loss 0.10330317169427872 Validation loss 0.0993608832359314 Accuracy 0.6990000009536743\n",
      "Output tensor([[0.7462],\n",
      "        [0.5892]])\n",
      "Iteration 90 Training loss 0.09827868640422821 Validation loss 0.0976247489452362 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5549],\n",
      "        [0.9241]])\n",
      "Iteration 100 Training loss 0.09794864803552628 Validation loss 0.09787458926439285 Accuracy 0.7059999704360962\n",
      "Output tensor([[0.5592],\n",
      "        [0.5219]])\n",
      "Iteration 110 Training loss 0.09332037717103958 Validation loss 0.0895596295595169 Accuracy 0.7425000071525574\n",
      "Output tensor([[0.4070],\n",
      "        [0.4912]])\n",
      "Iteration 120 Training loss 0.08842996507883072 Validation loss 0.0873403549194336 Accuracy 0.7565000057220459\n",
      "Output tensor([[0.4445],\n",
      "        [0.6733]])\n",
      "Iteration 130 Training loss 0.08923521637916565 Validation loss 0.08415381610393524 Accuracy 0.7630000114440918\n",
      "Output tensor([[0.8556],\n",
      "        [0.5192]])\n",
      "Iteration 140 Training loss 0.09306706488132477 Validation loss 0.09155946224927902 Accuracy 0.7350000143051147\n",
      "Output tensor([[0.2516],\n",
      "        [0.2476]])\n",
      "Iteration 150 Training loss 0.08262376487255096 Validation loss 0.08240817487239838 Accuracy 0.7714999914169312\n",
      "Output tensor([[0.6919],\n",
      "        [0.8350]])\n",
      "Iteration 160 Training loss 0.08804067224264145 Validation loss 0.08518487960100174 Accuracy 0.7565000057220459\n",
      "Output tensor([[0.5953],\n",
      "        [0.6229]])\n",
      "Iteration 170 Training loss 0.0893474593758583 Validation loss 0.08806980401277542 Accuracy 0.7459999918937683\n",
      "Output tensor([[0.0850],\n",
      "        [0.2859]])\n",
      "Iteration 180 Training loss 0.08116747438907623 Validation loss 0.08296110481023788 Accuracy 0.7664999961853027\n",
      "Output tensor([[0.7055],\n",
      "        [0.5005]])\n",
      "Iteration 190 Training loss 0.08345021307468414 Validation loss 0.0815337523818016 Accuracy 0.7754999995231628\n",
      "Output tensor([[0.5158],\n",
      "        [0.8205]])\n",
      "Iteration 200 Training loss 0.07955117523670197 Validation loss 0.07895225286483765 Accuracy 0.7724999785423279\n",
      "Output tensor([[0.4831],\n",
      "        [0.4114]])\n",
      "Iteration 210 Training loss 0.08043801039457321 Validation loss 0.07918177545070648 Accuracy 0.7649999856948853\n",
      "Output tensor([[0.1305],\n",
      "        [0.6579]])\n",
      "Iteration 220 Training loss 0.08268347382545471 Validation loss 0.07990367710590363 Accuracy 0.7559999823570251\n",
      "Output tensor([[0.2664],\n",
      "        [0.4914]])\n",
      "Iteration 230 Training loss 0.0787656158208847 Validation loss 0.07910775393247604 Accuracy 0.7630000114440918\n",
      "Output tensor([[0.4397],\n",
      "        [0.4248]])\n",
      "Iteration 240 Training loss 0.07958247512578964 Validation loss 0.07834412902593613 Accuracy 0.7689999938011169\n",
      "Output tensor([[0.8025],\n",
      "        [0.6603]])\n",
      "Iteration 250 Training loss 0.07896792888641357 Validation loss 0.07747413963079453 Accuracy 0.7745000123977661\n",
      "Output tensor([[0.2721],\n",
      "        [0.6996]])\n",
      "Iteration 260 Training loss 0.07789897918701172 Validation loss 0.07743922621011734 Accuracy 0.7799999713897705\n",
      "Output tensor([[0.2749],\n",
      "        [0.4654]])\n",
      "Iteration 270 Training loss 0.07868295162916183 Validation loss 0.07669135183095932 Accuracy 0.777999997138977\n",
      "Output tensor([[0.4214],\n",
      "        [0.8780]])\n",
      "Iteration 280 Training loss 0.07495363801717758 Validation loss 0.07695043832063675 Accuracy 0.7829999923706055\n",
      "Output tensor([[0.2508],\n",
      "        [0.6936]])\n",
      "Iteration 290 Training loss 0.07611250132322311 Validation loss 0.07665864378213882 Accuracy 0.7754999995231628\n",
      "Output tensor([[0.6704],\n",
      "        [0.4024]])\n",
      "Iteration 300 Training loss 0.07601255923509598 Validation loss 0.07647786289453506 Accuracy 0.7854999899864197\n",
      "Output tensor([[0.3028],\n",
      "        [0.1448]])\n",
      "Iteration 310 Training loss 0.07447836548089981 Validation loss 0.07570614665746689 Accuracy 0.7850000262260437\n",
      "Output tensor([[0.5508],\n",
      "        [0.2667]])\n",
      "Iteration 320 Training loss 0.07668101787567139 Validation loss 0.07676388323307037 Accuracy 0.7699999809265137\n",
      "Output tensor([[0.5292],\n",
      "        [0.5459]])\n",
      "Iteration 330 Training loss 0.0763321965932846 Validation loss 0.07531756907701492 Accuracy 0.7835000157356262\n",
      "Output tensor([[0.4370],\n",
      "        [0.5528]])\n",
      "Iteration 340 Training loss 0.07612194865942001 Validation loss 0.075393907725811 Accuracy 0.7799999713897705\n",
      "Output tensor([[0.3487],\n",
      "        [0.0544]])\n",
      "Iteration 350 Training loss 0.07406408339738846 Validation loss 0.07527128607034683 Accuracy 0.781000018119812\n",
      "Output tensor([[0.5487],\n",
      "        [0.7395]])\n",
      "Iteration 360 Training loss 0.0737992525100708 Validation loss 0.07515724748373032 Accuracy 0.781499981880188\n",
      "Output tensor([[0.6020],\n",
      "        [0.5783]])\n",
      "Iteration 370 Training loss 0.07725487649440765 Validation loss 0.07468197494745255 Accuracy 0.7860000133514404\n",
      "Output tensor([[0.7024],\n",
      "        [0.6837]])\n",
      "Iteration 380 Training loss 0.07463082671165466 Validation loss 0.07472916692495346 Accuracy 0.7835000157356262\n",
      "Output tensor([[0.9499],\n",
      "        [0.0330]])\n",
      "Iteration 390 Training loss 0.0759330689907074 Validation loss 0.07438330352306366 Accuracy 0.7850000262260437\n",
      "Output tensor([[0.7213],\n",
      "        [0.5529]])\n",
      "Iteration 400 Training loss 0.07520478963851929 Validation loss 0.07473881542682648 Accuracy 0.7914999723434448\n",
      "Output tensor([[0.6104],\n",
      "        [0.3799]])\n",
      "Iteration 410 Training loss 0.07069490104913712 Validation loss 0.07398968935012817 Accuracy 0.7919999957084656\n",
      "Output tensor([[0.5344],\n",
      "        [0.4953]])\n",
      "Iteration 420 Training loss 0.07699045538902283 Validation loss 0.07377245277166367 Accuracy 0.7889999747276306\n",
      "Output tensor([[0.4480],\n",
      "        [0.7645]])\n",
      "Iteration 430 Training loss 0.07471486181020737 Validation loss 0.07384657114744186 Accuracy 0.7860000133514404\n",
      "Output tensor([[0.0694],\n",
      "        [0.5944]])\n",
      "Iteration 440 Training loss 0.07628143578767776 Validation loss 0.07369523495435715 Accuracy 0.7854999899864197\n",
      "Output tensor([[0.1872],\n",
      "        [0.5996]])\n",
      "Iteration 450 Training loss 0.07365672290325165 Validation loss 0.07339008152484894 Accuracy 0.7900000214576721\n",
      "Output tensor([[0.8726],\n",
      "        [0.0661]])\n",
      "Iteration 460 Training loss 0.07376694679260254 Validation loss 0.07457055896520615 Accuracy 0.7785000205039978\n",
      "Output tensor([[0.2018],\n",
      "        [0.4234]])\n",
      "Iteration 470 Training loss 0.07328027486801147 Validation loss 0.07417744398117065 Accuracy 0.7804999947547913\n",
      "Output tensor([[0.2577],\n",
      "        [0.8772]])\n",
      "Iteration 480 Training loss 0.07322757691144943 Validation loss 0.0731562152504921 Accuracy 0.7960000038146973\n",
      "Output tensor([[0.5589],\n",
      "        [0.3046]])\n",
      "Iteration 490 Training loss 0.07235989719629288 Validation loss 0.07379184663295746 Accuracy 0.7835000157356262\n",
      "Output tensor([[0.3125],\n",
      "        [0.3333]])\n",
      "Iteration 500 Training loss 0.07477547973394394 Validation loss 0.07283297926187515 Accuracy 0.7910000085830688\n",
      "Output tensor([[0.6325],\n",
      "        [0.4548]])\n",
      "Iteration 510 Training loss 0.07720745354890823 Validation loss 0.07415513694286346 Accuracy 0.7789999842643738\n",
      "Output tensor([[0.9758],\n",
      "        [0.1795]])\n",
      "Iteration 520 Training loss 0.07431217283010483 Validation loss 0.0726950392127037 Accuracy 0.7879999876022339\n",
      "Output tensor([[0.8681],\n",
      "        [0.4150]])\n",
      "Iteration 530 Training loss 0.07400628924369812 Validation loss 0.0724693015217781 Accuracy 0.7940000295639038\n",
      "Output tensor([[0.7380],\n",
      "        [0.0300]])\n",
      "Iteration 540 Training loss 0.07044434547424316 Validation loss 0.07241357862949371 Accuracy 0.796500027179718\n",
      "Output tensor([[0.1244],\n",
      "        [0.4741]])\n",
      "Iteration 550 Training loss 0.07548742741346359 Validation loss 0.0725761353969574 Accuracy 0.7875000238418579\n",
      "Output tensor([[0.9071],\n",
      "        [0.4359]])\n",
      "Iteration 560 Training loss 0.07201717048883438 Validation loss 0.07262543588876724 Accuracy 0.796999990940094\n",
      "Output tensor([[0.5757],\n",
      "        [0.3698]])\n",
      "Iteration 570 Training loss 0.07177038490772247 Validation loss 0.07222982496023178 Accuracy 0.7885000109672546\n",
      "Output tensor([[0.5838],\n",
      "        [0.7091]])\n",
      "Iteration 580 Training loss 0.07524983584880829 Validation loss 0.07196394354104996 Accuracy 0.7940000295639038\n",
      "Output tensor([[0.4021],\n",
      "        [0.5940]])\n",
      "Iteration 590 Training loss 0.07538162171840668 Validation loss 0.0720987394452095 Accuracy 0.7889999747276306\n",
      "Output tensor([[0.8764],\n",
      "        [0.1323]])\n",
      "Iteration 600 Training loss 0.07093598693609238 Validation loss 0.0719105526804924 Accuracy 0.7914999723434448\n",
      "Output tensor([[0.2163],\n",
      "        [0.6713]])\n",
      "Iteration 610 Training loss 0.07244887202978134 Validation loss 0.07186613231897354 Accuracy 0.7994999885559082\n",
      "Output tensor([[0.9959],\n",
      "        [0.9075]])\n",
      "Iteration 620 Training loss 0.06870809942483902 Validation loss 0.07164118438959122 Accuracy 0.796500027179718\n",
      "Output tensor([[0.0877],\n",
      "        [0.6779]])\n",
      "Iteration 630 Training loss 0.07227862626314163 Validation loss 0.0715569481253624 Accuracy 0.7950000166893005\n",
      "Output tensor([[0.6067],\n",
      "        [0.8387]])\n",
      "Iteration 640 Training loss 0.07196037471294403 Validation loss 0.07205638289451599 Accuracy 0.7894999980926514\n",
      "Output tensor([[0.4292],\n",
      "        [0.9404]])\n",
      "Iteration 650 Training loss 0.0726587325334549 Validation loss 0.0713658258318901 Accuracy 0.796500027179718\n",
      "Output tensor([[0.2481],\n",
      "        [0.1171]])\n",
      "Iteration 660 Training loss 0.073104128241539 Validation loss 0.07141769677400589 Accuracy 0.7929999828338623\n",
      "Output tensor([[0.4067],\n",
      "        [0.4204]])\n",
      "Iteration 670 Training loss 0.07430790364742279 Validation loss 0.07122498750686646 Accuracy 0.7954999804496765\n",
      "Output tensor([[0.5834],\n",
      "        [0.5723]])\n",
      "Iteration 680 Training loss 0.07304848730564117 Validation loss 0.07124520093202591 Accuracy 0.7954999804496765\n",
      "Output tensor([[0.4556],\n",
      "        [0.1307]])\n",
      "Iteration 690 Training loss 0.07028524577617645 Validation loss 0.0711587592959404 Accuracy 0.7960000038146973\n",
      "Output tensor([[0.6192],\n",
      "        [0.4641]])\n",
      "Iteration 700 Training loss 0.07034125924110413 Validation loss 0.07142890989780426 Accuracy 0.7894999980926514\n",
      "Output tensor([[0.3665],\n",
      "        [0.8700]])\n",
      "Iteration 710 Training loss 0.070027656853199 Validation loss 0.07096569985151291 Accuracy 0.7950000166893005\n",
      "Output tensor([[0.9494],\n",
      "        [0.7159]])\n",
      "Iteration 720 Training loss 0.07292961329221725 Validation loss 0.07101772725582123 Accuracy 0.7950000166893005\n",
      "Output tensor([[0.2439],\n",
      "        [0.3998]])\n",
      "Iteration 730 Training loss 0.06877172738313675 Validation loss 0.07151220738887787 Accuracy 0.7894999980926514\n",
      "Output tensor([[0.8084],\n",
      "        [0.0350]])\n",
      "Iteration 740 Training loss 0.07195794582366943 Validation loss 0.07102125883102417 Accuracy 0.7925000190734863\n",
      "Output tensor([[0.8133],\n",
      "        [0.2213]])\n",
      "Iteration 750 Training loss 0.07006886601448059 Validation loss 0.07071825861930847 Accuracy 0.7954999804496765\n",
      "Output tensor([[0.6954],\n",
      "        [0.1307]])\n",
      "Iteration 760 Training loss 0.07420837134122849 Validation loss 0.0707612857222557 Accuracy 0.796500027179718\n",
      "Output tensor([[0.2837],\n",
      "        [0.8671]])\n",
      "Iteration 770 Training loss 0.06911571323871613 Validation loss 0.07111784815788269 Accuracy 0.7914999723434448\n",
      "Output tensor([[0.2271],\n",
      "        [0.2268]])\n",
      "Iteration 780 Training loss 0.07224474847316742 Validation loss 0.07059769332408905 Accuracy 0.8029999732971191\n",
      "Output tensor([[0.7475],\n",
      "        [0.1117]])\n",
      "Iteration 790 Training loss 0.07050572335720062 Validation loss 0.07110246270895004 Accuracy 0.7929999828338623\n",
      "Output tensor([[0.6280],\n",
      "        [0.1529]])\n",
      "Iteration 800 Training loss 0.07141081243753433 Validation loss 0.07117576897144318 Accuracy 0.7919999957084656\n",
      "Output tensor([[0.0643],\n",
      "        [0.8887]])\n",
      "Iteration 810 Training loss 0.07198035717010498 Validation loss 0.07036767154932022 Accuracy 0.796999990940094\n",
      "Output tensor([[0.2405],\n",
      "        [0.8970]])\n",
      "Iteration 820 Training loss 0.07357046008110046 Validation loss 0.07030430436134338 Accuracy 0.7979999780654907\n",
      "Output tensor([[0.5004],\n",
      "        [0.5778]])\n",
      "Iteration 830 Training loss 0.07153443247079849 Validation loss 0.07074924558401108 Accuracy 0.7940000295639038\n",
      "Output tensor([[0.9794],\n",
      "        [0.6442]])\n",
      "Iteration 840 Training loss 0.07098205387592316 Validation loss 0.0710253119468689 Accuracy 0.7904999852180481\n",
      "Output tensor([[0.6756],\n",
      "        [0.5358]])\n",
      "Iteration 850 Training loss 0.07075623422861099 Validation loss 0.07013224065303802 Accuracy 0.7990000247955322\n",
      "Output tensor([[0.4216],\n",
      "        [0.9361]])\n",
      "Iteration 860 Training loss 0.06906849145889282 Validation loss 0.07032818347215652 Accuracy 0.796500027179718\n",
      "Output tensor([[0.4277],\n",
      "        [0.1145]])\n",
      "Iteration 870 Training loss 0.06936781853437424 Validation loss 0.07092881947755814 Accuracy 0.7914999723434448\n",
      "Output tensor([[0.2689],\n",
      "        [0.8881]])\n",
      "Iteration 880 Training loss 0.0674314871430397 Validation loss 0.07017284631729126 Accuracy 0.796999990940094\n",
      "Output tensor([[0.4636],\n",
      "        [0.0695]])\n",
      "Iteration 890 Training loss 0.07052100449800491 Validation loss 0.07032980769872665 Accuracy 0.7960000038146973\n",
      "Output tensor([[0.2260],\n",
      "        [0.3102]])\n",
      "Iteration 900 Training loss 0.07156331837177277 Validation loss 0.07069511711597443 Accuracy 0.7929999828338623\n",
      "Output tensor([[0.8451],\n",
      "        [0.2622]])\n",
      "Iteration 910 Training loss 0.07094349712133408 Validation loss 0.06993589550256729 Accuracy 0.7985000014305115\n",
      "Output tensor([[0.0672],\n",
      "        [0.0351]])\n",
      "Iteration 920 Training loss 0.06751438975334167 Validation loss 0.07015529274940491 Accuracy 0.7960000038146973\n",
      "Output tensor([[0.9341],\n",
      "        [0.2622]])\n",
      "Iteration 930 Training loss 0.07268352806568146 Validation loss 0.07030636072158813 Accuracy 0.796500027179718\n",
      "Output tensor([[0.9778],\n",
      "        [0.4530]])\n",
      "Iteration 940 Training loss 0.0703153908252716 Validation loss 0.06965742260217667 Accuracy 0.8019999861717224\n",
      "Output tensor([[0.3192],\n",
      "        [0.1765]])\n",
      "Iteration 950 Training loss 0.07055198401212692 Validation loss 0.07008549571037292 Accuracy 0.796500027179718\n",
      "Output tensor([[0.1358],\n",
      "        [0.0435]])\n",
      "Iteration 960 Training loss 0.07360690087080002 Validation loss 0.06961549818515778 Accuracy 0.800000011920929\n",
      "Output tensor([[0.8251],\n",
      "        [0.5030]])\n",
      "Iteration 970 Training loss 0.06938676536083221 Validation loss 0.07017349451780319 Accuracy 0.796999990940094\n",
      "Output tensor([[0.8886],\n",
      "        [0.8915]])\n",
      "Iteration 980 Training loss 0.07104026526212692 Validation loss 0.06967392563819885 Accuracy 0.7975000143051147\n",
      "Output tensor([[0.1375],\n",
      "        [0.6873]])\n",
      "Iteration 990 Training loss 0.06907732039690018 Validation loss 0.06946069002151489 Accuracy 0.8009999990463257\n",
      "Output tensor([[0.8932],\n",
      "        [0.6597]])\n",
      "Iteration 1000 Training loss 0.06700529903173447 Validation loss 0.06947441399097443 Accuracy 0.8034999966621399\n",
      "Output tensor([[0.8452],\n",
      "        [0.3861]])\n",
      "Iteration 1010 Training loss 0.07307800650596619 Validation loss 0.06939534842967987 Accuracy 0.8004999756813049\n",
      "Output tensor([[0.2173],\n",
      "        [0.4252]])\n",
      "Iteration 1020 Training loss 0.06852878630161285 Validation loss 0.06937375664710999 Accuracy 0.8069999814033508\n",
      "Output tensor([[0.7119],\n",
      "        [0.1603]])\n",
      "Iteration 1030 Training loss 0.07336323708295822 Validation loss 0.06925061345100403 Accuracy 0.8029999732971191\n",
      "Output tensor([[0.1166],\n",
      "        [0.2202]])\n",
      "Iteration 1040 Training loss 0.06902054697275162 Validation loss 0.06965208798646927 Accuracy 0.7990000247955322\n",
      "Output tensor([[0.8522],\n",
      "        [0.4558]])\n",
      "Iteration 1050 Training loss 0.06949654966592789 Validation loss 0.06915616244077682 Accuracy 0.8034999966621399\n",
      "Output tensor([[0.2168],\n",
      "        [0.4164]])\n",
      "Iteration 1060 Training loss 0.06994687765836716 Validation loss 0.06916514784097672 Accuracy 0.8015000224113464\n",
      "Output tensor([[0.0570],\n",
      "        [0.1439]])\n",
      "Iteration 1070 Training loss 0.07215303182601929 Validation loss 0.0691978931427002 Accuracy 0.8004999756813049\n",
      "Output tensor([[0.2845],\n",
      "        [0.2691]])\n",
      "Iteration 1080 Training loss 0.07218536734580994 Validation loss 0.06941843777894974 Accuracy 0.800000011920929\n",
      "Output tensor([[0.1333],\n",
      "        [0.7123]])\n",
      "Iteration 1090 Training loss 0.07127155363559723 Validation loss 0.06907405704259872 Accuracy 0.8029999732971191\n",
      "Output tensor([[0.1267],\n",
      "        [0.9206]])\n",
      "Iteration 1100 Training loss 0.07060068845748901 Validation loss 0.0691288635134697 Accuracy 0.8025000095367432\n",
      "Output tensor([[0.6327],\n",
      "        [0.2257]])\n",
      "Iteration 1110 Training loss 0.07064269483089447 Validation loss 0.06895210593938828 Accuracy 0.8065000176429749\n",
      "Output tensor([[0.1975],\n",
      "        [0.3591]])\n",
      "Iteration 1120 Training loss 0.07111240923404694 Validation loss 0.06946567445993423 Accuracy 0.7990000247955322\n",
      "Output tensor([[0.7541],\n",
      "        [0.1530]])\n",
      "Iteration 1130 Training loss 0.06884529441595078 Validation loss 0.06901571899652481 Accuracy 0.8025000095367432\n",
      "Output tensor([[0.0719],\n",
      "        [0.0761]])\n",
      "Iteration 1140 Training loss 0.07110610604286194 Validation loss 0.06889156997203827 Accuracy 0.8040000200271606\n",
      "Output tensor([[0.3154],\n",
      "        [0.0852]])\n",
      "Iteration 1150 Training loss 0.06845059990882874 Validation loss 0.068911112844944 Accuracy 0.8019999861717224\n",
      "Output tensor([[0.8013],\n",
      "        [0.9457]])\n",
      "Iteration 1160 Training loss 0.07058296352624893 Validation loss 0.06898518651723862 Accuracy 0.8025000095367432\n",
      "Output tensor([[0.5943],\n",
      "        [0.5121]])\n",
      "Iteration 1170 Training loss 0.06844723969697952 Validation loss 0.0688149705529213 Accuracy 0.8105000257492065\n",
      "Output tensor([[0.1137],\n",
      "        [0.6808]])\n",
      "Iteration 1180 Training loss 0.06782745569944382 Validation loss 0.06881346553564072 Accuracy 0.8130000233650208\n",
      "Output tensor([[0.8743],\n",
      "        [0.9487]])\n",
      "Iteration 1190 Training loss 0.06940957903862 Validation loss 0.0687791258096695 Accuracy 0.8044999837875366\n",
      "Output tensor([[0.4368],\n",
      "        [0.3427]])\n",
      "Iteration 1200 Training loss 0.07094214111566544 Validation loss 0.06865940988063812 Accuracy 0.8050000071525574\n",
      "Output tensor([[0.0863],\n",
      "        [0.9251]])\n",
      "Iteration 1210 Training loss 0.07130604237318039 Validation loss 0.06881753355264664 Accuracy 0.8019999861717224\n",
      "Output tensor([[0.2461],\n",
      "        [0.7534]])\n",
      "Iteration 1220 Training loss 0.06810721755027771 Validation loss 0.06905096769332886 Accuracy 0.800000011920929\n",
      "Output tensor([[0.0652],\n",
      "        [0.6487]])\n",
      "Iteration 1230 Training loss 0.06798577308654785 Validation loss 0.06867508590221405 Accuracy 0.8025000095367432\n",
      "Output tensor([[0.9244],\n",
      "        [0.0839]])\n",
      "Iteration 1240 Training loss 0.06890515983104706 Validation loss 0.0693584755063057 Accuracy 0.8004999756813049\n",
      "Output tensor([[0.6593],\n",
      "        [0.9901]])\n",
      "Iteration 1250 Training loss 0.07439558207988739 Validation loss 0.0686764121055603 Accuracy 0.8015000224113464\n",
      "Output tensor([[0.5622],\n",
      "        [0.4539]])\n",
      "Iteration 1260 Training loss 0.07056796550750732 Validation loss 0.06893560290336609 Accuracy 0.8009999990463257\n",
      "Output tensor([[0.0927],\n",
      "        [0.7867]])\n",
      "Iteration 1270 Training loss 0.0674554705619812 Validation loss 0.06841715425252914 Accuracy 0.8084999918937683\n",
      "Output tensor([[0.4701],\n",
      "        [0.7231]])\n",
      "Iteration 1280 Training loss 0.06847398728132248 Validation loss 0.0683823823928833 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.5789],\n",
      "        [0.9825]])\n",
      "Iteration 1290 Training loss 0.06663249433040619 Validation loss 0.06840662658214569 Accuracy 0.8065000176429749\n",
      "Output tensor([[0.7828],\n",
      "        [0.8532]])\n",
      "Iteration 1300 Training loss 0.0691513791680336 Validation loss 0.06837158650159836 Accuracy 0.8075000047683716\n",
      "Output tensor([[0.8560],\n",
      "        [0.6405]])\n",
      "Iteration 1310 Training loss 0.07073195278644562 Validation loss 0.06916498392820358 Accuracy 0.800000011920929\n",
      "Output tensor([[0.8495],\n",
      "        [0.1189]])\n",
      "Iteration 1320 Training loss 0.06978587061166763 Validation loss 0.0686042383313179 Accuracy 0.8044999837875366\n",
      "Output tensor([[0.8045],\n",
      "        [0.5001]])\n",
      "Iteration 1330 Training loss 0.06853433698415756 Validation loss 0.06913554668426514 Accuracy 0.800000011920929\n",
      "Output tensor([[0.8761],\n",
      "        [0.0908]])\n",
      "Iteration 1340 Training loss 0.07030468434095383 Validation loss 0.06835659593343735 Accuracy 0.8069999814033508\n",
      "Output tensor([[0.9884],\n",
      "        [0.2256]])\n",
      "Iteration 1350 Training loss 0.06638279557228088 Validation loss 0.06818810850381851 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.8967],\n",
      "        [0.1503]])\n",
      "Iteration 1360 Training loss 0.06959904730319977 Validation loss 0.06837146729230881 Accuracy 0.8034999966621399\n",
      "Output tensor([[0.7890],\n",
      "        [0.2641]])\n",
      "Iteration 1370 Training loss 0.06598985195159912 Validation loss 0.0682910904288292 Accuracy 0.8059999942779541\n",
      "Output tensor([[0.0958],\n",
      "        [0.9581]])\n",
      "Iteration 1380 Training loss 0.06822621077299118 Validation loss 0.06852655112743378 Accuracy 0.8029999732971191\n",
      "Output tensor([[0.6835],\n",
      "        [0.4002]])\n",
      "Iteration 1390 Training loss 0.06598212569952011 Validation loss 0.0681639239192009 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.1656],\n",
      "        [0.0414]])\n",
      "Iteration 1400 Training loss 0.07055339217185974 Validation loss 0.06834492832422256 Accuracy 0.8059999942779541\n",
      "Output tensor([[0.4998],\n",
      "        [0.7499]])\n",
      "Iteration 1410 Training loss 0.07001037150621414 Validation loss 0.06803426146507263 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.1581],\n",
      "        [0.8864]])\n",
      "Iteration 1420 Training loss 0.06910405308008194 Validation loss 0.06814058125019073 Accuracy 0.8069999814033508\n",
      "Output tensor([[0.9096],\n",
      "        [0.9902]])\n",
      "Iteration 1430 Training loss 0.06751397997140884 Validation loss 0.06805764138698578 Accuracy 0.8115000128746033\n",
      "Output tensor([[0.2523],\n",
      "        [0.0565]])\n",
      "Iteration 1440 Training loss 0.06799304485321045 Validation loss 0.0679420530796051 Accuracy 0.8105000257492065\n",
      "Output tensor([[0.6446],\n",
      "        [0.0132]])\n",
      "Iteration 1450 Training loss 0.06911605596542358 Validation loss 0.06824856996536255 Accuracy 0.8059999942779541\n",
      "Output tensor([[0.2336],\n",
      "        [0.8043]])\n",
      "Iteration 1460 Training loss 0.06752496212720871 Validation loss 0.06784778088331223 Accuracy 0.8125\n",
      "Output tensor([[0.5748],\n",
      "        [0.0994]])\n",
      "Iteration 1470 Training loss 0.06511425226926804 Validation loss 0.0683467835187912 Accuracy 0.8054999709129333\n",
      "Output tensor([[0.4357],\n",
      "        [0.4354]])\n",
      "Iteration 1480 Training loss 0.07010466605424881 Validation loss 0.0678199902176857 Accuracy 0.8109999895095825\n",
      "Output tensor([[0.0167],\n",
      "        [0.7374]])\n",
      "Iteration 1490 Training loss 0.06719747185707092 Validation loss 0.06782268732786179 Accuracy 0.8125\n",
      "Output tensor([[0.5031],\n",
      "        [0.8755]])\n",
      "Iteration 1500 Training loss 0.07341007888317108 Validation loss 0.06830857694149017 Accuracy 0.8059999942779541\n",
      "Output tensor([[0.8100],\n",
      "        [0.5375]])\n",
      "Iteration 1510 Training loss 0.06969723850488663 Validation loss 0.06831520050764084 Accuracy 0.8059999942779541\n",
      "Output tensor([[0.5439],\n",
      "        [0.8049]])\n",
      "Iteration 1520 Training loss 0.06901434063911438 Validation loss 0.06773295998573303 Accuracy 0.809499979019165\n",
      "Output tensor([[0.9305],\n",
      "        [0.7324]])\n",
      "Iteration 1530 Training loss 0.06829940527677536 Validation loss 0.06774105876684189 Accuracy 0.8125\n",
      "Output tensor([[0.1366],\n",
      "        [0.4271]])\n",
      "Iteration 1540 Training loss 0.0676705613732338 Validation loss 0.06774605810642242 Accuracy 0.8115000128746033\n",
      "Output tensor([[0.0789],\n",
      "        [0.9086]])\n",
      "Iteration 1550 Training loss 0.06758752465248108 Validation loss 0.0678168460726738 Accuracy 0.809499979019165\n",
      "Output tensor([[0.0675],\n",
      "        [0.7474]])\n",
      "Iteration 1560 Training loss 0.06573815643787384 Validation loss 0.06760088354349136 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.9572],\n",
      "        [0.7570]])\n",
      "Iteration 1570 Training loss 0.06760097295045853 Validation loss 0.06802253425121307 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.1013],\n",
      "        [0.8539]])\n",
      "Iteration 1580 Training loss 0.06700041145086288 Validation loss 0.06760302186012268 Accuracy 0.8130000233650208\n",
      "Output tensor([[0.8842],\n",
      "        [0.7777]])\n",
      "Iteration 1590 Training loss 0.06914935261011124 Validation loss 0.06765375286340714 Accuracy 0.8125\n",
      "Output tensor([[0.1478],\n",
      "        [0.8343]])\n",
      "Iteration 1600 Training loss 0.06581316888332367 Validation loss 0.06802425533533096 Accuracy 0.8080000281333923\n",
      "Output tensor([[0.0380],\n",
      "        [0.7164]])\n",
      "Iteration 1610 Training loss 0.07000291347503662 Validation loss 0.06752075999975204 Accuracy 0.8115000128746033\n",
      "Output tensor([[0.7335],\n",
      "        [0.0156]])\n",
      "Iteration 1620 Training loss 0.06682931631803513 Validation loss 0.06764599680900574 Accuracy 0.8119999766349792\n",
      "Output tensor([[0.5287],\n",
      "        [0.3380]])\n",
      "Iteration 1630 Training loss 0.067789226770401 Validation loss 0.06758802384138107 Accuracy 0.8125\n",
      "Output tensor([[0.7316],\n",
      "        [0.9646]])\n",
      "Iteration 1640 Training loss 0.06935517489910126 Validation loss 0.06747183948755264 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.0422],\n",
      "        [0.6396]])\n",
      "Iteration 1650 Training loss 0.06491340696811676 Validation loss 0.06790528446435928 Accuracy 0.8084999918937683\n",
      "Output tensor([[0.7662],\n",
      "        [0.1035]])\n",
      "Iteration 1660 Training loss 0.0649806410074234 Validation loss 0.06759318709373474 Accuracy 0.8100000023841858\n",
      "Output tensor([[0.9506],\n",
      "        [0.8435]])\n",
      "Iteration 1670 Training loss 0.06458818167448044 Validation loss 0.06740966439247131 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.9506],\n",
      "        [0.2995]])\n",
      "Iteration 1680 Training loss 0.06524252146482468 Validation loss 0.06762698292732239 Accuracy 0.809499979019165\n",
      "Output tensor([[0.2847],\n",
      "        [0.1372]])\n",
      "Iteration 1690 Training loss 0.06452026218175888 Validation loss 0.06793560832738876 Accuracy 0.8080000281333923\n",
      "Output tensor([[0.1298],\n",
      "        [0.8050]])\n",
      "Iteration 1700 Training loss 0.06747941672801971 Validation loss 0.06751582771539688 Accuracy 0.8105000257492065\n",
      "Output tensor([[0.3632],\n",
      "        [0.8460]])\n",
      "Iteration 1710 Training loss 0.06679617613554001 Validation loss 0.06746496260166168 Accuracy 0.8100000023841858\n",
      "Output tensor([[0.2294],\n",
      "        [0.6138]])\n",
      "Iteration 1720 Training loss 0.06634298712015152 Validation loss 0.06740683317184448 Accuracy 0.8115000128746033\n",
      "Output tensor([[0.8799],\n",
      "        [0.3797]])\n",
      "Iteration 1730 Training loss 0.06537401676177979 Validation loss 0.06755931675434113 Accuracy 0.8105000257492065\n",
      "Output tensor([[0.7475],\n",
      "        [0.7923]])\n",
      "Iteration 1740 Training loss 0.06909368932247162 Validation loss 0.06732136011123657 Accuracy 0.8119999766349792\n",
      "Output tensor([[0.9323],\n",
      "        [0.0507]])\n",
      "Iteration 1750 Training loss 0.07094404846429825 Validation loss 0.06744303554296494 Accuracy 0.8109999895095825\n",
      "Output tensor([[0.0480],\n",
      "        [0.2112]])\n",
      "Iteration 1760 Training loss 0.06462831050157547 Validation loss 0.06752414256334305 Accuracy 0.8109999895095825\n",
      "Output tensor([[0.6711],\n",
      "        [0.3432]])\n",
      "Iteration 1770 Training loss 0.06986384838819504 Validation loss 0.06752867996692657 Accuracy 0.8100000023841858\n",
      "Output tensor([[0.9051],\n",
      "        [0.3804]])\n",
      "Iteration 1780 Training loss 0.06722795218229294 Validation loss 0.06717661768198013 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.7541],\n",
      "        [0.2219]])\n",
      "Iteration 1790 Training loss 0.06838329136371613 Validation loss 0.06716708093881607 Accuracy 0.8159999847412109\n",
      "Output tensor([[0.0868],\n",
      "        [0.7942]])\n",
      "Iteration 1800 Training loss 0.06531202793121338 Validation loss 0.06725719571113586 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.8129],\n",
      "        [0.9497]])\n",
      "Iteration 1810 Training loss 0.06786182522773743 Validation loss 0.06788360327482224 Accuracy 0.8084999918937683\n",
      "Output tensor([[0.5695],\n",
      "        [0.4596]])\n",
      "Iteration 1820 Training loss 0.06675828993320465 Validation loss 0.06755769997835159 Accuracy 0.809499979019165\n",
      "Output tensor([[0.9354],\n",
      "        [0.8881]])\n",
      "Iteration 1830 Training loss 0.06792110204696655 Validation loss 0.06711871922016144 Accuracy 0.8159999847412109\n",
      "Output tensor([[0.9524],\n",
      "        [0.4398]])\n",
      "Iteration 1840 Training loss 0.06641171127557755 Validation loss 0.06732267886400223 Accuracy 0.8119999766349792\n",
      "Output tensor([[0.1775],\n",
      "        [0.0271]])\n",
      "Iteration 1850 Training loss 0.06847751140594482 Validation loss 0.06712889671325684 Accuracy 0.8144999742507935\n",
      "Output tensor([[0.7687],\n",
      "        [0.3631]])\n",
      "Iteration 1860 Training loss 0.06493230909109116 Validation loss 0.06705737113952637 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.7180],\n",
      "        [0.5251]])\n",
      "Iteration 1870 Training loss 0.06729923188686371 Validation loss 0.06745663285255432 Accuracy 0.809499979019165\n",
      "Output tensor([[0.7757],\n",
      "        [0.7104]])\n",
      "Iteration 1880 Training loss 0.06906227022409439 Validation loss 0.0674394890666008 Accuracy 0.8100000023841858\n",
      "Output tensor([[0.8315],\n",
      "        [0.0599]])\n",
      "Iteration 1890 Training loss 0.0681828036904335 Validation loss 0.06721910089254379 Accuracy 0.8119999766349792\n",
      "Output tensor([[0.6037],\n",
      "        [0.9606]])\n",
      "Iteration 1900 Training loss 0.06843402236700058 Validation loss 0.06695429980754852 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.5935],\n",
      "        [0.7167]])\n",
      "Iteration 1910 Training loss 0.06641287356615067 Validation loss 0.0669265165925026 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.5138],\n",
      "        [0.8120]])\n",
      "Iteration 1920 Training loss 0.06861262023448944 Validation loss 0.06759873777627945 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.5641],\n",
      "        [0.0612]])\n",
      "Iteration 1930 Training loss 0.06718702614307404 Validation loss 0.06692300736904144 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.6466],\n",
      "        [0.9771]])\n",
      "Iteration 1940 Training loss 0.06850193440914154 Validation loss 0.06688400357961655 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.1857],\n",
      "        [0.7309]])\n",
      "Iteration 1950 Training loss 0.06715736538171768 Validation loss 0.06697806715965271 Accuracy 0.8130000233650208\n",
      "Output tensor([[0.6093],\n",
      "        [0.0660]])\n",
      "Iteration 1960 Training loss 0.06584285944700241 Validation loss 0.06684177368879318 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.0634],\n",
      "        [0.9765]])\n",
      "Iteration 1970 Training loss 0.0660204142332077 Validation loss 0.0668950155377388 Accuracy 0.815500020980835\n",
      "Output tensor([[0.0235],\n",
      "        [0.4607]])\n",
      "Iteration 1980 Training loss 0.06563261151313782 Validation loss 0.06685980409383774 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.3038],\n",
      "        [0.5026]])\n",
      "Iteration 1990 Training loss 0.0675821527838707 Validation loss 0.0668080523610115 Accuracy 0.8159999847412109\n",
      "Output tensor([[0.3467],\n",
      "        [0.0386]])\n",
      "Iteration 2000 Training loss 0.0683431625366211 Validation loss 0.06781730055809021 Accuracy 0.8084999918937683\n",
      "Output tensor([[0.0462],\n",
      "        [0.9916]])\n",
      "Iteration 2010 Training loss 0.06873169541358948 Validation loss 0.06686758249998093 Accuracy 0.8144999742507935\n",
      "Output tensor([[0.9641],\n",
      "        [0.4720]])\n",
      "Iteration 2020 Training loss 0.06851797550916672 Validation loss 0.06710318475961685 Accuracy 0.8130000233650208\n",
      "Output tensor([[0.5847],\n",
      "        [0.3186]])\n",
      "Iteration 2030 Training loss 0.06604896485805511 Validation loss 0.0670047476887703 Accuracy 0.8134999871253967\n",
      "Output tensor([[0.2302],\n",
      "        [0.0331]])\n",
      "Iteration 2040 Training loss 0.06726773083209991 Validation loss 0.06688571721315384 Accuracy 0.8125\n",
      "Output tensor([[0.4068],\n",
      "        [0.0442]])\n",
      "Iteration 2050 Training loss 0.06916944682598114 Validation loss 0.06670871376991272 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.0621],\n",
      "        [0.6462]])\n",
      "Iteration 2060 Training loss 0.06466133892536163 Validation loss 0.06707075238227844 Accuracy 0.8125\n",
      "Output tensor([[0.7154],\n",
      "        [0.9136]])\n",
      "Iteration 2070 Training loss 0.06754588335752487 Validation loss 0.06665608286857605 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.1090],\n",
      "        [0.7921]])\n",
      "Iteration 2080 Training loss 0.06989768892526627 Validation loss 0.06662259995937347 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.0351],\n",
      "        [0.0366]])\n",
      "Iteration 2090 Training loss 0.06436382979154587 Validation loss 0.06666071712970734 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.9344],\n",
      "        [0.8772]])\n",
      "Iteration 2100 Training loss 0.06883933395147324 Validation loss 0.06659389287233353 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.0919],\n",
      "        [0.1310]])\n",
      "Iteration 2110 Training loss 0.06566056609153748 Validation loss 0.06668313592672348 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.8412],\n",
      "        [0.8484]])\n",
      "Iteration 2120 Training loss 0.06703685224056244 Validation loss 0.06709497421979904 Accuracy 0.8109999895095825\n",
      "Output tensor([[0.3234],\n",
      "        [0.0781]])\n",
      "Iteration 2130 Training loss 0.06667052954435349 Validation loss 0.06677564978599548 Accuracy 0.8134999871253967\n",
      "Output tensor([[0.0632],\n",
      "        [0.5869]])\n",
      "Iteration 2140 Training loss 0.06786219030618668 Validation loss 0.06666876375675201 Accuracy 0.8134999871253967\n",
      "Output tensor([[0.0711],\n",
      "        [0.1499]])\n",
      "Iteration 2150 Training loss 0.06804441660642624 Validation loss 0.06705910712480545 Accuracy 0.8119999766349792\n",
      "Output tensor([[0.8726],\n",
      "        [0.0604]])\n",
      "Iteration 2160 Training loss 0.06787816435098648 Validation loss 0.06687657535076141 Accuracy 0.8125\n",
      "Output tensor([[0.4783],\n",
      "        [0.1136]])\n",
      "Iteration 2170 Training loss 0.06898973137140274 Validation loss 0.06656309962272644 Accuracy 0.8109999895095825\n",
      "Output tensor([[0.1793],\n",
      "        [0.6070]])\n",
      "Iteration 2180 Training loss 0.06653617322444916 Validation loss 0.06699880957603455 Accuracy 0.8119999766349792\n",
      "Output tensor([[0.5825],\n",
      "        [0.0080]])\n",
      "Iteration 2190 Training loss 0.06954403966665268 Validation loss 0.06659761816263199 Accuracy 0.815500020980835\n",
      "Output tensor([[0.8309],\n",
      "        [0.6166]])\n",
      "Iteration 2200 Training loss 0.0644153580069542 Validation loss 0.06658422201871872 Accuracy 0.8144999742507935\n",
      "Output tensor([[0.9639],\n",
      "        [0.2672]])\n",
      "Iteration 2210 Training loss 0.06602180004119873 Validation loss 0.06667986512184143 Accuracy 0.815500020980835\n",
      "Output tensor([[0.3082],\n",
      "        [0.1214]])\n",
      "Iteration 2220 Training loss 0.06465698033571243 Validation loss 0.06652995198965073 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.8197],\n",
      "        [0.9251]])\n",
      "Iteration 2230 Training loss 0.06467494368553162 Validation loss 0.06646427512168884 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.7801],\n",
      "        [0.9639]])\n",
      "Iteration 2240 Training loss 0.0681886300444603 Validation loss 0.06642275303602219 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.3216],\n",
      "        [0.5831]])\n",
      "Iteration 2250 Training loss 0.06350026279687881 Validation loss 0.06684903055429459 Accuracy 0.8119999766349792\n",
      "Output tensor([[0.0632],\n",
      "        [0.8040]])\n",
      "Iteration 2260 Training loss 0.0676412507891655 Validation loss 0.06637990474700928 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.8418],\n",
      "        [0.2078]])\n",
      "Iteration 2270 Training loss 0.06837297976016998 Validation loss 0.06659133732318878 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.9673],\n",
      "        [0.6372]])\n",
      "Iteration 2280 Training loss 0.06714744865894318 Validation loss 0.06678683310747147 Accuracy 0.8125\n",
      "Output tensor([[0.3810],\n",
      "        [0.4224]])\n",
      "Iteration 2290 Training loss 0.06464394181966782 Validation loss 0.06635551154613495 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.8456],\n",
      "        [0.8561]])\n",
      "Iteration 2300 Training loss 0.06958898901939392 Validation loss 0.0663919597864151 Accuracy 0.815500020980835\n",
      "Output tensor([[0.7960],\n",
      "        [0.2982]])\n",
      "Iteration 2310 Training loss 0.07050830125808716 Validation loss 0.06677646934986115 Accuracy 0.8119999766349792\n",
      "Output tensor([[0.0514],\n",
      "        [0.5752]])\n",
      "Iteration 2320 Training loss 0.06581812351942062 Validation loss 0.0665360614657402 Accuracy 0.8134999871253967\n",
      "Output tensor([[0.0786],\n",
      "        [0.1167]])\n",
      "Iteration 2330 Training loss 0.06380217522382736 Validation loss 0.06636478006839752 Accuracy 0.8159999847412109\n",
      "Output tensor([[0.1368],\n",
      "        [0.8400]])\n",
      "Iteration 2340 Training loss 0.06638257950544357 Validation loss 0.066322460770607 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.9985],\n",
      "        [0.5944]])\n",
      "Iteration 2350 Training loss 0.06834061443805695 Validation loss 0.06627540290355682 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.1471],\n",
      "        [0.2826]])\n",
      "Iteration 2360 Training loss 0.06579386442899704 Validation loss 0.06665500998497009 Accuracy 0.8134999871253967\n",
      "Output tensor([[0.3596],\n",
      "        [0.8213]])\n",
      "Iteration 2370 Training loss 0.06675935536623001 Validation loss 0.06626714766025543 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.7585],\n",
      "        [0.7446]])\n",
      "Iteration 2380 Training loss 0.0659080222249031 Validation loss 0.06628326326608658 Accuracy 0.8159999847412109\n",
      "Output tensor([[0.2137],\n",
      "        [0.1812]])\n",
      "Iteration 2390 Training loss 0.06774915009737015 Validation loss 0.0666716918349266 Accuracy 0.8125\n",
      "Output tensor([[0.7165],\n",
      "        [0.9926]])\n",
      "Iteration 2400 Training loss 0.0682695060968399 Validation loss 0.06656475365161896 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.9322],\n",
      "        [0.9401]])\n",
      "Iteration 2410 Training loss 0.06465818732976913 Validation loss 0.06621262431144714 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.9816],\n",
      "        [0.8848]])\n",
      "Iteration 2420 Training loss 0.06445565074682236 Validation loss 0.06672349572181702 Accuracy 0.8115000128746033\n",
      "Output tensor([[0.0265],\n",
      "        [0.5943]])\n",
      "Iteration 2430 Training loss 0.0654762014746666 Validation loss 0.0662900060415268 Accuracy 0.8144999742507935\n",
      "Output tensor([[0.4962],\n",
      "        [0.1588]])\n",
      "Iteration 2440 Training loss 0.06744080781936646 Validation loss 0.06706272810697556 Accuracy 0.8119999766349792\n",
      "Output tensor([[0.8422],\n",
      "        [0.0455]])\n",
      "Iteration 2450 Training loss 0.06585832685232162 Validation loss 0.06616760045289993 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.0554],\n",
      "        [0.4427]])\n",
      "Iteration 2460 Training loss 0.06394190341234207 Validation loss 0.06634481251239777 Accuracy 0.8125\n",
      "Output tensor([[0.9794],\n",
      "        [0.3578]])\n",
      "Iteration 2470 Training loss 0.0690421462059021 Validation loss 0.0663658007979393 Accuracy 0.8125\n",
      "Output tensor([[0.7481],\n",
      "        [0.1456]])\n",
      "Iteration 2480 Training loss 0.06543971598148346 Validation loss 0.06601658463478088 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.3694],\n",
      "        [0.7834]])\n",
      "Iteration 2490 Training loss 0.06903643161058426 Validation loss 0.06644031405448914 Accuracy 0.8134999871253967\n",
      "Output tensor([[0.5232],\n",
      "        [0.4492]])\n",
      "Iteration 2500 Training loss 0.06995537132024765 Validation loss 0.06599573791027069 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.0692],\n",
      "        [0.0155]])\n",
      "Iteration 2510 Training loss 0.06490182131528854 Validation loss 0.06614819169044495 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.9720],\n",
      "        [0.5423]])\n",
      "Iteration 2520 Training loss 0.06554102897644043 Validation loss 0.06606537103652954 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.6143],\n",
      "        [0.3918]])\n",
      "Iteration 2530 Training loss 0.06670701503753662 Validation loss 0.06604956090450287 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.1824],\n",
      "        [0.0334]])\n",
      "Iteration 2540 Training loss 0.06438374519348145 Validation loss 0.06597201526165009 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.0095],\n",
      "        [0.0476]])\n",
      "Iteration 2550 Training loss 0.06326717138290405 Validation loss 0.06624944508075714 Accuracy 0.8134999871253967\n",
      "Output tensor([[0.2222],\n",
      "        [0.0976]])\n",
      "Iteration 2560 Training loss 0.06630931049585342 Validation loss 0.06600730866193771 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.9981],\n",
      "        [0.4964]])\n",
      "Iteration 2570 Training loss 0.06425957381725311 Validation loss 0.06637369096279144 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.1219],\n",
      "        [0.2358]])\n",
      "Iteration 2580 Training loss 0.06826021522283554 Validation loss 0.06630030274391174 Accuracy 0.8130000233650208\n",
      "Output tensor([[0.9811],\n",
      "        [0.8748]])\n",
      "Iteration 2590 Training loss 0.06632772833108902 Validation loss 0.06614910066127777 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.9384],\n",
      "        [0.9855]])\n",
      "Iteration 2600 Training loss 0.06424298137426376 Validation loss 0.06629011780023575 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.5398],\n",
      "        [0.1363]])\n",
      "Iteration 2610 Training loss 0.06294996291399002 Validation loss 0.0659189224243164 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.3317],\n",
      "        [0.3464]])\n",
      "Iteration 2620 Training loss 0.06751587241888046 Validation loss 0.06599235534667969 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.9602],\n",
      "        [0.7298]])\n",
      "Iteration 2630 Training loss 0.06802200525999069 Validation loss 0.06629274040460587 Accuracy 0.815500020980835\n",
      "Output tensor([[0.3597],\n",
      "        [0.6705]])\n",
      "Iteration 2640 Training loss 0.06596723198890686 Validation loss 0.06603170931339264 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.9967],\n",
      "        [0.7843]])\n",
      "Iteration 2650 Training loss 0.06923437863588333 Validation loss 0.06587780267000198 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.1866],\n",
      "        [0.9589]])\n",
      "Iteration 2660 Training loss 0.06801581382751465 Validation loss 0.0663042664527893 Accuracy 0.815500020980835\n",
      "Output tensor([[0.5901],\n",
      "        [0.6921]])\n",
      "Iteration 2670 Training loss 0.06742103397846222 Validation loss 0.06583112478256226 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0651],\n",
      "        [0.4731]])\n",
      "Iteration 2680 Training loss 0.06624802201986313 Validation loss 0.06598472595214844 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.3616],\n",
      "        [0.9663]])\n",
      "Iteration 2690 Training loss 0.06588877737522125 Validation loss 0.06594551354646683 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.4638],\n",
      "        [0.3452]])\n",
      "Iteration 2700 Training loss 0.0669834092259407 Validation loss 0.0661287009716034 Accuracy 0.8134999871253967\n",
      "Output tensor([[0.1180],\n",
      "        [0.2820]])\n",
      "Iteration 2710 Training loss 0.06541462242603302 Validation loss 0.06580300629138947 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.8990],\n",
      "        [0.9694]])\n",
      "Iteration 2720 Training loss 0.06350420415401459 Validation loss 0.06578731536865234 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.5068],\n",
      "        [0.8200]])\n",
      "Iteration 2730 Training loss 0.06381384283304214 Validation loss 0.06607338041067123 Accuracy 0.8134999871253967\n",
      "Output tensor([[0.6819],\n",
      "        [0.9447]])\n",
      "Iteration 2740 Training loss 0.06844628602266312 Validation loss 0.06597308069467545 Accuracy 0.815500020980835\n",
      "Output tensor([[0.8935],\n",
      "        [0.1057]])\n",
      "Iteration 2750 Training loss 0.067549929022789 Validation loss 0.06573697924613953 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.9572],\n",
      "        [0.7348]])\n",
      "Iteration 2760 Training loss 0.06439843773841858 Validation loss 0.06589410454034805 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.3947],\n",
      "        [0.2573]])\n",
      "Iteration 2770 Training loss 0.06751147657632828 Validation loss 0.06568095088005066 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1840],\n",
      "        [0.4856]])\n",
      "Iteration 2780 Training loss 0.062109753489494324 Validation loss 0.06594429165124893 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.0938],\n",
      "        [0.2487]])\n",
      "Iteration 2790 Training loss 0.06471699476242065 Validation loss 0.06608480960130692 Accuracy 0.8144999742507935\n",
      "Output tensor([[0.2579],\n",
      "        [0.0299]])\n",
      "Iteration 2800 Training loss 0.06460823118686676 Validation loss 0.06565242260694504 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1540],\n",
      "        [0.9903]])\n",
      "Iteration 2810 Training loss 0.06404025107622147 Validation loss 0.06604418903589249 Accuracy 0.8144999742507935\n",
      "Output tensor([[0.9201],\n",
      "        [0.2283]])\n",
      "Iteration 2820 Training loss 0.06727570295333862 Validation loss 0.06576890498399734 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.8330],\n",
      "        [0.1468]])\n",
      "Iteration 2830 Training loss 0.062014542520046234 Validation loss 0.06586003303527832 Accuracy 0.8159999847412109\n",
      "Output tensor([[0.9176],\n",
      "        [0.4811]])\n",
      "Iteration 2840 Training loss 0.06941734999418259 Validation loss 0.06578543782234192 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.8787],\n",
      "        [0.9542]])\n",
      "Iteration 2850 Training loss 0.06458629667758942 Validation loss 0.06565012037754059 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.1806],\n",
      "        [0.0914]])\n",
      "Iteration 2860 Training loss 0.06196315214037895 Validation loss 0.0659601017832756 Accuracy 0.8144999742507935\n",
      "Output tensor([[0.2224],\n",
      "        [0.4272]])\n",
      "Iteration 2870 Training loss 0.06929446756839752 Validation loss 0.06590699404478073 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.2733],\n",
      "        [0.3720]])\n",
      "Iteration 2880 Training loss 0.06986376643180847 Validation loss 0.06566537916660309 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.3321],\n",
      "        [0.6743]])\n",
      "Iteration 2890 Training loss 0.06094706431031227 Validation loss 0.06576231122016907 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.0417],\n",
      "        [0.6496]])\n",
      "Iteration 2900 Training loss 0.06626991927623749 Validation loss 0.06565503031015396 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0804],\n",
      "        [0.9309]])\n",
      "Iteration 2910 Training loss 0.0642600730061531 Validation loss 0.06555517017841339 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.8272],\n",
      "        [0.1562]])\n",
      "Iteration 2920 Training loss 0.05949430912733078 Validation loss 0.0660167783498764 Accuracy 0.8134999871253967\n",
      "Output tensor([[0.6347],\n",
      "        [0.1260]])\n",
      "Iteration 2930 Training loss 0.06719566136598587 Validation loss 0.06569754332304001 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.8309],\n",
      "        [0.2436]])\n",
      "Iteration 2940 Training loss 0.06742602586746216 Validation loss 0.0655483603477478 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.1367],\n",
      "        [0.9660]])\n",
      "Iteration 2950 Training loss 0.06475570797920227 Validation loss 0.06566464155912399 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.2231],\n",
      "        [0.0426]])\n",
      "Iteration 2960 Training loss 0.06687901169061661 Validation loss 0.06599016487598419 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.8439],\n",
      "        [0.7234]])\n",
      "Iteration 2970 Training loss 0.06435760855674744 Validation loss 0.06549911201000214 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.9988],\n",
      "        [0.7166]])\n",
      "Iteration 2980 Training loss 0.06358370929956436 Validation loss 0.06547313183546066 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.8372],\n",
      "        [0.6923]])\n",
      "Iteration 2990 Training loss 0.06451967358589172 Validation loss 0.06547363102436066 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1267],\n",
      "        [0.4158]])\n",
      "Iteration 3000 Training loss 0.0622849315404892 Validation loss 0.06564787030220032 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.2445],\n",
      "        [0.2962]])\n",
      "Iteration 3010 Training loss 0.06617153435945511 Validation loss 0.06555003672838211 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.6163],\n",
      "        [0.1108]])\n",
      "Iteration 3020 Training loss 0.06456462293863297 Validation loss 0.06562343239784241 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.3237],\n",
      "        [0.3090]])\n",
      "Iteration 3030 Training loss 0.06665624678134918 Validation loss 0.06546404212713242 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.9995],\n",
      "        [0.9683]])\n",
      "Iteration 3040 Training loss 0.06135808676481247 Validation loss 0.06562288105487823 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.3459],\n",
      "        [0.1283]])\n",
      "Iteration 3050 Training loss 0.0656931921839714 Validation loss 0.06552890688180923 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.1832],\n",
      "        [0.0756]])\n",
      "Iteration 3060 Training loss 0.06620652973651886 Validation loss 0.06553857773542404 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.9747],\n",
      "        [0.1682]])\n",
      "Iteration 3070 Training loss 0.06440797448158264 Validation loss 0.06546071171760559 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.2171],\n",
      "        [0.2482]])\n",
      "Iteration 3080 Training loss 0.06529191881418228 Validation loss 0.06539179384708405 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1267],\n",
      "        [0.9674]])\n",
      "Iteration 3090 Training loss 0.06542500108480453 Validation loss 0.0655025914311409 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.2253],\n",
      "        [0.6623]])\n",
      "Iteration 3100 Training loss 0.0657881572842598 Validation loss 0.06560458987951279 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.0916],\n",
      "        [0.0552]])\n",
      "Iteration 3110 Training loss 0.06693266332149506 Validation loss 0.06554443389177322 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.9742],\n",
      "        [0.0490]])\n",
      "Iteration 3120 Training loss 0.06436490267515182 Validation loss 0.06552402675151825 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.4263],\n",
      "        [0.0421]])\n",
      "Iteration 3130 Training loss 0.06472562998533249 Validation loss 0.06538727134466171 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.9040],\n",
      "        [0.1721]])\n",
      "Iteration 3140 Training loss 0.06472098082304001 Validation loss 0.06544943153858185 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.8680],\n",
      "        [0.0567]])\n",
      "Iteration 3150 Training loss 0.06496703624725342 Validation loss 0.06528526544570923 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.8288],\n",
      "        [0.8940]])\n",
      "Iteration 3160 Training loss 0.06571570038795471 Validation loss 0.06557050347328186 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.9775],\n",
      "        [0.7123]])\n",
      "Iteration 3170 Training loss 0.06535013765096664 Validation loss 0.06536458432674408 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9294],\n",
      "        [0.9208]])\n",
      "Iteration 3180 Training loss 0.06362685561180115 Validation loss 0.06536440551280975 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1877],\n",
      "        [0.7392]])\n",
      "Iteration 3190 Training loss 0.06604878604412079 Validation loss 0.06567559391260147 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.9763],\n",
      "        [0.5989]])\n",
      "Iteration 3200 Training loss 0.06391908973455429 Validation loss 0.06541839241981506 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.6384],\n",
      "        [0.1439]])\n",
      "Iteration 3210 Training loss 0.06704065948724747 Validation loss 0.0652766078710556 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.0269],\n",
      "        [0.4645]])\n",
      "Iteration 3220 Training loss 0.06624478101730347 Validation loss 0.06541847437620163 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.6370],\n",
      "        [0.0905]])\n",
      "Iteration 3230 Training loss 0.06557460129261017 Validation loss 0.06531863659620285 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.7446],\n",
      "        [0.9699]])\n",
      "Iteration 3240 Training loss 0.06876513361930847 Validation loss 0.0654863566160202 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.8558],\n",
      "        [0.0490]])\n",
      "Iteration 3250 Training loss 0.06276045739650726 Validation loss 0.06563975661993027 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.6802],\n",
      "        [0.3602]])\n",
      "Iteration 3260 Training loss 0.06335245817899704 Validation loss 0.06520788371562958 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.7189],\n",
      "        [0.4805]])\n",
      "Iteration 3270 Training loss 0.0646774172782898 Validation loss 0.06533365696668625 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.1876],\n",
      "        [0.6563]])\n",
      "Iteration 3280 Training loss 0.06513597071170807 Validation loss 0.06525404751300812 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.3491],\n",
      "        [0.9936]])\n",
      "Iteration 3290 Training loss 0.06373622268438339 Validation loss 0.06547575443983078 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.6618],\n",
      "        [0.0308]])\n",
      "Iteration 3300 Training loss 0.06718626618385315 Validation loss 0.06521417945623398 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.7373],\n",
      "        [0.5275]])\n",
      "Iteration 3310 Training loss 0.06413063406944275 Validation loss 0.06527912616729736 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.4310],\n",
      "        [0.6393]])\n",
      "Iteration 3320 Training loss 0.06225615739822388 Validation loss 0.06537428498268127 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.7954],\n",
      "        [0.0768]])\n",
      "Iteration 3330 Training loss 0.06400987505912781 Validation loss 0.06574227660894394 Accuracy 0.8149999976158142\n",
      "Output tensor([[0.2338],\n",
      "        [0.9882]])\n",
      "Iteration 3340 Training loss 0.06493471562862396 Validation loss 0.06523130089044571 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.5213],\n",
      "        [0.8642]])\n",
      "Iteration 3350 Training loss 0.06676360964775085 Validation loss 0.06524084508419037 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.0786],\n",
      "        [0.0359]])\n",
      "Iteration 3360 Training loss 0.06274855881929398 Validation loss 0.06508172303438187 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.0495],\n",
      "        [0.8606]])\n",
      "Iteration 3370 Training loss 0.0648670345544815 Validation loss 0.06510911136865616 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0422],\n",
      "        [0.0347]])\n",
      "Iteration 3380 Training loss 0.0651000365614891 Validation loss 0.06501949578523636 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.0821],\n",
      "        [0.4580]])\n",
      "Iteration 3390 Training loss 0.06488577276468277 Validation loss 0.06515353918075562 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.7499],\n",
      "        [0.9944]])\n",
      "Iteration 3400 Training loss 0.06788267195224762 Validation loss 0.06504670530557632 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.3233],\n",
      "        [0.7484]])\n",
      "Iteration 3410 Training loss 0.06675828248262405 Validation loss 0.06511823832988739 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.1433],\n",
      "        [0.5870]])\n",
      "Iteration 3420 Training loss 0.06706453859806061 Validation loss 0.0651463121175766 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.0390],\n",
      "        [0.8197]])\n",
      "Iteration 3430 Training loss 0.06587845832109451 Validation loss 0.06512932479381561 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.5963],\n",
      "        [0.3186]])\n",
      "Iteration 3440 Training loss 0.06248440220952034 Validation loss 0.06512559205293655 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.2412],\n",
      "        [0.0187]])\n",
      "Iteration 3450 Training loss 0.06765331327915192 Validation loss 0.0650501549243927 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.0655],\n",
      "        [0.2340]])\n",
      "Iteration 3460 Training loss 0.0621967613697052 Validation loss 0.06516275554895401 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.2040],\n",
      "        [0.8724]])\n",
      "Iteration 3470 Training loss 0.06415463238954544 Validation loss 0.06522925198078156 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.9695],\n",
      "        [0.1582]])\n",
      "Iteration 3480 Training loss 0.06346762925386429 Validation loss 0.06524369865655899 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.2917],\n",
      "        [0.1443]])\n",
      "Iteration 3490 Training loss 0.06382954120635986 Validation loss 0.0649772435426712 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.8099],\n",
      "        [0.6265]])\n",
      "Iteration 3500 Training loss 0.06572671979665756 Validation loss 0.06524891406297684 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.0776],\n",
      "        [0.2807]])\n",
      "Iteration 3510 Training loss 0.06346599757671356 Validation loss 0.06511601060628891 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.6422],\n",
      "        [0.4278]])\n",
      "Iteration 3520 Training loss 0.06300141662359238 Validation loss 0.06501815468072891 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.4619],\n",
      "        [0.0251]])\n",
      "Iteration 3530 Training loss 0.06325791776180267 Validation loss 0.06512446701526642 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.2220],\n",
      "        [0.5206]])\n",
      "Iteration 3540 Training loss 0.06428544223308563 Validation loss 0.06499722599983215 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.4218],\n",
      "        [0.7725]])\n",
      "Iteration 3550 Training loss 0.06353437155485153 Validation loss 0.0650649443268776 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.9456],\n",
      "        [0.1310]])\n",
      "Iteration 3560 Training loss 0.061478469520807266 Validation loss 0.06500213593244553 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.5753],\n",
      "        [0.4794]])\n",
      "Iteration 3570 Training loss 0.0660305991768837 Validation loss 0.06543797999620438 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.9608],\n",
      "        [0.9998]])\n",
      "Iteration 3580 Training loss 0.06136912852525711 Validation loss 0.06512534618377686 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.3059],\n",
      "        [0.8466]])\n",
      "Iteration 3590 Training loss 0.06748156994581223 Validation loss 0.06499456614255905 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.7724],\n",
      "        [0.1018]])\n",
      "Iteration 3600 Training loss 0.06840117275714874 Validation loss 0.06491804867982864 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.2999],\n",
      "        [0.5707]])\n",
      "Iteration 3610 Training loss 0.06219787150621414 Validation loss 0.06531993299722672 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.2765],\n",
      "        [0.9653]])\n",
      "Iteration 3620 Training loss 0.06533332169055939 Validation loss 0.06507323682308197 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.9202],\n",
      "        [0.8533]])\n",
      "Iteration 3630 Training loss 0.06576552987098694 Validation loss 0.06484788656234741 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.1349],\n",
      "        [0.9473]])\n",
      "Iteration 3640 Training loss 0.06581094115972519 Validation loss 0.06499569118022919 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0315],\n",
      "        [0.9684]])\n",
      "Iteration 3650 Training loss 0.06331875175237656 Validation loss 0.06483373790979385 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.2088],\n",
      "        [0.1048]])\n",
      "Iteration 3660 Training loss 0.06373991072177887 Validation loss 0.06509606540203094 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.9052],\n",
      "        [0.9776]])\n",
      "Iteration 3670 Training loss 0.06628149002790451 Validation loss 0.06502101570367813 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.7382],\n",
      "        [0.6928]])\n",
      "Iteration 3680 Training loss 0.0623134970664978 Validation loss 0.06486716866493225 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.7862],\n",
      "        [0.9731]])\n",
      "Iteration 3690 Training loss 0.06589029729366302 Validation loss 0.0650349110364914 Accuracy 0.8165000081062317\n",
      "Output tensor([[0.5471],\n",
      "        [0.9737]])\n",
      "Iteration 3700 Training loss 0.06425359100103378 Validation loss 0.0648665502667427 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.7537],\n",
      "        [0.1742]])\n",
      "Iteration 3710 Training loss 0.060246940702199936 Validation loss 0.0648047924041748 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.8981],\n",
      "        [0.8190]])\n",
      "Iteration 3720 Training loss 0.06475844234228134 Validation loss 0.0649578869342804 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.3414],\n",
      "        [0.8118]])\n",
      "Iteration 3730 Training loss 0.06726937741041183 Validation loss 0.06474998593330383 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.8329],\n",
      "        [0.5451]])\n",
      "Iteration 3740 Training loss 0.06288269907236099 Validation loss 0.06502895802259445 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.6679],\n",
      "        [0.5159]])\n",
      "Iteration 3750 Training loss 0.06219470128417015 Validation loss 0.06478672474622726 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1952],\n",
      "        [0.9441]])\n",
      "Iteration 3760 Training loss 0.06380054354667664 Validation loss 0.0648449957370758 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.1718],\n",
      "        [0.0433]])\n",
      "Iteration 3770 Training loss 0.06196653097867966 Validation loss 0.06476730108261108 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.0371],\n",
      "        [0.9923]])\n",
      "Iteration 3780 Training loss 0.06366357952356339 Validation loss 0.06536529958248138 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.0433],\n",
      "        [0.8429]])\n",
      "Iteration 3790 Training loss 0.06491594761610031 Validation loss 0.0648217424750328 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.7624],\n",
      "        [0.0584]])\n",
      "Iteration 3800 Training loss 0.0645786002278328 Validation loss 0.06497402489185333 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.0626],\n",
      "        [0.2766]])\n",
      "Iteration 3810 Training loss 0.06350061297416687 Validation loss 0.06495767086744308 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.1685],\n",
      "        [0.2686]])\n",
      "Iteration 3820 Training loss 0.06356623768806458 Validation loss 0.06470128893852234 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.3683],\n",
      "        [0.9969]])\n",
      "Iteration 3830 Training loss 0.06720036268234253 Validation loss 0.06503881514072418 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.0359],\n",
      "        [0.8469]])\n",
      "Iteration 3840 Training loss 0.0649612694978714 Validation loss 0.06479355692863464 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.5310],\n",
      "        [0.9010]])\n",
      "Iteration 3850 Training loss 0.0611838698387146 Validation loss 0.06483419984579086 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.4877],\n",
      "        [0.4571]])\n",
      "Iteration 3860 Training loss 0.06844749301671982 Validation loss 0.0648302212357521 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.4196],\n",
      "        [0.0249]])\n",
      "Iteration 3870 Training loss 0.06533056497573853 Validation loss 0.06479382514953613 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0336],\n",
      "        [0.1629]])\n",
      "Iteration 3880 Training loss 0.06243814155459404 Validation loss 0.064959816634655 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.3057],\n",
      "        [0.0848]])\n",
      "Iteration 3890 Training loss 0.06343194097280502 Validation loss 0.06502465903759003 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.3789],\n",
      "        [0.0134]])\n",
      "Iteration 3900 Training loss 0.0651785135269165 Validation loss 0.06478006392717361 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.5132],\n",
      "        [0.9688]])\n",
      "Iteration 3910 Training loss 0.06521490216255188 Validation loss 0.06477683037519455 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0416],\n",
      "        [0.7617]])\n",
      "Iteration 3920 Training loss 0.06475695222616196 Validation loss 0.06505248695611954 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.9849],\n",
      "        [0.7871]])\n",
      "Iteration 3930 Training loss 0.06678353250026703 Validation loss 0.06488052010536194 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.8928],\n",
      "        [0.1293]])\n",
      "Iteration 3940 Training loss 0.06442581117153168 Validation loss 0.064638152718544 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.7867],\n",
      "        [0.1182]])\n",
      "Iteration 3950 Training loss 0.0654597356915474 Validation loss 0.06469327956438065 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.7176],\n",
      "        [0.3864]])\n",
      "Iteration 3960 Training loss 0.0609675757586956 Validation loss 0.06470999121665955 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.1988],\n",
      "        [0.4594]])\n",
      "Iteration 3970 Training loss 0.06667111068964005 Validation loss 0.06466716527938843 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.0982],\n",
      "        [0.8000]])\n",
      "Iteration 3980 Training loss 0.0645366683602333 Validation loss 0.06470295786857605 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.9112],\n",
      "        [0.3971]])\n",
      "Iteration 3990 Training loss 0.06506873667240143 Validation loss 0.06463668495416641 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.7127],\n",
      "        [0.2138]])\n",
      "Iteration 4000 Training loss 0.06272962689399719 Validation loss 0.06460227072238922 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.5869],\n",
      "        [0.6529]])\n",
      "Iteration 4010 Training loss 0.06472041457891464 Validation loss 0.06474465876817703 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.0254],\n",
      "        [0.5644]])\n",
      "Iteration 4020 Training loss 0.06536653637886047 Validation loss 0.06499087065458298 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.3954],\n",
      "        [0.0230]])\n",
      "Iteration 4030 Training loss 0.06234271079301834 Validation loss 0.06487292051315308 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.7838],\n",
      "        [0.9711]])\n",
      "Iteration 4040 Training loss 0.06428661197423935 Validation loss 0.06504150480031967 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.9085],\n",
      "        [0.0500]])\n",
      "Iteration 4050 Training loss 0.06358803808689117 Validation loss 0.06468750536441803 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.8256],\n",
      "        [0.9039]])\n",
      "Iteration 4060 Training loss 0.06539326161146164 Validation loss 0.0645115077495575 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.0880],\n",
      "        [0.7187]])\n",
      "Iteration 4070 Training loss 0.06373851746320724 Validation loss 0.0650368258357048 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.6492],\n",
      "        [0.0796]])\n",
      "Iteration 4080 Training loss 0.06647102534770966 Validation loss 0.06450081616640091 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9778],\n",
      "        [0.6947]])\n",
      "Iteration 4090 Training loss 0.06742412596940994 Validation loss 0.06473950296640396 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.3042],\n",
      "        [0.2661]])\n",
      "Iteration 4100 Training loss 0.06415857374668121 Validation loss 0.06503506749868393 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.9646],\n",
      "        [0.9486]])\n",
      "Iteration 4110 Training loss 0.06651479750871658 Validation loss 0.06449895352125168 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.7681],\n",
      "        [0.1258]])\n",
      "Iteration 4120 Training loss 0.06632796674966812 Validation loss 0.06465461850166321 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.3102],\n",
      "        [0.5931]])\n",
      "Iteration 4130 Training loss 0.06313133239746094 Validation loss 0.06450415402650833 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.2982],\n",
      "        [0.8816]])\n",
      "Iteration 4140 Training loss 0.0621684230864048 Validation loss 0.0647142231464386 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.3449],\n",
      "        [0.0769]])\n",
      "Iteration 4150 Training loss 0.06198149174451828 Validation loss 0.06538008898496628 Accuracy 0.8169999718666077\n",
      "Output tensor([[0.4303],\n",
      "        [0.6059]])\n",
      "Iteration 4160 Training loss 0.06326057761907578 Validation loss 0.06462295353412628 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.8453],\n",
      "        [0.1145]])\n",
      "Iteration 4170 Training loss 0.06234123930335045 Validation loss 0.06450815498828888 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0234],\n",
      "        [0.0268]])\n",
      "Iteration 4180 Training loss 0.06429450958967209 Validation loss 0.06444459408521652 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.6558],\n",
      "        [0.3853]])\n",
      "Iteration 4190 Training loss 0.06352213770151138 Validation loss 0.06451665610074997 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.9075],\n",
      "        [0.9527]])\n",
      "Iteration 4200 Training loss 0.06007070094347 Validation loss 0.06464730948209763 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.0896],\n",
      "        [0.3155]])\n",
      "Iteration 4210 Training loss 0.06436266750097275 Validation loss 0.06471864134073257 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.9722],\n",
      "        [0.0737]])\n",
      "Iteration 4220 Training loss 0.06279419362545013 Validation loss 0.06440368294715881 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.3749],\n",
      "        [0.9955]])\n",
      "Iteration 4230 Training loss 0.06597587466239929 Validation loss 0.06498004496097565 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.2474],\n",
      "        [0.7881]])\n",
      "Iteration 4240 Training loss 0.06442836672067642 Validation loss 0.064415842294693 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.6291],\n",
      "        [0.9716]])\n",
      "Iteration 4250 Training loss 0.06365794688463211 Validation loss 0.0648365244269371 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.9695],\n",
      "        [0.9046]])\n",
      "Iteration 4260 Training loss 0.06571140885353088 Validation loss 0.06442074477672577 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.5126],\n",
      "        [0.7845]])\n",
      "Iteration 4270 Training loss 0.06457929313182831 Validation loss 0.06475281715393066 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.8203],\n",
      "        [0.6247]])\n",
      "Iteration 4280 Training loss 0.06737838685512543 Validation loss 0.06468459963798523 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.0810],\n",
      "        [0.7077]])\n",
      "Iteration 4290 Training loss 0.06471806764602661 Validation loss 0.06486671417951584 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.3351],\n",
      "        [0.7131]])\n",
      "Iteration 4300 Training loss 0.063874252140522 Validation loss 0.06442093849182129 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.4133],\n",
      "        [0.1856]])\n",
      "Iteration 4310 Training loss 0.06565217673778534 Validation loss 0.06449964642524719 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.1323],\n",
      "        [0.2599]])\n",
      "Iteration 4320 Training loss 0.06222577393054962 Validation loss 0.0643579363822937 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.3951],\n",
      "        [0.0639]])\n",
      "Iteration 4330 Training loss 0.06466980278491974 Validation loss 0.06442500650882721 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1326],\n",
      "        [0.5280]])\n",
      "Iteration 4340 Training loss 0.06270061433315277 Validation loss 0.0650489404797554 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.7250],\n",
      "        [0.7097]])\n",
      "Iteration 4350 Training loss 0.06488947570323944 Validation loss 0.06438310444355011 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.9097],\n",
      "        [0.8327]])\n",
      "Iteration 4360 Training loss 0.06244846433401108 Validation loss 0.06436008960008621 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.1325],\n",
      "        [0.5780]])\n",
      "Iteration 4370 Training loss 0.06563961505889893 Validation loss 0.06465116888284683 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.4934],\n",
      "        [0.8049]])\n",
      "Iteration 4380 Training loss 0.06296345591545105 Validation loss 0.06439201533794403 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.0936],\n",
      "        [0.9760]])\n",
      "Iteration 4390 Training loss 0.0626688152551651 Validation loss 0.06447121500968933 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.1958],\n",
      "        [0.4252]])\n",
      "Iteration 4400 Training loss 0.06088043376803398 Validation loss 0.06481540203094482 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.1429],\n",
      "        [0.9093]])\n",
      "Iteration 4410 Training loss 0.06512105464935303 Validation loss 0.06490442901849747 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.2856],\n",
      "        [0.0442]])\n",
      "Iteration 4420 Training loss 0.06352320313453674 Validation loss 0.06425262242555618 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.9864],\n",
      "        [0.2833]])\n",
      "Iteration 4430 Training loss 0.06544876098632812 Validation loss 0.06428622454404831 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.5179],\n",
      "        [0.5504]])\n",
      "Iteration 4440 Training loss 0.06339316815137863 Validation loss 0.06442998349666595 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.4173],\n",
      "        [0.9581]])\n",
      "Iteration 4450 Training loss 0.061512093991041183 Validation loss 0.06423099339008331 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.3288],\n",
      "        [0.9733]])\n",
      "Iteration 4460 Training loss 0.06414961069822311 Validation loss 0.06439254432916641 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.0285],\n",
      "        [0.5717]])\n",
      "Iteration 4470 Training loss 0.0636412650346756 Validation loss 0.06422560662031174 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.6336],\n",
      "        [0.6545]])\n",
      "Iteration 4480 Training loss 0.06744858622550964 Validation loss 0.064207524061203 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.3412],\n",
      "        [0.0872]])\n",
      "Iteration 4490 Training loss 0.06265851855278015 Validation loss 0.06437958031892776 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1350],\n",
      "        [0.6740]])\n",
      "Iteration 4500 Training loss 0.06292698532342911 Validation loss 0.0643850564956665 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.9451],\n",
      "        [0.9497]])\n",
      "Iteration 4510 Training loss 0.062274474650621414 Validation loss 0.06460893899202347 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.3861],\n",
      "        [0.7791]])\n",
      "Iteration 4520 Training loss 0.064361572265625 Validation loss 0.06468737125396729 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.5472],\n",
      "        [0.0421]])\n",
      "Iteration 4530 Training loss 0.0652262344956398 Validation loss 0.0641915425658226 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.9523],\n",
      "        [0.1579]])\n",
      "Iteration 4540 Training loss 0.06294582784175873 Validation loss 0.06435101479291916 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9714],\n",
      "        [0.6639]])\n",
      "Iteration 4550 Training loss 0.06755537539720535 Validation loss 0.0641656443476677 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.8407],\n",
      "        [0.9303]])\n",
      "Iteration 4560 Training loss 0.06444033980369568 Validation loss 0.06444782763719559 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9761],\n",
      "        [0.9156]])\n",
      "Iteration 4570 Training loss 0.06500773876905441 Validation loss 0.06421128660440445 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.8275],\n",
      "        [0.0606]])\n",
      "Iteration 4580 Training loss 0.0658167377114296 Validation loss 0.06420038640499115 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.9051],\n",
      "        [0.5094]])\n",
      "Iteration 4590 Training loss 0.0649740919470787 Validation loss 0.06414106488227844 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.8574],\n",
      "        [0.0525]])\n",
      "Iteration 4600 Training loss 0.06419079005718231 Validation loss 0.06430120766162872 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.7923],\n",
      "        [0.3146]])\n",
      "Iteration 4610 Training loss 0.06291460990905762 Validation loss 0.06452855467796326 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.0575],\n",
      "        [0.5259]])\n",
      "Iteration 4620 Training loss 0.0681874230504036 Validation loss 0.06430935859680176 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1338],\n",
      "        [0.5444]])\n",
      "Iteration 4630 Training loss 0.06679229438304901 Validation loss 0.06418962776660919 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.7556],\n",
      "        [0.9794]])\n",
      "Iteration 4640 Training loss 0.06638757139444351 Validation loss 0.06446810811758041 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.6144],\n",
      "        [0.8599]])\n",
      "Iteration 4650 Training loss 0.06543631851673126 Validation loss 0.06446869671344757 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.4086],\n",
      "        [0.2752]])\n",
      "Iteration 4660 Training loss 0.06296508014202118 Validation loss 0.0643259659409523 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.7832],\n",
      "        [0.4967]])\n",
      "Iteration 4670 Training loss 0.06491320580244064 Validation loss 0.06424564868211746 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.0750],\n",
      "        [0.2345]])\n",
      "Iteration 4680 Training loss 0.06525448709726334 Validation loss 0.06412722170352936 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.9768],\n",
      "        [0.1093]])\n",
      "Iteration 4690 Training loss 0.06359882652759552 Validation loss 0.0644078329205513 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.5048],\n",
      "        [0.9890]])\n",
      "Iteration 4700 Training loss 0.062159422785043716 Validation loss 0.06417112797498703 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.0648],\n",
      "        [0.0473]])\n",
      "Iteration 4710 Training loss 0.0703403651714325 Validation loss 0.06466726958751678 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0485],\n",
      "        [0.4756]])\n",
      "Iteration 4720 Training loss 0.06380656361579895 Validation loss 0.06421597301959991 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.3546],\n",
      "        [0.9168]])\n",
      "Iteration 4730 Training loss 0.06249075010418892 Validation loss 0.06496317684650421 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.2230],\n",
      "        [0.6815]])\n",
      "Iteration 4740 Training loss 0.06576161086559296 Validation loss 0.06430475413799286 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.5704],\n",
      "        [0.2438]])\n",
      "Iteration 4750 Training loss 0.06433163583278656 Validation loss 0.064180888235569 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.9985],\n",
      "        [0.5458]])\n",
      "Iteration 4760 Training loss 0.06419523805379868 Validation loss 0.06436154991388321 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.9922],\n",
      "        [0.4402]])\n",
      "Iteration 4770 Training loss 0.06248554587364197 Validation loss 0.06406860798597336 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.8324],\n",
      "        [0.9683]])\n",
      "Iteration 4780 Training loss 0.06549246609210968 Validation loss 0.06448255479335785 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.7262],\n",
      "        [0.9027]])\n",
      "Iteration 4790 Training loss 0.060377221554517746 Validation loss 0.06404536962509155 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.3890],\n",
      "        [0.9155]])\n",
      "Iteration 4800 Training loss 0.06444378942251205 Validation loss 0.06407184153795242 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.4258],\n",
      "        [0.1468]])\n",
      "Iteration 4810 Training loss 0.061872415244579315 Validation loss 0.06410809606313705 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.9342],\n",
      "        [0.0258]])\n",
      "Iteration 4820 Training loss 0.06178481876850128 Validation loss 0.06462549418210983 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.1473],\n",
      "        [0.1281]])\n",
      "Iteration 4830 Training loss 0.06407104432582855 Validation loss 0.06411897391080856 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.7989],\n",
      "        [0.9240]])\n",
      "Iteration 4840 Training loss 0.06407678127288818 Validation loss 0.064501091837883 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0116],\n",
      "        [0.9622]])\n",
      "Iteration 4850 Training loss 0.06323239207267761 Validation loss 0.06406642496585846 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.3635],\n",
      "        [0.9895]])\n",
      "Iteration 4860 Training loss 0.0636698305606842 Validation loss 0.06414176523685455 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.6344],\n",
      "        [0.2793]])\n",
      "Iteration 4870 Training loss 0.06319735199213028 Validation loss 0.06405021995306015 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.3483],\n",
      "        [0.5543]])\n",
      "Iteration 4880 Training loss 0.06347065418958664 Validation loss 0.06406209617853165 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.3422],\n",
      "        [0.1581]])\n",
      "Iteration 4890 Training loss 0.06491618603467941 Validation loss 0.06455565243959427 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.1389],\n",
      "        [0.9855]])\n",
      "Iteration 4900 Training loss 0.06084776669740677 Validation loss 0.06400508433580399 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.0602],\n",
      "        [0.0588]])\n",
      "Iteration 4910 Training loss 0.06192811578512192 Validation loss 0.06445807963609695 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0560],\n",
      "        [0.3753]])\n",
      "Iteration 4920 Training loss 0.06356891989707947 Validation loss 0.06417792290449142 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9251],\n",
      "        [0.7512]])\n",
      "Iteration 4930 Training loss 0.06492754817008972 Validation loss 0.06407196074724197 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.0690],\n",
      "        [0.9741]])\n",
      "Iteration 4940 Training loss 0.06462515145540237 Validation loss 0.06404487788677216 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.2361],\n",
      "        [0.7572]])\n",
      "Iteration 4950 Training loss 0.06413093209266663 Validation loss 0.06394991278648376 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.0917],\n",
      "        [0.9257]])\n",
      "Iteration 4960 Training loss 0.06490164250135422 Validation loss 0.06394905596971512 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.0272],\n",
      "        [0.5131]])\n",
      "Iteration 4970 Training loss 0.06404820084571838 Validation loss 0.0641726478934288 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0385],\n",
      "        [0.8296]])\n",
      "Iteration 4980 Training loss 0.06068529561161995 Validation loss 0.0639464482665062 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.7364],\n",
      "        [0.8850]])\n",
      "Iteration 4990 Training loss 0.0642111673951149 Validation loss 0.06435536593198776 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.0753],\n",
      "        [0.1271]])\n",
      "Iteration 5000 Training loss 0.060790106654167175 Validation loss 0.0639229565858841 Accuracy 0.824999988079071\n",
      "Output tensor([[0.9681],\n",
      "        [0.9354]])\n",
      "Iteration 5010 Training loss 0.06323489546775818 Validation loss 0.06405822932720184 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.4811],\n",
      "        [0.0047]])\n",
      "Iteration 5020 Training loss 0.06169860064983368 Validation loss 0.06417126208543777 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0882],\n",
      "        [0.2623]])\n",
      "Iteration 5030 Training loss 0.05975739285349846 Validation loss 0.0643685907125473 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.0553],\n",
      "        [0.9394]])\n",
      "Iteration 5040 Training loss 0.06452300399541855 Validation loss 0.06423342227935791 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.9991],\n",
      "        [0.5078]])\n",
      "Iteration 5050 Training loss 0.06650982052087784 Validation loss 0.06398427486419678 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.0882],\n",
      "        [0.2365]])\n",
      "Iteration 5060 Training loss 0.062409136444330215 Validation loss 0.06406795978546143 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.5531],\n",
      "        [0.9960]])\n",
      "Iteration 5070 Training loss 0.06434071809053421 Validation loss 0.06435801833868027 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.0288],\n",
      "        [0.0123]])\n",
      "Iteration 5080 Training loss 0.06226757541298866 Validation loss 0.06414024531841278 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.1868],\n",
      "        [0.2034]])\n",
      "Iteration 5090 Training loss 0.06345470249652863 Validation loss 0.06384536623954773 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.2609],\n",
      "        [0.8430]])\n",
      "Iteration 5100 Training loss 0.06346164643764496 Validation loss 0.06403428316116333 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.8121],\n",
      "        [0.2061]])\n",
      "Iteration 5110 Training loss 0.06112165376543999 Validation loss 0.06394088268280029 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.5771],\n",
      "        [0.7750]])\n",
      "Iteration 5120 Training loss 0.06395425647497177 Validation loss 0.0641556903719902 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.6470],\n",
      "        [0.0085]])\n",
      "Iteration 5130 Training loss 0.06445753574371338 Validation loss 0.06463921815156937 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.1227],\n",
      "        [0.0759]])\n",
      "Iteration 5140 Training loss 0.0683722198009491 Validation loss 0.06421418488025665 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.0391],\n",
      "        [0.2196]])\n",
      "Iteration 5150 Training loss 0.06383480131626129 Validation loss 0.0642848089337349 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.5513],\n",
      "        [0.7734]])\n",
      "Iteration 5160 Training loss 0.060919951647520065 Validation loss 0.06411688774824142 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.9672],\n",
      "        [0.9647]])\n",
      "Iteration 5170 Training loss 0.06226283684372902 Validation loss 0.06394155323505402 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.9930],\n",
      "        [0.0909]])\n",
      "Iteration 5180 Training loss 0.06564758718013763 Validation loss 0.063975028693676 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9358],\n",
      "        [0.6866]])\n",
      "Iteration 5190 Training loss 0.06122473627328873 Validation loss 0.06434402614831924 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.8284],\n",
      "        [0.9587]])\n",
      "Iteration 5200 Training loss 0.05889035388827324 Validation loss 0.06381169706583023 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.9161],\n",
      "        [0.1590]])\n",
      "Iteration 5210 Training loss 0.061466094106435776 Validation loss 0.06389705836772919 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.3020],\n",
      "        [0.3285]])\n",
      "Iteration 5220 Training loss 0.06512864679098129 Validation loss 0.06401937454938889 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0948],\n",
      "        [0.8277]])\n",
      "Iteration 5230 Training loss 0.06536519527435303 Validation loss 0.06385179609060287 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.0558],\n",
      "        [0.6156]])\n",
      "Iteration 5240 Training loss 0.0650363638997078 Validation loss 0.06377027183771133 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.9559],\n",
      "        [0.4169]])\n",
      "Iteration 5250 Training loss 0.061784517019987106 Validation loss 0.06377451121807098 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.4777],\n",
      "        [0.1694]])\n",
      "Iteration 5260 Training loss 0.0617109052836895 Validation loss 0.06402836740016937 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.1628],\n",
      "        [0.1309]])\n",
      "Iteration 5270 Training loss 0.06555771082639694 Validation loss 0.06393925845623016 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.3412],\n",
      "        [0.1049]])\n",
      "Iteration 5280 Training loss 0.06497786939144135 Validation loss 0.06395792216062546 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.0473],\n",
      "        [0.5424]])\n",
      "Iteration 5290 Training loss 0.06297162175178528 Validation loss 0.06381551176309586 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.3396],\n",
      "        [0.4991]])\n",
      "Iteration 5300 Training loss 0.0664987862110138 Validation loss 0.06383288651704788 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5634],\n",
      "        [0.1848]])\n",
      "Iteration 5310 Training loss 0.061232682317495346 Validation loss 0.06388421356678009 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.4154],\n",
      "        [0.9530]])\n",
      "Iteration 5320 Training loss 0.062043026089668274 Validation loss 0.06407105177640915 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.7492],\n",
      "        [0.5988]])\n",
      "Iteration 5330 Training loss 0.06613443046808243 Validation loss 0.0637892559170723 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.0482],\n",
      "        [0.9175]])\n",
      "Iteration 5340 Training loss 0.06423582881689072 Validation loss 0.06381382793188095 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5429],\n",
      "        [0.6492]])\n",
      "Iteration 5350 Training loss 0.06540002673864365 Validation loss 0.06377489864826202 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.4763],\n",
      "        [0.0476]])\n",
      "Iteration 5360 Training loss 0.06307727098464966 Validation loss 0.06376949697732925 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.6244],\n",
      "        [0.2588]])\n",
      "Iteration 5370 Training loss 0.06196083128452301 Validation loss 0.06381002813577652 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.9924],\n",
      "        [0.7445]])\n",
      "Iteration 5380 Training loss 0.061687033623456955 Validation loss 0.06407555937767029 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.0409],\n",
      "        [0.0808]])\n",
      "Iteration 5390 Training loss 0.06401755660772324 Validation loss 0.06396385282278061 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.0367],\n",
      "        [0.3757]])\n",
      "Iteration 5400 Training loss 0.06269734352827072 Validation loss 0.06411995738744736 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.4330],\n",
      "        [0.8585]])\n",
      "Iteration 5410 Training loss 0.061107706278562546 Validation loss 0.06380119174718857 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.6470],\n",
      "        [0.1751]])\n",
      "Iteration 5420 Training loss 0.065652035176754 Validation loss 0.06420063972473145 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.1855],\n",
      "        [0.0366]])\n",
      "Iteration 5430 Training loss 0.06326207518577576 Validation loss 0.06429068744182587 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.2385],\n",
      "        [0.3179]])\n",
      "Iteration 5440 Training loss 0.06272053718566895 Validation loss 0.06407415866851807 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.9775],\n",
      "        [0.7941]])\n",
      "Iteration 5450 Training loss 0.0627928227186203 Validation loss 0.06378187239170074 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.2432],\n",
      "        [0.9017]])\n",
      "Iteration 5460 Training loss 0.06115061044692993 Validation loss 0.06394404917955399 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.3764],\n",
      "        [0.6261]])\n",
      "Iteration 5470 Training loss 0.06232348456978798 Validation loss 0.0637442097067833 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.5484],\n",
      "        [0.4042]])\n",
      "Iteration 5480 Training loss 0.06317578256130219 Validation loss 0.06369456648826599 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.1125],\n",
      "        [0.2521]])\n",
      "Iteration 5490 Training loss 0.06149134784936905 Validation loss 0.06366704404354095 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.4411],\n",
      "        [0.0702]])\n",
      "Iteration 5500 Training loss 0.0628473311662674 Validation loss 0.06398039311170578 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.1899],\n",
      "        [0.0720]])\n",
      "Iteration 5510 Training loss 0.06440751254558563 Validation loss 0.06376271694898605 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9256],\n",
      "        [0.9312]])\n",
      "Iteration 5520 Training loss 0.06342993676662445 Validation loss 0.0636432021856308 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.7948],\n",
      "        [0.1020]])\n",
      "Iteration 5530 Training loss 0.0619659386575222 Validation loss 0.06366763263940811 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.8469],\n",
      "        [0.9553]])\n",
      "Iteration 5540 Training loss 0.06518857181072235 Validation loss 0.0640079602599144 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.5611],\n",
      "        [0.8848]])\n",
      "Iteration 5550 Training loss 0.06352079659700394 Validation loss 0.06371631473302841 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.4679],\n",
      "        [0.7365]])\n",
      "Iteration 5560 Training loss 0.06388477981090546 Validation loss 0.06374840438365936 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.3146],\n",
      "        [0.9828]])\n",
      "Iteration 5570 Training loss 0.06220041587948799 Validation loss 0.06413088738918304 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.7169],\n",
      "        [0.9768]])\n",
      "Iteration 5580 Training loss 0.062388792634010315 Validation loss 0.06399664282798767 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.0560],\n",
      "        [0.8662]])\n",
      "Iteration 5590 Training loss 0.0621022954583168 Validation loss 0.06383904069662094 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.6686],\n",
      "        [0.1200]])\n",
      "Iteration 5600 Training loss 0.06044207885861397 Validation loss 0.06395337730646133 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.5406],\n",
      "        [0.1615]])\n",
      "Iteration 5610 Training loss 0.06270230561494827 Validation loss 0.06391343474388123 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.2675],\n",
      "        [0.0695]])\n",
      "Iteration 5620 Training loss 0.06214099004864693 Validation loss 0.06359292566776276 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.5378],\n",
      "        [0.6439]])\n",
      "Iteration 5630 Training loss 0.061321623623371124 Validation loss 0.06382884085178375 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9382],\n",
      "        [0.9373]])\n",
      "Iteration 5640 Training loss 0.059960536658763885 Validation loss 0.0637938380241394 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.4715],\n",
      "        [0.4632]])\n",
      "Iteration 5650 Training loss 0.06170300766825676 Validation loss 0.06400920450687408 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.8101],\n",
      "        [0.9715]])\n",
      "Iteration 5660 Training loss 0.06723138689994812 Validation loss 0.06367839127779007 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.8515],\n",
      "        [0.0500]])\n",
      "Iteration 5670 Training loss 0.06247115880250931 Validation loss 0.06365285068750381 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.2489],\n",
      "        [0.7119]])\n",
      "Iteration 5680 Training loss 0.06510338932275772 Validation loss 0.06357712298631668 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.5241],\n",
      "        [0.8945]])\n",
      "Iteration 5690 Training loss 0.061424724757671356 Validation loss 0.06356745958328247 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.8314],\n",
      "        [0.0409]])\n",
      "Iteration 5700 Training loss 0.06324011087417603 Validation loss 0.06353572010993958 Accuracy 0.824999988079071\n",
      "Output tensor([[0.5452],\n",
      "        [0.1273]])\n",
      "Iteration 5710 Training loss 0.06499557197093964 Validation loss 0.06359893828630447 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.4461],\n",
      "        [0.3726]])\n",
      "Iteration 5720 Training loss 0.05912946164608002 Validation loss 0.06383027136325836 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.3231],\n",
      "        [0.6544]])\n",
      "Iteration 5730 Training loss 0.06524065881967545 Validation loss 0.06359986960887909 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.2874],\n",
      "        [0.2383]])\n",
      "Iteration 5740 Training loss 0.06336516886949539 Validation loss 0.06349954009056091 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0245],\n",
      "        [0.9730]])\n",
      "Iteration 5750 Training loss 0.06353875249624252 Validation loss 0.06350251287221909 Accuracy 0.824999988079071\n",
      "Output tensor([[0.8016],\n",
      "        [0.9020]])\n",
      "Iteration 5760 Training loss 0.06350691616535187 Validation loss 0.0635126531124115 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.7605],\n",
      "        [0.0778]])\n",
      "Iteration 5770 Training loss 0.06667257100343704 Validation loss 0.06359713524580002 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.7273],\n",
      "        [0.9119]])\n",
      "Iteration 5780 Training loss 0.0630849078297615 Validation loss 0.06361538171768188 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.0233],\n",
      "        [0.3471]])\n",
      "Iteration 5790 Training loss 0.06449297815561295 Validation loss 0.06356870383024216 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.9311],\n",
      "        [0.9535]])\n",
      "Iteration 5800 Training loss 0.061093542724847794 Validation loss 0.06372136622667313 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.0731],\n",
      "        [0.3037]])\n",
      "Iteration 5810 Training loss 0.06223185732960701 Validation loss 0.06363330781459808 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.5108],\n",
      "        [0.0329]])\n",
      "Iteration 5820 Training loss 0.0635092556476593 Validation loss 0.06374295800924301 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.9115],\n",
      "        [0.0820]])\n",
      "Iteration 5830 Training loss 0.062266793102025986 Validation loss 0.06348950415849686 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.3753],\n",
      "        [0.2460]])\n",
      "Iteration 5840 Training loss 0.062098242342472076 Validation loss 0.06357934325933456 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.8634],\n",
      "        [0.0399]])\n",
      "Iteration 5850 Training loss 0.05983342230319977 Validation loss 0.06353031098842621 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.2158],\n",
      "        [0.7653]])\n",
      "Iteration 5860 Training loss 0.06105443462729454 Validation loss 0.06414126604795456 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.0814],\n",
      "        [0.8684]])\n",
      "Iteration 5870 Training loss 0.061695877462625504 Validation loss 0.0641762986779213 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.1031],\n",
      "        [0.6101]])\n",
      "Iteration 5880 Training loss 0.060335565358400345 Validation loss 0.0636485368013382 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.1344],\n",
      "        [0.7272]])\n",
      "Iteration 5890 Training loss 0.06313765794038773 Validation loss 0.0636659637093544 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.6365],\n",
      "        [0.3231]])\n",
      "Iteration 5900 Training loss 0.06803719699382782 Validation loss 0.06411928683519363 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.0971],\n",
      "        [0.0275]])\n",
      "Iteration 5910 Training loss 0.06234396994113922 Validation loss 0.0635499432682991 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.5182],\n",
      "        [0.7608]])\n",
      "Iteration 5920 Training loss 0.06307701766490936 Validation loss 0.06354275345802307 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.9837],\n",
      "        [0.4950]])\n",
      "Iteration 5930 Training loss 0.06145023927092552 Validation loss 0.06361539661884308 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.6483],\n",
      "        [0.1427]])\n",
      "Iteration 5940 Training loss 0.06434977799654007 Validation loss 0.06353408098220825 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.9643],\n",
      "        [0.9935]])\n",
      "Iteration 5950 Training loss 0.06399855762720108 Validation loss 0.06357039511203766 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.2507],\n",
      "        [0.3746]])\n",
      "Iteration 5960 Training loss 0.0630660131573677 Validation loss 0.06346198171377182 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.4874],\n",
      "        [0.2535]])\n",
      "Iteration 5970 Training loss 0.06009622663259506 Validation loss 0.06343381106853485 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5594],\n",
      "        [0.9979]])\n",
      "Iteration 5980 Training loss 0.06101919710636139 Validation loss 0.06362208724021912 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0655],\n",
      "        [0.9945]])\n",
      "Iteration 5990 Training loss 0.062433384358882904 Validation loss 0.06371702253818512 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.5991],\n",
      "        [0.7881]])\n",
      "Iteration 6000 Training loss 0.06395567208528519 Validation loss 0.0634675994515419 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.9835],\n",
      "        [0.5374]])\n",
      "Iteration 6010 Training loss 0.061650361865758896 Validation loss 0.0636235773563385 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.9867],\n",
      "        [0.6340]])\n",
      "Iteration 6020 Training loss 0.06139370799064636 Validation loss 0.06373974680900574 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.9805],\n",
      "        [0.9775]])\n",
      "Iteration 6030 Training loss 0.06241382658481598 Validation loss 0.06370056420564651 Accuracy 0.8190000057220459\n",
      "Output tensor([[0.0905],\n",
      "        [0.4934]])\n",
      "Iteration 6040 Training loss 0.0662599578499794 Validation loss 0.06358252465724945 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9010],\n",
      "        [0.9366]])\n",
      "Iteration 6050 Training loss 0.0647926926612854 Validation loss 0.06340460479259491 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.2011],\n",
      "        [0.9635]])\n",
      "Iteration 6060 Training loss 0.06278443336486816 Validation loss 0.06335523724555969 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.5898],\n",
      "        [0.9606]])\n",
      "Iteration 6070 Training loss 0.06290214508771896 Validation loss 0.06335070729255676 Accuracy 0.824999988079071\n",
      "Output tensor([[0.0458],\n",
      "        [0.9380]])\n",
      "Iteration 6080 Training loss 0.060316652059555054 Validation loss 0.06333640962839127 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.1723],\n",
      "        [0.4307]])\n",
      "Iteration 6090 Training loss 0.0660194680094719 Validation loss 0.06353256851434708 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.9516],\n",
      "        [0.7166]])\n",
      "Iteration 6100 Training loss 0.06532765179872513 Validation loss 0.06339191645383835 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.1892],\n",
      "        [0.9628]])\n",
      "Iteration 6110 Training loss 0.06048567220568657 Validation loss 0.06335710734128952 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.4037],\n",
      "        [0.3844]])\n",
      "Iteration 6120 Training loss 0.06453002244234085 Validation loss 0.0638745054602623 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.7796],\n",
      "        [0.0505]])\n",
      "Iteration 6130 Training loss 0.062441837042570114 Validation loss 0.06355419009923935 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.2650],\n",
      "        [0.6969]])\n",
      "Iteration 6140 Training loss 0.06427034735679626 Validation loss 0.06332416832447052 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.4038],\n",
      "        [0.3604]])\n",
      "Iteration 6150 Training loss 0.06540864706039429 Validation loss 0.06365669518709183 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.0613],\n",
      "        [0.0097]])\n",
      "Iteration 6160 Training loss 0.061379000544548035 Validation loss 0.06340517848730087 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.9833],\n",
      "        [0.8042]])\n",
      "Iteration 6170 Training loss 0.06484030187129974 Validation loss 0.06344497203826904 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.3641],\n",
      "        [0.9922]])\n",
      "Iteration 6180 Training loss 0.06422168016433716 Validation loss 0.06332702934741974 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.0846],\n",
      "        [0.0912]])\n",
      "Iteration 6190 Training loss 0.060211874544620514 Validation loss 0.06335698813199997 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.1752],\n",
      "        [0.4520]])\n",
      "Iteration 6200 Training loss 0.06389183551073074 Validation loss 0.06336376070976257 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.2761],\n",
      "        [0.8333]])\n",
      "Iteration 6210 Training loss 0.06116263568401337 Validation loss 0.06358586251735687 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9110],\n",
      "        [0.1154]])\n",
      "Iteration 6220 Training loss 0.0617443323135376 Validation loss 0.06357087194919586 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.1576],\n",
      "        [0.6756]])\n",
      "Iteration 6230 Training loss 0.06270162016153336 Validation loss 0.06384757161140442 Accuracy 0.8184999823570251\n",
      "Output tensor([[0.8843],\n",
      "        [0.3960]])\n",
      "Iteration 6240 Training loss 0.06286986172199249 Validation loss 0.06334102153778076 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.2490],\n",
      "        [0.8358]])\n",
      "Iteration 6250 Training loss 0.06153058260679245 Validation loss 0.06338919699192047 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.1801],\n",
      "        [0.0368]])\n",
      "Iteration 6260 Training loss 0.05981570854783058 Validation loss 0.06349053233861923 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.3052],\n",
      "        [0.9813]])\n",
      "Iteration 6270 Training loss 0.059269651770591736 Validation loss 0.06335122883319855 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.9761],\n",
      "        [0.3156]])\n",
      "Iteration 6280 Training loss 0.06173532083630562 Validation loss 0.06326573342084885 Accuracy 0.828000009059906\n",
      "Output tensor([[0.5376],\n",
      "        [0.5858]])\n",
      "Iteration 6290 Training loss 0.06506048142910004 Validation loss 0.06325197964906693 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.6260],\n",
      "        [0.0369]])\n",
      "Iteration 6300 Training loss 0.06391890347003937 Validation loss 0.06367933750152588 Accuracy 0.8174999952316284\n",
      "Output tensor([[0.3808],\n",
      "        [0.8481]])\n",
      "Iteration 6310 Training loss 0.06142015382647514 Validation loss 0.063302181661129 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.0394],\n",
      "        [0.3880]])\n",
      "Iteration 6320 Training loss 0.058658137917518616 Validation loss 0.06369845569133759 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.9861],\n",
      "        [0.8270]])\n",
      "Iteration 6330 Training loss 0.06498248875141144 Validation loss 0.06359250843524933 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.3653],\n",
      "        [0.9192]])\n",
      "Iteration 6340 Training loss 0.06195548176765442 Validation loss 0.06351664662361145 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.4470],\n",
      "        [0.9524]])\n",
      "Iteration 6350 Training loss 0.0612601712346077 Validation loss 0.06340384483337402 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.9607],\n",
      "        [0.5936]])\n",
      "Iteration 6360 Training loss 0.0705098956823349 Validation loss 0.0638716071844101 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.9978],\n",
      "        [0.3714]])\n",
      "Iteration 6370 Training loss 0.06098544970154762 Validation loss 0.0633777305483818 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.2437],\n",
      "        [0.9936]])\n",
      "Iteration 6380 Training loss 0.0652812048792839 Validation loss 0.06338253617286682 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.1085],\n",
      "        [0.7587]])\n",
      "Iteration 6390 Training loss 0.0634450763463974 Validation loss 0.06326158344745636 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.6545],\n",
      "        [0.6727]])\n",
      "Iteration 6400 Training loss 0.06232256442308426 Validation loss 0.06340888887643814 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.9612],\n",
      "        [0.8512]])\n",
      "Iteration 6410 Training loss 0.059957463294267654 Validation loss 0.06331923604011536 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.0118],\n",
      "        [0.8873]])\n",
      "Iteration 6420 Training loss 0.059446271508932114 Validation loss 0.06333070993423462 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.9835],\n",
      "        [0.6355]])\n",
      "Iteration 6430 Training loss 0.06439060717821121 Validation loss 0.06341089308261871 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.4565],\n",
      "        [0.4119]])\n",
      "Iteration 6440 Training loss 0.06040932983160019 Validation loss 0.06323818862438202 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.7850],\n",
      "        [0.3338]])\n",
      "Iteration 6450 Training loss 0.06317798048257828 Validation loss 0.06346707791090012 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.1451],\n",
      "        [0.1677]])\n",
      "Iteration 6460 Training loss 0.06313341110944748 Validation loss 0.06321553140878677 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0707],\n",
      "        [0.7435]])\n",
      "Iteration 6470 Training loss 0.06331946700811386 Validation loss 0.06367884576320648 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.5890],\n",
      "        [0.6090]])\n",
      "Iteration 6480 Training loss 0.060779839754104614 Validation loss 0.0631909891963005 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9862],\n",
      "        [0.9884]])\n",
      "Iteration 6490 Training loss 0.0622565858066082 Validation loss 0.06325223296880722 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.8769],\n",
      "        [0.1822]])\n",
      "Iteration 6500 Training loss 0.06462182849645615 Validation loss 0.0633326843380928 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.8506],\n",
      "        [0.5141]])\n",
      "Iteration 6510 Training loss 0.06335198134183884 Validation loss 0.06356237083673477 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.6129],\n",
      "        [0.3618]])\n",
      "Iteration 6520 Training loss 0.06006380915641785 Validation loss 0.06316480040550232 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0578],\n",
      "        [0.3999]])\n",
      "Iteration 6530 Training loss 0.05976322293281555 Validation loss 0.06333306431770325 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.6915],\n",
      "        [0.0074]])\n",
      "Iteration 6540 Training loss 0.06172472983598709 Validation loss 0.06332149356603622 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.6818],\n",
      "        [0.1661]])\n",
      "Iteration 6550 Training loss 0.06516733020544052 Validation loss 0.06375326961278915 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.0937],\n",
      "        [0.0556]])\n",
      "Iteration 6560 Training loss 0.06587284803390503 Validation loss 0.06343299895524979 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.7954],\n",
      "        [0.0456]])\n",
      "Iteration 6570 Training loss 0.06311830878257751 Validation loss 0.06343036144971848 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.3943],\n",
      "        [0.0103]])\n",
      "Iteration 6580 Training loss 0.05792196840047836 Validation loss 0.0632898285984993 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.8184],\n",
      "        [0.8895]])\n",
      "Iteration 6590 Training loss 0.0592733658850193 Validation loss 0.06349936872720718 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.3576],\n",
      "        [0.9761]])\n",
      "Iteration 6600 Training loss 0.06095797196030617 Validation loss 0.06341972202062607 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.9227],\n",
      "        [0.0174]])\n",
      "Iteration 6610 Training loss 0.06323081254959106 Validation loss 0.06314212828874588 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.7780],\n",
      "        [0.1372]])\n",
      "Iteration 6620 Training loss 0.06152670457959175 Validation loss 0.06364184617996216 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.7253],\n",
      "        [0.9928]])\n",
      "Iteration 6630 Training loss 0.062240008264780045 Validation loss 0.06374138593673706 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.4935],\n",
      "        [0.9178]])\n",
      "Iteration 6640 Training loss 0.06080564856529236 Validation loss 0.06332066655158997 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.8357],\n",
      "        [0.1131]])\n",
      "Iteration 6650 Training loss 0.059765830636024475 Validation loss 0.06313920766115189 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.2248],\n",
      "        [0.0984]])\n",
      "Iteration 6660 Training loss 0.058229200541973114 Validation loss 0.06342697143554688 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1457],\n",
      "        [0.6365]])\n",
      "Iteration 6670 Training loss 0.06164195388555527 Validation loss 0.06320755928754807 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.5940],\n",
      "        [0.0472]])\n",
      "Iteration 6680 Training loss 0.06080856919288635 Validation loss 0.06318917125463486 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.9339],\n",
      "        [0.1891]])\n",
      "Iteration 6690 Training loss 0.06112884730100632 Validation loss 0.06341153383255005 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.0228],\n",
      "        [0.0067]])\n",
      "Iteration 6700 Training loss 0.061021994799375534 Validation loss 0.06333933770656586 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.0952],\n",
      "        [0.8698]])\n",
      "Iteration 6710 Training loss 0.06277203559875488 Validation loss 0.06354641914367676 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.1322],\n",
      "        [0.2188]])\n",
      "Iteration 6720 Training loss 0.06193039193749428 Validation loss 0.06368052959442139 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.7137],\n",
      "        [0.0685]])\n",
      "Iteration 6730 Training loss 0.06216961145401001 Validation loss 0.06309621781110764 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.3347],\n",
      "        [0.5622]])\n",
      "Iteration 6740 Training loss 0.06167130917310715 Validation loss 0.06315989792346954 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.5822],\n",
      "        [0.0725]])\n",
      "Iteration 6750 Training loss 0.062342025339603424 Validation loss 0.06313648074865341 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.4758],\n",
      "        [0.4581]])\n",
      "Iteration 6760 Training loss 0.05991299822926521 Validation loss 0.06323759257793427 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.9998],\n",
      "        [0.3844]])\n",
      "Iteration 6770 Training loss 0.058015525341033936 Validation loss 0.06326660513877869 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.4987],\n",
      "        [0.9378]])\n",
      "Iteration 6780 Training loss 0.06166033819317818 Validation loss 0.06337345391511917 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.9103],\n",
      "        [0.9900]])\n",
      "Iteration 6790 Training loss 0.061579324305057526 Validation loss 0.0636693462729454 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.7255],\n",
      "        [0.1298]])\n",
      "Iteration 6800 Training loss 0.06136993318796158 Validation loss 0.06379992514848709 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.3035],\n",
      "        [0.0064]])\n",
      "Iteration 6810 Training loss 0.0630001649260521 Validation loss 0.06333256512880325 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.1283],\n",
      "        [0.6934]])\n",
      "Iteration 6820 Training loss 0.06439583748579025 Validation loss 0.06382330507040024 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.8488],\n",
      "        [0.9211]])\n",
      "Iteration 6830 Training loss 0.06171755865216255 Validation loss 0.06316777318716049 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.7923],\n",
      "        [0.0330]])\n",
      "Iteration 6840 Training loss 0.0623784102499485 Validation loss 0.06318703293800354 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.2226],\n",
      "        [0.3165]])\n",
      "Iteration 6850 Training loss 0.05810105800628662 Validation loss 0.06308669596910477 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.9981],\n",
      "        [0.9996]])\n",
      "Iteration 6860 Training loss 0.062417708337306976 Validation loss 0.06312697380781174 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.9802],\n",
      "        [0.2018]])\n",
      "Iteration 6870 Training loss 0.06052347272634506 Validation loss 0.06317080557346344 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.3003],\n",
      "        [0.0344]])\n",
      "Iteration 6880 Training loss 0.058678146451711655 Validation loss 0.06321611255407333 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.3107],\n",
      "        [0.2249]])\n",
      "Iteration 6890 Training loss 0.06248707324266434 Validation loss 0.06304297596216202 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0160],\n",
      "        [0.9539]])\n",
      "Iteration 6900 Training loss 0.06363264471292496 Validation loss 0.06302601844072342 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1439],\n",
      "        [0.5052]])\n",
      "Iteration 6910 Training loss 0.06317552924156189 Validation loss 0.06313014775514603 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.4078],\n",
      "        [0.9782]])\n",
      "Iteration 6920 Training loss 0.06452536582946777 Validation loss 0.06337866187095642 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.2656],\n",
      "        [0.9951]])\n",
      "Iteration 6930 Training loss 0.061883412301540375 Validation loss 0.06336063146591187 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.2702],\n",
      "        [0.5319]])\n",
      "Iteration 6940 Training loss 0.06359251588582993 Validation loss 0.0630536675453186 Accuracy 0.824999988079071\n",
      "Output tensor([[0.2377],\n",
      "        [0.0351]])\n",
      "Iteration 6950 Training loss 0.06058159098029137 Validation loss 0.06309359520673752 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.5430],\n",
      "        [0.0412]])\n",
      "Iteration 6960 Training loss 0.05977730080485344 Validation loss 0.06302042305469513 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0415],\n",
      "        [0.5516]])\n",
      "Iteration 6970 Training loss 0.06238216534256935 Validation loss 0.06346088647842407 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.0327],\n",
      "        [0.9922]])\n",
      "Iteration 6980 Training loss 0.06086335703730583 Validation loss 0.06325959414243698 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.9434],\n",
      "        [0.3616]])\n",
      "Iteration 6990 Training loss 0.06409146636724472 Validation loss 0.06335984915494919 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5010],\n",
      "        [0.7045]])\n",
      "Iteration 7000 Training loss 0.06254453212022781 Validation loss 0.06330478936433792 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5679],\n",
      "        [0.0867]])\n",
      "Iteration 7010 Training loss 0.06085851415991783 Validation loss 0.062984898686409 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0739],\n",
      "        [0.0167]])\n",
      "Iteration 7020 Training loss 0.05910061299800873 Validation loss 0.06301818788051605 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.7525],\n",
      "        [0.9966]])\n",
      "Iteration 7030 Training loss 0.06289414316415787 Validation loss 0.06326393783092499 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.4019],\n",
      "        [0.0431]])\n",
      "Iteration 7040 Training loss 0.06136365607380867 Validation loss 0.06311755627393723 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.6608],\n",
      "        [0.3412]])\n",
      "Iteration 7050 Training loss 0.06109505519270897 Validation loss 0.06378302723169327 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.0849],\n",
      "        [0.1660]])\n",
      "Iteration 7060 Training loss 0.062339115887880325 Validation loss 0.06310756504535675 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.4613],\n",
      "        [0.5880]])\n",
      "Iteration 7070 Training loss 0.06133062765002251 Validation loss 0.06322872638702393 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.4577],\n",
      "        [0.3551]])\n",
      "Iteration 7080 Training loss 0.06109052896499634 Validation loss 0.06320668756961823 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.0253],\n",
      "        [0.6578]])\n",
      "Iteration 7090 Training loss 0.061705078929662704 Validation loss 0.0635313168168068 Accuracy 0.8199999928474426\n",
      "Output tensor([[0.3105],\n",
      "        [0.5109]])\n",
      "Iteration 7100 Training loss 0.061411451548337936 Validation loss 0.06323696672916412 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.1010],\n",
      "        [0.7619]])\n",
      "Iteration 7110 Training loss 0.062040530145168304 Validation loss 0.06320318579673767 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.9966],\n",
      "        [0.7292]])\n",
      "Iteration 7120 Training loss 0.06403113901615143 Validation loss 0.06319749355316162 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.7955],\n",
      "        [0.0780]])\n",
      "Iteration 7130 Training loss 0.06051100790500641 Validation loss 0.06354065239429474 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.4055],\n",
      "        [0.6828]])\n",
      "Iteration 7140 Training loss 0.06156075373291969 Validation loss 0.06307515501976013 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.5540],\n",
      "        [0.8716]])\n",
      "Iteration 7150 Training loss 0.05971682816743851 Validation loss 0.06310427933931351 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.6553],\n",
      "        [0.0464]])\n",
      "Iteration 7160 Training loss 0.06405699253082275 Validation loss 0.0638699159026146 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.8806],\n",
      "        [0.7349]])\n",
      "Iteration 7170 Training loss 0.05971322953701019 Validation loss 0.0629253014922142 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.7475],\n",
      "        [0.1457]])\n",
      "Iteration 7180 Training loss 0.060485418885946274 Validation loss 0.06291528791189194 Accuracy 0.828499972820282\n",
      "Output tensor([[0.2327],\n",
      "        [0.3953]])\n",
      "Iteration 7190 Training loss 0.05777627229690552 Validation loss 0.06355161964893341 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.5564],\n",
      "        [0.5279]])\n",
      "Iteration 7200 Training loss 0.06017465516924858 Validation loss 0.06340747326612473 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.1058],\n",
      "        [0.2492]])\n",
      "Iteration 7210 Training loss 0.058241263031959534 Validation loss 0.06310799717903137 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5470],\n",
      "        [0.9971]])\n",
      "Iteration 7220 Training loss 0.06113097444176674 Validation loss 0.06293293833732605 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.4315],\n",
      "        [0.5974]])\n",
      "Iteration 7230 Training loss 0.060490041971206665 Validation loss 0.06308288127183914 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.9149],\n",
      "        [0.7653]])\n",
      "Iteration 7240 Training loss 0.06201467290520668 Validation loss 0.06342075765132904 Accuracy 0.8209999799728394\n",
      "Output tensor([[0.5638],\n",
      "        [0.9447]])\n",
      "Iteration 7250 Training loss 0.05892650783061981 Validation loss 0.06308618932962418 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.2384],\n",
      "        [0.0323]])\n",
      "Iteration 7260 Training loss 0.0593889094889164 Validation loss 0.0630829706788063 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.5043],\n",
      "        [0.8260]])\n",
      "Iteration 7270 Training loss 0.06243663653731346 Validation loss 0.06319612264633179 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.2907],\n",
      "        [0.6590]])\n",
      "Iteration 7280 Training loss 0.06369491666555405 Validation loss 0.06297948956489563 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.2632],\n",
      "        [0.9475]])\n",
      "Iteration 7290 Training loss 0.05934229865670204 Validation loss 0.063430555164814 Accuracy 0.8215000033378601\n",
      "Output tensor([[0.1381],\n",
      "        [0.0603]])\n",
      "Iteration 7300 Training loss 0.059304770082235336 Validation loss 0.06307925283908844 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.0447],\n",
      "        [0.9654]])\n",
      "Iteration 7310 Training loss 0.06282790005207062 Validation loss 0.06295328587293625 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.8346],\n",
      "        [0.8316]])\n",
      "Iteration 7320 Training loss 0.0637943297624588 Validation loss 0.06292098760604858 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.8658],\n",
      "        [0.7877]])\n",
      "Iteration 7330 Training loss 0.06057395786046982 Validation loss 0.06302322447299957 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5881],\n",
      "        [0.5782]])\n",
      "Iteration 7340 Training loss 0.0615367591381073 Validation loss 0.06307194381952286 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.8234],\n",
      "        [0.6491]])\n",
      "Iteration 7350 Training loss 0.061307601630687714 Validation loss 0.0629088282585144 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.1960],\n",
      "        [0.0074]])\n",
      "Iteration 7360 Training loss 0.06148112192749977 Validation loss 0.0632716715335846 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.5006],\n",
      "        [0.1300]])\n",
      "Iteration 7370 Training loss 0.06582621484994888 Validation loss 0.06308704614639282 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.0245],\n",
      "        [0.8034]])\n",
      "Iteration 7380 Training loss 0.06372492015361786 Validation loss 0.06299961358308792 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.0304],\n",
      "        [0.9991]])\n",
      "Iteration 7390 Training loss 0.059957049787044525 Validation loss 0.06285055726766586 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2481],\n",
      "        [0.1459]])\n",
      "Iteration 7400 Training loss 0.06003175303339958 Validation loss 0.06313187628984451 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.7421],\n",
      "        [0.1450]])\n",
      "Iteration 7410 Training loss 0.06104210019111633 Validation loss 0.06284590810537338 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0365],\n",
      "        [0.1590]])\n",
      "Iteration 7420 Training loss 0.06343510746955872 Validation loss 0.06338701397180557 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.0479],\n",
      "        [0.6413]])\n",
      "Iteration 7430 Training loss 0.061861008405685425 Validation loss 0.06302688270807266 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0336],\n",
      "        [0.1277]])\n",
      "Iteration 7440 Training loss 0.06252864748239517 Validation loss 0.06325230002403259 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8879],\n",
      "        [0.1169]])\n",
      "Iteration 7450 Training loss 0.06079550459980965 Validation loss 0.06306000053882599 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.0518],\n",
      "        [0.6838]])\n",
      "Iteration 7460 Training loss 0.061694927513599396 Validation loss 0.06296266615390778 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.2420],\n",
      "        [0.9882]])\n",
      "Iteration 7470 Training loss 0.06313620507717133 Validation loss 0.06284459680318832 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9838],\n",
      "        [0.9523]])\n",
      "Iteration 7480 Training loss 0.05888398736715317 Validation loss 0.06289584934711456 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.0292],\n",
      "        [0.6841]])\n",
      "Iteration 7490 Training loss 0.06232389807701111 Validation loss 0.06308144330978394 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0402],\n",
      "        [0.0551]])\n",
      "Iteration 7500 Training loss 0.06152678653597832 Validation loss 0.06307745724916458 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.8400],\n",
      "        [0.9544]])\n",
      "Iteration 7510 Training loss 0.060426779091358185 Validation loss 0.06283101439476013 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.8580],\n",
      "        [0.8572]])\n",
      "Iteration 7520 Training loss 0.059945009648799896 Validation loss 0.06324800103902817 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.9620],\n",
      "        [0.5745]])\n",
      "Iteration 7530 Training loss 0.06103229522705078 Validation loss 0.06281246989965439 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.7149],\n",
      "        [0.8526]])\n",
      "Iteration 7540 Training loss 0.05995935946702957 Validation loss 0.06278664618730545 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1872],\n",
      "        [0.5249]])\n",
      "Iteration 7550 Training loss 0.060255762189626694 Validation loss 0.06293909251689911 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.1650],\n",
      "        [0.9206]])\n",
      "Iteration 7560 Training loss 0.06350011378526688 Validation loss 0.06279344856739044 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.2474],\n",
      "        [0.1406]])\n",
      "Iteration 7570 Training loss 0.05848255753517151 Validation loss 0.06279582530260086 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2857],\n",
      "        [0.9744]])\n",
      "Iteration 7580 Training loss 0.06100945919752121 Validation loss 0.06311424821615219 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0840],\n",
      "        [0.9992]])\n",
      "Iteration 7590 Training loss 0.058737874031066895 Validation loss 0.06336056441068649 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.1024],\n",
      "        [0.9731]])\n",
      "Iteration 7600 Training loss 0.06312067806720734 Validation loss 0.0629887580871582 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8955],\n",
      "        [0.0095]])\n",
      "Iteration 7610 Training loss 0.06048440560698509 Validation loss 0.06324803829193115 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.1223],\n",
      "        [0.9133]])\n",
      "Iteration 7620 Training loss 0.06281423568725586 Validation loss 0.06339049339294434 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5695],\n",
      "        [0.3518]])\n",
      "Iteration 7630 Training loss 0.06216806173324585 Validation loss 0.06302093714475632 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9892],\n",
      "        [0.2941]])\n",
      "Iteration 7640 Training loss 0.06424254179000854 Validation loss 0.06277833133935928 Accuracy 0.828499972820282\n",
      "Output tensor([[0.1113],\n",
      "        [0.6724]])\n",
      "Iteration 7650 Training loss 0.06410058587789536 Validation loss 0.06289365142583847 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.9953],\n",
      "        [0.1263]])\n",
      "Iteration 7660 Training loss 0.06573794037103653 Validation loss 0.0630539208650589 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.5063],\n",
      "        [0.0407]])\n",
      "Iteration 7670 Training loss 0.06241217628121376 Validation loss 0.06299656629562378 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.7651],\n",
      "        [0.5094]])\n",
      "Iteration 7680 Training loss 0.062332604080438614 Validation loss 0.06294821947813034 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.1809],\n",
      "        [0.6748]])\n",
      "Iteration 7690 Training loss 0.06099167466163635 Validation loss 0.06294184923171997 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.0510],\n",
      "        [0.8073]])\n",
      "Iteration 7700 Training loss 0.06228938698768616 Validation loss 0.06290578097105026 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.7592],\n",
      "        [0.9815]])\n",
      "Iteration 7710 Training loss 0.06213703006505966 Validation loss 0.0628041923046112 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0092],\n",
      "        [0.0778]])\n",
      "Iteration 7720 Training loss 0.06097152829170227 Validation loss 0.06310170888900757 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.9934],\n",
      "        [0.9731]])\n",
      "Iteration 7730 Training loss 0.0614377036690712 Validation loss 0.06278149038553238 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.1704],\n",
      "        [0.3457]])\n",
      "Iteration 7740 Training loss 0.05819777399301529 Validation loss 0.06273793429136276 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.1118],\n",
      "        [0.8275]])\n",
      "Iteration 7750 Training loss 0.06664672493934631 Validation loss 0.06326799094676971 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.6766],\n",
      "        [0.5413]])\n",
      "Iteration 7760 Training loss 0.05884438380599022 Validation loss 0.06274878233671188 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0531],\n",
      "        [0.1310]])\n",
      "Iteration 7770 Training loss 0.06215985119342804 Validation loss 0.06279250234365463 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.7827],\n",
      "        [0.0336]])\n",
      "Iteration 7780 Training loss 0.05979921296238899 Validation loss 0.06309153884649277 Accuracy 0.824999988079071\n",
      "Output tensor([[0.0632],\n",
      "        [0.9895]])\n",
      "Iteration 7790 Training loss 0.05871189385652542 Validation loss 0.06300070881843567 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.9221],\n",
      "        [0.8796]])\n",
      "Iteration 7800 Training loss 0.06482899188995361 Validation loss 0.06278282403945923 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.7103],\n",
      "        [0.9717]])\n",
      "Iteration 7810 Training loss 0.060487374663352966 Validation loss 0.06277555972337723 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.8623],\n",
      "        [0.9512]])\n",
      "Iteration 7820 Training loss 0.06168241798877716 Validation loss 0.06306396424770355 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.2251],\n",
      "        [0.4329]])\n",
      "Iteration 7830 Training loss 0.062230661511421204 Validation loss 0.06280001997947693 Accuracy 0.824999988079071\n",
      "Output tensor([[0.9914],\n",
      "        [0.7827]])\n",
      "Iteration 7840 Training loss 0.06281674653291702 Validation loss 0.06272832304239273 Accuracy 0.828000009059906\n",
      "Output tensor([[0.4011],\n",
      "        [0.9158]])\n",
      "Iteration 7850 Training loss 0.061370573937892914 Validation loss 0.0627613440155983 Accuracy 0.824999988079071\n",
      "Output tensor([[0.9873],\n",
      "        [0.1409]])\n",
      "Iteration 7860 Training loss 0.06175052747130394 Validation loss 0.06309579312801361 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.7092],\n",
      "        [0.5532]])\n",
      "Iteration 7870 Training loss 0.06116208806633949 Validation loss 0.06312576681375504 Accuracy 0.824999988079071\n",
      "Output tensor([[0.2014],\n",
      "        [0.7924]])\n",
      "Iteration 7880 Training loss 0.06045330688357353 Validation loss 0.06317026168107986 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.3698],\n",
      "        [0.8562]])\n",
      "Iteration 7890 Training loss 0.06343362480401993 Validation loss 0.06268125772476196 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9091],\n",
      "        [0.8059]])\n",
      "Iteration 7900 Training loss 0.06133230775594711 Validation loss 0.06317830085754395 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.4345],\n",
      "        [0.9201]])\n",
      "Iteration 7910 Training loss 0.06275252997875214 Validation loss 0.06297953426837921 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.0423],\n",
      "        [0.0887]])\n",
      "Iteration 7920 Training loss 0.059948138892650604 Validation loss 0.06306977570056915 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8885],\n",
      "        [0.0046]])\n",
      "Iteration 7930 Training loss 0.060212522745132446 Validation loss 0.06290316581726074 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8462],\n",
      "        [0.0758]])\n",
      "Iteration 7940 Training loss 0.05976378917694092 Validation loss 0.06332017481327057 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9495],\n",
      "        [0.0416]])\n",
      "Iteration 7950 Training loss 0.06026679649949074 Validation loss 0.06306528300046921 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9209],\n",
      "        [0.4931]])\n",
      "Iteration 7960 Training loss 0.060628391802310944 Validation loss 0.06278562545776367 Accuracy 0.824999988079071\n",
      "Output tensor([[0.0104],\n",
      "        [0.4459]])\n",
      "Iteration 7970 Training loss 0.06251920014619827 Validation loss 0.06344103068113327 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.2684],\n",
      "        [0.8666]])\n",
      "Iteration 7980 Training loss 0.06075417250394821 Validation loss 0.06281469017267227 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.7054],\n",
      "        [0.7356]])\n",
      "Iteration 7990 Training loss 0.0633968859910965 Validation loss 0.06312771886587143 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8594],\n",
      "        [0.0108]])\n",
      "Iteration 8000 Training loss 0.0623023696243763 Validation loss 0.06276297569274902 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.8282],\n",
      "        [0.1502]])\n",
      "Iteration 8010 Training loss 0.06445474922657013 Validation loss 0.06283951550722122 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.4596],\n",
      "        [0.1130]])\n",
      "Iteration 8020 Training loss 0.06221723556518555 Validation loss 0.06310559064149857 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.8132],\n",
      "        [0.5679]])\n",
      "Iteration 8030 Training loss 0.05860191956162453 Validation loss 0.0626751035451889 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.6700],\n",
      "        [0.9210]])\n",
      "Iteration 8040 Training loss 0.05992542952299118 Validation loss 0.06277104467153549 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8607],\n",
      "        [0.5321]])\n",
      "Iteration 8050 Training loss 0.057807449251413345 Validation loss 0.0627879649400711 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0580],\n",
      "        [0.0258]])\n",
      "Iteration 8060 Training loss 0.06008891388773918 Validation loss 0.06277429312467575 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.4966],\n",
      "        [0.9801]])\n",
      "Iteration 8070 Training loss 0.05924634262919426 Validation loss 0.06291205435991287 Accuracy 0.824999988079071\n",
      "Output tensor([[0.1207],\n",
      "        [0.3308]])\n",
      "Iteration 8080 Training loss 0.06290414184331894 Validation loss 0.06293640285730362 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.1821],\n",
      "        [0.6489]])\n",
      "Iteration 8090 Training loss 0.06253548711538315 Validation loss 0.06262955069541931 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9819],\n",
      "        [0.1120]])\n",
      "Iteration 8100 Training loss 0.06385248899459839 Validation loss 0.06263647228479385 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.9805],\n",
      "        [0.0242]])\n",
      "Iteration 8110 Training loss 0.059516921639442444 Validation loss 0.06257910281419754 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.5318],\n",
      "        [0.1165]])\n",
      "Iteration 8120 Training loss 0.06224937364459038 Validation loss 0.06265773624181747 Accuracy 0.824999988079071\n",
      "Output tensor([[0.1418],\n",
      "        [0.7164]])\n",
      "Iteration 8130 Training loss 0.06065681204199791 Validation loss 0.06260231137275696 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9242],\n",
      "        [0.1257]])\n",
      "Iteration 8140 Training loss 0.05989648401737213 Validation loss 0.0629085898399353 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9861],\n",
      "        [0.0251]])\n",
      "Iteration 8150 Training loss 0.06176881119608879 Validation loss 0.06257442384958267 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.5538],\n",
      "        [0.8488]])\n",
      "Iteration 8160 Training loss 0.06023632362484932 Validation loss 0.06343258917331696 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.0679],\n",
      "        [0.2515]])\n",
      "Iteration 8170 Training loss 0.06006758287549019 Validation loss 0.0628150925040245 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.7115],\n",
      "        [0.6017]])\n",
      "Iteration 8180 Training loss 0.05759892612695694 Validation loss 0.06257432699203491 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0567],\n",
      "        [0.8226]])\n",
      "Iteration 8190 Training loss 0.05986771360039711 Validation loss 0.06303752213716507 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0324],\n",
      "        [0.5606]])\n",
      "Iteration 8200 Training loss 0.06346386671066284 Validation loss 0.06261546164751053 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.9879],\n",
      "        [0.8073]])\n",
      "Iteration 8210 Training loss 0.06083812564611435 Validation loss 0.06285735219717026 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8663],\n",
      "        [0.0612]])\n",
      "Iteration 8220 Training loss 0.06445028632879257 Validation loss 0.06303685903549194 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.3412],\n",
      "        [0.1063]])\n",
      "Iteration 8230 Training loss 0.0594143308699131 Validation loss 0.06264463067054749 Accuracy 0.824999988079071\n",
      "Output tensor([[0.0337],\n",
      "        [0.8567]])\n",
      "Iteration 8240 Training loss 0.05835297331213951 Validation loss 0.06256866455078125 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9153],\n",
      "        [0.9517]])\n",
      "Iteration 8250 Training loss 0.05954258143901825 Validation loss 0.06275326013565063 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8640],\n",
      "        [0.8589]])\n",
      "Iteration 8260 Training loss 0.05917796492576599 Validation loss 0.06282467395067215 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.4816],\n",
      "        [0.7607]])\n",
      "Iteration 8270 Training loss 0.05789310857653618 Validation loss 0.06309660524129868 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.6210],\n",
      "        [0.9479]])\n",
      "Iteration 8280 Training loss 0.05681722238659859 Validation loss 0.06267204135656357 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.5378],\n",
      "        [0.6170]])\n",
      "Iteration 8290 Training loss 0.05977068841457367 Validation loss 0.06270972639322281 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.7774],\n",
      "        [0.9853]])\n",
      "Iteration 8300 Training loss 0.06289158761501312 Validation loss 0.06258823722600937 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8245],\n",
      "        [0.9878]])\n",
      "Iteration 8310 Training loss 0.06056676432490349 Validation loss 0.06282240897417068 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.2620],\n",
      "        [0.7829]])\n",
      "Iteration 8320 Training loss 0.060386721044778824 Validation loss 0.06284809112548828 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.4879],\n",
      "        [0.1621]])\n",
      "Iteration 8330 Training loss 0.060417525470256805 Validation loss 0.06292828172445297 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.6053],\n",
      "        [0.2255]])\n",
      "Iteration 8340 Training loss 0.06291968375444412 Validation loss 0.06287432461977005 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.7718],\n",
      "        [0.7818]])\n",
      "Iteration 8350 Training loss 0.058724820613861084 Validation loss 0.06297241151332855 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.4005],\n",
      "        [0.5166]])\n",
      "Iteration 8360 Training loss 0.061895355582237244 Validation loss 0.06268852204084396 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.8257],\n",
      "        [0.9004]])\n",
      "Iteration 8370 Training loss 0.061206743121147156 Validation loss 0.06280922144651413 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9892],\n",
      "        [0.0350]])\n",
      "Iteration 8380 Training loss 0.06133425608277321 Validation loss 0.06262938678264618 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9491],\n",
      "        [0.6697]])\n",
      "Iteration 8390 Training loss 0.06027710437774658 Validation loss 0.06251393258571625 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8750],\n",
      "        [0.0715]])\n",
      "Iteration 8400 Training loss 0.06142581254243851 Validation loss 0.0625276267528534 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.3997],\n",
      "        [0.1683]])\n",
      "Iteration 8410 Training loss 0.0600210502743721 Validation loss 0.06277765333652496 Accuracy 0.824999988079071\n",
      "Output tensor([[0.9535],\n",
      "        [0.0272]])\n",
      "Iteration 8420 Training loss 0.06126868352293968 Validation loss 0.06363265216350555 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.3775],\n",
      "        [0.0363]])\n",
      "Iteration 8430 Training loss 0.060770951211452484 Validation loss 0.06296456605195999 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.5149],\n",
      "        [0.8543]])\n",
      "Iteration 8440 Training loss 0.05922161787748337 Validation loss 0.06272966414690018 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.5158],\n",
      "        [0.8199]])\n",
      "Iteration 8450 Training loss 0.0619831457734108 Validation loss 0.06287550926208496 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8202],\n",
      "        [0.9679]])\n",
      "Iteration 8460 Training loss 0.06063912808895111 Validation loss 0.06295132637023926 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9708],\n",
      "        [0.3740]])\n",
      "Iteration 8470 Training loss 0.0600188784301281 Validation loss 0.0629308670759201 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9979],\n",
      "        [0.1148]])\n",
      "Iteration 8480 Training loss 0.06062362715601921 Validation loss 0.06291597336530685 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.1055],\n",
      "        [0.0344]])\n",
      "Iteration 8490 Training loss 0.05956074222922325 Validation loss 0.06258269399404526 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.5888],\n",
      "        [0.8874]])\n",
      "Iteration 8500 Training loss 0.05667586997151375 Validation loss 0.06264738738536835 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.7517],\n",
      "        [0.0189]])\n",
      "Iteration 8510 Training loss 0.06548568606376648 Validation loss 0.0626361221075058 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.2064],\n",
      "        [0.7704]])\n",
      "Iteration 8520 Training loss 0.060641441494226456 Validation loss 0.06300117075443268 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9169],\n",
      "        [0.7572]])\n",
      "Iteration 8530 Training loss 0.06014730781316757 Validation loss 0.06257249414920807 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.1229],\n",
      "        [0.2022]])\n",
      "Iteration 8540 Training loss 0.059141211211681366 Validation loss 0.06272538751363754 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8138],\n",
      "        [0.9224]])\n",
      "Iteration 8550 Training loss 0.0627683475613594 Validation loss 0.06250957399606705 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.9208],\n",
      "        [0.0183]])\n",
      "Iteration 8560 Training loss 0.0608978271484375 Validation loss 0.06284985691308975 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.6018],\n",
      "        [0.9380]])\n",
      "Iteration 8570 Training loss 0.06308659166097641 Validation loss 0.062482573091983795 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.5982],\n",
      "        [0.8048]])\n",
      "Iteration 8580 Training loss 0.060729969292879105 Validation loss 0.06296371668577194 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0511],\n",
      "        [0.7440]])\n",
      "Iteration 8590 Training loss 0.05914667248725891 Validation loss 0.06268271058797836 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.1973],\n",
      "        [0.9294]])\n",
      "Iteration 8600 Training loss 0.056396640837192535 Validation loss 0.06277651339769363 Accuracy 0.824999988079071\n",
      "Output tensor([[0.4734],\n",
      "        [0.0596]])\n",
      "Iteration 8610 Training loss 0.05931764841079712 Validation loss 0.06279657036066055 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.3233],\n",
      "        [0.2526]])\n",
      "Iteration 8620 Training loss 0.061094678938388824 Validation loss 0.06276172399520874 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0998],\n",
      "        [0.5226]])\n",
      "Iteration 8630 Training loss 0.05831237882375717 Validation loss 0.06267443299293518 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.6132],\n",
      "        [0.0177]])\n",
      "Iteration 8640 Training loss 0.061734773218631744 Validation loss 0.06288676708936691 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9766],\n",
      "        [0.9936]])\n",
      "Iteration 8650 Training loss 0.058147627860307693 Validation loss 0.06253382563591003 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9155],\n",
      "        [0.3325]])\n",
      "Iteration 8660 Training loss 0.061094481498003006 Validation loss 0.06260102242231369 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.5514],\n",
      "        [0.7763]])\n",
      "Iteration 8670 Training loss 0.06362602859735489 Validation loss 0.06243578717112541 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9058],\n",
      "        [0.0155]])\n",
      "Iteration 8680 Training loss 0.0583113357424736 Validation loss 0.06259428709745407 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.5682],\n",
      "        [0.7899]])\n",
      "Iteration 8690 Training loss 0.0633130893111229 Validation loss 0.06275942176580429 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.4030],\n",
      "        [0.6837]])\n",
      "Iteration 8700 Training loss 0.05851788818836212 Validation loss 0.06249295920133591 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0747],\n",
      "        [0.1287]])\n",
      "Iteration 8710 Training loss 0.06526175141334534 Validation loss 0.0624220184981823 Accuracy 0.828000009059906\n",
      "Output tensor([[0.4282],\n",
      "        [0.9774]])\n",
      "Iteration 8720 Training loss 0.060764130204916 Validation loss 0.06263644248247147 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.2324],\n",
      "        [0.0118]])\n",
      "Iteration 8730 Training loss 0.05722880735993385 Validation loss 0.06248607486486435 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.3707],\n",
      "        [0.0254]])\n",
      "Iteration 8740 Training loss 0.05930311977863312 Validation loss 0.06277577579021454 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0497],\n",
      "        [0.8094]])\n",
      "Iteration 8750 Training loss 0.0633617714047432 Validation loss 0.06316664814949036 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.7038],\n",
      "        [0.0106]])\n",
      "Iteration 8760 Training loss 0.06303425878286362 Validation loss 0.06316500157117844 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.7922],\n",
      "        [0.9920]])\n",
      "Iteration 8770 Training loss 0.058558616787195206 Validation loss 0.06280947476625443 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.2462],\n",
      "        [0.7674]])\n",
      "Iteration 8780 Training loss 0.05929643288254738 Validation loss 0.06244358792901039 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.3456],\n",
      "        [0.9221]])\n",
      "Iteration 8790 Training loss 0.055991385132074356 Validation loss 0.06292881816625595 Accuracy 0.828000009059906\n",
      "Output tensor([[0.8443],\n",
      "        [0.8463]])\n",
      "Iteration 8800 Training loss 0.062221404165029526 Validation loss 0.062426790595054626 Accuracy 0.828000009059906\n",
      "Output tensor([[0.8292],\n",
      "        [0.2076]])\n",
      "Iteration 8810 Training loss 0.05828353017568588 Validation loss 0.0626092478632927 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.5865],\n",
      "        [0.3802]])\n",
      "Iteration 8820 Training loss 0.0617440864443779 Validation loss 0.06324561685323715 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.7506],\n",
      "        [0.7517]])\n",
      "Iteration 8830 Training loss 0.06045197695493698 Validation loss 0.06239383667707443 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9220],\n",
      "        [0.6730]])\n",
      "Iteration 8840 Training loss 0.05662555620074272 Validation loss 0.06279898434877396 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.1045],\n",
      "        [0.4675]])\n",
      "Iteration 8850 Training loss 0.062283530831336975 Validation loss 0.06246551498770714 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.1460],\n",
      "        [0.2092]])\n",
      "Iteration 8860 Training loss 0.06222841516137123 Validation loss 0.06256555765867233 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.1181],\n",
      "        [0.0540]])\n",
      "Iteration 8870 Training loss 0.0591994933784008 Validation loss 0.06257636845111847 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.5090],\n",
      "        [0.8771]])\n",
      "Iteration 8880 Training loss 0.05723340064287186 Validation loss 0.06290258467197418 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9338],\n",
      "        [0.3359]])\n",
      "Iteration 8890 Training loss 0.06198073551058769 Validation loss 0.062383946031332016 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8426],\n",
      "        [0.5243]])\n",
      "Iteration 8900 Training loss 0.06122787296772003 Validation loss 0.06243067979812622 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.3999],\n",
      "        [0.1392]])\n",
      "Iteration 8910 Training loss 0.06147494167089462 Validation loss 0.06250900030136108 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.0491],\n",
      "        [0.4665]])\n",
      "Iteration 8920 Training loss 0.06204787269234657 Validation loss 0.06265643984079361 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.2374],\n",
      "        [0.9568]])\n",
      "Iteration 8930 Training loss 0.06201919540762901 Validation loss 0.06260580569505692 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.5412],\n",
      "        [0.2006]])\n",
      "Iteration 8940 Training loss 0.058468930423259735 Validation loss 0.062489334493875504 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.4245],\n",
      "        [0.6189]])\n",
      "Iteration 8950 Training loss 0.060545265674591064 Validation loss 0.06254547834396362 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.6845],\n",
      "        [0.0453]])\n",
      "Iteration 8960 Training loss 0.06117077171802521 Validation loss 0.06239002197980881 Accuracy 0.828000009059906\n",
      "Output tensor([[0.3095],\n",
      "        [0.6503]])\n",
      "Iteration 8970 Training loss 0.05830018222332001 Validation loss 0.06242840364575386 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.0589],\n",
      "        [0.7772]])\n",
      "Iteration 8980 Training loss 0.06312613189220428 Validation loss 0.06261344254016876 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9299],\n",
      "        [0.7234]])\n",
      "Iteration 8990 Training loss 0.05846647918224335 Validation loss 0.06275293976068497 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.9946],\n",
      "        [0.0467]])\n",
      "Iteration 9000 Training loss 0.06027445197105408 Validation loss 0.062388598918914795 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0647],\n",
      "        [0.1659]])\n",
      "Iteration 9010 Training loss 0.06017531454563141 Validation loss 0.06249549984931946 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.9157],\n",
      "        [0.0213]])\n",
      "Iteration 9020 Training loss 0.05966189503669739 Validation loss 0.06245103478431702 Accuracy 0.824999988079071\n",
      "Output tensor([[0.5145],\n",
      "        [0.2739]])\n",
      "Iteration 9030 Training loss 0.06443080306053162 Validation loss 0.06272953748703003 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.8326],\n",
      "        [0.5703]])\n",
      "Iteration 9040 Training loss 0.06045312434434891 Validation loss 0.062337156385183334 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8259],\n",
      "        [0.6463]])\n",
      "Iteration 9050 Training loss 0.061237532645463943 Validation loss 0.06274677067995071 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.3469],\n",
      "        [0.0376]])\n",
      "Iteration 9060 Training loss 0.061672959476709366 Validation loss 0.06258785724639893 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.8045],\n",
      "        [0.3112]])\n",
      "Iteration 9070 Training loss 0.06118907034397125 Validation loss 0.06269898265600204 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.3557],\n",
      "        [0.9991]])\n",
      "Iteration 9080 Training loss 0.062138449400663376 Validation loss 0.06238780543208122 Accuracy 0.824999988079071\n",
      "Output tensor([[0.8970],\n",
      "        [0.0361]])\n",
      "Iteration 9090 Training loss 0.059030286967754364 Validation loss 0.062429070472717285 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.2697],\n",
      "        [0.7453]])\n",
      "Iteration 9100 Training loss 0.05673554167151451 Validation loss 0.06231479346752167 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.6984],\n",
      "        [0.0924]])\n",
      "Iteration 9110 Training loss 0.05736512690782547 Validation loss 0.06233111023902893 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9840],\n",
      "        [0.0328]])\n",
      "Iteration 9120 Training loss 0.05842210724949837 Validation loss 0.06233426555991173 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.6709],\n",
      "        [0.1462]])\n",
      "Iteration 9130 Training loss 0.059427399188280106 Validation loss 0.06260185688734055 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.7955],\n",
      "        [0.1159]])\n",
      "Iteration 9140 Training loss 0.06060240790247917 Validation loss 0.06264326721429825 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.6974],\n",
      "        [0.2487]])\n",
      "Iteration 9150 Training loss 0.06225713714957237 Validation loss 0.06232938915491104 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0811],\n",
      "        [0.4632]])\n",
      "Iteration 9160 Training loss 0.06226937845349312 Validation loss 0.062440887093544006 Accuracy 0.824999988079071\n",
      "Output tensor([[0.8158],\n",
      "        [0.1726]])\n",
      "Iteration 9170 Training loss 0.058410871773958206 Validation loss 0.06240670755505562 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.6270],\n",
      "        [0.9654]])\n",
      "Iteration 9180 Training loss 0.06241462007164955 Validation loss 0.06243901327252388 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.2689],\n",
      "        [0.0807]])\n",
      "Iteration 9190 Training loss 0.0602366104722023 Validation loss 0.06226755678653717 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0202],\n",
      "        [0.2179]])\n",
      "Iteration 9200 Training loss 0.058753132820129395 Validation loss 0.06278330832719803 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9378],\n",
      "        [0.6117]])\n",
      "Iteration 9210 Training loss 0.0587468296289444 Validation loss 0.0623776949942112 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.7222],\n",
      "        [0.6860]])\n",
      "Iteration 9220 Training loss 0.05955086648464203 Validation loss 0.06252546608448029 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.4441],\n",
      "        [0.9083]])\n",
      "Iteration 9230 Training loss 0.05930221825838089 Validation loss 0.06228887289762497 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0891],\n",
      "        [0.2656]])\n",
      "Iteration 9240 Training loss 0.05950653925538063 Validation loss 0.06258869916200638 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.9950],\n",
      "        [0.7599]])\n",
      "Iteration 9250 Training loss 0.06128813326358795 Validation loss 0.06235846132040024 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.1528],\n",
      "        [0.7049]])\n",
      "Iteration 9260 Training loss 0.0605413056910038 Validation loss 0.06268591433763504 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.7391],\n",
      "        [0.7861]])\n",
      "Iteration 9270 Training loss 0.06006964296102524 Validation loss 0.062333717942237854 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9111],\n",
      "        [0.3808]])\n",
      "Iteration 9280 Training loss 0.06146463006734848 Validation loss 0.062330689281225204 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9940],\n",
      "        [0.0294]])\n",
      "Iteration 9290 Training loss 0.056607622653245926 Validation loss 0.0623503252863884 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0598],\n",
      "        [0.3889]])\n",
      "Iteration 9300 Training loss 0.060688551515340805 Validation loss 0.06252117455005646 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.7177],\n",
      "        [0.8816]])\n",
      "Iteration 9310 Training loss 0.06267070770263672 Validation loss 0.062298018485307693 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.1748],\n",
      "        [0.0118]])\n",
      "Iteration 9320 Training loss 0.06207166239619255 Validation loss 0.06250843405723572 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.2095],\n",
      "        [0.5042]])\n",
      "Iteration 9330 Training loss 0.05985076725482941 Validation loss 0.062269166111946106 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8182],\n",
      "        [0.3890]])\n",
      "Iteration 9340 Training loss 0.05958755314350128 Validation loss 0.06269218027591705 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.2724],\n",
      "        [0.0955]])\n",
      "Iteration 9350 Training loss 0.06195532903075218 Validation loss 0.06234338879585266 Accuracy 0.824999988079071\n",
      "Output tensor([[0.9869],\n",
      "        [0.0967]])\n",
      "Iteration 9360 Training loss 0.062044452875852585 Validation loss 0.062291402369737625 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9411],\n",
      "        [0.4798]])\n",
      "Iteration 9370 Training loss 0.06358442455530167 Validation loss 0.06231154501438141 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.9393],\n",
      "        [0.1366]])\n",
      "Iteration 9380 Training loss 0.05951552093029022 Validation loss 0.062444742769002914 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9442],\n",
      "        [0.0235]])\n",
      "Iteration 9390 Training loss 0.06141430512070656 Validation loss 0.06248079240322113 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9938],\n",
      "        [0.1210]])\n",
      "Iteration 9400 Training loss 0.05990045145153999 Validation loss 0.062401991337537766 Accuracy 0.824999988079071\n",
      "Output tensor([[0.6676],\n",
      "        [0.3609]])\n",
      "Iteration 9410 Training loss 0.060190536081790924 Validation loss 0.06258542835712433 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.1623],\n",
      "        [0.6901]])\n",
      "Iteration 9420 Training loss 0.06098681688308716 Validation loss 0.062318429350852966 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9243],\n",
      "        [0.9434]])\n",
      "Iteration 9430 Training loss 0.054586198180913925 Validation loss 0.06222940608859062 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9677],\n",
      "        [0.1933]])\n",
      "Iteration 9440 Training loss 0.05913877859711647 Validation loss 0.0625612884759903 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.8533],\n",
      "        [0.0152]])\n",
      "Iteration 9450 Training loss 0.0625084862112999 Validation loss 0.06290371716022491 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9516],\n",
      "        [0.9227]])\n",
      "Iteration 9460 Training loss 0.06517685204744339 Validation loss 0.06274417787790298 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9545],\n",
      "        [0.9596]])\n",
      "Iteration 9470 Training loss 0.06020199507474899 Validation loss 0.062212541699409485 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.6342],\n",
      "        [0.5490]])\n",
      "Iteration 9480 Training loss 0.061444517225027084 Validation loss 0.06246856600046158 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9382],\n",
      "        [0.3560]])\n",
      "Iteration 9490 Training loss 0.060135044157505035 Validation loss 0.06250596791505814 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.8466],\n",
      "        [0.6672]])\n",
      "Iteration 9500 Training loss 0.05806650221347809 Validation loss 0.062431931495666504 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.5441],\n",
      "        [0.6747]])\n",
      "Iteration 9510 Training loss 0.060807790607213974 Validation loss 0.06224729120731354 Accuracy 0.828000009059906\n",
      "Output tensor([[0.3690],\n",
      "        [0.1279]])\n",
      "Iteration 9520 Training loss 0.059236470609903336 Validation loss 0.06243744492530823 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9852],\n",
      "        [0.3870]])\n",
      "Iteration 9530 Training loss 0.059965379536151886 Validation loss 0.06238408759236336 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0358],\n",
      "        [0.7819]])\n",
      "Iteration 9540 Training loss 0.05778191611170769 Validation loss 0.062225211411714554 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0219],\n",
      "        [0.8928]])\n",
      "Iteration 9550 Training loss 0.05849098041653633 Validation loss 0.0622456893324852 Accuracy 0.828499972820282\n",
      "Output tensor([[0.1909],\n",
      "        [0.0767]])\n",
      "Iteration 9560 Training loss 0.05970210209488869 Validation loss 0.062182340770959854 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4721],\n",
      "        [0.2802]])\n",
      "Iteration 9570 Training loss 0.0563090518116951 Validation loss 0.062292929738759995 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2074],\n",
      "        [0.0698]])\n",
      "Iteration 9580 Training loss 0.05773353949189186 Validation loss 0.06229810044169426 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2054],\n",
      "        [0.8806]])\n",
      "Iteration 9590 Training loss 0.05787722021341324 Validation loss 0.06228923052549362 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.5487],\n",
      "        [0.3128]])\n",
      "Iteration 9600 Training loss 0.06101737543940544 Validation loss 0.06220221891999245 Accuracy 0.828000009059906\n",
      "Output tensor([[0.7798],\n",
      "        [0.3374]])\n",
      "Iteration 9610 Training loss 0.05717414990067482 Validation loss 0.06218750774860382 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.6088],\n",
      "        [0.2747]])\n",
      "Iteration 9620 Training loss 0.058278683573007584 Validation loss 0.062351737171411514 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9869],\n",
      "        [0.9120]])\n",
      "Iteration 9630 Training loss 0.05961933359503746 Validation loss 0.06229982152581215 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8976],\n",
      "        [0.3306]])\n",
      "Iteration 9640 Training loss 0.06033509224653244 Validation loss 0.06242963671684265 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.2748],\n",
      "        [0.4759]])\n",
      "Iteration 9650 Training loss 0.05993317440152168 Validation loss 0.06245239078998566 Accuracy 0.824999988079071\n",
      "Output tensor([[0.3976],\n",
      "        [0.6709]])\n",
      "Iteration 9660 Training loss 0.06026134639978409 Validation loss 0.062190841883420944 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0071],\n",
      "        [0.9609]])\n",
      "Iteration 9670 Training loss 0.059748295694589615 Validation loss 0.06230562552809715 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.3432],\n",
      "        [0.0741]])\n",
      "Iteration 9680 Training loss 0.061306096613407135 Validation loss 0.06253530085086823 Accuracy 0.824999988079071\n",
      "Output tensor([[0.1108],\n",
      "        [0.1595]])\n",
      "Iteration 9690 Training loss 0.05844511464238167 Validation loss 0.06216845288872719 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9092],\n",
      "        [0.4968]])\n",
      "Iteration 9700 Training loss 0.05979081243276596 Validation loss 0.062465354800224304 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.7094],\n",
      "        [0.0426]])\n",
      "Iteration 9710 Training loss 0.061161160469055176 Validation loss 0.0625576451420784 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.4777],\n",
      "        [0.7307]])\n",
      "Iteration 9720 Training loss 0.06175975129008293 Validation loss 0.06237725540995598 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.8516],\n",
      "        [0.4005]])\n",
      "Iteration 9730 Training loss 0.06155726686120033 Validation loss 0.06233447790145874 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.2538],\n",
      "        [0.4074]])\n",
      "Iteration 9740 Training loss 0.06150111183524132 Validation loss 0.06270207464694977 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8665],\n",
      "        [0.0805]])\n",
      "Iteration 9750 Training loss 0.056628551334142685 Validation loss 0.06249995902180672 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.8688],\n",
      "        [0.8555]])\n",
      "Iteration 9760 Training loss 0.05960028991103172 Validation loss 0.06240151450037956 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0106],\n",
      "        [0.7797]])\n",
      "Iteration 9770 Training loss 0.061896465718746185 Validation loss 0.06222521886229515 Accuracy 0.828499972820282\n",
      "Output tensor([[0.4067],\n",
      "        [0.2185]])\n",
      "Iteration 9780 Training loss 0.06061137095093727 Validation loss 0.06250178068876266 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.7654],\n",
      "        [0.5529]])\n",
      "Iteration 9790 Training loss 0.06107986345887184 Validation loss 0.06214607506990433 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.2485],\n",
      "        [0.9397]])\n",
      "Iteration 9800 Training loss 0.056453902274370193 Validation loss 0.06226126104593277 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.1516],\n",
      "        [0.9035]])\n",
      "Iteration 9810 Training loss 0.06065309792757034 Validation loss 0.062473975121974945 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8081],\n",
      "        [0.8652]])\n",
      "Iteration 9820 Training loss 0.05661052465438843 Validation loss 0.06262612342834473 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2629],\n",
      "        [0.5406]])\n",
      "Iteration 9830 Training loss 0.060460422188043594 Validation loss 0.0624251589179039 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0056],\n",
      "        [0.0408]])\n",
      "Iteration 9840 Training loss 0.05902436748147011 Validation loss 0.062430500984191895 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9913],\n",
      "        [0.0789]])\n",
      "Iteration 9850 Training loss 0.05722201615571976 Validation loss 0.06231735274195671 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0329],\n",
      "        [0.8663]])\n",
      "Iteration 9860 Training loss 0.058978866785764694 Validation loss 0.06212484836578369 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0554],\n",
      "        [0.6253]])\n",
      "Iteration 9870 Training loss 0.05782149359583855 Validation loss 0.06300541013479233 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.1275],\n",
      "        [0.1842]])\n",
      "Iteration 9880 Training loss 0.05964997038245201 Validation loss 0.0623849481344223 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.5093],\n",
      "        [0.6954]])\n",
      "Iteration 9890 Training loss 0.06105869263410568 Validation loss 0.06244082748889923 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.2564],\n",
      "        [0.2240]])\n",
      "Iteration 9900 Training loss 0.06219486519694328 Validation loss 0.062153615057468414 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9742],\n",
      "        [0.5102]])\n",
      "Iteration 9910 Training loss 0.059420790523290634 Validation loss 0.06212936341762543 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1858],\n",
      "        [0.4284]])\n",
      "Iteration 9920 Training loss 0.05826850235462189 Validation loss 0.06242689490318298 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0598],\n",
      "        [0.6409]])\n",
      "Iteration 9930 Training loss 0.061800483614206314 Validation loss 0.06243353709578514 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.7075],\n",
      "        [0.0694]])\n",
      "Iteration 9940 Training loss 0.060783501714468 Validation loss 0.062263693660497665 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.7996],\n",
      "        [0.0855]])\n",
      "Iteration 9950 Training loss 0.05860036239027977 Validation loss 0.06241624429821968 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.1569],\n",
      "        [0.0366]])\n",
      "Iteration 9960 Training loss 0.0646171122789383 Validation loss 0.062438275665044785 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.5008],\n",
      "        [0.0817]])\n",
      "Iteration 9970 Training loss 0.06062045320868492 Validation loss 0.062324538826942444 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0494],\n",
      "        [0.5037]])\n",
      "Iteration 9980 Training loss 0.057972535490989685 Validation loss 0.06225808709859848 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0857],\n",
      "        [0.9343]])\n",
      "Iteration 9990 Training loss 0.05898137763142586 Validation loss 0.06259031593799591 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9205],\n",
      "        [0.9790]])\n",
      "Iteration 10000 Training loss 0.058128636330366135 Validation loss 0.06223182752728462 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0579],\n",
      "        [0.3756]])\n",
      "Iteration 10010 Training loss 0.06141195818781853 Validation loss 0.062367044389247894 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0350],\n",
      "        [0.6345]])\n",
      "Iteration 10020 Training loss 0.0586712583899498 Validation loss 0.06243661046028137 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.3499],\n",
      "        [0.8171]])\n",
      "Iteration 10030 Training loss 0.061722978949546814 Validation loss 0.06216256693005562 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0655],\n",
      "        [0.0524]])\n",
      "Iteration 10040 Training loss 0.05821516364812851 Validation loss 0.0625222772359848 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.1179],\n",
      "        [0.7219]])\n",
      "Iteration 10050 Training loss 0.058554667979478836 Validation loss 0.06211024150252342 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9497],\n",
      "        [0.9225]])\n",
      "Iteration 10060 Training loss 0.06138540431857109 Validation loss 0.06210571900010109 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.2148],\n",
      "        [0.2038]])\n",
      "Iteration 10070 Training loss 0.06171528622508049 Validation loss 0.0621165968477726 Accuracy 0.828499972820282\n",
      "Output tensor([[0.6413],\n",
      "        [0.9376]])\n",
      "Iteration 10080 Training loss 0.056801341474056244 Validation loss 0.062236588448286057 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0199],\n",
      "        [0.0245]])\n",
      "Iteration 10090 Training loss 0.060136578977108 Validation loss 0.062231771647930145 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.2682],\n",
      "        [0.0509]])\n",
      "Iteration 10100 Training loss 0.06023378670215607 Validation loss 0.06221971660852432 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.8893],\n",
      "        [0.5800]])\n",
      "Iteration 10110 Training loss 0.0615905337035656 Validation loss 0.06251927465200424 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0839],\n",
      "        [0.9792]])\n",
      "Iteration 10120 Training loss 0.058970749378204346 Validation loss 0.06231071427464485 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.1390],\n",
      "        [0.1548]])\n",
      "Iteration 10130 Training loss 0.0555458627641201 Validation loss 0.06212974339723587 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.5005],\n",
      "        [0.7847]])\n",
      "Iteration 10140 Training loss 0.059762485325336456 Validation loss 0.06236907094717026 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0604],\n",
      "        [0.0341]])\n",
      "Iteration 10150 Training loss 0.05845842510461807 Validation loss 0.06220540031790733 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0969],\n",
      "        [0.8263]])\n",
      "Iteration 10160 Training loss 0.06276558339595795 Validation loss 0.062398675829172134 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0847],\n",
      "        [0.9883]])\n",
      "Iteration 10170 Training loss 0.05940623581409454 Validation loss 0.0623355433344841 Accuracy 0.824999988079071\n",
      "Output tensor([[0.7546],\n",
      "        [0.1065]])\n",
      "Iteration 10180 Training loss 0.06026626378297806 Validation loss 0.062254052609205246 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.1288],\n",
      "        [0.9696]])\n",
      "Iteration 10190 Training loss 0.05893685668706894 Validation loss 0.062202345579862595 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9947],\n",
      "        [0.5147]])\n",
      "Iteration 10200 Training loss 0.05592372640967369 Validation loss 0.06249537691473961 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0554],\n",
      "        [0.3263]])\n",
      "Iteration 10210 Training loss 0.05678113177418709 Validation loss 0.06206519156694412 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0241],\n",
      "        [0.9471]])\n",
      "Iteration 10220 Training loss 0.058121029287576675 Validation loss 0.06234539672732353 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0033],\n",
      "        [0.8536]])\n",
      "Iteration 10230 Training loss 0.061648864299058914 Validation loss 0.06227909028530121 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9741],\n",
      "        [0.1738]])\n",
      "Iteration 10240 Training loss 0.06507451832294464 Validation loss 0.062141790986061096 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9389],\n",
      "        [0.0447]])\n",
      "Iteration 10250 Training loss 0.0567731149494648 Validation loss 0.06256837397813797 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9132],\n",
      "        [0.2488]])\n",
      "Iteration 10260 Training loss 0.05984487384557724 Validation loss 0.0625084936618805 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.7242],\n",
      "        [0.7741]])\n",
      "Iteration 10270 Training loss 0.05724237859249115 Validation loss 0.06233981251716614 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.9861],\n",
      "        [0.0352]])\n",
      "Iteration 10280 Training loss 0.06034925952553749 Validation loss 0.06205571070313454 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0562],\n",
      "        [0.6226]])\n",
      "Iteration 10290 Training loss 0.05891450121998787 Validation loss 0.06259731203317642 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8872],\n",
      "        [0.9608]])\n",
      "Iteration 10300 Training loss 0.0584150068461895 Validation loss 0.062407564371824265 Accuracy 0.824999988079071\n",
      "Output tensor([[0.9930],\n",
      "        [0.8735]])\n",
      "Iteration 10310 Training loss 0.05708031728863716 Validation loss 0.062363602221012115 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.5068],\n",
      "        [0.4413]])\n",
      "Iteration 10320 Training loss 0.060359857976436615 Validation loss 0.06211670860648155 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.1941],\n",
      "        [0.9423]])\n",
      "Iteration 10330 Training loss 0.05897345021367073 Validation loss 0.06243523582816124 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9172],\n",
      "        [0.0345]])\n",
      "Iteration 10340 Training loss 0.060479018837213516 Validation loss 0.06247933581471443 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.2050],\n",
      "        [0.8140]])\n",
      "Iteration 10350 Training loss 0.05952397733926773 Validation loss 0.06255491822957993 Accuracy 0.824999988079071\n",
      "Output tensor([[0.0160],\n",
      "        [0.8187]])\n",
      "Iteration 10360 Training loss 0.06169910356402397 Validation loss 0.06257336586713791 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.2687],\n",
      "        [0.4756]])\n",
      "Iteration 10370 Training loss 0.05784391611814499 Validation loss 0.06219260022044182 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.2958],\n",
      "        [0.0185]])\n",
      "Iteration 10380 Training loss 0.06084228307008743 Validation loss 0.06217600777745247 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0930],\n",
      "        [0.3642]])\n",
      "Iteration 10390 Training loss 0.05863359197974205 Validation loss 0.06208501383662224 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.5938],\n",
      "        [0.1214]])\n",
      "Iteration 10400 Training loss 0.062240395694971085 Validation loss 0.061996206641197205 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9151],\n",
      "        [0.7666]])\n",
      "Iteration 10410 Training loss 0.06000315770506859 Validation loss 0.062308743596076965 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8753],\n",
      "        [0.9274]])\n",
      "Iteration 10420 Training loss 0.05720129236578941 Validation loss 0.06212644279003143 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.9784],\n",
      "        [0.8267]])\n",
      "Iteration 10430 Training loss 0.06396061927080154 Validation loss 0.06206068769097328 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0659],\n",
      "        [0.1427]])\n",
      "Iteration 10440 Training loss 0.055330343544483185 Validation loss 0.062117405235767365 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9277],\n",
      "        [0.9289]])\n",
      "Iteration 10450 Training loss 0.061213959008455276 Validation loss 0.06251586973667145 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0381],\n",
      "        [0.2517]])\n",
      "Iteration 10460 Training loss 0.06034359335899353 Validation loss 0.062201324850320816 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0387],\n",
      "        [0.0128]])\n",
      "Iteration 10470 Training loss 0.05717675760388374 Validation loss 0.06209675222635269 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.8707],\n",
      "        [0.0826]])\n",
      "Iteration 10480 Training loss 0.058386124670505524 Validation loss 0.06202897056937218 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.5953],\n",
      "        [0.9496]])\n",
      "Iteration 10490 Training loss 0.06112973764538765 Validation loss 0.06238997355103493 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.8971],\n",
      "        [0.9921]])\n",
      "Iteration 10500 Training loss 0.05792466178536415 Validation loss 0.062384359538555145 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0279],\n",
      "        [0.8071]])\n",
      "Iteration 10510 Training loss 0.058330196887254715 Validation loss 0.062113940715789795 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.3459],\n",
      "        [0.9199]])\n",
      "Iteration 10520 Training loss 0.06269372999668121 Validation loss 0.061959970742464066 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5952],\n",
      "        [0.9950]])\n",
      "Iteration 10530 Training loss 0.059348464012145996 Validation loss 0.06210200488567352 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9754],\n",
      "        [0.6201]])\n",
      "Iteration 10540 Training loss 0.06124155595898628 Validation loss 0.062008172273635864 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.2376],\n",
      "        [0.0439]])\n",
      "Iteration 10550 Training loss 0.06132713705301285 Validation loss 0.06199114769697189 Accuracy 0.828499972820282\n",
      "Output tensor([[0.2368],\n",
      "        [0.1075]])\n",
      "Iteration 10560 Training loss 0.058566462248563766 Validation loss 0.06222684308886528 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.7178],\n",
      "        [0.3109]])\n",
      "Iteration 10570 Training loss 0.0630769357085228 Validation loss 0.06199730932712555 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.3889],\n",
      "        [0.2593]])\n",
      "Iteration 10580 Training loss 0.057815637439489365 Validation loss 0.06201775372028351 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9835],\n",
      "        [0.7696]])\n",
      "Iteration 10590 Training loss 0.05663580074906349 Validation loss 0.06243937835097313 Accuracy 0.824999988079071\n",
      "Output tensor([[0.7737],\n",
      "        [0.8080]])\n",
      "Iteration 10600 Training loss 0.05861451104283333 Validation loss 0.06264042109251022 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.1455],\n",
      "        [0.5881]])\n",
      "Iteration 10610 Training loss 0.05895065888762474 Validation loss 0.06230465695261955 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9225],\n",
      "        [0.9786]])\n",
      "Iteration 10620 Training loss 0.05952794477343559 Validation loss 0.06200632452964783 Accuracy 0.828000009059906\n",
      "Output tensor([[0.6534],\n",
      "        [0.3143]])\n",
      "Iteration 10630 Training loss 0.06291615962982178 Validation loss 0.06203513219952583 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0550],\n",
      "        [0.3203]])\n",
      "Iteration 10640 Training loss 0.0594211108982563 Validation loss 0.0621233731508255 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0338],\n",
      "        [0.8584]])\n",
      "Iteration 10650 Training loss 0.060899727046489716 Validation loss 0.06271631270647049 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.8020],\n",
      "        [0.7067]])\n",
      "Iteration 10660 Training loss 0.05720111355185509 Validation loss 0.062321748584508896 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8303],\n",
      "        [0.3948]])\n",
      "Iteration 10670 Training loss 0.058870431035757065 Validation loss 0.0621778778731823 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.2326],\n",
      "        [0.9010]])\n",
      "Iteration 10680 Training loss 0.057331353425979614 Validation loss 0.06214476376771927 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9194],\n",
      "        [0.4242]])\n",
      "Iteration 10690 Training loss 0.057994063943624496 Validation loss 0.0622606985270977 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.7128],\n",
      "        [0.1925]])\n",
      "Iteration 10700 Training loss 0.05793387442827225 Validation loss 0.062120094895362854 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.1796],\n",
      "        [0.0663]])\n",
      "Iteration 10710 Training loss 0.05967704951763153 Validation loss 0.062069762498140335 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.7204],\n",
      "        [0.9642]])\n",
      "Iteration 10720 Training loss 0.059926003217697144 Validation loss 0.06230509653687477 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.5884],\n",
      "        [0.9868]])\n",
      "Iteration 10730 Training loss 0.058223869651556015 Validation loss 0.06264307349920273 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.4696],\n",
      "        [0.6241]])\n",
      "Iteration 10740 Training loss 0.05996078625321388 Validation loss 0.062086161226034164 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9907],\n",
      "        [0.1033]])\n",
      "Iteration 10750 Training loss 0.05905435234308243 Validation loss 0.06225602328777313 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1566],\n",
      "        [0.8195]])\n",
      "Iteration 10760 Training loss 0.05624464526772499 Validation loss 0.06193484365940094 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8646],\n",
      "        [0.5823]])\n",
      "Iteration 10770 Training loss 0.055206041783094406 Validation loss 0.06189865246415138 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8328],\n",
      "        [0.4225]])\n",
      "Iteration 10780 Training loss 0.05942454934120178 Validation loss 0.061940159648656845 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9779],\n",
      "        [0.0200]])\n",
      "Iteration 10790 Training loss 0.059173040091991425 Validation loss 0.06197334825992584 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.3065],\n",
      "        [0.6219]])\n",
      "Iteration 10800 Training loss 0.05824356526136398 Validation loss 0.06192919239401817 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.5636],\n",
      "        [0.5027]])\n",
      "Iteration 10810 Training loss 0.06042883172631264 Validation loss 0.06191186234354973 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9967],\n",
      "        [0.4450]])\n",
      "Iteration 10820 Training loss 0.05827111750841141 Validation loss 0.061995841562747955 Accuracy 0.828499972820282\n",
      "Output tensor([[0.5627],\n",
      "        [0.9878]])\n",
      "Iteration 10830 Training loss 0.058850016444921494 Validation loss 0.062059156596660614 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.7385],\n",
      "        [0.1691]])\n",
      "Iteration 10840 Training loss 0.061445750296115875 Validation loss 0.062215324491262436 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.1658],\n",
      "        [0.6888]])\n",
      "Iteration 10850 Training loss 0.05728330835700035 Validation loss 0.062076386064291 Accuracy 0.828000009059906\n",
      "Output tensor([[0.5993],\n",
      "        [0.8977]])\n",
      "Iteration 10860 Training loss 0.05889580771327019 Validation loss 0.062191665172576904 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.6199],\n",
      "        [0.2509]])\n",
      "Iteration 10870 Training loss 0.057508934289216995 Validation loss 0.06269294023513794 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.9158],\n",
      "        [0.4418]])\n",
      "Iteration 10880 Training loss 0.05814100056886673 Validation loss 0.061924271285533905 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0078],\n",
      "        [0.2128]])\n",
      "Iteration 10890 Training loss 0.05783041566610336 Validation loss 0.0621965117752552 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9931],\n",
      "        [0.1096]])\n",
      "Iteration 10900 Training loss 0.06015168875455856 Validation loss 0.06209089979529381 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0126],\n",
      "        [0.1768]])\n",
      "Iteration 10910 Training loss 0.05932698771357536 Validation loss 0.06212117150425911 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0183],\n",
      "        [0.5503]])\n",
      "Iteration 10920 Training loss 0.05898464471101761 Validation loss 0.061916835606098175 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8575],\n",
      "        [0.4613]])\n",
      "Iteration 10930 Training loss 0.055194079875946045 Validation loss 0.06187133118510246 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.3474],\n",
      "        [0.0127]])\n",
      "Iteration 10940 Training loss 0.058164194226264954 Validation loss 0.06219777464866638 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.2542],\n",
      "        [0.8124]])\n",
      "Iteration 10950 Training loss 0.0569133460521698 Validation loss 0.0622059665620327 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.7544],\n",
      "        [0.4719]])\n",
      "Iteration 10960 Training loss 0.05904669314622879 Validation loss 0.06227029860019684 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0150],\n",
      "        [0.4684]])\n",
      "Iteration 10970 Training loss 0.0597250834107399 Validation loss 0.06197868287563324 Accuracy 0.828000009059906\n",
      "Output tensor([[0.8183],\n",
      "        [0.0615]])\n",
      "Iteration 10980 Training loss 0.060700513422489166 Validation loss 0.06230730563402176 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0185],\n",
      "        [0.9937]])\n",
      "Iteration 10990 Training loss 0.06242341548204422 Validation loss 0.06222165375947952 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9867],\n",
      "        [0.9634]])\n",
      "Iteration 11000 Training loss 0.05966659262776375 Validation loss 0.06183142215013504 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6053],\n",
      "        [0.1837]])\n",
      "Iteration 11010 Training loss 0.05853162333369255 Validation loss 0.06192042678594589 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2922],\n",
      "        [0.0423]])\n",
      "Iteration 11020 Training loss 0.05585901439189911 Validation loss 0.06185755133628845 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0554],\n",
      "        [0.3852]])\n",
      "Iteration 11030 Training loss 0.06136636435985565 Validation loss 0.06210792437195778 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.6234],\n",
      "        [0.1659]])\n",
      "Iteration 11040 Training loss 0.05938160419464111 Validation loss 0.06231686472892761 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.8540],\n",
      "        [0.4577]])\n",
      "Iteration 11050 Training loss 0.05991724878549576 Validation loss 0.061860255897045135 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9424],\n",
      "        [0.6610]])\n",
      "Iteration 11060 Training loss 0.05856270715594292 Validation loss 0.06201174110174179 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8830],\n",
      "        [0.0293]])\n",
      "Iteration 11070 Training loss 0.05994400382041931 Validation loss 0.061954300850629807 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.3721],\n",
      "        [0.9516]])\n",
      "Iteration 11080 Training loss 0.056046079844236374 Validation loss 0.06195338815450668 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.1575],\n",
      "        [0.3970]])\n",
      "Iteration 11090 Training loss 0.059117697179317474 Validation loss 0.062189992517232895 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.1318],\n",
      "        [0.9889]])\n",
      "Iteration 11100 Training loss 0.05749545618891716 Validation loss 0.062056198716163635 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0641],\n",
      "        [0.5156]])\n",
      "Iteration 11110 Training loss 0.06130030006170273 Validation loss 0.0619547963142395 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.5931],\n",
      "        [0.7427]])\n",
      "Iteration 11120 Training loss 0.06215902045369148 Validation loss 0.06189020723104477 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2219],\n",
      "        [0.9688]])\n",
      "Iteration 11130 Training loss 0.05622928962111473 Validation loss 0.06197867542505264 Accuracy 0.828000009059906\n",
      "Output tensor([[0.7908],\n",
      "        [0.4197]])\n",
      "Iteration 11140 Training loss 0.06359952688217163 Validation loss 0.06242794916033745 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.9330],\n",
      "        [0.0210]])\n",
      "Iteration 11150 Training loss 0.06060578301548958 Validation loss 0.062062498182058334 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.6369],\n",
      "        [0.0483]])\n",
      "Iteration 11160 Training loss 0.05633877217769623 Validation loss 0.06186331808567047 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0157],\n",
      "        [0.8142]])\n",
      "Iteration 11170 Training loss 0.05935024097561836 Validation loss 0.06195153668522835 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8134],\n",
      "        [0.8920]])\n",
      "Iteration 11180 Training loss 0.058481719344854355 Validation loss 0.062025655061006546 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.3501],\n",
      "        [0.5236]])\n",
      "Iteration 11190 Training loss 0.06007673218846321 Validation loss 0.06190839782357216 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1443],\n",
      "        [0.6561]])\n",
      "Iteration 11200 Training loss 0.06017984077334404 Validation loss 0.061839278787374496 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6607],\n",
      "        [0.9513]])\n",
      "Iteration 11210 Training loss 0.060269128531217575 Validation loss 0.06183873862028122 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1447],\n",
      "        [0.7979]])\n",
      "Iteration 11220 Training loss 0.056130409240722656 Validation loss 0.06218225881457329 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.1275],\n",
      "        [0.1323]])\n",
      "Iteration 11230 Training loss 0.05902932584285736 Validation loss 0.06189395859837532 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8932],\n",
      "        [0.8589]])\n",
      "Iteration 11240 Training loss 0.059808213263750076 Validation loss 0.062004853039979935 Accuracy 0.828000009059906\n",
      "Output tensor([[0.3366],\n",
      "        [0.6694]])\n",
      "Iteration 11250 Training loss 0.05777094513177872 Validation loss 0.061908066272735596 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1068],\n",
      "        [0.8326]])\n",
      "Iteration 11260 Training loss 0.05700723081827164 Validation loss 0.06233173981308937 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0689],\n",
      "        [0.9735]])\n",
      "Iteration 11270 Training loss 0.05895676836371422 Validation loss 0.06199070066213608 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.2442],\n",
      "        [0.8428]])\n",
      "Iteration 11280 Training loss 0.05834388732910156 Validation loss 0.06210269033908844 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0282],\n",
      "        [0.9594]])\n",
      "Iteration 11290 Training loss 0.06271195411682129 Validation loss 0.06180957704782486 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0746],\n",
      "        [0.6126]])\n",
      "Iteration 11300 Training loss 0.05810440704226494 Validation loss 0.0619075633585453 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0855],\n",
      "        [0.1272]])\n",
      "Iteration 11310 Training loss 0.06000271067023277 Validation loss 0.06184417009353638 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9991],\n",
      "        [0.9792]])\n",
      "Iteration 11320 Training loss 0.05738825350999832 Validation loss 0.06200343742966652 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9294],\n",
      "        [0.0236]])\n",
      "Iteration 11330 Training loss 0.05919412896037102 Validation loss 0.062009356915950775 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9030],\n",
      "        [0.0244]])\n",
      "Iteration 11340 Training loss 0.05813029035925865 Validation loss 0.06242390722036362 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.0321],\n",
      "        [0.9174]])\n",
      "Iteration 11350 Training loss 0.059383824467659 Validation loss 0.06187529116868973 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0741],\n",
      "        [0.9657]])\n",
      "Iteration 11360 Training loss 0.05824561044573784 Validation loss 0.06196034699678421 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.3846],\n",
      "        [0.9275]])\n",
      "Iteration 11370 Training loss 0.05872022360563278 Validation loss 0.06198284775018692 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0086],\n",
      "        [0.2082]])\n",
      "Iteration 11380 Training loss 0.05972453951835632 Validation loss 0.06221634894609451 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.7506],\n",
      "        [0.0084]])\n",
      "Iteration 11390 Training loss 0.053435105830430984 Validation loss 0.062177903950214386 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9179],\n",
      "        [0.2880]])\n",
      "Iteration 11400 Training loss 0.05908622220158577 Validation loss 0.061955343931913376 Accuracy 0.828499972820282\n",
      "Output tensor([[0.5997],\n",
      "        [0.0638]])\n",
      "Iteration 11410 Training loss 0.057940054684877396 Validation loss 0.06197476014494896 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4435],\n",
      "        [0.9509]])\n",
      "Iteration 11420 Training loss 0.06050584837794304 Validation loss 0.06248956918716431 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.3500],\n",
      "        [0.0311]])\n",
      "Iteration 11430 Training loss 0.055447425693273544 Validation loss 0.06191946566104889 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0299],\n",
      "        [0.1109]])\n",
      "Iteration 11440 Training loss 0.06168242171406746 Validation loss 0.06193472072482109 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0891],\n",
      "        [0.5928]])\n",
      "Iteration 11450 Training loss 0.05946742370724678 Validation loss 0.062011826783418655 Accuracy 0.828000009059906\n",
      "Output tensor([[0.5312],\n",
      "        [0.9792]])\n",
      "Iteration 11460 Training loss 0.05923505872488022 Validation loss 0.06191319227218628 Accuracy 0.828000009059906\n",
      "Output tensor([[0.6751],\n",
      "        [0.1147]])\n",
      "Iteration 11470 Training loss 0.0591869130730629 Validation loss 0.06221688538789749 Accuracy 0.824999988079071\n",
      "Output tensor([[0.7273],\n",
      "        [0.0687]])\n",
      "Iteration 11480 Training loss 0.059084732085466385 Validation loss 0.06181803345680237 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9916],\n",
      "        [0.9796]])\n",
      "Iteration 11490 Training loss 0.05754055082798004 Validation loss 0.061772748827934265 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8677],\n",
      "        [0.8644]])\n",
      "Iteration 11500 Training loss 0.05863730609416962 Validation loss 0.06258762627840042 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9086],\n",
      "        [0.7992]])\n",
      "Iteration 11510 Training loss 0.059473805129528046 Validation loss 0.06213993579149246 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9284],\n",
      "        [0.6898]])\n",
      "Iteration 11520 Training loss 0.055835459381341934 Validation loss 0.062145065516233444 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.7051],\n",
      "        [0.5371]])\n",
      "Iteration 11530 Training loss 0.06027558445930481 Validation loss 0.06200868636369705 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.5438],\n",
      "        [0.8714]])\n",
      "Iteration 11540 Training loss 0.05946580320596695 Validation loss 0.06193547695875168 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.3919],\n",
      "        [0.6483]])\n",
      "Iteration 11550 Training loss 0.059145621955394745 Validation loss 0.061945658177137375 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.5589],\n",
      "        [0.0896]])\n",
      "Iteration 11560 Training loss 0.058442775160074234 Validation loss 0.061746515333652496 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2350],\n",
      "        [0.9946]])\n",
      "Iteration 11570 Training loss 0.05638913810253143 Validation loss 0.062338173389434814 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.6843],\n",
      "        [0.9524]])\n",
      "Iteration 11580 Training loss 0.05964791402220726 Validation loss 0.061895702034235 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8165],\n",
      "        [0.8425]])\n",
      "Iteration 11590 Training loss 0.05795060470700264 Validation loss 0.06223224475979805 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.6179],\n",
      "        [0.8193]])\n",
      "Iteration 11600 Training loss 0.06087975576519966 Validation loss 0.06187067925930023 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9906],\n",
      "        [0.6666]])\n",
      "Iteration 11610 Training loss 0.05949423834681511 Validation loss 0.06194571033120155 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9478],\n",
      "        [0.8496]])\n",
      "Iteration 11620 Training loss 0.057995423674583435 Validation loss 0.061860229820013046 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0164],\n",
      "        [0.9815]])\n",
      "Iteration 11630 Training loss 0.05960551276803017 Validation loss 0.062032099813222885 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.3220],\n",
      "        [0.9996]])\n",
      "Iteration 11640 Training loss 0.05980975180864334 Validation loss 0.06177873536944389 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0075],\n",
      "        [0.7908]])\n",
      "Iteration 11650 Training loss 0.058126263320446014 Validation loss 0.06180674955248833 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0773],\n",
      "        [0.3344]])\n",
      "Iteration 11660 Training loss 0.058477140963077545 Validation loss 0.06175876036286354 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2029],\n",
      "        [0.2398]])\n",
      "Iteration 11670 Training loss 0.05716031789779663 Validation loss 0.06178523972630501 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0104],\n",
      "        [0.5116]])\n",
      "Iteration 11680 Training loss 0.060844648629426956 Validation loss 0.061993490904569626 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9970],\n",
      "        [0.1560]])\n",
      "Iteration 11690 Training loss 0.058322735130786896 Validation loss 0.06171613559126854 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4088],\n",
      "        [0.0431]])\n",
      "Iteration 11700 Training loss 0.059009719640016556 Validation loss 0.061970602720975876 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9056],\n",
      "        [0.9787]])\n",
      "Iteration 11710 Training loss 0.06134407967329025 Validation loss 0.061879225075244904 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9875],\n",
      "        [0.4743]])\n",
      "Iteration 11720 Training loss 0.05767285078763962 Validation loss 0.06206979975104332 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0716],\n",
      "        [0.9989]])\n",
      "Iteration 11730 Training loss 0.05852508544921875 Validation loss 0.061885587871074677 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.1721],\n",
      "        [0.6271]])\n",
      "Iteration 11740 Training loss 0.0586005337536335 Validation loss 0.062104299664497375 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9344],\n",
      "        [0.0072]])\n",
      "Iteration 11750 Training loss 0.06260137259960175 Validation loss 0.06186222657561302 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.6138],\n",
      "        [0.4641]])\n",
      "Iteration 11760 Training loss 0.06020491570234299 Validation loss 0.06181853637099266 Accuracy 0.828000009059906\n",
      "Output tensor([[0.6374],\n",
      "        [0.1797]])\n",
      "Iteration 11770 Training loss 0.05903398618102074 Validation loss 0.06239425763487816 Accuracy 0.8224999904632568\n",
      "Output tensor([[0.2309],\n",
      "        [0.4074]])\n",
      "Iteration 11780 Training loss 0.056492459028959274 Validation loss 0.06189509108662605 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9448],\n",
      "        [0.0228]])\n",
      "Iteration 11790 Training loss 0.05431298911571503 Validation loss 0.06198117136955261 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9194],\n",
      "        [0.0422]])\n",
      "Iteration 11800 Training loss 0.05777570977807045 Validation loss 0.06178935617208481 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0862],\n",
      "        [0.8454]])\n",
      "Iteration 11810 Training loss 0.05950528755784035 Validation loss 0.06185983866453171 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.4481],\n",
      "        [0.6623]])\n",
      "Iteration 11820 Training loss 0.05804257094860077 Validation loss 0.06234586238861084 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5986],\n",
      "        [0.6306]])\n",
      "Iteration 11830 Training loss 0.05793766677379608 Validation loss 0.06190400570631027 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0602],\n",
      "        [0.8970]])\n",
      "Iteration 11840 Training loss 0.05862877890467644 Validation loss 0.061801351606845856 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9706],\n",
      "        [0.0307]])\n",
      "Iteration 11850 Training loss 0.060671836137771606 Validation loss 0.06186864525079727 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9112],\n",
      "        [0.0902]])\n",
      "Iteration 11860 Training loss 0.05908334627747536 Validation loss 0.06176530569791794 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.7455],\n",
      "        [0.0347]])\n",
      "Iteration 11870 Training loss 0.05976172536611557 Validation loss 0.06178680807352066 Accuracy 0.828000009059906\n",
      "Output tensor([[0.8769],\n",
      "        [0.4582]])\n",
      "Iteration 11880 Training loss 0.05980205163359642 Validation loss 0.061870381236076355 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9760],\n",
      "        [0.8887]])\n",
      "Iteration 11890 Training loss 0.05698449909687042 Validation loss 0.06201371178030968 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2352],\n",
      "        [0.4192]])\n",
      "Iteration 11900 Training loss 0.05798180028796196 Validation loss 0.06194566190242767 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8877],\n",
      "        [0.0270]])\n",
      "Iteration 11910 Training loss 0.05715275555849075 Validation loss 0.062234483659267426 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.0441],\n",
      "        [0.8788]])\n",
      "Iteration 11920 Training loss 0.05989133566617966 Validation loss 0.06164408102631569 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0456],\n",
      "        [0.6995]])\n",
      "Iteration 11930 Training loss 0.060472190380096436 Validation loss 0.061652325093746185 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.6121],\n",
      "        [0.9978]])\n",
      "Iteration 11940 Training loss 0.06070041283965111 Validation loss 0.06166936084628105 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0259],\n",
      "        [0.9237]])\n",
      "Iteration 11950 Training loss 0.059059981256723404 Validation loss 0.06208194047212601 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.2808],\n",
      "        [0.9877]])\n",
      "Iteration 11960 Training loss 0.05342396721243858 Validation loss 0.0618315115571022 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.2220],\n",
      "        [0.2042]])\n",
      "Iteration 11970 Training loss 0.05634633079171181 Validation loss 0.06169216334819794 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9019],\n",
      "        [0.2786]])\n",
      "Iteration 11980 Training loss 0.058192748576402664 Validation loss 0.061810221523046494 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8713],\n",
      "        [0.0226]])\n",
      "Iteration 11990 Training loss 0.05514898523688316 Validation loss 0.06185955926775932 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0346],\n",
      "        [0.7585]])\n",
      "Iteration 12000 Training loss 0.05637697875499725 Validation loss 0.06178539991378784 Accuracy 0.828499972820282\n",
      "Output tensor([[0.5555],\n",
      "        [0.8713]])\n",
      "Iteration 12010 Training loss 0.05678817629814148 Validation loss 0.06194108724594116 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9086],\n",
      "        [0.2578]])\n",
      "Iteration 12020 Training loss 0.056673262268304825 Validation loss 0.062026046216487885 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.1184],\n",
      "        [0.9705]])\n",
      "Iteration 12030 Training loss 0.05805513635277748 Validation loss 0.061861488968133926 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9360],\n",
      "        [0.8641]])\n",
      "Iteration 12040 Training loss 0.056925561279058456 Validation loss 0.06184471771121025 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2889],\n",
      "        [0.3195]])\n",
      "Iteration 12050 Training loss 0.05760742723941803 Validation loss 0.06187530606985092 Accuracy 0.828000009059906\n",
      "Output tensor([[0.7739],\n",
      "        [0.0718]])\n",
      "Iteration 12060 Training loss 0.057008419185876846 Validation loss 0.06190028786659241 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2201],\n",
      "        [0.5515]])\n",
      "Iteration 12070 Training loss 0.05603505298495293 Validation loss 0.06164167448878288 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9655],\n",
      "        [0.0034]])\n",
      "Iteration 12080 Training loss 0.05949663743376732 Validation loss 0.06180351972579956 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0745],\n",
      "        [0.9744]])\n",
      "Iteration 12090 Training loss 0.06642623990774155 Validation loss 0.06173262745141983 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1687],\n",
      "        [0.7111]])\n",
      "Iteration 12100 Training loss 0.06134014204144478 Validation loss 0.061874888837337494 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0180],\n",
      "        [0.8948]])\n",
      "Iteration 12110 Training loss 0.06286310404539108 Validation loss 0.061630234122276306 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1094],\n",
      "        [0.2342]])\n",
      "Iteration 12120 Training loss 0.05757715925574303 Validation loss 0.06196009740233421 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0976],\n",
      "        [0.9943]])\n",
      "Iteration 12130 Training loss 0.060105789452791214 Validation loss 0.06172231212258339 Accuracy 0.828000009059906\n",
      "Output tensor([[0.7921],\n",
      "        [0.8264]])\n",
      "Iteration 12140 Training loss 0.058584265410900116 Validation loss 0.06182483583688736 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.1486],\n",
      "        [0.9713]])\n",
      "Iteration 12150 Training loss 0.058129459619522095 Validation loss 0.0616178885102272 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.5976],\n",
      "        [0.1039]])\n",
      "Iteration 12160 Training loss 0.05714751407504082 Validation loss 0.0616963692009449 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1963],\n",
      "        [0.1284]])\n",
      "Iteration 12170 Training loss 0.05629255622625351 Validation loss 0.06175278499722481 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.5083],\n",
      "        [0.5363]])\n",
      "Iteration 12180 Training loss 0.05555350333452225 Validation loss 0.06162373349070549 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8832],\n",
      "        [0.5220]])\n",
      "Iteration 12190 Training loss 0.06072842329740524 Validation loss 0.061816100031137466 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0865],\n",
      "        [0.9860]])\n",
      "Iteration 12200 Training loss 0.057713352143764496 Validation loss 0.0617276169359684 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.7376],\n",
      "        [0.0704]])\n",
      "Iteration 12210 Training loss 0.055550944060087204 Validation loss 0.06175514683127403 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0494],\n",
      "        [0.9955]])\n",
      "Iteration 12220 Training loss 0.0572204627096653 Validation loss 0.06172141060233116 Accuracy 0.828000009059906\n",
      "Output tensor([[0.4051],\n",
      "        [0.3200]])\n",
      "Iteration 12230 Training loss 0.056375350803136826 Validation loss 0.06257360428571701 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9840],\n",
      "        [0.7856]])\n",
      "Iteration 12240 Training loss 0.05649442970752716 Validation loss 0.06161832436919212 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0298],\n",
      "        [0.4506]])\n",
      "Iteration 12250 Training loss 0.0567060150206089 Validation loss 0.06171022728085518 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.4526],\n",
      "        [0.9612]])\n",
      "Iteration 12260 Training loss 0.05926716700196266 Validation loss 0.06171061843633652 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.6938],\n",
      "        [0.0500]])\n",
      "Iteration 12270 Training loss 0.0581536628305912 Validation loss 0.061718765646219254 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0397],\n",
      "        [0.4832]])\n",
      "Iteration 12280 Training loss 0.06025468185544014 Validation loss 0.062120962888002396 Accuracy 0.824999988079071\n",
      "Output tensor([[0.7743],\n",
      "        [0.9899]])\n",
      "Iteration 12290 Training loss 0.056318249553442 Validation loss 0.0620766282081604 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.8666],\n",
      "        [0.4109]])\n",
      "Iteration 12300 Training loss 0.058342330157756805 Validation loss 0.06189359351992607 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.1853],\n",
      "        [0.9917]])\n",
      "Iteration 12310 Training loss 0.05586673691868782 Validation loss 0.061746712774038315 Accuracy 0.828000009059906\n",
      "Output tensor([[0.8584],\n",
      "        [0.8810]])\n",
      "Iteration 12320 Training loss 0.05981948599219322 Validation loss 0.062207549810409546 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.0085],\n",
      "        [0.8944]])\n",
      "Iteration 12330 Training loss 0.058828167617321014 Validation loss 0.06170585751533508 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9884],\n",
      "        [0.7394]])\n",
      "Iteration 12340 Training loss 0.05807449296116829 Validation loss 0.06174270808696747 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0804],\n",
      "        [0.9637]])\n",
      "Iteration 12350 Training loss 0.05688268691301346 Validation loss 0.06201842427253723 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8024],\n",
      "        [0.0779]])\n",
      "Iteration 12360 Training loss 0.057247746735811234 Validation loss 0.062082406133413315 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.8684],\n",
      "        [0.6761]])\n",
      "Iteration 12370 Training loss 0.06594973057508469 Validation loss 0.06199474260210991 Accuracy 0.828000009059906\n",
      "Output tensor([[0.3968],\n",
      "        [0.6751]])\n",
      "Iteration 12380 Training loss 0.057592637836933136 Validation loss 0.06183278560638428 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0128],\n",
      "        [0.5961]])\n",
      "Iteration 12390 Training loss 0.060507357120513916 Validation loss 0.06171014532446861 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.7374],\n",
      "        [0.5464]])\n",
      "Iteration 12400 Training loss 0.05918815732002258 Validation loss 0.06192275509238243 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0451],\n",
      "        [0.8963]])\n",
      "Iteration 12410 Training loss 0.06128444895148277 Validation loss 0.061724234372377396 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.4929],\n",
      "        [0.0791]])\n",
      "Iteration 12420 Training loss 0.057214777916669846 Validation loss 0.06169618293642998 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.1654],\n",
      "        [0.5665]])\n",
      "Iteration 12430 Training loss 0.056549783796072006 Validation loss 0.061826299875974655 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9489],\n",
      "        [0.0619]])\n",
      "Iteration 12440 Training loss 0.06018568202853203 Validation loss 0.0619504414498806 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2229],\n",
      "        [0.0799]])\n",
      "Iteration 12450 Training loss 0.05687343329191208 Validation loss 0.06165096163749695 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.2304],\n",
      "        [0.7915]])\n",
      "Iteration 12460 Training loss 0.05822234973311424 Validation loss 0.062042564153671265 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.6184],\n",
      "        [0.2934]])\n",
      "Iteration 12470 Training loss 0.057973239570856094 Validation loss 0.06203726306557655 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8082],\n",
      "        [0.0075]])\n",
      "Iteration 12480 Training loss 0.05790463462471962 Validation loss 0.06174023449420929 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.4156],\n",
      "        [0.9997]])\n",
      "Iteration 12490 Training loss 0.05542420968413353 Validation loss 0.06159909814596176 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1807],\n",
      "        [0.9607]])\n",
      "Iteration 12500 Training loss 0.05900917202234268 Validation loss 0.061884865164756775 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9649],\n",
      "        [0.2026]])\n",
      "Iteration 12510 Training loss 0.05645474046468735 Validation loss 0.0619046613574028 Accuracy 0.828000009059906\n",
      "Output tensor([[0.8659],\n",
      "        [0.9309]])\n",
      "Iteration 12520 Training loss 0.05565401539206505 Validation loss 0.06156842038035393 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0315],\n",
      "        [0.9720]])\n",
      "Iteration 12530 Training loss 0.06003759801387787 Validation loss 0.062237177044153214 Accuracy 0.824999988079071\n",
      "Output tensor([[0.8383],\n",
      "        [0.0389]])\n",
      "Iteration 12540 Training loss 0.05669061094522476 Validation loss 0.06169012933969498 Accuracy 0.828000009059906\n",
      "Output tensor([[0.6297],\n",
      "        [0.8089]])\n",
      "Iteration 12550 Training loss 0.05702151358127594 Validation loss 0.0616014339029789 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9959],\n",
      "        [0.5693]])\n",
      "Iteration 12560 Training loss 0.05820939689874649 Validation loss 0.06176711246371269 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8539],\n",
      "        [0.2658]])\n",
      "Iteration 12570 Training loss 0.05798584222793579 Validation loss 0.06176641583442688 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9349],\n",
      "        [0.9357]])\n",
      "Iteration 12580 Training loss 0.05707015097141266 Validation loss 0.06185075268149376 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1207],\n",
      "        [0.9948]])\n",
      "Iteration 12590 Training loss 0.060080066323280334 Validation loss 0.06175811588764191 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8208],\n",
      "        [0.9936]])\n",
      "Iteration 12600 Training loss 0.060276806354522705 Validation loss 0.06168217957019806 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0065],\n",
      "        [0.3856]])\n",
      "Iteration 12610 Training loss 0.060890231281518936 Validation loss 0.062038496136665344 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9408],\n",
      "        [0.0113]])\n",
      "Iteration 12620 Training loss 0.058458875864744186 Validation loss 0.061729948967695236 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.7376],\n",
      "        [0.2244]])\n",
      "Iteration 12630 Training loss 0.060155801475048065 Validation loss 0.061982735991477966 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0604],\n",
      "        [0.9465]])\n",
      "Iteration 12640 Training loss 0.05863407254219055 Validation loss 0.06216837465763092 Accuracy 0.8240000009536743\n",
      "Output tensor([[0.2704],\n",
      "        [0.0828]])\n",
      "Iteration 12650 Training loss 0.05685795471072197 Validation loss 0.06158949062228203 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5256],\n",
      "        [0.9177]])\n",
      "Iteration 12660 Training loss 0.058054085820913315 Validation loss 0.06159893050789833 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0759],\n",
      "        [0.9073]])\n",
      "Iteration 12670 Training loss 0.055369701236486435 Validation loss 0.06174888461828232 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9449],\n",
      "        [0.7140]])\n",
      "Iteration 12680 Training loss 0.05608752369880676 Validation loss 0.06193298473954201 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9522],\n",
      "        [0.9302]])\n",
      "Iteration 12690 Training loss 0.06150205433368683 Validation loss 0.06173233687877655 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0846],\n",
      "        [0.9989]])\n",
      "Iteration 12700 Training loss 0.05511888861656189 Validation loss 0.061833154410123825 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9514],\n",
      "        [0.2158]])\n",
      "Iteration 12710 Training loss 0.05481995269656181 Validation loss 0.0616263747215271 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9934],\n",
      "        [0.1306]])\n",
      "Iteration 12720 Training loss 0.056318726390600204 Validation loss 0.06177838146686554 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9675],\n",
      "        [0.6313]])\n",
      "Iteration 12730 Training loss 0.05757007375359535 Validation loss 0.06161577254533768 Accuracy 0.828499972820282\n",
      "Output tensor([[0.6142],\n",
      "        [0.0795]])\n",
      "Iteration 12740 Training loss 0.059862468391656876 Validation loss 0.061551813036203384 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0743],\n",
      "        [0.1401]])\n",
      "Iteration 12750 Training loss 0.05676265060901642 Validation loss 0.06204133480787277 Accuracy 0.824999988079071\n",
      "Output tensor([[0.5275],\n",
      "        [0.0147]])\n",
      "Iteration 12760 Training loss 0.060151953250169754 Validation loss 0.06153618544340134 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3790],\n",
      "        [0.8394]])\n",
      "Iteration 12770 Training loss 0.05820157006382942 Validation loss 0.061503417789936066 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4391],\n",
      "        [0.9667]])\n",
      "Iteration 12780 Training loss 0.05615754425525665 Validation loss 0.06192546710371971 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.6362],\n",
      "        [0.9643]])\n",
      "Iteration 12790 Training loss 0.05881888046860695 Validation loss 0.06170839071273804 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4626],\n",
      "        [0.7716]])\n",
      "Iteration 12800 Training loss 0.06088931858539581 Validation loss 0.061742477118968964 Accuracy 0.828000009059906\n",
      "Output tensor([[0.3329],\n",
      "        [0.7397]])\n",
      "Iteration 12810 Training loss 0.05665501952171326 Validation loss 0.06157239153981209 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.5595],\n",
      "        [0.7726]])\n",
      "Iteration 12820 Training loss 0.05683794617652893 Validation loss 0.06154512241482735 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4970],\n",
      "        [0.0815]])\n",
      "Iteration 12830 Training loss 0.05908386409282684 Validation loss 0.06152693182229996 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1713],\n",
      "        [0.0583]])\n",
      "Iteration 12840 Training loss 0.061228394508361816 Validation loss 0.061862457543611526 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2671],\n",
      "        [0.9751]])\n",
      "Iteration 12850 Training loss 0.06205538287758827 Validation loss 0.06188803166151047 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0359],\n",
      "        [0.9971]])\n",
      "Iteration 12860 Training loss 0.0590808168053627 Validation loss 0.06167541444301605 Accuracy 0.828499972820282\n",
      "Output tensor([[0.6585],\n",
      "        [0.9079]])\n",
      "Iteration 12870 Training loss 0.05938426032662392 Validation loss 0.061875488609075546 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0177],\n",
      "        [0.3932]])\n",
      "Iteration 12880 Training loss 0.05766326189041138 Validation loss 0.06146533042192459 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8444],\n",
      "        [0.0353]])\n",
      "Iteration 12890 Training loss 0.0552830807864666 Validation loss 0.06180565431714058 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9118],\n",
      "        [0.1402]])\n",
      "Iteration 12900 Training loss 0.05417678877711296 Validation loss 0.06182931363582611 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.1807],\n",
      "        [0.6583]])\n",
      "Iteration 12910 Training loss 0.05690934881567955 Validation loss 0.06170729920268059 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.3126],\n",
      "        [0.4057]])\n",
      "Iteration 12920 Training loss 0.05885675176978111 Validation loss 0.06173209846019745 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1609],\n",
      "        [0.9221]])\n",
      "Iteration 12930 Training loss 0.0601431243121624 Validation loss 0.06152213364839554 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0564],\n",
      "        [0.9730]])\n",
      "Iteration 12940 Training loss 0.05594966560602188 Validation loss 0.061447951942682266 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0287],\n",
      "        [0.7293]])\n",
      "Iteration 12950 Training loss 0.05926563963294029 Validation loss 0.061597153544425964 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9960],\n",
      "        [0.2678]])\n",
      "Iteration 12960 Training loss 0.05990562587976456 Validation loss 0.06156443804502487 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0211],\n",
      "        [0.9618]])\n",
      "Iteration 12970 Training loss 0.06003671884536743 Validation loss 0.06175956130027771 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9132],\n",
      "        [0.5182]])\n",
      "Iteration 12980 Training loss 0.05504372715950012 Validation loss 0.061805255711078644 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.3309],\n",
      "        [0.7578]])\n",
      "Iteration 12990 Training loss 0.05685536563396454 Validation loss 0.061561424285173416 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4209],\n",
      "        [0.9994]])\n",
      "Iteration 13000 Training loss 0.05535024404525757 Validation loss 0.06172025948762894 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9369],\n",
      "        [0.6929]])\n",
      "Iteration 13010 Training loss 0.05944925919175148 Validation loss 0.061782706528902054 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0600],\n",
      "        [0.6829]])\n",
      "Iteration 13020 Training loss 0.057349298149347305 Validation loss 0.06162091717123985 Accuracy 0.828000009059906\n",
      "Output tensor([[0.6440],\n",
      "        [0.7085]])\n",
      "Iteration 13030 Training loss 0.060784582048654556 Validation loss 0.061550263315439224 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9275],\n",
      "        [0.7909]])\n",
      "Iteration 13040 Training loss 0.05840674415230751 Validation loss 0.06156822666525841 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9854],\n",
      "        [0.3329]])\n",
      "Iteration 13050 Training loss 0.05708032101392746 Validation loss 0.06148117035627365 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9139],\n",
      "        [0.3563]])\n",
      "Iteration 13060 Training loss 0.062159277498722076 Validation loss 0.06167241558432579 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9941],\n",
      "        [0.4446]])\n",
      "Iteration 13070 Training loss 0.057632867246866226 Validation loss 0.06196172535419464 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0976],\n",
      "        [0.3108]])\n",
      "Iteration 13080 Training loss 0.056818749755620956 Validation loss 0.061600059270858765 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0920],\n",
      "        [0.7681]])\n",
      "Iteration 13090 Training loss 0.05992026999592781 Validation loss 0.06172660365700722 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9926],\n",
      "        [0.6354]])\n",
      "Iteration 13100 Training loss 0.05954889580607414 Validation loss 0.06155920401215553 Accuracy 0.828000009059906\n",
      "Output tensor([[0.7385],\n",
      "        [0.1131]])\n",
      "Iteration 13110 Training loss 0.05682926997542381 Validation loss 0.06153535470366478 Accuracy 0.828000009059906\n",
      "Output tensor([[0.7684],\n",
      "        [0.5941]])\n",
      "Iteration 13120 Training loss 0.05808671563863754 Validation loss 0.06179945915937424 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0162],\n",
      "        [0.0552]])\n",
      "Iteration 13130 Training loss 0.05615171790122986 Validation loss 0.06208783760666847 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.7748],\n",
      "        [0.0814]])\n",
      "Iteration 13140 Training loss 0.05617523193359375 Validation loss 0.06143806502223015 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6233],\n",
      "        [0.0753]])\n",
      "Iteration 13150 Training loss 0.05509157106280327 Validation loss 0.06150360777974129 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0450],\n",
      "        [0.5239]])\n",
      "Iteration 13160 Training loss 0.060511816293001175 Validation loss 0.06161079928278923 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8685],\n",
      "        [0.5738]])\n",
      "Iteration 13170 Training loss 0.0584779717028141 Validation loss 0.06156833469867706 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8479],\n",
      "        [0.8516]])\n",
      "Iteration 13180 Training loss 0.05546250194311142 Validation loss 0.06154508888721466 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0062],\n",
      "        [0.2602]])\n",
      "Iteration 13190 Training loss 0.05770634859800339 Validation loss 0.06194162741303444 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0267],\n",
      "        [0.2824]])\n",
      "Iteration 13200 Training loss 0.05664052814245224 Validation loss 0.061792854219675064 Accuracy 0.828000009059906\n",
      "Output tensor([[0.7016],\n",
      "        [0.9886]])\n",
      "Iteration 13210 Training loss 0.056511782109737396 Validation loss 0.06160972639918327 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0171],\n",
      "        [0.4331]])\n",
      "Iteration 13220 Training loss 0.05748652666807175 Validation loss 0.06182900071144104 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.7936],\n",
      "        [0.7508]])\n",
      "Iteration 13230 Training loss 0.05310637131333351 Validation loss 0.061871230602264404 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9769],\n",
      "        [0.3344]])\n",
      "Iteration 13240 Training loss 0.05971495062112808 Validation loss 0.061608292162418365 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4574],\n",
      "        [0.0331]])\n",
      "Iteration 13250 Training loss 0.05741001293063164 Validation loss 0.06156475841999054 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2845],\n",
      "        [0.1742]])\n",
      "Iteration 13260 Training loss 0.058796461671590805 Validation loss 0.06164049357175827 Accuracy 0.828499972820282\n",
      "Output tensor([[0.7087],\n",
      "        [0.4244]])\n",
      "Iteration 13270 Training loss 0.05707353726029396 Validation loss 0.06157499924302101 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.7463],\n",
      "        [0.9921]])\n",
      "Iteration 13280 Training loss 0.05343054607510567 Validation loss 0.06156870350241661 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0058],\n",
      "        [0.1968]])\n",
      "Iteration 13290 Training loss 0.056405793875455856 Validation loss 0.061567582190036774 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9095],\n",
      "        [0.2993]])\n",
      "Iteration 13300 Training loss 0.06087939068675041 Validation loss 0.06200403347611427 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.1235],\n",
      "        [0.9436]])\n",
      "Iteration 13310 Training loss 0.05740514025092125 Validation loss 0.061907630413770676 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0193],\n",
      "        [0.7913]])\n",
      "Iteration 13320 Training loss 0.0592401959002018 Validation loss 0.061600685119628906 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8935],\n",
      "        [0.7513]])\n",
      "Iteration 13330 Training loss 0.0574604868888855 Validation loss 0.06158043444156647 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9853],\n",
      "        [0.8927]])\n",
      "Iteration 13340 Training loss 0.05601968243718147 Validation loss 0.06147368624806404 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9863],\n",
      "        [0.8886]])\n",
      "Iteration 13350 Training loss 0.057571060955524445 Validation loss 0.06161607429385185 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9357],\n",
      "        [0.1283]])\n",
      "Iteration 13360 Training loss 0.058414630591869354 Validation loss 0.06156190484762192 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.7551],\n",
      "        [0.8945]])\n",
      "Iteration 13370 Training loss 0.05845499411225319 Validation loss 0.061438918113708496 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9018],\n",
      "        [0.9624]])\n",
      "Iteration 13380 Training loss 0.05777886509895325 Validation loss 0.06150071322917938 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9197],\n",
      "        [0.3795]])\n",
      "Iteration 13390 Training loss 0.05942622572183609 Validation loss 0.06190340220928192 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.1287],\n",
      "        [0.8007]])\n",
      "Iteration 13400 Training loss 0.058128535747528076 Validation loss 0.06146284565329552 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8283],\n",
      "        [0.9914]])\n",
      "Iteration 13410 Training loss 0.05720527470111847 Validation loss 0.061591047793626785 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0444],\n",
      "        [0.8467]])\n",
      "Iteration 13420 Training loss 0.055354490876197815 Validation loss 0.06141633167862892 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7308],\n",
      "        [0.0558]])\n",
      "Iteration 13430 Training loss 0.05808275565505028 Validation loss 0.0614745169878006 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6178],\n",
      "        [0.6954]])\n",
      "Iteration 13440 Training loss 0.05721786990761757 Validation loss 0.061536919325590134 Accuracy 0.828499972820282\n",
      "Output tensor([[0.4980],\n",
      "        [0.9811]])\n",
      "Iteration 13450 Training loss 0.05887017399072647 Validation loss 0.06193535402417183 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.5877],\n",
      "        [0.4376]])\n",
      "Iteration 13460 Training loss 0.057330306619405746 Validation loss 0.0614822655916214 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0606],\n",
      "        [0.9962]])\n",
      "Iteration 13470 Training loss 0.05596993863582611 Validation loss 0.061607204377651215 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0920],\n",
      "        [0.0845]])\n",
      "Iteration 13480 Training loss 0.05889992043375969 Validation loss 0.06142538785934448 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.4566],\n",
      "        [0.7438]])\n",
      "Iteration 13490 Training loss 0.05568746104836464 Validation loss 0.0614769272506237 Accuracy 0.828499972820282\n",
      "Output tensor([[0.1177],\n",
      "        [0.9211]])\n",
      "Iteration 13500 Training loss 0.0552547462284565 Validation loss 0.06189759820699692 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1609],\n",
      "        [0.0027]])\n",
      "Iteration 13510 Training loss 0.0602106973528862 Validation loss 0.06171843037009239 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9969],\n",
      "        [0.8712]])\n",
      "Iteration 13520 Training loss 0.05858486890792847 Validation loss 0.06155582144856453 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9039],\n",
      "        [0.5079]])\n",
      "Iteration 13530 Training loss 0.055852554738521576 Validation loss 0.06166103109717369 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.5980],\n",
      "        [0.7382]])\n",
      "Iteration 13540 Training loss 0.058418259024620056 Validation loss 0.06154518201947212 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9438],\n",
      "        [0.7191]])\n",
      "Iteration 13550 Training loss 0.05766770616173744 Validation loss 0.06173170730471611 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.4608],\n",
      "        [0.0882]])\n",
      "Iteration 13560 Training loss 0.05775314196944237 Validation loss 0.061373185366392136 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4703],\n",
      "        [0.6078]])\n",
      "Iteration 13570 Training loss 0.056961238384246826 Validation loss 0.06152883917093277 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0268],\n",
      "        [0.8021]])\n",
      "Iteration 13580 Training loss 0.05906962230801582 Validation loss 0.06142536923289299 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.6201],\n",
      "        [0.0096]])\n",
      "Iteration 13590 Training loss 0.057693902403116226 Validation loss 0.061368491500616074 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1368],\n",
      "        [0.8423]])\n",
      "Iteration 13600 Training loss 0.056324515491724014 Validation loss 0.061370864510536194 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6285],\n",
      "        [0.2252]])\n",
      "Iteration 13610 Training loss 0.06055690348148346 Validation loss 0.0615667924284935 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0719],\n",
      "        [0.1464]])\n",
      "Iteration 13620 Training loss 0.05651659891009331 Validation loss 0.06151514872908592 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7403],\n",
      "        [0.1375]])\n",
      "Iteration 13630 Training loss 0.056261807680130005 Validation loss 0.06141031160950661 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1103],\n",
      "        [0.0495]])\n",
      "Iteration 13640 Training loss 0.05898398160934448 Validation loss 0.061470068991184235 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1355],\n",
      "        [0.8807]])\n",
      "Iteration 13650 Training loss 0.057480018585920334 Validation loss 0.06145225465297699 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0312],\n",
      "        [0.1718]])\n",
      "Iteration 13660 Training loss 0.058272287249565125 Validation loss 0.06173058971762657 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.3215],\n",
      "        [0.9549]])\n",
      "Iteration 13670 Training loss 0.05883074924349785 Validation loss 0.0618739128112793 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.3040],\n",
      "        [0.1318]])\n",
      "Iteration 13680 Training loss 0.05594361573457718 Validation loss 0.061346255242824554 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6071],\n",
      "        [0.2276]])\n",
      "Iteration 13690 Training loss 0.06008829548954964 Validation loss 0.06167619675397873 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.6627],\n",
      "        [0.0779]])\n",
      "Iteration 13700 Training loss 0.05372609570622444 Validation loss 0.061552900820970535 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9726],\n",
      "        [0.0368]])\n",
      "Iteration 13710 Training loss 0.05892804637551308 Validation loss 0.061495907604694366 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7713],\n",
      "        [0.3553]])\n",
      "Iteration 13720 Training loss 0.058630988001823425 Validation loss 0.061486151069402695 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9855],\n",
      "        [0.1929]])\n",
      "Iteration 13730 Training loss 0.05751713365316391 Validation loss 0.06174551323056221 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.5802],\n",
      "        [0.8434]])\n",
      "Iteration 13740 Training loss 0.05903061851859093 Validation loss 0.061747584491968155 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.7004],\n",
      "        [0.9568]])\n",
      "Iteration 13750 Training loss 0.056000106036663055 Validation loss 0.06141718477010727 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.7654],\n",
      "        [0.1469]])\n",
      "Iteration 13760 Training loss 0.056483879685401917 Validation loss 0.061333052814006805 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0782],\n",
      "        [0.0499]])\n",
      "Iteration 13770 Training loss 0.0558052696287632 Validation loss 0.06149241700768471 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9762],\n",
      "        [0.7720]])\n",
      "Iteration 13780 Training loss 0.05673449859023094 Validation loss 0.06194838508963585 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.4020],\n",
      "        [0.2045]])\n",
      "Iteration 13790 Training loss 0.05618398264050484 Validation loss 0.06147317588329315 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.3198],\n",
      "        [0.3728]])\n",
      "Iteration 13800 Training loss 0.057158879935741425 Validation loss 0.061554256826639175 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8559],\n",
      "        [0.6316]])\n",
      "Iteration 13810 Training loss 0.05649294704198837 Validation loss 0.06164678931236267 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2107],\n",
      "        [0.8455]])\n",
      "Iteration 13820 Training loss 0.056675635278224945 Validation loss 0.06162308529019356 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.3895],\n",
      "        [0.2096]])\n",
      "Iteration 13830 Training loss 0.05700954422354698 Validation loss 0.06160615757107735 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.1049],\n",
      "        [0.2979]])\n",
      "Iteration 13840 Training loss 0.056021951138973236 Validation loss 0.061344023793935776 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1040],\n",
      "        [0.3453]])\n",
      "Iteration 13850 Training loss 0.05950763821601868 Validation loss 0.061582304537296295 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0206],\n",
      "        [0.3163]])\n",
      "Iteration 13860 Training loss 0.05687984451651573 Validation loss 0.06160629913210869 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.6270],\n",
      "        [0.4763]])\n",
      "Iteration 13870 Training loss 0.058868616819381714 Validation loss 0.06152091920375824 Accuracy 0.828499972820282\n",
      "Output tensor([[0.5608],\n",
      "        [0.0359]])\n",
      "Iteration 13880 Training loss 0.05726596713066101 Validation loss 0.06163080036640167 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0631],\n",
      "        [0.1678]])\n",
      "Iteration 13890 Training loss 0.059257250279188156 Validation loss 0.06206201761960983 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0121],\n",
      "        [0.7576]])\n",
      "Iteration 13900 Training loss 0.055244408547878265 Validation loss 0.06157074123620987 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9286],\n",
      "        [0.0105]])\n",
      "Iteration 13910 Training loss 0.05512961000204086 Validation loss 0.06170939654111862 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.6728],\n",
      "        [0.2492]])\n",
      "Iteration 13920 Training loss 0.055860016494989395 Validation loss 0.061581771820783615 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0320],\n",
      "        [0.9364]])\n",
      "Iteration 13930 Training loss 0.057324133813381195 Validation loss 0.06133897602558136 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8991],\n",
      "        [0.9305]])\n",
      "Iteration 13940 Training loss 0.05695550516247749 Validation loss 0.06146365404129028 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1300],\n",
      "        [0.7745]])\n",
      "Iteration 13950 Training loss 0.05959976837038994 Validation loss 0.061367325484752655 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9280],\n",
      "        [0.3409]])\n",
      "Iteration 13960 Training loss 0.055119406431913376 Validation loss 0.06180836260318756 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9383],\n",
      "        [0.0920]])\n",
      "Iteration 13970 Training loss 0.057314515113830566 Validation loss 0.061340995132923126 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8899],\n",
      "        [0.7315]])\n",
      "Iteration 13980 Training loss 0.05690803751349449 Validation loss 0.06151515617966652 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9753],\n",
      "        [0.9808]])\n",
      "Iteration 13990 Training loss 0.05738550424575806 Validation loss 0.06134485453367233 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8725],\n",
      "        [0.5029]])\n",
      "Iteration 14000 Training loss 0.054857734590768814 Validation loss 0.06137153133749962 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1092],\n",
      "        [0.6823]])\n",
      "Iteration 14010 Training loss 0.05547242611646652 Validation loss 0.06186624988913536 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8146],\n",
      "        [0.8874]])\n",
      "Iteration 14020 Training loss 0.058039017021656036 Validation loss 0.06150829419493675 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0984],\n",
      "        [0.1423]])\n",
      "Iteration 14030 Training loss 0.056097257882356644 Validation loss 0.061304111033678055 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0455],\n",
      "        [0.9849]])\n",
      "Iteration 14040 Training loss 0.05937181040644646 Validation loss 0.061506487429142 Accuracy 0.828499972820282\n",
      "Output tensor([[0.8821],\n",
      "        [0.9626]])\n",
      "Iteration 14050 Training loss 0.05525240674614906 Validation loss 0.061310965567827225 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1720],\n",
      "        [0.0888]])\n",
      "Iteration 14060 Training loss 0.05723315104842186 Validation loss 0.06169360876083374 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9157],\n",
      "        [0.9881]])\n",
      "Iteration 14070 Training loss 0.06026187539100647 Validation loss 0.06132163107395172 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8452],\n",
      "        [0.0258]])\n",
      "Iteration 14080 Training loss 0.05969515070319176 Validation loss 0.06128944456577301 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9857],\n",
      "        [0.8750]])\n",
      "Iteration 14090 Training loss 0.05879431962966919 Validation loss 0.061576858162879944 Accuracy 0.828499972820282\n",
      "Output tensor([[0.6033],\n",
      "        [0.1719]])\n",
      "Iteration 14100 Training loss 0.05802144482731819 Validation loss 0.06156426668167114 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0950],\n",
      "        [0.4436]])\n",
      "Iteration 14110 Training loss 0.056483883410692215 Validation loss 0.0614449717104435 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9622],\n",
      "        [0.4934]])\n",
      "Iteration 14120 Training loss 0.05991613492369652 Validation loss 0.06144479662179947 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.3107],\n",
      "        [0.9486]])\n",
      "Iteration 14130 Training loss 0.057231537997722626 Validation loss 0.061546847224235535 Accuracy 0.828499972820282\n",
      "Output tensor([[0.6602],\n",
      "        [0.0186]])\n",
      "Iteration 14140 Training loss 0.05540134385228157 Validation loss 0.06151743605732918 Accuracy 0.828499972820282\n",
      "Output tensor([[0.6872],\n",
      "        [0.9630]])\n",
      "Iteration 14150 Training loss 0.05703822150826454 Validation loss 0.061263684183359146 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9237],\n",
      "        [0.0560]])\n",
      "Iteration 14160 Training loss 0.057984281331300735 Validation loss 0.06152614578604698 Accuracy 0.828499972820282\n",
      "Output tensor([[0.3810],\n",
      "        [0.5247]])\n",
      "Iteration 14170 Training loss 0.059287771582603455 Validation loss 0.06162702292203903 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4682],\n",
      "        [0.1802]])\n",
      "Iteration 14180 Training loss 0.05474230274558067 Validation loss 0.061647962778806686 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2085],\n",
      "        [0.9999]])\n",
      "Iteration 14190 Training loss 0.05977369844913483 Validation loss 0.06149517744779587 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5761],\n",
      "        [0.9749]])\n",
      "Iteration 14200 Training loss 0.05671965703368187 Validation loss 0.0612308569252491 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0461],\n",
      "        [0.8106]])\n",
      "Iteration 14210 Training loss 0.05657682195305824 Validation loss 0.06132076308131218 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4602],\n",
      "        [0.9428]])\n",
      "Iteration 14220 Training loss 0.057492222636938095 Validation loss 0.06129157915711403 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9809],\n",
      "        [0.0765]])\n",
      "Iteration 14230 Training loss 0.05809660255908966 Validation loss 0.06153346225619316 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.7451],\n",
      "        [0.7575]])\n",
      "Iteration 14240 Training loss 0.05712421238422394 Validation loss 0.0612868070602417 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.6648],\n",
      "        [0.0793]])\n",
      "Iteration 14250 Training loss 0.05777828395366669 Validation loss 0.061502035707235336 Accuracy 0.828499972820282\n",
      "Output tensor([[0.4268],\n",
      "        [0.9230]])\n",
      "Iteration 14260 Training loss 0.0569162592291832 Validation loss 0.06129740551114082 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8407],\n",
      "        [0.9691]])\n",
      "Iteration 14270 Training loss 0.05866594612598419 Validation loss 0.06138674542307854 Accuracy 0.828499972820282\n",
      "Output tensor([[0.6545],\n",
      "        [0.5541]])\n",
      "Iteration 14280 Training loss 0.0595400370657444 Validation loss 0.061421606689691544 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0557],\n",
      "        [0.6523]])\n",
      "Iteration 14290 Training loss 0.0556635707616806 Validation loss 0.06198161095380783 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9817],\n",
      "        [0.0505]])\n",
      "Iteration 14300 Training loss 0.056773994117975235 Validation loss 0.06138593330979347 Accuracy 0.828499972820282\n",
      "Output tensor([[0.8103],\n",
      "        [0.0916]])\n",
      "Iteration 14310 Training loss 0.05528955161571503 Validation loss 0.06130724400281906 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.3248],\n",
      "        [0.8091]])\n",
      "Iteration 14320 Training loss 0.053865667432546616 Validation loss 0.06138236075639725 Accuracy 0.828499972820282\n",
      "Output tensor([[0.7982],\n",
      "        [0.1073]])\n",
      "Iteration 14330 Training loss 0.05443638563156128 Validation loss 0.06127627566456795 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.6331],\n",
      "        [0.2079]])\n",
      "Iteration 14340 Training loss 0.05701650306582451 Validation loss 0.061681266874074936 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9474],\n",
      "        [0.9255]])\n",
      "Iteration 14350 Training loss 0.05763944238424301 Validation loss 0.06142070144414902 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6849],\n",
      "        [0.3901]])\n",
      "Iteration 14360 Training loss 0.058972373604774475 Validation loss 0.06125376373529434 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9675],\n",
      "        [0.8336]])\n",
      "Iteration 14370 Training loss 0.0540093369781971 Validation loss 0.06161261722445488 Accuracy 0.828000009059906\n",
      "Output tensor([[0.6052],\n",
      "        [0.9985]])\n",
      "Iteration 14380 Training loss 0.0587250255048275 Validation loss 0.06125052273273468 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4594],\n",
      "        [0.9533]])\n",
      "Iteration 14390 Training loss 0.05702800303697586 Validation loss 0.06177455559372902 Accuracy 0.828499972820282\n",
      "Output tensor([[0.8244],\n",
      "        [0.9721]])\n",
      "Iteration 14400 Training loss 0.05887069180607796 Validation loss 0.06137234345078468 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7871],\n",
      "        [0.1650]])\n",
      "Iteration 14410 Training loss 0.054738257080316544 Validation loss 0.06151755526661873 Accuracy 0.828499972820282\n",
      "Output tensor([[0.3528],\n",
      "        [0.9889]])\n",
      "Iteration 14420 Training loss 0.05566726624965668 Validation loss 0.06136295199394226 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5846],\n",
      "        [0.8878]])\n",
      "Iteration 14430 Training loss 0.05482637882232666 Validation loss 0.061319563537836075 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6716],\n",
      "        [0.9979]])\n",
      "Iteration 14440 Training loss 0.057125989347696304 Validation loss 0.06119019165635109 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9301],\n",
      "        [0.3535]])\n",
      "Iteration 14450 Training loss 0.058227866888046265 Validation loss 0.06162213906645775 Accuracy 0.828000009059906\n",
      "Output tensor([[0.3083],\n",
      "        [0.8556]])\n",
      "Iteration 14460 Training loss 0.05818478390574455 Validation loss 0.061406105756759644 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9922],\n",
      "        [0.5340]])\n",
      "Iteration 14470 Training loss 0.05656618997454643 Validation loss 0.06162644922733307 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0092],\n",
      "        [0.1728]])\n",
      "Iteration 14480 Training loss 0.05815984308719635 Validation loss 0.06118915602564812 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9826],\n",
      "        [0.9452]])\n",
      "Iteration 14490 Training loss 0.059078413993120193 Validation loss 0.06159568950533867 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.3952],\n",
      "        [0.8143]])\n",
      "Iteration 14500 Training loss 0.055658843368291855 Validation loss 0.0615040548145771 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9474],\n",
      "        [0.0349]])\n",
      "Iteration 14510 Training loss 0.05888815596699715 Validation loss 0.06133396178483963 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0232],\n",
      "        [0.9287]])\n",
      "Iteration 14520 Training loss 0.05621841549873352 Validation loss 0.06151117756962776 Accuracy 0.828499972820282\n",
      "Output tensor([[0.6454],\n",
      "        [0.3543]])\n",
      "Iteration 14530 Training loss 0.05938829481601715 Validation loss 0.06172879785299301 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.6123],\n",
      "        [0.0588]])\n",
      "Iteration 14540 Training loss 0.055328723043203354 Validation loss 0.06122959405183792 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9741],\n",
      "        [0.9558]])\n",
      "Iteration 14550 Training loss 0.05855359509587288 Validation loss 0.061295196413993835 Accuracy 0.828499972820282\n",
      "Output tensor([[0.5377],\n",
      "        [0.0587]])\n",
      "Iteration 14560 Training loss 0.05508263409137726 Validation loss 0.06164010986685753 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9098],\n",
      "        [0.3441]])\n",
      "Iteration 14570 Training loss 0.05576849356293678 Validation loss 0.06159490346908569 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1081],\n",
      "        [0.4859]])\n",
      "Iteration 14580 Training loss 0.0580957867205143 Validation loss 0.061577774584293365 Accuracy 0.828000009059906\n",
      "Output tensor([[0.4424],\n",
      "        [0.1162]])\n",
      "Iteration 14590 Training loss 0.058607008308172226 Validation loss 0.06175807863473892 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.5331],\n",
      "        [0.2358]])\n",
      "Iteration 14600 Training loss 0.05713016912341118 Validation loss 0.061367467045784 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8632],\n",
      "        [0.9835]])\n",
      "Iteration 14610 Training loss 0.05771972984075546 Validation loss 0.06142900139093399 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7646],\n",
      "        [0.8951]])\n",
      "Iteration 14620 Training loss 0.06077215448021889 Validation loss 0.06139719486236572 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9947],\n",
      "        [0.0726]])\n",
      "Iteration 14630 Training loss 0.0589216910302639 Validation loss 0.06152471899986267 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9382],\n",
      "        [0.9828]])\n",
      "Iteration 14640 Training loss 0.05613509193062782 Validation loss 0.06152372434735298 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9933],\n",
      "        [0.0328]])\n",
      "Iteration 14650 Training loss 0.0558788999915123 Validation loss 0.06126408651471138 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1980],\n",
      "        [0.7681]])\n",
      "Iteration 14660 Training loss 0.05649501830339432 Validation loss 0.06138901039958 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.5689],\n",
      "        [0.6480]])\n",
      "Iteration 14670 Training loss 0.054724715650081635 Validation loss 0.06143387407064438 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9340],\n",
      "        [0.9867]])\n",
      "Iteration 14680 Training loss 0.057853445410728455 Validation loss 0.06177711859345436 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9412],\n",
      "        [0.0659]])\n",
      "Iteration 14690 Training loss 0.05489946901798248 Validation loss 0.06133203208446503 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8667],\n",
      "        [0.8072]])\n",
      "Iteration 14700 Training loss 0.06148179620504379 Validation loss 0.061180856078863144 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.1262],\n",
      "        [0.2355]])\n",
      "Iteration 14710 Training loss 0.055979032069444656 Validation loss 0.06119102984666824 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2736],\n",
      "        [0.0684]])\n",
      "Iteration 14720 Training loss 0.06103317439556122 Validation loss 0.06162129342556 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0274],\n",
      "        [0.0305]])\n",
      "Iteration 14730 Training loss 0.055868908762931824 Validation loss 0.061253320425748825 Accuracy 0.828499972820282\n",
      "Output tensor([[0.1348],\n",
      "        [0.7930]])\n",
      "Iteration 14740 Training loss 0.054381825029850006 Validation loss 0.061379630118608475 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6668],\n",
      "        [0.0059]])\n",
      "Iteration 14750 Training loss 0.057428617030382156 Validation loss 0.061232294887304306 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0576],\n",
      "        [0.0276]])\n",
      "Iteration 14760 Training loss 0.05888768658041954 Validation loss 0.06120401248335838 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0103],\n",
      "        [0.7612]])\n",
      "Iteration 14770 Training loss 0.05711910128593445 Validation loss 0.06117989495396614 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9059],\n",
      "        [0.0532]])\n",
      "Iteration 14780 Training loss 0.05994910001754761 Validation loss 0.061291951686143875 Accuracy 0.828499972820282\n",
      "Output tensor([[0.4408],\n",
      "        [0.8804]])\n",
      "Iteration 14790 Training loss 0.05493617802858353 Validation loss 0.06138918176293373 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6571],\n",
      "        [0.3407]])\n",
      "Iteration 14800 Training loss 0.05767713859677315 Validation loss 0.06137654930353165 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6177],\n",
      "        [0.7954]])\n",
      "Iteration 14810 Training loss 0.05939878150820732 Validation loss 0.06135259196162224 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0377],\n",
      "        [0.3812]])\n",
      "Iteration 14820 Training loss 0.05720265582203865 Validation loss 0.061744775623083115 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.7571],\n",
      "        [0.7780]])\n",
      "Iteration 14830 Training loss 0.05537722259759903 Validation loss 0.06159847974777222 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0969],\n",
      "        [0.3357]])\n",
      "Iteration 14840 Training loss 0.05905262008309364 Validation loss 0.061402443796396255 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9892],\n",
      "        [0.6395]])\n",
      "Iteration 14850 Training loss 0.05666603147983551 Validation loss 0.061358582228422165 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0912],\n",
      "        [0.1024]])\n",
      "Iteration 14860 Training loss 0.055804621428251266 Validation loss 0.061284974217414856 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8204],\n",
      "        [0.6625]])\n",
      "Iteration 14870 Training loss 0.055717043578624725 Validation loss 0.06134003400802612 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7806],\n",
      "        [0.7122]])\n",
      "Iteration 14880 Training loss 0.05599834397435188 Validation loss 0.06143229827284813 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0199],\n",
      "        [0.1450]])\n",
      "Iteration 14890 Training loss 0.05953933298587799 Validation loss 0.061704445630311966 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1591],\n",
      "        [0.0141]])\n",
      "Iteration 14900 Training loss 0.060494523495435715 Validation loss 0.06133807450532913 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1451],\n",
      "        [0.7788]])\n",
      "Iteration 14910 Training loss 0.05611462518572807 Validation loss 0.061390481889247894 Accuracy 0.828499972820282\n",
      "Output tensor([[0.8370],\n",
      "        [0.5872]])\n",
      "Iteration 14920 Training loss 0.05807957798242569 Validation loss 0.06165255978703499 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0109],\n",
      "        [0.9955]])\n",
      "Iteration 14930 Training loss 0.04966702312231064 Validation loss 0.06141303479671478 Accuracy 0.828499972820282\n",
      "Output tensor([[0.3320],\n",
      "        [0.0857]])\n",
      "Iteration 14940 Training loss 0.055744655430316925 Validation loss 0.06143098697066307 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.2918],\n",
      "        [0.5135]])\n",
      "Iteration 14950 Training loss 0.05740430951118469 Validation loss 0.06116359680891037 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8038],\n",
      "        [0.6117]])\n",
      "Iteration 14960 Training loss 0.05797974392771721 Validation loss 0.061417099088430405 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0560],\n",
      "        [0.3168]])\n",
      "Iteration 14970 Training loss 0.05700031295418739 Validation loss 0.06143375113606453 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9810],\n",
      "        [0.8250]])\n",
      "Iteration 14980 Training loss 0.056020088493824005 Validation loss 0.061328284442424774 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.3231],\n",
      "        [0.0297]])\n",
      "Iteration 14990 Training loss 0.05492942035198212 Validation loss 0.06118736416101456 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9322],\n",
      "        [0.9725]])\n",
      "Iteration 15000 Training loss 0.05632053315639496 Validation loss 0.06133926659822464 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.5674],\n",
      "        [0.9409]])\n",
      "Iteration 15010 Training loss 0.056115709245204926 Validation loss 0.061376191675662994 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0105],\n",
      "        [0.3825]])\n",
      "Iteration 15020 Training loss 0.058258477598428726 Validation loss 0.061288099735975266 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0408],\n",
      "        [0.1180]])\n",
      "Iteration 15030 Training loss 0.05563882365822792 Validation loss 0.061361394822597504 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.4970],\n",
      "        [0.1665]])\n",
      "Iteration 15040 Training loss 0.05811770632863045 Validation loss 0.06129363179206848 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9823],\n",
      "        [0.0634]])\n",
      "Iteration 15050 Training loss 0.05660576745867729 Validation loss 0.06143992021679878 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0758],\n",
      "        [0.2587]])\n",
      "Iteration 15060 Training loss 0.05525493249297142 Validation loss 0.06152047589421272 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9668],\n",
      "        [0.2471]])\n",
      "Iteration 15070 Training loss 0.056582011282444 Validation loss 0.061444394290447235 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9960],\n",
      "        [0.5407]])\n",
      "Iteration 15080 Training loss 0.05784791707992554 Validation loss 0.061694029718637466 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9838],\n",
      "        [0.1293]])\n",
      "Iteration 15090 Training loss 0.056038372218608856 Validation loss 0.0615302175283432 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9602],\n",
      "        [0.9752]])\n",
      "Iteration 15100 Training loss 0.055664047598838806 Validation loss 0.061375901103019714 Accuracy 0.828499972820282\n",
      "Output tensor([[0.1296],\n",
      "        [0.0428]])\n",
      "Iteration 15110 Training loss 0.05983247980475426 Validation loss 0.06119941920042038 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0992],\n",
      "        [0.3560]])\n",
      "Iteration 15120 Training loss 0.056361403316259384 Validation loss 0.06137717515230179 Accuracy 0.828000009059906\n",
      "Output tensor([[0.7097],\n",
      "        [0.9254]])\n",
      "Iteration 15130 Training loss 0.05680090934038162 Validation loss 0.06143995746970177 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9777],\n",
      "        [0.6791]])\n",
      "Iteration 15140 Training loss 0.054096072912216187 Validation loss 0.061857085675001144 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.3721],\n",
      "        [0.3141]])\n",
      "Iteration 15150 Training loss 0.059367064386606216 Validation loss 0.0614597387611866 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.5886],\n",
      "        [0.5371]])\n",
      "Iteration 15160 Training loss 0.05595918744802475 Validation loss 0.06161265820264816 Accuracy 0.828499972820282\n",
      "Output tensor([[0.2780],\n",
      "        [0.0042]])\n",
      "Iteration 15170 Training loss 0.05798836052417755 Validation loss 0.06110772490501404 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1962],\n",
      "        [0.9479]])\n",
      "Iteration 15180 Training loss 0.05433391407132149 Validation loss 0.061229195445775986 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.3179],\n",
      "        [0.8039]])\n",
      "Iteration 15190 Training loss 0.05955689400434494 Validation loss 0.061343129724264145 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0624],\n",
      "        [0.8727]])\n",
      "Iteration 15200 Training loss 0.056063368916511536 Validation loss 0.062127646058797836 Accuracy 0.8234999775886536\n",
      "Output tensor([[0.0059],\n",
      "        [0.0398]])\n",
      "Iteration 15210 Training loss 0.05730956420302391 Validation loss 0.061130136251449585 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1849],\n",
      "        [0.5354]])\n",
      "Iteration 15220 Training loss 0.054802898317575455 Validation loss 0.06151757761836052 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9793],\n",
      "        [0.8281]])\n",
      "Iteration 15230 Training loss 0.058226969093084335 Validation loss 0.06138422712683678 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9851],\n",
      "        [0.8444]])\n",
      "Iteration 15240 Training loss 0.05933127924799919 Validation loss 0.06133299693465233 Accuracy 0.828499972820282\n",
      "Output tensor([[0.8570],\n",
      "        [0.0305]])\n",
      "Iteration 15250 Training loss 0.05934535339474678 Validation loss 0.06125955283641815 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9932],\n",
      "        [0.1256]])\n",
      "Iteration 15260 Training loss 0.05929296836256981 Validation loss 0.06114928424358368 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0702],\n",
      "        [0.0658]])\n",
      "Iteration 15270 Training loss 0.057687122374773026 Validation loss 0.0615636371076107 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.6999],\n",
      "        [0.7030]])\n",
      "Iteration 15280 Training loss 0.05813131853938103 Validation loss 0.06130193546414375 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.3525],\n",
      "        [0.0261]])\n",
      "Iteration 15290 Training loss 0.05987229943275452 Validation loss 0.06123080849647522 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.4094],\n",
      "        [0.0893]])\n",
      "Iteration 15300 Training loss 0.05970761924982071 Validation loss 0.0611385740339756 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8729],\n",
      "        [0.9804]])\n",
      "Iteration 15310 Training loss 0.0559026338160038 Validation loss 0.06126997619867325 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9015],\n",
      "        [0.8290]])\n",
      "Iteration 15320 Training loss 0.05474300682544708 Validation loss 0.06129096820950508 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0416],\n",
      "        [0.7821]])\n",
      "Iteration 15330 Training loss 0.05805707350373268 Validation loss 0.06122606247663498 Accuracy 0.828000009059906\n",
      "Output tensor([[0.5622],\n",
      "        [0.0177]])\n",
      "Iteration 15340 Training loss 0.05901164934039116 Validation loss 0.061192695051431656 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0294],\n",
      "        [0.0553]])\n",
      "Iteration 15350 Training loss 0.058727461844682693 Validation loss 0.06129435449838638 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0076],\n",
      "        [0.5664]])\n",
      "Iteration 15360 Training loss 0.057044435292482376 Validation loss 0.06168477609753609 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0041],\n",
      "        [0.1962]])\n",
      "Iteration 15370 Training loss 0.05367877334356308 Validation loss 0.06123685464262962 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.3128],\n",
      "        [0.6362]])\n",
      "Iteration 15380 Training loss 0.058500293642282486 Validation loss 0.06151772290468216 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4477],\n",
      "        [0.9996]])\n",
      "Iteration 15390 Training loss 0.05926375463604927 Validation loss 0.06116873770952225 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8240],\n",
      "        [0.1992]])\n",
      "Iteration 15400 Training loss 0.05581330880522728 Validation loss 0.06119919568300247 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7128],\n",
      "        [0.9848]])\n",
      "Iteration 15410 Training loss 0.05983353033661842 Validation loss 0.06122651696205139 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9627],\n",
      "        [0.0420]])\n",
      "Iteration 15420 Training loss 0.05597145855426788 Validation loss 0.061195507645606995 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1255],\n",
      "        [0.6466]])\n",
      "Iteration 15430 Training loss 0.05793676897883415 Validation loss 0.06118021532893181 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2984],\n",
      "        [0.7877]])\n",
      "Iteration 15440 Training loss 0.05585906654596329 Validation loss 0.06156458333134651 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0412],\n",
      "        [0.5925]])\n",
      "Iteration 15450 Training loss 0.055100228637456894 Validation loss 0.06127964332699776 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.5078],\n",
      "        [0.3662]])\n",
      "Iteration 15460 Training loss 0.05742225795984268 Validation loss 0.06155472248792648 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.4867],\n",
      "        [0.0151]])\n",
      "Iteration 15470 Training loss 0.057428423315286636 Validation loss 0.061201415956020355 Accuracy 0.828499972820282\n",
      "Output tensor([[0.8075],\n",
      "        [0.5764]])\n",
      "Iteration 15480 Training loss 0.05689530447125435 Validation loss 0.06166783347725868 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.7927],\n",
      "        [0.1750]])\n",
      "Iteration 15490 Training loss 0.056075967848300934 Validation loss 0.061242133378982544 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1834],\n",
      "        [0.9205]])\n",
      "Iteration 15500 Training loss 0.0536816380918026 Validation loss 0.061135757714509964 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1695],\n",
      "        [0.7867]])\n",
      "Iteration 15510 Training loss 0.055579349398612976 Validation loss 0.06108571216464043 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9978],\n",
      "        [0.4461]])\n",
      "Iteration 15520 Training loss 0.05726167559623718 Validation loss 0.061174627393484116 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1865],\n",
      "        [0.1259]])\n",
      "Iteration 15530 Training loss 0.05285715311765671 Validation loss 0.061455391347408295 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.7572],\n",
      "        [0.2104]])\n",
      "Iteration 15540 Training loss 0.05825148895382881 Validation loss 0.06118650734424591 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0188],\n",
      "        [0.7436]])\n",
      "Iteration 15550 Training loss 0.057127825915813446 Validation loss 0.06116974353790283 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.4437],\n",
      "        [0.0441]])\n",
      "Iteration 15560 Training loss 0.05908266454935074 Validation loss 0.06117423251271248 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8879],\n",
      "        [0.0014]])\n",
      "Iteration 15570 Training loss 0.05558842793107033 Validation loss 0.06120049208402634 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.4213],\n",
      "        [0.0786]])\n",
      "Iteration 15580 Training loss 0.056813716888427734 Validation loss 0.06113048642873764 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.1983],\n",
      "        [0.5408]])\n",
      "Iteration 15590 Training loss 0.05573684722185135 Validation loss 0.06117234006524086 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9971],\n",
      "        [0.9996]])\n",
      "Iteration 15600 Training loss 0.05146273970603943 Validation loss 0.061111126095056534 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8971],\n",
      "        [0.4545]])\n",
      "Iteration 15610 Training loss 0.0562729611992836 Validation loss 0.06148495525121689 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0481],\n",
      "        [0.1579]])\n",
      "Iteration 15620 Training loss 0.05913497135043144 Validation loss 0.06114547327160835 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7950],\n",
      "        [0.1606]])\n",
      "Iteration 15630 Training loss 0.054855652153491974 Validation loss 0.06132575124502182 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2671],\n",
      "        [0.3331]])\n",
      "Iteration 15640 Training loss 0.058723125606775284 Validation loss 0.06131495162844658 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0085],\n",
      "        [0.0820]])\n",
      "Iteration 15650 Training loss 0.0572982020676136 Validation loss 0.06110962852835655 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.1158],\n",
      "        [0.1108]])\n",
      "Iteration 15660 Training loss 0.058408454060554504 Validation loss 0.06133779510855675 Accuracy 0.828499972820282\n",
      "Output tensor([[0.1125],\n",
      "        [0.6367]])\n",
      "Iteration 15670 Training loss 0.05535171553492546 Validation loss 0.06127895414829254 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9940],\n",
      "        [0.0264]])\n",
      "Iteration 15680 Training loss 0.058148473501205444 Validation loss 0.0613841712474823 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9929],\n",
      "        [0.9869]])\n",
      "Iteration 15690 Training loss 0.05591091141104698 Validation loss 0.06112617626786232 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8735],\n",
      "        [0.8202]])\n",
      "Iteration 15700 Training loss 0.05588364973664284 Validation loss 0.06115327402949333 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.4717],\n",
      "        [0.6382]])\n",
      "Iteration 15710 Training loss 0.056192878633737564 Validation loss 0.061493273824453354 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.6333],\n",
      "        [0.0251]])\n",
      "Iteration 15720 Training loss 0.057103291153907776 Validation loss 0.06128397211432457 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0593],\n",
      "        [0.8866]])\n",
      "Iteration 15730 Training loss 0.05751992389559746 Validation loss 0.06120782345533371 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1309],\n",
      "        [0.8789]])\n",
      "Iteration 15740 Training loss 0.05717504397034645 Validation loss 0.06127530708909035 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8190],\n",
      "        [0.7237]])\n",
      "Iteration 15750 Training loss 0.05951331928372383 Validation loss 0.06105004996061325 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3717],\n",
      "        [0.2295]])\n",
      "Iteration 15760 Training loss 0.05754420533776283 Validation loss 0.06108789145946503 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8408],\n",
      "        [0.5708]])\n",
      "Iteration 15770 Training loss 0.057399675250053406 Validation loss 0.06110237166285515 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0933],\n",
      "        [0.0712]])\n",
      "Iteration 15780 Training loss 0.05464046448469162 Validation loss 0.061695415526628494 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9890],\n",
      "        [0.0436]])\n",
      "Iteration 15790 Training loss 0.058255936950445175 Validation loss 0.061352770775556564 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9496],\n",
      "        [0.7558]])\n",
      "Iteration 15800 Training loss 0.05536317080259323 Validation loss 0.061407603323459625 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.2208],\n",
      "        [0.8661]])\n",
      "Iteration 15810 Training loss 0.05391877889633179 Validation loss 0.061234451830387115 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.4978],\n",
      "        [0.0036]])\n",
      "Iteration 15820 Training loss 0.056553859263658524 Validation loss 0.061260417103767395 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0077],\n",
      "        [0.3509]])\n",
      "Iteration 15830 Training loss 0.05895761027932167 Validation loss 0.06162527576088905 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9311],\n",
      "        [0.9742]])\n",
      "Iteration 15840 Training loss 0.055639054626226425 Validation loss 0.061451252549886703 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8257],\n",
      "        [0.7445]])\n",
      "Iteration 15850 Training loss 0.05647746101021767 Validation loss 0.061131466180086136 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1336],\n",
      "        [0.1111]])\n",
      "Iteration 15860 Training loss 0.05445610731840134 Validation loss 0.061619482934474945 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.2240],\n",
      "        [0.7419]])\n",
      "Iteration 15870 Training loss 0.05737348273396492 Validation loss 0.06144620478153229 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1379],\n",
      "        [0.1275]])\n",
      "Iteration 15880 Training loss 0.05595799908041954 Validation loss 0.06130227819085121 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0139],\n",
      "        [0.4436]])\n",
      "Iteration 15890 Training loss 0.055273909121751785 Validation loss 0.061678629368543625 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8466],\n",
      "        [0.8478]])\n",
      "Iteration 15900 Training loss 0.054097190499305725 Validation loss 0.06107233092188835 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7293],\n",
      "        [0.1385]])\n",
      "Iteration 15910 Training loss 0.05668007582426071 Validation loss 0.061319977045059204 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0247],\n",
      "        [0.0267]])\n",
      "Iteration 15920 Training loss 0.05655664950609207 Validation loss 0.06101464107632637 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9808],\n",
      "        [0.2829]])\n",
      "Iteration 15930 Training loss 0.053526606410741806 Validation loss 0.06112523004412651 Accuracy 0.828499972820282\n",
      "Output tensor([[0.1149],\n",
      "        [0.0955]])\n",
      "Iteration 15940 Training loss 0.05387773737311363 Validation loss 0.061088938266038895 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8152],\n",
      "        [0.0673]])\n",
      "Iteration 15950 Training loss 0.05798003450036049 Validation loss 0.06115950271487236 Accuracy 0.828000009059906\n",
      "Output tensor([[0.8815],\n",
      "        [0.4632]])\n",
      "Iteration 15960 Training loss 0.05674503743648529 Validation loss 0.06127157807350159 Accuracy 0.828000009059906\n",
      "Output tensor([[0.5432],\n",
      "        [0.2033]])\n",
      "Iteration 15970 Training loss 0.05841114744544029 Validation loss 0.06107405945658684 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9004],\n",
      "        [0.4088]])\n",
      "Iteration 15980 Training loss 0.05549989268183708 Validation loss 0.061244890093803406 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9840],\n",
      "        [0.8187]])\n",
      "Iteration 15990 Training loss 0.056706517934799194 Validation loss 0.06130809336900711 Accuracy 0.828499972820282\n",
      "Output tensor([[0.5964],\n",
      "        [0.4819]])\n",
      "Iteration 16000 Training loss 0.05490567162632942 Validation loss 0.061353784054517746 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0175],\n",
      "        [0.0284]])\n",
      "Iteration 16010 Training loss 0.054783791303634644 Validation loss 0.06128289923071861 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0666],\n",
      "        [0.9656]])\n",
      "Iteration 16020 Training loss 0.05892471224069595 Validation loss 0.06100846081972122 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0095],\n",
      "        [0.8054]])\n",
      "Iteration 16030 Training loss 0.0580400787293911 Validation loss 0.0615372397005558 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9815],\n",
      "        [0.6956]])\n",
      "Iteration 16040 Training loss 0.05449874699115753 Validation loss 0.06136900931596756 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5591],\n",
      "        [0.2594]])\n",
      "Iteration 16050 Training loss 0.05651750788092613 Validation loss 0.06104856729507446 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9278],\n",
      "        [0.1834]])\n",
      "Iteration 16060 Training loss 0.05325949937105179 Validation loss 0.06123942881822586 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0709],\n",
      "        [0.7797]])\n",
      "Iteration 16070 Training loss 0.05384701490402222 Validation loss 0.061129990965127945 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9133],\n",
      "        [0.8833]])\n",
      "Iteration 16080 Training loss 0.05632409825921059 Validation loss 0.061233337968587875 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.3321],\n",
      "        [0.7721]])\n",
      "Iteration 16090 Training loss 0.054679080843925476 Validation loss 0.06118028983473778 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.7914],\n",
      "        [0.4999]])\n",
      "Iteration 16100 Training loss 0.05648794397711754 Validation loss 0.06106216832995415 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0089],\n",
      "        [0.0081]])\n",
      "Iteration 16110 Training loss 0.05721133574843407 Validation loss 0.061043135821819305 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0930],\n",
      "        [0.9290]])\n",
      "Iteration 16120 Training loss 0.0552460215985775 Validation loss 0.06148919090628624 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0292],\n",
      "        [0.9497]])\n",
      "Iteration 16130 Training loss 0.05735364556312561 Validation loss 0.06154820695519447 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.7527],\n",
      "        [0.7466]])\n",
      "Iteration 16140 Training loss 0.05548466742038727 Validation loss 0.06112748757004738 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9958],\n",
      "        [0.6928]])\n",
      "Iteration 16150 Training loss 0.05766098573803902 Validation loss 0.061487432569265366 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9950],\n",
      "        [0.0313]])\n",
      "Iteration 16160 Training loss 0.058105483651161194 Validation loss 0.061066076159477234 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5623],\n",
      "        [0.8799]])\n",
      "Iteration 16170 Training loss 0.052589818835258484 Validation loss 0.06138666719198227 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0220],\n",
      "        [0.9189]])\n",
      "Iteration 16180 Training loss 0.05759767070412636 Validation loss 0.06160037964582443 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.7857],\n",
      "        [0.0358]])\n",
      "Iteration 16190 Training loss 0.05516146868467331 Validation loss 0.06118801236152649 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7412],\n",
      "        [0.0236]])\n",
      "Iteration 16200 Training loss 0.05661644786596298 Validation loss 0.061243049800395966 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9670],\n",
      "        [0.0766]])\n",
      "Iteration 16210 Training loss 0.05494225025177002 Validation loss 0.06107527017593384 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1096],\n",
      "        [0.0172]])\n",
      "Iteration 16220 Training loss 0.05779967084527016 Validation loss 0.06137197092175484 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9633],\n",
      "        [0.9194]])\n",
      "Iteration 16230 Training loss 0.05348585173487663 Validation loss 0.06131327897310257 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9962],\n",
      "        [0.4705]])\n",
      "Iteration 16240 Training loss 0.05477344989776611 Validation loss 0.06127127632498741 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8639],\n",
      "        [0.3359]])\n",
      "Iteration 16250 Training loss 0.05675942450761795 Validation loss 0.06107078120112419 Accuracy 0.828000009059906\n",
      "Output tensor([[0.5992],\n",
      "        [0.5500]])\n",
      "Iteration 16260 Training loss 0.059469033032655716 Validation loss 0.06147795170545578 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.6018],\n",
      "        [0.9711]])\n",
      "Iteration 16270 Training loss 0.058189962059259415 Validation loss 0.06119674816727638 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9995],\n",
      "        [0.0362]])\n",
      "Iteration 16280 Training loss 0.057855237275362015 Validation loss 0.060979198664426804 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8180],\n",
      "        [0.0049]])\n",
      "Iteration 16290 Training loss 0.056225650012493134 Validation loss 0.061101283878088 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0881],\n",
      "        [0.3363]])\n",
      "Iteration 16300 Training loss 0.0534001924097538 Validation loss 0.06130090355873108 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1843],\n",
      "        [0.5086]])\n",
      "Iteration 16310 Training loss 0.060382094234228134 Validation loss 0.06128459423780441 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8375],\n",
      "        [0.2253]])\n",
      "Iteration 16320 Training loss 0.056470125913619995 Validation loss 0.06096026673913002 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.1609],\n",
      "        [0.7789]])\n",
      "Iteration 16330 Training loss 0.05160755664110184 Validation loss 0.06119326502084732 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0160],\n",
      "        [0.5923]])\n",
      "Iteration 16340 Training loss 0.05621455982327461 Validation loss 0.06158098578453064 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0669],\n",
      "        [0.0490]])\n",
      "Iteration 16350 Training loss 0.0583195686340332 Validation loss 0.06136832758784294 Accuracy 0.828499972820282\n",
      "Output tensor([[0.2784],\n",
      "        [0.1153]])\n",
      "Iteration 16360 Training loss 0.0574989952147007 Validation loss 0.06164954975247383 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.0231],\n",
      "        [0.8387]])\n",
      "Iteration 16370 Training loss 0.05728926882147789 Validation loss 0.061195388436317444 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0950],\n",
      "        [0.2615]])\n",
      "Iteration 16380 Training loss 0.05686835199594498 Validation loss 0.06109081208705902 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9186],\n",
      "        [0.0241]])\n",
      "Iteration 16390 Training loss 0.05896962434053421 Validation loss 0.06096639856696129 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1133],\n",
      "        [0.8297]])\n",
      "Iteration 16400 Training loss 0.05960933864116669 Validation loss 0.06096801906824112 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0052],\n",
      "        [0.2694]])\n",
      "Iteration 16410 Training loss 0.05629498511552811 Validation loss 0.0612085796892643 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7343],\n",
      "        [0.8500]])\n",
      "Iteration 16420 Training loss 0.05537578836083412 Validation loss 0.06104991212487221 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8943],\n",
      "        [0.9983]])\n",
      "Iteration 16430 Training loss 0.05378101021051407 Validation loss 0.06130196526646614 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5968],\n",
      "        [0.9160]])\n",
      "Iteration 16440 Training loss 0.055154260247945786 Validation loss 0.060973867774009705 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0473],\n",
      "        [0.9340]])\n",
      "Iteration 16450 Training loss 0.056867796927690506 Validation loss 0.06111007183790207 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0662],\n",
      "        [0.8581]])\n",
      "Iteration 16460 Training loss 0.051267195492982864 Validation loss 0.06125597283244133 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8858],\n",
      "        [0.1430]])\n",
      "Iteration 16470 Training loss 0.05540532246232033 Validation loss 0.061204321682453156 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8410],\n",
      "        [0.5787]])\n",
      "Iteration 16480 Training loss 0.054860811680555344 Validation loss 0.06092572957277298 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0262],\n",
      "        [0.0715]])\n",
      "Iteration 16490 Training loss 0.056841325014829636 Validation loss 0.06110186129808426 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9154],\n",
      "        [0.8256]])\n",
      "Iteration 16500 Training loss 0.0589308924973011 Validation loss 0.061123963445425034 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9389],\n",
      "        [0.8356]])\n",
      "Iteration 16510 Training loss 0.0556962676346302 Validation loss 0.06099734082818031 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7349],\n",
      "        [0.9941]])\n",
      "Iteration 16520 Training loss 0.0524536594748497 Validation loss 0.060992300510406494 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0531],\n",
      "        [0.8560]])\n",
      "Iteration 16530 Training loss 0.05699297785758972 Validation loss 0.06126762554049492 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9255],\n",
      "        [0.9541]])\n",
      "Iteration 16540 Training loss 0.056423120200634 Validation loss 0.06112931668758392 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8589],\n",
      "        [0.7420]])\n",
      "Iteration 16550 Training loss 0.0563131719827652 Validation loss 0.06116672605276108 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0192],\n",
      "        [0.9528]])\n",
      "Iteration 16560 Training loss 0.057419516146183014 Validation loss 0.06093747913837433 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.3904],\n",
      "        [0.9875]])\n",
      "Iteration 16570 Training loss 0.05345999822020531 Validation loss 0.06105852499604225 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6962],\n",
      "        [0.1086]])\n",
      "Iteration 16580 Training loss 0.056663863360881805 Validation loss 0.06140260398387909 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9900],\n",
      "        [0.0030]])\n",
      "Iteration 16590 Training loss 0.05288001894950867 Validation loss 0.061181169003248215 Accuracy 0.828499972820282\n",
      "Output tensor([[0.6790],\n",
      "        [0.0290]])\n",
      "Iteration 16600 Training loss 0.05626511201262474 Validation loss 0.06110478192567825 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9770],\n",
      "        [0.1130]])\n",
      "Iteration 16610 Training loss 0.05759153887629509 Validation loss 0.06123769283294678 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.4317],\n",
      "        [0.0729]])\n",
      "Iteration 16620 Training loss 0.05654916539788246 Validation loss 0.06121818721294403 Accuracy 0.828000009059906\n",
      "Output tensor([[0.4613],\n",
      "        [0.9980]])\n",
      "Iteration 16630 Training loss 0.0597371980547905 Validation loss 0.06100497394800186 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9653],\n",
      "        [0.0112]])\n",
      "Iteration 16640 Training loss 0.05432640761137009 Validation loss 0.061221975833177567 Accuracy 0.828499972820282\n",
      "Output tensor([[0.4446],\n",
      "        [0.0894]])\n",
      "Iteration 16650 Training loss 0.05421347916126251 Validation loss 0.061016786843538284 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0180],\n",
      "        [0.1742]])\n",
      "Iteration 16660 Training loss 0.05702746659517288 Validation loss 0.06095642223954201 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.2737],\n",
      "        [0.8398]])\n",
      "Iteration 16670 Training loss 0.056173812597990036 Validation loss 0.06175544112920761 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.4073],\n",
      "        [0.4198]])\n",
      "Iteration 16680 Training loss 0.055109672248363495 Validation loss 0.0611518919467926 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8216],\n",
      "        [0.1352]])\n",
      "Iteration 16690 Training loss 0.05931822955608368 Validation loss 0.06093527376651764 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.9800],\n",
      "        [0.9605]])\n",
      "Iteration 16700 Training loss 0.0546615906059742 Validation loss 0.06111046299338341 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8706],\n",
      "        [0.2043]])\n",
      "Iteration 16710 Training loss 0.056924864649772644 Validation loss 0.061478447169065475 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8622],\n",
      "        [0.3410]])\n",
      "Iteration 16720 Training loss 0.05719759687781334 Validation loss 0.06106237694621086 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8945],\n",
      "        [0.2105]])\n",
      "Iteration 16730 Training loss 0.05458196997642517 Validation loss 0.06147413328289986 Accuracy 0.828000009059906\n",
      "Output tensor([[0.5720],\n",
      "        [0.2229]])\n",
      "Iteration 16740 Training loss 0.05486929416656494 Validation loss 0.0610601007938385 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8284],\n",
      "        [0.2372]])\n",
      "Iteration 16750 Training loss 0.058180179446935654 Validation loss 0.061363592743873596 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.2942],\n",
      "        [0.0858]])\n",
      "Iteration 16760 Training loss 0.056273918598890305 Validation loss 0.06118655577301979 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9946],\n",
      "        [0.4518]])\n",
      "Iteration 16770 Training loss 0.05480387061834335 Validation loss 0.06130005791783333 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8682],\n",
      "        [0.9242]])\n",
      "Iteration 16780 Training loss 0.05706612020730972 Validation loss 0.061513084918260574 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9978],\n",
      "        [0.2240]])\n",
      "Iteration 16790 Training loss 0.0560983270406723 Validation loss 0.06139775738120079 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1536],\n",
      "        [0.0380]])\n",
      "Iteration 16800 Training loss 0.05644507706165314 Validation loss 0.06120337173342705 Accuracy 0.828499972820282\n",
      "Output tensor([[0.1326],\n",
      "        [0.9986]])\n",
      "Iteration 16810 Training loss 0.056602295488119125 Validation loss 0.06136234104633331 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9502],\n",
      "        [0.6789]])\n",
      "Iteration 16820 Training loss 0.05339962989091873 Validation loss 0.06090827286243439 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.3074],\n",
      "        [0.7524]])\n",
      "Iteration 16830 Training loss 0.05752374604344368 Validation loss 0.0609549954533577 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.7350],\n",
      "        [0.3750]])\n",
      "Iteration 16840 Training loss 0.05652104318141937 Validation loss 0.0609813816845417 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1568],\n",
      "        [0.1131]])\n",
      "Iteration 16850 Training loss 0.05244838446378708 Validation loss 0.0612189881503582 Accuracy 0.828499972820282\n",
      "Output tensor([[0.7757],\n",
      "        [0.3671]])\n",
      "Iteration 16860 Training loss 0.05225362256169319 Validation loss 0.06134446710348129 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.5762],\n",
      "        [0.9465]])\n",
      "Iteration 16870 Training loss 0.05294308438897133 Validation loss 0.06086275354027748 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0735],\n",
      "        [0.9429]])\n",
      "Iteration 16880 Training loss 0.05543547868728638 Validation loss 0.06121775507926941 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9697],\n",
      "        [0.9050]])\n",
      "Iteration 16890 Training loss 0.056988053023815155 Validation loss 0.061437495052814484 Accuracy 0.828000009059906\n",
      "Output tensor([[0.6610],\n",
      "        [0.6734]])\n",
      "Iteration 16900 Training loss 0.05212560296058655 Validation loss 0.061118125915527344 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9912],\n",
      "        [0.9973]])\n",
      "Iteration 16910 Training loss 0.056842170655727386 Validation loss 0.06097971275448799 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0917],\n",
      "        [0.0209]])\n",
      "Iteration 16920 Training loss 0.05545393377542496 Validation loss 0.060883402824401855 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9552],\n",
      "        [0.0465]])\n",
      "Iteration 16930 Training loss 0.056549035012722015 Validation loss 0.060943275690078735 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1561],\n",
      "        [0.1707]])\n",
      "Iteration 16940 Training loss 0.05168871209025383 Validation loss 0.06140681728720665 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0708],\n",
      "        [0.5822]])\n",
      "Iteration 16950 Training loss 0.055791884660720825 Validation loss 0.06120198965072632 Accuracy 0.828000009059906\n",
      "Output tensor([[0.8808],\n",
      "        [0.5816]])\n",
      "Iteration 16960 Training loss 0.059097498655319214 Validation loss 0.061035703867673874 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1625],\n",
      "        [0.0148]])\n",
      "Iteration 16970 Training loss 0.055551473051309586 Validation loss 0.06104067340493202 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0728],\n",
      "        [0.3985]])\n",
      "Iteration 16980 Training loss 0.05624997988343239 Validation loss 0.06115742400288582 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8811],\n",
      "        [0.0919]])\n",
      "Iteration 16990 Training loss 0.05749469995498657 Validation loss 0.0610184408724308 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0129],\n",
      "        [0.7597]])\n",
      "Iteration 17000 Training loss 0.05905338376760483 Validation loss 0.06109633296728134 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8297],\n",
      "        [0.7522]])\n",
      "Iteration 17010 Training loss 0.05908764898777008 Validation loss 0.06106137856841087 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1222],\n",
      "        [0.7612]])\n",
      "Iteration 17020 Training loss 0.055844251066446304 Validation loss 0.06097657233476639 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4589],\n",
      "        [0.9768]])\n",
      "Iteration 17030 Training loss 0.05656134709715843 Validation loss 0.06098879128694534 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.5241],\n",
      "        [0.0068]])\n",
      "Iteration 17040 Training loss 0.05760609358549118 Validation loss 0.061105549335479736 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.6435],\n",
      "        [0.7201]])\n",
      "Iteration 17050 Training loss 0.05668345466256142 Validation loss 0.06105189397931099 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2415],\n",
      "        [0.0620]])\n",
      "Iteration 17060 Training loss 0.057452715933322906 Validation loss 0.06088157743215561 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.6528],\n",
      "        [0.8779]])\n",
      "Iteration 17070 Training loss 0.05514386668801308 Validation loss 0.06084531173110008 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.8459],\n",
      "        [0.8774]])\n",
      "Iteration 17080 Training loss 0.05231214314699173 Validation loss 0.06102298945188522 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9811],\n",
      "        [0.6406]])\n",
      "Iteration 17090 Training loss 0.05166430026292801 Validation loss 0.061132319271564484 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9294],\n",
      "        [0.9836]])\n",
      "Iteration 17100 Training loss 0.058904413133859634 Validation loss 0.060918524861335754 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2697],\n",
      "        [0.8909]])\n",
      "Iteration 17110 Training loss 0.057875391095876694 Validation loss 0.06078726798295975 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9849],\n",
      "        [0.0784]])\n",
      "Iteration 17120 Training loss 0.0560271218419075 Validation loss 0.06122853606939316 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0232],\n",
      "        [0.4412]])\n",
      "Iteration 17130 Training loss 0.0557459220290184 Validation loss 0.06117359176278114 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8190],\n",
      "        [0.1652]])\n",
      "Iteration 17140 Training loss 0.055067192763090134 Validation loss 0.061351317912340164 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9779],\n",
      "        [0.2080]])\n",
      "Iteration 17150 Training loss 0.05605507269501686 Validation loss 0.060841552913188934 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.2153],\n",
      "        [0.9871]])\n",
      "Iteration 17160 Training loss 0.0538625493645668 Validation loss 0.06101979315280914 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8897],\n",
      "        [0.6778]])\n",
      "Iteration 17170 Training loss 0.05334286391735077 Validation loss 0.061172861605882645 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.4822],\n",
      "        [0.9271]])\n",
      "Iteration 17180 Training loss 0.05310734733939171 Validation loss 0.06089968606829643 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0999],\n",
      "        [0.8317]])\n",
      "Iteration 17190 Training loss 0.054792340844869614 Validation loss 0.060899171978235245 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8171],\n",
      "        [0.1338]])\n",
      "Iteration 17200 Training loss 0.0532660186290741 Validation loss 0.061228279024362564 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0365],\n",
      "        [0.2171]])\n",
      "Iteration 17210 Training loss 0.05668290704488754 Validation loss 0.06081381440162659 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.5170],\n",
      "        [0.5937]])\n",
      "Iteration 17220 Training loss 0.05585911497473717 Validation loss 0.061106957495212555 Accuracy 0.828000009059906\n",
      "Output tensor([[0.6319],\n",
      "        [0.0628]])\n",
      "Iteration 17230 Training loss 0.05530637130141258 Validation loss 0.06092340126633644 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7746],\n",
      "        [0.0224]])\n",
      "Iteration 17240 Training loss 0.057157885283231735 Validation loss 0.06094103306531906 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7079],\n",
      "        [0.2580]])\n",
      "Iteration 17250 Training loss 0.059251826256513596 Validation loss 0.06084759533405304 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9290],\n",
      "        [0.9605]])\n",
      "Iteration 17260 Training loss 0.05335253104567528 Validation loss 0.0612342394888401 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.3585],\n",
      "        [0.9106]])\n",
      "Iteration 17270 Training loss 0.05392315611243248 Validation loss 0.06080164387822151 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0059],\n",
      "        [0.9677]])\n",
      "Iteration 17280 Training loss 0.05571388825774193 Validation loss 0.0608249306678772 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.3779],\n",
      "        [0.1575]])\n",
      "Iteration 17290 Training loss 0.05215096101164818 Validation loss 0.06088795140385628 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1584],\n",
      "        [0.4542]])\n",
      "Iteration 17300 Training loss 0.05774659663438797 Validation loss 0.06094588339328766 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9217],\n",
      "        [0.0467]])\n",
      "Iteration 17310 Training loss 0.05366622656583786 Validation loss 0.061022255569696426 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2597],\n",
      "        [0.5652]])\n",
      "Iteration 17320 Training loss 0.05751277506351471 Validation loss 0.061056237667798996 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0896],\n",
      "        [0.0826]])\n",
      "Iteration 17330 Training loss 0.05702221021056175 Validation loss 0.06095295771956444 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7653],\n",
      "        [0.9543]])\n",
      "Iteration 17340 Training loss 0.05795382708311081 Validation loss 0.061011798679828644 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.3430],\n",
      "        [0.4005]])\n",
      "Iteration 17350 Training loss 0.056355345994234085 Validation loss 0.060870200395584106 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0810],\n",
      "        [0.2600]])\n",
      "Iteration 17360 Training loss 0.05869939550757408 Validation loss 0.061275068670511246 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.7818],\n",
      "        [0.5351]])\n",
      "Iteration 17370 Training loss 0.059606198221445084 Validation loss 0.061040740460157394 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9563],\n",
      "        [0.9687]])\n",
      "Iteration 17380 Training loss 0.05674562230706215 Validation loss 0.060875847935676575 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0153],\n",
      "        [0.1907]])\n",
      "Iteration 17390 Training loss 0.05995897948741913 Validation loss 0.06085354462265968 Accuracy 0.8364999890327454\n",
      "Output tensor([[0.3862],\n",
      "        [0.7442]])\n",
      "Iteration 17400 Training loss 0.05742816999554634 Validation loss 0.06087939813733101 Accuracy 0.8364999890327454\n",
      "Output tensor([[0.8652],\n",
      "        [0.9065]])\n",
      "Iteration 17410 Training loss 0.05224113538861275 Validation loss 0.06093804910778999 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9985],\n",
      "        [0.0478]])\n",
      "Iteration 17420 Training loss 0.05473651736974716 Validation loss 0.06083523854613304 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.2473],\n",
      "        [0.0468]])\n",
      "Iteration 17430 Training loss 0.0579114705324173 Validation loss 0.06096053496003151 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5075],\n",
      "        [0.1485]])\n",
      "Iteration 17440 Training loss 0.0542692095041275 Validation loss 0.06099607050418854 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0159],\n",
      "        [0.9866]])\n",
      "Iteration 17450 Training loss 0.0543062724173069 Validation loss 0.06084653362631798 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.6129],\n",
      "        [0.2717]])\n",
      "Iteration 17460 Training loss 0.05348785221576691 Validation loss 0.06098540499806404 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0866],\n",
      "        [0.0051]])\n",
      "Iteration 17470 Training loss 0.05698753148317337 Validation loss 0.061163246631622314 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8706],\n",
      "        [0.9988]])\n",
      "Iteration 17480 Training loss 0.056201186031103134 Validation loss 0.06101521477103233 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2715],\n",
      "        [0.0615]])\n",
      "Iteration 17490 Training loss 0.05801185593008995 Validation loss 0.06094879284501076 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0246],\n",
      "        [0.9146]])\n",
      "Iteration 17500 Training loss 0.05657130852341652 Validation loss 0.06104991212487221 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.1986],\n",
      "        [0.3229]])\n",
      "Iteration 17510 Training loss 0.05597495287656784 Validation loss 0.06101463735103607 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0729],\n",
      "        [0.0035]])\n",
      "Iteration 17520 Training loss 0.05222982168197632 Validation loss 0.06111239641904831 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9834],\n",
      "        [0.0351]])\n",
      "Iteration 17530 Training loss 0.05574176087975502 Validation loss 0.061229441314935684 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.5687],\n",
      "        [0.0101]])\n",
      "Iteration 17540 Training loss 0.05413881316781044 Validation loss 0.060830265283584595 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.8730],\n",
      "        [0.1263]])\n",
      "Iteration 17550 Training loss 0.053834427148103714 Validation loss 0.06093320995569229 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.5135],\n",
      "        [0.7506]])\n",
      "Iteration 17560 Training loss 0.054591596126556396 Validation loss 0.06103222072124481 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.7623],\n",
      "        [0.4923]])\n",
      "Iteration 17570 Training loss 0.05298609286546707 Validation loss 0.06094557046890259 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8510],\n",
      "        [0.2035]])\n",
      "Iteration 17580 Training loss 0.058218490332365036 Validation loss 0.060854025185108185 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1146],\n",
      "        [0.2725]])\n",
      "Iteration 17590 Training loss 0.05649435520172119 Validation loss 0.06112107262015343 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0023],\n",
      "        [0.0433]])\n",
      "Iteration 17600 Training loss 0.057413049042224884 Validation loss 0.06081484258174896 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8273],\n",
      "        [0.3276]])\n",
      "Iteration 17610 Training loss 0.058067791163921356 Validation loss 0.060942359268665314 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8288],\n",
      "        [0.3201]])\n",
      "Iteration 17620 Training loss 0.054978515952825546 Validation loss 0.06085621193051338 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.6093],\n",
      "        [0.2342]])\n",
      "Iteration 17630 Training loss 0.056734297424554825 Validation loss 0.061007507145404816 Accuracy 0.828000009059906\n",
      "Output tensor([[0.8516],\n",
      "        [0.3997]])\n",
      "Iteration 17640 Training loss 0.0568481907248497 Validation loss 0.06121044233441353 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9193],\n",
      "        [0.2772]])\n",
      "Iteration 17650 Training loss 0.05312937870621681 Validation loss 0.060987867414951324 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9232],\n",
      "        [0.1379]])\n",
      "Iteration 17660 Training loss 0.05767904594540596 Validation loss 0.06103552132844925 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9540],\n",
      "        [0.4419]])\n",
      "Iteration 17670 Training loss 0.05458661913871765 Validation loss 0.060783930122852325 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0382],\n",
      "        [0.4001]])\n",
      "Iteration 17680 Training loss 0.05593303591012955 Validation loss 0.06098763644695282 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0947],\n",
      "        [0.5496]])\n",
      "Iteration 17690 Training loss 0.05503802374005318 Validation loss 0.061254724860191345 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9829],\n",
      "        [0.9953]])\n",
      "Iteration 17700 Training loss 0.05932874605059624 Validation loss 0.060856301337480545 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8535],\n",
      "        [0.8240]])\n",
      "Iteration 17710 Training loss 0.05759372562170029 Validation loss 0.06107519194483757 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9582],\n",
      "        [0.2928]])\n",
      "Iteration 17720 Training loss 0.053712889552116394 Validation loss 0.06077776849269867 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4775],\n",
      "        [0.0162]])\n",
      "Iteration 17730 Training loss 0.05557667464017868 Validation loss 0.0611877366900444 Accuracy 0.828499972820282\n",
      "Output tensor([[0.8688],\n",
      "        [0.4108]])\n",
      "Iteration 17740 Training loss 0.05628236010670662 Validation loss 0.06096867471933365 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9820],\n",
      "        [0.8544]])\n",
      "Iteration 17750 Training loss 0.05215238034725189 Validation loss 0.061124708503484726 Accuracy 0.828499972820282\n",
      "Output tensor([[0.2394],\n",
      "        [0.9865]])\n",
      "Iteration 17760 Training loss 0.052882928401231766 Validation loss 0.06088801473379135 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.9970],\n",
      "        [0.5329]])\n",
      "Iteration 17770 Training loss 0.05393939092755318 Validation loss 0.060933876782655716 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.6927],\n",
      "        [0.4594]])\n",
      "Iteration 17780 Training loss 0.05750596523284912 Validation loss 0.06090760976076126 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.4183],\n",
      "        [0.2724]])\n",
      "Iteration 17790 Training loss 0.05329430475831032 Validation loss 0.060986366122961044 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1521],\n",
      "        [0.0444]])\n",
      "Iteration 17800 Training loss 0.05607955902814865 Validation loss 0.061499565839767456 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.6352],\n",
      "        [0.0947]])\n",
      "Iteration 17810 Training loss 0.0555211566388607 Validation loss 0.06166504696011543 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9650],\n",
      "        [0.0933]])\n",
      "Iteration 17820 Training loss 0.055569838732481 Validation loss 0.06098014861345291 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8897],\n",
      "        [0.7018]])\n",
      "Iteration 17830 Training loss 0.054025810211896896 Validation loss 0.06100868806242943 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0871],\n",
      "        [0.9904]])\n",
      "Iteration 17840 Training loss 0.057189345359802246 Validation loss 0.06097821891307831 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.4729],\n",
      "        [0.1474]])\n",
      "Iteration 17850 Training loss 0.055173132568597794 Validation loss 0.060980889946222305 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.6029],\n",
      "        [0.9685]])\n",
      "Iteration 17860 Training loss 0.05267196148633957 Validation loss 0.061368364840745926 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9551],\n",
      "        [0.0022]])\n",
      "Iteration 17870 Training loss 0.05270582437515259 Validation loss 0.06125929206609726 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.7428],\n",
      "        [0.8562]])\n",
      "Iteration 17880 Training loss 0.05612196773290634 Validation loss 0.06120423227548599 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.5111],\n",
      "        [0.9462]])\n",
      "Iteration 17890 Training loss 0.055954087525606155 Validation loss 0.06104756519198418 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2493],\n",
      "        [0.9685]])\n",
      "Iteration 17900 Training loss 0.0560089573264122 Validation loss 0.060939520597457886 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2354],\n",
      "        [0.2643]])\n",
      "Iteration 17910 Training loss 0.054344985634088516 Validation loss 0.060949720442295074 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0111],\n",
      "        [0.8000]])\n",
      "Iteration 17920 Training loss 0.05593207851052284 Validation loss 0.060887694358825684 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.1178],\n",
      "        [0.3151]])\n",
      "Iteration 17930 Training loss 0.05710745230317116 Validation loss 0.061216287314891815 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8627],\n",
      "        [0.9862]])\n",
      "Iteration 17940 Training loss 0.05416693910956383 Validation loss 0.06099848821759224 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.4246],\n",
      "        [0.5845]])\n",
      "Iteration 17950 Training loss 0.05533391237258911 Validation loss 0.06117873266339302 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0831],\n",
      "        [0.4802]])\n",
      "Iteration 17960 Training loss 0.05376633629202843 Validation loss 0.06086838245391846 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.6304],\n",
      "        [0.2425]])\n",
      "Iteration 17970 Training loss 0.05838795006275177 Validation loss 0.06099417060613632 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9532],\n",
      "        [0.9194]])\n",
      "Iteration 17980 Training loss 0.05627311393618584 Validation loss 0.061195410788059235 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0543],\n",
      "        [0.3195]])\n",
      "Iteration 17990 Training loss 0.060579314827919006 Validation loss 0.06079455465078354 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2515],\n",
      "        [0.0878]])\n",
      "Iteration 18000 Training loss 0.05512691289186478 Validation loss 0.06109889596700668 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1198],\n",
      "        [0.1571]])\n",
      "Iteration 18010 Training loss 0.05690178647637367 Validation loss 0.06106818467378616 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.8859],\n",
      "        [0.9162]])\n",
      "Iteration 18020 Training loss 0.0533943772315979 Validation loss 0.06128279119729996 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9086],\n",
      "        [0.0924]])\n",
      "Iteration 18030 Training loss 0.057507045567035675 Validation loss 0.06085418909788132 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.5180],\n",
      "        [0.6574]])\n",
      "Iteration 18040 Training loss 0.059654172509908676 Validation loss 0.0610625222325325 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.3222],\n",
      "        [0.0726]])\n",
      "Iteration 18050 Training loss 0.055579863488674164 Validation loss 0.06103372946381569 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0455],\n",
      "        [0.4577]])\n",
      "Iteration 18060 Training loss 0.05338931456208229 Validation loss 0.06086260825395584 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.6938],\n",
      "        [0.1827]])\n",
      "Iteration 18070 Training loss 0.056511931121349335 Validation loss 0.06104518100619316 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9088],\n",
      "        [0.3881]])\n",
      "Iteration 18080 Training loss 0.051566120237112045 Validation loss 0.06080956384539604 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.3377],\n",
      "        [0.6033]])\n",
      "Iteration 18090 Training loss 0.059745412319898605 Validation loss 0.060751888900995255 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9422],\n",
      "        [0.9775]])\n",
      "Iteration 18100 Training loss 0.05748564004898071 Validation loss 0.06114540994167328 Accuracy 0.828000009059906\n",
      "Output tensor([[0.1074],\n",
      "        [0.9585]])\n",
      "Iteration 18110 Training loss 0.054899487644433975 Validation loss 0.060864076018333435 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.1001],\n",
      "        [0.4075]])\n",
      "Iteration 18120 Training loss 0.054239630699157715 Validation loss 0.0608476921916008 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.0307],\n",
      "        [0.2642]])\n",
      "Iteration 18130 Training loss 0.05754774436354637 Validation loss 0.06110236048698425 Accuracy 0.828000009059906\n",
      "Output tensor([[0.4685],\n",
      "        [0.7977]])\n",
      "Iteration 18140 Training loss 0.055078186094760895 Validation loss 0.06112965941429138 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9278],\n",
      "        [0.0426]])\n",
      "Iteration 18150 Training loss 0.05698893964290619 Validation loss 0.06081634387373924 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8187],\n",
      "        [0.7521]])\n",
      "Iteration 18160 Training loss 0.05548248440027237 Validation loss 0.06104496866464615 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9835],\n",
      "        [0.8227]])\n",
      "Iteration 18170 Training loss 0.053800493478775024 Validation loss 0.06087464466691017 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9580],\n",
      "        [0.9060]])\n",
      "Iteration 18180 Training loss 0.053769830614328384 Validation loss 0.06089319661259651 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9564],\n",
      "        [0.3404]])\n",
      "Iteration 18190 Training loss 0.055916678160429 Validation loss 0.06077294424176216 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8650],\n",
      "        [0.7175]])\n",
      "Iteration 18200 Training loss 0.053856559097766876 Validation loss 0.0607706718146801 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0798],\n",
      "        [0.0178]])\n",
      "Iteration 18210 Training loss 0.057158470153808594 Validation loss 0.06094633415341377 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.7798],\n",
      "        [0.1790]])\n",
      "Iteration 18220 Training loss 0.0528765544295311 Validation loss 0.060949236154556274 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7248],\n",
      "        [0.8855]])\n",
      "Iteration 18230 Training loss 0.05569278821349144 Validation loss 0.06117945909500122 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.5899],\n",
      "        [0.4804]])\n",
      "Iteration 18240 Training loss 0.05643707513809204 Validation loss 0.06087645888328552 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0289],\n",
      "        [0.9928]])\n",
      "Iteration 18250 Training loss 0.05570393428206444 Validation loss 0.061170898377895355 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.8976],\n",
      "        [0.9975]])\n",
      "Iteration 18260 Training loss 0.05625786632299423 Validation loss 0.06127776950597763 Accuracy 0.828000009059906\n",
      "Output tensor([[0.0076],\n",
      "        [0.2639]])\n",
      "Iteration 18270 Training loss 0.0553310364484787 Validation loss 0.060949429869651794 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.3223],\n",
      "        [0.7775]])\n",
      "Iteration 18280 Training loss 0.055756501853466034 Validation loss 0.061020880937576294 Accuracy 0.828499972820282\n",
      "Output tensor([[0.4654],\n",
      "        [0.0160]])\n",
      "Iteration 18290 Training loss 0.05360732600092888 Validation loss 0.060814615339040756 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9794],\n",
      "        [0.8171]])\n",
      "Iteration 18300 Training loss 0.05317730829119682 Validation loss 0.06092103570699692 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9283],\n",
      "        [0.4874]])\n",
      "Iteration 18310 Training loss 0.058721333742141724 Validation loss 0.06118065491318703 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.3623],\n",
      "        [0.6816]])\n",
      "Iteration 18320 Training loss 0.05640667676925659 Validation loss 0.060697559267282486 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9815],\n",
      "        [0.9896]])\n",
      "Iteration 18330 Training loss 0.057276882231235504 Validation loss 0.06083718687295914 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9855],\n",
      "        [0.8158]])\n",
      "Iteration 18340 Training loss 0.05603863671422005 Validation loss 0.06094798445701599 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8699],\n",
      "        [0.9986]])\n",
      "Iteration 18350 Training loss 0.055837150663137436 Validation loss 0.060713592916727066 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9763],\n",
      "        [0.1838]])\n",
      "Iteration 18360 Training loss 0.052279580384492874 Validation loss 0.06077996641397476 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.6752],\n",
      "        [0.0539]])\n",
      "Iteration 18370 Training loss 0.054899636656045914 Validation loss 0.060740284621715546 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7223],\n",
      "        [0.0555]])\n",
      "Iteration 18380 Training loss 0.05624409392476082 Validation loss 0.06091576814651489 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9945],\n",
      "        [0.2211]])\n",
      "Iteration 18390 Training loss 0.05388718843460083 Validation loss 0.06117893010377884 Accuracy 0.828000009059906\n",
      "Output tensor([[0.5543],\n",
      "        [0.8894]])\n",
      "Iteration 18400 Training loss 0.05328202620148659 Validation loss 0.06080247461795807 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0379],\n",
      "        [0.0214]])\n",
      "Iteration 18410 Training loss 0.055610112845897675 Validation loss 0.060936812311410904 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7026],\n",
      "        [0.4516]])\n",
      "Iteration 18420 Training loss 0.055159058421850204 Validation loss 0.06076071783900261 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.1001],\n",
      "        [0.2148]])\n",
      "Iteration 18430 Training loss 0.0542527511715889 Validation loss 0.06144088879227638 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.8532],\n",
      "        [0.3489]])\n",
      "Iteration 18440 Training loss 0.05322818085551262 Validation loss 0.06107572466135025 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9777],\n",
      "        [0.1338]])\n",
      "Iteration 18450 Training loss 0.054433275014162064 Validation loss 0.06114599108695984 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.5324],\n",
      "        [0.7612]])\n",
      "Iteration 18460 Training loss 0.05500629171729088 Validation loss 0.06087382510304451 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0972],\n",
      "        [0.9746]])\n",
      "Iteration 18470 Training loss 0.055477701127529144 Validation loss 0.06095810607075691 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9996],\n",
      "        [0.7902]])\n",
      "Iteration 18480 Training loss 0.055127985775470734 Validation loss 0.060820240527391434 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.2050],\n",
      "        [0.9070]])\n",
      "Iteration 18490 Training loss 0.05639094486832619 Validation loss 0.06077030301094055 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.4764],\n",
      "        [0.7521]])\n",
      "Iteration 18500 Training loss 0.05637943744659424 Validation loss 0.06088721379637718 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.5814],\n",
      "        [0.6285]])\n",
      "Iteration 18510 Training loss 0.05875681713223457 Validation loss 0.06085239350795746 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0644],\n",
      "        [0.0766]])\n",
      "Iteration 18520 Training loss 0.05458148196339607 Validation loss 0.060784608125686646 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.3782],\n",
      "        [0.0485]])\n",
      "Iteration 18530 Training loss 0.05661490559577942 Validation loss 0.060752373188734055 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.8495],\n",
      "        [0.8237]])\n",
      "Iteration 18540 Training loss 0.057012889534235 Validation loss 0.060864485800266266 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0077],\n",
      "        [0.7498]])\n",
      "Iteration 18550 Training loss 0.05395812913775444 Validation loss 0.06123901531100273 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.5730],\n",
      "        [0.1554]])\n",
      "Iteration 18560 Training loss 0.054654479026794434 Validation loss 0.06077247112989426 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0087],\n",
      "        [0.8102]])\n",
      "Iteration 18570 Training loss 0.05425405129790306 Validation loss 0.060825660824775696 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9169],\n",
      "        [0.7056]])\n",
      "Iteration 18580 Training loss 0.0548756942152977 Validation loss 0.0609130933880806 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9733],\n",
      "        [0.3613]])\n",
      "Iteration 18590 Training loss 0.05556325241923332 Validation loss 0.06078549474477768 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0283],\n",
      "        [0.6231]])\n",
      "Iteration 18600 Training loss 0.057345278561115265 Validation loss 0.06116269901394844 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8209],\n",
      "        [0.0032]])\n",
      "Iteration 18610 Training loss 0.058726683259010315 Validation loss 0.06091143935918808 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0535],\n",
      "        [0.9995]])\n",
      "Iteration 18620 Training loss 0.0538589283823967 Validation loss 0.06107490509748459 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9877],\n",
      "        [0.5652]])\n",
      "Iteration 18630 Training loss 0.058965571224689484 Validation loss 0.06067707762122154 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0235],\n",
      "        [0.4941]])\n",
      "Iteration 18640 Training loss 0.055331747978925705 Validation loss 0.06067280098795891 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.2147],\n",
      "        [0.1003]])\n",
      "Iteration 18650 Training loss 0.05534140020608902 Validation loss 0.06080950051546097 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3484],\n",
      "        [0.1579]])\n",
      "Iteration 18660 Training loss 0.05530760809779167 Validation loss 0.06066495180130005 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0152],\n",
      "        [0.0169]])\n",
      "Iteration 18670 Training loss 0.05937674641609192 Validation loss 0.06068330258131027 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1014],\n",
      "        [0.0723]])\n",
      "Iteration 18680 Training loss 0.054920028895139694 Validation loss 0.060865651816129684 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9413],\n",
      "        [0.2224]])\n",
      "Iteration 18690 Training loss 0.054905965924263 Validation loss 0.060838859528303146 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0165],\n",
      "        [0.8566]])\n",
      "Iteration 18700 Training loss 0.052730705589056015 Validation loss 0.06079040467739105 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0462],\n",
      "        [0.1927]])\n",
      "Iteration 18710 Training loss 0.05698224529623985 Validation loss 0.060709379613399506 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0757],\n",
      "        [0.3947]])\n",
      "Iteration 18720 Training loss 0.05588890612125397 Validation loss 0.060697995126247406 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.4654],\n",
      "        [0.1292]])\n",
      "Iteration 18730 Training loss 0.056028783321380615 Validation loss 0.060683757066726685 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.3605],\n",
      "        [0.9867]])\n",
      "Iteration 18740 Training loss 0.052754342555999756 Validation loss 0.06094961613416672 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0442],\n",
      "        [0.0686]])\n",
      "Iteration 18750 Training loss 0.0567358173429966 Validation loss 0.06095169484615326 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9726],\n",
      "        [0.0116]])\n",
      "Iteration 18760 Training loss 0.057335760444402695 Validation loss 0.06111053377389908 Accuracy 0.8264999985694885\n",
      "Output tensor([[0.9959],\n",
      "        [0.4080]])\n",
      "Iteration 18770 Training loss 0.05524304136633873 Validation loss 0.06075116991996765 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.9938],\n",
      "        [0.9430]])\n",
      "Iteration 18780 Training loss 0.05570579692721367 Validation loss 0.0607173778116703 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0518],\n",
      "        [0.9616]])\n",
      "Iteration 18790 Training loss 0.05420733988285065 Validation loss 0.0608162023127079 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.5642],\n",
      "        [0.7728]])\n",
      "Iteration 18800 Training loss 0.056717149913311005 Validation loss 0.06071583554148674 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0510],\n",
      "        [0.2635]])\n",
      "Iteration 18810 Training loss 0.05391266569495201 Validation loss 0.060755204409360886 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1231],\n",
      "        [0.0600]])\n",
      "Iteration 18820 Training loss 0.054270315915346146 Validation loss 0.060820069164037704 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0078],\n",
      "        [0.3906]])\n",
      "Iteration 18830 Training loss 0.05632071569561958 Validation loss 0.06088956445455551 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3829],\n",
      "        [0.5428]])\n",
      "Iteration 18840 Training loss 0.05296410620212555 Validation loss 0.06128651648759842 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9342],\n",
      "        [0.2334]])\n",
      "Iteration 18850 Training loss 0.056546568870544434 Validation loss 0.06066398695111275 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.7242],\n",
      "        [0.3136]])\n",
      "Iteration 18860 Training loss 0.0544217973947525 Validation loss 0.060785915702581406 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.5482],\n",
      "        [0.9888]])\n",
      "Iteration 18870 Training loss 0.05350421741604805 Validation loss 0.06073414161801338 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.9280],\n",
      "        [0.1625]])\n",
      "Iteration 18880 Training loss 0.05792928859591484 Validation loss 0.060699477791786194 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.3776],\n",
      "        [0.9061]])\n",
      "Iteration 18890 Training loss 0.0591723807156086 Validation loss 0.06077287346124649 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1942],\n",
      "        [0.6339]])\n",
      "Iteration 18900 Training loss 0.055034127086400986 Validation loss 0.06112787500023842 Accuracy 0.824999988079071\n",
      "Output tensor([[0.0242],\n",
      "        [0.7697]])\n",
      "Iteration 18910 Training loss 0.05684402212500572 Validation loss 0.060629963874816895 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.5736],\n",
      "        [0.7624]])\n",
      "Iteration 18920 Training loss 0.05447130277752876 Validation loss 0.06080089136958122 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0473],\n",
      "        [0.9514]])\n",
      "Iteration 18930 Training loss 0.0564165934920311 Validation loss 0.06091082841157913 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4227],\n",
      "        [0.0105]])\n",
      "Iteration 18940 Training loss 0.0507291704416275 Validation loss 0.06076842546463013 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9710],\n",
      "        [0.4576]])\n",
      "Iteration 18950 Training loss 0.05389808863401413 Validation loss 0.06077571213245392 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1720],\n",
      "        [0.0894]])\n",
      "Iteration 18960 Training loss 0.0558180995285511 Validation loss 0.06067468225955963 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0944],\n",
      "        [0.9341]])\n",
      "Iteration 18970 Training loss 0.054017193615436554 Validation loss 0.06061898544430733 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6533],\n",
      "        [0.3943]])\n",
      "Iteration 18980 Training loss 0.05524227395653725 Validation loss 0.060736969113349915 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2968],\n",
      "        [0.9997]])\n",
      "Iteration 18990 Training loss 0.054785050451755524 Validation loss 0.060841843485832214 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8792],\n",
      "        [0.9657]])\n",
      "Iteration 19000 Training loss 0.05680481716990471 Validation loss 0.060999006032943726 Accuracy 0.828000009059906\n",
      "Output tensor([[0.2723],\n",
      "        [0.5892]])\n",
      "Iteration 19010 Training loss 0.05356423556804657 Validation loss 0.0608145035803318 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2289],\n",
      "        [0.8481]])\n",
      "Iteration 19020 Training loss 0.054692771285772324 Validation loss 0.060708507895469666 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9872],\n",
      "        [0.4411]])\n",
      "Iteration 19030 Training loss 0.05470418184995651 Validation loss 0.06099957972764969 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.1410],\n",
      "        [0.9968]])\n",
      "Iteration 19040 Training loss 0.05349605530500412 Validation loss 0.060821838676929474 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.4597],\n",
      "        [0.6634]])\n",
      "Iteration 19050 Training loss 0.05553112551569939 Validation loss 0.060940492898225784 Accuracy 0.828499972820282\n",
      "Output tensor([[0.3666],\n",
      "        [0.9669]])\n",
      "Iteration 19060 Training loss 0.0567660816013813 Validation loss 0.06097635626792908 Accuracy 0.828499972820282\n",
      "Output tensor([[0.2691],\n",
      "        [0.6599]])\n",
      "Iteration 19070 Training loss 0.052340514957904816 Validation loss 0.0607336163520813 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.2459],\n",
      "        [0.0065]])\n",
      "Iteration 19080 Training loss 0.05655520409345627 Validation loss 0.060891151428222656 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9025],\n",
      "        [0.6466]])\n",
      "Iteration 19090 Training loss 0.05547560751438141 Validation loss 0.060853004455566406 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0072],\n",
      "        [0.9645]])\n",
      "Iteration 19100 Training loss 0.05849302187561989 Validation loss 0.060671668499708176 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.5711],\n",
      "        [0.8971]])\n",
      "Iteration 19110 Training loss 0.056611038744449615 Validation loss 0.06061183288693428 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.7693],\n",
      "        [0.2902]])\n",
      "Iteration 19120 Training loss 0.052013687789440155 Validation loss 0.06101959943771362 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2677],\n",
      "        [0.1338]])\n",
      "Iteration 19130 Training loss 0.05448465794324875 Validation loss 0.06066872552037239 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2957],\n",
      "        [0.7221]])\n",
      "Iteration 19140 Training loss 0.0545692965388298 Validation loss 0.06064293906092644 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.7306],\n",
      "        [0.1223]])\n",
      "Iteration 19150 Training loss 0.055820122361183167 Validation loss 0.060693882405757904 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.1217],\n",
      "        [0.0307]])\n",
      "Iteration 19160 Training loss 0.05673542246222496 Validation loss 0.06094164401292801 Accuracy 0.828499972820282\n",
      "Output tensor([[0.7935],\n",
      "        [0.5829]])\n",
      "Iteration 19170 Training loss 0.05687176436185837 Validation loss 0.06063237413764 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1216],\n",
      "        [0.9902]])\n",
      "Iteration 19180 Training loss 0.05772336199879646 Validation loss 0.06092684343457222 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.4224],\n",
      "        [0.9080]])\n",
      "Iteration 19190 Training loss 0.056965187191963196 Validation loss 0.06067754328250885 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.1360],\n",
      "        [0.0638]])\n",
      "Iteration 19200 Training loss 0.05297290161252022 Validation loss 0.060588277876377106 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1346],\n",
      "        [0.7139]])\n",
      "Iteration 19210 Training loss 0.05502576380968094 Validation loss 0.06068413332104683 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4786],\n",
      "        [0.6401]])\n",
      "Iteration 19220 Training loss 0.05495183542370796 Validation loss 0.0606052465736866 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0313],\n",
      "        [0.9665]])\n",
      "Iteration 19230 Training loss 0.05176852270960808 Validation loss 0.0606561005115509 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1984],\n",
      "        [0.9930]])\n",
      "Iteration 19240 Training loss 0.05538028106093407 Validation loss 0.06072617322206497 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7716],\n",
      "        [0.9805]])\n",
      "Iteration 19250 Training loss 0.053331904113292694 Validation loss 0.060794711112976074 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4558],\n",
      "        [0.1234]])\n",
      "Iteration 19260 Training loss 0.056560415774583817 Validation loss 0.06078501045703888 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1075],\n",
      "        [0.5725]])\n",
      "Iteration 19270 Training loss 0.05829053744673729 Validation loss 0.06083338335156441 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6211],\n",
      "        [0.0369]])\n",
      "Iteration 19280 Training loss 0.05363612249493599 Validation loss 0.06071357801556587 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.5497],\n",
      "        [0.9318]])\n",
      "Iteration 19290 Training loss 0.052962157875299454 Validation loss 0.06077074259519577 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9935],\n",
      "        [0.1192]])\n",
      "Iteration 19300 Training loss 0.0549209900200367 Validation loss 0.06081276759505272 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1508],\n",
      "        [0.6931]])\n",
      "Iteration 19310 Training loss 0.05681205168366432 Validation loss 0.06073847785592079 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.1879],\n",
      "        [0.9930]])\n",
      "Iteration 19320 Training loss 0.05595725029706955 Validation loss 0.060743484646081924 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9112],\n",
      "        [0.3747]])\n",
      "Iteration 19330 Training loss 0.054798536002635956 Validation loss 0.06067413464188576 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0954],\n",
      "        [0.0453]])\n",
      "Iteration 19340 Training loss 0.05324098840355873 Validation loss 0.06111542508006096 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9748],\n",
      "        [0.3250]])\n",
      "Iteration 19350 Training loss 0.055084966123104095 Validation loss 0.06061762198805809 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.4529],\n",
      "        [0.8013]])\n",
      "Iteration 19360 Training loss 0.055027179419994354 Validation loss 0.06076114624738693 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0135],\n",
      "        [0.1312]])\n",
      "Iteration 19370 Training loss 0.05609611049294472 Validation loss 0.061307456344366074 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0251],\n",
      "        [0.0735]])\n",
      "Iteration 19380 Training loss 0.0552532933652401 Validation loss 0.060814421623945236 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.5972],\n",
      "        [0.0315]])\n",
      "Iteration 19390 Training loss 0.056942302733659744 Validation loss 0.06089801713824272 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1081],\n",
      "        [0.0893]])\n",
      "Iteration 19400 Training loss 0.05724743381142616 Validation loss 0.060939304530620575 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5457],\n",
      "        [0.5742]])\n",
      "Iteration 19410 Training loss 0.053232017904520035 Validation loss 0.06096932291984558 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8407],\n",
      "        [0.7098]])\n",
      "Iteration 19420 Training loss 0.0570971705019474 Validation loss 0.06062581017613411 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0750],\n",
      "        [0.9792]])\n",
      "Iteration 19430 Training loss 0.051976464688777924 Validation loss 0.060848839581012726 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.1328],\n",
      "        [0.2731]])\n",
      "Iteration 19440 Training loss 0.05317892134189606 Validation loss 0.06060861051082611 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0065],\n",
      "        [0.9968]])\n",
      "Iteration 19450 Training loss 0.05667227879166603 Validation loss 0.060663506388664246 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9695],\n",
      "        [0.9951]])\n",
      "Iteration 19460 Training loss 0.05352018401026726 Validation loss 0.06078876554965973 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7787],\n",
      "        [0.4834]])\n",
      "Iteration 19470 Training loss 0.05681333318352699 Validation loss 0.06077311933040619 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.5458],\n",
      "        [0.8337]])\n",
      "Iteration 19480 Training loss 0.055856164544820786 Validation loss 0.060906074941158295 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8387],\n",
      "        [0.2644]])\n",
      "Iteration 19490 Training loss 0.05658366531133652 Validation loss 0.060838524252176285 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0197],\n",
      "        [0.1737]])\n",
      "Iteration 19500 Training loss 0.054284822195768356 Validation loss 0.06113079935312271 Accuracy 0.828499972820282\n",
      "Output tensor([[0.8413],\n",
      "        [0.0162]])\n",
      "Iteration 19510 Training loss 0.05518660694360733 Validation loss 0.06106584891676903 Accuracy 0.8259999752044678\n",
      "Output tensor([[0.0211],\n",
      "        [0.8302]])\n",
      "Iteration 19520 Training loss 0.05162367969751358 Validation loss 0.060752686113119125 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2929],\n",
      "        [0.2519]])\n",
      "Iteration 19530 Training loss 0.05663703382015228 Validation loss 0.06082611531019211 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8775],\n",
      "        [0.3004]])\n",
      "Iteration 19540 Training loss 0.05695374310016632 Validation loss 0.06062821298837662 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0923],\n",
      "        [0.9017]])\n",
      "Iteration 19550 Training loss 0.05506204068660736 Validation loss 0.06063711643218994 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9880],\n",
      "        [0.0998]])\n",
      "Iteration 19560 Training loss 0.0572429820895195 Validation loss 0.06069530174136162 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0668],\n",
      "        [0.1894]])\n",
      "Iteration 19570 Training loss 0.05806005746126175 Validation loss 0.06069476902484894 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9302],\n",
      "        [0.9877]])\n",
      "Iteration 19580 Training loss 0.05653539299964905 Validation loss 0.06084359809756279 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0030],\n",
      "        [0.5023]])\n",
      "Iteration 19590 Training loss 0.05213304981589317 Validation loss 0.06079450249671936 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.4723],\n",
      "        [0.9165]])\n",
      "Iteration 19600 Training loss 0.053704626858234406 Validation loss 0.06063222512602806 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.3283],\n",
      "        [0.6625]])\n",
      "Iteration 19610 Training loss 0.05193306505680084 Validation loss 0.0607028529047966 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8271],\n",
      "        [0.1404]])\n",
      "Iteration 19620 Training loss 0.05651118606328964 Validation loss 0.0606074333190918 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.3443],\n",
      "        [0.0253]])\n",
      "Iteration 19630 Training loss 0.05619501322507858 Validation loss 0.06081046164035797 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2743],\n",
      "        [0.0707]])\n",
      "Iteration 19640 Training loss 0.05392127111554146 Validation loss 0.0607522577047348 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0331],\n",
      "        [0.1329]])\n",
      "Iteration 19650 Training loss 0.054627709090709686 Validation loss 0.06056421250104904 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0081],\n",
      "        [0.4965]])\n",
      "Iteration 19660 Training loss 0.05670023337006569 Validation loss 0.06060093641281128 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1939],\n",
      "        [0.0380]])\n",
      "Iteration 19670 Training loss 0.054261498153209686 Validation loss 0.060639359056949615 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7605],\n",
      "        [0.3072]])\n",
      "Iteration 19680 Training loss 0.05402420088648796 Validation loss 0.06053805723786354 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0522],\n",
      "        [0.7719]])\n",
      "Iteration 19690 Training loss 0.054009586572647095 Validation loss 0.06068958342075348 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1880],\n",
      "        [0.2839]])\n",
      "Iteration 19700 Training loss 0.05295044556260109 Validation loss 0.060638200491666794 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1363],\n",
      "        [0.5479]])\n",
      "Iteration 19710 Training loss 0.054817501455545425 Validation loss 0.06080125272274017 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5893],\n",
      "        [0.9789]])\n",
      "Iteration 19720 Training loss 0.049801431596279144 Validation loss 0.06069193780422211 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.6715],\n",
      "        [0.7990]])\n",
      "Iteration 19730 Training loss 0.05468221381306648 Validation loss 0.0606510303914547 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.7607],\n",
      "        [0.3835]])\n",
      "Iteration 19740 Training loss 0.053193099796772 Validation loss 0.06060855835676193 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1145],\n",
      "        [0.3235]])\n",
      "Iteration 19750 Training loss 0.060421593487262726 Validation loss 0.06066408008337021 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2008],\n",
      "        [0.4497]])\n",
      "Iteration 19760 Training loss 0.05822181701660156 Validation loss 0.06064679101109505 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6966],\n",
      "        [0.6701]])\n",
      "Iteration 19770 Training loss 0.05450854450464249 Validation loss 0.06086821109056473 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1862],\n",
      "        [0.5933]])\n",
      "Iteration 19780 Training loss 0.05430247262120247 Validation loss 0.06078627333045006 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8747],\n",
      "        [0.4530]])\n",
      "Iteration 19790 Training loss 0.05541342869400978 Validation loss 0.06058798357844353 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7969],\n",
      "        [0.9675]])\n",
      "Iteration 19800 Training loss 0.05547400563955307 Validation loss 0.06118326634168625 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9270],\n",
      "        [0.5435]])\n",
      "Iteration 19810 Training loss 0.051220573484897614 Validation loss 0.06053495407104492 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2742],\n",
      "        [0.1869]])\n",
      "Iteration 19820 Training loss 0.0550832524895668 Validation loss 0.0605003647506237 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8288],\n",
      "        [0.8418]])\n",
      "Iteration 19830 Training loss 0.053481001406908035 Validation loss 0.06049882248044014 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.4485],\n",
      "        [0.9415]])\n",
      "Iteration 19840 Training loss 0.05258829519152641 Validation loss 0.060646284371614456 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9979],\n",
      "        [0.3940]])\n",
      "Iteration 19850 Training loss 0.05765543878078461 Validation loss 0.06051456183195114 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9647],\n",
      "        [0.2652]])\n",
      "Iteration 19860 Training loss 0.056132812052965164 Validation loss 0.060739122331142426 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8583],\n",
      "        [0.3681]])\n",
      "Iteration 19870 Training loss 0.054542895406484604 Validation loss 0.0608852244913578 Accuracy 0.828499972820282\n",
      "Output tensor([[0.5834],\n",
      "        [0.6341]])\n",
      "Iteration 19880 Training loss 0.05405080318450928 Validation loss 0.06072158366441727 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9958],\n",
      "        [0.0961]])\n",
      "Iteration 19890 Training loss 0.056460753083229065 Validation loss 0.06056833639740944 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1128],\n",
      "        [0.6200]])\n",
      "Iteration 19900 Training loss 0.055200234055519104 Validation loss 0.06063496693968773 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0568],\n",
      "        [0.2325]])\n",
      "Iteration 19910 Training loss 0.05642203986644745 Validation loss 0.06064186245203018 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0072],\n",
      "        [0.7197]])\n",
      "Iteration 19920 Training loss 0.053674980998039246 Validation loss 0.06091221421957016 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9358],\n",
      "        [0.2327]])\n",
      "Iteration 19930 Training loss 0.05563286691904068 Validation loss 0.06114860251545906 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.6819],\n",
      "        [0.1216]])\n",
      "Iteration 19940 Training loss 0.05787162482738495 Validation loss 0.06062426418066025 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3416],\n",
      "        [0.8260]])\n",
      "Iteration 19950 Training loss 0.05518215522170067 Validation loss 0.06075097620487213 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2206],\n",
      "        [0.0224]])\n",
      "Iteration 19960 Training loss 0.05293433368206024 Validation loss 0.06081951782107353 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.4552],\n",
      "        [0.2266]])\n",
      "Iteration 19970 Training loss 0.05692330002784729 Validation loss 0.06100887060165405 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.3205],\n",
      "        [0.8109]])\n",
      "Iteration 19980 Training loss 0.05301222950220108 Validation loss 0.06084921956062317 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9796],\n",
      "        [0.6603]])\n",
      "Iteration 19990 Training loss 0.05340800806879997 Validation loss 0.060607146471738815 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3102],\n",
      "        [0.1798]])\n",
      "Iteration 20000 Training loss 0.05487602949142456 Validation loss 0.06074686348438263 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.3276],\n",
      "        [0.0909]])\n",
      "Iteration 20010 Training loss 0.05509531497955322 Validation loss 0.06072848662734032 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0123],\n",
      "        [0.8520]])\n",
      "Iteration 20020 Training loss 0.05120064690709114 Validation loss 0.060578715056180954 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1795],\n",
      "        [0.1632]])\n",
      "Iteration 20030 Training loss 0.05334249883890152 Validation loss 0.060719288885593414 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1142],\n",
      "        [0.0387]])\n",
      "Iteration 20040 Training loss 0.052882008254528046 Validation loss 0.06065550073981285 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8226],\n",
      "        [0.9952]])\n",
      "Iteration 20050 Training loss 0.05546734109520912 Validation loss 0.060602977871894836 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9974],\n",
      "        [0.1064]])\n",
      "Iteration 20060 Training loss 0.0572587214410305 Validation loss 0.06089048832654953 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0034],\n",
      "        [0.8439]])\n",
      "Iteration 20070 Training loss 0.05230084806680679 Validation loss 0.06087237223982811 Accuracy 0.828499972820282\n",
      "Output tensor([[0.7950],\n",
      "        [0.8170]])\n",
      "Iteration 20080 Training loss 0.055756766349077225 Validation loss 0.06065608561038971 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7431],\n",
      "        [0.0333]])\n",
      "Iteration 20090 Training loss 0.0549909807741642 Validation loss 0.06087707355618477 Accuracy 0.828000009059906\n",
      "Output tensor([[0.8982],\n",
      "        [0.3554]])\n",
      "Iteration 20100 Training loss 0.05402616411447525 Validation loss 0.06070529296994209 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.7513],\n",
      "        [0.4303]])\n",
      "Iteration 20110 Training loss 0.054906394332647324 Validation loss 0.060623303055763245 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.3458],\n",
      "        [0.0062]])\n",
      "Iteration 20120 Training loss 0.05671167001128197 Validation loss 0.060693878680467606 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0791],\n",
      "        [0.8132]])\n",
      "Iteration 20130 Training loss 0.05302532762289047 Validation loss 0.06065347045660019 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.3984],\n",
      "        [0.1835]])\n",
      "Iteration 20140 Training loss 0.05254067853093147 Validation loss 0.06080719828605652 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0015],\n",
      "        [0.7995]])\n",
      "Iteration 20150 Training loss 0.05420759692788124 Validation loss 0.06047927215695381 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0473],\n",
      "        [0.9091]])\n",
      "Iteration 20160 Training loss 0.054271601140499115 Validation loss 0.060648150742053986 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1730],\n",
      "        [0.2365]])\n",
      "Iteration 20170 Training loss 0.05470770597457886 Validation loss 0.06057608872652054 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.5515],\n",
      "        [0.4097]])\n",
      "Iteration 20180 Training loss 0.061092156916856766 Validation loss 0.06055673584342003 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0448],\n",
      "        [0.8599]])\n",
      "Iteration 20190 Training loss 0.05227821692824364 Validation loss 0.06091168150305748 Accuracy 0.828000009059906\n",
      "Output tensor([[0.9170],\n",
      "        [0.9804]])\n",
      "Iteration 20200 Training loss 0.05601634830236435 Validation loss 0.06100993975996971 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.2596],\n",
      "        [0.4226]])\n",
      "Iteration 20210 Training loss 0.05243247002363205 Validation loss 0.060590896755456924 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.5877],\n",
      "        [0.1867]])\n",
      "Iteration 20220 Training loss 0.052649207413196564 Validation loss 0.06092129647731781 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.5571],\n",
      "        [0.1887]])\n",
      "Iteration 20230 Training loss 0.05734707787632942 Validation loss 0.06091443449258804 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0258],\n",
      "        [0.6148]])\n",
      "Iteration 20240 Training loss 0.05095528066158295 Validation loss 0.06049323454499245 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2654],\n",
      "        [0.0817]])\n",
      "Iteration 20250 Training loss 0.05846155434846878 Validation loss 0.060488663613796234 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.3081],\n",
      "        [0.7762]])\n",
      "Iteration 20260 Training loss 0.053375206887722015 Validation loss 0.06065593659877777 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.1234],\n",
      "        [0.0029]])\n",
      "Iteration 20270 Training loss 0.05050436779856682 Validation loss 0.060533106327056885 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0053],\n",
      "        [0.3476]])\n",
      "Iteration 20280 Training loss 0.05346524342894554 Validation loss 0.06061244755983353 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5354],\n",
      "        [0.8938]])\n",
      "Iteration 20290 Training loss 0.054566655308008194 Validation loss 0.06050831452012062 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1660],\n",
      "        [0.0313]])\n",
      "Iteration 20300 Training loss 0.05515844002366066 Validation loss 0.06092345342040062 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0264],\n",
      "        [0.9981]])\n",
      "Iteration 20310 Training loss 0.04978363215923309 Validation loss 0.060525719076395035 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0723],\n",
      "        [0.1347]])\n",
      "Iteration 20320 Training loss 0.05160475894808769 Validation loss 0.06063728407025337 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.7406],\n",
      "        [0.3494]])\n",
      "Iteration 20330 Training loss 0.05423499271273613 Validation loss 0.06050952896475792 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7206],\n",
      "        [0.9823]])\n",
      "Iteration 20340 Training loss 0.05558830127120018 Validation loss 0.06113279610872269 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4106],\n",
      "        [0.6899]])\n",
      "Iteration 20350 Training loss 0.057343754917383194 Validation loss 0.060753755271434784 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9460],\n",
      "        [0.5490]])\n",
      "Iteration 20360 Training loss 0.054007984697818756 Validation loss 0.060782790184020996 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0223],\n",
      "        [0.9120]])\n",
      "Iteration 20370 Training loss 0.05598478764295578 Validation loss 0.06043514609336853 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9947],\n",
      "        [0.1101]])\n",
      "Iteration 20380 Training loss 0.0541960746049881 Validation loss 0.060570258647203445 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.7516],\n",
      "        [0.8466]])\n",
      "Iteration 20390 Training loss 0.05504100024700165 Validation loss 0.06045527011156082 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0677],\n",
      "        [0.9104]])\n",
      "Iteration 20400 Training loss 0.05963515117764473 Validation loss 0.060931600630283356 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8953],\n",
      "        [0.9620]])\n",
      "Iteration 20410 Training loss 0.055583517998456955 Validation loss 0.060691338032484055 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9894],\n",
      "        [0.1519]])\n",
      "Iteration 20420 Training loss 0.05505587160587311 Validation loss 0.06068990379571915 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1439],\n",
      "        [0.7675]])\n",
      "Iteration 20430 Training loss 0.055558960884809494 Validation loss 0.06082933396100998 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8615],\n",
      "        [0.0121]])\n",
      "Iteration 20440 Training loss 0.05649540573358536 Validation loss 0.060833416879177094 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9971],\n",
      "        [0.5515]])\n",
      "Iteration 20450 Training loss 0.05309439077973366 Validation loss 0.06084411218762398 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9925],\n",
      "        [0.9899]])\n",
      "Iteration 20460 Training loss 0.054900504648685455 Validation loss 0.06050112098455429 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1011],\n",
      "        [0.0099]])\n",
      "Iteration 20470 Training loss 0.05488872900605202 Validation loss 0.06056942045688629 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7740],\n",
      "        [0.9922]])\n",
      "Iteration 20480 Training loss 0.05471684783697128 Validation loss 0.060511961579322815 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8106],\n",
      "        [0.5644]])\n",
      "Iteration 20490 Training loss 0.054684046655893326 Validation loss 0.06071124225854874 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2436],\n",
      "        [0.4545]])\n",
      "Iteration 20500 Training loss 0.05776951462030411 Validation loss 0.06085724011063576 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.2172],\n",
      "        [0.4968]])\n",
      "Iteration 20510 Training loss 0.052179303020238876 Validation loss 0.060539986938238144 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.4283],\n",
      "        [0.6825]])\n",
      "Iteration 20520 Training loss 0.05618324130773544 Validation loss 0.06100337207317352 Accuracy 0.828000009059906\n",
      "Output tensor([[0.6950],\n",
      "        [0.7568]])\n",
      "Iteration 20530 Training loss 0.05347566306591034 Validation loss 0.0605802983045578 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9697],\n",
      "        [0.7907]])\n",
      "Iteration 20540 Training loss 0.053936298936605453 Validation loss 0.06060997396707535 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6025],\n",
      "        [0.8839]])\n",
      "Iteration 20550 Training loss 0.05338433384895325 Validation loss 0.060673873871564865 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.2738],\n",
      "        [0.0511]])\n",
      "Iteration 20560 Training loss 0.05069830268621445 Validation loss 0.06097384914755821 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0739],\n",
      "        [0.3316]])\n",
      "Iteration 20570 Training loss 0.05899593234062195 Validation loss 0.060541123151779175 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7771],\n",
      "        [0.8532]])\n",
      "Iteration 20580 Training loss 0.049538109451532364 Validation loss 0.06081633269786835 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0716],\n",
      "        [0.5562]])\n",
      "Iteration 20590 Training loss 0.0552377849817276 Validation loss 0.060471948236227036 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8780],\n",
      "        [0.9050]])\n",
      "Iteration 20600 Training loss 0.05646785721182823 Validation loss 0.060742784291505814 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9887],\n",
      "        [0.7554]])\n",
      "Iteration 20610 Training loss 0.05470729246735573 Validation loss 0.06104949116706848 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.3071],\n",
      "        [0.0279]])\n",
      "Iteration 20620 Training loss 0.055909961462020874 Validation loss 0.06061282008886337 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8441],\n",
      "        [0.7857]])\n",
      "Iteration 20630 Training loss 0.05365642532706261 Validation loss 0.06049925461411476 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.4510],\n",
      "        [0.6460]])\n",
      "Iteration 20640 Training loss 0.056757353246212006 Validation loss 0.0607357993721962 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9343],\n",
      "        [0.3780]])\n",
      "Iteration 20650 Training loss 0.05275058001279831 Validation loss 0.060495492070913315 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.2794],\n",
      "        [0.9841]])\n",
      "Iteration 20660 Training loss 0.05479865521192551 Validation loss 0.06061669439077377 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1526],\n",
      "        [0.6908]])\n",
      "Iteration 20670 Training loss 0.05732884258031845 Validation loss 0.06063058599829674 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7617],\n",
      "        [0.0240]])\n",
      "Iteration 20680 Training loss 0.055006470531225204 Validation loss 0.06086396053433418 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.5044],\n",
      "        [0.9381]])\n",
      "Iteration 20690 Training loss 0.05411430448293686 Validation loss 0.060709770768880844 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0562],\n",
      "        [0.9608]])\n",
      "Iteration 20700 Training loss 0.051386747509241104 Validation loss 0.060649972409009933 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2668],\n",
      "        [0.9875]])\n",
      "Iteration 20710 Training loss 0.05254475399851799 Validation loss 0.060480520129203796 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.3285],\n",
      "        [0.1134]])\n",
      "Iteration 20720 Training loss 0.0571088045835495 Validation loss 0.060677945613861084 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0890],\n",
      "        [0.2335]])\n",
      "Iteration 20730 Training loss 0.054279763251543045 Validation loss 0.06051093339920044 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.6492],\n",
      "        [0.9756]])\n",
      "Iteration 20740 Training loss 0.05481162294745445 Validation loss 0.06042788550257683 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9481],\n",
      "        [0.1513]])\n",
      "Iteration 20750 Training loss 0.05359148234128952 Validation loss 0.06070031598210335 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0424],\n",
      "        [0.1739]])\n",
      "Iteration 20760 Training loss 0.050769515335559845 Validation loss 0.060682378709316254 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0161],\n",
      "        [0.8798]])\n",
      "Iteration 20770 Training loss 0.05391433835029602 Validation loss 0.060560982674360275 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9959],\n",
      "        [0.8116]])\n",
      "Iteration 20780 Training loss 0.05462976545095444 Validation loss 0.06084576994180679 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6441],\n",
      "        [0.9586]])\n",
      "Iteration 20790 Training loss 0.0546407476067543 Validation loss 0.06065960228443146 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0592],\n",
      "        [0.9621]])\n",
      "Iteration 20800 Training loss 0.05537885054945946 Validation loss 0.06121273711323738 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8637],\n",
      "        [0.9788]])\n",
      "Iteration 20810 Training loss 0.05211057513952255 Validation loss 0.06049792468547821 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2167],\n",
      "        [0.7920]])\n",
      "Iteration 20820 Training loss 0.05376177653670311 Validation loss 0.06054452434182167 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.6517],\n",
      "        [0.2303]])\n",
      "Iteration 20830 Training loss 0.053716812282800674 Validation loss 0.060559432953596115 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7357],\n",
      "        [0.9173]])\n",
      "Iteration 20840 Training loss 0.053217820823192596 Validation loss 0.06073745712637901 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9797],\n",
      "        [0.8088]])\n",
      "Iteration 20850 Training loss 0.05587499961256981 Validation loss 0.06036675348877907 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4294],\n",
      "        [0.0545]])\n",
      "Iteration 20860 Training loss 0.053639329969882965 Validation loss 0.06066513806581497 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9506],\n",
      "        [0.2968]])\n",
      "Iteration 20870 Training loss 0.05600130930542946 Validation loss 0.060622308403253555 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0337],\n",
      "        [0.9337]])\n",
      "Iteration 20880 Training loss 0.05553137883543968 Validation loss 0.060561422258615494 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1269],\n",
      "        [0.0886]])\n",
      "Iteration 20890 Training loss 0.05407910421490669 Validation loss 0.060479819774627686 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5845],\n",
      "        [0.5514]])\n",
      "Iteration 20900 Training loss 0.05528908595442772 Validation loss 0.06063903868198395 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1824],\n",
      "        [0.2697]])\n",
      "Iteration 20910 Training loss 0.05613749101758003 Validation loss 0.06051313132047653 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.3598],\n",
      "        [0.3954]])\n",
      "Iteration 20920 Training loss 0.05226439982652664 Validation loss 0.060690537095069885 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8028],\n",
      "        [0.6554]])\n",
      "Iteration 20930 Training loss 0.056310586631298065 Validation loss 0.060417380183935165 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9945],\n",
      "        [0.0670]])\n",
      "Iteration 20940 Training loss 0.056822746992111206 Validation loss 0.060525085777044296 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.3138],\n",
      "        [0.8785]])\n",
      "Iteration 20950 Training loss 0.054012082517147064 Validation loss 0.06068382412195206 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8998],\n",
      "        [0.3417]])\n",
      "Iteration 20960 Training loss 0.05692862719297409 Validation loss 0.06061294674873352 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3197],\n",
      "        [0.9345]])\n",
      "Iteration 20970 Training loss 0.0529307946562767 Validation loss 0.06052171811461449 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8968],\n",
      "        [0.9442]])\n",
      "Iteration 20980 Training loss 0.058716319501399994 Validation loss 0.060609590262174606 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.6066],\n",
      "        [0.9912]])\n",
      "Iteration 20990 Training loss 0.0536520779132843 Validation loss 0.060636766254901886 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0128],\n",
      "        [0.1130]])\n",
      "Iteration 21000 Training loss 0.05465177446603775 Validation loss 0.06056145578622818 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4476],\n",
      "        [0.2104]])\n",
      "Iteration 21010 Training loss 0.055150553584098816 Validation loss 0.06096982583403587 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0352],\n",
      "        [0.7106]])\n",
      "Iteration 21020 Training loss 0.05476586893200874 Validation loss 0.06093940883874893 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6822],\n",
      "        [0.1779]])\n",
      "Iteration 21030 Training loss 0.054790616035461426 Validation loss 0.06050991639494896 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0219],\n",
      "        [0.2158]])\n",
      "Iteration 21040 Training loss 0.05602099001407623 Validation loss 0.061067186295986176 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8738],\n",
      "        [0.7638]])\n",
      "Iteration 21050 Training loss 0.05624241754412651 Validation loss 0.06047350913286209 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.4230],\n",
      "        [0.8314]])\n",
      "Iteration 21060 Training loss 0.05220647528767586 Validation loss 0.0604107640683651 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2840],\n",
      "        [0.6500]])\n",
      "Iteration 21070 Training loss 0.05413627251982689 Validation loss 0.06057518348097801 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1195],\n",
      "        [0.8093]])\n",
      "Iteration 21080 Training loss 0.056866638362407684 Validation loss 0.06073450297117233 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0124],\n",
      "        [0.5773]])\n",
      "Iteration 21090 Training loss 0.05274343863129616 Validation loss 0.06064585968852043 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0371],\n",
      "        [0.9877]])\n",
      "Iteration 21100 Training loss 0.055593956261873245 Validation loss 0.060591377317905426 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2271],\n",
      "        [0.9512]])\n",
      "Iteration 21110 Training loss 0.054871849715709686 Validation loss 0.06075408309698105 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9159],\n",
      "        [0.0828]])\n",
      "Iteration 21120 Training loss 0.05721987783908844 Validation loss 0.060695067048072815 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7303],\n",
      "        [0.6909]])\n",
      "Iteration 21130 Training loss 0.05473428592085838 Validation loss 0.061242230236530304 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0038],\n",
      "        [0.8634]])\n",
      "Iteration 21140 Training loss 0.05362924188375473 Validation loss 0.0606134869158268 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7990],\n",
      "        [0.1697]])\n",
      "Iteration 21150 Training loss 0.052026644349098206 Validation loss 0.0609162338078022 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0862],\n",
      "        [0.3102]])\n",
      "Iteration 21160 Training loss 0.05615921691060066 Validation loss 0.06042061001062393 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8677],\n",
      "        [0.0072]])\n",
      "Iteration 21170 Training loss 0.05171603336930275 Validation loss 0.060541603714227676 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.5821],\n",
      "        [0.1959]])\n",
      "Iteration 21180 Training loss 0.05342089757323265 Validation loss 0.06048410013318062 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6225],\n",
      "        [0.9575]])\n",
      "Iteration 21190 Training loss 0.051184188574552536 Validation loss 0.06061475723981857 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3836],\n",
      "        [0.3733]])\n",
      "Iteration 21200 Training loss 0.05543462932109833 Validation loss 0.06089198216795921 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9947],\n",
      "        [0.8885]])\n",
      "Iteration 21210 Training loss 0.05178244411945343 Validation loss 0.06066349893808365 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0742],\n",
      "        [0.1643]])\n",
      "Iteration 21220 Training loss 0.056531403213739395 Validation loss 0.06072213873267174 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2237],\n",
      "        [0.7605]])\n",
      "Iteration 21230 Training loss 0.056789420545101166 Validation loss 0.06090948358178139 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6642],\n",
      "        [0.9827]])\n",
      "Iteration 21240 Training loss 0.05101069062948227 Validation loss 0.06042845919728279 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8769],\n",
      "        [0.8291]])\n",
      "Iteration 21250 Training loss 0.054166533052921295 Validation loss 0.0604834221303463 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3186],\n",
      "        [0.9248]])\n",
      "Iteration 21260 Training loss 0.0530797503888607 Validation loss 0.06041667237877846 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.4724],\n",
      "        [0.3781]])\n",
      "Iteration 21270 Training loss 0.05210486426949501 Validation loss 0.06056470423936844 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5063],\n",
      "        [0.9564]])\n",
      "Iteration 21280 Training loss 0.0562952421605587 Validation loss 0.06049010157585144 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0785],\n",
      "        [0.0150]])\n",
      "Iteration 21290 Training loss 0.05303981527686119 Validation loss 0.06044168025255203 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.2445],\n",
      "        [0.0510]])\n",
      "Iteration 21300 Training loss 0.0555083267390728 Validation loss 0.060413211584091187 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9731],\n",
      "        [0.8454]])\n",
      "Iteration 21310 Training loss 0.053271785378456116 Validation loss 0.06059374660253525 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2455],\n",
      "        [0.8868]])\n",
      "Iteration 21320 Training loss 0.05405624210834503 Validation loss 0.06059340015053749 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.7886],\n",
      "        [0.7489]])\n",
      "Iteration 21330 Training loss 0.05292430892586708 Validation loss 0.06047584488987923 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0515],\n",
      "        [0.2889]])\n",
      "Iteration 21340 Training loss 0.056532736867666245 Validation loss 0.060497626662254333 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.4881],\n",
      "        [0.4524]])\n",
      "Iteration 21350 Training loss 0.05472920462489128 Validation loss 0.06073484569787979 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9040],\n",
      "        [0.8401]])\n",
      "Iteration 21360 Training loss 0.05151909217238426 Validation loss 0.06081673502922058 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1221],\n",
      "        [0.5051]])\n",
      "Iteration 21370 Training loss 0.05590219795703888 Validation loss 0.060768790543079376 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1631],\n",
      "        [0.1118]])\n",
      "Iteration 21380 Training loss 0.05552142113447189 Validation loss 0.06063871830701828 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8436],\n",
      "        [0.8395]])\n",
      "Iteration 21390 Training loss 0.05831993743777275 Validation loss 0.06040925160050392 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0870],\n",
      "        [0.6685]])\n",
      "Iteration 21400 Training loss 0.056026641279459 Validation loss 0.06045963987708092 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.3307],\n",
      "        [0.7237]])\n",
      "Iteration 21410 Training loss 0.05526360869407654 Validation loss 0.06042538583278656 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4288],\n",
      "        [0.9994]])\n",
      "Iteration 21420 Training loss 0.052156202495098114 Validation loss 0.060490213334560394 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0480],\n",
      "        [0.4361]])\n",
      "Iteration 21430 Training loss 0.056556183844804764 Validation loss 0.060476720333099365 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.7543],\n",
      "        [0.2470]])\n",
      "Iteration 21440 Training loss 0.05583546310663223 Validation loss 0.06059367209672928 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5269],\n",
      "        [0.3054]])\n",
      "Iteration 21450 Training loss 0.05147363990545273 Validation loss 0.061029769480228424 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.2093],\n",
      "        [0.9941]])\n",
      "Iteration 21460 Training loss 0.05798419192433357 Validation loss 0.060550887137651443 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9990],\n",
      "        [0.0830]])\n",
      "Iteration 21470 Training loss 0.05687306821346283 Validation loss 0.060552991926670074 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8923],\n",
      "        [0.5708]])\n",
      "Iteration 21480 Training loss 0.05746475234627724 Validation loss 0.06039772555232048 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0887],\n",
      "        [0.3067]])\n",
      "Iteration 21490 Training loss 0.05039218068122864 Validation loss 0.06067506968975067 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0262],\n",
      "        [0.0832]])\n",
      "Iteration 21500 Training loss 0.052059490233659744 Validation loss 0.06047472730278969 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6714],\n",
      "        [0.9153]])\n",
      "Iteration 21510 Training loss 0.05712476372718811 Validation loss 0.06114375218749046 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8495],\n",
      "        [0.1860]])\n",
      "Iteration 21520 Training loss 0.05349550023674965 Validation loss 0.06074898689985275 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9128],\n",
      "        [0.9313]])\n",
      "Iteration 21530 Training loss 0.05345717817544937 Validation loss 0.060453735291957855 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7069],\n",
      "        [0.2185]])\n",
      "Iteration 21540 Training loss 0.05363776534795761 Validation loss 0.060871247202157974 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0081],\n",
      "        [0.4554]])\n",
      "Iteration 21550 Training loss 0.057340819388628006 Validation loss 0.06052897870540619 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.2128],\n",
      "        [0.1610]])\n",
      "Iteration 21560 Training loss 0.05105815455317497 Validation loss 0.060411497950553894 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2876],\n",
      "        [0.8554]])\n",
      "Iteration 21570 Training loss 0.05422469973564148 Validation loss 0.060518357902765274 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0243],\n",
      "        [0.0819]])\n",
      "Iteration 21580 Training loss 0.054105982184410095 Validation loss 0.060445722192525864 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0431],\n",
      "        [0.1535]])\n",
      "Iteration 21590 Training loss 0.05233940854668617 Validation loss 0.06041192635893822 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6271],\n",
      "        [0.0318]])\n",
      "Iteration 21600 Training loss 0.05365392193198204 Validation loss 0.06054162606596947 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8633],\n",
      "        [0.8683]])\n",
      "Iteration 21610 Training loss 0.05686059221625328 Validation loss 0.06088751181960106 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0549],\n",
      "        [0.8580]])\n",
      "Iteration 21620 Training loss 0.054327234625816345 Validation loss 0.06056826934218407 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3126],\n",
      "        [0.0181]])\n",
      "Iteration 21630 Training loss 0.05191224440932274 Validation loss 0.06068241223692894 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9468],\n",
      "        [0.1855]])\n",
      "Iteration 21640 Training loss 0.05519794672727585 Validation loss 0.06103256344795227 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0354],\n",
      "        [0.6215]])\n",
      "Iteration 21650 Training loss 0.054009705781936646 Validation loss 0.06048298627138138 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9526],\n",
      "        [0.5452]])\n",
      "Iteration 21660 Training loss 0.05284767970442772 Validation loss 0.06040626019239426 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0114],\n",
      "        [0.8260]])\n",
      "Iteration 21670 Training loss 0.053379349410533905 Validation loss 0.060354139655828476 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0598],\n",
      "        [0.1316]])\n",
      "Iteration 21680 Training loss 0.05746432766318321 Validation loss 0.06047302111983299 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8107],\n",
      "        [0.5297]])\n",
      "Iteration 21690 Training loss 0.05293707922101021 Validation loss 0.06043726205825806 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9774],\n",
      "        [0.2187]])\n",
      "Iteration 21700 Training loss 0.05735700950026512 Validation loss 0.06089197099208832 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9998],\n",
      "        [0.7127]])\n",
      "Iteration 21710 Training loss 0.06009175628423691 Validation loss 0.06048255041241646 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2625],\n",
      "        [0.0632]])\n",
      "Iteration 21720 Training loss 0.05456632748246193 Validation loss 0.060713037848472595 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7548],\n",
      "        [0.2589]])\n",
      "Iteration 21730 Training loss 0.05380808189511299 Validation loss 0.06055956333875656 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9879],\n",
      "        [0.1165]])\n",
      "Iteration 21740 Training loss 0.05368245393037796 Validation loss 0.06037464365363121 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9992],\n",
      "        [0.4486]])\n",
      "Iteration 21750 Training loss 0.057302892208099365 Validation loss 0.06101103499531746 Accuracy 0.828000009059906\n",
      "Output tensor([[0.7378],\n",
      "        [0.4992]])\n",
      "Iteration 21760 Training loss 0.056239645928144455 Validation loss 0.060493502765893936 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.6947],\n",
      "        [0.0039]])\n",
      "Iteration 21770 Training loss 0.05338268727064133 Validation loss 0.06035276874899864 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0063],\n",
      "        [0.8927]])\n",
      "Iteration 21780 Training loss 0.05391986668109894 Validation loss 0.06090807542204857 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1181],\n",
      "        [0.0114]])\n",
      "Iteration 21790 Training loss 0.054664116352796555 Validation loss 0.06051896885037422 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0566],\n",
      "        [0.7324]])\n",
      "Iteration 21800 Training loss 0.053978193551301956 Validation loss 0.06043475121259689 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5009],\n",
      "        [0.7877]])\n",
      "Iteration 21810 Training loss 0.052597932517528534 Validation loss 0.060712989419698715 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0313],\n",
      "        [0.0283]])\n",
      "Iteration 21820 Training loss 0.054497912526130676 Validation loss 0.06041998043656349 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0552],\n",
      "        [0.0656]])\n",
      "Iteration 21830 Training loss 0.052343759685754776 Validation loss 0.060868117958307266 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0124],\n",
      "        [0.5652]])\n",
      "Iteration 21840 Training loss 0.05535778030753136 Validation loss 0.060797255486249924 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3101],\n",
      "        [0.5386]])\n",
      "Iteration 21850 Training loss 0.05329058691859245 Validation loss 0.060469284653663635 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3133],\n",
      "        [0.0300]])\n",
      "Iteration 21860 Training loss 0.05283794552087784 Validation loss 0.06038929894566536 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9878],\n",
      "        [0.1344]])\n",
      "Iteration 21870 Training loss 0.05450856313109398 Validation loss 0.06037372350692749 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9504],\n",
      "        [0.0137]])\n",
      "Iteration 21880 Training loss 0.05436558276414871 Validation loss 0.060596611350774765 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9929],\n",
      "        [0.0306]])\n",
      "Iteration 21890 Training loss 0.05472484603524208 Validation loss 0.060444265604019165 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0247],\n",
      "        [0.4950]])\n",
      "Iteration 21900 Training loss 0.054336708039045334 Validation loss 0.060621604323387146 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0157],\n",
      "        [0.7368]])\n",
      "Iteration 21910 Training loss 0.053542256355285645 Validation loss 0.06070568785071373 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.3844],\n",
      "        [0.0729]])\n",
      "Iteration 21920 Training loss 0.05364038050174713 Validation loss 0.060414593666791916 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0030],\n",
      "        [0.6542]])\n",
      "Iteration 21930 Training loss 0.05334436148405075 Validation loss 0.06070493161678314 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2790],\n",
      "        [0.2030]])\n",
      "Iteration 21940 Training loss 0.05519964545965195 Validation loss 0.06043709069490433 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5534],\n",
      "        [0.9747]])\n",
      "Iteration 21950 Training loss 0.053246449679136276 Validation loss 0.060459770262241364 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9334],\n",
      "        [0.8102]])\n",
      "Iteration 21960 Training loss 0.053680211305618286 Validation loss 0.061060674488544464 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.5993],\n",
      "        [0.9419]])\n",
      "Iteration 21970 Training loss 0.05301472917199135 Validation loss 0.06053266301751137 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8626],\n",
      "        [0.1394]])\n",
      "Iteration 21980 Training loss 0.05191541463136673 Validation loss 0.060375504195690155 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0075],\n",
      "        [0.2758]])\n",
      "Iteration 21990 Training loss 0.05259647220373154 Validation loss 0.06049704924225807 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0531],\n",
      "        [0.8091]])\n",
      "Iteration 22000 Training loss 0.06021444872021675 Validation loss 0.06039141118526459 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8288],\n",
      "        [0.9997]])\n",
      "Iteration 22010 Training loss 0.055819690227508545 Validation loss 0.06045554578304291 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1466],\n",
      "        [0.1731]])\n",
      "Iteration 22020 Training loss 0.054969362914562225 Validation loss 0.060736075043678284 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9052],\n",
      "        [0.6875]])\n",
      "Iteration 22030 Training loss 0.05407053977251053 Validation loss 0.06046191602945328 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.2843],\n",
      "        [0.6757]])\n",
      "Iteration 22040 Training loss 0.056171439588069916 Validation loss 0.06054900959134102 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0510],\n",
      "        [0.5496]])\n",
      "Iteration 22050 Training loss 0.05453076586127281 Validation loss 0.06045518442988396 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0826],\n",
      "        [0.9996]])\n",
      "Iteration 22060 Training loss 0.05413895472884178 Validation loss 0.06046935170888901 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1651],\n",
      "        [0.9726]])\n",
      "Iteration 22070 Training loss 0.05445254594087601 Validation loss 0.06069394573569298 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0194],\n",
      "        [0.9371]])\n",
      "Iteration 22080 Training loss 0.05517211928963661 Validation loss 0.061023008078336716 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1185],\n",
      "        [0.2596]])\n",
      "Iteration 22090 Training loss 0.05531580373644829 Validation loss 0.060320090502500534 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9370],\n",
      "        [0.9797]])\n",
      "Iteration 22100 Training loss 0.05365510657429695 Validation loss 0.0605476014316082 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0347],\n",
      "        [0.2108]])\n",
      "Iteration 22110 Training loss 0.05742223188281059 Validation loss 0.060704462230205536 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9992],\n",
      "        [0.5231]])\n",
      "Iteration 22120 Training loss 0.05397215858101845 Validation loss 0.06038868427276611 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9662],\n",
      "        [0.0882]])\n",
      "Iteration 22130 Training loss 0.05489829182624817 Validation loss 0.060337018221616745 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.7714],\n",
      "        [0.7526]])\n",
      "Iteration 22140 Training loss 0.0514841303229332 Validation loss 0.060729607939720154 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3914],\n",
      "        [0.9833]])\n",
      "Iteration 22150 Training loss 0.05127618461847305 Validation loss 0.060443535447120667 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0451],\n",
      "        [0.0790]])\n",
      "Iteration 22160 Training loss 0.053760696202516556 Validation loss 0.060363397002220154 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1298],\n",
      "        [0.6412]])\n",
      "Iteration 22170 Training loss 0.05458946153521538 Validation loss 0.060709111392498016 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1287],\n",
      "        [0.0043]])\n",
      "Iteration 22180 Training loss 0.05644163116812706 Validation loss 0.06037081778049469 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2463],\n",
      "        [0.2443]])\n",
      "Iteration 22190 Training loss 0.052719611674547195 Validation loss 0.06036711111664772 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9454],\n",
      "        [0.1421]])\n",
      "Iteration 22200 Training loss 0.051594771444797516 Validation loss 0.06073013320565224 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8031],\n",
      "        [0.5649]])\n",
      "Iteration 22210 Training loss 0.05683279410004616 Validation loss 0.06044205278158188 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9786],\n",
      "        [0.7837]])\n",
      "Iteration 22220 Training loss 0.0561964176595211 Validation loss 0.060330070555210114 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.4558],\n",
      "        [0.9637]])\n",
      "Iteration 22230 Training loss 0.05546795576810837 Validation loss 0.06056239455938339 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8732],\n",
      "        [0.0015]])\n",
      "Iteration 22240 Training loss 0.05622538924217224 Validation loss 0.060593098402023315 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8523],\n",
      "        [0.1439]])\n",
      "Iteration 22250 Training loss 0.054609861224889755 Validation loss 0.060386404395103455 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0666],\n",
      "        [0.1104]])\n",
      "Iteration 22260 Training loss 0.05439894646406174 Validation loss 0.06052352860569954 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0383],\n",
      "        [0.1119]])\n",
      "Iteration 22270 Training loss 0.05338750779628754 Validation loss 0.06045647710561752 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8964],\n",
      "        [0.9227]])\n",
      "Iteration 22280 Training loss 0.05500106140971184 Validation loss 0.06049780920147896 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0499],\n",
      "        [0.6313]])\n",
      "Iteration 22290 Training loss 0.05568617582321167 Validation loss 0.06040721759200096 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0899],\n",
      "        [0.0090]])\n",
      "Iteration 22300 Training loss 0.05179697647690773 Validation loss 0.060344211757183075 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9953],\n",
      "        [0.3645]])\n",
      "Iteration 22310 Training loss 0.05649629607796669 Validation loss 0.060442328453063965 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0616],\n",
      "        [0.9380]])\n",
      "Iteration 22320 Training loss 0.052594829350709915 Validation loss 0.06067639961838722 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3607],\n",
      "        [0.5388]])\n",
      "Iteration 22330 Training loss 0.05505751445889473 Validation loss 0.0602811723947525 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1090],\n",
      "        [0.8787]])\n",
      "Iteration 22340 Training loss 0.05686820670962334 Validation loss 0.06054767593741417 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9918],\n",
      "        [0.5626]])\n",
      "Iteration 22350 Training loss 0.05157604068517685 Validation loss 0.06035929545760155 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6719],\n",
      "        [0.9516]])\n",
      "Iteration 22360 Training loss 0.05309465900063515 Validation loss 0.06057848781347275 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9989],\n",
      "        [0.9972]])\n",
      "Iteration 22370 Training loss 0.05556878820061684 Validation loss 0.060401178896427155 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.5175],\n",
      "        [0.3397]])\n",
      "Iteration 22380 Training loss 0.05084817856550217 Validation loss 0.06031951308250427 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8718],\n",
      "        [0.6847]])\n",
      "Iteration 22390 Training loss 0.051338691264390945 Validation loss 0.06068125367164612 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0122],\n",
      "        [0.7207]])\n",
      "Iteration 22400 Training loss 0.056701842695474625 Validation loss 0.06069951131939888 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5688],\n",
      "        [0.8013]])\n",
      "Iteration 22410 Training loss 0.055316656827926636 Validation loss 0.06056147813796997 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6580],\n",
      "        [0.8674]])\n",
      "Iteration 22420 Training loss 0.055455807596445084 Validation loss 0.060536205768585205 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.9980],\n",
      "        [0.9646]])\n",
      "Iteration 22430 Training loss 0.05972940847277641 Validation loss 0.06076669320464134 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9846],\n",
      "        [0.9960]])\n",
      "Iteration 22440 Training loss 0.05621666833758354 Validation loss 0.0604047030210495 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1326],\n",
      "        [0.2476]])\n",
      "Iteration 22450 Training loss 0.05355047062039375 Validation loss 0.060513466596603394 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0591],\n",
      "        [0.0110]])\n",
      "Iteration 22460 Training loss 0.04848741367459297 Validation loss 0.06071978062391281 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0321],\n",
      "        [0.6418]])\n",
      "Iteration 22470 Training loss 0.055032432079315186 Validation loss 0.06037339195609093 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.6063],\n",
      "        [0.4737]])\n",
      "Iteration 22480 Training loss 0.053285110741853714 Validation loss 0.06051691621541977 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8584],\n",
      "        [0.9950]])\n",
      "Iteration 22490 Training loss 0.05342971906065941 Validation loss 0.06047709658741951 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1211],\n",
      "        [0.5600]])\n",
      "Iteration 22500 Training loss 0.05332611873745918 Validation loss 0.060312483459711075 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7273],\n",
      "        [0.8837]])\n",
      "Iteration 22510 Training loss 0.05339859426021576 Validation loss 0.060469336807727814 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4940],\n",
      "        [0.2338]])\n",
      "Iteration 22520 Training loss 0.05429496243596077 Validation loss 0.060372814536094666 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1900],\n",
      "        [0.8458]])\n",
      "Iteration 22530 Training loss 0.05357089638710022 Validation loss 0.06037953123450279 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0026],\n",
      "        [0.0255]])\n",
      "Iteration 22540 Training loss 0.05382572114467621 Validation loss 0.06027710437774658 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4350],\n",
      "        [0.9386]])\n",
      "Iteration 22550 Training loss 0.05328916758298874 Validation loss 0.06027904525399208 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7051],\n",
      "        [0.9497]])\n",
      "Iteration 22560 Training loss 0.05582518130540848 Validation loss 0.060306988656520844 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9489],\n",
      "        [0.2131]])\n",
      "Iteration 22570 Training loss 0.0570187084376812 Validation loss 0.060447271913290024 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1288],\n",
      "        [0.8034]])\n",
      "Iteration 22580 Training loss 0.04956010356545448 Validation loss 0.06043868139386177 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8983],\n",
      "        [0.9971]])\n",
      "Iteration 22590 Training loss 0.054717548191547394 Validation loss 0.06041724234819412 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5752],\n",
      "        [0.8362]])\n",
      "Iteration 22600 Training loss 0.05161629989743233 Validation loss 0.060551855713129044 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0309],\n",
      "        [0.1018]])\n",
      "Iteration 22610 Training loss 0.05281955376267433 Validation loss 0.06066038832068443 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8677],\n",
      "        [0.0077]])\n",
      "Iteration 22620 Training loss 0.0484161302447319 Validation loss 0.06028051674365997 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9008],\n",
      "        [0.7611]])\n",
      "Iteration 22630 Training loss 0.05431545525789261 Validation loss 0.06053442135453224 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7479],\n",
      "        [0.8778]])\n",
      "Iteration 22640 Training loss 0.053541701287031174 Validation loss 0.06053604930639267 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4965],\n",
      "        [0.8801]])\n",
      "Iteration 22650 Training loss 0.054385535418987274 Validation loss 0.061027560383081436 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8062],\n",
      "        [0.6866]])\n",
      "Iteration 22660 Training loss 0.0519014410674572 Validation loss 0.060346122831106186 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0864],\n",
      "        [0.7648]])\n",
      "Iteration 22670 Training loss 0.05454637110233307 Validation loss 0.0603841170668602 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1724],\n",
      "        [0.0164]])\n",
      "Iteration 22680 Training loss 0.05016767978668213 Validation loss 0.0605388842523098 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6132],\n",
      "        [0.3100]])\n",
      "Iteration 22690 Training loss 0.053628355264663696 Validation loss 0.06073214113712311 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4677],\n",
      "        [0.0413]])\n",
      "Iteration 22700 Training loss 0.053359054028987885 Validation loss 0.060507047921419144 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1795],\n",
      "        [0.9811]])\n",
      "Iteration 22710 Training loss 0.049734290689229965 Validation loss 0.060519300401210785 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0135],\n",
      "        [0.5515]])\n",
      "Iteration 22720 Training loss 0.050707440823316574 Validation loss 0.06033940985798836 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8001],\n",
      "        [0.9952]])\n",
      "Iteration 22730 Training loss 0.05175520107150078 Validation loss 0.06053658947348595 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0147],\n",
      "        [0.9123]])\n",
      "Iteration 22740 Training loss 0.05150055140256882 Validation loss 0.06105246767401695 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8505],\n",
      "        [0.8565]])\n",
      "Iteration 22750 Training loss 0.05445679649710655 Validation loss 0.060276176780462265 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5433],\n",
      "        [0.6541]])\n",
      "Iteration 22760 Training loss 0.049686308950185776 Validation loss 0.06076037883758545 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7948],\n",
      "        [0.7114]])\n",
      "Iteration 22770 Training loss 0.05271575227379799 Validation loss 0.06046430021524429 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.3465],\n",
      "        [0.9941]])\n",
      "Iteration 22780 Training loss 0.05344870686531067 Validation loss 0.06038558483123779 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0085],\n",
      "        [0.9907]])\n",
      "Iteration 22790 Training loss 0.05477244779467583 Validation loss 0.060789383947849274 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7418],\n",
      "        [0.9894]])\n",
      "Iteration 22800 Training loss 0.052235737442970276 Validation loss 0.06035931408405304 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1376],\n",
      "        [0.3306]])\n",
      "Iteration 22810 Training loss 0.057391975075006485 Validation loss 0.06045714020729065 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0154],\n",
      "        [0.0049]])\n",
      "Iteration 22820 Training loss 0.05283372476696968 Validation loss 0.06041717901825905 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8768],\n",
      "        [0.0201]])\n",
      "Iteration 22830 Training loss 0.05307970941066742 Validation loss 0.06028139591217041 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8824],\n",
      "        [0.9831]])\n",
      "Iteration 22840 Training loss 0.05436601862311363 Validation loss 0.060383521020412445 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9807],\n",
      "        [0.4495]])\n",
      "Iteration 22850 Training loss 0.05637681856751442 Validation loss 0.06036945804953575 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0681],\n",
      "        [0.8012]])\n",
      "Iteration 22860 Training loss 0.05501341074705124 Validation loss 0.06084320694208145 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1333],\n",
      "        [0.2658]])\n",
      "Iteration 22870 Training loss 0.05577568709850311 Validation loss 0.060240358114242554 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8907],\n",
      "        [0.3299]])\n",
      "Iteration 22880 Training loss 0.053962577134370804 Validation loss 0.06084474176168442 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2548],\n",
      "        [0.0796]])\n",
      "Iteration 22890 Training loss 0.052918531000614166 Validation loss 0.06036403030157089 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0657],\n",
      "        [0.7227]])\n",
      "Iteration 22900 Training loss 0.053530290722846985 Validation loss 0.06065772846341133 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0522],\n",
      "        [0.1239]])\n",
      "Iteration 22910 Training loss 0.05301051214337349 Validation loss 0.060582030564546585 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1292],\n",
      "        [0.6114]])\n",
      "Iteration 22920 Training loss 0.04863350838422775 Validation loss 0.060773815959692 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.2167],\n",
      "        [0.8363]])\n",
      "Iteration 22930 Training loss 0.053223587572574615 Validation loss 0.06076207756996155 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.5426],\n",
      "        [0.8521]])\n",
      "Iteration 22940 Training loss 0.05355976149439812 Validation loss 0.06039457023143768 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6305],\n",
      "        [0.6511]])\n",
      "Iteration 22950 Training loss 0.053824037313461304 Validation loss 0.06038939207792282 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1433],\n",
      "        [0.9799]])\n",
      "Iteration 22960 Training loss 0.05243457481265068 Validation loss 0.060562506318092346 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1299],\n",
      "        [0.3370]])\n",
      "Iteration 22970 Training loss 0.05436605215072632 Validation loss 0.06046315282583237 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5385],\n",
      "        [0.0164]])\n",
      "Iteration 22980 Training loss 0.05530781298875809 Validation loss 0.06030138581991196 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1057],\n",
      "        [0.6337]])\n",
      "Iteration 22990 Training loss 0.05269801244139671 Validation loss 0.06097076088190079 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9880],\n",
      "        [0.3378]])\n",
      "Iteration 23000 Training loss 0.049548327922821045 Validation loss 0.0602998286485672 Accuracy 0.8295000195503235\n",
      "Output tensor([[1.0000],\n",
      "        [0.7640]])\n",
      "Iteration 23010 Training loss 0.05015198513865471 Validation loss 0.060710061341524124 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9589],\n",
      "        [0.8639]])\n",
      "Iteration 23020 Training loss 0.052911169826984406 Validation loss 0.06047499552369118 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4687],\n",
      "        [0.0643]])\n",
      "Iteration 23030 Training loss 0.0557854026556015 Validation loss 0.06028911471366882 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3874],\n",
      "        [0.9823]])\n",
      "Iteration 23040 Training loss 0.05604328587651253 Validation loss 0.06099412962794304 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.6966],\n",
      "        [0.5002]])\n",
      "Iteration 23050 Training loss 0.05486304685473442 Validation loss 0.060300957411527634 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0882],\n",
      "        [0.5427]])\n",
      "Iteration 23060 Training loss 0.053249526768922806 Validation loss 0.060407042503356934 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.4156],\n",
      "        [0.3159]])\n",
      "Iteration 23070 Training loss 0.05126114934682846 Validation loss 0.060632575303316116 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0347],\n",
      "        [0.9742]])\n",
      "Iteration 23080 Training loss 0.055675994604825974 Validation loss 0.06036335602402687 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.6321],\n",
      "        [0.0121]])\n",
      "Iteration 23090 Training loss 0.05200909078121185 Validation loss 0.06023295596241951 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6283],\n",
      "        [0.6782]])\n",
      "Iteration 23100 Training loss 0.05369425565004349 Validation loss 0.06050204485654831 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9975],\n",
      "        [0.9920]])\n",
      "Iteration 23110 Training loss 0.056400127708911896 Validation loss 0.06030622497200966 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.2870],\n",
      "        [0.8639]])\n",
      "Iteration 23120 Training loss 0.05128766968846321 Validation loss 0.060256343334913254 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2992],\n",
      "        [0.4313]])\n",
      "Iteration 23130 Training loss 0.05565590783953667 Validation loss 0.060486502945423126 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9500],\n",
      "        [0.0288]])\n",
      "Iteration 23140 Training loss 0.054207153618335724 Validation loss 0.06028738245368004 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9029],\n",
      "        [0.1578]])\n",
      "Iteration 23150 Training loss 0.05628413334488869 Validation loss 0.0605536550283432 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0136],\n",
      "        [0.4332]])\n",
      "Iteration 23160 Training loss 0.05539421737194061 Validation loss 0.06032721698284149 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2205],\n",
      "        [0.5700]])\n",
      "Iteration 23170 Training loss 0.05320717766880989 Validation loss 0.06039584428071976 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0895],\n",
      "        [0.9724]])\n",
      "Iteration 23180 Training loss 0.052562590688467026 Validation loss 0.0602605864405632 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4323],\n",
      "        [0.0624]])\n",
      "Iteration 23190 Training loss 0.0529075562953949 Validation loss 0.060244668275117874 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3797],\n",
      "        [0.0629]])\n",
      "Iteration 23200 Training loss 0.0529882051050663 Validation loss 0.06037575379014015 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5892],\n",
      "        [0.0812]])\n",
      "Iteration 23210 Training loss 0.053408149629831314 Validation loss 0.060356222093105316 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9972],\n",
      "        [0.9991]])\n",
      "Iteration 23220 Training loss 0.05202367156744003 Validation loss 0.06044942885637283 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8096],\n",
      "        [0.3392]])\n",
      "Iteration 23230 Training loss 0.05268312245607376 Validation loss 0.06021717190742493 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9916],\n",
      "        [0.1714]])\n",
      "Iteration 23240 Training loss 0.0522141195833683 Validation loss 0.060254182666540146 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0261],\n",
      "        [0.9924]])\n",
      "Iteration 23250 Training loss 0.05121491104364395 Validation loss 0.06046881154179573 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.6102],\n",
      "        [0.2188]])\n",
      "Iteration 23260 Training loss 0.052159324288368225 Validation loss 0.06044155731797218 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0378],\n",
      "        [0.0282]])\n",
      "Iteration 23270 Training loss 0.05046439543366432 Validation loss 0.060394417494535446 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2325],\n",
      "        [0.0191]])\n",
      "Iteration 23280 Training loss 0.05491506680846214 Validation loss 0.06052682548761368 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.5200],\n",
      "        [0.9999]])\n",
      "Iteration 23290 Training loss 0.052467234432697296 Validation loss 0.060335706919431686 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0078],\n",
      "        [0.0083]])\n",
      "Iteration 23300 Training loss 0.05437718331813812 Validation loss 0.06035866588354111 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9983],\n",
      "        [0.8210]])\n",
      "Iteration 23310 Training loss 0.05368385463953018 Validation loss 0.060331955552101135 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8657],\n",
      "        [0.4395]])\n",
      "Iteration 23320 Training loss 0.0547783263027668 Validation loss 0.06060159206390381 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7981],\n",
      "        [0.2153]])\n",
      "Iteration 23330 Training loss 0.048987388610839844 Validation loss 0.060547009110450745 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4510],\n",
      "        [0.6105]])\n",
      "Iteration 23340 Training loss 0.05260385200381279 Validation loss 0.060487933456897736 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8227],\n",
      "        [0.4225]])\n",
      "Iteration 23350 Training loss 0.05289312079548836 Validation loss 0.06036726012825966 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0696],\n",
      "        [0.9974]])\n",
      "Iteration 23360 Training loss 0.05574754253029823 Validation loss 0.06035546958446503 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8643],\n",
      "        [0.9838]])\n",
      "Iteration 23370 Training loss 0.054134950041770935 Validation loss 0.060381241142749786 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0521],\n",
      "        [0.7443]])\n",
      "Iteration 23380 Training loss 0.051447369158267975 Validation loss 0.06024404987692833 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.6883],\n",
      "        [0.1520]])\n",
      "Iteration 23390 Training loss 0.05405297130346298 Validation loss 0.060360196977853775 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4756],\n",
      "        [0.9761]])\n",
      "Iteration 23400 Training loss 0.05101865530014038 Validation loss 0.06055936589837074 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6611],\n",
      "        [0.2973]])\n",
      "Iteration 23410 Training loss 0.05321968346834183 Validation loss 0.060221411287784576 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9765],\n",
      "        [0.2810]])\n",
      "Iteration 23420 Training loss 0.05099155381321907 Validation loss 0.060370758175849915 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7204],\n",
      "        [0.7107]])\n",
      "Iteration 23430 Training loss 0.05384961515665054 Validation loss 0.0601806715130806 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9434],\n",
      "        [0.4838]])\n",
      "Iteration 23440 Training loss 0.054148271679878235 Validation loss 0.06026865541934967 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0257],\n",
      "        [0.1069]])\n",
      "Iteration 23450 Training loss 0.0544896237552166 Validation loss 0.060240715742111206 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1349],\n",
      "        [0.9999]])\n",
      "Iteration 23460 Training loss 0.05541316419839859 Validation loss 0.06034174561500549 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1760],\n",
      "        [0.0689]])\n",
      "Iteration 23470 Training loss 0.05300527065992355 Validation loss 0.060480233281850815 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0125],\n",
      "        [0.8488]])\n",
      "Iteration 23480 Training loss 0.0508158802986145 Validation loss 0.0603615865111351 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9756],\n",
      "        [0.0288]])\n",
      "Iteration 23490 Training loss 0.054427407681941986 Validation loss 0.060602832585573196 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5114],\n",
      "        [0.6289]])\n",
      "Iteration 23500 Training loss 0.05423148348927498 Validation loss 0.06022058054804802 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7905],\n",
      "        [0.9995]])\n",
      "Iteration 23510 Training loss 0.0521579384803772 Validation loss 0.06027386337518692 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9887],\n",
      "        [0.3562]])\n",
      "Iteration 23520 Training loss 0.05375383049249649 Validation loss 0.06045013293623924 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7769],\n",
      "        [0.0702]])\n",
      "Iteration 23530 Training loss 0.05372164770960808 Validation loss 0.0604247972369194 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0070],\n",
      "        [0.6934]])\n",
      "Iteration 23540 Training loss 0.0560605563223362 Validation loss 0.06042318046092987 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.2064],\n",
      "        [0.2337]])\n",
      "Iteration 23550 Training loss 0.05302227661013603 Validation loss 0.06039389222860336 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5969],\n",
      "        [0.0183]])\n",
      "Iteration 23560 Training loss 0.054263368248939514 Validation loss 0.06049898639321327 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9379],\n",
      "        [0.8186]])\n",
      "Iteration 23570 Training loss 0.05196009576320648 Validation loss 0.060610074549913406 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2809],\n",
      "        [0.9906]])\n",
      "Iteration 23580 Training loss 0.05146190896630287 Validation loss 0.06051867827773094 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9777],\n",
      "        [0.3657]])\n",
      "Iteration 23590 Training loss 0.055036842823028564 Validation loss 0.060409337282180786 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8322],\n",
      "        [0.8615]])\n",
      "Iteration 23600 Training loss 0.05265951529145241 Validation loss 0.0603417307138443 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.3151],\n",
      "        [0.4991]])\n",
      "Iteration 23610 Training loss 0.05269362032413483 Validation loss 0.06027880683541298 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9817],\n",
      "        [0.1606]])\n",
      "Iteration 23620 Training loss 0.054904092103242874 Validation loss 0.060373853892087936 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0462],\n",
      "        [0.7510]])\n",
      "Iteration 23630 Training loss 0.05345742776989937 Validation loss 0.06047345697879791 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3976],\n",
      "        [0.0436]])\n",
      "Iteration 23640 Training loss 0.05539240315556526 Validation loss 0.060470737516880035 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0400],\n",
      "        [0.8514]])\n",
      "Iteration 23650 Training loss 0.05285335332155228 Validation loss 0.06068640947341919 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0699],\n",
      "        [0.7250]])\n",
      "Iteration 23660 Training loss 0.053699471056461334 Validation loss 0.06028689071536064 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0955],\n",
      "        [0.0101]])\n",
      "Iteration 23670 Training loss 0.054938141256570816 Validation loss 0.060349103063344955 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1780],\n",
      "        [0.1616]])\n",
      "Iteration 23680 Training loss 0.05404933914542198 Validation loss 0.06019895151257515 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9758],\n",
      "        [0.0324]])\n",
      "Iteration 23690 Training loss 0.05579517409205437 Validation loss 0.06026677042245865 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0335],\n",
      "        [0.0267]])\n",
      "Iteration 23700 Training loss 0.05485084280371666 Validation loss 0.060278985649347305 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6642],\n",
      "        [0.9639]])\n",
      "Iteration 23710 Training loss 0.052757926285266876 Validation loss 0.060266632586717606 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7992],\n",
      "        [0.9984]])\n",
      "Iteration 23720 Training loss 0.05295451357960701 Validation loss 0.060684364289045334 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9859],\n",
      "        [0.9819]])\n",
      "Iteration 23730 Training loss 0.0514494813978672 Validation loss 0.06043247506022453 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9946],\n",
      "        [0.0638]])\n",
      "Iteration 23740 Training loss 0.05318216606974602 Validation loss 0.06052398681640625 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0386],\n",
      "        [0.8863]])\n",
      "Iteration 23750 Training loss 0.05290941148996353 Validation loss 0.06033114343881607 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.2595],\n",
      "        [0.8731]])\n",
      "Iteration 23760 Training loss 0.05032175034284592 Validation loss 0.060205575078725815 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.4309],\n",
      "        [0.0866]])\n",
      "Iteration 23770 Training loss 0.05332104489207268 Validation loss 0.06048310548067093 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0516],\n",
      "        [0.4175]])\n",
      "Iteration 23780 Training loss 0.056101348251104355 Validation loss 0.06035011261701584 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9965],\n",
      "        [0.9988]])\n",
      "Iteration 23790 Training loss 0.05191950500011444 Validation loss 0.06059911847114563 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0234],\n",
      "        [0.7019]])\n",
      "Iteration 23800 Training loss 0.051483336836099625 Validation loss 0.060310784727334976 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5467],\n",
      "        [0.7264]])\n",
      "Iteration 23810 Training loss 0.05213126912713051 Validation loss 0.06026888266205788 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0196],\n",
      "        [0.5392]])\n",
      "Iteration 23820 Training loss 0.05430884286761284 Validation loss 0.060552723705768585 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9083],\n",
      "        [0.1577]])\n",
      "Iteration 23830 Training loss 0.05501098185777664 Validation loss 0.06020376458764076 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9672],\n",
      "        [0.4048]])\n",
      "Iteration 23840 Training loss 0.05150849372148514 Validation loss 0.06031356006860733 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.7474],\n",
      "        [0.5030]])\n",
      "Iteration 23850 Training loss 0.054408296942710876 Validation loss 0.060220371931791306 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.4984],\n",
      "        [0.0666]])\n",
      "Iteration 23860 Training loss 0.05230249837040901 Validation loss 0.060261067003011703 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2049],\n",
      "        [0.7785]])\n",
      "Iteration 23870 Training loss 0.05170981213450432 Validation loss 0.060244474560022354 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.7093],\n",
      "        [0.3221]])\n",
      "Iteration 23880 Training loss 0.053463954478502274 Validation loss 0.06020021438598633 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7868],\n",
      "        [0.0397]])\n",
      "Iteration 23890 Training loss 0.049492865800857544 Validation loss 0.06030043214559555 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9719],\n",
      "        [0.0146]])\n",
      "Iteration 23900 Training loss 0.05354992672801018 Validation loss 0.06036100536584854 Accuracy 0.828499972820282\n",
      "Output tensor([[0.0598],\n",
      "        [0.0053]])\n",
      "Iteration 23910 Training loss 0.05210648104548454 Validation loss 0.06043563783168793 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9133],\n",
      "        [0.9809]])\n",
      "Iteration 23920 Training loss 0.05347895994782448 Validation loss 0.06041346862912178 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8413],\n",
      "        [0.4328]])\n",
      "Iteration 23930 Training loss 0.04932202026247978 Validation loss 0.060205426067113876 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2838],\n",
      "        [0.0635]])\n",
      "Iteration 23940 Training loss 0.05360664427280426 Validation loss 0.0602366141974926 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9197],\n",
      "        [0.8031]])\n",
      "Iteration 23950 Training loss 0.054804474115371704 Validation loss 0.060522302985191345 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3988],\n",
      "        [0.5139]])\n",
      "Iteration 23960 Training loss 0.0547826811671257 Validation loss 0.06017030030488968 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9604],\n",
      "        [0.7055]])\n",
      "Iteration 23970 Training loss 0.05189173296093941 Validation loss 0.060646865516901016 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9362],\n",
      "        [0.0744]])\n",
      "Iteration 23980 Training loss 0.05505462363362312 Validation loss 0.06042391061782837 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9783],\n",
      "        [0.7947]])\n",
      "Iteration 23990 Training loss 0.056823357939720154 Validation loss 0.06021926924586296 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9384],\n",
      "        [0.9925]])\n",
      "Iteration 24000 Training loss 0.05365421995520592 Validation loss 0.06024756282567978 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9693],\n",
      "        [0.7305]])\n",
      "Iteration 24010 Training loss 0.05301439017057419 Validation loss 0.06043075770139694 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0934],\n",
      "        [0.3803]])\n",
      "Iteration 24020 Training loss 0.05372406914830208 Validation loss 0.06026623025536537 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.7139],\n",
      "        [0.0259]])\n",
      "Iteration 24030 Training loss 0.05215408280491829 Validation loss 0.06019681692123413 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6310],\n",
      "        [0.1327]])\n",
      "Iteration 24040 Training loss 0.05202134698629379 Validation loss 0.06042585149407387 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8772],\n",
      "        [0.7459]])\n",
      "Iteration 24050 Training loss 0.053272973746061325 Validation loss 0.06028187647461891 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9250],\n",
      "        [0.0285]])\n",
      "Iteration 24060 Training loss 0.053004760295152664 Validation loss 0.06033792719244957 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4303],\n",
      "        [0.0391]])\n",
      "Iteration 24070 Training loss 0.054176390171051025 Validation loss 0.06039002165198326 Accuracy 0.828499972820282\n",
      "Output tensor([[0.4617],\n",
      "        [0.1770]])\n",
      "Iteration 24080 Training loss 0.051450323313474655 Validation loss 0.060367975383996964 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.5002],\n",
      "        [0.0838]])\n",
      "Iteration 24090 Training loss 0.054501280188560486 Validation loss 0.06034740433096886 Accuracy 0.828499972820282\n",
      "Output tensor([[0.6952],\n",
      "        [0.9667]])\n",
      "Iteration 24100 Training loss 0.05531260371208191 Validation loss 0.0601450577378273 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.7341],\n",
      "        [0.2601]])\n",
      "Iteration 24110 Training loss 0.051369231194257736 Validation loss 0.06012516841292381 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8158],\n",
      "        [0.8879]])\n",
      "Iteration 24120 Training loss 0.05384154990315437 Validation loss 0.06022627651691437 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0235],\n",
      "        [0.4932]])\n",
      "Iteration 24130 Training loss 0.05305805057287216 Validation loss 0.06089891120791435 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2885],\n",
      "        [0.2244]])\n",
      "Iteration 24140 Training loss 0.0529121533036232 Validation loss 0.06028684601187706 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9597],\n",
      "        [0.7594]])\n",
      "Iteration 24150 Training loss 0.05705178901553154 Validation loss 0.060326747596263885 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.6560],\n",
      "        [0.9462]])\n",
      "Iteration 24160 Training loss 0.05321471393108368 Validation loss 0.06055259332060814 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1202],\n",
      "        [0.9226]])\n",
      "Iteration 24170 Training loss 0.05061504244804382 Validation loss 0.060208503156900406 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3137],\n",
      "        [0.9768]])\n",
      "Iteration 24180 Training loss 0.05097135156393051 Validation loss 0.06028178334236145 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8793],\n",
      "        [0.3970]])\n",
      "Iteration 24190 Training loss 0.05583144724369049 Validation loss 0.060430899262428284 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0096],\n",
      "        [0.4165]])\n",
      "Iteration 24200 Training loss 0.054523367434740067 Validation loss 0.0601346455514431 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8327],\n",
      "        [0.3305]])\n",
      "Iteration 24210 Training loss 0.05268579721450806 Validation loss 0.060246020555496216 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0163],\n",
      "        [0.4212]])\n",
      "Iteration 24220 Training loss 0.05362166836857796 Validation loss 0.06028713285923004 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0183],\n",
      "        [0.5908]])\n",
      "Iteration 24230 Training loss 0.05209074541926384 Validation loss 0.06030014902353287 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5954],\n",
      "        [0.6093]])\n",
      "Iteration 24240 Training loss 0.05405636876821518 Validation loss 0.06029443442821503 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.1316],\n",
      "        [0.1691]])\n",
      "Iteration 24250 Training loss 0.053473908454179764 Validation loss 0.06032451242208481 Accuracy 0.828499972820282\n",
      "Output tensor([[0.9276],\n",
      "        [0.0473]])\n",
      "Iteration 24260 Training loss 0.05283177271485329 Validation loss 0.06028832122683525 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9868],\n",
      "        [0.0315]])\n",
      "Iteration 24270 Training loss 0.054706674069166183 Validation loss 0.06016391143202782 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8429],\n",
      "        [0.2851]])\n",
      "Iteration 24280 Training loss 0.05263551324605942 Validation loss 0.060456182807683945 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8287],\n",
      "        [0.7688]])\n",
      "Iteration 24290 Training loss 0.05646254122257233 Validation loss 0.06038064882159233 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.3644],\n",
      "        [0.8163]])\n",
      "Iteration 24300 Training loss 0.05207792669534683 Validation loss 0.060362525284290314 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9999],\n",
      "        [0.2016]])\n",
      "Iteration 24310 Training loss 0.052082404494285583 Validation loss 0.060301583260297775 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9973],\n",
      "        [0.7948]])\n",
      "Iteration 24320 Training loss 0.05427732318639755 Validation loss 0.06013198569417 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9472],\n",
      "        [0.7822]])\n",
      "Iteration 24330 Training loss 0.05178804323077202 Validation loss 0.060806602239608765 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0451],\n",
      "        [0.0245]])\n",
      "Iteration 24340 Training loss 0.05457961931824684 Validation loss 0.060352426022291183 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0385],\n",
      "        [0.7361]])\n",
      "Iteration 24350 Training loss 0.053799331188201904 Validation loss 0.06016433611512184 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0028],\n",
      "        [0.7926]])\n",
      "Iteration 24360 Training loss 0.05433186516165733 Validation loss 0.0604357086122036 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1934],\n",
      "        [0.3690]])\n",
      "Iteration 24370 Training loss 0.05348856374621391 Validation loss 0.06026056408882141 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.6553],\n",
      "        [0.0055]])\n",
      "Iteration 24380 Training loss 0.056315548717975616 Validation loss 0.06081739068031311 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2034],\n",
      "        [0.8936]])\n",
      "Iteration 24390 Training loss 0.048529334366321564 Validation loss 0.06050910800695419 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8851],\n",
      "        [0.7654]])\n",
      "Iteration 24400 Training loss 0.05299847573041916 Validation loss 0.06010914593935013 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9920],\n",
      "        [0.9266]])\n",
      "Iteration 24410 Training loss 0.05456149950623512 Validation loss 0.060179755091667175 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9134],\n",
      "        [0.8845]])\n",
      "Iteration 24420 Training loss 0.05615505576133728 Validation loss 0.06082351505756378 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2777],\n",
      "        [0.9603]])\n",
      "Iteration 24430 Training loss 0.05257314443588257 Validation loss 0.060161784291267395 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8404],\n",
      "        [0.0794]])\n",
      "Iteration 24440 Training loss 0.05526100844144821 Validation loss 0.060635216534137726 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9183],\n",
      "        [0.9137]])\n",
      "Iteration 24450 Training loss 0.05559432879090309 Validation loss 0.06021147221326828 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7987],\n",
      "        [0.0800]])\n",
      "Iteration 24460 Training loss 0.051843080669641495 Validation loss 0.06052011996507645 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8546],\n",
      "        [0.0215]])\n",
      "Iteration 24470 Training loss 0.05136074125766754 Validation loss 0.06031222268939018 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6997],\n",
      "        [0.8499]])\n",
      "Iteration 24480 Training loss 0.049212682992219925 Validation loss 0.06051865965127945 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8708],\n",
      "        [0.9910]])\n",
      "Iteration 24490 Training loss 0.05305890738964081 Validation loss 0.06056367978453636 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.3123],\n",
      "        [0.9686]])\n",
      "Iteration 24500 Training loss 0.05417719855904579 Validation loss 0.060414548963308334 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9472],\n",
      "        [0.2778]])\n",
      "Iteration 24510 Training loss 0.05405505374073982 Validation loss 0.06022818386554718 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0296],\n",
      "        [0.2775]])\n",
      "Iteration 24520 Training loss 0.05918921157717705 Validation loss 0.060167498886585236 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8549],\n",
      "        [0.0804]])\n",
      "Iteration 24530 Training loss 0.05201118811964989 Validation loss 0.06015298515558243 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0793],\n",
      "        [0.8965]])\n",
      "Iteration 24540 Training loss 0.052160292863845825 Validation loss 0.060209959745407104 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3653],\n",
      "        [0.0431]])\n",
      "Iteration 24550 Training loss 0.05588322877883911 Validation loss 0.060238953679800034 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9998],\n",
      "        [0.8396]])\n",
      "Iteration 24560 Training loss 0.054561201483011246 Validation loss 0.06024687737226486 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5766],\n",
      "        [0.1243]])\n",
      "Iteration 24570 Training loss 0.05045027285814285 Validation loss 0.06022723764181137 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0128],\n",
      "        [0.1620]])\n",
      "Iteration 24580 Training loss 0.051414404064416885 Validation loss 0.06020671874284744 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.2531],\n",
      "        [0.5120]])\n",
      "Iteration 24590 Training loss 0.051666758954524994 Validation loss 0.06015113741159439 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3629],\n",
      "        [0.0171]])\n",
      "Iteration 24600 Training loss 0.054213155061006546 Validation loss 0.060322556644678116 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1155],\n",
      "        [0.9985]])\n",
      "Iteration 24610 Training loss 0.052161525934934616 Validation loss 0.06009949371218681 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9128],\n",
      "        [0.7661]])\n",
      "Iteration 24620 Training loss 0.054408248513936996 Validation loss 0.060214485973119736 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9980],\n",
      "        [0.7889]])\n",
      "Iteration 24630 Training loss 0.05245425924658775 Validation loss 0.060227952897548676 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.2147],\n",
      "        [0.1206]])\n",
      "Iteration 24640 Training loss 0.052564188838005066 Validation loss 0.060143861919641495 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9925],\n",
      "        [0.5082]])\n",
      "Iteration 24650 Training loss 0.05069109797477722 Validation loss 0.060504425317049026 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.7214],\n",
      "        [0.0297]])\n",
      "Iteration 24660 Training loss 0.05439656972885132 Validation loss 0.06034713611006737 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1840],\n",
      "        [0.4619]])\n",
      "Iteration 24670 Training loss 0.05049217492341995 Validation loss 0.06002356857061386 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9355],\n",
      "        [0.8306]])\n",
      "Iteration 24680 Training loss 0.05016004294157028 Validation loss 0.0603170208632946 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1174],\n",
      "        [0.7159]])\n",
      "Iteration 24690 Training loss 0.05178424343466759 Validation loss 0.06014939025044441 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6614],\n",
      "        [0.0672]])\n",
      "Iteration 24700 Training loss 0.05288558825850487 Validation loss 0.060132142156362534 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1911],\n",
      "        [0.4301]])\n",
      "Iteration 24710 Training loss 0.052688319236040115 Validation loss 0.06020322069525719 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2301],\n",
      "        [0.5790]])\n",
      "Iteration 24720 Training loss 0.05463263392448425 Validation loss 0.06031787768006325 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9945],\n",
      "        [0.0051]])\n",
      "Iteration 24730 Training loss 0.051121436059474945 Validation loss 0.060064587742090225 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7660],\n",
      "        [0.6381]])\n",
      "Iteration 24740 Training loss 0.05226784944534302 Validation loss 0.06026451289653778 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5917],\n",
      "        [0.9546]])\n",
      "Iteration 24750 Training loss 0.055861946195364 Validation loss 0.060425132513046265 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9693],\n",
      "        [0.8151]])\n",
      "Iteration 24760 Training loss 0.05228820815682411 Validation loss 0.06026028096675873 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0721],\n",
      "        [0.0243]])\n",
      "Iteration 24770 Training loss 0.04742727428674698 Validation loss 0.06015975773334503 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0486],\n",
      "        [0.9211]])\n",
      "Iteration 24780 Training loss 0.05318655073642731 Validation loss 0.06036149710416794 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4297],\n",
      "        [0.6686]])\n",
      "Iteration 24790 Training loss 0.05476761609315872 Validation loss 0.06031176075339317 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9816],\n",
      "        [0.9561]])\n",
      "Iteration 24800 Training loss 0.05144710838794708 Validation loss 0.060498666018247604 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0331],\n",
      "        [0.3541]])\n",
      "Iteration 24810 Training loss 0.05400475859642029 Validation loss 0.060094453394412994 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.4511],\n",
      "        [0.2096]])\n",
      "Iteration 24820 Training loss 0.054258957505226135 Validation loss 0.06041109934449196 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9930],\n",
      "        [0.7077]])\n",
      "Iteration 24830 Training loss 0.04924926534295082 Validation loss 0.060243621468544006 Accuracy 0.8274999856948853\n",
      "Output tensor([[0.0237],\n",
      "        [0.9892]])\n",
      "Iteration 24840 Training loss 0.05250047519803047 Validation loss 0.06033586710691452 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9915],\n",
      "        [0.5344]])\n",
      "Iteration 24850 Training loss 0.053522247821092606 Validation loss 0.06023278459906578 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.6880],\n",
      "        [0.5234]])\n",
      "Iteration 24860 Training loss 0.050946373492479324 Validation loss 0.060212645679712296 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6540],\n",
      "        [0.8124]])\n",
      "Iteration 24870 Training loss 0.054228730499744415 Validation loss 0.060218337923288345 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3123],\n",
      "        [0.3933]])\n",
      "Iteration 24880 Training loss 0.04971402883529663 Validation loss 0.06037411466240883 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9992],\n",
      "        [0.1068]])\n",
      "Iteration 24890 Training loss 0.05147319659590721 Validation loss 0.060587722808122635 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9961],\n",
      "        [0.0165]])\n",
      "Iteration 24900 Training loss 0.0533854216337204 Validation loss 0.06019239500164986 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0429],\n",
      "        [0.9837]])\n",
      "Iteration 24910 Training loss 0.053381867706775665 Validation loss 0.0602935291826725 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0067],\n",
      "        [0.3787]])\n",
      "Iteration 24920 Training loss 0.05171448364853859 Validation loss 0.06039668992161751 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8882],\n",
      "        [0.9157]])\n",
      "Iteration 24930 Training loss 0.054723069071769714 Validation loss 0.06017938628792763 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8459],\n",
      "        [0.0492]])\n",
      "Iteration 24940 Training loss 0.0537937730550766 Validation loss 0.060168325901031494 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7017],\n",
      "        [0.9967]])\n",
      "Iteration 24950 Training loss 0.05477364361286163 Validation loss 0.06024378910660744 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0660],\n",
      "        [0.7860]])\n",
      "Iteration 24960 Training loss 0.05302359536290169 Validation loss 0.060623008757829666 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6875],\n",
      "        [0.8572]])\n",
      "Iteration 24970 Training loss 0.050365615636110306 Validation loss 0.06016192585229874 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9665],\n",
      "        [0.0185]])\n",
      "Iteration 24980 Training loss 0.04949778690934181 Validation loss 0.060497380793094635 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0120],\n",
      "        [0.0957]])\n",
      "Iteration 24990 Training loss 0.05186411738395691 Validation loss 0.06006792560219765 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0663],\n",
      "        [0.0357]])\n",
      "Iteration 25000 Training loss 0.04863086715340614 Validation loss 0.06034153699874878 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0231],\n",
      "        [0.9900]])\n",
      "Iteration 25010 Training loss 0.0534956119954586 Validation loss 0.06017071008682251 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6704],\n",
      "        [0.9481]])\n",
      "Iteration 25020 Training loss 0.054897598922252655 Validation loss 0.06070319190621376 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7095],\n",
      "        [0.8594]])\n",
      "Iteration 25030 Training loss 0.0517842099070549 Validation loss 0.06013555824756622 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2696],\n",
      "        [0.3373]])\n",
      "Iteration 25040 Training loss 0.05269071087241173 Validation loss 0.06034623831510544 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9922],\n",
      "        [0.0225]])\n",
      "Iteration 25050 Training loss 0.049401551485061646 Validation loss 0.0600951723754406 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0652],\n",
      "        [0.8078]])\n",
      "Iteration 25060 Training loss 0.05167759209871292 Validation loss 0.06039394065737724 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0464],\n",
      "        [0.6689]])\n",
      "Iteration 25070 Training loss 0.052390556782484055 Validation loss 0.06027480959892273 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0126],\n",
      "        [0.1535]])\n",
      "Iteration 25080 Training loss 0.05070856586098671 Validation loss 0.06010976433753967 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8651],\n",
      "        [0.6531]])\n",
      "Iteration 25090 Training loss 0.05057890713214874 Validation loss 0.06035793572664261 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6128],\n",
      "        [0.9991]])\n",
      "Iteration 25100 Training loss 0.050956398248672485 Validation loss 0.06017176806926727 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6437],\n",
      "        [0.1529]])\n",
      "Iteration 25110 Training loss 0.05252561345696449 Validation loss 0.06013087183237076 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0756],\n",
      "        [0.9992]])\n",
      "Iteration 25120 Training loss 0.051592037081718445 Validation loss 0.06067120283842087 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1004],\n",
      "        [0.8991]])\n",
      "Iteration 25130 Training loss 0.055376987904310226 Validation loss 0.06014949083328247 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0679],\n",
      "        [0.9365]])\n",
      "Iteration 25140 Training loss 0.050125326961278915 Validation loss 0.060202743858098984 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1011],\n",
      "        [0.9935]])\n",
      "Iteration 25150 Training loss 0.05039297416806221 Validation loss 0.060485146939754486 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8752],\n",
      "        [0.9953]])\n",
      "Iteration 25160 Training loss 0.05341462790966034 Validation loss 0.0603959895670414 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3969],\n",
      "        [0.3098]])\n",
      "Iteration 25170 Training loss 0.0514424592256546 Validation loss 0.06015459820628166 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9644],\n",
      "        [0.0761]])\n",
      "Iteration 25180 Training loss 0.052141349762678146 Validation loss 0.06073889136314392 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7167],\n",
      "        [0.7089]])\n",
      "Iteration 25190 Training loss 0.054634202271699905 Validation loss 0.06017642840743065 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.6232],\n",
      "        [0.8979]])\n",
      "Iteration 25200 Training loss 0.0506388358771801 Validation loss 0.060392387211322784 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0155],\n",
      "        [0.6416]])\n",
      "Iteration 25210 Training loss 0.049536481499671936 Validation loss 0.06036854535341263 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2050],\n",
      "        [0.0801]])\n",
      "Iteration 25220 Training loss 0.052417460829019547 Validation loss 0.060361918061971664 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.4893],\n",
      "        [0.2329]])\n",
      "Iteration 25230 Training loss 0.05374990031123161 Validation loss 0.06024259328842163 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0183],\n",
      "        [0.0224]])\n",
      "Iteration 25240 Training loss 0.052170563489198685 Validation loss 0.06018729507923126 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5403],\n",
      "        [0.9585]])\n",
      "Iteration 25250 Training loss 0.05321800336241722 Validation loss 0.06009893864393234 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7725],\n",
      "        [0.4334]])\n",
      "Iteration 25260 Training loss 0.05387405678629875 Validation loss 0.06022316962480545 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0652],\n",
      "        [0.7976]])\n",
      "Iteration 25270 Training loss 0.049592163413763046 Validation loss 0.06007038801908493 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2827],\n",
      "        [0.0851]])\n",
      "Iteration 25280 Training loss 0.053767479956150055 Validation loss 0.060213711112737656 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0488],\n",
      "        [0.8880]])\n",
      "Iteration 25290 Training loss 0.05362462252378464 Validation loss 0.060219068080186844 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9711],\n",
      "        [0.9993]])\n",
      "Iteration 25300 Training loss 0.056128788739442825 Validation loss 0.06054098531603813 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9693],\n",
      "        [0.3359]])\n",
      "Iteration 25310 Training loss 0.05234132707118988 Validation loss 0.06007542461156845 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9999],\n",
      "        [0.0324]])\n",
      "Iteration 25320 Training loss 0.05592935532331467 Validation loss 0.06043011322617531 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.4742],\n",
      "        [0.0580]])\n",
      "Iteration 25330 Training loss 0.05226780101656914 Validation loss 0.06034034863114357 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.5364],\n",
      "        [0.4642]])\n",
      "Iteration 25340 Training loss 0.05530734732747078 Validation loss 0.06048121675848961 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8703],\n",
      "        [0.9240]])\n",
      "Iteration 25350 Training loss 0.04997017979621887 Validation loss 0.06045324727892876 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9505],\n",
      "        [0.9976]])\n",
      "Iteration 25360 Training loss 0.04889160022139549 Validation loss 0.06016024947166443 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2113],\n",
      "        [0.9422]])\n",
      "Iteration 25370 Training loss 0.052065346390008926 Validation loss 0.06022406741976738 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0594],\n",
      "        [0.2943]])\n",
      "Iteration 25380 Training loss 0.051705747842788696 Validation loss 0.06015446037054062 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0219],\n",
      "        [0.4002]])\n",
      "Iteration 25390 Training loss 0.050237711519002914 Validation loss 0.06023988500237465 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8904],\n",
      "        [0.0915]])\n",
      "Iteration 25400 Training loss 0.05396170914173126 Validation loss 0.06020234525203705 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.4100],\n",
      "        [0.0853]])\n",
      "Iteration 25410 Training loss 0.05144263431429863 Validation loss 0.060070641338825226 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6172],\n",
      "        [0.2319]])\n",
      "Iteration 25420 Training loss 0.05115183815360069 Validation loss 0.060142651200294495 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8296],\n",
      "        [0.0284]])\n",
      "Iteration 25430 Training loss 0.051722053438425064 Validation loss 0.0599992536008358 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9734],\n",
      "        [0.1398]])\n",
      "Iteration 25440 Training loss 0.05323564261198044 Validation loss 0.06025572493672371 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7054],\n",
      "        [0.9389]])\n",
      "Iteration 25450 Training loss 0.053921110928058624 Validation loss 0.06021922454237938 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.4501],\n",
      "        [0.2157]])\n",
      "Iteration 25460 Training loss 0.052580032497644424 Validation loss 0.06031285226345062 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9982],\n",
      "        [0.0148]])\n",
      "Iteration 25470 Training loss 0.05478678643703461 Validation loss 0.06019957736134529 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0288],\n",
      "        [0.9799]])\n",
      "Iteration 25480 Training loss 0.05318471044301987 Validation loss 0.06029951572418213 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0458],\n",
      "        [0.9640]])\n",
      "Iteration 25490 Training loss 0.05375529080629349 Validation loss 0.06018228828907013 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0680],\n",
      "        [0.1208]])\n",
      "Iteration 25500 Training loss 0.05303896218538284 Validation loss 0.060322996228933334 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2437],\n",
      "        [0.8122]])\n",
      "Iteration 25510 Training loss 0.05320128798484802 Validation loss 0.06020558252930641 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9045],\n",
      "        [0.0737]])\n",
      "Iteration 25520 Training loss 0.049630969762802124 Validation loss 0.06015465408563614 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9415],\n",
      "        [0.4344]])\n",
      "Iteration 25530 Training loss 0.053725484758615494 Validation loss 0.060122665017843246 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7491],\n",
      "        [0.0019]])\n",
      "Iteration 25540 Training loss 0.053985659033060074 Validation loss 0.06018517166376114 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0515],\n",
      "        [0.8466]])\n",
      "Iteration 25550 Training loss 0.052581168711185455 Validation loss 0.059998899698257446 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9734],\n",
      "        [0.1230]])\n",
      "Iteration 25560 Training loss 0.05303182452917099 Validation loss 0.060430511832237244 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.3188],\n",
      "        [0.0948]])\n",
      "Iteration 25570 Training loss 0.05602288991212845 Validation loss 0.06039111316204071 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8299],\n",
      "        [0.9570]])\n",
      "Iteration 25580 Training loss 0.05233803391456604 Validation loss 0.06008376181125641 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0296],\n",
      "        [0.0154]])\n",
      "Iteration 25590 Training loss 0.052327580749988556 Validation loss 0.06021585687994957 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9283],\n",
      "        [0.6481]])\n",
      "Iteration 25600 Training loss 0.05225447192788124 Validation loss 0.06009657308459282 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9129],\n",
      "        [0.1703]])\n",
      "Iteration 25610 Training loss 0.05451267957687378 Validation loss 0.0601145438849926 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9548],\n",
      "        [0.9949]])\n",
      "Iteration 25620 Training loss 0.05233537405729294 Validation loss 0.06018917262554169 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.6332],\n",
      "        [0.9933]])\n",
      "Iteration 25630 Training loss 0.0518200658261776 Validation loss 0.060192618519067764 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0701],\n",
      "        [0.9952]])\n",
      "Iteration 25640 Training loss 0.053803760558366776 Validation loss 0.06040593981742859 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.4047],\n",
      "        [0.0148]])\n",
      "Iteration 25650 Training loss 0.052214257419109344 Validation loss 0.06037694588303566 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1752],\n",
      "        [0.6758]])\n",
      "Iteration 25660 Training loss 0.054617781192064285 Validation loss 0.06021752953529358 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8838],\n",
      "        [0.0112]])\n",
      "Iteration 25670 Training loss 0.05300021916627884 Validation loss 0.060029249638319016 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.5049],\n",
      "        [0.9770]])\n",
      "Iteration 25680 Training loss 0.053381748497486115 Validation loss 0.060058534145355225 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8471],\n",
      "        [0.0276]])\n",
      "Iteration 25690 Training loss 0.050203803926706314 Validation loss 0.06027344614267349 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0246],\n",
      "        [0.9749]])\n",
      "Iteration 25700 Training loss 0.05578162893652916 Validation loss 0.06001405790448189 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9719],\n",
      "        [0.3591]])\n",
      "Iteration 25710 Training loss 0.054188262671232224 Validation loss 0.060367342084646225 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6284],\n",
      "        [0.9455]])\n",
      "Iteration 25720 Training loss 0.05103212594985962 Validation loss 0.06000392138957977 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0093],\n",
      "        [0.4259]])\n",
      "Iteration 25730 Training loss 0.051749337464571 Validation loss 0.06009660288691521 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0243],\n",
      "        [0.2019]])\n",
      "Iteration 25740 Training loss 0.050394684076309204 Validation loss 0.06008177623152733 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0790],\n",
      "        [0.1065]])\n",
      "Iteration 25750 Training loss 0.0500941164791584 Validation loss 0.06019657477736473 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0635],\n",
      "        [0.0089]])\n",
      "Iteration 25760 Training loss 0.05517827719449997 Validation loss 0.06007961183786392 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8501],\n",
      "        [0.2356]])\n",
      "Iteration 25770 Training loss 0.04951537400484085 Validation loss 0.06027119979262352 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9999],\n",
      "        [0.9011]])\n",
      "Iteration 25780 Training loss 0.05100177600979805 Validation loss 0.0600791797041893 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2844],\n",
      "        [0.0674]])\n",
      "Iteration 25790 Training loss 0.050736039876937866 Validation loss 0.06024201959371567 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7271],\n",
      "        [0.9604]])\n",
      "Iteration 25800 Training loss 0.05403705686330795 Validation loss 0.06036954000592232 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9933],\n",
      "        [0.0776]])\n",
      "Iteration 25810 Training loss 0.04907992109656334 Validation loss 0.06012141332030296 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0043],\n",
      "        [0.9663]])\n",
      "Iteration 25820 Training loss 0.04828069359064102 Validation loss 0.060013238340616226 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1050],\n",
      "        [0.0253]])\n",
      "Iteration 25830 Training loss 0.050047680735588074 Validation loss 0.06012001633644104 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3187],\n",
      "        [0.5543]])\n",
      "Iteration 25840 Training loss 0.05347460135817528 Validation loss 0.06012896075844765 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.4898],\n",
      "        [0.9768]])\n",
      "Iteration 25850 Training loss 0.054445791989564896 Validation loss 0.06026419997215271 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9999],\n",
      "        [0.6076]])\n",
      "Iteration 25860 Training loss 0.05256541073322296 Validation loss 0.060297731310129166 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8968],\n",
      "        [0.6062]])\n",
      "Iteration 25870 Training loss 0.05322440341114998 Validation loss 0.06010198965668678 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8865],\n",
      "        [0.9829]])\n",
      "Iteration 25880 Training loss 0.04864567518234253 Validation loss 0.060102008283138275 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2009],\n",
      "        [0.7704]])\n",
      "Iteration 25890 Training loss 0.055466149002313614 Validation loss 0.06024976819753647 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0029],\n",
      "        [0.9448]])\n",
      "Iteration 25900 Training loss 0.05384276434779167 Validation loss 0.06031443551182747 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9998],\n",
      "        [0.9622]])\n",
      "Iteration 25910 Training loss 0.05259481072425842 Validation loss 0.060177940875291824 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1941],\n",
      "        [0.8816]])\n",
      "Iteration 25920 Training loss 0.054909031838178635 Validation loss 0.06023078411817551 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3931],\n",
      "        [0.9409]])\n",
      "Iteration 25930 Training loss 0.05316074192523956 Validation loss 0.06000370532274246 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8812],\n",
      "        [0.4425]])\n",
      "Iteration 25940 Training loss 0.05051659420132637 Validation loss 0.060151901096105576 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0162],\n",
      "        [0.0390]])\n",
      "Iteration 25950 Training loss 0.050521742552518845 Validation loss 0.06025778874754906 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9694],\n",
      "        [0.1169]])\n",
      "Iteration 25960 Training loss 0.05530804768204689 Validation loss 0.06003507971763611 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8819],\n",
      "        [0.0173]])\n",
      "Iteration 25970 Training loss 0.05461064353585243 Validation loss 0.060487404465675354 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1997],\n",
      "        [0.0993]])\n",
      "Iteration 25980 Training loss 0.05239260569214821 Validation loss 0.060250476002693176 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.5772],\n",
      "        [0.9756]])\n",
      "Iteration 25990 Training loss 0.053657226264476776 Validation loss 0.05996617302298546 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.7686],\n",
      "        [0.1412]])\n",
      "Iteration 26000 Training loss 0.051850635558366776 Validation loss 0.05995485931634903 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.9095],\n",
      "        [0.1350]])\n",
      "Iteration 26010 Training loss 0.053584106266498566 Validation loss 0.06022600457072258 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0431],\n",
      "        [0.7498]])\n",
      "Iteration 26020 Training loss 0.05073880776762962 Validation loss 0.060068532824516296 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9776],\n",
      "        [0.1460]])\n",
      "Iteration 26030 Training loss 0.05090312287211418 Validation loss 0.06041891872882843 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.4469],\n",
      "        [0.4214]])\n",
      "Iteration 26040 Training loss 0.0525040477514267 Validation loss 0.060462456196546555 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0634],\n",
      "        [0.0513]])\n",
      "Iteration 26050 Training loss 0.05207669734954834 Validation loss 0.06019546836614609 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6693],\n",
      "        [0.0552]])\n",
      "Iteration 26060 Training loss 0.05543793365359306 Validation loss 0.06013527140021324 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2898],\n",
      "        [0.2475]])\n",
      "Iteration 26070 Training loss 0.05180894955992699 Validation loss 0.060068704187870026 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1211],\n",
      "        [0.3978]])\n",
      "Iteration 26080 Training loss 0.05211682990193367 Validation loss 0.06007370352745056 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3448],\n",
      "        [0.8568]])\n",
      "Iteration 26090 Training loss 0.05313532426953316 Validation loss 0.05993407964706421 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8494],\n",
      "        [0.9927]])\n",
      "Iteration 26100 Training loss 0.05390099063515663 Validation loss 0.05995102599263191 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0123],\n",
      "        [0.8679]])\n",
      "Iteration 26110 Training loss 0.05144227668642998 Validation loss 0.060650214552879333 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3656],\n",
      "        [0.0184]])\n",
      "Iteration 26120 Training loss 0.051036667078733444 Validation loss 0.060117095708847046 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9824],\n",
      "        [0.8937]])\n",
      "Iteration 26130 Training loss 0.05357247218489647 Validation loss 0.059977367520332336 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0165],\n",
      "        [0.1259]])\n",
      "Iteration 26140 Training loss 0.05028543248772621 Validation loss 0.06023957207798958 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9999],\n",
      "        [0.2076]])\n",
      "Iteration 26150 Training loss 0.050237298011779785 Validation loss 0.06025165691971779 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.7294],\n",
      "        [0.9632]])\n",
      "Iteration 26160 Training loss 0.05280637368559837 Validation loss 0.060060303658246994 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.5429],\n",
      "        [0.0744]])\n",
      "Iteration 26170 Training loss 0.04912181198596954 Validation loss 0.05991978943347931 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9994],\n",
      "        [0.0153]])\n",
      "Iteration 26180 Training loss 0.05390946567058563 Validation loss 0.06000882759690285 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6599],\n",
      "        [0.2054]])\n",
      "Iteration 26190 Training loss 0.0516841895878315 Validation loss 0.06017473712563515 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.5844],\n",
      "        [0.9359]])\n",
      "Iteration 26200 Training loss 0.05419137701392174 Validation loss 0.06055070087313652 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9695],\n",
      "        [0.2880]])\n",
      "Iteration 26210 Training loss 0.05076560750603676 Validation loss 0.060163356363773346 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9751],\n",
      "        [0.8809]])\n",
      "Iteration 26220 Training loss 0.05237419158220291 Validation loss 0.06008113920688629 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.5758],\n",
      "        [0.9456]])\n",
      "Iteration 26230 Training loss 0.05283142253756523 Validation loss 0.060087479650974274 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1489],\n",
      "        [0.3748]])\n",
      "Iteration 26240 Training loss 0.0518624484539032 Validation loss 0.06014641374349594 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2067],\n",
      "        [0.2651]])\n",
      "Iteration 26250 Training loss 0.05418236553668976 Validation loss 0.059985872358083725 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9129],\n",
      "        [0.3708]])\n",
      "Iteration 26260 Training loss 0.05168236419558525 Validation loss 0.059942178428173065 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.4639],\n",
      "        [0.3007]])\n",
      "Iteration 26270 Training loss 0.051389046013355255 Validation loss 0.0601794496178627 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.7430],\n",
      "        [0.9485]])\n",
      "Iteration 26280 Training loss 0.0541369654238224 Validation loss 0.05994253605604172 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.3074],\n",
      "        [0.9047]])\n",
      "Iteration 26290 Training loss 0.05202247574925423 Validation loss 0.06013251468539238 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6076],\n",
      "        [0.9953]])\n",
      "Iteration 26300 Training loss 0.05485665053129196 Validation loss 0.05998940393328667 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1697],\n",
      "        [0.9186]])\n",
      "Iteration 26310 Training loss 0.051970694214105606 Validation loss 0.06011083349585533 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0818],\n",
      "        [0.0690]])\n",
      "Iteration 26320 Training loss 0.0534115694463253 Validation loss 0.060140445828437805 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1748],\n",
      "        [0.1522]])\n",
      "Iteration 26330 Training loss 0.05491167679429054 Validation loss 0.05998963490128517 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0429],\n",
      "        [0.4584]])\n",
      "Iteration 26340 Training loss 0.0534740686416626 Validation loss 0.06055835634469986 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.2867],\n",
      "        [0.1810]])\n",
      "Iteration 26350 Training loss 0.05223236232995987 Validation loss 0.06036611273884773 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0741],\n",
      "        [0.9262]])\n",
      "Iteration 26360 Training loss 0.05193064361810684 Validation loss 0.060078080743551254 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8048],\n",
      "        [0.5854]])\n",
      "Iteration 26370 Training loss 0.048906609416007996 Validation loss 0.059982866048812866 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9998],\n",
      "        [0.0339]])\n",
      "Iteration 26380 Training loss 0.052126359194517136 Validation loss 0.06046615168452263 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0630],\n",
      "        [0.3101]])\n",
      "Iteration 26390 Training loss 0.051807060837745667 Validation loss 0.06023024395108223 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1584],\n",
      "        [0.7648]])\n",
      "Iteration 26400 Training loss 0.05236098915338516 Validation loss 0.060079239308834076 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.6180],\n",
      "        [0.9952]])\n",
      "Iteration 26410 Training loss 0.05450054630637169 Validation loss 0.06004830449819565 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1966],\n",
      "        [0.6523]])\n",
      "Iteration 26420 Training loss 0.05373619496822357 Validation loss 0.06042347848415375 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8767],\n",
      "        [0.5462]])\n",
      "Iteration 26430 Training loss 0.04984369874000549 Validation loss 0.06034741923213005 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7331],\n",
      "        [0.8939]])\n",
      "Iteration 26440 Training loss 0.05204242095351219 Validation loss 0.059953346848487854 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.5186],\n",
      "        [0.1444]])\n",
      "Iteration 26450 Training loss 0.05124276503920555 Validation loss 0.060029804706573486 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0523],\n",
      "        [0.9914]])\n",
      "Iteration 26460 Training loss 0.05098447948694229 Validation loss 0.060153815895318985 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.4534],\n",
      "        [0.6025]])\n",
      "Iteration 26470 Training loss 0.05187234655022621 Validation loss 0.05992288142442703 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8943],\n",
      "        [0.8901]])\n",
      "Iteration 26480 Training loss 0.05282455310225487 Validation loss 0.06035737693309784 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1100],\n",
      "        [0.7169]])\n",
      "Iteration 26490 Training loss 0.0487079918384552 Validation loss 0.060234036296606064 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6431],\n",
      "        [0.0554]])\n",
      "Iteration 26500 Training loss 0.05176100879907608 Validation loss 0.060321323573589325 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5700],\n",
      "        [0.7670]])\n",
      "Iteration 26510 Training loss 0.052413322031497955 Validation loss 0.05999379977583885 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2727],\n",
      "        [0.9398]])\n",
      "Iteration 26520 Training loss 0.05035613477230072 Validation loss 0.06006763502955437 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9209],\n",
      "        [0.9651]])\n",
      "Iteration 26530 Training loss 0.05230318382382393 Validation loss 0.060248810797929764 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0025],\n",
      "        [0.0440]])\n",
      "Iteration 26540 Training loss 0.04805869236588478 Validation loss 0.0601617805659771 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3580],\n",
      "        [0.1823]])\n",
      "Iteration 26550 Training loss 0.04888787865638733 Validation loss 0.060082126408815384 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1748],\n",
      "        [0.0706]])\n",
      "Iteration 26560 Training loss 0.052464842796325684 Validation loss 0.060079000890254974 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0162],\n",
      "        [0.3661]])\n",
      "Iteration 26570 Training loss 0.05243544653058052 Validation loss 0.06031164526939392 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4907],\n",
      "        [0.4732]])\n",
      "Iteration 26580 Training loss 0.05022153630852699 Validation loss 0.06028173863887787 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0210],\n",
      "        [0.4356]])\n",
      "Iteration 26590 Training loss 0.05095091462135315 Validation loss 0.059987250715494156 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9790],\n",
      "        [0.9108]])\n",
      "Iteration 26600 Training loss 0.053061265498399734 Validation loss 0.06019621342420578 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.2411],\n",
      "        [0.9873]])\n",
      "Iteration 26610 Training loss 0.05069882050156593 Validation loss 0.06023634970188141 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1567],\n",
      "        [0.2996]])\n",
      "Iteration 26620 Training loss 0.05541073530912399 Validation loss 0.06029230356216431 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1254],\n",
      "        [0.9877]])\n",
      "Iteration 26630 Training loss 0.05325557664036751 Validation loss 0.060089677572250366 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9844],\n",
      "        [0.0577]])\n",
      "Iteration 26640 Training loss 0.05385727435350418 Validation loss 0.060371752828359604 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9444],\n",
      "        [0.9186]])\n",
      "Iteration 26650 Training loss 0.05247854068875313 Validation loss 0.060147881507873535 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.7899],\n",
      "        [0.6784]])\n",
      "Iteration 26660 Training loss 0.05454136058688164 Validation loss 0.06010102853178978 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5532],\n",
      "        [0.7665]])\n",
      "Iteration 26670 Training loss 0.055605120956897736 Validation loss 0.059864092618227005 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.5736],\n",
      "        [0.9501]])\n",
      "Iteration 26680 Training loss 0.052524928003549576 Validation loss 0.06044996902346611 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6366],\n",
      "        [0.6752]])\n",
      "Iteration 26690 Training loss 0.05551113560795784 Validation loss 0.06023303419351578 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2452],\n",
      "        [0.9993]])\n",
      "Iteration 26700 Training loss 0.05220196396112442 Validation loss 0.060293663293123245 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1338],\n",
      "        [0.4689]])\n",
      "Iteration 26710 Training loss 0.05138678848743439 Validation loss 0.0600702129304409 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0357],\n",
      "        [0.6566]])\n",
      "Iteration 26720 Training loss 0.05329933017492294 Validation loss 0.06043744832277298 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1328],\n",
      "        [0.1997]])\n",
      "Iteration 26730 Training loss 0.051728006452322006 Validation loss 0.06032959371805191 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8337],\n",
      "        [0.8108]])\n",
      "Iteration 26740 Training loss 0.05075736343860626 Validation loss 0.06004270538687706 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7995],\n",
      "        [0.9854]])\n",
      "Iteration 26750 Training loss 0.05175543949007988 Validation loss 0.060098860412836075 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9802],\n",
      "        [0.7144]])\n",
      "Iteration 26760 Training loss 0.05271819233894348 Validation loss 0.059887491166591644 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0123],\n",
      "        [0.4454]])\n",
      "Iteration 26770 Training loss 0.05158371850848198 Validation loss 0.06021870672702789 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7642],\n",
      "        [0.9762]])\n",
      "Iteration 26780 Training loss 0.05071226879954338 Validation loss 0.06028052419424057 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0100],\n",
      "        [0.5152]])\n",
      "Iteration 26790 Training loss 0.0531826987862587 Validation loss 0.05997678264975548 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0217],\n",
      "        [0.2482]])\n",
      "Iteration 26800 Training loss 0.05510411784052849 Validation loss 0.06016596779227257 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9823],\n",
      "        [0.0942]])\n",
      "Iteration 26810 Training loss 0.052334852516651154 Validation loss 0.06030872091650963 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3353],\n",
      "        [0.7063]])\n",
      "Iteration 26820 Training loss 0.054358262568712234 Validation loss 0.06012856960296631 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1377],\n",
      "        [0.7699]])\n",
      "Iteration 26830 Training loss 0.052060965448617935 Validation loss 0.05990482494235039 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8383],\n",
      "        [0.9775]])\n",
      "Iteration 26840 Training loss 0.052426520735025406 Validation loss 0.06024397164583206 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9464],\n",
      "        [0.1719]])\n",
      "Iteration 26850 Training loss 0.0509883314371109 Validation loss 0.059965476393699646 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8393],\n",
      "        [0.3419]])\n",
      "Iteration 26860 Training loss 0.051748424768447876 Validation loss 0.05997240170836449 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9979],\n",
      "        [0.6037]])\n",
      "Iteration 26870 Training loss 0.0494258813560009 Validation loss 0.0601072683930397 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9994],\n",
      "        [0.7987]])\n",
      "Iteration 26880 Training loss 0.0541188083589077 Validation loss 0.06003822758793831 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9783],\n",
      "        [0.3303]])\n",
      "Iteration 26890 Training loss 0.05423831194639206 Validation loss 0.060085196048021317 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9439],\n",
      "        [0.3873]])\n",
      "Iteration 26900 Training loss 0.05088456720113754 Validation loss 0.06000130623579025 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7694],\n",
      "        [0.1178]])\n",
      "Iteration 26910 Training loss 0.0511738620698452 Validation loss 0.06004755571484566 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0280],\n",
      "        [0.0054]])\n",
      "Iteration 26920 Training loss 0.04819006472826004 Validation loss 0.06004327908158302 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5681],\n",
      "        [0.7979]])\n",
      "Iteration 26930 Training loss 0.04981209710240364 Validation loss 0.0600343756377697 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9844],\n",
      "        [0.8883]])\n",
      "Iteration 26940 Training loss 0.048672545701265335 Validation loss 0.06021071597933769 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.4256],\n",
      "        [0.0153]])\n",
      "Iteration 26950 Training loss 0.05371267348527908 Validation loss 0.06010957434773445 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9609],\n",
      "        [0.9102]])\n",
      "Iteration 26960 Training loss 0.05079957842826843 Validation loss 0.06010697782039642 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0267],\n",
      "        [0.3292]])\n",
      "Iteration 26970 Training loss 0.053626786917448044 Validation loss 0.05998331680893898 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9995],\n",
      "        [0.4272]])\n",
      "Iteration 26980 Training loss 0.050626229494810104 Validation loss 0.060121454298496246 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9250],\n",
      "        [0.6535]])\n",
      "Iteration 26990 Training loss 0.05245273560285568 Validation loss 0.060052573680877686 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.4857],\n",
      "        [0.4082]])\n",
      "Iteration 27000 Training loss 0.05193064734339714 Validation loss 0.0600506067276001 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7480],\n",
      "        [0.2297]])\n",
      "Iteration 27010 Training loss 0.05181974545121193 Validation loss 0.05987237021327019 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2433],\n",
      "        [0.0626]])\n",
      "Iteration 27020 Training loss 0.05489550903439522 Validation loss 0.060286205261945724 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0631],\n",
      "        [0.2500]])\n",
      "Iteration 27030 Training loss 0.05273841693997383 Validation loss 0.06054830178618431 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.1439],\n",
      "        [0.0868]])\n",
      "Iteration 27040 Training loss 0.05155660957098007 Validation loss 0.06025328114628792 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8345],\n",
      "        [0.3994]])\n",
      "Iteration 27050 Training loss 0.053100213408470154 Validation loss 0.059873878955841064 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.5754],\n",
      "        [0.9392]])\n",
      "Iteration 27060 Training loss 0.050713181495666504 Validation loss 0.05989622324705124 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0086],\n",
      "        [0.0107]])\n",
      "Iteration 27070 Training loss 0.051509492099285126 Validation loss 0.060010675340890884 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0842],\n",
      "        [0.2234]])\n",
      "Iteration 27080 Training loss 0.05420549958944321 Validation loss 0.05995276942849159 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9504],\n",
      "        [0.3425]])\n",
      "Iteration 27090 Training loss 0.05106339976191521 Validation loss 0.060196176171302795 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9877],\n",
      "        [0.8644]])\n",
      "Iteration 27100 Training loss 0.05270697921514511 Validation loss 0.06002654880285263 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.7623],\n",
      "        [0.9659]])\n",
      "Iteration 27110 Training loss 0.05246533825993538 Validation loss 0.06048225611448288 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7930],\n",
      "        [0.8746]])\n",
      "Iteration 27120 Training loss 0.052929919213056564 Validation loss 0.05983663350343704 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5662],\n",
      "        [0.9804]])\n",
      "Iteration 27130 Training loss 0.05268895626068115 Validation loss 0.06014332175254822 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2464],\n",
      "        [0.0754]])\n",
      "Iteration 27140 Training loss 0.05275402218103409 Validation loss 0.06004045903682709 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0250],\n",
      "        [0.3930]])\n",
      "Iteration 27150 Training loss 0.05168888717889786 Validation loss 0.06005512923002243 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1567],\n",
      "        [0.0007]])\n",
      "Iteration 27160 Training loss 0.05705997720360756 Validation loss 0.06004369258880615 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6375],\n",
      "        [0.4774]])\n",
      "Iteration 27170 Training loss 0.05129573121666908 Validation loss 0.06003080680966377 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9159],\n",
      "        [0.0204]])\n",
      "Iteration 27180 Training loss 0.05117755010724068 Validation loss 0.060061343014240265 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0051],\n",
      "        [0.9348]])\n",
      "Iteration 27190 Training loss 0.05184241011738777 Validation loss 0.06001152843236923 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9882],\n",
      "        [0.7572]])\n",
      "Iteration 27200 Training loss 0.05280927196145058 Validation loss 0.059855636209249496 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3473],\n",
      "        [0.7222]])\n",
      "Iteration 27210 Training loss 0.05384885519742966 Validation loss 0.060002148151397705 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2281],\n",
      "        [0.0122]])\n",
      "Iteration 27220 Training loss 0.05537177622318268 Validation loss 0.06018270179629326 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0425],\n",
      "        [0.1131]])\n",
      "Iteration 27230 Training loss 0.04794150963425636 Validation loss 0.06012006849050522 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1307],\n",
      "        [0.7530]])\n",
      "Iteration 27240 Training loss 0.050398461520671844 Validation loss 0.06039184331893921 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9759],\n",
      "        [0.4597]])\n",
      "Iteration 27250 Training loss 0.052310552448034286 Validation loss 0.05994606390595436 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.1052],\n",
      "        [0.1743]])\n",
      "Iteration 27260 Training loss 0.049565643072128296 Validation loss 0.060260385274887085 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.7273],\n",
      "        [0.1190]])\n",
      "Iteration 27270 Training loss 0.051816705614328384 Validation loss 0.06003511697053909 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3060],\n",
      "        [0.9530]])\n",
      "Iteration 27280 Training loss 0.05062487721443176 Validation loss 0.06016821041703224 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1548],\n",
      "        [0.9021]])\n",
      "Iteration 27290 Training loss 0.05169210955500603 Validation loss 0.06023300066590309 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.4845],\n",
      "        [0.3946]])\n",
      "Iteration 27300 Training loss 0.05036703497171402 Validation loss 0.06009530648589134 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6893],\n",
      "        [0.0900]])\n",
      "Iteration 27310 Training loss 0.053157296031713486 Validation loss 0.06029356271028519 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.2736],\n",
      "        [0.0202]])\n",
      "Iteration 27320 Training loss 0.054507430642843246 Validation loss 0.05994736775755882 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8683],\n",
      "        [0.0850]])\n",
      "Iteration 27330 Training loss 0.05308748409152031 Validation loss 0.06013033911585808 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.8927],\n",
      "        [0.8785]])\n",
      "Iteration 27340 Training loss 0.05157702416181564 Validation loss 0.05991863086819649 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0915],\n",
      "        [0.7505]])\n",
      "Iteration 27350 Training loss 0.05353519693017006 Validation loss 0.06008787825703621 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.6098],\n",
      "        [0.9873]])\n",
      "Iteration 27360 Training loss 0.05193394422531128 Validation loss 0.05987058952450752 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.7159],\n",
      "        [0.3450]])\n",
      "Iteration 27370 Training loss 0.055293817073106766 Validation loss 0.06011801213026047 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8022],\n",
      "        [0.9653]])\n",
      "Iteration 27380 Training loss 0.050291627645492554 Validation loss 0.059858955442905426 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9918],\n",
      "        [0.6065]])\n",
      "Iteration 27390 Training loss 0.0519179105758667 Validation loss 0.06003586947917938 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9898],\n",
      "        [0.9961]])\n",
      "Iteration 27400 Training loss 0.052365612238645554 Validation loss 0.06019376218318939 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0942],\n",
      "        [0.9136]])\n",
      "Iteration 27410 Training loss 0.05200144276022911 Validation loss 0.06010903790593147 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9476],\n",
      "        [0.6044]])\n",
      "Iteration 27420 Training loss 0.04992320016026497 Validation loss 0.06004119664430618 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7097],\n",
      "        [0.0967]])\n",
      "Iteration 27430 Training loss 0.05282173678278923 Validation loss 0.060130294412374496 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8559],\n",
      "        [0.2543]])\n",
      "Iteration 27440 Training loss 0.05590822175145149 Validation loss 0.06004265695810318 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0689],\n",
      "        [0.8578]])\n",
      "Iteration 27450 Training loss 0.05348619446158409 Validation loss 0.060004327446222305 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0679],\n",
      "        [0.0434]])\n",
      "Iteration 27460 Training loss 0.05476035922765732 Validation loss 0.060044173151254654 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9828],\n",
      "        [0.0031]])\n",
      "Iteration 27470 Training loss 0.051424384117126465 Validation loss 0.059867288917303085 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9615],\n",
      "        [0.2719]])\n",
      "Iteration 27480 Training loss 0.05330468714237213 Validation loss 0.060080572962760925 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9667],\n",
      "        [0.0441]])\n",
      "Iteration 27490 Training loss 0.0499044768512249 Validation loss 0.060287054628133774 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.8494],\n",
      "        [0.6691]])\n",
      "Iteration 27500 Training loss 0.05273948609828949 Validation loss 0.0599646270275116 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9909],\n",
      "        [0.9432]])\n",
      "Iteration 27510 Training loss 0.047199830412864685 Validation loss 0.059939902275800705 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.7730],\n",
      "        [0.8624]])\n",
      "Iteration 27520 Training loss 0.05354303866624832 Validation loss 0.05980556830763817 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0061],\n",
      "        [0.1664]])\n",
      "Iteration 27530 Training loss 0.05264044925570488 Validation loss 0.06001366302371025 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9902],\n",
      "        [0.9487]])\n",
      "Iteration 27540 Training loss 0.051077019423246384 Validation loss 0.0600273534655571 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.1349],\n",
      "        [0.9669]])\n",
      "Iteration 27550 Training loss 0.05291400104761124 Validation loss 0.05998733267188072 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.6282],\n",
      "        [0.0165]])\n",
      "Iteration 27560 Training loss 0.05238828435540199 Validation loss 0.060033317655324936 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9898],\n",
      "        [0.5196]])\n",
      "Iteration 27570 Training loss 0.05156354606151581 Validation loss 0.06012684851884842 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9126],\n",
      "        [0.4217]])\n",
      "Iteration 27580 Training loss 0.053629446774721146 Validation loss 0.060132745653390884 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.4362],\n",
      "        [0.9478]])\n",
      "Iteration 27590 Training loss 0.050140827894210815 Validation loss 0.060027528554201126 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9921],\n",
      "        [0.0218]])\n",
      "Iteration 27600 Training loss 0.05441836267709732 Validation loss 0.0600472018122673 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.6560],\n",
      "        [0.8254]])\n",
      "Iteration 27610 Training loss 0.05196376517415047 Validation loss 0.059939734637737274 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0772],\n",
      "        [0.8032]])\n",
      "Iteration 27620 Training loss 0.05736185982823372 Validation loss 0.06040462478995323 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.4126],\n",
      "        [0.1019]])\n",
      "Iteration 27630 Training loss 0.05514010041952133 Validation loss 0.06009111925959587 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1651],\n",
      "        [0.0089]])\n",
      "Iteration 27640 Training loss 0.05260183662176132 Validation loss 0.059978123754262924 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.6991],\n",
      "        [0.3313]])\n",
      "Iteration 27650 Training loss 0.05384841188788414 Validation loss 0.06013546139001846 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.3930],\n",
      "        [0.4737]])\n",
      "Iteration 27660 Training loss 0.050171758979558945 Validation loss 0.059800561517477036 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9987],\n",
      "        [0.8365]])\n",
      "Iteration 27670 Training loss 0.051848143339157104 Validation loss 0.05992664396762848 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0853],\n",
      "        [0.0408]])\n",
      "Iteration 27680 Training loss 0.05250050500035286 Validation loss 0.05997514724731445 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0104],\n",
      "        [0.8596]])\n",
      "Iteration 27690 Training loss 0.049610670655965805 Validation loss 0.05990057438611984 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0830],\n",
      "        [0.9392]])\n",
      "Iteration 27700 Training loss 0.048292435705661774 Validation loss 0.05991954356431961 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.8092],\n",
      "        [0.8241]])\n",
      "Iteration 27710 Training loss 0.05111173912882805 Validation loss 0.06053436920046806 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.6245],\n",
      "        [0.8974]])\n",
      "Iteration 27720 Training loss 0.04859534651041031 Validation loss 0.06017732992768288 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0165],\n",
      "        [0.9752]])\n",
      "Iteration 27730 Training loss 0.052949510514736176 Validation loss 0.060180291533470154 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9944],\n",
      "        [0.9916]])\n",
      "Iteration 27740 Training loss 0.049091726541519165 Validation loss 0.06007496640086174 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2402],\n",
      "        [0.0048]])\n",
      "Iteration 27750 Training loss 0.048880234360694885 Validation loss 0.05996498838067055 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0627],\n",
      "        [0.5234]])\n",
      "Iteration 27760 Training loss 0.051279742270708084 Validation loss 0.06007494777441025 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0262],\n",
      "        [0.5561]])\n",
      "Iteration 27770 Training loss 0.05076107755303383 Validation loss 0.059910111129283905 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.1484],\n",
      "        [0.9966]])\n",
      "Iteration 27780 Training loss 0.05077643692493439 Validation loss 0.06017662212252617 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2759],\n",
      "        [0.9773]])\n",
      "Iteration 27790 Training loss 0.05302802100777626 Validation loss 0.059949036687612534 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.8787],\n",
      "        [0.0057]])\n",
      "Iteration 27800 Training loss 0.055777184665203094 Validation loss 0.05999675765633583 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.3286],\n",
      "        [0.1118]])\n",
      "Iteration 27810 Training loss 0.051904238760471344 Validation loss 0.06004415825009346 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2212],\n",
      "        [0.0503]])\n",
      "Iteration 27820 Training loss 0.05107833072543144 Validation loss 0.059838034212589264 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.1353],\n",
      "        [0.0474]])\n",
      "Iteration 27830 Training loss 0.04880611598491669 Validation loss 0.060060374438762665 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0628],\n",
      "        [0.4766]])\n",
      "Iteration 27840 Training loss 0.04988770931959152 Validation loss 0.060093771666288376 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9824],\n",
      "        [0.9869]])\n",
      "Iteration 27850 Training loss 0.050833042711019516 Validation loss 0.06023820862174034 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8889],\n",
      "        [0.1529]])\n",
      "Iteration 27860 Training loss 0.05188129469752312 Validation loss 0.059915825724601746 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9775],\n",
      "        [0.3749]])\n",
      "Iteration 27870 Training loss 0.050192635506391525 Validation loss 0.06003614887595177 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0650],\n",
      "        [0.7580]])\n",
      "Iteration 27880 Training loss 0.050979483872652054 Validation loss 0.060035087168216705 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9256],\n",
      "        [0.1380]])\n",
      "Iteration 27890 Training loss 0.04793534800410271 Validation loss 0.060010362416505814 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0029],\n",
      "        [0.8608]])\n",
      "Iteration 27900 Training loss 0.047709379345178604 Validation loss 0.05979348346590996 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2481],\n",
      "        [0.9826]])\n",
      "Iteration 27910 Training loss 0.05116340517997742 Validation loss 0.06050901859998703 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.0960],\n",
      "        [0.5734]])\n",
      "Iteration 27920 Training loss 0.05120694637298584 Validation loss 0.06002194061875343 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.5305],\n",
      "        [0.0026]])\n",
      "Iteration 27930 Training loss 0.05161541327834129 Validation loss 0.05978529527783394 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0704],\n",
      "        [0.1863]])\n",
      "Iteration 27940 Training loss 0.0530901774764061 Validation loss 0.06014617159962654 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7311],\n",
      "        [0.7646]])\n",
      "Iteration 27950 Training loss 0.04977337643504143 Validation loss 0.06010902673006058 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2310],\n",
      "        [0.0324]])\n",
      "Iteration 27960 Training loss 0.055058740079402924 Validation loss 0.06021418422460556 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8924],\n",
      "        [0.9126]])\n",
      "Iteration 27970 Training loss 0.050652116537094116 Validation loss 0.06006552278995514 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0294],\n",
      "        [0.4026]])\n",
      "Iteration 27980 Training loss 0.052470989525318146 Validation loss 0.059959523379802704 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0235],\n",
      "        [0.8850]])\n",
      "Iteration 27990 Training loss 0.05069020017981529 Validation loss 0.06009724363684654 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5216],\n",
      "        [0.1933]])\n",
      "Iteration 28000 Training loss 0.05245906859636307 Validation loss 0.06020794063806534 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5610],\n",
      "        [0.1850]])\n",
      "Iteration 28010 Training loss 0.052008360624313354 Validation loss 0.0598592534661293 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0382],\n",
      "        [0.0507]])\n",
      "Iteration 28020 Training loss 0.050971619784832 Validation loss 0.06034087762236595 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.8463],\n",
      "        [0.0311]])\n",
      "Iteration 28030 Training loss 0.050143931061029434 Validation loss 0.05983103811740875 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.6825],\n",
      "        [0.9113]])\n",
      "Iteration 28040 Training loss 0.05105803534388542 Validation loss 0.060267020016908646 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.3884],\n",
      "        [0.9955]])\n",
      "Iteration 28050 Training loss 0.05301972106099129 Validation loss 0.06053623929619789 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.7806],\n",
      "        [0.9472]])\n",
      "Iteration 28060 Training loss 0.05090786889195442 Validation loss 0.05987650156021118 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7721],\n",
      "        [0.7687]])\n",
      "Iteration 28070 Training loss 0.052653416991233826 Validation loss 0.060058921575546265 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.3966],\n",
      "        [0.9880]])\n",
      "Iteration 28080 Training loss 0.05364936590194702 Validation loss 0.05994563177227974 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0349],\n",
      "        [0.9807]])\n",
      "Iteration 28090 Training loss 0.05281734839081764 Validation loss 0.06011933088302612 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0189],\n",
      "        [0.0490]])\n",
      "Iteration 28100 Training loss 0.05079354718327522 Validation loss 0.059881120920181274 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0158],\n",
      "        [0.9298]])\n",
      "Iteration 28110 Training loss 0.05128040537238121 Validation loss 0.06010933965444565 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0401],\n",
      "        [0.9808]])\n",
      "Iteration 28120 Training loss 0.05346158519387245 Validation loss 0.06013407185673714 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9071],\n",
      "        [0.8686]])\n",
      "Iteration 28130 Training loss 0.05238673463463783 Validation loss 0.05975719541311264 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9948],\n",
      "        [0.9986]])\n",
      "Iteration 28140 Training loss 0.05131307244300842 Validation loss 0.05994568020105362 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.5915],\n",
      "        [0.1403]])\n",
      "Iteration 28150 Training loss 0.04953329265117645 Validation loss 0.05978421866893768 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0803],\n",
      "        [0.8752]])\n",
      "Iteration 28160 Training loss 0.05283992365002632 Validation loss 0.05992716923356056 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2674],\n",
      "        [0.8010]])\n",
      "Iteration 28170 Training loss 0.05581467226147652 Validation loss 0.06005449965596199 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2170],\n",
      "        [0.5753]])\n",
      "Iteration 28180 Training loss 0.05100259184837341 Validation loss 0.059833478182554245 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.3635],\n",
      "        [0.7148]])\n",
      "Iteration 28190 Training loss 0.050258390605449677 Validation loss 0.059849612414836884 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.5008],\n",
      "        [0.0163]])\n",
      "Iteration 28200 Training loss 0.0491056926548481 Validation loss 0.06003246828913689 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.9987],\n",
      "        [0.0673]])\n",
      "Iteration 28210 Training loss 0.051080938428640366 Validation loss 0.0598958358168602 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.7621],\n",
      "        [0.0183]])\n",
      "Iteration 28220 Training loss 0.04879727587103844 Validation loss 0.059877753257751465 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.5306],\n",
      "        [0.0753]])\n",
      "Iteration 28230 Training loss 0.051598288118839264 Validation loss 0.05994183570146561 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9210],\n",
      "        [0.8599]])\n",
      "Iteration 28240 Training loss 0.051930926740169525 Validation loss 0.05986516550183296 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.7910],\n",
      "        [0.9559]])\n",
      "Iteration 28250 Training loss 0.04957713186740875 Validation loss 0.059855736792087555 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2676],\n",
      "        [0.1229]])\n",
      "Iteration 28260 Training loss 0.05162980407476425 Validation loss 0.05989069491624832 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0170],\n",
      "        [0.2985]])\n",
      "Iteration 28270 Training loss 0.0504465214908123 Validation loss 0.06004352122545242 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8523],\n",
      "        [0.8314]])\n",
      "Iteration 28280 Training loss 0.05093633383512497 Validation loss 0.05992376059293747 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7842],\n",
      "        [0.8530]])\n",
      "Iteration 28290 Training loss 0.051939062774181366 Validation loss 0.05993732437491417 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0857],\n",
      "        [0.7934]])\n",
      "Iteration 28300 Training loss 0.05323145538568497 Validation loss 0.05983956903219223 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0166],\n",
      "        [0.0055]])\n",
      "Iteration 28310 Training loss 0.05079991742968559 Validation loss 0.06012531369924545 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7235],\n",
      "        [0.7506]])\n",
      "Iteration 28320 Training loss 0.04999065399169922 Validation loss 0.059899620711803436 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8823],\n",
      "        [0.9401]])\n",
      "Iteration 28330 Training loss 0.05280981585383415 Validation loss 0.05990755930542946 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0952],\n",
      "        [0.6957]])\n",
      "Iteration 28340 Training loss 0.05392108112573624 Validation loss 0.06017199158668518 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0017],\n",
      "        [0.9835]])\n",
      "Iteration 28350 Training loss 0.05102841928601265 Validation loss 0.059955134987831116 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9666],\n",
      "        [0.5788]])\n",
      "Iteration 28360 Training loss 0.05231940746307373 Validation loss 0.06007768213748932 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9051],\n",
      "        [0.9663]])\n",
      "Iteration 28370 Training loss 0.049800291657447815 Validation loss 0.05999216437339783 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9964],\n",
      "        [0.8848]])\n",
      "Iteration 28380 Training loss 0.04938975349068642 Validation loss 0.05976375564932823 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7324],\n",
      "        [0.7389]])\n",
      "Iteration 28390 Training loss 0.054188039153814316 Validation loss 0.05991050601005554 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2328],\n",
      "        [0.8599]])\n",
      "Iteration 28400 Training loss 0.04844310134649277 Validation loss 0.05994943156838417 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7207],\n",
      "        [0.1260]])\n",
      "Iteration 28410 Training loss 0.05417003110051155 Validation loss 0.0599885955452919 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2963],\n",
      "        [0.3241]])\n",
      "Iteration 28420 Training loss 0.04987882822751999 Validation loss 0.059874746948480606 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9305],\n",
      "        [0.0477]])\n",
      "Iteration 28430 Training loss 0.05317581817507744 Validation loss 0.05976133793592453 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9942],\n",
      "        [0.0403]])\n",
      "Iteration 28440 Training loss 0.04985126480460167 Validation loss 0.05984215438365936 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9215],\n",
      "        [0.2438]])\n",
      "Iteration 28450 Training loss 0.0528457872569561 Validation loss 0.05981896072626114 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.9842],\n",
      "        [0.2545]])\n",
      "Iteration 28460 Training loss 0.05069199576973915 Validation loss 0.060043297708034515 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8183],\n",
      "        [0.8237]])\n",
      "Iteration 28470 Training loss 0.049982376396656036 Validation loss 0.06057557463645935 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.9790],\n",
      "        [0.9777]])\n",
      "Iteration 28480 Training loss 0.047584373503923416 Validation loss 0.06020369753241539 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0097],\n",
      "        [0.2477]])\n",
      "Iteration 28490 Training loss 0.0534181073307991 Validation loss 0.0598396435379982 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0988],\n",
      "        [0.7686]])\n",
      "Iteration 28500 Training loss 0.05090939253568649 Validation loss 0.059839118272066116 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0587],\n",
      "        [0.5922]])\n",
      "Iteration 28510 Training loss 0.0534476637840271 Validation loss 0.05997629836201668 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9578],\n",
      "        [0.8849]])\n",
      "Iteration 28520 Training loss 0.05011575669050217 Validation loss 0.05985112115740776 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.8623],\n",
      "        [0.0581]])\n",
      "Iteration 28530 Training loss 0.052679602056741714 Validation loss 0.06000833958387375 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0781],\n",
      "        [0.0302]])\n",
      "Iteration 28540 Training loss 0.051638323813676834 Validation loss 0.060386307537555695 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.5734],\n",
      "        [0.7521]])\n",
      "Iteration 28550 Training loss 0.049278005957603455 Validation loss 0.05998208001255989 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.3437],\n",
      "        [0.0229]])\n",
      "Iteration 28560 Training loss 0.05000358819961548 Validation loss 0.060072556138038635 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1191],\n",
      "        [0.7924]])\n",
      "Iteration 28570 Training loss 0.04963601008057594 Validation loss 0.05988094583153725 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.4074],\n",
      "        [0.0165]])\n",
      "Iteration 28580 Training loss 0.05313060060143471 Validation loss 0.05997435003519058 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0742],\n",
      "        [0.0422]])\n",
      "Iteration 28590 Training loss 0.04997745528817177 Validation loss 0.059698913246393204 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.2208],\n",
      "        [0.8905]])\n",
      "Iteration 28600 Training loss 0.05448145791888237 Validation loss 0.059800803661346436 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.1759],\n",
      "        [0.1043]])\n",
      "Iteration 28610 Training loss 0.05096040666103363 Validation loss 0.05978234112262726 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1817],\n",
      "        [0.1384]])\n",
      "Iteration 28620 Training loss 0.05291051045060158 Validation loss 0.05993111431598663 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8407],\n",
      "        [0.4126]])\n",
      "Iteration 28630 Training loss 0.05455711483955383 Validation loss 0.059907376766204834 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2354],\n",
      "        [0.2449]])\n",
      "Iteration 28640 Training loss 0.05218951404094696 Validation loss 0.05986920744180679 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.7104],\n",
      "        [0.1938]])\n",
      "Iteration 28650 Training loss 0.05184933543205261 Validation loss 0.06011499464511871 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9783],\n",
      "        [0.0102]])\n",
      "Iteration 28660 Training loss 0.05373387038707733 Validation loss 0.059753693640232086 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7954],\n",
      "        [0.1435]])\n",
      "Iteration 28670 Training loss 0.0476192831993103 Validation loss 0.060097284615039825 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1055],\n",
      "        [0.8313]])\n",
      "Iteration 28680 Training loss 0.05259859189391136 Validation loss 0.05981585383415222 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8333],\n",
      "        [0.7434]])\n",
      "Iteration 28690 Training loss 0.04915665090084076 Validation loss 0.059867408126592636 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.7661],\n",
      "        [0.0158]])\n",
      "Iteration 28700 Training loss 0.0478978268802166 Validation loss 0.059885863214731216 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.8476],\n",
      "        [0.9957]])\n",
      "Iteration 28710 Training loss 0.04939379170536995 Validation loss 0.05990901216864586 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.8317],\n",
      "        [0.0057]])\n",
      "Iteration 28720 Training loss 0.052434854209423065 Validation loss 0.06005728989839554 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0169],\n",
      "        [0.0071]])\n",
      "Iteration 28730 Training loss 0.050208356231451035 Validation loss 0.05981248989701271 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.8411],\n",
      "        [0.9858]])\n",
      "Iteration 28740 Training loss 0.05248863995075226 Validation loss 0.05971451848745346 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0654],\n",
      "        [0.4745]])\n",
      "Iteration 28750 Training loss 0.05185500159859657 Validation loss 0.05987212061882019 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.9552],\n",
      "        [0.0565]])\n",
      "Iteration 28760 Training loss 0.049768880009651184 Validation loss 0.06007114052772522 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2762],\n",
      "        [0.0625]])\n",
      "Iteration 28770 Training loss 0.04919349029660225 Validation loss 0.06005242466926575 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.0577],\n",
      "        [0.5657]])\n",
      "Iteration 28780 Training loss 0.05367838591337204 Validation loss 0.05981435626745224 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.7120],\n",
      "        [0.2789]])\n",
      "Iteration 28790 Training loss 0.04992217943072319 Validation loss 0.06014873832464218 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.4240],\n",
      "        [0.9551]])\n",
      "Iteration 28800 Training loss 0.05039484426379204 Validation loss 0.06002547964453697 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0317],\n",
      "        [0.0898]])\n",
      "Iteration 28810 Training loss 0.05047262832522392 Validation loss 0.05999469757080078 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8319],\n",
      "        [0.5630]])\n",
      "Iteration 28820 Training loss 0.04955286160111427 Validation loss 0.060012079775333405 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.3404],\n",
      "        [0.4166]])\n",
      "Iteration 28830 Training loss 0.052961621433496475 Validation loss 0.059906311333179474 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.4210],\n",
      "        [0.2865]])\n",
      "Iteration 28840 Training loss 0.05080244317650795 Validation loss 0.059903595596551895 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.7145],\n",
      "        [0.9596]])\n",
      "Iteration 28850 Training loss 0.05063815042376518 Validation loss 0.05976312234997749 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8460],\n",
      "        [0.6825]])\n",
      "Iteration 28860 Training loss 0.05160893499851227 Validation loss 0.05976034328341484 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1679],\n",
      "        [0.9734]])\n",
      "Iteration 28870 Training loss 0.052252452820539474 Validation loss 0.06001242250204086 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9871],\n",
      "        [0.7970]])\n",
      "Iteration 28880 Training loss 0.051959991455078125 Validation loss 0.059917155653238297 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9037],\n",
      "        [0.9999]])\n",
      "Iteration 28890 Training loss 0.049758993089199066 Validation loss 0.05974433198571205 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0344],\n",
      "        [0.0164]])\n",
      "Iteration 28900 Training loss 0.05063266679644585 Validation loss 0.0598030760884285 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7271],\n",
      "        [0.0485]])\n",
      "Iteration 28910 Training loss 0.04968112334609032 Validation loss 0.05989984795451164 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0461],\n",
      "        [0.4065]])\n",
      "Iteration 28920 Training loss 0.04925063997507095 Validation loss 0.05989670380949974 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9564],\n",
      "        [0.0111]])\n",
      "Iteration 28930 Training loss 0.050559815019369125 Validation loss 0.06000114232301712 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.3274],\n",
      "        [0.9152]])\n",
      "Iteration 28940 Training loss 0.05241630971431732 Validation loss 0.05977160483598709 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.3005],\n",
      "        [0.0024]])\n",
      "Iteration 28950 Training loss 0.04929004982113838 Validation loss 0.060275882482528687 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0553],\n",
      "        [0.9928]])\n",
      "Iteration 28960 Training loss 0.04918964207172394 Validation loss 0.060005441308021545 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8471],\n",
      "        [0.2829]])\n",
      "Iteration 28970 Training loss 0.04748375713825226 Validation loss 0.05973830074071884 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9863],\n",
      "        [0.5765]])\n",
      "Iteration 28980 Training loss 0.051499947905540466 Validation loss 0.05987854674458504 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1277],\n",
      "        [0.5392]])\n",
      "Iteration 28990 Training loss 0.050791967660188675 Validation loss 0.05990365520119667 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9980],\n",
      "        [0.4991]])\n",
      "Iteration 29000 Training loss 0.05139920487999916 Validation loss 0.05990660563111305 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9925],\n",
      "        [0.6373]])\n",
      "Iteration 29010 Training loss 0.054367706179618835 Validation loss 0.06004635617136955 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.2769],\n",
      "        [0.9788]])\n",
      "Iteration 29020 Training loss 0.04791939631104469 Validation loss 0.06023838743567467 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9772],\n",
      "        [0.0699]])\n",
      "Iteration 29030 Training loss 0.049456529319286346 Validation loss 0.05993280187249184 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9842],\n",
      "        [0.7975]])\n",
      "Iteration 29040 Training loss 0.05194215849041939 Validation loss 0.059852611273527145 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9837],\n",
      "        [0.0491]])\n",
      "Iteration 29050 Training loss 0.04952181503176689 Validation loss 0.059995830059051514 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9973],\n",
      "        [0.9995]])\n",
      "Iteration 29060 Training loss 0.0481426939368248 Validation loss 0.05981643497943878 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9264],\n",
      "        [0.9896]])\n",
      "Iteration 29070 Training loss 0.05168195068836212 Validation loss 0.059810247272253036 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.3164],\n",
      "        [0.0071]])\n",
      "Iteration 29080 Training loss 0.04881652072072029 Validation loss 0.05978170782327652 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.7196],\n",
      "        [0.5272]])\n",
      "Iteration 29090 Training loss 0.051119331270456314 Validation loss 0.05999227985739708 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0132],\n",
      "        [0.6223]])\n",
      "Iteration 29100 Training loss 0.05327865481376648 Validation loss 0.06007962301373482 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.2058],\n",
      "        [0.4158]])\n",
      "Iteration 29110 Training loss 0.04846607893705368 Validation loss 0.05986699089407921 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.8395],\n",
      "        [0.1393]])\n",
      "Iteration 29120 Training loss 0.0515940859913826 Validation loss 0.05988847091794014 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9777],\n",
      "        [0.0852]])\n",
      "Iteration 29130 Training loss 0.05097249150276184 Validation loss 0.05977384001016617 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.5766],\n",
      "        [0.8283]])\n",
      "Iteration 29140 Training loss 0.050941307097673416 Validation loss 0.060078103095293045 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1438],\n",
      "        [0.1768]])\n",
      "Iteration 29150 Training loss 0.05249505490064621 Validation loss 0.059784747660160065 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0214],\n",
      "        [0.5741]])\n",
      "Iteration 29160 Training loss 0.05178774893283844 Validation loss 0.0601593479514122 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7650],\n",
      "        [0.4999]])\n",
      "Iteration 29170 Training loss 0.050638146698474884 Validation loss 0.060069020837545395 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5433],\n",
      "        [0.8816]])\n",
      "Iteration 29180 Training loss 0.04853826016187668 Validation loss 0.059806350618600845 Accuracy 0.8355000019073486\n",
      "Output tensor([[0.0202],\n",
      "        [0.3050]])\n",
      "Iteration 29190 Training loss 0.05039015784859657 Validation loss 0.06010059267282486 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.0447],\n",
      "        [0.2802]])\n",
      "Iteration 29200 Training loss 0.04898754879832268 Validation loss 0.05991319194436073 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.4908],\n",
      "        [0.9985]])\n",
      "Iteration 29210 Training loss 0.052460625767707825 Validation loss 0.060095835477113724 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9941],\n",
      "        [0.7793]])\n",
      "Iteration 29220 Training loss 0.051843974739313126 Validation loss 0.059772368520498276 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.8887],\n",
      "        [0.7024]])\n",
      "Iteration 29230 Training loss 0.052136123180389404 Validation loss 0.059753648936748505 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0392],\n",
      "        [0.9614]])\n",
      "Iteration 29240 Training loss 0.050847090780735016 Validation loss 0.05969793722033501 Accuracy 0.8305000066757202\n",
      "Output tensor([[0.6955],\n",
      "        [0.0176]])\n",
      "Iteration 29250 Training loss 0.05354582890868187 Validation loss 0.06030348688364029 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1386],\n",
      "        [0.1803]])\n",
      "Iteration 29260 Training loss 0.051672011613845825 Validation loss 0.05978601798415184 Accuracy 0.8355000019073486\n",
      "Output tensor([[0.4755],\n",
      "        [0.9925]])\n",
      "Iteration 29270 Training loss 0.05362307280302048 Validation loss 0.060092512518167496 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.7791],\n",
      "        [0.2692]])\n",
      "Iteration 29280 Training loss 0.04900744929909706 Validation loss 0.05980074405670166 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.5500],\n",
      "        [0.1576]])\n",
      "Iteration 29290 Training loss 0.05062316358089447 Validation loss 0.059790920466184616 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.3386],\n",
      "        [0.4001]])\n",
      "Iteration 29300 Training loss 0.05274909734725952 Validation loss 0.060061439871788025 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0103],\n",
      "        [0.7508]])\n",
      "Iteration 29310 Training loss 0.05488119646906853 Validation loss 0.059809405356645584 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.0793],\n",
      "        [0.3099]])\n",
      "Iteration 29320 Training loss 0.053182538598775864 Validation loss 0.060086749494075775 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0194],\n",
      "        [0.1021]])\n",
      "Iteration 29330 Training loss 0.050930943340063095 Validation loss 0.05986221134662628 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.3789],\n",
      "        [0.8320]])\n",
      "Iteration 29340 Training loss 0.048987068235874176 Validation loss 0.05989145487546921 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.7020],\n",
      "        [0.8649]])\n",
      "Iteration 29350 Training loss 0.05213695764541626 Validation loss 0.059756189584732056 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8152],\n",
      "        [0.6243]])\n",
      "Iteration 29360 Training loss 0.049261923879384995 Validation loss 0.06004295498132706 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.9264],\n",
      "        [0.0080]])\n",
      "Iteration 29370 Training loss 0.052171699702739716 Validation loss 0.0598098561167717 Accuracy 0.8355000019073486\n",
      "Output tensor([[0.5861],\n",
      "        [0.7664]])\n",
      "Iteration 29380 Training loss 0.053122907876968384 Validation loss 0.06016375496983528 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9358],\n",
      "        [0.9915]])\n",
      "Iteration 29390 Training loss 0.04899465665221214 Validation loss 0.060044024139642715 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8166],\n",
      "        [0.9615]])\n",
      "Iteration 29400 Training loss 0.05016680434346199 Validation loss 0.05971621721982956 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.5151],\n",
      "        [0.5829]])\n",
      "Iteration 29410 Training loss 0.05365697294473648 Validation loss 0.05986781790852547 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.7777],\n",
      "        [0.1274]])\n",
      "Iteration 29420 Training loss 0.050960808992385864 Validation loss 0.06005455181002617 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8861],\n",
      "        [0.6659]])\n",
      "Iteration 29430 Training loss 0.04729852452874184 Validation loss 0.05979960039258003 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9318],\n",
      "        [0.8303]])\n",
      "Iteration 29440 Training loss 0.05186936631798744 Validation loss 0.059695541858673096 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.9880],\n",
      "        [0.9653]])\n",
      "Iteration 29450 Training loss 0.05198226869106293 Validation loss 0.059884488582611084 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8517],\n",
      "        [0.5460]])\n",
      "Iteration 29460 Training loss 0.05125409737229347 Validation loss 0.059708964079618454 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.4447],\n",
      "        [0.1572]])\n",
      "Iteration 29470 Training loss 0.049560051411390305 Validation loss 0.059880323708057404 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.4544],\n",
      "        [0.0221]])\n",
      "Iteration 29480 Training loss 0.05252538248896599 Validation loss 0.0598665326833725 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.8477],\n",
      "        [0.9172]])\n",
      "Iteration 29490 Training loss 0.05278057977557182 Validation loss 0.06012379378080368 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.5728],\n",
      "        [0.9868]])\n",
      "Iteration 29500 Training loss 0.04999440908432007 Validation loss 0.059849392622709274 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0126],\n",
      "        [0.9980]])\n",
      "Iteration 29510 Training loss 0.05056021735072136 Validation loss 0.05964469909667969 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0960],\n",
      "        [0.7754]])\n",
      "Iteration 29520 Training loss 0.05220213904976845 Validation loss 0.059736646711826324 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0102],\n",
      "        [0.9811]])\n",
      "Iteration 29530 Training loss 0.04914505034685135 Validation loss 0.05980704352259636 Accuracy 0.8355000019073486\n",
      "Output tensor([[0.8779],\n",
      "        [0.9985]])\n",
      "Iteration 29540 Training loss 0.05054337903857231 Validation loss 0.05997604504227638 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.1199],\n",
      "        [0.0120]])\n",
      "Iteration 29550 Training loss 0.04915418475866318 Validation loss 0.05987251177430153 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.7794],\n",
      "        [0.6912]])\n",
      "Iteration 29560 Training loss 0.04971979558467865 Validation loss 0.060487885028123856 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.4153],\n",
      "        [0.9469]])\n",
      "Iteration 29570 Training loss 0.04857100173830986 Validation loss 0.059763725847005844 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.1228],\n",
      "        [0.6134]])\n",
      "Iteration 29580 Training loss 0.051937367767095566 Validation loss 0.05992481857538223 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9861],\n",
      "        [0.9321]])\n",
      "Iteration 29590 Training loss 0.05028309300541878 Validation loss 0.05985065549612045 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.3902],\n",
      "        [0.3067]])\n",
      "Iteration 29600 Training loss 0.049763068556785583 Validation loss 0.06000677868723869 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2758],\n",
      "        [0.9355]])\n",
      "Iteration 29610 Training loss 0.04933686926960945 Validation loss 0.05966060981154442 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.7415],\n",
      "        [0.2633]])\n",
      "Iteration 29620 Training loss 0.052009839564561844 Validation loss 0.059932056814432144 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.0079],\n",
      "        [0.5394]])\n",
      "Iteration 29630 Training loss 0.04974660277366638 Validation loss 0.060085784643888474 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2422],\n",
      "        [0.8049]])\n",
      "Iteration 29640 Training loss 0.05039575323462486 Validation loss 0.05988380312919617 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0305],\n",
      "        [0.3751]])\n",
      "Iteration 29650 Training loss 0.05075356364250183 Validation loss 0.059770818799734116 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0371],\n",
      "        [0.0485]])\n",
      "Iteration 29660 Training loss 0.049535155296325684 Validation loss 0.0599355474114418 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.0605],\n",
      "        [0.0308]])\n",
      "Iteration 29670 Training loss 0.052548013627529144 Validation loss 0.059916820377111435 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9521],\n",
      "        [0.0352]])\n",
      "Iteration 29680 Training loss 0.05273916572332382 Validation loss 0.05981764197349548 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.3767],\n",
      "        [0.1803]])\n",
      "Iteration 29690 Training loss 0.050730396062135696 Validation loss 0.06021834537386894 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.7060],\n",
      "        [0.4940]])\n",
      "Iteration 29700 Training loss 0.04917706549167633 Validation loss 0.05987459421157837 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2815],\n",
      "        [0.7104]])\n",
      "Iteration 29710 Training loss 0.05121558532118797 Validation loss 0.05974062532186508 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.1076],\n",
      "        [0.9862]])\n",
      "Iteration 29720 Training loss 0.05049445480108261 Validation loss 0.05977059155702591 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.5388],\n",
      "        [0.6780]])\n",
      "Iteration 29730 Training loss 0.05167221650481224 Validation loss 0.05994213744997978 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.8913],\n",
      "        [0.8091]])\n",
      "Iteration 29740 Training loss 0.04879210889339447 Validation loss 0.059959776699543 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8916],\n",
      "        [0.6405]])\n",
      "Iteration 29750 Training loss 0.049807414412498474 Validation loss 0.05987241491675377 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.9837],\n",
      "        [0.0075]])\n",
      "Iteration 29760 Training loss 0.04678026959300041 Validation loss 0.05974505469202995 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0051],\n",
      "        [0.0578]])\n",
      "Iteration 29770 Training loss 0.05115876346826553 Validation loss 0.06003823131322861 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.6028],\n",
      "        [0.7436]])\n",
      "Iteration 29780 Training loss 0.050362005829811096 Validation loss 0.05993303656578064 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0042],\n",
      "        [0.8969]])\n",
      "Iteration 29790 Training loss 0.05007876455783844 Validation loss 0.05973188951611519 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.9582],\n",
      "        [0.9427]])\n",
      "Iteration 29800 Training loss 0.05297296121716499 Validation loss 0.05977452173829079 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.1234],\n",
      "        [0.6864]])\n",
      "Iteration 29810 Training loss 0.052509915083646774 Validation loss 0.059700820595026016 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.9939],\n",
      "        [0.4374]])\n",
      "Iteration 29820 Training loss 0.048722557723522186 Validation loss 0.0599437914788723 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.5672],\n",
      "        [0.6826]])\n",
      "Iteration 29830 Training loss 0.04974466562271118 Validation loss 0.05980391055345535 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.2241],\n",
      "        [0.0936]])\n",
      "Iteration 29840 Training loss 0.050304610282182693 Validation loss 0.06019994616508484 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.2722],\n",
      "        [0.9998]])\n",
      "Iteration 29850 Training loss 0.05212714150547981 Validation loss 0.05977782979607582 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.5253],\n",
      "        [0.3392]])\n",
      "Iteration 29860 Training loss 0.05133895203471184 Validation loss 0.0597195066511631 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2356],\n",
      "        [0.5907]])\n",
      "Iteration 29870 Training loss 0.05076969042420387 Validation loss 0.05973055213689804 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8469],\n",
      "        [0.0104]])\n",
      "Iteration 29880 Training loss 0.049335550516843796 Validation loss 0.05979851260781288 Accuracy 0.8335000276565552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103929890>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.1993],\n",
      "        [0.4435]])\n",
      "Iteration 29890 Training loss 0.0501721017062664 Validation loss 0.05994461849331856 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6914],\n",
      "        [0.0078]])\n",
      "Iteration 29900 Training loss 0.04957785829901695 Validation loss 0.0599031075835228 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9800],\n",
      "        [0.6833]])\n",
      "Iteration 29910 Training loss 0.052547186613082886 Validation loss 0.059761032462120056 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0667],\n",
      "        [0.9835]])\n",
      "Iteration 29920 Training loss 0.048657868057489395 Validation loss 0.059816159307956696 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0811],\n",
      "        [0.0372]])\n",
      "Iteration 29930 Training loss 0.05574357882142067 Validation loss 0.05968594178557396 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.6141],\n",
      "        [0.0023]])\n",
      "Iteration 29940 Training loss 0.04937027394771576 Validation loss 0.059675589203834534 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.3192],\n",
      "        [0.4969]])\n",
      "Iteration 29950 Training loss 0.050739835947752 Validation loss 0.059951379895210266 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1769],\n",
      "        [0.8584]])\n",
      "Iteration 29960 Training loss 0.04996677115559578 Validation loss 0.05963398888707161 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.8563],\n",
      "        [0.4796]])\n",
      "Iteration 29970 Training loss 0.050994981080293655 Validation loss 0.05984879285097122 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.0394],\n",
      "        [0.0205]])\n",
      "Iteration 29980 Training loss 0.05421198159456253 Validation loss 0.059921156615018845 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8357],\n",
      "        [0.0230]])\n",
      "Iteration 29990 Training loss 0.05224710702896118 Validation loss 0.06033770367503166 Accuracy 0.8289999961853027\n",
      "Output tensor([[0.9757],\n",
      "        [0.8262]])\n",
      "Iteration 30000 Training loss 0.050365470349788666 Validation loss 0.060051411390304565 Accuracy 0.8309999704360962\n",
      "Output tensor([[0.1938],\n",
      "        [0.9614]])\n",
      "Iteration 30010 Training loss 0.05146580561995506 Validation loss 0.05968622863292694 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2488],\n",
      "        [0.9219]])\n",
      "Iteration 30020 Training loss 0.04947958141565323 Validation loss 0.05977935343980789 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.6021],\n",
      "        [0.5745]])\n",
      "Iteration 30030 Training loss 0.04763602465391159 Validation loss 0.0600072406232357 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9588],\n",
      "        [0.0089]])\n",
      "Iteration 30040 Training loss 0.050350140780210495 Validation loss 0.05981380492448807 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.4673],\n",
      "        [0.7309]])\n",
      "Iteration 30050 Training loss 0.04739978909492493 Validation loss 0.05973270162940025 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.3775],\n",
      "        [0.9133]])\n",
      "Iteration 30060 Training loss 0.0495813712477684 Validation loss 0.06008899584412575 Accuracy 0.8320000171661377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103929890>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.9864],\n",
      "        [0.1095]])\n",
      "Iteration 30070 Training loss 0.049350619316101074 Validation loss 0.059823039919137955 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.5366],\n",
      "        [0.1565]])\n",
      "Iteration 30080 Training loss 0.050177499651908875 Validation loss 0.05986573547124863 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0120],\n",
      "        [0.8804]])\n",
      "Iteration 30090 Training loss 0.05411107838153839 Validation loss 0.059738073498010635 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9459],\n",
      "        [0.8324]])\n",
      "Iteration 30100 Training loss 0.05046871677041054 Validation loss 0.0597769096493721 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.9860],\n",
      "        [0.8864]])\n",
      "Iteration 30110 Training loss 0.05241421237587929 Validation loss 0.05978706479072571 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0222],\n",
      "        [0.2571]])\n",
      "Iteration 30120 Training loss 0.05134635791182518 Validation loss 0.05990256369113922 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.1340],\n",
      "        [0.7556]])\n",
      "Iteration 30130 Training loss 0.053122930228710175 Validation loss 0.059787020087242126 Accuracy 0.8330000042915344\n",
      "Output tensor([[0.2782],\n",
      "        [0.8720]])\n",
      "Iteration 30140 Training loss 0.05429808050394058 Validation loss 0.06021774932742119 Accuracy 0.8299999833106995\n",
      "Output tensor([[0.1510],\n",
      "        [0.6073]])\n",
      "Iteration 30150 Training loss 0.05092332139611244 Validation loss 0.06009959429502487 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.8804],\n",
      "        [0.0235]])\n",
      "Iteration 30160 Training loss 0.050674278289079666 Validation loss 0.05967407301068306 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0234],\n",
      "        [0.8177]])\n",
      "Iteration 30170 Training loss 0.05119173601269722 Validation loss 0.05980934575200081 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0181],\n",
      "        [0.3502]])\n",
      "Iteration 30180 Training loss 0.04853789508342743 Validation loss 0.05990862101316452 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0244],\n",
      "        [0.6107]])\n",
      "Iteration 30190 Training loss 0.0487980842590332 Validation loss 0.05998554825782776 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0614],\n",
      "        [0.0249]])\n",
      "Iteration 30200 Training loss 0.05302247032523155 Validation loss 0.059897180646657944 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.7150],\n",
      "        [0.9941]])\n",
      "Iteration 30210 Training loss 0.053999222815036774 Validation loss 0.059869471937417984 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0466],\n",
      "        [0.0783]])\n",
      "Iteration 30220 Training loss 0.05128535255789757 Validation loss 0.0597391352057457 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.3398],\n",
      "        [0.4201]])\n",
      "Iteration 30230 Training loss 0.049016229808330536 Validation loss 0.06001482158899307 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7077],\n",
      "        [0.2542]])\n",
      "Iteration 30240 Training loss 0.053115472197532654 Validation loss 0.059732697904109955 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.0305],\n",
      "        [0.9132]])\n",
      "Iteration 30250 Training loss 0.049637794494628906 Validation loss 0.0599391870200634 Accuracy 0.8330000042915344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103929890>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.0210],\n",
      "        [0.9647]])\n",
      "Iteration 30260 Training loss 0.05317400395870209 Validation loss 0.06005535647273064 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8864],\n",
      "        [0.0248]])\n",
      "Iteration 30270 Training loss 0.0492987222969532 Validation loss 0.05984516814351082 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9597],\n",
      "        [0.9693]])\n",
      "Iteration 30280 Training loss 0.05032286420464516 Validation loss 0.060149673372507095 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1177],\n",
      "        [0.9902]])\n",
      "Iteration 30290 Training loss 0.049568381160497665 Validation loss 0.059737224131822586 Accuracy 0.8355000019073486\n",
      "Output tensor([[0.9729],\n",
      "        [0.9709]])\n",
      "Iteration 30300 Training loss 0.05348576605319977 Validation loss 0.05969274789094925 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.2993],\n",
      "        [0.8581]])\n",
      "Iteration 30310 Training loss 0.04832414910197258 Validation loss 0.05985065549612045 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.5431],\n",
      "        [0.4158]])\n",
      "Iteration 30320 Training loss 0.05029202997684479 Validation loss 0.059707459062337875 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.9101],\n",
      "        [0.1991]])\n",
      "Iteration 30330 Training loss 0.05280569568276405 Validation loss 0.05984049290418625 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.9216],\n",
      "        [0.3496]])\n",
      "Iteration 30340 Training loss 0.048227790743112564 Validation loss 0.05969507247209549 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.9369],\n",
      "        [0.0788]])\n",
      "Iteration 30350 Training loss 0.05176014080643654 Validation loss 0.05966748669743538 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.1325],\n",
      "        [0.2605]])\n",
      "Iteration 30360 Training loss 0.0522538423538208 Validation loss 0.05986899882555008 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.4193],\n",
      "        [0.9067]])\n",
      "Iteration 30370 Training loss 0.05110759660601616 Validation loss 0.05994725972414017 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9724],\n",
      "        [0.6365]])\n",
      "Iteration 30380 Training loss 0.05226698890328407 Validation loss 0.059626877307891846 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9929],\n",
      "        [0.2025]])\n",
      "Iteration 30390 Training loss 0.05230793356895447 Validation loss 0.06009305641055107 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.3977],\n",
      "        [0.1422]])\n",
      "Iteration 30400 Training loss 0.05187368392944336 Validation loss 0.059722959995269775 Accuracy 0.8349999785423279\n",
      "Output tensor([[0.7894],\n",
      "        [0.0199]])\n",
      "Iteration 30410 Training loss 0.052282899618148804 Validation loss 0.059623073786497116 Accuracy 0.8314999938011169\n",
      "Output tensor([[0.6994],\n",
      "        [0.8900]])\n",
      "Iteration 30420 Training loss 0.04968736693263054 Validation loss 0.05966613069176674 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.0312],\n",
      "        [0.7307]])\n",
      "Iteration 30430 Training loss 0.049185413867235184 Validation loss 0.05969620123505592 Accuracy 0.8339999914169312\n",
      "Output tensor([[0.0875],\n",
      "        [0.0022]])\n",
      "Iteration 30440 Training loss 0.051263779401779175 Validation loss 0.059628959745168686 Accuracy 0.8324999809265137\n",
      "Output tensor([[0.9999],\n",
      "        [0.8261]])\n",
      "Iteration 30450 Training loss 0.050073809921741486 Validation loss 0.059805598109960556 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9841],\n",
      "        [0.2065]])\n",
      "Iteration 30460 Training loss 0.051300618797540665 Validation loss 0.059788238257169724 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9904],\n",
      "        [0.0629]])\n",
      "Iteration 30470 Training loss 0.051369115710258484 Validation loss 0.05969232693314552 Accuracy 0.8339999914169312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbinary_model_2_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 402\u001b[39m, in \u001b[36mbinary_classification_two_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, lr_decay_rate, reg1, reg2, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2, dropout_rate)\u001b[39m\n\u001b[32m    400\u001b[39m grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n\u001b[32m    401\u001b[39m grad_z2 = grad_output*sigmoid_derivative(z2); grad_z2  = grad_z2.to(dtype) \u001b[38;5;66;03m# shape(n_data, 1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m grad_h1 = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_z2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW2\u001b[49m\u001b[43m)\u001b[49m; grad_h1  = grad_h1.to(dtype)  \u001b[38;5;66;03m# shape (n_data, hidden_1_size)\u001b[39;00m\n\u001b[32m    403\u001b[39m grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) \u001b[38;5;66;03m# shape (n_data, hidden_1_size)\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "binary_model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2.4, 1e-5, 1e8, 0, 0, 1, 0.2, 10, True, True, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08bfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer = binary_classification_three_layer_NN(1024, 512, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c674f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer.train_layers(x_train, y_train, x_valid, y_valid, 2.37, 1e-3, 1e8, 0, 0, 0, 1, 0.2, 10, True, True, True, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347eb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer_1_untrained = binary_classification_three_layer_NN(1024, 512, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3149bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2.6, 1e-2, 1e8, 0, 0, 0, 1, 0.2, 10, True, False, True, 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73751469",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_4_layer = binary_classification_four_layer_NN(1024, 512, 512, 512, eps_init = 5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bbaf678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2.42, the number of datas used for the training is 96359802.10314962 and the number of iterations is 48179.\n",
      "Iteration 0 Training loss 0.1249876394867897 Validation loss 0.12492221593856812 Accuracy 0.5\n",
      "Iteration 10 Training loss 0.1228862777352333 Validation loss 0.12223587930202484 Accuracy 0.49950000643730164\n",
      "Iteration 20 Training loss 0.11870171874761581 Validation loss 0.11876973509788513 Accuracy 0.5540000200271606\n",
      "Iteration 30 Training loss 0.11115282773971558 Validation loss 0.10967712849378586 Accuracy 0.6215000152587891\n",
      "Iteration 40 Training loss 0.10325564444065094 Validation loss 0.10162336379289627 Accuracy 0.7605000138282776\n",
      "Iteration 50 Training loss 0.09976478666067123 Validation loss 0.09856440126895905 Accuracy 0.734000027179718\n",
      "Iteration 60 Training loss 0.11774495244026184 Validation loss 0.11792701482772827 Accuracy 0.5665000081062317\n",
      "Iteration 70 Training loss 0.09569137543439865 Validation loss 0.09220574796199799 Accuracy 0.7664999961853027\n",
      "Iteration 80 Training loss 0.08850355446338654 Validation loss 0.0864395722746849 Accuracy 0.7590000033378601\n",
      "Iteration 90 Training loss 0.09502993524074554 Validation loss 0.09277605265378952 Accuracy 0.7245000004768372\n",
      "Iteration 100 Training loss 0.08809799700975418 Validation loss 0.0859128087759018 Accuracy 0.7565000057220459\n",
      "Iteration 110 Training loss 0.07436622679233551 Validation loss 0.0746038556098938 Accuracy 0.7990000247955322\n",
      "Iteration 120 Training loss 0.0774572566151619 Validation loss 0.07736863940954208 Accuracy 0.7820000052452087\n",
      "Iteration 130 Training loss 0.08300528675317764 Validation loss 0.07632838189601898 Accuracy 0.7885000109672546\n",
      "Iteration 140 Training loss 0.0826992318034172 Validation loss 0.08095733076334 Accuracy 0.7695000171661377\n",
      "Iteration 150 Training loss 0.08050326257944107 Validation loss 0.07660739868879318 Accuracy 0.7825000286102295\n",
      "Iteration 160 Training loss 0.07029097527265549 Validation loss 0.06850092858076096 Accuracy 0.8100000023841858\n",
      "Iteration 170 Training loss 0.07254841923713684 Validation loss 0.07009424269199371 Accuracy 0.809499979019165\n",
      "Iteration 180 Training loss 0.08255189657211304 Validation loss 0.07794353365898132 Accuracy 0.7785000205039978\n",
      "Iteration 190 Training loss 0.07051600515842438 Validation loss 0.06890179961919785 Accuracy 0.8075000047683716\n",
      "Iteration 200 Training loss 0.07164667546749115 Validation loss 0.06818713247776031 Accuracy 0.8109999895095825\n",
      "Iteration 210 Training loss 0.06691831350326538 Validation loss 0.0695694237947464 Accuracy 0.8069999814033508\n",
      "Iteration 220 Training loss 0.07086531817913055 Validation loss 0.06999525427818298 Accuracy 0.8075000047683716\n",
      "Iteration 230 Training loss 0.07099611312150955 Validation loss 0.0674368366599083 Accuracy 0.8109999895095825\n",
      "Iteration 240 Training loss 0.06687507778406143 Validation loss 0.06607073545455933 Accuracy 0.8144999742507935\n",
      "Iteration 250 Training loss 0.0748770534992218 Validation loss 0.07021331787109375 Accuracy 0.8040000200271606\n",
      "Iteration 260 Training loss 0.0764416977763176 Validation loss 0.07378193736076355 Accuracy 0.796500027179718\n",
      "Iteration 270 Training loss 0.07376472651958466 Validation loss 0.06958331167697906 Accuracy 0.8054999709129333\n",
      "Iteration 280 Training loss 0.07040651142597198 Validation loss 0.06689788401126862 Accuracy 0.8115000128746033\n",
      "Iteration 290 Training loss 0.06833620369434357 Validation loss 0.0689249336719513 Accuracy 0.809499979019165\n",
      "Iteration 300 Training loss 0.06695115566253662 Validation loss 0.06606224179267883 Accuracy 0.8134999871253967\n",
      "Iteration 310 Training loss 0.06524626165628433 Validation loss 0.06651964783668518 Accuracy 0.8130000233650208\n",
      "Iteration 320 Training loss 0.06283263117074966 Validation loss 0.06385843455791473 Accuracy 0.8220000267028809\n",
      "Iteration 330 Training loss 0.06495514512062073 Validation loss 0.0648404136300087 Accuracy 0.8209999799728394\n",
      "Iteration 340 Training loss 0.0730774998664856 Validation loss 0.06732980906963348 Accuracy 0.8140000104904175\n",
      "Iteration 350 Training loss 0.05910243093967438 Validation loss 0.06179128587245941 Accuracy 0.8305000066757202\n",
      "Iteration 360 Training loss 0.06632078438997269 Validation loss 0.07279430329799652 Accuracy 0.7820000052452087\n",
      "Iteration 370 Training loss 0.061267126351594925 Validation loss 0.0623650960624218 Accuracy 0.8209999799728394\n",
      "Iteration 380 Training loss 0.05768035724759102 Validation loss 0.060977302491664886 Accuracy 0.8289999961853027\n",
      "Iteration 390 Training loss 0.06333503872156143 Validation loss 0.06629831343889236 Accuracy 0.8140000104904175\n",
      "Iteration 400 Training loss 0.061451803892850876 Validation loss 0.0634516254067421 Accuracy 0.8234999775886536\n",
      "Iteration 410 Training loss 0.06458340585231781 Validation loss 0.06568332016468048 Accuracy 0.8174999952316284\n",
      "Iteration 420 Training loss 0.05621618404984474 Validation loss 0.059170469641685486 Accuracy 0.8389999866485596\n",
      "Iteration 430 Training loss 0.07664401084184647 Validation loss 0.07302455604076385 Accuracy 0.800000011920929\n",
      "Iteration 440 Training loss 0.06497738510370255 Validation loss 0.0623040646314621 Accuracy 0.8255000114440918\n",
      "Iteration 450 Training loss 0.061021510511636734 Validation loss 0.06034886837005615 Accuracy 0.8389999866485596\n",
      "Iteration 460 Training loss 0.06077123433351517 Validation loss 0.05962801352143288 Accuracy 0.8374999761581421\n",
      "Iteration 470 Training loss 0.06197940930724144 Validation loss 0.06295331567525864 Accuracy 0.8259999752044678\n",
      "Iteration 480 Training loss 0.05544780567288399 Validation loss 0.0577535480260849 Accuracy 0.8410000205039978\n",
      "Iteration 490 Training loss 0.06589692831039429 Validation loss 0.06455416977405548 Accuracy 0.8209999799728394\n",
      "Iteration 500 Training loss 0.05201292410492897 Validation loss 0.05734749138355255 Accuracy 0.8395000100135803\n",
      "Iteration 510 Training loss 0.06554369628429413 Validation loss 0.06535935401916504 Accuracy 0.8195000290870667\n",
      "Iteration 520 Training loss 0.05398811772465706 Validation loss 0.05698290839791298 Accuracy 0.843999981880188\n",
      "Iteration 530 Training loss 0.0695919319987297 Validation loss 0.07055838406085968 Accuracy 0.8044999837875366\n",
      "Iteration 540 Training loss 0.05131852626800537 Validation loss 0.05829397588968277 Accuracy 0.8399999737739563\n",
      "Iteration 550 Training loss 0.060769155621528625 Validation loss 0.061785466969013214 Accuracy 0.8299999833106995\n",
      "Iteration 560 Training loss 0.0542999804019928 Validation loss 0.05858294665813446 Accuracy 0.8374999761581421\n",
      "Iteration 570 Training loss 0.06047341227531433 Validation loss 0.061317555606365204 Accuracy 0.8295000195503235\n",
      "Iteration 580 Training loss 0.05774243175983429 Validation loss 0.058887895196676254 Accuracy 0.8335000276565552\n",
      "Iteration 590 Training loss 0.049950119107961655 Validation loss 0.05619887635111809 Accuracy 0.8489999771118164\n",
      "Iteration 600 Training loss 0.05629425868391991 Validation loss 0.060840707272291183 Accuracy 0.8289999961853027\n",
      "Iteration 610 Training loss 0.04882420226931572 Validation loss 0.05615577846765518 Accuracy 0.8460000157356262\n",
      "Iteration 620 Training loss 0.06192977353930473 Validation loss 0.06444036960601807 Accuracy 0.8220000267028809\n",
      "Iteration 630 Training loss 0.05418337136507034 Validation loss 0.05776660516858101 Accuracy 0.8364999890327454\n",
      "Iteration 640 Training loss 0.06432061642408371 Validation loss 0.06714639067649841 Accuracy 0.8169999718666077\n",
      "Iteration 650 Training loss 0.05011766776442528 Validation loss 0.05730917304754257 Accuracy 0.8379999995231628\n",
      "Iteration 660 Training loss 0.05005950480699539 Validation loss 0.05612993985414505 Accuracy 0.8460000157356262\n",
      "Iteration 670 Training loss 0.05706430599093437 Validation loss 0.06335978955030441 Accuracy 0.828000009059906\n",
      "Iteration 680 Training loss 0.05650140345096588 Validation loss 0.060221731662750244 Accuracy 0.8320000171661377\n",
      "Iteration 690 Training loss 0.04934786260128021 Validation loss 0.05394132062792778 Accuracy 0.8554999828338623\n",
      "Iteration 700 Training loss 0.04869542643427849 Validation loss 0.05541091039776802 Accuracy 0.8450000286102295\n",
      "Iteration 710 Training loss 0.04797453060746193 Validation loss 0.053737573325634 Accuracy 0.8519999980926514\n",
      "Iteration 720 Training loss 0.0522056445479393 Validation loss 0.055232033133506775 Accuracy 0.8460000157356262\n",
      "Iteration 730 Training loss 0.05526544153690338 Validation loss 0.0628376230597496 Accuracy 0.828000009059906\n",
      "Iteration 740 Training loss 0.049931757152080536 Validation loss 0.05509280785918236 Accuracy 0.843999981880188\n",
      "Iteration 750 Training loss 0.05455348640680313 Validation loss 0.05755309388041496 Accuracy 0.8395000100135803\n",
      "Iteration 760 Training loss 0.04439380764961243 Validation loss 0.053328707814216614 Accuracy 0.8519999980926514\n",
      "Iteration 770 Training loss 0.04911419004201889 Validation loss 0.05543268471956253 Accuracy 0.8454999923706055\n",
      "Iteration 780 Training loss 0.05617580935359001 Validation loss 0.061099354177713394 Accuracy 0.8324999809265137\n",
      "Iteration 790 Training loss 0.04321059212088585 Validation loss 0.052993591874837875 Accuracy 0.8525000214576721\n",
      "Iteration 800 Training loss 0.05173896625638008 Validation loss 0.059072937816381454 Accuracy 0.8370000123977661\n",
      "Iteration 810 Training loss 0.05447870492935181 Validation loss 0.05952957272529602 Accuracy 0.8345000147819519\n",
      "Iteration 820 Training loss 0.053136471658945084 Validation loss 0.05768684670329094 Accuracy 0.8410000205039978\n",
      "Iteration 830 Training loss 0.04254081845283508 Validation loss 0.05142604187130928 Accuracy 0.8569999933242798\n",
      "Iteration 840 Training loss 0.05265069380402565 Validation loss 0.0617956705391407 Accuracy 0.8345000147819519\n",
      "Iteration 850 Training loss 0.048909157514572144 Validation loss 0.05589614808559418 Accuracy 0.843999981880188\n",
      "Iteration 860 Training loss 0.04448244348168373 Validation loss 0.055075980722904205 Accuracy 0.8485000133514404\n",
      "Iteration 870 Training loss 0.04089632257819176 Validation loss 0.052322324365377426 Accuracy 0.8554999828338623\n",
      "Iteration 880 Training loss 0.04384346306324005 Validation loss 0.054137762635946274 Accuracy 0.8489999771118164\n",
      "Iteration 890 Training loss 0.04446607083082199 Validation loss 0.05483153462409973 Accuracy 0.843500018119812\n",
      "Iteration 900 Training loss 0.0505889393389225 Validation loss 0.057441435754299164 Accuracy 0.8414999842643738\n",
      "Iteration 910 Training loss 0.04009684920310974 Validation loss 0.05095391348004341 Accuracy 0.8610000014305115\n",
      "Iteration 920 Training loss 0.05238278955221176 Validation loss 0.058858782052993774 Accuracy 0.840499997138977\n",
      "Iteration 930 Training loss 0.039959732443094254 Validation loss 0.05198432132601738 Accuracy 0.8550000190734863\n",
      "Iteration 940 Training loss 0.03575549274682999 Validation loss 0.05093058943748474 Accuracy 0.859000027179718\n",
      "Iteration 950 Training loss 0.04365589842200279 Validation loss 0.05246637761592865 Accuracy 0.8550000190734863\n",
      "Iteration 960 Training loss 0.034736305475234985 Validation loss 0.04836246371269226 Accuracy 0.8644999861717224\n",
      "Iteration 970 Training loss 0.06455240398645401 Validation loss 0.07111269235610962 Accuracy 0.8065000176429749\n",
      "Iteration 980 Training loss 0.13145983219146729 Validation loss 0.13474637269973755 Accuracy 0.6629999876022339\n",
      "Iteration 990 Training loss 0.04476960003376007 Validation loss 0.054056763648986816 Accuracy 0.8464999794960022\n",
      "Iteration 1000 Training loss 0.046750884503126144 Validation loss 0.05708150565624237 Accuracy 0.8399999737739563\n",
      "Iteration 1010 Training loss 0.036546189337968826 Validation loss 0.05226108059287071 Accuracy 0.8495000004768372\n",
      "Iteration 1020 Training loss 0.04168541729450226 Validation loss 0.05480683594942093 Accuracy 0.847000002861023\n",
      "Iteration 1030 Training loss 0.045842643827199936 Validation loss 0.06094267964363098 Accuracy 0.8295000195503235\n",
      "Iteration 1040 Training loss 0.03744331747293472 Validation loss 0.05151692032814026 Accuracy 0.8550000190734863\n",
      "Iteration 1050 Training loss 0.03774772584438324 Validation loss 0.05452844873070717 Accuracy 0.8514999747276306\n",
      "Iteration 1060 Training loss 0.04090259224176407 Validation loss 0.056526925414800644 Accuracy 0.8420000076293945\n",
      "Iteration 1070 Training loss 0.03371448442339897 Validation loss 0.05056124925613403 Accuracy 0.8575000166893005\n",
      "Iteration 1080 Training loss 0.04434928297996521 Validation loss 0.058623846620321274 Accuracy 0.8349999785423279\n",
      "Iteration 1090 Training loss 0.03840402513742447 Validation loss 0.05294596403837204 Accuracy 0.8525000214576721\n",
      "Iteration 1100 Training loss 0.04074966534972191 Validation loss 0.05776497721672058 Accuracy 0.8410000205039978\n",
      "Iteration 1110 Training loss 0.036467645317316055 Validation loss 0.051860466599464417 Accuracy 0.8560000061988831\n",
      "Iteration 1120 Training loss 0.03991660103201866 Validation loss 0.055886801332235336 Accuracy 0.8429999947547913\n",
      "Iteration 1130 Training loss 0.039928123354911804 Validation loss 0.0576719269156456 Accuracy 0.8395000100135803\n",
      "Iteration 1140 Training loss 0.034782085567712784 Validation loss 0.052925195544958115 Accuracy 0.8519999980926514\n",
      "Iteration 1150 Training loss 0.033514611423015594 Validation loss 0.053658243268728256 Accuracy 0.8525000214576721\n",
      "Iteration 1160 Training loss 0.042892422527074814 Validation loss 0.06004396080970764 Accuracy 0.8289999961853027\n",
      "Iteration 1170 Training loss 0.03772618621587753 Validation loss 0.054179392755031586 Accuracy 0.8485000133514404\n",
      "Iteration 1180 Training loss 0.03295525535941124 Validation loss 0.0515005961060524 Accuracy 0.8539999723434448\n",
      "Iteration 1190 Training loss 0.03454377129673958 Validation loss 0.05510105565190315 Accuracy 0.847000002861023\n",
      "Iteration 1200 Training loss 0.03186378628015518 Validation loss 0.049640994518995285 Accuracy 0.8619999885559082\n",
      "Iteration 1210 Training loss 0.04500732198357582 Validation loss 0.06178910657763481 Accuracy 0.8209999799728394\n",
      "Iteration 1220 Training loss 0.028765792027115822 Validation loss 0.049534834921360016 Accuracy 0.8659999966621399\n",
      "Iteration 1230 Training loss 0.03981870785355568 Validation loss 0.05714459717273712 Accuracy 0.8389999866485596\n",
      "Iteration 1240 Training loss 0.0365733839571476 Validation loss 0.05300043150782585 Accuracy 0.8539999723434448\n",
      "Iteration 1250 Training loss 0.0264142993837595 Validation loss 0.04875070974230766 Accuracy 0.8690000176429749\n",
      "Iteration 1260 Training loss 0.030279403552412987 Validation loss 0.05496800318360329 Accuracy 0.8464999794960022\n",
      "Iteration 1270 Training loss 0.030079200863838196 Validation loss 0.051412370055913925 Accuracy 0.8579999804496765\n",
      "Iteration 1280 Training loss 0.047548964619636536 Validation loss 0.06595016270875931 Accuracy 0.8195000290870667\n",
      "Iteration 1290 Training loss 0.10399912297725677 Validation loss 0.11022384464740753 Accuracy 0.7139999866485596\n",
      "Iteration 1300 Training loss 0.03040681593120098 Validation loss 0.04653798043727875 Accuracy 0.8730000257492065\n",
      "Iteration 1310 Training loss 0.030304623767733574 Validation loss 0.04989652708172798 Accuracy 0.8644999861717224\n",
      "Iteration 1320 Training loss 0.03470495715737343 Validation loss 0.056268807500600815 Accuracy 0.8414999842643738\n",
      "Iteration 1330 Training loss 0.025903671979904175 Validation loss 0.04642175883054733 Accuracy 0.8759999871253967\n",
      "Iteration 1340 Training loss 0.02291625551879406 Validation loss 0.046666219830513 Accuracy 0.8734999895095825\n",
      "Iteration 1350 Training loss 0.050956841558218 Validation loss 0.0694030150771141 Accuracy 0.800000011920929\n",
      "Iteration 1360 Training loss 0.027718733996152878 Validation loss 0.04815511777997017 Accuracy 0.8690000176429749\n",
      "Iteration 1370 Training loss 0.023810623213648796 Validation loss 0.04609829932451248 Accuracy 0.8744999766349792\n",
      "Iteration 1380 Training loss 0.023614659905433655 Validation loss 0.0480557344853878 Accuracy 0.8725000023841858\n",
      "Iteration 1390 Training loss 0.03321221470832825 Validation loss 0.05698966607451439 Accuracy 0.8424999713897705\n",
      "Iteration 1400 Training loss 0.02294180728495121 Validation loss 0.047707948833703995 Accuracy 0.8694999814033508\n",
      "Iteration 1410 Training loss 0.03276573866605759 Validation loss 0.05757129564881325 Accuracy 0.8420000076293945\n",
      "Iteration 1420 Training loss 0.0213285181671381 Validation loss 0.04673844948410988 Accuracy 0.8730000257492065\n",
      "Iteration 1430 Training loss 0.03523573651909828 Validation loss 0.05635324865579605 Accuracy 0.8504999876022339\n",
      "Iteration 1440 Training loss 0.022229092195630074 Validation loss 0.04746890440583229 Accuracy 0.8715000152587891\n",
      "Iteration 1450 Training loss 0.02291724644601345 Validation loss 0.04934583604335785 Accuracy 0.8650000095367432\n",
      "Iteration 1460 Training loss 0.04088965803384781 Validation loss 0.06681253015995026 Accuracy 0.8149999976158142\n",
      "Iteration 1470 Training loss 0.10014910250902176 Validation loss 0.11035184562206268 Accuracy 0.7250000238418579\n",
      "Iteration 1480 Training loss 0.023240985348820686 Validation loss 0.04678264632821083 Accuracy 0.8740000128746033\n",
      "Iteration 1490 Training loss 0.019293412566184998 Validation loss 0.04529115930199623 Accuracy 0.875\n",
      "Iteration 1500 Training loss 0.02268998697400093 Validation loss 0.050913918763399124 Accuracy 0.8619999885559082\n",
      "Iteration 1510 Training loss 0.018197862431406975 Validation loss 0.04394645616412163 Accuracy 0.8794999718666077\n",
      "Iteration 1520 Training loss 0.024476466700434685 Validation loss 0.05023902654647827 Accuracy 0.8604999780654907\n",
      "Iteration 1530 Training loss 0.01865047961473465 Validation loss 0.04577719047665596 Accuracy 0.8744999766349792\n",
      "Iteration 1540 Training loss 0.015537948347628117 Validation loss 0.045038092881441116 Accuracy 0.8784999847412109\n",
      "Iteration 1550 Training loss 0.024936378002166748 Validation loss 0.05522124841809273 Accuracy 0.8489999771118164\n",
      "Iteration 1560 Training loss 0.017003970220685005 Validation loss 0.045382194221019745 Accuracy 0.8774999976158142\n",
      "Iteration 1570 Training loss 0.019509311765432358 Validation loss 0.04867801442742348 Accuracy 0.8675000071525574\n",
      "Iteration 1580 Training loss 0.024662284180521965 Validation loss 0.0520491860806942 Accuracy 0.8544999957084656\n",
      "Iteration 1590 Training loss 0.014697873964905739 Validation loss 0.04363193362951279 Accuracy 0.8809999823570251\n",
      "Iteration 1600 Training loss 0.025232376530766487 Validation loss 0.05517561733722687 Accuracy 0.847000002861023\n",
      "Iteration 1610 Training loss 0.0628734901547432 Validation loss 0.08401376008987427 Accuracy 0.781000018119812\n",
      "Iteration 1620 Training loss 0.08588435500860214 Validation loss 0.09286738932132721 Accuracy 0.7559999823570251\n",
      "Iteration 1630 Training loss 0.022108610719442368 Validation loss 0.04467045143246651 Accuracy 0.8799999952316284\n",
      "Iteration 1640 Training loss 0.019191574305295944 Validation loss 0.04623217135667801 Accuracy 0.8765000104904175\n",
      "Iteration 1650 Training loss 0.021393535658717155 Validation loss 0.04670431837439537 Accuracy 0.8744999766349792\n",
      "Iteration 1660 Training loss 0.01612296886742115 Validation loss 0.04336028918623924 Accuracy 0.8824999928474426\n",
      "Iteration 1670 Training loss 0.03710437938570976 Validation loss 0.060289036482572556 Accuracy 0.8364999890327454\n",
      "Iteration 1680 Training loss 0.015291061252355576 Validation loss 0.043255630880594254 Accuracy 0.8830000162124634\n",
      "Iteration 1690 Training loss 0.011618332006037235 Validation loss 0.04369828850030899 Accuracy 0.8769999742507935\n",
      "Iteration 1700 Training loss 0.021281307563185692 Validation loss 0.05219654738903046 Accuracy 0.8604999780654907\n",
      "Iteration 1710 Training loss 0.017299218103289604 Validation loss 0.04630039259791374 Accuracy 0.8715000152587891\n",
      "Iteration 1720 Training loss 0.012492412701249123 Validation loss 0.043989285826683044 Accuracy 0.8809999823570251\n",
      "Iteration 1730 Training loss 0.02535218745470047 Validation loss 0.057251930236816406 Accuracy 0.8450000286102295\n",
      "Iteration 1740 Training loss 0.014434931799769402 Validation loss 0.045901939272880554 Accuracy 0.8774999976158142\n",
      "Iteration 1750 Training loss 0.012004111893475056 Validation loss 0.04348420724272728 Accuracy 0.8809999823570251\n",
      "Iteration 1760 Training loss 0.010717564262449741 Validation loss 0.04288048669695854 Accuracy 0.8834999799728394\n",
      "Iteration 1770 Training loss 0.011077926494181156 Validation loss 0.04388052225112915 Accuracy 0.8790000081062317\n",
      "Iteration 1780 Training loss 0.04210831969976425 Validation loss 0.06601554155349731 Accuracy 0.8240000009536743\n",
      "Iteration 1790 Training loss 0.040417466312646866 Validation loss 0.058125343173742294 Accuracy 0.8424999713897705\n",
      "Iteration 1800 Training loss 0.030671587213873863 Validation loss 0.052781131118535995 Accuracy 0.8565000295639038\n",
      "Iteration 1810 Training loss 0.017471207305788994 Validation loss 0.0475752055644989 Accuracy 0.8725000023841858\n",
      "Iteration 1820 Training loss 0.013824580237269402 Validation loss 0.04648469388484955 Accuracy 0.871999979019165\n",
      "Iteration 1830 Training loss 0.011102642863988876 Validation loss 0.043928373605012894 Accuracy 0.8840000033378601\n",
      "Iteration 1840 Training loss 0.020061487331986427 Validation loss 0.05428113788366318 Accuracy 0.8539999723434448\n",
      "Iteration 1850 Training loss 0.01055489294230938 Validation loss 0.04389653727412224 Accuracy 0.8865000009536743\n",
      "Iteration 1860 Training loss 0.007923712953925133 Validation loss 0.042937345802783966 Accuracy 0.8870000243186951\n",
      "Iteration 1870 Training loss 0.008665325120091438 Validation loss 0.04309094324707985 Accuracy 0.8855000138282776\n",
      "Iteration 1880 Training loss 0.007691794540733099 Validation loss 0.042687125504016876 Accuracy 0.8880000114440918\n",
      "Iteration 1890 Training loss 0.009207207709550858 Validation loss 0.042954280972480774 Accuracy 0.8870000243186951\n",
      "Iteration 1900 Training loss 0.008862335234880447 Validation loss 0.04296913370490074 Accuracy 0.8895000219345093\n",
      "Iteration 1910 Training loss 0.007434762082993984 Validation loss 0.042753640562295914 Accuracy 0.8840000033378601\n",
      "Iteration 1920 Training loss 0.17229191958904266 Validation loss 0.17878304421901703 Accuracy 0.5690000057220459\n",
      "Iteration 1930 Training loss 0.08948202431201935 Validation loss 0.09399296343326569 Accuracy 0.762499988079071\n",
      "Iteration 1940 Training loss 0.01568431779742241 Validation loss 0.046035461127758026 Accuracy 0.8675000071525574\n",
      "Iteration 1950 Training loss 0.012877492234110832 Validation loss 0.04547415301203728 Accuracy 0.8709999918937683\n",
      "Iteration 1960 Training loss 0.010891851037740707 Validation loss 0.04430793598294258 Accuracy 0.8805000185966492\n",
      "Iteration 1970 Training loss 0.021234694868326187 Validation loss 0.05206042900681496 Accuracy 0.8569999933242798\n",
      "Iteration 1980 Training loss 0.009493452496826649 Validation loss 0.043957967311143875 Accuracy 0.8799999952316284\n",
      "Iteration 1990 Training loss 0.009818704798817635 Validation loss 0.04317178949713707 Accuracy 0.8840000033378601\n",
      "Iteration 2000 Training loss 0.00885169580578804 Validation loss 0.04309499263763428 Accuracy 0.8855000138282776\n",
      "Iteration 2010 Training loss 0.006379671860486269 Validation loss 0.04307747259736061 Accuracy 0.8815000057220459\n",
      "Iteration 2020 Training loss 0.01109988708049059 Validation loss 0.04726327210664749 Accuracy 0.8759999871253967\n",
      "Iteration 2030 Training loss 0.1338987499475479 Validation loss 0.13958591222763062 Accuracy 0.6784999966621399\n",
      "Iteration 2040 Training loss 0.02725965343415737 Validation loss 0.054333943873643875 Accuracy 0.847000002861023\n",
      "Iteration 2050 Training loss 0.011436562985181808 Validation loss 0.04455844312906265 Accuracy 0.8794999718666077\n",
      "Iteration 2060 Training loss 0.009449769742786884 Validation loss 0.04320913553237915 Accuracy 0.8855000138282776\n",
      "Iteration 2070 Training loss 0.007317329291254282 Validation loss 0.04248303174972534 Accuracy 0.8824999928474426\n",
      "Iteration 2080 Training loss 0.008232456631958485 Validation loss 0.0427580252289772 Accuracy 0.8859999775886536\n",
      "Iteration 2090 Training loss 0.006767441518604755 Validation loss 0.043100785464048386 Accuracy 0.8855000138282776\n",
      "Iteration 2100 Training loss 0.006061449181288481 Validation loss 0.04358014464378357 Accuracy 0.8870000243186951\n",
      "Iteration 2110 Training loss 0.056205663830041885 Validation loss 0.07544180750846863 Accuracy 0.8119999766349792\n",
      "Iteration 2120 Training loss 0.03723364323377609 Validation loss 0.06500769406557083 Accuracy 0.8199999928474426\n",
      "Iteration 2130 Training loss 0.011309720575809479 Validation loss 0.04314631596207619 Accuracy 0.8824999928474426\n",
      "Iteration 2140 Training loss 0.008802279829978943 Validation loss 0.04307487607002258 Accuracy 0.8824999928474426\n",
      "Iteration 2150 Training loss 0.005393350962549448 Validation loss 0.042856402695178986 Accuracy 0.8849999904632568\n",
      "Iteration 2160 Training loss 0.005827795248478651 Validation loss 0.04278517886996269 Accuracy 0.887499988079071\n",
      "Iteration 2170 Training loss 0.005861079320311546 Validation loss 0.04238586127758026 Accuracy 0.8859999775886536\n",
      "Iteration 2180 Training loss 0.005553523078560829 Validation loss 0.04291575029492378 Accuracy 0.8859999775886536\n",
      "Iteration 2190 Training loss 0.006383966654539108 Validation loss 0.04293455556035042 Accuracy 0.8840000033378601\n",
      "Iteration 2200 Training loss 0.00521474564447999 Validation loss 0.043094176799058914 Accuracy 0.8845000267028809\n",
      "Iteration 2210 Training loss 0.0038817967288196087 Validation loss 0.04250882565975189 Accuracy 0.8865000009536743\n",
      "Iteration 2220 Training loss 0.0041669090278446674 Validation loss 0.042383164167404175 Accuracy 0.8895000219345093\n",
      "Iteration 2230 Training loss 0.004314891993999481 Validation loss 0.04215097054839134 Accuracy 0.8939999938011169\n",
      "Iteration 2240 Training loss 0.004463608376681805 Validation loss 0.04204913601279259 Accuracy 0.890999972820282\n",
      "Iteration 2250 Training loss 0.0036324418615549803 Validation loss 0.04220645874738693 Accuracy 0.8920000195503235\n",
      "Iteration 2260 Training loss 0.006149595137685537 Validation loss 0.04358517378568649 Accuracy 0.8830000162124634\n",
      "Iteration 2270 Training loss 0.0038908247370272875 Validation loss 0.04214094206690788 Accuracy 0.8899999856948853\n",
      "Iteration 2280 Training loss 0.0032666511833667755 Validation loss 0.042399369180202484 Accuracy 0.8889999985694885\n",
      "Iteration 2290 Training loss 0.004216968081891537 Validation loss 0.04244997352361679 Accuracy 0.8899999856948853\n",
      "Iteration 2300 Training loss 0.004608772695064545 Validation loss 0.043712202459573746 Accuracy 0.8855000138282776\n",
      "Iteration 2310 Training loss 0.004293205216526985 Validation loss 0.042152244597673416 Accuracy 0.8924999833106995\n",
      "Iteration 2320 Training loss 0.002641721162945032 Validation loss 0.042570535093545914 Accuracy 0.8895000219345093\n",
      "Iteration 2330 Training loss 0.0038915907498449087 Validation loss 0.04226172715425491 Accuracy 0.8920000195503235\n",
      "Iteration 2340 Training loss 0.003024044446647167 Validation loss 0.04214146360754967 Accuracy 0.8895000219345093\n",
      "Iteration 2350 Training loss 0.00380336819216609 Validation loss 0.042469192296266556 Accuracy 0.887499988079071\n",
      "Iteration 2360 Training loss 0.002648910041898489 Validation loss 0.04233486205339432 Accuracy 0.890500009059906\n",
      "Iteration 2370 Training loss 0.0033814753405749798 Validation loss 0.04235183820128441 Accuracy 0.8920000195503235\n",
      "Iteration 2380 Training loss 0.005339296534657478 Validation loss 0.04241916909813881 Accuracy 0.890500009059906\n",
      "Iteration 2390 Training loss 0.0031859641894698143 Validation loss 0.042442403733730316 Accuracy 0.8884999752044678\n",
      "Iteration 2400 Training loss 0.002241722075268626 Validation loss 0.04322488605976105 Accuracy 0.8884999752044678\n",
      "Iteration 2410 Training loss 0.0027492798399180174 Validation loss 0.042462218552827835 Accuracy 0.8884999752044678\n",
      "Iteration 2420 Training loss 0.0021819628309458494 Validation loss 0.04303760454058647 Accuracy 0.8889999985694885\n",
      "Iteration 2430 Training loss 0.002480501076206565 Validation loss 0.042266275733709335 Accuracy 0.890500009059906\n",
      "Iteration 2440 Training loss 0.0029265412595123053 Validation loss 0.04242032766342163 Accuracy 0.8934999704360962\n",
      "Iteration 2450 Training loss 0.0027253052685409784 Validation loss 0.04241812974214554 Accuracy 0.890500009059906\n",
      "Iteration 2460 Training loss 0.0031377787236124277 Validation loss 0.04260788857936859 Accuracy 0.8884999752044678\n",
      "Iteration 2470 Training loss 0.002296139020472765 Validation loss 0.04236343130469322 Accuracy 0.8899999856948853\n",
      "Iteration 2480 Training loss 0.0025834431871771812 Validation loss 0.04259609431028366 Accuracy 0.887499988079071\n",
      "Iteration 2490 Training loss 0.002202595118433237 Validation loss 0.0424782931804657 Accuracy 0.8880000114440918\n",
      "Iteration 2500 Training loss 0.0032601498533040285 Validation loss 0.0426270067691803 Accuracy 0.8899999856948853\n",
      "Iteration 2510 Training loss 0.002470581792294979 Validation loss 0.042364347726106644 Accuracy 0.8895000219345093\n",
      "Iteration 2520 Training loss 0.0027438902761787176 Validation loss 0.04258400946855545 Accuracy 0.8899999856948853\n",
      "Iteration 2530 Training loss 0.002299819141626358 Validation loss 0.04265207052230835 Accuracy 0.8884999752044678\n",
      "Iteration 2540 Training loss 0.002962601836770773 Validation loss 0.042644426226615906 Accuracy 0.8889999985694885\n",
      "Iteration 2550 Training loss 0.0013657191302627325 Validation loss 0.04239495098590851 Accuracy 0.8914999961853027\n",
      "Iteration 2560 Training loss 0.002408551285043359 Validation loss 0.042976267635822296 Accuracy 0.8899999856948853\n",
      "Iteration 2570 Training loss 0.0032480161171406507 Validation loss 0.0424155592918396 Accuracy 0.890999972820282\n",
      "Iteration 2580 Training loss 0.001741338986903429 Validation loss 0.0428834930062294 Accuracy 0.8889999985694885\n",
      "Iteration 2590 Training loss 0.0030597462318837643 Validation loss 0.04310072213411331 Accuracy 0.890999972820282\n",
      "Iteration 2600 Training loss 0.002339612925425172 Validation loss 0.04255813732743263 Accuracy 0.8914999961853027\n",
      "Iteration 2610 Training loss 0.0021769010927528143 Validation loss 0.042642828077077866 Accuracy 0.890999972820282\n",
      "Iteration 2620 Training loss 0.0017275010468438268 Validation loss 0.04265844076871872 Accuracy 0.8920000195503235\n",
      "Iteration 2630 Training loss 0.002329230308532715 Validation loss 0.04255169257521629 Accuracy 0.8895000219345093\n",
      "Iteration 2640 Training loss 0.0026692410465329885 Validation loss 0.042583879083395004 Accuracy 0.8920000195503235\n",
      "Iteration 2650 Training loss 0.002857704646885395 Validation loss 0.042754754424095154 Accuracy 0.8924999833106995\n",
      "Iteration 2660 Training loss 0.001675947685725987 Validation loss 0.04255863279104233 Accuracy 0.890999972820282\n",
      "Iteration 2670 Training loss 0.0017553241923451424 Validation loss 0.04274211451411247 Accuracy 0.8920000195503235\n",
      "Iteration 2680 Training loss 0.002420088741928339 Validation loss 0.04246394336223602 Accuracy 0.8914999961853027\n",
      "Iteration 2690 Training loss 0.002237520646303892 Validation loss 0.04287780448794365 Accuracy 0.8914999961853027\n",
      "Iteration 2700 Training loss 0.0012528692604973912 Validation loss 0.04267272725701332 Accuracy 0.8924999833106995\n",
      "Iteration 2710 Training loss 0.0014521961566060781 Validation loss 0.0427871011197567 Accuracy 0.8930000066757202\n",
      "Iteration 2720 Training loss 0.0031132635194808245 Validation loss 0.04283219948410988 Accuracy 0.8899999856948853\n",
      "Iteration 2730 Training loss 0.0014308617683127522 Validation loss 0.04257823899388313 Accuracy 0.8924999833106995\n",
      "Iteration 2740 Training loss 0.0016353682149201632 Validation loss 0.04280589148402214 Accuracy 0.890500009059906\n",
      "Iteration 2750 Training loss 0.0028652783948928118 Validation loss 0.04284333437681198 Accuracy 0.8914999961853027\n",
      "Iteration 2760 Training loss 0.002415842842310667 Validation loss 0.042549870908260345 Accuracy 0.8920000195503235\n",
      "Iteration 2770 Training loss 0.002855312544852495 Validation loss 0.042764559388160706 Accuracy 0.8884999752044678\n",
      "Iteration 2780 Training loss 0.0014870248269289732 Validation loss 0.042926859110593796 Accuracy 0.890999972820282\n",
      "Iteration 2790 Training loss 0.0013467612443491817 Validation loss 0.04284815862774849 Accuracy 0.8914999961853027\n",
      "Iteration 2800 Training loss 0.0022730857599526644 Validation loss 0.04281298443675041 Accuracy 0.890999972820282\n",
      "Iteration 2810 Training loss 0.002016139915212989 Validation loss 0.04305857792496681 Accuracy 0.8914999961853027\n",
      "Iteration 2820 Training loss 0.003917383961379528 Validation loss 0.043064095079898834 Accuracy 0.8920000195503235\n",
      "Iteration 2830 Training loss 0.0020302690099924803 Validation loss 0.04289422556757927 Accuracy 0.8914999961853027\n",
      "Iteration 2840 Training loss 0.0016542840749025345 Validation loss 0.04292481392621994 Accuracy 0.8914999961853027\n",
      "Iteration 2850 Training loss 0.0005465729045681655 Validation loss 0.04293408244848251 Accuracy 0.890999972820282\n",
      "Iteration 2860 Training loss 0.0022166103590279818 Validation loss 0.043008338660001755 Accuracy 0.8899999856948853\n",
      "Iteration 2870 Training loss 0.0020077533554285765 Validation loss 0.04300277680158615 Accuracy 0.8920000195503235\n",
      "Iteration 2880 Training loss 0.001991129945963621 Validation loss 0.043126627802848816 Accuracy 0.890999972820282\n",
      "Iteration 2890 Training loss 0.0017658693250268698 Validation loss 0.04316452890634537 Accuracy 0.8895000219345093\n",
      "Iteration 2900 Training loss 0.0007971786544658244 Validation loss 0.04301214590668678 Accuracy 0.8884999752044678\n",
      "Iteration 2910 Training loss 0.0018202997744083405 Validation loss 0.043083734810352325 Accuracy 0.890500009059906\n",
      "Iteration 2920 Training loss 0.0013158762594684958 Validation loss 0.04327933117747307 Accuracy 0.8889999985694885\n",
      "Iteration 2930 Training loss 0.001211709575727582 Validation loss 0.04312489554286003 Accuracy 0.8914999961853027\n",
      "Iteration 2940 Training loss 0.0018730515148490667 Validation loss 0.043138351291418076 Accuracy 0.8899999856948853\n",
      "Iteration 2950 Training loss 0.0015376636292785406 Validation loss 0.04318585246801376 Accuracy 0.8895000219345093\n",
      "Iteration 2960 Training loss 0.0018282871460542083 Validation loss 0.04303852841258049 Accuracy 0.8889999985694885\n",
      "Iteration 2970 Training loss 0.0010637915693223476 Validation loss 0.04290793836116791 Accuracy 0.890500009059906\n",
      "Iteration 2980 Training loss 0.0010195656213909388 Validation loss 0.043280601501464844 Accuracy 0.8899999856948853\n",
      "Iteration 2990 Training loss 0.0022591124288737774 Validation loss 0.04323502257466316 Accuracy 0.887499988079071\n",
      "Iteration 3000 Training loss 0.0015496390406042337 Validation loss 0.043156158179044724 Accuracy 0.8884999752044678\n",
      "Iteration 3010 Training loss 0.0022345934994518757 Validation loss 0.0431382991373539 Accuracy 0.8880000114440918\n",
      "Iteration 3020 Training loss 0.001956375315785408 Validation loss 0.042915865778923035 Accuracy 0.8880000114440918\n",
      "Iteration 3030 Training loss 0.0013859124155715108 Validation loss 0.0432344451546669 Accuracy 0.890500009059906\n",
      "Iteration 3040 Training loss 0.001976402709260583 Validation loss 0.0430140495300293 Accuracy 0.8880000114440918\n",
      "Iteration 3050 Training loss 0.0012132978299632668 Validation loss 0.04326600208878517 Accuracy 0.8880000114440918\n",
      "Iteration 3060 Training loss 0.0017390617867931724 Validation loss 0.04316892847418785 Accuracy 0.8889999985694885\n",
      "Iteration 3070 Training loss 0.0011589503847062588 Validation loss 0.04310072958469391 Accuracy 0.890999972820282\n",
      "Iteration 3080 Training loss 0.0014918260276317596 Validation loss 0.043117303401231766 Accuracy 0.890999972820282\n",
      "Iteration 3090 Training loss 0.0015472086379304528 Validation loss 0.04318836331367493 Accuracy 0.8884999752044678\n",
      "Iteration 3100 Training loss 0.0007484895177185535 Validation loss 0.04323403537273407 Accuracy 0.887499988079071\n",
      "Iteration 3110 Training loss 0.0016206231666728854 Validation loss 0.04322251304984093 Accuracy 0.8884999752044678\n",
      "Iteration 3120 Training loss 0.0014293664135038853 Validation loss 0.04334486275911331 Accuracy 0.8889999985694885\n",
      "Iteration 3130 Training loss 0.0019176371861249208 Validation loss 0.043317411094903946 Accuracy 0.8889999985694885\n",
      "Iteration 3140 Training loss 0.0008776632021181285 Validation loss 0.04333014041185379 Accuracy 0.8899999856948853\n",
      "Iteration 3150 Training loss 0.000644572835881263 Validation loss 0.043330155313014984 Accuracy 0.8895000219345093\n",
      "Iteration 3160 Training loss 0.0019112813752144575 Validation loss 0.043455637991428375 Accuracy 0.890500009059906\n",
      "Iteration 3170 Training loss 0.0019491006387397647 Validation loss 0.04317475110292435 Accuracy 0.8895000219345093\n",
      "Iteration 3180 Training loss 0.0008612727397121489 Validation loss 0.04324452579021454 Accuracy 0.8870000243186951\n",
      "Iteration 3190 Training loss 0.0011762231588363647 Validation loss 0.04329294711351395 Accuracy 0.8889999985694885\n",
      "Iteration 3200 Training loss 0.0013263493310660124 Validation loss 0.043306510895490646 Accuracy 0.887499988079071\n",
      "Iteration 3210 Training loss 0.0011313908034935594 Validation loss 0.04322725534439087 Accuracy 0.887499988079071\n",
      "Iteration 3220 Training loss 0.0011368554551154375 Validation loss 0.04331786930561066 Accuracy 0.8884999752044678\n",
      "Iteration 3230 Training loss 0.0018473443342372775 Validation loss 0.043105605989694595 Accuracy 0.8899999856948853\n",
      "Iteration 3240 Training loss 0.002227427903562784 Validation loss 0.04332306236028671 Accuracy 0.8880000114440918\n",
      "Iteration 3250 Training loss 0.0008743059006519616 Validation loss 0.0433020256459713 Accuracy 0.887499988079071\n",
      "Iteration 3260 Training loss 0.0012766983127221465 Validation loss 0.043376192450523376 Accuracy 0.8889999985694885\n",
      "Iteration 3270 Training loss 0.0006901590968482196 Validation loss 0.04333529621362686 Accuracy 0.8895000219345093\n",
      "Iteration 3280 Training loss 0.0011088718893006444 Validation loss 0.04316438362002373 Accuracy 0.8895000219345093\n",
      "Iteration 3290 Training loss 0.0008336178143508732 Validation loss 0.04313576593995094 Accuracy 0.8899999856948853\n",
      "Iteration 3300 Training loss 0.0015846581663936377 Validation loss 0.043280068784952164 Accuracy 0.8889999985694885\n",
      "Iteration 3310 Training loss 0.001152287470176816 Validation loss 0.043184198439121246 Accuracy 0.8914999961853027\n",
      "Iteration 3320 Training loss 0.001838524593040347 Validation loss 0.043136116117239 Accuracy 0.8895000219345093\n",
      "Iteration 3330 Training loss 0.0010757879354059696 Validation loss 0.04319695755839348 Accuracy 0.8889999985694885\n",
      "Iteration 3340 Training loss 0.001038776128552854 Validation loss 0.043176840990781784 Accuracy 0.8895000219345093\n",
      "Iteration 3350 Training loss 0.0010192056652158499 Validation loss 0.04324573650956154 Accuracy 0.8895000219345093\n",
      "Iteration 3360 Training loss 0.0013460398185998201 Validation loss 0.04327965900301933 Accuracy 0.8899999856948853\n",
      "Iteration 3370 Training loss 0.0007680961862206459 Validation loss 0.04323166236281395 Accuracy 0.8899999856948853\n",
      "Iteration 3380 Training loss 0.0014951210469007492 Validation loss 0.04324337840080261 Accuracy 0.890999972820282\n",
      "Iteration 3390 Training loss 0.0008473162306472659 Validation loss 0.04330098628997803 Accuracy 0.8914999961853027\n",
      "Iteration 3400 Training loss 0.0013069207780063152 Validation loss 0.043270546942949295 Accuracy 0.8914999961853027\n",
      "Iteration 3410 Training loss 0.0018131628166884184 Validation loss 0.04325231537222862 Accuracy 0.8924999833106995\n",
      "Iteration 3420 Training loss 0.001538200187496841 Validation loss 0.04334026947617531 Accuracy 0.8930000066757202\n",
      "Iteration 3430 Training loss 0.001860588090494275 Validation loss 0.04334348812699318 Accuracy 0.890999972820282\n",
      "Iteration 3440 Training loss 0.0013082862133160233 Validation loss 0.04328284412622452 Accuracy 0.8920000195503235\n",
      "Iteration 3450 Training loss 0.0015760507667437196 Validation loss 0.04321761429309845 Accuracy 0.890999972820282\n",
      "Iteration 3460 Training loss 0.0008101098355837166 Validation loss 0.04327298328280449 Accuracy 0.8924999833106995\n",
      "Iteration 3470 Training loss 0.001770369941368699 Validation loss 0.04333553463220596 Accuracy 0.8920000195503235\n",
      "Iteration 3480 Training loss 0.0012134122662246227 Validation loss 0.04326217249035835 Accuracy 0.890999972820282\n",
      "Iteration 3490 Training loss 0.0017566446913406253 Validation loss 0.04328393191099167 Accuracy 0.890999972820282\n",
      "Iteration 3500 Training loss 0.001593512250110507 Validation loss 0.0432710126042366 Accuracy 0.890500009059906\n",
      "Iteration 3510 Training loss 0.001023672753944993 Validation loss 0.043347228318452835 Accuracy 0.8914999961853027\n",
      "Iteration 3520 Training loss 0.0015264733228832483 Validation loss 0.04329444095492363 Accuracy 0.8930000066757202\n",
      "Iteration 3530 Training loss 0.0010397032601758838 Validation loss 0.04317063093185425 Accuracy 0.8924999833106995\n",
      "Iteration 3540 Training loss 0.0010748428758233786 Validation loss 0.04323561117053032 Accuracy 0.8930000066757202\n",
      "Iteration 3550 Training loss 0.0010302761802449822 Validation loss 0.043223749846220016 Accuracy 0.8924999833106995\n",
      "Iteration 3560 Training loss 0.001813411246985197 Validation loss 0.04333285614848137 Accuracy 0.890500009059906\n",
      "Iteration 3570 Training loss 0.0015254662139341235 Validation loss 0.04319901019334793 Accuracy 0.8920000195503235\n",
      "Iteration 3580 Training loss 0.002060896949842572 Validation loss 0.04328054562211037 Accuracy 0.8924999833106995\n",
      "Iteration 3590 Training loss 0.001499038189649582 Validation loss 0.04331504926085472 Accuracy 0.890999972820282\n",
      "Iteration 3600 Training loss 0.0014943900750949979 Validation loss 0.04329851269721985 Accuracy 0.8914999961853027\n",
      "Iteration 3610 Training loss 0.0007377119036391377 Validation loss 0.043367527425289154 Accuracy 0.8930000066757202\n",
      "Iteration 3620 Training loss 0.0020240668673068285 Validation loss 0.04334823787212372 Accuracy 0.8930000066757202\n",
      "Iteration 3630 Training loss 0.0007652760832570493 Validation loss 0.043292440474033356 Accuracy 0.8924999833106995\n",
      "Iteration 3640 Training loss 0.0012365158181637526 Validation loss 0.04334675520658493 Accuracy 0.8920000195503235\n",
      "Iteration 3650 Training loss 0.0010141676757484674 Validation loss 0.043281733989715576 Accuracy 0.8920000195503235\n",
      "Iteration 3660 Training loss 0.0012683449313044548 Validation loss 0.04336163029074669 Accuracy 0.8934999704360962\n",
      "Iteration 3670 Training loss 0.0004744119360111654 Validation loss 0.04340926185250282 Accuracy 0.8920000195503235\n",
      "Iteration 3680 Training loss 0.000988788204267621 Validation loss 0.04334387928247452 Accuracy 0.8934999704360962\n",
      "Iteration 3690 Training loss 0.000985541264526546 Validation loss 0.04336557164788246 Accuracy 0.8924999833106995\n",
      "Iteration 3700 Training loss 0.0007231291383504868 Validation loss 0.043349456042051315 Accuracy 0.8934999704360962\n",
      "Iteration 3710 Training loss 0.0005039268871769309 Validation loss 0.04337052255868912 Accuracy 0.8934999704360962\n",
      "Iteration 3720 Training loss 0.0007379665621556342 Validation loss 0.043374378234148026 Accuracy 0.8920000195503235\n",
      "Iteration 3730 Training loss 0.0009889387292787433 Validation loss 0.0433223731815815 Accuracy 0.8930000066757202\n",
      "Iteration 3740 Training loss 0.0017200105357915163 Validation loss 0.04331103339791298 Accuracy 0.8930000066757202\n",
      "Iteration 3750 Training loss 0.0017182354349642992 Validation loss 0.04328528419137001 Accuracy 0.8939999938011169\n",
      "Iteration 3760 Training loss 0.0009514746489003301 Validation loss 0.043334655463695526 Accuracy 0.8934999704360962\n",
      "Iteration 3770 Training loss 0.0004588238662108779 Validation loss 0.04340164363384247 Accuracy 0.8934999704360962\n",
      "Iteration 3780 Training loss 0.0004838126478716731 Validation loss 0.043387021869421005 Accuracy 0.890500009059906\n",
      "Iteration 3790 Training loss 0.0006894947146065533 Validation loss 0.04337779060006142 Accuracy 0.8934999704360962\n",
      "Iteration 3800 Training loss 0.0014793735463172197 Validation loss 0.043447501957416534 Accuracy 0.8920000195503235\n",
      "Iteration 3810 Training loss 0.001202658866532147 Validation loss 0.04335356131196022 Accuracy 0.8930000066757202\n",
      "Iteration 3820 Training loss 0.0012265952536836267 Validation loss 0.043437983840703964 Accuracy 0.8930000066757202\n",
      "Iteration 3830 Training loss 0.001449069008231163 Validation loss 0.04345454275608063 Accuracy 0.8914999961853027\n",
      "Iteration 3840 Training loss 0.001206349697895348 Validation loss 0.0433913953602314 Accuracy 0.8924999833106995\n",
      "Iteration 3850 Training loss 0.0014623543247580528 Validation loss 0.043429434299468994 Accuracy 0.8934999704360962\n",
      "Iteration 3860 Training loss 0.0012292222818359733 Validation loss 0.043206848204135895 Accuracy 0.8924999833106995\n",
      "Iteration 3870 Training loss 0.0007324486505240202 Validation loss 0.04335488751530647 Accuracy 0.8924999833106995\n",
      "Iteration 3880 Training loss 0.0014791818102821708 Validation loss 0.04339577630162239 Accuracy 0.8934999704360962\n",
      "Iteration 3890 Training loss 0.0014623692259192467 Validation loss 0.04343467578291893 Accuracy 0.8924999833106995\n",
      "Iteration 3900 Training loss 0.0007031565182842314 Validation loss 0.04346806928515434 Accuracy 0.890999972820282\n",
      "Iteration 3910 Training loss 0.0009803947759792209 Validation loss 0.04356652498245239 Accuracy 0.8924999833106995\n",
      "Iteration 3920 Training loss 0.0014718191232532263 Validation loss 0.04342593625187874 Accuracy 0.8930000066757202\n",
      "Iteration 3930 Training loss 0.0012019353453069925 Validation loss 0.04350130632519722 Accuracy 0.8914999961853027\n",
      "Iteration 3940 Training loss 0.0004369362141005695 Validation loss 0.043560318648815155 Accuracy 0.890999972820282\n",
      "Iteration 3950 Training loss 0.0009726944845169783 Validation loss 0.04350605979561806 Accuracy 0.8920000195503235\n",
      "Iteration 3960 Training loss 0.0006897380808368325 Validation loss 0.043563999235630035 Accuracy 0.8914999961853027\n",
      "Iteration 3970 Training loss 0.0007025361410342157 Validation loss 0.04359545186161995 Accuracy 0.890500009059906\n",
      "Iteration 3980 Training loss 0.0019812709651887417 Validation loss 0.04356466978788376 Accuracy 0.8924999833106995\n",
      "Iteration 3990 Training loss 0.0014660784509032965 Validation loss 0.0435495488345623 Accuracy 0.8930000066757202\n",
      "Iteration 4000 Training loss 0.0012174553703516722 Validation loss 0.04348813742399216 Accuracy 0.890500009059906\n",
      "Iteration 4010 Training loss 0.00044981774408370256 Validation loss 0.04355258867144585 Accuracy 0.8899999856948853\n",
      "Iteration 4020 Training loss 0.000692888512276113 Validation loss 0.04353678971529007 Accuracy 0.890999972820282\n",
      "Iteration 4030 Training loss 0.0019459371687844396 Validation loss 0.04354935884475708 Accuracy 0.890999972820282\n",
      "Iteration 4040 Training loss 0.0004697662079706788 Validation loss 0.04348265007138252 Accuracy 0.890999972820282\n",
      "Iteration 4050 Training loss 0.0016686120070517063 Validation loss 0.04355384781956673 Accuracy 0.8914999961853027\n",
      "Iteration 4060 Training loss 0.00045249020331539214 Validation loss 0.04348192736506462 Accuracy 0.890999972820282\n",
      "Iteration 4070 Training loss 0.0011965266894549131 Validation loss 0.04361633211374283 Accuracy 0.8930000066757202\n",
      "Iteration 4080 Training loss 0.001442223903723061 Validation loss 0.04366137459874153 Accuracy 0.8899999856948853\n",
      "Iteration 4090 Training loss 0.0012110491516068578 Validation loss 0.04365510493516922 Accuracy 0.890500009059906\n",
      "Iteration 4100 Training loss 0.00045703479554504156 Validation loss 0.04350926727056503 Accuracy 0.8914999961853027\n",
      "Iteration 4110 Training loss 0.0004630502371583134 Validation loss 0.043575141578912735 Accuracy 0.8914999961853027\n",
      "Iteration 4120 Training loss 0.0014339330373331904 Validation loss 0.04360999912023544 Accuracy 0.8899999856948853\n",
      "Iteration 4130 Training loss 0.0009189437841996551 Validation loss 0.043636489659547806 Accuracy 0.890999972820282\n",
      "Iteration 4140 Training loss 0.0011853529140353203 Validation loss 0.043610792607069016 Accuracy 0.890500009059906\n",
      "Iteration 4150 Training loss 0.00143697508610785 Validation loss 0.043632958084344864 Accuracy 0.8914999961853027\n",
      "Iteration 4160 Training loss 0.001427865936420858 Validation loss 0.043601781129837036 Accuracy 0.890999972820282\n",
      "Iteration 4170 Training loss 0.001192136318422854 Validation loss 0.04369371011853218 Accuracy 0.8920000195503235\n",
      "Iteration 4180 Training loss 0.0009292547474615276 Validation loss 0.04355898126959801 Accuracy 0.8920000195503235\n",
      "Iteration 4190 Training loss 0.0009287723805755377 Validation loss 0.043583907186985016 Accuracy 0.8895000219345093\n",
      "Iteration 4200 Training loss 0.001170690986327827 Validation loss 0.043639812618494034 Accuracy 0.8920000195503235\n",
      "Iteration 4210 Training loss 0.0011770569253712893 Validation loss 0.043642014265060425 Accuracy 0.8889999985694885\n",
      "Iteration 4220 Training loss 0.0011869468726217747 Validation loss 0.043651070445775986 Accuracy 0.8914999961853027\n",
      "Iteration 4230 Training loss 0.0011584347812458873 Validation loss 0.043647315353155136 Accuracy 0.890500009059906\n",
      "Iteration 4240 Training loss 0.00041111113387160003 Validation loss 0.04356994107365608 Accuracy 0.8920000195503235\n",
      "Iteration 4250 Training loss 0.0011818730272352695 Validation loss 0.043599724769592285 Accuracy 0.8889999985694885\n",
      "Iteration 4260 Training loss 0.0009264849941246212 Validation loss 0.043598372489213943 Accuracy 0.8899999856948853\n",
      "Iteration 4270 Training loss 0.0009060561424121261 Validation loss 0.04367707669734955 Accuracy 0.8895000219345093\n",
      "Iteration 4280 Training loss 0.000431614404078573 Validation loss 0.0436130091547966 Accuracy 0.8920000195503235\n",
      "Iteration 4290 Training loss 0.0011652985122054815 Validation loss 0.04361199587583542 Accuracy 0.890500009059906\n",
      "Iteration 4300 Training loss 0.0009169792174361646 Validation loss 0.04359963908791542 Accuracy 0.8914999961853027\n",
      "Iteration 4310 Training loss 0.000913064053747803 Validation loss 0.04357219859957695 Accuracy 0.890500009059906\n",
      "Iteration 4320 Training loss 0.001672975835390389 Validation loss 0.04369572922587395 Accuracy 0.8930000066757202\n",
      "Iteration 4330 Training loss 0.0006632754812017083 Validation loss 0.04357483610510826 Accuracy 0.890500009059906\n",
      "Iteration 4340 Training loss 0.001411966746672988 Validation loss 0.043654367327690125 Accuracy 0.8895000219345093\n",
      "Iteration 4350 Training loss 0.0011585605097934604 Validation loss 0.04364144429564476 Accuracy 0.890999972820282\n",
      "Iteration 4360 Training loss 0.0009199408814311028 Validation loss 0.0435989685356617 Accuracy 0.8895000219345093\n",
      "Iteration 4370 Training loss 0.0014203282771632075 Validation loss 0.043591346591711044 Accuracy 0.8889999985694885\n",
      "Iteration 4380 Training loss 0.0006629780982621014 Validation loss 0.043645553290843964 Accuracy 0.8914999961853027\n",
      "Iteration 4390 Training loss 0.0006467652856372297 Validation loss 0.043594833463430405 Accuracy 0.8914999961853027\n",
      "Iteration 4400 Training loss 0.0009078116272576153 Validation loss 0.04360361769795418 Accuracy 0.890999972820282\n",
      "Iteration 4410 Training loss 0.0014004340628162026 Validation loss 0.04360823705792427 Accuracy 0.890500009059906\n",
      "Iteration 4420 Training loss 0.0011545788729563355 Validation loss 0.04361799731850624 Accuracy 0.8895000219345093\n",
      "Iteration 4430 Training loss 0.0016523387748748064 Validation loss 0.04362461343407631 Accuracy 0.890999972820282\n",
      "Iteration 4440 Training loss 0.0009105644421651959 Validation loss 0.043697282671928406 Accuracy 0.890500009059906\n",
      "Iteration 4450 Training loss 0.0006670149741694331 Validation loss 0.04369392618536949 Accuracy 0.8899999856948853\n",
      "Iteration 4460 Training loss 0.0008895088685676455 Validation loss 0.04367662966251373 Accuracy 0.8895000219345093\n",
      "Iteration 4470 Training loss 0.0006523852935060859 Validation loss 0.043607085943222046 Accuracy 0.8889999985694885\n",
      "Iteration 4480 Training loss 0.0006368730682879686 Validation loss 0.0436142273247242 Accuracy 0.890500009059906\n",
      "Iteration 4490 Training loss 0.0011647000210359693 Validation loss 0.04366489127278328 Accuracy 0.8895000219345093\n",
      "Iteration 4500 Training loss 0.0009072438697330654 Validation loss 0.04364746809005737 Accuracy 0.8899999856948853\n",
      "Iteration 4510 Training loss 0.000393052730942145 Validation loss 0.04364020377397537 Accuracy 0.887499988079071\n",
      "Iteration 4520 Training loss 0.0008980776183307171 Validation loss 0.043647583574056625 Accuracy 0.8895000219345093\n",
      "Iteration 4530 Training loss 0.0014106456656008959 Validation loss 0.04363543912768364 Accuracy 0.890999972820282\n",
      "Iteration 4540 Training loss 0.001138809835538268 Validation loss 0.043622683733701706 Accuracy 0.890999972820282\n",
      "Iteration 4550 Training loss 0.0011490562465041876 Validation loss 0.043664656579494476 Accuracy 0.890500009059906\n",
      "Iteration 4560 Training loss 0.0013900160556659102 Validation loss 0.04364984109997749 Accuracy 0.8889999985694885\n",
      "Iteration 4570 Training loss 0.0011390858562663198 Validation loss 0.04358818009495735 Accuracy 0.8889999985694885\n",
      "Iteration 4580 Training loss 0.000398102100007236 Validation loss 0.04370151460170746 Accuracy 0.8889999985694885\n",
      "Iteration 4590 Training loss 0.0011500801192596555 Validation loss 0.04365578293800354 Accuracy 0.8899999856948853\n",
      "Iteration 4600 Training loss 0.0011420686496421695 Validation loss 0.043636567890644073 Accuracy 0.8899999856948853\n",
      "Iteration 4610 Training loss 0.0016436611767858267 Validation loss 0.043671540915966034 Accuracy 0.890500009059906\n",
      "Iteration 4620 Training loss 0.0006405481253750622 Validation loss 0.043691400438547134 Accuracy 0.890500009059906\n",
      "Iteration 4630 Training loss 0.0006347521557472646 Validation loss 0.043638646602630615 Accuracy 0.890500009059906\n",
      "Iteration 4640 Training loss 0.00038581984699703753 Validation loss 0.04359429329633713 Accuracy 0.8899999856948853\n",
      "Iteration 4650 Training loss 0.0006350050098262727 Validation loss 0.04362943395972252 Accuracy 0.890500009059906\n",
      "Iteration 4660 Training loss 0.000658673990983516 Validation loss 0.04362954571843147 Accuracy 0.8895000219345093\n",
      "Iteration 4670 Training loss 0.0006315354839898646 Validation loss 0.04366490617394447 Accuracy 0.8889999985694885\n",
      "Iteration 4680 Training loss 0.00088892673375085 Validation loss 0.04365107789635658 Accuracy 0.8884999752044678\n",
      "Iteration 4690 Training loss 0.0006459896103478968 Validation loss 0.04359664022922516 Accuracy 0.8914999961853027\n",
      "Iteration 4700 Training loss 0.001124291680753231 Validation loss 0.043581582605838776 Accuracy 0.8895000219345093\n",
      "Iteration 4710 Training loss 0.001145022688433528 Validation loss 0.04358287900686264 Accuracy 0.8895000219345093\n",
      "Iteration 4720 Training loss 0.0008825263939797878 Validation loss 0.043636031448841095 Accuracy 0.8899999856948853\n",
      "Iteration 4730 Training loss 0.00039490172639489174 Validation loss 0.04366070404648781 Accuracy 0.8895000219345093\n",
      "Iteration 4740 Training loss 0.0011230447562411427 Validation loss 0.04366165027022362 Accuracy 0.8914999961853027\n",
      "Iteration 4750 Training loss 0.0013812738470733166 Validation loss 0.04374340921640396 Accuracy 0.8899999856948853\n",
      "Iteration 4760 Training loss 0.0013788247015327215 Validation loss 0.043752945959568024 Accuracy 0.8899999856948853\n",
      "Iteration 4770 Training loss 0.00037040983443148434 Validation loss 0.04367271810770035 Accuracy 0.890500009059906\n",
      "Iteration 4780 Training loss 0.0006336775259114802 Validation loss 0.04373504966497421 Accuracy 0.8895000219345093\n",
      "Iteration 4790 Training loss 0.0011303079081699252 Validation loss 0.0437544547021389 Accuracy 0.890500009059906\n",
      "Iteration 4800 Training loss 0.0006297018262557685 Validation loss 0.04368062689900398 Accuracy 0.8895000219345093\n",
      "Iteration 4810 Training loss 0.0011480742832645774 Validation loss 0.04371980205178261 Accuracy 0.890999972820282\n",
      "Iteration 4820 Training loss 0.0013823480112478137 Validation loss 0.0437106192111969 Accuracy 0.8899999856948853\n",
      "Iteration 4830 Training loss 0.000882181862834841 Validation loss 0.04368262365460396 Accuracy 0.8899999856948853\n",
      "Iteration 4840 Training loss 0.0006190317799337208 Validation loss 0.04374517872929573 Accuracy 0.8884999752044678\n",
      "Iteration 4850 Training loss 0.0011379217030480504 Validation loss 0.043663010001182556 Accuracy 0.890999972820282\n",
      "Iteration 4860 Training loss 0.0006439193384721875 Validation loss 0.04371042177081108 Accuracy 0.8889999985694885\n",
      "Iteration 4870 Training loss 0.0021244846284389496 Validation loss 0.043686188757419586 Accuracy 0.890999972820282\n",
      "Iteration 4880 Training loss 0.0003815627715084702 Validation loss 0.04365689679980278 Accuracy 0.8899999856948853\n",
      "Iteration 4890 Training loss 0.0003645440738182515 Validation loss 0.04367517679929733 Accuracy 0.890500009059906\n",
      "Iteration 4900 Training loss 0.001121838460676372 Validation loss 0.0437154546380043 Accuracy 0.8914999961853027\n",
      "Iteration 4910 Training loss 0.0008774360176175833 Validation loss 0.04371073842048645 Accuracy 0.8895000219345093\n",
      "Iteration 4920 Training loss 0.0008621881133876741 Validation loss 0.04369233921170235 Accuracy 0.8899999856948853\n",
      "Iteration 4930 Training loss 0.0006424199673347175 Validation loss 0.043671853840351105 Accuracy 0.890500009059906\n",
      "Iteration 4940 Training loss 0.0011282170889899135 Validation loss 0.04371253028512001 Accuracy 0.8899999856948853\n",
      "Iteration 4950 Training loss 0.0011245680507272482 Validation loss 0.043639324605464935 Accuracy 0.890999972820282\n",
      "Iteration 4960 Training loss 0.000384397862944752 Validation loss 0.0436968058347702 Accuracy 0.890500009059906\n",
      "Iteration 4970 Training loss 0.0013714870437979698 Validation loss 0.04369165748357773 Accuracy 0.8895000219345093\n",
      "Iteration 4980 Training loss 0.0011297081364318728 Validation loss 0.043733008205890656 Accuracy 0.8899999856948853\n",
      "Iteration 4990 Training loss 0.000622183782979846 Validation loss 0.04374442249536514 Accuracy 0.8895000219345093\n",
      "Iteration 5000 Training loss 0.0008781385258771479 Validation loss 0.04369497299194336 Accuracy 0.8914999961853027\n",
      "Iteration 5010 Training loss 0.0011244664201512933 Validation loss 0.04372536391019821 Accuracy 0.890500009059906\n",
      "Iteration 5020 Training loss 0.0008833581232465804 Validation loss 0.04369852691888809 Accuracy 0.8895000219345093\n",
      "Iteration 5030 Training loss 0.00037938766763545573 Validation loss 0.0437803640961647 Accuracy 0.8899999856948853\n",
      "Iteration 5040 Training loss 0.0003923543554265052 Validation loss 0.04373135417699814 Accuracy 0.8889999985694885\n",
      "Iteration 5050 Training loss 0.0006129348184913397 Validation loss 0.04368927702307701 Accuracy 0.890500009059906\n",
      "Iteration 5060 Training loss 0.0006231419974938035 Validation loss 0.04368796572089195 Accuracy 0.8889999985694885\n",
      "Iteration 5070 Training loss 0.0011239803861826658 Validation loss 0.04370776563882828 Accuracy 0.8899999856948853\n",
      "Iteration 5080 Training loss 0.0013686077436432242 Validation loss 0.04366804286837578 Accuracy 0.8884999752044678\n",
      "Iteration 5090 Training loss 0.0006238000933080912 Validation loss 0.04364234209060669 Accuracy 0.8914999961853027\n",
      "Iteration 5100 Training loss 0.0011292228009551764 Validation loss 0.04366638883948326 Accuracy 0.8895000219345093\n",
      "Iteration 5110 Training loss 0.0008741248748265207 Validation loss 0.04368123784661293 Accuracy 0.890500009059906\n",
      "Iteration 5120 Training loss 0.0011181978043168783 Validation loss 0.04371242597699165 Accuracy 0.890500009059906\n",
      "Iteration 5130 Training loss 0.00037992181023582816 Validation loss 0.04371540993452072 Accuracy 0.8899999856948853\n",
      "Iteration 5140 Training loss 0.0013728959020227194 Validation loss 0.04372171685099602 Accuracy 0.8899999856948853\n",
      "Iteration 5150 Training loss 0.0008791319560259581 Validation loss 0.043687786906957626 Accuracy 0.890500009059906\n",
      "Iteration 5160 Training loss 0.0013695419766008854 Validation loss 0.04372282326221466 Accuracy 0.890500009059906\n",
      "Iteration 5170 Training loss 0.0008671071846038103 Validation loss 0.04372713342308998 Accuracy 0.8899999856948853\n",
      "Iteration 5180 Training loss 0.0016283701406791806 Validation loss 0.04367746412754059 Accuracy 0.890999972820282\n",
      "Iteration 5190 Training loss 0.000871223455760628 Validation loss 0.043725378811359406 Accuracy 0.890500009059906\n",
      "Iteration 5200 Training loss 0.00036472725332714617 Validation loss 0.04370914027094841 Accuracy 0.8899999856948853\n",
      "Iteration 5210 Training loss 0.0006125620566308498 Validation loss 0.04370561242103577 Accuracy 0.890500009059906\n",
      "Iteration 5220 Training loss 0.0008671536343172193 Validation loss 0.04370120167732239 Accuracy 0.8895000219345093\n",
      "Iteration 5230 Training loss 0.0011180868605151772 Validation loss 0.04372885078191757 Accuracy 0.890500009059906\n",
      "Iteration 5240 Training loss 0.0006112476112321019 Validation loss 0.043715812265872955 Accuracy 0.8889999985694885\n",
      "Iteration 5250 Training loss 0.001357465167529881 Validation loss 0.043691061437129974 Accuracy 0.890999972820282\n",
      "Iteration 5260 Training loss 0.0011118050897493958 Validation loss 0.04369288682937622 Accuracy 0.8920000195503235\n",
      "Iteration 5270 Training loss 0.0011208412470296025 Validation loss 0.043738722801208496 Accuracy 0.8899999856948853\n",
      "Iteration 5280 Training loss 0.00012065663031535223 Validation loss 0.043703850358724594 Accuracy 0.8899999856948853\n",
      "Iteration 5290 Training loss 0.0006094942218624055 Validation loss 0.04372132569551468 Accuracy 0.8895000219345093\n",
      "Iteration 5300 Training loss 0.0008606844930909574 Validation loss 0.04370293393731117 Accuracy 0.8895000219345093\n",
      "Iteration 5310 Training loss 0.0008627843344584107 Validation loss 0.04373471066355705 Accuracy 0.8899999856948853\n",
      "Iteration 5320 Training loss 0.00010476548777660355 Validation loss 0.04371266067028046 Accuracy 0.8914999961853027\n",
      "Iteration 5330 Training loss 0.0003529489622451365 Validation loss 0.04369817301630974 Accuracy 0.8914999961853027\n",
      "Iteration 5340 Training loss 0.0008602079469710588 Validation loss 0.04372800886631012 Accuracy 0.8899999856948853\n",
      "Iteration 5350 Training loss 0.0011131324572488666 Validation loss 0.04371888190507889 Accuracy 0.8895000219345093\n",
      "Iteration 5360 Training loss 0.0006115699070505798 Validation loss 0.043736595660448074 Accuracy 0.8899999856948853\n",
      "Iteration 5370 Training loss 0.0013517981860786676 Validation loss 0.0437343567609787 Accuracy 0.8895000219345093\n",
      "Iteration 5380 Training loss 0.0013618450611829758 Validation loss 0.04373428225517273 Accuracy 0.8899999856948853\n",
      "Iteration 5390 Training loss 0.0011301252525299788 Validation loss 0.043771713972091675 Accuracy 0.8895000219345093\n",
      "Iteration 5400 Training loss 0.00086528982501477 Validation loss 0.043721918016672134 Accuracy 0.8899999856948853\n",
      "Iteration 5410 Training loss 0.0010997334029525518 Validation loss 0.04369617998600006 Accuracy 0.8899999856948853\n",
      "Iteration 5420 Training loss 0.000615451019257307 Validation loss 0.04376218840479851 Accuracy 0.8899999856948853\n",
      "Iteration 5430 Training loss 0.000863214663695544 Validation loss 0.043771710246801376 Accuracy 0.8895000219345093\n",
      "Iteration 5440 Training loss 0.001357177272439003 Validation loss 0.04368432238698006 Accuracy 0.8899999856948853\n",
      "Iteration 5450 Training loss 0.000862878980115056 Validation loss 0.043696265667676926 Accuracy 0.8889999985694885\n",
      "Iteration 5460 Training loss 0.000858848390635103 Validation loss 0.0437040813267231 Accuracy 0.8889999985694885\n",
      "Iteration 5470 Training loss 0.0006105456850491464 Validation loss 0.043746188282966614 Accuracy 0.8899999856948853\n",
      "Iteration 5480 Training loss 0.000861063483171165 Validation loss 0.04378027468919754 Accuracy 0.8895000219345093\n",
      "Iteration 5490 Training loss 0.0008645044290460646 Validation loss 0.04372420161962509 Accuracy 0.890500009059906\n",
      "Iteration 5500 Training loss 0.001617283676750958 Validation loss 0.04375211149454117 Accuracy 0.890500009059906\n",
      "Iteration 5510 Training loss 0.0008546191384084523 Validation loss 0.0437447726726532 Accuracy 0.8899999856948853\n",
      "Iteration 5520 Training loss 0.0008634044206701219 Validation loss 0.04373236000537872 Accuracy 0.890999972820282\n",
      "Iteration 5530 Training loss 0.0006067405338399112 Validation loss 0.04374412074685097 Accuracy 0.8895000219345093\n",
      "Iteration 5540 Training loss 0.0008550938218832016 Validation loss 0.04377906024456024 Accuracy 0.8895000219345093\n",
      "Iteration 5550 Training loss 0.0013717192923650146 Validation loss 0.043743979185819626 Accuracy 0.8889999985694885\n",
      "Iteration 5560 Training loss 0.0008594415266998112 Validation loss 0.043752167373895645 Accuracy 0.8895000219345093\n",
      "Iteration 5570 Training loss 0.0005980569403618574 Validation loss 0.043748196214437485 Accuracy 0.8899999856948853\n",
      "Iteration 5580 Training loss 0.000604337255936116 Validation loss 0.043676845729351044 Accuracy 0.8914999961853027\n",
      "Iteration 5590 Training loss 0.0010988595895469189 Validation loss 0.043701063841581345 Accuracy 0.8899999856948853\n",
      "Iteration 5600 Training loss 0.0003632090229075402 Validation loss 0.0437217578291893 Accuracy 0.8920000195503235\n",
      "Iteration 5610 Training loss 0.0008562886505387723 Validation loss 0.04373752698302269 Accuracy 0.8914999961853027\n",
      "Iteration 5620 Training loss 0.0008591653895564377 Validation loss 0.04374144226312637 Accuracy 0.8899999856948853\n",
      "Iteration 5630 Training loss 0.0008574152016080916 Validation loss 0.04373694583773613 Accuracy 0.8899999856948853\n",
      "Iteration 5640 Training loss 0.0011154219973832369 Validation loss 0.04376029595732689 Accuracy 0.8899999856948853\n",
      "Iteration 5650 Training loss 0.0013568768044933677 Validation loss 0.04375825449824333 Accuracy 0.890500009059906\n",
      "Iteration 5660 Training loss 0.0010979115031659603 Validation loss 0.043761108070611954 Accuracy 0.8895000219345093\n",
      "Iteration 5670 Training loss 0.0008622885798104107 Validation loss 0.04375108331441879 Accuracy 0.8889999985694885\n",
      "Iteration 5680 Training loss 0.0008637604769319296 Validation loss 0.04377450421452522 Accuracy 0.890500009059906\n",
      "Iteration 5690 Training loss 0.0013470249250531197 Validation loss 0.04372616857290268 Accuracy 0.8889999985694885\n",
      "Iteration 5700 Training loss 0.00060965062584728 Validation loss 0.043763138353824615 Accuracy 0.8899999856948853\n",
      "Iteration 5710 Training loss 0.0011157130356878042 Validation loss 0.04376007989048958 Accuracy 0.890500009059906\n",
      "Iteration 5720 Training loss 0.0006065082270652056 Validation loss 0.04376097023487091 Accuracy 0.890500009059906\n",
      "Iteration 5730 Training loss 0.000609961396548897 Validation loss 0.04380107298493385 Accuracy 0.8899999856948853\n",
      "Iteration 5740 Training loss 0.0013564968248829246 Validation loss 0.04374336451292038 Accuracy 0.8895000219345093\n",
      "Iteration 5750 Training loss 0.0010966885602101684 Validation loss 0.043738219887018204 Accuracy 0.890500009059906\n",
      "Iteration 5760 Training loss 0.0011160133872181177 Validation loss 0.043786898255348206 Accuracy 0.8895000219345093\n",
      "Iteration 5770 Training loss 0.0008482486009597778 Validation loss 0.04377001151442528 Accuracy 0.8895000219345093\n",
      "Iteration 5780 Training loss 0.0008554925443604589 Validation loss 0.04374375194311142 Accuracy 0.890999972820282\n",
      "Iteration 5790 Training loss 0.00035187628236599267 Validation loss 0.043761979788541794 Accuracy 0.8899999856948853\n",
      "Iteration 5800 Training loss 0.0005969151388853788 Validation loss 0.04372001811861992 Accuracy 0.890999972820282\n",
      "Iteration 5810 Training loss 0.0008580015855841339 Validation loss 0.04372521489858627 Accuracy 0.8914999961853027\n",
      "Iteration 5820 Training loss 0.0011063627898693085 Validation loss 0.04379086196422577 Accuracy 0.8895000219345093\n",
      "Iteration 5830 Training loss 0.0005926083540543914 Validation loss 0.04372616484761238 Accuracy 0.8914999961853027\n",
      "Iteration 5840 Training loss 0.0011001533130183816 Validation loss 0.043752510100603104 Accuracy 0.890999972820282\n",
      "Iteration 5850 Training loss 0.0005991614889353514 Validation loss 0.043732598423957825 Accuracy 0.8895000219345093\n",
      "Iteration 5860 Training loss 0.0006106593646109104 Validation loss 0.04376940429210663 Accuracy 0.890500009059906\n",
      "Iteration 5870 Training loss 0.001098135020583868 Validation loss 0.043791159987449646 Accuracy 0.8899999856948853\n",
      "Iteration 5880 Training loss 0.0008519502589479089 Validation loss 0.043752964586019516 Accuracy 0.8895000219345093\n",
      "Iteration 5890 Training loss 0.0013429331593215466 Validation loss 0.04368376359343529 Accuracy 0.890500009059906\n",
      "Iteration 5900 Training loss 0.001106351613998413 Validation loss 0.043706588447093964 Accuracy 0.8914999961853027\n",
      "Iteration 5910 Training loss 0.0015972593100741506 Validation loss 0.0437496192753315 Accuracy 0.8889999985694885\n",
      "Iteration 5920 Training loss 0.0006058766157366335 Validation loss 0.043743785470724106 Accuracy 0.8895000219345093\n",
      "Iteration 5930 Training loss 0.0010984103428199887 Validation loss 0.04374950751662254 Accuracy 0.8895000219345093\n",
      "Iteration 5940 Training loss 0.001350328791886568 Validation loss 0.04374142363667488 Accuracy 0.890999972820282\n",
      "Iteration 5950 Training loss 0.00034909791429527104 Validation loss 0.043731559067964554 Accuracy 0.890500009059906\n",
      "Iteration 5960 Training loss 0.0015895579708740115 Validation loss 0.043762873858213425 Accuracy 0.890999972820282\n",
      "Iteration 5970 Training loss 0.0008510603220202029 Validation loss 0.04375128820538521 Accuracy 0.8914999961853027\n",
      "Iteration 5980 Training loss 0.00035257384297437966 Validation loss 0.043742068111896515 Accuracy 0.890500009059906\n",
      "Iteration 5990 Training loss 0.001096377382054925 Validation loss 0.04369523748755455 Accuracy 0.890500009059906\n",
      "Iteration 6000 Training loss 0.0008565023308619857 Validation loss 0.04373130947351456 Accuracy 0.8899999856948853\n",
      "Iteration 6010 Training loss 0.0011038389056921005 Validation loss 0.04372524470090866 Accuracy 0.890500009059906\n",
      "Iteration 6020 Training loss 0.000849489588290453 Validation loss 0.043693847954273224 Accuracy 0.8899999856948853\n",
      "Iteration 6030 Training loss 0.001100741676054895 Validation loss 0.043751899152994156 Accuracy 0.890500009059906\n",
      "Iteration 6040 Training loss 0.001094575272873044 Validation loss 0.04377235844731331 Accuracy 0.890500009059906\n",
      "Iteration 6050 Training loss 0.0008475187933072448 Validation loss 0.04372519999742508 Accuracy 0.890999972820282\n",
      "Iteration 6060 Training loss 0.00034249896998517215 Validation loss 0.04374498873949051 Accuracy 0.8899999856948853\n",
      "Iteration 6070 Training loss 0.00035050895530730486 Validation loss 0.04374045133590698 Accuracy 0.8899999856948853\n",
      "Iteration 6080 Training loss 0.0013432473642751575 Validation loss 0.04373526945710182 Accuracy 0.890500009059906\n",
      "Iteration 6090 Training loss 0.0006040474982000887 Validation loss 0.043715670704841614 Accuracy 0.890500009059906\n",
      "Iteration 6100 Training loss 0.0010873762657865882 Validation loss 0.04377371817827225 Accuracy 0.8899999856948853\n",
      "Iteration 6110 Training loss 0.0005970381898805499 Validation loss 0.0437617227435112 Accuracy 0.890500009059906\n",
      "Iteration 6120 Training loss 0.001092077698558569 Validation loss 0.04373415559530258 Accuracy 0.8899999856948853\n",
      "Iteration 6130 Training loss 0.0010975965997204185 Validation loss 0.04367395117878914 Accuracy 0.890500009059906\n",
      "Iteration 6140 Training loss 0.0008472642512060702 Validation loss 0.04368900880217552 Accuracy 0.890500009059906\n",
      "Iteration 6150 Training loss 0.0008429210283793509 Validation loss 0.04372734948992729 Accuracy 0.890500009059906\n",
      "Iteration 6160 Training loss 0.0010929253185167909 Validation loss 0.04367247223854065 Accuracy 0.890500009059906\n",
      "Iteration 6170 Training loss 0.0008463304839096963 Validation loss 0.043671321123838425 Accuracy 0.890999972820282\n",
      "Iteration 6180 Training loss 0.0010911923600360751 Validation loss 0.04368814453482628 Accuracy 0.890500009059906\n",
      "Iteration 6190 Training loss 9.025196777656674e-05 Validation loss 0.04370417073369026 Accuracy 0.8895000219345093\n",
      "Iteration 6200 Training loss 0.0008403861429542303 Validation loss 0.04370477423071861 Accuracy 0.8895000219345093\n",
      "Iteration 6210 Training loss 0.0010948387207463384 Validation loss 0.043741658329963684 Accuracy 0.8895000219345093\n",
      "Iteration 6220 Training loss 9.872379450825974e-05 Validation loss 0.043733906000852585 Accuracy 0.8899999856948853\n",
      "Iteration 6230 Training loss 0.0015899648424237967 Validation loss 0.04375950247049332 Accuracy 0.8889999985694885\n",
      "Iteration 6240 Training loss 0.0008528103353455663 Validation loss 0.04376966506242752 Accuracy 0.8889999985694885\n",
      "Iteration 6250 Training loss 0.0010934239253401756 Validation loss 0.04374713823199272 Accuracy 0.8895000219345093\n",
      "Iteration 6260 Training loss 0.0005951130297034979 Validation loss 0.04371471703052521 Accuracy 0.890500009059906\n",
      "Iteration 6270 Training loss 0.0010929269483312964 Validation loss 0.0437287911772728 Accuracy 0.8895000219345093\n",
      "Iteration 6280 Training loss 0.0005882921977899969 Validation loss 0.04372098296880722 Accuracy 0.890500009059906\n",
      "Iteration 6290 Training loss 0.001605581259354949 Validation loss 0.043706245720386505 Accuracy 0.8899999856948853\n",
      "Iteration 6300 Training loss 9.884864994091913e-05 Validation loss 0.043663911521434784 Accuracy 0.890999972820282\n",
      "Iteration 6310 Training loss 0.0006003112066537142 Validation loss 0.04373713582754135 Accuracy 0.8899999856948853\n",
      "Iteration 6320 Training loss 0.0008519965340383351 Validation loss 0.04370802268385887 Accuracy 0.8895000219345093\n",
      "Iteration 6330 Training loss 0.0006006078328937292 Validation loss 0.043753210455179214 Accuracy 0.8889999985694885\n",
      "Iteration 6340 Training loss 0.00034991969005204737 Validation loss 0.04373994842171669 Accuracy 0.8899999856948853\n",
      "Iteration 6350 Training loss 0.0013415093999356031 Validation loss 0.043747205287218094 Accuracy 0.8899999856948853\n",
      "Iteration 6360 Training loss 0.0015965604688972235 Validation loss 0.04371924698352814 Accuracy 0.890500009059906\n",
      "Iteration 6370 Training loss 0.0008414948242716491 Validation loss 0.04370444640517235 Accuracy 0.890500009059906\n",
      "Iteration 6380 Training loss 0.0005940858391113579 Validation loss 0.04368240386247635 Accuracy 0.890500009059906\n",
      "Iteration 6390 Training loss 0.0003428834315855056 Validation loss 0.043711159378290176 Accuracy 0.8899999856948853\n",
      "Iteration 6400 Training loss 0.0008414802723564208 Validation loss 0.04372277855873108 Accuracy 0.8920000195503235\n",
      "Iteration 6410 Training loss 0.001594172092154622 Validation loss 0.04369136691093445 Accuracy 0.8914999961853027\n",
      "Iteration 6420 Training loss 0.0015892180381342769 Validation loss 0.04366879165172577 Accuracy 0.890500009059906\n",
      "Iteration 6430 Training loss 0.0015889896312728524 Validation loss 0.04372665286064148 Accuracy 0.8899999856948853\n",
      "Iteration 6440 Training loss 0.0008514770306646824 Validation loss 0.04368458688259125 Accuracy 0.890999972820282\n",
      "Iteration 6450 Training loss 0.0008400214719586074 Validation loss 0.04369503632187843 Accuracy 0.890500009059906\n",
      "Iteration 6460 Training loss 0.0010858207242563367 Validation loss 0.04373180493712425 Accuracy 0.890500009059906\n",
      "Iteration 6470 Training loss 0.001096272491849959 Validation loss 0.043770287185907364 Accuracy 0.8895000219345093\n",
      "Iteration 6480 Training loss 0.0008398763020522892 Validation loss 0.043770596385002136 Accuracy 0.890500009059906\n",
      "Iteration 6490 Training loss 0.000590929645113647 Validation loss 0.0437045618891716 Accuracy 0.8899999856948853\n",
      "Iteration 6500 Training loss 0.001586570288054645 Validation loss 0.04374486580491066 Accuracy 0.8895000219345093\n",
      "Iteration 6510 Training loss 0.0013376292772591114 Validation loss 0.043696824461221695 Accuracy 0.8914999961853027\n",
      "Iteration 6520 Training loss 0.0005872782203368843 Validation loss 0.04374891519546509 Accuracy 0.890500009059906\n",
      "Iteration 6530 Training loss 0.00034030008828267455 Validation loss 0.04377172514796257 Accuracy 0.8889999985694885\n",
      "Iteration 6540 Training loss 0.0005907158483751118 Validation loss 0.04372363165020943 Accuracy 0.890500009059906\n",
      "Iteration 6550 Training loss 0.0008388740825466812 Validation loss 0.0437355563044548 Accuracy 0.890999972820282\n",
      "Iteration 6560 Training loss 0.0010917149484157562 Validation loss 0.04372545704245567 Accuracy 0.890500009059906\n",
      "Iteration 6570 Training loss 0.0015986497746780515 Validation loss 0.043749548494815826 Accuracy 0.8895000219345093\n",
      "Iteration 6580 Training loss 0.000837900850456208 Validation loss 0.04368359595537186 Accuracy 0.8899999856948853\n",
      "Iteration 6590 Training loss 0.0008384030661545694 Validation loss 0.04375860467553139 Accuracy 0.890999972820282\n",
      "Iteration 6600 Training loss 0.0003378189285285771 Validation loss 0.043702132999897 Accuracy 0.8899999856948853\n",
      "Iteration 6610 Training loss 0.0018380939727649093 Validation loss 0.04375087097287178 Accuracy 0.890999972820282\n",
      "Iteration 6620 Training loss 0.0008317164611071348 Validation loss 0.04374585300683975 Accuracy 0.890999972820282\n",
      "Iteration 6630 Training loss 0.0005877093062736094 Validation loss 0.043752625584602356 Accuracy 0.8899999856948853\n",
      "Iteration 6640 Training loss 0.0003437130362726748 Validation loss 0.04376472532749176 Accuracy 0.8895000219345093\n",
      "Iteration 6650 Training loss 0.00034431120730005205 Validation loss 0.043748755007982254 Accuracy 0.8889999985694885\n",
      "Iteration 6660 Training loss 0.0005808901623822749 Validation loss 0.04371519014239311 Accuracy 0.890500009059906\n",
      "Iteration 6670 Training loss 0.0008421880775131285 Validation loss 0.04376042261719704 Accuracy 0.890500009059906\n",
      "Iteration 6680 Training loss 0.0010939311468973756 Validation loss 0.043735213577747345 Accuracy 0.890999972820282\n",
      "Iteration 6690 Training loss 0.0013434377033263445 Validation loss 0.04374338313937187 Accuracy 0.8899999856948853\n",
      "Iteration 6700 Training loss 0.0013420295435935259 Validation loss 0.043706681579351425 Accuracy 0.8899999856948853\n",
      "Iteration 6710 Training loss 0.0010839003371074796 Validation loss 0.04373189061880112 Accuracy 0.8899999856948853\n",
      "Iteration 6720 Training loss 0.0010849075624719262 Validation loss 0.04367443174123764 Accuracy 0.890500009059906\n",
      "Iteration 6730 Training loss 0.0003307729202788323 Validation loss 0.043720122426748276 Accuracy 0.8899999856948853\n",
      "Iteration 6740 Training loss 0.0013327058404684067 Validation loss 0.043741028755903244 Accuracy 0.8899999856948853\n",
      "Iteration 6750 Training loss 0.0003354130021762103 Validation loss 0.04370076581835747 Accuracy 0.8899999856948853\n",
      "Iteration 6760 Training loss 0.0008300444460473955 Validation loss 0.04370855167508125 Accuracy 0.8895000219345093\n",
      "Iteration 6770 Training loss 0.0010819453746080399 Validation loss 0.043736062943935394 Accuracy 0.890500009059906\n",
      "Iteration 6780 Training loss 0.0013298895210027695 Validation loss 0.043723102658987045 Accuracy 0.8895000219345093\n",
      "Iteration 6790 Training loss 0.0013334649847820401 Validation loss 0.0437130443751812 Accuracy 0.890500009059906\n",
      "Iteration 6800 Training loss 0.0003374603402335197 Validation loss 0.04371396452188492 Accuracy 0.8899999856948853\n",
      "Iteration 6810 Training loss 0.0003438183630350977 Validation loss 0.04375029355287552 Accuracy 0.890500009059906\n",
      "Iteration 6820 Training loss 0.0005885042482987046 Validation loss 0.04373013228178024 Accuracy 0.8899999856948853\n",
      "Iteration 6830 Training loss 0.0005826742853969336 Validation loss 0.04372820630669594 Accuracy 0.890500009059906\n",
      "Iteration 6840 Training loss 0.0005847555585205555 Validation loss 0.043708685785532 Accuracy 0.890500009059906\n",
      "Iteration 6850 Training loss 9.320000390289351e-05 Validation loss 0.04372917860746384 Accuracy 0.8899999856948853\n",
      "Iteration 6860 Training loss 0.0013302804436534643 Validation loss 0.043725114315748215 Accuracy 0.890999972820282\n",
      "Iteration 6870 Training loss 0.0008348984410986304 Validation loss 0.04370328411459923 Accuracy 0.8895000219345093\n",
      "Iteration 6880 Training loss 0.0010877290042117238 Validation loss 0.04373607039451599 Accuracy 0.8899999856948853\n",
      "Iteration 6890 Training loss 0.001337310764938593 Validation loss 0.04371951147913933 Accuracy 0.8895000219345093\n",
      "Iteration 6900 Training loss 0.000591387040913105 Validation loss 0.04371440410614014 Accuracy 0.890500009059906\n",
      "Iteration 6910 Training loss 0.0005861597019247711 Validation loss 0.04373737424612045 Accuracy 0.890500009059906\n",
      "Iteration 6920 Training loss 0.0005889793974347413 Validation loss 0.04371319338679314 Accuracy 0.890999972820282\n",
      "Iteration 6930 Training loss 8.282931958092377e-05 Validation loss 0.04371350631117821 Accuracy 0.890500009059906\n",
      "Iteration 6940 Training loss 0.0008354867459274828 Validation loss 0.04368308559060097 Accuracy 0.890500009059906\n",
      "Iteration 6950 Training loss 0.0008382279775105417 Validation loss 0.04369623214006424 Accuracy 0.890500009059906\n",
      "Iteration 6960 Training loss 0.001332010026089847 Validation loss 0.043717849999666214 Accuracy 0.8899999856948853\n",
      "Iteration 6970 Training loss 0.0008312679710797966 Validation loss 0.04375718533992767 Accuracy 0.890500009059906\n",
      "Iteration 6980 Training loss 0.0008326037204824388 Validation loss 0.04373543709516525 Accuracy 0.890999972820282\n",
      "Iteration 6990 Training loss 0.0010864492505788803 Validation loss 0.04373210296034813 Accuracy 0.890500009059906\n",
      "Iteration 7000 Training loss 0.0003352009516675025 Validation loss 0.04370667412877083 Accuracy 0.890500009059906\n",
      "Iteration 7010 Training loss 0.0010878670727834105 Validation loss 0.04368468374013901 Accuracy 0.890500009059906\n",
      "Iteration 7020 Training loss 0.000333411677274853 Validation loss 0.04367684945464134 Accuracy 0.890500009059906\n",
      "Iteration 7030 Training loss 0.0023334422148764133 Validation loss 0.04367756471037865 Accuracy 0.8899999856948853\n",
      "Iteration 7040 Training loss 0.0008383277454413474 Validation loss 0.04369870200753212 Accuracy 0.8899999856948853\n",
      "Iteration 7050 Training loss 0.0010824038181453943 Validation loss 0.04365946725010872 Accuracy 0.890500009059906\n",
      "Iteration 7060 Training loss 0.0013428091770038009 Validation loss 0.0436948761343956 Accuracy 0.890500009059906\n",
      "Iteration 7070 Training loss 0.0015863885637372732 Validation loss 0.04370349645614624 Accuracy 0.890500009059906\n",
      "Iteration 7080 Training loss 0.0008331391727551818 Validation loss 0.04372700676321983 Accuracy 0.890999972820282\n",
      "Iteration 7090 Training loss 0.0010831542313098907 Validation loss 0.04370513930916786 Accuracy 0.890500009059906\n",
      "Iteration 7100 Training loss 0.0005873724003322423 Validation loss 0.04373630881309509 Accuracy 0.8899999856948853\n",
      "Iteration 7110 Training loss 0.0005976054817438126 Validation loss 0.04373704642057419 Accuracy 0.8899999856948853\n",
      "Iteration 7120 Training loss 0.0008295794832520187 Validation loss 0.0436902791261673 Accuracy 0.890500009059906\n",
      "Iteration 7130 Training loss 0.001086616306565702 Validation loss 0.04369610920548439 Accuracy 0.8899999856948853\n",
      "Iteration 7140 Training loss 0.0010776682756841183 Validation loss 0.04365220665931702 Accuracy 0.8899999856948853\n",
      "Iteration 7150 Training loss 0.0010849632089957595 Validation loss 0.04367801547050476 Accuracy 0.890999972820282\n",
      "Iteration 7160 Training loss 0.0008350083371624351 Validation loss 0.04364251345396042 Accuracy 0.890500009059906\n",
      "Iteration 7170 Training loss 0.0015848507173359394 Validation loss 0.04366568848490715 Accuracy 0.890500009059906\n",
      "Iteration 7180 Training loss 0.0008393179159611464 Validation loss 0.04367965832352638 Accuracy 0.890500009059906\n",
      "Iteration 7190 Training loss 0.0008422068203799427 Validation loss 0.04371023178100586 Accuracy 0.8899999856948853\n",
      "Iteration 7200 Training loss 0.0010849562240764499 Validation loss 0.04368341341614723 Accuracy 0.8899999856948853\n",
      "Iteration 7210 Training loss 0.0015844338340684772 Validation loss 0.04371529817581177 Accuracy 0.890500009059906\n",
      "Iteration 7220 Training loss 0.0008306418894790113 Validation loss 0.04369570314884186 Accuracy 0.8899999856948853\n",
      "Iteration 7230 Training loss 0.0005987102631479502 Validation loss 0.04372600093483925 Accuracy 0.8914999961853027\n",
      "Iteration 7240 Training loss 0.0013342861784622073 Validation loss 0.04373940825462341 Accuracy 0.890500009059906\n",
      "Iteration 7250 Training loss 0.0013331271475180984 Validation loss 0.04369090124964714 Accuracy 0.890500009059906\n",
      "Iteration 7260 Training loss 0.0015858289552852511 Validation loss 0.043705184012651443 Accuracy 0.890500009059906\n",
      "Iteration 7270 Training loss 0.0005803954554721713 Validation loss 0.04371572285890579 Accuracy 0.8899999856948853\n",
      "Iteration 7280 Training loss 0.001333260559476912 Validation loss 0.04367992281913757 Accuracy 0.890500009059906\n",
      "Iteration 7290 Training loss 0.0015829160111024976 Validation loss 0.04368804022669792 Accuracy 0.890500009059906\n",
      "Iteration 7300 Training loss 0.0005830210284329951 Validation loss 0.04368393495678902 Accuracy 0.890999972820282\n",
      "Iteration 7310 Training loss 0.0015790287870913744 Validation loss 0.043730758130550385 Accuracy 0.890999972820282\n",
      "Iteration 7320 Training loss 0.001336952904239297 Validation loss 0.04369313642382622 Accuracy 0.890500009059906\n",
      "Iteration 7330 Training loss 0.0015835039084777236 Validation loss 0.04370690509676933 Accuracy 0.890999972820282\n",
      "Iteration 7340 Training loss 0.0010824415367096663 Validation loss 0.043694354593753815 Accuracy 0.890500009059906\n",
      "Iteration 7350 Training loss 0.0008333215955644846 Validation loss 0.043663300573825836 Accuracy 0.890500009059906\n",
      "Iteration 7360 Training loss 0.00033127766801044345 Validation loss 0.04367745295166969 Accuracy 0.890999972820282\n",
      "Iteration 7370 Training loss 0.0005814682808704674 Validation loss 0.04366403818130493 Accuracy 0.890500009059906\n",
      "Iteration 7380 Training loss 0.0013281507417559624 Validation loss 0.04365887865424156 Accuracy 0.890999972820282\n",
      "Iteration 7390 Training loss 0.001081642578355968 Validation loss 0.04369388520717621 Accuracy 0.890500009059906\n",
      "Iteration 7400 Training loss 0.001584153389558196 Validation loss 0.04370010644197464 Accuracy 0.890500009059906\n",
      "Iteration 7410 Training loss 0.0008284549694508314 Validation loss 0.04368438571691513 Accuracy 0.8899999856948853\n",
      "Iteration 7420 Training loss 0.0013344527687877417 Validation loss 0.043715883046388626 Accuracy 0.890999972820282\n",
      "Iteration 7430 Training loss 0.0005784198292531073 Validation loss 0.04368134215474129 Accuracy 0.890999972820282\n",
      "Iteration 7440 Training loss 0.00033167595393024385 Validation loss 0.04368963465094566 Accuracy 0.890999972820282\n",
      "Iteration 7450 Training loss 0.00033016238012351096 Validation loss 0.043750353157520294 Accuracy 0.8899999856948853\n",
      "Iteration 7460 Training loss 0.000831884506624192 Validation loss 0.04372696578502655 Accuracy 0.890999972820282\n",
      "Iteration 7470 Training loss 0.0008261986658908427 Validation loss 0.04372551664710045 Accuracy 0.890500009059906\n",
      "Iteration 7480 Training loss 0.0013331413501873612 Validation loss 0.04371235892176628 Accuracy 0.890500009059906\n",
      "Iteration 7490 Training loss 0.0013301068684086204 Validation loss 0.04368693754076958 Accuracy 0.890500009059906\n",
      "Iteration 7500 Training loss 7.387527875835076e-05 Validation loss 0.04370780289173126 Accuracy 0.890500009059906\n",
      "Iteration 7510 Training loss 0.0018236162140965462 Validation loss 0.0436989888548851 Accuracy 0.890500009059906\n",
      "Iteration 7520 Training loss 0.0005810406873933971 Validation loss 0.04371907189488411 Accuracy 0.8889999985694885\n",
      "Iteration 7530 Training loss 0.0013256545644253492 Validation loss 0.04370327666401863 Accuracy 0.890500009059906\n",
      "Iteration 7540 Training loss 0.0008276132284663618 Validation loss 0.04367813095450401 Accuracy 0.890999972820282\n",
      "Iteration 7550 Training loss 0.0005792593583464622 Validation loss 0.0436859093606472 Accuracy 0.890500009059906\n",
      "Iteration 7560 Training loss 0.0008313347352668643 Validation loss 0.0437166765332222 Accuracy 0.8895000219345093\n",
      "Iteration 7570 Training loss 0.0010853949934244156 Validation loss 0.04370089992880821 Accuracy 0.8895000219345093\n",
      "Iteration 7580 Training loss 0.001077891793102026 Validation loss 0.04368811473250389 Accuracy 0.8889999985694885\n",
      "Iteration 7590 Training loss 0.0005843127146363258 Validation loss 0.04368218407034874 Accuracy 0.890500009059906\n",
      "Iteration 7600 Training loss 0.000587550166528672 Validation loss 0.04370943084359169 Accuracy 0.8899999856948853\n",
      "Iteration 7610 Training loss 0.0015797641826793551 Validation loss 0.04372313618659973 Accuracy 0.8899999856948853\n",
      "Iteration 7620 Training loss 0.0015788741875439882 Validation loss 0.04371928423643112 Accuracy 0.890500009059906\n",
      "Iteration 7630 Training loss 0.0008304968941956758 Validation loss 0.04372400417923927 Accuracy 0.8899999856948853\n",
      "Iteration 7640 Training loss 0.00033094509853981435 Validation loss 0.04373042657971382 Accuracy 0.8899999856948853\n",
      "Iteration 7650 Training loss 0.001077940221875906 Validation loss 0.043710578233003616 Accuracy 0.890500009059906\n",
      "Iteration 7660 Training loss 0.0008262553019449115 Validation loss 0.043679866939783096 Accuracy 0.8899999856948853\n",
      "Iteration 7670 Training loss 0.001824279548600316 Validation loss 0.043711788952350616 Accuracy 0.890500009059906\n",
      "Iteration 7680 Training loss 0.0008252563420683146 Validation loss 0.04370458424091339 Accuracy 0.890500009059906\n",
      "Iteration 7690 Training loss 0.00208313949406147 Validation loss 0.043665289878845215 Accuracy 0.8899999856948853\n",
      "Iteration 7700 Training loss 0.00032992003252729774 Validation loss 0.04366696998476982 Accuracy 0.8895000219345093\n",
      "Iteration 7710 Training loss 0.00207888288423419 Validation loss 0.04367797076702118 Accuracy 0.8899999856948853\n",
      "Iteration 7720 Training loss 0.001076859189197421 Validation loss 0.043697770684957504 Accuracy 0.8895000219345093\n",
      "Iteration 7730 Training loss 0.0018316948553547263 Validation loss 0.04371637850999832 Accuracy 0.8899999856948853\n",
      "Iteration 7740 Training loss 0.000831172801554203 Validation loss 0.04365936294198036 Accuracy 0.890500009059906\n",
      "Iteration 7750 Training loss 0.001068234327249229 Validation loss 0.04368066415190697 Accuracy 0.890500009059906\n",
      "Iteration 7760 Training loss 0.0008274638676084578 Validation loss 0.043702382594347 Accuracy 0.8899999856948853\n",
      "Iteration 7770 Training loss 0.0013269084738567472 Validation loss 0.04367252439260483 Accuracy 0.8895000219345093\n",
      "Iteration 7780 Training loss 0.0010733935050666332 Validation loss 0.04369008541107178 Accuracy 0.890500009059906\n",
      "Iteration 7790 Training loss 0.0008244688506238163 Validation loss 0.043686095625162125 Accuracy 0.8899999856948853\n",
      "Iteration 7800 Training loss 0.0008300103945657611 Validation loss 0.04367076978087425 Accuracy 0.8895000219345093\n",
      "Iteration 7810 Training loss 0.0013271564384922385 Validation loss 0.04372258856892586 Accuracy 0.890500009059906\n",
      "Iteration 7820 Training loss 0.00033370457822456956 Validation loss 0.04369980841875076 Accuracy 0.8895000219345093\n",
      "Iteration 7830 Training loss 0.0023274198174476624 Validation loss 0.04371796175837517 Accuracy 0.8899999856948853\n",
      "Iteration 7840 Training loss 0.0005863858968950808 Validation loss 0.04371725395321846 Accuracy 0.8895000219345093\n",
      "Iteration 7850 Training loss 0.0010874717263504863 Validation loss 0.04371222108602524 Accuracy 0.890500009059906\n",
      "Iteration 7860 Training loss 0.0015788455493748188 Validation loss 0.043626248836517334 Accuracy 0.890999972820282\n",
      "Iteration 7870 Training loss 0.0010816393187269568 Validation loss 0.04364565759897232 Accuracy 0.890500009059906\n",
      "Iteration 7880 Training loss 0.0005713846767321229 Validation loss 0.04366815835237503 Accuracy 0.890999972820282\n",
      "Iteration 7890 Training loss 0.0005805811379104853 Validation loss 0.043702997267246246 Accuracy 0.8899999856948853\n",
      "Iteration 7900 Training loss 0.0015760279493406415 Validation loss 0.04367244616150856 Accuracy 0.890500009059906\n",
      "Iteration 7910 Training loss 0.00033153785625472665 Validation loss 0.04369032755494118 Accuracy 0.8899999856948853\n",
      "Iteration 7920 Training loss 0.0018367175944149494 Validation loss 0.04369121417403221 Accuracy 0.890500009059906\n",
      "Iteration 7930 Training loss 0.000582934997510165 Validation loss 0.04370103403925896 Accuracy 0.890500009059906\n",
      "Iteration 7940 Training loss 0.0005762066575698555 Validation loss 0.043694354593753815 Accuracy 0.8899999856948853\n",
      "Iteration 7950 Training loss 0.0008265243959613144 Validation loss 0.04369913786649704 Accuracy 0.890500009059906\n",
      "Iteration 7960 Training loss 0.0015826061135157943 Validation loss 0.043667640537023544 Accuracy 0.890999972820282\n",
      "Iteration 7970 Training loss 0.0010729728965088725 Validation loss 0.04368314892053604 Accuracy 0.8899999856948853\n",
      "Iteration 7980 Training loss 0.000576882972382009 Validation loss 0.04367417097091675 Accuracy 0.890999972820282\n",
      "Iteration 7990 Training loss 0.0010828222148120403 Validation loss 0.043635476380586624 Accuracy 0.890999972820282\n",
      "Iteration 8000 Training loss 0.00033143325708806515 Validation loss 0.043669309467077255 Accuracy 0.8899999856948853\n",
      "Iteration 8010 Training loss 0.0013199429959058762 Validation loss 0.043694037944078445 Accuracy 0.890999972820282\n",
      "Iteration 8020 Training loss 0.0008268476813100278 Validation loss 0.04371216893196106 Accuracy 0.890500009059906\n",
      "Iteration 8030 Training loss 0.0008268760866485536 Validation loss 0.04369795322418213 Accuracy 0.8914999961853027\n",
      "Iteration 8040 Training loss 0.0005750089767389 Validation loss 0.04371372237801552 Accuracy 0.890999972820282\n",
      "Iteration 8050 Training loss 0.0010795854032039642 Validation loss 0.04369621351361275 Accuracy 0.890500009059906\n",
      "Iteration 8060 Training loss 0.0005787804257124662 Validation loss 0.04365910217165947 Accuracy 0.8899999856948853\n",
      "Iteration 8070 Training loss 0.0013278055703267455 Validation loss 0.04365525022149086 Accuracy 0.8895000219345093\n",
      "Iteration 8080 Training loss 0.0008214046829380095 Validation loss 0.04366820678114891 Accuracy 0.8899999856948853\n",
      "Iteration 8090 Training loss 0.0008267779485322535 Validation loss 0.04368086904287338 Accuracy 0.8895000219345093\n",
      "Iteration 8100 Training loss 0.0010849271202459931 Validation loss 0.043634045869112015 Accuracy 0.890999972820282\n",
      "Iteration 8110 Training loss 0.0010753765236586332 Validation loss 0.043677281588315964 Accuracy 0.890500009059906\n",
      "Iteration 8120 Training loss 0.0008259487221948802 Validation loss 0.043665800243616104 Accuracy 0.8899999856948853\n",
      "Iteration 8130 Training loss 0.0010730080539360642 Validation loss 0.04365021362900734 Accuracy 0.8899999856948853\n",
      "Iteration 8140 Training loss 0.0013294874224811792 Validation loss 0.04364202171564102 Accuracy 0.890500009059906\n",
      "Iteration 8150 Training loss 0.0005800540093332529 Validation loss 0.043668389320373535 Accuracy 0.890999972820282\n",
      "Iteration 8160 Training loss 0.0018247966654598713 Validation loss 0.043669313192367554 Accuracy 0.8899999856948853\n",
      "Iteration 8170 Training loss 0.0008279958856292069 Validation loss 0.043668899685144424 Accuracy 0.890500009059906\n",
      "Iteration 8180 Training loss 0.00032415156601928174 Validation loss 0.04367008060216904 Accuracy 0.890500009059906\n",
      "Iteration 8190 Training loss 0.0010811497922986746 Validation loss 0.04366010054945946 Accuracy 0.890500009059906\n",
      "Iteration 8200 Training loss 0.0013311648508533835 Validation loss 0.043672095984220505 Accuracy 0.890500009059906\n",
      "Iteration 8210 Training loss 0.0013215703656896949 Validation loss 0.04365107789635658 Accuracy 0.890999972820282\n",
      "Iteration 8220 Training loss 0.0005782023654319346 Validation loss 0.04366559162735939 Accuracy 0.890500009059906\n",
      "Iteration 8230 Training loss 0.0013285853201523423 Validation loss 0.04365568608045578 Accuracy 0.8895000219345093\n",
      "Iteration 8240 Training loss 0.0008244409691542387 Validation loss 0.04368535056710243 Accuracy 0.890500009059906\n",
      "Iteration 8250 Training loss 0.0008217562572099268 Validation loss 0.043658509850502014 Accuracy 0.8895000219345093\n",
      "Iteration 8260 Training loss 0.0010768868960440159 Validation loss 0.043654605746269226 Accuracy 0.8889999985694885\n",
      "Iteration 8270 Training loss 0.0005706500960513949 Validation loss 0.04366569593548775 Accuracy 0.8899999856948853\n",
      "Iteration 8280 Training loss 0.0013257927494123578 Validation loss 0.04367254674434662 Accuracy 0.8895000219345093\n",
      "Iteration 8290 Training loss 0.0010765176266431808 Validation loss 0.04364246129989624 Accuracy 0.8895000219345093\n",
      "Iteration 8300 Training loss 0.0005756113096140325 Validation loss 0.04364016652107239 Accuracy 0.8895000219345093\n",
      "Iteration 8310 Training loss 0.00058475456899032 Validation loss 0.04366098716855049 Accuracy 0.8899999856948853\n",
      "Iteration 8320 Training loss 0.001083840848878026 Validation loss 0.04364906996488571 Accuracy 0.890500009059906\n",
      "Iteration 8330 Training loss 0.001332514570094645 Validation loss 0.04367094486951828 Accuracy 0.890500009059906\n",
      "Iteration 8340 Training loss 0.0013239679392427206 Validation loss 0.043646134436130524 Accuracy 0.8899999856948853\n",
      "Iteration 8350 Training loss 0.0010750507935881615 Validation loss 0.04363004118204117 Accuracy 0.8895000219345093\n",
      "Iteration 8360 Training loss 0.0015797255327925086 Validation loss 0.043648768216371536 Accuracy 0.890999972820282\n",
      "Iteration 8370 Training loss 0.001072655082680285 Validation loss 0.04363403469324112 Accuracy 0.8895000219345093\n",
      "Iteration 8380 Training loss 0.0013255748199298978 Validation loss 0.04362550005316734 Accuracy 0.8899999856948853\n",
      "Iteration 8390 Training loss 0.0010710129281505942 Validation loss 0.043677039444446564 Accuracy 0.8895000219345093\n",
      "Iteration 8400 Training loss 0.0005789591814391315 Validation loss 0.04368400573730469 Accuracy 0.890500009059906\n",
      "Iteration 8410 Training loss 0.0005829417495988309 Validation loss 0.04366060346364975 Accuracy 0.8895000219345093\n",
      "Iteration 8420 Training loss 0.0013289988273754716 Validation loss 0.04366782307624817 Accuracy 0.8899999856948853\n",
      "Iteration 8430 Training loss 0.0013296649558469653 Validation loss 0.043666742742061615 Accuracy 0.890500009059906\n",
      "Iteration 8440 Training loss 0.0013309776550158858 Validation loss 0.04364994168281555 Accuracy 0.8899999856948853\n",
      "Iteration 8450 Training loss 0.0005763634108006954 Validation loss 0.04368748888373375 Accuracy 0.8899999856948853\n",
      "Iteration 8460 Training loss 0.001083377399481833 Validation loss 0.04362967982888222 Accuracy 0.8895000219345093\n",
      "Iteration 8470 Training loss 0.001331495470367372 Validation loss 0.04364750534296036 Accuracy 0.8895000219345093\n",
      "Iteration 8480 Training loss 0.0013223635032773018 Validation loss 0.04363711550831795 Accuracy 0.8899999856948853\n",
      "Iteration 8490 Training loss 0.0020672441460192204 Validation loss 0.04361524060368538 Accuracy 0.8899999856948853\n",
      "Iteration 8500 Training loss 8.458492084173486e-05 Validation loss 0.043645378202199936 Accuracy 0.8895000219345093\n",
      "Iteration 8510 Training loss 0.0008278512977994978 Validation loss 0.043657075613737106 Accuracy 0.8899999856948853\n",
      "Iteration 8520 Training loss 0.000829507946036756 Validation loss 0.0436774305999279 Accuracy 0.890999972820282\n",
      "Iteration 8530 Training loss 0.0015755477361381054 Validation loss 0.04368416592478752 Accuracy 0.890500009059906\n",
      "Iteration 8540 Training loss 0.001076115993782878 Validation loss 0.04370451346039772 Accuracy 0.890999972820282\n",
      "Iteration 8550 Training loss 0.001572701265104115 Validation loss 0.043652016669511795 Accuracy 0.8899999856948853\n",
      "Iteration 8560 Training loss 0.001328728860244155 Validation loss 0.04366738349199295 Accuracy 0.890500009059906\n",
      "Iteration 8570 Training loss 0.0008203915203921497 Validation loss 0.0436193011701107 Accuracy 0.8899999856948853\n",
      "Iteration 8580 Training loss 0.0013232659548521042 Validation loss 0.043654099106788635 Accuracy 0.8899999856948853\n",
      "Iteration 8590 Training loss 0.0008240612805821002 Validation loss 0.04365351051092148 Accuracy 0.8895000219345093\n",
      "Iteration 8600 Training loss 0.000829497235827148 Validation loss 0.04365990310907364 Accuracy 0.890999972820282\n",
      "Iteration 8610 Training loss 0.0008264404605142772 Validation loss 0.04362662881612778 Accuracy 0.8899999856948853\n",
      "Iteration 8620 Training loss 0.000827002979349345 Validation loss 0.043628714978694916 Accuracy 0.890999972820282\n",
      "Iteration 8630 Training loss 0.0003229091234970838 Validation loss 0.04362671449780464 Accuracy 0.8895000219345093\n",
      "Iteration 8640 Training loss 0.0003260454977862537 Validation loss 0.04364196956157684 Accuracy 0.890500009059906\n",
      "Iteration 8650 Training loss 0.0005825332482345402 Validation loss 0.04363603517413139 Accuracy 0.8895000219345093\n",
      "Iteration 8660 Training loss 0.0015749435406178236 Validation loss 0.04364679753780365 Accuracy 0.890999972820282\n",
      "Iteration 8670 Training loss 0.0010786482598632574 Validation loss 0.04362070560455322 Accuracy 0.890500009059906\n",
      "Iteration 8680 Training loss 0.0015746887074783444 Validation loss 0.04363325610756874 Accuracy 0.8899999856948853\n",
      "Iteration 8690 Training loss 0.0008243442862294614 Validation loss 0.04362017661333084 Accuracy 0.8899999856948853\n",
      "Iteration 8700 Training loss 0.0008178369025699794 Validation loss 0.0436125323176384 Accuracy 0.8899999856948853\n",
      "Iteration 8710 Training loss 0.0005745504167862236 Validation loss 0.04363030940294266 Accuracy 0.8914999961853027\n",
      "Iteration 8720 Training loss 0.000577262369915843 Validation loss 0.043622978031635284 Accuracy 0.890500009059906\n",
      "Iteration 8730 Training loss 0.00107327732257545 Validation loss 0.0436694398522377 Accuracy 0.8914999961853027\n",
      "Iteration 8740 Training loss 0.0008262402843683958 Validation loss 0.04365703836083412 Accuracy 0.8914999961853027\n",
      "Iteration 8750 Training loss 0.0023213643580675125 Validation loss 0.04363344982266426 Accuracy 0.890500009059906\n",
      "Iteration 8760 Training loss 0.0010789901716634631 Validation loss 0.043632399290800095 Accuracy 0.890999972820282\n",
      "Iteration 8770 Training loss 0.001826337305828929 Validation loss 0.04361165314912796 Accuracy 0.8899999856948853\n",
      "Iteration 8780 Training loss 0.0008195071713998914 Validation loss 0.04362787306308746 Accuracy 0.8899999856948853\n",
      "Iteration 8790 Training loss 0.0008229981758631766 Validation loss 0.043644461780786514 Accuracy 0.890500009059906\n",
      "Iteration 8800 Training loss 0.0008174611139111221 Validation loss 0.04361892491579056 Accuracy 0.890500009059906\n",
      "Iteration 8810 Training loss 0.0010672083590179682 Validation loss 0.04363371059298515 Accuracy 0.8914999961853027\n",
      "Iteration 8820 Training loss 0.0008189371437765658 Validation loss 0.04361043870449066 Accuracy 0.890500009059906\n",
      "Iteration 8830 Training loss 0.0013216345105320215 Validation loss 0.04362666606903076 Accuracy 0.890999972820282\n",
      "Iteration 8840 Training loss 0.0008261635666713119 Validation loss 0.043632782995700836 Accuracy 0.890999972820282\n",
      "Iteration 8850 Training loss 0.0018228921107947826 Validation loss 0.043615493923425674 Accuracy 0.8895000219345093\n",
      "Iteration 8860 Training loss 0.0013262784341350198 Validation loss 0.04363291338086128 Accuracy 0.890999972820282\n",
      "Iteration 8870 Training loss 0.0005758847109973431 Validation loss 0.04362686723470688 Accuracy 0.8889999985694885\n",
      "Iteration 8880 Training loss 0.0003201981890015304 Validation loss 0.04365810751914978 Accuracy 0.890500009059906\n",
      "Iteration 8890 Training loss 0.0010768912034109235 Validation loss 0.0436161570250988 Accuracy 0.890500009059906\n",
      "Iteration 8900 Training loss 0.0013238165993243456 Validation loss 0.04360250383615494 Accuracy 0.890500009059906\n",
      "Iteration 8910 Training loss 0.0008202639874070883 Validation loss 0.04361232742667198 Accuracy 0.8895000219345093\n",
      "Iteration 8920 Training loss 0.0010773225221782923 Validation loss 0.04364079236984253 Accuracy 0.8899999856948853\n",
      "Iteration 8930 Training loss 0.0005752904689870775 Validation loss 0.04361436516046524 Accuracy 0.8899999856948853\n",
      "Iteration 8940 Training loss 0.00031905490322969854 Validation loss 0.043626267462968826 Accuracy 0.8899999856948853\n",
      "Iteration 8950 Training loss 0.0005747850518673658 Validation loss 0.04360920563340187 Accuracy 0.890500009059906\n",
      "Iteration 8960 Training loss 0.0008263553027063608 Validation loss 0.04357418045401573 Accuracy 0.8895000219345093\n",
      "Iteration 8970 Training loss 0.0023170807398855686 Validation loss 0.043613798916339874 Accuracy 0.890500009059906\n",
      "Iteration 8980 Training loss 0.0010764299659058452 Validation loss 0.04361776262521744 Accuracy 0.890999972820282\n",
      "Iteration 8990 Training loss 0.0008164862520061433 Validation loss 0.04365244880318642 Accuracy 0.8914999961853027\n",
      "Iteration 9000 Training loss 0.001574971596710384 Validation loss 0.04364180937409401 Accuracy 0.8899999856948853\n",
      "Iteration 9010 Training loss 0.0010765118058770895 Validation loss 0.04362880811095238 Accuracy 0.8899999856948853\n",
      "Iteration 9020 Training loss 0.0008233824046328664 Validation loss 0.04358980432152748 Accuracy 0.8895000219345093\n",
      "Iteration 9030 Training loss 0.000818898668512702 Validation loss 0.04361991211771965 Accuracy 0.890500009059906\n",
      "Iteration 9040 Training loss 0.0005721893394365907 Validation loss 0.04363236948847771 Accuracy 0.8899999856948853\n",
      "Iteration 9050 Training loss 0.0005764343659393489 Validation loss 0.04363679885864258 Accuracy 0.8899999856948853\n",
      "Iteration 9060 Training loss 0.0013211237965151668 Validation loss 0.04361375793814659 Accuracy 0.8914999961853027\n",
      "Iteration 9070 Training loss 0.0010686788009479642 Validation loss 0.04365893453359604 Accuracy 0.890999972820282\n",
      "Iteration 9080 Training loss 0.0013257499085739255 Validation loss 0.0436190702021122 Accuracy 0.8899999856948853\n",
      "Iteration 9090 Training loss 0.001076563261449337 Validation loss 0.04361393675208092 Accuracy 0.8889999985694885\n",
      "Iteration 9100 Training loss 0.0010718049015849829 Validation loss 0.043632347136735916 Accuracy 0.8899999856948853\n",
      "Iteration 9110 Training loss 0.0018194089643657207 Validation loss 0.04357466846704483 Accuracy 0.8895000219345093\n",
      "Iteration 9120 Training loss 0.0015731831081211567 Validation loss 0.0436401292681694 Accuracy 0.8914999961853027\n",
      "Iteration 9130 Training loss 0.0018273863242939115 Validation loss 0.04365003854036331 Accuracy 0.8899999856948853\n",
      "Iteration 9140 Training loss 0.0003269913140684366 Validation loss 0.043635718524456024 Accuracy 0.8889999985694885\n",
      "Iteration 9150 Training loss 0.0008218232542276382 Validation loss 0.04364388436079025 Accuracy 0.890999972820282\n",
      "Iteration 9160 Training loss 0.001323541859164834 Validation loss 0.04360140860080719 Accuracy 0.8889999985694885\n",
      "Iteration 9170 Training loss 0.001319707604125142 Validation loss 0.04359995201230049 Accuracy 0.8899999856948853\n",
      "Iteration 9180 Training loss 0.0008194310939870775 Validation loss 0.04360508546233177 Accuracy 0.8899999856948853\n",
      "Iteration 9190 Training loss 0.0005767169059254229 Validation loss 0.04362427443265915 Accuracy 0.8899999856948853\n",
      "Iteration 9200 Training loss 0.0008283654460683465 Validation loss 0.043609362095594406 Accuracy 0.8889999985694885\n",
      "Iteration 9210 Training loss 0.0008205834310501814 Validation loss 0.04362265765666962 Accuracy 0.8899999856948853\n",
      "Iteration 9220 Training loss 0.0013194772182032466 Validation loss 0.043641895055770874 Accuracy 0.8889999985694885\n",
      "Iteration 9230 Training loss 0.001569944666698575 Validation loss 0.04361515864729881 Accuracy 0.8895000219345093\n",
      "Iteration 9240 Training loss 0.0005702535272575915 Validation loss 0.043610066175460815 Accuracy 0.8889999985694885\n",
      "Iteration 9250 Training loss 0.0008205361082218587 Validation loss 0.043654270470142365 Accuracy 0.890999972820282\n",
      "Iteration 9260 Training loss 0.0003176920290570706 Validation loss 0.04359932243824005 Accuracy 0.8899999856948853\n",
      "Iteration 9270 Training loss 0.000824861868750304 Validation loss 0.04361646622419357 Accuracy 0.8899999856948853\n",
      "Iteration 9280 Training loss 0.0005739459884352982 Validation loss 0.04361285641789436 Accuracy 0.890500009059906\n",
      "Iteration 9290 Training loss 0.0003208538400940597 Validation loss 0.04359095171093941 Accuracy 0.890500009059906\n",
      "Iteration 9300 Training loss 0.0005717378808185458 Validation loss 0.043601859360933304 Accuracy 0.890500009059906\n",
      "Iteration 9310 Training loss 0.0013276142999529839 Validation loss 0.04360278323292732 Accuracy 0.8899999856948853\n",
      "Iteration 9320 Training loss 0.0008250725222751498 Validation loss 0.04362013563513756 Accuracy 0.8899999856948853\n",
      "Iteration 9330 Training loss 0.0008302642381750047 Validation loss 0.04362807422876358 Accuracy 0.890500009059906\n",
      "Iteration 9340 Training loss 0.0003236658521927893 Validation loss 0.043615929782390594 Accuracy 0.890500009059906\n",
      "Iteration 9350 Training loss 0.0003182831278536469 Validation loss 0.04358775541186333 Accuracy 0.8899999856948853\n",
      "Iteration 9360 Training loss 0.0010722975712269545 Validation loss 0.04355408996343613 Accuracy 0.8895000219345093\n",
      "Iteration 9370 Training loss 0.0010767142521217465 Validation loss 0.04361408203840256 Accuracy 0.8899999856948853\n",
      "Iteration 9380 Training loss 0.0008213663822971284 Validation loss 0.043626196682453156 Accuracy 0.8895000219345093\n",
      "Iteration 9390 Training loss 0.0010655734222382307 Validation loss 0.04362273961305618 Accuracy 0.890500009059906\n",
      "Iteration 9400 Training loss 0.0010669344337657094 Validation loss 0.0435858815908432 Accuracy 0.8889999985694885\n",
      "Iteration 9410 Training loss 0.0013189840828999877 Validation loss 0.04360927641391754 Accuracy 0.890500009059906\n",
      "Iteration 9420 Training loss 0.0008200241718441248 Validation loss 0.04362545534968376 Accuracy 0.8899999856948853\n",
      "Iteration 9430 Training loss 0.0005716852028854191 Validation loss 0.0436122827231884 Accuracy 0.8895000219345093\n",
      "Iteration 9440 Training loss 0.0005768918781541288 Validation loss 0.043620575219392776 Accuracy 0.8895000219345093\n",
      "Iteration 9450 Training loss 0.0005735760787501931 Validation loss 0.04362485930323601 Accuracy 0.8895000219345093\n",
      "Iteration 9460 Training loss 0.0005749441334046423 Validation loss 0.04361164569854736 Accuracy 0.8884999752044678\n",
      "Iteration 9470 Training loss 0.0010675369994714856 Validation loss 0.04359455406665802 Accuracy 0.890999972820282\n",
      "Iteration 9480 Training loss 0.00032313543488271534 Validation loss 0.04360627755522728 Accuracy 0.890999972820282\n",
      "Iteration 9490 Training loss 0.0010679712286219 Validation loss 0.04361281916499138 Accuracy 0.890500009059906\n",
      "Iteration 9500 Training loss 0.0010713328374549747 Validation loss 0.04359391704201698 Accuracy 0.8899999856948853\n",
      "Iteration 9510 Training loss 0.0010701031424105167 Validation loss 0.043622419238090515 Accuracy 0.8899999856948853\n",
      "Iteration 9520 Training loss 0.0010664748260751367 Validation loss 0.04361472278833389 Accuracy 0.8899999856948853\n",
      "Iteration 9530 Training loss 0.0018204307416453958 Validation loss 0.04363930597901344 Accuracy 0.890500009059906\n",
      "Iteration 9540 Training loss 0.0008165242616087198 Validation loss 0.04355715587735176 Accuracy 0.8889999985694885\n",
      "Iteration 9550 Training loss 0.0005683673080056906 Validation loss 0.043627746403217316 Accuracy 0.8899999856948853\n",
      "Iteration 9560 Training loss 0.0015664190286770463 Validation loss 0.04359513521194458 Accuracy 0.8895000219345093\n",
      "Iteration 9570 Training loss 0.00031382415909320116 Validation loss 0.04358407482504845 Accuracy 0.8895000219345093\n",
      "Iteration 9580 Training loss 0.0015700755175203085 Validation loss 0.043609678745269775 Accuracy 0.890500009059906\n",
      "Iteration 9590 Training loss 0.001567501574754715 Validation loss 0.04357936605811119 Accuracy 0.890500009059906\n",
      "Iteration 9600 Training loss 0.0008204010664485395 Validation loss 0.043582797050476074 Accuracy 0.8889999985694885\n",
      "Iteration 9610 Training loss 0.0008167304913513362 Validation loss 0.04355451092123985 Accuracy 0.8889999985694885\n",
      "Iteration 9620 Training loss 0.0013216481311246753 Validation loss 0.0435718297958374 Accuracy 0.890500009059906\n",
      "Iteration 9630 Training loss 0.0015760862734168768 Validation loss 0.043561942875385284 Accuracy 0.890500009059906\n",
      "Iteration 9640 Training loss 0.0010690492345020175 Validation loss 0.043592825531959534 Accuracy 0.890500009059906\n",
      "Iteration 9650 Training loss 0.0010700465645641088 Validation loss 0.043548334389925 Accuracy 0.8889999985694885\n",
      "Iteration 9660 Training loss 0.001569901593029499 Validation loss 0.04356224834918976 Accuracy 0.8884999752044678\n",
      "Iteration 9670 Training loss 0.0008226120844483376 Validation loss 0.04357604309916496 Accuracy 0.8899999856948853\n",
      "Iteration 9680 Training loss 0.0008177371928468347 Validation loss 0.04358050599694252 Accuracy 0.8895000219345093\n",
      "Iteration 9690 Training loss 0.0005719168111681938 Validation loss 0.04360416531562805 Accuracy 0.8889999985694885\n",
      "Iteration 9700 Training loss 0.0008201299933716655 Validation loss 0.04359583929181099 Accuracy 0.8895000219345093\n",
      "Iteration 9710 Training loss 0.0013188159791752696 Validation loss 0.043626777827739716 Accuracy 0.8884999752044678\n",
      "Iteration 9720 Training loss 0.0013206069124862552 Validation loss 0.043599311262369156 Accuracy 0.8889999985694885\n",
      "Iteration 9730 Training loss 0.0010715675307437778 Validation loss 0.043599821627140045 Accuracy 0.8895000219345093\n",
      "Iteration 9740 Training loss 0.0005726351519115269 Validation loss 0.04362379014492035 Accuracy 0.8895000219345093\n",
      "Iteration 9750 Training loss 0.0013181230751797557 Validation loss 0.04357980936765671 Accuracy 0.8889999985694885\n",
      "Iteration 9760 Training loss 0.0008206681231968105 Validation loss 0.04360594227910042 Accuracy 0.8889999985694885\n",
      "Iteration 9770 Training loss 0.0013218540698289871 Validation loss 0.04359853267669678 Accuracy 0.8889999985694885\n",
      "Iteration 9780 Training loss 0.000569261028431356 Validation loss 0.04355870187282562 Accuracy 0.8884999752044678\n",
      "Iteration 9790 Training loss 0.0008208479266613722 Validation loss 0.04356004670262337 Accuracy 0.8889999985694885\n",
      "Iteration 9800 Training loss 0.0010741535807028413 Validation loss 0.04356074705719948 Accuracy 0.8884999752044678\n",
      "Iteration 9810 Training loss 0.0010688112815842032 Validation loss 0.04362070560455322 Accuracy 0.890500009059906\n",
      "Iteration 9820 Training loss 0.0005711849080398679 Validation loss 0.04358263686299324 Accuracy 0.8889999985694885\n",
      "Iteration 9830 Training loss 0.0005813402822241187 Validation loss 0.04356623440980911 Accuracy 0.8899999856948853\n",
      "Iteration 9840 Training loss 0.0003200172504875809 Validation loss 0.04357767105102539 Accuracy 0.8895000219345093\n",
      "Iteration 9850 Training loss 0.0005732849822379649 Validation loss 0.04360059276223183 Accuracy 0.8895000219345093\n",
      "Iteration 9860 Training loss 0.0008197143324650824 Validation loss 0.04360001161694527 Accuracy 0.8884999752044678\n",
      "Iteration 9870 Training loss 0.0010699393460527062 Validation loss 0.043587297201156616 Accuracy 0.8884999752044678\n",
      "Iteration 9880 Training loss 7.730200741207227e-05 Validation loss 0.043540533632040024 Accuracy 0.8889999985694885\n",
      "Iteration 9890 Training loss 0.00032340563484467566 Validation loss 0.04354814812541008 Accuracy 0.8895000219345093\n",
      "Iteration 9900 Training loss 0.0008208680083043873 Validation loss 0.04356875270605087 Accuracy 0.8899999856948853\n",
      "Iteration 9910 Training loss 0.001070881844498217 Validation loss 0.043561555445194244 Accuracy 0.8899999856948853\n",
      "Iteration 9920 Training loss 0.000566718983463943 Validation loss 0.043550848960876465 Accuracy 0.8899999856948853\n",
      "Iteration 9930 Training loss 0.0010684148874133825 Validation loss 0.043575286865234375 Accuracy 0.8895000219345093\n",
      "Iteration 9940 Training loss 0.000820403452962637 Validation loss 0.04354941099882126 Accuracy 0.8884999752044678\n",
      "Iteration 9950 Training loss 0.000812465907074511 Validation loss 0.04359784349799156 Accuracy 0.890500009059906\n",
      "Iteration 9960 Training loss 0.0003242064849473536 Validation loss 0.04358334094285965 Accuracy 0.8895000219345093\n",
      "Iteration 9970 Training loss 0.00107778690289706 Validation loss 0.04358749836683273 Accuracy 0.8895000219345093\n",
      "Iteration 9980 Training loss 0.0018137178849428892 Validation loss 0.04354670271277428 Accuracy 0.890500009059906\n",
      "Iteration 9990 Training loss 0.0008212125976569951 Validation loss 0.043526772409677505 Accuracy 0.8884999752044678\n",
      "Iteration 10000 Training loss 0.0005671193357557058 Validation loss 0.0435945950448513 Accuracy 0.890500009059906\n",
      "Iteration 10010 Training loss 0.0005722735077142715 Validation loss 0.043576229363679886 Accuracy 0.8899999856948853\n",
      "Iteration 10020 Training loss 0.0013223345158621669 Validation loss 0.043536003679037094 Accuracy 0.8895000219345093\n",
      "Iteration 10030 Training loss 0.0013286285102367401 Validation loss 0.043564215302467346 Accuracy 0.8889999985694885\n",
      "Iteration 10040 Training loss 0.0013124736724421382 Validation loss 0.04356524720788002 Accuracy 0.8895000219345093\n",
      "Iteration 10050 Training loss 0.0010706495959311724 Validation loss 0.043578002601861954 Accuracy 0.8899999856948853\n",
      "Iteration 10060 Training loss 0.0005672682309523225 Validation loss 0.043563056737184525 Accuracy 0.8899999856948853\n",
      "Iteration 10070 Training loss 0.0005686677759513259 Validation loss 0.04355547949671745 Accuracy 0.8899999856948853\n",
      "Iteration 10080 Training loss 0.0010690995259210467 Validation loss 0.04355024918913841 Accuracy 0.8899999856948853\n",
      "Iteration 10090 Training loss 0.0003223564999643713 Validation loss 0.043522316962480545 Accuracy 0.8895000219345093\n",
      "Iteration 10100 Training loss 0.0008239439921453595 Validation loss 0.043517038226127625 Accuracy 0.8895000219345093\n",
      "Iteration 10110 Training loss 0.0010755351977422833 Validation loss 0.043522000312805176 Accuracy 0.890500009059906\n",
      "Iteration 10120 Training loss 0.0018223843071609735 Validation loss 0.043541356921195984 Accuracy 0.8899999856948853\n",
      "Iteration 10130 Training loss 7.356460264418274e-05 Validation loss 0.04353547468781471 Accuracy 0.8895000219345093\n",
      "Iteration 10140 Training loss 0.001570073887705803 Validation loss 0.04353199154138565 Accuracy 0.8889999985694885\n",
      "Iteration 10150 Training loss 0.0013200427638366818 Validation loss 0.04354231059551239 Accuracy 0.8889999985694885\n",
      "Iteration 10160 Training loss 0.0008228030055761337 Validation loss 0.04353505000472069 Accuracy 0.8899999856948853\n",
      "Iteration 10170 Training loss 0.0005710672121495008 Validation loss 0.043541498482227325 Accuracy 0.8895000219345093\n",
      "Iteration 10180 Training loss 0.0008241877076216042 Validation loss 0.043536681681871414 Accuracy 0.8899999856948853\n",
      "Iteration 10190 Training loss 0.0018191522685810924 Validation loss 0.04352413862943649 Accuracy 0.8889999985694885\n",
      "Iteration 10200 Training loss 0.0005707584787160158 Validation loss 0.043581523001194 Accuracy 0.890500009059906\n",
      "Iteration 10210 Training loss 0.0013198589440435171 Validation loss 0.043547920882701874 Accuracy 0.890500009059906\n",
      "Iteration 10220 Training loss 0.0018269639695063233 Validation loss 0.04359390586614609 Accuracy 0.8899999856948853\n",
      "Iteration 10230 Training loss 0.0008191725937649608 Validation loss 0.04355362057685852 Accuracy 0.8899999856948853\n",
      "Iteration 10240 Training loss 0.0005783803062513471 Validation loss 0.04352007806301117 Accuracy 0.8899999856948853\n",
      "Iteration 10250 Training loss 0.0013185599818825722 Validation loss 0.04356527701020241 Accuracy 0.8895000219345093\n",
      "Iteration 10260 Training loss 0.000571193580981344 Validation loss 0.04354501888155937 Accuracy 0.8899999856948853\n",
      "Iteration 10270 Training loss 0.0005736438906751573 Validation loss 0.04351545870304108 Accuracy 0.8895000219345093\n",
      "Iteration 10280 Training loss 0.0005693756975233555 Validation loss 0.04355122148990631 Accuracy 0.8899999856948853\n",
      "Iteration 10290 Training loss 0.001073080813512206 Validation loss 0.04354047030210495 Accuracy 0.890500009059906\n",
      "Iteration 10300 Training loss 0.000571257492993027 Validation loss 0.043531909584999084 Accuracy 0.8895000219345093\n",
      "Iteration 10310 Training loss 0.0008169569773599505 Validation loss 0.04356499761343002 Accuracy 0.8899999856948853\n",
      "Iteration 10320 Training loss 0.0013147878926247358 Validation loss 0.043538663536310196 Accuracy 0.8899999856948853\n",
      "Iteration 10330 Training loss 0.0008170570363290608 Validation loss 0.04355669766664505 Accuracy 0.8889999985694885\n",
      "Iteration 10340 Training loss 0.0013209352036938071 Validation loss 0.04357929527759552 Accuracy 0.890500009059906\n",
      "Iteration 10350 Training loss 0.0010723453015089035 Validation loss 0.04355030134320259 Accuracy 0.890500009059906\n",
      "Iteration 10360 Training loss 0.00181763363070786 Validation loss 0.043535586446523666 Accuracy 0.8899999856948853\n",
      "Iteration 10370 Training loss 0.0010679803090170026 Validation loss 0.04354337602853775 Accuracy 0.890500009059906\n",
      "Iteration 10380 Training loss 0.0010671652853488922 Validation loss 0.043534666299819946 Accuracy 0.890500009059906\n",
      "Iteration 10390 Training loss 0.001072796294465661 Validation loss 0.04351537302136421 Accuracy 0.8895000219345093\n",
      "Iteration 10400 Training loss 0.0018186985980719328 Validation loss 0.043544284999370575 Accuracy 0.890500009059906\n",
      "Iteration 10410 Training loss 0.0010717998957261443 Validation loss 0.043518077582120895 Accuracy 0.8899999856948853\n",
      "Iteration 10420 Training loss 0.0005655782879330218 Validation loss 0.043560825288295746 Accuracy 0.8899999856948853\n",
      "Iteration 10430 Training loss 0.0015725902048870921 Validation loss 0.043540604412555695 Accuracy 0.8899999856948853\n",
      "Iteration 10440 Training loss 0.0010641615372151136 Validation loss 0.04355726018548012 Accuracy 0.8899999856948853\n",
      "Iteration 10450 Training loss 0.0013190701138228178 Validation loss 0.043535757809877396 Accuracy 0.8899999856948853\n",
      "Iteration 10460 Training loss 0.0010664358269423246 Validation loss 0.04354359582066536 Accuracy 0.8899999856948853\n",
      "Iteration 10470 Training loss 0.001321212388575077 Validation loss 0.04352852329611778 Accuracy 0.8899999856948853\n",
      "Iteration 10480 Training loss 0.001069521065801382 Validation loss 0.04352737218141556 Accuracy 0.8899999856948853\n",
      "Iteration 10490 Training loss 0.0005716291489079595 Validation loss 0.043535757809877396 Accuracy 0.8889999985694885\n",
      "Iteration 10500 Training loss 0.0008245782810263336 Validation loss 0.04355272278189659 Accuracy 0.8899999856948853\n",
      "Iteration 10510 Training loss 0.002070407848805189 Validation loss 0.04354172199964523 Accuracy 0.8895000219345093\n",
      "Iteration 10520 Training loss 0.0005687993834726512 Validation loss 0.04353221133351326 Accuracy 0.8895000219345093\n",
      "Iteration 10530 Training loss 0.0008230741368606687 Validation loss 0.0435447171330452 Accuracy 0.8895000219345093\n",
      "Iteration 10540 Training loss 0.0005706144729629159 Validation loss 0.04353225231170654 Accuracy 0.8895000219345093\n",
      "Iteration 10550 Training loss 0.0008153641247190535 Validation loss 0.04354528710246086 Accuracy 0.8895000219345093\n",
      "Iteration 10560 Training loss 0.0013184233102947474 Validation loss 0.04355146363377571 Accuracy 0.8899999856948853\n",
      "Iteration 10570 Training loss 0.0008222303004004061 Validation loss 0.04350589960813522 Accuracy 0.8889999985694885\n",
      "Iteration 10580 Training loss 0.0005714475410059094 Validation loss 0.04353425279259682 Accuracy 0.890500009059906\n",
      "Iteration 10590 Training loss 0.0005703033530153334 Validation loss 0.04353068396449089 Accuracy 0.8899999856948853\n",
      "Iteration 10600 Training loss 0.0008183809113688767 Validation loss 0.0435197614133358 Accuracy 0.890500009059906\n",
      "Iteration 10610 Training loss 0.0010684991721063852 Validation loss 0.04355182871222496 Accuracy 0.8899999856948853\n",
      "Iteration 10620 Training loss 0.0013109283754602075 Validation loss 0.04352898150682449 Accuracy 0.8895000219345093\n",
      "Iteration 10630 Training loss 0.0013228373136371374 Validation loss 0.04351859539747238 Accuracy 0.8895000219345093\n",
      "Iteration 10640 Training loss 7.115595508366823e-05 Validation loss 0.043516043573617935 Accuracy 0.890500009059906\n",
      "Iteration 10650 Training loss 0.0008264307980425656 Validation loss 0.04353083670139313 Accuracy 0.8899999856948853\n",
      "Iteration 10660 Training loss 0.000817622640170157 Validation loss 0.043502870947122574 Accuracy 0.8899999856948853\n",
      "Iteration 10670 Training loss 0.0013228317257016897 Validation loss 0.04352501034736633 Accuracy 0.8895000219345093\n",
      "Iteration 10680 Training loss 0.0008132977527566254 Validation loss 0.043557144701480865 Accuracy 0.8899999856948853\n",
      "Iteration 10690 Training loss 0.0010679216356948018 Validation loss 0.04353881999850273 Accuracy 0.8895000219345093\n",
      "Iteration 10700 Training loss 0.0005710002733394504 Validation loss 0.04353730008006096 Accuracy 0.8899999856948853\n",
      "Iteration 10710 Training loss 0.0005652614636346698 Validation loss 0.04354280233383179 Accuracy 0.890500009059906\n",
      "Iteration 10720 Training loss 0.00106804131064564 Validation loss 0.04353810101747513 Accuracy 0.890500009059906\n",
      "Iteration 10730 Training loss 0.0005739061743952334 Validation loss 0.04352143034338951 Accuracy 0.8895000219345093\n",
      "Iteration 10740 Training loss 0.0005682023474946618 Validation loss 0.043500740081071854 Accuracy 0.8895000219345093\n",
      "Iteration 10750 Training loss 0.000819236331153661 Validation loss 0.043522413820028305 Accuracy 0.8895000219345093\n",
      "Iteration 10760 Training loss 0.0008224429911933839 Validation loss 0.043501853942871094 Accuracy 0.8895000219345093\n",
      "Iteration 10770 Training loss 0.0008207341888919473 Validation loss 0.04349883645772934 Accuracy 0.890500009059906\n",
      "Iteration 10780 Training loss 0.000569277792237699 Validation loss 0.04354804381728172 Accuracy 0.8889999985694885\n",
      "Iteration 10790 Training loss 7.109415309969336e-05 Validation loss 0.043557099997997284 Accuracy 0.8895000219345093\n",
      "Iteration 10800 Training loss 0.0005716996965929866 Validation loss 0.04353368282318115 Accuracy 0.8895000219345093\n",
      "Iteration 10810 Training loss 0.0005660836468450725 Validation loss 0.04349329695105553 Accuracy 0.8899999856948853\n",
      "Iteration 10820 Training loss 0.0010731155052781105 Validation loss 0.043503742665052414 Accuracy 0.8899999856948853\n",
      "Iteration 10830 Training loss 0.0013186178402975202 Validation loss 0.04351246729493141 Accuracy 0.8899999856948853\n",
      "Iteration 10840 Training loss 0.0008238813607022166 Validation loss 0.04350404068827629 Accuracy 0.8899999856948853\n",
      "Iteration 10850 Training loss 0.0008218234870582819 Validation loss 0.04349120333790779 Accuracy 0.8899999856948853\n",
      "Iteration 10860 Training loss 0.001313312561251223 Validation loss 0.04351247474551201 Accuracy 0.890500009059906\n",
      "Iteration 10870 Training loss 0.000820531917270273 Validation loss 0.043504904955625534 Accuracy 0.8899999856948853\n",
      "Iteration 10880 Training loss 0.0008136546239256859 Validation loss 0.043499454855918884 Accuracy 0.8895000219345093\n",
      "Iteration 10890 Training loss 0.0010691364295780659 Validation loss 0.04354197531938553 Accuracy 0.8895000219345093\n",
      "Iteration 10900 Training loss 0.001311760046519339 Validation loss 0.04356494918465614 Accuracy 0.8899999856948853\n",
      "Iteration 10910 Training loss 0.000315507291816175 Validation loss 0.043521806597709656 Accuracy 0.890500009059906\n",
      "Iteration 10920 Training loss 0.0010719053680077195 Validation loss 0.043507836759090424 Accuracy 0.8899999856948853\n",
      "Iteration 10930 Training loss 0.000567435403354466 Validation loss 0.04346258193254471 Accuracy 0.8899999856948853\n",
      "Iteration 10940 Training loss 0.0013171643950045109 Validation loss 0.04352089390158653 Accuracy 0.8899999856948853\n",
      "Iteration 10950 Training loss 0.0008158338023349643 Validation loss 0.04354028403759003 Accuracy 0.890500009059906\n",
      "Iteration 10960 Training loss 0.0010672430507838726 Validation loss 0.04348938539624214 Accuracy 0.8889999985694885\n",
      "Iteration 10970 Training loss 0.001317733433097601 Validation loss 0.04348474740982056 Accuracy 0.8899999856948853\n",
      "Iteration 10980 Training loss 0.00081537856021896 Validation loss 0.04350099712610245 Accuracy 0.8899999856948853\n",
      "Iteration 10990 Training loss 6.692083115922287e-05 Validation loss 0.043500956147909164 Accuracy 0.890500009059906\n",
      "Iteration 11000 Training loss 0.0013175037456676364 Validation loss 0.04350806027650833 Accuracy 0.8899999856948853\n",
      "Iteration 11010 Training loss 0.000819026492536068 Validation loss 0.043534666299819946 Accuracy 0.8895000219345093\n",
      "Iteration 11020 Training loss 0.0013210643082857132 Validation loss 0.04352065175771713 Accuracy 0.8899999856948853\n",
      "Iteration 11030 Training loss 0.0010668301256373525 Validation loss 0.043475985527038574 Accuracy 0.8889999985694885\n",
      "Iteration 11040 Training loss 0.0010753278620541096 Validation loss 0.043494559824466705 Accuracy 0.890500009059906\n",
      "Iteration 11050 Training loss 0.0005688663222827017 Validation loss 0.04348386824131012 Accuracy 0.8899999856948853\n",
      "Iteration 11060 Training loss 0.0008196858107112348 Validation loss 0.043470416218042374 Accuracy 0.8889999985694885\n",
      "Iteration 11070 Training loss 0.00031483222846873105 Validation loss 0.04348522052168846 Accuracy 0.8899999856948853\n",
      "Iteration 11080 Training loss 0.0008203068864531815 Validation loss 0.04351187124848366 Accuracy 0.890500009059906\n",
      "Iteration 11090 Training loss 0.0008155794930644333 Validation loss 0.04348575323820114 Accuracy 0.890500009059906\n",
      "Iteration 11100 Training loss 0.0008199320291168988 Validation loss 0.04349817708134651 Accuracy 0.8895000219345093\n",
      "Iteration 11110 Training loss 0.0008179242722690105 Validation loss 0.043536797165870667 Accuracy 0.8899999856948853\n",
      "Iteration 11120 Training loss 0.0005712733836844563 Validation loss 0.04349454492330551 Accuracy 0.8899999856948853\n",
      "Iteration 11130 Training loss 0.0010765084298327565 Validation loss 0.04351355880498886 Accuracy 0.8895000219345093\n",
      "Iteration 11140 Training loss 0.0013173448387533426 Validation loss 0.04348602890968323 Accuracy 0.8899999856948853\n",
      "Iteration 11150 Training loss 0.00031473988201469183 Validation loss 0.04348349943757057 Accuracy 0.8899999856948853\n",
      "Iteration 11160 Training loss 0.0015684905229136348 Validation loss 0.043501321226358414 Accuracy 0.890500009059906\n",
      "Iteration 11170 Training loss 0.00031828274950385094 Validation loss 0.04349388927221298 Accuracy 0.8899999856948853\n",
      "Iteration 11180 Training loss 0.0013130707666277885 Validation loss 0.043511372059583664 Accuracy 0.890500009059906\n",
      "Iteration 11190 Training loss 0.0005692096310667694 Validation loss 0.04347918555140495 Accuracy 0.8895000219345093\n",
      "Iteration 11200 Training loss 0.0008166405605152249 Validation loss 0.04345456510782242 Accuracy 0.8899999856948853\n",
      "Iteration 11210 Training loss 0.0010699947597458959 Validation loss 0.04349407181143761 Accuracy 0.8899999856948853\n",
      "Iteration 11220 Training loss 0.0010713980300351977 Validation loss 0.04350774735212326 Accuracy 0.8895000219345093\n",
      "Iteration 11230 Training loss 0.0013161913957446814 Validation loss 0.0435124896466732 Accuracy 0.8899999856948853\n",
      "Iteration 11240 Training loss 0.000824489863589406 Validation loss 0.043520234525203705 Accuracy 0.8899999856948853\n",
      "Iteration 11250 Training loss 0.0010676572564989328 Validation loss 0.04349083825945854 Accuracy 0.8899999856948853\n",
      "Iteration 11260 Training loss 0.0010739616118371487 Validation loss 0.04349437728524208 Accuracy 0.8899999856948853\n",
      "Iteration 11270 Training loss 0.0008198123541660607 Validation loss 0.043487273156642914 Accuracy 0.8895000219345093\n",
      "Iteration 11280 Training loss 0.0008202675962820649 Validation loss 0.04353148490190506 Accuracy 0.8899999856948853\n",
      "Iteration 11290 Training loss 0.001064860261976719 Validation loss 0.04349016398191452 Accuracy 0.8899999856948853\n",
      "Iteration 11300 Training loss 0.0008231595857068896 Validation loss 0.04345747455954552 Accuracy 0.8899999856948853\n",
      "Iteration 11310 Training loss 6.794249929953367e-05 Validation loss 0.04345586523413658 Accuracy 0.8899999856948853\n",
      "Iteration 11320 Training loss 0.00056363147450611 Validation loss 0.04348965361714363 Accuracy 0.8899999856948853\n",
      "Iteration 11330 Training loss 0.0008166799671016634 Validation loss 0.04346023127436638 Accuracy 0.8895000219345093\n",
      "Iteration 11340 Training loss 0.0005658700247295201 Validation loss 0.043481919914484024 Accuracy 0.8899999856948853\n",
      "Iteration 11350 Training loss 0.0008169836946763098 Validation loss 0.043494321405887604 Accuracy 0.8899999856948853\n",
      "Iteration 11360 Training loss 0.0008129595662467182 Validation loss 0.0435170941054821 Accuracy 0.8895000219345093\n",
      "Iteration 11370 Training loss 0.0005675375577993691 Validation loss 0.043504782021045685 Accuracy 0.8895000219345093\n",
      "Iteration 11380 Training loss 0.0005680567119270563 Validation loss 0.04350339621305466 Accuracy 0.8899999856948853\n",
      "Iteration 11390 Training loss 0.0005664654891006649 Validation loss 0.04350430145859718 Accuracy 0.890500009059906\n",
      "Iteration 11400 Training loss 0.0010670062620192766 Validation loss 0.04349050670862198 Accuracy 0.890500009059906\n",
      "Iteration 11410 Training loss 0.0010736968833953142 Validation loss 0.04350350797176361 Accuracy 0.8899999856948853\n",
      "Iteration 11420 Training loss 0.0008158053969964385 Validation loss 0.04349439963698387 Accuracy 0.890500009059906\n",
      "Iteration 11430 Training loss 0.0005596663104370236 Validation loss 0.04349397122859955 Accuracy 0.8899999856948853\n",
      "Iteration 11440 Training loss 7.183684647316113e-05 Validation loss 0.04345592111349106 Accuracy 0.8899999856948853\n",
      "Iteration 11450 Training loss 0.0008194869733415544 Validation loss 0.04343978315591812 Accuracy 0.8899999856948853\n",
      "Iteration 11460 Training loss 0.0005768623086623847 Validation loss 0.04349719360470772 Accuracy 0.8899999856948853\n",
      "Iteration 11470 Training loss 0.0005693570710718632 Validation loss 0.04348275810480118 Accuracy 0.890500009059906\n",
      "Iteration 11480 Training loss 0.0008244583150371909 Validation loss 0.04346318915486336 Accuracy 0.890500009059906\n",
      "Iteration 11490 Training loss 0.0008195691043511033 Validation loss 0.04343061149120331 Accuracy 0.8899999856948853\n",
      "Iteration 11500 Training loss 0.0005631909007206559 Validation loss 0.04349309206008911 Accuracy 0.890500009059906\n",
      "Iteration 11510 Training loss 0.0008166939951479435 Validation loss 0.04344410076737404 Accuracy 0.8899999856948853\n",
      "Iteration 11520 Training loss 0.00031580543145537376 Validation loss 0.04348469525575638 Accuracy 0.8899999856948853\n",
      "Iteration 11530 Training loss 0.0008213870460167527 Validation loss 0.043470609933137894 Accuracy 0.890500009059906\n",
      "Iteration 11540 Training loss 0.0005665890639647841 Validation loss 0.04347222298383713 Accuracy 0.890500009059906\n",
      "Iteration 11550 Training loss 0.0010691600618883967 Validation loss 0.043508730828762054 Accuracy 0.890500009059906\n",
      "Iteration 11560 Training loss 0.0005714715807698667 Validation loss 0.04349746182560921 Accuracy 0.890500009059906\n",
      "Iteration 11570 Training loss 0.0010682133724913 Validation loss 0.04345288500189781 Accuracy 0.8899999856948853\n",
      "Iteration 11580 Training loss 0.0005753946024924517 Validation loss 0.043477144092321396 Accuracy 0.8899999856948853\n",
      "Iteration 11590 Training loss 0.0005710355471819639 Validation loss 0.04353716969490051 Accuracy 0.890500009059906\n",
      "Iteration 11600 Training loss 0.0005759227788075805 Validation loss 0.043483056128025055 Accuracy 0.8899999856948853\n",
      "Iteration 11610 Training loss 0.0010688739130273461 Validation loss 0.04348379373550415 Accuracy 0.890500009059906\n",
      "Iteration 11620 Training loss 0.0018218752229586244 Validation loss 0.04345931112766266 Accuracy 0.8899999856948853\n",
      "Iteration 11630 Training loss 0.0008117958204820752 Validation loss 0.04347502067685127 Accuracy 0.8899999856948853\n",
      "Iteration 11640 Training loss 0.0013216607039794326 Validation loss 0.04347562417387962 Accuracy 0.8899999856948853\n",
      "Iteration 11650 Training loss 0.0005707662203349173 Validation loss 0.043502163141965866 Accuracy 0.890500009059906\n",
      "Iteration 11660 Training loss 0.0008171336958184838 Validation loss 0.04347734898328781 Accuracy 0.8899999856948853\n",
      "Iteration 11670 Training loss 0.0010706221219152212 Validation loss 0.04350419342517853 Accuracy 0.890500009059906\n",
      "Iteration 11680 Training loss 0.0010675686644390225 Validation loss 0.04342611879110336 Accuracy 0.890500009059906\n",
      "Iteration 11690 Training loss 0.0008161155856214464 Validation loss 0.04352208599448204 Accuracy 0.890500009059906\n",
      "Iteration 11700 Training loss 0.00031930161640048027 Validation loss 0.043448179960250854 Accuracy 0.8899999856948853\n",
      "Iteration 11710 Training loss 0.001575832604430616 Validation loss 0.04344306141138077 Accuracy 0.8899999856948853\n",
      "Iteration 11720 Training loss 0.0008222289616242051 Validation loss 0.043482452630996704 Accuracy 0.890500009059906\n",
      "Iteration 11730 Training loss 0.0013184156268835068 Validation loss 0.043452441692352295 Accuracy 0.8899999856948853\n",
      "Iteration 11740 Training loss 6.681890226900578e-05 Validation loss 0.043477874249219894 Accuracy 0.890500009059906\n",
      "Iteration 11750 Training loss 0.0015640008496120572 Validation loss 0.04350186884403229 Accuracy 0.890500009059906\n",
      "Iteration 11760 Training loss 6.879514694446698e-05 Validation loss 0.04347938671708107 Accuracy 0.890500009059906\n",
      "Iteration 11770 Training loss 0.0008185210754163563 Validation loss 0.04343385621905327 Accuracy 0.8899999856948853\n",
      "Iteration 11780 Training loss 0.0010737827979028225 Validation loss 0.04345935210585594 Accuracy 0.8899999856948853\n",
      "Iteration 11790 Training loss 0.0008213237742893398 Validation loss 0.043512843549251556 Accuracy 0.8899999856948853\n",
      "Iteration 11800 Training loss 0.0005722918431274593 Validation loss 0.04345729202032089 Accuracy 0.8899999856948853\n",
      "Iteration 11810 Training loss 0.0010718089761212468 Validation loss 0.043512988835573196 Accuracy 0.8899999856948853\n",
      "Iteration 11820 Training loss 0.0010701828869059682 Validation loss 0.04349225014448166 Accuracy 0.8899999856948853\n",
      "Iteration 11830 Training loss 0.0005679003661498427 Validation loss 0.04348252713680267 Accuracy 0.890500009059906\n",
      "Iteration 11840 Training loss 0.0003189668932463974 Validation loss 0.04347676783800125 Accuracy 0.890500009059906\n",
      "Iteration 11850 Training loss 0.0015729686710983515 Validation loss 0.04351702332496643 Accuracy 0.890500009059906\n",
      "Iteration 11860 Training loss 0.0013201450929045677 Validation loss 0.04350889474153519 Accuracy 0.890999972820282\n",
      "Iteration 11870 Training loss 0.0013166165444999933 Validation loss 0.043497130274772644 Accuracy 0.890500009059906\n",
      "Iteration 11880 Training loss 0.0008172782836481929 Validation loss 0.0434424951672554 Accuracy 0.890500009059906\n",
      "Iteration 11890 Training loss 0.00131582363974303 Validation loss 0.043509989976882935 Accuracy 0.8899999856948853\n",
      "Iteration 11900 Training loss 0.0005754225421696901 Validation loss 0.043470267206430435 Accuracy 0.8899999856948853\n",
      "Iteration 11910 Training loss 0.00031972318538464606 Validation loss 0.04347597435116768 Accuracy 0.890500009059906\n",
      "Iteration 11920 Training loss 0.00131926778703928 Validation loss 0.0434616394340992 Accuracy 0.890500009059906\n",
      "Iteration 11930 Training loss 0.0008165200124494731 Validation loss 0.04346851259469986 Accuracy 0.8899999856948853\n",
      "Iteration 11940 Training loss 0.001315163099206984 Validation loss 0.04344604164361954 Accuracy 0.890500009059906\n",
      "Iteration 11950 Training loss 0.0010640305699780583 Validation loss 0.043459028005599976 Accuracy 0.8899999856948853\n",
      "Iteration 11960 Training loss 0.00032340676989406347 Validation loss 0.043445274233818054 Accuracy 0.890500009059906\n",
      "Iteration 11970 Training loss 0.0003230000438634306 Validation loss 0.04349791631102562 Accuracy 0.890999972820282\n",
      "Iteration 11980 Training loss 0.0003190056886523962 Validation loss 0.04345906898379326 Accuracy 0.890999972820282\n",
      "Iteration 11990 Training loss 0.001568294013850391 Validation loss 0.04347715899348259 Accuracy 0.890999972820282\n",
      "Iteration 12000 Training loss 0.0010631916811689734 Validation loss 0.043458789587020874 Accuracy 0.8899999856948853\n",
      "Iteration 12010 Training loss 0.0005650880048051476 Validation loss 0.04348742589354515 Accuracy 0.890500009059906\n",
      "Iteration 12020 Training loss 0.00032830756390467286 Validation loss 0.0434248186647892 Accuracy 0.8899999856948853\n",
      "Iteration 12030 Training loss 0.0010719664860516787 Validation loss 0.04344967007637024 Accuracy 0.890999972820282\n",
      "Iteration 12040 Training loss 0.0008139625424519181 Validation loss 0.04345192387700081 Accuracy 0.8899999856948853\n",
      "Iteration 12050 Training loss 0.0013181061949580908 Validation loss 0.043461188673973083 Accuracy 0.890500009059906\n",
      "Iteration 12060 Training loss 0.0015649470733478665 Validation loss 0.04346635192632675 Accuracy 0.890500009059906\n",
      "Iteration 12070 Training loss 0.001315563335083425 Validation loss 0.043434347957372665 Accuracy 0.890500009059906\n",
      "Iteration 12080 Training loss 0.0010570319136604667 Validation loss 0.04346764460206032 Accuracy 0.890999972820282\n",
      "Iteration 12090 Training loss 0.001065201940946281 Validation loss 0.043419141322374344 Accuracy 0.890999972820282\n",
      "Iteration 12100 Training loss 0.0010659446706995368 Validation loss 0.043439652770757675 Accuracy 0.890999972820282\n",
      "Iteration 12110 Training loss 0.000815942301414907 Validation loss 0.04345724731683731 Accuracy 0.890999972820282\n",
      "Iteration 12120 Training loss 0.0013210657052695751 Validation loss 0.043450698256492615 Accuracy 0.890500009059906\n",
      "Iteration 12130 Training loss 0.000816565880086273 Validation loss 0.04351150244474411 Accuracy 0.890500009059906\n",
      "Iteration 12140 Training loss 0.0008187081548385322 Validation loss 0.04349251091480255 Accuracy 0.890500009059906\n",
      "Iteration 12150 Training loss 0.0013183386763557792 Validation loss 0.043475572019815445 Accuracy 0.890500009059906\n",
      "Iteration 12160 Training loss 0.001313169370405376 Validation loss 0.043455820530653 Accuracy 0.890500009059906\n",
      "Iteration 12170 Training loss 0.000567787152249366 Validation loss 0.04346784949302673 Accuracy 0.890500009059906\n",
      "Iteration 12180 Training loss 0.0010676541132852435 Validation loss 0.04347139596939087 Accuracy 0.890999972820282\n",
      "Iteration 12190 Training loss 0.0008220614399760962 Validation loss 0.04349043220281601 Accuracy 0.890999972820282\n",
      "Iteration 12200 Training loss 0.0013148211874067783 Validation loss 0.04341423884034157 Accuracy 0.890999972820282\n",
      "Iteration 12210 Training loss 0.00231886631809175 Validation loss 0.04344293847680092 Accuracy 0.890999972820282\n",
      "Iteration 12220 Training loss 0.0008171603549271822 Validation loss 0.043447695672512054 Accuracy 0.890999972820282\n",
      "Iteration 12230 Training loss 0.0005718618631362915 Validation loss 0.04346742108464241 Accuracy 0.890500009059906\n",
      "Iteration 12240 Training loss 0.0003220640937797725 Validation loss 0.04344914108514786 Accuracy 0.890500009059906\n",
      "Iteration 12250 Training loss 0.0008178813150152564 Validation loss 0.04343739151954651 Accuracy 0.890999972820282\n",
      "Iteration 12260 Training loss 0.0010675921803340316 Validation loss 0.04343985766172409 Accuracy 0.890999972820282\n",
      "Iteration 12270 Training loss 0.0008170055807568133 Validation loss 0.0434730239212513 Accuracy 0.8914999961853027\n",
      "Iteration 12280 Training loss 0.001318878261372447 Validation loss 0.04347541928291321 Accuracy 0.890999972820282\n",
      "Iteration 12290 Training loss 0.001070090918801725 Validation loss 0.04346819221973419 Accuracy 0.890999972820282\n",
      "Iteration 12300 Training loss 0.00107255345210433 Validation loss 0.04346412047743797 Accuracy 0.890999972820282\n",
      "Iteration 12310 Training loss 0.0008200241136364639 Validation loss 0.04346861690282822 Accuracy 0.8914999961853027\n",
      "Iteration 12320 Training loss 0.0010671825148165226 Validation loss 0.04348147660493851 Accuracy 0.890999972820282\n",
      "Iteration 12330 Training loss 0.0008170721703208983 Validation loss 0.043429840356111526 Accuracy 0.890999972820282\n",
      "Iteration 12340 Training loss 7.064593228278682e-05 Validation loss 0.04343166947364807 Accuracy 0.890999972820282\n",
      "Iteration 12350 Training loss 0.0003165137313771993 Validation loss 0.04345303401350975 Accuracy 0.8914999961853027\n",
      "Iteration 12360 Training loss 0.0008193451794795692 Validation loss 0.043444305658340454 Accuracy 0.890999972820282\n",
      "Iteration 12370 Training loss 0.00032351361005567014 Validation loss 0.04347137361764908 Accuracy 0.890999972820282\n",
      "Iteration 12380 Training loss 0.0005627991049550474 Validation loss 0.043467115610837936 Accuracy 0.890999972820282\n",
      "Iteration 12390 Training loss 0.0005732224672101438 Validation loss 0.04343605786561966 Accuracy 0.890999972820282\n",
      "Iteration 12400 Training loss 0.0008231655228883028 Validation loss 0.043408095836639404 Accuracy 0.890999972820282\n",
      "Iteration 12410 Training loss 0.0013128697173669934 Validation loss 0.04344679042696953 Accuracy 0.890999972820282\n",
      "Iteration 12420 Training loss 0.001319022849202156 Validation loss 0.043414048850536346 Accuracy 0.890999972820282\n",
      "Iteration 12430 Training loss 0.0010690633207559586 Validation loss 0.043426744639873505 Accuracy 0.890999972820282\n",
      "Iteration 12440 Training loss 0.0013180217938497663 Validation loss 0.04345321282744408 Accuracy 0.890500009059906\n",
      "Iteration 12450 Training loss 0.0008164793252944946 Validation loss 0.043418243527412415 Accuracy 0.890999972820282\n",
      "Iteration 12460 Training loss 0.0008160530705936253 Validation loss 0.0433998703956604 Accuracy 0.8914999961853027\n",
      "Iteration 12470 Training loss 0.0005665568169206381 Validation loss 0.043415218591690063 Accuracy 0.8914999961853027\n",
      "Iteration 12480 Training loss 0.00031403981847688556 Validation loss 0.043410271406173706 Accuracy 0.8914999961853027\n",
      "Iteration 12490 Training loss 0.0005696389707736671 Validation loss 0.043445076793432236 Accuracy 0.890999972820282\n",
      "Iteration 12500 Training loss 0.0010695148957893252 Validation loss 0.04341086000204086 Accuracy 0.890999972820282\n",
      "Iteration 12510 Training loss 0.001064951065927744 Validation loss 0.043430741876363754 Accuracy 0.890500009059906\n",
      "Iteration 12520 Training loss 0.00131654751021415 Validation loss 0.04340479150414467 Accuracy 0.890500009059906\n",
      "Iteration 12530 Training loss 0.0003194004821125418 Validation loss 0.04344446584582329 Accuracy 0.890500009059906\n",
      "Iteration 12540 Training loss 0.0008132473449222744 Validation loss 0.043409232050180435 Accuracy 0.890999972820282\n",
      "Iteration 12550 Training loss 0.001065933727659285 Validation loss 0.04339655488729477 Accuracy 0.890500009059906\n",
      "Iteration 12560 Training loss 0.0010658741230145097 Validation loss 0.043431393802165985 Accuracy 0.890500009059906\n",
      "Iteration 12570 Training loss 0.0008121702121570706 Validation loss 0.04345246031880379 Accuracy 0.8914999961853027\n",
      "Iteration 12580 Training loss 0.0005657652509398758 Validation loss 0.04341806471347809 Accuracy 0.8914999961853027\n",
      "Iteration 12590 Training loss 0.001069107442162931 Validation loss 0.04345018044114113 Accuracy 0.890999972820282\n",
      "Iteration 12600 Training loss 0.0010703334119170904 Validation loss 0.04345923289656639 Accuracy 0.890500009059906\n",
      "Iteration 12610 Training loss 0.0008171601220965385 Validation loss 0.04344767704606056 Accuracy 0.890999972820282\n",
      "Iteration 12620 Training loss 0.0013097207993268967 Validation loss 0.04339131340384483 Accuracy 0.8914999961853027\n",
      "Iteration 12630 Training loss 0.000318622391205281 Validation loss 0.04343075677752495 Accuracy 0.8914999961853027\n",
      "Iteration 12640 Training loss 0.0005702490452677011 Validation loss 0.04343781620264053 Accuracy 0.8914999961853027\n",
      "Iteration 12650 Training loss 0.0008174523245543242 Validation loss 0.04344312101602554 Accuracy 0.890999972820282\n",
      "Iteration 12660 Training loss 0.0005751390708610415 Validation loss 0.043461017310619354 Accuracy 0.890500009059906\n",
      "Iteration 12670 Training loss 0.0010660015977919102 Validation loss 0.04345797374844551 Accuracy 0.890999972820282\n",
      "Iteration 12680 Training loss 0.0018161487532779574 Validation loss 0.043464239686727524 Accuracy 0.890999972820282\n",
      "Iteration 12690 Training loss 0.0008099001715891063 Validation loss 0.04341745004057884 Accuracy 0.890999972820282\n",
      "Iteration 12700 Training loss 0.002070280024781823 Validation loss 0.043419722467660904 Accuracy 0.8914999961853027\n",
      "Iteration 12710 Training loss 6.645168468821794e-05 Validation loss 0.04347110167145729 Accuracy 0.8899999856948853\n",
      "Iteration 12720 Training loss 0.001062480267137289 Validation loss 0.0434105210006237 Accuracy 0.8914999961853027\n",
      "Iteration 12730 Training loss 0.0010693982476368546 Validation loss 0.04337913170456886 Accuracy 0.890999972820282\n",
      "Iteration 12740 Training loss 0.0013209335738793015 Validation loss 0.043414726853370667 Accuracy 0.890999972820282\n",
      "Iteration 12750 Training loss 0.0015676012262701988 Validation loss 0.043425314128398895 Accuracy 0.8914999961853027\n",
      "Iteration 12760 Training loss 0.0013173625338822603 Validation loss 0.04345831274986267 Accuracy 0.890500009059906\n",
      "Iteration 12770 Training loss 0.0005709584220312536 Validation loss 0.04337843507528305 Accuracy 0.890500009059906\n",
      "Iteration 12780 Training loss 0.0015702557284384966 Validation loss 0.04343102127313614 Accuracy 0.890999972820282\n",
      "Iteration 12790 Training loss 0.0013174409978091717 Validation loss 0.04341782629489899 Accuracy 0.8914999961853027\n",
      "Iteration 12800 Training loss 0.0013153727632015944 Validation loss 0.043434664607048035 Accuracy 0.890999972820282\n",
      "Iteration 12810 Training loss 0.000817683176137507 Validation loss 0.043388862162828445 Accuracy 0.890999972820282\n",
      "Iteration 12820 Training loss 0.0010657215025275946 Validation loss 0.04344450682401657 Accuracy 0.890999972820282\n",
      "Iteration 12830 Training loss 0.001318794209510088 Validation loss 0.043419770896434784 Accuracy 0.8914999961853027\n",
      "Iteration 12840 Training loss 0.001312981708906591 Validation loss 0.04345104098320007 Accuracy 0.890999972820282\n",
      "Iteration 12850 Training loss 0.0005644769407808781 Validation loss 0.04346318170428276 Accuracy 0.8914999961853027\n",
      "Iteration 12860 Training loss 0.0010707201436161995 Validation loss 0.04344894737005234 Accuracy 0.8914999961853027\n",
      "Iteration 12870 Training loss 0.0013187562581151724 Validation loss 0.04341864958405495 Accuracy 0.8914999961853027\n",
      "Iteration 12880 Training loss 0.0005692082922905684 Validation loss 0.04340742900967598 Accuracy 0.8914999961853027\n",
      "Iteration 12890 Training loss 0.0013200950343161821 Validation loss 0.04343980550765991 Accuracy 0.890999972820282\n",
      "Iteration 12900 Training loss 0.0005708697717636824 Validation loss 0.043430108577013016 Accuracy 0.8920000195503235\n",
      "Iteration 12910 Training loss 0.00056478037731722 Validation loss 0.043434448540210724 Accuracy 0.890999972820282\n",
      "Iteration 12920 Training loss 0.0008122253930196166 Validation loss 0.0434085950255394 Accuracy 0.8920000195503235\n",
      "Iteration 12930 Training loss 0.0015665431274101138 Validation loss 0.043417587876319885 Accuracy 0.8914999961853027\n",
      "Iteration 12940 Training loss 0.0008192428504116833 Validation loss 0.043429117649793625 Accuracy 0.890500009059906\n",
      "Iteration 12950 Training loss 0.001067109638825059 Validation loss 0.04340790957212448 Accuracy 0.890500009059906\n",
      "Iteration 12960 Training loss 0.0010728235356509686 Validation loss 0.043446533381938934 Accuracy 0.890999972820282\n",
      "Iteration 12970 Training loss 0.0013193710474297404 Validation loss 0.043418999761343 Accuracy 0.890500009059906\n",
      "Iteration 12980 Training loss 0.00031469567329622805 Validation loss 0.043402452021837234 Accuracy 0.890999972820282\n",
      "Iteration 12990 Training loss 0.001060118549503386 Validation loss 0.043448179960250854 Accuracy 0.8914999961853027\n",
      "Iteration 13000 Training loss 0.001059145899489522 Validation loss 0.04338786005973816 Accuracy 0.8899999856948853\n",
      "Iteration 13010 Training loss 0.000818729808088392 Validation loss 0.043364111334085464 Accuracy 0.890500009059906\n",
      "Iteration 13020 Training loss 0.001306300750002265 Validation loss 0.04342091828584671 Accuracy 0.890500009059906\n",
      "Iteration 13030 Training loss 0.0008026484283618629 Validation loss 0.04337586089968681 Accuracy 0.8899999856948853\n",
      "Iteration 13040 Training loss 0.0008021958055905998 Validation loss 0.04341982305049896 Accuracy 0.8895000219345093\n",
      "Iteration 13050 Training loss 0.00204539205878973 Validation loss 0.04342697560787201 Accuracy 0.8899999856948853\n",
      "Iteration 13060 Training loss 0.0013167866272851825 Validation loss 0.04345123842358589 Accuracy 0.8895000219345093\n",
      "Iteration 13070 Training loss 0.0008202700410038233 Validation loss 0.043358538299798965 Accuracy 0.890999972820282\n",
      "Iteration 13080 Training loss 0.001326673780567944 Validation loss 0.04336995258927345 Accuracy 0.8899999856948853\n",
      "Iteration 13090 Training loss 0.0020664632320404053 Validation loss 0.04347607493400574 Accuracy 0.8899999856948853\n",
      "Iteration 13100 Training loss 0.0013127615675330162 Validation loss 0.043460775166749954 Accuracy 0.8899999856948853\n",
      "Iteration 13110 Training loss 0.00082386628491804 Validation loss 0.04341643676161766 Accuracy 0.890500009059906\n",
      "Iteration 13120 Training loss 0.000831022800412029 Validation loss 0.0434793084859848 Accuracy 0.8884999752044678\n",
      "Iteration 13130 Training loss 0.000829924363642931 Validation loss 0.043442778289318085 Accuracy 0.8889999985694885\n",
      "Iteration 13140 Training loss 0.0008290050900541246 Validation loss 0.04354013130068779 Accuracy 0.887499988079071\n",
      "Iteration 13150 Training loss 0.0005689962999895215 Validation loss 0.04349098354578018 Accuracy 0.8880000114440918\n",
      "Iteration 13160 Training loss 0.00014546545571647584 Validation loss 0.04337761551141739 Accuracy 0.8895000219345093\n",
      "Iteration 13170 Training loss 0.0008643374312669039 Validation loss 0.043387047946453094 Accuracy 0.8884999752044678\n",
      "Iteration 13180 Training loss 0.0005731142009608448 Validation loss 0.04344544932246208 Accuracy 0.8899999856948853\n",
      "Iteration 13190 Training loss 0.001079122768715024 Validation loss 0.04345192387700081 Accuracy 0.890500009059906\n",
      "Iteration 13200 Training loss 0.000832381600048393 Validation loss 0.04350341111421585 Accuracy 0.8880000114440918\n",
      "Iteration 13210 Training loss 0.0008280125912278891 Validation loss 0.04358847811818123 Accuracy 0.8880000114440918\n",
      "Iteration 13220 Training loss 0.0013522860826924443 Validation loss 0.043610453605651855 Accuracy 0.8880000114440918\n",
      "Iteration 13230 Training loss 0.0008418921497650445 Validation loss 0.04356402903795242 Accuracy 0.8889999985694885\n",
      "Iteration 13240 Training loss 0.00058497249847278 Validation loss 0.043643467128276825 Accuracy 0.8880000114440918\n",
      "Iteration 13250 Training loss 0.000826587260235101 Validation loss 0.04345684498548508 Accuracy 0.8895000219345093\n",
      "Iteration 13260 Training loss 0.0005816188058815897 Validation loss 0.043543197214603424 Accuracy 0.8880000114440918\n",
      "Iteration 13270 Training loss 7.173664926085621e-05 Validation loss 0.04347136616706848 Accuracy 0.8889999985694885\n",
      "Iteration 13280 Training loss 0.0015795979415997863 Validation loss 0.04355035349726677 Accuracy 0.8880000114440918\n",
      "Iteration 13290 Training loss 0.0013191041070967913 Validation loss 0.04352573677897453 Accuracy 0.8889999985694885\n",
      "Iteration 13300 Training loss 0.0010739266872406006 Validation loss 0.043598245829343796 Accuracy 0.8880000114440918\n",
      "Iteration 13310 Training loss 0.001102015608921647 Validation loss 0.04359007254242897 Accuracy 0.8884999752044678\n",
      "Iteration 13320 Training loss 0.0013155300403013825 Validation loss 0.04355001822113991 Accuracy 0.8880000114440918\n",
      "Iteration 13330 Training loss 0.0008231418905779719 Validation loss 0.043555472046136856 Accuracy 0.8889999985694885\n",
      "Iteration 13340 Training loss 0.000573426834307611 Validation loss 0.04355105757713318 Accuracy 0.8884999752044678\n",
      "Iteration 13350 Training loss 0.0005725976661778986 Validation loss 0.04356486722826958 Accuracy 0.8880000114440918\n",
      "Iteration 13360 Training loss 0.0005727359093725681 Validation loss 0.04353456571698189 Accuracy 0.8880000114440918\n",
      "Iteration 13370 Training loss 0.0008213747059926391 Validation loss 0.04356639087200165 Accuracy 0.8880000114440918\n",
      "Iteration 13380 Training loss 0.0010790745727717876 Validation loss 0.043537940829992294 Accuracy 0.8889999985694885\n",
      "Iteration 13390 Training loss 0.0008254642598330975 Validation loss 0.043530985713005066 Accuracy 0.8884999752044678\n",
      "Iteration 13400 Training loss 0.0015766764990985394 Validation loss 0.04351991415023804 Accuracy 0.8895000219345093\n",
      "Iteration 13410 Training loss 0.0005746711394749582 Validation loss 0.043498482555150986 Accuracy 0.8889999985694885\n",
      "Iteration 13420 Training loss 0.0013355393894016743 Validation loss 0.04351232945919037 Accuracy 0.8895000219345093\n",
      "Iteration 13430 Training loss 0.0008338349871337414 Validation loss 0.0435173474252224 Accuracy 0.890500009059906\n",
      "Iteration 13440 Training loss 0.0015737742651253939 Validation loss 0.043568000197410583 Accuracy 0.8889999985694885\n",
      "Iteration 13450 Training loss 0.00107638502959162 Validation loss 0.04355436563491821 Accuracy 0.8880000114440918\n",
      "Iteration 13460 Training loss 0.0010877877939492464 Validation loss 0.043538641184568405 Accuracy 0.8889999985694885\n",
      "Iteration 13470 Training loss 0.0008229634840972722 Validation loss 0.04353766515851021 Accuracy 0.8884999752044678\n",
      "Iteration 13480 Training loss 0.0008171614608727396 Validation loss 0.04356038197875023 Accuracy 0.8884999752044678\n",
      "Iteration 13490 Training loss 0.0013268672628328204 Validation loss 0.043533798307180405 Accuracy 0.8889999985694885\n",
      "Iteration 13500 Training loss 0.0008289333782158792 Validation loss 0.043578170239925385 Accuracy 0.8884999752044678\n",
      "Iteration 13510 Training loss 0.001084745628759265 Validation loss 0.04357478767633438 Accuracy 0.8884999752044678\n",
      "Iteration 13520 Training loss 0.0008176951669156551 Validation loss 0.04358785226941109 Accuracy 0.8884999752044678\n",
      "Iteration 13530 Training loss 0.0005799224018119276 Validation loss 0.04352331534028053 Accuracy 0.8895000219345093\n",
      "Iteration 13540 Training loss 0.0005716634332202375 Validation loss 0.0435212105512619 Accuracy 0.8884999752044678\n",
      "Iteration 13550 Training loss 0.001066915923729539 Validation loss 0.04356485977768898 Accuracy 0.8884999752044678\n",
      "Iteration 13560 Training loss 0.0003242928651161492 Validation loss 0.04354880377650261 Accuracy 0.8895000219345093\n",
      "Iteration 13570 Training loss 0.000829278607852757 Validation loss 0.04351644590497017 Accuracy 0.8884999752044678\n",
      "Iteration 13580 Training loss 0.0020756118465214968 Validation loss 0.04352972283959389 Accuracy 0.8895000219345093\n",
      "Iteration 13590 Training loss 0.0005773958400823176 Validation loss 0.04356438294053078 Accuracy 0.8895000219345093\n",
      "Iteration 13600 Training loss 0.0005658513400703669 Validation loss 0.04355572164058685 Accuracy 0.8895000219345093\n",
      "Iteration 13610 Training loss 0.001078213332220912 Validation loss 0.04353076592087746 Accuracy 0.8884999752044678\n",
      "Iteration 13620 Training loss 0.0008281520567834377 Validation loss 0.04350850731134415 Accuracy 0.8899999856948853\n",
      "Iteration 13630 Training loss 0.000822250556666404 Validation loss 0.04351998120546341 Accuracy 0.890500009059906\n",
      "Iteration 13640 Training loss 0.001321042189374566 Validation loss 0.043573569506406784 Accuracy 0.8895000219345093\n",
      "Iteration 13650 Training loss 0.0008292043348774314 Validation loss 0.04357631504535675 Accuracy 0.8899999856948853\n",
      "Iteration 13660 Training loss 0.0008155888062901795 Validation loss 0.043567292392253876 Accuracy 0.890999972820282\n",
      "Iteration 13670 Training loss 0.00032236866536550224 Validation loss 0.04353431239724159 Accuracy 0.890500009059906\n",
      "Iteration 13680 Training loss 0.0010663619032129645 Validation loss 0.04351891204714775 Accuracy 0.8899999856948853\n",
      "Iteration 13690 Training loss 0.0008277923916466534 Validation loss 0.043505508452653885 Accuracy 0.8899999856948853\n",
      "Iteration 13700 Training loss 0.0008236606372520328 Validation loss 0.043590985238552094 Accuracy 0.8889999985694885\n",
      "Iteration 13710 Training loss 0.0010803166078403592 Validation loss 0.0435163788497448 Accuracy 0.890500009059906\n",
      "Iteration 13720 Training loss 0.0008260842878371477 Validation loss 0.04355989024043083 Accuracy 0.8895000219345093\n",
      "Iteration 13730 Training loss 0.0008133997325785458 Validation loss 0.04351091384887695 Accuracy 0.8895000219345093\n",
      "Iteration 13740 Training loss 0.0008250453975051641 Validation loss 0.043617941439151764 Accuracy 0.8884999752044678\n",
      "Iteration 13750 Training loss 0.0005849606823176146 Validation loss 0.04358544573187828 Accuracy 0.8895000219345093\n",
      "Iteration 13760 Training loss 0.0013235347578302026 Validation loss 0.04353049024939537 Accuracy 0.8899999856948853\n",
      "Iteration 13770 Training loss 0.0005733300349675119 Validation loss 0.043496277183294296 Accuracy 0.8895000219345093\n",
      "Iteration 13780 Training loss 0.00032129560713656247 Validation loss 0.04357313737273216 Accuracy 0.8899999856948853\n",
      "Iteration 13790 Training loss 0.0013212248450145125 Validation loss 0.04355374351143837 Accuracy 0.890999972820282\n",
      "Iteration 13800 Training loss 0.0008211593376472592 Validation loss 0.04357463866472244 Accuracy 0.8899999856948853\n",
      "Iteration 13810 Training loss 0.00032559287501499057 Validation loss 0.043587252497673035 Accuracy 0.8899999856948853\n",
      "Iteration 13820 Training loss 0.0020738213788717985 Validation loss 0.04349946603178978 Accuracy 0.8895000219345093\n",
      "Iteration 13830 Training loss 0.0005704312352463603 Validation loss 0.04352714866399765 Accuracy 0.890500009059906\n",
      "Iteration 13840 Training loss 0.0005691964761354029 Validation loss 0.0435791015625 Accuracy 0.8889999985694885\n",
      "Iteration 13850 Training loss 0.0013191794278100133 Validation loss 0.043508633971214294 Accuracy 0.890500009059906\n",
      "Iteration 13860 Training loss 0.000575185869820416 Validation loss 0.043553344905376434 Accuracy 0.8899999856948853\n",
      "Iteration 13870 Training loss 0.001068493234924972 Validation loss 0.04352118819952011 Accuracy 0.8889999985694885\n",
      "Iteration 13880 Training loss 0.001070127822458744 Validation loss 0.04353617504239082 Accuracy 0.890500009059906\n",
      "Iteration 13890 Training loss 0.0010708613554015756 Validation loss 0.043532516807317734 Accuracy 0.890500009059906\n",
      "Iteration 13900 Training loss 0.0005721093039028347 Validation loss 0.04348757490515709 Accuracy 0.8889999985694885\n",
      "Iteration 13910 Training loss 0.0015707063721492887 Validation loss 0.04356143996119499 Accuracy 0.890500009059906\n",
      "Iteration 13920 Training loss 0.0010672156931832433 Validation loss 0.043522439897060394 Accuracy 0.8895000219345093\n",
      "Iteration 13930 Training loss 0.0008223747136071324 Validation loss 0.04354206845164299 Accuracy 0.8895000219345093\n",
      "Iteration 13940 Training loss 0.0005741947679780424 Validation loss 0.04349604994058609 Accuracy 0.8895000219345093\n",
      "Iteration 13950 Training loss 0.002067547058686614 Validation loss 0.04356873780488968 Accuracy 0.8895000219345093\n",
      "Iteration 13960 Training loss 0.000820385932456702 Validation loss 0.04349871724843979 Accuracy 0.8899999856948853\n",
      "Iteration 13970 Training loss 0.0008191515225917101 Validation loss 0.043535321950912476 Accuracy 0.8895000219345093\n",
      "Iteration 13980 Training loss 0.0008131519425660372 Validation loss 0.04350848123431206 Accuracy 0.8899999856948853\n",
      "Iteration 13990 Training loss 0.0005708670360036194 Validation loss 0.043566975742578506 Accuracy 0.8899999856948853\n",
      "Iteration 14000 Training loss 0.0005687212105840445 Validation loss 0.0435568168759346 Accuracy 0.8899999856948853\n",
      "Iteration 14010 Training loss 0.0003203678934369236 Validation loss 0.04348691180348396 Accuracy 0.890500009059906\n",
      "Iteration 14020 Training loss 0.0010661984561011195 Validation loss 0.04352196678519249 Accuracy 0.8899999856948853\n",
      "Iteration 14030 Training loss 0.0013251323252916336 Validation loss 0.04348490759730339 Accuracy 0.8899999856948853\n",
      "Iteration 14040 Training loss 0.0008267528028227389 Validation loss 0.04350887984037399 Accuracy 0.8899999856948853\n",
      "Iteration 14050 Training loss 0.0015701060183346272 Validation loss 0.04351865500211716 Accuracy 0.8899999856948853\n",
      "Iteration 14060 Training loss 0.0008256073924712837 Validation loss 0.043445125222206116 Accuracy 0.8889999985694885\n",
      "Iteration 14070 Training loss 0.0003164975205436349 Validation loss 0.04353044182062149 Accuracy 0.8895000219345093\n",
      "Iteration 14080 Training loss 0.0008193110697902739 Validation loss 0.043512001633644104 Accuracy 0.8895000219345093\n",
      "Iteration 14090 Training loss 0.0008189355139620602 Validation loss 0.043515682220458984 Accuracy 0.8895000219345093\n",
      "Iteration 14100 Training loss 0.0005722553469240665 Validation loss 0.04351865127682686 Accuracy 0.8895000219345093\n",
      "Iteration 14110 Training loss 0.0008264035568572581 Validation loss 0.04350173845887184 Accuracy 0.8889999985694885\n",
      "Iteration 14120 Training loss 0.0003259170625824481 Validation loss 0.043511711061000824 Accuracy 0.8895000219345093\n",
      "Iteration 14130 Training loss 0.0010702770669013262 Validation loss 0.04351628199219704 Accuracy 0.8895000219345093\n",
      "Iteration 14140 Training loss 0.0013227981980890036 Validation loss 0.0435323603451252 Accuracy 0.8899999856948853\n",
      "Iteration 14150 Training loss 0.000569441297557205 Validation loss 0.043502893298864365 Accuracy 0.8895000219345093\n",
      "Iteration 14160 Training loss 0.0015683904057368636 Validation loss 0.04354390501976013 Accuracy 0.8899999856948853\n",
      "Iteration 14170 Training loss 0.0010700294515118003 Validation loss 0.043540552258491516 Accuracy 0.8899999856948853\n",
      "Iteration 14180 Training loss 0.0005677500739693642 Validation loss 0.04354160279035568 Accuracy 0.8899999856948853\n",
      "Iteration 14190 Training loss 0.0010704121086746454 Validation loss 0.043539825826883316 Accuracy 0.8895000219345093\n",
      "Iteration 14200 Training loss 6.62799648125656e-05 Validation loss 0.04355160519480705 Accuracy 0.8899999856948853\n",
      "Iteration 14210 Training loss 0.0005724155926145613 Validation loss 0.04352530092000961 Accuracy 0.8899999856948853\n",
      "Iteration 14220 Training loss 0.0008209660300053656 Validation loss 0.04352983087301254 Accuracy 0.8899999856948853\n",
      "Iteration 14230 Training loss 0.0003239347424823791 Validation loss 0.04354047402739525 Accuracy 0.8899999856948853\n",
      "Iteration 14240 Training loss 0.0005709390388801694 Validation loss 0.043553292751312256 Accuracy 0.8899999856948853\n",
      "Iteration 14250 Training loss 0.0010722099104896188 Validation loss 0.04350882023572922 Accuracy 0.8895000219345093\n",
      "Iteration 14260 Training loss 0.0010663130087777972 Validation loss 0.043498747050762177 Accuracy 0.8895000219345093\n",
      "Iteration 14270 Training loss 0.00031882693292573094 Validation loss 0.04353417828679085 Accuracy 0.8895000219345093\n",
      "Iteration 14280 Training loss 0.001070821308530867 Validation loss 0.043500326573848724 Accuracy 0.890500009059906\n",
      "Iteration 14290 Training loss 0.0008161842124536633 Validation loss 0.043544646352529526 Accuracy 0.8899999856948853\n",
      "Iteration 14300 Training loss 0.0008185565238818526 Validation loss 0.04352690279483795 Accuracy 0.8899999856948853\n",
      "Iteration 14310 Training loss 0.001573022804223001 Validation loss 0.043511420488357544 Accuracy 0.8895000219345093\n",
      "Iteration 14320 Training loss 0.0013253558427095413 Validation loss 0.04349532350897789 Accuracy 0.8889999985694885\n",
      "Iteration 14330 Training loss 0.0005710992263630033 Validation loss 0.0435543991625309 Accuracy 0.8899999856948853\n",
      "Iteration 14340 Training loss 0.0013163528637960553 Validation loss 0.04348732531070709 Accuracy 0.8899999856948853\n",
      "Iteration 14350 Training loss 0.0005715846782550216 Validation loss 0.0435091033577919 Accuracy 0.8895000219345093\n",
      "Iteration 14360 Training loss 0.0013167369179427624 Validation loss 0.043472059071063995 Accuracy 0.8895000219345093\n",
      "Iteration 14370 Training loss 0.0010691110510379076 Validation loss 0.04348888248205185 Accuracy 0.8895000219345093\n",
      "Iteration 14380 Training loss 0.0008265964570455253 Validation loss 0.04351535812020302 Accuracy 0.8895000219345093\n",
      "Iteration 14390 Training loss 0.0003171062271576375 Validation loss 0.043539855629205704 Accuracy 0.8884999752044678\n",
      "Iteration 14400 Training loss 0.00031304353615269065 Validation loss 0.043531544506549835 Accuracy 0.8895000219345093\n",
      "Iteration 14410 Training loss 0.0013190897880122066 Validation loss 0.043500710278749466 Accuracy 0.8899999856948853\n",
      "Iteration 14420 Training loss 0.0013165296986699104 Validation loss 0.043567437678575516 Accuracy 0.890500009059906\n",
      "Iteration 14430 Training loss 0.0008146133623085916 Validation loss 0.043504081666469574 Accuracy 0.8899999856948853\n",
      "Iteration 14440 Training loss 0.001318480703048408 Validation loss 0.04353369027376175 Accuracy 0.8895000219345093\n",
      "Iteration 14450 Training loss 0.0010724300518631935 Validation loss 0.04351416230201721 Accuracy 0.8895000219345093\n",
      "Iteration 14460 Training loss 0.0008219213341362774 Validation loss 0.04353954270482063 Accuracy 0.8895000219345093\n",
      "Iteration 14470 Training loss 0.0013240146217867732 Validation loss 0.04354302957653999 Accuracy 0.8899999856948853\n",
      "Iteration 14480 Training loss 0.0010666751768440008 Validation loss 0.043512895703315735 Accuracy 0.8895000219345093\n",
      "Iteration 14490 Training loss 0.0008185047190636396 Validation loss 0.04353766143321991 Accuracy 0.8895000219345093\n",
      "Iteration 14500 Training loss 0.0008157009724527597 Validation loss 0.043510932475328445 Accuracy 0.8895000219345093\n",
      "Iteration 14510 Training loss 0.0008181160083040595 Validation loss 0.04351476952433586 Accuracy 0.8895000219345093\n",
      "Iteration 14520 Training loss 0.0005660952883772552 Validation loss 0.043523672968149185 Accuracy 0.8895000219345093\n",
      "Iteration 14530 Training loss 0.000570376287214458 Validation loss 0.04347459226846695 Accuracy 0.8895000219345093\n",
      "Iteration 14540 Training loss 0.0008181657758541405 Validation loss 0.043502435088157654 Accuracy 0.8895000219345093\n",
      "Iteration 14550 Training loss 0.0010663116117939353 Validation loss 0.04353122040629387 Accuracy 0.8899999856948853\n",
      "Iteration 14560 Training loss 0.0003217644989490509 Validation loss 0.04351231828331947 Accuracy 0.8899999856948853\n",
      "Iteration 14570 Training loss 0.0013119815848767757 Validation loss 0.04353373870253563 Accuracy 0.8899999856948853\n",
      "Iteration 14580 Training loss 0.0008186547784134746 Validation loss 0.04351676255464554 Accuracy 0.8899999856948853\n",
      "Iteration 14590 Training loss 0.0008150679059326649 Validation loss 0.04352754354476929 Accuracy 0.890500009059906\n",
      "Iteration 14600 Training loss 0.0010656964732334018 Validation loss 0.043507859110832214 Accuracy 0.8899999856948853\n",
      "Iteration 14610 Training loss 0.0010685818269848824 Validation loss 0.04352886602282524 Accuracy 0.8899999856948853\n",
      "Iteration 14620 Training loss 0.0008165107574313879 Validation loss 0.043509334325790405 Accuracy 0.8899999856948853\n",
      "Iteration 14630 Training loss 0.00031816441332921386 Validation loss 0.043508876115083694 Accuracy 0.8899999856948853\n",
      "Iteration 14640 Training loss 0.0008152297814376652 Validation loss 0.04349453002214432 Accuracy 0.890500009059906\n",
      "Iteration 14650 Training loss 0.0008268977398984134 Validation loss 0.04348203167319298 Accuracy 0.8899999856948853\n",
      "Iteration 14660 Training loss 0.0005664412165060639 Validation loss 0.04351360350847244 Accuracy 0.8899999856948853\n",
      "Iteration 14670 Training loss 0.0003200664941687137 Validation loss 0.043488286435604095 Accuracy 0.8895000219345093\n",
      "Iteration 14680 Training loss 0.0005711654084734619 Validation loss 0.04350629448890686 Accuracy 0.890500009059906\n",
      "Iteration 14690 Training loss 0.0008126521133817732 Validation loss 0.04347563534975052 Accuracy 0.8899999856948853\n",
      "Iteration 14700 Training loss 0.0008136156830005348 Validation loss 0.043554022908210754 Accuracy 0.8899999856948853\n",
      "Iteration 14710 Training loss 0.001067155972123146 Validation loss 0.043507859110832214 Accuracy 0.8899999856948853\n",
      "Iteration 14720 Training loss 0.0008217189460992813 Validation loss 0.043456144630908966 Accuracy 0.8895000219345093\n",
      "Iteration 14730 Training loss 0.0005719703040085733 Validation loss 0.043472640216350555 Accuracy 0.8899999856948853\n",
      "Iteration 14740 Training loss 0.0010671091731637716 Validation loss 0.043467629700899124 Accuracy 0.8899999856948853\n",
      "Iteration 14750 Training loss 0.0008236242574639618 Validation loss 0.043493375182151794 Accuracy 0.8899999856948853\n",
      "Iteration 14760 Training loss 0.0008128352928906679 Validation loss 0.04352336749434471 Accuracy 0.8895000219345093\n",
      "Iteration 14770 Training loss 0.0005678115412592888 Validation loss 0.0434955470263958 Accuracy 0.8899999856948853\n",
      "Iteration 14780 Training loss 0.0008123302250169218 Validation loss 0.043491896241903305 Accuracy 0.8895000219345093\n",
      "Iteration 14790 Training loss 0.0005696662701666355 Validation loss 0.0434870645403862 Accuracy 0.8899999856948853\n",
      "Iteration 14800 Training loss 0.001067907316610217 Validation loss 0.043447788804769516 Accuracy 0.8895000219345093\n",
      "Iteration 14810 Training loss 0.0008215527050197124 Validation loss 0.043485406786203384 Accuracy 0.8899999856948853\n",
      "Iteration 14820 Training loss 0.0008182174060493708 Validation loss 0.043466247618198395 Accuracy 0.8895000219345093\n",
      "Iteration 14830 Training loss 0.00031949023832567036 Validation loss 0.04349682107567787 Accuracy 0.8895000219345093\n",
      "Iteration 14840 Training loss 0.0008232821710407734 Validation loss 0.04352384805679321 Accuracy 0.8895000219345093\n",
      "Iteration 14850 Training loss 0.0003212969459127635 Validation loss 0.04350542649626732 Accuracy 0.8889999985694885\n",
      "Iteration 14860 Training loss 0.0008187844068743289 Validation loss 0.043490272015333176 Accuracy 0.8895000219345093\n",
      "Iteration 14870 Training loss 0.0013141134986653924 Validation loss 0.04347683861851692 Accuracy 0.8899999856948853\n",
      "Iteration 14880 Training loss 0.0008188260835595429 Validation loss 0.04345250874757767 Accuracy 0.8899999856948853\n",
      "Iteration 14890 Training loss 0.0005640255403704941 Validation loss 0.04351326450705528 Accuracy 0.8895000219345093\n",
      "Iteration 14900 Training loss 0.001321463263593614 Validation loss 0.043508972972631454 Accuracy 0.8895000219345093\n",
      "Iteration 14910 Training loss 0.001319267088547349 Validation loss 0.0434969961643219 Accuracy 0.8895000219345093\n",
      "Iteration 14920 Training loss 0.0003168820694554597 Validation loss 0.043487221002578735 Accuracy 0.8889999985694885\n",
      "Iteration 14930 Training loss 0.0010666234884411097 Validation loss 0.043524470180273056 Accuracy 0.8899999856948853\n",
      "Iteration 14940 Training loss 0.0010678491089493036 Validation loss 0.04345714673399925 Accuracy 0.8889999985694885\n",
      "Iteration 14950 Training loss 0.0013202338013797998 Validation loss 0.04351567104458809 Accuracy 0.8895000219345093\n",
      "Iteration 14960 Training loss 0.000566546805202961 Validation loss 0.04349875822663307 Accuracy 0.8889999985694885\n",
      "Iteration 14970 Training loss 0.0013220332330092788 Validation loss 0.04346706345677376 Accuracy 0.8889999985694885\n",
      "Iteration 14980 Training loss 0.0008159192511811852 Validation loss 0.04345322027802467 Accuracy 0.8889999985694885\n",
      "Iteration 14990 Training loss 0.0003173840814270079 Validation loss 0.04347679764032364 Accuracy 0.8895000219345093\n",
      "Iteration 15000 Training loss 6.231125735212117e-05 Validation loss 0.04349686577916145 Accuracy 0.8889999985694885\n",
      "Iteration 15010 Training loss 0.0008176447008736432 Validation loss 0.04344453290104866 Accuracy 0.8889999985694885\n",
      "Iteration 15020 Training loss 0.0003144630463793874 Validation loss 0.043485045433044434 Accuracy 0.8895000219345093\n",
      "Iteration 15030 Training loss 0.0003141738416161388 Validation loss 0.04349064081907272 Accuracy 0.8895000219345093\n",
      "Iteration 15040 Training loss 0.0005671767285093665 Validation loss 0.04348675534129143 Accuracy 0.8895000219345093\n",
      "Iteration 15050 Training loss 0.0010630879551172256 Validation loss 0.04348316043615341 Accuracy 0.8895000219345093\n",
      "Iteration 15060 Training loss 6.884508184157312e-05 Validation loss 0.04345429316163063 Accuracy 0.8895000219345093\n",
      "Iteration 15070 Training loss 0.0010724333114922047 Validation loss 0.043467700481414795 Accuracy 0.8895000219345093\n",
      "Iteration 15080 Training loss 0.0008184551261365414 Validation loss 0.04342963546514511 Accuracy 0.8889999985694885\n",
      "Iteration 15090 Training loss 0.000325666245771572 Validation loss 0.043405335396528244 Accuracy 0.8895000219345093\n",
      "Iteration 15100 Training loss 0.0010688960319384933 Validation loss 0.043460410088300705 Accuracy 0.8895000219345093\n",
      "Iteration 15110 Training loss 0.0008161605801433325 Validation loss 0.04346171021461487 Accuracy 0.8889999985694885\n",
      "Iteration 15120 Training loss 0.0013127984711900353 Validation loss 0.043445419520139694 Accuracy 0.8895000219345093\n",
      "Iteration 15130 Training loss 0.0010678372345864773 Validation loss 0.043448470532894135 Accuracy 0.8895000219345093\n",
      "Iteration 15140 Training loss 0.0010686011519283056 Validation loss 0.04344579949975014 Accuracy 0.8895000219345093\n",
      "Iteration 15150 Training loss 0.0008188897627405822 Validation loss 0.04346605762839317 Accuracy 0.8895000219345093\n",
      "Iteration 15160 Training loss 0.001069207675755024 Validation loss 0.04346606507897377 Accuracy 0.8889999985694885\n",
      "Iteration 15170 Training loss 0.0010691960342228413 Validation loss 0.04345489293336868 Accuracy 0.8895000219345093\n",
      "Iteration 15180 Training loss 0.0015675971517339349 Validation loss 0.0434156097471714 Accuracy 0.8895000219345093\n",
      "Iteration 15190 Training loss 0.0010663039283826947 Validation loss 0.04343144968152046 Accuracy 0.8889999985694885\n",
      "Iteration 15200 Training loss 0.001561859273351729 Validation loss 0.04341917484998703 Accuracy 0.8884999752044678\n",
      "Iteration 15210 Training loss 0.001070552272722125 Validation loss 0.04344520345330238 Accuracy 0.8899999856948853\n",
      "Iteration 15220 Training loss 0.0010673636570572853 Validation loss 0.04348154738545418 Accuracy 0.8889999985694885\n",
      "Iteration 15230 Training loss 0.001069378573447466 Validation loss 0.043483734130859375 Accuracy 0.8899999856948853\n",
      "Iteration 15240 Training loss 0.0010675863595679402 Validation loss 0.04346638172864914 Accuracy 0.8899999856948853\n",
      "Iteration 15250 Training loss 0.0005677980952896178 Validation loss 0.04348882660269737 Accuracy 0.8889999985694885\n",
      "Iteration 15260 Training loss 0.0015636770986020565 Validation loss 0.04347599670290947 Accuracy 0.8889999985694885\n",
      "Iteration 15270 Training loss 0.0005734238075092435 Validation loss 0.043443795293569565 Accuracy 0.8889999985694885\n",
      "Iteration 15280 Training loss 0.00156927271746099 Validation loss 0.04347827285528183 Accuracy 0.8889999985694885\n",
      "Iteration 15290 Training loss 0.0005702889175154269 Validation loss 0.043453529477119446 Accuracy 0.8884999752044678\n",
      "Iteration 15300 Training loss 0.0008221174357458949 Validation loss 0.043453194200992584 Accuracy 0.8899999856948853\n",
      "Iteration 15310 Training loss 0.0020651884842664003 Validation loss 0.04344940185546875 Accuracy 0.8899999856948853\n",
      "Iteration 15320 Training loss 0.0013167372671887279 Validation loss 0.04342488572001457 Accuracy 0.8895000219345093\n",
      "Iteration 15330 Training loss 0.00031855417182669044 Validation loss 0.04344514384865761 Accuracy 0.8884999752044678\n",
      "Iteration 15340 Training loss 0.00106773991137743 Validation loss 0.04342232644557953 Accuracy 0.8889999985694885\n",
      "Iteration 15350 Training loss 0.0008144471212290227 Validation loss 0.043441448360681534 Accuracy 0.8895000219345093\n",
      "Iteration 15360 Training loss 0.0010638521052896976 Validation loss 0.04341747239232063 Accuracy 0.8895000219345093\n",
      "Iteration 15370 Training loss 0.0015666953986510634 Validation loss 0.04342248663306236 Accuracy 0.8889999985694885\n",
      "Iteration 15380 Training loss 0.0008169165230356157 Validation loss 0.043452147394418716 Accuracy 0.8889999985694885\n",
      "Iteration 15390 Training loss 0.00031543237855657935 Validation loss 0.04339488223195076 Accuracy 0.8889999985694885\n",
      "Iteration 15400 Training loss 0.0010677074315026402 Validation loss 0.0434352345764637 Accuracy 0.8895000219345093\n",
      "Iteration 15410 Training loss 0.0008192592649720609 Validation loss 0.0434257797896862 Accuracy 0.8884999752044678\n",
      "Iteration 15420 Training loss 0.0015712645836174488 Validation loss 0.04343206062912941 Accuracy 0.8889999985694885\n",
      "Iteration 15430 Training loss 0.0018166196532547474 Validation loss 0.043432239443063736 Accuracy 0.8889999985694885\n",
      "Iteration 15440 Training loss 0.0013201787369325757 Validation loss 0.04343840479850769 Accuracy 0.8889999985694885\n",
      "Iteration 15450 Training loss 0.0010643762070685625 Validation loss 0.04343666136264801 Accuracy 0.8889999985694885\n",
      "Iteration 15460 Training loss 0.001573906047269702 Validation loss 0.04344276338815689 Accuracy 0.8889999985694885\n",
      "Iteration 15470 Training loss 0.0005681084003299475 Validation loss 0.043420154601335526 Accuracy 0.8889999985694885\n",
      "Iteration 15480 Training loss 0.0008147383923642337 Validation loss 0.04342179372906685 Accuracy 0.8889999985694885\n",
      "Iteration 15490 Training loss 0.0005677571753039956 Validation loss 0.04342721775174141 Accuracy 0.8895000219345093\n",
      "Iteration 15500 Training loss 0.001066210214048624 Validation loss 0.043441515415906906 Accuracy 0.8889999985694885\n",
      "Iteration 15510 Training loss 0.0003195806057192385 Validation loss 0.04344003647565842 Accuracy 0.8895000219345093\n",
      "Iteration 15520 Training loss 0.0013175596250221133 Validation loss 0.04341502487659454 Accuracy 0.8884999752044678\n",
      "Iteration 15530 Training loss 0.001566823455505073 Validation loss 0.04342404380440712 Accuracy 0.8889999985694885\n",
      "Iteration 15540 Training loss 0.0015651264693588018 Validation loss 0.043473731726408005 Accuracy 0.8889999985694885\n",
      "Iteration 15550 Training loss 0.0010628807358443737 Validation loss 0.04342044144868851 Accuracy 0.8884999752044678\n",
      "Iteration 15560 Training loss 0.0005686947260983288 Validation loss 0.043443236500024796 Accuracy 0.8884999752044678\n",
      "Iteration 15570 Training loss 0.000822890957351774 Validation loss 0.04344766214489937 Accuracy 0.8889999985694885\n",
      "Iteration 15580 Training loss 0.0005712841521017253 Validation loss 0.04341461509466171 Accuracy 0.8889999985694885\n",
      "Iteration 15590 Training loss 0.001071874750778079 Validation loss 0.04339074715971947 Accuracy 0.8889999985694885\n",
      "Iteration 15600 Training loss 0.0015655256574973464 Validation loss 0.04340464249253273 Accuracy 0.8884999752044678\n",
      "Iteration 15610 Training loss 0.0003134541620966047 Validation loss 0.04341277480125427 Accuracy 0.8884999752044678\n",
      "Iteration 15620 Training loss 0.0008178342832252383 Validation loss 0.04343858361244202 Accuracy 0.8884999752044678\n",
      "Iteration 15630 Training loss 0.0013180217938497663 Validation loss 0.04344300925731659 Accuracy 0.8889999985694885\n",
      "Iteration 15640 Training loss 0.0003168144612573087 Validation loss 0.04341239854693413 Accuracy 0.8889999985694885\n",
      "Iteration 15650 Training loss 0.0010675080120563507 Validation loss 0.04342595487833023 Accuracy 0.8895000219345093\n",
      "Iteration 15660 Training loss 0.00031677749939262867 Validation loss 0.043443355709314346 Accuracy 0.8895000219345093\n",
      "Iteration 15670 Training loss 0.00031889314414002 Validation loss 0.04342183843255043 Accuracy 0.8895000219345093\n",
      "Iteration 15680 Training loss 0.0008220479940064251 Validation loss 0.043414004147052765 Accuracy 0.8880000114440918\n",
      "Iteration 15690 Training loss 0.0008182195597328246 Validation loss 0.04343174770474434 Accuracy 0.8895000219345093\n",
      "Iteration 15700 Training loss 0.0010670438641682267 Validation loss 0.043453872203826904 Accuracy 0.8889999985694885\n",
      "Iteration 15710 Training loss 0.0005634669214487076 Validation loss 0.04343273118138313 Accuracy 0.8895000219345093\n",
      "Iteration 15720 Training loss 0.0003178176120854914 Validation loss 0.04345143958926201 Accuracy 0.8895000219345093\n",
      "Iteration 15730 Training loss 0.0013204965507611632 Validation loss 0.043462127447128296 Accuracy 0.8880000114440918\n",
      "Iteration 15740 Training loss 0.0015689482679590583 Validation loss 0.043427176773548126 Accuracy 0.8889999985694885\n",
      "Iteration 15750 Training loss 0.00031545889214612544 Validation loss 0.043471504002809525 Accuracy 0.8889999985694885\n",
      "Iteration 15760 Training loss 0.0003193650918547064 Validation loss 0.04343915730714798 Accuracy 0.8895000219345093\n",
      "Iteration 15770 Training loss 0.0015648659318685532 Validation loss 0.043447524309158325 Accuracy 0.8889999985694885\n",
      "Iteration 15780 Training loss 0.0013185234274715185 Validation loss 0.04345596954226494 Accuracy 0.8889999985694885\n",
      "Iteration 15790 Training loss 0.001068380195647478 Validation loss 0.04342278838157654 Accuracy 0.8889999985694885\n",
      "Iteration 15800 Training loss 0.0013150322483852506 Validation loss 0.043445974588394165 Accuracy 0.8884999752044678\n",
      "Iteration 15810 Training loss 0.0008161908481270075 Validation loss 0.04343419522047043 Accuracy 0.8884999752044678\n",
      "Iteration 15820 Training loss 0.0010672106873244047 Validation loss 0.04343121498823166 Accuracy 0.8889999985694885\n",
      "Iteration 15830 Training loss 0.0005653022672049701 Validation loss 0.04347516968846321 Accuracy 0.8889999985694885\n",
      "Iteration 15840 Training loss 0.00032224913593381643 Validation loss 0.043495453894138336 Accuracy 0.8889999985694885\n",
      "Iteration 15850 Training loss 0.000817919266410172 Validation loss 0.04350070282816887 Accuracy 0.8889999985694885\n",
      "Iteration 15860 Training loss 0.0005658133304677904 Validation loss 0.04345571994781494 Accuracy 0.8889999985694885\n",
      "Iteration 15870 Training loss 0.0008182419696822762 Validation loss 0.043467842042446136 Accuracy 0.8889999985694885\n",
      "Iteration 15880 Training loss 0.0008125813910737634 Validation loss 0.04344068840146065 Accuracy 0.8889999985694885\n",
      "Iteration 15890 Training loss 0.00031577469781041145 Validation loss 0.043432287871837616 Accuracy 0.8889999985694885\n",
      "Iteration 15900 Training loss 0.0008178759017027915 Validation loss 0.043438542634248734 Accuracy 0.8895000219345093\n",
      "Iteration 15910 Training loss 0.0010681157000362873 Validation loss 0.04340203478932381 Accuracy 0.8880000114440918\n",
      "Iteration 15920 Training loss 7.014024595264345e-05 Validation loss 0.04340810328722 Accuracy 0.8889999985694885\n",
      "Iteration 15930 Training loss 0.001066358294337988 Validation loss 0.04342217743396759 Accuracy 0.8895000219345093\n",
      "Iteration 15940 Training loss 0.001320487237535417 Validation loss 0.043409015983343124 Accuracy 0.8889999985694885\n",
      "Iteration 15950 Training loss 0.0013252258067950606 Validation loss 0.04340321943163872 Accuracy 0.8895000219345093\n",
      "Iteration 15960 Training loss 0.00031873557600192726 Validation loss 0.04339281842112541 Accuracy 0.8895000219345093\n",
      "Iteration 15970 Training loss 0.0010698740370571613 Validation loss 0.043386176228523254 Accuracy 0.8895000219345093\n",
      "Iteration 15980 Training loss 0.0018182164058089256 Validation loss 0.043393492698669434 Accuracy 0.8895000219345093\n",
      "Iteration 15990 Training loss 0.0008134269155561924 Validation loss 0.04340319707989693 Accuracy 0.8895000219345093\n",
      "Iteration 16000 Training loss 0.001314151450060308 Validation loss 0.043438930064439774 Accuracy 0.8895000219345093\n",
      "Iteration 16010 Training loss 0.001067328150384128 Validation loss 0.043447528034448624 Accuracy 0.8889999985694885\n",
      "Iteration 16020 Training loss 0.0005698685999959707 Validation loss 0.043424345552921295 Accuracy 0.8889999985694885\n",
      "Iteration 16030 Training loss 0.0015674831811338663 Validation loss 0.04339897260069847 Accuracy 0.8889999985694885\n",
      "Iteration 16040 Training loss 0.0010665482841432095 Validation loss 0.04344348609447479 Accuracy 0.8895000219345093\n",
      "Iteration 16050 Training loss 0.0008115805103443563 Validation loss 0.043413516134023666 Accuracy 0.8880000114440918\n",
      "Iteration 16060 Training loss 0.0008208950166590512 Validation loss 0.04343909025192261 Accuracy 0.8889999985694885\n",
      "Iteration 16070 Training loss 0.0008130129426717758 Validation loss 0.043405674397945404 Accuracy 0.8895000219345093\n",
      "Iteration 16080 Training loss 0.00057109200861305 Validation loss 0.04342426359653473 Accuracy 0.8895000219345093\n",
      "Iteration 16090 Training loss 0.0005665492499247193 Validation loss 0.0433998629450798 Accuracy 0.8889999985694885\n",
      "Iteration 16100 Training loss 0.0005699023604393005 Validation loss 0.043406080454587936 Accuracy 0.8895000219345093\n",
      "Iteration 16110 Training loss 0.0005693117273040116 Validation loss 0.043426092714071274 Accuracy 0.8895000219345093\n",
      "Iteration 16120 Training loss 0.001570548047311604 Validation loss 0.04339645057916641 Accuracy 0.8884999752044678\n",
      "Iteration 16130 Training loss 0.0008229198283515871 Validation loss 0.0434182807803154 Accuracy 0.8895000219345093\n",
      "Iteration 16140 Training loss 0.0010685132583603263 Validation loss 0.04343467578291893 Accuracy 0.8895000219345093\n",
      "Iteration 16150 Training loss 0.0013189484598115087 Validation loss 0.04341091215610504 Accuracy 0.8895000219345093\n",
      "Iteration 16160 Training loss 0.0005679890746250749 Validation loss 0.04338003322482109 Accuracy 0.8895000219345093\n",
      "Iteration 16170 Training loss 0.0008229815284721553 Validation loss 0.04339893162250519 Accuracy 0.8895000219345093\n",
      "Iteration 16180 Training loss 0.001320517621934414 Validation loss 0.043381642550230026 Accuracy 0.8895000219345093\n",
      "Iteration 16190 Training loss 0.0010683879954740405 Validation loss 0.0433821864426136 Accuracy 0.8895000219345093\n",
      "Iteration 16200 Training loss 0.0008158409036695957 Validation loss 0.04340711236000061 Accuracy 0.8889999985694885\n",
      "Iteration 16210 Training loss 0.00031384170870296657 Validation loss 0.04339579865336418 Accuracy 0.8884999752044678\n",
      "Iteration 16220 Training loss 0.0013246805174276233 Validation loss 0.04344703629612923 Accuracy 0.8884999752044678\n",
      "Iteration 16230 Training loss 0.0008170701912604272 Validation loss 0.043400127440690994 Accuracy 0.8895000219345093\n",
      "Iteration 16240 Training loss 0.0008183058234862983 Validation loss 0.04339660331606865 Accuracy 0.8889999985694885\n",
      "Iteration 16250 Training loss 0.0008176175178959966 Validation loss 0.04342160001397133 Accuracy 0.8895000219345093\n",
      "Iteration 16260 Training loss 7.185241702245548e-05 Validation loss 0.043424468487501144 Accuracy 0.8889999985694885\n",
      "Iteration 16270 Training loss 0.001321734394878149 Validation loss 0.043421097099781036 Accuracy 0.8889999985694885\n",
      "Iteration 16280 Training loss 0.0015717139467597008 Validation loss 0.04343321546912193 Accuracy 0.8895000219345093\n",
      "Iteration 16290 Training loss 0.0003220262879040092 Validation loss 0.043452974408864975 Accuracy 0.8895000219345093\n",
      "Iteration 16300 Training loss 0.001068375539034605 Validation loss 0.0434623584151268 Accuracy 0.8889999985694885\n",
      "Iteration 16310 Training loss 0.0005667614750564098 Validation loss 0.043437860906124115 Accuracy 0.8895000219345093\n",
      "Iteration 16320 Training loss 0.0005722703645005822 Validation loss 0.0434161052107811 Accuracy 0.8884999752044678\n",
      "Iteration 16330 Training loss 0.0005686762160621583 Validation loss 0.043457649648189545 Accuracy 0.8889999985694885\n",
      "Iteration 16340 Training loss 0.0008164749015122652 Validation loss 0.043406955897808075 Accuracy 0.8889999985694885\n",
      "Iteration 16350 Training loss 0.0005690030520781875 Validation loss 0.04344385117292404 Accuracy 0.8895000219345093\n",
      "Iteration 16360 Training loss 0.0013163647381588817 Validation loss 0.043417200446128845 Accuracy 0.8895000219345093\n",
      "Iteration 16370 Training loss 0.0010654532816261053 Validation loss 0.043393559753894806 Accuracy 0.8889999985694885\n",
      "Iteration 16380 Training loss 0.0015684902900829911 Validation loss 0.04338853061199188 Accuracy 0.8895000219345093\n",
      "Iteration 16390 Training loss 0.0005669455858878791 Validation loss 0.04339505732059479 Accuracy 0.8895000219345093\n",
      "Iteration 16400 Training loss 0.0005674423300661147 Validation loss 0.04338262602686882 Accuracy 0.8889999985694885\n",
      "Iteration 16410 Training loss 0.0010612281039357185 Validation loss 0.04341340810060501 Accuracy 0.8895000219345093\n",
      "Iteration 16420 Training loss 6.466279592132196e-05 Validation loss 0.04340707138180733 Accuracy 0.8895000219345093\n",
      "Iteration 16430 Training loss 0.0010664667934179306 Validation loss 0.04340028390288353 Accuracy 0.8884999752044678\n",
      "Iteration 16440 Training loss 0.0008241066825576127 Validation loss 0.043446846306324005 Accuracy 0.8889999985694885\n",
      "Iteration 16450 Training loss 0.0008173187961801887 Validation loss 0.04343383386731148 Accuracy 0.8889999985694885\n",
      "Iteration 16460 Training loss 0.0003193661104887724 Validation loss 0.04341613128781319 Accuracy 0.8889999985694885\n",
      "Iteration 16470 Training loss 0.0003196117759216577 Validation loss 0.04339349642395973 Accuracy 0.8884999752044678\n",
      "Iteration 16480 Training loss 7.442810601787642e-05 Validation loss 0.04337996989488602 Accuracy 0.8884999752044678\n",
      "Iteration 16490 Training loss 0.0008157335105352104 Validation loss 0.04336632788181305 Accuracy 0.8884999752044678\n",
      "Iteration 16500 Training loss 0.00031874049454927444 Validation loss 0.04339921474456787 Accuracy 0.8889999985694885\n",
      "Iteration 16510 Training loss 0.0008191122324205935 Validation loss 0.04342205077409744 Accuracy 0.8889999985694885\n",
      "Iteration 16520 Training loss 0.00032086382270790637 Validation loss 0.04346984252333641 Accuracy 0.8884999752044678\n",
      "Iteration 16530 Training loss 0.00031460999161936343 Validation loss 0.04341861605644226 Accuracy 0.8884999752044678\n",
      "Iteration 16540 Training loss 0.001067014061845839 Validation loss 0.043420203030109406 Accuracy 0.8884999752044678\n",
      "Iteration 16550 Training loss 0.0008186945342458785 Validation loss 0.04339936003088951 Accuracy 0.8889999985694885\n",
      "Iteration 16560 Training loss 0.0005665828357450664 Validation loss 0.043402623385190964 Accuracy 0.8889999985694885\n",
      "Iteration 16570 Training loss 0.0013172784820199013 Validation loss 0.04340822249650955 Accuracy 0.8884999752044678\n",
      "Iteration 16580 Training loss 0.00031386062619276345 Validation loss 0.04338059946894646 Accuracy 0.8895000219345093\n",
      "Iteration 16590 Training loss 0.0005643874756060541 Validation loss 0.043431952595710754 Accuracy 0.8889999985694885\n",
      "Iteration 16600 Training loss 0.0005685249343514442 Validation loss 0.043364591896533966 Accuracy 0.8895000219345093\n",
      "Iteration 16610 Training loss 0.0010653286008164287 Validation loss 0.04337886720895767 Accuracy 0.8889999985694885\n",
      "Iteration 16620 Training loss 0.0013191272737458348 Validation loss 0.04339999705553055 Accuracy 0.8895000219345093\n",
      "Iteration 16630 Training loss 0.0010664918227121234 Validation loss 0.0434560626745224 Accuracy 0.8889999985694885\n",
      "Iteration 16640 Training loss 0.0015698457136750221 Validation loss 0.0433989018201828 Accuracy 0.8889999985694885\n",
      "Iteration 16650 Training loss 0.0013153421459719539 Validation loss 0.043421659618616104 Accuracy 0.8884999752044678\n",
      "Iteration 16660 Training loss 0.0015674552414566278 Validation loss 0.04335015267133713 Accuracy 0.8895000219345093\n",
      "Iteration 16670 Training loss 0.000566452625207603 Validation loss 0.04339420422911644 Accuracy 0.8895000219345093\n",
      "Iteration 16680 Training loss 0.000318409176543355 Validation loss 0.04337674751877785 Accuracy 0.8895000219345093\n",
      "Iteration 16690 Training loss 0.0008201271994039416 Validation loss 0.04342019185423851 Accuracy 0.8884999752044678\n",
      "Iteration 16700 Training loss 0.0005630714586004615 Validation loss 0.04335162416100502 Accuracy 0.8895000219345093\n",
      "Iteration 16710 Training loss 0.0008140791906043887 Validation loss 0.043402526527643204 Accuracy 0.8884999752044678\n",
      "Iteration 16720 Training loss 0.0005630642408505082 Validation loss 0.04340284690260887 Accuracy 0.8895000219345093\n",
      "Iteration 16730 Training loss 0.000811427365988493 Validation loss 0.04338394105434418 Accuracy 0.8899999856948853\n",
      "Iteration 16740 Training loss 0.0008117252727970481 Validation loss 0.04338652640581131 Accuracy 0.8889999985694885\n",
      "Iteration 16750 Training loss 0.0003108241071458906 Validation loss 0.043392959982156754 Accuracy 0.8889999985694885\n",
      "Iteration 16760 Training loss 0.00032462182571180165 Validation loss 0.043379850685596466 Accuracy 0.8884999752044678\n",
      "Iteration 16770 Training loss 0.0005663831834681332 Validation loss 0.04339054599404335 Accuracy 0.8880000114440918\n",
      "Iteration 16780 Training loss 0.0010694552911445498 Validation loss 0.04339632764458656 Accuracy 0.8884999752044678\n",
      "Iteration 16790 Training loss 0.0005692105041816831 Validation loss 0.04338536411523819 Accuracy 0.8884999752044678\n",
      "Iteration 16800 Training loss 0.0010716263204813004 Validation loss 0.04337165132164955 Accuracy 0.8889999985694885\n",
      "Iteration 16810 Training loss 0.0008223881595768034 Validation loss 0.043388426303863525 Accuracy 0.8895000219345093\n",
      "Iteration 16820 Training loss 6.242416566237807e-05 Validation loss 0.043396979570388794 Accuracy 0.8884999752044678\n",
      "Iteration 16830 Training loss 0.0010664297733455896 Validation loss 0.04338059201836586 Accuracy 0.8889999985694885\n",
      "Iteration 16840 Training loss 6.147690146462992e-05 Validation loss 0.04338666424155235 Accuracy 0.8895000219345093\n",
      "Iteration 16850 Training loss 0.000570578093174845 Validation loss 0.04340890794992447 Accuracy 0.8889999985694885\n",
      "Iteration 16860 Training loss 0.0008147675544023514 Validation loss 0.043389804661273956 Accuracy 0.8889999985694885\n",
      "Iteration 16870 Training loss 0.0008136591059155762 Validation loss 0.043374281376600266 Accuracy 0.8884999752044678\n",
      "Iteration 16880 Training loss 0.0005685119540430605 Validation loss 0.043364908546209335 Accuracy 0.8895000219345093\n",
      "Iteration 16890 Training loss 0.001065500546246767 Validation loss 0.04338045045733452 Accuracy 0.8880000114440918\n",
      "Iteration 16900 Training loss 0.0005659000598825514 Validation loss 0.04336227849125862 Accuracy 0.8884999752044678\n",
      "Iteration 16910 Training loss 0.0008149596396833658 Validation loss 0.043368302285671234 Accuracy 0.8880000114440918\n",
      "Iteration 16920 Training loss 0.0008176742121577263 Validation loss 0.04335414245724678 Accuracy 0.8899999856948853\n",
      "Iteration 16930 Training loss 0.000564534799195826 Validation loss 0.043344225734472275 Accuracy 0.8884999752044678\n",
      "Iteration 16940 Training loss 0.0008141185389831662 Validation loss 0.04335303604602814 Accuracy 0.8889999985694885\n",
      "Iteration 16950 Training loss 0.0015689653810113668 Validation loss 0.043324943631887436 Accuracy 0.8895000219345093\n",
      "Iteration 16960 Training loss 0.0008207238861359656 Validation loss 0.043340783566236496 Accuracy 0.8884999752044678\n",
      "Iteration 16970 Training loss 6.641977961407974e-05 Validation loss 0.04334358870983124 Accuracy 0.8895000219345093\n",
      "Iteration 16980 Training loss 0.002067987574264407 Validation loss 0.04337555170059204 Accuracy 0.8884999752044678\n",
      "Iteration 16990 Training loss 0.0005598902353085577 Validation loss 0.043390028178691864 Accuracy 0.8895000219345093\n",
      "Iteration 17000 Training loss 0.0005710923578590155 Validation loss 0.04337798431515694 Accuracy 0.8889999985694885\n",
      "Iteration 17010 Training loss 0.0008214905392378569 Validation loss 0.043378736823797226 Accuracy 0.8889999985694885\n",
      "Iteration 17020 Training loss 0.0013102859957143664 Validation loss 0.04333584010601044 Accuracy 0.8889999985694885\n",
      "Iteration 17030 Training loss 0.0018235364696010947 Validation loss 0.0433490015566349 Accuracy 0.8889999985694885\n",
      "Iteration 17040 Training loss 0.0005656371358782053 Validation loss 0.043359871953725815 Accuracy 0.8884999752044678\n",
      "Iteration 17050 Training loss 0.0005684042698703706 Validation loss 0.04335664212703705 Accuracy 0.8884999752044678\n",
      "Iteration 17060 Training loss 0.0003147122624795884 Validation loss 0.043341945856809616 Accuracy 0.8884999752044678\n",
      "Iteration 17070 Training loss 0.0010675801895558834 Validation loss 0.043346986174583435 Accuracy 0.8889999985694885\n",
      "Iteration 17080 Training loss 0.0010700463317334652 Validation loss 0.04336335510015488 Accuracy 0.8889999985694885\n",
      "Iteration 17090 Training loss 0.0008186160121113062 Validation loss 0.0433710552752018 Accuracy 0.8895000219345093\n",
      "Iteration 17100 Training loss 0.001563665340654552 Validation loss 0.04336465150117874 Accuracy 0.8884999752044678\n",
      "Iteration 17110 Training loss 0.0003179339109919965 Validation loss 0.043363142758607864 Accuracy 0.8899999856948853\n",
      "Iteration 17120 Training loss 0.0005705395597033203 Validation loss 0.04336324334144592 Accuracy 0.8889999985694885\n",
      "Iteration 17130 Training loss 0.0008132929797284305 Validation loss 0.043393272906541824 Accuracy 0.8895000219345093\n",
      "Iteration 17140 Training loss 0.00031213153852149844 Validation loss 0.04336274787783623 Accuracy 0.8889999985694885\n",
      "Iteration 17150 Training loss 0.0005718524334952235 Validation loss 0.043354377150535583 Accuracy 0.8899999856948853\n",
      "Iteration 17160 Training loss 0.0008088442264124751 Validation loss 0.043379951268434525 Accuracy 0.8889999985694885\n",
      "Iteration 17170 Training loss 0.0010695166420191526 Validation loss 0.04336826503276825 Accuracy 0.8889999985694885\n",
      "Iteration 17180 Training loss 0.001313954358920455 Validation loss 0.04333381727337837 Accuracy 0.8889999985694885\n",
      "Iteration 17190 Training loss 0.0005666287033818662 Validation loss 0.04336617514491081 Accuracy 0.8884999752044678\n",
      "Iteration 17200 Training loss 0.001065958640538156 Validation loss 0.043353840708732605 Accuracy 0.8884999752044678\n",
      "Iteration 17210 Training loss 0.0013125342084094882 Validation loss 0.043345510959625244 Accuracy 0.8889999985694885\n",
      "Iteration 17220 Training loss 0.0005588585045188665 Validation loss 0.043364979326725006 Accuracy 0.8899999856948853\n",
      "Iteration 17230 Training loss 0.0005666581564582884 Validation loss 0.04336950182914734 Accuracy 0.8884999752044678\n",
      "Iteration 17240 Training loss 7.079650822561234e-05 Validation loss 0.04335940629243851 Accuracy 0.8889999985694885\n",
      "Iteration 17250 Training loss 0.0005622770404443145 Validation loss 0.043370507657527924 Accuracy 0.8889999985694885\n",
      "Iteration 17260 Training loss 0.0018226312240585685 Validation loss 0.043360862880945206 Accuracy 0.8884999752044678\n",
      "Iteration 17270 Training loss 0.00031479159952141345 Validation loss 0.043329864740371704 Accuracy 0.8889999985694885\n",
      "Iteration 17280 Training loss 0.0003172099532093853 Validation loss 0.043311044573783875 Accuracy 0.8889999985694885\n",
      "Iteration 17290 Training loss 0.0005638989969156682 Validation loss 0.04331010580062866 Accuracy 0.8895000219345093\n",
      "Iteration 17300 Training loss 0.0020642688032239676 Validation loss 0.043318744748830795 Accuracy 0.8895000219345093\n",
      "Iteration 17310 Training loss 0.0008156939875334501 Validation loss 0.04336155578494072 Accuracy 0.8884999752044678\n",
      "Iteration 17320 Training loss 0.0008150211069732904 Validation loss 0.043351463973522186 Accuracy 0.8889999985694885\n",
      "Iteration 17330 Training loss 0.0003236107877455652 Validation loss 0.04334519803524017 Accuracy 0.8884999752044678\n",
      "Iteration 17340 Training loss 0.000566651753615588 Validation loss 0.04334324970841408 Accuracy 0.8884999752044678\n",
      "Iteration 17350 Training loss 0.0005649106460623443 Validation loss 0.0433477945625782 Accuracy 0.8884999752044678\n",
      "Iteration 17360 Training loss 0.0003214593161828816 Validation loss 0.043314144015312195 Accuracy 0.8889999985694885\n",
      "Iteration 17370 Training loss 0.0005729255499318242 Validation loss 0.04332302138209343 Accuracy 0.8889999985694885\n",
      "Iteration 17380 Training loss 0.0005670455866493285 Validation loss 0.043358877301216125 Accuracy 0.8884999752044678\n",
      "Iteration 17390 Training loss 0.001310658873990178 Validation loss 0.04335054010152817 Accuracy 0.8889999985694885\n",
      "Iteration 17400 Training loss 0.0010662754066288471 Validation loss 0.043345775455236435 Accuracy 0.8895000219345093\n",
      "Iteration 17410 Training loss 0.0005643271142616868 Validation loss 0.04334786534309387 Accuracy 0.8889999985694885\n",
      "Iteration 17420 Training loss 0.0010701216524466872 Validation loss 0.04332233965396881 Accuracy 0.8895000219345093\n",
      "Iteration 17430 Training loss 0.0013176831416785717 Validation loss 0.043363023549318314 Accuracy 0.8884999752044678\n",
      "Iteration 17440 Training loss 6.62524689687416e-05 Validation loss 0.04334051534533501 Accuracy 0.8889999985694885\n",
      "Iteration 17450 Training loss 0.0003170375421177596 Validation loss 0.04336365684866905 Accuracy 0.8889999985694885\n",
      "Iteration 17460 Training loss 0.001565376645885408 Validation loss 0.04333595559000969 Accuracy 0.8884999752044678\n",
      "Iteration 17470 Training loss 0.0005664693308062851 Validation loss 0.043341636657714844 Accuracy 0.8895000219345093\n",
      "Iteration 17480 Training loss 0.0005684355273842812 Validation loss 0.043363943696022034 Accuracy 0.8889999985694885\n",
      "Iteration 17490 Training loss 0.0005652002291753888 Validation loss 0.04333258047699928 Accuracy 0.8889999985694885\n",
      "Iteration 17500 Training loss 0.0010734316892921925 Validation loss 0.04333071410655975 Accuracy 0.8895000219345093\n",
      "Iteration 17510 Training loss 0.0008168193744495511 Validation loss 0.04332513362169266 Accuracy 0.8889999985694885\n",
      "Iteration 17520 Training loss 0.0008162651211023331 Validation loss 0.04335661977529526 Accuracy 0.8889999985694885\n",
      "Iteration 17530 Training loss 0.0015675753820687532 Validation loss 0.043372102081775665 Accuracy 0.8889999985694885\n",
      "Iteration 17540 Training loss 0.0005664745112881064 Validation loss 0.04335569217801094 Accuracy 0.8895000219345093\n",
      "Iteration 17550 Training loss 0.0018154395511373878 Validation loss 0.04333158954977989 Accuracy 0.8889999985694885\n",
      "Iteration 17560 Training loss 0.0013125910190865397 Validation loss 0.04336974397301674 Accuracy 0.8889999985694885\n",
      "Iteration 17570 Training loss 0.00132128712721169 Validation loss 0.04329516366124153 Accuracy 0.8884999752044678\n",
      "Iteration 17580 Training loss 0.000566607341170311 Validation loss 0.043359484523534775 Accuracy 0.8880000114440918\n",
      "Iteration 17590 Training loss 0.0008136425167322159 Validation loss 0.04333239793777466 Accuracy 0.8884999752044678\n",
      "Iteration 17600 Training loss 0.0010642457054927945 Validation loss 0.04328242316842079 Accuracy 0.8889999985694885\n",
      "Iteration 17610 Training loss 0.0015670561697334051 Validation loss 0.043307311832904816 Accuracy 0.8889999985694885\n",
      "Iteration 17620 Training loss 0.0010651646880432963 Validation loss 0.04335416480898857 Accuracy 0.8884999752044678\n",
      "Iteration 17630 Training loss 0.0003174121957272291 Validation loss 0.043370772153139114 Accuracy 0.8884999752044678\n",
      "Iteration 17640 Training loss 0.0008139184792526066 Validation loss 0.04333703592419624 Accuracy 0.8899999856948853\n",
      "Iteration 17650 Training loss 0.00031676614889875054 Validation loss 0.0433356948196888 Accuracy 0.8889999985694885\n",
      "Iteration 17660 Training loss 0.0010678288526833057 Validation loss 0.04333333298563957 Accuracy 0.8880000114440918\n",
      "Iteration 17670 Training loss 0.001067801029421389 Validation loss 0.04333270341157913 Accuracy 0.8889999985694885\n",
      "Iteration 17680 Training loss 0.0010683424770832062 Validation loss 0.043323878198862076 Accuracy 0.8895000219345093\n",
      "Iteration 17690 Training loss 0.0008193604880943894 Validation loss 0.04331563413143158 Accuracy 0.8895000219345093\n",
      "Iteration 17700 Training loss 0.0008169325301423669 Validation loss 0.04329340159893036 Accuracy 0.8889999985694885\n",
      "Iteration 17710 Training loss 0.0005619610310532153 Validation loss 0.043293822556734085 Accuracy 0.8895000219345093\n",
      "Iteration 17720 Training loss 0.0010648773750290275 Validation loss 0.043312400579452515 Accuracy 0.8884999752044678\n",
      "Iteration 17730 Training loss 0.0010664508445188403 Validation loss 0.0432908833026886 Accuracy 0.8884999752044678\n",
      "Iteration 17740 Training loss 0.0015653589507564902 Validation loss 0.04333383962512016 Accuracy 0.8884999752044678\n",
      "Iteration 17750 Training loss 0.0008192275417968631 Validation loss 0.043338775634765625 Accuracy 0.8889999985694885\n",
      "Iteration 17760 Training loss 0.0003135894367005676 Validation loss 0.04330909624695778 Accuracy 0.8899999856948853\n",
      "Iteration 17770 Training loss 0.000564181711524725 Validation loss 0.043327219784259796 Accuracy 0.8895000219345093\n",
      "Iteration 17780 Training loss 0.0010688763577491045 Validation loss 0.04330642148852348 Accuracy 0.8889999985694885\n",
      "Iteration 17790 Training loss 0.0010641062399372458 Validation loss 0.043337684124708176 Accuracy 0.8880000114440918\n",
      "Iteration 17800 Training loss 0.0015614990843459964 Validation loss 0.04331028833985329 Accuracy 0.8889999985694885\n",
      "Iteration 17810 Training loss 0.0005664026248268783 Validation loss 0.04327006638050079 Accuracy 0.8884999752044678\n",
      "Iteration 17820 Training loss 0.0010699965059757233 Validation loss 0.04327462613582611 Accuracy 0.8884999752044678\n",
      "Iteration 17830 Training loss 0.0013202715199440718 Validation loss 0.04331732913851738 Accuracy 0.887499988079071\n",
      "Iteration 17840 Training loss 6.693560862913728e-05 Validation loss 0.04334288090467453 Accuracy 0.8880000114440918\n",
      "Iteration 17850 Training loss 0.0005642051110044122 Validation loss 0.04333193227648735 Accuracy 0.8889999985694885\n",
      "Iteration 17860 Training loss 0.0005634124390780926 Validation loss 0.04329242929816246 Accuracy 0.8889999985694885\n",
      "Iteration 17870 Training loss 0.0010643493151292205 Validation loss 0.04329129680991173 Accuracy 0.8889999985694885\n",
      "Iteration 17880 Training loss 0.0005659376038238406 Validation loss 0.043327003717422485 Accuracy 0.887499988079071\n",
      "Iteration 17890 Training loss 0.0003173595468979329 Validation loss 0.04329521209001541 Accuracy 0.8895000219345093\n",
      "Iteration 17900 Training loss 0.0013154286425560713 Validation loss 0.04329420253634453 Accuracy 0.8884999752044678\n",
      "Iteration 17910 Training loss 0.0008125326130539179 Validation loss 0.043253906071186066 Accuracy 0.8889999985694885\n",
      "Iteration 17920 Training loss 0.0005641710595227778 Validation loss 0.04330381378531456 Accuracy 0.8889999985694885\n",
      "Iteration 17930 Training loss 0.0010619095992296934 Validation loss 0.04332961142063141 Accuracy 0.8889999985694885\n",
      "Iteration 17940 Training loss 0.0013195418287068605 Validation loss 0.04327419772744179 Accuracy 0.8895000219345093\n",
      "Iteration 17950 Training loss 6.284459232119843e-05 Validation loss 0.04329630360007286 Accuracy 0.8889999985694885\n",
      "Iteration 17960 Training loss 0.0005685617215931416 Validation loss 0.04328383877873421 Accuracy 0.8889999985694885\n",
      "Iteration 17970 Training loss 0.0018178208265453577 Validation loss 0.04328141361474991 Accuracy 0.8889999985694885\n",
      "Iteration 17980 Training loss 0.0005636903224512935 Validation loss 0.04329552501440048 Accuracy 0.8889999985694885\n",
      "Iteration 17990 Training loss 0.0018178641330450773 Validation loss 0.043323200196027756 Accuracy 0.8884999752044678\n",
      "Iteration 18000 Training loss 0.0010728025808930397 Validation loss 0.043331846594810486 Accuracy 0.8884999752044678\n",
      "Iteration 18010 Training loss 0.0008108293986879289 Validation loss 0.04326988756656647 Accuracy 0.8895000219345093\n",
      "Iteration 18020 Training loss 0.0010675573721528053 Validation loss 0.04329952597618103 Accuracy 0.8880000114440918\n",
      "Iteration 18030 Training loss 0.0013204279821366072 Validation loss 0.04330642521381378 Accuracy 0.8884999752044678\n",
      "Iteration 18040 Training loss 0.0005704355426132679 Validation loss 0.04334745183587074 Accuracy 0.887499988079071\n",
      "Iteration 18050 Training loss 0.0003233531897421926 Validation loss 0.04332055523991585 Accuracy 0.8884999752044678\n",
      "Iteration 18060 Training loss 0.0013140334049239755 Validation loss 0.04331290349364281 Accuracy 0.8889999985694885\n",
      "Iteration 18070 Training loss 0.0010640732944011688 Validation loss 0.043331652879714966 Accuracy 0.887499988079071\n",
      "Iteration 18080 Training loss 0.001314583932980895 Validation loss 0.043367911130189896 Accuracy 0.8884999752044678\n",
      "Iteration 18090 Training loss 0.0008179329452104867 Validation loss 0.04334156587719917 Accuracy 0.8880000114440918\n",
      "Iteration 18100 Training loss 0.0008186707273125648 Validation loss 0.04330657050013542 Accuracy 0.8889999985694885\n",
      "Iteration 18110 Training loss 0.0013194967759773135 Validation loss 0.04331575706601143 Accuracy 0.8884999752044678\n",
      "Iteration 18120 Training loss 0.0010707346955314279 Validation loss 0.04330994188785553 Accuracy 0.8880000114440918\n",
      "Iteration 18130 Training loss 0.0013219700194895267 Validation loss 0.0432882197201252 Accuracy 0.8880000114440918\n",
      "Iteration 18140 Training loss 0.0008166213519871235 Validation loss 0.04332195967435837 Accuracy 0.887499988079071\n",
      "Iteration 18150 Training loss 0.0003172369033563882 Validation loss 0.043303389102220535 Accuracy 0.887499988079071\n",
      "Iteration 18160 Training loss 0.0010684359585866332 Validation loss 0.043288055807352066 Accuracy 0.8880000114440918\n",
      "Iteration 18170 Training loss 0.0008152960799634457 Validation loss 0.04327632859349251 Accuracy 0.8889999985694885\n",
      "Iteration 18180 Training loss 0.0008165896870195866 Validation loss 0.04331698641180992 Accuracy 0.8884999752044678\n",
      "Iteration 18190 Training loss 0.000319363665767014 Validation loss 0.043332938104867935 Accuracy 0.8884999752044678\n",
      "Iteration 18200 Training loss 0.001320944051258266 Validation loss 0.043306730687618256 Accuracy 0.887499988079071\n",
      "Iteration 18210 Training loss 0.0005644534830935299 Validation loss 0.043327976018190384 Accuracy 0.8884999752044678\n",
      "Iteration 18220 Training loss 0.0013190971221774817 Validation loss 0.043304719030857086 Accuracy 0.887499988079071\n",
      "Iteration 18230 Training loss 0.0008236512658186257 Validation loss 0.04330669716000557 Accuracy 0.8884999752044678\n",
      "Iteration 18240 Training loss 0.0005713714635930955 Validation loss 0.04329561069607735 Accuracy 0.8884999752044678\n",
      "Iteration 18250 Training loss 0.001065563876181841 Validation loss 0.04326965659856796 Accuracy 0.8889999985694885\n",
      "Iteration 18260 Training loss 0.0008211797685362399 Validation loss 0.04327944293618202 Accuracy 0.8884999752044678\n",
      "Iteration 18270 Training loss 0.0013154923217371106 Validation loss 0.043292928487062454 Accuracy 0.8880000114440918\n",
      "Iteration 18280 Training loss 0.001072789658792317 Validation loss 0.043281618505716324 Accuracy 0.8884999752044678\n",
      "Iteration 18290 Training loss 0.0010669424664229155 Validation loss 0.043311964720487595 Accuracy 0.8884999752044678\n",
      "Iteration 18300 Training loss 0.0008192893583327532 Validation loss 0.043317098170518875 Accuracy 0.8884999752044678\n",
      "Iteration 18310 Training loss 0.0005697472370229661 Validation loss 0.04331597313284874 Accuracy 0.887499988079071\n",
      "Iteration 18320 Training loss 0.00106746144592762 Validation loss 0.04331151396036148 Accuracy 0.8884999752044678\n",
      "Iteration 18330 Training loss 0.0003157254832331091 Validation loss 0.04330366477370262 Accuracy 0.8884999752044678\n",
      "Iteration 18340 Training loss 0.0010685417801141739 Validation loss 0.043336786329746246 Accuracy 0.8884999752044678\n",
      "Iteration 18350 Training loss 0.0008195376140065491 Validation loss 0.04330359026789665 Accuracy 0.8884999752044678\n",
      "Iteration 18360 Training loss 0.0003159009211231023 Validation loss 0.04329105466604233 Accuracy 0.8889999985694885\n",
      "Iteration 18370 Training loss 0.0005669926758855581 Validation loss 0.04329867660999298 Accuracy 0.8880000114440918\n",
      "Iteration 18380 Training loss 0.0010646095033735037 Validation loss 0.04329536110162735 Accuracy 0.8889999985694885\n",
      "Iteration 18390 Training loss 0.0008205856429412961 Validation loss 0.043293487280607224 Accuracy 0.8884999752044678\n",
      "Iteration 18400 Training loss 0.0003165877715218812 Validation loss 0.04328729957342148 Accuracy 0.8880000114440918\n",
      "Iteration 18410 Training loss 0.0010695309611037374 Validation loss 0.04325638711452484 Accuracy 0.8889999985694885\n",
      "Iteration 18420 Training loss 0.00031634996412321925 Validation loss 0.043284524232149124 Accuracy 0.8880000114440918\n",
      "Iteration 18430 Training loss 0.0003203093947377056 Validation loss 0.04328951984643936 Accuracy 0.8880000114440918\n",
      "Iteration 18440 Training loss 0.0008154292008839548 Validation loss 0.043289098888635635 Accuracy 0.8884999752044678\n",
      "Iteration 18450 Training loss 0.0008201600867323577 Validation loss 0.04330296069383621 Accuracy 0.8880000114440918\n",
      "Iteration 18460 Training loss 0.0010737709235399961 Validation loss 0.04329485446214676 Accuracy 0.8884999752044678\n",
      "Iteration 18470 Training loss 0.0008190901717171073 Validation loss 0.04328795522451401 Accuracy 0.887499988079071\n",
      "Iteration 18480 Training loss 0.0010672827484086156 Validation loss 0.043290868401527405 Accuracy 0.8884999752044678\n",
      "Iteration 18490 Training loss 0.0008120735874399543 Validation loss 0.043289847671985626 Accuracy 0.8884999752044678\n",
      "Iteration 18500 Training loss 0.0008234928245656192 Validation loss 0.04327935352921486 Accuracy 0.8884999752044678\n",
      "Iteration 18510 Training loss 0.0003155359881930053 Validation loss 0.043278057128190994 Accuracy 0.8884999752044678\n",
      "Iteration 18520 Training loss 0.0005694743595086038 Validation loss 0.04330024495720863 Accuracy 0.887499988079071\n",
      "Iteration 18530 Training loss 0.0010694981319829822 Validation loss 0.043309539556503296 Accuracy 0.8884999752044678\n",
      "Iteration 18540 Training loss 0.0010686932364478707 Validation loss 0.04328222945332527 Accuracy 0.8880000114440918\n",
      "Iteration 18550 Training loss 0.0008200562442652881 Validation loss 0.043295737355947495 Accuracy 0.8880000114440918\n",
      "Iteration 18560 Training loss 0.0003120523761026561 Validation loss 0.04327556863427162 Accuracy 0.8884999752044678\n",
      "Iteration 18570 Training loss 0.0010714387753978372 Validation loss 0.04330988973379135 Accuracy 0.8880000114440918\n",
      "Iteration 18580 Training loss 0.001817205105908215 Validation loss 0.043280649930238724 Accuracy 0.8880000114440918\n",
      "Iteration 18590 Training loss 0.0008152609807439148 Validation loss 0.043296363204717636 Accuracy 0.8880000114440918\n",
      "Iteration 18600 Training loss 0.0008134777308441699 Validation loss 0.043266091495752335 Accuracy 0.8880000114440918\n",
      "Iteration 18610 Training loss 0.0005680789472535253 Validation loss 0.04326191917061806 Accuracy 0.8880000114440918\n",
      "Iteration 18620 Training loss 0.0013163078110665083 Validation loss 0.04327021911740303 Accuracy 0.8880000114440918\n",
      "Iteration 18630 Training loss 0.001070338417775929 Validation loss 0.04327735677361488 Accuracy 0.8880000114440918\n",
      "Iteration 18640 Training loss 0.0013177675427868962 Validation loss 0.043292831629514694 Accuracy 0.887499988079071\n",
      "Iteration 18650 Training loss 0.0008209304651245475 Validation loss 0.043257925659418106 Accuracy 0.8884999752044678\n",
      "Iteration 18660 Training loss 0.0010695498203858733 Validation loss 0.04325580224394798 Accuracy 0.8880000114440918\n",
      "Iteration 18670 Training loss 0.0010661600390449166 Validation loss 0.04327070713043213 Accuracy 0.8880000114440918\n",
      "Iteration 18680 Training loss 0.0010649992618709803 Validation loss 0.04328875616192818 Accuracy 0.8880000114440918\n",
      "Iteration 18690 Training loss 0.0008170960936695337 Validation loss 0.04324325546622276 Accuracy 0.8880000114440918\n",
      "Iteration 18700 Training loss 0.0010678762337192893 Validation loss 0.04324769601225853 Accuracy 0.8889999985694885\n",
      "Iteration 18710 Training loss 0.000569271098356694 Validation loss 0.04324576258659363 Accuracy 0.8884999752044678\n",
      "Iteration 18720 Training loss 0.000812286976724863 Validation loss 0.04324214160442352 Accuracy 0.8880000114440918\n",
      "Iteration 18730 Training loss 0.0008143169688992202 Validation loss 0.04329782724380493 Accuracy 0.887499988079071\n",
      "Iteration 18740 Training loss 0.0003160071501042694 Validation loss 0.04327392578125 Accuracy 0.8880000114440918\n",
      "Iteration 18750 Training loss 0.0008233527769334614 Validation loss 0.043292686343193054 Accuracy 0.8884999752044678\n",
      "Iteration 18760 Training loss 0.00031963305082172155 Validation loss 0.043240245431661606 Accuracy 0.8884999752044678\n",
      "Iteration 18770 Training loss 0.0005667617660947144 Validation loss 0.04329806566238403 Accuracy 0.887499988079071\n",
      "Iteration 18780 Training loss 0.0010673695942386985 Validation loss 0.043256547302007675 Accuracy 0.8889999985694885\n",
      "Iteration 18790 Training loss 0.0005647906800732017 Validation loss 0.04324609786272049 Accuracy 0.8880000114440918\n",
      "Iteration 18800 Training loss 0.0005691532860510051 Validation loss 0.043291132897138596 Accuracy 0.8880000114440918\n",
      "Iteration 18810 Training loss 0.0010705140884965658 Validation loss 0.043265730142593384 Accuracy 0.8865000009536743\n",
      "Iteration 18820 Training loss 6.505857163574547e-05 Validation loss 0.04324128478765488 Accuracy 0.8880000114440918\n",
      "Iteration 18830 Training loss 0.0008179743308573961 Validation loss 0.04322686791419983 Accuracy 0.8870000243186951\n",
      "Iteration 18840 Training loss 0.0013178946683183312 Validation loss 0.043286558240652084 Accuracy 0.8880000114440918\n",
      "Iteration 18850 Training loss 0.0008147529442794621 Validation loss 0.043241530656814575 Accuracy 0.887499988079071\n",
      "Iteration 18860 Training loss 0.0005656796274706721 Validation loss 0.04326053336262703 Accuracy 0.8880000114440918\n",
      "Iteration 18870 Training loss 0.0005639776936732233 Validation loss 0.04326257482171059 Accuracy 0.8880000114440918\n",
      "Iteration 18880 Training loss 0.001069718855433166 Validation loss 0.04323115199804306 Accuracy 0.8880000114440918\n",
      "Iteration 18890 Training loss 0.001305974437855184 Validation loss 0.043279778212308884 Accuracy 0.8880000114440918\n",
      "Iteration 18900 Training loss 0.0010626125149428844 Validation loss 0.04329555481672287 Accuracy 0.8880000114440918\n",
      "Iteration 18910 Training loss 0.0003189831622876227 Validation loss 0.043265409767627716 Accuracy 0.8880000114440918\n",
      "Iteration 18920 Training loss 0.0005616667913272977 Validation loss 0.04331785440444946 Accuracy 0.8870000243186951\n",
      "Iteration 18930 Training loss 0.0015673560556024313 Validation loss 0.04327855631709099 Accuracy 0.887499988079071\n",
      "Iteration 18940 Training loss 0.0005727123934775591 Validation loss 0.04327571019530296 Accuracy 0.8880000114440918\n",
      "Iteration 18950 Training loss 0.0010693039512261748 Validation loss 0.04327346384525299 Accuracy 0.887499988079071\n",
      "Iteration 18960 Training loss 0.0008181847515515983 Validation loss 0.043233878910541534 Accuracy 0.8884999752044678\n",
      "Iteration 18970 Training loss 0.001063419389538467 Validation loss 0.04325813427567482 Accuracy 0.887499988079071\n",
      "Iteration 18980 Training loss 0.0010675895027816296 Validation loss 0.043273281306028366 Accuracy 0.8880000114440918\n",
      "Iteration 18990 Training loss 0.0005663682240992785 Validation loss 0.04327885061502457 Accuracy 0.887499988079071\n",
      "Iteration 19000 Training loss 0.0010618048254400492 Validation loss 0.043265510350465775 Accuracy 0.887499988079071\n",
      "Iteration 19010 Training loss 0.0010639455867931247 Validation loss 0.04325227439403534 Accuracy 0.8880000114440918\n",
      "Iteration 19020 Training loss 0.0013122432865202427 Validation loss 0.04323925822973251 Accuracy 0.8880000114440918\n",
      "Iteration 19030 Training loss 0.0003244499384891242 Validation loss 0.043234772980213165 Accuracy 0.8880000114440918\n",
      "Iteration 19040 Training loss 0.0008149446221068501 Validation loss 0.04326987639069557 Accuracy 0.8880000114440918\n",
      "Iteration 19050 Training loss 0.0015693682944402099 Validation loss 0.04326547309756279 Accuracy 0.8880000114440918\n",
      "Iteration 19060 Training loss 0.0005688262172043324 Validation loss 0.04327648878097534 Accuracy 0.887499988079071\n",
      "Iteration 19070 Training loss 0.0010684292064979672 Validation loss 0.04323149099946022 Accuracy 0.8884999752044678\n",
      "Iteration 19080 Training loss 0.00031438053702004254 Validation loss 0.04324823245406151 Accuracy 0.8880000114440918\n",
      "Iteration 19090 Training loss 0.0013141229283064604 Validation loss 0.04327009245753288 Accuracy 0.8880000114440918\n",
      "Iteration 19100 Training loss 0.0015695438487455249 Validation loss 0.04328896477818489 Accuracy 0.8880000114440918\n",
      "Iteration 19110 Training loss 0.0005687382072210312 Validation loss 0.04327918216586113 Accuracy 0.8880000114440918\n",
      "Iteration 19120 Training loss 0.0010707048932090402 Validation loss 0.04329074174165726 Accuracy 0.8880000114440918\n",
      "Iteration 19130 Training loss 0.0008204396581277251 Validation loss 0.043260134756565094 Accuracy 0.8880000114440918\n",
      "Iteration 19140 Training loss 0.00032030337024480104 Validation loss 0.04327640309929848 Accuracy 0.887499988079071\n",
      "Iteration 19150 Training loss 0.00031023912015371025 Validation loss 0.04323872551321983 Accuracy 0.887499988079071\n",
      "Iteration 19160 Training loss 0.0013191504403948784 Validation loss 0.043252624571323395 Accuracy 0.8880000114440918\n",
      "Iteration 19170 Training loss 0.0003217637713532895 Validation loss 0.04325873404741287 Accuracy 0.8880000114440918\n",
      "Iteration 19180 Training loss 0.000818671949673444 Validation loss 0.043235670775175095 Accuracy 0.8884999752044678\n",
      "Iteration 19190 Training loss 0.0010632968042045832 Validation loss 0.043198518455028534 Accuracy 0.8889999985694885\n",
      "Iteration 19200 Training loss 0.0010670059127733111 Validation loss 0.04321721941232681 Accuracy 0.8880000114440918\n",
      "Iteration 19210 Training loss 0.000321272702421993 Validation loss 0.043221376836299896 Accuracy 0.8880000114440918\n",
      "Iteration 19220 Training loss 0.0005653342814184725 Validation loss 0.04327119514346123 Accuracy 0.887499988079071\n",
      "Iteration 19230 Training loss 0.0010689669288694859 Validation loss 0.04324279725551605 Accuracy 0.887499988079071\n",
      "Iteration 19240 Training loss 0.00031679749372415245 Validation loss 0.04323102533817291 Accuracy 0.8880000114440918\n",
      "Iteration 19250 Training loss 0.0010648585157468915 Validation loss 0.0432448573410511 Accuracy 0.8880000114440918\n",
      "Iteration 19260 Training loss 0.0008174991817213595 Validation loss 0.0432906374335289 Accuracy 0.8870000243186951\n",
      "Iteration 19270 Training loss 0.0003127098607365042 Validation loss 0.043246787041425705 Accuracy 0.8884999752044678\n",
      "Iteration 19280 Training loss 0.0010676723904907703 Validation loss 0.043253347277641296 Accuracy 0.887499988079071\n",
      "Iteration 19290 Training loss 0.0008190952357836068 Validation loss 0.04321791231632233 Accuracy 0.8884999752044678\n",
      "Iteration 19300 Training loss 0.0008231644169427454 Validation loss 0.043266210705041885 Accuracy 0.8880000114440918\n",
      "Iteration 19310 Training loss 0.001315371715463698 Validation loss 0.043219827115535736 Accuracy 0.8884999752044678\n",
      "Iteration 19320 Training loss 0.0010675443336367607 Validation loss 0.043244387954473495 Accuracy 0.8884999752044678\n",
      "Iteration 19330 Training loss 0.0008194128167815506 Validation loss 0.04326830431818962 Accuracy 0.8889999985694885\n",
      "Iteration 19340 Training loss 0.000812441052403301 Validation loss 0.04328453540802002 Accuracy 0.8884999752044678\n",
      "Iteration 19350 Training loss 0.001316679292358458 Validation loss 0.04326782748103142 Accuracy 0.8889999985694885\n",
      "Iteration 19360 Training loss 0.0010656402446329594 Validation loss 0.04326276108622551 Accuracy 0.8884999752044678\n",
      "Iteration 19370 Training loss 0.0008140587597154081 Validation loss 0.04326434060931206 Accuracy 0.8880000114440918\n",
      "Iteration 19380 Training loss 0.001316926209256053 Validation loss 0.04321977123618126 Accuracy 0.8880000114440918\n",
      "Iteration 19390 Training loss 0.0005667327786795795 Validation loss 0.043263886123895645 Accuracy 0.8880000114440918\n",
      "Iteration 19400 Training loss 0.0010653261560946703 Validation loss 0.04322980344295502 Accuracy 0.8880000114440918\n",
      "Iteration 19410 Training loss 0.0005721933557651937 Validation loss 0.04327770322561264 Accuracy 0.887499988079071\n",
      "Iteration 19420 Training loss 0.0020697808358818293 Validation loss 0.04326656460762024 Accuracy 0.8880000114440918\n",
      "Iteration 19430 Training loss 0.0005680781323462725 Validation loss 0.04322722926735878 Accuracy 0.8884999752044678\n",
      "Iteration 19440 Training loss 0.001059326226823032 Validation loss 0.04328187555074692 Accuracy 0.8880000114440918\n",
      "Iteration 19450 Training loss 0.002069410402327776 Validation loss 0.04324987903237343 Accuracy 0.8880000114440918\n",
      "Iteration 19460 Training loss 0.0010652549099177122 Validation loss 0.04323649778962135 Accuracy 0.8880000114440918\n",
      "Iteration 19470 Training loss 0.0005685289506800473 Validation loss 0.04322671517729759 Accuracy 0.8880000114440918\n",
      "Iteration 19480 Training loss 0.0010630328906700015 Validation loss 0.0432695709168911 Accuracy 0.887499988079071\n",
      "Iteration 19490 Training loss 0.001561834942549467 Validation loss 0.043234389275312424 Accuracy 0.8889999985694885\n",
      "Iteration 19500 Training loss 0.0010668253526091576 Validation loss 0.043225523084402084 Accuracy 0.887499988079071\n",
      "Iteration 19510 Training loss 0.0005698950262740254 Validation loss 0.043214328587055206 Accuracy 0.8884999752044678\n",
      "Iteration 19520 Training loss 0.0008181066368706524 Validation loss 0.043275393545627594 Accuracy 0.8880000114440918\n",
      "Iteration 19530 Training loss 0.0010665092850103974 Validation loss 0.04323184862732887 Accuracy 0.887499988079071\n",
      "Iteration 19540 Training loss 0.0005700970650650561 Validation loss 0.04323280230164528 Accuracy 0.8870000243186951\n",
      "Iteration 19550 Training loss 0.0010671012569218874 Validation loss 0.043212149292230606 Accuracy 0.8880000114440918\n",
      "Iteration 19560 Training loss 0.0008224108605645597 Validation loss 0.04324454814195633 Accuracy 0.887499988079071\n",
      "Iteration 19570 Training loss 0.0010635689832270145 Validation loss 0.043216004967689514 Accuracy 0.887499988079071\n",
      "Iteration 19580 Training loss 0.0010695018572732806 Validation loss 0.04319765418767929 Accuracy 0.887499988079071\n",
      "Iteration 19590 Training loss 0.0013205655850470066 Validation loss 0.0432065911591053 Accuracy 0.887499988079071\n",
      "Iteration 19600 Training loss 0.0005690380930900574 Validation loss 0.04324527457356453 Accuracy 0.8880000114440918\n",
      "Iteration 19610 Training loss 0.0013151774182915688 Validation loss 0.043222058564424515 Accuracy 0.887499988079071\n",
      "Iteration 19620 Training loss 0.00031731114722788334 Validation loss 0.04322023317217827 Accuracy 0.8884999752044678\n",
      "Iteration 19630 Training loss 0.00031837160349823534 Validation loss 0.04322643205523491 Accuracy 0.8880000114440918\n",
      "Iteration 19640 Training loss 0.0008145522442646325 Validation loss 0.04323628544807434 Accuracy 0.8884999752044678\n",
      "Iteration 19650 Training loss 0.0015700379153713584 Validation loss 0.04321017116308212 Accuracy 0.8884999752044678\n",
      "Iteration 19660 Training loss 0.0003205186512786895 Validation loss 0.04325317218899727 Accuracy 0.887499988079071\n",
      "Iteration 19670 Training loss 0.00032150608603842556 Validation loss 0.04319421201944351 Accuracy 0.887499988079071\n",
      "Iteration 19680 Training loss 0.0003158347390126437 Validation loss 0.04327762499451637 Accuracy 0.887499988079071\n",
      "Iteration 19690 Training loss 0.0010742442682385445 Validation loss 0.04321310296654701 Accuracy 0.8884999752044678\n",
      "Iteration 19700 Training loss 0.00056653399951756 Validation loss 0.043261125683784485 Accuracy 0.8880000114440918\n",
      "Iteration 19710 Training loss 0.0015675585018470883 Validation loss 0.0432865172624588 Accuracy 0.8884999752044678\n",
      "Iteration 19720 Training loss 0.0008123675361275673 Validation loss 0.043235767632722855 Accuracy 0.8884999752044678\n",
      "Iteration 19730 Training loss 0.0005669425590895116 Validation loss 0.04318556189537048 Accuracy 0.8880000114440918\n",
      "Iteration 19740 Training loss 0.0010636158986017108 Validation loss 0.043206457048654556 Accuracy 0.8884999752044678\n",
      "Iteration 19750 Training loss 0.00032065631239674985 Validation loss 0.04327695816755295 Accuracy 0.8884999752044678\n",
      "Iteration 19760 Training loss 0.0008249563397839665 Validation loss 0.04320702329277992 Accuracy 0.8889999985694885\n",
      "Iteration 19770 Training loss 0.0003197600890416652 Validation loss 0.04321853816509247 Accuracy 0.8884999752044678\n",
      "Iteration 19780 Training loss 0.001065789139829576 Validation loss 0.043207671493291855 Accuracy 0.8884999752044678\n",
      "Iteration 19790 Training loss 0.0008177395793609321 Validation loss 0.04321076348423958 Accuracy 0.8880000114440918\n",
      "Iteration 19800 Training loss 0.0003190821735188365 Validation loss 0.04319342225790024 Accuracy 0.8889999985694885\n",
      "Iteration 19810 Training loss 0.0008163325837813318 Validation loss 0.04318211227655411 Accuracy 0.8889999985694885\n",
      "Iteration 19820 Training loss 0.0010625271825119853 Validation loss 0.043239325284957886 Accuracy 0.887499988079071\n",
      "Iteration 19830 Training loss 0.0008211269159801304 Validation loss 0.0432562381029129 Accuracy 0.8880000114440918\n",
      "Iteration 19840 Training loss 0.0008174906251952052 Validation loss 0.043241631239652634 Accuracy 0.8880000114440918\n",
      "Iteration 19850 Training loss 0.0010672749485820532 Validation loss 0.04323963075876236 Accuracy 0.8880000114440918\n",
      "Iteration 19860 Training loss 0.0008161379955708981 Validation loss 0.04317421838641167 Accuracy 0.8884999752044678\n",
      "Iteration 19870 Training loss 0.0008194709662348032 Validation loss 0.04325060918927193 Accuracy 0.8880000114440918\n",
      "Iteration 19880 Training loss 0.00032021611696109176 Validation loss 0.04323127865791321 Accuracy 0.8884999752044678\n",
      "Iteration 19890 Training loss 0.0013159889495000243 Validation loss 0.043228089809417725 Accuracy 0.8880000114440918\n",
      "Iteration 19900 Training loss 0.0005665667704306543 Validation loss 0.043240781873464584 Accuracy 0.8880000114440918\n",
      "Iteration 19910 Training loss 0.0008179332944564521 Validation loss 0.043199848383665085 Accuracy 0.8884999752044678\n",
      "Iteration 19920 Training loss 0.0003178698825649917 Validation loss 0.04321106895804405 Accuracy 0.8880000114440918\n",
      "Iteration 19930 Training loss 0.0008142567821778357 Validation loss 0.04324408620595932 Accuracy 0.8884999752044678\n",
      "Iteration 19940 Training loss 0.0005642389296554029 Validation loss 0.04318669065833092 Accuracy 0.8884999752044678\n",
      "Iteration 19950 Training loss 0.000813398917671293 Validation loss 0.04325464740395546 Accuracy 0.8884999752044678\n",
      "Iteration 19960 Training loss 0.0008135398966260254 Validation loss 0.043248116970062256 Accuracy 0.8884999752044678\n",
      "Iteration 19970 Training loss 6.981279148021713e-05 Validation loss 0.04325195401906967 Accuracy 0.8884999752044678\n",
      "Iteration 19980 Training loss 0.0008116358076222241 Validation loss 0.043224088847637177 Accuracy 0.8884999752044678\n",
      "Iteration 19990 Training loss 0.0015653885202482343 Validation loss 0.0432058684527874 Accuracy 0.8880000114440918\n",
      "Iteration 20000 Training loss 0.0008202543249353766 Validation loss 0.04319348186254501 Accuracy 0.8880000114440918\n",
      "Iteration 20010 Training loss 0.0015673446469008923 Validation loss 0.04323757812380791 Accuracy 0.8880000114440918\n",
      "Iteration 20020 Training loss 0.00032110349275171757 Validation loss 0.043181538581848145 Accuracy 0.8880000114440918\n",
      "Iteration 20030 Training loss 0.0005655409768223763 Validation loss 0.04320012032985687 Accuracy 0.8880000114440918\n",
      "Iteration 20040 Training loss 6.90078450134024e-05 Validation loss 0.0432012565433979 Accuracy 0.8880000114440918\n",
      "Iteration 20050 Training loss 0.0013131428277119994 Validation loss 0.04323199391365051 Accuracy 0.8880000114440918\n",
      "Iteration 20060 Training loss 0.0015690246364101768 Validation loss 0.04318566247820854 Accuracy 0.8880000114440918\n",
      "Iteration 20070 Training loss 0.0005746243987232447 Validation loss 0.04318951442837715 Accuracy 0.8880000114440918\n",
      "Iteration 20080 Training loss 0.0010679932311177254 Validation loss 0.04319913312792778 Accuracy 0.887499988079071\n",
      "Iteration 20090 Training loss 0.0013164187548682094 Validation loss 0.04319065809249878 Accuracy 0.8884999752044678\n",
      "Iteration 20100 Training loss 0.0013148663565516472 Validation loss 0.04318459704518318 Accuracy 0.8884999752044678\n",
      "Iteration 20110 Training loss 0.0013252587523311377 Validation loss 0.04317944124341011 Accuracy 0.8880000114440918\n",
      "Iteration 20120 Training loss 0.0003190807765349746 Validation loss 0.043205637484788895 Accuracy 0.8880000114440918\n",
      "Iteration 20130 Training loss 0.0005696510197594762 Validation loss 0.043217506259679794 Accuracy 0.8880000114440918\n",
      "Iteration 20140 Training loss 0.0008151190122589469 Validation loss 0.043158479034900665 Accuracy 0.8884999752044678\n",
      "Iteration 20150 Training loss 0.0008178710122592747 Validation loss 0.043198078870773315 Accuracy 0.8880000114440918\n",
      "Iteration 20160 Training loss 0.001068310346454382 Validation loss 0.04321693256497383 Accuracy 0.8884999752044678\n",
      "Iteration 20170 Training loss 0.0010675141820684075 Validation loss 0.04320497438311577 Accuracy 0.8884999752044678\n",
      "Iteration 20180 Training loss 0.0003186565882060677 Validation loss 0.04318389669060707 Accuracy 0.8889999985694885\n",
      "Iteration 20190 Training loss 0.0008161375881172717 Validation loss 0.04322898015379906 Accuracy 0.8880000114440918\n",
      "Iteration 20200 Training loss 0.000820110784843564 Validation loss 0.04321259632706642 Accuracy 0.8880000114440918\n",
      "Iteration 20210 Training loss 0.001569837797433138 Validation loss 0.04315507411956787 Accuracy 0.8884999752044678\n",
      "Iteration 20220 Training loss 0.0008134312811307609 Validation loss 0.043186843395233154 Accuracy 0.8880000114440918\n",
      "Iteration 20230 Training loss 0.0015679686330258846 Validation loss 0.04319899156689644 Accuracy 0.8880000114440918\n",
      "Iteration 20240 Training loss 0.0008198003051802516 Validation loss 0.04319816827774048 Accuracy 0.8880000114440918\n",
      "Iteration 20250 Training loss 0.0010671854251995683 Validation loss 0.04321463406085968 Accuracy 0.8884999752044678\n",
      "Iteration 20260 Training loss 0.0008159434655681252 Validation loss 0.04319734126329422 Accuracy 0.887499988079071\n",
      "Iteration 20270 Training loss 0.0018147008959203959 Validation loss 0.043231286108493805 Accuracy 0.8880000114440918\n",
      "Iteration 20280 Training loss 0.00032008119160309434 Validation loss 0.043197065591812134 Accuracy 0.8884999752044678\n",
      "Iteration 20290 Training loss 0.0013169717276468873 Validation loss 0.04320976510643959 Accuracy 0.8889999985694885\n",
      "Iteration 20300 Training loss 0.0010703845182433724 Validation loss 0.043191514909267426 Accuracy 0.8884999752044678\n",
      "Iteration 20310 Training loss 0.0005692218546755612 Validation loss 0.04322509095072746 Accuracy 0.8880000114440918\n",
      "Iteration 20320 Training loss 0.0013152025640010834 Validation loss 0.04322851821780205 Accuracy 0.8880000114440918\n",
      "Iteration 20330 Training loss 0.0005664138006977737 Validation loss 0.04322235658764839 Accuracy 0.8884999752044678\n",
      "Iteration 20340 Training loss 0.0013184227282181382 Validation loss 0.04320840537548065 Accuracy 0.8884999752044678\n",
      "Iteration 20350 Training loss 0.0005720533081330359 Validation loss 0.04322550818324089 Accuracy 0.8880000114440918\n",
      "Iteration 20360 Training loss 0.001317704445682466 Validation loss 0.043183524161577225 Accuracy 0.8880000114440918\n",
      "Iteration 20370 Training loss 0.0010631659533828497 Validation loss 0.043196942657232285 Accuracy 0.8884999752044678\n",
      "Iteration 20380 Training loss 0.0010641028638929129 Validation loss 0.04323447495698929 Accuracy 0.887499988079071\n",
      "Iteration 20390 Training loss 0.0003138465399388224 Validation loss 0.043137405067682266 Accuracy 0.8884999752044678\n",
      "Iteration 20400 Training loss 0.0005672374973073602 Validation loss 0.04318315535783768 Accuracy 0.8880000114440918\n",
      "Iteration 20410 Training loss 0.0005672966362908483 Validation loss 0.04317757487297058 Accuracy 0.8880000114440918\n",
      "Iteration 20420 Training loss 0.0008173334063030779 Validation loss 0.04320209473371506 Accuracy 0.8880000114440918\n",
      "Iteration 20430 Training loss 0.000315607525408268 Validation loss 0.043185167014598846 Accuracy 0.8884999752044678\n",
      "Iteration 20440 Training loss 0.000313158321660012 Validation loss 0.04318271577358246 Accuracy 0.8884999752044678\n",
      "Iteration 20450 Training loss 0.0013153760228306055 Validation loss 0.04317038133740425 Accuracy 0.8880000114440918\n",
      "Iteration 20460 Training loss 0.0013190441532060504 Validation loss 0.04316460341215134 Accuracy 0.8884999752044678\n",
      "Iteration 20470 Training loss 6.213528831722215e-05 Validation loss 0.043220773339271545 Accuracy 0.8884999752044678\n",
      "Iteration 20480 Training loss 0.0010677512036636472 Validation loss 0.04321333020925522 Accuracy 0.8884999752044678\n",
      "Iteration 20490 Training loss 0.0008154836832545698 Validation loss 0.04320749267935753 Accuracy 0.8880000114440918\n",
      "Iteration 20500 Training loss 0.0008207245264202356 Validation loss 0.04315609112381935 Accuracy 0.887499988079071\n",
      "Iteration 20510 Training loss 0.0013190658064559102 Validation loss 0.04318423569202423 Accuracy 0.8884999752044678\n",
      "Iteration 20520 Training loss 0.0010671322233974934 Validation loss 0.0431845486164093 Accuracy 0.8880000114440918\n",
      "Iteration 20530 Training loss 0.001567095285281539 Validation loss 0.04317732900381088 Accuracy 0.8880000114440918\n",
      "Iteration 20540 Training loss 0.0013179398374632 Validation loss 0.043137211352586746 Accuracy 0.8880000114440918\n",
      "Iteration 20550 Training loss 0.0008130657952278852 Validation loss 0.043183017522096634 Accuracy 0.8880000114440918\n",
      "Iteration 20560 Training loss 0.0008171094232238829 Validation loss 0.043206047266721725 Accuracy 0.8880000114440918\n",
      "Iteration 20570 Training loss 0.0005690456600859761 Validation loss 0.04317178949713707 Accuracy 0.887499988079071\n",
      "Iteration 20580 Training loss 0.0003190215793438256 Validation loss 0.04319333657622337 Accuracy 0.8884999752044678\n",
      "Iteration 20590 Training loss 0.0003131242119707167 Validation loss 0.04317415878176689 Accuracy 0.8889999985694885\n",
      "Iteration 20600 Training loss 0.0013187043368816376 Validation loss 0.04320618137717247 Accuracy 0.8880000114440918\n",
      "Iteration 20610 Training loss 0.000569009396713227 Validation loss 0.04319388419389725 Accuracy 0.8880000114440918\n",
      "Iteration 20620 Training loss 0.0010674576042219996 Validation loss 0.04321237653493881 Accuracy 0.8880000114440918\n",
      "Iteration 20630 Training loss 0.0010672338539734483 Validation loss 0.04322928935289383 Accuracy 0.8880000114440918\n",
      "Iteration 20640 Training loss 0.0005712246638722718 Validation loss 0.04320121929049492 Accuracy 0.8880000114440918\n",
      "Iteration 20650 Training loss 0.0015676827169954777 Validation loss 0.043224480003118515 Accuracy 0.8880000114440918\n",
      "Iteration 20660 Training loss 0.0008209299994632602 Validation loss 0.04317992553114891 Accuracy 0.8889999985694885\n",
      "Iteration 20670 Training loss 0.0008235517889261246 Validation loss 0.04318256303668022 Accuracy 0.8880000114440918\n",
      "Iteration 20680 Training loss 0.0005679321475327015 Validation loss 0.0432044118642807 Accuracy 0.8889999985694885\n",
      "Iteration 20690 Training loss 0.001066809520125389 Validation loss 0.043233323842287064 Accuracy 0.8884999752044678\n",
      "Iteration 20700 Training loss 0.000563507026527077 Validation loss 0.04318269342184067 Accuracy 0.8884999752044678\n",
      "Iteration 20710 Training loss 0.0013192708138376474 Validation loss 0.04323577135801315 Accuracy 0.8880000114440918\n",
      "Iteration 20720 Training loss 0.001073134015314281 Validation loss 0.0432082861661911 Accuracy 0.8884999752044678\n",
      "Iteration 20730 Training loss 0.0008135398384183645 Validation loss 0.04319000616669655 Accuracy 0.8884999752044678\n",
      "Iteration 20740 Training loss 0.0010664737783372402 Validation loss 0.0431584008038044 Accuracy 0.8889999985694885\n",
      "Iteration 20750 Training loss 0.0010667056776583195 Validation loss 0.043188728392124176 Accuracy 0.8884999752044678\n",
      "Iteration 20760 Training loss 0.0003166034002788365 Validation loss 0.0431637279689312 Accuracy 0.8884999752044678\n",
      "Iteration 20770 Training loss 0.001069741672836244 Validation loss 0.04319781810045242 Accuracy 0.8884999752044678\n",
      "Iteration 20780 Training loss 0.0008151056827045977 Validation loss 0.04319789260625839 Accuracy 0.8884999752044678\n",
      "Iteration 20790 Training loss 0.0005667313234880567 Validation loss 0.04318291321396828 Accuracy 0.8880000114440918\n",
      "Iteration 20800 Training loss 0.0005669298116117716 Validation loss 0.04316986724734306 Accuracy 0.8880000114440918\n",
      "Iteration 20810 Training loss 0.0010656655067577958 Validation loss 0.04318951070308685 Accuracy 0.8884999752044678\n",
      "Iteration 20820 Training loss 0.001069317222572863 Validation loss 0.043225813657045364 Accuracy 0.8884999752044678\n",
      "Iteration 20830 Training loss 0.001068347366526723 Validation loss 0.04324479028582573 Accuracy 0.8884999752044678\n",
      "Iteration 20840 Training loss 0.0013181385584175587 Validation loss 0.043203797191381454 Accuracy 0.8880000114440918\n",
      "Iteration 20850 Training loss 0.0003195609024260193 Validation loss 0.04317127913236618 Accuracy 0.8889999985694885\n",
      "Iteration 20860 Training loss 0.0003132834972348064 Validation loss 0.04322567582130432 Accuracy 0.8895000219345093\n",
      "Iteration 20870 Training loss 0.0010651476914063096 Validation loss 0.04323564097285271 Accuracy 0.8884999752044678\n",
      "Iteration 20880 Training loss 0.0013124902034178376 Validation loss 0.04324083775281906 Accuracy 0.8884999752044678\n",
      "Iteration 20890 Training loss 0.0018169479444622993 Validation loss 0.04319637268781662 Accuracy 0.8880000114440918\n",
      "Iteration 20900 Training loss 0.0010674111545085907 Validation loss 0.04317886382341385 Accuracy 0.8880000114440918\n",
      "Iteration 20910 Training loss 0.0015676404582336545 Validation loss 0.04323046654462814 Accuracy 0.8889999985694885\n",
      "Iteration 20920 Training loss 0.0005672044353559613 Validation loss 0.043132830411195755 Accuracy 0.8884999752044678\n",
      "Iteration 20930 Training loss 0.0010707811452448368 Validation loss 0.043201517313718796 Accuracy 0.8884999752044678\n",
      "Iteration 20940 Training loss 0.0008152773953042924 Validation loss 0.04314327985048294 Accuracy 0.8884999752044678\n",
      "Iteration 20950 Training loss 0.0010654483921825886 Validation loss 0.04318061098456383 Accuracy 0.8880000114440918\n",
      "Iteration 20960 Training loss 0.0015658217016607523 Validation loss 0.04317738115787506 Accuracy 0.8884999752044678\n",
      "Iteration 20970 Training loss 0.0005637187277898192 Validation loss 0.04316848888993263 Accuracy 0.8880000114440918\n",
      "Iteration 20980 Training loss 0.0013161193346604705 Validation loss 0.04317329823970795 Accuracy 0.8884999752044678\n",
      "Iteration 20990 Training loss 0.000818699540104717 Validation loss 0.04323311522603035 Accuracy 0.8880000114440918\n",
      "Iteration 21000 Training loss 0.001070163445547223 Validation loss 0.043191131204366684 Accuracy 0.8880000114440918\n",
      "Iteration 21010 Training loss 0.001063895528204739 Validation loss 0.04317115247249603 Accuracy 0.8884999752044678\n",
      "Iteration 21020 Training loss 0.0003114673891104758 Validation loss 0.04319941997528076 Accuracy 0.8889999985694885\n",
      "Iteration 21030 Training loss 0.000571215059608221 Validation loss 0.04316317290067673 Accuracy 0.8889999985694885\n",
      "Iteration 21040 Training loss 0.0008220442687161267 Validation loss 0.04313294589519501 Accuracy 0.8884999752044678\n",
      "Iteration 21050 Training loss 0.0010707268957048655 Validation loss 0.04312784597277641 Accuracy 0.8889999985694885\n",
      "Iteration 21060 Training loss 0.00031879093148745596 Validation loss 0.04314007610082626 Accuracy 0.8884999752044678\n",
      "Iteration 21070 Training loss 0.0008135763346217573 Validation loss 0.04319652169942856 Accuracy 0.8880000114440918\n",
      "Iteration 21080 Training loss 0.0005670013488270342 Validation loss 0.04320383444428444 Accuracy 0.8884999752044678\n",
      "Iteration 21090 Training loss 0.00031587740522809327 Validation loss 0.043182261288166046 Accuracy 0.8884999752044678\n",
      "Iteration 21100 Training loss 0.0013157952344045043 Validation loss 0.043186087161302567 Accuracy 0.8889999985694885\n",
      "Iteration 21110 Training loss 0.0005678361630998552 Validation loss 0.0431801974773407 Accuracy 0.8884999752044678\n",
      "Iteration 21120 Training loss 0.0005704602226614952 Validation loss 0.0431700125336647 Accuracy 0.8880000114440918\n",
      "Iteration 21130 Training loss 0.0010692290961742401 Validation loss 0.04319963604211807 Accuracy 0.8884999752044678\n",
      "Iteration 21140 Training loss 0.0010665185982361436 Validation loss 0.04320146143436432 Accuracy 0.8889999985694885\n",
      "Iteration 21150 Training loss 0.0015685991384088993 Validation loss 0.043182242661714554 Accuracy 0.8884999752044678\n",
      "Iteration 21160 Training loss 0.0010725775500759482 Validation loss 0.043177321553230286 Accuracy 0.8884999752044678\n",
      "Iteration 21170 Training loss 0.001063573989085853 Validation loss 0.043138593435287476 Accuracy 0.8884999752044678\n",
      "Iteration 21180 Training loss 0.0010653347708284855 Validation loss 0.0431559756398201 Accuracy 0.8884999752044678\n",
      "Iteration 21190 Training loss 0.0005727510433644056 Validation loss 0.04319063574075699 Accuracy 0.8880000114440918\n",
      "Iteration 21200 Training loss 0.0008158765849657357 Validation loss 0.043161679059267044 Accuracy 0.8884999752044678\n",
      "Iteration 21210 Training loss 0.0005707275704480708 Validation loss 0.043186500668525696 Accuracy 0.8884999752044678\n",
      "Iteration 21220 Training loss 0.0013135189656168222 Validation loss 0.043187886476516724 Accuracy 0.8884999752044678\n",
      "Iteration 21230 Training loss 0.001065561780706048 Validation loss 0.04318396747112274 Accuracy 0.8884999752044678\n",
      "Iteration 21240 Training loss 0.0015721033560112119 Validation loss 0.04320114850997925 Accuracy 0.8884999752044678\n",
      "Iteration 21250 Training loss 0.0005684900097548962 Validation loss 0.0431598499417305 Accuracy 0.8884999752044678\n",
      "Iteration 21260 Training loss 0.0008213500841520727 Validation loss 0.04318581894040108 Accuracy 0.8880000114440918\n",
      "Iteration 21270 Training loss 0.0005668230005539954 Validation loss 0.043151650577783585 Accuracy 0.8884999752044678\n",
      "Iteration 21280 Training loss 7.103942334651947e-05 Validation loss 0.04315917193889618 Accuracy 0.8884999752044678\n",
      "Iteration 21290 Training loss 0.000565917871426791 Validation loss 0.0431380532681942 Accuracy 0.8880000114440918\n",
      "Iteration 21300 Training loss 0.0008195819682441652 Validation loss 0.04319344460964203 Accuracy 0.8884999752044678\n",
      "Iteration 21310 Training loss 0.0013176016509532928 Validation loss 0.04316975548863411 Accuracy 0.887499988079071\n",
      "Iteration 21320 Training loss 0.0005694834981113672 Validation loss 0.043162934482097626 Accuracy 0.8880000114440918\n",
      "Iteration 21330 Training loss 0.0005641609313897789 Validation loss 0.04316216707229614 Accuracy 0.8880000114440918\n",
      "Iteration 21340 Training loss 0.0010681188432499766 Validation loss 0.043155670166015625 Accuracy 0.8880000114440918\n",
      "Iteration 21350 Training loss 0.0008213589899241924 Validation loss 0.04318302124738693 Accuracy 0.8880000114440918\n",
      "Iteration 21360 Training loss 0.0005726781091652811 Validation loss 0.0431867353618145 Accuracy 0.8884999752044678\n",
      "Iteration 21370 Training loss 0.0008141502039507031 Validation loss 0.04316798225045204 Accuracy 0.8880000114440918\n",
      "Iteration 21380 Training loss 0.0010639042593538761 Validation loss 0.04316866397857666 Accuracy 0.8884999752044678\n",
      "Iteration 21390 Training loss 0.00032025258406065404 Validation loss 0.04319591075181961 Accuracy 0.8880000114440918\n",
      "Iteration 21400 Training loss 0.0003190501593053341 Validation loss 0.043187085539102554 Accuracy 0.8884999752044678\n",
      "Iteration 21410 Training loss 0.0010666351299732924 Validation loss 0.04319888353347778 Accuracy 0.8884999752044678\n",
      "Iteration 21420 Training loss 0.0005686596850864589 Validation loss 0.0431954488158226 Accuracy 0.887499988079071\n",
      "Iteration 21430 Training loss 0.0008227018406614661 Validation loss 0.04323351010680199 Accuracy 0.8880000114440918\n",
      "Iteration 21440 Training loss 0.0008214400731958449 Validation loss 0.04318414255976677 Accuracy 0.8880000114440918\n",
      "Iteration 21450 Training loss 6.549115641973913e-05 Validation loss 0.043197859078645706 Accuracy 0.8889999985694885\n",
      "Iteration 21460 Training loss 0.0003177352773491293 Validation loss 0.0431453213095665 Accuracy 0.8880000114440918\n",
      "Iteration 21470 Training loss 0.0013177633518353105 Validation loss 0.04317170009016991 Accuracy 0.8880000114440918\n",
      "Iteration 21480 Training loss 0.000317227968480438 Validation loss 0.04318473860621452 Accuracy 0.8884999752044678\n",
      "Iteration 21490 Training loss 0.000819244422018528 Validation loss 0.043178021907806396 Accuracy 0.8889999985694885\n",
      "Iteration 21500 Training loss 0.0010652432683855295 Validation loss 0.043218787759542465 Accuracy 0.887499988079071\n",
      "Iteration 21510 Training loss 6.745242717443034e-05 Validation loss 0.04317953437566757 Accuracy 0.8880000114440918\n",
      "Iteration 21520 Training loss 0.0010673082433640957 Validation loss 0.043188318610191345 Accuracy 0.8889999985694885\n",
      "Iteration 21530 Training loss 0.0008228706428781152 Validation loss 0.043121326714754105 Accuracy 0.8884999752044678\n",
      "Iteration 21540 Training loss 0.0008177550625987351 Validation loss 0.04321102052927017 Accuracy 0.8884999752044678\n",
      "Iteration 21550 Training loss 0.0010653695790097117 Validation loss 0.043164052069187164 Accuracy 0.8889999985694885\n",
      "Iteration 21560 Training loss 0.0010653947247192264 Validation loss 0.04319902881979942 Accuracy 0.8884999752044678\n",
      "Iteration 21570 Training loss 0.001072085346095264 Validation loss 0.04318626970052719 Accuracy 0.8889999985694885\n",
      "Iteration 21580 Training loss 0.000563718203920871 Validation loss 0.04316689819097519 Accuracy 0.8880000114440918\n",
      "Iteration 21590 Training loss 0.0005668940721079707 Validation loss 0.04318058118224144 Accuracy 0.8884999752044678\n",
      "Iteration 21600 Training loss 0.0013206738512963057 Validation loss 0.043168097734451294 Accuracy 0.8880000114440918\n",
      "Iteration 21610 Training loss 0.0013186591677367687 Validation loss 0.043189533054828644 Accuracy 0.8880000114440918\n",
      "Iteration 21620 Training loss 0.0008159367134794593 Validation loss 0.043182406574487686 Accuracy 0.8889999985694885\n",
      "Iteration 21630 Training loss 0.000817547959741205 Validation loss 0.04315771535038948 Accuracy 0.8880000114440918\n",
      "Iteration 21640 Training loss 0.0005774105666205287 Validation loss 0.043116554617881775 Accuracy 0.8884999752044678\n",
      "Iteration 21650 Training loss 0.0005681799375452101 Validation loss 0.043127600103616714 Accuracy 0.8884999752044678\n",
      "Iteration 21660 Training loss 0.0003144890069961548 Validation loss 0.04315244033932686 Accuracy 0.8889999985694885\n",
      "Iteration 21670 Training loss 0.0005693751154467463 Validation loss 0.04313536733388901 Accuracy 0.8880000114440918\n",
      "Iteration 21680 Training loss 0.0010667750611901283 Validation loss 0.04316680505871773 Accuracy 0.8884999752044678\n",
      "Iteration 21690 Training loss 0.0003145436348859221 Validation loss 0.04315696284174919 Accuracy 0.8884999752044678\n",
      "Iteration 21700 Training loss 0.0008201435557566583 Validation loss 0.04314280301332474 Accuracy 0.8884999752044678\n",
      "Iteration 21710 Training loss 0.0010684182634577155 Validation loss 0.043140485882759094 Accuracy 0.8884999752044678\n",
      "Iteration 21720 Training loss 0.0008189291693270206 Validation loss 0.04310602694749832 Accuracy 0.8880000114440918\n",
      "Iteration 21730 Training loss 0.0010765702463686466 Validation loss 0.04316398873925209 Accuracy 0.8884999752044678\n",
      "Iteration 21740 Training loss 0.0005618201103061438 Validation loss 0.04319290444254875 Accuracy 0.8880000114440918\n",
      "Iteration 21750 Training loss 0.0005699489265680313 Validation loss 0.04314715415239334 Accuracy 0.8880000114440918\n",
      "Iteration 21760 Training loss 0.000569505151361227 Validation loss 0.04312564805150032 Accuracy 0.8880000114440918\n",
      "Iteration 21770 Training loss 0.0008174348040483892 Validation loss 0.043133459985256195 Accuracy 0.8880000114440918\n",
      "Iteration 21780 Training loss 0.0008188060019165277 Validation loss 0.04315413534641266 Accuracy 0.8884999752044678\n",
      "Iteration 21790 Training loss 0.00031668960582464933 Validation loss 0.04314146935939789 Accuracy 0.8884999752044678\n",
      "Iteration 21800 Training loss 0.0013201425317674875 Validation loss 0.04316399618983269 Accuracy 0.8884999752044678\n",
      "Iteration 21810 Training loss 0.0005673073465004563 Validation loss 0.04315723106265068 Accuracy 0.8884999752044678\n",
      "Iteration 21820 Training loss 0.0005650373641401529 Validation loss 0.04317696392536163 Accuracy 0.8889999985694885\n",
      "Iteration 21830 Training loss 0.0013158383080735803 Validation loss 0.043146904557943344 Accuracy 0.8889999985694885\n",
      "Iteration 21840 Training loss 0.0008177816634997725 Validation loss 0.043177295476198196 Accuracy 0.8880000114440918\n",
      "Iteration 21850 Training loss 0.0015668320702388883 Validation loss 0.04315754771232605 Accuracy 0.8889999985694885\n",
      "Iteration 21860 Training loss 0.0010682728607207537 Validation loss 0.0431385412812233 Accuracy 0.8884999752044678\n",
      "Iteration 21870 Training loss 0.00032253554672934115 Validation loss 0.04317685216665268 Accuracy 0.8889999985694885\n",
      "Iteration 21880 Training loss 0.0003168824187014252 Validation loss 0.043191056698560715 Accuracy 0.8880000114440918\n",
      "Iteration 21890 Training loss 0.0003278189687989652 Validation loss 0.043160002678632736 Accuracy 0.8884999752044678\n",
      "Iteration 21900 Training loss 0.0005677074659615755 Validation loss 0.04314688593149185 Accuracy 0.8880000114440918\n",
      "Iteration 21910 Training loss 0.0010672567877918482 Validation loss 0.04313688725233078 Accuracy 0.8884999752044678\n",
      "Iteration 21920 Training loss 0.00032211688812822104 Validation loss 0.04316785931587219 Accuracy 0.8880000114440918\n",
      "Iteration 21930 Training loss 0.0013224412687122822 Validation loss 0.043185729533433914 Accuracy 0.8880000114440918\n",
      "Iteration 21940 Training loss 0.0005643123295158148 Validation loss 0.0431605763733387 Accuracy 0.8880000114440918\n",
      "Iteration 21950 Training loss 0.0023178611882030964 Validation loss 0.04314693436026573 Accuracy 0.8884999752044678\n",
      "Iteration 21960 Training loss 0.00031692234915681183 Validation loss 0.043162669986486435 Accuracy 0.8884999752044678\n",
      "Iteration 21970 Training loss 0.0008186297491192818 Validation loss 0.043184638023376465 Accuracy 0.8889999985694885\n",
      "Iteration 21980 Training loss 0.0005720629123970866 Validation loss 0.04316377639770508 Accuracy 0.8884999752044678\n",
      "Iteration 21990 Training loss 0.0003225123800802976 Validation loss 0.04315299913287163 Accuracy 0.8880000114440918\n",
      "Iteration 22000 Training loss 0.0008140422287397087 Validation loss 0.04315812140703201 Accuracy 0.8884999752044678\n",
      "Iteration 22010 Training loss 0.001065912889316678 Validation loss 0.04316019266843796 Accuracy 0.8880000114440918\n",
      "Iteration 22020 Training loss 0.0008118597324937582 Validation loss 0.043187934905290604 Accuracy 0.8884999752044678\n",
      "Iteration 22030 Training loss 0.0005740296328440309 Validation loss 0.043132297694683075 Accuracy 0.8884999752044678\n",
      "Iteration 22040 Training loss 0.0005693010170944035 Validation loss 0.0431327261030674 Accuracy 0.8880000114440918\n",
      "Iteration 22050 Training loss 0.0013241805136203766 Validation loss 0.04312540963292122 Accuracy 0.8884999752044678\n",
      "Iteration 22060 Training loss 0.0008209259831346571 Validation loss 0.043120428919792175 Accuracy 0.8880000114440918\n",
      "Iteration 22070 Training loss 0.001318952883593738 Validation loss 0.04315228387713432 Accuracy 0.8884999752044678\n",
      "Iteration 22080 Training loss 0.0010710966307669878 Validation loss 0.04315284639596939 Accuracy 0.8880000114440918\n",
      "Iteration 22090 Training loss 0.0018176735611632466 Validation loss 0.04317254200577736 Accuracy 0.8884999752044678\n",
      "Iteration 22100 Training loss 0.0008182057063095272 Validation loss 0.04318225011229515 Accuracy 0.8884999752044678\n",
      "Iteration 22110 Training loss 0.0005711076664738357 Validation loss 0.04321195185184479 Accuracy 0.887499988079071\n",
      "Iteration 22120 Training loss 0.0005746844690293074 Validation loss 0.04315667971968651 Accuracy 0.8884999752044678\n",
      "Iteration 22130 Training loss 0.0010610329918563366 Validation loss 0.04317237064242363 Accuracy 0.8880000114440918\n",
      "Iteration 22140 Training loss 0.001062179682776332 Validation loss 0.043175358325242996 Accuracy 0.8880000114440918\n",
      "Iteration 22150 Training loss 0.0013178105000406504 Validation loss 0.04319538548588753 Accuracy 0.8884999752044678\n",
      "Iteration 22160 Training loss 0.0010702445870265365 Validation loss 0.043181031942367554 Accuracy 0.8880000114440918\n",
      "Iteration 22170 Training loss 0.0010727272601798177 Validation loss 0.043174758553504944 Accuracy 0.8880000114440918\n",
      "Iteration 22180 Training loss 0.0013196702348068357 Validation loss 0.04317311942577362 Accuracy 0.8884999752044678\n",
      "Iteration 22190 Training loss 0.0015712290769442916 Validation loss 0.043189529329538345 Accuracy 0.8884999752044678\n",
      "Iteration 22200 Training loss 0.0005668345256708562 Validation loss 0.04318036884069443 Accuracy 0.8884999752044678\n",
      "Iteration 22210 Training loss 0.00032121347612701356 Validation loss 0.04314199835062027 Accuracy 0.8884999752044678\n",
      "Iteration 22220 Training loss 0.00031330675119534135 Validation loss 0.04312626272439957 Accuracy 0.8884999752044678\n",
      "Iteration 22230 Training loss 0.0005663966294378042 Validation loss 0.04314460605382919 Accuracy 0.8884999752044678\n",
      "Iteration 22240 Training loss 0.0008203525212593377 Validation loss 0.04318602383136749 Accuracy 0.8889999985694885\n",
      "Iteration 22250 Training loss 0.0008187080966308713 Validation loss 0.04315628111362457 Accuracy 0.8884999752044678\n",
      "Iteration 22260 Training loss 0.00032068381551653147 Validation loss 0.04317290708422661 Accuracy 0.8884999752044678\n",
      "Iteration 22270 Training loss 0.0003266711428295821 Validation loss 0.04316871985793114 Accuracy 0.8880000114440918\n",
      "Iteration 22280 Training loss 0.0008141031721606851 Validation loss 0.04313262179493904 Accuracy 0.8884999752044678\n",
      "Iteration 22290 Training loss 0.001070139929652214 Validation loss 0.04313058778643608 Accuracy 0.8884999752044678\n",
      "Iteration 22300 Training loss 0.0003187402617186308 Validation loss 0.04316854104399681 Accuracy 0.8884999752044678\n",
      "Iteration 22310 Training loss 0.0013211998157203197 Validation loss 0.04315495118498802 Accuracy 0.8880000114440918\n",
      "Iteration 22320 Training loss 0.0008135791285894811 Validation loss 0.04310010373592377 Accuracy 0.8884999752044678\n",
      "Iteration 22330 Training loss 0.001323231030255556 Validation loss 0.043141257017850876 Accuracy 0.8884999752044678\n",
      "Iteration 22340 Training loss 0.0010689818300306797 Validation loss 0.04313567653298378 Accuracy 0.8884999752044678\n",
      "Iteration 22350 Training loss 6.900337029946968e-05 Validation loss 0.04316909611225128 Accuracy 0.8880000114440918\n",
      "Iteration 22360 Training loss 0.0003197213518433273 Validation loss 0.04306963458657265 Accuracy 0.8889999985694885\n",
      "Iteration 22370 Training loss 0.0010679104598239064 Validation loss 0.043094292283058167 Accuracy 0.8884999752044678\n",
      "Iteration 22380 Training loss 0.0005675150314345956 Validation loss 0.04310668259859085 Accuracy 0.8884999752044678\n",
      "Iteration 22390 Training loss 0.0008158920682035387 Validation loss 0.04316104203462601 Accuracy 0.8884999752044678\n",
      "Iteration 22400 Training loss 0.00056791229872033 Validation loss 0.04311477765440941 Accuracy 0.8884999752044678\n",
      "Iteration 22410 Training loss 0.0013227376621216536 Validation loss 0.043167594820261 Accuracy 0.8880000114440918\n",
      "Iteration 22420 Training loss 0.002316508674994111 Validation loss 0.043155793100595474 Accuracy 0.8889999985694885\n",
      "Iteration 22430 Training loss 0.0008257620502263308 Validation loss 0.04310356825590134 Accuracy 0.8889999985694885\n",
      "Iteration 22440 Training loss 0.000817979103885591 Validation loss 0.04315594211220741 Accuracy 0.8889999985694885\n",
      "Iteration 22450 Training loss 0.0005695927538909018 Validation loss 0.043169375509023666 Accuracy 0.8884999752044678\n",
      "Iteration 22460 Training loss 0.00031605185358785093 Validation loss 0.04314209893345833 Accuracy 0.8884999752044678\n",
      "Iteration 22470 Training loss 0.0013182488037273288 Validation loss 0.04313382878899574 Accuracy 0.8889999985694885\n",
      "Iteration 22480 Training loss 0.0008186050690710545 Validation loss 0.043118420988321304 Accuracy 0.8884999752044678\n",
      "Iteration 22490 Training loss 0.0008192057721316814 Validation loss 0.04312027245759964 Accuracy 0.8884999752044678\n",
      "Iteration 22500 Training loss 0.0013149044243618846 Validation loss 0.04314073920249939 Accuracy 0.8884999752044678\n",
      "Iteration 22510 Training loss 0.001070727244950831 Validation loss 0.043160323053598404 Accuracy 0.8880000114440918\n",
      "Iteration 22520 Training loss 0.0008177982526831329 Validation loss 0.04310029000043869 Accuracy 0.8884999752044678\n",
      "Iteration 22530 Training loss 0.0010703933658078313 Validation loss 0.043133072555065155 Accuracy 0.8889999985694885\n",
      "Iteration 22540 Training loss 0.00031967589166015387 Validation loss 0.04311903566122055 Accuracy 0.8884999752044678\n",
      "Iteration 22550 Training loss 0.0005650660605169833 Validation loss 0.043171171098947525 Accuracy 0.8889999985694885\n",
      "Iteration 22560 Training loss 0.0008152229129336774 Validation loss 0.04316278174519539 Accuracy 0.8884999752044678\n",
      "Iteration 22570 Training loss 0.001068705227226019 Validation loss 0.043130114674568176 Accuracy 0.8884999752044678\n",
      "Iteration 22580 Training loss 0.0008233561529777944 Validation loss 0.043086059391498566 Accuracy 0.8884999752044678\n",
      "Iteration 22590 Training loss 0.0010683295549824834 Validation loss 0.04309004917740822 Accuracy 0.8884999752044678\n",
      "Iteration 22600 Training loss 0.0013148802099749446 Validation loss 0.043071046471595764 Accuracy 0.8884999752044678\n",
      "Iteration 22610 Training loss 0.0008180664153769612 Validation loss 0.043115101754665375 Accuracy 0.8884999752044678\n",
      "Iteration 22620 Training loss 0.0013191617326810956 Validation loss 0.04313315823674202 Accuracy 0.8884999752044678\n",
      "Iteration 22630 Training loss 0.0008167376508936286 Validation loss 0.04311662167310715 Accuracy 0.8889999985694885\n",
      "Iteration 22640 Training loss 0.0008185300976037979 Validation loss 0.04309521242976189 Accuracy 0.8884999752044678\n",
      "Iteration 22650 Training loss 0.0003161201311741024 Validation loss 0.04310429096221924 Accuracy 0.8889999985694885\n",
      "Iteration 22660 Training loss 0.0005689564859494567 Validation loss 0.04315049946308136 Accuracy 0.8884999752044678\n",
      "Iteration 22670 Training loss 0.0005676866858266294 Validation loss 0.043092381209135056 Accuracy 0.8889999985694885\n",
      "Iteration 22680 Training loss 0.00032193074002861977 Validation loss 0.04310092329978943 Accuracy 0.8884999752044678\n",
      "Iteration 22690 Training loss 0.0005701197078451514 Validation loss 0.04305426776409149 Accuracy 0.8889999985694885\n",
      "Iteration 22700 Training loss 0.0003169123374391347 Validation loss 0.043117087334394455 Accuracy 0.8889999985694885\n",
      "Iteration 22710 Training loss 0.0005681546172127128 Validation loss 0.04310878738760948 Accuracy 0.8889999985694885\n",
      "Iteration 22720 Training loss 0.0008211595704779029 Validation loss 0.04312271252274513 Accuracy 0.8880000114440918\n",
      "Iteration 22730 Training loss 0.0010692394571378827 Validation loss 0.043110668659210205 Accuracy 0.8884999752044678\n",
      "Iteration 22740 Training loss 0.0015700887888669968 Validation loss 0.04313822463154793 Accuracy 0.8884999752044678\n",
      "Iteration 22750 Training loss 0.001070130616426468 Validation loss 0.04311373084783554 Accuracy 0.8884999752044678\n",
      "Iteration 22760 Training loss 0.001319431816227734 Validation loss 0.043117646127939224 Accuracy 0.8884999752044678\n",
      "Iteration 22770 Training loss 0.000566335569601506 Validation loss 0.04313494265079498 Accuracy 0.887499988079071\n",
      "Iteration 22780 Training loss 0.0010672473581507802 Validation loss 0.04310848191380501 Accuracy 0.8884999752044678\n",
      "Iteration 22790 Training loss 0.00031625639530830085 Validation loss 0.04307475686073303 Accuracy 0.8884999752044678\n",
      "Iteration 22800 Training loss 0.0005665324279107153 Validation loss 0.04308035224676132 Accuracy 0.8884999752044678\n",
      "Iteration 22810 Training loss 0.001323989825323224 Validation loss 0.043105076998472214 Accuracy 0.8884999752044678\n",
      "Iteration 22820 Training loss 0.0018177010351791978 Validation loss 0.04311041906476021 Accuracy 0.8889999985694885\n",
      "Iteration 22830 Training loss 0.00031877076253294945 Validation loss 0.043120063841342926 Accuracy 0.887499988079071\n",
      "Iteration 22840 Training loss 0.0005650653620250523 Validation loss 0.04314626380801201 Accuracy 0.8880000114440918\n",
      "Iteration 22850 Training loss 0.0005682845367118716 Validation loss 0.04315359517931938 Accuracy 0.887499988079071\n",
      "Iteration 22860 Training loss 0.0008220168529078364 Validation loss 0.0431377999484539 Accuracy 0.8880000114440918\n",
      "Iteration 22870 Training loss 0.0015682666562497616 Validation loss 0.04307637736201286 Accuracy 0.8880000114440918\n",
      "Iteration 22880 Training loss 0.0008202556055039167 Validation loss 0.04308350011706352 Accuracy 0.8889999985694885\n",
      "Iteration 22890 Training loss 0.00031465361826121807 Validation loss 0.043146248906850815 Accuracy 0.8880000114440918\n",
      "Iteration 22900 Training loss 0.0008212757529690862 Validation loss 0.043106164783239365 Accuracy 0.8884999752044678\n",
      "Iteration 22910 Training loss 0.0010678948601707816 Validation loss 0.043161965906620026 Accuracy 0.8880000114440918\n",
      "Iteration 22920 Training loss 0.0015664513921365142 Validation loss 0.04307227209210396 Accuracy 0.8889999985694885\n",
      "Iteration 22930 Training loss 0.0008102363790385425 Validation loss 0.04311707243323326 Accuracy 0.8884999752044678\n",
      "Iteration 22940 Training loss 0.0005648392252624035 Validation loss 0.04314511641860008 Accuracy 0.8884999752044678\n",
      "Iteration 22950 Training loss 0.0005683135823346674 Validation loss 0.04313960298895836 Accuracy 0.8884999752044678\n",
      "Iteration 22960 Training loss 0.0005653618136420846 Validation loss 0.043101489543914795 Accuracy 0.8880000114440918\n",
      "Iteration 22970 Training loss 0.0010650925105437636 Validation loss 0.04310613498091698 Accuracy 0.8880000114440918\n",
      "Iteration 22980 Training loss 0.0010640082182362676 Validation loss 0.04313596338033676 Accuracy 0.8884999752044678\n",
      "Iteration 22990 Training loss 0.0005705351359210908 Validation loss 0.043137889355421066 Accuracy 0.8895000219345093\n",
      "Iteration 23000 Training loss 0.00131663354113698 Validation loss 0.04314699023962021 Accuracy 0.8884999752044678\n",
      "Iteration 23010 Training loss 0.00032125867437571287 Validation loss 0.04314161837100983 Accuracy 0.8889999985694885\n",
      "Iteration 23020 Training loss 0.0013194993371143937 Validation loss 0.043100226670503616 Accuracy 0.8889999985694885\n",
      "Iteration 23030 Training loss 0.0008152145892381668 Validation loss 0.04312841594219208 Accuracy 0.8889999985694885\n",
      "Iteration 23040 Training loss 7.709621422691271e-05 Validation loss 0.043139416724443436 Accuracy 0.887499988079071\n",
      "Iteration 23050 Training loss 0.0005704088835045695 Validation loss 0.0431278757750988 Accuracy 0.8884999752044678\n",
      "Iteration 23060 Training loss 0.0010652589844539762 Validation loss 0.043138373643159866 Accuracy 0.8884999752044678\n",
      "Iteration 23070 Training loss 0.0010715650860220194 Validation loss 0.04309774190187454 Accuracy 0.8884999752044678\n",
      "Iteration 23080 Training loss 0.0003142205532640219 Validation loss 0.04313427209854126 Accuracy 0.8880000114440918\n",
      "Iteration 23090 Training loss 0.000818922882899642 Validation loss 0.0431097187101841 Accuracy 0.8889999985694885\n",
      "Iteration 23100 Training loss 0.00032124106655828655 Validation loss 0.04313823580741882 Accuracy 0.8880000114440918\n",
      "Iteration 23110 Training loss 0.0010668423492461443 Validation loss 0.04313432052731514 Accuracy 0.8895000219345093\n",
      "Iteration 23120 Training loss 0.0013209289172664285 Validation loss 0.043101776391267776 Accuracy 0.8884999752044678\n",
      "Iteration 23130 Training loss 0.0010708983754739165 Validation loss 0.04309786111116409 Accuracy 0.8889999985694885\n",
      "Iteration 23140 Training loss 0.0010661720298230648 Validation loss 0.04308555647730827 Accuracy 0.8884999752044678\n",
      "Iteration 23150 Training loss 0.0013167326105758548 Validation loss 0.043117955327034 Accuracy 0.8884999752044678\n",
      "Iteration 23160 Training loss 0.0005643102340400219 Validation loss 0.04312526434659958 Accuracy 0.8884999752044678\n",
      "Iteration 23170 Training loss 0.0015690027503296733 Validation loss 0.04314067214727402 Accuracy 0.8884999752044678\n",
      "Iteration 23180 Training loss 0.0005713229184038937 Validation loss 0.04309779405593872 Accuracy 0.8880000114440918\n",
      "Iteration 23190 Training loss 0.0008201804012060165 Validation loss 0.043117593973875046 Accuracy 0.8884999752044678\n",
      "Iteration 23200 Training loss 0.0003215293399989605 Validation loss 0.04310498759150505 Accuracy 0.8884999752044678\n",
      "Iteration 23210 Training loss 0.0005651523824781179 Validation loss 0.0431281216442585 Accuracy 0.8884999752044678\n",
      "Iteration 23220 Training loss 0.0010686322348192334 Validation loss 0.04310130327939987 Accuracy 0.8889999985694885\n",
      "Iteration 23230 Training loss 0.0013180951355025172 Validation loss 0.0431278720498085 Accuracy 0.8895000219345093\n",
      "Iteration 23240 Training loss 0.000567775045055896 Validation loss 0.043092940002679825 Accuracy 0.8895000219345093\n",
      "Iteration 23250 Training loss 0.0008182433666661382 Validation loss 0.04312097653746605 Accuracy 0.8895000219345093\n",
      "Iteration 23260 Training loss 0.0010714459931477904 Validation loss 0.04311855137348175 Accuracy 0.887499988079071\n",
      "Iteration 23270 Training loss 0.000816444749943912 Validation loss 0.04311177879571915 Accuracy 0.8884999752044678\n",
      "Iteration 23280 Training loss 0.0013153384206816554 Validation loss 0.04312465712428093 Accuracy 0.8884999752044678\n",
      "Iteration 23290 Training loss 0.0010701832361519337 Validation loss 0.043117035180330276 Accuracy 0.8884999752044678\n",
      "Iteration 23300 Training loss 0.0005662239855155349 Validation loss 0.043129026889801025 Accuracy 0.8880000114440918\n",
      "Iteration 23310 Training loss 0.0005651594256050885 Validation loss 0.0431518480181694 Accuracy 0.887499988079071\n",
      "Iteration 23320 Training loss 0.0005674681742675602 Validation loss 0.043125636875629425 Accuracy 0.8880000114440918\n",
      "Iteration 23330 Training loss 0.0008228007354773581 Validation loss 0.04310983791947365 Accuracy 0.8880000114440918\n",
      "Iteration 23340 Training loss 0.0008155102259479463 Validation loss 0.04309745877981186 Accuracy 0.8895000219345093\n",
      "Iteration 23350 Training loss 0.000570884847547859 Validation loss 0.04313230141997337 Accuracy 0.8884999752044678\n",
      "Iteration 23360 Training loss 0.0005649296217598021 Validation loss 0.043092675507068634 Accuracy 0.8895000219345093\n",
      "Iteration 23370 Training loss 0.0005685184733010828 Validation loss 0.04310310631990433 Accuracy 0.8895000219345093\n",
      "Iteration 23380 Training loss 0.0010691853240132332 Validation loss 0.04312627390027046 Accuracy 0.887499988079071\n",
      "Iteration 23390 Training loss 0.0015679114731028676 Validation loss 0.04310174658894539 Accuracy 0.8895000219345093\n",
      "Iteration 23400 Training loss 0.0005625170306302607 Validation loss 0.04307379946112633 Accuracy 0.8895000219345093\n",
      "Iteration 23410 Training loss 0.0010683101136237383 Validation loss 0.04312652349472046 Accuracy 0.8880000114440918\n",
      "Iteration 23420 Training loss 0.0005691266269423068 Validation loss 0.04312901571393013 Accuracy 0.8884999752044678\n",
      "Iteration 23430 Training loss 0.0008168146014213562 Validation loss 0.04312092065811157 Accuracy 0.8889999985694885\n",
      "Iteration 23440 Training loss 0.0008226846694014966 Validation loss 0.04308474808931351 Accuracy 0.8895000219345093\n",
      "Iteration 23450 Training loss 0.0005706697120331228 Validation loss 0.0431191623210907 Accuracy 0.8880000114440918\n",
      "Iteration 23460 Training loss 0.0008186445338651538 Validation loss 0.043103255331516266 Accuracy 0.8889999985694885\n",
      "Iteration 23470 Training loss 0.001069376477971673 Validation loss 0.043103836476802826 Accuracy 0.8889999985694885\n",
      "Iteration 23480 Training loss 0.0005688346573151648 Validation loss 0.04312212020158768 Accuracy 0.8889999985694885\n",
      "Iteration 23490 Training loss 0.0008200352895073593 Validation loss 0.04311098903417587 Accuracy 0.8889999985694885\n",
      "Iteration 23500 Training loss 0.0005694993305951357 Validation loss 0.04310992360115051 Accuracy 0.8884999752044678\n",
      "Iteration 23510 Training loss 0.0010717067634686828 Validation loss 0.043084755539894104 Accuracy 0.8895000219345093\n",
      "Iteration 23520 Training loss 0.0005688003730028868 Validation loss 0.04311954975128174 Accuracy 0.8884999752044678\n",
      "Iteration 23530 Training loss 0.0010661864653229713 Validation loss 0.043124426156282425 Accuracy 0.8889999985694885\n",
      "Iteration 23540 Training loss 0.0010698569240048528 Validation loss 0.04309334605932236 Accuracy 0.8889999985694885\n",
      "Iteration 23550 Training loss 0.0015691141597926617 Validation loss 0.04313407093286514 Accuracy 0.8880000114440918\n",
      "Iteration 23560 Training loss 0.0010729132918640971 Validation loss 0.043113306164741516 Accuracy 0.8884999752044678\n",
      "Iteration 23570 Training loss 0.0015665910905227065 Validation loss 0.04311361163854599 Accuracy 0.8884999752044678\n",
      "Iteration 23580 Training loss 0.001823009573854506 Validation loss 0.043122757226228714 Accuracy 0.887499988079071\n",
      "Iteration 23590 Training loss 0.0010721066500991583 Validation loss 0.04309777915477753 Accuracy 0.8889999985694885\n",
      "Iteration 23600 Training loss 0.001315699308179319 Validation loss 0.0431009978055954 Accuracy 0.8895000219345093\n",
      "Iteration 23610 Training loss 0.0008199084550142288 Validation loss 0.04312029853463173 Accuracy 0.8889999985694885\n",
      "Iteration 23620 Training loss 0.00132247363217175 Validation loss 0.04307255521416664 Accuracy 0.8895000219345093\n",
      "Iteration 23630 Training loss 0.0013226678129285574 Validation loss 0.04305552691221237 Accuracy 0.8895000219345093\n",
      "Iteration 23640 Training loss 0.00031443435000255704 Validation loss 0.04307013750076294 Accuracy 0.8895000219345093\n",
      "Iteration 23650 Training loss 0.0015718693612143397 Validation loss 0.04312065616250038 Accuracy 0.8880000114440918\n",
      "Iteration 23660 Training loss 0.0010685920715332031 Validation loss 0.04308019205927849 Accuracy 0.8889999985694885\n",
      "Iteration 23670 Training loss 0.001070766826160252 Validation loss 0.043094735592603683 Accuracy 0.8889999985694885\n",
      "Iteration 23680 Training loss 0.0008150847861543298 Validation loss 0.043100517243146896 Accuracy 0.8889999985694885\n",
      "Iteration 23690 Training loss 0.0010662171989679337 Validation loss 0.04312419518828392 Accuracy 0.887499988079071\n",
      "Iteration 23700 Training loss 0.0010697607649490237 Validation loss 0.04307883605360985 Accuracy 0.8889999985694885\n",
      "Iteration 23710 Training loss 0.00031988861155696213 Validation loss 0.04313545301556587 Accuracy 0.887499988079071\n",
      "Iteration 23720 Training loss 0.0005643751355819404 Validation loss 0.043104179203510284 Accuracy 0.8884999752044678\n",
      "Iteration 23730 Training loss 0.00107128219678998 Validation loss 0.043093252927064896 Accuracy 0.8895000219345093\n",
      "Iteration 23740 Training loss 0.0008208535728044808 Validation loss 0.04309602826833725 Accuracy 0.8889999985694885\n",
      "Iteration 23750 Training loss 0.0013213565107434988 Validation loss 0.043090641498565674 Accuracy 0.8889999985694885\n",
      "Iteration 23760 Training loss 0.000565587542951107 Validation loss 0.04310234636068344 Accuracy 0.8880000114440918\n",
      "Iteration 23770 Training loss 0.0003130699333269149 Validation loss 0.04307594150304794 Accuracy 0.8889999985694885\n",
      "Iteration 23780 Training loss 0.0008196825510822237 Validation loss 0.04312065616250038 Accuracy 0.8889999985694885\n",
      "Iteration 23790 Training loss 0.0003196366596966982 Validation loss 0.043116096407175064 Accuracy 0.8880000114440918\n",
      "Iteration 23800 Training loss 0.0005679106689058244 Validation loss 0.04308297857642174 Accuracy 0.8884999752044678\n",
      "Iteration 23810 Training loss 6.169456901261583e-05 Validation loss 0.043092310428619385 Accuracy 0.8899999856948853\n",
      "Iteration 23820 Training loss 0.0008230336825363338 Validation loss 0.04310115799307823 Accuracy 0.8895000219345093\n",
      "Iteration 23830 Training loss 0.0003165749949403107 Validation loss 0.04315779358148575 Accuracy 0.887499988079071\n",
      "Iteration 23840 Training loss 0.0008184075704775751 Validation loss 0.04314149543642998 Accuracy 0.8880000114440918\n",
      "Iteration 23850 Training loss 0.0010678970720618963 Validation loss 0.04311515763401985 Accuracy 0.8889999985694885\n",
      "Iteration 23860 Training loss 0.0008225739002227783 Validation loss 0.04317212849855423 Accuracy 0.887499988079071\n",
      "Iteration 23870 Training loss 0.0010696265380829573 Validation loss 0.043123260140419006 Accuracy 0.887499988079071\n",
      "Iteration 23880 Training loss 0.0008192673558369279 Validation loss 0.043067578226327896 Accuracy 0.8889999985694885\n",
      "Iteration 23890 Training loss 7.19146992196329e-05 Validation loss 0.04311209172010422 Accuracy 0.8880000114440918\n",
      "Iteration 23900 Training loss 0.000571798998862505 Validation loss 0.04306018725037575 Accuracy 0.8889999985694885\n",
      "Iteration 23910 Training loss 0.0003152757999487221 Validation loss 0.043116308748722076 Accuracy 0.887499988079071\n",
      "Iteration 23920 Training loss 0.0005711401463486254 Validation loss 0.04310166463255882 Accuracy 0.887499988079071\n",
      "Iteration 23930 Training loss 7.080155046423897e-05 Validation loss 0.043076395988464355 Accuracy 0.8899999856948853\n",
      "Iteration 23940 Training loss 0.000820362358354032 Validation loss 0.04312103986740112 Accuracy 0.8889999985694885\n",
      "Iteration 23950 Training loss 0.0005716028390452266 Validation loss 0.04313819110393524 Accuracy 0.8880000114440918\n",
      "Iteration 23960 Training loss 0.0003144944494124502 Validation loss 0.04307311028242111 Accuracy 0.8889999985694885\n",
      "Iteration 23970 Training loss 0.001567638712003827 Validation loss 0.04312651604413986 Accuracy 0.8889999985694885\n",
      "Iteration 23980 Training loss 0.0005748861003667116 Validation loss 0.043124809861183167 Accuracy 0.8880000114440918\n",
      "Iteration 23990 Training loss 0.0010710246860980988 Validation loss 0.04312590882182121 Accuracy 0.8895000219345093\n",
      "Iteration 24000 Training loss 0.0008130114292725921 Validation loss 0.04311766102910042 Accuracy 0.8884999752044678\n",
      "Iteration 24010 Training loss 0.0008204136393032968 Validation loss 0.04306383430957794 Accuracy 0.8895000219345093\n",
      "Iteration 24020 Training loss 0.0005725888186134398 Validation loss 0.04316132888197899 Accuracy 0.887499988079071\n",
      "Iteration 24030 Training loss 0.0008168126223608851 Validation loss 0.04309338331222534 Accuracy 0.8884999752044678\n",
      "Iteration 24040 Training loss 0.0008168898639269173 Validation loss 0.04308648034930229 Accuracy 0.8889999985694885\n",
      "Iteration 24050 Training loss 7.193534111138433e-05 Validation loss 0.043054673820734024 Accuracy 0.8889999985694885\n",
      "Iteration 24060 Training loss 0.0008156169787980616 Validation loss 0.0430268719792366 Accuracy 0.8895000219345093\n",
      "Iteration 24070 Training loss 0.0005651431856676936 Validation loss 0.04302040487527847 Accuracy 0.8884999752044678\n",
      "Iteration 24080 Training loss 0.0010709596099331975 Validation loss 0.043086856603622437 Accuracy 0.8889999985694885\n",
      "Iteration 24090 Training loss 0.0008210738888010383 Validation loss 0.04305613040924072 Accuracy 0.8884999752044678\n",
      "Iteration 24100 Training loss 0.0008193334797397256 Validation loss 0.04307159408926964 Accuracy 0.8884999752044678\n",
      "Iteration 24110 Training loss 0.0015692512970417738 Validation loss 0.043094225227832794 Accuracy 0.8889999985694885\n",
      "Iteration 24120 Training loss 0.0008223354816436768 Validation loss 0.0431024506688118 Accuracy 0.8889999985694885\n",
      "Iteration 24130 Training loss 0.0010708373738452792 Validation loss 0.04313462972640991 Accuracy 0.8884999752044678\n",
      "Iteration 24140 Training loss 0.0010649774922057986 Validation loss 0.04312915727496147 Accuracy 0.8880000114440918\n",
      "Iteration 24150 Training loss 0.0010691401548683643 Validation loss 0.0430852510035038 Accuracy 0.8889999985694885\n",
      "Iteration 24160 Training loss 0.0005654082633554935 Validation loss 0.04310750588774681 Accuracy 0.8889999985694885\n",
      "Iteration 24170 Training loss 0.0015675912145525217 Validation loss 0.04310629889369011 Accuracy 0.887499988079071\n",
      "Iteration 24180 Training loss 0.00031866892823018134 Validation loss 0.04305821657180786 Accuracy 0.8889999985694885\n",
      "Iteration 24190 Training loss 0.0008276901207864285 Validation loss 0.043092139065265656 Accuracy 0.8889999985694885\n",
      "Iteration 24200 Training loss 0.0018159352475777268 Validation loss 0.04307378828525543 Accuracy 0.8895000219345093\n",
      "Iteration 24210 Training loss 0.001820847624912858 Validation loss 0.04307679831981659 Accuracy 0.8889999985694885\n",
      "Iteration 24220 Training loss 0.0013138549402356148 Validation loss 0.04309208691120148 Accuracy 0.8889999985694885\n",
      "Iteration 24230 Training loss 0.00031435530399903655 Validation loss 0.043080490082502365 Accuracy 0.8895000219345093\n",
      "Iteration 24240 Training loss 0.0018190188566222787 Validation loss 0.04305048659443855 Accuracy 0.8895000219345093\n",
      "Iteration 24250 Training loss 0.0008246657671406865 Validation loss 0.04308605194091797 Accuracy 0.8880000114440918\n",
      "Iteration 24260 Training loss 0.0005721154739148915 Validation loss 0.043104689568281174 Accuracy 0.8884999752044678\n",
      "Iteration 24270 Training loss 0.0005679178866557777 Validation loss 0.043091580271720886 Accuracy 0.8880000114440918\n",
      "Iteration 24280 Training loss 6.215873872861266e-05 Validation loss 0.04304080456495285 Accuracy 0.8899999856948853\n",
      "Iteration 24290 Training loss 0.0010707818437367678 Validation loss 0.043040234595537186 Accuracy 0.8899999856948853\n",
      "Iteration 24300 Training loss 0.0013214676873758435 Validation loss 0.0431072898209095 Accuracy 0.8884999752044678\n",
      "Iteration 24310 Training loss 0.0008276348817162216 Validation loss 0.04309524968266487 Accuracy 0.8889999985694885\n",
      "Iteration 24320 Training loss 0.0010693803196772933 Validation loss 0.04310402274131775 Accuracy 0.8884999752044678\n",
      "Iteration 24330 Training loss 0.001570235937833786 Validation loss 0.043091583997011185 Accuracy 0.8884999752044678\n",
      "Iteration 24340 Training loss 0.0013170430902391672 Validation loss 0.04309762641787529 Accuracy 0.8880000114440918\n",
      "Iteration 24350 Training loss 0.0005658764275722206 Validation loss 0.04307772219181061 Accuracy 0.8884999752044678\n",
      "Iteration 24360 Training loss 0.0008191498927772045 Validation loss 0.043080296367406845 Accuracy 0.8884999752044678\n",
      "Iteration 24370 Training loss 0.0005626098718494177 Validation loss 0.04316142573952675 Accuracy 0.887499988079071\n",
      "Iteration 24380 Training loss 0.0013168170116841793 Validation loss 0.04310086369514465 Accuracy 0.8880000114440918\n",
      "Iteration 24390 Training loss 0.0008163917227648199 Validation loss 0.04311994090676308 Accuracy 0.8884999752044678\n",
      "Iteration 24400 Training loss 0.0008198709692806005 Validation loss 0.04311880096793175 Accuracy 0.8880000114440918\n",
      "Iteration 24410 Training loss 0.001818947959691286 Validation loss 0.04308376461267471 Accuracy 0.8889999985694885\n",
      "Iteration 24420 Training loss 0.0015683139208704233 Validation loss 0.04308149963617325 Accuracy 0.8884999752044678\n",
      "Iteration 24430 Training loss 0.00032017400371842086 Validation loss 0.0431002639234066 Accuracy 0.8884999752044678\n",
      "Iteration 24440 Training loss 0.001072087325155735 Validation loss 0.043061792850494385 Accuracy 0.8889999985694885\n",
      "Iteration 24450 Training loss 0.0015733856707811356 Validation loss 0.043070755898952484 Accuracy 0.8889999985694885\n",
      "Iteration 24460 Training loss 0.0008164473692886531 Validation loss 0.043087173253297806 Accuracy 0.8884999752044678\n",
      "Iteration 24470 Training loss 0.0013214314822107553 Validation loss 0.04313008859753609 Accuracy 0.8880000114440918\n",
      "Iteration 24480 Training loss 0.001067489618435502 Validation loss 0.04309559613466263 Accuracy 0.887499988079071\n",
      "Iteration 24490 Training loss 0.0013184227282181382 Validation loss 0.04308999702334404 Accuracy 0.8880000114440918\n",
      "Iteration 24500 Training loss 0.0008165751933120191 Validation loss 0.043076079338788986 Accuracy 0.8884999752044678\n",
      "Iteration 24510 Training loss 0.0008208171930164099 Validation loss 0.043086688965559006 Accuracy 0.887499988079071\n",
      "Iteration 24520 Training loss 0.001070047146640718 Validation loss 0.0430830754339695 Accuracy 0.887499988079071\n",
      "Iteration 24530 Training loss 0.0015652875881642103 Validation loss 0.04306328669190407 Accuracy 0.8889999985694885\n",
      "Iteration 24540 Training loss 0.001313549466431141 Validation loss 0.043119147419929504 Accuracy 0.8880000114440918\n",
      "Iteration 24550 Training loss 0.001818195334635675 Validation loss 0.04309895262122154 Accuracy 0.887499988079071\n",
      "Iteration 24560 Training loss 0.0008188824285753071 Validation loss 0.04309438541531563 Accuracy 0.8884999752044678\n",
      "Iteration 24570 Training loss 0.0003190945426467806 Validation loss 0.04310961812734604 Accuracy 0.8880000114440918\n",
      "Iteration 24580 Training loss 0.0005703881033696234 Validation loss 0.04311873018741608 Accuracy 0.8880000114440918\n",
      "Iteration 24590 Training loss 0.0013212808407843113 Validation loss 0.04312817379832268 Accuracy 0.887499988079071\n",
      "Iteration 24600 Training loss 6.961146573303267e-05 Validation loss 0.04304392263293266 Accuracy 0.8889999985694885\n",
      "Iteration 24610 Training loss 0.0013221797998994589 Validation loss 0.043064504861831665 Accuracy 0.8880000114440918\n",
      "Iteration 24620 Training loss 0.0010698955738916993 Validation loss 0.043119389563798904 Accuracy 0.887499988079071\n",
      "Iteration 24630 Training loss 0.0010713109513744712 Validation loss 0.04309266805648804 Accuracy 0.8884999752044678\n",
      "Iteration 24640 Training loss 0.0010691577335819602 Validation loss 0.04309406131505966 Accuracy 0.8884999752044678\n",
      "Iteration 24650 Training loss 0.0010699344566091895 Validation loss 0.04313140735030174 Accuracy 0.8880000114440918\n",
      "Iteration 24660 Training loss 0.0003196171310264617 Validation loss 0.04308084771037102 Accuracy 0.8880000114440918\n",
      "Iteration 24670 Training loss 0.0013163738185539842 Validation loss 0.04306969419121742 Accuracy 0.8880000114440918\n",
      "Iteration 24680 Training loss 0.0005657764268107712 Validation loss 0.04309317097067833 Accuracy 0.8880000114440918\n",
      "Iteration 24690 Training loss 0.00031550091807730496 Validation loss 0.043064385652542114 Accuracy 0.8884999752044678\n",
      "Iteration 24700 Training loss 0.0015678651398047805 Validation loss 0.04304716736078262 Accuracy 0.8884999752044678\n",
      "Iteration 24710 Training loss 0.0008200900047086179 Validation loss 0.04307540878653526 Accuracy 0.8880000114440918\n",
      "Iteration 24720 Training loss 0.0018201936036348343 Validation loss 0.043081291019916534 Accuracy 0.8884999752044678\n",
      "Iteration 24730 Training loss 0.000818649132270366 Validation loss 0.04304453730583191 Accuracy 0.8884999752044678\n",
      "Iteration 24740 Training loss 0.0015696667833253741 Validation loss 0.04306504875421524 Accuracy 0.8884999752044678\n",
      "Iteration 24750 Training loss 0.001325878081843257 Validation loss 0.043073300272226334 Accuracy 0.8884999752044678\n",
      "Iteration 24760 Training loss 0.0005693958955816925 Validation loss 0.043115753680467606 Accuracy 0.8880000114440918\n",
      "Iteration 24770 Training loss 0.0008172180387191474 Validation loss 0.04309946298599243 Accuracy 0.8880000114440918\n",
      "Iteration 24780 Training loss 0.0010660849511623383 Validation loss 0.04311414808034897 Accuracy 0.8880000114440918\n",
      "Iteration 24790 Training loss 0.0008233532425947487 Validation loss 0.043140996247529984 Accuracy 0.8880000114440918\n",
      "Iteration 24800 Training loss 0.0008194409892894328 Validation loss 0.04310569912195206 Accuracy 0.8884999752044678\n",
      "Iteration 24810 Training loss 0.0003215169708710164 Validation loss 0.043128903955221176 Accuracy 0.8880000114440918\n",
      "Iteration 24820 Training loss 0.0008183784084394574 Validation loss 0.04311317950487137 Accuracy 0.887499988079071\n",
      "Iteration 24830 Training loss 0.0005727543612010777 Validation loss 0.04314012452960014 Accuracy 0.887499988079071\n",
      "Iteration 24840 Training loss 0.0005641254829242826 Validation loss 0.04311920329928398 Accuracy 0.8880000114440918\n",
      "Iteration 24850 Training loss 0.0008201508899219334 Validation loss 0.043107032775878906 Accuracy 0.8884999752044678\n",
      "Iteration 24860 Training loss 0.0008155829855240881 Validation loss 0.04309159144759178 Accuracy 0.887499988079071\n",
      "Iteration 24870 Training loss 0.0013240148546174169 Validation loss 0.043111350387334824 Accuracy 0.8884999752044678\n",
      "Iteration 24880 Training loss 0.0008177821291610599 Validation loss 0.04307769984006882 Accuracy 0.8884999752044678\n",
      "Iteration 24890 Training loss 0.0003225910768378526 Validation loss 0.043121349066495895 Accuracy 0.8880000114440918\n",
      "Iteration 24900 Training loss 0.0013181972317397594 Validation loss 0.043095506727695465 Accuracy 0.8884999752044678\n",
      "Iteration 24910 Training loss 0.0010722125880420208 Validation loss 0.04312564805150032 Accuracy 0.8884999752044678\n",
      "Iteration 24920 Training loss 0.0003200665523763746 Validation loss 0.043072767555713654 Accuracy 0.8889999985694885\n",
      "Iteration 24930 Training loss 0.0005638739094138145 Validation loss 0.04306488856673241 Accuracy 0.8880000114440918\n",
      "Iteration 24940 Training loss 0.0008187160710804164 Validation loss 0.04308254271745682 Accuracy 0.887499988079071\n",
      "Iteration 24950 Training loss 0.0008194900001399219 Validation loss 0.04314984008669853 Accuracy 0.8884999752044678\n",
      "Iteration 24960 Training loss 0.0013187010772526264 Validation loss 0.04311180114746094 Accuracy 0.8880000114440918\n",
      "Iteration 24970 Training loss 0.0008164357859641314 Validation loss 0.04307739436626434 Accuracy 0.8889999985694885\n",
      "Iteration 24980 Training loss 0.0013201528927311301 Validation loss 0.04310818389058113 Accuracy 0.8884999752044678\n",
      "Iteration 24990 Training loss 0.0010662706336006522 Validation loss 0.043166887015104294 Accuracy 0.887499988079071\n",
      "Iteration 25000 Training loss 0.0005730365519411862 Validation loss 0.043097976595163345 Accuracy 0.8884999752044678\n",
      "Iteration 25010 Training loss 0.0008243924239650369 Validation loss 0.043108079582452774 Accuracy 0.8880000114440918\n",
      "Iteration 25020 Training loss 0.0008186421473510563 Validation loss 0.043043509125709534 Accuracy 0.8895000219345093\n",
      "Iteration 25030 Training loss 0.0010665106819942594 Validation loss 0.04307001084089279 Accuracy 0.8884999752044678\n",
      "Iteration 25040 Training loss 0.0008230134844779968 Validation loss 0.04311927780508995 Accuracy 0.8880000114440918\n",
      "Iteration 25050 Training loss 0.0010669530602172017 Validation loss 0.04309547320008278 Accuracy 0.8880000114440918\n",
      "Iteration 25060 Training loss 0.000313824275508523 Validation loss 0.04310905560851097 Accuracy 0.887499988079071\n",
      "Iteration 25070 Training loss 0.0008217305294238031 Validation loss 0.043053727596998215 Accuracy 0.8895000219345093\n",
      "Iteration 25080 Training loss 0.00107180280610919 Validation loss 0.043116044253110886 Accuracy 0.887499988079071\n",
      "Iteration 25090 Training loss 0.0010682897409424186 Validation loss 0.04313334450125694 Accuracy 0.887499988079071\n",
      "Iteration 25100 Training loss 0.0005675312131643295 Validation loss 0.04309527948498726 Accuracy 0.887499988079071\n",
      "Iteration 25110 Training loss 0.000818711647298187 Validation loss 0.04310980066657066 Accuracy 0.8880000114440918\n",
      "Iteration 25120 Training loss 0.0010681288549676538 Validation loss 0.043072886765003204 Accuracy 0.887499988079071\n",
      "Iteration 25130 Training loss 0.0015688150888308883 Validation loss 0.043085768818855286 Accuracy 0.8880000114440918\n",
      "Iteration 25140 Training loss 0.0015739871887490153 Validation loss 0.04305851459503174 Accuracy 0.8880000114440918\n",
      "Iteration 25150 Training loss 0.0003166481910739094 Validation loss 0.043073296546936035 Accuracy 0.8884999752044678\n",
      "Iteration 25160 Training loss 0.0013203369453549385 Validation loss 0.04306306689977646 Accuracy 0.8889999985694885\n",
      "Iteration 25170 Training loss 0.001564637990668416 Validation loss 0.043085403740406036 Accuracy 0.887499988079071\n",
      "Iteration 25180 Training loss 0.0005707656382583082 Validation loss 0.04309825226664543 Accuracy 0.8880000114440918\n",
      "Iteration 25190 Training loss 0.0008222957840189338 Validation loss 0.04309535399079323 Accuracy 0.8884999752044678\n",
      "Iteration 25200 Training loss 0.0010665238369256258 Validation loss 0.043116092681884766 Accuracy 0.8880000114440918\n",
      "Iteration 25210 Training loss 0.0013198452070355415 Validation loss 0.04306437075138092 Accuracy 0.8895000219345093\n",
      "Iteration 25220 Training loss 0.0008171726949512959 Validation loss 0.043100785464048386 Accuracy 0.887499988079071\n",
      "Iteration 25230 Training loss 0.0015708872815594077 Validation loss 0.043061837553977966 Accuracy 0.887499988079071\n",
      "Iteration 25240 Training loss 0.0015730877639725804 Validation loss 0.04303113371133804 Accuracy 0.8899999856948853\n",
      "Iteration 25250 Training loss 0.000815971870906651 Validation loss 0.0430513434112072 Accuracy 0.8884999752044678\n",
      "Iteration 25260 Training loss 0.00032255135010927916 Validation loss 0.04311506822705269 Accuracy 0.8880000114440918\n",
      "Iteration 25270 Training loss 0.0015654906164854765 Validation loss 0.04306356608867645 Accuracy 0.8880000114440918\n",
      "Iteration 25280 Training loss 0.0013174989726394415 Validation loss 0.043048713356256485 Accuracy 0.8895000219345093\n",
      "Iteration 25290 Training loss 0.0008125838357955217 Validation loss 0.04307626932859421 Accuracy 0.8884999752044678\n",
      "Iteration 25300 Training loss 0.0010751000372692943 Validation loss 0.04310937970876694 Accuracy 0.8880000114440918\n",
      "Iteration 25310 Training loss 0.0013241994893178344 Validation loss 0.04307340830564499 Accuracy 0.887499988079071\n",
      "Iteration 25320 Training loss 0.0003219379868824035 Validation loss 0.04306837543845177 Accuracy 0.890500009059906\n",
      "Iteration 25330 Training loss 0.0010699297999963164 Validation loss 0.04306110367178917 Accuracy 0.8889999985694885\n",
      "Iteration 25340 Training loss 0.00156458280980587 Validation loss 0.04308963567018509 Accuracy 0.8880000114440918\n",
      "Iteration 25350 Training loss 0.0008190840017050505 Validation loss 0.04311263561248779 Accuracy 0.8880000114440918\n",
      "Iteration 25360 Training loss 0.0008213169639930129 Validation loss 0.04307999834418297 Accuracy 0.8889999985694885\n",
      "Iteration 25370 Training loss 0.0013166244607418776 Validation loss 0.043069154024124146 Accuracy 0.8889999985694885\n",
      "Iteration 25380 Training loss 0.0015697235940024257 Validation loss 0.04310492053627968 Accuracy 0.8880000114440918\n",
      "Iteration 25390 Training loss 0.0010674898512661457 Validation loss 0.04306735470890999 Accuracy 0.8895000219345093\n",
      "Iteration 25400 Training loss 0.0008189431973733008 Validation loss 0.04310097545385361 Accuracy 0.887499988079071\n",
      "Iteration 25410 Training loss 0.001069138990715146 Validation loss 0.04309433698654175 Accuracy 0.8884999752044678\n",
      "Iteration 25420 Training loss 0.0008224985795095563 Validation loss 0.04309555143117905 Accuracy 0.8889999985694885\n",
      "Iteration 25430 Training loss 0.0008158488781191409 Validation loss 0.04311736300587654 Accuracy 0.8870000243186951\n",
      "Iteration 25440 Training loss 0.00032012726296670735 Validation loss 0.0430590845644474 Accuracy 0.8889999985694885\n",
      "Iteration 25450 Training loss 0.0015683927340433002 Validation loss 0.04306812956929207 Accuracy 0.8884999752044678\n",
      "Iteration 25460 Training loss 0.001565173384733498 Validation loss 0.04307283088564873 Accuracy 0.8884999752044678\n",
      "Iteration 25470 Training loss 0.0013196546351537108 Validation loss 0.043100662529468536 Accuracy 0.8889999985694885\n",
      "Iteration 25480 Training loss 0.0008155176765285432 Validation loss 0.043075062334537506 Accuracy 0.8880000114440918\n",
      "Iteration 25490 Training loss 0.000566545408219099 Validation loss 0.043061308562755585 Accuracy 0.8884999752044678\n",
      "Iteration 25500 Training loss 0.00031561829382553697 Validation loss 0.043077223002910614 Accuracy 0.8884999752044678\n",
      "Iteration 25510 Training loss 0.00057054782519117 Validation loss 0.04311145842075348 Accuracy 0.8880000114440918\n",
      "Iteration 25520 Training loss 0.0010688103502616286 Validation loss 0.043096840381622314 Accuracy 0.8880000114440918\n",
      "Iteration 25530 Training loss 0.0015695947222411633 Validation loss 0.043075915426015854 Accuracy 0.8889999985694885\n",
      "Iteration 25540 Training loss 0.0008187709026969969 Validation loss 0.04304663836956024 Accuracy 0.8899999856948853\n",
      "Iteration 25550 Training loss 0.0010683570289984345 Validation loss 0.043040964752435684 Accuracy 0.8899999856948853\n",
      "Iteration 25560 Training loss 0.0005730522098019719 Validation loss 0.043066009879112244 Accuracy 0.8889999985694885\n",
      "Iteration 25570 Training loss 0.0013206521980464458 Validation loss 0.04306758567690849 Accuracy 0.8884999752044678\n",
      "Iteration 25580 Training loss 0.00082017743261531 Validation loss 0.04307485371828079 Accuracy 0.8889999985694885\n",
      "Iteration 25590 Training loss 0.0005657373694702983 Validation loss 0.04304403066635132 Accuracy 0.8880000114440918\n",
      "Iteration 25600 Training loss 0.0005652004620060325 Validation loss 0.0430627316236496 Accuracy 0.8884999752044678\n",
      "Iteration 25610 Training loss 0.0008149671484716237 Validation loss 0.043071914464235306 Accuracy 0.8884999752044678\n",
      "Iteration 25620 Training loss 0.001064093317836523 Validation loss 0.04309789091348648 Accuracy 0.887499988079071\n",
      "Iteration 25630 Training loss 0.0008189723594114184 Validation loss 0.043120190501213074 Accuracy 0.887499988079071\n",
      "Iteration 25640 Training loss 0.00031691932235844433 Validation loss 0.043113287538290024 Accuracy 0.887499988079071\n",
      "Iteration 25650 Training loss 0.0010718683479353786 Validation loss 0.04306435212492943 Accuracy 0.8895000219345093\n",
      "Iteration 25660 Training loss 0.0010690880008041859 Validation loss 0.04307255521416664 Accuracy 0.8880000114440918\n",
      "Iteration 25670 Training loss 0.0005696380394510925 Validation loss 0.04309561103582382 Accuracy 0.8880000114440918\n",
      "Iteration 25680 Training loss 0.0010673271026462317 Validation loss 0.04306015744805336 Accuracy 0.8880000114440918\n",
      "Iteration 25690 Training loss 0.0008185617043636739 Validation loss 0.04307881370186806 Accuracy 0.8884999752044678\n",
      "Iteration 25700 Training loss 0.000320421124342829 Validation loss 0.04310904070734978 Accuracy 0.8880000114440918\n",
      "Iteration 25710 Training loss 0.0013175391359254718 Validation loss 0.04310114309191704 Accuracy 0.8880000114440918\n",
      "Iteration 25720 Training loss 0.0005679368623532355 Validation loss 0.04308453947305679 Accuracy 0.8884999752044678\n",
      "Iteration 25730 Training loss 0.0008181632729247212 Validation loss 0.043128903955221176 Accuracy 0.887499988079071\n",
      "Iteration 25740 Training loss 0.000563745794352144 Validation loss 0.04310380294919014 Accuracy 0.8884999752044678\n",
      "Iteration 25750 Training loss 0.0013275493402034044 Validation loss 0.0430806428194046 Accuracy 0.8884999752044678\n",
      "Iteration 25760 Training loss 0.0008187502389773726 Validation loss 0.043107446283102036 Accuracy 0.8880000114440918\n",
      "Iteration 25770 Training loss 0.0008213508408516645 Validation loss 0.043055739253759384 Accuracy 0.8895000219345093\n",
      "Iteration 25780 Training loss 0.0008183305035345256 Validation loss 0.04304363951086998 Accuracy 0.8895000219345093\n",
      "Iteration 25790 Training loss 0.0005719684413634241 Validation loss 0.043073028326034546 Accuracy 0.8880000114440918\n",
      "Iteration 25800 Training loss 0.0008188040228560567 Validation loss 0.04306412115693092 Accuracy 0.8880000114440918\n",
      "Iteration 25810 Training loss 0.0005670171231031418 Validation loss 0.04307815432548523 Accuracy 0.8884999752044678\n",
      "Iteration 25820 Training loss 0.0005757958861067891 Validation loss 0.04309408366680145 Accuracy 0.8880000114440918\n",
      "Iteration 25830 Training loss 0.0008205443737097085 Validation loss 0.04306903854012489 Accuracy 0.8889999985694885\n",
      "Iteration 25840 Training loss 0.0013168584555387497 Validation loss 0.04310912638902664 Accuracy 0.887499988079071\n",
      "Iteration 25850 Training loss 0.00031618057982996106 Validation loss 0.04308497905731201 Accuracy 0.8889999985694885\n",
      "Iteration 25860 Training loss 0.0005690315738320351 Validation loss 0.04305727407336235 Accuracy 0.8895000219345093\n",
      "Iteration 25870 Training loss 0.0008144070161506534 Validation loss 0.04310259222984314 Accuracy 0.887499988079071\n",
      "Iteration 25880 Training loss 0.0005688577657565475 Validation loss 0.04304966330528259 Accuracy 0.8889999985694885\n",
      "Iteration 25890 Training loss 0.000821489782538265 Validation loss 0.04304944723844528 Accuracy 0.8895000219345093\n",
      "Iteration 25900 Training loss 0.0005710021359845996 Validation loss 0.04305196553468704 Accuracy 0.8884999752044678\n",
      "Iteration 25910 Training loss 0.00031743195722810924 Validation loss 0.043075233697891235 Accuracy 0.8884999752044678\n",
      "Iteration 25920 Training loss 0.0008158762357197702 Validation loss 0.04308461770415306 Accuracy 0.8899999856948853\n",
      "Iteration 25930 Training loss 0.0013213084312155843 Validation loss 0.04307723417878151 Accuracy 0.890500009059906\n",
      "Iteration 25940 Training loss 0.0008180426666513085 Validation loss 0.04309450462460518 Accuracy 0.8889999985694885\n",
      "Iteration 25950 Training loss 0.0010701746214181185 Validation loss 0.043067704886198044 Accuracy 0.8880000114440918\n",
      "Iteration 25960 Training loss 0.0008183243335224688 Validation loss 0.04304815083742142 Accuracy 0.8884999752044678\n",
      "Iteration 25970 Training loss 0.0008217371068894863 Validation loss 0.043031129986047745 Accuracy 0.890500009059906\n",
      "Iteration 25980 Training loss 0.0008174360264092684 Validation loss 0.04308028891682625 Accuracy 0.8880000114440918\n",
      "Iteration 25990 Training loss 0.0008221715106628835 Validation loss 0.04305809736251831 Accuracy 0.8899999856948853\n",
      "Iteration 26000 Training loss 0.001065558404661715 Validation loss 0.043090492486953735 Accuracy 0.8889999985694885\n",
      "Iteration 26010 Training loss 0.0010650980984792113 Validation loss 0.04309408739209175 Accuracy 0.8884999752044678\n",
      "Iteration 26020 Training loss 0.00106307293754071 Validation loss 0.043057750910520554 Accuracy 0.8895000219345093\n",
      "Iteration 26030 Training loss 7.167502917582169e-05 Validation loss 0.04312879592180252 Accuracy 0.887499988079071\n",
      "Iteration 26040 Training loss 0.0018155764555558562 Validation loss 0.04307524114847183 Accuracy 0.8884999752044678\n",
      "Iteration 26050 Training loss 0.001318276161327958 Validation loss 0.043093807995319366 Accuracy 0.8880000114440918\n",
      "Iteration 26060 Training loss 0.00031524867517873645 Validation loss 0.04305297136306763 Accuracy 0.8884999752044678\n",
      "Iteration 26070 Training loss 0.00032040782389231026 Validation loss 0.04311726614832878 Accuracy 0.887499988079071\n",
      "Iteration 26080 Training loss 0.0005748362746089697 Validation loss 0.0430995337665081 Accuracy 0.8880000114440918\n",
      "Iteration 26090 Training loss 0.0005669012316502631 Validation loss 0.0430961549282074 Accuracy 0.8880000114440918\n",
      "Iteration 26100 Training loss 0.0005661071045324206 Validation loss 0.04303766041994095 Accuracy 0.8895000219345093\n",
      "Iteration 26110 Training loss 0.0008164159371517599 Validation loss 0.04309402406215668 Accuracy 0.8880000114440918\n",
      "Iteration 26120 Training loss 0.0013259801780804992 Validation loss 0.04301789030432701 Accuracy 0.8899999856948853\n",
      "Iteration 26130 Training loss 0.0008221810567192733 Validation loss 0.043062299489974976 Accuracy 0.8895000219345093\n",
      "Iteration 26140 Training loss 0.000319196202326566 Validation loss 0.04308480769395828 Accuracy 0.8895000219345093\n",
      "Iteration 26150 Training loss 0.0008195819682441652 Validation loss 0.04310774430632591 Accuracy 0.8880000114440918\n",
      "Iteration 26160 Training loss 0.0010655112564563751 Validation loss 0.04305364191532135 Accuracy 0.8895000219345093\n",
      "Iteration 26170 Training loss 0.0003184691595379263 Validation loss 0.04315996542572975 Accuracy 0.8880000114440918\n",
      "Iteration 26180 Training loss 0.0005712530110031366 Validation loss 0.043091051280498505 Accuracy 0.8889999985694885\n",
      "Iteration 26190 Training loss 0.0010677227983251214 Validation loss 0.043027251958847046 Accuracy 0.890999972820282\n",
      "Iteration 26200 Training loss 0.0008163133752532303 Validation loss 0.04309578612446785 Accuracy 0.8884999752044678\n",
      "Iteration 26210 Training loss 0.0005691591650247574 Validation loss 0.0430949330329895 Accuracy 0.8884999752044678\n",
      "Iteration 26220 Training loss 0.0010667586466297507 Validation loss 0.043057333678007126 Accuracy 0.8895000219345093\n",
      "Iteration 26230 Training loss 0.0013166161952540278 Validation loss 0.04308086261153221 Accuracy 0.8895000219345093\n",
      "Iteration 26240 Training loss 0.00032344262581318617 Validation loss 0.043044861406087875 Accuracy 0.8880000114440918\n",
      "Iteration 26250 Training loss 0.0005720063927583396 Validation loss 0.043063078075647354 Accuracy 0.8889999985694885\n",
      "Iteration 26260 Training loss 0.0005697896122001112 Validation loss 0.04302898049354553 Accuracy 0.8895000219345093\n",
      "Iteration 26270 Training loss 0.00032040858059190214 Validation loss 0.04308703541755676 Accuracy 0.8884999752044678\n",
      "Iteration 26280 Training loss 0.0008157630218192935 Validation loss 0.04314161837100983 Accuracy 0.8880000114440918\n",
      "Iteration 26290 Training loss 0.0013144762488082051 Validation loss 0.043097738176584244 Accuracy 0.8880000114440918\n",
      "Iteration 26300 Training loss 0.0010721511207520962 Validation loss 0.04306717589497566 Accuracy 0.8880000114440918\n",
      "Iteration 26310 Training loss 0.0008220446179620922 Validation loss 0.04305850341916084 Accuracy 0.8884999752044678\n",
      "Iteration 26320 Training loss 0.0008201589807868004 Validation loss 0.04306779429316521 Accuracy 0.8889999985694885\n",
      "Iteration 26330 Training loss 0.0013220562832430005 Validation loss 0.04303201287984848 Accuracy 0.8899999856948853\n",
      "Iteration 26340 Training loss 0.0005716125015169382 Validation loss 0.04305250570178032 Accuracy 0.8889999985694885\n",
      "Iteration 26350 Training loss 0.000323036772897467 Validation loss 0.04301515966653824 Accuracy 0.890500009059906\n",
      "Iteration 26360 Training loss 0.0008230358362197876 Validation loss 0.0430784597992897 Accuracy 0.8895000219345093\n",
      "Iteration 26370 Training loss 0.0010709811467677355 Validation loss 0.04308605566620827 Accuracy 0.8889999985694885\n",
      "Iteration 26380 Training loss 0.0013189568417146802 Validation loss 0.043013617396354675 Accuracy 0.890500009059906\n",
      "Iteration 26390 Training loss 0.0015703070675954223 Validation loss 0.043066903948783875 Accuracy 0.8889999985694885\n",
      "Iteration 26400 Training loss 0.0005668012308888137 Validation loss 0.04310741275548935 Accuracy 0.8889999985694885\n",
      "Iteration 26410 Training loss 0.0008233190746977925 Validation loss 0.04310395196080208 Accuracy 0.8884999752044678\n",
      "Iteration 26420 Training loss 0.0008206411730498075 Validation loss 0.04305581748485565 Accuracy 0.8899999856948853\n",
      "Iteration 26430 Training loss 0.0013171115424484015 Validation loss 0.04305414482951164 Accuracy 0.8899999856948853\n",
      "Iteration 26440 Training loss 0.0008215981652028859 Validation loss 0.04303453117609024 Accuracy 0.8895000219345093\n",
      "Iteration 26450 Training loss 0.0008175954571925104 Validation loss 0.043122366070747375 Accuracy 0.8884999752044678\n",
      "Iteration 26460 Training loss 0.0013231717748567462 Validation loss 0.0430557020008564 Accuracy 0.8895000219345093\n",
      "Iteration 26470 Training loss 0.0005734672886319458 Validation loss 0.043098334223032 Accuracy 0.8884999752044678\n",
      "Iteration 26480 Training loss 0.00107227370608598 Validation loss 0.043105702847242355 Accuracy 0.8884999752044678\n",
      "Iteration 26490 Training loss 0.0008193259127438068 Validation loss 0.04308284819126129 Accuracy 0.8884999752044678\n",
      "Iteration 26500 Training loss 0.001320409937761724 Validation loss 0.043057993054389954 Accuracy 0.8895000219345093\n",
      "Iteration 26510 Training loss 0.001063469098880887 Validation loss 0.04310380667448044 Accuracy 0.8884999752044678\n",
      "Iteration 26520 Training loss 0.001072620041668415 Validation loss 0.04309239238500595 Accuracy 0.8884999752044678\n",
      "Iteration 26530 Training loss 0.0005711083649657667 Validation loss 0.04310700297355652 Accuracy 0.8884999752044678\n",
      "Iteration 26540 Training loss 0.001568297273479402 Validation loss 0.043035414069890976 Accuracy 0.8889999985694885\n",
      "Iteration 26550 Training loss 0.00031645168201066554 Validation loss 0.0430799201130867 Accuracy 0.8889999985694885\n",
      "Iteration 26560 Training loss 0.0003137910971418023 Validation loss 0.043046578764915466 Accuracy 0.8899999856948853\n",
      "Iteration 26570 Training loss 0.0018165132496505976 Validation loss 0.04304018244147301 Accuracy 0.8889999985694885\n",
      "Iteration 26580 Training loss 0.0013227281160652637 Validation loss 0.043064817786216736 Accuracy 0.8895000219345093\n",
      "Iteration 26590 Training loss 0.0008178906282410026 Validation loss 0.043056588619947433 Accuracy 0.8899999856948853\n",
      "Iteration 26600 Training loss 0.0015661893412470818 Validation loss 0.04305148869752884 Accuracy 0.8889999985694885\n",
      "Iteration 26610 Training loss 0.0008186394697986543 Validation loss 0.043071448802948 Accuracy 0.8884999752044678\n",
      "Iteration 26620 Training loss 0.0008224124321714044 Validation loss 0.043042514473199844 Accuracy 0.8889999985694885\n",
      "Iteration 26630 Training loss 0.0013213260099291801 Validation loss 0.043042901903390884 Accuracy 0.8889999985694885\n",
      "Iteration 26640 Training loss 0.0018219372723251581 Validation loss 0.04305553436279297 Accuracy 0.8889999985694885\n",
      "Iteration 26650 Training loss 0.00031623622635379434 Validation loss 0.04309820383787155 Accuracy 0.8884999752044678\n",
      "Iteration 26660 Training loss 0.0008187048370018601 Validation loss 0.043101899325847626 Accuracy 0.8884999752044678\n",
      "Iteration 26670 Training loss 0.0018230932764708996 Validation loss 0.04309767112135887 Accuracy 0.8889999985694885\n",
      "Iteration 26680 Training loss 0.0010662825079634786 Validation loss 0.04309060797095299 Accuracy 0.8889999985694885\n",
      "Iteration 26690 Training loss 0.0015666799154132605 Validation loss 0.043071821331977844 Accuracy 0.8884999752044678\n",
      "Iteration 26700 Training loss 0.0005702052148990333 Validation loss 0.04304277151823044 Accuracy 0.8884999752044678\n",
      "Iteration 26710 Training loss 0.0005685334908775985 Validation loss 0.04306131973862648 Accuracy 0.8895000219345093\n",
      "Iteration 26720 Training loss 0.0005690532852895558 Validation loss 0.04305412247776985 Accuracy 0.8895000219345093\n",
      "Iteration 26730 Training loss 0.0005729973199777305 Validation loss 0.04305866360664368 Accuracy 0.8895000219345093\n",
      "Iteration 26740 Training loss 0.0008080276311375201 Validation loss 0.04304962977766991 Accuracy 0.8884999752044678\n",
      "Iteration 26750 Training loss 0.0008176857954822481 Validation loss 0.043050795793533325 Accuracy 0.8895000219345093\n",
      "Iteration 26760 Training loss 0.001568373991176486 Validation loss 0.043068766593933105 Accuracy 0.8889999985694885\n",
      "Iteration 26770 Training loss 0.0008218793664127588 Validation loss 0.043031882494688034 Accuracy 0.8884999752044678\n",
      "Iteration 26780 Training loss 0.0008188043721020222 Validation loss 0.043031468987464905 Accuracy 0.8895000219345093\n",
      "Iteration 26790 Training loss 0.0008190404041670263 Validation loss 0.04301552101969719 Accuracy 0.8895000219345093\n",
      "Iteration 26800 Training loss 0.0010664656292647123 Validation loss 0.04304962232708931 Accuracy 0.8884999752044678\n",
      "Iteration 26810 Training loss 0.0008215605048462749 Validation loss 0.04305514693260193 Accuracy 0.8884999752044678\n",
      "Iteration 26820 Training loss 0.0005612337263301015 Validation loss 0.04306231439113617 Accuracy 0.8884999752044678\n",
      "Iteration 26830 Training loss 0.0010753963142633438 Validation loss 0.04308800771832466 Accuracy 0.8889999985694885\n",
      "Iteration 26840 Training loss 0.0005696004955098033 Validation loss 0.043078064918518066 Accuracy 0.8889999985694885\n",
      "Iteration 26850 Training loss 0.001070187776349485 Validation loss 0.04302584379911423 Accuracy 0.8889999985694885\n",
      "Iteration 26860 Training loss 0.0020681750029325485 Validation loss 0.04310043901205063 Accuracy 0.8889999985694885\n",
      "Iteration 26870 Training loss 0.00031709755421616137 Validation loss 0.043047476559877396 Accuracy 0.8889999985694885\n",
      "Iteration 26880 Training loss 0.0005698089953511953 Validation loss 0.04301143065094948 Accuracy 0.8889999985694885\n",
      "Iteration 26890 Training loss 0.0010642718989402056 Validation loss 0.043061066418886185 Accuracy 0.8889999985694885\n",
      "Iteration 26900 Training loss 0.0005702442722395062 Validation loss 0.04306744411587715 Accuracy 0.8889999985694885\n",
      "Iteration 26910 Training loss 0.0010713988449424505 Validation loss 0.04308434948325157 Accuracy 0.8889999985694885\n",
      "Iteration 26920 Training loss 0.0005642845062538981 Validation loss 0.04305248335003853 Accuracy 0.8884999752044678\n",
      "Iteration 26930 Training loss 0.0008193582179956138 Validation loss 0.04305853694677353 Accuracy 0.8889999985694885\n",
      "Iteration 26940 Training loss 0.0008214779081754386 Validation loss 0.04306423291563988 Accuracy 0.8889999985694885\n",
      "Iteration 26950 Training loss 0.0013143165269866586 Validation loss 0.04306749254465103 Accuracy 0.8884999752044678\n",
      "Iteration 26960 Training loss 0.0005712999263778329 Validation loss 0.043135467916727066 Accuracy 0.8889999985694885\n",
      "Iteration 26970 Training loss 0.0008269496611319482 Validation loss 0.043083373457193375 Accuracy 0.8884999752044678\n",
      "Iteration 26980 Training loss 0.0008145114406943321 Validation loss 0.04305296018719673 Accuracy 0.8895000219345093\n",
      "Iteration 26990 Training loss 0.0008168047643266618 Validation loss 0.04306650161743164 Accuracy 0.8884999752044678\n",
      "Iteration 27000 Training loss 0.0015710039297118783 Validation loss 0.04306101053953171 Accuracy 0.8884999752044678\n",
      "Iteration 27010 Training loss 0.0008172341040335596 Validation loss 0.04301224648952484 Accuracy 0.8899999856948853\n",
      "Iteration 27020 Training loss 0.0010687295580282807 Validation loss 0.0430738739669323 Accuracy 0.8889999985694885\n",
      "Iteration 27030 Training loss 0.0005667494260706007 Validation loss 0.04306742176413536 Accuracy 0.8884999752044678\n",
      "Iteration 27040 Training loss 0.0008238297305069864 Validation loss 0.04298834130167961 Accuracy 0.8899999856948853\n",
      "Iteration 27050 Training loss 0.00031541523640044034 Validation loss 0.04302768409252167 Accuracy 0.8895000219345093\n",
      "Iteration 27060 Training loss 0.0010695339879021049 Validation loss 0.04302607476711273 Accuracy 0.890500009059906\n",
      "Iteration 27070 Training loss 0.0010655939113348722 Validation loss 0.04302584007382393 Accuracy 0.8895000219345093\n",
      "Iteration 27080 Training loss 0.0005691912374459207 Validation loss 0.04307282343506813 Accuracy 0.8889999985694885\n",
      "Iteration 27090 Training loss 0.0003180408966727555 Validation loss 0.0430767722427845 Accuracy 0.8884999752044678\n",
      "Iteration 27100 Training loss 0.0015680432552471757 Validation loss 0.04306042194366455 Accuracy 0.8884999752044678\n",
      "Iteration 27110 Training loss 0.000820911256596446 Validation loss 0.04305761680006981 Accuracy 0.8899999856948853\n",
      "Iteration 27120 Training loss 0.0008162970189005136 Validation loss 0.04305135831236839 Accuracy 0.8889999985694885\n",
      "Iteration 27130 Training loss 0.0013153369072824717 Validation loss 0.043068528175354004 Accuracy 0.8895000219345093\n",
      "Iteration 27140 Training loss 0.0008132572402246296 Validation loss 0.04306755214929581 Accuracy 0.8889999985694885\n",
      "Iteration 27150 Training loss 0.000569213880226016 Validation loss 0.043025679886341095 Accuracy 0.8889999985694885\n",
      "Iteration 27160 Training loss 0.0005718608153983951 Validation loss 0.043049123138189316 Accuracy 0.8889999985694885\n",
      "Iteration 27170 Training loss 0.00056889170082286 Validation loss 0.043028123676776886 Accuracy 0.8884999752044678\n",
      "Iteration 27180 Training loss 0.00031985764508135617 Validation loss 0.04300237074494362 Accuracy 0.8899999856948853\n",
      "Iteration 27190 Training loss 0.0018165112705901265 Validation loss 0.043047308921813965 Accuracy 0.8889999985694885\n",
      "Iteration 27200 Training loss 0.0008206283091567457 Validation loss 0.04306000471115112 Accuracy 0.8889999985694885\n",
      "Iteration 27210 Training loss 0.0003180167404934764 Validation loss 0.04302468150854111 Accuracy 0.8884999752044678\n",
      "Iteration 27220 Training loss 0.00056597706861794 Validation loss 0.04302090406417847 Accuracy 0.8889999985694885\n",
      "Iteration 27230 Training loss 0.0008183292229659855 Validation loss 0.04303095489740372 Accuracy 0.8884999752044678\n",
      "Iteration 27240 Training loss 6.671976734651253e-05 Validation loss 0.043039437383413315 Accuracy 0.8895000219345093\n",
      "Iteration 27250 Training loss 0.0008195507689379156 Validation loss 0.043022941797971725 Accuracy 0.8895000219345093\n",
      "Iteration 27260 Training loss 0.0008152598165906966 Validation loss 0.043005652725696564 Accuracy 0.8895000219345093\n",
      "Iteration 27270 Training loss 0.0010708438931033015 Validation loss 0.04305451363325119 Accuracy 0.8889999985694885\n",
      "Iteration 27280 Training loss 0.0018159050960093737 Validation loss 0.04306215047836304 Accuracy 0.8895000219345093\n",
      "Iteration 27290 Training loss 0.0010688744951039553 Validation loss 0.043068643659353256 Accuracy 0.8895000219345093\n",
      "Iteration 27300 Training loss 0.0005679815076291561 Validation loss 0.04303998500108719 Accuracy 0.8899999856948853\n",
      "Iteration 27310 Training loss 0.0010692293290048838 Validation loss 0.04305858165025711 Accuracy 0.8889999985694885\n",
      "Iteration 27320 Training loss 0.0003203132364433259 Validation loss 0.04303077235817909 Accuracy 0.8884999752044678\n",
      "Iteration 27330 Training loss 0.0005720810731872916 Validation loss 0.04308810085058212 Accuracy 0.8889999985694885\n",
      "Iteration 27340 Training loss 0.0018139408202841878 Validation loss 0.04302067682147026 Accuracy 0.8899999856948853\n",
      "Iteration 27350 Training loss 0.0005643371259793639 Validation loss 0.04307015240192413 Accuracy 0.8889999985694885\n",
      "Iteration 27360 Training loss 0.0008149680215865374 Validation loss 0.04304294288158417 Accuracy 0.8899999856948853\n",
      "Iteration 27370 Training loss 0.0003130338154733181 Validation loss 0.043049391359090805 Accuracy 0.8895000219345093\n",
      "Iteration 27380 Training loss 0.001571660628542304 Validation loss 0.04306389391422272 Accuracy 0.890500009059906\n",
      "Iteration 27390 Training loss 7.156348874559626e-05 Validation loss 0.04302356764674187 Accuracy 0.8889999985694885\n",
      "Iteration 27400 Training loss 0.000318741804221645 Validation loss 0.04307233914732933 Accuracy 0.8889999985694885\n",
      "Iteration 27410 Training loss 0.0008220697054639459 Validation loss 0.043019089847803116 Accuracy 0.8895000219345093\n",
      "Iteration 27420 Training loss 0.0013223446439951658 Validation loss 0.04304959252476692 Accuracy 0.8889999985694885\n",
      "Iteration 27430 Training loss 0.001819287077523768 Validation loss 0.04299544915556908 Accuracy 0.8899999856948853\n",
      "Iteration 27440 Training loss 0.000571002543438226 Validation loss 0.04300471022725105 Accuracy 0.8899999856948853\n",
      "Iteration 27450 Training loss 0.0008156276890076697 Validation loss 0.043014418333768845 Accuracy 0.8895000219345093\n",
      "Iteration 27460 Training loss 0.0010720054851844907 Validation loss 0.04303097352385521 Accuracy 0.8889999985694885\n",
      "Iteration 27470 Training loss 0.0003149673284497112 Validation loss 0.04305063560605049 Accuracy 0.8889999985694885\n",
      "Iteration 27480 Training loss 0.0005687240627594292 Validation loss 0.04308580979704857 Accuracy 0.8889999985694885\n",
      "Iteration 27490 Training loss 0.000568679824937135 Validation loss 0.04300747811794281 Accuracy 0.8899999856948853\n",
      "Iteration 27500 Training loss 0.0018207260873168707 Validation loss 0.04305967688560486 Accuracy 0.8895000219345093\n",
      "Iteration 27510 Training loss 0.0005684942589141428 Validation loss 0.04305453225970268 Accuracy 0.8899999856948853\n",
      "Iteration 27520 Training loss 0.0005715189618058503 Validation loss 0.043046653270721436 Accuracy 0.8889999985694885\n",
      "Iteration 27530 Training loss 0.0003218864731024951 Validation loss 0.04306719824671745 Accuracy 0.8889999985694885\n",
      "Iteration 27540 Training loss 0.0008151767542585731 Validation loss 0.04303354397416115 Accuracy 0.8895000219345093\n",
      "Iteration 27550 Training loss 0.0003202862571924925 Validation loss 0.04304873198270798 Accuracy 0.8889999985694885\n",
      "Iteration 27560 Training loss 0.001315309782512486 Validation loss 0.04308195412158966 Accuracy 0.8889999985694885\n",
      "Iteration 27570 Training loss 0.00032410171115770936 Validation loss 0.043092746287584305 Accuracy 0.8895000219345093\n",
      "Iteration 27580 Training loss 0.0010681397980079055 Validation loss 0.04308281093835831 Accuracy 0.8895000219345093\n",
      "Iteration 27590 Training loss 0.0025725222658365965 Validation loss 0.04305814579129219 Accuracy 0.8889999985694885\n",
      "Iteration 27600 Training loss 0.001319828792475164 Validation loss 0.04304979369044304 Accuracy 0.8899999856948853\n",
      "Iteration 27610 Training loss 0.0008153889211826026 Validation loss 0.04303327575325966 Accuracy 0.8899999856948853\n",
      "Iteration 27620 Training loss 0.0008182104211300611 Validation loss 0.043037109076976776 Accuracy 0.8895000219345093\n",
      "Iteration 27630 Training loss 0.0008170502842403948 Validation loss 0.04303651303052902 Accuracy 0.8889999985694885\n",
      "Iteration 27640 Training loss 0.001067174132913351 Validation loss 0.04306156188249588 Accuracy 0.8889999985694885\n",
      "Iteration 27650 Training loss 0.001071005710400641 Validation loss 0.043037887662649155 Accuracy 0.8895000219345093\n",
      "Iteration 27660 Training loss 0.001321083982475102 Validation loss 0.043021585792303085 Accuracy 0.8889999985694885\n",
      "Iteration 27670 Training loss 0.0010711996583268046 Validation loss 0.04304667189717293 Accuracy 0.8899999856948853\n",
      "Iteration 27680 Training loss 0.0005685306969098747 Validation loss 0.043018851429224014 Accuracy 0.8895000219345093\n",
      "Iteration 27690 Training loss 0.0013223632704466581 Validation loss 0.043042946606874466 Accuracy 0.8895000219345093\n",
      "Iteration 27700 Training loss 0.001572468550875783 Validation loss 0.04305963218212128 Accuracy 0.8889999985694885\n",
      "Iteration 27710 Training loss 6.782508717151359e-05 Validation loss 0.04308216646313667 Accuracy 0.8889999985694885\n",
      "Iteration 27720 Training loss 0.0005706616211682558 Validation loss 0.04310605674982071 Accuracy 0.8889999985694885\n",
      "Iteration 27730 Training loss 0.0005696351290680468 Validation loss 0.04307287558913231 Accuracy 0.8895000219345093\n",
      "Iteration 27740 Training loss 0.0005692637641914189 Validation loss 0.04312478378415108 Accuracy 0.8889999985694885\n",
      "Iteration 27750 Training loss 0.0018208607798442245 Validation loss 0.043069060891866684 Accuracy 0.8899999856948853\n",
      "Iteration 27760 Training loss 0.0008208040962927043 Validation loss 0.04303975775837898 Accuracy 0.8895000219345093\n",
      "Iteration 27770 Training loss 0.0010730210924521089 Validation loss 0.0430755540728569 Accuracy 0.8895000219345093\n",
      "Iteration 27780 Training loss 0.0010659482795745134 Validation loss 0.043070025742053986 Accuracy 0.8889999985694885\n",
      "Iteration 27790 Training loss 0.0008166934712789953 Validation loss 0.04306735098361969 Accuracy 0.8889999985694885\n",
      "Iteration 27800 Training loss 6.785375444451347e-05 Validation loss 0.04305137321352959 Accuracy 0.890500009059906\n",
      "Iteration 27810 Training loss 0.0010619127424433827 Validation loss 0.043063849210739136 Accuracy 0.8889999985694885\n",
      "Iteration 27820 Training loss 0.0010647218441590667 Validation loss 0.04303750768303871 Accuracy 0.8895000219345093\n",
      "Iteration 27830 Training loss 0.0005682336632162333 Validation loss 0.0430278517305851 Accuracy 0.8895000219345093\n",
      "Iteration 27840 Training loss 0.0013181944377720356 Validation loss 0.04307576268911362 Accuracy 0.8889999985694885\n",
      "Iteration 27850 Training loss 0.0010709366761147976 Validation loss 0.04304959252476692 Accuracy 0.8895000219345093\n",
      "Iteration 27860 Training loss 0.0013209725730121136 Validation loss 0.043091803789138794 Accuracy 0.8899999856948853\n",
      "Iteration 27870 Training loss 0.0008224915363825858 Validation loss 0.0430830754339695 Accuracy 0.8889999985694885\n",
      "Iteration 27880 Training loss 0.0015704393154010177 Validation loss 0.04306534305214882 Accuracy 0.8895000219345093\n",
      "Iteration 27890 Training loss 0.0010693763615563512 Validation loss 0.04307198151946068 Accuracy 0.8889999985694885\n",
      "Iteration 27900 Training loss 0.001819627359509468 Validation loss 0.04304913431406021 Accuracy 0.8889999985694885\n",
      "Iteration 27910 Training loss 0.0010696782264858484 Validation loss 0.04305456951260567 Accuracy 0.8889999985694885\n",
      "Iteration 27920 Training loss 6.734918133588508e-05 Validation loss 0.04306168481707573 Accuracy 0.8889999985694885\n",
      "Iteration 27930 Training loss 0.0013225226430222392 Validation loss 0.04309786111116409 Accuracy 0.8889999985694885\n",
      "Iteration 27940 Training loss 0.0008222000324167311 Validation loss 0.043063849210739136 Accuracy 0.8899999856948853\n",
      "Iteration 27950 Training loss 0.0010679280385375023 Validation loss 0.04307442530989647 Accuracy 0.8889999985694885\n",
      "Iteration 27960 Training loss 0.001069805002771318 Validation loss 0.04306798428297043 Accuracy 0.8889999985694885\n",
      "Iteration 27970 Training loss 0.0013217008672654629 Validation loss 0.043084219098091125 Accuracy 0.8889999985694885\n",
      "Iteration 27980 Training loss 0.0018215029267594218 Validation loss 0.04309844225645065 Accuracy 0.8889999985694885\n",
      "Iteration 27990 Training loss 0.0013192890910431743 Validation loss 0.04302603006362915 Accuracy 0.8899999856948853\n",
      "Iteration 28000 Training loss 6.581137131433934e-05 Validation loss 0.04306047037243843 Accuracy 0.8895000219345093\n",
      "Iteration 28010 Training loss 0.001063150237314403 Validation loss 0.043097373098134995 Accuracy 0.8895000219345093\n",
      "Iteration 28020 Training loss 0.0010694744996726513 Validation loss 0.04310598969459534 Accuracy 0.8889999985694885\n",
      "Iteration 28030 Training loss 0.0005708251846954226 Validation loss 0.04300806298851967 Accuracy 0.8895000219345093\n",
      "Iteration 28040 Training loss 0.0010663473512977362 Validation loss 0.0430813767015934 Accuracy 0.8889999985694885\n",
      "Iteration 28050 Training loss 0.0015707612037658691 Validation loss 0.043034810572862625 Accuracy 0.8889999985694885\n",
      "Iteration 28060 Training loss 0.0013143954565748572 Validation loss 0.0430612787604332 Accuracy 0.8889999985694885\n",
      "Iteration 28070 Training loss 0.0010693379445001483 Validation loss 0.04302150011062622 Accuracy 0.890500009059906\n",
      "Iteration 28080 Training loss 0.0010698226979002357 Validation loss 0.04306637868285179 Accuracy 0.8889999985694885\n",
      "Iteration 28090 Training loss 0.0005676394212059677 Validation loss 0.043020233511924744 Accuracy 0.8884999752044678\n",
      "Iteration 28100 Training loss 0.0003237590426579118 Validation loss 0.04309769347310066 Accuracy 0.8895000219345093\n",
      "Iteration 28110 Training loss 0.001073364051990211 Validation loss 0.04305245727300644 Accuracy 0.8889999985694885\n",
      "Iteration 28120 Training loss 0.001320539740845561 Validation loss 0.04304226487874985 Accuracy 0.8889999985694885\n",
      "Iteration 28130 Training loss 0.0010688470210880041 Validation loss 0.04305995628237724 Accuracy 0.8889999985694885\n",
      "Iteration 28140 Training loss 0.0003214517200831324 Validation loss 0.043055273592472076 Accuracy 0.8889999985694885\n",
      "Iteration 28150 Training loss 0.0008178467396646738 Validation loss 0.04303175210952759 Accuracy 0.8899999856948853\n",
      "Iteration 28160 Training loss 0.000817241903860122 Validation loss 0.04304491728544235 Accuracy 0.890500009059906\n",
      "Iteration 28170 Training loss 0.0010661585256457329 Validation loss 0.04306521639227867 Accuracy 0.8895000219345093\n",
      "Iteration 28180 Training loss 0.0005686260410584509 Validation loss 0.043042030185461044 Accuracy 0.8895000219345093\n",
      "Iteration 28190 Training loss 0.001070182304829359 Validation loss 0.04301745817065239 Accuracy 0.8899999856948853\n",
      "Iteration 28200 Training loss 0.0008206041529774666 Validation loss 0.04310975223779678 Accuracy 0.8889999985694885\n",
      "Iteration 28210 Training loss 0.0013145370176061988 Validation loss 0.04306716099381447 Accuracy 0.8889999985694885\n",
      "Iteration 28220 Training loss 0.00031867221696302295 Validation loss 0.04307517781853676 Accuracy 0.8889999985694885\n",
      "Iteration 28230 Training loss 0.0003219220379833132 Validation loss 0.04309698939323425 Accuracy 0.8889999985694885\n",
      "Iteration 28240 Training loss 0.0013193291379138827 Validation loss 0.04311926290392876 Accuracy 0.8889999985694885\n",
      "Iteration 28250 Training loss 0.0008185053011402488 Validation loss 0.04299475997686386 Accuracy 0.8899999856948853\n",
      "Iteration 28260 Training loss 0.0008230209350585938 Validation loss 0.04305613785982132 Accuracy 0.8889999985694885\n",
      "Iteration 28270 Training loss 0.0013203187845647335 Validation loss 0.04304823651909828 Accuracy 0.8899999856948853\n",
      "Iteration 28280 Training loss 0.0023183946032077074 Validation loss 0.04306522384285927 Accuracy 0.8889999985694885\n",
      "Iteration 28290 Training loss 0.0005639809533022344 Validation loss 0.04304572939872742 Accuracy 0.8889999985694885\n",
      "Iteration 28300 Training loss 0.0003183382796123624 Validation loss 0.04302755743265152 Accuracy 0.8884999752044678\n",
      "Iteration 28310 Training loss 0.001068811397999525 Validation loss 0.04302506893873215 Accuracy 0.8899999856948853\n",
      "Iteration 28320 Training loss 0.0005663529736921191 Validation loss 0.04303784295916557 Accuracy 0.8899999856948853\n",
      "Iteration 28330 Training loss 7.139265653677285e-05 Validation loss 0.04304885491728783 Accuracy 0.8889999985694885\n",
      "Iteration 28340 Training loss 0.0010671747149899602 Validation loss 0.04306413233280182 Accuracy 0.8895000219345093\n",
      "Iteration 28350 Training loss 0.0005698535242117941 Validation loss 0.043027304112911224 Accuracy 0.890500009059906\n",
      "Iteration 28360 Training loss 0.00031864031916484237 Validation loss 0.04299880191683769 Accuracy 0.8899999856948853\n",
      "Iteration 28370 Training loss 0.0010667989263311028 Validation loss 0.043028056621551514 Accuracy 0.890999972820282\n",
      "Iteration 28380 Training loss 0.0005718789179809391 Validation loss 0.04303688183426857 Accuracy 0.890500009059906\n",
      "Iteration 28390 Training loss 0.0010667741298675537 Validation loss 0.04307214915752411 Accuracy 0.8889999985694885\n",
      "Iteration 28400 Training loss 0.0013186900177970529 Validation loss 0.04306137189269066 Accuracy 0.8889999985694885\n",
      "Iteration 28410 Training loss 0.0008197476272471249 Validation loss 0.04303072392940521 Accuracy 0.8899999856948853\n",
      "Iteration 28420 Training loss 0.0008187215426005423 Validation loss 0.043087348341941833 Accuracy 0.8889999985694885\n",
      "Iteration 28430 Training loss 0.0005729600670747459 Validation loss 0.04309724271297455 Accuracy 0.8889999985694885\n",
      "Iteration 28440 Training loss 0.0005686308140866458 Validation loss 0.04301551729440689 Accuracy 0.8899999856948853\n",
      "Iteration 28450 Training loss 0.0008150138892233372 Validation loss 0.043039366602897644 Accuracy 0.8895000219345093\n",
      "Iteration 28460 Training loss 0.0008193835383281112 Validation loss 0.04305823892354965 Accuracy 0.8889999985694885\n",
      "Iteration 28470 Training loss 0.0008199389558285475 Validation loss 0.04301396757364273 Accuracy 0.890999972820282\n",
      "Iteration 28480 Training loss 0.0010718803387135267 Validation loss 0.04302056133747101 Accuracy 0.8899999856948853\n",
      "Iteration 28490 Training loss 0.0008185178157873452 Validation loss 0.04304531216621399 Accuracy 0.8889999985694885\n",
      "Iteration 28500 Training loss 0.0010756823467090726 Validation loss 0.04305705428123474 Accuracy 0.8889999985694885\n",
      "Iteration 28510 Training loss 0.0008188644424080849 Validation loss 0.04301147162914276 Accuracy 0.890500009059906\n",
      "Iteration 28520 Training loss 0.0008239203016273677 Validation loss 0.043021515011787415 Accuracy 0.8899999856948853\n",
      "Iteration 28530 Training loss 0.0005700243636965752 Validation loss 0.04305362328886986 Accuracy 0.8889999985694885\n",
      "Iteration 28540 Training loss 0.0008173002279363573 Validation loss 0.04304089397192001 Accuracy 0.8895000219345093\n",
      "Iteration 28550 Training loss 0.0008186607155948877 Validation loss 0.04303738847374916 Accuracy 0.890500009059906\n",
      "Iteration 28560 Training loss 0.0008144309977069497 Validation loss 0.04306330904364586 Accuracy 0.8895000219345093\n",
      "Iteration 28570 Training loss 0.0008207607897929847 Validation loss 0.04303700104355812 Accuracy 0.8895000219345093\n",
      "Iteration 28580 Training loss 0.0010696506360545754 Validation loss 0.04300573840737343 Accuracy 0.890500009059906\n",
      "Iteration 28590 Training loss 0.0008245599456131458 Validation loss 0.04299907758831978 Accuracy 0.8889999985694885\n",
      "Iteration 28600 Training loss 0.0013200687244534492 Validation loss 0.04294920712709427 Accuracy 0.890500009059906\n",
      "Iteration 28610 Training loss 0.0008152620284818113 Validation loss 0.0430196113884449 Accuracy 0.8899999856948853\n",
      "Iteration 28620 Training loss 0.0005730650736950338 Validation loss 0.043010372668504715 Accuracy 0.8895000219345093\n",
      "Iteration 28630 Training loss 0.0005650430102832615 Validation loss 0.043034110218286514 Accuracy 0.8889999985694885\n",
      "Iteration 28640 Training loss 0.0008189306245185435 Validation loss 0.04306485876441002 Accuracy 0.8889999985694885\n",
      "Iteration 28650 Training loss 0.0010687479516491294 Validation loss 0.04307591915130615 Accuracy 0.8889999985694885\n",
      "Iteration 28660 Training loss 0.0008246017387136817 Validation loss 0.04310327768325806 Accuracy 0.8895000219345093\n",
      "Iteration 28670 Training loss 0.0008177635027095675 Validation loss 0.04304257780313492 Accuracy 0.8889999985694885\n",
      "Iteration 28680 Training loss 0.0013264722656458616 Validation loss 0.042995765805244446 Accuracy 0.890500009059906\n",
      "Iteration 28690 Training loss 0.0008182653109543025 Validation loss 0.04303448647260666 Accuracy 0.8899999856948853\n",
      "Iteration 28700 Training loss 0.000566636212170124 Validation loss 0.043004050850868225 Accuracy 0.8899999856948853\n",
      "Iteration 28710 Training loss 0.0005731538403779268 Validation loss 0.04302074387669563 Accuracy 0.8884999752044678\n",
      "Iteration 28720 Training loss 0.001075160689651966 Validation loss 0.04301750287413597 Accuracy 0.8899999856948853\n",
      "Iteration 28730 Training loss 0.0010682549327611923 Validation loss 0.043007124215364456 Accuracy 0.890999972820282\n",
      "Iteration 28740 Training loss 0.000823860231321305 Validation loss 0.04305091127753258 Accuracy 0.8899999856948853\n",
      "Iteration 28750 Training loss 0.00032175646629184484 Validation loss 0.042967457324266434 Accuracy 0.890500009059906\n",
      "Iteration 28760 Training loss 0.00031751568894833326 Validation loss 0.043051812797784805 Accuracy 0.8889999985694885\n",
      "Iteration 28770 Training loss 0.0005634370027109981 Validation loss 0.043035078793764114 Accuracy 0.8889999985694885\n",
      "Iteration 28780 Training loss 0.0005674473359249532 Validation loss 0.043036896735429764 Accuracy 0.8889999985694885\n",
      "Iteration 28790 Training loss 0.0005767817492596805 Validation loss 0.04299597069621086 Accuracy 0.8899999856948853\n",
      "Iteration 28800 Training loss 0.0010718737030401826 Validation loss 0.04306397587060928 Accuracy 0.8889999985694885\n",
      "Iteration 28810 Training loss 0.0013167784782126546 Validation loss 0.043054383248090744 Accuracy 0.8895000219345093\n",
      "Iteration 28820 Training loss 0.00031932888668961823 Validation loss 0.043047115206718445 Accuracy 0.8889999985694885\n",
      "Iteration 28830 Training loss 0.0013150133891031146 Validation loss 0.04302796348929405 Accuracy 0.8899999856948853\n",
      "Iteration 28840 Training loss 0.0010698680998757482 Validation loss 0.04307674989104271 Accuracy 0.8884999752044678\n",
      "Iteration 28850 Training loss 0.0008176820119842887 Validation loss 0.043062057346105576 Accuracy 0.8899999856948853\n",
      "Iteration 28860 Training loss 0.0005696299485862255 Validation loss 0.0430440716445446 Accuracy 0.890500009059906\n",
      "Iteration 28870 Training loss 0.0008199485600925982 Validation loss 0.04306749254465103 Accuracy 0.8889999985694885\n",
      "Iteration 28880 Training loss 0.0005642595933750272 Validation loss 0.043059706687927246 Accuracy 0.8899999856948853\n",
      "Iteration 28890 Training loss 0.0005688929813914001 Validation loss 0.04307326301932335 Accuracy 0.8889999985694885\n",
      "Iteration 28900 Training loss 0.0005622341996058822 Validation loss 0.043029170483350754 Accuracy 0.8895000219345093\n",
      "Iteration 28910 Training loss 0.0020669582299888134 Validation loss 0.04307745024561882 Accuracy 0.8889999985694885\n",
      "Iteration 28920 Training loss 0.0005674531566910446 Validation loss 0.043081823736429214 Accuracy 0.8889999985694885\n",
      "Iteration 28930 Training loss 0.0005701961345039308 Validation loss 0.04310139641165733 Accuracy 0.8899999856948853\n",
      "Iteration 28940 Training loss 0.0008210696396417916 Validation loss 0.043074820190668106 Accuracy 0.8889999985694885\n",
      "Iteration 28950 Training loss 0.0018205036176368594 Validation loss 0.04310498759150505 Accuracy 0.8895000219345093\n",
      "Iteration 28960 Training loss 0.0008218922885134816 Validation loss 0.04304686561226845 Accuracy 0.8899999856948853\n",
      "Iteration 28970 Training loss 0.0013198268134146929 Validation loss 0.04305829852819443 Accuracy 0.890500009059906\n",
      "Iteration 28980 Training loss 0.0008235661662183702 Validation loss 0.04306595027446747 Accuracy 0.8895000219345093\n",
      "Iteration 28990 Training loss 0.0005708742537535727 Validation loss 0.04306095093488693 Accuracy 0.8899999856948853\n",
      "Iteration 29000 Training loss 0.0005662601906806231 Validation loss 0.043076127767562866 Accuracy 0.8895000219345093\n",
      "Iteration 29010 Training loss 0.0008232885738834739 Validation loss 0.04305656999349594 Accuracy 0.8895000219345093\n",
      "Iteration 29020 Training loss 0.0013151332968845963 Validation loss 0.04307698458433151 Accuracy 0.8899999856948853\n",
      "Iteration 29030 Training loss 0.0005745278322137892 Validation loss 0.04305022582411766 Accuracy 0.8899999856948853\n",
      "Iteration 29040 Training loss 0.0008230013772845268 Validation loss 0.043092988431453705 Accuracy 0.8895000219345093\n",
      "Iteration 29050 Training loss 0.000564544228836894 Validation loss 0.043044161051511765 Accuracy 0.8895000219345093\n",
      "Iteration 29060 Training loss 0.0005691564292646945 Validation loss 0.043090611696243286 Accuracy 0.8889999985694885\n",
      "Iteration 29070 Training loss 7.024451770121232e-05 Validation loss 0.04301958531141281 Accuracy 0.8899999856948853\n",
      "Iteration 29080 Training loss 0.0005705727380700409 Validation loss 0.04304611310362816 Accuracy 0.8889999985694885\n",
      "Iteration 29090 Training loss 0.00057134625967592 Validation loss 0.04307940974831581 Accuracy 0.8889999985694885\n",
      "Iteration 29100 Training loss 0.0008187766070477664 Validation loss 0.043012816458940506 Accuracy 0.8899999856948853\n",
      "Iteration 29110 Training loss 0.0005690588732250035 Validation loss 0.04308184236288071 Accuracy 0.8889999985694885\n",
      "Iteration 29120 Training loss 0.0005699145840480924 Validation loss 0.04306461662054062 Accuracy 0.8895000219345093\n",
      "Iteration 29130 Training loss 0.001074260100722313 Validation loss 0.043063439428806305 Accuracy 0.8895000219345093\n",
      "Iteration 29140 Training loss 0.0008219081209972501 Validation loss 0.04306328296661377 Accuracy 0.8895000219345093\n",
      "Iteration 29150 Training loss 0.0008207121281884611 Validation loss 0.04311153292655945 Accuracy 0.8895000219345093\n",
      "Iteration 29160 Training loss 0.0018185224616900086 Validation loss 0.04305035620927811 Accuracy 0.8895000219345093\n",
      "Iteration 29170 Training loss 0.0010697443503886461 Validation loss 0.043065790086984634 Accuracy 0.8889999985694885\n",
      "Iteration 29180 Training loss 0.0010693452786654234 Validation loss 0.04305577650666237 Accuracy 0.8899999856948853\n",
      "Iteration 29190 Training loss 0.001572460518218577 Validation loss 0.043070800602436066 Accuracy 0.8899999856948853\n",
      "Iteration 29200 Training loss 0.0013165595009922981 Validation loss 0.04304056614637375 Accuracy 0.8899999856948853\n",
      "Iteration 29210 Training loss 0.0005691100377589464 Validation loss 0.04302288591861725 Accuracy 0.890500009059906\n",
      "Iteration 29220 Training loss 0.0013163321418687701 Validation loss 0.04306488856673241 Accuracy 0.8895000219345093\n",
      "Iteration 29230 Training loss 0.0005666752113029361 Validation loss 0.043057266622781754 Accuracy 0.8895000219345093\n",
      "Iteration 29240 Training loss 0.0013209069147706032 Validation loss 0.04302005097270012 Accuracy 0.890500009059906\n",
      "Iteration 29250 Training loss 0.0013233136851340532 Validation loss 0.043024346232414246 Accuracy 0.890500009059906\n",
      "Iteration 29260 Training loss 0.0003206462424714118 Validation loss 0.04303161799907684 Accuracy 0.890500009059906\n",
      "Iteration 29270 Training loss 0.001066864700987935 Validation loss 0.04300116002559662 Accuracy 0.8899999856948853\n",
      "Iteration 29280 Training loss 0.001074234489351511 Validation loss 0.04302334412932396 Accuracy 0.8889999985694885\n",
      "Iteration 29290 Training loss 0.000819309672806412 Validation loss 0.04302329942584038 Accuracy 0.8899999856948853\n",
      "Iteration 29300 Training loss 0.0010725387837737799 Validation loss 0.04301871731877327 Accuracy 0.8899999856948853\n",
      "Iteration 29310 Training loss 0.0008231182582676411 Validation loss 0.043047551065683365 Accuracy 0.890500009059906\n",
      "Iteration 29320 Training loss 0.0003217293124180287 Validation loss 0.043053943663835526 Accuracy 0.8889999985694885\n",
      "Iteration 29330 Training loss 0.0013267484027892351 Validation loss 0.04305841773748398 Accuracy 0.8895000219345093\n",
      "Iteration 29340 Training loss 0.0010712331859394908 Validation loss 0.043060850352048874 Accuracy 0.8889999985694885\n",
      "Iteration 29350 Training loss 0.0013243756256997585 Validation loss 0.04304758831858635 Accuracy 0.8895000219345093\n",
      "Iteration 29360 Training loss 0.0005690327379852533 Validation loss 0.04300844669342041 Accuracy 0.8889999985694885\n",
      "Iteration 29370 Training loss 0.0015731448074802756 Validation loss 0.04307575896382332 Accuracy 0.8884999752044678\n",
      "Iteration 29380 Training loss 0.0005696026491932571 Validation loss 0.043043941259384155 Accuracy 0.8899999856948853\n",
      "Iteration 29390 Training loss 0.0010675941593945026 Validation loss 0.04304821044206619 Accuracy 0.890500009059906\n",
      "Iteration 29400 Training loss 0.0005697783781215549 Validation loss 0.04302344471216202 Accuracy 0.8895000219345093\n",
      "Iteration 29410 Training loss 0.0018212064169347286 Validation loss 0.04306286573410034 Accuracy 0.8884999752044678\n",
      "Iteration 29420 Training loss 0.0008192374953068793 Validation loss 0.043020956218242645 Accuracy 0.8895000219345093\n",
      "Iteration 29430 Training loss 0.0010717377299442887 Validation loss 0.04304369539022446 Accuracy 0.8895000219345093\n",
      "Iteration 29440 Training loss 0.0018222304061055183 Validation loss 0.042989376932382584 Accuracy 0.8899999856948853\n",
      "Iteration 29450 Training loss 0.0008202146273106337 Validation loss 0.043042510747909546 Accuracy 0.8895000219345093\n",
      "Iteration 29460 Training loss 0.001070742029696703 Validation loss 0.04302046447992325 Accuracy 0.8899999856948853\n",
      "Iteration 29470 Training loss 0.0005704387440346181 Validation loss 0.04307619482278824 Accuracy 0.8895000219345093\n",
      "Iteration 29480 Training loss 0.0005700939800590277 Validation loss 0.04301787540316582 Accuracy 0.8899999856948853\n",
      "Iteration 29490 Training loss 0.0003180340281687677 Validation loss 0.04305315762758255 Accuracy 0.8889999985694885\n",
      "Iteration 29500 Training loss 0.0003181765496265143 Validation loss 0.04301827773451805 Accuracy 0.8899999856948853\n",
      "Iteration 29510 Training loss 0.0015677910996600986 Validation loss 0.04302584007382393 Accuracy 0.8899999856948853\n",
      "Iteration 29520 Training loss 0.0008198133436962962 Validation loss 0.04306330904364586 Accuracy 0.8895000219345093\n",
      "Iteration 29530 Training loss 0.0003167820686940104 Validation loss 0.04301205649971962 Accuracy 0.890500009059906\n",
      "Iteration 29540 Training loss 7.157981599448249e-05 Validation loss 0.04303564503788948 Accuracy 0.8889999985694885\n",
      "Iteration 29550 Training loss 0.0015630627749487758 Validation loss 0.04303360730409622 Accuracy 0.8895000219345093\n",
      "Iteration 29560 Training loss 0.00057294248836115 Validation loss 0.04303022474050522 Accuracy 0.8895000219345093\n",
      "Iteration 29570 Training loss 0.00032044982071965933 Validation loss 0.04300542548298836 Accuracy 0.890500009059906\n",
      "Iteration 29580 Training loss 0.0008191436645574868 Validation loss 0.04304071143269539 Accuracy 0.8889999985694885\n",
      "Iteration 29590 Training loss 0.0008226549834944308 Validation loss 0.04309263452887535 Accuracy 0.8889999985694885\n",
      "Iteration 29600 Training loss 7.39831302780658e-05 Validation loss 0.04301521182060242 Accuracy 0.890500009059906\n",
      "Iteration 29610 Training loss 0.0013118828646838665 Validation loss 0.043036796152591705 Accuracy 0.8895000219345093\n",
      "Iteration 29620 Training loss 0.0003147234092466533 Validation loss 0.04298950731754303 Accuracy 0.890500009059906\n",
      "Iteration 29630 Training loss 6.68458451400511e-05 Validation loss 0.04304688423871994 Accuracy 0.890500009059906\n",
      "Iteration 29640 Training loss 0.000567059323657304 Validation loss 0.04303082078695297 Accuracy 0.890500009059906\n",
      "Iteration 29650 Training loss 0.00031588159617967904 Validation loss 0.043051231652498245 Accuracy 0.8899999856948853\n",
      "Iteration 29660 Training loss 0.0005698050954379141 Validation loss 0.04309380427002907 Accuracy 0.8889999985694885\n",
      "Iteration 29670 Training loss 0.0013176259817555547 Validation loss 0.04303360730409622 Accuracy 0.890500009059906\n",
      "Iteration 29680 Training loss 0.0008232545224018395 Validation loss 0.04304666072130203 Accuracy 0.8895000219345093\n",
      "Iteration 29690 Training loss 0.0010732566006481647 Validation loss 0.04308108612895012 Accuracy 0.8895000219345093\n",
      "Iteration 29700 Training loss 0.0010652372147887945 Validation loss 0.04302072897553444 Accuracy 0.8899999856948853\n",
      "Iteration 29710 Training loss 0.00032066635321825743 Validation loss 0.04300859943032265 Accuracy 0.8895000219345093\n",
      "Iteration 29720 Training loss 0.0010721927974373102 Validation loss 0.04308272898197174 Accuracy 0.8884999752044678\n",
      "Iteration 29730 Training loss 0.0010708257323130965 Validation loss 0.043054915964603424 Accuracy 0.8884999752044678\n",
      "Iteration 29740 Training loss 0.0003230179427191615 Validation loss 0.04300570860505104 Accuracy 0.890500009059906\n",
      "Iteration 29750 Training loss 0.0005748153198510408 Validation loss 0.043081678450107574 Accuracy 0.8889999985694885\n",
      "Iteration 29760 Training loss 0.0005736961611546576 Validation loss 0.043017949908971786 Accuracy 0.8899999856948853\n",
      "Iteration 29770 Training loss 0.0013193776831030846 Validation loss 0.04303381219506264 Accuracy 0.8895000219345093\n",
      "Iteration 29780 Training loss 0.0003177358885295689 Validation loss 0.04302901029586792 Accuracy 0.8895000219345093\n",
      "Iteration 29790 Training loss 0.0008209486841224134 Validation loss 0.043013621121644974 Accuracy 0.8895000219345093\n",
      "Iteration 29800 Training loss 0.0008206191123463213 Validation loss 0.043003544211387634 Accuracy 0.890999972820282\n",
      "Iteration 29810 Training loss 0.0003204546810593456 Validation loss 0.04300849884748459 Accuracy 0.890999972820282\n",
      "Iteration 29820 Training loss 0.0010695074452087283 Validation loss 0.04309040680527687 Accuracy 0.8889999985694885\n",
      "Iteration 29830 Training loss 0.0015689692227169871 Validation loss 0.04303091764450073 Accuracy 0.8899999856948853\n",
      "Iteration 29840 Training loss 6.858895358163863e-05 Validation loss 0.043030135333538055 Accuracy 0.8889999985694885\n",
      "Iteration 29850 Training loss 0.00031733285868540406 Validation loss 0.04303731769323349 Accuracy 0.8899999856948853\n",
      "Iteration 29860 Training loss 0.0003169432166032493 Validation loss 0.043014731258153915 Accuracy 0.8899999856948853\n",
      "Iteration 29870 Training loss 0.0010708295740187168 Validation loss 0.0430573895573616 Accuracy 0.8895000219345093\n",
      "Iteration 29880 Training loss 0.0008228872320614755 Validation loss 0.043048057705163956 Accuracy 0.8889999985694885\n",
      "Iteration 29890 Training loss 0.00057340192142874 Validation loss 0.0430205762386322 Accuracy 0.8899999856948853\n",
      "Iteration 29900 Training loss 0.0013209331082180142 Validation loss 0.043027233332395554 Accuracy 0.8895000219345093\n",
      "Iteration 29910 Training loss 6.94460904924199e-05 Validation loss 0.04299642890691757 Accuracy 0.8895000219345093\n",
      "Iteration 29920 Training loss 0.00032047423883341253 Validation loss 0.04308784380555153 Accuracy 0.8889999985694885\n",
      "Iteration 29930 Training loss 7.240602280944586e-05 Validation loss 0.04302500933408737 Accuracy 0.890999972820282\n",
      "Iteration 29940 Training loss 0.0005658493028022349 Validation loss 0.04303941875696182 Accuracy 0.8899999856948853\n",
      "Iteration 29950 Training loss 0.00032519103842787445 Validation loss 0.04303022846579552 Accuracy 0.8899999856948853\n",
      "Iteration 29960 Training loss 0.001063625211827457 Validation loss 0.04302044212818146 Accuracy 0.890500009059906\n",
      "Iteration 29970 Training loss 0.0013203677954152226 Validation loss 0.0429825484752655 Accuracy 0.890500009059906\n",
      "Iteration 29980 Training loss 6.98124713380821e-05 Validation loss 0.04304103925824165 Accuracy 0.8895000219345093\n",
      "Iteration 29990 Training loss 0.0015651745488867164 Validation loss 0.04305532947182655 Accuracy 0.8899999856948853\n",
      "Iteration 30000 Training loss 0.0010710578644648194 Validation loss 0.043027665466070175 Accuracy 0.8899999856948853\n",
      "Iteration 30010 Training loss 0.0005697987508028746 Validation loss 0.04305683821439743 Accuracy 0.8899999856948853\n",
      "Iteration 30020 Training loss 0.0008163285092450678 Validation loss 0.04301855340600014 Accuracy 0.8895000219345093\n",
      "Iteration 30030 Training loss 0.0015633964212611318 Validation loss 0.043018706142902374 Accuracy 0.8899999856948853\n",
      "Iteration 30040 Training loss 0.001071422011591494 Validation loss 0.043029654771089554 Accuracy 0.8899999856948853\n",
      "Iteration 30050 Training loss 0.0005668368539772928 Validation loss 0.04304696246981621 Accuracy 0.8899999856948853\n",
      "Iteration 30060 Training loss 0.0013209658209234476 Validation loss 0.043022796511650085 Accuracy 0.8895000219345093\n",
      "Iteration 30070 Training loss 0.0008239115122705698 Validation loss 0.04303582012653351 Accuracy 0.890500009059906\n",
      "Iteration 30080 Training loss 0.0013208537129685283 Validation loss 0.04302476346492767 Accuracy 0.8899999856948853\n",
      "Iteration 30090 Training loss 0.001314507331699133 Validation loss 0.043005701154470444 Accuracy 0.890500009059906\n",
      "Iteration 30100 Training loss 0.0013165889540687203 Validation loss 0.04304378107190132 Accuracy 0.8899999856948853\n",
      "Iteration 30110 Training loss 0.0005703713395632803 Validation loss 0.043018095195293427 Accuracy 0.890500009059906\n",
      "Iteration 30120 Training loss 0.001571553759276867 Validation loss 0.04304632171988487 Accuracy 0.8895000219345093\n",
      "Iteration 30130 Training loss 0.0008256553555838764 Validation loss 0.04299847409129143 Accuracy 0.890999972820282\n",
      "Iteration 30140 Training loss 0.001066294964402914 Validation loss 0.043031372129917145 Accuracy 0.890500009059906\n",
      "Iteration 30150 Training loss 0.0005725768860429525 Validation loss 0.0430438406765461 Accuracy 0.8889999985694885\n",
      "Iteration 30160 Training loss 0.0008161634323187172 Validation loss 0.04303257539868355 Accuracy 0.8895000219345093\n",
      "Iteration 30170 Training loss 0.0015687712002545595 Validation loss 0.04302734509110451 Accuracy 0.8895000219345093\n",
      "Iteration 30180 Training loss 0.0018201079219579697 Validation loss 0.04303388297557831 Accuracy 0.8895000219345093\n",
      "Iteration 30190 Training loss 0.00032186179305426776 Validation loss 0.043001122772693634 Accuracy 0.8899999856948853\n",
      "Iteration 30200 Training loss 0.0008240523748099804 Validation loss 0.04296630620956421 Accuracy 0.890500009059906\n",
      "Iteration 30210 Training loss 0.0005697929882444441 Validation loss 0.042995817959308624 Accuracy 0.890500009059906\n",
      "Iteration 30220 Training loss 0.0008193242829293013 Validation loss 0.04300525411963463 Accuracy 0.8895000219345093\n",
      "Iteration 30230 Training loss 6.395343370968476e-05 Validation loss 0.043027836829423904 Accuracy 0.8884999752044678\n",
      "Iteration 30240 Training loss 0.0013129443395882845 Validation loss 0.0430176705121994 Accuracy 0.8889999985694885\n",
      "Iteration 30250 Training loss 0.00031971276621334255 Validation loss 0.04299977794289589 Accuracy 0.8899999856948853\n",
      "Iteration 30260 Training loss 0.001318558119237423 Validation loss 0.04298171028494835 Accuracy 0.890500009059906\n",
      "Iteration 30270 Training loss 0.0005706139490939677 Validation loss 0.04303498938679695 Accuracy 0.8899999856948853\n",
      "Iteration 30280 Training loss 0.0008237865986302495 Validation loss 0.04300830513238907 Accuracy 0.890999972820282\n",
      "Iteration 30290 Training loss 0.000819132081232965 Validation loss 0.0430159792304039 Accuracy 0.890500009059906\n",
      "Iteration 30300 Training loss 0.0010680905543267727 Validation loss 0.042991477996110916 Accuracy 0.890999972820282\n",
      "Iteration 30310 Training loss 0.0013183748815208673 Validation loss 0.04303297400474548 Accuracy 0.8895000219345093\n",
      "Iteration 30320 Training loss 0.0010644567664712667 Validation loss 0.043015070259571075 Accuracy 0.8899999856948853\n",
      "Iteration 30330 Training loss 0.0003211962466593832 Validation loss 0.04300161823630333 Accuracy 0.890500009059906\n",
      "Iteration 30340 Training loss 0.000320032995659858 Validation loss 0.0430128388106823 Accuracy 0.890999972820282\n",
      "Iteration 30350 Training loss 0.0008208559011109173 Validation loss 0.04304397478699684 Accuracy 0.8899999856948853\n",
      "Iteration 30360 Training loss 0.0005705073708668351 Validation loss 0.04302822798490524 Accuracy 0.890999972820282\n",
      "Iteration 30370 Training loss 0.0008192849927581847 Validation loss 0.0431002713739872 Accuracy 0.8884999752044678\n",
      "Iteration 30380 Training loss 0.0008189172949641943 Validation loss 0.042993560433387756 Accuracy 0.890500009059906\n",
      "Iteration 30390 Training loss 0.000820957007817924 Validation loss 0.04303284361958504 Accuracy 0.8899999856948853\n",
      "Iteration 30400 Training loss 0.0008197725983336568 Validation loss 0.043025750666856766 Accuracy 0.890500009059906\n",
      "Iteration 30410 Training loss 0.00031897996086627245 Validation loss 0.04302448406815529 Accuracy 0.890500009059906\n",
      "Iteration 30420 Training loss 0.0015735792694613338 Validation loss 0.04306278005242348 Accuracy 0.8889999985694885\n",
      "Iteration 30430 Training loss 0.0008171446388587356 Validation loss 0.04301782324910164 Accuracy 0.8899999856948853\n",
      "Iteration 30440 Training loss 0.0013193791965022683 Validation loss 0.04305572807788849 Accuracy 0.8899999856948853\n",
      "Iteration 30450 Training loss 0.000825296388939023 Validation loss 0.04303569719195366 Accuracy 0.890500009059906\n",
      "Iteration 30460 Training loss 0.0015749346930533648 Validation loss 0.04302987456321716 Accuracy 0.890500009059906\n",
      "Iteration 30470 Training loss 0.0008189812651835382 Validation loss 0.043025314807891846 Accuracy 0.8899999856948853\n",
      "Iteration 30480 Training loss 0.0008164215250872076 Validation loss 0.0430263988673687 Accuracy 0.890500009059906\n",
      "Iteration 30490 Training loss 0.0008147977641783655 Validation loss 0.04301510751247406 Accuracy 0.8899999856948853\n",
      "Iteration 30500 Training loss 0.00032078532967716455 Validation loss 0.04301011934876442 Accuracy 0.8899999856948853\n",
      "Iteration 30510 Training loss 0.0005667242803610861 Validation loss 0.04300083592534065 Accuracy 0.8899999856948853\n",
      "Iteration 30520 Training loss 0.0008157400297932327 Validation loss 0.04306299611926079 Accuracy 0.8895000219345093\n",
      "Iteration 30530 Training loss 0.0010657624807208776 Validation loss 0.04304703325033188 Accuracy 0.8889999985694885\n",
      "Iteration 30540 Training loss 0.0008160218130797148 Validation loss 0.04303516447544098 Accuracy 0.890500009059906\n",
      "Iteration 30550 Training loss 0.0005677050212398171 Validation loss 0.0430087074637413 Accuracy 0.890999972820282\n",
      "Iteration 30560 Training loss 0.0008236043504439294 Validation loss 0.04300805553793907 Accuracy 0.890999972820282\n",
      "Iteration 30570 Training loss 7.145017298171297e-05 Validation loss 0.043025240302085876 Accuracy 0.890999972820282\n",
      "Iteration 30580 Training loss 0.001072665210813284 Validation loss 0.04300854355096817 Accuracy 0.890500009059906\n",
      "Iteration 30590 Training loss 0.0008194565307348967 Validation loss 0.04304433614015579 Accuracy 0.890999972820282\n",
      "Iteration 30600 Training loss 0.00032137095695361495 Validation loss 0.043042007833719254 Accuracy 0.8899999856948853\n",
      "Iteration 30610 Training loss 0.0008223678451031446 Validation loss 0.04303530231118202 Accuracy 0.8884999752044678\n",
      "Iteration 30620 Training loss 0.0003169053525198251 Validation loss 0.043007008731365204 Accuracy 0.8899999856948853\n",
      "Iteration 30630 Training loss 0.0005656372522935271 Validation loss 0.04304325953125954 Accuracy 0.8895000219345093\n",
      "Iteration 30640 Training loss 0.0008174200193025172 Validation loss 0.0430789515376091 Accuracy 0.8895000219345093\n",
      "Iteration 30650 Training loss 0.000566587783396244 Validation loss 0.04304342716932297 Accuracy 0.8895000219345093\n",
      "Iteration 30660 Training loss 6.451080844271928e-05 Validation loss 0.043016061186790466 Accuracy 0.890500009059906\n",
      "Iteration 30670 Training loss 0.001317436806857586 Validation loss 0.04297180101275444 Accuracy 0.890999972820282\n",
      "Iteration 30680 Training loss 0.0008218289003707469 Validation loss 0.042997509241104126 Accuracy 0.8899999856948853\n",
      "Iteration 30690 Training loss 0.00131795194465667 Validation loss 0.043001655489206314 Accuracy 0.8899999856948853\n",
      "Iteration 30700 Training loss 0.0010717601981014013 Validation loss 0.04306630417704582 Accuracy 0.8889999985694885\n",
      "Iteration 30710 Training loss 0.0008173366659320891 Validation loss 0.043043188750743866 Accuracy 0.8899999856948853\n",
      "Iteration 30720 Training loss 0.0015729718143120408 Validation loss 0.0430099256336689 Accuracy 0.8895000219345093\n",
      "Iteration 30730 Training loss 0.00031752738868817687 Validation loss 0.04302545264363289 Accuracy 0.8884999752044678\n",
      "Iteration 30740 Training loss 0.0010725311003625393 Validation loss 0.04301069676876068 Accuracy 0.8899999856948853\n",
      "Iteration 30750 Training loss 0.0008167625637724996 Validation loss 0.042987123131752014 Accuracy 0.8895000219345093\n",
      "Iteration 30760 Training loss 0.0010686276946216822 Validation loss 0.04302654415369034 Accuracy 0.8889999985694885\n",
      "Iteration 30770 Training loss 6.682458479190245e-05 Validation loss 0.04302782565355301 Accuracy 0.8899999856948853\n",
      "Iteration 30780 Training loss 0.0008186451159417629 Validation loss 0.04300820082426071 Accuracy 0.8895000219345093\n",
      "Iteration 30790 Training loss 0.0008177144336514175 Validation loss 0.04302086681127548 Accuracy 0.8895000219345093\n",
      "Iteration 30800 Training loss 0.0005718732718378305 Validation loss 0.043030355125665665 Accuracy 0.8895000219345093\n",
      "Iteration 30810 Training loss 0.0010711988434195518 Validation loss 0.043029654771089554 Accuracy 0.8899999856948853\n",
      "Iteration 30820 Training loss 0.0008168547064997256 Validation loss 0.04304932802915573 Accuracy 0.8899999856948853\n",
      "Iteration 30830 Training loss 0.00031807893537916243 Validation loss 0.04298856854438782 Accuracy 0.890500009059906\n",
      "Iteration 30840 Training loss 7.21901815268211e-05 Validation loss 0.04301276057958603 Accuracy 0.890500009059906\n",
      "Iteration 30850 Training loss 0.0005708623793907464 Validation loss 0.043011270463466644 Accuracy 0.8895000219345093\n",
      "Iteration 30860 Training loss 0.0008192025707103312 Validation loss 0.04303921386599541 Accuracy 0.8899999856948853\n",
      "Iteration 30870 Training loss 0.001067494973540306 Validation loss 0.04299008846282959 Accuracy 0.890999972820282\n",
      "Iteration 30880 Training loss 0.0013162418035790324 Validation loss 0.04303424432873726 Accuracy 0.8895000219345093\n",
      "Iteration 30890 Training loss 0.0008157217525877059 Validation loss 0.04300837591290474 Accuracy 0.8899999856948853\n",
      "Iteration 30900 Training loss 0.0010681016137823462 Validation loss 0.04302317649126053 Accuracy 0.8899999856948853\n",
      "Iteration 30910 Training loss 0.0008176233968697488 Validation loss 0.043017469346523285 Accuracy 0.890500009059906\n",
      "Iteration 30920 Training loss 0.00081962178228423 Validation loss 0.04298579320311546 Accuracy 0.890999972820282\n",
      "Iteration 30930 Training loss 0.0005694232531823218 Validation loss 0.04304485395550728 Accuracy 0.8889999985694885\n",
      "Iteration 30940 Training loss 0.0008236439898610115 Validation loss 0.04302220046520233 Accuracy 0.8895000219345093\n",
      "Iteration 30950 Training loss 0.000568935414776206 Validation loss 0.0430474728345871 Accuracy 0.8899999856948853\n",
      "Iteration 30960 Training loss 0.0005695567233487964 Validation loss 0.04302545264363289 Accuracy 0.8899999856948853\n",
      "Iteration 30970 Training loss 0.0015713341999799013 Validation loss 0.0430135577917099 Accuracy 0.890500009059906\n",
      "Iteration 30980 Training loss 0.0008174067479558289 Validation loss 0.04303387179970741 Accuracy 0.8895000219345093\n",
      "Iteration 30990 Training loss 0.00032360994373448193 Validation loss 0.043004110455513 Accuracy 0.890500009059906\n",
      "Iteration 31000 Training loss 0.001566699706017971 Validation loss 0.043055202811956406 Accuracy 0.8899999856948853\n",
      "Iteration 31010 Training loss 0.0010741297155618668 Validation loss 0.04309213161468506 Accuracy 0.8889999985694885\n",
      "Iteration 31020 Training loss 0.001069671125151217 Validation loss 0.04304634779691696 Accuracy 0.8895000219345093\n",
      "Iteration 31030 Training loss 0.0010697782272472978 Validation loss 0.04302317649126053 Accuracy 0.890500009059906\n",
      "Iteration 31040 Training loss 0.0005717219901271164 Validation loss 0.042989395558834076 Accuracy 0.890999972820282\n",
      "Iteration 31050 Training loss 0.0008178664138540626 Validation loss 0.04300607740879059 Accuracy 0.890500009059906\n",
      "Iteration 31060 Training loss 0.0005711279227398336 Validation loss 0.04300950467586517 Accuracy 0.8899999856948853\n",
      "Iteration 31070 Training loss 0.0005746085662394762 Validation loss 0.04303954169154167 Accuracy 0.8889999985694885\n",
      "Iteration 31080 Training loss 0.0003228211426176131 Validation loss 0.0430748276412487 Accuracy 0.8899999856948853\n",
      "Iteration 31090 Training loss 0.0008225077763199806 Validation loss 0.043030012398958206 Accuracy 0.890500009059906\n",
      "Iteration 31100 Training loss 0.0005708407261408865 Validation loss 0.04300830885767937 Accuracy 0.890999972820282\n",
      "Iteration 31110 Training loss 0.0013196129584684968 Validation loss 0.043055322021245956 Accuracy 0.8884999752044678\n",
      "Iteration 31120 Training loss 0.0003226216940674931 Validation loss 0.04301938787102699 Accuracy 0.890500009059906\n",
      "Iteration 31130 Training loss 0.0015686106635257602 Validation loss 0.04299516975879669 Accuracy 0.890999972820282\n",
      "Iteration 31140 Training loss 0.0005711120320484042 Validation loss 0.04298660531640053 Accuracy 0.890999972820282\n",
      "Iteration 31150 Training loss 0.0003244282561354339 Validation loss 0.04299848899245262 Accuracy 0.890999972820282\n",
      "Iteration 31160 Training loss 0.0008203564211726189 Validation loss 0.043043144047260284 Accuracy 0.8895000219345093\n",
      "Iteration 31170 Training loss 0.002316083526238799 Validation loss 0.043023787438869476 Accuracy 0.890999972820282\n",
      "Iteration 31180 Training loss 0.0005672890110872686 Validation loss 0.04306468740105629 Accuracy 0.8895000219345093\n",
      "Iteration 31190 Training loss 0.001325721968896687 Validation loss 0.04300219565629959 Accuracy 0.8899999856948853\n",
      "Iteration 31200 Training loss 0.0010653049685060978 Validation loss 0.04298777133226395 Accuracy 0.890500009059906\n",
      "Iteration 31210 Training loss 0.000820422952529043 Validation loss 0.043037284165620804 Accuracy 0.890999972820282\n",
      "Iteration 31220 Training loss 0.0013177035143598914 Validation loss 0.043015770614147186 Accuracy 0.890500009059906\n",
      "Iteration 31230 Training loss 0.0003147731476929039 Validation loss 0.04303154721856117 Accuracy 0.8899999856948853\n",
      "Iteration 31240 Training loss 0.0005707198870368302 Validation loss 0.042983923107385635 Accuracy 0.890500009059906\n",
      "Iteration 31250 Training loss 0.0010715852258726954 Validation loss 0.04298127815127373 Accuracy 0.8899999856948853\n",
      "Iteration 31260 Training loss 0.0008222843753173947 Validation loss 0.043027110397815704 Accuracy 0.8889999985694885\n",
      "Iteration 31270 Training loss 0.0018193951109424233 Validation loss 0.04302183538675308 Accuracy 0.890500009059906\n",
      "Iteration 31280 Training loss 0.0008167293854057789 Validation loss 0.043073005974292755 Accuracy 0.8884999752044678\n",
      "Iteration 31290 Training loss 0.0010682872962206602 Validation loss 0.043038949370384216 Accuracy 0.8899999856948853\n",
      "Iteration 31300 Training loss 0.0010701640276238322 Validation loss 0.043040018528699875 Accuracy 0.890500009059906\n",
      "Iteration 31310 Training loss 0.0005686231306754053 Validation loss 0.04307427629828453 Accuracy 0.890500009059906\n",
      "Iteration 31320 Training loss 0.0013233025092631578 Validation loss 0.042978398501873016 Accuracy 0.890999972820282\n",
      "Iteration 31330 Training loss 0.001069501624442637 Validation loss 0.04302537068724632 Accuracy 0.890500009059906\n",
      "Iteration 31340 Training loss 6.95834678481333e-05 Validation loss 0.04303685575723648 Accuracy 0.8899999856948853\n",
      "Iteration 31350 Training loss 0.001315964967943728 Validation loss 0.04298502579331398 Accuracy 0.890500009059906\n",
      "Iteration 31360 Training loss 0.0005691840779036283 Validation loss 0.043001845479011536 Accuracy 0.8899999856948853\n",
      "Iteration 31370 Training loss 0.0008192085078917444 Validation loss 0.04303279146552086 Accuracy 0.8899999856948853\n",
      "Iteration 31380 Training loss 0.001318891765549779 Validation loss 0.042982593178749084 Accuracy 0.890999972820282\n",
      "Iteration 31390 Training loss 0.0005697708111256361 Validation loss 0.04303506016731262 Accuracy 0.890500009059906\n",
      "Iteration 31400 Training loss 0.0005713665159419179 Validation loss 0.043008193373680115 Accuracy 0.890500009059906\n",
      "Iteration 31410 Training loss 0.001320420065894723 Validation loss 0.04303694888949394 Accuracy 0.8899999856948853\n",
      "Iteration 31420 Training loss 0.0008154987008310854 Validation loss 0.04306846484541893 Accuracy 0.8895000219345093\n",
      "Iteration 31430 Training loss 0.001075232052244246 Validation loss 0.04300295189023018 Accuracy 0.890999972820282\n",
      "Iteration 31440 Training loss 0.0005687313387170434 Validation loss 0.043024707585573196 Accuracy 0.8895000219345093\n",
      "Iteration 31450 Training loss 0.00181427295319736 Validation loss 0.043009210377931595 Accuracy 0.8899999856948853\n",
      "Iteration 31460 Training loss 0.0008134337840601802 Validation loss 0.04303581640124321 Accuracy 0.8895000219345093\n",
      "Iteration 31470 Training loss 0.0013236316153779626 Validation loss 0.04305163025856018 Accuracy 0.8899999856948853\n",
      "Iteration 31480 Training loss 0.00032112968619912863 Validation loss 0.043046969920396805 Accuracy 0.890500009059906\n",
      "Iteration 31490 Training loss 0.0013226590817794204 Validation loss 0.043037571012973785 Accuracy 0.8899999856948853\n",
      "Iteration 31500 Training loss 0.0015679975040256977 Validation loss 0.04302600771188736 Accuracy 0.890500009059906\n",
      "Iteration 31510 Training loss 0.001824883627705276 Validation loss 0.04299952834844589 Accuracy 0.890500009059906\n",
      "Iteration 31520 Training loss 0.0010729797650128603 Validation loss 0.04300769791007042 Accuracy 0.890500009059906\n",
      "Iteration 31530 Training loss 0.0010676600504666567 Validation loss 0.04301140084862709 Accuracy 0.890500009059906\n",
      "Iteration 31540 Training loss 0.0013137907953932881 Validation loss 0.04303623363375664 Accuracy 0.890500009059906\n",
      "Iteration 31550 Training loss 0.0010641403496265411 Validation loss 0.04302440956234932 Accuracy 0.8895000219345093\n",
      "Iteration 31560 Training loss 0.0008148561464622617 Validation loss 0.043042197823524475 Accuracy 0.890999972820282\n",
      "Iteration 31570 Training loss 0.0005694898427464068 Validation loss 0.04302751645445824 Accuracy 0.890500009059906\n",
      "Iteration 31580 Training loss 0.00031883586780168116 Validation loss 0.04307850822806358 Accuracy 0.8899999856948853\n",
      "Iteration 31590 Training loss 0.0015641100471839309 Validation loss 0.04298936948180199 Accuracy 0.890500009059906\n",
      "Iteration 31600 Training loss 0.0005689625977538526 Validation loss 0.043066125363111496 Accuracy 0.890500009059906\n",
      "Iteration 31610 Training loss 0.0008197246352210641 Validation loss 0.04306779429316521 Accuracy 0.8895000219345093\n",
      "Iteration 31620 Training loss 0.0010692233918234706 Validation loss 0.04302902892231941 Accuracy 0.8899999856948853\n",
      "Iteration 31630 Training loss 0.0003156985330861062 Validation loss 0.04306231066584587 Accuracy 0.890500009059906\n",
      "Iteration 31640 Training loss 0.0015724775148555636 Validation loss 0.04299443960189819 Accuracy 0.8899999856948853\n",
      "Iteration 31650 Training loss 0.000822021218482405 Validation loss 0.04307924583554268 Accuracy 0.890500009059906\n",
      "Iteration 31660 Training loss 0.0010700683342292905 Validation loss 0.043057896196842194 Accuracy 0.8895000219345093\n",
      "Iteration 31670 Training loss 0.0008191613014787436 Validation loss 0.0430368110537529 Accuracy 0.890500009059906\n",
      "Iteration 31680 Training loss 0.0005695678992196918 Validation loss 0.04304219409823418 Accuracy 0.8899999856948853\n",
      "Iteration 31690 Training loss 0.0005693603889085352 Validation loss 0.043081942945718765 Accuracy 0.8884999752044678\n",
      "Iteration 31700 Training loss 0.0003193784796167165 Validation loss 0.04306624457240105 Accuracy 0.8895000219345093\n",
      "Iteration 31710 Training loss 0.0008213252294808626 Validation loss 0.043024469166994095 Accuracy 0.890999972820282\n",
      "Iteration 31720 Training loss 0.0010679245460778475 Validation loss 0.04302813485264778 Accuracy 0.890500009059906\n",
      "Iteration 31730 Training loss 0.0010694221127778292 Validation loss 0.043028075248003006 Accuracy 0.890500009059906\n",
      "Iteration 31740 Training loss 0.0003184876113664359 Validation loss 0.0430176705121994 Accuracy 0.890999972820282\n",
      "Iteration 31750 Training loss 0.0010734337847679853 Validation loss 0.043048832565546036 Accuracy 0.890500009059906\n",
      "Iteration 31760 Training loss 6.902270979480818e-05 Validation loss 0.04306148365139961 Accuracy 0.890500009059906\n",
      "Iteration 31770 Training loss 0.0008211291860789061 Validation loss 0.04303249344229698 Accuracy 0.890500009059906\n",
      "Iteration 31780 Training loss 0.0013183376286178827 Validation loss 0.043040454387664795 Accuracy 0.8899999856948853\n",
      "Iteration 31790 Training loss 0.0013194265775382519 Validation loss 0.0430414043366909 Accuracy 0.8895000219345093\n",
      "Iteration 31800 Training loss 0.0008217015420086682 Validation loss 0.04303699731826782 Accuracy 0.890500009059906\n",
      "Iteration 31810 Training loss 0.0013202271657064557 Validation loss 0.043103527277708054 Accuracy 0.8884999752044678\n",
      "Iteration 31820 Training loss 0.0013164079282432795 Validation loss 0.043036192655563354 Accuracy 0.8899999856948853\n",
      "Iteration 31830 Training loss 0.0008160207653418183 Validation loss 0.04303918033838272 Accuracy 0.8899999856948853\n",
      "Iteration 31840 Training loss 0.00031862276955507696 Validation loss 0.04305531829595566 Accuracy 0.8895000219345093\n",
      "Iteration 31850 Training loss 0.0013215751387178898 Validation loss 0.04307990521192551 Accuracy 0.8889999985694885\n",
      "Iteration 31860 Training loss 0.0005750433774664998 Validation loss 0.043077073991298676 Accuracy 0.8884999752044678\n",
      "Iteration 31870 Training loss 0.0013201817637309432 Validation loss 0.04307712987065315 Accuracy 0.8884999752044678\n",
      "Iteration 31880 Training loss 0.0010673276847228408 Validation loss 0.04305912181735039 Accuracy 0.8889999985694885\n",
      "Iteration 31890 Training loss 0.0008188282372429967 Validation loss 0.04307686537504196 Accuracy 0.8884999752044678\n",
      "Iteration 31900 Training loss 0.0010653079953044653 Validation loss 0.043025169521570206 Accuracy 0.8895000219345093\n",
      "Iteration 31910 Training loss 6.576121086254716e-05 Validation loss 0.043075304478406906 Accuracy 0.8895000219345093\n",
      "Iteration 31920 Training loss 0.0010713286465033889 Validation loss 0.04299083724617958 Accuracy 0.8899999856948853\n",
      "Iteration 31930 Training loss 0.0013191249454393983 Validation loss 0.04300167039036751 Accuracy 0.8889999985694885\n",
      "Iteration 31940 Training loss 0.0008203120087273419 Validation loss 0.042988937348127365 Accuracy 0.890999972820282\n",
      "Iteration 31950 Training loss 0.0008155035902746022 Validation loss 0.04300965741276741 Accuracy 0.890500009059906\n",
      "Iteration 31960 Training loss 0.0005711192497983575 Validation loss 0.043016403913497925 Accuracy 0.8895000219345093\n",
      "Iteration 31970 Training loss 0.001065143384039402 Validation loss 0.04300251603126526 Accuracy 0.8895000219345093\n",
      "Iteration 31980 Training loss 0.0005679336609318852 Validation loss 0.04303926229476929 Accuracy 0.8889999985694885\n",
      "Iteration 31990 Training loss 0.001316178822889924 Validation loss 0.04304036870598793 Accuracy 0.8899999856948853\n",
      "Iteration 32000 Training loss 0.0008168842759914696 Validation loss 0.04298730567097664 Accuracy 0.890500009059906\n",
      "Iteration 32010 Training loss 0.0008227517828345299 Validation loss 0.04300292581319809 Accuracy 0.8899999856948853\n",
      "Iteration 32020 Training loss 0.0008149853674694896 Validation loss 0.04301492124795914 Accuracy 0.8899999856948853\n",
      "Iteration 32030 Training loss 0.0008200404117815197 Validation loss 0.04301124066114426 Accuracy 0.890500009059906\n",
      "Iteration 32040 Training loss 0.0008174829417839646 Validation loss 0.04300836846232414 Accuracy 0.8895000219345093\n",
      "Iteration 32050 Training loss 0.0013218740932643414 Validation loss 0.043009284883737564 Accuracy 0.8899999856948853\n",
      "Iteration 32060 Training loss 0.0005710047553293407 Validation loss 0.043034590780735016 Accuracy 0.8895000219345093\n",
      "Iteration 32070 Training loss 0.0008204024052247405 Validation loss 0.04303380474448204 Accuracy 0.8895000219345093\n",
      "Iteration 32080 Training loss 0.0013230852782726288 Validation loss 0.043041132390499115 Accuracy 0.8895000219345093\n",
      "Iteration 32090 Training loss 0.0013264002045616508 Validation loss 0.042993828654289246 Accuracy 0.890500009059906\n",
      "Iteration 32100 Training loss 0.0005716167506761849 Validation loss 0.04301014542579651 Accuracy 0.890500009059906\n",
      "Iteration 32110 Training loss 0.0005725828232243657 Validation loss 0.043068379163742065 Accuracy 0.8880000114440918\n",
      "Iteration 32120 Training loss 0.0010747088817879558 Validation loss 0.042979080229997635 Accuracy 0.8899999856948853\n",
      "Iteration 32130 Training loss 0.0010711654322221875 Validation loss 0.04298970848321915 Accuracy 0.8899999856948853\n",
      "Iteration 32140 Training loss 0.00106600031722337 Validation loss 0.043010588735342026 Accuracy 0.8899999856948853\n",
      "Iteration 32150 Training loss 0.0008215532870963216 Validation loss 0.04301131144165993 Accuracy 0.8899999856948853\n",
      "Iteration 32160 Training loss 0.00032304078922607005 Validation loss 0.04307592660188675 Accuracy 0.8884999752044678\n",
      "Iteration 32170 Training loss 0.0008212884422391653 Validation loss 0.04306675121188164 Accuracy 0.8884999752044678\n",
      "Iteration 32180 Training loss 0.0013230354525148869 Validation loss 0.04297346994280815 Accuracy 0.890999972820282\n",
      "Iteration 32190 Training loss 0.0013166996650397778 Validation loss 0.042990975081920624 Accuracy 0.890500009059906\n",
      "Iteration 32200 Training loss 0.001320997136645019 Validation loss 0.043007951229810715 Accuracy 0.890500009059906\n",
      "Iteration 32210 Training loss 0.0008249774691648781 Validation loss 0.043005675077438354 Accuracy 0.890500009059906\n",
      "Iteration 32220 Training loss 0.0010685210581868887 Validation loss 0.043016448616981506 Accuracy 0.8899999856948853\n",
      "Iteration 32230 Training loss 0.0008215441484935582 Validation loss 0.04310055822134018 Accuracy 0.8889999985694885\n",
      "Iteration 32240 Training loss 0.0013214382342994213 Validation loss 0.043002333492040634 Accuracy 0.890999972820282\n",
      "Iteration 32250 Training loss 0.0008216825663112104 Validation loss 0.0430234931409359 Accuracy 0.8899999856948853\n",
      "Iteration 32260 Training loss 0.000571957032661885 Validation loss 0.04309488832950592 Accuracy 0.8880000114440918\n",
      "Iteration 32270 Training loss 0.0015731191961094737 Validation loss 0.04301835596561432 Accuracy 0.890999972820282\n",
      "Iteration 32280 Training loss 0.00032095768256112933 Validation loss 0.04305830970406532 Accuracy 0.8895000219345093\n",
      "Iteration 32290 Training loss 0.0010752081871032715 Validation loss 0.04304556921124458 Accuracy 0.890500009059906\n",
      "Iteration 32300 Training loss 0.0015658690826967359 Validation loss 0.04301978275179863 Accuracy 0.8899999856948853\n",
      "Iteration 32310 Training loss 0.001064939540810883 Validation loss 0.04305640608072281 Accuracy 0.8899999856948853\n",
      "Iteration 32320 Training loss 0.0010672311764210463 Validation loss 0.043025482445955276 Accuracy 0.8895000219345093\n",
      "Iteration 32330 Training loss 7.360812742263079e-05 Validation loss 0.04300960525870323 Accuracy 0.890500009059906\n",
      "Iteration 32340 Training loss 0.0010702569270506501 Validation loss 0.04305536299943924 Accuracy 0.8889999985694885\n",
      "Iteration 32350 Training loss 0.0005689907120540738 Validation loss 0.043017759919166565 Accuracy 0.8895000219345093\n",
      "Iteration 32360 Training loss 0.0008237553993239999 Validation loss 0.04302511736750603 Accuracy 0.890500009059906\n",
      "Iteration 32370 Training loss 0.0005658683367073536 Validation loss 0.04302278906106949 Accuracy 0.890500009059906\n",
      "Iteration 32380 Training loss 0.000819701177533716 Validation loss 0.04299560561776161 Accuracy 0.890500009059906\n",
      "Iteration 32390 Training loss 0.00032140492112375796 Validation loss 0.04301173612475395 Accuracy 0.8899999856948853\n",
      "Iteration 32400 Training loss 0.000825776718556881 Validation loss 0.04305128753185272 Accuracy 0.8889999985694885\n",
      "Iteration 32410 Training loss 0.0005676497239619493 Validation loss 0.04298723116517067 Accuracy 0.890999972820282\n",
      "Iteration 32420 Training loss 0.0005697911255992949 Validation loss 0.04301045835018158 Accuracy 0.8899999856948853\n",
      "Iteration 32430 Training loss 0.000820778077468276 Validation loss 0.04301578551530838 Accuracy 0.8899999856948853\n",
      "Iteration 32440 Training loss 0.0013236569939181209 Validation loss 0.04302418977022171 Accuracy 0.8899999856948853\n",
      "Iteration 32450 Training loss 0.0013204391580075026 Validation loss 0.04300238564610481 Accuracy 0.890500009059906\n",
      "Iteration 32460 Training loss 0.0010636149672791362 Validation loss 0.04304724186658859 Accuracy 0.8895000219345093\n",
      "Iteration 32470 Training loss 0.0005702703492715955 Validation loss 0.04297443851828575 Accuracy 0.890500009059906\n",
      "Iteration 32480 Training loss 0.0013216363731771708 Validation loss 0.043021973222494125 Accuracy 0.8899999856948853\n",
      "Iteration 32490 Training loss 0.001317491172812879 Validation loss 0.043052107095718384 Accuracy 0.8899999856948853\n",
      "Iteration 32500 Training loss 0.0005693682469427586 Validation loss 0.043030306696891785 Accuracy 0.8889999985694885\n",
      "Iteration 32510 Training loss 0.001068253186531365 Validation loss 0.04298131912946701 Accuracy 0.890999972820282\n",
      "Iteration 32520 Training loss 0.0003199662605766207 Validation loss 0.04299866035580635 Accuracy 0.890500009059906\n",
      "Iteration 32530 Training loss 0.001066741649992764 Validation loss 0.04305807128548622 Accuracy 0.8889999985694885\n",
      "Iteration 32540 Training loss 0.0005705747753381729 Validation loss 0.04300258681178093 Accuracy 0.890500009059906\n",
      "Iteration 32550 Training loss 0.0008225535275414586 Validation loss 0.04304054379463196 Accuracy 0.8880000114440918\n",
      "Iteration 32560 Training loss 0.0008207144564948976 Validation loss 0.043003395199775696 Accuracy 0.8899999856948853\n",
      "Iteration 32570 Training loss 6.972304254304618e-05 Validation loss 0.04298217594623566 Accuracy 0.890500009059906\n",
      "Iteration 32580 Training loss 0.0010742262238636613 Validation loss 0.04307109862565994 Accuracy 0.8880000114440918\n",
      "Iteration 32590 Training loss 0.0013278764672577381 Validation loss 0.043042466044425964 Accuracy 0.8899999856948853\n",
      "Iteration 32600 Training loss 0.0008222025353461504 Validation loss 0.04304955154657364 Accuracy 0.8899999856948853\n",
      "Iteration 32610 Training loss 6.916638812981546e-05 Validation loss 0.04303257167339325 Accuracy 0.8899999856948853\n",
      "Iteration 32620 Training loss 0.0005688929813914001 Validation loss 0.043012604117393494 Accuracy 0.890500009059906\n",
      "Iteration 32630 Training loss 0.0005690704565495253 Validation loss 0.04303383454680443 Accuracy 0.8899999856948853\n",
      "Iteration 32640 Training loss 0.0013247973984107375 Validation loss 0.043065451085567474 Accuracy 0.8884999752044678\n",
      "Iteration 32650 Training loss 0.0005705583607777953 Validation loss 0.04294696822762489 Accuracy 0.890999972820282\n",
      "Iteration 32660 Training loss 0.0013224321883171797 Validation loss 0.043018024414777756 Accuracy 0.8899999856948853\n",
      "Iteration 32670 Training loss 7.101781375240535e-05 Validation loss 0.04298415407538414 Accuracy 0.8899999856948853\n",
      "Iteration 32680 Training loss 0.0005717019666917622 Validation loss 0.04301149770617485 Accuracy 0.890500009059906\n",
      "Iteration 32690 Training loss 0.001064594485796988 Validation loss 0.04300028830766678 Accuracy 0.8899999856948853\n",
      "Iteration 32700 Training loss 0.0010702807921916246 Validation loss 0.04302215948700905 Accuracy 0.8884999752044678\n",
      "Iteration 32710 Training loss 0.0010684916051104665 Validation loss 0.04299436882138252 Accuracy 0.8895000219345093\n",
      "Iteration 32720 Training loss 0.0013216810766607523 Validation loss 0.042990829795598984 Accuracy 0.890500009059906\n",
      "Iteration 32730 Training loss 0.0005699988687410951 Validation loss 0.043026141822338104 Accuracy 0.890500009059906\n",
      "Iteration 32740 Training loss 0.000572750112041831 Validation loss 0.04304865375161171 Accuracy 0.890500009059906\n",
      "Iteration 32750 Training loss 0.0015658901538699865 Validation loss 0.043081026524305344 Accuracy 0.8889999985694885\n",
      "Iteration 32760 Training loss 0.0015682000666856766 Validation loss 0.04309815913438797 Accuracy 0.8889999985694885\n",
      "Iteration 32770 Training loss 0.0005724821239709854 Validation loss 0.043038081377744675 Accuracy 0.8889999985694885\n",
      "Iteration 32780 Training loss 0.0005668624653480947 Validation loss 0.04302634671330452 Accuracy 0.8889999985694885\n",
      "Iteration 32790 Training loss 0.0015694760950282216 Validation loss 0.043065667152404785 Accuracy 0.8884999752044678\n",
      "Iteration 32800 Training loss 0.001319640432484448 Validation loss 0.043047476559877396 Accuracy 0.8889999985694885\n",
      "Iteration 32810 Training loss 0.0008183100144378841 Validation loss 0.04302004352211952 Accuracy 0.8884999752044678\n",
      "Iteration 32820 Training loss 0.0010682223364710808 Validation loss 0.04301222413778305 Accuracy 0.8895000219345093\n",
      "Iteration 32830 Training loss 0.0008178926655091345 Validation loss 0.042947154492139816 Accuracy 0.890500009059906\n",
      "Iteration 32840 Training loss 0.0005671570543199778 Validation loss 0.043034013360738754 Accuracy 0.8895000219345093\n",
      "Iteration 32850 Training loss 0.0008216669666580856 Validation loss 0.043005965650081635 Accuracy 0.890999972820282\n",
      "Iteration 32860 Training loss 0.0010701705468818545 Validation loss 0.04302620515227318 Accuracy 0.8895000219345093\n",
      "Iteration 32870 Training loss 0.0010695889359340072 Validation loss 0.04303276166319847 Accuracy 0.890500009059906\n",
      "Iteration 32880 Training loss 0.0008187610656023026 Validation loss 0.043034669011831284 Accuracy 0.8899999856948853\n",
      "Iteration 32890 Training loss 6.861078145448118e-05 Validation loss 0.04302331432700157 Accuracy 0.8899999856948853\n",
      "Iteration 32900 Training loss 0.0005720703629776835 Validation loss 0.04301919415593147 Accuracy 0.8899999856948853\n",
      "Iteration 32910 Training loss 0.0008152381633408368 Validation loss 0.042991943657398224 Accuracy 0.8899999856948853\n",
      "Iteration 32920 Training loss 0.0015739819500595331 Validation loss 0.04303542152047157 Accuracy 0.8895000219345093\n",
      "Iteration 32930 Training loss 0.000823300564661622 Validation loss 0.043026071041822433 Accuracy 0.8884999752044678\n",
      "Iteration 32940 Training loss 0.001821640646085143 Validation loss 0.043062176555395126 Accuracy 0.8889999985694885\n",
      "Iteration 32950 Training loss 0.0010728471679612994 Validation loss 0.04301638901233673 Accuracy 0.890500009059906\n",
      "Iteration 32960 Training loss 0.00031685613794252276 Validation loss 0.04302995279431343 Accuracy 0.8895000219345093\n",
      "Iteration 32970 Training loss 0.0018142261542379856 Validation loss 0.04304685443639755 Accuracy 0.8899999856948853\n",
      "Iteration 32980 Training loss 0.0008189381333068013 Validation loss 0.04306561127305031 Accuracy 0.8880000114440918\n",
      "Iteration 32990 Training loss 0.001319271046668291 Validation loss 0.043056029826402664 Accuracy 0.8889999985694885\n",
      "Iteration 33000 Training loss 0.0013221624540165067 Validation loss 0.043047815561294556 Accuracy 0.8895000219345093\n",
      "Iteration 33010 Training loss 0.000820128305349499 Validation loss 0.04300505667924881 Accuracy 0.8899999856948853\n",
      "Iteration 33020 Training loss 0.0008176635601557791 Validation loss 0.04300142079591751 Accuracy 0.8899999856948853\n",
      "Iteration 33030 Training loss 0.0008209258085116744 Validation loss 0.04298859462141991 Accuracy 0.8895000219345093\n",
      "Iteration 33040 Training loss 0.0013222477864474058 Validation loss 0.04303311929106712 Accuracy 0.8884999752044678\n",
      "Iteration 33050 Training loss 0.0008247523219324648 Validation loss 0.043016135692596436 Accuracy 0.890500009059906\n",
      "Iteration 33060 Training loss 0.0010667155729606748 Validation loss 0.04301700368523598 Accuracy 0.8899999856948853\n",
      "Iteration 33070 Training loss 0.0005717900930903852 Validation loss 0.04301542043685913 Accuracy 0.8895000219345093\n",
      "Iteration 33080 Training loss 0.000316885591018945 Validation loss 0.043082233518362045 Accuracy 0.8895000219345093\n",
      "Iteration 33090 Training loss 0.0003210545692127198 Validation loss 0.043056268244981766 Accuracy 0.8895000219345093\n",
      "Iteration 33100 Training loss 0.0015730124432593584 Validation loss 0.043018121272325516 Accuracy 0.8895000219345093\n",
      "Iteration 33110 Training loss 0.0015714796027168632 Validation loss 0.04303539916872978 Accuracy 0.8895000219345093\n",
      "Iteration 33120 Training loss 0.0008245685021393001 Validation loss 0.04301323741674423 Accuracy 0.890500009059906\n",
      "Iteration 33130 Training loss 0.000320532446494326 Validation loss 0.04299122467637062 Accuracy 0.890500009059906\n",
      "Iteration 33140 Training loss 7.266338070621714e-05 Validation loss 0.043030042201280594 Accuracy 0.8895000219345093\n",
      "Iteration 33150 Training loss 0.0005703752394765615 Validation loss 0.04298925772309303 Accuracy 0.8899999856948853\n",
      "Iteration 33160 Training loss 0.0005696086445823312 Validation loss 0.043018169701099396 Accuracy 0.8895000219345093\n",
      "Iteration 33170 Training loss 0.0015669079730287194 Validation loss 0.0430489182472229 Accuracy 0.8884999752044678\n",
      "Iteration 33180 Training loss 0.0003182064101565629 Validation loss 0.043070580810308456 Accuracy 0.8884999752044678\n",
      "Iteration 33190 Training loss 0.0008189156069420278 Validation loss 0.04302351921796799 Accuracy 0.8895000219345093\n",
      "Iteration 33200 Training loss 0.0008143424056470394 Validation loss 0.043059300631284714 Accuracy 0.8899999856948853\n",
      "Iteration 33210 Training loss 0.0015692815650254488 Validation loss 0.04302822798490524 Accuracy 0.8895000219345093\n",
      "Iteration 33220 Training loss 0.0013228805037215352 Validation loss 0.043023575097322464 Accuracy 0.8895000219345093\n",
      "Iteration 33230 Training loss 0.0008210510131902993 Validation loss 0.04306614398956299 Accuracy 0.8895000219345093\n",
      "Iteration 33240 Training loss 0.0010671940399333835 Validation loss 0.04301603138446808 Accuracy 0.8889999985694885\n",
      "Iteration 33250 Training loss 0.001321136485785246 Validation loss 0.04305945709347725 Accuracy 0.8889999985694885\n",
      "Iteration 33260 Training loss 0.0003166518290527165 Validation loss 0.04297463968396187 Accuracy 0.890500009059906\n",
      "Iteration 33270 Training loss 0.0003180651110596955 Validation loss 0.04305758327245712 Accuracy 0.8899999856948853\n",
      "Iteration 33280 Training loss 0.00131894089281559 Validation loss 0.04308478906750679 Accuracy 0.8895000219345093\n",
      "Iteration 33290 Training loss 0.00032150602783076465 Validation loss 0.043052591383457184 Accuracy 0.8895000219345093\n",
      "Iteration 33300 Training loss 0.0010687572648748755 Validation loss 0.042977843433618546 Accuracy 0.890500009059906\n",
      "Iteration 33310 Training loss 0.0005767399561591446 Validation loss 0.04304593801498413 Accuracy 0.8899999856948853\n",
      "Iteration 33320 Training loss 0.00032013747841119766 Validation loss 0.04302061349153519 Accuracy 0.8895000219345093\n",
      "Iteration 33330 Training loss 0.0005677121807821095 Validation loss 0.04303814098238945 Accuracy 0.8895000219345093\n",
      "Iteration 33340 Training loss 0.0010696498211473227 Validation loss 0.04303773492574692 Accuracy 0.8895000219345093\n",
      "Iteration 33350 Training loss 0.0013199454406276345 Validation loss 0.043036751449108124 Accuracy 0.8899999856948853\n",
      "Iteration 33360 Training loss 0.0010658081155270338 Validation loss 0.04302222654223442 Accuracy 0.8899999856948853\n",
      "Iteration 33370 Training loss 0.001072445185855031 Validation loss 0.04302305728197098 Accuracy 0.890500009059906\n",
      "Iteration 33380 Training loss 0.0005674443673342466 Validation loss 0.0430125817656517 Accuracy 0.8895000219345093\n",
      "Iteration 33390 Training loss 0.0003220567887183279 Validation loss 0.04302980750799179 Accuracy 0.8889999985694885\n",
      "Iteration 33400 Training loss 0.0003186687536071986 Validation loss 0.04301152750849724 Accuracy 0.8895000219345093\n",
      "Iteration 33410 Training loss 0.0015676685143262148 Validation loss 0.04302395135164261 Accuracy 0.8889999985694885\n",
      "Iteration 33420 Training loss 0.0005679503665305674 Validation loss 0.04301224648952484 Accuracy 0.8895000219345093\n",
      "Iteration 33430 Training loss 0.0015702072996646166 Validation loss 0.043044786900281906 Accuracy 0.8899999856948853\n",
      "Iteration 33440 Training loss 0.0013169713784009218 Validation loss 0.0430709607899189 Accuracy 0.8899999856948853\n",
      "Iteration 33450 Training loss 0.0003234660835005343 Validation loss 0.04303397983312607 Accuracy 0.8889999985694885\n",
      "Iteration 33460 Training loss 0.0008236561552621424 Validation loss 0.043037332594394684 Accuracy 0.8889999985694885\n",
      "Iteration 33470 Training loss 0.0003202604711987078 Validation loss 0.042991675436496735 Accuracy 0.890500009059906\n",
      "Iteration 33480 Training loss 0.0003217089979443699 Validation loss 0.04302104189991951 Accuracy 0.8884999752044678\n",
      "Iteration 33490 Training loss 0.0005704217473976314 Validation loss 0.04308484494686127 Accuracy 0.8889999985694885\n",
      "Iteration 33500 Training loss 0.0013180295936763287 Validation loss 0.043040577322244644 Accuracy 0.8895000219345093\n",
      "Iteration 33510 Training loss 0.0003186043177265674 Validation loss 0.04304145649075508 Accuracy 0.8895000219345093\n",
      "Iteration 33520 Training loss 0.0005729647818952799 Validation loss 0.04306602478027344 Accuracy 0.8895000219345093\n",
      "Iteration 33530 Training loss 0.0005697669112123549 Validation loss 0.04299457371234894 Accuracy 0.8895000219345093\n",
      "Iteration 33540 Training loss 0.0008204939076676965 Validation loss 0.042963817715644836 Accuracy 0.8899999856948853\n",
      "Iteration 33550 Training loss 0.0010753757087513804 Validation loss 0.04302021861076355 Accuracy 0.8889999985694885\n",
      "Iteration 33560 Training loss 0.0008211641106754541 Validation loss 0.04306049272418022 Accuracy 0.890500009059906\n",
      "Iteration 33570 Training loss 0.0013248109025880694 Validation loss 0.043018925935029984 Accuracy 0.8889999985694885\n",
      "Iteration 33580 Training loss 0.0013205230934545398 Validation loss 0.04303630813956261 Accuracy 0.8895000219345093\n",
      "Iteration 33590 Training loss 0.0008211543317884207 Validation loss 0.04303997382521629 Accuracy 0.8899999856948853\n",
      "Iteration 33600 Training loss 0.001067015458829701 Validation loss 0.0430460050702095 Accuracy 0.8880000114440918\n",
      "Iteration 33610 Training loss 0.0013200785033404827 Validation loss 0.043040212243795395 Accuracy 0.8895000219345093\n",
      "Iteration 33620 Training loss 0.0010683126747608185 Validation loss 0.04304393008351326 Accuracy 0.8889999985694885\n",
      "Iteration 33630 Training loss 0.0005738734034821391 Validation loss 0.04298282042145729 Accuracy 0.8895000219345093\n",
      "Iteration 33640 Training loss 0.000823836016934365 Validation loss 0.043065547943115234 Accuracy 0.8884999752044678\n",
      "Iteration 33650 Training loss 0.0010690880008041859 Validation loss 0.04304688423871994 Accuracy 0.890500009059906\n",
      "Iteration 33660 Training loss 0.0010694796219468117 Validation loss 0.04299164190888405 Accuracy 0.8895000219345093\n",
      "Iteration 33670 Training loss 0.0008179132710210979 Validation loss 0.043046921491622925 Accuracy 0.8884999752044678\n",
      "Iteration 33680 Training loss 0.0013211587211117148 Validation loss 0.0430154949426651 Accuracy 0.8895000219345093\n",
      "Iteration 33690 Training loss 0.0013194131897762418 Validation loss 0.04302860423922539 Accuracy 0.8895000219345093\n",
      "Iteration 33700 Training loss 0.0013224645517766476 Validation loss 0.04299725964665413 Accuracy 0.8899999856948853\n",
      "Iteration 33710 Training loss 0.0008194572874344885 Validation loss 0.04303327202796936 Accuracy 0.8889999985694885\n",
      "Iteration 33720 Training loss 0.0008179504657164216 Validation loss 0.04306761547923088 Accuracy 0.8884999752044678\n",
      "Iteration 33730 Training loss 0.0013205914292484522 Validation loss 0.0430787168443203 Accuracy 0.8899999856948853\n",
      "Iteration 33740 Training loss 0.0010700483107939363 Validation loss 0.043027281761169434 Accuracy 0.8899999856948853\n",
      "Iteration 33750 Training loss 0.0018182791536673903 Validation loss 0.043012771755456924 Accuracy 0.8889999985694885\n",
      "Iteration 33760 Training loss 0.0013182153925299644 Validation loss 0.042992379516363144 Accuracy 0.8895000219345093\n",
      "Iteration 33770 Training loss 0.0010641284752637148 Validation loss 0.043023429811000824 Accuracy 0.8889999985694885\n",
      "Iteration 33780 Training loss 0.000822350149974227 Validation loss 0.04300214350223541 Accuracy 0.8895000219345093\n",
      "Iteration 33790 Training loss 0.0010667755268514156 Validation loss 0.043022215366363525 Accuracy 0.8895000219345093\n",
      "Iteration 33800 Training loss 0.00206976430490613 Validation loss 0.04302172735333443 Accuracy 0.8899999856948853\n",
      "Iteration 33810 Training loss 0.0013193486956879497 Validation loss 0.04301402345299721 Accuracy 0.8884999752044678\n",
      "Iteration 33820 Training loss 0.0010771489469334483 Validation loss 0.043027691543102264 Accuracy 0.8895000219345093\n",
      "Iteration 33830 Training loss 0.000824783812277019 Validation loss 0.04299408942461014 Accuracy 0.8899999856948853\n",
      "Iteration 33840 Training loss 0.0015710771549493074 Validation loss 0.043035462498664856 Accuracy 0.8889999985694885\n",
      "Iteration 33850 Training loss 0.0005705266376025975 Validation loss 0.04299284517765045 Accuracy 0.8895000219345093\n",
      "Iteration 33860 Training loss 0.0005668393569067121 Validation loss 0.04301510006189346 Accuracy 0.8899999856948853\n",
      "Iteration 33870 Training loss 0.0003194111632183194 Validation loss 0.04302342236042023 Accuracy 0.8895000219345093\n",
      "Iteration 33880 Training loss 0.0008218383300118148 Validation loss 0.04304422065615654 Accuracy 0.8889999985694885\n",
      "Iteration 33890 Training loss 0.001069566234946251 Validation loss 0.04302549734711647 Accuracy 0.8899999856948853\n",
      "Iteration 33900 Training loss 0.001066642114892602 Validation loss 0.043025147169828415 Accuracy 0.8895000219345093\n",
      "Iteration 33910 Training loss 0.0005761545617133379 Validation loss 0.043029773980379105 Accuracy 0.890500009059906\n",
      "Iteration 33920 Training loss 0.0013196618529036641 Validation loss 0.04306529834866524 Accuracy 0.8899999856948853\n",
      "Iteration 33930 Training loss 0.0005688027013093233 Validation loss 0.0430729053914547 Accuracy 0.8895000219345093\n",
      "Iteration 33940 Training loss 0.0005748281837441027 Validation loss 0.04304739087820053 Accuracy 0.8895000219345093\n",
      "Iteration 33950 Training loss 0.0008175788098014891 Validation loss 0.043023623526096344 Accuracy 0.8895000219345093\n",
      "Iteration 33960 Training loss 0.0008218917064368725 Validation loss 0.043010298162698746 Accuracy 0.8895000219345093\n",
      "Iteration 33970 Training loss 0.0008252099505625665 Validation loss 0.043015070259571075 Accuracy 0.8895000219345093\n",
      "Iteration 33980 Training loss 0.0013147128047421575 Validation loss 0.04307817667722702 Accuracy 0.8899999856948853\n",
      "Iteration 33990 Training loss 0.0013214972568675876 Validation loss 0.043065719306468964 Accuracy 0.8899999856948853\n",
      "Iteration 34000 Training loss 0.0015664283419027925 Validation loss 0.04305320605635643 Accuracy 0.8889999985694885\n",
      "Iteration 34010 Training loss 0.0008220591116696596 Validation loss 0.043038297444581985 Accuracy 0.8895000219345093\n",
      "Iteration 34020 Training loss 0.00032056920463219285 Validation loss 0.043037496507167816 Accuracy 0.8895000219345093\n",
      "Iteration 34030 Training loss 0.001065786462277174 Validation loss 0.043026141822338104 Accuracy 0.8899999856948853\n",
      "Iteration 34040 Training loss 0.0013203517301008105 Validation loss 0.043021298944950104 Accuracy 0.8895000219345093\n",
      "Iteration 34050 Training loss 0.0005683609051629901 Validation loss 0.043038222938776016 Accuracy 0.8895000219345093\n",
      "Iteration 34060 Training loss 0.0010692415526136756 Validation loss 0.04303814098238945 Accuracy 0.8899999856948853\n",
      "Iteration 34070 Training loss 0.0003205871907994151 Validation loss 0.04307982698082924 Accuracy 0.8889999985694885\n",
      "Iteration 34080 Training loss 7.011951674940065e-05 Validation loss 0.043079692870378494 Accuracy 0.8895000219345093\n",
      "Iteration 34090 Training loss 0.0005691379774361849 Validation loss 0.04302078112959862 Accuracy 0.8889999985694885\n",
      "Iteration 34100 Training loss 0.0010748793138191104 Validation loss 0.043037865310907364 Accuracy 0.8895000219345093\n",
      "Iteration 34110 Training loss 0.0010718925623223186 Validation loss 0.043010372668504715 Accuracy 0.8895000219345093\n",
      "Iteration 34120 Training loss 0.0008222167962230742 Validation loss 0.04303934797644615 Accuracy 0.8895000219345093\n",
      "Iteration 34130 Training loss 0.0010681923013180494 Validation loss 0.0430569089949131 Accuracy 0.8895000219345093\n",
      "Iteration 34140 Training loss 0.0003202510706614703 Validation loss 0.04305272549390793 Accuracy 0.8895000219345093\n",
      "Iteration 34150 Training loss 0.000823416979983449 Validation loss 0.04307634383440018 Accuracy 0.8895000219345093\n",
      "Iteration 34160 Training loss 0.0010693059302866459 Validation loss 0.043044883757829666 Accuracy 0.8889999985694885\n",
      "Iteration 34170 Training loss 0.0008194783004000783 Validation loss 0.04311314970254898 Accuracy 0.8895000219345093\n",
      "Iteration 34180 Training loss 0.0010690963827073574 Validation loss 0.04307039827108383 Accuracy 0.8895000219345093\n",
      "Iteration 34190 Training loss 0.0010736588155850768 Validation loss 0.043049346655607224 Accuracy 0.8899999856948853\n",
      "Iteration 34200 Training loss 0.0010741003789007664 Validation loss 0.04307452216744423 Accuracy 0.8895000219345093\n",
      "Iteration 34210 Training loss 0.0003259895311202854 Validation loss 0.043012604117393494 Accuracy 0.8895000219345093\n",
      "Iteration 34220 Training loss 0.0013188108569011092 Validation loss 0.043036531656980515 Accuracy 0.8895000219345093\n",
      "Iteration 34230 Training loss 0.0008192485547624528 Validation loss 0.043071869760751724 Accuracy 0.8899999856948853\n",
      "Iteration 34240 Training loss 0.0008214002591557801 Validation loss 0.0430782325565815 Accuracy 0.8899999856948853\n",
      "Iteration 34250 Training loss 0.0008193803951144218 Validation loss 0.04304476082324982 Accuracy 0.8895000219345093\n",
      "Iteration 34260 Training loss 0.0008196908165700734 Validation loss 0.04306779056787491 Accuracy 0.8895000219345093\n",
      "Iteration 34270 Training loss 0.001575877657160163 Validation loss 0.0431244820356369 Accuracy 0.8895000219345093\n",
      "Iteration 34280 Training loss 0.0010665867011994123 Validation loss 0.043036088347435 Accuracy 0.8895000219345093\n",
      "Iteration 34290 Training loss 0.0008165045874193311 Validation loss 0.04306609183549881 Accuracy 0.8889999985694885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103929890>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34300 Training loss 0.0005699087050743401 Validation loss 0.043063852936029434 Accuracy 0.8899999856948853\n",
      "Iteration 34310 Training loss 0.0008234556880779564 Validation loss 0.04307478666305542 Accuracy 0.8895000219345093\n",
      "Iteration 34320 Training loss 0.0010690581984817982 Validation loss 0.043084852397441864 Accuracy 0.8895000219345093\n",
      "Iteration 34330 Training loss 0.0010777533752843738 Validation loss 0.043080564588308334 Accuracy 0.8895000219345093\n",
      "Iteration 34340 Training loss 0.0010728149209171534 Validation loss 0.043091170489788055 Accuracy 0.8895000219345093\n",
      "Iteration 34350 Training loss 7.273317169165239e-05 Validation loss 0.043068476021289825 Accuracy 0.8899999856948853\n",
      "Iteration 34360 Training loss 0.0013178855879232287 Validation loss 0.043059881776571274 Accuracy 0.8899999856948853\n",
      "Iteration 34370 Training loss 0.001071872073225677 Validation loss 0.04302206262946129 Accuracy 0.8895000219345093\n",
      "Iteration 34380 Training loss 0.0003231304872315377 Validation loss 0.0430799275636673 Accuracy 0.8895000219345093\n",
      "Iteration 34390 Training loss 0.0005659020389430225 Validation loss 0.043033331632614136 Accuracy 0.8895000219345093\n",
      "Iteration 34400 Training loss 0.001068664831109345 Validation loss 0.04306420311331749 Accuracy 0.8899999856948853\n",
      "Iteration 34410 Training loss 0.0008195279515348375 Validation loss 0.04301948472857475 Accuracy 0.8895000219345093\n",
      "Iteration 34420 Training loss 0.000824925082270056 Validation loss 0.04304514080286026 Accuracy 0.8895000219345093\n",
      "Iteration 34430 Training loss 0.000316200457746163 Validation loss 0.04303640499711037 Accuracy 0.8895000219345093\n",
      "Iteration 34440 Training loss 0.0003196412872057408 Validation loss 0.043084412813186646 Accuracy 0.8895000219345093\n",
      "Iteration 34450 Training loss 0.0008170904475264251 Validation loss 0.04309149831533432 Accuracy 0.8889999985694885\n",
      "Iteration 34460 Training loss 0.001317385584115982 Validation loss 0.043040681630373 Accuracy 0.8895000219345093\n",
      "Iteration 34470 Training loss 0.0013169061858206987 Validation loss 0.04306476563215256 Accuracy 0.8899999856948853\n",
      "Iteration 34480 Training loss 7.595127681270242e-05 Validation loss 0.043058231472969055 Accuracy 0.8899999856948853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103929890>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34490 Training loss 0.0008161904406733811 Validation loss 0.04304404556751251 Accuracy 0.8899999856948853\n",
      "Iteration 34500 Training loss 0.0008206662605516613 Validation loss 0.04308800399303436 Accuracy 0.8899999856948853\n",
      "Iteration 34510 Training loss 0.0005692439153790474 Validation loss 0.04306637495756149 Accuracy 0.8895000219345093\n",
      "Iteration 34520 Training loss 7.177391671575606e-05 Validation loss 0.04306180030107498 Accuracy 0.8899999856948853\n",
      "Iteration 34530 Training loss 0.0010699518024921417 Validation loss 0.04305076226592064 Accuracy 0.8895000219345093\n",
      "Iteration 34540 Training loss 0.0008228010847233236 Validation loss 0.043029844760894775 Accuracy 0.8889999985694885\n",
      "Iteration 34550 Training loss 0.0005697642918676138 Validation loss 0.04307372868061066 Accuracy 0.8895000219345093\n",
      "Iteration 34560 Training loss 0.00132481730543077 Validation loss 0.04302767291665077 Accuracy 0.8895000219345093\n",
      "Iteration 34570 Training loss 0.0005708620883524418 Validation loss 0.04304558038711548 Accuracy 0.8895000219345093\n",
      "Iteration 34580 Training loss 0.000574285804759711 Validation loss 0.04303527995944023 Accuracy 0.8899999856948853\n",
      "Iteration 34590 Training loss 0.0015716876368969679 Validation loss 0.04303188994526863 Accuracy 0.8899999856948853\n",
      "Iteration 34600 Training loss 0.0013173491461202502 Validation loss 0.04306139051914215 Accuracy 0.8889999985694885\n",
      "Iteration 34610 Training loss 0.0018180208280682564 Validation loss 0.04307563602924347 Accuracy 0.8899999856948853\n",
      "Iteration 34620 Training loss 7.018539326963946e-05 Validation loss 0.04303887113928795 Accuracy 0.8889999985694885\n",
      "Iteration 34630 Training loss 0.0005749331903643906 Validation loss 0.04300694912672043 Accuracy 0.8895000219345093\n",
      "Iteration 34640 Training loss 0.0015724132535979152 Validation loss 0.04308425262570381 Accuracy 0.8884999752044678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103929890>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34650 Training loss 0.0013158555375412107 Validation loss 0.04304435849189758 Accuracy 0.8889999985694885\n",
      "Iteration 34660 Training loss 0.0008224374032579362 Validation loss 0.0430477149784565 Accuracy 0.8895000219345093\n",
      "Iteration 34670 Training loss 0.0010679855477064848 Validation loss 0.043052636086940765 Accuracy 0.8899999856948853\n",
      "Iteration 34680 Training loss 0.0008180628065019846 Validation loss 0.04301370307803154 Accuracy 0.8895000219345093\n",
      "Iteration 34690 Training loss 0.0013216326478868723 Validation loss 0.04299300163984299 Accuracy 0.890500009059906\n",
      "Iteration 34700 Training loss 0.0015670597786083817 Validation loss 0.043031252920627594 Accuracy 0.8899999856948853\n",
      "Iteration 34710 Training loss 0.001321535324677825 Validation loss 0.0429999865591526 Accuracy 0.8899999856948853\n",
      "Iteration 34720 Training loss 0.0008188012870959938 Validation loss 0.043049801141023636 Accuracy 0.8899999856948853\n",
      "Iteration 34730 Training loss 0.00132210913579911 Validation loss 0.04304357245564461 Accuracy 0.8899999856948853\n",
      "Iteration 34740 Training loss 0.00032563600689172745 Validation loss 0.04304056987166405 Accuracy 0.8899999856948853\n",
      "Iteration 34750 Training loss 0.0013165045529603958 Validation loss 0.043031200766563416 Accuracy 0.8895000219345093\n",
      "Iteration 34760 Training loss 0.0010716167744249105 Validation loss 0.043084245175123215 Accuracy 0.8895000219345093\n",
      "Iteration 34770 Training loss 0.0008192548411898315 Validation loss 0.043043602257966995 Accuracy 0.8895000219345093\n",
      "Iteration 34780 Training loss 0.0020664234180003405 Validation loss 0.04306050390005112 Accuracy 0.8899999856948853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103929890>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34790 Training loss 0.00081682950258255 Validation loss 0.04304416477680206 Accuracy 0.8895000219345093\n",
      "Iteration 34800 Training loss 0.0010741084115579724 Validation loss 0.043053001165390015 Accuracy 0.8899999856948853\n",
      "Iteration 34810 Training loss 0.000578713312279433 Validation loss 0.04303881153464317 Accuracy 0.8899999856948853\n",
      "Iteration 34820 Training loss 0.0005758429761044681 Validation loss 0.04309160262346268 Accuracy 0.8899999856948853\n",
      "Iteration 34830 Training loss 0.0013162823161110282 Validation loss 0.043015412986278534 Accuracy 0.8895000219345093\n",
      "Iteration 34840 Training loss 0.000573242490645498 Validation loss 0.0430593304336071 Accuracy 0.8899999856948853\n",
      "Iteration 34850 Training loss 0.0008222777396440506 Validation loss 0.04303552955389023 Accuracy 0.8895000219345093\n",
      "Iteration 34860 Training loss 0.0008228285005316138 Validation loss 0.043092384934425354 Accuracy 0.8895000219345093\n",
      "Iteration 34870 Training loss 0.0003208722628187388 Validation loss 0.043047476559877396 Accuracy 0.8899999856948853\n",
      "Iteration 34880 Training loss 0.0005691832629963756 Validation loss 0.04307615011930466 Accuracy 0.8899999856948853\n",
      "Iteration 34890 Training loss 0.0008176039555110037 Validation loss 0.043043628334999084 Accuracy 0.8899999856948853\n",
      "Iteration 34900 Training loss 0.0010709201451390982 Validation loss 0.043053895235061646 Accuracy 0.8899999856948853\n",
      "Iteration 34910 Training loss 0.00032122639822773635 Validation loss 0.04306858405470848 Accuracy 0.8899999856948853\n",
      "Iteration 34920 Training loss 0.001073177088983357 Validation loss 0.0430847704410553 Accuracy 0.890500009059906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103929890>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34930 Training loss 0.0015717883361503482 Validation loss 0.043037351220846176 Accuracy 0.8895000219345093\n",
      "Iteration 34940 Training loss 0.0008251528488472104 Validation loss 0.043004173785448074 Accuracy 0.8895000219345093\n",
      "Iteration 34950 Training loss 0.0005697467131540179 Validation loss 0.04306185990571976 Accuracy 0.8895000219345093\n",
      "Iteration 34960 Training loss 0.0008199086878448725 Validation loss 0.0430184081196785 Accuracy 0.8895000219345093\n",
      "Iteration 34970 Training loss 0.0013163466937839985 Validation loss 0.0430731438100338 Accuracy 0.8899999856948853\n",
      "Iteration 34980 Training loss 0.0008213600958697498 Validation loss 0.04304572939872742 Accuracy 0.8895000219345093\n",
      "Iteration 34990 Training loss 0.0008143701707012951 Validation loss 0.04302232712507248 Accuracy 0.8895000219345093\n",
      "Iteration 35000 Training loss 0.0010752720991149545 Validation loss 0.043042369186878204 Accuracy 0.8899999856948853\n",
      "Iteration 35010 Training loss 0.0008188625797629356 Validation loss 0.0430946946144104 Accuracy 0.8899999856948853\n",
      "Iteration 35020 Training loss 0.001071742968633771 Validation loss 0.043068818747997284 Accuracy 0.8899999856948853\n",
      "Iteration 35030 Training loss 0.000570333213545382 Validation loss 0.04307287558913231 Accuracy 0.8895000219345093\n",
      "Iteration 35040 Training loss 0.0008144390303641558 Validation loss 0.04307495802640915 Accuracy 0.8895000219345093\n",
      "Iteration 35050 Training loss 0.0013177917571738362 Validation loss 0.04303659871220589 Accuracy 0.8899999856948853\n",
      "Iteration 35060 Training loss 0.0010716435499489307 Validation loss 0.043090324848890305 Accuracy 0.8899999856948853\n",
      "Iteration 35070 Training loss 0.001071111299097538 Validation loss 0.04304346442222595 Accuracy 0.8899999856948853\n",
      "Iteration 35080 Training loss 0.0013234553625807166 Validation loss 0.043021004647016525 Accuracy 0.8895000219345093\n",
      "Iteration 35090 Training loss 0.001069903140887618 Validation loss 0.04306069761514664 Accuracy 0.8899999856948853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103929890>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35100 Training loss 0.001316872308962047 Validation loss 0.043076928704977036 Accuracy 0.8899999856948853\n",
      "Iteration 35110 Training loss 0.001070058555342257 Validation loss 0.04308571666479111 Accuracy 0.8895000219345093\n",
      "Iteration 35120 Training loss 0.0005727476091124117 Validation loss 0.04306793212890625 Accuracy 0.8899999856948853\n",
      "Iteration 35130 Training loss 0.000816338520962745 Validation loss 0.04307865723967552 Accuracy 0.8899999856948853\n",
      "Iteration 35140 Training loss 0.001071619102731347 Validation loss 0.0430748425424099 Accuracy 0.8899999856948853\n",
      "Iteration 35150 Training loss 0.0005676166038028896 Validation loss 0.04303716868162155 Accuracy 0.8895000219345093\n",
      "Iteration 35160 Training loss 0.0008151676156558096 Validation loss 0.04307814687490463 Accuracy 0.8895000219345093\n",
      "Iteration 35170 Training loss 0.0005715865991078317 Validation loss 0.04306873306632042 Accuracy 0.8899999856948853\n",
      "Iteration 35180 Training loss 0.0005695846048183739 Validation loss 0.043064963072538376 Accuracy 0.8895000219345093\n",
      "Iteration 35190 Training loss 0.0010732781374827027 Validation loss 0.0430678129196167 Accuracy 0.8889999985694885\n",
      "Iteration 35200 Training loss 0.0008202243479900062 Validation loss 0.04305442050099373 Accuracy 0.8899999856948853\n",
      "Iteration 35210 Training loss 0.0005708091775886714 Validation loss 0.043071966618299484 Accuracy 0.8899999856948853\n",
      "Iteration 35220 Training loss 0.0010732623049989343 Validation loss 0.04307634383440018 Accuracy 0.8895000219345093\n",
      "Iteration 35230 Training loss 0.00031731926719658077 Validation loss 0.04307427629828453 Accuracy 0.8899999856948853\n",
      "Iteration 35240 Training loss 0.0008193039684556425 Validation loss 0.04308174178004265 Accuracy 0.8899999856948853\n",
      "Iteration 35250 Training loss 0.0010701207211241126 Validation loss 0.04306973144412041 Accuracy 0.8895000219345093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103929890>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35260 Training loss 0.0015658163465559483 Validation loss 0.04307115823030472 Accuracy 0.8899999856948853\n",
      "Iteration 35270 Training loss 0.0013278948608785868 Validation loss 0.04307736083865166 Accuracy 0.8899999856948853\n",
      "Iteration 35280 Training loss 0.0005741789937019348 Validation loss 0.04302912577986717 Accuracy 0.8895000219345093\n",
      "Iteration 35290 Training loss 0.0008242575568147004 Validation loss 0.043040305376052856 Accuracy 0.8895000219345093\n",
      "Iteration 35300 Training loss 0.0015672134468331933 Validation loss 0.043114952743053436 Accuracy 0.8899999856948853\n",
      "Iteration 35310 Training loss 0.00031552999280393124 Validation loss 0.04305311664938927 Accuracy 0.8895000219345093\n",
      "Iteration 35320 Training loss 0.0008175924886018038 Validation loss 0.04308277741074562 Accuracy 0.8899999856948853\n",
      "Iteration 35330 Training loss 6.331880285870284e-05 Validation loss 0.043075334280729294 Accuracy 0.8895000219345093\n",
      "Iteration 35340 Training loss 0.0003161704808007926 Validation loss 0.043048545718193054 Accuracy 0.8899999856948853\n",
      "Iteration 35350 Training loss 0.0005669399979524314 Validation loss 0.04309212788939476 Accuracy 0.8895000219345093\n",
      "Iteration 35360 Training loss 0.0008160232682712376 Validation loss 0.043051980435848236 Accuracy 0.8899999856948853\n",
      "Iteration 35370 Training loss 0.0008201290620490909 Validation loss 0.04308060556650162 Accuracy 0.8899999856948853\n",
      "Iteration 35380 Training loss 0.0008181544253602624 Validation loss 0.043043266981840134 Accuracy 0.8895000219345093\n",
      "Iteration 35390 Training loss 0.0008192177629098296 Validation loss 0.043073512613773346 Accuracy 0.8895000219345093\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbinary_model_4_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 733\u001b[39m, in \u001b[36mbinary_classification_four_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, lr_decay_rate, reg1, reg2, reg3, reg4, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2, train_layer_3, train_layer_4, dropout_rate)\u001b[39m\n\u001b[32m    730\u001b[39m grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) \u001b[38;5;66;03m# shape (n_data, hidden_1_size)\u001b[39;00m\n\u001b[32m    732\u001b[39m \u001b[38;5;66;03m# Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m grad_W1 = (\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_z1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_minibatch\u001b[49m\u001b[43m)\u001b[49m/x_minibatch.shape[\u001b[32m0\u001b[39m]).to(dtype) \u001b[38;5;66;03m# shape (hidden_1_size, input_dimension)\u001b[39;00m\n\u001b[32m    734\u001b[39m \u001b[38;5;66;03m# Calcul de la moyenne empirique de dLoss/db1 par backpropagation\u001b[39;00m\n\u001b[32m    735\u001b[39m grad_b1 = (torch.mean(grad_z1, dim=\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m1\u001b[39m)).to(dtype) \n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "binary_model_4_layer.train_layers(x_train, y_train, x_valid, y_valid, 2.42, 1e-2, 1e8, 1e-3, 1e-3, 1e-3, 1e-3, 1, 0.2, 10, True, True, True, True, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "122fd711",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_4_layer_extreme_trained = binary_classification_four_layer_NN(1024, 512, 512, 512, eps_init = 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43452ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_4_layer_extreme_trained.train_layers(x_train, y_train, x_valid, y_valid, 2.45, 1e-1, 5e8, 1e-3, 1e-3, 1e-3, 1e-3, 1, 0.2, 10, True, False, False, True, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429aa977",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_5_layer = binary_classification_five_layer_NN(1024, 512, 512, 512, 512, eps_init = 5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94803c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2.37, the number of datas used for the training is 68136669.50093094 and the number of iterations is 34068.\n",
      "Iteration 0 Training loss 0.12499884516000748 Validation loss 0.12499914318323135 Accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 Training loss 0.12498527020215988 Validation loss 0.12498287856578827 Accuracy 0.5\n",
      "Iteration 20 Training loss 0.1249694973230362 Validation loss 0.1249675452709198 Accuracy 0.6290000081062317\n",
      "Iteration 30 Training loss 0.12495198100805283 Validation loss 0.12494880706071854 Accuracy 0.6840000152587891\n",
      "Iteration 40 Training loss 0.12493114173412323 Validation loss 0.12492766231298447 Accuracy 0.6075000166893005\n",
      "Iteration 50 Training loss 0.12491574138402939 Validation loss 0.12490221858024597 Accuracy 0.5559999942779541\n",
      "Iteration 60 Training loss 0.12488749623298645 Validation loss 0.12487311661243439 Accuracy 0.5245000123977661\n",
      "Iteration 70 Training loss 0.12485364079475403 Validation loss 0.1248384639620781 Accuracy 0.5109999775886536\n",
      "Iteration 80 Training loss 0.12484239041805267 Validation loss 0.12479759007692337 Accuracy 0.5015000104904175\n",
      "Iteration 90 Training loss 0.1247750073671341 Validation loss 0.12473946809768677 Accuracy 0.49950000643730164\n",
      "Iteration 100 Training loss 0.124611034989357 Validation loss 0.1246640533208847 Accuracy 0.49900001287460327\n",
      "Iteration 110 Training loss 0.12457772344350815 Validation loss 0.12455961853265762 Accuracy 0.49950000643730164\n",
      "Iteration 120 Training loss 0.1244213804602623 Validation loss 0.12444059550762177 Accuracy 0.49950000643730164\n",
      "Iteration 130 Training loss 0.12423727661371231 Validation loss 0.1242859810590744 Accuracy 0.49950000643730164\n",
      "Iteration 140 Training loss 0.12437335401773453 Validation loss 0.12408147752285004 Accuracy 0.5\n",
      "Iteration 150 Training loss 0.1238778829574585 Validation loss 0.12387404590845108 Accuracy 0.5\n",
      "Iteration 160 Training loss 0.12376955896615982 Validation loss 0.12357889115810394 Accuracy 0.5\n",
      "Iteration 170 Training loss 0.12350374460220337 Validation loss 0.12327536195516586 Accuracy 0.5\n",
      "Iteration 180 Training loss 0.12239161878824234 Validation loss 0.12288932502269745 Accuracy 0.5\n",
      "Iteration 190 Training loss 0.12275244295597076 Validation loss 0.1223127543926239 Accuracy 0.5\n",
      "Iteration 200 Training loss 0.12173638492822647 Validation loss 0.1214546337723732 Accuracy 0.49950000643730164\n",
      "Iteration 210 Training loss 0.1204061359167099 Validation loss 0.11983740329742432 Accuracy 0.5109999775886536\n",
      "Iteration 220 Training loss 0.1178346648812294 Validation loss 0.11720318347215652 Accuracy 0.5929999947547913\n",
      "Iteration 230 Training loss 0.11355435848236084 Validation loss 0.11253466457128525 Accuracy 0.6205000281333923\n",
      "Iteration 240 Training loss 0.11457868665456772 Validation loss 0.11289525032043457 Accuracy 0.7105000019073486\n",
      "Iteration 250 Training loss 0.11691772192716599 Validation loss 0.11409343034029007 Accuracy 0.6334999799728394\n",
      "Iteration 260 Training loss 0.10808726400136948 Validation loss 0.10678721964359283 Accuracy 0.6899999976158142\n",
      "Iteration 270 Training loss 0.10841221362352371 Validation loss 0.10738079994916916 Accuracy 0.6639999747276306\n",
      "Iteration 280 Training loss 0.10426624864339828 Validation loss 0.09997236728668213 Accuracy 0.7124999761581421\n",
      "Iteration 290 Training loss 0.09390711039304733 Validation loss 0.09137529879808426 Accuracy 0.7509999871253967\n",
      "Iteration 300 Training loss 0.09819719195365906 Validation loss 0.09411166608333588 Accuracy 0.7260000109672546\n",
      "Iteration 310 Training loss 0.09517181664705276 Validation loss 0.08903007209300995 Accuracy 0.7505000233650208\n",
      "Iteration 320 Training loss 0.08805770426988602 Validation loss 0.08448399603366852 Accuracy 0.7670000195503235\n",
      "Iteration 330 Training loss 0.08507378399372101 Validation loss 0.08007155358791351 Accuracy 0.7854999899864197\n",
      "Iteration 340 Training loss 0.08422906696796417 Validation loss 0.08086992055177689 Accuracy 0.7799999713897705\n",
      "Iteration 350 Training loss 0.08578553795814514 Validation loss 0.08246929943561554 Accuracy 0.7695000171661377\n",
      "Iteration 360 Training loss 0.08586645871400833 Validation loss 0.07773090898990631 Accuracy 0.7889999747276306\n",
      "Iteration 370 Training loss 0.07723185420036316 Validation loss 0.07302346080541611 Accuracy 0.8004999756813049\n",
      "Iteration 380 Training loss 0.07876526564359665 Validation loss 0.07546330988407135 Accuracy 0.7914999723434448\n",
      "Iteration 390 Training loss 0.07535114884376526 Validation loss 0.07316333055496216 Accuracy 0.8004999756813049\n",
      "Iteration 400 Training loss 0.07875339686870575 Validation loss 0.07611225545406342 Accuracy 0.7914999723434448\n",
      "Iteration 410 Training loss 0.07545445114374161 Validation loss 0.07014930248260498 Accuracy 0.8044999837875366\n",
      "Iteration 420 Training loss 0.07880614697933197 Validation loss 0.07349159568548203 Accuracy 0.8009999990463257\n",
      "Iteration 430 Training loss 0.07371161133050919 Validation loss 0.07104191929101944 Accuracy 0.8054999709129333\n",
      "Iteration 440 Training loss 0.07888134568929672 Validation loss 0.07181822508573532 Accuracy 0.8054999709129333\n",
      "Iteration 450 Training loss 0.07491226494312286 Validation loss 0.06889032572507858 Accuracy 0.8075000047683716\n",
      "Iteration 460 Training loss 0.07579541206359863 Validation loss 0.07179687917232513 Accuracy 0.8065000176429749\n",
      "Iteration 470 Training loss 0.07339169085025787 Validation loss 0.06969527900218964 Accuracy 0.8084999918937683\n",
      "Iteration 480 Training loss 0.07264547795057297 Validation loss 0.06881237775087357 Accuracy 0.8100000023841858\n",
      "Iteration 490 Training loss 0.07057363539934158 Validation loss 0.06658203154802322 Accuracy 0.8159999847412109\n",
      "Iteration 500 Training loss 0.07158707827329636 Validation loss 0.06602071225643158 Accuracy 0.8195000290870667\n",
      "Iteration 510 Training loss 0.07149291038513184 Validation loss 0.07081803679466248 Accuracy 0.8065000176429749\n",
      "Iteration 520 Training loss 0.07344599813222885 Validation loss 0.06812950223684311 Accuracy 0.8109999895095825\n",
      "Iteration 530 Training loss 0.06493354588747025 Validation loss 0.06461057811975479 Accuracy 0.8184999823570251\n",
      "Iteration 540 Training loss 0.06435679644346237 Validation loss 0.06318697333335876 Accuracy 0.824999988079071\n",
      "Iteration 550 Training loss 0.07206592708826065 Validation loss 0.07334496825933456 Accuracy 0.7839999794960022\n",
      "Iteration 560 Training loss 0.07515687495470047 Validation loss 0.07662751525640488 Accuracy 0.7760000228881836\n",
      "Iteration 570 Training loss 0.059942204505205154 Validation loss 0.06479782611131668 Accuracy 0.8130000233650208\n",
      "Iteration 580 Training loss 0.07970167696475983 Validation loss 0.081687793135643 Accuracy 0.7580000162124634\n",
      "Iteration 590 Training loss 0.06966949999332428 Validation loss 0.06962646543979645 Accuracy 0.7960000038146973\n",
      "Iteration 600 Training loss 0.0638299509882927 Validation loss 0.061950989067554474 Accuracy 0.828000009059906\n",
      "Iteration 610 Training loss 0.06884194910526276 Validation loss 0.07158961147069931 Accuracy 0.7925000190734863\n",
      "Iteration 620 Training loss 0.06229323893785477 Validation loss 0.06565888226032257 Accuracy 0.8109999895095825\n",
      "Iteration 630 Training loss 0.07238568365573883 Validation loss 0.07377447187900543 Accuracy 0.7860000133514404\n",
      "Iteration 640 Training loss 0.06895831972360611 Validation loss 0.07533040642738342 Accuracy 0.781499981880188\n",
      "Iteration 650 Training loss 0.06688157469034195 Validation loss 0.06816089153289795 Accuracy 0.8029999732971191\n",
      "Iteration 660 Training loss 0.059725284576416016 Validation loss 0.06345980614423752 Accuracy 0.8220000267028809\n",
      "Iteration 670 Training loss 0.07030114531517029 Validation loss 0.06899317353963852 Accuracy 0.8029999732971191\n",
      "Iteration 680 Training loss 0.0644133910536766 Validation loss 0.06685826182365417 Accuracy 0.8059999942779541\n",
      "Iteration 690 Training loss 0.058537017554044724 Validation loss 0.06101050600409508 Accuracy 0.8324999809265137\n",
      "Iteration 700 Training loss 0.06546381115913391 Validation loss 0.06844012439250946 Accuracy 0.8044999837875366\n",
      "Iteration 710 Training loss 0.057729076594114304 Validation loss 0.061816245317459106 Accuracy 0.8255000114440918\n",
      "Iteration 720 Training loss 0.06140340119600296 Validation loss 0.06475406140089035 Accuracy 0.8149999976158142\n",
      "Iteration 730 Training loss 0.06804510951042175 Validation loss 0.0687185674905777 Accuracy 0.8025000095367432\n",
      "Iteration 740 Training loss 0.05853872746229172 Validation loss 0.06298262625932693 Accuracy 0.8165000081062317\n",
      "Iteration 750 Training loss 0.05729076638817787 Validation loss 0.05841723829507828 Accuracy 0.8314999938011169\n",
      "Iteration 760 Training loss 0.05427594482898712 Validation loss 0.057004716247320175 Accuracy 0.8420000076293945\n",
      "Iteration 770 Training loss 0.06826287508010864 Validation loss 0.06672007590532303 Accuracy 0.8174999952316284\n",
      "Iteration 780 Training loss 0.056565795093774796 Validation loss 0.058474425226449966 Accuracy 0.8410000205039978\n",
      "Iteration 790 Training loss 0.06870666146278381 Validation loss 0.06616521626710892 Accuracy 0.8195000290870667\n",
      "Iteration 800 Training loss 0.05387010797858238 Validation loss 0.057745371013879776 Accuracy 0.8424999713897705\n",
      "Iteration 810 Training loss 0.06735467165708542 Validation loss 0.06636551767587662 Accuracy 0.8224999904632568\n",
      "Iteration 820 Training loss 0.06563182920217514 Validation loss 0.0631895512342453 Accuracy 0.8274999856948853\n",
      "Iteration 830 Training loss 0.05644010379910469 Validation loss 0.0565505214035511 Accuracy 0.8489999771118164\n",
      "Iteration 840 Training loss 0.05976516008377075 Validation loss 0.05809697136282921 Accuracy 0.8429999947547913\n",
      "Iteration 850 Training loss 0.06528115272521973 Validation loss 0.06381998211145401 Accuracy 0.8289999961853027\n",
      "Iteration 860 Training loss 0.05864456668496132 Validation loss 0.058522943407297134 Accuracy 0.8399999737739563\n",
      "Iteration 870 Training loss 0.06195906177163124 Validation loss 0.061323318630456924 Accuracy 0.8320000171661377\n",
      "Iteration 880 Training loss 0.05333055928349495 Validation loss 0.054932549595832825 Accuracy 0.8510000109672546\n",
      "Iteration 890 Training loss 0.05122029781341553 Validation loss 0.05690457671880722 Accuracy 0.8395000100135803\n",
      "Iteration 900 Training loss 0.052178483456373215 Validation loss 0.05981864780187607 Accuracy 0.8324999809265137\n",
      "Iteration 910 Training loss 0.05685746669769287 Validation loss 0.06371564418077469 Accuracy 0.8195000290870667\n",
      "Iteration 920 Training loss 0.05786940082907677 Validation loss 0.06530037522315979 Accuracy 0.8134999871253967\n",
      "Iteration 930 Training loss 0.05184583365917206 Validation loss 0.05725911632180214 Accuracy 0.8370000123977661\n",
      "Iteration 940 Training loss 0.0533340759575367 Validation loss 0.06130281835794449 Accuracy 0.8264999985694885\n",
      "Iteration 950 Training loss 0.05320733040571213 Validation loss 0.05933094769716263 Accuracy 0.8330000042915344\n",
      "Iteration 960 Training loss 0.05153774097561836 Validation loss 0.05861879885196686 Accuracy 0.8355000019073486\n",
      "Iteration 970 Training loss 0.05781595781445503 Validation loss 0.0654999390244484 Accuracy 0.8169999718666077\n",
      "Iteration 980 Training loss 0.0560167022049427 Validation loss 0.06255005300045013 Accuracy 0.8224999904632568\n",
      "Iteration 990 Training loss 0.04822278767824173 Validation loss 0.05582136660814285 Accuracy 0.8414999842643738\n",
      "Iteration 1000 Training loss 0.05647412687540054 Validation loss 0.06082557514309883 Accuracy 0.828499972820282\n",
      "Iteration 1010 Training loss 0.04846645146608353 Validation loss 0.05575278773903847 Accuracy 0.8420000076293945\n",
      "Iteration 1020 Training loss 0.055615901947021484 Validation loss 0.0645839273929596 Accuracy 0.8169999718666077\n",
      "Iteration 1030 Training loss 0.05374427139759064 Validation loss 0.06321067363023758 Accuracy 0.8190000057220459\n",
      "Iteration 1040 Training loss 0.053233552724123 Validation loss 0.0630732849240303 Accuracy 0.8209999799728394\n",
      "Iteration 1050 Training loss 0.055585019290447235 Validation loss 0.06449922919273376 Accuracy 0.8159999847412109\n",
      "Iteration 1060 Training loss 0.05699995905160904 Validation loss 0.06201072782278061 Accuracy 0.8240000009536743\n",
      "Iteration 1070 Training loss 0.05050960183143616 Validation loss 0.059588659554719925 Accuracy 0.8309999704360962\n",
      "Iteration 1080 Training loss 0.052814777940511703 Validation loss 0.05982634425163269 Accuracy 0.8299999833106995\n",
      "Iteration 1090 Training loss 0.04678948223590851 Validation loss 0.05712577700614929 Accuracy 0.8370000123977661\n",
      "Iteration 1100 Training loss 0.04576355218887329 Validation loss 0.05759934335947037 Accuracy 0.8374999761581421\n",
      "Iteration 1110 Training loss 0.0511644221842289 Validation loss 0.05971891060471535 Accuracy 0.8299999833106995\n",
      "Iteration 1120 Training loss 0.05115451663732529 Validation loss 0.06080295890569687 Accuracy 0.8264999985694885\n",
      "Iteration 1130 Training loss 0.04890081658959389 Validation loss 0.058828189969062805 Accuracy 0.8305000066757202\n",
      "Iteration 1140 Training loss 0.04784464091062546 Validation loss 0.05699487403035164 Accuracy 0.8385000228881836\n",
      "Iteration 1150 Training loss 0.055790018290281296 Validation loss 0.06566586345434189 Accuracy 0.8144999742507935\n",
      "Iteration 1160 Training loss 0.05465562269091606 Validation loss 0.06284287571907043 Accuracy 0.824999988079071\n",
      "Iteration 1170 Training loss 0.04909452050924301 Validation loss 0.05931825935840607 Accuracy 0.8320000171661377\n",
      "Iteration 1180 Training loss 0.04086751863360405 Validation loss 0.056136354804039 Accuracy 0.8410000205039978\n",
      "Iteration 1190 Training loss 0.050153542309999466 Validation loss 0.06041797995567322 Accuracy 0.8289999961853027\n",
      "Iteration 1200 Training loss 0.04197153449058533 Validation loss 0.05676247552037239 Accuracy 0.8395000100135803\n",
      "Iteration 1210 Training loss 0.051584672182798386 Validation loss 0.0647868663072586 Accuracy 0.8184999823570251\n",
      "Iteration 1220 Training loss 0.04533352702856064 Validation loss 0.058798015117645264 Accuracy 0.8324999809265137\n",
      "Iteration 1230 Training loss 0.04776548221707344 Validation loss 0.05827309563755989 Accuracy 0.8345000147819519\n",
      "Iteration 1240 Training loss 0.04610529914498329 Validation loss 0.05754194036126137 Accuracy 0.8360000252723694\n",
      "Iteration 1250 Training loss 0.04453715682029724 Validation loss 0.05694763734936714 Accuracy 0.8355000019073486\n",
      "Iteration 1260 Training loss 0.043378304690122604 Validation loss 0.05444059148430824 Accuracy 0.843500018119812\n",
      "Iteration 1270 Training loss 0.04407595843076706 Validation loss 0.059680670499801636 Accuracy 0.8320000171661377\n",
      "Iteration 1280 Training loss 0.038372550159692764 Validation loss 0.05331241711974144 Accuracy 0.8479999899864197\n",
      "Iteration 1290 Training loss 0.04729863628745079 Validation loss 0.06001129746437073 Accuracy 0.8289999961853027\n",
      "Iteration 1300 Training loss 0.044911906123161316 Validation loss 0.05824580416083336 Accuracy 0.828000009059906\n",
      "Iteration 1310 Training loss 0.04502815380692482 Validation loss 0.058996498584747314 Accuracy 0.8305000066757202\n",
      "Iteration 1320 Training loss 0.04449795186519623 Validation loss 0.05536109209060669 Accuracy 0.8410000205039978\n",
      "Iteration 1330 Training loss 0.045344337821006775 Validation loss 0.057264022529125214 Accuracy 0.8360000252723694\n",
      "Iteration 1340 Training loss 0.04402642697095871 Validation loss 0.05823049321770668 Accuracy 0.8339999914169312\n",
      "Iteration 1350 Training loss 0.05450357124209404 Validation loss 0.06719514727592468 Accuracy 0.8190000057220459\n",
      "Iteration 1360 Training loss 0.04893173277378082 Validation loss 0.06131085753440857 Accuracy 0.8255000114440918\n",
      "Iteration 1370 Training loss 0.03241097182035446 Validation loss 0.05041726306080818 Accuracy 0.8629999756813049\n",
      "Iteration 1380 Training loss 0.03344331681728363 Validation loss 0.04921254143118858 Accuracy 0.8644999861717224\n",
      "Iteration 1390 Training loss 0.05542009696364403 Validation loss 0.06872116029262543 Accuracy 0.815500020980835\n",
      "Iteration 1400 Training loss 0.047260332852602005 Validation loss 0.0534905381500721 Accuracy 0.8544999957084656\n",
      "Iteration 1410 Training loss 0.039517518132925034 Validation loss 0.050445280969142914 Accuracy 0.8619999885559082\n",
      "Iteration 1420 Training loss 0.03325778990983963 Validation loss 0.04747682437300682 Accuracy 0.8709999918937683\n",
      "Iteration 1430 Training loss 0.03497586399316788 Validation loss 0.049229785799980164 Accuracy 0.8659999966621399\n",
      "Iteration 1440 Training loss 0.08965644985437393 Validation loss 0.09223578125238419 Accuracy 0.7400000095367432\n",
      "Iteration 1450 Training loss 0.04324568435549736 Validation loss 0.050169456750154495 Accuracy 0.8650000095367432\n",
      "Iteration 1460 Training loss 0.04026629030704498 Validation loss 0.05037827044725418 Accuracy 0.862500011920929\n",
      "Iteration 1470 Training loss 0.04772564396262169 Validation loss 0.056728560477495193 Accuracy 0.8489999771118164\n",
      "Iteration 1480 Training loss 0.033992700278759 Validation loss 0.048309799283742905 Accuracy 0.8675000071525574\n",
      "Iteration 1490 Training loss 0.04969065263867378 Validation loss 0.05814047157764435 Accuracy 0.8414999842643738\n",
      "Iteration 1500 Training loss 0.03153134882450104 Validation loss 0.04714879021048546 Accuracy 0.8730000257492065\n",
      "Iteration 1510 Training loss 0.04950375482439995 Validation loss 0.06014806404709816 Accuracy 0.8414999842643738\n",
      "Iteration 1520 Training loss 0.03470723703503609 Validation loss 0.04862341284751892 Accuracy 0.8650000095367432\n",
      "Iteration 1530 Training loss 0.04437265917658806 Validation loss 0.05623013898730278 Accuracy 0.8489999771118164\n",
      "Iteration 1540 Training loss 0.038682352751493454 Validation loss 0.05119854956865311 Accuracy 0.8654999732971191\n",
      "Iteration 1550 Training loss 0.03590792417526245 Validation loss 0.049989182502031326 Accuracy 0.8659999966621399\n",
      "Iteration 1560 Training loss 0.04459511488676071 Validation loss 0.05684204027056694 Accuracy 0.8500000238418579\n",
      "Iteration 1570 Training loss 0.043140243738889694 Validation loss 0.05762935057282448 Accuracy 0.8495000004768372\n",
      "Iteration 1580 Training loss 0.026170402765274048 Validation loss 0.046268709003925323 Accuracy 0.8755000233650208\n",
      "Iteration 1590 Training loss 0.043577760457992554 Validation loss 0.05824645981192589 Accuracy 0.8450000286102295\n",
      "Iteration 1600 Training loss 0.09647245705127716 Validation loss 0.09880071133375168 Accuracy 0.7229999899864197\n",
      "Iteration 1610 Training loss 0.034103915095329285 Validation loss 0.04761671647429466 Accuracy 0.8694999814033508\n",
      "Iteration 1620 Training loss 0.03312746062874794 Validation loss 0.05324292555451393 Accuracy 0.8550000190734863\n",
      "Iteration 1630 Training loss 0.0310516394674778 Validation loss 0.05050274729728699 Accuracy 0.862500011920929\n",
      "Iteration 1640 Training loss 0.02775999717414379 Validation loss 0.0505954883992672 Accuracy 0.859000027179718\n",
      "Iteration 1650 Training loss 0.03605088219046593 Validation loss 0.057892218232154846 Accuracy 0.8395000100135803\n",
      "Iteration 1660 Training loss 0.03460879251360893 Validation loss 0.05557422712445259 Accuracy 0.843999981880188\n",
      "Iteration 1670 Training loss 0.022534336894750595 Validation loss 0.04903030022978783 Accuracy 0.8640000224113464\n",
      "Iteration 1680 Training loss 0.04627465829253197 Validation loss 0.06610307097434998 Accuracy 0.8184999823570251\n",
      "Iteration 1690 Training loss 0.025074131786823273 Validation loss 0.04828463867306709 Accuracy 0.8654999732971191\n",
      "Iteration 1700 Training loss 0.029532214626669884 Validation loss 0.053198765963315964 Accuracy 0.8565000295639038\n",
      "Iteration 1710 Training loss 0.05468512326478958 Validation loss 0.07229199260473251 Accuracy 0.8069999814033508\n",
      "Iteration 1720 Training loss 0.02182118408381939 Validation loss 0.04641476273536682 Accuracy 0.8740000128746033\n",
      "Iteration 1730 Training loss 0.025427326560020447 Validation loss 0.047537170350551605 Accuracy 0.8654999732971191\n",
      "Iteration 1740 Training loss 0.03766728192567825 Validation loss 0.06109520047903061 Accuracy 0.8360000252723694\n",
      "Iteration 1750 Training loss 0.08786635845899582 Validation loss 0.09776538610458374 Accuracy 0.7419999837875366\n",
      "Iteration 1760 Training loss 0.02894028089940548 Validation loss 0.047772493213415146 Accuracy 0.8690000176429749\n",
      "Iteration 1770 Training loss 0.03374496102333069 Validation loss 0.05186405032873154 Accuracy 0.8610000014305115\n",
      "Iteration 1780 Training loss 0.023975202813744545 Validation loss 0.04694351926445961 Accuracy 0.8740000128746033\n",
      "Iteration 1790 Training loss 0.01867726817727089 Validation loss 0.045421019196510315 Accuracy 0.8755000233650208\n",
      "Iteration 1800 Training loss 0.04581558704376221 Validation loss 0.061942700296640396 Accuracy 0.8349999785423279\n",
      "Iteration 1810 Training loss 0.021010227501392365 Validation loss 0.04562918096780777 Accuracy 0.8740000128746033\n",
      "Iteration 1820 Training loss 0.02238290198147297 Validation loss 0.04732627794146538 Accuracy 0.8725000023841858\n",
      "Iteration 1830 Training loss 0.07220608741044998 Validation loss 0.08548468351364136 Accuracy 0.7730000019073486\n",
      "Iteration 1840 Training loss 0.027507508173584938 Validation loss 0.04687626659870148 Accuracy 0.875\n",
      "Iteration 1850 Training loss 0.020851626992225647 Validation loss 0.046012863516807556 Accuracy 0.8740000128746033\n",
      "Iteration 1860 Training loss 0.015974393114447594 Validation loss 0.04537680745124817 Accuracy 0.8744999766349792\n",
      "Iteration 1870 Training loss 0.061866067349910736 Validation loss 0.0732005387544632 Accuracy 0.8084999918937683\n",
      "Iteration 1880 Training loss 0.152069091796875 Validation loss 0.15620669722557068 Accuracy 0.6134999990463257\n",
      "Iteration 1890 Training loss 0.025837786495685577 Validation loss 0.047618575394153595 Accuracy 0.875\n",
      "Iteration 1900 Training loss 0.02463158220052719 Validation loss 0.04837532714009285 Accuracy 0.8715000152587891\n",
      "Iteration 1910 Training loss 0.03088577650487423 Validation loss 0.051131635904312134 Accuracy 0.8650000095367432\n",
      "Iteration 1920 Training loss 0.019232217222452164 Validation loss 0.04498487710952759 Accuracy 0.8794999718666077\n",
      "Iteration 1930 Training loss 0.025823859497904778 Validation loss 0.05008159950375557 Accuracy 0.8700000047683716\n",
      "Iteration 1940 Training loss 0.01646108366549015 Validation loss 0.04562718793749809 Accuracy 0.8784999847412109\n",
      "Iteration 1950 Training loss 0.050008054822683334 Validation loss 0.06562241166830063 Accuracy 0.8215000033378601\n",
      "Iteration 1960 Training loss 0.050707023590803146 Validation loss 0.06087431311607361 Accuracy 0.8445000052452087\n",
      "Iteration 1970 Training loss 0.024171601980924606 Validation loss 0.045363374054431915 Accuracy 0.878000020980835\n",
      "Iteration 1980 Training loss 0.0373590849339962 Validation loss 0.05626573786139488 Accuracy 0.8464999794960022\n",
      "Iteration 1990 Training loss 0.018033551052212715 Validation loss 0.044085703790187836 Accuracy 0.8740000128746033\n",
      "Iteration 2000 Training loss 0.015601124614477158 Validation loss 0.04553399607539177 Accuracy 0.8759999871253967\n",
      "Iteration 2010 Training loss 0.052849456667900085 Validation loss 0.07596014440059662 Accuracy 0.7954999804496765\n",
      "Iteration 2020 Training loss 0.014695593155920506 Validation loss 0.04519962891936302 Accuracy 0.8765000104904175\n",
      "Iteration 2030 Training loss 0.01687140204012394 Validation loss 0.04903159290552139 Accuracy 0.8665000200271606\n",
      "Iteration 2040 Training loss 0.03601965308189392 Validation loss 0.06736741960048676 Accuracy 0.8119999766349792\n",
      "Iteration 2050 Training loss 0.02762417681515217 Validation loss 0.060092367231845856 Accuracy 0.8320000171661377\n",
      "Iteration 2060 Training loss 0.014303014613687992 Validation loss 0.04479159787297249 Accuracy 0.8744999766349792\n",
      "Iteration 2070 Training loss 0.011496802791953087 Validation loss 0.045874860137701035 Accuracy 0.875\n",
      "Iteration 2080 Training loss 0.02772570587694645 Validation loss 0.06196814402937889 Accuracy 0.8345000147819519\n",
      "Iteration 2090 Training loss 0.014264858327805996 Validation loss 0.0447237566113472 Accuracy 0.8805000185966492\n",
      "Iteration 2100 Training loss 0.01025470346212387 Validation loss 0.044907961040735245 Accuracy 0.8765000104904175\n",
      "Iteration 2110 Training loss 0.014164772816002369 Validation loss 0.04716677591204643 Accuracy 0.875\n",
      "Iteration 2120 Training loss 0.0929398462176323 Validation loss 0.10086581110954285 Accuracy 0.7409999966621399\n",
      "Iteration 2130 Training loss 0.10021224617958069 Validation loss 0.11089654266834259 Accuracy 0.7415000200271606\n",
      "Iteration 2140 Training loss 0.023630812764167786 Validation loss 0.05306670442223549 Accuracy 0.8510000109672546\n",
      "Iteration 2150 Training loss 0.014544310048222542 Validation loss 0.0471077598631382 Accuracy 0.8690000176429749\n",
      "Iteration 2160 Training loss 0.030302703380584717 Validation loss 0.06434310972690582 Accuracy 0.8230000138282776\n",
      "Iteration 2170 Training loss 0.013403884135186672 Validation loss 0.04458865523338318 Accuracy 0.8755000233650208\n",
      "Iteration 2180 Training loss 0.009412053972482681 Validation loss 0.0460241436958313 Accuracy 0.8709999918937683\n",
      "Iteration 2190 Training loss 0.029945669695734978 Validation loss 0.061455026268959045 Accuracy 0.8370000123977661\n",
      "Iteration 2200 Training loss 0.040583536028862 Validation loss 0.06743400543928146 Accuracy 0.8215000033378601\n",
      "Iteration 2210 Training loss 0.010564455762505531 Validation loss 0.04437945410609245 Accuracy 0.875\n",
      "Iteration 2220 Training loss 0.010232208296656609 Validation loss 0.04476851597428322 Accuracy 0.8799999952316284\n",
      "Iteration 2230 Training loss 0.010043167509138584 Validation loss 0.04454865679144859 Accuracy 0.8809999823570251\n",
      "Iteration 2240 Training loss 0.00871739536523819 Validation loss 0.04538727551698685 Accuracy 0.878000020980835\n",
      "Iteration 2250 Training loss 0.010265934281051159 Validation loss 0.04588138684630394 Accuracy 0.8784999847412109\n",
      "Iteration 2260 Training loss 0.009051290340721607 Validation loss 0.04494883865118027 Accuracy 0.8820000290870667\n",
      "Iteration 2270 Training loss 0.006372551899403334 Validation loss 0.04588982090353966 Accuracy 0.8744999766349792\n",
      "Iteration 2280 Training loss 0.06998341530561447 Validation loss 0.0895247831940651 Accuracy 0.781499981880188\n",
      "Iteration 2290 Training loss 0.050466641783714294 Validation loss 0.07492084056138992 Accuracy 0.8075000047683716\n",
      "Iteration 2300 Training loss 0.013527371920645237 Validation loss 0.0467987135052681 Accuracy 0.875\n",
      "Iteration 2310 Training loss 0.009614300914108753 Validation loss 0.04476355016231537 Accuracy 0.875\n",
      "Iteration 2320 Training loss 0.008134634234011173 Validation loss 0.04756993055343628 Accuracy 0.8725000023841858\n",
      "Iteration 2330 Training loss 0.05193382129073143 Validation loss 0.07695145905017853 Accuracy 0.7944999933242798\n",
      "Iteration 2340 Training loss 0.05029680207371712 Validation loss 0.07298152148723602 Accuracy 0.8190000057220459\n",
      "Iteration 2350 Training loss 0.014004056341946125 Validation loss 0.047089118510484695 Accuracy 0.8715000152587891\n",
      "Iteration 2360 Training loss 0.009503009729087353 Validation loss 0.045260872691869736 Accuracy 0.875\n",
      "Iteration 2370 Training loss 0.00893422681838274 Validation loss 0.04544800519943237 Accuracy 0.8784999847412109\n",
      "Iteration 2380 Training loss 0.007204368710517883 Validation loss 0.04588592052459717 Accuracy 0.8740000128746033\n",
      "Iteration 2390 Training loss 0.00781028438359499 Validation loss 0.04730372503399849 Accuracy 0.8740000128746033\n",
      "Iteration 2400 Training loss 0.011362721212208271 Validation loss 0.052565399557352066 Accuracy 0.8619999885559082\n",
      "Iteration 2410 Training loss 0.006228832993656397 Validation loss 0.04468951374292374 Accuracy 0.8774999976158142\n",
      "Iteration 2420 Training loss 0.006353715900331736 Validation loss 0.045079052448272705 Accuracy 0.8769999742507935\n",
      "Iteration 2430 Training loss 0.006839660461992025 Validation loss 0.0455239862203598 Accuracy 0.8765000104904175\n",
      "Iteration 2440 Training loss 0.005440820008516312 Validation loss 0.045414745807647705 Accuracy 0.878000020980835\n",
      "Iteration 2450 Training loss 0.00541142001748085 Validation loss 0.04528900980949402 Accuracy 0.8774999976158142\n",
      "Iteration 2460 Training loss 0.006300080567598343 Validation loss 0.0458194836974144 Accuracy 0.8774999976158142\n",
      "Iteration 2470 Training loss 0.0037787535693496466 Validation loss 0.045388273894786835 Accuracy 0.878000020980835\n",
      "Iteration 2480 Training loss 0.005306623876094818 Validation loss 0.04532260075211525 Accuracy 0.8799999952316284\n",
      "Iteration 2490 Training loss 0.004508090205490589 Validation loss 0.045873936265707016 Accuracy 0.878000020980835\n",
      "Iteration 2500 Training loss 0.004770258907228708 Validation loss 0.04559973254799843 Accuracy 0.8805000185966492\n",
      "Iteration 2510 Training loss 0.0045312559232115746 Validation loss 0.04633660987019539 Accuracy 0.878000020980835\n",
      "Iteration 2520 Training loss 0.0042408122681081295 Validation loss 0.045592062175273895 Accuracy 0.8799999952316284\n",
      "Iteration 2530 Training loss 0.004241275601089001 Validation loss 0.04636183753609657 Accuracy 0.8774999976158142\n",
      "Iteration 2540 Training loss 0.0029259889852255583 Validation loss 0.04598565772175789 Accuracy 0.8790000081062317\n",
      "Iteration 2550 Training loss 0.004499724600464106 Validation loss 0.04607551917433739 Accuracy 0.8784999847412109\n",
      "Iteration 2560 Training loss 0.0042998348362743855 Validation loss 0.04681278392672539 Accuracy 0.8769999742507935\n",
      "Iteration 2570 Training loss 0.003783922642469406 Validation loss 0.04646553844213486 Accuracy 0.8784999847412109\n",
      "Iteration 2580 Training loss 0.002528538927435875 Validation loss 0.046453025192022324 Accuracy 0.8790000081062317\n",
      "Iteration 2590 Training loss 0.004773812834173441 Validation loss 0.046572212129831314 Accuracy 0.8784999847412109\n",
      "Iteration 2600 Training loss 0.0036931501235812902 Validation loss 0.04663714021444321 Accuracy 0.8759999871253967\n",
      "Iteration 2610 Training loss 0.003332507563754916 Validation loss 0.04664549231529236 Accuracy 0.8774999976158142\n",
      "Iteration 2620 Training loss 0.003455059602856636 Validation loss 0.04660816118121147 Accuracy 0.8769999742507935\n",
      "Iteration 2630 Training loss 0.0036358176730573177 Validation loss 0.04653244465589523 Accuracy 0.8769999742507935\n",
      "Iteration 2640 Training loss 0.0049436818808317184 Validation loss 0.046456292271614075 Accuracy 0.8784999847412109\n",
      "Iteration 2650 Training loss 0.0039006241131573915 Validation loss 0.046813271939754486 Accuracy 0.878000020980835\n",
      "Iteration 2660 Training loss 0.0027038909029215574 Validation loss 0.04637863487005234 Accuracy 0.8774999976158142\n",
      "Iteration 2670 Training loss 0.003199848346412182 Validation loss 0.0466184988617897 Accuracy 0.8774999976158142\n",
      "Iteration 2680 Training loss 0.0027167091611772776 Validation loss 0.04632001370191574 Accuracy 0.8794999718666077\n",
      "Iteration 2690 Training loss 0.0023154960945248604 Validation loss 0.046382296830415726 Accuracy 0.878000020980835\n",
      "Iteration 2700 Training loss 0.0026210190262645483 Validation loss 0.04646502062678337 Accuracy 0.8790000081062317\n",
      "Iteration 2710 Training loss 0.0027468360494822264 Validation loss 0.04629600793123245 Accuracy 0.878000020980835\n",
      "Iteration 2720 Training loss 0.004997864365577698 Validation loss 0.046529870480298996 Accuracy 0.8809999823570251\n",
      "Iteration 2730 Training loss 0.002465875819325447 Validation loss 0.04633944109082222 Accuracy 0.878000020980835\n",
      "Iteration 2740 Training loss 0.002935439581051469 Validation loss 0.04656579717993736 Accuracy 0.8794999718666077\n",
      "Iteration 2750 Training loss 0.0021561826579272747 Validation loss 0.04649307206273079 Accuracy 0.8794999718666077\n",
      "Iteration 2760 Training loss 0.0032564587891101837 Validation loss 0.04672175645828247 Accuracy 0.8794999718666077\n",
      "Iteration 2770 Training loss 0.002266072202473879 Validation loss 0.04659092426300049 Accuracy 0.8805000185966492\n",
      "Iteration 2780 Training loss 0.0026895736809819937 Validation loss 0.04671420156955719 Accuracy 0.8805000185966492\n",
      "Iteration 2790 Training loss 0.0034359621349722147 Validation loss 0.04643017426133156 Accuracy 0.8794999718666077\n",
      "Iteration 2800 Training loss 0.001822166726924479 Validation loss 0.046574097126722336 Accuracy 0.8824999928474426\n",
      "Iteration 2810 Training loss 0.0029803309589624405 Validation loss 0.046649269759655 Accuracy 0.8809999823570251\n",
      "Iteration 2820 Training loss 0.003421849338337779 Validation loss 0.04660719260573387 Accuracy 0.8794999718666077\n",
      "Iteration 2830 Training loss 0.003062416333705187 Validation loss 0.04682016745209694 Accuracy 0.8784999847412109\n",
      "Iteration 2840 Training loss 0.00335066975094378 Validation loss 0.046680450439453125 Accuracy 0.8809999823570251\n",
      "Iteration 2850 Training loss 0.0028526014648377895 Validation loss 0.04683970659971237 Accuracy 0.8809999823570251\n",
      "Iteration 2860 Training loss 0.0038594298530369997 Validation loss 0.04689253494143486 Accuracy 0.8815000057220459\n",
      "Iteration 2870 Training loss 0.0025962907820940018 Validation loss 0.046673841774463654 Accuracy 0.8815000057220459\n",
      "Iteration 2880 Training loss 0.0020845727995038033 Validation loss 0.04689550772309303 Accuracy 0.8799999952316284\n",
      "Iteration 2890 Training loss 0.0038177173119038343 Validation loss 0.047019049525260925 Accuracy 0.8794999718666077\n",
      "Iteration 2900 Training loss 0.0020893430337309837 Validation loss 0.04685122147202492 Accuracy 0.8809999823570251\n",
      "Iteration 2910 Training loss 0.002594390418380499 Validation loss 0.046811286360025406 Accuracy 0.8799999952316284\n",
      "Iteration 2920 Training loss 0.004287536256015301 Validation loss 0.046962592750787735 Accuracy 0.8799999952316284\n",
      "Iteration 2930 Training loss 0.0023683723993599415 Validation loss 0.046936821192502975 Accuracy 0.8799999952316284\n",
      "Iteration 2940 Training loss 0.0022873845882713795 Validation loss 0.046921826899051666 Accuracy 0.8824999928474426\n",
      "Iteration 2950 Training loss 0.002288747811689973 Validation loss 0.04705284908413887 Accuracy 0.8809999823570251\n",
      "Iteration 2960 Training loss 0.00226246053352952 Validation loss 0.04709890857338905 Accuracy 0.8794999718666077\n",
      "Iteration 2970 Training loss 0.0023071381729096174 Validation loss 0.047253333032131195 Accuracy 0.8805000185966492\n",
      "Iteration 2980 Training loss 0.0025529232807457447 Validation loss 0.047356292605400085 Accuracy 0.8805000185966492\n",
      "Iteration 2990 Training loss 0.0020321933552622795 Validation loss 0.047165364027023315 Accuracy 0.8809999823570251\n",
      "Iteration 3000 Training loss 0.0026319457683712244 Validation loss 0.04708915948867798 Accuracy 0.8799999952316284\n",
      "Iteration 3010 Training loss 0.001560429809615016 Validation loss 0.04726661741733551 Accuracy 0.8799999952316284\n",
      "Iteration 3020 Training loss 0.00254806037992239 Validation loss 0.04724422097206116 Accuracy 0.8790000081062317\n",
      "Iteration 3030 Training loss 0.0028039030730724335 Validation loss 0.04716423526406288 Accuracy 0.8799999952316284\n",
      "Iteration 3040 Training loss 0.0017232665559276938 Validation loss 0.04740874096751213 Accuracy 0.8805000185966492\n",
      "Iteration 3050 Training loss 0.0017546620219945908 Validation loss 0.047263823449611664 Accuracy 0.8790000081062317\n",
      "Iteration 3060 Training loss 0.0022552066948264837 Validation loss 0.04746740311384201 Accuracy 0.8784999847412109\n",
      "Iteration 3070 Training loss 0.00245410343632102 Validation loss 0.04760625958442688 Accuracy 0.8809999823570251\n",
      "Iteration 3080 Training loss 0.002003980567678809 Validation loss 0.047511450946331024 Accuracy 0.8784999847412109\n",
      "Iteration 3090 Training loss 0.003220802638679743 Validation loss 0.047667935490608215 Accuracy 0.8794999718666077\n",
      "Iteration 3100 Training loss 0.0021996761206537485 Validation loss 0.04751169681549072 Accuracy 0.8805000185966492\n",
      "Iteration 3110 Training loss 0.002192189684137702 Validation loss 0.047562506049871445 Accuracy 0.8799999952316284\n",
      "Iteration 3120 Training loss 0.0029472997412085533 Validation loss 0.0475086085498333 Accuracy 0.8790000081062317\n",
      "Iteration 3130 Training loss 0.002200207207351923 Validation loss 0.04760599881410599 Accuracy 0.8790000081062317\n",
      "Iteration 3140 Training loss 0.0034554859157651663 Validation loss 0.04767899215221405 Accuracy 0.8784999847412109\n",
      "Iteration 3150 Training loss 0.001943865790963173 Validation loss 0.0475851334631443 Accuracy 0.8784999847412109\n",
      "Iteration 3160 Training loss 0.0019188184523954988 Validation loss 0.0476834811270237 Accuracy 0.8805000185966492\n",
      "Iteration 3170 Training loss 0.0019435412250459194 Validation loss 0.04765027388930321 Accuracy 0.8784999847412109\n",
      "Iteration 3180 Training loss 0.0019169650040566921 Validation loss 0.04772104322910309 Accuracy 0.878000020980835\n",
      "Iteration 3190 Training loss 0.003156425664201379 Validation loss 0.04767945408821106 Accuracy 0.8790000081062317\n",
      "Iteration 3200 Training loss 0.002913989359512925 Validation loss 0.04772596433758736 Accuracy 0.8794999718666077\n",
      "Iteration 3210 Training loss 0.0026625494938343763 Validation loss 0.04764380678534508 Accuracy 0.8794999718666077\n",
      "Iteration 3220 Training loss 0.0021715222392231226 Validation loss 0.0476815290749073 Accuracy 0.8809999823570251\n",
      "Iteration 3230 Training loss 0.0023907236754894257 Validation loss 0.047647103667259216 Accuracy 0.8799999952316284\n",
      "Iteration 3240 Training loss 0.0021716831251978874 Validation loss 0.04767769202589989 Accuracy 0.8794999718666077\n",
      "Iteration 3250 Training loss 0.0019061213824898005 Validation loss 0.047868940979242325 Accuracy 0.8794999718666077\n",
      "Iteration 3260 Training loss 0.0029297976288944483 Validation loss 0.04771727696061134 Accuracy 0.8809999823570251\n",
      "Iteration 3270 Training loss 0.002389664063230157 Validation loss 0.04767344892024994 Accuracy 0.8805000185966492\n",
      "Iteration 3280 Training loss 0.0023954182397574186 Validation loss 0.047758761793375015 Accuracy 0.8799999952316284\n",
      "Iteration 3290 Training loss 0.0018957557622343302 Validation loss 0.0477730929851532 Accuracy 0.8805000185966492\n",
      "Iteration 3300 Training loss 0.0026134043000638485 Validation loss 0.04785818234086037 Accuracy 0.8799999952316284\n",
      "Iteration 3310 Training loss 0.002144549274817109 Validation loss 0.04768906161189079 Accuracy 0.8799999952316284\n",
      "Iteration 3320 Training loss 0.0026487777940928936 Validation loss 0.04782252386212349 Accuracy 0.8805000185966492\n",
      "Iteration 3330 Training loss 0.0028664700221270323 Validation loss 0.04785364493727684 Accuracy 0.8799999952316284\n",
      "Iteration 3340 Training loss 0.0026256211567670107 Validation loss 0.047913871705532074 Accuracy 0.8794999718666077\n",
      "Iteration 3350 Training loss 0.003143309149891138 Validation loss 0.04779815673828125 Accuracy 0.8799999952316284\n",
      "Iteration 3360 Training loss 0.002129965228959918 Validation loss 0.04789881035685539 Accuracy 0.8799999952316284\n",
      "Iteration 3370 Training loss 0.0023680406156927347 Validation loss 0.04778943955898285 Accuracy 0.8790000081062317\n",
      "Iteration 3380 Training loss 0.0016355052357539535 Validation loss 0.04783356562256813 Accuracy 0.8805000185966492\n",
      "Iteration 3390 Training loss 0.003107520518824458 Validation loss 0.047842491418123245 Accuracy 0.8794999718666077\n",
      "Iteration 3400 Training loss 0.0023468765430152416 Validation loss 0.04787736386060715 Accuracy 0.8799999952316284\n",
      "Iteration 3410 Training loss 0.001628513098694384 Validation loss 0.04800736904144287 Accuracy 0.8799999952316284\n",
      "Iteration 3420 Training loss 0.0021126484498381615 Validation loss 0.047943390905857086 Accuracy 0.8805000185966492\n",
      "Iteration 3430 Training loss 0.0033687285613268614 Validation loss 0.04794415459036827 Accuracy 0.8805000185966492\n",
      "Iteration 3440 Training loss 0.0016226597363129258 Validation loss 0.04784993827342987 Accuracy 0.8799999952316284\n",
      "Iteration 3450 Training loss 0.0023513741325587034 Validation loss 0.0480097196996212 Accuracy 0.8784999847412109\n",
      "Iteration 3460 Training loss 0.0031012233812361956 Validation loss 0.04791482537984848 Accuracy 0.8794999718666077\n",
      "Iteration 3470 Training loss 0.003113811369985342 Validation loss 0.04796142503619194 Accuracy 0.8799999952316284\n",
      "Iteration 3480 Training loss 0.0026030445005744696 Validation loss 0.04807228967547417 Accuracy 0.8794999718666077\n",
      "Iteration 3490 Training loss 0.0033633767161518335 Validation loss 0.0479435957968235 Accuracy 0.8790000081062317\n",
      "Iteration 3500 Training loss 0.001857681549154222 Validation loss 0.048073019832372665 Accuracy 0.8790000081062317\n",
      "Iteration 3510 Training loss 0.0023453894536942244 Validation loss 0.04795384034514427 Accuracy 0.8794999718666077\n",
      "Iteration 3520 Training loss 0.0031016762368381023 Validation loss 0.04796300455927849 Accuracy 0.8790000081062317\n",
      "Iteration 3530 Training loss 0.0028459567110985518 Validation loss 0.04789796099066734 Accuracy 0.8784999847412109\n",
      "Iteration 3540 Training loss 0.002836773870512843 Validation loss 0.04797270894050598 Accuracy 0.8784999847412109\n",
      "Iteration 3550 Training loss 0.0023482833057641983 Validation loss 0.04798939451575279 Accuracy 0.8805000185966492\n",
      "Iteration 3560 Training loss 0.0020897688809782267 Validation loss 0.047999441623687744 Accuracy 0.8794999718666077\n",
      "Iteration 3570 Training loss 0.0020892294123768806 Validation loss 0.04796185344457626 Accuracy 0.8794999718666077\n",
      "Iteration 3580 Training loss 0.0023336068261414766 Validation loss 0.048027414828538895 Accuracy 0.8805000185966492\n",
      "Iteration 3590 Training loss 0.003838838078081608 Validation loss 0.048075802624225616 Accuracy 0.8799999952316284\n",
      "Iteration 3600 Training loss 0.002354976022616029 Validation loss 0.04808788001537323 Accuracy 0.8799999952316284\n",
      "Iteration 3610 Training loss 0.0028398067224770784 Validation loss 0.0480278842151165 Accuracy 0.8790000081062317\n",
      "Iteration 3620 Training loss 0.0020916240755468607 Validation loss 0.04808006435632706 Accuracy 0.8794999718666077\n",
      "Iteration 3630 Training loss 0.002834549406543374 Validation loss 0.04816005378961563 Accuracy 0.8794999718666077\n",
      "Iteration 3640 Training loss 0.0023401707876473665 Validation loss 0.048132915049791336 Accuracy 0.8805000185966492\n",
      "Iteration 3650 Training loss 0.0025828483048826456 Validation loss 0.04814668744802475 Accuracy 0.8809999823570251\n",
      "Iteration 3660 Training loss 0.001834677648730576 Validation loss 0.048093244433403015 Accuracy 0.8799999952316284\n",
      "Iteration 3670 Training loss 0.0028216459322720766 Validation loss 0.04816330596804619 Accuracy 0.8805000185966492\n",
      "Iteration 3680 Training loss 0.002338648308068514 Validation loss 0.04819963127374649 Accuracy 0.8799999952316284\n",
      "Iteration 3690 Training loss 0.0020824968814849854 Validation loss 0.048083964735269547 Accuracy 0.8794999718666077\n",
      "Iteration 3700 Training loss 0.00233100401237607 Validation loss 0.04811272397637367 Accuracy 0.8794999718666077\n",
      "Iteration 3710 Training loss 0.0028350206557661295 Validation loss 0.04806011542677879 Accuracy 0.8799999952316284\n",
      "Iteration 3720 Training loss 0.0013289517955854535 Validation loss 0.048184189945459366 Accuracy 0.8805000185966492\n",
      "Iteration 3730 Training loss 0.0035643195733428 Validation loss 0.04815055802464485 Accuracy 0.8799999952316284\n",
      "Iteration 3740 Training loss 0.0023233171086758375 Validation loss 0.04822776839137077 Accuracy 0.8799999952316284\n",
      "Iteration 3750 Training loss 0.0025759220588952303 Validation loss 0.04818518087267876 Accuracy 0.8799999952316284\n",
      "Iteration 3760 Training loss 0.0018379964167252183 Validation loss 0.04818306118249893 Accuracy 0.8790000081062317\n",
      "Iteration 3770 Training loss 0.002323316875845194 Validation loss 0.04826100915670395 Accuracy 0.8809999823570251\n",
      "Iteration 3780 Training loss 0.0033271037973463535 Validation loss 0.04820837080478668 Accuracy 0.8809999823570251\n",
      "Iteration 3790 Training loss 0.002818487584590912 Validation loss 0.04827520623803139 Accuracy 0.8799999952316284\n",
      "Iteration 3800 Training loss 0.0015743471449241042 Validation loss 0.0482579730451107 Accuracy 0.8809999823570251\n",
      "Iteration 3810 Training loss 0.0023119249381124973 Validation loss 0.048360809683799744 Accuracy 0.8794999718666077\n",
      "Iteration 3820 Training loss 0.0018251726869493723 Validation loss 0.04841355234384537 Accuracy 0.8790000081062317\n",
      "Iteration 3830 Training loss 0.0020773764699697495 Validation loss 0.04831597954034805 Accuracy 0.8805000185966492\n",
      "Iteration 3840 Training loss 0.002577798906713724 Validation loss 0.04842037335038185 Accuracy 0.8799999952316284\n",
      "Iteration 3850 Training loss 0.00132145662792027 Validation loss 0.04831690713763237 Accuracy 0.8805000185966492\n",
      "Iteration 3860 Training loss 0.002104598330333829 Validation loss 0.04856160655617714 Accuracy 0.8794999718666077\n",
      "Iteration 3870 Training loss 0.0018266795668751001 Validation loss 0.048483122140169144 Accuracy 0.8809999823570251\n",
      "Iteration 3880 Training loss 0.0025908660609275103 Validation loss 0.048487693071365356 Accuracy 0.8799999952316284\n",
      "Iteration 3890 Training loss 0.0015792426420375705 Validation loss 0.04841705039143562 Accuracy 0.8805000185966492\n",
      "Iteration 3900 Training loss 0.0033176043070852757 Validation loss 0.048493169248104095 Accuracy 0.8799999952316284\n",
      "Iteration 3910 Training loss 0.003321794793009758 Validation loss 0.04849182069301605 Accuracy 0.8790000081062317\n",
      "Iteration 3920 Training loss 0.0025775812100619078 Validation loss 0.04846582189202309 Accuracy 0.8799999952316284\n",
      "Iteration 3930 Training loss 0.0013249058974906802 Validation loss 0.04854186996817589 Accuracy 0.8799999952316284\n",
      "Iteration 3940 Training loss 0.0023262198083102703 Validation loss 0.0484929122030735 Accuracy 0.8809999823570251\n",
      "Iteration 3950 Training loss 0.0013214482460170984 Validation loss 0.04850313067436218 Accuracy 0.8799999952316284\n",
      "Iteration 3960 Training loss 0.0025639717932790518 Validation loss 0.04856567829847336 Accuracy 0.8799999952316284\n",
      "Iteration 3970 Training loss 0.0008267855737358332 Validation loss 0.04856812581419945 Accuracy 0.8799999952316284\n",
      "Iteration 3980 Training loss 0.00156386848539114 Validation loss 0.04856693372130394 Accuracy 0.8799999952316284\n",
      "Iteration 3990 Training loss 0.002323388122022152 Validation loss 0.04851812496781349 Accuracy 0.8799999952316284\n",
      "Iteration 4000 Training loss 0.0030624158680438995 Validation loss 0.04849351942539215 Accuracy 0.8794999718666077\n",
      "Iteration 4010 Training loss 0.0020680043380707502 Validation loss 0.04848637804389 Accuracy 0.8794999718666077\n",
      "Iteration 4020 Training loss 0.0018117261352017522 Validation loss 0.04852406680583954 Accuracy 0.8799999952316284\n",
      "Iteration 4030 Training loss 0.002062252489849925 Validation loss 0.04855372756719589 Accuracy 0.8794999718666077\n",
      "Iteration 4040 Training loss 0.0013048868859186769 Validation loss 0.048647139221429825 Accuracy 0.8794999718666077\n",
      "Iteration 4050 Training loss 0.0023109011817723513 Validation loss 0.04856066033244133 Accuracy 0.8799999952316284\n",
      "Iteration 4060 Training loss 0.0028118386399000883 Validation loss 0.048521045595407486 Accuracy 0.8794999718666077\n",
      "Iteration 4070 Training loss 0.003059621900320053 Validation loss 0.04853227734565735 Accuracy 0.8799999952316284\n",
      "Iteration 4080 Training loss 0.0028029782697558403 Validation loss 0.048561446368694305 Accuracy 0.8790000081062317\n",
      "Iteration 4090 Training loss 0.0015583218773826957 Validation loss 0.048580821603536606 Accuracy 0.8799999952316284\n",
      "Iteration 4100 Training loss 0.0020574647933244705 Validation loss 0.04849429056048393 Accuracy 0.8799999952316284\n",
      "Iteration 4110 Training loss 0.001305692014284432 Validation loss 0.04855380207300186 Accuracy 0.8790000081062317\n",
      "Iteration 4120 Training loss 0.0018089566146954894 Validation loss 0.048600755631923676 Accuracy 0.8794999718666077\n",
      "Iteration 4130 Training loss 0.002059325110167265 Validation loss 0.048608530312776566 Accuracy 0.8794999718666077\n",
      "Iteration 4140 Training loss 0.002048429101705551 Validation loss 0.04861998185515404 Accuracy 0.8794999718666077\n",
      "Iteration 4150 Training loss 0.0025600804947316647 Validation loss 0.04859013855457306 Accuracy 0.8794999718666077\n",
      "Iteration 4160 Training loss 0.0017982529243454337 Validation loss 0.048549674451351166 Accuracy 0.8799999952316284\n",
      "Iteration 4170 Training loss 0.0018046514596790075 Validation loss 0.04855590686202049 Accuracy 0.8794999718666077\n",
      "Iteration 4180 Training loss 0.0020499592646956444 Validation loss 0.04859347268939018 Accuracy 0.8799999952316284\n",
      "Iteration 4190 Training loss 0.0035546026192605495 Validation loss 0.04861418157815933 Accuracy 0.8799999952316284\n",
      "Iteration 4200 Training loss 0.0028054118156433105 Validation loss 0.04857601970434189 Accuracy 0.8784999847412109\n",
      "Iteration 4210 Training loss 0.002052772557362914 Validation loss 0.048629030585289 Accuracy 0.8809999823570251\n",
      "Iteration 4220 Training loss 0.0025583633687347174 Validation loss 0.04866775870323181 Accuracy 0.8809999823570251\n",
      "Iteration 4230 Training loss 0.001550450106151402 Validation loss 0.04858133941888809 Accuracy 0.8799999952316284\n",
      "Iteration 4240 Training loss 0.002054841024801135 Validation loss 0.04859870299696922 Accuracy 0.8790000081062317\n",
      "Iteration 4250 Training loss 0.002058479469269514 Validation loss 0.04868483170866966 Accuracy 0.8784999847412109\n",
      "Iteration 4260 Training loss 0.002548843389376998 Validation loss 0.04864373803138733 Accuracy 0.8799999952316284\n",
      "Iteration 4270 Training loss 0.002551092067733407 Validation loss 0.04863385856151581 Accuracy 0.8794999718666077\n",
      "Iteration 4280 Training loss 0.0015527429059147835 Validation loss 0.048661332577466965 Accuracy 0.8794999718666077\n",
      "Iteration 4290 Training loss 0.0025464724749326706 Validation loss 0.048693761229515076 Accuracy 0.8784999847412109\n",
      "Iteration 4300 Training loss 0.0025539651978760958 Validation loss 0.04869011789560318 Accuracy 0.8805000185966492\n",
      "Iteration 4310 Training loss 0.0020514968782663345 Validation loss 0.04874560981988907 Accuracy 0.8799999952316284\n",
      "Iteration 4320 Training loss 0.002302576554939151 Validation loss 0.04875240847468376 Accuracy 0.8799999952316284\n",
      "Iteration 4330 Training loss 0.0025516911409795284 Validation loss 0.04867658019065857 Accuracy 0.8784999847412109\n",
      "Iteration 4340 Training loss 0.0027969845104962587 Validation loss 0.04870336875319481 Accuracy 0.8805000185966492\n",
      "Iteration 4350 Training loss 0.0018044179305434227 Validation loss 0.048698265105485916 Accuracy 0.8794999718666077\n",
      "Iteration 4360 Training loss 0.0020470889285206795 Validation loss 0.04869406670331955 Accuracy 0.8790000081062317\n",
      "Iteration 4370 Training loss 0.001298743300139904 Validation loss 0.048730429261922836 Accuracy 0.8794999718666077\n",
      "Iteration 4380 Training loss 0.0022978982888162136 Validation loss 0.04870307818055153 Accuracy 0.8799999952316284\n",
      "Iteration 4390 Training loss 0.0015483156312257051 Validation loss 0.04874451458454132 Accuracy 0.8799999952316284\n",
      "Iteration 4400 Training loss 0.0030470823403447866 Validation loss 0.048703666776418686 Accuracy 0.8790000081062317\n",
      "Iteration 4410 Training loss 0.0035416579339653254 Validation loss 0.04877663403749466 Accuracy 0.8790000081062317\n",
      "Iteration 4420 Training loss 0.0020446290727704763 Validation loss 0.04881337657570839 Accuracy 0.8784999847412109\n",
      "Iteration 4430 Training loss 0.002545255236327648 Validation loss 0.04880037158727646 Accuracy 0.8790000081062317\n",
      "Iteration 4440 Training loss 0.0027922755107283592 Validation loss 0.04883020371198654 Accuracy 0.8799999952316284\n",
      "Iteration 4450 Training loss 0.0025360286235809326 Validation loss 0.04876810684800148 Accuracy 0.8790000081062317\n",
      "Iteration 4460 Training loss 0.0005431764293462038 Validation loss 0.048862189054489136 Accuracy 0.8784999847412109\n",
      "Iteration 4470 Training loss 0.0015439966227859259 Validation loss 0.04924819618463516 Accuracy 0.8784999847412109\n",
      "Iteration 4480 Training loss 0.001551802852191031 Validation loss 0.049254629760980606 Accuracy 0.8790000081062317\n",
      "Iteration 4490 Training loss 0.002042270265519619 Validation loss 0.04915544390678406 Accuracy 0.8790000081062317\n",
      "Iteration 4500 Training loss 0.0015651100547984242 Validation loss 0.049076370894908905 Accuracy 0.8784999847412109\n",
      "Iteration 4510 Training loss 0.0015427968464791775 Validation loss 0.04910079389810562 Accuracy 0.8790000081062317\n",
      "Iteration 4520 Training loss 0.0020474186167120934 Validation loss 0.04909197986125946 Accuracy 0.8784999847412109\n",
      "Iteration 4530 Training loss 0.0017927224980667233 Validation loss 0.04904664680361748 Accuracy 0.8784999847412109\n",
      "Iteration 4540 Training loss 0.0020463394466787577 Validation loss 0.04909386858344078 Accuracy 0.8784999847412109\n",
      "Iteration 4550 Training loss 0.0017946797888725996 Validation loss 0.04899461567401886 Accuracy 0.8790000081062317\n",
      "Iteration 4560 Training loss 0.003296521957963705 Validation loss 0.049049124121665955 Accuracy 0.8784999847412109\n",
      "Iteration 4570 Training loss 0.0020474609918892384 Validation loss 0.0489649623632431 Accuracy 0.8790000081062317\n",
      "Iteration 4580 Training loss 0.0017900284146890044 Validation loss 0.049039795994758606 Accuracy 0.8790000081062317\n",
      "Iteration 4590 Training loss 0.0025426275096833706 Validation loss 0.04901743680238724 Accuracy 0.8790000081062317\n",
      "Iteration 4600 Training loss 0.001299000927247107 Validation loss 0.049052584916353226 Accuracy 0.8790000081062317\n",
      "Iteration 4610 Training loss 0.0015449755592271686 Validation loss 0.049072667956352234 Accuracy 0.8784999847412109\n",
      "Iteration 4620 Training loss 0.0005421303794719279 Validation loss 0.04903394728899002 Accuracy 0.8784999847412109\n",
      "Iteration 4630 Training loss 0.002794454572722316 Validation loss 0.0490044429898262 Accuracy 0.8790000081062317\n",
      "Iteration 4640 Training loss 0.0017891444731503725 Validation loss 0.04910263419151306 Accuracy 0.8790000081062317\n",
      "Iteration 4650 Training loss 0.0020388872362673283 Validation loss 0.04909495264291763 Accuracy 0.8784999847412109\n",
      "Iteration 4660 Training loss 0.002046314300969243 Validation loss 0.04906705021858215 Accuracy 0.878000020980835\n",
      "Iteration 4670 Training loss 0.0015384755097329617 Validation loss 0.04902222752571106 Accuracy 0.8784999847412109\n",
      "Iteration 4680 Training loss 0.0020394071470946074 Validation loss 0.04913980886340141 Accuracy 0.8790000081062317\n",
      "Iteration 4690 Training loss 0.002286437898874283 Validation loss 0.049030400812625885 Accuracy 0.878000020980835\n",
      "Iteration 4700 Training loss 0.002538743894547224 Validation loss 0.049069374799728394 Accuracy 0.8774999976158142\n",
      "Iteration 4710 Training loss 0.0017863587709143758 Validation loss 0.04910579323768616 Accuracy 0.8784999847412109\n",
      "Iteration 4720 Training loss 0.0020398381166160107 Validation loss 0.04905968904495239 Accuracy 0.8790000081062317\n",
      "Iteration 4730 Training loss 0.0025395345874130726 Validation loss 0.04909903183579445 Accuracy 0.8794999718666077\n",
      "Iteration 4740 Training loss 0.001786867855116725 Validation loss 0.04914183169603348 Accuracy 0.8794999718666077\n",
      "Iteration 4750 Training loss 0.001540298224426806 Validation loss 0.0490838848054409 Accuracy 0.8794999718666077\n",
      "Iteration 4760 Training loss 0.0027829273603856564 Validation loss 0.0490838848054409 Accuracy 0.8794999718666077\n",
      "Iteration 4770 Training loss 0.0007901985081844032 Validation loss 0.04909113422036171 Accuracy 0.8790000081062317\n",
      "Iteration 4780 Training loss 0.0020426223054528236 Validation loss 0.04910079389810562 Accuracy 0.878000020980835\n",
      "Iteration 4790 Training loss 0.002534580649808049 Validation loss 0.04910792410373688 Accuracy 0.8794999718666077\n",
      "Iteration 4800 Training loss 0.001536552095785737 Validation loss 0.04916004464030266 Accuracy 0.8794999718666077\n",
      "Iteration 4810 Training loss 0.0022808837238699198 Validation loss 0.04910297691822052 Accuracy 0.8794999718666077\n",
      "Iteration 4820 Training loss 0.002038671402260661 Validation loss 0.04914368689060211 Accuracy 0.8790000081062317\n",
      "Iteration 4830 Training loss 0.0017845883267000318 Validation loss 0.049135688692331314 Accuracy 0.8790000081062317\n",
      "Iteration 4840 Training loss 0.002537420252338052 Validation loss 0.04914381355047226 Accuracy 0.8794999718666077\n",
      "Iteration 4850 Training loss 0.0027853241190314293 Validation loss 0.04914151504635811 Accuracy 0.8794999718666077\n",
      "Iteration 4860 Training loss 0.003035099944099784 Validation loss 0.04914916679263115 Accuracy 0.8790000081062317\n",
      "Iteration 4870 Training loss 0.002288816962391138 Validation loss 0.049120862036943436 Accuracy 0.8794999718666077\n",
      "Iteration 4880 Training loss 0.0010358452564105392 Validation loss 0.04909418150782585 Accuracy 0.878000020980835\n",
      "Iteration 4890 Training loss 0.0025306728202849627 Validation loss 0.04912760108709335 Accuracy 0.878000020980835\n",
      "Iteration 4900 Training loss 0.00203887652605772 Validation loss 0.04910970479249954 Accuracy 0.8784999847412109\n",
      "Iteration 4910 Training loss 0.0007872315472923219 Validation loss 0.04911614954471588 Accuracy 0.878000020980835\n",
      "Iteration 4920 Training loss 0.0017808811971917748 Validation loss 0.049145523458719254 Accuracy 0.8790000081062317\n",
      "Iteration 4930 Training loss 0.0012885004980489612 Validation loss 0.049141958355903625 Accuracy 0.878000020980835\n",
      "Iteration 4940 Training loss 0.0027842612471431494 Validation loss 0.049159012734889984 Accuracy 0.8790000081062317\n",
      "Iteration 4950 Training loss 0.003032183274626732 Validation loss 0.04918222874403 Accuracy 0.8794999718666077\n",
      "Iteration 4960 Training loss 0.0017845117254182696 Validation loss 0.04912763461470604 Accuracy 0.8794999718666077\n",
      "Iteration 4970 Training loss 0.0010359156876802444 Validation loss 0.049133867025375366 Accuracy 0.8794999718666077\n",
      "Iteration 4980 Training loss 0.0015332176117226481 Validation loss 0.04910212755203247 Accuracy 0.8784999847412109\n",
      "Iteration 4990 Training loss 0.0022863878402858973 Validation loss 0.04911390691995621 Accuracy 0.8794999718666077\n",
      "Iteration 5000 Training loss 0.0015314194606617093 Validation loss 0.04917449131608009 Accuracy 0.8790000081062317\n",
      "Iteration 5010 Training loss 0.002782918745651841 Validation loss 0.04912747070193291 Accuracy 0.8774999976158142\n",
      "Iteration 5020 Training loss 0.0030350680463016033 Validation loss 0.04913080111145973 Accuracy 0.8784999847412109\n",
      "Iteration 5030 Training loss 0.001531491638161242 Validation loss 0.04918021336197853 Accuracy 0.8790000081062317\n",
      "Iteration 5040 Training loss 0.0010326432529836893 Validation loss 0.04917796328663826 Accuracy 0.8784999847412109\n",
      "Iteration 5050 Training loss 0.003032336011528969 Validation loss 0.04917305335402489 Accuracy 0.8790000081062317\n",
      "Iteration 5060 Training loss 0.0025273305363953114 Validation loss 0.04920969530940056 Accuracy 0.8784999847412109\n",
      "Iteration 5070 Training loss 0.002029344905167818 Validation loss 0.04914408549666405 Accuracy 0.8784999847412109\n",
      "Iteration 5080 Training loss 0.002531288657337427 Validation loss 0.04920841380953789 Accuracy 0.878000020980835\n",
      "Iteration 5090 Training loss 0.0015289902221411467 Validation loss 0.04925885796546936 Accuracy 0.8794999718666077\n",
      "Iteration 5100 Training loss 0.0032805732917040586 Validation loss 0.04926008731126785 Accuracy 0.8790000081062317\n",
      "Iteration 5110 Training loss 0.0015333114424720407 Validation loss 0.04922260344028473 Accuracy 0.878000020980835\n",
      "Iteration 5120 Training loss 0.002282638568431139 Validation loss 0.04923892393708229 Accuracy 0.8784999847412109\n",
      "Iteration 5130 Training loss 0.000532398174982518 Validation loss 0.04923858493566513 Accuracy 0.8794999718666077\n",
      "Iteration 5140 Training loss 0.0015318588120862842 Validation loss 0.04920500889420509 Accuracy 0.8794999718666077\n",
      "Iteration 5150 Training loss 0.0015297876670956612 Validation loss 0.04921441897749901 Accuracy 0.8794999718666077\n",
      "Iteration 5160 Training loss 0.0012790880864486098 Validation loss 0.049175750464200974 Accuracy 0.8794999718666077\n",
      "Iteration 5170 Training loss 0.0015267982380464673 Validation loss 0.049222681671381 Accuracy 0.8794999718666077\n",
      "Iteration 5180 Training loss 0.001280280645005405 Validation loss 0.04926992207765579 Accuracy 0.8790000081062317\n",
      "Iteration 5190 Training loss 0.0017807421972975135 Validation loss 0.049197062849998474 Accuracy 0.878000020980835\n",
      "Iteration 5200 Training loss 0.0015306957066059113 Validation loss 0.04922816529870033 Accuracy 0.8784999847412109\n",
      "Iteration 5210 Training loss 0.0020291663240641356 Validation loss 0.04925794154405594 Accuracy 0.8794999718666077\n",
      "Iteration 5220 Training loss 0.0010327333584427834 Validation loss 0.04925481975078583 Accuracy 0.8794999718666077\n",
      "Iteration 5230 Training loss 0.0015280001098290086 Validation loss 0.04923849552869797 Accuracy 0.8794999718666077\n",
      "Iteration 5240 Training loss 0.002030320232734084 Validation loss 0.04923638701438904 Accuracy 0.8784999847412109\n",
      "Iteration 5250 Training loss 0.0010334003018215299 Validation loss 0.0492282398045063 Accuracy 0.8794999718666077\n",
      "Iteration 5260 Training loss 0.002530365949496627 Validation loss 0.049247562885284424 Accuracy 0.8794999718666077\n",
      "Iteration 5270 Training loss 0.001778207253664732 Validation loss 0.04926541820168495 Accuracy 0.8790000081062317\n",
      "Iteration 5280 Training loss 0.0020293169654905796 Validation loss 0.049266085028648376 Accuracy 0.8784999847412109\n",
      "Iteration 5290 Training loss 0.0012807204620912671 Validation loss 0.04928058385848999 Accuracy 0.8794999718666077\n",
      "Iteration 5300 Training loss 0.0015254589961841702 Validation loss 0.049299970269203186 Accuracy 0.8794999718666077\n",
      "Iteration 5310 Training loss 0.002029897877946496 Validation loss 0.049264390021562576 Accuracy 0.8784999847412109\n",
      "Iteration 5320 Training loss 0.0015271534211933613 Validation loss 0.04931570589542389 Accuracy 0.8784999847412109\n",
      "Iteration 5330 Training loss 0.0020257788710296154 Validation loss 0.049320366233587265 Accuracy 0.8790000081062317\n",
      "Iteration 5340 Training loss 0.0015250315191224217 Validation loss 0.04932227358222008 Accuracy 0.8790000081062317\n",
      "Iteration 5350 Training loss 0.0017762770876288414 Validation loss 0.04930407553911209 Accuracy 0.8794999718666077\n",
      "Iteration 5360 Training loss 0.0035255057737231255 Validation loss 0.04931465908885002 Accuracy 0.8794999718666077\n",
      "Iteration 5370 Training loss 0.0012719188816845417 Validation loss 0.04933976009488106 Accuracy 0.8794999718666077\n",
      "Iteration 5380 Training loss 0.0012779064709320664 Validation loss 0.049329858273267746 Accuracy 0.8790000081062317\n",
      "Iteration 5390 Training loss 0.0032758479937911034 Validation loss 0.049322884529829025 Accuracy 0.8790000081062317\n",
      "Iteration 5400 Training loss 0.0015227022813633084 Validation loss 0.04934743791818619 Accuracy 0.8794999718666077\n",
      "Iteration 5410 Training loss 0.0017788951518014073 Validation loss 0.04933815076947212 Accuracy 0.8794999718666077\n",
      "Iteration 5420 Training loss 0.0017814076272770762 Validation loss 0.049323517829179764 Accuracy 0.8790000081062317\n",
      "Iteration 5430 Training loss 0.002527124946936965 Validation loss 0.049327198415994644 Accuracy 0.8784999847412109\n",
      "Iteration 5440 Training loss 0.002025436144322157 Validation loss 0.04934085160493851 Accuracy 0.8794999718666077\n",
      "Iteration 5450 Training loss 0.0015265134861692786 Validation loss 0.04931323230266571 Accuracy 0.8790000081062317\n",
      "Iteration 5460 Training loss 0.0017770533449947834 Validation loss 0.049370281398296356 Accuracy 0.8794999718666077\n",
      "Iteration 5470 Training loss 0.0012750634923577309 Validation loss 0.04935375601053238 Accuracy 0.8790000081062317\n",
      "Iteration 5480 Training loss 0.00227410439401865 Validation loss 0.0493774376809597 Accuracy 0.8794999718666077\n",
      "Iteration 5490 Training loss 0.0032776459120213985 Validation loss 0.04933542758226395 Accuracy 0.8794999718666077\n",
      "Iteration 5500 Training loss 0.00202834396623075 Validation loss 0.04938901588320732 Accuracy 0.8794999718666077\n",
      "Iteration 5510 Training loss 0.002027203794568777 Validation loss 0.04939129203557968 Accuracy 0.8790000081062317\n",
      "Iteration 5520 Training loss 0.0025299396365880966 Validation loss 0.049414291977882385 Accuracy 0.8794999718666077\n",
      "Iteration 5530 Training loss 0.0010250341147184372 Validation loss 0.049391962587833405 Accuracy 0.8794999718666077\n",
      "Iteration 5540 Training loss 0.002526599681004882 Validation loss 0.04939250275492668 Accuracy 0.8794999718666077\n",
      "Iteration 5550 Training loss 0.002528566401451826 Validation loss 0.049390848726034164 Accuracy 0.8790000081062317\n",
      "Iteration 5560 Training loss 0.0015259089414030313 Validation loss 0.04944611340761185 Accuracy 0.8790000081062317\n",
      "Iteration 5570 Training loss 0.001775736571289599 Validation loss 0.04952589422464371 Accuracy 0.8794999718666077\n",
      "Iteration 5580 Training loss 0.0017816758481785655 Validation loss 0.05029817298054695 Accuracy 0.8790000081062317\n",
      "Iteration 5590 Training loss 0.002027350477874279 Validation loss 0.049720484763383865 Accuracy 0.878000020980835\n",
      "Iteration 5600 Training loss 0.002537580905482173 Validation loss 0.050035301595926285 Accuracy 0.8774999976158142\n",
      "Iteration 5610 Training loss 0.0017768696416169405 Validation loss 0.049851153045892715 Accuracy 0.878000020980835\n",
      "Iteration 5620 Training loss 0.0020321125630289316 Validation loss 0.049838319420814514 Accuracy 0.8759999871253967\n",
      "Iteration 5630 Training loss 0.0020305898506194353 Validation loss 0.049804508686065674 Accuracy 0.8774999976158142\n",
      "Iteration 5640 Training loss 0.0010282121365889907 Validation loss 0.04975811019539833 Accuracy 0.878000020980835\n",
      "Iteration 5650 Training loss 0.001037210808135569 Validation loss 0.04967697709798813 Accuracy 0.878000020980835\n",
      "Iteration 5660 Training loss 0.002029179595410824 Validation loss 0.04966099560260773 Accuracy 0.878000020980835\n",
      "Iteration 5670 Training loss 0.002277474384754896 Validation loss 0.04963851347565651 Accuracy 0.8784999847412109\n",
      "Iteration 5680 Training loss 0.003771913005039096 Validation loss 0.04961290955543518 Accuracy 0.878000020980835\n",
      "Iteration 5690 Training loss 0.002781275659799576 Validation loss 0.04960646852850914 Accuracy 0.8784999847412109\n",
      "Iteration 5700 Training loss 0.0027778339572250843 Validation loss 0.04963910952210426 Accuracy 0.878000020980835\n",
      "Iteration 5710 Training loss 0.0012782783014699817 Validation loss 0.049687471240758896 Accuracy 0.878000020980835\n",
      "Iteration 5720 Training loss 0.0025264720898121595 Validation loss 0.04963802546262741 Accuracy 0.878000020980835\n",
      "Iteration 5730 Training loss 0.002026547910645604 Validation loss 0.04961780086159706 Accuracy 0.8769999742507935\n",
      "Iteration 5740 Training loss 0.002026638016104698 Validation loss 0.04956278204917908 Accuracy 0.878000020980835\n",
      "Iteration 5750 Training loss 0.0020260449964553118 Validation loss 0.04954793304204941 Accuracy 0.8774999976158142\n",
      "Iteration 5760 Training loss 0.003025500802323222 Validation loss 0.04955708980560303 Accuracy 0.8774999976158142\n",
      "Iteration 5770 Training loss 0.0015254135942086577 Validation loss 0.049525629729032516 Accuracy 0.878000020980835\n",
      "Iteration 5780 Training loss 0.002268863609060645 Validation loss 0.049528319388628006 Accuracy 0.878000020980835\n",
      "Iteration 5790 Training loss 0.0027635698206722736 Validation loss 0.0495590977370739 Accuracy 0.8784999847412109\n",
      "Iteration 5800 Training loss 0.0015952442772686481 Validation loss 0.049328241497278214 Accuracy 0.8799999952316284\n",
      "Iteration 5810 Training loss 0.002531543141230941 Validation loss 0.04901767894625664 Accuracy 0.8799999952316284\n",
      "Iteration 5820 Training loss 0.002029995433986187 Validation loss 0.04906085133552551 Accuracy 0.8794999718666077\n",
      "Iteration 5830 Training loss 0.0015295658959075809 Validation loss 0.04908236116170883 Accuracy 0.8794999718666077\n",
      "Iteration 5840 Training loss 0.0015271729789674282 Validation loss 0.049182746559381485 Accuracy 0.8794999718666077\n",
      "Iteration 5850 Training loss 0.0015313546173274517 Validation loss 0.04915502294898033 Accuracy 0.8794999718666077\n",
      "Iteration 5860 Training loss 0.0027757929638028145 Validation loss 0.0491267554461956 Accuracy 0.8794999718666077\n",
      "Iteration 5870 Training loss 0.0017757840687409043 Validation loss 0.049137894064188004 Accuracy 0.8790000081062317\n",
      "Iteration 5880 Training loss 0.002775851869955659 Validation loss 0.04918805509805679 Accuracy 0.8790000081062317\n",
      "Iteration 5890 Training loss 0.0015260390937328339 Validation loss 0.04924627020955086 Accuracy 0.8784999847412109\n",
      "Iteration 5900 Training loss 0.0010255055967718363 Validation loss 0.04921065270900726 Accuracy 0.8790000081062317\n",
      "Iteration 5910 Training loss 0.0017749953549355268 Validation loss 0.0492040179669857 Accuracy 0.8790000081062317\n",
      "Iteration 5920 Training loss 0.001274565700441599 Validation loss 0.04920955002307892 Accuracy 0.8790000081062317\n",
      "Iteration 5930 Training loss 0.0012759086675941944 Validation loss 0.04925926774740219 Accuracy 0.8784999847412109\n",
      "Iteration 5940 Training loss 0.0017753707943484187 Validation loss 0.04924095794558525 Accuracy 0.8784999847412109\n",
      "Iteration 5950 Training loss 0.001525241881608963 Validation loss 0.049251046031713486 Accuracy 0.8784999847412109\n",
      "Iteration 5960 Training loss 0.0010277641704306006 Validation loss 0.04923631623387337 Accuracy 0.878000020980835\n",
      "Iteration 5970 Training loss 0.002275132806971669 Validation loss 0.04929947480559349 Accuracy 0.8784999847412109\n",
      "Iteration 5980 Training loss 0.0020263923797756433 Validation loss 0.04929555952548981 Accuracy 0.878000020980835\n",
      "Iteration 5990 Training loss 0.0007750811637379229 Validation loss 0.049277618527412415 Accuracy 0.8784999847412109\n",
      "Iteration 6000 Training loss 0.0020264633931219578 Validation loss 0.04930610582232475 Accuracy 0.8774999976158142\n",
      "Iteration 6010 Training loss 0.0010224365396425128 Validation loss 0.049311257898807526 Accuracy 0.8774999976158142\n",
      "Iteration 6020 Training loss 0.0017731826519593596 Validation loss 0.04928695037961006 Accuracy 0.878000020980835\n",
      "Iteration 6030 Training loss 0.001774596399627626 Validation loss 0.04928560182452202 Accuracy 0.878000020980835\n",
      "Iteration 6040 Training loss 0.0020267951767891645 Validation loss 0.04927045479416847 Accuracy 0.878000020980835\n",
      "Iteration 6050 Training loss 0.0010227450402453542 Validation loss 0.049321312457323074 Accuracy 0.8784999847412109\n",
      "Iteration 6060 Training loss 0.0017762427451089025 Validation loss 0.04933903366327286 Accuracy 0.8784999847412109\n",
      "Iteration 6070 Training loss 0.0020252931863069534 Validation loss 0.049315065145492554 Accuracy 0.8784999847412109\n",
      "Iteration 6080 Training loss 0.0017745919758453965 Validation loss 0.049292292445898056 Accuracy 0.8784999847412109\n",
      "Iteration 6090 Training loss 0.001272308174520731 Validation loss 0.049292150884866714 Accuracy 0.8784999847412109\n",
      "Iteration 6100 Training loss 0.001021470525301993 Validation loss 0.04930972307920456 Accuracy 0.878000020980835\n",
      "Iteration 6110 Training loss 0.0022730722557753325 Validation loss 0.04934859648346901 Accuracy 0.8784999847412109\n",
      "Iteration 6120 Training loss 0.002772328443825245 Validation loss 0.0493188314139843 Accuracy 0.8784999847412109\n",
      "Iteration 6130 Training loss 0.0007736290572211146 Validation loss 0.049304742366075516 Accuracy 0.8784999847412109\n",
      "Iteration 6140 Training loss 0.0015214572194963694 Validation loss 0.049311697483062744 Accuracy 0.8784999847412109\n",
      "Iteration 6150 Training loss 0.0027746909763664007 Validation loss 0.04931127652525902 Accuracy 0.8784999847412109\n",
      "Iteration 6160 Training loss 0.0015201905043795705 Validation loss 0.049294594675302505 Accuracy 0.8784999847412109\n",
      "Iteration 6170 Training loss 0.00127311609685421 Validation loss 0.04933152347803116 Accuracy 0.8784999847412109\n",
      "Iteration 6180 Training loss 0.001273641362786293 Validation loss 0.04933588206768036 Accuracy 0.8784999847412109\n",
      "Iteration 6190 Training loss 0.0015217955224215984 Validation loss 0.04935182258486748 Accuracy 0.8784999847412109\n",
      "Iteration 6200 Training loss 0.0020207224879413843 Validation loss 0.04935760423541069 Accuracy 0.878000020980835\n",
      "Iteration 6210 Training loss 0.002021553460508585 Validation loss 0.04933862388134003 Accuracy 0.878000020980835\n",
      "Iteration 6220 Training loss 0.0017709010280668736 Validation loss 0.049363527446985245 Accuracy 0.8784999847412109\n",
      "Iteration 6230 Training loss 0.002271021017804742 Validation loss 0.04935939610004425 Accuracy 0.8784999847412109\n",
      "Iteration 6240 Training loss 0.0015195156447589397 Validation loss 0.04939701408147812 Accuracy 0.8784999847412109\n",
      "Iteration 6250 Training loss 0.003020538715645671 Validation loss 0.04939287155866623 Accuracy 0.8784999847412109\n",
      "Iteration 6260 Training loss 0.0010212260531261563 Validation loss 0.04936419799923897 Accuracy 0.8784999847412109\n",
      "Iteration 6270 Training loss 0.0025182547979056835 Validation loss 0.04936784878373146 Accuracy 0.8784999847412109\n",
      "Iteration 6280 Training loss 0.0010202095145359635 Validation loss 0.04934559389948845 Accuracy 0.878000020980835\n",
      "Iteration 6290 Training loss 0.0022712205536663532 Validation loss 0.04935513436794281 Accuracy 0.8784999847412109\n",
      "Iteration 6300 Training loss 0.0025226783473044634 Validation loss 0.04936419054865837 Accuracy 0.8784999847412109\n",
      "Iteration 6310 Training loss 0.00152113800868392 Validation loss 0.04939112067222595 Accuracy 0.8784999847412109\n",
      "Iteration 6320 Training loss 0.0025215938221663237 Validation loss 0.04937782511115074 Accuracy 0.8784999847412109\n",
      "Iteration 6330 Training loss 0.0015203317161649466 Validation loss 0.04940584674477577 Accuracy 0.878000020980835\n",
      "Iteration 6340 Training loss 0.0010228172177448869 Validation loss 0.049381572753190994 Accuracy 0.8784999847412109\n",
      "Iteration 6350 Training loss 0.001268423511646688 Validation loss 0.04939798638224602 Accuracy 0.8784999847412109\n",
      "Iteration 6360 Training loss 0.0027689954731613398 Validation loss 0.04940074309706688 Accuracy 0.8784999847412109\n",
      "Iteration 6370 Training loss 0.00176734768319875 Validation loss 0.04944347217679024 Accuracy 0.8784999847412109\n",
      "Iteration 6380 Training loss 0.0022699746768921614 Validation loss 0.04943104833364487 Accuracy 0.8784999847412109\n",
      "Iteration 6390 Training loss 0.0025197204668074846 Validation loss 0.04945038631558418 Accuracy 0.8784999847412109\n",
      "Iteration 6400 Training loss 0.0010198643431067467 Validation loss 0.049406446516513824 Accuracy 0.878000020980835\n",
      "Iteration 6410 Training loss 0.0017712649423629045 Validation loss 0.0494212843477726 Accuracy 0.878000020980835\n",
      "Iteration 6420 Training loss 0.0020184374880045652 Validation loss 0.04944956675171852 Accuracy 0.878000020980835\n",
      "Iteration 6430 Training loss 0.0022681665141135454 Validation loss 0.04945463314652443 Accuracy 0.8784999847412109\n",
      "Iteration 6440 Training loss 0.0015172403072938323 Validation loss 0.04943960905075073 Accuracy 0.8784999847412109\n",
      "Iteration 6450 Training loss 0.0017691509565338492 Validation loss 0.049400389194488525 Accuracy 0.8784999847412109\n",
      "Iteration 6460 Training loss 0.0030206870287656784 Validation loss 0.04941853508353233 Accuracy 0.8784999847412109\n",
      "Iteration 6470 Training loss 0.0027704385574907064 Validation loss 0.04944535344839096 Accuracy 0.8784999847412109\n",
      "Iteration 6480 Training loss 0.0015203978400677443 Validation loss 0.04942866042256355 Accuracy 0.878000020980835\n",
      "Iteration 6490 Training loss 0.001769988564774394 Validation loss 0.04944037273526192 Accuracy 0.8784999847412109\n",
      "Iteration 6500 Training loss 0.002769254148006439 Validation loss 0.04946109652519226 Accuracy 0.8784999847412109\n",
      "Iteration 6510 Training loss 0.0015181382186710835 Validation loss 0.049459390342235565 Accuracy 0.8790000081062317\n",
      "Iteration 6520 Training loss 0.0012703263200819492 Validation loss 0.04949754849076271 Accuracy 0.8784999847412109\n",
      "Iteration 6530 Training loss 0.0022706552408635616 Validation loss 0.04944059997797012 Accuracy 0.8784999847412109\n",
      "Iteration 6540 Training loss 0.0025183376856148243 Validation loss 0.04946846142411232 Accuracy 0.8784999847412109\n",
      "Iteration 6550 Training loss 0.0015206718817353249 Validation loss 0.04947398230433464 Accuracy 0.8790000081062317\n",
      "Iteration 6560 Training loss 0.0010207020677626133 Validation loss 0.04948614537715912 Accuracy 0.8790000081062317\n",
      "Iteration 6570 Training loss 0.0012691958108916879 Validation loss 0.04945814609527588 Accuracy 0.8790000081062317\n",
      "Iteration 6580 Training loss 0.0025200843811035156 Validation loss 0.04949409142136574 Accuracy 0.8790000081062317\n",
      "Iteration 6590 Training loss 0.002016977174207568 Validation loss 0.049478139728307724 Accuracy 0.8790000081062317\n",
      "Iteration 6600 Training loss 0.0027662902139127254 Validation loss 0.04946644976735115 Accuracy 0.8790000081062317\n",
      "Iteration 6610 Training loss 0.0015180627815425396 Validation loss 0.04946140944957733 Accuracy 0.8784999847412109\n",
      "Iteration 6620 Training loss 0.002017817460000515 Validation loss 0.04947447404265404 Accuracy 0.8790000081062317\n",
      "Iteration 6630 Training loss 0.0015184954972937703 Validation loss 0.04949156567454338 Accuracy 0.8790000081062317\n",
      "Iteration 6640 Training loss 0.002517854329198599 Validation loss 0.04952600970864296 Accuracy 0.8790000081062317\n",
      "Iteration 6650 Training loss 0.0020187378395348787 Validation loss 0.049520861357450485 Accuracy 0.8790000081062317\n",
      "Iteration 6660 Training loss 0.0022670787293463945 Validation loss 0.04953474923968315 Accuracy 0.8790000081062317\n",
      "Iteration 6670 Training loss 0.0032660276629030704 Validation loss 0.04951169714331627 Accuracy 0.8790000081062317\n",
      "Iteration 6680 Training loss 0.002268193755298853 Validation loss 0.04953327029943466 Accuracy 0.8784999847412109\n",
      "Iteration 6690 Training loss 0.002518105087801814 Validation loss 0.04951753839850426 Accuracy 0.8784999847412109\n",
      "Iteration 6700 Training loss 0.0025211391039192677 Validation loss 0.04953655228018761 Accuracy 0.8790000081062317\n",
      "Iteration 6710 Training loss 0.0025172834284603596 Validation loss 0.049536194652318954 Accuracy 0.8790000081062317\n",
      "Iteration 6720 Training loss 0.0015166755765676498 Validation loss 0.04954306408762932 Accuracy 0.8790000081062317\n",
      "Iteration 6730 Training loss 0.002765255980193615 Validation loss 0.0495561920106411 Accuracy 0.8784999847412109\n",
      "Iteration 6740 Training loss 0.003268231637775898 Validation loss 0.0495365746319294 Accuracy 0.8790000081062317\n",
      "Iteration 6750 Training loss 0.0022670470643788576 Validation loss 0.049524158239364624 Accuracy 0.8790000081062317\n",
      "Iteration 6760 Training loss 0.00226728362031281 Validation loss 0.0494973249733448 Accuracy 0.8790000081062317\n",
      "Iteration 6770 Training loss 0.0007678172551095486 Validation loss 0.04952138289809227 Accuracy 0.8790000081062317\n",
      "Iteration 6780 Training loss 0.0012672374723479152 Validation loss 0.049528807401657104 Accuracy 0.8784999847412109\n",
      "Iteration 6790 Training loss 0.001517888973467052 Validation loss 0.04952557384967804 Accuracy 0.8790000081062317\n",
      "Iteration 6800 Training loss 0.0017680285964161158 Validation loss 0.04953181371092796 Accuracy 0.8790000081062317\n",
      "Iteration 6810 Training loss 0.002516999375075102 Validation loss 0.04953739792108536 Accuracy 0.8790000081062317\n",
      "Iteration 6820 Training loss 0.002515430562198162 Validation loss 0.04953239485621452 Accuracy 0.8790000081062317\n",
      "Iteration 6830 Training loss 0.0030167899094522 Validation loss 0.04953515902161598 Accuracy 0.8790000081062317\n",
      "Iteration 6840 Training loss 0.003016473725438118 Validation loss 0.049517448991537094 Accuracy 0.8790000081062317\n",
      "Iteration 6850 Training loss 0.0017670270754024386 Validation loss 0.049541059881448746 Accuracy 0.8784999847412109\n",
      "Iteration 6860 Training loss 0.002517351182177663 Validation loss 0.049529194831848145 Accuracy 0.8790000081062317\n",
      "Iteration 6870 Training loss 0.0020171392243355513 Validation loss 0.04956895485520363 Accuracy 0.8784999847412109\n",
      "Iteration 6880 Training loss 0.0030168972443789244 Validation loss 0.04956599697470665 Accuracy 0.8784999847412109\n",
      "Iteration 6890 Training loss 0.0010184475686401129 Validation loss 0.04957198724150658 Accuracy 0.8790000081062317\n",
      "Iteration 6900 Training loss 0.002517785644158721 Validation loss 0.04960525408387184 Accuracy 0.8790000081062317\n",
      "Iteration 6910 Training loss 0.002265508286654949 Validation loss 0.04958614334464073 Accuracy 0.8790000081062317\n",
      "Iteration 6920 Training loss 0.001518706325441599 Validation loss 0.04957301914691925 Accuracy 0.8790000081062317\n",
      "Iteration 6930 Training loss 0.0010176151990890503 Validation loss 0.04957016929984093 Accuracy 0.8790000081062317\n",
      "Iteration 6940 Training loss 0.0012657521292567253 Validation loss 0.049601033329963684 Accuracy 0.8790000081062317\n",
      "Iteration 6950 Training loss 0.0015159237664192915 Validation loss 0.04964287579059601 Accuracy 0.8784999847412109\n",
      "Iteration 6960 Training loss 0.0022663597483187914 Validation loss 0.049618300050497055 Accuracy 0.8784999847412109\n",
      "Iteration 6970 Training loss 0.0015162480995059013 Validation loss 0.04964367672801018 Accuracy 0.8784999847412109\n",
      "Iteration 6980 Training loss 0.002517189597710967 Validation loss 0.049632079899311066 Accuracy 0.8784999847412109\n",
      "Iteration 6990 Training loss 0.00201624259352684 Validation loss 0.049606479704380035 Accuracy 0.8784999847412109\n",
      "Iteration 7000 Training loss 0.003015596652403474 Validation loss 0.049616169184446335 Accuracy 0.8784999847412109\n",
      "Iteration 7010 Training loss 0.001264228718355298 Validation loss 0.04960770532488823 Accuracy 0.8790000081062317\n",
      "Iteration 7020 Training loss 0.002515635220333934 Validation loss 0.049590371549129486 Accuracy 0.8790000081062317\n",
      "Iteration 7030 Training loss 0.0017664734041318297 Validation loss 0.049607425928115845 Accuracy 0.8790000081062317\n",
      "Iteration 7040 Training loss 0.0017650952795520425 Validation loss 0.04961177334189415 Accuracy 0.8790000081062317\n",
      "Iteration 7050 Training loss 0.001019307179376483 Validation loss 0.04958977922797203 Accuracy 0.8790000081062317\n",
      "Iteration 7060 Training loss 0.0020158691331744194 Validation loss 0.04958880692720413 Accuracy 0.8790000081062317\n",
      "Iteration 7070 Training loss 0.0012650506105273962 Validation loss 0.049608200788497925 Accuracy 0.8790000081062317\n",
      "Iteration 7080 Training loss 0.0015159088652580976 Validation loss 0.04960193485021591 Accuracy 0.8790000081062317\n",
      "Iteration 7090 Training loss 0.0022647567093372345 Validation loss 0.04962974414229393 Accuracy 0.8784999847412109\n",
      "Iteration 7100 Training loss 0.0027662767097353935 Validation loss 0.04961458593606949 Accuracy 0.8790000081062317\n",
      "Iteration 7110 Training loss 0.0020130800548940897 Validation loss 0.049614399671554565 Accuracy 0.8790000081062317\n",
      "Iteration 7120 Training loss 0.0010161708341911435 Validation loss 0.04960538446903229 Accuracy 0.8790000081062317\n",
      "Iteration 7130 Training loss 0.0015158213209360838 Validation loss 0.04959310218691826 Accuracy 0.8790000081062317\n",
      "Iteration 7140 Training loss 0.0012651000870391726 Validation loss 0.04962962493300438 Accuracy 0.8784999847412109\n",
      "Iteration 7150 Training loss 0.002014470985159278 Validation loss 0.0496336854994297 Accuracy 0.8784999847412109\n",
      "Iteration 7160 Training loss 0.00151550630107522 Validation loss 0.04962252825498581 Accuracy 0.8784999847412109\n",
      "Iteration 7170 Training loss 0.001516329008154571 Validation loss 0.04963044822216034 Accuracy 0.8784999847412109\n",
      "Iteration 7180 Training loss 0.0025150785222649574 Validation loss 0.049646079540252686 Accuracy 0.8784999847412109\n",
      "Iteration 7190 Training loss 0.002013551304116845 Validation loss 0.04964153468608856 Accuracy 0.8784999847412109\n",
      "Iteration 7200 Training loss 0.0015144678764045238 Validation loss 0.04962357506155968 Accuracy 0.8784999847412109\n",
      "Iteration 7210 Training loss 0.001766242436133325 Validation loss 0.049636270850896835 Accuracy 0.8784999847412109\n",
      "Iteration 7220 Training loss 0.0027651337441056967 Validation loss 0.04964343458414078 Accuracy 0.8784999847412109\n",
      "Iteration 7230 Training loss 0.0030155363492667675 Validation loss 0.04964236542582512 Accuracy 0.8790000081062317\n",
      "Iteration 7240 Training loss 0.0017663781763985753 Validation loss 0.04964841529726982 Accuracy 0.8790000081062317\n",
      "Iteration 7250 Training loss 0.0037627893034368753 Validation loss 0.049635931849479675 Accuracy 0.8790000081062317\n",
      "Iteration 7260 Training loss 0.002015182515606284 Validation loss 0.04965362697839737 Accuracy 0.8784999847412109\n",
      "Iteration 7270 Training loss 0.0017643673345446587 Validation loss 0.04964963346719742 Accuracy 0.8790000081062317\n",
      "Iteration 7280 Training loss 0.0020162872970104218 Validation loss 0.049658749252557755 Accuracy 0.8784999847412109\n",
      "Iteration 7290 Training loss 0.0022643841803073883 Validation loss 0.049640804529190063 Accuracy 0.8784999847412109\n",
      "Iteration 7300 Training loss 0.0015156851150095463 Validation loss 0.04965526610612869 Accuracy 0.8784999847412109\n",
      "Iteration 7310 Training loss 0.0012612438295036554 Validation loss 0.04964478313922882 Accuracy 0.8784999847412109\n",
      "Iteration 7320 Training loss 0.0020151566714048386 Validation loss 0.04966704919934273 Accuracy 0.8790000081062317\n",
      "Iteration 7330 Training loss 0.0022651134058833122 Validation loss 0.049654897302389145 Accuracy 0.8784999847412109\n",
      "Iteration 7340 Training loss 0.003265190636739135 Validation loss 0.04964787885546684 Accuracy 0.8790000081062317\n",
      "Iteration 7350 Training loss 0.001514521660283208 Validation loss 0.049645937979221344 Accuracy 0.8790000081062317\n",
      "Iteration 7360 Training loss 0.0020141967106610537 Validation loss 0.04964623600244522 Accuracy 0.8790000081062317\n",
      "Iteration 7370 Training loss 0.0017637189012020826 Validation loss 0.04965434595942497 Accuracy 0.8784999847412109\n",
      "Iteration 7380 Training loss 0.0015155147993937135 Validation loss 0.0496518649160862 Accuracy 0.8784999847412109\n",
      "Iteration 7390 Training loss 0.0015142550691962242 Validation loss 0.04965047538280487 Accuracy 0.8784999847412109\n",
      "Iteration 7400 Training loss 0.0022635930217802525 Validation loss 0.049666907638311386 Accuracy 0.8790000081062317\n",
      "Iteration 7410 Training loss 0.0022632868494838476 Validation loss 0.049653224647045135 Accuracy 0.8790000081062317\n",
      "Iteration 7420 Training loss 0.0007637734524905682 Validation loss 0.04966716840863228 Accuracy 0.8790000081062317\n",
      "Iteration 7430 Training loss 0.0020122944843024015 Validation loss 0.04967768117785454 Accuracy 0.8790000081062317\n",
      "Iteration 7440 Training loss 0.0017638931749388576 Validation loss 0.04968037083745003 Accuracy 0.8790000081062317\n",
      "Iteration 7450 Training loss 0.0010146290296688676 Validation loss 0.04967952519655228 Accuracy 0.8790000081062317\n",
      "Iteration 7460 Training loss 0.0017624113243073225 Validation loss 0.049689896404743195 Accuracy 0.8784999847412109\n",
      "Iteration 7470 Training loss 0.00226446520537138 Validation loss 0.049699779599905014 Accuracy 0.8784999847412109\n",
      "Iteration 7480 Training loss 0.0017633929383009672 Validation loss 0.04969006031751633 Accuracy 0.8784999847412109\n",
      "Iteration 7490 Training loss 0.0022640081588178873 Validation loss 0.04968112334609032 Accuracy 0.8784999847412109\n",
      "Iteration 7500 Training loss 0.0017621228471398354 Validation loss 0.04966834932565689 Accuracy 0.8784999847412109\n",
      "Iteration 7510 Training loss 0.0027641672641038895 Validation loss 0.04970492422580719 Accuracy 0.8784999847412109\n",
      "Iteration 7520 Training loss 0.0022628563456237316 Validation loss 0.04968716949224472 Accuracy 0.8784999847412109\n",
      "Iteration 7530 Training loss 0.0012638721382245421 Validation loss 0.049714699387550354 Accuracy 0.8784999847412109\n",
      "Iteration 7540 Training loss 0.0027640049811452627 Validation loss 0.04969671741127968 Accuracy 0.8790000081062317\n",
      "Iteration 7550 Training loss 0.0020137839019298553 Validation loss 0.04969676956534386 Accuracy 0.8784999847412109\n",
      "Iteration 7560 Training loss 0.002264102455228567 Validation loss 0.049708060920238495 Accuracy 0.8784999847412109\n",
      "Iteration 7570 Training loss 0.002263089641928673 Validation loss 0.0497296042740345 Accuracy 0.8790000081062317\n",
      "Iteration 7580 Training loss 0.0012627955293282866 Validation loss 0.04973988980054855 Accuracy 0.8790000081062317\n",
      "Iteration 7590 Training loss 0.0012649627169594169 Validation loss 0.04974524676799774 Accuracy 0.8790000081062317\n",
      "Iteration 7600 Training loss 0.001014329376630485 Validation loss 0.04973866045475006 Accuracy 0.8790000081062317\n",
      "Iteration 7610 Training loss 0.0017646511550992727 Validation loss 0.0497363843023777 Accuracy 0.8790000081062317\n",
      "Iteration 7620 Training loss 0.0022624412085860968 Validation loss 0.04974815249443054 Accuracy 0.8784999847412109\n",
      "Iteration 7630 Training loss 0.002014368074014783 Validation loss 0.049750134348869324 Accuracy 0.8790000081062317\n",
      "Iteration 7640 Training loss 0.0020146588794887066 Validation loss 0.0497615709900856 Accuracy 0.8790000081062317\n",
      "Iteration 7650 Training loss 0.0012636258034035563 Validation loss 0.049737412482500076 Accuracy 0.8790000081062317\n",
      "Iteration 7660 Training loss 0.001762971980497241 Validation loss 0.04974978044629097 Accuracy 0.8784999847412109\n",
      "Iteration 7670 Training loss 0.0012644152157008648 Validation loss 0.04974783584475517 Accuracy 0.8784999847412109\n",
      "Iteration 7680 Training loss 0.0020133061334490776 Validation loss 0.049739763140678406 Accuracy 0.8784999847412109\n",
      "Iteration 7690 Training loss 0.0020153610967099667 Validation loss 0.04973661154508591 Accuracy 0.8790000081062317\n",
      "Iteration 7700 Training loss 0.002512791659682989 Validation loss 0.04974266141653061 Accuracy 0.8784999847412109\n",
      "Iteration 7710 Training loss 0.0012632833095267415 Validation loss 0.04974070563912392 Accuracy 0.8784999847412109\n",
      "Iteration 7720 Training loss 0.002760410076007247 Validation loss 0.049732208251953125 Accuracy 0.8784999847412109\n",
      "Iteration 7730 Training loss 0.0007627619779668748 Validation loss 0.049741629511117935 Accuracy 0.8784999847412109\n",
      "Iteration 7740 Training loss 0.000761926406994462 Validation loss 0.04973525181412697 Accuracy 0.8784999847412109\n",
      "Iteration 7750 Training loss 0.0012632913421839476 Validation loss 0.04974379763007164 Accuracy 0.8784999847412109\n",
      "Iteration 7760 Training loss 0.0022622966207563877 Validation loss 0.04974601790308952 Accuracy 0.8784999847412109\n",
      "Iteration 7770 Training loss 0.0025130962021648884 Validation loss 0.049744658172130585 Accuracy 0.8784999847412109\n",
      "Iteration 7780 Training loss 0.0020121594425290823 Validation loss 0.04974491894245148 Accuracy 0.8784999847412109\n",
      "Iteration 7790 Training loss 0.0017629179637879133 Validation loss 0.0497363805770874 Accuracy 0.8784999847412109\n",
      "Iteration 7800 Training loss 0.0007623295532539487 Validation loss 0.049742333590984344 Accuracy 0.8784999847412109\n",
      "Iteration 7810 Training loss 0.0022623431868851185 Validation loss 0.0497450977563858 Accuracy 0.8784999847412109\n",
      "Iteration 7820 Training loss 0.001262311008758843 Validation loss 0.049766212701797485 Accuracy 0.8784999847412109\n",
      "Iteration 7830 Training loss 0.001262203324586153 Validation loss 0.04974256083369255 Accuracy 0.8790000081062317\n",
      "Iteration 7840 Training loss 0.002262665191665292 Validation loss 0.04975257068872452 Accuracy 0.8784999847412109\n",
      "Iteration 7850 Training loss 0.004012443125247955 Validation loss 0.04976619780063629 Accuracy 0.8784999847412109\n",
      "Iteration 7860 Training loss 0.0015128280501812696 Validation loss 0.049755439162254333 Accuracy 0.8784999847412109\n",
      "Iteration 7870 Training loss 0.0015137384179979563 Validation loss 0.0497545562684536 Accuracy 0.8790000081062317\n",
      "Iteration 7880 Training loss 0.0027624128852039576 Validation loss 0.04976882413029671 Accuracy 0.8784999847412109\n",
      "Iteration 7890 Training loss 0.0017629392677918077 Validation loss 0.04976838082075119 Accuracy 0.8784999847412109\n",
      "Iteration 7900 Training loss 0.002262595109641552 Validation loss 0.04978189244866371 Accuracy 0.8790000081062317\n",
      "Iteration 7910 Training loss 0.0027629488613456488 Validation loss 0.049772921949625015 Accuracy 0.8790000081062317\n",
      "Iteration 7920 Training loss 0.001763666863553226 Validation loss 0.049777477979660034 Accuracy 0.8784999847412109\n",
      "Iteration 7930 Training loss 0.0010121248196810484 Validation loss 0.049736034125089645 Accuracy 0.8790000081062317\n",
      "Iteration 7940 Training loss 0.0017624414758756757 Validation loss 0.04976719617843628 Accuracy 0.8790000081062317\n",
      "Iteration 7950 Training loss 0.0027611451223492622 Validation loss 0.049786102026700974 Accuracy 0.8790000081062317\n",
      "Iteration 7960 Training loss 0.00226140976883471 Validation loss 0.04975420981645584 Accuracy 0.8790000081062317\n",
      "Iteration 7970 Training loss 0.0020128770265728235 Validation loss 0.04975173994898796 Accuracy 0.8790000081062317\n",
      "Iteration 7980 Training loss 0.0015120948664844036 Validation loss 0.049768026918172836 Accuracy 0.8790000081062317\n",
      "Iteration 7990 Training loss 0.001762521918863058 Validation loss 0.04975331947207451 Accuracy 0.8790000081062317\n",
      "Iteration 8000 Training loss 0.002012204146012664 Validation loss 0.049775928258895874 Accuracy 0.8790000081062317\n",
      "Iteration 8010 Training loss 0.0010121958330273628 Validation loss 0.049769189208745956 Accuracy 0.8784999847412109\n",
      "Iteration 8020 Training loss 0.0022622374817728996 Validation loss 0.049795858561992645 Accuracy 0.8784999847412109\n",
      "Iteration 8030 Training loss 0.002262361813336611 Validation loss 0.04979480803012848 Accuracy 0.8784999847412109\n",
      "Iteration 8040 Training loss 0.001761679188348353 Validation loss 0.04980257898569107 Accuracy 0.8784999847412109\n",
      "Iteration 8050 Training loss 0.0020106672309339046 Validation loss 0.04980296269059181 Accuracy 0.8784999847412109\n",
      "Iteration 8060 Training loss 0.0010134750045835972 Validation loss 0.04979879781603813 Accuracy 0.8784999847412109\n",
      "Iteration 8070 Training loss 0.0030104219913482666 Validation loss 0.049805980175733566 Accuracy 0.8784999847412109\n",
      "Iteration 8080 Training loss 0.002262595109641552 Validation loss 0.049792952835559845 Accuracy 0.8790000081062317\n",
      "Iteration 8090 Training loss 0.002011822070926428 Validation loss 0.049809593707323074 Accuracy 0.8790000081062317\n",
      "Iteration 8100 Training loss 0.0027618866879493 Validation loss 0.04977899044752121 Accuracy 0.8790000081062317\n",
      "Iteration 8110 Training loss 0.0010120923398062587 Validation loss 0.04976697638630867 Accuracy 0.8790000081062317\n",
      "Iteration 8120 Training loss 0.0017621946753934026 Validation loss 0.04978703707456589 Accuracy 0.8784999847412109\n",
      "Iteration 8130 Training loss 0.002011474221944809 Validation loss 0.049780018627643585 Accuracy 0.8784999847412109\n",
      "Iteration 8140 Training loss 0.002011867007240653 Validation loss 0.04978825896978378 Accuracy 0.8784999847412109\n",
      "Iteration 8150 Training loss 0.0017608612542971969 Validation loss 0.04978783801198006 Accuracy 0.8784999847412109\n",
      "Iteration 8160 Training loss 0.0020111470948904753 Validation loss 0.04981110617518425 Accuracy 0.8784999847412109\n",
      "Iteration 8170 Training loss 0.0015117218717932701 Validation loss 0.0498168058693409 Accuracy 0.8784999847412109\n",
      "Iteration 8180 Training loss 0.0017610847717151046 Validation loss 0.04981648549437523 Accuracy 0.8784999847412109\n",
      "Iteration 8190 Training loss 0.003260243684053421 Validation loss 0.04980297014117241 Accuracy 0.8790000081062317\n",
      "Iteration 8200 Training loss 0.0015121628530323505 Validation loss 0.04982734099030495 Accuracy 0.8784999847412109\n",
      "Iteration 8210 Training loss 0.00326120899990201 Validation loss 0.049820877611637115 Accuracy 0.8790000081062317\n",
      "Iteration 8220 Training loss 0.0017611784860491753 Validation loss 0.04982461780309677 Accuracy 0.8790000081062317\n",
      "Iteration 8230 Training loss 0.002011003438383341 Validation loss 0.04981747642159462 Accuracy 0.8790000081062317\n",
      "Iteration 8240 Training loss 0.001512021292001009 Validation loss 0.049826208502054214 Accuracy 0.8784999847412109\n",
      "Iteration 8250 Training loss 0.001760821440257132 Validation loss 0.04982491955161095 Accuracy 0.8790000081062317\n",
      "Iteration 8260 Training loss 0.0035092588514089584 Validation loss 0.04982971027493477 Accuracy 0.8790000081062317\n",
      "Iteration 8270 Training loss 0.0010116483317688107 Validation loss 0.04981130734086037 Accuracy 0.8784999847412109\n",
      "Iteration 8280 Training loss 0.0012596583692356944 Validation loss 0.04978778958320618 Accuracy 0.8790000081062317\n",
      "Iteration 8290 Training loss 0.0015118266455829144 Validation loss 0.049828387796878815 Accuracy 0.8784999847412109\n",
      "Iteration 8300 Training loss 0.0030115910340100527 Validation loss 0.04981469735503197 Accuracy 0.8784999847412109\n",
      "Iteration 8310 Training loss 0.0022620002273470163 Validation loss 0.04981158673763275 Accuracy 0.8784999847412109\n",
      "Iteration 8320 Training loss 0.002010730793699622 Validation loss 0.04981284961104393 Accuracy 0.8784999847412109\n",
      "Iteration 8330 Training loss 0.0012605050578713417 Validation loss 0.04981638491153717 Accuracy 0.8790000081062317\n",
      "Iteration 8340 Training loss 0.0015122262993827462 Validation loss 0.04983602464199066 Accuracy 0.8784999847412109\n",
      "Iteration 8350 Training loss 0.0020117913372814655 Validation loss 0.04983893409371376 Accuracy 0.8790000081062317\n",
      "Iteration 8360 Training loss 0.0017621276201680303 Validation loss 0.04984970763325691 Accuracy 0.8784999847412109\n",
      "Iteration 8370 Training loss 0.0020118989050388336 Validation loss 0.04984397441148758 Accuracy 0.8784999847412109\n",
      "Iteration 8380 Training loss 0.003509541740640998 Validation loss 0.049856510013341904 Accuracy 0.8790000081062317\n",
      "Iteration 8390 Training loss 0.0022608034778386354 Validation loss 0.04986799508333206 Accuracy 0.8784999847412109\n",
      "Iteration 8400 Training loss 0.00201017246581614 Validation loss 0.049862395972013474 Accuracy 0.8784999847412109\n",
      "Iteration 8410 Training loss 0.003262612968683243 Validation loss 0.049855466932058334 Accuracy 0.8790000081062317\n",
      "Iteration 8420 Training loss 0.001761704683303833 Validation loss 0.04987674579024315 Accuracy 0.8790000081062317\n",
      "Iteration 8430 Training loss 0.0012608402175828815 Validation loss 0.049859028309583664 Accuracy 0.8790000081062317\n",
      "Iteration 8440 Training loss 0.001510944333858788 Validation loss 0.04984942823648453 Accuracy 0.8790000081062317\n",
      "Iteration 8450 Training loss 0.002510268008336425 Validation loss 0.04986274987459183 Accuracy 0.8790000081062317\n",
      "Iteration 8460 Training loss 0.0020100902765989304 Validation loss 0.04985809698700905 Accuracy 0.8790000081062317\n",
      "Iteration 8470 Training loss 0.0025109027046710253 Validation loss 0.0498657189309597 Accuracy 0.8790000081062317\n",
      "Iteration 8480 Training loss 0.0015119945164769888 Validation loss 0.04985405504703522 Accuracy 0.8790000081062317\n",
      "Iteration 8490 Training loss 0.0010119249345734715 Validation loss 0.04984522983431816 Accuracy 0.8790000081062317\n",
      "Iteration 8500 Training loss 0.001261777593754232 Validation loss 0.049862731248140335 Accuracy 0.8790000081062317\n",
      "Iteration 8510 Training loss 0.0017611489165574312 Validation loss 0.049867384135723114 Accuracy 0.8784999847412109\n",
      "Iteration 8520 Training loss 0.0015110759995877743 Validation loss 0.04985656216740608 Accuracy 0.8790000081062317\n",
      "Iteration 8530 Training loss 0.002009701682254672 Validation loss 0.04986313730478287 Accuracy 0.8790000081062317\n",
      "Iteration 8540 Training loss 0.0012605063384398818 Validation loss 0.04986460134387016 Accuracy 0.8790000081062317\n",
      "Iteration 8550 Training loss 0.0015114810084924102 Validation loss 0.049836885184049606 Accuracy 0.8790000081062317\n",
      "Iteration 8560 Training loss 0.002261683577671647 Validation loss 0.049851518124341965 Accuracy 0.8790000081062317\n",
      "Iteration 8570 Training loss 0.00200998829677701 Validation loss 0.04984192177653313 Accuracy 0.8790000081062317\n",
      "Iteration 8580 Training loss 0.0025107397232204676 Validation loss 0.04984989017248154 Accuracy 0.8784999847412109\n",
      "Iteration 8590 Training loss 0.002012043260037899 Validation loss 0.04985014721751213 Accuracy 0.8790000081062317\n",
      "Iteration 8600 Training loss 0.002511050319299102 Validation loss 0.04984978958964348 Accuracy 0.8790000081062317\n",
      "Iteration 8610 Training loss 0.0010115343611687422 Validation loss 0.04983314871788025 Accuracy 0.8790000081062317\n",
      "Iteration 8620 Training loss 0.0020118688698858023 Validation loss 0.04984414950013161 Accuracy 0.8790000081062317\n",
      "Iteration 8630 Training loss 0.0015111290849745274 Validation loss 0.049856510013341904 Accuracy 0.8790000081062317\n",
      "Iteration 8640 Training loss 0.0025109935086220503 Validation loss 0.04986361786723137 Accuracy 0.8784999847412109\n",
      "Iteration 8650 Training loss 0.0015105358324944973 Validation loss 0.04985302686691284 Accuracy 0.8784999847412109\n",
      "Iteration 8660 Training loss 0.0017608089838176966 Validation loss 0.04988210275769234 Accuracy 0.8784999847412109\n",
      "Iteration 8670 Training loss 0.0030102406162768602 Validation loss 0.04987797513604164 Accuracy 0.8790000081062317\n",
      "Iteration 8680 Training loss 0.001509728142991662 Validation loss 0.04987338185310364 Accuracy 0.8784999847412109\n",
      "Iteration 8690 Training loss 0.0022599606309086084 Validation loss 0.04985521361231804 Accuracy 0.8790000081062317\n",
      "Iteration 8700 Training loss 0.002260728506371379 Validation loss 0.049884133040905 Accuracy 0.8784999847412109\n",
      "Iteration 8710 Training loss 0.0020100281108170748 Validation loss 0.049874138087034225 Accuracy 0.8790000081062317\n",
      "Iteration 8720 Training loss 0.0015100190648809075 Validation loss 0.04987598583102226 Accuracy 0.8790000081062317\n",
      "Iteration 8730 Training loss 0.0007606783183291554 Validation loss 0.04987703636288643 Accuracy 0.8790000081062317\n",
      "Iteration 8740 Training loss 0.0010113527532666922 Validation loss 0.049880143254995346 Accuracy 0.8784999847412109\n",
      "Iteration 8750 Training loss 0.0015094017144292593 Validation loss 0.0499022975564003 Accuracy 0.8784999847412109\n",
      "Iteration 8760 Training loss 0.0025095611345022917 Validation loss 0.04991253465414047 Accuracy 0.8784999847412109\n",
      "Iteration 8770 Training loss 0.002510411897674203 Validation loss 0.04989929124712944 Accuracy 0.8784999847412109\n",
      "Iteration 8780 Training loss 0.0007611485198140144 Validation loss 0.04990581050515175 Accuracy 0.8784999847412109\n",
      "Iteration 8790 Training loss 0.0020093529019504786 Validation loss 0.04988986626267433 Accuracy 0.8790000081062317\n",
      "Iteration 8800 Training loss 0.0010106160771101713 Validation loss 0.049902599304914474 Accuracy 0.8784999847412109\n",
      "Iteration 8810 Training loss 0.0017604648601263762 Validation loss 0.049898017197847366 Accuracy 0.8784999847412109\n",
      "Iteration 8820 Training loss 0.002010582946240902 Validation loss 0.04988262057304382 Accuracy 0.8784999847412109\n",
      "Iteration 8830 Training loss 0.0015104475896805525 Validation loss 0.049889542162418365 Accuracy 0.8790000081062317\n",
      "Iteration 8840 Training loss 0.0020095687359571457 Validation loss 0.04989036172628403 Accuracy 0.8790000081062317\n",
      "Iteration 8850 Training loss 0.0022596402559429407 Validation loss 0.049916502088308334 Accuracy 0.8784999847412109\n",
      "Iteration 8860 Training loss 0.0020099878311157227 Validation loss 0.0499078743159771 Accuracy 0.8784999847412109\n",
      "Iteration 8870 Training loss 0.002509636804461479 Validation loss 0.04989781603217125 Accuracy 0.8790000081062317\n",
      "Iteration 8880 Training loss 0.002011049771681428 Validation loss 0.0499107725918293 Accuracy 0.8790000081062317\n",
      "Iteration 8890 Training loss 0.002259277505800128 Validation loss 0.0498967170715332 Accuracy 0.8790000081062317\n",
      "Iteration 8900 Training loss 0.00250983121804893 Validation loss 0.04990396276116371 Accuracy 0.8790000081062317\n",
      "Iteration 8910 Training loss 0.0012610518606379628 Validation loss 0.04992343485355377 Accuracy 0.8784999847412109\n",
      "Iteration 8920 Training loss 0.0005097255343571305 Validation loss 0.049910012632608414 Accuracy 0.8790000081062317\n",
      "Iteration 8930 Training loss 0.0015099794836714864 Validation loss 0.049923449754714966 Accuracy 0.8784999847412109\n",
      "Iteration 8940 Training loss 0.002259529661387205 Validation loss 0.049903810024261475 Accuracy 0.8784999847412109\n",
      "Iteration 8950 Training loss 0.001009713509120047 Validation loss 0.04991230368614197 Accuracy 0.8790000081062317\n",
      "Iteration 8960 Training loss 0.0015093848342075944 Validation loss 0.049910079687833786 Accuracy 0.8790000081062317\n",
      "Iteration 8970 Training loss 0.0027598002925515175 Validation loss 0.049924805760383606 Accuracy 0.8790000081062317\n",
      "Iteration 8980 Training loss 0.0017598300473764539 Validation loss 0.049917105585336685 Accuracy 0.8790000081062317\n",
      "Iteration 8990 Training loss 0.0022590416483581066 Validation loss 0.0499177910387516 Accuracy 0.8790000081062317\n",
      "Iteration 9000 Training loss 0.00225989636965096 Validation loss 0.0499330535531044 Accuracy 0.8784999847412109\n",
      "Iteration 9010 Training loss 0.001510642934590578 Validation loss 0.04991131275892258 Accuracy 0.8790000081062317\n",
      "Iteration 9020 Training loss 0.0022576015908271074 Validation loss 0.049919888377189636 Accuracy 0.8790000081062317\n",
      "Iteration 9030 Training loss 0.0027595318388193846 Validation loss 0.04991668835282326 Accuracy 0.8790000081062317\n",
      "Iteration 9040 Training loss 0.002759624971076846 Validation loss 0.04992567375302315 Accuracy 0.8790000081062317\n",
      "Iteration 9050 Training loss 0.003009380307048559 Validation loss 0.04991215839982033 Accuracy 0.8790000081062317\n",
      "Iteration 9060 Training loss 0.0015091063687577844 Validation loss 0.049920037388801575 Accuracy 0.8790000081062317\n",
      "Iteration 9070 Training loss 0.0017604079330340028 Validation loss 0.04993545636534691 Accuracy 0.8790000081062317\n",
      "Iteration 9080 Training loss 0.001259724609553814 Validation loss 0.04994586110115051 Accuracy 0.8790000081062317\n",
      "Iteration 9090 Training loss 0.002508652862161398 Validation loss 0.0499572791159153 Accuracy 0.8790000081062317\n",
      "Iteration 9100 Training loss 0.002009682124480605 Validation loss 0.049963951110839844 Accuracy 0.8790000081062317\n",
      "Iteration 9110 Training loss 0.0015101319877430797 Validation loss 0.04994424059987068 Accuracy 0.8790000081062317\n",
      "Iteration 9120 Training loss 0.0025089597329497337 Validation loss 0.04995089769363403 Accuracy 0.8790000081062317\n",
      "Iteration 9130 Training loss 0.0017594703240320086 Validation loss 0.04995259642601013 Accuracy 0.8784999847412109\n",
      "Iteration 9140 Training loss 0.0017599551938474178 Validation loss 0.049955468624830246 Accuracy 0.8790000081062317\n",
      "Iteration 9150 Training loss 0.0030095602851361036 Validation loss 0.04994700476527214 Accuracy 0.8790000081062317\n",
      "Iteration 9160 Training loss 0.002009413205087185 Validation loss 0.049956731498241425 Accuracy 0.8790000081062317\n",
      "Iteration 9170 Training loss 0.002509618643671274 Validation loss 0.049950700253248215 Accuracy 0.8790000081062317\n",
      "Iteration 9180 Training loss 0.0012595603475347161 Validation loss 0.04994629696011543 Accuracy 0.8790000081062317\n",
      "Iteration 9190 Training loss 0.0027591180987656116 Validation loss 0.04994462430477142 Accuracy 0.8790000081062317\n",
      "Iteration 9200 Training loss 0.0022600830998271704 Validation loss 0.049954649060964584 Accuracy 0.8790000081062317\n",
      "Iteration 9210 Training loss 0.0010098929051309824 Validation loss 0.04997638985514641 Accuracy 0.8784999847412109\n",
      "Iteration 9220 Training loss 0.0015088943764567375 Validation loss 0.049950968474149704 Accuracy 0.8790000081062317\n",
      "Iteration 9230 Training loss 0.0022593368776142597 Validation loss 0.04994944483041763 Accuracy 0.8790000081062317\n",
      "Iteration 9240 Training loss 0.0020082418341189623 Validation loss 0.0499647818505764 Accuracy 0.8790000081062317\n",
      "Iteration 9250 Training loss 0.002509564394131303 Validation loss 0.049968961626291275 Accuracy 0.8790000081062317\n",
      "Iteration 9260 Training loss 0.001260122051462531 Validation loss 0.049956824630498886 Accuracy 0.8790000081062317\n",
      "Iteration 9270 Training loss 0.0030083628371357918 Validation loss 0.04995570704340935 Accuracy 0.8790000081062317\n",
      "Iteration 9280 Training loss 0.0007587358704768121 Validation loss 0.04994197189807892 Accuracy 0.8790000081062317\n",
      "Iteration 9290 Training loss 0.0022599531803280115 Validation loss 0.04995584115386009 Accuracy 0.8790000081062317\n",
      "Iteration 9300 Training loss 0.0020089042373001575 Validation loss 0.04997773841023445 Accuracy 0.8790000081062317\n",
      "Iteration 9310 Training loss 0.001010296051390469 Validation loss 0.049969155341386795 Accuracy 0.8790000081062317\n",
      "Iteration 9320 Training loss 0.002508510136976838 Validation loss 0.0499841533601284 Accuracy 0.8790000081062317\n",
      "Iteration 9330 Training loss 0.0015099650481715798 Validation loss 0.049992356449365616 Accuracy 0.8790000081062317\n",
      "Iteration 9340 Training loss 0.0015100870514288545 Validation loss 0.049976274371147156 Accuracy 0.8790000081062317\n",
      "Iteration 9350 Training loss 0.002009354764595628 Validation loss 0.049986839294433594 Accuracy 0.8790000081062317\n",
      "Iteration 9360 Training loss 0.001508731977082789 Validation loss 0.049970127642154694 Accuracy 0.8790000081062317\n",
      "Iteration 9370 Training loss 0.0027584119234234095 Validation loss 0.04997999593615532 Accuracy 0.8790000081062317\n",
      "Iteration 9380 Training loss 0.0027575562708079815 Validation loss 0.04997986555099487 Accuracy 0.8790000081062317\n",
      "Iteration 9390 Training loss 0.0025087008252739906 Validation loss 0.04999244213104248 Accuracy 0.8784999847412109\n",
      "Iteration 9400 Training loss 0.0015088232466951013 Validation loss 0.04999946430325508 Accuracy 0.8784999847412109\n",
      "Iteration 9410 Training loss 0.001507507055066526 Validation loss 0.049983780831098557 Accuracy 0.8790000081062317\n",
      "Iteration 9420 Training loss 0.002509306650608778 Validation loss 0.049994051456451416 Accuracy 0.8790000081062317\n",
      "Iteration 9430 Training loss 0.003009120235219598 Validation loss 0.05000763759016991 Accuracy 0.8790000081062317\n",
      "Iteration 9440 Training loss 0.0025097576435655355 Validation loss 0.05001142621040344 Accuracy 0.8790000081062317\n",
      "Iteration 9450 Training loss 0.002507962519302964 Validation loss 0.05000728368759155 Accuracy 0.8790000081062317\n",
      "Iteration 9460 Training loss 0.002508754376322031 Validation loss 0.05002494528889656 Accuracy 0.8790000081062317\n",
      "Iteration 9470 Training loss 0.002759223571047187 Validation loss 0.05002608522772789 Accuracy 0.8790000081062317\n",
      "Iteration 9480 Training loss 0.0010091183939948678 Validation loss 0.05002867057919502 Accuracy 0.8790000081062317\n",
      "Iteration 9490 Training loss 0.0022590626031160355 Validation loss 0.05001608282327652 Accuracy 0.8790000081062317\n",
      "Iteration 9500 Training loss 0.0012583746574819088 Validation loss 0.05002684146165848 Accuracy 0.8790000081062317\n",
      "Iteration 9510 Training loss 0.0015094408299773932 Validation loss 0.050019338726997375 Accuracy 0.8790000081062317\n",
      "Iteration 9520 Training loss 0.0022597669158130884 Validation loss 0.050025101751089096 Accuracy 0.8784999847412109\n",
      "Iteration 9530 Training loss 0.0022590351291000843 Validation loss 0.05003543198108673 Accuracy 0.8784999847412109\n",
      "Iteration 9540 Training loss 0.001259135315194726 Validation loss 0.05003320425748825 Accuracy 0.8784999847412109\n",
      "Iteration 9550 Training loss 0.002759152790531516 Validation loss 0.05001106113195419 Accuracy 0.8790000081062317\n",
      "Iteration 9560 Training loss 0.002259432105347514 Validation loss 0.050009142607450485 Accuracy 0.8790000081062317\n",
      "Iteration 9570 Training loss 0.0025083140935748816 Validation loss 0.05001416429877281 Accuracy 0.8784999847412109\n",
      "Iteration 9580 Training loss 0.0017582327127456665 Validation loss 0.05002745985984802 Accuracy 0.8784999847412109\n",
      "Iteration 9590 Training loss 0.0022589561995118856 Validation loss 0.05003034323453903 Accuracy 0.8784999847412109\n",
      "Iteration 9600 Training loss 0.0020087070297449827 Validation loss 0.0500306598842144 Accuracy 0.8784999847412109\n",
      "Iteration 9610 Training loss 0.002009144052863121 Validation loss 0.050019606947898865 Accuracy 0.8790000081062317\n",
      "Iteration 9620 Training loss 0.002258743392303586 Validation loss 0.050003498792648315 Accuracy 0.8790000081062317\n",
      "Iteration 9630 Training loss 0.0032581568229943514 Validation loss 0.0500173456966877 Accuracy 0.8784999847412109\n",
      "Iteration 9640 Training loss 0.003007718129083514 Validation loss 0.05000702664256096 Accuracy 0.8784999847412109\n",
      "Iteration 9650 Training loss 0.001508106361143291 Validation loss 0.049999624490737915 Accuracy 0.8790000081062317\n",
      "Iteration 9660 Training loss 0.0007588177104480565 Validation loss 0.05000493675470352 Accuracy 0.8790000081062317\n",
      "Iteration 9670 Training loss 0.0020081207621842623 Validation loss 0.050028033554553986 Accuracy 0.8784999847412109\n",
      "Iteration 9680 Training loss 0.0027581467293202877 Validation loss 0.05002710223197937 Accuracy 0.8784999847412109\n",
      "Iteration 9690 Training loss 0.0015094858827069402 Validation loss 0.05003998428583145 Accuracy 0.8784999847412109\n",
      "Iteration 9700 Training loss 0.003258324693888426 Validation loss 0.0500354990363121 Accuracy 0.8784999847412109\n",
      "Iteration 9710 Training loss 0.0020072355400770903 Validation loss 0.050025515258312225 Accuracy 0.8784999847412109\n",
      "Iteration 9720 Training loss 0.0025090049020946026 Validation loss 0.050030194222927094 Accuracy 0.8784999847412109\n",
      "Iteration 9730 Training loss 0.0012579596368595958 Validation loss 0.05003676563501358 Accuracy 0.8784999847412109\n",
      "Iteration 9740 Training loss 0.0015085302293300629 Validation loss 0.0500289611518383 Accuracy 0.8784999847412109\n",
      "Iteration 9750 Training loss 0.002509485464543104 Validation loss 0.05002675577998161 Accuracy 0.8784999847412109\n",
      "Iteration 9760 Training loss 0.0017581945285201073 Validation loss 0.05004173889756203 Accuracy 0.8784999847412109\n",
      "Iteration 9770 Training loss 0.0020085179712623358 Validation loss 0.05005987733602524 Accuracy 0.8784999847412109\n",
      "Iteration 9780 Training loss 0.0022581797093153 Validation loss 0.050054680556058884 Accuracy 0.8784999847412109\n",
      "Iteration 9790 Training loss 0.0015085757477208972 Validation loss 0.050047170370817184 Accuracy 0.8784999847412109\n",
      "Iteration 9800 Training loss 0.0007584290578961372 Validation loss 0.050050489604473114 Accuracy 0.8784999847412109\n",
      "Iteration 9810 Training loss 0.0027578589506447315 Validation loss 0.05006157606840134 Accuracy 0.8784999847412109\n",
      "Iteration 9820 Training loss 0.0017580558778718114 Validation loss 0.05004151538014412 Accuracy 0.8784999847412109\n",
      "Iteration 9830 Training loss 0.0015088313957676291 Validation loss 0.05005934089422226 Accuracy 0.8784999847412109\n",
      "Iteration 9840 Training loss 0.0020089091267436743 Validation loss 0.050049301236867905 Accuracy 0.8784999847412109\n",
      "Iteration 9850 Training loss 0.0037581080105155706 Validation loss 0.050058379769325256 Accuracy 0.878000020980835\n",
      "Iteration 9860 Training loss 0.0017578898696228862 Validation loss 0.050071652978658676 Accuracy 0.8784999847412109\n",
      "Iteration 9870 Training loss 0.0012589858379215002 Validation loss 0.05006740242242813 Accuracy 0.878000020980835\n",
      "Iteration 9880 Training loss 0.002257751068100333 Validation loss 0.05007195100188255 Accuracy 0.878000020980835\n",
      "Iteration 9890 Training loss 0.0012584715150296688 Validation loss 0.050073958933353424 Accuracy 0.878000020980835\n",
      "Iteration 9900 Training loss 0.003258617827668786 Validation loss 0.050057731568813324 Accuracy 0.878000020980835\n",
      "Iteration 9910 Training loss 0.0010088254930451512 Validation loss 0.050070274621248245 Accuracy 0.878000020980835\n",
      "Iteration 9920 Training loss 0.0035086669959127903 Validation loss 0.05005934089422226 Accuracy 0.8784999847412109\n",
      "Iteration 9930 Training loss 0.0015084913466125727 Validation loss 0.05006684735417366 Accuracy 0.8784999847412109\n",
      "Iteration 9940 Training loss 0.001508269109763205 Validation loss 0.05006542056798935 Accuracy 0.8784999847412109\n",
      "Iteration 9950 Training loss 0.0017578783445060253 Validation loss 0.050069659948349 Accuracy 0.8784999847412109\n",
      "Iteration 9960 Training loss 0.0015084019396454096 Validation loss 0.05005386471748352 Accuracy 0.8784999847412109\n",
      "Iteration 9970 Training loss 0.0012585457880049944 Validation loss 0.05004957318305969 Accuracy 0.8794999718666077\n",
      "Iteration 9980 Training loss 0.0022579673677682877 Validation loss 0.05005575343966484 Accuracy 0.8784999847412109\n",
      "Iteration 9990 Training loss 0.0020084755960851908 Validation loss 0.05005962401628494 Accuracy 0.8784999847412109\n",
      "Iteration 10000 Training loss 0.0030081633012741804 Validation loss 0.05006076395511627 Accuracy 0.8784999847412109\n",
      "Iteration 10010 Training loss 0.002258250257000327 Validation loss 0.05007023736834526 Accuracy 0.8784999847412109\n",
      "Iteration 10020 Training loss 0.0015086217317730188 Validation loss 0.05006952956318855 Accuracy 0.8784999847412109\n",
      "Iteration 10030 Training loss 0.0030076492112129927 Validation loss 0.05004873499274254 Accuracy 0.8790000081062317\n",
      "Iteration 10040 Training loss 0.002258902182802558 Validation loss 0.0500527024269104 Accuracy 0.8790000081062317\n",
      "Iteration 10050 Training loss 0.0022562311496585608 Validation loss 0.05005046725273132 Accuracy 0.8790000081062317\n",
      "Iteration 10060 Training loss 0.0012593826977536082 Validation loss 0.050045277923345566 Accuracy 0.8794999718666077\n",
      "Iteration 10070 Training loss 0.003758488455787301 Validation loss 0.050056811422109604 Accuracy 0.8784999847412109\n",
      "Iteration 10080 Training loss 0.0020082169212400913 Validation loss 0.05006980150938034 Accuracy 0.8784999847412109\n",
      "Iteration 10090 Training loss 0.00250755506567657 Validation loss 0.05007017403841019 Accuracy 0.8784999847412109\n",
      "Iteration 10100 Training loss 0.0010091396979987621 Validation loss 0.050072796642780304 Accuracy 0.8784999847412109\n",
      "Iteration 10110 Training loss 0.0037571359425783157 Validation loss 0.05006562918424606 Accuracy 0.878000020980835\n",
      "Iteration 10120 Training loss 0.0015081624733284116 Validation loss 0.050064947456121445 Accuracy 0.8784999847412109\n",
      "Iteration 10130 Training loss 0.0012594186700880527 Validation loss 0.05007345974445343 Accuracy 0.8784999847412109\n",
      "Iteration 10140 Training loss 0.0017584840534254909 Validation loss 0.0500737801194191 Accuracy 0.8790000081062317\n",
      "Iteration 10150 Training loss 0.002008428331464529 Validation loss 0.0500541627407074 Accuracy 0.8790000081062317\n",
      "Iteration 10160 Training loss 0.0017573980148881674 Validation loss 0.050064314156770706 Accuracy 0.8790000081062317\n",
      "Iteration 10170 Training loss 0.002008873736485839 Validation loss 0.05007464438676834 Accuracy 0.8784999847412109\n",
      "Iteration 10180 Training loss 0.0020070644095540047 Validation loss 0.05007811635732651 Accuracy 0.8784999847412109\n",
      "Iteration 10190 Training loss 0.0037578914780169725 Validation loss 0.05007172375917435 Accuracy 0.8784999847412109\n",
      "Iteration 10200 Training loss 0.0015080699231475592 Validation loss 0.05008325353264809 Accuracy 0.8784999847412109\n",
      "Iteration 10210 Training loss 0.0012590017868205905 Validation loss 0.05009105056524277 Accuracy 0.8784999847412109\n",
      "Iteration 10220 Training loss 0.002008830662816763 Validation loss 0.050090014934539795 Accuracy 0.8784999847412109\n",
      "Iteration 10230 Training loss 0.0022571596782654524 Validation loss 0.05007503926753998 Accuracy 0.8790000081062317\n",
      "Iteration 10240 Training loss 0.0012588065583258867 Validation loss 0.05008377879858017 Accuracy 0.8784999847412109\n",
      "Iteration 10250 Training loss 0.003507575485855341 Validation loss 0.050086986273527145 Accuracy 0.8784999847412109\n",
      "Iteration 10260 Training loss 0.0022573242895305157 Validation loss 0.05008387938141823 Accuracy 0.8784999847412109\n",
      "Iteration 10270 Training loss 0.0015082985628396273 Validation loss 0.050079524517059326 Accuracy 0.878000020980835\n",
      "Iteration 10280 Training loss 0.002006945665925741 Validation loss 0.05009451508522034 Accuracy 0.8784999847412109\n",
      "Iteration 10290 Training loss 0.0022585387341678143 Validation loss 0.05008450150489807 Accuracy 0.8784999847412109\n",
      "Iteration 10300 Training loss 0.0012569058453664184 Validation loss 0.050078652799129486 Accuracy 0.8784999847412109\n",
      "Iteration 10310 Training loss 0.001758023165166378 Validation loss 0.05009104683995247 Accuracy 0.8784999847412109\n",
      "Iteration 10320 Training loss 0.0015082567697390914 Validation loss 0.05007757619023323 Accuracy 0.8784999847412109\n",
      "Iteration 10330 Training loss 0.0020078844390809536 Validation loss 0.05007665231823921 Accuracy 0.8790000081062317\n",
      "Iteration 10340 Training loss 0.0015084081096574664 Validation loss 0.05009721964597702 Accuracy 0.878000020980835\n",
      "Iteration 10350 Training loss 0.00225832499563694 Validation loss 0.05010279268026352 Accuracy 0.8784999847412109\n",
      "Iteration 10360 Training loss 0.0025072451680898666 Validation loss 0.05009950324892998 Accuracy 0.8784999847412109\n",
      "Iteration 10370 Training loss 0.0017564232693985105 Validation loss 0.050096482038497925 Accuracy 0.8784999847412109\n",
      "Iteration 10380 Training loss 0.0007571225287392735 Validation loss 0.05008864775300026 Accuracy 0.8790000081062317\n",
      "Iteration 10390 Training loss 0.001257748925127089 Validation loss 0.05009863153100014 Accuracy 0.8790000081062317\n",
      "Iteration 10400 Training loss 0.0015085050836205482 Validation loss 0.05010225996375084 Accuracy 0.8784999847412109\n",
      "Iteration 10410 Training loss 0.0027572112157940865 Validation loss 0.05011596158146858 Accuracy 0.8784999847412109\n",
      "Iteration 10420 Training loss 0.0025074833538383245 Validation loss 0.05010466277599335 Accuracy 0.878000020980835\n",
      "Iteration 10430 Training loss 0.0012573620770126581 Validation loss 0.05010762810707092 Accuracy 0.878000020980835\n",
      "Iteration 10440 Training loss 0.0015077233547344804 Validation loss 0.0501069575548172 Accuracy 0.878000020980835\n",
      "Iteration 10450 Training loss 0.0037580367643386126 Validation loss 0.050108402967453 Accuracy 0.8790000081062317\n",
      "Iteration 10460 Training loss 0.002508742269128561 Validation loss 0.050113290548324585 Accuracy 0.8784999847412109\n",
      "Iteration 10470 Training loss 0.0025071643758565187 Validation loss 0.050120674073696136 Accuracy 0.8784999847412109\n",
      "Iteration 10480 Training loss 0.0015073484973981977 Validation loss 0.05011828988790512 Accuracy 0.8784999847412109\n",
      "Iteration 10490 Training loss 0.001758549944497645 Validation loss 0.050121430307626724 Accuracy 0.8784999847412109\n",
      "Iteration 10500 Training loss 0.0017572608776390553 Validation loss 0.05012647062540054 Accuracy 0.8784999847412109\n",
      "Iteration 10510 Training loss 0.0012582351919263601 Validation loss 0.05011043697595596 Accuracy 0.8784999847412109\n",
      "Iteration 10520 Training loss 0.002757829148322344 Validation loss 0.05010852962732315 Accuracy 0.8790000081062317\n",
      "Iteration 10530 Training loss 0.0022578826174139977 Validation loss 0.0501042865216732 Accuracy 0.8790000081062317\n",
      "Iteration 10540 Training loss 0.0010077881161123514 Validation loss 0.05011691153049469 Accuracy 0.8784999847412109\n",
      "Iteration 10550 Training loss 0.0015079649165272713 Validation loss 0.05011625587940216 Accuracy 0.8790000081062317\n",
      "Iteration 10560 Training loss 0.0012568396050482988 Validation loss 0.05010947585105896 Accuracy 0.8784999847412109\n",
      "Iteration 10570 Training loss 0.0012571716215461493 Validation loss 0.05011904984712601 Accuracy 0.8790000081062317\n",
      "Iteration 10580 Training loss 0.0027570456732064486 Validation loss 0.05013469234108925 Accuracy 0.8784999847412109\n",
      "Iteration 10590 Training loss 0.002507240977138281 Validation loss 0.050140365958213806 Accuracy 0.8784999847412109\n",
      "Iteration 10600 Training loss 0.0027576740831136703 Validation loss 0.05012645572423935 Accuracy 0.878000020980835\n",
      "Iteration 10610 Training loss 0.0022579801734536886 Validation loss 0.05013366788625717 Accuracy 0.878000020980835\n",
      "Iteration 10620 Training loss 0.001507870969362557 Validation loss 0.05012667179107666 Accuracy 0.878000020980835\n",
      "Iteration 10630 Training loss 0.0017583732260391116 Validation loss 0.05012745037674904 Accuracy 0.8784999847412109\n",
      "Iteration 10640 Training loss 0.002006359165534377 Validation loss 0.050118379294872284 Accuracy 0.8784999847412109\n",
      "Iteration 10650 Training loss 0.0012571602128446102 Validation loss 0.05012814328074455 Accuracy 0.8784999847412109\n",
      "Iteration 10660 Training loss 0.002757736248895526 Validation loss 0.05013023689389229 Accuracy 0.8784999847412109\n",
      "Iteration 10670 Training loss 0.002008233219385147 Validation loss 0.050125185400247574 Accuracy 0.8790000081062317\n",
      "Iteration 10680 Training loss 0.0012574461288750172 Validation loss 0.05012400448322296 Accuracy 0.8790000081062317\n",
      "Iteration 10690 Training loss 0.002007348695769906 Validation loss 0.05013183876872063 Accuracy 0.8784999847412109\n",
      "Iteration 10700 Training loss 0.0012572179548442364 Validation loss 0.05012273043394089 Accuracy 0.8790000081062317\n",
      "Iteration 10710 Training loss 0.002756585134193301 Validation loss 0.05012921616435051 Accuracy 0.8784999847412109\n",
      "Iteration 10720 Training loss 0.0017580077983438969 Validation loss 0.0501381978392601 Accuracy 0.878000020980835\n",
      "Iteration 10730 Training loss 0.0017567977774888277 Validation loss 0.05014396086335182 Accuracy 0.878000020980835\n",
      "Iteration 10740 Training loss 0.001256701652891934 Validation loss 0.05013947933912277 Accuracy 0.8784999847412109\n",
      "Iteration 10750 Training loss 0.0015076064737513661 Validation loss 0.050139062106609344 Accuracy 0.8784999847412109\n",
      "Iteration 10760 Training loss 0.0017574116354808211 Validation loss 0.05013136938214302 Accuracy 0.8784999847412109\n",
      "Iteration 10770 Training loss 0.0032578648533672094 Validation loss 0.05014070123434067 Accuracy 0.8790000081062317\n",
      "Iteration 10780 Training loss 0.0012579469475895166 Validation loss 0.05013538897037506 Accuracy 0.8790000081062317\n",
      "Iteration 10790 Training loss 0.0027574454434216022 Validation loss 0.05013313889503479 Accuracy 0.8784999847412109\n",
      "Iteration 10800 Training loss 0.0032570897601544857 Validation loss 0.050139419734478 Accuracy 0.8790000081062317\n",
      "Iteration 10810 Training loss 0.001506880158558488 Validation loss 0.05014408007264137 Accuracy 0.8784999847412109\n",
      "Iteration 10820 Training loss 0.002256590174511075 Validation loss 0.05012458935379982 Accuracy 0.8790000081062317\n",
      "Iteration 10830 Training loss 0.0017579611157998443 Validation loss 0.050133660435676575 Accuracy 0.8790000081062317\n",
      "Iteration 10840 Training loss 0.001257454976439476 Validation loss 0.050129037350416183 Accuracy 0.8784999847412109\n",
      "Iteration 10850 Training loss 0.002008248819038272 Validation loss 0.050140924751758575 Accuracy 0.8784999847412109\n",
      "Iteration 10860 Training loss 0.002506701974198222 Validation loss 0.05013551190495491 Accuracy 0.8790000081062317\n",
      "Iteration 10870 Training loss 0.0025071215350180864 Validation loss 0.05015012249350548 Accuracy 0.8790000081062317\n",
      "Iteration 10880 Training loss 0.0027568682562559843 Validation loss 0.050157204270362854 Accuracy 0.8784999847412109\n",
      "Iteration 10890 Training loss 0.0037559233605861664 Validation loss 0.05016303434967995 Accuracy 0.8784999847412109\n",
      "Iteration 10900 Training loss 0.0027576752472668886 Validation loss 0.05015858635306358 Accuracy 0.878000020980835\n",
      "Iteration 10910 Training loss 0.002257007174193859 Validation loss 0.05015431344509125 Accuracy 0.8790000081062317\n",
      "Iteration 10920 Training loss 0.002007369650527835 Validation loss 0.05014432594180107 Accuracy 0.8790000081062317\n",
      "Iteration 10930 Training loss 0.00250724283978343 Validation loss 0.050148673355579376 Accuracy 0.8790000081062317\n",
      "Iteration 10940 Training loss 0.002756464993581176 Validation loss 0.050144318491220474 Accuracy 0.8784999847412109\n",
      "Iteration 10950 Training loss 0.001256634364835918 Validation loss 0.05014472082257271 Accuracy 0.8784999847412109\n",
      "Iteration 10960 Training loss 0.0022562770172953606 Validation loss 0.050160087645053864 Accuracy 0.8784999847412109\n",
      "Iteration 10970 Training loss 0.0022580819204449654 Validation loss 0.050166334956884384 Accuracy 0.8784999847412109\n",
      "Iteration 10980 Training loss 0.002256775042042136 Validation loss 0.05015753582119942 Accuracy 0.8784999847412109\n",
      "Iteration 10990 Training loss 0.0015077091520652175 Validation loss 0.05015154182910919 Accuracy 0.8784999847412109\n",
      "Iteration 11000 Training loss 0.0015069013461470604 Validation loss 0.05015479028224945 Accuracy 0.8784999847412109\n",
      "Iteration 11010 Training loss 0.0012562283081933856 Validation loss 0.05014575272798538 Accuracy 0.8784999847412109\n",
      "Iteration 11020 Training loss 0.0020071840845048428 Validation loss 0.05014562979340553 Accuracy 0.8784999847412109\n",
      "Iteration 11030 Training loss 0.002006890717893839 Validation loss 0.05014629662036896 Accuracy 0.8784999847412109\n",
      "Iteration 11040 Training loss 0.001507214386947453 Validation loss 0.05014973506331444 Accuracy 0.8784999847412109\n",
      "Iteration 11050 Training loss 0.002255955245345831 Validation loss 0.050153687596321106 Accuracy 0.8784999847412109\n",
      "Iteration 11060 Training loss 0.0010072917211800814 Validation loss 0.050160765647888184 Accuracy 0.8790000081062317\n",
      "Iteration 11070 Training loss 0.0032570979092270136 Validation loss 0.05016576126217842 Accuracy 0.8784999847412109\n",
      "Iteration 11080 Training loss 0.0015065541956573725 Validation loss 0.05015389993786812 Accuracy 0.8784999847412109\n",
      "Iteration 11090 Training loss 0.0027581413742154837 Validation loss 0.05016997084021568 Accuracy 0.8784999847412109\n",
      "Iteration 11100 Training loss 0.0017565047601237893 Validation loss 0.05016548931598663 Accuracy 0.8784999847412109\n",
      "Iteration 11110 Training loss 0.002506639575585723 Validation loss 0.0501786433160305 Accuracy 0.8784999847412109\n",
      "Iteration 11120 Training loss 0.0017559321131557226 Validation loss 0.05018409714102745 Accuracy 0.878000020980835\n",
      "Iteration 11130 Training loss 0.0012575535802170634 Validation loss 0.05018288269639015 Accuracy 0.8790000081062317\n",
      "Iteration 11140 Training loss 0.0022563496604561806 Validation loss 0.05017916113138199 Accuracy 0.8784999847412109\n",
      "Iteration 11150 Training loss 0.0015066605992615223 Validation loss 0.05017591267824173 Accuracy 0.8784999847412109\n",
      "Iteration 11160 Training loss 0.0022570632863789797 Validation loss 0.05018232762813568 Accuracy 0.8784999847412109\n",
      "Iteration 11170 Training loss 0.0010066215181723237 Validation loss 0.05018337070941925 Accuracy 0.8784999847412109\n",
      "Iteration 11180 Training loss 0.0022568644490092993 Validation loss 0.05017906799912453 Accuracy 0.8784999847412109\n",
      "Iteration 11190 Training loss 0.001756056328304112 Validation loss 0.05018548667430878 Accuracy 0.8790000081062317\n",
      "Iteration 11200 Training loss 0.0027567849028855562 Validation loss 0.0501735582947731 Accuracy 0.8790000081062317\n",
      "Iteration 11210 Training loss 0.002256831619888544 Validation loss 0.050191864371299744 Accuracy 0.8790000081062317\n",
      "Iteration 11220 Training loss 0.0017565269954502583 Validation loss 0.05018141493201256 Accuracy 0.8790000081062317\n",
      "Iteration 11230 Training loss 0.001506700529716909 Validation loss 0.05017310008406639 Accuracy 0.8790000081062317\n",
      "Iteration 11240 Training loss 0.0020067510195076466 Validation loss 0.05018506571650505 Accuracy 0.8784999847412109\n",
      "Iteration 11250 Training loss 0.0017570382915437222 Validation loss 0.050192102789878845 Accuracy 0.8790000081062317\n",
      "Iteration 11260 Training loss 0.0010074099991470575 Validation loss 0.05019097030162811 Accuracy 0.8790000081062317\n",
      "Iteration 11270 Training loss 0.0017562428256496787 Validation loss 0.05019375681877136 Accuracy 0.8790000081062317\n",
      "Iteration 11280 Training loss 0.0015060757286846638 Validation loss 0.050198376178741455 Accuracy 0.8790000081062317\n",
      "Iteration 11290 Training loss 0.002256921958178282 Validation loss 0.05019744858145714 Accuracy 0.8784999847412109\n",
      "Iteration 11300 Training loss 0.002755690598860383 Validation loss 0.0501907654106617 Accuracy 0.8784999847412109\n",
      "Iteration 11310 Training loss 0.002756677567958832 Validation loss 0.05019807443022728 Accuracy 0.8790000081062317\n",
      "Iteration 11320 Training loss 0.0010069208219647408 Validation loss 0.05020216852426529 Accuracy 0.8784999847412109\n",
      "Iteration 11330 Training loss 0.0012563435593619943 Validation loss 0.05020938441157341 Accuracy 0.8784999847412109\n",
      "Iteration 11340 Training loss 0.002007209463045001 Validation loss 0.05018620565533638 Accuracy 0.8784999847412109\n",
      "Iteration 11350 Training loss 0.0020063291303813457 Validation loss 0.050191909074783325 Accuracy 0.8784999847412109\n",
      "Iteration 11360 Training loss 0.002006090944632888 Validation loss 0.05019161105155945 Accuracy 0.8784999847412109\n",
      "Iteration 11370 Training loss 0.0010068283881992102 Validation loss 0.05019598454236984 Accuracy 0.8784999847412109\n",
      "Iteration 11380 Training loss 0.003006875980645418 Validation loss 0.05020057410001755 Accuracy 0.8784999847412109\n",
      "Iteration 11390 Training loss 0.002256757812574506 Validation loss 0.05019937455654144 Accuracy 0.8784999847412109\n",
      "Iteration 11400 Training loss 0.0022566590923815966 Validation loss 0.050213638693094254 Accuracy 0.8784999847412109\n",
      "Iteration 11410 Training loss 0.002507186261937022 Validation loss 0.05022451654076576 Accuracy 0.878000020980835\n",
      "Iteration 11420 Training loss 0.0012563432101160288 Validation loss 0.05021713674068451 Accuracy 0.8790000081062317\n",
      "Iteration 11430 Training loss 0.0030066773761063814 Validation loss 0.05020837485790253 Accuracy 0.8784999847412109\n",
      "Iteration 11440 Training loss 0.0017560756532475352 Validation loss 0.05020489543676376 Accuracy 0.8784999847412109\n",
      "Iteration 11450 Training loss 0.0017561918357387185 Validation loss 0.050215668976306915 Accuracy 0.8784999847412109\n",
      "Iteration 11460 Training loss 0.0015066538471728563 Validation loss 0.05021040141582489 Accuracy 0.8784999847412109\n",
      "Iteration 11470 Training loss 0.002256183885037899 Validation loss 0.05021433159708977 Accuracy 0.8784999847412109\n",
      "Iteration 11480 Training loss 0.0012573101557791233 Validation loss 0.05021420493721962 Accuracy 0.8784999847412109\n",
      "Iteration 11490 Training loss 0.002256161067634821 Validation loss 0.05021185427904129 Accuracy 0.8784999847412109\n",
      "Iteration 11500 Training loss 0.0007572030881419778 Validation loss 0.050212085247039795 Accuracy 0.8784999847412109\n",
      "Iteration 11510 Training loss 0.0012567943194881082 Validation loss 0.05020499974489212 Accuracy 0.878000020980835\n",
      "Iteration 11520 Training loss 0.00275686988607049 Validation loss 0.05020740628242493 Accuracy 0.878000020980835\n",
      "Iteration 11530 Training loss 0.0025064421351999044 Validation loss 0.05021869018673897 Accuracy 0.8784999847412109\n",
      "Iteration 11540 Training loss 0.0010065911337733269 Validation loss 0.05021822452545166 Accuracy 0.8784999847412109\n",
      "Iteration 11550 Training loss 0.0022561592049896717 Validation loss 0.050222668796777725 Accuracy 0.8784999847412109\n",
      "Iteration 11560 Training loss 0.00200627651065588 Validation loss 0.05021172761917114 Accuracy 0.878000020980835\n",
      "Iteration 11570 Training loss 0.00300681684166193 Validation loss 0.05020761862397194 Accuracy 0.878000020980835\n",
      "Iteration 11580 Training loss 0.002505536889657378 Validation loss 0.05021849647164345 Accuracy 0.878000020980835\n",
      "Iteration 11590 Training loss 0.0015066135674715042 Validation loss 0.05022144690155983 Accuracy 0.8784999847412109\n",
      "Iteration 11600 Training loss 0.0027575483545660973 Validation loss 0.0502299964427948 Accuracy 0.8784999847412109\n",
      "Iteration 11610 Training loss 0.00225621135905385 Validation loss 0.05023001506924629 Accuracy 0.8784999847412109\n",
      "Iteration 11620 Training loss 0.003005673410370946 Validation loss 0.050223808735609055 Accuracy 0.8784999847412109\n",
      "Iteration 11630 Training loss 0.002755837980657816 Validation loss 0.050235237926244736 Accuracy 0.8784999847412109\n",
      "Iteration 11640 Training loss 0.002756432630121708 Validation loss 0.050224438309669495 Accuracy 0.8784999847412109\n",
      "Iteration 11650 Training loss 0.002506156684830785 Validation loss 0.05023587495088577 Accuracy 0.8784999847412109\n",
      "Iteration 11660 Training loss 0.003005673410370946 Validation loss 0.05024104192852974 Accuracy 0.8784999847412109\n",
      "Iteration 11670 Training loss 0.00225640251301229 Validation loss 0.050234321504831314 Accuracy 0.8784999847412109\n",
      "Iteration 11680 Training loss 0.0017571873031556606 Validation loss 0.050227731466293335 Accuracy 0.8784999847412109\n",
      "Iteration 11690 Training loss 0.0012566238874569535 Validation loss 0.050239551812410355 Accuracy 0.8784999847412109\n",
      "Iteration 11700 Training loss 0.002006066730245948 Validation loss 0.050224147737026215 Accuracy 0.8784999847412109\n",
      "Iteration 11710 Training loss 0.0015061063459143043 Validation loss 0.05023200809955597 Accuracy 0.8784999847412109\n",
      "Iteration 11720 Training loss 0.0017556434031575918 Validation loss 0.05023784562945366 Accuracy 0.8784999847412109\n",
      "Iteration 11730 Training loss 0.0020064685959368944 Validation loss 0.05024002119898796 Accuracy 0.8784999847412109\n",
      "Iteration 11740 Training loss 0.002755444496870041 Validation loss 0.05023296922445297 Accuracy 0.8784999847412109\n",
      "Iteration 11750 Training loss 0.002506363671272993 Validation loss 0.05023631453514099 Accuracy 0.8784999847412109\n",
      "Iteration 11760 Training loss 0.00225670263171196 Validation loss 0.05024228245019913 Accuracy 0.8784999847412109\n",
      "Iteration 11770 Training loss 0.0020055552013218403 Validation loss 0.05022341385483742 Accuracy 0.878000020980835\n",
      "Iteration 11780 Training loss 0.0017560252454131842 Validation loss 0.05022271350026131 Accuracy 0.8784999847412109\n",
      "Iteration 11790 Training loss 0.001256067887879908 Validation loss 0.050225578248500824 Accuracy 0.8784999847412109\n",
      "Iteration 11800 Training loss 0.0010060605127364397 Validation loss 0.05023347586393356 Accuracy 0.8784999847412109\n",
      "Iteration 11810 Training loss 0.001256386749446392 Validation loss 0.050225164741277695 Accuracy 0.8784999847412109\n",
      "Iteration 11820 Training loss 0.0012565292418003082 Validation loss 0.05024319514632225 Accuracy 0.8784999847412109\n",
      "Iteration 11830 Training loss 0.0015057821292430162 Validation loss 0.05023737996816635 Accuracy 0.8784999847412109\n",
      "Iteration 11840 Training loss 0.002005808288231492 Validation loss 0.050229430198669434 Accuracy 0.8784999847412109\n",
      "Iteration 11850 Training loss 0.002006381982937455 Validation loss 0.050227973610162735 Accuracy 0.8790000081062317\n",
      "Iteration 11860 Training loss 0.0022567848209291697 Validation loss 0.05023081228137016 Accuracy 0.8784999847412109\n",
      "Iteration 11870 Training loss 0.0017557914834469557 Validation loss 0.050230249762535095 Accuracy 0.8784999847412109\n",
      "Iteration 11880 Training loss 0.0025057338643819094 Validation loss 0.05023578554391861 Accuracy 0.8784999847412109\n",
      "Iteration 11890 Training loss 0.002006539376452565 Validation loss 0.05024712532758713 Accuracy 0.8784999847412109\n",
      "Iteration 11900 Training loss 0.0015064476756379008 Validation loss 0.05025695264339447 Accuracy 0.8784999847412109\n",
      "Iteration 11910 Training loss 0.003005321603268385 Validation loss 0.0502588227391243 Accuracy 0.8784999847412109\n",
      "Iteration 11920 Training loss 0.0007561943493783474 Validation loss 0.05025959759950638 Accuracy 0.8784999847412109\n",
      "Iteration 11930 Training loss 0.0025066316593438387 Validation loss 0.05025142803788185 Accuracy 0.8784999847412109\n",
      "Iteration 11940 Training loss 0.0017567829927429557 Validation loss 0.05024724826216698 Accuracy 0.8784999847412109\n",
      "Iteration 11950 Training loss 0.0025059052277356386 Validation loss 0.05023634433746338 Accuracy 0.8784999847412109\n",
      "Iteration 11960 Training loss 0.002506186719983816 Validation loss 0.05024583265185356 Accuracy 0.8784999847412109\n",
      "Iteration 11970 Training loss 0.0012566122459247708 Validation loss 0.050238508731126785 Accuracy 0.8784999847412109\n",
      "Iteration 11980 Training loss 0.0025063790380954742 Validation loss 0.05024275183677673 Accuracy 0.8784999847412109\n",
      "Iteration 11990 Training loss 0.002005744958296418 Validation loss 0.05024508386850357 Accuracy 0.8784999847412109\n",
      "Iteration 12000 Training loss 0.002255511935800314 Validation loss 0.05024291202425957 Accuracy 0.8784999847412109\n",
      "Iteration 12010 Training loss 0.0017556465463712811 Validation loss 0.05024547502398491 Accuracy 0.8784999847412109\n",
      "Iteration 12020 Training loss 0.002505110576748848 Validation loss 0.05024217441678047 Accuracy 0.8784999847412109\n",
      "Iteration 12030 Training loss 0.002755464054644108 Validation loss 0.05024592950940132 Accuracy 0.8784999847412109\n",
      "Iteration 12040 Training loss 0.0017558789113536477 Validation loss 0.05025562271475792 Accuracy 0.8790000081062317\n",
      "Iteration 12050 Training loss 0.0020061801187694073 Validation loss 0.050245627760887146 Accuracy 0.8784999847412109\n",
      "Iteration 12060 Training loss 0.0015062150778248906 Validation loss 0.050242651253938675 Accuracy 0.878000020980835\n",
      "Iteration 12070 Training loss 0.0030056312680244446 Validation loss 0.050246600061655045 Accuracy 0.878000020980835\n",
      "Iteration 12080 Training loss 0.0017555711092427373 Validation loss 0.05025974288582802 Accuracy 0.8784999847412109\n",
      "Iteration 12090 Training loss 0.0012558054877445102 Validation loss 0.050253741443157196 Accuracy 0.8784999847412109\n",
      "Iteration 12100 Training loss 0.0022561841178685427 Validation loss 0.05025644600391388 Accuracy 0.8784999847412109\n",
      "Iteration 12110 Training loss 0.002006212715059519 Validation loss 0.05026620626449585 Accuracy 0.8784999847412109\n",
      "Iteration 12120 Training loss 0.001506792032159865 Validation loss 0.050250258296728134 Accuracy 0.878000020980835\n",
      "Iteration 12130 Training loss 0.0017554272199049592 Validation loss 0.0502457395195961 Accuracy 0.878000020980835\n",
      "Iteration 12140 Training loss 0.0015059607103466988 Validation loss 0.05026484280824661 Accuracy 0.8784999847412109\n",
      "Iteration 12150 Training loss 0.002005617134273052 Validation loss 0.05027354508638382 Accuracy 0.8784999847412109\n",
      "Iteration 12160 Training loss 0.002756107598543167 Validation loss 0.05026838555932045 Accuracy 0.8784999847412109\n",
      "Iteration 12170 Training loss 0.0020061845425516367 Validation loss 0.05026658624410629 Accuracy 0.8784999847412109\n",
      "Iteration 12180 Training loss 0.0017562852008268237 Validation loss 0.05026141554117203 Accuracy 0.8784999847412109\n",
      "Iteration 12190 Training loss 0.0027556747663766146 Validation loss 0.05026102066040039 Accuracy 0.8784999847412109\n",
      "Iteration 12200 Training loss 0.0017561378190293908 Validation loss 0.050253111869096756 Accuracy 0.8784999847412109\n",
      "Iteration 12210 Training loss 0.0017556987004354596 Validation loss 0.05025080218911171 Accuracy 0.8784999847412109\n",
      "Iteration 12220 Training loss 0.0017562760040163994 Validation loss 0.05026570335030556 Accuracy 0.8784999847412109\n",
      "Iteration 12230 Training loss 0.0017560658743605018 Validation loss 0.05026625096797943 Accuracy 0.8784999847412109\n",
      "Iteration 12240 Training loss 0.0012552924454212189 Validation loss 0.05026821792125702 Accuracy 0.8784999847412109\n",
      "Iteration 12250 Training loss 0.002755486872047186 Validation loss 0.05025767907500267 Accuracy 0.8784999847412109\n",
      "Iteration 12260 Training loss 0.0020048110745847225 Validation loss 0.05026373267173767 Accuracy 0.8784999847412109\n",
      "Iteration 12270 Training loss 0.001755599630996585 Validation loss 0.05027223378419876 Accuracy 0.8784999847412109\n",
      "Iteration 12280 Training loss 0.0022560690995305777 Validation loss 0.050283052027225494 Accuracy 0.8784999847412109\n",
      "Iteration 12290 Training loss 0.002006365219131112 Validation loss 0.0502806156873703 Accuracy 0.8784999847412109\n",
      "Iteration 12300 Training loss 0.0020056322682648897 Validation loss 0.05027810484170914 Accuracy 0.8784999847412109\n",
      "Iteration 12310 Training loss 0.0015058558201417327 Validation loss 0.05026663839817047 Accuracy 0.878000020980835\n",
      "Iteration 12320 Training loss 0.003005483653396368 Validation loss 0.050277527421712875 Accuracy 0.8784999847412109\n",
      "Iteration 12330 Training loss 0.002755900612100959 Validation loss 0.05027080699801445 Accuracy 0.878000020980835\n",
      "Iteration 12340 Training loss 0.0017554182559251785 Validation loss 0.05028165504336357 Accuracy 0.8784999847412109\n",
      "Iteration 12350 Training loss 0.0027562608011066914 Validation loss 0.050273217260837555 Accuracy 0.8784999847412109\n",
      "Iteration 12360 Training loss 0.00225508539006114 Validation loss 0.05029316991567612 Accuracy 0.8784999847412109\n",
      "Iteration 12370 Training loss 0.002005401998758316 Validation loss 0.05028504133224487 Accuracy 0.8784999847412109\n",
      "Iteration 12380 Training loss 0.002256532898172736 Validation loss 0.05029197782278061 Accuracy 0.8784999847412109\n",
      "Iteration 12390 Training loss 0.0017554312944412231 Validation loss 0.050301555544137955 Accuracy 0.8784999847412109\n",
      "Iteration 12400 Training loss 0.0022559063509106636 Validation loss 0.05028711259365082 Accuracy 0.8784999847412109\n",
      "Iteration 12410 Training loss 0.0015047758352011442 Validation loss 0.050292618572711945 Accuracy 0.8784999847412109\n",
      "Iteration 12420 Training loss 0.002254895865917206 Validation loss 0.050287459045648575 Accuracy 0.878000020980835\n",
      "Iteration 12430 Training loss 0.0012553895357996225 Validation loss 0.05030159652233124 Accuracy 0.8784999847412109\n",
      "Iteration 12440 Training loss 0.00200631539337337 Validation loss 0.05028581991791725 Accuracy 0.878000020980835\n",
      "Iteration 12450 Training loss 0.0017556226812303066 Validation loss 0.05029044672846794 Accuracy 0.878000020980835\n",
      "Iteration 12460 Training loss 0.0017565872985869646 Validation loss 0.05028127133846283 Accuracy 0.878000020980835\n",
      "Iteration 12470 Training loss 0.002755549503490329 Validation loss 0.050285469740629196 Accuracy 0.878000020980835\n",
      "Iteration 12480 Training loss 0.0015056324191391468 Validation loss 0.050279803574085236 Accuracy 0.878000020980835\n",
      "Iteration 12490 Training loss 0.0005048739840276539 Validation loss 0.05030350014567375 Accuracy 0.8784999847412109\n",
      "Iteration 12500 Training loss 0.003005228005349636 Validation loss 0.0503089539706707 Accuracy 0.8784999847412109\n",
      "Iteration 12510 Training loss 0.0010056061437353492 Validation loss 0.05030957609415054 Accuracy 0.8784999847412109\n",
      "Iteration 12520 Training loss 0.0027562312316149473 Validation loss 0.050321873277425766 Accuracy 0.8784999847412109\n",
      "Iteration 12530 Training loss 0.0017559140687808394 Validation loss 0.050310730934143066 Accuracy 0.8784999847412109\n",
      "Iteration 12540 Training loss 0.0025051706470549107 Validation loss 0.05031641572713852 Accuracy 0.8784999847412109\n",
      "Iteration 12550 Training loss 0.0005053404020145535 Validation loss 0.05032237991690636 Accuracy 0.8784999847412109\n",
      "Iteration 12560 Training loss 0.000505584292113781 Validation loss 0.05031581223011017 Accuracy 0.8784999847412109\n",
      "Iteration 12570 Training loss 0.002256030449643731 Validation loss 0.05031805485486984 Accuracy 0.8784999847412109\n",
      "Iteration 12580 Training loss 0.001505683409050107 Validation loss 0.050315890461206436 Accuracy 0.8784999847412109\n",
      "Iteration 12590 Training loss 0.0012559606693685055 Validation loss 0.050319645553827286 Accuracy 0.8784999847412109\n",
      "Iteration 12600 Training loss 0.0012558352900668979 Validation loss 0.050323572009801865 Accuracy 0.8784999847412109\n",
      "Iteration 12610 Training loss 0.0020062432158738375 Validation loss 0.050325460731983185 Accuracy 0.8784999847412109\n",
      "Iteration 12620 Training loss 0.0015055672265589237 Validation loss 0.05031951144337654 Accuracy 0.8784999847412109\n",
      "Iteration 12630 Training loss 0.001755487290211022 Validation loss 0.050319816917181015 Accuracy 0.8784999847412109\n",
      "Iteration 12640 Training loss 0.0022548597771674395 Validation loss 0.05032176151871681 Accuracy 0.8784999847412109\n",
      "Iteration 12650 Training loss 0.002506229095160961 Validation loss 0.050319816917181015 Accuracy 0.8784999847412109\n",
      "Iteration 12660 Training loss 0.001755386940203607 Validation loss 0.05031965672969818 Accuracy 0.8784999847412109\n",
      "Iteration 12670 Training loss 0.0027552200481295586 Validation loss 0.05032067000865936 Accuracy 0.878000020980835\n",
      "Iteration 12680 Training loss 0.002505027689039707 Validation loss 0.05032800883054733 Accuracy 0.878000020980835\n",
      "Iteration 12690 Training loss 0.0022557852789759636 Validation loss 0.050323113799095154 Accuracy 0.878000020980835\n",
      "Iteration 12700 Training loss 0.002005805494263768 Validation loss 0.050327446311712265 Accuracy 0.8784999847412109\n",
      "Iteration 12710 Training loss 0.0022561284713447094 Validation loss 0.05033552646636963 Accuracy 0.8784999847412109\n",
      "Iteration 12720 Training loss 0.0012554877903312445 Validation loss 0.0503220409154892 Accuracy 0.878000020980835\n",
      "Iteration 12730 Training loss 0.001755339908413589 Validation loss 0.05032183602452278 Accuracy 0.878000020980835\n",
      "Iteration 12740 Training loss 0.002005617134273052 Validation loss 0.05033717676997185 Accuracy 0.8784999847412109\n",
      "Iteration 12750 Training loss 0.002505301730707288 Validation loss 0.05033492296934128 Accuracy 0.878000020980835\n",
      "Iteration 12760 Training loss 0.0012552818516269326 Validation loss 0.05032630264759064 Accuracy 0.878000020980835\n",
      "Iteration 12770 Training loss 0.0017547591123729944 Validation loss 0.05032632499933243 Accuracy 0.878000020980835\n",
      "Iteration 12780 Training loss 0.0005056929076090455 Validation loss 0.05031545087695122 Accuracy 0.878000020980835\n",
      "Iteration 12790 Training loss 0.0020055987406522036 Validation loss 0.05032458156347275 Accuracy 0.878000020980835\n",
      "Iteration 12800 Training loss 0.0010058743646368384 Validation loss 0.050338659435510635 Accuracy 0.878000020980835\n",
      "Iteration 12810 Training loss 0.0017547340830788016 Validation loss 0.05033828690648079 Accuracy 0.878000020980835\n",
      "Iteration 12820 Training loss 0.0015060043660923839 Validation loss 0.05034593492746353 Accuracy 0.878000020980835\n",
      "Iteration 12830 Training loss 0.002255979459732771 Validation loss 0.05033906549215317 Accuracy 0.878000020980835\n",
      "Iteration 12840 Training loss 0.0032548445742577314 Validation loss 0.05033920332789421 Accuracy 0.878000020980835\n",
      "Iteration 12850 Training loss 0.0010047005489468575 Validation loss 0.050343845039606094 Accuracy 0.878000020980835\n",
      "Iteration 12860 Training loss 0.002504621399566531 Validation loss 0.05034714937210083 Accuracy 0.878000020980835\n",
      "Iteration 12870 Training loss 0.0012561104958876967 Validation loss 0.050350122153759 Accuracy 0.878000020980835\n",
      "Iteration 12880 Training loss 0.0027562303002923727 Validation loss 0.05034429207444191 Accuracy 0.878000020980835\n",
      "Iteration 12890 Training loss 0.0020054816268384457 Validation loss 0.05033182352781296 Accuracy 0.878000020980835\n",
      "Iteration 12900 Training loss 0.0037557792384177446 Validation loss 0.05033530667424202 Accuracy 0.878000020980835\n",
      "Iteration 12910 Training loss 0.001006099279038608 Validation loss 0.050327785313129425 Accuracy 0.878000020980835\n",
      "Iteration 12920 Training loss 0.0010047630639746785 Validation loss 0.05033986642956734 Accuracy 0.878000020980835\n",
      "Iteration 12930 Training loss 0.00150567595846951 Validation loss 0.05032965913414955 Accuracy 0.878000020980835\n",
      "Iteration 12940 Training loss 0.0017551176715642214 Validation loss 0.05032569542527199 Accuracy 0.878000020980835\n",
      "Iteration 12950 Training loss 0.0015058647841215134 Validation loss 0.050334230065345764 Accuracy 0.878000020980835\n",
      "Iteration 12960 Training loss 0.0010060009080916643 Validation loss 0.05033620074391365 Accuracy 0.878000020980835\n",
      "Iteration 12970 Training loss 0.001256152056157589 Validation loss 0.05035465210676193 Accuracy 0.8784999847412109\n",
      "Iteration 12980 Training loss 0.00175615050829947 Validation loss 0.05034618452191353 Accuracy 0.8784999847412109\n",
      "Iteration 12990 Training loss 0.0030054182279855013 Validation loss 0.050326909869909286 Accuracy 0.878000020980835\n",
      "Iteration 13000 Training loss 0.0010050899581983685 Validation loss 0.050323400646448135 Accuracy 0.878000020980835\n",
      "Iteration 13010 Training loss 0.003254422452300787 Validation loss 0.05031069740653038 Accuracy 0.878000020980835\n",
      "Iteration 13020 Training loss 0.0015060282312333584 Validation loss 0.05031396448612213 Accuracy 0.878000020980835\n",
      "Iteration 13030 Training loss 0.0025044893845915794 Validation loss 0.05030859261751175 Accuracy 0.878000020980835\n",
      "Iteration 13040 Training loss 0.0017557252431288362 Validation loss 0.05029851570725441 Accuracy 0.878000020980835\n",
      "Iteration 13050 Training loss 0.0027545285411179066 Validation loss 0.05029907077550888 Accuracy 0.878000020980835\n",
      "Iteration 13060 Training loss 0.0012547193327918649 Validation loss 0.050321683287620544 Accuracy 0.878000020980835\n",
      "Iteration 13070 Training loss 0.0020045877899974585 Validation loss 0.05033525824546814 Accuracy 0.878000020980835\n",
      "Iteration 13080 Training loss 0.0020054513588547707 Validation loss 0.05033252388238907 Accuracy 0.878000020980835\n",
      "Iteration 13090 Training loss 0.002755989320576191 Validation loss 0.05033707991242409 Accuracy 0.878000020980835\n",
      "Iteration 13100 Training loss 0.0010057457257062197 Validation loss 0.05034632980823517 Accuracy 0.878000020980835\n",
      "Iteration 13110 Training loss 0.0022556064650416374 Validation loss 0.05035248398780823 Accuracy 0.878000020980835\n",
      "Iteration 13120 Training loss 0.0010050993878394365 Validation loss 0.05035427585244179 Accuracy 0.878000020980835\n",
      "Iteration 13130 Training loss 0.0027546249330043793 Validation loss 0.05036122351884842 Accuracy 0.878000020980835\n",
      "Iteration 13140 Training loss 0.002505283569917083 Validation loss 0.05035471171140671 Accuracy 0.878000020980835\n",
      "Iteration 13150 Training loss 0.0012555475113913417 Validation loss 0.0503501333296299 Accuracy 0.878000020980835\n",
      "Iteration 13160 Training loss 0.001505964552052319 Validation loss 0.05034545436501503 Accuracy 0.8784999847412109\n",
      "Iteration 13170 Training loss 0.002754839137196541 Validation loss 0.05034574121236801 Accuracy 0.8784999847412109\n",
      "Iteration 13180 Training loss 0.002255704952403903 Validation loss 0.05033974349498749 Accuracy 0.878000020980835\n",
      "Iteration 13190 Training loss 0.001755289500579238 Validation loss 0.05033809691667557 Accuracy 0.878000020980835\n",
      "Iteration 13200 Training loss 0.0017556245438754559 Validation loss 0.05033479258418083 Accuracy 0.878000020980835\n",
      "Iteration 13210 Training loss 0.00200529582798481 Validation loss 0.05033878982067108 Accuracy 0.878000020980835\n",
      "Iteration 13220 Training loss 0.0022555957548320293 Validation loss 0.050340376794338226 Accuracy 0.878000020980835\n",
      "Iteration 13230 Training loss 0.0037548085674643517 Validation loss 0.05035140737891197 Accuracy 0.878000020980835\n",
      "Iteration 13240 Training loss 0.0020053915213793516 Validation loss 0.05034404993057251 Accuracy 0.878000020980835\n",
      "Iteration 13250 Training loss 0.0022554683964699507 Validation loss 0.05033627897500992 Accuracy 0.878000020980835\n",
      "Iteration 13260 Training loss 0.001505840104073286 Validation loss 0.05032818764448166 Accuracy 0.878000020980835\n",
      "Iteration 13270 Training loss 0.0030047884210944176 Validation loss 0.050338003784418106 Accuracy 0.878000020980835\n",
      "Iteration 13280 Training loss 0.0012554812710732222 Validation loss 0.05034314841032028 Accuracy 0.878000020980835\n",
      "Iteration 13290 Training loss 0.00225443416275084 Validation loss 0.05034676566720009 Accuracy 0.878000020980835\n",
      "Iteration 13300 Training loss 0.0017545083537697792 Validation loss 0.050348278135061264 Accuracy 0.878000020980835\n",
      "Iteration 13310 Training loss 0.0020060339011251926 Validation loss 0.05034555122256279 Accuracy 0.878000020980835\n",
      "Iteration 13320 Training loss 0.0015053169336169958 Validation loss 0.05033954232931137 Accuracy 0.878000020980835\n",
      "Iteration 13330 Training loss 0.002005398040637374 Validation loss 0.050341106951236725 Accuracy 0.878000020980835\n",
      "Iteration 13340 Training loss 0.002754945307970047 Validation loss 0.05035395920276642 Accuracy 0.878000020980835\n",
      "Iteration 13350 Training loss 0.0012552407570183277 Validation loss 0.050343941897153854 Accuracy 0.878000020980835\n",
      "Iteration 13360 Training loss 0.0017556104576215148 Validation loss 0.05034782364964485 Accuracy 0.878000020980835\n",
      "Iteration 13370 Training loss 0.0020054688211530447 Validation loss 0.05035943165421486 Accuracy 0.878000020980835\n",
      "Iteration 13380 Training loss 0.003004848724231124 Validation loss 0.05035422369837761 Accuracy 0.878000020980835\n",
      "Iteration 13390 Training loss 0.002255834639072418 Validation loss 0.05036594346165657 Accuracy 0.8784999847412109\n",
      "Iteration 13400 Training loss 0.002754409331828356 Validation loss 0.050360120832920074 Accuracy 0.8784999847412109\n",
      "Iteration 13410 Training loss 0.0022545726969838142 Validation loss 0.05035528540611267 Accuracy 0.878000020980835\n",
      "Iteration 13420 Training loss 0.0022561424411833286 Validation loss 0.050360679626464844 Accuracy 0.8784999847412109\n",
      "Iteration 13430 Training loss 0.0010057080071419477 Validation loss 0.05036137253046036 Accuracy 0.8784999847412109\n",
      "Iteration 13440 Training loss 0.00175534060690552 Validation loss 0.0503469742834568 Accuracy 0.878000020980835\n",
      "Iteration 13450 Training loss 0.002504748059436679 Validation loss 0.05034760385751724 Accuracy 0.878000020980835\n",
      "Iteration 13460 Training loss 0.002504922216758132 Validation loss 0.050355810672044754 Accuracy 0.878000020980835\n",
      "Iteration 13470 Training loss 0.0017557734390720725 Validation loss 0.05034825578331947 Accuracy 0.878000020980835\n",
      "Iteration 13480 Training loss 0.0035051084123551846 Validation loss 0.0503612756729126 Accuracy 0.878000020980835\n",
      "Iteration 13490 Training loss 0.002754993038251996 Validation loss 0.050368666648864746 Accuracy 0.878000020980835\n",
      "Iteration 13500 Training loss 0.002504219999536872 Validation loss 0.05037939175963402 Accuracy 0.878000020980835\n",
      "Iteration 13510 Training loss 0.0035043777897953987 Validation loss 0.05037751421332359 Accuracy 0.878000020980835\n",
      "Iteration 13520 Training loss 0.0015052290400490165 Validation loss 0.0503728874027729 Accuracy 0.878000020980835\n",
      "Iteration 13530 Training loss 0.0020049873273819685 Validation loss 0.05037160962820053 Accuracy 0.878000020980835\n",
      "Iteration 13540 Training loss 0.0020054830238223076 Validation loss 0.05037860944867134 Accuracy 0.878000020980835\n",
      "Iteration 13550 Training loss 0.0017552871722728014 Validation loss 0.05038786306977272 Accuracy 0.878000020980835\n",
      "Iteration 13560 Training loss 0.002505175769329071 Validation loss 0.05038287490606308 Accuracy 0.878000020980835\n",
      "Iteration 13570 Training loss 0.0025051801931113005 Validation loss 0.05037596821784973 Accuracy 0.878000020980835\n",
      "Iteration 13580 Training loss 0.0015044170431792736 Validation loss 0.05037846788764 Accuracy 0.878000020980835\n",
      "Iteration 13590 Training loss 0.0015054796822369099 Validation loss 0.050372399389743805 Accuracy 0.878000020980835\n",
      "Iteration 13600 Training loss 0.0020050925668329 Validation loss 0.05037745088338852 Accuracy 0.878000020980835\n",
      "Iteration 13610 Training loss 0.001255021896213293 Validation loss 0.05036735534667969 Accuracy 0.878000020980835\n",
      "Iteration 13620 Training loss 0.0020048501901328564 Validation loss 0.05037549510598183 Accuracy 0.878000020980835\n",
      "Iteration 13630 Training loss 0.0022547345142811537 Validation loss 0.05038376525044441 Accuracy 0.878000020980835\n",
      "Iteration 13640 Training loss 0.001755152246914804 Validation loss 0.050374194979667664 Accuracy 0.878000020980835\n",
      "Iteration 13650 Training loss 0.0030038885306566954 Validation loss 0.050381213426589966 Accuracy 0.878000020980835\n",
      "Iteration 13660 Training loss 0.002504716394469142 Validation loss 0.05038854107260704 Accuracy 0.878000020980835\n",
      "Iteration 13670 Training loss 0.001755345962010324 Validation loss 0.05039184167981148 Accuracy 0.878000020980835\n",
      "Iteration 13680 Training loss 0.0010038104373961687 Validation loss 0.05037597939372063 Accuracy 0.878000020980835\n",
      "Iteration 13690 Training loss 0.0022548839915543795 Validation loss 0.05038227513432503 Accuracy 0.878000020980835\n",
      "Iteration 13700 Training loss 0.001505424501374364 Validation loss 0.050384651869535446 Accuracy 0.878000020980835\n",
      "Iteration 13710 Training loss 0.0010044138180091977 Validation loss 0.050377458333969116 Accuracy 0.878000020980835\n",
      "Iteration 13720 Training loss 0.0020050585735589266 Validation loss 0.05038028582930565 Accuracy 0.878000020980835\n",
      "Iteration 13730 Training loss 0.0022551454603672028 Validation loss 0.05037245154380798 Accuracy 0.878000020980835\n",
      "Iteration 13740 Training loss 0.0017555298982188106 Validation loss 0.0503801628947258 Accuracy 0.878000020980835\n",
      "Iteration 13750 Training loss 0.0005050683976151049 Validation loss 0.05036839470267296 Accuracy 0.878000020980835\n",
      "Iteration 13760 Training loss 0.0012547204969450831 Validation loss 0.05037405341863632 Accuracy 0.878000020980835\n",
      "Iteration 13770 Training loss 0.0017548322211951017 Validation loss 0.050387024879455566 Accuracy 0.878000020980835\n",
      "Iteration 13780 Training loss 0.0015037641860544682 Validation loss 0.05038785561919212 Accuracy 0.878000020980835\n",
      "Iteration 13790 Training loss 0.002255083294585347 Validation loss 0.050386693328619 Accuracy 0.878000020980835\n",
      "Iteration 13800 Training loss 0.0025041040498763323 Validation loss 0.050381138920784 Accuracy 0.878000020980835\n",
      "Iteration 13810 Training loss 0.0010046720271930099 Validation loss 0.050380367785692215 Accuracy 0.878000020980835\n",
      "Iteration 13820 Training loss 0.002255292609333992 Validation loss 0.05037125572562218 Accuracy 0.878000020980835\n",
      "Iteration 13830 Training loss 0.0017553652869537473 Validation loss 0.050379592925310135 Accuracy 0.878000020980835\n",
      "Iteration 13840 Training loss 0.0017549728509038687 Validation loss 0.050385162234306335 Accuracy 0.878000020980835\n",
      "Iteration 13850 Training loss 0.0020044578704982996 Validation loss 0.050390053540468216 Accuracy 0.878000020980835\n",
      "Iteration 13860 Training loss 0.0010046029929071665 Validation loss 0.05039076507091522 Accuracy 0.878000020980835\n",
      "Iteration 13870 Training loss 0.0027538456488400698 Validation loss 0.05037832632660866 Accuracy 0.878000020980835\n",
      "Iteration 13880 Training loss 0.002254669088870287 Validation loss 0.05040012300014496 Accuracy 0.878000020980835\n",
      "Iteration 13890 Training loss 0.0012543491320684552 Validation loss 0.05039634555578232 Accuracy 0.878000020980835\n",
      "Iteration 13900 Training loss 0.0012551690451800823 Validation loss 0.050397083163261414 Accuracy 0.878000020980835\n",
      "Iteration 13910 Training loss 0.003253659699112177 Validation loss 0.05039045214653015 Accuracy 0.878000020980835\n",
      "Iteration 13920 Training loss 0.002004496520385146 Validation loss 0.05038880184292793 Accuracy 0.878000020980835\n",
      "Iteration 13930 Training loss 0.0025047354865819216 Validation loss 0.050389084964990616 Accuracy 0.878000020980835\n",
      "Iteration 13940 Training loss 0.0012552774278447032 Validation loss 0.0503888837993145 Accuracy 0.878000020980835\n",
      "Iteration 13950 Training loss 0.002254737075418234 Validation loss 0.050388507544994354 Accuracy 0.878000020980835\n",
      "Iteration 13960 Training loss 0.001004655146971345 Validation loss 0.05039632320404053 Accuracy 0.878000020980835\n",
      "Iteration 13970 Training loss 0.00175471103284508 Validation loss 0.05039440467953682 Accuracy 0.878000020980835\n",
      "Iteration 13980 Training loss 0.0017544094007462263 Validation loss 0.05039459094405174 Accuracy 0.878000020980835\n",
      "Iteration 13990 Training loss 0.002504869597032666 Validation loss 0.05039488151669502 Accuracy 0.878000020980835\n",
      "Iteration 14000 Training loss 0.0027547574136406183 Validation loss 0.05038854852318764 Accuracy 0.878000020980835\n",
      "Iteration 14010 Training loss 0.0020049738232046366 Validation loss 0.050385285168886185 Accuracy 0.878000020980835\n",
      "Iteration 14020 Training loss 0.0005050753825344145 Validation loss 0.05039268359541893 Accuracy 0.878000020980835\n",
      "Iteration 14030 Training loss 0.0020049195736646652 Validation loss 0.050392087548971176 Accuracy 0.878000020980835\n",
      "Iteration 14040 Training loss 0.0017551588825881481 Validation loss 0.05038991570472717 Accuracy 0.878000020980835\n",
      "Iteration 14050 Training loss 0.0030039448756724596 Validation loss 0.05040046200156212 Accuracy 0.878000020980835\n",
      "Iteration 14060 Training loss 0.002254801569506526 Validation loss 0.050397034734487534 Accuracy 0.878000020980835\n",
      "Iteration 14070 Training loss 0.0027548447251319885 Validation loss 0.05040280520915985 Accuracy 0.878000020980835\n",
      "Iteration 14080 Training loss 0.002004863228648901 Validation loss 0.05040707066655159 Accuracy 0.878000020980835\n",
      "Iteration 14090 Training loss 0.0017553884536027908 Validation loss 0.05040326714515686 Accuracy 0.878000020980835\n",
      "Iteration 14100 Training loss 0.0015048384666442871 Validation loss 0.05039627104997635 Accuracy 0.878000020980835\n",
      "Iteration 14110 Training loss 0.0035042748786509037 Validation loss 0.05039552226662636 Accuracy 0.878000020980835\n",
      "Iteration 14120 Training loss 0.0020045535638928413 Validation loss 0.050394732505083084 Accuracy 0.878000020980835\n",
      "Iteration 14130 Training loss 0.0012554372660815716 Validation loss 0.05039670690894127 Accuracy 0.878000020980835\n",
      "Iteration 14140 Training loss 0.0027544009499251842 Validation loss 0.050382379442453384 Accuracy 0.878000020980835\n",
      "Iteration 14150 Training loss 0.0012537951115518808 Validation loss 0.05039571225643158 Accuracy 0.878000020980835\n",
      "Iteration 14160 Training loss 0.001255399314686656 Validation loss 0.05040761083364487 Accuracy 0.878000020980835\n",
      "Iteration 14170 Training loss 0.002003635512664914 Validation loss 0.050410933792591095 Accuracy 0.878000020980835\n",
      "Iteration 14180 Training loss 0.0030036387033760548 Validation loss 0.050406090915203094 Accuracy 0.878000020980835\n",
      "Iteration 14190 Training loss 0.0015049666399136186 Validation loss 0.050400037318468094 Accuracy 0.878000020980835\n",
      "Iteration 14200 Training loss 0.0015048894565552473 Validation loss 0.050395745784044266 Accuracy 0.878000020980835\n",
      "Iteration 14210 Training loss 0.002755749737843871 Validation loss 0.050394054502248764 Accuracy 0.878000020980835\n",
      "Iteration 14220 Training loss 0.0030050810892134905 Validation loss 0.05040491372346878 Accuracy 0.878000020980835\n",
      "Iteration 14230 Training loss 0.001504693180322647 Validation loss 0.05039595812559128 Accuracy 0.878000020980835\n",
      "Iteration 14240 Training loss 0.002504082163795829 Validation loss 0.050397783517837524 Accuracy 0.878000020980835\n",
      "Iteration 14250 Training loss 0.0030056822579354048 Validation loss 0.05039606988430023 Accuracy 0.878000020980835\n",
      "Iteration 14260 Training loss 0.0025050658732652664 Validation loss 0.05039551481604576 Accuracy 0.878000020980835\n",
      "Iteration 14270 Training loss 0.001005179830826819 Validation loss 0.050392456352710724 Accuracy 0.878000020980835\n",
      "Iteration 14280 Training loss 0.002005241112783551 Validation loss 0.05040533468127251 Accuracy 0.878000020980835\n",
      "Iteration 14290 Training loss 0.001005246420390904 Validation loss 0.05040504410862923 Accuracy 0.878000020980835\n",
      "Iteration 14300 Training loss 0.0015048969071358442 Validation loss 0.05042370408773422 Accuracy 0.878000020980835\n",
      "Iteration 14310 Training loss 0.002254621358588338 Validation loss 0.05042731389403343 Accuracy 0.878000020980835\n",
      "Iteration 14320 Training loss 0.0022533100564032793 Validation loss 0.05040920525789261 Accuracy 0.878000020980835\n",
      "Iteration 14330 Training loss 0.0022547522094100714 Validation loss 0.05041172355413437 Accuracy 0.878000020980835\n",
      "Iteration 14340 Training loss 0.0020044127013534307 Validation loss 0.050403840839862823 Accuracy 0.878000020980835\n",
      "Iteration 14350 Training loss 0.001504819723777473 Validation loss 0.05041074380278587 Accuracy 0.878000020980835\n",
      "Iteration 14360 Training loss 0.0017537253443151712 Validation loss 0.05041223019361496 Accuracy 0.878000020980835\n",
      "Iteration 14370 Training loss 0.001754791010171175 Validation loss 0.050410106778144836 Accuracy 0.878000020980835\n",
      "Iteration 14380 Training loss 0.0030034398660063744 Validation loss 0.0504087470471859 Accuracy 0.878000020980835\n",
      "Iteration 14390 Training loss 0.0022532406728714705 Validation loss 0.05041198804974556 Accuracy 0.878000020980835\n",
      "Iteration 14400 Training loss 0.002254076302051544 Validation loss 0.05040738731622696 Accuracy 0.878000020980835\n",
      "Iteration 14410 Training loss 0.001754545490257442 Validation loss 0.050411514937877655 Accuracy 0.878000020980835\n",
      "Iteration 14420 Training loss 0.0017542182467877865 Validation loss 0.0504138357937336 Accuracy 0.878000020980835\n",
      "Iteration 14430 Training loss 0.0020054024644196033 Validation loss 0.0504138208925724 Accuracy 0.878000020980835\n",
      "Iteration 14440 Training loss 0.002253463724628091 Validation loss 0.050411079078912735 Accuracy 0.878000020980835\n",
      "Iteration 14450 Training loss 0.0027547976933419704 Validation loss 0.050413746386766434 Accuracy 0.878000020980835\n",
      "Iteration 14460 Training loss 0.0010046742390841246 Validation loss 0.05042006075382233 Accuracy 0.878000020980835\n",
      "Iteration 14470 Training loss 0.0010053586447611451 Validation loss 0.050419967621564865 Accuracy 0.878000020980835\n",
      "Iteration 14480 Training loss 0.0017546183662489057 Validation loss 0.05041130632162094 Accuracy 0.878000020980835\n",
      "Iteration 14490 Training loss 0.002004878129810095 Validation loss 0.05041802302002907 Accuracy 0.878000020980835\n",
      "Iteration 14500 Training loss 0.001753601129166782 Validation loss 0.05041505768895149 Accuracy 0.878000020980835\n",
      "Iteration 14510 Training loss 0.0020057240035384893 Validation loss 0.050420600920915604 Accuracy 0.878000020980835\n",
      "Iteration 14520 Training loss 0.002254392486065626 Validation loss 0.05042639002203941 Accuracy 0.878000020980835\n",
      "Iteration 14530 Training loss 0.0015050559304654598 Validation loss 0.05042972415685654 Accuracy 0.878000020980835\n",
      "Iteration 14540 Training loss 0.0030025900341570377 Validation loss 0.05043374001979828 Accuracy 0.878000020980835\n",
      "Iteration 14550 Training loss 0.00250431289896369 Validation loss 0.05043419450521469 Accuracy 0.878000020980835\n",
      "Iteration 14560 Training loss 0.0010051154531538486 Validation loss 0.05043394863605499 Accuracy 0.878000020980835\n",
      "Iteration 14570 Training loss 0.0017550623742863536 Validation loss 0.05042458325624466 Accuracy 0.878000020980835\n",
      "Iteration 14580 Training loss 0.001755107776261866 Validation loss 0.05042659118771553 Accuracy 0.878000020980835\n",
      "Iteration 14590 Training loss 0.0022547815460711718 Validation loss 0.050419241189956665 Accuracy 0.878000020980835\n",
      "Iteration 14600 Training loss 0.002755231224000454 Validation loss 0.050425056368112564 Accuracy 0.878000020980835\n",
      "Iteration 14610 Training loss 0.002504370640963316 Validation loss 0.050429318100214005 Accuracy 0.878000020980835\n",
      "Iteration 14620 Training loss 0.0012549464590847492 Validation loss 0.05043782666325569 Accuracy 0.878000020980835\n",
      "Iteration 14630 Training loss 0.0017549117328599095 Validation loss 0.050447091460227966 Accuracy 0.878000020980835\n",
      "Iteration 14640 Training loss 0.0025037697050720453 Validation loss 0.05044921115040779 Accuracy 0.878000020980835\n",
      "Iteration 14650 Training loss 0.0017532672500237823 Validation loss 0.050447676330804825 Accuracy 0.878000020980835\n",
      "Iteration 14660 Training loss 0.0025034165009856224 Validation loss 0.0504569336771965 Accuracy 0.878000020980835\n",
      "Iteration 14670 Training loss 0.001005065394565463 Validation loss 0.05045522004365921 Accuracy 0.878000020980835\n",
      "Iteration 14680 Training loss 0.00200434192083776 Validation loss 0.05046761408448219 Accuracy 0.878000020980835\n",
      "Iteration 14690 Training loss 0.002754880813881755 Validation loss 0.05046553164720535 Accuracy 0.878000020980835\n",
      "Iteration 14700 Training loss 0.0010047177784144878 Validation loss 0.050460733473300934 Accuracy 0.878000020980835\n",
      "Iteration 14710 Training loss 0.0022545980755239725 Validation loss 0.05044566094875336 Accuracy 0.878000020980835\n",
      "Iteration 14720 Training loss 0.0012546770740300417 Validation loss 0.05043928697705269 Accuracy 0.878000020980835\n",
      "Iteration 14730 Training loss 0.002504276344552636 Validation loss 0.0504353791475296 Accuracy 0.878000020980835\n",
      "Iteration 14740 Training loss 0.0015026659239083529 Validation loss 0.05044444277882576 Accuracy 0.878000020980835\n",
      "Iteration 14750 Training loss 0.0017547343159094453 Validation loss 0.05043647810816765 Accuracy 0.878000020980835\n",
      "Iteration 14760 Training loss 0.0025043708737939596 Validation loss 0.05043492838740349 Accuracy 0.878000020980835\n",
      "Iteration 14770 Training loss 0.0025045634247362614 Validation loss 0.05043677240610123 Accuracy 0.878000020980835\n",
      "Iteration 14780 Training loss 0.0012545869685709476 Validation loss 0.05044124647974968 Accuracy 0.878000020980835\n",
      "Iteration 14790 Training loss 0.00100417691282928 Validation loss 0.05043768137693405 Accuracy 0.878000020980835\n",
      "Iteration 14800 Training loss 0.002005014568567276 Validation loss 0.05044085159897804 Accuracy 0.878000020980835\n",
      "Iteration 14810 Training loss 0.0017539617838338017 Validation loss 0.0504491813480854 Accuracy 0.878000020980835\n",
      "Iteration 14820 Training loss 0.002254118677228689 Validation loss 0.05044694244861603 Accuracy 0.878000020980835\n",
      "Iteration 14830 Training loss 0.0025029631797224283 Validation loss 0.05045243725180626 Accuracy 0.878000020980835\n",
      "Iteration 14840 Training loss 0.0017546124290674925 Validation loss 0.05045642331242561 Accuracy 0.878000020980835\n",
      "Iteration 14850 Training loss 0.0012550258543342352 Validation loss 0.050441924482584 Accuracy 0.878000020980835\n",
      "Iteration 14860 Training loss 0.0022541251964867115 Validation loss 0.05044419690966606 Accuracy 0.878000020980835\n",
      "Iteration 14870 Training loss 0.002254780847579241 Validation loss 0.050446268171072006 Accuracy 0.878000020980835\n",
      "Iteration 14880 Training loss 0.0017543467693030834 Validation loss 0.050448983907699585 Accuracy 0.878000020980835\n",
      "Iteration 14890 Training loss 0.0015047749038785696 Validation loss 0.0504450686275959 Accuracy 0.878000020980835\n",
      "Iteration 14900 Training loss 0.0017544684233143926 Validation loss 0.05044151842594147 Accuracy 0.878000020980835\n",
      "Iteration 14910 Training loss 0.0020049307495355606 Validation loss 0.050449565052986145 Accuracy 0.878000020980835\n",
      "Iteration 14920 Training loss 0.002004815498366952 Validation loss 0.050457071512937546 Accuracy 0.878000020980835\n",
      "Iteration 14930 Training loss 0.0020024580880999565 Validation loss 0.05046457052230835 Accuracy 0.878000020980835\n",
      "Iteration 14940 Training loss 0.0015043944586068392 Validation loss 0.050470020622015 Accuracy 0.878000020980835\n",
      "Iteration 14950 Training loss 0.0017531225457787514 Validation loss 0.05046340823173523 Accuracy 0.878000020980835\n",
      "Iteration 14960 Training loss 0.0025039403699338436 Validation loss 0.05045608431100845 Accuracy 0.878000020980835\n",
      "Iteration 14970 Training loss 0.0015031215734779835 Validation loss 0.05045130476355553 Accuracy 0.878000020980835\n",
      "Iteration 14980 Training loss 0.0010046642273664474 Validation loss 0.05045297369360924 Accuracy 0.878000020980835\n",
      "Iteration 14990 Training loss 0.0020055007189512253 Validation loss 0.050451382994651794 Accuracy 0.878000020980835\n",
      "Iteration 15000 Training loss 0.0020027398131787777 Validation loss 0.05044448748230934 Accuracy 0.878000020980835\n",
      "Iteration 15010 Training loss 0.001753329299390316 Validation loss 0.050458915531635284 Accuracy 0.878000020980835\n",
      "Iteration 15020 Training loss 0.002003422472625971 Validation loss 0.05045301839709282 Accuracy 0.878000020980835\n",
      "Iteration 15030 Training loss 0.0020042448304593563 Validation loss 0.050462111830711365 Accuracy 0.878000020980835\n",
      "Iteration 15040 Training loss 0.0025030141696333885 Validation loss 0.05045970901846886 Accuracy 0.878000020980835\n",
      "Iteration 15050 Training loss 0.002504709642380476 Validation loss 0.05045122653245926 Accuracy 0.878000020980835\n",
      "Iteration 15060 Training loss 0.0030031073838472366 Validation loss 0.05043543502688408 Accuracy 0.878000020980835\n",
      "Iteration 15070 Training loss 0.001753121381625533 Validation loss 0.05043403059244156 Accuracy 0.878000020980835\n",
      "Iteration 15080 Training loss 0.0007544400868937373 Validation loss 0.05043007805943489 Accuracy 0.878000020980835\n",
      "Iteration 15090 Training loss 0.002004423877224326 Validation loss 0.050439223647117615 Accuracy 0.878000020980835\n",
      "Iteration 15100 Training loss 0.0030046359170228243 Validation loss 0.050448182970285416 Accuracy 0.878000020980835\n",
      "Iteration 15110 Training loss 0.0020052737090736628 Validation loss 0.05045820027589798 Accuracy 0.878000020980835\n",
      "Iteration 15120 Training loss 0.0007549871224910021 Validation loss 0.05046442151069641 Accuracy 0.878000020980835\n",
      "Iteration 15130 Training loss 0.0025040253531187773 Validation loss 0.05046926066279411 Accuracy 0.878000020980835\n",
      "Iteration 15140 Training loss 0.0025045929942280054 Validation loss 0.050463173538446426 Accuracy 0.878000020980835\n",
      "Iteration 15150 Training loss 0.0017532441997900605 Validation loss 0.050449471920728683 Accuracy 0.878000020980835\n",
      "Iteration 15160 Training loss 0.001254193834029138 Validation loss 0.05044405162334442 Accuracy 0.878000020980835\n",
      "Iteration 15170 Training loss 0.0015040042344480753 Validation loss 0.05046114698052406 Accuracy 0.878000020980835\n",
      "Iteration 15180 Training loss 0.001752373413182795 Validation loss 0.05045075714588165 Accuracy 0.878000020980835\n",
      "Iteration 15190 Training loss 0.0015044767642393708 Validation loss 0.05045565590262413 Accuracy 0.878000020980835\n",
      "Iteration 15200 Training loss 0.0015042462619021535 Validation loss 0.050455160439014435 Accuracy 0.878000020980835\n",
      "Iteration 15210 Training loss 0.0015043133171275258 Validation loss 0.050457663834095 Accuracy 0.878000020980835\n",
      "Iteration 15220 Training loss 0.0017544205766171217 Validation loss 0.050470609217882156 Accuracy 0.878000020980835\n",
      "Iteration 15230 Training loss 0.001004313468001783 Validation loss 0.05046564340591431 Accuracy 0.878000020980835\n",
      "Iteration 15240 Training loss 0.0022536807227879763 Validation loss 0.05045932158827782 Accuracy 0.878000020980835\n",
      "Iteration 15250 Training loss 0.003004277590662241 Validation loss 0.05046195164322853 Accuracy 0.878000020980835\n",
      "Iteration 15260 Training loss 0.0017538334941491485 Validation loss 0.050468504428863525 Accuracy 0.878000020980835\n",
      "Iteration 15270 Training loss 0.002252525184303522 Validation loss 0.05048181116580963 Accuracy 0.878000020980835\n",
      "Iteration 15280 Training loss 0.001754392171278596 Validation loss 0.050488151609897614 Accuracy 0.878000020980835\n",
      "Iteration 15290 Training loss 0.0022542166989296675 Validation loss 0.05051861330866814 Accuracy 0.8774999976158142\n",
      "Iteration 15300 Training loss 0.000999981421045959 Validation loss 0.0504935160279274 Accuracy 0.8774999976158142\n",
      "Iteration 15310 Training loss 0.002254378516227007 Validation loss 0.05053863301873207 Accuracy 0.878000020980835\n",
      "Iteration 15320 Training loss 0.0020047768484801054 Validation loss 0.05061239004135132 Accuracy 0.8774999976158142\n",
      "Iteration 15330 Training loss 0.0017727193189784884 Validation loss 0.05072501301765442 Accuracy 0.8769999742507935\n",
      "Iteration 15340 Training loss 0.001523251412436366 Validation loss 0.05051841214299202 Accuracy 0.8784999847412109\n",
      "Iteration 15350 Training loss 0.0027563530020415783 Validation loss 0.05046653747558594 Accuracy 0.8784999847412109\n",
      "Iteration 15360 Training loss 0.0007551498129032552 Validation loss 0.05044206976890564 Accuracy 0.8774999976158142\n",
      "Iteration 15370 Training loss 0.002256903564557433 Validation loss 0.050474945455789566 Accuracy 0.8774999976158142\n",
      "Iteration 15380 Training loss 0.0017565598245710135 Validation loss 0.05046775937080383 Accuracy 0.8774999976158142\n",
      "Iteration 15390 Training loss 0.001506949309259653 Validation loss 0.050436072051525116 Accuracy 0.8769999742507935\n",
      "Iteration 15400 Training loss 0.00125626299995929 Validation loss 0.05040394887328148 Accuracy 0.8774999976158142\n",
      "Iteration 15410 Training loss 0.0012560891918838024 Validation loss 0.05037998780608177 Accuracy 0.8774999976158142\n",
      "Iteration 15420 Training loss 0.0015058009885251522 Validation loss 0.050396326929330826 Accuracy 0.8765000104904175\n",
      "Iteration 15430 Training loss 0.0020059545058757067 Validation loss 0.05037649720907211 Accuracy 0.8769999742507935\n",
      "Iteration 15440 Training loss 0.0022560572251677513 Validation loss 0.05037730187177658 Accuracy 0.8769999742507935\n",
      "Iteration 15450 Training loss 0.0017578804399818182 Validation loss 0.050382036715745926 Accuracy 0.8769999742507935\n",
      "Iteration 15460 Training loss 0.0015053534880280495 Validation loss 0.05039428919553757 Accuracy 0.8765000104904175\n",
      "Iteration 15470 Training loss 0.0007558389916084707 Validation loss 0.0503879077732563 Accuracy 0.8769999742507935\n",
      "Iteration 15480 Training loss 0.0015052430098876357 Validation loss 0.05038825049996376 Accuracy 0.8769999742507935\n",
      "Iteration 15490 Training loss 0.002255181083455682 Validation loss 0.050392888486385345 Accuracy 0.8769999742507935\n",
      "Iteration 15500 Training loss 0.0025056127924472094 Validation loss 0.05038749799132347 Accuracy 0.8765000104904175\n",
      "Iteration 15510 Training loss 0.0022572583984583616 Validation loss 0.05040730908513069 Accuracy 0.8765000104904175\n",
      "Iteration 15520 Training loss 0.0010049077682197094 Validation loss 0.05040309950709343 Accuracy 0.8765000104904175\n",
      "Iteration 15530 Training loss 0.0032555090729147196 Validation loss 0.050383310765028 Accuracy 0.8765000104904175\n",
      "Iteration 15540 Training loss 0.001506599597632885 Validation loss 0.050401318818330765 Accuracy 0.8769999742507935\n",
      "Iteration 15550 Training loss 0.0012561936164274812 Validation loss 0.05041243135929108 Accuracy 0.8765000104904175\n",
      "Iteration 15560 Training loss 0.002506469376385212 Validation loss 0.05040174722671509 Accuracy 0.8769999742507935\n",
      "Iteration 15570 Training loss 0.00200650654733181 Validation loss 0.05040661618113518 Accuracy 0.8765000104904175\n",
      "Iteration 15580 Training loss 0.003256264142692089 Validation loss 0.05040055513381958 Accuracy 0.8765000104904175\n",
      "Iteration 15590 Training loss 0.0022550139110535383 Validation loss 0.05039845407009125 Accuracy 0.8769999742507935\n",
      "Iteration 15600 Training loss 0.0017551089404150844 Validation loss 0.0504012256860733 Accuracy 0.8769999742507935\n",
      "Iteration 15610 Training loss 0.001005041878670454 Validation loss 0.05040491744875908 Accuracy 0.8774999976158142\n",
      "Iteration 15620 Training loss 0.0015047979541122913 Validation loss 0.05040394887328148 Accuracy 0.8774999976158142\n",
      "Iteration 15630 Training loss 0.0007563175167888403 Validation loss 0.05041784793138504 Accuracy 0.8774999976158142\n",
      "Iteration 15640 Training loss 0.002756464295089245 Validation loss 0.05041651055216789 Accuracy 0.8774999976158142\n",
      "Iteration 15650 Training loss 0.002005242044106126 Validation loss 0.050421927124261856 Accuracy 0.8774999976158142\n",
      "Iteration 15660 Training loss 0.0025053746066987514 Validation loss 0.050423186272382736 Accuracy 0.8774999976158142\n",
      "Iteration 15670 Training loss 0.001505255582742393 Validation loss 0.050439782440662384 Accuracy 0.8774999976158142\n",
      "Iteration 15680 Training loss 0.0017545113805681467 Validation loss 0.05043688789010048 Accuracy 0.8774999976158142\n",
      "Iteration 15690 Training loss 0.0020059996750205755 Validation loss 0.05044488608837128 Accuracy 0.8774999976158142\n",
      "Iteration 15700 Training loss 0.00100501358974725 Validation loss 0.05044172704219818 Accuracy 0.8774999976158142\n",
      "Iteration 15710 Training loss 0.0015051966765895486 Validation loss 0.05044681578874588 Accuracy 0.8774999976158142\n",
      "Iteration 15720 Training loss 0.0017553599318489432 Validation loss 0.05044427514076233 Accuracy 0.8774999976158142\n",
      "Iteration 15730 Training loss 0.001005173777230084 Validation loss 0.05044151470065117 Accuracy 0.8774999976158142\n",
      "Iteration 15740 Training loss 0.001005563884973526 Validation loss 0.05043511837720871 Accuracy 0.8774999976158142\n",
      "Iteration 15750 Training loss 0.0012548447120934725 Validation loss 0.050442636013031006 Accuracy 0.8774999976158142\n",
      "Iteration 15760 Training loss 0.0010062683140859008 Validation loss 0.05045649781823158 Accuracy 0.8774999976158142\n",
      "Iteration 15770 Training loss 0.0020051735918968916 Validation loss 0.05046242102980614 Accuracy 0.8774999976158142\n",
      "Iteration 15780 Training loss 0.0012552151456475258 Validation loss 0.05046374350786209 Accuracy 0.8774999976158142\n",
      "Iteration 15790 Training loss 0.00225490122102201 Validation loss 0.05048106238245964 Accuracy 0.8774999976158142\n",
      "Iteration 15800 Training loss 0.002005574991926551 Validation loss 0.05047793686389923 Accuracy 0.8774999976158142\n",
      "Iteration 15810 Training loss 0.0020054150372743607 Validation loss 0.05046432465314865 Accuracy 0.8774999976158142\n",
      "Iteration 15820 Training loss 0.0010047625983133912 Validation loss 0.0504816509783268 Accuracy 0.8774999976158142\n",
      "Iteration 15830 Training loss 0.0025052777491509914 Validation loss 0.05049607530236244 Accuracy 0.8774999976158142\n",
      "Iteration 15840 Training loss 0.0017559666885063052 Validation loss 0.05048911273479462 Accuracy 0.8774999976158142\n",
      "Iteration 15850 Training loss 0.0017563548171892762 Validation loss 0.05048805847764015 Accuracy 0.8774999976158142\n",
      "Iteration 15860 Training loss 0.0020062171388417482 Validation loss 0.0504789799451828 Accuracy 0.878000020980835\n",
      "Iteration 15870 Training loss 0.0027555825654417276 Validation loss 0.05048874020576477 Accuracy 0.878000020980835\n",
      "Iteration 15880 Training loss 0.0020043125841766596 Validation loss 0.05049104243516922 Accuracy 0.878000020980835\n",
      "Iteration 15890 Training loss 0.0015053534880280495 Validation loss 0.05048462375998497 Accuracy 0.878000020980835\n",
      "Iteration 15900 Training loss 0.0010046808747574687 Validation loss 0.05049237608909607 Accuracy 0.8774999976158142\n",
      "Iteration 15910 Training loss 0.0017545167356729507 Validation loss 0.050494760274887085 Accuracy 0.8774999976158142\n",
      "Iteration 15920 Training loss 0.0020052422769367695 Validation loss 0.050501853227615356 Accuracy 0.878000020980835\n",
      "Iteration 15930 Training loss 0.001755174482241273 Validation loss 0.050513509660959244 Accuracy 0.8784999847412109\n",
      "Iteration 15940 Training loss 0.0015053055249154568 Validation loss 0.05050051212310791 Accuracy 0.8784999847412109\n",
      "Iteration 15950 Training loss 0.0017553786747157574 Validation loss 0.05048483982682228 Accuracy 0.878000020980835\n",
      "Iteration 15960 Training loss 0.0025054344441741705 Validation loss 0.05050184205174446 Accuracy 0.878000020980835\n",
      "Iteration 15970 Training loss 0.0015047938795760274 Validation loss 0.05052154138684273 Accuracy 0.878000020980835\n",
      "Iteration 15980 Training loss 0.0012556269066408277 Validation loss 0.05052197352051735 Accuracy 0.8774999976158142\n",
      "Iteration 15990 Training loss 0.001254788599908352 Validation loss 0.05053260922431946 Accuracy 0.878000020980835\n",
      "Iteration 16000 Training loss 0.0017548499163240194 Validation loss 0.050528544932603836 Accuracy 0.878000020980835\n",
      "Iteration 16010 Training loss 0.0022550071589648724 Validation loss 0.05052459239959717 Accuracy 0.878000020980835\n",
      "Iteration 16020 Training loss 0.0015050377696752548 Validation loss 0.05053257197141647 Accuracy 0.8774999976158142\n",
      "Iteration 16030 Training loss 0.0015054168179631233 Validation loss 0.050530869513750076 Accuracy 0.8774999976158142\n",
      "Iteration 16040 Training loss 0.0022547903936356306 Validation loss 0.05052601546049118 Accuracy 0.8774999976158142\n",
      "Iteration 16050 Training loss 0.0025046837981790304 Validation loss 0.05052949860692024 Accuracy 0.8774999976158142\n",
      "Iteration 16060 Training loss 0.0007552329334430397 Validation loss 0.05052982270717621 Accuracy 0.8774999976158142\n",
      "Iteration 16070 Training loss 0.0015053903916850686 Validation loss 0.050534144043922424 Accuracy 0.8774999976158142\n",
      "Iteration 16080 Training loss 0.0015053596580401063 Validation loss 0.05053798109292984 Accuracy 0.878000020980835\n",
      "Iteration 16090 Training loss 0.0015054838731884956 Validation loss 0.05053147301077843 Accuracy 0.878000020980835\n",
      "Iteration 16100 Training loss 0.0012546953512355685 Validation loss 0.05052204802632332 Accuracy 0.8774999976158142\n",
      "Iteration 16110 Training loss 0.0025049271062016487 Validation loss 0.05053368955850601 Accuracy 0.8774999976158142\n",
      "Iteration 16120 Training loss 0.001505332998931408 Validation loss 0.050541359931230545 Accuracy 0.8774999976158142\n",
      "Iteration 16130 Training loss 0.0020050881430506706 Validation loss 0.05053616315126419 Accuracy 0.8774999976158142\n",
      "Iteration 16140 Training loss 0.002754197223111987 Validation loss 0.05053139477968216 Accuracy 0.8774999976158142\n",
      "Iteration 16150 Training loss 0.0012544047785922885 Validation loss 0.05053357407450676 Accuracy 0.8774999976158142\n",
      "Iteration 16160 Training loss 0.0027544861659407616 Validation loss 0.050540242344141006 Accuracy 0.8774999976158142\n",
      "Iteration 16170 Training loss 0.0022560216020792723 Validation loss 0.05054039880633354 Accuracy 0.8774999976158142\n",
      "Iteration 16180 Training loss 0.0012552815023809671 Validation loss 0.05054451897740364 Accuracy 0.878000020980835\n",
      "Iteration 16190 Training loss 0.00225457432679832 Validation loss 0.05054989457130432 Accuracy 0.878000020980835\n",
      "Iteration 16200 Training loss 0.0017546098679304123 Validation loss 0.05054757371544838 Accuracy 0.878000020980835\n",
      "Iteration 16210 Training loss 0.001755277393385768 Validation loss 0.05054309591650963 Accuracy 0.878000020980835\n",
      "Iteration 16220 Training loss 0.0017552718054503202 Validation loss 0.05053892359137535 Accuracy 0.878000020980835\n",
      "Iteration 16230 Training loss 0.0027557702269405127 Validation loss 0.05054744705557823 Accuracy 0.878000020980835\n",
      "Iteration 16240 Training loss 0.0015053872484713793 Validation loss 0.05056293308734894 Accuracy 0.878000020980835\n",
      "Iteration 16250 Training loss 0.002255290048196912 Validation loss 0.050539273768663406 Accuracy 0.8784999847412109\n",
      "Iteration 16260 Training loss 0.0007542287348769605 Validation loss 0.05053355544805527 Accuracy 0.878000020980835\n",
      "Iteration 16270 Training loss 0.001505363150499761 Validation loss 0.05054483190178871 Accuracy 0.878000020980835\n",
      "Iteration 16280 Training loss 0.0015046648913994431 Validation loss 0.050552889704704285 Accuracy 0.878000020980835\n",
      "Iteration 16290 Training loss 0.0015050122747197747 Validation loss 0.050558902323246 Accuracy 0.878000020980835\n",
      "Iteration 16300 Training loss 0.002004740061238408 Validation loss 0.05055641755461693 Accuracy 0.878000020980835\n",
      "Iteration 16310 Training loss 0.002004374051466584 Validation loss 0.05056048929691315 Accuracy 0.8774999976158142\n",
      "Iteration 16320 Training loss 0.0020048003643751144 Validation loss 0.050547827035188675 Accuracy 0.878000020980835\n",
      "Iteration 16330 Training loss 0.002004756359383464 Validation loss 0.050559721887111664 Accuracy 0.8784999847412109\n",
      "Iteration 16340 Training loss 0.0012550352839753032 Validation loss 0.05056223273277283 Accuracy 0.878000020980835\n",
      "Iteration 16350 Training loss 0.001255140989087522 Validation loss 0.05056026205420494 Accuracy 0.8784999847412109\n",
      "Iteration 16360 Training loss 0.0012549285311251879 Validation loss 0.050555113703012466 Accuracy 0.878000020980835\n",
      "Iteration 16370 Training loss 0.0030048051849007607 Validation loss 0.050539303570985794 Accuracy 0.878000020980835\n",
      "Iteration 16380 Training loss 0.0025045964866876602 Validation loss 0.05053732916712761 Accuracy 0.878000020980835\n",
      "Iteration 16390 Training loss 0.0020049582235515118 Validation loss 0.050553131848573685 Accuracy 0.8784999847412109\n",
      "Iteration 16400 Training loss 0.001754774828441441 Validation loss 0.05054810270667076 Accuracy 0.8784999847412109\n",
      "Iteration 16410 Training loss 0.0010049010161310434 Validation loss 0.05055232346057892 Accuracy 0.878000020980835\n",
      "Iteration 16420 Training loss 0.001755633857101202 Validation loss 0.05055776238441467 Accuracy 0.8784999847412109\n",
      "Iteration 16430 Training loss 0.0027545022312551737 Validation loss 0.050540417432785034 Accuracy 0.8784999847412109\n",
      "Iteration 16440 Training loss 0.003004113445058465 Validation loss 0.05053735896945 Accuracy 0.8784999847412109\n",
      "Iteration 16450 Training loss 0.0015043894527480006 Validation loss 0.05055256560444832 Accuracy 0.878000020980835\n",
      "Iteration 16460 Training loss 0.0010050542186945677 Validation loss 0.050556253641843796 Accuracy 0.8784999847412109\n",
      "Iteration 16470 Training loss 0.003503992222249508 Validation loss 0.050547514110803604 Accuracy 0.8784999847412109\n",
      "Iteration 16480 Training loss 0.003003813559189439 Validation loss 0.050545625388622284 Accuracy 0.8784999847412109\n",
      "Iteration 16490 Training loss 0.002255236729979515 Validation loss 0.050544463098049164 Accuracy 0.8784999847412109\n",
      "Iteration 16500 Training loss 0.002004444831982255 Validation loss 0.05053821578621864 Accuracy 0.8784999847412109\n",
      "Iteration 16510 Training loss 0.0017538954271003604 Validation loss 0.050533004105091095 Accuracy 0.878000020980835\n",
      "Iteration 16520 Training loss 0.0015050281072035432 Validation loss 0.05053494870662689 Accuracy 0.878000020980835\n",
      "Iteration 16530 Training loss 0.0025047315284609795 Validation loss 0.050549279898405075 Accuracy 0.8784999847412109\n",
      "Iteration 16540 Training loss 0.0015048475470393896 Validation loss 0.05055839195847511 Accuracy 0.8784999847412109\n",
      "Iteration 16550 Training loss 0.002004313049837947 Validation loss 0.05055027827620506 Accuracy 0.8784999847412109\n",
      "Iteration 16560 Training loss 0.0010043284855782986 Validation loss 0.05054767429828644 Accuracy 0.8784999847412109\n",
      "Iteration 16570 Training loss 0.0025048027746379375 Validation loss 0.05056121200323105 Accuracy 0.878000020980835\n",
      "Iteration 16580 Training loss 0.0017540823901072145 Validation loss 0.050569988787174225 Accuracy 0.878000020980835\n",
      "Iteration 16590 Training loss 0.0015047936467453837 Validation loss 0.050579734146595 Accuracy 0.8769999742507935\n",
      "Iteration 16600 Training loss 0.0017545364098623395 Validation loss 0.05058763548731804 Accuracy 0.8774999976158142\n",
      "Iteration 16610 Training loss 0.0025038656312972307 Validation loss 0.05057116970419884 Accuracy 0.878000020980835\n",
      "Iteration 16620 Training loss 0.0022550811991095543 Validation loss 0.05056948959827423 Accuracy 0.8774999976158142\n",
      "Iteration 16630 Training loss 0.0012543154880404472 Validation loss 0.0505649633705616 Accuracy 0.878000020980835\n",
      "Iteration 16640 Training loss 0.0010042175417765975 Validation loss 0.050554413348436356 Accuracy 0.8784999847412109\n",
      "Iteration 16650 Training loss 0.002003944246098399 Validation loss 0.050559960305690765 Accuracy 0.8774999976158142\n",
      "Iteration 16660 Training loss 0.0022543296217918396 Validation loss 0.05055834352970123 Accuracy 0.8774999976158142\n",
      "Iteration 16670 Training loss 0.0027535129338502884 Validation loss 0.05056522414088249 Accuracy 0.8774999976158142\n",
      "Iteration 16680 Training loss 0.002254016464576125 Validation loss 0.05056383088231087 Accuracy 0.878000020980835\n",
      "Iteration 16690 Training loss 0.001754168071784079 Validation loss 0.05056184157729149 Accuracy 0.8774999976158142\n",
      "Iteration 16700 Training loss 0.0020046010613441467 Validation loss 0.05056323483586311 Accuracy 0.8774999976158142\n",
      "Iteration 16710 Training loss 0.0015044001629576087 Validation loss 0.05056285113096237 Accuracy 0.878000020980835\n",
      "Iteration 16720 Training loss 0.0015043296152725816 Validation loss 0.05057018995285034 Accuracy 0.8774999976158142\n",
      "Iteration 16730 Training loss 0.0012539132731035352 Validation loss 0.05057511106133461 Accuracy 0.8774999976158142\n",
      "Iteration 16740 Training loss 0.0017536361701786518 Validation loss 0.05056812986731529 Accuracy 0.878000020980835\n",
      "Iteration 16750 Training loss 0.002503772731870413 Validation loss 0.05056039243936539 Accuracy 0.878000020980835\n",
      "Iteration 16760 Training loss 0.002255471423268318 Validation loss 0.050569623708724976 Accuracy 0.878000020980835\n",
      "Iteration 16770 Training loss 0.0027543811593204737 Validation loss 0.05057145655155182 Accuracy 0.878000020980835\n",
      "Iteration 16780 Training loss 0.002754297573119402 Validation loss 0.05056501179933548 Accuracy 0.878000020980835\n",
      "Iteration 16790 Training loss 0.0012544686906039715 Validation loss 0.050574883818626404 Accuracy 0.878000020980835\n",
      "Iteration 16800 Training loss 0.0007547280401922762 Validation loss 0.050572752952575684 Accuracy 0.878000020980835\n",
      "Iteration 16810 Training loss 0.00125408498570323 Validation loss 0.05057619884610176 Accuracy 0.878000020980835\n",
      "Iteration 16820 Training loss 0.0025042202323675156 Validation loss 0.05056445300579071 Accuracy 0.878000020980835\n",
      "Iteration 16830 Training loss 0.001754106255248189 Validation loss 0.05056590959429741 Accuracy 0.878000020980835\n",
      "Iteration 16840 Training loss 0.0027539324946701527 Validation loss 0.050569046288728714 Accuracy 0.878000020980835\n",
      "Iteration 16850 Training loss 0.0015035034157335758 Validation loss 0.0505678616464138 Accuracy 0.878000020980835\n",
      "Iteration 16860 Training loss 0.002504492411389947 Validation loss 0.05057216063141823 Accuracy 0.878000020980835\n",
      "Iteration 16870 Training loss 0.0020037556532770395 Validation loss 0.05057429522275925 Accuracy 0.878000020980835\n",
      "Iteration 16880 Training loss 0.0015040944563224912 Validation loss 0.050581030547618866 Accuracy 0.878000020980835\n",
      "Iteration 16890 Training loss 0.0030039027333259583 Validation loss 0.05057193711400032 Accuracy 0.878000020980835\n",
      "Iteration 16900 Training loss 0.0017541892593726516 Validation loss 0.05057680234313011 Accuracy 0.878000020980835\n",
      "Iteration 16910 Training loss 0.0012547133956104517 Validation loss 0.05057847127318382 Accuracy 0.878000020980835\n",
      "Iteration 16920 Training loss 0.00200440501794219 Validation loss 0.05058000981807709 Accuracy 0.878000020980835\n",
      "Iteration 16930 Training loss 0.0017547475872561336 Validation loss 0.05057651549577713 Accuracy 0.878000020980835\n",
      "Iteration 16940 Training loss 0.002254418795928359 Validation loss 0.05056897550821304 Accuracy 0.878000020980835\n",
      "Iteration 16950 Training loss 0.0032539479434490204 Validation loss 0.05057341605424881 Accuracy 0.878000020980835\n",
      "Iteration 16960 Training loss 0.0017543191788718104 Validation loss 0.05058521404862404 Accuracy 0.878000020980835\n",
      "Iteration 16970 Training loss 0.0010045201051980257 Validation loss 0.05058834329247475 Accuracy 0.878000020980835\n",
      "Iteration 16980 Training loss 0.0010045480448752642 Validation loss 0.050591304898262024 Accuracy 0.878000020980835\n",
      "Iteration 16990 Training loss 0.000754165172111243 Validation loss 0.050590645521879196 Accuracy 0.878000020980835\n",
      "Iteration 17000 Training loss 0.0020043335389345884 Validation loss 0.05059812217950821 Accuracy 0.878000020980835\n",
      "Iteration 17010 Training loss 0.0015040317084640265 Validation loss 0.05058756843209267 Accuracy 0.878000020980835\n",
      "Iteration 17020 Training loss 0.0017539778491482139 Validation loss 0.05059688165783882 Accuracy 0.878000020980835\n",
      "Iteration 17030 Training loss 0.001754466793499887 Validation loss 0.050596632063388824 Accuracy 0.878000020980835\n",
      "Iteration 17040 Training loss 0.001753972377628088 Validation loss 0.050603386014699936 Accuracy 0.878000020980835\n",
      "Iteration 17050 Training loss 0.002003556350246072 Validation loss 0.05059220269322395 Accuracy 0.878000020980835\n",
      "Iteration 17060 Training loss 0.0007547260611318052 Validation loss 0.0506022647023201 Accuracy 0.878000020980835\n",
      "Iteration 17070 Training loss 0.001503981533460319 Validation loss 0.05061303451657295 Accuracy 0.878000020980835\n",
      "Iteration 17080 Training loss 0.0010046649258583784 Validation loss 0.05060536786913872 Accuracy 0.878000020980835\n",
      "Iteration 17090 Training loss 0.0015041930601000786 Validation loss 0.05060621351003647 Accuracy 0.878000020980835\n",
      "Iteration 17100 Training loss 0.0020040611270815134 Validation loss 0.05059952288866043 Accuracy 0.878000020980835\n",
      "Iteration 17110 Training loss 0.002504409523680806 Validation loss 0.0506105050444603 Accuracy 0.878000020980835\n",
      "Iteration 17120 Training loss 0.0022544818930327892 Validation loss 0.05062523111701012 Accuracy 0.878000020980835\n",
      "Iteration 17130 Training loss 0.002253895625472069 Validation loss 0.050615984946489334 Accuracy 0.878000020980835\n",
      "Iteration 17140 Training loss 0.002254471415653825 Validation loss 0.05061328783631325 Accuracy 0.878000020980835\n",
      "Iteration 17150 Training loss 0.0027539553120732307 Validation loss 0.05060864984989166 Accuracy 0.878000020980835\n",
      "Iteration 17160 Training loss 0.0017542422283440828 Validation loss 0.05060996487736702 Accuracy 0.878000020980835\n",
      "Iteration 17170 Training loss 0.001254047267138958 Validation loss 0.0506138876080513 Accuracy 0.878000020980835\n",
      "Iteration 17180 Training loss 0.002004210837185383 Validation loss 0.05060058832168579 Accuracy 0.878000020980835\n",
      "Iteration 17190 Training loss 0.0012543811462819576 Validation loss 0.050594668835401535 Accuracy 0.878000020980835\n",
      "Iteration 17200 Training loss 0.0012537002330645919 Validation loss 0.050584498792886734 Accuracy 0.878000020980835\n",
      "Iteration 17210 Training loss 0.0012541147880256176 Validation loss 0.0505872406065464 Accuracy 0.878000020980835\n",
      "Iteration 17220 Training loss 0.0017539814580231905 Validation loss 0.050594717264175415 Accuracy 0.878000020980835\n",
      "Iteration 17230 Training loss 0.002004111185669899 Validation loss 0.05059930309653282 Accuracy 0.878000020980835\n",
      "Iteration 17240 Training loss 0.0025042137131094933 Validation loss 0.05059046298265457 Accuracy 0.878000020980835\n",
      "Iteration 17250 Training loss 0.0017546755261719227 Validation loss 0.050600916147232056 Accuracy 0.878000020980835\n",
      "Iteration 17260 Training loss 0.0032538569066673517 Validation loss 0.05060483515262604 Accuracy 0.878000020980835\n",
      "Iteration 17270 Training loss 0.0012545576319098473 Validation loss 0.050615519285202026 Accuracy 0.878000020980835\n",
      "Iteration 17280 Training loss 0.0017539296532049775 Validation loss 0.05061453580856323 Accuracy 0.878000020980835\n",
      "Iteration 17290 Training loss 0.002254325430840254 Validation loss 0.05060433968901634 Accuracy 0.878000020980835\n",
      "Iteration 17300 Training loss 0.002504032803699374 Validation loss 0.05060834065079689 Accuracy 0.878000020980835\n",
      "Iteration 17310 Training loss 0.0017538705142214894 Validation loss 0.050607070326805115 Accuracy 0.878000020980835\n",
      "Iteration 17320 Training loss 0.0015050553483888507 Validation loss 0.05061005800962448 Accuracy 0.878000020980835\n",
      "Iteration 17330 Training loss 0.0017540311673656106 Validation loss 0.05060239136219025 Accuracy 0.878000020980835\n",
      "Iteration 17340 Training loss 0.001504180720075965 Validation loss 0.05059855803847313 Accuracy 0.878000020980835\n",
      "Iteration 17350 Training loss 0.0015040847938507795 Validation loss 0.0506046898663044 Accuracy 0.878000020980835\n",
      "Iteration 17360 Training loss 0.0020042548421770334 Validation loss 0.05059979856014252 Accuracy 0.878000020980835\n",
      "Iteration 17370 Training loss 0.002254115417599678 Validation loss 0.05060430243611336 Accuracy 0.878000020980835\n",
      "Iteration 17380 Training loss 0.00325353373773396 Validation loss 0.050596751272678375 Accuracy 0.878000020980835\n",
      "Iteration 17390 Training loss 0.001504082465544343 Validation loss 0.050597138702869415 Accuracy 0.878000020980835\n",
      "Iteration 17400 Training loss 0.0010043284855782986 Validation loss 0.05060175433754921 Accuracy 0.878000020980835\n",
      "Iteration 17410 Training loss 0.0022543054074048996 Validation loss 0.050609298050403595 Accuracy 0.8784999847412109\n",
      "Iteration 17420 Training loss 0.002254192251712084 Validation loss 0.050613563507795334 Accuracy 0.8784999847412109\n",
      "Iteration 17430 Training loss 0.0005043668788857758 Validation loss 0.05061628296971321 Accuracy 0.8784999847412109\n",
      "Iteration 17440 Training loss 0.0005037765367887914 Validation loss 0.050616927444934845 Accuracy 0.878000020980835\n",
      "Iteration 17450 Training loss 0.0010039533954113722 Validation loss 0.05061620473861694 Accuracy 0.878000020980835\n",
      "Iteration 17460 Training loss 0.0007534586475230753 Validation loss 0.050612471997737885 Accuracy 0.878000020980835\n",
      "Iteration 17470 Training loss 0.001503950683400035 Validation loss 0.05061781406402588 Accuracy 0.878000020980835\n",
      "Iteration 17480 Training loss 0.0015040283324196935 Validation loss 0.05059792473912239 Accuracy 0.878000020980835\n",
      "Iteration 17490 Training loss 0.0017543711001053452 Validation loss 0.05061016231775284 Accuracy 0.878000020980835\n",
      "Iteration 17500 Training loss 0.0015036464901641011 Validation loss 0.050617873668670654 Accuracy 0.878000020980835\n",
      "Iteration 17510 Training loss 0.0030041388235986233 Validation loss 0.05061667039990425 Accuracy 0.878000020980835\n",
      "Iteration 17520 Training loss 0.0015044581377878785 Validation loss 0.050626203417778015 Accuracy 0.878000020980835\n",
      "Iteration 17530 Training loss 0.0020039710216224194 Validation loss 0.050624214112758636 Accuracy 0.878000020980835\n",
      "Iteration 17540 Training loss 0.002503817668184638 Validation loss 0.05061659216880798 Accuracy 0.878000020980835\n",
      "Iteration 17550 Training loss 0.0030034470837563276 Validation loss 0.05061902105808258 Accuracy 0.878000020980835\n",
      "Iteration 17560 Training loss 0.0017538988031446934 Validation loss 0.050632644444704056 Accuracy 0.878000020980835\n",
      "Iteration 17570 Training loss 0.0020034618210047483 Validation loss 0.05061586946249008 Accuracy 0.878000020980835\n",
      "Iteration 17580 Training loss 0.0017544510774314404 Validation loss 0.050618767738342285 Accuracy 0.878000020980835\n",
      "Iteration 17590 Training loss 0.003003644524142146 Validation loss 0.0506260059773922 Accuracy 0.878000020980835\n",
      "Iteration 17600 Training loss 0.0015034450916573405 Validation loss 0.05062217637896538 Accuracy 0.8784999847412109\n",
      "Iteration 17610 Training loss 0.0020045535638928413 Validation loss 0.05062749981880188 Accuracy 0.8784999847412109\n",
      "Iteration 17620 Training loss 0.0012538685696199536 Validation loss 0.050624411553144455 Accuracy 0.878000020980835\n",
      "Iteration 17630 Training loss 0.001004340942017734 Validation loss 0.05062754824757576 Accuracy 0.878000020980835\n",
      "Iteration 17640 Training loss 0.0012538493610918522 Validation loss 0.050635743886232376 Accuracy 0.878000020980835\n",
      "Iteration 17650 Training loss 0.001754297991283238 Validation loss 0.05063580721616745 Accuracy 0.878000020980835\n",
      "Iteration 17660 Training loss 0.0025039035826921463 Validation loss 0.05063281208276749 Accuracy 0.878000020980835\n",
      "Iteration 17670 Training loss 0.00125362619291991 Validation loss 0.05063789710402489 Accuracy 0.878000020980835\n",
      "Iteration 17680 Training loss 0.002004171023145318 Validation loss 0.05063861981034279 Accuracy 0.878000020980835\n",
      "Iteration 17690 Training loss 0.0020037260837852955 Validation loss 0.05063946172595024 Accuracy 0.8784999847412109\n",
      "Iteration 17700 Training loss 0.0022536946926265955 Validation loss 0.050631288439035416 Accuracy 0.878000020980835\n",
      "Iteration 17710 Training loss 0.0015040707075968385 Validation loss 0.05063778907060623 Accuracy 0.878000020980835\n",
      "Iteration 17720 Training loss 0.0017536099767312407 Validation loss 0.05063127726316452 Accuracy 0.878000020980835\n",
      "Iteration 17730 Training loss 0.0020045265555381775 Validation loss 0.050634756684303284 Accuracy 0.878000020980835\n",
      "Iteration 17740 Training loss 0.0017535140505060554 Validation loss 0.050623632967472076 Accuracy 0.878000020980835\n",
      "Iteration 17750 Training loss 0.001004091463983059 Validation loss 0.05061651021242142 Accuracy 0.878000020980835\n",
      "Iteration 17760 Training loss 0.002503757830709219 Validation loss 0.050622645765542984 Accuracy 0.878000020980835\n",
      "Iteration 17770 Training loss 0.0010042310459539294 Validation loss 0.05062224715948105 Accuracy 0.878000020980835\n",
      "Iteration 17780 Training loss 0.0022540960926562548 Validation loss 0.050633687525987625 Accuracy 0.878000020980835\n",
      "Iteration 17790 Training loss 0.0020038383081555367 Validation loss 0.05064401403069496 Accuracy 0.878000020980835\n",
      "Iteration 17800 Training loss 0.0010040504857897758 Validation loss 0.05063873156905174 Accuracy 0.8784999847412109\n",
      "Iteration 17810 Training loss 0.0017537843668833375 Validation loss 0.05063566565513611 Accuracy 0.878000020980835\n",
      "Iteration 17820 Training loss 0.0012537187431007624 Validation loss 0.050630856305360794 Accuracy 0.878000020980835\n",
      "Iteration 17830 Training loss 0.003503385465592146 Validation loss 0.05063124746084213 Accuracy 0.8784999847412109\n",
      "Iteration 17840 Training loss 0.0012537308502942324 Validation loss 0.05062344670295715 Accuracy 0.878000020980835\n",
      "Iteration 17850 Training loss 0.001504517742432654 Validation loss 0.050632454454898834 Accuracy 0.878000020980835\n",
      "Iteration 17860 Training loss 0.0020040806848555803 Validation loss 0.0506448894739151 Accuracy 0.878000020980835\n",
      "Iteration 17870 Training loss 0.001503701671026647 Validation loss 0.05064386874437332 Accuracy 0.878000020980835\n",
      "Iteration 17880 Training loss 0.0030033010989427567 Validation loss 0.05064539238810539 Accuracy 0.878000020980835\n",
      "Iteration 17890 Training loss 0.0017541422275826335 Validation loss 0.0506453737616539 Accuracy 0.878000020980835\n",
      "Iteration 17900 Training loss 0.0010037488536909223 Validation loss 0.050635192543268204 Accuracy 0.878000020980835\n",
      "Iteration 17910 Training loss 0.0022543298546224833 Validation loss 0.05065390095114708 Accuracy 0.8784999847412109\n",
      "Iteration 17920 Training loss 0.001253840746358037 Validation loss 0.050649069249629974 Accuracy 0.878000020980835\n",
      "Iteration 17930 Training loss 0.002253471640869975 Validation loss 0.05064057186245918 Accuracy 0.878000020980835\n",
      "Iteration 17940 Training loss 0.0025032442063093185 Validation loss 0.05064447969198227 Accuracy 0.878000020980835\n",
      "Iteration 17950 Training loss 0.002253346610814333 Validation loss 0.050642598420381546 Accuracy 0.878000020980835\n",
      "Iteration 17960 Training loss 0.001753317890688777 Validation loss 0.05064936727285385 Accuracy 0.878000020980835\n",
      "Iteration 17970 Training loss 0.00200407556258142 Validation loss 0.050661563873291016 Accuracy 0.8784999847412109\n",
      "Iteration 17980 Training loss 0.0012538628652691841 Validation loss 0.05067471042275429 Accuracy 0.8784999847412109\n",
      "Iteration 17990 Training loss 0.0015040836296975613 Validation loss 0.05067894607782364 Accuracy 0.8784999847412109\n",
      "Iteration 18000 Training loss 0.0020039777737110853 Validation loss 0.05068902298808098 Accuracy 0.8784999847412109\n",
      "Iteration 18010 Training loss 0.0015038794372230768 Validation loss 0.05067148804664612 Accuracy 0.8784999847412109\n",
      "Iteration 18020 Training loss 0.002003985457122326 Validation loss 0.05067817121744156 Accuracy 0.8784999847412109\n",
      "Iteration 18030 Training loss 0.0017538534011691809 Validation loss 0.05067535117268562 Accuracy 0.8784999847412109\n",
      "Iteration 18040 Training loss 0.002253478392958641 Validation loss 0.050675105303525925 Accuracy 0.8784999847412109\n",
      "Iteration 18050 Training loss 0.0015038209967315197 Validation loss 0.05068052187561989 Accuracy 0.8774999976158142\n",
      "Iteration 18060 Training loss 0.0017538039246574044 Validation loss 0.05067432299256325 Accuracy 0.8784999847412109\n",
      "Iteration 18070 Training loss 0.002253912156447768 Validation loss 0.05068659037351608 Accuracy 0.878000020980835\n",
      "Iteration 18080 Training loss 0.0022536583710461855 Validation loss 0.050683557987213135 Accuracy 0.8784999847412109\n",
      "Iteration 18090 Training loss 0.0017535188235342503 Validation loss 0.050683535635471344 Accuracy 0.8784999847412109\n",
      "Iteration 18100 Training loss 0.0012538677547127008 Validation loss 0.0506964772939682 Accuracy 0.8784999847412109\n",
      "Iteration 18110 Training loss 0.0017538531683385372 Validation loss 0.05069177597761154 Accuracy 0.8784999847412109\n",
      "Iteration 18120 Training loss 0.0012534046545624733 Validation loss 0.05069005861878395 Accuracy 0.8784999847412109\n",
      "Iteration 18130 Training loss 0.003003485966473818 Validation loss 0.05069555714726448 Accuracy 0.8784999847412109\n",
      "Iteration 18140 Training loss 0.0032538981176912785 Validation loss 0.05070316791534424 Accuracy 0.8784999847412109\n",
      "Iteration 18150 Training loss 0.00200362759642303 Validation loss 0.05069051682949066 Accuracy 0.8784999847412109\n",
      "Iteration 18160 Training loss 0.001003626617603004 Validation loss 0.050698600709438324 Accuracy 0.8784999847412109\n",
      "Iteration 18170 Training loss 0.0020039533264935017 Validation loss 0.05070527642965317 Accuracy 0.8784999847412109\n",
      "Iteration 18180 Training loss 0.0015038003912195563 Validation loss 0.0507025383412838 Accuracy 0.8784999847412109\n",
      "Iteration 18190 Training loss 0.0015037490520626307 Validation loss 0.05070074647665024 Accuracy 0.8784999847412109\n",
      "Iteration 18200 Training loss 0.002253948710858822 Validation loss 0.05069943889975548 Accuracy 0.8784999847412109\n",
      "Iteration 18210 Training loss 0.0015036053955554962 Validation loss 0.05069277063012123 Accuracy 0.8784999847412109\n",
      "Iteration 18220 Training loss 0.0012534327106550336 Validation loss 0.05069150775671005 Accuracy 0.878000020980835\n",
      "Iteration 18230 Training loss 0.0010041121859103441 Validation loss 0.05069596692919731 Accuracy 0.8784999847412109\n",
      "Iteration 18240 Training loss 0.0017529273172840476 Validation loss 0.05069389194250107 Accuracy 0.878000020980835\n",
      "Iteration 18250 Training loss 0.0010035814484581351 Validation loss 0.050696056336164474 Accuracy 0.8784999847412109\n",
      "Iteration 18260 Training loss 0.0025029266253113747 Validation loss 0.050693899393081665 Accuracy 0.8784999847412109\n",
      "Iteration 18270 Training loss 0.0020035451743751764 Validation loss 0.05070527642965317 Accuracy 0.8784999847412109\n",
      "Iteration 18280 Training loss 0.0020036089699715376 Validation loss 0.05070342868566513 Accuracy 0.878000020980835\n",
      "Iteration 18290 Training loss 0.0017535228980705142 Validation loss 0.05071457847952843 Accuracy 0.878000020980835\n",
      "Iteration 18300 Training loss 0.0010035423329100013 Validation loss 0.050697240978479385 Accuracy 0.878000020980835\n",
      "Iteration 18310 Training loss 0.0020033291075378656 Validation loss 0.05068998783826828 Accuracy 0.878000020980835\n",
      "Iteration 18320 Training loss 0.0017537262756377459 Validation loss 0.05070056393742561 Accuracy 0.878000020980835\n",
      "Iteration 18330 Training loss 0.002253244398161769 Validation loss 0.050703272223472595 Accuracy 0.878000020980835\n",
      "Iteration 18340 Training loss 0.0007544114487245679 Validation loss 0.05070197954773903 Accuracy 0.878000020980835\n",
      "Iteration 18350 Training loss 0.0015037264674901962 Validation loss 0.0507030263543129 Accuracy 0.878000020980835\n",
      "Iteration 18360 Training loss 0.0015039589488878846 Validation loss 0.05070814490318298 Accuracy 0.878000020980835\n",
      "Iteration 18370 Training loss 0.0007535081822425127 Validation loss 0.05071942135691643 Accuracy 0.878000020980835\n",
      "Iteration 18380 Training loss 0.002003311412408948 Validation loss 0.05070864036679268 Accuracy 0.878000020980835\n",
      "Iteration 18390 Training loss 0.002253396436572075 Validation loss 0.05070606246590614 Accuracy 0.878000020980835\n",
      "Iteration 18400 Training loss 0.0010029234690591693 Validation loss 0.050714243203401566 Accuracy 0.8784999847412109\n",
      "Iteration 18410 Training loss 0.0015037907287478447 Validation loss 0.050717975944280624 Accuracy 0.8784999847412109\n",
      "Iteration 18420 Training loss 0.0017536255763843656 Validation loss 0.050730157643556595 Accuracy 0.8784999847412109\n",
      "Iteration 18430 Training loss 0.0022533871233463287 Validation loss 0.050723616033792496 Accuracy 0.878000020980835\n",
      "Iteration 18440 Training loss 0.002004156121984124 Validation loss 0.050722505897283554 Accuracy 0.878000020980835\n",
      "Iteration 18450 Training loss 0.0022537116892635822 Validation loss 0.05072230473160744 Accuracy 0.8784999847412109\n",
      "Iteration 18460 Training loss 0.0015031244838610291 Validation loss 0.0507228784263134 Accuracy 0.878000020980835\n",
      "Iteration 18470 Training loss 0.0020037516951560974 Validation loss 0.05072653666138649 Accuracy 0.8784999847412109\n",
      "Iteration 18480 Training loss 0.0007537754136137664 Validation loss 0.05072510614991188 Accuracy 0.878000020980835\n",
      "Iteration 18490 Training loss 0.0020033393520861864 Validation loss 0.05073244497179985 Accuracy 0.878000020980835\n",
      "Iteration 18500 Training loss 0.0010036122985184193 Validation loss 0.050731733441352844 Accuracy 0.878000020980835\n",
      "Iteration 18510 Training loss 0.0017535923980176449 Validation loss 0.05073925107717514 Accuracy 0.8784999847412109\n",
      "Iteration 18520 Training loss 0.001752685522660613 Validation loss 0.05072823166847229 Accuracy 0.8784999847412109\n",
      "Iteration 18530 Training loss 0.0020034851040691137 Validation loss 0.05073783919215202 Accuracy 0.8784999847412109\n",
      "Iteration 18540 Training loss 0.002503707306459546 Validation loss 0.0507320761680603 Accuracy 0.878000020980835\n",
      "Iteration 18550 Training loss 0.003003115765750408 Validation loss 0.0507257878780365 Accuracy 0.878000020980835\n",
      "Iteration 18560 Training loss 0.0017536318628117442 Validation loss 0.05072873830795288 Accuracy 0.878000020980835\n",
      "Iteration 18570 Training loss 0.0017533386126160622 Validation loss 0.050733551383018494 Accuracy 0.878000020980835\n",
      "Iteration 18580 Training loss 0.002003225265070796 Validation loss 0.05073314160108566 Accuracy 0.878000020980835\n",
      "Iteration 18590 Training loss 0.002503836527466774 Validation loss 0.05073660984635353 Accuracy 0.8774999976158142\n",
      "Iteration 18600 Training loss 0.0017538941465318203 Validation loss 0.0507274828851223 Accuracy 0.878000020980835\n",
      "Iteration 18610 Training loss 0.002503402531147003 Validation loss 0.05072580277919769 Accuracy 0.878000020980835\n",
      "Iteration 18620 Training loss 0.0015026871114969254 Validation loss 0.05073315277695656 Accuracy 0.878000020980835\n",
      "Iteration 18630 Training loss 0.00275376602075994 Validation loss 0.05073842778801918 Accuracy 0.878000020980835\n",
      "Iteration 18640 Training loss 0.001753513584844768 Validation loss 0.0507359616458416 Accuracy 0.878000020980835\n",
      "Iteration 18650 Training loss 0.0027532526291906834 Validation loss 0.0507429875433445 Accuracy 0.8784999847412109\n",
      "Iteration 18660 Training loss 0.0017531134653836489 Validation loss 0.050743646919727325 Accuracy 0.8784999847412109\n",
      "Iteration 18670 Training loss 0.001753591583110392 Validation loss 0.050748467445373535 Accuracy 0.8784999847412109\n",
      "Iteration 18680 Training loss 0.0012537689181044698 Validation loss 0.05074312537908554 Accuracy 0.8784999847412109\n",
      "Iteration 18690 Training loss 0.0022532513830810785 Validation loss 0.050744909793138504 Accuracy 0.8784999847412109\n",
      "Iteration 18700 Training loss 0.001753624645061791 Validation loss 0.050733841955661774 Accuracy 0.878000020980835\n",
      "Iteration 18710 Training loss 0.003003455465659499 Validation loss 0.05073629692196846 Accuracy 0.878000020980835\n",
      "Iteration 18720 Training loss 0.002253208076581359 Validation loss 0.0507427379488945 Accuracy 0.878000020980835\n",
      "Iteration 18730 Training loss 0.0027525541372597218 Validation loss 0.05074024200439453 Accuracy 0.878000020980835\n",
      "Iteration 18740 Training loss 0.0022530779242515564 Validation loss 0.05075328052043915 Accuracy 0.878000020980835\n",
      "Iteration 18750 Training loss 0.0012536103604361415 Validation loss 0.05077047273516655 Accuracy 0.8784999847412109\n",
      "Iteration 18760 Training loss 0.0010037552565336227 Validation loss 0.05076075717806816 Accuracy 0.878000020980835\n",
      "Iteration 18770 Training loss 0.001503197243437171 Validation loss 0.05075104534626007 Accuracy 0.878000020980835\n",
      "Iteration 18780 Training loss 0.0015037325210869312 Validation loss 0.05075357109308243 Accuracy 0.878000020980835\n",
      "Iteration 18790 Training loss 0.0027535611297935247 Validation loss 0.05074131116271019 Accuracy 0.878000020980835\n",
      "Iteration 18800 Training loss 0.0025030190590769053 Validation loss 0.050742682069540024 Accuracy 0.878000020980835\n",
      "Iteration 18810 Training loss 0.0020035712514072657 Validation loss 0.05075933411717415 Accuracy 0.878000020980835\n",
      "Iteration 18820 Training loss 0.001753359567373991 Validation loss 0.05076126381754875 Accuracy 0.878000020980835\n",
      "Iteration 18830 Training loss 0.002003417117521167 Validation loss 0.05077043175697327 Accuracy 0.878000020980835\n",
      "Iteration 18840 Training loss 0.0005034724017605186 Validation loss 0.05076868459582329 Accuracy 0.878000020980835\n",
      "Iteration 18850 Training loss 0.002252039033919573 Validation loss 0.05075459182262421 Accuracy 0.878000020980835\n",
      "Iteration 18860 Training loss 0.0025034856516867876 Validation loss 0.05076632648706436 Accuracy 0.878000020980835\n",
      "Iteration 18870 Training loss 0.002253764308989048 Validation loss 0.05077402666211128 Accuracy 0.878000020980835\n",
      "Iteration 18880 Training loss 0.0012528887018561363 Validation loss 0.05078016594052315 Accuracy 0.8784999847412109\n",
      "Iteration 18890 Training loss 0.0020030562300235033 Validation loss 0.05078070983290672 Accuracy 0.8784999847412109\n",
      "Iteration 18900 Training loss 0.0025032793637365103 Validation loss 0.05078459903597832 Accuracy 0.8774999976158142\n",
      "Iteration 18910 Training loss 0.0032533579505980015 Validation loss 0.050788089632987976 Accuracy 0.878000020980835\n",
      "Iteration 18920 Training loss 0.0012522873003035784 Validation loss 0.050782207399606705 Accuracy 0.878000020980835\n",
      "Iteration 18930 Training loss 0.0020031738094985485 Validation loss 0.050800979137420654 Accuracy 0.8784999847412109\n",
      "Iteration 18940 Training loss 0.002502196468412876 Validation loss 0.05080561712384224 Accuracy 0.8784999847412109\n",
      "Iteration 18950 Training loss 0.0015033211093395948 Validation loss 0.050807759165763855 Accuracy 0.8784999847412109\n",
      "Iteration 18960 Training loss 0.0015036881668493152 Validation loss 0.05079393833875656 Accuracy 0.878000020980835\n",
      "Iteration 18970 Training loss 0.001752423937432468 Validation loss 0.05081147700548172 Accuracy 0.878000020980835\n",
      "Iteration 18980 Training loss 0.0020020860247313976 Validation loss 0.05083815008401871 Accuracy 0.8784999847412109\n",
      "Iteration 18990 Training loss 0.001003120094537735 Validation loss 0.05083567649126053 Accuracy 0.8784999847412109\n",
      "Iteration 19000 Training loss 0.0012540221214294434 Validation loss 0.05085887759923935 Accuracy 0.8784999847412109\n",
      "Iteration 19010 Training loss 0.001753428252413869 Validation loss 0.05083443969488144 Accuracy 0.8784999847412109\n",
      "Iteration 19020 Training loss 0.0012535136193037033 Validation loss 0.050918836146593094 Accuracy 0.8774999976158142\n",
      "Iteration 19030 Training loss 0.0012533084955066442 Validation loss 0.05095961317420006 Accuracy 0.8769999742507935\n",
      "Iteration 19040 Training loss 0.0010044225491583347 Validation loss 0.05100783705711365 Accuracy 0.878000020980835\n",
      "Iteration 19050 Training loss 0.0022545477841049433 Validation loss 0.051082298159599304 Accuracy 0.8774999976158142\n",
      "Iteration 19060 Training loss 0.002004774520173669 Validation loss 0.051300499588251114 Accuracy 0.8759999871253967\n",
      "Iteration 19070 Training loss 0.0020114860963076353 Validation loss 0.051492709666490555 Accuracy 0.8759999871253967\n",
      "Iteration 19080 Training loss 0.001754772150889039 Validation loss 0.051208287477493286 Accuracy 0.8774999976158142\n",
      "Iteration 19090 Training loss 0.0017600309802219272 Validation loss 0.05105213820934296 Accuracy 0.8765000104904175\n",
      "Iteration 19100 Training loss 0.0015045437030494213 Validation loss 0.05104605108499527 Accuracy 0.8765000104904175\n",
      "Iteration 19110 Training loss 0.0020076469518244267 Validation loss 0.05097939819097519 Accuracy 0.8769999742507935\n",
      "Iteration 19120 Training loss 0.0012568566016852856 Validation loss 0.05094809830188751 Accuracy 0.8774999976158142\n",
      "Iteration 19130 Training loss 0.001004880527034402 Validation loss 0.050884611904621124 Accuracy 0.878000020980835\n",
      "Iteration 19140 Training loss 0.002004555892199278 Validation loss 0.05093831941485405 Accuracy 0.8769999742507935\n",
      "Iteration 19150 Training loss 0.001754983444698155 Validation loss 0.05091032758355141 Accuracy 0.8769999742507935\n",
      "Iteration 19160 Training loss 0.0025045452639460564 Validation loss 0.05085986480116844 Accuracy 0.8769999742507935\n",
      "Iteration 19170 Training loss 0.0027571560349315405 Validation loss 0.05087801441550255 Accuracy 0.8769999742507935\n",
      "Iteration 19180 Training loss 0.0012565774377435446 Validation loss 0.050881702452898026 Accuracy 0.8769999742507935\n",
      "Iteration 19190 Training loss 0.003254891838878393 Validation loss 0.050868019461631775 Accuracy 0.8765000104904175\n",
      "Iteration 19200 Training loss 0.0010040135821327567 Validation loss 0.05086030438542366 Accuracy 0.8765000104904175\n",
      "Iteration 19210 Training loss 0.002255219267681241 Validation loss 0.05087852478027344 Accuracy 0.8769999742507935\n",
      "Iteration 19220 Training loss 0.0015046787448227406 Validation loss 0.050857655704021454 Accuracy 0.8765000104904175\n",
      "Iteration 19230 Training loss 0.0010046818060800433 Validation loss 0.05083451047539711 Accuracy 0.8765000104904175\n",
      "Iteration 19240 Training loss 0.002254291670396924 Validation loss 0.050845760852098465 Accuracy 0.8765000104904175\n",
      "Iteration 19250 Training loss 0.0022554551251232624 Validation loss 0.0508306585252285 Accuracy 0.8765000104904175\n",
      "Iteration 19260 Training loss 0.0012562698684632778 Validation loss 0.05081799253821373 Accuracy 0.8765000104904175\n",
      "Iteration 19270 Training loss 0.0022540371865034103 Validation loss 0.05085419490933418 Accuracy 0.8765000104904175\n",
      "Iteration 19280 Training loss 0.0017550208140164614 Validation loss 0.05084811523556709 Accuracy 0.8765000104904175\n",
      "Iteration 19290 Training loss 0.001753891003318131 Validation loss 0.05086556449532509 Accuracy 0.8759999871253967\n",
      "Iteration 19300 Training loss 0.00050526880659163 Validation loss 0.05086450278759003 Accuracy 0.8769999742507935\n",
      "Iteration 19310 Training loss 0.0020036515779793262 Validation loss 0.05084902048110962 Accuracy 0.8765000104904175\n",
      "Iteration 19320 Training loss 0.002254713559523225 Validation loss 0.050867706537246704 Accuracy 0.8765000104904175\n",
      "Iteration 19330 Training loss 0.0022548362612724304 Validation loss 0.050866272300481796 Accuracy 0.8765000104904175\n",
      "Iteration 19340 Training loss 0.002504588570445776 Validation loss 0.05084121972322464 Accuracy 0.8765000104904175\n",
      "Iteration 19350 Training loss 0.0010043754009529948 Validation loss 0.050842199474573135 Accuracy 0.8765000104904175\n",
      "Iteration 19360 Training loss 0.0007543805404566228 Validation loss 0.05085165798664093 Accuracy 0.8765000104904175\n",
      "Iteration 19370 Training loss 0.0015039597637951374 Validation loss 0.05084551125764847 Accuracy 0.8765000104904175\n",
      "Iteration 19380 Training loss 0.003004038240760565 Validation loss 0.05086364597082138 Accuracy 0.8759999871253967\n",
      "Iteration 19390 Training loss 0.0012546117650344968 Validation loss 0.05084054917097092 Accuracy 0.8765000104904175\n",
      "Iteration 19400 Training loss 0.0025042162742465734 Validation loss 0.05084099993109703 Accuracy 0.8765000104904175\n",
      "Iteration 19410 Training loss 0.003004011930897832 Validation loss 0.05082649365067482 Accuracy 0.8765000104904175\n",
      "Iteration 19420 Training loss 0.002004459034651518 Validation loss 0.05083833634853363 Accuracy 0.8759999871253967\n",
      "Iteration 19430 Training loss 0.0015043399762362242 Validation loss 0.050851669162511826 Accuracy 0.8759999871253967\n",
      "Iteration 19440 Training loss 0.0017545371083542705 Validation loss 0.05084838718175888 Accuracy 0.8759999871253967\n",
      "Iteration 19450 Training loss 0.0015040187863633037 Validation loss 0.05083824321627617 Accuracy 0.8765000104904175\n",
      "Iteration 19460 Training loss 0.002754269167780876 Validation loss 0.05083190277218819 Accuracy 0.8765000104904175\n",
      "Iteration 19470 Training loss 0.002254370367154479 Validation loss 0.0508289597928524 Accuracy 0.8765000104904175\n",
      "Iteration 19480 Training loss 0.0012537341099232435 Validation loss 0.050845079123973846 Accuracy 0.8759999871253967\n",
      "Iteration 19490 Training loss 0.002004226902499795 Validation loss 0.050839003175497055 Accuracy 0.8759999871253967\n",
      "Iteration 19500 Training loss 0.002503729425370693 Validation loss 0.05083930864930153 Accuracy 0.8759999871253967\n",
      "Iteration 19510 Training loss 0.001753897056914866 Validation loss 0.05085129290819168 Accuracy 0.8759999871253967\n",
      "Iteration 19520 Training loss 0.0020043980330228806 Validation loss 0.050861380994319916 Accuracy 0.8759999871253967\n",
      "Iteration 19530 Training loss 0.0025037815794348717 Validation loss 0.05086192488670349 Accuracy 0.8759999871253967\n",
      "Iteration 19540 Training loss 0.0007545330445282161 Validation loss 0.05086005479097366 Accuracy 0.8759999871253967\n",
      "Iteration 19550 Training loss 0.0017543749418109655 Validation loss 0.050867125391960144 Accuracy 0.8759999871253967\n",
      "Iteration 19560 Training loss 0.0012538559967651963 Validation loss 0.05085737630724907 Accuracy 0.8759999871253967\n",
      "Iteration 19570 Training loss 0.001503610867075622 Validation loss 0.050850436091423035 Accuracy 0.8759999871253967\n",
      "Iteration 19580 Training loss 0.002003910019993782 Validation loss 0.05085253342986107 Accuracy 0.8759999871253967\n",
      "Iteration 19590 Training loss 0.0017536103259772062 Validation loss 0.05084776133298874 Accuracy 0.8759999871253967\n",
      "Iteration 19600 Training loss 0.0010039685294032097 Validation loss 0.050844255834817886 Accuracy 0.8759999871253967\n",
      "Iteration 19610 Training loss 0.0017543868161737919 Validation loss 0.050845179706811905 Accuracy 0.8759999871253967\n",
      "Iteration 19620 Training loss 0.002003659028559923 Validation loss 0.05086170509457588 Accuracy 0.8759999871253967\n",
      "Iteration 19630 Training loss 0.0012546989601105452 Validation loss 0.050859756767749786 Accuracy 0.8759999871253967\n",
      "Iteration 19640 Training loss 0.003253419417887926 Validation loss 0.05085244029760361 Accuracy 0.8759999871253967\n",
      "Iteration 19650 Training loss 0.002004217356443405 Validation loss 0.05084993690252304 Accuracy 0.8759999871253967\n",
      "Iteration 19660 Training loss 0.0020038485527038574 Validation loss 0.05085454508662224 Accuracy 0.8759999871253967\n",
      "Iteration 19670 Training loss 0.002253409009426832 Validation loss 0.05085281282663345 Accuracy 0.8759999871253967\n",
      "Iteration 19680 Training loss 0.0022536313626915216 Validation loss 0.050853412598371506 Accuracy 0.8759999871253967\n",
      "Iteration 19690 Training loss 0.0012536804424598813 Validation loss 0.05084028095006943 Accuracy 0.8765000104904175\n",
      "Iteration 19700 Training loss 0.0012537690345197916 Validation loss 0.05085546523332596 Accuracy 0.8759999871253967\n",
      "Iteration 19710 Training loss 0.002004319801926613 Validation loss 0.050871774554252625 Accuracy 0.8759999871253967\n",
      "Iteration 19720 Training loss 0.0015031497459858656 Validation loss 0.05087438225746155 Accuracy 0.8759999871253967\n",
      "Iteration 19730 Training loss 0.0015038051642477512 Validation loss 0.05086664482951164 Accuracy 0.8759999871253967\n",
      "Iteration 19740 Training loss 0.0012538990704342723 Validation loss 0.0508781298995018 Accuracy 0.8759999871253967\n",
      "Iteration 19750 Training loss 0.002003737259656191 Validation loss 0.05087023973464966 Accuracy 0.8759999871253967\n",
      "Iteration 19760 Training loss 0.0012543158372864127 Validation loss 0.05087162181735039 Accuracy 0.8759999871253967\n",
      "Iteration 19770 Training loss 0.003003467805683613 Validation loss 0.050874870270490646 Accuracy 0.8759999871253967\n",
      "Iteration 19780 Training loss 0.0022534294985234737 Validation loss 0.0508769229054451 Accuracy 0.8759999871253967\n",
      "Iteration 19790 Training loss 0.0017542748246341944 Validation loss 0.05087253451347351 Accuracy 0.8759999871253967\n",
      "Iteration 19800 Training loss 0.001253351685591042 Validation loss 0.05086108297109604 Accuracy 0.8765000104904175\n",
      "Iteration 19810 Training loss 0.0027532384265214205 Validation loss 0.05086914449930191 Accuracy 0.8765000104904175\n",
      "Iteration 19820 Training loss 0.0007536911289207637 Validation loss 0.05088777467608452 Accuracy 0.8759999871253967\n",
      "Iteration 19830 Training loss 0.001503212726674974 Validation loss 0.0508880652487278 Accuracy 0.8759999871253967\n",
      "Iteration 19840 Training loss 0.0015038395067676902 Validation loss 0.05089244991540909 Accuracy 0.8759999871253967\n",
      "Iteration 19850 Training loss 0.0017536390805616975 Validation loss 0.050893813371658325 Accuracy 0.8759999871253967\n",
      "Iteration 19860 Training loss 0.0025037992745637894 Validation loss 0.05088898539543152 Accuracy 0.8759999871253967\n",
      "Iteration 19870 Training loss 0.002004115143790841 Validation loss 0.050881531089544296 Accuracy 0.8765000104904175\n",
      "Iteration 19880 Training loss 0.0015043263556435704 Validation loss 0.0508929044008255 Accuracy 0.8759999871253967\n",
      "Iteration 19890 Training loss 0.0030037665273994207 Validation loss 0.05089171975851059 Accuracy 0.8759999871253967\n",
      "Iteration 19900 Training loss 0.0025039894971996546 Validation loss 0.05088838189840317 Accuracy 0.8765000104904175\n",
      "Iteration 19910 Training loss 0.0020038108341395855 Validation loss 0.05089399963617325 Accuracy 0.8765000104904175\n",
      "Iteration 19920 Training loss 0.002003046916797757 Validation loss 0.0509021058678627 Accuracy 0.8765000104904175\n",
      "Iteration 19930 Training loss 0.001003383775241673 Validation loss 0.050906479358673096 Accuracy 0.8765000104904175\n",
      "Iteration 19940 Training loss 0.0012542776530608535 Validation loss 0.050900813192129135 Accuracy 0.8765000104904175\n",
      "Iteration 19950 Training loss 0.0012537311995401978 Validation loss 0.050897080451250076 Accuracy 0.8765000104904175\n",
      "Iteration 19960 Training loss 0.0010037035681307316 Validation loss 0.0509110726416111 Accuracy 0.8765000104904175\n",
      "Iteration 19970 Training loss 0.0022538311313837767 Validation loss 0.05090733617544174 Accuracy 0.8765000104904175\n",
      "Iteration 19980 Training loss 0.001503717852756381 Validation loss 0.05091196671128273 Accuracy 0.8765000104904175\n",
      "Iteration 19990 Training loss 0.0022536683827638626 Validation loss 0.050910208374261856 Accuracy 0.8765000104904175\n",
      "Iteration 20000 Training loss 0.0012540117604658008 Validation loss 0.05090843141078949 Accuracy 0.8765000104904175\n",
      "Iteration 20010 Training loss 0.0020035794004797935 Validation loss 0.050910066813230515 Accuracy 0.8765000104904175\n",
      "Iteration 20020 Training loss 0.001003728830255568 Validation loss 0.05091065913438797 Accuracy 0.8765000104904175\n",
      "Iteration 20030 Training loss 0.0012534046545624733 Validation loss 0.05090625584125519 Accuracy 0.8765000104904175\n",
      "Iteration 20040 Training loss 0.003503236686810851 Validation loss 0.050891853868961334 Accuracy 0.8765000104904175\n",
      "Iteration 20050 Training loss 0.001253599999472499 Validation loss 0.05090318247675896 Accuracy 0.8765000104904175\n",
      "Iteration 20060 Training loss 0.0015040187863633037 Validation loss 0.050902657210826874 Accuracy 0.8765000104904175\n",
      "Iteration 20070 Training loss 0.0017535515362396836 Validation loss 0.05091625079512596 Accuracy 0.8765000104904175\n",
      "Iteration 20080 Training loss 0.0030031937640160322 Validation loss 0.050918545573949814 Accuracy 0.8765000104904175\n",
      "Iteration 20090 Training loss 0.002753873821347952 Validation loss 0.050924886018037796 Accuracy 0.8765000104904175\n",
      "Iteration 20100 Training loss 0.0015038585988804698 Validation loss 0.050918206572532654 Accuracy 0.8765000104904175\n",
      "Iteration 20110 Training loss 0.002003108849748969 Validation loss 0.05092813819646835 Accuracy 0.8765000104904175\n",
      "Iteration 20120 Training loss 0.0020033582113683224 Validation loss 0.050926126539707184 Accuracy 0.8765000104904175\n",
      "Iteration 20130 Training loss 0.0015036986442282796 Validation loss 0.050933342427015305 Accuracy 0.8765000104904175\n",
      "Iteration 20140 Training loss 0.0020033693872392178 Validation loss 0.05093536153435707 Accuracy 0.8765000104904175\n",
      "Iteration 20150 Training loss 0.0005039392854087055 Validation loss 0.05093011632561684 Accuracy 0.8765000104904175\n",
      "Iteration 20160 Training loss 0.0027536703273653984 Validation loss 0.05092887580394745 Accuracy 0.8765000104904175\n",
      "Iteration 20170 Training loss 0.0025031401310116053 Validation loss 0.05092788115143776 Accuracy 0.8765000104904175\n",
      "Iteration 20180 Training loss 0.0015040829312056303 Validation loss 0.050919219851493835 Accuracy 0.8765000104904175\n",
      "Iteration 20190 Training loss 0.001753264688886702 Validation loss 0.050918515771627426 Accuracy 0.8765000104904175\n",
      "Iteration 20200 Training loss 0.0007535179029218853 Validation loss 0.05091983452439308 Accuracy 0.8765000104904175\n",
      "Iteration 20210 Training loss 0.0010038726031780243 Validation loss 0.05091835930943489 Accuracy 0.8765000104904175\n",
      "Iteration 20220 Training loss 0.0020034974440932274 Validation loss 0.050925031304359436 Accuracy 0.8765000104904175\n",
      "Iteration 20230 Training loss 0.0012538692681118846 Validation loss 0.05092606320977211 Accuracy 0.8765000104904175\n",
      "Iteration 20240 Training loss 0.0015034418320283294 Validation loss 0.05093022435903549 Accuracy 0.8765000104904175\n",
      "Iteration 20250 Training loss 0.0015030489303171635 Validation loss 0.05093096196651459 Accuracy 0.8765000104904175\n",
      "Iteration 20260 Training loss 0.002753472188487649 Validation loss 0.05093393102288246 Accuracy 0.8765000104904175\n",
      "Iteration 20270 Training loss 0.0015032319352030754 Validation loss 0.05093329772353172 Accuracy 0.8765000104904175\n",
      "Iteration 20280 Training loss 0.0015030658105388284 Validation loss 0.050929825752973557 Accuracy 0.8765000104904175\n",
      "Iteration 20290 Training loss 0.002253857674077153 Validation loss 0.05093017965555191 Accuracy 0.8765000104904175\n",
      "Iteration 20300 Training loss 0.0015041299629956484 Validation loss 0.05092790350317955 Accuracy 0.8765000104904175\n",
      "Iteration 20310 Training loss 0.0017532208003103733 Validation loss 0.05093187093734741 Accuracy 0.8765000104904175\n",
      "Iteration 20320 Training loss 0.0017536166124045849 Validation loss 0.050934016704559326 Accuracy 0.8765000104904175\n",
      "Iteration 20330 Training loss 0.0010039048502221704 Validation loss 0.050940919667482376 Accuracy 0.8765000104904175\n",
      "Iteration 20340 Training loss 0.002753881039097905 Validation loss 0.05094524472951889 Accuracy 0.8765000104904175\n",
      "Iteration 20350 Training loss 0.002253369428217411 Validation loss 0.05095168203115463 Accuracy 0.8765000104904175\n",
      "Iteration 20360 Training loss 0.001003861310891807 Validation loss 0.05094754695892334 Accuracy 0.8765000104904175\n",
      "Iteration 20370 Training loss 0.0012538882438093424 Validation loss 0.05094438046216965 Accuracy 0.8765000104904175\n",
      "Iteration 20380 Training loss 0.0025040749460458755 Validation loss 0.05094466730952263 Accuracy 0.8765000104904175\n",
      "Iteration 20390 Training loss 0.0022532460279762745 Validation loss 0.050934650003910065 Accuracy 0.8765000104904175\n",
      "Iteration 20400 Training loss 0.0017533161444589496 Validation loss 0.05093333125114441 Accuracy 0.8765000104904175\n",
      "Iteration 20410 Training loss 0.0017536289524286985 Validation loss 0.05093678832054138 Accuracy 0.8765000104904175\n",
      "Iteration 20420 Training loss 0.0025033666752278805 Validation loss 0.0509561225771904 Accuracy 0.8765000104904175\n",
      "Iteration 20430 Training loss 0.0020030781161040068 Validation loss 0.05095384269952774 Accuracy 0.8765000104904175\n",
      "Iteration 20440 Training loss 0.0020032634492963552 Validation loss 0.05095496028661728 Accuracy 0.8765000104904175\n",
      "Iteration 20450 Training loss 0.002503321273252368 Validation loss 0.050947897136211395 Accuracy 0.8765000104904175\n",
      "Iteration 20460 Training loss 0.001503754872828722 Validation loss 0.05094953998923302 Accuracy 0.8769999742507935\n",
      "Iteration 20470 Training loss 0.0005035279900766909 Validation loss 0.05095294862985611 Accuracy 0.8769999742507935\n",
      "Iteration 20480 Training loss 0.0025034365244209766 Validation loss 0.050942957401275635 Accuracy 0.8769999742507935\n",
      "Iteration 20490 Training loss 0.00175333337392658 Validation loss 0.050941772758960724 Accuracy 0.8769999742507935\n",
      "Iteration 20500 Training loss 0.001753411372192204 Validation loss 0.05094949156045914 Accuracy 0.8769999742507935\n",
      "Iteration 20510 Training loss 0.0012532566906884313 Validation loss 0.05094355344772339 Accuracy 0.8769999742507935\n",
      "Iteration 20520 Training loss 0.0012537826551124454 Validation loss 0.05094384774565697 Accuracy 0.8769999742507935\n",
      "Iteration 20530 Training loss 0.0025032833218574524 Validation loss 0.0509432815015316 Accuracy 0.8769999742507935\n",
      "Iteration 20540 Training loss 0.001503797248005867 Validation loss 0.050938062369823456 Accuracy 0.8769999742507935\n",
      "Iteration 20550 Training loss 0.002003811299800873 Validation loss 0.05094120651483536 Accuracy 0.8769999742507935\n",
      "Iteration 20560 Training loss 0.0027532135136425495 Validation loss 0.050951529294252396 Accuracy 0.8769999742507935\n",
      "Iteration 20570 Training loss 0.001503316918388009 Validation loss 0.050953298807144165 Accuracy 0.8769999742507935\n",
      "Iteration 20580 Training loss 0.0017529764445498586 Validation loss 0.05095134302973747 Accuracy 0.8769999742507935\n",
      "Iteration 20590 Training loss 0.0010032720165327191 Validation loss 0.0509476512670517 Accuracy 0.8769999742507935\n",
      "Iteration 20600 Training loss 0.0017534298822283745 Validation loss 0.05095275864005089 Accuracy 0.8769999742507935\n",
      "Iteration 20610 Training loss 0.002252619480714202 Validation loss 0.050954803824424744 Accuracy 0.8769999742507935\n",
      "Iteration 20620 Training loss 0.0012529176892712712 Validation loss 0.05094889923930168 Accuracy 0.8769999742507935\n",
      "Iteration 20630 Training loss 0.002503129420801997 Validation loss 0.05094296112656593 Accuracy 0.8769999742507935\n",
      "Iteration 20640 Training loss 0.001503672101534903 Validation loss 0.0509549081325531 Accuracy 0.8769999742507935\n",
      "Iteration 20650 Training loss 0.002253076061606407 Validation loss 0.050955675542354584 Accuracy 0.8769999742507935\n",
      "Iteration 20660 Training loss 0.0015033703530207276 Validation loss 0.05095241591334343 Accuracy 0.8769999742507935\n",
      "Iteration 20670 Training loss 0.0015034080715849996 Validation loss 0.05095604807138443 Accuracy 0.8769999742507935\n",
      "Iteration 20680 Training loss 0.002753454726189375 Validation loss 0.05096335709095001 Accuracy 0.8769999742507935\n",
      "Iteration 20690 Training loss 0.001753058284521103 Validation loss 0.05096187815070152 Accuracy 0.8769999742507935\n",
      "Iteration 20700 Training loss 0.001752756885252893 Validation loss 0.050959229469299316 Accuracy 0.8769999742507935\n",
      "Iteration 20710 Training loss 0.0017537277890369296 Validation loss 0.050962723791599274 Accuracy 0.8769999742507935\n",
      "Iteration 20720 Training loss 0.001252825022675097 Validation loss 0.05096016824245453 Accuracy 0.8769999742507935\n",
      "Iteration 20730 Training loss 0.00025294069200754166 Validation loss 0.05096694827079773 Accuracy 0.8769999742507935\n",
      "Iteration 20740 Training loss 0.0017532550264149904 Validation loss 0.05096855014562607 Accuracy 0.8769999742507935\n",
      "Iteration 20750 Training loss 0.001253472175449133 Validation loss 0.050961315631866455 Accuracy 0.8769999742507935\n",
      "Iteration 20760 Training loss 0.002002962864935398 Validation loss 0.05096113681793213 Accuracy 0.8769999742507935\n",
      "Iteration 20770 Training loss 0.0010036425665020943 Validation loss 0.050961654633283615 Accuracy 0.8769999742507935\n",
      "Iteration 20780 Training loss 0.0020037656649947166 Validation loss 0.05095309391617775 Accuracy 0.8769999742507935\n",
      "Iteration 20790 Training loss 0.00250385538674891 Validation loss 0.050953369587659836 Accuracy 0.8769999742507935\n",
      "Iteration 20800 Training loss 0.0012534128036350012 Validation loss 0.05096147581934929 Accuracy 0.8769999742507935\n",
      "Iteration 20810 Training loss 0.002003622939810157 Validation loss 0.0509561263024807 Accuracy 0.8769999742507935\n",
      "Iteration 20820 Training loss 0.0020033973269164562 Validation loss 0.0509670190513134 Accuracy 0.8769999742507935\n",
      "Iteration 20830 Training loss 0.0027532896492630243 Validation loss 0.05096156895160675 Accuracy 0.8769999742507935\n",
      "Iteration 20840 Training loss 0.0015030991053208709 Validation loss 0.050968948751688004 Accuracy 0.8769999742507935\n",
      "Iteration 20850 Training loss 0.001253579743206501 Validation loss 0.05096739903092384 Accuracy 0.8769999742507935\n",
      "Iteration 20860 Training loss 0.002253212034702301 Validation loss 0.05097510293126106 Accuracy 0.8769999742507935\n",
      "Iteration 20870 Training loss 0.0012541547184810042 Validation loss 0.05096865072846413 Accuracy 0.8769999742507935\n",
      "Iteration 20880 Training loss 0.0022533661685884 Validation loss 0.05096518620848656 Accuracy 0.8769999742507935\n",
      "Iteration 20890 Training loss 0.0012540493626147509 Validation loss 0.05097038298845291 Accuracy 0.8769999742507935\n",
      "Iteration 20900 Training loss 0.0025033855345100164 Validation loss 0.05097566545009613 Accuracy 0.8769999742507935\n",
      "Iteration 20910 Training loss 0.0022537605836987495 Validation loss 0.050968628376722336 Accuracy 0.8769999742507935\n",
      "Iteration 20920 Training loss 0.0012531430693343282 Validation loss 0.05096534639596939 Accuracy 0.8769999742507935\n",
      "Iteration 20930 Training loss 0.002003657165914774 Validation loss 0.05097376927733421 Accuracy 0.8769999742507935\n",
      "Iteration 20940 Training loss 0.0015030648792162538 Validation loss 0.05098527669906616 Accuracy 0.8774999976158142\n",
      "Iteration 20950 Training loss 0.002253087470307946 Validation loss 0.050982389599084854 Accuracy 0.8774999976158142\n",
      "Iteration 20960 Training loss 0.002002839231863618 Validation loss 0.05097264423966408 Accuracy 0.8769999742507935\n",
      "Iteration 20970 Training loss 0.0030027914326637983 Validation loss 0.050967928022146225 Accuracy 0.8769999742507935\n",
      "Iteration 20980 Training loss 0.0007531099254265428 Validation loss 0.0509764589369297 Accuracy 0.8769999742507935\n",
      "Iteration 20990 Training loss 0.0015030890936031938 Validation loss 0.05096939206123352 Accuracy 0.8769999742507935\n",
      "Iteration 21000 Training loss 0.0027523222379386425 Validation loss 0.05096327140927315 Accuracy 0.8769999742507935\n",
      "Iteration 21010 Training loss 0.0015035715186968446 Validation loss 0.0509575717151165 Accuracy 0.8769999742507935\n",
      "Iteration 21020 Training loss 0.0015032555675134063 Validation loss 0.05095452442765236 Accuracy 0.8769999742507935\n",
      "Iteration 21030 Training loss 0.0022534779272973537 Validation loss 0.050955504179000854 Accuracy 0.8769999742507935\n",
      "Iteration 21040 Training loss 0.0025029010139405727 Validation loss 0.050963278859853745 Accuracy 0.8769999742507935\n",
      "Iteration 21050 Training loss 0.003003380261361599 Validation loss 0.0509670227766037 Accuracy 0.8769999742507935\n",
      "Iteration 21060 Training loss 0.0017531998455524445 Validation loss 0.05098061263561249 Accuracy 0.8769999742507935\n",
      "Iteration 21070 Training loss 0.0002534715458750725 Validation loss 0.05098407715559006 Accuracy 0.8769999742507935\n",
      "Iteration 21080 Training loss 0.0025036653969436884 Validation loss 0.05097679793834686 Accuracy 0.8769999742507935\n",
      "Iteration 21090 Training loss 0.002502942457795143 Validation loss 0.0509803406894207 Accuracy 0.8769999742507935\n",
      "Iteration 21100 Training loss 0.0022528728004544973 Validation loss 0.050973646342754364 Accuracy 0.8769999742507935\n",
      "Iteration 21110 Training loss 0.0007533601601608098 Validation loss 0.05097506567835808 Accuracy 0.8769999742507935\n",
      "Iteration 21120 Training loss 0.0007535028271377087 Validation loss 0.050979919731616974 Accuracy 0.8769999742507935\n",
      "Iteration 21130 Training loss 0.0010029857512563467 Validation loss 0.050983037799596786 Accuracy 0.8769999742507935\n",
      "Iteration 21140 Training loss 0.0017536347731947899 Validation loss 0.05097964033484459 Accuracy 0.8769999742507935\n",
      "Iteration 21150 Training loss 0.002253204584121704 Validation loss 0.05097582936286926 Accuracy 0.8769999742507935\n",
      "Iteration 21160 Training loss 0.0020035249181091785 Validation loss 0.05097052454948425 Accuracy 0.8769999742507935\n",
      "Iteration 21170 Training loss 0.0017531835474073887 Validation loss 0.0509689599275589 Accuracy 0.8769999742507935\n",
      "Iteration 21180 Training loss 0.0007532049785368145 Validation loss 0.05096828565001488 Accuracy 0.8769999742507935\n",
      "Iteration 21190 Training loss 0.001753251883201301 Validation loss 0.05097690597176552 Accuracy 0.8769999742507935\n",
      "Iteration 21200 Training loss 0.0025027103256434202 Validation loss 0.05097505450248718 Accuracy 0.8769999742507935\n",
      "Iteration 21210 Training loss 0.001003467128612101 Validation loss 0.05098968744277954 Accuracy 0.8769999742507935\n",
      "Iteration 21220 Training loss 0.0020035775378346443 Validation loss 0.05098157376050949 Accuracy 0.8769999742507935\n",
      "Iteration 21230 Training loss 0.0012529180385172367 Validation loss 0.05099301412701607 Accuracy 0.8769999742507935\n",
      "Iteration 21240 Training loss 0.0027525329496711493 Validation loss 0.05098642781376839 Accuracy 0.8769999742507935\n",
      "Iteration 21250 Training loss 0.002003398258239031 Validation loss 0.050982870161533356 Accuracy 0.8769999742507935\n",
      "Iteration 21260 Training loss 0.0020031204912811518 Validation loss 0.05097050964832306 Accuracy 0.8769999742507935\n",
      "Iteration 21270 Training loss 0.002003338420763612 Validation loss 0.05096016451716423 Accuracy 0.8769999742507935\n",
      "Iteration 21280 Training loss 0.0020027642603963614 Validation loss 0.050969626754522324 Accuracy 0.8769999742507935\n",
      "Iteration 21290 Training loss 0.001252798712812364 Validation loss 0.05097350478172302 Accuracy 0.8769999742507935\n",
      "Iteration 21300 Training loss 0.002003377303481102 Validation loss 0.05098414793610573 Accuracy 0.8769999742507935\n",
      "Iteration 21310 Training loss 0.001503176405094564 Validation loss 0.05098012462258339 Accuracy 0.8769999742507935\n",
      "Iteration 21320 Training loss 0.0010034989099949598 Validation loss 0.05099198967218399 Accuracy 0.8769999742507935\n",
      "Iteration 21330 Training loss 0.0017533834325149655 Validation loss 0.0509856715798378 Accuracy 0.8769999742507935\n",
      "Iteration 21340 Training loss 0.002252674661576748 Validation loss 0.05098828300833702 Accuracy 0.8769999742507935\n",
      "Iteration 21350 Training loss 0.002003598026931286 Validation loss 0.05098439007997513 Accuracy 0.8769999742507935\n",
      "Iteration 21360 Training loss 0.0017529453616589308 Validation loss 0.0509839728474617 Accuracy 0.8769999742507935\n",
      "Iteration 21370 Training loss 0.001253275666385889 Validation loss 0.05098455399274826 Accuracy 0.8769999742507935\n",
      "Iteration 21380 Training loss 0.002252709586173296 Validation loss 0.05098384991288185 Accuracy 0.8769999742507935\n",
      "Iteration 21390 Training loss 0.0015029124915599823 Validation loss 0.050988633185625076 Accuracy 0.8769999742507935\n",
      "Iteration 21400 Training loss 0.002253343816846609 Validation loss 0.050985343754291534 Accuracy 0.8769999742507935\n",
      "Iteration 21410 Training loss 0.002253052545711398 Validation loss 0.05099115148186684 Accuracy 0.8769999742507935\n",
      "Iteration 21420 Training loss 0.0017528635216876864 Validation loss 0.0509939081966877 Accuracy 0.8769999742507935\n",
      "Iteration 21430 Training loss 0.0017532214988023043 Validation loss 0.05099323391914368 Accuracy 0.8769999742507935\n",
      "Iteration 21440 Training loss 0.002753084758296609 Validation loss 0.05099399387836456 Accuracy 0.8769999742507935\n",
      "Iteration 21450 Training loss 0.002002644119784236 Validation loss 0.05098775401711464 Accuracy 0.8769999742507935\n",
      "Iteration 21460 Training loss 0.0015031411312520504 Validation loss 0.050991304218769073 Accuracy 0.8769999742507935\n",
      "Iteration 21470 Training loss 0.0017534235958009958 Validation loss 0.05098588392138481 Accuracy 0.8769999742507935\n",
      "Iteration 21480 Training loss 0.0017528440803289413 Validation loss 0.050983939319849014 Accuracy 0.8769999742507935\n",
      "Iteration 21490 Training loss 0.0015028328634798527 Validation loss 0.05100321024656296 Accuracy 0.8769999742507935\n",
      "Iteration 21500 Training loss 0.002752841915935278 Validation loss 0.051013313233852386 Accuracy 0.8769999742507935\n",
      "Iteration 21510 Training loss 0.0020035565830767155 Validation loss 0.0510207824409008 Accuracy 0.8769999742507935\n",
      "Iteration 21520 Training loss 0.0010028232354670763 Validation loss 0.051023002713918686 Accuracy 0.8769999742507935\n",
      "Iteration 21530 Training loss 0.0005031051696278155 Validation loss 0.05101868510246277 Accuracy 0.8769999742507935\n",
      "Iteration 21540 Training loss 0.001253111520782113 Validation loss 0.05102493613958359 Accuracy 0.8769999742507935\n",
      "Iteration 21550 Training loss 0.002253116574138403 Validation loss 0.051024794578552246 Accuracy 0.8769999742507935\n",
      "Iteration 21560 Training loss 0.0020030299201607704 Validation loss 0.051019977778196335 Accuracy 0.8769999742507935\n",
      "Iteration 21570 Training loss 0.001253204420208931 Validation loss 0.05100598931312561 Accuracy 0.8769999742507935\n",
      "Iteration 21580 Training loss 0.0010029657278209925 Validation loss 0.051004860550165176 Accuracy 0.8769999742507935\n",
      "Iteration 21590 Training loss 0.0020028743892908096 Validation loss 0.05100812390446663 Accuracy 0.8769999742507935\n",
      "Iteration 21600 Training loss 0.0025027189403772354 Validation loss 0.05100240558385849 Accuracy 0.8769999742507935\n",
      "Iteration 21610 Training loss 0.0015033616218715906 Validation loss 0.051010582596063614 Accuracy 0.8769999742507935\n",
      "Iteration 21620 Training loss 0.001503094914369285 Validation loss 0.05100511014461517 Accuracy 0.8769999742507935\n",
      "Iteration 21630 Training loss 0.0022530362475663424 Validation loss 0.0510074719786644 Accuracy 0.8769999742507935\n",
      "Iteration 21640 Training loss 0.0025027021765708923 Validation loss 0.050999827682971954 Accuracy 0.8769999742507935\n",
      "Iteration 21650 Training loss 0.0022530925925821066 Validation loss 0.050998151302337646 Accuracy 0.8769999742507935\n",
      "Iteration 21660 Training loss 0.0015033022500574589 Validation loss 0.05100133270025253 Accuracy 0.8769999742507935\n",
      "Iteration 21670 Training loss 0.0010032040299847722 Validation loss 0.0509924553334713 Accuracy 0.8769999742507935\n",
      "Iteration 21680 Training loss 0.0020027882419526577 Validation loss 0.050997551530599594 Accuracy 0.8769999742507935\n",
      "Iteration 21690 Training loss 0.0022524953819811344 Validation loss 0.05100197345018387 Accuracy 0.8769999742507935\n",
      "Iteration 21700 Training loss 0.0025025049690157175 Validation loss 0.051006391644477844 Accuracy 0.8769999742507935\n",
      "Iteration 21710 Training loss 0.0017528817988932133 Validation loss 0.0510065071284771 Accuracy 0.8769999742507935\n",
      "Iteration 21720 Training loss 0.0025026940274983644 Validation loss 0.05100102350115776 Accuracy 0.8769999742507935\n",
      "Iteration 21730 Training loss 0.0012533309636637568 Validation loss 0.051011212170124054 Accuracy 0.8769999742507935\n",
      "Iteration 21740 Training loss 0.0017531347693875432 Validation loss 0.0510159470140934 Accuracy 0.8765000104904175\n",
      "Iteration 21750 Training loss 0.0022529764100909233 Validation loss 0.051024097949266434 Accuracy 0.8759999871253967\n",
      "Iteration 21760 Training loss 0.00225256965495646 Validation loss 0.05103597417473793 Accuracy 0.8759999871253967\n",
      "Iteration 21770 Training loss 0.0015026995679363608 Validation loss 0.051031894981861115 Accuracy 0.8759999871253967\n",
      "Iteration 21780 Training loss 0.002252735896036029 Validation loss 0.05102309584617615 Accuracy 0.8759999871253967\n",
      "Iteration 21790 Training loss 0.001752794487401843 Validation loss 0.051029060035943985 Accuracy 0.8759999871253967\n",
      "Iteration 21800 Training loss 0.001252446905709803 Validation loss 0.05102432146668434 Accuracy 0.8759999871253967\n",
      "Iteration 21810 Training loss 0.0010033254511654377 Validation loss 0.05101477727293968 Accuracy 0.8765000104904175\n",
      "Iteration 21820 Training loss 0.0020033943001180887 Validation loss 0.05101257562637329 Accuracy 0.8765000104904175\n",
      "Iteration 21830 Training loss 0.001253190916031599 Validation loss 0.051008064299821854 Accuracy 0.8769999742507935\n",
      "Iteration 21840 Training loss 0.0015026391483843327 Validation loss 0.05100969225168228 Accuracy 0.8769999742507935\n",
      "Iteration 21850 Training loss 0.0020029291044920683 Validation loss 0.05101441591978073 Accuracy 0.8769999742507935\n",
      "Iteration 21860 Training loss 0.0027530211955308914 Validation loss 0.05101969465613365 Accuracy 0.8769999742507935\n",
      "Iteration 21870 Training loss 0.0022529331035912037 Validation loss 0.05101090297102928 Accuracy 0.8769999742507935\n",
      "Iteration 21880 Training loss 0.0022526246029883623 Validation loss 0.051011595875024796 Accuracy 0.8769999742507935\n",
      "Iteration 21890 Training loss 0.002502400428056717 Validation loss 0.05101608857512474 Accuracy 0.8769999742507935\n",
      "Iteration 21900 Training loss 0.0015028116758912802 Validation loss 0.05102113261818886 Accuracy 0.8765000104904175\n",
      "Iteration 21910 Training loss 0.0030030885245651007 Validation loss 0.05103067681193352 Accuracy 0.8759999871253967\n",
      "Iteration 21920 Training loss 0.0007530181901529431 Validation loss 0.05102383345365524 Accuracy 0.8765000104904175\n",
      "Iteration 21930 Training loss 0.003252715105190873 Validation loss 0.05102375149726868 Accuracy 0.8759999871253967\n",
      "Iteration 21940 Training loss 0.0015027374029159546 Validation loss 0.05102923512458801 Accuracy 0.8759999871253967\n",
      "Iteration 21950 Training loss 0.002752376953139901 Validation loss 0.05102395638823509 Accuracy 0.8765000104904175\n",
      "Iteration 21960 Training loss 0.0032527681905776262 Validation loss 0.05102544277906418 Accuracy 0.8765000104904175\n",
      "Iteration 21970 Training loss 0.001752546289935708 Validation loss 0.0510263592004776 Accuracy 0.8765000104904175\n",
      "Iteration 21980 Training loss 0.0015028116758912802 Validation loss 0.051029425114393234 Accuracy 0.8769999742507935\n",
      "Iteration 21990 Training loss 0.001503066043369472 Validation loss 0.051032423973083496 Accuracy 0.8765000104904175\n",
      "Iteration 22000 Training loss 0.0020028972066938877 Validation loss 0.051034484058618546 Accuracy 0.8759999871253967\n",
      "Iteration 22010 Training loss 0.002502894029021263 Validation loss 0.05103209614753723 Accuracy 0.8765000104904175\n",
      "Iteration 22020 Training loss 0.002752866828814149 Validation loss 0.051021430641412735 Accuracy 0.8769999742507935\n",
      "Iteration 22030 Training loss 0.0032528399024158716 Validation loss 0.051027677953243256 Accuracy 0.8769999742507935\n",
      "Iteration 22040 Training loss 0.0020027889404445887 Validation loss 0.05102386325597763 Accuracy 0.8769999742507935\n",
      "Iteration 22050 Training loss 0.0025029703974723816 Validation loss 0.05102769285440445 Accuracy 0.8769999742507935\n",
      "Iteration 22060 Training loss 0.001753073069266975 Validation loss 0.05102788284420967 Accuracy 0.8765000104904175\n",
      "Iteration 22070 Training loss 0.002002722816541791 Validation loss 0.051025860011577606 Accuracy 0.8759999871253967\n",
      "Iteration 22080 Training loss 0.001752795185893774 Validation loss 0.051032766699790955 Accuracy 0.8759999871253967\n",
      "Iteration 22090 Training loss 0.0017522780690342188 Validation loss 0.05103616788983345 Accuracy 0.8769999742507935\n",
      "Iteration 22100 Training loss 0.0010028700344264507 Validation loss 0.05104304477572441 Accuracy 0.8759999871253967\n",
      "Iteration 22110 Training loss 0.0025025783106684685 Validation loss 0.051044363528490067 Accuracy 0.8759999871253967\n",
      "Iteration 22120 Training loss 0.0015032640658318996 Validation loss 0.051044441759586334 Accuracy 0.8759999871253967\n",
      "Iteration 22130 Training loss 0.001752710435539484 Validation loss 0.05104998126626015 Accuracy 0.8759999871253967\n",
      "Iteration 22140 Training loss 0.002003293251618743 Validation loss 0.05104401335120201 Accuracy 0.8759999871253967\n",
      "Iteration 22150 Training loss 0.0010027455864474177 Validation loss 0.05105002224445343 Accuracy 0.8759999871253967\n",
      "Iteration 22160 Training loss 0.0027527823112905025 Validation loss 0.05105091258883476 Accuracy 0.8759999871253967\n",
      "Iteration 22170 Training loss 0.0010023261420428753 Validation loss 0.05104993283748627 Accuracy 0.8759999871253967\n",
      "Iteration 22180 Training loss 0.0015025673201307654 Validation loss 0.051048047840595245 Accuracy 0.8759999871253967\n",
      "Iteration 22190 Training loss 0.002503221621736884 Validation loss 0.051037415862083435 Accuracy 0.8759999871253967\n",
      "Iteration 22200 Training loss 0.0017527170712128282 Validation loss 0.05104425922036171 Accuracy 0.8759999871253967\n",
      "Iteration 22210 Training loss 0.0017524700378999114 Validation loss 0.051033906638622284 Accuracy 0.8759999871253967\n",
      "Iteration 22220 Training loss 0.0027528190985322 Validation loss 0.05102650448679924 Accuracy 0.8765000104904175\n",
      "Iteration 22230 Training loss 0.0015028886264190078 Validation loss 0.051032986491918564 Accuracy 0.8765000104904175\n",
      "Iteration 22240 Training loss 0.0010028723627328873 Validation loss 0.0510462261736393 Accuracy 0.8759999871253967\n",
      "Iteration 22250 Training loss 0.0015029845526441932 Validation loss 0.05104999616742134 Accuracy 0.8759999871253967\n",
      "Iteration 22260 Training loss 0.0017526268493384123 Validation loss 0.051045000553131104 Accuracy 0.8765000104904175\n",
      "Iteration 22270 Training loss 0.0017525465227663517 Validation loss 0.05104563757777214 Accuracy 0.8759999871253967\n",
      "Iteration 22280 Training loss 0.00250226235948503 Validation loss 0.05104628577828407 Accuracy 0.8759999871253967\n",
      "Iteration 22290 Training loss 0.00200219196267426 Validation loss 0.05105052888393402 Accuracy 0.8759999871253967\n",
      "Iteration 22300 Training loss 0.0007530542789027095 Validation loss 0.05105036124587059 Accuracy 0.8759999871253967\n",
      "Iteration 22310 Training loss 0.0017526543233543634 Validation loss 0.051044680178165436 Accuracy 0.8759999871253967\n",
      "Iteration 22320 Training loss 0.0020026525016874075 Validation loss 0.05103880912065506 Accuracy 0.8759999871253967\n",
      "Iteration 22330 Training loss 0.002003129804506898 Validation loss 0.051035866141319275 Accuracy 0.8765000104904175\n",
      "Iteration 22340 Training loss 0.002752321306616068 Validation loss 0.051043327897787094 Accuracy 0.8765000104904175\n",
      "Iteration 22350 Training loss 0.0017526099691167474 Validation loss 0.05103616043925285 Accuracy 0.8759999871253967\n",
      "Iteration 22360 Training loss 0.0005033054039813578 Validation loss 0.05103857442736626 Accuracy 0.8759999871253967\n",
      "Iteration 22370 Training loss 0.0005025681457482278 Validation loss 0.05104469135403633 Accuracy 0.8759999871253967\n",
      "Iteration 22380 Training loss 0.002002932131290436 Validation loss 0.05104338005185127 Accuracy 0.8759999871253967\n",
      "Iteration 22390 Training loss 0.003002679906785488 Validation loss 0.05105038359761238 Accuracy 0.8759999871253967\n",
      "Iteration 22400 Training loss 0.001502973958849907 Validation loss 0.051049478352069855 Accuracy 0.8759999871253967\n",
      "Iteration 22410 Training loss 0.0022525270469486713 Validation loss 0.051042407751083374 Accuracy 0.8759999871253967\n",
      "Iteration 22420 Training loss 0.0002530451456550509 Validation loss 0.0510484017431736 Accuracy 0.8759999871253967\n",
      "Iteration 22430 Training loss 0.0017532489728182554 Validation loss 0.05105336010456085 Accuracy 0.8759999871253967\n",
      "Iteration 22440 Training loss 0.0025026388466358185 Validation loss 0.05105495825409889 Accuracy 0.8759999871253967\n",
      "Iteration 22450 Training loss 0.0012527147773653269 Validation loss 0.051054444164037704 Accuracy 0.8759999871253967\n",
      "Iteration 22460 Training loss 0.002752819564193487 Validation loss 0.0510585680603981 Accuracy 0.8759999871253967\n",
      "Iteration 22470 Training loss 0.0017530577024444938 Validation loss 0.051057904958724976 Accuracy 0.8759999871253967\n",
      "Iteration 22480 Training loss 0.0025025387294590473 Validation loss 0.05105604603886604 Accuracy 0.8759999871253967\n",
      "Iteration 22490 Training loss 0.001502431696280837 Validation loss 0.05106062442064285 Accuracy 0.8759999871253967\n",
      "Iteration 22500 Training loss 0.0027524027973413467 Validation loss 0.05106354504823685 Accuracy 0.8759999871253967\n",
      "Iteration 22510 Training loss 0.002252693520858884 Validation loss 0.05106087401509285 Accuracy 0.8759999871253967\n",
      "Iteration 22520 Training loss 0.0017528615426272154 Validation loss 0.05106024071574211 Accuracy 0.8759999871253967\n",
      "Iteration 22530 Training loss 0.0010032257996499538 Validation loss 0.05105985328555107 Accuracy 0.8765000104904175\n",
      "Iteration 22540 Training loss 0.0015027843182906508 Validation loss 0.05104869231581688 Accuracy 0.8765000104904175\n",
      "Iteration 22550 Training loss 0.0017529010074213147 Validation loss 0.05105723440647125 Accuracy 0.8759999871253967\n",
      "Iteration 22560 Training loss 0.0025025701615959406 Validation loss 0.05105961114168167 Accuracy 0.8759999871253967\n",
      "Iteration 22570 Training loss 0.0015027339104562998 Validation loss 0.05106489360332489 Accuracy 0.8759999871253967\n",
      "Iteration 22580 Training loss 0.0015026440378278494 Validation loss 0.05106017366051674 Accuracy 0.8765000104904175\n",
      "Iteration 22590 Training loss 0.0017526998417451978 Validation loss 0.0510600246489048 Accuracy 0.8765000104904175\n",
      "Iteration 22600 Training loss 0.0015026917681097984 Validation loss 0.051063764840364456 Accuracy 0.8765000104904175\n",
      "Iteration 22610 Training loss 0.0020026040729135275 Validation loss 0.051064807921648026 Accuracy 0.8765000104904175\n",
      "Iteration 22620 Training loss 0.0010027664247900248 Validation loss 0.051064856350421906 Accuracy 0.8765000104904175\n",
      "Iteration 22630 Training loss 0.0017528602620586753 Validation loss 0.051063310354948044 Accuracy 0.8765000104904175\n",
      "Iteration 22640 Training loss 0.003002278506755829 Validation loss 0.0510639064013958 Accuracy 0.8765000104904175\n",
      "Iteration 22650 Training loss 0.002002907218411565 Validation loss 0.051062971353530884 Accuracy 0.8765000104904175\n",
      "Iteration 22660 Training loss 0.0022522401995956898 Validation loss 0.05106517672538757 Accuracy 0.8765000104904175\n",
      "Iteration 22670 Training loss 0.003002361161634326 Validation loss 0.05106499046087265 Accuracy 0.8765000104904175\n",
      "Iteration 22680 Training loss 0.0015025853645056486 Validation loss 0.051073912531137466 Accuracy 0.8759999871253967\n",
      "Iteration 22690 Training loss 0.0027524330653250217 Validation loss 0.05106861889362335 Accuracy 0.8759999871253967\n",
      "Iteration 22700 Training loss 0.0025028404779732227 Validation loss 0.05106394737958908 Accuracy 0.8759999871253967\n",
      "Iteration 22710 Training loss 0.0020025973208248615 Validation loss 0.05105653777718544 Accuracy 0.8765000104904175\n",
      "Iteration 22720 Training loss 0.002502878662198782 Validation loss 0.0510542131960392 Accuracy 0.8765000104904175\n",
      "Iteration 22730 Training loss 0.0012524439953267574 Validation loss 0.051049891859292984 Accuracy 0.8765000104904175\n",
      "Iteration 22740 Training loss 0.002002973109483719 Validation loss 0.05105268210172653 Accuracy 0.8765000104904175\n",
      "Iteration 22750 Training loss 0.001502844039350748 Validation loss 0.05105127394199371 Accuracy 0.8765000104904175\n",
      "Iteration 22760 Training loss 0.0007525932742282748 Validation loss 0.05105120316147804 Accuracy 0.8765000104904175\n",
      "Iteration 22770 Training loss 0.0012529237428680062 Validation loss 0.05105036869645119 Accuracy 0.8765000104904175\n",
      "Iteration 22780 Training loss 0.0010026912204921246 Validation loss 0.051054779440164566 Accuracy 0.8765000104904175\n",
      "Iteration 22790 Training loss 0.0017529621254652739 Validation loss 0.05105763301253319 Accuracy 0.8765000104904175\n",
      "Iteration 22800 Training loss 0.0012522771721705794 Validation loss 0.05106189846992493 Accuracy 0.8765000104904175\n",
      "Iteration 22810 Training loss 0.001752973534166813 Validation loss 0.051059067249298096 Accuracy 0.8765000104904175\n",
      "Iteration 22820 Training loss 0.002002750989049673 Validation loss 0.05106259137392044 Accuracy 0.8765000104904175\n",
      "Iteration 22830 Training loss 0.0005024262354709208 Validation loss 0.05107041075825691 Accuracy 0.8759999871253967\n",
      "Iteration 22840 Training loss 0.001252631889656186 Validation loss 0.051070716232061386 Accuracy 0.8759999871253967\n",
      "Iteration 22850 Training loss 0.0015024978201836348 Validation loss 0.0510648749768734 Accuracy 0.8765000104904175\n",
      "Iteration 22860 Training loss 0.0030024421866983175 Validation loss 0.051057420670986176 Accuracy 0.8765000104904175\n",
      "Iteration 22870 Training loss 0.001002687611617148 Validation loss 0.051062677055597305 Accuracy 0.8765000104904175\n",
      "Iteration 22880 Training loss 0.002752674976363778 Validation loss 0.05106101185083389 Accuracy 0.8765000104904175\n",
      "Iteration 22890 Training loss 0.0022524469532072544 Validation loss 0.05106744170188904 Accuracy 0.8765000104904175\n",
      "Iteration 22900 Training loss 0.0025029336102306843 Validation loss 0.0510605052113533 Accuracy 0.8765000104904175\n",
      "Iteration 22910 Training loss 0.0025026530493050814 Validation loss 0.05106797814369202 Accuracy 0.8765000104904175\n",
      "Iteration 22920 Training loss 0.0005026886356063187 Validation loss 0.05107802897691727 Accuracy 0.8765000104904175\n",
      "Iteration 22930 Training loss 0.002002370310947299 Validation loss 0.05107438564300537 Accuracy 0.8765000104904175\n",
      "Iteration 22940 Training loss 0.003002630081027746 Validation loss 0.05106797441840172 Accuracy 0.8765000104904175\n",
      "Iteration 22950 Training loss 0.0017528845928609371 Validation loss 0.05106477439403534 Accuracy 0.8765000104904175\n",
      "Iteration 22960 Training loss 0.002002580789849162 Validation loss 0.05107085034251213 Accuracy 0.8765000104904175\n",
      "Iteration 22970 Training loss 0.0012524130288511515 Validation loss 0.051068391650915146 Accuracy 0.8765000104904175\n",
      "Iteration 22980 Training loss 0.002252257661893964 Validation loss 0.05107532814145088 Accuracy 0.8765000104904175\n",
      "Iteration 22990 Training loss 0.0027521911542862654 Validation loss 0.05107097700238228 Accuracy 0.8765000104904175\n",
      "Iteration 23000 Training loss 0.001502482919022441 Validation loss 0.05107356235384941 Accuracy 0.8765000104904175\n",
      "Iteration 23010 Training loss 0.0030020137783139944 Validation loss 0.05107428878545761 Accuracy 0.8765000104904175\n",
      "Iteration 23020 Training loss 0.0017524113645777106 Validation loss 0.05107458680868149 Accuracy 0.8765000104904175\n",
      "Iteration 23030 Training loss 0.000752830586861819 Validation loss 0.05108047276735306 Accuracy 0.8765000104904175\n",
      "Iteration 23040 Training loss 0.0012525706551969051 Validation loss 0.051080476492643356 Accuracy 0.8765000104904175\n",
      "Iteration 23050 Training loss 0.0012528953375294805 Validation loss 0.05107589066028595 Accuracy 0.8765000104904175\n",
      "Iteration 23060 Training loss 0.002252435777336359 Validation loss 0.05106879398226738 Accuracy 0.8765000104904175\n",
      "Iteration 23070 Training loss 0.002253144746646285 Validation loss 0.05107763782143593 Accuracy 0.8765000104904175\n",
      "Iteration 23080 Training loss 0.0022526325192302465 Validation loss 0.051089782267808914 Accuracy 0.8765000104904175\n",
      "Iteration 23090 Training loss 0.0022523505613207817 Validation loss 0.05108100175857544 Accuracy 0.8765000104904175\n",
      "Iteration 23100 Training loss 0.0015022677835077047 Validation loss 0.051082246005535126 Accuracy 0.8765000104904175\n",
      "Iteration 23110 Training loss 0.0020029146689921618 Validation loss 0.05108862742781639 Accuracy 0.8765000104904175\n",
      "Iteration 23120 Training loss 0.0015029407804831862 Validation loss 0.051090821623802185 Accuracy 0.8765000104904175\n",
      "Iteration 23130 Training loss 0.0020023297984153032 Validation loss 0.05109195038676262 Accuracy 0.8765000104904175\n",
      "Iteration 23140 Training loss 0.002002573339268565 Validation loss 0.051092736423015594 Accuracy 0.8765000104904175\n",
      "Iteration 23150 Training loss 0.00200295215472579 Validation loss 0.05109219253063202 Accuracy 0.8765000104904175\n",
      "Iteration 23160 Training loss 0.0015027406625449657 Validation loss 0.05108548700809479 Accuracy 0.8765000104904175\n",
      "Iteration 23170 Training loss 0.0012525657657533884 Validation loss 0.05108890309929848 Accuracy 0.8765000104904175\n",
      "Iteration 23180 Training loss 0.0017530358163639903 Validation loss 0.05109034851193428 Accuracy 0.8765000104904175\n",
      "Iteration 23190 Training loss 0.002751980209723115 Validation loss 0.051084257662296295 Accuracy 0.8765000104904175\n",
      "Iteration 23200 Training loss 0.003252702532336116 Validation loss 0.05108233541250229 Accuracy 0.8765000104904175\n",
      "Iteration 23210 Training loss 0.0020026983693242073 Validation loss 0.05107957869768143 Accuracy 0.8765000104904175\n",
      "Iteration 23220 Training loss 0.0012530965032055974 Validation loss 0.05108549818396568 Accuracy 0.8765000104904175\n",
      "Iteration 23230 Training loss 0.0015023486921563745 Validation loss 0.05108017846941948 Accuracy 0.8765000104904175\n",
      "Iteration 23240 Training loss 0.0025026488583534956 Validation loss 0.0510835275053978 Accuracy 0.8765000104904175\n",
      "Iteration 23250 Training loss 0.0017527206800878048 Validation loss 0.05108688026666641 Accuracy 0.8765000104904175\n",
      "Iteration 23260 Training loss 0.003252224763855338 Validation loss 0.05108267813920975 Accuracy 0.8765000104904175\n",
      "Iteration 23270 Training loss 0.002252730308100581 Validation loss 0.05108962208032608 Accuracy 0.8765000104904175\n",
      "Iteration 23280 Training loss 0.001002930453978479 Validation loss 0.05109207704663277 Accuracy 0.8765000104904175\n",
      "Iteration 23290 Training loss 0.0017522948328405619 Validation loss 0.05109217017889023 Accuracy 0.8765000104904175\n",
      "Iteration 23300 Training loss 0.0020025831181555986 Validation loss 0.05109259858727455 Accuracy 0.8765000104904175\n",
      "Iteration 23310 Training loss 0.001002582022920251 Validation loss 0.05108650028705597 Accuracy 0.8765000104904175\n",
      "Iteration 23320 Training loss 0.0015027286717668176 Validation loss 0.051091086119413376 Accuracy 0.8765000104904175\n",
      "Iteration 23330 Training loss 0.002752486616373062 Validation loss 0.05109530687332153 Accuracy 0.8765000104904175\n",
      "Iteration 23340 Training loss 0.00250223558396101 Validation loss 0.05109109356999397 Accuracy 0.8765000104904175\n",
      "Iteration 23350 Training loss 0.0015025227330625057 Validation loss 0.051086969673633575 Accuracy 0.8765000104904175\n",
      "Iteration 23360 Training loss 0.0027517967391759157 Validation loss 0.05109243839979172 Accuracy 0.8765000104904175\n",
      "Iteration 23370 Training loss 0.0017524930881336331 Validation loss 0.0510900504887104 Accuracy 0.8765000104904175\n",
      "Iteration 23380 Training loss 0.0007525592809543014 Validation loss 0.05109092593193054 Accuracy 0.8765000104904175\n",
      "Iteration 23390 Training loss 0.0020030413288623095 Validation loss 0.051087211817502975 Accuracy 0.8765000104904175\n",
      "Iteration 23400 Training loss 0.0032519439700990915 Validation loss 0.051094040274620056 Accuracy 0.8765000104904175\n",
      "Iteration 23410 Training loss 0.0025023811031132936 Validation loss 0.051085978746414185 Accuracy 0.8765000104904175\n",
      "Iteration 23420 Training loss 0.0022525808308273554 Validation loss 0.05108712986111641 Accuracy 0.8765000104904175\n",
      "Iteration 23430 Training loss 0.0012528147781267762 Validation loss 0.051090020686388016 Accuracy 0.8765000104904175\n",
      "Iteration 23440 Training loss 0.0017525763250887394 Validation loss 0.05108943581581116 Accuracy 0.8765000104904175\n",
      "Iteration 23450 Training loss 0.0017522919224575162 Validation loss 0.0510886088013649 Accuracy 0.8765000104904175\n",
      "Iteration 23460 Training loss 0.0020027917344123125 Validation loss 0.05108792707324028 Accuracy 0.8765000104904175\n",
      "Iteration 23470 Training loss 0.0027525725308805704 Validation loss 0.05108696222305298 Accuracy 0.8765000104904175\n",
      "Iteration 23480 Training loss 0.0017522190464660525 Validation loss 0.051083046942949295 Accuracy 0.8765000104904175\n",
      "Iteration 23490 Training loss 0.0010024934308603406 Validation loss 0.05109642073512077 Accuracy 0.8765000104904175\n",
      "Iteration 23500 Training loss 0.0017521969275549054 Validation loss 0.05109813064336777 Accuracy 0.8765000104904175\n",
      "Iteration 23510 Training loss 0.0030024019069969654 Validation loss 0.051092177629470825 Accuracy 0.8765000104904175\n",
      "Iteration 23520 Training loss 0.0017525349976494908 Validation loss 0.051090944558382034 Accuracy 0.8765000104904175\n",
      "Iteration 23530 Training loss 0.0017525415169075131 Validation loss 0.05109642818570137 Accuracy 0.8765000104904175\n",
      "Iteration 23540 Training loss 0.0022522497456520796 Validation loss 0.05109401047229767 Accuracy 0.8765000104904175\n",
      "Iteration 23550 Training loss 0.001252691843546927 Validation loss 0.05109121650457382 Accuracy 0.8765000104904175\n",
      "Iteration 23560 Training loss 0.0015022949082776904 Validation loss 0.05109584331512451 Accuracy 0.8765000104904175\n",
      "Iteration 23570 Training loss 0.0012528210645541549 Validation loss 0.05110492557287216 Accuracy 0.8765000104904175\n",
      "Iteration 23580 Training loss 0.0015027003828436136 Validation loss 0.05110756307840347 Accuracy 0.8765000104904175\n",
      "Iteration 23590 Training loss 0.0027522442396730185 Validation loss 0.05112244561314583 Accuracy 0.8765000104904175\n",
      "Iteration 23600 Training loss 0.0007526872796006501 Validation loss 0.05110429972410202 Accuracy 0.8765000104904175\n",
      "Iteration 23610 Training loss 0.0015026872279122472 Validation loss 0.05109816789627075 Accuracy 0.8765000104904175\n",
      "Iteration 23620 Training loss 0.00175223418045789 Validation loss 0.051105815917253494 Accuracy 0.8765000104904175\n",
      "Iteration 23630 Training loss 0.0012522982433438301 Validation loss 0.051103465259075165 Accuracy 0.8765000104904175\n",
      "Iteration 23640 Training loss 0.0017527415184304118 Validation loss 0.05110587552189827 Accuracy 0.8765000104904175\n",
      "Iteration 23650 Training loss 0.001502751256339252 Validation loss 0.05110378563404083 Accuracy 0.8765000104904175\n",
      "Iteration 23660 Training loss 0.0017527982126921415 Validation loss 0.05110207572579384 Accuracy 0.8765000104904175\n",
      "Iteration 23670 Training loss 0.0035021412186324596 Validation loss 0.05110486224293709 Accuracy 0.8765000104904175\n",
      "Iteration 23680 Training loss 0.0010024970397353172 Validation loss 0.05110521614551544 Accuracy 0.8765000104904175\n",
      "Iteration 23690 Training loss 0.0025026402436196804 Validation loss 0.05111248046159744 Accuracy 0.8765000104904175\n",
      "Iteration 23700 Training loss 0.001002238248474896 Validation loss 0.05111069604754448 Accuracy 0.8765000104904175\n",
      "Iteration 23710 Training loss 0.0020025467965751886 Validation loss 0.05111204460263252 Accuracy 0.8765000104904175\n",
      "Iteration 23720 Training loss 0.0022524946834892035 Validation loss 0.05110597237944603 Accuracy 0.8765000104904175\n",
      "Iteration 23730 Training loss 0.002252624137327075 Validation loss 0.05110595375299454 Accuracy 0.8765000104904175\n",
      "Iteration 23740 Training loss 0.002002518158406019 Validation loss 0.05110301077365875 Accuracy 0.8765000104904175\n",
      "Iteration 23750 Training loss 0.0015028278576210141 Validation loss 0.05110448598861694 Accuracy 0.8765000104904175\n",
      "Iteration 23760 Training loss 0.0010021416237577796 Validation loss 0.05110964551568031 Accuracy 0.8765000104904175\n",
      "Iteration 23770 Training loss 0.002752541331574321 Validation loss 0.05111011490225792 Accuracy 0.8765000104904175\n",
      "Iteration 23780 Training loss 0.0020021414384245872 Validation loss 0.05111227184534073 Accuracy 0.8765000104904175\n",
      "Iteration 23790 Training loss 0.0017523952992632985 Validation loss 0.05111315846443176 Accuracy 0.8765000104904175\n",
      "Iteration 23800 Training loss 0.001752033713273704 Validation loss 0.05110853165388107 Accuracy 0.8765000104904175\n",
      "Iteration 23810 Training loss 0.0017521711997687817 Validation loss 0.051109835505485535 Accuracy 0.8765000104904175\n",
      "Iteration 23820 Training loss 0.0035026550758630037 Validation loss 0.05111538618803024 Accuracy 0.8765000104904175\n",
      "Iteration 23830 Training loss 0.0015028688358142972 Validation loss 0.05111727863550186 Accuracy 0.8765000104904175\n",
      "Iteration 23840 Training loss 0.0027526693884283304 Validation loss 0.05110818147659302 Accuracy 0.8765000104904175\n",
      "Iteration 23850 Training loss 0.0017521679401397705 Validation loss 0.05111130326986313 Accuracy 0.8765000104904175\n",
      "Iteration 23860 Training loss 0.0012526698410511017 Validation loss 0.051117539405822754 Accuracy 0.8765000104904175\n",
      "Iteration 23870 Training loss 0.0017525448929518461 Validation loss 0.05110931396484375 Accuracy 0.8765000104904175\n",
      "Iteration 23880 Training loss 0.0017522528069093823 Validation loss 0.051111575216054916 Accuracy 0.8765000104904175\n",
      "Iteration 23890 Training loss 0.0015025223838165402 Validation loss 0.051113951951265335 Accuracy 0.8765000104904175\n",
      "Iteration 23900 Training loss 0.0017526092706248164 Validation loss 0.05111381784081459 Accuracy 0.8765000104904175\n",
      "Iteration 23910 Training loss 0.0030019190162420273 Validation loss 0.05110777169466019 Accuracy 0.8765000104904175\n",
      "Iteration 23920 Training loss 0.0017528324387967587 Validation loss 0.051109690219163895 Accuracy 0.8765000104904175\n",
      "Iteration 23930 Training loss 0.0025023813359439373 Validation loss 0.05111724138259888 Accuracy 0.8759999871253967\n",
      "Iteration 23940 Training loss 0.002001948421820998 Validation loss 0.051123328506946564 Accuracy 0.8765000104904175\n",
      "Iteration 23950 Training loss 0.002502400428056717 Validation loss 0.05112319439649582 Accuracy 0.8765000104904175\n",
      "Iteration 23960 Training loss 0.0027526780031621456 Validation loss 0.05112109333276749 Accuracy 0.8765000104904175\n",
      "Iteration 23970 Training loss 0.0017523965798318386 Validation loss 0.051118429750204086 Accuracy 0.8765000104904175\n",
      "Iteration 23980 Training loss 0.001502151950262487 Validation loss 0.051123324781656265 Accuracy 0.8765000104904175\n",
      "Iteration 23990 Training loss 0.002002130961045623 Validation loss 0.05113203823566437 Accuracy 0.8759999871253967\n",
      "Iteration 24000 Training loss 0.0017521105473861098 Validation loss 0.05112892761826515 Accuracy 0.8765000104904175\n",
      "Iteration 24010 Training loss 0.0015025058528408408 Validation loss 0.05113030597567558 Accuracy 0.8765000104904175\n",
      "Iteration 24020 Training loss 0.001002156874164939 Validation loss 0.051128845661878586 Accuracy 0.8765000104904175\n",
      "Iteration 24030 Training loss 0.0015024234307929873 Validation loss 0.05113312602043152 Accuracy 0.8765000104904175\n",
      "Iteration 24040 Training loss 0.00225200317800045 Validation loss 0.05113082379102707 Accuracy 0.8765000104904175\n",
      "Iteration 24050 Training loss 0.0007523953099735081 Validation loss 0.051132552325725555 Accuracy 0.8765000104904175\n",
      "Iteration 24060 Training loss 0.0017527088057249784 Validation loss 0.05113688111305237 Accuracy 0.8759999871253967\n",
      "Iteration 24070 Training loss 0.002001921646296978 Validation loss 0.05112801119685173 Accuracy 0.8765000104904175\n",
      "Iteration 24080 Training loss 0.0020023200195282698 Validation loss 0.05113296955823898 Accuracy 0.8765000104904175\n",
      "Iteration 24090 Training loss 0.0012525427155196667 Validation loss 0.05113332346081734 Accuracy 0.8765000104904175\n",
      "Iteration 24100 Training loss 0.001002568518742919 Validation loss 0.05112922564148903 Accuracy 0.8765000104904175\n",
      "Iteration 24110 Training loss 0.0017523257993161678 Validation loss 0.051127839833498 Accuracy 0.8765000104904175\n",
      "Iteration 24120 Training loss 0.003002393525093794 Validation loss 0.05112902820110321 Accuracy 0.8765000104904175\n",
      "Iteration 24130 Training loss 0.0012521928874775767 Validation loss 0.05112404748797417 Accuracy 0.8765000104904175\n",
      "Iteration 24140 Training loss 0.001752322306856513 Validation loss 0.05112174153327942 Accuracy 0.8765000104904175\n",
      "Iteration 24150 Training loss 0.002252499805763364 Validation loss 0.05112224444746971 Accuracy 0.8765000104904175\n",
      "Iteration 24160 Training loss 0.0015022492734715343 Validation loss 0.051129117608070374 Accuracy 0.8759999871253967\n",
      "Iteration 24170 Training loss 0.0015026872279122472 Validation loss 0.05112576112151146 Accuracy 0.8759999871253967\n",
      "Iteration 24180 Training loss 0.003001950914040208 Validation loss 0.05112282559275627 Accuracy 0.8765000104904175\n",
      "Iteration 24190 Training loss 0.0025026791263371706 Validation loss 0.05111910402774811 Accuracy 0.8765000104904175\n",
      "Iteration 24200 Training loss 0.0012526470236480236 Validation loss 0.051117803901433945 Accuracy 0.8769999742507935\n",
      "Iteration 24210 Training loss 0.0015026683686301112 Validation loss 0.05111546441912651 Accuracy 0.8769999742507935\n",
      "Iteration 24220 Training loss 0.0020018713548779488 Validation loss 0.051113616675138474 Accuracy 0.8769999742507935\n",
      "Iteration 24230 Training loss 0.0020022683311253786 Validation loss 0.05111879110336304 Accuracy 0.8769999742507935\n",
      "Iteration 24240 Training loss 0.001752599491737783 Validation loss 0.051108963787555695 Accuracy 0.8769999742507935\n",
      "Iteration 24250 Training loss 0.0025026421062648296 Validation loss 0.05111479014158249 Accuracy 0.8769999742507935\n",
      "Iteration 24260 Training loss 0.0012528800871223211 Validation loss 0.05111338198184967 Accuracy 0.8769999742507935\n",
      "Iteration 24270 Training loss 0.00250177551060915 Validation loss 0.05111323297023773 Accuracy 0.8769999742507935\n",
      "Iteration 24280 Training loss 0.002252293983474374 Validation loss 0.05110657587647438 Accuracy 0.8769999742507935\n",
      "Iteration 24290 Training loss 0.0027519026771187782 Validation loss 0.05110882595181465 Accuracy 0.8769999742507935\n",
      "Iteration 24300 Training loss 0.0015024079475551844 Validation loss 0.051108893007040024 Accuracy 0.8769999742507935\n",
      "Iteration 24310 Training loss 0.001752791809849441 Validation loss 0.0511150062084198 Accuracy 0.8769999742507935\n",
      "Iteration 24320 Training loss 0.0017519374378025532 Validation loss 0.051116663962602615 Accuracy 0.8769999742507935\n",
      "Iteration 24330 Training loss 0.0010021032067015767 Validation loss 0.051127612590789795 Accuracy 0.8769999742507935\n",
      "Iteration 24340 Training loss 0.0027519322466105223 Validation loss 0.05112919583916664 Accuracy 0.8769999742507935\n",
      "Iteration 24350 Training loss 0.0012527396902441978 Validation loss 0.051126811653375626 Accuracy 0.8769999742507935\n",
      "Iteration 24360 Training loss 0.0025023675989359617 Validation loss 0.05112263187766075 Accuracy 0.8769999742507935\n",
      "Iteration 24370 Training loss 0.00225208792835474 Validation loss 0.05111714079976082 Accuracy 0.8769999742507935\n",
      "Iteration 24380 Training loss 0.0015025484608486295 Validation loss 0.05112440139055252 Accuracy 0.8769999742507935\n",
      "Iteration 24390 Training loss 0.0007529833819717169 Validation loss 0.051128920167684555 Accuracy 0.8769999742507935\n",
      "Iteration 24400 Training loss 0.002752422820776701 Validation loss 0.05113408342003822 Accuracy 0.8769999742507935\n",
      "Iteration 24410 Training loss 0.0030021858401596546 Validation loss 0.051124803721904755 Accuracy 0.8769999742507935\n",
      "Iteration 24420 Training loss 0.0010029167169705033 Validation loss 0.051124051213264465 Accuracy 0.8769999742507935\n",
      "Iteration 24430 Training loss 0.0022524509113281965 Validation loss 0.051136985421180725 Accuracy 0.8765000104904175\n",
      "Iteration 24440 Training loss 0.0020026820711791515 Validation loss 0.05113295838236809 Accuracy 0.8769999742507935\n",
      "Iteration 24450 Training loss 0.0015024811727926135 Validation loss 0.05113338306546211 Accuracy 0.8765000104904175\n",
      "Iteration 24460 Training loss 0.0027520370204001665 Validation loss 0.05113282799720764 Accuracy 0.8769999742507935\n",
      "Iteration 24470 Training loss 0.002751923631876707 Validation loss 0.051132187247276306 Accuracy 0.8769999742507935\n",
      "Iteration 24480 Training loss 0.0017521146219223738 Validation loss 0.05112893506884575 Accuracy 0.8769999742507935\n",
      "Iteration 24490 Training loss 0.002002246445044875 Validation loss 0.05113297700881958 Accuracy 0.8769999742507935\n",
      "Iteration 24500 Training loss 0.0025024996139109135 Validation loss 0.05112238973379135 Accuracy 0.8769999742507935\n",
      "Iteration 24510 Training loss 0.0017524966970086098 Validation loss 0.051131926476955414 Accuracy 0.8769999742507935\n",
      "Iteration 24520 Training loss 0.002002320485189557 Validation loss 0.05113204941153526 Accuracy 0.8769999742507935\n",
      "Iteration 24530 Training loss 0.0020017889328300953 Validation loss 0.05113181471824646 Accuracy 0.8769999742507935\n",
      "Iteration 24540 Training loss 0.0015022060833871365 Validation loss 0.05113106593489647 Accuracy 0.8769999742507935\n",
      "Iteration 24550 Training loss 0.0025018295273184776 Validation loss 0.05114098638296127 Accuracy 0.8769999742507935\n",
      "Iteration 24560 Training loss 0.0022520418278872967 Validation loss 0.0511346310377121 Accuracy 0.8769999742507935\n",
      "Iteration 24570 Training loss 0.00300190644338727 Validation loss 0.05113383010029793 Accuracy 0.8769999742507935\n",
      "Iteration 24580 Training loss 0.0022529277484863997 Validation loss 0.05113392695784569 Accuracy 0.8769999742507935\n",
      "Iteration 24590 Training loss 0.0010024013463407755 Validation loss 0.05113861709833145 Accuracy 0.8769999742507935\n",
      "Iteration 24600 Training loss 0.0022520979400724173 Validation loss 0.05113697797060013 Accuracy 0.8769999742507935\n",
      "Iteration 24610 Training loss 0.003002236830070615 Validation loss 0.05114259570837021 Accuracy 0.8769999742507935\n",
      "Iteration 24620 Training loss 0.002252612030133605 Validation loss 0.05113460496068001 Accuracy 0.8769999742507935\n",
      "Iteration 24630 Training loss 0.0017518735257908702 Validation loss 0.051130518317222595 Accuracy 0.8769999742507935\n",
      "Iteration 24640 Training loss 0.0037522008642554283 Validation loss 0.05112249031662941 Accuracy 0.8769999742507935\n",
      "Iteration 24650 Training loss 0.002502349903807044 Validation loss 0.05112526938319206 Accuracy 0.8769999742507935\n",
      "Iteration 24660 Training loss 0.003002206329256296 Validation loss 0.05112507566809654 Accuracy 0.8769999742507935\n",
      "Iteration 24670 Training loss 0.002752666361629963 Validation loss 0.05112284794449806 Accuracy 0.8769999742507935\n",
      "Iteration 24680 Training loss 0.002252552192658186 Validation loss 0.05112146586179733 Accuracy 0.8769999742507935\n",
      "Iteration 24690 Training loss 0.002252453239634633 Validation loss 0.051126036792993546 Accuracy 0.8769999742507935\n",
      "Iteration 24700 Training loss 0.0015025832690298557 Validation loss 0.05113277584314346 Accuracy 0.8769999742507935\n",
      "Iteration 24710 Training loss 0.001002384815365076 Validation loss 0.05112193524837494 Accuracy 0.8769999742507935\n",
      "Iteration 24720 Training loss 0.001502318773418665 Validation loss 0.051121946424245834 Accuracy 0.8769999742507935\n",
      "Iteration 24730 Training loss 0.003001054748892784 Validation loss 0.051123909652233124 Accuracy 0.8769999742507935\n",
      "Iteration 24740 Training loss 0.0017523611895740032 Validation loss 0.05112578719854355 Accuracy 0.8769999742507935\n",
      "Iteration 24750 Training loss 0.0022519927006214857 Validation loss 0.05112914368510246 Accuracy 0.8769999742507935\n",
      "Iteration 24760 Training loss 0.0010026074014604092 Validation loss 0.05113868787884712 Accuracy 0.8769999742507935\n",
      "Iteration 24770 Training loss 0.0010024302173405886 Validation loss 0.05113925784826279 Accuracy 0.8769999742507935\n",
      "Iteration 24780 Training loss 0.0020024168770760298 Validation loss 0.05113355070352554 Accuracy 0.8769999742507935\n",
      "Iteration 24790 Training loss 0.0017513788770884275 Validation loss 0.05112488567829132 Accuracy 0.8765000104904175\n",
      "Iteration 24800 Training loss 0.0007522983942180872 Validation loss 0.05113235488533974 Accuracy 0.8765000104904175\n",
      "Iteration 24810 Training loss 0.0017525320872664452 Validation loss 0.051120925694704056 Accuracy 0.8765000104904175\n",
      "Iteration 24820 Training loss 0.0020016536582261324 Validation loss 0.05112488567829132 Accuracy 0.8765000104904175\n",
      "Iteration 24830 Training loss 0.0015025257598608732 Validation loss 0.051121871918439865 Accuracy 0.8759999871253967\n",
      "Iteration 24840 Training loss 0.0015020255232229829 Validation loss 0.051130667328834534 Accuracy 0.8765000104904175\n",
      "Iteration 24850 Training loss 0.0007526704575866461 Validation loss 0.05112989991903305 Accuracy 0.8759999871253967\n",
      "Iteration 24860 Training loss 0.0015024481108412147 Validation loss 0.05113304406404495 Accuracy 0.8759999871253967\n",
      "Iteration 24870 Training loss 0.0022510660346597433 Validation loss 0.05112900584936142 Accuracy 0.8759999871253967\n",
      "Iteration 24880 Training loss 0.0017526065930724144 Validation loss 0.051130302250385284 Accuracy 0.8755000233650208\n",
      "Iteration 24890 Training loss 0.0017517480300739408 Validation loss 0.05112959444522858 Accuracy 0.8759999871253967\n",
      "Iteration 24900 Training loss 0.002002396620810032 Validation loss 0.051127154380083084 Accuracy 0.8759999871253967\n",
      "Iteration 24910 Training loss 0.0022516008466482162 Validation loss 0.0511360839009285 Accuracy 0.8759999871253967\n",
      "Iteration 24920 Training loss 0.0030017660465091467 Validation loss 0.05113121122121811 Accuracy 0.8759999871253967\n",
      "Iteration 24930 Training loss 0.001252166461199522 Validation loss 0.0511297844350338 Accuracy 0.8759999871253967\n",
      "Iteration 24940 Training loss 0.0022521282080560923 Validation loss 0.05112125352025032 Accuracy 0.8759999871253967\n",
      "Iteration 24950 Training loss 0.0010023419745266438 Validation loss 0.05112165957689285 Accuracy 0.8759999871253967\n",
      "Iteration 24960 Training loss 0.0010025680530816317 Validation loss 0.051125865429639816 Accuracy 0.8759999871253967\n",
      "Iteration 24970 Training loss 0.0022522436920553446 Validation loss 0.05112868547439575 Accuracy 0.8759999871253967\n",
      "Iteration 24980 Training loss 0.0017517277738079429 Validation loss 0.05112742260098457 Accuracy 0.8759999871253967\n",
      "Iteration 24990 Training loss 0.002252499805763364 Validation loss 0.05112624913454056 Accuracy 0.8755000233650208\n",
      "Iteration 25000 Training loss 0.001752516021952033 Validation loss 0.05112006515264511 Accuracy 0.8759999871253967\n",
      "Iteration 25010 Training loss 0.0030018119141459465 Validation loss 0.05112118646502495 Accuracy 0.8755000233650208\n",
      "Iteration 25020 Training loss 0.0012527910294011235 Validation loss 0.05113157629966736 Accuracy 0.8755000233650208\n",
      "Iteration 25030 Training loss 0.0015025425236672163 Validation loss 0.05112394690513611 Accuracy 0.8755000233650208\n",
      "Iteration 25040 Training loss 0.0030009618494659662 Validation loss 0.05112295225262642 Accuracy 0.8755000233650208\n",
      "Iteration 25050 Training loss 0.0017524054273962975 Validation loss 0.05112621188163757 Accuracy 0.8755000233650208\n",
      "Iteration 25060 Training loss 0.0020017095375806093 Validation loss 0.051125138998031616 Accuracy 0.8755000233650208\n",
      "Iteration 25070 Training loss 0.002751927822828293 Validation loss 0.051131125539541245 Accuracy 0.8755000233650208\n",
      "Iteration 25080 Training loss 0.0022518413607031107 Validation loss 0.05112457275390625 Accuracy 0.8759999871253967\n",
      "Iteration 25090 Training loss 0.0020023479592055082 Validation loss 0.051121439784765244 Accuracy 0.8755000233650208\n",
      "Iteration 25100 Training loss 0.0017522011185064912 Validation loss 0.051113028079271317 Accuracy 0.8759999871253967\n",
      "Iteration 25110 Training loss 0.002752346685156226 Validation loss 0.0511188879609108 Accuracy 0.8755000233650208\n",
      "Iteration 25120 Training loss 0.0027524104807525873 Validation loss 0.0511159785091877 Accuracy 0.8755000233650208\n",
      "Iteration 25130 Training loss 0.0025021671317517757 Validation loss 0.05110573023557663 Accuracy 0.8755000233650208\n",
      "Iteration 25140 Training loss 0.0012524717021733522 Validation loss 0.05111407861113548 Accuracy 0.8755000233650208\n",
      "Iteration 25150 Training loss 0.0025021566543728113 Validation loss 0.05110255628824234 Accuracy 0.8755000233650208\n",
      "Iteration 25160 Training loss 0.0020024364348500967 Validation loss 0.05110551789402962 Accuracy 0.8755000233650208\n",
      "Iteration 25170 Training loss 0.0017525770235806704 Validation loss 0.0511055588722229 Accuracy 0.8755000233650208\n",
      "Iteration 25180 Training loss 0.0010019009932875633 Validation loss 0.051101312041282654 Accuracy 0.8755000233650208\n",
      "Iteration 25190 Training loss 0.0020024471450597048 Validation loss 0.051094476133584976 Accuracy 0.8755000233650208\n",
      "Iteration 25200 Training loss 0.002502431394532323 Validation loss 0.051099568605422974 Accuracy 0.8755000233650208\n",
      "Iteration 25210 Training loss 0.001502309925854206 Validation loss 0.051099274307489395 Accuracy 0.8755000233650208\n",
      "Iteration 25220 Training loss 0.0010028437245637178 Validation loss 0.0510893315076828 Accuracy 0.8759999871253967\n",
      "Iteration 25230 Training loss 0.0015021865256130695 Validation loss 0.051101088523864746 Accuracy 0.8755000233650208\n",
      "Iteration 25240 Training loss 0.0017523645656183362 Validation loss 0.05110013112425804 Accuracy 0.8755000233650208\n",
      "Iteration 25250 Training loss 0.0015024890890344977 Validation loss 0.05110020190477371 Accuracy 0.8755000233650208\n",
      "Iteration 25260 Training loss 0.0010006757220253348 Validation loss 0.05110703781247139 Accuracy 0.8755000233650208\n",
      "Iteration 25270 Training loss 0.002752476604655385 Validation loss 0.05110250040888786 Accuracy 0.8755000233650208\n",
      "Iteration 25280 Training loss 0.0012523103505373001 Validation loss 0.051103200763463974 Accuracy 0.8755000233650208\n",
      "Iteration 25290 Training loss 0.0027508700732141733 Validation loss 0.051102105528116226 Accuracy 0.8755000233650208\n",
      "Iteration 25300 Training loss 0.001502363127656281 Validation loss 0.051113080233335495 Accuracy 0.8755000233650208\n",
      "Iteration 25310 Training loss 0.0022521757055073977 Validation loss 0.05109354853630066 Accuracy 0.875\n",
      "Iteration 25320 Training loss 0.0027526021003723145 Validation loss 0.05109608545899391 Accuracy 0.875\n",
      "Iteration 25330 Training loss 0.0017500311369076371 Validation loss 0.05107242614030838 Accuracy 0.8755000233650208\n",
      "Iteration 25340 Training loss 0.002002514898777008 Validation loss 0.05105171352624893 Accuracy 0.8755000233650208\n",
      "Iteration 25350 Training loss 0.002501932205632329 Validation loss 0.05104814097285271 Accuracy 0.8755000233650208\n",
      "Iteration 25360 Training loss 0.0022525847889482975 Validation loss 0.05103747919201851 Accuracy 0.8755000233650208\n",
      "Iteration 25370 Training loss 0.0005019219242967665 Validation loss 0.05102172866463661 Accuracy 0.875\n",
      "Iteration 25380 Training loss 0.0014950665645301342 Validation loss 0.051008544862270355 Accuracy 0.875\n",
      "Iteration 25390 Training loss 0.0017522864509373903 Validation loss 0.050999291241168976 Accuracy 0.875\n",
      "Iteration 25400 Training loss 0.002503097988665104 Validation loss 0.05102641135454178 Accuracy 0.875\n",
      "Iteration 25410 Training loss 0.0022521675564348698 Validation loss 0.05100526660680771 Accuracy 0.875\n",
      "Iteration 25420 Training loss 0.0017524478025734425 Validation loss 0.05100877583026886 Accuracy 0.875\n",
      "Iteration 25430 Training loss 0.0014831852167844772 Validation loss 0.051005929708480835 Accuracy 0.875\n",
      "Iteration 25440 Training loss 0.002004117937758565 Validation loss 0.051013246178627014 Accuracy 0.8759999871253967\n",
      "Iteration 25450 Training loss 0.001505486317910254 Validation loss 0.05111565440893173 Accuracy 0.8744999766349792\n",
      "Iteration 25460 Training loss 0.0027550349477678537 Validation loss 0.051152266561985016 Accuracy 0.8765000104904175\n",
      "Iteration 25470 Training loss 0.001005075522698462 Validation loss 0.05120288208127022 Accuracy 0.8765000104904175\n",
      "Iteration 25480 Training loss 0.0017546751769259572 Validation loss 0.051227230578660965 Accuracy 0.8759999871253967\n",
      "Iteration 25490 Training loss 0.0030048233456909657 Validation loss 0.05124790593981743 Accuracy 0.8759999871253967\n",
      "Iteration 25500 Training loss 0.002507080091163516 Validation loss 0.05125783756375313 Accuracy 0.8774999976158142\n",
      "Iteration 25510 Training loss 0.0027536938432604074 Validation loss 0.051256485283374786 Accuracy 0.8765000104904175\n",
      "Iteration 25520 Training loss 0.0015046203043311834 Validation loss 0.05123751983046532 Accuracy 0.8759999871253967\n",
      "Iteration 25530 Training loss 0.002004842273890972 Validation loss 0.0512462742626667 Accuracy 0.8759999871253967\n",
      "Iteration 25540 Training loss 0.002003476256504655 Validation loss 0.051242340356111526 Accuracy 0.8759999871253967\n",
      "Iteration 25550 Training loss 0.0012570875696837902 Validation loss 0.05123058706521988 Accuracy 0.8759999871253967\n",
      "Iteration 25560 Training loss 0.003004307160153985 Validation loss 0.05123135447502136 Accuracy 0.8759999871253967\n",
      "Iteration 25570 Training loss 0.0015034052776172757 Validation loss 0.05123649537563324 Accuracy 0.8765000104904175\n",
      "Iteration 25580 Training loss 0.0012538881273940206 Validation loss 0.05124235153198242 Accuracy 0.8765000104904175\n",
      "Iteration 25590 Training loss 0.0020039239898324013 Validation loss 0.05125109851360321 Accuracy 0.8765000104904175\n",
      "Iteration 25600 Training loss 0.0032535805366933346 Validation loss 0.05123535916209221 Accuracy 0.8765000104904175\n",
      "Iteration 25610 Training loss 0.0022569529246538877 Validation loss 0.051233306527137756 Accuracy 0.8759999871253967\n",
      "Iteration 25620 Training loss 0.0012535799760371447 Validation loss 0.051232822239398956 Accuracy 0.8759999871253967\n",
      "Iteration 25630 Training loss 0.002003646455705166 Validation loss 0.051234740763902664 Accuracy 0.8755000233650208\n",
      "Iteration 25640 Training loss 0.0010031391866505146 Validation loss 0.051224708557128906 Accuracy 0.8759999871253967\n",
      "Iteration 25650 Training loss 0.0017528760945424438 Validation loss 0.05121922120451927 Accuracy 0.8755000233650208\n",
      "Iteration 25660 Training loss 0.001504362327978015 Validation loss 0.05121387541294098 Accuracy 0.8759999871253967\n",
      "Iteration 25670 Training loss 0.002253868617117405 Validation loss 0.05122312530875206 Accuracy 0.8759999871253967\n",
      "Iteration 25680 Training loss 0.001253316760994494 Validation loss 0.051235221326351166 Accuracy 0.8755000233650208\n",
      "Iteration 25690 Training loss 0.0015053871320560575 Validation loss 0.05124467611312866 Accuracy 0.8755000233650208\n",
      "Iteration 25700 Training loss 0.0012532516848295927 Validation loss 0.05124899372458458 Accuracy 0.8755000233650208\n",
      "Iteration 25710 Training loss 0.0025037042796611786 Validation loss 0.0512479729950428 Accuracy 0.875\n",
      "Iteration 25720 Training loss 0.002253445563837886 Validation loss 0.05124400183558464 Accuracy 0.875\n",
      "Iteration 25730 Training loss 0.0017528660828247666 Validation loss 0.051236655563116074 Accuracy 0.8755000233650208\n",
      "Iteration 25740 Training loss 0.0015049877110868692 Validation loss 0.05124475434422493 Accuracy 0.875\n",
      "Iteration 25750 Training loss 0.0020031039603054523 Validation loss 0.051262207329273224 Accuracy 0.875\n",
      "Iteration 25760 Training loss 0.0020026129204779863 Validation loss 0.05125243216753006 Accuracy 0.875\n",
      "Iteration 25770 Training loss 0.002003636909648776 Validation loss 0.05126351863145828 Accuracy 0.875\n",
      "Iteration 25780 Training loss 0.002003992907702923 Validation loss 0.051266416907310486 Accuracy 0.875\n",
      "Iteration 25790 Training loss 0.0015041728038340807 Validation loss 0.051278624683618546 Accuracy 0.875\n",
      "Iteration 25800 Training loss 0.0015028720954433084 Validation loss 0.051285918802022934 Accuracy 0.875\n",
      "Iteration 25810 Training loss 0.0015037021366879344 Validation loss 0.05128632113337517 Accuracy 0.875\n",
      "Iteration 25820 Training loss 0.0025048383977264166 Validation loss 0.051279179751873016 Accuracy 0.875\n",
      "Iteration 25830 Training loss 0.0015028859488666058 Validation loss 0.05128515139222145 Accuracy 0.875\n",
      "Iteration 25840 Training loss 0.001752285985276103 Validation loss 0.05129239708185196 Accuracy 0.875\n",
      "Iteration 25850 Training loss 0.0015034307725727558 Validation loss 0.051303356885910034 Accuracy 0.875\n",
      "Iteration 25860 Training loss 0.0030050494242459536 Validation loss 0.051305145025253296 Accuracy 0.875\n",
      "Iteration 25870 Training loss 0.001254588132724166 Validation loss 0.0512915663421154 Accuracy 0.875\n",
      "Iteration 25880 Training loss 0.0012533979024738073 Validation loss 0.051286470144987106 Accuracy 0.875\n",
      "Iteration 25890 Training loss 0.0022524523083120584 Validation loss 0.051288291811943054 Accuracy 0.875\n",
      "Iteration 25900 Training loss 0.0022544311359524727 Validation loss 0.05128612369298935 Accuracy 0.875\n",
      "Iteration 25910 Training loss 0.002004323760047555 Validation loss 0.05129437521100044 Accuracy 0.875\n",
      "Iteration 25920 Training loss 0.002253540325909853 Validation loss 0.05128471925854683 Accuracy 0.875\n",
      "Iteration 25930 Training loss 0.0015035050455480814 Validation loss 0.051290132105350494 Accuracy 0.875\n",
      "Iteration 25940 Training loss 0.001252675079740584 Validation loss 0.05129989609122276 Accuracy 0.875\n",
      "Iteration 25950 Training loss 0.0015029513742774725 Validation loss 0.05128760635852814 Accuracy 0.875\n",
      "Iteration 25960 Training loss 0.0025024586357176304 Validation loss 0.05129651725292206 Accuracy 0.875\n",
      "Iteration 25970 Training loss 0.0010029867989942431 Validation loss 0.05129624903202057 Accuracy 0.875\n",
      "Iteration 25980 Training loss 0.0017536289524286985 Validation loss 0.05131112039089203 Accuracy 0.875\n",
      "Iteration 25990 Training loss 0.0010029195109382272 Validation loss 0.05129668489098549 Accuracy 0.875\n",
      "Iteration 26000 Training loss 0.00200326694175601 Validation loss 0.05128595605492592 Accuracy 0.8744999766349792\n",
      "Iteration 26010 Training loss 0.002253357321023941 Validation loss 0.05129307508468628 Accuracy 0.875\n",
      "Iteration 26020 Training loss 0.0015021520666778088 Validation loss 0.05130152776837349 Accuracy 0.875\n",
      "Iteration 26030 Training loss 0.001003157813102007 Validation loss 0.051311712712049484 Accuracy 0.8755000233650208\n",
      "Iteration 26040 Training loss 0.0017543276771903038 Validation loss 0.05131460353732109 Accuracy 0.8755000233650208\n",
      "Iteration 26050 Training loss 0.0015029854839667678 Validation loss 0.05131104588508606 Accuracy 0.875\n",
      "Iteration 26060 Training loss 0.0017537135863676667 Validation loss 0.05129174888134003 Accuracy 0.8744999766349792\n",
      "Iteration 26070 Training loss 0.0017524079885333776 Validation loss 0.05130516737699509 Accuracy 0.875\n",
      "Iteration 26080 Training loss 0.001753566088154912 Validation loss 0.05130576342344284 Accuracy 0.8744999766349792\n",
      "Iteration 26090 Training loss 0.0037536625750362873 Validation loss 0.051300883293151855 Accuracy 0.8744999766349792\n",
      "Iteration 26100 Training loss 0.0027525529731065035 Validation loss 0.05129539594054222 Accuracy 0.8744999766349792\n",
      "Iteration 26110 Training loss 0.0010024507064372301 Validation loss 0.05129963532090187 Accuracy 0.875\n",
      "Iteration 26120 Training loss 0.0020021984819322824 Validation loss 0.05130597576498985 Accuracy 0.875\n",
      "Iteration 26130 Training loss 0.0017528351163491607 Validation loss 0.05130663886666298 Accuracy 0.875\n",
      "Iteration 26140 Training loss 0.0017531045014038682 Validation loss 0.051309049129486084 Accuracy 0.875\n",
      "Iteration 26150 Training loss 0.002003076020628214 Validation loss 0.0513056144118309 Accuracy 0.8744999766349792\n",
      "Iteration 26160 Training loss 0.0017535863444209099 Validation loss 0.05130981281399727 Accuracy 0.875\n",
      "Iteration 26170 Training loss 0.0015027272747829556 Validation loss 0.05132419615983963 Accuracy 0.8744999766349792\n",
      "Iteration 26180 Training loss 0.0020022550597786903 Validation loss 0.05132240429520607 Accuracy 0.8744999766349792\n",
      "Iteration 26190 Training loss 0.0012525157071650028 Validation loss 0.05132853612303734 Accuracy 0.875\n",
      "Iteration 26200 Training loss 0.0022528250701725483 Validation loss 0.051324471831321716 Accuracy 0.8744999766349792\n",
      "Iteration 26210 Training loss 0.0020028711296617985 Validation loss 0.051307518035173416 Accuracy 0.8744999766349792\n",
      "Iteration 26220 Training loss 0.0015025550965219736 Validation loss 0.051317885518074036 Accuracy 0.8744999766349792\n",
      "Iteration 26230 Training loss 0.0017522933194413781 Validation loss 0.05132137984037399 Accuracy 0.8744999766349792\n",
      "Iteration 26240 Training loss 0.002252903301268816 Validation loss 0.05132367089390755 Accuracy 0.8744999766349792\n",
      "Iteration 26250 Training loss 0.002252563601359725 Validation loss 0.05132674798369408 Accuracy 0.8744999766349792\n",
      "Iteration 26260 Training loss 0.0015028772177174687 Validation loss 0.05134086683392525 Accuracy 0.8744999766349792\n",
      "Iteration 26270 Training loss 0.001502700848504901 Validation loss 0.051340773701667786 Accuracy 0.8744999766349792\n",
      "Iteration 26280 Training loss 0.0015026528853923082 Validation loss 0.05134553834795952 Accuracy 0.875\n",
      "Iteration 26290 Training loss 0.0012530983658507466 Validation loss 0.051348015666007996 Accuracy 0.875\n",
      "Iteration 26300 Training loss 0.0025018525775521994 Validation loss 0.051352888345718384 Accuracy 0.875\n",
      "Iteration 26310 Training loss 0.002002934692427516 Validation loss 0.05136268958449364 Accuracy 0.875\n",
      "Iteration 26320 Training loss 0.0020026599522680044 Validation loss 0.05136207118630409 Accuracy 0.875\n",
      "Iteration 26330 Training loss 0.0022532916627824306 Validation loss 0.05136153846979141 Accuracy 0.875\n",
      "Iteration 26340 Training loss 0.0025029464159160852 Validation loss 0.0513516403734684 Accuracy 0.875\n",
      "Iteration 26350 Training loss 0.002002160530537367 Validation loss 0.0513421930372715 Accuracy 0.875\n",
      "Iteration 26360 Training loss 0.0022522497456520796 Validation loss 0.05134465917944908 Accuracy 0.875\n",
      "Iteration 26370 Training loss 0.0017530899494886398 Validation loss 0.051350612193346024 Accuracy 0.875\n",
      "Iteration 26380 Training loss 0.0017526289448142052 Validation loss 0.05134543031454086 Accuracy 0.875\n",
      "Iteration 26390 Training loss 0.002252589212730527 Validation loss 0.05134272947907448 Accuracy 0.875\n",
      "Iteration 26400 Training loss 0.001502527273260057 Validation loss 0.05134423077106476 Accuracy 0.875\n",
      "Iteration 26410 Training loss 0.0017526081064715981 Validation loss 0.0513419583439827 Accuracy 0.875\n",
      "Iteration 26420 Training loss 0.002002617809921503 Validation loss 0.051340315490961075 Accuracy 0.875\n",
      "Iteration 26430 Training loss 0.0020034918561577797 Validation loss 0.051338884979486465 Accuracy 0.875\n",
      "Iteration 26440 Training loss 0.0005027026054449379 Validation loss 0.05133257433772087 Accuracy 0.875\n",
      "Iteration 26450 Training loss 0.0012531694956123829 Validation loss 0.051336318254470825 Accuracy 0.875\n",
      "Iteration 26460 Training loss 0.0012522562174126506 Validation loss 0.05133134499192238 Accuracy 0.875\n",
      "Iteration 26470 Training loss 0.00100189249496907 Validation loss 0.05132592096924782 Accuracy 0.875\n",
      "Iteration 26480 Training loss 0.0012535885907709599 Validation loss 0.05132833123207092 Accuracy 0.875\n",
      "Iteration 26490 Training loss 0.0025030276738107204 Validation loss 0.05133185535669327 Accuracy 0.875\n",
      "Iteration 26500 Training loss 0.0010030390694737434 Validation loss 0.05133907496929169 Accuracy 0.875\n",
      "Iteration 26510 Training loss 0.0005029254825785756 Validation loss 0.051344260573387146 Accuracy 0.875\n",
      "Iteration 26520 Training loss 0.0017529488541185856 Validation loss 0.051337823271751404 Accuracy 0.875\n",
      "Iteration 26530 Training loss 0.0007527437410317361 Validation loss 0.051333751529455185 Accuracy 0.875\n",
      "Iteration 26540 Training loss 0.0022534551098942757 Validation loss 0.05133313313126564 Accuracy 0.875\n",
      "Iteration 26550 Training loss 0.0017519834218546748 Validation loss 0.051330722868442535 Accuracy 0.875\n",
      "Iteration 26560 Training loss 0.002751707099378109 Validation loss 0.051336463540792465 Accuracy 0.875\n",
      "Iteration 26570 Training loss 0.0017531554913148284 Validation loss 0.05134863406419754 Accuracy 0.875\n",
      "Iteration 26580 Training loss 0.001502887811511755 Validation loss 0.05135292187333107 Accuracy 0.875\n",
      "Iteration 26590 Training loss 0.0025018786545842886 Validation loss 0.051339682191610336 Accuracy 0.875\n",
      "Iteration 26600 Training loss 0.002001990331336856 Validation loss 0.051330775022506714 Accuracy 0.875\n",
      "Iteration 26610 Training loss 0.0020027291029691696 Validation loss 0.051345329731702805 Accuracy 0.8744999766349792\n",
      "Iteration 26620 Training loss 0.0022525463718920946 Validation loss 0.051345955580472946 Accuracy 0.875\n",
      "Iteration 26630 Training loss 0.0010030277771875262 Validation loss 0.05134397745132446 Accuracy 0.8744999766349792\n",
      "Iteration 26640 Training loss 0.0017524014692753553 Validation loss 0.05134101212024689 Accuracy 0.875\n",
      "Iteration 26650 Training loss 0.0017524234717711806 Validation loss 0.051348406821489334 Accuracy 0.875\n",
      "Iteration 26660 Training loss 0.0017529662000015378 Validation loss 0.05135383456945419 Accuracy 0.875\n",
      "Iteration 26670 Training loss 0.0022523358929902315 Validation loss 0.05135802552103996 Accuracy 0.8744999766349792\n",
      "Iteration 26680 Training loss 0.002002459019422531 Validation loss 0.05135301500558853 Accuracy 0.875\n",
      "Iteration 26690 Training loss 0.0007528134156018496 Validation loss 0.05137207731604576 Accuracy 0.8744999766349792\n",
      "Iteration 26700 Training loss 0.002002791501581669 Validation loss 0.05135921388864517 Accuracy 0.8744999766349792\n",
      "Iteration 26710 Training loss 0.0015028055058792233 Validation loss 0.05134863778948784 Accuracy 0.875\n",
      "Iteration 26720 Training loss 0.0012534193228930235 Validation loss 0.0513482429087162 Accuracy 0.8744999766349792\n",
      "Iteration 26730 Training loss 0.0017527767922729254 Validation loss 0.05135774239897728 Accuracy 0.8744999766349792\n",
      "Iteration 26740 Training loss 0.0012525167549028993 Validation loss 0.05135712772607803 Accuracy 0.8744999766349792\n",
      "Iteration 26750 Training loss 0.0015015779063105583 Validation loss 0.051354940980672836 Accuracy 0.8744999766349792\n",
      "Iteration 26760 Training loss 0.002002483932301402 Validation loss 0.05135731026530266 Accuracy 0.8744999766349792\n",
      "Iteration 26770 Training loss 0.0012526909122243524 Validation loss 0.051355618983507156 Accuracy 0.8744999766349792\n",
      "Iteration 26780 Training loss 0.0017516266088932753 Validation loss 0.05135733634233475 Accuracy 0.8744999766349792\n",
      "Iteration 26790 Training loss 0.0032525910064578056 Validation loss 0.051350172609090805 Accuracy 0.8744999766349792\n",
      "Iteration 26800 Training loss 0.0025028216186910868 Validation loss 0.051351264119148254 Accuracy 0.8744999766349792\n",
      "Iteration 26810 Training loss 0.0017533049685880542 Validation loss 0.05135961249470711 Accuracy 0.8744999766349792\n",
      "Iteration 26820 Training loss 0.0012515764683485031 Validation loss 0.051357924938201904 Accuracy 0.8744999766349792\n",
      "Iteration 26830 Training loss 0.0017533170757815242 Validation loss 0.05135355517268181 Accuracy 0.875\n",
      "Iteration 26840 Training loss 0.0027515068650245667 Validation loss 0.051349058747291565 Accuracy 0.875\n",
      "Iteration 26850 Training loss 0.0005024387501180172 Validation loss 0.051352161914110184 Accuracy 0.875\n",
      "Iteration 26860 Training loss 0.0022529265843331814 Validation loss 0.05135028436779976 Accuracy 0.875\n",
      "Iteration 26870 Training loss 0.0015027857152745128 Validation loss 0.05135096237063408 Accuracy 0.875\n",
      "Iteration 26880 Training loss 0.0017526652663946152 Validation loss 0.05135427415370941 Accuracy 0.875\n",
      "Iteration 26890 Training loss 0.0010031639831140637 Validation loss 0.05135548859834671 Accuracy 0.875\n",
      "Iteration 26900 Training loss 0.0020029135048389435 Validation loss 0.05136163532733917 Accuracy 0.8744999766349792\n",
      "Iteration 26910 Training loss 0.0020028105936944485 Validation loss 0.051365405321121216 Accuracy 0.8744999766349792\n",
      "Iteration 26920 Training loss 0.0020030250307172537 Validation loss 0.05136313661932945 Accuracy 0.8744999766349792\n",
      "Iteration 26930 Training loss 0.0020026518031954765 Validation loss 0.05134817212820053 Accuracy 0.875\n",
      "Iteration 26940 Training loss 0.0017528609605506063 Validation loss 0.05135403573513031 Accuracy 0.875\n",
      "Iteration 26950 Training loss 0.0020024580880999565 Validation loss 0.051353976130485535 Accuracy 0.875\n",
      "Iteration 26960 Training loss 0.0022512159775942564 Validation loss 0.051351387053728104 Accuracy 0.875\n",
      "Iteration 26970 Training loss 0.0010017735185101628 Validation loss 0.0513572059571743 Accuracy 0.875\n",
      "Iteration 26980 Training loss 0.001751884468831122 Validation loss 0.051368966698646545 Accuracy 0.8744999766349792\n",
      "Iteration 26990 Training loss 0.0015029434580355883 Validation loss 0.051377251744270325 Accuracy 0.8744999766349792\n",
      "Iteration 27000 Training loss 0.0002525924937799573 Validation loss 0.05138343200087547 Accuracy 0.8744999766349792\n",
      "Iteration 27010 Training loss 0.0015022894367575645 Validation loss 0.05137813091278076 Accuracy 0.8744999766349792\n",
      "Iteration 27020 Training loss 0.0017513222992420197 Validation loss 0.05136805772781372 Accuracy 0.8744999766349792\n",
      "Iteration 27030 Training loss 0.002251774538308382 Validation loss 0.051358986645936966 Accuracy 0.8744999766349792\n",
      "Iteration 27040 Training loss 0.0022523601073771715 Validation loss 0.051362503319978714 Accuracy 0.8744999766349792\n",
      "Iteration 27050 Training loss 0.0022524548694491386 Validation loss 0.05136040598154068 Accuracy 0.8744999766349792\n",
      "Iteration 27060 Training loss 0.0015026640612632036 Validation loss 0.051369987428188324 Accuracy 0.8744999766349792\n",
      "Iteration 27070 Training loss 0.003002583049237728 Validation loss 0.05136571079492569 Accuracy 0.8744999766349792\n",
      "Iteration 27080 Training loss 0.002752427477389574 Validation loss 0.051377423107624054 Accuracy 0.8744999766349792\n",
      "Iteration 27090 Training loss 0.0017530746990814805 Validation loss 0.051374029368162155 Accuracy 0.8744999766349792\n",
      "Iteration 27100 Training loss 0.0017522440757602453 Validation loss 0.051366083323955536 Accuracy 0.8744999766349792\n",
      "Iteration 27110 Training loss 0.001501244492828846 Validation loss 0.05136716365814209 Accuracy 0.8744999766349792\n",
      "Iteration 27120 Training loss 0.0012525466736406088 Validation loss 0.0513647124171257 Accuracy 0.8744999766349792\n",
      "Iteration 27130 Training loss 0.0020023700781166553 Validation loss 0.05137524753808975 Accuracy 0.8744999766349792\n",
      "Iteration 27140 Training loss 0.0015018387930467725 Validation loss 0.051374778151512146 Accuracy 0.8744999766349792\n",
      "Iteration 27150 Training loss 0.002503048861399293 Validation loss 0.05138945206999779 Accuracy 0.8740000128746033\n",
      "Iteration 27160 Training loss 0.001502696773968637 Validation loss 0.05139031261205673 Accuracy 0.8740000128746033\n",
      "Iteration 27170 Training loss 0.0020027277059853077 Validation loss 0.05138612538576126 Accuracy 0.8744999766349792\n",
      "Iteration 27180 Training loss 0.0015013004885986447 Validation loss 0.05137847736477852 Accuracy 0.8744999766349792\n",
      "Iteration 27190 Training loss 0.002253057435154915 Validation loss 0.051389146596193314 Accuracy 0.8740000128746033\n",
      "Iteration 27200 Training loss 0.002001710468903184 Validation loss 0.05139432102441788 Accuracy 0.8740000128746033\n",
      "Iteration 27210 Training loss 0.0010026078671216965 Validation loss 0.05140101537108421 Accuracy 0.8740000128746033\n",
      "Iteration 27220 Training loss 0.0015023107407614589 Validation loss 0.05139102414250374 Accuracy 0.8740000128746033\n",
      "Iteration 27230 Training loss 0.001502683386206627 Validation loss 0.05138425901532173 Accuracy 0.8740000128746033\n",
      "Iteration 27240 Training loss 0.0012514209374785423 Validation loss 0.051389358937740326 Accuracy 0.8740000128746033\n",
      "Iteration 27250 Training loss 0.0015026156324893236 Validation loss 0.05138913542032242 Accuracy 0.8740000128746033\n",
      "Iteration 27260 Training loss 0.0015024769818410277 Validation loss 0.051376763731241226 Accuracy 0.8744999766349792\n",
      "Iteration 27270 Training loss 0.002502333838492632 Validation loss 0.05138852819800377 Accuracy 0.8740000128746033\n",
      "Iteration 27280 Training loss 0.001751067116856575 Validation loss 0.05139640346169472 Accuracy 0.8740000128746033\n",
      "Iteration 27290 Training loss 0.0020027202554047108 Validation loss 0.0514078214764595 Accuracy 0.8740000128746033\n",
      "Iteration 27300 Training loss 0.0015028248308226466 Validation loss 0.05139793083071709 Accuracy 0.8740000128746033\n",
      "Iteration 27310 Training loss 0.0030012219212949276 Validation loss 0.05140000209212303 Accuracy 0.8740000128746033\n",
      "Iteration 27320 Training loss 0.0020010280422866344 Validation loss 0.0513930507004261 Accuracy 0.8740000128746033\n",
      "Iteration 27330 Training loss 0.0012528450461104512 Validation loss 0.05139966681599617 Accuracy 0.8740000128746033\n",
      "Iteration 27340 Training loss 0.0017510917969048023 Validation loss 0.051403433084487915 Accuracy 0.8740000128746033\n",
      "Iteration 27350 Training loss 0.0012530987150967121 Validation loss 0.051408711820840836 Accuracy 0.8740000128746033\n",
      "Iteration 27360 Training loss 0.0017524991417303681 Validation loss 0.051412276923656464 Accuracy 0.8734999895095825\n",
      "Iteration 27370 Training loss 0.00225099828094244 Validation loss 0.05140219256281853 Accuracy 0.8740000128746033\n",
      "Iteration 27380 Training loss 0.00025252572959288955 Validation loss 0.051400743424892426 Accuracy 0.8740000128746033\n",
      "Iteration 27390 Training loss 0.00225084344856441 Validation loss 0.05139836296439171 Accuracy 0.8740000128746033\n",
      "Iteration 27400 Training loss 0.0025024241767823696 Validation loss 0.05140424892306328 Accuracy 0.8740000128746033\n",
      "Iteration 27410 Training loss 0.002500652801245451 Validation loss 0.051405806094408035 Accuracy 0.8740000128746033\n",
      "Iteration 27420 Training loss 0.001752522774040699 Validation loss 0.05140533298254013 Accuracy 0.8740000128746033\n",
      "Iteration 27430 Training loss 0.0012528079096227884 Validation loss 0.05140209197998047 Accuracy 0.8740000128746033\n",
      "Iteration 27440 Training loss 0.00150241085793823 Validation loss 0.05140551179647446 Accuracy 0.8744999766349792\n",
      "Iteration 27450 Training loss 0.0010024529183283448 Validation loss 0.05142854154109955 Accuracy 0.8740000128746033\n",
      "Iteration 27460 Training loss 0.0017511300975456834 Validation loss 0.05140553042292595 Accuracy 0.8740000128746033\n",
      "Iteration 27470 Training loss 0.0010023583890870214 Validation loss 0.051418669521808624 Accuracy 0.8734999895095825\n",
      "Iteration 27480 Training loss 0.0017528824973851442 Validation loss 0.05141705274581909 Accuracy 0.8734999895095825\n",
      "Iteration 27490 Training loss 0.0012524776393547654 Validation loss 0.051410552114248276 Accuracy 0.8740000128746033\n",
      "Iteration 27500 Training loss 0.002003104193136096 Validation loss 0.051406700164079666 Accuracy 0.8740000128746033\n",
      "Iteration 27510 Training loss 0.0025008132215589285 Validation loss 0.05140887573361397 Accuracy 0.8744999766349792\n",
      "Iteration 27520 Training loss 0.0017507030861452222 Validation loss 0.051411841064691544 Accuracy 0.8740000128746033\n",
      "Iteration 27530 Training loss 0.0020024662371724844 Validation loss 0.051416900008916855 Accuracy 0.8744999766349792\n",
      "Iteration 27540 Training loss 0.0025006732903420925 Validation loss 0.051419954746961594 Accuracy 0.8744999766349792\n",
      "Iteration 27550 Training loss 0.0022525577805936337 Validation loss 0.05142401158809662 Accuracy 0.8740000128746033\n",
      "Iteration 27560 Training loss 0.002500424860045314 Validation loss 0.05141879618167877 Accuracy 0.8744999766349792\n",
      "Iteration 27570 Training loss 0.0022524434607475996 Validation loss 0.051459912210702896 Accuracy 0.8740000128746033\n",
      "Iteration 27580 Training loss 0.0017527468735352159 Validation loss 0.051508400589227676 Accuracy 0.8740000128746033\n",
      "Iteration 27590 Training loss 0.0015030947979539633 Validation loss 0.051569268107414246 Accuracy 0.8740000128746033\n",
      "Iteration 27600 Training loss 0.0025022930931299925 Validation loss 0.05158454179763794 Accuracy 0.8740000128746033\n",
      "Iteration 27610 Training loss 0.0017468503210693598 Validation loss 0.05164892226457596 Accuracy 0.8744999766349792\n",
      "Iteration 27620 Training loss 0.0022525815293192863 Validation loss 0.05169161409139633 Accuracy 0.8744999766349792\n",
      "Iteration 27630 Training loss 0.00175290007609874 Validation loss 0.051910437643527985 Accuracy 0.8744999766349792\n",
      "Iteration 27640 Training loss 0.002726220991462469 Validation loss 0.05192085728049278 Accuracy 0.8740000128746033\n",
      "Iteration 27650 Training loss 0.0021649845875799656 Validation loss 0.0521673820912838 Accuracy 0.875\n",
      "Iteration 27660 Training loss 0.0020055975764989853 Validation loss 0.05266648903489113 Accuracy 0.875\n",
      "Iteration 27670 Training loss 0.0025052702985703945 Validation loss 0.05230033025145531 Accuracy 0.8759999871253967\n",
      "Iteration 27680 Training loss 0.002255979459732771 Validation loss 0.05221962183713913 Accuracy 0.8755000233650208\n",
      "Iteration 27690 Training loss 0.0015038520796224475 Validation loss 0.05201400816440582 Accuracy 0.875\n",
      "Iteration 27700 Training loss 0.001508886693045497 Validation loss 0.05195410177111626 Accuracy 0.8755000233650208\n",
      "Iteration 27710 Training loss 0.0015046803746372461 Validation loss 0.051915932446718216 Accuracy 0.8759999871253967\n",
      "Iteration 27720 Training loss 0.002754402346909046 Validation loss 0.05195636674761772 Accuracy 0.875\n",
      "Iteration 27730 Training loss 0.0030067695770412683 Validation loss 0.05189308524131775 Accuracy 0.8759999871253967\n",
      "Iteration 27740 Training loss 0.002253591548651457 Validation loss 0.05186689645051956 Accuracy 0.8759999871253967\n",
      "Iteration 27750 Training loss 0.002504454692825675 Validation loss 0.05185317248106003 Accuracy 0.8759999871253967\n",
      "Iteration 27760 Training loss 0.0010037800529971719 Validation loss 0.05183005705475807 Accuracy 0.8765000104904175\n",
      "Iteration 27770 Training loss 0.002253691665828228 Validation loss 0.05177224799990654 Accuracy 0.8759999871253967\n",
      "Iteration 27780 Training loss 0.001503307488746941 Validation loss 0.05173521488904953 Accuracy 0.8755000233650208\n",
      "Iteration 27790 Training loss 0.0007552210008725524 Validation loss 0.05176367238163948 Accuracy 0.8759999871253967\n",
      "Iteration 27800 Training loss 0.0022540311329066753 Validation loss 0.05172273889183998 Accuracy 0.8755000233650208\n",
      "Iteration 27810 Training loss 0.0010031413985416293 Validation loss 0.05167623981833458 Accuracy 0.8755000233650208\n",
      "Iteration 27820 Training loss 0.0015048886416479945 Validation loss 0.0516657717525959 Accuracy 0.875\n",
      "Iteration 27830 Training loss 0.002003119559958577 Validation loss 0.051681023091077805 Accuracy 0.8755000233650208\n",
      "Iteration 27840 Training loss 0.0020032948814332485 Validation loss 0.05166078358888626 Accuracy 0.875\n",
      "Iteration 27850 Training loss 0.0020047095604240894 Validation loss 0.05165531486272812 Accuracy 0.875\n",
      "Iteration 27860 Training loss 0.0012532720575109124 Validation loss 0.05165144428610802 Accuracy 0.8744999766349792\n",
      "Iteration 27870 Training loss 0.0010034014703705907 Validation loss 0.05165237560868263 Accuracy 0.8744999766349792\n",
      "Iteration 27880 Training loss 0.001504470594227314 Validation loss 0.05162420496344566 Accuracy 0.8744999766349792\n",
      "Iteration 27890 Training loss 0.002254496095702052 Validation loss 0.05162985995411873 Accuracy 0.875\n",
      "Iteration 27900 Training loss 0.0032536275684833527 Validation loss 0.051608022302389145 Accuracy 0.875\n",
      "Iteration 27910 Training loss 0.001753900432959199 Validation loss 0.051579829305410385 Accuracy 0.8744999766349792\n",
      "Iteration 27920 Training loss 0.0015035696560516953 Validation loss 0.05155975744128227 Accuracy 0.875\n",
      "Iteration 27930 Training loss 0.0005046726437285542 Validation loss 0.05155995115637779 Accuracy 0.875\n",
      "Iteration 27940 Training loss 0.0010032537393271923 Validation loss 0.051559265702962875 Accuracy 0.875\n",
      "Iteration 27950 Training loss 0.0010029703844338655 Validation loss 0.051532845944166183 Accuracy 0.875\n",
      "Iteration 27960 Training loss 0.0010032070567831397 Validation loss 0.05151709169149399 Accuracy 0.875\n",
      "Iteration 27970 Training loss 0.001504099229350686 Validation loss 0.051502812653779984 Accuracy 0.8755000233650208\n",
      "Iteration 27980 Training loss 0.0017533039208501577 Validation loss 0.05149736627936363 Accuracy 0.8755000233650208\n",
      "Iteration 27990 Training loss 0.0015049474313855171 Validation loss 0.05147692188620567 Accuracy 0.8755000233650208\n",
      "Iteration 28000 Training loss 0.002253184327855706 Validation loss 0.0514824204146862 Accuracy 0.8755000233650208\n",
      "Iteration 28010 Training loss 0.00200303690508008 Validation loss 0.051502615213394165 Accuracy 0.8755000233650208\n",
      "Iteration 28020 Training loss 0.00200379965826869 Validation loss 0.05148926377296448 Accuracy 0.8755000233650208\n",
      "Iteration 28030 Training loss 0.0010029085678979754 Validation loss 0.05149148032069206 Accuracy 0.8755000233650208\n",
      "Iteration 28040 Training loss 0.0020033607725054026 Validation loss 0.05146858096122742 Accuracy 0.8755000233650208\n",
      "Iteration 28050 Training loss 0.0012528256047517061 Validation loss 0.05148548632860184 Accuracy 0.8759999871253967\n",
      "Iteration 28060 Training loss 0.002253137528896332 Validation loss 0.051478445529937744 Accuracy 0.8759999871253967\n",
      "Iteration 28070 Training loss 0.0017531955381855369 Validation loss 0.05145695433020592 Accuracy 0.8759999871253967\n",
      "Iteration 28080 Training loss 0.0015030766371637583 Validation loss 0.05147198587656021 Accuracy 0.8765000104904175\n",
      "Iteration 28090 Training loss 0.0010030949488282204 Validation loss 0.05148935317993164 Accuracy 0.8759999871253967\n",
      "Iteration 28100 Training loss 0.002503033494576812 Validation loss 0.05146678909659386 Accuracy 0.8765000104904175\n",
      "Iteration 28110 Training loss 0.00125331140588969 Validation loss 0.05146348476409912 Accuracy 0.8765000104904175\n",
      "Iteration 28120 Training loss 0.0005027876468375325 Validation loss 0.051455773413181305 Accuracy 0.8769999742507935\n",
      "Iteration 28130 Training loss 0.0025026751682162285 Validation loss 0.05145413428544998 Accuracy 0.8769999742507935\n",
      "Iteration 28140 Training loss 0.0030031013302505016 Validation loss 0.051456160843372345 Accuracy 0.8765000104904175\n",
      "Iteration 28150 Training loss 0.002253282582387328 Validation loss 0.051440730690956116 Accuracy 0.8765000104904175\n",
      "Iteration 28160 Training loss 0.0012538735754787922 Validation loss 0.051442746073007584 Accuracy 0.8765000104904175\n",
      "Iteration 28170 Training loss 0.0012535512214526534 Validation loss 0.05145514756441116 Accuracy 0.8765000104904175\n",
      "Iteration 28180 Training loss 0.001753033371642232 Validation loss 0.05145405977964401 Accuracy 0.8769999742507935\n",
      "Iteration 28190 Training loss 0.001003759098239243 Validation loss 0.05144750326871872 Accuracy 0.8769999742507935\n",
      "Iteration 28200 Training loss 0.002502613002434373 Validation loss 0.05146444961428642 Accuracy 0.8769999742507935\n",
      "Iteration 28210 Training loss 0.0017531819175928831 Validation loss 0.051451425999403 Accuracy 0.8765000104904175\n",
      "Iteration 28220 Training loss 0.0007524346001446247 Validation loss 0.05143408849835396 Accuracy 0.8769999742507935\n",
      "Iteration 28230 Training loss 0.002002851804718375 Validation loss 0.05144118517637253 Accuracy 0.8769999742507935\n",
      "Iteration 28240 Training loss 0.0010029200930148363 Validation loss 0.05143415927886963 Accuracy 0.8765000104904175\n",
      "Iteration 28250 Training loss 0.0017531593330204487 Validation loss 0.0514284111559391 Accuracy 0.8765000104904175\n",
      "Iteration 28260 Training loss 0.002253569196909666 Validation loss 0.05142318084836006 Accuracy 0.8769999742507935\n",
      "Iteration 28270 Training loss 0.003002788871526718 Validation loss 0.05143657699227333 Accuracy 0.8769999742507935\n",
      "Iteration 28280 Training loss 0.001753170508891344 Validation loss 0.051426585763692856 Accuracy 0.8769999742507935\n",
      "Iteration 28290 Training loss 0.0020027547143399715 Validation loss 0.051431749016046524 Accuracy 0.8769999742507935\n",
      "Iteration 28300 Training loss 0.0017528545577079058 Validation loss 0.05142666772007942 Accuracy 0.8769999742507935\n",
      "Iteration 28310 Training loss 0.002503344090655446 Validation loss 0.05143088474869728 Accuracy 0.8769999742507935\n",
      "Iteration 28320 Training loss 0.0017526590963825583 Validation loss 0.05142361670732498 Accuracy 0.8769999742507935\n",
      "Iteration 28330 Training loss 0.0015024520689621568 Validation loss 0.05142579600214958 Accuracy 0.8769999742507935\n",
      "Iteration 28340 Training loss 0.0027529308572411537 Validation loss 0.05141064524650574 Accuracy 0.8769999742507935\n",
      "Iteration 28350 Training loss 0.0012526712380349636 Validation loss 0.05141056701540947 Accuracy 0.8769999742507935\n",
      "Iteration 28360 Training loss 0.0025026260409504175 Validation loss 0.05140912905335426 Accuracy 0.8769999742507935\n",
      "Iteration 28370 Training loss 0.002253467682749033 Validation loss 0.051406122744083405 Accuracy 0.8769999742507935\n",
      "Iteration 28380 Training loss 0.002252894453704357 Validation loss 0.051403820514678955 Accuracy 0.8769999742507935\n",
      "Iteration 28390 Training loss 0.0027527669444680214 Validation loss 0.051418475806713104 Accuracy 0.8765000104904175\n",
      "Iteration 28400 Training loss 0.003252893453463912 Validation loss 0.05141337960958481 Accuracy 0.8765000104904175\n",
      "Iteration 28410 Training loss 0.0017533329082652926 Validation loss 0.051420263946056366 Accuracy 0.8759999871253967\n",
      "Iteration 28420 Training loss 0.0020030750893056393 Validation loss 0.05140693113207817 Accuracy 0.8765000104904175\n",
      "Iteration 28430 Training loss 0.0010028452379629016 Validation loss 0.051423329859972 Accuracy 0.8765000104904175\n",
      "Iteration 28440 Training loss 0.0020028743892908096 Validation loss 0.051411256194114685 Accuracy 0.8765000104904175\n",
      "Iteration 28450 Training loss 0.0030027444008737803 Validation loss 0.05141895264387131 Accuracy 0.8765000104904175\n",
      "Iteration 28460 Training loss 0.002502741292119026 Validation loss 0.05140914022922516 Accuracy 0.8765000104904175\n",
      "Iteration 28470 Training loss 0.002252990147098899 Validation loss 0.051405441015958786 Accuracy 0.8765000104904175\n",
      "Iteration 28480 Training loss 0.0022528539411723614 Validation loss 0.05142566189169884 Accuracy 0.8759999871253967\n",
      "Iteration 28490 Training loss 0.001003001700155437 Validation loss 0.05141235515475273 Accuracy 0.8765000104904175\n",
      "Iteration 28500 Training loss 0.002252776175737381 Validation loss 0.05140295997262001 Accuracy 0.8765000104904175\n",
      "Iteration 28510 Training loss 0.002502590650692582 Validation loss 0.051410142332315445 Accuracy 0.8765000104904175\n",
      "Iteration 28520 Training loss 0.0012530939420685172 Validation loss 0.05141103267669678 Accuracy 0.8765000104904175\n",
      "Iteration 28530 Training loss 0.0025027827359735966 Validation loss 0.05139966681599617 Accuracy 0.8765000104904175\n",
      "Iteration 28540 Training loss 0.002002752386033535 Validation loss 0.05140502750873566 Accuracy 0.8765000104904175\n",
      "Iteration 28550 Training loss 0.0007525546825490892 Validation loss 0.05140286684036255 Accuracy 0.8765000104904175\n",
      "Iteration 28560 Training loss 0.0015031119110062718 Validation loss 0.05139555037021637 Accuracy 0.8765000104904175\n",
      "Iteration 28570 Training loss 0.0010024645598605275 Validation loss 0.051402535289525986 Accuracy 0.8765000104904175\n",
      "Iteration 28580 Training loss 0.0015027899062260985 Validation loss 0.0513923317193985 Accuracy 0.8765000104904175\n",
      "Iteration 28590 Training loss 0.0020028629805892706 Validation loss 0.05139646679162979 Accuracy 0.8765000104904175\n",
      "Iteration 28600 Training loss 0.0022526653483510017 Validation loss 0.051410041749477386 Accuracy 0.8765000104904175\n",
      "Iteration 28610 Training loss 0.0015028933994472027 Validation loss 0.051402609795331955 Accuracy 0.8765000104904175\n",
      "Iteration 28620 Training loss 0.0020025314297527075 Validation loss 0.05140405893325806 Accuracy 0.8765000104904175\n",
      "Iteration 28630 Training loss 0.0017526190495118499 Validation loss 0.05139162391424179 Accuracy 0.8765000104904175\n",
      "Iteration 28640 Training loss 0.002002578927204013 Validation loss 0.05140191689133644 Accuracy 0.8765000104904175\n",
      "Iteration 28650 Training loss 0.001502706902101636 Validation loss 0.05139670893549919 Accuracy 0.8765000104904175\n",
      "Iteration 28660 Training loss 0.0012530997628346086 Validation loss 0.05139317363500595 Accuracy 0.8765000104904175\n",
      "Iteration 28670 Training loss 0.0012529680971056223 Validation loss 0.05139166861772537 Accuracy 0.8765000104904175\n",
      "Iteration 28680 Training loss 0.001502741826698184 Validation loss 0.051385726779699326 Accuracy 0.8769999742507935\n",
      "Iteration 28690 Training loss 0.0007528740679845214 Validation loss 0.05138282850384712 Accuracy 0.8765000104904175\n",
      "Iteration 28700 Training loss 0.001002723933197558 Validation loss 0.051388930529356 Accuracy 0.8769999742507935\n",
      "Iteration 28710 Training loss 0.0012525393394753337 Validation loss 0.051392510533332825 Accuracy 0.8765000104904175\n",
      "Iteration 28720 Training loss 0.0017525861039757729 Validation loss 0.051375724375247955 Accuracy 0.8769999742507935\n",
      "Iteration 28730 Training loss 0.001502852886915207 Validation loss 0.05139169469475746 Accuracy 0.8765000104904175\n",
      "Iteration 28740 Training loss 0.0017524617724120617 Validation loss 0.051393769681453705 Accuracy 0.8765000104904175\n",
      "Iteration 28750 Training loss 0.0010028447723016143 Validation loss 0.05139464884996414 Accuracy 0.8765000104904175\n",
      "Iteration 28760 Training loss 0.00150271225720644 Validation loss 0.05139937624335289 Accuracy 0.8765000104904175\n",
      "Iteration 28770 Training loss 0.0010027543175965548 Validation loss 0.05139036476612091 Accuracy 0.8765000104904175\n",
      "Iteration 28780 Training loss 0.0027528537902981043 Validation loss 0.0513823926448822 Accuracy 0.8765000104904175\n",
      "Iteration 28790 Training loss 0.00100249785464257 Validation loss 0.0513828806579113 Accuracy 0.8765000104904175\n",
      "Iteration 28800 Training loss 0.0015031552175059915 Validation loss 0.05138035863637924 Accuracy 0.8765000104904175\n",
      "Iteration 28810 Training loss 0.0015030141221359372 Validation loss 0.05138678476214409 Accuracy 0.8765000104904175\n",
      "Iteration 28820 Training loss 0.002502841642126441 Validation loss 0.05138523504137993 Accuracy 0.8765000104904175\n",
      "Iteration 28830 Training loss 0.0020030690357089043 Validation loss 0.051378119736909866 Accuracy 0.8765000104904175\n",
      "Iteration 28840 Training loss 0.0025024167262017727 Validation loss 0.051370274275541306 Accuracy 0.8765000104904175\n",
      "Iteration 28850 Training loss 0.0027525904588401318 Validation loss 0.051381926983594894 Accuracy 0.8765000104904175\n",
      "Iteration 28860 Training loss 0.0010023029753938317 Validation loss 0.0513872355222702 Accuracy 0.8765000104904175\n",
      "Iteration 28870 Training loss 0.0010024309158325195 Validation loss 0.05138978734612465 Accuracy 0.8765000104904175\n",
      "Iteration 28880 Training loss 0.0005029234453104436 Validation loss 0.05138864368200302 Accuracy 0.8765000104904175\n",
      "Iteration 28890 Training loss 0.0017528056632727385 Validation loss 0.051381684839725494 Accuracy 0.8765000104904175\n",
      "Iteration 28900 Training loss 0.002252749400213361 Validation loss 0.05138664320111275 Accuracy 0.8765000104904175\n",
      "Iteration 28910 Training loss 0.0025024767965078354 Validation loss 0.051388099789619446 Accuracy 0.8769999742507935\n",
      "Iteration 28920 Training loss 0.0007523616077378392 Validation loss 0.05138860642910004 Accuracy 0.8769999742507935\n",
      "Iteration 28930 Training loss 0.0030025134328752756 Validation loss 0.05139120668172836 Accuracy 0.8769999742507935\n",
      "Iteration 28940 Training loss 0.0015026639448478818 Validation loss 0.05138169229030609 Accuracy 0.8769999742507935\n",
      "Iteration 28950 Training loss 0.0025026709772646427 Validation loss 0.05137792229652405 Accuracy 0.8769999742507935\n",
      "Iteration 28960 Training loss 0.0012527916114777327 Validation loss 0.051375288516283035 Accuracy 0.8765000104904175\n",
      "Iteration 28970 Training loss 0.001752423238940537 Validation loss 0.05138203874230385 Accuracy 0.8769999742507935\n",
      "Iteration 28980 Training loss 0.0012521696044132113 Validation loss 0.05138234421610832 Accuracy 0.8769999742507935\n",
      "Iteration 28990 Training loss 0.002502499148249626 Validation loss 0.051394835114479065 Accuracy 0.8765000104904175\n",
      "Iteration 29000 Training loss 0.0030026210006326437 Validation loss 0.05139944329857826 Accuracy 0.8765000104904175\n",
      "Iteration 29010 Training loss 0.0015026183100417256 Validation loss 0.051395732909440994 Accuracy 0.8765000104904175\n",
      "Iteration 29020 Training loss 0.0022522720973938704 Validation loss 0.0513911098241806 Accuracy 0.8769999742507935\n",
      "Iteration 29030 Training loss 0.0010023231152445078 Validation loss 0.05139075964689255 Accuracy 0.8769999742507935\n",
      "Iteration 29040 Training loss 0.0025028504896909 Validation loss 0.051385995000600815 Accuracy 0.8765000104904175\n",
      "Iteration 29050 Training loss 0.0022525181993842125 Validation loss 0.051387540996074677 Accuracy 0.8765000104904175\n",
      "Iteration 29060 Training loss 0.0010020973859354854 Validation loss 0.05139550939202309 Accuracy 0.8765000104904175\n",
      "Iteration 29070 Training loss 0.00075246935011819 Validation loss 0.05139710009098053 Accuracy 0.8765000104904175\n",
      "Iteration 29080 Training loss 0.0005024639540351927 Validation loss 0.0514003150165081 Accuracy 0.8765000104904175\n",
      "Iteration 29090 Training loss 0.0010023684008046985 Validation loss 0.051392342895269394 Accuracy 0.8765000104904175\n",
      "Iteration 29100 Training loss 0.0015026837354525924 Validation loss 0.051393888890743256 Accuracy 0.8765000104904175\n",
      "Iteration 29110 Training loss 0.001752763520926237 Validation loss 0.05139141529798508 Accuracy 0.8765000104904175\n",
      "Iteration 29120 Training loss 0.0020024976693093777 Validation loss 0.05138867720961571 Accuracy 0.8765000104904175\n",
      "Iteration 29130 Training loss 0.0010026567615568638 Validation loss 0.05138809606432915 Accuracy 0.8765000104904175\n",
      "Iteration 29140 Training loss 0.0025021599140018225 Validation loss 0.051394324749708176 Accuracy 0.8765000104904175\n",
      "Iteration 29150 Training loss 0.001252408022992313 Validation loss 0.051392484456300735 Accuracy 0.8765000104904175\n",
      "Iteration 29160 Training loss 0.001252761110663414 Validation loss 0.0513855442404747 Accuracy 0.8765000104904175\n",
      "Iteration 29170 Training loss 0.002002543769776821 Validation loss 0.05138262361288071 Accuracy 0.8765000104904175\n",
      "Iteration 29180 Training loss 0.0012522154720500112 Validation loss 0.05137926712632179 Accuracy 0.8765000104904175\n",
      "Iteration 29190 Training loss 0.0020027838181704283 Validation loss 0.05137922614812851 Accuracy 0.8765000104904175\n",
      "Iteration 29200 Training loss 0.0022526259999722242 Validation loss 0.05138292536139488 Accuracy 0.8765000104904175\n",
      "Iteration 29210 Training loss 0.0020022280514240265 Validation loss 0.0513833686709404 Accuracy 0.8765000104904175\n",
      "Iteration 29220 Training loss 0.003002411685883999 Validation loss 0.05137982964515686 Accuracy 0.8769999742507935\n",
      "Iteration 29230 Training loss 0.001252342713996768 Validation loss 0.05137334018945694 Accuracy 0.8769999742507935\n",
      "Iteration 29240 Training loss 0.0012526013888418674 Validation loss 0.05136536434292793 Accuracy 0.8769999742507935\n",
      "Iteration 29250 Training loss 0.0015025959582999349 Validation loss 0.051372230052948 Accuracy 0.8769999742507935\n",
      "Iteration 29260 Training loss 0.002752358326688409 Validation loss 0.05137601122260094 Accuracy 0.8769999742507935\n",
      "Iteration 29270 Training loss 0.0017526287119835615 Validation loss 0.051377005875110626 Accuracy 0.8769999742507935\n",
      "Iteration 29280 Training loss 0.0012526012724265456 Validation loss 0.051376935094594955 Accuracy 0.8769999742507935\n",
      "Iteration 29290 Training loss 0.0032525735441595316 Validation loss 0.051375966519117355 Accuracy 0.8769999742507935\n",
      "Iteration 29300 Training loss 0.001502191531471908 Validation loss 0.05138588696718216 Accuracy 0.8765000104904175\n",
      "Iteration 29310 Training loss 0.0012527822982519865 Validation loss 0.05138325318694115 Accuracy 0.8765000104904175\n",
      "Iteration 29320 Training loss 0.0015023701125755906 Validation loss 0.051380615681409836 Accuracy 0.8765000104904175\n",
      "Iteration 29330 Training loss 0.0010027262615039945 Validation loss 0.05138072744011879 Accuracy 0.8765000104904175\n",
      "Iteration 29340 Training loss 0.0017524334834888577 Validation loss 0.05139340087771416 Accuracy 0.8765000104904175\n",
      "Iteration 29350 Training loss 0.0017526241717860103 Validation loss 0.05139799416065216 Accuracy 0.8765000104904175\n",
      "Iteration 29360 Training loss 0.002752678468823433 Validation loss 0.051393814384937286 Accuracy 0.8765000104904175\n",
      "Iteration 29370 Training loss 0.002252528676763177 Validation loss 0.05139223113656044 Accuracy 0.8765000104904175\n",
      "Iteration 29380 Training loss 0.0012522850884124637 Validation loss 0.05139379948377609 Accuracy 0.8765000104904175\n",
      "Iteration 29390 Training loss 0.002002253895625472 Validation loss 0.0513797365128994 Accuracy 0.8769999742507935\n",
      "Iteration 29400 Training loss 0.0007524508400820196 Validation loss 0.051379334181547165 Accuracy 0.8769999742507935\n",
      "Iteration 29410 Training loss 0.0020022005774080753 Validation loss 0.0513916090130806 Accuracy 0.8765000104904175\n",
      "Iteration 29420 Training loss 0.0015025261091068387 Validation loss 0.051386747509241104 Accuracy 0.8769999742507935\n",
      "Iteration 29430 Training loss 0.0030020095873624086 Validation loss 0.05139346420764923 Accuracy 0.8769999742507935\n",
      "Iteration 29440 Training loss 0.0010023225331678987 Validation loss 0.05138939619064331 Accuracy 0.8769999742507935\n",
      "Iteration 29450 Training loss 0.0017525784205645323 Validation loss 0.05139485001564026 Accuracy 0.8769999742507935\n",
      "Iteration 29460 Training loss 0.0017525928560644388 Validation loss 0.05139008164405823 Accuracy 0.8769999742507935\n",
      "Iteration 29470 Training loss 0.0015023088781163096 Validation loss 0.051392413675785065 Accuracy 0.8769999742507935\n",
      "Iteration 29480 Training loss 0.0022523177322000265 Validation loss 0.05139518901705742 Accuracy 0.8769999742507935\n",
      "Iteration 29490 Training loss 0.0012525785714387894 Validation loss 0.05139412730932236 Accuracy 0.8769999742507935\n",
      "Iteration 29500 Training loss 0.0015025229658931494 Validation loss 0.05139676481485367 Accuracy 0.8769999742507935\n",
      "Iteration 29510 Training loss 0.0015020392602309585 Validation loss 0.05139532312750816 Accuracy 0.8769999742507935\n",
      "Iteration 29520 Training loss 0.0015022975858300924 Validation loss 0.051400184631347656 Accuracy 0.8769999742507935\n",
      "Iteration 29530 Training loss 0.0012524807825684547 Validation loss 0.05140208452939987 Accuracy 0.8769999742507935\n",
      "Iteration 29540 Training loss 0.0010022365022450686 Validation loss 0.051398150622844696 Accuracy 0.8769999742507935\n",
      "Iteration 29550 Training loss 0.0020023088436573744 Validation loss 0.05139423534274101 Accuracy 0.8769999742507935\n",
      "Iteration 29560 Training loss 0.0015023117884993553 Validation loss 0.05140391364693642 Accuracy 0.8769999742507935\n",
      "Iteration 29570 Training loss 0.0007527166162617505 Validation loss 0.051410652697086334 Accuracy 0.8765000104904175\n",
      "Iteration 29580 Training loss 0.0010026361560449004 Validation loss 0.05141439661383629 Accuracy 0.8769999742507935\n",
      "Iteration 29590 Training loss 0.0025022656191140413 Validation loss 0.05140739306807518 Accuracy 0.8769999742507935\n",
      "Iteration 29600 Training loss 0.0010020388290286064 Validation loss 0.05139849707484245 Accuracy 0.8769999742507935\n",
      "Iteration 29610 Training loss 0.0017525424482300878 Validation loss 0.05140263959765434 Accuracy 0.8769999742507935\n",
      "Iteration 29620 Training loss 0.0007521676016040146 Validation loss 0.05140502005815506 Accuracy 0.8769999742507935\n",
      "Iteration 29630 Training loss 0.0015028066700324416 Validation loss 0.051403168588876724 Accuracy 0.8769999742507935\n",
      "Iteration 29640 Training loss 0.0012522035976871848 Validation loss 0.05140255019068718 Accuracy 0.8769999742507935\n",
      "Iteration 29650 Training loss 0.0017529195174574852 Validation loss 0.051399558782577515 Accuracy 0.8769999742507935\n",
      "Iteration 29660 Training loss 0.0022522902581840754 Validation loss 0.05140148103237152 Accuracy 0.8769999742507935\n",
      "Iteration 29670 Training loss 0.002752413507550955 Validation loss 0.05140546336770058 Accuracy 0.8769999742507935\n",
      "Iteration 29680 Training loss 0.002252064412459731 Validation loss 0.051403265446424484 Accuracy 0.8769999742507935\n",
      "Iteration 29690 Training loss 0.0017523096175864339 Validation loss 0.051411110907793045 Accuracy 0.8769999742507935\n",
      "Iteration 29700 Training loss 0.0020021472591906786 Validation loss 0.05140551179647446 Accuracy 0.8769999742507935\n",
      "Iteration 29710 Training loss 0.0015020654536783695 Validation loss 0.05140199139714241 Accuracy 0.8769999742507935\n",
      "Iteration 29720 Training loss 0.001752368756569922 Validation loss 0.051404353231191635 Accuracy 0.8769999742507935\n",
      "Iteration 29730 Training loss 0.0010023548966273665 Validation loss 0.05140279605984688 Accuracy 0.8769999742507935\n",
      "Iteration 29740 Training loss 0.002502479124814272 Validation loss 0.05140803009271622 Accuracy 0.8769999742507935\n",
      "Iteration 29750 Training loss 0.001002251636236906 Validation loss 0.051411572843790054 Accuracy 0.8769999742507935\n",
      "Iteration 29760 Training loss 0.0010022972710430622 Validation loss 0.051413390785455704 Accuracy 0.8769999742507935\n",
      "Iteration 29770 Training loss 0.001502164639532566 Validation loss 0.05140715837478638 Accuracy 0.8769999742507935\n",
      "Iteration 29780 Training loss 0.002502117305994034 Validation loss 0.051407840102910995 Accuracy 0.8769999742507935\n",
      "Iteration 29790 Training loss 0.0015024137683212757 Validation loss 0.051406074315309525 Accuracy 0.8769999742507935\n",
      "Iteration 29800 Training loss 0.0012524412013590336 Validation loss 0.0513981394469738 Accuracy 0.8769999742507935\n",
      "Iteration 29810 Training loss 0.0020025279372930527 Validation loss 0.0513988733291626 Accuracy 0.8769999742507935\n",
      "Iteration 29820 Training loss 0.0010024196235463023 Validation loss 0.051401782780885696 Accuracy 0.8769999742507935\n",
      "Iteration 29830 Training loss 0.002502112416550517 Validation loss 0.051401685923337936 Accuracy 0.8769999742507935\n",
      "Iteration 29840 Training loss 0.0015021737199276686 Validation loss 0.05140097811818123 Accuracy 0.8769999742507935\n",
      "Iteration 29850 Training loss 0.0022523091174662113 Validation loss 0.05140797048807144 Accuracy 0.8769999742507935\n",
      "Iteration 29860 Training loss 0.0012527324724942446 Validation loss 0.051410116255283356 Accuracy 0.8769999742507935\n",
      "Iteration 29870 Training loss 0.0017522136913612485 Validation loss 0.05140724033117294 Accuracy 0.8769999742507935\n",
      "Iteration 29880 Training loss 0.0015023243613541126 Validation loss 0.05140461400151253 Accuracy 0.8769999742507935\n",
      "Iteration 29890 Training loss 0.0017521067056804895 Validation loss 0.051403824239969254 Accuracy 0.8769999742507935\n",
      "Iteration 29900 Training loss 0.002002251334488392 Validation loss 0.051405180245637894 Accuracy 0.8769999742507935\n",
      "Iteration 29910 Training loss 0.0025018586311489344 Validation loss 0.051401205360889435 Accuracy 0.8769999742507935\n",
      "Iteration 29920 Training loss 0.001501963590271771 Validation loss 0.05140097811818123 Accuracy 0.8769999742507935\n",
      "Iteration 29930 Training loss 0.002252183621749282 Validation loss 0.051392655819654465 Accuracy 0.8769999742507935\n",
      "Iteration 29940 Training loss 0.0005024406127631664 Validation loss 0.051388081163167953 Accuracy 0.8769999742507935\n",
      "Iteration 29950 Training loss 0.002502258401364088 Validation loss 0.051390182226896286 Accuracy 0.8769999742507935\n",
      "Iteration 29960 Training loss 0.0012523207115009427 Validation loss 0.051389068365097046 Accuracy 0.8769999742507935\n",
      "Iteration 29970 Training loss 0.0020020476076751947 Validation loss 0.051392458379268646 Accuracy 0.8769999742507935\n",
      "Iteration 29980 Training loss 0.0020020902156829834 Validation loss 0.051398247480392456 Accuracy 0.8769999742507935\n",
      "Iteration 29990 Training loss 0.0022519612684845924 Validation loss 0.05140232294797897 Accuracy 0.8769999742507935\n",
      "Iteration 30000 Training loss 0.0017525135772302747 Validation loss 0.051404766738414764 Accuracy 0.8769999742507935\n",
      "Iteration 30010 Training loss 0.002252263016998768 Validation loss 0.05140348896384239 Accuracy 0.8769999742507935\n",
      "Iteration 30020 Training loss 0.0017520887777209282 Validation loss 0.051393888890743256 Accuracy 0.8769999742507935\n",
      "Iteration 30030 Training loss 0.002252065110951662 Validation loss 0.05139423906803131 Accuracy 0.8769999742507935\n",
      "Iteration 30040 Training loss 0.000752404099330306 Validation loss 0.05139366164803505 Accuracy 0.8769999742507935\n",
      "Iteration 30050 Training loss 0.0025025184731930494 Validation loss 0.05139344930648804 Accuracy 0.8769999742507935\n",
      "Iteration 30060 Training loss 0.0012524471385404468 Validation loss 0.051398977637290955 Accuracy 0.8769999742507935\n",
      "Iteration 30070 Training loss 0.0017521799309179187 Validation loss 0.051397982984781265 Accuracy 0.8769999742507935\n",
      "Iteration 30080 Training loss 0.002502295421436429 Validation loss 0.05139683559536934 Accuracy 0.8769999742507935\n",
      "Iteration 30090 Training loss 0.002252461388707161 Validation loss 0.05139857158064842 Accuracy 0.8769999742507935\n",
      "Iteration 30100 Training loss 0.0022524588275700808 Validation loss 0.05139244347810745 Accuracy 0.8769999742507935\n",
      "Iteration 30110 Training loss 0.0020021898671984673 Validation loss 0.051410868763923645 Accuracy 0.8769999742507935\n",
      "Iteration 30120 Training loss 0.0015020491555333138 Validation loss 0.05140838772058487 Accuracy 0.8769999742507935\n",
      "Iteration 30130 Training loss 0.002502638380974531 Validation loss 0.05140924081206322 Accuracy 0.8769999742507935\n",
      "Iteration 30140 Training loss 0.0020023519173264503 Validation loss 0.051401007920503616 Accuracy 0.8769999742507935\n",
      "Iteration 30150 Training loss 0.0020022804383188486 Validation loss 0.05140344798564911 Accuracy 0.8769999742507935\n",
      "Iteration 30160 Training loss 0.002502373419702053 Validation loss 0.05140284448862076 Accuracy 0.8769999742507935\n",
      "Iteration 30170 Training loss 0.0027520337607711554 Validation loss 0.051402825862169266 Accuracy 0.8769999742507935\n",
      "Iteration 30180 Training loss 0.0010022311471402645 Validation loss 0.051412418484687805 Accuracy 0.8769999742507935\n",
      "Iteration 30190 Training loss 0.002002568216994405 Validation loss 0.051413360983133316 Accuracy 0.8769999742507935\n",
      "Iteration 30200 Training loss 0.0020022052340209484 Validation loss 0.05140628665685654 Accuracy 0.8769999742507935\n",
      "Iteration 30210 Training loss 0.0017520287074148655 Validation loss 0.05140317231416702 Accuracy 0.8769999742507935\n",
      "Iteration 30220 Training loss 0.00225225486792624 Validation loss 0.051412805914878845 Accuracy 0.8769999742507935\n",
      "Iteration 30230 Training loss 0.002502006245777011 Validation loss 0.05140889063477516 Accuracy 0.8769999742507935\n",
      "Iteration 30240 Training loss 0.002501874463632703 Validation loss 0.05141882970929146 Accuracy 0.8769999742507935\n",
      "Iteration 30250 Training loss 0.0027521508745849133 Validation loss 0.051410965621471405 Accuracy 0.8769999742507935\n",
      "Iteration 30260 Training loss 0.0015021618455648422 Validation loss 0.05140434578061104 Accuracy 0.8769999742507935\n",
      "Iteration 30270 Training loss 0.0010023898212239146 Validation loss 0.05140312761068344 Accuracy 0.8769999742507935\n",
      "Iteration 30280 Training loss 0.00250210240483284 Validation loss 0.05140083655714989 Accuracy 0.8769999742507935\n",
      "Iteration 30290 Training loss 0.0012520741438493133 Validation loss 0.05139201506972313 Accuracy 0.8769999742507935\n",
      "Iteration 30300 Training loss 0.00175205257255584 Validation loss 0.05140320584177971 Accuracy 0.8769999742507935\n",
      "Iteration 30310 Training loss 0.0015022511361166835 Validation loss 0.051391784101724625 Accuracy 0.8769999742507935\n",
      "Iteration 30320 Training loss 0.000751976971514523 Validation loss 0.051398321986198425 Accuracy 0.8769999742507935\n",
      "Iteration 30330 Training loss 0.0012521558674052358 Validation loss 0.05140211433172226 Accuracy 0.8769999742507935\n",
      "Iteration 30340 Training loss 0.00200213142670691 Validation loss 0.05139949172735214 Accuracy 0.8769999742507935\n",
      "Iteration 30350 Training loss 0.0017523192800581455 Validation loss 0.051396340131759644 Accuracy 0.8769999742507935\n",
      "Iteration 30360 Training loss 0.0022522371727973223 Validation loss 0.051403556019067764 Accuracy 0.8769999742507935\n",
      "Iteration 30370 Training loss 0.0022520667407661676 Validation loss 0.05139922723174095 Accuracy 0.8769999742507935\n",
      "Iteration 30380 Training loss 0.0012521655298769474 Validation loss 0.051397666335105896 Accuracy 0.8769999742507935\n",
      "Iteration 30390 Training loss 0.0012519005686044693 Validation loss 0.05139358341693878 Accuracy 0.8769999742507935\n",
      "Iteration 30400 Training loss 0.0015020368155092 Validation loss 0.051394205540418625 Accuracy 0.8769999742507935\n",
      "Iteration 30410 Training loss 0.0005022754776291549 Validation loss 0.05139748379588127 Accuracy 0.8769999742507935\n",
      "Iteration 30420 Training loss 0.001252169837243855 Validation loss 0.0513901449739933 Accuracy 0.8769999742507935\n",
      "Iteration 30430 Training loss 0.0030023562721908092 Validation loss 0.05138629674911499 Accuracy 0.8765000104904175\n",
      "Iteration 30440 Training loss 0.0020022306125611067 Validation loss 0.051395826041698456 Accuracy 0.8769999742507935\n",
      "Iteration 30450 Training loss 0.0012523758923634887 Validation loss 0.05140051245689392 Accuracy 0.8765000104904175\n",
      "Iteration 30460 Training loss 0.0017519986722618341 Validation loss 0.0514005608856678 Accuracy 0.8765000104904175\n",
      "Iteration 30470 Training loss 0.001252007088623941 Validation loss 0.051400765776634216 Accuracy 0.8765000104904175\n",
      "Iteration 30480 Training loss 0.0020020531956106424 Validation loss 0.051401328295469284 Accuracy 0.8769999742507935\n",
      "Iteration 30490 Training loss 0.0020024063996970654 Validation loss 0.05139998719096184 Accuracy 0.8769999742507935\n",
      "Iteration 30500 Training loss 0.001751790288835764 Validation loss 0.0514078252017498 Accuracy 0.8769999742507935\n",
      "Iteration 30510 Training loss 0.0010021505877375603 Validation loss 0.05141125246882439 Accuracy 0.8769999742507935\n",
      "Iteration 30520 Training loss 0.0017518787644803524 Validation loss 0.051405906677246094 Accuracy 0.8765000104904175\n",
      "Iteration 30530 Training loss 0.002002072986215353 Validation loss 0.05140387639403343 Accuracy 0.8765000104904175\n",
      "Iteration 30540 Training loss 0.002001922344788909 Validation loss 0.05140138417482376 Accuracy 0.8769999742507935\n",
      "Iteration 30550 Training loss 0.002502238377928734 Validation loss 0.05140386149287224 Accuracy 0.8769999742507935\n",
      "Iteration 30560 Training loss 0.0030019807163625956 Validation loss 0.05139746144413948 Accuracy 0.8769999742507935\n",
      "Iteration 30570 Training loss 0.0020020087249577045 Validation loss 0.05139867961406708 Accuracy 0.8769999742507935\n",
      "Iteration 30580 Training loss 0.0012519919546321034 Validation loss 0.051402416080236435 Accuracy 0.8769999742507935\n",
      "Iteration 30590 Training loss 0.0017522043781355023 Validation loss 0.05140351504087448 Accuracy 0.8769999742507935\n",
      "Iteration 30600 Training loss 0.001002064673230052 Validation loss 0.05140308290719986 Accuracy 0.8769999742507935\n",
      "Iteration 30610 Training loss 0.0015022229636088014 Validation loss 0.051402103155851364 Accuracy 0.8769999742507935\n",
      "Iteration 30620 Training loss 0.0020020208321511745 Validation loss 0.05139672011137009 Accuracy 0.8769999742507935\n",
      "Iteration 30630 Training loss 0.0012521416647359729 Validation loss 0.051406051963567734 Accuracy 0.8769999742507935\n",
      "Iteration 30640 Training loss 0.0010019154287874699 Validation loss 0.05140811949968338 Accuracy 0.8769999742507935\n",
      "Iteration 30650 Training loss 0.002252220408990979 Validation loss 0.05140208080410957 Accuracy 0.8769999742507935\n",
      "Iteration 30660 Training loss 0.0027524821925908327 Validation loss 0.05140572786331177 Accuracy 0.8769999742507935\n",
      "Iteration 30670 Training loss 0.0015020351856946945 Validation loss 0.051408857107162476 Accuracy 0.8769999742507935\n",
      "Iteration 30680 Training loss 0.0010020221816375852 Validation loss 0.05141155794262886 Accuracy 0.8769999742507935\n",
      "Iteration 30690 Training loss 0.002252199687063694 Validation loss 0.051408249884843826 Accuracy 0.8769999742507935\n",
      "Iteration 30700 Training loss 0.0012523126788437366 Validation loss 0.051412053406238556 Accuracy 0.8765000104904175\n",
      "Iteration 30710 Training loss 0.0017523030983284116 Validation loss 0.05141732841730118 Accuracy 0.8765000104904175\n",
      "Iteration 30720 Training loss 0.0020021216478198767 Validation loss 0.05140933766961098 Accuracy 0.8769999742507935\n",
      "Iteration 30730 Training loss 0.0027523126918822527 Validation loss 0.051412153989076614 Accuracy 0.8769999742507935\n",
      "Iteration 30740 Training loss 0.0022520923521369696 Validation loss 0.05141303315758705 Accuracy 0.8769999742507935\n",
      "Iteration 30750 Training loss 0.0022521072532981634 Validation loss 0.051414575427770615 Accuracy 0.8769999742507935\n",
      "Iteration 30760 Training loss 0.002252392703667283 Validation loss 0.051417991518974304 Accuracy 0.8769999742507935\n",
      "Iteration 30770 Training loss 0.0022519268095493317 Validation loss 0.05141603574156761 Accuracy 0.8769999742507935\n",
      "Iteration 30780 Training loss 0.0020020967349410057 Validation loss 0.05141686275601387 Accuracy 0.8769999742507935\n",
      "Iteration 30790 Training loss 0.0020020105876028538 Validation loss 0.051416050642728806 Accuracy 0.8765000104904175\n",
      "Iteration 30800 Training loss 0.002002285560593009 Validation loss 0.05141729861497879 Accuracy 0.8765000104904175\n",
      "Iteration 30810 Training loss 0.003002018900588155 Validation loss 0.051425736397504807 Accuracy 0.8769999742507935\n",
      "Iteration 30820 Training loss 0.002502287272363901 Validation loss 0.05142436549067497 Accuracy 0.8769999742507935\n",
      "Iteration 30830 Training loss 0.001752267125993967 Validation loss 0.05142217129468918 Accuracy 0.8765000104904175\n",
      "Iteration 30840 Training loss 0.0017523063579574227 Validation loss 0.05141935870051384 Accuracy 0.8769999742507935\n",
      "Iteration 30850 Training loss 0.00225228164345026 Validation loss 0.05141393095254898 Accuracy 0.8765000104904175\n",
      "Iteration 30860 Training loss 0.001502065802924335 Validation loss 0.051412276923656464 Accuracy 0.8769999742507935\n",
      "Iteration 30870 Training loss 0.0012521266471594572 Validation loss 0.05141231417655945 Accuracy 0.8769999742507935\n",
      "Iteration 30880 Training loss 0.002752139465883374 Validation loss 0.05141311511397362 Accuracy 0.8765000104904175\n",
      "Iteration 30890 Training loss 0.0020022231619805098 Validation loss 0.05141520872712135 Accuracy 0.8765000104904175\n",
      "Iteration 30900 Training loss 0.0020018527284264565 Validation loss 0.05141562968492508 Accuracy 0.8765000104904175\n",
      "Iteration 30910 Training loss 0.002502020914107561 Validation loss 0.05141446739435196 Accuracy 0.8769999742507935\n",
      "Iteration 30920 Training loss 0.000502046023029834 Validation loss 0.051411330699920654 Accuracy 0.8769999742507935\n",
      "Iteration 30930 Training loss 0.002002146327868104 Validation loss 0.05140550434589386 Accuracy 0.8769999742507935\n",
      "Iteration 30940 Training loss 0.0012522846227511764 Validation loss 0.05141080915927887 Accuracy 0.8769999742507935\n",
      "Iteration 30950 Training loss 0.0025025000795722008 Validation loss 0.05141976848244667 Accuracy 0.8769999742507935\n",
      "Iteration 30960 Training loss 0.0017522507114335895 Validation loss 0.0514211431145668 Accuracy 0.8769999742507935\n",
      "Iteration 30970 Training loss 0.0012519811280071735 Validation loss 0.05142674967646599 Accuracy 0.8765000104904175\n",
      "Iteration 30980 Training loss 0.0010020204354077578 Validation loss 0.05142951384186745 Accuracy 0.8765000104904175\n",
      "Iteration 30990 Training loss 0.001751916017383337 Validation loss 0.051428377628326416 Accuracy 0.8765000104904175\n",
      "Iteration 31000 Training loss 0.0015020177233964205 Validation loss 0.05142774432897568 Accuracy 0.8765000104904175\n",
      "Iteration 31010 Training loss 0.0017521160189062357 Validation loss 0.05142482370138168 Accuracy 0.8765000104904175\n",
      "Iteration 31020 Training loss 0.0015018198173493147 Validation loss 0.05141910910606384 Accuracy 0.8769999742507935\n",
      "Iteration 31030 Training loss 0.0007521695224568248 Validation loss 0.05142665281891823 Accuracy 0.8769999742507935\n",
      "Iteration 31040 Training loss 0.0010020376648753881 Validation loss 0.05142643675208092 Accuracy 0.8765000104904175\n",
      "Iteration 31050 Training loss 0.0020022052340209484 Validation loss 0.05142619088292122 Accuracy 0.8765000104904175\n",
      "Iteration 31060 Training loss 0.0010019096080213785 Validation loss 0.05142252519726753 Accuracy 0.8765000104904175\n",
      "Iteration 31070 Training loss 0.001002059318125248 Validation loss 0.051414605230093 Accuracy 0.8765000104904175\n",
      "Iteration 31080 Training loss 0.0025022944901138544 Validation loss 0.05141022428870201 Accuracy 0.8765000104904175\n",
      "Iteration 31090 Training loss 0.002252290491014719 Validation loss 0.051406264305114746 Accuracy 0.8769999742507935\n",
      "Iteration 31100 Training loss 0.002501838142052293 Validation loss 0.05141918733716011 Accuracy 0.8769999742507935\n",
      "Iteration 31110 Training loss 0.0025019648019224405 Validation loss 0.051421552896499634 Accuracy 0.8765000104904175\n",
      "Iteration 31120 Training loss 0.0017521069385111332 Validation loss 0.05142393335700035 Accuracy 0.8765000104904175\n",
      "Iteration 31130 Training loss 0.0012521095341071486 Validation loss 0.05142684653401375 Accuracy 0.8765000104904175\n",
      "Iteration 31140 Training loss 0.0007520643412135541 Validation loss 0.051431603729724884 Accuracy 0.8765000104904175\n",
      "Iteration 31150 Training loss 0.002502222079783678 Validation loss 0.05142560228705406 Accuracy 0.8765000104904175\n",
      "Iteration 31160 Training loss 0.0027520915027707815 Validation loss 0.05142330750823021 Accuracy 0.8765000104904175\n",
      "Iteration 31170 Training loss 0.002002294175326824 Validation loss 0.051425740122795105 Accuracy 0.8765000104904175\n",
      "Iteration 31180 Training loss 0.0017519101966172457 Validation loss 0.05142133682966232 Accuracy 0.8765000104904175\n",
      "Iteration 31190 Training loss 0.002002380322664976 Validation loss 0.05142945796251297 Accuracy 0.8765000104904175\n",
      "Iteration 31200 Training loss 0.0027520451694726944 Validation loss 0.05142495036125183 Accuracy 0.8765000104904175\n",
      "Iteration 31210 Training loss 0.0015021631261333823 Validation loss 0.05142054706811905 Accuracy 0.8765000104904175\n",
      "Iteration 31220 Training loss 0.0010020388290286064 Validation loss 0.0514126755297184 Accuracy 0.8765000104904175\n",
      "Iteration 31230 Training loss 0.0012522463221102953 Validation loss 0.05141458660364151 Accuracy 0.8765000104904175\n",
      "Iteration 31240 Training loss 0.0015019592829048634 Validation loss 0.05141756311058998 Accuracy 0.8765000104904175\n",
      "Iteration 31250 Training loss 0.002001934451982379 Validation loss 0.05141318589448929 Accuracy 0.8765000104904175\n",
      "Iteration 31260 Training loss 0.0017517937812954187 Validation loss 0.05142275243997574 Accuracy 0.8765000104904175\n",
      "Iteration 31270 Training loss 0.0017519606044515967 Validation loss 0.05142422020435333 Accuracy 0.8765000104904175\n",
      "Iteration 31280 Training loss 0.001752131269313395 Validation loss 0.051421601325273514 Accuracy 0.8769999742507935\n",
      "Iteration 31290 Training loss 0.0015020491555333138 Validation loss 0.051414914429187775 Accuracy 0.8765000104904175\n",
      "Iteration 31300 Training loss 0.0025019822642207146 Validation loss 0.051418885588645935 Accuracy 0.8765000104904175\n",
      "Iteration 31310 Training loss 0.0017521390691399574 Validation loss 0.0514175221323967 Accuracy 0.8769999742507935\n",
      "Iteration 31320 Training loss 0.002002125373110175 Validation loss 0.051416028290987015 Accuracy 0.8769999742507935\n",
      "Iteration 31330 Training loss 0.002252201782539487 Validation loss 0.05142023414373398 Accuracy 0.8765000104904175\n",
      "Iteration 31340 Training loss 0.0010020971531048417 Validation loss 0.051422636955976486 Accuracy 0.8769999742507935\n",
      "Iteration 31350 Training loss 0.0012521271128207445 Validation loss 0.05142056569457054 Accuracy 0.8765000104904175\n",
      "Iteration 31360 Training loss 0.0012522961478680372 Validation loss 0.05141304060816765 Accuracy 0.8765000104904175\n",
      "Iteration 31370 Training loss 0.0010020622285082936 Validation loss 0.051417768001556396 Accuracy 0.8765000104904175\n",
      "Iteration 31380 Training loss 0.0010021565249189734 Validation loss 0.051420360803604126 Accuracy 0.8765000104904175\n",
      "Iteration 31390 Training loss 0.002752260770648718 Validation loss 0.051417794078588486 Accuracy 0.8765000104904175\n",
      "Iteration 31400 Training loss 0.0015020205173641443 Validation loss 0.05142118036746979 Accuracy 0.8765000104904175\n",
      "Iteration 31410 Training loss 0.002252081874758005 Validation loss 0.05142400413751602 Accuracy 0.8765000104904175\n",
      "Iteration 31420 Training loss 0.0030019369442015886 Validation loss 0.05142984166741371 Accuracy 0.8765000104904175\n",
      "Iteration 31430 Training loss 0.0025021738838404417 Validation loss 0.05143110826611519 Accuracy 0.8765000104904175\n",
      "Iteration 31440 Training loss 0.0025018753949552774 Validation loss 0.05142908915877342 Accuracy 0.8765000104904175\n",
      "Iteration 31450 Training loss 0.001252066227607429 Validation loss 0.05142149329185486 Accuracy 0.8765000104904175\n",
      "Iteration 31460 Training loss 0.002251919824630022 Validation loss 0.051420073956251144 Accuracy 0.8765000104904175\n",
      "Iteration 31470 Training loss 0.0015022719744592905 Validation loss 0.05142580345273018 Accuracy 0.8765000104904175\n",
      "Iteration 31480 Training loss 0.001501835067756474 Validation loss 0.051420241594314575 Accuracy 0.8765000104904175\n",
      "Iteration 31490 Training loss 0.0012520666932687163 Validation loss 0.0514243058860302 Accuracy 0.8765000104904175\n",
      "Iteration 31500 Training loss 0.002752015134319663 Validation loss 0.05142959579825401 Accuracy 0.8765000104904175\n",
      "Iteration 31510 Training loss 0.0020020429510623217 Validation loss 0.05142916738986969 Accuracy 0.8765000104904175\n",
      "Iteration 31520 Training loss 0.0010018767789006233 Validation loss 0.051430169492959976 Accuracy 0.8765000104904175\n",
      "Iteration 31530 Training loss 0.0017519609536975622 Validation loss 0.05142872780561447 Accuracy 0.8765000104904175\n",
      "Iteration 31540 Training loss 0.002251786645501852 Validation loss 0.05143086984753609 Accuracy 0.8765000104904175\n",
      "Iteration 31550 Training loss 0.0007519456557929516 Validation loss 0.05142846703529358 Accuracy 0.8765000104904175\n",
      "Iteration 31560 Training loss 0.0015018198173493147 Validation loss 0.05142882466316223 Accuracy 0.8765000104904175\n",
      "Iteration 31570 Training loss 0.00250202021561563 Validation loss 0.05142902955412865 Accuracy 0.8765000104904175\n",
      "Iteration 31580 Training loss 0.0030022975988686085 Validation loss 0.051428649574518204 Accuracy 0.8765000104904175\n",
      "Iteration 31590 Training loss 0.0030018265824764967 Validation loss 0.05142810568213463 Accuracy 0.8765000104904175\n",
      "Iteration 31600 Training loss 0.0007519835489802063 Validation loss 0.05142838880419731 Accuracy 0.8765000104904175\n",
      "Iteration 31610 Training loss 0.0010019608307629824 Validation loss 0.051429133862257004 Accuracy 0.8765000104904175\n",
      "Iteration 31620 Training loss 0.001752006239257753 Validation loss 0.051436617970466614 Accuracy 0.8765000104904175\n",
      "Iteration 31630 Training loss 0.0010019524488598108 Validation loss 0.0514330118894577 Accuracy 0.8765000104904175\n",
      "Iteration 31640 Training loss 0.0012521670432761312 Validation loss 0.05143687501549721 Accuracy 0.8765000104904175\n",
      "Iteration 31650 Training loss 0.002001944463700056 Validation loss 0.051431186497211456 Accuracy 0.8765000104904175\n",
      "Iteration 31660 Training loss 0.002501722425222397 Validation loss 0.05143864452838898 Accuracy 0.8765000104904175\n",
      "Iteration 31670 Training loss 0.0015020169084891677 Validation loss 0.051444169133901596 Accuracy 0.8765000104904175\n",
      "Iteration 31680 Training loss 0.0020018601790070534 Validation loss 0.051449019461870193 Accuracy 0.8765000104904175\n",
      "Iteration 31690 Training loss 0.0030018629040569067 Validation loss 0.051449619233608246 Accuracy 0.8765000104904175\n",
      "Iteration 31700 Training loss 0.001501973601989448 Validation loss 0.051444265991449356 Accuracy 0.8765000104904175\n",
      "Iteration 31710 Training loss 0.0017519781831651926 Validation loss 0.0514441654086113 Accuracy 0.8765000104904175\n",
      "Iteration 31720 Training loss 0.0017519359244033694 Validation loss 0.05144587531685829 Accuracy 0.8765000104904175\n",
      "Iteration 31730 Training loss 0.0030019651167094707 Validation loss 0.051442552357912064 Accuracy 0.8765000104904175\n",
      "Iteration 31740 Training loss 0.001002314849756658 Validation loss 0.0514494888484478 Accuracy 0.8765000104904175\n",
      "Iteration 31750 Training loss 0.0020017691422253847 Validation loss 0.051444776356220245 Accuracy 0.8765000104904175\n",
      "Iteration 31760 Training loss 0.002002023160457611 Validation loss 0.05144593492150307 Accuracy 0.8765000104904175\n",
      "Iteration 31770 Training loss 0.0027519722934812307 Validation loss 0.05144011601805687 Accuracy 0.8765000104904175\n",
      "Iteration 31780 Training loss 0.0017521248664706945 Validation loss 0.051437895745038986 Accuracy 0.8765000104904175\n",
      "Iteration 31790 Training loss 0.002751872641965747 Validation loss 0.051446471363306046 Accuracy 0.8765000104904175\n",
      "Iteration 31800 Training loss 0.0017518987879157066 Validation loss 0.05144592374563217 Accuracy 0.8765000104904175\n",
      "Iteration 31810 Training loss 0.002502107759937644 Validation loss 0.05144387111067772 Accuracy 0.8765000104904175\n",
      "Iteration 31820 Training loss 0.0017518121749162674 Validation loss 0.05144604668021202 Accuracy 0.8765000104904175\n",
      "Iteration 31830 Training loss 0.0017517921514809132 Validation loss 0.051452070474624634 Accuracy 0.8765000104904175\n",
      "Iteration 31840 Training loss 0.0012518331641331315 Validation loss 0.0514533706009388 Accuracy 0.8765000104904175\n",
      "Iteration 31850 Training loss 0.00175178493373096 Validation loss 0.051452212035655975 Accuracy 0.8765000104904175\n",
      "Iteration 31860 Training loss 0.0012519238516688347 Validation loss 0.05144573748111725 Accuracy 0.8765000104904175\n",
      "Iteration 31870 Training loss 0.001751800999045372 Validation loss 0.05144529044628143 Accuracy 0.8765000104904175\n",
      "Iteration 31880 Training loss 0.0015018833801150322 Validation loss 0.051449257880449295 Accuracy 0.8765000104904175\n",
      "Iteration 31890 Training loss 0.0010020193876698613 Validation loss 0.05144653469324112 Accuracy 0.8765000104904175\n",
      "Iteration 31900 Training loss 0.001501937280409038 Validation loss 0.05144219100475311 Accuracy 0.8765000104904175\n",
      "Iteration 31910 Training loss 0.002252172213047743 Validation loss 0.05144049972295761 Accuracy 0.8765000104904175\n",
      "Iteration 31920 Training loss 0.0007519478094764054 Validation loss 0.05144323781132698 Accuracy 0.8765000104904175\n",
      "Iteration 31930 Training loss 0.0015021590515971184 Validation loss 0.05144127830862999 Accuracy 0.8765000104904175\n",
      "Iteration 31940 Training loss 0.002251973608508706 Validation loss 0.051448896527290344 Accuracy 0.8765000104904175\n",
      "Iteration 31950 Training loss 0.001501953462138772 Validation loss 0.051447875797748566 Accuracy 0.8765000104904175\n",
      "Iteration 31960 Training loss 0.002001852495595813 Validation loss 0.05143836513161659 Accuracy 0.8765000104904175\n",
      "Iteration 31970 Training loss 0.0017517488449811935 Validation loss 0.051428209990262985 Accuracy 0.8765000104904175\n",
      "Iteration 31980 Training loss 0.0022522129584103823 Validation loss 0.05143100023269653 Accuracy 0.8765000104904175\n",
      "Iteration 31990 Training loss 0.001501803519204259 Validation loss 0.05142558366060257 Accuracy 0.8765000104904175\n",
      "Iteration 32000 Training loss 0.0010019199689850211 Validation loss 0.05142555385828018 Accuracy 0.8765000104904175\n",
      "Iteration 32010 Training loss 0.0025021196343004704 Validation loss 0.051426392048597336 Accuracy 0.8765000104904175\n",
      "Iteration 32020 Training loss 0.00125197134912014 Validation loss 0.05142325162887573 Accuracy 0.8765000104904175\n",
      "Iteration 32030 Training loss 0.0020020881202071905 Validation loss 0.051427267491817474 Accuracy 0.8765000104904175\n",
      "Iteration 32040 Training loss 0.0007520356448367238 Validation loss 0.05143262818455696 Accuracy 0.8765000104904175\n",
      "Iteration 32050 Training loss 0.0007518963539041579 Validation loss 0.05144194886088371 Accuracy 0.8765000104904175\n",
      "Iteration 32060 Training loss 0.0012519066222012043 Validation loss 0.05144130811095238 Accuracy 0.8765000104904175\n",
      "Iteration 32070 Training loss 0.0005018016672693193 Validation loss 0.0514395609498024 Accuracy 0.8765000104904175\n",
      "Iteration 32080 Training loss 0.0005018880474381149 Validation loss 0.05143371596932411 Accuracy 0.8765000104904175\n",
      "Iteration 32090 Training loss 0.0010018838802352548 Validation loss 0.051435891538858414 Accuracy 0.8765000104904175\n",
      "Iteration 32100 Training loss 0.00225210003554821 Validation loss 0.05143531784415245 Accuracy 0.8765000104904175\n",
      "Iteration 32110 Training loss 0.000752102117985487 Validation loss 0.05143824219703674 Accuracy 0.8765000104904175\n",
      "Iteration 32120 Training loss 0.0017519831890240312 Validation loss 0.051434922963380814 Accuracy 0.8765000104904175\n",
      "Iteration 32130 Training loss 0.0020021519158035517 Validation loss 0.0514284148812294 Accuracy 0.8765000104904175\n",
      "Iteration 32140 Training loss 0.002001762855798006 Validation loss 0.05142589658498764 Accuracy 0.8765000104904175\n",
      "Iteration 32150 Training loss 0.0017518410459160805 Validation loss 0.05142391100525856 Accuracy 0.8765000104904175\n",
      "Iteration 32160 Training loss 0.001751827192492783 Validation loss 0.05142800509929657 Accuracy 0.8765000104904175\n",
      "Iteration 32170 Training loss 0.002001814777031541 Validation loss 0.05142315477132797 Accuracy 0.8765000104904175\n",
      "Iteration 32180 Training loss 0.001251621637493372 Validation loss 0.051427438855171204 Accuracy 0.8765000104904175\n",
      "Iteration 32190 Training loss 0.0017518060049042106 Validation loss 0.051428504288196564 Accuracy 0.8765000104904175\n",
      "Iteration 32200 Training loss 0.0017516892403364182 Validation loss 0.0514303483068943 Accuracy 0.8765000104904175\n",
      "Iteration 32210 Training loss 0.002251784084364772 Validation loss 0.05143401399254799 Accuracy 0.8765000104904175\n",
      "Iteration 32220 Training loss 0.0017518197419121861 Validation loss 0.05143530294299126 Accuracy 0.8765000104904175\n",
      "Iteration 32230 Training loss 0.0012520812451839447 Validation loss 0.051431722939014435 Accuracy 0.8765000104904175\n",
      "Iteration 32240 Training loss 0.001501760445535183 Validation loss 0.051424235105514526 Accuracy 0.8765000104904175\n",
      "Iteration 32250 Training loss 0.002501613227650523 Validation loss 0.051425088196992874 Accuracy 0.8765000104904175\n",
      "Iteration 32260 Training loss 0.0027518190909177065 Validation loss 0.051427509635686874 Accuracy 0.8765000104904175\n",
      "Iteration 32270 Training loss 0.0017518209060654044 Validation loss 0.05142834410071373 Accuracy 0.8765000104904175\n",
      "Iteration 32280 Training loss 0.0015017864061519504 Validation loss 0.051431793719530106 Accuracy 0.8765000104904175\n",
      "Iteration 32290 Training loss 0.002251633210107684 Validation loss 0.05143503472208977 Accuracy 0.8765000104904175\n",
      "Iteration 32300 Training loss 0.0015020410064607859 Validation loss 0.051432736217975616 Accuracy 0.8765000104904175\n",
      "Iteration 32310 Training loss 0.0025018295273184776 Validation loss 0.05143043398857117 Accuracy 0.8765000104904175\n",
      "Iteration 32320 Training loss 0.0017518854001536965 Validation loss 0.05143778398633003 Accuracy 0.8765000104904175\n",
      "Iteration 32330 Training loss 0.0020017395727336407 Validation loss 0.05143411457538605 Accuracy 0.8765000104904175\n",
      "Iteration 32340 Training loss 0.001751838019117713 Validation loss 0.0514373853802681 Accuracy 0.8765000104904175\n",
      "Iteration 32350 Training loss 0.0012519748415797949 Validation loss 0.05143889784812927 Accuracy 0.8765000104904175\n",
      "Iteration 32360 Training loss 0.0007522029336541891 Validation loss 0.051442280411720276 Accuracy 0.8765000104904175\n",
      "Iteration 32370 Training loss 0.0007519578211940825 Validation loss 0.051438625901937485 Accuracy 0.8765000104904175\n",
      "Iteration 32380 Training loss 0.0012517214054241776 Validation loss 0.0514385960996151 Accuracy 0.8765000104904175\n",
      "Iteration 32390 Training loss 0.0017517446540296078 Validation loss 0.05144616216421127 Accuracy 0.8765000104904175\n",
      "Iteration 32400 Training loss 0.002001949120312929 Validation loss 0.0514427125453949 Accuracy 0.8765000104904175\n",
      "Iteration 32410 Training loss 0.0020018441136926413 Validation loss 0.051443953067064285 Accuracy 0.8765000104904175\n",
      "Iteration 32420 Training loss 0.0010018092580139637 Validation loss 0.051451824605464935 Accuracy 0.8765000104904175\n",
      "Iteration 32430 Training loss 0.0007520440267398953 Validation loss 0.051447466015815735 Accuracy 0.8765000104904175\n",
      "Iteration 32440 Training loss 0.0010019383626058698 Validation loss 0.05144966393709183 Accuracy 0.8765000104904175\n",
      "Iteration 32450 Training loss 0.0022520720958709717 Validation loss 0.05145064368844032 Accuracy 0.8765000104904175\n",
      "Iteration 32460 Training loss 0.0010019229957833886 Validation loss 0.051447514444589615 Accuracy 0.8765000104904175\n",
      "Iteration 32470 Training loss 0.003502021776512265 Validation loss 0.05144839361310005 Accuracy 0.8765000104904175\n",
      "Iteration 32480 Training loss 0.001502068480476737 Validation loss 0.05144490674138069 Accuracy 0.8765000104904175\n",
      "Iteration 32490 Training loss 0.0017517762025818229 Validation loss 0.05143791809678078 Accuracy 0.8765000104904175\n",
      "Iteration 32500 Training loss 0.0020015286281704903 Validation loss 0.05144021660089493 Accuracy 0.8765000104904175\n",
      "Iteration 32510 Training loss 0.0012523253681138158 Validation loss 0.05144058167934418 Accuracy 0.8765000104904175\n",
      "Iteration 32520 Training loss 0.002002095803618431 Validation loss 0.05144467204809189 Accuracy 0.8765000104904175\n",
      "Iteration 32530 Training loss 0.0010016096057370305 Validation loss 0.05144623667001724 Accuracy 0.8765000104904175\n",
      "Iteration 32540 Training loss 0.0025016660802066326 Validation loss 0.05144704133272171 Accuracy 0.8765000104904175\n",
      "Iteration 32550 Training loss 0.0020017381757497787 Validation loss 0.05144312605261803 Accuracy 0.8765000104904175\n",
      "Iteration 32560 Training loss 0.0007521377992816269 Validation loss 0.05143974348902702 Accuracy 0.8765000104904175\n",
      "Iteration 32570 Training loss 0.0010017093736678362 Validation loss 0.051447462290525436 Accuracy 0.8765000104904175\n",
      "Iteration 32580 Training loss 0.0012518333969637752 Validation loss 0.051448166370391846 Accuracy 0.8765000104904175\n",
      "Iteration 32590 Training loss 0.001001914031803608 Validation loss 0.05145030841231346 Accuracy 0.8765000104904175\n",
      "Iteration 32600 Training loss 0.0010017153108492494 Validation loss 0.051454514265060425 Accuracy 0.8765000104904175\n",
      "Iteration 32610 Training loss 0.001501791295595467 Validation loss 0.05145806074142456 Accuracy 0.8765000104904175\n",
      "Iteration 32620 Training loss 0.002251744968816638 Validation loss 0.05146249383687973 Accuracy 0.8765000104904175\n",
      "Iteration 32630 Training loss 0.0015017719706520438 Validation loss 0.051458071917295456 Accuracy 0.8765000104904175\n",
      "Iteration 32640 Training loss 0.002251859987154603 Validation loss 0.051460206508636475 Accuracy 0.8765000104904175\n",
      "Iteration 32650 Training loss 0.0012518716976046562 Validation loss 0.051462434232234955 Accuracy 0.8765000104904175\n",
      "Iteration 32660 Training loss 0.0017516941297799349 Validation loss 0.051457732915878296 Accuracy 0.8765000104904175\n",
      "Iteration 32670 Training loss 0.0012516684364527464 Validation loss 0.05145308002829552 Accuracy 0.8765000104904175\n",
      "Iteration 32680 Training loss 0.0012517293216660619 Validation loss 0.05145987123250961 Accuracy 0.8765000104904175\n",
      "Iteration 32690 Training loss 0.002501832088455558 Validation loss 0.05145682021975517 Accuracy 0.8765000104904175\n",
      "Iteration 32700 Training loss 0.0020018115174025297 Validation loss 0.05145316198468208 Accuracy 0.8765000104904175\n",
      "Iteration 32710 Training loss 0.0020016080234199762 Validation loss 0.0514507032930851 Accuracy 0.8765000104904175\n",
      "Iteration 32720 Training loss 0.0022514795418828726 Validation loss 0.05145594850182533 Accuracy 0.8765000104904175\n",
      "Iteration 32730 Training loss 0.0017517268424853683 Validation loss 0.05146116018295288 Accuracy 0.8765000104904175\n",
      "Iteration 32740 Training loss 0.0012516436399891973 Validation loss 0.05146219581365585 Accuracy 0.8765000104904175\n",
      "Iteration 32750 Training loss 0.002251968951895833 Validation loss 0.05146653205156326 Accuracy 0.8765000104904175\n",
      "Iteration 32760 Training loss 0.001001947559416294 Validation loss 0.0514637753367424 Accuracy 0.8765000104904175\n",
      "Iteration 32770 Training loss 0.0010017193853855133 Validation loss 0.05146258324384689 Accuracy 0.8765000104904175\n",
      "Iteration 32780 Training loss 0.0017516467487439513 Validation loss 0.05145484209060669 Accuracy 0.8765000104904175\n",
      "Iteration 32790 Training loss 0.0010018073953688145 Validation loss 0.051450639963150024 Accuracy 0.8765000104904175\n",
      "Iteration 32800 Training loss 0.001751708798110485 Validation loss 0.051453232765197754 Accuracy 0.8765000104904175\n",
      "Iteration 32810 Training loss 0.0015018057310953736 Validation loss 0.0514536090195179 Accuracy 0.8765000104904175\n",
      "Iteration 32820 Training loss 0.0027518614660948515 Validation loss 0.051447317004203796 Accuracy 0.8765000104904175\n",
      "Iteration 32830 Training loss 0.0012516427086666226 Validation loss 0.05145154893398285 Accuracy 0.8765000104904175\n",
      "Iteration 32840 Training loss 0.0025018234737217426 Validation loss 0.05144646391272545 Accuracy 0.8765000104904175\n",
      "Iteration 32850 Training loss 0.0017518555978313088 Validation loss 0.05144951492547989 Accuracy 0.8765000104904175\n",
      "Iteration 32860 Training loss 0.0017517866799607873 Validation loss 0.05145028978586197 Accuracy 0.8765000104904175\n",
      "Iteration 32870 Training loss 0.0015015749959275126 Validation loss 0.05145667493343353 Accuracy 0.8765000104904175\n",
      "Iteration 32880 Training loss 0.0015016080578789115 Validation loss 0.051449455320835114 Accuracy 0.8765000104904175\n",
      "Iteration 32890 Training loss 0.002251844387501478 Validation loss 0.05144893750548363 Accuracy 0.8765000104904175\n",
      "Iteration 32900 Training loss 0.0010020541958510876 Validation loss 0.05144978314638138 Accuracy 0.8765000104904175\n",
      "Iteration 32910 Training loss 0.0015016957186162472 Validation loss 0.05145369470119476 Accuracy 0.8765000104904175\n",
      "Iteration 32920 Training loss 0.0025018220767378807 Validation loss 0.05145472660660744 Accuracy 0.8765000104904175\n",
      "Iteration 32930 Training loss 0.0017516639782115817 Validation loss 0.05145378038287163 Accuracy 0.8765000104904175\n",
      "Iteration 32940 Training loss 0.0020019225776195526 Validation loss 0.051454585045576096 Accuracy 0.8765000104904175\n",
      "Iteration 32950 Training loss 0.002501861425116658 Validation loss 0.05145701766014099 Accuracy 0.8765000104904175\n",
      "Iteration 32960 Training loss 0.002001914894208312 Validation loss 0.05145559459924698 Accuracy 0.8765000104904175\n",
      "Iteration 32970 Training loss 0.0010018574539572 Validation loss 0.05145680159330368 Accuracy 0.8765000104904175\n",
      "Iteration 32980 Training loss 0.0017517914529889822 Validation loss 0.051453977823257446 Accuracy 0.8765000104904175\n",
      "Iteration 32990 Training loss 0.001251889392733574 Validation loss 0.05145592242479324 Accuracy 0.8765000104904175\n",
      "Iteration 33000 Training loss 0.002001896034926176 Validation loss 0.05145413056015968 Accuracy 0.8765000104904175\n",
      "Iteration 33010 Training loss 0.001752093667164445 Validation loss 0.051454171538352966 Accuracy 0.8765000104904175\n",
      "Iteration 33020 Training loss 0.0015018892008811235 Validation loss 0.051453862339258194 Accuracy 0.8765000104904175\n",
      "Iteration 33030 Training loss 0.0012519003357738256 Validation loss 0.051456063985824585 Accuracy 0.8765000104904175\n",
      "Iteration 33040 Training loss 0.001251672045327723 Validation loss 0.051458269357681274 Accuracy 0.8765000104904175\n",
      "Iteration 33050 Training loss 0.0017517202068120241 Validation loss 0.05145970731973648 Accuracy 0.8765000104904175\n",
      "Iteration 33060 Training loss 0.0017520833062008023 Validation loss 0.051461223512887955 Accuracy 0.8765000104904175\n",
      "Iteration 33070 Training loss 0.001751881092786789 Validation loss 0.05145690590143204 Accuracy 0.8765000104904175\n",
      "Iteration 33080 Training loss 0.0012517118593677878 Validation loss 0.05145666375756264 Accuracy 0.8765000104904175\n",
      "Iteration 33090 Training loss 0.0020017700735479593 Validation loss 0.05145261064171791 Accuracy 0.8765000104904175\n",
      "Iteration 33100 Training loss 0.0017516770167276263 Validation loss 0.051456108689308167 Accuracy 0.8765000104904175\n",
      "Iteration 33110 Training loss 0.00250176596455276 Validation loss 0.051453571766614914 Accuracy 0.8765000104904175\n",
      "Iteration 33120 Training loss 0.0027517846319824457 Validation loss 0.05144967511296272 Accuracy 0.8765000104904175\n",
      "Iteration 33130 Training loss 0.0017516844673082232 Validation loss 0.05144703760743141 Accuracy 0.8765000104904175\n",
      "Iteration 33140 Training loss 0.0017517008818686008 Validation loss 0.05144743621349335 Accuracy 0.8765000104904175\n",
      "Iteration 33150 Training loss 0.0012520679738372564 Validation loss 0.05144810304045677 Accuracy 0.8765000104904175\n",
      "Iteration 33160 Training loss 0.0012517180293798447 Validation loss 0.05144219845533371 Accuracy 0.8765000104904175\n",
      "Iteration 33170 Training loss 0.0025018758606165648 Validation loss 0.051446426659822464 Accuracy 0.8765000104904175\n",
      "Iteration 33180 Training loss 0.0020015882328152657 Validation loss 0.0514444038271904 Accuracy 0.8765000104904175\n",
      "Iteration 33190 Training loss 0.0005016010254621506 Validation loss 0.051445893943309784 Accuracy 0.8765000104904175\n",
      "Iteration 33200 Training loss 0.002001645974814892 Validation loss 0.05145087465643883 Accuracy 0.8765000104904175\n",
      "Iteration 33210 Training loss 0.0020019549410790205 Validation loss 0.051451653242111206 Accuracy 0.8765000104904175\n",
      "Iteration 33220 Training loss 0.0010017981985583901 Validation loss 0.0514485239982605 Accuracy 0.8765000104904175\n",
      "Iteration 33230 Training loss 0.0015019695274531841 Validation loss 0.051455192267894745 Accuracy 0.8765000104904175\n",
      "Iteration 33240 Training loss 0.0020018531940877438 Validation loss 0.051452286541461945 Accuracy 0.8765000104904175\n",
      "Iteration 33250 Training loss 0.0022518085315823555 Validation loss 0.05145537480711937 Accuracy 0.8765000104904175\n",
      "Iteration 33260 Training loss 0.002501833951100707 Validation loss 0.051456697285175323 Accuracy 0.8765000104904175\n",
      "Iteration 33270 Training loss 0.0027518081478774548 Validation loss 0.051460620015859604 Accuracy 0.8765000104904175\n",
      "Iteration 33280 Training loss 0.0012519668089225888 Validation loss 0.051458217203617096 Accuracy 0.8765000104904175\n",
      "Iteration 33290 Training loss 0.0017520467517897487 Validation loss 0.05145573243498802 Accuracy 0.8765000104904175\n",
      "Iteration 33300 Training loss 0.0012515154667198658 Validation loss 0.05145636945962906 Accuracy 0.8765000104904175\n",
      "Iteration 33310 Training loss 0.0032516971696168184 Validation loss 0.05145098268985748 Accuracy 0.8765000104904175\n",
      "Iteration 33320 Training loss 0.0012517900904640555 Validation loss 0.051451269537210464 Accuracy 0.8765000104904175\n",
      "Iteration 33330 Training loss 0.0017517105443403125 Validation loss 0.05145266652107239 Accuracy 0.8765000104904175\n",
      "Iteration 33340 Training loss 0.0022517142351716757 Validation loss 0.05145173892378807 Accuracy 0.8765000104904175\n",
      "Iteration 33350 Training loss 0.0015015725512057543 Validation loss 0.05145242437720299 Accuracy 0.8765000104904175\n",
      "Iteration 33360 Training loss 0.0020017451606690884 Validation loss 0.05145673826336861 Accuracy 0.8765000104904175\n",
      "Iteration 33370 Training loss 0.0012517869472503662 Validation loss 0.051453571766614914 Accuracy 0.8765000104904175\n",
      "Iteration 33380 Training loss 0.0007516691111959517 Validation loss 0.05146041139960289 Accuracy 0.8765000104904175\n",
      "Iteration 33390 Training loss 0.0022520599886775017 Validation loss 0.05145936459302902 Accuracy 0.8765000104904175\n",
      "Iteration 33400 Training loss 0.0020016266498714685 Validation loss 0.05145591124892235 Accuracy 0.8765000104904175\n",
      "Iteration 33410 Training loss 0.001001921365968883 Validation loss 0.05145861953496933 Accuracy 0.8765000104904175\n",
      "Iteration 33420 Training loss 0.0010018027387559414 Validation loss 0.051461111754179 Accuracy 0.8765000104904175\n",
      "Iteration 33430 Training loss 0.0012520396849140525 Validation loss 0.05145680159330368 Accuracy 0.8765000104904175\n",
      "Iteration 33440 Training loss 0.0020016375929117203 Validation loss 0.05145474895834923 Accuracy 0.8765000104904175\n",
      "Iteration 33450 Training loss 0.0017518778331577778 Validation loss 0.05146210640668869 Accuracy 0.8765000104904175\n",
      "Iteration 33460 Training loss 0.0010016768937930465 Validation loss 0.05146242305636406 Accuracy 0.8765000104904175\n",
      "Iteration 33470 Training loss 0.0022517931647598743 Validation loss 0.051461007446050644 Accuracy 0.8765000104904175\n",
      "Iteration 33480 Training loss 0.0032517535146325827 Validation loss 0.051460832357406616 Accuracy 0.8765000104904175\n",
      "Iteration 33490 Training loss 0.0017517153173685074 Validation loss 0.051457442343235016 Accuracy 0.8765000104904175\n",
      "Iteration 33500 Training loss 0.002001868560910225 Validation loss 0.051459696143865585 Accuracy 0.8765000104904175\n",
      "Iteration 33510 Training loss 0.0015018605627119541 Validation loss 0.05145704373717308 Accuracy 0.8765000104904175\n",
      "Iteration 33520 Training loss 0.003001475939527154 Validation loss 0.05145612359046936 Accuracy 0.8765000104904175\n",
      "Iteration 33530 Training loss 0.0017517342930659652 Validation loss 0.05145389586687088 Accuracy 0.8765000104904175\n",
      "Iteration 33540 Training loss 0.001251746783964336 Validation loss 0.05145487189292908 Accuracy 0.8765000104904175\n",
      "Iteration 33550 Training loss 0.0015016597462818027 Validation loss 0.051453132182359695 Accuracy 0.8765000104904175\n",
      "Iteration 33560 Training loss 0.002501638140529394 Validation loss 0.051454443484544754 Accuracy 0.8765000104904175\n",
      "Iteration 33570 Training loss 0.0012517257127910852 Validation loss 0.05145469680428505 Accuracy 0.8765000104904175\n",
      "Iteration 33580 Training loss 0.0015017715049907565 Validation loss 0.051456648856401443 Accuracy 0.8765000104904175\n",
      "Iteration 33590 Training loss 0.0012515143025666475 Validation loss 0.051452264189720154 Accuracy 0.8765000104904175\n",
      "Iteration 33600 Training loss 0.0017518337117508054 Validation loss 0.05145128816366196 Accuracy 0.8765000104904175\n",
      "Iteration 33610 Training loss 0.0017516902880743146 Validation loss 0.05145381763577461 Accuracy 0.8765000104904175\n",
      "Iteration 33620 Training loss 0.00175171357113868 Validation loss 0.05145633593201637 Accuracy 0.8765000104904175\n",
      "Iteration 33630 Training loss 0.0012517684372141957 Validation loss 0.051456790417432785 Accuracy 0.8765000104904175\n",
      "Iteration 33640 Training loss 0.002001620829105377 Validation loss 0.051452476531267166 Accuracy 0.8765000104904175\n",
      "Iteration 33650 Training loss 0.0007517967605963349 Validation loss 0.051456719636917114 Accuracy 0.8765000104904175\n",
      "Iteration 33660 Training loss 0.0020015635527670383 Validation loss 0.05145694687962532 Accuracy 0.8765000104904175\n",
      "Iteration 33670 Training loss 0.0005017546354793012 Validation loss 0.05145643651485443 Accuracy 0.8765000104904175\n",
      "Iteration 33680 Training loss 0.002001633169129491 Validation loss 0.05145755410194397 Accuracy 0.8765000104904175\n",
      "Iteration 33690 Training loss 0.0010017439490184188 Validation loss 0.05145462602376938 Accuracy 0.8765000104904175\n",
      "Iteration 33700 Training loss 0.0017519204411655664 Validation loss 0.05145256593823433 Accuracy 0.8769999742507935\n",
      "Iteration 33710 Training loss 0.00125177763402462 Validation loss 0.05145901069045067 Accuracy 0.8765000104904175\n",
      "Iteration 33720 Training loss 0.002501782961189747 Validation loss 0.051457200199365616 Accuracy 0.8765000104904175\n",
      "Iteration 33730 Training loss 0.003751688404008746 Validation loss 0.0514569953083992 Accuracy 0.8765000104904175\n",
      "Iteration 33740 Training loss 0.0015015315730124712 Validation loss 0.051456518471241 Accuracy 0.8765000104904175\n",
      "Iteration 33750 Training loss 0.0020015977788716555 Validation loss 0.051455557346343994 Accuracy 0.8765000104904175\n",
      "Iteration 33760 Training loss 0.0017515512881800532 Validation loss 0.05145817622542381 Accuracy 0.8765000104904175\n",
      "Iteration 33770 Training loss 0.002501706127077341 Validation loss 0.05145946517586708 Accuracy 0.8765000104904175\n",
      "Iteration 33780 Training loss 0.0025017866864800453 Validation loss 0.05146026611328125 Accuracy 0.8765000104904175\n",
      "Iteration 33790 Training loss 0.0020020012743771076 Validation loss 0.05145909637212753 Accuracy 0.8765000104904175\n",
      "Iteration 33800 Training loss 0.0007516235928051174 Validation loss 0.051463477313518524 Accuracy 0.8765000104904175\n",
      "Iteration 33810 Training loss 0.0012515779817476869 Validation loss 0.0514615997672081 Accuracy 0.8765000104904175\n",
      "Iteration 33820 Training loss 0.0022515978198498487 Validation loss 0.05146143585443497 Accuracy 0.8765000104904175\n",
      "Iteration 33830 Training loss 0.0015018387930467725 Validation loss 0.05146341770887375 Accuracy 0.8765000104904175\n",
      "Iteration 33840 Training loss 0.001001817756332457 Validation loss 0.05145857110619545 Accuracy 0.8765000104904175\n",
      "Iteration 33850 Training loss 0.003501742146909237 Validation loss 0.051461342722177505 Accuracy 0.8765000104904175\n",
      "Iteration 33860 Training loss 0.0017516264924779534 Validation loss 0.05146168917417526 Accuracy 0.8765000104904175\n",
      "Iteration 33870 Training loss 0.002251950791105628 Validation loss 0.05146301910281181 Accuracy 0.8765000104904175\n",
      "Iteration 33880 Training loss 0.0025014658458530903 Validation loss 0.051460035145282745 Accuracy 0.8765000104904175\n",
      "Iteration 33890 Training loss 0.0027515729889273643 Validation loss 0.05146237462759018 Accuracy 0.8765000104904175\n",
      "Iteration 33900 Training loss 0.0017517354572191834 Validation loss 0.05146380513906479 Accuracy 0.8765000104904175\n",
      "Iteration 33910 Training loss 0.001751726376824081 Validation loss 0.05146456137299538 Accuracy 0.8765000104904175\n",
      "Iteration 33920 Training loss 0.0022515617311000824 Validation loss 0.05146746709942818 Accuracy 0.8765000104904175\n",
      "Iteration 33930 Training loss 0.0020019158255308867 Validation loss 0.05146390572190285 Accuracy 0.8765000104904175\n",
      "Iteration 33940 Training loss 0.0012517966097220778 Validation loss 0.05146099999547005 Accuracy 0.8765000104904175\n",
      "Iteration 33950 Training loss 0.0010018048342317343 Validation loss 0.05146027356386185 Accuracy 0.8765000104904175\n",
      "Iteration 33960 Training loss 0.002251638798043132 Validation loss 0.05146436765789986 Accuracy 0.8765000104904175\n",
      "Iteration 33970 Training loss 0.0015017036348581314 Validation loss 0.0514594204723835 Accuracy 0.8769999742507935\n",
      "Iteration 33980 Training loss 0.0017516559455543756 Validation loss 0.05146120488643646 Accuracy 0.8769999742507935\n",
      "Iteration 33990 Training loss 0.0017515748040750623 Validation loss 0.05145867168903351 Accuracy 0.8769999742507935\n",
      "Iteration 34000 Training loss 0.002001590793952346 Validation loss 0.05146403610706329 Accuracy 0.8769999742507935\n",
      "Iteration 34010 Training loss 0.0025016774889081717 Validation loss 0.05146358907222748 Accuracy 0.8769999742507935\n",
      "Iteration 34020 Training loss 0.0017516997177153826 Validation loss 0.05146302282810211 Accuracy 0.8769999742507935\n",
      "Iteration 34030 Training loss 0.002001633169129491 Validation loss 0.051463812589645386 Accuracy 0.8765000104904175\n",
      "Iteration 34040 Training loss 0.002001471584662795 Validation loss 0.051463332027196884 Accuracy 0.8765000104904175\n",
      "Iteration 34050 Training loss 0.002251620637252927 Validation loss 0.051464587450027466 Accuracy 0.8765000104904175\n",
      "Iteration 34060 Training loss 0.0032518941443413496 Validation loss 0.05147058889269829 Accuracy 0.8765000104904175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_5_layer.train_layers(x_train, y_train, x_valid, y_valid, 2.37, 1e-2, 5e8, 0, 0, 0, 0, 0, 1, 0.2, 10, True, True, True, True, True, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(3072,2048,10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-5, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 2048, 2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = True\n",
    "date = \"13_05_25\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = binary_model_5_layer\n",
    "    model_name = date + '_CIFAR10_model_(1024+512+512+512+512+1)_1_BIS' #\n",
    "    save_path = \"Classifiers/\" + date + '/' + model_name + '/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0, np.max([np.nan_to_num(model.training_loss_trajectory,nan=0), np.nan_to_num(model.validation_loss_trajectory, nan=0)])+0.01)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_of_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    data = np.column_stack((np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/validation_loss_of_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0, np.max([np.nan_to_num(model.training_loss_trajectory,nan=0), np.nan_to_num(model.validation_loss_trajectory, nan=0)])+0.01)\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c380b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Post-processing/13_05_25/Overfitting_dynamic/Dataset_1_ct/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f4269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8305c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_envelope(x,y):\n",
    "    minima_idx = argrelextrema(y, np.less, order=5)[0]\n",
    "    x_sampled = x[minima_idx]\n",
    "    y = y[minima_idx]\n",
    "    interp = interp1d(x_sampled, y, kind='linear', fill_value='extrapolate')\n",
    "    y = interp(x)\n",
    "    return np.array([x, y]).transpose()\n",
    "\n",
    "def upper_envelope(x,y):\n",
    "    maxima_idx = argrelextrema(y, np.greater, order=5)[0]\n",
    "    x_sampled = x[maxima_idx]\n",
    "    y = y[maxima_idx]\n",
    "    interp = interp1d(x_sampled, y, kind='linear', fill_value='extrapolate')\n",
    "    y = interp(x)\n",
    "    return np.array([x, y]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01e1426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Import model and plot performances\n",
    "\n",
    "date = '13_05_25'\n",
    "save_path = 'Post-processing/13_05_25/Overfitting_dynamic/Dataset_1_ct/'\n",
    "\n",
    "# Plot the performances\n",
    "model_name_1 = \"CIFAR10_model_(1024+512+1)_1_BIS\"\n",
    "model_name_2 = \"CIFAR10_model_(1024+512+512+1)_1_BIS\"\n",
    "model_name_3 = \"CIFAR10_model_(1024+512+512+512+1)_1_BIS\"\n",
    "model_name_4 = \"CIFAR10_model_(1024+512+512+512+512+1)_1_BIS\"\n",
    "\n",
    "curve_TL_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/loss_training_of_' + date + '_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_TL_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/loss_training_of_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_TL_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/loss_training_of_' + date + '_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_TL_model_4 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_4 + '/figures/loss_training_of_' + date + '_' + model_name_4 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "\n",
    "curve_VL_model_1 =  np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/validation_loss_of_' + date + '_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_VL_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/validation_loss_of_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_VL_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/validation_loss_of_' + date + '_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_VL_model_4 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_4 + '/figures/validation_loss_of_' + date + '_' + model_name_4 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "\n",
    "# Extracting the lower envelope\n",
    "curve_VL_model_1 = lower_envelope(curve_VL_model_1[:, 0], curve_VL_model_1[:, 1])\n",
    "curve_VL_model_2 = lower_envelope(curve_VL_model_2[:, 0], curve_VL_model_2[:, 1])\n",
    "curve_VL_model_3 = lower_envelope(curve_VL_model_3[:, 0], curve_VL_model_3[:, 1])\n",
    "curve_VL_model_4 = lower_envelope(curve_VL_model_4[:, 0], curve_VL_model_4[:, 1])\n",
    "\n",
    "curve_TL_model_1 = lower_envelope(curve_TL_model_1[:, 0], curve_TL_model_1[:, 1])\n",
    "curve_TL_model_2 = lower_envelope(curve_TL_model_2[:, 0], curve_TL_model_2[:, 1])\n",
    "curve_TL_model_3 = lower_envelope(curve_TL_model_3[:, 0], curve_TL_model_3[:, 1])\n",
    "curve_TL_model_4 = lower_envelope(curve_TL_model_4[:, 0], curve_TL_model_4[:, 1])\n",
    "\n",
    "plt.plot(curve_VL_model_1[:, 0], curve_VL_model_1[:, 1],'--', color = 'blue', label = 'VL_(1024+512+1)')\n",
    "plt.plot(curve_VL_model_2[:, 0], curve_VL_model_2[:, 1],'--', color = 'green', label = 'VL_(1024+512+512+1)')\n",
    "plt.plot(curve_VL_model_3[:, 0], curve_VL_model_3[:, 1],'--', color = 'orange', label = 'VL_(1024+512+512+512+1)')\n",
    "plt.plot(curve_VL_model_4[:, 0], curve_VL_model_4[:, 1],'--', color = 'red', label = 'VL_(1024+512+512+512+512+1)')\n",
    "\n",
    "plt.plot(curve_TL_model_1[:, 0], curve_TL_model_1[:, 1], '.', markersize = '2', color = 'blue', label = 'TL_(1024+512+1)')\n",
    "plt.plot(curve_TL_model_2[:, 0], curve_TL_model_2[:, 1], '.', markersize = '2', color = 'green', label = 'TL_(1024+512+512+1)')\n",
    "plt.plot(curve_TL_model_3[:, 0], curve_TL_model_3[:, 1], '.', markersize = '2', color = 'orange', label = 'TL_(1024+512+512+512+1)')\n",
    "plt.plot(curve_TL_model_4[:, 0], curve_TL_model_4[:, 1], '.', markersize = '2', color = 'red', label = 'TL_(1024+512+512+512+512+1)')\n",
    "\n",
    "plt.xlim(-200,20000)\n",
    "plt.ylim(0,np.max([np.max(curve_TL_model_4[:, 1]), np.max(curve_TL_model_3[:, 1]), np.max(curve_TL_model_2[:, 1]), np.max(curve_TL_model_1[:, 1])]) + 0.01)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration N')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training dynamic on dataset 1_ct for the different architectures', pad = 20)\n",
    "legend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "for handle in legend.legend_handles:\n",
    "    handle.set_markersize(15)   \n",
    "plt.savefig(save_path + \"Comparison_overfitting_dynamic_dataset_1_ct.png\", bbox_inches='tight')\n",
    "plt.savefig(save_path + \"Comparison_overfitting_dynammic_dataset_1_ct.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_3.png\", bbox_inches='tight')\n",
    "# plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_3.svg\", bbox_inches='tight')\n",
    "# plt.show()\n",
    "# Plots of performances\n",
    "# accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(accuracy)\n",
    "# kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_accuracy)\n",
    "# loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(loss)\n",
    "# kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_loss)\n",
    "# plt.show()\n",
    "\n",
    "# Import datas\n",
    "# accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f093d17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "date = '13_05_25'\n",
    "save_path = 'Post-processing/13_05_25/Accuracies_of_models/Dataset_1_ct/'\n",
    "\n",
    "# Plot the performances\n",
    "model_name_1 = \"CIFAR10_model_(1024+512+1)_1_BIS\"\n",
    "model_name_2 = \"CIFAR10_model_(1024+512+512+1)_1_BIS\"\n",
    "model_name_3 = \"CIFAR10_model_(1024+512+512+512+1)_1_BIS\"\n",
    "model_name_4 = \"CIFAR10_model_(1024+512+512+512+512+1)_1_BIS\"\n",
    "\n",
    "curve_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/accuracy_of_' + date + '_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/accuracy_of_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/accuracy_of_' + date + '_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_model_4 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_4 + '/figures/accuracy_of_' + date + '_' + model_name_4 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "curve_model_1 = upper_envelope(curve_model_1[:, 0], curve_model_1[:, 1])\n",
    "curve_model_2 = upper_envelope(curve_model_2[:, 0], curve_model_2[:, 1])\n",
    "curve_model_3 = upper_envelope(curve_model_3[:, 0], curve_model_3[:, 1])\n",
    "curve_model_4 = upper_envelope(curve_model_4[:, 0], curve_model_4[:, 1])\n",
    "\n",
    "plt.plot(curve_model_4[:, 0], curve_model_4[:, 1], '.', markersize = '2', color = 'red', label = '(1024+512+512+512+512+1)')\n",
    "plt.plot(curve_model_3[:, 0], curve_model_3[:, 1], '.', markersize = '2', color = 'orange', label = '(1024+512+512+512+1)')\n",
    "plt.plot(curve_model_2[:, 0], curve_model_2[:, 1], '.', markersize = '2', color = 'green', label = '(1024+512+512+1)')\n",
    "plt.plot(curve_model_1[:, 0], curve_model_1[:, 1], '.', markersize = '2', color = 'blue', label = '(1024+512+1)')\n",
    "\n",
    "plt.xlim(-200,22000)\n",
    "plt.ylim(0.45,1)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration N')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies on dataset 1_ct for the different architectures', pad = 20)\n",
    "legend = plt.legend()\n",
    "for handle in legend.legend_handles:\n",
    "    handle.set_markersize(15)\n",
    "plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_1_ct.png\", bbox_inches='tight')\n",
    "plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_1_ct.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "# Plots of performances\n",
    "# accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(accuracy)\n",
    "# kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_accuracy)\n",
    "# loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(loss)\n",
    "# kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_loss)\n",
    "# plt.show()\n",
    "\n",
    "# Import datas\n",
    "# accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7666003d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "date = '13_05_25'\n",
    "save_path = 'Post-processing/13_05_25/Overfitting_dynamic/Dataset_2_ctpb/'\n",
    "\n",
    "# Plot the performances\n",
    "model_name_1 = \"CIFAR10_model_(1024+512+1)_2_ctpb\"\n",
    "model_name_2 = \"CIFAR10_model_(1024+512+512+1)_2_ctpb\"\n",
    "model_name_3 = \"CIFAR10_model_(1024+512+512+512+1)_2_ctpb\"\n",
    "model_name_4 = \"CIFAR10_model_(1024+512+512+512+512+1)_2_ctpb\"\n",
    "\n",
    "curve_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/accuracy_of_' + date + '_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/accuracy_of_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/accuracy_of_' + date + '_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_model_4 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_4 + '/figures/accuracy_of_' + date + '_' + model_name_4 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "curve_model_1 = upper_envelope(curve_model_1[:, 0], curve_model_1[:, 1])\n",
    "curve_model_2 = upper_envelope(curve_model_2[:, 0], curve_model_2[:, 1])\n",
    "curve_model_3 = upper_envelope(curve_model_3[:, 0], curve_model_3[:, 1])\n",
    "curve_model_4 = upper_envelope(curve_model_4[:, 0], curve_model_4[:, 1])\n",
    "\n",
    "plt.plot(curve_model_4[:, 0], curve_model_4[:, 1], '.', markersize = '2', color = 'red', label = '(1024+512+512+512+512+1)')\n",
    "plt.plot(curve_model_3[:, 0], curve_model_3[:, 1], '.', markersize = '2', color = 'orange', label = '(1024+512+512+512+1)')\n",
    "plt.plot(curve_model_2[:, 0], curve_model_2[:, 1], '.', markersize = '2', color = 'green', label = '(1024+512+512+1)')\n",
    "plt.plot(curve_model_1[:, 0], curve_model_1[:, 1], '.', markersize = '2', color = 'blue', label = '(1024+512+1)')\n",
    "\n",
    "plt.xlim(-200,22000)\n",
    "plt.ylim(0.45,1)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration N')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies on dataset 2_ctpb for the different architectures', pad = 20)\n",
    "legend = plt.legend()\n",
    "for handle in legend.legend_handles:\n",
    "    handle.set_markersize(15)\n",
    "plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_2_ctpb.png\", bbox_inches='tight')\n",
    "plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_2_ctpb.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "# Plots of performances\n",
    "# accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(accuracy)\n",
    "# kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_accuracy)\n",
    "# loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(loss)\n",
    "# kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_loss)\n",
    "# plt.show()\n",
    "\n",
    "# Import datas\n",
    "# accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e50ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Import model and plot performances\n",
    "\n",
    "date = '13_05_25'\n",
    "save_path = 'Post-processing/13_05_25/Overfitting_dynamic/Dataset_2_ctpb/'\n",
    "\n",
    "# Plot the performances\n",
    "model_name_1 = \"CIFAR10_model_(1024+512+1)_2_ctpb\"\n",
    "model_name_2 = \"CIFAR10_model_(1024+512+512+1)_2_ctpb\"\n",
    "model_name_3 = \"CIFAR10_model_(1024+512+512+512+1)_2_ctpb\"\n",
    "model_name_4 = \"CIFAR10_model_(1024+512+512+512+512+1)_2_ctpb\"\n",
    "\n",
    "curve_TL_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/loss_training_of_' + date + '_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_TL_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/loss_training_of_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_TL_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/loss_training_of_' + date + '_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_TL_model_4 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_4 + '/figures/loss_training_of_' + date + '_' + model_name_4 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "\n",
    "curve_VL_model_1 =  np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/validation_loss_of_' + date + '_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_VL_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/validation_loss_of_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_VL_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/validation_loss_of_' + date + '_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_VL_model_4 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_4 + '/figures/validation_loss_of_' + date + '_' + model_name_4 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "\n",
    "# Extracting the lower envelope\n",
    "curve_VL_model_1 = lower_envelope(curve_VL_model_1[:, 0], curve_VL_model_1[:, 1])\n",
    "curve_VL_model_2 = lower_envelope(curve_VL_model_2[:, 0], curve_VL_model_2[:, 1])\n",
    "curve_VL_model_3 = lower_envelope(curve_VL_model_3[:, 0], curve_VL_model_3[:, 1])\n",
    "curve_VL_model_4 = lower_envelope(curve_VL_model_4[:, 0], curve_VL_model_4[:, 1])\n",
    "\n",
    "curve_TL_model_1 = lower_envelope(curve_TL_model_1[:, 0], curve_TL_model_1[:, 1])\n",
    "curve_TL_model_2 = lower_envelope(curve_TL_model_2[:, 0], curve_TL_model_2[:, 1])\n",
    "curve_TL_model_3 = lower_envelope(curve_TL_model_3[:, 0], curve_TL_model_3[:, 1])\n",
    "curve_TL_model_4 = lower_envelope(curve_TL_model_4[:, 0], curve_TL_model_4[:, 1])\n",
    "\n",
    "plt.plot(curve_VL_model_1[:, 0], curve_VL_model_1[:, 1],'--', color = 'blue', label = 'VL_(1024+512+1)')\n",
    "plt.plot(curve_VL_model_2[:, 0], curve_VL_model_2[:, 1],'--', color = 'green', label = 'VL_(1024+512+512+1)')\n",
    "plt.plot(curve_VL_model_3[:, 0], curve_VL_model_3[:, 1],'--', color = 'orange', label = 'VL_(1024+512+512+512+1)')\n",
    "plt.plot(curve_VL_model_4[:, 0], curve_VL_model_4[:, 1],'--', color = 'red', label = 'VL_(1024+512+512+512+512+1)')\n",
    "\n",
    "plt.plot(curve_TL_model_1[:, 0], curve_TL_model_1[:, 1], '.', markersize = '2', color = 'blue', label = 'TL_(1024+512+1)')\n",
    "plt.plot(curve_TL_model_2[:, 0], curve_TL_model_2[:, 1], '.', markersize = '2', color = 'green', label = 'TL_(1024+512+512+1)')\n",
    "plt.plot(curve_TL_model_3[:, 0], curve_TL_model_3[:, 1], '.', markersize = '2', color = 'orange', label = 'TL_(1024+512+512+512+1)')\n",
    "plt.plot(curve_TL_model_4[:, 0], curve_TL_model_4[:, 1], '.', markersize = '2', color = 'red', label = 'TL_(1024+512+512+512+512+1)')\n",
    "\n",
    "plt.xlim(-200,20000)\n",
    "plt.ylim(0,np.max([np.max(curve_TL_model_4[:, 1]), np.max(curve_TL_model_3[:, 1]), np.max(curve_TL_model_2[:, 1]), np.max(curve_TL_model_1[:, 1])]) + 0.01)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration N')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training dynamic on dataset 2_ctpb for the different architectures', pad = 20)\n",
    "legend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "for handle in legend.legend_handles:\n",
    "    handle.set_markersize(15)   \n",
    "plt.savefig(save_path + \"Comparison_overfitting_dynamic_dataset_2_ctpb.png\", bbox_inches='tight')\n",
    "plt.savefig(save_path + \"Comparison_overfitting_dynammic_dataset_2_ctpb.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_3.png\", bbox_inches='tight')\n",
    "# plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_3.svg\", bbox_inches='tight')\n",
    "# plt.show()\n",
    "# Plots of performances\n",
    "# accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(accuracy)\n",
    "# kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_accuracy)\n",
    "# loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(loss)\n",
    "# kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_loss)\n",
    "# plt.show()\n",
    "\n",
    "# Import datas\n",
    "# accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54ed687c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Import model and plot performances\n",
    "\n",
    "date = '13_05_25'\n",
    "save_path = 'Post-processing/13_05_25/Overfitting_dynamic/Dataset_1/'\n",
    "\n",
    "# Plot the performances\n",
    "model_name_1 = \"CIFAR10_model_(1024+512+1)_1\"\n",
    "model_name_2 = \"CIFAR10_model_(1024+512+512+1)_1\"\n",
    "model_name_3 = \"CIFAR10_model_(1024+512+512+512+1)_1\"\n",
    "model_name_4 = \"CIFAR10_model_(1024+512+512+512+512+1)_1\"\n",
    "\n",
    "curve_y_TL_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/kappa_loss_training_' + date + '_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_y_TL_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/kappa_loss_training_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_y_TL_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/kappa_loss_training_' + date + '_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_y_TL_model_4 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_4 + '/figures/kappa_loss_training_' + date + '_' + model_name_4 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "curve_x_TL_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/loss_training_' + date + '_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_x_TL_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/loss_training_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_x_TL_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/loss_training_' + date + '_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_x_TL_model_4 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_4 + '/figures/loss_training_' + date + '_' + model_name_4 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "curve_TL_model_1 = np.concatenate((curve_x_TL_model_1[:,0].reshape(-1,1), curve_y_TL_model_1[:,1].reshape(-1,1)), axis=1)\n",
    "curve_TL_model_2 = np.concatenate((curve_x_TL_model_2[:,0].reshape(-1,1), curve_y_TL_model_2[:,1].reshape(-1,1)), axis=1)\n",
    "curve_TL_model_3 = np.concatenate((curve_x_TL_model_3[:,0].reshape(-1,1), curve_y_TL_model_3[:,1].reshape(-1,1)), axis=1)\n",
    "curve_TL_model_4 = np.concatenate((curve_x_TL_model_4[:,0].reshape(-1,1), curve_y_TL_model_4[:,1].reshape(-1,1)), axis=1)\n",
    "\n",
    "curve_y_VL_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/kappa_loss_validation_' + date + '_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_y_VL_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/kappa_loss_validation_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_y_VL_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/kappa_loss_validation_' + date + '_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_y_VL_model_4 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_4 + '/figures/kappa_loss_validation_' + date + '_' + model_name_4 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "curve_x_VL_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/loss_training_' + date + '_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_x_VL_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/loss_training_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_x_VL_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/loss_training_' + date + '_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_x_VL_model_4 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_4 + '/figures/loss_training_' + date + '_' + model_name_4 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "curve_VL_model_1 = np.concatenate((curve_x_VL_model_1[:,0].reshape(-1,1), curve_y_VL_model_1[:,1].reshape(-1,1)), axis=1)\n",
    "curve_VL_model_2 = np.concatenate((curve_x_VL_model_2[:,0].reshape(-1,1), curve_y_VL_model_2[:,1].reshape(-1,1)), axis=1)\n",
    "curve_VL_model_3 = np.concatenate((curve_x_VL_model_3[:,0].reshape(-1,1), curve_y_VL_model_3[:,1].reshape(-1,1)), axis=1)\n",
    "curve_VL_model_4 = np.concatenate((curve_x_VL_model_4[:,0].reshape(-1,1), curve_y_VL_model_4[:,1].reshape(-1,1)), axis=1)\n",
    "\n",
    "# Extracting the lower envelope\n",
    "curve_VL_model_1 = lower_envelope(curve_VL_model_1[:, 0], curve_VL_model_1[:, 1])\n",
    "curve_VL_model_2 = lower_envelope(curve_VL_model_2[:, 0], curve_VL_model_2[:, 1])\n",
    "curve_VL_model_3 = lower_envelope(curve_VL_model_3[:, 0], curve_VL_model_3[:, 1])\n",
    "curve_VL_model_4 = lower_envelope(curve_VL_model_4[:, 0], curve_VL_model_4[:, 1])\n",
    "\n",
    "curve_TL_model_1 = lower_envelope(curve_TL_model_1[:, 0], curve_TL_model_1[:, 1])\n",
    "curve_TL_model_2 = lower_envelope(curve_TL_model_2[:, 0], curve_TL_model_2[:, 1])\n",
    "curve_TL_model_3 = lower_envelope(curve_TL_model_3[:, 0], curve_TL_model_3[:, 1])\n",
    "curve_TL_model_4 = lower_envelope(curve_TL_model_4[:, 0], curve_TL_model_4[:, 1])\n",
    "\n",
    "plt.plot(curve_VL_model_1[:, 0], curve_VL_model_1[:, 1],'--', color = 'blue', label = 'VL_(1024+512+1)')\n",
    "plt.plot(curve_VL_model_2[:, 0], curve_VL_model_2[:, 1],'--', color = 'green', label = 'VL_(1024+512+512+1)')\n",
    "plt.plot(curve_VL_model_3[:, 0], curve_VL_model_3[:, 1],'--', color = 'orange', label = 'VL_(1024+512+512+512+1)')\n",
    "plt.plot(curve_VL_model_4[:, 0], curve_VL_model_4[:, 1],'--', color = 'red', label = 'VL_(1024+512+512+512+512+1)')\n",
    "\n",
    "plt.plot(curve_TL_model_1[:, 0], curve_TL_model_1[:, 1], '.', markersize = '2', color = 'blue', label = 'TL_(1024+512+1)')\n",
    "plt.plot(curve_TL_model_2[:, 0], curve_TL_model_2[:, 1], '.', markersize = '2', color = 'green', label = 'TL_(1024+512+512+1)')\n",
    "plt.plot(curve_TL_model_3[:, 0], curve_TL_model_3[:, 1], '.', markersize = '2', color = 'orange', label = 'TL_(1024+512+512+512+1)')\n",
    "plt.plot(curve_TL_model_4[:, 0], curve_TL_model_4[:, 1], '.', markersize = '2', color = 'red', label = 'TL_(1024+512+512+512+512+1)')\n",
    "\n",
    "plt.xlim(-200,20000)\n",
    "plt.ylim(0,np.max([np.max(curve_TL_model_4[:, 1]), np.max(curve_TL_model_3[:, 1]), np.max(curve_TL_model_2[:, 1]), np.max(curve_TL_model_1[:, 1])]) + 0.01)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration N')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training dynamic on dataset 1 for the different architectures', pad = 20)\n",
    "legend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "for handle in legend.legend_handles:\n",
    "    handle.set_markersize(15)   \n",
    "plt.savefig(save_path + \"Comparison_overfitting_dynamic_dataset_1.png\", bbox_inches='tight')\n",
    "plt.savefig(save_path + \"Comparison_overfitting_dynammic_dataset_1.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_3.png\", bbox_inches='tight')\n",
    "# plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_3.svg\", bbox_inches='tight')\n",
    "# plt.show()\n",
    "# Plots of performances\n",
    "# accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(accuracy)\n",
    "# kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_accuracy)\n",
    "# loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(loss)\n",
    "# kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_loss)\n",
    "# plt.show()\n",
    "\n",
    "# Import datas\n",
    "# accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 layers - Training first layer : True - Training second layer : True - kappa = 2.45 - lr = 1e-05 - lr_decay_rate = 100000000.0 - reg1 = 0 - reg2 = 0 - eps_init = 1 - fraction_batch = 0.2 - observation rate = 10 - Train layer 1 = True - Train layer 2 = True - Dropout rate = 0.4\n",
      "[0.         0.48416665]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "date = '07_05_25'\n",
    "model_name = \"CIFAR10_model_(1024+512+1)_save_3\"\n",
    "curve_datas = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name + '/figures/accuracy_of_' + model_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "assessed_model = torch.load('Classifiers/' + date + '/' + date + '_' +  model_name + '/' + model_name + \".pt\", weights_only=False)\n",
    "save_path = 'Post-processing/08_05_25/Dataset_3/'\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plot the performances\n",
    "model_name_1 = \"CIFAR10_model_(1024+512+1)_3\"\n",
    "model_name_2 = \"CIFAR10_model_(1024+512+512+1)_3\"\n",
    "model_name_3 = \"CIFAR10_model_(1024+512-512+1)_3\"\n",
    "\n",
    "curve_datas_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/accuracy_of_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_datas_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/accuracy_of_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_datas_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/accuracy_of_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "print(curve_datas_model_1[0])\n",
    "plt.plot(curve_datas_model_3[:, 0], curve_datas_model_3[:, 1], '.', markersize = '1', color = 'blue', label = ' (1024+512-512+1)')\n",
    "plt.plot(curve_datas_model_2[:, 0], curve_datas_model_2[:, 1], '.', markersize = '1', color = 'red', label = '(1024+512+512+1)')\n",
    "plt.plot(curve_datas_model_1[:, 0], curve_datas_model_1[:, 1], '.', markersize = '1', color = 'green', label = '(1024+512+1)')\n",
    "plt.xlim(-200,np.min([np.max(curve_datas_model_1[:, 0]), np.max(curve_datas_model_2[:, 0]), np.max(curve_datas_model_3[:, 0])]))\n",
    "plt.ylim(0.45,1)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration N')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies on dataset 3 for the different architectures', pad = 20)\n",
    "legend = plt.legend()\n",
    "for handle in legend.legend_handles:\n",
    "    handle.set_markersize(15)\n",
    "plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_3.png\", bbox_inches='tight')\n",
    "plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_3.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "# Plots of performances\n",
    "# accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(accuracy)\n",
    "# kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_accuracy)\n",
    "# loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(loss)\n",
    "# kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_loss)\n",
    "# plt.show()\n",
    "\n",
    "# Import datas\n",
    "# accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70c105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
