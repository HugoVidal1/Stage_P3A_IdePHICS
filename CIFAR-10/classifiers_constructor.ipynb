{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 22\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10]) torch.Size([10000, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAEkCAYAAABQRik9AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs/UmsbGmSJobZmXz2O78xIjIyI7Mqq6rJKlY3KU6LBihoQS20IbTUSoDW0pY7QSAEQQtJK0ECtJG2lBYiBIECAUEAQbBZ1c2uqSsrx8gY3nxHn88ofJ+ZnXP8xsuseM9fdKlLv0V63vvu9et+/D//YPbZZ59FTdM0EixYsGDBggX7/2uL/64vIFiwYMGCBQv2d2/BIQgWLFiwYMGCBYcgWLBgwYIFCxYcgmDBggULFixYcAiCBQsWLFiwYLDgEAQLFixYsGDBgkMQLFiwYMGCBQsOQbBgwYIFCxZMRNJv86S6ruXZs2cyn88liqLv/qr+FTLoOi0WC3n69KnE8bf3r8KY/mYLY/rhLYzph7cwph/ewpj+HY9p8y3syy+/hJphePyWB8boXSyMaRjTMKZ/Px5hTMOYyt+TMf1WCAG8Ltj/4//5f5fpdMrv4YWpJxZJHOtX/Rl/u/c1iuL2ue698bm9f7/d7HdR0/dg9n7VPhOv0zT85LS6FvwL3hEe+H0cJ/YaNceott+pejO+Svu1U3Tu/7t7vv5MZLVayX//P/wftGP0bc2f/x/8R/9IhoMRRlGKspSyqCWOa4mSWrJ0ILPZjONXlpVUVS3b1VaKXSlV0eijrCTPc45zlsX0AIejTOI0kaZsMAySDjIZjUaC2xQPIj7v9HQm2SCVbJRKksaSF7nk5U6kiaSpEyl2hVy9uJX1Opdf//JSVqtCvvfZj+Xhk08kjbaS4RHXMk4KXvOLN1vZbEt58/qFrJZLKepKilLvQWtVI+PhUP74xz+Wk6OZ/Orr5/Lq6kaWq6XcLhb08qu6lrpuZLPevveY/s//p/+hjEeZRLFwbAfZkPcMrx9FjWAaxFEko3QoSRzLIEslSRKJ8YgxVzGfdE5lSaJzNda5jLHM0kzSRL8mSSqDIcY25vc+r/HkprHPj/lvcy+yeYXf+Lzl3OUj1klV63xN8DdRJFVV8bXKsuT3nPt2ff4czG38uC5LqatKyrKQoig4xzGmy/VW/vH/8D9+7zH91a9+LicnZ3zP57drebXayV/+zVfy//4nP5FGEslGM5mOB/IHP3wkx/ORTFKRQSQyyBIZDlJZrHby8vJWqrqRotbxn08GMsoSPoZJIsPRUIbjCe/TbpNLJI2kuFdxJFmKexRLnGCcI9nsclmu11IUtWzWBa/r5BhzOpPVcivbXc71UhQlX/fkZMZtcbXeSlVXHBf84NHFkTw8n8kwjeRogHvRSCw1x7eWRJoI/8KV8K7qw7bYu7s7+cH3f/DeY/p//E/+JxI1kVQl1nwsSYx7XUpR4LML1yW+QVCH6YFx0K8iCS6i3a8a3m9cMceLcwPzDtMp4vUzOxwP+AnKJuPPKs4NkZLzpZZ8V8hmk/NlMd/xtaowj0UGo0TXSIw9JtX1ZHNrV2x5zwZpJCnmYoQZIRIliaRpxj0oz/Wzp0Pcw0gS7lUixa6SYot5jfcvZbst5H/9f/i/vfeY/q/+T/9Lmc0mtt/rfij+FeOBocDQcU01kpcF1xX3nqrm0dDUOqZca701mvo+gddL9EzDuuddsPmEvUv3mkqaupKI91VvIL9iTPm7hvt9XeF9sVfi/nXnjp+L2FPwfv2zrG6wR9bc9/V19LNIkurkMOPf1CK77U7+9//x/+Zbjem3cgj84vCCOKD2f64D3x+4/t/4z3Covf3nf5tT4O+1f0jfvzb+tNeWAQOGmYyv7hDgZvrTu4N9/9F/ne4rHvVb/85f711hKn/+0elMxqOxxJJIXuBwLaVpsGEV3OyxCUZRIoPhkGM9nYy54deF0CHAZN5ut9xMxmMcUIkM7GtTR1yMo9FQxuMxz5s4bSSFQ3A2lSxLpagqLurFaiH5ciORpBInKTfaomp42BdFw4fEqSSDocR487qSNI1kPh1KWTay3EYSJ6UsF0Mpi51IGUsT6YLiQsRCwRaLTSKJJcVni/0A1sXl88nv7/uO6XiYyXQChyCS8Wgi4+GU46SOUyNxjEcs09FIUl5PIgmuIdGDH4eVLnw4BHZtiR7ag+FA0iyTLBnIIB3y+9FowgOZi7dHy+EnxhzBYW0HtjuqWNQ9V2nfw4VDYJsB3rusSt4jbPr4HHQGeDiYI8DNRp9b5blUeF4RS5Hopl/axn3QPJ3N5OgIcGws62Qo5TiX8fM7adIRD846GUmVjKTOJlKnQ1nkO5GqlOlkILNsKIuilFd3leRlJTt4BJHIRYF7lcp8GPHrOE6lHo64BlbbigtvlOlBOJJUUtE5g6+7upHFLpI8b2SxwphEEo9jGcWJ3OzgqNshV9YyaRpJZ3qI3e1EihKbsgYFx5jvo6kMh7HMJolg642l2nMIKjgEmB96xupDh/OgMcVGXZdYYxXXBNY67i8dgkgdAn6lz4e9Rv+dJnCUGq75jOdcI0mEA6WRlK6L7pcSY55FUjexOaWYc3A6a3UIilrqquGhhAcOsLrembepRwN/3wjXOD60vwOdARxmjTpd/FsEM3gHXDMdF6wJvKbIdqOzPRvHEqcRthKeXfm2lHxd0Ckq8kp2eXHQmE4mIxmPR61DgAO55+HzIIc1di7Eux0PZgRWJRwAC/TUISj5XN2jIhkMBnRw1NHYP9fcMeufF3jg/bGf9E3PpFqGCH7wPubo988dd0YQpOB9fC/x3zO4SWL9Ozj+GGQ8FwPbGw/8nL/7lmP6rRwCt9bj6jkDPiiOBPTfuEMJus3+/gH6NofgvlPR2W9xCHyw/FoZoenGf39AHMnQiLE75PVl3vKVXpkjEI00kSME6tkfYq9eLGSQbi3+0ENEP0VtUSf+ichBP8N0NpThMCVigImMMR9NJvS6h2NMVvXsS3iOuSIOm20uN7crHmpJpp7q1dWS1141hUWylT6aUqTJpcxLiZJSkmElg6F6+Mu7a3n+VSRNsZImv6MzIE+OGH0gksBg8qBCJNc0kpgjpmPdSGGfQT1rvT/umePzOEIA5+EQ08WokRw3zTSSQTaQ2WTYji+uaZgm3LwwvnhuGqdEADA+iEgxluq04HNhc4kkGw4kgUMQq1OgyAIOY2y0uGt6/fwMHsnDacZzcHEJNlT8TDckOHfYnNoNhQABtmu9j/ha1hrBuJNroR/HFBsp5zU2WESzvtHFMZ0VLPABECbzzd7fMM8tAMDHwOabJXI+H0rdJJJlA8nSWNa3KynXW8m3a6nKXB4/PJPJdCw1rkES2RaVvLq8o2M0Sk4kkZGU2UjqKJXn1xu5+nIheV7IYrHhGoBDgD18Mh7SgR0MM6IAQD822w3HDhsq5lMZ3fA56w0QAqAGisxUeSHllSJQi8WKc83j5pPZWI7HI4nnI7kYDzW6xgHKPQTHm6IDvdtJlOdDZKlx1+uokRKHPVBBTKGEk0h3gKbU2N6WQ8V50EiuQa85ArpXNI05BHR28bpYz+oA4JiOolTiFEFFLFVU8Wc5Aopa1yAPfjw78z1SJwzmD9+hLmS7hXNaSF7oQcsAi082FC2i69Q6N5gpMR2LRrKh7pv4ZxPXUlaNYKvBmYv5AzQBKETDBfn+tstzrkE4OHZx5jzbNafqODu6ggPV95zakDk9x/B9YutMn5tjP23RHHUS4LQ7WteFMnam4DXw2u25aPedT9J55X/Rodhx+z0dA3caar1OPUaB4pqDQMdLnR68z/2AlUhCYfDMt7D3cgi6SK5LB+w7CG871Psphd988P82R8GGuvfc+7/qOQU2oP3Bvf+68GAd6nHHoHtNG1RGtlhWdoM5l3zA+x7i+9nV66XCvjiQUmziBi23DpZO1i2gvKaRR09OuNiKqiTchY14NB7xbwcjTHaR3Q4eN7xtpBMAHZd0DvRAwy3HCa0HTiOYLJWMx3idgXrINTYAOASFJFktaYZ0RCSr5a1sdjspNysp1ndyejKV4xlSDplUjSIYfnBy0SiazQMVC1S3N78HOm5+0MEp8O8PbsBJ3A8fJJIEUF8C7z6T4XCgiDxXJGBN3cCAtBBxw6aUDYkYDLJMYUKL0hFZ4HOkg4HEOGjjlE4BI4A2FWVQIyA8LF7Fbnmw8NMSotFNlNuvzy+LJtTx7EUC0j8Ieo4S/07nKw83vAeiWQZ/Pp/t2unMpDLKD/QIemseHyuNRMZZLCdTOARAUnRebRYbWTeNrDdLyYudTGcTecTjFwdTInnVyNX1ko7Qw5OJTAaYOzxG5M3tSv7qV294mN8tNhyj8UDTBtPJSIa4h0gr8D5izCrC08NEnYZtVdEBZOqtqrk2BkMgb5UsiyU3czgEOARwtYhib+/WcjsdyxSHW61zl84P9loPXvbKsd6esnwfwzkDP7qO8RX3rJYGpz8uDFE8Pl9vKQAa5uHV3g9i33yuruNasH1EMVz7Up1TwzyiOJMk08MbTggOo6JCSqBzRDVN4R9Mo+UU49GIbHaNIHhfb0WWa10rSENivIGuII2ja71huhJrSCck9gKk2nAwNtLEcFTUka2Q0kRKQhS5xd5XHbj0iQSWOR3Gbs9XZwDRNuaErhd66vcsUofXAlxLOiuawHQAdjBNkftZmMIfuI8+23/8noe2Oe1vyXzbBbbf9gNu/C0QXHwmoIRlDeSqfy7qu/Cz0Y/U9I4jDLqvVnSYvxOHwC/Uv+6nDO7xA34jn2D/8P9NB/9vcwje5lTcRwh0sXTwij7Ff9veOT0UezwC/Z85EvgJYDZ7nb3neRR/IEKwWe+4eLjuQOkg+qAwnl6pOi042PF1s0YEawdo1UixrWS1RDSERafQIg84ROQFJkPFxQEnYTBM5TgFJyGS3VbzWHGCgwtjmqsHar4rPNI83xHG8+h9t15KVa2kyndS7dbcjJ+/uibXIUknimQUBdGBzBaJjqV6ubqJ2zj2IbbejdPf9/DY97DRcChj5PWxYAkR4vSsmOZo0wFRLMPBmP8mklLVMhqNZTSetAgBNhI4a+4QaISRcMPDEU+Ivq5lvdxJXpTy8vUVc9cYa4yZIguRTMdjOTs94nWdnZ/yoHanpQTch1yijw8dAN1AMttseCBhgwD606InOC4VQWA6GXnJFv3wqMjRO4XaD7K9daubIRzBbDDEsPI9cI1ZZlwGQs9AqEq5uVnIdlvyd6PhQObzCRGCs5O5XJxMZTodyXiEVAxQhJoHZApoAJHlQPPio+mQf8sIL4kJteOgh8M3shTU6dFMRkOkwRRpQhRalApt55jHyNUC3q5qmUwyGWWpTCdDmU9HMhikTK3o1DN4qbZDg4e17xqODuwHN+9jJTZsbPpwhEmxiaQGdF0UbboThtQc3quEo+5JcN5nJgAsgtc9OK90DQORwQGsqJ0eGNjL+LlidQjqGuvTokrsBYZYtB+r/03SEBnJhkjfaP4/TbH/4UDyeQuekzrXiqgA5dSBo6ODvcn2AHxuZxjhd5rigEdx2JjySuzsoHPNw7ym5xUVJXkSvqZa9IAHvO4JRJTNC3MYn1dpqQQGEoRtPTWgr8dUXs8aplc6BpX+qe13FmS2XDYc3sYN4nnSdO/NNDLRAUcIsBdZ0Oif2XlJjSJadAJ6KYh3sXdGCLro2hGBLge8nx7oezsdSUv/3X3tv6aOWRfV95/rr7P/9TetSKcA7R/kvsB45PXuVD9l0Hce9PFNGIYbt6UN3jXXdd/ubhcyMpi6aQY8nHFA5Tsj99jz3Mu7u2mk2G0JxSFvvNkWcnu3YTRRxQobJvC46QPrvUDufLPZyvxoIpP5gBN/tVwzYhqOUqILcBpkrVBkmqjDsV7lTDkAKsO/lzdLWa827QQGZJvXJXNrp0fHPOjqfKcOAfgGUSJFXcoOqY0WllPngAegr5G+Y9ZuE+9v0+lE5lPP6zOB0oZjgO6HyP2nqczmR8wJbrZbRpXkGyD9QkfAIE+bn4T8+b2FUnbxZV7wwLu5W8p/9d/+tVxdL2Sx3souLxm1YjwfnJ/Ijz59Kuenx/KHo4FMkOM08lFhOX8SvMxJwthgI6Xz4muIkQs2ZhsnSx+0GcwKKAKuU+85+QyEdM1pt03kva07Ec0ZSHnfwZ9AfrkudazmM/AysL52Ekel5NudvHp5Sf7JaDDkS5yfHhHqfvroVB6dzXkPcL3DYQIXQupEJJsBvm9kmGnUO4PjMB7xkIfzhg1vk1cywOdFGiHN5NHFqZwcjdvD5vpmKa/f3ElVVLJZb+ik5RtE0o2MTyZyNBvK6dGESBc4DHxd26t4iFnYnMa13ssWKNH941CQIK/B76glx35X4sDFOi+k3OU8UMF14NCTS2AOCL5y0eAwBo9Ad7uyAmfI0g2Wu8bBDzQR6xv7NVADncuFwc5IoWXd/shFgjfFexm8R/4EoH7l3gyySNKRRrsNMT9LWeDzFLU0uD91LQnQmgREPL1uRR4UGSAE7kkbHr5ARuDwHq6MgyvhZ08S7jV0unBfQXBmis0dZzUnCpIonCk/IE0UHemfF3QwDL1UhBHryc4K5yT0rgNjxp3MCYb2ev5v/MAdTPUs9s8dIAIM6ow/5KOsyKsFKUaAbu/TPYeAV/6O59N7IwT9w7/vDNy/iC6FcP/vO8gTRhgGkKrnY97yGvq9v3f727d66i2o0uMC+GsC6tdvuufcd0R8MqiH2ZFQugoDy7kdiBDoxKO/zq/KOq1JNILnmqYGMdv74BBHBApSUZNIO7k4hu21K5TIcSPLGItEc9O73c6Iahr11xU8YqxDg7bhECAqBhELe0LTMIoaYkNoYhkPQBospMgLGeBwm0xkOBzK/OhYYfYGbG7kfvE3A1lt1nKzuGHkvNlhQ3JgTiOODm35cDbKMhkC2idMCDgZnyWVDFA/qgIGyP2nrBRglcBgyK+Z/ZwHsafCzCHwSILzgzdEwyCM4+1iKVe3C3n+6kpeX97KalsQ3lWCM6DZmu+5zUv55OlDOndjvBfvqeUOjTjE+YpozXKpfaewdVz3kCp71t66xMbQOVmEnzFPDrBtUcv1YscD+c1qJ9ebXJ6/WcnlHX4G8EU3qiZLySW429ay3jayLApJ1rXE2HBHqKSpZJkr+ez17ZYRLx2COOXrA5bm9AX0HIkUip7LZoN5XvD9EYXmu0q2u1rKKJLbppBd3siLyxXJiBp3idwutnJ1u2PqbAGSYVXzebCrZSF5Fcmzy5Vk2VBGg0TmI8DJvQG3r8iyYS2Mh4lMhphDMeeYgXjvbfyc9r3GjEAMKqkade6AGNCZM04JUgqaesJxatUmiK4R9CPqR5CC39EhQCmH8hJ0tWFf0L1Stz/l9CAdhR1C9wvMI3UImDLwEgcjEerVgguDUgYkgHC44k8VWUEKAPeG3Ahz+IHm8O0M2XIGf1PHdgbqteGigHIgEj5oTOGMuBNP4jL2VuB5ihQpebIj9Ub4t4WQFTlHsdRE5tQJJ0ZrxGKgURgGXZu6dj3yhxO6F3y2oU0PeSYq6GPgfCBcj6VKzTnwc5GVRc4dsvRjf6v0s1BvjaK69Antb/wcfRek4CBS4Te5A/ukwj5C0EKZTqywiYHIFd4MNnBu2GlKr+3+++rrfJMH0HEZfJCs7Osth8w+C/SbDsE3HAHP5/aqFfp5bhy2JJUcZEoYxDpI8pKTjeU/25wQK3L7hLEqZUkD+txsEJUnMhwW+neETGNJkUyNGlltwJzFpmFEsKSRwTjlZnF5dWUH0EAXSqmeJQ7vDKV0USqT4YATKwXs1UQyOB4KQHKwiAG3rtZrWSyWkoHNfzwjh+HJ049lOBrroZqmcjGfy9l0Ls9fPJe/+dnfyM3tjSxWG/rVSJGyLIkbkS4MdXTffQK/zY6PZnI000gfETPgfjg5A1QTpJmMR3NuFmmmaYXhaGLVBSBBmQPg88oXojkCGnkhwaib4Xq3kV9++UyevbqU//qf/0Sev76RskIqwcrEkliOpkP5+Zcv5eNH53IyG8iDs2P53pPHMpuMNXrKMLrYZh2+1dDQ844tl8CQmc4nUUcSBofCowavUyMSgxwkUkZg/R9g1+tc3rzK5Xqxla9e3cmLq5U8v1zJL76+Uag7AtKUyMMNoshE7u5y2W4K2e7WdAZRmjk5RgUNSK47HgPX6xcySjH3QNJM5HpdyGLVZl+JEBSApSOk1taSxhsealiidApynTuv6i0Pvs9fbbhmsFk3QMaKknwEjAPy5ZhWeog38vJmyUP+Vy+2cnZ0RScGZF3OP7/3ximajlAaGcvTh0fy6ZNjOZ4M5JOLVHYH0jLIpycZDPcKjkwhZZPLrtlyTdY1c4BGzsW6KUVi3H9wJdS5VgQMfgHmDj4aSMEIADQ1oHwIXGiiiAPmCSAYpCvhuMJhSxBggJiJv9GDidC/IUtYA3ThuDbBFQIGWckg3XH5brfYnyLZbDDmiLpL4x3xLvJaYqQymNvCa0TSVHAI7HvOcSXFefnd+xrWCNY4VlNGLgP2OfWCsA6WlldHKgtOARBQVETp3Yi4lzJgiFEui0AB+4bObQ2b9GzpOEDK1ICzidcDgY8ROsbQyL0slyXv15x8lh3iOnANltIFQmvuEcYDKYg2XUEkA2cjPhecFkMI7LxlcGdpZez7fiY6kvAu9k6nWf8gvv+9ftWP5F+7n3UwnEMvWkJVye3trex2ucxmU5mMxzIcjcgifvt7378Oc0a6t9vLD913CjqEoO8QKDlQf94jINqfY1PQfwNi60ICj9QOJRXuVZsZa9y9WCeOtTl3fw7Z+F6frrlt8gD6ZBWDPD3/7BMTByMOw0E25lfktbFwJtlYxgOMfSrj6ZD14usUkYoIuOpYCshv1tQWgCcPzkJKJ2AwGEmSoiRnIKPpjCWSJ6dn8vDkhJDXqzeveUHUVEA6YYDJ3Z+sb7tP7+8UgOAGtEIRAlQB6OfGoQOEQEuHNK+upEAlk3VuQO86+gSeBtUWJXO+3AjKUm6XW7lZrOUWZLptTkeOETPyzzyX4XY1zG0vpmvWzqNeH4u42ZvXWhqmN71PkvG51stB9glv99djb13er1s+xG7Xuby8LuXNzYaOwKurhVzdbmW1yZUUiIgxTWSx2tJZxc9325KoEPQpBnUhdbbjnMRBjXlwW1eyiSMZJCVTTEuQYLFB23jTH6rVIUCJnPJs1IHFgQW0TPk2WopZNjvOcXfgsEmDR6MRqx48TsEocaiiXj5CQIJrhx6FsbUNt/bs0GQUyzBDWSqc8IzvezYfEfE5xAjRM8Aou0eDg6DUawBsz03Tax3wcyAD2PQVIXAmScsK1E9r3zuaaRU9xm73yBdrGA+OC8YAe0iqP8D6V4DCUEaS7Dq+Ae53CoKg/32N8QYaZmkAlvfiPZTW2PIwPAhr92HTf3Gy9odAC9uXMHSa1Q6R1KwOUF5T0qSaxuTVaRVJY2OFa8an1dJUdaz7a25PE6BNgRo52tMUhhq4M8fXwS3tPnj7Oq6H068xI9rTnW5tynCfxL//mff0c3rVc98dQsDykvuaA/3KAruWe//u/l5ra8ttIa9evZa7u4X8yX/zz/j9Zz/8TD56+lQ++vip/OhHP7DXNtoU31PZqs6+bwmN9ruOJ+gD6TOjI8l0Q9s9tz959AbZb9oBVlIhIFcnqTjjm0S7AzcFMFCZuwK5DZoCOysRYQlZI6tVrqid+R28hgj5RxzOyDEDHlf0AOkAvV6Mi+a4AC0OEhCohsyRP7h4SLGes+NzGQ5Hcnp8KuPxRC5Oz+ScwjMaFcNJe/Hqml8Xd3ckGC5WK+bbk3Eh2exUy/JA8opjuVuWst7t5JMHH8vs/Fw+/eH35ceffiKPX7yQ0/MHcnV1Jcenp1LmO3kwGVPAxLUVNEp6t4n72wy6DkezY0OcgFhgfGNJjW0Mh0Bzo+6xA/nQ+6uQoObIbcCVMGWpnJ9/fSVfvb7jQQMo+vL6Vv7qpy/k9m4p2xI504HUFQRDVLAJB3/SVLLKYrkdDeTZy0tuvh9/9JEcMfrSSe0bBcsN/WD3CU/nwevebDu3++RptlbPwZwcF0ZhFAQyndVUv6/9l//ipfzZr+7k2aulbFdL2SEnj/kPaJOkMc1nbpavOVXJK4BjhMO7iWSXb2S52dki1NB6Ba4GImA6nGCYI5J3ZndHnLM40xx/h2v5DzvkNUcbLbc9VNWqaFoHuQsGYNyopZHF3UbTb1xjOsaExLGGDGFjDBjVcn46lYcXM/ns8bHkxSdS5SiNfH/Li41UJcSVdu2cIQEXgjYYlVRz/shpYxxYEmwQN4N8kgy9VFVfE1G+Vs7YXlEndJ50rICFgxSLr3DqWZIgq/VSNutcEIdNR54es0oVEGhjVDWh/DWiPgn2hCwpZTLItWxXUjpcm10s610kaaGcgRFSFOQv4Bk6P3iG2AGshzDuXcX7q37wYY4reEscD5Ir7XzifVUy8Xgy1n3GUiGK5mHvrU3gR8cfvkBZ4vNxcssg05K/HpWGawt7NiF+Ox9YFWCljHQOQDxG+pDVJOqMUdLEXTOsgTQVVHt2aWshSqFz0R0AJT2S70DoRVMgihLiHoEvYnPDkAt8FqAQ4CN8RwhBRwDcdwj4k99CIHSo39MgDcV0lsuVPH/+Qr766muZTmc8mE7PTtrn9pN59z0i99jcu7Rwaw8Z8Pe2LFUvL6Rf973Rfai6DcoIa9nnRW7OYW1Mskon2aHm+WkoCzo7nSWRDhNzYXUpJAaSOCdIKlN+AEsFDbbSxCEOCx0NpGFGw7FMxzM5np/JaDiR89MH/NnZ2YVMJ9joLuTh+UULg603O8nz1FI6qJbbyBaLBhs2lMpQw9TqCQg3CkQ32HCSbCiT2VxOz885aVeLJfP0V5evZbfdyFgTjl253gc2fF4sKFQ/pCgjTEGk9HKiTrHSST/mA3b5zV6ErV91bPFZbhdrefH6RnY5xqiUm7uFvLlZUwGPjhiVxQp9baIIWvbDcqiiMC6FVnRYoqQtIeum8T7y1s1Kj3h6iJulNL7pqN9Hlw5ztl5er+Sr10v56vVCZLeGooxWURKa9vWBqgndIMlNITkFE1eJa4y8rDSNEb0Vfqq/2+Vj9bnOjTHPvHWEOqKnonldzTeRNbtedwruI4j+b49M8wrVNHrg18i7t0EGmPlaSrmDeiCqblD+W1VMHyB10hTbg8YU+XscOnAI6AxA8IsXr7l/1vXzIwAN4CQ0QjMidp8tzjD3j6fzCE4Aon+W+1bmQNkG7Okl6glGCX+/y3VzQbZWhbs0ANN0LRAZQNNY55q2bKCkCk4Dg7zaHD9FLlnSjJJGIhA6YfFVU+/KKcCBy7Rxb51h/jhj/hDr5r2dBhgcA1u8Is61Q5DyBeIEAaU4Bn8DBzruBQIurGPdg0s635627nRsuI575cLOC/F/R84vwLgzUPPx91tiPzP9AUepXQnR077tV+xhPWe3AwR6nD7XyjGn5V3QwXdLgH9j47FLaYWJvBbVhSr6kLCyP50PAAGJzWYjr1+9ka+/fi7j0ZQTDWmD3/mdH1FOFtCjvqe+hg8Ay+FAbEOOkFGH5vJxOB+dHFNNDpGHrxnfEFzzovUc94hYPpm6z9pWIDgZpmWNOmoASPKwnJeSeVTBD1511qiELgRWvPwQkxA5TuTDEYFiMiPPjEkLjxiMdSrgIQ9JCEs3ttEYNduZPP3od+Wzz/6hDEczmR8/UEKdRc3lMJNVmsplNZZyic+D+7Kkw/bi6loPsTKWMhpLkUFarpEB8msYb+TfBwPJdzt58cWXslus5Oarr0VWG6k+/Vimo7GkDx/KfDyW9WotTx48kvVyKV/99Cdye3VJdAPBC3KHiFoiREZMgUKz4P0jhen0WMbjOT1vyKwqWqL3Gf9ry3SsVI8qcPgP5WzmWFIIyup59YDChljL189eyF/+1c+YKnhzvWTebrne8DmoFhlmqaziWPJsgCSt5qwjra/2QlIYkBKI98B50TLIrv7ZipvbdeWbRee59GVM7QC1qDs2cRUeHCAWcZMfyCA7LOH9iy8u5eY2l3xbyINRJGfzgYzSQmZZzvKzyQQbqtXSN418/eJObm53EqdTidMZxZwGoynFsdKBohgst41jOqjQf+DGyzLZnPLXfU0KssHx3NGIaUV1DAwhaBnVeq10jD1XbCWdcMowLkhXcZwJCfS0SnDtMT5DLINU+TVRBBXGWF7eruVmvZN1kcubyxuZD0R+9fWVNOVhDkFdbulUbjea907SoeWWNW1EwR5KkoPr4ntZIw1QSRMiQhrBZoX+G04Y5GqhVFk4uVRr55O0kCguZTAuuJegJh979XiC98O4IJhQYloJZVLmp/Vva0HqBVUNlUxGWl3AOU0VT92jjo8Smc4IaLFEEumbqlaZZBI9ETjkTmZ2XQ3lFShpD87GYQ4BVVuhi6ILz/i29hmwX6Naiyq/0BvJ5OOnn8jp6QM6R00E0TDgQUBqt/Ly+Vey221ls17IerXjHozKiT4M72kV7GWqzpjSScC5gOourlV8VlQdYL72UXQCNIqyJnjvPrxvZ5enZKhWWmmZe1HscwOckAgHs1/a7by9d0m/viOpsJMp9g+lX/tpgs5T6Z5rUGwPVVANfiiSLeXm+lbevLlkJHdzc0cGOwQvEBa3bOteeSAGe73eyHa7k8XdkheAvwWUMp5CkhfyZu689LfhPs9hPwq7/70uvV6kZbWp/aoFeLiEdg4yXUx46GSzXH+GshkvlXQOgMpnYiFXRU4P1meOch0UJgKqgZkHHf7JcCTn50/k+z/6NyQbTmUwBgITkcTChSmV7KSWZZVIvW1kt8U9ARFsI29Wa0JgjQx1sXCjBGEIgi9jwomT0Ui2qxXz6flmLevrG8mwIW93MgTbP8vkaD6XYpfL8eRIlre3snz+UnaLNdXTuPeZoBFLJmNs5ofxMobDiWRD5UjsswK6WuKWMGq5ap8XXtet0KGxfXEfOGdrubm5la+fv5DL64U8e3XD11QHK5GTk7nJz2LMQLCqKOcMT1Tz2B6zas8B3EOVjbEqEp+ze9EB/AmHwuxvewTX+2VUcAqVRGUcFyxycCcORGPAGVivAexUMkkjeTBLZZaVcjosZTio5GQOchpEaYzlvLgVWd1Jkp5ImjUyGKHkdUwNB8rXUklRnd/ZVNNZ2KxRhrnd1nKT6oHu+yPUMlE+N5sNZDaFDLfmhdVp9koljT7pBOC1KFetTt1up5vpeKywqzD6VwdQ9ybV5NAKgqGiSPFUqgaOOhySSJ5f5bJcgS8ykNc3K5EDHQI49Dggt7tGBnDs04FCzUQFgV4oIjIQLUvWPDLmzo7Mfk29WCmsY0w476pItjsIMlm9uuphSTbEfBSJsT0K9hIcRhBwaqQeGevf9jUvedS1acgDHSZF252gqfSXhofbONXrANqwKxS5xPhFFEHTe7XewknTk44Oa2+FYi28g4bOW40IlRG1e+CSBgLU/9c9M4O0eoYy2WN58vAjiZOhKjlSPKmW5WrBtNhyuZANerOgQgplXSRvWtRtmgCuY6DnHtAVdXgQrGnqzrkBFog4MtBWQyhC3M6LnsPvCIQ7y3g+vvfyQxjnC+Wo9b9+BZKnvb+tvTdFvn/wd86A5i5BFKS6kjWNAJlsPj+yGk+Ue2ED1YdDI8vlWl6/vpSry2u5vV1IVU1kNBl2xK66kbu7W1mt1vLll1/KF198RS3wxXLFKcXSofFI/uiP/0guLi7k/PxU5nP0XTA8vd00v0XvBMtTdk7EN9MJMERE+AyHmLKiVTNcURTlFVSArUBKsUY8QAxA2kL0goiviRKF7W3SV0kJtgChVhLWmkiOHjyRi0efyPTR96Uan0gZJbLYQLNAnaqW0V7XcgV2LL3ajWxWCymrXLa7Ba9tejSTwWAsx+dTmY6gLz+QyWhMdOBkPpXNaiVZvpG7mxtZLJfy4s1L+fyLz+Wv/+YnMhlP5Oj42IhIWlOtCXPk1SpJSezSEjNtuPIBihAxD21DcFTA9SP0Jwa5GSyt+W8vf+qeq8ROO5QsOvv+0wv5d/7od8kdePbqinnxl29uiR5sNmuNgrireZ04BIaApKRU2suGQ6od4l4mezrnXo5lqmbcxPZFuNqP13NmNUr2+dmxZ5xc2OcnHmKMTqKY2vmTTOR4iEctDyaljIelnB1tJEsgXKP13y+zO7mrLqUuc6m3G0nqYxlPhpR7HiMiwuEB3YyolEmUykhwKFacx4NkJ3VWSB1rPTg+C5pVAXQ5mkYymyuiws2xrGSz2pJYCI0B7D+Z/Q7l8tFQyYU7ux+YvzjgtSyu08RgaSqIcglKa+Gw1BJlCvUu86FUcSS3RCOB9sSyxaF24OE1GasaYTYsJRvEPJhZK8/0VMOeD7i3qALoyGQoJFAxK85YF67iNoJoVNFMkAPjHJGr9bwBe36oqQDICMN5S9MtkVTJEB9rYx8sCaYbqBegzY0ilJRmiqLh0CbfPkGjIkVW0swqX8gPiNnnAU4AHQVLabEnBOf3TjkSVlLpkDmCHYiB5UhdHDRPwWkBGmB8MMugslKilf9WEiyegPJiVGgRPdiCIAu+TS7L5VIW1ws6BOu7ney2udTDjBU0LFmE400+knIUmtI5P7r/gLSI5m/gVoBrQ7NDPqfGQC0xxpZVUOjtsN8sSZ9vCAKrUDyp5mXEeg0wncuYx4ri7KfcO+fgO6syeFu1gXs8uCHX19eyXq+ZEsDB89FHH8t0ik5L8EittBClRg6XxjH5BOh09frNlVxf3/ETnT84sTpwzZff3NwQSfizP/tz+af/9J+R3AKBHSwWRM1ovjSZTgjVoMHF0RF09u+xtv82OZG9Cob9z77/9/pVCWrvb4OBaeCzTg2TwyBzhzhB6iG8pw4UNPnhEERQDrN6ViywCnW9McqWSomgVIjGLY8/kkef/ZHMzr4n1eRUdkUu18tLLXcxed0YXmdVMfLFo0TXw91ayYgZpJEzmZwMJBvP5dFHM3l8MZbpYChH47FMR0N5eHoq69VSohwO3Wv5J3/yz+T56xfyi89/JafnJ3J+fiGffO97vNfjbEimtjZJqbgJpRC0NUaywmOHlx1iU+KGQB1x9QmVFKolOW0DIi8x9Y6dzoBuHQEn53g+spHPPnkoF6czeX19I89evZYXb27kn/5lTnb966sFSaaAt1VXA5uu3kPK7g4HdAjoFMAp7olyeQoK892dAqJke5/Lvtrf9POO3Of6z23ziV2K5BBD/h+sFkzXWSZyMhI5G9fy5KiQ8aCQB0drGaSNjCcDjv0vs1t5Xb2Wbb6S7e4O7Yhkcnoiw6aW42ys5GIpeCiPgOqAQY95kTQyRDlbljOnr/trLLNxI8NRJMezWI7mFo2hWCwv5WazoUxyubplcyeQVdGTAmsFVTRwCLYgriWxQCQRUxBbp6oDKiMczsDQHDgoeBP1mcBZjOSuHEqZpPLyZs0tE2ttC0gdTsEBhs+UDCoZ5pVkSL1QEEeri3DN621h9xVQtpau0bkFYRMRuBOrqVdicROJZvBboP2gpX+DoR64IA2SH2AQP7UMmpqSxuAXshlaCWQB2S6tIFBZANXz4OGllEs6f9OJpogilOzRqbfSPM08sERStRHwTqqcSIcApbOCFIVF14aIDod6OB5i6PuRVJBKNwEhO1Txupqq0D1ARZzgEEREz8AJKnZAlbRHBhyB28uFrFYLWd9uVSkWApIDI0Kag4kFoa+viq9w9NkzBqlfpkThENlewpQu0iJamoj0qDacc7QMCqkqme7mFVAgZbKmxMoVdW/qPjdRBF6Xljo64qp76b8EhODtKEHUst3RFvjly5eyWABq3FBDHBH748cPjQ3fcQrwlR2f0DUPIi+XV9xI6/qxvQ82ThXVgeOAtqNwDrRlpx6c3nTixYsXHNizszM5PT3lAu823h4967egBAoz9aKt/d/2WYxyqFFHwCasOc0mw+9OlnUOIyMb3mxKvQBX+cQEL9C2OEokRR7QNAXANp/PzmU6fyDZYEKiDJjMEYh/tXZUo4gHHAKWgVorTkq+qjIbcls43ADVYkMB+rBFJzZTdoXl+DuJ5OjkhH+HlAaclOura/n8l7+W7WpjiMJIkuNjqXZbUskylkkadMhoXDdDai4ceH6pB98r47HyzZaoT7YvsX77t3UU23tfxUWpMmwwKr6iyQ699gR165oOAenz5m4lf/E3vybpEMx7rw1uPXT7nN5ZzsoEOi6O33uHfCw1oGiBeyoGn5j331Ye6LFvZbA9qT0zvseBTWPaz4EDILGOdZTvg5sAnQPkOAEdq5MChweSzU2DRkTq5EBLAL+bFnP9rGzAo5A/FPpY0UFpilpL6I0EvKdqa04auS2bjazXK3kG/go28cWSkDH2GXT3RIAwQBtwQtPKp6Cj5yerATFMF/hYWdpFHVOVlyGPg1CdJnj8cej65xyxgwsReUtsZWmxCg7xeogeImCwaBfXnfQroTSqZ8GkddFGJlMRTKQ78Bmxpl0YzBu2OU9GK0LoEOAwrfF3Gt3rvK/VITCWMucTER5tpOSOK66ZjbmYDrLywxgcDhzAONTgRNQyYirEnT2NsNlvhCjEYUZuCUtPlU/DQ5PvofOHiLRxxvC53qAkGg7etuKD6ejNhufXq+cvGdTmm41C9ttCe81YMzjsj6haQFA7HENSG6kak4C3FvbKo1R9GHI5oprOJc3KM2EqHNZItUeMh6Onbb+x/7ZtpxH89ZUPbb+DI+bjr+l6Lan3tPu3sUNVdd7KGbhb3Mmby0v5i7/4S/nyyy/k9PRczk4v5KOPnsq/8cd/RKgUUB++sh1umrDsZbddswTxl7/8pVR1Ib/3+z+USCBhqh27wDd48+aNvHj+Ur7+6pmKy+DwMwIWPLu//Mt/IV8/ey6np2dycX5BmdjBbGKbsp/2v+UD+UFCfLv9lPZZ2yfJhzI0ohpMVF60U7ZyOM02wlgkY9mcyu6OsgkdJvwNIvrNBge5q96JzKZzGQ7H8vjhZ/Lg8e+ybj7foctbLhFbF6NzGZq9lJJh82VnxK2y4aH3XQIdiGRAAteAgi2ILrZVTvGYpWzlVhI5nc/leDqnl/34o09kfnQi4//mTzgpf/35r+Xy+Wv59JOPpNls5fT4SEY/+FSiupS0KWQEBUHmuyrtTU6YHQcNVBIP4xDQgzZ1Lz/IaTZPvA+BC3cwb99rhgU40Tuc+Y6r+exaToczOWdpn5YCbXZb+Yf/4Ify6s21lMVWvnj2Wr5+fWfRnbHDEY+ixryCcIi3MAZciGjCdnizPtOewUWvxlw7I1b6dxRKcTSrSzd0csX2kZ1/cmhFRysrrQ5BAlgd5MA0o5jO7TIn4UwFsUSG2VDOT0750YoCFTSVXF9ekuMzOwFyNyRTHeNTgntVaeMYQMkkAPZkYFTLwVahlYgu7+7k2bOv5Qr7zD//5ySrgqeCe/Tw0SM5PTmRp0+fUvxJV5Ruzg1ScSaVS0TOuBpekdPqGvDIx3OhPwDCH+55JnWUGa51uM4uqhew1hDMIE3AVFbbrlZLh+nEUHYXYbndT1wronr2K1E0DMqmsPEIQUMkg4lKDXfkb6wJJUCD8e/aAdQPQHtziDwxz69fhwNFzEq2YjceAYW2cGpYbwM4DRA9M3ni1VZ7R8QZUmSaGozinRJrlV8nM/JHMPaqp8DqBDSV4stnkmNOHGAsL/ay4rZIxQ5NIMmGLCV0+Cr5+c/+Rn5W/VQ261LW64KicKvlhlE8UqA4pxC1cw1ZhYtC9RX3xuPTE+V1ReDHxHJ3vZTVeiXpMJZsGLNhHCTj1Zlywh/uMXpnoIpB0YZ+czddbg79g88Cgn1GlFj9BeuEyJQ8dD00uKOK51u0Sup3SGu/l0PgpRFvM/wcdeDQk5/Opm2qAJH9NWDWZ8+1ExbqkFerXi5bBwOsztu7OyILm82urbvEJooUAYiE+BuNnno9yCyKw83A4r27vSOSgPswp0PwPlF9Hw3oVSDoJ23f9xBr88V9wqPlBv1dtP4UkrDQHECTF7CfC7JKARP59WAzxZgMh1MZj2dEBuIkk6jO26qELgLdZ6t3XQQULoc/D30E8ATAnh8PB5RwzQBXQCWuKChL/PrqkpsqCHKI1vBzRDiIfraI4tCmdreTbL2W12/esJIAWv9rCPwQCdGa6G+Whx04pn0H0H/+tyXUeyGXl6qqUI2/Zh8N040N1TAguaFq5tGDMx6Il3dbkYVGw50WRi+co+OmokmtEmJ7F/3/e6mr+5LFruzYG6+eSKoJ+PgQ9KWODzGLZjRBrG11PbqEw7nFIV5KZV0VQeKjk8o665iQMZwqOERY44iO0HkTm6giJZo6a8e4X8bsZVx1Ldvdjt8jvfX69Ru5ubkmb2W32dAxbXuj2H3rd47Ul+8rERppzt5P//MiTS377a9Do3W03x86pJsNiIOx5Ohn7IQz1MVrU7v2Ppa5deNkSWBXvg1/E/XznBJ19zPs/3HScwhUGqgTEbKSS23pDhImhJ+iPSVHRZRQX69kWFX8bKRgmwBtswxyIPzZ3UBJ0ZCqBhkxS5WLwbnCRmwgGtp8ZNoCOXNFMLQ3he6zcMAO13Xp1k1bcusImxEstapBr4edMXN059zJagl+g44ZpctVjk33VRevI8dDUVk8N8s2Ug1rmc6QTxBWvW3WW9Jjkh3GdsimXTjzwK9xwSZ+Xht/r8BQQmevj4ITCLHlojqHKLIF3qZXoAGjnoNKI/EGfWrsa/G3ceY+BEKgAIgun34jIRzen376qTx69IgH2Pc++Z78+vOv5PNffSG3N7fyi1/8kp7O0fGcXtHN9bU6A9buE/yDn/3sZ/z+Hzz7PfIAptMpB+j66pakQzgGienQuzdoFyVvXr3h+/zsZz+X0WgoP/zsM7k4P9+riPh2RIu/LTXwYVIGJKXwDlobW6/pdsVFQG1RJtPxhI7WxdmFzKfH8vrmuby+emlEHxXjQDYAh8z89GM5Pnsk4/mFxMlAmjyXEgiAidXgnjEnZf2+kUctILtJLfVSompH4aCj2ZzO1EcXp3I8m0k20mYli7tbuV5esXTwb37611bepgxxqBJSCIebSCLrupbb7Vbu1iv5+S9/Tq82X28Yzd0WiVTDuTRUoUO7W9txD0Rg1OHsKjS8Rpcvz1vXg+T5u06JUv/GlNXavzDCEEvBtEKAhCtCtJHMplNyY/7xv/vH8ub6Vq4X/x958frSyKx4IGrHwsQ4gVCFVEQqKZr99NF9E9XYd3SVcOpQMr6nwp7CB61uv1WbMb3kf6jlTgrBe8+Q9zbmgrV9rebeVXFwhXZEm1wWb5ZSFSD3wfmsZV0MJC9TqWQko+mEvRDW21Kq3UZevXnFNsYPnzxiwyLqRaQopbUyWxLBmKux9zaho6aQy8sbWa2W8uL5C/nrn/y17LZbWdze8D589PQJCcwnp6cym88lHY6k8DJMokJoEQ1oCvfD25pj01dhpCiGGIx9Rna94+mlBwQ3y0ayppYUEHp7b9/fvnqRCSrTUA2gLW41hQCAgqvAHQJVI5aEbXRtwzfVb6pcY9+wBkZA8ngWA7LWRL5mn12xsKdZwFQCzyg0RVGdgc3OyCht5011HoYjqJvCCex098HCwIE0BNSPQ6vU9TGfILWmKUB15ER2pR5+mnlpZDwWXutuB0Ejb7AAkabDxhQoAAeA4kcqyrOXRcPhTicb6zkm52CH9tirjVxdLeRofiRPnzyRElygBs5aroRvekW6uLabLXluOIhvrxfWfl7b0F+9uqJMe17tpKh2Mj+eyoPH5zKZjOTh41Mlh3MKQkJaHTA4ZkBtGOVTx0PFxPQ+l1KkiVRlJlWhDcuwVvo8PlSTsZkTnViXgta1VJS5ijX9y0oZ3Deye5EvThLm8OFF4SBHAxxEBozaUZqFnHbdUGO9X2+M6J8Mz7s7IgpwGtiytIFq34o5HS+/2Bdh0YEA+QM5HCAM+Hv8DfWtmVfZ5xF8W3t7dNVBtYcYS05sE4/32tbajmBCKYB82YiHxELvjmgiFvY9IEY0iUF5IUiAOKQ8SiY5zoRNqNFugKeL83QfyfL5OCyxmTIgVHYR6QegEAGpQf4WCMHlpbKGUWcLpUXcK6R4uKHGdAzWuCdFQUeNXRsNIkXVQ4wSL3IiVFJU25ceNKQdhN7CbhotuJCVoksG+PphQZaulkK5VKzzOPQl/as1oeLzAZnivoA0OGA3QxzmaOVLqdQ96K8TIGGEhDx8y0pucwU9ZKDPZfnmkHQRak9c6W1j0KIHHyDNZc6aq2GyogViVUUl2w3KKNUpxHzIGzBFEhXQsTDfldNQjQF0C88tS9WNb2qtTnAdjX4tta7rnK+BdX17cyPXN9f8iv2CURQqcNh9cSTZEIJUGTfPFh9om/r0BtwFx1otFetpb7Cs/8Puhq4bqzaxOpWDhhNqgmURSwlCWwkeFHL4cAD0Hb2/PSJoVhx1xSdOGdHDStHoFiFgTwKDMvT5TolVU60NEUhTEHWwWJj9C1COx49rmvqmuZIWkVTQLcF+bLwHfMV7blWjTJscMZp2VErTGl5CCaM8Mlj5cKgHIls4BFt3vQ93XPurpX+HWiGgzsU3Hok+tD9GrfwXQCzUUUBFnGoIdJlc3Y+9o6A3DUPFG9NjDL4KyctcduWOKBg61zIljLRACvS14w5wre9VsHmPBJu5Vnqu79cTybNUoE5RF6Iz2W3nR5vy6bvYOzoEXTjTtte91/IYX3H4Y3F+//s/YIXBxcUj+f73fygvX7yQv/oXf0UP/8svv+KhgtaxiPgxLQfDVHb5Vl6+esmyt+Q/x+Y6ooIhPvmzr78mmRBwoed/ydbswYA8aKpSfv3rX8t2uyGk+/1PP2XVwfHR3NiXB0ZLHxAhGE1wuGs+GIc9dPi1Ea627MWhBFIkIkKk3bIG/cgjOc5GUs1O9RJA5AIsl2KBj2Ry+kRG508lSgZSbNdS7nZS7lQrPSp2hO2Taqc66gabsq98FMkOTtpuJzuUIn71UtbDgeR3CxmNMmlEe9xDdwDCMWhS8/pmoRsMkAg0lGlG0hw9kGY05OOqbOSvnr3hQbBcojkNIDBFRcr5sZyiD0KWkh2OUsuSUNxhUcJ6tWIUjbFh3g/d/zx3R7nRgvMWfRiQn2u7GPYKl0lMskYi3Cy1not/j4NQS46sSoaNoRq5ODujIuPJfC7z8UgWmx317sFHSYdDvh8Ib3ig2Q/KD10xr4P6rLUxhUY0SuAaixNl3Ruq0QllGZnQ2iFrbaeT03pd0g40dY7UQcF1oadDKlvZIKrbbCXfWtkqvtaV7BoAwBVltisQqQgNV4y8Lq+ec8yLfC3zozm5J5PpvNsI27IqzedjztxY5dLlmzdydfmG6anV3TXH/xj8lNFIzi+O5ejomIgixItGI0RUfSIxNnfr0Ged52BO2NWmMTiwrMeF1qvZruf9BKyboLmvh9hFUss2LmUH4l0CQhnkESJpTIpcHQFDiIj+6K47ySIZ43k83BHlNnJX1OQKbS3lAFEj4IDqXNWGHlg6xQgTCeZSLsz3jwapjJJYjk9QkhdLnoAh38hys6UWyTBLJEtiWWwiuVlUPOCLDbOHsnTRLUIakdwtG64HnTieqjHxKOM5ofoBNBj2ZDFnBtmjQ7sdKrHRuRNwusH27zrBasMqoK4DIiNoHof3XS9RUaUl2ZDUh3MLATysobPTE6ZpVVIcL1LL7R04Q+BioY38Tl48+5KCeNhb0LcE3Jq4alhdRBVaqWWzW0tRYy9w5WRzVE1LwwM8/Me+GtagCBgPuB4FRJsiaFd4B177LNa3JaOuBCAf/bxMPyH4egfU5d3KDp3z29di7h2K7r14u0gV7aF6swwH2Hxj+eLLL8hwx6YNJACHD7zCJEHrWUwOkDtWnLOf/+pzlhOOxugKp1A1lKjgQHh/A+9H0Hm/utDBQ8D9A0qAjaTrK/92v/7b5Vn2Cru+Zerhtxs7YZlDAOUs9B0gP8AgO+yoqqymrn2MSAHlinEqk2ykyBfbIGu9cJyOqQiXjmbKAMeGar21sfKoBohD2RnU9h82myyOhMEJFlVRyg4H+A7EqpLqcnW1lrrOpdhtybxF45qb241GMtmY7wdILB4OpBlk0qSpbNENbaks3TVeC2VO3GxFMvAhsky2ywm/R+dFpB6Ulvb+BtgNDoiXrHJBenkm0hmQ3aWYFXaleykFJ+IQcen6aFgT6VbQiAVWnrOGwwbnbjRiBMXOndDYiBAt61yhkibU+lAOZzoEmoYwONKqHFhK6p3UbC7fd75hLarT8h56A7BX3aDvf7j5a8AZ0fIpIgS4X+gch42HyI+OMX4OVkGFDn6o3beUR1nsZHl3w3WLhmYgWmq+d39lOsGvKnOiSjfXV1z/b16/IjMcPyt2O4lHQ2o8AJUZj4YyHg9lgHrxAbQOXLRnP13Ig7wVenJhqj4/RDkeHZVxn1vi6MChozpBQyhqH2gJpKaUu4iVKDvWor0RHQJwogaRHA31SkBuhBPfbGp2XyTJzK6atesmGc1ue1aBYOq9pojcgKsoGUo7s1gejNkvXDbpgDILIIrmRUSHALwAkOxWaFqECzKJ4hyIgQUWeOGcJZEeBLtYT5/Lo4rWnP52z9nvgPvYYftp62D7e7UIkP3bglnuqThErdMozpkU7dsR1CA4MQ0dPB/lwpPpWLs9cp0Puc8hwMJaAKET5Yl5Dp0RbdxWQZQoVsEyZ/mzpwD2C/j5RCitc2p73RbR9+aj9g9SR8uJzkocVL0K3Q/AMUMwqOdDu09Y1cp3J128B7b4YnHik5c2dXCnGwbzIcr/kN9KG5YVDkcZS9N+8pO/kauraynKLSee6tBnshSRFy9fqg4/IzlINmr9Jj4g2uz2B9KlZr1/PUpHsGF9+fXX8pO/+Rt59OiBHJ/M+dr9s78TcPjtANSHqyvYt1SGLeEMYhaJqOeq3Uu8zCjmIkNO7Op2yTphOAwSzzgR0oFOnMHRTNJsLHOIBmGSowZ+EMkoGcpsAI8xl2LL5vXSTBJVfaxSCq3ks5Hk+VTy7VRWZyMukiEIiSA/lTuyoZEjrKpYinos22Qk1aiRCWqn6ZJqIXRNTXakFxAVbm1CmkQwo2yT24X+YQaiZCKfPH0ov/vkTDXcucBy+b/+Z//5e48pCFCKICmZRxEYzBPUkJs+PA9pXWyuhdFCxW2GQA8//tuy9X7UKjELvQrgvOkc3KFt9WbL0qNtXrEZDHLh6KcAlAs9I05PjuX0eE6HwV64/4V5WfA4NJ+u4wV2u6uikenN1JJ+JkQhpMCxrKlDGXRX8QqGPlHhENPXAX8nyyJJ61ziCghJIikIgtC3Tyca3ecJ4Wdo329LEIEr2exUznW9WJMj9OLr53I9vJLk2Uv2v/A+BXC0yAMBT2EHZ7KU66tLCmDtdhvJtxs+Z5Si6iZrkTWmF/OC+wXuORw+dOL0/UEjNe1u18Li/FjejQ+RtOZ3qf2Be8R0FtTvrHHMB9wL/q0nO5EqVwcW/jqvp9OpB2/CCYPKIdCDdphibeq9oJ/fRLKGoFATMXLHV7wW9gsSP60xHJo4ESCgdEokO5AAi0Ze327l9fVWyjH2dD3EHgxzzrUHc/BTgNSiLTAO+4wEwV2BDpjgBjRytQLJDgeVpTjY7U9RCVQ8UAqZDHghYgYHpfD8OfcHdVgQFB+qmQFxoR2JEd2N8sCeZwV1CFCarQHYg/OHMpucyMXZQ/n46ZKo9eL2junPMeTjUS67Xct6vSRSgnQ0zpA//KN/wNTX61cviSSsFnfUbxlkU0mjRIbTsSTDOcX1ZiczytKDf0XuLFNnyMNougbzVHsq3FcV9FRWR7j1MlAilabiBscEIm8gmMbJriXW6nqAQGD13SIEbf6ldQZ8v/H6Tv+qECM8KjygVDafT+Xy8orSkNApAGJweXXZlt6gbpkEFdMhV7lRrfV0r4mliq71bx4QytaYRyfsh2YdOR8Qy/n1F7/m7Pi9H//YYN6+Atxv2Cjv/fibTsGH2RbQQIUHJX39RGLQcAn3KfNYEV/I4WpEtlhtlMk7GFE+mFAgBUgGcnJ8IelgzBwqoglsGoDmOLHHqZRFInmypUOQQlIUsGSDVseJFBUEXFCHO5bNbqSIAhwPVAtcXUmxQ/c+/DuRHDIz6BGQoqzGo1cdk912xYoDyLoiBYAIsKo0/wtsDl/II8C9SsdsSQxxo08uTljPPIhRE7w7yCHQOlw95NkjoldyF1OBZV/0w9sk9zSMVbamjdLvNQkiP0CbzTCGoJOq/ciBeAFOR18O0hFiRKoQJULqC10YpyRqkoXd00pwc8UxndumzeBdRnGtIL95kzFrv+1Mao8wPMqiyJF+995j2V2Y8RsYjeCwjSWpMHdVfhiHCNQzMSfoHGH6WDc+jkteyHoFgZdctmvMDXAIgOIk4ho/vGfM26qQEwZwtwEki1IwbLhbHjAYOvwe6pnQGYDmATk1pqcB4zVaZQ73CJBlW/a5i195rN85BESOOCfMKWETHNPK6FVzfIgx/f2LnQwa8GuApqjwYdvmXKcXrxXQr7heAvU7+nuwXgtKIbUpl36mkoJLDOIVzrc0HTl3A0VWofq+2TZSrXN5vgT3KpYGEW6dyDFaUkNNczKSeJjKZIoUG6B26O4nsitFbreRbPJGnl/nFGoq7ZDfVbXk0DJJQBxUwhy6IMIpuF1HDC5WdCpU/wD6M5ipkC4oQYY4wFg2TQT5m/L0INkC5uf6gZ5AGsnx0bE8fvhUzk5yWT/csYz1cwSeZSnpeMSvz589l9VyRWE3dA393vc+kR/9zmdMfePzAeleL2/Zn0QaOKNQoRzKZD6V4Xgo0/mE0vdVVGgTLQpCIZo33QRPT7WBRj/NZa2/3BOlRLFCDJ5mwB4NQ4UTHNn+Z/e+Ed8hqbCbhL/p+/5POwNZI2E5IsiFk8mYGyQuvCUT4YZRjFs3a/zMvR1C/vYhAeEgV6svq7lS3DiOl+WM8F7YFJarlfz611/wgPv0k0/k6Ggujx5dMN/TJ/S97Yr7P/5N8gWHbgsJnADmq7EhQRbT63cN8sEqA3+UzGYoYJWSNLkMQV6DstVgIJMJcqYTOXn0mJrxo7NHko6Rpwa0RYUSa6eMCG3GSQWmNLcFOB/I91rbTggNbdA8Ki/l7nZDsoxsUOolUlRbslZRkYDa8wjRKXZ9J8TBwQAHACSvOue1UhnNDi0lMCo8iXHDV1QzjLOBHDGqs1pfICSHjCk2ccNFufgpRmJ3Cxu+KLqkuWKNvFuEwGqxlRjV5fb7TgSPEqtjp5mIDYlJJipDYRlCkoistGxzYrA28pHOa2g5lOZwEADE76gPr0z7tqGYVRa08Fu7cXhqo0d+auuRFZHR3u6HmUOR7BkyGkhW7CRtcBBZ/4ZKZYP1elwdDsqXihBoX4i6g1FtzL16Qv8OcwtaAwrHblaql4GNXMXMNBhQB05r8IHK4CsE0OggQVjHJNIxF5wz1FZ9tt87+XSfOOwHPiI39BdALwaW8vX6uPiIHGJFkUkEohnaZNtVtsmItkK1x2CIlaC3h/c4wukttK03iuoGKcmPtTYmpkUwz/u7ZI1MokYezSK5PoaSYCSv1pWMAYxAMRJqp8h5a4JMRYsbsO+h66/COlP0qZhrhQKZ8kQntCSVSJ126pUC3S+bRjZlSs4DHAo6jOQrW9oAjmRRyn/z5+8/pngtLb9WRVIVIbPDkWMEDhMQjwm7vTpBG3yzq8trotAXF2dEnR6cnhKJevH0McvmL6+v5HaxkJOTo7YMHj+HswBHhCJx251sGZiaxkVTEw1H+mowBp8ATo+qNnoqoL3NbYqllzoxpUFShLxDpFV++Jpvy5tZrNZ1H7VXeydi4XukDPpf73/f/WwfidcLRt4FeVZ4NFARg9CQyh2rpKRH+2AOs++BfeUB4x3fOEYzLna+hUVK3tWMFIwEjGPAmhn7Kvz1X/+EKmYgfV1cnMv8aMquZ45kfKtPvk8G/WAGh4BlI6T8Z3QIGAxC1xxjwlWjncAo9dsUXOwjSMimDdXyMJbD+Yk8+N6nMpzN5ej8IR2DDB24cOghn53htVXeEveCBSp4TebJAOEBLWgkL2tZ52jusZVff/lKZLkWIQcAHn3FSE/Z5QVFhqoSbX9x36w2qpVVQEkSZIlRl6ybAZwNDGHmnRJxn+JY5oOBXExndIgwIgM5rM88IThDBZhCssYwHG+iAWmvoZKql7WFyR4Je3GwCxSZcZGbat1+/k+dAXAICI9STx69J5DfHspsMqJIDkoUpxN1hJ0U1O4FlqNm62uOpaEZ98/y1lFpT/994mCL2Fk6Dbr0H4C97UQtOOMg7sVgT9dDqXYpSxCB0iXsJuiVMRrFbLZbq0bQkirkarlmPX3UE2ThpwEyYAJOgGKBJgBRwevhvsLJ1cMNojYN+6AgXTOeLg1ix16DgxzljCDDYvGqKE/nELjkc0+Lo+Vi6IHCw2IAgjRez9QKP6BtdgPWFNaA0a2ktZUjtMHgjLCDnJ/D+DDOaXHvQKNNRNpKflWp/q7ZOyWHgdDBQafAv8gcmtHQgzmOJL9I5MWylp9cFjLKaokHA5lJIlMQ73hMUGRcOQd1IcOmkRmU+JJGPmZs5nr7eo8YwMFng3hRr013g7Vn7ZuVA9FpI2CPWW0L+d/9p+8/pipXrM4AkDVveY7/2NoYAZWkMp8eMYU3Aqk5islpe/HiORHsjz56JCfHx/KHv//75AO9fPGSKMAvfvW5fP38OXkU0C9AxRsI7khlAZUAlwDKrNU2V6jEZOFBjI9HAxnNJlT4zAU8MY30VQZZZdu7Q5MlG235IAwoivKGO4XKjhuzz3NRUTNrNW5CbH9nZYe+GznAeP/AJamryMnkxGENL8vbCXt5WEuGsDIQ5AVdehX/j4hja4iBVxowgsCGz458lptCU5PtTso8p77BV18/Iwt0sfihCu7gRrF5wD264G8CCvo/9/zUBxgxFzuhXDAdJ6Ag2qI1HSqDdDyacXODfsOIB8uEUFQ2msvw5JFk45mMT84lG00kzkZUV1PmjkmutiQVj2q1lhhoAGRjl+stGcXbXSG3qy1FoZ49fy0byE7fXslmvWLFAlq2EqoilVmhVIW1dYAcdUHkn5Lhr5r8YMEndm/BVvZcsff0wxKBc4Be8xvjT7z3eBoRsCWZUujFGofwOjvxGvsDy9PpRuk5W24sLnPcQrm9MFP/WGF6QvZ7QbulIqBPnsh4mMkIZDfoD1j6q1+S6EiXvVXn1TPpu6c00kb9/us2OjTGsj2tLTvSUsfDVPUUwdLXcf2PyLrpteVlhGQVmi9K1NZrem8yQX8R1IR7qsBa4BpJq02atFUGKmGNcXKiFw9AQzq0eZfr1Mc8/Py+sMc9OAVs7WvVJa1aZZfe3OdgdpUeHp33F7iX9v52vtF7WIqWTobSWUVBp1/eeS/sFtjngjjnwdUDsd4sbY4OohxR0+rv3T1WIKAHwqbQRkNOy97mkQwRwaYgK6sywwbEwKSWXR3JCPuQqXvyb0wPwastvEwX6SONzinSq5/BKEaKbnXokVeGacGvPhVpzgN7Gxlh2crwrJmeDYBF5Xpu4LxQKfwlrxvt2XOWwSryjPnJEveiYGUbSOpvLt9QDAuHP1ApkFwhgIfzSBECm/8kz2pKbLfJ6SRgz8zGmSRIVWQmLkSAQB08csL0bqs2jM9xHx8jjHY31LAkK+dthcus5TErktjEqSth/FZT8rDhf7sb0Bcr4kfsHdx5vmWL41/84lfy1VdfcUDZ7rjNA6pQDjbHwsqucpS7tYdNRAQBuuj9AVSIKmE5F5XzAOlQsndnOudospLL06dP5LMf/IAL7+HDC6IIHVXsHcxYnIca64ZR8xqJTAe1jLNa5qNIzqcZGfhn8zkZ+JPzJ5KNp3Ly5COZnJzIeH4mo6NzqbKR5KMTaSCrmkCmlQoh6nHTEbDaaofwGDEZyaeq5dXdLVtIf/HVc/ny6xfsNPni5Wsy8TeLG2WB18iNQWtOBVl0v9RNCnAxnRnAwBFyhliE6Iqnylp8HkU3cCrDyWlkhPwzfk9EIJJBU8uszuV4GMnHR6mskT8+wPzgUmIWSEwQslKomURDyv52kZiOUd9R0uYoPFwylcZWmFVL55izcwVDSyO4NgGiXmZ92HAGmu7Q9B/IxclMzo8UHUDEwC5z7kHgtRBRwAHuOZ6UNe4p6RFF87yhaXGwzMj/hoCGE9H8gNTGWCi/PWhMvQQKyA6qQ0ZjqYqBlMZ+LotaKkS6zKM2sspT2VVAE+by5MkF2wZXzRWFhJa3qCwqJYbAit0nDr31euB7DIEioFJDN0cQuLA5A4nwHg+tk0N1zqGmo7JMRiOopM75HOcmkYCFg9MP3Xbtds6IqtRpcNLvQ8G5at0p94SkDrXpqUg1Re5A8XOQCDmYhg+XlroCORXPZ++ISCKQ04ygRr8ffQCGXRqGjisEosBq5/U2stlCtz+nmuCvXkIkSpkHePbxWPg4Kmo5G4jkTSRvNiV1Ck4hhlMPQXVWxWI08Ems06IltiEYpU6Iincp74H5Wz7MDVDHAAe0lXPyp6x68LbfiQwPbBg1yIaKDhKZ1IoenikcU+WF4FrgCEBgaLuuZJC+kevrW/4MFSvkTW238vXXX3Ou/eSvfyKXby7lZ7/4JUnqQLavr260xfZuR/+MrRswOZJKSqKTqKGMWGZb5pUMxgOih+B3zU7n/AoUGAjLrgDvaNfuJ/1utGodUR9jRd0OQ4Sca6T9YLRDK4IQIM8wih29Q0/pw6WLFaHpnAH73f0Po3kRFZ7Ah0XOBl6Wevu9yKaFbR0F8YiHXSo67YHea/eN4g9ew4x8JPuiA+rWfA9u+so6MSoZw+EWC9V7sOLfSjz8AHYEretpRklgHByIJGejTE5nmYwGQzk9PpFsOJLJ+WNJ0Ur44qGMj45lMD3ho0qHEg3nUiOPi5SDiw1hkiDH6pfOQwS5O2iLI++KvG4pry+v6RBcXl2z6gOTfXF3Q2JgsQEEW0kSsVdZFwlYdM35bxEKGzShtSp185WYBZQA+ULcczawAaGHB13W6tSTL4LDGockW0Fj4R4oX9pPr9+zNrPmZ0KnHN0jixn3wDaqrnqm2Q/WW3xAf4voVRdnN4E4JpmWxY0gmGPQ9/61OtRnOua9selPPY0mv9kfWteby93inuu90gDj7d07D7E+mZfNeeDIGFvcu0OixKosYxlPUxnP50zTbHelrJNEFjepdZ3zz66vqxEdeBeZzGdT/iwfDVvuwcbSW2wew5awXqal16K9TbQrKJyXlifQwqp+NLluyX0uxj63gI6Hd7v8xnw6XOypyYbSkIyZsn8GezsAKUDlDp0TpFj0vXSAFGrnAcrDX/tdMBCvzWW1e85eLFi1DJZENoXIzbqR5baWq1Uj652mDQnyc/3iOUoMZOqQpw+hH4mB7iFooeiRqyX2yi59GNqWw71OfC71bE6tEl9dw0HlexnJEmGQw8fUX4P5dtUMgbPdrk0iTVoCyMMzAuep07OpjJOCOVchPVqW8uo1tC+gQAinYaUCeWjURTK7SguT9G7ZPVeGdGlv7LMY4C1aKEP1kQ35FMnwLq89cLCrJG6Pwt78xHMth9ilujohr/uj966iZO8vXdyuECVxtOlWmolQ2InqPdsBG65Waw7qrfUagAelhO1WSf8bG56/l3aJ06+FwXc+2UgM80ii1PSB143jz/OiklsK7Izl669fcPNAR0QImew7Bd96AD7IJvsf/OEn8vh8Ts90hIN+NpdkciTZ0YUkw7GMjh6wJGswO5YYudPhWKJsoPW/OO2x+aGGH1dD8mrDshv24fZJhcVQ1rJabeSr5694DyDchPtw8+aK+bPteiW7zYrEw7raKBmGkYARH43dzoOchz+KBmIZZ9Yx0Eqi3CFw2fstyDiXaLmsvAN46JMH53QAIK+KJw5PT+XoR78rsrqTZ6++oujRIealqd4KVNELXHOynzaxaLpbxjYFYkS2SsaE6FCbXjAxFz+Y3StHyoXjvt2S2IYNAJDsEFUTg1SO5hP5+OkDeXxxpk4BDoHevWkFeBApOpzadkHsEAIVI9KfYQNSh8NSZ7ZGtPY+pkPH6/IPdSCapVEnNtaGWu13t5E0q5XU6y03OkjOQgCFuhcgA25BIBN58v3fld/9B/860bmb26VcX18RRUADNPS4gMPuvUzQuvzs5EROT0/kRz/6IccJKT8483/5F38hz54/5/qGM6/LVQmryPXjM0+mU65nlHiC56DBhzZNYoqjnQOdE9V3B33rpGJfpDniCE40qgMJ2XY73IegE1UnZ1Khth9BDJzmFhHQByJQHq7sBImx1VwWtAHgQHhqFNHv5m6ryCqbBtXsO4IUQF7UsikqeXFTyp/+bCvLHRyCiiqDI2v289WNqiCihHCBvgmYX8NYpujuWazlFJyvVSWzHZwuMNltHFxTv1aBLa3m6Hqi0KHIrJU3iL3WstK4dBxD7ZYonVLhgWJPebmTBrWXJCiuW4Jte2CatkCBklnoY7A8u2aIPz2ak0P1059/wbX85tUrliG+efWaWjbQxwFywM6ctt+1KUWqIQO5QXvJVBpwtpJMSrhcOaoucsnrGzqru02p2ganExlNR4oqGpFaQUtNwaIigXLWdA51zpY2HzjKrB6w3gju79I78cYGlsrr9XL42+wgHFGJT17+9M0+9t2/raSCWs26SPFg3sURAFNe2yf9f/PA9ef5e/JpvQoEdkozONdnHJnHhGFKbi5ACHAYKslpn2G8X07w3SEDbo/PxvL4fCajwUCGJ2eSHZ1KND2V+PixxIOJpEcPVAlvMlMyHOAiEDF3pTRwpqy8TgEO835Nt5wbDfJT6KiWV7Jab+Ty6pbyr8+evZTlYiF31zeyhbfL6gAskqqFBNl3xjqD0RGw3LtK7+rhP2gZ/Bq9ARXQn1iLVaRuDI3BQkLnLV+UCjtqG2IZjqXebWQHsg486gOtLQ6zChU/VFvnz5xFe3LXYtijGf8bhzN9LqKsp89ON/YyvuM4m6BJX1YaTgFTBeNhq4/QyvL2EIKuDNGjKa7wFm7tLtBoYi1S0/1Sc92oUgBSYGz1vbV4yJjq+tPyVOjuluSSsFyYh0G/fa/C9Pi848mEqRfEo4D94WwpdLt/bXQWJyMqOT54cMHNE4cynAJUJKmGu6YYPIInyc5Y9jj0SSaEkxxDHMYOl77jtMcD6MatyyD4waGVE4jSVWf+m/vbweM5GEqNkmBKBmsky+3HuRQYX8wllPHidyCGEh3BoY+ORHrAwW9YbWopUQaLrjpRLdUgljoFF6CW1a6W21Utr5eVrLa1LKgFIFKhTbKRW7k/woEHCoByQSgnQugGJcp1IWmJUmUlEbZ5LVMmJJqhMIU6yXuljkYmNBIuU0Qs29X7RqTHqyT87w6wuofSaeTsqJvvObwwNoHAQYleBugAG0UqTATNjMVyJevlSp6h/fF6LXc34LoBNVCZbPadsfdzsK8t9cP6s9QOP6chJuwBsc2lTCoZDsDDaiQlpyCVihoiHUXEEUpbcG1aUfcsl/Y2iXnnTLVzu+O9tO3R3+EYOwghUF9AcwbOauxbp6qGD6CesDoCOBxsI2lH4puLrSWfmGfeilb0SqqckMWyw1g3HDoLzHdqDTrYxiAaoSYcOcVnz17QG//RD38oZ6dnkg20rvrvws6fjCQZjSSHlObskcjxJzI9OpOjBx/RAWAO22RytdeB5TGzRBocTiSuGUSljQhlWTaS17Xc3d3IarmQq5uFPH99LW+u7+SvfglJ553slkuK6iTlTkbYYFinhPFGSRdbH1EohIIh1gJG9tIGKpSSGmEFpWLUPuQk9Q59uN9biYqNNCDxoDwMjMn6sZZYYgNrann95kp++tNfyUxyeRinLEM8xBAlA3bmIWHNQBw+hIEh3ONsaW4UjGSS71TfwhehHroayXcw8X6vBCIQRUzhndVqR8Y75jh4ApNxJidHM3n6+IGcHc9ZCorxRXkmiUOW/4OxpXELWVsxOu6NVUFQEbEnyarma8Kuj7Ah9BE6OHlP6+QA8zGEM4ComQo1OaoBAMXC0cF8tSgMXQnrUl68eCn1n/25wdoiyzt0MUUTm4LVB3DQlUuQshT54YMHcnFxIQ8vLvieXyyWjM7wPCrIGYeiFbSJtcQQjgDKb8EdQB65k61VHpIHJW0J5/3NytME1KpAVGVOD+rqt5FsN0j1H+6o9i0fz6QZDFoVR592XtgCchr3MqhuOkIAUR8KfJWyvl7K9YtXJK7dXK5JYluCGFfXMh0kMs5i2aJqqKzldlPJ7RLtlhuKFzFwSFxGXEcH0ecyV0GhMYnBjSzWhdxlIvMj8GnSVhlVk/96fACh4Nhx+nb9ASI0XBrouoLSqYqqKLLVOo4U4rLDmumuw/ZhbZIXfcMh8A6ybRkiD+FadgUE8UoZj45kMJyiaQZLH3dwdEmDALEbirCJ1ECfa3AGnNCnCOS+fPCQZw02ZdwHGh1LdHzWYIooD9Cvciuj5UiSQSLJUMvkByNFspznRFI0UxumbGqp99ZpuBfP8H2oPKuVXUDr3yWsfSeHwEG1vfy9sTd/G4amLGTN7SvBxxrE3GM/3r/w1uHx5jTtdfyGlAJzmtBPV0a5s0qVsIhNRyVUb2/uZJBm3GiwyaAjIG/AO0ArH8omR1hkmZRVJtVgJtXoXKLpuUyADOBz2GLRTmeKBDE9AGfAoixy93AMoE7Z+tbDm0d74vr2RtavL+XVl8/l1fWdfPXrX3Njm4BgiMnD3KtJ5HolglUA6BqOBDRE7QtlioN2E+JeTh1cA5UvdZK2adKD2IQe9PCud1u+n5bbgYinssqo40Werh7E8mTm/czf37S0UCNlLE7WorP3us1BHsLdPEqwkxlXwJnprfPpTm2vx4CjDw4mMfqENn8B0qwekCorik5wyh84PppRqhekTiXSGRnIKjXc2eN6MnKhOgQK/7VypCzhUnEgPsV7GDgaYJGzCyh9yFpZZ+xjjSEqxUFFhMA2KIwhNi7FPdE7U9ikrJCv1TlPB1qtYrlZHLh4ZCxRBCFswLLMOUszJ5r3JREZJV0anXWwfYfSIF2I+wzkAeRClYXutanmZO2a++h97ZwCz736HsP9xD4newWA8wcRHY517wUORBBLiNojteH5Y0ec+I9ul01MsVIXGFA3cHJKWd3V8npVy3YFJUc4aYXcrHeyK2uZQc45VVnjdVXTMdjugCJo0MBdAocLyxCtcyZgdqjdgf9DcSEoC0JhElUJWs7Nrpr3HQIrkeOOYERjHIjU/4Czh6oiQOl0CEwsiwqbmPtAaTU1yZ4hB3MI3FF3Make8uf8p/YMAdqFALKWIdZgYhw19isB0qHIBkq2U2BybLBl6QLuyxqVayUVAiktacYD74azxh1Nfr62kk6Js/gBqt4gXjRqhmhUQ04JxitRZjI5FnBwWC3EsTOHybs2tmQj+4ze+8Sqt4jCvMOQvh+p0PbJPrmsZWexNEjzHiAOYoNE3gXfIzJFVcHz5y+Yx1aP35jV995DP2dHTNTDEJPNcuJekiP3FhM2K4ridR6Vl39hE8X7PXv+Qparpfzyl58zevzoo8fy+MlDI+J8iMZH394+v7qVKeuyR1LlX0v9Mpej42O5e/nSmPFW122Ql9fV9iNUPQgUMsTYX6OiIs9lcXOrPSM2W4nXGxkiHzhopEygYqgtNFUO2priWE8Dd+aBEHiFQku06qV48CuKyFDfwFwB7x9gcwR6EGhFi1wcCJ2A3dAeFFE6ZNMHeNvFtTSXqSTHMxmfPezl+N/P6GjyMG2UGCSapybURgfKoA/nuPDz4BC/R241Zn87xu02Ylr3Pv8puSqyWK7l+m5limGxzKcTeXB2ImdHcxmjvwG6fKIbYK9TJTUN6Ob5uGqai3eDfABzFPp8Ajq7jtgYgmHX7kpnSvL7QMlubjRe263Oi3WD5gMaFosdehoUUhdoCFPJYovOl7Xky7Us8jc2h7QbHNUsS61cYaqFHJdSrq6v5Ve/+lzevHlDjgEO4C+/AqdkxaZG2/XGyGAd0Q/OHubXycmxnJyd8jFAYy1zklX50drJuvPmwuteRmefsXVmbW8DRwbpLEj+AkBkN7kPmEVkR0ikJloRml57Yt5Wdfo4G9gAQA8Y9ARZLzZy/WYpX7y4lhI6wlXJFEkF4mEcUymSlTLSULMElQhlBtlxketc0zt4dbjeIBCuIRZEZU2FuqE0CDXBn17m8npda7Oy6VCGw5QN2VStU3kYUFpUYS7T46AYkpIFkdlIaogcOWlPgwoXTNK9TZn/+reHTVjW/KdI8Vg62iXGbc4Q9eG91wonlQfHeizZ+fHk7EhOz85Zbj2bj63seqGlhqs5z7IcpPTFnQYIhUb/2ilTFQXJQTF+kZ9jWj3s6T2tgkLXWCXQKXm1QhUU02yxyHionAvcQ46rOt50VHy+tPoDtifZ5GQKih0lvfz0X0LKoO1o0LKw1Sihap68MzLBXr+6uqJzgKgcCx4EDfaa3vO63+5zO5zkv+Tn67VM7mjfWkpH6gCYsf63JmXLvGJdy7MXL+TqeiC//NXnWhY2m8jjp4+t1vnt6Yvvyr64upYH47Egw7p89UxWuzcUx7g9/4rpguF4ZrXWNrlMdVBZsyY8Y/rWRa1iLpCGRu4VEw6tOLUuO5NRmdMhAPII+A+vs24Kye0QwvaBA3pCRwBQrG6oBVAd87Y1Au2RFf0fNmYx8o7WvAibLcrdotm8TQsVVSG3t9d0EB+MBzJCb4m7WJq0kjh7KKPBJy0U9r7mal7MpbJUr8slM7q22nbvaNhGhEA5gJb00l1dROq10/pR6OAbQY2bI7qkrTZyc6ea5xiDKXp4nJ/I6dFMJlDXTBKTTtVX9Ly25SQU3nRpWpRHskzSyW8WlZkTopU9NshtuWEXNVtwYld+uEETg1AknQL1p6Coi/ct0BVvV0mOXgVr5YtgTuFgKPKVlM2GiAC6bvJzsvW5tZo2Eig+A+q90acATuRXX33J5/BncG7vtBFa+4FsU4TTPD+ayREcglMQEk9V+MifZ9AwG8swPaRjylbCppPRzpteubQ6BAgqEslQaof74s6XfBjLUZKHVKrl4J2l7ggBRlsvT5s0eQroNl/K1fVaLi8X8sWLG5Kpj8da9luZ6A+loysTMLPGQc0gZgOkW3ZGVIdgEKkzsOQa79Q3c9bSV/LTq1yGaS3zB5GM64GcpCPJIMVLrpKpxKZFqxZLtT6TMOYF44AzZjzJfCCeav2cpkKTrhQQlA9C5AcYulzihYBgsaG6dxK0PL6mfXAvNbXZVHAG1CGIk4YdcZ88/R7ltSGohiDm8jV0WDYk04LPcnd1JXWRSw31zWpn0bt3JcRVKI/BgwkPjsDFUocepOJaih3xf5V5LNCfBw6BqexivUEG3MqduV5AIemf7vQXuwZ/rVH90Z73jkFB+u75GX0jRPtsykHVui63hoGAI5AX2kcABDZ3CJg33GxZXYAqA/ZFh8ePydI5yb2ov3WVu5yu/9sV3npwn76/HlwJIz7tekftfCrSqcet7OtGnr14LnEWy/x4JrP5lC2Sj45mFgH9Ldso3/zwrfZIxpKxrVnFutlxql0KEW2iUUW6UV1u3wDZ1MbL4Awp0F/pgaZkLp30gE8hk8s+8eOxxMulJHd3fJ4gQrPQEupllCSGoh0auyD3174lDnGF2z3v16/jdqTIN1IXBXEomR3wmAOLZQ6FPhF5cn4is8lEnhzP5HQ8lvP5VC5wD46OZN1AQEUOdghwADjoquxmTSEQSm5FZuxw72nAt4JhZm3b1LZFXLSnieGvg/dcrNZyu1gxGkWDHJSQHs0m1DVXwRQ/+BVVaMsbLbJQBTrdlPulgn7Q+7X3iYQkoJmDrlU1HXrWFkXSh/wAHJmWeNE5gD4HoUK5y0vZbDUdEA2GrDkfRKlkMcoMSynRppuNr7Bx6sERA/7GBgmHHaWC5iCAX4C1DGcA+4ZHePfpRizrHEDKHOqFLvikDoByHZSrsUW9OFx+KikCAvceFQarEuIGTwdOA7oQJmzggxbii0Us63VMdOND8goRK5OvQDDLyyF7vRLafjCOXOgcZaOo9VbWm0I2uUaXOFhR3UOY2vgA1GZBhRXXckeoJfAM9IxOiaUQSJrshtYjdlQpNHUpr+52MnuzkWI4ltlIHXzC++ABoGaeB5YiBYhyveueoo4qooQ9Phloy3XtJKrpTpbhmm+LKPqgMYUz3jLsTRKfiLWeW+QWtUgRzgLVScH+t1ze0jnJtytG5CcnM5lOhnJ2ckzO2Q20ChYreRaLLNB/B3uvS8zjqDZPvE2RW3k81z3uga1bEFSJTphcP8iNFatGKskTVAdpf5DEOBjaj0W5G0piVnlioD6cIzw8NfXjzkdL8u8LsH1wDkGv5hEqgxAYQiSKQ99Z01iEr169IhLw7NkzevhX5hDgoNIca8m/d41yVVjyN7kf0phuc0ui0MH0RbInO8oJgVKdSGKDhogMJFBIM7IYFlRVyLaI5C9/8tfyxddf6uZRlvLR0yfyez/+HdNa/9s20HvM7/e0JzKXIo9kV5UyOprIZP5QlsuFvLq6cvHq/Z7oJuaC6BHpDi3WMTZ92qu0EJHZfEaiFZTiIG88ePNGfvniOTeUZodOhMhz6ya6y7eyWK/ZLKZCcySUzFkFgebcrRmHhcgOt/rhBRi+v1eTOGokMGwquHcXJ0cyylL5g0+fyunxkfzgyUN5iF7jo7GMxlNuYtelyFJv3XsbHBFAfDh8AB+ng0HbOKff+MPZdt7LQEl+XUKXi9kIfV6yyIODuTwnLNmcKit5fXkjz19d8hADSRXlho8vTokQAB1ABQacK4UQ9VqU+Ibyp4697S1PPT2oSIw6VbrgjTVPidL9SdivmmlRNSU6HDSmvBpXU7SWziw1xSHUCGHszSaXxQJlv6UcnY6oBIpug9DRwGFRgVxKOW5VLMTODTnsKgLT2pixAueicwggHsOSVdwbG48WYSFfBD3tJ5znTK8l2gMFKQigUNiHNBBBn4OIzwUkW+falRNrAY6wQj5MSgruBsZ6OF5KFA/k9XYk18WAHVTdI/gQumSY75QeYDW1O9UuU+ypg47Eqz+PWMJ5e7OU27uN3GxwAMPpVu2PAXuHNCTFoclQDRGwFnEzEbGoJmEQzgA6ciJVoA2fOnTZ4ediV8q2EfnFi6VcbyvZHp/L+eyhrgsruEerdMwLBGEUO0KLc+sHQilufo/f4fMS9yDHiU6DyTDrwYlUL2D09zes+2yo0t+YMyxBt4OXFS9jqNNq/T8GlIJaFRpwreXVKygK5nLGrrhDefr0glwzdERE34Ovvnomr1+9kb/MInn+1ReS4yDHPmpEXgqaASVhRK/BE9NMVprogQCIsCTZ4+D3scG/0dOiRDChcxj7++x4xkCRBM0MqULN7pLvNACCZdleCw6wp1E8jSXMVqPUa+72wR0CZfnWdAIUAYBq01UrmYjNACkBIAFoFrFYLlhXzoVtDGWvPeZr3iuJehsPyj1nel/AlSxnspdH92t0P9jIVd41kYKeNXJf3uUOhFItQcT1QjURAjKffPwRa0TTKfTmDY/tXZtfkWPH/ZKv97GbRS5b8C2qSObjWqasm4eyXKYyz5WmVVzGmekQi4S6yNY9RY18AXVic4M883g45GtpG2CNUkneMUiZJWTIH65Xsr69lQolYRVKvFQsSFmx/YCw301rv6uWpi+AJiBqyFnF4D9H6uBkds4GPw8fXDCvPp8fyQCtrVGSZiEEU04HOlrt9fk1WmR//161oh/9KdRHpdq/Nx6FL7oWXfD8nRGyWNYKTQKdY5AshuIevrbqgwSWDNmx3CGJVv5vxG/G1PT64U56t8etsXDO84StuFHb7GTfaT60rawNjiFRRrgjl8flq7soVh1WdHgcssKHaqC4vWCjEeUYSF0nrPmO4koKEPcIt2opVX/9dtkSVbm8fz24FuR1kZoEz8CbouHv4QTg51j/4LDgNeCkYB0wqi3RzMuaLjkRjo4vujCiXPSOCMFlfSyLZiIb5NJ76bEDl75WvXCPciqITxJDhhwF6hO5zSnw+a2cEYX4lYOK9IY+h2Q3Ruj2KsqtbKNzOhx7Ajj+1h0vDKx37gGpdqUsoCfT6xHAe8ADqSZ3QccPCoiK+nrD6Zh8kR6q6g4Bn9eVBBZASw8wvfdeSeKYSP81+993VUN6VjSy3a3l5vaKlSrltJBhNpQ5+qyQIHsrl5ev+RWdN8G/8hJHr2JodRisEVG/9JJCWixh9fdUjQAn21PPB/9GFUJetJ1C2ZgKAQSJ5BpEeAMjToMWUWQnCd2PsJdQnM/apn8nHIIGEB4U7gr5xS9/IX/x538lb16/kS+++JID7S2JoQvN6J+ysdpngDWc5hAw2rSIxdMNnUKh5Wp7i7/FD9oNYh9KbW91b9NnPqslH2mzE5bxYZM2LxFRCNIWf/bnfyGff/65/PG/8UdyfnYmJ5AG/t7H9NQAHe1bD2v+AEzjf/LXL2XRjGTXJPKvDz+S332o7Z4RqQPmfHN9zaYxSoxT+U2mmqFLbsdV28DDpC3HozGdm9OjE5kfHbX+CzYHtBvOk5SbMV4V6R2oFr559iUfo8lMtufnrB0fPHnMJlA6tpoOYE2/aePzvQE5VrUsUZJTFfyalwUXOx7oZzCIE5kfH8t/5w9/X86OZvI73/uEhDuSztD0pgS0q/oRLBc9sImMs31Vo0GZJLjaLgvUiZTw4EEFh4t4tChbl98n2cmQA8B0yspuupwv87Uo0dqxDwRmKcZ/OhnJCVCfCUSOzJO3yJ6pMp/vNl+9D0BbAev8DOcaaDtFg+lt/lvtt+ZJcUBX2oueeeeEB6wiYAfCLjpsSgxkxF0jHOPc1FJTPWgoFds0FAg6Pj21jpKJVMVOCikJH08GE77QYLCiA7UqlAXPKg3IG7gzYN0NY++L4eqIzLnrxglk79mz5+xVAgcfTZe0mZXQIUAO2IMCyjgDLQK5DfwlUzElLE6SHA40HFS6J0XxEF3GpD7+gTSTB5IPz6UePvhgaQNE+glJhUauczYu3xyOkyKACvRgbZhPEEOifchyTYw8iHhLVAdEIiN0ekTFBkTD0kymicgRm4sJeQX4ezRTQzxLFT2k1yy/jhfA73hc4Z9JLGePT2Q4HjCix893USqLWh0CV5IgbE7ui116p56hOjkgF7c9QQy1cjTBIXVDQ9doz36A6brqxKcoY2wNhGBKMOzOF606whjoOrm82sr11WuWss6nx2x+dHN1JdPJTP7Zn/638pO//qlcvXkjb16/UATAUiOa+tvngBB14WfTz4xY1g9xoARQgCXCq7xC84gUdQOKjfHHNWXDjH1rxs1YMiiejtXBxv4tLinu/WEopgbnQVESDSK/o5SBDrguLkBxgP2vLSUAw2JjPpF5O6+L1pK0fiTTMqz7kZynDfqyyGZ7zsE3FuM3axRa8ldb6ggYpmuI4hwBLAj8Hp8FOU50rgK3QXuo90iL7Xt37/MhiFowwONQFiwgX2q18CS52E3uRCd8s3APuy+napGktTv1H7XiLS1krBBW3D/oXKITTaZw3+AsbLdcED5e/XHtxmJfIhSHgpaTGtHQiI7KRtfrB1yH9AUiR0By2yq3OmTL/dmEPjSa1fJBzQ2apNJbn9cNXxdhv+2zdmOLxaoRCDEr0xJnu2OmRkrt+kf2uzbnATKCr/eusJfq6hpEKQLRH+O++9vl8H2deGmnf7/3ms5A9tf5AGIEGtd0yBucEyAcbflwL1/pVT1ehoUIEXMa5DwSTY1wioMoRc6ZB8M9foIdFvwtoyBf6+YsWL4WjgD+DafAUwN4AJVcraG+qRLm2tdBCaU1/ga6KEDIUEfPBlXqrMU1nCds5kAUUbJ8LpFMpYnm0uxJZBwezRIhoJ9vFQwtSMD2p/pZMI/hgCL3bGkg5ZN0vBc4BdQyMi/Vsodvv8Y2/WAanZircLhAZhuiC6z2WEHwBMh6NBshdKdcI7ogUsjcUgzKa+gu3DE1EE7dCffcue87+iRtJtZ2ESfy1kgB3YgDzNeSR+bOY2irpFjybCkYJyD7mhEtBYaqLfa/AWThJeJ5liUDdsy9ubkm4g2nuMUPe8JX3Vo0B8RPKKNeMcXCVIkhbZzH6Hto2gh2nwlY1bEqbUZIwSoXLqnsoGfvFAtS7gfGbXpdCbXvgmK/m0MQxS08B23nly9fkmihfchVmrh7c//qEKx3nyL300RFFN7DV6AIrFl3mGzv8PGNrf3JN/6tm4DWyytUah4gogBrM6u1WXEbaaPu2cvqQJJEOeKf/tN/Lp98/FQ++uiJHMtc0kwjlP1xkA9m1dkn8tnTj0iy++jxhcwmQ1muEbkDkioIEYEZjWZB/qmsh50uNL8eziddyTtwOqSRq/WS0O3x0ZGcn58ToUFXM5QdbhrTB9gupVovZRjXcjqfMpu3u7uRFGNVPQGmoLl37k9635AOAEfAW1V73S8u4wgpgGgiWyAFKHekPoEiS9e3iNawWFMZs/5aUZxuTLW2eb0xNvl7GvXwoWXA4Fqbvmj3tW5uanc25WewPTDdd0M+2o5p6qgoYci6OnJT7kraEKGuQJSFc3x1K9fXC86ZyXgk8+lYTmZTGQ9RQ9JDtKwMiciASf1iAWfZsG0dzKu0Tm2aBrD5YvhylzpQBrUKHEH+FusIcnO4Z7rRYcP5Jtz+btY6G6i0oqjQTuJiK7Fxgnzd4ndMdeWqMTAbQnnwSMo8k22E1rONoGoNL7TFxytRJ2/OKO6D5aPZSCvWVtl66OiYIKJXhVF1wmCQmdUmZtueGJr+nshIj1fhXQsR3SoHBhs5+gno3CN8jbWBHHQNwnMi2XYryXAn0UgVPA/V23cDERPNcEEydxgfRscHczHRUr2WM+GkSaiJjiACpeWpPBCwtUVQf4ikMHgazeEYXICTYgexwv6qZko3KKplfn4s0/NjGc+mcnJxSuekKCGek8rTz57K5GgqVb7l3EKkWrFcEoJJLmHcIRt+thNBN1cBvy66Apkujdf2OrSfxJFsWAj5/qYNfhxJ7VJuSAupBoo6U96oin1WiKzphUIDZTgaM2VwfvxAxsOxHB+fymg44RxcQ+dhh1QojnCUeGr12ng6oxMBVBd7AvdJapF0suet0J6lbJGu4ji2KRrb3aFfUkI/RTU06jzjOOEsG7CVeE6EoEJzCTS2KiG8l0iWaNt3goqePsG1fFfNjVhHarKlYO9SVhTtc9tGJQbTtGzf+y9gfbEN2vbUgSMGv/VGf8MpaH/TZoq+QUF2roHBqQqfdEkzJZNpZ0TcPEj7vnr9moppFEOpKpLDvCObkza7tPM3OQzvasn4SE7OLuT8eEYSGqBmbA6IXKB0RXGdnnIuPVofTIfBek4YERpsytCTh+QpJjnSB82pab9rm1E9ECGXij72BQlvOLgY5WJzLoYtsc4hdEd6sHHDYwWcBQU5PAclOozAoAyJ8jqJqZaom7kqSa63Wx7QWbaQdYZD3whFpiqJTRmH74b1uYciBErJpcyyoyxtRNMfP/foVUTNqyT6Wg8q9mTxsVUJOPrFMiuo6O1yQsBo551mE8nIA9EHyEEW3Ld5yz6XgOsguS+x3HOKbYftUAqNZLvkrwGVngZhak0fbQb1Q+Dctp7YghjdCoG8ObRvT2G/g96D0s3o7AaVepD+pJYMTDqgKCjzp8gLcukgA5scq5fYmsiUwudwJTxVomkq/V7RClwDSsI67kGXdtibGzau2nMDh26m+jqcHJoywINpHfSgx5oBW5x8j1YWphuQA4wKfdYfQp17R0Ewh9Up4Pce0TOy1Twz88ktDN9dCkoH8R9V8pAmiBqImrZlwOhOicPIK4hgg9FApqczmR8fyfkTKERGAlV0lCafP30k06OplPmGgQCDOLxrr5ytB0bppZgDxss23kJbc2AHX5cdMdE5rNMmJp/kEGtbAfdBtBYd4Agp0kLBRM3neyfMyPLtCBTR0XMyHst4OOH3gxTQEDhyWj3lVCNXPwQ6niYZHVZ0kt3Ts+k20b042asSfO36XkT5C2ZRECgrQbzMEyl4EDRKLkxE6hR7J6rCtLKGSt1I3dg+0u4H7rh9aIeARCkM1nBI+Bf5OmwMo9Gwze1hCZHFbspuXirkrRr34f9O7x0bt0e87QbT8/Z/88Hb0o5s8+uU9FrSGG48IF0rQWILSbZKHrIRhTL0M3p+L16+Yg4f7S3hbWVnJ/QaW03sPfLN4fbf+7f+Nfn06ROZon52MKCyF9qPNldXXJDH4yE93O3G0jA2FvBGtde3wk8wbizgS4BaAr5GiSgN0Vsiw9FU4nUu6zqVdYWyK1UUQ0lciiWOsK2ays3dQpYv39ChYN03dfH1EHXHydMOIMvNhiDcaOkRxnh9c83oEJUSQJJG46FM5zOmlv70n/85xx0LDGPr+hVUE0SaxljPcEoOsQJIU4JFpRUZToxsZ5BLbju8YnwIJ+eQoQyv2sqHlDCrf+qlrCp2lXADQKnh7VLba+PfIKUezacym074GEJroX1vSze0l9KvsFGHAzl01qH3HD2mF6HsB9gQ0QDFrAiBdQ5NmzNVx4YyWxYpOwfi/a3bzLry0y737ZsQSap5QR4JxgIDh2tNmqHEo6EI4Ph8RTQwroEY1Jx/eOA/ZLexQcYsRzZMzBxYrfVWFr3rL5C5bV39OiRAN39cl4s9uoCa8m3gwKatQwAkhqqf+J6lgKoYifkArIKy4b35+uEMhWZa9tdWTziHhee5EizdSWCqitF4QyngdIBr09QYHvok9R548EOJTxqigao4quOBtvCYF3S+0I58PpGLjx5aRQ7GA2RY5SjgvbBvaqdAhdSr3lzVQ7+X6+DvetuyoWGpo2JGnrQiWRsGddbRJsF1wN7X4Jj7XOwcAU/Qd5Lfuhw6tVxeUxPJbDiRJw8ey2Q8lSfnT2SQgYSK800FzahdgfkCsiw4GpMpz8Sj03PuYzukoQudy6oFYmulO5C4HqgGydeKW4Ku9yfgNKsrCxDgmBpCSyKoCkiw6mDMpIxUOG9ZywkAvNkT4YNoFB7fjUNgGsv4QO4UIIUAIRHdCDSXV/aER/TQt5yXbX99NKDjEihMu5dP3XMIfvOGtsf+1z/oJ4jbyA4qcbimIk3YZAc3UCFizddjs72+uZXTm1tZLJd0DKCA5tFkV87Vi/YODLz+0Y8/lYfnDwj3LBsIBUVys1wSEsKEQAc4bSgjUucl4UF8JKQC2OVN69KsGQrKV4zsVid0BthBDbDncCxxupa8TmRnrVLxRNy7jGVIA5YiFRjnl695gDBlYSVxTuDCJEPUi0MOh/jReMT33MIBqSq5Xi5J5GKb6dVKqqO5DCcjOhcgfym3pNPmp1BQmkmaDrgxouOak9Te1yhCg4iD7H73lq0ta7sJ9VjcliroW1fV0cF4MFXhRAmTOhuY90hx4IEqA0QQJBSOR1wf6G6I++QCGy2/havXnGWfuRbdeBqFlcWsClG5V7wX0B9srnq9uoO2ZWnScwrateBtmQ9zCDwN16r/oWTOIz13CnDAVVr2hHHwUkEiJE0q8XAgNVrsboFMFYzE4RDggAdSABVEfOVBY3K4HCs/SZg+17wz861EGfEZMT66ISpnRhVHWbPRGxcdGx1TqoDSCVDSI4lYeLA0zrYQBsJKclUdh/4cOTxxoPhYL9i28mpGiUw6q/x1V3aoanRMeSBdQPKs6+N3R6wHno2pEDI9C9Eeu36MjCta4qTLJkM5ujhpS2Ah3DU8mvJzMy3GEm4ndHd8lbbaAn5fOyw9qVyWxer192+jX6n+vJP/VbLtYaOKPcpLzvUS2lC+PWc49nttrXX8IolknI3k4ekFSYSPHzySLB2wP8l2g8ZvGryqZgUaEw1lNJnzLJzMj7gnxDcLYJztvdU0rvXisRRCkvTWDeSgvVqH0TzujjoEEJkiGIj9BqXwILljDuSQNE64d1ON1RwCoAW4xQlIlabCCI4YzoDviEMgFLrBB/nkk09kjTaod3fy+skblkesVgtOHvwM/6ZankHvLDU0lnBbLrUH6ylsBA2BTh633/e5f/DfJ/u58+A1u/0qhQ4y5UEDUShTSGTkQl4DYF0l06CV63KFUsQrbhoPLs4ph3mf2PihEIK62Mjt4pLCIZt4IttoyLrss7OHfH+o3WHgh3OVGOYBh3IgwNHcaLXmmnl6y60S8hS0bUXjjpzeK1pxRoORTI7OpIGGerVlJIoHc7KUPIWwiUHsgGBJtqwlgZhLksgIEUOq9c4gLdX5Tq7WWtYFmWSM6+3dnbWs3UlRl7LJdY7gdbS2HBNX88IadSl7nHK/QGRwHw4cW5CBgIr4gmzj7LZE0lruOrxoZX/e6MWfS0jReAZk8dsG5hsn9QOqRm4Wa7m5Q727yqXCIRhPRjIeQeoVkQQcT9+UZG9OtyQoT23pb22Ka3qCKpG2cWGukmjHUEo3mw7t0LwlBWl6XAQnax5uXb8EHTt1ptyxc9Ekj7k68mO990AUhLUIBBR8j/EAzaeGkhWlJHEpJWrnnZDszo31i0iSTAa9zDMdVeOzIDok6dn0GRxtxBrXqEwjJ27GLmlrKolIo5EE2ettj3lBvVNWrHRImd2hDzCauBY9dO8Pc5dwb5OTXUUsxoEBja6fbp4aXEyCpB5yKhWg8wky4/r6OjZwxDEvIVo2QnWG/w3HwVqdG1KlnL/onuPiL+dj7VPX/u0pVf69CZa1A6iukDqT4EOxPVYrrPS+ppoCrkbYlUcz1QFn1arGtNGZr0HV+EiiWBarlXzx1ddMETx/9oppADQ9wn1Kx0P56PufSL4rZbfNGTA+OH+gJbaDMV9nud6y4git5HebtXajrEo6n5DYRin4x598QnQdar3Yo9GADhwkyu4TtUFKTp3BorDeB8j7JLivIvEgJR8CY6YpA0WO2EUUxFA0VkoV4XxXe+cqg/FkLMPRUH70ox/JyckZI8Gr6yuKgKj+wIaCRPiwUChElLhaQSBEm8rAY+kTgvoZcc1PdbLE/YffvI58cZ9k2IetvBNjm2XfyzUWBn/DswNSoHXTA/5+DYLYYikvXrzgjfk+SuTmsw9WanTf6t1CLvOlrFFylZ1JkR4T6n74+GMtkZyMlMPAQcKBAJEdOAMxtfGVwKUODg5/fE6U+JDIUyrhq5JUCrTnGE1ldvZI4uFCqs2NRHUpKyABEBBCSSa68FV2kFSNrJdLyfJcZsfHvDcj5NIgecw8bynb3ZZaFECJcO+RP3eSDtP2mMBb6M+rB0unENsgUiNEZbSsjDByo8qKx6Osgz/f04bDCZ0Oj7NZG20qhVoV4PBc10WzzW32WcMGj8NKtviFp67wsbLnEy7+N9dLubxZUqkPrw0nAKkCEAvhTLIXfFu33SfLdiQoHpImr6vQtLel1RyzVh/GRBu6g6yrMvA2LupgG/rhKZkPUMrZ0i3sgHGnXZ0NfX0XceqcAodMraMjDy2kL5RISZXBNJYpytpQZpuXkqa56Yb451CHgDoFOKwo0oIGUdrQCA4QUjVM8cCx7CnUcdXwgI9V456OgW552s5Wr4NoEGW3UxIbVDcCZX64CkUTQR5DFcK7lHD9rUPKKgbgv+4UanVIp6GhA++te/UpumaQTsRDS8wqaYh+uuAVu06oAoCz3nkw6XwnLwrlwDhABpmMZzOZHs81n87OfIrW0iWAQ8CzW9Fb7xTYNV/qEAHdps0B8Pa7tjejFLDPodE29uYQeCViDYfgMCSLxNRaEeq20sU4YIpgWV8TSwV6oEcyXixye7uQ26uFzquVcp4++uT7MpsfSToZyg9+/CNN0ZSQJp/K0ycf8fUh24397/puyRJaFzori1yK3U7R1JMTErz/4T/6R/LgwQP54te/ltevX8nrV6/YZhkVIqqcqOX4XDOQYUZlAYI1kz9KIHFcoE8E0sHWkh3tD8YVUQNv0ubp2X7l0gfWIdAvuIkQ+Dg+PiZhCA+wyEHGw0IDVArHwB0CwO+LO+1dQI2CquyEiky8RglFqoet0bwe4N/QJGg37w66b9MO9y7XCUROvvHIxks9SIyi1n8hSQIinjXAyAu5vL7m5gHhovl2RwSBTSfujcWhoUJSb2XIzn+VLG9fyfX2kgS/IRjWg4HUx3OtwrCURZGjvA8cAiAbmsfG82B+oNRYCJhcVpdeQvgFsOpmLVm5lrTcUJ6TbWhBkmm9fxO6sLJM1CRjc6S8KKDD3UbKLYoT0CNhR4cADqGrT7Zd9nz029SZ0uioDeCwXdvGF5v1kAJK89lUnjx6yNf51c9/+t5jqg09ulZEzBVamSCBeGOtM2gxoqFdcWs+7/QgR8MRdKXUhd66nSbstELKYJtrZM5W1V25oXOp2+iaBKL+u3V8GVVB3L8eL09qnWDrWOSscd2bbVM2x0cl6TthIodLD7F7aWHd0Ft9/Y586Q9XcmTTKDD9uc6xtjGGgFxVYhubK+ax5luxmSdaOmdIB5r/0Im0SoBkMJQ4AwKED5nQAWU767oTf+omnn5PgR6UPII3BGeU7HLlvLTX20M9vIrHo1c4CehApwjBfpndYWaQecvPsGZs/YZxunL06fa2qAxwCJnETBwILBcEyuTzzO4BKxaUtDocDTSFhPwj9p4skXSoqF8JtoFJ6FJkyOAIKAu2KTebvy5E5WOwP79NPrs9LJDiQVmjrn1NB2uznlbjhfl53W/Qj+UQowgWv1P+BTkCRCN6zMe2Pt9RQ3PARYMDlmKXleyQnkuFB//Z2SkF1MA5wTRGk0TfJz0YY+fOwUCms6nUJSqxdhKNx3J0dCSz2Ux+9MPPGFwCKcBeiHmLCj0N5PZ0oy2V6ETTTmStrZ7gmrd9gmkakONrKUT1CxTlir7b5kYqx6gb1jG8neNTq9/UOnTkiQkfLxY8/NG9jM2Nrm9I0sPh8fr1GzoDr9+85vP9uUWO3gjoI1+00F+LDuyxWHtEq1Za/r5D0OnM9/XfXZQistfGdQDm1Tp4RLWaH1quVvLzn/+SbZJ/7/d+n3KnR/O5pCng+/169UNtkF/JDOSUupbPP/9afvr5CxlFsUwlltFkLOePHzO3CR0YNBl68fVLub255WaI3eD49Ew+/vT7zNNmgI9I4lJuRGIP8ALQgKbabGWyvpFytZTFm5dyu1xQLhMtXq2VipU7KVdkOoJewECGVv5yg/sJsuB6LYvFHcfMS8D6HI9OOUtzihhzTFKQb3AftKOZkhJx3Y8fXsj3njyWxw8fyB/+/o95yP4X/8V/8d5jWuUly9ccRvfARbMDFgGyq5tpZVhE+w2HkgphuhmjlBDGvJwJBpHZvivk9fWdXCJ3COg5RbtjEG7HMhikWtHhlQNU+FOHVysfbGp7R068R39fsEQwyswomoT3BrOfzpr3QveKA633VhUz3E+H6PegtAPMCu4s2mQUTodAv6ewCx9onKNd2bAnYFMstivcFGly9JKvVOwKPBa2nq1kyFRRLINU5bJx/0eUb4X6o/a3j7MRoftsPJFsNGFaIIewjvE2pEHPD96hDnVpUQzAqRkPC2zu+Dcdglr/Fu2EqwYOLQ5Fa10JiBi7I1BLCK5BYwUImhb6yYexjs+iDsh9MrpzXPRpnnYyv5rNcNJMV248BU+lke0CTaS01BcpAk7fFKjZQM5Oj9gJ7+7NHcuGB5OBjGZjkWEjW9moc5cYX4gkQiX68R0ipKoSKZtCyia3eWUIlHUrVHnvmOqqZa1NvOgQo6Q2gSPmeK2pebGPAvbfgv0uxtlQduVhFUbT6aztY8G9nb0ysOd3PAb96q3eDUGwnH9R17JBlRbULXc7GUWRPPnosfzgBz+UJ0+fyvnFA8l3lew2pbx+fSl//md/IZvtpk2Tg/P1OH1MiXZ0y0SQ8/DiQh48uJB/99/7t5lmwN8hMEZq+tWr1yzR9tbeft91f/BKOKuIMWYhwEGOOFM7xsVCOem2ECi7oKqMpGP4f5mjT99JcyOLVpiXNflcbECIcC1HlKb6wXCQaNle2vZJ9Z/xIC5z1v7zYN5uZYeDyz44y2ZYb201pAbztOIx99AAdwpgbY6rjbT6EG03MNyITRIYDPnCFNWSFF2+0JURoiYb6pcjjQCtdKOnf5gEohmIfNgE0QkLtf8N2sfawdzksVTYRKmihXGopco3UqMmmHm3WPLxWvLtWhpAoYCx4bGCwQ2vnOpVxmSGRjyUGVcL2ayW2saTDV+MYW/1wm1JsTHtNVLWiQpEgaWm+FtIa5ogj94DX3Bd9K/ojB5Q3qlPJ7eWe85nMx6eD8/P5dHDC7k4P6XjhblxiKmc6H5U0HGJ/f89YNmPnluZ7J51xFedM10gqukHzGPA1jCF6lRvgyx3lo4i92fUMYdwtUFsz8/tiU/1DnGPIY03tleW6I6vB2s9cKG1LiF3YETbJ5E75Bp/M7W3/+j4ApRottgaa5yfxRtOtblk5w72P2FHqlOCZXcPjULX5s/7fKSWAGz3j+Wn5CD05maPtOwOnqYbDH0hxG5NvfaEwD7MmOr7upDO/kC7g9qB7jYlrITPn4mmRtDBz8YoExYpt+gXoWkFKBYmqEYYJzIaDGR2jMi1ktX1kgclmuckQAigekfNBS2FJBLpVQUk26mMNgifBbujaqMoJ+JpJ0PrIMnul9B+0XLklsvSEvo8Ste/L5uSzgPQiaxOpfqGMuy7me4/TlpWUaf+ePoY9+dAHyVI0JIdUupszJXJdDyRk6NjOTk6kuP5XI5nM9mmUNzMSaxm1QWruRQhYEUFUr2jIZ1PoOgPHz1kF04GRBJR7h8ieAg8UbbtUv4tr6hdx3Z9nr5kQKEl3145gc+YRhmbQinpUNMf2nFWH79JmO1ghwBeE5vGtMxNO4jt/XxhgvgDEsbxkbB8A4zN09MzksoePHhIj+riwQX1xZGrB4qwXCw0j4L38KY4yI37V0Qe1l5zb9PxmmNf0AY19oEgZeJ7HtaYtGR3YgOAF7mU9WpFr2owHJG5fXl1w+jrq69fSZIMZTSayNzb+La1h3KwHQ2HhJDgUEHW92w6FuhjTeAgoH6bpSYNddfxWWfjoQyiGRuToCkJgq1iB3njTOp0wgMezoMSZVR3gApkWcreEn/1Nz8hcebVm0tCVcfTqYwHA7Z4xSO1v0HuC7ktWHsfKDoD4lyHQrVdIZ0xT9IWoN2RJCl6FHT5beZpo0genp3KdDKR3/vdH8pHhgx89PiRQrpAQ9BE5gArip3UJbgWyr3gfccBbSWmfr0dx0oZ2/YvJZx5pUNPVIQOjWls6KFT0cl6/epSLi8xX7RqY2QVOHCS8nzbOglsH+wlUVbx4Js7on7X5GjnlymaeW8aRGKMYPWXNsEVuQBzGabrQCPkrtzqQ5hjxspfoNQyCIDY0NHJkFLl2vKVcsomiAJnG+klUDwhrgXof4gQF9yIHO2QlRlO7rupXno1BSVljSpBHYAYaT6oaiIFprK7kD7GPoLnA3FsW6q3pDJNS3kuuUUO+3kf4zRhzgMt2hlMTrg9LiSebCUeFlJnfTr94cZqLJQGml5DK0FuB1SXVFJrHRiOsaZCpiNwoAYye/yAB+Gb5qVsFxuZXhzL+Hgqs9O5HD864WENB2Fzt5bb17c8vAbTkYxPp1RfXOdLEu5yD77sPZ1YiEMHKMGu3sqmXvPqUvCSKCuNaqdYsmbI56zLtWzLFREF3G/M/CyGsp/luIiIDcmF2lU5nzuswSuKJa8OCwY8LQgj699Z/C7oZXMCXBzVwnHiKeZHSpng8emEfKnHZxdyNJvLP/yDf02ePn5COXfwnO6KmqWcdYXqg6Vs1gtZLW5I6sY8A9p1fnYiF6cn8vHHT+WP/vAPOb9evkIfhDv5r/6r/1pevHjJ4GxHB87ONTgXVgraCmgBRcVnoGQ4kI+SQarq0yh3Z36MZkwDmZ2MZTRBWsOCEqYuB98dh4BtZY0U2Io/9KIpJxx1cJ2pE0L9aajPLKaa45hNp1pWN5noJmB17L5wXcrWlQy1rSVY3Vaz2SMe1r2mS56z9sXzzdxpx0HwqMC9YrKNAQtyc1EEA17cAu2c2bSmq/nvf95DTJnW2C7hxWcyGQ9k2KD3eCSR9YaAk4MNj13ZQCpB/pB11rr5o9FGlaAkUPXX2MaYClha/IIyIsC0gPpRBbBcr5n/1+5+VrbWE8fQaKS2+uOaiAA5An7fvfTGCHieA9cv3utASV/qBKh2AQ6EIcg1sxkfQAaePnzASo7zs1MlPtEhPAySpWPaHmD70bSjH33koONvuRvZc3h9W+6RvfD5tFpGeS6YK3jAXJ+ARELLp6rMqMJWbbOo1px05Ve599P2K1+bndO6Ch3zVIyMbvnm+1offN8PBHH3SYVYfz1tkfbhT7VyK6+iUBVRJZCRi0PNKidaOvqo81b15f17nZGuFEnRIjgaLKe0g9yqmPq9S/rR32/aC7p/q1OgaA7iAU291ZFxHIAUWJqowwX6rIr3M3feWuSI48kr8wJtu1Af097lWt0wKn4iko/RdwHlxamUW6Qbh1QYhENw8uBUBZeQomHJrHU9JIcAfTYgZIRIHaXD2hq+lfx1NIZ6/JGlhcBoMv6AReSszrDOhYjygRIk3Ct0DkaVzVNzCBIQKm3vhSMCdBTfd+2s3s98Dt5H0/w88ojb0SkPbbBfpUnKKrrj2Vwm44k8eviQDgH/jf4wqOxohRk1cAJBk43gUMoOZByVayhpZQo3Zdn6yemxrFfaV+Pq+oZpc6QKgN2qCJZ3tbW5a+lK5ZK02s7t/OZawF8zsE1Zdlgn2qGzq6axqpl3rDB6J4cAkZv3J8dXfl9azh+TKe9PJtuMTNzFeyAgL4MDF1492O0Ke6m2gZZNaP6j3XTsb7F4fNH7w0WNeGhZZEDHwn7Pw54NTPTvVGJV69QJYVoZjffLdl15/D65GfD5P//Zz4heXJweyekRPLGU8JtOssPTsxsKtSwI6z94NJF/c/opFbuSOpG71U5+/uxabpcb+YtffiV3q61cnJzIbDImpNWkiVyvFrL44istRUwHzHmTQ5DElEbl2Jlc5u3dQp49+5pR1QaiQ2BbI7fKvg6VlAkOt4LNTkA2hDytb9B7qRnL1Wr0pdrwWgJFiKDlByAqAdv+9OSIql9PHz3gQvvBJx+TXPPg/IxpAywmsHS9AkCVvt7f4AAxOjBZWm9XTHIb56Q2FdFrdwayM/47REnrhHXj4CFGgSvnEShnBqQgCBOtjJGMEiAQClE+Sb2GTPtisNKiFfAxYmKrt97BgbpuzAyJIOplqpUdk9BTNL7WTF2PwkkWcTKdZ2NyaNmhseBdkIZrBu1VmoIRK+v3TQUTaAc3OpQWWjMdPEZQBSTuhr81Geyq5pzDWELHASqVIEdBT55OAhvRgPRWSJRDpL8SSUvjEFScy0D33GHF+GLMHQ3QgARljbqPOFG5TSO0iJGlNyyPrFA4yh5raYpcUrTRZWVHT6LvYEtsC4Z3xKOmdUrdNXcJH/xMfRLA4OCaYI9ERA49kFRG8xGRwOnRSJKmIjdgAB7LdCIZtExQjbSEZj49MokztNady+mDc5GJsvt5IGOCmzKpLnWP8AeSRUD8gOVYyWvvkKVEDvYtVia4I2oQPsfSHHBz7FShEk5ELANiDcpEOFQ+a7VZEAkk/yHtysnb8NWFiNxJNDLq0fSIqQFowvz4s9/lfnV+ckbED/MHXDeWeiaJ5NBiSRJ5eHEu//6/928zGACvi5wFC1jxvnAgMB9v7q7k1cvX8id/+k/k8s0VOyaWxVZTLKY3oXtSoyWEVhlDnhPFsfYdfCcZY51QkXazkqou5KyZk4eBVBD2VE8hvstsfSeHAIetOwNoCMTc/26nAjTgBmy1N3R/MfarBNwpcPljzz+3BLh7Hk2LQngk7y1XvazIFjvf21QIvQTPn0tRDfudQ6gdCa7zHruqBtJHyXPAhnp5ecn3Wdwt+PmSeEyyRnuDDvQIkKuLQcJpaplOM5nNBmRMN3Uq9dVC8i9eymK9kq9evJKr26VuIsgZDXV8F5udvHh1RQ9ziEMYJVbjkZZxAenAJMUBjzJCkAHvFiowZLLCOCSrStnHzpp3rgjr6jnOBmHej7D4fyYuxL4QVj9kBCNEL0hHXBwfyfHRXD773idk23726ffoCHiNPuaEkhO9UctBQ6qHuKMY9gCvosHhZRu/7lYgSunftMiWOz/8P1fh65T4vCRS0TJ1NrEhAMFhnTzvgbaa9q9e9oX2sEqS9bjL3tuQCxXV+WbGr0Vm9tAti7bc7ufukVO8pzF/kLUBcT9VZ49e+1qXyHXBLG3Hq04Cy93wOdzxMdGcstcYCg8VNzL9B0cl6MzbuAE+BbO77Zmg5a7uxGHj5mbcHvxde2ltbmNj4ukD/4jWTRXHJk4+dBjlPfW9oyXOHo4O8FV4WLqjpt0p7cpa7oeOrB2qHFLP3eNaMH8VOgbBEIc82hXX21RSEFrR2RECYmhIhJ4j1FXo9ubBaCSj6USKrGRE3yJPvbnljXIA/adxJmlTShprUypUMPlzmAboT8kWnDNnxn9GR8EOZjZJVOIuHMm9v39PQ8oB5Go6NFaF1XYutYCjnVh2ebhCpALm07mcn57L9z/+Hsl/2KMwTojske72IKMxhAFVdd/75OOWSI/cPlBlls2iLDRN2U0WJdk3dzfy1VdfsgfQZgM9lorAHY7t/tpm3xWmeQ0965Xw9g/3Filg8AYBOf05UAnnMOmzgRx8R6RCVarTyMjV6PBAlzEcxPi+36muhTz2PoRuJs4HoHgRNlUS1ZSf0BKC+qQf+4qfAUnolxH57zishgx09ae+EShC4NrVRByYp1SnoW3KgjwQPO7xhAcrIhtENfg9VRlB3osme9d5iK12G8lj7/iH1Egsu20l61VJB+Dl1a28QW3s4lZul0v55ReVPHv5QiajAWWHTycj+Xc+fczPK+weJ3Kz2dIRWEJS93YhG0gJA/ZHBLbe6BiZahuXM+pV8XlHE+0Pjw3Z1a32GPim9oYcVQrHBOVjWjngI4GmRdiMgQY8uriQh+dndARAHjw9OyGkNh2jARKi7VpysrvBVlcNCj0ADotm/TDvasa7jc7LyfYcVmoMMDnbHgo8XLjwsUl5OVi8F62ng5Gk8MizAR+D2KISLkrrGU+iEqBII1XaQdXNabtCK/20XaGNZlTZU0V52lyoE3t7REQnbCFv63LwJIWas324ddAmKoK2spYoyWWQKSPbL1s7GTZs3w1SK9RB725uZJgl0owgBIa0kavFKUKAuaZtiEvO2xrVA1Ya54I2yk1xgmCsXRZ5UFsXU1aCaA0+O2pa59W9edGmDlS6OJKhKqz2SF26iSpC0DUyUrSjU975UAgByicHbWlhiwYYg7DtIunuVisI1NMtgPOCXD1EhviZbL+kvgGi40yKCqlHjOFUkqxmLhxO2WA0lnQwliLeMMLUjomd08UIvq141Cok7IZpo+Q/OgQ9pUqS2HBvGOy6SJxxW4zXwQ59dPz1P82QoKgZkHsr6vfehnGEmJAS6rrgz1MFmBe8HqwjKjh2aFFVl3J7dys//eXPOT8cqYbsOs48IgQsmY1Z+YXfYf7g3ECVAXk0prmCQBVBGdCv1WotN1fXMp6O5KQ5ltlsYvsIUlIq1e0dGEl6taoYdw489e6yy47C63qAngRQokRmR7ifSpzFcdCmINAX4zshFRrBz1EBDAJK0dD1ED+7ublpywX5Md6Sv7vvIHjk7sjBXiXAb/h7l07GV3hyjhQ4uoDn+O/dcfD3UohSIwoc8PCu4NXh+vFvODUsTcugMjdUpr6hI/g9xGbsQj7IxrDOc0L26D6228XMny8X6Mm9kuvFSl7dLuT6biF3q4XcLe/k6uaGG+YpnIHpSM4+/Vj++I9+xL4MRTaQXVXLT5+9kqu7lZRbbMYLuVuv5Wq50LyhL26MlzX4gBeNDl+zo2PyJbBBM0K7x9L3+4G/pWwmKwiGykpmBC6Ea6Er8OlHT+X3fvSZPH30SH782WfKuk/VQUO6ghUTeA+KIOhpQuKx5YwPsZaD3YZ/HQCrt65XY85NQxvqKCZrHBnCiRE1HShshNQUCJKWVoCnADEr1MWrQwAJaJXppUOA9IHBpVTDRPdFXfG8hrZNtBNejUegPRUUVSA0SxKGXgMbKeHwdT5hW16Le+LManWnHB6H1LUrFh5mLpGNuZHLrtpIAmcg0i6lzKliI0N3UGmYMoIzDYdgcVdLQXW1EX8+mGANaWMZF8PpnAItsXQRHUdSMGx7nACiNZ4a1HHE2MP5wp4AB7SPJnrFjL8m9wtzDLDGCZlzzvc09ltCst+nroSrWxmHGA4nkE/3nRWdrUpa7faY/powZ4VcGYWaVbnQO0PCtN4QXSLKKtU0Ax35iiqaUVVKhs6k2ZjESeVkdE2P2PzTI1LTRiApVhIC/DA2APa5aEhap6Hg1WgGc7POqZMOV2dAHQLQE3G91jX9IKOjzkhed7qu6gR8CxVjokCRdWlNWkcGlW0l99hfffE5X4tCelXFnD8CV84vzBvMGaQCUc5tgaQ7BM41wn7HFvIsF0Z5ck7CX4rGRGi3XYNwjGAYDaO0mRWrYNCjAI7LCKTodseyua1pWd+7vImg9wKazhCgKE8nZqdT02F4h+30nR0CfyirWJnF/r3n/DuI027S/i1rJ0W/VKiD9OUb6EAnEuNwrS5yd1D6h77/bavUNNSD3Z2GtvaAC0CjU9xUXL/mhNeaA0fziiyTp0+eEOZGHSnySpgIffLUoXZ9V0h5h8gITXKAspSyXG7k5nqhRML1WqKikBG0CoZjWTe55HUp6WBAyVFJM1nnpWzqRq7v1rIpSvnyxRu5ulvKJRoVbXfWZtWXppprMkBmGDlYLCSMxWq11DwiBWTUhXA5VK+JpcY5c/OAxFVO93x6xDKcj588lpP5XD779BN5+uQxS3XwnpRNBdkR424wuYprWK8+mzeuhX+IvXxzzfchH4X153CXLQZzrX9sXnrKqFNCPYJO6Aqm/AJTeWNJUddSVVO9mudTSDyWhrk75W+o5rluknukwd7EUREhaxJk5ZhaxOIk1+7w2WPLt5Il9lvrhKffarrG2dMts/6gEfUjCGa92J1kZ6I/2q+hYYVFieu0hmKEotGutUplEDdSATlANES1QCVjYn5qlzjr/GeHs0ZGvp6VpIr1BwcMv2ATKDrv3nG132/EDv0eQdl/xvvbpJSBBd8Gj4ibtPa4bxPZ/UH7hvN/+JiquBM+q6V8jCKi8xDRet8h6Kc2VOiJzgD2NFx/MqS+PrT3UakDnQpVu9V5rx9BoXTcoxiwNlA+ii0BStc2353j3KUqWjKpQf+OJHixGbkbLkrVU4htC0Qd/dLEmP4NEjJNQTYJHV8IUoFdciBEgHbF3O+NG9T5U1bhw2/18MWW5A6M84mLqpC79UIRGiPyQesV3QVZTlmgKVcicaPKkM5vQ8fB1Oaoi5epMJClAeJMLi7OtMwdxHwEpvla8hL6GfWeQ6BrqoeOQw4dlWDGjXHVUU9vtlyYGPcAgZm+Lz82nIPku+IQ2MHvELsTC0EqRC7PhVNcPrVl/fbK/vZ4AV4v7Lmlt6AC/e+dW+AOiTsGTkTsQ8H+M8CHUIqiLO7xsf4Mvavt94Ta7Xq8rTMHxmBf/A1gJkhOIvLA36jXdXBWlvb1y1yuFhtZrLfy/OWVvLq85aF8c3sj00Eqn5wd4T7LHD0kCKstuWkiop8enUg8HMoVkI6qkV++uZO79Vb+5tdf0SGAeh5IV/uLXNnh7nwtV0suAuS17m7viJAAAtbDRD1QbMKsHKD2OeA99eQ18k3YyOfH3/9ETo6P5A//4PepNohxw5gpSdBQICJAmKQmDWuRKwhP8M6xWFhag3K0A+znv/5C8nIrk9GIpEZN/QxkmI2N76DyntRI4AWYI0tmea8u2dIaMAiosBzIG93Ygc9DGjLSqPyQjGMFNECfp8/tNqaGEXvLfu513/NmOy3vxf/IlTjbPgZdfXHHF1A0Qdtbd6mKlq3cF4p6T9Ogry+zrEgKu7YlifJWcP1TKLQVdJJQToXeIFBtI0JQFXTO2NgI6bLViqgbEDo6vwUQEI/otUxswBSIzsEM6ajRSDIgd+D74DDfZbLZapfJflDhKCEezm+CeVSVIIKqIaKjfBrV1biHWbf3YP9Q/jAr31o447hhCrOWmFK1dgabI9p/TyeToV05iI4IaFh3PBhJlE2ppDccjKQZDmU0gJMmTGOlDXRi8E5aFjyfT9kvIkOFAeTFKzgE+C3e3/c2IzlaGguHDA57+M5wxLyFMp5egngJfx8blSF8+h+QLe3V4NRXD0zgBqDoEoECKhfqKCXBEA3SDrH57EhGLPlVonTXKEznBMiMHFPj17TONekctWzzgiiplm07wbMRyeDAYp5qC2g0GOLr2npm4MHU4JDr3/l2SDfiLdNkKI8ePuCeV/KsRG+XpeTFVgXSIpXyhgCankXgwml5L+bGaARHR7lhCFaVTAjOHlghBoZi3WCfsupYlkBiryu/w14GbvsEpm7zeuty8dRcDznQMqKeg/BbYm5HCLyywEmJzhHww9wjovsoQr/qQG+gRxAoD/EyDWt+MZm0kA8iNtTLewWEbyY2ACbbepg9e3Up14sNm2LcIN+/WjFfOR8P5Xg8kkfnZ7zmW3AdcWiDAwCGdVHIcrORy7tEPn95SYW4F9crWSFnRQ6BVn603foc5OwYVUo5MWIWJz6gWouETeaxJ/TR1XCTQY96+8FATo7mrHr4yNCAE3eceuxelaW1ueHUV9/gehwPXKuyaw+DuAGrk2yG6HCElIbske30/nVRn/Javumc9sfJf+QOJzqR7cne9jz17uztcWF6FQXti2GTxj1ijX3XVMqdZ80haz5YM1T+XsoBcW5E+x8jZqu9t2v3L+58va9pttWi7CSWITposoRS4VjCzZaKcidSywuxrjSyZOOWUkgAxt+hMgjzmHlYQ4jaVI/zW1xkxQhinhbEMQO430XQoqgU1CwpaXC/RJSKjz38u6XLEXXR0i1IIySpcwWMoNjxzr6xq304+WJ9Nf4/IjvrislD02+/pyg8iGJfAaSwIDs84KOJIa0NHtFQIjTaYR8GJSXq4a2vB6GhZjBSYqLdK23s41UOSnSM0ROhPdTNIfB1bIJEKk4HpwB/a5UPbLKMaicEMNCbwKNruKaImK0wU2jU6hTdb2tKJ7+/oWss28Kb+me/JFbfskPkYPppu0ofsdQX1/M9ASvlTNj8ivfPJIo8WbmfxQttiTF9LBtalmcaCjBAWoCgNSqQFJHF32trcawJq8oA18EqEhzhYBDBa8D7mmga8s493mu3xUXfUftj70Jhm5Pn5JXc0yqA76UF+rxcPx06lv++Q9ChO15fbVHOPXIiczV2PUreUIKHVxz0I3/8DPkf3zjwAPyCTZiNINDBEVr64zFhT3hiKg2LxaK76V5Out+xq/1g72//r//yT2Vtyn9oSoTH47NT+eGn35OLk2P5o9/5IcsAs5/+Sp5f38nNciWXN7d83CyW8nkcyZ/97Fd8rYITCB0MPQ+uTgu3ciOntKQlO+hAesF7J5tdK9YEJIDPNUi9Vf6ysTg9msmTRw/k4uxU/uB3fkeOplP53kdPGSU6ZIbXRuTGFrLuwmJM23RQxPwyhY7McaNHC0a09WR4X0OjLThWUVnIBEJTY52PZYSeFaqXT8KRRQ+qgaEIgM9YbzKDBjd93TxsWswfetrMWv3i+tlwKlOn0Z1kLG4cZpqu6ioGvKJhtVnJhpoQmkMfpqkcjdUp9fbMehBaRzvUkeMaemVcujHpPWfzK1wz2lHiPpB8V2iHyQMMpC92o0xiOcqG8iCeSCw5I2oEdZscuVKRwXjK2BLjgHW0g6ONwy0SWUNOGt1Q10teF+6RVrwgQsW8RZmVHnZRaumIoTri6XAog2HGZjwsq0U2G/KwWSp5CQe5lOUKOVnolXjTICA0qsRH5U9zTjmm1NZvJM1whCUSsZxxKA1ywhRfcqKpl1OqNLTKXLMJ8+ESxrGKLal2BKJ4J42ZYI4TXN2NpHQ8Hjh8Y6mSqcgEZYMjqdIhmz41s3OJmpGUg6Eezk0ssemSVAUi+0Sq44dKqhvOOA5ZPJSxQIm1x1eATgAukQcSxg0S6OpQGF5FiFy/g/hNIlE1E4mGkjaJZDLW8kO2JsC91IoTHHzqFCj3gWqKWDeULh5JWh8oXTyZs+LCgws89JzSXD8QbRg7kGJ+9ZC0Gmx9aWQ41n4XykVQUjCcx3GOnj0myYz+b+h3YIjycKidcyH7jgOb9885Uez4rpgIHWumFiMSueN4Yl6fdV9kx9KSHACIcGGmlaWRM7HGMQ+hUcO1rugiCYhRxCZKvo0pEmuO43fpEHi+vnuY597W9/b+wKKYdz00XZyhXQzGuu3/vhNxiPachL6gUb/2eA8xwMM4B3u8A0IsIIQBAu4xdtsot0t3+AUeWmmAQzGuK7YTng4zmQwyHrg4bM+Pj+RofsQSKwg5zaCVPR4RlvKyE9TsF+Vu3xt8a/1OJ3PTlf+YtK79bVsh4qQ7kxzmPWa3Q62tPz2ey4OzU7k4O5MHZ2dEUcCzQGoF+cW2m1lLljMm/L37p0I01ra1Xxt+IEIAdEJL/rTkqEuX9JCq9tm/SZZWv3Z/b6V1Fml6xYwT4/rIU/vK9hmZP/UqmL66zD0kC3OYuXcr9exXW+xd3lsqXL6x5D0lQ+2VTgDofU05H9IhBBAJK6FiCR2Sgu1gEUGR3Ih7CC/QoOY2NGLEBJ6AVhIwcnUSaQ+5chDJSz2VSOWpE0O8XMWR0Ggsaa0pFwwLN2ZHa9oI33cUQ4GsJLKN9S19wwZGRuLs6td9jC1saafSgQhBm5vvOQG+Xhwx6L27zjOfDHBOwHhH+suaG7CKBW3O4RxYy2b2ElHmu+tssLGXH1rIcTegClq1lr0Xjkp+1xIq4Vxqe3oc/urjG4bQYK9M1CkgogAXy5FjT3t4lYESNRtA23CrItcvULpiRYnv97d+makKBGH9KorFUlybxzo/NHztOBZNixB0JYBdyk0DUF2TvDOejnOxIidT2mf1UsJu++nutvJjtGKrDULUHbCoP2ZApl0YrdLIzRq3+XW5FDzPNGrq+P7QI0d8Fw7BdDJlHmOQDaVkz2l9q+16o4x9kvOUoNLHWd0v6IJThxk6Ru03eLvtxt1t1u0m6N28eugCPDVHB/AHuBZMiPu8Bxi/RhG9RScaOSmR/wYkDAKHl9G0evEtVvDB7B89fiTnZ3OySs8ePJBjHLDTuZyeXnCygKgX5bk8eXgho9GAfRWQq8dXPNzrxVcICbGM0ssrcViRiaz9DN5qvg8DDXCHz3pqjyHKkWibTxz2P/j0E8oMP338WH7wySdau4taXYu+qHnQSkR7V8PeWrB7BqhYSWP6b2Xma75dyXiHOQQ/+vQHcnE6J8ud1SZWP9wuHJMx7aD7jkBY90orAVcDsVAnoGto5DC3zyseikUpo4kRFg1Bw88gwEVNhkxZ7hCPsSfwjeGANhHK32pJs1odAtkfQ21frNE+CSV+4/qEKYSw3PTgFINEajwNbHDmtB9iSD6xbjoVGQ0TmQ8yefP6Sr788gsiXK+vb3h98/FAx3085diRZxKP+P1sPuWGtVmiZruQuMTBUthmGkmMtJ+tUQTLXIvm38YN+iGU0hS1RXvKM4CjjqOM1UDjEZ20ETqwgssBR4G8ikbQBJAOnLWVbdBsCaVzyOeyw6oOLUZ/MMTGiv0B5cC4B1oipslZm7dyuJWeSmGGwqo4rBkYTHPYTOwZmVPbNTNahI+FfPJwwmqXuFJm+WAy0RJEE7Yh42ez0OlbNNKAp4GmZqjoQH65ziTKZjLOTvZFyLzcMMJcMsQEh2wzkhRaxyTU+lw2pxmvhXtWj0UaPBwJ6BwyJjFbkSs4BHDgsAcAVUjZy+UQQ/UAOj9i/aL0FesrF+yJOOg1mvax5SnUTyfWpiaKQ5oBuadSNdh1oS2sL1Sn4fOxuIfESa1U0u6KWqHg5cm67yjUT5fIZNFBToQDpSifo7q6z0ByGOkPaDSQZO3nKbsaQsNAg1bsLdjjtJFdJCU+L1R2d9ZLgh0xvyOlQi1V04MT7Y/Ho7FsR2N+jw+Fm0qlPyy6PdGU7rxvKwZ6aYVOqLPHZL7vHNxzCPosXUQv96N/TWN0ZUd9WBqHZnIPQWgf2BysdhueGa/PuiXq9RtFpksDH2SP51N5+uBcJtORnD95KicPHrKb23h6zMMVbaVxGbPxmLD2xSl+LrJYrUl+UQ0HFcPAEGPyYLI7mQ/tY9uo6K1X0CbH25wrCVwgig1VbWs+1Xv86OJcPnn6hA7Bx0+ftogRxnG92ZnSmbl32EjpPfccOlPZuq8Y5+jMoVGs2xFajE5m2h+gl3d2R7StN79/83iP9/kT+ne6QTDnzzlmyIZBuC6zq4hCN6pe7oY+A239vGmUu/kYWii/B0K30bFdv+4JXVipa6xLaeH57XF1r0rhYLO8JV5MK0uwjipKe8MxXTkRqy65EVfgGACVhkM1wOkOeH/MzTBOc0aFUVKwtNOjK/iIMcmE9z5/r+EW2Nl125Ss5OfFsU8CIvcfVUZE0x/s66pRaeqv1giJuQ22Xi5JcITokUb7Gk0yamMZmgkB9VyAPQ7IgW4BL4Vr1e+X56XtjtphYyvH2gVrNYULBsVeYkeCQK2HGYMZj+DxWSFZ7GcwujsWUqO8kw+gUVC3A8TfkSpdQ4ByQdhjGyWNg52CSF6RAK3a6ta358whrW66g5YG0ZbjHt26vD0cApQ4WgRcp3qwHjKmhhopIOXRfXevnL3fBur8sPQWRZHfnmCYkzhbErnrKnTIR6t02SKNfW6d6VnwPnZv2LZANwSIFV9W8uotoVUASduuKzrkZcrKLdL76zLxhtLX0HtsJCoNxfAmc9+VQwCdZ2xe/EqlpqlGj6MB2eloDAGkAO2OcZAhaqdH7k0Cdc0pKcU4B6pg1XMQfssa63Tlpefl6STEgGI5tyWRvQjOCYjajSrSr5Y7JkHMNBDag8pUwRQdBKRlb9srqeFlfwBS4b/1b/5DObo4I0M4nUwkHaHFciRb5rp04iE3/fGjB1Kcncj5+bmsdrls8pLlhuypTXSmlLvFUh0Ei8CdrMWxsFy9dtbyiERTLToZ9fNMpxM5Oztjcw+kLCgkNAOxciCPHlxQ3hPtoHXMQYozURce/picnePHMepptntXL1ibcrqvTMmPfNioIoUBXkifANo355e57kXrmDCaVaKUVgui3FA97VapsMcNQHUEHU9zErQcT9njRGkoiLJf9ufvpcQh7UxGdCIxVKAtj7JOoi5KQoh4n0joDk7nYHTAuN4gHW9ch8o2v78lUSk7NGuSSG7KWF42iVzlA1k1U8mRr89wuEMLX/sQbMqVSLylGiUidrDHx9MJP9twOpcBoiewscGYNx4EymsF1TSG2onfI6xV1HSj4oB6DMivomwLOVnt78DILlap2MoOAa90h9tQEiVoJAcqgEAAeXWUfhVAcowfwCqOSJpUtUZqpm0SacYnIqO5RCDwEeauJQN1+13o22+xqsrQ8smib00R8M75+rGDTOeP6go0hOVtDaGuPgMBsJZ8cSsFOBtbNGtDHjwl0oEDl42vGpEcHIICI5IpIZGGSBcVIa4B42lFTTco4genBRwH76fRguzdWqDPqH/r8w+IBTgamo/X+c2xRqm3q1TaQYw5Awct3x42pqhITRINNuio5ztbi4ZoG1nV+UPYsLjagBJHfkB7Ca8/vL+LrjvsiYjKSYg2fRAoGeKrr2+m1chtA0hizlUrrKWywt4YziX9XTbf+Qt0km1t6F7eOUtwdPGR4Hhh/uueoSPP8sSp7qtAGVTy7TtwCBAtImXAPH0UaxkESFRxI+PJWLb5ViWNd1tGFDk2IVUB7XrMtFuWeVz7AEK3ob0temvTDf6jXuzTY6v3qwva3Kw1RqIAEh5Wn9wiCVSY8uYRXiLpjozplrXUgQ/HIfjhZz+QwcmplHFK0k7h/RfQ9pj68IClIh7OeMuLi3PGCDvrdgghF1QVAMa+uVuYnkJuToA6BOwEh6oDa8SjNeQGa2+hHa/90/E4OT6mfgA0Fx6fX5CEOZrgPqcyGU20NaiVHGkpU8+ztnxan/LhzgcdE9TfcqNwtro6Bd0tdIfgsDziCFUh0ItgFOaNtrxEqPPsnMPQtQq1VAGfbTlPO0hjwKuE9+yD6QDwgQOqX8WizXlsgzUypzuXvRHSPCDKBFs1uJ4eQS/zrRevjGwis14e5sviHrLScl8MBiWx0Dg272tRpP0H8CrrKpKbKpZllcquGXAORMlImgiO+JYwcVEBMWpkZH3pccBDBRMHGEpmeWADbbDeIzwQ0kJqq/qB0BY+AwIM/p5KlnDUte47rxrZFJb6ManaARXaYpJYSyMCuiQOrhHvpyVzKHXD3qCvk2OPwuHPiBv3Q9NENZ2QVJrBRCIK+GC7RAYeqR1PH7y/1ThkIyBsurfsq7y2A2/7nJYBK3lbNyZvcY77XKIFOtJxOyV5otMo5XPJetfAi/MUnVCJqLiENuaplrj1+QqoGGBkWqlgEFInIBSqdKGtEXMgvGLMo4q2VDxRmQQ4Blnqa0MxDt83tNJTnYoI92J3GEJAAVVGcKppgfUJiB+Rd9RXAW25YzrY3gW3sYZlMG3nbObPhxOPVARIm94lEw4HOvUCgfYqmEg7t7rqpqcnyHkxBVR3+LBXoJwRX11AawBHyiaB6kRoeWY7P5iXQLAKR1q1QIiKmS6Itl9XpLuq0u/GIWhhT6vxZUMbI19MplOOLhjegAnRovHq6spqjXeEtcnMBtO43Q671ED/6286YjuqVxdptZHo3mZsLVp7IkoYJEcIfND9566S6F9hWrKHz4ZDQ/a5BHqh+1/f0+4GKdsdY/NkPbJBonx3eIhW2QFngddl5Uj0rCmsAZLxkFwDPMjW5gHvJDaTCTVY2zUGKNeL9wRsaJ8Zf4HIGuWDSBuMJ1PNX0P4BNKe2IgdXmW0fC+baoefogL2OYhQWI7WdFA1KrZ713IL+l8PM9bkm1zpfUeyK9/ze3q/Ja7vbBq12cDwvmCj0br+VCIiAhplYk5jHDvnUCM6tAHHxsGqANMk6ORHHZXR+MMDQkW8un4HHRVn/9Dv1eTs/caJXiDuaatqUwNsdfjfz+ajodytlSdyKyMeMKvkodQnUF7LJZ6uNDdaIwfcle1BiAiVBovJRF4mZ2STT5IR5+0u20oVGekX8x7CRYk6jNqUBoiEOrCMmKxdrXOEiuFOP59dY84yuo5zoZCxlrIVkZYlF4kiEuVIHREgDVh32oYRDkDEa9b5wtNFZPJYmsGxxOlU2zenqZxOBlxDhxiFfK0cUNNORknn3G2IGmkKY18wSJU1UXqNXLHqjKSWZiDvAWjpVtGsMkukGhoBFFWfPJC1HHG7Q8DQSDyMJR6YmJXOIkliqhYwiUWEhloFiGZNg6PHM3CEUQmIyhSg+1pEst7pugKrXhFJRV9ZmcM8uyKCLJ1LVC3wUNNtqCPy6kW5I2/9KqxrrzuTXqVTkZuiY6p9OTTaV7KzkSi59+l+yjMD3AJLeTKvT1Eok+5u0zw9584WrIqx6RxgMAVlVK98wl4BR7F1GnQ/ahEGchn0GjyAQDIHThily1zjhe1RvyNSIczJd/yKEg1o6k/HJLYhfbDbbSkGBKQAeeebm1u5u72V2+aOteGtQ9DTLvg2FJ2WjPgbShHdIcDARRYZ+yHfTxnAvPTkPvEQz+mqFbQbIjYpeLgeSd63QxGCG9Tzk6+iSn582AQiRAwyGb43NTMXJ8GqQoACAiTIhpr7t8jGZhyjHW+0QzJb2yzAyFWav3PFuQ7k1yWdN+AKI6pSB8OhcCXKtC5u/wa1P2MUxs57nWYE0AaPgNUZ8Jr7LoK3bw4aU4XZMAp+2PdZ9h1xSvNz8o33bp1VTxca0oBvWNKKRc/GTxYhFCUfzp9whj8OtcFA02usjW6voJcn5obbMev9GfuOkj6znzL7TUMEEpJqdSj5iYeNickcYvPJWOSqll1ZyHU9liVqzpOJ1GcPlGxVKeRZ16h4AelJnRyWFOLnaSpFMqRjNIkhNw4CVCF1osgRIHztlaeHuhNVy4SxpFaOeNUBnHkj06qQEwiCPn7gjXT33Hs8NJklWkZd4MD9wku32zwRnPB75Od0Lk0yYrkYHIJRlsrZfEAJ2kMM7ZUryfSQNrQSsLlQMbGROkZUCz/FG99o1AfeA9JBQBHXW01/otQO82OX2+sADSlq/lwP4Uh20KxhFSGQCZTnAk1sJEGEOVIBLU9BaUkb0ijqsBYl5hTSBppGcfZ9p5dh1nO48VyUKyoT3js5+kGFvViFyvCAv5A00cHCRH4Ne/90oM7LfY14q5oVQAa8b0Rt7deV2Og8hIjl6YogeNdTneu6x6k4nmoxMFDGnktnouuuyOvoB7Cu/GvkQ9U26YSU/Dzal3ruHAJ1XiywNZn5VrCM60TTEv1U1Lexg4qT1VvSGmtcIFrb4vuTk1PyDOAADAZDPtJ0QGdhcZcqw3WHnItD9RovOfTcW93tYdW9571UwT3rIwW/kVT4llLE/X9rG1C8C2rWYV7K8rb3O8gw/03hi5/PXe+21bAd5NSsgdKdsd0Jizn8bo1ReqgArw3CoDikKNOpTNU2B9569d37KgLSCe24rBEnIyIWk8jVwP5eft4JUTyMtUaXSmxwwg2+cqGPdty6Gqp79++wIf361ZUMn2YyBY/Ac+wO3beDrFFZ751NYMWt5x7Z4utLblMrwbr8YfNkqsE/mzXtcbKh11/38f8ug4Y1pJUJjlB4LlehTiMyQosiVRVE1THwG9c5If7eMJfj1lSOyfUeYJ88PJa7LTa6WCpU9CB/35aNAi1RCNWSSKr6qIOltfxe0osNFZ/BCG8xIpykljhVB46SSm0fe5wUGh17pznfVNGBMEvwnjVLntt1aOWDvn76PJI2P2zwvKJnGrH6HEFkzta8nXCvSDpiE6/5ZCQns5F8/PBEHp3PpS6Usf6+djGHfHIqyHxgOlUohWhzzppOUpndrixS2fz0JmQHZNYq3BKkbsmL0nbb2QCVLaokOhii6VkkJSLOGjr+dvibFgAkB2JWHTonRdN5SiXQiBbIAK6PJYveEMicNkdtHMjq1rA6Iip77qk487Ya1H+oQ8D5EUcyQsv27WHVMM7n8W2U5HCTSOfeZsiop/Ac9te5E7VkwDZIATpb4jzxcw6BsLVC5/pWJJnthr3xGz+rrXHjJuHlHKFmZ0rrxqgFDbpvekM+mCMOup8a4dQChE7i3++X7q0U2EJlgpHiWTwK9CNtvhuHwA/bvjk8p9EQRGUqmYyn/PBwDJAyuLq6ljeXlyQevn71imTD2+trkvpYLgj4BpG918T336B/gHyba+ylC7p+C+U3SYX29T5C4IqHWFiA7NQRsJrct+RqDz69AKlhW3LYyJrUUHHOHC6XufUSGSIF3jnMQkv1OD2y1WtCVEgUoBeBe76v/STWMMOp3p7I4fNtstL5gNwxoSmVLm6Pcv/4ViPtJXmdrDQkVIFgdPoCvIZ2OnfEEYfzD83D/Mlf/ULmJLuiX7zmAvsFwRQh4UFrcrVOxkmtsZDVJ3t+lNB1rmWtaZETbUFJIcJwbInDgebH/foR8SJvXhY6D5nvtWtzbS9+bFvELN9qCaqW82fkq7D/arvhvJzN5jJjrbFJCPfutX8+OhGEwrUtMNflCMpxh5Vy/tv/AB0rb+SrV3dyebWSm7t115ei7WOveXiFtfXg2Lf9+4pGLBx3Y2675M9bnUXXOHEZZ7QGchSlddT679HnGvX/7akYb9LsR78ic/obaxhk69Lh3+89OpXPPjqT7z0+lj/80VPJt+h78v72e49GEqOEG3B8haoNfG5UPsDR8AOlv0bsIEOwQqKoSPng2HhO6rCzukdMwDB1LXuvVrGUSqx8GE0X4u9UKc8Z6Vr149oGFhSA7wANgpbE6uhLhwo6B8IRNt9n9HDu/a19llbSnvC8tuxFN9ZDDOXwZapEYKz/AUTWKOlrfQkszeNBlJ4VqhQaIUpHHt+c/codcwgDkayHfgLWQIqBmiK2bEaEYBhOa9KRhnmmYX+lZgfks/WzTeOpZJmmF+A8+HnUJ9O6FL93GO5Qly6V2TrHCcT5wJsayRAlqHZuKOkUFSfZh3UIfNNBz+dvHMwW1WiuUg8CfHDdFJXhj5rNPoufN4EDD6iwY8C37Pe3vHf7dnv9DdofavRsEqfMFULwxya3pzncYcFgwwHAze1KOWoyor03AjZzjWqglGZ5ybd0iVguFm+9zm87puv1RlESZ40bOc9h51aVvj2FnWNu5DK7B61aeB9+v7exOnKyd+tcm99yW1454cRJlrZB952l+FBIM/ERSpX2pN/sXXbmELRtqOEQlNab3kkyTtDpiYLozzWihrLkIWN6h5JMPsZtr3XfvFqGAPUaNMp1S1P0UreeGPYzJ1wC3ULZJz4P5hi786GpFLUfuo6dSupEZ8yc9xalobgG6izscRXUYmzO99IJWoZUyW4LolIp6w2aoIArj3JaQ4L6fUJsNOkUWp8BOAaoLIjTUkaVyArJ3APGtNxtpSm3+EakxPdIEfhc0YNVqyDcEdhX8mtLuPbef78kDPX+BmO1ZV29p3YQfs8576M2v/Ez+Ev04Nq+U9BRmdURAIOHwkSeuiOZFFyJrTQgSxYb2eGe7NYHjWmOlrrs7NiVrioP45t5dCcS+gMOAQJ8lsP7Z/KD2r4nxwi+JviPLMHVQ1+5CVDmU/6EsjCUkKROAYSlOkRL14s6ef42Opb9Ut7evuEjbiWA/RRv/35344b9BTCEdsc8ZEx3223L6OcB6h16wWEzzgiNyIEhHAbPRdQosZRBOwcdE8FeAL6Qcllc+RIPsvnjVKq0E+fTKiKcQ1jH2l0XX5keYVl8JnVqMuh2jd4XyAcdr4PAo+pLe/eQL+f0pXEqdYrdATm6bl+lUBc7Lb7D2m++hX355Zf9oDA83vLAGL2LhTENYxrG9O/HI4xpGFP5ezKmEf7vb3Ma4Fk9e/ZM5vP5BxOP+ftiGD4gJ09NqOfbWhjT32xhTD+8hTH98BbG9MNbGNO/2zH9Vg5BsGDBggULFuzvtx3YritYsGDBggUL9vfBgkMQLFiwYMGCBQsOQbBgwYIFCxYsOATBggULFixYsOAQBAsWLFiwYMFgwSEIFixYsGDBggWHIFiwYMGCBQsWHIJgwYIFCxYsWHAIggULFixYsGCw4BAECxYsWLBgwYJDECxYsGDBggULDkGwYMGCBQsWLDgEwYIFCxYsWDBYcAiCBQsWLFiwYMEhCBYsWLBgwYIFhyBYsGDBggULFhyCYMGCBQsWLBgsOATBggULFixYsOAQBAsWLFiwYMGCQxAsWLBgwYIFCw5BsGDBggULFgwWHIJgwYIFCxYsWHAIggULFixYsGDBIQgWLFiwYMGCBYcgWLBgwYIFCwYLDkGwYMGCBQsWLDgEwYIFCxYsWLDgEAQLFixYsGDBgkMQLFiwYMGCBYMFhyBYsGDBggULFhyCYMGCBQsWLFhwCIIFCxYsWLBgwSEIFixYsGDBgsGCQxAsWLBgwYIFCw5BsGDBggULFiw4BMGCBQsWLFiw4BAECxYsWLBgwWDBIQgWLFiwYMGCBYcgWLBgwYIFCxYcgmDBggULFixYcAiCBQsWLFiwYLDgEAQLFixYsGDBgkMQLFiwYMGCBQsOQbBgwYIFCxYsOATBggULFixYMFhwCIIFCxYsWLBgwSEIFixYsGDBggWHIFiwYMGCBQsWHIJgwYIFCxYsGCw4BMGCBQsWLFiw4BAECxYsWLBgwYJDECxYsGDBggULDkGwYMGCBQsWDBYcgmDBggULFixYcAiCBQsWLFiwYMEhCBYsWLBgwYIFhyBYsGDBggULBgsOQbBgwYIFCxYsOATBggULFixYsOAQBAsWLFiwYMGCQxAsWLBgwYIFgwWHIFiwYMGCBQsWHIJgwYIFCxYsWHAIggULFixYsGDBIQgWLFiwYMGCwYJDECxYsGDBggULDkGwYMGCBQsWLDgEwYIFCxYsWLDgEAQLFixYsGDBYMEhCBYsWLBgwYIFhyBYsGDBggULFhyCYMGCBQsWLFhwCIIFCxYsWLBgsOAQBAsWLFiwYMGCQxAsWLBgwYIFCw5BsGDBggULFiw4BMGCBQsWLFgwWHAIggULFixYsGDBIQgWLFiwYMGCBYcgWLBgwYIFCxYcgmDBggULFiwYLDgEwYIFCxYsWLDgEAQLFixYsGDBgkMQLFiwYMGCBQsOQbBgwYIFCxYMFhyCYMGCBQsWLFhwCIIFCxYsWLBgwSEIFixYsGDBggWHIFiwYMGCBQsGCw5BsGDBggULFiw4BMGCBQsWLFiw4BAECxYsWLBgwYJDECxYsGDBggWDBYcgWLBgwYIFCxYcgmDBggULFixYcAiCBQsWLFiwYMEhCBYsWLBgwYLBgkMQLFiwYMGCBQsOQbBgwYIFCxYsOATBggULFixYsOAQBAsWLFiwYMFgwSEIFixYsGDBggWHIFiwYMGCBQsWHIJgwYIFCxYsWHAIggULFixYsGCw4BAECxYsWLBgwYJDECxYsGDBggULDkGwYMGCBQsWLDgEwYIFCxYsWDBYcAiCBQsWLFiwYMEhCBYsWLBgwYIFhyBYsGDBggULFhyCYMGCBQsWLBgsOATBggULFixYsOAQBAsWLFiwYMGCQxAsWLBgwYIFCw5BsGDBggULFgwWHIJgwYIFCxYsWHAIggULFixYsGDBIQgWLFiwYMGCBYcgWLBgwYIFCwYLDkGwYMGCBQsWLDgEwYIFCxYsWLDgEAQLFixYsGDBgkMQLFiwYMGCBYMFhyBYsGDBggULFhyCYMGCBQsWLJhI+m2eVNe1PHv2TObzuURR9N1f1b9C1jSNLBYLefr0qcTxt/evwpj+Zgtj+uEtjOn/74xpsGD/SjsE2BA++eST7/5q/hW2L7/8Uj7++ONv/fwwpn+7hTH98BbG9O9+TIMF+1faIUB0APuTf/bPZTKb0jNerday3uRSFqXkeSFFUchicSv5LpfLN5ey225kubiW7WYt+W4rRb6VQRrLLEulLEu5u11KXpRyvVzLrizl8aPHcn5+LpPZTI5OTmQ8HsvZ+bkMBplMxjPJskxOT0/5c/xskKUiUSNNI4xc4KHHcSRpmvLfuJ6yquDGS43cSBxJlia89t1uJ1VZyQbXX1YicSJRFEsELz+JJYljGaaZIB7C8/GIolqiqNEsS5RIXhSy3m5luVjIf/cf//vtGH1b8+f/u783k/FwKHGciCRjkWQg66KSu20pdSNSCd5TP1ecRDIdT2UwzESaSJomlrpupCpxbfr54ijC5XFscM1xVEtVF1KWG9ltK7m93UpTi2TpSJI4lcFoIFmS8jOnUSTpQGQ4wtdEJvOJjCZj+b0f/7EcH1/w3pZFxft8e3Mp48lUHj5+yvfmWFalLFYb3tff/wd/IL/7uz+W66sref7VV/L6zQv50z/5L2W9uhNpNiJNKVLtpKkL2exKWW9zaapUpBxIVTbylz999t5j+j/63/6nko2nHLdYIomaCJOED4xmwzsrEmMg8CxGvrzb/E/Nv3bf8ku09xv+lb4LX4g/iOx1901fnxO2N69wv/h3fIZF4Py3/l6/6rt3EXr31X+mz+1fLL7WnEO1RJJvVvJ/+Z/9R+89pv/J//k/k/F0op8PwbAPKZYDRgDzlyNhkTLnHxanjjbmomAN2VrEf2kT897EDe5RNwL+//oxY0x1kbgUW8l6H5tEKsl4v4oEv8PY2n2tU4lqXEcldVTu39fa5gDXj3At+L2obLyBiuhzan2O/V4f+FkjddXIdr2S/8X/+N3HNFiwf6UdAt90BqOZDIZjqetKFi/fyIvnL2WzXsvi7k5Wq5U8f/FcyqKQpiyBNUq5W0lV7GS5XMpiuZDT2Uzmjx5I2ohkdSNN3XD7wOvv8p3cLe7k5u5Ovvz6azoA09lMsjSV2XQuw8GA0NzxyTEdh7PzMxmNRnJ0NJckSSTL4AjEkqbYmEQ2263keW6bih6W4/GQiznfFVJVlaxWeE7JjaCqG4ngDCQJH+Ns0DoW+JvBIJE0jXWDrSPZ7nIpKpEky/fG6NuaP386nsgwG0oUJ5LXiZR1JHUVCX0ZHO5pxs12MpnSKcgGmaQJHJKSTklV1VJgvHEz65ivm2buHEU86KMolSweSxRjoxvaRmgbeIINF5skttuGnw9beBQlMhzBGRv9f9v7zy45tvRKDN7hI9KWgb2mm002F0fvev//v5DW0geNNEPT9hoAhapKF95o7f2ck5m43ZzpRl5JbCIPWV24QFVmZJjzuG2wXi9wd7dEeajR1C32+2dd034YkRUbxHGCNMsRpwmSvNDvv3n9lb6SKENbtdpk725fIksz9O0Ow9Cg3D2jbRp03YRhcJFWCdx40TlNiiXiYm6pFIMNv4cRIgUtiwI6Bz4x0DliULKAYLHDgoj9lQvidiu50OKCv0tG7Qf87zPw6SweY7deX4HTjoeJHH/+p8mFvrvXY9Cy7/azSlr5OhZhXR5xShp8UnAMXOOgMznwf3zS8ZnndLleKlnXWbOYrK/weCzufLjPF2FkKEc4DojGEcE0IBp6nZeQn0Xfx7NzFCieH8/HWW6k1wuYPE1AmGCKYvRhhDpJMIYBuji1Z2WKgClE1CdKCqazhEAh3l9zn5fxWdblsntu5DlTQjD8+YRgHHXdlBCMI5LYrtN1lHJdX1RC4Nc4QkGAwXS33ePDhwcc9jtsnh41S/vd73+Hoe9RxKw4A6BvgLHDZrvD0/MGcd9jWK/s4e96TMPgHrJRVTs31qpusC9LBeUsz5UQLOcLZFmmB7UsD3qaGRj5YC4Wc9tktTGfNj1l/MNgG5QCHrsA/udCPejWVQjVsRi4cXGv4Gb8kyqOx8eyW8FzOP0bOxDsNFx0AUJW/ux2RMDAZAAYBh47D5DbZQQEMaI4RZQkCgrc1JjAtO5aMDHgPtcz+AUB0sC6CVMQaaOM9BXq+JM01scbevuYqiNdwFE1NzKABBgn6zZYYsFEi8mS/Tffs6pqjFOA/f6ANLVji8MQSZLq8/B6JUmGNHXfkxxZVihhDMbWBUUmXBP6ngmZDkTBVMdzwbJw4ypAV3Fq61ZQdjE0sMpZ96L1EfR3PLsWpk9RwycOOmb/Bx8IfJmsxXLz/AfOgoW793hf238yYJ8lBGef2d+nOhoXsPxr6Uv3tI/xVrH6Xz9VsvoXS3LOj+MzF59ne878R7aER59Gh+rex32PpxHRxKduQDQNCIcBUd/qeKJxUEcg4Oc6Xi93Lx4P017HCgYgDZhkTJgY/KcUQTyh56sHAQYltewMMClghy9CqJSEz7idE13XsyaK/7MCfMBEOACfZOsG+nNoe4E7TIxKyCwx0Ge/4gau60tOCDoGgucNqqrEv/3mX/F//h//B8a+Q982CtSbxweM/YA+jRGHAXIGkihA09TYlaWq9O8eH1WtKbGYgFZVDAPCoEqXWXoUWYUbMmsfOhx2z6gPIcKpx9PDOzx//IB3P36Hm5tbfPXNN5jP5/j666+QZgw8rKhD9BxjNJ2CICvsiBuw9hjblBlYWa2GYYyhqlVx99OAfhwxRLHa6JZIMDBb8sCNZhh6JRAMwk3boumsQ/C5KytixEmi4D2EFhyTMEEWxgCThSTXsXYDExDrVvD/Bh4nkyl+oiS18Yl2WaBl5dUD3dgj6Owix64KYwXFDxSlsTV4mRRxx9UmPSCMrFcQhDFmxRzL+RLr1RK3NyukcYo6a7HdbFAUMyRphojjBp6vONV/L5ZrJXKLxVLJwXy2wMsXr7Wx3qzv9XN9l2PoGzRVg7psEUa9ri2OQfDPtdz/8qXP4z6uGxrYSEQJFoOUtYRbjlzUDTn97rH17/7eV90+oDJgHRvQDB68L1yQ9xU9RwaB63Icy373H+xGHf/KJSj+ta2LoBaBez37MV1X/b/LcFwya8HZ3lttbp8MHN/NJSG8f49Jy+ctVuhR6Ds31hlgkI30Lkwk+bl437G1NSKoawR8nusaU10jZBeLST8/VN8pMZhUvdu1GN1n1PVTh8GCeMpgPk1KNpnYDkmOISmQpClGdoHiGNksw8RxWbbUyE3XKAwwBqr7rfPgEl4/WbBbzAV/913HpL6CLZ/88WMrh3TJkB4hy9ev67q+5IRgxG63x267wR//8Af8yz//N22yWRyirmvsnp/UpkTKFnWIeD5TG7lrWxyqSpvyu+cNIs75VRUHGFit8ztHDEOvYMCf0yxcbboBVV0pK+cclG3z58ePeP9uhfuXL9H2nbAFt7c3bvOzjY+VO/EN4cgRADDyf1xlyASBGwA3lTAc1f73nQAePiuyXv/m5pdq9Vrw1H42DMekgInEJSvJIgVnbq2qpoJRm59PCIJkpvfkOfTJiDoWDG76ChFwTKLjtORqHO08oteAVFVcomo/QqIECYjixI1HYgWLaeiAkZ9lwDh1CjR5PtOoYrmYY7WcIwpipHGH+Wyuap+jAiZJ/Ioj4jpSLJcrjXqYTMRRrMTh9vYOfdditbrVz45DhmFo8fjA5HCLcOJx8Fy7Ku6yfOBYSXt8iRKfgAHFgk0S2vtoSqEL7Gf//vftmp+/Fs+1/1t2MGycYHP9E8JcL3iagbt1DNFnAdsnAj4J8GOL89/jZfbYgeM44NivsN/neTv9vDtxPpPgsbnv/vg/d/E5UcveJQO+O8A7V6OBiQnliKBjV3DAuN9bMrA/6M/sCIYV8Su8L1vrBIWNkgJddwexMNiBvSY7DCk7C/wzq36eo3SOIJ0R6IJpvgDYNVstEKQpsA6BdESXJOgiYoCsB6Qz7qp+u0SWPB+7Abr2dl79tfLXSB0au6zHZhA7WzwJ14Tgur7ohGCzP2DzvFWFyDEA58jcZOvQAhaDFQN4yYfYPUR1mqBue/XuuAFX/QDWpikDER+qiBshW80JMlbKcXJMCrKUD/WEiSA6B/TRI9s1KLfPbhY8YXN3pyp2tVrh9es3yItCmwdb2Xxtvje/sz142uz5QDMy+g3djRnYDuTPusBg3QQrDRQI3HyWS8HwbEP+nEXcQsjPHcaWEIQjot5an2xjErCp8YTOrQsHTE543njcLjGwF7NeKAO/At1k1Tbn6JNv0WNQNTfCRjRRmOg8MFFIogRZmmM2T7BczvHixWuslktV9Xxvft40AdbrtVDVAjSqWmXnhGOfDpvNRviNLJsJV8D6XCDQNFXC0MQx9lWnrlFD/AZfQ19Wfquiv7S9rREHwwhPD2tYAiYtEeDfZhNnxKzWT0Ic/G8bxtj9oT+dIACuaA9cu/g0TvBDB6uTT2A7/kEJph/H+GUxSedcn/Y4HvDdCA8mPI10lCH5bMV1IjhS4vVUW9tjEs4gkToCdxy8ny+lxVlXwIInj1kz/YmVPJghY6ordQuH/RYjE+XdDn1dYyhLDOUBAfcEjYtGhENnZyviqMsdt14vQETAKzELGjtMyPoOoTuPStqTGmNaMpNGWPB7jGi/A5gQdBMwm2NcR5iILwgC9G4f8ufziLc4jgXc+f5kSmTJpDulNsnyoxt/L/DwrxnBdX3JCcH7j894fPiI56cnfHh4xNPTM4JxULvXI2/5vW47PUFl0wnsV/U9gigCIUX7rhM2OMxzmznHsUByRNoXOdvjNqtkgFrkmbLwmGX7OKI8HAQUPNQlnp9rPH98wB//8HuxEbIksu95hjuBA9nyTk6AQQYtlxBwns4VMtiHDKA2omC3syNSkAmBEODsQLIlbijy0bfp3Uw3TlgZ/1Wn8E9WkCYI8wxTRLDgiLQfMTTWJu07fub6CGTSBWNrlB2OiJgG1xk47vXc/C1pUNLigWbj6doQt8EuQKDOBgOSdQhWizmKfI7Fco43b+6VYH37zd9hPjfQIzdDXqcoi/H61SvM5iscDhXef3iwjkk36NwdykodH2EiwgCrxQqvXrwUOyTPC9R1hbJshD2p6wZdz86QJQRWdRrY8ZKVxpExMY4JQaQ5djLx3hsxF3fDmvodN3/HPuA6zpaPFbydV78swLukwJ16JhLnycORzcAuhM73EdPnr5La34bRsATId6h43Y6VvusY/HSE4scDXMPggW02QvBBy/7OStrwZ0gIIj0nlnAwuRIuhX/P4+bYcLvFUNcof3yHrip1fck0IsOobSoXbQdLnvidz09sz5jHdcRTgGQECjIQmNBPI4q2QTiMGBsbQQxxipFUGIJt0wJhHCGaFQiyFGhbTMuVkoNwnqMPYyXads78J/Fdmk8TAiY3dqmY+POcW0J3nDTwGWTy7XAfvL0vrAWu67r+w62/KpqVZYXdvhSQjOh8vwV6xO5xJqr2PFH7DAoExAVIQrY37e3UzWbFCwYZNzJgO7zrDD8gIB07j702Mv4MK+Ixy7RRCP3LIOQwCH1TY0NsArsFT0/GEsjnArRNAtNZi8/qONus9d2Br/zmrSpIrUDXytUIwnUBuClxV3EbMTdZof5Jf7xkRSnYMxnGCE3P8cWAupsMbMcg7oONh1sfUV2nl7Czfpoc+6rQUoOzfyNi0KPS3UuMxw2SnQYG0gLr9R1WqwUWi7VYBhwN8PP2PNfdiK7r0bU2vrC2K8GKmY6r5mhjZIcn0HVjgtIzYPSdobe1M7tzyFFDHCs8T0Q5eiCgm8V/7lKg/eQlDKHOqpZXK9W5sQDkJxQehGg4c9/eP/3++Wvpt9yI4QjuO5vzK4FzuBgDpBrbwV8dHwE9mO5Ebz3hAKw1f+r8WAJwxlrwIDyH1Dcg3E8TGP9gXr7Cn9x6PL96Bsl0qStUZJ1UNaryoM5A1TSGsxl6YVpcCLZj10hw0nVgYmH/R1AsU1X73K06KAQO9jq1Smg7YnxIJwYCUm3FGA6RsChpU2CzteSX44r5ChPvSY3jjsSRs67L2fH4kcDZqMCPRo73gkCojkrpwJoaoVzXdf0nWn9VNPvh3XtRDckqOJQ1oih1c1PbUv0mzPm06G+kohHkN43K9hVwlRCM2jgY7JNgjjCZ0FYTxrYxUGCWoQ9DtGOHKYpUrbLqm7MKADUQchzySq3pzWGPqTzgt//9/0KxWGhc8OLVa7z9+mvc3t8jTjIkWWEMgdBAZD6EstpTW9dV4OLyE//AqkMgNLbIE1Xl3VBLs0DVBP8tiTELcnH8L1lRukI1pmj6AJsdKZq1AIQt5//cEokjOKsk/TLon40DjFrnWgWiZ7EqNqqVAgyTIXK+1Xrm8fr2Mqs7Q/ezmR5GKVarO/zqV/+E1XKBr96+tusBm/kemi32hxK7fYXnzUEg0I4joDjBze2dvj9tHnVdilmG5XKmkcHhsMOh3KFpa3REmjM4xyHyIkM/zDEeDiibxmFGfKV2wTkVIt0negSFcBQzIY+APAyw1DghQDOEaFgJajxDECbvB6Ob6RDOkfA+ovgxwRm2wGEKNTbhD/K8CPQ5MCFgNWzjHd/MUUcgcvNsItxdYOfPe9aHqHYO1S86HE+MymkXQDUOYrLFz2fvz2SWyTDvFb7vp/S5y4IXc2Ky7BgY/fntugb1Zot6u8PHP/xeiUC72WLoOtRDq2S9mQY0TI7IguGLSCfDxiWLIELCpEDqBSHCYULEez8MMCO1cBqQdRz1tGhaJhwt6qBDEzSGn2HHKggw5+iLOh3sRM7niJIZewwYl7cYb3MVHKQnWsfGUTh1KOcJlKWG4px4UKfrAPBeUF53liAS69NdCna5ruv6W04IGj7wbAEKWT8pKKp6gVWKg6tW2MoWiIv0Q2IFHDXKZv4uMfikjcnN0LQL+ICPBB1yg+TPeKEQ0uCcoEkSJ8hTEx1i5akHvuvQVTX2243Agsv1SslBVrBJwfYxOxXu7RzgTDPws3Y8N5mYx63jdy3dP5nx+pdwQMML0dv9EKAdJjT9hLYb0YqCZ4HptGl5qtkZUfu8Aj77O1+1nsncOI63B6z9pMQ88utNeyBKUuTFTGDAPCuUHLV1o/NcVZVwI4d9Ld0JJVSs3CLrRNj5MmEnfUUEEA6oa27oFCGyDZRsEL5rMZupiqYwVVhWjnN2NvT97PVpZXwU9nGV3Qk86DADvvr+5GTan70GgOek6x5096E/wwzYJz6jf83zs/0THIF/7fNjPAcLeobAMfmwatTT8kxo6SfX/d9tqhgq7tKE4Kji495QGAxW/+wEEA9CPYm2QTd06uy144iOzydBuHzWAmBwEZbJDO8V0vjYwQui1PaJYUIw2P7AfyNGgpTCcAzRsmuAANUEVBrs84UoTWSNr5gjIbIZOBo8lAh3JYYwR581KirgMCW8H5X8kzap8+oAqAIUu3JBokrnJ9jdO2fn+dohuC586QnBx/ffoT5sMLZ75HmMuxd3GNsWXVNhYMCoOSuEgGl8wNPU2s3Gzx4Nmd824rSzJc2ZNNm/fLj4GnXXY8xz8ZSVGEQB+jjCPk7QsFJ3gTpPEixuZ0LLE2DfNC3ePTyJpfCHf/4X/PD7P+Kw3eLpq7e4ub3H/SsCDWe4uXt5AuNx3NAPEinqXTXOajgvqOAXIhUKnwkJldTOgqef9QqDwIrssvb29x9KHLoANRMBjSWYZLHLYgmKxwl82h1wM2MflFyr+riFjacWsrXhTzNtl/qcRhAu0QhYYRH8lxdYrdbqytzcrHVtfvvjH/D89Ijf/Pb3+PHdB4FEy6pDmufqDDApqPYlemoQjAGyyACEBBM+PR3ww3ffSdFxGDqkaYy//9Wv1XV5/+EdNttn/PG779FwFNGMaMrLWBv6+AKhqcFr10vt+0k0tB7UbWgMcBbH6PldQlAB+olBzGdivEdIo6OC4oCusVFINp8jKQoX+8mOGVC1jc5pRoCm5t/UZAjU3iabwicU4uC7TpRm1Hou7L1YPCeRC1ruKpHLbxRG9sZHBUUet66/15xwI6sTQcE6XqJXRpyDc9plz98laxpaDD3b746hMUHJ98d3P6AuSzxWOxsNRRQD4kjQzql9EZpvHTcljoMxPtIsQhoH2keWN2sF67HvkfQdkrpE0LXsTaFrJzznEaohxdMU4Fm9rgFD0COZRtwNA9IxxN12j6weMPzmewyPHQ43G+zuK0R5hmy1NvCtOka8xlIdsGecXzrXg/YXUqX1nWJnGlm6AsGdaFOu6JHgsu7gdV3X33aHoDpgpNjQ1OmhybKUvQFR1tTS9Nmzo+h51T/j7bD97aRLg1ABQ0jwvrHWpksYKGzETUGUKcqaUpqUrUBl8dauzxlskkTV6yyPUcU1np+3BBSg2u0xHSpsn5+RzwvESYrZYqU2Ot/nyChwyGyb91qA9YqHqm4FxvrTzsCfnyt//qrqHoyB7Az42b9VIk4ExlWjx3c8o8ed6k+XDBy7A24Of6J2n47/+JtnXHv3vgKgRZHGPNwM2YnhTKGpGwE6t5stNk8b1N0gueFZP2I2X+rcMmAyvtlM2/ZcBiImgPv9TjoVDJ78XLP5HHmWoawr/UyePyMmWIzX2RroF51TH7Q+AY5JfMYSBWEf1EYedU/6kYGnE55NkgWa5TkYOwZES1iPZ9CdTGpXWDIW63VtOGDAQQldMYGUzoNOipuy2XjnpOxzEtbS3wjoZnoGNuga0TuJQGM5eOS/7yWcDcG9qA/fX12Qn+FeFUjV4T+cyujAgC1F0MbwJUyoWN2T5itxqxCTklOTHA41tiKTwM4Nu3HEBDEJX6xXOr8sLKK2QTQ1uicHIhjZRYsi1FGEkmJYU6g2PvEF6QSNI9lzWvSDAnW3r9BjjwMyPEdzxMWAWVSYEikTAp0t3gOjAWWV+FNIiQkBMMYGMhbzxAkwCQ/lRz76zp+/jgyu6wtOCOrnj8izEEUaYteyvulRtZWqPKL/qVbIlSalwGJ3t3fisfvWqzYNsRLsgWNAT7KFoy4maEPTKpgoXBJFpoBHCl3bHYWMuAaqGe73mOUpbpczbU63y6Xm2Y9lIwW/ZrfH5v2D5o7VrsRyfSsFPormzFYrdTD4clEcIQ/oj0BhnRh5kdr27DZW0ik1F2ZA0IyXGzorr0mUwIaMigvWrukxBoYTsKDs1eBcwFYF+efyj7Mk4RPe9PmowP2bF31RL9am6/ZzpmNg7Vsi7gd0Y4eu5yYfoS63GPsWHz/8iHc//qhKUNiLfrDxUdvJu4Dnbve0l9ATzyevHccAP7x/j932GQ8ffhT4jNrvAnwWc7T5gPLAMQQrWnZkciDp0ScnsZ3PXUeAnhDjpjRHDY2Kn6+rgN0HwxDM2CXIMUSZsBp2niwhYMgJxg5xu8PYNKjfmUx3336DdnyFLM5QpLm6R0ctfrWjBwEux35EWiRYUDSnPQDtM+qqxod3H5QTLF+/kd9CulggzgsXaCy0x8KGDAibnTj9iKlJQbxNTK1shxlxv+HGbibN65KCI03QknNeD/lEXLDmUYS51w4hkHBihRwI5Z/mGW5e3ivR2h0oad0jn6dIiPDnh+1HdeLKA++fyTA6UYCbmzmKIse3v/w7vP3l32Ga+Lotxs0j2n+p0B9GHJ4ztFOAbRpjNw3YTSNKyTI3QM/PPuKQZxovjim7hpklLIcKz8MG/1aFiGdLFG0qRtM6MdE0dhemgKwHS1IZ3MlmUtcxdXidiHocAWY5NTYizLII85w+CT3ivkY0sDN0Xdf1n2f9VbtEW+2xTOcyKapJQ3JKghwVcIZYlqV5BSTsINCDYCG/AWtXe765cYolDxAFyLNUWTqNAUgv0gbIaspV6aqy1EpnJ8I2PfoLcLcbhxyz1BQEC7YFxwl7+hRQlKipUbJNzbFA2wsZv7i5Q97OEUlW19G/RFGUXI1GGZ41YLLKpP71amWyuvVzZFbtPBYCukRTvGCRWRDxWLye/k9EcaycdD98rpp3Pvb2/3z2Yyf1O58t+HbB2Wu6mb+CClvnxG2MgzE+hhZdW0lQaL/dYrfZqAvAc8aWPM8Lf6budjZeGULklCnmyCeOBSr7uGUQ3GO7eTxWmOw6VBW7TJF0CIibYLVMYSPmKkzUThSxz1tHyKRHlTuTKM61iTWJyp06Vl24xpBEmHLq459+32rDUWI7Ud8g6Er0249odjsE8yWmYoEop0JkrtPKhMApVqm50VeNlDKzZIlUtDf+W4Wu2mH/4QexR9JipsAuwGt+tEdSmGeqqkDV19KYnoICU5C4ZoJLBPy9oUrWQQXOEPKekGIo/hC9Rz5+5sqiAIVGfJzl9+hGAxdK2podJXYLhx41RYfaAPPZDAVR/j0/x4gyLNFRdpxPWkQ9oUDeIvPFTHTh12+/4hXCFLRosgDb72I0XYRdEKPDhJqmXylQT73eP6BWB5U4gxEkGEg0LM4QBKklBEOLcqjxUO8QtQGKtJYfSpSHSCNqoowaIXk2CcczYitQmiTl5uSVCgNRY/OUAMYUcyYL/LmxQ8hx0HVd1xfbIagq1EmIkU/0OCGVZ4HpfRsq2rvvMYiSQlcjrVMT36FCHYO9YxmwihIeQGp3IWazAgvqELBNOg6mTUCxEQEGTXg/kP6uqQfKtEXuegf9bJ6xlRuI4pgiVhuz3Q5IRX3iXDPA7OEBxbxGTMBcMWmDkJeAdNqNwuTV5AzZfXKc8xuwiZEEkmhl14KV4CVLWgg/QYSdOgSfjis+/Zk/D3b89372/M/nwjfazONY53+5WOhcPG+eMLQVioCtciZ8Ju3M4MOEiVREnk92RzqOaIJRLIK2rRC3mbwM+t2EnlLMHUFne11zts2Z5O22FZI4k2YBaYqHw1aARVIazzn2n7uO549cd6cU4KmIwUDxK6tcg/qAkMJPHF2lsdOsd3RTUinjCDdzakQM2FIxB52qwrCvMBxalOVOct57YgimCQXZHdOEDz9+wGF3wNe//AZF9A2SdoOs3mBodkirZ0z9hG5HYS0gWy5tjKVxi9ovCIYD0DUYdh+VEISrl1L21GCCCZwElRzI8OjM50Yd5/eST4hc4LtkzRcFFvNC770n3bBpkQasmq0Vn87IuDEmBEHHd4sVZiwGmCT1PdLdDs3Uakx4/+JOeiEv7+4xm81w++IFcnaNmgl1vVcHcCxrjFUt50t5a4QxRtFYfW+ESWgiKqmMDhzrgNc56SmlPJLMqyS0azvU2wOypMNs4J4TS9EQlOl2NFdhTDi24ynmPsWtQIky8RItyppFApVPO2Rhj1V08ji4ruv6QjEEFao4wJhQ8CNQQkCuuU8IGJgHDAoUEiiqaykQ2kw6OQZZtds70s+AIRsxxZAfwYyCIpqdkiZG5bxY1fl+t1WSYej/0GxKWb3TRnlvmwy3R7EbYrafQ+zIhT70SNsGedei4wYxW2G+rDFf32lz5fF6W2HjzPuK0jMbTqqEVnG5atrN3NWJuNDciMHTSyL/OaT4T5kNf26dB/mf/r19tz/zM50nEfwjrw+7ODz/y6UlBE9Pj2irDBlrs4EAQksITG8gEQAxylLJUe+rg0Yp+3KrSpQsBfogbKs9dgSGsdLl65CGR2yIFN7eSyyIDR/zMyJLhT/DTdcQ35esI1/e60iw1e7HMUOiFjzfL6j25pA351ircOffgg4TvyyMcBdlCKIBzzHvBbo2MkhVGJoWh7JB0/XYlpVx5ZnAjhN++O//io8fPiIPOry6mSMdNpg1zxjqHdLyGQPZJBuCYCcsX71211/efBhJY233mJoS7fZBzqFJvkSUkUpnCbGh4C3IH1X0jpx6Z9R01Jdw/LkL592r1QzLvBCivytr1CM9N0gFLtQdWNzfGAYjoj5Qixc3N1jMZsKFMKmMZyl23QF5nuGbX/9S99uL9T1m+Qzru9coigXGvsZQtxiqBkNZYSgbBWkCE5UQcJTkaI8CTfaUF5cut1McNQIjhc+ingwEZlnEsXSyWyduaRWkGFJTrAxTJzgkd2ViIkyFSNREVTjGk66aViOksY91/yyzCfP55SZc13Vdf9MJwf5wQJ6w0h8RMpBSNc+ZBLGimqg6F7LlbA/LT7nzwVHMJ5HxDdvHrC5Ma500RfKSA6Qhp5NGJ+IjRxVDJiHG5TbYmQ/W1O0XCtgZFvnRhAVyVqidZr80D0oeHzTamC2XmB0WmPp7KfFxjkntA0/fOm6uUgaMTezEKcjJZtXTzSQf+/MAi04ys/6d/7Jk4H/0715b4VM/9z+/ibHF++rlC9wu57i7uVFSxUgjbr2uD+WQrerkWIFGS+zQeNtYOw10f3QgMiHgU9kcc6zCxK6rbebKypuULa/3x86B2fUyGTjT5P/MZbNzB9TzAjOuw85KO8jmQNACh535PbQHBC1ZJSkQ8V52dFNRU9mxsuDrNfsjGjExgZkadVKax0e970x+DwGyqcEsGhC1B3S7RwxBiTAYkQQT5lS+ZOckpi7GyUGQYAQxEPhdYEez49a7MuElnebo3GlCOabBf7wD3H1wGhOdXBM+QcN+1irZTVFbbkJ52KM67JUQc1zEZ4TPIHEo9LFIySahymhMSetOuCH2Mfh8R2KyZMiKHPlsJr8M4U740jQL2+/RHQ5KuJhI8Pd95yZkvyfopQHAzcHwEQ54q+eSAEyeE/ofBEimDBk7BByDkQHBcUfbKhlIU+oeGP3YmDgGvvTy5aa2aSwQr0ZJ0bAdr8sIrBJ2yK6gwuv6kqWLP3zQw8E2X56nNpMLIqwWC831mRRo5s6xAbXLXTvcZt7c6Cjok+v3aH5DyhRFjMAqLiHAMMYsYQaeCbjWVjQ14myaPuzci6zF573l2SGQbXLERIOe6EBPOVzO/J3mQVOX0lWP9wfsy1q0urIqhXRvf/V3MkW6vbtHHN8cCd1H9LbrUnCzoqAOnQiPND6JnBCxfunA+y8L+n/Jv/25YP9pt2P6059zQ/YX93f4L//4j1jNC7y+XQpMeHj8oOsp6+c4MQ63XBd53is0HdHlPYbJkkC9rJgClDhOdY6rkvLLnRIzYjq4isIwAyrvmMQpISBmg9iM0JKQC5booA4lLhEdOl0yAWGgTzIEy1cI6hLT+3eYiCfYczQ1IJ7dIJ1bYqgRg4z5KPPMefGImN2Nic6MDYKxRDTu0B+esf/dvyoCr796KTOvJfZIsg5J9RGHHyJk/LzrEEUQ4OU8QzMCuzxGl1L62qpQm8fTQGiQTfgYj6hTggEDTFmKMePojUnxJ7YGzgzo05GQ/eGMceC0GC9Zz08PqOM9xm7A9nGL3fMBWZZjVrB7QdBdIvWieP7CmA0cycjWfELd1aJ7Jtwzigyz5QLz5RLL2xszwSIQECO6ssT+3Xv0Hx/RHUoxGMQ8oj5B2CNmUi+RI3YiRkwJpcfNu0KJH4GYUy+AYDhOKJBgjoVcP/u+1p6yTzL0XYI5BcuY5Lozw5+XMyIxSryXcRR9cGwPTnMIjB3RzEPM4hhdfaGC1nVd13+w9VdDj/280rfWZYxDxzE2h1PSdkIkVJ07s6H1aHY/hxfjYGAL2kBJav93DBoReoIFR7Z0XXtbm6Xj4h+DmgNgidpor+vR5AL69VaRqmwVJ9uqUNo08xPs2a5tG2xulgJGWpVmVra0Q+brklIp4SE3lzx/3yNIkonHpbJ6f3J2/+fdgZ/+23kX4H/2Oz9NGuTP4ACUpi8fCSPQNwEIAbO26FGf10xhcOqY8DoygKuqcvbSujasbXnNHQrecBlO3pfgPrW9z/jznjUX/Fzn0f/JMQ5Ev3NGP0wek14V7MRKva0wHUJnN50pQaUQxEQdASorctTh7mc5cVIRUPflhISVvzPMzWIgTwOs5gmKaMI8C5AGHeKAyQ9HKcTG2MiLwTLgM8Bu25HGRrDa4P5+UBImkgmFuuTQ6ZghZwo5AiMez9k5fsDOrzMlPskgf+ZqDgdMIamXo7BExIbwuSdbQM/HwNGQOSLqGvKciWHA7wQWEhvRy/q4ryt0lKzuWowJGQKtnvOhMTMkuiTqd3iOnP8BEzGCLZmUOTUEAUXNusocPeWKyPMIJga8LiNS/Y4BWoVd6g3HQv8Ndgi8FrPtH6ZZYfRTp8DqbBglo0wVRdqCdRwjcD+5XDPjuq7rbzYhIL2MM29VdaJ0GShtGS0FIiSIiJ0CBoqGrTnORp12ANvHavFTxayb8LzZ6t+LjKODCOHA9muGjsBC4gXkwOdAQk721WfzkoWlqmFkIDceR0Od82HCbs+5rnNTY1fC8eqVLJRbdAfg8PSgQF9tP2C5XuPm7h4rSe9mSHN2P3K8evVKGx6pjUebYH4eF2QYRGnOQ47+z7/+XDXnuxPOjOXISvgUXOj/7U9++9/pHrDDws9SlnuUhx3uVjN1C0gx/Pjjd1b5OyKed6Yzmd8RdddiW5WWFDmVQdH1iCXIMuQ8b06TQswMd3zcjMkiJwfd2xMLwOVplpT1veTsaXM3AyPKEbOxw8SR3Z6QokGkwsYZBiaESYfm6T36HytE928Q3R+AJMc4W2qeTyEuNNXRLpvPQExPjaFGHI9oE+A2N7ng13cFZvMC6X2uICQqYzRIwGnMlpimGBm7ZgQO1qVm1mHbIh4mJGOPdKxFUez3G6PnzW40xgjzBQJ2NsjjV04lG04nguSNt04eDPblEjElcZFEmC5Zj7/7rTEzeibBTNrpYhkjzXhfjBhqpo+8vhynjMI+MCEPagKLK/SknO5J4azx+Nvfopqxdo8QrBt1COi6WT+8Q/3DjxrlRG0rKeJ4ahTYi6mTxHE8Ek5IEGCATloh1CEIFfipipIyQePPjj3mQYtV0KMWsoC05QGHmvLZPaKCGhHMASnKxfNK1U2TelYHQvoq9tjZ4MaShl6CFR2SsUbflBed0+u6rv9o66/aJY7gJ+n/O9c/LZPxVcLAAM7sXxXDyTKY69zRTRL6asnbhmaGOSG6KLKkQsmADYRZaKjC8a50PrF3FbzRjMZPQIum8EYdf5sB6jedbCnbkAxQ1X5nrUYyIJTcZMiyGg0lj93mP7Q5kiSS0iHNkuxVDaDHz2jB7edc/6NgeMIBeOlkE1OiZbRjR5wBB/05Pz//n4IKneUzP0dv5lJyNXRg0WM36BN+o/99AwTaOTDJajvL9udPjHh9EftJG9tfxzOzqU80GH6+9RNTYAMYsuKmPXaXqgpFWwF1CVQ7UWkH3uscjXBs1ZqPhTFoiKoj2tzuBCnuMaEl0j6JkKUR5sRPkA5H90xW/JRn7giqZELMwEODqAYtbcOrSi6B4UhXS75XIxEkAuhCjglIbWQSzsBPkR/XhVGa5gWsjvRSd53PMCliJTiWxSWLlEGOykjBQ0DNBo4z6GrK+19OZNaV8H/X9eoahFQm7TuNQWKOERlwqefA1vzhgC7JMIYdojBGX5WY6GrIZICFBBN6Vf2TkgIDXloyQqqnWv1ToOSLGxk7Nvp52qfr7/jlsBmuQCD+paMqJbta2lgomCSwyFGYyr6fi3iZ34RTr9I5qJsObXPtEFzXF5wQcE85sJKsAocjyLW5MbPO0gS3q6VmndS5J8iItsfUOveiROwOUOBHNMOikFbBwOBDo5CWmTxbqfZlbVmrfjnFl0wrNQvi2KkUkjJoKHIG5fFQI+wHdSn43qxwyZv3GgJC1AvEFeKmWBrugOOF7Q7PZY3Nu/fSWqfWGelLcZ7pvW7WSyGjf/2P/4Q3b9+qpRxlucBJrK6p0f+zrT+jLXD8J8dC8AHeBH4oM7zC27dvFdB//PFHHRPPv7Qa/gJ8gddWoA49wWJ1RSdLAgGpST84dT/XepYRkHULOFqhgNBAyhaFcthm189Ygse2+FFC2G2wai2Tty4gGummBgRlqcat/hOz+QsWsSlMSr0EsHdP1CSdwbmtrEpf3uo6h2WNjKOPZoP6+0d0QYIymony1u5KjC3vkz2GtsewHTDMnnGfT3gzo95diOX9jebYBMtx5DKPR8xCYLsr0ewrPDY7fHd4VuerqPe6Vv/2+BH7bsKuBm7fPyALWszCVtOKOGGHZYbidoGoWFpSwH/wlsf+mkjEz9I1UXo9q8CNNvhlzJgIIxWfLlj9u/fqjAj+RzAwmRplhMNHO9fzjB2MiZaoGhPQ+ZCJTepkk8n0yampMPaInh4R7RIcEKJjt8ZhSbr3HxBvHhF2LbKRLf8Jq5hJFZBz1DBOKGmMRuwOLbcmMoRC5GEKNipm4YCMSYGom53GNbOwQyb1Tw0DREFmTtM0nYqBZIrocqKCYmIS47Q4dL863QUn96D9i0niQEQEkxrZOl/XdX2pCQFbsKxe2K5XVWQPkiiBTvObFTkrVsnDEhfgqnHNCJkQUJbUqRB6VoBkUAXQA7qwRyOFMLdP6FdtdkpQWKJgGCN13QGxAMheiKy6NWGhSbrv4r4TlBVYe1p2uyFFSjj7jNT2ZtCTec8waeOhpwC3jp7vG0W4v1uLo//ixQus12skBSvCxFXGlFr9fwJYdCYqdLbOZZPluBjHSgru7+8VwKneyL8np/+nDIN/b51oluwScGZOTEB/tO71Vr6+YnJ2O0epYy+M662l/fFbB+FPNQXU9FEiZ8Hq+JeeBvAzLDPO8tLNnrlhLV/53ms2PSJKWX0HUgpEmqEuNxjKDbopQj3uWeRiv2lFExzKVpz6pgvRlT2SVYJbzr/pgVEwEXCa+ExgQyCN+F4dxrZCuW/w7kOlqvdmMmnuzeMW27pDtn6nY83jHkPcKegWqyXiaMSM44I4U/LlP8QnvRfPHPEJnySavZevc1h0JmJ/boz016zxYO1xeYEY+1FaCX21R+SwBKzE0dR67pkYkDXAy6D5Pjn8wp1MGAnGizp0z08Yea9ynMQX3T4jbKgAyFGBVeWpOmFKNeV+yK4DwYGRUhOmphHyMUKmvcE6AuoOEljIDoHrFHjHKTGQZMlNcS3iNKi2yL3CvTZxMbIAtfPrk1idA+0nA7qpl5qq5Lqv67q+1ISAGw+DDStIqtFlWaYHxoI7JF3MwMJ5dN20Ah9Vkvd1ErkMBKoIuW80GCIigicBjDAQSW2zfvLY2bJm+1VzUg+cY+UxjigJ7lGGb5swK82MfPokQZrF2vBnWYK6muPQ1NiWB7VuiRJmNVOkhdqo1EmgwZECm3TfqWIWoe17lNudYQXqrQCGL+/v1LG4efkGd0kuXj75zfz6f6RB8Im+jCVNmt87xoA6H/2ga/D1269shv3qldQi/9f/7X/FO3Y8jva7Nu45t3r1zD6ejziJsFwulfSQglkdShz2ezw+PSvJ4GfVIIDXRa9Fyh2TOW8dLNV3nT9T7fNJg7/uBogTndBR+bwtsOOQOpqoUR0v7RAo9DmqoD+ZJ38DfXrnXkdMTITg9i2CfIls9x7xLkFYtqg/HtDse3z/+4P8Jqg8yASzuB2RrUYcghgfIxp1Rchmhe6NshqVXKZpjTHqcNhXaMoG20e+Djn1wI/yzxjxwO4BA9t3P0gB9Pa2QPpygWg2R3L3tXAOY0S4Ik+YXUNdP+JrJwbfo6/iT9gENtJS14yJDzsy7KL1l92nQV1hPs8MkyPdjgDx0CI9bJDy+atLSwiYOJAlUTd6Xg3wZ06oM6ku8vYgHShEsI+odoaONEzSLasK8diq3Z8wc3Ofk539lsVF0yIbRhTEhaBHG/QySqLJkLqFzhEypIYWYnUWOWog0JA4B6YIljxRK8GxEfi+fBOBPAlGZIfBWBn8PY3mHDCZoyV2OEg5VcJ89TK4rv9k669GGjGIVqLtLRRwqEegqpwa4+VBHQH+DEGFDW1RZZVs25UU8ViNycGsFYZAM0X5mtvrsw0dx5z1Q0Fe8rps9UtO2Ob2DIQNfdAcRJ1GOcv7wjbngI1GU1Fs8g7j8zOe99SEt5mhsAb8zhkvFcza1uhcPAYGR9KJuhE1Ndk58ijZIY3w4d0PCpZRWmB1/8bJFlO+92ecI54Uhf9keU/7c6wAzwXBmgzkHN/w2MvqgH/913/Bx48fnWmUxxv8FAdgwVLaD3EiHYKb9VqaEvQpYGKx3e1w2B8EJPMGSBbkTVxXHHlRMA0nb3hve30fivW3+r1TN0idG/3ZrG6PZkY/QzKgd2dSaYpHp/Pn1C2trc7Kz5sRxcDqBcL5WkI1SdxgnLaI+2eMVYOHdztsD71sffl7L6MEN0mEKoywjUIU8xzzWxo8BajLVkFlRrnjiAlxg65ucdhVePhxJwMr0ioNBMugMiJOPmKoN0iCe9zd0lMhQrx+iSgvMHGkps/jgpQzKGBCoBn6Ed9xOm9+Bi7IjOh0LNFZGV94n1LgiyZCFGwimJcwDLbmK/M0SKtSwTSoKpNwJrBXOCID5bm01il8WXdpKA/CCiEcZDKVNB1ySgITz+NuVxYDvG70xuibBskIZAQI03sjYPeRe4ipnsqlQ7/HAoUalcQXOOtoz45xTpPsDrBDSAllnhrhCGSNPR4TAmGPjuZQ/LfhLCmwsc11XdeXK11MdzoB81gEsEI2bfuMSmGScjcanpTmvOSvYwV4FzwJgygBONHR9HpUL1Q7sUHdd6r2ubOzYo+zxNqxZ2wDp9tqwbELFMAkfEJZZW4iLCsmA8gx2JnfuQEdS9KaAmBfMZeSB+cAAGJvSURBVGHpECaJEhuam1DPnO3D5axQEOZhMNHgzLnab7F5fkLy7gOetzs8Pj7icNhfdAEsUJ2EkFRJn0nQGmlMoHJ3LrkXuTa4OOiTNk2er/v7W7TtHF+9foXqsFOFr66NA1PhJy6AfEV6FXBz+/jxEb//3R+xmOXYPa30ud59eFZy18u5jhr0NveXZLMDFZ5cAS0SC/h2GiyYi2RIMR4i/JlImKCM+/RnHPmzc3BhUqBEVZSxE8ju/H48nXvrDCFKxQiIizWisUYx5bhrRgRFjTePEeb7VueB98+rFwXuXmQoshCLwuipCYmHw4SnPTEyHaasRRk16Cr+N9ERxBckCChZPDgb3cy8BRb3Kyxv51i/fYX1198gWd0BaY4pSsxFUZRHA8d6uuFJqtjjI1yixet8LrnNoCUb8UjP3SVrf9hhaFcIySwYQsx4PAQJUqeBgdSxCpgIsKMh0KVGM0ax5L2h9EXJKfUDQgVr0jvpL9CzdU9jId79THY5v+PzTXfVcUDWDchGMgoMU9AHEzoqJRHUO5kbKmXIPaslZEJAYKab/xs942T+pMRQRpYDSQOWODuqsg/4ZCXw/rRnkPc8uy69Gy90AoZe13V9sQnBjipiAt6YwRDHB6r+ZhYwmHVLvc7bpDqaGkVnDKU9nlDsCgrGCKCpTTtM0njv+0p+9fOcVCSb989izv79eIDob2Mq8MEk0IcP7m7PWSDdCnNhDJic8L0IQCTwzhvykHK02e70vew6iZAQAR0RiEamhN4rwt1qYchxArwjSt522G8+Yowy+bJv9we8+/G9RI4uWQz+x+Ldz7qdWqLhJ6yK5lzatPjN4/4oRcxAjRCLLMMvv36rrsDf//IbTH2taqatSrR9h1YW1aYf4N+Mr0u6ZteH+PHHd0gCM4mi0FTbtHj/zoSJYipRhhHo/JslRjl0jCwf9hXXbd9lZWUVrAiFASmbnDGTg0/5WXLVPVXS87ztfrF492nQ/qxzKh0KdoHOGQY/tQH2IJVQwZffKWBEcGxW7LHIExTbGo91hv2+weF5h75t8ebrGe5fZGqZkyETBTnSgPfEgPebTklmnVVYxI1D1hvwsJhliJxWM4M4E10GxpuvvsLtq3vc//JrvPj132FKcgz53AK52vx83qjj78yJXCZ4RAQ4oyuT3CAWh8Jg9mwwUOpTh7HYEpesp80TfvGSIExqLIRYMkFvO8SUGe46tPu9iUsdeTjOc4AHJiYO73Pey8S+JPrsCTUYaGpFz4MkxcjJISF7o+F6xFLoaokcFV2v7gAppPxi4tMSj8FxBDsvEp0qbMxACWOaQfGa+uvuugTHZpS1iyS0NFBxkMeiYodvwmeFI0YCovl3vA42fmOnhc8Qu5TDNSG4ri97ZHDWsnaUOyHRZbDihH2I2iUKmWZCYgYMqPhw17U6AxkVwli1z+fSBygytroTBWQGctMWCFXxekoatzXNJ11Ak4ocZ7+cG0YGSOpddcIAL7VB5SLm4Cfa1ZmQEb/IKKBjmjf/M9YWDU4GjBETkxDRyGAhjoNR04YOTblHHz3gULViGMge+WdYR/CdA9hZ4eeoZc4ngrK42sRY1Do3SAUGJ7xiYkAB7m/XKF+/UlDnz5B7vT2UtpGRIigau831rZoKMZ/NFbxpMLTbHcx3QAqFVuXzHCupkn+DfRdtyw/mz0ha/l7xy0YNHktwcnU8/oYfF3lL5gszgjiJkabJeaP6E18KE+052U1b2hViDBKMYYYkG1DcrpQI/fIb2jQ32BSxzud6HWJeBMjiBHnKzkCEtuowcHzk7L3rnk6JI9ZpivkswW0EfEszqGHErjUp3jgxfYv71y+xenmP+c0Nwnyu5ISjNJ0ClzT7c3Y8WWfeBb65ojRKn8kJGIXsGjhr5P8JsPQvWZ1AfgbWO34d00H/AJ2uvlNjNuyLtCzUsHfzHDImTMxIapJSNTSOv45T4D0i+s062ap23wXxVFhSBk921fp/dQsYuhtRE4eQwk7uGeKYRTgXJ9Xk1UapiErpSPeeYkpwDCjNFEsMAtkkO1oi1TQ9TsZpPVzXdX2xoEKv6MUKmxQ3zvmpE85NgZucnOQkQjPiltLGcYyH52c8PD6JfVDkhWb+L1++NJGX+CT4w3dg1ZXTt4AbDjcMzmYdbTHmRsv2OJHYaaaqmXQjzrift+ws9NjsDARHWiPn4ZyxUz+A20BC5HcAyaeGPZHkVs0oweDbS3nPql8qGRqAammOjrLBHbB7aLD98b2Q6Icx+1lohz/drP0YRZUWbaaJQqf3QxRqDk2kdBywPRoZ4JLnn19DiySN8V9+/St89eoO3371FR4+Pun8f/f+QQGc142fl9dBZlFONCggtpI69WWJD+8/KmlaLJaWPKkjY8jsoW8kyFTXrYBZpzzGjwk8PdJ37C1IcWQgHQdVhV51z2MmvIqBgRANQPn5a0nb7cXC9myPwnfdJK9T4V3uLDzI6gZhmEtDeL7I8Yv1HOHQ49dfv0bfdPjxj48CCfbTBv20x2I2x3qxxnbT4Df/8hFt2aLtazRTh03doUKPu5sX+OrrV3gb5PincIm6G/Buu1XySuYKE+L89TdIb18gLAqEi4VzPLQASlgc109FqE56HjZCssGSqW0w6Yp8MuASZQ8wvGQdHEA1ZL+DQdI9O9JHIPNAniR+Dk+GhT1T6gB2zquCAVvuqGx3MbFtMMU90oZU4hgtPS+cgiOrbybnYUvVRr4GPwdFpkZ0DNaSMDZ6oOEDeC5qAwfTRnmK0aYBhmzlaLqxYyZQWptCRPQoCVCTHllWVv1rHzvpFtDMSncHGRQugSQ2QeMFnt8riOC6vmwdAm7YJD//RCPfm6q4mTEDPzcGAt0IWMtT2t06ExQ65fHLBSP4JMMJFmlOzQ1G1eLJU0AUQ7omMmHIcxR5fqQVjQQXxa02KRPLGZUUWPHKbgJpW9aSNgvjSGMKGS2NxunXpiv53rMyxwdnJ3xklKfRNriAHQzCqS7cFI4dbadEeHpjO2YB5Dw/0FryOk553Whrs1GCfsc26YwWxXQwnBWmptj1WC1IraQgTq/PyoTJFBjts9N4qK3MxpeJHrs4voPgK1Lrujhxo7OGxukPnwopHN32jtWrMR2sovQYibPf81XmhS0Cf9yWjzibYKtFTzbBuu9clahKz0R8zBeDdtoRon5EmlhAnSUTJo5LplDJYBZHSGkHzbl3xNFUgOU804w9IoJ9CGSctbphQCIeIEXcdNhUDNIB8pTOkRGyjGp/seyXlUwzmHK0oEO05+M8GbCEwA9ALLHx4xZ/36gTckzGjIN/6Tml54jhPw2YOxEIyDHfT7oPhl04JbXGKHEV/sDOCSEBdnyyE2ZgJ4ul6WRzbh1H17p3fiT8XSU0pCcrGbCEQO9xnvRJa4DnxSUKXixJz8250Zodo3Q1ZFxm78nvNvJyP/OJPNGZxbQ+jx3bdV3XF5sQzGYL93BbW9638LxboZC+ArjNNPMnmE9qf/2Equ6UTHhwICtVL3HM9jQxCUTs392skL98IdliqgNK6GU+UwXx+uUdlrRJpkFKkaGqK2w2W80yF1EiKiKrMCYP+0OJqmlUCXM8QXlkyhBzU5AKoY47Rpd5vr0bRRwH+kb1YjVOO2c50KkFG2FGQR4lKBFSYZsvWA4Rf4S/uXxENtDH+SdllwmFNnlmbmJJTkEYWlCHyLMEGSMTZ8YDkdmTkOCzNEFLKeibGHG6ELjzmBDMZkeRJyYFHz+8x8f3H7CbRuy2z6IzUsJYSpLs2PDaTibUwn22DBtEIa8pU5KzjdM577BqO+IMzH7QtcD/h/pLP4sSgeHpPKLdEkt2KJRFie1i7+Stg1t5MUzI4hCLMMIq63Ef04Z4h+37f8a4LxE/tYibAcWyQDRb2jU77JCNA97cp3iBHG9//Y3wAk8fHlEfDvj//dOv8P//L7/E7nGHD7//EdFQItx8MJ1+LBCMGaJuhXgoAPodELCi5NgpSLouwE+HMVwCdbKj7oygcna/iManuRfv14TXnLN6U1Jk8L1kvbi9Q87kgmJfPUkHBA5OoviZhoV3rLT2O0GW/PepaTG1re0V/aA9QD4CvusW0aSsQbNLRQ0mFZNdjYnVf2heGRwb9C29BywR8BgbmZ05GqmbENp5ihIxC6RBIIYGEDsBIlMNMTEtD8ikRTfxI6obRFV0wFhnJe2ZBjyHtGPWuLSjPsUVQ3BdX3BC4Nv7vkL0hjXioTujI/4bq0sGdAabjK1uKQuaJYnX4ze2wqQZPBkL/N5R7KMtzCnxE2EViiDFCu7LJROCHLMZOeQhKtEGI+QdOcUmeKSKg06HFEZh4O6odD4hG6hsRl0DPfk6RiHtBXoKTgmBC2q+XvfdEG6Exuu2DYMqccOlY0RvHvRTAR//3/pnJ81MtL+qJYsSXo3OG+58giUwLSh9EX+QZ7k+QxwaVYv/zevJ5Ij/vhPQyzZIJmgj7XYdKtwjt9lyDSeTS9Y1OeIBTjRGd+JOc/sjSONMeOiTqH9m2HPMFC7tupzmyvYWXujppI/gzy1/VE53Zg+AJGKSFWgUJX5As0dQ7zUuinoCODOkpAPK+IgUuQl5ZliTxTLHGCYU68B+GrFazrBaLzBWNZ6orz80CKhux/uNyoE9wZdswRMlb+33Yz9Dx8Ye2E/OxZm2gsf0aOxFMJ1AcqQ6MlDlSgpMtdA6QZesPM1tPKXndsAYEm1vSP2TfbUTsnLMAiVlx/m/055gEFa3nQwKglHduI40XlfbqxPvbhd9ft3XHqvk//6sM+ATUtcBZEdEhmV+XzriU04qmL62P8qau6Dvnxkl4+5elJ23w0CoM0CmQU8V1Kt08XV9wQkBqyzJDVORUNamjYRr+OAIzT9fYCTCn+CxaEQXtdbmIxXLIaCJeJfCoTfocXM476nO5ODDw0epAdIAhZV93TTIsxSv72+Aca4KuCAfejnHerlQdXI41KqO0jHAdrOTxgDlSfm4c87P9+Y2wPeUO2MQoKUCnavOuR3UFBvqWhWz9EtnwrCYZdI04HuaHwK/OK8HIm5mlwoTuZGE/dEDoE4UvHOKviojt+EykN+sV1gs5mo5s1vhzaNcr1ZeDY8fPqDsJmwb26wNVBhgl1hyR4wFg8Vuu5HMNK8XMRr8exM/YnfET/gtvPPaLGYzNW553TvGN1ERbRM9EQnd7wlMKJkXN+s2aujxM7pl++/lc1keM8FiHpR6+vKfw8n7cu4tbImpC84p0JRMWOaTOihEmdOYl+c8d6A2quFRLoh4jbGmRHeI+TwVHa39+L0g8Pm+AU9K3O4wthsMhwf0D39AvykxfngwhPssk/xvMvTIGQZ5bamA6dgwx86A63D4++OYaDm1xTzoJaCzmlpJ9n744ff48ccHrF9/hZuvfiFr4iTLlTxfsl5QwhsNop7zeVLyWo0M+DzovpPc7+nasbaWtohzvPQJgX0OowmaUAIwtvR14EOXAGQXxZTlzjWeY5eP9ywreKZH9gIW7JmMHXUX/DnSaNDuw6OfwUQvBKEfzAuBPyfrY7O4pvqpsAX8TAInU6mTSRlHaLRxt+YWWR8sNtTRbBrtVdd1XV8whsBVhqEBzVhZMylgAGUQmXHjUUt50EMn2WLGNKF/rcqm4hjpPVaNUxkwOXGH41DzxQO90DmzpHGJ+ruBM+DpjhUwhUtitv6LuYL6PDew2+bdI6Zu0PHwd7nRioLI7L/zs2WrEju2/xiMnUYyg1rZEEzI0QbtjwmUJDOCfCjjVXPOKFWzsTPkcn8Zy8AqvDNgoU8GHNvANSssETgCDiclZtw0mShZxW4dguNMmdCqukG52+PQjTg0rpvjkg+TDiajg52BEG1TaaPja1NoiqMa2+gNJ+KPQ3I+UpFMkXbEdNAYysu9ejyJpQ4nUKGdX0NnO6W6I5DQ/Yy99FFJ7pIlyiGP25lxGRj2lBDY+XWCV0ejLMoNh8gpbpVM1mliIujEbRKHgmeAESeBojw9xyYxoiS1+/OwxVB3SHoGQKLnqV1QYWz3GA5PGPcVpv3eettMJDlTZ9eG1DbGR51LO04f5Hzy4u+VU1VsVWzC1jhG5FODeGzQbh6wffcdUvpcvHqjZ+rcEfNz1yzLkLDjxhGFEks7qQzaerZ57X0CcJZ82TjR8CEKtq4DoI4IFzsxjnXC7l0UpHIfjGe5gu8UUbLcnnkxCww0YOfmKCZ0SkX0WTnRFMbRAIJmkGR4m2NPQMqkluQqVRVzgvc/95nGtBCGyjQOJG/MP3bWGegd6JFGTtd1XV8sy+AYrE7tSlMmtIBBHIASArepSTOe81LhAXJEHR89ew2fWLDCZOAW0l2teNtHGHDmC2q6h05sKMShrPG02WsT19931BCwdjqDX9d0aucuixnu1jeqBgwFbcFNtslDj83eVAg9ay4h+DHL0PHoeAysUPIceWr0yFmWUvsMCZOGrpUfA9HOpJHFl+KKjna1x7/4SVVoRkFUapPmQJYIT0HEfpplmrc+PD2h6VvMF6zsY1WgZj7kAokEXqyaUZ3oWBWmH8BN0Ko5/m5RpFguFroGlKhmx0FKlDqHvRtbsONjm+dJOZF5klVf3GTpCEf2h1zkGED0ZX8WPO1cNMYnEfbjFy8lSI6W6nECPDCznRAe3dEoXTLozoWn5w1tJ9Or7nmLzcMW1WaPGrkkttV4ZpeA9s639xLIKaNB57fdb9DtGwwhhY4iBN0eUb9DPpS4c66HqUY/nJ9TijtAXzZotntM6QHTslIDyqZIwuif0exsVi+qnhMdSoIR99mAdOqwLj8ibHdYlt8j33+PtHmBaKBrICl+5zLOn3lOsxjhSDGiAD2lnAmsdaqfPI8dGRwOqMlLOuP4jlLglMf2Cb+72B4XYwBTf01MmIgmUexqpItCnZKAnS9Si8mcOGpJ8L49PTeWaFhi4SYSTj2b70NmxGCiYywChG05USWp1zF0e4DJW7c3rwLSpNldiEzaWM8bvU+GFkFQIwi4U9QiRF/XdX2xCYFp1h+p0FoywhFamMZHPSK1hk0oxNrxBPmFCrCisGneyN90jARWmkoI5GByTAw4414sFtpEpLIXAoeqwdNmp2RAlS29EDgWYJu1ZuY+mOBLUeD2Zg3KDFKsiK1Z4hMOZYmyqrA9vENF5TlXxcwCYJZQJnbCKOAjW5ZUo0uxmM8wyzMsyJag4ltdoZHYT4uyrkxs5pJ14ued/+WxkjUaGn1kSL+iB09uyUCa6tz5hICjmPu7pYSF2AYV5/+odW+capOBNqDUKSGwBEQmNEmEgjLQy7kSJgpP9QSHLuZmuOvkCU3JjV0f67xY58EEk/xnYbeFoxz/+fy4QsmJu/a85Fah29zX8CiXZwS85pqZOzjIMSk4ujsaYHRikqP5MGvIo7utRlxdu0PzvMPmYYeKVNZ1hpGjGXU/JiRphsWiQD22qNqNEqRuv0WzKTHkMwntBN0BUbdD0Ve4paANEwImSmqfMRcL0FUtot0BU1ECTYmR+BY+lj4zZgBTPPdIe8PH8CiScMRdRm3/Dsv9RwTNIxbVD8gOPyCpv0U0tghJjfBCYRcs4hGC3hIZCoH1qpCNccAOXE13TCV9DoifpCiiWNcic1RkbzjlWREmDXySlRboOIkQpQmSxQxB2ykhYLbqpYdPemdnF5ZjC40BLCHw2hOGC2BCYF0YpnOGpDixeSYmAuzgtAcM1cYM2Khkyvuo4PGHSIIcWURmTweElhD0QXVy6Lyu6/oihYm4cUvYJ5KYyECaj55J51UuYRbO4TkqsBIgPnYCWBGmav0xWHBeby/pqG0u0xB4z5XuG1cpE2msfx56bDe55Hifnp+P80k5zLmWYMFp4RRIMe5pu1V3QF+udclgL3c2dggorkNes9DNtmF7nX0PnjzUrWkTpHRhjMz3XS36DDlb0oQmX7hO44Cz945du1ub7iRzHSUG/YCEG3DbYk+55iTC83avYPq42WLWZJqXsl1KuqFAXto43fzW8dMNcX8C+/nkwXdfCEUgpkCjXm72TK5cpcmEYBg4KpnM4IriTTErxRPUO6cOBKUN+Tvkr5PZMJ/rM1EJ0QCn1Hng5zRb3KZuse/Ki+lcHC0xATyJ+HCMdUaR43lWMmsVracc9kGPDi2qZofH7Q8oHx7ww/tHVIcKYVSYL0AeIuxCDEGi+lCtfb6QEPAWlAanvy+KHFX3ul5CVruqxVPb4zBGKKoGSVKjL/aoownxco2UmhbS9T+m20dqobdu1rgoIDh0wDIacZe0yLsKwfY9huf3yMoNlkONnCZBrIWVuP25pPOvWzWfayfty46VGCQM5OzC8KkmulazextzeXni47iD/3OmrWQqlvZnJQO6BNbNY4dO91REtsEZFddPfBxo1DonblTBV2TSwv0mTxGR/qkvMz1KQ2ogMg8bpHRIUzNqm0zVVg6XY1diaMzzRE6JTvsjTWN1CovMdAzYceH553eaMl3Xdf1nWn9lNONMn5WngZ+IBbCAYwIkpBJyQ6ha6t8zcNoDzjkz1Qe1GRcZmr7Dh83eXsN1BLi0SdO4qK1RTRN2zxslCNKHn0b8+EMioN9slmNO0JHaf5M0Du5Wa1W3//D2GyyKGR42G/zx3Y/OEIa/M8OrFy+UDNC3nr/XHCq9diAntdo5nQ0YqZpI//kgxNOu1KYzz+imSOpipvcm02DJz1ZfhiHQiMUlIeZOaNbSPGYmAl1r2gcm5Tohm+dq3+7oKqdRAauoWI6O5LTz+GYECroRi418GRSIv/CtVJ4TA1aSn23tadvheSnYsaFMb3WwROPu9hZDyka/VabTSEAVK6UR88VCbe584EjB4QamAHmeSAY4cAkBg7QPAPe3d0okFurCJOjaGm1XYbfZoSkNgHrJqkkHqxO7Yx0T4ig/oD84Pw6V6q6qDCLUQ4M6rDB9fI/uX/8rtg+P+K//8gdRZm+YAN6yRRMgStjdoIdBIAU/jgtIqRtp2BNG6NgdY3va3btl3eBhU+LdtsHvDx0O44Cb5wMyshaCDnG3xXI2x015kJRvFHvTJ7bhfaPc6Q6wBR8MuEla3MU9fplXSKYNHt79C8of/oDFZocXXYVkKJGMBADyXr+8Q3BoW5QE1UncybXomQyIKBBiksGZAW2FrWFAdy2go1i2C+jqUWmad65YyQo8QkiqpP9ylGb/+e1FHItFHQNPNzRKa0iKLUeQdGXMOYo0W2pSjGfE/DAl4EiAWCZiZtgJ2r7HuPsRU0+sx07YGo7i4ijBoiCzKcNiniLLE7RRjyxsMQwBuoTmZlelwuv6kjEEZ7QtVnyc/bOaZ+7Nv/MgQvL2mSAQqc6KnBKveRabQx83FRcg2FJukvY0h5ZHAhXwOMMzAZ7pmBCYrDDRyGIr0C3RYEbuOCYlBDfzlRTPNoe9gqaPAwQzUtiI711Rb55ujG2rv+dMMXLWp6qCx1GKh8Zw4GcZUYrrz5Z6qk0iobZ/mjgmw+cviQ9zc3PSsP78CqzHz2fsPwchOxnX6Fzy3PSDjjGKWyUA7AZEhdk4G4DQ8B2aqzo/em2ybK9yaw6d0JG6MUwIqBZp1EpjIZJ33R/ZBh4gxiWDGvo8SPwpMTtkV4mzMyTgImfK2vVHUU+9XTYxIgImSsSKiRBf39Faj+ZHn7e8roSOUVXruZnS2TfXvp4C1vqsH2tMKDH2tRKiXqMDamRwZk4LZNPOsArXNA6VDRNYx88dxKId1vWAehpQHTq0FdvpQBmlOIQTdkONfT8hKBuwX1YUTMBGJHWnBEWaif6p9IZMXrXSBcM8HLGKOyzDBmm7QVQ/Y9hv0e72mJrGjIW82t5Rbvyyrosl7Seg6PFg/LngfSqbYF5zN/o7+5kjxvBP8hL/ly7wB3Lv0LjRqzF+IsRwZlDkOwMaCzKZyFIlBFMc0l1ZmKCOIxZwlNkjJiuo5/mhG+MBI/UR+kqMEYIHxWRQh8y++Cx4JdPYdQMniaOZ6mkXXzEE1/VFSxczePHJ5wyfssPJscXPTedwOJjoUNtp8xe7inzsxQxvXr5QoD/stwrou91WHgd0kWOFz82XowHzSLDEgjblxhV3SPcj2M7J3rrApgDKyj9JsdnWAgJ+/+4HfHx+NO19JgO0Xv7uO/2sUYpGJQXS5Hczbmfoq0C147ghCLDZ7tX5mNP0JnFzeYfOp/wsq/pLVkopVIYWBWYNSJDFqV6bHYE2GCTK0h2cmZNzmvQAvabt8bQtUTcDsvijOgThy0BdjOHMVIYANHm4d60DqNlcl7Nh72dgwK8RcR4jGGN1RfjZu7pCSfoVhab4+hoDsYUaInWCP0ma675oOWKhqY+MfxjcbUJPwaopn6kqpLQwMSXsDnCU5LUP4tg0KzQyumDJ2padH4FXTTRLiY+AaQQvmpa+ATT4VbHWB6KtvsbuUYkpk4GhHjBUPZp9JUWjaSDQ1eSBw4BUvw4Bu0TNgCFZokly/PDdD3h83OA2n+Ht8h4P+xg/rF7gu7bEb7sS26pD/sNHJZz37RKrcoZxtcPs0CEpYkQZ7zPqI4jkL6lgzcFjagoEeJW2+KfZHrN+g9nTf0f//IT9736Lpz98QNulCKm3IT8Fh5mQQ99lwSvNZgiqvQFWidl3HiJckln26oNMdlxl75OB8xzCcP6nCG9XwP6GnQbCkkVKKhv0FEFiF2fwSZElD/pMZCewE0FxrfncHEtXK1EXD2ODauqxnVpsuz1qCXUtlOi39ZOsnKf9TnbKUfOMqCdQmVbuE+IkQDEzRVV2FOhqGAczeVck4YQ8YbJOk69JxcR1XdeXSzt02nmGCDYikK8sZXzDgCPtdNt8fAvcixWx0mTg4RYgMSJWYHGnqlZKhXIZs+ruyFL3YLAzkSD7g5s5nlEGGdyfdzu0Q6/5et2wyuP7mOWyOPguQHCpO6AEZDwGdm68/PN2T15BICAijzUcM0wDuxwmn8pkiKpqDMyXLAm1yKTIqiv9Hytv8aR5hVhxsRKPhbb2FEVfBfNYGbgYfKuqUWXIz0WPCT+KsM/lJKJdkqCW68SqR9JuR5Uek4tmAGf845iHO69ZR/vrfaSTOXiidQrIaggxsmqieJFDlUvkRR2Ck4iSAqrDdZyWEz+i5vzF1A1/z/gi20v5/qnIj5LAkDTYHknYI4049jJq3fHLAdlOfgs8dkO36xxTPZJF5xShnSbU3Yiy5uiL9zmliiM0UYImitEFETqCGgeFG/1M27KFPeo6BhHvR0tkiPew9+TzQJbLhCycMAt7LMIG2VQhqDaYymd0ZSXpaSPYMZgds+njvP2SJb0KMldUFFgny3FMT98/WX8q6v0nmhPO9Mq6CQ7rIUXDHmNdY2hI8+O18L/jPpXrKHjMAceAYRJj5P0amYlUSdAvFTXHAA2Bzkml6xR0peii6CpTSiTwksBD0jfZGRNg2bAtR0qqv3U8U1KgZ+t2Xdd1fbnCRHSQYw/bqZ+RN+yFdfgg950JDjEhkPMbAwkFPThHZKsfAVpWtf0ocxwFMNqhUsxI80nTJ2AVpFleZPrp3ueA1CYGtWO7OU5kcsSgfCgrff/uu++cBoKNJvzxDe49jdHlKhf3kHswoakAMmloxGgQTqBIsUgjvF7TzGauv99VtYBJeyeactFqzeyHm2EWpqq48zjBjJscqXl5plZy0NJtr5ONMDEFBBnWFTEQA7ohQF2TFtaJIrnkeKar0TQHYOKG10tYiep6bbkzjjVBntyIxxxBymAun2e7ltw4xwHLWWEdFMcmMVk/4woax982cPHhE7tmBF6NdBOW655nfhMfEWDq2I0wESACvQharCujgpKlEMUZ1nd3aJoLcRlMsJh4ekS6yweU7jDBGWmeRfpjo0Tgdtlhnvd4mUZ4mc1QtikeKb3b8bNZN2CezzCfLTArlkgzmhAN2JdUBexQNyEOTYTvyx7PJfARGQ7ZDF2+QjC7w9BuUEU79IlROgmWQ7RAENLZMEXVxThsA2y/+4hk3qHtF2p9B6zKwwDrtEYR9XiZV3hT1HgVPOPt+D2Gw3vsfv9fcfj4jP3HDQ7bFn1Bj2onHc3nj0wQsUIu6xC8ffUWKefupGR2nL/zWTQrYtkuO1wGcwUlnyHdI11ScHRqPOkq8P6J85k8EqQ1EoaowwkHduTGAeV33ysZSHY1QnYJzh4zviZxNkmSI+JesF5ijEJ8GFqUbYXfP73H+90GD2OCH4cUbTTDId3as9+XEpGK2xZp3yOZWsThqCQgLzLEZBatV0pMw4BfoT6fhM7qElW1w3pZ4MX6DpEYIdd1XV9qQkCVQlX4DrHvOdzcY1kpq3IdJPhj9DIvxOroXHItNEdBUpX05fjxR0c2BRWHUyAo0UVuJ0BqmwFlkdMURZpKc0BBizzwrsN2t0dDPQR3fL6qkyiSuNI2QzcuvjOLcf4M1t2gqgnR/GxhR7iZpRoVzIsMK7bxGcMdqryiO9uFYC0zL/ICLmYhzQSJM0tWZEmUYWCrMmvVKRin2qRjNaqhRWEvZTWeo7pigB9Q1xXqhEwBs2/1pkf8PnStm69bRT4kfB9rpxMAKL0AaTsMOgbytHh+Vbkd/QpcgXhWdYu2KBFCV886W2E/bzefeqOeuWm06844ZoervEmrPIIcP3NZvncq7U7AdOO4nYyzRiHGZ+mE5WzCKg2xzmIS5/HI+4AdJQdMZIfL3DkpWEWHT3OO7DomqoG+Dn2APf/MQJ9kmBKqEWaYQjISLAiSMpoPE4Z4CdBdkWJc9MboInSHFgg6RLWTpRTmgpa99J+YMIsn3CY9VlODYqSr5w7d7hHtdqvuQNeOGHPr4nig3ukJvOw+LTjuIZWSe4CEwpikO6MlL0zkLY/PVDfd2T+9kJIBTzFMEaTZEQMQdA2Glq6lLQ4UcBpGLDqqmZ7orEe/Aj4fpJeSMcRCJQzQdLU6A89VhY+HA56GGE9DgiHq0aTmY5KisY7L0CMiqJnS0XI0jKW3wZEVwbAyRJN3CRNf63ayi8lktS/YLWE34soyuK4vOCF49eo1nh8DVIcIfVdJpZDBmZslM3YCxIQB4Ixbsrf0KGjw/Pysiozc/+enJxkPSVpWlaeRsn1L05uWcDNoBPZySofififIw0x4hDcv7rFeLfDVqxfaLPjaRHP/7//tX/HxeaMHlwmCdS/M8CjOEm1NSj68GJC86UmJPG2iDJR5ngpA+L/8wze4Xc1xO19gnuegdXrNr37EpmJbuMW//dtvP/sCFIulsxUe3IZE34EYc26WlIoOY2HNOW9mVT9MBjakQFHGjYkVVsoqliBAA71tNx/R11ukDhzIjUwVDr+7qpHjFJ63p91GvPY45CaX6FjqhtdvQkVwqBzljEJmrdlI53Uc2J7mhkkwISVmKwUv46fzs2RIGDxdx4eMkI6qb4OxAMyZztDh3qFOrBSyWLQRf/5KuLmbg9ERf8IkQyBKp4rH5tYqo6HRhK/vYry4CVHUHdKqwrjd4emHB2yfD7of0jTCar3Eze2tulJjM6AsK2y2G7E42j5AOUUY0gwRMvzdPy2RJQFevrlFEwxougbt8w5xDfz93Uv0U472/tcYsxWyWYwki3BfJHg9y9AVS+yyW9E8x5iqfcAq7/EiG/E2q/F1/Ii8eg9s/oj+8QG791vsHg+o6hENNQJ0Tu16yZZYCdjlre2Krfa2RNeUAuEJ08COUsBnilfQvEpiKjjKUNoBZH2PiJAN0naZMNJkLE0Qv/0WwXyFgHvBMCHePSN/6syCSFKDZtRl44RJIlC++yNhqNlMDpG0Medz+TgMeKZk93KFiZTXpkdUcqwXYuh4LSlXbfUAFQzJQ+G+5HFQTIRpYja0g4kh2R0kYCrFpEhl5bPDDmhVk21wdTu8ri84IVjf3EiUh6Atynsa15s0KArBhDIy4oMVl6WruHu17neHg4mX1DWeFKyb4+zdMANnb6IWPlvTjt/tFNr4eOZxhjxNcXezxtdvXuPl/Q1+/XdfmwxxR+GhSnRDbh4EA3JZB4JjACYEmYIDg6GAhc4RT+1HF0C4WTAwL+czLOYFfvWLr/Hybq1uBOl4U5xijHNU3YinA1UaL3M8y9jircwNjoGMMskUciGIyUtFs3LnMUa9ibDyJEVphITcaHVLTPWPeuwEHpb7Dfo6xELSxgQGUmehU4ekdy56Xc8NrZcXRdu0iMIMUZhqBFGWlgh0rAK5iSbUl4+xuqkxazM3umH3wVdQxC3wPARC51vnwiyr1YlQlWugRh4731tgw4hJj+DphnI4BrHLEgKeE+IUjLliZlrWLnCyNAw04YQinTBPgRfLCG/WRvOcti3GssT+cYNyV+nY+NlJW10ul0rQhm5AU7bYPu8x8r+TAjW7N7F1XV5/c4/72xnW8SSku3QRDiWiIcXbJQWzVqi+/QcMixfICiYEMe7GFq+nBvtsjkOywEjMjbAWE2ZZjHXe4TbpcB/tEQzPwP4Bw/YR1XOJctMYFoHiULwmovsZtuZo6HRhUsAkjhV839VIh15yyTyfTArkseGMN2Kxdcyt00KqMxSSEFSAiYltkSEoCowvX2Fa3yHg6LDpEPcd0udHjcqER1EiILmwY6/DgK/sDiSiBw5hiAMTUtKUhxHbcUJbzDAtF8C+QjCVGssNNa9lgCHMrDPo5Ki8OqUZNdm4TB0y4k5cJ5FJsjlLGuWSxQ61Ja4JwXV90QkBlQOr1Vp/7vsKZblTJUman7WZrepm203yseRl8+EhvYdCOU2jdiBbrUfXsTPgm1qdR89355rmKI5MOO7ubnGzXOLmZo1MGv6hXo/VM5UF+fByRrtaVma2A2v900mRm8dyfaN3Fd7A+SvwPahrkJPD7LjRfF12A+QTkNB6JpZBUMXWPJOChK/NLsJMFsqXrPlyhb57VlBWR0U1Xo+a9q9qzZbS0OdnENaBNsystqkkWNcaKzBBE82MlCoG3NgMe/rVQokNRXE2+1oJXFmVRvukfSuThwNpnq21USnM0zFxs46Nt4oNwl7B/f6wkoqhKeWZoozGEgrm1h4WMp9y1SFNYQw8SuQG36+sam3EeUa7XyrAETzJUZKpXUr9kSqQ7WUYAnfUR6MoT2rzrXNq1vM8LbIQyxyYx0ARTNjta2y/f8DuwxPK/UFjgRcvbpEUM6xuligWhYk0NSV1upFOtIbOEL+4xxClyL9eYghi3GQ9imiU2RDn91lW4M2rX2CibFbxK0zpEruvv0I3v0EuC+sAy7FDPnTosxwznuOIYl4dsmjE26LFV2mFFfaIBvolbNBvntBstqi3A+o9q1tD3TOQUi1UXxwJuS8D837+6sYGcTRhIiuEFtodO0dCBZ2ohM7A7Hh/CEvAEQGTM/4zJbBjRGQFzOYIb24Q3N6i/fiMsemUrDLQsu0ij4hpMPEzJpQGCZGlc8rnktihNAbvlD2pxEzs1ysUYYhez20MbA/okw3KQ4Om3hqTyGEgna+ViSzxeANnveVotxxXeo9Ddd9IQ8xipHGGIk9kO25izdd1Xf951l8VzW7Wa1XirJgO5VZB0dRpjSbYtVQFoyhNpqCqLZit/6FxyYCbwTkTHS7PF+fyqHPNts+qgYzt5zTFq1ev8NXrl1iwXZjnzpSnRhhkmM/WokIyWSibDp0sbel0yGo0kC/Cmzdv9Pqb3c46B+4Ybm6WWC5NJlmVqwM0MtGI0zm6KUFLPjoDXUyzE27yOZarJcL4suC1vr3BgdK1DOSc7/dErTNKGiuirAmOhAKNQFouIaBU8XjgBhYiCUmHIz3uIBGgHRHT3Oxe9BhvJjztK7x/OmizpueBsBL6bpoMvKbjyP820yp+Vo8T8dxzVu0vX1RYr2gpTQAgZWwn9FUrb3lJ5stZkvRRAiUHxFQvdBoG/Gy7PccKUCJlmAHbaIexVcDhDHhfbpUcXbZcgvnJlwkSmXJfrfOzmqW4mSVYpiNmwYjH5wM+/uZHPDEpeN5Knvirty8wW69x9+IG+WqO/eMjunIvWV0mBHQSXL59jWS2wOrFt4iTFO3Db9EfHnEomYD1mM3m+PabeyBaIlr/I8ZsgY9vvkYzm2E2DchoTjQMmDGYJgnms6Xon7fhDvOwwy/nNb5K9ljUW9Hk+NrN4wfUH0uUjx2qHe8dU10MyTAIaBhGcS2KA8USS7q069INVGscQV/ooAsQtOwCmBWyQUmsU0TjH7E31LKy51gBnd8p/8sEarlCuFwienmP4PYlmqpFvaVQGX0kqBcwYiYxBo54Rpkbsbsgum+RI5sXSsyHLEFDzZCqRUPszd0t5kWBaDbHjHbNTxsgKbB93mG3sc6mxzcQ1KpDZJIj63ZLCpgQMNgzIaCAkWGWzHgpTg3bM8uJKyL759ohuK4vOCG4u71Ru5kt9f3+GVV5sHZoQ0EcxwIIAmTUT2egUqcgdPQ4Z0PqrIfF8w2cx7nHD7iOwPn3c7qU2fta21lz6XHAoaolQ8qgxE4FQYXsALCVr+DE7gUD6mAgsKNEr6PAmQgQN1Buml49zYxu+mHCoW4EIiTNkIAwgsTCOFPyIQnnC33mDZzl5GklqWuKguyXCoTZWaeD8ipsoQr/EIyg3RLlk73Ko8x76ENMsJSQigFaIuDLFnXVS6fARgXWJrU2KDfuBHFClgffx6lO+sTMHaMqPrFIejEdCLrSCEOzYvtZyUB7qqkUFjvUUWVuRbxuMmeyWt2SBOObm0x1p+6E/yIn/JJlFtHjJ1bB3iNQjAt9EYwfIufxNxyl0IvgIKOhUXbbsTAay9sFZus5klmCKAuRL2ZGnSx6xHmHeLVAMSsQ5xkKAjSZ4Dhn33yxRLK+R1PFCPIMYzjHMLsBkhlSziryGAnV/cSG4P3MQBSjoBpiCKyiEXOCHqMWWdggGiugPWBsKnRkOFTUSqCsNcdrhvKUSqACKe9nU9s0YORlIwPZ/zoFRkkz6+2czC8RBFIOtJ9Rsvepj6X92VE3KS0u8CHBqhQoo/0xtQGUmJp+goZVTntALX7nQcxOXzov0AchWgVtoE8T/TdFg2gh3Q2TGEV8VjhyM5wTGUvsuNlxcv9hkOc92JPhIjAvwMZCQr0HdgmcPoK6A4Q/hBw1saMTIndjuuu6ri82Ifj1P/49bl7cYbvdCYGfpgU2z094/+P36Cj7WtEOljr2qehn8/kMRVEYwK9tpcHPwGoVqSkQCk/gHdzOdAb83NdJz+u7fr6tMeUpgeAKOu/KvYJq3fZKAH7zhx/wvDtgfzio+jUt9FACPvu9CSexwmYFkOeFsAPS4z9LCIQzYHCdBnkZ8He++eor3K5vFISZEAikKK7/ZcFLWgnUb5D+AZXTavMdGCl6wnk6wVqA9nwe11BLfW25WhuY09lGMyB3nP2zhS8wFrALa0xdhE3VYbNvjR7mKZydBUmOgQie5PhhIB5i6jCqFXzKCJiEyRuhqnHYlIg4lqEqHKy67SbaRpvnA8cbrPRKavjXpRI3qjoS2+AV95hA8vNxU2bCSBh5FA6II45/OkzySfj8xfPJbo7uIyfuY3gCOt71SIMKBTsE6LCm1O6mkklR+f0H7P/4A8bdAXfrArP1Cm9/9QrF7RpYk84XY758hSgwKii1A4IsQbBeCClvU6sBTTCimSbcfvUNlt/+PfaHEA8fY7Rjis20VhI2m7HtTCieIRsYwFsGcQS4mUJkwYivkwGLoMVduMcy2ADtg2R2u+cHlB92KD92qLcjmnLC2DGAhggnPnlE1KdAnCGIBJ3DpStKQ/RRiIZ04yMwc0Q0cUzFziAFqkJ0vF95HEex4VNiQCErJiwRE1F2wgjgTPeod1vsN88IKd3s6K1skhkOgWBZCl9xTJBgdrNC8eIW+6HHc9viMI6o4wgtsQxFgSkrsN+WeKYmQ8+EleeZHcSZ9o+6oSnRhNWKY8IUk7Awpppqo8UJM5I/4sCZXwn6LBwDja04BpsXIW6WmY03ruu6vlgMwbywtnkYCHFdHipVX7vNxhQKm0qIXE9FksYAW3tH4Z8AY2wBnxQuBWdnoexFjGx5ENhJVMWL8PhWv4SGpMJmhj/b/UHBtWoMo8DqQGwFp5surwXREQ157elvpqfAKsI48qwazDTFyaYKZWzKjGnKMUWiL0mlyuToslYsA7neT10JGseobNaXr7X4vwxy6hhIpfDUhqem0MBa3WvMn5v36Ge42VEB0rsJWvVsoM5TB8Ym7B665f/X073s9aj4yKSANsBdnds5pm8Fuy8NQYuUijWWyEBBHkrA8hqz9OJ79q2qKsOikpEQo6PwUmjt2mnk9SEj5GeYzWpY7DtNlogwSLGSnUUTinBExo4Vma4UqhlaURDpBxHMMky3CxSrOfIiQZZFmJiBUqWO97Tvi/Acsg1upEKEIyltZC1OailnKeWtycgB8oRIeuIEKnRBi37KEEx8HTPn8fx8O8YeedAhD1vkQYt4ahGMrKBrdQcGjt8aYmMMu+PkNhx4z3AEJtjljawuBxWqw0ZGBe0cyDHgc8MOgaP2+ismRcojjNHOuaSxvfAVgbxdj6ltMRB8zEDO701DYJLJXByvoeOQ8vcoFMRnVmW64IA0XhQ+glLFfB3pObKTRSxOw3PmNJRktmQ4JFb+XPN5ri/5GwSDNDGq0hRImaixI0D10POngU2KhMJF/GLCIGXR67quLzQh+PqbN3jZ0Ra3xv1qjV+8/Rp//O475GmG3W6D7wIPMuNDSfpeJN620X04Ex8Qd6b2R2AZq+uy2juGwEEsBCUPMq73HQIL5NwUykOJfZoAt7fK+CN6EYxs63f43ffvNT7Ys40qAxNuGV5JL1LQ2mx2BhqkOE4UYZ7PxfkvskIWv2otsmfIDVC5iSUITBruX7zC7c2d4NRC13vU9IVKhVVJIygT/OFYoKZ6mtPJF1CQyOeRHQtjZkibndSq3U6KbjIiynIzZgo6gCp/8pKgTCsrsgGEBJhYH9vnpFzyHJvne9vysxA1TWwHgYa9wGLmYeA+G42RMGG/eQIohrQtMGyf1GY/9DXaccADRys8PucvwX3X1A4j5HSh00Y/2Kinz/X3/RAhrbnR816h0RWvRSqhmEuWKJJql7ugzURvHARSW8UT/m42Yhn3uO1LzCkL3NYK1verFPn/8q0Tfhokh5vdZghTIJmHCLJQNtLsmCiJGKl6FyGqbXQTtKa+eJezTTZDnnXIukdEXY2h2+qZmDcUSY7x0L5CFc5Qx7doI7IX6MhHQFuLRbJDETS4CT9gFlSID0+Y2mf0Tx/Rv3+Hw8Mznp4a7DakiBKIx/zRkkp2lNgZQJxY/5v8fly+tocaVdmhKgcEY6LnJIo7JFShIti1ZaVt11zXwLEMknEAawD5hPD+I6j48Qnjfo+Ko5osR/e0Qbyj2VOLQuZpvJe9uqSJDwRMyOIII++bKEIzDthTe4AZcVaow9IQ4Ny3OGxLHDZ73QPsoJl0uY0CBbqMA/zDr97i1et7BXbiJOmg+vHDg9GRideZyKwgfsccU/jsFEmKeZFgMeN9yk7JFVR4XV9wQsDNmnN0tqqb21Zz66qqcHNzqw3/aTaX7sA0UmDFVyusxo0+JTtkR/FhoB7HCOnATgHn/wY2pKKaQETHZ+20nXkzHyYNqnTVTrQ5n7oDdaNkQAwDJ6DuNwFVzBLBIY3O5v7mwpioHcmOhddUMAc8g6JpVk6DpqzQz1BRT8HVid6xs3DJ0vhEFCezyxXi+ajubnNUo2YOn9gzq1vS0u9+xCBdeSA1nySVRWa8w+SBWgzOLc5LyQkMZV+idckamdWe/b6aHvpRV9X50QFppF2NtpnQRKZO10+WqEiXgLN7cra9na3kfWmyFB8BXCY0Rcg4xwhmm+um/eKAM0H7lIf61y977zNTHefiyOELg+4injCPRqRTj3hgNO2EvaDVbbAyi2adEp6IlHa8lN62OXLvujhj2KEPyM4IdU9TO2EavM9FIbOnNKBVdYV0KJFNzwLLBUpWI+FshrBDN+UIRucPEY1IpgZ5XCFHjXSqkIBJB1kNlXUHqJJZt2hrSh1zPGPdr6Oit3r5Nm+3e99LTF+WFrCl3vYTWlpxM7VjZe/uNTvJvqPl8QonUy2fHEjBkMfqRoZDtEfPMVVZKblnjc8Z/fH6qdkl0wJlGuwOCMcgTI31ZQZdWMMscdtRQUHcELNg3m9kBVFNUwN/oxZQ62OxyHGzpg03ry3ZLhP6urRuF1+DnQbDoR47nsTRpgm7A9ZpYHJ7Xdf15SoVBgFmRY5ZnmCWZnj78iXu79ZYrZZ4fn7C3d0a5WGPj+9/RKPxQaQgzWoiy3MFM0oOUySnLFkxsUVo1sjrFR/ShboPB9rummuuRg7r9Q1SqYelqir++MMHlPsK81mOu9uV5uVsTTM56BiwOoLTbA4oTnpMGWDTKWdQf/PqBWZFgW++/Rar9VoJAY9RM0ZS+aIY8/nyqIhIVsWSALGMhkFE6FPZLBSG4NIaYb95RDsyGerQqdquTf/ftV3ZkDVgk0NkOXl9a5CyBU9KYGcdjyyzli1hCD2QpQSAsn1KbrhZU1PjgDt1UfiEpxMjIM0TIDLVx641gSmOgLgbkm3BTm0xhsi4yY+swLZI8gg3txmCJMLLxY25zFF4hkmgOgIG/jTVxUCbqX03yijH28JkCkTIeazpx1/KOiRHPnbAREn3OuzALBqwSjq8zErMQ1IDHZ+eGQ9BrskcyZK6v878ULxFm41LMEliPOyB9MjzAXOeFI2ZdFKklqckmH4OTBanZ4SHHYp6i7j7UcZSfbszEFvzHSossIveogxvEUWU8E0xSzu8CkqkYYtl/4BkOiB+/u8Iygf07/+A6t0jdu9rPD9OIKmEzTPJQrOOZQwlroUsHyrzubEW72clWhcsMib2LdU5gYJ6GcQKkMkged8eYUr8wiTmhfUkXELrck8vECYcS9sImBhSjIjFAp+nkf4gofA8UpeWe/UkiWQF5KyQBkhLB9K2x4G4ockgoojZ+mf3hkqHg8CKFDtios2OGlPXjOwIUPhrhiyN8fJugZf3c3P2jIB5FoiGykSAoEQ+588EmRIcW3NPabGc5VgtCu1hEvy65gPX9WWbGwUSzqE4Dmk9Xv6WWTX56WW5l4shA0l5ID7ADIcYbBnYNa92zoIeD5CkJiLDn/FIaCKOmRCw4OLv0b1QwD+phI3Y8EGtagkGvbhdqE1pCmZEsJuSGFvpXlOA7Wn+G6v/nBbJNzfSK3jx4oX+bLLBodTnaHVLRDJHEkwkmMgoMchyiaHQSlWVjwMVJj1HIp+/mqbWa46s5gnoY2nqbF2djZR5yGscbN0JzURFmzKaFGfwNAQiP5sblcR4OO9kq5sObmOPsG1UhSapVWC2vFHNKA13BhGOa9qEBXOPmizvaUCeUYAmQNZPSAhvqMksOSBOTCgqyQPkLwqEaYyeUrwyNfJGSW58wN9PCYLkdSAOg8fhHAgnbtrBMQm8dJ8ly0JKefqgxp5gCzqNRmQhA3mLGZMBOtz50RRR5jxhAe2YTWNBxlND+Amegja6YFIgAIprgxAy7xsfGiVxTEEQZYegGxB2z0jGj8BI/YtHUXXrgd2AuX4uCg4IqeqYZBphrLJWo4NsfEA0lYiqD0D5HsPhGe2uREtBrBIgNlNMDSe6qUMk3VejAt8h4DNqQj+XLCbB7BBQEbBlR4+wYSL9pUTJCttK6aMZEJfMO0/yySYOagZb1lxopJdA/L+wGXxeJf5FCojrHAnTI3cldQJ6BmJSY8kusCHUMWmbJqpkmuS0HhR3bniP0biIz+wsJ5iYz3eKxYzqnMYeMIiIY+DQqIrHGJiImZkfjWKe0KGTCbKAlRfep9d1XX/b9seaXzvan2ulr9c0enmD25ulOgdlecCbF7c4HHZ4//69ZnNekpcPKythJgSsEvlIiX7onfGkGc+AbFgCdhQ4onj56jUW87l0+HkM1W6L7X6H9WqGgjrm8wJvX95juZgjm8/QdL3GG3lq1sJsVvL92IHIsxxv37wR+2E+X7ggYBt7HPeq1MQtX90o4NvkIEBeFEpKyIogIl/JQpZezDJ4+5IVJTsNEZp+iY4jFMaYiAGfyRINpSYcKCzUUkiIFK0Od3dL3N3TaAco5qzEQ+SybIW5xDHgMBGLIiyHAXek/XHTdYmXU4ZVxOb3mLztJHegLFOZbNUh4OuQBTAhamj4A+wfD9i+j1DMM9ze3SAtEqxezhFnkUSInC/iEdCpbodGwN6bwpIQJTNOmlboSKlUnoHKPnPlwYB1YfcnRxbsEFBnYBUOuAkqpN0HRMS6OLEbe0dn7S3nR/M74JJy4rEZ7v0QXLojpUav18D/tODHcQLCXr4SDCYTSL8smUkhHCtVw7fjOyzGFMv+Cc2UYxgdpoJeCm2GMRrRxQcMU41o8yNQfkC12WK7abHfjqgPTCZJQyWrAuiZDKstkWJIC0xUgZT3BQPqUUrws1dG45+8RNiGqIYOH5sOTIU3bhRTiEUQYMZ76UwAIo0m/ZzZOVuQJwPDNAzN7XCg1HQYoUtjhAUlmwMMORH+nOYYAKaWSxp1OWochhptFKNM5OKkhEeiWI76LKMlCRrZ9WARw32Kip73t9bhXMwJjaS9sVGiKTiFoVOgl9UxKYmLOYos0ziJ1zJlV0ujEKM/n2uoXNd1fYEJAWfYw7FiZZBfLmfIixxdd4P725UC5ou7Nfa7Hf6v//bf8MMPPxznyTIgaljFmx4BF2etmp+6pCCJWwVBBgwCFBl47+9Zya/R1pUU75qqwk7VfCfAGoPh6/tbLCl/WsxR9wNW8wyzLLGZYD+KYnh3/1JB/f72TqOANC+sInEYgzBm+50mNhkWlKlNThRJCiExKRAFkMZHiZnd8DNdsl6+CNQlkfAf9fDH3HjPMYM8E6lC3OmPH3aoqw7PcSA09OtXK/zil6+RpJMSAgZWVkZibTgPB0NVm4ogvQSOhlGiTxkATrNVzb1p3JNZLSeyggEPWWZxJEE6HaoJUzPh4ftYOMPZPMf6Zo18FuPFfY44C9EMvTZmBidOlY+jfN+WcDLHvI+UEAi9bZu6+2djS1ywipBVtuFDhKUgBTbosMaA+dAgaT6KLge2vZUZJQbGY+gKLFH1+BdWlyfMvF8i2rnv3g5aUFoFISo7MiCxlW4mx9TpINWSyQCTgwEJWQNjiJYAxoHV/qSqn61xDC8lg9wlrIFbpLsPCKv3qHcl9tsO5R5grkYwoQpiJ0Mg5Ai7L2kuamPEGYJm8JfXskoIKJWdBtK2aBoTI2MSkIQhFklEEgZqUV6dSiRlwDl399ZkDv9CmipXygSVzx6tvTnKSlPOuTDSmGuW2GFTxIyBngJYXY9N3WBTtwYkpNx2EiAjfkBMHI4IhlNC4HAyLDpWqxlmswxfv73BrEj0JRgvixwGdz7HYrlEoil6tgb3D9Jn+7Y+SwgMcHspoPi6rutvOiFQ8HN0QpP9NU67KX0BWZaoNffm9SvU65VkaBmAiQmgZj5dx7SJRDG+/vorJxZEkCCFRGjXa6MCAvckbkNlvabBh4cHVeV0HdRMOo6xoMYB2/nSNY/w5uWNtAP6cCPtAG5OROGTAcG5X17McHd7qyBhSormA2C6BATlcX6ZY3UToChmyIpCyQjHBNzqZ4sFZrNCWwxNgnxnwxB4n7/uX86VEKiIc+eTwYXIe75+EuXqDOyfD2TGicZG+15uaEVBsSRKL9scQRRGzWLNcMg+n6teHcjTCy+Jfnn239bFcfNytV+pVWAqlAY8HPm2Jo+bRQhyOvpFmqMyNHYSxWEi4IGKDhgafGo1rd6BCwg+UZBrnoCP/hxcdEqP4yhjlHiZWmIbErRDgk2fgV19D7hTMsokCS0ohsvkiroIOnR/eZWwOLCpAG/8b0ti7Ef4uZhAMTSRleG0JIYO02GLbvsREwG3u61m3EqMe+BQERAb4NAAuypAtBxRhCtEaYxknqgt3zQLxHWNTTXKcrlsqEfBtrbRAJkQkAZI1j3ojpkQdR8jcKg4il3xHrpkkX55c7sQ3qd63KMZD9JiKFujClYjg3uAGYOmEk+7r9IJSDVBmRDpJucUxradjPiGMBKuRcZoEWf43GdGuTeqvndZT1NWsvw+kJHU9lLnnOvZJQ3UILLt0ImNw+/t1Dn8ih2LHlWNx3hvhsa2EZbWUXUdBdcSYfJiDZPE/WWWZ5iGhd1LuidsVHop5fi6rutvOiGQoMy0cuI9bLV1pvxFdbEAAvlxY31NB0JAhjAfHj7id7/7HX77298qoWALmwGLlSUDz+PjFlXVqJNQkoLXE19AQ5FJ3GCOGP7tN79Rhf72zWusl0uBAOlrQLlhCQvlKV69eSEwEK1knzYlqsNeWIbFao03X71FXsxxe//qiHkQnoFAK+fNPgYhiizHOrlTErNYrJRoaAYZADe3NxJa4s5CCebwLKhesr75FbsVZsRzmr4aQ8CaqikadgYeNuqOkPKUpwHWy0xfElJZ2IyWAlDnAVVUOKf46KLzMUHwpjfsIGjjTmNTIOTIJMk0CmlabtB0SjTfg471LtH2swRYZBiTWBUhN1O6KDKr6R2IT0wQbrbuPHnFQP/e/ku/PfF9vAmR6TJdskg9IwiMAYVjI2khBBH6IMM45fixmSMYYnkasNpju1qS26hQEMQXdkii2qRsORsXQ4L3SoQxpOIgcQZMBjiiYaAjaJNKnJWBN/tKugYYqG/QYNgd0LynA2WH+rHC0I447OjxMeHjU4jtLsRTG+JDE2H2ssWbaCk1vgWWAtrFh3uEhxCbXYfddofNYcS2M2ov47yRVNlWSjCkc0zFGiPFk5pJM4UgZlftMhOu1brAYp6grRd4Fwb4OLQaXz2UTOI5e7d+ie/Y8dlhNyqbqFwamLvgeNJI4L2ZUhpcDB5j+bSc2QvURzqys+lmYsD/psVzXUt7gCnqKjZgasL9hPflNOFA6eumRNlTaKpBHiXIEoI1mRRaks3nigkoRcCGnsZhDq3jGDUaazKxYzKQEDcUYcViIM+lqMg9kDRmClElFGa4ruv6UhMCudepOjrR+bhp+mU2sz7o0NOA4h8zrNdr3N/fG5K+bY7MAXGCo1Qywz3ndwSe5Zlei5axNs4l0MhoRQT81ewiNI2qLTISdmUpql46y0wLndxhCvZQRc1VBsr+GdBa2xTJfJAwj3rzIdJ8hsR5I7BLoHGGSkN2I6yL4IGRx64CW589AYyXRa/Ii6BYeXyih7m5L9usnKFqFu8qeZNZFYnrpN3iDFs4MzYhmvN1PgP3Rbtv57sq2X9XVWQ4cdOb45/ZETIBGgVHqSNGRgNzmAH7cloHEnM6UQ/98UnHxX9G40cehYPs+AzUZZTRz1+SsZYUs4k+DUOIgdbQpGN2IcJhjmCMEcuDY0JCZDyDhKy3qb4XISVFMDDdBH2OkXgSOhqy+0IlQIah2Hn5GVYgYPuZ55csF4kdEfVXo29qtLzHqx7lnoDMAZvdJIfCD/sUz2WExz7Bhz7DvJ0haGbIkwJdVyCZIuTjTN2LEjOUEwmJA9qJHhKkxpm0tbos+gy8N4nHCE2Yh8cuGu5lwUuqgQ5AaJpPHizoSBnS/aAOgBUHcjxUosAk0XopFIFSV0rdFKL0LVHgWI5DQqZw7AvwM1VU7eSIgfoG/aAxIPU6CFgdlcBbt8aaDo6GzFGVElJnjOacL9kNVAeM96xuM8MAcMzCDswxQXVjK98l89RVEzIz906OGM/mYJfdqNd1XX/LCUGWF8dZL0VkiBZnxk1etmbX4meHSKUhHmKxnOsBIsbg5asXjjtPeeMI88VCDxx9ByhB/P7hAc/PG/z47gN+97vvNEJ43jyLpUDqD6sFVg0K5uUefbXH2DbSfV8uZviF81/fPT9iv90jn6WYzVNRirq2VneCFTQ3H3YtVEFScTCM8PabX+D126+REBy3utWxcyMLxkBzclIeSVOkoyIrGSYGPCaa9RwO5cUJASlrhIt7TUBPO1QS0/CcdYqfrHSLWWqyy+wqUFyIgitUvKOhkVRhTp0Lm3MawE9fki42F0UmS9oE+TkFomP4p+wtAV6Zrmk4cVPmbk/agev5c+ROL/sZrYsndATPBaOa7ef8AFH4jxutNSgENlQMcdx1baiWeLl8SB0nfl2ymjHGc8UOSy+2gdKaYBTFMJhihONb/f0s7KROGA1W3SaoxP0n8C8enkTtzCN2g0zRkjRaZDMgMcXKOEoFoCtYfU4dku4RwVABhwcE9RbBeBBFs9rU2D7u0OwHPPyBEt8TftiFOHQBflPN8a6d4zm8wYfwHvN6ia+2X2M25Pg6W2OWhliPPfJwhm3YYx8OqFBhO27UxRkmhlA2AgJpOgw9KaMpij7GrOc9zNxrkIvlJSvUOGpS659APCbC4ubrnpvQtqT5EdfA7M7a9ErmHdpCwFUPMqU6pRJ9FgFASBdMPnNKBowXQmVOKZRKdcm8MHgvSmwoijASd+CkhemWyf1Bltwe5zQaNZgJPgHEZBVlKccafNxGDJJTJw7KfEnIVCAu6BjofSbrOm3CaxKQG518ToQpva7r+mI7BK4F/alC+XmW7FvAtvEzMEmpLsswLpa2gfU29yfIT89cFKnyJ16A7UMi3A+HWl0DD+rb7elUxtY12QdmWiLNAGqZ140CZElPdQZBSv5Km4WIegPUmSyyjQvZ7q/rylwQY7rymXCJKn9nVuRR41K809+Fp787a3WbDPBlF+CsdvdD9uOZFWJdoj/299KDF42S+AV+Vo4HbFey6t5pFng5WW1s/gB/YjZz7A54eeKT/uxJyoavY6A0P3UwSWpujqaQQIVBSUOz6mJyYeX+n7ynvydOoxEnsexGGhJ88t70F55U5TCuzcBrJPEmg/y5T0S9Brb4GYTMnEf9FgUj+9TRSI1+ekwwIZgQjQbC5IweQSZef4xMgjaqRAUIzVSNB0qgYkRjgnBKBKLrkaFVZQ9VwXUQogpC1FGBOpmhiWbo4jm6dI46LOSX0IU5Oo7nEo6qekzZAkO2QJ8G6JMGI8cZDIo8m0mBkaMefoWJRmfGLrDre+k5ZRLgX8tjPzyExoQlPaXY3SdiELjnxJ9fJzjEpMI6BRTDkuWQPbvuegkO6P0nHC7Gt/RN7Mg/NXKFOspre68MT3JQt4vPixMckwARbalNPuKsy+kArf5BdF4NR6rmUejK6MmSYuaHvnYIrutLTggYoFU9+56u29tF73Kbjh8XaHNgm68jIj/Fkpm4jIRSBeBGWgMjMlrhTqP0ABi4f/HLA/7h1/8oat1uVwpU+MMP71CWJf7wx9/h+ekRz22DckfJ4hbvHjfYHirU3Wh2xVmM29laQjtsE3If25V74+DHuRs7sFPQY54vxDRYrla4vbtTB8SEZiyJ0YwyYaLAmePgrJslYacOSZrnSmQuWVM1IqJzGvHLLph6dUcipomjaFvO4+1824w/1Z9pU8xrAjVcuTm778ekzRr5onS666YWvVr6Lnlz0ZuJlI1tojNqqYkY8RzKzKenLDT5+yFul9Sh4KZI8N4onX2OVNlncP56x41VYPcRaFoHQnXvPLCi04EaYt+CyiB3ycuWG0W4VrrsmRU77DxHnLVLNIeCVRTHMcGkFnN1EaapxRisVVmHA9UILZtUkOlOHhaxLIYDFLTLZWJBLNrQIO5ahMMaSdAJj9AXHZoXNdp5j3o66HoOJUGFATIssZoK5MkS62SNJM8wv1uL737zYol5FuFNEmMZNgjGHFW4AD5uUcUfFJxmxUxYGCUNlAn/5tcI1q8QFkthYxReqS7oZag/c9WHyrREBhvFkcvPoJ3RfpwS2KF5bRC1b1oZLoE6JgVmzqWr4x6ZIxvS0RD1b/6+dH+ve1EX0s6/9B0Gc8y0ToA8NnWvEyfAjgW9BoY4Esh5NsuxWBRYr5bqEMxTiktRn8MAg/IlCWIVGY1wJ65fJRqy7XccLxITcxyXSt+E1uhX6eLr+qI7BI53fW5g5DZfv3wFel5RilYo++BUQEN1CpxZEQHHXqqUGXmaFUoSGLB9p4CbOlkKdFasK25MRP6TZjShdhrq8a4UMO42p6KgWfpGVM5rOevvDSEcOTChWhMmREPBocxRCvnf5nhofH3bMFz7faSGgrUmbV8zj4NLpYvlreLn8372SYQzm62sWOlofDSvscAuy2WKqEiy1pG//pxQim8GOGAf+8ea/P6ZfexoFXz8Xdv0/DXm+wg8NnBGHInbbdu8GyMQCahN03H0/Xd3ICrwHIPET5/lIedm017u1t87P9dyIcg6JzyfOk9WY7I9bYRTtp5PAkRjEDs8BMWFKM40Hl0TiTUgnc8j0JkQ9JRmZoXaZ+Lax+PSZuagQuKAMRow5A069BiWe1PTIw1moCDPHFlADYg50nSJiP4JRapglmbUuohRFBNmcYZ0tUa0poxxgGHFzhillm/0bIXJHBPpusu1xhrUthBg1o9jLjyPHLnx89rzbIly0rHi5n1BNL/LD48DKjvPzlnqSCf1Ak/+mE7MEgd69ZW4b274loMb2x8Bsp4ZYDKI+m8b7dg4jBgbfnn2DPeGNImQSlODOkfmc6BBBq/3GY7FE009k8DjCcw51ZlQOWDkdV3Xl0s7VOudX2yxG4qcD6hmes7O2FrALuayrRqnmrc1NatyZ7srUx1WXpTQZefg1GFgZS6hkQm4vWFl2stlsSpLqe7dv1vjj7PUJEdjKieyWqPyGyV6R8S7Ck3duWplQpxy7l+gWM7w9qtvVVW9/PqXCnCrmzvREV++eYP7ly/PBIpI6ytMq1wt98lUDNsOm+1eWAdiDyJK8Hog0meuJFyhSIqjO5+qIhcsyWcPWH3GHeKUn4nf5/pMWR7aV0GE9kKJjFDZx53fqQBSz4BVE8cnx3EOK0ZPDbQEgXgQS5AKZLO1859gh6UX1qJuaCd9MGXFng5+VILkDNb5I4S1ExvK1IL12hKeyqVrkRDh7TfxCb1AeGwXM6GhkiXHDsQkXNh18QmpY8p7uILAYUftBeYvNjIgqtO8KyyJGqMJfURGAqtedw842zyvUGfTGJomAbvWBZCBYkAR0uAeUbSQ6yEDpUEuW/TJgCokgHZE1IfIpwDrKUU2UQ+BUsBUmqRDYirO+800IWMHqu3VNZnyBNGLpbpgCVUfKaCzvFVCgGRhCcH6JTC7QZzPEBfE+JjkZzgZWPezl54LYkyA5ZJeDRNm8wzFPEHTdNhsSoFsD4fOuhFn4zR+VweKdD69lhd0MnRBJ2aRdQB830pB2I0P+H+iMkqnhCJjvDeJmaAAlwmm8U1yWlEj179TrXS9XGC9IjuCe4xJFFuQd9gBObGyc8YEhM+DZ7p4SrVn5pANEaMbDctko0Uycq4dguv6ghMCzbNdEmCb/PgJhew0A3ZZtJNNJSCIoCbiAygSZKh/8pddpeUSCn55ND+DBFvj3EjyPBbI8HDYakNo6wP2u63ay4GjubV9I7BQVbfouwF106PtB8wXAdY3M0Rxhpu7eySU9nNiSMv1rcYEi9UK8yUVFw3MxJkjqYw8PlIr5RzYtAIQHg4HJQekPtLP4VLaYRxST33m5pU2MhA4k58t7MwaWOjmAhHn0kmBKMkQpxESBYZM1SWPgx2M81mx79gycWJC4FHTqvpbo3X5a8bxAANMnBRImHRItpmOlKO6MCHFWoiqD1L0HStbbopATme/gIY/JiYVhjkCCf1Ya93bVmuWHxtQzGRsJ0RDa1LNsARCP6tN1wWOz1zHAtLOgrPoPd2nvrKTs50fclAxz7EjaHtLrvw4hugkOGTmUydZ3jNhBQ9OEX4idgiFBeJoJg68RHpitoE6jAnVB9nqJneDTIAAs5FcfWZzlmDwFQoCdkNgRitkAkP53BAsx4C8opgW7wee/xSREgJqVC+kUhjM1wizuUkhE1VPYSJx9Wyc9PnrJGpV8NmIgJTiTxQqangdmeSb7wCfdRmJne0HRie2UZC5MjPQmu6F6Y7oylky4GWgHKX1KIEtl1KqmZImyktAfIFpCkgVkSMuKWsSVzFpVDCbpchysoO8NoJhoSh+xnvc3sHug1PzgXgE16JwIzjfKbTPY0kCO2XXdV3/mdZf3e/mQ61Wv6sgfQDyYjAe2MdFAJ9J/TYo6WrmyGx+VswZ/XzGCv50GDLV6e31fbUgwaM4wJs3r5ArUPPvLGCzZckNoalLPcjsxPL3dodS78nj2ux3GhF898P3mM3mePXmG40JFssVZvO5sACcxXNTC9n+pFqhVO6cXPNkwkm73f74JSnjguOPy2azXVigAc1Z+DlsAxXnf6Tin5v3h5RTtnk9udHkxEtZz6nrjRMTCqMv6owdhQjcKEbjAgOf+XNKQyN1EMRC4H+70chEXAQFmGIEXW7g0IhaCcR6UOq5wdCHGEjfk1+C50bQkYgJYmYitsIteMCgtYr9uRKdjHXzYCMYI6VxYyd6nXP3yzjzCisOEMZNm8nJUeHhmL95eWWbQAs46f0fxwAdxWuIPwhMmOqkz+8SqyNQj8BIBiQGEWcbHdD0ZzQPBx0Lr12imcWUeGV/U9cLp0hjiFBfhsRnNcyxXBd3okrS/In0vCktkAcJlnGHF9Fc50xBjcmMG2tEQ49woCgPUw6zfg4oCdxfNtoSVkVa/zweXmeqjfJc0BjMzi+T/VnRqyPYdeT5n5ICju3qiiLHHqznjKMmAitDtKLRugGP7h2dOQyD3dSs/lkskFFEUTI9v7PMqnVnl02xrixzttcBsFwUuFlTfphiZBR54p0p/cxjF+lIcxXrIXbAZ2P8SK/E0sQzNVXTu2YXkl3O67quL1e6WJk+q/3eqeGdUOE+UeBmKR8Cgd5aBVJW1PtD5UyNnHp8YN0AyhwfX1yv0+v3DClv80qOFRhovv76De7vbrQh3N3dac4vBsMw4LDbac7ZNaW+f3x+xu5wwIcP7/Hhh+/lb8CSdr2+xau336r6pdPhcrW2Wat2fHYprAKhSA8ffGoX8LirssZ2u8N2u8d2u9VGsiQQ8sKEoA9mSgi4d8tGWF8cy4TaDBXYwhFJHqvC5mdW8HZCOVNAiuAMEbn0zrrZWp4n8SEGRNMT8K10E2rRlYj8z5pC4jglaLsCEWUJg+Ikc5wNiBJ2FajwZsfG7kUYsb2v7dNVc6y6uHla0PDHcnYXOQtnJn+GSlcwZvtegkYmCnTJkoG0R5CH8TG4+HawwG0Ok2HQSwtQnlmphEAeCHLGOaFn9ePOSMd1B9QpozKhftYsgDsKGp3InvbrPB9ylzQevyUp+hueLT2ICZMCdgXYHcKINqglr0sgJ4F7SOeY5RGifECU96b0yepcVr2kA9JuuUPEL7EgjIVAU6DABdbPXXHqAKYC0bITNEk0iaqfvI4UF+JxSFiQI8KmsRGiklwbER5KX0C48877m12RKEATnzqMR5yMKzS4VouZioC7uxvc3q4taXAMC3b1uNgNUGIs7RDSkQvc3CwkgMSuhq5Fb3bMdvnOdC/cqJBvN/SWnJvypx2PJQQ0+YolgEaAMaXEr+u6vlz7Y0fL8ywD3+b3X3oYz3zoOXMkkDB3/uJKCCSnapAyzSQdYttHDWvL2cyZrocMfhLvET2JG2okCWEqB1JAiLN+At3oTiixkd6C1my1wr48iDtcUV0MgRITzv4O5eHYDlcVewa6shmxKZepupXOuTkoqg0q7X3fPnRSwxes5+0zstbmu/wcBhRktde7c0Ct9Un4BVP/M8+HrgvAuFnXqWyTpcHuDKNUj58FPHO8cxQqP9f9yahDSHop87EbUxorg+Yx2rhNAVGOjLrW7AJZQhAzIVDFqDr7pPEvLf+Tw935ex9BZfLrNSYAS3eNbCg/W1+WEJgTpAe4WSD21bw/EN+S1s/4MZcT21F84P2mrsGntE0/N3C6Ssc8wViUpzvJ/+nI+pTew5lcsrANJr3L88UjY6jlO7LatwzVPBOSMEVInQh1O4iOHREn7PCwO8Gqlm8ulQUDGDpfEJ+QWcpz2X16nvQbENQ+i4SbmMw4uIIBUc3nhIm50WZH9FIjtITAKnMm23xN54SZWYGhy+Lkyfm6+mys9uc0F0uxXi2wWs7t+kgvhGJIBiyWEZVjAPA7uwpSQ/RJ2BlKQZ/BMgFLLpiYHfcgSwKPz6JPrsUw4AhmkMU6O6DXdV1fbEJA0x8zxbEA48cEflTAh9hmbfzpSW09AtU45yZ4z9wODcxGsBpRvgzqfj4nTXxtIuYkVml0YIHXVwz8eVIESRXM0kyOhVzM5o0qZ3PC3X6Hqq7wz//8LwI3Pj894Tf/9hvJJD98eK+N55tvf6FjUJWvhIabk2mxy6KVQMm2E/VP39kipASrYzBY4nIZRe73f/ydkORWOVtQ8XxqVTFursmP57Eb/EvO2tVeT+KjRXORFsJkCDHtv44WPJYQHMOChHbON0lvBhOeWAtHIX8XKN3rmcgRgwG7PI6nHzlQmEtExC1nheqAYe5lXFDUEFl1Ma+6Ic9Ny4DJBzUiLlkUn6GVrX02u44K/OKs25udB2edFeExT8cqVc4/0W84AT/5QY84RIfLMOzHeCYEdUI4GlDerKmJWUhIEVWHx5ICmk+xs2D3lDl0YmSUjZFTaQ+TzIASXs1kAgt2BTVqJbCqlo4DgymZMiagc0wGxKa4DKip9vlIYDDNx5j4m8ofpYMJgeCYzaSws2NAVfXe9fqyhNJ3E61A2O9rS3iFHTnpVxjdl3TAk/bHrCBYMMF6vRKF0F8OgmPpc6JRhtt7TFOEoNtU3UVvwa1HynXN+Hm4BxFrITr1aJiXvg/1nBv2xScDHivF1+aIjnimGofyssT1uq7rbzIh8DNpUv98IuABWp5dYCJETAiC48NsQCL+O8cA1uI0Hj8fNloNk06Yqk3sF0F7fB9P8zP+vDmW+WOxKtUyeE+FlA+6Zp2mjMeHml8UNPKJi5gNYYiS4MBiL4tm2jP7Vr1jIyopSDkSGUf9O+WSDUxYmoiSzE8ClAdD3Z+fo790+Z9nNWxzUl+lWkvzWHU6Lr1ps1iG4Gft/dAi6mMlDaKE9TYzl8DOGbDO9Ql8H+AsIThRFU9EMJ8guArVHcgptnkUAs/ViJSVqlQSHU/cB30R9xza/ExwScfj5WKVEJzhR0RH5XXrLjqnTVUqWJ2ciY4/8QkBT2TPoxqNn+2fv+eJNqkz6AfP7idNDIffrTXeszvlxZbsgM46B/Y6kRgVIXo6GTq3RVIhvcgOJZSY7FlDw8Yw/FueEXofEtmi/ppTfuyUEDhqqs4tOweUlWZSYPc7gyPPySXnlJofGO158qZRAqv2xhSwj8iiwNrr/nd9weClju3vzCmQYzw+T1aJn95Tr6WR0+n8J1RaDBisO9SNPX9cZKoYqJFZjzPuigyDZMdktFbdZTwGvo4SJwOShhIncxLXZEJ0nRRR+We/p5yLkfEbRdD4VdWfd59e13X9R13B9BfczX/84x/x7bff/r9zRH+j6w9/+AO++eabv/jnr+f0f76u5/TnX9dz+v/9Ob2u6/qbTghYPX///fdYLpcX0+z+sy2ePnYRvvrqq7/KDvV6Tv/9dT2nP/+6ntP/OOf0uq7rbzohuK7ruq7ruq7ruq7/3Oua1l7XdV3XdV3XdV3XNSG4ruu6ruu6ruu6rmtCcF3XdV3XdV3XdV3XhOC6ruu6ruu6ruu6uK4JwXVd13Vd13Vd13VdE4Lruq7ruq7ruq7ruiYE13Vd13Vd13VduC7g/wb3ITbavCr6tAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 32, 32, 3]) torch.Size([10000, 1]) tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) torch.Size([2000, 32, 32, 3]) torch.Size([2000, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1024]) torch.Size([2000, 1024])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "transform_data = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) # Apply the (x - mean)/var operation on the components of the data # if x is in [0,1] then Normalise(x) is in [-1,1] # is applied on the three channels RGB\n",
    "])\n",
    "\n",
    "# Data import\n",
    "dtype = torch.float32\n",
    "trainset = torchvision.datasets.CIFAR10(root = './datas', train= True, download = True, transform = transform_data)\n",
    "validset = torchvision.datasets.CIFAR10(root = './datas', train = False, download = True, transform = transform_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 1)\n",
    "\n",
    "x_train, y_train_raw, x_valid, y_valid_raw = torch.tensor(trainset.data), torch.tensor(trainset.targets), torch.tensor(validset.data), torch.tensor(validset.targets)\n",
    "\n",
    "# Modification du format des données shape (n_data,1) -> (n_data, n_classes)\n",
    "y_train = torch.zeros((y_train_raw.shape[0], torch.max(y_train_raw)+1))\n",
    "for i, y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros((y_valid_raw.shape[0], torch.max(y_valid_raw)+1))\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "print(y_train.shape, y_valid.shape)\n",
    "\n",
    "# Binary reduction of the classes # To avoid using softmax, we regroup classes in two classes\n",
    "class_binary_reduction = True\n",
    "determination_des_classes = True\n",
    "black_and_white_images = True\n",
    "\n",
    "if class_binary_reduction :\n",
    "    if determination_des_classes :\n",
    "        # Determination des classes\n",
    "        class_list = []\n",
    "        class_index = 0\n",
    "        for i in range (x_train.shape[0]):\n",
    "            if y_train[i, class_index] == 1:\n",
    "                class_list.append(x_train[i])\n",
    "                class_index += 1\n",
    "            if len(class_list) == len(y_train[0]):\n",
    "                break\n",
    "        for i, image in enumerate(class_list):\n",
    "            plt.subplot(2, int(len(class_list)/2+1),i+1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "                \n",
    "    # classe1 = [0, 1, 8, 9]  # vehicles\n",
    "    # classe2 = [2, 3, 4, 5]  # animals\n",
    "    \n",
    "    # classe1 = [1, 3, 4]  # elk, truck, dog\n",
    "    # classe2 = [5, 7, 9]  # horse, car, cat\n",
    "\n",
    "    classe1 = [1]  # elk\n",
    "    classe2 = [5]  # horse\n",
    "    \n",
    "    # Création des masques pour les échantillons appartenant à ces classes\n",
    "    mask_classe1_train = y_train[:, classe1].sum(dim=1) > 0  # True si appartient à classe1\n",
    "    mask_classe2_train = y_train[:, classe2].sum(dim=1) > 0  # True si appartient à classe2\n",
    "    \n",
    "    mask_classe1_valid = y_valid[:, classe1].sum(dim=1) > 0\n",
    "    mask_classe2_valid = y_valid[:, classe2].sum(dim=1) > 0\n",
    "    \n",
    "    # Filtrage des exemples concernés\n",
    "    mask_train = torch.logical_or(mask_classe1_train, mask_classe2_train)\n",
    "    mask_valid = torch.logical_or(mask_classe1_valid, mask_classe2_valid)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid]\n",
    "\n",
    "    # Création du vecteur de labels binaires (1 pour classe1, 0 pour classe2)\n",
    "    y_train = (y_train[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    y_valid = (y_valid[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    \n",
    "    # Avec tanh\n",
    "    \n",
    "    # x_train = 2*(x_train-0.5)\n",
    "    # y_train = 2*(y_train-0.5)\n",
    "    # x_valid = 2*(x_valid-0.5)\n",
    "    # y_valid = 2*(y_valid-0.5)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_train[0:10], x_valid.shape, y_valid.shape)\n",
    "\n",
    "if black_and_white_images :\n",
    "    x_train = 0.299*x_train[:,:,:,0] + 0.587*x_train[:,:,:,1] + 0.114*x_train[:,:,:,2]\n",
    "    x_valid = 0.299*x_valid[:,:,:,0] + 0.587*x_valid[:,:,:,1] + 0.114*x_valid[:,:,:,2]\n",
    "    for i, image in enumerate(x_train[0:10]):\n",
    "        plt.subplot(2, int(len(x_train[0:10])/2+1),i+1)\n",
    "        plt.imshow(image, cmap = 'grey')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    x_train, x_valid = (x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])).to(dtype), x_valid.reshape(x_valid.shape[0], x_valid.shape[1]*x_valid.shape[2]).to(dtype)\n",
    "    print(x_train.shape, x_valid.shape)\n",
    "\n",
    "else :    \n",
    "    x_train, x_valid = (x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]*x_train.shape[3])).to(dtype), x_valid.reshape(x_valid.shape[0], x_valid.shape[1]*x_valid.shape[2]*x_valid.shape[3]).to(dtype)\n",
    "    print(x_train.shape, x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45060724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on :  cpu\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# else :\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Computing on : \", device)\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0).to(device),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).type(dtype).to(device)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    s = s.to(device)\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return torch.exp(-x)/((1 + torch.exp(-x))**2)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (torch.tanh(x)**2)).to(device)\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output); grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = (torch.einsum('no,noz->nz',grad_output,softmax_derivative(output))).to(dtype) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "\n",
    "class binary_classification_two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size, lr=1e-3, lr_decay_rate = 1e9, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.lr = lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(1, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x, dropout_rate=0):\n",
    "        if dropout_rate != 0 :    \n",
    "            dropout_mask_1 = ((torch.rand((x.shape[0], self.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n",
    "        else : \n",
    "            dropout_mask_1 = 1\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)*dropout_mask_1  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = torch.sigmoid(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, lr_decay_rate = 1e9, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True, dropout_rate = 0):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - lr_decay_rate = \" + str(lr_decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) + ' - observation rate = ' + str(observation_rate) + ' - Train layer 1 = ' + str(train_layer_1) + ' - Train layer 2 = ' + str(train_layer_2) + ' - Dropout rate = ' + str(dropout_rate)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.lr_decay_rate = torch.tensor(lr_decay_rate)\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch, dropout_rate)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                print(\"Output\", output[20:22])\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = grad_output*sigmoid_derivative(z2); grad_z2  = grad_z2.to(dtype) # shape(n_data, 1)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # if i == 250:\n",
    "            #     break\n",
    "\n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "class binary_classification_three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, lr=0.01, lr_decay_rate = 1e9, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, dropout_rate = 0):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.lr = lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(1, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x, dropout_rate=0):\n",
    "        if dropout_rate != 0 :    \n",
    "            dropout_mask_1 = ((torch.rand((x.shape[0], self.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_2 = ((torch.rand((x.shape[0], self.hidden_2_size)) > dropout_rate).to(dtype)).to(device)\n",
    "        else : \n",
    "            dropout_mask_1, dropout_mask_2 = 1, 1\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)*dropout_mask_1  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2)*dropout_mask_2 # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, 1)\n",
    "        output = sigmoid(z3) # output layer # shape (n_data, 1)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, lr_decay_rate = 1e9, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True, dropout_rate = 0):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - lr_decay_rate \" + str(lr_decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.decay_rate = lr_decay_rate\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Dropout\n",
    "            # dropout_mask = ((torch.rand((minibatch_size, x_train.shape[1])) > dropout_rate).to(dtype)).to(device)\n",
    "            # x_minibatch = x_minibatch*dropout_mask\n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch, dropout_rate)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = grad_output*sigmoid_derivative(z3).to(dtype) # shape (n_data, 1) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n",
    "            \n",
    "            # if i == 12000:\n",
    "            #     break\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\" \n",
    "\n",
    "class binary_classification_four_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, hidden_3_size, lr=0.01, lr_decay_rate = 1e9, reg1 =0, reg2 = 0, reg3=0, eps_init=1, fraction_batch=0.01, observation_rate = 10, dropout_rate = 0):\n",
    "        \"\"\"\n",
    "        Constructor of the four-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_four_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.hidden_3_size = hidden_3_size\n",
    "        self.lr = lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(hidden_3_size, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.W4 = eps_init*torch.randn(1, hidden_3_size, dtype = dtype) / np.sqrt(hidden_3_size)\n",
    "        \n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(hidden_3_size, 1, dtype=dtype)-1)\n",
    "        self.b4 = eps_init*(2*torch.rand(1, 1, dtype = dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.W4 = self.W4.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        self.b4 = self.b4.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x, dropout_rate=0):\n",
    "        if dropout_rate != 0 :    \n",
    "            dropout_mask_1 = ((torch.rand((x.shape[0], self.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_2 = ((torch.rand((x.shape[0], self.hidden_2_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_3 = ((torch.rand((x.shape[0], self.hidden_3_size)) > dropout_rate).to(dtype)).to(device)\n",
    "        else : \n",
    "            dropout_mask_1, dropout_mask_2, dropout_mask_3 = 1, 1, 1\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)*dropout_mask_1  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2)*dropout_mask_2 # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, hidden_3_size)\n",
    "        h3 = ReLU(z3)*dropout_mask_3 # hidden neurons layer_3\n",
    "        z4 = (torch.mm(self.W4, z3.t()) + self.b4).t() # shape (n_data,1)\n",
    "        output = sigmoid(z4) # output layer # shape (n_data, 1)\n",
    "        return output, z4, h3, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, lr_decay_rate = 1e9, reg1 = 0, reg2 = 0, reg3 = 0, reg4 =0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True, train_layer_4 = True, dropout_rate = 0):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - Training fourth layer = \" + str(train_layer_4) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - lr_decay_rate \" + str(lr_decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) + \" - dropout rate = \" + str(dropout_rate)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.decay_rate = lr_decay_rate\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.reg4 = reg4\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Dropout\n",
    "            # dropout_mask = ((torch.rand((minibatch_size, x_train.shape[1])) > dropout_rate).to(dtype)).to(device)\n",
    "            # x_minibatch = x_minibatch*dropout_mask\n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z4, h3, z3, h2, z2, h1, z1 = self.forward(x_minibatch, dropout_rate)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)  # shape (n_data, 1)\n",
    "            grad_z4 = grad_output*sigmoid_derivative(z4).to(dtype)  # shape (n_data, 1)\n",
    "            grad_h3 = (torch.mm(grad_z4, self.W4)).to(dtype)  # shape (n_data, hidden_3_size)\n",
    "            grad_z3 = grad_h3*sigmoid_derivative(z3).to(dtype) # shape (n_data, hidden_3_size) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            grad_W4 = (torch.mm(grad_z4.t(),h3)/x_minibatch.shape[0]).to(dtype)\n",
    "            \n",
    "            grad_b4 = (torch.mean(grad_z4, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            if train_layer_4:\n",
    "                self.W4 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_3_size))*grad_W4/(eps_init**2) + self.reg4*self.W4)).to(dtype)\n",
    "                self.b4 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_3_size))*grad_b4/(eps_init**2) + self.reg4*self.b4)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z4, h3, z3, h2, z2, h1, z1, grad_output, grad_z4, grad_h3, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3, grad_W4, grad_b4\n",
    "            gc.collect()\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\" \n",
    "    \n",
    "    \n",
    "class binary_classification_five_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, hidden_3_size, hidden_4_size, lr=0.01, lr_decay_rate = 1e9, reg1=0, reg2=0, reg3=0, reg4=0, reg5=0, eps_init=1, fraction_batch=0.01, observation_rate = 10, dropout_rate = 0):\n",
    "        \"\"\"\n",
    "        Constructor of the five-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_five_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.hidden_3_size = hidden_3_size\n",
    "        self.hidden_4_size = hidden_4_size\n",
    "        self.lr = lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.reg4 = reg4\n",
    "        self.reg5 = reg5\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(hidden_3_size, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.W4 = eps_init*torch.randn(hidden_4_size, hidden_3_size, dtype = dtype) / np.sqrt(hidden_3_size)\n",
    "        self.W5 = eps_init*torch.randn(1, hidden_4_size, dtype = dtype) / np.sqrt(hidden_4_size)\n",
    "        \n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(hidden_3_size, 1, dtype=dtype)-1)\n",
    "        self.b4 = eps_init*(2*torch.rand(hidden_4_size, 1, dtype = dtype)-1)\n",
    "        self.b5 = eps_init*(2*torch.rand(1, 1, dtype = dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.W4 = self.W4.to(device)\n",
    "        self.W5 = self.W5.to(device)\n",
    "\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        self.b4 = self.b4.to(device)\n",
    "        self.b5 = self.b5.to(device)\n",
    "\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x, dropout_rate=0):\n",
    "        if dropout_rate != 0 :    \n",
    "            dropout_mask_1 = ((torch.rand((x.shape[0], self.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_2 = ((torch.rand((x.shape[0], self.hidden_2_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_3 = ((torch.rand((x.shape[0], self.hidden_3_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_4 = ((torch.rand((x.shape[0], self.hidden_4_size)) > dropout_rate).to(dtype)).to(device)\n",
    "        else : \n",
    "            dropout_mask_1, dropout_mask_2, dropout_mask_3, dropout_mask_4 = 1, 1, 1, 1\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)*dropout_mask_1  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2)*dropout_mask_2 # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, hidden_3_size)\n",
    "        h3 = ReLU(z3)*dropout_mask_3 # hidden neurons layer_3\n",
    "        z4 = (torch.mm(self.W4, z3.t()) + self.b4).t() # shape (n_data,1)\n",
    "        h4 = ReLU(z4)*dropout_mask_4\n",
    "        z5 = torch.mm(self.W5, h4.t() + self.b5).t()\n",
    "        output = sigmoid(z5) # output layer # shape (n_data, 1)\n",
    "        return output, z5, h4, z4, h3, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, lr_decay_rate = 1e9, reg1 = 0, reg2 = 0, reg3 = 0, reg4 =0, reg5=0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True, train_layer_4 = True, train_layer_5=True, dropout_rate = 0):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - Training fourth layer = \" + str(train_layer_4) + \" - Training fifth layer = \" + str(train_layer_5) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - lr_decay_rate \" + str(lr_decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) + \" - dropout rate = \" + str(dropout_rate)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.decay_rate = lr_decay_rate\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.reg4 = reg4\n",
    "        self.reg5 = reg5\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Dropout\n",
    "            # dropout_mask = ((torch.rand((minibatch_size, x_train.shape[1])) > dropout_rate).to(dtype)).to(device)\n",
    "            # x_minibatch = x_minibatch*dropout_mask\n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z5, h4, z4, h3, z3, h2, z2, h1, z1 = self.forward(x_minibatch, dropout_rate)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss_tot = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)  # shape (n_data, 1)\n",
    "            grad_z5 = grad_output*sigmoid_derivative(z5).to(dtype)  # shape (n_data, 1)\n",
    "            grad_h4 = torch.mm(grad_z5, self.W5)    # shape (n_data, hidden_4_size)\n",
    "            grad_z4 = grad_h4*ReLU_derivative(z4)   # shape (n_data, hidden_4_size)\n",
    "            grad_h3 = (torch.mm(grad_z4, self.W4)).to(dtype)  # shape (n_data, hidden_3_size)\n",
    "            grad_z3 = grad_h3*sigmoid_derivative(z3).to(dtype) # shape (n_data, hidden_3_size) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_W4 = (torch.mm(grad_z4.t(),h3)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b4 = (torch.mean(grad_z4, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation            \n",
    "            grad_W5 = (torch.mm(grad_z5.t(), h4)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b5 = (torch.mean(grad_z5, dim=0)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            if train_layer_4:\n",
    "                self.W4 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_3_size))*grad_W4/(eps_init**2) + self.reg4*self.W4)).to(dtype)\n",
    "                self.b4 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_3_size))*grad_b4/(eps_init**2) + self.reg4*self.b4)).to(dtype)\n",
    "            if train_layer_5:\n",
    "                self.W5 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_4_size))*grad_W5/(eps_init**2) +  self.reg5*self.W5)).to(dtype)\n",
    "                self.b5 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_4_size))*grad_b5/(eps_init**2) + self.reg5*self.b5)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z5, h4, z4, h3, z3, h2, z2, h1, z1, grad_output, grad_z5, grad_h4, grad_z4, grad_h3, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3, grad_W4, grad_b4, grad_W5, grad_b5\n",
    "            gc.collect()\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\" \n",
    "    \n",
    "    \n",
    "class torch_nn(nn.Module):\n",
    "    def __init__(self, list_of_layer_size, trained_layer_list, dropout = 0, optimizer = optim.Adam()):\n",
    "        super.__init__()\n",
    "        \n",
    "        # Classification type \n",
    "        self.binary = (list_of_layer_size[-1] == 1)\n",
    "    \n",
    "        # Last layer activation function\n",
    "        if self.binary:\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else : \n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Building the model\n",
    "        self.layers = []\n",
    "        self.dropout = dropout\n",
    "        for i in range(1,len(list_of_layer_size)) : \n",
    "            # Adding layer mentionning if it should be trained\n",
    "            self.layers.append(nn.Linear(list_of_layer_size[i], list_of_layer_size[i-1]))\n",
    "            self.layers[-1].requires_grad = self.trained_list[i-1]\n",
    "            self.layers.append(nn.Dropout(self.dropout))\n",
    "            if i < len(list_of_layer_size)-2:\n",
    "                self.layers.append(nn.ReLU)\n",
    "            else :\n",
    "                self.layers.append(self.activation)\n",
    "        self.model = nn.Sequential(*self.layers) \n",
    "\n",
    "        # Training arguments\n",
    "        self.fraction_minibatch = None\n",
    "        self.kappa = 0\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        if trained_layer_list == []:  \n",
    "            self.trained_list = [True for i in range(len(list_of_layer_size)-1)]\n",
    "        else : \n",
    "            self.trained_list = trained_layer_list\n",
    "        self.optimizer = optimizer(self.model.parameters())\n",
    "        self.lr = None\n",
    "        self.lr_decay_rate = None\n",
    "        self.lr_decay_norm = None\n",
    "        self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, self.lr_decay_rate, self.lr_decay_norm)\n",
    "        \n",
    "        # Architecture\n",
    "        self.architecture = ''\n",
    "        \n",
    "        # Recording parameters\n",
    "        self.observation_interval = 0\n",
    "        self.training_loss_trajectory = []\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.accuracy_trajectroy = []\n",
    "        \n",
    "    def training(self, training_inputs, training_targets, validation_inputs, validation_targets, kappa = 2, observation_interval=10, lr=1e-2, lr_decay_rate=1e8, lr_decay_norm=0, trained_layer_list=[], fraction_minibatch=0.1):\n",
    "        \n",
    "        # Formating datas\n",
    "        training_inputs, training_targets = torch.tensor(training_inputs).to(device), torch.tensor(training_targets).to(device)\n",
    "        validation_inputs, validation_targets = torch.tensor(validation_inputs).to(device), torch.tensor(validation_targets).to(device)\n",
    "        \n",
    "        # Updating the trained list\n",
    "        if trained_layer_list != [] :\n",
    "            self.trained_list = trained_layer_list\n",
    "            \n",
    "        # Initializing optimizer\n",
    "        self.lr = lr\n",
    "        self.optimizer = self.optimizer(lr=self.lr)\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.lr_decay_norm = lr_decay_norm\n",
    "        self.lr_scheduler = self.lr_scheduler(self.lr_decay_rate, self.lr_decay_norm)\n",
    "        \n",
    "        # Minibatching \n",
    "        self.fraction_minibatch = fraction_minibatch\n",
    "        random_indices = torch.randperm(training_inputs.shape[0]*self.fraction_minibatch)\n",
    "        minibatch_inputs = training_inputs[random_indices]\n",
    "        \n",
    "        # Determining number of iterations\n",
    "        self.kappa = kappa\n",
    "        n_iterations = training_inputs.shape[1]**self.kappa\n",
    "        n_datas = n_iterations*training_inputs.shape[0]*self.fraction_minibatch\n",
    "        \n",
    "        # Recording the observation interval\n",
    "        self.observation_interval = observation_interval\n",
    "        \n",
    "        # Training !\n",
    "        for epoch in range(n_iterations):\n",
    "            \n",
    "            # Make prediction\n",
    "            training_pred = self.model(minibatch_inputs)\n",
    "            validation_pred = self.model(validation_inputs)\n",
    "            \n",
    "            # Record performances\n",
    "            if i == observation_interval :\n",
    "                \n",
    "                # Computing generalisation and estimated loss and generalisation accuracy\n",
    "                training_loss = torch.mean((training_pred - training_targets)**2, dim=0)\n",
    "                validation_loss = torch.mean((validation_pred - validation_targets)**2, dim=0)\n",
    "                if self.binary: \n",
    "                    accuracy = torch.mean(((validation_pred > 0.5)&(validation_targets > 0.5)).to(dtype))\n",
    "                else :\n",
    "                    accuracy = torch.mean((torch.argmax(validation_pred, dim=1) == torch.argmax(validation_targets, dim=1)).to(dtype))\n",
    "                \n",
    "                # Record \n",
    "                self.training_loss_trajectory.append(training_loss)\n",
    "                self.validation_loss_trajectory.append(validation_loss)\n",
    "                self.accuracy_trajectory.append(accuracy)\n",
    "                \n",
    "                # Display informations about training\n",
    "                print(f\"Iteration '{i}' - Accuracy : '{accuracy}' - Training loss : '{training_loss}' - Validation loss : '{validation_loss}\")\n",
    "\n",
    "            # Computing loss gradient by backpropagation\n",
    "            training_loss.backward()\n",
    "            \n",
    "            # Updating parameters\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return 'Training done'\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36584c48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binary_classification_two_layer_NN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m binary_model_2_layer = \u001b[43mbinary_classification_two_layer_NN\u001b[49m(\u001b[32m1024\u001b[39m, \u001b[32m512\u001b[39m, eps_init = \u001b[32m1e-2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'binary_classification_two_layer_NN' is not defined"
     ]
    }
   ],
   "source": [
    "binary_model_2_layer = binary_classification_two_layer_NN(1024, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57236d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2.55, 1e-5, 1e8, 0, 0, 1, 0.2, 10, True, True, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer = binary_classification_three_layer_NN(1024, 512, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c674f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer.train_layers(x_train, y_train, x_valid, y_valid, 2.3, 1e-3, 1e8, 0, 0, 0, 1, 0.2, 10, True, True, True, 0.7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347eb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer_1_untrained = binary_classification_three_layer_NN(1024, 512, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3149bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2.6, 1e-2, 1e8, 0, 0, 0, 1, 0.2, 10, True, False, True, 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73751469",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_4_layer = binary_classification_four_layer_NN(1024, 512, 512, 512, eps_init = 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbaf678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2.6, the number of datas used for the training is 335544320.0000002 and the number of iterations is 167772.\n",
      "Iteration 0 Training loss 0.12490908056497574 Validation loss 0.12452017515897751 Accuracy 0.5745000243186951\n",
      "Iteration 10 Training loss 0.11853552609682083 Validation loss 0.11254864186048508 Accuracy 0.6780000329017639\n",
      "Iteration 20 Training loss 0.10843633115291595 Validation loss 0.09701646119356155 Accuracy 0.7000000476837158\n",
      "Iteration 30 Training loss 0.10438546538352966 Validation loss 0.10422158241271973 Accuracy 0.6645000576972961\n",
      "Iteration 40 Training loss 0.09644795954227448 Validation loss 0.09279250353574753 Accuracy 0.7240000367164612\n",
      "Iteration 50 Training loss 0.09662909060716629 Validation loss 0.09754127264022827 Accuracy 0.7110000252723694\n",
      "Iteration 60 Training loss 0.09286392480134964 Validation loss 0.09699703007936478 Accuracy 0.718500018119812\n",
      "Iteration 70 Training loss 0.09133835136890411 Validation loss 0.09527596086263657 Accuracy 0.7355000376701355\n",
      "Iteration 80 Training loss 0.08780815452337265 Validation loss 0.09414151310920715 Accuracy 0.7410000562667847\n",
      "Iteration 90 Training loss 0.08711432665586472 Validation loss 0.09296127408742905 Accuracy 0.7460000514984131\n",
      "Iteration 100 Training loss 0.08351312577724457 Validation loss 0.08527292311191559 Accuracy 0.7640000581741333\n",
      "Iteration 110 Training loss 0.08616191148757935 Validation loss 0.08844947069883347 Accuracy 0.7625000476837158\n",
      "Iteration 120 Training loss 0.08327411860227585 Validation loss 0.09445080161094666 Accuracy 0.7440000176429749\n",
      "Iteration 130 Training loss 0.0822230875492096 Validation loss 0.09121447056531906 Accuracy 0.7555000185966492\n",
      "Iteration 140 Training loss 0.08900900930166245 Validation loss 0.09458404779434204 Accuracy 0.7480000257492065\n",
      "Iteration 150 Training loss 0.07616392523050308 Validation loss 0.0763840302824974 Accuracy 0.7980000376701355\n",
      "Iteration 160 Training loss 0.07869775593280792 Validation loss 0.0729023665189743 Accuracy 0.8010000586509705\n",
      "Iteration 170 Training loss 0.07380572706460953 Validation loss 0.07235567271709442 Accuracy 0.8060000538825989\n",
      "Iteration 180 Training loss 0.07489563524723053 Validation loss 0.0720411092042923 Accuracy 0.8040000200271606\n",
      "Iteration 190 Training loss 0.07696986943483353 Validation loss 0.07381810992956161 Accuracy 0.8035000562667847\n",
      "Iteration 200 Training loss 0.06807415932416916 Validation loss 0.07653169333934784 Accuracy 0.8005000352859497\n",
      "Iteration 210 Training loss 0.0765475183725357 Validation loss 0.08487937599420547 Accuracy 0.7780000567436218\n",
      "Iteration 220 Training loss 0.07532394677400589 Validation loss 0.09329559653997421 Accuracy 0.7520000338554382\n",
      "Iteration 230 Training loss 0.07666224986314774 Validation loss 0.08431310206651688 Accuracy 0.7760000228881836\n",
      "Iteration 240 Training loss 0.0712868794798851 Validation loss 0.07755985856056213 Accuracy 0.7975000143051147\n",
      "Iteration 250 Training loss 0.06862691044807434 Validation loss 0.06940547376871109 Accuracy 0.8180000185966492\n",
      "Iteration 260 Training loss 0.07256921380758286 Validation loss 0.06796615570783615 Accuracy 0.8240000605583191\n",
      "Iteration 270 Training loss 0.07332272082567215 Validation loss 0.06900917738676071 Accuracy 0.8200000524520874\n",
      "Iteration 280 Training loss 0.07160616666078568 Validation loss 0.07755379378795624 Accuracy 0.7990000247955322\n",
      "Iteration 290 Training loss 0.06763890385627747 Validation loss 0.06720752269029617 Accuracy 0.8230000138282776\n",
      "Iteration 300 Training loss 0.07171934098005295 Validation loss 0.06801415234804153 Accuracy 0.8230000138282776\n",
      "Iteration 310 Training loss 0.06783182919025421 Validation loss 0.06545111536979675 Accuracy 0.8240000605583191\n",
      "Iteration 320 Training loss 0.0717458501458168 Validation loss 0.06660177558660507 Accuracy 0.8280000686645508\n",
      "Iteration 330 Training loss 0.06704655289649963 Validation loss 0.06460577994585037 Accuracy 0.82750004529953\n",
      "Iteration 340 Training loss 0.06580911576747894 Validation loss 0.06399712711572647 Accuracy 0.8250000476837158\n",
      "Iteration 350 Training loss 0.0648161768913269 Validation loss 0.06315971910953522 Accuracy 0.82750004529953\n",
      "Iteration 360 Training loss 0.06788571923971176 Validation loss 0.062328312546014786 Accuracy 0.8320000171661377\n",
      "Iteration 370 Training loss 0.07417620718479156 Validation loss 0.06855913251638412 Accuracy 0.8220000267028809\n",
      "Iteration 380 Training loss 0.06602919101715088 Validation loss 0.0614548996090889 Accuracy 0.8295000195503235\n",
      "Iteration 390 Training loss 0.0638229176402092 Validation loss 0.06113231182098389 Accuracy 0.8330000638961792\n",
      "Iteration 400 Training loss 0.06253039091825485 Validation loss 0.06030183658003807 Accuracy 0.8355000615119934\n",
      "Iteration 410 Training loss 0.06624642014503479 Validation loss 0.06023694574832916 Accuracy 0.8385000228881836\n",
      "Iteration 420 Training loss 0.06094865873456001 Validation loss 0.0593126155436039 Accuracy 0.8405000567436218\n",
      "Iteration 430 Training loss 0.06471913307905197 Validation loss 0.059592410922050476 Accuracy 0.846500039100647\n",
      "Iteration 440 Training loss 0.0662548765540123 Validation loss 0.05906930938363075 Accuracy 0.8445000648498535\n",
      "Iteration 450 Training loss 0.06240216642618179 Validation loss 0.05790869519114494 Accuracy 0.8445000648498535\n",
      "Iteration 460 Training loss 0.06459218263626099 Validation loss 0.057445965707302094 Accuracy 0.846500039100647\n",
      "Iteration 470 Training loss 0.06644850224256516 Validation loss 0.05809050053358078 Accuracy 0.8495000600814819\n",
      "Iteration 480 Training loss 0.06781738251447678 Validation loss 0.05972222983837128 Accuracy 0.8490000367164612\n",
      "Iteration 490 Training loss 0.06335242837667465 Validation loss 0.05657929927110672 Accuracy 0.8480000495910645\n",
      "Iteration 500 Training loss 0.05805418640375137 Validation loss 0.06060178577899933 Accuracy 0.8415000438690186\n",
      "Iteration 510 Training loss 0.06781517714262009 Validation loss 0.07911323755979538 Accuracy 0.7955000400543213\n",
      "Iteration 520 Training loss 0.05623851343989372 Validation loss 0.05856805294752121 Accuracy 0.8500000238418579\n",
      "Iteration 530 Training loss 0.06667716056108475 Validation loss 0.0725027397274971 Accuracy 0.8130000233650208\n",
      "Iteration 540 Training loss 0.06055241450667381 Validation loss 0.06589306890964508 Accuracy 0.8300000429153442\n",
      "Iteration 550 Training loss 0.0597054548561573 Validation loss 0.05877363681793213 Accuracy 0.8505000472068787\n",
      "Iteration 560 Training loss 0.05830218642950058 Validation loss 0.057115159928798676 Accuracy 0.8565000295639038\n",
      "Iteration 570 Training loss 0.05829041078686714 Validation loss 0.07124161720275879 Accuracy 0.8185000419616699\n",
      "Iteration 580 Training loss 0.053446974605321884 Validation loss 0.0642043948173523 Accuracy 0.8370000123977661\n",
      "Iteration 590 Training loss 0.05706765502691269 Validation loss 0.05520878732204437 Accuracy 0.8530000448226929\n",
      "Iteration 600 Training loss 0.05852104350924492 Validation loss 0.05817323923110962 Accuracy 0.8535000681877136\n",
      "Iteration 610 Training loss 0.059529539197683334 Validation loss 0.07390610128641129 Accuracy 0.8105000257492065\n",
      "Iteration 620 Training loss 0.055957019329071045 Validation loss 0.05544591695070267 Accuracy 0.8555000424385071\n",
      "Iteration 630 Training loss 0.0566190741956234 Validation loss 0.05391673371195793 Accuracy 0.8615000247955322\n",
      "Iteration 640 Training loss 0.05989691987633705 Validation loss 0.05450411140918732 Accuracy 0.862500011920929\n",
      "Iteration 650 Training loss 0.054845526814460754 Validation loss 0.05389228090643883 Accuracy 0.862000048160553\n",
      "Iteration 660 Training loss 0.05380135774612427 Validation loss 0.05842123553156853 Accuracy 0.8550000190734863\n",
      "Iteration 670 Training loss 0.06141716241836548 Validation loss 0.06600126624107361 Accuracy 0.8335000276565552\n",
      "Iteration 680 Training loss 0.05866043269634247 Validation loss 0.06857554614543915 Accuracy 0.8300000429153442\n",
      "Iteration 690 Training loss 0.05913817882537842 Validation loss 0.06661584228277206 Accuracy 0.8335000276565552\n",
      "Iteration 700 Training loss 0.0568353645503521 Validation loss 0.060678623616695404 Accuracy 0.8540000319480896\n",
      "Iteration 710 Training loss 0.052992548793554306 Validation loss 0.05641020089387894 Accuracy 0.8575000166893005\n",
      "Iteration 720 Training loss 0.054496899247169495 Validation loss 0.058024294674396515 Accuracy 0.8580000400543213\n",
      "Iteration 730 Training loss 0.054953012615442276 Validation loss 0.05753808096051216 Accuracy 0.8575000166893005\n",
      "Iteration 740 Training loss 0.055162519216537476 Validation loss 0.05311870202422142 Accuracy 0.8640000224113464\n",
      "Iteration 750 Training loss 0.054258789867162704 Validation loss 0.053735408931970596 Accuracy 0.8645000457763672\n",
      "Iteration 760 Training loss 0.05396800860762596 Validation loss 0.052424218505620956 Accuracy 0.8665000200271606\n",
      "Iteration 770 Training loss 0.05587845668196678 Validation loss 0.05297424644231796 Accuracy 0.8650000691413879\n",
      "Iteration 780 Training loss 0.06145837903022766 Validation loss 0.05833300203084946 Accuracy 0.8500000238418579\n",
      "Iteration 790 Training loss 0.05220489576458931 Validation loss 0.05257586017251015 Accuracy 0.8650000691413879\n",
      "Iteration 800 Training loss 0.057683899998664856 Validation loss 0.05554363504052162 Accuracy 0.8600000143051147\n",
      "Iteration 810 Training loss 0.05143550783395767 Validation loss 0.052810221910476685 Accuracy 0.862500011920929\n",
      "Iteration 820 Training loss 0.05764836445450783 Validation loss 0.05606401711702347 Accuracy 0.8615000247955322\n",
      "Iteration 830 Training loss 0.05639045685529709 Validation loss 0.05821162834763527 Accuracy 0.8580000400543213\n",
      "Iteration 840 Training loss 0.05402517318725586 Validation loss 0.05528581514954567 Accuracy 0.8630000352859497\n",
      "Iteration 850 Training loss 0.05388934537768364 Validation loss 0.056862667202949524 Accuracy 0.8595000505447388\n",
      "Iteration 860 Training loss 0.05614043399691582 Validation loss 0.05745561420917511 Accuracy 0.8595000505447388\n",
      "Iteration 870 Training loss 0.05130401998758316 Validation loss 0.0517619363963604 Accuracy 0.8660000562667847\n",
      "Iteration 880 Training loss 0.05071140453219414 Validation loss 0.05182197690010071 Accuracy 0.8700000643730164\n",
      "Iteration 890 Training loss 0.05107453465461731 Validation loss 0.05196215584874153 Accuracy 0.8705000281333923\n",
      "Iteration 900 Training loss 0.05296013131737709 Validation loss 0.05202263221144676 Accuracy 0.8680000305175781\n"
     ]
    }
   ],
   "source": [
    "binary_model_4_layer.train_layers(x_train, y_train, x_valid, y_valid, 2.6, 1e-2, 1e8, 0, 0, 0, 0, 1, 0.2, 10, True, True, True, True, 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "122fd711",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_4_layer_extreme_trained = binary_classification_four_layer_NN(1024, 512, 512, 512, eps_init = 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43452ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2.45, the number of datas used for the training is 118632832.03031458 and the number of iterations is 59316.\n",
      "Iteration 0 Training loss 0.12539519369602203 Validation loss 0.12574827671051025 Accuracy 0.4325000047683716\n",
      "Iteration 10 Training loss 0.10047008097171783 Validation loss 0.09203044325113297 Accuracy 0.7139999866485596\n",
      "Iteration 20 Training loss 0.1087561696767807 Validation loss 0.11372732371091843 Accuracy 0.6610000133514404\n",
      "Iteration 30 Training loss 0.0931905061006546 Validation loss 0.0894446074962616 Accuracy 0.7329999804496765\n",
      "Iteration 40 Training loss 0.0950341522693634 Validation loss 0.0944846048951149 Accuracy 0.7329999804496765\n",
      "Iteration 50 Training loss 0.09136393666267395 Validation loss 0.09274795651435852 Accuracy 0.7394999861717224\n",
      "Iteration 60 Training loss 0.09249433875083923 Validation loss 0.09605371952056885 Accuracy 0.7394999861717224\n",
      "Iteration 70 Training loss 0.0891943871974945 Validation loss 0.08455349504947662 Accuracy 0.765500009059906\n",
      "Iteration 80 Training loss 0.08025811612606049 Validation loss 0.08056599646806717 Accuracy 0.7820000052452087\n",
      "Iteration 90 Training loss 0.08118142932653427 Validation loss 0.07878831773996353 Accuracy 0.7854999899864197\n",
      "Iteration 100 Training loss 0.07921281456947327 Validation loss 0.07796603441238403 Accuracy 0.7894999980926514\n",
      "Iteration 110 Training loss 0.08448769897222519 Validation loss 0.08278752118349075 Accuracy 0.7749999761581421\n",
      "Iteration 120 Training loss 0.07865138351917267 Validation loss 0.07424869388341904 Accuracy 0.7990000247955322\n",
      "Iteration 130 Training loss 0.07722777128219604 Validation loss 0.06985403597354889 Accuracy 0.8115000128746033\n",
      "Iteration 140 Training loss 0.07521986216306686 Validation loss 0.07295385748147964 Accuracy 0.8029999732971191\n",
      "Iteration 150 Training loss 0.07141054421663284 Validation loss 0.06757252663373947 Accuracy 0.8184999823570251\n",
      "Iteration 160 Training loss 0.06995426118373871 Validation loss 0.0658157616853714 Accuracy 0.8199999928474426\n",
      "Iteration 170 Training loss 0.07272466272115707 Validation loss 0.06686335057020187 Accuracy 0.8205000162124634\n",
      "Iteration 180 Training loss 0.06914711743593216 Validation loss 0.06444387882947922 Accuracy 0.8270000219345093\n",
      "Iteration 190 Training loss 0.06729066371917725 Validation loss 0.06354265660047531 Accuracy 0.8274999856948853\n",
      "Iteration 200 Training loss 0.0794527605175972 Validation loss 0.08116233348846436 Accuracy 0.7860000133514404\n",
      "Iteration 210 Training loss 0.07086918503046036 Validation loss 0.06547458469867706 Accuracy 0.8245000243186951\n",
      "Iteration 220 Training loss 0.07492604106664658 Validation loss 0.07036174833774567 Accuracy 0.815500020980835\n",
      "Iteration 230 Training loss 0.0656176283955574 Validation loss 0.06230673938989639 Accuracy 0.8385000228881836\n",
      "Iteration 240 Training loss 0.06454495340585709 Validation loss 0.06486685574054718 Accuracy 0.8264999985694885\n",
      "Iteration 250 Training loss 0.05998297035694122 Validation loss 0.0608665756881237 Accuracy 0.8445000052452087\n",
      "Iteration 260 Training loss 0.06564934551715851 Validation loss 0.05989374592900276 Accuracy 0.8450000286102295\n",
      "Iteration 270 Training loss 0.07077945023775101 Validation loss 0.061328619718551636 Accuracy 0.8429999947547913\n",
      "Iteration 280 Training loss 0.07430236786603928 Validation loss 0.0735534057021141 Accuracy 0.8069999814033508\n",
      "Iteration 290 Training loss 0.06829351931810379 Validation loss 0.061689820140600204 Accuracy 0.8399999737739563\n",
      "Iteration 300 Training loss 0.06711950153112411 Validation loss 0.060162436217069626 Accuracy 0.8450000286102295\n",
      "Iteration 310 Training loss 0.06737657636404037 Validation loss 0.06743989139795303 Accuracy 0.8245000243186951\n",
      "Iteration 320 Training loss 0.06657177954912186 Validation loss 0.05934779345989227 Accuracy 0.8504999876022339\n",
      "Iteration 330 Training loss 0.05858781561255455 Validation loss 0.05799410864710808 Accuracy 0.8514999747276306\n",
      "Iteration 340 Training loss 0.06533823907375336 Validation loss 0.06148543208837509 Accuracy 0.8349999785423279\n",
      "Iteration 350 Training loss 0.06884509325027466 Validation loss 0.06610269099473953 Accuracy 0.8255000114440918\n",
      "Iteration 360 Training loss 0.05978052690625191 Validation loss 0.05865039676427841 Accuracy 0.847000002861023\n",
      "Iteration 370 Training loss 0.06672081351280212 Validation loss 0.06247960776090622 Accuracy 0.8395000100135803\n",
      "Iteration 380 Training loss 0.06256525218486786 Validation loss 0.056834883987903595 Accuracy 0.8510000109672546\n",
      "Iteration 390 Training loss 0.06521834433078766 Validation loss 0.05961087346076965 Accuracy 0.843500018119812\n",
      "Iteration 400 Training loss 0.06561162322759628 Validation loss 0.062168046832084656 Accuracy 0.8379999995231628\n",
      "Iteration 410 Training loss 0.06624789535999298 Validation loss 0.05922422185540199 Accuracy 0.8445000052452087\n",
      "Iteration 420 Training loss 0.06137677654623985 Validation loss 0.05807888135313988 Accuracy 0.847000002861023\n",
      "Iteration 430 Training loss 0.06543426960706711 Validation loss 0.0595410093665123 Accuracy 0.8475000262260437\n",
      "Iteration 440 Training loss 0.058995988219976425 Validation loss 0.05597234517335892 Accuracy 0.8510000109672546\n",
      "Iteration 450 Training loss 0.06550291180610657 Validation loss 0.05842909216880798 Accuracy 0.8485000133514404\n",
      "Iteration 460 Training loss 0.06041906774044037 Validation loss 0.05787333473563194 Accuracy 0.847000002861023\n",
      "Iteration 470 Training loss 0.06358855217695236 Validation loss 0.05759948119521141 Accuracy 0.8510000109672546\n",
      "Iteration 480 Training loss 0.06162741035223007 Validation loss 0.058962829411029816 Accuracy 0.8464999794960022\n",
      "Iteration 490 Training loss 0.05777609348297119 Validation loss 0.055684272199869156 Accuracy 0.8529999852180481\n",
      "Iteration 500 Training loss 0.06698042154312134 Validation loss 0.061246857047080994 Accuracy 0.8450000286102295\n",
      "Iteration 510 Training loss 0.06959428638219833 Validation loss 0.060896605253219604 Accuracy 0.8410000205039978\n",
      "Iteration 520 Training loss 0.06484510004520416 Validation loss 0.062010593712329865 Accuracy 0.8389999866485596\n",
      "Iteration 530 Training loss 0.06372684985399246 Validation loss 0.059380143880844116 Accuracy 0.8450000286102295\n",
      "Iteration 540 Training loss 0.059306010603904724 Validation loss 0.054779473692178726 Accuracy 0.859499990940094\n",
      "Iteration 550 Training loss 0.057649049907922745 Validation loss 0.0556119903922081 Accuracy 0.8525000214576721\n",
      "Iteration 560 Training loss 0.0613594576716423 Validation loss 0.06000540778040886 Accuracy 0.843500018119812\n",
      "Iteration 570 Training loss 0.05634472891688347 Validation loss 0.05587657168507576 Accuracy 0.8525000214576721\n",
      "Iteration 580 Training loss 0.055798713117837906 Validation loss 0.055558137595653534 Accuracy 0.8519999980926514\n",
      "Iteration 590 Training loss 0.05760286748409271 Validation loss 0.05502787604928017 Accuracy 0.8560000061988831\n",
      "Iteration 600 Training loss 0.06159036606550217 Validation loss 0.05610926076769829 Accuracy 0.8544999957084656\n",
      "Iteration 610 Training loss 0.05751262232661247 Validation loss 0.05525048077106476 Accuracy 0.8579999804496765\n",
      "Iteration 620 Training loss 0.06504083424806595 Validation loss 0.06421517580747604 Accuracy 0.8339999914169312\n",
      "Iteration 630 Training loss 0.059274572879076004 Validation loss 0.05564253032207489 Accuracy 0.8575000166893005\n",
      "Iteration 640 Training loss 0.058455146849155426 Validation loss 0.054865144193172455 Accuracy 0.859499990940094\n",
      "Iteration 650 Training loss 0.0549154207110405 Validation loss 0.054748162627220154 Accuracy 0.8569999933242798\n",
      "Iteration 660 Training loss 0.05598615109920502 Validation loss 0.05532696470618248 Accuracy 0.8575000166893005\n",
      "Iteration 670 Training loss 0.05937571823596954 Validation loss 0.05531657859683037 Accuracy 0.8575000166893005\n",
      "Iteration 680 Training loss 0.0662480890750885 Validation loss 0.06394501030445099 Accuracy 0.8379999995231628\n",
      "Iteration 690 Training loss 0.07210652530193329 Validation loss 0.07071317732334137 Accuracy 0.8205000162124634\n",
      "Iteration 700 Training loss 0.058223459869623184 Validation loss 0.05583666265010834 Accuracy 0.8560000061988831\n",
      "Iteration 710 Training loss 0.056849151849746704 Validation loss 0.05636034905910492 Accuracy 0.8544999957084656\n",
      "Iteration 720 Training loss 0.06001944839954376 Validation loss 0.06102178618311882 Accuracy 0.843500018119812\n",
      "Iteration 730 Training loss 0.05659511685371399 Validation loss 0.05486668646335602 Accuracy 0.8575000166893005\n",
      "Iteration 740 Training loss 0.05632321909070015 Validation loss 0.057207152247428894 Accuracy 0.8500000238418579\n",
      "Iteration 750 Training loss 0.05892195180058479 Validation loss 0.056525859981775284 Accuracy 0.8519999980926514\n",
      "Iteration 760 Training loss 0.06195417046546936 Validation loss 0.055576957762241364 Accuracy 0.8544999957084656\n",
      "Iteration 770 Training loss 0.05736422911286354 Validation loss 0.05369895324110985 Accuracy 0.8575000166893005\n",
      "Iteration 780 Training loss 0.06077118590474129 Validation loss 0.05703694000840187 Accuracy 0.8519999980926514\n",
      "Iteration 790 Training loss 0.060684219002723694 Validation loss 0.05649639293551445 Accuracy 0.8519999980926514\n",
      "Iteration 800 Training loss 0.06111646443605423 Validation loss 0.059148482978343964 Accuracy 0.8450000286102295\n",
      "Iteration 810 Training loss 0.05871926248073578 Validation loss 0.05655002221465111 Accuracy 0.8539999723434448\n",
      "Iteration 820 Training loss 0.05478770658373833 Validation loss 0.05286769941449165 Accuracy 0.8610000014305115\n",
      "Iteration 830 Training loss 0.05709216371178627 Validation loss 0.055564895272254944 Accuracy 0.8554999828338623\n",
      "Iteration 840 Training loss 0.06442811340093613 Validation loss 0.05885783210396767 Accuracy 0.8460000157356262\n",
      "Iteration 850 Training loss 0.05529295280575752 Validation loss 0.05321457237005234 Accuracy 0.8619999885559082\n",
      "Iteration 860 Training loss 0.06343111395835876 Validation loss 0.05701243877410889 Accuracy 0.8514999747276306\n",
      "Iteration 870 Training loss 0.06064128875732422 Validation loss 0.05443790927529335 Accuracy 0.8579999804496765\n",
      "Iteration 880 Training loss 0.06413983553647995 Validation loss 0.05495123565196991 Accuracy 0.859000027179718\n",
      "Iteration 890 Training loss 0.0653025358915329 Validation loss 0.059672921895980835 Accuracy 0.8464999794960022\n",
      "Iteration 900 Training loss 0.06350831687450409 Validation loss 0.057651273906230927 Accuracy 0.8510000109672546\n",
      "Iteration 910 Training loss 0.058000434190034866 Validation loss 0.054685067385435104 Accuracy 0.8554999828338623\n",
      "Iteration 920 Training loss 0.05505248159170151 Validation loss 0.05538969486951828 Accuracy 0.8569999933242798\n",
      "Iteration 930 Training loss 0.06160440295934677 Validation loss 0.055881768465042114 Accuracy 0.8529999852180481\n",
      "Iteration 940 Training loss 0.057428352534770966 Validation loss 0.05411742255091667 Accuracy 0.859000027179718\n",
      "Iteration 950 Training loss 0.05456392839550972 Validation loss 0.05246150121092796 Accuracy 0.8640000224113464\n",
      "Iteration 960 Training loss 0.05543764680624008 Validation loss 0.05347862094640732 Accuracy 0.8600000143051147\n",
      "Iteration 970 Training loss 0.05354468524456024 Validation loss 0.05210372060537338 Accuracy 0.8665000200271606\n",
      "Iteration 980 Training loss 0.058333273977041245 Validation loss 0.05632760748267174 Accuracy 0.8550000190734863\n",
      "Iteration 990 Training loss 0.05906311795115471 Validation loss 0.05605948343873024 Accuracy 0.8554999828338623\n",
      "Iteration 1000 Training loss 0.05597493052482605 Validation loss 0.05354844033718109 Accuracy 0.8610000014305115\n",
      "Iteration 1010 Training loss 0.05242852121591568 Validation loss 0.05367690324783325 Accuracy 0.8604999780654907\n",
      "Iteration 1020 Training loss 0.05941992998123169 Validation loss 0.054179660975933075 Accuracy 0.8575000166893005\n",
      "Iteration 1030 Training loss 0.05840151011943817 Validation loss 0.05783649533987045 Accuracy 0.8510000109672546\n",
      "Iteration 1040 Training loss 0.05328565835952759 Validation loss 0.054666053503751755 Accuracy 0.859000027179718\n",
      "Iteration 1050 Training loss 0.05572742968797684 Validation loss 0.052530091255903244 Accuracy 0.8640000224113464\n",
      "Iteration 1060 Training loss 0.057792779058218 Validation loss 0.053566887974739075 Accuracy 0.8569999933242798\n",
      "Iteration 1070 Training loss 0.054639995098114014 Validation loss 0.05314291641116142 Accuracy 0.8650000095367432\n",
      "Iteration 1080 Training loss 0.057618483901023865 Validation loss 0.05552060529589653 Accuracy 0.8554999828338623\n",
      "Iteration 1090 Training loss 0.05827564373612404 Validation loss 0.057010259479284286 Accuracy 0.8525000214576721\n",
      "Iteration 1100 Training loss 0.0581752248108387 Validation loss 0.053896643221378326 Accuracy 0.8610000014305115\n",
      "Iteration 1110 Training loss 0.055624380707740784 Validation loss 0.053220443427562714 Accuracy 0.8600000143051147\n",
      "Iteration 1120 Training loss 0.06074681878089905 Validation loss 0.058619361370801926 Accuracy 0.8510000109672546\n",
      "Iteration 1130 Training loss 0.05848073586821556 Validation loss 0.0573405921459198 Accuracy 0.8500000238418579\n",
      "Iteration 1140 Training loss 0.05982360243797302 Validation loss 0.05426419898867607 Accuracy 0.8604999780654907\n",
      "Iteration 1150 Training loss 0.0674397274851799 Validation loss 0.058464694768190384 Accuracy 0.8500000238418579\n",
      "Iteration 1160 Training loss 0.061566177755594254 Validation loss 0.055380966514348984 Accuracy 0.8539999723434448\n",
      "Iteration 1170 Training loss 0.06275901943445206 Validation loss 0.05836711823940277 Accuracy 0.8510000109672546\n",
      "Iteration 1180 Training loss 0.059931680560112 Validation loss 0.05584275349974632 Accuracy 0.8575000166893005\n",
      "Iteration 1190 Training loss 0.05139379948377609 Validation loss 0.052010104060173035 Accuracy 0.8650000095367432\n",
      "Iteration 1200 Training loss 0.054367173463106155 Validation loss 0.05134442076086998 Accuracy 0.8665000200271606\n",
      "Iteration 1210 Training loss 0.06351642310619354 Validation loss 0.05574653670191765 Accuracy 0.8550000190734863\n",
      "Iteration 1220 Training loss 0.05983003228902817 Validation loss 0.0580543614923954 Accuracy 0.8529999852180481\n",
      "Iteration 1230 Training loss 0.05438527092337608 Validation loss 0.0532657615840435 Accuracy 0.8585000038146973\n",
      "Iteration 1240 Training loss 0.05264149233698845 Validation loss 0.05159645900130272 Accuracy 0.8679999709129333\n",
      "Iteration 1250 Training loss 0.05744723603129387 Validation loss 0.05386822670698166 Accuracy 0.8569999933242798\n",
      "Iteration 1260 Training loss 0.06382232159376144 Validation loss 0.06167706474661827 Accuracy 0.8489999771118164\n",
      "Iteration 1270 Training loss 0.05339478328824043 Validation loss 0.05223068222403526 Accuracy 0.8634999990463257\n",
      "Iteration 1280 Training loss 0.06591761112213135 Validation loss 0.06011508032679558 Accuracy 0.847000002861023\n",
      "Iteration 1290 Training loss 0.057843681424856186 Validation loss 0.05649043619632721 Accuracy 0.8560000061988831\n",
      "Iteration 1300 Training loss 0.06522836536169052 Validation loss 0.0572136789560318 Accuracy 0.8525000214576721\n",
      "Iteration 1310 Training loss 0.0514804981648922 Validation loss 0.05140865594148636 Accuracy 0.8675000071525574\n",
      "Iteration 1320 Training loss 0.05161914974451065 Validation loss 0.052201613783836365 Accuracy 0.8640000224113464\n",
      "Iteration 1330 Training loss 0.055809665471315384 Validation loss 0.05327913910150528 Accuracy 0.8615000247955322\n",
      "Iteration 1340 Training loss 0.05527212470769882 Validation loss 0.05325203016400337 Accuracy 0.8610000014305115\n",
      "Iteration 1350 Training loss 0.05638878419995308 Validation loss 0.05259156599640846 Accuracy 0.8640000224113464\n",
      "Iteration 1360 Training loss 0.05841654911637306 Validation loss 0.05212033912539482 Accuracy 0.8640000224113464\n",
      "Iteration 1370 Training loss 0.05480198934674263 Validation loss 0.054563529789447784 Accuracy 0.8604999780654907\n",
      "Iteration 1380 Training loss 0.06377173215150833 Validation loss 0.058329422026872635 Accuracy 0.8554999828338623\n",
      "Iteration 1390 Training loss 0.055210139602422714 Validation loss 0.05579863116145134 Accuracy 0.8569999933242798\n",
      "Iteration 1400 Training loss 0.058480069041252136 Validation loss 0.057939860969781876 Accuracy 0.8539999723434448\n",
      "Iteration 1410 Training loss 0.05136004090309143 Validation loss 0.051419563591480255 Accuracy 0.8675000071525574\n",
      "Iteration 1420 Training loss 0.05620737001299858 Validation loss 0.05445109307765961 Accuracy 0.859000027179718\n",
      "Iteration 1430 Training loss 0.055993180721998215 Validation loss 0.05583512783050537 Accuracy 0.8569999933242798\n",
      "Iteration 1440 Training loss 0.05760965123772621 Validation loss 0.05385768041014671 Accuracy 0.8569999933242798\n",
      "Iteration 1450 Training loss 0.053741704672575 Validation loss 0.052862498909235 Accuracy 0.8610000014305115\n",
      "Iteration 1460 Training loss 0.05082070082426071 Validation loss 0.051262956112623215 Accuracy 0.8679999709129333\n",
      "Iteration 1470 Training loss 0.05573918670415878 Validation loss 0.05471540987491608 Accuracy 0.859499990940094\n",
      "Iteration 1480 Training loss 0.05162357538938522 Validation loss 0.05165717005729675 Accuracy 0.8684999942779541\n",
      "Iteration 1490 Training loss 0.054049052298069 Validation loss 0.0522124320268631 Accuracy 0.8679999709129333\n",
      "Iteration 1500 Training loss 0.06522068381309509 Validation loss 0.05750688537955284 Accuracy 0.8569999933242798\n",
      "Iteration 1510 Training loss 0.06244790181517601 Validation loss 0.056817714124917984 Accuracy 0.8579999804496765\n",
      "Iteration 1520 Training loss 0.055068474262952805 Validation loss 0.05275915563106537 Accuracy 0.8644999861717224\n",
      "Iteration 1530 Training loss 0.05403647571802139 Validation loss 0.05201556906104088 Accuracy 0.8650000095367432\n",
      "Iteration 1540 Training loss 0.05499814450740814 Validation loss 0.05284726619720459 Accuracy 0.8634999990463257\n",
      "Iteration 1550 Training loss 0.06034723296761513 Validation loss 0.0576632097363472 Accuracy 0.8544999957084656\n",
      "Iteration 1560 Training loss 0.058789730072021484 Validation loss 0.05143680050969124 Accuracy 0.8669999837875366\n",
      "Iteration 1570 Training loss 0.05731998756527901 Validation loss 0.052975382655858994 Accuracy 0.862500011920929\n",
      "Iteration 1580 Training loss 0.052149638533592224 Validation loss 0.05091661214828491 Accuracy 0.8679999709129333\n",
      "Iteration 1590 Training loss 0.06252467632293701 Validation loss 0.05500869080424309 Accuracy 0.859499990940094\n",
      "Iteration 1600 Training loss 0.055637795478105545 Validation loss 0.05071838200092316 Accuracy 0.8705000281333923\n",
      "Iteration 1610 Training loss 0.059140440076589584 Validation loss 0.058207061141729355 Accuracy 0.8539999723434448\n",
      "Iteration 1620 Training loss 0.06013602390885353 Validation loss 0.055760931223630905 Accuracy 0.859499990940094\n",
      "Iteration 1630 Training loss 0.05858856812119484 Validation loss 0.053939517587423325 Accuracy 0.8619999885559082\n",
      "Iteration 1640 Training loss 0.052824966609478 Validation loss 0.051367636770009995 Accuracy 0.8675000071525574\n",
      "Iteration 1650 Training loss 0.04981086030602455 Validation loss 0.05112290009856224 Accuracy 0.8700000047683716\n",
      "Iteration 1660 Training loss 0.05881652235984802 Validation loss 0.05542296543717384 Accuracy 0.8600000143051147\n",
      "Iteration 1670 Training loss 0.06068519502878189 Validation loss 0.05560942366719246 Accuracy 0.859499990940094\n",
      "Iteration 1680 Training loss 0.05936773866415024 Validation loss 0.05687396228313446 Accuracy 0.8565000295639038\n",
      "Iteration 1690 Training loss 0.05555465817451477 Validation loss 0.05285501480102539 Accuracy 0.8634999990463257\n",
      "Iteration 1700 Training loss 0.053967904299497604 Validation loss 0.05300341919064522 Accuracy 0.8634999990463257\n",
      "Iteration 1710 Training loss 0.058404602110385895 Validation loss 0.052399538457393646 Accuracy 0.8650000095367432\n",
      "Iteration 1720 Training loss 0.055555637925863266 Validation loss 0.05514545366168022 Accuracy 0.8610000014305115\n",
      "Iteration 1730 Training loss 0.05803552269935608 Validation loss 0.0545685850083828 Accuracy 0.8610000014305115\n",
      "Iteration 1740 Training loss 0.051417093724012375 Validation loss 0.05117683485150337 Accuracy 0.8694999814033508\n",
      "Iteration 1750 Training loss 0.05668678134679794 Validation loss 0.05272595211863518 Accuracy 0.8650000095367432\n",
      "Iteration 1760 Training loss 0.05923474580049515 Validation loss 0.05312459543347359 Accuracy 0.8629999756813049\n",
      "Iteration 1770 Training loss 0.05660651996731758 Validation loss 0.05333862081170082 Accuracy 0.8610000014305115\n",
      "Iteration 1780 Training loss 0.05882418900728226 Validation loss 0.05592505633831024 Accuracy 0.859499990940094\n",
      "Iteration 1790 Training loss 0.06149293854832649 Validation loss 0.05818722024559975 Accuracy 0.8539999723434448\n",
      "Iteration 1800 Training loss 0.04966292902827263 Validation loss 0.05164056271314621 Accuracy 0.8659999966621399\n",
      "Iteration 1810 Training loss 0.056476935744285583 Validation loss 0.05097015202045441 Accuracy 0.8700000047683716\n",
      "Iteration 1820 Training loss 0.0520876869559288 Validation loss 0.05102730542421341 Accuracy 0.8700000047683716\n",
      "Iteration 1830 Training loss 0.05445261299610138 Validation loss 0.05071118101477623 Accuracy 0.8700000047683716\n",
      "Iteration 1840 Training loss 0.05387045443058014 Validation loss 0.051032084971666336 Accuracy 0.8700000047683716\n",
      "Iteration 1850 Training loss 0.06505119055509567 Validation loss 0.057931672781705856 Accuracy 0.8550000190734863\n",
      "Iteration 1860 Training loss 0.052861932665109634 Validation loss 0.05141837149858475 Accuracy 0.8665000200271606\n",
      "Iteration 1870 Training loss 0.05467800796031952 Validation loss 0.05082998052239418 Accuracy 0.8690000176429749\n",
      "Iteration 1880 Training loss 0.05499928072094917 Validation loss 0.05152801424264908 Accuracy 0.8679999709129333\n",
      "Iteration 1890 Training loss 0.05494047701358795 Validation loss 0.05124517157673836 Accuracy 0.8709999918937683\n",
      "Iteration 1900 Training loss 0.05308057367801666 Validation loss 0.05209909379482269 Accuracy 0.8650000095367432\n",
      "Iteration 1910 Training loss 0.04845385625958443 Validation loss 0.05023333802819252 Accuracy 0.8694999814033508\n",
      "Iteration 1920 Training loss 0.05538402497768402 Validation loss 0.05074673146009445 Accuracy 0.8700000047683716\n",
      "Iteration 1930 Training loss 0.06263174116611481 Validation loss 0.06051339954137802 Accuracy 0.8500000238418579\n",
      "Iteration 1940 Training loss 0.054365433752536774 Validation loss 0.051727429032325745 Accuracy 0.8694999814033508\n",
      "Iteration 1950 Training loss 0.0590369887650013 Validation loss 0.05573929101228714 Accuracy 0.8610000014305115\n",
      "Iteration 1960 Training loss 0.055269092321395874 Validation loss 0.05266229063272476 Accuracy 0.8669999837875366\n",
      "Iteration 1970 Training loss 0.04951472580432892 Validation loss 0.050590287894010544 Accuracy 0.871999979019165\n",
      "Iteration 1980 Training loss 0.0507381372153759 Validation loss 0.05068734288215637 Accuracy 0.8705000281333923\n",
      "Iteration 1990 Training loss 0.0530020073056221 Validation loss 0.050771307200193405 Accuracy 0.8675000071525574\n",
      "Iteration 2000 Training loss 0.05294547975063324 Validation loss 0.05139722302556038 Accuracy 0.8709999918937683\n",
      "Iteration 2010 Training loss 0.05722115561366081 Validation loss 0.053829580545425415 Accuracy 0.8640000224113464\n",
      "Iteration 2020 Training loss 0.052729152143001556 Validation loss 0.05082165077328682 Accuracy 0.8679999709129333\n",
      "Iteration 2030 Training loss 0.051956262439489365 Validation loss 0.05023379623889923 Accuracy 0.8715000152587891\n",
      "Iteration 2040 Training loss 0.05233624204993248 Validation loss 0.050402212888002396 Accuracy 0.8700000047683716\n",
      "Iteration 2050 Training loss 0.053635261952877045 Validation loss 0.05133355036377907 Accuracy 0.8700000047683716\n",
      "Iteration 2060 Training loss 0.05578140914440155 Validation loss 0.0569281168282032 Accuracy 0.8560000061988831\n",
      "Iteration 2070 Training loss 0.05381138622760773 Validation loss 0.05098191648721695 Accuracy 0.8700000047683716\n",
      "Iteration 2080 Training loss 0.050640132278203964 Validation loss 0.05072184279561043 Accuracy 0.8709999918937683\n",
      "Iteration 2090 Training loss 0.05160561576485634 Validation loss 0.0503266267478466 Accuracy 0.8725000023841858\n",
      "Iteration 2100 Training loss 0.05600466579198837 Validation loss 0.050777725875377655 Accuracy 0.871999979019165\n",
      "Iteration 2110 Training loss 0.05846966803073883 Validation loss 0.05159497633576393 Accuracy 0.8690000176429749\n",
      "Iteration 2120 Training loss 0.05414939299225807 Validation loss 0.05020029470324516 Accuracy 0.871999979019165\n",
      "Iteration 2130 Training loss 0.05580953136086464 Validation loss 0.0504152737557888 Accuracy 0.8709999918937683\n",
      "Iteration 2140 Training loss 0.053046394139528275 Validation loss 0.0500122494995594 Accuracy 0.8730000257492065\n",
      "Iteration 2150 Training loss 0.05134650319814682 Validation loss 0.05080154538154602 Accuracy 0.8679999709129333\n",
      "Iteration 2160 Training loss 0.07891473174095154 Validation loss 0.09929408133029938 Accuracy 0.7580000162124634\n",
      "Iteration 2170 Training loss 0.06727665662765503 Validation loss 0.07500330358743668 Accuracy 0.8165000081062317\n",
      "Iteration 2180 Training loss 0.05736264958977699 Validation loss 0.0673455148935318 Accuracy 0.8330000042915344\n",
      "Iteration 2190 Training loss 0.05476601421833038 Validation loss 0.06297897547483444 Accuracy 0.8410000205039978\n",
      "Iteration 2200 Training loss 0.06266386806964874 Validation loss 0.06728700548410416 Accuracy 0.8320000171661377\n",
      "Iteration 2210 Training loss 0.06268306821584702 Validation loss 0.07404498755931854 Accuracy 0.815500020980835\n",
      "Iteration 2220 Training loss 0.0548115000128746 Validation loss 0.06568003445863724 Accuracy 0.8345000147819519\n",
      "Iteration 2230 Training loss 0.05420156195759773 Validation loss 0.05949759855866432 Accuracy 0.8485000133514404\n",
      "Iteration 2240 Training loss 0.05840378627181053 Validation loss 0.06784458458423615 Accuracy 0.8309999704360962\n",
      "Iteration 2250 Training loss 0.054538097232580185 Validation loss 0.06957538425922394 Accuracy 0.828499972820282\n",
      "Iteration 2260 Training loss 0.051991041749715805 Validation loss 0.06348270177841187 Accuracy 0.840499997138977\n",
      "Iteration 2270 Training loss 0.051028911024332047 Validation loss 0.06109633669257164 Accuracy 0.8495000004768372\n",
      "Iteration 2280 Training loss 0.052313029766082764 Validation loss 0.060142882168293 Accuracy 0.8485000133514404\n",
      "Iteration 2290 Training loss 0.04995904490351677 Validation loss 0.05532603710889816 Accuracy 0.8600000143051147\n",
      "Iteration 2300 Training loss 0.059886422008275986 Validation loss 0.07070564478635788 Accuracy 0.8259999752044678\n",
      "Iteration 2310 Training loss 0.06255831569433212 Validation loss 0.07399104535579681 Accuracy 0.8220000267028809\n",
      "Iteration 2320 Training loss 0.057743363082408905 Validation loss 0.06775947660207748 Accuracy 0.8324999809265137\n",
      "Iteration 2330 Training loss 0.050047699362039566 Validation loss 0.05173566937446594 Accuracy 0.8684999942779541\n",
      "Iteration 2340 Training loss 0.056655626744031906 Validation loss 0.05351042374968529 Accuracy 0.8634999990463257\n",
      "Iteration 2350 Training loss 0.051172446459531784 Validation loss 0.05053626373410225 Accuracy 0.8705000281333923\n",
      "Iteration 2360 Training loss 0.05555509030818939 Validation loss 0.049609456211328506 Accuracy 0.8694999814033508\n",
      "Iteration 2370 Training loss 0.06307004392147064 Validation loss 0.05857086554169655 Accuracy 0.8539999723434448\n",
      "Iteration 2380 Training loss 0.057656899094581604 Validation loss 0.052780631929636 Accuracy 0.8669999837875366\n",
      "Iteration 2390 Training loss 0.05699986219406128 Validation loss 0.05226866155862808 Accuracy 0.8644999861717224\n",
      "Iteration 2400 Training loss 0.049500178545713425 Validation loss 0.04978538304567337 Accuracy 0.8725000023841858\n",
      "Iteration 2410 Training loss 0.052831001579761505 Validation loss 0.0495138093829155 Accuracy 0.875\n",
      "Iteration 2420 Training loss 0.051784999668598175 Validation loss 0.05023929476737976 Accuracy 0.8709999918937683\n",
      "Iteration 2430 Training loss 0.0655318945646286 Validation loss 0.07774293422698975 Accuracy 0.8075000047683716\n",
      "Iteration 2440 Training loss 0.054018646478652954 Validation loss 0.05810186639428139 Accuracy 0.8535000085830688\n",
      "Iteration 2450 Training loss 0.058934684842824936 Validation loss 0.0668085366487503 Accuracy 0.8355000019073486\n",
      "Iteration 2460 Training loss 0.05348384752869606 Validation loss 0.06747052073478699 Accuracy 0.8314999938011169\n",
      "Iteration 2470 Training loss 0.051642365753650665 Validation loss 0.0598231665790081 Accuracy 0.8489999771118164\n",
      "Iteration 2480 Training loss 0.0555887334048748 Validation loss 0.05856504291296005 Accuracy 0.8519999980926514\n",
      "Iteration 2490 Training loss 0.050286665558815 Validation loss 0.061857156455516815 Accuracy 0.8454999923706055\n",
      "Iteration 2500 Training loss 0.05473631992936134 Validation loss 0.06299586594104767 Accuracy 0.843500018119812\n",
      "Iteration 2510 Training loss 0.04982627183198929 Validation loss 0.049780137836933136 Accuracy 0.8715000152587891\n",
      "Iteration 2520 Training loss 0.05246511846780777 Validation loss 0.04934099689126015 Accuracy 0.8725000023841858\n",
      "Iteration 2530 Training loss 0.054883234202861786 Validation loss 0.05068313702940941 Accuracy 0.8679999709129333\n",
      "Iteration 2540 Training loss 0.05804653465747833 Validation loss 0.05194783955812454 Accuracy 0.8684999942779541\n",
      "Iteration 2550 Training loss 0.053277138620615005 Validation loss 0.05365984886884689 Accuracy 0.8650000095367432\n",
      "Iteration 2560 Training loss 0.06285920739173889 Validation loss 0.05469885841012001 Accuracy 0.8610000014305115\n",
      "Iteration 2570 Training loss 0.0589718334376812 Validation loss 0.0514090396463871 Accuracy 0.8679999709129333\n",
      "Iteration 2580 Training loss 0.050614699721336365 Validation loss 0.049539756029844284 Accuracy 0.8769999742507935\n",
      "Iteration 2590 Training loss 0.05934860557317734 Validation loss 0.053750112652778625 Accuracy 0.8644999861717224\n",
      "Iteration 2600 Training loss 0.053880784660577774 Validation loss 0.04989570751786232 Accuracy 0.8744999766349792\n",
      "Iteration 2610 Training loss 0.052291203290224075 Validation loss 0.05008513852953911 Accuracy 0.8734999895095825\n",
      "Iteration 2620 Training loss 0.05716812238097191 Validation loss 0.05163136124610901 Accuracy 0.8665000200271606\n",
      "Iteration 2630 Training loss 0.060069963335990906 Validation loss 0.056681107729673386 Accuracy 0.8569999933242798\n",
      "Iteration 2640 Training loss 0.05285462364554405 Validation loss 0.050312548875808716 Accuracy 0.8730000257492065\n",
      "Iteration 2650 Training loss 0.048041973263025284 Validation loss 0.04910808056592941 Accuracy 0.8740000128746033\n",
      "Iteration 2660 Training loss 0.05140749365091324 Validation loss 0.05036959424614906 Accuracy 0.8734999895095825\n",
      "Iteration 2670 Training loss 0.05607886612415314 Validation loss 0.05498538166284561 Accuracy 0.8615000247955322\n",
      "Iteration 2680 Training loss 0.05527115613222122 Validation loss 0.05325055867433548 Accuracy 0.8650000095367432\n",
      "Iteration 2690 Training loss 0.05266021192073822 Validation loss 0.051677361130714417 Accuracy 0.8700000047683716\n",
      "Iteration 2700 Training loss 0.0587674044072628 Validation loss 0.05629553273320198 Accuracy 0.8600000143051147\n",
      "Iteration 2710 Training loss 0.05273944139480591 Validation loss 0.0494142509996891 Accuracy 0.8740000128746033\n",
      "Iteration 2720 Training loss 0.058908507227897644 Validation loss 0.05474536120891571 Accuracy 0.8585000038146973\n",
      "Iteration 2730 Training loss 0.048781540244817734 Validation loss 0.04980229586362839 Accuracy 0.8734999895095825\n",
      "Iteration 2740 Training loss 0.050719209015369415 Validation loss 0.050628598779439926 Accuracy 0.8725000023841858\n",
      "Iteration 2750 Training loss 0.049856215715408325 Validation loss 0.04944067820906639 Accuracy 0.8740000128746033\n",
      "Iteration 2760 Training loss 0.056241415441036224 Validation loss 0.050617337226867676 Accuracy 0.8725000023841858\n",
      "Iteration 2770 Training loss 0.058915264904499054 Validation loss 0.05418683588504791 Accuracy 0.8644999861717224\n",
      "Iteration 2780 Training loss 0.054624754935503006 Validation loss 0.053006596863269806 Accuracy 0.8654999732971191\n",
      "Iteration 2790 Training loss 0.05393968150019646 Validation loss 0.051422495394945145 Accuracy 0.8665000200271606\n",
      "Iteration 2800 Training loss 0.050206370651721954 Validation loss 0.05194736644625664 Accuracy 0.8684999942779541\n",
      "Iteration 2810 Training loss 0.05689355731010437 Validation loss 0.05272138863801956 Accuracy 0.8659999966621399\n",
      "Iteration 2820 Training loss 0.05821957439184189 Validation loss 0.052016038447618484 Accuracy 0.8700000047683716\n",
      "Iteration 2830 Training loss 0.05873861163854599 Validation loss 0.054420460015535355 Accuracy 0.862500011920929\n",
      "Iteration 2840 Training loss 0.051614563912153244 Validation loss 0.052198249846696854 Accuracy 0.8675000071525574\n",
      "Iteration 2850 Training loss 0.061129130423069 Validation loss 0.053692322224378586 Accuracy 0.8654999732971191\n",
      "Iteration 2860 Training loss 0.05244525149464607 Validation loss 0.0507398322224617 Accuracy 0.8700000047683716\n",
      "Iteration 2870 Training loss 0.057336822152137756 Validation loss 0.05269842594861984 Accuracy 0.8650000095367432\n",
      "Iteration 2880 Training loss 0.058093369007110596 Validation loss 0.05368794873356819 Accuracy 0.8640000224113464\n",
      "Iteration 2890 Training loss 0.07175377756357193 Validation loss 0.0633574053645134 Accuracy 0.8445000052452087\n",
      "Iteration 2900 Training loss 0.052963193506002426 Validation loss 0.0502324141561985 Accuracy 0.8694999814033508\n",
      "Iteration 2910 Training loss 0.05049578845500946 Validation loss 0.04977130889892578 Accuracy 0.8734999895095825\n",
      "Iteration 2920 Training loss 0.06340298056602478 Validation loss 0.05874144658446312 Accuracy 0.859000027179718\n",
      "Iteration 2930 Training loss 0.05657919868826866 Validation loss 0.05475276708602905 Accuracy 0.8640000224113464\n",
      "Iteration 2940 Training loss 0.05219898000359535 Validation loss 0.050815001130104065 Accuracy 0.8709999918937683\n",
      "Iteration 2950 Training loss 0.052117008715867996 Validation loss 0.05053245276212692 Accuracy 0.8690000176429749\n",
      "Iteration 2960 Training loss 0.05714581534266472 Validation loss 0.05379136651754379 Accuracy 0.8640000224113464\n",
      "Iteration 2970 Training loss 0.061957720667123795 Validation loss 0.05352336913347244 Accuracy 0.8654999732971191\n",
      "Iteration 2980 Training loss 0.053083695471286774 Validation loss 0.050728585571050644 Accuracy 0.8705000281333923\n",
      "Iteration 2990 Training loss 0.057360995560884476 Validation loss 0.051876250654459 Accuracy 0.8700000047683716\n",
      "Iteration 3000 Training loss 0.061430927366018295 Validation loss 0.056548163294792175 Accuracy 0.8604999780654907\n",
      "Iteration 3010 Training loss 0.04784352704882622 Validation loss 0.04982782155275345 Accuracy 0.8755000233650208\n",
      "Iteration 3020 Training loss 0.05281316116452217 Validation loss 0.0521758571267128 Accuracy 0.8659999966621399\n",
      "Iteration 3030 Training loss 0.05711858347058296 Validation loss 0.05233202129602432 Accuracy 0.8669999837875366\n",
      "Iteration 3040 Training loss 0.05442298948764801 Validation loss 0.05143437534570694 Accuracy 0.8679999709129333\n",
      "Iteration 3050 Training loss 0.05327227711677551 Validation loss 0.05040934681892395 Accuracy 0.8709999918937683\n",
      "Iteration 3060 Training loss 0.06200168654322624 Validation loss 0.05880628153681755 Accuracy 0.8539999723434448\n",
      "Iteration 3070 Training loss 0.06425468623638153 Validation loss 0.055747177451848984 Accuracy 0.859000027179718\n",
      "Iteration 3080 Training loss 0.05791937932372093 Validation loss 0.05161113664507866 Accuracy 0.8675000071525574\n",
      "Iteration 3090 Training loss 0.048570021986961365 Validation loss 0.04984873905777931 Accuracy 0.8730000257492065\n",
      "Iteration 3100 Training loss 0.0656491070985794 Validation loss 0.06008148193359375 Accuracy 0.8504999876022339\n",
      "Iteration 3110 Training loss 0.055893369019031525 Validation loss 0.05228869616985321 Accuracy 0.8659999966621399\n",
      "Iteration 3120 Training loss 0.05888303369283676 Validation loss 0.05551351234316826 Accuracy 0.859000027179718\n",
      "Iteration 3130 Training loss 0.055054303258657455 Validation loss 0.05086076259613037 Accuracy 0.8669999837875366\n",
      "Iteration 3140 Training loss 0.05880283564329147 Validation loss 0.053941596299409866 Accuracy 0.8629999756813049\n",
      "Iteration 3150 Training loss 0.05637209489941597 Validation loss 0.05080793425440788 Accuracy 0.8694999814033508\n",
      "Iteration 3160 Training loss 0.05291549861431122 Validation loss 0.05014829337596893 Accuracy 0.871999979019165\n",
      "Iteration 3170 Training loss 0.059744950383901596 Validation loss 0.05303781479597092 Accuracy 0.8644999861717224\n",
      "Iteration 3180 Training loss 0.05452194809913635 Validation loss 0.051174867898225784 Accuracy 0.8679999709129333\n",
      "Iteration 3190 Training loss 0.06342198699712753 Validation loss 0.06063009053468704 Accuracy 0.8500000238418579\n",
      "Iteration 3200 Training loss 0.053272001445293427 Validation loss 0.05069757252931595 Accuracy 0.8705000281333923\n",
      "Iteration 3210 Training loss 0.05233398824930191 Validation loss 0.04930025339126587 Accuracy 0.875\n",
      "Iteration 3220 Training loss 0.06229884922504425 Validation loss 0.05469205230474472 Accuracy 0.8610000014305115\n",
      "Iteration 3230 Training loss 0.05721906200051308 Validation loss 0.051673147827386856 Accuracy 0.8675000071525574\n",
      "Iteration 3240 Training loss 0.054140303283929825 Validation loss 0.05022777244448662 Accuracy 0.8694999814033508\n",
      "Iteration 3250 Training loss 0.053566139191389084 Validation loss 0.050680674612522125 Accuracy 0.8715000152587891\n",
      "Iteration 3260 Training loss 0.05571718513965607 Validation loss 0.05453765019774437 Accuracy 0.8634999990463257\n",
      "Iteration 3270 Training loss 0.05822662264108658 Validation loss 0.052508026361465454 Accuracy 0.8669999837875366\n",
      "Iteration 3280 Training loss 0.0591251477599144 Validation loss 0.05306180194020271 Accuracy 0.8650000095367432\n",
      "Iteration 3290 Training loss 0.050095152109861374 Validation loss 0.05126010999083519 Accuracy 0.8665000200271606\n",
      "Iteration 3300 Training loss 0.0526183545589447 Validation loss 0.051432959735393524 Accuracy 0.8659999966621399\n",
      "Iteration 3310 Training loss 0.045908115804195404 Validation loss 0.05004578456282616 Accuracy 0.8734999895095825\n",
      "Iteration 3320 Training loss 0.05823546648025513 Validation loss 0.05490484461188316 Accuracy 0.8600000143051147\n",
      "Iteration 3330 Training loss 0.04999087005853653 Validation loss 0.049732133746147156 Accuracy 0.8734999895095825\n",
      "Iteration 3340 Training loss 0.05336056277155876 Validation loss 0.050350770354270935 Accuracy 0.8715000152587891\n",
      "Iteration 3350 Training loss 0.051376309245824814 Validation loss 0.052180904895067215 Accuracy 0.8669999837875366\n",
      "Iteration 3360 Training loss 0.05697326734662056 Validation loss 0.05006469041109085 Accuracy 0.8715000152587891\n",
      "Iteration 3370 Training loss 0.06212277337908745 Validation loss 0.05455654487013817 Accuracy 0.8640000224113464\n",
      "Iteration 3380 Training loss 0.056822676211595535 Validation loss 0.05356506258249283 Accuracy 0.8629999756813049\n",
      "Iteration 3390 Training loss 0.05813932791352272 Validation loss 0.05067437142133713 Accuracy 0.8684999942779541\n",
      "Iteration 3400 Training loss 0.05612757429480553 Validation loss 0.04926358535885811 Accuracy 0.8740000128746033\n",
      "Iteration 3410 Training loss 0.051628030836582184 Validation loss 0.051269348710775375 Accuracy 0.8675000071525574\n",
      "Iteration 3420 Training loss 0.06539463996887207 Validation loss 0.060304831713438034 Accuracy 0.8525000214576721\n",
      "Iteration 3430 Training loss 0.05183747783303261 Validation loss 0.049938566982746124 Accuracy 0.875\n",
      "Iteration 3440 Training loss 0.057369064539670944 Validation loss 0.05125071108341217 Accuracy 0.8705000281333923\n",
      "Iteration 3450 Training loss 0.05892330780625343 Validation loss 0.05456157773733139 Accuracy 0.8629999756813049\n",
      "Iteration 3460 Training loss 0.056691255420446396 Validation loss 0.05354630947113037 Accuracy 0.8640000224113464\n",
      "Iteration 3470 Training loss 0.054248515516519547 Validation loss 0.050179507583379745 Accuracy 0.8734999895095825\n",
      "Iteration 3480 Training loss 0.05980170518159866 Validation loss 0.05695712938904762 Accuracy 0.8629999756813049\n",
      "Iteration 3490 Training loss 0.057633500546216965 Validation loss 0.05052528157830238 Accuracy 0.8705000281333923\n",
      "Iteration 3500 Training loss 0.04861025512218475 Validation loss 0.04987576603889465 Accuracy 0.8734999895095825\n",
      "Iteration 3510 Training loss 0.055531322956085205 Validation loss 0.05058490112423897 Accuracy 0.8684999942779541\n",
      "Iteration 3520 Training loss 0.05790434405207634 Validation loss 0.053823523223400116 Accuracy 0.862500011920929\n",
      "Iteration 3530 Training loss 0.04772698134183884 Validation loss 0.04910986125469208 Accuracy 0.8744999766349792\n",
      "Iteration 3540 Training loss 0.05878575146198273 Validation loss 0.05291575565934181 Accuracy 0.8644999861717224\n",
      "Iteration 3550 Training loss 0.061483900994062424 Validation loss 0.05601714178919792 Accuracy 0.8585000038146973\n",
      "Iteration 3560 Training loss 0.05769289284944534 Validation loss 0.053583115339279175 Accuracy 0.8669999837875366\n",
      "Iteration 3570 Training loss 0.053852006793022156 Validation loss 0.052396196871995926 Accuracy 0.8659999966621399\n",
      "Iteration 3580 Training loss 0.04864908382296562 Validation loss 0.04976135119795799 Accuracy 0.8744999766349792\n",
      "Iteration 3590 Training loss 0.05733548849821091 Validation loss 0.054508790373802185 Accuracy 0.8619999885559082\n",
      "Iteration 3600 Training loss 0.05796908214688301 Validation loss 0.0520520843565464 Accuracy 0.8659999966621399\n",
      "Iteration 3610 Training loss 0.05851053446531296 Validation loss 0.05499475076794624 Accuracy 0.8585000038146973\n",
      "Iteration 3620 Training loss 0.05253005400300026 Validation loss 0.051452767103910446 Accuracy 0.8665000200271606\n",
      "Iteration 3630 Training loss 0.05016624182462692 Validation loss 0.05003546178340912 Accuracy 0.8725000023841858\n",
      "Iteration 3640 Training loss 0.05756847932934761 Validation loss 0.054855216294527054 Accuracy 0.862500011920929\n",
      "Iteration 3650 Training loss 0.05883811041712761 Validation loss 0.053737789392471313 Accuracy 0.8644999861717224\n",
      "Iteration 3660 Training loss 0.05635876581072807 Validation loss 0.05096384510397911 Accuracy 0.8690000176429749\n",
      "Iteration 3670 Training loss 0.053955141454935074 Validation loss 0.05243121087551117 Accuracy 0.8650000095367432\n",
      "Iteration 3680 Training loss 0.057487137615680695 Validation loss 0.05180136859416962 Accuracy 0.8665000200271606\n",
      "Iteration 3690 Training loss 0.05584176257252693 Validation loss 0.05171023681759834 Accuracy 0.8679999709129333\n",
      "Iteration 3700 Training loss 0.06019061431288719 Validation loss 0.056930117309093475 Accuracy 0.859499990940094\n",
      "Iteration 3710 Training loss 0.05210091173648834 Validation loss 0.05005800724029541 Accuracy 0.871999979019165\n",
      "Iteration 3720 Training loss 0.05276288464665413 Validation loss 0.04980039969086647 Accuracy 0.8740000128746033\n",
      "Iteration 3730 Training loss 0.05272652953863144 Validation loss 0.050607502460479736 Accuracy 0.8715000152587891\n",
      "Iteration 3740 Training loss 0.05108049884438515 Validation loss 0.05064697936177254 Accuracy 0.8700000047683716\n",
      "Iteration 3750 Training loss 0.059408172965049744 Validation loss 0.05514124408364296 Accuracy 0.8629999756813049\n",
      "Iteration 3760 Training loss 0.05525854974985123 Validation loss 0.053336743265390396 Accuracy 0.8644999861717224\n",
      "Iteration 3770 Training loss 0.05568445846438408 Validation loss 0.05276859924197197 Accuracy 0.8650000095367432\n",
      "Iteration 3780 Training loss 0.05871440842747688 Validation loss 0.05126770958304405 Accuracy 0.8675000071525574\n",
      "Iteration 3790 Training loss 0.05260885879397392 Validation loss 0.051774993538856506 Accuracy 0.8654999732971191\n",
      "Iteration 3800 Training loss 0.056070536375045776 Validation loss 0.05315211042761803 Accuracy 0.8629999756813049\n",
      "Iteration 3810 Training loss 0.05129567161202431 Validation loss 0.04985521733760834 Accuracy 0.8740000128746033\n",
      "Iteration 3820 Training loss 0.055181656032800674 Validation loss 0.05213231220841408 Accuracy 0.8675000071525574\n",
      "Iteration 3830 Training loss 0.05814013630151749 Validation loss 0.05265320464968681 Accuracy 0.8669999837875366\n",
      "Iteration 3840 Training loss 0.053518906235694885 Validation loss 0.049835868179798126 Accuracy 0.871999979019165\n",
      "Iteration 3850 Training loss 0.05968702957034111 Validation loss 0.052452538162469864 Accuracy 0.8679999709129333\n",
      "Iteration 3860 Training loss 0.04850184544920921 Validation loss 0.05018625780940056 Accuracy 0.8734999895095825\n",
      "Iteration 3870 Training loss 0.06412391364574432 Validation loss 0.05483878403902054 Accuracy 0.859499990940094\n",
      "Iteration 3880 Training loss 0.05701373144984245 Validation loss 0.05413883551955223 Accuracy 0.8629999756813049\n",
      "Iteration 3890 Training loss 0.05636633187532425 Validation loss 0.05067019537091255 Accuracy 0.8684999942779541\n",
      "Iteration 3900 Training loss 0.05923382192850113 Validation loss 0.05407777056097984 Accuracy 0.8619999885559082\n",
      "Iteration 3910 Training loss 0.05459422990679741 Validation loss 0.05064499005675316 Accuracy 0.8715000152587891\n",
      "Iteration 3920 Training loss 0.06197807192802429 Validation loss 0.05450110882520676 Accuracy 0.8640000224113464\n",
      "Iteration 3930 Training loss 0.05441640317440033 Validation loss 0.04981376230716705 Accuracy 0.8740000128746033\n",
      "Iteration 3940 Training loss 0.05931701511144638 Validation loss 0.053095657378435135 Accuracy 0.8644999861717224\n",
      "Iteration 3950 Training loss 0.05405445024371147 Validation loss 0.05090532451868057 Accuracy 0.8679999709129333\n",
      "Iteration 3960 Training loss 0.055573880672454834 Validation loss 0.05133213847875595 Accuracy 0.8705000281333923\n",
      "Iteration 3970 Training loss 0.05293501541018486 Validation loss 0.04982965812087059 Accuracy 0.8725000023841858\n",
      "Iteration 3980 Training loss 0.056727197021245956 Validation loss 0.050614625215530396 Accuracy 0.8705000281333923\n",
      "Iteration 3990 Training loss 0.05555599182844162 Validation loss 0.052854202687740326 Accuracy 0.8654999732971191\n",
      "Iteration 4000 Training loss 0.0606648325920105 Validation loss 0.051953352987766266 Accuracy 0.8640000224113464\n",
      "Iteration 4010 Training loss 0.059250663965940475 Validation loss 0.052922192960977554 Accuracy 0.8650000095367432\n",
      "Iteration 4020 Training loss 0.05296783521771431 Validation loss 0.04994911327958107 Accuracy 0.8725000023841858\n",
      "Iteration 4030 Training loss 0.052600689232349396 Validation loss 0.05013464763760567 Accuracy 0.8700000047683716\n",
      "Iteration 4040 Training loss 0.05476520583033562 Validation loss 0.05028344318270683 Accuracy 0.8705000281333923\n",
      "Iteration 4050 Training loss 0.05415154621005058 Validation loss 0.05366671830415726 Accuracy 0.8659999966621399\n",
      "Iteration 4060 Training loss 0.0545043870806694 Validation loss 0.05172869563102722 Accuracy 0.8659999966621399\n",
      "Iteration 4070 Training loss 0.04949581250548363 Validation loss 0.049961838871240616 Accuracy 0.8740000128746033\n",
      "Iteration 4080 Training loss 0.053076379001140594 Validation loss 0.050212256610393524 Accuracy 0.8715000152587891\n",
      "Iteration 4090 Training loss 0.06046442314982414 Validation loss 0.057795535773038864 Accuracy 0.8585000038146973\n",
      "Iteration 4100 Training loss 0.06134778633713722 Validation loss 0.05503014475107193 Accuracy 0.8634999990463257\n",
      "Iteration 4110 Training loss 0.057445451617240906 Validation loss 0.05388950929045677 Accuracy 0.8650000095367432\n",
      "Iteration 4120 Training loss 0.053960464894771576 Validation loss 0.05040154606103897 Accuracy 0.871999979019165\n",
      "Iteration 4130 Training loss 0.05464988946914673 Validation loss 0.0503915473818779 Accuracy 0.8744999766349792\n",
      "Iteration 4140 Training loss 0.04813487455248833 Validation loss 0.049663200974464417 Accuracy 0.8744999766349792\n",
      "Iteration 4150 Training loss 0.04542502015829086 Validation loss 0.04970491677522659 Accuracy 0.8744999766349792\n",
      "Iteration 4160 Training loss 0.04975108802318573 Validation loss 0.049381449818611145 Accuracy 0.8734999895095825\n",
      "Iteration 4170 Training loss 0.060353610664606094 Validation loss 0.05435842275619507 Accuracy 0.8654999732971191\n",
      "Iteration 4180 Training loss 0.059805359691381454 Validation loss 0.054448094218969345 Accuracy 0.8604999780654907\n",
      "Iteration 4190 Training loss 0.05745527520775795 Validation loss 0.052654463797807693 Accuracy 0.8669999837875366\n",
      "Iteration 4200 Training loss 0.06203467771410942 Validation loss 0.05613870918750763 Accuracy 0.8610000014305115\n",
      "Iteration 4210 Training loss 0.06075764447450638 Validation loss 0.05290329456329346 Accuracy 0.8665000200271606\n",
      "Iteration 4220 Training loss 0.05736394599080086 Validation loss 0.053949031978845596 Accuracy 0.8644999861717224\n",
      "Iteration 4230 Training loss 0.05349156633019447 Validation loss 0.05253809317946434 Accuracy 0.8669999837875366\n",
      "Iteration 4240 Training loss 0.049120064824819565 Validation loss 0.049542173743247986 Accuracy 0.8734999895095825\n",
      "Iteration 4250 Training loss 0.05688602104783058 Validation loss 0.05168347805738449 Accuracy 0.8659999966621399\n",
      "Iteration 4260 Training loss 0.06202548369765282 Validation loss 0.05439897999167442 Accuracy 0.8640000224113464\n",
      "Iteration 4270 Training loss 0.05402658134698868 Validation loss 0.05116621404886246 Accuracy 0.8669999837875366\n",
      "Iteration 4280 Training loss 0.05317071080207825 Validation loss 0.050500307232141495 Accuracy 0.871999979019165\n",
      "Iteration 4290 Training loss 0.053797647356987 Validation loss 0.05108814314007759 Accuracy 0.8690000176429749\n",
      "Iteration 4300 Training loss 0.05010790750384331 Validation loss 0.049016524106264114 Accuracy 0.8744999766349792\n",
      "Iteration 4310 Training loss 0.05541713535785675 Validation loss 0.05088689923286438 Accuracy 0.8665000200271606\n",
      "Iteration 4320 Training loss 0.05936108157038689 Validation loss 0.05653386563062668 Accuracy 0.8600000143051147\n",
      "Iteration 4330 Training loss 0.05323507636785507 Validation loss 0.050252433866262436 Accuracy 0.8700000047683716\n",
      "Iteration 4340 Training loss 0.06194164603948593 Validation loss 0.05560198053717613 Accuracy 0.8585000038146973\n",
      "Iteration 4350 Training loss 0.04854981601238251 Validation loss 0.04959278553724289 Accuracy 0.8734999895095825\n",
      "Iteration 4360 Training loss 0.06044552102684975 Validation loss 0.053891513496637344 Accuracy 0.8634999990463257\n",
      "Iteration 4370 Training loss 0.057349059730768204 Validation loss 0.05391953885555267 Accuracy 0.8629999756813049\n",
      "Iteration 4380 Training loss 0.05587191507220268 Validation loss 0.05180830880999565 Accuracy 0.8640000224113464\n",
      "Iteration 4390 Training loss 0.05353612080216408 Validation loss 0.04988599196076393 Accuracy 0.8730000257492065\n",
      "Iteration 4400 Training loss 0.056722234934568405 Validation loss 0.051789041608572006 Accuracy 0.8644999861717224\n",
      "Iteration 4410 Training loss 0.0573144368827343 Validation loss 0.050618719309568405 Accuracy 0.8700000047683716\n",
      "Iteration 4420 Training loss 0.05610814690589905 Validation loss 0.05436498299241066 Accuracy 0.862500011920929\n",
      "Iteration 4430 Training loss 0.05459396541118622 Validation loss 0.050472307950258255 Accuracy 0.8690000176429749\n",
      "Iteration 4440 Training loss 0.053770918399095535 Validation loss 0.049644261598587036 Accuracy 0.8730000257492065\n",
      "Iteration 4450 Training loss 0.057094305753707886 Validation loss 0.051467347890138626 Accuracy 0.8679999709129333\n",
      "Iteration 4460 Training loss 0.06053929030895233 Validation loss 0.05391370505094528 Accuracy 0.8644999861717224\n",
      "Iteration 4470 Training loss 0.050906289368867874 Validation loss 0.05070551484823227 Accuracy 0.8700000047683716\n",
      "Iteration 4480 Training loss 0.052218444645404816 Validation loss 0.050099726766347885 Accuracy 0.8690000176429749\n",
      "Iteration 4490 Training loss 0.054770905524492264 Validation loss 0.05240224301815033 Accuracy 0.8650000095367432\n",
      "Iteration 4500 Training loss 0.049781229346990585 Validation loss 0.05033333599567413 Accuracy 0.8725000023841858\n",
      "Iteration 4510 Training loss 0.05043674632906914 Validation loss 0.04918914660811424 Accuracy 0.875\n",
      "Iteration 4520 Training loss 0.057711079716682434 Validation loss 0.05036555230617523 Accuracy 0.8730000257492065\n",
      "Iteration 4530 Training loss 0.06240637227892876 Validation loss 0.05472790077328682 Accuracy 0.8629999756813049\n",
      "Iteration 4540 Training loss 0.06006838008761406 Validation loss 0.055620867758989334 Accuracy 0.8600000143051147\n",
      "Iteration 4550 Training loss 0.060294803231954575 Validation loss 0.05307278037071228 Accuracy 0.8640000224113464\n",
      "Iteration 4560 Training loss 0.05901442840695381 Validation loss 0.05599413439631462 Accuracy 0.8560000061988831\n",
      "Iteration 4570 Training loss 0.05047118291258812 Validation loss 0.0494062714278698 Accuracy 0.875\n",
      "Iteration 4580 Training loss 0.06081206724047661 Validation loss 0.05351562425494194 Accuracy 0.8634999990463257\n",
      "Iteration 4590 Training loss 0.05390479415655136 Validation loss 0.050789352506399155 Accuracy 0.8700000047683716\n",
      "Iteration 4600 Training loss 0.05961943045258522 Validation loss 0.054601263254880905 Accuracy 0.8634999990463257\n",
      "Iteration 4610 Training loss 0.05564242601394653 Validation loss 0.05307057872414589 Accuracy 0.862500011920929\n",
      "Iteration 4620 Training loss 0.053144603967666626 Validation loss 0.05022822692990303 Accuracy 0.8765000104904175\n",
      "Iteration 4630 Training loss 0.055591702461242676 Validation loss 0.05190823972225189 Accuracy 0.8644999861717224\n",
      "Iteration 4640 Training loss 0.05391126871109009 Validation loss 0.05102661997079849 Accuracy 0.8669999837875366\n",
      "Iteration 4650 Training loss 0.05379065126180649 Validation loss 0.051389992237091064 Accuracy 0.8654999732971191\n",
      "Iteration 4660 Training loss 0.0523696132004261 Validation loss 0.05185359716415405 Accuracy 0.8634999990463257\n",
      "Iteration 4670 Training loss 0.06070932000875473 Validation loss 0.05687019228935242 Accuracy 0.8615000247955322\n",
      "Iteration 4680 Training loss 0.0612928606569767 Validation loss 0.051341477781534195 Accuracy 0.8650000095367432\n",
      "Iteration 4690 Training loss 0.05326487496495247 Validation loss 0.0497044213116169 Accuracy 0.8730000257492065\n",
      "Iteration 4700 Training loss 0.051054708659648895 Validation loss 0.05000651627779007 Accuracy 0.871999979019165\n",
      "Iteration 4710 Training loss 0.05335056409239769 Validation loss 0.04940912500023842 Accuracy 0.8744999766349792\n",
      "Iteration 4720 Training loss 0.05164814740419388 Validation loss 0.05024121701717377 Accuracy 0.8730000257492065\n",
      "Iteration 4730 Training loss 0.054196350276470184 Validation loss 0.05141473561525345 Accuracy 0.8675000071525574\n",
      "Iteration 4740 Training loss 0.05575556308031082 Validation loss 0.05147066339850426 Accuracy 0.8650000095367432\n",
      "Iteration 4750 Training loss 0.05948872119188309 Validation loss 0.056557465344667435 Accuracy 0.859000027179718\n",
      "Iteration 4760 Training loss 0.05205043405294418 Validation loss 0.050527479499578476 Accuracy 0.8684999942779541\n",
      "Iteration 4770 Training loss 0.05480411648750305 Validation loss 0.05033889785408974 Accuracy 0.871999979019165\n",
      "Iteration 4780 Training loss 0.06012233719229698 Validation loss 0.05260827764868736 Accuracy 0.8650000095367432\n",
      "Iteration 4790 Training loss 0.054373063147068024 Validation loss 0.05055174231529236 Accuracy 0.8709999918937683\n",
      "Iteration 4800 Training loss 0.05205560475587845 Validation loss 0.04964512214064598 Accuracy 0.8740000128746033\n",
      "Iteration 4810 Training loss 0.05934430658817291 Validation loss 0.051344823092222214 Accuracy 0.8665000200271606\n",
      "Iteration 4820 Training loss 0.05522901937365532 Validation loss 0.051475465297698975 Accuracy 0.8659999966621399\n",
      "Iteration 4830 Training loss 0.05295178294181824 Validation loss 0.05223119258880615 Accuracy 0.8675000071525574\n",
      "Iteration 4840 Training loss 0.05936489999294281 Validation loss 0.051332518458366394 Accuracy 0.8659999966621399\n",
      "Iteration 4850 Training loss 0.06289787590503693 Validation loss 0.05740891769528389 Accuracy 0.8554999828338623\n",
      "Iteration 4860 Training loss 0.0618600994348526 Validation loss 0.05376864969730377 Accuracy 0.8634999990463257\n",
      "Iteration 4870 Training loss 0.05432666465640068 Validation loss 0.051547516137361526 Accuracy 0.8675000071525574\n",
      "Iteration 4880 Training loss 0.061006154865026474 Validation loss 0.05309901386499405 Accuracy 0.862500011920929\n",
      "Iteration 4890 Training loss 0.05933104082942009 Validation loss 0.05335530266165733 Accuracy 0.8644999861717224\n",
      "Iteration 4900 Training loss 0.061615411192178726 Validation loss 0.05337567254900932 Accuracy 0.862500011920929\n",
      "Iteration 4910 Training loss 0.0542437918484211 Validation loss 0.05038811266422272 Accuracy 0.8730000257492065\n",
      "Iteration 4920 Training loss 0.05691508203744888 Validation loss 0.05442594364285469 Accuracy 0.8604999780654907\n",
      "Iteration 4930 Training loss 0.05299236997961998 Validation loss 0.0511777438223362 Accuracy 0.8690000176429749\n",
      "Iteration 4940 Training loss 0.05397629365324974 Validation loss 0.055328618735075 Accuracy 0.8634999990463257\n",
      "Iteration 4950 Training loss 0.05386070907115936 Validation loss 0.055501654744148254 Accuracy 0.8610000014305115\n",
      "Iteration 4960 Training loss 0.053644005209207535 Validation loss 0.05775156244635582 Accuracy 0.8535000085830688\n",
      "Iteration 4970 Training loss 0.06276500225067139 Validation loss 0.06853194534778595 Accuracy 0.8345000147819519\n",
      "Iteration 4980 Training loss 0.05287475883960724 Validation loss 0.06123064085841179 Accuracy 0.8479999899864197\n",
      "Iteration 4990 Training loss 0.05354003980755806 Validation loss 0.06133337318897247 Accuracy 0.8475000262260437\n",
      "Iteration 5000 Training loss 0.06300392746925354 Validation loss 0.06843502819538116 Accuracy 0.8335000276565552\n",
      "Iteration 5010 Training loss 0.05949104204773903 Validation loss 0.06332296878099442 Accuracy 0.8450000286102295\n",
      "Iteration 5020 Training loss 0.05857997387647629 Validation loss 0.06272261589765549 Accuracy 0.843999981880188\n",
      "Iteration 5030 Training loss 0.06404334306716919 Validation loss 0.06840898096561432 Accuracy 0.8335000276565552\n",
      "Iteration 5040 Training loss 0.055395159870386124 Validation loss 0.05995815247297287 Accuracy 0.8489999771118164\n",
      "Iteration 5050 Training loss 0.05144501104950905 Validation loss 0.054434631019830704 Accuracy 0.8634999990463257\n",
      "Iteration 5060 Training loss 0.050403207540512085 Validation loss 0.05731214955449104 Accuracy 0.8544999957084656\n",
      "Iteration 5070 Training loss 0.057805903255939484 Validation loss 0.06350166350603104 Accuracy 0.8420000076293945\n",
      "Iteration 5080 Training loss 0.057032011449337006 Validation loss 0.060427844524383545 Accuracy 0.8489999771118164\n",
      "Iteration 5090 Training loss 0.06109750270843506 Validation loss 0.06662831455469131 Accuracy 0.8364999890327454\n",
      "Iteration 5100 Training loss 0.0509815476834774 Validation loss 0.054231464862823486 Accuracy 0.8610000014305115\n",
      "Iteration 5110 Training loss 0.059033941477537155 Validation loss 0.06364869326353073 Accuracy 0.843999981880188\n",
      "Iteration 5120 Training loss 0.06125977262854576 Validation loss 0.06662089377641678 Accuracy 0.8374999761581421\n",
      "Iteration 5130 Training loss 0.05564955994486809 Validation loss 0.05685849115252495 Accuracy 0.8575000166893005\n",
      "Iteration 5140 Training loss 0.059636764228343964 Validation loss 0.060021284967660904 Accuracy 0.8514999747276306\n",
      "Iteration 5150 Training loss 0.05373267084360123 Validation loss 0.05779761075973511 Accuracy 0.8535000085830688\n",
      "Iteration 5160 Training loss 0.06004521995782852 Validation loss 0.06235499680042267 Accuracy 0.843999981880188\n",
      "Iteration 5170 Training loss 0.056478649377822876 Validation loss 0.05936937779188156 Accuracy 0.8485000133514404\n",
      "Iteration 5180 Training loss 0.05074724555015564 Validation loss 0.06029704958200455 Accuracy 0.8500000238418579\n",
      "Iteration 5190 Training loss 0.052773766219615936 Validation loss 0.055487167090177536 Accuracy 0.859499990940094\n",
      "Iteration 5200 Training loss 0.05675652250647545 Validation loss 0.06623101979494095 Accuracy 0.8370000123977661\n",
      "Iteration 5210 Training loss 0.05717099457979202 Validation loss 0.0620889775454998 Accuracy 0.8464999794960022\n",
      "Iteration 5220 Training loss 0.056712470948696136 Validation loss 0.06561268866062164 Accuracy 0.8395000100135803\n",
      "Iteration 5230 Training loss 0.05542055889964104 Validation loss 0.061613865196704865 Accuracy 0.8445000052452087\n",
      "Iteration 5240 Training loss 0.05924738943576813 Validation loss 0.061814554035663605 Accuracy 0.8445000052452087\n",
      "Iteration 5250 Training loss 0.06225152686238289 Validation loss 0.06615524739027023 Accuracy 0.8360000252723694\n",
      "Iteration 5260 Training loss 0.060726068913936615 Validation loss 0.06266513466835022 Accuracy 0.843500018119812\n",
      "Iteration 5270 Training loss 0.061166051775217056 Validation loss 0.06472364068031311 Accuracy 0.8410000205039978\n",
      "Iteration 5280 Training loss 0.06273731589317322 Validation loss 0.06642315536737442 Accuracy 0.8370000123977661\n",
      "Iteration 5290 Training loss 0.048353161662817 Validation loss 0.054684076458215714 Accuracy 0.8610000014305115\n",
      "Iteration 5300 Training loss 0.055655986070632935 Validation loss 0.06143364682793617 Accuracy 0.8460000157356262\n",
      "Iteration 5310 Training loss 0.06035947799682617 Validation loss 0.06830615550279617 Accuracy 0.8330000042915344\n",
      "Iteration 5320 Training loss 0.05479555204510689 Validation loss 0.060353294014930725 Accuracy 0.8479999899864197\n",
      "Iteration 5330 Training loss 0.06427190452814102 Validation loss 0.06829406321048737 Accuracy 0.8330000042915344\n",
      "Iteration 5340 Training loss 0.04868968576192856 Validation loss 0.05127507820725441 Accuracy 0.8700000047683716\n",
      "Iteration 5350 Training loss 0.0738401785492897 Validation loss 0.07564486563205719 Accuracy 0.8105000257492065\n",
      "Iteration 5360 Training loss 0.05780332162976265 Validation loss 0.06212393194437027 Accuracy 0.8460000157356262\n",
      "Iteration 5370 Training loss 0.05497434362769127 Validation loss 0.058575376868247986 Accuracy 0.8510000109672546\n",
      "Iteration 5380 Training loss 0.056755706667900085 Validation loss 0.06365513056516647 Accuracy 0.840499997138977\n",
      "Iteration 5390 Training loss 0.053241364657878876 Validation loss 0.06035012751817703 Accuracy 0.8479999899864197\n",
      "Iteration 5400 Training loss 0.060423459857702255 Validation loss 0.06170567497611046 Accuracy 0.8460000157356262\n",
      "Iteration 5410 Training loss 0.05716640502214432 Validation loss 0.06354493647813797 Accuracy 0.8410000205039978\n",
      "Iteration 5420 Training loss 0.04971440136432648 Validation loss 0.057589855045080185 Accuracy 0.8550000190734863\n",
      "Iteration 5430 Training loss 0.051403023302555084 Validation loss 0.052873048931360245 Accuracy 0.8654999732971191\n",
      "Iteration 5440 Training loss 0.06600400805473328 Validation loss 0.06997445970773697 Accuracy 0.8305000066757202\n",
      "Iteration 5450 Training loss 0.05516429618000984 Validation loss 0.05514463037252426 Accuracy 0.8629999756813049\n",
      "Iteration 5460 Training loss 0.06044715270400047 Validation loss 0.06609402596950531 Accuracy 0.8374999761581421\n",
      "Iteration 5470 Training loss 0.053810060024261475 Validation loss 0.05825302004814148 Accuracy 0.8519999980926514\n",
      "Iteration 5480 Training loss 0.06366005539894104 Validation loss 0.06908442825078964 Accuracy 0.8299999833106995\n",
      "Iteration 5490 Training loss 0.06021304428577423 Validation loss 0.06465215981006622 Accuracy 0.8395000100135803\n",
      "Iteration 5500 Training loss 0.05510219559073448 Validation loss 0.06106864660978317 Accuracy 0.8485000133514404\n",
      "Iteration 5510 Training loss 0.058359093964099884 Validation loss 0.0638238862156868 Accuracy 0.8414999842643738\n",
      "Iteration 5520 Training loss 0.058365315198898315 Validation loss 0.06436773389577866 Accuracy 0.8429999947547913\n",
      "Iteration 5530 Training loss 0.059336449950933456 Validation loss 0.06558670103549957 Accuracy 0.8395000100135803\n",
      "Iteration 5540 Training loss 0.06324807554483414 Validation loss 0.06751316785812378 Accuracy 0.8345000147819519\n",
      "Iteration 5550 Training loss 0.05697854608297348 Validation loss 0.05754576995968819 Accuracy 0.8544999957084656\n",
      "Iteration 5560 Training loss 0.06047458201646805 Validation loss 0.06474915146827698 Accuracy 0.8410000205039978\n",
      "Iteration 5570 Training loss 0.06056581065058708 Validation loss 0.06711827218532562 Accuracy 0.8349999785423279\n",
      "Iteration 5580 Training loss 0.059710536152124405 Validation loss 0.06887991726398468 Accuracy 0.8305000066757202\n",
      "Iteration 5590 Training loss 0.05858277156949043 Validation loss 0.0634005069732666 Accuracy 0.8420000076293945\n",
      "Iteration 5600 Training loss 0.0638154000043869 Validation loss 0.06880731135606766 Accuracy 0.8339999914169312\n",
      "Iteration 5610 Training loss 0.05167875811457634 Validation loss 0.05279814079403877 Accuracy 0.8634999990463257\n",
      "Iteration 5620 Training loss 0.056884557008743286 Validation loss 0.06540808081626892 Accuracy 0.8399999737739563\n",
      "Iteration 5630 Training loss 0.059405021369457245 Validation loss 0.06654595583677292 Accuracy 0.8360000252723694\n",
      "Iteration 5640 Training loss 0.06352343410253525 Validation loss 0.06691887974739075 Accuracy 0.8364999890327454\n",
      "Iteration 5650 Training loss 0.06061747297644615 Validation loss 0.06678246706724167 Accuracy 0.8370000123977661\n",
      "Iteration 5660 Training loss 0.0573626309633255 Validation loss 0.06022683531045914 Accuracy 0.8519999980926514\n",
      "Iteration 5670 Training loss 0.05785742774605751 Validation loss 0.062417831271886826 Accuracy 0.8454999923706055\n",
      "Iteration 5680 Training loss 0.06311853229999542 Validation loss 0.06265907734632492 Accuracy 0.8464999794960022\n",
      "Iteration 5690 Training loss 0.05738268420100212 Validation loss 0.06336332112550735 Accuracy 0.843500018119812\n",
      "Iteration 5700 Training loss 0.06171335279941559 Validation loss 0.06624911725521088 Accuracy 0.8374999761581421\n",
      "Iteration 5710 Training loss 0.05517362803220749 Validation loss 0.06321288645267487 Accuracy 0.8429999947547913\n",
      "Iteration 5720 Training loss 0.05919557809829712 Validation loss 0.06308503448963165 Accuracy 0.843500018119812\n",
      "Iteration 5730 Training loss 0.049221474677324295 Validation loss 0.058040376752614975 Accuracy 0.8539999723434448\n",
      "Iteration 5740 Training loss 0.0669143944978714 Validation loss 0.07212217897176743 Accuracy 0.8230000138282776\n",
      "Iteration 5750 Training loss 0.06411529332399368 Validation loss 0.0714561864733696 Accuracy 0.828499972820282\n",
      "Iteration 5760 Training loss 0.06098633632063866 Validation loss 0.06533151119947433 Accuracy 0.8410000205039978\n",
      "Iteration 5770 Training loss 0.05760226771235466 Validation loss 0.06327096372842789 Accuracy 0.8429999947547913\n",
      "Iteration 5780 Training loss 0.06698477268218994 Validation loss 0.0690327137708664 Accuracy 0.8295000195503235\n",
      "Iteration 5790 Training loss 0.0549381822347641 Validation loss 0.05728675797581673 Accuracy 0.8550000190734863\n",
      "Iteration 5800 Training loss 0.05980304256081581 Validation loss 0.06838376820087433 Accuracy 0.8335000276565552\n",
      "Iteration 5810 Training loss 0.05515037849545479 Validation loss 0.057300321757793427 Accuracy 0.8544999957084656\n",
      "Iteration 5820 Training loss 0.057969480752944946 Validation loss 0.06038496270775795 Accuracy 0.8495000004768372\n",
      "Iteration 5830 Training loss 0.059827759861946106 Validation loss 0.06180616840720177 Accuracy 0.8460000157356262\n",
      "Iteration 5840 Training loss 0.060892052948474884 Validation loss 0.06846103072166443 Accuracy 0.8320000171661377\n",
      "Iteration 5850 Training loss 0.05222215875983238 Validation loss 0.0603463277220726 Accuracy 0.8489999771118164\n",
      "Iteration 5860 Training loss 0.05972810462117195 Validation loss 0.06895536184310913 Accuracy 0.8324999809265137\n",
      "Iteration 5870 Training loss 0.06288831681013107 Validation loss 0.06627452373504639 Accuracy 0.8389999866485596\n",
      "Iteration 5880 Training loss 0.05304822698235512 Validation loss 0.056603819131851196 Accuracy 0.8585000038146973\n",
      "Iteration 5890 Training loss 0.06963301450014114 Validation loss 0.07640533894300461 Accuracy 0.8125\n",
      "Iteration 5900 Training loss 0.05237317457795143 Validation loss 0.05842134729027748 Accuracy 0.8535000085830688\n",
      "Iteration 5910 Training loss 0.06207939237356186 Validation loss 0.06870172172784805 Accuracy 0.8335000276565552\n",
      "Iteration 5920 Training loss 0.05589011684060097 Validation loss 0.060007184743881226 Accuracy 0.8489999771118164\n",
      "Iteration 5930 Training loss 0.05159463360905647 Validation loss 0.06035290285944939 Accuracy 0.8485000133514404\n",
      "Iteration 5940 Training loss 0.0592525340616703 Validation loss 0.06330200284719467 Accuracy 0.843500018119812\n",
      "Iteration 5950 Training loss 0.06446760892868042 Validation loss 0.06939327716827393 Accuracy 0.8320000171661377\n",
      "Iteration 5960 Training loss 0.05652112513780594 Validation loss 0.05717315152287483 Accuracy 0.8560000061988831\n",
      "Iteration 5970 Training loss 0.05968061462044716 Validation loss 0.06423401087522507 Accuracy 0.8420000076293945\n",
      "Iteration 5980 Training loss 0.052412185817956924 Validation loss 0.05940753221511841 Accuracy 0.8510000109672546\n",
      "Iteration 5990 Training loss 0.057262688875198364 Validation loss 0.06343881040811539 Accuracy 0.8429999947547913\n",
      "Iteration 6000 Training loss 0.06137005239725113 Validation loss 0.06180468201637268 Accuracy 0.843500018119812\n",
      "Iteration 6010 Training loss 0.06137022376060486 Validation loss 0.06813137978315353 Accuracy 0.8335000276565552\n",
      "Iteration 6020 Training loss 0.052690066397190094 Validation loss 0.05631830170750618 Accuracy 0.8575000166893005\n",
      "Iteration 6030 Training loss 0.06915254145860672 Validation loss 0.07354380935430527 Accuracy 0.8220000267028809\n",
      "Iteration 6040 Training loss 0.050313882529735565 Validation loss 0.05612457916140556 Accuracy 0.8600000143051147\n",
      "Iteration 6050 Training loss 0.05152946338057518 Validation loss 0.056143227964639664 Accuracy 0.8560000061988831\n",
      "Iteration 6060 Training loss 0.061313461512327194 Validation loss 0.0683128759264946 Accuracy 0.8324999809265137\n",
      "Iteration 6070 Training loss 0.05923576280474663 Validation loss 0.06527302414178848 Accuracy 0.8379999995231628\n",
      "Iteration 6080 Training loss 0.056650903075933456 Validation loss 0.06220308318734169 Accuracy 0.843500018119812\n",
      "Iteration 6090 Training loss 0.05183793604373932 Validation loss 0.059399787336587906 Accuracy 0.8514999747276306\n",
      "Iteration 6100 Training loss 0.05561348795890808 Validation loss 0.06548512727022171 Accuracy 0.8385000228881836\n",
      "Iteration 6110 Training loss 0.06334532797336578 Validation loss 0.06464123725891113 Accuracy 0.8374999761581421\n",
      "Iteration 6120 Training loss 0.06334386765956879 Validation loss 0.06401416659355164 Accuracy 0.843999981880188\n",
      "Iteration 6130 Training loss 0.06333202868700027 Validation loss 0.06799586117267609 Accuracy 0.8339999914169312\n",
      "Iteration 6140 Training loss 0.06342289596796036 Validation loss 0.06748757511377335 Accuracy 0.8349999785423279\n",
      "Iteration 6150 Training loss 0.052832480520009995 Validation loss 0.054834961891174316 Accuracy 0.8629999756813049\n",
      "Iteration 6160 Training loss 0.0670989602804184 Validation loss 0.058982521295547485 Accuracy 0.8535000085830688\n",
      "Iteration 6170 Training loss 0.05421470105648041 Validation loss 0.051215607672929764 Accuracy 0.8665000200271606\n",
      "Iteration 6180 Training loss 0.05116075277328491 Validation loss 0.049427952617406845 Accuracy 0.875\n",
      "Iteration 6190 Training loss 0.0529131218791008 Validation loss 0.05154643580317497 Accuracy 0.8665000200271606\n",
      "Iteration 6200 Training loss 0.054694581776857376 Validation loss 0.05336847901344299 Accuracy 0.8644999861717224\n",
      "Iteration 6210 Training loss 0.06293859332799911 Validation loss 0.05441678687930107 Accuracy 0.8640000224113464\n",
      "Iteration 6220 Training loss 0.059137314558029175 Validation loss 0.05253859981894493 Accuracy 0.8669999837875366\n",
      "Iteration 6230 Training loss 0.05796823278069496 Validation loss 0.053098246455192566 Accuracy 0.8650000095367432\n",
      "Iteration 6240 Training loss 0.05358683690428734 Validation loss 0.05030493438243866 Accuracy 0.8694999814033508\n",
      "Iteration 6250 Training loss 0.06317633390426636 Validation loss 0.053558673709630966 Accuracy 0.8634999990463257\n",
      "Iteration 6260 Training loss 0.06024494022130966 Validation loss 0.0514654777944088 Accuracy 0.8705000281333923\n",
      "Iteration 6270 Training loss 0.056914396584033966 Validation loss 0.050717223435640335 Accuracy 0.8730000257492065\n",
      "Iteration 6280 Training loss 0.05337485671043396 Validation loss 0.05204369127750397 Accuracy 0.8659999966621399\n",
      "Iteration 6290 Training loss 0.05358880013227463 Validation loss 0.05129009857773781 Accuracy 0.8675000071525574\n",
      "Iteration 6300 Training loss 0.05333780497312546 Validation loss 0.05062294751405716 Accuracy 0.8715000152587891\n",
      "Iteration 6310 Training loss 0.05764790251851082 Validation loss 0.05085943266749382 Accuracy 0.8659999966621399\n",
      "Iteration 6320 Training loss 0.058627478778362274 Validation loss 0.05314793437719345 Accuracy 0.8654999732971191\n",
      "Iteration 6330 Training loss 0.054355718195438385 Validation loss 0.05058785155415535 Accuracy 0.8730000257492065\n",
      "Iteration 6340 Training loss 0.05467991158366203 Validation loss 0.05024515464901924 Accuracy 0.8715000152587891\n",
      "Iteration 6350 Training loss 0.060848865658044815 Validation loss 0.05397345870733261 Accuracy 0.8644999861717224\n",
      "Iteration 6360 Training loss 0.05580233782529831 Validation loss 0.05119501054286957 Accuracy 0.8679999709129333\n",
      "Iteration 6370 Training loss 0.0571318119764328 Validation loss 0.050975773483514786 Accuracy 0.8700000047683716\n",
      "Iteration 6380 Training loss 0.05636248365044594 Validation loss 0.051627323031425476 Accuracy 0.8669999837875366\n",
      "Iteration 6390 Training loss 0.055904656648635864 Validation loss 0.05163102596998215 Accuracy 0.8669999837875366\n",
      "Iteration 6400 Training loss 0.055368635803461075 Validation loss 0.05223167687654495 Accuracy 0.8640000224113464\n",
      "Iteration 6410 Training loss 0.06893689185380936 Validation loss 0.05625487118959427 Accuracy 0.859000027179718\n",
      "Iteration 6420 Training loss 0.05633101984858513 Validation loss 0.050701018422842026 Accuracy 0.8700000047683716\n",
      "Iteration 6430 Training loss 0.055891960859298706 Validation loss 0.05104998126626015 Accuracy 0.8700000047683716\n",
      "Iteration 6440 Training loss 0.05459826812148094 Validation loss 0.05067504942417145 Accuracy 0.8694999814033508\n",
      "Iteration 6450 Training loss 0.055921707302331924 Validation loss 0.04995935037732124 Accuracy 0.8700000047683716\n",
      "Iteration 6460 Training loss 0.05580449849367142 Validation loss 0.051487550139427185 Accuracy 0.8705000281333923\n",
      "Iteration 6470 Training loss 0.05510272830724716 Validation loss 0.05278091877698898 Accuracy 0.862500011920929\n",
      "Iteration 6480 Training loss 0.058186423033475876 Validation loss 0.051834579557180405 Accuracy 0.8644999861717224\n",
      "Iteration 6490 Training loss 0.060871466994285583 Validation loss 0.0541662760078907 Accuracy 0.8644999861717224\n",
      "Iteration 6500 Training loss 0.0571732260286808 Validation loss 0.0511757954955101 Accuracy 0.8684999942779541\n",
      "Iteration 6510 Training loss 0.05156337097287178 Validation loss 0.051620759069919586 Accuracy 0.8679999709129333\n",
      "Iteration 6520 Training loss 0.06022246927022934 Validation loss 0.052283093333244324 Accuracy 0.8654999732971191\n",
      "Iteration 6530 Training loss 0.05607284978032112 Validation loss 0.053974200040102005 Accuracy 0.8644999861717224\n",
      "Iteration 6540 Training loss 0.05295306444168091 Validation loss 0.050320256501436234 Accuracy 0.8715000152587891\n",
      "Iteration 6550 Training loss 0.056148942559957504 Validation loss 0.051660679280757904 Accuracy 0.8659999966621399\n",
      "Iteration 6560 Training loss 0.05375877395272255 Validation loss 0.0518571138381958 Accuracy 0.8665000200271606\n",
      "Iteration 6570 Training loss 0.05910946801304817 Validation loss 0.05365746468305588 Accuracy 0.8640000224113464\n",
      "Iteration 6580 Training loss 0.05676477029919624 Validation loss 0.054183151572942734 Accuracy 0.859499990940094\n",
      "Iteration 6590 Training loss 0.06068216636776924 Validation loss 0.051655225455760956 Accuracy 0.8690000176429749\n",
      "Iteration 6600 Training loss 0.05371401086449623 Validation loss 0.05021350085735321 Accuracy 0.871999979019165\n",
      "Iteration 6610 Training loss 0.05181596800684929 Validation loss 0.049804460257291794 Accuracy 0.8744999766349792\n",
      "Iteration 6620 Training loss 0.07677397131919861 Validation loss 0.06358157843351364 Accuracy 0.8450000286102295\n",
      "Iteration 6630 Training loss 0.05081288143992424 Validation loss 0.04992016777396202 Accuracy 0.8734999895095825\n",
      "Iteration 6640 Training loss 0.061025481671094894 Validation loss 0.05252949148416519 Accuracy 0.8644999861717224\n",
      "Iteration 6650 Training loss 0.05221769958734512 Validation loss 0.050521936267614365 Accuracy 0.871999979019165\n",
      "Iteration 6660 Training loss 0.06605608761310577 Validation loss 0.056061625480651855 Accuracy 0.8575000166893005\n",
      "Iteration 6670 Training loss 0.0577714778482914 Validation loss 0.050813790410757065 Accuracy 0.8705000281333923\n",
      "Iteration 6680 Training loss 0.058510370552539825 Validation loss 0.05489504337310791 Accuracy 0.8610000014305115\n",
      "Iteration 6690 Training loss 0.05349332466721535 Validation loss 0.0493842214345932 Accuracy 0.8725000023841858\n",
      "Iteration 6700 Training loss 0.05862964317202568 Validation loss 0.05334605276584625 Accuracy 0.8654999732971191\n",
      "Iteration 6710 Training loss 0.0599403902888298 Validation loss 0.053719595074653625 Accuracy 0.862500011920929\n",
      "Iteration 6720 Training loss 0.052464064210653305 Validation loss 0.05047638714313507 Accuracy 0.8725000023841858\n",
      "Iteration 6730 Training loss 0.058594297617673874 Validation loss 0.05482105538249016 Accuracy 0.8619999885559082\n",
      "Iteration 6740 Training loss 0.05771545320749283 Validation loss 0.05075804516673088 Accuracy 0.8715000152587891\n",
      "Iteration 6750 Training loss 0.061083823442459106 Validation loss 0.05188754200935364 Accuracy 0.8675000071525574\n",
      "Iteration 6760 Training loss 0.0661703422665596 Validation loss 0.05447916314005852 Accuracy 0.862500011920929\n",
      "Iteration 6770 Training loss 0.054036956280469894 Validation loss 0.052476346492767334 Accuracy 0.8629999756813049\n",
      "Iteration 6780 Training loss 0.0511159747838974 Validation loss 0.050937965512275696 Accuracy 0.8690000176429749\n",
      "Iteration 6790 Training loss 0.05879303812980652 Validation loss 0.05177747458219528 Accuracy 0.8665000200271606\n",
      "Iteration 6800 Training loss 0.05032513663172722 Validation loss 0.050024546682834625 Accuracy 0.8740000128746033\n",
      "Iteration 6810 Training loss 0.048482056707143784 Validation loss 0.050191283226013184 Accuracy 0.871999979019165\n",
      "Iteration 6820 Training loss 0.05453919991850853 Validation loss 0.04939347878098488 Accuracy 0.8755000233650208\n",
      "Iteration 6830 Training loss 0.061230242252349854 Validation loss 0.052880529314279556 Accuracy 0.8650000095367432\n",
      "Iteration 6840 Training loss 0.05424012243747711 Validation loss 0.0492975153028965 Accuracy 0.8744999766349792\n",
      "Iteration 6850 Training loss 0.059460412710905075 Validation loss 0.05267803743481636 Accuracy 0.8629999756813049\n",
      "Iteration 6860 Training loss 0.05910903587937355 Validation loss 0.05118554085493088 Accuracy 0.8659999966621399\n",
      "Iteration 6870 Training loss 0.057455435395240784 Validation loss 0.05119062960147858 Accuracy 0.8675000071525574\n",
      "Iteration 6880 Training loss 0.0612170472741127 Validation loss 0.05060453712940216 Accuracy 0.8705000281333923\n",
      "Iteration 6890 Training loss 0.061480823904275894 Validation loss 0.05619151517748833 Accuracy 0.859499990940094\n",
      "Iteration 6900 Training loss 0.05768556147813797 Validation loss 0.050733137875795364 Accuracy 0.8725000023841858\n",
      "Iteration 6910 Training loss 0.060806773602962494 Validation loss 0.0549609549343586 Accuracy 0.8600000143051147\n",
      "Iteration 6920 Training loss 0.061953213065862656 Validation loss 0.05339646711945534 Accuracy 0.8634999990463257\n",
      "Iteration 6930 Training loss 0.055548157542943954 Validation loss 0.05103641748428345 Accuracy 0.8694999814033508\n",
      "Iteration 6940 Training loss 0.06256753206253052 Validation loss 0.0561712421476841 Accuracy 0.859499990940094\n",
      "Iteration 6950 Training loss 0.056226640939712524 Validation loss 0.05009614676237106 Accuracy 0.8715000152587891\n",
      "Iteration 6960 Training loss 0.06065221130847931 Validation loss 0.05201609805226326 Accuracy 0.8669999837875366\n",
      "Iteration 6970 Training loss 0.05283842235803604 Validation loss 0.050959330052137375 Accuracy 0.8694999814033508\n",
      "Iteration 6980 Training loss 0.050545837730169296 Validation loss 0.05103577300906181 Accuracy 0.8679999709129333\n",
      "Iteration 6990 Training loss 0.05917500704526901 Validation loss 0.053309258073568344 Accuracy 0.8644999861717224\n",
      "Iteration 7000 Training loss 0.05123218894004822 Validation loss 0.05050482973456383 Accuracy 0.8694999814033508\n",
      "Iteration 7010 Training loss 0.056350477039813995 Validation loss 0.0525587759912014 Accuracy 0.8650000095367432\n",
      "Iteration 7020 Training loss 0.05583487078547478 Validation loss 0.04957907646894455 Accuracy 0.8730000257492065\n",
      "Iteration 7030 Training loss 0.060330551117658615 Validation loss 0.05234092101454735 Accuracy 0.8659999966621399\n",
      "Iteration 7040 Training loss 0.05757959932088852 Validation loss 0.052639517933130264 Accuracy 0.8629999756813049\n",
      "Iteration 7050 Training loss 0.05948484316468239 Validation loss 0.05409211292862892 Accuracy 0.8604999780654907\n",
      "Iteration 7060 Training loss 0.059441689401865005 Validation loss 0.05540908873081207 Accuracy 0.862500011920929\n",
      "Iteration 7070 Training loss 0.0603894367814064 Validation loss 0.05100584402680397 Accuracy 0.8669999837875366\n",
      "Iteration 7080 Training loss 0.058673709630966187 Validation loss 0.05334586650133133 Accuracy 0.8619999885559082\n",
      "Iteration 7090 Training loss 0.058394331485033035 Validation loss 0.0522671602666378 Accuracy 0.8679999709129333\n",
      "Iteration 7100 Training loss 0.05863571539521217 Validation loss 0.05571384355425835 Accuracy 0.8569999933242798\n",
      "Iteration 7110 Training loss 0.062253259122371674 Validation loss 0.051885247230529785 Accuracy 0.8640000224113464\n",
      "Iteration 7120 Training loss 0.05770431458950043 Validation loss 0.05064051225781441 Accuracy 0.8709999918937683\n",
      "Iteration 7130 Training loss 0.05814787745475769 Validation loss 0.05361257493495941 Accuracy 0.8634999990463257\n",
      "Iteration 7140 Training loss 0.06494640558958054 Validation loss 0.0533578060567379 Accuracy 0.8650000095367432\n",
      "Iteration 7150 Training loss 0.054790228605270386 Validation loss 0.052015576511621475 Accuracy 0.8679999709129333\n",
      "Iteration 7160 Training loss 0.0574004165828228 Validation loss 0.05058705806732178 Accuracy 0.8715000152587891\n",
      "Iteration 7170 Training loss 0.04917352274060249 Validation loss 0.04957305267453194 Accuracy 0.8730000257492065\n",
      "Iteration 7180 Training loss 0.05028386041522026 Validation loss 0.04954437166452408 Accuracy 0.8715000152587891\n",
      "Iteration 7190 Training loss 0.06538088619709015 Validation loss 0.056100450456142426 Accuracy 0.859000027179718\n",
      "Iteration 7200 Training loss 0.06125590205192566 Validation loss 0.05390523374080658 Accuracy 0.8615000247955322\n",
      "Iteration 7210 Training loss 0.0601256787776947 Validation loss 0.05344235152006149 Accuracy 0.8654999732971191\n",
      "Iteration 7220 Training loss 0.060978129506111145 Validation loss 0.05200618878006935 Accuracy 0.8654999732971191\n",
      "Iteration 7230 Training loss 0.053372982889413834 Validation loss 0.050490837544202805 Accuracy 0.8715000152587891\n",
      "Iteration 7240 Training loss 0.05864103138446808 Validation loss 0.05328032746911049 Accuracy 0.8644999861717224\n",
      "Iteration 7250 Training loss 0.05389517918229103 Validation loss 0.050772570073604584 Accuracy 0.8700000047683716\n",
      "Iteration 7260 Training loss 0.056277863681316376 Validation loss 0.05273807793855667 Accuracy 0.8665000200271606\n",
      "Iteration 7270 Training loss 0.06401947885751724 Validation loss 0.05453256890177727 Accuracy 0.8629999756813049\n",
      "Iteration 7280 Training loss 0.06666397303342819 Validation loss 0.05684088543057442 Accuracy 0.8569999933242798\n",
      "Iteration 7290 Training loss 0.0628410279750824 Validation loss 0.053681280463933945 Accuracy 0.8619999885559082\n",
      "Iteration 7300 Training loss 0.06669099628925323 Validation loss 0.0575714036822319 Accuracy 0.8569999933242798\n",
      "Iteration 7310 Training loss 0.05482671409845352 Validation loss 0.05073734372854233 Accuracy 0.8730000257492065\n",
      "Iteration 7320 Training loss 0.05569944158196449 Validation loss 0.05005133897066116 Accuracy 0.8740000128746033\n",
      "Iteration 7330 Training loss 0.06474330276250839 Validation loss 0.0745646134018898 Accuracy 0.8199999928474426\n",
      "Iteration 7340 Training loss 0.054510198533535004 Validation loss 0.05737466737627983 Accuracy 0.8579999804496765\n",
      "Iteration 7350 Training loss 0.06068035960197449 Validation loss 0.06492239981889725 Accuracy 0.8385000228881836\n",
      "Iteration 7360 Training loss 0.06293778866529465 Validation loss 0.06650621443986893 Accuracy 0.8379999995231628\n",
      "Iteration 7370 Training loss 0.05059083551168442 Validation loss 0.05324115231633186 Accuracy 0.8619999885559082\n",
      "Iteration 7380 Training loss 0.06715304404497147 Validation loss 0.06928704679012299 Accuracy 0.8330000042915344\n",
      "Iteration 7390 Training loss 0.054824575781822205 Validation loss 0.06403055042028427 Accuracy 0.840499997138977\n",
      "Iteration 7400 Training loss 0.05963024124503136 Validation loss 0.0683714747428894 Accuracy 0.8345000147819519\n",
      "Iteration 7410 Training loss 0.059184540063142776 Validation loss 0.06397432088851929 Accuracy 0.8424999713897705\n",
      "Iteration 7420 Training loss 0.052799247205257416 Validation loss 0.0557137057185173 Accuracy 0.862500011920929\n",
      "Iteration 7430 Training loss 0.055751074105501175 Validation loss 0.06012259051203728 Accuracy 0.8489999771118164\n",
      "Iteration 7440 Training loss 0.06043406203389168 Validation loss 0.06619790196418762 Accuracy 0.8360000252723694\n",
      "Iteration 7450 Training loss 0.065031498670578 Validation loss 0.07097434997558594 Accuracy 0.8270000219345093\n",
      "Iteration 7460 Training loss 0.0638933926820755 Validation loss 0.06436845660209656 Accuracy 0.8414999842643738\n",
      "Iteration 7470 Training loss 0.05293860659003258 Validation loss 0.05631961673498154 Accuracy 0.859499990940094\n",
      "Iteration 7480 Training loss 0.05544937774538994 Validation loss 0.0602959580719471 Accuracy 0.8500000238418579\n",
      "Iteration 7490 Training loss 0.05439784377813339 Validation loss 0.060100007802248 Accuracy 0.8489999771118164\n",
      "Iteration 7500 Training loss 0.05708499252796173 Validation loss 0.06095448508858681 Accuracy 0.8479999899864197\n",
      "Iteration 7510 Training loss 0.059803642332553864 Validation loss 0.06693319231271744 Accuracy 0.8349999785423279\n",
      "Iteration 7520 Training loss 0.05978718400001526 Validation loss 0.06113098934292793 Accuracy 0.847000002861023\n",
      "Iteration 7530 Training loss 0.06116284057497978 Validation loss 0.06477352976799011 Accuracy 0.8399999737739563\n",
      "Iteration 7540 Training loss 0.06205017864704132 Validation loss 0.06500010937452316 Accuracy 0.8399999737739563\n",
      "Iteration 7550 Training loss 0.051377072930336 Validation loss 0.05199529230594635 Accuracy 0.8690000176429749\n",
      "Iteration 7560 Training loss 0.06354689598083496 Validation loss 0.05571909248828888 Accuracy 0.859499990940094\n",
      "Iteration 7570 Training loss 0.05785982683300972 Validation loss 0.05152084678411484 Accuracy 0.8675000071525574\n",
      "Iteration 7580 Training loss 0.053587090224027634 Validation loss 0.05072991922497749 Accuracy 0.871999979019165\n",
      "Iteration 7590 Training loss 0.05578583478927612 Validation loss 0.051299482583999634 Accuracy 0.8694999814033508\n",
      "Iteration 7600 Training loss 0.05138404667377472 Validation loss 0.04968622326850891 Accuracy 0.871999979019165\n",
      "Iteration 7610 Training loss 0.06222565099596977 Validation loss 0.05462614446878433 Accuracy 0.8604999780654907\n",
      "Iteration 7620 Training loss 0.05751287192106247 Validation loss 0.050829771906137466 Accuracy 0.8684999942779541\n",
      "Iteration 7630 Training loss 0.05944206565618515 Validation loss 0.05062013491988182 Accuracy 0.871999979019165\n",
      "Iteration 7640 Training loss 0.05315761640667915 Validation loss 0.050664085894823074 Accuracy 0.8730000257492065\n",
      "Iteration 7650 Training loss 0.0597594752907753 Validation loss 0.05349216237664223 Accuracy 0.8640000224113464\n",
      "Iteration 7660 Training loss 0.05832753702998161 Validation loss 0.05015876516699791 Accuracy 0.8734999895095825\n",
      "Iteration 7670 Training loss 0.05548981577157974 Validation loss 0.05047561228275299 Accuracy 0.8730000257492065\n",
      "Iteration 7680 Training loss 0.06454844772815704 Validation loss 0.05521651729941368 Accuracy 0.8604999780654907\n",
      "Iteration 7690 Training loss 0.05392017588019371 Validation loss 0.05122191831469536 Accuracy 0.8709999918937683\n",
      "Iteration 7700 Training loss 0.0644906759262085 Validation loss 0.055267028510570526 Accuracy 0.859000027179718\n",
      "Iteration 7710 Training loss 0.05858544632792473 Validation loss 0.05157811939716339 Accuracy 0.8654999732971191\n",
      "Iteration 7720 Training loss 0.05640257149934769 Validation loss 0.05036112293601036 Accuracy 0.8709999918937683\n",
      "Iteration 7730 Training loss 0.05319217965006828 Validation loss 0.050258569419384 Accuracy 0.8715000152587891\n",
      "Iteration 7740 Training loss 0.05581096559762955 Validation loss 0.05068717524409294 Accuracy 0.8715000152587891\n",
      "Iteration 7750 Training loss 0.06323372572660446 Validation loss 0.055567674338817596 Accuracy 0.8600000143051147\n",
      "Iteration 7760 Training loss 0.060814689844846725 Validation loss 0.051899731159210205 Accuracy 0.8659999966621399\n",
      "Iteration 7770 Training loss 0.05639154091477394 Validation loss 0.0516088493168354 Accuracy 0.8684999942779541\n",
      "Iteration 7780 Training loss 0.05678053945302963 Validation loss 0.0540490485727787 Accuracy 0.8619999885559082\n",
      "Iteration 7790 Training loss 0.05708307772874832 Validation loss 0.05169754847884178 Accuracy 0.8684999942779541\n",
      "Iteration 7800 Training loss 0.05671311914920807 Validation loss 0.0518551766872406 Accuracy 0.8659999966621399\n",
      "Iteration 7810 Training loss 0.06523224711418152 Validation loss 0.05463121458888054 Accuracy 0.859499990940094\n",
      "Iteration 7820 Training loss 0.057605113834142685 Validation loss 0.05249281972646713 Accuracy 0.8644999861717224\n",
      "Iteration 7830 Training loss 0.07261721789836884 Validation loss 0.05809485912322998 Accuracy 0.8550000190734863\n",
      "Iteration 7840 Training loss 0.061334606260061264 Validation loss 0.051028426736593246 Accuracy 0.8705000281333923\n",
      "Iteration 7850 Training loss 0.057340335100889206 Validation loss 0.050529010593891144 Accuracy 0.871999979019165\n",
      "Iteration 7860 Training loss 0.05746467411518097 Validation loss 0.05188683420419693 Accuracy 0.8654999732971191\n",
      "Iteration 7870 Training loss 0.05542544648051262 Validation loss 0.05007880553603172 Accuracy 0.8675000071525574\n",
      "Iteration 7880 Training loss 0.06936784833669662 Validation loss 0.059447694569826126 Accuracy 0.8514999747276306\n",
      "Iteration 7890 Training loss 0.05396862328052521 Validation loss 0.0512932650744915 Accuracy 0.8690000176429749\n",
      "Iteration 7900 Training loss 0.06374090909957886 Validation loss 0.05440220981836319 Accuracy 0.8619999885559082\n",
      "Iteration 7910 Training loss 0.05517594888806343 Validation loss 0.05237116664648056 Accuracy 0.8665000200271606\n",
      "Iteration 7920 Training loss 0.05906212329864502 Validation loss 0.05155219882726669 Accuracy 0.8675000071525574\n",
      "Iteration 7930 Training loss 0.058220282196998596 Validation loss 0.05348402261734009 Accuracy 0.8615000247955322\n",
      "Iteration 7940 Training loss 0.05414111912250519 Validation loss 0.05132129415869713 Accuracy 0.8679999709129333\n",
      "Iteration 7950 Training loss 0.05666937306523323 Validation loss 0.051492560654878616 Accuracy 0.8679999709129333\n",
      "Iteration 7960 Training loss 0.05530491843819618 Validation loss 0.05111405998468399 Accuracy 0.8705000281333923\n",
      "Iteration 7970 Training loss 0.05675867199897766 Validation loss 0.05073939636349678 Accuracy 0.8734999895095825\n",
      "Iteration 7980 Training loss 0.059068504720926285 Validation loss 0.05489606410264969 Accuracy 0.8604999780654907\n",
      "Iteration 7990 Training loss 0.05312131717801094 Validation loss 0.051448337733745575 Accuracy 0.8684999942779541\n",
      "Iteration 8000 Training loss 0.05974413454532623 Validation loss 0.0513627789914608 Accuracy 0.8700000047683716\n",
      "Iteration 8010 Training loss 0.0647217407822609 Validation loss 0.05367797613143921 Accuracy 0.8650000095367432\n",
      "Iteration 8020 Training loss 0.05792054161429405 Validation loss 0.05304228514432907 Accuracy 0.8644999861717224\n",
      "Iteration 8030 Training loss 0.05935398116707802 Validation loss 0.05368782579898834 Accuracy 0.8640000224113464\n",
      "Iteration 8040 Training loss 0.06364866346120834 Validation loss 0.05465402826666832 Accuracy 0.8615000247955322\n",
      "Iteration 8050 Training loss 0.06202590465545654 Validation loss 0.054682713001966476 Accuracy 0.8619999885559082\n",
      "Iteration 8060 Training loss 0.05234735459089279 Validation loss 0.05036461353302002 Accuracy 0.8734999895095825\n",
      "Iteration 8070 Training loss 0.07095824182033539 Validation loss 0.05861891806125641 Accuracy 0.8510000109672546\n",
      "Iteration 8080 Training loss 0.06893907487392426 Validation loss 0.05791071802377701 Accuracy 0.8514999747276306\n",
      "Iteration 8090 Training loss 0.06122281774878502 Validation loss 0.054848186671733856 Accuracy 0.8610000014305115\n",
      "Iteration 8100 Training loss 0.05392983555793762 Validation loss 0.05043564736843109 Accuracy 0.8715000152587891\n",
      "Iteration 8110 Training loss 0.058696456253528595 Validation loss 0.05163703113794327 Accuracy 0.8690000176429749\n",
      "Iteration 8120 Training loss 0.05848429724574089 Validation loss 0.05071476101875305 Accuracy 0.8725000023841858\n",
      "Iteration 8130 Training loss 0.05514662340283394 Validation loss 0.05152391642332077 Accuracy 0.8694999814033508\n",
      "Iteration 8140 Training loss 0.0555044487118721 Validation loss 0.0513632670044899 Accuracy 0.8700000047683716\n",
      "Iteration 8150 Training loss 0.07273474335670471 Validation loss 0.061375465244054794 Accuracy 0.8495000004768372\n",
      "Iteration 8160 Training loss 0.05431235581636429 Validation loss 0.05041809752583504 Accuracy 0.871999979019165\n",
      "Iteration 8170 Training loss 0.059706687927246094 Validation loss 0.05258670076727867 Accuracy 0.8650000095367432\n",
      "Iteration 8180 Training loss 0.056940022855997086 Validation loss 0.05126923695206642 Accuracy 0.8654999732971191\n",
      "Iteration 8190 Training loss 0.057839587330818176 Validation loss 0.052114490419626236 Accuracy 0.8669999837875366\n",
      "Iteration 8200 Training loss 0.05446299910545349 Validation loss 0.052323877811431885 Accuracy 0.8679999709129333\n",
      "Iteration 8210 Training loss 0.05802208557724953 Validation loss 0.05192829668521881 Accuracy 0.8659999966621399\n",
      "Iteration 8220 Training loss 0.054037436842918396 Validation loss 0.05133841931819916 Accuracy 0.8700000047683716\n",
      "Iteration 8230 Training loss 0.0628373995423317 Validation loss 0.054500762373209 Accuracy 0.8619999885559082\n",
      "Iteration 8240 Training loss 0.06418629735708237 Validation loss 0.05563056096434593 Accuracy 0.859000027179718\n",
      "Iteration 8250 Training loss 0.05052472651004791 Validation loss 0.05133220553398132 Accuracy 0.8675000071525574\n",
      "Iteration 8260 Training loss 0.05856708064675331 Validation loss 0.05388883501291275 Accuracy 0.8615000247955322\n",
      "Iteration 8270 Training loss 0.056282393634319305 Validation loss 0.05246013030409813 Accuracy 0.8640000224113464\n",
      "Iteration 8280 Training loss 0.06052614748477936 Validation loss 0.05350469797849655 Accuracy 0.8634999990463257\n",
      "Iteration 8290 Training loss 0.059685688465833664 Validation loss 0.051183346658945084 Accuracy 0.8705000281333923\n",
      "Iteration 8300 Training loss 0.05672454461455345 Validation loss 0.05038983374834061 Accuracy 0.8740000128746033\n",
      "Iteration 8310 Training loss 0.062032151967287064 Validation loss 0.054667457938194275 Accuracy 0.8610000014305115\n",
      "Iteration 8320 Training loss 0.05478501692414284 Validation loss 0.050065211951732635 Accuracy 0.8709999918937683\n",
      "Iteration 8330 Training loss 0.0634767934679985 Validation loss 0.05446021258831024 Accuracy 0.8585000038146973\n",
      "Iteration 8340 Training loss 0.05584917217493057 Validation loss 0.05136192589998245 Accuracy 0.8700000047683716\n",
      "Iteration 8350 Training loss 0.059707146137952805 Validation loss 0.05322445556521416 Accuracy 0.8640000224113464\n",
      "Iteration 8360 Training loss 0.0533883199095726 Validation loss 0.05007380247116089 Accuracy 0.8734999895095825\n",
      "Iteration 8370 Training loss 0.06265786290168762 Validation loss 0.053638845682144165 Accuracy 0.8634999990463257\n",
      "Iteration 8380 Training loss 0.06509944051504135 Validation loss 0.05519721657037735 Accuracy 0.859499990940094\n",
      "Iteration 8390 Training loss 0.05900971591472626 Validation loss 0.05200446397066116 Accuracy 0.8694999814033508\n",
      "Iteration 8400 Training loss 0.05890471860766411 Validation loss 0.05235735699534416 Accuracy 0.8650000095367432\n",
      "Iteration 8410 Training loss 0.05293518304824829 Validation loss 0.050606247037649155 Accuracy 0.8744999766349792\n",
      "Iteration 8420 Training loss 0.0674135610461235 Validation loss 0.05296368896961212 Accuracy 0.8665000200271606\n",
      "Iteration 8430 Training loss 0.05505318194627762 Validation loss 0.050185833126306534 Accuracy 0.8715000152587891\n",
      "Iteration 8440 Training loss 0.0655236542224884 Validation loss 0.05644315481185913 Accuracy 0.8544999957084656\n",
      "Iteration 8450 Training loss 0.051619455218315125 Validation loss 0.050368186086416245 Accuracy 0.8725000023841858\n",
      "Iteration 8460 Training loss 0.0587252713739872 Validation loss 0.051522135734558105 Accuracy 0.8700000047683716\n",
      "Iteration 8470 Training loss 0.05429788678884506 Validation loss 0.05006604641675949 Accuracy 0.8734999895095825\n",
      "Iteration 8480 Training loss 0.060735154896974564 Validation loss 0.05332805588841438 Accuracy 0.8634999990463257\n",
      "Iteration 8490 Training loss 0.0588175430893898 Validation loss 0.05118411406874657 Accuracy 0.8694999814033508\n",
      "Iteration 8500 Training loss 0.0580601766705513 Validation loss 0.05170276761054993 Accuracy 0.8690000176429749\n",
      "Iteration 8510 Training loss 0.05912647396326065 Validation loss 0.05333757400512695 Accuracy 0.8629999756813049\n",
      "Iteration 8520 Training loss 0.06274192035198212 Validation loss 0.05339562147855759 Accuracy 0.8604999780654907\n",
      "Iteration 8530 Training loss 0.06070104241371155 Validation loss 0.05423995852470398 Accuracy 0.8604999780654907\n",
      "Iteration 8540 Training loss 0.05470791831612587 Validation loss 0.051280878484249115 Accuracy 0.871999979019165\n",
      "Iteration 8550 Training loss 0.04758847504854202 Validation loss 0.05032826215028763 Accuracy 0.8715000152587891\n",
      "Iteration 8560 Training loss 0.07265550643205643 Validation loss 0.06254179030656815 Accuracy 0.8464999794960022\n",
      "Iteration 8570 Training loss 0.06183324381709099 Validation loss 0.05323198810219765 Accuracy 0.8610000014305115\n",
      "Iteration 8580 Training loss 0.061345040798187256 Validation loss 0.05221150070428848 Accuracy 0.8650000095367432\n",
      "Iteration 8590 Training loss 0.05793212726712227 Validation loss 0.05126214772462845 Accuracy 0.8700000047683716\n",
      "Iteration 8600 Training loss 0.05022154375910759 Validation loss 0.0507301390171051 Accuracy 0.8709999918937683\n",
      "Iteration 8610 Training loss 0.0672544613480568 Validation loss 0.05747579038143158 Accuracy 0.8535000085830688\n",
      "Iteration 8620 Training loss 0.06002471223473549 Validation loss 0.05280959978699684 Accuracy 0.8665000200271606\n",
      "Iteration 8630 Training loss 0.05531318485736847 Validation loss 0.05099228769540787 Accuracy 0.8715000152587891\n",
      "Iteration 8640 Training loss 0.05901914834976196 Validation loss 0.05165638029575348 Accuracy 0.8690000176429749\n",
      "Iteration 8650 Training loss 0.06137137487530708 Validation loss 0.0526481494307518 Accuracy 0.8644999861717224\n",
      "Iteration 8660 Training loss 0.061226338148117065 Validation loss 0.05401459336280823 Accuracy 0.8629999756813049\n",
      "Iteration 8670 Training loss 0.0529918447136879 Validation loss 0.05173410102725029 Accuracy 0.8690000176429749\n",
      "Iteration 8680 Training loss 0.06766617298126221 Validation loss 0.07378806173801422 Accuracy 0.8224999904632568\n",
      "Iteration 8690 Training loss 0.05650093033909798 Validation loss 0.06327170133590698 Accuracy 0.8429999947547913\n",
      "Iteration 8700 Training loss 0.06100776791572571 Validation loss 0.06831782311201096 Accuracy 0.8349999785423279\n",
      "Iteration 8710 Training loss 0.06681188941001892 Validation loss 0.06901739537715912 Accuracy 0.8324999809265137\n",
      "Iteration 8720 Training loss 0.05746110901236534 Validation loss 0.060084983706474304 Accuracy 0.8489999771118164\n",
      "Iteration 8730 Training loss 0.05615834519267082 Validation loss 0.0579451359808445 Accuracy 0.8575000166893005\n",
      "Iteration 8740 Training loss 0.051666077226400375 Validation loss 0.060205891728401184 Accuracy 0.8489999771118164\n",
      "Iteration 8750 Training loss 0.060958366841077805 Validation loss 0.062460463494062424 Accuracy 0.8454999923706055\n",
      "Iteration 8760 Training loss 0.05690210312604904 Validation loss 0.0635971948504448 Accuracy 0.8420000076293945\n",
      "Iteration 8770 Training loss 0.058387961238622665 Validation loss 0.06593313813209534 Accuracy 0.8374999761581421\n",
      "Iteration 8780 Training loss 0.06331860274076462 Validation loss 0.06729689985513687 Accuracy 0.8360000252723694\n",
      "Iteration 8790 Training loss 0.06515350937843323 Validation loss 0.07010391354560852 Accuracy 0.8299999833106995\n",
      "Iteration 8800 Training loss 0.06215295568108559 Validation loss 0.06755273789167404 Accuracy 0.8360000252723694\n",
      "Iteration 8810 Training loss 0.05301533639431 Validation loss 0.054468169808387756 Accuracy 0.8634999990463257\n",
      "Iteration 8820 Training loss 0.054538119584321976 Validation loss 0.06046994775533676 Accuracy 0.8510000109672546\n",
      "Iteration 8830 Training loss 0.05530616268515587 Validation loss 0.06437760591506958 Accuracy 0.8424999713897705\n",
      "Iteration 8840 Training loss 0.059052255004644394 Validation loss 0.059952668845653534 Accuracy 0.8514999747276306\n",
      "Iteration 8850 Training loss 0.05656572803854942 Validation loss 0.06061428785324097 Accuracy 0.8495000004768372\n",
      "Iteration 8860 Training loss 0.06167515739798546 Validation loss 0.06261768192052841 Accuracy 0.8445000052452087\n",
      "Iteration 8870 Training loss 0.062246255576610565 Validation loss 0.06755416095256805 Accuracy 0.8355000019073486\n",
      "Iteration 8880 Training loss 0.05546025559306145 Validation loss 0.056842900812625885 Accuracy 0.8600000143051147\n",
      "Iteration 8890 Training loss 0.05328301340341568 Validation loss 0.06405992060899734 Accuracy 0.8420000076293945\n",
      "Iteration 8900 Training loss 0.0608694925904274 Validation loss 0.06536057591438293 Accuracy 0.8399999737739563\n",
      "Iteration 8910 Training loss 0.06730261445045471 Validation loss 0.07590097934007645 Accuracy 0.8140000104904175\n",
      "Iteration 8920 Training loss 0.058974284678697586 Validation loss 0.06620746105909348 Accuracy 0.8385000228881836\n",
      "Iteration 8930 Training loss 0.058672305196523666 Validation loss 0.06665206700563431 Accuracy 0.8364999890327454\n",
      "Iteration 8940 Training loss 0.06225050240755081 Validation loss 0.07065753638744354 Accuracy 0.828499972820282\n",
      "Iteration 8950 Training loss 0.06273341178894043 Validation loss 0.0631311908364296 Accuracy 0.8454999923706055\n",
      "Iteration 8960 Training loss 0.0574980303645134 Validation loss 0.06181960180401802 Accuracy 0.8464999794960022\n",
      "Iteration 8970 Training loss 0.06358259916305542 Validation loss 0.06399804353713989 Accuracy 0.843999981880188\n",
      "Iteration 8980 Training loss 0.05232102796435356 Validation loss 0.055563099682331085 Accuracy 0.8610000014305115\n",
      "Iteration 8990 Training loss 0.061807047575712204 Validation loss 0.06439503282308578 Accuracy 0.8410000205039978\n",
      "Iteration 9000 Training loss 0.055785369127988815 Validation loss 0.06260798871517181 Accuracy 0.8454999923706055\n",
      "Iteration 9010 Training loss 0.06475988775491714 Validation loss 0.0747271254658699 Accuracy 0.8190000057220459\n",
      "Iteration 9020 Training loss 0.05625840276479721 Validation loss 0.06074192747473717 Accuracy 0.8475000262260437\n",
      "Iteration 9030 Training loss 0.05809655785560608 Validation loss 0.06292037665843964 Accuracy 0.843999981880188\n",
      "Iteration 9040 Training loss 0.06636402755975723 Validation loss 0.07245689630508423 Accuracy 0.8245000243186951\n",
      "Iteration 9050 Training loss 0.0596417635679245 Validation loss 0.06341742724180222 Accuracy 0.843999981880188\n",
      "Iteration 9060 Training loss 0.06062975153326988 Validation loss 0.06263002008199692 Accuracy 0.8460000157356262\n",
      "Iteration 9070 Training loss 0.05350937321782112 Validation loss 0.0583379790186882 Accuracy 0.8544999957084656\n",
      "Iteration 9080 Training loss 0.05922984331846237 Validation loss 0.06026153266429901 Accuracy 0.8504999876022339\n",
      "Iteration 9090 Training loss 0.060068730264902115 Validation loss 0.06408233195543289 Accuracy 0.8429999947547913\n",
      "Iteration 9100 Training loss 0.06053564324975014 Validation loss 0.07168685644865036 Accuracy 0.8255000114440918\n",
      "Iteration 9110 Training loss 0.05918032303452492 Validation loss 0.06512171030044556 Accuracy 0.8395000100135803\n",
      "Iteration 9120 Training loss 0.06002394109964371 Validation loss 0.06631464511156082 Accuracy 0.8385000228881836\n",
      "Iteration 9130 Training loss 0.05008114129304886 Validation loss 0.056154023855924606 Accuracy 0.8619999885559082\n",
      "Iteration 9140 Training loss 0.06555912643671036 Validation loss 0.07129518687725067 Accuracy 0.8270000219345093\n",
      "Iteration 9150 Training loss 0.047993022948503494 Validation loss 0.05439313128590584 Accuracy 0.8654999732971191\n",
      "Iteration 9160 Training loss 0.05238046869635582 Validation loss 0.05252707377076149 Accuracy 0.8700000047683716\n",
      "Iteration 9170 Training loss 0.06332078576087952 Validation loss 0.07009311020374298 Accuracy 0.828499972820282\n",
      "Iteration 9180 Training loss 0.060452934354543686 Validation loss 0.06515622138977051 Accuracy 0.8399999737739563\n",
      "Iteration 9190 Training loss 0.05501851812005043 Validation loss 0.060177281498909 Accuracy 0.8500000238418579\n",
      "Iteration 9200 Training loss 0.06207200884819031 Validation loss 0.07054934650659561 Accuracy 0.8289999961853027\n",
      "Iteration 9210 Training loss 0.056596942245960236 Validation loss 0.06451692432165146 Accuracy 0.840499997138977\n",
      "Iteration 9220 Training loss 0.0586501844227314 Validation loss 0.059618644416332245 Accuracy 0.8514999747276306\n",
      "Iteration 9230 Training loss 0.0660034567117691 Validation loss 0.06962572038173676 Accuracy 0.8309999704360962\n",
      "Iteration 9240 Training loss 0.06194532662630081 Validation loss 0.06613203138113022 Accuracy 0.8370000123977661\n",
      "Iteration 9250 Training loss 0.059913452714681625 Validation loss 0.061258088797330856 Accuracy 0.8495000004768372\n",
      "Iteration 9260 Training loss 0.05577641353011131 Validation loss 0.060869522392749786 Accuracy 0.8464999794960022\n",
      "Iteration 9270 Training loss 0.05441710725426674 Validation loss 0.060033995658159256 Accuracy 0.8510000109672546\n",
      "Iteration 9280 Training loss 0.0576246976852417 Validation loss 0.06437864899635315 Accuracy 0.8420000076293945\n",
      "Iteration 9290 Training loss 0.05532398819923401 Validation loss 0.062285348773002625 Accuracy 0.8450000286102295\n",
      "Iteration 9300 Training loss 0.06733854115009308 Validation loss 0.07020751386880875 Accuracy 0.8295000195503235\n",
      "Iteration 9310 Training loss 0.055983152240514755 Validation loss 0.0626721978187561 Accuracy 0.8454999923706055\n",
      "Iteration 9320 Training loss 0.05177588760852814 Validation loss 0.05628357455134392 Accuracy 0.8604999780654907\n",
      "Iteration 9330 Training loss 0.06275724619626999 Validation loss 0.06500836461782455 Accuracy 0.843999981880188\n",
      "Iteration 9340 Training loss 0.060217030346393585 Validation loss 0.06576300412416458 Accuracy 0.8395000100135803\n",
      "Iteration 9350 Training loss 0.06445884704589844 Validation loss 0.06444068253040314 Accuracy 0.8424999713897705\n",
      "Iteration 9360 Training loss 0.057721469551324844 Validation loss 0.0628686174750328 Accuracy 0.8445000052452087\n",
      "Iteration 9370 Training loss 0.05900008976459503 Validation loss 0.061579491943120956 Accuracy 0.8464999794960022\n",
      "Iteration 9380 Training loss 0.06634441018104553 Validation loss 0.06774123013019562 Accuracy 0.8339999914169312\n",
      "Iteration 9390 Training loss 0.05630996450781822 Validation loss 0.05738489702343941 Accuracy 0.8554999828338623\n",
      "Iteration 9400 Training loss 0.06037977710366249 Validation loss 0.06669099628925323 Accuracy 0.8379999995231628\n",
      "Iteration 9410 Training loss 0.058990754187107086 Validation loss 0.06764990836381912 Accuracy 0.8364999890327454\n",
      "Iteration 9420 Training loss 0.05821695178747177 Validation loss 0.062306199222803116 Accuracy 0.8464999794960022\n",
      "Iteration 9430 Training loss 0.05973038077354431 Validation loss 0.06653329730033875 Accuracy 0.8374999761581421\n",
      "Iteration 9440 Training loss 0.06036435067653656 Validation loss 0.06488914042711258 Accuracy 0.8389999866485596\n",
      "Iteration 9450 Training loss 0.06334585696458817 Validation loss 0.06592563539743423 Accuracy 0.8399999737739563\n",
      "Iteration 9460 Training loss 0.058286648243665695 Validation loss 0.06343510001897812 Accuracy 0.843999981880188\n",
      "Iteration 9470 Training loss 0.05276184529066086 Validation loss 0.05914095789194107 Accuracy 0.8525000214576721\n",
      "Iteration 9480 Training loss 0.054242853075265884 Validation loss 0.059451185166835785 Accuracy 0.8514999747276306\n",
      "Iteration 9490 Training loss 0.05944531410932541 Validation loss 0.0617271289229393 Accuracy 0.8454999923706055\n",
      "Iteration 9500 Training loss 0.061077967286109924 Validation loss 0.06418828666210175 Accuracy 0.8420000076293945\n",
      "Iteration 9510 Training loss 0.05528821423649788 Validation loss 0.06185733526945114 Accuracy 0.8464999794960022\n",
      "Iteration 9520 Training loss 0.05822295323014259 Validation loss 0.06268415600061417 Accuracy 0.8454999923706055\n",
      "Iteration 9530 Training loss 0.056918345391750336 Validation loss 0.061031099408864975 Accuracy 0.8504999876022339\n",
      "Iteration 9540 Training loss 0.05355696752667427 Validation loss 0.058153923600912094 Accuracy 0.8539999723434448\n",
      "Iteration 9550 Training loss 0.06365738064050674 Validation loss 0.06758293509483337 Accuracy 0.8379999995231628\n",
      "Iteration 9560 Training loss 0.05375969037413597 Validation loss 0.05718056112527847 Accuracy 0.8585000038146973\n",
      "Iteration 9570 Training loss 0.0661076232790947 Validation loss 0.06737140566110611 Accuracy 0.8349999785423279\n",
      "Iteration 9580 Training loss 0.0628233253955841 Validation loss 0.06855795532464981 Accuracy 0.8309999704360962\n",
      "Iteration 9590 Training loss 0.05931994691491127 Validation loss 0.0631774514913559 Accuracy 0.843999981880188\n",
      "Iteration 9600 Training loss 0.06326517462730408 Validation loss 0.06704225391149521 Accuracy 0.8370000123977661\n",
      "Iteration 9610 Training loss 0.053664229810237885 Validation loss 0.060765888541936874 Accuracy 0.8495000004768372\n",
      "Iteration 9620 Training loss 0.05743136256933212 Validation loss 0.06077316403388977 Accuracy 0.8504999876022339\n",
      "Iteration 9630 Training loss 0.055186476558446884 Validation loss 0.06388217955827713 Accuracy 0.8429999947547913\n",
      "Iteration 9640 Training loss 0.0685526430606842 Validation loss 0.0771341621875763 Accuracy 0.8090000152587891\n",
      "Iteration 9650 Training loss 0.06361714005470276 Validation loss 0.06578608602285385 Accuracy 0.8395000100135803\n",
      "Iteration 9660 Training loss 0.05252009257674217 Validation loss 0.05369986966252327 Accuracy 0.8644999861717224\n",
      "Iteration 9670 Training loss 0.056987736374139786 Validation loss 0.060683928430080414 Accuracy 0.8504999876022339\n",
      "Iteration 9680 Training loss 0.05153091251850128 Validation loss 0.05349970981478691 Accuracy 0.8665000200271606\n",
      "Iteration 9690 Training loss 0.05973236635327339 Validation loss 0.06720628589391708 Accuracy 0.8374999761581421\n",
      "Iteration 9700 Training loss 0.0598708875477314 Validation loss 0.06229158118367195 Accuracy 0.8450000286102295\n",
      "Iteration 9710 Training loss 0.06407522410154343 Validation loss 0.06488646566867828 Accuracy 0.8420000076293945\n",
      "Iteration 9720 Training loss 0.062390655279159546 Validation loss 0.06381768733263016 Accuracy 0.8429999947547913\n",
      "Iteration 9730 Training loss 0.05689564347267151 Validation loss 0.06019481644034386 Accuracy 0.8504999876022339\n",
      "Iteration 9740 Training loss 0.05991413816809654 Validation loss 0.06301991641521454 Accuracy 0.843999981880188\n",
      "Iteration 9750 Training loss 0.05787210166454315 Validation loss 0.05980195850133896 Accuracy 0.8514999747276306\n",
      "Iteration 9760 Training loss 0.058356188237667084 Validation loss 0.06180872395634651 Accuracy 0.847000002861023\n",
      "Iteration 9770 Training loss 0.05741484835743904 Validation loss 0.060877393931150436 Accuracy 0.8500000238418579\n",
      "Iteration 9780 Training loss 0.05613880604505539 Validation loss 0.06084143742918968 Accuracy 0.8495000004768372\n",
      "Iteration 9790 Training loss 0.06437188386917114 Validation loss 0.07118051499128342 Accuracy 0.8255000114440918\n",
      "Iteration 9800 Training loss 0.062074266374111176 Validation loss 0.06641373783349991 Accuracy 0.8385000228881836\n",
      "Iteration 9810 Training loss 0.0619143471121788 Validation loss 0.06799496710300446 Accuracy 0.8335000276565552\n",
      "Iteration 9820 Training loss 0.05524710193276405 Validation loss 0.054427262395620346 Accuracy 0.8665000200271606\n",
      "Iteration 9830 Training loss 0.06478886306285858 Validation loss 0.06733986735343933 Accuracy 0.8330000042915344\n",
      "Iteration 9840 Training loss 0.06063726916909218 Validation loss 0.06601416319608688 Accuracy 0.8374999761581421\n",
      "Iteration 9850 Training loss 0.05047011002898216 Validation loss 0.05185013636946678 Accuracy 0.8679999709129333\n",
      "Iteration 9860 Training loss 0.0673704519867897 Validation loss 0.0728486105799675 Accuracy 0.824999988079071\n",
      "Iteration 9870 Training loss 0.0648815706372261 Validation loss 0.07147523015737534 Accuracy 0.828499972820282\n",
      "Iteration 9880 Training loss 0.061451446264982224 Validation loss 0.06305097043514252 Accuracy 0.8450000286102295\n",
      "Iteration 9890 Training loss 0.0501372292637825 Validation loss 0.05744745582342148 Accuracy 0.8560000061988831\n",
      "Iteration 9900 Training loss 0.05080021917819977 Validation loss 0.0618254616856575 Accuracy 0.8475000262260437\n",
      "Iteration 9910 Training loss 0.05320150777697563 Validation loss 0.05446840822696686 Accuracy 0.8615000247955322\n",
      "Iteration 9920 Training loss 0.0682467371225357 Validation loss 0.06024152413010597 Accuracy 0.8475000262260437\n",
      "Iteration 9930 Training loss 0.060022734105587006 Validation loss 0.052596528083086014 Accuracy 0.8665000200271606\n",
      "Iteration 9940 Training loss 0.06587833911180496 Validation loss 0.05755769833922386 Accuracy 0.8529999852180481\n",
      "Iteration 9950 Training loss 0.05917768552899361 Validation loss 0.051675789058208466 Accuracy 0.8684999942779541\n",
      "Iteration 9960 Training loss 0.05904954671859741 Validation loss 0.050971925258636475 Accuracy 0.8725000023841858\n",
      "Iteration 9970 Training loss 0.0588466078042984 Validation loss 0.05089966580271721 Accuracy 0.8725000023841858\n",
      "Iteration 9980 Training loss 0.05981719121336937 Validation loss 0.05227673798799515 Accuracy 0.8679999709129333\n",
      "Iteration 9990 Training loss 0.04924136400222778 Validation loss 0.05055660381913185 Accuracy 0.8700000047683716\n",
      "Iteration 10000 Training loss 0.05686410516500473 Validation loss 0.052561722695827484 Accuracy 0.8690000176429749\n",
      "Iteration 10010 Training loss 0.06038966774940491 Validation loss 0.05439700186252594 Accuracy 0.859000027179718\n",
      "Iteration 10020 Training loss 0.05733266472816467 Validation loss 0.05311914160847664 Accuracy 0.8675000071525574\n",
      "Iteration 10030 Training loss 0.05747975781559944 Validation loss 0.053291987627744675 Accuracy 0.8644999861717224\n",
      "Iteration 10040 Training loss 0.06025709584355354 Validation loss 0.05185395106673241 Accuracy 0.8684999942779541\n",
      "Iteration 10050 Training loss 0.05629672482609749 Validation loss 0.05247111618518829 Accuracy 0.8679999709129333\n",
      "Iteration 10060 Training loss 0.05616477131843567 Validation loss 0.052824657410383224 Accuracy 0.8650000095367432\n",
      "Iteration 10070 Training loss 0.057534974068403244 Validation loss 0.05123288929462433 Accuracy 0.8725000023841858\n",
      "Iteration 10080 Training loss 0.06111668422818184 Validation loss 0.05567746236920357 Accuracy 0.8544999957084656\n",
      "Iteration 10090 Training loss 0.055694472044706345 Validation loss 0.05111033096909523 Accuracy 0.8709999918937683\n",
      "Iteration 10100 Training loss 0.05621744692325592 Validation loss 0.052021048963069916 Accuracy 0.8679999709129333\n",
      "Iteration 10110 Training loss 0.057857804000377655 Validation loss 0.054011087864637375 Accuracy 0.8634999990463257\n",
      "Iteration 10120 Training loss 0.061859287321567535 Validation loss 0.054331887513399124 Accuracy 0.8619999885559082\n",
      "Iteration 10130 Training loss 0.05711459368467331 Validation loss 0.05262283235788345 Accuracy 0.8659999966621399\n",
      "Iteration 10140 Training loss 0.057766128331422806 Validation loss 0.05068114772439003 Accuracy 0.8694999814033508\n",
      "Iteration 10150 Training loss 0.06157619506120682 Validation loss 0.05272084102034569 Accuracy 0.8679999709129333\n",
      "Iteration 10160 Training loss 0.0574132464826107 Validation loss 0.051380135118961334 Accuracy 0.8705000281333923\n",
      "Iteration 10170 Training loss 0.05499832332134247 Validation loss 0.050742216408252716 Accuracy 0.8715000152587891\n",
      "Iteration 10180 Training loss 0.058715641498565674 Validation loss 0.05257238820195198 Accuracy 0.8675000071525574\n",
      "Iteration 10190 Training loss 0.05464579164981842 Validation loss 0.05115259438753128 Accuracy 0.8709999918937683\n",
      "Iteration 10200 Training loss 0.056601665914058685 Validation loss 0.04985548555850983 Accuracy 0.8705000281333923\n",
      "Iteration 10210 Training loss 0.06113177165389061 Validation loss 0.05196462199091911 Accuracy 0.8684999942779541\n",
      "Iteration 10220 Training loss 0.059752076864242554 Validation loss 0.05228966474533081 Accuracy 0.8675000071525574\n",
      "Iteration 10230 Training loss 0.05631357803940773 Validation loss 0.05335650593042374 Accuracy 0.8644999861717224\n",
      "Iteration 10240 Training loss 0.058130234479904175 Validation loss 0.05265342816710472 Accuracy 0.8675000071525574\n",
      "Iteration 10250 Training loss 0.053271401673555374 Validation loss 0.05052812397480011 Accuracy 0.8715000152587891\n",
      "Iteration 10260 Training loss 0.05294406786561012 Validation loss 0.051566462963819504 Accuracy 0.8725000023841858\n",
      "Iteration 10270 Training loss 0.06633554399013519 Validation loss 0.05571041256189346 Accuracy 0.859000027179718\n",
      "Iteration 10280 Training loss 0.0551794208586216 Validation loss 0.050646062940359116 Accuracy 0.8715000152587891\n",
      "Iteration 10290 Training loss 0.060936395078897476 Validation loss 0.050932627171278 Accuracy 0.8684999942779541\n",
      "Iteration 10300 Training loss 0.05576404929161072 Validation loss 0.05109262838959694 Accuracy 0.8694999814033508\n",
      "Iteration 10310 Training loss 0.054762568324804306 Validation loss 0.05263207107782364 Accuracy 0.8679999709129333\n",
      "Iteration 10320 Training loss 0.06195012480020523 Validation loss 0.05422547832131386 Accuracy 0.8644999861717224\n",
      "Iteration 10330 Training loss 0.06723779439926147 Validation loss 0.05678710341453552 Accuracy 0.8535000085830688\n",
      "Iteration 10340 Training loss 0.05748552083969116 Validation loss 0.05113695561885834 Accuracy 0.871999979019165\n",
      "Iteration 10350 Training loss 0.055145591497421265 Validation loss 0.05068136379122734 Accuracy 0.8684999942779541\n",
      "Iteration 10360 Training loss 0.05901498347520828 Validation loss 0.05237406864762306 Accuracy 0.8679999709129333\n",
      "Iteration 10370 Training loss 0.06181301921606064 Validation loss 0.05254019796848297 Accuracy 0.8675000071525574\n",
      "Iteration 10380 Training loss 0.05570877715945244 Validation loss 0.05095173791050911 Accuracy 0.871999979019165\n",
      "Iteration 10390 Training loss 0.05347549542784691 Validation loss 0.051247488707304 Accuracy 0.8715000152587891\n",
      "Iteration 10400 Training loss 0.06575197726488113 Validation loss 0.055580198764801025 Accuracy 0.8579999804496765\n",
      "Iteration 10410 Training loss 0.05837172269821167 Validation loss 0.05086173117160797 Accuracy 0.8684999942779541\n",
      "Iteration 10420 Training loss 0.05887963995337486 Validation loss 0.05363285914063454 Accuracy 0.8629999756813049\n",
      "Iteration 10430 Training loss 0.06388840079307556 Validation loss 0.05412759631872177 Accuracy 0.8619999885559082\n",
      "Iteration 10440 Training loss 0.06203409656882286 Validation loss 0.05450626462697983 Accuracy 0.859000027179718\n",
      "Iteration 10450 Training loss 0.05804109945893288 Validation loss 0.05133399739861488 Accuracy 0.8694999814033508\n",
      "Iteration 10460 Training loss 0.059299297630786896 Validation loss 0.052059073001146317 Accuracy 0.8684999942779541\n",
      "Iteration 10470 Training loss 0.055636655539274216 Validation loss 0.052967533469200134 Accuracy 0.8669999837875366\n",
      "Iteration 10480 Training loss 0.06851783394813538 Validation loss 0.057482898235321045 Accuracy 0.8519999980926514\n",
      "Iteration 10490 Training loss 0.05373392626643181 Validation loss 0.05226695165038109 Accuracy 0.8690000176429749\n",
      "Iteration 10500 Training loss 0.057895053178071976 Validation loss 0.05221598967909813 Accuracy 0.8679999709129333\n",
      "Iteration 10510 Training loss 0.059860408306121826 Validation loss 0.05427170544862747 Accuracy 0.8600000143051147\n",
      "Iteration 10520 Training loss 0.05999311804771423 Validation loss 0.05258582904934883 Accuracy 0.8650000095367432\n",
      "Iteration 10530 Training loss 0.0647391825914383 Validation loss 0.05552396923303604 Accuracy 0.8575000166893005\n",
      "Iteration 10540 Training loss 0.057609107345342636 Validation loss 0.051421064883470535 Accuracy 0.8694999814033508\n",
      "Iteration 10550 Training loss 0.06612451374530792 Validation loss 0.05505972355604172 Accuracy 0.8610000014305115\n",
      "Iteration 10560 Training loss 0.05655740201473236 Validation loss 0.05164012312889099 Accuracy 0.8709999918937683\n",
      "Iteration 10570 Training loss 0.0650811567902565 Validation loss 0.05650108680129051 Accuracy 0.8560000061988831\n",
      "Iteration 10580 Training loss 0.062099434435367584 Validation loss 0.054496001452207565 Accuracy 0.8600000143051147\n",
      "Iteration 10590 Training loss 0.06129591539502144 Validation loss 0.05430403724312782 Accuracy 0.8604999780654907\n",
      "Iteration 10600 Training loss 0.058523308485746384 Validation loss 0.053529251366853714 Accuracy 0.8629999756813049\n",
      "Iteration 10610 Training loss 0.05997855216264725 Validation loss 0.05307546630501747 Accuracy 0.8665000200271606\n",
      "Iteration 10620 Training loss 0.06006314605474472 Validation loss 0.05155724287033081 Accuracy 0.8700000047683716\n",
      "Iteration 10630 Training loss 0.061797402799129486 Validation loss 0.05302178114652634 Accuracy 0.8659999966621399\n",
      "Iteration 10640 Training loss 0.061583854258060455 Validation loss 0.054738376289606094 Accuracy 0.8615000247955322\n",
      "Iteration 10650 Training loss 0.06158638000488281 Validation loss 0.05480260029435158 Accuracy 0.8600000143051147\n",
      "Iteration 10660 Training loss 0.06485915184020996 Validation loss 0.055411163717508316 Accuracy 0.8604999780654907\n",
      "Iteration 10670 Training loss 0.06526322662830353 Validation loss 0.056291937828063965 Accuracy 0.8565000295639038\n",
      "Iteration 10680 Training loss 0.057010337710380554 Validation loss 0.051555417478084564 Accuracy 0.8690000176429749\n",
      "Iteration 10690 Training loss 0.058330193161964417 Validation loss 0.05214648321270943 Accuracy 0.8669999837875366\n",
      "Iteration 10700 Training loss 0.06197422742843628 Validation loss 0.053903382271528244 Accuracy 0.8659999966621399\n",
      "Iteration 10710 Training loss 0.07238104939460754 Validation loss 0.05610088258981705 Accuracy 0.8569999933242798\n",
      "Iteration 10720 Training loss 0.05475713312625885 Validation loss 0.05121821537613869 Accuracy 0.8715000152587891\n",
      "Iteration 10730 Training loss 0.05901864543557167 Validation loss 0.05372866988182068 Accuracy 0.8650000095367432\n",
      "Iteration 10740 Training loss 0.05653548985719681 Validation loss 0.05129866302013397 Accuracy 0.8705000281333923\n",
      "Iteration 10750 Training loss 0.06483374536037445 Validation loss 0.05466418340802193 Accuracy 0.8619999885559082\n",
      "Iteration 10760 Training loss 0.061133552342653275 Validation loss 0.052092138677835464 Accuracy 0.8669999837875366\n",
      "Iteration 10770 Training loss 0.05420902743935585 Validation loss 0.051987942308187485 Accuracy 0.8669999837875366\n",
      "Iteration 10780 Training loss 0.059024933725595474 Validation loss 0.05323177948594093 Accuracy 0.8659999966621399\n",
      "Iteration 10790 Training loss 0.057965610176324844 Validation loss 0.05229944735765457 Accuracy 0.8650000095367432\n",
      "Iteration 10800 Training loss 0.06153614819049835 Validation loss 0.05360673740506172 Accuracy 0.8634999990463257\n",
      "Iteration 10810 Training loss 0.0573471337556839 Validation loss 0.05161600559949875 Accuracy 0.8694999814033508\n",
      "Iteration 10820 Training loss 0.058599747717380524 Validation loss 0.05117617920041084 Accuracy 0.8679999709129333\n",
      "Iteration 10830 Training loss 0.06630167365074158 Validation loss 0.056441038846969604 Accuracy 0.8544999957084656\n",
      "Iteration 10840 Training loss 0.05649204179644585 Validation loss 0.05152717977762222 Accuracy 0.8684999942779541\n",
      "Iteration 10850 Training loss 0.060346320271492004 Validation loss 0.05180060490965843 Accuracy 0.8694999814033508\n",
      "Iteration 10860 Training loss 0.05884662643074989 Validation loss 0.055649612098932266 Accuracy 0.8600000143051147\n",
      "Iteration 10870 Training loss 0.06346423178911209 Validation loss 0.055678244680166245 Accuracy 0.859000027179718\n",
      "Iteration 10880 Training loss 0.052745699882507324 Validation loss 0.05114792659878731 Accuracy 0.8694999814033508\n",
      "Iteration 10890 Training loss 0.05969148129224777 Validation loss 0.051959577947854996 Accuracy 0.8669999837875366\n",
      "Iteration 10900 Training loss 0.05959848314523697 Validation loss 0.05413332208991051 Accuracy 0.8629999756813049\n",
      "Iteration 10910 Training loss 0.05803016200661659 Validation loss 0.05321505665779114 Accuracy 0.8644999861717224\n",
      "Iteration 10920 Training loss 0.05532095208764076 Validation loss 0.05092403292655945 Accuracy 0.8705000281333923\n",
      "Iteration 10930 Training loss 0.06667055934667587 Validation loss 0.061177413910627365 Accuracy 0.8495000004768372\n",
      "Iteration 10940 Training loss 0.05941871553659439 Validation loss 0.0528193935751915 Accuracy 0.8669999837875366\n",
      "Iteration 10950 Training loss 0.06956083327531815 Validation loss 0.05486930534243584 Accuracy 0.8585000038146973\n",
      "Iteration 10960 Training loss 0.06054475158452988 Validation loss 0.054218269884586334 Accuracy 0.8619999885559082\n",
      "Iteration 10970 Training loss 0.061615996062755585 Validation loss 0.05292977765202522 Accuracy 0.8654999732971191\n",
      "Iteration 10980 Training loss 0.05318669229745865 Validation loss 0.05124298483133316 Accuracy 0.8694999814033508\n",
      "Iteration 10990 Training loss 0.05458729714155197 Validation loss 0.05166679248213768 Accuracy 0.8694999814033508\n",
      "Iteration 11000 Training loss 0.05673094093799591 Validation loss 0.051929064095020294 Accuracy 0.8705000281333923\n",
      "Iteration 11010 Training loss 0.062275972217321396 Validation loss 0.05465743690729141 Accuracy 0.8629999756813049\n",
      "Iteration 11020 Training loss 0.053774215281009674 Validation loss 0.05149272084236145 Accuracy 0.8705000281333923\n",
      "Iteration 11030 Training loss 0.05719465762376785 Validation loss 0.051626402884721756 Accuracy 0.8725000023841858\n",
      "Iteration 11040 Training loss 0.05996719375252724 Validation loss 0.054568640887737274 Accuracy 0.8610000014305115\n",
      "Iteration 11050 Training loss 0.06147246062755585 Validation loss 0.052925918251276016 Accuracy 0.8640000224113464\n",
      "Iteration 11060 Training loss 0.05649779736995697 Validation loss 0.05055335909128189 Accuracy 0.8684999942779541\n",
      "Iteration 11070 Training loss 0.06402058899402618 Validation loss 0.05290624126791954 Accuracy 0.8675000071525574\n",
      "Iteration 11080 Training loss 0.05576391890645027 Validation loss 0.05140051245689392 Accuracy 0.8690000176429749\n",
      "Iteration 11090 Training loss 0.0559237040579319 Validation loss 0.05357791483402252 Accuracy 0.8665000200271606\n",
      "Iteration 11100 Training loss 0.06433068960905075 Validation loss 0.054339803755283356 Accuracy 0.862500011920929\n",
      "Iteration 11110 Training loss 0.06793474406003952 Validation loss 0.058927129954099655 Accuracy 0.8529999852180481\n",
      "Iteration 11120 Training loss 0.05774082988500595 Validation loss 0.05258813500404358 Accuracy 0.8650000095367432\n",
      "Iteration 11130 Training loss 0.05647625774145126 Validation loss 0.051912352442741394 Accuracy 0.8669999837875366\n",
      "Iteration 11140 Training loss 0.06513538211584091 Validation loss 0.055965542793273926 Accuracy 0.8579999804496765\n",
      "Iteration 11150 Training loss 0.054651565849781036 Validation loss 0.05111003667116165 Accuracy 0.8690000176429749\n",
      "Iteration 11160 Training loss 0.06700129061937332 Validation loss 0.059323739260435104 Accuracy 0.8504999876022339\n",
      "Iteration 11170 Training loss 0.05672077089548111 Validation loss 0.051460668444633484 Accuracy 0.8705000281333923\n",
      "Iteration 11180 Training loss 0.05513948202133179 Validation loss 0.05208290368318558 Accuracy 0.8669999837875366\n",
      "Iteration 11190 Training loss 0.05860927700996399 Validation loss 0.05105459690093994 Accuracy 0.8690000176429749\n",
      "Iteration 11200 Training loss 0.0662594884634018 Validation loss 0.053877897560596466 Accuracy 0.8640000224113464\n",
      "Iteration 11210 Training loss 0.05414734035730362 Validation loss 0.05178393796086311 Accuracy 0.8690000176429749\n",
      "Iteration 11220 Training loss 0.05762773007154465 Validation loss 0.0542144849896431 Accuracy 0.8619999885559082\n",
      "Iteration 11230 Training loss 0.06593267619609833 Validation loss 0.059023041278123856 Accuracy 0.8535000085830688\n",
      "Iteration 11240 Training loss 0.06242390349507332 Validation loss 0.055554166436195374 Accuracy 0.8569999933242798\n",
      "Iteration 11250 Training loss 0.05799625813961029 Validation loss 0.051539551466703415 Accuracy 0.8700000047683716\n",
      "Iteration 11260 Training loss 0.05915716663002968 Validation loss 0.055082451552152634 Accuracy 0.859000027179718\n",
      "Iteration 11270 Training loss 0.06622320413589478 Validation loss 0.05644380301237106 Accuracy 0.8569999933242798\n",
      "Iteration 11280 Training loss 0.06542001664638519 Validation loss 0.054760102182626724 Accuracy 0.8615000247955322\n",
      "Iteration 11290 Training loss 0.06389770656824112 Validation loss 0.055518988519907 Accuracy 0.8569999933242798\n",
      "Iteration 11300 Training loss 0.059695519506931305 Validation loss 0.05114487558603287 Accuracy 0.8700000047683716\n",
      "Iteration 11310 Training loss 0.05850712209939957 Validation loss 0.05337606742978096 Accuracy 0.8650000095367432\n",
      "Iteration 11320 Training loss 0.06292933225631714 Validation loss 0.056723397225141525 Accuracy 0.8550000190734863\n",
      "Iteration 11330 Training loss 0.05431608855724335 Validation loss 0.051647286862134933 Accuracy 0.8675000071525574\n",
      "Iteration 11340 Training loss 0.06428682059049606 Validation loss 0.054651398211717606 Accuracy 0.8604999780654907\n",
      "Iteration 11350 Training loss 0.05414610728621483 Validation loss 0.052822623401880264 Accuracy 0.8659999966621399\n",
      "Iteration 11360 Training loss 0.06318409740924835 Validation loss 0.05621479079127312 Accuracy 0.8550000190734863\n",
      "Iteration 11370 Training loss 0.06147203594446182 Validation loss 0.05311733856797218 Accuracy 0.8640000224113464\n",
      "Iteration 11380 Training loss 0.05767068266868591 Validation loss 0.05097639560699463 Accuracy 0.8705000281333923\n",
      "Iteration 11390 Training loss 0.06197754293680191 Validation loss 0.05416104197502136 Accuracy 0.8619999885559082\n",
      "Iteration 11400 Training loss 0.0578329861164093 Validation loss 0.05165679007768631 Accuracy 0.8694999814033508\n",
      "Iteration 11410 Training loss 0.06272264569997787 Validation loss 0.054767806082963943 Accuracy 0.8585000038146973\n",
      "Iteration 11420 Training loss 0.0564698688685894 Validation loss 0.0516611710190773 Accuracy 0.8715000152587891\n",
      "Iteration 11430 Training loss 0.06578076630830765 Validation loss 0.05710220709443092 Accuracy 0.8535000085830688\n",
      "Iteration 11440 Training loss 0.05217365175485611 Validation loss 0.051962438970804214 Accuracy 0.8675000071525574\n",
      "Iteration 11450 Training loss 0.06161963567137718 Validation loss 0.06536523252725601 Accuracy 0.8385000228881836\n",
      "Iteration 11460 Training loss 0.056365642696619034 Validation loss 0.058967627584934235 Accuracy 0.8525000214576721\n",
      "Iteration 11470 Training loss 0.06008262559771538 Validation loss 0.06786328554153442 Accuracy 0.8345000147819519\n",
      "Iteration 11480 Training loss 0.059809040278196335 Validation loss 0.06735119968652725 Accuracy 0.8379999995231628\n",
      "Iteration 11490 Training loss 0.058195870369672775 Validation loss 0.06229023635387421 Accuracy 0.8445000052452087\n",
      "Iteration 11500 Training loss 0.05563466250896454 Validation loss 0.06039460003376007 Accuracy 0.8495000004768372\n",
      "Iteration 11510 Training loss 0.06449709832668304 Validation loss 0.06553231924772263 Accuracy 0.8385000228881836\n",
      "Iteration 11520 Training loss 0.05594921112060547 Validation loss 0.05972667038440704 Accuracy 0.8529999852180481\n",
      "Iteration 11530 Training loss 0.05308612063527107 Validation loss 0.059646546840667725 Accuracy 0.8489999771118164\n",
      "Iteration 11540 Training loss 0.05859925225377083 Validation loss 0.06266661733388901 Accuracy 0.8454999923706055\n",
      "Iteration 11550 Training loss 0.051324523985385895 Validation loss 0.05877268686890602 Accuracy 0.8560000061988831\n",
      "Iteration 11560 Training loss 0.05815182626247406 Validation loss 0.06252148747444153 Accuracy 0.8454999923706055\n",
      "Iteration 11570 Training loss 0.06229401379823685 Validation loss 0.06985202431678772 Accuracy 0.8264999985694885\n",
      "Iteration 11580 Training loss 0.05371430143713951 Validation loss 0.05920166149735451 Accuracy 0.8514999747276306\n",
      "Iteration 11590 Training loss 0.05712812766432762 Validation loss 0.05801953747868538 Accuracy 0.8569999933242798\n",
      "Iteration 11600 Training loss 0.06250302493572235 Validation loss 0.06617601960897446 Accuracy 0.8355000019073486\n",
      "Iteration 11610 Training loss 0.05935467407107353 Validation loss 0.06332434713840485 Accuracy 0.8450000286102295\n",
      "Iteration 11620 Training loss 0.05206235498189926 Validation loss 0.05822737142443657 Accuracy 0.8569999933242798\n",
      "Iteration 11630 Training loss 0.057266462594270706 Validation loss 0.06097323074936867 Accuracy 0.8464999794960022\n",
      "Iteration 11640 Training loss 0.0652681514620781 Validation loss 0.06678827106952667 Accuracy 0.8355000019073486\n",
      "Iteration 11650 Training loss 0.05533118546009064 Validation loss 0.05840479955077171 Accuracy 0.8535000085830688\n",
      "Iteration 11660 Training loss 0.05988635867834091 Validation loss 0.06509331613779068 Accuracy 0.8410000205039978\n",
      "Iteration 11670 Training loss 0.060866858810186386 Validation loss 0.06726337224245071 Accuracy 0.8364999890327454\n",
      "Iteration 11680 Training loss 0.05945993587374687 Validation loss 0.06216387078166008 Accuracy 0.8460000157356262\n",
      "Iteration 11690 Training loss 0.05891987308859825 Validation loss 0.06483784317970276 Accuracy 0.8410000205039978\n",
      "Iteration 11700 Training loss 0.06868895888328552 Validation loss 0.0730632022023201 Accuracy 0.8195000290870667\n",
      "Iteration 11710 Training loss 0.06306833028793335 Validation loss 0.06449571251869202 Accuracy 0.8424999713897705\n",
      "Iteration 11720 Training loss 0.05388667434453964 Validation loss 0.06251233071088791 Accuracy 0.8460000157356262\n",
      "Iteration 11730 Training loss 0.06094207987189293 Validation loss 0.06536336988210678 Accuracy 0.8399999737739563\n",
      "Iteration 11740 Training loss 0.056064385920763016 Validation loss 0.061809416860342026 Accuracy 0.8460000157356262\n",
      "Iteration 11750 Training loss 0.058287613093853 Validation loss 0.06458926200866699 Accuracy 0.8420000076293945\n",
      "Iteration 11760 Training loss 0.0579637847840786 Validation loss 0.06251085549592972 Accuracy 0.843999981880188\n",
      "Iteration 11770 Training loss 0.053472720086574554 Validation loss 0.054887671023607254 Accuracy 0.8634999990463257\n",
      "Iteration 11780 Training loss 0.050030868500471115 Validation loss 0.058568697422742844 Accuracy 0.8560000061988831\n",
      "Iteration 11790 Training loss 0.06641222536563873 Validation loss 0.06933096051216125 Accuracy 0.8309999704360962\n",
      "Iteration 11800 Training loss 0.058751679956912994 Validation loss 0.0653834193944931 Accuracy 0.8410000205039978\n",
      "Iteration 11810 Training loss 0.062023043632507324 Validation loss 0.06685512512922287 Accuracy 0.8364999890327454\n",
      "Iteration 11820 Training loss 0.0579429492354393 Validation loss 0.06047741696238518 Accuracy 0.8479999899864197\n",
      "Iteration 11830 Training loss 0.06223433092236519 Validation loss 0.06405987590551376 Accuracy 0.8420000076293945\n",
      "Iteration 11840 Training loss 0.05749427154660225 Validation loss 0.062251269817352295 Accuracy 0.8450000286102295\n",
      "Iteration 11850 Training loss 0.05421049892902374 Validation loss 0.057025350630283356 Accuracy 0.8579999804496765\n",
      "Iteration 11860 Training loss 0.06396529078483582 Validation loss 0.06686490774154663 Accuracy 0.8355000019073486\n",
      "Iteration 11870 Training loss 0.06860686838626862 Validation loss 0.07062865048646927 Accuracy 0.828000009059906\n",
      "Iteration 11880 Training loss 0.05979340523481369 Validation loss 0.05969204008579254 Accuracy 0.8510000109672546\n",
      "Iteration 11890 Training loss 0.07072699069976807 Validation loss 0.07337982207536697 Accuracy 0.824999988079071\n",
      "Iteration 11900 Training loss 0.05753462761640549 Validation loss 0.05849673971533775 Accuracy 0.8535000085830688\n",
      "Iteration 11910 Training loss 0.05508958548307419 Validation loss 0.06175888702273369 Accuracy 0.8489999771118164\n",
      "Iteration 11920 Training loss 0.06106714531779289 Validation loss 0.06557652354240417 Accuracy 0.840499997138977\n",
      "Iteration 11930 Training loss 0.05597760155797005 Validation loss 0.0639299750328064 Accuracy 0.8445000052452087\n",
      "Iteration 11940 Training loss 0.05482560023665428 Validation loss 0.059897035360336304 Accuracy 0.8489999771118164\n",
      "Iteration 11950 Training loss 0.06690920889377594 Validation loss 0.06839697808027267 Accuracy 0.8330000042915344\n",
      "Iteration 11960 Training loss 0.06248776987195015 Validation loss 0.06501142680644989 Accuracy 0.8420000076293945\n",
      "Iteration 11970 Training loss 0.05347563698887825 Validation loss 0.05817612633109093 Accuracy 0.8554999828338623\n",
      "Iteration 11980 Training loss 0.05918537452816963 Validation loss 0.06694549322128296 Accuracy 0.8339999914169312\n",
      "Iteration 11990 Training loss 0.06039145588874817 Validation loss 0.06549692898988724 Accuracy 0.8389999866485596\n",
      "Iteration 12000 Training loss 0.06412482261657715 Validation loss 0.06794565171003342 Accuracy 0.8324999809265137\n",
      "Iteration 12010 Training loss 0.057559769600629807 Validation loss 0.05899907648563385 Accuracy 0.8550000190734863\n",
      "Iteration 12020 Training loss 0.0586438924074173 Validation loss 0.06375197321176529 Accuracy 0.843999981880188\n",
      "Iteration 12030 Training loss 0.06799973547458649 Validation loss 0.07301164418458939 Accuracy 0.8259999752044678\n",
      "Iteration 12040 Training loss 0.0668678879737854 Validation loss 0.06658553332090378 Accuracy 0.8364999890327454\n",
      "Iteration 12050 Training loss 0.057226646691560745 Validation loss 0.06028028577566147 Accuracy 0.8485000133514404\n",
      "Iteration 12060 Training loss 0.05650244653224945 Validation loss 0.05728141963481903 Accuracy 0.8554999828338623\n",
      "Iteration 12070 Training loss 0.05722826346755028 Validation loss 0.06301596015691757 Accuracy 0.8460000157356262\n",
      "Iteration 12080 Training loss 0.06575268507003784 Validation loss 0.07032972574234009 Accuracy 0.8314999938011169\n",
      "Iteration 12090 Training loss 0.06020144745707512 Validation loss 0.060861021280288696 Accuracy 0.8489999771118164\n",
      "Iteration 12100 Training loss 0.056529637426137924 Validation loss 0.05966395512223244 Accuracy 0.8500000238418579\n",
      "Iteration 12110 Training loss 0.05921214446425438 Validation loss 0.059646666049957275 Accuracy 0.8539999723434448\n",
      "Iteration 12120 Training loss 0.06131180375814438 Validation loss 0.06935981661081314 Accuracy 0.8305000066757202\n",
      "Iteration 12130 Training loss 0.061933014541864395 Validation loss 0.06512231379747391 Accuracy 0.8410000205039978\n",
      "Iteration 12140 Training loss 0.06495870649814606 Validation loss 0.06628301739692688 Accuracy 0.8364999890327454\n",
      "Iteration 12150 Training loss 0.05634775757789612 Validation loss 0.06113435700535774 Accuracy 0.847000002861023\n",
      "Iteration 12160 Training loss 0.0594937764108181 Validation loss 0.06909637153148651 Accuracy 0.8289999961853027\n",
      "Iteration 12170 Training loss 0.059487275779247284 Validation loss 0.06781159341335297 Accuracy 0.8330000042915344\n",
      "Iteration 12180 Training loss 0.05707767233252525 Validation loss 0.061790239065885544 Accuracy 0.8479999899864197\n",
      "Iteration 12190 Training loss 0.06594023108482361 Validation loss 0.06868288666009903 Accuracy 0.8320000171661377\n",
      "Iteration 12200 Training loss 0.05159766972064972 Validation loss 0.05740007013082504 Accuracy 0.8585000038146973\n",
      "Iteration 12210 Training loss 0.06226373836398125 Validation loss 0.06464142352342606 Accuracy 0.840499997138977\n",
      "Iteration 12220 Training loss 0.06428908556699753 Validation loss 0.07001945376396179 Accuracy 0.8309999704360962\n",
      "Iteration 12230 Training loss 0.05661308392882347 Validation loss 0.06025269627571106 Accuracy 0.8525000214576721\n",
      "Iteration 12240 Training loss 0.062468212097883224 Validation loss 0.06827978044748306 Accuracy 0.8345000147819519\n",
      "Iteration 12250 Training loss 0.06318870186805725 Validation loss 0.0652691200375557 Accuracy 0.8389999866485596\n",
      "Iteration 12260 Training loss 0.06082525849342346 Validation loss 0.06652384996414185 Accuracy 0.8360000252723694\n",
      "Iteration 12270 Training loss 0.05585097521543503 Validation loss 0.05902114510536194 Accuracy 0.8525000214576721\n",
      "Iteration 12280 Training loss 0.05249812453985214 Validation loss 0.05208709090948105 Accuracy 0.8700000047683716\n",
      "Iteration 12290 Training loss 0.06564170122146606 Validation loss 0.056986577808856964 Accuracy 0.8550000190734863\n",
      "Iteration 12300 Training loss 0.05600474029779434 Validation loss 0.051954541355371475 Accuracy 0.8669999837875366\n",
      "Iteration 12310 Training loss 0.06179135665297508 Validation loss 0.05395079031586647 Accuracy 0.8634999990463257\n",
      "Iteration 12320 Training loss 0.05988909676671028 Validation loss 0.0529194250702858 Accuracy 0.8675000071525574\n",
      "Iteration 12330 Training loss 0.0584854781627655 Validation loss 0.05403321236371994 Accuracy 0.8650000095367432\n",
      "Iteration 12340 Training loss 0.055319856852293015 Validation loss 0.05194409191608429 Accuracy 0.8654999732971191\n",
      "Iteration 12350 Training loss 0.06028595566749573 Validation loss 0.0545814149081707 Accuracy 0.8619999885559082\n",
      "Iteration 12360 Training loss 0.06012861803174019 Validation loss 0.05149208754301071 Accuracy 0.8679999709129333\n",
      "Iteration 12370 Training loss 0.06478840112686157 Validation loss 0.05692266672849655 Accuracy 0.8575000166893005\n",
      "Iteration 12380 Training loss 0.059510618448257446 Validation loss 0.054451536387205124 Accuracy 0.8604999780654907\n",
      "Iteration 12390 Training loss 0.06715691089630127 Validation loss 0.05875493213534355 Accuracy 0.8514999747276306\n",
      "Iteration 12400 Training loss 0.058530043810606 Validation loss 0.05292128771543503 Accuracy 0.8659999966621399\n",
      "Iteration 12410 Training loss 0.05920350179076195 Validation loss 0.0550270602107048 Accuracy 0.8619999885559082\n",
      "Iteration 12420 Training loss 0.06271561980247498 Validation loss 0.05264152213931084 Accuracy 0.8659999966621399\n",
      "Iteration 12430 Training loss 0.05596565082669258 Validation loss 0.05199597775936127 Accuracy 0.8650000095367432\n",
      "Iteration 12440 Training loss 0.06793659180402756 Validation loss 0.05845823138952255 Accuracy 0.8529999852180481\n",
      "Iteration 12450 Training loss 0.05637991428375244 Validation loss 0.051290448755025864 Accuracy 0.8684999942779541\n",
      "Iteration 12460 Training loss 0.06667286902666092 Validation loss 0.056918758898973465 Accuracy 0.8550000190734863\n",
      "Iteration 12470 Training loss 0.05559539049863815 Validation loss 0.051439810544252396 Accuracy 0.8669999837875366\n",
      "Iteration 12480 Training loss 0.06459622830152512 Validation loss 0.05680413171648979 Accuracy 0.8569999933242798\n",
      "Iteration 12490 Training loss 0.05843109264969826 Validation loss 0.0533348023891449 Accuracy 0.8640000224113464\n",
      "Iteration 12500 Training loss 0.05957356095314026 Validation loss 0.053571220487356186 Accuracy 0.8650000095367432\n",
      "Iteration 12510 Training loss 0.054423946887254715 Validation loss 0.05218380689620972 Accuracy 0.8650000095367432\n",
      "Iteration 12520 Training loss 0.057568687945604324 Validation loss 0.05184420570731163 Accuracy 0.8700000047683716\n",
      "Iteration 12530 Training loss 0.05893531069159508 Validation loss 0.05231885239481926 Accuracy 0.8654999732971191\n",
      "Iteration 12540 Training loss 0.061192650347948074 Validation loss 0.05392800271511078 Accuracy 0.8640000224113464\n",
      "Iteration 12550 Training loss 0.05697540566325188 Validation loss 0.05192729830741882 Accuracy 0.8665000200271606\n",
      "Iteration 12560 Training loss 0.060496993362903595 Validation loss 0.053913794457912445 Accuracy 0.8644999861717224\n",
      "Iteration 12570 Training loss 0.05394742265343666 Validation loss 0.052712928503751755 Accuracy 0.8650000095367432\n",
      "Iteration 12580 Training loss 0.059552598744630814 Validation loss 0.05287325009703636 Accuracy 0.8654999732971191\n",
      "Iteration 12590 Training loss 0.06487807631492615 Validation loss 0.05454250052571297 Accuracy 0.8619999885559082\n",
      "Iteration 12600 Training loss 0.06581883877515793 Validation loss 0.0547010637819767 Accuracy 0.859000027179718\n",
      "Iteration 12610 Training loss 0.057151105254888535 Validation loss 0.052689068019390106 Accuracy 0.8659999966621399\n",
      "Iteration 12620 Training loss 0.05740116164088249 Validation loss 0.052039679139852524 Accuracy 0.8659999966621399\n",
      "Iteration 12630 Training loss 0.06566420197486877 Validation loss 0.05478815734386444 Accuracy 0.8615000247955322\n",
      "Iteration 12640 Training loss 0.0536125972867012 Validation loss 0.05148177966475487 Accuracy 0.8659999966621399\n",
      "Iteration 12650 Training loss 0.06208148971199989 Validation loss 0.05539501830935478 Accuracy 0.8604999780654907\n",
      "Iteration 12660 Training loss 0.05505063012242317 Validation loss 0.052042048424482346 Accuracy 0.8665000200271606\n",
      "Iteration 12670 Training loss 0.06243282929062843 Validation loss 0.05664048716425896 Accuracy 0.8544999957084656\n",
      "Iteration 12680 Training loss 0.06145542114973068 Validation loss 0.054350320249795914 Accuracy 0.8629999756813049\n",
      "Iteration 12690 Training loss 0.05705012381076813 Validation loss 0.05213332921266556 Accuracy 0.8669999837875366\n",
      "Iteration 12700 Training loss 0.057371705770492554 Validation loss 0.052247170358896255 Accuracy 0.8665000200271606\n",
      "Iteration 12710 Training loss 0.06538834422826767 Validation loss 0.057446978986263275 Accuracy 0.8525000214576721\n",
      "Iteration 12720 Training loss 0.060951683670282364 Validation loss 0.05529659613966942 Accuracy 0.859000027179718\n",
      "Iteration 12730 Training loss 0.05887775495648384 Validation loss 0.052057500928640366 Accuracy 0.8644999861717224\n",
      "Iteration 12740 Training loss 0.06653039902448654 Validation loss 0.05649057403206825 Accuracy 0.8575000166893005\n",
      "Iteration 12750 Training loss 0.06105189397931099 Validation loss 0.055585481226444244 Accuracy 0.859499990940094\n",
      "Iteration 12760 Training loss 0.06720780581235886 Validation loss 0.05740669369697571 Accuracy 0.8565000295639038\n",
      "Iteration 12770 Training loss 0.06967127323150635 Validation loss 0.0572032704949379 Accuracy 0.8544999957084656\n",
      "Iteration 12780 Training loss 0.05167018994688988 Validation loss 0.051927704364061356 Accuracy 0.8665000200271606\n",
      "Iteration 12790 Training loss 0.07375252991914749 Validation loss 0.05992218106985092 Accuracy 0.8500000238418579\n",
      "Iteration 12800 Training loss 0.05718947574496269 Validation loss 0.052837271243333817 Accuracy 0.8640000224113464\n",
      "Iteration 12810 Training loss 0.06614601612091064 Validation loss 0.05419980362057686 Accuracy 0.8619999885559082\n",
      "Iteration 12820 Training loss 0.06429517269134521 Validation loss 0.057533957064151764 Accuracy 0.8544999957084656\n",
      "Iteration 12830 Training loss 0.05649494752287865 Validation loss 0.052195753902196884 Accuracy 0.862500011920929\n",
      "Iteration 12840 Training loss 0.06346045434474945 Validation loss 0.055098358541727066 Accuracy 0.8619999885559082\n",
      "Iteration 12850 Training loss 0.07407337427139282 Validation loss 0.061546534299850464 Accuracy 0.8460000157356262\n",
      "Iteration 12860 Training loss 0.05372265353798866 Validation loss 0.05390315130352974 Accuracy 0.8669999837875366\n",
      "Iteration 12870 Training loss 0.05425513535737991 Validation loss 0.052019353955984116 Accuracy 0.8675000071525574\n",
      "Iteration 12880 Training loss 0.06038772687315941 Validation loss 0.055097512900829315 Accuracy 0.8619999885559082\n",
      "Iteration 12890 Training loss 0.06293654441833496 Validation loss 0.05436818674206734 Accuracy 0.8644999861717224\n",
      "Iteration 12900 Training loss 0.06207453832030296 Validation loss 0.055044159293174744 Accuracy 0.8604999780654907\n",
      "Iteration 12910 Training loss 0.06034271791577339 Validation loss 0.054373160004615784 Accuracy 0.8650000095367432\n",
      "Iteration 12920 Training loss 0.06428626924753189 Validation loss 0.05478521063923836 Accuracy 0.8619999885559082\n",
      "Iteration 12930 Training loss 0.06044314056634903 Validation loss 0.05380532145500183 Accuracy 0.8629999756813049\n",
      "Iteration 12940 Training loss 0.059913937002420425 Validation loss 0.052572719752788544 Accuracy 0.8640000224113464\n",
      "Iteration 12950 Training loss 0.05739179253578186 Validation loss 0.053510069847106934 Accuracy 0.862500011920929\n",
      "Iteration 12960 Training loss 0.05704322084784508 Validation loss 0.05282273143529892 Accuracy 0.8679999709129333\n",
      "Iteration 12970 Training loss 0.06148926541209221 Validation loss 0.05572700873017311 Accuracy 0.859499990940094\n",
      "Iteration 12980 Training loss 0.05463125556707382 Validation loss 0.052545592188835144 Accuracy 0.8659999966621399\n",
      "Iteration 12990 Training loss 0.06853795051574707 Validation loss 0.05612293630838394 Accuracy 0.8579999804496765\n",
      "Iteration 13000 Training loss 0.05756432190537453 Validation loss 0.051689133048057556 Accuracy 0.8669999837875366\n",
      "Iteration 13010 Training loss 0.05741548165678978 Validation loss 0.05229943245649338 Accuracy 0.8679999709129333\n",
      "Iteration 13020 Training loss 0.05892685428261757 Validation loss 0.05382203310728073 Accuracy 0.8650000095367432\n",
      "Iteration 13030 Training loss 0.06659474223852158 Validation loss 0.05538878217339516 Accuracy 0.8615000247955322\n",
      "Iteration 13040 Training loss 0.06110893189907074 Validation loss 0.056287482380867004 Accuracy 0.8575000166893005\n",
      "Iteration 13050 Training loss 0.05768688768148422 Validation loss 0.05454862490296364 Accuracy 0.8640000224113464\n",
      "Iteration 13060 Training loss 0.05893990397453308 Validation loss 0.05271097645163536 Accuracy 0.8644999861717224\n",
      "Iteration 13070 Training loss 0.07199677079916 Validation loss 0.06034126877784729 Accuracy 0.8504999876022339\n",
      "Iteration 13080 Training loss 0.05446822941303253 Validation loss 0.05284435302019119 Accuracy 0.8669999837875366\n",
      "Iteration 13090 Training loss 0.05667886883020401 Validation loss 0.054216720163822174 Accuracy 0.8629999756813049\n",
      "Iteration 13100 Training loss 0.0561683364212513 Validation loss 0.052920274436473846 Accuracy 0.8615000247955322\n",
      "Iteration 13110 Training loss 0.06530202180147171 Validation loss 0.05399936065077782 Accuracy 0.8640000224113464\n",
      "Iteration 13120 Training loss 0.05663469806313515 Validation loss 0.05216198414564133 Accuracy 0.8665000200271606\n",
      "Iteration 13130 Training loss 0.0625913068652153 Validation loss 0.05300750583410263 Accuracy 0.8629999756813049\n",
      "Iteration 13140 Training loss 0.059952300041913986 Validation loss 0.05273072049021721 Accuracy 0.8644999861717224\n",
      "Iteration 13150 Training loss 0.05734475702047348 Validation loss 0.05267927423119545 Accuracy 0.8669999837875366\n",
      "Iteration 13160 Training loss 0.05781881883740425 Validation loss 0.053389713168144226 Accuracy 0.8629999756813049\n",
      "Iteration 13170 Training loss 0.05711410939693451 Validation loss 0.054207514971494675 Accuracy 0.8610000014305115\n",
      "Iteration 13180 Training loss 0.06540517508983612 Validation loss 0.05572376772761345 Accuracy 0.8579999804496765\n",
      "Iteration 13190 Training loss 0.059521134942770004 Validation loss 0.05306126922369003 Accuracy 0.8644999861717224\n",
      "Iteration 13200 Training loss 0.061066221445798874 Validation loss 0.05450085178017616 Accuracy 0.859499990940094\n",
      "Iteration 13210 Training loss 0.057693641632795334 Validation loss 0.05234701931476593 Accuracy 0.8659999966621399\n",
      "Iteration 13220 Training loss 0.06292218714952469 Validation loss 0.055190835148096085 Accuracy 0.859499990940094\n",
      "Iteration 13230 Training loss 0.06044624373316765 Validation loss 0.05222531780600548 Accuracy 0.8650000095367432\n",
      "Iteration 13240 Training loss 0.06797462701797485 Validation loss 0.057414330542087555 Accuracy 0.8565000295639038\n",
      "Iteration 13250 Training loss 0.06321924924850464 Validation loss 0.05344807356595993 Accuracy 0.8610000014305115\n",
      "Iteration 13260 Training loss 0.05380551144480705 Validation loss 0.05224165320396423 Accuracy 0.8629999756813049\n",
      "Iteration 13270 Training loss 0.07042937725782394 Validation loss 0.06020931899547577 Accuracy 0.8504999876022339\n",
      "Iteration 13280 Training loss 0.05598127841949463 Validation loss 0.054094962775707245 Accuracy 0.8634999990463257\n",
      "Iteration 13290 Training loss 0.05738289654254913 Validation loss 0.05264442414045334 Accuracy 0.8654999732971191\n",
      "Iteration 13300 Training loss 0.05699140578508377 Validation loss 0.05332142487168312 Accuracy 0.8615000247955322\n",
      "Iteration 13310 Training loss 0.06609231233596802 Validation loss 0.05480784550309181 Accuracy 0.8610000014305115\n",
      "Iteration 13320 Training loss 0.056579094380140305 Validation loss 0.052576370537281036 Accuracy 0.8654999732971191\n",
      "Iteration 13330 Training loss 0.060578279197216034 Validation loss 0.05340771749615669 Accuracy 0.8634999990463257\n",
      "Iteration 13340 Training loss 0.06088688597083092 Validation loss 0.05471020191907883 Accuracy 0.862500011920929\n",
      "Iteration 13350 Training loss 0.05943680554628372 Validation loss 0.05329771339893341 Accuracy 0.8644999861717224\n",
      "Iteration 13360 Training loss 0.05995936691761017 Validation loss 0.05382506176829338 Accuracy 0.8640000224113464\n",
      "Iteration 13370 Training loss 0.0564114935696125 Validation loss 0.05288010835647583 Accuracy 0.8650000095367432\n",
      "Iteration 13380 Training loss 0.05729405954480171 Validation loss 0.05530836433172226 Accuracy 0.8610000014305115\n",
      "Iteration 13390 Training loss 0.06199217960238457 Validation loss 0.055472858250141144 Accuracy 0.859499990940094\n",
      "Iteration 13400 Training loss 0.06034093722701073 Validation loss 0.054966457188129425 Accuracy 0.8640000224113464\n",
      "Iteration 13410 Training loss 0.05668654665350914 Validation loss 0.053366705775260925 Accuracy 0.8640000224113464\n",
      "Iteration 13420 Training loss 0.06081996113061905 Validation loss 0.05325492098927498 Accuracy 0.8640000224113464\n",
      "Iteration 13430 Training loss 0.056857794523239136 Validation loss 0.052676912397146225 Accuracy 0.8675000071525574\n",
      "Iteration 13440 Training loss 0.06941331177949905 Validation loss 0.057346828281879425 Accuracy 0.8554999828338623\n",
      "Iteration 13450 Training loss 0.0673748329281807 Validation loss 0.057285062968730927 Accuracy 0.8554999828338623\n",
      "Iteration 13460 Training loss 0.056331656873226166 Validation loss 0.0529959537088871 Accuracy 0.8629999756813049\n",
      "Iteration 13470 Training loss 0.06853790581226349 Validation loss 0.05566558986902237 Accuracy 0.8575000166893005\n",
      "Iteration 13480 Training loss 0.06305030733346939 Validation loss 0.0545600950717926 Accuracy 0.862500011920929\n",
      "Iteration 13490 Training loss 0.06573158502578735 Validation loss 0.05461743474006653 Accuracy 0.862500011920929\n",
      "Iteration 13500 Training loss 0.0673711821436882 Validation loss 0.05343741923570633 Accuracy 0.862500011920929\n",
      "Iteration 13510 Training loss 0.05853293463587761 Validation loss 0.0536789633333683 Accuracy 0.8654999732971191\n",
      "Iteration 13520 Training loss 0.05667584761977196 Validation loss 0.051859062165021896 Accuracy 0.8640000224113464\n",
      "Iteration 13530 Training loss 0.07593049108982086 Validation loss 0.06171497702598572 Accuracy 0.8479999899864197\n",
      "Iteration 13540 Training loss 0.05811109021306038 Validation loss 0.05205494910478592 Accuracy 0.8665000200271606\n",
      "Iteration 13550 Training loss 0.05819282680749893 Validation loss 0.052428774535655975 Accuracy 0.8644999861717224\n",
      "Iteration 13560 Training loss 0.06285635381937027 Validation loss 0.056213539093732834 Accuracy 0.8554999828338623\n",
      "Iteration 13570 Training loss 0.05617336183786392 Validation loss 0.052192024886608124 Accuracy 0.8659999966621399\n",
      "Iteration 13580 Training loss 0.05331113189458847 Validation loss 0.05227315425872803 Accuracy 0.8640000224113464\n",
      "Iteration 13590 Training loss 0.06208529323339462 Validation loss 0.05686190724372864 Accuracy 0.8575000166893005\n",
      "Iteration 13600 Training loss 0.06913761049509048 Validation loss 0.05757320299744606 Accuracy 0.8550000190734863\n",
      "Iteration 13610 Training loss 0.05547717213630676 Validation loss 0.05200902000069618 Accuracy 0.8654999732971191\n",
      "Iteration 13620 Training loss 0.05190819501876831 Validation loss 0.05283021554350853 Accuracy 0.8684999942779541\n",
      "Iteration 13630 Training loss 0.0634024441242218 Validation loss 0.05444721877574921 Accuracy 0.8610000014305115\n",
      "Iteration 13640 Training loss 0.06273107975721359 Validation loss 0.05391417816281319 Accuracy 0.8629999756813049\n",
      "Iteration 13650 Training loss 0.06161215901374817 Validation loss 0.05255423113703728 Accuracy 0.8650000095367432\n",
      "Iteration 13660 Training loss 0.06996392458677292 Validation loss 0.056893203407526016 Accuracy 0.8575000166893005\n",
      "Iteration 13670 Training loss 0.061165932565927505 Validation loss 0.0549798309803009 Accuracy 0.859499990940094\n",
      "Iteration 13680 Training loss 0.06503637880086899 Validation loss 0.059128597378730774 Accuracy 0.8535000085830688\n",
      "Iteration 13690 Training loss 0.05730036273598671 Validation loss 0.052546799182891846 Accuracy 0.8675000071525574\n",
      "Iteration 13700 Training loss 0.06362680345773697 Validation loss 0.05380580946803093 Accuracy 0.8619999885559082\n",
      "Iteration 13710 Training loss 0.06318787485361099 Validation loss 0.05501716583967209 Accuracy 0.8619999885559082\n",
      "Iteration 13720 Training loss 0.05954510718584061 Validation loss 0.05607116222381592 Accuracy 0.8565000295639038\n",
      "Iteration 13730 Training loss 0.06108760088682175 Validation loss 0.05279792845249176 Accuracy 0.8644999861717224\n",
      "Iteration 13740 Training loss 0.06317801028490067 Validation loss 0.052841950207948685 Accuracy 0.8634999990463257\n",
      "Iteration 13750 Training loss 0.07065416127443314 Validation loss 0.05762971565127373 Accuracy 0.8544999957084656\n",
      "Iteration 13760 Training loss 0.06279275566339493 Validation loss 0.05453096702694893 Accuracy 0.862500011920929\n",
      "Iteration 13770 Training loss 0.05751156434416771 Validation loss 0.05347573012113571 Accuracy 0.8615000247955322\n",
      "Iteration 13780 Training loss 0.06375540792942047 Validation loss 0.05463303625583649 Accuracy 0.8610000014305115\n",
      "Iteration 13790 Training loss 0.0655517727136612 Validation loss 0.05834466218948364 Accuracy 0.8519999980926514\n",
      "Iteration 13800 Training loss 0.058160457760095596 Validation loss 0.05319838598370552 Accuracy 0.862500011920929\n",
      "Iteration 13810 Training loss 0.06379585713148117 Validation loss 0.05375197157263756 Accuracy 0.862500011920929\n",
      "Iteration 13820 Training loss 0.05949420854449272 Validation loss 0.05250414460897446 Accuracy 0.8615000247955322\n",
      "Iteration 13830 Training loss 0.060534194111824036 Validation loss 0.05425373464822769 Accuracy 0.862500011920929\n",
      "Iteration 13840 Training loss 0.06184984743595123 Validation loss 0.05389559641480446 Accuracy 0.8629999756813049\n",
      "Iteration 13850 Training loss 0.06344062834978104 Validation loss 0.05338278040289879 Accuracy 0.8640000224113464\n",
      "Iteration 13860 Training loss 0.06161916255950928 Validation loss 0.05387667566537857 Accuracy 0.8615000247955322\n",
      "Iteration 13870 Training loss 0.05537419021129608 Validation loss 0.05244970694184303 Accuracy 0.8654999732971191\n",
      "Iteration 13880 Training loss 0.058568645268678665 Validation loss 0.05406724661588669 Accuracy 0.8619999885559082\n",
      "Iteration 13890 Training loss 0.06907957047224045 Validation loss 0.05718955025076866 Accuracy 0.8560000061988831\n",
      "Iteration 13900 Training loss 0.058594752103090286 Validation loss 0.05284322425723076 Accuracy 0.8659999966621399\n",
      "Iteration 13910 Training loss 0.07063362002372742 Validation loss 0.056565817445516586 Accuracy 0.8565000295639038\n",
      "Iteration 13920 Training loss 0.06711378693580627 Validation loss 0.057620566338300705 Accuracy 0.859000027179718\n",
      "Iteration 13930 Training loss 0.06359884142875671 Validation loss 0.055732276290655136 Accuracy 0.8585000038146973\n",
      "Iteration 13940 Training loss 0.05733843147754669 Validation loss 0.05329301580786705 Accuracy 0.8659999966621399\n",
      "Iteration 13950 Training loss 0.06769382953643799 Validation loss 0.05696931481361389 Accuracy 0.8535000085830688\n",
      "Iteration 13960 Training loss 0.06137406826019287 Validation loss 0.053616713732481 Accuracy 0.8604999780654907\n",
      "Iteration 13970 Training loss 0.06368934363126755 Validation loss 0.05363006517291069 Accuracy 0.8604999780654907\n",
      "Iteration 13980 Training loss 0.05998218059539795 Validation loss 0.05455959960818291 Accuracy 0.8634999990463257\n",
      "Iteration 13990 Training loss 0.06238206475973129 Validation loss 0.05578453093767166 Accuracy 0.8575000166893005\n",
      "Iteration 14000 Training loss 0.056024711579084396 Validation loss 0.053011663258075714 Accuracy 0.8684999942779541\n",
      "Iteration 14010 Training loss 0.06487607210874557 Validation loss 0.055378735065460205 Accuracy 0.8600000143051147\n",
      "Iteration 14020 Training loss 0.06219841167330742 Validation loss 0.054379742592573166 Accuracy 0.8610000014305115\n",
      "Iteration 14030 Training loss 0.06266247481107712 Validation loss 0.054873764514923096 Accuracy 0.8615000247955322\n",
      "Iteration 14040 Training loss 0.06027182936668396 Validation loss 0.05335718393325806 Accuracy 0.862500011920929\n",
      "Iteration 14050 Training loss 0.05895313620567322 Validation loss 0.05497407168149948 Accuracy 0.8615000247955322\n",
      "Iteration 14060 Training loss 0.06830538809299469 Validation loss 0.057704053819179535 Accuracy 0.8550000190734863\n",
      "Iteration 14070 Training loss 0.06262341886758804 Validation loss 0.053658775985240936 Accuracy 0.8604999780654907\n",
      "Iteration 14080 Training loss 0.0623941645026207 Validation loss 0.05582688748836517 Accuracy 0.8569999933242798\n",
      "Iteration 14090 Training loss 0.05761405825614929 Validation loss 0.060431815683841705 Accuracy 0.8525000214576721\n",
      "Iteration 14100 Training loss 0.06301792711019516 Validation loss 0.06639137864112854 Accuracy 0.8379999995231628\n",
      "Iteration 14110 Training loss 0.06347113102674484 Validation loss 0.06706906855106354 Accuracy 0.8364999890327454\n",
      "Iteration 14120 Training loss 0.06605420261621475 Validation loss 0.06845223158597946 Accuracy 0.8330000042915344\n",
      "Iteration 14130 Training loss 0.06579326093196869 Validation loss 0.0728757455945015 Accuracy 0.8230000138282776\n",
      "Iteration 14140 Training loss 0.04917772859334946 Validation loss 0.0581459105014801 Accuracy 0.8560000061988831\n",
      "Iteration 14150 Training loss 0.07155293226242065 Validation loss 0.07882121205329895 Accuracy 0.8065000176429749\n",
      "Iteration 14160 Training loss 0.06454168260097504 Validation loss 0.07039687037467957 Accuracy 0.8295000195503235\n",
      "Iteration 14170 Training loss 0.06095193326473236 Validation loss 0.06320711225271225 Accuracy 0.8450000286102295\n",
      "Iteration 14180 Training loss 0.05785186588764191 Validation loss 0.05844862759113312 Accuracy 0.8569999933242798\n",
      "Iteration 14190 Training loss 0.06626828014850616 Validation loss 0.0696730837225914 Accuracy 0.8309999704360962\n",
      "Iteration 14200 Training loss 0.053384557366371155 Validation loss 0.059705134481191635 Accuracy 0.8535000085830688\n",
      "Iteration 14210 Training loss 0.06272182613611221 Validation loss 0.07019628584384918 Accuracy 0.8299999833106995\n",
      "Iteration 14220 Training loss 0.05163836106657982 Validation loss 0.06113539636135101 Accuracy 0.8500000238418579\n",
      "Iteration 14230 Training loss 0.06228293105959892 Validation loss 0.06455563753843307 Accuracy 0.8414999842643738\n",
      "Iteration 14240 Training loss 0.05529433488845825 Validation loss 0.058225300163030624 Accuracy 0.8569999933242798\n",
      "Iteration 14250 Training loss 0.06634163111448288 Validation loss 0.07220182567834854 Accuracy 0.8270000219345093\n",
      "Iteration 14260 Training loss 0.06105444207787514 Validation loss 0.06745396554470062 Accuracy 0.8339999914169312\n",
      "Iteration 14270 Training loss 0.05640418455004692 Validation loss 0.059499505907297134 Accuracy 0.8535000085830688\n",
      "Iteration 14280 Training loss 0.06603869050741196 Validation loss 0.07238835096359253 Accuracy 0.8230000138282776\n",
      "Iteration 14290 Training loss 0.0648370161652565 Validation loss 0.06934748589992523 Accuracy 0.8324999809265137\n",
      "Iteration 14300 Training loss 0.06062058359384537 Validation loss 0.06853141635656357 Accuracy 0.8324999809265137\n",
      "Iteration 14310 Training loss 0.06133786588907242 Validation loss 0.07056253403425217 Accuracy 0.8289999961853027\n",
      "Iteration 14320 Training loss 0.06805188208818436 Validation loss 0.0712338462471962 Accuracy 0.828499972820282\n",
      "Iteration 14330 Training loss 0.0666324645280838 Validation loss 0.0701250433921814 Accuracy 0.8305000066757202\n",
      "Iteration 14340 Training loss 0.05698194354772568 Validation loss 0.06059728562831879 Accuracy 0.8504999876022339\n",
      "Iteration 14350 Training loss 0.06002889946103096 Validation loss 0.0655507892370224 Accuracy 0.840499997138977\n",
      "Iteration 14360 Training loss 0.0638507753610611 Validation loss 0.06703691184520721 Accuracy 0.8349999785423279\n",
      "Iteration 14370 Training loss 0.0647004246711731 Validation loss 0.06915505230426788 Accuracy 0.8324999809265137\n",
      "Iteration 14380 Training loss 0.05675181373953819 Validation loss 0.060628656297922134 Accuracy 0.8519999980926514\n",
      "Iteration 14390 Training loss 0.06203904375433922 Validation loss 0.07175297290086746 Accuracy 0.828000009059906\n",
      "Iteration 14400 Training loss 0.056627653539180756 Validation loss 0.0625273585319519 Accuracy 0.8489999771118164\n",
      "Iteration 14410 Training loss 0.05570799857378006 Validation loss 0.05983437970280647 Accuracy 0.8514999747276306\n",
      "Iteration 14420 Training loss 0.06580009311437607 Validation loss 0.06644803285598755 Accuracy 0.8364999890327454\n",
      "Iteration 14430 Training loss 0.07136699557304382 Validation loss 0.0722828060388565 Accuracy 0.8240000009536743\n",
      "Iteration 14440 Training loss 0.061774641275405884 Validation loss 0.0656948834657669 Accuracy 0.8374999761581421\n",
      "Iteration 14450 Training loss 0.05837807431817055 Validation loss 0.06540881842374802 Accuracy 0.8395000100135803\n",
      "Iteration 14460 Training loss 0.061997488141059875 Validation loss 0.063441202044487 Accuracy 0.843500018119812\n",
      "Iteration 14470 Training loss 0.06383471190929413 Validation loss 0.06990573555231094 Accuracy 0.8305000066757202\n",
      "Iteration 14480 Training loss 0.06106211245059967 Validation loss 0.0645119771361351 Accuracy 0.843500018119812\n",
      "Iteration 14490 Training loss 0.060106001794338226 Validation loss 0.06390941143035889 Accuracy 0.8450000286102295\n",
      "Iteration 14500 Training loss 0.0631880834698677 Validation loss 0.070543073117733 Accuracy 0.8264999985694885\n",
      "Iteration 14510 Training loss 0.05407397449016571 Validation loss 0.0539950355887413 Accuracy 0.8650000095367432\n",
      "Iteration 14520 Training loss 0.06675931811332703 Validation loss 0.05738931894302368 Accuracy 0.8544999957084656\n",
      "Iteration 14530 Training loss 0.06357549875974655 Validation loss 0.0551183708012104 Accuracy 0.8610000014305115\n",
      "Iteration 14540 Training loss 0.06004619225859642 Validation loss 0.054316017776727676 Accuracy 0.8619999885559082\n",
      "Iteration 14550 Training loss 0.06468243896961212 Validation loss 0.05512169003486633 Accuracy 0.8610000014305115\n",
      "Iteration 14560 Training loss 0.06262392550706863 Validation loss 0.05314929410815239 Accuracy 0.8629999756813049\n",
      "Iteration 14570 Training loss 0.061503611505031586 Validation loss 0.05522698536515236 Accuracy 0.859000027179718\n",
      "Iteration 14580 Training loss 0.061522476375103 Validation loss 0.05517102777957916 Accuracy 0.8604999780654907\n",
      "Iteration 14590 Training loss 0.054652608931064606 Validation loss 0.053427260369062424 Accuracy 0.8615000247955322\n",
      "Iteration 14600 Training loss 0.061314906924963 Validation loss 0.052762411534786224 Accuracy 0.8634999990463257\n",
      "Iteration 14610 Training loss 0.07271423190832138 Validation loss 0.059522658586502075 Accuracy 0.8500000238418579\n",
      "Iteration 14620 Training loss 0.061456985771656036 Validation loss 0.05547347292304039 Accuracy 0.859000027179718\n",
      "Iteration 14630 Training loss 0.05763637647032738 Validation loss 0.055201973766088486 Accuracy 0.859499990940094\n",
      "Iteration 14640 Training loss 0.060983575880527496 Validation loss 0.053248871117830276 Accuracy 0.8629999756813049\n",
      "Iteration 14650 Training loss 0.07091578096151352 Validation loss 0.057579103857278824 Accuracy 0.8560000061988831\n",
      "Iteration 14660 Training loss 0.06741055101156235 Validation loss 0.05586244910955429 Accuracy 0.859000027179718\n",
      "Iteration 14670 Training loss 0.06476182490587234 Validation loss 0.05513386428356171 Accuracy 0.8619999885559082\n",
      "Iteration 14680 Training loss 0.05925958231091499 Validation loss 0.05249933898448944 Accuracy 0.8659999966621399\n",
      "Iteration 14690 Training loss 0.06325836479663849 Validation loss 0.05531648173928261 Accuracy 0.859499990940094\n",
      "Iteration 14700 Training loss 0.06051287055015564 Validation loss 0.052696723490953445 Accuracy 0.8665000200271606\n",
      "Iteration 14710 Training loss 0.0636870414018631 Validation loss 0.054338984191417694 Accuracy 0.8585000038146973\n",
      "Iteration 14720 Training loss 0.06076328828930855 Validation loss 0.054741889238357544 Accuracy 0.8600000143051147\n",
      "Iteration 14730 Training loss 0.06313420832157135 Validation loss 0.05509127303957939 Accuracy 0.8604999780654907\n",
      "Iteration 14740 Training loss 0.06663143634796143 Validation loss 0.05689970776438713 Accuracy 0.8565000295639038\n",
      "Iteration 14750 Training loss 0.06092508137226105 Validation loss 0.05329878255724907 Accuracy 0.8619999885559082\n",
      "Iteration 14760 Training loss 0.0594477578997612 Validation loss 0.052759818732738495 Accuracy 0.8640000224113464\n",
      "Iteration 14770 Training loss 0.06046469137072563 Validation loss 0.05335013195872307 Accuracy 0.8619999885559082\n",
      "Iteration 14780 Training loss 0.05915931612253189 Validation loss 0.05460815504193306 Accuracy 0.8604999780654907\n",
      "Iteration 14790 Training loss 0.05975930020213127 Validation loss 0.053133003413677216 Accuracy 0.8644999861717224\n",
      "Iteration 14800 Training loss 0.05619722604751587 Validation loss 0.053582534193992615 Accuracy 0.8629999756813049\n",
      "Iteration 14810 Training loss 0.06393817067146301 Validation loss 0.05535517632961273 Accuracy 0.8610000014305115\n",
      "Iteration 14820 Training loss 0.057575684040784836 Validation loss 0.052728958427906036 Accuracy 0.8619999885559082\n",
      "Iteration 14830 Training loss 0.06417126953601837 Validation loss 0.05674180015921593 Accuracy 0.8565000295639038\n",
      "Iteration 14840 Training loss 0.05973409488797188 Validation loss 0.05390443280339241 Accuracy 0.8600000143051147\n",
      "Iteration 14850 Training loss 0.06226099655032158 Validation loss 0.05575624853372574 Accuracy 0.859000027179718\n",
      "Iteration 14860 Training loss 0.06357765197753906 Validation loss 0.05432453379034996 Accuracy 0.8600000143051147\n",
      "Iteration 14870 Training loss 0.07120410352945328 Validation loss 0.05778898671269417 Accuracy 0.8550000190734863\n",
      "Iteration 14880 Training loss 0.077479787170887 Validation loss 0.06372030824422836 Accuracy 0.843500018119812\n",
      "Iteration 14890 Training loss 0.061370912939310074 Validation loss 0.053148332983255386 Accuracy 0.8610000014305115\n",
      "Iteration 14900 Training loss 0.05801942199468613 Validation loss 0.05337198078632355 Accuracy 0.8629999756813049\n",
      "Iteration 14910 Training loss 0.06993692368268967 Validation loss 0.06070956587791443 Accuracy 0.8500000238418579\n",
      "Iteration 14920 Training loss 0.0621243380010128 Validation loss 0.05644358694553375 Accuracy 0.8575000166893005\n",
      "Iteration 14930 Training loss 0.05716172605752945 Validation loss 0.053990136831998825 Accuracy 0.8650000095367432\n",
      "Iteration 14940 Training loss 0.06409986317157745 Validation loss 0.05523713678121567 Accuracy 0.859000027179718\n",
      "Iteration 14950 Training loss 0.06936408579349518 Validation loss 0.05689535662531853 Accuracy 0.8569999933242798\n",
      "Iteration 14960 Training loss 0.05650777369737625 Validation loss 0.05298541113734245 Accuracy 0.8644999861717224\n",
      "Iteration 14970 Training loss 0.06868701428174973 Validation loss 0.05556204542517662 Accuracy 0.8610000014305115\n",
      "Iteration 14980 Training loss 0.061297204345464706 Validation loss 0.053563009947538376 Accuracy 0.862500011920929\n",
      "Iteration 14990 Training loss 0.06172211095690727 Validation loss 0.05548455938696861 Accuracy 0.8615000247955322\n",
      "Iteration 15000 Training loss 0.055065277963876724 Validation loss 0.053057342767715454 Accuracy 0.8665000200271606\n",
      "Iteration 15010 Training loss 0.06515894830226898 Validation loss 0.05406811460852623 Accuracy 0.8610000014305115\n",
      "Iteration 15020 Training loss 0.06746334582567215 Validation loss 0.05563919618725777 Accuracy 0.8604999780654907\n",
      "Iteration 15030 Training loss 0.06290511786937714 Validation loss 0.05764605104923248 Accuracy 0.8560000061988831\n",
      "Iteration 15040 Training loss 0.06564722210168839 Validation loss 0.05622255429625511 Accuracy 0.8560000061988831\n",
      "Iteration 15050 Training loss 0.06906088441610336 Validation loss 0.057065967470407486 Accuracy 0.8554999828338623\n",
      "Iteration 15060 Training loss 0.06140708178281784 Validation loss 0.05543949082493782 Accuracy 0.8569999933242798\n",
      "Iteration 15070 Training loss 0.058514222502708435 Validation loss 0.053589362651109695 Accuracy 0.8604999780654907\n",
      "Iteration 15080 Training loss 0.0728578194975853 Validation loss 0.06012020632624626 Accuracy 0.8500000238418579\n",
      "Iteration 15090 Training loss 0.05548673868179321 Validation loss 0.05286038666963577 Accuracy 0.8650000095367432\n",
      "Iteration 15100 Training loss 0.058329951018095016 Validation loss 0.0530555322766304 Accuracy 0.862500011920929\n",
      "Iteration 15110 Training loss 0.05366315692663193 Validation loss 0.05726093426346779 Accuracy 0.8585000038146973\n",
      "Iteration 15120 Training loss 0.0654807910323143 Validation loss 0.0696837306022644 Accuracy 0.8295000195503235\n",
      "Iteration 15130 Training loss 0.0642186626791954 Validation loss 0.06730107218027115 Accuracy 0.8349999785423279\n",
      "Iteration 15140 Training loss 0.06860288977622986 Validation loss 0.0697551891207695 Accuracy 0.8309999704360962\n",
      "Iteration 15150 Training loss 0.06260307878255844 Validation loss 0.06224729120731354 Accuracy 0.8485000133514404\n",
      "Iteration 15160 Training loss 0.05850353464484215 Validation loss 0.06397459656000137 Accuracy 0.8454999923706055\n",
      "Iteration 15170 Training loss 0.06781884282827377 Validation loss 0.0681099072098732 Accuracy 0.8330000042915344\n",
      "Iteration 15180 Training loss 0.05734417587518692 Validation loss 0.06328627467155457 Accuracy 0.843500018119812\n",
      "Iteration 15190 Training loss 0.06309738755226135 Validation loss 0.06431831419467926 Accuracy 0.843500018119812\n",
      "Iteration 15200 Training loss 0.0542558878660202 Validation loss 0.0640646368265152 Accuracy 0.843500018119812\n",
      "Iteration 15210 Training loss 0.051634930074214935 Validation loss 0.057508040219545364 Accuracy 0.859000027179718\n",
      "Iteration 15220 Training loss 0.06308433413505554 Validation loss 0.066362164914608 Accuracy 0.8370000123977661\n",
      "Iteration 15230 Training loss 0.05937868356704712 Validation loss 0.061677366495132446 Accuracy 0.8489999771118164\n",
      "Iteration 15240 Training loss 0.06131734326481819 Validation loss 0.06579316407442093 Accuracy 0.8385000228881836\n",
      "Iteration 15250 Training loss 0.06121644750237465 Validation loss 0.06418360769748688 Accuracy 0.8424999713897705\n",
      "Iteration 15260 Training loss 0.06330489367246628 Validation loss 0.06522106379270554 Accuracy 0.8414999842643738\n",
      "Iteration 15270 Training loss 0.05439978465437889 Validation loss 0.05913303419947624 Accuracy 0.8535000085830688\n",
      "Iteration 15280 Training loss 0.05378950759768486 Validation loss 0.0559980571269989 Accuracy 0.859499990940094\n",
      "Iteration 15290 Training loss 0.07369165867567062 Validation loss 0.059998076409101486 Accuracy 0.8500000238418579\n",
      "Iteration 15300 Training loss 0.05706566199660301 Validation loss 0.052907753735780716 Accuracy 0.8644999861717224\n",
      "Iteration 15310 Training loss 0.06757128983736038 Validation loss 0.05617016553878784 Accuracy 0.8579999804496765\n",
      "Iteration 15320 Training loss 0.06082499772310257 Validation loss 0.0535668283700943 Accuracy 0.8604999780654907\n",
      "Iteration 15330 Training loss 0.060569893568754196 Validation loss 0.052784115076065063 Accuracy 0.8619999885559082\n",
      "Iteration 15340 Training loss 0.06231982260942459 Validation loss 0.053565047681331635 Accuracy 0.8619999885559082\n",
      "Iteration 15350 Training loss 0.06827159970998764 Validation loss 0.05641354247927666 Accuracy 0.8565000295639038\n",
      "Iteration 15360 Training loss 0.054890334606170654 Validation loss 0.0535033643245697 Accuracy 0.8629999756813049\n",
      "Iteration 15370 Training loss 0.056904710829257965 Validation loss 0.05357324704527855 Accuracy 0.8610000014305115\n",
      "Iteration 15380 Training loss 0.06424450129270554 Validation loss 0.05439627170562744 Accuracy 0.8585000038146973\n",
      "Iteration 15390 Training loss 0.06008463352918625 Validation loss 0.05289598926901817 Accuracy 0.8629999756813049\n",
      "Iteration 15400 Training loss 0.06062202528119087 Validation loss 0.053463611751794815 Accuracy 0.8615000247955322\n",
      "Iteration 15410 Training loss 0.06684856116771698 Validation loss 0.05671912431716919 Accuracy 0.8575000166893005\n",
      "Iteration 15420 Training loss 0.06201157718896866 Validation loss 0.05312277376651764 Accuracy 0.862500011920929\n",
      "Iteration 15430 Training loss 0.05941345915198326 Validation loss 0.053157441318035126 Accuracy 0.8619999885559082\n",
      "Iteration 15440 Training loss 0.06639688462018967 Validation loss 0.05665728822350502 Accuracy 0.8560000061988831\n",
      "Iteration 15450 Training loss 0.05865026265382767 Validation loss 0.053557176142930984 Accuracy 0.8619999885559082\n",
      "Iteration 15460 Training loss 0.05695268511772156 Validation loss 0.05263322591781616 Accuracy 0.8650000095367432\n",
      "Iteration 15470 Training loss 0.06370449811220169 Validation loss 0.06810405105352402 Accuracy 0.8335000276565552\n",
      "Iteration 15480 Training loss 0.057396095246076584 Validation loss 0.05943151190876961 Accuracy 0.8560000061988831\n",
      "Iteration 15490 Training loss 0.0603049173951149 Validation loss 0.07000868767499924 Accuracy 0.828499972820282\n",
      "Iteration 15500 Training loss 0.06370902061462402 Validation loss 0.06770804524421692 Accuracy 0.8330000042915344\n",
      "Iteration 15510 Training loss 0.07443192601203918 Validation loss 0.07515039294958115 Accuracy 0.8169999718666077\n",
      "Iteration 15520 Training loss 0.06713244318962097 Validation loss 0.07687070965766907 Accuracy 0.8130000233650208\n",
      "Iteration 15530 Training loss 0.057092200964689255 Validation loss 0.05995338410139084 Accuracy 0.8504999876022339\n",
      "Iteration 15540 Training loss 0.07449045032262802 Validation loss 0.08152270317077637 Accuracy 0.7994999885559082\n",
      "Iteration 15550 Training loss 0.06040468439459801 Validation loss 0.06385309994220734 Accuracy 0.8424999713897705\n",
      "Iteration 15560 Training loss 0.06187788024544716 Validation loss 0.06703091412782669 Accuracy 0.8355000019073486\n",
      "Iteration 15570 Training loss 0.06200291961431503 Validation loss 0.0671236515045166 Accuracy 0.8349999785423279\n",
      "Iteration 15580 Training loss 0.06656207889318466 Validation loss 0.0742323026061058 Accuracy 0.8230000138282776\n",
      "Iteration 15590 Training loss 0.06058207154273987 Validation loss 0.06697921454906464 Accuracy 0.8360000252723694\n",
      "Iteration 15600 Training loss 0.06329120695590973 Validation loss 0.06342600286006927 Accuracy 0.8445000052452087\n",
      "Iteration 15610 Training loss 0.061990752816200256 Validation loss 0.07235318422317505 Accuracy 0.8224999904632568\n",
      "Iteration 15620 Training loss 0.0611591637134552 Validation loss 0.0649179071187973 Accuracy 0.843500018119812\n",
      "Iteration 15630 Training loss 0.05475088208913803 Validation loss 0.06001483276486397 Accuracy 0.8529999852180481\n",
      "Iteration 15640 Training loss 0.06354013085365295 Validation loss 0.06609503924846649 Accuracy 0.8374999761581421\n",
      "Iteration 15650 Training loss 0.07170140743255615 Validation loss 0.07716476917266846 Accuracy 0.809499979019165\n",
      "Iteration 15660 Training loss 0.06283611059188843 Validation loss 0.06768660992383957 Accuracy 0.8335000276565552\n",
      "Iteration 15670 Training loss 0.07333540916442871 Validation loss 0.07212573289871216 Accuracy 0.8274999856948853\n",
      "Iteration 15680 Training loss 0.06261128932237625 Validation loss 0.07103922218084335 Accuracy 0.8314999938011169\n",
      "Iteration 15690 Training loss 0.06396959722042084 Validation loss 0.0733041763305664 Accuracy 0.8195000290870667\n",
      "Iteration 15700 Training loss 0.07099616527557373 Validation loss 0.07468266040086746 Accuracy 0.8205000162124634\n",
      "Iteration 15710 Training loss 0.06630995869636536 Validation loss 0.06556623429059982 Accuracy 0.840499997138977\n",
      "Iteration 15720 Training loss 0.06228584423661232 Validation loss 0.06828437745571136 Accuracy 0.8335000276565552\n",
      "Iteration 15730 Training loss 0.061362914741039276 Validation loss 0.06890429556369781 Accuracy 0.8309999704360962\n",
      "Iteration 15740 Training loss 0.0657477080821991 Validation loss 0.07173872739076614 Accuracy 0.8259999752044678\n",
      "Iteration 15750 Training loss 0.06379946321249008 Validation loss 0.06581626832485199 Accuracy 0.8395000100135803\n",
      "Iteration 15760 Training loss 0.057614684104919434 Validation loss 0.061501648277044296 Accuracy 0.8500000238418579\n",
      "Iteration 15770 Training loss 0.06513077020645142 Validation loss 0.07304241508245468 Accuracy 0.8224999904632568\n",
      "Iteration 15780 Training loss 0.05871383845806122 Validation loss 0.06433097273111343 Accuracy 0.843999981880188\n",
      "Iteration 15790 Training loss 0.06023436039686203 Validation loss 0.06587386876344681 Accuracy 0.840499997138977\n",
      "Iteration 15800 Training loss 0.05607130378484726 Validation loss 0.05915822461247444 Accuracy 0.8539999723434448\n",
      "Iteration 15810 Training loss 0.0651341900229454 Validation loss 0.0691300481557846 Accuracy 0.8320000171661377\n",
      "Iteration 15820 Training loss 0.07152356207370758 Validation loss 0.07572871446609497 Accuracy 0.8165000081062317\n",
      "Iteration 15830 Training loss 0.05777299404144287 Validation loss 0.0609353631734848 Accuracy 0.8495000004768372\n",
      "Iteration 15840 Training loss 0.06372927129268646 Validation loss 0.06682387739419937 Accuracy 0.8360000252723694\n",
      "Iteration 15850 Training loss 0.05532918497920036 Validation loss 0.06048507243394852 Accuracy 0.8495000004768372\n",
      "Iteration 15860 Training loss 0.060334403067827225 Validation loss 0.05331026390194893 Accuracy 0.8619999885559082\n",
      "Iteration 15870 Training loss 0.061324506998062134 Validation loss 0.05398108810186386 Accuracy 0.862500011920929\n",
      "Iteration 15880 Training loss 0.05780602991580963 Validation loss 0.05381298065185547 Accuracy 0.859499990940094\n",
      "Iteration 15890 Training loss 0.06471360474824905 Validation loss 0.05782454460859299 Accuracy 0.8539999723434448\n",
      "Iteration 15900 Training loss 0.06256787478923798 Validation loss 0.05460751801729202 Accuracy 0.859000027179718\n",
      "Iteration 15910 Training loss 0.0700286254286766 Validation loss 0.05597981438040733 Accuracy 0.859000027179718\n",
      "Iteration 15920 Training loss 0.06055296212434769 Validation loss 0.0541980154812336 Accuracy 0.8610000014305115\n",
      "Iteration 15930 Training loss 0.06353817135095596 Validation loss 0.05574316158890724 Accuracy 0.8554999828338623\n",
      "Iteration 15940 Training loss 0.056514889001846313 Validation loss 0.06540894508361816 Accuracy 0.8420000076293945\n",
      "Iteration 15950 Training loss 0.056692831218242645 Validation loss 0.062475208193063736 Accuracy 0.8475000262260437\n",
      "Iteration 15960 Training loss 0.05629554018378258 Validation loss 0.05931851640343666 Accuracy 0.8519999980926514\n",
      "Iteration 15970 Training loss 0.06201152130961418 Validation loss 0.06531748920679092 Accuracy 0.8424999713897705\n",
      "Iteration 15980 Training loss 0.06453073024749756 Validation loss 0.06520821899175644 Accuracy 0.840499997138977\n",
      "Iteration 15990 Training loss 0.065680593252182 Validation loss 0.06967537850141525 Accuracy 0.8299999833106995\n",
      "Iteration 16000 Training loss 0.06021132692694664 Validation loss 0.06772319227457047 Accuracy 0.8345000147819519\n",
      "Iteration 16010 Training loss 0.06520607322454453 Validation loss 0.06670884042978287 Accuracy 0.8349999785423279\n",
      "Iteration 16020 Training loss 0.06255222111940384 Validation loss 0.06798969209194183 Accuracy 0.8324999809265137\n",
      "Iteration 16030 Training loss 0.06733939796686172 Validation loss 0.07296215742826462 Accuracy 0.8220000267028809\n",
      "Iteration 16040 Training loss 0.06649090349674225 Validation loss 0.06588592380285263 Accuracy 0.8389999866485596\n",
      "Iteration 16050 Training loss 0.06412652134895325 Validation loss 0.07194920629262924 Accuracy 0.8274999856948853\n",
      "Iteration 16060 Training loss 0.07217832654714584 Validation loss 0.07562117278575897 Accuracy 0.8215000033378601\n",
      "Iteration 16070 Training loss 0.06916702538728714 Validation loss 0.06945344060659409 Accuracy 0.8324999809265137\n",
      "Iteration 16080 Training loss 0.055294521152973175 Validation loss 0.06388527899980545 Accuracy 0.8450000286102295\n",
      "Iteration 16090 Training loss 0.06118200346827507 Validation loss 0.06627213209867477 Accuracy 0.8385000228881836\n",
      "Iteration 16100 Training loss 0.06318330019712448 Validation loss 0.0703977569937706 Accuracy 0.8289999961853027\n",
      "Iteration 16110 Training loss 0.05820302665233612 Validation loss 0.06284160912036896 Accuracy 0.8460000157356262\n",
      "Iteration 16120 Training loss 0.06168156489729881 Validation loss 0.06852138787508011 Accuracy 0.8320000171661377\n",
      "Iteration 16130 Training loss 0.06312809884548187 Validation loss 0.07224096357822418 Accuracy 0.8224999904632568\n",
      "Iteration 16140 Training loss 0.051667388528585434 Validation loss 0.05865246057510376 Accuracy 0.8554999828338623\n",
      "Iteration 16150 Training loss 0.0622158981859684 Validation loss 0.06569799780845642 Accuracy 0.8379999995231628\n",
      "Iteration 16160 Training loss 0.060575518757104874 Validation loss 0.06740188598632812 Accuracy 0.8349999785423279\n",
      "Iteration 16170 Training loss 0.070559561252594 Validation loss 0.0720810666680336 Accuracy 0.824999988079071\n",
      "Iteration 16180 Training loss 0.05847489461302757 Validation loss 0.05496588349342346 Accuracy 0.862500011920929\n",
      "Iteration 16190 Training loss 0.06494127213954926 Validation loss 0.05784854292869568 Accuracy 0.8535000085830688\n",
      "Iteration 16200 Training loss 0.06605105847120285 Validation loss 0.05646136403083801 Accuracy 0.8575000166893005\n",
      "Iteration 16210 Training loss 0.06758135557174683 Validation loss 0.05885946750640869 Accuracy 0.8529999852180481\n",
      "Iteration 16220 Training loss 0.06159868836402893 Validation loss 0.05392147973179817 Accuracy 0.8615000247955322\n",
      "Iteration 16230 Training loss 0.07120763510465622 Validation loss 0.06117737293243408 Accuracy 0.8514999747276306\n",
      "Iteration 16240 Training loss 0.05262751877307892 Validation loss 0.05301779881119728 Accuracy 0.8629999756813049\n",
      "Iteration 16250 Training loss 0.07253008335828781 Validation loss 0.07810189574956894 Accuracy 0.8119999766349792\n",
      "Iteration 16260 Training loss 0.05865447223186493 Validation loss 0.06219584122300148 Accuracy 0.847000002861023\n",
      "Iteration 16270 Training loss 0.05854006111621857 Validation loss 0.06261368095874786 Accuracy 0.8445000052452087\n",
      "Iteration 16280 Training loss 0.06616126745939255 Validation loss 0.06847671419382095 Accuracy 0.8314999938011169\n",
      "Iteration 16290 Training loss 0.05695844069123268 Validation loss 0.060221899300813675 Accuracy 0.8500000238418579\n",
      "Iteration 16300 Training loss 0.06888707727193832 Validation loss 0.07452981173992157 Accuracy 0.8215000033378601\n",
      "Iteration 16310 Training loss 0.06132298707962036 Validation loss 0.07132774591445923 Accuracy 0.8274999856948853\n",
      "Iteration 16320 Training loss 0.058942895382642746 Validation loss 0.06762527674436569 Accuracy 0.8335000276565552\n",
      "Iteration 16330 Training loss 0.06532280147075653 Validation loss 0.0702391117811203 Accuracy 0.8299999833106995\n",
      "Iteration 16340 Training loss 0.05894254148006439 Validation loss 0.06214427202939987 Accuracy 0.8460000157356262\n",
      "Iteration 16350 Training loss 0.06140732392668724 Validation loss 0.06769444048404694 Accuracy 0.8335000276565552\n",
      "Iteration 16360 Training loss 0.06555314362049103 Validation loss 0.06781580299139023 Accuracy 0.8345000147819519\n",
      "Iteration 16370 Training loss 0.056317124515771866 Validation loss 0.06712355464696884 Accuracy 0.8364999890327454\n",
      "Iteration 16380 Training loss 0.06341055035591125 Validation loss 0.0659424439072609 Accuracy 0.8395000100135803\n",
      "Iteration 16390 Training loss 0.06263303011655807 Validation loss 0.0668068379163742 Accuracy 0.8355000019073486\n",
      "Iteration 16400 Training loss 0.06806677579879761 Validation loss 0.0699378177523613 Accuracy 0.828499972820282\n",
      "Iteration 16410 Training loss 0.06563019007444382 Validation loss 0.07068111002445221 Accuracy 0.8309999704360962\n",
      "Iteration 16420 Training loss 0.05792694911360741 Validation loss 0.06342888623476028 Accuracy 0.8460000157356262\n",
      "Iteration 16430 Training loss 0.05572553351521492 Validation loss 0.06359520554542542 Accuracy 0.8429999947547913\n",
      "Iteration 16440 Training loss 0.062149904668331146 Validation loss 0.06510017067193985 Accuracy 0.8424999713897705\n",
      "Iteration 16450 Training loss 0.06396221369504929 Validation loss 0.06808363646268845 Accuracy 0.8330000042915344\n",
      "Iteration 16460 Training loss 0.05748851224780083 Validation loss 0.06029732525348663 Accuracy 0.8525000214576721\n",
      "Iteration 16470 Training loss 0.058194346725940704 Validation loss 0.06983642280101776 Accuracy 0.8314999938011169\n",
      "Iteration 16480 Training loss 0.061427511274814606 Validation loss 0.06600510329008102 Accuracy 0.8379999995231628\n",
      "Iteration 16490 Training loss 0.054309744387865067 Validation loss 0.05643710494041443 Accuracy 0.8619999885559082\n",
      "Iteration 16500 Training loss 0.06909274309873581 Validation loss 0.07481379806995392 Accuracy 0.8209999799728394\n",
      "Iteration 16510 Training loss 0.057499922811985016 Validation loss 0.061417680233716965 Accuracy 0.8475000262260437\n",
      "Iteration 16520 Training loss 0.05664247274398804 Validation loss 0.058553632348775864 Accuracy 0.8544999957084656\n",
      "Iteration 16530 Training loss 0.056833721697330475 Validation loss 0.05354493856430054 Accuracy 0.8650000095367432\n",
      "Iteration 16540 Training loss 0.060895562171936035 Validation loss 0.06654885411262512 Accuracy 0.8364999890327454\n",
      "Iteration 16550 Training loss 0.05990489199757576 Validation loss 0.06419627368450165 Accuracy 0.840499997138977\n",
      "Iteration 16560 Training loss 0.06630787998437881 Validation loss 0.07228759676218033 Accuracy 0.8220000267028809\n",
      "Iteration 16570 Training loss 0.06983175128698349 Validation loss 0.07277755439281464 Accuracy 0.8220000267028809\n",
      "Iteration 16580 Training loss 0.057250335812568665 Validation loss 0.06117457151412964 Accuracy 0.8504999876022339\n",
      "Iteration 16590 Training loss 0.07017877697944641 Validation loss 0.07254405319690704 Accuracy 0.824999988079071\n",
      "Iteration 16600 Training loss 0.06650850176811218 Validation loss 0.06828616559505463 Accuracy 0.8330000042915344\n",
      "Iteration 16610 Training loss 0.05573652684688568 Validation loss 0.06071906164288521 Accuracy 0.8500000238418579\n",
      "Iteration 16620 Training loss 0.054606374353170395 Validation loss 0.062129661440849304 Accuracy 0.8485000133514404\n",
      "Iteration 16630 Training loss 0.0655900314450264 Validation loss 0.07200741767883301 Accuracy 0.8259999752044678\n",
      "Iteration 16640 Training loss 0.06313206255435944 Validation loss 0.06771820038557053 Accuracy 0.8320000171661377\n",
      "Iteration 16650 Training loss 0.06390152126550674 Validation loss 0.06569036841392517 Accuracy 0.8389999866485596\n",
      "Iteration 16660 Training loss 0.06289419531822205 Validation loss 0.0670362189412117 Accuracy 0.8355000019073486\n",
      "Iteration 16670 Training loss 0.056922223418951035 Validation loss 0.06381811946630478 Accuracy 0.8450000286102295\n",
      "Iteration 16680 Training loss 0.07174031436443329 Validation loss 0.0797659307718277 Accuracy 0.8059999942779541\n",
      "Iteration 16690 Training loss 0.07244275510311127 Validation loss 0.07493805140256882 Accuracy 0.8180000185966492\n",
      "Iteration 16700 Training loss 0.06416747719049454 Validation loss 0.07010354101657867 Accuracy 0.8314999938011169\n",
      "Iteration 16710 Training loss 0.06838008016347885 Validation loss 0.07167437672615051 Accuracy 0.8270000219345093\n",
      "Iteration 16720 Training loss 0.05258627235889435 Validation loss 0.05849364399909973 Accuracy 0.8569999933242798\n",
      "Iteration 16730 Training loss 0.06417347490787506 Validation loss 0.07533583045005798 Accuracy 0.8159999847412109\n",
      "Iteration 16740 Training loss 0.0566345751285553 Validation loss 0.059618767350912094 Accuracy 0.8514999747276306\n",
      "Iteration 16750 Training loss 0.06621753424406052 Validation loss 0.0676061362028122 Accuracy 0.8335000276565552\n",
      "Iteration 16760 Training loss 0.05770700052380562 Validation loss 0.06065415218472481 Accuracy 0.8514999747276306\n",
      "Iteration 16770 Training loss 0.06631207466125488 Validation loss 0.07213405519723892 Accuracy 0.8245000243186951\n",
      "Iteration 16780 Training loss 0.06939854472875595 Validation loss 0.07499690353870392 Accuracy 0.8205000162124634\n",
      "Iteration 16790 Training loss 0.06469617784023285 Validation loss 0.07015994191169739 Accuracy 0.8274999856948853\n",
      "Iteration 16800 Training loss 0.05643199011683464 Validation loss 0.063339002430439 Accuracy 0.843500018119812\n",
      "Iteration 16810 Training loss 0.06705056875944138 Validation loss 0.07035419344902039 Accuracy 0.8270000219345093\n",
      "Iteration 16820 Training loss 0.0536881722509861 Validation loss 0.05648649483919144 Accuracy 0.8575000166893005\n",
      "Iteration 16830 Training loss 0.054050467908382416 Validation loss 0.05341436341404915 Accuracy 0.8615000247955322\n",
      "Iteration 16840 Training loss 0.06791187077760696 Validation loss 0.058594852685928345 Accuracy 0.8514999747276306\n",
      "Iteration 16850 Training loss 0.0605933852493763 Validation loss 0.05393660441040993 Accuracy 0.862500011920929\n",
      "Iteration 16860 Training loss 0.06759874522686005 Validation loss 0.05724834278225899 Accuracy 0.8539999723434448\n",
      "Iteration 16870 Training loss 0.055697567760944366 Validation loss 0.05396284535527229 Accuracy 0.8665000200271606\n",
      "Iteration 16880 Training loss 0.06420838832855225 Validation loss 0.05566827952861786 Accuracy 0.8600000143051147\n",
      "Iteration 16890 Training loss 0.06331563740968704 Validation loss 0.053782857954502106 Accuracy 0.8610000014305115\n",
      "Iteration 16900 Training loss 0.0672297328710556 Validation loss 0.053551603108644485 Accuracy 0.862500011920929\n",
      "Iteration 16910 Training loss 0.06357908248901367 Validation loss 0.055357351899147034 Accuracy 0.8585000038146973\n",
      "Iteration 16920 Training loss 0.06097128987312317 Validation loss 0.05507194995880127 Accuracy 0.8600000143051147\n",
      "Iteration 16930 Training loss 0.0688324049115181 Validation loss 0.05771574005484581 Accuracy 0.8560000061988831\n",
      "Iteration 16940 Training loss 0.06055546551942825 Validation loss 0.05452686920762062 Accuracy 0.8585000038146973\n",
      "Iteration 16950 Training loss 0.05779062956571579 Validation loss 0.05339230224490166 Accuracy 0.8640000224113464\n",
      "Iteration 16960 Training loss 0.06723350286483765 Validation loss 0.056545380502939224 Accuracy 0.8554999828338623\n",
      "Iteration 16970 Training loss 0.061894673854112625 Validation loss 0.05468214303255081 Accuracy 0.8579999804496765\n",
      "Iteration 16980 Training loss 0.06472013145685196 Validation loss 0.05679396167397499 Accuracy 0.8585000038146973\n",
      "Iteration 16990 Training loss 0.0637652575969696 Validation loss 0.055911850184202194 Accuracy 0.8579999804496765\n",
      "Iteration 17000 Training loss 0.06265448778867722 Validation loss 0.05609966441988945 Accuracy 0.8600000143051147\n",
      "Iteration 17010 Training loss 0.06159262731671333 Validation loss 0.05425717309117317 Accuracy 0.8610000014305115\n",
      "Iteration 17020 Training loss 0.06479157507419586 Validation loss 0.057564880698919296 Accuracy 0.8554999828338623\n",
      "Iteration 17030 Training loss 0.05736328661441803 Validation loss 0.053763192147016525 Accuracy 0.8610000014305115\n",
      "Iteration 17040 Training loss 0.06049288064241409 Validation loss 0.054228365421295166 Accuracy 0.8610000014305115\n",
      "Iteration 17050 Training loss 0.07105453312397003 Validation loss 0.05766427516937256 Accuracy 0.8535000085830688\n",
      "Iteration 17060 Training loss 0.06514564156532288 Validation loss 0.05447842553257942 Accuracy 0.8604999780654907\n",
      "Iteration 17070 Training loss 0.06530497968196869 Validation loss 0.054913513362407684 Accuracy 0.859499990940094\n",
      "Iteration 17080 Training loss 0.05494813993573189 Validation loss 0.05338330939412117 Accuracy 0.8619999885559082\n",
      "Iteration 17090 Training loss 0.05864277109503746 Validation loss 0.05485676974058151 Accuracy 0.8610000014305115\n",
      "Iteration 17100 Training loss 0.05807282775640488 Validation loss 0.05358896777033806 Accuracy 0.8610000014305115\n",
      "Iteration 17110 Training loss 0.06752009689807892 Validation loss 0.05714511498808861 Accuracy 0.8539999723434448\n",
      "Iteration 17120 Training loss 0.059949133545160294 Validation loss 0.055323339998722076 Accuracy 0.8600000143051147\n",
      "Iteration 17130 Training loss 0.06281295418739319 Validation loss 0.05691887065768242 Accuracy 0.8560000061988831\n",
      "Iteration 17140 Training loss 0.05627886950969696 Validation loss 0.053732965141534805 Accuracy 0.8610000014305115\n",
      "Iteration 17150 Training loss 0.06332223117351532 Validation loss 0.05635836720466614 Accuracy 0.8565000295639038\n",
      "Iteration 17160 Training loss 0.06799517571926117 Validation loss 0.05974888429045677 Accuracy 0.8519999980926514\n",
      "Iteration 17170 Training loss 0.06923704594373703 Validation loss 0.06123071908950806 Accuracy 0.8500000238418579\n",
      "Iteration 17180 Training loss 0.06894196569919586 Validation loss 0.06102760136127472 Accuracy 0.8489999771118164\n",
      "Iteration 17190 Training loss 0.060624152421951294 Validation loss 0.05361630395054817 Accuracy 0.8629999756813049\n",
      "Iteration 17200 Training loss 0.06317317485809326 Validation loss 0.05699395760893822 Accuracy 0.8560000061988831\n",
      "Iteration 17210 Training loss 0.066717728972435 Validation loss 0.058285780251026154 Accuracy 0.8539999723434448\n",
      "Iteration 17220 Training loss 0.06905997544527054 Validation loss 0.05883317068219185 Accuracy 0.8514999747276306\n",
      "Iteration 17230 Training loss 0.06270675361156464 Validation loss 0.05521342158317566 Accuracy 0.8560000061988831\n",
      "Iteration 17240 Training loss 0.058088015764951706 Validation loss 0.053612079471349716 Accuracy 0.8644999861717224\n",
      "Iteration 17250 Training loss 0.0675211176276207 Validation loss 0.057738397270441055 Accuracy 0.8525000214576721\n",
      "Iteration 17260 Training loss 0.06104863062500954 Validation loss 0.054201819002628326 Accuracy 0.8600000143051147\n",
      "Iteration 17270 Training loss 0.06492096185684204 Validation loss 0.05461467057466507 Accuracy 0.859499990940094\n",
      "Iteration 17280 Training loss 0.06462018191814423 Validation loss 0.05524981766939163 Accuracy 0.8575000166893005\n",
      "Iteration 17290 Training loss 0.06760422885417938 Validation loss 0.05726907402276993 Accuracy 0.8535000085830688\n",
      "Iteration 17300 Training loss 0.06088622659444809 Validation loss 0.05483835190534592 Accuracy 0.8560000061988831\n",
      "Iteration 17310 Training loss 0.06558161973953247 Validation loss 0.057587556540966034 Accuracy 0.8554999828338623\n",
      "Iteration 17320 Training loss 0.057987745851278305 Validation loss 0.05352908372879028 Accuracy 0.8640000224113464\n",
      "Iteration 17330 Training loss 0.0598352774977684 Validation loss 0.05591046065092087 Accuracy 0.859000027179718\n",
      "Iteration 17340 Training loss 0.06727275252342224 Validation loss 0.05654698610305786 Accuracy 0.859000027179718\n",
      "Iteration 17350 Training loss 0.06111019849777222 Validation loss 0.05405768007040024 Accuracy 0.8600000143051147\n",
      "Iteration 17360 Training loss 0.05456935986876488 Validation loss 0.05359262228012085 Accuracy 0.8604999780654907\n",
      "Iteration 17370 Training loss 0.06320879608392715 Validation loss 0.05532831698656082 Accuracy 0.8610000014305115\n",
      "Iteration 17380 Training loss 0.058038130402565 Validation loss 0.05537071079015732 Accuracy 0.8560000061988831\n",
      "Iteration 17390 Training loss 0.05893890559673309 Validation loss 0.05472002550959587 Accuracy 0.8604999780654907\n",
      "Iteration 17400 Training loss 0.060600653290748596 Validation loss 0.054769307374954224 Accuracy 0.859499990940094\n",
      "Iteration 17410 Training loss 0.06141315773129463 Validation loss 0.05560970678925514 Accuracy 0.8585000038146973\n",
      "Iteration 17420 Training loss 0.05959422513842583 Validation loss 0.05328904092311859 Accuracy 0.8610000014305115\n",
      "Iteration 17430 Training loss 0.06249464303255081 Validation loss 0.05639881640672684 Accuracy 0.8585000038146973\n",
      "Iteration 17440 Training loss 0.06981044262647629 Validation loss 0.059506870806217194 Accuracy 0.8525000214576721\n",
      "Iteration 17450 Training loss 0.056085459887981415 Validation loss 0.05409099906682968 Accuracy 0.8600000143051147\n",
      "Iteration 17460 Training loss 0.060744039714336395 Validation loss 0.054557982832193375 Accuracy 0.859000027179718\n",
      "Iteration 17470 Training loss 0.05859469994902611 Validation loss 0.05404537916183472 Accuracy 0.8604999780654907\n",
      "Iteration 17480 Training loss 0.06329572200775146 Validation loss 0.053380824625492096 Accuracy 0.8634999990463257\n",
      "Iteration 17490 Training loss 0.06448350846767426 Validation loss 0.05487881600856781 Accuracy 0.859499990940094\n",
      "Iteration 17500 Training loss 0.07260075211524963 Validation loss 0.05821260064840317 Accuracy 0.8539999723434448\n",
      "Iteration 17510 Training loss 0.06331685930490494 Validation loss 0.05592994764447212 Accuracy 0.8579999804496765\n",
      "Iteration 17520 Training loss 0.062236785888671875 Validation loss 0.054454199969768524 Accuracy 0.859499990940094\n",
      "Iteration 17530 Training loss 0.056396935135126114 Validation loss 0.06611867249011993 Accuracy 0.8410000205039978\n",
      "Iteration 17540 Training loss 0.06608401238918304 Validation loss 0.07258160412311554 Accuracy 0.8234999775886536\n",
      "Iteration 17550 Training loss 0.06193497031927109 Validation loss 0.06810342520475388 Accuracy 0.8349999785423279\n",
      "Iteration 17560 Training loss 0.057553041726350784 Validation loss 0.06484832614660263 Accuracy 0.8414999842643738\n",
      "Iteration 17570 Training loss 0.06697635352611542 Validation loss 0.06966935843229294 Accuracy 0.8299999833106995\n",
      "Iteration 17580 Training loss 0.05784732103347778 Validation loss 0.06478878855705261 Accuracy 0.8414999842643738\n",
      "Iteration 17590 Training loss 0.058541033416986465 Validation loss 0.06429731845855713 Accuracy 0.8429999947547913\n",
      "Iteration 17600 Training loss 0.06831645965576172 Validation loss 0.0732206404209137 Accuracy 0.8230000138282776\n",
      "Iteration 17610 Training loss 0.05728998780250549 Validation loss 0.06534869968891144 Accuracy 0.840499997138977\n",
      "Iteration 17620 Training loss 0.06341011077165604 Validation loss 0.06975043565034866 Accuracy 0.8295000195503235\n",
      "Iteration 17630 Training loss 0.06988034397363663 Validation loss 0.07196878641843796 Accuracy 0.8245000243186951\n",
      "Iteration 17640 Training loss 0.06180794909596443 Validation loss 0.06279417127370834 Accuracy 0.8464999794960022\n",
      "Iteration 17650 Training loss 0.06712989509105682 Validation loss 0.07381633669137955 Accuracy 0.8205000162124634\n",
      "Iteration 17660 Training loss 0.056616026908159256 Validation loss 0.06530342251062393 Accuracy 0.8399999737739563\n",
      "Iteration 17670 Training loss 0.0617559477686882 Validation loss 0.06628407537937164 Accuracy 0.8385000228881836\n",
      "Iteration 17680 Training loss 0.06630712747573853 Validation loss 0.0726940855383873 Accuracy 0.8234999775886536\n",
      "Iteration 17690 Training loss 0.06698223203420639 Validation loss 0.07063654810190201 Accuracy 0.8289999961853027\n",
      "Iteration 17700 Training loss 0.060133133083581924 Validation loss 0.06577759236097336 Accuracy 0.8395000100135803\n",
      "Iteration 17710 Training loss 0.05592316761612892 Validation loss 0.059107400476932526 Accuracy 0.8554999828338623\n",
      "Iteration 17720 Training loss 0.06309760361909866 Validation loss 0.06974457204341888 Accuracy 0.828499972820282\n",
      "Iteration 17730 Training loss 0.06231914833188057 Validation loss 0.06636407226324081 Accuracy 0.8360000252723694\n",
      "Iteration 17740 Training loss 0.06798496842384338 Validation loss 0.07304249703884125 Accuracy 0.8230000138282776\n",
      "Iteration 17750 Training loss 0.06228959560394287 Validation loss 0.07113797217607498 Accuracy 0.828499972820282\n",
      "Iteration 17760 Training loss 0.059649109840393066 Validation loss 0.06595832854509354 Accuracy 0.8389999866485596\n",
      "Iteration 17770 Training loss 0.06809224933385849 Validation loss 0.07091309875249863 Accuracy 0.8299999833106995\n",
      "Iteration 17780 Training loss 0.061101868748664856 Validation loss 0.06869342178106308 Accuracy 0.8335000276565552\n",
      "Iteration 17790 Training loss 0.05554136633872986 Validation loss 0.06402943283319473 Accuracy 0.843999981880188\n",
      "Iteration 17800 Training loss 0.057798080146312714 Validation loss 0.061452485620975494 Accuracy 0.8485000133514404\n",
      "Iteration 17810 Training loss 0.07068217545747757 Validation loss 0.07578215003013611 Accuracy 0.8230000138282776\n",
      "Iteration 17820 Training loss 0.05812743678689003 Validation loss 0.0573149211704731 Accuracy 0.8600000143051147\n",
      "Iteration 17830 Training loss 0.06397576630115509 Validation loss 0.07221025228500366 Accuracy 0.8255000114440918\n",
      "Iteration 17840 Training loss 0.05703694373369217 Validation loss 0.059027016162872314 Accuracy 0.8539999723434448\n",
      "Iteration 17850 Training loss 0.0645827129483223 Validation loss 0.0687885731458664 Accuracy 0.8335000276565552\n",
      "Iteration 17860 Training loss 0.059843700379133224 Validation loss 0.06521214544773102 Accuracy 0.8395000100135803\n",
      "Iteration 17870 Training loss 0.0621192641556263 Validation loss 0.06547299027442932 Accuracy 0.8424999713897705\n",
      "Iteration 17880 Training loss 0.06366652995347977 Validation loss 0.07343406975269318 Accuracy 0.8220000267028809\n",
      "Iteration 17890 Training loss 0.061566416174173355 Validation loss 0.06868285685777664 Accuracy 0.8339999914169312\n",
      "Iteration 17900 Training loss 0.05785383656620979 Validation loss 0.06735766679048538 Accuracy 0.8345000147819519\n",
      "Iteration 17910 Training loss 0.05903327837586403 Validation loss 0.06467019021511078 Accuracy 0.8450000286102295\n",
      "Iteration 17920 Training loss 0.06421075761318207 Validation loss 0.06879662722349167 Accuracy 0.8330000042915344\n",
      "Iteration 17930 Training loss 0.06879235059022903 Validation loss 0.07680022716522217 Accuracy 0.8140000104904175\n",
      "Iteration 17940 Training loss 0.06360321491956711 Validation loss 0.06385745853185654 Accuracy 0.8450000286102295\n",
      "Iteration 17950 Training loss 0.06354247033596039 Validation loss 0.06783529371023178 Accuracy 0.8339999914169312\n",
      "Iteration 17960 Training loss 0.06462184339761734 Validation loss 0.06918004900217056 Accuracy 0.8314999938011169\n",
      "Iteration 17970 Training loss 0.06089536100625992 Validation loss 0.06707202643156052 Accuracy 0.8370000123977661\n",
      "Iteration 17980 Training loss 0.06242159754037857 Validation loss 0.0692058578133583 Accuracy 0.8330000042915344\n",
      "Iteration 17990 Training loss 0.05562002584338188 Validation loss 0.0626126080751419 Accuracy 0.8479999899864197\n",
      "Iteration 18000 Training loss 0.055471912026405334 Validation loss 0.06155792623758316 Accuracy 0.8485000133514404\n",
      "Iteration 18010 Training loss 0.06012199446558952 Validation loss 0.06772500276565552 Accuracy 0.8339999914169312\n",
      "Iteration 18020 Training loss 0.06423678249120712 Validation loss 0.06934711337089539 Accuracy 0.8324999809265137\n",
      "Iteration 18030 Training loss 0.06113767251372337 Validation loss 0.06696339696645737 Accuracy 0.8355000019073486\n",
      "Iteration 18040 Training loss 0.06489181518554688 Validation loss 0.06708397716283798 Accuracy 0.8364999890327454\n",
      "Iteration 18050 Training loss 0.06290869414806366 Validation loss 0.06663224846124649 Accuracy 0.8370000123977661\n",
      "Iteration 18060 Training loss 0.068819060921669 Validation loss 0.07038874924182892 Accuracy 0.828499972820282\n",
      "Iteration 18070 Training loss 0.06238025054335594 Validation loss 0.07096416503190994 Accuracy 0.828000009059906\n",
      "Iteration 18080 Training loss 0.06266669929027557 Validation loss 0.0678272396326065 Accuracy 0.8339999914169312\n",
      "Iteration 18090 Training loss 0.06189176067709923 Validation loss 0.0630386620759964 Accuracy 0.847000002861023\n",
      "Iteration 18100 Training loss 0.05653248727321625 Validation loss 0.06381551176309586 Accuracy 0.843999981880188\n",
      "Iteration 18110 Training loss 0.06628461927175522 Validation loss 0.07285565882921219 Accuracy 0.8255000114440918\n",
      "Iteration 18120 Training loss 0.06084974855184555 Validation loss 0.06606388837099075 Accuracy 0.843500018119812\n",
      "Iteration 18130 Training loss 0.049297548830509186 Validation loss 0.05851367115974426 Accuracy 0.8554999828338623\n",
      "Iteration 18140 Training loss 0.05812714993953705 Validation loss 0.05627712979912758 Accuracy 0.8604999780654907\n",
      "Iteration 18150 Training loss 0.07052379101514816 Validation loss 0.0688135102391243 Accuracy 0.8335000276565552\n",
      "Iteration 18160 Training loss 0.055051784962415695 Validation loss 0.06310954689979553 Accuracy 0.8475000262260437\n",
      "Iteration 18170 Training loss 0.06474495679140091 Validation loss 0.06708678603172302 Accuracy 0.8360000252723694\n",
      "Iteration 18180 Training loss 0.06729843467473984 Validation loss 0.07290966808795929 Accuracy 0.8195000290870667\n",
      "Iteration 18190 Training loss 0.0672055035829544 Validation loss 0.07268788665533066 Accuracy 0.824999988079071\n",
      "Iteration 18200 Training loss 0.0667620599269867 Validation loss 0.07502727210521698 Accuracy 0.815500020980835\n",
      "Iteration 18210 Training loss 0.058082327246665955 Validation loss 0.06528709083795547 Accuracy 0.8424999713897705\n",
      "Iteration 18220 Training loss 0.07143072783946991 Validation loss 0.07541496306657791 Accuracy 0.8149999976158142\n",
      "Iteration 18230 Training loss 0.06374925374984741 Validation loss 0.06768305599689484 Accuracy 0.8339999914169312\n",
      "Iteration 18240 Training loss 0.06265851110219955 Validation loss 0.06620475649833679 Accuracy 0.8360000252723694\n",
      "Iteration 18250 Training loss 0.05245622619986534 Validation loss 0.05826739966869354 Accuracy 0.8550000190734863\n",
      "Iteration 18260 Training loss 0.062040314078330994 Validation loss 0.0696973204612732 Accuracy 0.8314999938011169\n",
      "Iteration 18270 Training loss 0.055283550173044205 Validation loss 0.06220531463623047 Accuracy 0.8485000133514404\n",
      "Iteration 18280 Training loss 0.0707472413778305 Validation loss 0.07734263688325882 Accuracy 0.8100000023841858\n",
      "Iteration 18290 Training loss 0.06491352617740631 Validation loss 0.07897330820560455 Accuracy 0.8069999814033508\n",
      "Iteration 18300 Training loss 0.06770128011703491 Validation loss 0.07446511834859848 Accuracy 0.8209999799728394\n",
      "Iteration 18310 Training loss 0.05501551926136017 Validation loss 0.05894092097878456 Accuracy 0.8550000190734863\n",
      "Iteration 18320 Training loss 0.055222541093826294 Validation loss 0.05388861522078514 Accuracy 0.8650000095367432\n",
      "Iteration 18330 Training loss 0.05914951488375664 Validation loss 0.05418964847922325 Accuracy 0.862500011920929\n",
      "Iteration 18340 Training loss 0.06333712488412857 Validation loss 0.05582469701766968 Accuracy 0.8579999804496765\n",
      "Iteration 18350 Training loss 0.07075318694114685 Validation loss 0.058367058634757996 Accuracy 0.8550000190734863\n",
      "Iteration 18360 Training loss 0.06281912326812744 Validation loss 0.05526192858815193 Accuracy 0.8565000295639038\n",
      "Iteration 18370 Training loss 0.05582113191485405 Validation loss 0.05396513640880585 Accuracy 0.8600000143051147\n",
      "Iteration 18380 Training loss 0.06756328791379929 Validation loss 0.055695030838251114 Accuracy 0.8569999933242798\n",
      "Iteration 18390 Training loss 0.06552485376596451 Validation loss 0.05380888655781746 Accuracy 0.8615000247955322\n",
      "Iteration 18400 Training loss 0.066807821393013 Validation loss 0.055410560220479965 Accuracy 0.8585000038146973\n",
      "Iteration 18410 Training loss 0.06805931031703949 Validation loss 0.05893055722117424 Accuracy 0.8560000061988831\n",
      "Iteration 18420 Training loss 0.06349565088748932 Validation loss 0.05570484325289726 Accuracy 0.8560000061988831\n",
      "Iteration 18430 Training loss 0.0601833276450634 Validation loss 0.0536893755197525 Accuracy 0.862500011920929\n",
      "Iteration 18440 Training loss 0.07674238085746765 Validation loss 0.061923548579216 Accuracy 0.8485000133514404\n",
      "Iteration 18450 Training loss 0.05811852589249611 Validation loss 0.05411994457244873 Accuracy 0.8619999885559082\n",
      "Iteration 18460 Training loss 0.06326665729284286 Validation loss 0.05639878660440445 Accuracy 0.8560000061988831\n",
      "Iteration 18470 Training loss 0.06496119499206543 Validation loss 0.05619734153151512 Accuracy 0.8585000038146973\n",
      "Iteration 18480 Training loss 0.05921832099556923 Validation loss 0.05399719625711441 Accuracy 0.8634999990463257\n",
      "Iteration 18490 Training loss 0.06172866374254227 Validation loss 0.05734969675540924 Accuracy 0.8554999828338623\n",
      "Iteration 18500 Training loss 0.05911698192358017 Validation loss 0.053809236735105515 Accuracy 0.8619999885559082\n",
      "Iteration 18510 Training loss 0.06324875354766846 Validation loss 0.05431697145104408 Accuracy 0.8619999885559082\n",
      "Iteration 18520 Training loss 0.062143560498952866 Validation loss 0.05377931892871857 Accuracy 0.8604999780654907\n",
      "Iteration 18530 Training loss 0.06230046972632408 Validation loss 0.05657527223229408 Accuracy 0.8560000061988831\n",
      "Iteration 18540 Training loss 0.0639207735657692 Validation loss 0.0556253045797348 Accuracy 0.8560000061988831\n",
      "Iteration 18550 Training loss 0.060163214802742004 Validation loss 0.05479047819972038 Accuracy 0.8569999933242798\n",
      "Iteration 18560 Training loss 0.05832091346383095 Validation loss 0.05394536256790161 Accuracy 0.8619999885559082\n",
      "Iteration 18570 Training loss 0.061550986021757126 Validation loss 0.05380241572856903 Accuracy 0.8600000143051147\n",
      "Iteration 18580 Training loss 0.055227722972631454 Validation loss 0.05666225403547287 Accuracy 0.859499990940094\n",
      "Iteration 18590 Training loss 0.06490657478570938 Validation loss 0.07102368026971817 Accuracy 0.8270000219345093\n",
      "Iteration 18600 Training loss 0.0661628320813179 Validation loss 0.07362362742424011 Accuracy 0.8245000243186951\n",
      "Iteration 18610 Training loss 0.06253266334533691 Validation loss 0.07036708295345306 Accuracy 0.8289999961853027\n",
      "Iteration 18620 Training loss 0.05864430218935013 Validation loss 0.0636274516582489 Accuracy 0.8454999923706055\n",
      "Iteration 18630 Training loss 0.06489857286214828 Validation loss 0.06863940507173538 Accuracy 0.8320000171661377\n",
      "Iteration 18640 Training loss 0.06772533059120178 Validation loss 0.068158358335495 Accuracy 0.8335000276565552\n",
      "Iteration 18650 Training loss 0.06112613528966904 Validation loss 0.06613984704017639 Accuracy 0.8389999866485596\n",
      "Iteration 18660 Training loss 0.06239691749215126 Validation loss 0.07047682255506516 Accuracy 0.8309999704360962\n",
      "Iteration 18670 Training loss 0.07038682699203491 Validation loss 0.07020124793052673 Accuracy 0.8274999856948853\n",
      "Iteration 18680 Training loss 0.05721966549754143 Validation loss 0.06654021888971329 Accuracy 0.8395000100135803\n",
      "Iteration 18690 Training loss 0.06299076974391937 Validation loss 0.06658804416656494 Accuracy 0.8395000100135803\n",
      "Iteration 18700 Training loss 0.06929121166467667 Validation loss 0.07634599506855011 Accuracy 0.8165000081062317\n",
      "Iteration 18710 Training loss 0.06374718248844147 Validation loss 0.06923176348209381 Accuracy 0.8320000171661377\n",
      "Iteration 18720 Training loss 0.06653311103582382 Validation loss 0.07180552184581757 Accuracy 0.8264999985694885\n",
      "Iteration 18730 Training loss 0.06123233214020729 Validation loss 0.06822599470615387 Accuracy 0.8349999785423279\n",
      "Iteration 18740 Training loss 0.05669772997498512 Validation loss 0.06013871356844902 Accuracy 0.8514999747276306\n",
      "Iteration 18750 Training loss 0.06431033462285995 Validation loss 0.073362797498703 Accuracy 0.8240000009536743\n",
      "Iteration 18760 Training loss 0.0658787414431572 Validation loss 0.06845118850469589 Accuracy 0.8345000147819519\n",
      "Iteration 18770 Training loss 0.05997241288423538 Validation loss 0.06699525564908981 Accuracy 0.8379999995231628\n",
      "Iteration 18780 Training loss 0.062029220163822174 Validation loss 0.06177421659231186 Accuracy 0.847000002861023\n",
      "Iteration 18790 Training loss 0.06401229649782181 Validation loss 0.07347927987575531 Accuracy 0.8209999799728394\n",
      "Iteration 18800 Training loss 0.06446238607168198 Validation loss 0.07383085042238235 Accuracy 0.8234999775886536\n",
      "Iteration 18810 Training loss 0.061820317059755325 Validation loss 0.0705885961651802 Accuracy 0.8295000195503235\n",
      "Iteration 18820 Training loss 0.0678439736366272 Validation loss 0.07547294348478317 Accuracy 0.8209999799728394\n",
      "Iteration 18830 Training loss 0.05730630084872246 Validation loss 0.06507395952939987 Accuracy 0.8414999842643738\n",
      "Iteration 18840 Training loss 0.06792618334293365 Validation loss 0.0713912844657898 Accuracy 0.8274999856948853\n",
      "Iteration 18850 Training loss 0.06158353015780449 Validation loss 0.06939414888620377 Accuracy 0.8299999833106995\n",
      "Iteration 18860 Training loss 0.06381534039974213 Validation loss 0.07040536403656006 Accuracy 0.8289999961853027\n",
      "Iteration 18870 Training loss 0.0658719465136528 Validation loss 0.06712111830711365 Accuracy 0.8385000228881836\n",
      "Iteration 18880 Training loss 0.05876852944493294 Validation loss 0.06390083581209183 Accuracy 0.843999981880188\n",
      "Iteration 18890 Training loss 0.06252358853816986 Validation loss 0.06883325427770615 Accuracy 0.8339999914169312\n",
      "Iteration 18900 Training loss 0.06148046627640724 Validation loss 0.06387598812580109 Accuracy 0.843500018119812\n",
      "Iteration 18910 Training loss 0.06297099590301514 Validation loss 0.06697697937488556 Accuracy 0.8364999890327454\n",
      "Iteration 18920 Training loss 0.06112198159098625 Validation loss 0.0639423131942749 Accuracy 0.8454999923706055\n",
      "Iteration 18930 Training loss 0.06075085327029228 Validation loss 0.07036291807889938 Accuracy 0.8314999938011169\n",
      "Iteration 18940 Training loss 0.060145530849695206 Validation loss 0.061764735728502274 Accuracy 0.8464999794960022\n",
      "Iteration 18950 Training loss 0.06427435576915741 Validation loss 0.07082455605268478 Accuracy 0.828499972820282\n",
      "Iteration 18960 Training loss 0.06672599911689758 Validation loss 0.07043188810348511 Accuracy 0.8270000219345093\n",
      "Iteration 18970 Training loss 0.06033407524228096 Validation loss 0.06703060865402222 Accuracy 0.8379999995231628\n",
      "Iteration 18980 Training loss 0.06811539828777313 Validation loss 0.07362464815378189 Accuracy 0.8224999904632568\n",
      "Iteration 18990 Training loss 0.06228315830230713 Validation loss 0.06429005414247513 Accuracy 0.8424999713897705\n",
      "Iteration 19000 Training loss 0.06589630246162415 Validation loss 0.07079577445983887 Accuracy 0.8270000219345093\n",
      "Iteration 19010 Training loss 0.05933353304862976 Validation loss 0.0630989745259285 Accuracy 0.8445000052452087\n",
      "Iteration 19020 Training loss 0.07264735549688339 Validation loss 0.081557996571064 Accuracy 0.800000011920929\n",
      "Iteration 19030 Training loss 0.05760510638356209 Validation loss 0.06703046709299088 Accuracy 0.8374999761581421\n",
      "Iteration 19040 Training loss 0.060754768550395966 Validation loss 0.06375580281019211 Accuracy 0.8429999947547913\n",
      "Iteration 19050 Training loss 0.06074906140565872 Validation loss 0.06572036445140839 Accuracy 0.8389999866485596\n",
      "Iteration 19060 Training loss 0.06314347684383392 Validation loss 0.06303650885820389 Accuracy 0.8445000052452087\n",
      "Iteration 19070 Training loss 0.05872787535190582 Validation loss 0.061379142105579376 Accuracy 0.8495000004768372\n",
      "Iteration 19080 Training loss 0.06180494651198387 Validation loss 0.06940961629152298 Accuracy 0.8305000066757202\n",
      "Iteration 19090 Training loss 0.06530530005693436 Validation loss 0.07163798063993454 Accuracy 0.8270000219345093\n",
      "Iteration 19100 Training loss 0.06680712848901749 Validation loss 0.06900733709335327 Accuracy 0.8314999938011169\n",
      "Iteration 19110 Training loss 0.06359173357486725 Validation loss 0.06812524050474167 Accuracy 0.8314999938011169\n",
      "Iteration 19120 Training loss 0.06767827272415161 Validation loss 0.07099071145057678 Accuracy 0.828000009059906\n",
      "Iteration 19130 Training loss 0.057489100843667984 Validation loss 0.06339241564273834 Accuracy 0.8460000157356262\n",
      "Iteration 19140 Training loss 0.06634604185819626 Validation loss 0.06913034617900848 Accuracy 0.8330000042915344\n",
      "Iteration 19150 Training loss 0.06329546868801117 Validation loss 0.06832665950059891 Accuracy 0.8330000042915344\n",
      "Iteration 19160 Training loss 0.056002113968133926 Validation loss 0.05628010258078575 Accuracy 0.862500011920929\n",
      "Iteration 19170 Training loss 0.06710027158260345 Validation loss 0.06994237750768661 Accuracy 0.8295000195503235\n",
      "Iteration 19180 Training loss 0.06034841015934944 Validation loss 0.06636729836463928 Accuracy 0.8389999866485596\n",
      "Iteration 19190 Training loss 0.0598946250975132 Validation loss 0.06687145680189133 Accuracy 0.8360000252723694\n",
      "Iteration 19200 Training loss 0.07701700180768967 Validation loss 0.08404379338026047 Accuracy 0.7954999804496765\n",
      "Iteration 19210 Training loss 0.05459477752447128 Validation loss 0.060408663004636765 Accuracy 0.8529999852180481\n",
      "Iteration 19220 Training loss 0.06267023831605911 Validation loss 0.06789150089025497 Accuracy 0.8339999914169312\n",
      "Iteration 19230 Training loss 0.06687884032726288 Validation loss 0.07133888453245163 Accuracy 0.8255000114440918\n",
      "Iteration 19240 Training loss 0.06478570401668549 Validation loss 0.06689530611038208 Accuracy 0.8385000228881836\n",
      "Iteration 19250 Training loss 0.06311865150928497 Validation loss 0.06571774184703827 Accuracy 0.8374999761581421\n",
      "Iteration 19260 Training loss 0.07066123932600021 Validation loss 0.07161764800548553 Accuracy 0.8264999985694885\n",
      "Iteration 19270 Training loss 0.0529046393930912 Validation loss 0.05703749135136604 Accuracy 0.8619999885559082\n",
      "Iteration 19280 Training loss 0.07101234793663025 Validation loss 0.07435238361358643 Accuracy 0.8205000162124634\n",
      "Iteration 19290 Training loss 0.06626639515161514 Validation loss 0.07056045532226562 Accuracy 0.824999988079071\n",
      "Iteration 19300 Training loss 0.06294625252485275 Validation loss 0.06382063776254654 Accuracy 0.8445000052452087\n",
      "Iteration 19310 Training loss 0.06443756818771362 Validation loss 0.06481612473726273 Accuracy 0.840499997138977\n",
      "Iteration 19320 Training loss 0.061246827244758606 Validation loss 0.06485611200332642 Accuracy 0.8414999842643738\n",
      "Iteration 19330 Training loss 0.06268823891878128 Validation loss 0.06879456341266632 Accuracy 0.8335000276565552\n",
      "Iteration 19340 Training loss 0.058572594076395035 Validation loss 0.06286516785621643 Accuracy 0.8454999923706055\n",
      "Iteration 19350 Training loss 0.06112729758024216 Validation loss 0.06756654381752014 Accuracy 0.8345000147819519\n",
      "Iteration 19360 Training loss 0.06235475093126297 Validation loss 0.06800145655870438 Accuracy 0.8355000019073486\n",
      "Iteration 19370 Training loss 0.0655926838517189 Validation loss 0.070099376142025 Accuracy 0.8295000195503235\n",
      "Iteration 19380 Training loss 0.06364879012107849 Validation loss 0.06835830956697464 Accuracy 0.8335000276565552\n",
      "Iteration 19390 Training loss 0.07029904425144196 Validation loss 0.07400581985712051 Accuracy 0.8190000057220459\n",
      "Iteration 19400 Training loss 0.06974388659000397 Validation loss 0.07067851722240448 Accuracy 0.828000009059906\n",
      "Iteration 19410 Training loss 0.05990505963563919 Validation loss 0.06671743094921112 Accuracy 0.8360000252723694\n",
      "Iteration 19420 Training loss 0.05992555990815163 Validation loss 0.060775186866521835 Accuracy 0.8495000004768372\n",
      "Iteration 19430 Training loss 0.06519249826669693 Validation loss 0.06648203730583191 Accuracy 0.8339999914169312\n",
      "Iteration 19440 Training loss 0.054878801107406616 Validation loss 0.05969624221324921 Accuracy 0.8539999723434448\n",
      "Iteration 19450 Training loss 0.061432626098394394 Validation loss 0.06856820732355118 Accuracy 0.8309999704360962\n",
      "Iteration 19460 Training loss 0.05765658989548683 Validation loss 0.064302459359169 Accuracy 0.840499997138977\n",
      "Iteration 19470 Training loss 0.06265292316675186 Validation loss 0.0661383792757988 Accuracy 0.8364999890327454\n",
      "Iteration 19480 Training loss 0.05801374465227127 Validation loss 0.06194518879055977 Accuracy 0.8485000133514404\n",
      "Iteration 19490 Training loss 0.07093971967697144 Validation loss 0.07195120304822922 Accuracy 0.828499972820282\n",
      "Iteration 19500 Training loss 0.05576550215482712 Validation loss 0.05629659444093704 Accuracy 0.8610000014305115\n",
      "Iteration 19510 Training loss 0.06165425106883049 Validation loss 0.07014117389917374 Accuracy 0.8295000195503235\n",
      "Iteration 19520 Training loss 0.06727917492389679 Validation loss 0.07143525779247284 Accuracy 0.8245000243186951\n",
      "Iteration 19530 Training loss 0.06501638889312744 Validation loss 0.068153515458107 Accuracy 0.8339999914169312\n",
      "Iteration 19540 Training loss 0.06973940879106522 Validation loss 0.06951846182346344 Accuracy 0.8314999938011169\n",
      "Iteration 19550 Training loss 0.061410143971443176 Validation loss 0.06524258106946945 Accuracy 0.8385000228881836\n",
      "Iteration 19560 Training loss 0.05687330663204193 Validation loss 0.06357985734939575 Accuracy 0.8475000262260437\n",
      "Iteration 19570 Training loss 0.06827027350664139 Validation loss 0.07102460414171219 Accuracy 0.8289999961853027\n",
      "Iteration 19580 Training loss 0.06008953973650932 Validation loss 0.06695617735385895 Accuracy 0.8385000228881836\n",
      "Iteration 19590 Training loss 0.06053677201271057 Validation loss 0.06596751511096954 Accuracy 0.8385000228881836\n",
      "Iteration 19600 Training loss 0.060243263840675354 Validation loss 0.06370614469051361 Accuracy 0.8464999794960022\n",
      "Iteration 19610 Training loss 0.05471247062087059 Validation loss 0.058398209512233734 Accuracy 0.859499990940094\n",
      "Iteration 19620 Training loss 0.057431407272815704 Validation loss 0.06462441384792328 Accuracy 0.8445000052452087\n",
      "Iteration 19630 Training loss 0.06882401555776596 Validation loss 0.0745648667216301 Accuracy 0.8224999904632568\n",
      "Iteration 19640 Training loss 0.06360385566949844 Validation loss 0.06883332878351212 Accuracy 0.8324999809265137\n",
      "Iteration 19650 Training loss 0.06219654157757759 Validation loss 0.07087786495685577 Accuracy 0.828499972820282\n",
      "Iteration 19660 Training loss 0.06078152358531952 Validation loss 0.07146231085062027 Accuracy 0.8264999985694885\n",
      "Iteration 19670 Training loss 0.058048635721206665 Validation loss 0.06271934509277344 Accuracy 0.8485000133514404\n",
      "Iteration 19680 Training loss 0.05537934973835945 Validation loss 0.06459271162748337 Accuracy 0.8429999947547913\n",
      "Iteration 19690 Training loss 0.06534663587808609 Validation loss 0.07011096179485321 Accuracy 0.828499972820282\n",
      "Iteration 19700 Training loss 0.06279832124710083 Validation loss 0.06728999316692352 Accuracy 0.8360000252723694\n",
      "Iteration 19710 Training loss 0.06581782549619675 Validation loss 0.07149525731801987 Accuracy 0.8289999961853027\n",
      "Iteration 19720 Training loss 0.05868757888674736 Validation loss 0.05847729742527008 Accuracy 0.8550000190734863\n",
      "Iteration 19730 Training loss 0.06724856048822403 Validation loss 0.07866451144218445 Accuracy 0.8090000152587891\n",
      "Iteration 19740 Training loss 0.06087074056267738 Validation loss 0.0638081282377243 Accuracy 0.843500018119812\n",
      "Iteration 19750 Training loss 0.06432679295539856 Validation loss 0.07165613770484924 Accuracy 0.8264999985694885\n",
      "Iteration 19760 Training loss 0.06439228355884552 Validation loss 0.0697554498910904 Accuracy 0.8305000066757202\n",
      "Iteration 19770 Training loss 0.05883120000362396 Validation loss 0.06260904669761658 Accuracy 0.8485000133514404\n",
      "Iteration 19780 Training loss 0.06542294472455978 Validation loss 0.07036120444536209 Accuracy 0.828499972820282\n",
      "Iteration 19790 Training loss 0.06297115236520767 Validation loss 0.06834356486797333 Accuracy 0.8360000252723694\n",
      "Iteration 19800 Training loss 0.06321220844984055 Validation loss 0.06465791165828705 Accuracy 0.8414999842643738\n",
      "Iteration 19810 Training loss 0.06567903608083725 Validation loss 0.0697476789355278 Accuracy 0.8299999833106995\n",
      "Iteration 19820 Training loss 0.05843978002667427 Validation loss 0.06095993518829346 Accuracy 0.8519999980926514\n",
      "Iteration 19830 Training loss 0.06534566730260849 Validation loss 0.06807322800159454 Accuracy 0.8339999914169312\n",
      "Iteration 19840 Training loss 0.06576351821422577 Validation loss 0.06838757544755936 Accuracy 0.8339999914169312\n",
      "Iteration 19850 Training loss 0.05993084982037544 Validation loss 0.06147868186235428 Accuracy 0.8500000238418579\n",
      "Iteration 19860 Training loss 0.06604380905628204 Validation loss 0.0750763937830925 Accuracy 0.8125\n",
      "Iteration 19870 Training loss 0.062888503074646 Validation loss 0.06455683708190918 Accuracy 0.8410000205039978\n",
      "Iteration 19880 Training loss 0.060002926737070084 Validation loss 0.06554762274026871 Accuracy 0.8414999842643738\n",
      "Iteration 19890 Training loss 0.06668607890605927 Validation loss 0.07203762233257294 Accuracy 0.8264999985694885\n",
      "Iteration 19900 Training loss 0.06167631596326828 Validation loss 0.07128161191940308 Accuracy 0.8274999856948853\n",
      "Iteration 19910 Training loss 0.062025245279073715 Validation loss 0.06726839393377304 Accuracy 0.8385000228881836\n",
      "Iteration 19920 Training loss 0.06258098781108856 Validation loss 0.07109048217535019 Accuracy 0.8295000195503235\n",
      "Iteration 19930 Training loss 0.05658211559057236 Validation loss 0.05749453604221344 Accuracy 0.8575000166893005\n",
      "Iteration 19940 Training loss 0.061474982649087906 Validation loss 0.06817474216222763 Accuracy 0.8339999914169312\n",
      "Iteration 19950 Training loss 0.06468378007411957 Validation loss 0.07113058120012283 Accuracy 0.828000009059906\n",
      "Iteration 19960 Training loss 0.06968291103839874 Validation loss 0.07430491596460342 Accuracy 0.8190000057220459\n",
      "Iteration 19970 Training loss 0.06399356573820114 Validation loss 0.06409188359975815 Accuracy 0.8429999947547913\n",
      "Iteration 19980 Training loss 0.06467065215110779 Validation loss 0.07045406848192215 Accuracy 0.8274999856948853\n",
      "Iteration 19990 Training loss 0.06470733880996704 Validation loss 0.07004127651453018 Accuracy 0.8274999856948853\n",
      "Iteration 20000 Training loss 0.05999332666397095 Validation loss 0.06395470350980759 Accuracy 0.843500018119812\n",
      "Iteration 20010 Training loss 0.06625454127788544 Validation loss 0.07273613661527634 Accuracy 0.8240000009536743\n",
      "Iteration 20020 Training loss 0.059649430215358734 Validation loss 0.06935897469520569 Accuracy 0.8314999938011169\n",
      "Iteration 20030 Training loss 0.056856296956539154 Validation loss 0.06578274816274643 Accuracy 0.8389999866485596\n",
      "Iteration 20040 Training loss 0.05998118966817856 Validation loss 0.06687689572572708 Accuracy 0.8364999890327454\n",
      "Iteration 20050 Training loss 0.06520478427410126 Validation loss 0.07274527102708817 Accuracy 0.8270000219345093\n",
      "Iteration 20060 Training loss 0.06554829329252243 Validation loss 0.06578461825847626 Accuracy 0.8374999761581421\n",
      "Iteration 20070 Training loss 0.059387121349573135 Validation loss 0.062468938529491425 Accuracy 0.8454999923706055\n",
      "Iteration 20080 Training loss 0.06573057919740677 Validation loss 0.07070264965295792 Accuracy 0.828499972820282\n",
      "Iteration 20090 Training loss 0.07255089282989502 Validation loss 0.08055967837572098 Accuracy 0.8015000224113464\n",
      "Iteration 20100 Training loss 0.06302610784769058 Validation loss 0.069986492395401 Accuracy 0.8289999961853027\n",
      "Iteration 20110 Training loss 0.05897260084748268 Validation loss 0.06399954855442047 Accuracy 0.8420000076293945\n",
      "Iteration 20120 Training loss 0.07180259376764297 Validation loss 0.07833747565746307 Accuracy 0.8109999895095825\n",
      "Iteration 20130 Training loss 0.059907373040914536 Validation loss 0.06405861675739288 Accuracy 0.8445000052452087\n",
      "Iteration 20140 Training loss 0.05987325310707092 Validation loss 0.06816107034683228 Accuracy 0.8349999785423279\n",
      "Iteration 20150 Training loss 0.06415218114852905 Validation loss 0.07098384946584702 Accuracy 0.8274999856948853\n",
      "Iteration 20160 Training loss 0.060962799936532974 Validation loss 0.06408452242612839 Accuracy 0.843500018119812\n",
      "Iteration 20170 Training loss 0.05693303048610687 Validation loss 0.06412407755851746 Accuracy 0.8424999713897705\n",
      "Iteration 20180 Training loss 0.06514016538858414 Validation loss 0.06771636754274368 Accuracy 0.8345000147819519\n",
      "Iteration 20190 Training loss 0.0717925876379013 Validation loss 0.07503233104944229 Accuracy 0.8215000033378601\n",
      "Iteration 20200 Training loss 0.06899572908878326 Validation loss 0.07125292718410492 Accuracy 0.828499972820282\n",
      "Iteration 20210 Training loss 0.06126653403043747 Validation loss 0.06575146317481995 Accuracy 0.8364999890327454\n",
      "Iteration 20220 Training loss 0.07395707815885544 Validation loss 0.07661435753107071 Accuracy 0.8180000185966492\n",
      "Iteration 20230 Training loss 0.058189742267131805 Validation loss 0.06115558370947838 Accuracy 0.8485000133514404\n",
      "Iteration 20240 Training loss 0.05620171129703522 Validation loss 0.05345284938812256 Accuracy 0.8619999885559082\n",
      "Iteration 20250 Training loss 0.06817631423473358 Validation loss 0.0583798848092556 Accuracy 0.8544999957084656\n",
      "Iteration 20260 Training loss 0.06891560554504395 Validation loss 0.054509300738573074 Accuracy 0.8610000014305115\n",
      "Iteration 20270 Training loss 0.060724008828401566 Validation loss 0.054495587944984436 Accuracy 0.8600000143051147\n",
      "Iteration 20280 Training loss 0.06773555278778076 Validation loss 0.056425027549266815 Accuracy 0.8569999933242798\n",
      "Iteration 20290 Training loss 0.060428470373153687 Validation loss 0.054750144481658936 Accuracy 0.8610000014305115\n",
      "Iteration 20300 Training loss 0.0666150227189064 Validation loss 0.05816365033388138 Accuracy 0.8539999723434448\n",
      "Iteration 20310 Training loss 0.05843409150838852 Validation loss 0.05612833425402641 Accuracy 0.8539999723434448\n",
      "Iteration 20320 Training loss 0.062487564980983734 Validation loss 0.055405523627996445 Accuracy 0.8565000295639038\n",
      "Iteration 20330 Training loss 0.06214620918035507 Validation loss 0.05440560728311539 Accuracy 0.8615000247955322\n",
      "Iteration 20340 Training loss 0.06844760477542877 Validation loss 0.05822504311800003 Accuracy 0.8565000295639038\n",
      "Iteration 20350 Training loss 0.061138078570365906 Validation loss 0.055569153279066086 Accuracy 0.8560000061988831\n",
      "Iteration 20360 Training loss 0.060067810118198395 Validation loss 0.05486780405044556 Accuracy 0.8600000143051147\n",
      "Iteration 20370 Training loss 0.0650775209069252 Validation loss 0.05674373731017113 Accuracy 0.8554999828338623\n",
      "Iteration 20380 Training loss 0.06026150658726692 Validation loss 0.054318059235811234 Accuracy 0.8640000224113464\n",
      "Iteration 20390 Training loss 0.061941877007484436 Validation loss 0.05553770065307617 Accuracy 0.8560000061988831\n",
      "Iteration 20400 Training loss 0.06674738973379135 Validation loss 0.055058516561985016 Accuracy 0.859000027179718\n",
      "Iteration 20410 Training loss 0.06799174100160599 Validation loss 0.05754990875720978 Accuracy 0.8544999957084656\n",
      "Iteration 20420 Training loss 0.06901419162750244 Validation loss 0.05566967651247978 Accuracy 0.8560000061988831\n",
      "Iteration 20430 Training loss 0.06721613556146622 Validation loss 0.05851300060749054 Accuracy 0.8554999828338623\n",
      "Iteration 20440 Training loss 0.05744365230202675 Validation loss 0.05441061407327652 Accuracy 0.8604999780654907\n",
      "Iteration 20450 Training loss 0.07393040508031845 Validation loss 0.05890775844454765 Accuracy 0.8535000085830688\n",
      "Iteration 20460 Training loss 0.06037435680627823 Validation loss 0.05467076227068901 Accuracy 0.8604999780654907\n",
      "Iteration 20470 Training loss 0.06951363384723663 Validation loss 0.05954708531498909 Accuracy 0.8525000214576721\n",
      "Iteration 20480 Training loss 0.06425361335277557 Validation loss 0.055285949259996414 Accuracy 0.8569999933242798\n",
      "Iteration 20490 Training loss 0.06313002854585648 Validation loss 0.05745243281126022 Accuracy 0.8565000295639038\n",
      "Iteration 20500 Training loss 0.05443594232201576 Validation loss 0.054364535957574844 Accuracy 0.8634999990463257\n",
      "Iteration 20510 Training loss 0.05829191952943802 Validation loss 0.05377736687660217 Accuracy 0.8619999885559082\n",
      "Iteration 20520 Training loss 0.06299816817045212 Validation loss 0.05583112686872482 Accuracy 0.8569999933242798\n",
      "Iteration 20530 Training loss 0.06510382890701294 Validation loss 0.05510660633444786 Accuracy 0.8579999804496765\n",
      "Iteration 20540 Training loss 0.0646003857254982 Validation loss 0.05724382773041725 Accuracy 0.8550000190734863\n",
      "Iteration 20550 Training loss 0.06324880570173264 Validation loss 0.0549997016787529 Accuracy 0.859000027179718\n",
      "Iteration 20560 Training loss 0.055055923759937286 Validation loss 0.0543006956577301 Accuracy 0.8600000143051147\n",
      "Iteration 20570 Training loss 0.0670098215341568 Validation loss 0.05554909631609917 Accuracy 0.859000027179718\n",
      "Iteration 20580 Training loss 0.05522426962852478 Validation loss 0.053721897304058075 Accuracy 0.862500011920929\n",
      "Iteration 20590 Training loss 0.06386374682188034 Validation loss 0.05671839043498039 Accuracy 0.8554999828338623\n",
      "Iteration 20600 Training loss 0.06521110236644745 Validation loss 0.056259192526340485 Accuracy 0.8535000085830688\n",
      "Iteration 20610 Training loss 0.06299977004528046 Validation loss 0.054621949791908264 Accuracy 0.8604999780654907\n",
      "Iteration 20620 Training loss 0.05520869046449661 Validation loss 0.053654659539461136 Accuracy 0.862500011920929\n",
      "Iteration 20630 Training loss 0.056314617395401 Validation loss 0.05688909813761711 Accuracy 0.859499990940094\n",
      "Iteration 20640 Training loss 0.06475454568862915 Validation loss 0.07516445219516754 Accuracy 0.8174999952316284\n",
      "Iteration 20650 Training loss 0.05917326360940933 Validation loss 0.0664348304271698 Accuracy 0.8355000019073486\n",
      "Iteration 20660 Training loss 0.05843222141265869 Validation loss 0.061573196202516556 Accuracy 0.8504999876022339\n",
      "Iteration 20670 Training loss 0.05294499173760414 Validation loss 0.05674079433083534 Accuracy 0.8585000038146973\n",
      "Iteration 20680 Training loss 0.05776703730225563 Validation loss 0.06305928528308868 Accuracy 0.8460000157356262\n",
      "Iteration 20690 Training loss 0.06885717809200287 Validation loss 0.07258755713701248 Accuracy 0.8230000138282776\n",
      "Iteration 20700 Training loss 0.058141373097896576 Validation loss 0.06537128984928131 Accuracy 0.8414999842643738\n",
      "Iteration 20710 Training loss 0.06602071970701218 Validation loss 0.07603015750646591 Accuracy 0.8140000104904175\n",
      "Iteration 20720 Training loss 0.0691131055355072 Validation loss 0.06970522552728653 Accuracy 0.8320000171661377\n",
      "Iteration 20730 Training loss 0.06436415016651154 Validation loss 0.07062290608882904 Accuracy 0.8264999985694885\n",
      "Iteration 20740 Training loss 0.058892205357551575 Validation loss 0.06225515902042389 Accuracy 0.8460000157356262\n",
      "Iteration 20750 Training loss 0.06476999074220657 Validation loss 0.06631436198949814 Accuracy 0.8349999785423279\n",
      "Iteration 20760 Training loss 0.06037318333983421 Validation loss 0.06573348492383957 Accuracy 0.8389999866485596\n",
      "Iteration 20770 Training loss 0.05674595385789871 Validation loss 0.06202433630824089 Accuracy 0.8510000109672546\n",
      "Iteration 20780 Training loss 0.06858214735984802 Validation loss 0.07317443192005157 Accuracy 0.8230000138282776\n",
      "Iteration 20790 Training loss 0.06411023437976837 Validation loss 0.06647825986146927 Accuracy 0.8385000228881836\n",
      "Iteration 20800 Training loss 0.06346737593412399 Validation loss 0.07061584293842316 Accuracy 0.8309999704360962\n",
      "Iteration 20810 Training loss 0.06074827164411545 Validation loss 0.06840305030345917 Accuracy 0.8339999914169312\n",
      "Iteration 20820 Training loss 0.056634459644556046 Validation loss 0.059985533356666565 Accuracy 0.8510000109672546\n",
      "Iteration 20830 Training loss 0.06627136468887329 Validation loss 0.07366739213466644 Accuracy 0.8240000009536743\n",
      "Iteration 20840 Training loss 0.05913090333342552 Validation loss 0.06566954404115677 Accuracy 0.8374999761581421\n",
      "Iteration 20850 Training loss 0.07046583294868469 Validation loss 0.07591790705919266 Accuracy 0.8140000104904175\n",
      "Iteration 20860 Training loss 0.0623411163687706 Validation loss 0.06835020333528519 Accuracy 0.8320000171661377\n",
      "Iteration 20870 Training loss 0.060337603092193604 Validation loss 0.06709256023168564 Accuracy 0.8364999890327454\n",
      "Iteration 20880 Training loss 0.05653505399823189 Validation loss 0.05656813830137253 Accuracy 0.859499990940094\n",
      "Iteration 20890 Training loss 0.07278962433338165 Validation loss 0.05924544483423233 Accuracy 0.8550000190734863\n",
      "Iteration 20900 Training loss 0.06580771505832672 Validation loss 0.056747809052467346 Accuracy 0.8529999852180481\n",
      "Iteration 20910 Training loss 0.06947927922010422 Validation loss 0.060414381325244904 Accuracy 0.8519999980926514\n",
      "Iteration 20920 Training loss 0.06073661148548126 Validation loss 0.053976576775312424 Accuracy 0.8610000014305115\n",
      "Iteration 20930 Training loss 0.06667239218950272 Validation loss 0.055929552763700485 Accuracy 0.8565000295639038\n",
      "Iteration 20940 Training loss 0.06551244109869003 Validation loss 0.05699662119150162 Accuracy 0.8544999957084656\n",
      "Iteration 20950 Training loss 0.0647849515080452 Validation loss 0.05449661612510681 Accuracy 0.8610000014305115\n",
      "Iteration 20960 Training loss 0.06080467626452446 Validation loss 0.05398201569914818 Accuracy 0.862500011920929\n",
      "Iteration 20970 Training loss 0.05951762944459915 Validation loss 0.05365709587931633 Accuracy 0.862500011920929\n",
      "Iteration 20980 Training loss 0.06388604640960693 Validation loss 0.05710989981889725 Accuracy 0.8569999933242798\n",
      "Iteration 20990 Training loss 0.06533219665288925 Validation loss 0.055765144526958466 Accuracy 0.8550000190734863\n",
      "Iteration 21000 Training loss 0.058801937848329544 Validation loss 0.05370999872684479 Accuracy 0.8644999861717224\n",
      "Iteration 21010 Training loss 0.06632061302661896 Validation loss 0.05849585682153702 Accuracy 0.8519999980926514\n",
      "Iteration 21020 Training loss 0.06504488736391068 Validation loss 0.055460281670093536 Accuracy 0.8575000166893005\n",
      "Iteration 21030 Training loss 0.06849734485149384 Validation loss 0.056596722453832626 Accuracy 0.8575000166893005\n",
      "Iteration 21040 Training loss 0.06345286965370178 Validation loss 0.05514329671859741 Accuracy 0.8575000166893005\n",
      "Iteration 21050 Training loss 0.07406576722860336 Validation loss 0.06238012760877609 Accuracy 0.843500018119812\n",
      "Iteration 21060 Training loss 0.058649543672800064 Validation loss 0.05437851324677467 Accuracy 0.8634999990463257\n",
      "Iteration 21070 Training loss 0.05704405903816223 Validation loss 0.05486579239368439 Accuracy 0.8634999990463257\n",
      "Iteration 21080 Training loss 0.05854523554444313 Validation loss 0.05519787222146988 Accuracy 0.8550000190734863\n",
      "Iteration 21090 Training loss 0.06619869917631149 Validation loss 0.056313224136829376 Accuracy 0.8550000190734863\n",
      "Iteration 21100 Training loss 0.06058318167924881 Validation loss 0.05519762262701988 Accuracy 0.859499990940094\n",
      "Iteration 21110 Training loss 0.06739543378353119 Validation loss 0.05751724913716316 Accuracy 0.8539999723434448\n",
      "Iteration 21120 Training loss 0.05457809567451477 Validation loss 0.0540921725332737 Accuracy 0.8629999756813049\n",
      "Iteration 21130 Training loss 0.06037796661257744 Validation loss 0.053833283483982086 Accuracy 0.8619999885559082\n",
      "Iteration 21140 Training loss 0.07076860219240189 Validation loss 0.05943406745791435 Accuracy 0.8535000085830688\n",
      "Iteration 21150 Training loss 0.06890182942152023 Validation loss 0.05705667659640312 Accuracy 0.8565000295639038\n",
      "Iteration 21160 Training loss 0.0598355233669281 Validation loss 0.05393695831298828 Accuracy 0.8619999885559082\n",
      "Iteration 21170 Training loss 0.06622044742107391 Validation loss 0.058430127799510956 Accuracy 0.8539999723434448\n",
      "Iteration 21180 Training loss 0.0685567781329155 Validation loss 0.05673275887966156 Accuracy 0.8585000038146973\n",
      "Iteration 21190 Training loss 0.05777391046285629 Validation loss 0.054547689855098724 Accuracy 0.8604999780654907\n",
      "Iteration 21200 Training loss 0.0664927065372467 Validation loss 0.0565398633480072 Accuracy 0.8560000061988831\n",
      "Iteration 21210 Training loss 0.06361574679613113 Validation loss 0.0555594377219677 Accuracy 0.8575000166893005\n",
      "Iteration 21220 Training loss 0.07216621190309525 Validation loss 0.06040351092815399 Accuracy 0.8519999980926514\n",
      "Iteration 21230 Training loss 0.060431282967329025 Validation loss 0.05468485876917839 Accuracy 0.859000027179718\n",
      "Iteration 21240 Training loss 0.06409646570682526 Validation loss 0.05754418298602104 Accuracy 0.8550000190734863\n",
      "Iteration 21250 Training loss 0.06746214628219604 Validation loss 0.05705377459526062 Accuracy 0.8539999723434448\n",
      "Iteration 21260 Training loss 0.06390100717544556 Validation loss 0.05623622238636017 Accuracy 0.8554999828338623\n",
      "Iteration 21270 Training loss 0.055863771587610245 Validation loss 0.05389868840575218 Accuracy 0.8629999756813049\n",
      "Iteration 21280 Training loss 0.0689917728304863 Validation loss 0.05597184598445892 Accuracy 0.8579999804496765\n",
      "Iteration 21290 Training loss 0.06704829633235931 Validation loss 0.05693647637963295 Accuracy 0.8554999828338623\n",
      "Iteration 21300 Training loss 0.0652192234992981 Validation loss 0.055808525532484055 Accuracy 0.8554999828338623\n",
      "Iteration 21310 Training loss 0.05518117919564247 Validation loss 0.0555373840034008 Accuracy 0.8610000014305115\n",
      "Iteration 21320 Training loss 0.06143350154161453 Validation loss 0.05366995558142662 Accuracy 0.8644999861717224\n",
      "Iteration 21330 Training loss 0.07385312020778656 Validation loss 0.06073059141635895 Accuracy 0.8500000238418579\n",
      "Iteration 21340 Training loss 0.057421714067459106 Validation loss 0.05461842566728592 Accuracy 0.8585000038146973\n",
      "Iteration 21350 Training loss 0.05877259746193886 Validation loss 0.053564202040433884 Accuracy 0.8634999990463257\n",
      "Iteration 21360 Training loss 0.060727838426828384 Validation loss 0.05440499261021614 Accuracy 0.8604999780654907\n",
      "Iteration 21370 Training loss 0.06476736813783646 Validation loss 0.05424242839217186 Accuracy 0.8600000143051147\n",
      "Iteration 21380 Training loss 0.05879351124167442 Validation loss 0.05427570268511772 Accuracy 0.8600000143051147\n",
      "Iteration 21390 Training loss 0.06528770178556442 Validation loss 0.05519617348909378 Accuracy 0.8569999933242798\n",
      "Iteration 21400 Training loss 0.053944867104291916 Validation loss 0.05406005308032036 Accuracy 0.8644999861717224\n",
      "Iteration 21410 Training loss 0.06329916417598724 Validation loss 0.05472496896982193 Accuracy 0.859499990940094\n",
      "Iteration 21420 Training loss 0.06284917145967484 Validation loss 0.05630003660917282 Accuracy 0.8554999828338623\n",
      "Iteration 21430 Training loss 0.0661960318684578 Validation loss 0.05600013956427574 Accuracy 0.8579999804496765\n",
      "Iteration 21440 Training loss 0.06686216592788696 Validation loss 0.05618847534060478 Accuracy 0.8544999957084656\n",
      "Iteration 21450 Training loss 0.06198536232113838 Validation loss 0.05593492090702057 Accuracy 0.8575000166893005\n",
      "Iteration 21460 Training loss 0.054382119327783585 Validation loss 0.05385879799723625 Accuracy 0.8629999756813049\n",
      "Iteration 21470 Training loss 0.060412295162677765 Validation loss 0.06681501120328903 Accuracy 0.8370000123977661\n",
      "Iteration 21480 Training loss 0.06856738030910492 Validation loss 0.07288787513971329 Accuracy 0.8255000114440918\n",
      "Iteration 21490 Training loss 0.059666626155376434 Validation loss 0.06550595164299011 Accuracy 0.8414999842643738\n",
      "Iteration 21500 Training loss 0.06475882232189178 Validation loss 0.06711668521165848 Accuracy 0.8364999890327454\n",
      "Iteration 21510 Training loss 0.06759830564260483 Validation loss 0.07098089903593063 Accuracy 0.828000009059906\n",
      "Iteration 21520 Training loss 0.061470892280340195 Validation loss 0.06586279720067978 Accuracy 0.8349999785423279\n",
      "Iteration 21530 Training loss 0.06724432110786438 Validation loss 0.06942891329526901 Accuracy 0.8324999809265137\n",
      "Iteration 21540 Training loss 0.0656210333108902 Validation loss 0.06448125094175339 Accuracy 0.8424999713897705\n",
      "Iteration 21550 Training loss 0.06450552493333817 Validation loss 0.06995397806167603 Accuracy 0.8299999833106995\n",
      "Iteration 21560 Training loss 0.0650937482714653 Validation loss 0.06749308854341507 Accuracy 0.8360000252723694\n",
      "Iteration 21570 Training loss 0.07322320342063904 Validation loss 0.07544984668493271 Accuracy 0.8134999871253967\n",
      "Iteration 21580 Training loss 0.055822961032390594 Validation loss 0.060289617627859116 Accuracy 0.8514999747276306\n",
      "Iteration 21590 Training loss 0.062155693769454956 Validation loss 0.06991299241781235 Accuracy 0.8274999856948853\n",
      "Iteration 21600 Training loss 0.0613039992749691 Validation loss 0.06718718260526657 Accuracy 0.8349999785423279\n",
      "Iteration 21610 Training loss 0.06551928073167801 Validation loss 0.07397421449422836 Accuracy 0.8215000033378601\n",
      "Iteration 21620 Training loss 0.05119728296995163 Validation loss 0.05588886886835098 Accuracy 0.8604999780654907\n",
      "Iteration 21630 Training loss 0.06240236014127731 Validation loss 0.05369548499584198 Accuracy 0.8629999756813049\n",
      "Iteration 21640 Training loss 0.05617719516158104 Validation loss 0.0538233183324337 Accuracy 0.8604999780654907\n",
      "Iteration 21650 Training loss 0.06446447968482971 Validation loss 0.05509774014353752 Accuracy 0.8569999933242798\n",
      "Iteration 21660 Training loss 0.06723550707101822 Validation loss 0.0582900233566761 Accuracy 0.8550000190734863\n",
      "Iteration 21670 Training loss 0.06747468560934067 Validation loss 0.05633001774549484 Accuracy 0.8529999852180481\n",
      "Iteration 21680 Training loss 0.06918998807668686 Validation loss 0.0564102940261364 Accuracy 0.8560000061988831\n",
      "Iteration 21690 Training loss 0.055072132498025894 Validation loss 0.054567184299230576 Accuracy 0.8634999990463257\n",
      "Iteration 21700 Training loss 0.07048752903938293 Validation loss 0.0589827299118042 Accuracy 0.8565000295639038\n",
      "Iteration 21710 Training loss 0.06160365790128708 Validation loss 0.05375434830784798 Accuracy 0.862500011920929\n",
      "Iteration 21720 Training loss 0.06542370468378067 Validation loss 0.05599573627114296 Accuracy 0.8565000295639038\n",
      "Iteration 21730 Training loss 0.06538363546133041 Validation loss 0.054691482335329056 Accuracy 0.859000027179718\n",
      "Iteration 21740 Training loss 0.0748986303806305 Validation loss 0.05996540188789368 Accuracy 0.8525000214576721\n",
      "Iteration 21750 Training loss 0.05917792394757271 Validation loss 0.05399603769183159 Accuracy 0.8619999885559082\n",
      "Iteration 21760 Training loss 0.06568090617656708 Validation loss 0.056370675563812256 Accuracy 0.8544999957084656\n",
      "Iteration 21770 Training loss 0.061413854360580444 Validation loss 0.05470627173781395 Accuracy 0.8600000143051147\n",
      "Iteration 21780 Training loss 0.06024104356765747 Validation loss 0.05468986928462982 Accuracy 0.8579999804496765\n",
      "Iteration 21790 Training loss 0.06306309998035431 Validation loss 0.05440455675125122 Accuracy 0.859499990940094\n",
      "Iteration 21800 Training loss 0.06267984211444855 Validation loss 0.0539143905043602 Accuracy 0.8619999885559082\n",
      "Iteration 21810 Training loss 0.0626518726348877 Validation loss 0.05436873063445091 Accuracy 0.8610000014305115\n",
      "Iteration 21820 Training loss 0.06305836141109467 Validation loss 0.05493954196572304 Accuracy 0.8569999933242798\n",
      "Iteration 21830 Training loss 0.06715820729732513 Validation loss 0.0549682192504406 Accuracy 0.8575000166893005\n",
      "Iteration 21840 Training loss 0.06353641301393509 Validation loss 0.05469602346420288 Accuracy 0.8575000166893005\n",
      "Iteration 21850 Training loss 0.06438644975423813 Validation loss 0.0559566356241703 Accuracy 0.8539999723434448\n",
      "Iteration 21860 Training loss 0.06054995208978653 Validation loss 0.05502248927950859 Accuracy 0.8579999804496765\n",
      "Iteration 21870 Training loss 0.06719042360782623 Validation loss 0.0540732741355896 Accuracy 0.8619999885559082\n",
      "Iteration 21880 Training loss 0.06250949203968048 Validation loss 0.05492840334773064 Accuracy 0.8569999933242798\n",
      "Iteration 21890 Training loss 0.06066454201936722 Validation loss 0.053590673953294754 Accuracy 0.862500011920929\n",
      "Iteration 21900 Training loss 0.0691140666604042 Validation loss 0.05645791441202164 Accuracy 0.8560000061988831\n",
      "Iteration 21910 Training loss 0.061352428048849106 Validation loss 0.05397138372063637 Accuracy 0.8619999885559082\n",
      "Iteration 21920 Training loss 0.05722357705235481 Validation loss 0.05393851920962334 Accuracy 0.8610000014305115\n",
      "Iteration 21930 Training loss 0.06056320294737816 Validation loss 0.05437394604086876 Accuracy 0.859499990940094\n",
      "Iteration 21940 Training loss 0.06051454320549965 Validation loss 0.05418956279754639 Accuracy 0.8604999780654907\n",
      "Iteration 21950 Training loss 0.056844111531972885 Validation loss 0.05405490845441818 Accuracy 0.8634999990463257\n",
      "Iteration 21960 Training loss 0.0667031854391098 Validation loss 0.055248092859983444 Accuracy 0.8575000166893005\n",
      "Iteration 21970 Training loss 0.0663444846868515 Validation loss 0.05522574484348297 Accuracy 0.8554999828338623\n",
      "Iteration 21980 Training loss 0.06805583089590073 Validation loss 0.058510176837444305 Accuracy 0.8529999852180481\n",
      "Iteration 21990 Training loss 0.06188918650150299 Validation loss 0.0540170855820179 Accuracy 0.8604999780654907\n",
      "Iteration 22000 Training loss 0.05577648803591728 Validation loss 0.05376718193292618 Accuracy 0.8619999885559082\n",
      "Iteration 22010 Training loss 0.05765358358621597 Validation loss 0.05527961626648903 Accuracy 0.8550000190734863\n",
      "Iteration 22020 Training loss 0.06220205873250961 Validation loss 0.05391889810562134 Accuracy 0.8604999780654907\n",
      "Iteration 22030 Training loss 0.06673751771450043 Validation loss 0.05694146454334259 Accuracy 0.8544999957084656\n",
      "Iteration 22040 Training loss 0.06731151044368744 Validation loss 0.05531669780611992 Accuracy 0.8575000166893005\n",
      "Iteration 22050 Training loss 0.05771958827972412 Validation loss 0.054053016006946564 Accuracy 0.8619999885559082\n",
      "Iteration 22060 Training loss 0.07699307054281235 Validation loss 0.06015309318900108 Accuracy 0.8514999747276306\n",
      "Iteration 22070 Training loss 0.06112804263830185 Validation loss 0.056466735899448395 Accuracy 0.8554999828338623\n",
      "Iteration 22080 Training loss 0.06386119872331619 Validation loss 0.05610932409763336 Accuracy 0.8569999933242798\n",
      "Iteration 22090 Training loss 0.07115834951400757 Validation loss 0.057946622371673584 Accuracy 0.8529999852180481\n",
      "Iteration 22100 Training loss 0.05500968173146248 Validation loss 0.05393942818045616 Accuracy 0.8604999780654907\n",
      "Iteration 22110 Training loss 0.060926612466573715 Validation loss 0.05383189022541046 Accuracy 0.8610000014305115\n",
      "Iteration 22120 Training loss 0.06370262801647186 Validation loss 0.0548253059387207 Accuracy 0.8575000166893005\n",
      "Iteration 22130 Training loss 0.07293485105037689 Validation loss 0.061329953372478485 Accuracy 0.8479999899864197\n",
      "Iteration 22140 Training loss 0.062332045286893845 Validation loss 0.05366376042366028 Accuracy 0.8600000143051147\n",
      "Iteration 22150 Training loss 0.05501171573996544 Validation loss 0.05410528555512428 Accuracy 0.8610000014305115\n",
      "Iteration 22160 Training loss 0.06912876665592194 Validation loss 0.057098716497421265 Accuracy 0.8550000190734863\n",
      "Iteration 22170 Training loss 0.05675441026687622 Validation loss 0.05409875512123108 Accuracy 0.8615000247955322\n",
      "Iteration 22180 Training loss 0.07120032608509064 Validation loss 0.05792192742228508 Accuracy 0.8519999980926514\n",
      "Iteration 22190 Training loss 0.06499257683753967 Validation loss 0.056332264095544815 Accuracy 0.8554999828338623\n",
      "Iteration 22200 Training loss 0.056456927210092545 Validation loss 0.0539197102189064 Accuracy 0.8610000014305115\n",
      "Iteration 22210 Training loss 0.07743965089321136 Validation loss 0.06044559180736542 Accuracy 0.8500000238418579\n",
      "Iteration 22220 Training loss 0.06592611223459244 Validation loss 0.0556374192237854 Accuracy 0.8575000166893005\n",
      "Iteration 22230 Training loss 0.06271157413721085 Validation loss 0.054947640746831894 Accuracy 0.8579999804496765\n",
      "Iteration 22240 Training loss 0.07285179197788239 Validation loss 0.05721070244908333 Accuracy 0.8544999957084656\n",
      "Iteration 22250 Training loss 0.06380897760391235 Validation loss 0.05516135320067406 Accuracy 0.8554999828338623\n",
      "Iteration 22260 Training loss 0.061179932206869125 Validation loss 0.054377295076847076 Accuracy 0.8579999804496765\n",
      "Iteration 22270 Training loss 0.06320171058177948 Validation loss 0.05646814778447151 Accuracy 0.8539999723434448\n",
      "Iteration 22280 Training loss 0.06652935594320297 Validation loss 0.05767088010907173 Accuracy 0.8535000085830688\n",
      "Iteration 22290 Training loss 0.06253350526094437 Validation loss 0.055322788655757904 Accuracy 0.8575000166893005\n",
      "Iteration 22300 Training loss 0.06133733317255974 Validation loss 0.05664685741066933 Accuracy 0.8529999852180481\n",
      "Iteration 22310 Training loss 0.06055865436792374 Validation loss 0.054787278175354004 Accuracy 0.859000027179718\n",
      "Iteration 22320 Training loss 0.07144816219806671 Validation loss 0.05832379683852196 Accuracy 0.8519999980926514\n",
      "Iteration 22330 Training loss 0.06320783495903015 Validation loss 0.05752706155180931 Accuracy 0.8565000295639038\n",
      "Iteration 22340 Training loss 0.06791912764310837 Validation loss 0.05554040148854256 Accuracy 0.8569999933242798\n",
      "Iteration 22350 Training loss 0.06879445165395737 Validation loss 0.05636974424123764 Accuracy 0.8554999828338623\n",
      "Iteration 22360 Training loss 0.06662469357252121 Validation loss 0.05861213803291321 Accuracy 0.8529999852180481\n",
      "Iteration 22370 Training loss 0.06361127644777298 Validation loss 0.055224888026714325 Accuracy 0.859000027179718\n",
      "Iteration 22380 Training loss 0.06672049313783646 Validation loss 0.05664161592721939 Accuracy 0.8550000190734863\n",
      "Iteration 22390 Training loss 0.05386829748749733 Validation loss 0.05419229343533516 Accuracy 0.8604999780654907\n",
      "Iteration 22400 Training loss 0.06089216470718384 Validation loss 0.0545535534620285 Accuracy 0.8600000143051147\n",
      "Iteration 22410 Training loss 0.0650288388133049 Validation loss 0.057225074619054794 Accuracy 0.8539999723434448\n",
      "Iteration 22420 Training loss 0.0638667419552803 Validation loss 0.05674653500318527 Accuracy 0.8539999723434448\n",
      "Iteration 22430 Training loss 0.06003587692975998 Validation loss 0.05358521267771721 Accuracy 0.8634999990463257\n",
      "Iteration 22440 Training loss 0.06061435863375664 Validation loss 0.05605829134583473 Accuracy 0.8560000061988831\n",
      "Iteration 22450 Training loss 0.07174502313137054 Validation loss 0.0592147521674633 Accuracy 0.8554999828338623\n",
      "Iteration 22460 Training loss 0.06037934496998787 Validation loss 0.054499246180057526 Accuracy 0.8610000014305115\n",
      "Iteration 22470 Training loss 0.061481039971113205 Validation loss 0.055286675691604614 Accuracy 0.8569999933242798\n",
      "Iteration 22480 Training loss 0.0570388063788414 Validation loss 0.05381634458899498 Accuracy 0.8619999885559082\n",
      "Iteration 22490 Training loss 0.0669085904955864 Validation loss 0.05567999929189682 Accuracy 0.8575000166893005\n",
      "Iteration 22500 Training loss 0.07377211004495621 Validation loss 0.05985459312796593 Accuracy 0.8519999980926514\n",
      "Iteration 22510 Training loss 0.06031525507569313 Validation loss 0.05362880229949951 Accuracy 0.8619999885559082\n",
      "Iteration 22520 Training loss 0.06267144531011581 Validation loss 0.053996894508600235 Accuracy 0.8610000014305115\n",
      "Iteration 22530 Training loss 0.06889845430850983 Validation loss 0.05880074203014374 Accuracy 0.8550000190734863\n",
      "Iteration 22540 Training loss 0.0636150985956192 Validation loss 0.05504217743873596 Accuracy 0.8575000166893005\n",
      "Iteration 22550 Training loss 0.06431086361408234 Validation loss 0.05551081895828247 Accuracy 0.8565000295639038\n",
      "Iteration 22560 Training loss 0.05902751162648201 Validation loss 0.0536159873008728 Accuracy 0.8644999861717224\n",
      "Iteration 22570 Training loss 0.06153743341565132 Validation loss 0.05429520457983017 Accuracy 0.8610000014305115\n",
      "Iteration 22580 Training loss 0.05678374320268631 Validation loss 0.05353996530175209 Accuracy 0.8640000224113464\n",
      "Iteration 22590 Training loss 0.061506133526563644 Validation loss 0.054652199149131775 Accuracy 0.859499990940094\n",
      "Iteration 22600 Training loss 0.06132833659648895 Validation loss 0.054246433079242706 Accuracy 0.8615000247955322\n",
      "Iteration 22610 Training loss 0.06159545108675957 Validation loss 0.0564306266605854 Accuracy 0.8569999933242798\n",
      "Iteration 22620 Training loss 0.05751306936144829 Validation loss 0.0541696771979332 Accuracy 0.8610000014305115\n",
      "Iteration 22630 Training loss 0.06697428971529007 Validation loss 0.0577414408326149 Accuracy 0.8544999957084656\n",
      "Iteration 22640 Training loss 0.06033601239323616 Validation loss 0.0538797453045845 Accuracy 0.8629999756813049\n",
      "Iteration 22650 Training loss 0.06353498250246048 Validation loss 0.05835234746336937 Accuracy 0.8550000190734863\n",
      "Iteration 22660 Training loss 0.06040431931614876 Validation loss 0.05418074503540993 Accuracy 0.8600000143051147\n",
      "Iteration 22670 Training loss 0.06791828572750092 Validation loss 0.05715465545654297 Accuracy 0.8519999980926514\n",
      "Iteration 22680 Training loss 0.05632614716887474 Validation loss 0.053777165710926056 Accuracy 0.8604999780654907\n",
      "Iteration 22690 Training loss 0.06546654552221298 Validation loss 0.054930076003074646 Accuracy 0.8565000295639038\n",
      "Iteration 22700 Training loss 0.06723359227180481 Validation loss 0.059069398790597916 Accuracy 0.8519999980926514\n",
      "Iteration 22710 Training loss 0.061086975038051605 Validation loss 0.05610806867480278 Accuracy 0.8525000214576721\n",
      "Iteration 22720 Training loss 0.06876150518655777 Validation loss 0.05862943083047867 Accuracy 0.8550000190734863\n",
      "Iteration 22730 Training loss 0.07143780589103699 Validation loss 0.059892162680625916 Accuracy 0.8519999980926514\n",
      "Iteration 22740 Training loss 0.0731864646077156 Validation loss 0.06124931573867798 Accuracy 0.8495000004768372\n",
      "Iteration 22750 Training loss 0.059673141688108444 Validation loss 0.054709531366825104 Accuracy 0.8610000014305115\n",
      "Iteration 22760 Training loss 0.052591968327760696 Validation loss 0.0544084869325161 Accuracy 0.8634999990463257\n",
      "Iteration 22770 Training loss 0.07523567229509354 Validation loss 0.0647411122918129 Accuracy 0.840499997138977\n",
      "Iteration 22780 Training loss 0.06322527676820755 Validation loss 0.05646215006709099 Accuracy 0.8529999852180481\n",
      "Iteration 22790 Training loss 0.05789879709482193 Validation loss 0.054864414036273956 Accuracy 0.8600000143051147\n",
      "Iteration 22800 Training loss 0.06978747248649597 Validation loss 0.055766914039850235 Accuracy 0.8575000166893005\n",
      "Iteration 22810 Training loss 0.06642058491706848 Validation loss 0.05694419890642166 Accuracy 0.8550000190734863\n",
      "Iteration 22820 Training loss 0.061111290007829666 Validation loss 0.05492192134261131 Accuracy 0.8569999933242798\n",
      "Iteration 22830 Training loss 0.06491800397634506 Validation loss 0.056095708161592484 Accuracy 0.8565000295639038\n",
      "Iteration 22840 Training loss 0.0564689114689827 Validation loss 0.05420897901058197 Accuracy 0.8629999756813049\n",
      "Iteration 22850 Training loss 0.061560194939374924 Validation loss 0.0562603622674942 Accuracy 0.8565000295639038\n",
      "Iteration 22860 Training loss 0.061378564685583115 Validation loss 0.054920494556427 Accuracy 0.8575000166893005\n",
      "Iteration 22870 Training loss 0.06217212229967117 Validation loss 0.05517730861902237 Accuracy 0.8575000166893005\n",
      "Iteration 22880 Training loss 0.06908169388771057 Validation loss 0.06251805275678635 Accuracy 0.8460000157356262\n",
      "Iteration 22890 Training loss 0.06252230703830719 Validation loss 0.05456866696476936 Accuracy 0.8600000143051147\n",
      "Iteration 22900 Training loss 0.06158295273780823 Validation loss 0.0543174147605896 Accuracy 0.8604999780654907\n",
      "Iteration 22910 Training loss 0.05695990473031998 Validation loss 0.054377518594264984 Accuracy 0.8615000247955322\n",
      "Iteration 22920 Training loss 0.05811174958944321 Validation loss 0.05466964840888977 Accuracy 0.8610000014305115\n",
      "Iteration 22930 Training loss 0.06594430655241013 Validation loss 0.05574962869286537 Accuracy 0.8544999957084656\n",
      "Iteration 22940 Training loss 0.07222399860620499 Validation loss 0.0589115209877491 Accuracy 0.8535000085830688\n",
      "Iteration 22950 Training loss 0.06174185872077942 Validation loss 0.05483865737915039 Accuracy 0.859499990940094\n",
      "Iteration 22960 Training loss 0.06057349592447281 Validation loss 0.05480118468403816 Accuracy 0.8579999804496765\n",
      "Iteration 22970 Training loss 0.057841166853904724 Validation loss 0.054230500012636185 Accuracy 0.859499990940094\n",
      "Iteration 22980 Training loss 0.066108837723732 Validation loss 0.0552048459649086 Accuracy 0.8565000295639038\n",
      "Iteration 22990 Training loss 0.0672067254781723 Validation loss 0.05640047416090965 Accuracy 0.8569999933242798\n",
      "Iteration 23000 Training loss 0.061140671372413635 Validation loss 0.055364418774843216 Accuracy 0.8569999933242798\n",
      "Iteration 23010 Training loss 0.06748971343040466 Validation loss 0.056970953941345215 Accuracy 0.8539999723434448\n",
      "Iteration 23020 Training loss 0.07259932905435562 Validation loss 0.057385727763175964 Accuracy 0.8535000085830688\n",
      "Iteration 23030 Training loss 0.0562933087348938 Validation loss 0.05459991469979286 Accuracy 0.8604999780654907\n",
      "Iteration 23040 Training loss 0.06392645835876465 Validation loss 0.05413830280303955 Accuracy 0.8610000014305115\n",
      "Iteration 23050 Training loss 0.06094953790307045 Validation loss 0.05596521124243736 Accuracy 0.8539999723434448\n",
      "Iteration 23060 Training loss 0.06627772003412247 Validation loss 0.0570332407951355 Accuracy 0.8539999723434448\n",
      "Iteration 23070 Training loss 0.061327941715717316 Validation loss 0.05398678407073021 Accuracy 0.8600000143051147\n",
      "Iteration 23080 Training loss 0.055960070341825485 Validation loss 0.05457167699933052 Accuracy 0.8610000014305115\n",
      "Iteration 23090 Training loss 0.0666344165802002 Validation loss 0.056294526904821396 Accuracy 0.8560000061988831\n",
      "Iteration 23100 Training loss 0.06161237508058548 Validation loss 0.054664138704538345 Accuracy 0.8585000038146973\n",
      "Iteration 23110 Training loss 0.061368346214294434 Validation loss 0.05666119232773781 Accuracy 0.8544999957084656\n",
      "Iteration 23120 Training loss 0.06257879734039307 Validation loss 0.054836563766002655 Accuracy 0.8575000166893005\n",
      "Iteration 23130 Training loss 0.06764383614063263 Validation loss 0.0574914887547493 Accuracy 0.8560000061988831\n",
      "Iteration 23140 Training loss 0.06121833622455597 Validation loss 0.05613188073039055 Accuracy 0.8554999828338623\n",
      "Iteration 23150 Training loss 0.0627397745847702 Validation loss 0.05565148964524269 Accuracy 0.8575000166893005\n",
      "Iteration 23160 Training loss 0.0570911169052124 Validation loss 0.05475896596908569 Accuracy 0.8579999804496765\n",
      "Iteration 23170 Training loss 0.06886450201272964 Validation loss 0.0552450567483902 Accuracy 0.8569999933242798\n",
      "Iteration 23180 Training loss 0.06165197864174843 Validation loss 0.05403097718954086 Accuracy 0.8610000014305115\n",
      "Iteration 23190 Training loss 0.06762845069169998 Validation loss 0.05719180405139923 Accuracy 0.8560000061988831\n",
      "Iteration 23200 Training loss 0.06111942231655121 Validation loss 0.05471080169081688 Accuracy 0.8604999780654907\n",
      "Iteration 23210 Training loss 0.06414593756198883 Validation loss 0.05442795529961586 Accuracy 0.8610000014305115\n",
      "Iteration 23220 Training loss 0.0652000829577446 Validation loss 0.05773693323135376 Accuracy 0.8539999723434448\n",
      "Iteration 23230 Training loss 0.056902844458818436 Validation loss 0.05401606112718582 Accuracy 0.8634999990463257\n",
      "Iteration 23240 Training loss 0.059737902134656906 Validation loss 0.05406917631626129 Accuracy 0.8634999990463257\n",
      "Iteration 23250 Training loss 0.05709270015358925 Validation loss 0.05379100143909454 Accuracy 0.862500011920929\n",
      "Iteration 23260 Training loss 0.07062298059463501 Validation loss 0.0577518604695797 Accuracy 0.859000027179718\n",
      "Iteration 23270 Training loss 0.06004147604107857 Validation loss 0.05662362650036812 Accuracy 0.8560000061988831\n",
      "Iteration 23280 Training loss 0.060978859663009644 Validation loss 0.05558757111430168 Accuracy 0.8569999933242798\n",
      "Iteration 23290 Training loss 0.06259353458881378 Validation loss 0.05442800372838974 Accuracy 0.8610000014305115\n",
      "Iteration 23300 Training loss 0.06319600343704224 Validation loss 0.054756782948970795 Accuracy 0.8600000143051147\n",
      "Iteration 23310 Training loss 0.05713183432817459 Validation loss 0.0545407272875309 Accuracy 0.8629999756813049\n",
      "Iteration 23320 Training loss 0.06365598738193512 Validation loss 0.056611012667417526 Accuracy 0.8510000109672546\n",
      "Iteration 23330 Training loss 0.06355065852403641 Validation loss 0.05419785529375076 Accuracy 0.862500011920929\n",
      "Iteration 23340 Training loss 0.07065985351800919 Validation loss 0.05714643746614456 Accuracy 0.8544999957084656\n",
      "Iteration 23350 Training loss 0.06053951382637024 Validation loss 0.05590952932834625 Accuracy 0.8565000295639038\n",
      "Iteration 23360 Training loss 0.05816534906625748 Validation loss 0.05401584506034851 Accuracy 0.862500011920929\n",
      "Iteration 23370 Training loss 0.06339835375547409 Validation loss 0.05468517914414406 Accuracy 0.8575000166893005\n",
      "Iteration 23380 Training loss 0.059748973697423935 Validation loss 0.05519520118832588 Accuracy 0.8565000295639038\n",
      "Iteration 23390 Training loss 0.06525581330060959 Validation loss 0.055662814527750015 Accuracy 0.8565000295639038\n",
      "Iteration 23400 Training loss 0.054970383644104004 Validation loss 0.05468568578362465 Accuracy 0.8604999780654907\n",
      "Iteration 23410 Training loss 0.0726488009095192 Validation loss 0.062270600348711014 Accuracy 0.8479999899864197\n",
      "Iteration 23420 Training loss 0.061523307114839554 Validation loss 0.05578647181391716 Accuracy 0.8560000061988831\n",
      "Iteration 23430 Training loss 0.05845043808221817 Validation loss 0.053946785628795624 Accuracy 0.8619999885559082\n",
      "Iteration 23440 Training loss 0.06477946788072586 Validation loss 0.0564589686691761 Accuracy 0.8550000190734863\n",
      "Iteration 23450 Training loss 0.06020018085837364 Validation loss 0.05429501459002495 Accuracy 0.8604999780654907\n",
      "Iteration 23460 Training loss 0.06546882539987564 Validation loss 0.05665550380945206 Accuracy 0.8535000085830688\n",
      "Iteration 23470 Training loss 0.06436577439308167 Validation loss 0.054778359830379486 Accuracy 0.8569999933242798\n",
      "Iteration 23480 Training loss 0.061309490352869034 Validation loss 0.0565313883125782 Accuracy 0.8539999723434448\n",
      "Iteration 23490 Training loss 0.06788045912981033 Validation loss 0.056050512939691544 Accuracy 0.8544999957084656\n",
      "Iteration 23500 Training loss 0.06373605877161026 Validation loss 0.05470796301960945 Accuracy 0.859000027179718\n",
      "Iteration 23510 Training loss 0.05808982253074646 Validation loss 0.054576326161623 Accuracy 0.8604999780654907\n",
      "Iteration 23520 Training loss 0.06128331646323204 Validation loss 0.05355878546833992 Accuracy 0.8615000247955322\n",
      "Iteration 23530 Training loss 0.06329265981912613 Validation loss 0.054326966404914856 Accuracy 0.8615000247955322\n",
      "Iteration 23540 Training loss 0.06474591046571732 Validation loss 0.05842414125800133 Accuracy 0.8565000295639038\n",
      "Iteration 23550 Training loss 0.06873928755521774 Validation loss 0.05602126196026802 Accuracy 0.8535000085830688\n",
      "Iteration 23560 Training loss 0.060907915234565735 Validation loss 0.054224081337451935 Accuracy 0.8604999780654907\n",
      "Iteration 23570 Training loss 0.05899668484926224 Validation loss 0.05376102030277252 Accuracy 0.8629999756813049\n",
      "Iteration 23580 Training loss 0.0596635602414608 Validation loss 0.05476592853665352 Accuracy 0.8600000143051147\n",
      "Iteration 23590 Training loss 0.06285854429006577 Validation loss 0.05408957600593567 Accuracy 0.8610000014305115\n",
      "Iteration 23600 Training loss 0.05608232319355011 Validation loss 0.0541311651468277 Accuracy 0.8585000038146973\n",
      "Iteration 23610 Training loss 0.08192005753517151 Validation loss 0.06762824207544327 Accuracy 0.8355000019073486\n",
      "Iteration 23620 Training loss 0.06107068806886673 Validation loss 0.054490379989147186 Accuracy 0.8585000038146973\n",
      "Iteration 23630 Training loss 0.056741226464509964 Validation loss 0.05433913320302963 Accuracy 0.8600000143051147\n",
      "Iteration 23640 Training loss 0.0669766440987587 Validation loss 0.05705485865473747 Accuracy 0.8535000085830688\n",
      "Iteration 23650 Training loss 0.06314820796251297 Validation loss 0.05517171323299408 Accuracy 0.859000027179718\n",
      "Iteration 23660 Training loss 0.06667669862508774 Validation loss 0.056887947022914886 Accuracy 0.8539999723434448\n",
      "Iteration 23670 Training loss 0.0637182891368866 Validation loss 0.054988279938697815 Accuracy 0.859499990940094\n",
      "Iteration 23680 Training loss 0.07024650275707245 Validation loss 0.05625642463564873 Accuracy 0.8544999957084656\n",
      "Iteration 23690 Training loss 0.06481780856847763 Validation loss 0.05576181784272194 Accuracy 0.8565000295639038\n",
      "Iteration 23700 Training loss 0.061089638620615005 Validation loss 0.05418844521045685 Accuracy 0.8619999885559082\n",
      "Iteration 23710 Training loss 0.060589104890823364 Validation loss 0.05575551465153694 Accuracy 0.8554999828338623\n",
      "Iteration 23720 Training loss 0.06391119211912155 Validation loss 0.05464515835046768 Accuracy 0.8579999804496765\n",
      "Iteration 23730 Training loss 0.055724989622831345 Validation loss 0.053874727338552475 Accuracy 0.859499990940094\n",
      "Iteration 23740 Training loss 0.06049538403749466 Validation loss 0.05567445978522301 Accuracy 0.8560000061988831\n",
      "Iteration 23750 Training loss 0.0604204386472702 Validation loss 0.05380307883024216 Accuracy 0.8600000143051147\n",
      "Iteration 23760 Training loss 0.07489096373319626 Validation loss 0.06004105135798454 Accuracy 0.8510000109672546\n",
      "Iteration 23770 Training loss 0.05768674239516258 Validation loss 0.054390180855989456 Accuracy 0.8650000095367432\n",
      "Iteration 23780 Training loss 0.0658409371972084 Validation loss 0.05846254900097847 Accuracy 0.8544999957084656\n",
      "Iteration 23790 Training loss 0.06126057356595993 Validation loss 0.054732684046030045 Accuracy 0.8569999933242798\n",
      "Iteration 23800 Training loss 0.06715206801891327 Validation loss 0.05670391768217087 Accuracy 0.8560000061988831\n",
      "Iteration 23810 Training loss 0.06482233852148056 Validation loss 0.05761232599616051 Accuracy 0.8544999957084656\n",
      "Iteration 23820 Training loss 0.05757270008325577 Validation loss 0.05382182449102402 Accuracy 0.862500011920929\n",
      "Iteration 23830 Training loss 0.06334227323532104 Validation loss 0.056306976824998856 Accuracy 0.8560000061988831\n",
      "Iteration 23840 Training loss 0.07160453498363495 Validation loss 0.05706648901104927 Accuracy 0.8565000295639038\n",
      "Iteration 23850 Training loss 0.06731990724802017 Validation loss 0.05695325881242752 Accuracy 0.8529999852180481\n",
      "Iteration 23860 Training loss 0.05846216529607773 Validation loss 0.05406856909394264 Accuracy 0.8619999885559082\n",
      "Iteration 23870 Training loss 0.061064463108778 Validation loss 0.053784627467393875 Accuracy 0.8640000224113464\n",
      "Iteration 23880 Training loss 0.058615852147340775 Validation loss 0.055179379880428314 Accuracy 0.8569999933242798\n",
      "Iteration 23890 Training loss 0.062054749578237534 Validation loss 0.05550738424062729 Accuracy 0.8569999933242798\n",
      "Iteration 23900 Training loss 0.06631243973970413 Validation loss 0.05539466813206673 Accuracy 0.8550000190734863\n",
      "Iteration 23910 Training loss 0.0604264922440052 Validation loss 0.05518016591668129 Accuracy 0.8554999828338623\n",
      "Iteration 23920 Training loss 0.06656171381473541 Validation loss 0.06051789969205856 Accuracy 0.8519999980926514\n",
      "Iteration 23930 Training loss 0.0629609078168869 Validation loss 0.05408509448170662 Accuracy 0.8610000014305115\n",
      "Iteration 23940 Training loss 0.05861339718103409 Validation loss 0.054801132529973984 Accuracy 0.8600000143051147\n",
      "Iteration 23950 Training loss 0.05740417540073395 Validation loss 0.05441739037632942 Accuracy 0.862500011920929\n",
      "Iteration 23960 Training loss 0.06915109604597092 Validation loss 0.05995902419090271 Accuracy 0.8519999980926514\n",
      "Iteration 23970 Training loss 0.06837520003318787 Validation loss 0.05662991479039192 Accuracy 0.8554999828338623\n",
      "Iteration 23980 Training loss 0.06338431686162949 Validation loss 0.057030145078897476 Accuracy 0.8550000190734863\n",
      "Iteration 23990 Training loss 0.0588473305106163 Validation loss 0.05421723797917366 Accuracy 0.862500011920929\n",
      "Iteration 24000 Training loss 0.06351817399263382 Validation loss 0.05485747009515762 Accuracy 0.859000027179718\n",
      "Iteration 24010 Training loss 0.06206268072128296 Validation loss 0.053791794925928116 Accuracy 0.8629999756813049\n",
      "Iteration 24020 Training loss 0.06423928588628769 Validation loss 0.05537717416882515 Accuracy 0.8565000295639038\n",
      "Iteration 24030 Training loss 0.0634382888674736 Validation loss 0.05693001672625542 Accuracy 0.8575000166893005\n",
      "Iteration 24040 Training loss 0.06138836592435837 Validation loss 0.05501136928796768 Accuracy 0.859499990940094\n",
      "Iteration 24050 Training loss 0.06723601371049881 Validation loss 0.05725051835179329 Accuracy 0.8529999852180481\n",
      "Iteration 24060 Training loss 0.058854829519987106 Validation loss 0.054630838334560394 Accuracy 0.8585000038146973\n",
      "Iteration 24070 Training loss 0.06397895514965057 Validation loss 0.05719684064388275 Accuracy 0.8529999852180481\n",
      "Iteration 24080 Training loss 0.06967421621084213 Validation loss 0.05940275639295578 Accuracy 0.8504999876022339\n",
      "Iteration 24090 Training loss 0.0632171481847763 Validation loss 0.05546078458428383 Accuracy 0.8554999828338623\n",
      "Iteration 24100 Training loss 0.0539027638733387 Validation loss 0.05432687699794769 Accuracy 0.8600000143051147\n",
      "Iteration 24110 Training loss 0.06888100504875183 Validation loss 0.05656246095895767 Accuracy 0.8565000295639038\n",
      "Iteration 24120 Training loss 0.059670060873031616 Validation loss 0.054729197174310684 Accuracy 0.8604999780654907\n",
      "Iteration 24130 Training loss 0.062056589871644974 Validation loss 0.05505577102303505 Accuracy 0.8585000038146973\n",
      "Iteration 24140 Training loss 0.05766121670603752 Validation loss 0.05505768582224846 Accuracy 0.859000027179718\n",
      "Iteration 24150 Training loss 0.06521069258451462 Validation loss 0.057681236416101456 Accuracy 0.8539999723434448\n",
      "Iteration 24160 Training loss 0.05989369377493858 Validation loss 0.057614389806985855 Accuracy 0.8544999957084656\n",
      "Iteration 24170 Training loss 0.059165649116039276 Validation loss 0.054026756435632706 Accuracy 0.8610000014305115\n",
      "Iteration 24180 Training loss 0.0697711706161499 Validation loss 0.05866638198494911 Accuracy 0.8544999957084656\n",
      "Iteration 24190 Training loss 0.06301140040159225 Validation loss 0.05572758615016937 Accuracy 0.8560000061988831\n",
      "Iteration 24200 Training loss 0.0604032538831234 Validation loss 0.0543842613697052 Accuracy 0.8615000247955322\n",
      "Iteration 24210 Training loss 0.05904421582818031 Validation loss 0.054595865309238434 Accuracy 0.8579999804496765\n",
      "Iteration 24220 Training loss 0.061500102281570435 Validation loss 0.05735521391034126 Accuracy 0.8569999933242798\n",
      "Iteration 24230 Training loss 0.06110488250851631 Validation loss 0.055244337767362595 Accuracy 0.8575000166893005\n",
      "Iteration 24240 Training loss 0.05488848313689232 Validation loss 0.055529944598674774 Accuracy 0.8560000061988831\n",
      "Iteration 24250 Training loss 0.062473446130752563 Validation loss 0.0574524961411953 Accuracy 0.8539999723434448\n",
      "Iteration 24260 Training loss 0.06918635964393616 Validation loss 0.05685610696673393 Accuracy 0.8560000061988831\n",
      "Iteration 24270 Training loss 0.06132517382502556 Validation loss 0.056422483175992966 Accuracy 0.8554999828338623\n",
      "Iteration 24280 Training loss 0.06148666515946388 Validation loss 0.05676114186644554 Accuracy 0.8575000166893005\n",
      "Iteration 24290 Training loss 0.06076618656516075 Validation loss 0.054481614381074905 Accuracy 0.8604999780654907\n",
      "Iteration 24300 Training loss 0.056013841181993484 Validation loss 0.05445569381117821 Accuracy 0.8604999780654907\n",
      "Iteration 24310 Training loss 0.06312421709299088 Validation loss 0.0562095120549202 Accuracy 0.8565000295639038\n",
      "Iteration 24320 Training loss 0.06880973279476166 Validation loss 0.05948476493358612 Accuracy 0.8550000190734863\n",
      "Iteration 24330 Training loss 0.0622425451874733 Validation loss 0.05645217001438141 Accuracy 0.8565000295639038\n",
      "Iteration 24340 Training loss 0.05950184166431427 Validation loss 0.05563236400485039 Accuracy 0.8560000061988831\n",
      "Iteration 24350 Training loss 0.06250813603401184 Validation loss 0.055218711495399475 Accuracy 0.8544999957084656\n",
      "Iteration 24360 Training loss 0.06048441305756569 Validation loss 0.05468900129199028 Accuracy 0.8604999780654907\n",
      "Iteration 24370 Training loss 0.07487689703702927 Validation loss 0.05760888382792473 Accuracy 0.8569999933242798\n",
      "Iteration 24380 Training loss 0.06426792591810226 Validation loss 0.05553536117076874 Accuracy 0.8579999804496765\n",
      "Iteration 24390 Training loss 0.06066243723034859 Validation loss 0.05525706335902214 Accuracy 0.8569999933242798\n",
      "Iteration 24400 Training loss 0.05749779939651489 Validation loss 0.05419126898050308 Accuracy 0.8619999885559082\n",
      "Iteration 24410 Training loss 0.06103392690420151 Validation loss 0.05485495552420616 Accuracy 0.8610000014305115\n",
      "Iteration 24420 Training loss 0.05944276973605156 Validation loss 0.05466494336724281 Accuracy 0.8585000038146973\n",
      "Iteration 24430 Training loss 0.06665574014186859 Validation loss 0.05717625096440315 Accuracy 0.8550000190734863\n",
      "Iteration 24440 Training loss 0.06294459849596024 Validation loss 0.05591454356908798 Accuracy 0.8554999828338623\n",
      "Iteration 24450 Training loss 0.06310456991195679 Validation loss 0.055404890328645706 Accuracy 0.8554999828338623\n",
      "Iteration 24460 Training loss 0.07602336257696152 Validation loss 0.06167750805616379 Accuracy 0.8479999899864197\n",
      "Iteration 24470 Training loss 0.062313247472047806 Validation loss 0.056141674518585205 Accuracy 0.8560000061988831\n",
      "Iteration 24480 Training loss 0.0711815357208252 Validation loss 0.06039676070213318 Accuracy 0.8529999852180481\n",
      "Iteration 24490 Training loss 0.053551074117422104 Validation loss 0.05407760664820671 Accuracy 0.8619999885559082\n",
      "Iteration 24500 Training loss 0.06758987158536911 Validation loss 0.05639486387372017 Accuracy 0.8544999957084656\n",
      "Iteration 24510 Training loss 0.06731180846691132 Validation loss 0.05712772160768509 Accuracy 0.8529999852180481\n",
      "Iteration 24520 Training loss 0.060297466814517975 Validation loss 0.05576452612876892 Accuracy 0.8560000061988831\n",
      "Iteration 24530 Training loss 0.06199092045426369 Validation loss 0.055672358721494675 Accuracy 0.8560000061988831\n",
      "Iteration 24540 Training loss 0.060213468968868256 Validation loss 0.05391533300280571 Accuracy 0.8615000247955322\n",
      "Iteration 24550 Training loss 0.06371767818927765 Validation loss 0.05449429154396057 Accuracy 0.8600000143051147\n",
      "Iteration 24560 Training loss 0.055244069546461105 Validation loss 0.05408007651567459 Accuracy 0.8619999885559082\n",
      "Iteration 24570 Training loss 0.06593775749206543 Validation loss 0.05704530328512192 Accuracy 0.8529999852180481\n",
      "Iteration 24580 Training loss 0.05983901396393776 Validation loss 0.05485852062702179 Accuracy 0.8585000038146973\n",
      "Iteration 24590 Training loss 0.05754586309194565 Validation loss 0.05425520986318588 Accuracy 0.8619999885559082\n",
      "Iteration 24600 Training loss 0.05421782284975052 Validation loss 0.05735311284661293 Accuracy 0.8610000014305115\n",
      "Iteration 24610 Training loss 0.0644262358546257 Validation loss 0.07044371217489243 Accuracy 0.828499972820282\n",
      "Iteration 24620 Training loss 0.0606345571577549 Validation loss 0.06843625754117966 Accuracy 0.8320000171661377\n",
      "Iteration 24630 Training loss 0.061370763927698135 Validation loss 0.06503591686487198 Accuracy 0.8379999995231628\n",
      "Iteration 24640 Training loss 0.0635252445936203 Validation loss 0.06876546889543533 Accuracy 0.8309999704360962\n",
      "Iteration 24650 Training loss 0.0631166324019432 Validation loss 0.06673198193311691 Accuracy 0.8364999890327454\n",
      "Iteration 24660 Training loss 0.06805136799812317 Validation loss 0.07007718086242676 Accuracy 0.8289999961853027\n",
      "Iteration 24670 Training loss 0.05738973245024681 Validation loss 0.06483060866594315 Accuracy 0.8410000205039978\n",
      "Iteration 24680 Training loss 0.058648936450481415 Validation loss 0.060027241706848145 Accuracy 0.8510000109672546\n",
      "Iteration 24690 Training loss 0.06523390114307404 Validation loss 0.065569669008255 Accuracy 0.8399999737739563\n",
      "Iteration 24700 Training loss 0.06341589242219925 Validation loss 0.06557653844356537 Accuracy 0.8395000100135803\n",
      "Iteration 24710 Training loss 0.06393350660800934 Validation loss 0.06580980122089386 Accuracy 0.8399999737739563\n",
      "Iteration 24720 Training loss 0.05901049077510834 Validation loss 0.06055893748998642 Accuracy 0.8495000004768372\n",
      "Iteration 24730 Training loss 0.06862858682870865 Validation loss 0.07409573346376419 Accuracy 0.8199999928474426\n",
      "Iteration 24740 Training loss 0.061195872724056244 Validation loss 0.06565466523170471 Accuracy 0.8420000076293945\n",
      "Iteration 24750 Training loss 0.059125155210494995 Validation loss 0.06541552394628525 Accuracy 0.8420000076293945\n",
      "Iteration 24760 Training loss 0.06608045846223831 Validation loss 0.0724165290594101 Accuracy 0.8240000009536743\n",
      "Iteration 24770 Training loss 0.060148935765028 Validation loss 0.06461985409259796 Accuracy 0.8424999713897705\n",
      "Iteration 24780 Training loss 0.06043025851249695 Validation loss 0.06305072456598282 Accuracy 0.8479999899864197\n",
      "Iteration 24790 Training loss 0.05721443518996239 Validation loss 0.0600740984082222 Accuracy 0.8519999980926514\n",
      "Iteration 24800 Training loss 0.061551738530397415 Validation loss 0.0635935366153717 Accuracy 0.8424999713897705\n",
      "Iteration 24810 Training loss 0.06640291959047318 Validation loss 0.06687603145837784 Accuracy 0.8364999890327454\n",
      "Iteration 24820 Training loss 0.06740185618400574 Validation loss 0.06882717460393906 Accuracy 0.8330000042915344\n",
      "Iteration 24830 Training loss 0.06147470697760582 Validation loss 0.061237040907144547 Accuracy 0.8495000004768372\n",
      "Iteration 24840 Training loss 0.062464844435453415 Validation loss 0.06722403317689896 Accuracy 0.8379999995231628\n",
      "Iteration 24850 Training loss 0.06384606659412384 Validation loss 0.06521651893854141 Accuracy 0.8420000076293945\n",
      "Iteration 24860 Training loss 0.0631319209933281 Validation loss 0.06997322291135788 Accuracy 0.828000009059906\n",
      "Iteration 24870 Training loss 0.07043686509132385 Validation loss 0.07438783347606659 Accuracy 0.8199999928474426\n",
      "Iteration 24880 Training loss 0.06340258568525314 Validation loss 0.0704444870352745 Accuracy 0.8274999856948853\n",
      "Iteration 24890 Training loss 0.057835739105939865 Validation loss 0.06783707439899445 Accuracy 0.8330000042915344\n",
      "Iteration 24900 Training loss 0.06951730698347092 Validation loss 0.07279623299837112 Accuracy 0.8234999775886536\n",
      "Iteration 24910 Training loss 0.06278005242347717 Validation loss 0.06457105278968811 Accuracy 0.8445000052452087\n",
      "Iteration 24920 Training loss 0.06065237522125244 Validation loss 0.06696194410324097 Accuracy 0.8339999914169312\n",
      "Iteration 24930 Training loss 0.06549698114395142 Validation loss 0.06706850975751877 Accuracy 0.8379999995231628\n",
      "Iteration 24940 Training loss 0.062028951942920685 Validation loss 0.06200100854039192 Accuracy 0.8479999899864197\n",
      "Iteration 24950 Training loss 0.06717617809772491 Validation loss 0.06947647780179977 Accuracy 0.8289999961853027\n",
      "Iteration 24960 Training loss 0.06498260051012039 Validation loss 0.06878054887056351 Accuracy 0.8305000066757202\n",
      "Iteration 24970 Training loss 0.06504655629396439 Validation loss 0.06516578793525696 Accuracy 0.8420000076293945\n",
      "Iteration 24980 Training loss 0.06683127582073212 Validation loss 0.07175643742084503 Accuracy 0.8240000009536743\n",
      "Iteration 24990 Training loss 0.06251340359449387 Validation loss 0.06788961589336395 Accuracy 0.8349999785423279\n",
      "Iteration 25000 Training loss 0.07008856534957886 Validation loss 0.07220380753278732 Accuracy 0.8230000138282776\n",
      "Iteration 25010 Training loss 0.062193863093853 Validation loss 0.07134441286325455 Accuracy 0.8259999752044678\n",
      "Iteration 25020 Training loss 0.05929737910628319 Validation loss 0.061953965574502945 Accuracy 0.8475000262260437\n",
      "Iteration 25030 Training loss 0.058364640921354294 Validation loss 0.06425464898347855 Accuracy 0.8424999713897705\n",
      "Iteration 25040 Training loss 0.06382571160793304 Validation loss 0.06822793185710907 Accuracy 0.8339999914169312\n",
      "Iteration 25050 Training loss 0.055500324815511703 Validation loss 0.05959121137857437 Accuracy 0.8554999828338623\n",
      "Iteration 25060 Training loss 0.0725199282169342 Validation loss 0.0759439617395401 Accuracy 0.8134999871253967\n",
      "Iteration 25070 Training loss 0.06024771183729172 Validation loss 0.06204528734087944 Accuracy 0.8489999771118164\n",
      "Iteration 25080 Training loss 0.05629625916481018 Validation loss 0.06412431597709656 Accuracy 0.8445000052452087\n",
      "Iteration 25090 Training loss 0.06472024321556091 Validation loss 0.06813786178827286 Accuracy 0.8314999938011169\n",
      "Iteration 25100 Training loss 0.06913914531469345 Validation loss 0.06851776689291 Accuracy 0.8320000171661377\n",
      "Iteration 25110 Training loss 0.06465679407119751 Validation loss 0.06901123374700546 Accuracy 0.8305000066757202\n",
      "Iteration 25120 Training loss 0.062482282519340515 Validation loss 0.06823838502168655 Accuracy 0.8320000171661377\n",
      "Iteration 25130 Training loss 0.06303264200687408 Validation loss 0.06904694437980652 Accuracy 0.8339999914169312\n",
      "Iteration 25140 Training loss 0.06309828907251358 Validation loss 0.0709681287407875 Accuracy 0.828000009059906\n",
      "Iteration 25150 Training loss 0.06253742426633835 Validation loss 0.06886233389377594 Accuracy 0.8314999938011169\n",
      "Iteration 25160 Training loss 0.06427953392267227 Validation loss 0.0628434494137764 Accuracy 0.8445000052452087\n",
      "Iteration 25170 Training loss 0.06356201320886612 Validation loss 0.06739524006843567 Accuracy 0.8345000147819519\n",
      "Iteration 25180 Training loss 0.06640254706144333 Validation loss 0.07347287237644196 Accuracy 0.8245000243186951\n",
      "Iteration 25190 Training loss 0.05757181718945503 Validation loss 0.06522311270236969 Accuracy 0.8389999866485596\n",
      "Iteration 25200 Training loss 0.07133422046899796 Validation loss 0.07481402903795242 Accuracy 0.8184999823570251\n",
      "Iteration 25210 Training loss 0.06488130241632462 Validation loss 0.06942665576934814 Accuracy 0.8309999704360962\n",
      "Iteration 25220 Training loss 0.06456440687179565 Validation loss 0.07141971588134766 Accuracy 0.8234999775886536\n",
      "Iteration 25230 Training loss 0.06169741228222847 Validation loss 0.06677286326885223 Accuracy 0.8364999890327454\n",
      "Iteration 25240 Training loss 0.0645410493016243 Validation loss 0.0661318376660347 Accuracy 0.8364999890327454\n",
      "Iteration 25250 Training loss 0.054535772651433945 Validation loss 0.05839473009109497 Accuracy 0.8569999933242798\n",
      "Iteration 25260 Training loss 0.06783192604780197 Validation loss 0.07321527600288391 Accuracy 0.8205000162124634\n",
      "Iteration 25270 Training loss 0.06390410661697388 Validation loss 0.06367327272891998 Accuracy 0.8454999923706055\n",
      "Iteration 25280 Training loss 0.05855149030685425 Validation loss 0.06402697414159775 Accuracy 0.8410000205039978\n",
      "Iteration 25290 Training loss 0.059455059468746185 Validation loss 0.06179976835846901 Accuracy 0.8489999771118164\n",
      "Iteration 25300 Training loss 0.07002820074558258 Validation loss 0.07049532979726791 Accuracy 0.828000009059906\n",
      "Iteration 25310 Training loss 0.06380066275596619 Validation loss 0.06524144113063812 Accuracy 0.8395000100135803\n",
      "Iteration 25320 Training loss 0.06259602308273315 Validation loss 0.06902557611465454 Accuracy 0.8289999961853027\n",
      "Iteration 25330 Training loss 0.054916467517614365 Validation loss 0.06162673607468605 Accuracy 0.8489999771118164\n",
      "Iteration 25340 Training loss 0.06131674349308014 Validation loss 0.06358842551708221 Accuracy 0.8414999842643738\n",
      "Iteration 25350 Training loss 0.06359183043241501 Validation loss 0.06715512275695801 Accuracy 0.8345000147819519\n",
      "Iteration 25360 Training loss 0.06686332821846008 Validation loss 0.06736484169960022 Accuracy 0.8330000042915344\n",
      "Iteration 25370 Training loss 0.0643688514828682 Validation loss 0.06603509187698364 Accuracy 0.8399999737739563\n",
      "Iteration 25380 Training loss 0.0665539875626564 Validation loss 0.07356075942516327 Accuracy 0.8230000138282776\n",
      "Iteration 25390 Training loss 0.0663791075348854 Validation loss 0.07035540789365768 Accuracy 0.828499972820282\n",
      "Iteration 25400 Training loss 0.06430956721305847 Validation loss 0.06824303418397903 Accuracy 0.8324999809265137\n",
      "Iteration 25410 Training loss 0.06819896399974823 Validation loss 0.07031136006116867 Accuracy 0.8289999961853027\n",
      "Iteration 25420 Training loss 0.05775216594338417 Validation loss 0.06044582277536392 Accuracy 0.8514999747276306\n",
      "Iteration 25430 Training loss 0.05974163860082626 Validation loss 0.0651085153222084 Accuracy 0.8424999713897705\n",
      "Iteration 25440 Training loss 0.054898135364055634 Validation loss 0.0590021088719368 Accuracy 0.8560000061988831\n",
      "Iteration 25450 Training loss 0.0717582106590271 Validation loss 0.07355108857154846 Accuracy 0.8215000033378601\n",
      "Iteration 25460 Training loss 0.06618697941303253 Validation loss 0.069438636302948 Accuracy 0.8274999856948853\n",
      "Iteration 25470 Training loss 0.06756199896335602 Validation loss 0.06780233979225159 Accuracy 0.8339999914169312\n",
      "Iteration 25480 Training loss 0.066008061170578 Validation loss 0.0668521299958229 Accuracy 0.8379999995231628\n",
      "Iteration 25490 Training loss 0.060440730303525925 Validation loss 0.06336597353219986 Accuracy 0.8460000157356262\n",
      "Iteration 25500 Training loss 0.058480966836214066 Validation loss 0.05979912355542183 Accuracy 0.8539999723434448\n",
      "Iteration 25510 Training loss 0.0679270476102829 Validation loss 0.06921619176864624 Accuracy 0.8295000195503235\n",
      "Iteration 25520 Training loss 0.05952346697449684 Validation loss 0.06520518660545349 Accuracy 0.840499997138977\n",
      "Iteration 25530 Training loss 0.06165586784482002 Validation loss 0.06413077563047409 Accuracy 0.8385000228881836\n",
      "Iteration 25540 Training loss 0.06741953641176224 Validation loss 0.0704914852976799 Accuracy 0.828499972820282\n",
      "Iteration 25550 Training loss 0.06714017689228058 Validation loss 0.0673176646232605 Accuracy 0.8345000147819519\n",
      "Iteration 25560 Training loss 0.06055990979075432 Validation loss 0.06667312979698181 Accuracy 0.8355000019073486\n",
      "Iteration 25570 Training loss 0.056796424090862274 Validation loss 0.0588298961520195 Accuracy 0.8544999957084656\n",
      "Iteration 25580 Training loss 0.06688693165779114 Validation loss 0.06920059025287628 Accuracy 0.8299999833106995\n",
      "Iteration 25590 Training loss 0.06050572171807289 Validation loss 0.06416214257478714 Accuracy 0.8429999947547913\n",
      "Iteration 25600 Training loss 0.06586586683988571 Validation loss 0.06794685125350952 Accuracy 0.8330000042915344\n",
      "Iteration 25610 Training loss 0.06131948158144951 Validation loss 0.06391603499650955 Accuracy 0.843500018119812\n",
      "Iteration 25620 Training loss 0.060234230011701584 Validation loss 0.061449356377124786 Accuracy 0.8500000238418579\n",
      "Iteration 25630 Training loss 0.06470780819654465 Validation loss 0.0702042207121849 Accuracy 0.8289999961853027\n",
      "Iteration 25640 Training loss 0.06888416409492493 Validation loss 0.07668305933475494 Accuracy 0.8144999742507935\n",
      "Iteration 25650 Training loss 0.07000329345464706 Validation loss 0.07067370414733887 Accuracy 0.828000009059906\n",
      "Iteration 25660 Training loss 0.06179284676909447 Validation loss 0.06429895758628845 Accuracy 0.8420000076293945\n",
      "Iteration 25670 Training loss 0.05956210568547249 Validation loss 0.06834490597248077 Accuracy 0.8339999914169312\n",
      "Iteration 25680 Training loss 0.06764094531536102 Validation loss 0.06509369611740112 Accuracy 0.8399999737739563\n",
      "Iteration 25690 Training loss 0.05845681205391884 Validation loss 0.05856587365269661 Accuracy 0.8554999828338623\n",
      "Iteration 25700 Training loss 0.06260168552398682 Validation loss 0.06600242853164673 Accuracy 0.8410000205039978\n",
      "Iteration 25710 Training loss 0.05531468987464905 Validation loss 0.06126474589109421 Accuracy 0.8514999747276306\n",
      "Iteration 25720 Training loss 0.0651358962059021 Validation loss 0.0662754625082016 Accuracy 0.8395000100135803\n",
      "Iteration 25730 Training loss 0.06431751698255539 Validation loss 0.06837978959083557 Accuracy 0.8305000066757202\n",
      "Iteration 25740 Training loss 0.06397943198680878 Validation loss 0.06260465085506439 Accuracy 0.847000002861023\n",
      "Iteration 25750 Training loss 0.06426908075809479 Validation loss 0.06562593579292297 Accuracy 0.8395000100135803\n",
      "Iteration 25760 Training loss 0.05982524901628494 Validation loss 0.06718193739652634 Accuracy 0.8360000252723694\n",
      "Iteration 25770 Training loss 0.06977930665016174 Validation loss 0.07018468528985977 Accuracy 0.8289999961853027\n",
      "Iteration 25780 Training loss 0.059417229145765305 Validation loss 0.06036579981446266 Accuracy 0.8519999980926514\n",
      "Iteration 25790 Training loss 0.061189308762550354 Validation loss 0.0665680319070816 Accuracy 0.8389999866485596\n",
      "Iteration 25800 Training loss 0.062465861439704895 Validation loss 0.06792657822370529 Accuracy 0.8355000019073486\n",
      "Iteration 25810 Training loss 0.06432897597551346 Validation loss 0.06847326457500458 Accuracy 0.8309999704360962\n",
      "Iteration 25820 Training loss 0.060909707099199295 Validation loss 0.06569519639015198 Accuracy 0.8385000228881836\n",
      "Iteration 25830 Training loss 0.06461358815431595 Validation loss 0.06533157825469971 Accuracy 0.840499997138977\n",
      "Iteration 25840 Training loss 0.06602706015110016 Validation loss 0.06768935918807983 Accuracy 0.8335000276565552\n",
      "Iteration 25850 Training loss 0.07323788851499557 Validation loss 0.07492347061634064 Accuracy 0.8220000267028809\n",
      "Iteration 25860 Training loss 0.06651153415441513 Validation loss 0.0690823644399643 Accuracy 0.8314999938011169\n",
      "Iteration 25870 Training loss 0.05809583514928818 Validation loss 0.0663057193160057 Accuracy 0.8370000123977661\n",
      "Iteration 25880 Training loss 0.06321834027767181 Validation loss 0.0661316066980362 Accuracy 0.8379999995231628\n",
      "Iteration 25890 Training loss 0.06019417569041252 Validation loss 0.06676258146762848 Accuracy 0.8379999995231628\n",
      "Iteration 25900 Training loss 0.06397281587123871 Validation loss 0.06543046981096268 Accuracy 0.8414999842643738\n",
      "Iteration 25910 Training loss 0.061877403408288956 Validation loss 0.06252807378768921 Accuracy 0.8495000004768372\n",
      "Iteration 25920 Training loss 0.061207689344882965 Validation loss 0.0655796229839325 Accuracy 0.8399999737739563\n",
      "Iteration 25930 Training loss 0.05456685274839401 Validation loss 0.06219935789704323 Accuracy 0.8460000157356262\n",
      "Iteration 25940 Training loss 0.06418958306312561 Validation loss 0.06588633358478546 Accuracy 0.8395000100135803\n",
      "Iteration 25950 Training loss 0.05986574664711952 Validation loss 0.06813745945692062 Accuracy 0.8330000042915344\n",
      "Iteration 25960 Training loss 0.057003203779459 Validation loss 0.05898556485772133 Accuracy 0.8535000085830688\n",
      "Iteration 25970 Training loss 0.06782855093479156 Validation loss 0.07211324572563171 Accuracy 0.8245000243186951\n",
      "Iteration 25980 Training loss 0.0603751614689827 Validation loss 0.06444110721349716 Accuracy 0.8399999737739563\n",
      "Iteration 25990 Training loss 0.05843757465481758 Validation loss 0.06132936105132103 Accuracy 0.8489999771118164\n",
      "Iteration 26000 Training loss 0.06633604317903519 Validation loss 0.06743091344833374 Accuracy 0.8355000019073486\n",
      "Iteration 26010 Training loss 0.06308558583259583 Validation loss 0.06509566307067871 Accuracy 0.840499997138977\n",
      "Iteration 26020 Training loss 0.059856582432985306 Validation loss 0.05517030507326126 Accuracy 0.8575000166893005\n",
      "Iteration 26030 Training loss 0.06290948390960693 Validation loss 0.05639047548174858 Accuracy 0.8550000190734863\n",
      "Iteration 26040 Training loss 0.060600630939006805 Validation loss 0.054818298667669296 Accuracy 0.8575000166893005\n",
      "Iteration 26050 Training loss 0.05941303074359894 Validation loss 0.053873032331466675 Accuracy 0.8619999885559082\n",
      "Iteration 26060 Training loss 0.06595064699649811 Validation loss 0.055489104241132736 Accuracy 0.8569999933242798\n",
      "Iteration 26070 Training loss 0.06526617705821991 Validation loss 0.0579841174185276 Accuracy 0.8539999723434448\n",
      "Iteration 26080 Training loss 0.05715860426425934 Validation loss 0.053741518408060074 Accuracy 0.8650000095367432\n",
      "Iteration 26090 Training loss 0.06808876991271973 Validation loss 0.056237053126096725 Accuracy 0.8565000295639038\n",
      "Iteration 26100 Training loss 0.06934170424938202 Validation loss 0.05682108923792839 Accuracy 0.8560000061988831\n",
      "Iteration 26110 Training loss 0.07027696073055267 Validation loss 0.06011844426393509 Accuracy 0.8539999723434448\n",
      "Iteration 26120 Training loss 0.061157312244176865 Validation loss 0.05432175099849701 Accuracy 0.8610000014305115\n",
      "Iteration 26130 Training loss 0.0634613186120987 Validation loss 0.05545812472701073 Accuracy 0.8565000295639038\n",
      "Iteration 26140 Training loss 0.05962200090289116 Validation loss 0.05350889265537262 Accuracy 0.8629999756813049\n",
      "Iteration 26150 Training loss 0.06362549960613251 Validation loss 0.05726269260048866 Accuracy 0.8554999828338623\n",
      "Iteration 26160 Training loss 0.06165679916739464 Validation loss 0.05513240769505501 Accuracy 0.8575000166893005\n",
      "Iteration 26170 Training loss 0.061421964317560196 Validation loss 0.05692886561155319 Accuracy 0.8579999804496765\n",
      "Iteration 26180 Training loss 0.06619971245527267 Validation loss 0.05943653732538223 Accuracy 0.8550000190734863\n",
      "Iteration 26190 Training loss 0.06056568771600723 Validation loss 0.05408119410276413 Accuracy 0.8604999780654907\n",
      "Iteration 26200 Training loss 0.057488661259412766 Validation loss 0.05538041517138481 Accuracy 0.8565000295639038\n",
      "Iteration 26210 Training loss 0.0684175193309784 Validation loss 0.05994611978530884 Accuracy 0.8539999723434448\n",
      "Iteration 26220 Training loss 0.06255501508712769 Validation loss 0.05484617128968239 Accuracy 0.8560000061988831\n",
      "Iteration 26230 Training loss 0.06554844230413437 Validation loss 0.05708985775709152 Accuracy 0.8535000085830688\n",
      "Iteration 26240 Training loss 0.05327291041612625 Validation loss 0.05402945354580879 Accuracy 0.8634999990463257\n",
      "Iteration 26250 Training loss 0.06358376145362854 Validation loss 0.05601518228650093 Accuracy 0.8560000061988831\n",
      "Iteration 26260 Training loss 0.07401997596025467 Validation loss 0.0634998306632042 Accuracy 0.8445000052452087\n",
      "Iteration 26270 Training loss 0.07152315229177475 Validation loss 0.0608331523835659 Accuracy 0.8510000109672546\n",
      "Iteration 26280 Training loss 0.06244565173983574 Validation loss 0.05645311623811722 Accuracy 0.8539999723434448\n",
      "Iteration 26290 Training loss 0.06296874582767487 Validation loss 0.0553804412484169 Accuracy 0.8554999828338623\n",
      "Iteration 26300 Training loss 0.06466174870729446 Validation loss 0.05659792944788933 Accuracy 0.8544999957084656\n",
      "Iteration 26310 Training loss 0.063224658370018 Validation loss 0.055663466453552246 Accuracy 0.8544999957084656\n",
      "Iteration 26320 Training loss 0.05543520674109459 Validation loss 0.05561848357319832 Accuracy 0.8550000190734863\n",
      "Iteration 26330 Training loss 0.053309570997953415 Validation loss 0.055001646280288696 Accuracy 0.8634999990463257\n",
      "Iteration 26340 Training loss 0.08021394908428192 Validation loss 0.06640245020389557 Accuracy 0.8355000019073486\n",
      "Iteration 26350 Training loss 0.05711447447538376 Validation loss 0.054271843284368515 Accuracy 0.859000027179718\n",
      "Iteration 26360 Training loss 0.057703807950019836 Validation loss 0.05418001487851143 Accuracy 0.8610000014305115\n",
      "Iteration 26370 Training loss 0.053495895117521286 Validation loss 0.059697475284338 Accuracy 0.8514999747276306\n",
      "Iteration 26380 Training loss 0.06870131194591522 Validation loss 0.07010377198457718 Accuracy 0.828499972820282\n",
      "Iteration 26390 Training loss 0.06599466502666473 Validation loss 0.06661587208509445 Accuracy 0.8389999866485596\n",
      "Iteration 26400 Training loss 0.06568031013011932 Validation loss 0.0656367614865303 Accuracy 0.8389999866485596\n",
      "Iteration 26410 Training loss 0.06750250607728958 Validation loss 0.06879029422998428 Accuracy 0.8335000276565552\n",
      "Iteration 26420 Training loss 0.06547223031520844 Validation loss 0.06734834611415863 Accuracy 0.8385000228881836\n",
      "Iteration 26430 Training loss 0.061289817094802856 Validation loss 0.061124227941036224 Accuracy 0.8495000004768372\n",
      "Iteration 26440 Training loss 0.06130701303482056 Validation loss 0.06456110626459122 Accuracy 0.8424999713897705\n",
      "Iteration 26450 Training loss 0.0635240450501442 Validation loss 0.06603167951107025 Accuracy 0.8374999761581421\n",
      "Iteration 26460 Training loss 0.056733567267656326 Validation loss 0.06278545409440994 Accuracy 0.8460000157356262\n",
      "Iteration 26470 Training loss 0.06827446818351746 Validation loss 0.0670117512345314 Accuracy 0.8374999761581421\n",
      "Iteration 26480 Training loss 0.06318650394678116 Validation loss 0.06539124995470047 Accuracy 0.8385000228881836\n",
      "Iteration 26490 Training loss 0.06029732897877693 Validation loss 0.06236445531249046 Accuracy 0.8500000238418579\n",
      "Iteration 26500 Training loss 0.0657903403043747 Validation loss 0.06590566039085388 Accuracy 0.8399999737739563\n",
      "Iteration 26510 Training loss 0.059006087481975555 Validation loss 0.06019160524010658 Accuracy 0.8535000085830688\n",
      "Iteration 26520 Training loss 0.059095486998558044 Validation loss 0.06254835426807404 Accuracy 0.8475000262260437\n",
      "Iteration 26530 Training loss 0.06314651668071747 Validation loss 0.06261558085680008 Accuracy 0.8424999713897705\n",
      "Iteration 26540 Training loss 0.06456319987773895 Validation loss 0.06165476515889168 Accuracy 0.8475000262260437\n",
      "Iteration 26550 Training loss 0.06834288686513901 Validation loss 0.06827956438064575 Accuracy 0.8335000276565552\n",
      "Iteration 26560 Training loss 0.061528343707323074 Validation loss 0.06498605012893677 Accuracy 0.8395000100135803\n",
      "Iteration 26570 Training loss 0.06862998753786087 Validation loss 0.0656607374548912 Accuracy 0.8389999866485596\n",
      "Iteration 26580 Training loss 0.07109633833169937 Validation loss 0.0700426921248436 Accuracy 0.828499972820282\n",
      "Iteration 26590 Training loss 0.06101059541106224 Validation loss 0.06293399631977081 Accuracy 0.8450000286102295\n",
      "Iteration 26600 Training loss 0.05905210226774216 Validation loss 0.059431545436382294 Accuracy 0.8539999723434448\n",
      "Iteration 26610 Training loss 0.06541438400745392 Validation loss 0.07021600008010864 Accuracy 0.8274999856948853\n",
      "Iteration 26620 Training loss 0.0641406998038292 Validation loss 0.068740114569664 Accuracy 0.8309999704360962\n",
      "Iteration 26630 Training loss 0.05847516655921936 Validation loss 0.06099902465939522 Accuracy 0.8485000133514404\n",
      "Iteration 26640 Training loss 0.05927906930446625 Validation loss 0.05912277474999428 Accuracy 0.8535000085830688\n",
      "Iteration 26650 Training loss 0.062180425971746445 Validation loss 0.06751753389835358 Accuracy 0.8349999785423279\n",
      "Iteration 26660 Training loss 0.058578845113515854 Validation loss 0.059965699911117554 Accuracy 0.8519999980926514\n",
      "Iteration 26670 Training loss 0.06156192719936371 Validation loss 0.06219428405165672 Accuracy 0.8460000157356262\n",
      "Iteration 26680 Training loss 0.06716690957546234 Validation loss 0.07046690583229065 Accuracy 0.828000009059906\n",
      "Iteration 26690 Training loss 0.0638512521982193 Validation loss 0.07054891437292099 Accuracy 0.8274999856948853\n",
      "Iteration 26700 Training loss 0.07566455006599426 Validation loss 0.0729152038693428 Accuracy 0.8234999775886536\n",
      "Iteration 26710 Training loss 0.06249065324664116 Validation loss 0.06427507847547531 Accuracy 0.8424999713897705\n",
      "Iteration 26720 Training loss 0.062157854437828064 Validation loss 0.06994767487049103 Accuracy 0.8295000195503235\n",
      "Iteration 26730 Training loss 0.0657450333237648 Validation loss 0.07281428575515747 Accuracy 0.8234999775886536\n",
      "Iteration 26740 Training loss 0.06288883090019226 Validation loss 0.06532080471515656 Accuracy 0.8389999866485596\n",
      "Iteration 26750 Training loss 0.06787124276161194 Validation loss 0.0681338682770729 Accuracy 0.8345000147819519\n",
      "Iteration 26760 Training loss 0.06088954582810402 Validation loss 0.06482118368148804 Accuracy 0.8385000228881836\n",
      "Iteration 26770 Training loss 0.06269712001085281 Validation loss 0.06642121821641922 Accuracy 0.8385000228881836\n",
      "Iteration 26780 Training loss 0.06820055842399597 Validation loss 0.06729001551866531 Accuracy 0.8370000123977661\n",
      "Iteration 26790 Training loss 0.06369840353727341 Validation loss 0.06799422949552536 Accuracy 0.8335000276565552\n",
      "Iteration 26800 Training loss 0.062130507081747055 Validation loss 0.06503186374902725 Accuracy 0.8445000052452087\n",
      "Iteration 26810 Training loss 0.0621802918612957 Validation loss 0.06561128050088882 Accuracy 0.840499997138977\n",
      "Iteration 26820 Training loss 0.066553495824337 Validation loss 0.06345608085393906 Accuracy 0.8450000286102295\n",
      "Iteration 26830 Training loss 0.0636131688952446 Validation loss 0.06537504494190216 Accuracy 0.8414999842643738\n",
      "Iteration 26840 Training loss 0.065146803855896 Validation loss 0.06904356181621552 Accuracy 0.8309999704360962\n",
      "Iteration 26850 Training loss 0.06371563673019409 Validation loss 0.06635043025016785 Accuracy 0.8374999761581421\n",
      "Iteration 26860 Training loss 0.06262408941984177 Validation loss 0.06910925358533859 Accuracy 0.8309999704360962\n",
      "Iteration 26870 Training loss 0.059003110975027084 Validation loss 0.059848587960004807 Accuracy 0.8544999957084656\n",
      "Iteration 26880 Training loss 0.05955886095762253 Validation loss 0.06626790016889572 Accuracy 0.8385000228881836\n",
      "Iteration 26890 Training loss 0.06425359845161438 Validation loss 0.07035006582736969 Accuracy 0.8274999856948853\n",
      "Iteration 26900 Training loss 0.07270950824022293 Validation loss 0.07368666678667068 Accuracy 0.8199999928474426\n",
      "Iteration 26910 Training loss 0.05824512988328934 Validation loss 0.06544565409421921 Accuracy 0.843500018119812\n",
      "Iteration 26920 Training loss 0.056495726108551025 Validation loss 0.05756286531686783 Accuracy 0.859000027179718\n",
      "Iteration 26930 Training loss 0.06630121171474457 Validation loss 0.06900408118963242 Accuracy 0.8330000042915344\n",
      "Iteration 26940 Training loss 0.06060928851366043 Validation loss 0.062301602214574814 Accuracy 0.8460000157356262\n",
      "Iteration 26950 Training loss 0.05868370458483696 Validation loss 0.06003529950976372 Accuracy 0.8529999852180481\n",
      "Iteration 26960 Training loss 0.05917605385184288 Validation loss 0.0659981444478035 Accuracy 0.8389999866485596\n",
      "Iteration 26970 Training loss 0.07679974287748337 Validation loss 0.08151017129421234 Accuracy 0.7979999780654907\n",
      "Iteration 26980 Training loss 0.06161439046263695 Validation loss 0.06565206497907639 Accuracy 0.8385000228881836\n",
      "Iteration 26990 Training loss 0.060514919459819794 Validation loss 0.06486834585666656 Accuracy 0.8410000205039978\n",
      "Iteration 27000 Training loss 0.07297880947589874 Validation loss 0.07290033251047134 Accuracy 0.8230000138282776\n",
      "Iteration 27010 Training loss 0.06074921786785126 Validation loss 0.06760352849960327 Accuracy 0.8364999890327454\n",
      "Iteration 27020 Training loss 0.06584993004798889 Validation loss 0.07312239706516266 Accuracy 0.8234999775886536\n",
      "Iteration 27030 Training loss 0.0628184974193573 Validation loss 0.06376680731773376 Accuracy 0.8454999923706055\n",
      "Iteration 27040 Training loss 0.05876197665929794 Validation loss 0.06468963623046875 Accuracy 0.8410000205039978\n",
      "Iteration 27050 Training loss 0.060768283903598785 Validation loss 0.06798393279314041 Accuracy 0.8335000276565552\n",
      "Iteration 27060 Training loss 0.06892291456460953 Validation loss 0.07028058171272278 Accuracy 0.8295000195503235\n",
      "Iteration 27070 Training loss 0.06051023304462433 Validation loss 0.06918688863515854 Accuracy 0.8335000276565552\n",
      "Iteration 27080 Training loss 0.0613357312977314 Validation loss 0.06506844609975815 Accuracy 0.8420000076293945\n",
      "Iteration 27090 Training loss 0.061439692974090576 Validation loss 0.06720627099275589 Accuracy 0.8374999761581421\n",
      "Iteration 27100 Training loss 0.06448165327310562 Validation loss 0.06632755696773529 Accuracy 0.8410000205039978\n",
      "Iteration 27110 Training loss 0.060806404799222946 Validation loss 0.06359114497900009 Accuracy 0.8445000052452087\n",
      "Iteration 27120 Training loss 0.06505148112773895 Validation loss 0.06678128242492676 Accuracy 0.8374999761581421\n",
      "Iteration 27130 Training loss 0.06850457936525345 Validation loss 0.06889739632606506 Accuracy 0.8345000147819519\n",
      "Iteration 27140 Training loss 0.061767060309648514 Validation loss 0.06505411118268967 Accuracy 0.8420000076293945\n",
      "Iteration 27150 Training loss 0.06732191890478134 Validation loss 0.06776498258113861 Accuracy 0.8364999890327454\n",
      "Iteration 27160 Training loss 0.0637001171708107 Validation loss 0.06760865449905396 Accuracy 0.8360000252723694\n",
      "Iteration 27170 Training loss 0.06399022787809372 Validation loss 0.0649646446108818 Accuracy 0.8414999842643738\n",
      "Iteration 27180 Training loss 0.05390620604157448 Validation loss 0.05933544412255287 Accuracy 0.8529999852180481\n",
      "Iteration 27190 Training loss 0.07039824873209 Validation loss 0.06974493712186813 Accuracy 0.8320000171661377\n",
      "Iteration 27200 Training loss 0.07309974730014801 Validation loss 0.07224268466234207 Accuracy 0.8224999904632568\n",
      "Iteration 27210 Training loss 0.06177063658833504 Validation loss 0.061484675854444504 Accuracy 0.8485000133514404\n",
      "Iteration 27220 Training loss 0.06538909673690796 Validation loss 0.06773960590362549 Accuracy 0.8360000252723694\n",
      "Iteration 27230 Training loss 0.05964220315217972 Validation loss 0.06293284147977829 Accuracy 0.8450000286102295\n",
      "Iteration 27240 Training loss 0.06319696456193924 Validation loss 0.06597314029932022 Accuracy 0.8410000205039978\n",
      "Iteration 27250 Training loss 0.06123100966215134 Validation loss 0.06690888851881027 Accuracy 0.8389999866485596\n",
      "Iteration 27260 Training loss 0.06360210478305817 Validation loss 0.06782560795545578 Accuracy 0.8360000252723694\n",
      "Iteration 27270 Training loss 0.056275177747011185 Validation loss 0.05966900661587715 Accuracy 0.8535000085830688\n",
      "Iteration 27280 Training loss 0.06127512454986572 Validation loss 0.06873433291912079 Accuracy 0.8335000276565552\n",
      "Iteration 27290 Training loss 0.06430227309465408 Validation loss 0.07215019315481186 Accuracy 0.8245000243186951\n",
      "Iteration 27300 Training loss 0.05757806450128555 Validation loss 0.062315598130226135 Accuracy 0.8460000157356262\n",
      "Iteration 27310 Training loss 0.07200436294078827 Validation loss 0.06905815005302429 Accuracy 0.8330000042915344\n",
      "Iteration 27320 Training loss 0.062308527529239655 Validation loss 0.06387996673583984 Accuracy 0.8454999923706055\n",
      "Iteration 27330 Training loss 0.05913912132382393 Validation loss 0.06317581981420517 Accuracy 0.8479999899864197\n",
      "Iteration 27340 Training loss 0.06890420615673065 Validation loss 0.06988588720560074 Accuracy 0.8299999833106995\n",
      "Iteration 27350 Training loss 0.06517358869314194 Validation loss 0.06711331754922867 Accuracy 0.8370000123977661\n",
      "Iteration 27360 Training loss 0.06643322855234146 Validation loss 0.06558254361152649 Accuracy 0.8395000100135803\n",
      "Iteration 27370 Training loss 0.061487071216106415 Validation loss 0.06581922620534897 Accuracy 0.8395000100135803\n",
      "Iteration 27380 Training loss 0.06450743228197098 Validation loss 0.06669119745492935 Accuracy 0.8364999890327454\n",
      "Iteration 27390 Training loss 0.06092432141304016 Validation loss 0.06495356559753418 Accuracy 0.8420000076293945\n",
      "Iteration 27400 Training loss 0.0655779242515564 Validation loss 0.07080568373203278 Accuracy 0.828499972820282\n",
      "Iteration 27410 Training loss 0.060394152998924255 Validation loss 0.06412488967180252 Accuracy 0.8450000286102295\n",
      "Iteration 27420 Training loss 0.0605173222720623 Validation loss 0.06575681269168854 Accuracy 0.8410000205039978\n",
      "Iteration 27430 Training loss 0.06327782571315765 Validation loss 0.06779226660728455 Accuracy 0.8360000252723694\n",
      "Iteration 27440 Training loss 0.05991005524992943 Validation loss 0.06414739042520523 Accuracy 0.8464999794960022\n",
      "Iteration 27450 Training loss 0.06082182377576828 Validation loss 0.061770252883434296 Accuracy 0.8500000238418579\n",
      "Iteration 27460 Training loss 0.06179909035563469 Validation loss 0.06350266188383102 Accuracy 0.8450000286102295\n",
      "Iteration 27470 Training loss 0.06969378888607025 Validation loss 0.07100746035575867 Accuracy 0.8270000219345093\n",
      "Iteration 27480 Training loss 0.0721537321805954 Validation loss 0.07374246418476105 Accuracy 0.8240000009536743\n",
      "Iteration 27490 Training loss 0.056536681950092316 Validation loss 0.05950796604156494 Accuracy 0.8539999723434448\n",
      "Iteration 27500 Training loss 0.06201746687293053 Validation loss 0.06461409479379654 Accuracy 0.843999981880188\n",
      "Iteration 27510 Training loss 0.06390466541051865 Validation loss 0.06437604129314423 Accuracy 0.8414999842643738\n",
      "Iteration 27520 Training loss 0.06025904789566994 Validation loss 0.0649639219045639 Accuracy 0.8389999866485596\n",
      "Iteration 27530 Training loss 0.06046662479639053 Validation loss 0.06055481359362602 Accuracy 0.8514999747276306\n",
      "Iteration 27540 Training loss 0.06339137256145477 Validation loss 0.06612608581781387 Accuracy 0.8385000228881836\n",
      "Iteration 27550 Training loss 0.06392795592546463 Validation loss 0.0666096955537796 Accuracy 0.8379999995231628\n",
      "Iteration 27560 Training loss 0.0636134073138237 Validation loss 0.06406953185796738 Accuracy 0.8414999842643738\n",
      "Iteration 27570 Training loss 0.06178317964076996 Validation loss 0.06306234747171402 Accuracy 0.843500018119812\n",
      "Iteration 27580 Training loss 0.0631164014339447 Validation loss 0.06375391036272049 Accuracy 0.8460000157356262\n",
      "Iteration 27590 Training loss 0.06365767866373062 Validation loss 0.07018542289733887 Accuracy 0.8299999833106995\n",
      "Iteration 27600 Training loss 0.06360927224159241 Validation loss 0.0675375759601593 Accuracy 0.8355000019073486\n",
      "Iteration 27610 Training loss 0.06484338641166687 Validation loss 0.06153884157538414 Accuracy 0.8500000238418579\n",
      "Iteration 27620 Training loss 0.06207758188247681 Validation loss 0.0659351721405983 Accuracy 0.8410000205039978\n",
      "Iteration 27630 Training loss 0.07643064856529236 Validation loss 0.07333377003669739 Accuracy 0.8245000243186951\n",
      "Iteration 27640 Training loss 0.05756273493170738 Validation loss 0.06079140305519104 Accuracy 0.8504999876022339\n",
      "Iteration 27650 Training loss 0.0640123039484024 Validation loss 0.06511731445789337 Accuracy 0.8399999737739563\n",
      "Iteration 27660 Training loss 0.060539692640304565 Validation loss 0.06423179060220718 Accuracy 0.8420000076293945\n",
      "Iteration 27670 Training loss 0.07733842730522156 Validation loss 0.07329507172107697 Accuracy 0.824999988079071\n",
      "Iteration 27680 Training loss 0.06396182626485825 Validation loss 0.06705620139837265 Accuracy 0.8385000228881836\n",
      "Iteration 27690 Training loss 0.05661163106560707 Validation loss 0.059411827474832535 Accuracy 0.8529999852180481\n",
      "Iteration 27700 Training loss 0.0689140111207962 Validation loss 0.07436569035053253 Accuracy 0.8190000057220459\n",
      "Iteration 27710 Training loss 0.05907340347766876 Validation loss 0.06329655647277832 Accuracy 0.8450000286102295\n",
      "Iteration 27720 Training loss 0.059903692454099655 Validation loss 0.06478754431009293 Accuracy 0.8429999947547913\n",
      "Iteration 27730 Training loss 0.0680825486779213 Validation loss 0.06587029993534088 Accuracy 0.8389999866485596\n",
      "Iteration 27740 Training loss 0.06099697947502136 Validation loss 0.0662950724363327 Accuracy 0.8385000228881836\n",
      "Iteration 27750 Training loss 0.06272053718566895 Validation loss 0.06349731236696243 Accuracy 0.8460000157356262\n",
      "Iteration 27760 Training loss 0.07009266316890717 Validation loss 0.06995447725057602 Accuracy 0.8305000066757202\n",
      "Iteration 27770 Training loss 0.06257578730583191 Validation loss 0.062335189431905746 Accuracy 0.8454999923706055\n",
      "Iteration 27780 Training loss 0.07414760440587997 Validation loss 0.07439490407705307 Accuracy 0.8184999823570251\n",
      "Iteration 27790 Training loss 0.05720860883593559 Validation loss 0.061820581555366516 Accuracy 0.847000002861023\n",
      "Iteration 27800 Training loss 0.06654144078493118 Validation loss 0.06848759949207306 Accuracy 0.8314999938011169\n",
      "Iteration 27810 Training loss 0.06173931062221527 Validation loss 0.06124411150813103 Accuracy 0.8504999876022339\n",
      "Iteration 27820 Training loss 0.06556464731693268 Validation loss 0.07004756480455399 Accuracy 0.828000009059906\n",
      "Iteration 27830 Training loss 0.06457884609699249 Validation loss 0.06240884214639664 Accuracy 0.8479999899864197\n",
      "Iteration 27840 Training loss 0.054375920444726944 Validation loss 0.05799000710248947 Accuracy 0.8569999933242798\n",
      "Iteration 27850 Training loss 0.06470613926649094 Validation loss 0.06536231935024261 Accuracy 0.8399999737739563\n",
      "Iteration 27860 Training loss 0.06842188537120819 Validation loss 0.07494981586933136 Accuracy 0.8190000057220459\n",
      "Iteration 27870 Training loss 0.06428569555282593 Validation loss 0.05456392467021942 Accuracy 0.8604999780654907\n",
      "Iteration 27880 Training loss 0.05566232278943062 Validation loss 0.05436406657099724 Accuracy 0.8619999885559082\n",
      "Iteration 27890 Training loss 0.07115620374679565 Validation loss 0.0584830641746521 Accuracy 0.8550000190734863\n",
      "Iteration 27900 Training loss 0.06531958281993866 Validation loss 0.057208817452192307 Accuracy 0.8554999828338623\n",
      "Iteration 27910 Training loss 0.06231926754117012 Validation loss 0.05653362348675728 Accuracy 0.8539999723434448\n",
      "Iteration 27920 Training loss 0.06405007839202881 Validation loss 0.05822525918483734 Accuracy 0.8544999957084656\n",
      "Iteration 27930 Training loss 0.05940241366624832 Validation loss 0.05405425652861595 Accuracy 0.859499990940094\n",
      "Iteration 27940 Training loss 0.057846058160066605 Validation loss 0.05423618480563164 Accuracy 0.8575000166893005\n",
      "Iteration 27950 Training loss 0.06659398227930069 Validation loss 0.06051226332783699 Accuracy 0.8525000214576721\n",
      "Iteration 27960 Training loss 0.05687330290675163 Validation loss 0.054007042199373245 Accuracy 0.8634999990463257\n",
      "Iteration 27970 Training loss 0.06541185081005096 Validation loss 0.058770801872015 Accuracy 0.8550000190734863\n",
      "Iteration 27980 Training loss 0.061218321323394775 Validation loss 0.05623481795191765 Accuracy 0.8539999723434448\n",
      "Iteration 27990 Training loss 0.05773371830582619 Validation loss 0.05637025460600853 Accuracy 0.8529999852180481\n",
      "Iteration 28000 Training loss 0.06631428748369217 Validation loss 0.05714743956923485 Accuracy 0.8544999957084656\n",
      "Iteration 28010 Training loss 0.061148930341005325 Validation loss 0.05437936261296272 Accuracy 0.859499990940094\n",
      "Iteration 28020 Training loss 0.06921856105327606 Validation loss 0.059961386024951935 Accuracy 0.8525000214576721\n",
      "Iteration 28030 Training loss 0.0602397546172142 Validation loss 0.05547301471233368 Accuracy 0.8565000295639038\n",
      "Iteration 28040 Training loss 0.06776315718889236 Validation loss 0.05955351144075394 Accuracy 0.8544999957084656\n",
      "Iteration 28050 Training loss 0.06505521386861801 Validation loss 0.0600002221763134 Accuracy 0.8539999723434448\n",
      "Iteration 28060 Training loss 0.05526188760995865 Validation loss 0.05436084046959877 Accuracy 0.859000027179718\n",
      "Iteration 28070 Training loss 0.06493285298347473 Validation loss 0.06627355515956879 Accuracy 0.8385000228881836\n",
      "Iteration 28080 Training loss 0.06862282007932663 Validation loss 0.07266225665807724 Accuracy 0.824999988079071\n",
      "Iteration 28090 Training loss 0.07170592993497849 Validation loss 0.07437943667173386 Accuracy 0.8230000138282776\n",
      "Iteration 28100 Training loss 0.059633322060108185 Validation loss 0.06696976721286774 Accuracy 0.8360000252723694\n",
      "Iteration 28110 Training loss 0.07305290549993515 Validation loss 0.07338838279247284 Accuracy 0.8205000162124634\n",
      "Iteration 28120 Training loss 0.06143469363451004 Validation loss 0.06731448322534561 Accuracy 0.8360000252723694\n",
      "Iteration 28130 Training loss 0.06399523466825485 Validation loss 0.06421023607254028 Accuracy 0.843500018119812\n",
      "Iteration 28140 Training loss 0.06450820714235306 Validation loss 0.07022566348314285 Accuracy 0.8274999856948853\n",
      "Iteration 28150 Training loss 0.056751035153865814 Validation loss 0.05845869332551956 Accuracy 0.8550000190734863\n",
      "Iteration 28160 Training loss 0.06596960127353668 Validation loss 0.06930100917816162 Accuracy 0.8314999938011169\n",
      "Iteration 28170 Training loss 0.06724295020103455 Validation loss 0.07043049484491348 Accuracy 0.8270000219345093\n",
      "Iteration 28180 Training loss 0.0663609653711319 Validation loss 0.07094253599643707 Accuracy 0.8274999856948853\n",
      "Iteration 28190 Training loss 0.06753953546285629 Validation loss 0.0666121393442154 Accuracy 0.8385000228881836\n",
      "Iteration 28200 Training loss 0.06213865056633949 Validation loss 0.06271172314882278 Accuracy 0.8454999923706055\n",
      "Iteration 28210 Training loss 0.06255106627941132 Validation loss 0.05922585353255272 Accuracy 0.8544999957084656\n",
      "Iteration 28220 Training loss 0.0645982027053833 Validation loss 0.06818000972270966 Accuracy 0.8355000019073486\n",
      "Iteration 28230 Training loss 0.06327909231185913 Validation loss 0.05950235575437546 Accuracy 0.8544999957084656\n",
      "Iteration 28240 Training loss 0.0713895857334137 Validation loss 0.06983983516693115 Accuracy 0.828000009059906\n",
      "Iteration 28250 Training loss 0.05996024236083031 Validation loss 0.06129009649157524 Accuracy 0.8479999899864197\n",
      "Iteration 28260 Training loss 0.05817164108157158 Validation loss 0.059392329305410385 Accuracy 0.8539999723434448\n",
      "Iteration 28270 Training loss 0.059528883546590805 Validation loss 0.06700598448514938 Accuracy 0.8370000123977661\n",
      "Iteration 28280 Training loss 0.06550516188144684 Validation loss 0.06890442967414856 Accuracy 0.8314999938011169\n",
      "Iteration 28290 Training loss 0.0693749189376831 Validation loss 0.06833138316869736 Accuracy 0.8335000276565552\n",
      "Iteration 28300 Training loss 0.0633375346660614 Validation loss 0.06482212990522385 Accuracy 0.8414999842643738\n",
      "Iteration 28310 Training loss 0.06810767203569412 Validation loss 0.07221930474042892 Accuracy 0.8230000138282776\n",
      "Iteration 28320 Training loss 0.06457722932100296 Validation loss 0.06321932375431061 Accuracy 0.8450000286102295\n",
      "Iteration 28330 Training loss 0.06795863807201385 Validation loss 0.07296724617481232 Accuracy 0.8230000138282776\n",
      "Iteration 28340 Training loss 0.06539379060268402 Validation loss 0.06733833998441696 Accuracy 0.8360000252723694\n",
      "Iteration 28350 Training loss 0.061127159744501114 Validation loss 0.06253711879253387 Accuracy 0.8450000286102295\n",
      "Iteration 28360 Training loss 0.05846991017460823 Validation loss 0.05896855890750885 Accuracy 0.8550000190734863\n",
      "Iteration 28370 Training loss 0.06333248317241669 Validation loss 0.06593995541334152 Accuracy 0.8385000228881836\n",
      "Iteration 28380 Training loss 0.06226680800318718 Validation loss 0.06417038291692734 Accuracy 0.8399999737739563\n",
      "Iteration 28390 Training loss 0.06530524790287018 Validation loss 0.07356056571006775 Accuracy 0.8220000267028809\n",
      "Iteration 28400 Training loss 0.06434661149978638 Validation loss 0.06597460061311722 Accuracy 0.8395000100135803\n",
      "Iteration 28410 Training loss 0.06749088317155838 Validation loss 0.06851417571306229 Accuracy 0.8360000252723694\n",
      "Iteration 28420 Training loss 0.06650761514902115 Validation loss 0.06789083778858185 Accuracy 0.8370000123977661\n",
      "Iteration 28430 Training loss 0.05777539685368538 Validation loss 0.0615014024078846 Accuracy 0.8495000004768372\n",
      "Iteration 28440 Training loss 0.06512799859046936 Validation loss 0.06932403892278671 Accuracy 0.8309999704360962\n",
      "Iteration 28450 Training loss 0.06230781599879265 Validation loss 0.06385771185159683 Accuracy 0.8454999923706055\n",
      "Iteration 28460 Training loss 0.06137439236044884 Validation loss 0.06740455329418182 Accuracy 0.8355000019073486\n",
      "Iteration 28470 Training loss 0.06370361894369125 Validation loss 0.062325943261384964 Accuracy 0.8464999794960022\n",
      "Iteration 28480 Training loss 0.06214608997106552 Validation loss 0.06651613116264343 Accuracy 0.8389999866485596\n",
      "Iteration 28490 Training loss 0.06312698870897293 Validation loss 0.06511383503675461 Accuracy 0.8414999842643738\n",
      "Iteration 28500 Training loss 0.06352341175079346 Validation loss 0.06431671977043152 Accuracy 0.8424999713897705\n",
      "Iteration 28510 Training loss 0.06237807497382164 Validation loss 0.06867030262947083 Accuracy 0.8345000147819519\n",
      "Iteration 28520 Training loss 0.06511665135622025 Validation loss 0.06786643713712692 Accuracy 0.8374999761581421\n",
      "Iteration 28530 Training loss 0.05853452906012535 Validation loss 0.06045718118548393 Accuracy 0.8519999980926514\n",
      "Iteration 28540 Training loss 0.0592944361269474 Validation loss 0.0632949247956276 Accuracy 0.8450000286102295\n",
      "Iteration 28550 Training loss 0.06824957579374313 Validation loss 0.06867782771587372 Accuracy 0.8330000042915344\n",
      "Iteration 28560 Training loss 0.05538564920425415 Validation loss 0.05889812484383583 Accuracy 0.8560000061988831\n",
      "Iteration 28570 Training loss 0.07018409669399261 Validation loss 0.07213743776082993 Accuracy 0.8259999752044678\n",
      "Iteration 28580 Training loss 0.06393451988697052 Validation loss 0.0665578618645668 Accuracy 0.8399999737739563\n",
      "Iteration 28590 Training loss 0.0642685666680336 Validation loss 0.06370394676923752 Accuracy 0.8414999842643738\n",
      "Iteration 28600 Training loss 0.060184232890605927 Validation loss 0.06501482427120209 Accuracy 0.8420000076293945\n",
      "Iteration 28610 Training loss 0.07059744000434875 Validation loss 0.07174625992774963 Accuracy 0.8240000009536743\n",
      "Iteration 28620 Training loss 0.060190021991729736 Validation loss 0.06138569861650467 Accuracy 0.8504999876022339\n",
      "Iteration 28630 Training loss 0.06632009893655777 Validation loss 0.06893865764141083 Accuracy 0.8305000066757202\n",
      "Iteration 28640 Training loss 0.06333934515714645 Validation loss 0.06353700906038284 Accuracy 0.8460000157356262\n",
      "Iteration 28650 Training loss 0.06423728913068771 Validation loss 0.06647581607103348 Accuracy 0.8374999761581421\n",
      "Iteration 28660 Training loss 0.06318704038858414 Validation loss 0.06493023782968521 Accuracy 0.8429999947547913\n",
      "Iteration 28670 Training loss 0.05932547524571419 Validation loss 0.058944977819919586 Accuracy 0.8535000085830688\n",
      "Iteration 28680 Training loss 0.06683249026536942 Validation loss 0.06692717224359512 Accuracy 0.8370000123977661\n",
      "Iteration 28690 Training loss 0.056796006858348846 Validation loss 0.06194771081209183 Accuracy 0.8479999899864197\n",
      "Iteration 28700 Training loss 0.06920460611581802 Validation loss 0.07074923813343048 Accuracy 0.8259999752044678\n",
      "Iteration 28710 Training loss 0.0657409280538559 Validation loss 0.0717749074101448 Accuracy 0.8234999775886536\n",
      "Iteration 28720 Training loss 0.05809042230248451 Validation loss 0.06021365523338318 Accuracy 0.8525000214576721\n",
      "Iteration 28730 Training loss 0.06311673671007156 Validation loss 0.06385165452957153 Accuracy 0.843500018119812\n",
      "Iteration 28740 Training loss 0.05745203420519829 Validation loss 0.058633241802453995 Accuracy 0.8560000061988831\n",
      "Iteration 28750 Training loss 0.06217449903488159 Validation loss 0.0668163076043129 Accuracy 0.8389999866485596\n",
      "Iteration 28760 Training loss 0.05714581161737442 Validation loss 0.05929102376103401 Accuracy 0.8535000085830688\n",
      "Iteration 28770 Training loss 0.07004643231630325 Validation loss 0.06897881627082825 Accuracy 0.8309999704360962\n",
      "Iteration 28780 Training loss 0.05996148660778999 Validation loss 0.06475821137428284 Accuracy 0.8429999947547913\n",
      "Iteration 28790 Training loss 0.0829789862036705 Validation loss 0.08411671966314316 Accuracy 0.7954999804496765\n",
      "Iteration 28800 Training loss 0.06282723695039749 Validation loss 0.06485681235790253 Accuracy 0.8395000100135803\n",
      "Iteration 28810 Training loss 0.06160471588373184 Validation loss 0.06210291385650635 Accuracy 0.8475000262260437\n",
      "Iteration 28820 Training loss 0.0647943764925003 Validation loss 0.06791996210813522 Accuracy 0.8320000171661377\n",
      "Iteration 28830 Training loss 0.06567244976758957 Validation loss 0.06272175908088684 Accuracy 0.8454999923706055\n",
      "Iteration 28840 Training loss 0.062396466732025146 Validation loss 0.06400047987699509 Accuracy 0.8445000052452087\n",
      "Iteration 28850 Training loss 0.06294876337051392 Validation loss 0.06307723373174667 Accuracy 0.847000002861023\n",
      "Iteration 28860 Training loss 0.06521628797054291 Validation loss 0.06820088624954224 Accuracy 0.8360000252723694\n",
      "Iteration 28870 Training loss 0.06807354092597961 Validation loss 0.06900573521852493 Accuracy 0.8320000171661377\n",
      "Iteration 28880 Training loss 0.06374754756689072 Validation loss 0.06417664140462875 Accuracy 0.8450000286102295\n",
      "Iteration 28890 Training loss 0.0653936117887497 Validation loss 0.06717266887426376 Accuracy 0.8374999761581421\n",
      "Iteration 28900 Training loss 0.06455585360527039 Validation loss 0.060178495943546295 Accuracy 0.8514999747276306\n",
      "Iteration 28910 Training loss 0.05568563938140869 Validation loss 0.0601060688495636 Accuracy 0.8525000214576721\n",
      "Iteration 28920 Training loss 0.0645575225353241 Validation loss 0.0654100552201271 Accuracy 0.8395000100135803\n",
      "Iteration 28930 Training loss 0.06894949078559875 Validation loss 0.06505558639764786 Accuracy 0.8420000076293945\n",
      "Iteration 28940 Training loss 0.07075557857751846 Validation loss 0.06538654863834381 Accuracy 0.8374999761581421\n",
      "Iteration 28950 Training loss 0.06294021755456924 Validation loss 0.0567985475063324 Accuracy 0.8585000038146973\n",
      "Iteration 28960 Training loss 0.05484883114695549 Validation loss 0.05408178269863129 Accuracy 0.8654999732971191\n",
      "Iteration 28970 Training loss 0.06384682655334473 Validation loss 0.06050840765237808 Accuracy 0.8525000214576721\n",
      "Iteration 28980 Training loss 0.06701114028692245 Validation loss 0.058993805199861526 Accuracy 0.8539999723434448\n",
      "Iteration 28990 Training loss 0.06965547800064087 Validation loss 0.060092803090810776 Accuracy 0.8560000061988831\n",
      "Iteration 29000 Training loss 0.059483274817466736 Validation loss 0.05737258121371269 Accuracy 0.8544999957084656\n",
      "Iteration 29010 Training loss 0.05969209223985672 Validation loss 0.05519810691475868 Accuracy 0.8579999804496765\n",
      "Iteration 29020 Training loss 0.06689941138029099 Validation loss 0.0569734163582325 Accuracy 0.8544999957084656\n",
      "Iteration 29030 Training loss 0.06152328476309776 Validation loss 0.05582791194319725 Accuracy 0.8565000295639038\n",
      "Iteration 29040 Training loss 0.0628662183880806 Validation loss 0.055978477001190186 Accuracy 0.8575000166893005\n",
      "Iteration 29050 Training loss 0.05965778976678848 Validation loss 0.05559685081243515 Accuracy 0.8575000166893005\n",
      "Iteration 29060 Training loss 0.06399236619472504 Validation loss 0.055637333542108536 Accuracy 0.8550000190734863\n",
      "Iteration 29070 Training loss 0.05568433925509453 Validation loss 0.05460791662335396 Accuracy 0.8644999861717224\n",
      "Iteration 29080 Training loss 0.05795780196785927 Validation loss 0.05382855236530304 Accuracy 0.8665000200271606\n",
      "Iteration 29090 Training loss 0.06490521132946014 Validation loss 0.05581088736653328 Accuracy 0.8554999828338623\n",
      "Iteration 29100 Training loss 0.06518334150314331 Validation loss 0.056216634809970856 Accuracy 0.8550000190734863\n",
      "Iteration 29110 Training loss 0.06843385100364685 Validation loss 0.05844028294086456 Accuracy 0.8544999957084656\n",
      "Iteration 29120 Training loss 0.0675615519285202 Validation loss 0.060089416801929474 Accuracy 0.8539999723434448\n",
      "Iteration 29130 Training loss 0.06580589711666107 Validation loss 0.05717786028981209 Accuracy 0.8514999747276306\n",
      "Iteration 29140 Training loss 0.062596894800663 Validation loss 0.05718344822525978 Accuracy 0.8519999980926514\n",
      "Iteration 29150 Training loss 0.058117516338825226 Validation loss 0.05495058372616768 Accuracy 0.8579999804496765\n",
      "Iteration 29160 Training loss 0.05732569098472595 Validation loss 0.05507659167051315 Accuracy 0.8579999804496765\n",
      "Iteration 29170 Training loss 0.056157950311899185 Validation loss 0.05666210129857063 Accuracy 0.8550000190734863\n",
      "Iteration 29180 Training loss 0.06396438181400299 Validation loss 0.05906900763511658 Accuracy 0.8504999876022339\n",
      "Iteration 29190 Training loss 0.06609389930963516 Validation loss 0.05901820957660675 Accuracy 0.8539999723434448\n",
      "Iteration 29200 Training loss 0.0646899864077568 Validation loss 0.057764533907175064 Accuracy 0.8525000214576721\n",
      "Iteration 29210 Training loss 0.063060462474823 Validation loss 0.05569642409682274 Accuracy 0.8550000190734863\n",
      "Iteration 29220 Training loss 0.05899171903729439 Validation loss 0.055772118270397186 Accuracy 0.8554999828338623\n",
      "Iteration 29230 Training loss 0.06085079163312912 Validation loss 0.055612653493881226 Accuracy 0.8554999828338623\n",
      "Iteration 29240 Training loss 0.05965197831392288 Validation loss 0.05509667471051216 Accuracy 0.8585000038146973\n",
      "Iteration 29250 Training loss 0.0661507174372673 Validation loss 0.057587187737226486 Accuracy 0.8535000085830688\n",
      "Iteration 29260 Training loss 0.06896862387657166 Validation loss 0.0620805099606514 Accuracy 0.8500000238418579\n",
      "Iteration 29270 Training loss 0.06186527758836746 Validation loss 0.05588902160525322 Accuracy 0.8544999957084656\n",
      "Iteration 29280 Training loss 0.06994733959436417 Validation loss 0.0624111033976078 Accuracy 0.8460000157356262\n",
      "Iteration 29290 Training loss 0.06221874803304672 Validation loss 0.05878102779388428 Accuracy 0.8525000214576721\n",
      "Iteration 29300 Training loss 0.06265570968389511 Validation loss 0.05689273029565811 Accuracy 0.8535000085830688\n",
      "Iteration 29310 Training loss 0.06613583117723465 Validation loss 0.06053043529391289 Accuracy 0.8500000238418579\n",
      "Iteration 29320 Training loss 0.05979640781879425 Validation loss 0.05731302127242088 Accuracy 0.8565000295639038\n",
      "Iteration 29330 Training loss 0.07020390033721924 Validation loss 0.06226421520113945 Accuracy 0.8489999771118164\n",
      "Iteration 29340 Training loss 0.06347890943288803 Validation loss 0.05586674436926842 Accuracy 0.8539999723434448\n",
      "Iteration 29350 Training loss 0.06695027649402618 Validation loss 0.05781768262386322 Accuracy 0.8539999723434448\n",
      "Iteration 29360 Training loss 0.05577784776687622 Validation loss 0.054052673280239105 Accuracy 0.8654999732971191\n",
      "Iteration 29370 Training loss 0.07046392560005188 Validation loss 0.07008254528045654 Accuracy 0.8295000195503235\n",
      "Iteration 29380 Training loss 0.06708662211894989 Validation loss 0.06572774797677994 Accuracy 0.8389999866485596\n",
      "Iteration 29390 Training loss 0.062396738678216934 Validation loss 0.06600622087717056 Accuracy 0.8385000228881836\n",
      "Iteration 29400 Training loss 0.051807720214128494 Validation loss 0.0582268126308918 Accuracy 0.8565000295639038\n",
      "Iteration 29410 Training loss 0.06831299513578415 Validation loss 0.06993217021226883 Accuracy 0.828499972820282\n",
      "Iteration 29420 Training loss 0.05778007581830025 Validation loss 0.05639480799436569 Accuracy 0.8600000143051147\n",
      "Iteration 29430 Training loss 0.06820788234472275 Validation loss 0.059337835758924484 Accuracy 0.8514999747276306\n",
      "Iteration 29440 Training loss 0.06643453240394592 Validation loss 0.05697128176689148 Accuracy 0.8550000190734863\n",
      "Iteration 29450 Training loss 0.07749020308256149 Validation loss 0.06395597755908966 Accuracy 0.843500018119812\n",
      "Iteration 29460 Training loss 0.06058964505791664 Validation loss 0.05508118495345116 Accuracy 0.8585000038146973\n",
      "Iteration 29470 Training loss 0.06666215509176254 Validation loss 0.05976484715938568 Accuracy 0.8514999747276306\n",
      "Iteration 29480 Training loss 0.07109721004962921 Validation loss 0.06131865829229355 Accuracy 0.8510000109672546\n",
      "Iteration 29490 Training loss 0.06553643941879272 Validation loss 0.05697174742817879 Accuracy 0.8544999957084656\n",
      "Iteration 29500 Training loss 0.06377482414245605 Validation loss 0.05681319534778595 Accuracy 0.8550000190734863\n",
      "Iteration 29510 Training loss 0.06095854565501213 Validation loss 0.055049121379852295 Accuracy 0.8579999804496765\n",
      "Iteration 29520 Training loss 0.06830868124961853 Validation loss 0.05685853958129883 Accuracy 0.8585000038146973\n",
      "Iteration 29530 Training loss 0.06087362766265869 Validation loss 0.05507638677954674 Accuracy 0.8560000061988831\n",
      "Iteration 29540 Training loss 0.056138090789318085 Validation loss 0.05412473902106285 Accuracy 0.8585000038146973\n",
      "Iteration 29550 Training loss 0.06516485661268234 Validation loss 0.05980450659990311 Accuracy 0.8525000214576721\n",
      "Iteration 29560 Training loss 0.05890612304210663 Validation loss 0.05437160283327103 Accuracy 0.859499990940094\n",
      "Iteration 29570 Training loss 0.06450051814317703 Validation loss 0.05910605564713478 Accuracy 0.8544999957084656\n",
      "Iteration 29580 Training loss 0.06339108943939209 Validation loss 0.05765260010957718 Accuracy 0.8560000061988831\n",
      "Iteration 29590 Training loss 0.061063140630722046 Validation loss 0.05650678649544716 Accuracy 0.8539999723434448\n",
      "Iteration 29600 Training loss 0.06835401058197021 Validation loss 0.06091127172112465 Accuracy 0.8514999747276306\n",
      "Iteration 29610 Training loss 0.06044542044401169 Validation loss 0.05695115774869919 Accuracy 0.8539999723434448\n",
      "Iteration 29620 Training loss 0.062201127409935 Validation loss 0.05743364617228508 Accuracy 0.8535000085830688\n",
      "Iteration 29630 Training loss 0.06624620407819748 Validation loss 0.05861562490463257 Accuracy 0.8519999980926514\n",
      "Iteration 29640 Training loss 0.06406692415475845 Validation loss 0.05921570956707001 Accuracy 0.8554999828338623\n",
      "Iteration 29650 Training loss 0.0624542310833931 Validation loss 0.055327411741018295 Accuracy 0.8560000061988831\n",
      "Iteration 29660 Training loss 0.06644075363874435 Validation loss 0.06123388186097145 Accuracy 0.8500000238418579\n",
      "Iteration 29670 Training loss 0.06301463395357132 Validation loss 0.05808544158935547 Accuracy 0.8560000061988831\n",
      "Iteration 29680 Training loss 0.06525862216949463 Validation loss 0.05835399031639099 Accuracy 0.8544999957084656\n",
      "Iteration 29690 Training loss 0.0634092465043068 Validation loss 0.056286878883838654 Accuracy 0.8550000190734863\n",
      "Iteration 29700 Training loss 0.06978888064622879 Validation loss 0.06085013970732689 Accuracy 0.8525000214576721\n",
      "Iteration 29710 Training loss 0.06362416595220566 Validation loss 0.05796748399734497 Accuracy 0.8539999723434448\n",
      "Iteration 29720 Training loss 0.0622263178229332 Validation loss 0.05670734867453575 Accuracy 0.8510000109672546\n",
      "Iteration 29730 Training loss 0.06065255403518677 Validation loss 0.057056911289691925 Accuracy 0.8535000085830688\n",
      "Iteration 29740 Training loss 0.06325597316026688 Validation loss 0.05651729553937912 Accuracy 0.8569999933242798\n",
      "Iteration 29750 Training loss 0.07278147339820862 Validation loss 0.059060465544462204 Accuracy 0.8529999852180481\n",
      "Iteration 29760 Training loss 0.058501239866018295 Validation loss 0.05475270003080368 Accuracy 0.8604999780654907\n",
      "Iteration 29770 Training loss 0.06409958750009537 Validation loss 0.05709318444132805 Accuracy 0.8535000085830688\n",
      "Iteration 29780 Training loss 0.07025614380836487 Validation loss 0.0621812529861927 Accuracy 0.847000002861023\n",
      "Iteration 29790 Training loss 0.06242188438773155 Validation loss 0.054353151470422745 Accuracy 0.8600000143051147\n",
      "Iteration 29800 Training loss 0.05920839682221413 Validation loss 0.05512760952115059 Accuracy 0.8579999804496765\n",
      "Iteration 29810 Training loss 0.06395391374826431 Validation loss 0.05491763353347778 Accuracy 0.8579999804496765\n",
      "Iteration 29820 Training loss 0.062571220099926 Validation loss 0.05654677376151085 Accuracy 0.8539999723434448\n",
      "Iteration 29830 Training loss 0.0647323802113533 Validation loss 0.05632016062736511 Accuracy 0.8569999933242798\n",
      "Iteration 29840 Training loss 0.06413551419973373 Validation loss 0.05588433891534805 Accuracy 0.8554999828338623\n",
      "Iteration 29850 Training loss 0.06558552384376526 Validation loss 0.05599990114569664 Accuracy 0.8544999957084656\n",
      "Iteration 29860 Training loss 0.056748852133750916 Validation loss 0.055080596357584 Accuracy 0.8575000166893005\n",
      "Iteration 29870 Training loss 0.06792891025543213 Validation loss 0.06135883927345276 Accuracy 0.8475000262260437\n",
      "Iteration 29880 Training loss 0.06326708197593689 Validation loss 0.05585898458957672 Accuracy 0.8579999804496765\n",
      "Iteration 29890 Training loss 0.06505157798528671 Validation loss 0.057500675320625305 Accuracy 0.8525000214576721\n",
      "Iteration 29900 Training loss 0.060436081141233444 Validation loss 0.05668637901544571 Accuracy 0.8535000085830688\n",
      "Iteration 29910 Training loss 0.06513755023479462 Validation loss 0.05530407652258873 Accuracy 0.8554999828338623\n",
      "Iteration 29920 Training loss 0.06399193406105042 Validation loss 0.055988412350416183 Accuracy 0.8560000061988831\n",
      "Iteration 29930 Training loss 0.0630546435713768 Validation loss 0.05627825856208801 Accuracy 0.8535000085830688\n",
      "Iteration 29940 Training loss 0.058269891887903214 Validation loss 0.0547315739095211 Accuracy 0.8579999804496765\n",
      "Iteration 29950 Training loss 0.06309778988361359 Validation loss 0.055280257016420364 Accuracy 0.8579999804496765\n",
      "Iteration 29960 Training loss 0.058843594044446945 Validation loss 0.054357822984457016 Accuracy 0.8585000038146973\n",
      "Iteration 29970 Training loss 0.0660630539059639 Validation loss 0.056351594626903534 Accuracy 0.8554999828338623\n",
      "Iteration 29980 Training loss 0.06993148475885391 Validation loss 0.05987653508782387 Accuracy 0.8539999723434448\n",
      "Iteration 29990 Training loss 0.06310909986495972 Validation loss 0.05771562084555626 Accuracy 0.8529999852180481\n",
      "Iteration 30000 Training loss 0.06454011052846909 Validation loss 0.05657448619604111 Accuracy 0.8529999852180481\n",
      "Iteration 30010 Training loss 0.06267773360013962 Validation loss 0.05550496652722359 Accuracy 0.8579999804496765\n",
      "Iteration 30020 Training loss 0.06890783458948135 Validation loss 0.06061932444572449 Accuracy 0.8535000085830688\n",
      "Iteration 30030 Training loss 0.06743008643388748 Validation loss 0.05771667882800102 Accuracy 0.8514999747276306\n",
      "Iteration 30040 Training loss 0.06064723804593086 Validation loss 0.05595431104302406 Accuracy 0.8544999957084656\n",
      "Iteration 30050 Training loss 0.05624118074774742 Validation loss 0.053421296179294586 Accuracy 0.8650000095367432\n",
      "Iteration 30060 Training loss 0.060137275606393814 Validation loss 0.056147538125514984 Accuracy 0.8550000190734863\n",
      "Iteration 30070 Training loss 0.059060364961624146 Validation loss 0.05439586564898491 Accuracy 0.8579999804496765\n",
      "Iteration 30080 Training loss 0.0661933422088623 Validation loss 0.0630524531006813 Accuracy 0.8450000286102295\n",
      "Iteration 30090 Training loss 0.05662689357995987 Validation loss 0.05536261573433876 Accuracy 0.8619999885559082\n",
      "Iteration 30100 Training loss 0.06553561240434647 Validation loss 0.058076806366443634 Accuracy 0.8544999957084656\n",
      "Iteration 30110 Training loss 0.07242029160261154 Validation loss 0.0615384615957737 Accuracy 0.8500000238418579\n",
      "Iteration 30120 Training loss 0.0651257261633873 Validation loss 0.05578193441033363 Accuracy 0.8554999828338623\n",
      "Iteration 30130 Training loss 0.06417106091976166 Validation loss 0.056384649127721786 Accuracy 0.8560000061988831\n",
      "Iteration 30140 Training loss 0.055744197219610214 Validation loss 0.055245429277420044 Accuracy 0.8579999804496765\n",
      "Iteration 30150 Training loss 0.06413732469081879 Validation loss 0.05822082981467247 Accuracy 0.8525000214576721\n",
      "Iteration 30160 Training loss 0.07410696148872375 Validation loss 0.06084880977869034 Accuracy 0.8510000109672546\n",
      "Iteration 30170 Training loss 0.06701435893774033 Validation loss 0.05696503445506096 Accuracy 0.8525000214576721\n",
      "Iteration 30180 Training loss 0.057328496128320694 Validation loss 0.05682225897908211 Accuracy 0.8519999980926514\n",
      "Iteration 30190 Training loss 0.05880094692111015 Validation loss 0.055155009031295776 Accuracy 0.8600000143051147\n",
      "Iteration 30200 Training loss 0.06310790777206421 Validation loss 0.05704176053404808 Accuracy 0.8550000190734863\n",
      "Iteration 30210 Training loss 0.07044152915477753 Validation loss 0.06228015199303627 Accuracy 0.8460000157356262\n",
      "Iteration 30220 Training loss 0.06705804169178009 Validation loss 0.058126289397478104 Accuracy 0.8544999957084656\n",
      "Iteration 30230 Training loss 0.06091912090778351 Validation loss 0.05516185984015465 Accuracy 0.8585000038146973\n",
      "Iteration 30240 Training loss 0.07020843774080276 Validation loss 0.06120366230607033 Accuracy 0.8500000238418579\n",
      "Iteration 30250 Training loss 0.06394034624099731 Validation loss 0.05705779418349266 Accuracy 0.8550000190734863\n",
      "Iteration 30260 Training loss 0.06979396939277649 Validation loss 0.060879189521074295 Accuracy 0.8495000004768372\n",
      "Iteration 30270 Training loss 0.05971864238381386 Validation loss 0.05666584149003029 Accuracy 0.8554999828338623\n",
      "Iteration 30280 Training loss 0.06375524401664734 Validation loss 0.05778069049119949 Accuracy 0.8569999933242798\n",
      "Iteration 30290 Training loss 0.07197033613920212 Validation loss 0.06123500317335129 Accuracy 0.8514999747276306\n",
      "Iteration 30300 Training loss 0.056616805493831635 Validation loss 0.054771788418293 Accuracy 0.8615000247955322\n",
      "Iteration 30310 Training loss 0.06945810467004776 Validation loss 0.061218392103910446 Accuracy 0.8510000109672546\n",
      "Iteration 30320 Training loss 0.06841163337230682 Validation loss 0.06254640221595764 Accuracy 0.8479999899864197\n",
      "Iteration 30330 Training loss 0.06664498895406723 Validation loss 0.06183344125747681 Accuracy 0.8489999771118164\n",
      "Iteration 30340 Training loss 0.06503069400787354 Validation loss 0.055235132575035095 Accuracy 0.8585000038146973\n",
      "Iteration 30350 Training loss 0.05806379020214081 Validation loss 0.0551542267203331 Accuracy 0.8579999804496765\n",
      "Iteration 30360 Training loss 0.057522762566804886 Validation loss 0.05521348863840103 Accuracy 0.8600000143051147\n",
      "Iteration 30370 Training loss 0.05714965984225273 Validation loss 0.05668272823095322 Accuracy 0.8560000061988831\n",
      "Iteration 30380 Training loss 0.06072767823934555 Validation loss 0.05651137977838516 Accuracy 0.8569999933242798\n",
      "Iteration 30390 Training loss 0.06317904591560364 Validation loss 0.05641980841755867 Accuracy 0.8560000061988831\n",
      "Iteration 30400 Training loss 0.06445936858654022 Validation loss 0.06010613963007927 Accuracy 0.8525000214576721\n",
      "Iteration 30410 Training loss 0.0662965252995491 Validation loss 0.061661794781684875 Accuracy 0.8460000157356262\n",
      "Iteration 30420 Training loss 0.06831046938896179 Validation loss 0.05940321460366249 Accuracy 0.8539999723434448\n",
      "Iteration 30430 Training loss 0.07206462323665619 Validation loss 0.06301454454660416 Accuracy 0.8460000157356262\n",
      "Iteration 30440 Training loss 0.07993911951780319 Validation loss 0.06494516879320145 Accuracy 0.8414999842643738\n",
      "Iteration 30450 Training loss 0.057668305933475494 Validation loss 0.05514343082904816 Accuracy 0.8569999933242798\n",
      "Iteration 30460 Training loss 0.0658106654882431 Validation loss 0.06074438616633415 Accuracy 0.8525000214576721\n",
      "Iteration 30470 Training loss 0.06887100636959076 Validation loss 0.05846402049064636 Accuracy 0.8535000085830688\n",
      "Iteration 30480 Training loss 0.06490467488765717 Validation loss 0.0544806532561779 Accuracy 0.8575000166893005\n",
      "Iteration 30490 Training loss 0.06908980756998062 Validation loss 0.05840442702174187 Accuracy 0.8550000190734863\n",
      "Iteration 30500 Training loss 0.06734362989664078 Validation loss 0.059625301510095596 Accuracy 0.8529999852180481\n",
      "Iteration 30510 Training loss 0.05641551315784454 Validation loss 0.053863175213336945 Accuracy 0.8640000224113464\n",
      "Iteration 30520 Training loss 0.05874250829219818 Validation loss 0.0547717809677124 Accuracy 0.859000027179718\n",
      "Iteration 30530 Training loss 0.07285866141319275 Validation loss 0.06521685421466827 Accuracy 0.8374999761581421\n",
      "Iteration 30540 Training loss 0.06557710468769073 Validation loss 0.06021986901760101 Accuracy 0.8550000190734863\n",
      "Iteration 30550 Training loss 0.07424058020114899 Validation loss 0.0625670775771141 Accuracy 0.8485000133514404\n",
      "Iteration 30560 Training loss 0.06130356341600418 Validation loss 0.05603363364934921 Accuracy 0.8560000061988831\n",
      "Iteration 30570 Training loss 0.07017074525356293 Validation loss 0.06382114440202713 Accuracy 0.843999981880188\n",
      "Iteration 30580 Training loss 0.05773317813873291 Validation loss 0.056861989200115204 Accuracy 0.8569999933242798\n",
      "Iteration 30590 Training loss 0.0724526047706604 Validation loss 0.0605035275220871 Accuracy 0.8535000085830688\n",
      "Iteration 30600 Training loss 0.060261018574237823 Validation loss 0.05480149760842323 Accuracy 0.859000027179718\n",
      "Iteration 30610 Training loss 0.0756610855460167 Validation loss 0.06561063975095749 Accuracy 0.840499997138977\n",
      "Iteration 30620 Training loss 0.057927459478378296 Validation loss 0.05533583462238312 Accuracy 0.8575000166893005\n",
      "Iteration 30630 Training loss 0.06681635975837708 Validation loss 0.059750281274318695 Accuracy 0.8485000133514404\n",
      "Iteration 30640 Training loss 0.06209579482674599 Validation loss 0.054922353476285934 Accuracy 0.8579999804496765\n",
      "Iteration 30650 Training loss 0.0658147931098938 Validation loss 0.0598609633743763 Accuracy 0.8510000109672546\n",
      "Iteration 30660 Training loss 0.07154930382966995 Validation loss 0.06139347702264786 Accuracy 0.8500000238418579\n",
      "Iteration 30670 Training loss 0.06517354398965836 Validation loss 0.05960511043667793 Accuracy 0.8495000004768372\n",
      "Iteration 30680 Training loss 0.06753837317228317 Validation loss 0.05978619307279587 Accuracy 0.8525000214576721\n",
      "Iteration 30690 Training loss 0.06174195930361748 Validation loss 0.057978056371212006 Accuracy 0.8535000085830688\n",
      "Iteration 30700 Training loss 0.0635245144367218 Validation loss 0.05634906888008118 Accuracy 0.8539999723434448\n",
      "Iteration 30710 Training loss 0.07015353441238403 Validation loss 0.061010267585515976 Accuracy 0.8500000238418579\n",
      "Iteration 30720 Training loss 0.05863502621650696 Validation loss 0.05468103289604187 Accuracy 0.8600000143051147\n",
      "Iteration 30730 Training loss 0.07555647939443588 Validation loss 0.06409179419279099 Accuracy 0.843999981880188\n",
      "Iteration 30740 Training loss 0.07050849497318268 Validation loss 0.06079502031207085 Accuracy 0.8504999876022339\n",
      "Iteration 30750 Training loss 0.05920131504535675 Validation loss 0.05695123225450516 Accuracy 0.8544999957084656\n",
      "Iteration 30760 Training loss 0.06246655434370041 Validation loss 0.05865906551480293 Accuracy 0.8529999852180481\n",
      "Iteration 30770 Training loss 0.06237289682030678 Validation loss 0.05786239355802536 Accuracy 0.8519999980926514\n",
      "Iteration 30780 Training loss 0.06546668708324432 Validation loss 0.058208364993333817 Accuracy 0.8514999747276306\n",
      "Iteration 30790 Training loss 0.07378976047039032 Validation loss 0.06459908187389374 Accuracy 0.843500018119812\n",
      "Iteration 30800 Training loss 0.060755655169487 Validation loss 0.05588914453983307 Accuracy 0.8569999933242798\n",
      "Iteration 30810 Training loss 0.060733094811439514 Validation loss 0.056649476289749146 Accuracy 0.8569999933242798\n",
      "Iteration 30820 Training loss 0.0630936473608017 Validation loss 0.05717841163277626 Accuracy 0.8544999957084656\n",
      "Iteration 30830 Training loss 0.06777489930391312 Validation loss 0.058123376220464706 Accuracy 0.8544999957084656\n",
      "Iteration 30840 Training loss 0.059631019830703735 Validation loss 0.056860871613025665 Accuracy 0.8575000166893005\n",
      "Iteration 30850 Training loss 0.07087060809135437 Validation loss 0.06247642636299133 Accuracy 0.847000002861023\n",
      "Iteration 30860 Training loss 0.06387601047754288 Validation loss 0.057512953877449036 Accuracy 0.8544999957084656\n",
      "Iteration 30870 Training loss 0.06058206036686897 Validation loss 0.05765943601727486 Accuracy 0.8535000085830688\n",
      "Iteration 30880 Training loss 0.07733706384897232 Validation loss 0.07019516080617905 Accuracy 0.8289999961853027\n",
      "Iteration 30890 Training loss 0.06463254243135452 Validation loss 0.06047678738832474 Accuracy 0.8529999852180481\n",
      "Iteration 30900 Training loss 0.06387956440448761 Validation loss 0.05755561962723732 Accuracy 0.8544999957084656\n",
      "Iteration 30910 Training loss 0.06081068515777588 Validation loss 0.056376922875642776 Accuracy 0.8569999933242798\n",
      "Iteration 30920 Training loss 0.06884227693080902 Validation loss 0.06418324261903763 Accuracy 0.8424999713897705\n",
      "Iteration 30930 Training loss 0.06624292582273483 Validation loss 0.06169812008738518 Accuracy 0.8510000109672546\n",
      "Iteration 30940 Training loss 0.0673278346657753 Validation loss 0.05940597504377365 Accuracy 0.8554999828338623\n",
      "Iteration 30950 Training loss 0.06430815160274506 Validation loss 0.05916664004325867 Accuracy 0.8525000214576721\n",
      "Iteration 30960 Training loss 0.07238597422838211 Validation loss 0.061472199857234955 Accuracy 0.8514999747276306\n",
      "Iteration 30970 Training loss 0.06619836390018463 Validation loss 0.06179087981581688 Accuracy 0.8489999771118164\n",
      "Iteration 30980 Training loss 0.0662570521235466 Validation loss 0.05999407172203064 Accuracy 0.8519999980926514\n",
      "Iteration 30990 Training loss 0.0654095858335495 Validation loss 0.06314423680305481 Accuracy 0.8475000262260437\n",
      "Iteration 31000 Training loss 0.06176679581403732 Validation loss 0.05628431215882301 Accuracy 0.8579999804496765\n",
      "Iteration 31010 Training loss 0.06965876370668411 Validation loss 0.06276647746562958 Accuracy 0.8445000052452087\n",
      "Iteration 31020 Training loss 0.06314904987812042 Validation loss 0.05926145240664482 Accuracy 0.8544999957084656\n",
      "Iteration 31030 Training loss 0.06073825806379318 Validation loss 0.05528365448117256 Accuracy 0.8610000014305115\n",
      "Iteration 31040 Training loss 0.06576365232467651 Validation loss 0.06272397190332413 Accuracy 0.8460000157356262\n",
      "Iteration 31050 Training loss 0.06796476989984512 Validation loss 0.0632915198802948 Accuracy 0.8454999923706055\n",
      "Iteration 31060 Training loss 0.06931862235069275 Validation loss 0.063605897128582 Accuracy 0.8445000052452087\n",
      "Iteration 31070 Training loss 0.06076188012957573 Validation loss 0.05498284101486206 Accuracy 0.8619999885559082\n",
      "Iteration 31080 Training loss 0.07327830791473389 Validation loss 0.06610357016324997 Accuracy 0.8379999995231628\n",
      "Iteration 31090 Training loss 0.06857621669769287 Validation loss 0.060593292117118835 Accuracy 0.8544999957084656\n",
      "Iteration 31100 Training loss 0.06400411576032639 Validation loss 0.057237572968006134 Accuracy 0.8535000085830688\n",
      "Iteration 31110 Training loss 0.06033250316977501 Validation loss 0.05807511880993843 Accuracy 0.8544999957084656\n",
      "Iteration 31120 Training loss 0.062224794179201126 Validation loss 0.05838756635785103 Accuracy 0.8544999957084656\n",
      "Iteration 31130 Training loss 0.06484278291463852 Validation loss 0.05910133197903633 Accuracy 0.8529999852180481\n",
      "Iteration 31140 Training loss 0.06482063233852386 Validation loss 0.059372857213020325 Accuracy 0.8504999876022339\n",
      "Iteration 31150 Training loss 0.06975936144590378 Validation loss 0.0569615513086319 Accuracy 0.8560000061988831\n",
      "Iteration 31160 Training loss 0.06319792568683624 Validation loss 0.057969845831394196 Accuracy 0.8539999723434448\n",
      "Iteration 31170 Training loss 0.06800498813390732 Validation loss 0.059601470828056335 Accuracy 0.8519999980926514\n",
      "Iteration 31180 Training loss 0.06621687859296799 Validation loss 0.05841992050409317 Accuracy 0.8514999747276306\n",
      "Iteration 31190 Training loss 0.06576034426689148 Validation loss 0.05875390022993088 Accuracy 0.8560000061988831\n",
      "Iteration 31200 Training loss 0.05858374387025833 Validation loss 0.05808791518211365 Accuracy 0.8519999980926514\n",
      "Iteration 31210 Training loss 0.06294764578342438 Validation loss 0.056560222059488297 Accuracy 0.8569999933242798\n",
      "Iteration 31220 Training loss 0.0632161796092987 Validation loss 0.05675347521901131 Accuracy 0.8565000295639038\n",
      "Iteration 31230 Training loss 0.05721093341708183 Validation loss 0.05509607866406441 Accuracy 0.8615000247955322\n",
      "Iteration 31240 Training loss 0.07660545408725739 Validation loss 0.06741659343242645 Accuracy 0.8370000123977661\n",
      "Iteration 31250 Training loss 0.06094437092542648 Validation loss 0.057613883167505264 Accuracy 0.8519999980926514\n",
      "Iteration 31260 Training loss 0.06747648119926453 Validation loss 0.059002432972192764 Accuracy 0.8539999723434448\n",
      "Iteration 31270 Training loss 0.06390231847763062 Validation loss 0.05761057138442993 Accuracy 0.8525000214576721\n",
      "Iteration 31280 Training loss 0.07737697660923004 Validation loss 0.06565944850444794 Accuracy 0.840499997138977\n",
      "Iteration 31290 Training loss 0.06148587539792061 Validation loss 0.05659862607717514 Accuracy 0.8550000190734863\n",
      "Iteration 31300 Training loss 0.06855593621730804 Validation loss 0.06083608791232109 Accuracy 0.8489999771118164\n",
      "Iteration 31310 Training loss 0.0679328441619873 Validation loss 0.059804268181324005 Accuracy 0.8529999852180481\n",
      "Iteration 31320 Training loss 0.0642404556274414 Validation loss 0.05790184810757637 Accuracy 0.8525000214576721\n",
      "Iteration 31330 Training loss 0.06878495961427689 Validation loss 0.06144939735531807 Accuracy 0.8500000238418579\n",
      "Iteration 31340 Training loss 0.07238417118787766 Validation loss 0.06502541899681091 Accuracy 0.840499997138977\n",
      "Iteration 31350 Training loss 0.06328874826431274 Validation loss 0.058017000555992126 Accuracy 0.8519999980926514\n",
      "Iteration 31360 Training loss 0.06079430878162384 Validation loss 0.057976122945547104 Accuracy 0.8525000214576721\n",
      "Iteration 31370 Training loss 0.06685596704483032 Validation loss 0.058753035962581635 Accuracy 0.8504999876022339\n",
      "Iteration 31380 Training loss 0.06047028675675392 Validation loss 0.05505415052175522 Accuracy 0.8579999804496765\n",
      "Iteration 31390 Training loss 0.07064129412174225 Validation loss 0.05952627584338188 Accuracy 0.8544999957084656\n",
      "Iteration 31400 Training loss 0.0660223513841629 Validation loss 0.05980429798364639 Accuracy 0.8565000295639038\n",
      "Iteration 31410 Training loss 0.06282654404640198 Validation loss 0.058882806450128555 Accuracy 0.8529999852180481\n",
      "Iteration 31420 Training loss 0.059653833508491516 Validation loss 0.05485910549759865 Accuracy 0.8615000247955322\n",
      "Iteration 31430 Training loss 0.07587078958749771 Validation loss 0.06414151191711426 Accuracy 0.843500018119812\n",
      "Iteration 31440 Training loss 0.06995613127946854 Validation loss 0.06500603258609772 Accuracy 0.8420000076293945\n",
      "Iteration 31450 Training loss 0.06574320048093796 Validation loss 0.06110166758298874 Accuracy 0.8529999852180481\n",
      "Iteration 31460 Training loss 0.06266281753778458 Validation loss 0.058338362723588943 Accuracy 0.8525000214576721\n",
      "Iteration 31470 Training loss 0.08138244599103928 Validation loss 0.07079162448644638 Accuracy 0.8270000219345093\n",
      "Iteration 31480 Training loss 0.05843169242143631 Validation loss 0.05909701809287071 Accuracy 0.8510000109672546\n",
      "Iteration 31490 Training loss 0.0665690079331398 Validation loss 0.061027973890304565 Accuracy 0.8485000133514404\n",
      "Iteration 31500 Training loss 0.06385354697704315 Validation loss 0.05961828678846359 Accuracy 0.8535000085830688\n",
      "Iteration 31510 Training loss 0.06043311208486557 Validation loss 0.06026557460427284 Accuracy 0.8525000214576721\n",
      "Iteration 31520 Training loss 0.06506536900997162 Validation loss 0.059023547917604446 Accuracy 0.8544999957084656\n",
      "Iteration 31530 Training loss 0.06482844054698944 Validation loss 0.05818815529346466 Accuracy 0.8539999723434448\n",
      "Iteration 31540 Training loss 0.06190890446305275 Validation loss 0.05831983685493469 Accuracy 0.8529999852180481\n",
      "Iteration 31550 Training loss 0.06909007579088211 Validation loss 0.06239631026983261 Accuracy 0.8475000262260437\n",
      "Iteration 31560 Training loss 0.06567614525556564 Validation loss 0.05882544815540314 Accuracy 0.8519999980926514\n",
      "Iteration 31570 Training loss 0.06355725228786469 Validation loss 0.05713612586259842 Accuracy 0.8554999828338623\n",
      "Iteration 31580 Training loss 0.06355898082256317 Validation loss 0.05778368562459946 Accuracy 0.8535000085830688\n",
      "Iteration 31590 Training loss 0.06999527662992477 Validation loss 0.06126530468463898 Accuracy 0.8519999980926514\n",
      "Iteration 31600 Training loss 0.06787274032831192 Validation loss 0.06288611143827438 Accuracy 0.8475000262260437\n",
      "Iteration 31610 Training loss 0.07633040100336075 Validation loss 0.06522941589355469 Accuracy 0.8420000076293945\n",
      "Iteration 31620 Training loss 0.0724591538310051 Validation loss 0.062298811972141266 Accuracy 0.8504999876022339\n",
      "Iteration 31630 Training loss 0.06492988765239716 Validation loss 0.05884931981563568 Accuracy 0.8525000214576721\n",
      "Iteration 31640 Training loss 0.0686243325471878 Validation loss 0.05950186029076576 Accuracy 0.8510000109672546\n",
      "Iteration 31650 Training loss 0.06520422548055649 Validation loss 0.05762527510523796 Accuracy 0.8525000214576721\n",
      "Iteration 31660 Training loss 0.06529077887535095 Validation loss 0.05611705034971237 Accuracy 0.8634999990463257\n",
      "Iteration 31670 Training loss 0.06773046404123306 Validation loss 0.06462611258029938 Accuracy 0.8420000076293945\n",
      "Iteration 31680 Training loss 0.07479190081357956 Validation loss 0.06892088800668716 Accuracy 0.8355000019073486\n",
      "Iteration 31690 Training loss 0.06907820701599121 Validation loss 0.06385941058397293 Accuracy 0.8420000076293945\n",
      "Iteration 31700 Training loss 0.06606080383062363 Validation loss 0.058696553111076355 Accuracy 0.8544999957084656\n",
      "Iteration 31710 Training loss 0.07213234156370163 Validation loss 0.06538880616426468 Accuracy 0.8385000228881836\n",
      "Iteration 31720 Training loss 0.06751410663127899 Validation loss 0.061802249401807785 Accuracy 0.8475000262260437\n",
      "Iteration 31730 Training loss 0.060492172837257385 Validation loss 0.059074752032756805 Accuracy 0.8554999828338623\n",
      "Iteration 31740 Training loss 0.06427542120218277 Validation loss 0.06000802665948868 Accuracy 0.8525000214576721\n",
      "Iteration 31750 Training loss 0.06359389424324036 Validation loss 0.0619063638150692 Accuracy 0.8500000238418579\n",
      "Iteration 31760 Training loss 0.06694233417510986 Validation loss 0.06325643509626389 Accuracy 0.8479999899864197\n",
      "Iteration 31770 Training loss 0.059630654752254486 Validation loss 0.05842491239309311 Accuracy 0.8550000190734863\n",
      "Iteration 31780 Training loss 0.06792458891868591 Validation loss 0.06586331129074097 Accuracy 0.8364999890327454\n",
      "Iteration 31790 Training loss 0.061611954122781754 Validation loss 0.05574873089790344 Accuracy 0.859499990940094\n",
      "Iteration 31800 Training loss 0.07734005898237228 Validation loss 0.07225137948989868 Accuracy 0.8295000195503235\n",
      "Iteration 31810 Training loss 0.06212323158979416 Validation loss 0.0595705471932888 Accuracy 0.8519999980926514\n",
      "Iteration 31820 Training loss 0.06947502493858337 Validation loss 0.0609012208878994 Accuracy 0.8500000238418579\n",
      "Iteration 31830 Training loss 0.06897903233766556 Validation loss 0.062472764402627945 Accuracy 0.8485000133514404\n",
      "Iteration 31840 Training loss 0.06649336218833923 Validation loss 0.06429421901702881 Accuracy 0.8420000076293945\n",
      "Iteration 31850 Training loss 0.06365228444337845 Validation loss 0.06097755581140518 Accuracy 0.8495000004768372\n",
      "Iteration 31860 Training loss 0.06553168594837189 Validation loss 0.0594867579638958 Accuracy 0.8525000214576721\n",
      "Iteration 31870 Training loss 0.06543580442667007 Validation loss 0.06322797387838364 Accuracy 0.847000002861023\n",
      "Iteration 31880 Training loss 0.06930188089609146 Validation loss 0.059719569981098175 Accuracy 0.8535000085830688\n",
      "Iteration 31890 Training loss 0.06495141237974167 Validation loss 0.057065147906541824 Accuracy 0.8600000143051147\n",
      "Iteration 31900 Training loss 0.06776949763298035 Validation loss 0.06035464629530907 Accuracy 0.8500000238418579\n",
      "Iteration 31910 Training loss 0.07287543267011642 Validation loss 0.062269557267427444 Accuracy 0.8485000133514404\n",
      "Iteration 31920 Training loss 0.05907756835222244 Validation loss 0.05946267023682594 Accuracy 0.8539999723434448\n",
      "Iteration 31930 Training loss 0.06725820153951645 Validation loss 0.06240531802177429 Accuracy 0.8495000004768372\n",
      "Iteration 31940 Training loss 0.06709440797567368 Validation loss 0.06079579517245293 Accuracy 0.8529999852180481\n",
      "Iteration 31950 Training loss 0.0708492174744606 Validation loss 0.06459882855415344 Accuracy 0.843500018119812\n",
      "Iteration 31960 Training loss 0.06107104942202568 Validation loss 0.05880667269229889 Accuracy 0.8535000085830688\n",
      "Iteration 31970 Training loss 0.07179629057645798 Validation loss 0.06651917845010757 Accuracy 0.8379999995231628\n",
      "Iteration 31980 Training loss 0.06905051320791245 Validation loss 0.06447215378284454 Accuracy 0.8420000076293945\n",
      "Iteration 31990 Training loss 0.061057426035404205 Validation loss 0.055774979293346405 Accuracy 0.8579999804496765\n",
      "Iteration 32000 Training loss 0.07535915821790695 Validation loss 0.06626932322978973 Accuracy 0.8364999890327454\n",
      "Iteration 32010 Training loss 0.07349491119384766 Validation loss 0.06466612219810486 Accuracy 0.8445000052452087\n",
      "Iteration 32020 Training loss 0.06698042154312134 Validation loss 0.06353126466274261 Accuracy 0.8454999923706055\n",
      "Iteration 32030 Training loss 0.06479863077402115 Validation loss 0.06228818744421005 Accuracy 0.8489999771118164\n",
      "Iteration 32040 Training loss 0.06508706510066986 Validation loss 0.05898822844028473 Accuracy 0.8565000295639038\n",
      "Iteration 32050 Training loss 0.07393545657396317 Validation loss 0.06668631732463837 Accuracy 0.8370000123977661\n",
      "Iteration 32060 Training loss 0.06524912267923355 Validation loss 0.06504809856414795 Accuracy 0.8395000100135803\n",
      "Iteration 32070 Training loss 0.06750417500734329 Validation loss 0.06049807742238045 Accuracy 0.8544999957084656\n",
      "Iteration 32080 Training loss 0.07386478781700134 Validation loss 0.0674092024564743 Accuracy 0.8339999914169312\n",
      "Iteration 32090 Training loss 0.06330719590187073 Validation loss 0.05701964721083641 Accuracy 0.8535000085830688\n",
      "Iteration 32100 Training loss 0.06926126033067703 Validation loss 0.06160489097237587 Accuracy 0.8519999980926514\n",
      "Iteration 32110 Training loss 0.06290162354707718 Validation loss 0.060997702181339264 Accuracy 0.8504999876022339\n",
      "Iteration 32120 Training loss 0.06604006886482239 Validation loss 0.05895761772990227 Accuracy 0.8519999980926514\n",
      "Iteration 32130 Training loss 0.0690053179860115 Validation loss 0.06363922357559204 Accuracy 0.8454999923706055\n",
      "Iteration 32140 Training loss 0.06886018812656403 Validation loss 0.06629179418087006 Accuracy 0.840499997138977\n",
      "Iteration 32150 Training loss 0.06330391019582748 Validation loss 0.05945421755313873 Accuracy 0.8519999980926514\n",
      "Iteration 32160 Training loss 0.07417681068181992 Validation loss 0.06620745360851288 Accuracy 0.8355000019073486\n",
      "Iteration 32170 Training loss 0.06272343546152115 Validation loss 0.05783034488558769 Accuracy 0.8579999804496765\n",
      "Iteration 32180 Training loss 0.06931747496128082 Validation loss 0.06337203085422516 Accuracy 0.847000002861023\n",
      "Iteration 32190 Training loss 0.07448390126228333 Validation loss 0.06493241339921951 Accuracy 0.8395000100135803\n",
      "Iteration 32200 Training loss 0.06885027885437012 Validation loss 0.06328056752681732 Accuracy 0.847000002861023\n",
      "Iteration 32210 Training loss 0.06731285154819489 Validation loss 0.0598977655172348 Accuracy 0.8554999828338623\n",
      "Iteration 32220 Training loss 0.06457186490297318 Validation loss 0.05818082392215729 Accuracy 0.8550000190734863\n",
      "Iteration 32230 Training loss 0.07500428706407547 Validation loss 0.0681593045592308 Accuracy 0.8364999890327454\n",
      "Iteration 32240 Training loss 0.0671810731291771 Validation loss 0.06321728974580765 Accuracy 0.8475000262260437\n",
      "Iteration 32250 Training loss 0.07070326805114746 Validation loss 0.06294092535972595 Accuracy 0.847000002861023\n",
      "Iteration 32260 Training loss 0.06789663434028625 Validation loss 0.06031567603349686 Accuracy 0.8539999723434448\n",
      "Iteration 32270 Training loss 0.06614741683006287 Validation loss 0.05841132253408432 Accuracy 0.8569999933242798\n",
      "Iteration 32280 Training loss 0.06587619334459305 Validation loss 0.06037716567516327 Accuracy 0.8525000214576721\n",
      "Iteration 32290 Training loss 0.06518809497356415 Validation loss 0.05906812101602554 Accuracy 0.8535000085830688\n",
      "Iteration 32300 Training loss 0.06601697951555252 Validation loss 0.06325595080852509 Accuracy 0.8479999899864197\n",
      "Iteration 32310 Training loss 0.07815215736627579 Validation loss 0.0666598528623581 Accuracy 0.8385000228881836\n",
      "Iteration 32320 Training loss 0.06695687770843506 Validation loss 0.06318454444408417 Accuracy 0.8454999923706055\n",
      "Iteration 32330 Training loss 0.06350189447402954 Validation loss 0.058899130672216415 Accuracy 0.8544999957084656\n",
      "Iteration 32340 Training loss 0.06775561720132828 Validation loss 0.05837515741586685 Accuracy 0.8565000295639038\n",
      "Iteration 32350 Training loss 0.0689762681722641 Validation loss 0.0639185756444931 Accuracy 0.8429999947547913\n",
      "Iteration 32360 Training loss 0.06454648077487946 Validation loss 0.061473701149225235 Accuracy 0.8529999852180481\n",
      "Iteration 32370 Training loss 0.06917668133974075 Validation loss 0.05948904901742935 Accuracy 0.8550000190734863\n",
      "Iteration 32380 Training loss 0.06743639707565308 Validation loss 0.06190646067261696 Accuracy 0.8504999876022339\n",
      "Iteration 32390 Training loss 0.06997843831777573 Validation loss 0.06038329377770424 Accuracy 0.8554999828338623\n",
      "Iteration 32400 Training loss 0.06606727093458176 Validation loss 0.06240953132510185 Accuracy 0.8519999980926514\n",
      "Iteration 32410 Training loss 0.06973879039287567 Validation loss 0.06076131388545036 Accuracy 0.8500000238418579\n",
      "Iteration 32420 Training loss 0.06381748616695404 Validation loss 0.06068100780248642 Accuracy 0.8525000214576721\n",
      "Iteration 32430 Training loss 0.07112453877925873 Validation loss 0.06022021919488907 Accuracy 0.8544999957084656\n",
      "Iteration 32440 Training loss 0.07252485305070877 Validation loss 0.06481485068798065 Accuracy 0.8414999842643738\n",
      "Iteration 32450 Training loss 0.06131364032626152 Validation loss 0.056220460683107376 Accuracy 0.8600000143051147\n",
      "Iteration 32460 Training loss 0.07000145316123962 Validation loss 0.06462493538856506 Accuracy 0.8445000052452087\n",
      "Iteration 32470 Training loss 0.07114747166633606 Validation loss 0.06297151744365692 Accuracy 0.8500000238418579\n",
      "Iteration 32480 Training loss 0.0691717118024826 Validation loss 0.06095440685749054 Accuracy 0.8519999980926514\n",
      "Iteration 32490 Training loss 0.07147134095430374 Validation loss 0.06578925997018814 Accuracy 0.8395000100135803\n",
      "Iteration 32500 Training loss 0.0652780756354332 Validation loss 0.05892443284392357 Accuracy 0.8550000190734863\n",
      "Iteration 32510 Training loss 0.06555123627185822 Validation loss 0.05805370211601257 Accuracy 0.8554999828338623\n",
      "Iteration 32520 Training loss 0.06924690306186676 Validation loss 0.06195811182260513 Accuracy 0.8479999899864197\n",
      "Iteration 32530 Training loss 0.06326717138290405 Validation loss 0.05787549540400505 Accuracy 0.8565000295639038\n",
      "Iteration 32540 Training loss 0.07109858840703964 Validation loss 0.06308577954769135 Accuracy 0.8460000157356262\n",
      "Iteration 32550 Training loss 0.06104234233498573 Validation loss 0.0570969395339489 Accuracy 0.859499990940094\n",
      "Iteration 32560 Training loss 0.07438497990369797 Validation loss 0.06557310372591019 Accuracy 0.8410000205039978\n",
      "Iteration 32570 Training loss 0.06739217787981033 Validation loss 0.06472472846508026 Accuracy 0.8429999947547913\n",
      "Iteration 32580 Training loss 0.0717272013425827 Validation loss 0.06175366789102554 Accuracy 0.8529999852180481\n",
      "Iteration 32590 Training loss 0.07091400772333145 Validation loss 0.06375456601381302 Accuracy 0.8429999947547913\n",
      "Iteration 32600 Training loss 0.06310687214136124 Validation loss 0.0603201687335968 Accuracy 0.8500000238418579\n",
      "Iteration 32610 Training loss 0.0661054402589798 Validation loss 0.06298554688692093 Accuracy 0.847000002861023\n",
      "Iteration 32620 Training loss 0.06427164375782013 Validation loss 0.06322910636663437 Accuracy 0.8454999923706055\n",
      "Iteration 32630 Training loss 0.07394134253263474 Validation loss 0.06381086260080338 Accuracy 0.8454999923706055\n",
      "Iteration 32640 Training loss 0.06982561945915222 Validation loss 0.06307028234004974 Accuracy 0.8504999876022339\n",
      "Iteration 32650 Training loss 0.06179226562380791 Validation loss 0.05734759569168091 Accuracy 0.8579999804496765\n",
      "Iteration 32660 Training loss 0.0683920681476593 Validation loss 0.06184357777237892 Accuracy 0.8500000238418579\n",
      "Iteration 32670 Training loss 0.06683655083179474 Validation loss 0.056514181196689606 Accuracy 0.8615000247955322\n",
      "Iteration 32680 Training loss 0.07231665402650833 Validation loss 0.0659543126821518 Accuracy 0.8395000100135803\n",
      "Iteration 32690 Training loss 0.061808470636606216 Validation loss 0.056605782359838486 Accuracy 0.8600000143051147\n",
      "Iteration 32700 Training loss 0.06981821358203888 Validation loss 0.06411567330360413 Accuracy 0.843500018119812\n",
      "Iteration 32710 Training loss 0.07155128568410873 Validation loss 0.06159654259681702 Accuracy 0.8504999876022339\n",
      "Iteration 32720 Training loss 0.07428310811519623 Validation loss 0.06818953156471252 Accuracy 0.8335000276565552\n",
      "Iteration 32730 Training loss 0.066900834441185 Validation loss 0.0623958595097065 Accuracy 0.8495000004768372\n",
      "Iteration 32740 Training loss 0.06874965876340866 Validation loss 0.06361301243305206 Accuracy 0.8445000052452087\n",
      "Iteration 32750 Training loss 0.06771967560052872 Validation loss 0.0587707944214344 Accuracy 0.8525000214576721\n",
      "Iteration 32760 Training loss 0.07001744210720062 Validation loss 0.06276395916938782 Accuracy 0.8489999771118164\n",
      "Iteration 32770 Training loss 0.07931604236364365 Validation loss 0.0665426179766655 Accuracy 0.8355000019073486\n",
      "Iteration 32780 Training loss 0.06639204174280167 Validation loss 0.06238093972206116 Accuracy 0.8514999747276306\n",
      "Iteration 32790 Training loss 0.06682976335287094 Validation loss 0.05998142063617706 Accuracy 0.8535000085830688\n",
      "Iteration 32800 Training loss 0.07136871665716171 Validation loss 0.0669134259223938 Accuracy 0.8364999890327454\n",
      "Iteration 32810 Training loss 0.06967534124851227 Validation loss 0.06447844952344894 Accuracy 0.8414999842643738\n",
      "Iteration 32820 Training loss 0.06618358194828033 Validation loss 0.0633927434682846 Accuracy 0.8450000286102295\n",
      "Iteration 32830 Training loss 0.06984864920377731 Validation loss 0.06061440333724022 Accuracy 0.8529999852180481\n",
      "Iteration 32840 Training loss 0.0697675496339798 Validation loss 0.06503809243440628 Accuracy 0.8399999737739563\n",
      "Iteration 32850 Training loss 0.07237566262483597 Validation loss 0.06050984933972359 Accuracy 0.8539999723434448\n",
      "Iteration 32860 Training loss 0.061337076127529144 Validation loss 0.05769272893667221 Accuracy 0.8604999780654907\n",
      "Iteration 32870 Training loss 0.07076616585254669 Validation loss 0.06657411903142929 Accuracy 0.8374999761581421\n",
      "Iteration 32880 Training loss 0.07245956361293793 Validation loss 0.06305017322301865 Accuracy 0.8464999794960022\n",
      "Iteration 32890 Training loss 0.06600846350193024 Validation loss 0.05891045182943344 Accuracy 0.8554999828338623\n",
      "Iteration 32900 Training loss 0.07089739292860031 Validation loss 0.06011928990483284 Accuracy 0.8529999852180481\n",
      "Iteration 32910 Training loss 0.07401487976312637 Validation loss 0.06492055207490921 Accuracy 0.8454999923706055\n",
      "Iteration 32920 Training loss 0.06517297774553299 Validation loss 0.05809108912944794 Accuracy 0.8565000295639038\n",
      "Iteration 32930 Training loss 0.06790966540575027 Validation loss 0.062435317784547806 Accuracy 0.8489999771118164\n",
      "Iteration 32940 Training loss 0.06917178630828857 Validation loss 0.059411078691482544 Accuracy 0.8544999957084656\n",
      "Iteration 32950 Training loss 0.07098288834095001 Validation loss 0.06076952442526817 Accuracy 0.8529999852180481\n",
      "Iteration 32960 Training loss 0.07037562876939774 Validation loss 0.06166582182049751 Accuracy 0.8500000238418579\n",
      "Iteration 32970 Training loss 0.07161642611026764 Validation loss 0.06382311135530472 Accuracy 0.8454999923706055\n",
      "Iteration 32980 Training loss 0.07360009849071503 Validation loss 0.06643052399158478 Accuracy 0.8379999995231628\n",
      "Iteration 32990 Training loss 0.06469982862472534 Validation loss 0.058867137879133224 Accuracy 0.8560000061988831\n",
      "Iteration 33000 Training loss 0.06855959445238113 Validation loss 0.060808319598436356 Accuracy 0.8514999747276306\n",
      "Iteration 33010 Training loss 0.07231777906417847 Validation loss 0.060691773891448975 Accuracy 0.8544999957084656\n",
      "Iteration 33020 Training loss 0.07011383771896362 Validation loss 0.0635218545794487 Accuracy 0.8454999923706055\n",
      "Iteration 33030 Training loss 0.06355608999729156 Validation loss 0.059934865683317184 Accuracy 0.8550000190734863\n",
      "Iteration 33040 Training loss 0.07520031929016113 Validation loss 0.0678609311580658 Accuracy 0.8330000042915344\n",
      "Iteration 33050 Training loss 0.06010657921433449 Validation loss 0.06134425476193428 Accuracy 0.8519999980926514\n",
      "Iteration 33060 Training loss 0.06533024460077286 Validation loss 0.0583934485912323 Accuracy 0.8579999804496765\n",
      "Iteration 33070 Training loss 0.06886201351881027 Validation loss 0.06024036556482315 Accuracy 0.8550000190734863\n",
      "Iteration 33080 Training loss 0.07685109227895737 Validation loss 0.06332570314407349 Accuracy 0.8460000157356262\n",
      "Iteration 33090 Training loss 0.06268806010484695 Validation loss 0.05874207988381386 Accuracy 0.8550000190734863\n",
      "Iteration 33100 Training loss 0.056194886565208435 Validation loss 0.05699066072702408 Accuracy 0.8600000143051147\n",
      "Iteration 33110 Training loss 0.07487545907497406 Validation loss 0.06493199616670609 Accuracy 0.8395000100135803\n",
      "Iteration 33120 Training loss 0.06492464989423752 Validation loss 0.056748855859041214 Accuracy 0.8610000014305115\n",
      "Iteration 33130 Training loss 0.07140159606933594 Validation loss 0.0627962276339531 Accuracy 0.847000002861023\n",
      "Iteration 33140 Training loss 0.069540835916996 Validation loss 0.06285171210765839 Accuracy 0.8460000157356262\n",
      "Iteration 33150 Training loss 0.060061898082494736 Validation loss 0.057600948959589005 Accuracy 0.8600000143051147\n",
      "Iteration 33160 Training loss 0.0678185522556305 Validation loss 0.0628138929605484 Accuracy 0.847000002861023\n",
      "Iteration 33170 Training loss 0.07072705030441284 Validation loss 0.06362661719322205 Accuracy 0.8414999842643738\n",
      "Iteration 33180 Training loss 0.07085925340652466 Validation loss 0.06245074421167374 Accuracy 0.8460000157356262\n",
      "Iteration 33190 Training loss 0.06765475124120712 Validation loss 0.058001138269901276 Accuracy 0.8565000295639038\n",
      "Iteration 33200 Training loss 0.0698641836643219 Validation loss 0.059833038598299026 Accuracy 0.8519999980926514\n",
      "Iteration 33210 Training loss 0.06724181771278381 Validation loss 0.060317154973745346 Accuracy 0.8539999723434448\n",
      "Iteration 33220 Training loss 0.06964820623397827 Validation loss 0.060082368552684784 Accuracy 0.8529999852180481\n",
      "Iteration 33230 Training loss 0.0712408646941185 Validation loss 0.06127621605992317 Accuracy 0.8510000109672546\n",
      "Iteration 33240 Training loss 0.06293889880180359 Validation loss 0.056332170963287354 Accuracy 0.8600000143051147\n",
      "Iteration 33250 Training loss 0.06193843111395836 Validation loss 0.05600598454475403 Accuracy 0.8575000166893005\n",
      "Iteration 33260 Training loss 0.06109341233968735 Validation loss 0.058788277208805084 Accuracy 0.8560000061988831\n",
      "Iteration 33270 Training loss 0.07744184881448746 Validation loss 0.06486745178699493 Accuracy 0.8395000100135803\n",
      "Iteration 33280 Training loss 0.07243294268846512 Validation loss 0.06199187412858009 Accuracy 0.8500000238418579\n",
      "Iteration 33290 Training loss 0.06008882820606232 Validation loss 0.05779001861810684 Accuracy 0.8619999885559082\n",
      "Iteration 33300 Training loss 0.06892424076795578 Validation loss 0.0626625195145607 Accuracy 0.8485000133514404\n",
      "Iteration 33310 Training loss 0.06616580486297607 Validation loss 0.062004316598176956 Accuracy 0.8519999980926514\n",
      "Iteration 33320 Training loss 0.06704472005367279 Validation loss 0.06353291124105453 Accuracy 0.8450000286102295\n",
      "Iteration 33330 Training loss 0.07716130465269089 Validation loss 0.06551112234592438 Accuracy 0.843999981880188\n",
      "Iteration 33340 Training loss 0.06860389560461044 Validation loss 0.060601405799388885 Accuracy 0.8529999852180481\n",
      "Iteration 33350 Training loss 0.07282978296279907 Validation loss 0.06471884995698929 Accuracy 0.843500018119812\n",
      "Iteration 33360 Training loss 0.06767687201499939 Validation loss 0.05939265340566635 Accuracy 0.8565000295639038\n",
      "Iteration 33370 Training loss 0.06417767703533173 Validation loss 0.06144608557224274 Accuracy 0.8495000004768372\n",
      "Iteration 33380 Training loss 0.08351746946573257 Validation loss 0.06839786469936371 Accuracy 0.8320000171661377\n",
      "Iteration 33390 Training loss 0.06382691860198975 Validation loss 0.05736425146460533 Accuracy 0.8615000247955322\n",
      "Iteration 33400 Training loss 0.06531262397766113 Validation loss 0.06124177947640419 Accuracy 0.8519999980926514\n",
      "Iteration 33410 Training loss 0.07046424597501755 Validation loss 0.056426115334033966 Accuracy 0.8615000247955322\n",
      "Iteration 33420 Training loss 0.06479570269584656 Validation loss 0.05769950896501541 Accuracy 0.859000027179718\n",
      "Iteration 33430 Training loss 0.0713307335972786 Validation loss 0.06276153773069382 Accuracy 0.8479999899864197\n",
      "Iteration 33440 Training loss 0.07454881817102432 Validation loss 0.06573072820901871 Accuracy 0.8395000100135803\n",
      "Iteration 33450 Training loss 0.06277962774038315 Validation loss 0.05822349712252617 Accuracy 0.8565000295639038\n",
      "Iteration 33460 Training loss 0.0693262368440628 Validation loss 0.06656530499458313 Accuracy 0.8349999785423279\n",
      "Iteration 33470 Training loss 0.06736254692077637 Validation loss 0.05789608880877495 Accuracy 0.859499990940094\n",
      "Iteration 33480 Training loss 0.06956856697797775 Validation loss 0.06046021357178688 Accuracy 0.8519999980926514\n",
      "Iteration 33490 Training loss 0.07108291983604431 Validation loss 0.059725768864154816 Accuracy 0.8544999957084656\n",
      "Iteration 33500 Training loss 0.05862236022949219 Validation loss 0.0577266700565815 Accuracy 0.8585000038146973\n",
      "Iteration 33510 Training loss 0.07291800528764725 Validation loss 0.06311766803264618 Accuracy 0.8445000052452087\n",
      "Iteration 33520 Training loss 0.07092446833848953 Validation loss 0.06291121989488602 Accuracy 0.8489999771118164\n",
      "Iteration 33530 Training loss 0.05970935896039009 Validation loss 0.05733608081936836 Accuracy 0.8610000014305115\n",
      "Iteration 33540 Training loss 0.06858081370592117 Validation loss 0.060688186436891556 Accuracy 0.8525000214576721\n",
      "Iteration 33550 Training loss 0.061698783189058304 Validation loss 0.056958429515361786 Accuracy 0.859000027179718\n",
      "Iteration 33560 Training loss 0.06620274484157562 Validation loss 0.05816434696316719 Accuracy 0.859000027179718\n",
      "Iteration 33570 Training loss 0.06361363083124161 Validation loss 0.05893401801586151 Accuracy 0.8560000061988831\n",
      "Iteration 33580 Training loss 0.06691940873861313 Validation loss 0.06395138055086136 Accuracy 0.8450000286102295\n",
      "Iteration 33590 Training loss 0.07366589456796646 Validation loss 0.06530265510082245 Accuracy 0.8399999737739563\n",
      "Iteration 33600 Training loss 0.05837289243936539 Validation loss 0.05717644467949867 Accuracy 0.859499990940094\n",
      "Iteration 33610 Training loss 0.07768423855304718 Validation loss 0.06627223640680313 Accuracy 0.8374999761581421\n",
      "Iteration 33620 Training loss 0.07612667232751846 Validation loss 0.06357790529727936 Accuracy 0.8454999923706055\n",
      "Iteration 33630 Training loss 0.06501968204975128 Validation loss 0.05931666120886803 Accuracy 0.8560000061988831\n",
      "Iteration 33640 Training loss 0.06646658480167389 Validation loss 0.05953320488333702 Accuracy 0.859000027179718\n",
      "Iteration 33650 Training loss 0.06245388835668564 Validation loss 0.05741368606686592 Accuracy 0.8569999933242798\n",
      "Iteration 33660 Training loss 0.07586278021335602 Validation loss 0.06268186867237091 Accuracy 0.8489999771118164\n",
      "Iteration 33670 Training loss 0.06212149187922478 Validation loss 0.0582505501806736 Accuracy 0.8604999780654907\n",
      "Iteration 33680 Training loss 0.06568382680416107 Validation loss 0.05981122702360153 Accuracy 0.8565000295639038\n",
      "Iteration 33690 Training loss 0.07765613496303558 Validation loss 0.0621953047811985 Accuracy 0.8525000214576721\n",
      "Iteration 33700 Training loss 0.07152646780014038 Validation loss 0.06303513795137405 Accuracy 0.8464999794960022\n",
      "Iteration 33710 Training loss 0.07616187632083893 Validation loss 0.06317095458507538 Accuracy 0.8464999794960022\n",
      "Iteration 33720 Training loss 0.06970535218715668 Validation loss 0.05974343791604042 Accuracy 0.8544999957084656\n",
      "Iteration 33730 Training loss 0.07474333047866821 Validation loss 0.06392383575439453 Accuracy 0.8424999713897705\n",
      "Iteration 33740 Training loss 0.06930383294820786 Validation loss 0.0591496080160141 Accuracy 0.8535000085830688\n",
      "Iteration 33750 Training loss 0.07524992525577545 Validation loss 0.06163224205374718 Accuracy 0.8519999980926514\n",
      "Iteration 33760 Training loss 0.061010003089904785 Validation loss 0.05704030022025108 Accuracy 0.8550000190734863\n",
      "Iteration 33770 Training loss 0.06182361766695976 Validation loss 0.056270942091941833 Accuracy 0.8604999780654907\n",
      "Iteration 33780 Training loss 0.07469891011714935 Validation loss 0.07422371953725815 Accuracy 0.8230000138282776\n",
      "Iteration 33790 Training loss 0.07490188628435135 Validation loss 0.07335308939218521 Accuracy 0.8255000114440918\n",
      "Iteration 33800 Training loss 0.06389337033033371 Validation loss 0.061951179057359695 Accuracy 0.8500000238418579\n",
      "Iteration 33810 Training loss 0.06919705122709274 Validation loss 0.0695122703909874 Accuracy 0.8314999938011169\n",
      "Iteration 33820 Training loss 0.06862302869558334 Validation loss 0.06483057886362076 Accuracy 0.8429999947547913\n",
      "Iteration 33830 Training loss 0.06882905215024948 Validation loss 0.06404148787260056 Accuracy 0.843999981880188\n",
      "Iteration 33840 Training loss 0.06964793056249619 Validation loss 0.0666288286447525 Accuracy 0.8385000228881836\n",
      "Iteration 33850 Training loss 0.07108595222234726 Validation loss 0.06878369301557541 Accuracy 0.8349999785423279\n",
      "Iteration 33860 Training loss 0.058865491300821304 Validation loss 0.06282547116279602 Accuracy 0.8464999794960022\n",
      "Iteration 33870 Training loss 0.07314527779817581 Validation loss 0.06768506020307541 Accuracy 0.8349999785423279\n",
      "Iteration 33880 Training loss 0.07360658049583435 Validation loss 0.0712326318025589 Accuracy 0.8264999985694885\n",
      "Iteration 33890 Training loss 0.07236140221357346 Validation loss 0.07350662350654602 Accuracy 0.8230000138282776\n",
      "Iteration 33900 Training loss 0.0742373988032341 Validation loss 0.07432243227958679 Accuracy 0.8184999823570251\n",
      "Iteration 33910 Training loss 0.06276863813400269 Validation loss 0.06197618320584297 Accuracy 0.8479999899864197\n",
      "Iteration 33920 Training loss 0.06494960188865662 Validation loss 0.059911735355854034 Accuracy 0.8495000004768372\n",
      "Iteration 33930 Training loss 0.0709596797823906 Validation loss 0.06747157871723175 Accuracy 0.8355000019073486\n",
      "Iteration 33940 Training loss 0.06440458446741104 Validation loss 0.05978791043162346 Accuracy 0.8525000214576721\n",
      "Iteration 33950 Training loss 0.0744004175066948 Validation loss 0.07330438494682312 Accuracy 0.824999988079071\n",
      "Iteration 33960 Training loss 0.07212299108505249 Validation loss 0.07833559811115265 Accuracy 0.8144999742507935\n",
      "Iteration 33970 Training loss 0.07289944589138031 Validation loss 0.07053358852863312 Accuracy 0.8309999704360962\n",
      "Iteration 33980 Training loss 0.06877321749925613 Validation loss 0.06731829792261124 Accuracy 0.8364999890327454\n",
      "Iteration 33990 Training loss 0.07449763268232346 Validation loss 0.07260982692241669 Accuracy 0.8270000219345093\n",
      "Iteration 34000 Training loss 0.07559891045093536 Validation loss 0.071282297372818 Accuracy 0.828499972820282\n",
      "Iteration 34010 Training loss 0.07084127515554428 Validation loss 0.07547908276319504 Accuracy 0.8209999799728394\n",
      "Iteration 34020 Training loss 0.0728340670466423 Validation loss 0.07261506468057632 Accuracy 0.8255000114440918\n",
      "Iteration 34030 Training loss 0.06864514201879501 Validation loss 0.06645289808511734 Accuracy 0.8399999737739563\n",
      "Iteration 34040 Training loss 0.07036258280277252 Validation loss 0.06898017972707748 Accuracy 0.8345000147819519\n",
      "Iteration 34050 Training loss 0.07539593428373337 Validation loss 0.0730169489979744 Accuracy 0.8264999985694885\n",
      "Iteration 34060 Training loss 0.06514867395162582 Validation loss 0.06782430410385132 Accuracy 0.8370000123977661\n",
      "Iteration 34070 Training loss 0.06250271201133728 Validation loss 0.06585146486759186 Accuracy 0.840499997138977\n",
      "Iteration 34080 Training loss 0.061785995960235596 Validation loss 0.05921946465969086 Accuracy 0.8514999747276306\n",
      "Iteration 34090 Training loss 0.07293219864368439 Validation loss 0.06689426302909851 Accuracy 0.840499997138977\n",
      "Iteration 34100 Training loss 0.07981013506650925 Validation loss 0.07713239639997482 Accuracy 0.815500020980835\n",
      "Iteration 34110 Training loss 0.07155224680900574 Validation loss 0.06988620012998581 Accuracy 0.8335000276565552\n",
      "Iteration 34120 Training loss 0.07908474653959274 Validation loss 0.07902558892965317 Accuracy 0.8130000233650208\n",
      "Iteration 34130 Training loss 0.07412898540496826 Validation loss 0.06840221583843231 Accuracy 0.8364999890327454\n",
      "Iteration 34140 Training loss 0.07747798413038254 Validation loss 0.07244667410850525 Accuracy 0.8274999856948853\n",
      "Iteration 34150 Training loss 0.06717284023761749 Validation loss 0.062327392399311066 Accuracy 0.8500000238418579\n",
      "Iteration 34160 Training loss 0.07421635091304779 Validation loss 0.07218960672616959 Accuracy 0.8259999752044678\n",
      "Iteration 34170 Training loss 0.07191426306962967 Validation loss 0.06956393271684647 Accuracy 0.8309999704360962\n",
      "Iteration 34180 Training loss 0.0614829882979393 Validation loss 0.06498225033283234 Accuracy 0.843500018119812\n",
      "Iteration 34190 Training loss 0.06575936824083328 Validation loss 0.05907071381807327 Accuracy 0.8569999933242798\n",
      "Iteration 34200 Training loss 0.07683999836444855 Validation loss 0.0633138045668602 Accuracy 0.8445000052452087\n",
      "Iteration 34210 Training loss 0.06657116860151291 Validation loss 0.05783934146165848 Accuracy 0.8585000038146973\n",
      "Iteration 34220 Training loss 0.06975996494293213 Validation loss 0.059376705437898636 Accuracy 0.8585000038146973\n",
      "Iteration 34230 Training loss 0.07104822993278503 Validation loss 0.06130770966410637 Accuracy 0.8519999980926514\n",
      "Iteration 34240 Training loss 0.06094265729188919 Validation loss 0.05885850265622139 Accuracy 0.8544999957084656\n",
      "Iteration 34250 Training loss 0.0654057189822197 Validation loss 0.05866899713873863 Accuracy 0.8554999828338623\n",
      "Iteration 34260 Training loss 0.07308965921401978 Validation loss 0.06291577219963074 Accuracy 0.8495000004768372\n",
      "Iteration 34270 Training loss 0.06367231905460358 Validation loss 0.057817183434963226 Accuracy 0.8579999804496765\n",
      "Iteration 34280 Training loss 0.06962887942790985 Validation loss 0.06002373620867729 Accuracy 0.8569999933242798\n",
      "Iteration 34290 Training loss 0.05837725102901459 Validation loss 0.05703352764248848 Accuracy 0.859499990940094\n",
      "Iteration 34300 Training loss 0.07288802415132523 Validation loss 0.05996580794453621 Accuracy 0.8550000190734863\n",
      "Iteration 34310 Training loss 0.06880942732095718 Validation loss 0.06109322980046272 Accuracy 0.8544999957084656\n",
      "Iteration 34320 Training loss 0.07334309071302414 Validation loss 0.0618436224758625 Accuracy 0.8514999747276306\n",
      "Iteration 34330 Training loss 0.06380187720060349 Validation loss 0.059187162667512894 Accuracy 0.8575000166893005\n",
      "Iteration 34340 Training loss 0.06930975615978241 Validation loss 0.06033005565404892 Accuracy 0.8514999747276306\n",
      "Iteration 34350 Training loss 0.07040350139141083 Validation loss 0.06092919781804085 Accuracy 0.8514999747276306\n",
      "Iteration 34360 Training loss 0.07437039911746979 Validation loss 0.06121741235256195 Accuracy 0.8535000085830688\n",
      "Iteration 34370 Training loss 0.07082971930503845 Validation loss 0.0627191960811615 Accuracy 0.8495000004768372\n",
      "Iteration 34380 Training loss 0.0730803906917572 Validation loss 0.061364926397800446 Accuracy 0.8514999747276306\n",
      "Iteration 34390 Training loss 0.07581491023302078 Validation loss 0.06400182843208313 Accuracy 0.8460000157356262\n",
      "Iteration 34400 Training loss 0.07131600379943848 Validation loss 0.060225967317819595 Accuracy 0.8529999852180481\n",
      "Iteration 34410 Training loss 0.07150653004646301 Validation loss 0.06264628469944 Accuracy 0.8500000238418579\n",
      "Iteration 34420 Training loss 0.06843496859073639 Validation loss 0.060864321887493134 Accuracy 0.8535000085830688\n",
      "Iteration 34430 Training loss 0.07104391604661942 Validation loss 0.06231706216931343 Accuracy 0.8510000109672546\n",
      "Iteration 34440 Training loss 0.06220733001828194 Validation loss 0.05784926936030388 Accuracy 0.8604999780654907\n",
      "Iteration 34450 Training loss 0.07209797948598862 Validation loss 0.06887520104646683 Accuracy 0.8330000042915344\n",
      "Iteration 34460 Training loss 0.06659721583127975 Validation loss 0.06569613516330719 Accuracy 0.8410000205039978\n",
      "Iteration 34470 Training loss 0.06998568773269653 Validation loss 0.06832978874444962 Accuracy 0.8360000252723694\n",
      "Iteration 34480 Training loss 0.06657469272613525 Validation loss 0.058558836579322815 Accuracy 0.8554999828338623\n",
      "Iteration 34490 Training loss 0.0666327029466629 Validation loss 0.05699894577264786 Accuracy 0.859000027179718\n",
      "Iteration 34500 Training loss 0.06663189828395844 Validation loss 0.06404859572649002 Accuracy 0.8450000286102295\n",
      "Iteration 34510 Training loss 0.07882780581712723 Validation loss 0.07084791362285614 Accuracy 0.8299999833106995\n",
      "Iteration 34520 Training loss 0.06674965471029282 Validation loss 0.06798135489225388 Accuracy 0.8379999995231628\n",
      "Iteration 34530 Training loss 0.06993626058101654 Validation loss 0.07085715234279633 Accuracy 0.8309999704360962\n",
      "Iteration 34540 Training loss 0.07256428897380829 Validation loss 0.06900554895401001 Accuracy 0.8324999809265137\n",
      "Iteration 34550 Training loss 0.0644245520234108 Validation loss 0.057316504418849945 Accuracy 0.8569999933242798\n",
      "Iteration 34560 Training loss 0.07567431777715683 Validation loss 0.07423855364322662 Accuracy 0.8255000114440918\n",
      "Iteration 34570 Training loss 0.06727740913629532 Validation loss 0.06586268544197083 Accuracy 0.840499997138977\n",
      "Iteration 34580 Training loss 0.06686216592788696 Validation loss 0.0628691092133522 Accuracy 0.8460000157356262\n",
      "Iteration 34590 Training loss 0.07802680879831314 Validation loss 0.06556332111358643 Accuracy 0.8410000205039978\n",
      "Iteration 34600 Training loss 0.06531433016061783 Validation loss 0.05801069736480713 Accuracy 0.8554999828338623\n",
      "Iteration 34610 Training loss 0.0757964476943016 Validation loss 0.06405087560415268 Accuracy 0.8464999794960022\n",
      "Iteration 34620 Training loss 0.08007032424211502 Validation loss 0.06622396409511566 Accuracy 0.8389999866485596\n",
      "Iteration 34630 Training loss 0.0708908811211586 Validation loss 0.059032149612903595 Accuracy 0.8569999933242798\n",
      "Iteration 34640 Training loss 0.07650443911552429 Validation loss 0.06414733827114105 Accuracy 0.8475000262260437\n",
      "Iteration 34650 Training loss 0.07512861490249634 Validation loss 0.06210546940565109 Accuracy 0.8514999747276306\n",
      "Iteration 34660 Training loss 0.07457012683153152 Validation loss 0.06166783347725868 Accuracy 0.8475000262260437\n",
      "Iteration 34670 Training loss 0.06530530005693436 Validation loss 0.05942654237151146 Accuracy 0.8579999804496765\n",
      "Iteration 34680 Training loss 0.07440898567438126 Validation loss 0.060687657445669174 Accuracy 0.8554999828338623\n",
      "Iteration 34690 Training loss 0.06996945291757584 Validation loss 0.05852174013853073 Accuracy 0.8550000190734863\n",
      "Iteration 34700 Training loss 0.06575750559568405 Validation loss 0.058720048516988754 Accuracy 0.8575000166893005\n",
      "Iteration 34710 Training loss 0.06750547140836716 Validation loss 0.05749867856502533 Accuracy 0.8585000038146973\n",
      "Iteration 34720 Training loss 0.08059760183095932 Validation loss 0.0667690858244896 Accuracy 0.8379999995231628\n",
      "Iteration 34730 Training loss 0.06300551444292068 Validation loss 0.05756865814328194 Accuracy 0.8600000143051147\n",
      "Iteration 34740 Training loss 0.06389569491147995 Validation loss 0.05891294404864311 Accuracy 0.8579999804496765\n",
      "Iteration 34750 Training loss 0.0689699649810791 Validation loss 0.05906731262803078 Accuracy 0.8579999804496765\n",
      "Iteration 34760 Training loss 0.06864706426858902 Validation loss 0.06135758385062218 Accuracy 0.8514999747276306\n",
      "Iteration 34770 Training loss 0.0690499097108841 Validation loss 0.05912576988339424 Accuracy 0.8560000061988831\n",
      "Iteration 34780 Training loss 0.07318183779716492 Validation loss 0.06407982110977173 Accuracy 0.8460000157356262\n",
      "Iteration 34790 Training loss 0.07467107474803925 Validation loss 0.06191384792327881 Accuracy 0.8489999771118164\n",
      "Iteration 34800 Training loss 0.07569006830453873 Validation loss 0.06048327684402466 Accuracy 0.8554999828338623\n",
      "Iteration 34810 Training loss 0.06682833284139633 Validation loss 0.05843399092555046 Accuracy 0.8579999804496765\n",
      "Iteration 34820 Training loss 0.06599816679954529 Validation loss 0.057669952511787415 Accuracy 0.859000027179718\n",
      "Iteration 34830 Training loss 0.0780089944601059 Validation loss 0.06299062818288803 Accuracy 0.8500000238418579\n",
      "Iteration 34840 Training loss 0.07606449723243713 Validation loss 0.06163353845477104 Accuracy 0.8544999957084656\n",
      "Iteration 34850 Training loss 0.07124710083007812 Validation loss 0.06033431366086006 Accuracy 0.8565000295639038\n",
      "Iteration 34860 Training loss 0.0677044466137886 Validation loss 0.05943993851542473 Accuracy 0.8585000038146973\n",
      "Iteration 34870 Training loss 0.06967590004205704 Validation loss 0.05840207636356354 Accuracy 0.8569999933242798\n",
      "Iteration 34880 Training loss 0.08009593188762665 Validation loss 0.0651598647236824 Accuracy 0.8414999842643738\n",
      "Iteration 34890 Training loss 0.06559562683105469 Validation loss 0.05866798385977745 Accuracy 0.8600000143051147\n",
      "Iteration 34900 Training loss 0.06787121295928955 Validation loss 0.06053481251001358 Accuracy 0.8550000190734863\n",
      "Iteration 34910 Training loss 0.06823768466711044 Validation loss 0.0607324056327343 Accuracy 0.8510000109672546\n",
      "Iteration 34920 Training loss 0.07607636600732803 Validation loss 0.06201818957924843 Accuracy 0.8500000238418579\n",
      "Iteration 34930 Training loss 0.06980881094932556 Validation loss 0.05963600426912308 Accuracy 0.8569999933242798\n",
      "Iteration 34940 Training loss 0.07207945734262466 Validation loss 0.061492666602134705 Accuracy 0.8544999957084656\n",
      "Iteration 34950 Training loss 0.0697978287935257 Validation loss 0.06147725507616997 Accuracy 0.8544999957084656\n",
      "Iteration 34960 Training loss 0.07967027276754379 Validation loss 0.06629883497953415 Accuracy 0.8360000252723694\n",
      "Iteration 34970 Training loss 0.07217852026224136 Validation loss 0.06173887848854065 Accuracy 0.8535000085830688\n",
      "Iteration 34980 Training loss 0.06552115827798843 Validation loss 0.060388900339603424 Accuracy 0.8514999747276306\n",
      "Iteration 34990 Training loss 0.07518485933542252 Validation loss 0.0739540383219719 Accuracy 0.8259999752044678\n",
      "Iteration 35000 Training loss 0.07847034186124802 Validation loss 0.07015956193208694 Accuracy 0.8345000147819519\n",
      "Iteration 35010 Training loss 0.07002831250429153 Validation loss 0.06421681493520737 Accuracy 0.8460000157356262\n",
      "Iteration 35020 Training loss 0.085500568151474 Validation loss 0.07945743203163147 Accuracy 0.809499979019165\n",
      "Iteration 35030 Training loss 0.06630834937095642 Validation loss 0.06949107348918915 Accuracy 0.8360000252723694\n",
      "Iteration 35040 Training loss 0.07999344170093536 Validation loss 0.0834042951464653 Accuracy 0.8034999966621399\n",
      "Iteration 35050 Training loss 0.06704597920179367 Validation loss 0.06361936777830124 Accuracy 0.8460000157356262\n",
      "Iteration 35060 Training loss 0.06498678773641586 Validation loss 0.06319346278905869 Accuracy 0.8504999876022339\n",
      "Iteration 35070 Training loss 0.06986606866121292 Validation loss 0.07179560512304306 Accuracy 0.8289999961853027\n",
      "Iteration 35080 Training loss 0.07204810529947281 Validation loss 0.06705030798912048 Accuracy 0.8395000100135803\n",
      "Iteration 35090 Training loss 0.06272654980421066 Validation loss 0.062308501452207565 Accuracy 0.8529999852180481\n",
      "Iteration 35100 Training loss 0.06618888676166534 Validation loss 0.06599302589893341 Accuracy 0.8414999842643738\n",
      "Iteration 35110 Training loss 0.09281670302152634 Validation loss 0.09225844591856003 Accuracy 0.7829999923706055\n",
      "Iteration 35120 Training loss 0.0665382668375969 Validation loss 0.06311141699552536 Accuracy 0.8464999794960022\n",
      "Iteration 35130 Training loss 0.06670532375574112 Validation loss 0.06555725634098053 Accuracy 0.8420000076293945\n",
      "Iteration 35140 Training loss 0.0825086161494255 Validation loss 0.0831470713019371 Accuracy 0.8029999732971191\n",
      "Iteration 35150 Training loss 0.06704597175121307 Validation loss 0.06903067231178284 Accuracy 0.8355000019073486\n",
      "Iteration 35160 Training loss 0.07031320035457611 Validation loss 0.06850428879261017 Accuracy 0.8364999890327454\n",
      "Iteration 35170 Training loss 0.08581965416669846 Validation loss 0.08472536504268646 Accuracy 0.8040000200271606\n",
      "Iteration 35180 Training loss 0.07514346390962601 Validation loss 0.07598952203989029 Accuracy 0.8195000290870667\n",
      "Iteration 35190 Training loss 0.07580114901065826 Validation loss 0.07290714979171753 Accuracy 0.8264999985694885\n",
      "Iteration 35200 Training loss 0.06957020610570908 Validation loss 0.06703320145606995 Accuracy 0.8370000123977661\n",
      "Iteration 35210 Training loss 0.07699364423751831 Validation loss 0.0724942609667778 Accuracy 0.8259999752044678\n",
      "Iteration 35220 Training loss 0.07233647257089615 Validation loss 0.0714966431260109 Accuracy 0.8289999961853027\n",
      "Iteration 35230 Training loss 0.07063105702400208 Validation loss 0.07070490717887878 Accuracy 0.8320000171661377\n",
      "Iteration 35240 Training loss 0.06748579442501068 Validation loss 0.06804970651865005 Accuracy 0.8385000228881836\n",
      "Iteration 35250 Training loss 0.07383047789335251 Validation loss 0.07166125625371933 Accuracy 0.8320000171661377\n",
      "Iteration 35260 Training loss 0.07312147319316864 Validation loss 0.06905215978622437 Accuracy 0.8339999914169312\n",
      "Iteration 35270 Training loss 0.06969635933637619 Validation loss 0.07081449776887894 Accuracy 0.8305000066757202\n",
      "Iteration 35280 Training loss 0.07754568010568619 Validation loss 0.07283332943916321 Accuracy 0.8274999856948853\n",
      "Iteration 35290 Training loss 0.0738271027803421 Validation loss 0.07547248899936676 Accuracy 0.8240000009536743\n",
      "Iteration 35300 Training loss 0.0650034174323082 Validation loss 0.06048893555998802 Accuracy 0.8514999747276306\n",
      "Iteration 35310 Training loss 0.06254942715167999 Validation loss 0.05837062746286392 Accuracy 0.8579999804496765\n",
      "Iteration 35320 Training loss 0.0826457217335701 Validation loss 0.07689332216978073 Accuracy 0.8174999952316284\n",
      "Iteration 35330 Training loss 0.07417450100183487 Validation loss 0.0717131644487381 Accuracy 0.8289999961853027\n",
      "Iteration 35340 Training loss 0.07160200923681259 Validation loss 0.0770842581987381 Accuracy 0.8165000081062317\n",
      "Iteration 35350 Training loss 0.07826864719390869 Validation loss 0.0805116593837738 Accuracy 0.8109999895095825\n",
      "Iteration 35360 Training loss 0.08011957257986069 Validation loss 0.07710184156894684 Accuracy 0.8205000162124634\n",
      "Iteration 35370 Training loss 0.07340432703495026 Validation loss 0.07068779319524765 Accuracy 0.8309999704360962\n",
      "Iteration 35380 Training loss 0.07467556744813919 Validation loss 0.08078400790691376 Accuracy 0.8105000257492065\n",
      "Iteration 35390 Training loss 0.06906933337450027 Validation loss 0.06774893403053284 Accuracy 0.8395000100135803\n",
      "Iteration 35400 Training loss 0.07227851450443268 Validation loss 0.07448342442512512 Accuracy 0.824999988079071\n",
      "Iteration 35410 Training loss 0.07312019169330597 Validation loss 0.07431943714618683 Accuracy 0.8234999775886536\n",
      "Iteration 35420 Training loss 0.06843914836645126 Validation loss 0.06872393190860748 Accuracy 0.8370000123977661\n",
      "Iteration 35430 Training loss 0.07652001827955246 Validation loss 0.07772340625524521 Accuracy 0.8159999847412109\n",
      "Iteration 35440 Training loss 0.06709940731525421 Validation loss 0.07021555304527283 Accuracy 0.8339999914169312\n",
      "Iteration 35450 Training loss 0.06987996399402618 Validation loss 0.07253359258174896 Accuracy 0.8295000195503235\n",
      "Iteration 35460 Training loss 0.062496624886989594 Validation loss 0.059954434633255005 Accuracy 0.8510000109672546\n",
      "Iteration 35470 Training loss 0.07592558860778809 Validation loss 0.07731808722019196 Accuracy 0.8195000290870667\n",
      "Iteration 35480 Training loss 0.06712073087692261 Validation loss 0.06204688921570778 Accuracy 0.8510000109672546\n",
      "Iteration 35490 Training loss 0.08282727003097534 Validation loss 0.08570370078086853 Accuracy 0.8004999756813049\n",
      "Iteration 35500 Training loss 0.06355084478855133 Validation loss 0.06415766477584839 Accuracy 0.8475000262260437\n",
      "Iteration 35510 Training loss 0.07647356390953064 Validation loss 0.06321027874946594 Accuracy 0.8495000004768372\n",
      "Iteration 35520 Training loss 0.06357559561729431 Validation loss 0.05847031995654106 Accuracy 0.8610000014305115\n",
      "Iteration 35530 Training loss 0.08147311210632324 Validation loss 0.06572893261909485 Accuracy 0.8454999923706055\n",
      "Iteration 35540 Training loss 0.07566354423761368 Validation loss 0.06143335625529289 Accuracy 0.8525000214576721\n",
      "Iteration 35550 Training loss 0.08016631752252579 Validation loss 0.06138446927070618 Accuracy 0.8514999747276306\n",
      "Iteration 35560 Training loss 0.0826808363199234 Validation loss 0.06439089775085449 Accuracy 0.8464999794960022\n",
      "Iteration 35570 Training loss 0.0651434138417244 Validation loss 0.059270791709423065 Accuracy 0.8539999723434448\n",
      "Iteration 35580 Training loss 0.0837007611989975 Validation loss 0.06507784128189087 Accuracy 0.8460000157356262\n",
      "Iteration 35590 Training loss 0.06977973133325577 Validation loss 0.06042221188545227 Accuracy 0.8579999804496765\n",
      "Iteration 35600 Training loss 0.07723142951726913 Validation loss 0.06204528734087944 Accuracy 0.8519999980926514\n",
      "Iteration 35610 Training loss 0.07125736773014069 Validation loss 0.060474880039691925 Accuracy 0.8544999957084656\n",
      "Iteration 35620 Training loss 0.07258395850658417 Validation loss 0.06367028504610062 Accuracy 0.847000002861023\n",
      "Iteration 35630 Training loss 0.0680127888917923 Validation loss 0.058298561722040176 Accuracy 0.8569999933242798\n",
      "Iteration 35640 Training loss 0.07016061246395111 Validation loss 0.0712767019867897 Accuracy 0.8309999704360962\n",
      "Iteration 35650 Training loss 0.07217998057603836 Validation loss 0.0705777257680893 Accuracy 0.8335000276565552\n",
      "Iteration 35660 Training loss 0.080247662961483 Validation loss 0.07832994312047958 Accuracy 0.8130000233650208\n",
      "Iteration 35670 Training loss 0.06295368820428848 Validation loss 0.0660691037774086 Accuracy 0.8429999947547913\n",
      "Iteration 35680 Training loss 0.07319881021976471 Validation loss 0.07370023429393768 Accuracy 0.8230000138282776\n",
      "Iteration 35690 Training loss 0.07003908604383469 Validation loss 0.06558222323656082 Accuracy 0.8445000052452087\n",
      "Iteration 35700 Training loss 0.06754237413406372 Validation loss 0.06782922148704529 Accuracy 0.8385000228881836\n",
      "Iteration 35710 Training loss 0.07431934773921967 Validation loss 0.06745455414056778 Accuracy 0.840499997138977\n",
      "Iteration 35720 Training loss 0.07475171238183975 Validation loss 0.07600089907646179 Accuracy 0.8220000267028809\n",
      "Iteration 35730 Training loss 0.07268478721380234 Validation loss 0.06754188239574432 Accuracy 0.8385000228881836\n",
      "Iteration 35740 Training loss 0.08354034274816513 Validation loss 0.0812765434384346 Accuracy 0.8090000152587891\n",
      "Iteration 35750 Training loss 0.07230909913778305 Validation loss 0.07170022279024124 Accuracy 0.8305000066757202\n",
      "Iteration 35760 Training loss 0.0793379545211792 Validation loss 0.07814531773328781 Accuracy 0.8174999952316284\n",
      "Iteration 35770 Training loss 0.07258904725313187 Validation loss 0.06551788747310638 Accuracy 0.843999981880188\n",
      "Iteration 35780 Training loss 0.07402551174163818 Validation loss 0.0731450617313385 Accuracy 0.8259999752044678\n",
      "Iteration 35790 Training loss 0.05897983908653259 Validation loss 0.05823688581585884 Accuracy 0.862500011920929\n",
      "Iteration 35800 Training loss 0.0712408795952797 Validation loss 0.062220267951488495 Accuracy 0.8500000238418579\n",
      "Iteration 35810 Training loss 0.057122524827718735 Validation loss 0.05930258333683014 Accuracy 0.8569999933242798\n",
      "Iteration 35820 Training loss 0.08468398451805115 Validation loss 0.08496230095624924 Accuracy 0.8029999732971191\n",
      "Iteration 35830 Training loss 0.07515020668506622 Validation loss 0.07166767865419388 Accuracy 0.8305000066757202\n",
      "Iteration 35840 Training loss 0.07030154764652252 Validation loss 0.06979674845933914 Accuracy 0.8339999914169312\n",
      "Iteration 35850 Training loss 0.06383593380451202 Validation loss 0.06108352541923523 Accuracy 0.8479999899864197\n",
      "Iteration 35860 Training loss 0.06877566874027252 Validation loss 0.0667090117931366 Accuracy 0.8399999737739563\n",
      "Iteration 35870 Training loss 0.07327967882156372 Validation loss 0.07405548542737961 Accuracy 0.8274999856948853\n",
      "Iteration 35880 Training loss 0.07803823053836823 Validation loss 0.07620485126972198 Accuracy 0.8215000033378601\n",
      "Iteration 35890 Training loss 0.0728083997964859 Validation loss 0.07353714853525162 Accuracy 0.8270000219345093\n",
      "Iteration 35900 Training loss 0.07075373828411102 Validation loss 0.06603889167308807 Accuracy 0.843999981880188\n",
      "Iteration 35910 Training loss 0.07652264088392258 Validation loss 0.08036676049232483 Accuracy 0.8109999895095825\n",
      "Iteration 35920 Training loss 0.0713338777422905 Validation loss 0.07164189964532852 Accuracy 0.8314999938011169\n",
      "Iteration 35930 Training loss 0.06772824376821518 Validation loss 0.06768135726451874 Accuracy 0.8389999866485596\n",
      "Iteration 35940 Training loss 0.06932923942804337 Validation loss 0.07124355435371399 Accuracy 0.8330000042915344\n",
      "Iteration 35950 Training loss 0.07115434855222702 Validation loss 0.07037583738565445 Accuracy 0.8305000066757202\n",
      "Iteration 35960 Training loss 0.06756871938705444 Validation loss 0.06669019907712936 Accuracy 0.8399999737739563\n",
      "Iteration 35970 Training loss 0.07447326183319092 Validation loss 0.07439159601926804 Accuracy 0.828499972820282\n",
      "Iteration 35980 Training loss 0.08914727717638016 Validation loss 0.08453197777271271 Accuracy 0.8029999732971191\n",
      "Iteration 35990 Training loss 0.06495781987905502 Validation loss 0.0696447491645813 Accuracy 0.8349999785423279\n",
      "Iteration 36000 Training loss 0.07509656250476837 Validation loss 0.07060655206441879 Accuracy 0.8320000171661377\n",
      "Iteration 36010 Training loss 0.0713881403207779 Validation loss 0.06852322071790695 Accuracy 0.8395000100135803\n",
      "Iteration 36020 Training loss 0.08129756897687912 Validation loss 0.07919298857450485 Accuracy 0.8149999976158142\n",
      "Iteration 36030 Training loss 0.0784226804971695 Validation loss 0.06880490481853485 Accuracy 0.8385000228881836\n",
      "Iteration 36040 Training loss 0.07023666054010391 Validation loss 0.06241612136363983 Accuracy 0.8495000004768372\n",
      "Iteration 36050 Training loss 0.07345214486122131 Validation loss 0.06874649226665497 Accuracy 0.8385000228881836\n",
      "Iteration 36060 Training loss 0.07785922288894653 Validation loss 0.08021920174360275 Accuracy 0.8130000233650208\n",
      "Iteration 36070 Training loss 0.07315763831138611 Validation loss 0.07415644824504852 Accuracy 0.8255000114440918\n",
      "Iteration 36080 Training loss 0.06764146685600281 Validation loss 0.06205954775214195 Accuracy 0.8519999980926514\n",
      "Iteration 36090 Training loss 0.06875967234373093 Validation loss 0.06559120118618011 Accuracy 0.8460000157356262\n",
      "Iteration 36100 Training loss 0.0765998438000679 Validation loss 0.07439374178647995 Accuracy 0.8224999904632568\n",
      "Iteration 36110 Training loss 0.08830175548791885 Validation loss 0.09218534827232361 Accuracy 0.7860000133514404\n",
      "Iteration 36120 Training loss 0.06931211799383163 Validation loss 0.06532794237136841 Accuracy 0.8445000052452087\n",
      "Iteration 36130 Training loss 0.06391593813896179 Validation loss 0.06070326641201973 Accuracy 0.8525000214576721\n",
      "Iteration 36140 Training loss 0.09222386032342911 Validation loss 0.07320673763751984 Accuracy 0.8274999856948853\n",
      "Iteration 36150 Training loss 0.08514822274446487 Validation loss 0.07053627073764801 Accuracy 0.8314999938011169\n",
      "Iteration 36160 Training loss 0.06859476864337921 Validation loss 0.0601242296397686 Accuracy 0.8585000038146973\n",
      "Iteration 36170 Training loss 0.07059714198112488 Validation loss 0.060137923806905746 Accuracy 0.8575000166893005\n",
      "Iteration 36180 Training loss 0.07717974483966827 Validation loss 0.0657636746764183 Accuracy 0.8475000262260437\n",
      "Iteration 36190 Training loss 0.06776147335767746 Validation loss 0.06006093695759773 Accuracy 0.8544999957084656\n",
      "Iteration 36200 Training loss 0.07872098684310913 Validation loss 0.06555113196372986 Accuracy 0.8424999713897705\n",
      "Iteration 36210 Training loss 0.06702020019292831 Validation loss 0.05972471460700035 Accuracy 0.8539999723434448\n",
      "Iteration 36220 Training loss 0.09539678692817688 Validation loss 0.0868375152349472 Accuracy 0.796999990940094\n",
      "Iteration 36230 Training loss 0.08225969970226288 Validation loss 0.08366785198450089 Accuracy 0.8059999942779541\n",
      "Iteration 36240 Training loss 0.07484892755746841 Validation loss 0.0732128843665123 Accuracy 0.8274999856948853\n",
      "Iteration 36250 Training loss 0.07589136064052582 Validation loss 0.07099269330501556 Accuracy 0.8345000147819519\n",
      "Iteration 36260 Training loss 0.06472167372703552 Validation loss 0.05963297188282013 Accuracy 0.8519999980926514\n",
      "Iteration 36270 Training loss 0.07928389310836792 Validation loss 0.06440737843513489 Accuracy 0.8454999923706055\n",
      "Iteration 36280 Training loss 0.07841063290834427 Validation loss 0.06562637537717819 Accuracy 0.8475000262260437\n",
      "Iteration 36290 Training loss 0.07437869906425476 Validation loss 0.061505965888500214 Accuracy 0.8529999852180481\n",
      "Iteration 36300 Training loss 0.07386428117752075 Validation loss 0.061429087072610855 Accuracy 0.8539999723434448\n",
      "Iteration 36310 Training loss 0.08442198485136032 Validation loss 0.06533083319664001 Accuracy 0.8429999947547913\n",
      "Iteration 36320 Training loss 0.07718174159526825 Validation loss 0.06048602983355522 Accuracy 0.8569999933242798\n",
      "Iteration 36330 Training loss 0.07824951410293579 Validation loss 0.06285075098276138 Accuracy 0.8504999876022339\n",
      "Iteration 36340 Training loss 0.06661286950111389 Validation loss 0.06018533930182457 Accuracy 0.8575000166893005\n",
      "Iteration 36350 Training loss 0.07076510787010193 Validation loss 0.06261716037988663 Accuracy 0.8544999957084656\n",
      "Iteration 36360 Training loss 0.07451047003269196 Validation loss 0.0642055943608284 Accuracy 0.8479999899864197\n",
      "Iteration 36370 Training loss 0.08968575298786163 Validation loss 0.06920517235994339 Accuracy 0.8389999866485596\n",
      "Iteration 36380 Training loss 0.07801870256662369 Validation loss 0.06215672940015793 Accuracy 0.8500000238418579\n",
      "Iteration 36390 Training loss 0.07511743158102036 Validation loss 0.06400088965892792 Accuracy 0.8485000133514404\n",
      "Iteration 36400 Training loss 0.0705462396144867 Validation loss 0.060367610305547714 Accuracy 0.8565000295639038\n",
      "Iteration 36410 Training loss 0.07146592438220978 Validation loss 0.06201301887631416 Accuracy 0.8510000109672546\n",
      "Iteration 36420 Training loss 0.07398112118244171 Validation loss 0.05868777260184288 Accuracy 0.8610000014305115\n",
      "Iteration 36430 Training loss 0.07200124859809875 Validation loss 0.06182393059134483 Accuracy 0.8525000214576721\n",
      "Iteration 36440 Training loss 0.06558185815811157 Validation loss 0.05965292826294899 Accuracy 0.859499990940094\n",
      "Iteration 36450 Training loss 0.06793206930160522 Validation loss 0.06028231978416443 Accuracy 0.8500000238418579\n",
      "Iteration 36460 Training loss 0.07999227941036224 Validation loss 0.0654599592089653 Accuracy 0.8454999923706055\n",
      "Iteration 36470 Training loss 0.07665035128593445 Validation loss 0.06342969834804535 Accuracy 0.8504999876022339\n",
      "Iteration 36480 Training loss 0.08548197150230408 Validation loss 0.06932780891656876 Accuracy 0.8389999866485596\n",
      "Iteration 36490 Training loss 0.06432831287384033 Validation loss 0.060075461864471436 Accuracy 0.8565000295639038\n",
      "Iteration 36500 Training loss 0.07058623433113098 Validation loss 0.07137174159288406 Accuracy 0.8314999938011169\n",
      "Iteration 36510 Training loss 0.07066486030817032 Validation loss 0.06459443271160126 Accuracy 0.8495000004768372\n",
      "Iteration 36520 Training loss 0.07124906033277512 Validation loss 0.07181941717863083 Accuracy 0.8324999809265137\n",
      "Iteration 36530 Training loss 0.07293792068958282 Validation loss 0.07074744254350662 Accuracy 0.8379999995231628\n",
      "Iteration 36540 Training loss 0.08910040557384491 Validation loss 0.089352086186409 Accuracy 0.796500027179718\n",
      "Iteration 36550 Training loss 0.07054638862609863 Validation loss 0.07029509544372559 Accuracy 0.8360000252723694\n",
      "Iteration 36560 Training loss 0.0755038857460022 Validation loss 0.07886134088039398 Accuracy 0.8174999952316284\n",
      "Iteration 36570 Training loss 0.07530749589204788 Validation loss 0.07334145903587341 Accuracy 0.8274999856948853\n",
      "Iteration 36580 Training loss 0.07703027874231339 Validation loss 0.07846095412969589 Accuracy 0.8184999823570251\n",
      "Iteration 36590 Training loss 0.0754527747631073 Validation loss 0.07820481806993484 Accuracy 0.8205000162124634\n",
      "Iteration 36600 Training loss 0.0736410990357399 Validation loss 0.06261632591485977 Accuracy 0.8504999876022339\n",
      "Iteration 36610 Training loss 0.08191850781440735 Validation loss 0.07956335693597794 Accuracy 0.8149999976158142\n",
      "Iteration 36620 Training loss 0.08370427787303925 Validation loss 0.08221535384654999 Accuracy 0.8115000128746033\n",
      "Iteration 36630 Training loss 0.07432590425014496 Validation loss 0.07394038885831833 Accuracy 0.8264999985694885\n",
      "Iteration 36640 Training loss 0.06354806572198868 Validation loss 0.060004089027643204 Accuracy 0.8569999933242798\n",
      "Iteration 36650 Training loss 0.06968434154987335 Validation loss 0.061965759843587875 Accuracy 0.8529999852180481\n",
      "Iteration 36660 Training loss 0.0733119323849678 Validation loss 0.06431766599416733 Accuracy 0.8485000133514404\n",
      "Iteration 36670 Training loss 0.07115094363689423 Validation loss 0.0635395497083664 Accuracy 0.8495000004768372\n",
      "Iteration 36680 Training loss 0.07284548133611679 Validation loss 0.06195157766342163 Accuracy 0.8539999723434448\n",
      "Iteration 36690 Training loss 0.07721462100744247 Validation loss 0.06144693121314049 Accuracy 0.8519999980926514\n",
      "Iteration 36700 Training loss 0.07965049147605896 Validation loss 0.06697461754083633 Accuracy 0.8424999713897705\n",
      "Iteration 36710 Training loss 0.08180306106805801 Validation loss 0.069109708070755 Accuracy 0.8355000019073486\n",
      "Iteration 36720 Training loss 0.06974073499441147 Validation loss 0.06109801307320595 Accuracy 0.8554999828338623\n",
      "Iteration 36730 Training loss 0.07973670959472656 Validation loss 0.0688788965344429 Accuracy 0.8389999866485596\n",
      "Iteration 36740 Training loss 0.07702958583831787 Validation loss 0.06381537765264511 Accuracy 0.8485000133514404\n",
      "Iteration 36750 Training loss 0.0700962096452713 Validation loss 0.06395521759986877 Accuracy 0.8485000133514404\n",
      "Iteration 36760 Training loss 0.07543338090181351 Validation loss 0.06345392763614655 Accuracy 0.8514999747276306\n",
      "Iteration 36770 Training loss 0.0693315714597702 Validation loss 0.06399595737457275 Accuracy 0.8495000004768372\n",
      "Iteration 36780 Training loss 0.08406378328800201 Validation loss 0.06651168316602707 Accuracy 0.8475000262260437\n",
      "Iteration 36790 Training loss 0.07300018519163132 Validation loss 0.06254294514656067 Accuracy 0.8514999747276306\n",
      "Iteration 36800 Training loss 0.07535513490438461 Validation loss 0.06499632447957993 Accuracy 0.847000002861023\n",
      "Iteration 36810 Training loss 0.07068946957588196 Validation loss 0.060456641018390656 Accuracy 0.8550000190734863\n",
      "Iteration 36820 Training loss 0.06438668817281723 Validation loss 0.060234520584344864 Accuracy 0.8569999933242798\n",
      "Iteration 36830 Training loss 0.07343676686286926 Validation loss 0.06484176963567734 Accuracy 0.8485000133514404\n",
      "Iteration 36840 Training loss 0.06945528835058212 Validation loss 0.06112373247742653 Accuracy 0.8569999933242798\n",
      "Iteration 36850 Training loss 0.08342070132493973 Validation loss 0.06879033148288727 Accuracy 0.8374999761581421\n",
      "Iteration 36860 Training loss 0.0838712751865387 Validation loss 0.06662818044424057 Accuracy 0.843500018119812\n",
      "Iteration 36870 Training loss 0.0683174580335617 Validation loss 0.06122284010052681 Accuracy 0.8525000214576721\n",
      "Iteration 36880 Training loss 0.06885069608688354 Validation loss 0.06287404894828796 Accuracy 0.8544999957084656\n",
      "Iteration 36890 Training loss 0.08490999042987823 Validation loss 0.0718022882938385 Accuracy 0.8330000042915344\n",
      "Iteration 36900 Training loss 0.08199892193078995 Validation loss 0.06905560195446014 Accuracy 0.8410000205039978\n",
      "Iteration 36910 Training loss 0.07858619838953018 Validation loss 0.06486254185438156 Accuracy 0.8485000133514404\n",
      "Iteration 36920 Training loss 0.07938540726900101 Validation loss 0.06806781888008118 Accuracy 0.8379999995231628\n",
      "Iteration 36930 Training loss 0.07723892480134964 Validation loss 0.06414677202701569 Accuracy 0.8510000109672546\n",
      "Iteration 36940 Training loss 0.07546862959861755 Validation loss 0.06555333733558655 Accuracy 0.8475000262260437\n",
      "Iteration 36950 Training loss 0.07449084520339966 Validation loss 0.0620742030441761 Accuracy 0.8539999723434448\n",
      "Iteration 36960 Training loss 0.07617770880460739 Validation loss 0.07566814869642258 Accuracy 0.8224999904632568\n",
      "Iteration 36970 Training loss 0.07159681618213654 Validation loss 0.06492297351360321 Accuracy 0.8489999771118164\n",
      "Iteration 36980 Training loss 0.07094204425811768 Validation loss 0.07087083160877228 Accuracy 0.8345000147819519\n",
      "Iteration 36990 Training loss 0.083046093583107 Validation loss 0.07782015949487686 Accuracy 0.8199999928474426\n",
      "Iteration 37000 Training loss 0.07132250815629959 Validation loss 0.06571095436811447 Accuracy 0.8464999794960022\n",
      "Iteration 37010 Training loss 0.07401266694068909 Validation loss 0.07075794786214828 Accuracy 0.8370000123977661\n",
      "Iteration 37020 Training loss 0.08426533639431 Validation loss 0.07984573394060135 Accuracy 0.8144999742507935\n",
      "Iteration 37030 Training loss 0.07710471749305725 Validation loss 0.07438632100820541 Accuracy 0.8274999856948853\n",
      "Iteration 37040 Training loss 0.08244730532169342 Validation loss 0.07465939223766327 Accuracy 0.828000009059906\n",
      "Iteration 37050 Training loss 0.0633653923869133 Validation loss 0.06149286404252052 Accuracy 0.8525000214576721\n",
      "Iteration 37060 Training loss 0.07525720447301865 Validation loss 0.07052737474441528 Accuracy 0.8360000252723694\n",
      "Iteration 37070 Training loss 0.07997867465019226 Validation loss 0.07804840058088303 Accuracy 0.8195000290870667\n",
      "Iteration 37080 Training loss 0.07629488408565521 Validation loss 0.07283058017492294 Accuracy 0.8299999833106995\n",
      "Iteration 37090 Training loss 0.07454832643270493 Validation loss 0.06968487799167633 Accuracy 0.8385000228881836\n",
      "Iteration 37100 Training loss 0.08829767256975174 Validation loss 0.08342870324850082 Accuracy 0.8065000176429749\n",
      "Iteration 37110 Training loss 0.07338528335094452 Validation loss 0.06852138787508011 Accuracy 0.8395000100135803\n",
      "Iteration 37120 Training loss 0.06997089833021164 Validation loss 0.06111952289938927 Accuracy 0.8539999723434448\n",
      "Iteration 37130 Training loss 0.08094999939203262 Validation loss 0.07986912876367569 Accuracy 0.8144999742507935\n",
      "Iteration 37140 Training loss 0.07339778542518616 Validation loss 0.06741221249103546 Accuracy 0.8445000052452087\n",
      "Iteration 37150 Training loss 0.07346170395612717 Validation loss 0.06989367306232452 Accuracy 0.8355000019073486\n",
      "Iteration 37160 Training loss 0.08000805974006653 Validation loss 0.07802911847829819 Accuracy 0.8190000057220459\n",
      "Iteration 37170 Training loss 0.07124935835599899 Validation loss 0.06479960680007935 Accuracy 0.8450000286102295\n",
      "Iteration 37180 Training loss 0.08253481984138489 Validation loss 0.07584131509065628 Accuracy 0.8224999904632568\n",
      "Iteration 37190 Training loss 0.07995057851076126 Validation loss 0.07475502043962479 Accuracy 0.8245000243186951\n",
      "Iteration 37200 Training loss 0.06920475512742996 Validation loss 0.06505753099918365 Accuracy 0.8500000238418579\n",
      "Iteration 37210 Training loss 0.0703282281756401 Validation loss 0.06524042040109634 Accuracy 0.8504999876022339\n",
      "Iteration 37220 Training loss 0.07947302609682083 Validation loss 0.08221971243619919 Accuracy 0.8084999918937683\n",
      "Iteration 37230 Training loss 0.07747342437505722 Validation loss 0.07685008645057678 Accuracy 0.8224999904632568\n",
      "Iteration 37240 Training loss 0.07533898949623108 Validation loss 0.07506081461906433 Accuracy 0.8230000138282776\n",
      "Iteration 37250 Training loss 0.08784684538841248 Validation loss 0.08324595540761948 Accuracy 0.8105000257492065\n",
      "Iteration 37260 Training loss 0.07081234455108643 Validation loss 0.06982076168060303 Accuracy 0.8385000228881836\n",
      "Iteration 37270 Training loss 0.08217253535985947 Validation loss 0.07460447400808334 Accuracy 0.8259999752044678\n",
      "Iteration 37280 Training loss 0.06810168921947479 Validation loss 0.06415370106697083 Accuracy 0.8479999899864197\n",
      "Iteration 37290 Training loss 0.07175202667713165 Validation loss 0.0613834373652935 Accuracy 0.8565000295639038\n",
      "Iteration 37300 Training loss 0.08974867314100266 Validation loss 0.0717693567276001 Accuracy 0.8330000042915344\n",
      "Iteration 37310 Training loss 0.07931975275278091 Validation loss 0.06407437473535538 Accuracy 0.8510000109672546\n",
      "Iteration 37320 Training loss 0.0845407098531723 Validation loss 0.06747239828109741 Accuracy 0.8429999947547913\n",
      "Iteration 37330 Training loss 0.07262295484542847 Validation loss 0.06461892277002335 Accuracy 0.8489999771118164\n",
      "Iteration 37340 Training loss 0.06886055320501328 Validation loss 0.0611848421394825 Accuracy 0.8525000214576721\n",
      "Iteration 37350 Training loss 0.07975798100233078 Validation loss 0.0676821619272232 Accuracy 0.8410000205039978\n",
      "Iteration 37360 Training loss 0.08260352164506912 Validation loss 0.06780063360929489 Accuracy 0.8399999737739563\n",
      "Iteration 37370 Training loss 0.08514045923948288 Validation loss 0.07377810031175613 Accuracy 0.8295000195503235\n",
      "Iteration 37380 Training loss 0.0773131474852562 Validation loss 0.06444898992776871 Accuracy 0.8504999876022339\n",
      "Iteration 37390 Training loss 0.07755311578512192 Validation loss 0.0644650086760521 Accuracy 0.8514999747276306\n",
      "Iteration 37400 Training loss 0.09105928987264633 Validation loss 0.07322262227535248 Accuracy 0.8309999704360962\n",
      "Iteration 37410 Training loss 0.06805868446826935 Validation loss 0.061418768018484116 Accuracy 0.8585000038146973\n",
      "Iteration 37420 Training loss 0.07577559351921082 Validation loss 0.0651775524020195 Accuracy 0.8514999747276306\n",
      "Iteration 37430 Training loss 0.07165872305631638 Validation loss 0.06403480470180511 Accuracy 0.8500000238418579\n",
      "Iteration 37440 Training loss 0.07719599455595016 Validation loss 0.06606607884168625 Accuracy 0.8475000262260437\n",
      "Iteration 37450 Training loss 0.07488458603620529 Validation loss 0.06956280767917633 Accuracy 0.8389999866485596\n",
      "Iteration 37460 Training loss 0.08332466334104538 Validation loss 0.06666392832994461 Accuracy 0.8464999794960022\n",
      "Iteration 37470 Training loss 0.0660901665687561 Validation loss 0.06371620297431946 Accuracy 0.8504999876022339\n",
      "Iteration 37480 Training loss 0.06848028302192688 Validation loss 0.061727043241262436 Accuracy 0.8504999876022339\n",
      "Iteration 37490 Training loss 0.0904330462217331 Validation loss 0.09278754144906998 Accuracy 0.7904999852180481\n",
      "Iteration 37500 Training loss 0.07853575050830841 Validation loss 0.08079656958580017 Accuracy 0.8140000104904175\n",
      "Iteration 37510 Training loss 0.08306826651096344 Validation loss 0.08593708276748657 Accuracy 0.8044999837875366\n",
      "Iteration 37520 Training loss 0.06799343973398209 Validation loss 0.06954234838485718 Accuracy 0.8389999866485596\n",
      "Iteration 37530 Training loss 0.07104355096817017 Validation loss 0.06850944459438324 Accuracy 0.8429999947547913\n",
      "Iteration 37540 Training loss 0.08602674305438995 Validation loss 0.0844888985157013 Accuracy 0.8080000281333923\n",
      "Iteration 37550 Training loss 0.07961475849151611 Validation loss 0.07452120631933212 Accuracy 0.8255000114440918\n",
      "Iteration 37560 Training loss 0.08320233970880508 Validation loss 0.09028761088848114 Accuracy 0.7979999780654907\n",
      "Iteration 37570 Training loss 0.071673184633255 Validation loss 0.06570637226104736 Accuracy 0.8489999771118164\n",
      "Iteration 37580 Training loss 0.08730810135602951 Validation loss 0.08657609671354294 Accuracy 0.8019999861717224\n",
      "Iteration 37590 Training loss 0.0693523958325386 Validation loss 0.07002906501293182 Accuracy 0.8370000123977661\n",
      "Iteration 37600 Training loss 0.07685071974992752 Validation loss 0.06948985159397125 Accuracy 0.8420000076293945\n",
      "Iteration 37610 Training loss 0.07627980411052704 Validation loss 0.07150944322347641 Accuracy 0.8345000147819519\n",
      "Iteration 37620 Training loss 0.07700950652360916 Validation loss 0.07665155082941055 Accuracy 0.8220000267028809\n",
      "Iteration 37630 Training loss 0.08188258111476898 Validation loss 0.07948510348796844 Accuracy 0.8144999742507935\n",
      "Iteration 37640 Training loss 0.07789639383554459 Validation loss 0.07759500294923782 Accuracy 0.8215000033378601\n",
      "Iteration 37650 Training loss 0.08078532665967941 Validation loss 0.07941871881484985 Accuracy 0.8180000185966492\n",
      "Iteration 37660 Training loss 0.07818158715963364 Validation loss 0.07165631651878357 Accuracy 0.8355000019073486\n",
      "Iteration 37670 Training loss 0.07708390802145004 Validation loss 0.07831388711929321 Accuracy 0.8174999952316284\n",
      "Iteration 37680 Training loss 0.06896765530109406 Validation loss 0.06414709985256195 Accuracy 0.8460000157356262\n",
      "Iteration 37690 Training loss 0.08395324647426605 Validation loss 0.08096233010292053 Accuracy 0.8134999871253967\n",
      "Iteration 37700 Training loss 0.07879667729139328 Validation loss 0.07446717470884323 Accuracy 0.824999988079071\n",
      "Iteration 37710 Training loss 0.08037811517715454 Validation loss 0.07749362289905548 Accuracy 0.8190000057220459\n",
      "Iteration 37720 Training loss 0.0673469752073288 Validation loss 0.0657513216137886 Accuracy 0.8485000133514404\n",
      "Iteration 37730 Training loss 0.08439750969409943 Validation loss 0.07138705253601074 Accuracy 0.8370000123977661\n",
      "Iteration 37740 Training loss 0.08075955510139465 Validation loss 0.07217078655958176 Accuracy 0.8330000042915344\n",
      "Iteration 37750 Training loss 0.08262605220079422 Validation loss 0.06429143995046616 Accuracy 0.8525000214576721\n",
      "Iteration 37760 Training loss 0.0775533989071846 Validation loss 0.06920066475868225 Accuracy 0.8379999995231628\n",
      "Iteration 37770 Training loss 0.08429324626922607 Validation loss 0.07075193524360657 Accuracy 0.8385000228881836\n",
      "Iteration 37780 Training loss 0.07910173386335373 Validation loss 0.06713804602622986 Accuracy 0.843999981880188\n",
      "Iteration 37790 Training loss 0.08333311229944229 Validation loss 0.0731685534119606 Accuracy 0.8335000276565552\n",
      "Iteration 37800 Training loss 0.07676609605550766 Validation loss 0.06773919612169266 Accuracy 0.843999981880188\n",
      "Iteration 37810 Training loss 0.07210522890090942 Validation loss 0.061612967401742935 Accuracy 0.8575000166893005\n",
      "Iteration 37820 Training loss 0.07131806761026382 Validation loss 0.06356781721115112 Accuracy 0.8539999723434448\n",
      "Iteration 37830 Training loss 0.0817246213555336 Validation loss 0.07110246270895004 Accuracy 0.8370000123977661\n",
      "Iteration 37840 Training loss 0.07156140357255936 Validation loss 0.06355398148298264 Accuracy 0.8525000214576721\n",
      "Iteration 37850 Training loss 0.08954815566539764 Validation loss 0.07082311809062958 Accuracy 0.8389999866485596\n",
      "Iteration 37860 Training loss 0.07652631402015686 Validation loss 0.06532417237758636 Accuracy 0.8500000238418579\n",
      "Iteration 37870 Training loss 0.07857487350702286 Validation loss 0.06582826375961304 Accuracy 0.8495000004768372\n",
      "Iteration 37880 Training loss 0.09463396668434143 Validation loss 0.07628312706947327 Accuracy 0.8270000219345093\n",
      "Iteration 37890 Training loss 0.06771282851696014 Validation loss 0.06343664228916168 Accuracy 0.8550000190734863\n",
      "Iteration 37900 Training loss 0.08217430114746094 Validation loss 0.07125295698642731 Accuracy 0.8379999995231628\n",
      "Iteration 37910 Training loss 0.07788433879613876 Validation loss 0.06676031649112701 Accuracy 0.847000002861023\n",
      "Iteration 37920 Training loss 0.07130537182092667 Validation loss 0.06361314654350281 Accuracy 0.8510000109672546\n",
      "Iteration 37930 Training loss 0.07658505439758301 Validation loss 0.06492751836776733 Accuracy 0.8500000238418579\n",
      "Iteration 37940 Training loss 0.07753223925828934 Validation loss 0.06681740283966064 Accuracy 0.847000002861023\n",
      "Iteration 37950 Training loss 0.07992832362651825 Validation loss 0.06604950875043869 Accuracy 0.8475000262260437\n",
      "Iteration 37960 Training loss 0.08344237506389618 Validation loss 0.07179056107997894 Accuracy 0.8355000019073486\n",
      "Iteration 37970 Training loss 0.07721392810344696 Validation loss 0.06649548560380936 Accuracy 0.8489999771118164\n",
      "Iteration 37980 Training loss 0.07568679004907608 Validation loss 0.06533987820148468 Accuracy 0.8489999771118164\n",
      "Iteration 37990 Training loss 0.07066783308982849 Validation loss 0.0667385384440422 Accuracy 0.8460000157356262\n",
      "Iteration 38000 Training loss 0.06752148270606995 Validation loss 0.06588470190763474 Accuracy 0.8464999794960022\n",
      "Iteration 38010 Training loss 0.09140269458293915 Validation loss 0.07727295160293579 Accuracy 0.8259999752044678\n",
      "Iteration 38020 Training loss 0.07620645314455032 Validation loss 0.0684182196855545 Accuracy 0.8450000286102295\n",
      "Iteration 38030 Training loss 0.07778429985046387 Validation loss 0.06661823391914368 Accuracy 0.8479999899864197\n",
      "Iteration 38040 Training loss 0.09096688032150269 Validation loss 0.07938235998153687 Accuracy 0.8220000267028809\n",
      "Iteration 38050 Training loss 0.0773644894361496 Validation loss 0.06924287229776382 Accuracy 0.8424999713897705\n",
      "Iteration 38060 Training loss 0.07297523319721222 Validation loss 0.0626092255115509 Accuracy 0.8550000190734863\n",
      "Iteration 38070 Training loss 0.07860570400953293 Validation loss 0.07237505912780762 Accuracy 0.8345000147819519\n",
      "Iteration 38080 Training loss 0.0749095007777214 Validation loss 0.0674189105629921 Accuracy 0.8475000262260437\n",
      "Iteration 38090 Training loss 0.07337541878223419 Validation loss 0.06787010282278061 Accuracy 0.843999981880188\n",
      "Iteration 38100 Training loss 0.07432392984628677 Validation loss 0.06515222042798996 Accuracy 0.8479999899864197\n",
      "Iteration 38110 Training loss 0.08980230987071991 Validation loss 0.07807764410972595 Accuracy 0.8240000009536743\n",
      "Iteration 38120 Training loss 0.07700179517269135 Validation loss 0.06477494537830353 Accuracy 0.8489999771118164\n",
      "Iteration 38130 Training loss 0.07694194465875626 Validation loss 0.06723018735647202 Accuracy 0.8475000262260437\n",
      "Iteration 38140 Training loss 0.07308971881866455 Validation loss 0.06443766504526138 Accuracy 0.8500000238418579\n",
      "Iteration 38150 Training loss 0.08871635794639587 Validation loss 0.07404181361198425 Accuracy 0.8345000147819519\n",
      "Iteration 38160 Training loss 0.08676007390022278 Validation loss 0.0702400729060173 Accuracy 0.8399999737739563\n",
      "Iteration 38170 Training loss 0.07480642199516296 Validation loss 0.06495711952447891 Accuracy 0.8495000004768372\n",
      "Iteration 38180 Training loss 0.09113264828920364 Validation loss 0.07981173694133759 Accuracy 0.8199999928474426\n",
      "Iteration 38190 Training loss 0.0747494027018547 Validation loss 0.06341281533241272 Accuracy 0.8550000190734863\n",
      "Iteration 38200 Training loss 0.07446819543838501 Validation loss 0.07001719623804092 Accuracy 0.8420000076293945\n",
      "Iteration 38210 Training loss 0.07875769585371017 Validation loss 0.06897220015525818 Accuracy 0.840499997138977\n",
      "Iteration 38220 Training loss 0.08318110555410385 Validation loss 0.06792654097080231 Accuracy 0.8450000286102295\n",
      "Iteration 38230 Training loss 0.07307884842157364 Validation loss 0.06731351464986801 Accuracy 0.8460000157356262\n",
      "Iteration 38240 Training loss 0.08448745310306549 Validation loss 0.07145576179027557 Accuracy 0.8370000123977661\n",
      "Iteration 38250 Training loss 0.08196462690830231 Validation loss 0.07067564874887466 Accuracy 0.8399999737739563\n",
      "Iteration 38260 Training loss 0.07321354001760483 Validation loss 0.06438044458627701 Accuracy 0.8554999828338623\n",
      "Iteration 38270 Training loss 0.07992777228355408 Validation loss 0.06805289536714554 Accuracy 0.8450000286102295\n",
      "Iteration 38280 Training loss 0.08248855918645859 Validation loss 0.07171188294887543 Accuracy 0.8345000147819519\n",
      "Iteration 38290 Training loss 0.07250051945447922 Validation loss 0.06444961577653885 Accuracy 0.8519999980926514\n",
      "Iteration 38300 Training loss 0.06609799712896347 Validation loss 0.06268081068992615 Accuracy 0.8569999933242798\n",
      "Iteration 38310 Training loss 0.09094326198101044 Validation loss 0.088250532746315 Accuracy 0.8019999861717224\n",
      "Iteration 38320 Training loss 0.07023562490940094 Validation loss 0.06665056943893433 Accuracy 0.8450000286102295\n",
      "Iteration 38330 Training loss 0.08394169807434082 Validation loss 0.08200584352016449 Accuracy 0.8130000233650208\n",
      "Iteration 38340 Training loss 0.09439214318990707 Validation loss 0.08272261172533035 Accuracy 0.8134999871253967\n",
      "Iteration 38350 Training loss 0.07850906997919083 Validation loss 0.07024307548999786 Accuracy 0.8414999842643738\n",
      "Iteration 38360 Training loss 0.08081714808940887 Validation loss 0.07340855151414871 Accuracy 0.8320000171661377\n",
      "Iteration 38370 Training loss 0.08083224296569824 Validation loss 0.0799700915813446 Accuracy 0.815500020980835\n",
      "Iteration 38380 Training loss 0.07589171081781387 Validation loss 0.0722322091460228 Accuracy 0.8360000252723694\n",
      "Iteration 38390 Training loss 0.07054828107357025 Validation loss 0.06738586723804474 Accuracy 0.8454999923706055\n",
      "Iteration 38400 Training loss 0.08503842353820801 Validation loss 0.08187773823738098 Accuracy 0.815500020980835\n",
      "Iteration 38410 Training loss 0.07358364015817642 Validation loss 0.06898214668035507 Accuracy 0.8414999842643738\n",
      "Iteration 38420 Training loss 0.07444377988576889 Validation loss 0.06800952553749084 Accuracy 0.8454999923706055\n",
      "Iteration 38430 Training loss 0.08432640880346298 Validation loss 0.07661443203687668 Accuracy 0.8240000009536743\n",
      "Iteration 38440 Training loss 0.07584366202354431 Validation loss 0.06744249165058136 Accuracy 0.8450000286102295\n",
      "Iteration 38450 Training loss 0.07683993130922318 Validation loss 0.06795340031385422 Accuracy 0.8454999923706055\n",
      "Iteration 38460 Training loss 0.0805477574467659 Validation loss 0.07320734113454819 Accuracy 0.8345000147819519\n",
      "Iteration 38470 Training loss 0.08278891444206238 Validation loss 0.07720930129289627 Accuracy 0.824999988079071\n",
      "Iteration 38480 Training loss 0.08881689608097076 Validation loss 0.07903410494327545 Accuracy 0.8169999718666077\n",
      "Iteration 38490 Training loss 0.08797416090965271 Validation loss 0.07953554391860962 Accuracy 0.8184999823570251\n",
      "Iteration 38500 Training loss 0.06579317897558212 Validation loss 0.06491998583078384 Accuracy 0.8450000286102295\n",
      "Iteration 38510 Training loss 0.0842939019203186 Validation loss 0.07527583092451096 Accuracy 0.8320000171661377\n",
      "Iteration 38520 Training loss 0.07458122819662094 Validation loss 0.07016342878341675 Accuracy 0.8399999737739563\n",
      "Iteration 38530 Training loss 0.0856904536485672 Validation loss 0.07968144863843918 Accuracy 0.8215000033378601\n",
      "Iteration 38540 Training loss 0.07720563560724258 Validation loss 0.07111893594264984 Accuracy 0.8349999785423279\n",
      "Iteration 38550 Training loss 0.08846230059862137 Validation loss 0.07150819897651672 Accuracy 0.8389999866485596\n",
      "Iteration 38560 Training loss 0.08362482488155365 Validation loss 0.06996706128120422 Accuracy 0.840499997138977\n",
      "Iteration 38570 Training loss 0.07700660824775696 Validation loss 0.0656096413731575 Accuracy 0.8504999876022339\n",
      "Iteration 38580 Training loss 0.06258711218833923 Validation loss 0.06288202106952667 Accuracy 0.8575000166893005\n",
      "Iteration 38590 Training loss 0.09067323058843613 Validation loss 0.0780981257557869 Accuracy 0.8270000219345093\n",
      "Iteration 38600 Training loss 0.08355765789747238 Validation loss 0.07275955379009247 Accuracy 0.8349999785423279\n",
      "Iteration 38610 Training loss 0.08011642098426819 Validation loss 0.06884367763996124 Accuracy 0.843500018119812\n",
      "Iteration 38620 Training loss 0.07005395740270615 Validation loss 0.06699033826589584 Accuracy 0.847000002861023\n",
      "Iteration 38630 Training loss 0.0711163580417633 Validation loss 0.06425922363996506 Accuracy 0.8560000061988831\n",
      "Iteration 38640 Training loss 0.07178214192390442 Validation loss 0.06523797661066055 Accuracy 0.8529999852180481\n",
      "Iteration 38650 Training loss 0.09431402385234833 Validation loss 0.07942578196525574 Accuracy 0.8190000057220459\n",
      "Iteration 38660 Training loss 0.0788077563047409 Validation loss 0.06913711875677109 Accuracy 0.8454999923706055\n",
      "Iteration 38670 Training loss 0.0836443156003952 Validation loss 0.07126513868570328 Accuracy 0.8379999995231628\n",
      "Iteration 38680 Training loss 0.08755043894052505 Validation loss 0.0746074989438057 Accuracy 0.8314999938011169\n",
      "Iteration 38690 Training loss 0.08211297541856766 Validation loss 0.06752818077802658 Accuracy 0.8464999794960022\n",
      "Iteration 38700 Training loss 0.08127119392156601 Validation loss 0.0731518417596817 Accuracy 0.8335000276565552\n",
      "Iteration 38710 Training loss 0.08429460227489471 Validation loss 0.0689004510641098 Accuracy 0.843500018119812\n",
      "Iteration 38720 Training loss 0.07249884307384491 Validation loss 0.0664796456694603 Accuracy 0.8479999899864197\n",
      "Iteration 38730 Training loss 0.07990185916423798 Validation loss 0.06871211528778076 Accuracy 0.8460000157356262\n",
      "Iteration 38740 Training loss 0.0836431086063385 Validation loss 0.0734112560749054 Accuracy 0.8349999785423279\n",
      "Iteration 38750 Training loss 0.07490407675504684 Validation loss 0.068281389772892 Accuracy 0.8445000052452087\n",
      "Iteration 38760 Training loss 0.08638536185026169 Validation loss 0.07700707763433456 Accuracy 0.824999988079071\n",
      "Iteration 38770 Training loss 0.08133415877819061 Validation loss 0.07036947458982468 Accuracy 0.8420000076293945\n",
      "Iteration 38780 Training loss 0.0834265872836113 Validation loss 0.07241719216108322 Accuracy 0.8389999866485596\n",
      "Iteration 38790 Training loss 0.08226072788238525 Validation loss 0.07066940516233444 Accuracy 0.8395000100135803\n",
      "Iteration 38800 Training loss 0.07360953092575073 Validation loss 0.06986858695745468 Accuracy 0.8414999842643738\n",
      "Iteration 38810 Training loss 0.08082429319620132 Validation loss 0.07611335813999176 Accuracy 0.828000009059906\n",
      "Iteration 38820 Training loss 0.0855126827955246 Validation loss 0.07265622168779373 Accuracy 0.8395000100135803\n",
      "Iteration 38830 Training loss 0.07670930027961731 Validation loss 0.06635063141584396 Accuracy 0.8495000004768372\n",
      "Iteration 38840 Training loss 0.09155541658401489 Validation loss 0.08341895788908005 Accuracy 0.8134999871253967\n",
      "Iteration 38850 Training loss 0.08818917721509933 Validation loss 0.0778849795460701 Accuracy 0.8274999856948853\n",
      "Iteration 38860 Training loss 0.07478166371583939 Validation loss 0.0657489150762558 Accuracy 0.8514999747276306\n",
      "Iteration 38870 Training loss 0.08092723786830902 Validation loss 0.07076655328273773 Accuracy 0.8429999947547913\n",
      "Iteration 38880 Training loss 0.0942520871758461 Validation loss 0.09509263932704926 Accuracy 0.7870000004768372\n",
      "Iteration 38890 Training loss 0.08126315474510193 Validation loss 0.0740409567952156 Accuracy 0.8339999914169312\n",
      "Iteration 38900 Training loss 0.07728175073862076 Validation loss 0.06641138345003128 Accuracy 0.8454999923706055\n",
      "Iteration 38910 Training loss 0.093985415995121 Validation loss 0.08277197182178497 Accuracy 0.8144999742507935\n",
      "Iteration 38920 Training loss 0.07603270560503006 Validation loss 0.06755204498767853 Accuracy 0.8424999713897705\n",
      "Iteration 38930 Training loss 0.10072869807481766 Validation loss 0.09540419280529022 Accuracy 0.7860000133514404\n",
      "Iteration 38940 Training loss 0.08663886785507202 Validation loss 0.0725262388586998 Accuracy 0.8395000100135803\n",
      "Iteration 38950 Training loss 0.08196853846311569 Validation loss 0.07410341501235962 Accuracy 0.8324999809265137\n",
      "Iteration 38960 Training loss 0.07552147656679153 Validation loss 0.07398410886526108 Accuracy 0.8320000171661377\n",
      "Iteration 38970 Training loss 0.08815187215805054 Validation loss 0.08204203844070435 Accuracy 0.8195000290870667\n",
      "Iteration 38980 Training loss 0.076622374355793 Validation loss 0.07365971058607101 Accuracy 0.8339999914169312\n",
      "Iteration 38990 Training loss 0.07729745656251907 Validation loss 0.06979534775018692 Accuracy 0.847000002861023\n",
      "Iteration 39000 Training loss 0.07863299548625946 Validation loss 0.0720142126083374 Accuracy 0.8364999890327454\n",
      "Iteration 39010 Training loss 0.09099573642015457 Validation loss 0.08472610265016556 Accuracy 0.8125\n",
      "Iteration 39020 Training loss 0.09425415843725204 Validation loss 0.08786483108997345 Accuracy 0.8084999918937683\n",
      "Iteration 39030 Training loss 0.07387136667966843 Validation loss 0.06888606399297714 Accuracy 0.843999981880188\n",
      "Iteration 39040 Training loss 0.07443470507860184 Validation loss 0.06499763578176498 Accuracy 0.8525000214576721\n",
      "Iteration 39050 Training loss 0.08562179654836655 Validation loss 0.07564856112003326 Accuracy 0.8309999704360962\n",
      "Iteration 39060 Training loss 0.0953175500035286 Validation loss 0.08623510599136353 Accuracy 0.8044999837875366\n",
      "Iteration 39070 Training loss 0.07553347200155258 Validation loss 0.07071304321289062 Accuracy 0.8410000205039978\n",
      "Iteration 39080 Training loss 0.0717848390340805 Validation loss 0.0650678500533104 Accuracy 0.8550000190734863\n",
      "Iteration 39090 Training loss 0.10005217045545578 Validation loss 0.08269910514354706 Accuracy 0.8130000233650208\n",
      "Iteration 39100 Training loss 0.09032020717859268 Validation loss 0.07777556031942368 Accuracy 0.8230000138282776\n",
      "Iteration 39110 Training loss 0.0823427066206932 Validation loss 0.06935004889965057 Accuracy 0.8424999713897705\n",
      "Iteration 39120 Training loss 0.08550895005464554 Validation loss 0.073053739964962 Accuracy 0.8364999890327454\n",
      "Iteration 39130 Training loss 0.07551529258489609 Validation loss 0.06642434000968933 Accuracy 0.8500000238418579\n",
      "Iteration 39140 Training loss 0.08755230903625488 Validation loss 0.07297541946172714 Accuracy 0.8374999761581421\n",
      "Iteration 39150 Training loss 0.08561109751462936 Validation loss 0.07365269958972931 Accuracy 0.8360000252723694\n",
      "Iteration 39160 Training loss 0.07552225142717361 Validation loss 0.07065337896347046 Accuracy 0.8450000286102295\n",
      "Iteration 39170 Training loss 0.08557837456464767 Validation loss 0.06900849938392639 Accuracy 0.8429999947547913\n",
      "Iteration 39180 Training loss 0.0838426724076271 Validation loss 0.07293111085891724 Accuracy 0.8339999914169312\n",
      "Iteration 39190 Training loss 0.08583559095859528 Validation loss 0.07620719820261002 Accuracy 0.8295000195503235\n",
      "Iteration 39200 Training loss 0.08531554043292999 Validation loss 0.07232698053121567 Accuracy 0.8374999761581421\n",
      "Iteration 39210 Training loss 0.07683958113193512 Validation loss 0.06998387724161148 Accuracy 0.843500018119812\n",
      "Iteration 39220 Training loss 0.07219348102807999 Validation loss 0.06895434856414795 Accuracy 0.8424999713897705\n",
      "Iteration 39230 Training loss 0.07269760221242905 Validation loss 0.06512782722711563 Accuracy 0.8529999852180481\n",
      "Iteration 39240 Training loss 0.08093983680009842 Validation loss 0.07573986798524857 Accuracy 0.8330000042915344\n",
      "Iteration 39250 Training loss 0.08322896808385849 Validation loss 0.07796092331409454 Accuracy 0.8259999752044678\n",
      "Iteration 39260 Training loss 0.0835045874118805 Validation loss 0.07455930858850479 Accuracy 0.8349999785423279\n",
      "Iteration 39270 Training loss 0.09141932427883148 Validation loss 0.07784056663513184 Accuracy 0.828499972820282\n",
      "Iteration 39280 Training loss 0.08248837292194366 Validation loss 0.07621505111455917 Accuracy 0.8324999809265137\n",
      "Iteration 39290 Training loss 0.07776995003223419 Validation loss 0.07034462690353394 Accuracy 0.8399999737739563\n",
      "Iteration 39300 Training loss 0.08433903753757477 Validation loss 0.07834675908088684 Accuracy 0.8259999752044678\n",
      "Iteration 39310 Training loss 0.08648677915334702 Validation loss 0.07881982624530792 Accuracy 0.8270000219345093\n",
      "Iteration 39320 Training loss 0.07632233947515488 Validation loss 0.06555327028036118 Accuracy 0.8519999980926514\n",
      "Iteration 39330 Training loss 0.09251831471920013 Validation loss 0.0883973240852356 Accuracy 0.8015000224113464\n",
      "Iteration 39340 Training loss 0.08540438115596771 Validation loss 0.07892140001058578 Accuracy 0.8264999985694885\n",
      "Iteration 39350 Training loss 0.08483056724071503 Validation loss 0.07920833677053452 Accuracy 0.8274999856948853\n",
      "Iteration 39360 Training loss 0.08442728966474533 Validation loss 0.07993915677070618 Accuracy 0.8255000114440918\n",
      "Iteration 39370 Training loss 0.08891579508781433 Validation loss 0.08341603726148605 Accuracy 0.8144999742507935\n",
      "Iteration 39380 Training loss 0.07829108834266663 Validation loss 0.07124188542366028 Accuracy 0.8389999866485596\n",
      "Iteration 39390 Training loss 0.07334084063768387 Validation loss 0.06641265749931335 Accuracy 0.8454999923706055\n",
      "Iteration 39400 Training loss 0.08423005789518356 Validation loss 0.07765013724565506 Accuracy 0.8264999985694885\n",
      "Iteration 39410 Training loss 0.07770742475986481 Validation loss 0.06724012643098831 Accuracy 0.8450000286102295\n",
      "Iteration 39420 Training loss 0.08026726543903351 Validation loss 0.06617233902215958 Accuracy 0.8485000133514404\n",
      "Iteration 39430 Training loss 0.08789635449647903 Validation loss 0.07610339671373367 Accuracy 0.8320000171661377\n",
      "Iteration 39440 Training loss 0.07081837207078934 Validation loss 0.06715426594018936 Accuracy 0.8504999876022339\n",
      "Iteration 39450 Training loss 0.08776403218507767 Validation loss 0.08460808545351028 Accuracy 0.8115000128746033\n",
      "Iteration 39460 Training loss 0.09136632829904556 Validation loss 0.08658254891633987 Accuracy 0.8109999895095825\n",
      "Iteration 39470 Training loss 0.07987852394580841 Validation loss 0.07093743979930878 Accuracy 0.843999981880188\n",
      "Iteration 39480 Training loss 0.08105291426181793 Validation loss 0.07372240722179413 Accuracy 0.8374999761581421\n",
      "Iteration 39490 Training loss 0.08185935020446777 Validation loss 0.07010755687952042 Accuracy 0.8389999866485596\n",
      "Iteration 39500 Training loss 0.10024520754814148 Validation loss 0.08596833050251007 Accuracy 0.8105000257492065\n",
      "Iteration 39510 Training loss 0.08343763649463654 Validation loss 0.08291468024253845 Accuracy 0.8149999976158142\n",
      "Iteration 39520 Training loss 0.08119233697652817 Validation loss 0.07524316757917404 Accuracy 0.8339999914169312\n",
      "Iteration 39530 Training loss 0.09760961681604385 Validation loss 0.08746524155139923 Accuracy 0.8084999918937683\n",
      "Iteration 39540 Training loss 0.07380104064941406 Validation loss 0.06763027608394623 Accuracy 0.8479999899864197\n",
      "Iteration 39550 Training loss 0.08073288947343826 Validation loss 0.06987138092517853 Accuracy 0.8424999713897705\n",
      "Iteration 39560 Training loss 0.08214130252599716 Validation loss 0.0705762505531311 Accuracy 0.8429999947547913\n",
      "Iteration 39570 Training loss 0.073932945728302 Validation loss 0.06592059880495071 Accuracy 0.8550000190734863\n",
      "Iteration 39580 Training loss 0.07858891040086746 Validation loss 0.06648814678192139 Accuracy 0.8500000238418579\n",
      "Iteration 39590 Training loss 0.0922790989279747 Validation loss 0.08301223814487457 Accuracy 0.8125\n",
      "Iteration 39600 Training loss 0.09056296199560165 Validation loss 0.07405846565961838 Accuracy 0.8349999785423279\n",
      "Iteration 39610 Training loss 0.08415299654006958 Validation loss 0.06910574436187744 Accuracy 0.8454999923706055\n",
      "Iteration 39620 Training loss 0.08201009780168533 Validation loss 0.06941360980272293 Accuracy 0.843500018119812\n",
      "Iteration 39630 Training loss 0.08041751384735107 Validation loss 0.07071855664253235 Accuracy 0.843500018119812\n",
      "Iteration 39640 Training loss 0.08634339272975922 Validation loss 0.07240408658981323 Accuracy 0.840499997138977\n",
      "Iteration 39650 Training loss 0.07756001502275467 Validation loss 0.07315303385257721 Accuracy 0.8379999995231628\n",
      "Iteration 39660 Training loss 0.08145346492528915 Validation loss 0.07198052853345871 Accuracy 0.840499997138977\n",
      "Iteration 39670 Training loss 0.08365952223539352 Validation loss 0.07117629051208496 Accuracy 0.8399999737739563\n",
      "Iteration 39680 Training loss 0.08853522688150406 Validation loss 0.06709351390600204 Accuracy 0.8535000085830688\n",
      "Iteration 39690 Training loss 0.09150416404008865 Validation loss 0.08321801573038101 Accuracy 0.8134999871253967\n",
      "Iteration 39700 Training loss 0.0876573696732521 Validation loss 0.07812274247407913 Accuracy 0.8255000114440918\n",
      "Iteration 39710 Training loss 0.08968187123537064 Validation loss 0.07344120740890503 Accuracy 0.8410000205039978\n",
      "Iteration 39720 Training loss 0.08760946989059448 Validation loss 0.07296465337276459 Accuracy 0.8395000100135803\n",
      "Iteration 39730 Training loss 0.09799261391162872 Validation loss 0.07417868077754974 Accuracy 0.8385000228881836\n",
      "Iteration 39740 Training loss 0.09956732392311096 Validation loss 0.08496330678462982 Accuracy 0.8090000152587891\n",
      "Iteration 39750 Training loss 0.08116838335990906 Validation loss 0.0680653527379036 Accuracy 0.8485000133514404\n",
      "Iteration 39760 Training loss 0.07496724277734756 Validation loss 0.07029253989458084 Accuracy 0.8399999737739563\n",
      "Iteration 39770 Training loss 0.09618368744850159 Validation loss 0.08806414157152176 Accuracy 0.8090000152587891\n",
      "Iteration 39780 Training loss 0.09697325527667999 Validation loss 0.0994514748454094 Accuracy 0.7835000157356262\n",
      "Iteration 39790 Training loss 0.07990137487649918 Validation loss 0.07153347879648209 Accuracy 0.8389999866485596\n",
      "Iteration 39800 Training loss 0.09060543030500412 Validation loss 0.08466417342424393 Accuracy 0.8180000185966492\n",
      "Iteration 39810 Training loss 0.08095429837703705 Validation loss 0.06983144581317902 Accuracy 0.8420000076293945\n",
      "Iteration 39820 Training loss 0.08008608222007751 Validation loss 0.06915117055177689 Accuracy 0.8429999947547913\n",
      "Iteration 39830 Training loss 0.08358222246170044 Validation loss 0.07822230458259583 Accuracy 0.8274999856948853\n",
      "Iteration 39840 Training loss 0.08437301963567734 Validation loss 0.0691516175866127 Accuracy 0.8464999794960022\n",
      "Iteration 39850 Training loss 0.0892791748046875 Validation loss 0.07280217111110687 Accuracy 0.8414999842643738\n",
      "Iteration 39860 Training loss 0.08614417910575867 Validation loss 0.07552754878997803 Accuracy 0.8330000042915344\n",
      "Iteration 39870 Training loss 0.08524133265018463 Validation loss 0.0675840899348259 Accuracy 0.8464999794960022\n",
      "Iteration 39880 Training loss 0.09390400350093842 Validation loss 0.08319563418626785 Accuracy 0.8125\n",
      "Iteration 39890 Training loss 0.09131868183612823 Validation loss 0.07040683180093765 Accuracy 0.843500018119812\n",
      "Iteration 39900 Training loss 0.07880043983459473 Validation loss 0.06829851120710373 Accuracy 0.8485000133514404\n",
      "Iteration 39910 Training loss 0.08535736054182053 Validation loss 0.07116755098104477 Accuracy 0.8429999947547913\n",
      "Iteration 39920 Training loss 0.08859594911336899 Validation loss 0.07449980825185776 Accuracy 0.8339999914169312\n",
      "Iteration 39930 Training loss 0.0896296352148056 Validation loss 0.06869655102491379 Accuracy 0.8464999794960022\n",
      "Iteration 39940 Training loss 0.0888330414891243 Validation loss 0.07650282233953476 Accuracy 0.8305000066757202\n",
      "Iteration 39950 Training loss 0.07928073406219482 Validation loss 0.07044842094182968 Accuracy 0.8445000052452087\n",
      "Iteration 39960 Training loss 0.06931424140930176 Validation loss 0.067949578166008 Accuracy 0.8495000004768372\n",
      "Iteration 39970 Training loss 0.08880692720413208 Validation loss 0.07534902542829514 Accuracy 0.8364999890327454\n",
      "Iteration 39980 Training loss 0.08809906244277954 Validation loss 0.07516159117221832 Accuracy 0.8355000019073486\n",
      "Iteration 39990 Training loss 0.08423268049955368 Validation loss 0.06928002834320068 Accuracy 0.8450000286102295\n",
      "Iteration 40000 Training loss 0.07738696783781052 Validation loss 0.06940051168203354 Accuracy 0.8450000286102295\n",
      "Iteration 40010 Training loss 0.0830397680401802 Validation loss 0.07116848975419998 Accuracy 0.843999981880188\n",
      "Iteration 40020 Training loss 0.08776513487100601 Validation loss 0.07265176624059677 Accuracy 0.8399999737739563\n",
      "Iteration 40030 Training loss 0.0813586413860321 Validation loss 0.07194393128156662 Accuracy 0.8414999842643738\n",
      "Iteration 40040 Training loss 0.0723099634051323 Validation loss 0.06810230016708374 Accuracy 0.8475000262260437\n",
      "Iteration 40050 Training loss 0.08156174421310425 Validation loss 0.07627727836370468 Accuracy 0.8320000171661377\n",
      "Iteration 40060 Training loss 0.08086483925580978 Validation loss 0.07110819965600967 Accuracy 0.8420000076293945\n",
      "Iteration 40070 Training loss 0.08615431189537048 Validation loss 0.07531752437353134 Accuracy 0.8364999890327454\n",
      "Iteration 40080 Training loss 0.09936169534921646 Validation loss 0.07743360847234726 Accuracy 0.8289999961853027\n",
      "Iteration 40090 Training loss 0.09155306965112686 Validation loss 0.08216261863708496 Accuracy 0.8184999823570251\n",
      "Iteration 40100 Training loss 0.08227664977312088 Validation loss 0.06995794922113419 Accuracy 0.8450000286102295\n",
      "Iteration 40110 Training loss 0.08641267567873001 Validation loss 0.07079371064901352 Accuracy 0.843999981880188\n",
      "Iteration 40120 Training loss 0.09406761080026627 Validation loss 0.09111829102039337 Accuracy 0.8015000224113464\n",
      "Iteration 40130 Training loss 0.08695276081562042 Validation loss 0.08262751251459122 Accuracy 0.8230000138282776\n",
      "Iteration 40140 Training loss 0.09342040866613388 Validation loss 0.09220368415117264 Accuracy 0.7994999885559082\n",
      "Iteration 40150 Training loss 0.08456677198410034 Validation loss 0.08443467319011688 Accuracy 0.8184999823570251\n",
      "Iteration 40160 Training loss 0.08115241676568985 Validation loss 0.07249618321657181 Accuracy 0.8389999866485596\n",
      "Iteration 40170 Training loss 0.07899567484855652 Validation loss 0.0754978358745575 Accuracy 0.8370000123977661\n",
      "Iteration 40180 Training loss 0.08303020149469376 Validation loss 0.082318514585495 Accuracy 0.8215000033378601\n",
      "Iteration 40190 Training loss 0.07728119194507599 Validation loss 0.0710260272026062 Accuracy 0.8429999947547913\n",
      "Iteration 40200 Training loss 0.08954296261072159 Validation loss 0.08432791382074356 Accuracy 0.8165000081062317\n",
      "Iteration 40210 Training loss 0.09150227904319763 Validation loss 0.08851976692676544 Accuracy 0.809499979019165\n",
      "Iteration 40220 Training loss 0.09366656839847565 Validation loss 0.09139101952314377 Accuracy 0.8029999732971191\n",
      "Iteration 40230 Training loss 0.0814293697476387 Validation loss 0.06859510391950607 Accuracy 0.8519999980926514\n",
      "Iteration 40240 Training loss 0.09574420005083084 Validation loss 0.095316082239151 Accuracy 0.7944999933242798\n",
      "Iteration 40250 Training loss 0.08688338100910187 Validation loss 0.07616803050041199 Accuracy 0.8330000042915344\n",
      "Iteration 40260 Training loss 0.09482350945472717 Validation loss 0.0793486163020134 Accuracy 0.8259999752044678\n",
      "Iteration 40270 Training loss 0.0886729285120964 Validation loss 0.06867845356464386 Accuracy 0.8489999771118164\n",
      "Iteration 40280 Training loss 0.07861363142728806 Validation loss 0.07192450016736984 Accuracy 0.8420000076293945\n",
      "Iteration 40290 Training loss 0.09792542457580566 Validation loss 0.09737841784954071 Accuracy 0.7875000238418579\n",
      "Iteration 40300 Training loss 0.08481323719024658 Validation loss 0.07577893882989883 Accuracy 0.8339999914169312\n",
      "Iteration 40310 Training loss 0.08469230681657791 Validation loss 0.07288550585508347 Accuracy 0.8345000147819519\n",
      "Iteration 40320 Training loss 0.10381819307804108 Validation loss 0.10278628021478653 Accuracy 0.777999997138977\n",
      "Iteration 40330 Training loss 0.07814330607652664 Validation loss 0.06787317991256714 Accuracy 0.8479999899864197\n",
      "Iteration 40340 Training loss 0.08495575189590454 Validation loss 0.07080618292093277 Accuracy 0.8429999947547913\n",
      "Iteration 40350 Training loss 0.08641841262578964 Validation loss 0.07266318053007126 Accuracy 0.8395000100135803\n",
      "Iteration 40360 Training loss 0.09315991401672363 Validation loss 0.07816547900438309 Accuracy 0.8274999856948853\n",
      "Iteration 40370 Training loss 0.08404497802257538 Validation loss 0.07100533694028854 Accuracy 0.8445000052452087\n",
      "Iteration 40380 Training loss 0.07979203760623932 Validation loss 0.0702645480632782 Accuracy 0.8450000286102295\n",
      "Iteration 40390 Training loss 0.0765710175037384 Validation loss 0.06885935366153717 Accuracy 0.8495000004768372\n",
      "Iteration 40400 Training loss 0.09038003534078598 Validation loss 0.08139245212078094 Accuracy 0.8215000033378601\n",
      "Iteration 40410 Training loss 0.08633901923894882 Validation loss 0.06759701669216156 Accuracy 0.8504999876022339\n",
      "Iteration 40420 Training loss 0.09799236804246902 Validation loss 0.08158204704523087 Accuracy 0.8180000185966492\n",
      "Iteration 40430 Training loss 0.10557364672422409 Validation loss 0.0864361971616745 Accuracy 0.8075000047683716\n",
      "Iteration 40440 Training loss 0.09258067607879639 Validation loss 0.07527671009302139 Accuracy 0.8345000147819519\n",
      "Iteration 40450 Training loss 0.07333789765834808 Validation loss 0.07739214599132538 Accuracy 0.8314999938011169\n",
      "Iteration 40460 Training loss 0.08609142154455185 Validation loss 0.0719197615981102 Accuracy 0.8420000076293945\n",
      "Iteration 40470 Training loss 0.09338480234146118 Validation loss 0.0784936174750328 Accuracy 0.8270000219345093\n",
      "Iteration 40480 Training loss 0.08814344555139542 Validation loss 0.07394900918006897 Accuracy 0.840499997138977\n",
      "Iteration 40490 Training loss 0.07874784618616104 Validation loss 0.06945453584194183 Accuracy 0.8450000286102295\n",
      "Iteration 40500 Training loss 0.07992586493492126 Validation loss 0.07058447599411011 Accuracy 0.843500018119812\n",
      "Iteration 40510 Training loss 0.09127645194530487 Validation loss 0.08520811796188354 Accuracy 0.8180000185966492\n",
      "Iteration 40520 Training loss 0.08692991733551025 Validation loss 0.08802725374698639 Accuracy 0.8100000023841858\n",
      "Iteration 40530 Training loss 0.0910831168293953 Validation loss 0.08965794742107391 Accuracy 0.8054999709129333\n",
      "Iteration 40540 Training loss 0.08336354047060013 Validation loss 0.0840645283460617 Accuracy 0.8199999928474426\n",
      "Iteration 40550 Training loss 0.07899443060159683 Validation loss 0.07455655932426453 Accuracy 0.8374999761581421\n",
      "Iteration 40560 Training loss 0.08190341293811798 Validation loss 0.07080859690904617 Accuracy 0.8464999794960022\n",
      "Iteration 40570 Training loss 0.07886538654565811 Validation loss 0.07079484313726425 Accuracy 0.843999981880188\n",
      "Iteration 40580 Training loss 0.08010728657245636 Validation loss 0.07104651629924774 Accuracy 0.8445000052452087\n",
      "Iteration 40590 Training loss 0.08724121004343033 Validation loss 0.07665267586708069 Accuracy 0.8305000066757202\n",
      "Iteration 40600 Training loss 0.0862056091427803 Validation loss 0.07155320793390274 Accuracy 0.847000002861023\n",
      "Iteration 40610 Training loss 0.08342356979846954 Validation loss 0.0787406638264656 Accuracy 0.8314999938011169\n",
      "Iteration 40620 Training loss 0.0846952497959137 Validation loss 0.06978851556777954 Accuracy 0.847000002861023\n",
      "Iteration 40630 Training loss 0.0960177481174469 Validation loss 0.0835074782371521 Accuracy 0.8140000104904175\n",
      "Iteration 40640 Training loss 0.0851716622710228 Validation loss 0.07061058282852173 Accuracy 0.8464999794960022\n",
      "Iteration 40650 Training loss 0.09641530364751816 Validation loss 0.07761640846729279 Accuracy 0.8295000195503235\n",
      "Iteration 40660 Training loss 0.08701969683170319 Validation loss 0.07262518256902695 Accuracy 0.8410000205039978\n",
      "Iteration 40670 Training loss 0.11438234150409698 Validation loss 0.09775571525096893 Accuracy 0.7889999747276306\n",
      "Iteration 40680 Training loss 0.09096543490886688 Validation loss 0.07316666841506958 Accuracy 0.8399999737739563\n",
      "Iteration 40690 Training loss 0.07798994332551956 Validation loss 0.0715271458029747 Accuracy 0.8424999713897705\n",
      "Iteration 40700 Training loss 0.09027709066867828 Validation loss 0.07200177758932114 Accuracy 0.8420000076293945\n",
      "Iteration 40710 Training loss 0.0987846627831459 Validation loss 0.0829806923866272 Accuracy 0.8134999871253967\n",
      "Iteration 40720 Training loss 0.0839521512389183 Validation loss 0.07024430483579636 Accuracy 0.847000002861023\n",
      "Iteration 40730 Training loss 0.08825447410345078 Validation loss 0.07694393396377563 Accuracy 0.8335000276565552\n",
      "Iteration 40740 Training loss 0.09695112705230713 Validation loss 0.0752527043223381 Accuracy 0.8349999785423279\n",
      "Iteration 40750 Training loss 0.08822288364171982 Validation loss 0.07243245840072632 Accuracy 0.8410000205039978\n",
      "Iteration 40760 Training loss 0.08958136290311813 Validation loss 0.07433068007230759 Accuracy 0.8399999737739563\n",
      "Iteration 40770 Training loss 0.07913011312484741 Validation loss 0.0695403441786766 Accuracy 0.8454999923706055\n",
      "Iteration 40780 Training loss 0.09492715448141098 Validation loss 0.07522252202033997 Accuracy 0.8360000252723694\n",
      "Iteration 40790 Training loss 0.08214055746793747 Validation loss 0.07038004696369171 Accuracy 0.8479999899864197\n",
      "Iteration 40800 Training loss 0.08981882780790329 Validation loss 0.07601933926343918 Accuracy 0.8355000019073486\n",
      "Iteration 40810 Training loss 0.0946420282125473 Validation loss 0.07492101937532425 Accuracy 0.8364999890327454\n",
      "Iteration 40820 Training loss 0.09278246760368347 Validation loss 0.07219298183917999 Accuracy 0.8414999842643738\n",
      "Iteration 40830 Training loss 0.08240808546543121 Validation loss 0.07035280019044876 Accuracy 0.8454999923706055\n",
      "Iteration 40840 Training loss 0.08187581598758698 Validation loss 0.07041796296834946 Accuracy 0.843999981880188\n",
      "Iteration 40850 Training loss 0.08872691541910172 Validation loss 0.07090652734041214 Accuracy 0.8454999923706055\n",
      "Iteration 40860 Training loss 0.08618351817131042 Validation loss 0.0701790302991867 Accuracy 0.8454999923706055\n",
      "Iteration 40870 Training loss 0.08147606253623962 Validation loss 0.07103949040174484 Accuracy 0.8445000052452087\n",
      "Iteration 40880 Training loss 0.09158607572317123 Validation loss 0.08677496016025543 Accuracy 0.8119999766349792\n",
      "Iteration 40890 Training loss 0.08587735146284103 Validation loss 0.07062381505966187 Accuracy 0.843999981880188\n",
      "Iteration 40900 Training loss 0.0929517075419426 Validation loss 0.07425887882709503 Accuracy 0.8399999737739563\n",
      "Iteration 40910 Training loss 0.08538233488798141 Validation loss 0.07231643795967102 Accuracy 0.843500018119812\n",
      "Iteration 40920 Training loss 0.08522182703018188 Validation loss 0.07252699136734009 Accuracy 0.8414999842643738\n",
      "Iteration 40930 Training loss 0.08886568248271942 Validation loss 0.07849530875682831 Accuracy 0.8274999856948853\n",
      "Iteration 40940 Training loss 0.08380140364170074 Validation loss 0.07115969806909561 Accuracy 0.843500018119812\n",
      "Iteration 40950 Training loss 0.09510531276464462 Validation loss 0.08054561167955399 Accuracy 0.8220000267028809\n",
      "Iteration 40960 Training loss 0.08708261698484421 Validation loss 0.06895144283771515 Accuracy 0.8485000133514404\n",
      "Iteration 40970 Training loss 0.0873190388083458 Validation loss 0.0793599933385849 Accuracy 0.828499972820282\n",
      "Iteration 40980 Training loss 0.08547593653202057 Validation loss 0.0793597549200058 Accuracy 0.828000009059906\n",
      "Iteration 40990 Training loss 0.09402348101139069 Validation loss 0.08263727277517319 Accuracy 0.8215000033378601\n",
      "Iteration 41000 Training loss 0.08511393517255783 Validation loss 0.06991060823202133 Accuracy 0.8454999923706055\n",
      "Iteration 41010 Training loss 0.08474977314472198 Validation loss 0.06869041174650192 Accuracy 0.8514999747276306\n",
      "Iteration 41020 Training loss 0.0997966080904007 Validation loss 0.09813006222248077 Accuracy 0.7900000214576721\n",
      "Iteration 41030 Training loss 0.09264123439788818 Validation loss 0.09073365479707718 Accuracy 0.8050000071525574\n",
      "Iteration 41040 Training loss 0.09077657759189606 Validation loss 0.08376266062259674 Accuracy 0.8205000162124634\n",
      "Iteration 41050 Training loss 0.08439577370882034 Validation loss 0.07412784546613693 Accuracy 0.840499997138977\n",
      "Iteration 41060 Training loss 0.10164300352334976 Validation loss 0.0883999913930893 Accuracy 0.8075000047683716\n",
      "Iteration 41070 Training loss 0.08399970084428787 Validation loss 0.06990808248519897 Accuracy 0.8500000238418579\n",
      "Iteration 41080 Training loss 0.08489055186510086 Validation loss 0.07110000401735306 Accuracy 0.8464999794960022\n",
      "Iteration 41090 Training loss 0.07950970530509949 Validation loss 0.0705152079463005 Accuracy 0.8450000286102295\n",
      "Iteration 41100 Training loss 0.10635087639093399 Validation loss 0.08852533996105194 Accuracy 0.8050000071525574\n",
      "Iteration 41110 Training loss 0.0868680402636528 Validation loss 0.0715138241648674 Accuracy 0.8454999923706055\n",
      "Iteration 41120 Training loss 0.07914085686206818 Validation loss 0.07132595777511597 Accuracy 0.8464999794960022\n",
      "Iteration 41130 Training loss 0.08507602661848068 Validation loss 0.07074546068906784 Accuracy 0.8475000262260437\n",
      "Iteration 41140 Training loss 0.08846757560968399 Validation loss 0.08946363627910614 Accuracy 0.8075000047683716\n",
      "Iteration 41150 Training loss 0.08882278203964233 Validation loss 0.07753748446702957 Accuracy 0.8320000171661377\n",
      "Iteration 41160 Training loss 0.08997254818677902 Validation loss 0.0722094178199768 Accuracy 0.8454999923706055\n",
      "Iteration 41170 Training loss 0.09389366209506989 Validation loss 0.07474496215581894 Accuracy 0.8364999890327454\n",
      "Iteration 41180 Training loss 0.0988360121846199 Validation loss 0.0811227336525917 Accuracy 0.8195000290870667\n",
      "Iteration 41190 Training loss 0.09022927284240723 Validation loss 0.07143263518810272 Accuracy 0.8445000052452087\n",
      "Iteration 41200 Training loss 0.09119143337011337 Validation loss 0.07196879386901855 Accuracy 0.8460000157356262\n",
      "Iteration 41210 Training loss 0.08119862526655197 Validation loss 0.07331536710262299 Accuracy 0.8410000205039978\n",
      "Iteration 41220 Training loss 0.09561764448881149 Validation loss 0.09316709637641907 Accuracy 0.8015000224113464\n",
      "Iteration 41230 Training loss 0.0929161086678505 Validation loss 0.0820360854268074 Accuracy 0.8234999775886536\n",
      "Iteration 41240 Training loss 0.09274251013994217 Validation loss 0.08766086399555206 Accuracy 0.8130000233650208\n",
      "Iteration 41250 Training loss 0.08126327395439148 Validation loss 0.06947099417448044 Accuracy 0.8500000238418579\n",
      "Iteration 41260 Training loss 0.08211517333984375 Validation loss 0.07526097446680069 Accuracy 0.8374999761581421\n",
      "Iteration 41270 Training loss 0.08923571556806564 Validation loss 0.08881697058677673 Accuracy 0.8084999918937683\n",
      "Iteration 41280 Training loss 0.0943487286567688 Validation loss 0.10242461413145065 Accuracy 0.781000018119812\n",
      "Iteration 41290 Training loss 0.09514513611793518 Validation loss 0.08814623951911926 Accuracy 0.8125\n",
      "Iteration 41300 Training loss 0.08594448864459991 Validation loss 0.08223617076873779 Accuracy 0.8255000114440918\n",
      "Iteration 41310 Training loss 0.08806333690881729 Validation loss 0.07053999602794647 Accuracy 0.8485000133514404\n",
      "Iteration 41320 Training loss 0.09334401786327362 Validation loss 0.07315047085285187 Accuracy 0.840499997138977\n",
      "Iteration 41330 Training loss 0.08647344261407852 Validation loss 0.07191955298185349 Accuracy 0.8454999923706055\n",
      "Iteration 41340 Training loss 0.0796050876379013 Validation loss 0.07011271268129349 Accuracy 0.847000002861023\n",
      "Iteration 41350 Training loss 0.09292639791965485 Validation loss 0.09183204919099808 Accuracy 0.8009999990463257\n",
      "Iteration 41360 Training loss 0.09198186546564102 Validation loss 0.08307588845491409 Accuracy 0.8240000009536743\n",
      "Iteration 41370 Training loss 0.08958727866411209 Validation loss 0.0783875435590744 Accuracy 0.8305000066757202\n",
      "Iteration 41380 Training loss 0.08717066049575806 Validation loss 0.08834655582904816 Accuracy 0.8149999976158142\n",
      "Iteration 41390 Training loss 0.0924094170331955 Validation loss 0.0914226695895195 Accuracy 0.8065000176429749\n",
      "Iteration 41400 Training loss 0.08799710124731064 Validation loss 0.08502981066703796 Accuracy 0.8144999742507935\n",
      "Iteration 41410 Training loss 0.08598477393388748 Validation loss 0.07853573560714722 Accuracy 0.8299999833106995\n",
      "Iteration 41420 Training loss 0.09752792119979858 Validation loss 0.10061053931713104 Accuracy 0.7850000262260437\n",
      "Iteration 41430 Training loss 0.08301357179880142 Validation loss 0.07672597467899323 Accuracy 0.8309999704360962\n",
      "Iteration 41440 Training loss 0.08659692108631134 Validation loss 0.07504730671644211 Accuracy 0.8364999890327454\n",
      "Iteration 41450 Training loss 0.08637604117393494 Validation loss 0.07470681518316269 Accuracy 0.8364999890327454\n",
      "Iteration 41460 Training loss 0.09671288728713989 Validation loss 0.09315326064825058 Accuracy 0.8009999990463257\n",
      "Iteration 41470 Training loss 0.08920099586248398 Validation loss 0.07761317491531372 Accuracy 0.8330000042915344\n",
      "Iteration 41480 Training loss 0.10405262559652328 Validation loss 0.10775737464427948 Accuracy 0.7695000171661377\n",
      "Iteration 41490 Training loss 0.08258840441703796 Validation loss 0.07402318716049194 Accuracy 0.8395000100135803\n",
      "Iteration 41500 Training loss 0.09176167100667953 Validation loss 0.09268956631422043 Accuracy 0.8025000095367432\n",
      "Iteration 41510 Training loss 0.08954333513975143 Validation loss 0.07034574449062347 Accuracy 0.8464999794960022\n",
      "Iteration 41520 Training loss 0.0915912613272667 Validation loss 0.07257623225450516 Accuracy 0.8414999842643738\n",
      "Iteration 41530 Training loss 0.09108135849237442 Validation loss 0.07221990823745728 Accuracy 0.8424999713897705\n",
      "Iteration 41540 Training loss 0.08483836054801941 Validation loss 0.07041867077350616 Accuracy 0.8485000133514404\n",
      "Iteration 41550 Training loss 0.09860280156135559 Validation loss 0.07329712063074112 Accuracy 0.8429999947547913\n",
      "Iteration 41560 Training loss 0.08205004036426544 Validation loss 0.08509162813425064 Accuracy 0.8184999823570251\n",
      "Iteration 41570 Training loss 0.09270326048135757 Validation loss 0.08491093665361404 Accuracy 0.8190000057220459\n",
      "Iteration 41580 Training loss 0.09109602123498917 Validation loss 0.0952015072107315 Accuracy 0.7975000143051147\n",
      "Iteration 41590 Training loss 0.08445192873477936 Validation loss 0.08181290328502655 Accuracy 0.824999988079071\n",
      "Iteration 41600 Training loss 0.10268433392047882 Validation loss 0.10431720316410065 Accuracy 0.7774999737739563\n",
      "Iteration 41610 Training loss 0.08887805789709091 Validation loss 0.08299732953310013 Accuracy 0.8224999904632568\n",
      "Iteration 41620 Training loss 0.0921252891421318 Validation loss 0.09377393871545792 Accuracy 0.8025000095367432\n",
      "Iteration 41630 Training loss 0.088678278028965 Validation loss 0.08271332830190659 Accuracy 0.8215000033378601\n",
      "Iteration 41640 Training loss 0.08810633420944214 Validation loss 0.08414572477340698 Accuracy 0.8195000290870667\n",
      "Iteration 41650 Training loss 0.09392323344945908 Validation loss 0.07128913700580597 Accuracy 0.8464999794960022\n",
      "Iteration 41660 Training loss 0.07659390568733215 Validation loss 0.0715351402759552 Accuracy 0.843500018119812\n",
      "Iteration 41670 Training loss 0.08680374175310135 Validation loss 0.07090650498867035 Accuracy 0.8464999794960022\n",
      "Iteration 41680 Training loss 0.0856100395321846 Validation loss 0.0875585675239563 Accuracy 0.8130000233650208\n",
      "Iteration 41690 Training loss 0.10903012752532959 Validation loss 0.10558061301708221 Accuracy 0.7745000123977661\n",
      "Iteration 41700 Training loss 0.09745144844055176 Validation loss 0.07217465341091156 Accuracy 0.8460000157356262\n",
      "Iteration 41710 Training loss 0.08665728569030762 Validation loss 0.08216318488121033 Accuracy 0.8264999985694885\n",
      "Iteration 41720 Training loss 0.09162795543670654 Validation loss 0.07346641272306442 Accuracy 0.8399999737739563\n",
      "Iteration 41730 Training loss 0.08120578527450562 Validation loss 0.07788675278425217 Accuracy 0.8309999704360962\n",
      "Iteration 41740 Training loss 0.09856103360652924 Validation loss 0.10608961433172226 Accuracy 0.7705000042915344\n",
      "Iteration 41750 Training loss 0.09403296560049057 Validation loss 0.09634716808795929 Accuracy 0.7940000295639038\n",
      "Iteration 41760 Training loss 0.08596847951412201 Validation loss 0.07110683619976044 Accuracy 0.843999981880188\n",
      "Iteration 41770 Training loss 0.09092182666063309 Validation loss 0.07078830152750015 Accuracy 0.8464999794960022\n",
      "Iteration 41780 Training loss 0.08552446216344833 Validation loss 0.09059929847717285 Accuracy 0.8059999942779541\n",
      "Iteration 41790 Training loss 0.08905989676713943 Validation loss 0.07604040205478668 Accuracy 0.8339999914169312\n",
      "Iteration 41800 Training loss 0.09170468896627426 Validation loss 0.08164693415164948 Accuracy 0.8230000138282776\n",
      "Iteration 41810 Training loss 0.09108903259038925 Validation loss 0.0782962366938591 Accuracy 0.8309999704360962\n",
      "Iteration 41820 Training loss 0.09028495848178864 Validation loss 0.09195948392152786 Accuracy 0.8065000176429749\n",
      "Iteration 41830 Training loss 0.0809900239109993 Validation loss 0.07260731607675552 Accuracy 0.843500018119812\n",
      "Iteration 41840 Training loss 0.09818767011165619 Validation loss 0.10322257876396179 Accuracy 0.7804999947547913\n",
      "Iteration 41850 Training loss 0.0919908657670021 Validation loss 0.09134860336780548 Accuracy 0.8054999709129333\n",
      "Iteration 41860 Training loss 0.09763647615909576 Validation loss 0.09843821823596954 Accuracy 0.7879999876022339\n",
      "Iteration 41870 Training loss 0.10007460415363312 Validation loss 0.09629714488983154 Accuracy 0.7950000166893005\n",
      "Iteration 41880 Training loss 0.08973872661590576 Validation loss 0.09772590547800064 Accuracy 0.7935000061988831\n",
      "Iteration 41890 Training loss 0.08848123997449875 Validation loss 0.07914617657661438 Accuracy 0.828000009059906\n",
      "Iteration 41900 Training loss 0.08920630812644958 Validation loss 0.07696269452571869 Accuracy 0.8330000042915344\n",
      "Iteration 41910 Training loss 0.09815897792577744 Validation loss 0.10268627852201462 Accuracy 0.7795000076293945\n",
      "Iteration 41920 Training loss 0.08984129130840302 Validation loss 0.08472775667905807 Accuracy 0.8199999928474426\n",
      "Iteration 41930 Training loss 0.08917704224586487 Validation loss 0.08451241999864578 Accuracy 0.8199999928474426\n",
      "Iteration 41940 Training loss 0.08688769489526749 Validation loss 0.08894377201795578 Accuracy 0.8084999918937683\n",
      "Iteration 41950 Training loss 0.08815136551856995 Validation loss 0.08629922568798065 Accuracy 0.8149999976158142\n",
      "Iteration 41960 Training loss 0.09890364855527878 Validation loss 0.10016246885061264 Accuracy 0.7875000238418579\n",
      "Iteration 41970 Training loss 0.08687711507081985 Validation loss 0.07516898959875107 Accuracy 0.8395000100135803\n",
      "Iteration 41980 Training loss 0.08799232542514801 Validation loss 0.07159163057804108 Accuracy 0.8475000262260437\n",
      "Iteration 41990 Training loss 0.08222441375255585 Validation loss 0.0760662630200386 Accuracy 0.8345000147819519\n",
      "Iteration 42000 Training loss 0.10601583123207092 Validation loss 0.11267156898975372 Accuracy 0.7630000114440918\n",
      "Iteration 42010 Training loss 0.08913195133209229 Validation loss 0.09454827755689621 Accuracy 0.8019999861717224\n",
      "Iteration 42020 Training loss 0.10818372666835785 Validation loss 0.11047796905040741 Accuracy 0.7639999985694885\n",
      "Iteration 42030 Training loss 0.08874396979808807 Validation loss 0.09315133839845657 Accuracy 0.8025000095367432\n",
      "Iteration 42040 Training loss 0.08501271158456802 Validation loss 0.07208789885044098 Accuracy 0.8445000052452087\n",
      "Iteration 42050 Training loss 0.09104081243276596 Validation loss 0.09741971641778946 Accuracy 0.7904999852180481\n",
      "Iteration 42060 Training loss 0.08293280750513077 Validation loss 0.07384409010410309 Accuracy 0.8399999737739563\n",
      "Iteration 42070 Training loss 0.11243365705013275 Validation loss 0.11079425364732742 Accuracy 0.765500009059906\n",
      "Iteration 42080 Training loss 0.08102937042713165 Validation loss 0.07385293394327164 Accuracy 0.8379999995231628\n",
      "Iteration 42090 Training loss 0.08465611189603806 Validation loss 0.07039130479097366 Accuracy 0.8454999923706055\n",
      "Iteration 42100 Training loss 0.08971281349658966 Validation loss 0.0797531008720398 Accuracy 0.8289999961853027\n",
      "Iteration 42110 Training loss 0.10886313021183014 Validation loss 0.10197222977876663 Accuracy 0.7820000052452087\n",
      "Iteration 42120 Training loss 0.08468370884656906 Validation loss 0.08223803341388702 Accuracy 0.8245000243186951\n",
      "Iteration 42130 Training loss 0.09410284459590912 Validation loss 0.09365196526050568 Accuracy 0.8029999732971191\n",
      "Iteration 42140 Training loss 0.09040872007608414 Validation loss 0.08486489206552505 Accuracy 0.8205000162124634\n",
      "Iteration 42150 Training loss 0.08037397265434265 Validation loss 0.07113849371671677 Accuracy 0.847000002861023\n",
      "Iteration 42160 Training loss 0.09792359918355942 Validation loss 0.07533833384513855 Accuracy 0.8355000019073486\n",
      "Iteration 42170 Training loss 0.09114768356084824 Validation loss 0.07189507782459259 Accuracy 0.8479999899864197\n",
      "Iteration 42180 Training loss 0.08428166061639786 Validation loss 0.07172977924346924 Accuracy 0.8479999899864197\n",
      "Iteration 42190 Training loss 0.08566054701805115 Validation loss 0.07378925383090973 Accuracy 0.8399999737739563\n",
      "Iteration 42200 Training loss 0.0885053426027298 Validation loss 0.07108508050441742 Accuracy 0.8460000157356262\n",
      "Iteration 42210 Training loss 0.10346391052007675 Validation loss 0.08595362305641174 Accuracy 0.8130000233650208\n",
      "Iteration 42220 Training loss 0.09287930279970169 Validation loss 0.0731232762336731 Accuracy 0.8429999947547913\n",
      "Iteration 42230 Training loss 0.08328410238027573 Validation loss 0.07129700481891632 Accuracy 0.8475000262260437\n",
      "Iteration 42240 Training loss 0.09094586223363876 Validation loss 0.08747884631156921 Accuracy 0.8140000104904175\n",
      "Iteration 42250 Training loss 0.08369279652833939 Validation loss 0.070782870054245 Accuracy 0.8479999899864197\n",
      "Iteration 42260 Training loss 0.09122880548238754 Validation loss 0.0728524699807167 Accuracy 0.843999981880188\n",
      "Iteration 42270 Training loss 0.09437581151723862 Validation loss 0.07408799976110458 Accuracy 0.840499997138977\n",
      "Iteration 42280 Training loss 0.10045081377029419 Validation loss 0.07996440678834915 Accuracy 0.828499972820282\n",
      "Iteration 42290 Training loss 0.08596385270357132 Validation loss 0.07239536941051483 Accuracy 0.843500018119812\n",
      "Iteration 42300 Training loss 0.08709364384412766 Validation loss 0.08384203910827637 Accuracy 0.8184999823570251\n",
      "Iteration 42310 Training loss 0.08724687993526459 Validation loss 0.07462482899427414 Accuracy 0.8395000100135803\n",
      "Iteration 42320 Training loss 0.08776327967643738 Validation loss 0.07185807824134827 Accuracy 0.847000002861023\n",
      "Iteration 42330 Training loss 0.10027032345533371 Validation loss 0.07926036417484283 Accuracy 0.8295000195503235\n",
      "Iteration 42340 Training loss 0.09257028251886368 Validation loss 0.07164204120635986 Accuracy 0.847000002861023\n",
      "Iteration 42350 Training loss 0.08624839037656784 Validation loss 0.07382670789957047 Accuracy 0.8429999947547913\n",
      "Iteration 42360 Training loss 0.09404740482568741 Validation loss 0.0729316994547844 Accuracy 0.8420000076293945\n",
      "Iteration 42370 Training loss 0.08778280019760132 Validation loss 0.0721975713968277 Accuracy 0.843500018119812\n",
      "Iteration 42380 Training loss 0.09689358621835709 Validation loss 0.0746559351682663 Accuracy 0.8399999737739563\n",
      "Iteration 42390 Training loss 0.090497225522995 Validation loss 0.07256143540143967 Accuracy 0.8460000157356262\n",
      "Iteration 42400 Training loss 0.08943131566047668 Validation loss 0.07173088192939758 Accuracy 0.8445000052452087\n",
      "Iteration 42410 Training loss 0.08823896944522858 Validation loss 0.0738389790058136 Accuracy 0.8395000100135803\n",
      "Iteration 42420 Training loss 0.09357002377510071 Validation loss 0.07386712729930878 Accuracy 0.8420000076293945\n",
      "Iteration 42430 Training loss 0.09360595047473907 Validation loss 0.07468708604574203 Accuracy 0.8395000100135803\n",
      "Iteration 42440 Training loss 0.09244532138109207 Validation loss 0.08642024546861649 Accuracy 0.815500020980835\n",
      "Iteration 42450 Training loss 0.10698362439870834 Validation loss 0.11357943713665009 Accuracy 0.7609999775886536\n",
      "Iteration 42460 Training loss 0.08153007179498672 Validation loss 0.07334674149751663 Accuracy 0.8410000205039978\n",
      "Iteration 42470 Training loss 0.08708958327770233 Validation loss 0.07142201066017151 Accuracy 0.843999981880188\n",
      "Iteration 42480 Training loss 0.09469426423311234 Validation loss 0.07156495749950409 Accuracy 0.847000002861023\n",
      "Iteration 42490 Training loss 0.08657940477132797 Validation loss 0.07185102254152298 Accuracy 0.8429999947547913\n",
      "Iteration 42500 Training loss 0.10253322124481201 Validation loss 0.0788244754076004 Accuracy 0.8274999856948853\n",
      "Iteration 42510 Training loss 0.10205404460430145 Validation loss 0.07963431626558304 Accuracy 0.828000009059906\n",
      "Iteration 42520 Training loss 0.08198412507772446 Validation loss 0.07658424228429794 Accuracy 0.8385000228881836\n",
      "Iteration 42530 Training loss 0.09350283443927765 Validation loss 0.0838303491473198 Accuracy 0.8220000267028809\n",
      "Iteration 42540 Training loss 0.08866669982671738 Validation loss 0.09213521331548691 Accuracy 0.8065000176429749\n",
      "Iteration 42550 Training loss 0.0957888811826706 Validation loss 0.09322115778923035 Accuracy 0.8040000200271606\n",
      "Iteration 42560 Training loss 0.08143804967403412 Validation loss 0.07221610099077225 Accuracy 0.8450000286102295\n",
      "Iteration 42570 Training loss 0.08069974929094315 Validation loss 0.07395095378160477 Accuracy 0.8374999761581421\n",
      "Iteration 42580 Training loss 0.09029532223939896 Validation loss 0.08604075759649277 Accuracy 0.8169999718666077\n",
      "Iteration 42590 Training loss 0.09918893873691559 Validation loss 0.09335465729236603 Accuracy 0.8004999756813049\n",
      "Iteration 42600 Training loss 0.0879935473203659 Validation loss 0.07223820686340332 Accuracy 0.8464999794960022\n",
      "Iteration 42610 Training loss 0.09063172340393066 Validation loss 0.07173948734998703 Accuracy 0.8475000262260437\n",
      "Iteration 42620 Training loss 0.09444740414619446 Validation loss 0.07403400540351868 Accuracy 0.8420000076293945\n",
      "Iteration 42630 Training loss 0.09140457957983017 Validation loss 0.0798170194029808 Accuracy 0.8289999961853027\n",
      "Iteration 42640 Training loss 0.08765243738889694 Validation loss 0.07428157329559326 Accuracy 0.8395000100135803\n",
      "Iteration 42650 Training loss 0.09366633743047714 Validation loss 0.08533358573913574 Accuracy 0.8184999823570251\n",
      "Iteration 42660 Training loss 0.09184955805540085 Validation loss 0.07866999506950378 Accuracy 0.8330000042915344\n",
      "Iteration 42670 Training loss 0.08806250989437103 Validation loss 0.0813201367855072 Accuracy 0.8240000009536743\n",
      "Iteration 42680 Training loss 0.08718863129615784 Validation loss 0.07365898787975311 Accuracy 0.8424999713897705\n",
      "Iteration 42690 Training loss 0.08192005753517151 Validation loss 0.07131116092205048 Accuracy 0.8464999794960022\n",
      "Iteration 42700 Training loss 0.08771195262670517 Validation loss 0.07257603108882904 Accuracy 0.8475000262260437\n",
      "Iteration 42710 Training loss 0.09457724541425705 Validation loss 0.07465914636850357 Accuracy 0.8420000076293945\n",
      "Iteration 42720 Training loss 0.08331373333930969 Validation loss 0.07389508932828903 Accuracy 0.8414999842643738\n",
      "Iteration 42730 Training loss 0.10131246596574783 Validation loss 0.07508747279644012 Accuracy 0.8379999995231628\n",
      "Iteration 42740 Training loss 0.08907722681760788 Validation loss 0.07322563230991364 Accuracy 0.843500018119812\n",
      "Iteration 42750 Training loss 0.08969199657440186 Validation loss 0.07429550588130951 Accuracy 0.8395000100135803\n",
      "Iteration 42760 Training loss 0.10458281636238098 Validation loss 0.08286300301551819 Accuracy 0.8215000033378601\n",
      "Iteration 42770 Training loss 0.09511060267686844 Validation loss 0.09485797584056854 Accuracy 0.7975000143051147\n",
      "Iteration 42780 Training loss 0.09544704854488373 Validation loss 0.08478564769029617 Accuracy 0.8190000057220459\n",
      "Iteration 42790 Training loss 0.09673066437244415 Validation loss 0.07499298453330994 Accuracy 0.8389999866485596\n",
      "Iteration 42800 Training loss 0.0986885353922844 Validation loss 0.07630978524684906 Accuracy 0.8364999890327454\n",
      "Iteration 42810 Training loss 0.08899365365505219 Validation loss 0.07225840538740158 Accuracy 0.847000002861023\n",
      "Iteration 42820 Training loss 0.08707460761070251 Validation loss 0.07509709149599075 Accuracy 0.8385000228881836\n",
      "Iteration 42830 Training loss 0.11598876863718033 Validation loss 0.09740594029426575 Accuracy 0.7914999723434448\n",
      "Iteration 42840 Training loss 0.09060235321521759 Validation loss 0.0719345360994339 Accuracy 0.8460000157356262\n",
      "Iteration 42850 Training loss 0.08441758900880814 Validation loss 0.08121780306100845 Accuracy 0.828499972820282\n",
      "Iteration 42860 Training loss 0.08788757026195526 Validation loss 0.07235261797904968 Accuracy 0.843500018119812\n",
      "Iteration 42870 Training loss 0.10593219101428986 Validation loss 0.10408806055784225 Accuracy 0.7785000205039978\n",
      "Iteration 42880 Training loss 0.07937248796224594 Validation loss 0.07583656162023544 Accuracy 0.8389999866485596\n",
      "Iteration 42890 Training loss 0.08331109583377838 Validation loss 0.0801134705543518 Accuracy 0.8295000195503235\n",
      "Iteration 42900 Training loss 0.09733142703771591 Validation loss 0.09721262753009796 Accuracy 0.7929999828338623\n",
      "Iteration 42910 Training loss 0.0923389196395874 Validation loss 0.08471054583787918 Accuracy 0.8184999823570251\n",
      "Iteration 42920 Training loss 0.0958826020359993 Validation loss 0.09842770546674728 Accuracy 0.7910000085830688\n",
      "Iteration 42930 Training loss 0.08299186825752258 Validation loss 0.07686229795217514 Accuracy 0.8370000123977661\n",
      "Iteration 42940 Training loss 0.09004101157188416 Validation loss 0.0852464959025383 Accuracy 0.8209999799728394\n",
      "Iteration 42950 Training loss 0.09528135508298874 Validation loss 0.09837211668491364 Accuracy 0.7900000214576721\n",
      "Iteration 42960 Training loss 0.08927541971206665 Validation loss 0.10087697207927704 Accuracy 0.784500002861023\n",
      "Iteration 42970 Training loss 0.08943836390972137 Validation loss 0.07870733737945557 Accuracy 0.8320000171661377\n",
      "Iteration 42980 Training loss 0.09766454249620438 Validation loss 0.08327510207891464 Accuracy 0.8215000033378601\n",
      "Iteration 42990 Training loss 0.09100483357906342 Validation loss 0.07236660271883011 Accuracy 0.8485000133514404\n",
      "Iteration 43000 Training loss 0.08994977921247482 Validation loss 0.07221517711877823 Accuracy 0.8454999923706055\n",
      "Iteration 43010 Training loss 0.09812695533037186 Validation loss 0.09686096012592316 Accuracy 0.7929999828338623\n",
      "Iteration 43020 Training loss 0.093645378947258 Validation loss 0.08852601796388626 Accuracy 0.8109999895095825\n",
      "Iteration 43030 Training loss 0.09782613813877106 Validation loss 0.09366081655025482 Accuracy 0.8025000095367432\n",
      "Iteration 43040 Training loss 0.09209942817687988 Validation loss 0.10244189202785492 Accuracy 0.7804999947547913\n",
      "Iteration 43050 Training loss 0.08505263924598694 Validation loss 0.08807312697172165 Accuracy 0.8134999871253967\n",
      "Iteration 43060 Training loss 0.092950239777565 Validation loss 0.07131738215684891 Accuracy 0.8495000004768372\n",
      "Iteration 43070 Training loss 0.10396063327789307 Validation loss 0.0833057314157486 Accuracy 0.8209999799728394\n",
      "Iteration 43080 Training loss 0.08606186509132385 Validation loss 0.07217591255903244 Accuracy 0.8424999713897705\n",
      "Iteration 43090 Training loss 0.08875033259391785 Validation loss 0.08775132149457932 Accuracy 0.8159999847412109\n",
      "Iteration 43100 Training loss 0.08483637124300003 Validation loss 0.07717065513134003 Accuracy 0.8360000252723694\n",
      "Iteration 43110 Training loss 0.09663021564483643 Validation loss 0.09088382869958878 Accuracy 0.8054999709129333\n",
      "Iteration 43120 Training loss 0.08381715416908264 Validation loss 0.07289157807826996 Accuracy 0.8445000052452087\n",
      "Iteration 43130 Training loss 0.10051359981298447 Validation loss 0.09931205213069916 Accuracy 0.7900000214576721\n",
      "Iteration 43140 Training loss 0.08731994032859802 Validation loss 0.07411248981952667 Accuracy 0.8414999842643738\n",
      "Iteration 43150 Training loss 0.10096639394760132 Validation loss 0.11136862635612488 Accuracy 0.765999972820282\n",
      "Iteration 43160 Training loss 0.08486463874578476 Validation loss 0.0782116949558258 Accuracy 0.8309999704360962\n",
      "Iteration 43170 Training loss 0.10525538772344589 Validation loss 0.11461298912763596 Accuracy 0.7590000033378601\n",
      "Iteration 43180 Training loss 0.091796875 Validation loss 0.07926863431930542 Accuracy 0.8320000171661377\n",
      "Iteration 43190 Training loss 0.09492681920528412 Validation loss 0.10252626985311508 Accuracy 0.7825000286102295\n",
      "Iteration 43200 Training loss 0.10208647698163986 Validation loss 0.11776863038539886 Accuracy 0.7540000081062317\n",
      "Iteration 43210 Training loss 0.09373104572296143 Validation loss 0.08477528393268585 Accuracy 0.8205000162124634\n",
      "Iteration 43220 Training loss 0.1006573960185051 Validation loss 0.09637781232595444 Accuracy 0.7975000143051147\n",
      "Iteration 43230 Training loss 0.08397070318460464 Validation loss 0.08049389719963074 Accuracy 0.828499972820282\n",
      "Iteration 43240 Training loss 0.09782885760068893 Validation loss 0.10803215950727463 Accuracy 0.7714999914169312\n",
      "Iteration 43250 Training loss 0.08329406380653381 Validation loss 0.07669317722320557 Accuracy 0.8364999890327454\n",
      "Iteration 43260 Training loss 0.07923982292413712 Validation loss 0.07182737439870834 Accuracy 0.8475000262260437\n",
      "Iteration 43270 Training loss 0.0863223448395729 Validation loss 0.07955428957939148 Accuracy 0.8299999833106995\n",
      "Iteration 43280 Training loss 0.09918157011270523 Validation loss 0.07295458763837814 Accuracy 0.843500018119812\n",
      "Iteration 43290 Training loss 0.08579006046056747 Validation loss 0.07352137565612793 Accuracy 0.8410000205039978\n",
      "Iteration 43300 Training loss 0.10219927132129669 Validation loss 0.08790285140275955 Accuracy 0.8084999918937683\n",
      "Iteration 43310 Training loss 0.08748150616884232 Validation loss 0.0709628164768219 Accuracy 0.8479999899864197\n",
      "Iteration 43320 Training loss 0.09768180549144745 Validation loss 0.07876186817884445 Accuracy 0.8295000195503235\n",
      "Iteration 43330 Training loss 0.08958690613508224 Validation loss 0.07192671298980713 Accuracy 0.8479999899864197\n",
      "Iteration 43340 Training loss 0.08866173028945923 Validation loss 0.08581248670816422 Accuracy 0.8184999823570251\n",
      "Iteration 43350 Training loss 0.09249518066644669 Validation loss 0.07347796857357025 Accuracy 0.8385000228881836\n",
      "Iteration 43360 Training loss 0.090249203145504 Validation loss 0.09633888304233551 Accuracy 0.7940000295639038\n",
      "Iteration 43370 Training loss 0.0865616500377655 Validation loss 0.0726560726761818 Accuracy 0.8429999947547913\n",
      "Iteration 43380 Training loss 0.08877597004175186 Validation loss 0.07157536596059799 Accuracy 0.8479999899864197\n",
      "Iteration 43390 Training loss 0.09098648279905319 Validation loss 0.07168478518724442 Accuracy 0.8479999899864197\n",
      "Iteration 43400 Training loss 0.09944123029708862 Validation loss 0.07307617366313934 Accuracy 0.8464999794960022\n",
      "Iteration 43410 Training loss 0.10955768078565598 Validation loss 0.08779608458280563 Accuracy 0.8115000128746033\n",
      "Iteration 43420 Training loss 0.08864697068929672 Validation loss 0.0973782017827034 Accuracy 0.7914999723434448\n",
      "Iteration 43430 Training loss 0.08618254214525223 Validation loss 0.0890578180551529 Accuracy 0.809499979019165\n",
      "Iteration 43440 Training loss 0.12095259130001068 Validation loss 0.09515386819839478 Accuracy 0.7960000038146973\n",
      "Iteration 43450 Training loss 0.09427880495786667 Validation loss 0.07314620167016983 Accuracy 0.8460000157356262\n",
      "Iteration 43460 Training loss 0.08584591001272202 Validation loss 0.07235776633024216 Accuracy 0.8450000286102295\n",
      "Iteration 43470 Training loss 0.08574791252613068 Validation loss 0.07231102138757706 Accuracy 0.843999981880188\n",
      "Iteration 43480 Training loss 0.09648657590150833 Validation loss 0.10959593206644058 Accuracy 0.7699999809265137\n",
      "Iteration 43490 Training loss 0.09274808317422867 Validation loss 0.09868045151233673 Accuracy 0.7910000085830688\n",
      "Iteration 43500 Training loss 0.09052646905183792 Validation loss 0.08079710602760315 Accuracy 0.8274999856948853\n",
      "Iteration 43510 Training loss 0.08275847136974335 Validation loss 0.07387993484735489 Accuracy 0.8395000100135803\n",
      "Iteration 43520 Training loss 0.10046707093715668 Validation loss 0.07396654039621353 Accuracy 0.8414999842643738\n",
      "Iteration 43530 Training loss 0.09098242223262787 Validation loss 0.07180899381637573 Accuracy 0.8489999771118164\n",
      "Iteration 43540 Training loss 0.08147168159484863 Validation loss 0.08236142992973328 Accuracy 0.8245000243186951\n",
      "Iteration 43550 Training loss 0.08696499466896057 Validation loss 0.08204013109207153 Accuracy 0.828000009059906\n",
      "Iteration 43560 Training loss 0.08706815540790558 Validation loss 0.0950070172548294 Accuracy 0.796999990940094\n",
      "Iteration 43570 Training loss 0.09492683410644531 Validation loss 0.0951756164431572 Accuracy 0.7975000143051147\n",
      "Iteration 43580 Training loss 0.08065345883369446 Validation loss 0.0746706873178482 Accuracy 0.8389999866485596\n",
      "Iteration 43590 Training loss 0.09580172598361969 Validation loss 0.10817436128854752 Accuracy 0.7730000019073486\n",
      "Iteration 43600 Training loss 0.0999460220336914 Validation loss 0.10720144957304001 Accuracy 0.7739999890327454\n",
      "Iteration 43610 Training loss 0.08323829621076584 Validation loss 0.08683686703443527 Accuracy 0.815500020980835\n",
      "Iteration 43620 Training loss 0.09024328738451004 Validation loss 0.08672482520341873 Accuracy 0.8159999847412109\n",
      "Iteration 43630 Training loss 0.09228689968585968 Validation loss 0.08924899995326996 Accuracy 0.8100000023841858\n",
      "Iteration 43640 Training loss 0.10527490079402924 Validation loss 0.1139218807220459 Accuracy 0.762499988079071\n",
      "Iteration 43650 Training loss 0.08277387171983719 Validation loss 0.08497840911149979 Accuracy 0.8209999799728394\n",
      "Iteration 43660 Training loss 0.09792254865169525 Validation loss 0.10083863884210587 Accuracy 0.784500002861023\n",
      "Iteration 43670 Training loss 0.10590707510709763 Validation loss 0.10428712517023087 Accuracy 0.7799999713897705\n",
      "Iteration 43680 Training loss 0.0889086052775383 Validation loss 0.09039659798145294 Accuracy 0.8069999814033508\n",
      "Iteration 43690 Training loss 0.09129898250102997 Validation loss 0.08903251588344574 Accuracy 0.8119999766349792\n",
      "Iteration 43700 Training loss 0.08590248227119446 Validation loss 0.09738228470087051 Accuracy 0.7925000190734863\n",
      "Iteration 43710 Training loss 0.09518107771873474 Validation loss 0.07311738282442093 Accuracy 0.8460000157356262\n",
      "Iteration 43720 Training loss 0.08526700735092163 Validation loss 0.07085156440734863 Accuracy 0.8479999899864197\n",
      "Iteration 43730 Training loss 0.09558159857988358 Validation loss 0.11424766480922699 Accuracy 0.7605000138282776\n",
      "Iteration 43740 Training loss 0.08871917426586151 Validation loss 0.07400751858949661 Accuracy 0.8395000100135803\n",
      "Iteration 43750 Training loss 0.08467373996973038 Validation loss 0.07217257469892502 Accuracy 0.843999981880188\n",
      "Iteration 43760 Training loss 0.0875975638628006 Validation loss 0.07785530388355255 Accuracy 0.8330000042915344\n",
      "Iteration 43770 Training loss 0.09883755445480347 Validation loss 0.10621193051338196 Accuracy 0.7730000019073486\n",
      "Iteration 43780 Training loss 0.09703799337148666 Validation loss 0.10897787660360336 Accuracy 0.7684999704360962\n",
      "Iteration 43790 Training loss 0.1011715680360794 Validation loss 0.1135554388165474 Accuracy 0.7649999856948853\n",
      "Iteration 43800 Training loss 0.09715786576271057 Validation loss 0.10511810332536697 Accuracy 0.7774999737739563\n",
      "Iteration 43810 Training loss 0.08709097653627396 Validation loss 0.07749444991350174 Accuracy 0.8349999785423279\n",
      "Iteration 43820 Training loss 0.08057457208633423 Validation loss 0.07173905521631241 Accuracy 0.8454999923706055\n",
      "Iteration 43830 Training loss 0.10096421092748642 Validation loss 0.08830255270004272 Accuracy 0.8125\n",
      "Iteration 43840 Training loss 0.09614343196153641 Validation loss 0.10590075701475143 Accuracy 0.7760000228881836\n",
      "Iteration 43850 Training loss 0.09909361600875854 Validation loss 0.11351010203361511 Accuracy 0.7605000138282776\n",
      "Iteration 43860 Training loss 0.0829240009188652 Validation loss 0.08095572143793106 Accuracy 0.828000009059906\n",
      "Iteration 43870 Training loss 0.09252980351448059 Validation loss 0.07107160240411758 Accuracy 0.847000002861023\n",
      "Iteration 43880 Training loss 0.08746962249279022 Validation loss 0.07302496582269669 Accuracy 0.8454999923706055\n",
      "Iteration 43890 Training loss 0.1111988052725792 Validation loss 0.08340950310230255 Accuracy 0.8174999952316284\n",
      "Iteration 43900 Training loss 0.09277752786874771 Validation loss 0.08787570893764496 Accuracy 0.8144999742507935\n",
      "Iteration 43910 Training loss 0.08734587579965591 Validation loss 0.0748823806643486 Accuracy 0.840499997138977\n",
      "Iteration 43920 Training loss 0.08336562663316727 Validation loss 0.07395117729902267 Accuracy 0.8410000205039978\n",
      "Iteration 43930 Training loss 0.08601240813732147 Validation loss 0.07404711842536926 Accuracy 0.843500018119812\n",
      "Iteration 43940 Training loss 0.08652260899543762 Validation loss 0.0752694234251976 Accuracy 0.8395000100135803\n",
      "Iteration 43950 Training loss 0.08093041926622391 Validation loss 0.07249423116445541 Accuracy 0.843999981880188\n",
      "Iteration 43960 Training loss 0.09338890016078949 Validation loss 0.0814288929104805 Accuracy 0.8215000033378601\n",
      "Iteration 43970 Training loss 0.09556051343679428 Validation loss 0.07410864531993866 Accuracy 0.8424999713897705\n",
      "Iteration 43980 Training loss 0.09127967059612274 Validation loss 0.07360368967056274 Accuracy 0.8450000286102295\n",
      "Iteration 43990 Training loss 0.09542562812566757 Validation loss 0.0777638852596283 Accuracy 0.8324999809265137\n",
      "Iteration 44000 Training loss 0.08577153831720352 Validation loss 0.07366889715194702 Accuracy 0.8399999737739563\n",
      "Iteration 44010 Training loss 0.08525044471025467 Validation loss 0.07410873472690582 Accuracy 0.8389999866485596\n",
      "Iteration 44020 Training loss 0.09911082684993744 Validation loss 0.10401121526956558 Accuracy 0.7770000100135803\n",
      "Iteration 44030 Training loss 0.08593171834945679 Validation loss 0.0976310670375824 Accuracy 0.7935000061988831\n",
      "Iteration 44040 Training loss 0.08185384422540665 Validation loss 0.07116193324327469 Accuracy 0.847000002861023\n",
      "Iteration 44050 Training loss 0.08584848791360855 Validation loss 0.07228467613458633 Accuracy 0.843500018119812\n",
      "Iteration 44060 Training loss 0.0923907682299614 Validation loss 0.09089959412813187 Accuracy 0.8075000047683716\n",
      "Iteration 44070 Training loss 0.09141527116298676 Validation loss 0.090630903840065 Accuracy 0.8090000152587891\n",
      "Iteration 44080 Training loss 0.09175015985965729 Validation loss 0.09812949597835541 Accuracy 0.7925000190734863\n",
      "Iteration 44090 Training loss 0.10533731430768967 Validation loss 0.10361672192811966 Accuracy 0.7799999713897705\n",
      "Iteration 44100 Training loss 0.09209176898002625 Validation loss 0.07372461259365082 Accuracy 0.8429999947547913\n",
      "Iteration 44110 Training loss 0.08968430757522583 Validation loss 0.07137040048837662 Accuracy 0.8479999899864197\n",
      "Iteration 44120 Training loss 0.10323013365268707 Validation loss 0.07823178917169571 Accuracy 0.8339999914169312\n",
      "Iteration 44130 Training loss 0.08344213664531708 Validation loss 0.0715060606598854 Accuracy 0.847000002861023\n",
      "Iteration 44140 Training loss 0.09801852703094482 Validation loss 0.07745170593261719 Accuracy 0.8355000019073486\n",
      "Iteration 44150 Training loss 0.08199936151504517 Validation loss 0.07274919748306274 Accuracy 0.8424999713897705\n",
      "Iteration 44160 Training loss 0.08691395074129105 Validation loss 0.07132301479578018 Accuracy 0.8485000133514404\n",
      "Iteration 44170 Training loss 0.09382106363773346 Validation loss 0.07055295258760452 Accuracy 0.8485000133514404\n",
      "Iteration 44180 Training loss 0.10224302113056183 Validation loss 0.08282379060983658 Accuracy 0.8220000267028809\n",
      "Iteration 44190 Training loss 0.09199261665344238 Validation loss 0.07470837980508804 Accuracy 0.840499997138977\n",
      "Iteration 44200 Training loss 0.10072968155145645 Validation loss 0.10568813979625702 Accuracy 0.7764999866485596\n",
      "Iteration 44210 Training loss 0.08874674886465073 Validation loss 0.0832599550485611 Accuracy 0.8245000243186951\n",
      "Iteration 44220 Training loss 0.0915575623512268 Validation loss 0.09266947954893112 Accuracy 0.8050000071525574\n",
      "Iteration 44230 Training loss 0.09491560608148575 Validation loss 0.10253218561410904 Accuracy 0.784500002861023\n",
      "Iteration 44240 Training loss 0.10202784836292267 Validation loss 0.09611444175243378 Accuracy 0.7960000038146973\n",
      "Iteration 44250 Training loss 0.09562571346759796 Validation loss 0.09789889305830002 Accuracy 0.7925000190734863\n",
      "Iteration 44260 Training loss 0.08946099877357483 Validation loss 0.0980072095990181 Accuracy 0.7894999980926514\n",
      "Iteration 44270 Training loss 0.09281367808580399 Validation loss 0.09687421470880508 Accuracy 0.7944999933242798\n",
      "Iteration 44280 Training loss 0.0923445001244545 Validation loss 0.07464970648288727 Accuracy 0.8379999995231628\n",
      "Iteration 44290 Training loss 0.08095426857471466 Validation loss 0.07131475955247879 Accuracy 0.8504999876022339\n",
      "Iteration 44300 Training loss 0.09145506471395493 Validation loss 0.07246723026037216 Accuracy 0.847000002861023\n",
      "Iteration 44310 Training loss 0.0859515592455864 Validation loss 0.07146389782428741 Accuracy 0.8464999794960022\n",
      "Iteration 44320 Training loss 0.07962671667337418 Validation loss 0.07269654422998428 Accuracy 0.8450000286102295\n",
      "Iteration 44330 Training loss 0.10083860903978348 Validation loss 0.10525090247392654 Accuracy 0.7754999995231628\n",
      "Iteration 44340 Training loss 0.0880652517080307 Validation loss 0.0718764215707779 Accuracy 0.8464999794960022\n",
      "Iteration 44350 Training loss 0.08683774620294571 Validation loss 0.07384717464447021 Accuracy 0.8420000076293945\n",
      "Iteration 44360 Training loss 0.08976706117391586 Validation loss 0.0740998387336731 Accuracy 0.8454999923706055\n",
      "Iteration 44370 Training loss 0.08380070328712463 Validation loss 0.0735877975821495 Accuracy 0.8399999737739563\n",
      "Iteration 44380 Training loss 0.08872378617525101 Validation loss 0.07347748428583145 Accuracy 0.843500018119812\n",
      "Iteration 44390 Training loss 0.08460652083158493 Validation loss 0.07075702399015427 Accuracy 0.8475000262260437\n",
      "Iteration 44400 Training loss 0.09304312616586685 Validation loss 0.07665464282035828 Accuracy 0.8385000228881836\n",
      "Iteration 44410 Training loss 0.09360839426517487 Validation loss 0.07207018882036209 Accuracy 0.8479999899864197\n",
      "Iteration 44420 Training loss 0.08902033418416977 Validation loss 0.07324355840682983 Accuracy 0.8429999947547913\n",
      "Iteration 44430 Training loss 0.09072142094373703 Validation loss 0.07642067223787308 Accuracy 0.8364999890327454\n",
      "Iteration 44440 Training loss 0.09602533280849457 Validation loss 0.0779259130358696 Accuracy 0.8345000147819519\n",
      "Iteration 44450 Training loss 0.0926719531416893 Validation loss 0.07238627970218658 Accuracy 0.8420000076293945\n",
      "Iteration 44460 Training loss 0.10158747434616089 Validation loss 0.09967818111181259 Accuracy 0.7875000238418579\n",
      "Iteration 44470 Training loss 0.11065984517335892 Validation loss 0.10980290174484253 Accuracy 0.7689999938011169\n",
      "Iteration 44480 Training loss 0.0878954827785492 Validation loss 0.0733092874288559 Accuracy 0.8399999737739563\n",
      "Iteration 44490 Training loss 0.09403455257415771 Validation loss 0.0728747695684433 Accuracy 0.8450000286102295\n",
      "Iteration 44500 Training loss 0.09863819926977158 Validation loss 0.07553308457136154 Accuracy 0.8385000228881836\n",
      "Iteration 44510 Training loss 0.0827745646238327 Validation loss 0.0710010677576065 Accuracy 0.847000002861023\n",
      "Iteration 44520 Training loss 0.08687666803598404 Validation loss 0.07174853980541229 Accuracy 0.8464999794960022\n",
      "Iteration 44530 Training loss 0.08234190195798874 Validation loss 0.08203034847974777 Accuracy 0.8255000114440918\n",
      "Iteration 44540 Training loss 0.08353860676288605 Validation loss 0.08654839545488358 Accuracy 0.8165000081062317\n",
      "Iteration 44550 Training loss 0.0862397849559784 Validation loss 0.0723448097705841 Accuracy 0.843999981880188\n",
      "Iteration 44560 Training loss 0.08708672225475311 Validation loss 0.07168659567832947 Accuracy 0.843500018119812\n",
      "Iteration 44570 Training loss 0.10098443925380707 Validation loss 0.07401596009731293 Accuracy 0.8399999737739563\n",
      "Iteration 44580 Training loss 0.09828092902898788 Validation loss 0.079506516456604 Accuracy 0.8270000219345093\n",
      "Iteration 44590 Training loss 0.09369122236967087 Validation loss 0.07252167910337448 Accuracy 0.8475000262260437\n",
      "Iteration 44600 Training loss 0.09465759992599487 Validation loss 0.07899308949708939 Accuracy 0.8299999833106995\n",
      "Iteration 44610 Training loss 0.09670283645391464 Validation loss 0.08138609677553177 Accuracy 0.8234999775886536\n",
      "Iteration 44620 Training loss 0.08157818764448166 Validation loss 0.07280188053846359 Accuracy 0.843500018119812\n",
      "Iteration 44630 Training loss 0.08647686243057251 Validation loss 0.08881984651088715 Accuracy 0.8130000233650208\n",
      "Iteration 44640 Training loss 0.0842495709657669 Validation loss 0.07304897904396057 Accuracy 0.8420000076293945\n",
      "Iteration 44650 Training loss 0.08562083542346954 Validation loss 0.08115780353546143 Accuracy 0.8309999704360962\n",
      "Iteration 44660 Training loss 0.07969100028276443 Validation loss 0.0753563717007637 Accuracy 0.8389999866485596\n",
      "Iteration 44670 Training loss 0.09125521779060364 Validation loss 0.07490313798189163 Accuracy 0.8395000100135803\n",
      "Iteration 44680 Training loss 0.09105478972196579 Validation loss 0.07180287688970566 Accuracy 0.8460000157356262\n",
      "Iteration 44690 Training loss 0.08644852787256241 Validation loss 0.0729183554649353 Accuracy 0.8424999713897705\n",
      "Iteration 44700 Training loss 0.09114766865968704 Validation loss 0.0984124168753624 Accuracy 0.7875000238418579\n",
      "Iteration 44710 Training loss 0.08966074138879776 Validation loss 0.09199594706296921 Accuracy 0.8044999837875366\n",
      "Iteration 44720 Training loss 0.08289576321840286 Validation loss 0.07543591409921646 Accuracy 0.8379999995231628\n",
      "Iteration 44730 Training loss 0.08969002962112427 Validation loss 0.09001626819372177 Accuracy 0.809499979019165\n",
      "Iteration 44740 Training loss 0.09636332839727402 Validation loss 0.10099122673273087 Accuracy 0.7829999923706055\n",
      "Iteration 44750 Training loss 0.08993858098983765 Validation loss 0.08078347891569138 Accuracy 0.8299999833106995\n",
      "Iteration 44760 Training loss 0.10247249156236649 Validation loss 0.10609454661607742 Accuracy 0.7770000100135803\n",
      "Iteration 44770 Training loss 0.08830469846725464 Validation loss 0.07762263715267181 Accuracy 0.8370000123977661\n",
      "Iteration 44780 Training loss 0.08960549533367157 Validation loss 0.0725291445851326 Accuracy 0.8424999713897705\n",
      "Iteration 44790 Training loss 0.08622213453054428 Validation loss 0.07231655716896057 Accuracy 0.8450000286102295\n",
      "Iteration 44800 Training loss 0.09476756304502487 Validation loss 0.10478122532367706 Accuracy 0.7774999737739563\n",
      "Iteration 44810 Training loss 0.08728377521038055 Validation loss 0.07144702970981598 Accuracy 0.8454999923706055\n",
      "Iteration 44820 Training loss 0.08328604698181152 Validation loss 0.07206109166145325 Accuracy 0.8454999923706055\n",
      "Iteration 44830 Training loss 0.10753347724676132 Validation loss 0.1157197654247284 Accuracy 0.7584999799728394\n",
      "Iteration 44840 Training loss 0.10034167021512985 Validation loss 0.08185212314128876 Accuracy 0.8240000009536743\n",
      "Iteration 44850 Training loss 0.0884796753525734 Validation loss 0.0753883495926857 Accuracy 0.8399999737739563\n",
      "Iteration 44860 Training loss 0.08455656468868256 Validation loss 0.0727807953953743 Accuracy 0.8454999923706055\n",
      "Iteration 44870 Training loss 0.08538760244846344 Validation loss 0.07282479852437973 Accuracy 0.8445000052452087\n",
      "Iteration 44880 Training loss 0.08629470318555832 Validation loss 0.08416848629713058 Accuracy 0.8215000033378601\n",
      "Iteration 44890 Training loss 0.09049734473228455 Validation loss 0.08789310604333878 Accuracy 0.8130000233650208\n",
      "Iteration 44900 Training loss 0.0951298177242279 Validation loss 0.10179445147514343 Accuracy 0.7795000076293945\n",
      "Iteration 44910 Training loss 0.08467221260070801 Validation loss 0.07715953141450882 Accuracy 0.8385000228881836\n",
      "Iteration 44920 Training loss 0.08181475847959518 Validation loss 0.07748263329267502 Accuracy 0.8370000123977661\n",
      "Iteration 44930 Training loss 0.0800020769238472 Validation loss 0.07178114354610443 Accuracy 0.8454999923706055\n",
      "Iteration 44940 Training loss 0.07833792269229889 Validation loss 0.07472971081733704 Accuracy 0.8370000123977661\n",
      "Iteration 44950 Training loss 0.08684176951646805 Validation loss 0.07219219207763672 Accuracy 0.8450000286102295\n",
      "Iteration 44960 Training loss 0.10129813849925995 Validation loss 0.08027888834476471 Accuracy 0.8274999856948853\n",
      "Iteration 44970 Training loss 0.09693530201911926 Validation loss 0.07468318194150925 Accuracy 0.8389999866485596\n",
      "Iteration 44980 Training loss 0.11481980979442596 Validation loss 0.09001357108354568 Accuracy 0.8069999814033508\n",
      "Iteration 44990 Training loss 0.09289537370204926 Validation loss 0.0727958083152771 Accuracy 0.843500018119812\n",
      "Iteration 45000 Training loss 0.07737267762422562 Validation loss 0.08405932784080505 Accuracy 0.8220000267028809\n",
      "Iteration 45010 Training loss 0.08618280291557312 Validation loss 0.08188756555318832 Accuracy 0.8274999856948853\n",
      "Iteration 45020 Training loss 0.1119488924741745 Validation loss 0.13019169867038727 Accuracy 0.7304999828338623\n",
      "Iteration 45030 Training loss 0.08787175267934799 Validation loss 0.07358331978321075 Accuracy 0.8414999842643738\n",
      "Iteration 45040 Training loss 0.10050519555807114 Validation loss 0.07909603416919708 Accuracy 0.8305000066757202\n",
      "Iteration 45050 Training loss 0.0822053775191307 Validation loss 0.07352317869663239 Accuracy 0.8385000228881836\n",
      "Iteration 45060 Training loss 0.08942065387964249 Validation loss 0.09138012677431107 Accuracy 0.8044999837875366\n",
      "Iteration 45070 Training loss 0.08681011945009232 Validation loss 0.07817061245441437 Accuracy 0.8345000147819519\n",
      "Iteration 45080 Training loss 0.0907401591539383 Validation loss 0.07257193326950073 Accuracy 0.8454999923706055\n",
      "Iteration 45090 Training loss 0.1063503697514534 Validation loss 0.08891919255256653 Accuracy 0.8084999918937683\n",
      "Iteration 45100 Training loss 0.08388365060091019 Validation loss 0.07269115000963211 Accuracy 0.8410000205039978\n",
      "Iteration 45110 Training loss 0.08687037974596024 Validation loss 0.09085506945848465 Accuracy 0.8054999709129333\n",
      "Iteration 45120 Training loss 0.07801109552383423 Validation loss 0.08227317780256271 Accuracy 0.8289999961853027\n",
      "Iteration 45130 Training loss 0.08314396440982819 Validation loss 0.07167120277881622 Accuracy 0.8479999899864197\n",
      "Iteration 45140 Training loss 0.08413779735565186 Validation loss 0.07747060805559158 Accuracy 0.8355000019073486\n",
      "Iteration 45150 Training loss 0.08415473997592926 Validation loss 0.07230202853679657 Accuracy 0.8429999947547913\n",
      "Iteration 45160 Training loss 0.1047242060303688 Validation loss 0.10432328283786774 Accuracy 0.7795000076293945\n",
      "Iteration 45170 Training loss 0.09300059825181961 Validation loss 0.09946119785308838 Accuracy 0.7900000214576721\n",
      "Iteration 45180 Training loss 0.08733303844928741 Validation loss 0.08145119249820709 Accuracy 0.8264999985694885\n",
      "Iteration 45190 Training loss 0.08884400874376297 Validation loss 0.0715496838092804 Accuracy 0.8454999923706055\n",
      "Iteration 45200 Training loss 0.08221829682588577 Validation loss 0.07242605835199356 Accuracy 0.8445000052452087\n",
      "Iteration 45210 Training loss 0.09411358833312988 Validation loss 0.07436571270227432 Accuracy 0.8399999737739563\n",
      "Iteration 45220 Training loss 0.09579121321439743 Validation loss 0.08140935003757477 Accuracy 0.8224999904632568\n",
      "Iteration 45230 Training loss 0.0893864780664444 Validation loss 0.07392744719982147 Accuracy 0.8414999842643738\n",
      "Iteration 45240 Training loss 0.0820896178483963 Validation loss 0.07173120975494385 Accuracy 0.8450000286102295\n",
      "Iteration 45250 Training loss 0.09853123128414154 Validation loss 0.07610571384429932 Accuracy 0.8410000205039978\n",
      "Iteration 45260 Training loss 0.08161008358001709 Validation loss 0.07539661973714828 Accuracy 0.8385000228881836\n",
      "Iteration 45270 Training loss 0.08622564375400543 Validation loss 0.08780957013368607 Accuracy 0.8125\n",
      "Iteration 45280 Training loss 0.10801547020673752 Validation loss 0.11081062257289886 Accuracy 0.7670000195503235\n",
      "Iteration 45290 Training loss 0.08143194764852524 Validation loss 0.08350210636854172 Accuracy 0.824999988079071\n",
      "Iteration 45300 Training loss 0.09290072321891785 Validation loss 0.0718916580080986 Accuracy 0.847000002861023\n",
      "Iteration 45310 Training loss 0.09101960062980652 Validation loss 0.08574268966913223 Accuracy 0.8195000290870667\n",
      "Iteration 45320 Training loss 0.08307894319295883 Validation loss 0.07107717543840408 Accuracy 0.8460000157356262\n",
      "Iteration 45330 Training loss 0.09487587958574295 Validation loss 0.10105781257152557 Accuracy 0.7825000286102295\n",
      "Iteration 45340 Training loss 0.11974240094423294 Validation loss 0.10049384832382202 Accuracy 0.7889999747276306\n",
      "Iteration 45350 Training loss 0.08900657296180725 Validation loss 0.0737481489777565 Accuracy 0.8429999947547913\n",
      "Iteration 45360 Training loss 0.07984253764152527 Validation loss 0.07499110698699951 Accuracy 0.8399999737739563\n",
      "Iteration 45370 Training loss 0.09103874862194061 Validation loss 0.07370191812515259 Accuracy 0.8395000100135803\n",
      "Iteration 45380 Training loss 0.08522970229387283 Validation loss 0.07239779084920883 Accuracy 0.843999981880188\n",
      "Iteration 45390 Training loss 0.0969293862581253 Validation loss 0.09350642561912537 Accuracy 0.7994999885559082\n",
      "Iteration 45400 Training loss 0.09363111853599548 Validation loss 0.09894243627786636 Accuracy 0.7879999876022339\n",
      "Iteration 45410 Training loss 0.09343936294317245 Validation loss 0.08006556332111359 Accuracy 0.8305000066757202\n",
      "Iteration 45420 Training loss 0.08504539728164673 Validation loss 0.07856301218271255 Accuracy 0.8330000042915344\n",
      "Iteration 45430 Training loss 0.09102166444063187 Validation loss 0.07323138415813446 Accuracy 0.8429999947547913\n",
      "Iteration 45440 Training loss 0.09392187744379044 Validation loss 0.105990469455719 Accuracy 0.7754999995231628\n",
      "Iteration 45450 Training loss 0.10005419701337814 Validation loss 0.10100527852773666 Accuracy 0.7864999771118164\n",
      "Iteration 45460 Training loss 0.0905415415763855 Validation loss 0.09434124827384949 Accuracy 0.7994999885559082\n",
      "Iteration 45470 Training loss 0.1030082255601883 Validation loss 0.08152864128351212 Accuracy 0.8230000138282776\n",
      "Iteration 45480 Training loss 0.08591925352811813 Validation loss 0.07301633805036545 Accuracy 0.8450000286102295\n",
      "Iteration 45490 Training loss 0.08532079309225082 Validation loss 0.07341570407152176 Accuracy 0.8420000076293945\n",
      "Iteration 45500 Training loss 0.09791270643472672 Validation loss 0.08246724307537079 Accuracy 0.8224999904632568\n",
      "Iteration 45510 Training loss 0.09004905819892883 Validation loss 0.07434336841106415 Accuracy 0.8424999713897705\n",
      "Iteration 45520 Training loss 0.08529792726039886 Validation loss 0.07518038898706436 Accuracy 0.8389999866485596\n",
      "Iteration 45530 Training loss 0.08334929496049881 Validation loss 0.07196494936943054 Accuracy 0.8429999947547913\n",
      "Iteration 45540 Training loss 0.1014397144317627 Validation loss 0.07950367778539658 Accuracy 0.828499972820282\n",
      "Iteration 45550 Training loss 0.09184735268354416 Validation loss 0.0732220932841301 Accuracy 0.8450000286102295\n",
      "Iteration 45560 Training loss 0.09080617129802704 Validation loss 0.07162733376026154 Accuracy 0.8485000133514404\n",
      "Iteration 45570 Training loss 0.07975497096776962 Validation loss 0.0709143653512001 Accuracy 0.8485000133514404\n",
      "Iteration 45580 Training loss 0.08503861725330353 Validation loss 0.07325728237628937 Accuracy 0.843500018119812\n",
      "Iteration 45590 Training loss 0.08798705786466599 Validation loss 0.07493714243173599 Accuracy 0.8399999737739563\n",
      "Iteration 45600 Training loss 0.07904675602912903 Validation loss 0.07689514011144638 Accuracy 0.8370000123977661\n",
      "Iteration 45610 Training loss 0.08507837355136871 Validation loss 0.07195860892534256 Accuracy 0.8445000052452087\n",
      "Iteration 45620 Training loss 0.0814327672123909 Validation loss 0.07161244004964828 Accuracy 0.8429999947547913\n",
      "Iteration 45630 Training loss 0.08305907249450684 Validation loss 0.07113371044397354 Accuracy 0.8485000133514404\n",
      "Iteration 45640 Training loss 0.09781455248594284 Validation loss 0.10243632644414902 Accuracy 0.7825000286102295\n",
      "Iteration 45650 Training loss 0.0895354151725769 Validation loss 0.08379471302032471 Accuracy 0.824999988079071\n",
      "Iteration 45660 Training loss 0.08895671367645264 Validation loss 0.08797962963581085 Accuracy 0.8149999976158142\n",
      "Iteration 45670 Training loss 0.0797910988330841 Validation loss 0.07290522754192352 Accuracy 0.8420000076293945\n",
      "Iteration 45680 Training loss 0.09772372990846634 Validation loss 0.10647280514240265 Accuracy 0.7749999761581421\n",
      "Iteration 45690 Training loss 0.09505875408649445 Validation loss 0.09964614361524582 Accuracy 0.7885000109672546\n",
      "Iteration 45700 Training loss 0.07928123325109482 Validation loss 0.07879102230072021 Accuracy 0.8330000042915344\n",
      "Iteration 45710 Training loss 0.08528322726488113 Validation loss 0.07515598833560944 Accuracy 0.8389999866485596\n",
      "Iteration 45720 Training loss 0.09068615734577179 Validation loss 0.10263828933238983 Accuracy 0.7785000205039978\n",
      "Iteration 45730 Training loss 0.10567173361778259 Validation loss 0.10756249725818634 Accuracy 0.7710000276565552\n",
      "Iteration 45740 Training loss 0.08519192785024643 Validation loss 0.09000654518604279 Accuracy 0.8100000023841858\n",
      "Iteration 45750 Training loss 0.08236154168844223 Validation loss 0.08091837167739868 Accuracy 0.8305000066757202\n",
      "Iteration 45760 Training loss 0.08736080676317215 Validation loss 0.0736503005027771 Accuracy 0.8395000100135803\n",
      "Iteration 45770 Training loss 0.08627643436193466 Validation loss 0.07654522359371185 Accuracy 0.8349999785423279\n",
      "Iteration 45780 Training loss 0.0872836783528328 Validation loss 0.09464463591575623 Accuracy 0.7990000247955322\n",
      "Iteration 45790 Training loss 0.09462589770555496 Validation loss 0.09468510746955872 Accuracy 0.8015000224113464\n",
      "Iteration 45800 Training loss 0.10718623548746109 Validation loss 0.11559286713600159 Accuracy 0.7590000033378601\n",
      "Iteration 45810 Training loss 0.08528561145067215 Validation loss 0.0908392146229744 Accuracy 0.8080000281333923\n",
      "Iteration 45820 Training loss 0.08000706136226654 Validation loss 0.07412043958902359 Accuracy 0.840499997138977\n",
      "Iteration 45830 Training loss 0.08385727554559708 Validation loss 0.07201404869556427 Accuracy 0.8450000286102295\n",
      "Iteration 45840 Training loss 0.11234978586435318 Validation loss 0.11492002755403519 Accuracy 0.7599999904632568\n",
      "Iteration 45850 Training loss 0.0785301923751831 Validation loss 0.07521247863769531 Accuracy 0.8379999995231628\n",
      "Iteration 45860 Training loss 0.08601155132055283 Validation loss 0.08610807359218597 Accuracy 0.8174999952316284\n",
      "Iteration 45870 Training loss 0.09695614129304886 Validation loss 0.09459114074707031 Accuracy 0.7990000247955322\n",
      "Iteration 45880 Training loss 0.08423858880996704 Validation loss 0.07674431055784225 Accuracy 0.8370000123977661\n",
      "Iteration 45890 Training loss 0.07535194605588913 Validation loss 0.0783209279179573 Accuracy 0.8335000276565552\n",
      "Iteration 45900 Training loss 0.09295900166034698 Validation loss 0.10374372452497482 Accuracy 0.777999997138977\n",
      "Iteration 45910 Training loss 0.09662377834320068 Validation loss 0.10540187358856201 Accuracy 0.7760000228881836\n",
      "Iteration 45920 Training loss 0.079177126288414 Validation loss 0.074021115899086 Accuracy 0.8395000100135803\n",
      "Iteration 45930 Training loss 0.08905939012765884 Validation loss 0.0741167888045311 Accuracy 0.8410000205039978\n",
      "Iteration 45940 Training loss 0.08150497078895569 Validation loss 0.07229617983102798 Accuracy 0.8429999947547913\n",
      "Iteration 45950 Training loss 0.08987752348184586 Validation loss 0.07244028896093369 Accuracy 0.8485000133514404\n",
      "Iteration 45960 Training loss 0.08072502166032791 Validation loss 0.07284543663263321 Accuracy 0.8414999842643738\n",
      "Iteration 45970 Training loss 0.08429700881242752 Validation loss 0.07113005220890045 Accuracy 0.8464999794960022\n",
      "Iteration 45980 Training loss 0.09962361305952072 Validation loss 0.08005280047655106 Accuracy 0.828000009059906\n",
      "Iteration 45990 Training loss 0.10135234147310257 Validation loss 0.08004853874444962 Accuracy 0.828000009059906\n",
      "Iteration 46000 Training loss 0.08238101750612259 Validation loss 0.0754382312297821 Accuracy 0.8364999890327454\n",
      "Iteration 46010 Training loss 0.08360529690980911 Validation loss 0.07278800755739212 Accuracy 0.843500018119812\n",
      "Iteration 46020 Training loss 0.08176478743553162 Validation loss 0.07329270988702774 Accuracy 0.8424999713897705\n",
      "Iteration 46030 Training loss 0.08733239769935608 Validation loss 0.08412562310695648 Accuracy 0.8234999775886536\n",
      "Iteration 46040 Training loss 0.08399414271116257 Validation loss 0.07997008413076401 Accuracy 0.828499972820282\n",
      "Iteration 46050 Training loss 0.08987406641244888 Validation loss 0.09720049053430557 Accuracy 0.7950000166893005\n",
      "Iteration 46060 Training loss 0.09617993235588074 Validation loss 0.09802015870809555 Accuracy 0.7935000061988831\n",
      "Iteration 46070 Training loss 0.08232830464839935 Validation loss 0.08415134996175766 Accuracy 0.8245000243186951\n",
      "Iteration 46080 Training loss 0.09248732030391693 Validation loss 0.10871753096580505 Accuracy 0.7695000171661377\n",
      "Iteration 46090 Training loss 0.083824522793293 Validation loss 0.09805769473314285 Accuracy 0.7904999852180481\n",
      "Iteration 46100 Training loss 0.08681049197912216 Validation loss 0.07395263016223907 Accuracy 0.840499997138977\n",
      "Iteration 46110 Training loss 0.09845313429832458 Validation loss 0.07829121500253677 Accuracy 0.8324999809265137\n",
      "Iteration 46120 Training loss 0.0892309918999672 Validation loss 0.09326997399330139 Accuracy 0.8015000224113464\n",
      "Iteration 46130 Training loss 0.08749750256538391 Validation loss 0.09463414549827576 Accuracy 0.796999990940094\n",
      "Iteration 46140 Training loss 0.10339439660310745 Validation loss 0.11546682566404343 Accuracy 0.7584999799728394\n",
      "Iteration 46150 Training loss 0.08638735115528107 Validation loss 0.08407717943191528 Accuracy 0.8234999775886536\n",
      "Iteration 46160 Training loss 0.09525126963853836 Validation loss 0.1074378490447998 Accuracy 0.7730000019073486\n",
      "Iteration 46170 Training loss 0.09496814757585526 Validation loss 0.10031953454017639 Accuracy 0.7854999899864197\n",
      "Iteration 46180 Training loss 0.08342766761779785 Validation loss 0.08063636720180511 Accuracy 0.828499972820282\n",
      "Iteration 46190 Training loss 0.07819195091724396 Validation loss 0.0802532210946083 Accuracy 0.8330000042915344\n",
      "Iteration 46200 Training loss 0.09623608738183975 Validation loss 0.10386931151151657 Accuracy 0.7785000205039978\n",
      "Iteration 46210 Training loss 0.08763048052787781 Validation loss 0.09524301439523697 Accuracy 0.7975000143051147\n",
      "Iteration 46220 Training loss 0.09686393290758133 Validation loss 0.11541647464036942 Accuracy 0.7574999928474426\n",
      "Iteration 46230 Training loss 0.08802756667137146 Validation loss 0.09206368029117584 Accuracy 0.8040000200271606\n",
      "Iteration 46240 Training loss 0.10239110887050629 Validation loss 0.11412730067968369 Accuracy 0.7609999775886536\n",
      "Iteration 46250 Training loss 0.08457852154970169 Validation loss 0.07989832758903503 Accuracy 0.8299999833106995\n",
      "Iteration 46260 Training loss 0.07526761293411255 Validation loss 0.08120695501565933 Accuracy 0.828000009059906\n",
      "Iteration 46270 Training loss 0.09879252314567566 Validation loss 0.07882281392812729 Accuracy 0.828499972820282\n",
      "Iteration 46280 Training loss 0.09295995533466339 Validation loss 0.07302026450634003 Accuracy 0.8460000157356262\n",
      "Iteration 46290 Training loss 0.08806063234806061 Validation loss 0.07189340144395828 Accuracy 0.8445000052452087\n",
      "Iteration 46300 Training loss 0.08484656363725662 Validation loss 0.07191314548254013 Accuracy 0.843500018119812\n",
      "Iteration 46310 Training loss 0.09051141142845154 Validation loss 0.06986437737941742 Accuracy 0.8489999771118164\n",
      "Iteration 46320 Training loss 0.082795150578022 Validation loss 0.07122199982404709 Accuracy 0.8464999794960022\n",
      "Iteration 46330 Training loss 0.08213810622692108 Validation loss 0.09522980451583862 Accuracy 0.7979999780654907\n",
      "Iteration 46340 Training loss 0.08150454610586166 Validation loss 0.0735604390501976 Accuracy 0.840499997138977\n",
      "Iteration 46350 Training loss 0.08083748072385788 Validation loss 0.07000499218702316 Accuracy 0.8504999876022339\n",
      "Iteration 46360 Training loss 0.08451952040195465 Validation loss 0.092493437230587 Accuracy 0.8034999966621399\n",
      "Iteration 46370 Training loss 0.08715646713972092 Validation loss 0.08800492435693741 Accuracy 0.8105000257492065\n",
      "Iteration 46380 Training loss 0.09708785265684128 Validation loss 0.07304571568965912 Accuracy 0.843500018119812\n",
      "Iteration 46390 Training loss 0.09407053887844086 Validation loss 0.07573992758989334 Accuracy 0.8364999890327454\n",
      "Iteration 46400 Training loss 0.08649446070194244 Validation loss 0.07018394023180008 Accuracy 0.8489999771118164\n",
      "Iteration 46410 Training loss 0.08509457856416702 Validation loss 0.07270284742116928 Accuracy 0.843500018119812\n",
      "Iteration 46420 Training loss 0.08341016620397568 Validation loss 0.07035791128873825 Accuracy 0.8479999899864197\n",
      "Iteration 46430 Training loss 0.10644806176424026 Validation loss 0.07914767414331436 Accuracy 0.8299999833106995\n",
      "Iteration 46440 Training loss 0.07914827018976212 Validation loss 0.07533230632543564 Accuracy 0.8360000252723694\n",
      "Iteration 46450 Training loss 0.09974690526723862 Validation loss 0.11291429400444031 Accuracy 0.7639999985694885\n",
      "Iteration 46460 Training loss 0.0912405326962471 Validation loss 0.09487392008304596 Accuracy 0.7979999780654907\n",
      "Iteration 46470 Training loss 0.08571048080921173 Validation loss 0.08176934719085693 Accuracy 0.8274999856948853\n",
      "Iteration 46480 Training loss 0.08001647144556046 Validation loss 0.08098461478948593 Accuracy 0.828499972820282\n",
      "Iteration 46490 Training loss 0.09616144746541977 Validation loss 0.10520074516534805 Accuracy 0.777999997138977\n",
      "Iteration 46500 Training loss 0.099001944065094 Validation loss 0.11021754145622253 Accuracy 0.765999972820282\n",
      "Iteration 46510 Training loss 0.08374963700771332 Validation loss 0.07278475910425186 Accuracy 0.8424999713897705\n",
      "Iteration 46520 Training loss 0.08054766803979874 Validation loss 0.07985499501228333 Accuracy 0.8324999809265137\n",
      "Iteration 46530 Training loss 0.0964403823018074 Validation loss 0.10980061441659927 Accuracy 0.7699999809265137\n",
      "Iteration 46540 Training loss 0.09138208627700806 Validation loss 0.08097703754901886 Accuracy 0.8295000195503235\n",
      "Iteration 46550 Training loss 0.09926454722881317 Validation loss 0.1142740249633789 Accuracy 0.7615000009536743\n",
      "Iteration 46560 Training loss 0.09428920596837997 Validation loss 0.10935135185718536 Accuracy 0.7689999938011169\n",
      "Iteration 46570 Training loss 0.07635993510484695 Validation loss 0.08703956753015518 Accuracy 0.8144999742507935\n",
      "Iteration 46580 Training loss 0.09050478786230087 Validation loss 0.09844614565372467 Accuracy 0.7904999852180481\n",
      "Iteration 46590 Training loss 0.08560584485530853 Validation loss 0.08909861743450165 Accuracy 0.8109999895095825\n",
      "Iteration 46600 Training loss 0.10531923174858093 Validation loss 0.12042663246393204 Accuracy 0.7490000128746033\n",
      "Iteration 46610 Training loss 0.08247274905443192 Validation loss 0.08185325562953949 Accuracy 0.828499972820282\n",
      "Iteration 46620 Training loss 0.08692468702793121 Validation loss 0.0731235221028328 Accuracy 0.8450000286102295\n",
      "Iteration 46630 Training loss 0.07832682132720947 Validation loss 0.07787347584962845 Accuracy 0.8349999785423279\n",
      "Iteration 46640 Training loss 0.09188452363014221 Validation loss 0.10072486102581024 Accuracy 0.7854999899864197\n",
      "Iteration 46650 Training loss 0.08965499699115753 Validation loss 0.08358514308929443 Accuracy 0.8240000009536743\n",
      "Iteration 46660 Training loss 0.07936041057109833 Validation loss 0.08056793361902237 Accuracy 0.8320000171661377\n",
      "Iteration 46670 Training loss 0.07935500144958496 Validation loss 0.07040251046419144 Accuracy 0.847000002861023\n",
      "Iteration 46680 Training loss 0.11178150027990341 Validation loss 0.08507023006677628 Accuracy 0.8134999871253967\n",
      "Iteration 46690 Training loss 0.08515729010105133 Validation loss 0.07316138595342636 Accuracy 0.8460000157356262\n",
      "Iteration 46700 Training loss 0.10585979372262955 Validation loss 0.11241583526134491 Accuracy 0.7670000195503235\n",
      "Iteration 46710 Training loss 0.081741102039814 Validation loss 0.07395529001951218 Accuracy 0.840499997138977\n",
      "Iteration 46720 Training loss 0.10323696583509445 Validation loss 0.07191149890422821 Accuracy 0.8489999771118164\n",
      "Iteration 46730 Training loss 0.08537960797548294 Validation loss 0.07046835124492645 Accuracy 0.8504999876022339\n",
      "Iteration 46740 Training loss 0.09708260744810104 Validation loss 0.0771835446357727 Accuracy 0.8345000147819519\n",
      "Iteration 46750 Training loss 0.08151658624410629 Validation loss 0.08729081600904465 Accuracy 0.8134999871253967\n",
      "Iteration 46760 Training loss 0.08493848890066147 Validation loss 0.09285937994718552 Accuracy 0.8044999837875366\n",
      "Iteration 46770 Training loss 0.08819050341844559 Validation loss 0.093927763402462 Accuracy 0.7990000247955322\n",
      "Iteration 46780 Training loss 0.07920389622449875 Validation loss 0.07962722331285477 Accuracy 0.8330000042915344\n",
      "Iteration 46790 Training loss 0.08197270333766937 Validation loss 0.08867011964321136 Accuracy 0.8105000257492065\n",
      "Iteration 46800 Training loss 0.09143847972154617 Validation loss 0.07284002006053925 Accuracy 0.843999981880188\n",
      "Iteration 46810 Training loss 0.08811724931001663 Validation loss 0.07255510985851288 Accuracy 0.8410000205039978\n",
      "Iteration 46820 Training loss 0.09331835806369781 Validation loss 0.07225840538740158 Accuracy 0.8464999794960022\n",
      "Iteration 46830 Training loss 0.08380439132452011 Validation loss 0.07195095717906952 Accuracy 0.8479999899864197\n",
      "Iteration 46840 Training loss 0.08609684556722641 Validation loss 0.0745721310377121 Accuracy 0.8395000100135803\n",
      "Iteration 46850 Training loss 0.09043261408805847 Validation loss 0.07199230045080185 Accuracy 0.847000002861023\n",
      "Iteration 46860 Training loss 0.0735725536942482 Validation loss 0.0771525576710701 Accuracy 0.8360000252723694\n",
      "Iteration 46870 Training loss 0.08591170608997345 Validation loss 0.09451362490653992 Accuracy 0.7994999885559082\n",
      "Iteration 46880 Training loss 0.10021065920591354 Validation loss 0.11489058285951614 Accuracy 0.7605000138282776\n",
      "Iteration 46890 Training loss 0.08081512153148651 Validation loss 0.08054237067699432 Accuracy 0.8289999961853027\n",
      "Iteration 46900 Training loss 0.07960338145494461 Validation loss 0.07595283538103104 Accuracy 0.8374999761581421\n",
      "Iteration 46910 Training loss 0.07803681492805481 Validation loss 0.07056690752506256 Accuracy 0.847000002861023\n",
      "Iteration 46920 Training loss 0.09598278254270554 Validation loss 0.0719350203871727 Accuracy 0.843500018119812\n",
      "Iteration 46930 Training loss 0.08929213136434555 Validation loss 0.07311763614416122 Accuracy 0.8410000205039978\n",
      "Iteration 46940 Training loss 0.08922167122364044 Validation loss 0.07312268018722534 Accuracy 0.8424999713897705\n",
      "Iteration 46950 Training loss 0.09009823948144913 Validation loss 0.0715542733669281 Accuracy 0.8475000262260437\n",
      "Iteration 46960 Training loss 0.08083513379096985 Validation loss 0.07227001339197159 Accuracy 0.843500018119812\n",
      "Iteration 46970 Training loss 0.09823151677846909 Validation loss 0.11239558458328247 Accuracy 0.7639999985694885\n",
      "Iteration 46980 Training loss 0.08585383743047714 Validation loss 0.07655830681324005 Accuracy 0.8360000252723694\n",
      "Iteration 46990 Training loss 0.08332520723342896 Validation loss 0.09147540479898453 Accuracy 0.8054999709129333\n",
      "Iteration 47000 Training loss 0.1042524129152298 Validation loss 0.11837581545114517 Accuracy 0.7484999895095825\n",
      "Iteration 47010 Training loss 0.09692685306072235 Validation loss 0.1145891472697258 Accuracy 0.7605000138282776\n",
      "Iteration 47020 Training loss 0.0888989046216011 Validation loss 0.10202553123235703 Accuracy 0.781499981880188\n",
      "Iteration 47030 Training loss 0.08116301894187927 Validation loss 0.08033718168735504 Accuracy 0.8314999938011169\n",
      "Iteration 47040 Training loss 0.09088944643735886 Validation loss 0.10782188177108765 Accuracy 0.7739999890327454\n",
      "Iteration 47050 Training loss 0.08919267356395721 Validation loss 0.08227952569723129 Accuracy 0.8289999961853027\n",
      "Iteration 47060 Training loss 0.10194490104913712 Validation loss 0.11315629631280899 Accuracy 0.7615000009536743\n",
      "Iteration 47070 Training loss 0.09427031129598618 Validation loss 0.07236278802156448 Accuracy 0.8445000052452087\n",
      "Iteration 47080 Training loss 0.08799106627702713 Validation loss 0.07326508313417435 Accuracy 0.843500018119812\n",
      "Iteration 47090 Training loss 0.10710377246141434 Validation loss 0.08915229886770248 Accuracy 0.8090000152587891\n",
      "Iteration 47100 Training loss 0.08599251508712769 Validation loss 0.07135295122861862 Accuracy 0.847000002861023\n",
      "Iteration 47110 Training loss 0.08362914621829987 Validation loss 0.07096481323242188 Accuracy 0.8485000133514404\n",
      "Iteration 47120 Training loss 0.08292601257562637 Validation loss 0.07116612792015076 Accuracy 0.8460000157356262\n",
      "Iteration 47130 Training loss 0.0869474858045578 Validation loss 0.09197210520505905 Accuracy 0.8054999709129333\n",
      "Iteration 47140 Training loss 0.08401774615049362 Validation loss 0.08969366550445557 Accuracy 0.8084999918937683\n",
      "Iteration 47150 Training loss 0.08703135699033737 Validation loss 0.09650537371635437 Accuracy 0.7954999804496765\n",
      "Iteration 47160 Training loss 0.0884665995836258 Validation loss 0.10375498235225677 Accuracy 0.7774999737739563\n",
      "Iteration 47170 Training loss 0.08169136941432953 Validation loss 0.07835876196622849 Accuracy 0.8330000042915344\n",
      "Iteration 47180 Training loss 0.08050259202718735 Validation loss 0.0724058523774147 Accuracy 0.843500018119812\n",
      "Iteration 47190 Training loss 0.09415333718061447 Validation loss 0.07238856703042984 Accuracy 0.8485000133514404\n",
      "Iteration 47200 Training loss 0.0904315784573555 Validation loss 0.07073648273944855 Accuracy 0.8460000157356262\n",
      "Iteration 47210 Training loss 0.0944499522447586 Validation loss 0.07294607907533646 Accuracy 0.8420000076293945\n",
      "Iteration 47220 Training loss 0.0834098532795906 Validation loss 0.09584417939186096 Accuracy 0.7944999933242798\n",
      "Iteration 47230 Training loss 0.09184112399816513 Validation loss 0.09988722205162048 Accuracy 0.7894999980926514\n",
      "Iteration 47240 Training loss 0.08306653052568436 Validation loss 0.0838082879781723 Accuracy 0.8240000009536743\n",
      "Iteration 47250 Training loss 0.08153309673070908 Validation loss 0.07179983705282211 Accuracy 0.8454999923706055\n",
      "Iteration 47260 Training loss 0.08475831896066666 Validation loss 0.09546021372079849 Accuracy 0.796500027179718\n",
      "Iteration 47270 Training loss 0.09398694336414337 Validation loss 0.1115555688738823 Accuracy 0.7664999961853027\n",
      "Iteration 47280 Training loss 0.09199963510036469 Validation loss 0.10970783978700638 Accuracy 0.7710000276565552\n",
      "Iteration 47290 Training loss 0.09072841703891754 Validation loss 0.10257043689489365 Accuracy 0.781499981880188\n",
      "Iteration 47300 Training loss 0.08073614537715912 Validation loss 0.08082632720470428 Accuracy 0.8339999914169312\n",
      "Iteration 47310 Training loss 0.08245278149843216 Validation loss 0.09143639355897903 Accuracy 0.8065000176429749\n",
      "Iteration 47320 Training loss 0.0748709887266159 Validation loss 0.07565107941627502 Accuracy 0.8364999890327454\n",
      "Iteration 47330 Training loss 0.0944010317325592 Validation loss 0.07204221934080124 Accuracy 0.8454999923706055\n",
      "Iteration 47340 Training loss 0.09763166308403015 Validation loss 0.0724402368068695 Accuracy 0.8414999842643738\n",
      "Iteration 47350 Training loss 0.08381113409996033 Validation loss 0.07081828266382217 Accuracy 0.8485000133514404\n",
      "Iteration 47360 Training loss 0.09352205693721771 Validation loss 0.07158435136079788 Accuracy 0.8475000262260437\n",
      "Iteration 47370 Training loss 0.0906485915184021 Validation loss 0.07293423265218735 Accuracy 0.8445000052452087\n",
      "Iteration 47380 Training loss 0.08249977231025696 Validation loss 0.0715133547782898 Accuracy 0.8479999899864197\n",
      "Iteration 47390 Training loss 0.09328629076480865 Validation loss 0.07132422178983688 Accuracy 0.8445000052452087\n",
      "Iteration 47400 Training loss 0.0809539332985878 Validation loss 0.08003006130456924 Accuracy 0.8309999704360962\n",
      "Iteration 47410 Training loss 0.09734765440225601 Validation loss 0.11250579357147217 Accuracy 0.7664999961853027\n",
      "Iteration 47420 Training loss 0.08997433632612228 Validation loss 0.10349694639444351 Accuracy 0.781000018119812\n",
      "Iteration 47430 Training loss 0.10383454710245132 Validation loss 0.11819140613079071 Accuracy 0.7549999952316284\n",
      "Iteration 47440 Training loss 0.08132844418287277 Validation loss 0.09304481744766235 Accuracy 0.800000011920929\n",
      "Iteration 47450 Training loss 0.08390972763299942 Validation loss 0.08263636380434036 Accuracy 0.8270000219345093\n",
      "Iteration 47460 Training loss 0.08915463089942932 Validation loss 0.07189038395881653 Accuracy 0.847000002861023\n",
      "Iteration 47470 Training loss 0.09159347414970398 Validation loss 0.07227860391139984 Accuracy 0.843500018119812\n",
      "Iteration 47480 Training loss 0.0900118499994278 Validation loss 0.07182054221630096 Accuracy 0.8479999899864197\n",
      "Iteration 47490 Training loss 0.07358859479427338 Validation loss 0.07097923010587692 Accuracy 0.8454999923706055\n",
      "Iteration 47500 Training loss 0.091264508664608 Validation loss 0.07241630554199219 Accuracy 0.8429999947547913\n",
      "Iteration 47510 Training loss 0.09740980714559555 Validation loss 0.07305864989757538 Accuracy 0.8445000052452087\n",
      "Iteration 47520 Training loss 0.08611118048429489 Validation loss 0.07181808352470398 Accuracy 0.8445000052452087\n",
      "Iteration 47530 Training loss 0.0806102454662323 Validation loss 0.08097302168607712 Accuracy 0.8309999704360962\n",
      "Iteration 47540 Training loss 0.08170875906944275 Validation loss 0.0892733782529831 Accuracy 0.8109999895095825\n",
      "Iteration 47550 Training loss 0.11656059324741364 Validation loss 0.1319979727268219 Accuracy 0.7245000004768372\n",
      "Iteration 47560 Training loss 0.10149138420820236 Validation loss 0.11049167811870575 Accuracy 0.7670000195503235\n",
      "Iteration 47570 Training loss 0.08913453668355942 Validation loss 0.09671027213335037 Accuracy 0.7929999828338623\n",
      "Iteration 47580 Training loss 0.07450810074806213 Validation loss 0.07558762282133102 Accuracy 0.8360000252723694\n",
      "Iteration 47590 Training loss 0.08012782037258148 Validation loss 0.07223791629076004 Accuracy 0.8450000286102295\n",
      "Iteration 47600 Training loss 0.08740678429603577 Validation loss 0.07237651199102402 Accuracy 0.8475000262260437\n",
      "Iteration 47610 Training loss 0.08166828751564026 Validation loss 0.07218483090400696 Accuracy 0.843500018119812\n",
      "Iteration 47620 Training loss 0.07946591079235077 Validation loss 0.07998915761709213 Accuracy 0.8314999938011169\n",
      "Iteration 47630 Training loss 0.10041836649179459 Validation loss 0.07243609428405762 Accuracy 0.8460000157356262\n",
      "Iteration 47640 Training loss 0.07820188999176025 Validation loss 0.07243891805410385 Accuracy 0.8454999923706055\n",
      "Iteration 47650 Training loss 0.10890811681747437 Validation loss 0.07545005530118942 Accuracy 0.8364999890327454\n",
      "Iteration 47660 Training loss 0.07750347256660461 Validation loss 0.08260459452867508 Accuracy 0.8259999752044678\n",
      "Iteration 47670 Training loss 0.09921429306268692 Validation loss 0.07285737246274948 Accuracy 0.8429999947547913\n",
      "Iteration 47680 Training loss 0.08682626485824585 Validation loss 0.07255680859088898 Accuracy 0.8454999923706055\n",
      "Iteration 47690 Training loss 0.0972166582942009 Validation loss 0.11031744629144669 Accuracy 0.7674999833106995\n",
      "Iteration 47700 Training loss 0.09433923661708832 Validation loss 0.10292106866836548 Accuracy 0.7785000205039978\n",
      "Iteration 47710 Training loss 0.08213341236114502 Validation loss 0.07273267209529877 Accuracy 0.8429999947547913\n",
      "Iteration 47720 Training loss 0.08383914083242416 Validation loss 0.07231752574443817 Accuracy 0.8460000157356262\n",
      "Iteration 47730 Training loss 0.09052061289548874 Validation loss 0.09143614023923874 Accuracy 0.8059999942779541\n",
      "Iteration 47740 Training loss 0.087569959461689 Validation loss 0.105026975274086 Accuracy 0.777999997138977\n",
      "Iteration 47750 Training loss 0.08262123912572861 Validation loss 0.08265843242406845 Accuracy 0.8289999961853027\n",
      "Iteration 47760 Training loss 0.08561695367097855 Validation loss 0.07494991272687912 Accuracy 0.8389999866485596\n",
      "Iteration 47770 Training loss 0.07642430812120438 Validation loss 0.08596137166023254 Accuracy 0.8184999823570251\n",
      "Iteration 47780 Training loss 0.09095853567123413 Validation loss 0.10471022874116898 Accuracy 0.7749999761581421\n",
      "Iteration 47790 Training loss 0.08453316986560822 Validation loss 0.07679257541894913 Accuracy 0.8360000252723694\n",
      "Iteration 47800 Training loss 0.09515202045440674 Validation loss 0.0730324238538742 Accuracy 0.843500018119812\n",
      "Iteration 47810 Training loss 0.08350509405136108 Validation loss 0.07186352461576462 Accuracy 0.843999981880188\n",
      "Iteration 47820 Training loss 0.10643517971038818 Validation loss 0.07666344940662384 Accuracy 0.8345000147819519\n",
      "Iteration 47830 Training loss 0.08535018563270569 Validation loss 0.07084459811449051 Accuracy 0.8460000157356262\n",
      "Iteration 47840 Training loss 0.08403370529413223 Validation loss 0.07062381505966187 Accuracy 0.8485000133514404\n",
      "Iteration 47850 Training loss 0.08791513741016388 Validation loss 0.07205662131309509 Accuracy 0.8420000076293945\n",
      "Iteration 47860 Training loss 0.0980151817202568 Validation loss 0.07583260536193848 Accuracy 0.8349999785423279\n",
      "Iteration 47870 Training loss 0.08111677318811417 Validation loss 0.07143767178058624 Accuracy 0.8475000262260437\n",
      "Iteration 47880 Training loss 0.08637548983097076 Validation loss 0.07042063772678375 Accuracy 0.8510000109672546\n",
      "Iteration 47890 Training loss 0.08147233724594116 Validation loss 0.07069623470306396 Accuracy 0.8485000133514404\n",
      "Iteration 47900 Training loss 0.08133187144994736 Validation loss 0.07091546058654785 Accuracy 0.8454999923706055\n",
      "Iteration 47910 Training loss 0.08428550511598587 Validation loss 0.0731525868177414 Accuracy 0.8389999866485596\n",
      "Iteration 47920 Training loss 0.08838724344968796 Validation loss 0.10717430710792542 Accuracy 0.7735000252723694\n",
      "Iteration 47930 Training loss 0.09315817058086395 Validation loss 0.11435123533010483 Accuracy 0.762499988079071\n",
      "Iteration 47940 Training loss 0.09243043512105942 Validation loss 0.1025884598493576 Accuracy 0.7804999947547913\n",
      "Iteration 47950 Training loss 0.07643093168735504 Validation loss 0.07752777636051178 Accuracy 0.8335000276565552\n",
      "Iteration 47960 Training loss 0.08720427006483078 Validation loss 0.07178233563899994 Accuracy 0.8485000133514404\n",
      "Iteration 47970 Training loss 0.08490515500307083 Validation loss 0.08313176035881042 Accuracy 0.8230000138282776\n",
      "Iteration 47980 Training loss 0.1002177894115448 Validation loss 0.11235654354095459 Accuracy 0.7630000114440918\n",
      "Iteration 47990 Training loss 0.09649258106946945 Validation loss 0.07316320389509201 Accuracy 0.843500018119812\n",
      "Iteration 48000 Training loss 0.08167839795351028 Validation loss 0.07196233421564102 Accuracy 0.8429999947547913\n",
      "Iteration 48010 Training loss 0.09853548556566238 Validation loss 0.07407189905643463 Accuracy 0.8389999866485596\n",
      "Iteration 48020 Training loss 0.07777523994445801 Validation loss 0.07703125476837158 Accuracy 0.8355000019073486\n",
      "Iteration 48030 Training loss 0.081842802464962 Validation loss 0.07081413269042969 Accuracy 0.847000002861023\n",
      "Iteration 48040 Training loss 0.09809830039739609 Validation loss 0.07394964247941971 Accuracy 0.8399999737739563\n",
      "Iteration 48050 Training loss 0.08652270585298538 Validation loss 0.07128465920686722 Accuracy 0.847000002861023\n",
      "Iteration 48060 Training loss 0.08665170520544052 Validation loss 0.07240135967731476 Accuracy 0.8410000205039978\n",
      "Iteration 48070 Training loss 0.08156195282936096 Validation loss 0.07687465846538544 Accuracy 0.8364999890327454\n",
      "Iteration 48080 Training loss 0.09059356153011322 Validation loss 0.07330180704593658 Accuracy 0.8445000052452087\n",
      "Iteration 48090 Training loss 0.09010443091392517 Validation loss 0.07241319119930267 Accuracy 0.8485000133514404\n",
      "Iteration 48100 Training loss 0.0787166953086853 Validation loss 0.07510670274496078 Accuracy 0.8385000228881836\n",
      "Iteration 48110 Training loss 0.08439720422029495 Validation loss 0.072414331138134 Accuracy 0.8424999713897705\n",
      "Iteration 48120 Training loss 0.09225062280893326 Validation loss 0.07066204398870468 Accuracy 0.8489999771118164\n",
      "Iteration 48130 Training loss 0.09324446320533752 Validation loss 0.07346310466527939 Accuracy 0.8389999866485596\n",
      "Iteration 48140 Training loss 0.08097165822982788 Validation loss 0.07299136370420456 Accuracy 0.8429999947547913\n",
      "Iteration 48150 Training loss 0.08853485435247421 Validation loss 0.07150336354970932 Accuracy 0.8489999771118164\n",
      "Iteration 48160 Training loss 0.08809580653905869 Validation loss 0.07177058607339859 Accuracy 0.8450000286102295\n",
      "Iteration 48170 Training loss 0.07897720485925674 Validation loss 0.07309498637914658 Accuracy 0.8410000205039978\n",
      "Iteration 48180 Training loss 0.08534421026706696 Validation loss 0.07012917101383209 Accuracy 0.8489999771118164\n",
      "Iteration 48190 Training loss 0.11014813184738159 Validation loss 0.0809738039970398 Accuracy 0.8220000267028809\n",
      "Iteration 48200 Training loss 0.08335550129413605 Validation loss 0.07135229557752609 Accuracy 0.8454999923706055\n",
      "Iteration 48210 Training loss 0.07664372026920319 Validation loss 0.07981279492378235 Accuracy 0.828499972820282\n",
      "Iteration 48220 Training loss 0.10040295869112015 Validation loss 0.07568471878767014 Accuracy 0.8364999890327454\n",
      "Iteration 48230 Training loss 0.09590528905391693 Validation loss 0.0726480558514595 Accuracy 0.843999981880188\n",
      "Iteration 48240 Training loss 0.09689754992723465 Validation loss 0.0720391720533371 Accuracy 0.8450000286102295\n",
      "Iteration 48250 Training loss 0.07582493871450424 Validation loss 0.07439415156841278 Accuracy 0.8395000100135803\n",
      "Iteration 48260 Training loss 0.0816851481795311 Validation loss 0.07032451778650284 Accuracy 0.8445000052452087\n",
      "Iteration 48270 Training loss 0.07433728128671646 Validation loss 0.0721367821097374 Accuracy 0.8454999923706055\n",
      "Iteration 48280 Training loss 0.08684257417917252 Validation loss 0.10455095022916794 Accuracy 0.777999997138977\n",
      "Iteration 48290 Training loss 0.09007970988750458 Validation loss 0.11487344652414322 Accuracy 0.7570000290870667\n",
      "Iteration 48300 Training loss 0.0875527411699295 Validation loss 0.10663671791553497 Accuracy 0.7730000019073486\n",
      "Iteration 48310 Training loss 0.10233476758003235 Validation loss 0.07317785173654556 Accuracy 0.8464999794960022\n",
      "Iteration 48320 Training loss 0.08060240000486374 Validation loss 0.08289143443107605 Accuracy 0.824999988079071\n",
      "Iteration 48330 Training loss 0.07804332673549652 Validation loss 0.0755595713853836 Accuracy 0.8374999761581421\n",
      "Iteration 48340 Training loss 0.08655208349227905 Validation loss 0.0907428041100502 Accuracy 0.8065000176429749\n",
      "Iteration 48350 Training loss 0.07835683971643448 Validation loss 0.07595730572938919 Accuracy 0.8364999890327454\n",
      "Iteration 48360 Training loss 0.0836498811841011 Validation loss 0.08854256570339203 Accuracy 0.8109999895095825\n",
      "Iteration 48370 Training loss 0.11770761013031006 Validation loss 0.1405193954706192 Accuracy 0.7049999833106995\n",
      "Iteration 48380 Training loss 0.08086782693862915 Validation loss 0.08335233479738235 Accuracy 0.8245000243186951\n",
      "Iteration 48390 Training loss 0.08774153143167496 Validation loss 0.09382791817188263 Accuracy 0.7985000014305115\n",
      "Iteration 48400 Training loss 0.07647425681352615 Validation loss 0.09332428872585297 Accuracy 0.7990000247955322\n",
      "Iteration 48410 Training loss 0.0958794504404068 Validation loss 0.11187714338302612 Accuracy 0.7634999752044678\n",
      "Iteration 48420 Training loss 0.09554102271795273 Validation loss 0.07209520041942596 Accuracy 0.843500018119812\n",
      "Iteration 48430 Training loss 0.09392622858285904 Validation loss 0.07287850975990295 Accuracy 0.8420000076293945\n",
      "Iteration 48440 Training loss 0.09218612313270569 Validation loss 0.07294455170631409 Accuracy 0.840499997138977\n",
      "Iteration 48450 Training loss 0.07586238533258438 Validation loss 0.07300732284784317 Accuracy 0.8414999842643738\n",
      "Iteration 48460 Training loss 0.09514859318733215 Validation loss 0.0713120773434639 Accuracy 0.8464999794960022\n",
      "Iteration 48470 Training loss 0.07749495655298233 Validation loss 0.07446123659610748 Accuracy 0.8385000228881836\n",
      "Iteration 48480 Training loss 0.07972703874111176 Validation loss 0.07347282767295837 Accuracy 0.8429999947547913\n",
      "Iteration 48490 Training loss 0.0895252376794815 Validation loss 0.11296786367893219 Accuracy 0.7630000114440918\n",
      "Iteration 48500 Training loss 0.09703794121742249 Validation loss 0.1088186502456665 Accuracy 0.7680000066757202\n",
      "Iteration 48510 Training loss 0.09191565215587616 Validation loss 0.11117912083864212 Accuracy 0.7645000219345093\n",
      "Iteration 48520 Training loss 0.0820554718375206 Validation loss 0.1031189039349556 Accuracy 0.7785000205039978\n",
      "Iteration 48530 Training loss 0.09173434227705002 Validation loss 0.10188309848308563 Accuracy 0.7829999923706055\n",
      "Iteration 48540 Training loss 0.09705027937889099 Validation loss 0.11264930665493011 Accuracy 0.7620000243186951\n",
      "Iteration 48550 Training loss 0.09331853687763214 Validation loss 0.11195515096187592 Accuracy 0.7620000243186951\n",
      "Iteration 48560 Training loss 0.0825992301106453 Validation loss 0.08517447859048843 Accuracy 0.8209999799728394\n",
      "Iteration 48570 Training loss 0.08332807570695877 Validation loss 0.08521262556314468 Accuracy 0.8169999718666077\n",
      "Iteration 48580 Training loss 0.09210994839668274 Validation loss 0.10850396752357483 Accuracy 0.7689999938011169\n",
      "Iteration 48590 Training loss 0.09019754081964493 Validation loss 0.09668941050767899 Accuracy 0.7940000295639038\n",
      "Iteration 48600 Training loss 0.08947405219078064 Validation loss 0.07144881784915924 Accuracy 0.8475000262260437\n",
      "Iteration 48610 Training loss 0.08369450271129608 Validation loss 0.07008828967809677 Accuracy 0.8489999771118164\n",
      "Iteration 48620 Training loss 0.08163587003946304 Validation loss 0.07230061292648315 Accuracy 0.8454999923706055\n",
      "Iteration 48630 Training loss 0.09174773097038269 Validation loss 0.07008134573698044 Accuracy 0.8454999923706055\n",
      "Iteration 48640 Training loss 0.07503784447908401 Validation loss 0.07700187712907791 Accuracy 0.8345000147819519\n",
      "Iteration 48650 Training loss 0.10318681597709656 Validation loss 0.1251828670501709 Accuracy 0.7365000247955322\n",
      "Iteration 48660 Training loss 0.07828570157289505 Validation loss 0.1058320477604866 Accuracy 0.7745000123977661\n",
      "Iteration 48670 Training loss 0.09045515954494476 Validation loss 0.10636437684297562 Accuracy 0.7749999761581421\n",
      "Iteration 48680 Training loss 0.07955409586429596 Validation loss 0.09168849140405655 Accuracy 0.8044999837875366\n",
      "Iteration 48690 Training loss 0.08659592270851135 Validation loss 0.11928807944059372 Accuracy 0.75\n",
      "Iteration 48700 Training loss 0.08120881766080856 Validation loss 0.089715376496315 Accuracy 0.8084999918937683\n",
      "Iteration 48710 Training loss 0.09415470063686371 Validation loss 0.1129797026515007 Accuracy 0.7615000009536743\n",
      "Iteration 48720 Training loss 0.07363792508840561 Validation loss 0.07839356362819672 Accuracy 0.8330000042915344\n",
      "Iteration 48730 Training loss 0.08426003903150558 Validation loss 0.07133494317531586 Accuracy 0.8424999713897705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10372c750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48740 Training loss 0.08996888995170593 Validation loss 0.07075582444667816 Accuracy 0.847000002861023\n",
      "Iteration 48750 Training loss 0.0830358937382698 Validation loss 0.07630180567502975 Accuracy 0.8360000252723694\n",
      "Iteration 48760 Training loss 0.07976933568716049 Validation loss 0.08699435740709305 Accuracy 0.8149999976158142\n",
      "Iteration 48770 Training loss 0.08695105463266373 Validation loss 0.10173919796943665 Accuracy 0.7804999947547913\n",
      "Iteration 48780 Training loss 0.0851362943649292 Validation loss 0.09911248087882996 Accuracy 0.7879999876022339\n",
      "Iteration 48790 Training loss 0.08772051334381104 Validation loss 0.09910766780376434 Accuracy 0.7829999923706055\n",
      "Iteration 48800 Training loss 0.08081568777561188 Validation loss 0.08388137072324753 Accuracy 0.8234999775886536\n",
      "Iteration 48810 Training loss 0.11692534387111664 Validation loss 0.09003859013319016 Accuracy 0.8050000071525574\n",
      "Iteration 48820 Training loss 0.07902012765407562 Validation loss 0.07105572521686554 Accuracy 0.8454999923706055\n",
      "Iteration 48830 Training loss 0.07649687677621841 Validation loss 0.07588435709476471 Accuracy 0.8345000147819519\n",
      "Iteration 48840 Training loss 0.07900864630937576 Validation loss 0.07399210333824158 Accuracy 0.8395000100135803\n",
      "Iteration 48850 Training loss 0.09945333749055862 Validation loss 0.11915574967861176 Accuracy 0.746999979019165\n",
      "Iteration 48860 Training loss 0.09859408438205719 Validation loss 0.11185646057128906 Accuracy 0.7634999752044678\n",
      "Iteration 48870 Training loss 0.0827837809920311 Validation loss 0.07002625614404678 Accuracy 0.8495000004768372\n",
      "Iteration 48880 Training loss 0.08760382235050201 Validation loss 0.07177340984344482 Accuracy 0.847000002861023\n",
      "Iteration 48890 Training loss 0.09562720358371735 Validation loss 0.07106810063123703 Accuracy 0.8485000133514404\n",
      "Iteration 48900 Training loss 0.07944532483816147 Validation loss 0.0749320238828659 Accuracy 0.8379999995231628\n",
      "Iteration 48910 Training loss 0.0967051163315773 Validation loss 0.0726507380604744 Accuracy 0.8450000286102295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10372c750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48920 Training loss 0.08108805865049362 Validation loss 0.08836613595485687 Accuracy 0.8119999766349792\n",
      "Iteration 48930 Training loss 0.09847169369459152 Validation loss 0.12285201251506805 Accuracy 0.7419999837875366\n",
      "Iteration 48940 Training loss 0.08996948599815369 Validation loss 0.10864199697971344 Accuracy 0.7724999785423279\n",
      "Iteration 48950 Training loss 0.08696544170379639 Validation loss 0.09768363833427429 Accuracy 0.7944999933242798\n",
      "Iteration 48960 Training loss 0.08865617960691452 Validation loss 0.10110311955213547 Accuracy 0.784500002861023\n",
      "Iteration 48970 Training loss 0.08316905051469803 Validation loss 0.09552042931318283 Accuracy 0.7960000038146973\n",
      "Iteration 48980 Training loss 0.10038965195417404 Validation loss 0.12059373408555984 Accuracy 0.7459999918937683\n",
      "Iteration 48990 Training loss 0.08466314524412155 Validation loss 0.07909878343343735 Accuracy 0.8309999704360962\n",
      "Iteration 49000 Training loss 0.0997406467795372 Validation loss 0.11488810926675797 Accuracy 0.7570000290870667\n",
      "Iteration 49010 Training loss 0.0731382817029953 Validation loss 0.07652035355567932 Accuracy 0.8345000147819519\n",
      "Iteration 49020 Training loss 0.08602678030729294 Validation loss 0.0707053691148758 Accuracy 0.8479999899864197\n",
      "Iteration 49030 Training loss 0.09999896585941315 Validation loss 0.0713687464594841 Accuracy 0.8489999771118164\n",
      "Iteration 49040 Training loss 0.07685542106628418 Validation loss 0.07209251075983047 Accuracy 0.8464999794960022\n",
      "Iteration 49050 Training loss 0.08704255521297455 Validation loss 0.10981972515583038 Accuracy 0.765999972820282\n",
      "Iteration 49060 Training loss 0.08994539082050323 Validation loss 0.10451726615428925 Accuracy 0.7764999866485596\n",
      "Iteration 49070 Training loss 0.10364321619272232 Validation loss 0.11399818956851959 Accuracy 0.7590000033378601\n",
      "Iteration 49080 Training loss 0.0953778475522995 Validation loss 0.12015875428915024 Accuracy 0.7490000128746033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10372c750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49090 Training loss 0.08023453503847122 Validation loss 0.08782308548688889 Accuracy 0.8159999847412109\n",
      "Iteration 49100 Training loss 0.08983085304498672 Validation loss 0.09654420614242554 Accuracy 0.7940000295639038\n",
      "Iteration 49110 Training loss 0.08479776233434677 Validation loss 0.0978468582034111 Accuracy 0.7919999957084656\n",
      "Iteration 49120 Training loss 0.07658701390028 Validation loss 0.07565144449472427 Accuracy 0.8364999890327454\n",
      "Iteration 49130 Training loss 0.08401454240083694 Validation loss 0.07997272908687592 Accuracy 0.8314999938011169\n",
      "Iteration 49140 Training loss 0.09968633204698563 Validation loss 0.1244037002325058 Accuracy 0.7394999861717224\n",
      "Iteration 49150 Training loss 0.07954378426074982 Validation loss 0.08982580155134201 Accuracy 0.8054999709129333\n",
      "Iteration 49160 Training loss 0.0905771255493164 Validation loss 0.07078395038843155 Accuracy 0.847000002861023\n",
      "Iteration 49170 Training loss 0.09693612158298492 Validation loss 0.07224220782518387 Accuracy 0.8460000157356262\n",
      "Iteration 49180 Training loss 0.08397170901298523 Validation loss 0.07088526338338852 Accuracy 0.843500018119812\n",
      "Iteration 49190 Training loss 0.08267857134342194 Validation loss 0.06987509876489639 Accuracy 0.8479999899864197\n",
      "Iteration 49200 Training loss 0.0925552174448967 Validation loss 0.07180247455835342 Accuracy 0.8454999923706055\n",
      "Iteration 49210 Training loss 0.08932574093341827 Validation loss 0.07213621586561203 Accuracy 0.8445000052452087\n",
      "Iteration 49220 Training loss 0.09574205428361893 Validation loss 0.07225669175386429 Accuracy 0.8445000052452087\n",
      "Iteration 49230 Training loss 0.07632743567228317 Validation loss 0.07419513911008835 Accuracy 0.8410000205039978\n",
      "Iteration 49240 Training loss 0.0895586609840393 Validation loss 0.07159504294395447 Accuracy 0.8454999923706055\n",
      "Iteration 49250 Training loss 0.09067101776599884 Validation loss 0.07134394347667694 Accuracy 0.847000002861023\n",
      "Iteration 49260 Training loss 0.10543808341026306 Validation loss 0.074581578373909 Accuracy 0.8399999737739563\n",
      "Iteration 49270 Training loss 0.0800294503569603 Validation loss 0.07042142003774643 Accuracy 0.847000002861023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10372c750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49280 Training loss 0.08699583262205124 Validation loss 0.06955582648515701 Accuracy 0.8510000109672546\n",
      "Iteration 49290 Training loss 0.07917668670415878 Validation loss 0.07106032222509384 Accuracy 0.8485000133514404\n",
      "Iteration 49300 Training loss 0.08205582201480865 Validation loss 0.09286706894636154 Accuracy 0.8040000200271606\n",
      "Iteration 49310 Training loss 0.08187082409858704 Validation loss 0.09736745059490204 Accuracy 0.7914999723434448\n",
      "Iteration 49320 Training loss 0.08733668178319931 Validation loss 0.10988681763410568 Accuracy 0.7695000171661377\n",
      "Iteration 49330 Training loss 0.0854441449046135 Validation loss 0.09877493977546692 Accuracy 0.7885000109672546\n",
      "Iteration 49340 Training loss 0.07928292453289032 Validation loss 0.07173632830381393 Accuracy 0.8460000157356262\n",
      "Iteration 49350 Training loss 0.11082768440246582 Validation loss 0.1330229640007019 Accuracy 0.7200000286102295\n",
      "Iteration 49360 Training loss 0.07908385992050171 Validation loss 0.0799480602145195 Accuracy 0.8309999704360962\n",
      "Iteration 49370 Training loss 0.07272781431674957 Validation loss 0.08399239927530289 Accuracy 0.8199999928474426\n",
      "Iteration 49380 Training loss 0.09559686481952667 Validation loss 0.11693219840526581 Accuracy 0.7565000057220459\n",
      "Iteration 49390 Training loss 0.0830221101641655 Validation loss 0.10627685487270355 Accuracy 0.7760000228881836\n",
      "Iteration 49400 Training loss 0.09734529256820679 Validation loss 0.11254451423883438 Accuracy 0.762499988079071\n",
      "Iteration 49410 Training loss 0.08019343763589859 Validation loss 0.08902338892221451 Accuracy 0.8100000023841858\n",
      "Iteration 49420 Training loss 0.08321548253297806 Validation loss 0.07229451090097427 Accuracy 0.843500018119812\n",
      "Iteration 49430 Training loss 0.09319441020488739 Validation loss 0.07145757228136063 Accuracy 0.847000002861023\n",
      "Iteration 49440 Training loss 0.09197971969842911 Validation loss 0.0713527500629425 Accuracy 0.8454999923706055\n",
      "Iteration 49450 Training loss 0.090632863342762 Validation loss 0.07222413271665573 Accuracy 0.8450000286102295\n",
      "Iteration 49460 Training loss 0.0891290158033371 Validation loss 0.07063070684671402 Accuracy 0.8495000004768372\n",
      "Iteration 49470 Training loss 0.08654499053955078 Validation loss 0.07129775732755661 Accuracy 0.8460000157356262\n",
      "Iteration 49480 Training loss 0.09855000674724579 Validation loss 0.07213085144758224 Accuracy 0.8450000286102295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10372c750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49490 Training loss 0.09023655951023102 Validation loss 0.07060069590806961 Accuracy 0.8489999771118164\n",
      "Iteration 49500 Training loss 0.08185897022485733 Validation loss 0.07482447475194931 Accuracy 0.8370000123977661\n",
      "Iteration 49510 Training loss 0.08155958354473114 Validation loss 0.07481252402067184 Accuracy 0.8374999761581421\n",
      "Iteration 49520 Training loss 0.08616579324007034 Validation loss 0.07013802975416183 Accuracy 0.8479999899864197\n",
      "Iteration 49530 Training loss 0.08951770514249802 Validation loss 0.07294250279664993 Accuracy 0.8429999947547913\n",
      "Iteration 49540 Training loss 0.08300971984863281 Validation loss 0.07566141337156296 Accuracy 0.8370000123977661\n",
      "Iteration 49550 Training loss 0.07816833257675171 Validation loss 0.07576066255569458 Accuracy 0.8360000252723694\n",
      "Iteration 49560 Training loss 0.08190876245498657 Validation loss 0.09764377772808075 Accuracy 0.7919999957084656\n",
      "Iteration 49570 Training loss 0.08126388490200043 Validation loss 0.09948679059743881 Accuracy 0.7870000004768372\n",
      "Iteration 49580 Training loss 0.07773878425359726 Validation loss 0.07875388860702515 Accuracy 0.8305000066757202\n",
      "Iteration 49590 Training loss 0.07977521419525146 Validation loss 0.07283356040716171 Accuracy 0.8429999947547913\n",
      "Iteration 49600 Training loss 0.10442005842924118 Validation loss 0.07179715484380722 Accuracy 0.843999981880188\n",
      "Iteration 49610 Training loss 0.099258653819561 Validation loss 0.07205010205507278 Accuracy 0.843999981880188\n",
      "Iteration 49620 Training loss 0.07784931361675262 Validation loss 0.07152857631444931 Accuracy 0.8424999713897705\n",
      "Iteration 49630 Training loss 0.081916943192482 Validation loss 0.07149090617895126 Accuracy 0.8464999794960022\n",
      "Iteration 49640 Training loss 0.09315400570631027 Validation loss 0.07155949622392654 Accuracy 0.847000002861023\n",
      "Iteration 49650 Training loss 0.09791691601276398 Validation loss 0.07193108648061752 Accuracy 0.843999981880188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10372c750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49660 Training loss 0.0822795107960701 Validation loss 0.07118899375200272 Accuracy 0.8429999947547913\n",
      "Iteration 49670 Training loss 0.08688926696777344 Validation loss 0.07108206301927567 Accuracy 0.8475000262260437\n",
      "Iteration 49680 Training loss 0.0767403244972229 Validation loss 0.07354486733675003 Accuracy 0.8420000076293945\n",
      "Iteration 49690 Training loss 0.08745138347148895 Validation loss 0.0709242969751358 Accuracy 0.8475000262260437\n",
      "Iteration 49700 Training loss 0.08143042773008347 Validation loss 0.06976999342441559 Accuracy 0.8479999899864197\n",
      "Iteration 49710 Training loss 0.08631914108991623 Validation loss 0.07224470376968384 Accuracy 0.8454999923706055\n",
      "Iteration 49720 Training loss 0.08128959685564041 Validation loss 0.06996232271194458 Accuracy 0.8495000004768372\n",
      "Iteration 49730 Training loss 0.09267988055944443 Validation loss 0.0711570531129837 Accuracy 0.8479999899864197\n",
      "Iteration 49740 Training loss 0.09924192726612091 Validation loss 0.07261983305215836 Accuracy 0.843500018119812\n",
      "Iteration 49750 Training loss 0.08361194282770157 Validation loss 0.07297682762145996 Accuracy 0.8454999923706055\n",
      "Iteration 49760 Training loss 0.07339175045490265 Validation loss 0.07301884144544601 Accuracy 0.843500018119812\n",
      "Iteration 49770 Training loss 0.08983665704727173 Validation loss 0.07163289934396744 Accuracy 0.8450000286102295\n",
      "Iteration 49780 Training loss 0.09049278497695923 Validation loss 0.07099432498216629 Accuracy 0.8485000133514404\n",
      "Iteration 49790 Training loss 0.08360020816326141 Validation loss 0.07088091224431992 Accuracy 0.8445000052452087\n",
      "Iteration 49800 Training loss 0.09099386632442474 Validation loss 0.10777520388364792 Accuracy 0.7720000147819519\n",
      "Iteration 49810 Training loss 0.09299701452255249 Validation loss 0.11294223368167877 Accuracy 0.7630000114440918\n",
      "Iteration 49820 Training loss 0.08654026687145233 Validation loss 0.08936217427253723 Accuracy 0.8100000023841858\n",
      "Iteration 49830 Training loss 0.0775318294763565 Validation loss 0.07172609120607376 Accuracy 0.8454999923706055\n",
      "Iteration 49840 Training loss 0.08829721063375473 Validation loss 0.07166807353496552 Accuracy 0.8479999899864197\n",
      "Iteration 49850 Training loss 0.08320245146751404 Validation loss 0.0708601251244545 Accuracy 0.8460000157356262\n",
      "Iteration 49860 Training loss 0.07820592075586319 Validation loss 0.07209610193967819 Accuracy 0.843999981880188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10372c750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49870 Training loss 0.07674725353717804 Validation loss 0.07298997789621353 Accuracy 0.8445000052452087\n",
      "Iteration 49880 Training loss 0.08874142169952393 Validation loss 0.07131124287843704 Accuracy 0.8479999899864197\n",
      "Iteration 49890 Training loss 0.08071534335613251 Validation loss 0.07389528304338455 Accuracy 0.8410000205039978\n",
      "Iteration 49900 Training loss 0.09571411460638046 Validation loss 0.07116641104221344 Accuracy 0.8464999794960022\n",
      "Iteration 49910 Training loss 0.10419999063014984 Validation loss 0.07475755363702774 Accuracy 0.8399999737739563\n",
      "Iteration 49920 Training loss 0.08649322390556335 Validation loss 0.0729338675737381 Accuracy 0.8429999947547913\n",
      "Iteration 49930 Training loss 0.07546736299991608 Validation loss 0.07079844921827316 Accuracy 0.8479999899864197\n",
      "Iteration 49940 Training loss 0.08860841393470764 Validation loss 0.07102595269680023 Accuracy 0.847000002861023\n",
      "Iteration 49950 Training loss 0.08516782522201538 Validation loss 0.0719393789768219 Accuracy 0.847000002861023\n",
      "Iteration 49960 Training loss 0.07911432534456253 Validation loss 0.0737193301320076 Accuracy 0.8420000076293945\n",
      "Iteration 49970 Training loss 0.10508004575967789 Validation loss 0.07358146458864212 Accuracy 0.8395000100135803\n",
      "Iteration 49980 Training loss 0.07863140106201172 Validation loss 0.074031762778759 Accuracy 0.8424999713897705\n",
      "Iteration 49990 Training loss 0.09059195965528488 Validation loss 0.07070854306221008 Accuracy 0.8475000262260437\n",
      "Iteration 50000 Training loss 0.09039769321680069 Validation loss 0.07042402774095535 Accuracy 0.8489999771118164\n",
      "Iteration 50010 Training loss 0.08504786342382431 Validation loss 0.07049117982387543 Accuracy 0.847000002861023\n",
      "Iteration 50020 Training loss 0.0934026762843132 Validation loss 0.11802349984645844 Accuracy 0.7519999742507935\n",
      "Iteration 50030 Training loss 0.07695642113685608 Validation loss 0.07679473608732224 Accuracy 0.8324999809265137\n",
      "Iteration 50040 Training loss 0.09359080344438553 Validation loss 0.07152248173952103 Accuracy 0.8454999923706055\n",
      "Iteration 50050 Training loss 0.07956918329000473 Validation loss 0.08434204757213593 Accuracy 0.8209999799728394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10372c750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50060 Training loss 0.07788003981113434 Validation loss 0.08512306213378906 Accuracy 0.8190000057220459\n",
      "Iteration 50070 Training loss 0.07964672148227692 Validation loss 0.08109330385923386 Accuracy 0.8299999833106995\n",
      "Iteration 50080 Training loss 0.09535051882266998 Validation loss 0.06963174790143967 Accuracy 0.8475000262260437\n",
      "Iteration 50090 Training loss 0.10770321637392044 Validation loss 0.07451784610748291 Accuracy 0.8410000205039978\n",
      "Iteration 50100 Training loss 0.0945001021027565 Validation loss 0.0716056302189827 Accuracy 0.843999981880188\n",
      "Iteration 50110 Training loss 0.0802672803401947 Validation loss 0.07750380784273148 Accuracy 0.8330000042915344\n",
      "Iteration 50120 Training loss 0.07460138946771622 Validation loss 0.08876483887434006 Accuracy 0.8080000281333923\n",
      "Iteration 50130 Training loss 0.10425601154565811 Validation loss 0.13131262362003326 Accuracy 0.7254999876022339\n",
      "Iteration 50140 Training loss 0.08266526460647583 Validation loss 0.09758596122264862 Accuracy 0.7929999828338623\n",
      "Iteration 50150 Training loss 0.08872003853321075 Validation loss 0.109585240483284 Accuracy 0.7699999809265137\n",
      "Iteration 50160 Training loss 0.08361654728651047 Validation loss 0.10526459664106369 Accuracy 0.7749999761581421\n",
      "Iteration 50170 Training loss 0.09144513309001923 Validation loss 0.11304261535406113 Accuracy 0.7639999985694885\n",
      "Iteration 50180 Training loss 0.08367700129747391 Validation loss 0.10390850901603699 Accuracy 0.7785000205039978\n",
      "Iteration 50190 Training loss 0.09536907076835632 Validation loss 0.11023921519517899 Accuracy 0.7680000066757202\n",
      "Iteration 50200 Training loss 0.1004890725016594 Validation loss 0.11382325738668442 Accuracy 0.762499988079071\n",
      "Iteration 50210 Training loss 0.0835496187210083 Validation loss 0.10632007569074631 Accuracy 0.7745000123977661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10372c750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50220 Training loss 0.08737310767173767 Validation loss 0.0969134196639061 Accuracy 0.7944999933242798\n",
      "Iteration 50230 Training loss 0.07822567969560623 Validation loss 0.09688342362642288 Accuracy 0.7914999723434448\n",
      "Iteration 50240 Training loss 0.08198247104883194 Validation loss 0.0711817592382431 Accuracy 0.8460000157356262\n",
      "Iteration 50250 Training loss 0.07772182673215866 Validation loss 0.10347139090299606 Accuracy 0.7804999947547913\n",
      "Iteration 50260 Training loss 0.0892404168844223 Validation loss 0.10928951948881149 Accuracy 0.7670000195503235\n",
      "Iteration 50270 Training loss 0.1023349016904831 Validation loss 0.13068369030952454 Accuracy 0.7260000109672546\n",
      "Iteration 50280 Training loss 0.08380613476037979 Validation loss 0.11087647825479507 Accuracy 0.7680000066757202\n",
      "Iteration 50290 Training loss 0.07890235632658005 Validation loss 0.08224334567785263 Accuracy 0.8255000114440918\n",
      "Iteration 50300 Training loss 0.08724556863307953 Validation loss 0.11432796716690063 Accuracy 0.7590000033378601\n",
      "Iteration 50310 Training loss 0.08332881331443787 Validation loss 0.09711102396249771 Accuracy 0.7940000295639038\n",
      "Iteration 50320 Training loss 0.09246636927127838 Validation loss 0.1194557324051857 Accuracy 0.7494999766349792\n",
      "Iteration 50330 Training loss 0.09721284359693527 Validation loss 0.12169093638658524 Accuracy 0.7440000176429749\n",
      "Iteration 50340 Training loss 0.08377347141504288 Validation loss 0.09368068724870682 Accuracy 0.8015000224113464\n",
      "Iteration 50350 Training loss 0.08414919674396515 Validation loss 0.09863390773534775 Accuracy 0.7910000085830688\n",
      "Iteration 50360 Training loss 0.08514954149723053 Validation loss 0.10390886664390564 Accuracy 0.777999997138977\n",
      "Iteration 50370 Training loss 0.07686867564916611 Validation loss 0.07931379973888397 Accuracy 0.8295000195503235\n",
      "Iteration 50380 Training loss 0.07591764628887177 Validation loss 0.07363498210906982 Accuracy 0.8420000076293945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbinary_model_4_layer_extreme_trained\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.45\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5e8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 704\u001b[39m, in \u001b[36mbinary_classification_four_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, lr_decay_rate, reg1, reg2, reg3, reg4, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2, train_layer_3, train_layer_4, dropout_rate)\u001b[39m\n\u001b[32m    697\u001b[39m x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \u001b[38;5;66;03m# sélection un lot de données aléatoires parmis les données d'entrainement \u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;66;03m# Dropout\u001b[39;00m\n\u001b[32m    700\u001b[39m \u001b[38;5;66;03m# dropout_mask = ((torch.rand((minibatch_size, x_train.shape[1])) > dropout_rate).to(dtype)).to(device)\u001b[39;00m\n\u001b[32m    701\u001b[39m \u001b[38;5;66;03m# x_minibatch = x_minibatch*dropout_mask\u001b[39;00m\n\u001b[32m    702\u001b[39m \n\u001b[32m    703\u001b[39m \u001b[38;5;66;03m# Calcul de la prédiction\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m output, z4, h3, z3, h2, z2, h1, z1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_minibatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[38;5;28mself\u001b[39m.observation_rate == \u001b[32m0\u001b[39m:    \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 653\u001b[39m, in \u001b[36mbinary_classification_four_layer_NN.forward\u001b[39m\u001b[34m(self, x, dropout_rate)\u001b[39m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dropout_rate != \u001b[32m0\u001b[39m :    \n\u001b[32m    652\u001b[39m     dropout_mask_1 = ((torch.rand((x.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     dropout_mask_2 = ((\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_2_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m > dropout_rate).to(dtype)).to(device)\n\u001b[32m    654\u001b[39m     dropout_mask_3 = ((torch.rand((x.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.hidden_3_size)) > dropout_rate).to(dtype)).to(device)\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m : \n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "binary_model_4_layer_extreme_trained.train_layers(x_train, y_train, x_valid, y_valid, 2.45, 1e-1, 5e8, 1e-2, 1e-2, 1e-2, 1e-2, 1, 0.2, 10, True, False, False, True, 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(3072,2048,10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-5, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 2048, 2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = True\n",
    "date = \"09_05_25\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = binary_model_4_layer_extreme_trained\n",
    "    model_name = date + '_CIFAR10_model_(1024+512-512-512+1)_1_save_12' #\n",
    "    save_path = \"Classifiers/\" + date + '/' + model_name + '/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    #\n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0, np.max([np.nan_to_num(model.training_loss_trajectory,nan=0), np.nan_to_num(model.validation_loss_trajectory, nan=0)])+0.01)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0, np.max([np.nan_to_num(model.training_loss_trajectory,nan=0), np.nan_to_num(model.validation_loss_trajectory, nan=0)])+0.01)\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c380b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Post-processing/08_05_25/Dataset_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 layers - Training first layer : True - Training second layer : True - kappa = 2.45 - lr = 1e-05 - lr_decay_rate = 100000000.0 - reg1 = 0 - reg2 = 0 - eps_init = 1 - fraction_batch = 0.2 - observation rate = 10 - Train layer 1 = True - Train layer 2 = True - Dropout rate = 0.4\n",
      "[0.         0.48416665]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "date = '07_05_25'\n",
    "model_name = \"CIFAR10_model_(1024+512+1)_save_3\"\n",
    "curve_datas = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name + '/figures/accuracy_of_' + model_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "assessed_model = torch.load('Classifiers/' + date + '/' + date + '_' +  model_name + '/' + model_name + \".pt\", weights_only=False)\n",
    "save_path = 'Post-processing/08_05_25/Dataset_3/'\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plot the performances\n",
    "model_name_1 = \"CIFAR10_model_(1024+512+1)_3\"\n",
    "model_name_2 = \"CIFAR10_model_(1024+512+512+1)_3\"\n",
    "model_name_3 = \"CIFAR10_model_(1024+512-512+1)_3\"\n",
    "\n",
    "curve_datas_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/accuracy_of_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_datas_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/accuracy_of_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_datas_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/accuracy_of_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "print(curve_datas_model_1[0])\n",
    "plt.plot(curve_datas_model_3[:, 0], curve_datas_model_3[:, 1], '.', markersize = '1', color = 'blue', label = ' (1024+512-512+1)')\n",
    "plt.plot(curve_datas_model_2[:, 0], curve_datas_model_2[:, 1], '.', markersize = '1', color = 'red', label = '(1024+512+512+1)')\n",
    "plt.plot(curve_datas_model_1[:, 0], curve_datas_model_1[:, 1], '.', markersize = '1', color = 'green', label = '(1024+512+1)')\n",
    "plt.xlim(-200,np.min([np.max(curve_datas_model_1[:, 0]), np.max(curve_datas_model_2[:, 0]), np.max(curve_datas_model_3[:, 0])]))\n",
    "plt.ylim(0.45,1)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration N')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies on dataset 3 for the different architectures', pad = 20)\n",
    "legend = plt.legend()\n",
    "for handle in legend.legend_handles:\n",
    "    handle.set_markersize(15)\n",
    "plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_3.png\", bbox_inches='tight')\n",
    "plt.savefig(save_path + \"Zoomed_Comparison_accuracy_dataset_3.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "# Plots of performances\n",
    "# accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(accuracy)\n",
    "# kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_accuracy)\n",
    "# loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(loss)\n",
    "# kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_loss)\n",
    "# plt.show()\n",
    "\n",
    "# Import datas\n",
    "# accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70c105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
