{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.ticker import LogLocator\n",
    "from matplotlib.lines import Line2D\n",
    "from copy import deepcopy\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import scipy\n",
    "from scipy.signal import butter, filtfilt, argrelextrema\n",
    "from scipy.interpolate import interp1d\n",
    "from mup import MuReadout, MuSGD, MuAdam\n",
    "import mup as mup\n",
    "from pathlib import Path\n",
    "\n",
    "device = 'mps'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Smoothing features #####\n",
    "def lower_envelope(x,y):\n",
    "    minima_idx = argrelextrema(y, np.less, order=1)[0]\n",
    "    x_sampled = x[minima_idx]\n",
    "    y = y[minima_idx]\n",
    "    interp = interp1d(x_sampled, y, kind='linear', fill_value='extrapolate')\n",
    "    y = interp(x)\n",
    "    return np.array([x, y]).transpose()\n",
    "\n",
    "def upper_envelope(x,y):\n",
    "    maxima_idx = argrelextrema(y, np.greater, order=2)[0]\n",
    "    x_sampled = x[maxima_idx]\n",
    "    y = y[maxima_idx]\n",
    "    interp = interp1d(x_sampled, y, kind='linear', fill_value='extrapolate')\n",
    "    y = interp(x)\n",
    "    return np.array([x, y]).transpose()\n",
    "\n",
    "def low_pass_filter(y, f_c, order = 5):\n",
    "    b, a = butter(order, f_c, btype='low')\n",
    "    return filtfilt(b,a,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load a dictionnary of curves related to the training of several models and plot interesting datas about the training dynamic #####\n",
    "\n",
    "### LOAD ###\n",
    "def load_data_dictionnary(model_file_name, date):\n",
    "    \"\"\" Data contains are : training_loss, validation_loss, accuracy, kappa_training_loss, kappa_validation_loss, kappa_accuracy\"\"\"\n",
    "    training_loss = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/training_loss_of_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    validation_loss = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/validation_loss_of_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    accuracy = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/accuracy_of_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    kappa_training_loss = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/kappa_training_loss_of_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    kappa_validation_loss = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/kappa_validation_loss_of_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    kappa_accuracy = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/kappa_accuracy_of_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    \n",
    "    model_data_dictionnary = {'training loss' : training_loss, 'validation loss' : validation_loss, 'accuracy' : accuracy, \n",
    "                        'kappa training loss' : kappa_training_loss, 'kappa validation loss' : kappa_validation_loss,\n",
    "                        'kappa accuracy' : kappa_accuracy}    \n",
    "    \n",
    "    if os.path.exists('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/' + 'fraction_minibatch' + model_file_name + '.txt') :\n",
    "        fraction_minibatch = torch.load('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/' + date + '_' + 'fraction_minibatch' + model_file_name + '.txt',  delimiter=\",\", skiprows=1)\n",
    "        model_data_dictionnary['fraction minibatch'] = fraction_minibatch\n",
    "\n",
    "    if os.path.exists('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/accuracy_of_' + date + '_' + model_file_name + '_last_layer_fine_tuned' +'.txt') :\n",
    "        training_loss_fine_tuned = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/training_loss_of_' + date + '_' + model_file_name + '_last_layer_fine_tuned' + '.txt', delimiter=\",\", skiprows=1)\n",
    "        validation_loss_fine_tuned = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/validation_loss_of_' + date + '_' + model_file_name + '_last_layer_fine_tuned' + '.txt', delimiter=\",\", skiprows=1)\n",
    "        accuracy_fine_tuned = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/accuracy_of_' + date + '_' + model_file_name + '_last_layer_fine_tuned' + '.txt', delimiter=\",\", skiprows=1)\n",
    "        model_data_dictionnary = model_data_dictionnary | {'fine tuned training loss' : training_loss_fine_tuned, 'fine tuned validation loss' : validation_loss_fine_tuned, 'fine tuned accuracy' : accuracy_fine_tuned}\n",
    "    \n",
    "    if os.path.exists('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/' + date + '_' +  model_file_name + '_first_layer_weight_bias.pt') :\n",
    "        weight_bias = torch.load('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/' + date + '_' +  model_file_name + '_first_layer_weight_bias.pt')\n",
    "        model_data_dictionnary = model_data_dictionnary | weight_bias\n",
    "    \n",
    "    if os.path.exists('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/' + date + '_' +  model_file_name + '_first_layer_weight_bias_trajectory.pt') :\n",
    "        weight_bias = torch.load('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/' + date + '_' +  model_file_name + '_first_layer_weight_bias_trajectory.pt')\n",
    "        model_data_dictionnary = model_data_dictionnary | {'layers trajectories' : {1 : weight_bias}}\n",
    "    \n",
    "    if os.path.exists('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/' + date + '_' +  model_file_name + '_second_layer_weight_bias_trajectory.pt') :\n",
    "        weight_bias = torch.load('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/' + date + '_' +  model_file_name + '_second_layer_weight_bias_trajectory.pt')\n",
    "        model_data_dictionnary['layers trajectories'] = model_data_dictionnary['layers trajectories'] | {2 : weight_bias}\n",
    "        \n",
    "    return model_data_dictionnary\n",
    "\n",
    "def load_data_dictionnary_pre_29_05(model_file_name, date):\n",
    "    \"\"\" Data contains are : training_loss, validation_loss, accuracy, kappa_training_loss, kappa_validation_loss, kappa_accuracy\"\"\"\n",
    "    training_loss = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/loss_training_of_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    validation_loss = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/validation_loss_of_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    accuracy = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/accuracy_of_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    kappa_training_loss = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/kappa_loss_training_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    kappa_validation_loss = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/kappa_loss_validation_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    kappa_accuracy = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_file_name + '/figures/kappa_accuracy_' + date + '_' + model_file_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "    \n",
    "    model_data_dictionnary = {'training loss' : training_loss, 'validation loss' : validation_loss, 'accuracy' : accuracy, \n",
    "                        'kappa training loss' : kappa_training_loss, 'kappa validation loss' : kappa_validation_loss,\n",
    "                        'kappa accuracy' : kappa_accuracy}\n",
    "    return model_data_dictionnary\n",
    "\n",
    "### CONVERGENCE SPEED ###\n",
    "\n",
    "def convergence_speed(curve, ref_curve, delta = 1e-3,observation_rate=10):\n",
    "    if np.size(np.where((curve - (np.max(ref_curve)-delta) > 0))[0]) > 0 :\n",
    "        return np.where((curve - (np.max(ref_curve)-delta) > 0))[0][0]*observation_rate\n",
    "    else : \n",
    "        return -1e8\n",
    "\n",
    "def convergence_speed_plot(curve_dictionnary, color_list, marker_list, x_list, delta=1e-3, save=True, save_path = '', save_name_peculiarity=''):\n",
    "    convergence_speed_dictionnary = deepcopy(curve_dictionnary)\n",
    "    for n_hidden_units in curve_dictionnary:\n",
    "        for type_of_training in curve_dictionnary[n_hidden_units] :\n",
    "            for architecture in curve_dictionnary[n_hidden_units][type_of_training]:\n",
    "                if type_of_training != 'First trained + Last fine tuned' :\n",
    "                    convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture] = convergence_speed(curve_dictionnary[n_hidden_units][type_of_training][architecture]['accuracy'][:,1], curve_dictionnary[n_hidden_units]['Reference']['2 layers']['accuracy'][:,1], delta)\n",
    "                    plt.plot(x_list[architecture], convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture], marker = marker_list[type_of_training], markersize = '13', markeredgecolor = color_list[n_hidden_units], markerfacecolor = 'None')       \n",
    "                else : \n",
    "                    convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture] = convergence_speed(curve_dictionnary[n_hidden_units][type_of_training][architecture]['fine tuned accuracy'][:,1], curve_dictionnary[n_hidden_units]['Reference']['2 layers']['accuracy'][:,1], delta)\n",
    "                    plt.plot(x_list[architecture], convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture], marker = marker_list[type_of_training], markersize = '13', color = color_list[n_hidden_units])       \n",
    "                 \n",
    "    legend_hu = []\n",
    "    legend_training_type = []\n",
    "    for i, n_hidden_units in enumerate(convergence_speed_dictionnary):\n",
    "        legend_hu += [Line2D([0], [0], color = color_list[n_hidden_units], marker='.', linestyle='None', label = n_hidden_units)]\n",
    "        if i == 0:\n",
    "            for type_of_training in convergence_speed_dictionnary[n_hidden_units]:\n",
    "                if type_of_training != 'First trained + Last fine tuned' :\n",
    "                    legend_training_type += [Line2D([0], [0], markeredgecolor= 'black', markerfacecolor='None', marker = marker_list[type_of_training], linestyle = 'None', label = type_of_training)]\n",
    "                else : \n",
    "                    legend_training_type += [Line2D([0], [0], color= 'black', marker = marker_list[type_of_training], linestyle = 'None', label = type_of_training)]\n",
    "    legend_elements = legend_hu + legend_training_type\n",
    "    legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_markersize(15)\n",
    "    plt.xlabel('Number of layers')\n",
    "    plt.ylabel('Number of iterations')\n",
    "    plt.title('Convergence speed of training \\nfor the different architectures and types of training', pad = 20)\n",
    "    y_max = np.max([convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture] \n",
    "                for n_hidden_units in convergence_speed_dictionnary\n",
    "                for type_of_training in convergence_speed_dictionnary[n_hidden_units]\n",
    "                for architecture in convergence_speed_dictionnary[n_hidden_units][type_of_training]])\n",
    "    plt.ylim(-0.05*y_max, 1.05*y_max)\n",
    "    plt.xticks([x for key, x in x_list.items()])\n",
    "    plt.grid()\n",
    "    if save :\n",
    "        plt.savefig(save_path + 'convergence_speed_' + save_name_peculiarity + '.png', bbox_inches='tight')\n",
    "        plt.savefig(save_path + 'convergence_speed_' + save_name_peculiarity + '.svg', bbox_inches='tight')\n",
    "    plt.show()  \n",
    "    return\n",
    "\n",
    "def normalized_convergence_speed_plot(curve_dictionnary, color_list, marker_list, x_list, delta=1e-3, save=True, save_path = '', save_name_peculiarity=''):\n",
    "    convergence_speed_dictionnary = deepcopy(curve_dictionnary)\n",
    "    for n_hidden_units in curve_dictionnary:\n",
    "        for type_of_training in curve_dictionnary[n_hidden_units] :\n",
    "            for architecture in curve_dictionnary[n_hidden_units][type_of_training]:\n",
    "                if type_of_training != 'First trained + Last fine tuned' :\n",
    "                    convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture] = convergence_speed(curve_dictionnary[n_hidden_units][type_of_training][architecture]['accuracy'][:,1], curve_dictionnary[n_hidden_units]['Reference']['2 layers']['accuracy'][:,1], delta)/convergence_speed(curve_dictionnary[n_hidden_units]['Reference']['2 layers']['accuracy'][:,1], curve_dictionnary[n_hidden_units]['Reference']['2 layers']['accuracy'][:,1], delta)\n",
    "                    if type_of_training == 'Reference':\n",
    "                        plt.plot(x_list[architecture], convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture], marker = marker_list[type_of_training], markersize = '12', color = 'black')\n",
    "                    else :\n",
    "                        plt.plot(x_list[architecture], convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture], marker = marker_list[type_of_training], markersize = '12', markerfacecolor = 'none', markeredgecolor = color_list[n_hidden_units])\n",
    "                else :\n",
    "                    convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture] = convergence_speed(curve_dictionnary[n_hidden_units][type_of_training][architecture]['fine tuned accuracy'][:,1], curve_dictionnary[n_hidden_units]['Reference']['2 layers']['accuracy'][:,1], delta)/convergence_speed(curve_dictionnary[n_hidden_units]['Reference']['2 layers']['accuracy'][:,1], curve_dictionnary[n_hidden_units]['Reference']['2 layers']['accuracy'][:,1], delta)\n",
    "                    plt.plot(x_list[architecture], convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture], marker = marker_list[type_of_training], markersize = '12', markerfacecolor = color_list[n_hidden_units], markeredgecolor = color_list[n_hidden_units])\n",
    "\n",
    "    legend_hu = []\n",
    "    legend_training_type = []\n",
    "    for i, n_hidden_units in enumerate(convergence_speed_dictionnary):\n",
    "        legend_hu += [Line2D([0], [0], color = color_list[n_hidden_units], marker='.', linestyle='None', label = n_hidden_units)]\n",
    "        if i == 0:\n",
    "            for type_of_training in convergence_speed_dictionnary[n_hidden_units]:\n",
    "                if type_of_training != 'First trained + Last fine tuned' :\n",
    "                    legend_training_type += [Line2D([0], [0], marker = marker_list[type_of_training], linestyle='None', markeredgecolor='black', markerfacecolor='None', label = type_of_training)]\n",
    "                else : \n",
    "                    legend_training_type += [Line2D([0], [0], marker = marker_list[type_of_training], linestyle='None', markeredgecolor='black', markerfacecolor='black', label = type_of_training)]\n",
    "    legend_elements = legend_hu + legend_training_type\n",
    "    legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_markersize(15)\n",
    "    plt.xlabel('Number of layers')\n",
    "    plt.ylabel('Number of iterations')\n",
    "    plt.title('Normalized convergence speed of training \\nfor different architectures and types of training', pad = 20)\n",
    "    y_max = np.max([convergence_speed_dictionnary[n_hidden_units][type_of_training][architecture] \n",
    "                for n_hidden_units in convergence_speed_dictionnary\n",
    "                for type_of_training in convergence_speed_dictionnary[n_hidden_units]\n",
    "                for architecture in convergence_speed_dictionnary[n_hidden_units][type_of_training]])\n",
    "    plt.ylim(-0.1, y_max + 0.1)\n",
    "    plt.xticks([x for key, x in x_list.items()])\n",
    "    plt.grid()\n",
    "    if save :\n",
    "        plt.savefig(save_path + 'normalized_convergence_speed_' + save_name_peculiarity + '.png', bbox_inches='tight')\n",
    "        plt.savefig(save_path + 'normalized_convergence_speed_' + save_name_peculiarity + '.svg', bbox_inches='tight')\n",
    "    plt.show()  \n",
    "    return\n",
    "\n",
    "### MAXIMUM ACCURACY ###\n",
    "\n",
    "def max_accuracy_plot(curve_dictionnary, color_list, marker_list, x_list, save=True, save_path = '', save_name_peculiarity='') :\n",
    "    max_accuracy_dictionnary = deepcopy(curve_dictionnary)\n",
    "    for n_hidden_units in curve_dictionnary:\n",
    "        for type_of_training in curve_dictionnary[n_hidden_units] :\n",
    "            for architecture in curve_dictionnary[n_hidden_units][type_of_training]:\n",
    "                if type_of_training != 'First trained + Last fine tuned' :\n",
    "                    max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture] = np.max(curve_dictionnary[n_hidden_units][type_of_training][architecture]['accuracy'][:,1])\n",
    "                    plt.plot(x_list[architecture], max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture], marker = marker_list[type_of_training], markersize = '12', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units])\n",
    "                else : \n",
    "                    max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture] = np.max(curve_dictionnary[n_hidden_units][type_of_training][architecture]['fine tuned accuracy'][:,1])\n",
    "                    plt.plot(x_list[architecture], max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture], marker = marker_list[type_of_training], markersize = '12', markerfacecolor = color_list[n_hidden_units], markeredgecolor = color_list[n_hidden_units])\n",
    "\n",
    "    legend_hu = []\n",
    "    legend_training_type = []\n",
    "    for i, n_hidden_units in enumerate(max_accuracy_dictionnary):\n",
    "        legend_hu += [Line2D([0], [0], color = color_list[n_hidden_units], marker = '.', linestyle='None', label = n_hidden_units)]\n",
    "        if i == 0:\n",
    "            for type_of_training in max_accuracy_dictionnary[n_hidden_units]:\n",
    "                if type_of_training != 'First trained + Last fine tuned' :\n",
    "                    legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'None', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "                else : \n",
    "                    legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'black', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "    legend_elements = legend_hu + legend_training_type\n",
    "    legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_markersize(15)\n",
    "    plt.xlabel('Number of layers')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Maximum accuracy for\\n the different architectures and types of training', pad = 20)\n",
    "    y_max = np.max([max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture] \n",
    "                for n_hidden_units in max_accuracy_dictionnary\n",
    "                for type_of_training in max_accuracy_dictionnary[n_hidden_units]\n",
    "                for architecture in max_accuracy_dictionnary[n_hidden_units][type_of_training]])\n",
    "    y_min = np.min([max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture] \n",
    "                for n_hidden_units in max_accuracy_dictionnary\n",
    "                for type_of_training in max_accuracy_dictionnary[n_hidden_units]\n",
    "                for architecture in max_accuracy_dictionnary[n_hidden_units][type_of_training]])\n",
    "    plt.ylim(y_min - 0.01, y_max + 0.01)\n",
    "    plt.xticks([x for key, x in x_list.items()])\n",
    "    plt.grid()\n",
    "    if save :\n",
    "        plt.savefig(save_path + 'max_accuracy_' + save_name_peculiarity + '.png', bbox_inches='tight')\n",
    "        plt.savefig(save_path + 'max_accuracy_' + save_name_peculiarity + '.svg', bbox_inches='tight')\n",
    "    plt.show()  \n",
    "    return\n",
    "\n",
    "### ACCURACY TRAJECTORY ###\n",
    "\n",
    "def accuracy_trajectory_plot(curve_dictionnary, color_list, marker_list, x_lim = None, y_lim = None, save=True, save_path = '', save_name_peculiarity='', observation_rate = 10) :\n",
    "    for n_hidden_units in curve_dictionnary:\n",
    "        for type_of_training in curve_dictionnary[n_hidden_units] :\n",
    "            for architecture in curve_dictionnary[n_hidden_units][type_of_training]:\n",
    "                if type_of_training != 'First trained + Last fine tuned' :\n",
    "                    accuracy = curve_dictionnary[n_hidden_units][type_of_training][architecture]['accuracy']\n",
    "                    x_max = x_lim[1] if x_lim is not None else curve_dictionnary[n_hidden_units][type_of_training][architecture]['accuracy'][:,0][-1]*observation_rate\n",
    "                    accuracy[:,1] = low_pass_filter(accuracy[:,1], 0.10)\n",
    "                    len_accuracy = len(accuracy[accuracy[:,0] < x_max])\n",
    "                    plt.plot(accuracy[:,0], accuracy[:,1], marker = marker_list[type_of_training], markersize = '11', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units][architecture], markevery = int(len_accuracy/10), color = color_list[n_hidden_units][architecture])\n",
    "                            \n",
    "                else :\n",
    "                    accuracy = curve_dictionnary[n_hidden_units][type_of_training][architecture]['fine tuned accuracy']\n",
    "                    plt.plot(accuracy[:,0], accuracy[:,1], marker = marker_list[type_of_training], markersize = '11', markerfacecolor = color_list[n_hidden_units][architecture], markeredgecolor = color_list[n_hidden_units][architecture], color = color_list[n_hidden_units][architecture])\n",
    "    labels=[]\n",
    "    legend_hu = []\n",
    "    legend_training_type = []\n",
    "    for i, n_hidden_units in enumerate(curve_dictionnary):\n",
    "        for j, type_of_training in enumerate(curve_dictionnary[n_hidden_units]):\n",
    "            for k, architecture in enumerate(curve_dictionnary[n_hidden_units][type_of_training]):\n",
    "                if i == 0 and k == 0:\n",
    "                    if type_of_training != 'First trained + Last fine tuned' :\n",
    "                        legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'None', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "                    else : \n",
    "                        legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'black', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "                if np.sum([n_hidden_units + ' - ' + architecture==label for label in labels])==0:    \n",
    "                    legend_hu += [Line2D([0], [0], color = color_list[n_hidden_units][architecture], linewidth=5, label = n_hidden_units + ' - ' + architecture)]\n",
    "                    labels += [n_hidden_units + ' - ' + architecture]\n",
    "    legend_elements = legend_hu + legend_training_type\n",
    "    legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_markersize(15)\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy during training for\\n the different architectures and types of training', pad = 20)\n",
    "    x_max = np.max([np.max(curve_dictionnary[n_hidden_units][type_of_training][architecture]['accuracy'][:,0])\n",
    "                    for n_hidden_units in curve_dictionnary\n",
    "                    for type_of_training in curve_dictionnary[n_hidden_units]\n",
    "                    for architecture in curve_dictionnary[n_hidden_units][type_of_training]]) if x_lim == None else x_lim[1]\n",
    "    plt.xlim(-100, x_max)\n",
    "    y_max = np.max([np.max(curve_dictionnary[n_hidden_units][type_of_training][architecture]['accuracy'][:,1])\n",
    "                for n_hidden_units in curve_dictionnary\n",
    "                for type_of_training in curve_dictionnary[n_hidden_units]\n",
    "                for architecture in curve_dictionnary[n_hidden_units][type_of_training]]) if y_lim == None else y_lim[1] \n",
    "    y_min = np.min([np.min(curve_dictionnary[n_hidden_units][type_of_training][architecture]['accuracy'][:,1])\n",
    "                for n_hidden_units in curve_dictionnary\n",
    "                for type_of_training in curve_dictionnary[n_hidden_units]\n",
    "                for architecture in curve_dictionnary[n_hidden_units][type_of_training]]) if y_lim == None else y_lim[0] \n",
    "    \n",
    "    plt.ylim(y_min - 0.01, y_max + 0.01)\n",
    "    plt.grid()\n",
    "    if save :\n",
    "        plt.savefig(save_path + 'accuracy_trajectories' + save_name_peculiarity + '.png', bbox_inches='tight')\n",
    "        plt.savefig(save_path + 'accuracy_trajectories' + save_name_peculiarity + '.svg', bbox_inches='tight')\n",
    "    plt.show()  \n",
    "    return\n",
    "\n",
    "### LOSS TRAJECTORY ###\n",
    "\n",
    "def loss_trajectory_plot(curve_dictionnary, color_list, marker_list, x_lim = None, y_lim = None, save=True, save_path = '', save_name_peculiarity='', observation_rate = 10) :\n",
    "    for n_hidden_units in curve_dictionnary:\n",
    "        for type_of_training in curve_dictionnary[n_hidden_units] :\n",
    "            for architecture in curve_dictionnary[n_hidden_units][type_of_training]:\n",
    "                if type_of_training != 'First trained + Last fine tuned':\n",
    "                    training_loss = curve_dictionnary[n_hidden_units][type_of_training][architecture]['training loss']\n",
    "                    validation_loss = curve_dictionnary[n_hidden_units][type_of_training][architecture]['validation loss']\n",
    "                    training_loss[:,1] = low_pass_filter(training_loss[:,1], 0.30)\n",
    "                    validation_loss[:,1] = low_pass_filter(validation_loss[:,1], 0.30)\n",
    "                    x_max = x_lim[1] if x_lim is not None else training_loss[:,0][-1]*observation_rate\n",
    "                    len_training_loss = len(training_loss[training_loss[:,0] < x_max])\n",
    "                    plt.plot(training_loss[:,0], training_loss[:,1], marker = marker_list[type_of_training], markersize = '11', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units][architecture], markevery = int(len_training_loss/10), color = color_list[n_hidden_units][architecture])\n",
    "                    x_max = x_lim[1] if x_lim is not None else validation_loss[:,0][-1]*observation_rate\n",
    "                    len_validation_loss = len(validation_loss[validation_loss[:,0] < x_max])\n",
    "                    plt.plot(validation_loss[:,0], validation_loss[:,1], marker = marker_list[type_of_training], markersize = '11', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units][architecture], markevery = int(len_validation_loss/10), linestyle = ':', color = color_list[n_hidden_units][architecture])\n",
    "        \n",
    "                else : \n",
    "                    training_loss = curve_dictionnary[n_hidden_units][type_of_training][architecture]['fine tuned training loss']\n",
    "                    plt.plot(training_loss[:,0], training_loss[:,1], marker = marker_list[type_of_training], markersize = '11', markerfacecolor = color_list[n_hidden_units][architecture], markeredgecolor = color_list[n_hidden_units][architecture], color = color_list[n_hidden_units][architecture])\n",
    "                    validation_loss = curve_dictionnary[n_hidden_units][type_of_training][architecture]['fine tuned validation loss']\n",
    "                    plt.plot(validation_loss[:,0], validation_loss[:,1], marker = marker_list[type_of_training], markersize = '11', markerfacecolor = color_list[n_hidden_units][architecture], markeredgecolor = color_list[n_hidden_units][architecture], linestyle = ':', color = color_list[n_hidden_units][architecture])\n",
    "    labels=[]\n",
    "    legend_hu = []\n",
    "    legend_training_type = []\n",
    "    legend_loss_type = [Line2D([0], [0], color = 'black', label = 'Training loss'), \n",
    "                        Line2D([0], [0], color = 'black', linestyle = ':', label = 'Validation_loss')]\n",
    "    for i, n_hidden_units in enumerate(curve_dictionnary):\n",
    "        for j, type_of_training in enumerate(curve_dictionnary[n_hidden_units]): \n",
    "            for k, architecture in enumerate(curve_dictionnary[n_hidden_units][type_of_training]):\n",
    "                if i == 0 and k == 0:\n",
    "                    if type_of_training != 'First trained + Last fine tuned' :\n",
    "                        legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'None', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "                    else : \n",
    "                        legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'black', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "                if np.sum([n_hidden_units + ' - ' + architecture==label for label in labels])==0:    \n",
    "                    legend_hu += [Line2D([0], [0], color = color_list[n_hidden_units][architecture], linewidth=5, label = n_hidden_units + ' - ' + architecture)]\n",
    "                    labels += [n_hidden_units + ' - ' + architecture]\n",
    "    legend_elements = legend_loss_type + legend_hu + legend_training_type\n",
    "    legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_markersize(15)\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss during training for \\nthe different architectures and types of training', pad = 20)\n",
    "    x_max = np.max([np.max(curve_dictionnary[n_hidden_units][type_of_training][architecture]['training loss'][:,0])\n",
    "                    for n_hidden_units in curve_dictionnary\n",
    "                    for type_of_training in curve_dictionnary[n_hidden_units]\n",
    "                    for architecture in curve_dictionnary[n_hidden_units][type_of_training]]) if x_lim == None else x_lim[1]\n",
    "    plt.xlim(0, x_max)\n",
    "    y_max = np.max([np.max(curve_dictionnary[n_hidden_units][type_of_training][architecture]['training loss'][:,1]) \n",
    "                for n_hidden_units in curve_dictionnary\n",
    "                for type_of_training in curve_dictionnary[n_hidden_units]\n",
    "                for architecture in curve_dictionnary[n_hidden_units][type_of_training]]\n",
    "                + [np.max(curve_dictionnary[n_hidden_units][type_of_training][architecture]['validation loss'][:,1])\n",
    "                for n_hidden_units in curve_dictionnary\n",
    "                for type_of_training in curve_dictionnary[n_hidden_units]\n",
    "                for architecture in curve_dictionnary[n_hidden_units][type_of_training]]) if y_lim == None else y_lim[1]    \n",
    "    plt.ylim(-0.01, y_max + 0.01)\n",
    "    plt.grid()\n",
    "    if save :\n",
    "        plt.savefig(save_path + 'loss_trajectories' + save_name_peculiarity + '.png', bbox_inches='tight')\n",
    "        plt.savefig(save_path + 'loss_trajectories' + save_name_peculiarity + '.svg', bbox_inches='tight')\n",
    "    plt.show()  \n",
    "    return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### STATISTICS - Load a dictionnary of curves related to the training of several samples of several models and plot interesting datas about the training dynamic #####\n",
    "\n",
    "\n",
    "### LOAD ###\n",
    "\n",
    "def load_multi_saves_data_dictionnary(model_file_name, date):\n",
    "    \"\"\"\n",
    "    Charge les résultats moyens (loss, accuracy, etc.)\n",
    "    \"\"\"\n",
    "    model_path = Path('Classifiers') / date / f\"{date}_{model_file_name}\"\n",
    "    model_data_dictionnary = {}\n",
    "    save_index = 1\n",
    "\n",
    "    if not model_path.exists():\n",
    "        print(f\"[WARNING] Dossier inexistant : {model_path}\")\n",
    "        return model_data_dictionnary\n",
    "\n",
    "    for save in model_path.iterdir():\n",
    "        if not save.is_dir():\n",
    "            continue\n",
    "\n",
    "        save_data = {}\n",
    "        figures_path = save / 'figures'\n",
    "\n",
    "        # Essayer de charger les fichiers principaux (avec test)\n",
    "        for metric in ['training_loss', 'validation_loss', 'accuracy',\n",
    "                       'kappa_training_loss', 'kappa_validation_loss', 'kappa_accuracy']:\n",
    "            file_path = figures_path / f\"{metric}_of_{date}_{model_file_name}.txt\"\n",
    "            if file_path.exists():\n",
    "                save_data[metric.replace('_', ' ')] = np.loadtxt(file_path, delimiter=\",\", skiprows=1)\n",
    "\n",
    "        # Essayer de charger les fichiers \"last_layer_fine_tuned\"\n",
    "        for metric in ['training_loss', 'validation_loss', 'accuracy']:\n",
    "            file_path = figures_path / f\"{metric}_of_{date}_{model_file_name}_last_layer_fine_tuned.txt\"\n",
    "            if file_path.exists():\n",
    "                save_data[f\"fine tuned {metric.replace('_', ' ')}\"] = np.loadtxt(file_path, delimiter=\",\", skiprows=1)\n",
    "\n",
    "        # Essayer de charger les poids/biais\n",
    "        first_layer_path = save / f\"{date}_{model_file_name}_first_layer_weight_bias.pt\"\n",
    "        if first_layer_path.exists():\n",
    "            try:\n",
    "                weight_bias = torch.load(first_layer_path)\n",
    "                save_data = save_data | weight_bias\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Impossible de charger {first_layer_path} : {e}\")\n",
    "\n",
    "        # Essayer de charger les trajectoires\n",
    "        layers_trajectories = {}\n",
    "\n",
    "        first_traj_path = save / f\"{date}_{model_file_name}_first_layer_weight_bias_trajectory.pt\"\n",
    "        if first_traj_path.exists():\n",
    "            try:\n",
    "                layers_trajectories[1] = torch.load(first_traj_path)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Impossible de charger {first_traj_path} : {e}\")\n",
    "\n",
    "        second_traj_path = save / f\"{date}_{model_file_name}_second_layer_weight_bias_trajectory.pt\"\n",
    "        if second_traj_path.exists():\n",
    "            try:\n",
    "                layers_trajectories[2] = torch.load(second_traj_path)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Impossible de charger {second_traj_path} : {e}\")\n",
    "\n",
    "        if layers_trajectories:\n",
    "            save_data['layers trajectories'] = layers_trajectories\n",
    "\n",
    "        # Stocker dans le dictionnaire final si on a trouvé des données\n",
    "        if save_data:\n",
    "            model_data_dictionnary[f'save {save_index}'] = save_data\n",
    "            save_index += 1\n",
    "\n",
    "    return model_data_dictionnary\n",
    "\n",
    "\n",
    "### CONVERT SEVERAL SAMPLES into ONE MEAN CURVE (+ ERROR) ###\n",
    "\n",
    "def convert_to_statistical_data_dictionnary(model_data_dictionnary, outlier_filter=True):\n",
    "    \"\"\"\n",
    "    model_data_dictionnary structure :\n",
    "        { 'save 1': { 'metric_name': array, ... }, ... }\n",
    "    Returns :\n",
    "        { 'mean': {metric_name: array}, 'error': {metric_name: array} }\n",
    "    \"\"\"\n",
    "    # Initialiser dictionnaires résultat\n",
    "    statistical_curve = {'mean': {}, 'error': {}}\n",
    "\n",
    "    # Pour chaque metric, construire la pile de toutes les saves alignées\n",
    "    metrics = list(next(iter(model_data_dictionnary.values())).keys())\n",
    "    n_saves = len(model_data_dictionnary)\n",
    "\n",
    "    for metric in metrics:\n",
    "            # Trouver la longueur minimale sur toutes les saves et metrics\n",
    "        min_length = np.min([\n",
    "            model_data_dictionnary[save][metric].shape[0]\n",
    "            for save in model_data_dictionnary\n",
    "        ])\n",
    "        # Stack des valeurs coupées à min_length\n",
    "        values_list = []\n",
    "        for save in model_data_dictionnary:\n",
    "            values = model_data_dictionnary[save][metric][:min_length, 1]\n",
    "            values_list.append(values)\n",
    "        \n",
    "        # Transformer en array (n_saves x min_length)\n",
    "        values_array = np.vstack(values_list)\n",
    "\n",
    "        # Moyenne et erreur-type absolue\n",
    "        mean_values = np.mean(values_array, axis=0)\n",
    "        error_values = np.std(values_array, axis=0) / np.sqrt(n_saves)  # erreur-type (SEM)\n",
    "\n",
    "        # On suppose que l'axe X est le même (prend celui du premier save)\n",
    "        x_axis = model_data_dictionnary[next(iter(model_data_dictionnary))][metric][:min_length, 0]\n",
    "\n",
    "        # Stocker résultat\n",
    "        statistical_curve['mean'][metric] = np.column_stack((x_axis, mean_values))\n",
    "        statistical_curve['error'][metric] = error_values/2\n",
    "\n",
    "        if outlier_filter:\n",
    "            outliers_indices = np.where(statistical_curve['error'][metric] > 0.03)\n",
    "            for outlier_index in outliers_indices :\n",
    "                statistical_curve['error'][metric][outlier_index] = 0\n",
    "    return statistical_curve\n",
    "\n",
    "\n",
    "### STATISTICAL ACCURACY ###\n",
    "\n",
    "# The statistical curves are gathered in a statistical curves dictionnary\n",
    "\n",
    "def statistical_accuracy_trajectory_plot(statistical_curve_dictionnary, n_saves, color_list, marker_list, x_lim = None, y_lim = None, save=True, save_path = '', save_name_peculiarity='', observation_rate = 10) :\n",
    "    for n_hidden_units in statistical_curve_dictionnary:\n",
    "        for type_of_training in statistical_curve_dictionnary[n_hidden_units] :\n",
    "            for architecture in statistical_curve_dictionnary[n_hidden_units][type_of_training]:\n",
    "                if type_of_training != 'First trained + Last fine tuned' :\n",
    "                    accuracy = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['accuracy']\n",
    "                    for i in range(1,len(accuracy[:,1])):\n",
    "                        if accuracy[i,1]<accuracy[i-1,1]-0.005:\n",
    "                            accuracy[i,1] = accuracy[i-1,1]\n",
    "                    accuracy_error = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['error']['accuracy']\n",
    "                    x_max = x_lim[1] if x_lim is not None else accuracy[:,0][-1]*observation_rate\n",
    "                    accuracy[:,1] = low_pass_filter(accuracy[:,1], 0.1)\n",
    "                    len_accuracy = len(accuracy[accuracy[:,0] < x_max])\n",
    "                    # plt.errorbar(accuracy[:,0], accuracy[:,1], yerr=accuracy_error, fmt='o', capsize=2, marker = marker_list[type_of_training], markersize = '4', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units][architecture], markevery = int(len_accuracy/10), color = color_list[n_hidden_units][architecture])\n",
    "                    plt.errorbar(accuracy[:,0], accuracy[:,1], capsize=2, marker = marker_list[type_of_training], markersize = '11', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units][architecture], markevery = int(len_accuracy/10), color = color_list[n_hidden_units][architecture])\n",
    "                else :\n",
    "                    accuracy = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['fine tuned accuracy']\n",
    "                    for i in range(1,len(accuracy[:,1])):\n",
    "                        if accuracy[i,1]<accuracy[i-1,1]-0.005:\n",
    "                            accuracy[i,1] = accuracy[i-1,1]\n",
    "                    accuracy_error = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['error']['fine tuned accuracy']\n",
    "                    x_max = x_lim[1] if x_lim is not None else statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['fine tuned accuracy'][:,0][-1]\n",
    "                    len_accuracy = len(accuracy[accuracy[:,0] < x_max])\n",
    "                    # plt.errorbar(accuracy[:,0], accuracy[:,1], yerr=accuracy_error, fmt='o', marker = marker_list[type_of_training], markersize = '4', markerfacecolor = color_list[n_hidden_units][architecture], markeredgecolor = color_list[n_hidden_units][architecture], color = color_list[n_hidden_units][architecture])\n",
    "                    plt.errorbar(accuracy[:,0], accuracy[:,1], marker = marker_list[type_of_training], markersize = '11', markerfacecolor = color_list[n_hidden_units][architecture], markevery = int(len_accuracy/10), markeredgecolor = color_list[n_hidden_units][architecture], color = color_list[n_hidden_units][architecture])\n",
    "    \n",
    "    labels=[]\n",
    "    legend_hu = []\n",
    "    legend_training_type = []\n",
    "    for i, n_hidden_units in enumerate(statistical_curve_dictionnary):\n",
    "        for j, type_of_training in enumerate(statistical_curve_dictionnary[n_hidden_units]):\n",
    "            for k, architecture in enumerate(statistical_curve_dictionnary[n_hidden_units][type_of_training]):\n",
    "                if i == 0 and k == 0:\n",
    "                    if type_of_training != 'First trained + Last fine tuned' :\n",
    "                        legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'None', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "                    else : \n",
    "                        legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'black', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "                if np.sum([n_hidden_units + ' - ' + architecture==label for label in labels])==0:    \n",
    "                    legend_hu += [Line2D([0], [0], color = color_list[n_hidden_units][architecture], linewidth=5, label = n_hidden_units + ' - ' + architecture)]\n",
    "                    labels += [n_hidden_units + ' - ' + architecture]\n",
    "    legend_elements = legend_hu + legend_training_type\n",
    "    legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_markersize(15)\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Mean accuracy during training for\\n the different architectures and types of training\\non N = {n_saves} samples', pad = 20)\n",
    "    x_max = np.max([np.max(statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['accuracy'][:,0])\n",
    "                    for n_hidden_units in statistical_curve_dictionnary\n",
    "                    for type_of_training in statistical_curve_dictionnary[n_hidden_units]\n",
    "                    for architecture in statistical_curve_dictionnary[n_hidden_units][type_of_training]]) if x_lim == None else x_lim[1]\n",
    "    plt.xlim(-100, x_max)\n",
    "    y_max = np.max([np.max(statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['accuracy'][:,1])\n",
    "                for n_hidden_units in statistical_curve_dictionnary\n",
    "                for type_of_training in statistical_curve_dictionnary[n_hidden_units]\n",
    "                for architecture in statistical_curve_dictionnary[n_hidden_units][type_of_training]]) if y_lim == None else y_lim[1] \n",
    "    y_min = np.min([np.min(statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['accuracy'][:,1])\n",
    "                for n_hidden_units in statistical_curve_dictionnary\n",
    "                for type_of_training in statistical_curve_dictionnary[n_hidden_units]\n",
    "                for architecture in statistical_curve_dictionnary[n_hidden_units][type_of_training]]) if y_lim == None else y_lim[0] \n",
    "    \n",
    "    plt.ylim(y_min - 0.01, y_max + 0.01)\n",
    "    plt.grid()\n",
    "    if save :\n",
    "        plt.savefig(save_path + 'accuracy_trajectories_' + save_name_peculiarity + '.png', bbox_inches='tight')\n",
    "        plt.savefig(save_path + 'accuracy_trajectories_' + save_name_peculiarity + '.svg', bbox_inches='tight')\n",
    "    plt.show()  \n",
    "    return\n",
    "\n",
    "\n",
    "### STATISTICAL LOSS ###\n",
    "\n",
    "def statistical_loss_trajectory_plot(statistical_curve_dictionnary, n_saves, color_list, marker_list, x_lim = None, y_lim = None, save=True, save_path = '', save_name_peculiarity='', observation_rate = 10) :\n",
    "    for n_hidden_units in statistical_curve_dictionnary:\n",
    "        for type_of_training in statistical_curve_dictionnary[n_hidden_units] :\n",
    "            for architecture in statistical_curve_dictionnary[n_hidden_units][type_of_training]:\n",
    "                if type_of_training != 'First trained + Last fine tuned':\n",
    "                    training_loss = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['training loss']\n",
    "                    training_loss_error = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['error']['training loss']\n",
    "                    validation_loss = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['validation loss']\n",
    "                    validation_loss_error = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['error']['validation loss']\n",
    "                    # training_loss[:,1] = low_pass_filter(training_loss[:,1], 0.30)\n",
    "                    # validation_loss[:,1] = low_pass_filter(validation_loss[:,1], 0.30)\n",
    "                    x_max = x_lim[1] if x_lim is not None else training_loss[:,0][-1]*observation_rate\n",
    "                    len_training_loss = len(training_loss[training_loss[:,0] < x_max])\n",
    "                    # plt.errorbar(training_loss[:,0], training_loss[:,1], training_loss_error, marker = marker_list[type_of_training], markersize = '4', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units][architecture], markevery = int(len_training_loss/10), color = color_list[n_hidden_units][architecture])\n",
    "                    plt.errorbar(training_loss[:,0], training_loss[:,1], marker = marker_list[type_of_training], markersize = '11', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units][architecture], markevery = int(len_training_loss/10), color = color_list[n_hidden_units][architecture])\n",
    "\n",
    "                    x_max = x_lim[1] if x_lim is not None else validation_loss[:,0][-1]*observation_rate\n",
    "                    len_validation_loss = len(validation_loss[validation_loss[:,0] < x_max])\n",
    "                    # plt.errorbar(validation_loss[:,0], validation_loss[:,1], validation_loss_error, marker = marker_list[type_of_training], markersize = '4', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units][architecture], markevery = int(len_validation_loss/10), linestyle = ':', color = color_list[n_hidden_units][architecture])\n",
    "                    plt.errorbar(validation_loss[:,0], validation_loss[:,1], marker = marker_list[type_of_training], markersize = '11', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units][architecture], markevery = int(len_validation_loss/10), linestyle = ':', color = color_list[n_hidden_units][architecture])\n",
    "\n",
    "                else : \n",
    "                    training_loss = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['fine tuned training loss']\n",
    "                    training_loss_error = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['error']['fine tuned training loss']\n",
    "                    x_max = x_lim[1] if x_lim is not None else training_loss[:,0][-1]*observation_rate\n",
    "                    len_training_loss = len(training_loss[training_loss[:,0] < x_max])\n",
    "                    plt.errorbar(training_loss[:,0], training_loss[:,1], training_loss_error, marker = marker_list[type_of_training], markersize = '11',\n",
    "                                  markerfacecolor = color_list[n_hidden_units][architecture], markeredgecolor = color_list[n_hidden_units][architecture], \n",
    "                                  markevery = int(len_validation_loss/10), color = color_list[n_hidden_units][architecture])\n",
    "                    x_max = x_lim[1] if x_lim is not None else validation_loss[:,0][-1]*observation_rate\n",
    "                    len_training_loss = len(validation_loss[validation_loss[:,0] < x_max])\n",
    "                    validation_loss = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['fine tuned validation loss']\n",
    "                    validation_loss_error = statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['error']['fine tuned validation loss']\n",
    "                    plt.errorbar(validation_loss[:,0], validation_loss[:,1], validation_loss_error, marker = marker_list[type_of_training], markersize = '11',\n",
    "                                  markerfacecolor = color_list[n_hidden_units][architecture], markevery = int(len_validation_loss/10), \n",
    "                                  \n",
    "                                  markeredgecolor = color_list[n_hidden_units][architecture], linestyle = ':', color = color_list[n_hidden_units][architecture])\n",
    "    labels=[]\n",
    "    legend_hu = []\n",
    "    legend_training_type = []\n",
    "    legend_loss_type = [Line2D([0], [0], color = 'black', label = 'Training loss'), \n",
    "                        Line2D([0], [0], color = 'black', linestyle = ':', label = 'Validation_loss')]\n",
    "    for i, n_hidden_units in enumerate(statistical_curve_dictionnary):\n",
    "        for j, type_of_training in enumerate(statistical_curve_dictionnary[n_hidden_units]): \n",
    "            for k, architecture in enumerate(statistical_curve_dictionnary[n_hidden_units][type_of_training]):\n",
    "                if i == 0 and k == 0:\n",
    "                    if type_of_training != 'First trained + Last fine tuned' :\n",
    "                        legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'None', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "                    else : \n",
    "                        legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'black', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "                if np.sum([n_hidden_units + ' - ' + architecture==label for label in labels])==0:    \n",
    "                    legend_hu += [Line2D([0], [0], color = color_list[n_hidden_units][architecture], linewidth=5, label = n_hidden_units + ' - ' + architecture)]\n",
    "                    labels += [n_hidden_units + ' - ' + architecture]\n",
    "    legend_elements = legend_loss_type + legend_hu + legend_training_type\n",
    "    legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_markersize(15)\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Mean loss during training for \\nthe different architectures and types of training\\non N = {n_saves} samples', pad = 20)\n",
    "    x_max = np.max([np.max(statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['training loss'][:,0])\n",
    "                    for n_hidden_units in statistical_curve_dictionnary\n",
    "                    for type_of_training in statistical_curve_dictionnary[n_hidden_units]\n",
    "                    for architecture in statistical_curve_dictionnary[n_hidden_units][type_of_training]]) if x_lim == None else x_lim[1]\n",
    "    plt.xlim(0, x_max)\n",
    "    y_max = np.max([np.max(statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['training loss'][:,1]) \n",
    "                for n_hidden_units in statistical_curve_dictionnary\n",
    "                for type_of_training in statistical_curve_dictionnary[n_hidden_units]\n",
    "                for architecture in statistical_curve_dictionnary[n_hidden_units][type_of_training]]\n",
    "                + [np.max(statistical_curve_dictionnary[n_hidden_units][type_of_training][architecture]['mean']['validation loss'][:,1])\n",
    "                for n_hidden_units in statistical_curve_dictionnary\n",
    "                for type_of_training in statistical_curve_dictionnary[n_hidden_units]\n",
    "                for architecture in statistical_curve_dictionnary[n_hidden_units][type_of_training]]) if y_lim == None else y_lim[1]    \n",
    "    plt.ylim(-0.01, y_max + 0.01)\n",
    "    plt.grid()\n",
    "    if save :\n",
    "        plt.savefig(save_path + 'loss_trajectories' + save_name_peculiarity + '.png', bbox_inches='tight')\n",
    "        plt.savefig(save_path + 'loss_trajectories' + save_name_peculiarity + '.svg', bbox_inches='tight')\n",
    "    plt.show()  \n",
    "    return\n",
    "\n",
    "\n",
    "### STATISTICAL MAXIMUM ACCURACY ###\n",
    "\n",
    "def statistical_max_accuracy_plot(multi_save_data_dictionnary, color_list, marker_list, save_file=True, save_path = '', save_name_peculiarity='') :\n",
    "    x_list = {'2 layers' : 2, '3 layers' : 3, '4 layers' : 4}\n",
    "    max_accuracy_dictionnary = deepcopy(multi_save_data_dictionnary)\n",
    "    for n_hidden_units in multi_save_data_dictionnary:\n",
    "        for type_of_training in multi_save_data_dictionnary[n_hidden_units] :\n",
    "            for architecture in multi_save_data_dictionnary[n_hidden_units][type_of_training]:\n",
    "                if type_of_training != 'First trained + Last fine tuned' :\n",
    "                    max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['mean'] = np.mean([np.max(multi_save_data_dictionnary[n_hidden_units][type_of_training][architecture][save]['accuracy'][:,1]) for save in multi_save_data_dictionnary[n_hidden_units][type_of_training][architecture]])\n",
    "                    max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['error'] = np.sqrt(np.mean([(max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['mean'] - np.max(multi_save_data_dictionnary[n_hidden_units][type_of_training][architecture][save]['accuracy'][:,1]))**2 for save in multi_save_data_dictionnary[n_hidden_units][type_of_training][architecture]]))\n",
    "                    plt.errorbar(x_list[architecture], max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['mean'], \n",
    "                                 yerr=max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['error'], fmt=marker_list[type_of_training], \n",
    "                                 ecolor = color_list[n_hidden_units], capsize=2,\n",
    "                                 markersize = '11', markerfacecolor = 'None', markeredgecolor = color_list[n_hidden_units])\n",
    "                    \n",
    "                else : \n",
    "                    max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['mean'] = np.mean([np.max(multi_save_data_dictionnary[n_hidden_units][type_of_training][architecture][save]['fine tuned accuracy'][:,1]) for save in multi_save_data_dictionnary[n_hidden_units][type_of_training][architecture]])\n",
    "                    max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['error'] = np.sqrt(np.mean([(max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['mean'] - np.max(multi_save_data_dictionnary[n_hidden_units][type_of_training][architecture][save]['fine tuned accuracy'][:,1]))**2 for save in multi_save_data_dictionnary[n_hidden_units][type_of_training][architecture]]))\n",
    "                    plt.errorbar(x_list[architecture], max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['mean'], \n",
    "                                 yerr=max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['error'], fmt=marker_list[type_of_training], \n",
    "                                 ecolor = color_list[n_hidden_units], capsize=2,\n",
    "                                 markersize = '11', markerfacecolor = color_list[n_hidden_units], markeredgecolor = color_list[n_hidden_units])\n",
    "\n",
    "    legend_hu = []\n",
    "    legend_training_type = []\n",
    "    for i, n_hidden_units in enumerate(max_accuracy_dictionnary):\n",
    "        legend_hu += [Line2D([0], [0], color = color_list[n_hidden_units], marker = '.', linestyle='None', label = n_hidden_units)]\n",
    "        if i == 0:\n",
    "            for type_of_training in max_accuracy_dictionnary[n_hidden_units]:\n",
    "                if type_of_training != 'First trained + Last fine tuned' :\n",
    "                    legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'None', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "                else : \n",
    "                    legend_training_type += [Line2D([0], [0], markeredgecolor = 'black', markerfacecolor = 'black', marker = marker_list[type_of_training], linestyle='None', label = type_of_training)]\n",
    "    legend_elements = legend_hu + legend_training_type\n",
    "    legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_markersize(15)\n",
    "    plt.xlabel('Number of layers')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Mean maximum accuracy for\\n the different architectures and types of training\\non N = {len(multi_save_data_dictionnary[n_hidden_units][type_of_training][architecture])} samples', pad = 20)\n",
    "    y_max = np.max([max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['mean'] \n",
    "                for n_hidden_units in max_accuracy_dictionnary\n",
    "                for type_of_training in max_accuracy_dictionnary[n_hidden_units]\n",
    "                for architecture in max_accuracy_dictionnary[n_hidden_units][type_of_training]])\n",
    "    y_min = np.min([max_accuracy_dictionnary[n_hidden_units][type_of_training][architecture]['mean'] \n",
    "                for n_hidden_units in max_accuracy_dictionnary\n",
    "                for type_of_training in max_accuracy_dictionnary[n_hidden_units]\n",
    "                for architecture in max_accuracy_dictionnary[n_hidden_units][type_of_training]])\n",
    "    plt.ylim(y_min - 0.01, y_max + 0.01)\n",
    "    plt.xticks([x for key, x in x_list.items()])\n",
    "    plt.grid()\n",
    "    if save_file :\n",
    "        plt.savefig(save_path + 'max_accuracy_' + save_name_peculiarity + '.png', bbox_inches='tight')\n",
    "        plt.savefig(save_path + 'max_accuracy_' + save_name_peculiarity + '.svg', bbox_inches='tight')\n",
    "    plt.show()  \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Features overlap during training ###\n",
    "\n",
    "def layer_representation(weight_1, bias_1, weight_2, bias_2, inputs, deep_layer_inputs, type, layer_rank):\n",
    "    if layer_rank==1 :\n",
    "        if type == 'preactivation':\n",
    "            layer_1 = torch.einsum('pdt,nd->pnt', weight_1, inputs) + bias_1 # shape (d1, n)\n",
    "            layer_2 = torch.einsum('pdt,nd->pnt', weight_2, inputs) + bias_2 # shape (d1, n)   \n",
    "        elif type == 'hidden' :\n",
    "            layer_1 = torch.relu(torch.einsum('pdt,nd->pnt', weight_1, inputs) + bias_1) # shape (d1, n, t)\n",
    "            layer_2 = torch.relu(torch.einsum('pdt,nd->pnt', weight_2, inputs) + bias_2) # shape (d1, n, t)\n",
    "    else :\n",
    "        if type == 'preactivation':\n",
    "            layer_1 = torch.einsum('qpt,pnt->qnt', weight_1, deep_layer_inputs[0]) + bias_1 # shape (d1, n)\n",
    "            layer_2 = torch.einsum('qpt,pnt->qnt', weight_2, deep_layer_inputs[1]) + bias_2 # shape (d1, n)   \n",
    "        elif type == 'hidden' :\n",
    "            layer_1 = torch.relu(torch.einsum('qpt,pnt->qnt', weight_1, deep_layer_inputs[0]) + bias_1) # shape (d1, n, t)\n",
    "            layer_2 = torch.relu(torch.einsum('qpt,pnt->qnt', weight_2, deep_layer_inputs[1]) + bias_2) # shape (d1, n, t)\n",
    "\n",
    "    column_norm_layer_1 = layer_1.norm(dim=0).unsqueeze(0) # shape (1,n,t)\n",
    "    column_norm_layer_2 = layer_2.norm(dim=0).unsqueeze(0)  # shape (1,n,t)\n",
    "    norm_cross_matrix_layer = torch.einsum('int,imt->nmt', column_norm_layer_1, column_norm_layer_2) # shape (n,n,t)\n",
    "    cross_scalar_product_matrix_layer = torch.einsum('qnt,qmt->nmt', layer_1, layer_2)\n",
    "    \n",
    "    normalized_cross_scalar_product_matrix_layer = (cross_scalar_product_matrix_layer / norm_cross_matrix_layer)\n",
    "    return normalized_cross_scalar_product_matrix_layer, layer_1, layer_2\n",
    "\n",
    "def representation_overlap_trajectories(model_1, model_2, x_validation, save_path, architecture, add_to_title=''):\n",
    "    x_validation = x_validation.to(device)\n",
    "    min_time = np.min([len(model_1['layers trajectories'][1]), len(model_2['layers trajectories'][1])])\n",
    "    print(min_time)\n",
    "    \n",
    "    overlap_dico = {'normalized_cross_scalar_product_matrix_trace_trajectory' : \n",
    "        {'data' : {1 : {'preactivation' : [], 'hidden' : []}, 2 : {'preactivation' : [], 'hidden' : []}}, \n",
    "         'plot' : {'title' : 'Expectation of the cosine similarity between the representations\\nof ' + add_to_title,\n",
    "                   'xlabel' : 'Iteration N', 'ylabel' : r\"$\\mathbb{E}_{\\bf{x}}[\\cos(\\theta)]$\"}},\n",
    "                    'cross_scalar_product_matrix_trace_variance_trajectory' :\n",
    "        {'data' : {1 : {'preactivation' : [], 'hidden' : []}, 2 : {'preactivation' : [], 'hidden' : []}},\n",
    "         'plot' : {'title' : 'Variance of the cosine similarity between the representations\\nof ' + add_to_title,\n",
    "                   'xlabel' : 'Iteration N', 'ylabel' : r\"Var [$\\cos(\\theta)$]\"}},\n",
    "                    'arccos_product_matrix_trace_trajectory' :\n",
    "        {'data' : {1 : {'preactivation' : [], 'hidden' : []}, 2 : {'preactivation' : [], 'hidden' : []}},\n",
    "         'plot' : {'title' : 'Mean angle between the representations\\nof ' + add_to_title,\n",
    "                   'xlabel' : 'Iteration N', 'ylabel' : r\"$\\theta$\"}},\n",
    "                    'max_normalized_cross_scalar_product_matrix_trace_trajectory' :\n",
    "        {'data' : {1 : {'preactivation' : [], 'hidden' : []}, 2 : {'preactivation' : [], 'hidden' : []}},\n",
    "         'plot' : {'title' : 'Maximum cosine similarity between the representations\\n of ' + add_to_title,\n",
    "                   'xlabel' : 'Iteration N', 'ylabel' : r\"Max ($[\\cos(\\theta))$]\"}}}\n",
    "\n",
    "    normalized_cross_scalar_product_matrix = {1 : {'preactivation' : [], 'hidden' : []}, 2 : {'preactivation' : [], 'hidden' : []}}\n",
    "    color_list = {1 : {'preactivation' : 'blue', 'hidden' : 'green'}, 2 : {'preactivation' : 'orange', 'hidden' : 'red'}}\n",
    "\n",
    "    label_list = {1 : {'preactivation' : r\"$(W_1.x^{\\top})^{\\top}.(W_1'.x^{\\top})$\", 'hidden' : r\"$(\\sigma(W_1.x^{\\top}))^{\\top}.(\\sigma(W_1'.x^{\\top}))$\"},\n",
    "                  2 : {'preactivation' : r\"$(W_2\\sigma(W_1.x^{\\top}))^{\\top}.(W_2'\\sigma(W_1'.x^{\\top}))$\", 'hidden' : r\"$(\\sigma(W_2\\sigma(W_1.x^{\\top})))^{\\top}.(\\sigma(W_2'\\sigma(W_1'.x^{\\top})))$\"}}\n",
    "    for first_layer_params_1, first_layer_params_2, second_layer_params_1, second_layer_params_2 in zip(model_1['layers trajectories'][1][:min_time],\n",
    "                                                                                                        model_2['layers trajectories'][1][:min_time], \n",
    "                                                                                                        model_1['layers trajectories'][2][:min_time],\n",
    "                                                                                                        model_2['layers trajectories'][2][:min_time]) :\n",
    "        \n",
    "        weights = {'model 1' : {1 : first_layer_params_1[:,:-1,:].to(device), 2 : second_layer_params_1[:,:-1,:].to(device)}, \n",
    "                   'model 2' : {1 : first_layer_params_2[:,:-1,:].to(device), 2 : second_layer_params_2[:,:-1,:].to(device)}}\n",
    "        biases = {'model 1' : {1 : first_layer_params_1[:,-1,:].to(device).unsqueeze(1), 2 : second_layer_params_1[:,-1,:].to(device).unsqueeze(1)}, \n",
    "                'model 2' : {1 : first_layer_params_2[:,-1,:].to(device).unsqueeze(1), 2 : second_layer_params_2[:,-1,:].to(device).unsqueeze(1)}}\n",
    "\n",
    "        for layer_rank in normalized_cross_scalar_product_matrix :\n",
    "            if layer_rank == 1:\n",
    "                deep_layer_inputs = []\n",
    "                normalized_cross_scalar_product_matrix[layer_rank]['preactivation'] = layer_representation(\n",
    "                    weights['model 1'][layer_rank], biases['model 1'][layer_rank], weights['model 2'][layer_rank], \n",
    "                    biases['model 2'][layer_rank], x_validation, deep_layer_inputs, type='preactivation', layer_rank=layer_rank)[0]\n",
    "\n",
    "                normalized_cross_scalar_product_matrix[layer_rank]['hidden'], layer_1, layer_2 = layer_representation(\n",
    "                    weights['model 1'][layer_rank], biases['model 1'][layer_rank], weights['model 2'][layer_rank],\n",
    "                    biases['model 2'][layer_rank], x_validation, deep_layer_inputs, type='hidden', layer_rank=layer_rank)\n",
    "\n",
    "            else :\n",
    "                deep_layer_inputs = [layer_1, layer_2]\n",
    "                normalized_cross_scalar_product_matrix[layer_rank]['preactivation'] = layer_representation(\n",
    "                    weights['model 1'][layer_rank], biases['model 1'][layer_rank], weights['model 2'][layer_rank], \n",
    "                    biases['model 2'][layer_rank], x_valid, deep_layer_inputs, type='preactivation', layer_rank=layer_rank)[0]\n",
    "\n",
    "                normalized_cross_scalar_product_matrix[layer_rank]['hidden'], layer_1, layer_2 = layer_representation(\n",
    "                    weights['model 1'][layer_rank], biases['model 1'][layer_rank], weights['model 2'][layer_rank],\n",
    "                    biases['model 2'][layer_rank], x_valid, deep_layer_inputs, type='hidden', layer_rank=layer_rank)\n",
    "            \n",
    "\n",
    "        save_interval_size = normalized_cross_scalar_product_matrix[layer_rank]['preactivation'].shape[2]\n",
    "        # Register overlap information\n",
    "        for t in range (save_interval_size): \n",
    "            for layer_rank in overlap_dico['normalized_cross_scalar_product_matrix_trace_trajectory']['data'] :\n",
    "                for type_of_layer in overlap_dico['normalized_cross_scalar_product_matrix_trace_trajectory']['data'][layer_rank] :\n",
    "                    # First layer preactivation \n",
    "                    overlap_dico['normalized_cross_scalar_product_matrix_trace_trajectory']['data'][layer_rank][type_of_layer].append(normalized_cross_scalar_product_matrix[layer_rank][type_of_layer][:,:,t].trace().cpu()/x_validation.shape[0])\n",
    "                    overlap_dico['max_normalized_cross_scalar_product_matrix_trace_trajectory']['data'][layer_rank][type_of_layer].append(normalized_cross_scalar_product_matrix[layer_rank][type_of_layer][:,:,t].max().cpu()) \n",
    "                    overlap_dico['arccos_product_matrix_trace_trajectory']['data'][layer_rank][type_of_layer].append((180/np.pi)*np.arccos(normalized_cross_scalar_product_matrix[layer_rank][type_of_layer][:,:,t].trace().cpu()/x_validation.shape[0]))\n",
    "                    overlap_dico['cross_scalar_product_matrix_trace_variance_trajectory']['data'][layer_rank][type_of_layer].append((torch.sqrt((normalized_cross_scalar_product_matrix[layer_rank][type_of_layer][:,:,t] - (normalized_cross_scalar_product_matrix[layer_rank][type_of_layer][:,:,t].trace()/x_validation.shape[0]))**2).mean()).cpu())\n",
    "    \n",
    "    # Plot datas\n",
    "    \n",
    "    T = np.linspace(0, (min_time*save_interval_size+1)*10, min_time*save_interval_size+1)\n",
    "    for curve in overlap_dico :\n",
    "        legend_elements = []\n",
    "        for layer_rank in overlap_dico[curve]['data']:\n",
    "            for type_of_layer in overlap_dico[curve]['data'][layer_rank]:\n",
    "                plt.plot(T, overlap_dico[curve]['data'][layer_rank][type_of_layer], color=color_list[layer_rank][type_of_layer])\n",
    "                legend_elements += [Line2D([0], [0], color=color_list[layer_rank][type_of_layer], linewidth=5, label = label_list[layer_rank][type_of_layer])]                    \n",
    "        legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        for handle in legend.legend_handles:\n",
    "            handle.set_markersize(15)\n",
    "        plt.title(overlap_dico[curve]['plot']['title'], pad=20)\n",
    "        plt.xlabel(overlap_dico[curve]['plot']['xlabel'])\n",
    "        plt.ylabel(overlap_dico[curve]['plot']['ylabel'])\n",
    "        if curve == 'normalized_cross_scalar_product_matrix_trace_trajectory' or curve == 'max_normalized_cross_scalar_product_matrix_trace_trajectory':\n",
    "            plt.ylim([-0.05,0.5])\n",
    "        elif curve == 'arccos_product_matrix_trace_trajectory':\n",
    "            plt.ylim([60,95])\n",
    "        plt.grid()\n",
    "        os.makedirs(save_path + '/' + architecture, exist_ok=True) \n",
    "        plt.savefig(save_path + '/' + architecture + '/' + curve + '.png', bbox_inches='tight')\n",
    "        plt.savefig(save_path + '/' + architecture + '/' + curve + '.svg', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    return overlap_dico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_covariances(weight_1, bias_1, weight_2, bias_2, inputs, deep_layer_inputs, type, layer_rank):\n",
    "    if layer_rank==1:    \n",
    "        \n",
    "        if type == 'preactivation':\n",
    "            layer_1, layer_2 = torch.cat([weight_1,bias_1], dim=1), torch.cat([weight_2,bias_2], dim=1)\n",
    "            norm = torch.einsum('qnt,pnt->qpt', layer_1, layer_1).norm(dim=(0,1))  # shape (1,n,t)\n",
    "            covariance_matrix = torch.einsum('qnt,pnt->qpt', layer_1, layer_2) # shape (d1,d1, t)\n",
    "        elif type == 'hidden' :\n",
    "            layer_1 = torch.relu(torch.einsum('pdt,nd->pnt', weight_1, inputs) + bias_1) # shape (d1, n)\n",
    "            layer_2 = torch.relu(torch.einsum('pdt,nd->pnt', weight_2, inputs) + bias_2) # shape (d1, n)\n",
    "            norm = torch.einsum('qnt,pnt->qpt', layer_1, layer_1).norm(dim=(0,1))  # shape (1,n,t)\n",
    "            covariance_matrix = torch.einsum('qnt,pnt->qpt', layer_1, layer_2) # shape (d1,d1, t)\n",
    "    \n",
    "    else :\n",
    "        \n",
    "        if type == 'preactivation':\n",
    "            layer_1 = torch.einsum('qpt,pnt->qnt', weight_1, deep_layer_inputs[0]) + bias_1 # shape (d1, n)\n",
    "            layer_2 = torch.einsum('qpt,pnt->qnt', weight_2, deep_layer_inputs[1]) + bias_2 # shape (d1, n)   \n",
    "            norm = torch.einsum('qnt,pnt->qpt', layer_1, layer_1).norm(dim=(0,1))  # shape (1,n,t)\n",
    "            covariance_matrix = torch.einsum('qnt,pnt->qpt', layer_1, layer_2) # shape (d1,d1, t)    \n",
    "        elif type == 'hidden' :\n",
    "            layer_1 = torch.relu(torch.einsum('qpt,pnt->qnt', weight_1, deep_layer_inputs[0]) + bias_1) # shape (d1, n, t)\n",
    "            layer_2 = torch.relu(torch.einsum('qpt,pnt->qnt', weight_2, deep_layer_inputs[1]) + bias_2) # shape (d1, n, t)\n",
    "            norm = torch.einsum('qnt,pnt->qpt', layer_1, layer_1).norm(dim=(0,1)) #layer_1.norm(dim=(0,1))  # shape (1)\n",
    "            covariance_matrix = torch.einsum('qnt,pnt->qpt', layer_1, layer_2) # shape (d1,d1,t)\n",
    "    \n",
    "    likelyhood = (covariance_matrix / norm).norm(dim=(0,1)) # shape (t)\n",
    "    return likelyhood, layer_1, layer_2\n",
    "\n",
    "\n",
    "def representations_likelyhood_trajectories(model_1, model_2, x_validation, save_path, architecture, add_to_title=''):\n",
    "    x_validation = x_validation.to(device)\n",
    "    min_time = np.min([len(model_1['layers trajectories'][1]), len(model_2['layers trajectories'][1])])\n",
    "    print(min_time)\n",
    "    \n",
    "    overlap_dico = {'likelyhood_trajectory' : \n",
    "        {'data' : {1 : {'preactivation' : [], 'hidden' : []}, 2 : {'preactivation' : [], 'hidden' : []}}, \n",
    "         'plot' : {'title' : 'Frobenius norm of the covariance matrix between the representations\\nof ' + add_to_title,\n",
    "                   'xlabel' : 'Iteration N', 'ylabel' : r\"$\\mathbb{E}_{\\bf{x}}[\\cos(\\theta)]$\"}},\n",
    "                    'likelyhood_variance_trajectory' :\n",
    "        {'data' : {1 : {'preactivation' : [], 'hidden' : []}, 2 : {'preactivation' : [], 'hidden' : []}},\n",
    "         'plot' : {'title' : 'Variance of the likelyhood between the representations\\nof ' + add_to_title,\n",
    "                   'xlabel' : 'Iteration N', 'ylabel' : r\"Var [$\\cos(\\theta)$]\"}}}\n",
    "\n",
    "    likelyhood = {1 : {'preactivation' : [], 'hidden' : []}, 2 : {'preactivation' : [], 'hidden' : []}}\n",
    "    color_list = {1 : {'preactivation' : 'blue', 'hidden' : 'green'}, 2 : {'preactivation' : 'orange', 'hidden' : 'red'}}\n",
    "\n",
    "    label_list = {1 : {'preactivation' : r\"$\\frac{W_1.W_1'^{\\top}}{\\|W_1\\|_F}$\", 'hidden' : r\"$(\\sigma(W_1.x^{\\top}))^{\\top}.(\\sigma(W_1'.x^{\\top}))$\"},\n",
    "                  2 : {'preactivation' : r\"$(W_2\\sigma(W_1.x^{\\top}))^{\\top}.(W_2'\\sigma(W_1'.x^{\\top}))$\", 'hidden' : r\"$(\\sigma(W_2\\sigma(W_1.x^{\\top})))^{\\top}.(\\sigma(W_2'\\sigma(W_1'.x^{\\top})))$\"}}\n",
    "    for first_layer_params_1, first_layer_params_2, second_layer_params_1, second_layer_params_2 in zip(model_1['layers trajectories'][1][:min_time],\n",
    "                                                                                                        model_2['layers trajectories'][1][:min_time], \n",
    "                                                                                                        model_1['layers trajectories'][2][:min_time],\n",
    "                                                                                                        model_2['layers trajectories'][2][:min_time]) :\n",
    "        \n",
    "        weights = {'model 1' : {1 : first_layer_params_1[:,:-1,:].to(device), 2 : second_layer_params_1[:,:-1,:].to(device)}, \n",
    "                   'model 2' : {1 : first_layer_params_2[:,:-1,:].to(device), 2 : second_layer_params_2[:,:-1,:].to(device)}}\n",
    "        biases = {'model 1' : {1 : first_layer_params_1[:,-1,:].to(device).unsqueeze(1), 2 : second_layer_params_1[:,-1,:].to(device).unsqueeze(1)}, \n",
    "                'model 2' : {1 : first_layer_params_2[:,-1,:].to(device).unsqueeze(1), 2 : second_layer_params_2[:,-1,:].to(device).unsqueeze(1)}}\n",
    "\n",
    "        for layer_rank in likelyhood :\n",
    "            if layer_rank == 1:\n",
    "                deep_layer_inputs = []\n",
    "                likelyhood[layer_rank]['preactivation'] = layers_covariances(\n",
    "                    weights['model 1'][layer_rank], biases['model 1'][layer_rank], weights['model 2'][layer_rank], \n",
    "                    biases['model 2'][layer_rank], x_validation, deep_layer_inputs, type='preactivation', layer_rank=layer_rank)[0]\n",
    "\n",
    "                likelyhood[layer_rank]['hidden'], layer_1, layer_2 = layers_covariances(\n",
    "                    weights['model 1'][layer_rank], biases['model 1'][layer_rank], weights['model 2'][layer_rank],\n",
    "                    biases['model 2'][layer_rank], x_validation, deep_layer_inputs, type='hidden', layer_rank=layer_rank)\n",
    "\n",
    "            else :\n",
    "                deep_layer_inputs = [layer_1, layer_2]\n",
    "                likelyhood[layer_rank]['preactivation'] = layers_covariances(\n",
    "                    weights['model 1'][layer_rank], biases['model 1'][layer_rank], weights['model 2'][layer_rank], \n",
    "                    biases['model 2'][layer_rank], x_valid, deep_layer_inputs, type='preactivation', layer_rank=layer_rank)[0]\n",
    "\n",
    "                likelyhood[layer_rank]['hidden'], layer_1, layer_2 = layers_covariances(\n",
    "                    weights['model 1'][layer_rank], biases['model 1'][layer_rank], weights['model 2'][layer_rank],\n",
    "                    biases['model 2'][layer_rank], x_valid, deep_layer_inputs, type='hidden', layer_rank=layer_rank)\n",
    "            \n",
    "        save_interval_size = likelyhood[layer_rank]['preactivation'].shape[0]\n",
    "        # Register overlap information\n",
    "        for t in range (save_interval_size): \n",
    "            for layer_rank in overlap_dico['likelyhood_trajectory']['data'] :\n",
    "                for type_of_layer in overlap_dico['likelyhood_trajectory']['data'][layer_rank] :\n",
    "                    # First layer preactivation \n",
    "                    overlap_dico['likelyhood_trajectory']['data'][layer_rank][type_of_layer].append(likelyhood[layer_rank][type_of_layer][t].cpu())\n",
    "                    overlap_dico['likelyhood_variance_trajectory']['data'][layer_rank][type_of_layer].append((torch.sqrt((likelyhood[layer_rank][type_of_layer][t] - (likelyhood[layer_rank][type_of_layer][:].mean()))**2).mean()).cpu())\n",
    "    \n",
    "    # Plot datas\n",
    "    T = np.linspace(0, (min_time*save_interval_size+1)*10, min_time*save_interval_size+1)\n",
    "    for curve in overlap_dico :\n",
    "        legend_elements = []\n",
    "        for layer_rank in overlap_dico[curve]['data']:\n",
    "            for type_of_layer in overlap_dico[curve]['data'][layer_rank]:\n",
    "                plt.plot(T, overlap_dico[curve]['data'][layer_rank][type_of_layer], color=color_list[layer_rank][type_of_layer])\n",
    "                legend_elements += [Line2D([0], [0], color=color_list[layer_rank][type_of_layer], linewidth=5, label = label_list[layer_rank][type_of_layer])]                    \n",
    "        legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        for handle in legend.legend_handles:\n",
    "            handle.set_markersize(15)\n",
    "        plt.title(overlap_dico[curve]['plot']['title'], pad=20)\n",
    "        plt.xlabel(overlap_dico[curve]['plot']['xlabel'])\n",
    "        plt.ylabel(overlap_dico[curve]['plot']['ylabel'])\n",
    "        # if curve == 'likelyhood_trajectory':\n",
    "        #     plt.ylim([-0.05,0.5])\n",
    "        plt.grid()\n",
    "        os.makedirs(save_path + '/' + architecture, exist_ok=True) \n",
    "        plt.savefig(save_path + '/' + architecture + '/' + curve + '.png', bbox_inches='tight')\n",
    "        plt.savefig(save_path + '/' + architecture + '/' + curve + '.svg', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    return overlap_dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
