{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from utils import mnist_reader\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 18\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033bbdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "x_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "x_valid, y_valid = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "print(max(y_train), min(y_train))\n",
    "\n",
    "x_train, y_train_raw, x_valid, y_valid_raw = torch.tensor(x_train, dtype=dtype), torch.tensor(y_train, dtype=dtype), torch.tensor(x_valid, dtype=dtype), torch.tensor(y_valid,dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a2ce08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[43my_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[31mValueError\u001b[39m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "print(min(y_train.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "torch.Size([60000, 10]) torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "y_train = torch.zeros(y_train_raw.shape[0], 10)\n",
    "for i,y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros(y_valid_raw.shape[0], 10)\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "    \n",
    "print(y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb401276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 10]) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([10000, 10]) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_train[0:5])\n",
    "print(y_valid.shape, y_valid[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_sample = X_train[0].reshape(28,28)\n",
    "print(X_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672602f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_sample, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "45060724",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).to(dtype)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, coef_iter = 1, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - coef_iter = \" + str(coef_iter) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        kappa_max = 1 + coef_iter\n",
    "        max_iter = self.input_dimension**(kappa_max)\n",
    "        print(\"max_iter\", max_iter)\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_minibatches = int(max_iter / self.fraction_batch) # Nombre de minibatches utilisés pour l'apprentissage de la première couche\n",
    "        for i in range(N_minibatches):\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            if i % self.observation_rate == 0:    \n",
    "                # Suivi de l'apprentissage\n",
    "                unwanted_time_begin = time.time()\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                unwanted_time += time.time() - unwanted_time_begin\n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0] # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = torch.mean(grad_z1, dim=0).unsqueeze(1) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0] # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = torch.mean(grad_z2, dim=0).unsqueeze(1)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)\n",
    "                self.b2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)\n",
    "            self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, coef_iter = 1, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - coef_iter = \" + str(coef_iter) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        kappa_max = 1 + coef_iter\n",
    "        max_iter = self.input_dimension**(kappa_max)\n",
    "        print(\"max_iter\", max_iter)\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_minibatches = int(max_iter / self.fraction_batch) # Nombre de minibatches utilisés pour l'apprentissage\n",
    "        for i in range(N_minibatches):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            if i % self.observation_rate == 0:\n",
    "                unwanted_time_begin = time.time() \n",
    "            # Suivi de l'apprentissage\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss pour chaque classe\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                unwanted_time += time.time() - unwanted_time_begin\n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch\n",
    "            grad_z3 = torch.einsum('no,noz->nz',grad_output,softmax_derivative(output)) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = torch.mm(grad_z3, self.W3) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = grad_h2*ReLU_derivative(z2) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = torch.mm(grad_z2, self.W2)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0] # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = torch.mean(grad_z1, dim=0).unsqueeze(1) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0] # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = torch.mean(grad_z2, dim=0).unsqueeze(1)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = torch.mean(grad_z3,dim=0).unsqueeze(1)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)\n",
    "                self.b2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)\n",
    "                self.b3 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)\n",
    "            self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(784,512,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter 614656\n",
      "Iteration 0 Training loss 0.09224962443113327 Validation loss 0.09098991006612778 Accuracy 0.07339999824762344\n",
      "Iteration 10 Training loss 0.08876975625753403 Validation loss 0.08949742466211319 Accuracy 0.09619999676942825\n",
      "Iteration 20 Training loss 0.08129862695932388 Validation loss 0.08108747750520706 Accuracy 0.1817999929189682\n",
      "Iteration 30 Training loss 0.07469066977500916 Validation loss 0.07456433773040771 Accuracy 0.24959999322891235\n",
      "Iteration 40 Training loss 0.07427629083395004 Validation loss 0.07315141707658768 Accuracy 0.26499998569488525\n",
      "Iteration 50 Training loss 0.07086373120546341 Validation loss 0.07350478321313858 Accuracy 0.2623000144958496\n",
      "Iteration 60 Training loss 0.07267791777849197 Validation loss 0.07260712236166 Accuracy 0.2720000147819519\n",
      "Iteration 70 Training loss 0.07455632835626602 Validation loss 0.07307075709104538 Accuracy 0.266400009393692\n",
      "Iteration 80 Training loss 0.073715440928936 Validation loss 0.07260952889919281 Accuracy 0.27239999175071716\n",
      "Iteration 90 Training loss 0.0774199366569519 Validation loss 0.07284049689769745 Accuracy 0.2703000009059906\n",
      "Iteration 100 Training loss 0.07219769805669785 Validation loss 0.07254966348409653 Accuracy 0.2727000117301941\n",
      "Iteration 110 Training loss 0.07404065877199173 Validation loss 0.07253625243902206 Accuracy 0.2728999853134155\n",
      "Iteration 120 Training loss 0.06933597475290298 Validation loss 0.07243061810731888 Accuracy 0.274399995803833\n",
      "Iteration 130 Training loss 0.07353199273347855 Validation loss 0.07300756126642227 Accuracy 0.2680000066757202\n",
      "Iteration 140 Training loss 0.07384146749973297 Validation loss 0.07227960228919983 Accuracy 0.2759000062942505\n",
      "Iteration 150 Training loss 0.07258642464876175 Validation loss 0.0728001743555069 Accuracy 0.2705000042915344\n",
      "Iteration 160 Training loss 0.07041803002357483 Validation loss 0.07216153293848038 Accuracy 0.2770000100135803\n",
      "Iteration 170 Training loss 0.07062386721372604 Validation loss 0.07202520221471786 Accuracy 0.27869999408721924\n",
      "Iteration 180 Training loss 0.07108646631240845 Validation loss 0.07230177521705627 Accuracy 0.2759999930858612\n",
      "Iteration 190 Training loss 0.07357235997915268 Validation loss 0.07213575392961502 Accuracy 0.2775000035762787\n",
      "Iteration 200 Training loss 0.07124751806259155 Validation loss 0.07219487428665161 Accuracy 0.27649998664855957\n",
      "Iteration 210 Training loss 0.07143000513315201 Validation loss 0.07296225428581238 Accuracy 0.2689000070095062\n",
      "Iteration 220 Training loss 0.07049095630645752 Validation loss 0.07209544628858566 Accuracy 0.27799999713897705\n",
      "Iteration 230 Training loss 0.07044649869203568 Validation loss 0.0736614391207695 Accuracy 0.26089999079704285\n",
      "Iteration 240 Training loss 0.06985172629356384 Validation loss 0.07226517051458359 Accuracy 0.2757999897003174\n",
      "Iteration 250 Training loss 0.07117151468992233 Validation loss 0.07299696654081345 Accuracy 0.26759999990463257\n",
      "Iteration 260 Training loss 0.07467886805534363 Validation loss 0.07309053838253021 Accuracy 0.2662999927997589\n",
      "Iteration 270 Training loss 0.07073646783828735 Validation loss 0.07221941649913788 Accuracy 0.27649998664855957\n",
      "Iteration 280 Training loss 0.07412325590848923 Validation loss 0.0719628781080246 Accuracy 0.2793999910354614\n",
      "Iteration 290 Training loss 0.07291784882545471 Validation loss 0.07184658199548721 Accuracy 0.28040000796318054\n",
      "Iteration 300 Training loss 0.07097338885068893 Validation loss 0.07270196080207825 Accuracy 0.2718999981880188\n",
      "Iteration 310 Training loss 0.07372745871543884 Validation loss 0.0722469836473465 Accuracy 0.27630001306533813\n",
      "Iteration 320 Training loss 0.07290977239608765 Validation loss 0.07193920761346817 Accuracy 0.27970001101493835\n",
      "Iteration 330 Training loss 0.07279349118471146 Validation loss 0.07257208973169327 Accuracy 0.2727999985218048\n",
      "Iteration 340 Training loss 0.07191959023475647 Validation loss 0.07189232110977173 Accuracy 0.27970001101493835\n",
      "Iteration 350 Training loss 0.07284163683652878 Validation loss 0.07183491438627243 Accuracy 0.2802000045776367\n",
      "Iteration 360 Training loss 0.07097510993480682 Validation loss 0.07167959213256836 Accuracy 0.28200000524520874\n",
      "Iteration 370 Training loss 0.06850511580705643 Validation loss 0.06820265203714371 Accuracy 0.3158000111579895\n",
      "Iteration 380 Training loss 0.07294880598783493 Validation loss 0.07159426063299179 Accuracy 0.28119999170303345\n",
      "Iteration 390 Training loss 0.06519082933664322 Validation loss 0.06523838639259338 Accuracy 0.34360000491142273\n",
      "Iteration 400 Training loss 0.06306502968072891 Validation loss 0.06198788434267044 Accuracy 0.37610000371932983\n",
      "Iteration 410 Training loss 0.06275192648172379 Validation loss 0.06308115273714066 Accuracy 0.36660000681877136\n",
      "Iteration 420 Training loss 0.06019488349556923 Validation loss 0.06073490157723427 Accuracy 0.3889999985694885\n",
      "Iteration 430 Training loss 0.055968668311834335 Validation loss 0.058324072510004044 Accuracy 0.4147000014781952\n",
      "Iteration 440 Training loss 0.05521880090236664 Validation loss 0.05829954892396927 Accuracy 0.4147000014781952\n",
      "Iteration 450 Training loss 0.05716521292924881 Validation loss 0.057974863797426224 Accuracy 0.4187999963760376\n",
      "Iteration 460 Training loss 0.06096462532877922 Validation loss 0.059397101402282715 Accuracy 0.40380001068115234\n",
      "Iteration 470 Training loss 0.06014006957411766 Validation loss 0.05860813334584236 Accuracy 0.4115000069141388\n",
      "Iteration 480 Training loss 0.05843230336904526 Validation loss 0.06246129795908928 Accuracy 0.3736000061035156\n",
      "Iteration 490 Training loss 0.05724162980914116 Validation loss 0.06014053896069527 Accuracy 0.3970000147819519\n",
      "Iteration 500 Training loss 0.056715670973062515 Validation loss 0.057730115950107574 Accuracy 0.42100000381469727\n",
      "Iteration 510 Training loss 0.062282465398311615 Validation loss 0.058959804475307465 Accuracy 0.4090999960899353\n",
      "Iteration 520 Training loss 0.059513285756111145 Validation loss 0.057478416711091995 Accuracy 0.42320001125335693\n",
      "Iteration 530 Training loss 0.05569993704557419 Validation loss 0.05888349190354347 Accuracy 0.4090999960899353\n",
      "Iteration 540 Training loss 0.05483841523528099 Validation loss 0.05814056470990181 Accuracy 0.41659998893737793\n",
      "Iteration 550 Training loss 0.05562034249305725 Validation loss 0.057804498821496964 Accuracy 0.42080000042915344\n",
      "Iteration 560 Training loss 0.05849313363432884 Validation loss 0.05762960761785507 Accuracy 0.42149999737739563\n",
      "Iteration 570 Training loss 0.05575190857052803 Validation loss 0.05793650075793266 Accuracy 0.4187999963760376\n",
      "Iteration 580 Training loss 0.05361737683415413 Validation loss 0.057148370891809464 Accuracy 0.4269999861717224\n",
      "Iteration 590 Training loss 0.05967891588807106 Validation loss 0.0575140118598938 Accuracy 0.42309999465942383\n",
      "Iteration 600 Training loss 0.05923962965607643 Validation loss 0.062060024589300156 Accuracy 0.3781999945640564\n",
      "Iteration 610 Training loss 0.05614151060581207 Validation loss 0.05682501941919327 Accuracy 0.43070000410079956\n",
      "Iteration 620 Training loss 0.06071143597364426 Validation loss 0.0573091059923172 Accuracy 0.42559999227523804\n",
      "Iteration 630 Training loss 0.05925115942955017 Validation loss 0.05884988233447075 Accuracy 0.4101000130176544\n",
      "Iteration 640 Training loss 0.05772992596030235 Validation loss 0.058865927159786224 Accuracy 0.4092000126838684\n",
      "Iteration 650 Training loss 0.0557333305478096 Validation loss 0.05692620202898979 Accuracy 0.42879998683929443\n",
      "Iteration 660 Training loss 0.05890538915991783 Validation loss 0.0581626333296299 Accuracy 0.4165000021457672\n",
      "Iteration 670 Training loss 0.06132495030760765 Validation loss 0.05766114220023155 Accuracy 0.42160001397132874\n",
      "Iteration 680 Training loss 0.05774731934070587 Validation loss 0.05828630551695824 Accuracy 0.41530001163482666\n",
      "Iteration 690 Training loss 0.05618999898433685 Validation loss 0.056602127850055695 Accuracy 0.4325000047683716\n",
      "Iteration 700 Training loss 0.056574586778879166 Validation loss 0.056675270199775696 Accuracy 0.43140000104904175\n",
      "Iteration 710 Training loss 0.059713881462812424 Validation loss 0.058979474008083344 Accuracy 0.40849998593330383\n",
      "Iteration 720 Training loss 0.05662564933300018 Validation loss 0.05621689558029175 Accuracy 0.4366999864578247\n",
      "Iteration 730 Training loss 0.055432964116334915 Validation loss 0.056198637932538986 Accuracy 0.43630000948905945\n",
      "Iteration 740 Training loss 0.0606120266020298 Validation loss 0.056636616587638855 Accuracy 0.4318999946117401\n",
      "Iteration 750 Training loss 0.057959239929914474 Validation loss 0.058830250054597855 Accuracy 0.40959998965263367\n",
      "Iteration 760 Training loss 0.05547444522380829 Validation loss 0.05634686350822449 Accuracy 0.43470001220703125\n",
      "Iteration 770 Training loss 0.054733652621507645 Validation loss 0.057384129613637924 Accuracy 0.4242999851703644\n",
      "Iteration 780 Training loss 0.05727967992424965 Validation loss 0.05720175802707672 Accuracy 0.42660000920295715\n",
      "Iteration 790 Training loss 0.057730309665203094 Validation loss 0.05623175948858261 Accuracy 0.4357999861240387\n",
      "Iteration 800 Training loss 0.06251940131187439 Validation loss 0.06211321800947189 Accuracy 0.3765000104904175\n",
      "Iteration 810 Training loss 0.05458357185125351 Validation loss 0.05616070330142975 Accuracy 0.43639999628067017\n",
      "Iteration 820 Training loss 0.05450719594955444 Validation loss 0.05692274868488312 Accuracy 0.4291999936103821\n",
      "Iteration 830 Training loss 0.05632040277123451 Validation loss 0.056846000254154205 Accuracy 0.43050000071525574\n",
      "Iteration 840 Training loss 0.06128237396478653 Validation loss 0.05684565380215645 Accuracy 0.42989999055862427\n",
      "Iteration 850 Training loss 0.055838119238615036 Validation loss 0.056316643953323364 Accuracy 0.4348999857902527\n",
      "Iteration 860 Training loss 0.05675791576504707 Validation loss 0.05725850537419319 Accuracy 0.4255000054836273\n",
      "Iteration 870 Training loss 0.056231144815683365 Validation loss 0.0563211515545845 Accuracy 0.4348999857902527\n",
      "Iteration 880 Training loss 0.05660485848784447 Validation loss 0.0570404976606369 Accuracy 0.427700012922287\n",
      "Iteration 890 Training loss 0.05751034989953041 Validation loss 0.05741645023226738 Accuracy 0.4242999851703644\n",
      "Iteration 900 Training loss 0.05604582279920578 Validation loss 0.05650605633854866 Accuracy 0.4334999918937683\n",
      "Iteration 910 Training loss 0.05580975487828255 Validation loss 0.0567411407828331 Accuracy 0.4309999942779541\n",
      "Iteration 920 Training loss 0.05578947439789772 Validation loss 0.05611115321516991 Accuracy 0.4375\n",
      "Iteration 930 Training loss 0.059607986360788345 Validation loss 0.05604880675673485 Accuracy 0.43860000371932983\n",
      "Iteration 940 Training loss 0.06089162826538086 Validation loss 0.06069532409310341 Accuracy 0.39169999957084656\n",
      "Iteration 950 Training loss 0.05589984729886055 Validation loss 0.05680932477116585 Accuracy 0.4309999942779541\n",
      "Iteration 960 Training loss 0.05588078871369362 Validation loss 0.05626717954874039 Accuracy 0.4359999895095825\n",
      "Iteration 970 Training loss 0.055039092898368835 Validation loss 0.056802231818437576 Accuracy 0.4309000074863434\n",
      "Iteration 980 Training loss 0.05694751814007759 Validation loss 0.05588157847523689 Accuracy 0.4401000142097473\n",
      "Iteration 990 Training loss 0.056546248495578766 Validation loss 0.05585407093167305 Accuracy 0.44020000100135803\n",
      "Iteration 1000 Training loss 0.05796097218990326 Validation loss 0.05624696612358093 Accuracy 0.43630000948905945\n",
      "Iteration 1010 Training loss 0.055288080126047134 Validation loss 0.05587849020957947 Accuracy 0.44029998779296875\n",
      "Iteration 1020 Training loss 0.051933206617832184 Validation loss 0.055817510932683945 Accuracy 0.4410000145435333\n",
      "Iteration 1030 Training loss 0.05646447092294693 Validation loss 0.05902596190571785 Accuracy 0.40860000252723694\n",
      "Iteration 1040 Training loss 0.0568639412522316 Validation loss 0.055720508098602295 Accuracy 0.4415000081062317\n",
      "Iteration 1050 Training loss 0.05526009202003479 Validation loss 0.05622844770550728 Accuracy 0.436599999666214\n",
      "Iteration 1060 Training loss 0.05719287693500519 Validation loss 0.05584912747144699 Accuracy 0.44029998779296875\n",
      "Iteration 1070 Training loss 0.05542313680052757 Validation loss 0.056086741387844086 Accuracy 0.43810001015663147\n",
      "Iteration 1080 Training loss 0.054160624742507935 Validation loss 0.05599792301654816 Accuracy 0.4390000104904175\n",
      "Iteration 1090 Training loss 0.05512358248233795 Validation loss 0.05619601160287857 Accuracy 0.43689998984336853\n",
      "Iteration 1100 Training loss 0.05726132169365883 Validation loss 0.05587511882185936 Accuracy 0.4401000142097473\n",
      "Iteration 1110 Training loss 0.05490046367049217 Validation loss 0.05677826330065727 Accuracy 0.4311999976634979\n",
      "Iteration 1120 Training loss 0.05296887829899788 Validation loss 0.05611763149499893 Accuracy 0.4377000033855438\n",
      "Iteration 1130 Training loss 0.05565476417541504 Validation loss 0.05574774369597435 Accuracy 0.4410000145435333\n",
      "Iteration 1140 Training loss 0.053938716650009155 Validation loss 0.0559820793569088 Accuracy 0.439300000667572\n",
      "Iteration 1150 Training loss 0.06255949288606644 Validation loss 0.058601636439561844 Accuracy 0.4124999940395355\n",
      "Iteration 1160 Training loss 0.05228586867451668 Validation loss 0.05576707050204277 Accuracy 0.44119998812675476\n",
      "Iteration 1170 Training loss 0.05458676069974899 Validation loss 0.05600321665406227 Accuracy 0.4392000138759613\n",
      "Iteration 1180 Training loss 0.05973014235496521 Validation loss 0.05939288064837456 Accuracy 0.4047999978065491\n",
      "Iteration 1190 Training loss 0.05557030066847801 Validation loss 0.05584011226892471 Accuracy 0.4406000077724457\n",
      "Iteration 1200 Training loss 0.055016178637742996 Validation loss 0.05619984492659569 Accuracy 0.4366999864578247\n",
      "Iteration 1210 Training loss 0.054462332278490067 Validation loss 0.056434061378240585 Accuracy 0.4341999888420105\n",
      "Iteration 1220 Training loss 0.0571453757584095 Validation loss 0.05656808614730835 Accuracy 0.4332999885082245\n",
      "Iteration 1230 Training loss 0.057970575988292694 Validation loss 0.05610348656773567 Accuracy 0.43810001015663147\n",
      "Iteration 1240 Training loss 0.05592862516641617 Validation loss 0.057102445513010025 Accuracy 0.4278999865055084\n",
      "Iteration 1250 Training loss 0.05759017914533615 Validation loss 0.057240672409534454 Accuracy 0.4268999993801117\n",
      "Iteration 1260 Training loss 0.05362886190414429 Validation loss 0.055904511362314224 Accuracy 0.4399999976158142\n",
      "Iteration 1270 Training loss 0.05598815530538559 Validation loss 0.05625567212700844 Accuracy 0.43650001287460327\n",
      "Iteration 1280 Training loss 0.05604925379157066 Validation loss 0.05668109282851219 Accuracy 0.4318000078201294\n",
      "Iteration 1290 Training loss 0.053792573511600494 Validation loss 0.056045643985271454 Accuracy 0.43869999051094055\n",
      "Iteration 1300 Training loss 0.056781087070703506 Validation loss 0.05646422877907753 Accuracy 0.4341000020503998\n",
      "Iteration 1310 Training loss 0.05617162212729454 Validation loss 0.05628953501582146 Accuracy 0.4357999861240387\n",
      "Iteration 1320 Training loss 0.058712106198072433 Validation loss 0.05623204633593559 Accuracy 0.4359999895095825\n",
      "Iteration 1330 Training loss 0.05638403445482254 Validation loss 0.05612160637974739 Accuracy 0.43799999356269836\n",
      "Iteration 1340 Training loss 0.05791236460208893 Validation loss 0.05985334888100624 Accuracy 0.400299996137619\n",
      "Iteration 1350 Training loss 0.059516001492738724 Validation loss 0.056077245622873306 Accuracy 0.4381999969482422\n",
      "Iteration 1360 Training loss 0.057820506393909454 Validation loss 0.05614031106233597 Accuracy 0.4375\n",
      "Iteration 1370 Training loss 0.05694111809134483 Validation loss 0.05679327994585037 Accuracy 0.4311999976634979\n",
      "Iteration 1380 Training loss 0.05759299173951149 Validation loss 0.05581768974661827 Accuracy 0.4406999945640564\n",
      "Iteration 1390 Training loss 0.053737323731184006 Validation loss 0.05638211593031883 Accuracy 0.43529999256134033\n",
      "Iteration 1400 Training loss 0.053779397159814835 Validation loss 0.056005001068115234 Accuracy 0.4390999972820282\n",
      "Iteration 1410 Training loss 0.057057902216911316 Validation loss 0.05607489123940468 Accuracy 0.43849998712539673\n",
      "Iteration 1420 Training loss 0.05808742344379425 Validation loss 0.0559997521340847 Accuracy 0.43880000710487366\n",
      "Iteration 1430 Training loss 0.05498504638671875 Validation loss 0.05589596554636955 Accuracy 0.4401000142097473\n",
      "Iteration 1440 Training loss 0.05421034246683121 Validation loss 0.0560079962015152 Accuracy 0.4388999938964844\n",
      "Iteration 1450 Training loss 0.05892590433359146 Validation loss 0.05603925883769989 Accuracy 0.4383000135421753\n",
      "Iteration 1460 Training loss 0.0552910752594471 Validation loss 0.05566108971834183 Accuracy 0.44190001487731934\n",
      "Iteration 1470 Training loss 0.05685761570930481 Validation loss 0.05569174885749817 Accuracy 0.4417000114917755\n",
      "Iteration 1480 Training loss 0.05425534024834633 Validation loss 0.05624827742576599 Accuracy 0.4368000030517578\n",
      "Iteration 1490 Training loss 0.05762790888547897 Validation loss 0.055681757628917694 Accuracy 0.4422999918460846\n",
      "Iteration 1500 Training loss 0.05609985440969467 Validation loss 0.055727455765008926 Accuracy 0.44179999828338623\n",
      "Iteration 1510 Training loss 0.054601073265075684 Validation loss 0.055923253297805786 Accuracy 0.4392000138759613\n",
      "Iteration 1520 Training loss 0.0546504482626915 Validation loss 0.0555935874581337 Accuracy 0.4429999887943268\n",
      "Iteration 1530 Training loss 0.05357757955789566 Validation loss 0.05564792826771736 Accuracy 0.4417000114917755\n",
      "Iteration 1540 Training loss 0.05570327863097191 Validation loss 0.055839259177446365 Accuracy 0.4399999976158142\n",
      "Iteration 1550 Training loss 0.06049680709838867 Validation loss 0.05835690349340439 Accuracy 0.414900004863739\n",
      "Iteration 1560 Training loss 0.05660001561045647 Validation loss 0.05699172243475914 Accuracy 0.42829999327659607\n",
      "Iteration 1570 Training loss 0.055115457624197006 Validation loss 0.055499929934740067 Accuracy 0.4440000057220459\n",
      "Iteration 1580 Training loss 0.05399172380566597 Validation loss 0.05550238862633705 Accuracy 0.44350001215934753\n",
      "Iteration 1590 Training loss 0.059010740369558334 Validation loss 0.055857256054878235 Accuracy 0.4399000108242035\n",
      "Iteration 1600 Training loss 0.05664233863353729 Validation loss 0.056178320199251175 Accuracy 0.43720000982284546\n",
      "Iteration 1610 Training loss 0.05439624935388565 Validation loss 0.057429615408182144 Accuracy 0.42419999837875366\n",
      "Iteration 1620 Training loss 0.05180555582046509 Validation loss 0.055906616151332855 Accuracy 0.439300000667572\n",
      "Iteration 1630 Training loss 0.055733658373355865 Validation loss 0.056081373244524 Accuracy 0.4383000135421753\n",
      "Iteration 1640 Training loss 0.0559554249048233 Validation loss 0.055813904851675034 Accuracy 0.4406000077724457\n",
      "Iteration 1650 Training loss 0.0547541007399559 Validation loss 0.055688973516225815 Accuracy 0.44209998846054077\n",
      "Iteration 1660 Training loss 0.055571943521499634 Validation loss 0.05560612305998802 Accuracy 0.44269999861717224\n",
      "Iteration 1670 Training loss 0.05359215289354324 Validation loss 0.056014593690633774 Accuracy 0.4388999938964844\n",
      "Iteration 1680 Training loss 0.05801718309521675 Validation loss 0.057236913591623306 Accuracy 0.4262000024318695\n",
      "Iteration 1690 Training loss 0.05652579292654991 Validation loss 0.05616787075996399 Accuracy 0.43709999322891235\n",
      "Iteration 1700 Training loss 0.056000467389822006 Validation loss 0.05647962540388107 Accuracy 0.43369999527931213\n",
      "Iteration 1710 Training loss 0.05630270019173622 Validation loss 0.055696044117212296 Accuracy 0.4417000114917755\n",
      "Iteration 1720 Training loss 0.05758758634328842 Validation loss 0.055723730474710464 Accuracy 0.4415000081062317\n",
      "Iteration 1730 Training loss 0.05650542303919792 Validation loss 0.05599537119269371 Accuracy 0.43849998712539673\n",
      "Iteration 1740 Training loss 0.05316184088587761 Validation loss 0.05564991384744644 Accuracy 0.4422000050544739\n",
      "Iteration 1750 Training loss 0.056800566613674164 Validation loss 0.05594240874052048 Accuracy 0.439300000667572\n",
      "Iteration 1760 Training loss 0.054419200867414474 Validation loss 0.0568808987736702 Accuracy 0.42989999055862427\n",
      "Iteration 1770 Training loss 0.05960095673799515 Validation loss 0.058035679161548615 Accuracy 0.4185999929904938\n",
      "Iteration 1780 Training loss 0.05078800022602081 Validation loss 0.055495455861091614 Accuracy 0.44339999556541443\n",
      "Iteration 1790 Training loss 0.05756134167313576 Validation loss 0.05610783025622368 Accuracy 0.4375999867916107\n",
      "Iteration 1800 Training loss 0.055623650550842285 Validation loss 0.05560389161109924 Accuracy 0.44269999861717224\n",
      "Iteration 1810 Training loss 0.05666313320398331 Validation loss 0.055541615933179855 Accuracy 0.4429999887943268\n",
      "Iteration 1820 Training loss 0.05429428070783615 Validation loss 0.05564209818840027 Accuracy 0.4422999918460846\n",
      "Iteration 1830 Training loss 0.05607706680893898 Validation loss 0.05564885586500168 Accuracy 0.4422999918460846\n",
      "Iteration 1840 Training loss 0.05408233031630516 Validation loss 0.05531596019864082 Accuracy 0.445499986410141\n",
      "Iteration 1850 Training loss 0.05684521049261093 Validation loss 0.05535084381699562 Accuracy 0.4447000026702881\n",
      "Iteration 1860 Training loss 0.05931464210152626 Validation loss 0.056369490921497345 Accuracy 0.4348999857902527\n",
      "Iteration 1870 Training loss 0.05407845228910446 Validation loss 0.05684834346175194 Accuracy 0.4300999939441681\n",
      "Iteration 1880 Training loss 0.05625411123037338 Validation loss 0.05699032172560692 Accuracy 0.42910000681877136\n",
      "Iteration 1890 Training loss 0.05357200652360916 Validation loss 0.05545258894562721 Accuracy 0.4438000023365021\n",
      "Iteration 1900 Training loss 0.05755782499909401 Validation loss 0.054921310395002365 Accuracy 0.4490000009536743\n",
      "Iteration 1910 Training loss 0.05524164438247681 Validation loss 0.05520613119006157 Accuracy 0.4462999999523163\n",
      "Iteration 1920 Training loss 0.047351643443107605 Validation loss 0.05262527987360954 Accuracy 0.4713999927043915\n",
      "Iteration 1930 Training loss 0.04614982008934021 Validation loss 0.05027395486831665 Accuracy 0.4957999885082245\n",
      "Iteration 1940 Training loss 0.050135497003793716 Validation loss 0.04922619089484215 Accuracy 0.5060999989509583\n",
      "Iteration 1950 Training loss 0.04861120507121086 Validation loss 0.04880056530237198 Accuracy 0.5103999972343445\n",
      "Iteration 1960 Training loss 0.05015047639608383 Validation loss 0.04914296418428421 Accuracy 0.5069000124931335\n",
      "Iteration 1970 Training loss 0.04978775978088379 Validation loss 0.04935889691114426 Accuracy 0.5052000284194946\n",
      "Iteration 1980 Training loss 0.04813273996114731 Validation loss 0.04855748638510704 Accuracy 0.5131000280380249\n",
      "Iteration 1990 Training loss 0.04693584144115448 Validation loss 0.04829216003417969 Accuracy 0.5156999826431274\n",
      "Iteration 2000 Training loss 0.0471721887588501 Validation loss 0.049061380326747894 Accuracy 0.5081999897956848\n",
      "Iteration 2010 Training loss 0.05076234042644501 Validation loss 0.04881753399968147 Accuracy 0.5101000070571899\n",
      "Iteration 2020 Training loss 0.04849080741405487 Validation loss 0.049077294766902924 Accuracy 0.5078999996185303\n",
      "Iteration 2030 Training loss 0.04784974828362465 Validation loss 0.04917645454406738 Accuracy 0.5067999958992004\n",
      "Iteration 2040 Training loss 0.047942694276571274 Validation loss 0.048528894782066345 Accuracy 0.5131999850273132\n",
      "Iteration 2050 Training loss 0.04876837506890297 Validation loss 0.04841794818639755 Accuracy 0.5145000219345093\n",
      "Iteration 2060 Training loss 0.0461178794503212 Validation loss 0.04878241568803787 Accuracy 0.5109999775886536\n",
      "Iteration 2070 Training loss 0.04773446545004845 Validation loss 0.04804159328341484 Accuracy 0.5184999704360962\n",
      "Iteration 2080 Training loss 0.04929525777697563 Validation loss 0.04906252771615982 Accuracy 0.5076000094413757\n",
      "Iteration 2090 Training loss 0.045269545167684555 Validation loss 0.04937087744474411 Accuracy 0.504800021648407\n",
      "Iteration 2100 Training loss 0.04742353409528732 Validation loss 0.048106949776411057 Accuracy 0.5175999999046326\n",
      "Iteration 2110 Training loss 0.04773867875337601 Validation loss 0.04988083615899086 Accuracy 0.5001999735832214\n",
      "Iteration 2120 Training loss 0.04261895641684532 Validation loss 0.048115555197000504 Accuracy 0.517799973487854\n",
      "Iteration 2130 Training loss 0.04484105482697487 Validation loss 0.05007493495941162 Accuracy 0.49779999256134033\n",
      "Iteration 2140 Training loss 0.04979008063673973 Validation loss 0.04836796224117279 Accuracy 0.5149999856948853\n",
      "Iteration 2150 Training loss 0.04660030081868172 Validation loss 0.0483953058719635 Accuracy 0.5149000287055969\n",
      "Iteration 2160 Training loss 0.04656432196497917 Validation loss 0.04879878833889961 Accuracy 0.510200023651123\n",
      "Iteration 2170 Training loss 0.04668820649385452 Validation loss 0.04876549914479256 Accuracy 0.5108000040054321\n",
      "Iteration 2180 Training loss 0.048604656010866165 Validation loss 0.048686251044273376 Accuracy 0.5113999843597412\n",
      "Iteration 2190 Training loss 0.04982633516192436 Validation loss 0.04952193796634674 Accuracy 0.5037999749183655\n",
      "Iteration 2200 Training loss 0.04368908330798149 Validation loss 0.04798032343387604 Accuracy 0.51910001039505\n",
      "Iteration 2210 Training loss 0.05319579690694809 Validation loss 0.05002600699663162 Accuracy 0.4984000027179718\n",
      "Iteration 2220 Training loss 0.04848654568195343 Validation loss 0.04857344552874565 Accuracy 0.5131999850273132\n",
      "Iteration 2230 Training loss 0.04779127985239029 Validation loss 0.04977220296859741 Accuracy 0.5008000135421753\n",
      "Iteration 2240 Training loss 0.05029286816716194 Validation loss 0.04874978959560394 Accuracy 0.5117999911308289\n",
      "Iteration 2250 Training loss 0.05300707370042801 Validation loss 0.048636771738529205 Accuracy 0.512499988079071\n",
      "Iteration 2260 Training loss 0.04661240801215172 Validation loss 0.0479513555765152 Accuracy 0.5192000269889832\n",
      "Iteration 2270 Training loss 0.04775421693921089 Validation loss 0.047584496438503265 Accuracy 0.5231999754905701\n",
      "Iteration 2280 Training loss 0.04968525841832161 Validation loss 0.04760899022221565 Accuracy 0.5227000117301941\n",
      "Iteration 2290 Training loss 0.04781242832541466 Validation loss 0.04787778481841087 Accuracy 0.5200999975204468\n",
      "Iteration 2300 Training loss 0.04870077595114708 Validation loss 0.05111166462302208 Accuracy 0.4876999855041504\n",
      "Iteration 2310 Training loss 0.05132119730114937 Validation loss 0.049154769629240036 Accuracy 0.5074999928474426\n",
      "Iteration 2320 Training loss 0.04816124588251114 Validation loss 0.048104774206876755 Accuracy 0.517799973487854\n",
      "Iteration 2330 Training loss 0.04860932379961014 Validation loss 0.04986005276441574 Accuracy 0.4997999966144562\n",
      "Iteration 2340 Training loss 0.046131059527397156 Validation loss 0.04838458076119423 Accuracy 0.515500009059906\n",
      "Iteration 2350 Training loss 0.048204731196165085 Validation loss 0.04773952066898346 Accuracy 0.5214999914169312\n",
      "Iteration 2360 Training loss 0.05004927143454552 Validation loss 0.04821370914578438 Accuracy 0.5163999795913696\n",
      "Iteration 2370 Training loss 0.05096740648150444 Validation loss 0.049250707030296326 Accuracy 0.5059999823570251\n",
      "Iteration 2380 Training loss 0.047633733600378036 Validation loss 0.04817017912864685 Accuracy 0.5170000195503235\n",
      "Iteration 2390 Training loss 0.04790313169360161 Validation loss 0.04772287607192993 Accuracy 0.5214999914169312\n",
      "Iteration 2400 Training loss 0.04507496580481529 Validation loss 0.04758710041642189 Accuracy 0.5231000185012817\n",
      "Iteration 2410 Training loss 0.045111775398254395 Validation loss 0.047766659408807755 Accuracy 0.520799994468689\n",
      "Iteration 2420 Training loss 0.0443945974111557 Validation loss 0.04787646606564522 Accuracy 0.5196999907493591\n",
      "Iteration 2430 Training loss 0.04712006449699402 Validation loss 0.047811489552259445 Accuracy 0.520799994468689\n",
      "Iteration 2440 Training loss 0.04704167693853378 Validation loss 0.04901086539030075 Accuracy 0.5080000162124634\n",
      "Iteration 2450 Training loss 0.04888419434428215 Validation loss 0.04830281808972359 Accuracy 0.5156999826431274\n",
      "Iteration 2460 Training loss 0.048041339963674545 Validation loss 0.04854155331850052 Accuracy 0.5130000114440918\n",
      "Iteration 2470 Training loss 0.04647856578230858 Validation loss 0.04771747067570686 Accuracy 0.5220999717712402\n",
      "Iteration 2480 Training loss 0.048050232231616974 Validation loss 0.04836549982428551 Accuracy 0.5152999758720398\n",
      "Iteration 2490 Training loss 0.04776062071323395 Validation loss 0.048330191522836685 Accuracy 0.5157999992370605\n",
      "Iteration 2500 Training loss 0.04735451936721802 Validation loss 0.04791955649852753 Accuracy 0.5198000073432922\n",
      "Iteration 2510 Training loss 0.04927656054496765 Validation loss 0.04876483231782913 Accuracy 0.5113000273704529\n",
      "Iteration 2520 Training loss 0.050076670944690704 Validation loss 0.04881470650434494 Accuracy 0.510699987411499\n",
      "Iteration 2530 Training loss 0.045333221554756165 Validation loss 0.04774520546197891 Accuracy 0.521399974822998\n",
      "Iteration 2540 Training loss 0.046610407531261444 Validation loss 0.047632958739995956 Accuracy 0.5220999717712402\n",
      "Iteration 2550 Training loss 0.045434076339006424 Validation loss 0.04819551110267639 Accuracy 0.5167999863624573\n",
      "Iteration 2560 Training loss 0.04723662883043289 Validation loss 0.048166435211896896 Accuracy 0.5174999833106995\n",
      "Iteration 2570 Training loss 0.0482943020761013 Validation loss 0.04755321145057678 Accuracy 0.5231000185012817\n",
      "Iteration 2580 Training loss 0.04882504418492317 Validation loss 0.05058866739273071 Accuracy 0.4927000105381012\n",
      "Iteration 2590 Training loss 0.042508020997047424 Validation loss 0.047885894775390625 Accuracy 0.5199999809265137\n",
      "Iteration 2600 Training loss 0.04596966877579689 Validation loss 0.04739664867520332 Accuracy 0.5250999927520752\n",
      "Iteration 2610 Training loss 0.0515514612197876 Validation loss 0.050428614020347595 Accuracy 0.4941999912261963\n",
      "Iteration 2620 Training loss 0.04980720579624176 Validation loss 0.047374434769153595 Accuracy 0.5253999829292297\n",
      "Iteration 2630 Training loss 0.0497610941529274 Validation loss 0.048987578600645065 Accuracy 0.5091999769210815\n",
      "Iteration 2640 Training loss 0.04783979430794716 Validation loss 0.04740529879927635 Accuracy 0.5250999927520752\n",
      "Iteration 2650 Training loss 0.0509941540658474 Validation loss 0.04812128096818924 Accuracy 0.5178999900817871\n",
      "Iteration 2660 Training loss 0.05048062279820442 Validation loss 0.04874999076128006 Accuracy 0.5113999843597412\n",
      "Iteration 2670 Training loss 0.04580431059002876 Validation loss 0.04821707680821419 Accuracy 0.5166000127792358\n",
      "Iteration 2680 Training loss 0.04879346862435341 Validation loss 0.04814014956355095 Accuracy 0.5181000232696533\n",
      "Iteration 2690 Training loss 0.04901200905442238 Validation loss 0.047407735139131546 Accuracy 0.5250999927520752\n",
      "Iteration 2700 Training loss 0.045974843204021454 Validation loss 0.0478588342666626 Accuracy 0.5202999711036682\n",
      "Iteration 2710 Training loss 0.04295661300420761 Validation loss 0.04790866747498512 Accuracy 0.5200999975204468\n",
      "Iteration 2720 Training loss 0.04625993221998215 Validation loss 0.04717258736491203 Accuracy 0.5271999835968018\n",
      "Iteration 2730 Training loss 0.04758598655462265 Validation loss 0.04823838919401169 Accuracy 0.5163999795913696\n",
      "Iteration 2740 Training loss 0.04607078433036804 Validation loss 0.0471823625266552 Accuracy 0.5271999835968018\n",
      "Iteration 2750 Training loss 0.04689206928014755 Validation loss 0.047893986105918884 Accuracy 0.5199999809265137\n",
      "Iteration 2760 Training loss 0.04928210377693176 Validation loss 0.047348905354738235 Accuracy 0.525600016117096\n",
      "Iteration 2770 Training loss 0.04694483056664467 Validation loss 0.04722890630364418 Accuracy 0.5267999768257141\n",
      "Iteration 2780 Training loss 0.04632348194718361 Validation loss 0.047647032886743546 Accuracy 0.522599995136261\n",
      "Iteration 2790 Training loss 0.047735873609781265 Validation loss 0.047039542347192764 Accuracy 0.5285000205039978\n",
      "Iteration 2800 Training loss 0.045606449246406555 Validation loss 0.0471208356320858 Accuracy 0.527899980545044\n",
      "Iteration 2810 Training loss 0.046307917684316635 Validation loss 0.047285083681344986 Accuracy 0.5256999731063843\n",
      "Iteration 2820 Training loss 0.046423930674791336 Validation loss 0.047839052975177765 Accuracy 0.5206000208854675\n",
      "Iteration 2830 Training loss 0.046015094965696335 Validation loss 0.0470217689871788 Accuracy 0.5285999774932861\n",
      "Iteration 2840 Training loss 0.049462009221315384 Validation loss 0.05062836781144142 Accuracy 0.4927999973297119\n",
      "Iteration 2850 Training loss 0.0469287671148777 Validation loss 0.0493692047894001 Accuracy 0.5051000118255615\n",
      "Iteration 2860 Training loss 0.04478667676448822 Validation loss 0.04726027697324753 Accuracy 0.5264000296592712\n",
      "Iteration 2870 Training loss 0.047171659767627716 Validation loss 0.047014232724905014 Accuracy 0.5289999842643738\n",
      "Iteration 2880 Training loss 0.04524623602628708 Validation loss 0.04707729071378708 Accuracy 0.5284000039100647\n",
      "Iteration 2890 Training loss 0.04543059319257736 Validation loss 0.04755270853638649 Accuracy 0.5236999988555908\n",
      "Iteration 2900 Training loss 0.04765312746167183 Validation loss 0.04742996022105217 Accuracy 0.5248000025749207\n",
      "Iteration 2910 Training loss 0.042002465575933456 Validation loss 0.047542423009872437 Accuracy 0.5231999754905701\n",
      "Iteration 2920 Training loss 0.050071846693754196 Validation loss 0.04965272545814514 Accuracy 0.5019000172615051\n",
      "Iteration 2930 Training loss 0.04867550730705261 Validation loss 0.047569140791893005 Accuracy 0.5230000019073486\n",
      "Iteration 2940 Training loss 0.04580048471689224 Validation loss 0.04705708473920822 Accuracy 0.5281999707221985\n",
      "Iteration 2950 Training loss 0.04631056264042854 Validation loss 0.04705706611275673 Accuracy 0.5284000039100647\n",
      "Iteration 2960 Training loss 0.046028461307287216 Validation loss 0.048806920647621155 Accuracy 0.5110999941825867\n",
      "Iteration 2970 Training loss 0.05218024551868439 Validation loss 0.04819488152861595 Accuracy 0.5164999961853027\n",
      "Iteration 2980 Training loss 0.04726588353514671 Validation loss 0.04725760594010353 Accuracy 0.5263000130653381\n",
      "Iteration 2990 Training loss 0.04995147883892059 Validation loss 0.04714245721697807 Accuracy 0.527400016784668\n",
      "Iteration 3000 Training loss 0.046148039400577545 Validation loss 0.04682229459285736 Accuracy 0.5309000015258789\n",
      "Iteration 3010 Training loss 0.04349391534924507 Validation loss 0.0479523241519928 Accuracy 0.519599974155426\n",
      "Iteration 3020 Training loss 0.04392489790916443 Validation loss 0.047691430896520615 Accuracy 0.5217000246047974\n",
      "Iteration 3030 Training loss 0.04656147211790085 Validation loss 0.0497431606054306 Accuracy 0.5013999938964844\n",
      "Iteration 3040 Training loss 0.04823225736618042 Validation loss 0.047098707407712936 Accuracy 0.5282999873161316\n",
      "Iteration 3050 Training loss 0.04623879864811897 Validation loss 0.04727226495742798 Accuracy 0.5264000296592712\n",
      "Iteration 3060 Training loss 0.047797247767448425 Validation loss 0.047773975878953934 Accuracy 0.5210000276565552\n",
      "Iteration 3070 Training loss 0.05025050416588783 Validation loss 0.04743567481637001 Accuracy 0.5246999859809875\n",
      "Iteration 3080 Training loss 0.045158762484788895 Validation loss 0.04695126414299011 Accuracy 0.5295000076293945\n",
      "Iteration 3090 Training loss 0.04762894660234451 Validation loss 0.04757469892501831 Accuracy 0.5234000086784363\n",
      "Iteration 3100 Training loss 0.044071756303310394 Validation loss 0.047330789268016815 Accuracy 0.5253000259399414\n",
      "Iteration 3110 Training loss 0.04460004344582558 Validation loss 0.047798316925764084 Accuracy 0.5210999846458435\n",
      "Iteration 3120 Training loss 0.04504713416099548 Validation loss 0.046986669301986694 Accuracy 0.52920001745224\n",
      "Iteration 3130 Training loss 0.04592161998152733 Validation loss 0.047503434121608734 Accuracy 0.5238000154495239\n",
      "Iteration 3140 Training loss 0.04645330831408501 Validation loss 0.04708382859826088 Accuracy 0.5282999873161316\n",
      "Iteration 3150 Training loss 0.04570149630308151 Validation loss 0.0472947359085083 Accuracy 0.526199996471405\n",
      "Iteration 3160 Training loss 0.047488708049058914 Validation loss 0.04694921523332596 Accuracy 0.5295000076293945\n",
      "Iteration 3170 Training loss 0.04627199470996857 Validation loss 0.049961090087890625 Accuracy 0.499099999666214\n",
      "Iteration 3180 Training loss 0.04819910228252411 Validation loss 0.046882182359695435 Accuracy 0.5303000211715698\n",
      "Iteration 3190 Training loss 0.049647510051727295 Validation loss 0.050543975085020065 Accuracy 0.49320000410079956\n",
      "Iteration 3200 Training loss 0.049080219119787216 Validation loss 0.047500547021627426 Accuracy 0.5235999822616577\n",
      "Iteration 3210 Training loss 0.04764541611075401 Validation loss 0.04941054806113243 Accuracy 0.5048999786376953\n",
      "Iteration 3220 Training loss 0.044425465166568756 Validation loss 0.047538116574287415 Accuracy 0.5235000252723694\n",
      "Iteration 3230 Training loss 0.04554925486445427 Validation loss 0.047856878489255905 Accuracy 0.520799994468689\n",
      "Iteration 3240 Training loss 0.045304447412490845 Validation loss 0.04740992560982704 Accuracy 0.5246000289916992\n",
      "Iteration 3250 Training loss 0.048468295484781265 Validation loss 0.048555634915828705 Accuracy 0.5127000212669373\n",
      "Iteration 3260 Training loss 0.048104409128427505 Validation loss 0.048387520015239716 Accuracy 0.5146999955177307\n",
      "Iteration 3270 Training loss 0.04359821602702141 Validation loss 0.047273848205804825 Accuracy 0.5264999866485596\n",
      "Iteration 3280 Training loss 0.046294037252664566 Validation loss 0.04733222350478172 Accuracy 0.525600016117096\n",
      "Iteration 3290 Training loss 0.0476231649518013 Validation loss 0.047959256917238235 Accuracy 0.5192999839782715\n",
      "Iteration 3300 Training loss 0.047959666699171066 Validation loss 0.04699346795678139 Accuracy 0.5289000272750854\n",
      "Iteration 3310 Training loss 0.046425990760326385 Validation loss 0.04773120954632759 Accuracy 0.5217000246047974\n",
      "Iteration 3320 Training loss 0.04278373718261719 Validation loss 0.047147154808044434 Accuracy 0.5271999835968018\n",
      "Iteration 3330 Training loss 0.04678535461425781 Validation loss 0.04679102450609207 Accuracy 0.531000018119812\n",
      "Iteration 3340 Training loss 0.04843073710799217 Validation loss 0.04735913127660751 Accuracy 0.5249999761581421\n",
      "Iteration 3350 Training loss 0.04604510962963104 Validation loss 0.04693705216050148 Accuracy 0.5300999879837036\n",
      "Iteration 3360 Training loss 0.0459635928273201 Validation loss 0.047038402408361435 Accuracy 0.5286999940872192\n",
      "Iteration 3370 Training loss 0.04600708186626434 Validation loss 0.046851567924022675 Accuracy 0.5303999781608582\n",
      "Iteration 3380 Training loss 0.044660501182079315 Validation loss 0.047975797206163406 Accuracy 0.5192000269889832\n",
      "Iteration 3390 Training loss 0.04192126542329788 Validation loss 0.046896226704120636 Accuracy 0.5299999713897705\n",
      "Iteration 3400 Training loss 0.04680021107196808 Validation loss 0.04716136306524277 Accuracy 0.5271000266075134\n",
      "Iteration 3410 Training loss 0.048293039202690125 Validation loss 0.04768432676792145 Accuracy 0.5221999883651733\n",
      "Iteration 3420 Training loss 0.04375270754098892 Validation loss 0.04687684029340744 Accuracy 0.5299999713897705\n",
      "Iteration 3430 Training loss 0.04690230265259743 Validation loss 0.04789977893233299 Accuracy 0.5200999975204468\n",
      "Iteration 3440 Training loss 0.04858214780688286 Validation loss 0.047333601862192154 Accuracy 0.5256999731063843\n",
      "Iteration 3450 Training loss 0.04762439802289009 Validation loss 0.04723315313458443 Accuracy 0.5268999934196472\n",
      "Iteration 3460 Training loss 0.04695799946784973 Validation loss 0.047366492450237274 Accuracy 0.5254999995231628\n",
      "Iteration 3470 Training loss 0.04417087882757187 Validation loss 0.047587793320417404 Accuracy 0.5232999920845032\n",
      "Iteration 3480 Training loss 0.04505225643515587 Validation loss 0.04689650237560272 Accuracy 0.5303000211715698\n",
      "Iteration 3490 Training loss 0.04694676771759987 Validation loss 0.047274671494960785 Accuracy 0.5264999866485596\n",
      "Iteration 3500 Training loss 0.04868028685450554 Validation loss 0.047305457293987274 Accuracy 0.5257999897003174\n",
      "Iteration 3510 Training loss 0.043678756803274155 Validation loss 0.04754742234945297 Accuracy 0.5238000154495239\n",
      "Iteration 3520 Training loss 0.04583081975579262 Validation loss 0.04729285091161728 Accuracy 0.5256999731063843\n",
      "Iteration 3530 Training loss 0.048626698553562164 Validation loss 0.04724755510687828 Accuracy 0.5266000032424927\n",
      "Iteration 3540 Training loss 0.04965875670313835 Validation loss 0.05019266530871391 Accuracy 0.4968999922275543\n",
      "Iteration 3550 Training loss 0.049784839153289795 Validation loss 0.04812680557370186 Accuracy 0.5174000263214111\n",
      "Iteration 3560 Training loss 0.04574679210782051 Validation loss 0.04719122126698494 Accuracy 0.5271000266075134\n",
      "Iteration 3570 Training loss 0.04850662127137184 Validation loss 0.046822383999824524 Accuracy 0.5309000015258789\n",
      "Iteration 3580 Training loss 0.04665834456682205 Validation loss 0.04725898802280426 Accuracy 0.5264000296592712\n",
      "Iteration 3590 Training loss 0.04643978551030159 Validation loss 0.04703045263886452 Accuracy 0.5286999940872192\n",
      "Iteration 3600 Training loss 0.04873839393258095 Validation loss 0.0469825305044651 Accuracy 0.5289000272750854\n",
      "Iteration 3610 Training loss 0.046624623239040375 Validation loss 0.04695737361907959 Accuracy 0.5295000076293945\n",
      "Iteration 3620 Training loss 0.04475446045398712 Validation loss 0.046975087374448776 Accuracy 0.5289999842643738\n",
      "Iteration 3630 Training loss 0.04568682238459587 Validation loss 0.047717656940221786 Accuracy 0.5220000147819519\n",
      "Iteration 3640 Training loss 0.04295698553323746 Validation loss 0.04683142527937889 Accuracy 0.5304999947547913\n",
      "Iteration 3650 Training loss 0.04876263812184334 Validation loss 0.047465384006500244 Accuracy 0.5242999792098999\n",
      "Iteration 3660 Training loss 0.04970017820596695 Validation loss 0.04858200252056122 Accuracy 0.5134000182151794\n",
      "Iteration 3670 Training loss 0.049611061811447144 Validation loss 0.04671788215637207 Accuracy 0.5320000052452087\n",
      "Iteration 3680 Training loss 0.04708441346883774 Validation loss 0.047674618661403656 Accuracy 0.5217999815940857\n",
      "Iteration 3690 Training loss 0.043785788118839264 Validation loss 0.047053128480911255 Accuracy 0.5282999873161316\n",
      "Iteration 3700 Training loss 0.04477634280920029 Validation loss 0.0486249178647995 Accuracy 0.5126000046730042\n",
      "Iteration 3710 Training loss 0.046671368181705475 Validation loss 0.04702592268586159 Accuracy 0.5289000272750854\n",
      "Iteration 3720 Training loss 0.04760653153061867 Validation loss 0.04703381657600403 Accuracy 0.5285999774932861\n",
      "Iteration 3730 Training loss 0.046292755752801895 Validation loss 0.047679148614406586 Accuracy 0.5224999785423279\n",
      "Iteration 3740 Training loss 0.043458521366119385 Validation loss 0.046843525022268295 Accuracy 0.5304999947547913\n",
      "Iteration 3750 Training loss 0.048894476145505905 Validation loss 0.04728931188583374 Accuracy 0.5260999798774719\n",
      "Iteration 3760 Training loss 0.04690852388739586 Validation loss 0.04718278720974922 Accuracy 0.5267000198364258\n",
      "Iteration 3770 Training loss 0.04744056239724159 Validation loss 0.04678764194250107 Accuracy 0.531000018119812\n",
      "Iteration 3780 Training loss 0.040352366864681244 Validation loss 0.04702101647853851 Accuracy 0.5289999842643738\n",
      "Iteration 3790 Training loss 0.04537791386246681 Validation loss 0.0466904491186142 Accuracy 0.5321000218391418\n",
      "Iteration 3800 Training loss 0.045256633311510086 Validation loss 0.04774162545800209 Accuracy 0.520799994468689\n",
      "Iteration 3810 Training loss 0.042703624814748764 Validation loss 0.046691108494997025 Accuracy 0.5324000120162964\n",
      "Iteration 3820 Training loss 0.046667613089084625 Validation loss 0.04717101529240608 Accuracy 0.5271999835968018\n",
      "Iteration 3830 Training loss 0.04713369905948639 Validation loss 0.04734610393643379 Accuracy 0.5254999995231628\n",
      "Iteration 3840 Training loss 0.04759647697210312 Validation loss 0.048846349120140076 Accuracy 0.5105999708175659\n",
      "Iteration 3850 Training loss 0.0475880391895771 Validation loss 0.04758024588227272 Accuracy 0.5231000185012817\n",
      "Iteration 3860 Training loss 0.04531814903020859 Validation loss 0.047178711742162704 Accuracy 0.527400016784668\n",
      "Iteration 3870 Training loss 0.04403473064303398 Validation loss 0.047262050211429596 Accuracy 0.5264999866485596\n",
      "Iteration 3880 Training loss 0.04648541659116745 Validation loss 0.046938106417655945 Accuracy 0.5299999713897705\n",
      "Iteration 3890 Training loss 0.04950260743498802 Validation loss 0.049327630549669266 Accuracy 0.5054000020027161\n",
      "Iteration 3900 Training loss 0.04445610195398331 Validation loss 0.046768881380558014 Accuracy 0.5311999917030334\n",
      "Iteration 3910 Training loss 0.05044000595808029 Validation loss 0.048365332186222076 Accuracy 0.5146999955177307\n",
      "Iteration 3920 Training loss 0.05068439617753029 Validation loss 0.05139331519603729 Accuracy 0.4844000041484833\n",
      "Iteration 3930 Training loss 0.04621082544326782 Validation loss 0.048385635018348694 Accuracy 0.5146999955177307\n",
      "Iteration 3940 Training loss 0.04805568605661392 Validation loss 0.04744826257228851 Accuracy 0.5246000289916992\n",
      "Iteration 3950 Training loss 0.049237217754125595 Validation loss 0.04737566411495209 Accuracy 0.5246999859809875\n",
      "Iteration 3960 Training loss 0.046964313834905624 Validation loss 0.04677775874733925 Accuracy 0.5309000015258789\n",
      "Iteration 3970 Training loss 0.04549465328454971 Validation loss 0.047779712826013565 Accuracy 0.5209000110626221\n",
      "Iteration 3980 Training loss 0.04671083763241768 Validation loss 0.04801074042916298 Accuracy 0.5182999968528748\n",
      "Iteration 3990 Training loss 0.04831227287650108 Validation loss 0.046765655279159546 Accuracy 0.5314000248908997\n",
      "Iteration 4000 Training loss 0.04770945385098457 Validation loss 0.04664283245801926 Accuracy 0.5320000052452087\n",
      "Iteration 4010 Training loss 0.04472162947058678 Validation loss 0.047075580805540085 Accuracy 0.5277000069618225\n",
      "Iteration 4020 Training loss 0.04426194354891777 Validation loss 0.04680003970861435 Accuracy 0.5302000045776367\n",
      "Iteration 4030 Training loss 0.046757981181144714 Validation loss 0.04736257344484329 Accuracy 0.5238000154495239\n",
      "Iteration 4040 Training loss 0.04530462250113487 Validation loss 0.04769187420606613 Accuracy 0.521399974822998\n",
      "Iteration 4050 Training loss 0.04768449813127518 Validation loss 0.04711158573627472 Accuracy 0.5273000001907349\n",
      "Iteration 4060 Training loss 0.04791414365172386 Validation loss 0.04727776348590851 Accuracy 0.5260999798774719\n",
      "Iteration 4070 Training loss 0.04948880895972252 Validation loss 0.04726174846291542 Accuracy 0.5264000296592712\n",
      "Iteration 4080 Training loss 0.05176037549972534 Validation loss 0.04690800607204437 Accuracy 0.5297999978065491\n",
      "Iteration 4090 Training loss 0.04597422853112221 Validation loss 0.046433888375759125 Accuracy 0.534600019454956\n",
      "Iteration 4100 Training loss 0.04654379189014435 Validation loss 0.047348711639642715 Accuracy 0.5252000093460083\n",
      "Iteration 4110 Training loss 0.04374098777770996 Validation loss 0.04663965851068497 Accuracy 0.5325000286102295\n",
      "Iteration 4120 Training loss 0.04757115617394447 Validation loss 0.04712670296430588 Accuracy 0.5268999934196472\n",
      "Iteration 4130 Training loss 0.047232091426849365 Validation loss 0.04678823798894882 Accuracy 0.5307999849319458\n",
      "Iteration 4140 Training loss 0.047520946711301804 Validation loss 0.04896513745188713 Accuracy 0.5092999935150146\n",
      "Iteration 4150 Training loss 0.04788050428032875 Validation loss 0.047354329377412796 Accuracy 0.5253999829292297\n",
      "Iteration 4160 Training loss 0.0475839301943779 Validation loss 0.04662485048174858 Accuracy 0.5327000021934509\n",
      "Iteration 4170 Training loss 0.049810029566287994 Validation loss 0.04667019471526146 Accuracy 0.5317999720573425\n",
      "Iteration 4180 Training loss 0.04596024006605148 Validation loss 0.04688403382897377 Accuracy 0.5299000144004822\n",
      "Iteration 4190 Training loss 0.04659106582403183 Validation loss 0.04845734313130379 Accuracy 0.51419997215271\n",
      "Iteration 4200 Training loss 0.04719472676515579 Validation loss 0.04703845828771591 Accuracy 0.5282999873161316\n",
      "Iteration 4210 Training loss 0.04654228314757347 Validation loss 0.04776866361498833 Accuracy 0.5206000208854675\n",
      "Iteration 4220 Training loss 0.047205064445734024 Validation loss 0.04700084030628204 Accuracy 0.5288000106811523\n",
      "Iteration 4230 Training loss 0.04576965421438217 Validation loss 0.04743026942014694 Accuracy 0.5242000222206116\n",
      "Iteration 4240 Training loss 0.04432336986064911 Validation loss 0.04686415195465088 Accuracy 0.5300999879837036\n",
      "Iteration 4250 Training loss 0.0464693158864975 Validation loss 0.0467141754925251 Accuracy 0.5318999886512756\n",
      "Iteration 4260 Training loss 0.04480918496847153 Validation loss 0.04673812910914421 Accuracy 0.5309000015258789\n",
      "Iteration 4270 Training loss 0.046446725726127625 Validation loss 0.04667068272829056 Accuracy 0.5315999984741211\n",
      "Iteration 4280 Training loss 0.04876433312892914 Validation loss 0.04802468791604042 Accuracy 0.5180000066757202\n",
      "Iteration 4290 Training loss 0.048420730978250504 Validation loss 0.04693889617919922 Accuracy 0.5284000039100647\n",
      "Iteration 4300 Training loss 0.04505692422389984 Validation loss 0.047512296587228775 Accuracy 0.5231000185012817\n",
      "Iteration 4310 Training loss 0.04593073949217796 Validation loss 0.04728032648563385 Accuracy 0.5250999927520752\n",
      "Iteration 4320 Training loss 0.051717452704906464 Validation loss 0.04706025496125221 Accuracy 0.527999997138977\n",
      "Iteration 4330 Training loss 0.04997749999165535 Validation loss 0.0472571887075901 Accuracy 0.5253000259399414\n",
      "Iteration 4340 Training loss 0.04685807228088379 Validation loss 0.04789411276578903 Accuracy 0.519599974155426\n",
      "Iteration 4350 Training loss 0.04325442388653755 Validation loss 0.0467393733561039 Accuracy 0.5310999751091003\n",
      "Iteration 4360 Training loss 0.04515489563345909 Validation loss 0.046729590743780136 Accuracy 0.5311999917030334\n",
      "Iteration 4370 Training loss 0.04742642119526863 Validation loss 0.04676854982972145 Accuracy 0.5307000279426575\n",
      "Iteration 4380 Training loss 0.044797200709581375 Validation loss 0.04686575010418892 Accuracy 0.5295000076293945\n",
      "Iteration 4390 Training loss 0.04480964317917824 Validation loss 0.04701802879571915 Accuracy 0.5285999774932861\n",
      "Iteration 4400 Training loss 0.045128632336854935 Validation loss 0.04700109735131264 Accuracy 0.5291000008583069\n",
      "Iteration 4410 Training loss 0.041648395359516144 Validation loss 0.04712912440299988 Accuracy 0.5271000266075134\n",
      "Iteration 4420 Training loss 0.04728066921234131 Validation loss 0.047079723328351974 Accuracy 0.5281000137329102\n",
      "Iteration 4430 Training loss 0.044274624437093735 Validation loss 0.04751632362604141 Accuracy 0.5236999988555908\n",
      "Iteration 4440 Training loss 0.047842808067798615 Validation loss 0.0472199022769928 Accuracy 0.5270000100135803\n",
      "Iteration 4450 Training loss 0.044717371463775635 Validation loss 0.04761578142642975 Accuracy 0.5231000185012817\n",
      "Iteration 4460 Training loss 0.04988875240087509 Validation loss 0.04675956070423126 Accuracy 0.5314000248908997\n",
      "Iteration 4470 Training loss 0.04412363842129707 Validation loss 0.04666231945157051 Accuracy 0.5327000021934509\n",
      "Iteration 4480 Training loss 0.04449272155761719 Validation loss 0.047191470861434937 Accuracy 0.5267999768257141\n",
      "Iteration 4490 Training loss 0.04629579186439514 Validation loss 0.046525340527296066 Accuracy 0.5338000059127808\n",
      "Iteration 4500 Training loss 0.04617500677704811 Validation loss 0.0473397932946682 Accuracy 0.5253999829292297\n",
      "Iteration 4510 Training loss 0.04985339939594269 Validation loss 0.04740328714251518 Accuracy 0.5246000289916992\n",
      "Iteration 4520 Training loss 0.04408226162195206 Validation loss 0.047084078192710876 Accuracy 0.527899980545044\n",
      "Iteration 4530 Training loss 0.04366930201649666 Validation loss 0.04669985920190811 Accuracy 0.5310999751091003\n",
      "Iteration 4540 Training loss 0.04912298545241356 Validation loss 0.04783913493156433 Accuracy 0.5202000141143799\n",
      "Iteration 4550 Training loss 0.043073032051324844 Validation loss 0.046934857964515686 Accuracy 0.5296000242233276\n",
      "Iteration 4560 Training loss 0.04547436907887459 Validation loss 0.047032348811626434 Accuracy 0.5285000205039978\n",
      "Iteration 4570 Training loss 0.0465390607714653 Validation loss 0.04764995723962784 Accuracy 0.522599995136261\n",
      "Iteration 4580 Training loss 0.046555206179618835 Validation loss 0.04749833047389984 Accuracy 0.5238999724388123\n",
      "Iteration 4590 Training loss 0.04349956288933754 Validation loss 0.04744033142924309 Accuracy 0.5246000289916992\n",
      "Iteration 4600 Training loss 0.04908348619937897 Validation loss 0.04786152020096779 Accuracy 0.5199999809265137\n",
      "Iteration 4610 Training loss 0.04533134400844574 Validation loss 0.04714104160666466 Accuracy 0.5275999903678894\n",
      "Iteration 4620 Training loss 0.04830870404839516 Validation loss 0.047095492482185364 Accuracy 0.527899980545044\n",
      "Iteration 4630 Training loss 0.04278158023953438 Validation loss 0.04651599004864693 Accuracy 0.5333999991416931\n",
      "Iteration 4640 Training loss 0.04714425280690193 Validation loss 0.04661170765757561 Accuracy 0.5324000120162964\n",
      "Iteration 4650 Training loss 0.042822666466236115 Validation loss 0.0466911680996418 Accuracy 0.531499981880188\n",
      "Iteration 4660 Training loss 0.04694880172610283 Validation loss 0.04660924896597862 Accuracy 0.5325000286102295\n",
      "Iteration 4670 Training loss 0.04763270914554596 Validation loss 0.046413686126470566 Accuracy 0.5343999862670898\n",
      "Iteration 4680 Training loss 0.04926047846674919 Validation loss 0.048446133732795715 Accuracy 0.5142999887466431\n",
      "Iteration 4690 Training loss 0.0474247932434082 Validation loss 0.048128943890333176 Accuracy 0.5167999863624573\n",
      "Iteration 4700 Training loss 0.04638250917196274 Validation loss 0.04764324054121971 Accuracy 0.5220000147819519\n",
      "Iteration 4710 Training loss 0.04767075553536415 Validation loss 0.04656944423913956 Accuracy 0.53329998254776\n",
      "Iteration 4720 Training loss 0.047243520617485046 Validation loss 0.04764919355511665 Accuracy 0.5224000215530396\n",
      "Iteration 4730 Training loss 0.04629261791706085 Validation loss 0.04704039916396141 Accuracy 0.5285000205039978\n",
      "Iteration 4740 Training loss 0.04694540798664093 Validation loss 0.04657893627882004 Accuracy 0.532800018787384\n",
      "Iteration 4750 Training loss 0.04489323869347572 Validation loss 0.04656949266791344 Accuracy 0.53329998254776\n",
      "Iteration 4760 Training loss 0.043249573558568954 Validation loss 0.046824175864458084 Accuracy 0.5307999849319458\n",
      "Iteration 4770 Training loss 0.04661979526281357 Validation loss 0.0473441407084465 Accuracy 0.5253999829292297\n",
      "Iteration 4780 Training loss 0.04200248792767525 Validation loss 0.04656166955828667 Accuracy 0.53329998254776\n",
      "Iteration 4790 Training loss 0.04569509997963905 Validation loss 0.04649517685174942 Accuracy 0.5339000225067139\n",
      "Iteration 4800 Training loss 0.04839141666889191 Validation loss 0.04700794816017151 Accuracy 0.5285000205039978\n",
      "Iteration 4810 Training loss 0.047912582755088806 Validation loss 0.047296829521656036 Accuracy 0.5250999927520752\n",
      "Iteration 4820 Training loss 0.04916105791926384 Validation loss 0.046945806592702866 Accuracy 0.5292999744415283\n",
      "Iteration 4830 Training loss 0.04823889583349228 Validation loss 0.048939529806375504 Accuracy 0.5088000297546387\n",
      "Iteration 4840 Training loss 0.04396443068981171 Validation loss 0.04776451736688614 Accuracy 0.5210000276565552\n",
      "Iteration 4850 Training loss 0.0491640642285347 Validation loss 0.04654407128691673 Accuracy 0.5335000157356262\n",
      "Iteration 4860 Training loss 0.047361038625240326 Validation loss 0.046818289905786514 Accuracy 0.5307000279426575\n",
      "Iteration 4870 Training loss 0.04772189259529114 Validation loss 0.0466095507144928 Accuracy 0.532800018787384\n",
      "Iteration 4880 Training loss 0.0451371930539608 Validation loss 0.04718736186623573 Accuracy 0.5266000032424927\n",
      "Iteration 4890 Training loss 0.04641987010836601 Validation loss 0.046794552356004715 Accuracy 0.5307999849319458\n",
      "Iteration 4900 Training loss 0.046246159821748734 Validation loss 0.046616289764642715 Accuracy 0.532800018787384\n",
      "Iteration 4910 Training loss 0.04291439801454544 Validation loss 0.04665990173816681 Accuracy 0.5317000150680542\n",
      "Iteration 4920 Training loss 0.046628762036561966 Validation loss 0.046886757016181946 Accuracy 0.5300999879837036\n",
      "Iteration 4930 Training loss 0.042426224797964096 Validation loss 0.04690536484122276 Accuracy 0.5295000076293945\n",
      "Iteration 4940 Training loss 0.04299073666334152 Validation loss 0.04669874161481857 Accuracy 0.5317999720573425\n",
      "Iteration 4950 Training loss 0.044388704001903534 Validation loss 0.04651954025030136 Accuracy 0.5338000059127808\n",
      "Iteration 4960 Training loss 0.04790700227022171 Validation loss 0.046951744705438614 Accuracy 0.5289000272750854\n",
      "Iteration 4970 Training loss 0.04813949018716812 Validation loss 0.04651656746864319 Accuracy 0.5329999923706055\n",
      "Iteration 4980 Training loss 0.046856243163347244 Validation loss 0.0479884147644043 Accuracy 0.5185999870300293\n",
      "Iteration 4990 Training loss 0.04397643357515335 Validation loss 0.04632159322500229 Accuracy 0.5353999733924866\n",
      "Iteration 5000 Training loss 0.044448964297771454 Validation loss 0.046577997505664825 Accuracy 0.5325000286102295\n",
      "Iteration 5010 Training loss 0.04313361272215843 Validation loss 0.04663391038775444 Accuracy 0.5318999886512756\n",
      "Iteration 5020 Training loss 0.04567557945847511 Validation loss 0.04651469737291336 Accuracy 0.5340999960899353\n",
      "Iteration 5030 Training loss 0.0443052314221859 Validation loss 0.04650767147541046 Accuracy 0.5335999727249146\n",
      "Iteration 5040 Training loss 0.0453169010579586 Validation loss 0.04640521481633186 Accuracy 0.5347999930381775\n",
      "Iteration 5050 Training loss 0.044016625732183456 Validation loss 0.04700750485062599 Accuracy 0.5289999842643738\n",
      "Iteration 5060 Training loss 0.04522376507520676 Validation loss 0.046721190214157104 Accuracy 0.5313000082969666\n",
      "Iteration 5070 Training loss 0.04517005756497383 Validation loss 0.04685114324092865 Accuracy 0.5304999947547913\n",
      "Iteration 5080 Training loss 0.045629169791936874 Validation loss 0.04636514186859131 Accuracy 0.5351999998092651\n",
      "Iteration 5090 Training loss 0.04492442309856415 Validation loss 0.04677772894501686 Accuracy 0.5313000082969666\n",
      "Iteration 5100 Training loss 0.048451557755470276 Validation loss 0.046764273196458817 Accuracy 0.5311999917030334\n",
      "Iteration 5110 Training loss 0.046379052102565765 Validation loss 0.04673049971461296 Accuracy 0.5314000248908997\n",
      "Iteration 5120 Training loss 0.047585297375917435 Validation loss 0.04677868261933327 Accuracy 0.5311999917030334\n",
      "Iteration 5130 Training loss 0.049532439559698105 Validation loss 0.048278018832206726 Accuracy 0.5159000158309937\n",
      "Iteration 5140 Training loss 0.047249749302864075 Validation loss 0.04677212983369827 Accuracy 0.5306000113487244\n",
      "Iteration 5150 Training loss 0.04647371545433998 Validation loss 0.04702656343579292 Accuracy 0.5284000039100647\n",
      "Iteration 5160 Training loss 0.04796691983938217 Validation loss 0.04734330251812935 Accuracy 0.5253000259399414\n",
      "Iteration 5170 Training loss 0.04765574634075165 Validation loss 0.046659916639328 Accuracy 0.5317999720573425\n",
      "Iteration 5180 Training loss 0.04311034083366394 Validation loss 0.04681184142827988 Accuracy 0.5299999713897705\n",
      "Iteration 5190 Training loss 0.043580759316682816 Validation loss 0.04661089926958084 Accuracy 0.5325999855995178\n",
      "Iteration 5200 Training loss 0.04552430659532547 Validation loss 0.046834032982587814 Accuracy 0.5293999910354614\n",
      "Iteration 5210 Training loss 0.04707175865769386 Validation loss 0.04674170911312103 Accuracy 0.531499981880188\n",
      "Iteration 5220 Training loss 0.046587564051151276 Validation loss 0.04757671803236008 Accuracy 0.5228999853134155\n",
      "Iteration 5230 Training loss 0.04494314640760422 Validation loss 0.047601889818906784 Accuracy 0.5220999717712402\n",
      "Iteration 5240 Training loss 0.04382439702749252 Validation loss 0.047239016741514206 Accuracy 0.5263000130653381\n",
      "Iteration 5250 Training loss 0.04530114307999611 Validation loss 0.046912677586078644 Accuracy 0.529699981212616\n",
      "Iteration 5260 Training loss 0.04424752667546272 Validation loss 0.046444330364465714 Accuracy 0.5343999862670898\n",
      "Iteration 5270 Training loss 0.04680728167295456 Validation loss 0.04655267670750618 Accuracy 0.5331000089645386\n",
      "Iteration 5280 Training loss 0.047086622565984726 Validation loss 0.04730263724923134 Accuracy 0.5256999731063843\n",
      "Iteration 5290 Training loss 0.04436846822500229 Validation loss 0.046912044286727905 Accuracy 0.5297999978065491\n",
      "Iteration 5300 Training loss 0.04803069680929184 Validation loss 0.047448135912418365 Accuracy 0.5235000252723694\n",
      "Iteration 5310 Training loss 0.04386657103896141 Validation loss 0.04690297320485115 Accuracy 0.5299000144004822\n",
      "Iteration 5320 Training loss 0.04867458716034889 Validation loss 0.047051407396793365 Accuracy 0.5282999873161316\n",
      "Iteration 5330 Training loss 0.04430631920695305 Validation loss 0.04666519537568092 Accuracy 0.5318999886512756\n",
      "Iteration 5340 Training loss 0.046408239752054214 Validation loss 0.04693927243351936 Accuracy 0.52920001745224\n",
      "Iteration 5350 Training loss 0.04571591690182686 Validation loss 0.04682884365320206 Accuracy 0.5306000113487244\n",
      "Iteration 5360 Training loss 0.04975733533501625 Validation loss 0.04644010215997696 Accuracy 0.5343000292778015\n",
      "Iteration 5370 Training loss 0.043434128165245056 Validation loss 0.04684026539325714 Accuracy 0.5299000144004822\n",
      "Iteration 5380 Training loss 0.04742477089166641 Validation loss 0.04642830416560173 Accuracy 0.5339999794960022\n",
      "Iteration 5390 Training loss 0.04593818262219429 Validation loss 0.04675173759460449 Accuracy 0.5307999849319458\n",
      "Iteration 5400 Training loss 0.04987935349345207 Validation loss 0.04681939631700516 Accuracy 0.5299999713897705\n",
      "Iteration 5410 Training loss 0.046711817383766174 Validation loss 0.046761829406023026 Accuracy 0.531000018119812\n",
      "Iteration 5420 Training loss 0.046358611434698105 Validation loss 0.047006189823150635 Accuracy 0.5288000106811523\n",
      "Iteration 5430 Training loss 0.04490658640861511 Validation loss 0.04663383215665817 Accuracy 0.5324000120162964\n",
      "Iteration 5440 Training loss 0.04280875250697136 Validation loss 0.046894583851099014 Accuracy 0.5299999713897705\n",
      "Iteration 5450 Training loss 0.04710524529218674 Validation loss 0.04744800180196762 Accuracy 0.5238000154495239\n",
      "Iteration 5460 Training loss 0.04693784564733505 Validation loss 0.04824141040444374 Accuracy 0.5162000060081482\n",
      "Iteration 5470 Training loss 0.0456780381500721 Validation loss 0.0467185340821743 Accuracy 0.5317000150680542\n",
      "Iteration 5480 Training loss 0.050396449863910675 Validation loss 0.04766841232776642 Accuracy 0.5221999883651733\n",
      "Iteration 5490 Training loss 0.04231901094317436 Validation loss 0.04689541086554527 Accuracy 0.5292999744415283\n",
      "Iteration 5500 Training loss 0.04903535544872284 Validation loss 0.046380482614040375 Accuracy 0.535099983215332\n",
      "Iteration 5510 Training loss 0.04260127246379852 Validation loss 0.046467822045087814 Accuracy 0.5340999960899353\n",
      "Iteration 5520 Training loss 0.0470784455537796 Validation loss 0.04672162979841232 Accuracy 0.5317000150680542\n",
      "Iteration 5530 Training loss 0.04810714349150658 Validation loss 0.04694594815373421 Accuracy 0.5293999910354614\n",
      "Iteration 5540 Training loss 0.047133784741163254 Validation loss 0.04641539230942726 Accuracy 0.534500002861023\n",
      "Iteration 5550 Training loss 0.04520133510231972 Validation loss 0.04668407142162323 Accuracy 0.5320000052452087\n",
      "Iteration 5560 Training loss 0.04663762077689171 Validation loss 0.04646508768200874 Accuracy 0.5340999960899353\n",
      "Iteration 5570 Training loss 0.05063793808221817 Validation loss 0.0464857816696167 Accuracy 0.5335000157356262\n",
      "Iteration 5580 Training loss 0.04848041757941246 Validation loss 0.04687542840838432 Accuracy 0.5299000144004822\n",
      "Iteration 5590 Training loss 0.045587316155433655 Validation loss 0.04647640138864517 Accuracy 0.5342000126838684\n",
      "Iteration 5600 Training loss 0.04655200615525246 Validation loss 0.04740462824702263 Accuracy 0.5249999761581421\n",
      "Iteration 5610 Training loss 0.04947097599506378 Validation loss 0.04660651460289955 Accuracy 0.5327000021934509\n",
      "Iteration 5620 Training loss 0.04510334134101868 Validation loss 0.04656905308365822 Accuracy 0.5333999991416931\n",
      "Iteration 5630 Training loss 0.04641688987612724 Validation loss 0.04675113782286644 Accuracy 0.5313000082969666\n",
      "Iteration 5640 Training loss 0.04295996204018593 Validation loss 0.04730860888957977 Accuracy 0.5254999995231628\n",
      "Iteration 5650 Training loss 0.04359794035553932 Validation loss 0.04676787927746773 Accuracy 0.5311999917030334\n",
      "Iteration 5660 Training loss 0.04929063469171524 Validation loss 0.047046959400177 Accuracy 0.5281000137329102\n",
      "Iteration 5670 Training loss 0.043598998337984085 Validation loss 0.04714076966047287 Accuracy 0.527400016784668\n",
      "Iteration 5680 Training loss 0.04626920074224472 Validation loss 0.04674096032977104 Accuracy 0.531499981880188\n",
      "Iteration 5690 Training loss 0.04879053309559822 Validation loss 0.04651089385151863 Accuracy 0.5342000126838684\n",
      "Iteration 5700 Training loss 0.04984644055366516 Validation loss 0.047011230140924454 Accuracy 0.5288000106811523\n",
      "Iteration 5710 Training loss 0.041584234684705734 Validation loss 0.046649884432554245 Accuracy 0.5321000218391418\n",
      "Iteration 5720 Training loss 0.04680920019745827 Validation loss 0.0466621033847332 Accuracy 0.5322999954223633\n",
      "Iteration 5730 Training loss 0.04699726775288582 Validation loss 0.04630997031927109 Accuracy 0.5357999801635742\n",
      "Iteration 5740 Training loss 0.04851815104484558 Validation loss 0.0466628335416317 Accuracy 0.5325000286102295\n",
      "Iteration 5750 Training loss 0.04815465956926346 Validation loss 0.04639663174748421 Accuracy 0.5349000096321106\n",
      "Iteration 5760 Training loss 0.046753983944654465 Validation loss 0.04633466154336929 Accuracy 0.5354999899864197\n",
      "Iteration 5770 Training loss 0.04365585371851921 Validation loss 0.04635284096002579 Accuracy 0.535099983215332\n",
      "Iteration 5780 Training loss 0.043826863169670105 Validation loss 0.0464620403945446 Accuracy 0.5343999862670898\n",
      "Iteration 5790 Training loss 0.047275666147470474 Validation loss 0.04661567509174347 Accuracy 0.5328999757766724\n",
      "Iteration 5800 Training loss 0.04815854877233505 Validation loss 0.0464126355946064 Accuracy 0.5349000096321106\n",
      "Iteration 5810 Training loss 0.04084836691617966 Validation loss 0.046414464712142944 Accuracy 0.5343999862670898\n",
      "Iteration 5820 Training loss 0.04629665985703468 Validation loss 0.046586308628320694 Accuracy 0.5328999757766724\n",
      "Iteration 5830 Training loss 0.045239344239234924 Validation loss 0.04677714407444 Accuracy 0.5313000082969666\n",
      "Iteration 5840 Training loss 0.04784630239009857 Validation loss 0.04646287485957146 Accuracy 0.5338000059127808\n",
      "Iteration 5850 Training loss 0.043523773550987244 Validation loss 0.046340860426425934 Accuracy 0.5351999998092651\n",
      "Iteration 5860 Training loss 0.04571883752942085 Validation loss 0.04631325230002403 Accuracy 0.5357000231742859\n",
      "Iteration 5870 Training loss 0.04659084230661392 Validation loss 0.048873789608478546 Accuracy 0.5102999806404114\n",
      "Iteration 5880 Training loss 0.04373014345765114 Validation loss 0.04656369239091873 Accuracy 0.5329999923706055\n",
      "Iteration 5890 Training loss 0.04607173800468445 Validation loss 0.04649671912193298 Accuracy 0.5338000059127808\n",
      "Iteration 5900 Training loss 0.04497472196817398 Validation loss 0.04645531252026558 Accuracy 0.5343999862670898\n",
      "Iteration 5910 Training loss 0.045969441533088684 Validation loss 0.04624325782060623 Accuracy 0.5364999771118164\n",
      "Iteration 5920 Training loss 0.04625554010272026 Validation loss 0.04644963517785072 Accuracy 0.5343000292778015\n",
      "Iteration 5930 Training loss 0.046054139733314514 Validation loss 0.04708421975374222 Accuracy 0.5278000235557556\n",
      "Iteration 5940 Training loss 0.04588588327169418 Validation loss 0.046785205602645874 Accuracy 0.531000018119812\n",
      "Iteration 5950 Training loss 0.048294808715581894 Validation loss 0.04650906100869179 Accuracy 0.5335999727249146\n",
      "Iteration 5960 Training loss 0.04451153427362442 Validation loss 0.04680303856730461 Accuracy 0.531000018119812\n",
      "Iteration 5970 Training loss 0.04544975608587265 Validation loss 0.046547528356313705 Accuracy 0.5333999991416931\n",
      "Iteration 5980 Training loss 0.04423755779862404 Validation loss 0.046367861330509186 Accuracy 0.5353000164031982\n",
      "Iteration 5990 Training loss 0.04615940526127815 Validation loss 0.04637552425265312 Accuracy 0.5351999998092651\n",
      "Iteration 6000 Training loss 0.04865352436900139 Validation loss 0.047088801860809326 Accuracy 0.5273000001907349\n",
      "Iteration 6010 Training loss 0.04430951923131943 Validation loss 0.04658861458301544 Accuracy 0.5328999757766724\n",
      "Iteration 6020 Training loss 0.047828253358602524 Validation loss 0.04825112968683243 Accuracy 0.5157999992370605\n",
      "Iteration 6030 Training loss 0.04614987224340439 Validation loss 0.0468359999358654 Accuracy 0.5307000279426575\n",
      "Iteration 6040 Training loss 0.04881737381219864 Validation loss 0.04659067839384079 Accuracy 0.532800018787384\n",
      "Iteration 6050 Training loss 0.0444437675178051 Validation loss 0.04629488289356232 Accuracy 0.5357999801635742\n",
      "Iteration 6060 Training loss 0.04377152398228645 Validation loss 0.04645216837525368 Accuracy 0.5343999862670898\n",
      "Iteration 6070 Training loss 0.04552065581083298 Validation loss 0.046703994274139404 Accuracy 0.531499981880188\n",
      "Iteration 6080 Training loss 0.04592243582010269 Validation loss 0.0470062755048275 Accuracy 0.5288000106811523\n",
      "Iteration 6090 Training loss 0.04581455513834953 Validation loss 0.04662620276212692 Accuracy 0.5325999855995178\n",
      "Iteration 6100 Training loss 0.042274102568626404 Validation loss 0.04629724472761154 Accuracy 0.5360000133514404\n",
      "Iteration 6110 Training loss 0.04712635651230812 Validation loss 0.046629805117845535 Accuracy 0.532800018787384\n",
      "Iteration 6120 Training loss 0.04319334402680397 Validation loss 0.046262480318546295 Accuracy 0.5358999967575073\n",
      "Iteration 6130 Training loss 0.0477713868021965 Validation loss 0.04651820659637451 Accuracy 0.5336999893188477\n",
      "Iteration 6140 Training loss 0.049174703657627106 Validation loss 0.046523358672857285 Accuracy 0.5335000157356262\n",
      "Iteration 6150 Training loss 0.046492692083120346 Validation loss 0.04648154228925705 Accuracy 0.53329998254776\n",
      "Iteration 6160 Training loss 0.046484243124723434 Validation loss 0.04620124399662018 Accuracy 0.5365999937057495\n",
      "Iteration 6170 Training loss 0.04575547203421593 Validation loss 0.04643320292234421 Accuracy 0.5340999960899353\n",
      "Iteration 6180 Training loss 0.04870864003896713 Validation loss 0.046572089195251465 Accuracy 0.5329999923706055\n",
      "Iteration 6190 Training loss 0.04433853179216385 Validation loss 0.04630041867494583 Accuracy 0.5357000231742859\n",
      "Iteration 6200 Training loss 0.046364907175302505 Validation loss 0.046237774193286896 Accuracy 0.5360999703407288\n",
      "Iteration 6210 Training loss 0.046611420810222626 Validation loss 0.046595294028520584 Accuracy 0.5322999954223633\n",
      "Iteration 6220 Training loss 0.046578548848629 Validation loss 0.046589575707912445 Accuracy 0.5327000021934509\n",
      "Iteration 6230 Training loss 0.043419916182756424 Validation loss 0.04681818187236786 Accuracy 0.5296000242233276\n",
      "Iteration 6240 Training loss 0.04836094751954079 Validation loss 0.04735250398516655 Accuracy 0.5246999859809875\n",
      "Iteration 6250 Training loss 0.047056809067726135 Validation loss 0.046903494745492935 Accuracy 0.5296000242233276\n",
      "Iteration 6260 Training loss 0.04605143144726753 Validation loss 0.04707492142915726 Accuracy 0.527899980545044\n",
      "Iteration 6270 Training loss 0.046751052141189575 Validation loss 0.047507837414741516 Accuracy 0.5235000252723694\n",
      "Iteration 6280 Training loss 0.04438972473144531 Validation loss 0.046334847807884216 Accuracy 0.5353000164031982\n",
      "Iteration 6290 Training loss 0.0460067018866539 Validation loss 0.04657527431845665 Accuracy 0.532800018787384\n",
      "Iteration 6300 Training loss 0.04513264819979668 Validation loss 0.04616016522049904 Accuracy 0.5371000170707703\n",
      "Iteration 6310 Training loss 0.04425261542201042 Validation loss 0.046556372195482254 Accuracy 0.5332000255584717\n",
      "Iteration 6320 Training loss 0.046009790152311325 Validation loss 0.04768559709191322 Accuracy 0.5218999981880188\n",
      "Iteration 6330 Training loss 0.04452534019947052 Validation loss 0.0467086024582386 Accuracy 0.5317999720573425\n",
      "Iteration 6340 Training loss 0.046063151210546494 Validation loss 0.04700073227286339 Accuracy 0.5282999873161316\n",
      "Iteration 6350 Training loss 0.046593524515628815 Validation loss 0.04663076251745224 Accuracy 0.5317000150680542\n",
      "Iteration 6360 Training loss 0.04531679302453995 Validation loss 0.046792857348918915 Accuracy 0.5306000113487244\n",
      "Iteration 6370 Training loss 0.043254125863313675 Validation loss 0.04658355563879013 Accuracy 0.5322999954223633\n",
      "Iteration 6380 Training loss 0.04541539400815964 Validation loss 0.04662616178393364 Accuracy 0.5321999788284302\n",
      "Iteration 6390 Training loss 0.05078735202550888 Validation loss 0.047127753496170044 Accuracy 0.5263000130653381\n",
      "Iteration 6400 Training loss 0.042903583496809006 Validation loss 0.047121431678533554 Accuracy 0.5267000198364258\n",
      "Iteration 6410 Training loss 0.044718317687511444 Validation loss 0.046840518712997437 Accuracy 0.5297999978065491\n",
      "Iteration 6420 Training loss 0.04505011439323425 Validation loss 0.046705909073352814 Accuracy 0.531499981880188\n",
      "Iteration 6430 Training loss 0.04429174214601517 Validation loss 0.04636172950267792 Accuracy 0.5349000096321106\n",
      "Iteration 6440 Training loss 0.04800983518362045 Validation loss 0.04670311510562897 Accuracy 0.5313000082969666\n",
      "Iteration 6450 Training loss 0.0464579202234745 Validation loss 0.04827742651104927 Accuracy 0.5157999992370605\n",
      "Iteration 6460 Training loss 0.04615119472146034 Validation loss 0.04642650485038757 Accuracy 0.5340999960899353\n",
      "Iteration 6470 Training loss 0.04657944664359093 Validation loss 0.04641333967447281 Accuracy 0.534600019454956\n",
      "Iteration 6480 Training loss 0.045059870928525925 Validation loss 0.04710973799228668 Accuracy 0.5278000235557556\n",
      "Iteration 6490 Training loss 0.04796120151877403 Validation loss 0.04643520340323448 Accuracy 0.5343000292778015\n",
      "Iteration 6500 Training loss 0.04685632884502411 Validation loss 0.04639468714594841 Accuracy 0.535099983215332\n",
      "Iteration 6510 Training loss 0.04656338319182396 Validation loss 0.04810298979282379 Accuracy 0.5174999833106995\n",
      "Iteration 6520 Training loss 0.04722341522574425 Validation loss 0.04632244631648064 Accuracy 0.5354999899864197\n",
      "Iteration 6530 Training loss 0.04473332688212395 Validation loss 0.046362489461898804 Accuracy 0.5353000164031982\n",
      "Iteration 6540 Training loss 0.0490020290017128 Validation loss 0.04636188969016075 Accuracy 0.5347999930381775\n",
      "Iteration 6550 Training loss 0.04664568230509758 Validation loss 0.046255312860012054 Accuracy 0.5360000133514404\n",
      "Iteration 6560 Training loss 0.04485439136624336 Validation loss 0.04640920087695122 Accuracy 0.5346999764442444\n",
      "Iteration 6570 Training loss 0.043523579835891724 Validation loss 0.046716850250959396 Accuracy 0.5317000150680542\n",
      "Iteration 6580 Training loss 0.04696232080459595 Validation loss 0.04660410061478615 Accuracy 0.532800018787384\n",
      "Iteration 6590 Training loss 0.04549950733780861 Validation loss 0.04663712903857231 Accuracy 0.5324000120162964\n",
      "Iteration 6600 Training loss 0.0465812087059021 Validation loss 0.04648347198963165 Accuracy 0.5338000059127808\n",
      "Iteration 6610 Training loss 0.048290252685546875 Validation loss 0.046496424823999405 Accuracy 0.5336999893188477\n",
      "Iteration 6620 Training loss 0.04730614274740219 Validation loss 0.047170497477054596 Accuracy 0.5273000001907349\n",
      "Iteration 6630 Training loss 0.04794711247086525 Validation loss 0.04639020934700966 Accuracy 0.5346999764442444\n",
      "Iteration 6640 Training loss 0.04828594624996185 Validation loss 0.04665005952119827 Accuracy 0.5325000286102295\n",
      "Iteration 6650 Training loss 0.047345515340566635 Validation loss 0.04632622376084328 Accuracy 0.5354999899864197\n",
      "Iteration 6660 Training loss 0.04696299135684967 Validation loss 0.04684622958302498 Accuracy 0.5299999713897705\n",
      "Iteration 6670 Training loss 0.04097193479537964 Validation loss 0.04632125049829483 Accuracy 0.5357000231742859\n",
      "Iteration 6680 Training loss 0.04356303811073303 Validation loss 0.04636550694704056 Accuracy 0.5350000262260437\n",
      "Iteration 6690 Training loss 0.04417160153388977 Validation loss 0.04638773947954178 Accuracy 0.5350000262260437\n",
      "Iteration 6700 Training loss 0.04292960464954376 Validation loss 0.046365536749362946 Accuracy 0.535099983215332\n",
      "Iteration 6710 Training loss 0.04409877583384514 Validation loss 0.04615385830402374 Accuracy 0.5370000004768372\n",
      "Iteration 6720 Training loss 0.0465540774166584 Validation loss 0.04619569703936577 Accuracy 0.5364999771118164\n",
      "Iteration 6730 Training loss 0.04826370254158974 Validation loss 0.04706665128469467 Accuracy 0.5282999873161316\n",
      "Iteration 6740 Training loss 0.046662114560604095 Validation loss 0.04638771340250969 Accuracy 0.534600019454956\n",
      "Iteration 6750 Training loss 0.045366302132606506 Validation loss 0.0470547080039978 Accuracy 0.5285000205039978\n",
      "Iteration 6760 Training loss 0.04891238734126091 Validation loss 0.046399228274822235 Accuracy 0.5350000262260437\n",
      "Iteration 6770 Training loss 0.04522047936916351 Validation loss 0.04645506665110588 Accuracy 0.5339999794960022\n",
      "Iteration 6780 Training loss 0.0472240075469017 Validation loss 0.046591561287641525 Accuracy 0.5324000120162964\n",
      "Iteration 6790 Training loss 0.04448482394218445 Validation loss 0.04631882905960083 Accuracy 0.5349000096321106\n",
      "Iteration 6800 Training loss 0.0464886873960495 Validation loss 0.0464266799390316 Accuracy 0.5342000126838684\n",
      "Iteration 6810 Training loss 0.04442838206887245 Validation loss 0.04681245982646942 Accuracy 0.5302000045776367\n",
      "Iteration 6820 Training loss 0.04671735689043999 Validation loss 0.04664121940732002 Accuracy 0.5321000218391418\n",
      "Iteration 6830 Training loss 0.043579090386629105 Validation loss 0.04650438576936722 Accuracy 0.5335999727249146\n",
      "Iteration 6840 Training loss 0.04347244277596474 Validation loss 0.046424198895692825 Accuracy 0.534500002861023\n",
      "Iteration 6850 Training loss 0.043486252427101135 Validation loss 0.04650457203388214 Accuracy 0.5335999727249146\n",
      "Iteration 6860 Training loss 0.04291457682847977 Validation loss 0.04670949652791023 Accuracy 0.5315999984741211\n",
      "Iteration 6870 Training loss 0.047123078256845474 Validation loss 0.04642033204436302 Accuracy 0.5335999727249146\n",
      "Iteration 6880 Training loss 0.0457327663898468 Validation loss 0.047056641429662704 Accuracy 0.527999997138977\n",
      "Iteration 6890 Training loss 0.04010873660445213 Validation loss 0.04644332081079483 Accuracy 0.5342000126838684\n",
      "Iteration 6900 Training loss 0.046238016337156296 Validation loss 0.04755614325404167 Accuracy 0.5228999853134155\n",
      "Iteration 6910 Training loss 0.046918369829654694 Validation loss 0.0466138981282711 Accuracy 0.532800018787384\n",
      "Iteration 6920 Training loss 0.04673028737306595 Validation loss 0.04673609882593155 Accuracy 0.531499981880188\n",
      "Iteration 6930 Training loss 0.04231144115328789 Validation loss 0.04652675613760948 Accuracy 0.5339999794960022\n",
      "Iteration 6940 Training loss 0.043762482702732086 Validation loss 0.04664821922779083 Accuracy 0.5325999855995178\n",
      "Iteration 6950 Training loss 0.04718609154224396 Validation loss 0.04625394567847252 Accuracy 0.5364999771118164\n",
      "Iteration 6960 Training loss 0.04183396324515343 Validation loss 0.04630040004849434 Accuracy 0.5357999801635742\n",
      "Iteration 6970 Training loss 0.04586651548743248 Validation loss 0.046716686338186264 Accuracy 0.5317000150680542\n",
      "Iteration 6980 Training loss 0.0467723123729229 Validation loss 0.04671433940529823 Accuracy 0.5315999984741211\n",
      "Iteration 6990 Training loss 0.045247238129377365 Validation loss 0.046780362725257874 Accuracy 0.5306000113487244\n",
      "Iteration 7000 Training loss 0.04973919317126274 Validation loss 0.04716211557388306 Accuracy 0.5268999934196472\n",
      "Iteration 7010 Training loss 0.045736800879240036 Validation loss 0.04764966666698456 Accuracy 0.5224999785423279\n",
      "Iteration 7020 Training loss 0.04377789422869682 Validation loss 0.04625646397471428 Accuracy 0.536300003528595\n",
      "Iteration 7030 Training loss 0.046615101397037506 Validation loss 0.046758584678173065 Accuracy 0.5307999849319458\n",
      "Iteration 7040 Training loss 0.044709015637636185 Validation loss 0.046565547585487366 Accuracy 0.5332000255584717\n",
      "Iteration 7050 Training loss 0.04683929309248924 Validation loss 0.0464363619685173 Accuracy 0.5343000292778015\n",
      "Iteration 7060 Training loss 0.04474418982863426 Validation loss 0.04631225764751434 Accuracy 0.5357999801635742\n",
      "Iteration 7070 Training loss 0.04699870944023132 Validation loss 0.046635039150714874 Accuracy 0.5327000021934509\n",
      "Iteration 7080 Training loss 0.04599299281835556 Validation loss 0.046653855592012405 Accuracy 0.5322999954223633\n",
      "Iteration 7090 Training loss 0.04598013684153557 Validation loss 0.0465533584356308 Accuracy 0.53329998254776\n",
      "Iteration 7100 Training loss 0.04331449419260025 Validation loss 0.04651303589344025 Accuracy 0.5335999727249146\n",
      "Iteration 7110 Training loss 0.04682224988937378 Validation loss 0.046845704317092896 Accuracy 0.5300999879837036\n",
      "Iteration 7120 Training loss 0.044667571783065796 Validation loss 0.046246487647295 Accuracy 0.5364999771118164\n",
      "Iteration 7130 Training loss 0.04244733229279518 Validation loss 0.04661552235484123 Accuracy 0.5324000120162964\n",
      "Iteration 7140 Training loss 0.04497312381863594 Validation loss 0.04642125591635704 Accuracy 0.534500002861023\n",
      "Iteration 7150 Training loss 0.04707648605108261 Validation loss 0.04630998149514198 Accuracy 0.5360000133514404\n",
      "Iteration 7160 Training loss 0.04337722808122635 Validation loss 0.046378329396247864 Accuracy 0.5350000262260437\n",
      "Iteration 7170 Training loss 0.0476493313908577 Validation loss 0.046419017016887665 Accuracy 0.534600019454956\n",
      "Iteration 7180 Training loss 0.04764880985021591 Validation loss 0.046383995562791824 Accuracy 0.5349000096321106\n",
      "Iteration 7190 Training loss 0.0463884174823761 Validation loss 0.04638923704624176 Accuracy 0.5346999764442444\n",
      "Iteration 7200 Training loss 0.040234774351119995 Validation loss 0.04644964262843132 Accuracy 0.5340999960899353\n",
      "Iteration 7210 Training loss 0.04425831139087677 Validation loss 0.04631518945097923 Accuracy 0.5354999899864197\n",
      "Iteration 7220 Training loss 0.04703119769692421 Validation loss 0.046164125204086304 Accuracy 0.5371000170707703\n",
      "Iteration 7230 Training loss 0.04420502111315727 Validation loss 0.04643844813108444 Accuracy 0.534500002861023\n",
      "Iteration 7240 Training loss 0.043307382613420486 Validation loss 0.046431899070739746 Accuracy 0.5343000292778015\n",
      "Iteration 7250 Training loss 0.04761000722646713 Validation loss 0.04665185138583183 Accuracy 0.5321000218391418\n",
      "Iteration 7260 Training loss 0.04678615182638168 Validation loss 0.046132296323776245 Accuracy 0.5371999740600586\n",
      "Iteration 7270 Training loss 0.04648428410291672 Validation loss 0.04628163203597069 Accuracy 0.5357999801635742\n",
      "Iteration 7280 Training loss 0.04749526083469391 Validation loss 0.04688253998756409 Accuracy 0.529699981212616\n",
      "Iteration 7290 Training loss 0.047770753502845764 Validation loss 0.04751528427004814 Accuracy 0.5231000185012817\n",
      "Iteration 7300 Training loss 0.043533243238925934 Validation loss 0.046220604330301285 Accuracy 0.536300003528595\n",
      "Iteration 7310 Training loss 0.0437033586204052 Validation loss 0.046441636979579926 Accuracy 0.5339999794960022\n",
      "Iteration 7320 Training loss 0.05044873431324959 Validation loss 0.04661612957715988 Accuracy 0.5325999855995178\n",
      "Iteration 7330 Training loss 0.04474028944969177 Validation loss 0.04611969739198685 Accuracy 0.5378000140190125\n",
      "Iteration 7340 Training loss 0.04658955708146095 Validation loss 0.046722251921892166 Accuracy 0.5317999720573425\n",
      "Iteration 7350 Training loss 0.04415822774171829 Validation loss 0.04646632820367813 Accuracy 0.5339000225067139\n",
      "Iteration 7360 Training loss 0.04594296216964722 Validation loss 0.046546634286642075 Accuracy 0.5335000157356262\n",
      "Iteration 7370 Training loss 0.0432862862944603 Validation loss 0.04646483436226845 Accuracy 0.5340999960899353\n",
      "Iteration 7380 Training loss 0.04562556371092796 Validation loss 0.04680800437927246 Accuracy 0.5306000113487244\n",
      "Iteration 7390 Training loss 0.05014195665717125 Validation loss 0.04639333114027977 Accuracy 0.5350000262260437\n",
      "Iteration 7400 Training loss 0.04467233270406723 Validation loss 0.04606963321566582 Accuracy 0.538100004196167\n",
      "Iteration 7410 Training loss 0.04542747139930725 Validation loss 0.04614517092704773 Accuracy 0.5374000072479248\n",
      "Iteration 7420 Training loss 0.04524824023246765 Validation loss 0.04680125042796135 Accuracy 0.5307999849319458\n",
      "Iteration 7430 Training loss 0.04565375670790672 Validation loss 0.04618912935256958 Accuracy 0.536899983882904\n",
      "Iteration 7440 Training loss 0.03974432870745659 Validation loss 0.04623202979564667 Accuracy 0.5360999703407288\n",
      "Iteration 7450 Training loss 0.04375426843762398 Validation loss 0.046196531504392624 Accuracy 0.5364999771118164\n",
      "Iteration 7460 Training loss 0.051200494170188904 Validation loss 0.04896333068609238 Accuracy 0.5092999935150146\n",
      "Iteration 7470 Training loss 0.04731065034866333 Validation loss 0.04640873149037361 Accuracy 0.534600019454956\n",
      "Iteration 7480 Training loss 0.0458868034183979 Validation loss 0.046655986458063126 Accuracy 0.5322999954223633\n",
      "Iteration 7490 Training loss 0.047366172075271606 Validation loss 0.046248339116573334 Accuracy 0.5358999967575073\n",
      "Iteration 7500 Training loss 0.04837379604578018 Validation loss 0.046346504241228104 Accuracy 0.5350000262260437\n",
      "Iteration 7510 Training loss 0.04546842351555824 Validation loss 0.04623796045780182 Accuracy 0.5360000133514404\n",
      "Iteration 7520 Training loss 0.043926164507865906 Validation loss 0.046616170555353165 Accuracy 0.5321999788284302\n",
      "Iteration 7530 Training loss 0.04353766515851021 Validation loss 0.04642118513584137 Accuracy 0.534500002861023\n",
      "Iteration 7540 Training loss 0.043238382786512375 Validation loss 0.04627756401896477 Accuracy 0.5356000065803528\n",
      "Iteration 7550 Training loss 0.04709644615650177 Validation loss 0.04653004929423332 Accuracy 0.5331000089645386\n",
      "Iteration 7560 Training loss 0.044688817113637924 Validation loss 0.046769291162490845 Accuracy 0.5302000045776367\n",
      "Iteration 7570 Training loss 0.045745253562927246 Validation loss 0.04637744277715683 Accuracy 0.534600019454956\n",
      "Iteration 7580 Training loss 0.045003391802310944 Validation loss 0.04667264595627785 Accuracy 0.5317999720573425\n",
      "Iteration 7590 Training loss 0.04644954949617386 Validation loss 0.04643737152218819 Accuracy 0.5336999893188477\n",
      "Iteration 7600 Training loss 0.044960957020521164 Validation loss 0.04647289216518402 Accuracy 0.5338000059127808\n",
      "Iteration 7610 Training loss 0.04253856837749481 Validation loss 0.046419091522693634 Accuracy 0.5343000292778015\n",
      "Iteration 7620 Training loss 0.04241211339831352 Validation loss 0.0463537760078907 Accuracy 0.5353000164031982\n",
      "Iteration 7630 Training loss 0.04510222002863884 Validation loss 0.046113114804029465 Accuracy 0.5375000238418579\n",
      "Iteration 7640 Training loss 0.041641153395175934 Validation loss 0.046858686953783035 Accuracy 0.5302000045776367\n",
      "Iteration 7650 Training loss 0.042208231985569 Validation loss 0.04655197262763977 Accuracy 0.53329998254776\n",
      "Iteration 7660 Training loss 0.04236766695976257 Validation loss 0.046842314302921295 Accuracy 0.5299000144004822\n",
      "Iteration 7670 Training loss 0.046777237206697464 Validation loss 0.04620741680264473 Accuracy 0.5365999937057495\n",
      "Iteration 7680 Training loss 0.046401847153902054 Validation loss 0.04618329554796219 Accuracy 0.5360000133514404\n",
      "Iteration 7690 Training loss 0.045357197523117065 Validation loss 0.04625321924686432 Accuracy 0.5353000164031982\n",
      "Iteration 7700 Training loss 0.04709766060113907 Validation loss 0.04618096351623535 Accuracy 0.5364000201225281\n",
      "Iteration 7710 Training loss 0.043950118124485016 Validation loss 0.046320509165525436 Accuracy 0.5354999899864197\n",
      "Iteration 7720 Training loss 0.0425582081079483 Validation loss 0.0469566211104393 Accuracy 0.5289999842643738\n",
      "Iteration 7730 Training loss 0.04614630714058876 Validation loss 0.04631522297859192 Accuracy 0.5351999998092651\n",
      "Iteration 7740 Training loss 0.04679353907704353 Validation loss 0.04676031693816185 Accuracy 0.531000018119812\n",
      "Iteration 7750 Training loss 0.04728200286626816 Validation loss 0.046471092849969864 Accuracy 0.5340999960899353\n",
      "Iteration 7760 Training loss 0.044260136783123016 Validation loss 0.046494804322719574 Accuracy 0.5335999727249146\n",
      "Iteration 7770 Training loss 0.050410736352205276 Validation loss 0.04677974432706833 Accuracy 0.5304999947547913\n",
      "Iteration 7780 Training loss 0.04406537488102913 Validation loss 0.04632808640599251 Accuracy 0.5354999899864197\n",
      "Iteration 7790 Training loss 0.04417966678738594 Validation loss 0.046180784702301025 Accuracy 0.536899983882904\n",
      "Iteration 7800 Training loss 0.05085168033838272 Validation loss 0.04639634117484093 Accuracy 0.5338000059127808\n",
      "Iteration 7810 Training loss 0.0465722419321537 Validation loss 0.04685702174901962 Accuracy 0.5299999713897705\n",
      "Iteration 7820 Training loss 0.047655075788497925 Validation loss 0.04650354012846947 Accuracy 0.5335999727249146\n",
      "Iteration 7830 Training loss 0.04701266437768936 Validation loss 0.046036772429943085 Accuracy 0.5378000140190125\n",
      "Iteration 7840 Training loss 0.04755650833249092 Validation loss 0.04647623002529144 Accuracy 0.5339000225067139\n",
      "Iteration 7850 Training loss 0.045741379261016846 Validation loss 0.04685164988040924 Accuracy 0.5303999781608582\n",
      "Iteration 7860 Training loss 0.04745922237634659 Validation loss 0.04627776890993118 Accuracy 0.5357000231742859\n",
      "Iteration 7870 Training loss 0.043923269957304 Validation loss 0.04621679708361626 Accuracy 0.5367000102996826\n",
      "Iteration 7880 Training loss 0.04474278911948204 Validation loss 0.04662372171878815 Accuracy 0.5325000286102295\n",
      "Iteration 7890 Training loss 0.0430910661816597 Validation loss 0.04631202667951584 Accuracy 0.535099983215332\n",
      "Iteration 7900 Training loss 0.04519575461745262 Validation loss 0.046179842203855515 Accuracy 0.5367000102996826\n",
      "Iteration 7910 Training loss 0.042933642864227295 Validation loss 0.04610584303736687 Accuracy 0.5376999974250793\n",
      "Iteration 7920 Training loss 0.04376430809497833 Validation loss 0.046135444194078445 Accuracy 0.5375999808311462\n",
      "Iteration 7930 Training loss 0.0422360859811306 Validation loss 0.04647733271121979 Accuracy 0.5340999960899353\n",
      "Iteration 7940 Training loss 0.044302307069301605 Validation loss 0.04673193395137787 Accuracy 0.531000018119812\n",
      "Iteration 7950 Training loss 0.042750731110572815 Validation loss 0.04601217806339264 Accuracy 0.5385000109672546\n",
      "Iteration 7960 Training loss 0.044436015188694 Validation loss 0.046425286680459976 Accuracy 0.5339000225067139\n",
      "Iteration 7970 Training loss 0.04665103554725647 Validation loss 0.0464334711432457 Accuracy 0.5335999727249146\n",
      "Iteration 7980 Training loss 0.04718659445643425 Validation loss 0.04766528308391571 Accuracy 0.521399974822998\n",
      "Iteration 7990 Training loss 0.0435403548181057 Validation loss 0.046311698853969574 Accuracy 0.535099983215332\n",
      "Iteration 8000 Training loss 0.04718172177672386 Validation loss 0.04612381383776665 Accuracy 0.5371000170707703\n",
      "Iteration 8010 Training loss 0.046883098781108856 Validation loss 0.04608569294214249 Accuracy 0.5372999906539917\n",
      "Iteration 8020 Training loss 0.04253125935792923 Validation loss 0.046294815838336945 Accuracy 0.5346999764442444\n",
      "Iteration 8030 Training loss 0.0443110466003418 Validation loss 0.04640838876366615 Accuracy 0.5342000126838684\n",
      "Iteration 8040 Training loss 0.047135788947343826 Validation loss 0.04597064480185509 Accuracy 0.5385000109672546\n",
      "Iteration 8050 Training loss 0.04704996198415756 Validation loss 0.04614641144871712 Accuracy 0.5364999771118164\n",
      "Iteration 8060 Training loss 0.04753660410642624 Validation loss 0.04603103548288345 Accuracy 0.5378000140190125\n",
      "Iteration 8070 Training loss 0.0423005037009716 Validation loss 0.04624480381608009 Accuracy 0.5358999967575073\n",
      "Iteration 8080 Training loss 0.04749713093042374 Validation loss 0.04618941247463226 Accuracy 0.5367000102996826\n",
      "Iteration 8090 Training loss 0.047041233628988266 Validation loss 0.04602676257491112 Accuracy 0.5379999876022339\n",
      "Iteration 8100 Training loss 0.040890175849199295 Validation loss 0.04646549001336098 Accuracy 0.5333999991416931\n",
      "Iteration 8110 Training loss 0.051694225519895554 Validation loss 0.0464462973177433 Accuracy 0.53329998254776\n",
      "Iteration 8120 Training loss 0.043012749403715134 Validation loss 0.04609011113643646 Accuracy 0.5370000004768372\n",
      "Iteration 8130 Training loss 0.04544089362025261 Validation loss 0.04600399732589722 Accuracy 0.538100004196167\n",
      "Iteration 8140 Training loss 0.04502389580011368 Validation loss 0.04615578427910805 Accuracy 0.536300003528595\n",
      "Iteration 8150 Training loss 0.049386750906705856 Validation loss 0.045794323086738586 Accuracy 0.5407999753952026\n",
      "Iteration 8160 Training loss 0.0435209646821022 Validation loss 0.04294903203845024 Accuracy 0.5684999823570251\n",
      "Iteration 8170 Training loss 0.03789035603404045 Validation loss 0.040053997188806534 Accuracy 0.5975000262260437\n",
      "Iteration 8180 Training loss 0.038193173706531525 Validation loss 0.040140606462955475 Accuracy 0.5968999862670898\n",
      "Iteration 8190 Training loss 0.03834469988942146 Validation loss 0.038474034518003464 Accuracy 0.6132000088691711\n",
      "Iteration 8200 Training loss 0.03644877299666405 Validation loss 0.03825480118393898 Accuracy 0.6154999732971191\n",
      "Iteration 8210 Training loss 0.03672447055578232 Validation loss 0.0380200631916523 Accuracy 0.6179999709129333\n",
      "Iteration 8220 Training loss 0.03607459366321564 Validation loss 0.03762521594762802 Accuracy 0.6222000122070312\n",
      "Iteration 8230 Training loss 0.038814887404441833 Validation loss 0.037681739777326584 Accuracy 0.6215000152587891\n",
      "Iteration 8240 Training loss 0.037382885813713074 Validation loss 0.03859899565577507 Accuracy 0.6121000051498413\n",
      "Iteration 8250 Training loss 0.038039837032556534 Validation loss 0.037796832621097565 Accuracy 0.6207000017166138\n",
      "Iteration 8260 Training loss 0.03712061420083046 Validation loss 0.03781808540225029 Accuracy 0.6202999949455261\n",
      "Iteration 8270 Training loss 0.03432387113571167 Validation loss 0.037767235189676285 Accuracy 0.620199978351593\n",
      "Iteration 8280 Training loss 0.03589422255754471 Validation loss 0.037652235478162766 Accuracy 0.621399998664856\n",
      "Iteration 8290 Training loss 0.03548049181699753 Validation loss 0.0375760979950428 Accuracy 0.6226000189781189\n",
      "Iteration 8300 Training loss 0.035625141113996506 Validation loss 0.037415552884340286 Accuracy 0.6244999766349792\n",
      "Iteration 8310 Training loss 0.03584347665309906 Validation loss 0.037429433315992355 Accuracy 0.6243000030517578\n",
      "Iteration 8320 Training loss 0.03768165037035942 Validation loss 0.03755656257271767 Accuracy 0.6226999759674072\n",
      "Iteration 8330 Training loss 0.03537854924798012 Validation loss 0.03776971995830536 Accuracy 0.6207000017166138\n",
      "Iteration 8340 Training loss 0.03464110940694809 Validation loss 0.03730921447277069 Accuracy 0.6255000233650208\n",
      "Iteration 8350 Training loss 0.03766864538192749 Validation loss 0.03879089280962944 Accuracy 0.61080002784729\n",
      "Iteration 8360 Training loss 0.03771834447979927 Validation loss 0.037479467689991 Accuracy 0.6238999962806702\n",
      "Iteration 8370 Training loss 0.03700866550207138 Validation loss 0.0372636616230011 Accuracy 0.6256999969482422\n",
      "Iteration 8380 Training loss 0.033971309661865234 Validation loss 0.0374288484454155 Accuracy 0.6233999729156494\n",
      "Iteration 8390 Training loss 0.03549012169241905 Validation loss 0.03719533607363701 Accuracy 0.6265000104904175\n",
      "Iteration 8400 Training loss 0.03573641926050186 Validation loss 0.036927059292793274 Accuracy 0.6291000247001648\n",
      "Iteration 8410 Training loss 0.032853931188583374 Validation loss 0.03704030439257622 Accuracy 0.628000020980835\n",
      "Iteration 8420 Training loss 0.03516552224755287 Validation loss 0.037162840366363525 Accuracy 0.6269999742507935\n",
      "Iteration 8430 Training loss 0.03195628523826599 Validation loss 0.03708720579743385 Accuracy 0.6279000043869019\n",
      "Iteration 8440 Training loss 0.037411559373140335 Validation loss 0.03696170449256897 Accuracy 0.6288999915122986\n",
      "Iteration 8450 Training loss 0.03618606925010681 Validation loss 0.037510115653276443 Accuracy 0.6237000226974487\n",
      "Iteration 8460 Training loss 0.03550625219941139 Validation loss 0.037134092301130295 Accuracy 0.6276999711990356\n",
      "Iteration 8470 Training loss 0.03656207397580147 Validation loss 0.03692448511719704 Accuracy 0.6291000247001648\n",
      "Iteration 8480 Training loss 0.03502131253480911 Validation loss 0.03743596374988556 Accuracy 0.6247000098228455\n",
      "Iteration 8490 Training loss 0.03766458109021187 Validation loss 0.03722378984093666 Accuracy 0.6266000270843506\n",
      "Iteration 8500 Training loss 0.035275302827358246 Validation loss 0.03718045726418495 Accuracy 0.6272000074386597\n",
      "Iteration 8510 Training loss 0.03670873865485191 Validation loss 0.037180669605731964 Accuracy 0.6273999810218811\n",
      "Iteration 8520 Training loss 0.032576821744441986 Validation loss 0.03708032891154289 Accuracy 0.6281999945640564\n",
      "Iteration 8530 Training loss 0.03931443393230438 Validation loss 0.03741271793842316 Accuracy 0.6247000098228455\n",
      "Iteration 8540 Training loss 0.03689219057559967 Validation loss 0.03723893687129021 Accuracy 0.6259999871253967\n",
      "Iteration 8550 Training loss 0.034746721386909485 Validation loss 0.037567801773548126 Accuracy 0.6233000159263611\n",
      "Iteration 8560 Training loss 0.03715505823493004 Validation loss 0.03699334338307381 Accuracy 0.6288999915122986\n",
      "Iteration 8570 Training loss 0.03827141225337982 Validation loss 0.038099996745586395 Accuracy 0.6179999709129333\n",
      "Iteration 8580 Training loss 0.03574410825967789 Validation loss 0.03706520423293114 Accuracy 0.6277999877929688\n",
      "Iteration 8590 Training loss 0.03873158618807793 Validation loss 0.03742387518286705 Accuracy 0.6233999729156494\n",
      "Iteration 8600 Training loss 0.033643174916505814 Validation loss 0.03710438311100006 Accuracy 0.6276000142097473\n",
      "Iteration 8610 Training loss 0.0343967042863369 Validation loss 0.037136610597372055 Accuracy 0.6270999908447266\n",
      "Iteration 8620 Training loss 0.03456258773803711 Validation loss 0.03709368407726288 Accuracy 0.6279000043869019\n",
      "Iteration 8630 Training loss 0.03394836187362671 Validation loss 0.03700549900531769 Accuracy 0.6284000277519226\n",
      "Iteration 8640 Training loss 0.034877847880125046 Validation loss 0.036855876445770264 Accuracy 0.6302000284194946\n",
      "Iteration 8650 Training loss 0.032942190766334534 Validation loss 0.03703618049621582 Accuracy 0.6280999779701233\n",
      "Iteration 8660 Training loss 0.037890683859586716 Validation loss 0.03705456480383873 Accuracy 0.6269999742507935\n",
      "Iteration 8670 Training loss 0.03499455749988556 Validation loss 0.03701362386345863 Accuracy 0.6280999779701233\n",
      "Iteration 8680 Training loss 0.03536861017346382 Validation loss 0.0371580608189106 Accuracy 0.6265000104904175\n",
      "Iteration 8690 Training loss 0.0343126580119133 Validation loss 0.03697787970304489 Accuracy 0.6291999816894531\n",
      "Iteration 8700 Training loss 0.03643530607223511 Validation loss 0.039376236498355865 Accuracy 0.6047000288963318\n",
      "Iteration 8710 Training loss 0.035101450979709625 Validation loss 0.037008266896009445 Accuracy 0.6281999945640564\n",
      "Iteration 8720 Training loss 0.03442462533712387 Validation loss 0.03684011846780777 Accuracy 0.6297000050544739\n",
      "Iteration 8730 Training loss 0.03549528121948242 Validation loss 0.03695541247725487 Accuracy 0.628600001335144\n",
      "Iteration 8740 Training loss 0.034055501222610474 Validation loss 0.0368884839117527 Accuracy 0.6292999982833862\n",
      "Iteration 8750 Training loss 0.035982511937618256 Validation loss 0.03685030713677406 Accuracy 0.630299985408783\n",
      "Iteration 8760 Training loss 0.03855926916003227 Validation loss 0.03744298592209816 Accuracy 0.6241000294685364\n",
      "Iteration 8770 Training loss 0.03613383695483208 Validation loss 0.03792261332273483 Accuracy 0.6190999746322632\n",
      "Iteration 8780 Training loss 0.03357761725783348 Validation loss 0.03721464052796364 Accuracy 0.6263999938964844\n",
      "Iteration 8790 Training loss 0.03615709766745567 Validation loss 0.0368378721177578 Accuracy 0.630299985408783\n",
      "Iteration 8800 Training loss 0.03769697993993759 Validation loss 0.036834388971328735 Accuracy 0.6306999921798706\n",
      "Iteration 8810 Training loss 0.03559684753417969 Validation loss 0.03715454041957855 Accuracy 0.6273000240325928\n",
      "Iteration 8820 Training loss 0.03256141021847725 Validation loss 0.03780147805809975 Accuracy 0.6207000017166138\n",
      "Iteration 8830 Training loss 0.03548683971166611 Validation loss 0.0367722287774086 Accuracy 0.6312999725341797\n",
      "Iteration 8840 Training loss 0.035264577716588974 Validation loss 0.03684678301215172 Accuracy 0.6301000118255615\n",
      "Iteration 8850 Training loss 0.037431471049785614 Validation loss 0.036749325692653656 Accuracy 0.6310999989509583\n",
      "Iteration 8860 Training loss 0.037190236151218414 Validation loss 0.03707941249012947 Accuracy 0.6283000111579895\n",
      "Iteration 8870 Training loss 0.03667939826846123 Validation loss 0.03671995922923088 Accuracy 0.6309999823570251\n",
      "Iteration 8880 Training loss 0.0362592414021492 Validation loss 0.03721786290407181 Accuracy 0.6263999938964844\n",
      "Iteration 8890 Training loss 0.036123473197221756 Validation loss 0.036929283291101456 Accuracy 0.6291999816894531\n",
      "Iteration 8900 Training loss 0.03424029052257538 Validation loss 0.03661163151264191 Accuracy 0.6326000094413757\n",
      "Iteration 8910 Training loss 0.03821936249732971 Validation loss 0.03686688840389252 Accuracy 0.629800021648407\n",
      "Iteration 8920 Training loss 0.033459730446338654 Validation loss 0.03688167780637741 Accuracy 0.6295999884605408\n",
      "Iteration 8930 Training loss 0.03328365460038185 Validation loss 0.036578137427568436 Accuracy 0.6323999762535095\n",
      "Iteration 8940 Training loss 0.03721360117197037 Validation loss 0.036671824753284454 Accuracy 0.6316999793052673\n",
      "Iteration 8950 Training loss 0.03557264432311058 Validation loss 0.03699775040149689 Accuracy 0.6287000179290771\n",
      "Iteration 8960 Training loss 0.03871454298496246 Validation loss 0.036835167557001114 Accuracy 0.6298999786376953\n",
      "Iteration 8970 Training loss 0.03972608968615532 Validation loss 0.03685909882187843 Accuracy 0.6294999718666077\n",
      "Iteration 8980 Training loss 0.039568863809108734 Validation loss 0.03692186623811722 Accuracy 0.6291999816894531\n",
      "Iteration 8990 Training loss 0.04017951339483261 Validation loss 0.03731893002986908 Accuracy 0.6252999901771545\n",
      "Iteration 9000 Training loss 0.035970814526081085 Validation loss 0.03685736283659935 Accuracy 0.630299985408783\n",
      "Iteration 9010 Training loss 0.038438327610492706 Validation loss 0.036912377923727036 Accuracy 0.6295999884605408\n",
      "Iteration 9020 Training loss 0.038987789303064346 Validation loss 0.03709038719534874 Accuracy 0.6277999877929688\n",
      "Iteration 9030 Training loss 0.037574540823698044 Validation loss 0.037245333194732666 Accuracy 0.6259999871253967\n",
      "Iteration 9040 Training loss 0.03487544134259224 Validation loss 0.03667863830924034 Accuracy 0.6320000290870667\n",
      "Iteration 9050 Training loss 0.03827495872974396 Validation loss 0.036778055131435394 Accuracy 0.630299985408783\n",
      "Iteration 9060 Training loss 0.035022057592868805 Validation loss 0.037233609706163406 Accuracy 0.6259999871253967\n",
      "Iteration 9070 Training loss 0.03701091185212135 Validation loss 0.03694048896431923 Accuracy 0.6287999749183655\n",
      "Iteration 9080 Training loss 0.03673282638192177 Validation loss 0.036706406623125076 Accuracy 0.6312000155448914\n",
      "Iteration 9090 Training loss 0.030980778858065605 Validation loss 0.037770334631204605 Accuracy 0.6198999881744385\n",
      "Iteration 9100 Training loss 0.033268533647060394 Validation loss 0.036687061190605164 Accuracy 0.6312999725341797\n",
      "Iteration 9110 Training loss 0.03505206108093262 Validation loss 0.03717447817325592 Accuracy 0.6261000037193298\n",
      "Iteration 9120 Training loss 0.034117378294467926 Validation loss 0.037148673087358475 Accuracy 0.6265000104904175\n",
      "Iteration 9130 Training loss 0.034645356237888336 Validation loss 0.03729995712637901 Accuracy 0.6245999932289124\n",
      "Iteration 9140 Training loss 0.03555309772491455 Validation loss 0.037150751799345016 Accuracy 0.626800000667572\n",
      "Iteration 9150 Training loss 0.037368278950452805 Validation loss 0.0376678891479969 Accuracy 0.621399998664856\n",
      "Iteration 9160 Training loss 0.03255622461438179 Validation loss 0.03676043078303337 Accuracy 0.6306999921798706\n",
      "Iteration 9170 Training loss 0.03233268857002258 Validation loss 0.03685404360294342 Accuracy 0.629800021648407\n",
      "Iteration 9180 Training loss 0.03594223037362099 Validation loss 0.037333689630031586 Accuracy 0.6241999864578247\n",
      "Iteration 9190 Training loss 0.036894965916872025 Validation loss 0.03697522357106209 Accuracy 0.6287999749183655\n",
      "Iteration 9200 Training loss 0.03426816686987877 Validation loss 0.037014394998550415 Accuracy 0.6277999877929688\n",
      "Iteration 9210 Training loss 0.03410248085856438 Validation loss 0.03743430972099304 Accuracy 0.6233999729156494\n",
      "Iteration 9220 Training loss 0.033424269407987595 Validation loss 0.03733086585998535 Accuracy 0.6243000030517578\n",
      "Iteration 9230 Training loss 0.03774295002222061 Validation loss 0.03677057474851608 Accuracy 0.6309000253677368\n",
      "Iteration 9240 Training loss 0.037522345781326294 Validation loss 0.037668146193027496 Accuracy 0.6218000054359436\n",
      "Iteration 9250 Training loss 0.03302013501524925 Validation loss 0.03714429959654808 Accuracy 0.6269999742507935\n",
      "Iteration 9260 Training loss 0.035047419369220734 Validation loss 0.03730381652712822 Accuracy 0.6255000233650208\n",
      "Iteration 9270 Training loss 0.03587045148015022 Validation loss 0.03675370663404465 Accuracy 0.6309999823570251\n",
      "Iteration 9280 Training loss 0.033887557685375214 Validation loss 0.03658502548933029 Accuracy 0.6327000260353088\n",
      "Iteration 9290 Training loss 0.03625393286347389 Validation loss 0.036576997488737106 Accuracy 0.6323000192642212\n",
      "Iteration 9300 Training loss 0.034703727811574936 Validation loss 0.0367097407579422 Accuracy 0.6312000155448914\n",
      "Iteration 9310 Training loss 0.03281427174806595 Validation loss 0.037679996341466904 Accuracy 0.6215000152587891\n",
      "Iteration 9320 Training loss 0.03513698652386665 Validation loss 0.03675921633839607 Accuracy 0.6305000185966492\n",
      "Iteration 9330 Training loss 0.03246866539120674 Validation loss 0.03659365326166153 Accuracy 0.6327999830245972\n",
      "Iteration 9340 Training loss 0.03363125026226044 Validation loss 0.03647966682910919 Accuracy 0.6337000131607056\n",
      "Iteration 9350 Training loss 0.03936130926012993 Validation loss 0.03748474642634392 Accuracy 0.6237000226974487\n",
      "Iteration 9360 Training loss 0.03777770325541496 Validation loss 0.037226080894470215 Accuracy 0.6251999735832214\n",
      "Iteration 9370 Training loss 0.033593595027923584 Validation loss 0.03656608238816261 Accuracy 0.6328999996185303\n",
      "Iteration 9380 Training loss 0.036637794226408005 Validation loss 0.03868242725729942 Accuracy 0.6107000112533569\n",
      "Iteration 9390 Training loss 0.03561446815729141 Validation loss 0.03710715100169182 Accuracy 0.6273000240325928\n",
      "Iteration 9400 Training loss 0.03520825132727623 Validation loss 0.036751341074705124 Accuracy 0.6309999823570251\n",
      "Iteration 9410 Training loss 0.033814068883657455 Validation loss 0.03640832006931305 Accuracy 0.6345000267028809\n",
      "Iteration 9420 Training loss 0.03351178765296936 Validation loss 0.03671583533287048 Accuracy 0.6312000155448914\n",
      "Iteration 9430 Training loss 0.03255576267838478 Validation loss 0.03663431480526924 Accuracy 0.6323999762535095\n",
      "Iteration 9440 Training loss 0.03215719014406204 Validation loss 0.03649577125906944 Accuracy 0.6337000131607056\n",
      "Iteration 9450 Training loss 0.03538025915622711 Validation loss 0.03652309253811836 Accuracy 0.6331999897956848\n",
      "Iteration 9460 Training loss 0.034895092248916626 Validation loss 0.03658425435423851 Accuracy 0.6327999830245972\n",
      "Iteration 9470 Training loss 0.03185347467660904 Validation loss 0.0363573357462883 Accuracy 0.6353999972343445\n",
      "Iteration 9480 Training loss 0.03416157141327858 Validation loss 0.0365479476749897 Accuracy 0.6328999996185303\n",
      "Iteration 9490 Training loss 0.033970028162002563 Validation loss 0.03651007264852524 Accuracy 0.6337000131607056\n",
      "Iteration 9500 Training loss 0.03412517160177231 Validation loss 0.03642876446247101 Accuracy 0.6337000131607056\n",
      "Iteration 9510 Training loss 0.037718337029218674 Validation loss 0.03691922500729561 Accuracy 0.6291999816894531\n",
      "Iteration 9520 Training loss 0.03643409162759781 Validation loss 0.03721405938267708 Accuracy 0.6262999773025513\n",
      "Iteration 9530 Training loss 0.03720306232571602 Validation loss 0.03717571869492531 Accuracy 0.6266000270843506\n",
      "Iteration 9540 Training loss 0.037717871367931366 Validation loss 0.036662399768829346 Accuracy 0.6308000087738037\n",
      "Iteration 9550 Training loss 0.036242350935935974 Validation loss 0.036805037409067154 Accuracy 0.6302000284194946\n",
      "Iteration 9560 Training loss 0.03759647160768509 Validation loss 0.03713291510939598 Accuracy 0.6263999938964844\n",
      "Iteration 9570 Training loss 0.0344565287232399 Validation loss 0.0366063266992569 Accuracy 0.6320000290870667\n",
      "Iteration 9580 Training loss 0.033191610127687454 Validation loss 0.036508020013570786 Accuracy 0.6327999830245972\n",
      "Iteration 9590 Training loss 0.034981220960617065 Validation loss 0.03637177497148514 Accuracy 0.6340000033378601\n",
      "Iteration 9600 Training loss 0.0324314720928669 Validation loss 0.03718934208154678 Accuracy 0.6259999871253967\n",
      "Iteration 9610 Training loss 0.035583946853876114 Validation loss 0.03683455288410187 Accuracy 0.6297000050544739\n",
      "Iteration 9620 Training loss 0.03189895674586296 Validation loss 0.03684643656015396 Accuracy 0.6294000148773193\n",
      "Iteration 9630 Training loss 0.03514568880200386 Validation loss 0.03658939525485039 Accuracy 0.6323000192642212\n",
      "Iteration 9640 Training loss 0.034992534667253494 Validation loss 0.03658881038427353 Accuracy 0.6326000094413757\n",
      "Iteration 9650 Training loss 0.035092685371637344 Validation loss 0.03724811226129532 Accuracy 0.6252999901771545\n",
      "Iteration 9660 Training loss 0.03450127691030502 Validation loss 0.036609407514333725 Accuracy 0.6319000124931335\n",
      "Iteration 9670 Training loss 0.03173595666885376 Validation loss 0.036846261471509933 Accuracy 0.6298999786376953\n",
      "Iteration 9680 Training loss 0.032352834939956665 Validation loss 0.03640830144286156 Accuracy 0.6334999799728394\n",
      "Iteration 9690 Training loss 0.034063853323459625 Validation loss 0.03667803853750229 Accuracy 0.6310999989509583\n",
      "Iteration 9700 Training loss 0.035698290914297104 Validation loss 0.03647685423493385 Accuracy 0.6335999965667725\n",
      "Iteration 9710 Training loss 0.0351412333548069 Validation loss 0.0363541804254055 Accuracy 0.6352999806404114\n",
      "Iteration 9720 Training loss 0.03660118579864502 Validation loss 0.0367533341050148 Accuracy 0.6315000057220459\n",
      "Iteration 9730 Training loss 0.03494543954730034 Validation loss 0.03728549927473068 Accuracy 0.6258000135421753\n",
      "Iteration 9740 Training loss 0.03490433841943741 Validation loss 0.036761537194252014 Accuracy 0.6309000253677368\n",
      "Iteration 9750 Training loss 0.03483687713742256 Validation loss 0.03642585501074791 Accuracy 0.6344000101089478\n",
      "Iteration 9760 Training loss 0.033649858087301254 Validation loss 0.036246638745069504 Accuracy 0.6359000205993652\n",
      "Iteration 9770 Training loss 0.036757692694664 Validation loss 0.03696209564805031 Accuracy 0.6287999749183655\n",
      "Iteration 9780 Training loss 0.03862326964735985 Validation loss 0.03671295568346977 Accuracy 0.6310999989509583\n",
      "Iteration 9790 Training loss 0.03893402963876724 Validation loss 0.03636329248547554 Accuracy 0.6341000199317932\n",
      "Iteration 9800 Training loss 0.0366818904876709 Validation loss 0.036371055990457535 Accuracy 0.6341999769210815\n",
      "Iteration 9810 Training loss 0.03262344375252724 Validation loss 0.03678496927022934 Accuracy 0.6299999952316284\n",
      "Iteration 9820 Training loss 0.03664741665124893 Validation loss 0.03666951507329941 Accuracy 0.6313999891281128\n",
      "Iteration 9830 Training loss 0.03157266229391098 Validation loss 0.03637721762061119 Accuracy 0.6345999836921692\n",
      "Iteration 9840 Training loss 0.03136490657925606 Validation loss 0.036738861352205276 Accuracy 0.6308000087738037\n",
      "Iteration 9850 Training loss 0.03501966595649719 Validation loss 0.0365406796336174 Accuracy 0.6330999732017517\n",
      "Iteration 9860 Training loss 0.03575076907873154 Validation loss 0.03655471280217171 Accuracy 0.6327999830245972\n",
      "Iteration 9870 Training loss 0.03393206745386124 Validation loss 0.03688748553395271 Accuracy 0.6290000081062317\n",
      "Iteration 9880 Training loss 0.035184022039175034 Validation loss 0.036502398550510406 Accuracy 0.633400022983551\n",
      "Iteration 9890 Training loss 0.03510419279336929 Validation loss 0.0366513654589653 Accuracy 0.631600022315979\n",
      "Iteration 9900 Training loss 0.03647594153881073 Validation loss 0.03681008145213127 Accuracy 0.6301000118255615\n",
      "Iteration 9910 Training loss 0.03667454048991203 Validation loss 0.03683573752641678 Accuracy 0.6301000118255615\n",
      "Iteration 9920 Training loss 0.03596649691462517 Validation loss 0.03658328950405121 Accuracy 0.6326000094413757\n",
      "Iteration 9930 Training loss 0.03829341009259224 Validation loss 0.03679962828755379 Accuracy 0.6298999786376953\n",
      "Iteration 9940 Training loss 0.0383804589509964 Validation loss 0.03647833317518234 Accuracy 0.6335999965667725\n",
      "Iteration 9950 Training loss 0.037564679980278015 Validation loss 0.036489419639110565 Accuracy 0.6328999996185303\n",
      "Iteration 9960 Training loss 0.03700469806790352 Validation loss 0.037048839032649994 Accuracy 0.6276000142097473\n",
      "Iteration 9970 Training loss 0.03487790375947952 Validation loss 0.03637206181883812 Accuracy 0.635200023651123\n",
      "Iteration 9980 Training loss 0.033649615943431854 Validation loss 0.03660868480801582 Accuracy 0.6330999732017517\n",
      "Iteration 9990 Training loss 0.03715752810239792 Validation loss 0.03660785034298897 Accuracy 0.6330000162124634\n",
      "Iteration 10000 Training loss 0.03706943243741989 Validation loss 0.037277061492204666 Accuracy 0.6254000067710876\n",
      "Iteration 10010 Training loss 0.0315408892929554 Validation loss 0.03672463074326515 Accuracy 0.6309000253677368\n",
      "Iteration 10020 Training loss 0.03553234785795212 Validation loss 0.03675632178783417 Accuracy 0.6305999755859375\n",
      "Iteration 10030 Training loss 0.034283384680747986 Validation loss 0.03637511655688286 Accuracy 0.6348999738693237\n",
      "Iteration 10040 Training loss 0.035388801246881485 Validation loss 0.03664886951446533 Accuracy 0.6323999762535095\n",
      "Iteration 10050 Training loss 0.03683862462639809 Validation loss 0.03656889870762825 Accuracy 0.6323000192642212\n",
      "Iteration 10060 Training loss 0.041252266615629196 Validation loss 0.03688787668943405 Accuracy 0.6292999982833862\n",
      "Iteration 10070 Training loss 0.028088675811886787 Validation loss 0.03639993071556091 Accuracy 0.6348000168800354\n",
      "Iteration 10080 Training loss 0.036199599504470825 Validation loss 0.037815019488334656 Accuracy 0.6200000047683716\n",
      "Iteration 10090 Training loss 0.034786924719810486 Validation loss 0.036682918667793274 Accuracy 0.6308000087738037\n",
      "Iteration 10100 Training loss 0.034058745950460434 Validation loss 0.03677878528833389 Accuracy 0.6299999952316284\n",
      "Iteration 10110 Training loss 0.03783397749066353 Validation loss 0.0376657098531723 Accuracy 0.6208999752998352\n",
      "Iteration 10120 Training loss 0.03347400948405266 Validation loss 0.036646757274866104 Accuracy 0.6320000290870667\n",
      "Iteration 10130 Training loss 0.039536748081445694 Validation loss 0.03658302128314972 Accuracy 0.632099986076355\n",
      "Iteration 10140 Training loss 0.03462643548846245 Validation loss 0.03680109232664108 Accuracy 0.630299985408783\n",
      "Iteration 10150 Training loss 0.03568008914589882 Validation loss 0.036413948982954025 Accuracy 0.6334999799728394\n",
      "Iteration 10160 Training loss 0.0353783555328846 Validation loss 0.03647235780954361 Accuracy 0.6333000063896179\n",
      "Iteration 10170 Training loss 0.03452208638191223 Validation loss 0.036466434597969055 Accuracy 0.6333000063896179\n",
      "Iteration 10180 Training loss 0.030684160068631172 Validation loss 0.036567576229572296 Accuracy 0.6322000026702881\n",
      "Iteration 10190 Training loss 0.036186568439006805 Validation loss 0.03641634061932564 Accuracy 0.633899986743927\n",
      "Iteration 10200 Training loss 0.03623879328370094 Validation loss 0.036244023591279984 Accuracy 0.6352999806404114\n",
      "Iteration 10210 Training loss 0.03582169488072395 Validation loss 0.03640289232134819 Accuracy 0.6342999935150146\n",
      "Iteration 10220 Training loss 0.032553780823946 Validation loss 0.03643697127699852 Accuracy 0.6341999769210815\n",
      "Iteration 10230 Training loss 0.037070807069540024 Validation loss 0.03650739789009094 Accuracy 0.633400022983551\n",
      "Iteration 10240 Training loss 0.035528797656297684 Validation loss 0.036740031093358994 Accuracy 0.6309999823570251\n",
      "Iteration 10250 Training loss 0.03625679016113281 Validation loss 0.03713752329349518 Accuracy 0.626800000667572\n",
      "Iteration 10260 Training loss 0.034278593957424164 Validation loss 0.03645111247897148 Accuracy 0.6342999935150146\n",
      "Iteration 10270 Training loss 0.035645492374897 Validation loss 0.03686166927218437 Accuracy 0.6302000284194946\n",
      "Iteration 10280 Training loss 0.03303941339254379 Validation loss 0.03656521067023277 Accuracy 0.6330999732017517\n",
      "Iteration 10290 Training loss 0.03455458953976631 Validation loss 0.03662886098027229 Accuracy 0.6317999958992004\n",
      "Iteration 10300 Training loss 0.03300020843744278 Validation loss 0.03721321374177933 Accuracy 0.6262999773025513\n",
      "Iteration 10310 Training loss 0.036803364753723145 Validation loss 0.03744872659444809 Accuracy 0.6236000061035156\n",
      "Iteration 10320 Training loss 0.03411649167537689 Validation loss 0.03643398359417915 Accuracy 0.6341999769210815\n",
      "Iteration 10330 Training loss 0.03702743351459503 Validation loss 0.036592718213796616 Accuracy 0.6326000094413757\n",
      "Iteration 10340 Training loss 0.034489817917346954 Validation loss 0.03640440106391907 Accuracy 0.6345000267028809\n",
      "Iteration 10350 Training loss 0.03840072453022003 Validation loss 0.03702205419540405 Accuracy 0.6280999779701233\n",
      "Iteration 10360 Training loss 0.035907793790102005 Validation loss 0.03647250682115555 Accuracy 0.6335999965667725\n",
      "Iteration 10370 Training loss 0.03592580556869507 Validation loss 0.03641873598098755 Accuracy 0.6347000002861023\n",
      "Iteration 10380 Training loss 0.034590013325214386 Validation loss 0.036821138113737106 Accuracy 0.6306999921798706\n",
      "Iteration 10390 Training loss 0.0358925424516201 Validation loss 0.036393146961927414 Accuracy 0.6348999738693237\n",
      "Iteration 10400 Training loss 0.034587062895298004 Validation loss 0.036400388926267624 Accuracy 0.6348000168800354\n",
      "Iteration 10410 Training loss 0.03681277483701706 Validation loss 0.036419760435819626 Accuracy 0.6342999935150146\n",
      "Iteration 10420 Training loss 0.03700939565896988 Validation loss 0.03638889640569687 Accuracy 0.6347000002861023\n",
      "Iteration 10430 Training loss 0.03608495742082596 Validation loss 0.0362001471221447 Accuracy 0.6362000107765198\n",
      "Iteration 10440 Training loss 0.03489379212260246 Validation loss 0.03632451221346855 Accuracy 0.6347000002861023\n",
      "Iteration 10450 Training loss 0.03606265410780907 Validation loss 0.03724600747227669 Accuracy 0.6255999803543091\n",
      "Iteration 10460 Training loss 0.03571070730686188 Validation loss 0.03654249757528305 Accuracy 0.6328999996185303\n",
      "Iteration 10470 Training loss 0.03563505411148071 Validation loss 0.03673173487186432 Accuracy 0.6306999921798706\n",
      "Iteration 10480 Training loss 0.03818085417151451 Validation loss 0.03795892372727394 Accuracy 0.6183000206947327\n",
      "Iteration 10490 Training loss 0.03540395200252533 Validation loss 0.03688276186585426 Accuracy 0.6297000050544739\n",
      "Iteration 10500 Training loss 0.03360748291015625 Validation loss 0.03651842102408409 Accuracy 0.6331999897956848\n",
      "Iteration 10510 Training loss 0.033429667353630066 Validation loss 0.036309339106082916 Accuracy 0.6349999904632568\n",
      "Iteration 10520 Training loss 0.03556869924068451 Validation loss 0.036802299320697784 Accuracy 0.6294999718666077\n",
      "Iteration 10530 Training loss 0.03453677520155907 Validation loss 0.03620561212301254 Accuracy 0.6362000107765198\n",
      "Iteration 10540 Training loss 0.035039015114307404 Validation loss 0.03655223920941353 Accuracy 0.6324999928474426\n",
      "Iteration 10550 Training loss 0.03472064435482025 Validation loss 0.03735004737973213 Accuracy 0.6244000196456909\n",
      "Iteration 10560 Training loss 0.03849087283015251 Validation loss 0.03662078082561493 Accuracy 0.6323000192642212\n",
      "Iteration 10570 Training loss 0.03411395847797394 Validation loss 0.036423180252313614 Accuracy 0.6342999935150146\n",
      "Iteration 10580 Training loss 0.03858928009867668 Validation loss 0.037847183644771576 Accuracy 0.6195999979972839\n",
      "Iteration 10590 Training loss 0.0358169861137867 Validation loss 0.03637589514255524 Accuracy 0.6342999935150146\n",
      "Iteration 10600 Training loss 0.04086689651012421 Validation loss 0.03630886971950531 Accuracy 0.6348999738693237\n",
      "Iteration 10610 Training loss 0.03466746583580971 Validation loss 0.036676011979579926 Accuracy 0.6310999989509583\n",
      "Iteration 10620 Training loss 0.03296201303601265 Validation loss 0.03709477186203003 Accuracy 0.6266999840736389\n",
      "Iteration 10630 Training loss 0.036706868559122086 Validation loss 0.03664647787809372 Accuracy 0.6316999793052673\n",
      "Iteration 10640 Training loss 0.03226205334067345 Validation loss 0.03607766330242157 Accuracy 0.6373999714851379\n",
      "Iteration 10650 Training loss 0.037000883370637894 Validation loss 0.036461807787418365 Accuracy 0.6335999965667725\n",
      "Iteration 10660 Training loss 0.03696515038609505 Validation loss 0.03689449280500412 Accuracy 0.6295999884605408\n",
      "Iteration 10670 Training loss 0.03587689623236656 Validation loss 0.036281831562519073 Accuracy 0.6363000273704529\n",
      "Iteration 10680 Training loss 0.034865256398916245 Validation loss 0.0366387665271759 Accuracy 0.6323999762535095\n",
      "Iteration 10690 Training loss 0.03586827963590622 Validation loss 0.03640339896082878 Accuracy 0.6349999904632568\n",
      "Iteration 10700 Training loss 0.03960432484745979 Validation loss 0.036494944244623184 Accuracy 0.6335999965667725\n",
      "Iteration 10710 Training loss 0.03715908154845238 Validation loss 0.03642720729112625 Accuracy 0.6341000199317932\n",
      "Iteration 10720 Training loss 0.03476479649543762 Validation loss 0.03633832558989525 Accuracy 0.6351000070571899\n",
      "Iteration 10730 Training loss 0.03307287022471428 Validation loss 0.03668695688247681 Accuracy 0.6316999793052673\n",
      "Iteration 10740 Training loss 0.0339418463408947 Validation loss 0.03662732243537903 Accuracy 0.6317999958992004\n",
      "Iteration 10750 Training loss 0.032233890146017075 Validation loss 0.036284346133470535 Accuracy 0.6353999972343445\n",
      "Iteration 10760 Training loss 0.03348202630877495 Validation loss 0.0365738570690155 Accuracy 0.632099986076355\n",
      "Iteration 10770 Training loss 0.03455237299203873 Validation loss 0.03642784804105759 Accuracy 0.6345999836921692\n",
      "Iteration 10780 Training loss 0.03844296187162399 Validation loss 0.03722041845321655 Accuracy 0.6266000270843506\n",
      "Iteration 10790 Training loss 0.03552549332380295 Validation loss 0.03660232573747635 Accuracy 0.6322000026702881\n",
      "Iteration 10800 Training loss 0.03661646321415901 Validation loss 0.03673982992768288 Accuracy 0.6305999755859375\n",
      "Iteration 10810 Training loss 0.03356426581740379 Validation loss 0.036329858005046844 Accuracy 0.6355000138282776\n",
      "Iteration 10820 Training loss 0.03304491937160492 Validation loss 0.036534179002046585 Accuracy 0.6331999897956848\n",
      "Iteration 10830 Training loss 0.03910127282142639 Validation loss 0.036734115332365036 Accuracy 0.6315000057220459\n",
      "Iteration 10840 Training loss 0.03306904807686806 Validation loss 0.036867085844278336 Accuracy 0.6301000118255615\n",
      "Iteration 10850 Training loss 0.0368557870388031 Validation loss 0.036941975355148315 Accuracy 0.6292999982833862\n",
      "Iteration 10860 Training loss 0.03674953430891037 Validation loss 0.03694668412208557 Accuracy 0.6291999816894531\n",
      "Iteration 10870 Training loss 0.03425349295139313 Validation loss 0.03658098354935646 Accuracy 0.6324999928474426\n",
      "Iteration 10880 Training loss 0.03221026435494423 Validation loss 0.036084290593862534 Accuracy 0.6377999782562256\n",
      "Iteration 10890 Training loss 0.035151418298482895 Validation loss 0.03678753599524498 Accuracy 0.6294000148773193\n",
      "Iteration 10900 Training loss 0.03773762658238411 Validation loss 0.0370292104780674 Accuracy 0.6263999938964844\n",
      "Iteration 10910 Training loss 0.0369710698723793 Validation loss 0.036655962467193604 Accuracy 0.6312000155448914\n",
      "Iteration 10920 Training loss 0.03171176463365555 Validation loss 0.037250857800245285 Accuracy 0.6255000233650208\n",
      "Iteration 10930 Training loss 0.03429615870118141 Validation loss 0.03650414198637009 Accuracy 0.633899986743927\n",
      "Iteration 10940 Training loss 0.0352940559387207 Validation loss 0.036305516958236694 Accuracy 0.6355999708175659\n",
      "Iteration 10950 Training loss 0.03313865512609482 Validation loss 0.03623460978269577 Accuracy 0.6358000040054321\n",
      "Iteration 10960 Training loss 0.033082302659749985 Validation loss 0.036253780126571655 Accuracy 0.6363999843597412\n",
      "Iteration 10970 Training loss 0.03897612914443016 Validation loss 0.036460619419813156 Accuracy 0.6338000297546387\n",
      "Iteration 10980 Training loss 0.03444230556488037 Validation loss 0.036347635090351105 Accuracy 0.6344000101089478\n",
      "Iteration 10990 Training loss 0.03743675351142883 Validation loss 0.036342065781354904 Accuracy 0.6347000002861023\n",
      "Iteration 11000 Training loss 0.03828667104244232 Validation loss 0.036607056856155396 Accuracy 0.6310999989509583\n",
      "Iteration 11010 Training loss 0.038742147386074066 Validation loss 0.03652209788560867 Accuracy 0.6338000297546387\n",
      "Iteration 11020 Training loss 0.03514323756098747 Validation loss 0.03660878539085388 Accuracy 0.6324999928474426\n",
      "Iteration 11030 Training loss 0.035544879734516144 Validation loss 0.03656354919075966 Accuracy 0.6330000162124634\n",
      "Iteration 11040 Training loss 0.0353022925555706 Validation loss 0.036615896970033646 Accuracy 0.6324999928474426\n",
      "Iteration 11050 Training loss 0.03639731556177139 Validation loss 0.03627965226769447 Accuracy 0.6360999941825867\n",
      "Iteration 11060 Training loss 0.03677598387002945 Validation loss 0.038181133568286896 Accuracy 0.616599977016449\n",
      "Iteration 11070 Training loss 0.03217107802629471 Validation loss 0.0366210862994194 Accuracy 0.6322000026702881\n",
      "Iteration 11080 Training loss 0.03634199872612953 Validation loss 0.03630264103412628 Accuracy 0.6353999972343445\n",
      "Iteration 11090 Training loss 0.03636705130338669 Validation loss 0.03643731027841568 Accuracy 0.6338000297546387\n",
      "Iteration 11100 Training loss 0.032667748630046844 Validation loss 0.03648742660880089 Accuracy 0.6338000297546387\n",
      "Iteration 11110 Training loss 0.03224611282348633 Validation loss 0.037092480808496475 Accuracy 0.6272000074386597\n",
      "Iteration 11120 Training loss 0.032247159630060196 Validation loss 0.036255817860364914 Accuracy 0.6353999972343445\n",
      "Iteration 11130 Training loss 0.033419497311115265 Validation loss 0.03677862137556076 Accuracy 0.6309999823570251\n",
      "Iteration 11140 Training loss 0.035692762583494186 Validation loss 0.03749379515647888 Accuracy 0.6237999796867371\n",
      "Iteration 11150 Training loss 0.03818308189511299 Validation loss 0.037201397120952606 Accuracy 0.6252999901771545\n",
      "Iteration 11160 Training loss 0.03443707153201103 Validation loss 0.037452731281518936 Accuracy 0.6233000159263611\n",
      "Iteration 11170 Training loss 0.035949066281318665 Validation loss 0.03703409805893898 Accuracy 0.6273000240325928\n",
      "Iteration 11180 Training loss 0.034531109035015106 Validation loss 0.037545137107372284 Accuracy 0.6216999888420105\n",
      "Iteration 11190 Training loss 0.03333859145641327 Validation loss 0.036777347326278687 Accuracy 0.6305999755859375\n",
      "Iteration 11200 Training loss 0.03492743521928787 Validation loss 0.03630708530545235 Accuracy 0.6355999708175659\n",
      "Iteration 11210 Training loss 0.03516548126935959 Validation loss 0.03677460178732872 Accuracy 0.6306999921798706\n",
      "Iteration 11220 Training loss 0.034026190638542175 Validation loss 0.03631283715367317 Accuracy 0.6355999708175659\n",
      "Iteration 11230 Training loss 0.03539577126502991 Validation loss 0.03625278174877167 Accuracy 0.6355000138282776\n",
      "Iteration 11240 Training loss 0.0323384553194046 Validation loss 0.03628239408135414 Accuracy 0.6355000138282776\n",
      "Iteration 11250 Training loss 0.03636624664068222 Validation loss 0.036705728620290756 Accuracy 0.6310999989509583\n",
      "Iteration 11260 Training loss 0.03225674852728844 Validation loss 0.036205366253852844 Accuracy 0.6360999941825867\n",
      "Iteration 11270 Training loss 0.03386308625340462 Validation loss 0.036027904599905014 Accuracy 0.6377000212669373\n",
      "Iteration 11280 Training loss 0.03795978054404259 Validation loss 0.03637295961380005 Accuracy 0.635200023651123\n",
      "Iteration 11290 Training loss 0.034649379551410675 Validation loss 0.03693980351090431 Accuracy 0.6280999779701233\n",
      "Iteration 11300 Training loss 0.03395185247063637 Validation loss 0.03643932193517685 Accuracy 0.6330999732017517\n",
      "Iteration 11310 Training loss 0.03708721697330475 Validation loss 0.03654373064637184 Accuracy 0.6327000260353088\n",
      "Iteration 11320 Training loss 0.03319137915968895 Validation loss 0.03705503046512604 Accuracy 0.6273000240325928\n",
      "Iteration 11330 Training loss 0.03669636696577072 Validation loss 0.03673151507973671 Accuracy 0.6299999952316284\n",
      "Iteration 11340 Training loss 0.03403506428003311 Validation loss 0.036138538271188736 Accuracy 0.6366000175476074\n",
      "Iteration 11350 Training loss 0.03544837236404419 Validation loss 0.036400433629751205 Accuracy 0.6341000199317932\n",
      "Iteration 11360 Training loss 0.03579304739832878 Validation loss 0.036296993494033813 Accuracy 0.6351000070571899\n",
      "Iteration 11370 Training loss 0.035634566098451614 Validation loss 0.03637652471661568 Accuracy 0.6344000101089478\n",
      "Iteration 11380 Training loss 0.03628799319267273 Validation loss 0.036311596632003784 Accuracy 0.6344000101089478\n",
      "Iteration 11390 Training loss 0.03258741274476051 Validation loss 0.036052946001291275 Accuracy 0.6370999813079834\n",
      "Iteration 11400 Training loss 0.032169751822948456 Validation loss 0.036083564162254333 Accuracy 0.6373000144958496\n",
      "Iteration 11410 Training loss 0.03457380831241608 Validation loss 0.03666222095489502 Accuracy 0.6317999958992004\n",
      "Iteration 11420 Training loss 0.03524114564061165 Validation loss 0.03668348118662834 Accuracy 0.6316999793052673\n",
      "Iteration 11430 Training loss 0.036158010363578796 Validation loss 0.036381009966135025 Accuracy 0.6331999897956848\n",
      "Iteration 11440 Training loss 0.03805563971400261 Validation loss 0.03666554391384125 Accuracy 0.631600022315979\n",
      "Iteration 11450 Training loss 0.035901881754398346 Validation loss 0.036974068731069565 Accuracy 0.6284999847412109\n",
      "Iteration 11460 Training loss 0.03186938539147377 Validation loss 0.0367421880364418 Accuracy 0.6309000253677368\n",
      "Iteration 11470 Training loss 0.03391973674297333 Validation loss 0.03636224567890167 Accuracy 0.635200023651123\n",
      "Iteration 11480 Training loss 0.03452350199222565 Validation loss 0.03631901741027832 Accuracy 0.6355000138282776\n",
      "Iteration 11490 Training loss 0.03426029160618782 Validation loss 0.03617576137185097 Accuracy 0.6366999745368958\n",
      "Iteration 11500 Training loss 0.034165237098932266 Validation loss 0.0367288663983345 Accuracy 0.6309999823570251\n",
      "Iteration 11510 Training loss 0.036060627549886703 Validation loss 0.036580439656972885 Accuracy 0.6334999799728394\n",
      "Iteration 11520 Training loss 0.032880883663892746 Validation loss 0.03612704202532768 Accuracy 0.6370999813079834\n",
      "Iteration 11530 Training loss 0.04093679413199425 Validation loss 0.03697258606553078 Accuracy 0.6276000142097473\n",
      "Iteration 11540 Training loss 0.03377147763967514 Validation loss 0.0360158234834671 Accuracy 0.6380000114440918\n",
      "Iteration 11550 Training loss 0.03443038836121559 Validation loss 0.03656039386987686 Accuracy 0.6323999762535095\n",
      "Iteration 11560 Training loss 0.03780002519488335 Validation loss 0.03702278435230255 Accuracy 0.6276999711990356\n",
      "Iteration 11570 Training loss 0.03092600591480732 Validation loss 0.0370454415678978 Accuracy 0.6274999976158142\n",
      "Iteration 11580 Training loss 0.03407102823257446 Validation loss 0.03645482659339905 Accuracy 0.6333000063896179\n",
      "Iteration 11590 Training loss 0.03567374125123024 Validation loss 0.03625742346048355 Accuracy 0.6355000138282776\n",
      "Iteration 11600 Training loss 0.03488444909453392 Validation loss 0.0363483689725399 Accuracy 0.6345999836921692\n",
      "Iteration 11610 Training loss 0.033092860132455826 Validation loss 0.03618304803967476 Accuracy 0.6358000040054321\n",
      "Iteration 11620 Training loss 0.03670944273471832 Validation loss 0.036301787942647934 Accuracy 0.6348000168800354\n",
      "Iteration 11630 Training loss 0.03577084466814995 Validation loss 0.03612687066197395 Accuracy 0.6367999911308289\n",
      "Iteration 11640 Training loss 0.03523413464426994 Validation loss 0.037351273000240326 Accuracy 0.6248000264167786\n",
      "Iteration 11650 Training loss 0.035687416791915894 Validation loss 0.03647904843091965 Accuracy 0.6334999799728394\n",
      "Iteration 11660 Training loss 0.03629589453339577 Validation loss 0.03661756217479706 Accuracy 0.6316999793052673\n",
      "Iteration 11670 Training loss 0.036608804017305374 Validation loss 0.036301255226135254 Accuracy 0.6349999904632568\n",
      "Iteration 11680 Training loss 0.033529266715049744 Validation loss 0.036687325686216354 Accuracy 0.6308000087738037\n",
      "Iteration 11690 Training loss 0.0325465053319931 Validation loss 0.03658467158675194 Accuracy 0.6320000290870667\n",
      "Iteration 11700 Training loss 0.03493545949459076 Validation loss 0.03623358532786369 Accuracy 0.6355999708175659\n",
      "Iteration 11710 Training loss 0.033440228551626205 Validation loss 0.036042794585227966 Accuracy 0.6378999948501587\n",
      "Iteration 11720 Training loss 0.035627737641334534 Validation loss 0.03762923553586006 Accuracy 0.6223000288009644\n",
      "Iteration 11730 Training loss 0.03638473153114319 Validation loss 0.0363929308950901 Accuracy 0.6341000199317932\n",
      "Iteration 11740 Training loss 0.03684799373149872 Validation loss 0.036366209387779236 Accuracy 0.6341000199317932\n",
      "Iteration 11750 Training loss 0.03627348318696022 Validation loss 0.037804607301950455 Accuracy 0.6194000244140625\n",
      "Iteration 11760 Training loss 0.037110332399606705 Validation loss 0.03642139956355095 Accuracy 0.6330999732017517\n",
      "Iteration 11770 Training loss 0.03359215706586838 Validation loss 0.03627339005470276 Accuracy 0.635200023651123\n",
      "Iteration 11780 Training loss 0.031031902879476547 Validation loss 0.03623585030436516 Accuracy 0.6358000040054321\n",
      "Iteration 11790 Training loss 0.03388158231973648 Validation loss 0.03613460808992386 Accuracy 0.636900007724762\n",
      "Iteration 11800 Training loss 0.03555432707071304 Validation loss 0.036391548812389374 Accuracy 0.6342999935150146\n",
      "Iteration 11810 Training loss 0.03749067336320877 Validation loss 0.03611098974943161 Accuracy 0.636900007724762\n",
      "Iteration 11820 Training loss 0.03500979021191597 Validation loss 0.03677847236394882 Accuracy 0.6299999952316284\n",
      "Iteration 11830 Training loss 0.03452540934085846 Validation loss 0.03604608401656151 Accuracy 0.6376000046730042\n",
      "Iteration 11840 Training loss 0.03388535976409912 Validation loss 0.036189012229442596 Accuracy 0.6363999843597412\n",
      "Iteration 11850 Training loss 0.03432980924844742 Validation loss 0.036118894815444946 Accuracy 0.6365000009536743\n",
      "Iteration 11860 Training loss 0.03526107966899872 Validation loss 0.03615010157227516 Accuracy 0.6359000205993652\n",
      "Iteration 11870 Training loss 0.03739633783698082 Validation loss 0.0359678715467453 Accuracy 0.6381000280380249\n",
      "Iteration 11880 Training loss 0.03754761442542076 Validation loss 0.03628097102046013 Accuracy 0.6351000070571899\n",
      "Iteration 11890 Training loss 0.032633960247039795 Validation loss 0.036171723157167435 Accuracy 0.6363000273704529\n",
      "Iteration 11900 Training loss 0.03565073758363724 Validation loss 0.0367216095328331 Accuracy 0.6313999891281128\n",
      "Iteration 11910 Training loss 0.03347275033593178 Validation loss 0.03624185919761658 Accuracy 0.635699987411499\n",
      "Iteration 11920 Training loss 0.0336017943918705 Validation loss 0.036562561988830566 Accuracy 0.6327000260353088\n",
      "Iteration 11930 Training loss 0.03179258480668068 Validation loss 0.035938333719968796 Accuracy 0.6376000046730042\n",
      "Iteration 11940 Training loss 0.035357099026441574 Validation loss 0.0361676849424839 Accuracy 0.6362000107765198\n",
      "Iteration 11950 Training loss 0.03564385324716568 Validation loss 0.03667810931801796 Accuracy 0.6312999725341797\n",
      "Iteration 11960 Training loss 0.036411676555871964 Validation loss 0.03654426336288452 Accuracy 0.6327000260353088\n",
      "Iteration 11970 Training loss 0.03778279200196266 Validation loss 0.03657790273427963 Accuracy 0.6323000192642212\n",
      "Iteration 11980 Training loss 0.03912486135959625 Validation loss 0.037231575697660446 Accuracy 0.6262000203132629\n",
      "Iteration 11990 Training loss 0.035680338740348816 Validation loss 0.03615759685635567 Accuracy 0.6362000107765198\n",
      "Iteration 12000 Training loss 0.03736210986971855 Validation loss 0.03704224154353142 Accuracy 0.6277999877929688\n",
      "Iteration 12010 Training loss 0.03301028534770012 Validation loss 0.03643061965703964 Accuracy 0.6330999732017517\n",
      "Iteration 12020 Training loss 0.035651180893182755 Validation loss 0.03595130518078804 Accuracy 0.6388000249862671\n",
      "Iteration 12030 Training loss 0.03479033708572388 Validation loss 0.036386892199516296 Accuracy 0.6342999935150146\n",
      "Iteration 12040 Training loss 0.033201493322849274 Validation loss 0.03668932244181633 Accuracy 0.6305000185966492\n",
      "Iteration 12050 Training loss 0.03530999273061752 Validation loss 0.03599819168448448 Accuracy 0.637499988079071\n",
      "Iteration 12060 Training loss 0.03590056672692299 Validation loss 0.03642342612147331 Accuracy 0.6335999965667725\n",
      "Iteration 12070 Training loss 0.035799019038677216 Validation loss 0.03584481030702591 Accuracy 0.6395000219345093\n",
      "Iteration 12080 Training loss 0.03743014857172966 Validation loss 0.03637726604938507 Accuracy 0.6342999935150146\n",
      "Iteration 12090 Training loss 0.03818980231881142 Validation loss 0.03781568259000778 Accuracy 0.6204000115394592\n",
      "Iteration 12100 Training loss 0.033104170113801956 Validation loss 0.035898834466934204 Accuracy 0.6391000151634216\n",
      "Iteration 12110 Training loss 0.03558729216456413 Validation loss 0.035745758563280106 Accuracy 0.6406000256538391\n",
      "Iteration 12120 Training loss 0.031078936532139778 Validation loss 0.0360027551651001 Accuracy 0.638700008392334\n",
      "Iteration 12130 Training loss 0.03588486835360527 Validation loss 0.03610064834356308 Accuracy 0.6373000144958496\n",
      "Iteration 12140 Training loss 0.035887543112039566 Validation loss 0.036908023059368134 Accuracy 0.6295999884605408\n",
      "Iteration 12150 Training loss 0.0347832553088665 Validation loss 0.036396026611328125 Accuracy 0.6349999904632568\n",
      "Iteration 12160 Training loss 0.03697805106639862 Validation loss 0.03616858273744583 Accuracy 0.6355999708175659\n",
      "Iteration 12170 Training loss 0.03760324418544769 Validation loss 0.036645978689193726 Accuracy 0.6306999921798706\n",
      "Iteration 12180 Training loss 0.0334072969853878 Validation loss 0.036119066178798676 Accuracy 0.6370000243186951\n",
      "Iteration 12190 Training loss 0.03587616980075836 Validation loss 0.03613058477640152 Accuracy 0.6366999745368958\n",
      "Iteration 12200 Training loss 0.03496363013982773 Validation loss 0.0362718366086483 Accuracy 0.635200023651123\n",
      "Iteration 12210 Training loss 0.03673553094267845 Validation loss 0.03600708395242691 Accuracy 0.6377000212669373\n",
      "Iteration 12220 Training loss 0.03396708890795708 Validation loss 0.03661543130874634 Accuracy 0.6317999958992004\n",
      "Iteration 12230 Training loss 0.03230447694659233 Validation loss 0.03649434447288513 Accuracy 0.6328999996185303\n",
      "Iteration 12240 Training loss 0.035023435950279236 Validation loss 0.036286454647779465 Accuracy 0.6348000168800354\n",
      "Iteration 12250 Training loss 0.036755193024873734 Validation loss 0.03612044081091881 Accuracy 0.6370000243186951\n",
      "Iteration 12260 Training loss 0.03126634657382965 Validation loss 0.036572933197021484 Accuracy 0.6322000026702881\n",
      "Iteration 12270 Training loss 0.03542548418045044 Validation loss 0.036592066287994385 Accuracy 0.6317999958992004\n",
      "Iteration 12280 Training loss 0.03406562656164169 Validation loss 0.03644523397088051 Accuracy 0.633400022983551\n",
      "Iteration 12290 Training loss 0.03452537581324577 Validation loss 0.03604092076420784 Accuracy 0.6383000016212463\n",
      "Iteration 12300 Training loss 0.036418940871953964 Validation loss 0.03619353473186493 Accuracy 0.6363999843597412\n",
      "Iteration 12310 Training loss 0.034680791199207306 Validation loss 0.036183394491672516 Accuracy 0.635699987411499\n",
      "Iteration 12320 Training loss 0.034468814730644226 Validation loss 0.03633948788046837 Accuracy 0.6345000267028809\n",
      "Iteration 12330 Training loss 0.036926914006471634 Validation loss 0.036047156900167465 Accuracy 0.637499988079071\n",
      "Iteration 12340 Training loss 0.03637779876589775 Validation loss 0.03600918874144554 Accuracy 0.6373999714851379\n",
      "Iteration 12350 Training loss 0.03463339805603027 Validation loss 0.03614106774330139 Accuracy 0.635699987411499\n",
      "Iteration 12360 Training loss 0.033651988953351974 Validation loss 0.036190710961818695 Accuracy 0.6355000138282776\n",
      "Iteration 12370 Training loss 0.03532709926366806 Validation loss 0.03625428304076195 Accuracy 0.6348999738693237\n",
      "Iteration 12380 Training loss 0.035374391824007034 Validation loss 0.035685181617736816 Accuracy 0.6407999992370605\n",
      "Iteration 12390 Training loss 0.033044859766960144 Validation loss 0.036021195352077484 Accuracy 0.6377000212669373\n",
      "Iteration 12400 Training loss 0.0347871407866478 Validation loss 0.03586972504854202 Accuracy 0.6391000151634216\n",
      "Iteration 12410 Training loss 0.03780167177319527 Validation loss 0.036179181188344955 Accuracy 0.6359999775886536\n",
      "Iteration 12420 Training loss 0.032992150634527206 Validation loss 0.03591934219002724 Accuracy 0.6378999948501587\n",
      "Iteration 12430 Training loss 0.0353202223777771 Validation loss 0.03649303317070007 Accuracy 0.6323000192642212\n",
      "Iteration 12440 Training loss 0.03277040645480156 Validation loss 0.03636620193719864 Accuracy 0.6334999799728394\n",
      "Iteration 12450 Training loss 0.03393096849322319 Validation loss 0.035959359258413315 Accuracy 0.6377999782562256\n",
      "Iteration 12460 Training loss 0.03572199121117592 Validation loss 0.03747987747192383 Accuracy 0.6223000288009644\n",
      "Iteration 12470 Training loss 0.03385522589087486 Validation loss 0.03709035366773605 Accuracy 0.6273999810218811\n",
      "Iteration 12480 Training loss 0.036525625735521317 Validation loss 0.03603743016719818 Accuracy 0.6384999752044678\n",
      "Iteration 12490 Training loss 0.032786279916763306 Validation loss 0.03589635342359543 Accuracy 0.6395000219345093\n",
      "Iteration 12500 Training loss 0.03444311395287514 Validation loss 0.03595493733882904 Accuracy 0.6384000182151794\n",
      "Iteration 12510 Training loss 0.029674474149942398 Validation loss 0.03624856472015381 Accuracy 0.6353999972343445\n",
      "Iteration 12520 Training loss 0.03805198147892952 Validation loss 0.036401569843292236 Accuracy 0.6337000131607056\n",
      "Iteration 12530 Training loss 0.03565077483654022 Validation loss 0.036781396716833115 Accuracy 0.6299999952316284\n",
      "Iteration 12540 Training loss 0.03390362113714218 Validation loss 0.03689327836036682 Accuracy 0.6292999982833862\n",
      "Iteration 12550 Training loss 0.03554416075348854 Validation loss 0.03644236922264099 Accuracy 0.6337000131607056\n",
      "Iteration 12560 Training loss 0.037407036870718 Validation loss 0.036167655140161514 Accuracy 0.6359000205993652\n",
      "Iteration 12570 Training loss 0.03316623345017433 Validation loss 0.03608676418662071 Accuracy 0.6370999813079834\n",
      "Iteration 12580 Training loss 0.032888323068618774 Validation loss 0.03620430827140808 Accuracy 0.6363999843597412\n",
      "Iteration 12590 Training loss 0.03533665090799332 Validation loss 0.03639062121510506 Accuracy 0.6342999935150146\n",
      "Iteration 12600 Training loss 0.03430451825261116 Validation loss 0.03622126951813698 Accuracy 0.6352999806404114\n",
      "Iteration 12610 Training loss 0.03635934740304947 Validation loss 0.03643866628408432 Accuracy 0.6328999996185303\n",
      "Iteration 12620 Training loss 0.03522557020187378 Validation loss 0.0361156240105629 Accuracy 0.6367999911308289\n",
      "Iteration 12630 Training loss 0.037444885820150375 Validation loss 0.03674688562750816 Accuracy 0.6298999786376953\n",
      "Iteration 12640 Training loss 0.032116036862134933 Validation loss 0.03622709959745407 Accuracy 0.6359999775886536\n",
      "Iteration 12650 Training loss 0.03626904636621475 Validation loss 0.03671476989984512 Accuracy 0.6308000087738037\n",
      "Iteration 12660 Training loss 0.03613878786563873 Validation loss 0.03666121885180473 Accuracy 0.6312000155448914\n",
      "Iteration 12670 Training loss 0.03068946674466133 Validation loss 0.03604288399219513 Accuracy 0.6371999979019165\n",
      "Iteration 12680 Training loss 0.034343522042036057 Validation loss 0.036127060651779175 Accuracy 0.6365000009536743\n",
      "Iteration 12690 Training loss 0.03238203749060631 Validation loss 0.03645719587802887 Accuracy 0.6331999897956848\n",
      "Iteration 12700 Training loss 0.03413368761539459 Validation loss 0.035788025707006454 Accuracy 0.6395000219345093\n",
      "Iteration 12710 Training loss 0.03431442752480507 Validation loss 0.035832710564136505 Accuracy 0.6388999819755554\n",
      "Iteration 12720 Training loss 0.03628207743167877 Validation loss 0.036053624004125595 Accuracy 0.6365000009536743\n",
      "Iteration 12730 Training loss 0.03543782979249954 Validation loss 0.03603072091937065 Accuracy 0.637499988079071\n",
      "Iteration 12740 Training loss 0.03518117219209671 Validation loss 0.035951100289821625 Accuracy 0.6385999917984009\n",
      "Iteration 12750 Training loss 0.037475235760211945 Validation loss 0.03573993593454361 Accuracy 0.6407999992370605\n",
      "Iteration 12760 Training loss 0.03635053336620331 Validation loss 0.036364104598760605 Accuracy 0.6334999799728394\n",
      "Iteration 12770 Training loss 0.03231198713183403 Validation loss 0.03641195222735405 Accuracy 0.6341999769210815\n",
      "Iteration 12780 Training loss 0.0347011536359787 Validation loss 0.035891883075237274 Accuracy 0.638700008392334\n",
      "Iteration 12790 Training loss 0.03395009785890579 Validation loss 0.03623054549098015 Accuracy 0.6359000205993652\n",
      "Iteration 12800 Training loss 0.03647395968437195 Validation loss 0.03590589016675949 Accuracy 0.6384000182151794\n",
      "Iteration 12810 Training loss 0.03817886859178543 Validation loss 0.03622140735387802 Accuracy 0.6355999708175659\n",
      "Iteration 12820 Training loss 0.03679630905389786 Validation loss 0.036234017461538315 Accuracy 0.6359999775886536\n",
      "Iteration 12830 Training loss 0.03393227979540825 Validation loss 0.03652454912662506 Accuracy 0.6323000192642212\n",
      "Iteration 12840 Training loss 0.03434273600578308 Validation loss 0.03699248656630516 Accuracy 0.6276000142097473\n",
      "Iteration 12850 Training loss 0.035010822117328644 Validation loss 0.03625061735510826 Accuracy 0.6355000138282776\n",
      "Iteration 12860 Training loss 0.03831574320793152 Validation loss 0.03610515967011452 Accuracy 0.6365000009536743\n",
      "Iteration 12870 Training loss 0.0361633263528347 Validation loss 0.03603450953960419 Accuracy 0.6373999714851379\n",
      "Iteration 12880 Training loss 0.034188564866781235 Validation loss 0.035863470286130905 Accuracy 0.6389999985694885\n",
      "Iteration 12890 Training loss 0.0363684706389904 Validation loss 0.03683644160628319 Accuracy 0.6288999915122986\n",
      "Iteration 12900 Training loss 0.03420674428343773 Validation loss 0.03606375306844711 Accuracy 0.6365000009536743\n",
      "Iteration 12910 Training loss 0.0331089161336422 Validation loss 0.03598151355981827 Accuracy 0.6384000182151794\n",
      "Iteration 12920 Training loss 0.036839257925748825 Validation loss 0.035795096307992935 Accuracy 0.6403999924659729\n",
      "Iteration 12930 Training loss 0.03705589845776558 Validation loss 0.035904694348573685 Accuracy 0.6388999819755554\n",
      "Iteration 12940 Training loss 0.03519965708255768 Validation loss 0.036153070628643036 Accuracy 0.6363999843597412\n",
      "Iteration 12950 Training loss 0.033875733613967896 Validation loss 0.03617201745510101 Accuracy 0.6358000040054321\n",
      "Iteration 12960 Training loss 0.03400587663054466 Validation loss 0.03701557219028473 Accuracy 0.6276999711990356\n",
      "Iteration 12970 Training loss 0.03582063317298889 Validation loss 0.035867564380168915 Accuracy 0.6392999887466431\n",
      "Iteration 12980 Training loss 0.03637278825044632 Validation loss 0.036412645131349564 Accuracy 0.6341999769210815\n",
      "Iteration 12990 Training loss 0.03694726154208183 Validation loss 0.03587348014116287 Accuracy 0.6395000219345093\n",
      "Iteration 13000 Training loss 0.032309237867593765 Validation loss 0.03651127964258194 Accuracy 0.6327999830245972\n",
      "Iteration 13010 Training loss 0.034636594355106354 Validation loss 0.036840975284576416 Accuracy 0.6290000081062317\n",
      "Iteration 13020 Training loss 0.03634653612971306 Validation loss 0.036189883947372437 Accuracy 0.6363000273704529\n",
      "Iteration 13030 Training loss 0.035812679678201675 Validation loss 0.03624243289232254 Accuracy 0.635699987411499\n",
      "Iteration 13040 Training loss 0.03252558782696724 Validation loss 0.03627600520849228 Accuracy 0.6348999738693237\n",
      "Iteration 13050 Training loss 0.03593769669532776 Validation loss 0.03656348958611488 Accuracy 0.6323999762535095\n",
      "Iteration 13060 Training loss 0.03409869968891144 Validation loss 0.0359739251434803 Accuracy 0.6383000016212463\n",
      "Iteration 13070 Training loss 0.03607702627778053 Validation loss 0.03609137237071991 Accuracy 0.637499988079071\n",
      "Iteration 13080 Training loss 0.038976456969976425 Validation loss 0.036799199879169464 Accuracy 0.6306999921798706\n",
      "Iteration 13090 Training loss 0.03385664150118828 Validation loss 0.03649203106760979 Accuracy 0.6337000131607056\n",
      "Iteration 13100 Training loss 0.03364110738039017 Validation loss 0.03603528439998627 Accuracy 0.6377000212669373\n",
      "Iteration 13110 Training loss 0.03633159026503563 Validation loss 0.036097485572099686 Accuracy 0.6370999813079834\n",
      "Iteration 13120 Training loss 0.03803607448935509 Validation loss 0.03664732351899147 Accuracy 0.6315000057220459\n",
      "Iteration 13130 Training loss 0.03678687661886215 Validation loss 0.035834901034832 Accuracy 0.6399000287055969\n",
      "Iteration 13140 Training loss 0.03649807721376419 Validation loss 0.036379892379045486 Accuracy 0.6348999738693237\n",
      "Iteration 13150 Training loss 0.035370051860809326 Validation loss 0.03660447895526886 Accuracy 0.632099986076355\n",
      "Iteration 13160 Training loss 0.03459765017032623 Validation loss 0.0360829159617424 Accuracy 0.6370999813079834\n",
      "Iteration 13170 Training loss 0.036461081355810165 Validation loss 0.036110423505306244 Accuracy 0.636900007724762\n",
      "Iteration 13180 Training loss 0.034627366811037064 Validation loss 0.036061156541109085 Accuracy 0.6373999714851379\n",
      "Iteration 13190 Training loss 0.038905851542949677 Validation loss 0.0359414704144001 Accuracy 0.6391000151634216\n",
      "Iteration 13200 Training loss 0.03491116687655449 Validation loss 0.036102503538131714 Accuracy 0.6377999782562256\n",
      "Iteration 13210 Training loss 0.03433726355433464 Validation loss 0.03595452755689621 Accuracy 0.6389999985694885\n",
      "Iteration 13220 Training loss 0.034690842032432556 Validation loss 0.03601871430873871 Accuracy 0.6384000182151794\n",
      "Iteration 13230 Training loss 0.03796679526567459 Validation loss 0.035989463329315186 Accuracy 0.638700008392334\n",
      "Iteration 13240 Training loss 0.035852644592523575 Validation loss 0.03622233495116234 Accuracy 0.6362000107765198\n",
      "Iteration 13250 Training loss 0.03583303838968277 Validation loss 0.036384452134370804 Accuracy 0.6335999965667725\n",
      "Iteration 13260 Training loss 0.03579380363225937 Validation loss 0.03674306720495224 Accuracy 0.6294000148773193\n",
      "Iteration 13270 Training loss 0.03571103885769844 Validation loss 0.03693011403083801 Accuracy 0.6280999779701233\n",
      "Iteration 13280 Training loss 0.03589494898915291 Validation loss 0.03604796156287193 Accuracy 0.6367999911308289\n",
      "Iteration 13290 Training loss 0.035705775022506714 Validation loss 0.036129649728536606 Accuracy 0.6365000009536743\n",
      "Iteration 13300 Training loss 0.03508748114109039 Validation loss 0.0364040732383728 Accuracy 0.6327999830245972\n",
      "Iteration 13310 Training loss 0.0358404703438282 Validation loss 0.03585690259933472 Accuracy 0.6395000219345093\n",
      "Iteration 13320 Training loss 0.03349617123603821 Validation loss 0.03599701449275017 Accuracy 0.6384000182151794\n",
      "Iteration 13330 Training loss 0.03480047732591629 Validation loss 0.035901572555303574 Accuracy 0.6384000182151794\n",
      "Iteration 13340 Training loss 0.03470853716135025 Validation loss 0.036328770220279694 Accuracy 0.6341000199317932\n",
      "Iteration 13350 Training loss 0.033999621868133545 Validation loss 0.03611978515982628 Accuracy 0.6363999843597412\n",
      "Iteration 13360 Training loss 0.03274981305003166 Validation loss 0.036246221512556076 Accuracy 0.6355999708175659\n",
      "Iteration 13370 Training loss 0.034379828721284866 Validation loss 0.03589986264705658 Accuracy 0.6384000182151794\n",
      "Iteration 13380 Training loss 0.03519516438245773 Validation loss 0.03658551350235939 Accuracy 0.631600022315979\n",
      "Iteration 13390 Training loss 0.03011026419699192 Validation loss 0.03618353605270386 Accuracy 0.6359999775886536\n",
      "Iteration 13400 Training loss 0.034156203269958496 Validation loss 0.03607720881700516 Accuracy 0.6376000046730042\n",
      "Iteration 13410 Training loss 0.03461851924657822 Validation loss 0.03596556559205055 Accuracy 0.6380000114440918\n",
      "Iteration 13420 Training loss 0.035436976701021194 Validation loss 0.03646857291460037 Accuracy 0.6335999965667725\n",
      "Iteration 13430 Training loss 0.030068710446357727 Validation loss 0.0363132506608963 Accuracy 0.635699987411499\n",
      "Iteration 13440 Training loss 0.037548959255218506 Validation loss 0.03683853894472122 Accuracy 0.6291000247001648\n",
      "Iteration 13450 Training loss 0.03711780905723572 Validation loss 0.035813119262456894 Accuracy 0.6392999887466431\n",
      "Iteration 13460 Training loss 0.03364284336566925 Validation loss 0.037342753261327744 Accuracy 0.6248000264167786\n",
      "Iteration 13470 Training loss 0.033562447875738144 Validation loss 0.0363178625702858 Accuracy 0.6351000070571899\n",
      "Iteration 13480 Training loss 0.03366005793213844 Validation loss 0.03606557473540306 Accuracy 0.6366999745368958\n",
      "Iteration 13490 Training loss 0.03676031157374382 Validation loss 0.036251336336135864 Accuracy 0.6353999972343445\n",
      "Iteration 13500 Training loss 0.034066032618284225 Validation loss 0.03703676164150238 Accuracy 0.6272000074386597\n",
      "Iteration 13510 Training loss 0.03447052463889122 Validation loss 0.03606261685490608 Accuracy 0.6371999979019165\n",
      "Iteration 13520 Training loss 0.035230182111263275 Validation loss 0.03624926880002022 Accuracy 0.6355999708175659\n",
      "Iteration 13530 Training loss 0.031181182712316513 Validation loss 0.03683223947882652 Accuracy 0.6294999718666077\n",
      "Iteration 13540 Training loss 0.033689700067043304 Validation loss 0.03672391548752785 Accuracy 0.6308000087738037\n",
      "Iteration 13550 Training loss 0.035506002604961395 Validation loss 0.0361490324139595 Accuracy 0.6363000273704529\n",
      "Iteration 13560 Training loss 0.03544824570417404 Validation loss 0.03667604550719261 Accuracy 0.6308000087738037\n",
      "Iteration 13570 Training loss 0.03725241497159004 Validation loss 0.036674488335847855 Accuracy 0.6313999891281128\n",
      "Iteration 13580 Training loss 0.035109858959913254 Validation loss 0.03636876121163368 Accuracy 0.6338000297546387\n",
      "Iteration 13590 Training loss 0.039224669337272644 Validation loss 0.03626188635826111 Accuracy 0.6344000101089478\n",
      "Iteration 13600 Training loss 0.035546574741601944 Validation loss 0.036141443997621536 Accuracy 0.6359999775886536\n",
      "Iteration 13610 Training loss 0.03186013177037239 Validation loss 0.03632510453462601 Accuracy 0.6342999935150146\n",
      "Iteration 13620 Training loss 0.0319715179502964 Validation loss 0.03618821129202843 Accuracy 0.6366000175476074\n",
      "Iteration 13630 Training loss 0.03428911790251732 Validation loss 0.035993751138448715 Accuracy 0.6388000249862671\n",
      "Iteration 13640 Training loss 0.03523600101470947 Validation loss 0.03594786301255226 Accuracy 0.638700008392334\n",
      "Iteration 13650 Training loss 0.034324176609516144 Validation loss 0.036989472806453705 Accuracy 0.6287000179290771\n",
      "Iteration 13660 Training loss 0.03437688574194908 Validation loss 0.036363016813993454 Accuracy 0.6345999836921692\n",
      "Iteration 13670 Training loss 0.03568844497203827 Validation loss 0.03640918433666229 Accuracy 0.6341999769210815\n",
      "Iteration 13680 Training loss 0.03302833065390587 Validation loss 0.03603067621588707 Accuracy 0.6377000212669373\n",
      "Iteration 13690 Training loss 0.03231075033545494 Validation loss 0.035958874970674515 Accuracy 0.6381999850273132\n",
      "Iteration 13700 Training loss 0.03752356022596359 Validation loss 0.03598560392856598 Accuracy 0.6377999782562256\n",
      "Iteration 13710 Training loss 0.03524146229028702 Validation loss 0.03593554347753525 Accuracy 0.6381999850273132\n",
      "Iteration 13720 Training loss 0.03515215963125229 Validation loss 0.03605355694890022 Accuracy 0.6367999911308289\n",
      "Iteration 13730 Training loss 0.037195976823568344 Validation loss 0.03592471778392792 Accuracy 0.6384999752044678\n",
      "Iteration 13740 Training loss 0.03690284118056297 Validation loss 0.03616219758987427 Accuracy 0.6358000040054321\n",
      "Iteration 13750 Training loss 0.03312280401587486 Validation loss 0.03609692305326462 Accuracy 0.6373999714851379\n",
      "Iteration 13760 Training loss 0.034209638833999634 Validation loss 0.035866983234882355 Accuracy 0.6395000219345093\n",
      "Iteration 13770 Training loss 0.034315235912799835 Validation loss 0.03596944734454155 Accuracy 0.6384999752044678\n",
      "Iteration 13780 Training loss 0.03323226049542427 Validation loss 0.036041900515556335 Accuracy 0.6377999782562256\n",
      "Iteration 13790 Training loss 0.034260183572769165 Validation loss 0.035870909690856934 Accuracy 0.6388000249862671\n",
      "Iteration 13800 Training loss 0.03287985175848007 Validation loss 0.03597161918878555 Accuracy 0.6385999917984009\n",
      "Iteration 13810 Training loss 0.034195899963378906 Validation loss 0.03600126877427101 Accuracy 0.6378999948501587\n",
      "Iteration 13820 Training loss 0.0352037139236927 Validation loss 0.03585612401366234 Accuracy 0.6395000219345093\n",
      "Iteration 13830 Training loss 0.03320593759417534 Validation loss 0.03646435961127281 Accuracy 0.6331999897956848\n",
      "Iteration 13840 Training loss 0.037786077708005905 Validation loss 0.03733301907777786 Accuracy 0.6238999962806702\n",
      "Iteration 13850 Training loss 0.03158289194107056 Validation loss 0.03618711978197098 Accuracy 0.6360999941825867\n",
      "Iteration 13860 Training loss 0.037922728806734085 Validation loss 0.03635634109377861 Accuracy 0.633899986743927\n",
      "Iteration 13870 Training loss 0.036389756947755814 Validation loss 0.03591356426477432 Accuracy 0.6384000182151794\n",
      "Iteration 13880 Training loss 0.03603686764836311 Validation loss 0.036232609301805496 Accuracy 0.6348000168800354\n",
      "Iteration 13890 Training loss 0.03426661342382431 Validation loss 0.036534182727336884 Accuracy 0.6324999928474426\n",
      "Iteration 13900 Training loss 0.03449712321162224 Validation loss 0.03589964285492897 Accuracy 0.6380000114440918\n",
      "Iteration 13910 Training loss 0.03237244859337807 Validation loss 0.03611672669649124 Accuracy 0.6365000009536743\n",
      "Iteration 13920 Training loss 0.036084458231925964 Validation loss 0.03653853014111519 Accuracy 0.6323999762535095\n",
      "Iteration 13930 Training loss 0.038009244948625565 Validation loss 0.03632095828652382 Accuracy 0.6341999769210815\n",
      "Iteration 13940 Training loss 0.039162930101156235 Validation loss 0.035957857966423035 Accuracy 0.6380000114440918\n",
      "Iteration 13950 Training loss 0.03275061398744583 Validation loss 0.03580319508910179 Accuracy 0.6399000287055969\n",
      "Iteration 13960 Training loss 0.037128545343875885 Validation loss 0.03662446141242981 Accuracy 0.6315000057220459\n",
      "Iteration 13970 Training loss 0.036002010107040405 Validation loss 0.03605622425675392 Accuracy 0.6370999813079834\n",
      "Iteration 13980 Training loss 0.032799266278743744 Validation loss 0.036071255803108215 Accuracy 0.6373999714851379\n",
      "Iteration 13990 Training loss 0.03263365104794502 Validation loss 0.036294884979724884 Accuracy 0.6352999806404114\n",
      "Iteration 14000 Training loss 0.036546673625707626 Validation loss 0.03663960099220276 Accuracy 0.6313999891281128\n",
      "Iteration 14010 Training loss 0.03537894785404205 Validation loss 0.036090247333049774 Accuracy 0.6367999911308289\n",
      "Iteration 14020 Training loss 0.035795751959085464 Validation loss 0.03641301393508911 Accuracy 0.6337000131607056\n",
      "Iteration 14030 Training loss 0.03322366997599602 Validation loss 0.036307353526353836 Accuracy 0.635200023651123\n",
      "Iteration 14040 Training loss 0.03183853253722191 Validation loss 0.03634515777230263 Accuracy 0.6347000002861023\n",
      "Iteration 14050 Training loss 0.0342840738594532 Validation loss 0.03605736419558525 Accuracy 0.6377000212669373\n",
      "Iteration 14060 Training loss 0.033549919724464417 Validation loss 0.036118414252996445 Accuracy 0.6367999911308289\n",
      "Iteration 14070 Training loss 0.03710373863577843 Validation loss 0.03617855906486511 Accuracy 0.6366000175476074\n",
      "Iteration 14080 Training loss 0.03453418239951134 Validation loss 0.0363333486020565 Accuracy 0.6348999738693237\n",
      "Iteration 14090 Training loss 0.03298202529549599 Validation loss 0.03599528223276138 Accuracy 0.6377999782562256\n",
      "Iteration 14100 Training loss 0.0350387766957283 Validation loss 0.03649895638227463 Accuracy 0.633400022983551\n",
      "Iteration 14110 Training loss 0.0365617610514164 Validation loss 0.036589786410331726 Accuracy 0.6323000192642212\n",
      "Iteration 14120 Training loss 0.03295215591788292 Validation loss 0.036203861236572266 Accuracy 0.6363000273704529\n",
      "Iteration 14130 Training loss 0.03456949442625046 Validation loss 0.03654361143708229 Accuracy 0.6327999830245972\n",
      "Iteration 14140 Training loss 0.03155514597892761 Validation loss 0.03617540001869202 Accuracy 0.6367999911308289\n",
      "Iteration 14150 Training loss 0.034632183611392975 Validation loss 0.03596571087837219 Accuracy 0.6388999819755554\n",
      "Iteration 14160 Training loss 0.035176295787096024 Validation loss 0.03630822151899338 Accuracy 0.6355999708175659\n",
      "Iteration 14170 Training loss 0.03370845690369606 Validation loss 0.03617538511753082 Accuracy 0.6360999941825867\n",
      "Iteration 14180 Training loss 0.034913696348667145 Validation loss 0.0361240953207016 Accuracy 0.6370999813079834\n",
      "Iteration 14190 Training loss 0.03373955190181732 Validation loss 0.03604540973901749 Accuracy 0.6378999948501587\n",
      "Iteration 14200 Training loss 0.033840078860521317 Validation loss 0.03595241531729698 Accuracy 0.6381000280380249\n",
      "Iteration 14210 Training loss 0.036469511687755585 Validation loss 0.036231182515621185 Accuracy 0.6360999941825867\n",
      "Iteration 14220 Training loss 0.03989458084106445 Validation loss 0.03663574531674385 Accuracy 0.6323000192642212\n",
      "Iteration 14230 Training loss 0.03395408019423485 Validation loss 0.03578140214085579 Accuracy 0.6406999826431274\n",
      "Iteration 14240 Training loss 0.03328920900821686 Validation loss 0.03592628985643387 Accuracy 0.6391000151634216\n",
      "Iteration 14250 Training loss 0.03319115191698074 Validation loss 0.035922177135944366 Accuracy 0.63919997215271\n",
      "Iteration 14260 Training loss 0.035461731255054474 Validation loss 0.036266956478357315 Accuracy 0.6348000168800354\n",
      "Iteration 14270 Training loss 0.034095875918865204 Validation loss 0.035915445536375046 Accuracy 0.6392999887466431\n",
      "Iteration 14280 Training loss 0.03472008928656578 Validation loss 0.03606723994016647 Accuracy 0.6377999782562256\n",
      "Iteration 14290 Training loss 0.03189927339553833 Validation loss 0.036102280020713806 Accuracy 0.6376000046730042\n",
      "Iteration 14300 Training loss 0.0333552323281765 Validation loss 0.036091648042201996 Accuracy 0.6371999979019165\n",
      "Iteration 14310 Training loss 0.03542831912636757 Validation loss 0.036780066788196564 Accuracy 0.6305999755859375\n",
      "Iteration 14320 Training loss 0.03456495329737663 Validation loss 0.03576049581170082 Accuracy 0.6412000060081482\n",
      "Iteration 14330 Training loss 0.037937864661216736 Validation loss 0.03608936443924904 Accuracy 0.6373000144958496\n",
      "Iteration 14340 Training loss 0.033281829208135605 Validation loss 0.03625011816620827 Accuracy 0.6355999708175659\n",
      "Iteration 14350 Training loss 0.03608410060405731 Validation loss 0.036006778478622437 Accuracy 0.6385999917984009\n",
      "Iteration 14360 Training loss 0.033492498099803925 Validation loss 0.03622818738222122 Accuracy 0.635699987411499\n",
      "Iteration 14370 Training loss 0.035219281911849976 Validation loss 0.035971783101558685 Accuracy 0.6388000249862671\n",
      "Iteration 14380 Training loss 0.03148626536130905 Validation loss 0.03656388819217682 Accuracy 0.632099986076355\n",
      "Iteration 14390 Training loss 0.0337190106511116 Validation loss 0.03612891212105751 Accuracy 0.6363999843597412\n",
      "Iteration 14400 Training loss 0.03344370052218437 Validation loss 0.03650466725230217 Accuracy 0.6324999928474426\n",
      "Iteration 14410 Training loss 0.032786790281534195 Validation loss 0.03594383969902992 Accuracy 0.6391000151634216\n",
      "Iteration 14420 Training loss 0.03541383147239685 Validation loss 0.03605770692229271 Accuracy 0.6381999850273132\n",
      "Iteration 14430 Training loss 0.03611176460981369 Validation loss 0.03604715317487717 Accuracy 0.6380000114440918\n",
      "Iteration 14440 Training loss 0.03630010038614273 Validation loss 0.03620437905192375 Accuracy 0.6355000138282776\n",
      "Iteration 14450 Training loss 0.031646572053432465 Validation loss 0.035779159516096115 Accuracy 0.6406000256538391\n",
      "Iteration 14460 Training loss 0.03535814955830574 Validation loss 0.03620065748691559 Accuracy 0.6355999708175659\n",
      "Iteration 14470 Training loss 0.0350424088537693 Validation loss 0.03594168648123741 Accuracy 0.6381000280380249\n",
      "Iteration 14480 Training loss 0.03101552650332451 Validation loss 0.036324046552181244 Accuracy 0.6338000297546387\n",
      "Iteration 14490 Training loss 0.036609262228012085 Validation loss 0.03572320193052292 Accuracy 0.6402000188827515\n",
      "Iteration 14500 Training loss 0.03562688082456589 Validation loss 0.03576236963272095 Accuracy 0.640500009059906\n",
      "Iteration 14510 Training loss 0.03462209925055504 Validation loss 0.03605762869119644 Accuracy 0.6366000175476074\n",
      "Iteration 14520 Training loss 0.03459560498595238 Validation loss 0.03566419333219528 Accuracy 0.6409000158309937\n",
      "Iteration 14530 Training loss 0.03620162978768349 Validation loss 0.03591533377766609 Accuracy 0.638700008392334\n",
      "Iteration 14540 Training loss 0.03282273933291435 Validation loss 0.03584979102015495 Accuracy 0.63919997215271\n",
      "Iteration 14550 Training loss 0.03542768210172653 Validation loss 0.03607001528143883 Accuracy 0.6363999843597412\n",
      "Iteration 14560 Training loss 0.03755175694823265 Validation loss 0.03590165823698044 Accuracy 0.6385999917984009\n",
      "Iteration 14570 Training loss 0.03800065442919731 Validation loss 0.03831670433282852 Accuracy 0.6144000291824341\n",
      "Iteration 14580 Training loss 0.034648388624191284 Validation loss 0.0359979085624218 Accuracy 0.6378999948501587\n",
      "Iteration 14590 Training loss 0.035818956792354584 Validation loss 0.03653435409069061 Accuracy 0.6327000260353088\n",
      "Iteration 14600 Training loss 0.03292118385434151 Validation loss 0.03588768467307091 Accuracy 0.6385999917984009\n",
      "Iteration 14610 Training loss 0.035808265209198 Validation loss 0.03576689586043358 Accuracy 0.6395999789237976\n",
      "Iteration 14620 Training loss 0.037187203764915466 Validation loss 0.0357990525662899 Accuracy 0.6398000121116638\n",
      "Iteration 14630 Training loss 0.036115434020757675 Validation loss 0.035879574716091156 Accuracy 0.6388000249862671\n",
      "Iteration 14640 Training loss 0.03379982337355614 Validation loss 0.035741958767175674 Accuracy 0.6398000121116638\n",
      "Iteration 14650 Training loss 0.03475961834192276 Validation loss 0.03592132404446602 Accuracy 0.638700008392334\n",
      "Iteration 14660 Training loss 0.0312217865139246 Validation loss 0.03593027964234352 Accuracy 0.6381999850273132\n",
      "Iteration 14670 Training loss 0.03423731029033661 Validation loss 0.03571600094437599 Accuracy 0.6406999826431274\n",
      "Iteration 14680 Training loss 0.03795741870999336 Validation loss 0.03586985543370247 Accuracy 0.6392999887466431\n",
      "Iteration 14690 Training loss 0.035490889102220535 Validation loss 0.03587658330798149 Accuracy 0.6395999789237976\n",
      "Iteration 14700 Training loss 0.03691798076033592 Validation loss 0.036078207194805145 Accuracy 0.6378999948501587\n",
      "Iteration 14710 Training loss 0.03463241085410118 Validation loss 0.03607548773288727 Accuracy 0.6373000144958496\n",
      "Iteration 14720 Training loss 0.03585214912891388 Validation loss 0.035935789346694946 Accuracy 0.63919997215271\n",
      "Iteration 14730 Training loss 0.03551126644015312 Validation loss 0.035836730152368546 Accuracy 0.6399999856948853\n",
      "Iteration 14740 Training loss 0.038602109998464584 Validation loss 0.03596418350934982 Accuracy 0.638700008392334\n",
      "Iteration 14750 Training loss 0.036650821566581726 Validation loss 0.036273881793022156 Accuracy 0.6355000138282776\n",
      "Iteration 14760 Training loss 0.03646502643823624 Validation loss 0.036545269191265106 Accuracy 0.6323999762535095\n",
      "Iteration 14770 Training loss 0.03653896972537041 Validation loss 0.03582453727722168 Accuracy 0.6399999856948853\n",
      "Iteration 14780 Training loss 0.03376758098602295 Validation loss 0.03569380193948746 Accuracy 0.6413000226020813\n",
      "Iteration 14790 Training loss 0.03455027937889099 Validation loss 0.03614173084497452 Accuracy 0.6363999843597412\n",
      "Iteration 14800 Training loss 0.03575733304023743 Validation loss 0.03588183969259262 Accuracy 0.638700008392334\n",
      "Iteration 14810 Training loss 0.03375773876905441 Validation loss 0.036117181181907654 Accuracy 0.6363999843597412\n",
      "Iteration 14820 Training loss 0.04122437909245491 Validation loss 0.03595723211765289 Accuracy 0.6381000280380249\n",
      "Iteration 14830 Training loss 0.03352398797869682 Validation loss 0.03585270419716835 Accuracy 0.6388000249862671\n",
      "Iteration 14840 Training loss 0.037140052765607834 Validation loss 0.03621933236718178 Accuracy 0.6353999972343445\n",
      "Iteration 14850 Training loss 0.03641597926616669 Validation loss 0.03637801110744476 Accuracy 0.6338000297546387\n",
      "Iteration 14860 Training loss 0.0338766910135746 Validation loss 0.03587926924228668 Accuracy 0.6392999887466431\n",
      "Iteration 14870 Training loss 0.03239819407463074 Validation loss 0.03591976687312126 Accuracy 0.6384999752044678\n",
      "Iteration 14880 Training loss 0.03549397364258766 Validation loss 0.03588541969656944 Accuracy 0.6378999948501587\n",
      "Iteration 14890 Training loss 0.03404125198721886 Validation loss 0.03583193197846413 Accuracy 0.6394000053405762\n",
      "Iteration 14900 Training loss 0.03560800477862358 Validation loss 0.03583690896630287 Accuracy 0.6391000151634216\n",
      "Iteration 14910 Training loss 0.03443333879113197 Validation loss 0.03611692786216736 Accuracy 0.6366999745368958\n",
      "Iteration 14920 Training loss 0.03453735262155533 Validation loss 0.0359189435839653 Accuracy 0.6389999985694885\n",
      "Iteration 14930 Training loss 0.03477101773023605 Validation loss 0.03592848777770996 Accuracy 0.6384999752044678\n",
      "Iteration 14940 Training loss 0.03154880926012993 Validation loss 0.03603097051382065 Accuracy 0.6377999782562256\n",
      "Iteration 14950 Training loss 0.03305239602923393 Validation loss 0.035995520651340485 Accuracy 0.6377000212669373\n",
      "Iteration 14960 Training loss 0.036964211612939835 Validation loss 0.03590260073542595 Accuracy 0.63919997215271\n",
      "Iteration 14970 Training loss 0.03667178004980087 Validation loss 0.03558669611811638 Accuracy 0.6419000029563904\n",
      "Iteration 14980 Training loss 0.036722443997859955 Validation loss 0.03573061153292656 Accuracy 0.6406999826431274\n",
      "Iteration 14990 Training loss 0.03205030411481857 Validation loss 0.036448754370212555 Accuracy 0.6330999732017517\n",
      "Iteration 15000 Training loss 0.03138774633407593 Validation loss 0.03582856059074402 Accuracy 0.63919997215271\n",
      "Iteration 15010 Training loss 0.033618565648794174 Validation loss 0.03595177084207535 Accuracy 0.6371999979019165\n",
      "Iteration 15020 Training loss 0.03369499370455742 Validation loss 0.03583332896232605 Accuracy 0.6398000121116638\n",
      "Iteration 15030 Training loss 0.03434079885482788 Validation loss 0.03588034212589264 Accuracy 0.6388999819755554\n",
      "Iteration 15040 Training loss 0.03525064140558243 Validation loss 0.03654064983129501 Accuracy 0.6324999928474426\n",
      "Iteration 15050 Training loss 0.032753266394138336 Validation loss 0.036122728139162064 Accuracy 0.6363999843597412\n",
      "Iteration 15060 Training loss 0.03668666630983353 Validation loss 0.035805992782115936 Accuracy 0.6395000219345093\n",
      "Iteration 15070 Training loss 0.03496713936328888 Validation loss 0.036090560257434845 Accuracy 0.6366999745368958\n",
      "Iteration 15080 Training loss 0.03176090493798256 Validation loss 0.03602936491370201 Accuracy 0.6373999714851379\n",
      "Iteration 15090 Training loss 0.03608892858028412 Validation loss 0.03585096448659897 Accuracy 0.638700008392334\n",
      "Iteration 15100 Training loss 0.03323116898536682 Validation loss 0.03637884929776192 Accuracy 0.6341000199317932\n",
      "Iteration 15110 Training loss 0.0344555526971817 Validation loss 0.03600399196147919 Accuracy 0.6378999948501587\n",
      "Iteration 15120 Training loss 0.03686094284057617 Validation loss 0.036035843193531036 Accuracy 0.6370999813079834\n",
      "Iteration 15130 Training loss 0.03382575139403343 Validation loss 0.03574657067656517 Accuracy 0.6395999789237976\n",
      "Iteration 15140 Training loss 0.03655766323208809 Validation loss 0.0365632027387619 Accuracy 0.6320000290870667\n",
      "Iteration 15150 Training loss 0.03593304008245468 Validation loss 0.036226943135261536 Accuracy 0.6355999708175659\n",
      "Iteration 15160 Training loss 0.032054353505373 Validation loss 0.03584461659193039 Accuracy 0.6396999955177307\n",
      "Iteration 15170 Training loss 0.03553798049688339 Validation loss 0.03621523082256317 Accuracy 0.6362000107765198\n",
      "Iteration 15180 Training loss 0.03437408432364464 Validation loss 0.03588096797466278 Accuracy 0.6399000287055969\n",
      "Iteration 15190 Training loss 0.034257981926202774 Validation loss 0.0368269719183445 Accuracy 0.6297000050544739\n",
      "Iteration 15200 Training loss 0.03319492191076279 Validation loss 0.03577492758631706 Accuracy 0.6406999826431274\n",
      "Iteration 15210 Training loss 0.03372647985816002 Validation loss 0.0357816144824028 Accuracy 0.6398000121116638\n",
      "Iteration 15220 Training loss 0.03669479861855507 Validation loss 0.035955365747213364 Accuracy 0.6383000016212463\n",
      "Iteration 15230 Training loss 0.033293064683675766 Validation loss 0.03589056804776192 Accuracy 0.6384999752044678\n",
      "Iteration 15240 Training loss 0.030198048800230026 Validation loss 0.03600862994790077 Accuracy 0.6378999948501587\n",
      "Iteration 15250 Training loss 0.034081265330314636 Validation loss 0.0359768383204937 Accuracy 0.6388999819755554\n",
      "Iteration 15260 Training loss 0.037814911454916 Validation loss 0.03588101640343666 Accuracy 0.6394000053405762\n",
      "Iteration 15270 Training loss 0.03516557440161705 Validation loss 0.037067532539367676 Accuracy 0.6276999711990356\n",
      "Iteration 15280 Training loss 0.03425521031022072 Validation loss 0.036192044615745544 Accuracy 0.6363999843597412\n",
      "Iteration 15290 Training loss 0.037831734865903854 Validation loss 0.036179687827825546 Accuracy 0.6360999941825867\n",
      "Iteration 15300 Training loss 0.03527252376079559 Validation loss 0.03599628061056137 Accuracy 0.6377999782562256\n",
      "Iteration 15310 Training loss 0.03836463391780853 Validation loss 0.035921912640333176 Accuracy 0.6391000151634216\n",
      "Iteration 15320 Training loss 0.03441964089870453 Validation loss 0.035914257168769836 Accuracy 0.6394000053405762\n",
      "Iteration 15330 Training loss 0.03389763832092285 Validation loss 0.0358385369181633 Accuracy 0.6401000022888184\n",
      "Iteration 15340 Training loss 0.03836314007639885 Validation loss 0.03623218461871147 Accuracy 0.6355999708175659\n",
      "Iteration 15350 Training loss 0.03365084156394005 Validation loss 0.036259230226278305 Accuracy 0.6355999708175659\n",
      "Iteration 15360 Training loss 0.03648336976766586 Validation loss 0.036443520337343216 Accuracy 0.633400022983551\n",
      "Iteration 15370 Training loss 0.03477046638727188 Validation loss 0.036547061055898666 Accuracy 0.6312999725341797\n",
      "Iteration 15380 Training loss 0.03400726988911629 Validation loss 0.03577025234699249 Accuracy 0.6395999789237976\n",
      "Iteration 15390 Training loss 0.03253974765539169 Validation loss 0.035944219678640366 Accuracy 0.6381000280380249\n",
      "Iteration 15400 Training loss 0.03781551495194435 Validation loss 0.036959458142519 Accuracy 0.6276999711990356\n",
      "Iteration 15410 Training loss 0.03342833369970322 Validation loss 0.035761456936597824 Accuracy 0.6394000053405762\n",
      "Iteration 15420 Training loss 0.03285421058535576 Validation loss 0.035798657685518265 Accuracy 0.6392999887466431\n",
      "Iteration 15430 Training loss 0.032518867403268814 Validation loss 0.03591788187623024 Accuracy 0.6383000016212463\n",
      "Iteration 15440 Training loss 0.033308643847703934 Validation loss 0.03623294085264206 Accuracy 0.6351000070571899\n",
      "Iteration 15450 Training loss 0.032665811479091644 Validation loss 0.03562488406896591 Accuracy 0.6412000060081482\n",
      "Iteration 15460 Training loss 0.033725984394550323 Validation loss 0.03550942987203598 Accuracy 0.642300009727478\n",
      "Iteration 15470 Training loss 0.03142109885811806 Validation loss 0.035576846450567245 Accuracy 0.641700029373169\n",
      "Iteration 15480 Training loss 0.033981990069150925 Validation loss 0.03578370437026024 Accuracy 0.6402999758720398\n",
      "Iteration 15490 Training loss 0.035562288016080856 Validation loss 0.03577115386724472 Accuracy 0.6401000022888184\n",
      "Iteration 15500 Training loss 0.03252093866467476 Validation loss 0.03583231940865517 Accuracy 0.6399999856948853\n",
      "Iteration 15510 Training loss 0.034827571362257004 Validation loss 0.03565078228712082 Accuracy 0.6414999961853027\n",
      "Iteration 15520 Training loss 0.035789817571640015 Validation loss 0.036409877240657806 Accuracy 0.6340000033378601\n",
      "Iteration 15530 Training loss 0.03447655588388443 Validation loss 0.03556399047374725 Accuracy 0.6421999931335449\n",
      "Iteration 15540 Training loss 0.031357765197753906 Validation loss 0.035849086940288544 Accuracy 0.6398000121116638\n",
      "Iteration 15550 Training loss 0.035360489040613174 Validation loss 0.03576533868908882 Accuracy 0.6401000022888184\n",
      "Iteration 15560 Training loss 0.03282446414232254 Validation loss 0.035630956292152405 Accuracy 0.6419000029563904\n",
      "Iteration 15570 Training loss 0.03245936334133148 Validation loss 0.035879701375961304 Accuracy 0.6394000053405762\n",
      "Iteration 15580 Training loss 0.03384930267930031 Validation loss 0.035799965262413025 Accuracy 0.6391000151634216\n",
      "Iteration 15590 Training loss 0.03341694176197052 Validation loss 0.035642944276332855 Accuracy 0.6409000158309937\n",
      "Iteration 15600 Training loss 0.0371544323861599 Validation loss 0.03536904603242874 Accuracy 0.6435999870300293\n",
      "Iteration 15610 Training loss 0.03337407857179642 Validation loss 0.03575202822685242 Accuracy 0.6402999758720398\n",
      "Iteration 15620 Training loss 0.034233208745718 Validation loss 0.03573991358280182 Accuracy 0.6402000188827515\n",
      "Iteration 15630 Training loss 0.03611154854297638 Validation loss 0.03611491620540619 Accuracy 0.6359999775886536\n",
      "Iteration 15640 Training loss 0.03507636860013008 Validation loss 0.03601328283548355 Accuracy 0.6378999948501587\n",
      "Iteration 15650 Training loss 0.03408553823828697 Validation loss 0.035918816924095154 Accuracy 0.6385999917984009\n",
      "Iteration 15660 Training loss 0.03464391455054283 Validation loss 0.03591489419341087 Accuracy 0.6388999819755554\n",
      "Iteration 15670 Training loss 0.032085590064525604 Validation loss 0.03732943534851074 Accuracy 0.6241999864578247\n",
      "Iteration 15680 Training loss 0.03564970940351486 Validation loss 0.035540007054805756 Accuracy 0.6421999931335449\n",
      "Iteration 15690 Training loss 0.035079870373010635 Validation loss 0.03679903969168663 Accuracy 0.6292999982833862\n",
      "Iteration 15700 Training loss 0.036452777683734894 Validation loss 0.03730003163218498 Accuracy 0.6238999962806702\n",
      "Iteration 15710 Training loss 0.03647293895483017 Validation loss 0.03606288135051727 Accuracy 0.6366000175476074\n",
      "Iteration 15720 Training loss 0.03367754444479942 Validation loss 0.03603615239262581 Accuracy 0.6370999813079834\n",
      "Iteration 15730 Training loss 0.03575658053159714 Validation loss 0.03593020141124725 Accuracy 0.6384999752044678\n",
      "Iteration 15740 Training loss 0.035946790128946304 Validation loss 0.03535152226686478 Accuracy 0.6444000005722046\n",
      "Iteration 15750 Training loss 0.03405310586094856 Validation loss 0.035611383616924286 Accuracy 0.6407999992370605\n",
      "Iteration 15760 Training loss 0.031094970181584358 Validation loss 0.035689134150743484 Accuracy 0.640999972820282\n",
      "Iteration 15770 Training loss 0.03603614866733551 Validation loss 0.03573165833950043 Accuracy 0.6406000256538391\n",
      "Iteration 15780 Training loss 0.035102590918540955 Validation loss 0.0356404148042202 Accuracy 0.6413999795913696\n",
      "Iteration 15790 Training loss 0.03467678651213646 Validation loss 0.03642446920275688 Accuracy 0.6334999799728394\n",
      "Iteration 15800 Training loss 0.03865240514278412 Validation loss 0.035939183086156845 Accuracy 0.638700008392334\n",
      "Iteration 15810 Training loss 0.031077774241566658 Validation loss 0.03671979159116745 Accuracy 0.6301000118255615\n",
      "Iteration 15820 Training loss 0.035582538694143295 Validation loss 0.03548292815685272 Accuracy 0.6427000164985657\n",
      "Iteration 15830 Training loss 0.03099869191646576 Validation loss 0.03551440313458443 Accuracy 0.6419000029563904\n",
      "Iteration 15840 Training loss 0.03494676202535629 Validation loss 0.03592902049422264 Accuracy 0.6381999850273132\n",
      "Iteration 15850 Training loss 0.0312255397439003 Validation loss 0.03539183735847473 Accuracy 0.6439999938011169\n",
      "Iteration 15860 Training loss 0.033040497452020645 Validation loss 0.03563155606389046 Accuracy 0.6402000188827515\n",
      "Iteration 15870 Training loss 0.03205413743853569 Validation loss 0.035776346921920776 Accuracy 0.6399000287055969\n",
      "Iteration 15880 Training loss 0.03571566566824913 Validation loss 0.03551243245601654 Accuracy 0.642799973487854\n",
      "Iteration 15890 Training loss 0.038352444767951965 Validation loss 0.035708580166101456 Accuracy 0.6407999992370605\n",
      "Iteration 15900 Training loss 0.03660913184285164 Validation loss 0.035998180508613586 Accuracy 0.6378999948501587\n",
      "Iteration 15910 Training loss 0.036104995757341385 Validation loss 0.03594067692756653 Accuracy 0.6388000249862671\n",
      "Iteration 15920 Training loss 0.033820975571870804 Validation loss 0.03563018888235092 Accuracy 0.6414999961853027\n",
      "Iteration 15930 Training loss 0.0354217067360878 Validation loss 0.03627555072307587 Accuracy 0.6359000205993652\n",
      "Iteration 15940 Training loss 0.03634781017899513 Validation loss 0.03713580593466759 Accuracy 0.6254000067710876\n",
      "Iteration 15950 Training loss 0.03496484458446503 Validation loss 0.03581905737519264 Accuracy 0.6388999819755554\n",
      "Iteration 15960 Training loss 0.0398203581571579 Validation loss 0.038542840629816055 Accuracy 0.6123999953269958\n",
      "Iteration 15970 Training loss 0.03143998235464096 Validation loss 0.035578735172748566 Accuracy 0.641700029373169\n",
      "Iteration 15980 Training loss 0.033580560237169266 Validation loss 0.03608059883117676 Accuracy 0.6370000243186951\n",
      "Iteration 15990 Training loss 0.03167980909347534 Validation loss 0.035786908119916916 Accuracy 0.6402000188827515\n",
      "Iteration 16000 Training loss 0.03296836465597153 Validation loss 0.03524119034409523 Accuracy 0.6449999809265137\n",
      "Iteration 16010 Training loss 0.030942237004637718 Validation loss 0.035272903740406036 Accuracy 0.6446999907493591\n",
      "Iteration 16020 Training loss 0.036262307316064835 Validation loss 0.035839229822158813 Accuracy 0.6392999887466431\n",
      "Iteration 16030 Training loss 0.03542330488562584 Validation loss 0.035505298525094986 Accuracy 0.6421999931335449\n",
      "Iteration 16040 Training loss 0.03631572797894478 Validation loss 0.03525503724813461 Accuracy 0.6444000005722046\n",
      "Iteration 16050 Training loss 0.03380722180008888 Validation loss 0.03557639196515083 Accuracy 0.6416000127792358\n",
      "Iteration 16060 Training loss 0.03481816500425339 Validation loss 0.036139488220214844 Accuracy 0.6362000107765198\n",
      "Iteration 16070 Training loss 0.03607553988695145 Validation loss 0.03547770902514458 Accuracy 0.6424000263214111\n",
      "Iteration 16080 Training loss 0.033569857478141785 Validation loss 0.03562885895371437 Accuracy 0.640999972820282\n",
      "Iteration 16090 Training loss 0.032863982021808624 Validation loss 0.035713404417037964 Accuracy 0.640500009059906\n",
      "Iteration 16100 Training loss 0.03637239709496498 Validation loss 0.035701651126146317 Accuracy 0.640500009059906\n",
      "Iteration 16110 Training loss 0.03423113003373146 Validation loss 0.0366392508149147 Accuracy 0.6309000253677368\n",
      "Iteration 16120 Training loss 0.033751554787158966 Validation loss 0.03542781621217728 Accuracy 0.6434000134468079\n",
      "Iteration 16130 Training loss 0.03460574150085449 Validation loss 0.035643063485622406 Accuracy 0.6410999894142151\n",
      "Iteration 16140 Training loss 0.032721035182476044 Validation loss 0.035387005656957626 Accuracy 0.6430000066757202\n",
      "Iteration 16150 Training loss 0.0371447429060936 Validation loss 0.035601966083049774 Accuracy 0.6416000127792358\n",
      "Iteration 16160 Training loss 0.03404852747917175 Validation loss 0.03569769859313965 Accuracy 0.6403999924659729\n",
      "Iteration 16170 Training loss 0.03544054180383682 Validation loss 0.035430874675512314 Accuracy 0.6427000164985657\n",
      "Iteration 16180 Training loss 0.037870023399591446 Validation loss 0.0385248102247715 Accuracy 0.6115999817848206\n",
      "Iteration 16190 Training loss 0.028865791857242584 Validation loss 0.03691500425338745 Accuracy 0.6280999779701233\n",
      "Iteration 16200 Training loss 0.03535447269678116 Validation loss 0.035767585039138794 Accuracy 0.6402000188827515\n",
      "Iteration 16210 Training loss 0.03267441689968109 Validation loss 0.03530740737915039 Accuracy 0.6439999938011169\n",
      "Iteration 16220 Training loss 0.0352814644575119 Validation loss 0.03564341738820076 Accuracy 0.6403999924659729\n",
      "Iteration 16230 Training loss 0.033533189445734024 Validation loss 0.03542908653616905 Accuracy 0.642799973487854\n",
      "Iteration 16240 Training loss 0.0321175716817379 Validation loss 0.035248443484306335 Accuracy 0.6444000005722046\n",
      "Iteration 16250 Training loss 0.034172702580690384 Validation loss 0.03579295426607132 Accuracy 0.6384999752044678\n",
      "Iteration 16260 Training loss 0.03537672758102417 Validation loss 0.03578381985425949 Accuracy 0.6396999955177307\n",
      "Iteration 16270 Training loss 0.03684301674365997 Validation loss 0.035693228244781494 Accuracy 0.6410999894142151\n",
      "Iteration 16280 Training loss 0.03609006106853485 Validation loss 0.03571788966655731 Accuracy 0.6403999924659729\n",
      "Iteration 16290 Training loss 0.030394235625863075 Validation loss 0.03592947497963905 Accuracy 0.6378999948501587\n",
      "Iteration 16300 Training loss 0.034368954598903656 Validation loss 0.03635421022772789 Accuracy 0.6334999799728394\n",
      "Iteration 16310 Training loss 0.03389546275138855 Validation loss 0.035578157752752304 Accuracy 0.6413999795913696\n",
      "Iteration 16320 Training loss 0.035557545721530914 Validation loss 0.03541437163949013 Accuracy 0.6425999999046326\n",
      "Iteration 16330 Training loss 0.03348550572991371 Validation loss 0.03525347635149956 Accuracy 0.64410001039505\n",
      "Iteration 16340 Training loss 0.03233649581670761 Validation loss 0.03560967743396759 Accuracy 0.6406000256538391\n",
      "Iteration 16350 Training loss 0.03302321210503578 Validation loss 0.03549296036362648 Accuracy 0.641700029373169\n",
      "Iteration 16360 Training loss 0.036596402525901794 Validation loss 0.035513702780008316 Accuracy 0.6416000127792358\n",
      "Iteration 16370 Training loss 0.032911937683820724 Validation loss 0.03510988876223564 Accuracy 0.6462000012397766\n",
      "Iteration 16380 Training loss 0.03860652819275856 Validation loss 0.037858255207538605 Accuracy 0.6187000274658203\n",
      "Iteration 16390 Training loss 0.03219771012663841 Validation loss 0.03560693934559822 Accuracy 0.6412000060081482\n",
      "Iteration 16400 Training loss 0.03379053622484207 Validation loss 0.03510769084095955 Accuracy 0.6467000246047974\n",
      "Iteration 16410 Training loss 0.03324170410633087 Validation loss 0.035154879093170166 Accuracy 0.6449999809265137\n",
      "Iteration 16420 Training loss 0.03346819058060646 Validation loss 0.03532687574625015 Accuracy 0.6439999938011169\n",
      "Iteration 16430 Training loss 0.03460492938756943 Validation loss 0.0353863351047039 Accuracy 0.64410001039505\n",
      "Iteration 16440 Training loss 0.0349845327436924 Validation loss 0.03546677902340889 Accuracy 0.6414999961853027\n",
      "Iteration 16450 Training loss 0.03196283057332039 Validation loss 0.03504715487360954 Accuracy 0.6467999815940857\n",
      "Iteration 16460 Training loss 0.031124239787459373 Validation loss 0.035562530159950256 Accuracy 0.6416000127792358\n",
      "Iteration 16470 Training loss 0.03388109430670738 Validation loss 0.03607441484928131 Accuracy 0.6362000107765198\n",
      "Iteration 16480 Training loss 0.030940858647227287 Validation loss 0.03555722534656525 Accuracy 0.642300009727478\n",
      "Iteration 16490 Training loss 0.032992783933877945 Validation loss 0.035521507263183594 Accuracy 0.6424000263214111\n",
      "Iteration 16500 Training loss 0.03359031304717064 Validation loss 0.035652045160532 Accuracy 0.6401000022888184\n",
      "Iteration 16510 Training loss 0.03193733096122742 Validation loss 0.03610760718584061 Accuracy 0.6355000138282776\n",
      "Iteration 16520 Training loss 0.03325877711176872 Validation loss 0.035315338522195816 Accuracy 0.64410001039505\n",
      "Iteration 16530 Training loss 0.034499965608119965 Validation loss 0.035568587481975555 Accuracy 0.6407999992370605\n",
      "Iteration 16540 Training loss 0.0363241508603096 Validation loss 0.0355682335793972 Accuracy 0.6414999961853027\n",
      "Iteration 16550 Training loss 0.031855035573244095 Validation loss 0.03514116257429123 Accuracy 0.6449999809265137\n",
      "Iteration 16560 Training loss 0.03509027510881424 Validation loss 0.035309165716171265 Accuracy 0.64410001039505\n",
      "Iteration 16570 Training loss 0.03454473242163658 Validation loss 0.03518998250365257 Accuracy 0.6446999907493591\n",
      "Iteration 16580 Training loss 0.03218613564968109 Validation loss 0.03516356647014618 Accuracy 0.6459000110626221\n",
      "Iteration 16590 Training loss 0.03329786658287048 Validation loss 0.035789329558610916 Accuracy 0.6398000121116638\n",
      "Iteration 16600 Training loss 0.033956561237573624 Validation loss 0.03579895943403244 Accuracy 0.6395000219345093\n",
      "Iteration 16610 Training loss 0.03536815941333771 Validation loss 0.035686980932950974 Accuracy 0.6403999924659729\n",
      "Iteration 16620 Training loss 0.036184098571538925 Validation loss 0.0351017490029335 Accuracy 0.6471999883651733\n",
      "Iteration 16630 Training loss 0.0328899510204792 Validation loss 0.03570059314370155 Accuracy 0.6407999992370605\n",
      "Iteration 16640 Training loss 0.03420376032590866 Validation loss 0.035330239683389664 Accuracy 0.6437000036239624\n",
      "Iteration 16650 Training loss 0.033959124237298965 Validation loss 0.03586394712328911 Accuracy 0.6383000016212463\n",
      "Iteration 16660 Training loss 0.032937634736299515 Validation loss 0.03595850244164467 Accuracy 0.6370000243186951\n",
      "Iteration 16670 Training loss 0.03291960805654526 Validation loss 0.035695645958185196 Accuracy 0.6398000121116638\n",
      "Iteration 16680 Training loss 0.036157406866550446 Validation loss 0.03525765985250473 Accuracy 0.6438999772071838\n",
      "Iteration 16690 Training loss 0.035576917231082916 Validation loss 0.035430584102869034 Accuracy 0.6425999999046326\n",
      "Iteration 16700 Training loss 0.03281327337026596 Validation loss 0.03616826981306076 Accuracy 0.6353999972343445\n",
      "Iteration 16710 Training loss 0.034284722059965134 Validation loss 0.035568948835134506 Accuracy 0.6410999894142151\n",
      "Iteration 16720 Training loss 0.0351896733045578 Validation loss 0.035368021577596664 Accuracy 0.6428999900817871\n",
      "Iteration 16730 Training loss 0.03469325602054596 Validation loss 0.03641394153237343 Accuracy 0.6327999830245972\n",
      "Iteration 16740 Training loss 0.030132614076137543 Validation loss 0.035829029977321625 Accuracy 0.6384000182151794\n",
      "Iteration 16750 Training loss 0.03428516536951065 Validation loss 0.0354461707174778 Accuracy 0.6431000232696533\n",
      "Iteration 16760 Training loss 0.03501934930682182 Validation loss 0.03570680692791939 Accuracy 0.6395999789237976\n",
      "Iteration 16770 Training loss 0.0329282321035862 Validation loss 0.03615347295999527 Accuracy 0.6355999708175659\n",
      "Iteration 16780 Training loss 0.03206651657819748 Validation loss 0.03531897813081741 Accuracy 0.6438000202178955\n",
      "Iteration 16790 Training loss 0.036021262407302856 Validation loss 0.03585783764719963 Accuracy 0.6385999917984009\n",
      "Iteration 16800 Training loss 0.03496118634939194 Validation loss 0.036200255155563354 Accuracy 0.6349999904632568\n",
      "Iteration 16810 Training loss 0.03250676393508911 Validation loss 0.035402845591306686 Accuracy 0.6431999802589417\n",
      "Iteration 16820 Training loss 0.034110382199287415 Validation loss 0.03536451980471611 Accuracy 0.6438999772071838\n",
      "Iteration 16830 Training loss 0.03419648855924606 Validation loss 0.03514624387025833 Accuracy 0.6455000042915344\n",
      "Iteration 16840 Training loss 0.031080296263098717 Validation loss 0.035410888493061066 Accuracy 0.6427000164985657\n",
      "Iteration 16850 Training loss 0.03427004814147949 Validation loss 0.03557876497507095 Accuracy 0.6410999894142151\n",
      "Iteration 16860 Training loss 0.03058251366019249 Validation loss 0.03542965278029442 Accuracy 0.6420999765396118\n",
      "Iteration 16870 Training loss 0.03947057947516441 Validation loss 0.03746052831411362 Accuracy 0.6209999918937683\n",
      "Iteration 16880 Training loss 0.03441699594259262 Validation loss 0.03598814830183983 Accuracy 0.6363999843597412\n",
      "Iteration 16890 Training loss 0.03572874516248703 Validation loss 0.035630472004413605 Accuracy 0.6403999924659729\n",
      "Iteration 16900 Training loss 0.030679048970341682 Validation loss 0.03542463481426239 Accuracy 0.642799973487854\n",
      "Iteration 16910 Training loss 0.030880579724907875 Validation loss 0.035446710884571075 Accuracy 0.6424999833106995\n",
      "Iteration 16920 Training loss 0.03366141393780708 Validation loss 0.03573976457118988 Accuracy 0.6392999887466431\n",
      "Iteration 16930 Training loss 0.03423108905553818 Validation loss 0.03583994507789612 Accuracy 0.6385999917984009\n",
      "Iteration 16940 Training loss 0.03385104984045029 Validation loss 0.03531012311577797 Accuracy 0.6437000036239624\n",
      "Iteration 16950 Training loss 0.031575970351696014 Validation loss 0.03569220378994942 Accuracy 0.6402000188827515\n",
      "Iteration 16960 Training loss 0.03330613300204277 Validation loss 0.03516797348856926 Accuracy 0.6456999778747559\n",
      "Iteration 16970 Training loss 0.03324958682060242 Validation loss 0.03560288995504379 Accuracy 0.6407999992370605\n",
      "Iteration 16980 Training loss 0.03390146791934967 Validation loss 0.036352064460515976 Accuracy 0.6344000101089478\n",
      "Iteration 16990 Training loss 0.03325740620493889 Validation loss 0.035473525524139404 Accuracy 0.6420000195503235\n",
      "Iteration 17000 Training loss 0.033994004130363464 Validation loss 0.035251718014478683 Accuracy 0.6445000171661377\n",
      "Iteration 17010 Training loss 0.03020668216049671 Validation loss 0.035411372780799866 Accuracy 0.6424999833106995\n",
      "Iteration 17020 Training loss 0.03192458674311638 Validation loss 0.03519262745976448 Accuracy 0.644599974155426\n",
      "Iteration 17030 Training loss 0.03302411362528801 Validation loss 0.03532065823674202 Accuracy 0.6435999870300293\n",
      "Iteration 17040 Training loss 0.03412634879350662 Validation loss 0.035542719066143036 Accuracy 0.6419000029563904\n",
      "Iteration 17050 Training loss 0.03476748615503311 Validation loss 0.03545893728733063 Accuracy 0.6424000263214111\n",
      "Iteration 17060 Training loss 0.03598559647798538 Validation loss 0.0349462628364563 Accuracy 0.6470999717712402\n",
      "Iteration 17070 Training loss 0.034771304577589035 Validation loss 0.03506020829081535 Accuracy 0.6460000276565552\n",
      "Iteration 17080 Training loss 0.03294184431433678 Validation loss 0.03526481240987778 Accuracy 0.6435999870300293\n",
      "Iteration 17090 Training loss 0.03374161198735237 Validation loss 0.03557095676660538 Accuracy 0.6410999894142151\n",
      "Iteration 17100 Training loss 0.035662129521369934 Validation loss 0.035531893372535706 Accuracy 0.640999972820282\n",
      "Iteration 17110 Training loss 0.03215727582573891 Validation loss 0.0353056974709034 Accuracy 0.64410001039505\n",
      "Iteration 17120 Training loss 0.03449530899524689 Validation loss 0.0361790806055069 Accuracy 0.6345999836921692\n",
      "Iteration 17130 Training loss 0.035105571150779724 Validation loss 0.03547214716672897 Accuracy 0.6414999961853027\n",
      "Iteration 17140 Training loss 0.032674241811037064 Validation loss 0.03529473766684532 Accuracy 0.6439999938011169\n",
      "Iteration 17150 Training loss 0.035891227424144745 Validation loss 0.035163700580596924 Accuracy 0.6453999876976013\n",
      "Iteration 17160 Training loss 0.033717941492795944 Validation loss 0.03545088693499565 Accuracy 0.6420000195503235\n",
      "Iteration 17170 Training loss 0.03250637277960777 Validation loss 0.0354473739862442 Accuracy 0.6424999833106995\n",
      "Iteration 17180 Training loss 0.03468950465321541 Validation loss 0.03607559576630592 Accuracy 0.6367999911308289\n",
      "Iteration 17190 Training loss 0.033427536487579346 Validation loss 0.03530255705118179 Accuracy 0.6445000171661377\n",
      "Iteration 17200 Training loss 0.03410271182656288 Validation loss 0.03569068759679794 Accuracy 0.6402999758720398\n",
      "Iteration 17210 Training loss 0.03447487950325012 Validation loss 0.035839248448610306 Accuracy 0.6384000182151794\n",
      "Iteration 17220 Training loss 0.03042401373386383 Validation loss 0.035888321697711945 Accuracy 0.6388000249862671\n",
      "Iteration 17230 Training loss 0.03261084109544754 Validation loss 0.03552323207259178 Accuracy 0.6413000226020813\n",
      "Iteration 17240 Training loss 0.03453553840517998 Validation loss 0.03566041588783264 Accuracy 0.6402999758720398\n",
      "Iteration 17250 Training loss 0.03477385267615318 Validation loss 0.035829149186611176 Accuracy 0.6388000249862671\n",
      "Iteration 17260 Training loss 0.032213110476732254 Validation loss 0.03538546711206436 Accuracy 0.6434999704360962\n",
      "Iteration 17270 Training loss 0.03420261666178703 Validation loss 0.03587808832526207 Accuracy 0.6383000016212463\n",
      "Iteration 17280 Training loss 0.03689846396446228 Validation loss 0.0356244258582592 Accuracy 0.6413000226020813\n",
      "Iteration 17290 Training loss 0.034671101719141006 Validation loss 0.035498812794685364 Accuracy 0.642799973487854\n",
      "Iteration 17300 Training loss 0.034111637622117996 Validation loss 0.03563668951392174 Accuracy 0.6409000158309937\n",
      "Iteration 17310 Training loss 0.03449881449341774 Validation loss 0.035471390932798386 Accuracy 0.6421999931335449\n",
      "Iteration 17320 Training loss 0.03147798031568527 Validation loss 0.03509725630283356 Accuracy 0.6464999914169312\n",
      "Iteration 17330 Training loss 0.035927094519138336 Validation loss 0.03544597327709198 Accuracy 0.6431000232696533\n",
      "Iteration 17340 Training loss 0.03667706623673439 Validation loss 0.0355912446975708 Accuracy 0.6424999833106995\n",
      "Iteration 17350 Training loss 0.03292306140065193 Validation loss 0.035183750092983246 Accuracy 0.6456000208854675\n",
      "Iteration 17360 Training loss 0.03499746695160866 Validation loss 0.03523171320557594 Accuracy 0.6456000208854675\n",
      "Iteration 17370 Training loss 0.03500520437955856 Validation loss 0.035996224731206894 Accuracy 0.637499988079071\n",
      "Iteration 17380 Training loss 0.03483664244413376 Validation loss 0.03561163321137428 Accuracy 0.6401000022888184\n",
      "Iteration 17390 Training loss 0.03301716595888138 Validation loss 0.03526528179645538 Accuracy 0.6449999809265137\n",
      "Iteration 17400 Training loss 0.030340353026986122 Validation loss 0.03573404252529144 Accuracy 0.6396999955177307\n",
      "Iteration 17410 Training loss 0.031326811760663986 Validation loss 0.035293348133563995 Accuracy 0.644599974155426\n",
      "Iteration 17420 Training loss 0.032432425767183304 Validation loss 0.03538059815764427 Accuracy 0.6424999833106995\n",
      "Iteration 17430 Training loss 0.03475591167807579 Validation loss 0.036064546555280685 Accuracy 0.635699987411499\n",
      "Iteration 17440 Training loss 0.030414484441280365 Validation loss 0.035406146198511124 Accuracy 0.642300009727478\n",
      "Iteration 17450 Training loss 0.031847745180130005 Validation loss 0.03502888232469559 Accuracy 0.6460999846458435\n",
      "Iteration 17460 Training loss 0.02992180362343788 Validation loss 0.035298265516757965 Accuracy 0.6431000232696533\n",
      "Iteration 17470 Training loss 0.03324190899729729 Validation loss 0.03492414578795433 Accuracy 0.6473000049591064\n",
      "Iteration 17480 Training loss 0.03207873925566673 Validation loss 0.03509189933538437 Accuracy 0.6450999975204468\n",
      "Iteration 17490 Training loss 0.03216632455587387 Validation loss 0.03525185212492943 Accuracy 0.6446999907493591\n",
      "Iteration 17500 Training loss 0.0317998081445694 Validation loss 0.035248417407274246 Accuracy 0.64410001039505\n",
      "Iteration 17510 Training loss 0.03157313913106918 Validation loss 0.03545178100466728 Accuracy 0.6416000127792358\n",
      "Iteration 17520 Training loss 0.03850135952234268 Validation loss 0.03781410679221153 Accuracy 0.6176999807357788\n",
      "Iteration 17530 Training loss 0.03548293560743332 Validation loss 0.0358380451798439 Accuracy 0.6381999850273132\n",
      "Iteration 17540 Training loss 0.03447158634662628 Validation loss 0.03678431734442711 Accuracy 0.6276999711990356\n",
      "Iteration 17550 Training loss 0.03356287255883217 Validation loss 0.033894218504428864 Accuracy 0.6561999917030334\n",
      "Iteration 17560 Training loss 0.03133239597082138 Validation loss 0.03526404872536659 Accuracy 0.644599974155426\n",
      "Iteration 17570 Training loss 0.031418152153491974 Validation loss 0.034142881631851196 Accuracy 0.6553999781608582\n",
      "Iteration 17580 Training loss 0.03154611587524414 Validation loss 0.03334571421146393 Accuracy 0.6628999710083008\n",
      "Iteration 17590 Training loss 0.03141685575246811 Validation loss 0.03395181521773338 Accuracy 0.6575000286102295\n",
      "Iteration 17600 Training loss 0.03191036731004715 Validation loss 0.03390798717737198 Accuracy 0.6578999757766724\n",
      "Iteration 17610 Training loss 0.03119083121418953 Validation loss 0.0345304012298584 Accuracy 0.6514000296592712\n",
      "Iteration 17620 Training loss 0.030763298273086548 Validation loss 0.03338931128382683 Accuracy 0.6625000238418579\n",
      "Iteration 17630 Training loss 0.03243183717131615 Validation loss 0.033850401639938354 Accuracy 0.6579999923706055\n",
      "Iteration 17640 Training loss 0.031715020537376404 Validation loss 0.033574581146240234 Accuracy 0.6607999801635742\n",
      "Iteration 17650 Training loss 0.030663203448057175 Validation loss 0.03354591876268387 Accuracy 0.661300003528595\n",
      "Iteration 17660 Training loss 0.02894715964794159 Validation loss 0.03249217942357063 Accuracy 0.6711999773979187\n",
      "Iteration 17670 Training loss 0.030091531574726105 Validation loss 0.03319666534662247 Accuracy 0.6639999747276306\n",
      "Iteration 17680 Training loss 0.03206596523523331 Validation loss 0.033258356153964996 Accuracy 0.6646000146865845\n",
      "Iteration 17690 Training loss 0.03171933814883232 Validation loss 0.033439889550209045 Accuracy 0.6615999937057495\n",
      "Iteration 17700 Training loss 0.03506911173462868 Validation loss 0.03357713297009468 Accuracy 0.6603999733924866\n",
      "Iteration 17710 Training loss 0.0311060082167387 Validation loss 0.03277599439024925 Accuracy 0.6682999730110168\n",
      "Iteration 17720 Training loss 0.0313410684466362 Validation loss 0.033643025904893875 Accuracy 0.6600000262260437\n",
      "Iteration 17730 Training loss 0.03289259225130081 Validation loss 0.03294961154460907 Accuracy 0.6656000018119812\n",
      "Iteration 17740 Training loss 0.032470282167196274 Validation loss 0.033801671117544174 Accuracy 0.6577000021934509\n",
      "Iteration 17750 Training loss 0.035219501703977585 Validation loss 0.0339185930788517 Accuracy 0.6572999954223633\n",
      "Iteration 17760 Training loss 0.032085951417684555 Validation loss 0.032970719039440155 Accuracy 0.6671000123023987\n",
      "Iteration 17770 Training loss 0.03477490320801735 Validation loss 0.03291824087500572 Accuracy 0.6671000123023987\n",
      "Iteration 17780 Training loss 0.028829429298639297 Validation loss 0.03278106078505516 Accuracy 0.6687999963760376\n",
      "Iteration 17790 Training loss 0.03304489329457283 Validation loss 0.03272738680243492 Accuracy 0.6692000031471252\n",
      "Iteration 17800 Training loss 0.032998014241456985 Validation loss 0.032726578414440155 Accuracy 0.6696000099182129\n",
      "Iteration 17810 Training loss 0.031077245250344276 Validation loss 0.03286676108837128 Accuracy 0.6680999994277954\n",
      "Iteration 17820 Training loss 0.0320197269320488 Validation loss 0.03376011177897453 Accuracy 0.6589999794960022\n",
      "Iteration 17830 Training loss 0.028665320947766304 Validation loss 0.033211927860975266 Accuracy 0.6639000177383423\n",
      "Iteration 17840 Training loss 0.03218025714159012 Validation loss 0.03395168110728264 Accuracy 0.6565999984741211\n",
      "Iteration 17850 Training loss 0.032156746834516525 Validation loss 0.03280491754412651 Accuracy 0.6682000160217285\n",
      "Iteration 17860 Training loss 0.03019920364022255 Validation loss 0.03288894519209862 Accuracy 0.6672999858856201\n",
      "Iteration 17870 Training loss 0.03604259714484215 Validation loss 0.037126533687114716 Accuracy 0.6244999766349792\n",
      "Iteration 17880 Training loss 0.03385322913527489 Validation loss 0.03493371978402138 Accuracy 0.6467000246047974\n",
      "Iteration 17890 Training loss 0.033059261739254 Validation loss 0.03317716717720032 Accuracy 0.6642000079154968\n",
      "Iteration 17900 Training loss 0.02970099076628685 Validation loss 0.033295854926109314 Accuracy 0.6638000011444092\n",
      "Iteration 17910 Training loss 0.03076740726828575 Validation loss 0.03273788094520569 Accuracy 0.669700026512146\n",
      "Iteration 17920 Training loss 0.03425858914852142 Validation loss 0.034330353140830994 Accuracy 0.6528000235557556\n",
      "Iteration 17930 Training loss 0.035286009311676025 Validation loss 0.03351417928934097 Accuracy 0.6611999869346619\n",
      "Iteration 17940 Training loss 0.028486410155892372 Validation loss 0.0329604335129261 Accuracy 0.6662999987602234\n",
      "Iteration 17950 Training loss 0.030528932809829712 Validation loss 0.032674022018909454 Accuracy 0.6686999797821045\n",
      "Iteration 17960 Training loss 0.032336510717868805 Validation loss 0.03253550827503204 Accuracy 0.670799970626831\n",
      "Iteration 17970 Training loss 0.03157026320695877 Validation loss 0.033206235617399216 Accuracy 0.6646999716758728\n",
      "Iteration 17980 Training loss 0.031491003930568695 Validation loss 0.03275614604353905 Accuracy 0.6686000227928162\n",
      "Iteration 17990 Training loss 0.031056851148605347 Validation loss 0.032765716314315796 Accuracy 0.6693999767303467\n",
      "Iteration 18000 Training loss 0.03055119514465332 Validation loss 0.03375818580389023 Accuracy 0.6586999893188477\n",
      "Iteration 18010 Training loss 0.029950803145766258 Validation loss 0.0331999808549881 Accuracy 0.6636999845504761\n",
      "Iteration 18020 Training loss 0.030829764902591705 Validation loss 0.032736074179410934 Accuracy 0.6690000295639038\n",
      "Iteration 18030 Training loss 0.03446436673402786 Validation loss 0.03376244381070137 Accuracy 0.6582000255584717\n",
      "Iteration 18040 Training loss 0.030072659254074097 Validation loss 0.033083315938711166 Accuracy 0.6658999919891357\n",
      "Iteration 18050 Training loss 0.028244128450751305 Validation loss 0.032314516603946686 Accuracy 0.6728000044822693\n",
      "Iteration 18060 Training loss 0.0330343022942543 Validation loss 0.03318013250827789 Accuracy 0.6650000214576721\n",
      "Iteration 18070 Training loss 0.03036872111260891 Validation loss 0.03348428010940552 Accuracy 0.6614000201225281\n",
      "Iteration 18080 Training loss 0.03131438419222832 Validation loss 0.033019885420799255 Accuracy 0.6661999821662903\n",
      "Iteration 18090 Training loss 0.03333083912730217 Validation loss 0.03520361706614494 Accuracy 0.6444000005722046\n",
      "Iteration 18100 Training loss 0.03055596724152565 Validation loss 0.03273874893784523 Accuracy 0.6699000000953674\n",
      "Iteration 18110 Training loss 0.030797861516475677 Validation loss 0.03213629499077797 Accuracy 0.6751000285148621\n",
      "Iteration 18120 Training loss 0.0313255712389946 Validation loss 0.03257773816585541 Accuracy 0.6696000099182129\n",
      "Iteration 18130 Training loss 0.03032330609858036 Validation loss 0.03321463242173195 Accuracy 0.6626999974250793\n",
      "Iteration 18140 Training loss 0.032020390033721924 Validation loss 0.03217357397079468 Accuracy 0.6741999983787537\n",
      "Iteration 18150 Training loss 0.028756171464920044 Validation loss 0.031896572560071945 Accuracy 0.677299976348877\n",
      "Iteration 18160 Training loss 0.03148729354143143 Validation loss 0.03458540886640549 Accuracy 0.6506999731063843\n",
      "Iteration 18170 Training loss 0.030782857909798622 Validation loss 0.032196734100580215 Accuracy 0.6748999953269958\n",
      "Iteration 18180 Training loss 0.031108221039175987 Validation loss 0.03251779079437256 Accuracy 0.6712999939918518\n",
      "Iteration 18190 Training loss 0.02624197117984295 Validation loss 0.032211240381002426 Accuracy 0.6744999885559082\n",
      "Iteration 18200 Training loss 0.027164556086063385 Validation loss 0.033421073108911514 Accuracy 0.6621999740600586\n",
      "Iteration 18210 Training loss 0.03222325071692467 Validation loss 0.032481685280799866 Accuracy 0.6710000038146973\n",
      "Iteration 18220 Training loss 0.03028303012251854 Validation loss 0.032908570021390915 Accuracy 0.6675999760627747\n",
      "Iteration 18230 Training loss 0.030564073473215103 Validation loss 0.03261026367545128 Accuracy 0.6704999804496765\n",
      "Iteration 18240 Training loss 0.03170129656791687 Validation loss 0.03395263850688934 Accuracy 0.6557999849319458\n",
      "Iteration 18250 Training loss 0.0324489064514637 Validation loss 0.03205355629324913 Accuracy 0.6751999855041504\n",
      "Iteration 18260 Training loss 0.028768645599484444 Validation loss 0.032061170786619186 Accuracy 0.6764000058174133\n",
      "Iteration 18270 Training loss 0.031602345407009125 Validation loss 0.03260115906596184 Accuracy 0.6705999970436096\n",
      "Iteration 18280 Training loss 0.029029853641986847 Validation loss 0.03218434751033783 Accuracy 0.6751000285148621\n",
      "Iteration 18290 Training loss 0.030951229855418205 Validation loss 0.0329509973526001 Accuracy 0.6664999723434448\n",
      "Iteration 18300 Training loss 0.030660782009363174 Validation loss 0.03285469114780426 Accuracy 0.6672999858856201\n",
      "Iteration 18310 Training loss 0.028409769758582115 Validation loss 0.03258702531456947 Accuracy 0.6704999804496765\n",
      "Iteration 18320 Training loss 0.03210768848657608 Validation loss 0.031900689005851746 Accuracy 0.6780999898910522\n",
      "Iteration 18330 Training loss 0.03263328969478607 Validation loss 0.03254189342260361 Accuracy 0.6721000075340271\n",
      "Iteration 18340 Training loss 0.031147561967372894 Validation loss 0.03369724750518799 Accuracy 0.6603000164031982\n",
      "Iteration 18350 Training loss 0.029322637245059013 Validation loss 0.03328507021069527 Accuracy 0.6633999943733215\n",
      "Iteration 18360 Training loss 0.030897334218025208 Validation loss 0.03175445273518562 Accuracy 0.6790000200271606\n",
      "Iteration 18370 Training loss 0.030754651874303818 Validation loss 0.03198504447937012 Accuracy 0.6757000088691711\n",
      "Iteration 18380 Training loss 0.029936401173472404 Validation loss 0.032251372933387756 Accuracy 0.6740000247955322\n",
      "Iteration 18390 Training loss 0.030896971002221107 Validation loss 0.03212061896920204 Accuracy 0.6739000082015991\n",
      "Iteration 18400 Training loss 0.02935795858502388 Validation loss 0.032052181661129 Accuracy 0.6758000254631042\n",
      "Iteration 18410 Training loss 0.030092870816588402 Validation loss 0.032180000096559525 Accuracy 0.6739000082015991\n",
      "Iteration 18420 Training loss 0.029595540836453438 Validation loss 0.0321911945939064 Accuracy 0.6739000082015991\n",
      "Iteration 18430 Training loss 0.03074819967150688 Validation loss 0.032154954969882965 Accuracy 0.6743999719619751\n",
      "Iteration 18440 Training loss 0.026716893538832664 Validation loss 0.03292662277817726 Accuracy 0.6679999828338623\n",
      "Iteration 18450 Training loss 0.02988412044942379 Validation loss 0.03241793438792229 Accuracy 0.6718999743461609\n",
      "Iteration 18460 Training loss 0.028284024447202682 Validation loss 0.032522052526474 Accuracy 0.6704000234603882\n",
      "Iteration 18470 Training loss 0.030351532623171806 Validation loss 0.03202371671795845 Accuracy 0.6766999959945679\n",
      "Iteration 18480 Training loss 0.029030047357082367 Validation loss 0.03252020850777626 Accuracy 0.6704000234603882\n",
      "Iteration 18490 Training loss 0.033127542585134506 Validation loss 0.032628413289785385 Accuracy 0.6699000000953674\n",
      "Iteration 18500 Training loss 0.029915038496255875 Validation loss 0.031873006373643875 Accuracy 0.6780999898910522\n",
      "Iteration 18510 Training loss 0.02992376871407032 Validation loss 0.03226549178361893 Accuracy 0.6740000247955322\n",
      "Iteration 18520 Training loss 0.030701523646712303 Validation loss 0.03272055462002754 Accuracy 0.6686999797821045\n",
      "Iteration 18530 Training loss 0.02861551195383072 Validation loss 0.033044490963220596 Accuracy 0.6654000282287598\n",
      "Iteration 18540 Training loss 0.027394907549023628 Validation loss 0.03227127715945244 Accuracy 0.6733999848365784\n",
      "Iteration 18550 Training loss 0.03025181032717228 Validation loss 0.031776659190654755 Accuracy 0.678600013256073\n",
      "Iteration 18560 Training loss 0.02970105968415737 Validation loss 0.03236929327249527 Accuracy 0.6723999977111816\n",
      "Iteration 18570 Training loss 0.030636552721261978 Validation loss 0.03203470632433891 Accuracy 0.6761999726295471\n",
      "Iteration 18580 Training loss 0.03232802823185921 Validation loss 0.032137881964445114 Accuracy 0.675599992275238\n",
      "Iteration 18590 Training loss 0.031883686780929565 Validation loss 0.03317062556743622 Accuracy 0.6653000116348267\n",
      "Iteration 18600 Training loss 0.029810460284352303 Validation loss 0.032510463148355484 Accuracy 0.6714000105857849\n",
      "Iteration 18610 Training loss 0.03116949275135994 Validation loss 0.03189762309193611 Accuracy 0.6773999929428101\n",
      "Iteration 18620 Training loss 0.032099202275276184 Validation loss 0.03316550701856613 Accuracy 0.6660000085830688\n",
      "Iteration 18630 Training loss 0.03229471668601036 Validation loss 0.03407708927989006 Accuracy 0.6564000248908997\n",
      "Iteration 18640 Training loss 0.03614794835448265 Validation loss 0.03395288437604904 Accuracy 0.6560999751091003\n",
      "Iteration 18650 Training loss 0.03263363987207413 Validation loss 0.03258958086371422 Accuracy 0.669700026512146\n",
      "Iteration 18660 Training loss 0.030208999291062355 Validation loss 0.03190050274133682 Accuracy 0.6777999997138977\n",
      "Iteration 18670 Training loss 0.027521606534719467 Validation loss 0.03211406245827675 Accuracy 0.6757000088691711\n",
      "Iteration 18680 Training loss 0.031201399862766266 Validation loss 0.0330200158059597 Accuracy 0.6671000123023987\n",
      "Iteration 18690 Training loss 0.030068499967455864 Validation loss 0.032100629061460495 Accuracy 0.676800012588501\n",
      "Iteration 18700 Training loss 0.02936571277678013 Validation loss 0.03204192966222763 Accuracy 0.6771000027656555\n",
      "Iteration 18710 Training loss 0.03104608878493309 Validation loss 0.032469019293785095 Accuracy 0.6715999841690063\n",
      "Iteration 18720 Training loss 0.030153609812259674 Validation loss 0.032965920865535736 Accuracy 0.6675000190734863\n",
      "Iteration 18730 Training loss 0.030255930498242378 Validation loss 0.03282962739467621 Accuracy 0.6675999760627747\n",
      "Iteration 18740 Training loss 0.029803818091750145 Validation loss 0.03413963317871094 Accuracy 0.6556000113487244\n",
      "Iteration 18750 Training loss 0.028814278542995453 Validation loss 0.03177497908473015 Accuracy 0.6789000034332275\n",
      "Iteration 18760 Training loss 0.033156342804431915 Validation loss 0.031932439655065536 Accuracy 0.6782000064849854\n",
      "Iteration 18770 Training loss 0.029055749997496605 Validation loss 0.031867772340774536 Accuracy 0.6786999702453613\n",
      "Iteration 18780 Training loss 0.029437070712447166 Validation loss 0.03235946223139763 Accuracy 0.6733999848365784\n",
      "Iteration 18790 Training loss 0.033384211361408234 Validation loss 0.03291625902056694 Accuracy 0.6687999963760376\n",
      "Iteration 18800 Training loss 0.031566984951496124 Validation loss 0.032229259610176086 Accuracy 0.6754999756813049\n",
      "Iteration 18810 Training loss 0.032260529696941376 Validation loss 0.03235090151429176 Accuracy 0.6732000112533569\n",
      "Iteration 18820 Training loss 0.0313134603202343 Validation loss 0.032409533858299255 Accuracy 0.671999990940094\n",
      "Iteration 18830 Training loss 0.03199813514947891 Validation loss 0.031959131360054016 Accuracy 0.6765000224113464\n",
      "Iteration 18840 Training loss 0.030513886362314224 Validation loss 0.03233404457569122 Accuracy 0.6728000044822693\n",
      "Iteration 18850 Training loss 0.030434178188443184 Validation loss 0.0323074646294117 Accuracy 0.6735000014305115\n",
      "Iteration 18860 Training loss 0.031016936525702477 Validation loss 0.03249829262495041 Accuracy 0.6710000038146973\n",
      "Iteration 18870 Training loss 0.02748838998377323 Validation loss 0.03162861242890358 Accuracy 0.6794000267982483\n",
      "Iteration 18880 Training loss 0.03415904566645622 Validation loss 0.03527950122952461 Accuracy 0.6444000005722046\n",
      "Iteration 18890 Training loss 0.029181910678744316 Validation loss 0.0318865142762661 Accuracy 0.6776000261306763\n",
      "Iteration 18900 Training loss 0.030715635046362877 Validation loss 0.03220445662736893 Accuracy 0.6743999719619751\n",
      "Iteration 18910 Training loss 0.030785929411649704 Validation loss 0.03251725062727928 Accuracy 0.6704000234603882\n",
      "Iteration 18920 Training loss 0.03197455406188965 Validation loss 0.03282712772488594 Accuracy 0.6676999926567078\n",
      "Iteration 18930 Training loss 0.027529140934348106 Validation loss 0.03240392729640007 Accuracy 0.6726999878883362\n",
      "Iteration 18940 Training loss 0.03072980046272278 Validation loss 0.03249334916472435 Accuracy 0.6723999977111816\n",
      "Iteration 18950 Training loss 0.030808867886662483 Validation loss 0.03182445466518402 Accuracy 0.6779999732971191\n",
      "Iteration 18960 Training loss 0.029670855030417442 Validation loss 0.03173433989286423 Accuracy 0.6783999800682068\n",
      "Iteration 18970 Training loss 0.031797558069229126 Validation loss 0.03175682574510574 Accuracy 0.6789000034332275\n",
      "Iteration 18980 Training loss 0.029636042192578316 Validation loss 0.03167296573519707 Accuracy 0.679099977016449\n",
      "Iteration 18990 Training loss 0.03114832006394863 Validation loss 0.032079048454761505 Accuracy 0.6754999756813049\n",
      "Iteration 19000 Training loss 0.029794199392199516 Validation loss 0.03245876729488373 Accuracy 0.6708999872207642\n",
      "Iteration 19010 Training loss 0.030402258038520813 Validation loss 0.033926717936992645 Accuracy 0.6572999954223633\n",
      "Iteration 19020 Training loss 0.030349982902407646 Validation loss 0.0320633202791214 Accuracy 0.6754000186920166\n",
      "Iteration 19030 Training loss 0.026816416531801224 Validation loss 0.031890448182821274 Accuracy 0.6771000027656555\n",
      "Iteration 19040 Training loss 0.02993139624595642 Validation loss 0.0320117324590683 Accuracy 0.6765000224113464\n",
      "Iteration 19050 Training loss 0.032418087124824524 Validation loss 0.033137645572423935 Accuracy 0.6650999784469604\n",
      "Iteration 19060 Training loss 0.032497625797986984 Validation loss 0.032506126910448074 Accuracy 0.6700000166893005\n",
      "Iteration 19070 Training loss 0.03175778314471245 Validation loss 0.031787361949682236 Accuracy 0.6783000230789185\n",
      "Iteration 19080 Training loss 0.02867862395942211 Validation loss 0.032752979546785355 Accuracy 0.6680999994277954\n",
      "Iteration 19090 Training loss 0.028110461309552193 Validation loss 0.03241598233580589 Accuracy 0.6718999743461609\n",
      "Iteration 19100 Training loss 0.028016814962029457 Validation loss 0.033432163298130035 Accuracy 0.6629999876022339\n",
      "Iteration 19110 Training loss 0.028949473053216934 Validation loss 0.031946051865816116 Accuracy 0.6773999929428101\n",
      "Iteration 19120 Training loss 0.03149363026022911 Validation loss 0.032677944749593735 Accuracy 0.670199990272522\n",
      "Iteration 19130 Training loss 0.032394878566265106 Validation loss 0.03269800916314125 Accuracy 0.6692000031471252\n",
      "Iteration 19140 Training loss 0.03148249164223671 Validation loss 0.03196953237056732 Accuracy 0.6769000291824341\n",
      "Iteration 19150 Training loss 0.03051912412047386 Validation loss 0.032679539173841476 Accuracy 0.6700999736785889\n",
      "Iteration 19160 Training loss 0.03238362446427345 Validation loss 0.03199474513530731 Accuracy 0.677299976348877\n",
      "Iteration 19170 Training loss 0.034350987523794174 Validation loss 0.033319298177957535 Accuracy 0.6638000011444092\n",
      "Iteration 19180 Training loss 0.025296837091445923 Validation loss 0.03163258358836174 Accuracy 0.6801000237464905\n",
      "Iteration 19190 Training loss 0.02976354770362377 Validation loss 0.03183279186487198 Accuracy 0.6783999800682068\n",
      "Iteration 19200 Training loss 0.0301262978464365 Validation loss 0.032283734530210495 Accuracy 0.6739000082015991\n",
      "Iteration 19210 Training loss 0.030041327700018883 Validation loss 0.03161220625042915 Accuracy 0.6801000237464905\n",
      "Iteration 19220 Training loss 0.030338872224092484 Validation loss 0.03200486674904823 Accuracy 0.676800012588501\n",
      "Iteration 19230 Training loss 0.030849747359752655 Validation loss 0.03204159811139107 Accuracy 0.6765999794006348\n",
      "Iteration 19240 Training loss 0.03166474401950836 Validation loss 0.032031841576099396 Accuracy 0.676800012588501\n",
      "Iteration 19250 Training loss 0.03276852145791054 Validation loss 0.03263819217681885 Accuracy 0.6705999970436096\n",
      "Iteration 19260 Training loss 0.03168999403715134 Validation loss 0.031713683158159256 Accuracy 0.6797000169754028\n",
      "Iteration 19270 Training loss 0.028210800141096115 Validation loss 0.03185936436057091 Accuracy 0.6783000230789185\n",
      "Iteration 19280 Training loss 0.030684666708111763 Validation loss 0.032013509422540665 Accuracy 0.6771000027656555\n",
      "Iteration 19290 Training loss 0.029084451496601105 Validation loss 0.03248366340994835 Accuracy 0.6710000038146973\n",
      "Iteration 19300 Training loss 0.02752263844013214 Validation loss 0.03337576985359192 Accuracy 0.6636000275611877\n",
      "Iteration 19310 Training loss 0.02869407646358013 Validation loss 0.032401349395513535 Accuracy 0.6733999848365784\n",
      "Iteration 19320 Training loss 0.02837900072336197 Validation loss 0.03300308436155319 Accuracy 0.6662999987602234\n",
      "Iteration 19330 Training loss 0.028117744252085686 Validation loss 0.03202398866415024 Accuracy 0.6764000058174133\n",
      "Iteration 19340 Training loss 0.033141374588012695 Validation loss 0.0329667292535305 Accuracy 0.6674000024795532\n",
      "Iteration 19350 Training loss 0.028934305533766747 Validation loss 0.031501177698373795 Accuracy 0.6818000078201294\n",
      "Iteration 19360 Training loss 0.030578454956412315 Validation loss 0.03233771398663521 Accuracy 0.6725999712944031\n",
      "Iteration 19370 Training loss 0.030402719974517822 Validation loss 0.03239499405026436 Accuracy 0.6725999712944031\n",
      "Iteration 19380 Training loss 0.030751971527934074 Validation loss 0.03144397586584091 Accuracy 0.6826000213623047\n",
      "Iteration 19390 Training loss 0.03225113824009895 Validation loss 0.032086003571748734 Accuracy 0.6754999756813049\n",
      "Iteration 19400 Training loss 0.032046593725681305 Validation loss 0.0316164568066597 Accuracy 0.680899977684021\n",
      "Iteration 19410 Training loss 0.03040938824415207 Validation loss 0.03160111978650093 Accuracy 0.6802999973297119\n",
      "Iteration 19420 Training loss 0.02664126642048359 Validation loss 0.03154291957616806 Accuracy 0.6808000206947327\n",
      "Iteration 19430 Training loss 0.02921278402209282 Validation loss 0.03154755383729935 Accuracy 0.680400013923645\n",
      "Iteration 19440 Training loss 0.027501268312335014 Validation loss 0.031528301537036896 Accuracy 0.6801999807357788\n",
      "Iteration 19450 Training loss 0.030633114278316498 Validation loss 0.03234259411692619 Accuracy 0.6725999712944031\n",
      "Iteration 19460 Training loss 0.03185737505555153 Validation loss 0.033246830105781555 Accuracy 0.6628999710083008\n",
      "Iteration 19470 Training loss 0.03156324103474617 Validation loss 0.031979482620954514 Accuracy 0.6762999892234802\n",
      "Iteration 19480 Training loss 0.030644403770565987 Validation loss 0.03183094784617424 Accuracy 0.6765999794006348\n",
      "Iteration 19490 Training loss 0.02579684928059578 Validation loss 0.03171456232666969 Accuracy 0.6793000102043152\n",
      "Iteration 19500 Training loss 0.030513480305671692 Validation loss 0.03171442449092865 Accuracy 0.679099977016449\n",
      "Iteration 19510 Training loss 0.029943881556391716 Validation loss 0.03215969726443291 Accuracy 0.6751000285148621\n",
      "Iteration 19520 Training loss 0.032114919275045395 Validation loss 0.03153305500745773 Accuracy 0.6814000010490417\n",
      "Iteration 19530 Training loss 0.026683561503887177 Validation loss 0.03179927170276642 Accuracy 0.678600013256073\n",
      "Iteration 19540 Training loss 0.030306609347462654 Validation loss 0.0325675792992115 Accuracy 0.6699000000953674\n",
      "Iteration 19550 Training loss 0.029340146109461784 Validation loss 0.03162677213549614 Accuracy 0.6794000267982483\n",
      "Iteration 19560 Training loss 0.03003442846238613 Validation loss 0.03187111020088196 Accuracy 0.6771000027656555\n",
      "Iteration 19570 Training loss 0.029402051120996475 Validation loss 0.03183130547404289 Accuracy 0.677299976348877\n",
      "Iteration 19580 Training loss 0.033345192670822144 Validation loss 0.03210721164941788 Accuracy 0.6751999855041504\n",
      "Iteration 19590 Training loss 0.03222661837935448 Validation loss 0.03216060996055603 Accuracy 0.6740000247955322\n",
      "Iteration 19600 Training loss 0.029159672558307648 Validation loss 0.03294571116566658 Accuracy 0.666100025177002\n",
      "Iteration 19610 Training loss 0.02516697347164154 Validation loss 0.03145650029182434 Accuracy 0.6822999715805054\n",
      "Iteration 19620 Training loss 0.029742177575826645 Validation loss 0.032455768436193466 Accuracy 0.6718999743461609\n",
      "Iteration 19630 Training loss 0.03353046998381615 Validation loss 0.03264397755265236 Accuracy 0.6700999736785889\n",
      "Iteration 19640 Training loss 0.029786420986056328 Validation loss 0.031252212822437286 Accuracy 0.6836000084877014\n",
      "Iteration 19650 Training loss 0.025635050609707832 Validation loss 0.031714603304862976 Accuracy 0.680400013923645\n",
      "Iteration 19660 Training loss 0.0285891592502594 Validation loss 0.031985290348529816 Accuracy 0.6758999824523926\n",
      "Iteration 19670 Training loss 0.02753530628979206 Validation loss 0.0321185402572155 Accuracy 0.6740999817848206\n",
      "Iteration 19680 Training loss 0.0314517505466938 Validation loss 0.03260362520813942 Accuracy 0.6700999736785889\n",
      "Iteration 19690 Training loss 0.02948121912777424 Validation loss 0.03161703050136566 Accuracy 0.6794000267982483\n",
      "Iteration 19700 Training loss 0.02846267633140087 Validation loss 0.03152373060584068 Accuracy 0.6802999973297119\n",
      "Iteration 19710 Training loss 0.028632741421461105 Validation loss 0.03266250714659691 Accuracy 0.6689000129699707\n",
      "Iteration 19720 Training loss 0.030104009434580803 Validation loss 0.032363902777433395 Accuracy 0.6722999811172485\n",
      "Iteration 19730 Training loss 0.029842574149370193 Validation loss 0.03230166807770729 Accuracy 0.6735000014305115\n",
      "Iteration 19740 Training loss 0.026826243847608566 Validation loss 0.031834717839956284 Accuracy 0.6780999898910522\n",
      "Iteration 19750 Training loss 0.02822869084775448 Validation loss 0.03180861473083496 Accuracy 0.6782000064849854\n",
      "Iteration 19760 Training loss 0.03212317079305649 Validation loss 0.03299421817064285 Accuracy 0.666100025177002\n",
      "Iteration 19770 Training loss 0.03336349129676819 Validation loss 0.032403476536273956 Accuracy 0.6721000075340271\n",
      "Iteration 19780 Training loss 0.03065803460776806 Validation loss 0.032830506563186646 Accuracy 0.6672000288963318\n",
      "Iteration 19790 Training loss 0.032019130885601044 Validation loss 0.03305533155798912 Accuracy 0.6656000018119812\n",
      "Iteration 19800 Training loss 0.02934601530432701 Validation loss 0.03169364854693413 Accuracy 0.6786999702453613\n",
      "Iteration 19810 Training loss 0.03126273676753044 Validation loss 0.033961109817028046 Accuracy 0.6560999751091003\n",
      "Iteration 19820 Training loss 0.031567610800266266 Validation loss 0.031494658440351486 Accuracy 0.6816999912261963\n",
      "Iteration 19830 Training loss 0.02996521070599556 Validation loss 0.03150163218379021 Accuracy 0.6812999844551086\n",
      "Iteration 19840 Training loss 0.03066733479499817 Validation loss 0.0323246568441391 Accuracy 0.67330002784729\n",
      "Iteration 19850 Training loss 0.028412504121661186 Validation loss 0.03184862807393074 Accuracy 0.6769000291824341\n",
      "Iteration 19860 Training loss 0.0288185216486454 Validation loss 0.031920645385980606 Accuracy 0.6771000027656555\n",
      "Iteration 19870 Training loss 0.026629826053977013 Validation loss 0.0315987691283226 Accuracy 0.6805999875068665\n",
      "Iteration 19880 Training loss 0.02704775705933571 Validation loss 0.03197428211569786 Accuracy 0.6761000156402588\n",
      "Iteration 19890 Training loss 0.027120277285575867 Validation loss 0.031176697462797165 Accuracy 0.6837000250816345\n",
      "Iteration 19900 Training loss 0.02735999785363674 Validation loss 0.031652290374040604 Accuracy 0.6798999905586243\n",
      "Iteration 19910 Training loss 0.029635444283485413 Validation loss 0.03188157454133034 Accuracy 0.6765999794006348\n",
      "Iteration 19920 Training loss 0.033731166273355484 Validation loss 0.03416632488369942 Accuracy 0.6542999744415283\n",
      "Iteration 19930 Training loss 0.028540827333927155 Validation loss 0.03137925639748573 Accuracy 0.6812999844551086\n",
      "Iteration 19940 Training loss 0.030565012246370316 Validation loss 0.03201459348201752 Accuracy 0.6758999824523926\n",
      "Iteration 19950 Training loss 0.030586333945393562 Validation loss 0.031908247619867325 Accuracy 0.6769000291824341\n",
      "Iteration 19960 Training loss 0.03249046206474304 Validation loss 0.035324715077877045 Accuracy 0.6427000164985657\n",
      "Iteration 19970 Training loss 0.029673241078853607 Validation loss 0.03183429688215256 Accuracy 0.6786999702453613\n",
      "Iteration 19980 Training loss 0.030385727062821388 Validation loss 0.03276405856013298 Accuracy 0.667900025844574\n",
      "Iteration 19990 Training loss 0.03341291472315788 Validation loss 0.03142125904560089 Accuracy 0.6819999814033508\n",
      "Iteration 20000 Training loss 0.030074674636125565 Validation loss 0.031853243708610535 Accuracy 0.6776999831199646\n",
      "Iteration 20010 Training loss 0.03090648353099823 Validation loss 0.032129526138305664 Accuracy 0.6747000217437744\n",
      "Iteration 20020 Training loss 0.03367771580815315 Validation loss 0.03234570845961571 Accuracy 0.6726999878883362\n",
      "Iteration 20030 Training loss 0.029047396034002304 Validation loss 0.03139385208487511 Accuracy 0.6819999814033508\n",
      "Iteration 20040 Training loss 0.02769649773836136 Validation loss 0.03192818537354469 Accuracy 0.6775000095367432\n",
      "Iteration 20050 Training loss 0.03156444430351257 Validation loss 0.03214437887072563 Accuracy 0.6751999855041504\n",
      "Iteration 20060 Training loss 0.029206840321421623 Validation loss 0.03224055841565132 Accuracy 0.6736000180244446\n",
      "Iteration 20070 Training loss 0.030440540984272957 Validation loss 0.03142720088362694 Accuracy 0.682200014591217\n",
      "Iteration 20080 Training loss 0.028152862563729286 Validation loss 0.03159835934638977 Accuracy 0.6802999973297119\n",
      "Iteration 20090 Training loss 0.029834335669875145 Validation loss 0.03117211163043976 Accuracy 0.6840999722480774\n",
      "Iteration 20100 Training loss 0.030067330226302147 Validation loss 0.031393084675073624 Accuracy 0.6823999881744385\n",
      "Iteration 20110 Training loss 0.028508584946393967 Validation loss 0.031630758196115494 Accuracy 0.6801000237464905\n",
      "Iteration 20120 Training loss 0.028265681117773056 Validation loss 0.03172142058610916 Accuracy 0.678600013256073\n",
      "Iteration 20130 Training loss 0.031260427087545395 Validation loss 0.031715985387563705 Accuracy 0.6800000071525574\n",
      "Iteration 20140 Training loss 0.03304888680577278 Validation loss 0.03165988624095917 Accuracy 0.6794000267982483\n",
      "Iteration 20150 Training loss 0.027632977813482285 Validation loss 0.031180590391159058 Accuracy 0.6841999888420105\n",
      "Iteration 20160 Training loss 0.031460460275411606 Validation loss 0.03157920017838478 Accuracy 0.6801999807357788\n",
      "Iteration 20170 Training loss 0.029815366491675377 Validation loss 0.03139231353998184 Accuracy 0.6819000244140625\n",
      "Iteration 20180 Training loss 0.03071007877588272 Validation loss 0.03177018091082573 Accuracy 0.679099977016449\n",
      "Iteration 20190 Training loss 0.02884726971387863 Validation loss 0.031961552798748016 Accuracy 0.6765999794006348\n",
      "Iteration 20200 Training loss 0.03315158933401108 Validation loss 0.03276558965444565 Accuracy 0.6686999797821045\n",
      "Iteration 20210 Training loss 0.02872367389500141 Validation loss 0.03145510330796242 Accuracy 0.680400013923645\n",
      "Iteration 20220 Training loss 0.027105573564767838 Validation loss 0.03210007771849632 Accuracy 0.6743999719619751\n",
      "Iteration 20230 Training loss 0.030071448534727097 Validation loss 0.03142017498612404 Accuracy 0.6815999746322632\n",
      "Iteration 20240 Training loss 0.0267318245023489 Validation loss 0.0316745825111866 Accuracy 0.6791999936103821\n",
      "Iteration 20250 Training loss 0.02980848215520382 Validation loss 0.03146674484014511 Accuracy 0.6818000078201294\n",
      "Iteration 20260 Training loss 0.031216328963637352 Validation loss 0.03264334797859192 Accuracy 0.6699000000953674\n",
      "Iteration 20270 Training loss 0.02999274991452694 Validation loss 0.03135015070438385 Accuracy 0.6823999881744385\n",
      "Iteration 20280 Training loss 0.030333302915096283 Validation loss 0.03169288858771324 Accuracy 0.6800000071525574\n",
      "Iteration 20290 Training loss 0.03035532869398594 Validation loss 0.03176173195242882 Accuracy 0.6791999936103821\n",
      "Iteration 20300 Training loss 0.02892891876399517 Validation loss 0.03187912330031395 Accuracy 0.678600013256073\n",
      "Iteration 20310 Training loss 0.03130986914038658 Validation loss 0.031915489584207535 Accuracy 0.6780999898910522\n",
      "Iteration 20320 Training loss 0.028530731797218323 Validation loss 0.03170944005250931 Accuracy 0.6796000003814697\n",
      "Iteration 20330 Training loss 0.026588717475533485 Validation loss 0.03125680610537529 Accuracy 0.6836000084877014\n",
      "Iteration 20340 Training loss 0.02986779250204563 Validation loss 0.032019685953855515 Accuracy 0.6751000285148621\n",
      "Iteration 20350 Training loss 0.028622008860111237 Validation loss 0.03190693259239197 Accuracy 0.6762999892234802\n",
      "Iteration 20360 Training loss 0.029251432046294212 Validation loss 0.031433071941137314 Accuracy 0.6819999814033508\n",
      "Iteration 20370 Training loss 0.029342086985707283 Validation loss 0.031640782952308655 Accuracy 0.6779999732971191\n",
      "Iteration 20380 Training loss 0.029263529926538467 Validation loss 0.03200796991586685 Accuracy 0.6758999824523926\n",
      "Iteration 20390 Training loss 0.028780201449990273 Validation loss 0.0317143052816391 Accuracy 0.6783000230789185\n",
      "Iteration 20400 Training loss 0.028088591992855072 Validation loss 0.03135943040251732 Accuracy 0.6819999814033508\n",
      "Iteration 20410 Training loss 0.030794136226177216 Validation loss 0.032139476388692856 Accuracy 0.6733999848365784\n",
      "Iteration 20420 Training loss 0.031178968027234077 Validation loss 0.031360164284706116 Accuracy 0.6829000115394592\n",
      "Iteration 20430 Training loss 0.02986566349864006 Validation loss 0.031255416572093964 Accuracy 0.6834999918937683\n",
      "Iteration 20440 Training loss 0.029469748958945274 Validation loss 0.032100968062877655 Accuracy 0.6747999787330627\n",
      "Iteration 20450 Training loss 0.02775806188583374 Validation loss 0.031111381947994232 Accuracy 0.6844000220298767\n",
      "Iteration 20460 Training loss 0.029171263799071312 Validation loss 0.03169574588537216 Accuracy 0.6794999837875366\n",
      "Iteration 20470 Training loss 0.029615290462970734 Validation loss 0.03152336925268173 Accuracy 0.6801999807357788\n",
      "Iteration 20480 Training loss 0.027273310348391533 Validation loss 0.03134884312748909 Accuracy 0.6822999715805054\n",
      "Iteration 20490 Training loss 0.032344669103622437 Validation loss 0.032219503074884415 Accuracy 0.6725000143051147\n",
      "Iteration 20500 Training loss 0.03182021155953407 Validation loss 0.03215915337204933 Accuracy 0.6751000285148621\n",
      "Iteration 20510 Training loss 0.03218669816851616 Validation loss 0.03272786736488342 Accuracy 0.6671000123023987\n",
      "Iteration 20520 Training loss 0.029633909463882446 Validation loss 0.03204621002078056 Accuracy 0.6751999855041504\n",
      "Iteration 20530 Training loss 0.029588092118501663 Validation loss 0.032024744898080826 Accuracy 0.675599992275238\n",
      "Iteration 20540 Training loss 0.03171335533261299 Validation loss 0.031238824129104614 Accuracy 0.6830000281333923\n",
      "Iteration 20550 Training loss 0.029301738366484642 Validation loss 0.031771108508110046 Accuracy 0.6779999732971191\n",
      "Iteration 20560 Training loss 0.028901251032948494 Validation loss 0.03215854987502098 Accuracy 0.6726999878883362\n",
      "Iteration 20570 Training loss 0.02929554507136345 Validation loss 0.03240419551730156 Accuracy 0.6725999712944031\n",
      "Iteration 20580 Training loss 0.026695793494582176 Validation loss 0.03163471817970276 Accuracy 0.6794999837875366\n",
      "Iteration 20590 Training loss 0.028563449159264565 Validation loss 0.031425874680280685 Accuracy 0.680899977684021\n",
      "Iteration 20600 Training loss 0.03158729150891304 Validation loss 0.03173701837658882 Accuracy 0.678600013256073\n",
      "Iteration 20610 Training loss 0.0322175994515419 Validation loss 0.031457070261240005 Accuracy 0.6808000206947327\n",
      "Iteration 20620 Training loss 0.029629135504364967 Validation loss 0.03140067309141159 Accuracy 0.6804999709129333\n",
      "Iteration 20630 Training loss 0.029966937378048897 Validation loss 0.03182021528482437 Accuracy 0.6765999794006348\n",
      "Iteration 20640 Training loss 0.03100423887372017 Validation loss 0.03188800811767578 Accuracy 0.6765999794006348\n",
      "Iteration 20650 Training loss 0.027810342609882355 Validation loss 0.03135016933083534 Accuracy 0.6818000078201294\n",
      "Iteration 20660 Training loss 0.027538102120161057 Validation loss 0.03167995810508728 Accuracy 0.6791999936103821\n",
      "Iteration 20670 Training loss 0.028842654079198837 Validation loss 0.031181400641798973 Accuracy 0.6840999722480774\n",
      "Iteration 20680 Training loss 0.028767019510269165 Validation loss 0.031174859032034874 Accuracy 0.6848000288009644\n",
      "Iteration 20690 Training loss 0.030048711225390434 Validation loss 0.0313275009393692 Accuracy 0.6822999715805054\n",
      "Iteration 20700 Training loss 0.02953999675810337 Validation loss 0.0317620225250721 Accuracy 0.6771000027656555\n",
      "Iteration 20710 Training loss 0.02697746828198433 Validation loss 0.03134812042117119 Accuracy 0.6819999814033508\n",
      "Iteration 20720 Training loss 0.0300441924482584 Validation loss 0.03195258229970932 Accuracy 0.6747999787330627\n",
      "Iteration 20730 Training loss 0.029829658567905426 Validation loss 0.031411826610565186 Accuracy 0.6819999814033508\n",
      "Iteration 20740 Training loss 0.03136991336941719 Validation loss 0.03175840154290199 Accuracy 0.6794000267982483\n",
      "Iteration 20750 Training loss 0.027587005868554115 Validation loss 0.03180459141731262 Accuracy 0.6776000261306763\n",
      "Iteration 20760 Training loss 0.031942218542099 Validation loss 0.03165892884135246 Accuracy 0.6786999702453613\n",
      "Iteration 20770 Training loss 0.029798397794365883 Validation loss 0.03168511018157005 Accuracy 0.678600013256073\n",
      "Iteration 20780 Training loss 0.029075149446725845 Validation loss 0.0316961295902729 Accuracy 0.6797000169754028\n",
      "Iteration 20790 Training loss 0.03218778595328331 Validation loss 0.031649161130189896 Accuracy 0.6815000176429749\n",
      "Iteration 20800 Training loss 0.028056465089321136 Validation loss 0.03326484560966492 Accuracy 0.6644999980926514\n",
      "Iteration 20810 Training loss 0.02911437302827835 Validation loss 0.031705304980278015 Accuracy 0.6800000071525574\n",
      "Iteration 20820 Training loss 0.030760549008846283 Validation loss 0.032369427382946014 Accuracy 0.6729999780654907\n",
      "Iteration 20830 Training loss 0.03346571326255798 Validation loss 0.03170732036232948 Accuracy 0.6802999973297119\n",
      "Iteration 20840 Training loss 0.028188392519950867 Validation loss 0.031279124319553375 Accuracy 0.6834999918937683\n",
      "Iteration 20850 Training loss 0.029966440051794052 Validation loss 0.03270285576581955 Accuracy 0.6692000031471252\n",
      "Iteration 20860 Training loss 0.027929548174142838 Validation loss 0.03206053376197815 Accuracy 0.6747000217437744\n",
      "Iteration 20870 Training loss 0.02776460535824299 Validation loss 0.030927246436476707 Accuracy 0.6865000128746033\n",
      "Iteration 20880 Training loss 0.026652866974473 Validation loss 0.03159545361995697 Accuracy 0.6818000078201294\n",
      "Iteration 20890 Training loss 0.030280187726020813 Validation loss 0.03179726004600525 Accuracy 0.6793000102043152\n",
      "Iteration 20900 Training loss 0.030242446810007095 Validation loss 0.032021358609199524 Accuracy 0.6765999794006348\n",
      "Iteration 20910 Training loss 0.026069357991218567 Validation loss 0.03127019852399826 Accuracy 0.6814000010490417\n",
      "Iteration 20920 Training loss 0.02762841060757637 Validation loss 0.0312870554625988 Accuracy 0.6823999881744385\n",
      "Iteration 20930 Training loss 0.03058922290802002 Validation loss 0.031153451651334763 Accuracy 0.6833999752998352\n",
      "Iteration 20940 Training loss 0.02950083464384079 Validation loss 0.03108901157975197 Accuracy 0.684499979019165\n",
      "Iteration 20950 Training loss 0.023686811327934265 Validation loss 0.03140836954116821 Accuracy 0.6815000176429749\n",
      "Iteration 20960 Training loss 0.03045937418937683 Validation loss 0.031663164496421814 Accuracy 0.678600013256073\n",
      "Iteration 20970 Training loss 0.027163462713360786 Validation loss 0.03144701570272446 Accuracy 0.6812999844551086\n",
      "Iteration 20980 Training loss 0.02956821210682392 Validation loss 0.031208254396915436 Accuracy 0.6837000250816345\n",
      "Iteration 20990 Training loss 0.031737279146909714 Validation loss 0.03259356692433357 Accuracy 0.6703000068664551\n",
      "Iteration 21000 Training loss 0.031352825462818146 Validation loss 0.03146754950284958 Accuracy 0.6812000274658203\n",
      "Iteration 21010 Training loss 0.030828073620796204 Validation loss 0.03159964829683304 Accuracy 0.6798999905586243\n",
      "Iteration 21020 Training loss 0.030723243951797485 Validation loss 0.031642504036426544 Accuracy 0.6793000102043152\n",
      "Iteration 21030 Training loss 0.02971004694700241 Validation loss 0.03183247894048691 Accuracy 0.6782000064849854\n",
      "Iteration 21040 Training loss 0.03038732334971428 Validation loss 0.03132231533527374 Accuracy 0.6837999820709229\n",
      "Iteration 21050 Training loss 0.032505884766578674 Validation loss 0.031179165467619896 Accuracy 0.6844000220298767\n",
      "Iteration 21060 Training loss 0.029899727553129196 Validation loss 0.03140494227409363 Accuracy 0.6823999881744385\n",
      "Iteration 21070 Training loss 0.03046111762523651 Validation loss 0.0315202996134758 Accuracy 0.6819999814033508\n",
      "Iteration 21080 Training loss 0.029955321922898293 Validation loss 0.031498972326517105 Accuracy 0.6819000244140625\n",
      "Iteration 21090 Training loss 0.030542561784386635 Validation loss 0.03138060122728348 Accuracy 0.6829000115394592\n",
      "Iteration 21100 Training loss 0.029432306066155434 Validation loss 0.03131188079714775 Accuracy 0.6829000115394592\n",
      "Iteration 21110 Training loss 0.02745951898396015 Validation loss 0.03134112432599068 Accuracy 0.6822999715805054\n",
      "Iteration 21120 Training loss 0.027001220732927322 Validation loss 0.03143879026174545 Accuracy 0.6807000041007996\n",
      "Iteration 21130 Training loss 0.028342433273792267 Validation loss 0.03139694407582283 Accuracy 0.6832000017166138\n",
      "Iteration 21140 Training loss 0.030646733939647675 Validation loss 0.031218046322464943 Accuracy 0.6829000115394592\n",
      "Iteration 21150 Training loss 0.028487268835306168 Validation loss 0.03179618716239929 Accuracy 0.6783000230789185\n",
      "Iteration 21160 Training loss 0.028111616149544716 Validation loss 0.03167950361967087 Accuracy 0.6783999800682068\n",
      "Iteration 21170 Training loss 0.02715398743748665 Validation loss 0.03168446198105812 Accuracy 0.6791999936103821\n",
      "Iteration 21180 Training loss 0.026195652782917023 Validation loss 0.031363315880298615 Accuracy 0.6815999746322632\n",
      "Iteration 21190 Training loss 0.028277667239308357 Validation loss 0.030956806614995003 Accuracy 0.6858999729156494\n",
      "Iteration 21200 Training loss 0.027464063838124275 Validation loss 0.03118288330733776 Accuracy 0.6830000281333923\n",
      "Iteration 21210 Training loss 0.027917418628931046 Validation loss 0.031116362661123276 Accuracy 0.684499979019165\n",
      "Iteration 21220 Training loss 0.027354646474123 Validation loss 0.031113604083657265 Accuracy 0.6840000152587891\n",
      "Iteration 21230 Training loss 0.03214872255921364 Validation loss 0.03257545083761215 Accuracy 0.6693000197410583\n",
      "Iteration 21240 Training loss 0.028999915346503258 Validation loss 0.032395150512456894 Accuracy 0.6722000241279602\n",
      "Iteration 21250 Training loss 0.029808610677719116 Validation loss 0.031198810786008835 Accuracy 0.6854000091552734\n",
      "Iteration 21260 Training loss 0.028802653774619102 Validation loss 0.032266467809677124 Accuracy 0.6725999712944031\n",
      "Iteration 21270 Training loss 0.028299104422330856 Validation loss 0.0317474827170372 Accuracy 0.6791999936103821\n",
      "Iteration 21280 Training loss 0.03272448480129242 Validation loss 0.031125715002417564 Accuracy 0.6841999888420105\n",
      "Iteration 21290 Training loss 0.030620580539107323 Validation loss 0.03239535912871361 Accuracy 0.671500027179718\n",
      "Iteration 21300 Training loss 0.030295321717858315 Validation loss 0.03137554973363876 Accuracy 0.6812000274658203\n",
      "Iteration 21310 Training loss 0.031441766768693924 Validation loss 0.03142007440328598 Accuracy 0.680899977684021\n",
      "Iteration 21320 Training loss 0.03036283142864704 Validation loss 0.031217362731695175 Accuracy 0.6834999918937683\n",
      "Iteration 21330 Training loss 0.030014825984835625 Validation loss 0.031402893364429474 Accuracy 0.6812999844551086\n",
      "Iteration 21340 Training loss 0.02767331153154373 Validation loss 0.031509000808000565 Accuracy 0.6802999973297119\n",
      "Iteration 21350 Training loss 0.030455751344561577 Validation loss 0.03154018521308899 Accuracy 0.6794999837875366\n",
      "Iteration 21360 Training loss 0.029235783964395523 Validation loss 0.0314207561314106 Accuracy 0.6811000108718872\n",
      "Iteration 21370 Training loss 0.03303844854235649 Validation loss 0.0335012823343277 Accuracy 0.661300003528595\n",
      "Iteration 21380 Training loss 0.031089240685105324 Validation loss 0.03209243714809418 Accuracy 0.6747999787330627\n",
      "Iteration 21390 Training loss 0.030743535608053207 Validation loss 0.03156128525733948 Accuracy 0.6797999739646912\n",
      "Iteration 21400 Training loss 0.03233615681529045 Validation loss 0.03153858706355095 Accuracy 0.680400013923645\n",
      "Iteration 21410 Training loss 0.02871530130505562 Validation loss 0.031317658722400665 Accuracy 0.6825000047683716\n",
      "Iteration 21420 Training loss 0.02668333426117897 Validation loss 0.03169207647442818 Accuracy 0.679099977016449\n",
      "Iteration 21430 Training loss 0.028682276606559753 Validation loss 0.03148522228002548 Accuracy 0.6808000206947327\n",
      "Iteration 21440 Training loss 0.030419331043958664 Validation loss 0.03138478472828865 Accuracy 0.6819000244140625\n",
      "Iteration 21450 Training loss 0.030391810461878777 Validation loss 0.0313107892870903 Accuracy 0.6825000047683716\n",
      "Iteration 21460 Training loss 0.03125562146306038 Validation loss 0.031248530372977257 Accuracy 0.6827999949455261\n",
      "Iteration 21470 Training loss 0.03238390013575554 Validation loss 0.03132517635822296 Accuracy 0.6818000078201294\n",
      "Iteration 21480 Training loss 0.026165058836340904 Validation loss 0.03154301643371582 Accuracy 0.6805999875068665\n",
      "Iteration 21490 Training loss 0.027590269222855568 Validation loss 0.03160815313458443 Accuracy 0.6794999837875366\n",
      "Iteration 21500 Training loss 0.029331548139452934 Validation loss 0.031519897282123566 Accuracy 0.6797999739646912\n",
      "Iteration 21510 Training loss 0.029563624411821365 Validation loss 0.031710684299468994 Accuracy 0.6796000003814697\n",
      "Iteration 21520 Training loss 0.025959515944123268 Validation loss 0.03172537684440613 Accuracy 0.6786999702453613\n",
      "Iteration 21530 Training loss 0.028476784005761147 Validation loss 0.031332675367593765 Accuracy 0.6829000115394592\n",
      "Iteration 21540 Training loss 0.029305869713425636 Validation loss 0.03156823664903641 Accuracy 0.6804999709129333\n",
      "Iteration 21550 Training loss 0.027074147015810013 Validation loss 0.03238018602132797 Accuracy 0.6717000007629395\n",
      "Iteration 21560 Training loss 0.0335552915930748 Validation loss 0.033890899270772934 Accuracy 0.6554999947547913\n",
      "Iteration 21570 Training loss 0.027495605871081352 Validation loss 0.031483810395002365 Accuracy 0.6796000003814697\n",
      "Iteration 21580 Training loss 0.02726728841662407 Validation loss 0.03102440945804119 Accuracy 0.6851999759674072\n",
      "Iteration 21590 Training loss 0.027029139921069145 Validation loss 0.03170612454414368 Accuracy 0.6783999800682068\n",
      "Iteration 21600 Training loss 0.030823908746242523 Validation loss 0.031080586835741997 Accuracy 0.6840999722480774\n",
      "Iteration 21610 Training loss 0.02824096940457821 Validation loss 0.031066933646798134 Accuracy 0.6837999820709229\n",
      "Iteration 21620 Training loss 0.02995753474533558 Validation loss 0.03143634647130966 Accuracy 0.680400013923645\n",
      "Iteration 21630 Training loss 0.03144056722521782 Validation loss 0.0319843515753746 Accuracy 0.675599992275238\n",
      "Iteration 21640 Training loss 0.03022516332566738 Validation loss 0.03127669543027878 Accuracy 0.6830000281333923\n",
      "Iteration 21650 Training loss 0.02960420772433281 Validation loss 0.03202645480632782 Accuracy 0.6743999719619751\n",
      "Iteration 21660 Training loss 0.029654182493686676 Validation loss 0.03153201937675476 Accuracy 0.680899977684021\n",
      "Iteration 21670 Training loss 0.027553997933864594 Validation loss 0.03129110857844353 Accuracy 0.6832000017166138\n",
      "Iteration 21680 Training loss 0.03042900562286377 Validation loss 0.031585805118083954 Accuracy 0.6800000071525574\n",
      "Iteration 21690 Training loss 0.025948939844965935 Validation loss 0.031164228916168213 Accuracy 0.6841999888420105\n",
      "Iteration 21700 Training loss 0.029269739985466003 Validation loss 0.031150419265031815 Accuracy 0.6841999888420105\n",
      "Iteration 21710 Training loss 0.028146715834736824 Validation loss 0.0316087082028389 Accuracy 0.6811000108718872\n",
      "Iteration 21720 Training loss 0.02887805923819542 Validation loss 0.031290389597415924 Accuracy 0.6822999715805054\n",
      "Iteration 21730 Training loss 0.02805187553167343 Validation loss 0.032057251781225204 Accuracy 0.675000011920929\n",
      "Iteration 21740 Training loss 0.029409319162368774 Validation loss 0.03148360922932625 Accuracy 0.6801999807357788\n",
      "Iteration 21750 Training loss 0.027948886156082153 Validation loss 0.031220147386193275 Accuracy 0.6829000115394592\n",
      "Iteration 21760 Training loss 0.02791842818260193 Validation loss 0.03154947608709335 Accuracy 0.6815000176429749\n",
      "Iteration 21770 Training loss 0.026443857699632645 Validation loss 0.03231294825673103 Accuracy 0.6714000105857849\n",
      "Iteration 21780 Training loss 0.027140283957123756 Validation loss 0.03176189213991165 Accuracy 0.6775000095367432\n",
      "Iteration 21790 Training loss 0.032234035432338715 Validation loss 0.03209033980965614 Accuracy 0.6740999817848206\n",
      "Iteration 21800 Training loss 0.027622157707810402 Validation loss 0.0311089176684618 Accuracy 0.6837000250816345\n",
      "Iteration 21810 Training loss 0.029511576518416405 Validation loss 0.031534962356090546 Accuracy 0.6797000169754028\n",
      "Iteration 21820 Training loss 0.03137313202023506 Validation loss 0.0313606821000576 Accuracy 0.6808000206947327\n",
      "Iteration 21830 Training loss 0.027535807341337204 Validation loss 0.031167402863502502 Accuracy 0.6833000183105469\n",
      "Iteration 21840 Training loss 0.03393367677927017 Validation loss 0.03155843913555145 Accuracy 0.6789000034332275\n",
      "Iteration 21850 Training loss 0.030916158109903336 Validation loss 0.03125429525971413 Accuracy 0.6830999851226807\n",
      "Iteration 21860 Training loss 0.028933756053447723 Validation loss 0.03129683434963226 Accuracy 0.6814000010490417\n",
      "Iteration 21870 Training loss 0.0255881454795599 Validation loss 0.03216521069407463 Accuracy 0.67330002784729\n",
      "Iteration 21880 Training loss 0.02593807503581047 Validation loss 0.03163515031337738 Accuracy 0.6784999966621399\n",
      "Iteration 21890 Training loss 0.02764090895652771 Validation loss 0.03154886141419411 Accuracy 0.679099977016449\n",
      "Iteration 21900 Training loss 0.0279312115162611 Validation loss 0.03126378729939461 Accuracy 0.682200014591217\n",
      "Iteration 21910 Training loss 0.030671872198581696 Validation loss 0.03219076618552208 Accuracy 0.6733999848365784\n",
      "Iteration 21920 Training loss 0.027733447030186653 Validation loss 0.03218923881649971 Accuracy 0.6747000217437744\n",
      "Iteration 21930 Training loss 0.026766596361994743 Validation loss 0.03137757629156113 Accuracy 0.6832000017166138\n",
      "Iteration 21940 Training loss 0.02855234593153 Validation loss 0.031350426375865936 Accuracy 0.6812000274658203\n",
      "Iteration 21950 Training loss 0.028098689392209053 Validation loss 0.03128547966480255 Accuracy 0.6815000176429749\n",
      "Iteration 21960 Training loss 0.025749636813998222 Validation loss 0.031215837225317955 Accuracy 0.6825000047683716\n",
      "Iteration 21970 Training loss 0.02894313633441925 Validation loss 0.031195448711514473 Accuracy 0.6825000047683716\n",
      "Iteration 21980 Training loss 0.025921402499079704 Validation loss 0.031115835532546043 Accuracy 0.6837999820709229\n",
      "Iteration 21990 Training loss 0.02685953862965107 Validation loss 0.031036652624607086 Accuracy 0.6844000220298767\n",
      "Iteration 22000 Training loss 0.029737839475274086 Validation loss 0.03205716237425804 Accuracy 0.6740999817848206\n",
      "Iteration 22010 Training loss 0.02854950726032257 Validation loss 0.031511154025793076 Accuracy 0.6804999709129333\n",
      "Iteration 22020 Training loss 0.030436905100941658 Validation loss 0.0308841485530138 Accuracy 0.6863999962806702\n",
      "Iteration 22030 Training loss 0.026844562962651253 Validation loss 0.031629778444767 Accuracy 0.6784999966621399\n",
      "Iteration 22040 Training loss 0.03044082224369049 Validation loss 0.031018849462270737 Accuracy 0.6844000220298767\n",
      "Iteration 22050 Training loss 0.027394022792577744 Validation loss 0.031055409461259842 Accuracy 0.6848999857902527\n",
      "Iteration 22060 Training loss 0.02571479231119156 Validation loss 0.03151668235659599 Accuracy 0.680400013923645\n",
      "Iteration 22070 Training loss 0.027675682678818703 Validation loss 0.03143075481057167 Accuracy 0.6808000206947327\n",
      "Iteration 22080 Training loss 0.028500903397798538 Validation loss 0.03135759010910988 Accuracy 0.6815000176429749\n",
      "Iteration 22090 Training loss 0.031890127807855606 Validation loss 0.03428209200501442 Accuracy 0.6514000296592712\n",
      "Iteration 22100 Training loss 0.030722005292773247 Validation loss 0.031843151897192 Accuracy 0.6757000088691711\n",
      "Iteration 22110 Training loss 0.03054848313331604 Validation loss 0.0322045162320137 Accuracy 0.6722000241279602\n",
      "Iteration 22120 Training loss 0.02960807830095291 Validation loss 0.031964048743247986 Accuracy 0.675000011920929\n",
      "Iteration 22130 Training loss 0.03021608479321003 Validation loss 0.031341154128313065 Accuracy 0.6826000213623047\n",
      "Iteration 22140 Training loss 0.025769928470253944 Validation loss 0.03276725485920906 Accuracy 0.6668999791145325\n",
      "Iteration 22150 Training loss 0.02982398495078087 Validation loss 0.031691454350948334 Accuracy 0.6777999997138977\n",
      "Iteration 22160 Training loss 0.02518746629357338 Validation loss 0.03149597346782684 Accuracy 0.6796000003814697\n",
      "Iteration 22170 Training loss 0.030300725251436234 Validation loss 0.032165687531232834 Accuracy 0.6732000112533569\n",
      "Iteration 22180 Training loss 0.026035789400339127 Validation loss 0.03115476854145527 Accuracy 0.6832000017166138\n",
      "Iteration 22190 Training loss 0.030075857415795326 Validation loss 0.03122963197529316 Accuracy 0.682699978351593\n",
      "Iteration 22200 Training loss 0.027799997478723526 Validation loss 0.031217817217111588 Accuracy 0.6820999979972839\n",
      "Iteration 22210 Training loss 0.026183530688285828 Validation loss 0.03182980418205261 Accuracy 0.6765999794006348\n",
      "Iteration 22220 Training loss 0.029961813241243362 Validation loss 0.031095469370484352 Accuracy 0.6841999888420105\n",
      "Iteration 22230 Training loss 0.03148552030324936 Validation loss 0.031426288187503815 Accuracy 0.6808000206947327\n",
      "Iteration 22240 Training loss 0.029581312090158463 Validation loss 0.03095295839011669 Accuracy 0.6840000152587891\n",
      "Iteration 22250 Training loss 0.028774607926607132 Validation loss 0.030819863080978394 Accuracy 0.6862000226974487\n",
      "Iteration 22260 Training loss 0.028984958305954933 Validation loss 0.03134709596633911 Accuracy 0.680899977684021\n",
      "Iteration 22270 Training loss 0.028273893520236015 Validation loss 0.03121853992342949 Accuracy 0.6811000108718872\n",
      "Iteration 22280 Training loss 0.026648299768567085 Validation loss 0.031244125217199326 Accuracy 0.6823999881744385\n",
      "Iteration 22290 Training loss 0.028414286673069 Validation loss 0.03217712789773941 Accuracy 0.6726999878883362\n",
      "Iteration 22300 Training loss 0.031622450798749924 Validation loss 0.03169555589556694 Accuracy 0.6773999929428101\n",
      "Iteration 22310 Training loss 0.029911400750279427 Validation loss 0.03169190138578415 Accuracy 0.6787999868392944\n",
      "Iteration 22320 Training loss 0.027037767693400383 Validation loss 0.03120294399559498 Accuracy 0.684499979019165\n",
      "Iteration 22330 Training loss 0.026928454637527466 Validation loss 0.031613267958164215 Accuracy 0.6790000200271606\n",
      "Iteration 22340 Training loss 0.028555680066347122 Validation loss 0.03140554577112198 Accuracy 0.6812999844551086\n",
      "Iteration 22350 Training loss 0.031024688854813576 Validation loss 0.03143482655286789 Accuracy 0.6800000071525574\n",
      "Iteration 22360 Training loss 0.027919787913560867 Validation loss 0.031691212207078934 Accuracy 0.6769000291824341\n",
      "Iteration 22370 Training loss 0.029568888247013092 Validation loss 0.031132685020565987 Accuracy 0.6830999851226807\n",
      "Iteration 22380 Training loss 0.030455972999334335 Validation loss 0.03136668726801872 Accuracy 0.6815000176429749\n",
      "Iteration 22390 Training loss 0.027963686734437943 Validation loss 0.031247420236468315 Accuracy 0.682699978351593\n",
      "Iteration 22400 Training loss 0.02878117933869362 Validation loss 0.03088802471756935 Accuracy 0.685699999332428\n",
      "Iteration 22410 Training loss 0.029871495440602303 Validation loss 0.031035402789711952 Accuracy 0.6837999820709229\n",
      "Iteration 22420 Training loss 0.027491647750139236 Validation loss 0.031961895525455475 Accuracy 0.6754999756813049\n",
      "Iteration 22430 Training loss 0.03047601506114006 Validation loss 0.031081601977348328 Accuracy 0.6837999820709229\n",
      "Iteration 22440 Training loss 0.02994374930858612 Validation loss 0.031108828261494637 Accuracy 0.6837999820709229\n",
      "Iteration 22450 Training loss 0.030103400349617004 Validation loss 0.03155696392059326 Accuracy 0.6783000230789185\n",
      "Iteration 22460 Training loss 0.030086252838373184 Validation loss 0.031128093600273132 Accuracy 0.6837999820709229\n",
      "Iteration 22470 Training loss 0.03531349077820778 Validation loss 0.03180646896362305 Accuracy 0.6769999861717224\n",
      "Iteration 22480 Training loss 0.029836349189281464 Validation loss 0.031067082658410072 Accuracy 0.6830999851226807\n",
      "Iteration 22490 Training loss 0.02819105237722397 Validation loss 0.03132300078868866 Accuracy 0.6804999709129333\n",
      "Iteration 22500 Training loss 0.03145665302872658 Validation loss 0.03156410902738571 Accuracy 0.678600013256073\n",
      "Iteration 22510 Training loss 0.027510501444339752 Validation loss 0.03129461780190468 Accuracy 0.6825000047683716\n",
      "Iteration 22520 Training loss 0.027909334748983383 Validation loss 0.03125285357236862 Accuracy 0.6837000250816345\n",
      "Iteration 22530 Training loss 0.029013972729444504 Validation loss 0.03129946440458298 Accuracy 0.6823999881744385\n",
      "Iteration 22540 Training loss 0.03172077238559723 Validation loss 0.03147673234343529 Accuracy 0.6801000237464905\n",
      "Iteration 22550 Training loss 0.030566619709134102 Validation loss 0.03156917914748192 Accuracy 0.6787999868392944\n",
      "Iteration 22560 Training loss 0.02656148187816143 Validation loss 0.031240342184901237 Accuracy 0.6830000281333923\n",
      "Iteration 22570 Training loss 0.03129161521792412 Validation loss 0.032192252576351166 Accuracy 0.6733999848365784\n",
      "Iteration 22580 Training loss 0.027534490451216698 Validation loss 0.03160860762000084 Accuracy 0.6776000261306763\n",
      "Iteration 22590 Training loss 0.02568432316184044 Validation loss 0.03162846341729164 Accuracy 0.6779000163078308\n",
      "Iteration 22600 Training loss 0.0287492573261261 Validation loss 0.03097800351679325 Accuracy 0.6848999857902527\n",
      "Iteration 22610 Training loss 0.032170847058296204 Validation loss 0.03143027424812317 Accuracy 0.6811000108718872\n",
      "Iteration 22620 Training loss 0.03129161149263382 Validation loss 0.031763143837451935 Accuracy 0.6769000291824341\n",
      "Iteration 22630 Training loss 0.029992850497364998 Validation loss 0.031183717772364616 Accuracy 0.6820999979972839\n",
      "Iteration 22640 Training loss 0.027484983205795288 Validation loss 0.031097380444407463 Accuracy 0.6836000084877014\n",
      "Iteration 22650 Training loss 0.028085792437195778 Validation loss 0.03145617991685867 Accuracy 0.6812999844551086\n",
      "Iteration 22660 Training loss 0.02734961174428463 Validation loss 0.0312007088214159 Accuracy 0.6829000115394592\n",
      "Iteration 22670 Training loss 0.029721390455961227 Validation loss 0.03181969374418259 Accuracy 0.677299976348877\n",
      "Iteration 22680 Training loss 0.029119295999407768 Validation loss 0.0317426472902298 Accuracy 0.6769999861717224\n",
      "Iteration 22690 Training loss 0.0280860997736454 Validation loss 0.03100484423339367 Accuracy 0.6848000288009644\n",
      "Iteration 22700 Training loss 0.02660573273897171 Validation loss 0.03192540258169174 Accuracy 0.6754000186920166\n",
      "Iteration 22710 Training loss 0.027029160410165787 Validation loss 0.03184277564287186 Accuracy 0.6766999959945679\n",
      "Iteration 22720 Training loss 0.024583734571933746 Validation loss 0.03124331124126911 Accuracy 0.6812000274658203\n",
      "Iteration 22730 Training loss 0.030613211914896965 Validation loss 0.031355809420347214 Accuracy 0.6812999844551086\n",
      "Iteration 22740 Training loss 0.028176134452223778 Validation loss 0.03106490708887577 Accuracy 0.683899998664856\n",
      "Iteration 22750 Training loss 0.027975967153906822 Validation loss 0.031283922493457794 Accuracy 0.6830999851226807\n",
      "Iteration 22760 Training loss 0.031096292659640312 Validation loss 0.03201332688331604 Accuracy 0.6741999983787537\n",
      "Iteration 22770 Training loss 0.026961781084537506 Validation loss 0.030987396836280823 Accuracy 0.6840000152587891\n",
      "Iteration 22780 Training loss 0.03083362616598606 Validation loss 0.031007612124085426 Accuracy 0.6837999820709229\n",
      "Iteration 22790 Training loss 0.02763543650507927 Validation loss 0.031479526311159134 Accuracy 0.6801000237464905\n",
      "Iteration 22800 Training loss 0.027660159394145012 Validation loss 0.03134840726852417 Accuracy 0.6818000078201294\n",
      "Iteration 22810 Training loss 0.027299385517835617 Validation loss 0.0308216605335474 Accuracy 0.6873000264167786\n",
      "Iteration 22820 Training loss 0.026674283668398857 Validation loss 0.03096640110015869 Accuracy 0.6858000159263611\n",
      "Iteration 22830 Training loss 0.02800966612994671 Validation loss 0.032329026609659195 Accuracy 0.671999990940094\n",
      "Iteration 22840 Training loss 0.025139598175883293 Validation loss 0.030848564580082893 Accuracy 0.6872000098228455\n",
      "Iteration 22850 Training loss 0.02617242932319641 Validation loss 0.03152599558234215 Accuracy 0.6802999973297119\n",
      "Iteration 22860 Training loss 0.026416948065161705 Validation loss 0.03140876069664955 Accuracy 0.680400013923645\n",
      "Iteration 22870 Training loss 0.029247881844639778 Validation loss 0.03150593116879463 Accuracy 0.6783000230789185\n",
      "Iteration 22880 Training loss 0.028500856831669807 Validation loss 0.03201036527752876 Accuracy 0.6733999848365784\n",
      "Iteration 22890 Training loss 0.027053726837038994 Validation loss 0.031383346766233444 Accuracy 0.6802999973297119\n",
      "Iteration 22900 Training loss 0.03072725236415863 Validation loss 0.031814854592084885 Accuracy 0.6751000285148621\n",
      "Iteration 22910 Training loss 0.027751533314585686 Validation loss 0.031195146963000298 Accuracy 0.6826000213623047\n",
      "Iteration 22920 Training loss 0.02841476909816265 Validation loss 0.0320892333984375 Accuracy 0.6743000149726868\n",
      "Iteration 22930 Training loss 0.026840774342417717 Validation loss 0.03120891936123371 Accuracy 0.6823999881744385\n",
      "Iteration 22940 Training loss 0.029145600274205208 Validation loss 0.03129713609814644 Accuracy 0.6826000213623047\n",
      "Iteration 22950 Training loss 0.030756207183003426 Validation loss 0.03181877359747887 Accuracy 0.6783999800682068\n",
      "Iteration 22960 Training loss 0.029578227549791336 Validation loss 0.031308721750974655 Accuracy 0.6812999844551086\n",
      "Iteration 22970 Training loss 0.03001434914767742 Validation loss 0.03111577220261097 Accuracy 0.6830000281333923\n",
      "Iteration 22980 Training loss 0.026889102533459663 Validation loss 0.03082495927810669 Accuracy 0.6854000091552734\n",
      "Iteration 22990 Training loss 0.02881661057472229 Validation loss 0.03087298572063446 Accuracy 0.6848000288009644\n",
      "Iteration 23000 Training loss 0.03023480996489525 Validation loss 0.031278096139431 Accuracy 0.6815999746322632\n",
      "Iteration 23010 Training loss 0.030837802216410637 Validation loss 0.031178291887044907 Accuracy 0.6833000183105469\n",
      "Iteration 23020 Training loss 0.030913274735212326 Validation loss 0.03191494569182396 Accuracy 0.6758999824523926\n",
      "Iteration 23030 Training loss 0.03134658560156822 Validation loss 0.031059540808200836 Accuracy 0.6832000017166138\n",
      "Iteration 23040 Training loss 0.030485885217785835 Validation loss 0.031646087765693665 Accuracy 0.679099977016449\n",
      "Iteration 23050 Training loss 0.030145088210701942 Validation loss 0.031655509024858475 Accuracy 0.6783000230789185\n",
      "Iteration 23060 Training loss 0.02643771842122078 Validation loss 0.03125378116965294 Accuracy 0.6830000281333923\n",
      "Iteration 23070 Training loss 0.027981005609035492 Validation loss 0.030976025387644768 Accuracy 0.6855000257492065\n",
      "Iteration 23080 Training loss 0.028288744390010834 Validation loss 0.031323257833719254 Accuracy 0.6815999746322632\n",
      "Iteration 23090 Training loss 0.031190119683742523 Validation loss 0.03096018359065056 Accuracy 0.6851999759674072\n",
      "Iteration 23100 Training loss 0.029717188328504562 Validation loss 0.03130975365638733 Accuracy 0.6820999979972839\n",
      "Iteration 23110 Training loss 0.03022157773375511 Validation loss 0.031491316854953766 Accuracy 0.6796000003814697\n",
      "Iteration 23120 Training loss 0.030306197702884674 Validation loss 0.031090419739484787 Accuracy 0.6836000084877014\n",
      "Iteration 23130 Training loss 0.027732225134968758 Validation loss 0.030879316851496696 Accuracy 0.6851999759674072\n",
      "Iteration 23140 Training loss 0.026453198865056038 Validation loss 0.03111726976931095 Accuracy 0.6829000115394592\n",
      "Iteration 23150 Training loss 0.028621843084692955 Validation loss 0.03206845372915268 Accuracy 0.6725999712944031\n",
      "Iteration 23160 Training loss 0.02949663996696472 Validation loss 0.03141896799206734 Accuracy 0.6787999868392944\n",
      "Iteration 23170 Training loss 0.027946345508098602 Validation loss 0.030985789373517036 Accuracy 0.6843000054359436\n",
      "Iteration 23180 Training loss 0.027962129563093185 Validation loss 0.0310599897056818 Accuracy 0.6826000213623047\n",
      "Iteration 23190 Training loss 0.03296623378992081 Validation loss 0.03097432665526867 Accuracy 0.6852999925613403\n",
      "Iteration 23200 Training loss 0.030861977487802505 Validation loss 0.03086838871240616 Accuracy 0.6868000030517578\n",
      "Iteration 23210 Training loss 0.02851133421063423 Validation loss 0.031152687966823578 Accuracy 0.6830999851226807\n",
      "Iteration 23220 Training loss 0.030596287921071053 Validation loss 0.030877187848091125 Accuracy 0.6847000122070312\n",
      "Iteration 23230 Training loss 0.029388001188635826 Validation loss 0.030886132270097733 Accuracy 0.684499979019165\n",
      "Iteration 23240 Training loss 0.02827000431716442 Validation loss 0.03089148923754692 Accuracy 0.6855999827384949\n",
      "Iteration 23250 Training loss 0.03074352815747261 Validation loss 0.030920451506972313 Accuracy 0.6851000189781189\n",
      "Iteration 23260 Training loss 0.02972354181110859 Validation loss 0.03112024813890457 Accuracy 0.6826000213623047\n",
      "Iteration 23270 Training loss 0.02950914204120636 Validation loss 0.031268227845430374 Accuracy 0.6802999973297119\n",
      "Iteration 23280 Training loss 0.027374453842639923 Validation loss 0.03161414712667465 Accuracy 0.6787999868392944\n",
      "Iteration 23290 Training loss 0.028751183301210403 Validation loss 0.031289197504520416 Accuracy 0.6801999807357788\n",
      "Iteration 23300 Training loss 0.028240494430065155 Validation loss 0.030944528058171272 Accuracy 0.6837999820709229\n",
      "Iteration 23310 Training loss 0.02706829644739628 Validation loss 0.03071361407637596 Accuracy 0.6862000226974487\n",
      "Iteration 23320 Training loss 0.027577269822359085 Validation loss 0.031249599531292915 Accuracy 0.6811000108718872\n",
      "Iteration 23330 Training loss 0.02799166925251484 Validation loss 0.03094550222158432 Accuracy 0.6863999962806702\n",
      "Iteration 23340 Training loss 0.02774038165807724 Validation loss 0.031228752806782722 Accuracy 0.682200014591217\n",
      "Iteration 23350 Training loss 0.02973910979926586 Validation loss 0.03146280348300934 Accuracy 0.6801000237464905\n",
      "Iteration 23360 Training loss 0.028839021921157837 Validation loss 0.03101348876953125 Accuracy 0.6851999759674072\n",
      "Iteration 23370 Training loss 0.0257781520485878 Validation loss 0.030887315049767494 Accuracy 0.6861000061035156\n",
      "Iteration 23380 Training loss 0.028872050344944 Validation loss 0.03083934634923935 Accuracy 0.6850000023841858\n",
      "Iteration 23390 Training loss 0.024518277496099472 Validation loss 0.030696505680680275 Accuracy 0.6865000128746033\n",
      "Iteration 23400 Training loss 0.029291173443198204 Validation loss 0.030657902359962463 Accuracy 0.6859999895095825\n",
      "Iteration 23410 Training loss 0.02638852410018444 Validation loss 0.030819561332464218 Accuracy 0.6862999796867371\n",
      "Iteration 23420 Training loss 0.027783343568444252 Validation loss 0.03141308203339577 Accuracy 0.6793000102043152\n",
      "Iteration 23430 Training loss 0.02943073771893978 Validation loss 0.031223861500620842 Accuracy 0.6826000213623047\n",
      "Iteration 23440 Training loss 0.03026542067527771 Validation loss 0.03128018230199814 Accuracy 0.6815000176429749\n",
      "Iteration 23450 Training loss 0.028044534847140312 Validation loss 0.03093727119266987 Accuracy 0.6852999925613403\n",
      "Iteration 23460 Training loss 0.028998764231801033 Validation loss 0.03105691820383072 Accuracy 0.6836000084877014\n",
      "Iteration 23470 Training loss 0.02916637435555458 Validation loss 0.030765864998102188 Accuracy 0.6855999827384949\n",
      "Iteration 23480 Training loss 0.027276506647467613 Validation loss 0.031244369223713875 Accuracy 0.6815999746322632\n",
      "Iteration 23490 Training loss 0.028224129229784012 Validation loss 0.031205181032419205 Accuracy 0.6800000071525574\n",
      "Iteration 23500 Training loss 0.029431521892547607 Validation loss 0.031243594363331795 Accuracy 0.6800000071525574\n",
      "Iteration 23510 Training loss 0.026616165414452553 Validation loss 0.031261902302503586 Accuracy 0.6801000237464905\n",
      "Iteration 23520 Training loss 0.0284001212567091 Validation loss 0.030782600864768028 Accuracy 0.6858000159263611\n",
      "Iteration 23530 Training loss 0.025124821811914444 Validation loss 0.031249526888132095 Accuracy 0.6802999973297119\n",
      "Iteration 23540 Training loss 0.02397340163588524 Validation loss 0.030673310160636902 Accuracy 0.6859999895095825\n",
      "Iteration 23550 Training loss 0.030274134129285812 Validation loss 0.031462132930755615 Accuracy 0.6786999702453613\n",
      "Iteration 23560 Training loss 0.026509592309594154 Validation loss 0.031410183757543564 Accuracy 0.6819000244140625\n",
      "Iteration 23570 Training loss 0.026786845177412033 Validation loss 0.03212810680270195 Accuracy 0.6722999811172485\n",
      "Iteration 23580 Training loss 0.028961744159460068 Validation loss 0.030624443665146828 Accuracy 0.6862000226974487\n",
      "Iteration 23590 Training loss 0.02772091142833233 Validation loss 0.030719023197889328 Accuracy 0.685699999332428\n",
      "Iteration 23600 Training loss 0.028258126229047775 Validation loss 0.0304130669683218 Accuracy 0.6891999840736389\n",
      "Iteration 23610 Training loss 0.027468010783195496 Validation loss 0.031143872067332268 Accuracy 0.6830999851226807\n",
      "Iteration 23620 Training loss 0.026166081428527832 Validation loss 0.030966423451900482 Accuracy 0.6830000281333923\n",
      "Iteration 23630 Training loss 0.025226542726159096 Validation loss 0.030702712014317513 Accuracy 0.6858999729156494\n",
      "Iteration 23640 Training loss 0.029355209320783615 Validation loss 0.030641816556453705 Accuracy 0.6859999895095825\n",
      "Iteration 23650 Training loss 0.03010314516723156 Validation loss 0.030419927090406418 Accuracy 0.6881999969482422\n",
      "Iteration 23660 Training loss 0.02753092534840107 Validation loss 0.03062807209789753 Accuracy 0.6858000159263611\n",
      "Iteration 23670 Training loss 0.026119405403733253 Validation loss 0.030686616897583008 Accuracy 0.6868000030517578\n",
      "Iteration 23680 Training loss 0.02732505276799202 Validation loss 0.03098134510219097 Accuracy 0.6837000250816345\n",
      "Iteration 23690 Training loss 0.027992241084575653 Validation loss 0.031130502000451088 Accuracy 0.6826000213623047\n",
      "Iteration 23700 Training loss 0.02916589379310608 Validation loss 0.03056451492011547 Accuracy 0.6866999864578247\n",
      "Iteration 23710 Training loss 0.026623588055372238 Validation loss 0.030613625422120094 Accuracy 0.6873999834060669\n",
      "Iteration 23720 Training loss 0.0293112825602293 Validation loss 0.030729815363883972 Accuracy 0.6859999895095825\n",
      "Iteration 23730 Training loss 0.0278888288885355 Validation loss 0.03061233088374138 Accuracy 0.6862999796867371\n",
      "Iteration 23740 Training loss 0.029072018340229988 Validation loss 0.03063630685210228 Accuracy 0.6873999834060669\n",
      "Iteration 23750 Training loss 0.028678802773356438 Validation loss 0.030636129900813103 Accuracy 0.6858000159263611\n",
      "Iteration 23760 Training loss 0.029511304572224617 Validation loss 0.03055485524237156 Accuracy 0.6861000061035156\n",
      "Iteration 23770 Training loss 0.025442181155085564 Validation loss 0.031216802075505257 Accuracy 0.6804999709129333\n",
      "Iteration 23780 Training loss 0.030737433582544327 Validation loss 0.030889099463820457 Accuracy 0.6851000189781189\n",
      "Iteration 23790 Training loss 0.028372861444950104 Validation loss 0.031079741194844246 Accuracy 0.682699978351593\n",
      "Iteration 23800 Training loss 0.02795298583805561 Validation loss 0.030752604827284813 Accuracy 0.6858999729156494\n",
      "Iteration 23810 Training loss 0.03024916909635067 Validation loss 0.030932683497667313 Accuracy 0.683899998664856\n",
      "Iteration 23820 Training loss 0.02919911779463291 Validation loss 0.030562756583094597 Accuracy 0.6863999962806702\n",
      "Iteration 23830 Training loss 0.02924622781574726 Validation loss 0.030863383784890175 Accuracy 0.683899998664856\n",
      "Iteration 23840 Training loss 0.028665857389569283 Validation loss 0.03111237846314907 Accuracy 0.6811000108718872\n",
      "Iteration 23850 Training loss 0.032303664833307266 Validation loss 0.03150224685668945 Accuracy 0.6797999739646912\n",
      "Iteration 23860 Training loss 0.027866028249263763 Validation loss 0.030471976846456528 Accuracy 0.6870999932289124\n",
      "Iteration 23870 Training loss 0.02538224868476391 Validation loss 0.03057318553328514 Accuracy 0.6873999834060669\n",
      "Iteration 23880 Training loss 0.03056471236050129 Validation loss 0.031071331351995468 Accuracy 0.6818000078201294\n",
      "Iteration 23890 Training loss 0.0300177913159132 Validation loss 0.030998989939689636 Accuracy 0.682699978351593\n",
      "Iteration 23900 Training loss 0.032030295580625534 Validation loss 0.031196536496281624 Accuracy 0.6825000047683716\n",
      "Iteration 23910 Training loss 0.031101521104574203 Validation loss 0.030524713918566704 Accuracy 0.6873000264167786\n",
      "Iteration 23920 Training loss 0.02745809592306614 Validation loss 0.030555156990885735 Accuracy 0.6863999962806702\n",
      "Iteration 23930 Training loss 0.02911210060119629 Validation loss 0.031657081097364426 Accuracy 0.6761000156402588\n",
      "Iteration 23940 Training loss 0.031128868460655212 Validation loss 0.030945850536227226 Accuracy 0.6852999925613403\n",
      "Iteration 23950 Training loss 0.027783706784248352 Validation loss 0.030442392453551292 Accuracy 0.6875\n",
      "Iteration 23960 Training loss 0.02775939367711544 Validation loss 0.03188566491007805 Accuracy 0.6728000044822693\n",
      "Iteration 23970 Training loss 0.028678342700004578 Validation loss 0.03087928518652916 Accuracy 0.6829000115394592\n",
      "Iteration 23980 Training loss 0.02806435152888298 Validation loss 0.030594833195209503 Accuracy 0.6858999729156494\n",
      "Iteration 23990 Training loss 0.022847114130854607 Validation loss 0.03064749576151371 Accuracy 0.6875\n",
      "Iteration 24000 Training loss 0.028034737333655357 Validation loss 0.031499188393354416 Accuracy 0.6765999794006348\n",
      "Iteration 24010 Training loss 0.02485080249607563 Validation loss 0.030976109206676483 Accuracy 0.6820999979972839\n",
      "Iteration 24020 Training loss 0.030065098777413368 Validation loss 0.030648838728666306 Accuracy 0.6854000091552734\n",
      "Iteration 24030 Training loss 0.026029588654637337 Validation loss 0.030576620250940323 Accuracy 0.6862000226974487\n",
      "Iteration 24040 Training loss 0.02589499205350876 Validation loss 0.030482590198516846 Accuracy 0.6861000061035156\n",
      "Iteration 24050 Training loss 0.02919456735253334 Validation loss 0.030585385859012604 Accuracy 0.6862000226974487\n",
      "Iteration 24060 Training loss 0.029898926615715027 Validation loss 0.030622396618127823 Accuracy 0.6863999962806702\n",
      "Iteration 24070 Training loss 0.0316525362432003 Validation loss 0.031379397958517075 Accuracy 0.6801999807357788\n",
      "Iteration 24080 Training loss 0.02678203396499157 Validation loss 0.03052462823688984 Accuracy 0.6880999803543091\n",
      "Iteration 24090 Training loss 0.029068954288959503 Validation loss 0.031059738248586655 Accuracy 0.6836000084877014\n",
      "Iteration 24100 Training loss 0.025724977254867554 Validation loss 0.0313309021294117 Accuracy 0.6782000064849854\n",
      "Iteration 24110 Training loss 0.024530252441763878 Validation loss 0.031090663745999336 Accuracy 0.6822999715805054\n",
      "Iteration 24120 Training loss 0.028455544263124466 Validation loss 0.031048862263560295 Accuracy 0.6800000071525574\n",
      "Iteration 24130 Training loss 0.030222803354263306 Validation loss 0.031359437853097916 Accuracy 0.6775000095367432\n",
      "Iteration 24140 Training loss 0.026968296617269516 Validation loss 0.030549025163054466 Accuracy 0.6869999766349792\n",
      "Iteration 24150 Training loss 0.027189580723643303 Validation loss 0.03087763488292694 Accuracy 0.6837999820709229\n",
      "Iteration 24160 Training loss 0.024464886635541916 Validation loss 0.03108222596347332 Accuracy 0.680899977684021\n",
      "Iteration 24170 Training loss 0.026910707354545593 Validation loss 0.030742699280381203 Accuracy 0.6851999759674072\n",
      "Iteration 24180 Training loss 0.031087076291441917 Validation loss 0.03128635510802269 Accuracy 0.679099977016449\n",
      "Iteration 24190 Training loss 0.025976257398724556 Validation loss 0.03050762228667736 Accuracy 0.6866000294685364\n",
      "Iteration 24200 Training loss 0.026739614084362984 Validation loss 0.03077172301709652 Accuracy 0.6855000257492065\n",
      "Iteration 24210 Training loss 0.02859378047287464 Validation loss 0.03213314339518547 Accuracy 0.6705999970436096\n",
      "Iteration 24220 Training loss 0.029148263856768608 Validation loss 0.03068372793495655 Accuracy 0.6832000017166138\n",
      "Iteration 24230 Training loss 0.02789577655494213 Validation loss 0.030816223472356796 Accuracy 0.6837000250816345\n",
      "Iteration 24240 Training loss 0.031069038435816765 Validation loss 0.03177079185843468 Accuracy 0.6710000038146973\n",
      "Iteration 24250 Training loss 0.02928907237946987 Validation loss 0.03083030693233013 Accuracy 0.6848000288009644\n",
      "Iteration 24260 Training loss 0.02851255051791668 Validation loss 0.03088599070906639 Accuracy 0.6850000023841858\n",
      "Iteration 24270 Training loss 0.027528507634997368 Validation loss 0.030421733856201172 Accuracy 0.6869000196456909\n",
      "Iteration 24280 Training loss 0.03137528896331787 Validation loss 0.03088998608291149 Accuracy 0.6836000084877014\n",
      "Iteration 24290 Training loss 0.026960061863064766 Validation loss 0.030539829283952713 Accuracy 0.6834999918937683\n",
      "Iteration 24300 Training loss 0.026649508625268936 Validation loss 0.03068525902926922 Accuracy 0.6848000288009644\n",
      "Iteration 24310 Training loss 0.025999363511800766 Validation loss 0.030850877985358238 Accuracy 0.6833000183105469\n",
      "Iteration 24320 Training loss 0.026175901293754578 Validation loss 0.03111695870757103 Accuracy 0.6790000200271606\n",
      "Iteration 24330 Training loss 0.02863520011305809 Validation loss 0.03148878738284111 Accuracy 0.676800012588501\n",
      "Iteration 24340 Training loss 0.03137430548667908 Validation loss 0.030924545601010323 Accuracy 0.682699978351593\n",
      "Iteration 24350 Training loss 0.02725614607334137 Validation loss 0.03057599626481533 Accuracy 0.6848999857902527\n",
      "Iteration 24360 Training loss 0.025889534503221512 Validation loss 0.03082944266498089 Accuracy 0.6848000288009644\n",
      "Iteration 24370 Training loss 0.024981889873743057 Validation loss 0.030786057934165 Accuracy 0.6836000084877014\n",
      "Iteration 24380 Training loss 0.027933968231081963 Validation loss 0.030501188710331917 Accuracy 0.6876000165939331\n",
      "Iteration 24390 Training loss 0.026007438078522682 Validation loss 0.03033999539911747 Accuracy 0.6879000067710876\n",
      "Iteration 24400 Training loss 0.028240643441677094 Validation loss 0.031142432242631912 Accuracy 0.680899977684021\n",
      "Iteration 24410 Training loss 0.029091864824295044 Validation loss 0.030563032254576683 Accuracy 0.6858999729156494\n",
      "Iteration 24420 Training loss 0.031289272010326385 Validation loss 0.03204317018389702 Accuracy 0.671500027179718\n",
      "Iteration 24430 Training loss 0.025381771847605705 Validation loss 0.030283769592642784 Accuracy 0.6881999969482422\n",
      "Iteration 24440 Training loss 0.027782121673226357 Validation loss 0.03025687299668789 Accuracy 0.6886000037193298\n",
      "Iteration 24450 Training loss 0.029449796304106712 Validation loss 0.030230386182665825 Accuracy 0.6886000037193298\n",
      "Iteration 24460 Training loss 0.027712076902389526 Validation loss 0.030465202406048775 Accuracy 0.685699999332428\n",
      "Iteration 24470 Training loss 0.026617838069796562 Validation loss 0.030508991330862045 Accuracy 0.6848999857902527\n",
      "Iteration 24480 Training loss 0.027046166360378265 Validation loss 0.03093765117228031 Accuracy 0.6815999746322632\n",
      "Iteration 24490 Training loss 0.028186574578285217 Validation loss 0.030681965872645378 Accuracy 0.6833999752998352\n",
      "Iteration 24500 Training loss 0.02845711261034012 Validation loss 0.03055809624493122 Accuracy 0.6837000250816345\n",
      "Iteration 24510 Training loss 0.029215048998594284 Validation loss 0.030665606260299683 Accuracy 0.682699978351593\n",
      "Iteration 24520 Training loss 0.025957630947232246 Validation loss 0.03015884757041931 Accuracy 0.6873000264167786\n",
      "Iteration 24530 Training loss 0.027569672092795372 Validation loss 0.031953416764736176 Accuracy 0.6700999736785889\n",
      "Iteration 24540 Training loss 0.025711888447403908 Validation loss 0.030088474974036217 Accuracy 0.6881999969482422\n",
      "Iteration 24550 Training loss 0.028950147330760956 Validation loss 0.030451001599431038 Accuracy 0.684499979019165\n",
      "Iteration 24560 Training loss 0.027037419378757477 Validation loss 0.03007606416940689 Accuracy 0.6873999834060669\n",
      "Iteration 24570 Training loss 0.02782924473285675 Validation loss 0.030325038358569145 Accuracy 0.6866000294685364\n",
      "Iteration 24580 Training loss 0.029966264963150024 Validation loss 0.030809558928012848 Accuracy 0.6830000281333923\n",
      "Iteration 24590 Training loss 0.02980676479637623 Validation loss 0.0305797029286623 Accuracy 0.6869000196456909\n",
      "Iteration 24600 Training loss 0.028030598536133766 Validation loss 0.030411018058657646 Accuracy 0.6847000122070312\n",
      "Iteration 24610 Training loss 0.027550093829631805 Validation loss 0.030114755034446716 Accuracy 0.6905999779701233\n",
      "Iteration 24620 Training loss 0.02606930024921894 Validation loss 0.029744958505034447 Accuracy 0.6916999816894531\n",
      "Iteration 24630 Training loss 0.02593328431248665 Validation loss 0.030335810035467148 Accuracy 0.6865000128746033\n",
      "Iteration 24640 Training loss 0.029879383742809296 Validation loss 0.030307704582810402 Accuracy 0.6876000165939331\n",
      "Iteration 24650 Training loss 0.025148572400212288 Validation loss 0.03030000440776348 Accuracy 0.6873999834060669\n",
      "Iteration 24660 Training loss 0.02779364585876465 Validation loss 0.03041384369134903 Accuracy 0.6841999888420105\n",
      "Iteration 24670 Training loss 0.030842628329992294 Validation loss 0.030652308836579323 Accuracy 0.6877999901771545\n",
      "Iteration 24680 Training loss 0.030163338407874107 Validation loss 0.03051435947418213 Accuracy 0.6869000196456909\n",
      "Iteration 24690 Training loss 0.027577193453907967 Validation loss 0.030408285558223724 Accuracy 0.6862999796867371\n",
      "Iteration 24700 Training loss 0.031031392514705658 Validation loss 0.031089043244719505 Accuracy 0.6815999746322632\n",
      "Iteration 24710 Training loss 0.02624056674540043 Validation loss 0.030103791505098343 Accuracy 0.6865000128746033\n",
      "Iteration 24720 Training loss 0.027762170881032944 Validation loss 0.030206382274627686 Accuracy 0.6901000142097473\n",
      "Iteration 24730 Training loss 0.027340535074472427 Validation loss 0.03075152263045311 Accuracy 0.682200014591217\n",
      "Iteration 24740 Training loss 0.028057824820280075 Validation loss 0.03055424615740776 Accuracy 0.6830999851226807\n",
      "Iteration 24750 Training loss 0.029333394020795822 Validation loss 0.030846089124679565 Accuracy 0.6814000010490417\n",
      "Iteration 24760 Training loss 0.02645937167108059 Validation loss 0.030586980283260345 Accuracy 0.6815999746322632\n",
      "Iteration 24770 Training loss 0.02669687569141388 Validation loss 0.030914079397916794 Accuracy 0.6796000003814697\n",
      "Iteration 24780 Training loss 0.027581918984651566 Validation loss 0.030325980857014656 Accuracy 0.685699999332428\n",
      "Iteration 24790 Training loss 0.026153966784477234 Validation loss 0.030136559158563614 Accuracy 0.6865000128746033\n",
      "Iteration 24800 Training loss 0.02519252337515354 Validation loss 0.030393224209547043 Accuracy 0.6843000054359436\n",
      "Iteration 24810 Training loss 0.024101264774799347 Validation loss 0.030306674540042877 Accuracy 0.6837999820709229\n",
      "Iteration 24820 Training loss 0.027120288461446762 Validation loss 0.030097678303718567 Accuracy 0.6861000061035156\n",
      "Iteration 24830 Training loss 0.026857858523726463 Validation loss 0.030293114483356476 Accuracy 0.6845999956130981\n",
      "Iteration 24840 Training loss 0.02517426386475563 Validation loss 0.030563483014702797 Accuracy 0.6815000176429749\n",
      "Iteration 24850 Training loss 0.023529566824436188 Validation loss 0.02971280924975872 Accuracy 0.689300000667572\n",
      "Iteration 24860 Training loss 0.024717088788747787 Validation loss 0.030484749004244804 Accuracy 0.6818000078201294\n",
      "Iteration 24870 Training loss 0.025708714500069618 Validation loss 0.031176362186670303 Accuracy 0.682200014591217\n",
      "Iteration 24880 Training loss 0.027110107243061066 Validation loss 0.030115799978375435 Accuracy 0.6845999956130981\n",
      "Iteration 24890 Training loss 0.026446064934134483 Validation loss 0.030548978596925735 Accuracy 0.6863999962806702\n",
      "Iteration 24900 Training loss 0.02381463162600994 Validation loss 0.02999746799468994 Accuracy 0.6869999766349792\n",
      "Iteration 24910 Training loss 0.02905512973666191 Validation loss 0.03071683831512928 Accuracy 0.6809999942779541\n",
      "Iteration 24920 Training loss 0.02640560455620289 Validation loss 0.03002144955098629 Accuracy 0.6852999925613403\n",
      "Iteration 24930 Training loss 0.027210088446736336 Validation loss 0.030081139877438545 Accuracy 0.6855999827384949\n",
      "Iteration 24940 Training loss 0.029702870175242424 Validation loss 0.029891762882471085 Accuracy 0.6865000128746033\n",
      "Iteration 24950 Training loss 0.025879399850964546 Validation loss 0.029828010126948357 Accuracy 0.6869000196456909\n",
      "Iteration 24960 Training loss 0.026321927085518837 Validation loss 0.030816111713647842 Accuracy 0.6797000169754028\n",
      "Iteration 24970 Training loss 0.026999687775969505 Validation loss 0.03005574271082878 Accuracy 0.6873999834060669\n",
      "Iteration 24980 Training loss 0.023925071582198143 Validation loss 0.029508210718631744 Accuracy 0.6897000074386597\n",
      "Iteration 24990 Training loss 0.025173906236886978 Validation loss 0.029772240668535233 Accuracy 0.6891999840736389\n",
      "Iteration 25000 Training loss 0.026295166462659836 Validation loss 0.02972157672047615 Accuracy 0.6884999871253967\n",
      "Iteration 25010 Training loss 0.028876198455691338 Validation loss 0.031000297516584396 Accuracy 0.6825000047683716\n",
      "Iteration 25020 Training loss 0.028350424021482468 Validation loss 0.029971012845635414 Accuracy 0.6847000122070312\n",
      "Iteration 25030 Training loss 0.027028782293200493 Validation loss 0.030558481812477112 Accuracy 0.6830000281333923\n",
      "Iteration 25040 Training loss 0.02968093939125538 Validation loss 0.03019331581890583 Accuracy 0.6869999766349792\n",
      "Iteration 25050 Training loss 0.025198407471179962 Validation loss 0.029532765969634056 Accuracy 0.6881999969482422\n",
      "Iteration 25060 Training loss 0.024553243070840836 Validation loss 0.030420005321502686 Accuracy 0.6884999871253967\n",
      "Iteration 25070 Training loss 0.02553802914917469 Validation loss 0.030246959999203682 Accuracy 0.6852999925613403\n",
      "Iteration 25080 Training loss 0.026297444477677345 Validation loss 0.030072277411818504 Accuracy 0.682200014591217\n",
      "Iteration 25090 Training loss 0.028132397681474686 Validation loss 0.030176110565662384 Accuracy 0.6873999834060669\n",
      "Iteration 25100 Training loss 0.026757895946502686 Validation loss 0.030144182965159416 Accuracy 0.6836000084877014\n",
      "Iteration 25110 Training loss 0.024323521181941032 Validation loss 0.02990097366273403 Accuracy 0.6847000122070312\n",
      "Iteration 25120 Training loss 0.025864167138934135 Validation loss 0.029614757746458054 Accuracy 0.6868000030517578\n",
      "Iteration 25130 Training loss 0.026936698704957962 Validation loss 0.029435832053422928 Accuracy 0.6880000233650208\n",
      "Iteration 25140 Training loss 0.029954098165035248 Validation loss 0.030646730214357376 Accuracy 0.6823999881744385\n",
      "Iteration 25150 Training loss 0.02768869698047638 Validation loss 0.03031368926167488 Accuracy 0.6833000183105469\n",
      "Iteration 25160 Training loss 0.02728109247982502 Validation loss 0.030099572613835335 Accuracy 0.6812000274658203\n",
      "Iteration 25170 Training loss 0.02817964181303978 Validation loss 0.029499921947717667 Accuracy 0.6881999969482422\n",
      "Iteration 25180 Training loss 0.02649713307619095 Validation loss 0.02994055673480034 Accuracy 0.6869000196456909\n",
      "Iteration 25190 Training loss 0.029678426682949066 Validation loss 0.030116038396954536 Accuracy 0.6891999840736389\n",
      "Iteration 25200 Training loss 0.022223306819796562 Validation loss 0.02944091334939003 Accuracy 0.690500020980835\n",
      "Iteration 25210 Training loss 0.032655663788318634 Validation loss 0.030141731724143028 Accuracy 0.6861000061035156\n",
      "Iteration 25220 Training loss 0.02421645075082779 Validation loss 0.02934880368411541 Accuracy 0.6876999735832214\n",
      "Iteration 25230 Training loss 0.027933018282055855 Validation loss 0.030317801982164383 Accuracy 0.6850000023841858\n",
      "Iteration 25240 Training loss 0.02979128248989582 Validation loss 0.030745498836040497 Accuracy 0.6866000294685364\n",
      "Iteration 25250 Training loss 0.029612110927700996 Validation loss 0.02973550371825695 Accuracy 0.6859999895095825\n",
      "Iteration 25260 Training loss 0.024793896824121475 Validation loss 0.02943972684442997 Accuracy 0.6888999938964844\n",
      "Iteration 25270 Training loss 0.027151448652148247 Validation loss 0.029484888538718224 Accuracy 0.6873999834060669\n",
      "Iteration 25280 Training loss 0.02787640318274498 Validation loss 0.030071571469306946 Accuracy 0.6851000189781189\n",
      "Iteration 25290 Training loss 0.028747444972395897 Validation loss 0.029746072366833687 Accuracy 0.6876000165939331\n",
      "Iteration 25300 Training loss 0.032192397862672806 Validation loss 0.030125169083476067 Accuracy 0.6863999962806702\n",
      "Iteration 25310 Training loss 0.02498731017112732 Validation loss 0.029991764575242996 Accuracy 0.6858999729156494\n",
      "Iteration 25320 Training loss 0.026319129392504692 Validation loss 0.029512973502278328 Accuracy 0.6872000098228455\n",
      "Iteration 25330 Training loss 0.024629490450024605 Validation loss 0.02978442795574665 Accuracy 0.6833000183105469\n",
      "Iteration 25340 Training loss 0.02898973599076271 Validation loss 0.03025965392589569 Accuracy 0.6881999969482422\n",
      "Iteration 25350 Training loss 0.02947501838207245 Validation loss 0.030446946620941162 Accuracy 0.6782000064849854\n",
      "Iteration 25360 Training loss 0.02743780054152012 Validation loss 0.030088290572166443 Accuracy 0.6868000030517578\n",
      "Iteration 25370 Training loss 0.02721457928419113 Validation loss 0.02947305142879486 Accuracy 0.6859999895095825\n",
      "Iteration 25380 Training loss 0.026653340086340904 Validation loss 0.029448842629790306 Accuracy 0.6847000122070312\n",
      "Iteration 25390 Training loss 0.0246907789260149 Validation loss 0.03028797172009945 Accuracy 0.6848999857902527\n",
      "Iteration 25400 Training loss 0.02323109656572342 Validation loss 0.02952849119901657 Accuracy 0.6862000226974487\n",
      "Iteration 25410 Training loss 0.026499781757593155 Validation loss 0.02926972322165966 Accuracy 0.6859999895095825\n",
      "Iteration 25420 Training loss 0.026284337043762207 Validation loss 0.02998953126370907 Accuracy 0.6834999918937683\n",
      "Iteration 25430 Training loss 0.024648208171129227 Validation loss 0.02891942299902439 Accuracy 0.6887999773025513\n",
      "Iteration 25440 Training loss 0.027656227350234985 Validation loss 0.030516771599650383 Accuracy 0.6858000159263611\n",
      "Iteration 25450 Training loss 0.025407826527953148 Validation loss 0.029717763885855675 Accuracy 0.6814000010490417\n",
      "Iteration 25460 Training loss 0.025859519839286804 Validation loss 0.029409589245915413 Accuracy 0.6862000226974487\n",
      "Iteration 25470 Training loss 0.02394104190170765 Validation loss 0.030490627512335777 Accuracy 0.6815000176429749\n",
      "Iteration 25480 Training loss 0.025411807000637054 Validation loss 0.02917870692908764 Accuracy 0.6894000172615051\n",
      "Iteration 25490 Training loss 0.025839542970061302 Validation loss 0.029545361176133156 Accuracy 0.6879000067710876\n",
      "Iteration 25500 Training loss 0.027409106492996216 Validation loss 0.029609326273202896 Accuracy 0.6887999773025513\n",
      "Iteration 25510 Training loss 0.02614898420870304 Validation loss 0.02940092794597149 Accuracy 0.6879000067710876\n",
      "Iteration 25520 Training loss 0.02667289413511753 Validation loss 0.029374882578849792 Accuracy 0.6858999729156494\n",
      "Iteration 25530 Training loss 0.0281345434486866 Validation loss 0.0307446476072073 Accuracy 0.680400013923645\n",
      "Iteration 25540 Training loss 0.02633974701166153 Validation loss 0.030059898272156715 Accuracy 0.6816999912261963\n",
      "Iteration 25550 Training loss 0.025191091001033783 Validation loss 0.028835801407694817 Accuracy 0.6883999705314636\n",
      "Iteration 25560 Training loss 0.024675501510500908 Validation loss 0.028832660987973213 Accuracy 0.6890000104904175\n",
      "Iteration 25570 Training loss 0.025167711079120636 Validation loss 0.029989639297127724 Accuracy 0.680899977684021\n",
      "Iteration 25580 Training loss 0.024887608364224434 Validation loss 0.028447670862078667 Accuracy 0.6898999810218811\n",
      "Iteration 25590 Training loss 0.02906191162765026 Validation loss 0.02922321856021881 Accuracy 0.6880999803543091\n",
      "Iteration 25600 Training loss 0.02420748956501484 Validation loss 0.029629399999976158 Accuracy 0.6814000010490417\n",
      "Iteration 25610 Training loss 0.025157105177640915 Validation loss 0.028959404677152634 Accuracy 0.6876999735832214\n",
      "Iteration 25620 Training loss 0.02449214644730091 Validation loss 0.028680678457021713 Accuracy 0.6866000294685364\n",
      "Iteration 25630 Training loss 0.024215377867221832 Validation loss 0.029287319630384445 Accuracy 0.6855999827384949\n",
      "Iteration 25640 Training loss 0.026070287451148033 Validation loss 0.02855686843395233 Accuracy 0.6899999976158142\n",
      "Iteration 25650 Training loss 0.024787986651062965 Validation loss 0.030435683205723763 Accuracy 0.6801999807357788\n",
      "Iteration 25660 Training loss 0.02486049383878708 Validation loss 0.02972445636987686 Accuracy 0.682699978351593\n",
      "Iteration 25670 Training loss 0.027587778866291046 Validation loss 0.028426894918084145 Accuracy 0.6888999938964844\n",
      "Iteration 25680 Training loss 0.026264755055308342 Validation loss 0.030239209532737732 Accuracy 0.6791999936103821\n",
      "Iteration 25690 Training loss 0.028265981003642082 Validation loss 0.0284440740942955 Accuracy 0.6888999938964844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[197]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_2_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[195]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mtwo_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, coef_iter, lr, reg1, reg2, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2)\u001b[39m\n\u001b[32m     99\u001b[39m grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[\u001b[32m0\u001b[39m] \u001b[38;5;66;03m# shape (hidden_1_size, input_dimension)\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Calcul de la moyenne empirique de dLoss/db1 par backpropagation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m grad_b1 = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_z1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.unsqueeze(\u001b[32m1\u001b[39m) \n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\u001b[39;00m\n\u001b[32m    103\u001b[39m grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[\u001b[32m0\u001b[39m] \u001b[38;5;66;03m# shape (number_of_classes, hidden_1_size)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 1, 1e-3, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 256, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c253240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter 614656\n",
      "Iteration 0 Training loss 0.09037034213542938 Validation loss 0.0911577120423317 Accuracy 0.06849999725818634\n",
      "Iteration 10 Training loss 0.09045189619064331 Validation loss 0.08996108174324036 Accuracy 0.09989999979734421\n",
      "Iteration 20 Training loss 0.08905860036611557 Validation loss 0.08990060538053513 Accuracy 0.10000000149011612\n",
      "Iteration 30 Training loss 0.08181218057870865 Validation loss 0.08094397187232971 Accuracy 0.188400000333786\n",
      "Iteration 40 Training loss 0.08204544335603714 Validation loss 0.0807521715760231 Accuracy 0.1906999945640564\n",
      "Iteration 50 Training loss 0.07834230363368988 Validation loss 0.08011805266141891 Accuracy 0.19679999351501465\n",
      "Iteration 60 Training loss 0.08536844700574875 Validation loss 0.08033107221126556 Accuracy 0.19529999792575836\n",
      "Iteration 70 Training loss 0.07998275011777878 Validation loss 0.0800274908542633 Accuracy 0.19830000400543213\n",
      "Iteration 80 Training loss 0.07854796200990677 Validation loss 0.08001963049173355 Accuracy 0.19830000400543213\n",
      "Iteration 90 Training loss 0.08045439422130585 Validation loss 0.08001154661178589 Accuracy 0.19840000569820404\n",
      "Iteration 100 Training loss 0.08068906515836716 Validation loss 0.07998121529817581 Accuracy 0.1987999975681305\n",
      "Iteration 110 Training loss 0.08041918277740479 Validation loss 0.08005619794130325 Accuracy 0.19779999554157257\n",
      "Iteration 120 Training loss 0.07962453365325928 Validation loss 0.08015395700931549 Accuracy 0.19609999656677246\n",
      "Iteration 130 Training loss 0.07902385294437408 Validation loss 0.07996341586112976 Accuracy 0.1988999992609024\n",
      "Iteration 140 Training loss 0.07837272435426712 Validation loss 0.08000253885984421 Accuracy 0.19900000095367432\n",
      "Iteration 150 Training loss 0.08057446032762527 Validation loss 0.08008471876382828 Accuracy 0.1979999989271164\n",
      "Iteration 160 Training loss 0.07746390998363495 Validation loss 0.0799466073513031 Accuracy 0.19949999451637268\n",
      "Iteration 170 Training loss 0.0797993466258049 Validation loss 0.08007998764514923 Accuracy 0.19820000231266022\n",
      "Iteration 180 Training loss 0.08171632140874863 Validation loss 0.07991757988929749 Accuracy 0.1995999962091446\n",
      "Iteration 190 Training loss 0.07913453131914139 Validation loss 0.07975443452596664 Accuracy 0.20059999823570251\n",
      "Iteration 200 Training loss 0.0729529857635498 Validation loss 0.07469101995229721 Accuracy 0.2502000033855438\n",
      "Iteration 210 Training loss 0.06898452341556549 Validation loss 0.07280883193016052 Accuracy 0.2680000066757202\n",
      "Iteration 220 Training loss 0.06988199055194855 Validation loss 0.0721573755145073 Accuracy 0.2757999897003174\n",
      "Iteration 230 Training loss 0.0670866072177887 Validation loss 0.07198978215456009 Accuracy 0.27790001034736633\n",
      "Iteration 240 Training loss 0.07294974476099014 Validation loss 0.07238175719976425 Accuracy 0.27309998869895935\n",
      "Iteration 250 Training loss 0.07450404018163681 Validation loss 0.0720013901591301 Accuracy 0.2773999869823456\n",
      "Iteration 260 Training loss 0.06687469035387039 Validation loss 0.07199012488126755 Accuracy 0.27810001373291016\n",
      "Iteration 270 Training loss 0.07159215211868286 Validation loss 0.07200335711240768 Accuracy 0.2775999903678894\n",
      "Iteration 280 Training loss 0.07209012657403946 Validation loss 0.07280241698026657 Accuracy 0.26919999718666077\n",
      "Iteration 290 Training loss 0.0730651319026947 Validation loss 0.07220634818077087 Accuracy 0.27480000257492065\n",
      "Iteration 300 Training loss 0.07294273376464844 Validation loss 0.07268518209457397 Accuracy 0.2711000144481659\n",
      "Iteration 310 Training loss 0.07174446433782578 Validation loss 0.07159677892923355 Accuracy 0.2815999984741211\n",
      "Iteration 320 Training loss 0.07046180963516235 Validation loss 0.07168979197740555 Accuracy 0.28040000796318054\n",
      "Iteration 330 Training loss 0.07607615739107132 Validation loss 0.07313749194145203 Accuracy 0.2662000060081482\n",
      "Iteration 340 Training loss 0.07190624624490738 Validation loss 0.07183246314525604 Accuracy 0.27880001068115234\n",
      "Iteration 350 Training loss 0.07574765384197235 Validation loss 0.07146116346120834 Accuracy 0.28299999237060547\n",
      "Iteration 360 Training loss 0.07416567951440811 Validation loss 0.07139338552951813 Accuracy 0.28349998593330383\n",
      "Iteration 370 Training loss 0.06967479735612869 Validation loss 0.07125064730644226 Accuracy 0.2840000092983246\n",
      "Iteration 380 Training loss 0.06508859992027283 Validation loss 0.06425546109676361 Accuracy 0.353300005197525\n",
      "Iteration 390 Training loss 0.06186794489622116 Validation loss 0.06439634412527084 Accuracy 0.3499999940395355\n",
      "Iteration 400 Training loss 0.0624857023358345 Validation loss 0.06648372858762741 Accuracy 0.3296000063419342\n",
      "Iteration 410 Training loss 0.060964833945035934 Validation loss 0.06558450311422348 Accuracy 0.33980000019073486\n",
      "Iteration 420 Training loss 0.06331581622362137 Validation loss 0.06365528702735901 Accuracy 0.3596999943256378\n",
      "Iteration 430 Training loss 0.06232038885354996 Validation loss 0.0642608031630516 Accuracy 0.35260000824928284\n",
      "Iteration 440 Training loss 0.06493185460567474 Validation loss 0.06373128294944763 Accuracy 0.3596000075340271\n",
      "Iteration 450 Training loss 0.06407807767391205 Validation loss 0.06429433077573776 Accuracy 0.35370001196861267\n",
      "Iteration 460 Training loss 0.06012653559446335 Validation loss 0.06336122751235962 Accuracy 0.36250001192092896\n",
      "Iteration 470 Training loss 0.062299005687236786 Validation loss 0.0645693764090538 Accuracy 0.3513999879360199\n",
      "Iteration 480 Training loss 0.06408561766147614 Validation loss 0.06352473795413971 Accuracy 0.3617999851703644\n",
      "Iteration 490 Training loss 0.0626082718372345 Validation loss 0.06353659182786942 Accuracy 0.36160001158714294\n",
      "Iteration 500 Training loss 0.06309930235147476 Validation loss 0.06395553052425385 Accuracy 0.3571000099182129\n",
      "Iteration 510 Training loss 0.06275996565818787 Validation loss 0.06363540142774582 Accuracy 0.3598000109195709\n",
      "Iteration 520 Training loss 0.063437819480896 Validation loss 0.06393197923898697 Accuracy 0.357699990272522\n",
      "Iteration 530 Training loss 0.0652013048529625 Validation loss 0.06362919509410858 Accuracy 0.3587999939918518\n",
      "Iteration 540 Training loss 0.0647539421916008 Validation loss 0.06326906383037567 Accuracy 0.36390000581741333\n",
      "Iteration 550 Training loss 0.059570129960775375 Validation loss 0.06304589658975601 Accuracy 0.36579999327659607\n",
      "Iteration 560 Training loss 0.0622604563832283 Validation loss 0.06319545209407806 Accuracy 0.3637000024318695\n",
      "Iteration 570 Training loss 0.06379011273384094 Validation loss 0.06397707760334015 Accuracy 0.3573000133037567\n",
      "Iteration 580 Training loss 0.06642395257949829 Validation loss 0.06278558820486069 Accuracy 0.3677999973297119\n",
      "Iteration 590 Training loss 0.0629751905798912 Validation loss 0.06303129345178604 Accuracy 0.36570000648498535\n",
      "Iteration 600 Training loss 0.05953890457749367 Validation loss 0.06358800828456879 Accuracy 0.3610000014305115\n",
      "Iteration 610 Training loss 0.06405895948410034 Validation loss 0.062907375395298 Accuracy 0.3675000071525574\n",
      "Iteration 620 Training loss 0.06483384966850281 Validation loss 0.06382057815790176 Accuracy 0.3578000068664551\n",
      "Iteration 630 Training loss 0.0638924315571785 Validation loss 0.06395237147808075 Accuracy 0.35519999265670776\n",
      "Iteration 640 Training loss 0.06317803263664246 Validation loss 0.06276126950979233 Accuracy 0.3677000105381012\n",
      "Iteration 650 Training loss 0.06445808708667755 Validation loss 0.06417426466941833 Accuracy 0.35519999265670776\n",
      "Iteration 660 Training loss 0.06082572415471077 Validation loss 0.06336163729429245 Accuracy 0.3625999987125397\n",
      "Iteration 670 Training loss 0.06251984089612961 Validation loss 0.06293830275535583 Accuracy 0.3666999936103821\n",
      "Iteration 680 Training loss 0.062224019318819046 Validation loss 0.06272782385349274 Accuracy 0.36910000443458557\n",
      "Iteration 690 Training loss 0.06464755535125732 Validation loss 0.06310402601957321 Accuracy 0.3643999993801117\n",
      "Iteration 700 Training loss 0.05964647978544235 Validation loss 0.06373231112957001 Accuracy 0.3594000041484833\n",
      "Iteration 710 Training loss 0.06150675565004349 Validation loss 0.06258181482553482 Accuracy 0.3691999912261963\n",
      "Iteration 720 Training loss 0.06186450645327568 Validation loss 0.06255993992090225 Accuracy 0.3709999918937683\n",
      "Iteration 730 Training loss 0.06194053217768669 Validation loss 0.06344940513372421 Accuracy 0.36070001125335693\n",
      "Iteration 740 Training loss 0.06373968720436096 Validation loss 0.06288843601942062 Accuracy 0.3666999936103821\n",
      "Iteration 750 Training loss 0.06066832318902016 Validation loss 0.06289143860340118 Accuracy 0.3668999969959259\n",
      "Iteration 760 Training loss 0.06256639957427979 Validation loss 0.06388697028160095 Accuracy 0.3578999936580658\n",
      "Iteration 770 Training loss 0.06794822216033936 Validation loss 0.06325346976518631 Accuracy 0.36329999566078186\n",
      "Iteration 780 Training loss 0.06306003034114838 Validation loss 0.06264995783567429 Accuracy 0.3682999908924103\n",
      "Iteration 790 Training loss 0.06494558602571487 Validation loss 0.0630159080028534 Accuracy 0.36579999327659607\n",
      "Iteration 800 Training loss 0.06411664932966232 Validation loss 0.06369291245937347 Accuracy 0.35920000076293945\n",
      "Iteration 810 Training loss 0.06249600276350975 Validation loss 0.06278946995735168 Accuracy 0.3668999969959259\n",
      "Iteration 820 Training loss 0.05808546021580696 Validation loss 0.06275010854005814 Accuracy 0.36880001425743103\n",
      "Iteration 830 Training loss 0.06333038955926895 Validation loss 0.063353531062603 Accuracy 0.3621000051498413\n",
      "Iteration 840 Training loss 0.06375685334205627 Validation loss 0.06247429549694061 Accuracy 0.3714999854564667\n",
      "Iteration 850 Training loss 0.06590526551008224 Validation loss 0.06353304535150528 Accuracy 0.36039999127388\n",
      "Iteration 860 Training loss 0.0586932972073555 Validation loss 0.06254878640174866 Accuracy 0.3702999949455261\n",
      "Iteration 870 Training loss 0.06570494920015335 Validation loss 0.06244652345776558 Accuracy 0.3718999922275543\n",
      "Iteration 880 Training loss 0.06337263435125351 Validation loss 0.062387529760599136 Accuracy 0.37220001220703125\n",
      "Iteration 890 Training loss 0.058315105736255646 Validation loss 0.059196364134550095 Accuracy 0.40139999985694885\n",
      "Iteration 900 Training loss 0.05323340371251106 Validation loss 0.05434335395693779 Accuracy 0.4523000121116638\n",
      "Iteration 910 Training loss 0.053834158927202225 Validation loss 0.054316915571689606 Accuracy 0.45329999923706055\n",
      "Iteration 920 Training loss 0.05409975349903107 Validation loss 0.05408809706568718 Accuracy 0.4560000002384186\n",
      "Iteration 930 Training loss 0.05105510354042053 Validation loss 0.053618788719177246 Accuracy 0.4602999985218048\n",
      "Iteration 940 Training loss 0.05443963035941124 Validation loss 0.055719032883644104 Accuracy 0.4375999867916107\n",
      "Iteration 950 Training loss 0.053401824086904526 Validation loss 0.05436984822154045 Accuracy 0.4528999924659729\n",
      "Iteration 960 Training loss 0.047232452780008316 Validation loss 0.050561629235744476 Accuracy 0.4894999861717224\n",
      "Iteration 970 Training loss 0.04245813190937042 Validation loss 0.04859725758433342 Accuracy 0.5094000101089478\n",
      "Iteration 980 Training loss 0.05029300972819328 Validation loss 0.05022859945893288 Accuracy 0.4934999942779541\n",
      "Iteration 990 Training loss 0.0437273308634758 Validation loss 0.04814736917614937 Accuracy 0.5148000121116638\n",
      "Iteration 1000 Training loss 0.04735935106873512 Validation loss 0.04846764728426933 Accuracy 0.5109999775886536\n",
      "Iteration 1010 Training loss 0.049699656665325165 Validation loss 0.04942692071199417 Accuracy 0.5006999969482422\n",
      "Iteration 1020 Training loss 0.04365497827529907 Validation loss 0.04335737228393555 Accuracy 0.5615000128746033\n",
      "Iteration 1030 Training loss 0.047660768032073975 Validation loss 0.0467548631131649 Accuracy 0.5277000069618225\n",
      "Iteration 1040 Training loss 0.03748362883925438 Validation loss 0.040213461965322495 Accuracy 0.5938000082969666\n",
      "Iteration 1050 Training loss 0.03791515529155731 Validation loss 0.042126912623643875 Accuracy 0.5759000182151794\n",
      "Iteration 1060 Training loss 0.04457941651344299 Validation loss 0.04110337048768997 Accuracy 0.5843999981880188\n",
      "Iteration 1070 Training loss 0.040126923471689224 Validation loss 0.039900440722703934 Accuracy 0.597100019454956\n",
      "Iteration 1080 Training loss 0.038482438772916794 Validation loss 0.04005369171500206 Accuracy 0.5949000120162964\n",
      "Iteration 1090 Training loss 0.03895271196961403 Validation loss 0.0404103547334671 Accuracy 0.5931000113487244\n",
      "Iteration 1100 Training loss 0.04008973762392998 Validation loss 0.039338089525699615 Accuracy 0.6033999919891357\n",
      "Iteration 1110 Training loss 0.03912568837404251 Validation loss 0.0417923629283905 Accuracy 0.5785999894142151\n",
      "Iteration 1120 Training loss 0.03828489035367966 Validation loss 0.039483267813920975 Accuracy 0.6018999814987183\n",
      "Iteration 1130 Training loss 0.03907568380236626 Validation loss 0.03924858570098877 Accuracy 0.6040999889373779\n",
      "Iteration 1140 Training loss 0.03698619082570076 Validation loss 0.03895861655473709 Accuracy 0.6068000197410583\n",
      "Iteration 1150 Training loss 0.039001598954200745 Validation loss 0.03884374350309372 Accuracy 0.6085000038146973\n",
      "Iteration 1160 Training loss 0.03878907114267349 Validation loss 0.03937681019306183 Accuracy 0.6029999852180481\n",
      "Iteration 1170 Training loss 0.03899881988763809 Validation loss 0.038518719375133514 Accuracy 0.61080002784729\n",
      "Iteration 1180 Training loss 0.04014211893081665 Validation loss 0.03927956148982048 Accuracy 0.6039999723434448\n",
      "Iteration 1190 Training loss 0.03715949505567551 Validation loss 0.03903445973992348 Accuracy 0.6057999730110168\n",
      "Iteration 1200 Training loss 0.039362579584121704 Validation loss 0.039178267121315 Accuracy 0.6050000190734863\n",
      "Iteration 1210 Training loss 0.039418015629053116 Validation loss 0.03985567390918732 Accuracy 0.5975000262260437\n",
      "Iteration 1220 Training loss 0.043593790382146835 Validation loss 0.04081646353006363 Accuracy 0.5861999988555908\n",
      "Iteration 1230 Training loss 0.03964228555560112 Validation loss 0.03828778490424156 Accuracy 0.6133000254631042\n",
      "Iteration 1240 Training loss 0.037026647478342056 Validation loss 0.03825288638472557 Accuracy 0.6141999959945679\n",
      "Iteration 1250 Training loss 0.037825148552656174 Validation loss 0.03814985975623131 Accuracy 0.6152999997138977\n",
      "Iteration 1260 Training loss 0.04138600826263428 Validation loss 0.039940014481544495 Accuracy 0.5964000225067139\n",
      "Iteration 1270 Training loss 0.03654451668262482 Validation loss 0.03790513426065445 Accuracy 0.6168000102043152\n",
      "Iteration 1280 Training loss 0.03693106770515442 Validation loss 0.040599554777145386 Accuracy 0.5888000130653381\n",
      "Iteration 1290 Training loss 0.03488998860120773 Validation loss 0.03876227140426636 Accuracy 0.6080999970436096\n",
      "Iteration 1300 Training loss 0.0387791283428669 Validation loss 0.03832033649086952 Accuracy 0.6114000082015991\n",
      "Iteration 1310 Training loss 0.03814997151494026 Validation loss 0.03814429044723511 Accuracy 0.6136000156402588\n",
      "Iteration 1320 Training loss 0.03783995658159256 Validation loss 0.038435474038124084 Accuracy 0.6123999953269958\n",
      "Iteration 1330 Training loss 0.036143090575933456 Validation loss 0.0381900891661644 Accuracy 0.6118000149726868\n",
      "Iteration 1340 Training loss 0.04178126901388168 Validation loss 0.039784785360097885 Accuracy 0.5983999967575073\n",
      "Iteration 1350 Training loss 0.03672897815704346 Validation loss 0.03838752210140228 Accuracy 0.6133000254631042\n",
      "Iteration 1360 Training loss 0.03738032281398773 Validation loss 0.038624510169029236 Accuracy 0.6090999841690063\n",
      "Iteration 1370 Training loss 0.03768761828541756 Validation loss 0.037926752120256424 Accuracy 0.616599977016449\n",
      "Iteration 1380 Training loss 0.03956761583685875 Validation loss 0.038090433925390244 Accuracy 0.6159999966621399\n",
      "Iteration 1390 Training loss 0.038620200008153915 Validation loss 0.038945943117141724 Accuracy 0.6067000031471252\n",
      "Iteration 1400 Training loss 0.035287968814373016 Validation loss 0.03957170993089676 Accuracy 0.6007999777793884\n",
      "Iteration 1410 Training loss 0.03763283044099808 Validation loss 0.0381612703204155 Accuracy 0.6141999959945679\n",
      "Iteration 1420 Training loss 0.04107063636183739 Validation loss 0.040038108825683594 Accuracy 0.5960000157356262\n",
      "Iteration 1430 Training loss 0.03699977695941925 Validation loss 0.03983369469642639 Accuracy 0.5978000164031982\n",
      "Iteration 1440 Training loss 0.036744624376297 Validation loss 0.03856009989976883 Accuracy 0.6115999817848206\n",
      "Iteration 1450 Training loss 0.039314549416303635 Validation loss 0.03846799209713936 Accuracy 0.6122999787330627\n",
      "Iteration 1460 Training loss 0.03623262792825699 Validation loss 0.0384826622903347 Accuracy 0.6108999848365784\n",
      "Iteration 1470 Training loss 0.03570067882537842 Validation loss 0.03813009709119797 Accuracy 0.614300012588501\n",
      "Iteration 1480 Training loss 0.03548728674650192 Validation loss 0.03856294974684715 Accuracy 0.6098999977111816\n",
      "Iteration 1490 Training loss 0.034405454993247986 Validation loss 0.03775069862604141 Accuracy 0.6195999979972839\n",
      "Iteration 1500 Training loss 0.039166953414678574 Validation loss 0.03853204846382141 Accuracy 0.6107000112533569\n",
      "Iteration 1510 Training loss 0.03635570406913757 Validation loss 0.03798266872763634 Accuracy 0.6168000102043152\n",
      "Iteration 1520 Training loss 0.03873612359166145 Validation loss 0.038673035800457 Accuracy 0.6079000234603882\n",
      "Iteration 1530 Training loss 0.03430919721722603 Validation loss 0.03780634328722954 Accuracy 0.6179999709129333\n",
      "Iteration 1540 Training loss 0.03725622221827507 Validation loss 0.038901038467884064 Accuracy 0.6075999736785889\n",
      "Iteration 1550 Training loss 0.03576378524303436 Validation loss 0.03805256634950638 Accuracy 0.6144000291824341\n",
      "Iteration 1560 Training loss 0.03878004848957062 Validation loss 0.03803405910730362 Accuracy 0.6158000230789185\n",
      "Iteration 1570 Training loss 0.04262213036417961 Validation loss 0.03899765759706497 Accuracy 0.6062999963760376\n",
      "Iteration 1580 Training loss 0.038642313331365585 Validation loss 0.03840766102075577 Accuracy 0.6097999811172485\n",
      "Iteration 1590 Training loss 0.033996134996414185 Validation loss 0.03809972479939461 Accuracy 0.6151999831199646\n",
      "Iteration 1600 Training loss 0.03569730743765831 Validation loss 0.03751106560230255 Accuracy 0.621999979019165\n",
      "Iteration 1610 Training loss 0.035116031765937805 Validation loss 0.03769853338599205 Accuracy 0.6207000017166138\n",
      "Iteration 1620 Training loss 0.036405373364686966 Validation loss 0.03763139992952347 Accuracy 0.6176999807357788\n",
      "Iteration 1630 Training loss 0.03839702531695366 Validation loss 0.038158830255270004 Accuracy 0.614300012588501\n",
      "Iteration 1640 Training loss 0.03557923808693886 Validation loss 0.037892263382673264 Accuracy 0.6182000041007996\n",
      "Iteration 1650 Training loss 0.03811754286289215 Validation loss 0.03882276266813278 Accuracy 0.6061000227928162\n",
      "Iteration 1660 Training loss 0.038699306547641754 Validation loss 0.03833985701203346 Accuracy 0.6114000082015991\n",
      "Iteration 1670 Training loss 0.0365707129240036 Validation loss 0.03830738365650177 Accuracy 0.6126000285148621\n",
      "Iteration 1680 Training loss 0.04113435000181198 Validation loss 0.038551826030015945 Accuracy 0.6097999811172485\n",
      "Iteration 1690 Training loss 0.03874676674604416 Validation loss 0.03753785043954849 Accuracy 0.6189000010490417\n",
      "Iteration 1700 Training loss 0.03565256670117378 Validation loss 0.03788885101675987 Accuracy 0.6172000169754028\n",
      "Iteration 1710 Training loss 0.032728176563978195 Validation loss 0.03806348145008087 Accuracy 0.6148999929428101\n",
      "Iteration 1720 Training loss 0.03494543209671974 Validation loss 0.03815740719437599 Accuracy 0.6122999787330627\n",
      "Iteration 1730 Training loss 0.033965881913900375 Validation loss 0.036975543946027756 Accuracy 0.6258999705314636\n",
      "Iteration 1740 Training loss 0.038239941000938416 Validation loss 0.038408152759075165 Accuracy 0.6126999855041504\n",
      "Iteration 1750 Training loss 0.0374576710164547 Validation loss 0.03778310492634773 Accuracy 0.6180999875068665\n",
      "Iteration 1760 Training loss 0.036867640912532806 Validation loss 0.03747956454753876 Accuracy 0.6202999949455261\n",
      "Iteration 1770 Training loss 0.037764303386211395 Validation loss 0.03976423293352127 Accuracy 0.597599983215332\n",
      "Iteration 1780 Training loss 0.03768447786569595 Validation loss 0.03814273327589035 Accuracy 0.6151000261306763\n",
      "Iteration 1790 Training loss 0.0377134308218956 Validation loss 0.03724617511034012 Accuracy 0.6212999820709229\n",
      "Iteration 1800 Training loss 0.03737093135714531 Validation loss 0.03708038106560707 Accuracy 0.6258999705314636\n",
      "Iteration 1810 Training loss 0.03761742636561394 Validation loss 0.03680833801627159 Accuracy 0.6270999908447266\n",
      "Iteration 1820 Training loss 0.036099012941122055 Validation loss 0.037527889013290405 Accuracy 0.6190000176429749\n",
      "Iteration 1830 Training loss 0.03725476190447807 Validation loss 0.03745092824101448 Accuracy 0.621399998664856\n",
      "Iteration 1840 Training loss 0.03919943422079086 Validation loss 0.03756606578826904 Accuracy 0.6202999949455261\n",
      "Iteration 1850 Training loss 0.03336567059159279 Validation loss 0.037688277661800385 Accuracy 0.619700014591217\n",
      "Iteration 1860 Training loss 0.035712797194719315 Validation loss 0.03794318810105324 Accuracy 0.6152999997138977\n",
      "Iteration 1870 Training loss 0.03638235852122307 Validation loss 0.03827429562807083 Accuracy 0.6123999953269958\n",
      "Iteration 1880 Training loss 0.04137207940220833 Validation loss 0.03865771368145943 Accuracy 0.6068000197410583\n",
      "Iteration 1890 Training loss 0.04321525618433952 Validation loss 0.042865555733442307 Accuracy 0.564300000667572\n",
      "Iteration 1900 Training loss 0.037795890122652054 Validation loss 0.03774413466453552 Accuracy 0.6175000071525574\n",
      "Iteration 1910 Training loss 0.03809620067477226 Validation loss 0.03746361285448074 Accuracy 0.6189000010490417\n",
      "Iteration 1920 Training loss 0.030633870512247086 Validation loss 0.036267269402742386 Accuracy 0.6306999921798706\n",
      "Iteration 1930 Training loss 0.031069718301296234 Validation loss 0.03606326878070831 Accuracy 0.6324999928474426\n",
      "Iteration 1940 Training loss 0.03767533227801323 Validation loss 0.03737727925181389 Accuracy 0.6219000220298767\n",
      "Iteration 1950 Training loss 0.032666128128767014 Validation loss 0.03690065070986748 Accuracy 0.6251999735832214\n",
      "Iteration 1960 Training loss 0.034335702657699585 Validation loss 0.03555388003587723 Accuracy 0.6381000280380249\n",
      "Iteration 1970 Training loss 0.03663258999586105 Validation loss 0.03782546892762184 Accuracy 0.6164000034332275\n",
      "Iteration 1980 Training loss 0.03299301862716675 Validation loss 0.0357629656791687 Accuracy 0.6381999850273132\n",
      "Iteration 1990 Training loss 0.03297366574406624 Validation loss 0.03618043288588524 Accuracy 0.635200023651123\n",
      "Iteration 2000 Training loss 0.03691480681300163 Validation loss 0.03730277344584465 Accuracy 0.6230999827384949\n",
      "Iteration 2010 Training loss 0.038131870329380035 Validation loss 0.03739941120147705 Accuracy 0.6218000054359436\n",
      "Iteration 2020 Training loss 0.03401570022106171 Validation loss 0.03622756525874138 Accuracy 0.6342999935150146\n",
      "Iteration 2030 Training loss 0.035106055438518524 Validation loss 0.03623829409480095 Accuracy 0.6308000087738037\n",
      "Iteration 2040 Training loss 0.03448371961712837 Validation loss 0.03537999093532562 Accuracy 0.6413999795913696\n",
      "Iteration 2050 Training loss 0.03675117343664169 Validation loss 0.040032513439655304 Accuracy 0.5939000248908997\n",
      "Iteration 2060 Training loss 0.03790201246738434 Validation loss 0.0392526350915432 Accuracy 0.6019999980926514\n",
      "Iteration 2070 Training loss 0.034998271614313126 Validation loss 0.03576108068227768 Accuracy 0.6381000280380249\n",
      "Iteration 2080 Training loss 0.03234816715121269 Validation loss 0.036432940512895584 Accuracy 0.628600001335144\n",
      "Iteration 2090 Training loss 0.03510972484946251 Validation loss 0.03817106410861015 Accuracy 0.6103000044822693\n",
      "Iteration 2100 Training loss 0.03306474909186363 Validation loss 0.03482479229569435 Accuracy 0.6473000049591064\n",
      "Iteration 2110 Training loss 0.03514080494642258 Validation loss 0.03767567500472069 Accuracy 0.616599977016449\n",
      "Iteration 2120 Training loss 0.033151961863040924 Validation loss 0.03478269651532173 Accuracy 0.6491000056266785\n",
      "Iteration 2130 Training loss 0.03466613590717316 Validation loss 0.0365179181098938 Accuracy 0.628000020980835\n",
      "Iteration 2140 Training loss 0.03341040015220642 Validation loss 0.035270724445581436 Accuracy 0.640999972820282\n",
      "Iteration 2150 Training loss 0.03659442067146301 Validation loss 0.03643450140953064 Accuracy 0.6290000081062317\n",
      "Iteration 2160 Training loss 0.03404289484024048 Validation loss 0.034245267510414124 Accuracy 0.650600016117096\n",
      "Iteration 2170 Training loss 0.03477896749973297 Validation loss 0.033970605581998825 Accuracy 0.6536999940872192\n",
      "Iteration 2180 Training loss 0.03226075693964958 Validation loss 0.034611690789461136 Accuracy 0.6482999920845032\n",
      "Iteration 2190 Training loss 0.03233135864138603 Validation loss 0.03425370901823044 Accuracy 0.6506999731063843\n",
      "Iteration 2200 Training loss 0.034699492156505585 Validation loss 0.034847915172576904 Accuracy 0.6456999778747559\n",
      "Iteration 2210 Training loss 0.035120878368616104 Validation loss 0.03757170960307121 Accuracy 0.6190000176429749\n",
      "Iteration 2220 Training loss 0.036176566034555435 Validation loss 0.034576382488012314 Accuracy 0.6481999754905701\n",
      "Iteration 2230 Training loss 0.03820928558707237 Validation loss 0.036375872790813446 Accuracy 0.6302000284194946\n",
      "Iteration 2240 Training loss 0.032483670860528946 Validation loss 0.03409221023321152 Accuracy 0.6539999842643738\n",
      "Iteration 2250 Training loss 0.03360584378242493 Validation loss 0.035280175507068634 Accuracy 0.6434999704360962\n",
      "Iteration 2260 Training loss 0.037789423018693924 Validation loss 0.03507789224386215 Accuracy 0.6431000232696533\n",
      "Iteration 2270 Training loss 0.03253203257918358 Validation loss 0.03588584437966347 Accuracy 0.6352999806404114\n",
      "Iteration 2280 Training loss 0.03562505915760994 Validation loss 0.03498564288020134 Accuracy 0.6438999772071838\n",
      "Iteration 2290 Training loss 0.032559290528297424 Validation loss 0.0342748686671257 Accuracy 0.652899980545044\n",
      "Iteration 2300 Training loss 0.03851640969514847 Validation loss 0.03701839968562126 Accuracy 0.6243000030517578\n",
      "Iteration 2310 Training loss 0.034400857985019684 Validation loss 0.0346391424536705 Accuracy 0.6471999883651733\n",
      "Iteration 2320 Training loss 0.03304354473948479 Validation loss 0.03504591062664986 Accuracy 0.6444000005722046\n",
      "Iteration 2330 Training loss 0.03371981903910637 Validation loss 0.03419901058077812 Accuracy 0.6541000008583069\n",
      "Iteration 2340 Training loss 0.03413097560405731 Validation loss 0.03615278750658035 Accuracy 0.6330000162124634\n",
      "Iteration 2350 Training loss 0.033024031668901443 Validation loss 0.0337650440633297 Accuracy 0.6577000021934509\n",
      "Iteration 2360 Training loss 0.030873902142047882 Validation loss 0.035294871777296066 Accuracy 0.6406000256538391\n",
      "Iteration 2370 Training loss 0.03223776817321777 Validation loss 0.034503430128097534 Accuracy 0.6496000289916992\n",
      "Iteration 2380 Training loss 0.03717835992574692 Validation loss 0.03497076779603958 Accuracy 0.6449999809265137\n",
      "Iteration 2390 Training loss 0.03375751152634621 Validation loss 0.03523452952504158 Accuracy 0.642799973487854\n",
      "Iteration 2400 Training loss 0.0353587381541729 Validation loss 0.035401228815317154 Accuracy 0.640500009059906\n",
      "Iteration 2410 Training loss 0.0317780040204525 Validation loss 0.03464343771338463 Accuracy 0.6486999988555908\n",
      "Iteration 2420 Training loss 0.0312538743019104 Validation loss 0.03598187118768692 Accuracy 0.6352999806404114\n",
      "Iteration 2430 Training loss 0.03252619877457619 Validation loss 0.0344449058175087 Accuracy 0.6502000093460083\n",
      "Iteration 2440 Training loss 0.03204746171832085 Validation loss 0.035018764436244965 Accuracy 0.6460999846458435\n",
      "Iteration 2450 Training loss 0.03417662903666496 Validation loss 0.03564451262354851 Accuracy 0.6395999789237976\n",
      "Iteration 2460 Training loss 0.03461872786283493 Validation loss 0.034693643450737 Accuracy 0.6478000283241272\n",
      "Iteration 2470 Training loss 0.030387097969651222 Validation loss 0.03461287170648575 Accuracy 0.6503000259399414\n",
      "Iteration 2480 Training loss 0.035876788198947906 Validation loss 0.03381488844752312 Accuracy 0.6577000021934509\n",
      "Iteration 2490 Training loss 0.03330644220113754 Validation loss 0.03631872683763504 Accuracy 0.6306999921798706\n",
      "Iteration 2500 Training loss 0.030037349089980125 Validation loss 0.03420920670032501 Accuracy 0.654699981212616\n",
      "Iteration 2510 Training loss 0.03130088746547699 Validation loss 0.03476749360561371 Accuracy 0.6486999988555908\n",
      "Iteration 2520 Training loss 0.03312404081225395 Validation loss 0.03461270406842232 Accuracy 0.6470999717712402\n",
      "Iteration 2530 Training loss 0.03500482067465782 Validation loss 0.03428453207015991 Accuracy 0.6520000100135803\n",
      "Iteration 2540 Training loss 0.032260674983263016 Validation loss 0.033759262412786484 Accuracy 0.656000018119812\n",
      "Iteration 2550 Training loss 0.030677927657961845 Validation loss 0.033912159502506256 Accuracy 0.656000018119812\n",
      "Iteration 2560 Training loss 0.034378666430711746 Validation loss 0.03419901058077812 Accuracy 0.6514999866485596\n",
      "Iteration 2570 Training loss 0.0329536534845829 Validation loss 0.03456593677401543 Accuracy 0.6499000191688538\n",
      "Iteration 2580 Training loss 0.03486586734652519 Validation loss 0.034750018268823624 Accuracy 0.6488999724388123\n",
      "Iteration 2590 Training loss 0.03602185472846031 Validation loss 0.035883016884326935 Accuracy 0.635200023651123\n",
      "Iteration 2600 Training loss 0.03166915848851204 Validation loss 0.03340807184576988 Accuracy 0.659500002861023\n",
      "Iteration 2610 Training loss 0.03353406861424446 Validation loss 0.03364779055118561 Accuracy 0.6572999954223633\n",
      "Iteration 2620 Training loss 0.034395731985569 Validation loss 0.034858062863349915 Accuracy 0.6467999815940857\n",
      "Iteration 2630 Training loss 0.034290336072444916 Validation loss 0.035114169120788574 Accuracy 0.6444000005722046\n",
      "Iteration 2640 Training loss 0.0328734815120697 Validation loss 0.034252848476171494 Accuracy 0.6531999707221985\n",
      "Iteration 2650 Training loss 0.03608075901865959 Validation loss 0.03338942304253578 Accuracy 0.6610999703407288\n",
      "Iteration 2660 Training loss 0.03468625247478485 Validation loss 0.03387628495693207 Accuracy 0.656499981880188\n",
      "Iteration 2670 Training loss 0.036518484354019165 Validation loss 0.03398577868938446 Accuracy 0.6560999751091003\n",
      "Iteration 2680 Training loss 0.03571329638361931 Validation loss 0.03385749086737633 Accuracy 0.656499981880188\n",
      "Iteration 2690 Training loss 0.03343730792403221 Validation loss 0.03381457179784775 Accuracy 0.6571999788284302\n",
      "Iteration 2700 Training loss 0.03385614603757858 Validation loss 0.034334830939769745 Accuracy 0.652999997138977\n",
      "Iteration 2710 Training loss 0.033192042261362076 Validation loss 0.033793479204177856 Accuracy 0.6575000286102295\n",
      "Iteration 2720 Training loss 0.032127294689416885 Validation loss 0.03330600634217262 Accuracy 0.6625000238418579\n",
      "Iteration 2730 Training loss 0.030707325786352158 Validation loss 0.03428002819418907 Accuracy 0.6513000130653381\n",
      "Iteration 2740 Training loss 0.029934925958514214 Validation loss 0.033683814108371735 Accuracy 0.6571000218391418\n",
      "Iteration 2750 Training loss 0.03244178369641304 Validation loss 0.033778294920921326 Accuracy 0.6568999886512756\n",
      "Iteration 2760 Training loss 0.03112909197807312 Validation loss 0.03363728150725365 Accuracy 0.6586999893188477\n",
      "Iteration 2770 Training loss 0.03275024890899658 Validation loss 0.03382282331585884 Accuracy 0.6553999781608582\n",
      "Iteration 2780 Training loss 0.032453812658786774 Validation loss 0.03440668433904648 Accuracy 0.6514000296592712\n",
      "Iteration 2790 Training loss 0.03351696953177452 Validation loss 0.03346065804362297 Accuracy 0.6597999930381775\n",
      "Iteration 2800 Training loss 0.03007281757891178 Validation loss 0.03356380760669708 Accuracy 0.6581000089645386\n",
      "Iteration 2810 Training loss 0.035375118255615234 Validation loss 0.03466999903321266 Accuracy 0.6478999853134155\n",
      "Iteration 2820 Training loss 0.034106794744729996 Validation loss 0.03421209380030632 Accuracy 0.6539999842643738\n",
      "Iteration 2830 Training loss 0.031028473749756813 Validation loss 0.034681618213653564 Accuracy 0.6481999754905701\n",
      "Iteration 2840 Training loss 0.03193426877260208 Validation loss 0.033727679401636124 Accuracy 0.6589000225067139\n",
      "Iteration 2850 Training loss 0.036086853593587875 Validation loss 0.03433641046285629 Accuracy 0.650600016117096\n",
      "Iteration 2860 Training loss 0.034879423677921295 Validation loss 0.03386778011918068 Accuracy 0.6578999757766724\n",
      "Iteration 2870 Training loss 0.03200209140777588 Validation loss 0.034007687121629715 Accuracy 0.6541000008583069\n",
      "Iteration 2880 Training loss 0.028790928423404694 Validation loss 0.033870596438646317 Accuracy 0.654699981212616\n",
      "Iteration 2890 Training loss 0.034469686448574066 Validation loss 0.03519783541560173 Accuracy 0.6428999900817871\n",
      "Iteration 2900 Training loss 0.034447405487298965 Validation loss 0.03440682217478752 Accuracy 0.6517000198364258\n",
      "Iteration 2910 Training loss 0.0298616886138916 Validation loss 0.03310185298323631 Accuracy 0.6625000238418579\n",
      "Iteration 2920 Training loss 0.03626455366611481 Validation loss 0.03716070577502251 Accuracy 0.6212000250816345\n",
      "Iteration 2930 Training loss 0.03800889477133751 Validation loss 0.0361313670873642 Accuracy 0.6330999732017517\n",
      "Iteration 2940 Training loss 0.030169829726219177 Validation loss 0.03429066389799118 Accuracy 0.6523000001907349\n",
      "Iteration 2950 Training loss 0.03257159888744354 Validation loss 0.034890104085206985 Accuracy 0.6471999883651733\n",
      "Iteration 2960 Training loss 0.030289391055703163 Validation loss 0.033094555139541626 Accuracy 0.6642000079154968\n",
      "Iteration 2970 Training loss 0.029121404513716698 Validation loss 0.034438055008649826 Accuracy 0.6496000289916992\n",
      "Iteration 2980 Training loss 0.03416172042489052 Validation loss 0.037450432777404785 Accuracy 0.6194000244140625\n",
      "Iteration 2990 Training loss 0.03014414943754673 Validation loss 0.03310782089829445 Accuracy 0.6629999876022339\n",
      "Iteration 3000 Training loss 0.03173695504665375 Validation loss 0.03316820040345192 Accuracy 0.6625000238418579\n",
      "Iteration 3010 Training loss 0.03744357451796532 Validation loss 0.034010693430900574 Accuracy 0.6534000039100647\n",
      "Iteration 3020 Training loss 0.03053230606019497 Validation loss 0.03430953249335289 Accuracy 0.6504999995231628\n",
      "Iteration 3030 Training loss 0.031555186957120895 Validation loss 0.03412092104554176 Accuracy 0.6527000069618225\n",
      "Iteration 3040 Training loss 0.03140537068247795 Validation loss 0.033881790935993195 Accuracy 0.6557000279426575\n",
      "Iteration 3050 Training loss 0.03339293971657753 Validation loss 0.03393103927373886 Accuracy 0.6561999917030334\n",
      "Iteration 3060 Training loss 0.02904982678592205 Validation loss 0.03322151303291321 Accuracy 0.6621999740600586\n",
      "Iteration 3070 Training loss 0.0334617905318737 Validation loss 0.034350648522377014 Accuracy 0.649399995803833\n",
      "Iteration 3080 Training loss 0.03298573940992355 Validation loss 0.03360145911574364 Accuracy 0.657800018787384\n",
      "Iteration 3090 Training loss 0.03376547992229462 Validation loss 0.03388318046927452 Accuracy 0.6543999910354614\n",
      "Iteration 3100 Training loss 0.02899751625955105 Validation loss 0.03320593759417534 Accuracy 0.6610999703407288\n",
      "Iteration 3110 Training loss 0.034155238419771194 Validation loss 0.03440803661942482 Accuracy 0.650600016117096\n",
      "Iteration 3120 Training loss 0.03330173343420029 Validation loss 0.033099059015512466 Accuracy 0.6621999740600586\n",
      "Iteration 3130 Training loss 0.02742953412234783 Validation loss 0.033189136534929276 Accuracy 0.6620000004768372\n",
      "Iteration 3140 Training loss 0.032732829451560974 Validation loss 0.0335039384663105 Accuracy 0.6575000286102295\n",
      "Iteration 3150 Training loss 0.030247798189520836 Validation loss 0.03460566699504852 Accuracy 0.6470999717712402\n",
      "Iteration 3160 Training loss 0.03377493470907211 Validation loss 0.033845145255327225 Accuracy 0.6565999984741211\n",
      "Iteration 3170 Training loss 0.03500233218073845 Validation loss 0.0333443321287632 Accuracy 0.6615999937057495\n",
      "Iteration 3180 Training loss 0.033368419855833054 Validation loss 0.034729547798633575 Accuracy 0.6477000117301941\n",
      "Iteration 3190 Training loss 0.029891517013311386 Validation loss 0.03428034484386444 Accuracy 0.6520000100135803\n",
      "Iteration 3200 Training loss 0.03164265304803848 Validation loss 0.03325558453798294 Accuracy 0.6624000072479248\n",
      "Iteration 3210 Training loss 0.02977088838815689 Validation loss 0.03360084444284439 Accuracy 0.6586999893188477\n",
      "Iteration 3220 Training loss 0.027662895619869232 Validation loss 0.033438704907894135 Accuracy 0.6614000201225281\n",
      "Iteration 3230 Training loss 0.03166358545422554 Validation loss 0.033973414450883865 Accuracy 0.6547999978065491\n",
      "Iteration 3240 Training loss 0.03253956139087677 Validation loss 0.033625081181526184 Accuracy 0.6593999862670898\n",
      "Iteration 3250 Training loss 0.03643398731946945 Validation loss 0.03393026813864708 Accuracy 0.6557999849319458\n",
      "Iteration 3260 Training loss 0.03439127653837204 Validation loss 0.03327546641230583 Accuracy 0.6625999808311462\n",
      "Iteration 3270 Training loss 0.031302519142627716 Validation loss 0.034859851002693176 Accuracy 0.6464999914169312\n",
      "Iteration 3280 Training loss 0.02925886958837509 Validation loss 0.03300017490983009 Accuracy 0.6647999882698059\n",
      "Iteration 3290 Training loss 0.028763368725776672 Validation loss 0.032860614359378815 Accuracy 0.6654999852180481\n",
      "Iteration 3300 Training loss 0.03171403333544731 Validation loss 0.0334223210811615 Accuracy 0.6593000292778015\n",
      "Iteration 3310 Training loss 0.02884264849126339 Validation loss 0.034516576677560806 Accuracy 0.6492000222206116\n",
      "Iteration 3320 Training loss 0.036554936319589615 Validation loss 0.03372509405016899 Accuracy 0.6593000292778015\n",
      "Iteration 3330 Training loss 0.03288799896836281 Validation loss 0.03323209658265114 Accuracy 0.6621000170707703\n",
      "Iteration 3340 Training loss 0.028706248849630356 Validation loss 0.03369705006480217 Accuracy 0.6571000218391418\n",
      "Iteration 3350 Training loss 0.03443584591150284 Validation loss 0.03343338891863823 Accuracy 0.6603000164031982\n",
      "Iteration 3360 Training loss 0.03498612344264984 Validation loss 0.03411474451422691 Accuracy 0.6538000106811523\n",
      "Iteration 3370 Training loss 0.036335986107587814 Validation loss 0.0350312776863575 Accuracy 0.6442000269889832\n",
      "Iteration 3380 Training loss 0.030731283128261566 Validation loss 0.032806769013404846 Accuracy 0.6664000153541565\n",
      "Iteration 3390 Training loss 0.03054600954055786 Validation loss 0.03301066905260086 Accuracy 0.6638000011444092\n",
      "Iteration 3400 Training loss 0.03325518220663071 Validation loss 0.034274276345968246 Accuracy 0.6514000296592712\n",
      "Iteration 3410 Training loss 0.03365283086895943 Validation loss 0.03360014036297798 Accuracy 0.6570000052452087\n",
      "Iteration 3420 Training loss 0.029970720410346985 Validation loss 0.03294597566127777 Accuracy 0.6656000018119812\n",
      "Iteration 3430 Training loss 0.0306754931807518 Validation loss 0.0329524427652359 Accuracy 0.6658999919891357\n",
      "Iteration 3440 Training loss 0.03520667925477028 Validation loss 0.0342874750494957 Accuracy 0.652400016784668\n",
      "Iteration 3450 Training loss 0.03250692039728165 Validation loss 0.03391338512301445 Accuracy 0.6535000205039978\n",
      "Iteration 3460 Training loss 0.035542529076337814 Validation loss 0.03636142238974571 Accuracy 0.6283000111579895\n",
      "Iteration 3470 Training loss 0.03374996781349182 Validation loss 0.0338546484708786 Accuracy 0.6550999879837036\n",
      "Iteration 3480 Training loss 0.031081024557352066 Validation loss 0.03614065796136856 Accuracy 0.6331999897956848\n",
      "Iteration 3490 Training loss 0.02959943562746048 Validation loss 0.03274383395910263 Accuracy 0.666700005531311\n",
      "Iteration 3500 Training loss 0.03437294065952301 Validation loss 0.03327767178416252 Accuracy 0.6610999703407288\n",
      "Iteration 3510 Training loss 0.03355557844042778 Validation loss 0.033914584666490555 Accuracy 0.6547999978065491\n",
      "Iteration 3520 Training loss 0.03041771613061428 Validation loss 0.03335460275411606 Accuracy 0.6625000238418579\n",
      "Iteration 3530 Training loss 0.03397933766245842 Validation loss 0.03471524268388748 Accuracy 0.6460999846458435\n",
      "Iteration 3540 Training loss 0.033516258001327515 Validation loss 0.03474759683012962 Accuracy 0.6464999914169312\n",
      "Iteration 3550 Training loss 0.031263746321201324 Validation loss 0.03298736363649368 Accuracy 0.6636999845504761\n",
      "Iteration 3560 Training loss 0.02711976319551468 Validation loss 0.033452313393354416 Accuracy 0.6606000065803528\n",
      "Iteration 3570 Training loss 0.029062770307064056 Validation loss 0.03337361663579941 Accuracy 0.660099983215332\n",
      "Iteration 3580 Training loss 0.03310104086995125 Validation loss 0.033092137426137924 Accuracy 0.6618000268936157\n",
      "Iteration 3590 Training loss 0.02646726742386818 Validation loss 0.03290680795907974 Accuracy 0.6636999845504761\n",
      "Iteration 3600 Training loss 0.03418073430657387 Validation loss 0.033412206918001175 Accuracy 0.6593999862670898\n",
      "Iteration 3610 Training loss 0.03486567363142967 Validation loss 0.03274355083703995 Accuracy 0.6668999791145325\n",
      "Iteration 3620 Training loss 0.0307996217161417 Validation loss 0.033866800367832184 Accuracy 0.6550999879837036\n",
      "Iteration 3630 Training loss 0.034011732786893845 Validation loss 0.03274181857705116 Accuracy 0.664900004863739\n",
      "Iteration 3640 Training loss 0.03409559652209282 Validation loss 0.033867791295051575 Accuracy 0.6538000106811523\n",
      "Iteration 3650 Training loss 0.026254549622535706 Validation loss 0.032972920686006546 Accuracy 0.6629999876022339\n",
      "Iteration 3660 Training loss 0.028698384761810303 Validation loss 0.03371134400367737 Accuracy 0.659600019454956\n",
      "Iteration 3670 Training loss 0.034565847367048264 Validation loss 0.03544145077466965 Accuracy 0.6419000029563904\n",
      "Iteration 3680 Training loss 0.03308587148785591 Validation loss 0.03355751931667328 Accuracy 0.6579999923706055\n",
      "Iteration 3690 Training loss 0.029604095965623856 Validation loss 0.033002085983753204 Accuracy 0.6629999876022339\n",
      "Iteration 3700 Training loss 0.033115778118371964 Validation loss 0.03466397896409035 Accuracy 0.6442000269889832\n",
      "Iteration 3710 Training loss 0.02731035090982914 Validation loss 0.0328441821038723 Accuracy 0.6660000085830688\n",
      "Iteration 3720 Training loss 0.03549390658736229 Validation loss 0.03385663032531738 Accuracy 0.6539999842643738\n",
      "Iteration 3730 Training loss 0.027654169127345085 Validation loss 0.032995421439409256 Accuracy 0.6621000170707703\n",
      "Iteration 3740 Training loss 0.03354613110423088 Validation loss 0.03327333927154541 Accuracy 0.6625999808311462\n",
      "Iteration 3750 Training loss 0.03029489889740944 Validation loss 0.032926060259342194 Accuracy 0.6650999784469604\n",
      "Iteration 3760 Training loss 0.0324067622423172 Validation loss 0.03345750644803047 Accuracy 0.6593999862670898\n",
      "Iteration 3770 Training loss 0.0320400707423687 Validation loss 0.032818298786878586 Accuracy 0.664900004863739\n",
      "Iteration 3780 Training loss 0.03062771074473858 Validation loss 0.03330473601818085 Accuracy 0.6596999764442444\n",
      "Iteration 3790 Training loss 0.03136301785707474 Validation loss 0.03334695100784302 Accuracy 0.6593999862670898\n",
      "Iteration 3800 Training loss 0.0304550901055336 Validation loss 0.033128246665000916 Accuracy 0.6620000004768372\n",
      "Iteration 3810 Training loss 0.032017000019550323 Validation loss 0.03355192020535469 Accuracy 0.6578999757766724\n",
      "Iteration 3820 Training loss 0.03376910835504532 Validation loss 0.03254926577210426 Accuracy 0.6678000092506409\n",
      "Iteration 3830 Training loss 0.03135095164179802 Validation loss 0.033063407987356186 Accuracy 0.6629999876022339\n",
      "Iteration 3840 Training loss 0.029587583616375923 Validation loss 0.033071380108594894 Accuracy 0.6632999777793884\n",
      "Iteration 3850 Training loss 0.030580533668398857 Validation loss 0.03270578756928444 Accuracy 0.6662999987602234\n",
      "Iteration 3860 Training loss 0.0336417481303215 Validation loss 0.03419851139187813 Accuracy 0.6500999927520752\n",
      "Iteration 3870 Training loss 0.030845947563648224 Validation loss 0.032475244253873825 Accuracy 0.6682999730110168\n",
      "Iteration 3880 Training loss 0.029431529343128204 Validation loss 0.03323989361524582 Accuracy 0.6608999967575073\n",
      "Iteration 3890 Training loss 0.033595480024814606 Validation loss 0.03300695866346359 Accuracy 0.6615999937057495\n",
      "Iteration 3900 Training loss 0.029741737991571426 Validation loss 0.03294841945171356 Accuracy 0.6626999974250793\n",
      "Iteration 3910 Training loss 0.03230386599898338 Validation loss 0.034863799810409546 Accuracy 0.6434000134468079\n",
      "Iteration 3920 Training loss 0.03528624773025513 Validation loss 0.03280111402273178 Accuracy 0.6661999821662903\n",
      "Iteration 3930 Training loss 0.029605774208903313 Validation loss 0.03252416104078293 Accuracy 0.6690999865531921\n",
      "Iteration 3940 Training loss 0.03006424941122532 Validation loss 0.032245904207229614 Accuracy 0.6697999835014343\n",
      "Iteration 3950 Training loss 0.0328267402946949 Validation loss 0.03245049715042114 Accuracy 0.6682000160217285\n",
      "Iteration 3960 Training loss 0.03401533514261246 Validation loss 0.03271006420254707 Accuracy 0.664900004863739\n",
      "Iteration 3970 Training loss 0.030965210869908333 Validation loss 0.03307172656059265 Accuracy 0.6618000268936157\n",
      "Iteration 3980 Training loss 0.029366889968514442 Validation loss 0.03234308585524559 Accuracy 0.670799970626831\n",
      "Iteration 3990 Training loss 0.029870549216866493 Validation loss 0.03238893672823906 Accuracy 0.6679999828338623\n",
      "Iteration 4000 Training loss 0.030113261193037033 Validation loss 0.032749779522418976 Accuracy 0.6653000116348267\n",
      "Iteration 4010 Training loss 0.03142726048827171 Validation loss 0.03290565311908722 Accuracy 0.6638000011444092\n",
      "Iteration 4020 Training loss 0.03079281374812126 Validation loss 0.03323313593864441 Accuracy 0.661899983882904\n",
      "Iteration 4030 Training loss 0.030677983537316322 Validation loss 0.03240496665239334 Accuracy 0.6686000227928162\n",
      "Iteration 4040 Training loss 0.031425487250089645 Validation loss 0.033350322395563126 Accuracy 0.6577000021934509\n",
      "Iteration 4050 Training loss 0.03223222494125366 Validation loss 0.033147137612104416 Accuracy 0.659500002861023\n",
      "Iteration 4060 Training loss 0.030379725620150566 Validation loss 0.03322429582476616 Accuracy 0.6621999740600586\n",
      "Iteration 4070 Training loss 0.0316939651966095 Validation loss 0.03290987387299538 Accuracy 0.666100025177002\n",
      "Iteration 4080 Training loss 0.029583299532532692 Validation loss 0.03248009830713272 Accuracy 0.6672000288963318\n",
      "Iteration 4090 Training loss 0.031803566962480545 Validation loss 0.03300836309790611 Accuracy 0.6621999740600586\n",
      "Iteration 4100 Training loss 0.0304326880723238 Validation loss 0.032653480768203735 Accuracy 0.6657999753952026\n",
      "Iteration 4110 Training loss 0.02735518477857113 Validation loss 0.03290078043937683 Accuracy 0.6628000140190125\n",
      "Iteration 4120 Training loss 0.02894848957657814 Validation loss 0.03252952918410301 Accuracy 0.6660000085830688\n",
      "Iteration 4130 Training loss 0.026752296835184097 Validation loss 0.03328247740864754 Accuracy 0.6590999960899353\n",
      "Iteration 4140 Training loss 0.031551070511341095 Validation loss 0.032477132976055145 Accuracy 0.6679999828338623\n",
      "Iteration 4150 Training loss 0.03467142581939697 Validation loss 0.03281822055578232 Accuracy 0.664900004863739\n",
      "Iteration 4160 Training loss 0.030144240707159042 Validation loss 0.03319818526506424 Accuracy 0.6610000133514404\n",
      "Iteration 4170 Training loss 0.027881456539034843 Validation loss 0.03313026949763298 Accuracy 0.6621000170707703\n",
      "Iteration 4180 Training loss 0.029744567349553108 Validation loss 0.032572224736213684 Accuracy 0.6662999987602234\n",
      "Iteration 4190 Training loss 0.03167486935853958 Validation loss 0.03307025507092476 Accuracy 0.6622999906539917\n",
      "Iteration 4200 Training loss 0.029462529346346855 Validation loss 0.0327053926885128 Accuracy 0.664900004863739\n",
      "Iteration 4210 Training loss 0.03205231949687004 Validation loss 0.032497238367795944 Accuracy 0.6675000190734863\n",
      "Iteration 4220 Training loss 0.029351163655519485 Validation loss 0.03277282789349556 Accuracy 0.6625000238418579\n",
      "Iteration 4230 Training loss 0.030612219125032425 Validation loss 0.03240146115422249 Accuracy 0.66839998960495\n",
      "Iteration 4240 Training loss 0.03180059790611267 Validation loss 0.032552506774663925 Accuracy 0.6661999821662903\n",
      "Iteration 4250 Training loss 0.030560098588466644 Validation loss 0.03286806493997574 Accuracy 0.6646000146865845\n",
      "Iteration 4260 Training loss 0.03002576157450676 Validation loss 0.03287821635603905 Accuracy 0.6647999882698059\n",
      "Iteration 4270 Training loss 0.03046506457030773 Validation loss 0.033286355435848236 Accuracy 0.6581000089645386\n",
      "Iteration 4280 Training loss 0.029353739693760872 Validation loss 0.032115887850522995 Accuracy 0.6715999841690063\n",
      "Iteration 4290 Training loss 0.03188689798116684 Validation loss 0.03247235342860222 Accuracy 0.6694999933242798\n",
      "Iteration 4300 Training loss 0.030682001262903214 Validation loss 0.03303007781505585 Accuracy 0.6607999801635742\n",
      "Iteration 4310 Training loss 0.032341521233320236 Validation loss 0.03271753340959549 Accuracy 0.6635000109672546\n",
      "Iteration 4320 Training loss 0.03703669086098671 Validation loss 0.03583827242255211 Accuracy 0.6333000063896179\n",
      "Iteration 4330 Training loss 0.03359374776482582 Validation loss 0.032190192490816116 Accuracy 0.6690999865531921\n",
      "Iteration 4340 Training loss 0.02789015881717205 Validation loss 0.032554805278778076 Accuracy 0.6650999784469604\n",
      "Iteration 4350 Training loss 0.02798883244395256 Validation loss 0.032148152589797974 Accuracy 0.6690999865531921\n",
      "Iteration 4360 Training loss 0.027645042166113853 Validation loss 0.03226585313677788 Accuracy 0.6674000024795532\n",
      "Iteration 4370 Training loss 0.028817595914006233 Validation loss 0.032992225140333176 Accuracy 0.6621000170707703\n",
      "Iteration 4380 Training loss 0.03242295980453491 Validation loss 0.03227175399661064 Accuracy 0.6697999835014343\n",
      "Iteration 4390 Training loss 0.03037336654961109 Validation loss 0.03286750614643097 Accuracy 0.6624000072479248\n",
      "Iteration 4400 Training loss 0.03168975189328194 Validation loss 0.03180701285600662 Accuracy 0.6718000173568726\n",
      "Iteration 4410 Training loss 0.0318760871887207 Validation loss 0.03308175131678581 Accuracy 0.661899983882904\n",
      "Iteration 4420 Training loss 0.033108219504356384 Validation loss 0.03337927535176277 Accuracy 0.657800018787384\n",
      "Iteration 4430 Training loss 0.030356135219335556 Validation loss 0.032885029911994934 Accuracy 0.6610000133514404\n",
      "Iteration 4440 Training loss 0.030911780893802643 Validation loss 0.032185014337301254 Accuracy 0.6690999865531921\n",
      "Iteration 4450 Training loss 0.030435701832175255 Validation loss 0.03247945010662079 Accuracy 0.6651999950408936\n",
      "Iteration 4460 Training loss 0.031119249761104584 Validation loss 0.03215896338224411 Accuracy 0.6690999865531921\n",
      "Iteration 4470 Training loss 0.027256520465016365 Validation loss 0.03220488876104355 Accuracy 0.6672999858856201\n",
      "Iteration 4480 Training loss 0.029359780251979828 Validation loss 0.03327661752700806 Accuracy 0.6571999788284302\n",
      "Iteration 4490 Training loss 0.02910330891609192 Validation loss 0.03252362459897995 Accuracy 0.6653000116348267\n",
      "Iteration 4500 Training loss 0.0344243124127388 Validation loss 0.0321531780064106 Accuracy 0.6682000160217285\n",
      "Iteration 4510 Training loss 0.03040403686463833 Validation loss 0.03203326836228371 Accuracy 0.66839998960495\n",
      "Iteration 4520 Training loss 0.031772974878549576 Validation loss 0.03202614188194275 Accuracy 0.6717000007629395\n",
      "Iteration 4530 Training loss 0.03286848962306976 Validation loss 0.03259662911295891 Accuracy 0.6661999821662903\n",
      "Iteration 4540 Training loss 0.030087992548942566 Validation loss 0.03252945467829704 Accuracy 0.6643999814987183\n",
      "Iteration 4550 Training loss 0.027905214577913284 Validation loss 0.03213105350732803 Accuracy 0.6690999865531921\n",
      "Iteration 4560 Training loss 0.02918166294693947 Validation loss 0.03261309862136841 Accuracy 0.6646000146865845\n",
      "Iteration 4570 Training loss 0.029126927256584167 Validation loss 0.03216124698519707 Accuracy 0.6693000197410583\n",
      "Iteration 4580 Training loss 0.03129718825221062 Validation loss 0.032414112240076065 Accuracy 0.6653000116348267\n",
      "Iteration 4590 Training loss 0.03174195438623428 Validation loss 0.032895419746637344 Accuracy 0.6625000238418579\n",
      "Iteration 4600 Training loss 0.032650381326675415 Validation loss 0.0322299487888813 Accuracy 0.6682999730110168\n",
      "Iteration 4610 Training loss 0.028045445680618286 Validation loss 0.03259395807981491 Accuracy 0.6639000177383423\n",
      "Iteration 4620 Training loss 0.03219728171825409 Validation loss 0.03312205150723457 Accuracy 0.6588000059127808\n",
      "Iteration 4630 Training loss 0.030719362199306488 Validation loss 0.03310877084732056 Accuracy 0.6588000059127808\n",
      "Iteration 4640 Training loss 0.03095814771950245 Validation loss 0.032332953065633774 Accuracy 0.6650999784469604\n",
      "Iteration 4650 Training loss 0.029477538540959358 Validation loss 0.0317467600107193 Accuracy 0.6711000204086304\n",
      "Iteration 4660 Training loss 0.027568060904741287 Validation loss 0.03203031048178673 Accuracy 0.6690999865531921\n",
      "Iteration 4670 Training loss 0.030891673639416695 Validation loss 0.03232216835021973 Accuracy 0.6635000109672546\n",
      "Iteration 4680 Training loss 0.029737550765275955 Validation loss 0.03158508241176605 Accuracy 0.6722000241279602\n",
      "Iteration 4690 Training loss 0.030833527445793152 Validation loss 0.0318228043615818 Accuracy 0.6718999743461609\n",
      "Iteration 4700 Training loss 0.03165600448846817 Validation loss 0.03171205148100853 Accuracy 0.6743999719619751\n",
      "Iteration 4710 Training loss 0.02838755212724209 Validation loss 0.03250611200928688 Accuracy 0.6658999919891357\n",
      "Iteration 4720 Training loss 0.028400162234902382 Validation loss 0.03242482990026474 Accuracy 0.6704999804496765\n",
      "Iteration 4730 Training loss 0.028311042115092278 Validation loss 0.03194471076130867 Accuracy 0.6686999797821045\n",
      "Iteration 4740 Training loss 0.029216399416327477 Validation loss 0.03283917158842087 Accuracy 0.6604999899864197\n",
      "Iteration 4750 Training loss 0.030051633715629578 Validation loss 0.03166362643241882 Accuracy 0.6711000204086304\n",
      "Iteration 4760 Training loss 0.030830269679427147 Validation loss 0.031874824315309525 Accuracy 0.6711999773979187\n",
      "Iteration 4770 Training loss 0.03270643576979637 Validation loss 0.031490717083215714 Accuracy 0.6721000075340271\n",
      "Iteration 4780 Training loss 0.03070823848247528 Validation loss 0.03140828758478165 Accuracy 0.6736000180244446\n",
      "Iteration 4790 Training loss 0.03420894965529442 Validation loss 0.033782538026571274 Accuracy 0.6520000100135803\n",
      "Iteration 4800 Training loss 0.027818117290735245 Validation loss 0.03193138912320137 Accuracy 0.6687999963760376\n",
      "Iteration 4810 Training loss 0.0323638990521431 Validation loss 0.03150678798556328 Accuracy 0.6739000082015991\n",
      "Iteration 4820 Training loss 0.03172755613923073 Validation loss 0.03324804827570915 Accuracy 0.6600000262260437\n",
      "Iteration 4830 Training loss 0.031733669340610504 Validation loss 0.03158140555024147 Accuracy 0.6733999848365784\n",
      "Iteration 4840 Training loss 0.032645124942064285 Validation loss 0.03243337199091911 Accuracy 0.6669999957084656\n",
      "Iteration 4850 Training loss 0.031165514141321182 Validation loss 0.03170887008309364 Accuracy 0.6708999872207642\n",
      "Iteration 4860 Training loss 0.030329592525959015 Validation loss 0.03221367299556732 Accuracy 0.6700000166893005\n",
      "Iteration 4870 Training loss 0.026693234220147133 Validation loss 0.031619470566511154 Accuracy 0.6723999977111816\n",
      "Iteration 4880 Training loss 0.02742351032793522 Validation loss 0.03174281120300293 Accuracy 0.6700000166893005\n",
      "Iteration 4890 Training loss 0.028274262323975563 Validation loss 0.03131434693932533 Accuracy 0.6740000247955322\n",
      "Iteration 4900 Training loss 0.028604306280612946 Validation loss 0.03180277720093727 Accuracy 0.6693999767303467\n",
      "Iteration 4910 Training loss 0.03128720447421074 Validation loss 0.03124835714697838 Accuracy 0.6758000254631042\n",
      "Iteration 4920 Training loss 0.03254608064889908 Validation loss 0.032450757920742035 Accuracy 0.6628000140190125\n",
      "Iteration 4930 Training loss 0.03026147000491619 Validation loss 0.03178251162171364 Accuracy 0.6711999773979187\n",
      "Iteration 4940 Training loss 0.030498595908284187 Validation loss 0.03217297047376633 Accuracy 0.6668000221252441\n",
      "Iteration 4950 Training loss 0.028029372915625572 Validation loss 0.03208298236131668 Accuracy 0.6714000105857849\n",
      "Iteration 4960 Training loss 0.03056732751429081 Validation loss 0.031947191804647446 Accuracy 0.6672000288963318\n",
      "Iteration 4970 Training loss 0.03223487734794617 Validation loss 0.033955540508031845 Accuracy 0.6534000039100647\n",
      "Iteration 4980 Training loss 0.030511565506458282 Validation loss 0.03167673572897911 Accuracy 0.6725000143051147\n",
      "Iteration 4990 Training loss 0.02793765999376774 Validation loss 0.03148859366774559 Accuracy 0.6744999885559082\n",
      "Iteration 5000 Training loss 0.028942624107003212 Validation loss 0.0323602668941021 Accuracy 0.6654999852180481\n",
      "Iteration 5010 Training loss 0.030487602576613426 Validation loss 0.031608667224645615 Accuracy 0.6699000000953674\n",
      "Iteration 5020 Training loss 0.0306932982057333 Validation loss 0.03133110702037811 Accuracy 0.6736000180244446\n",
      "Iteration 5030 Training loss 0.02832827717065811 Validation loss 0.031168095767498016 Accuracy 0.6748999953269958\n",
      "Iteration 5040 Training loss 0.03051583841443062 Validation loss 0.03200056031346321 Accuracy 0.666700005531311\n",
      "Iteration 5050 Training loss 0.028223266825079918 Validation loss 0.03073245659470558 Accuracy 0.6780999898910522\n",
      "Iteration 5060 Training loss 0.03087148256599903 Validation loss 0.03260413929820061 Accuracy 0.6610999703407288\n",
      "Iteration 5070 Training loss 0.02956860139966011 Validation loss 0.031288985162973404 Accuracy 0.6729000210762024\n",
      "Iteration 5080 Training loss 0.031189773231744766 Validation loss 0.03248476982116699 Accuracy 0.6642000079154968\n",
      "Iteration 5090 Training loss 0.027905341237783432 Validation loss 0.0307142473757267 Accuracy 0.6783999800682068\n",
      "Iteration 5100 Training loss 0.030132029205560684 Validation loss 0.03203200548887253 Accuracy 0.6651999950408936\n",
      "Iteration 5110 Training loss 0.03220837190747261 Validation loss 0.030883170664310455 Accuracy 0.6765999794006348\n",
      "Iteration 5120 Training loss 0.029316721484065056 Validation loss 0.031131044030189514 Accuracy 0.671999990940094\n",
      "Iteration 5130 Training loss 0.02793743833899498 Validation loss 0.03128650411963463 Accuracy 0.6736999750137329\n",
      "Iteration 5140 Training loss 0.02932479791343212 Validation loss 0.03159283474087715 Accuracy 0.6735000014305115\n",
      "Iteration 5150 Training loss 0.030642395839095116 Validation loss 0.031229985877871513 Accuracy 0.6736000180244446\n",
      "Iteration 5160 Training loss 0.0269964337348938 Validation loss 0.031002415344119072 Accuracy 0.6747000217437744\n",
      "Iteration 5170 Training loss 0.031223909929394722 Validation loss 0.03331424668431282 Accuracy 0.6550999879837036\n",
      "Iteration 5180 Training loss 0.03027842938899994 Validation loss 0.03164201229810715 Accuracy 0.6699000000953674\n",
      "Iteration 5190 Training loss 0.02795305848121643 Validation loss 0.032209668308496475 Accuracy 0.6620000004768372\n",
      "Iteration 5200 Training loss 0.028042709454894066 Validation loss 0.032044459134340286 Accuracy 0.6662999987602234\n",
      "Iteration 5210 Training loss 0.02618653140962124 Validation loss 0.030830461531877518 Accuracy 0.6761999726295471\n",
      "Iteration 5220 Training loss 0.026124807074666023 Validation loss 0.03126702085137367 Accuracy 0.671999990940094\n",
      "Iteration 5230 Training loss 0.02940322458744049 Validation loss 0.030913041904568672 Accuracy 0.6748999953269958\n",
      "Iteration 5240 Training loss 0.030328312888741493 Validation loss 0.03194752335548401 Accuracy 0.6689000129699707\n",
      "Iteration 5250 Training loss 0.03239988535642624 Validation loss 0.03128943219780922 Accuracy 0.6704999804496765\n",
      "Iteration 5260 Training loss 0.030061259865760803 Validation loss 0.031442791223526 Accuracy 0.6725000143051147\n",
      "Iteration 5270 Training loss 0.029512746259570122 Validation loss 0.031440433114767075 Accuracy 0.6690999865531921\n",
      "Iteration 5280 Training loss 0.02879631519317627 Validation loss 0.03168309107422829 Accuracy 0.6741999983787537\n",
      "Iteration 5290 Training loss 0.029220636934041977 Validation loss 0.031069960445165634 Accuracy 0.6722999811172485\n",
      "Iteration 5300 Training loss 0.03047315962612629 Validation loss 0.03155786171555519 Accuracy 0.6693000197410583\n",
      "Iteration 5310 Training loss 0.029058756306767464 Validation loss 0.03134679049253464 Accuracy 0.670199990272522\n",
      "Iteration 5320 Training loss 0.028264537453651428 Validation loss 0.031236637383699417 Accuracy 0.6694999933242798\n",
      "Iteration 5330 Training loss 0.0320255383849144 Validation loss 0.03307875618338585 Accuracy 0.6606000065803528\n",
      "Iteration 5340 Training loss 0.030842239037156105 Validation loss 0.03162762150168419 Accuracy 0.6739000082015991\n",
      "Iteration 5350 Training loss 0.03118293546140194 Validation loss 0.03127311170101166 Accuracy 0.6711999773979187\n",
      "Iteration 5360 Training loss 0.028507594019174576 Validation loss 0.03157590702176094 Accuracy 0.66839998960495\n",
      "Iteration 5370 Training loss 0.028455764055252075 Validation loss 0.030699538066983223 Accuracy 0.6736999750137329\n",
      "Iteration 5380 Training loss 0.03198462352156639 Validation loss 0.032320842146873474 Accuracy 0.6654999852180481\n",
      "Iteration 5390 Training loss 0.03140736743807793 Validation loss 0.031565479934215546 Accuracy 0.666100025177002\n",
      "Iteration 5400 Training loss 0.028006218373775482 Validation loss 0.031835805624723434 Accuracy 0.666100025177002\n",
      "Iteration 5410 Training loss 0.0278556440025568 Validation loss 0.030979732051491737 Accuracy 0.6743000149726868\n",
      "Iteration 5420 Training loss 0.03185952827334404 Validation loss 0.032363295555114746 Accuracy 0.6643000245094299\n",
      "Iteration 5430 Training loss 0.030312765389680862 Validation loss 0.03063124604523182 Accuracy 0.6794000267982483\n",
      "Iteration 5440 Training loss 0.03034006431698799 Validation loss 0.0313577763736248 Accuracy 0.6728000044822693\n",
      "Iteration 5450 Training loss 0.02883022278547287 Validation loss 0.031085815280675888 Accuracy 0.6729000210762024\n",
      "Iteration 5460 Training loss 0.030019309371709824 Validation loss 0.03212998807430267 Accuracy 0.6600000262260437\n",
      "Iteration 5470 Training loss 0.031648341566324234 Validation loss 0.03194909170269966 Accuracy 0.6733999848365784\n",
      "Iteration 5480 Training loss 0.02763039618730545 Validation loss 0.032093632966279984 Accuracy 0.6685000061988831\n",
      "Iteration 5490 Training loss 0.03041256032884121 Validation loss 0.03097146935760975 Accuracy 0.6736999750137329\n",
      "Iteration 5500 Training loss 0.02848554030060768 Validation loss 0.03052191436290741 Accuracy 0.6761999726295471\n",
      "Iteration 5510 Training loss 0.02629963867366314 Validation loss 0.030436435714364052 Accuracy 0.6787999868392944\n",
      "Iteration 5520 Training loss 0.030513450503349304 Validation loss 0.03040607087314129 Accuracy 0.6754000186920166\n",
      "Iteration 5530 Training loss 0.030697977170348167 Validation loss 0.030580682680010796 Accuracy 0.6771000027656555\n",
      "Iteration 5540 Training loss 0.029899979010224342 Validation loss 0.030300380662083626 Accuracy 0.6776999831199646\n",
      "Iteration 5550 Training loss 0.02498677559196949 Validation loss 0.031641390174627304 Accuracy 0.6726999878883362\n",
      "Iteration 5560 Training loss 0.031615883111953735 Validation loss 0.03135346621274948 Accuracy 0.6693999767303467\n",
      "Iteration 5570 Training loss 0.027725882828235626 Validation loss 0.030949784442782402 Accuracy 0.669700026512146\n",
      "Iteration 5580 Training loss 0.02612471766769886 Validation loss 0.030200988054275513 Accuracy 0.6779999732971191\n",
      "Iteration 5590 Training loss 0.02860379032790661 Validation loss 0.031005214899778366 Accuracy 0.6740000247955322\n",
      "Iteration 5600 Training loss 0.027009520679712296 Validation loss 0.030508222058415413 Accuracy 0.6740999817848206\n",
      "Iteration 5610 Training loss 0.03244776278734207 Validation loss 0.03209061175584793 Accuracy 0.6621999740600586\n",
      "Iteration 5620 Training loss 0.028836753219366074 Validation loss 0.030420958995819092 Accuracy 0.675599992275238\n",
      "Iteration 5630 Training loss 0.030595896765589714 Validation loss 0.032130129635334015 Accuracy 0.6658999919891357\n",
      "Iteration 5640 Training loss 0.027674321085214615 Validation loss 0.030970605090260506 Accuracy 0.6669999957084656\n",
      "Iteration 5650 Training loss 0.02725808136165142 Validation loss 0.030202217400074005 Accuracy 0.6765000224113464\n",
      "Iteration 5660 Training loss 0.029564885422587395 Validation loss 0.03192786127328873 Accuracy 0.6628999710083008\n",
      "Iteration 5670 Training loss 0.02778099849820137 Validation loss 0.030311960726976395 Accuracy 0.6704999804496765\n",
      "Iteration 5680 Training loss 0.028484463691711426 Validation loss 0.03152745962142944 Accuracy 0.6620000004768372\n",
      "Iteration 5690 Training loss 0.02999141998589039 Validation loss 0.030806345865130424 Accuracy 0.6712999939918518\n",
      "Iteration 5700 Training loss 0.025752944871783257 Validation loss 0.030988233163952827 Accuracy 0.6725000143051147\n",
      "Iteration 5710 Training loss 0.030762679874897003 Validation loss 0.030563557520508766 Accuracy 0.6725999712944031\n",
      "Iteration 5720 Training loss 0.03109600581228733 Validation loss 0.03177832067012787 Accuracy 0.6671000123023987\n",
      "Iteration 5730 Training loss 0.028971023857593536 Validation loss 0.030492326244711876 Accuracy 0.6725999712944031\n",
      "Iteration 5740 Training loss 0.027697047218680382 Validation loss 0.02989416942000389 Accuracy 0.6758999824523926\n",
      "Iteration 5750 Training loss 0.027088187634944916 Validation loss 0.03015832230448723 Accuracy 0.6736999750137329\n",
      "Iteration 5760 Training loss 0.028213828802108765 Validation loss 0.030638612806797028 Accuracy 0.6736999750137329\n",
      "Iteration 5770 Training loss 0.026622675359249115 Validation loss 0.03193330392241478 Accuracy 0.6694999933242798\n",
      "Iteration 5780 Training loss 0.02840324118733406 Validation loss 0.029903754591941833 Accuracy 0.6765000224113464\n",
      "Iteration 5790 Training loss 0.02787291444838047 Validation loss 0.031016012653708458 Accuracy 0.6741999983787537\n",
      "Iteration 5800 Training loss 0.03167466074228287 Validation loss 0.031277310103178024 Accuracy 0.6689000129699707\n",
      "Iteration 5810 Training loss 0.02971094474196434 Validation loss 0.03053015097975731 Accuracy 0.6692000031471252\n",
      "Iteration 5820 Training loss 0.02710345946252346 Validation loss 0.03010454587638378 Accuracy 0.6754999756813049\n",
      "Iteration 5830 Training loss 0.028882263228297234 Validation loss 0.02998708002269268 Accuracy 0.6732000112533569\n",
      "Iteration 5840 Training loss 0.029151855036616325 Validation loss 0.031206265091896057 Accuracy 0.6697999835014343\n",
      "Iteration 5850 Training loss 0.025164389982819557 Validation loss 0.030223261564970016 Accuracy 0.670799970626831\n",
      "Iteration 5860 Training loss 0.02723097987473011 Validation loss 0.030117681249976158 Accuracy 0.6747000217437744\n",
      "Iteration 5870 Training loss 0.030952559784054756 Validation loss 0.03130204230546951 Accuracy 0.67330002784729\n",
      "Iteration 5880 Training loss 0.03013414703309536 Validation loss 0.030726127326488495 Accuracy 0.6646999716758728\n",
      "Iteration 5890 Training loss 0.02748277597129345 Validation loss 0.03175964578986168 Accuracy 0.6704000234603882\n",
      "Iteration 5900 Training loss 0.03066922537982464 Validation loss 0.032686300575733185 Accuracy 0.6593999862670898\n",
      "Iteration 5910 Training loss 0.030468355864286423 Validation loss 0.029913270846009254 Accuracy 0.6718000173568726\n",
      "Iteration 5920 Training loss 0.025964505970478058 Validation loss 0.02931988798081875 Accuracy 0.6762999892234802\n",
      "Iteration 5930 Training loss 0.027527622878551483 Validation loss 0.029512807726860046 Accuracy 0.6773999929428101\n",
      "Iteration 5940 Training loss 0.027679841965436935 Validation loss 0.03076760098338127 Accuracy 0.6748999953269958\n",
      "Iteration 5950 Training loss 0.03110388107597828 Validation loss 0.030568502843379974 Accuracy 0.6769000291824341\n",
      "Iteration 5960 Training loss 0.030716532841324806 Validation loss 0.029490182176232338 Accuracy 0.6771000027656555\n",
      "Iteration 5970 Training loss 0.03135716915130615 Validation loss 0.0311975609511137 Accuracy 0.6685000061988831\n",
      "Iteration 5980 Training loss 0.029289795085787773 Validation loss 0.030609067529439926 Accuracy 0.6729000210762024\n",
      "Iteration 5990 Training loss 0.026236986741423607 Validation loss 0.029596274718642235 Accuracy 0.6748999953269958\n",
      "Iteration 6000 Training loss 0.029213406145572662 Validation loss 0.029250478371977806 Accuracy 0.6769000291824341\n",
      "Iteration 6010 Training loss 0.024851588532328606 Validation loss 0.029284419491887093 Accuracy 0.6779999732971191\n",
      "Iteration 6020 Training loss 0.024270331487059593 Validation loss 0.024546757340431213 Accuracy 0.7330999970436096\n",
      "Iteration 6030 Training loss 0.022816920652985573 Validation loss 0.0241033174097538 Accuracy 0.7455999851226807\n",
      "Iteration 6040 Training loss 0.02156693860888481 Validation loss 0.023457709699869156 Accuracy 0.7470999956130981\n",
      "Iteration 6050 Training loss 0.021623995155096054 Validation loss 0.022941486909985542 Accuracy 0.7520999908447266\n",
      "Iteration 6060 Training loss 0.021610137075185776 Validation loss 0.022673798725008965 Accuracy 0.7570000290870667\n",
      "Iteration 6070 Training loss 0.01774407923221588 Validation loss 0.022508950904011726 Accuracy 0.7591999769210815\n",
      "Iteration 6080 Training loss 0.020581306889653206 Validation loss 0.02207355946302414 Accuracy 0.7630000114440918\n",
      "Iteration 6090 Training loss 0.02027016133069992 Validation loss 0.022677438333630562 Accuracy 0.7610999941825867\n",
      "Iteration 6100 Training loss 0.021146873012185097 Validation loss 0.022623881697654724 Accuracy 0.760699987411499\n",
      "Iteration 6110 Training loss 0.019289931282401085 Validation loss 0.021959882229566574 Accuracy 0.7638000249862671\n",
      "Iteration 6120 Training loss 0.02164563536643982 Validation loss 0.022311870008707047 Accuracy 0.7616999745368958\n",
      "Iteration 6130 Training loss 0.02412003092467785 Validation loss 0.024507440626621246 Accuracy 0.7379000186920166\n",
      "Iteration 6140 Training loss 0.020277654752135277 Validation loss 0.02261793613433838 Accuracy 0.7591000199317932\n",
      "Iteration 6150 Training loss 0.01740642637014389 Validation loss 0.02207934856414795 Accuracy 0.7609999775886536\n",
      "Iteration 6160 Training loss 0.01872064359486103 Validation loss 0.021692577749490738 Accuracy 0.765999972820282\n",
      "Iteration 6170 Training loss 0.021748514845967293 Validation loss 0.02206403762102127 Accuracy 0.7599999904632568\n",
      "Iteration 6180 Training loss 0.02051151916384697 Validation loss 0.02207052893936634 Accuracy 0.7621999979019165\n",
      "Iteration 6190 Training loss 0.01933232881128788 Validation loss 0.0220484659075737 Accuracy 0.7639999985694885\n",
      "Iteration 6200 Training loss 0.019266534596681595 Validation loss 0.021690813824534416 Accuracy 0.765500009059906\n",
      "Iteration 6210 Training loss 0.019053945317864418 Validation loss 0.02197636291384697 Accuracy 0.7633000016212463\n",
      "Iteration 6220 Training loss 0.020106887444853783 Validation loss 0.02194788306951523 Accuracy 0.7641000151634216\n",
      "Iteration 6230 Training loss 0.018492411822080612 Validation loss 0.022477006539702415 Accuracy 0.7567999958992004\n",
      "Iteration 6240 Training loss 0.01826520450413227 Validation loss 0.02233394794166088 Accuracy 0.7603999972343445\n",
      "Iteration 6250 Training loss 0.02002853713929653 Validation loss 0.021944889798760414 Accuracy 0.7652000188827515\n",
      "Iteration 6260 Training loss 0.022091718390583992 Validation loss 0.022879375144839287 Accuracy 0.7548999786376953\n",
      "Iteration 6270 Training loss 0.019589653238654137 Validation loss 0.022031135857105255 Accuracy 0.7652999758720398\n",
      "Iteration 6280 Training loss 0.01678317040205002 Validation loss 0.021748565137386322 Accuracy 0.7682999968528748\n",
      "Iteration 6290 Training loss 0.019884265959262848 Validation loss 0.02176033705472946 Accuracy 0.7627999782562256\n",
      "Iteration 6300 Training loss 0.0207474734634161 Validation loss 0.022630879655480385 Accuracy 0.7620999813079834\n",
      "Iteration 6310 Training loss 0.01762731745839119 Validation loss 0.021671973168849945 Accuracy 0.7678999900817871\n",
      "Iteration 6320 Training loss 0.02089857868850231 Validation loss 0.022935695946216583 Accuracy 0.7508000135421753\n",
      "Iteration 6330 Training loss 0.01900296099483967 Validation loss 0.02209261991083622 Accuracy 0.7578999996185303\n",
      "Iteration 6340 Training loss 0.018940849229693413 Validation loss 0.02256181836128235 Accuracy 0.7587000131607056\n",
      "Iteration 6350 Training loss 0.016801876947283745 Validation loss 0.02162409946322441 Accuracy 0.7671999931335449\n",
      "Iteration 6360 Training loss 0.016694534569978714 Validation loss 0.021303405985236168 Accuracy 0.7663999795913696\n",
      "Iteration 6370 Training loss 0.014386139810085297 Validation loss 0.021708233281970024 Accuracy 0.7617999911308289\n",
      "Iteration 6380 Training loss 0.01980193704366684 Validation loss 0.02194465510547161 Accuracy 0.7610999941825867\n",
      "Iteration 6390 Training loss 0.01986505836248398 Validation loss 0.02174026146531105 Accuracy 0.7631000280380249\n",
      "Iteration 6400 Training loss 0.019312718883156776 Validation loss 0.022976724430918694 Accuracy 0.7565000057220459\n",
      "Iteration 6410 Training loss 0.018035322427749634 Validation loss 0.021091187372803688 Accuracy 0.7702000141143799\n",
      "Iteration 6420 Training loss 0.01916721649467945 Validation loss 0.021173972636461258 Accuracy 0.7667999863624573\n",
      "Iteration 6430 Training loss 0.015519761480391026 Validation loss 0.01725865714251995 Accuracy 0.8172000050544739\n",
      "Iteration 6440 Training loss 0.011218143627047539 Validation loss 0.014927483163774014 Accuracy 0.842199981212616\n",
      "Iteration 6450 Training loss 0.011908601969480515 Validation loss 0.01464911364018917 Accuracy 0.8442000150680542\n",
      "Iteration 6460 Training loss 0.011267341673374176 Validation loss 0.014224727638065815 Accuracy 0.8495000004768372\n",
      "Iteration 6470 Training loss 0.011250353418290615 Validation loss 0.014826428145170212 Accuracy 0.8432000279426575\n",
      "Iteration 6480 Training loss 0.010708113200962543 Validation loss 0.015122288838028908 Accuracy 0.840499997138977\n",
      "Iteration 6490 Training loss 0.011582712642848492 Validation loss 0.014107713475823402 Accuracy 0.8513000011444092\n",
      "Iteration 6500 Training loss 0.012775802984833717 Validation loss 0.013948455452919006 Accuracy 0.8526999950408936\n",
      "Iteration 6510 Training loss 0.009966538287699223 Validation loss 0.01415225863456726 Accuracy 0.8508999943733215\n",
      "Iteration 6520 Training loss 0.012508123181760311 Validation loss 0.014771801419556141 Accuracy 0.8436999917030334\n",
      "Iteration 6530 Training loss 0.012512272223830223 Validation loss 0.014987445436418056 Accuracy 0.8427000045776367\n",
      "Iteration 6540 Training loss 0.012042796239256859 Validation loss 0.014430377632379532 Accuracy 0.8467000126838684\n",
      "Iteration 6550 Training loss 0.01091533899307251 Validation loss 0.01494066696614027 Accuracy 0.8425999879837036\n",
      "Iteration 6560 Training loss 0.01222633570432663 Validation loss 0.014421931467950344 Accuracy 0.8460999727249146\n",
      "Iteration 6570 Training loss 0.007913944311439991 Validation loss 0.014068002812564373 Accuracy 0.8500999808311462\n",
      "Iteration 6580 Training loss 0.01194045227020979 Validation loss 0.01377837173640728 Accuracy 0.8549000024795532\n",
      "Iteration 6590 Training loss 0.010358663275837898 Validation loss 0.013821464963257313 Accuracy 0.8535000085830688\n",
      "Iteration 6600 Training loss 0.013961159624159336 Validation loss 0.014513936825096607 Accuracy 0.8464999794960022\n",
      "Iteration 6610 Training loss 0.008883059024810791 Validation loss 0.01440660934895277 Accuracy 0.8468000292778015\n",
      "Iteration 6620 Training loss 0.011924952268600464 Validation loss 0.013685637153685093 Accuracy 0.8560000061988831\n",
      "Iteration 6630 Training loss 0.013233094476163387 Validation loss 0.013774164952337742 Accuracy 0.8539000153541565\n",
      "Iteration 6640 Training loss 0.009890489280223846 Validation loss 0.014227259904146194 Accuracy 0.8503999710083008\n",
      "Iteration 6650 Training loss 0.01311394851654768 Validation loss 0.013977187685668468 Accuracy 0.8525000214576721\n",
      "Iteration 6660 Training loss 0.01462509110569954 Validation loss 0.014908582903444767 Accuracy 0.8442000150680542\n",
      "Iteration 6670 Training loss 0.012983868829905987 Validation loss 0.013478493317961693 Accuracy 0.8569999933242798\n",
      "Iteration 6680 Training loss 0.009387029334902763 Validation loss 0.013873486779630184 Accuracy 0.8539999723434448\n",
      "Iteration 6690 Training loss 0.011799871921539307 Validation loss 0.013922021724283695 Accuracy 0.8519999980926514\n",
      "Iteration 6700 Training loss 0.012078803963959217 Validation loss 0.014096016064286232 Accuracy 0.8507000207901001\n",
      "Iteration 6710 Training loss 0.011160999536514282 Validation loss 0.013375156559050083 Accuracy 0.858299970626831\n",
      "Iteration 6720 Training loss 0.01354204211384058 Validation loss 0.013735486194491386 Accuracy 0.855400025844574\n",
      "Iteration 6730 Training loss 0.011275405064225197 Validation loss 0.01354036945849657 Accuracy 0.8561000227928162\n",
      "Iteration 6740 Training loss 0.012207751162350178 Validation loss 0.014016557484865189 Accuracy 0.8518000245094299\n",
      "Iteration 6750 Training loss 0.010372147895395756 Validation loss 0.014470160938799381 Accuracy 0.8464000225067139\n",
      "Iteration 6760 Training loss 0.01012719888240099 Validation loss 0.01342157181352377 Accuracy 0.8575000166893005\n",
      "Iteration 6770 Training loss 0.009526856243610382 Validation loss 0.013707997277379036 Accuracy 0.8547000288963318\n",
      "Iteration 6780 Training loss 0.009932310320436954 Validation loss 0.01342025026679039 Accuracy 0.857200026512146\n",
      "Iteration 6790 Training loss 0.010138651356101036 Validation loss 0.013530290685594082 Accuracy 0.8571000099182129\n",
      "Iteration 6800 Training loss 0.010279922746121883 Validation loss 0.013794602826237679 Accuracy 0.8547000288963318\n",
      "Iteration 6810 Training loss 0.009874889627099037 Validation loss 0.013391387648880482 Accuracy 0.8586999773979187\n",
      "Iteration 6820 Training loss 0.009529230184853077 Validation loss 0.01444236095994711 Accuracy 0.847100019454956\n",
      "Iteration 6830 Training loss 0.00967415701597929 Validation loss 0.013685795478522778 Accuracy 0.8539999723434448\n",
      "Iteration 6840 Training loss 0.012285619042813778 Validation loss 0.013692586682736874 Accuracy 0.8551999926567078\n",
      "Iteration 6850 Training loss 0.011016856878995895 Validation loss 0.013476542197167873 Accuracy 0.8569999933242798\n",
      "Iteration 6860 Training loss 0.010832635685801506 Validation loss 0.014516902156174183 Accuracy 0.8465999960899353\n",
      "Iteration 6870 Training loss 0.011109664104878902 Validation loss 0.013812622055411339 Accuracy 0.8547000288963318\n",
      "Iteration 6880 Training loss 0.01172665785998106 Validation loss 0.013604236766695976 Accuracy 0.8568999767303467\n",
      "Iteration 6890 Training loss 0.01265343464910984 Validation loss 0.013684632256627083 Accuracy 0.8561000227928162\n",
      "Iteration 6900 Training loss 0.011703300289809704 Validation loss 0.013113107532262802 Accuracy 0.8611999750137329\n",
      "Iteration 6910 Training loss 0.010143415071070194 Validation loss 0.013403354212641716 Accuracy 0.8579000234603882\n",
      "Iteration 6920 Training loss 0.009073556400835514 Validation loss 0.013551263138651848 Accuracy 0.8564000129699707\n",
      "Iteration 6930 Training loss 0.009098236449062824 Validation loss 0.013569274917244911 Accuracy 0.8568999767303467\n",
      "Iteration 6940 Training loss 0.008730795234441757 Validation loss 0.013190288096666336 Accuracy 0.8604000210762024\n",
      "Iteration 6950 Training loss 0.011453384533524513 Validation loss 0.013788464479148388 Accuracy 0.8546000123023987\n",
      "Iteration 6960 Training loss 0.011358977295458317 Validation loss 0.01339599210768938 Accuracy 0.8597000241279602\n",
      "Iteration 6970 Training loss 0.009485478512942791 Validation loss 0.01310429722070694 Accuracy 0.8615999817848206\n",
      "Iteration 6980 Training loss 0.008882826194167137 Validation loss 0.013198801316320896 Accuracy 0.8601999878883362\n",
      "Iteration 6990 Training loss 0.011553807184100151 Validation loss 0.014544074423611164 Accuracy 0.847000002861023\n",
      "Iteration 7000 Training loss 0.010494666174054146 Validation loss 0.013267678208649158 Accuracy 0.8590999841690063\n",
      "Iteration 7010 Training loss 0.011498447507619858 Validation loss 0.013507859781384468 Accuracy 0.8572999835014343\n",
      "Iteration 7020 Training loss 0.011620218865573406 Validation loss 0.013462391681969166 Accuracy 0.8571000099182129\n",
      "Iteration 7030 Training loss 0.010845310054719448 Validation loss 0.01363785658031702 Accuracy 0.855400025844574\n",
      "Iteration 7040 Training loss 0.011463304050266743 Validation loss 0.013331012800335884 Accuracy 0.859000027179718\n",
      "Iteration 7050 Training loss 0.00987896230071783 Validation loss 0.013432332314550877 Accuracy 0.8582000136375427\n",
      "Iteration 7060 Training loss 0.008707675151526928 Validation loss 0.013178469613194466 Accuracy 0.8603000044822693\n",
      "Iteration 7070 Training loss 0.008884023874998093 Validation loss 0.013179515488445759 Accuracy 0.8607000112533569\n",
      "Iteration 7080 Training loss 0.009859350509941578 Validation loss 0.013956740498542786 Accuracy 0.8533999919891357\n",
      "Iteration 7090 Training loss 0.009450304321944714 Validation loss 0.013463925570249557 Accuracy 0.858299970626831\n",
      "Iteration 7100 Training loss 0.011433705687522888 Validation loss 0.013166408985853195 Accuracy 0.8610000014305115\n",
      "Iteration 7110 Training loss 0.009203658439218998 Validation loss 0.0132803525775671 Accuracy 0.859000027179718\n",
      "Iteration 7120 Training loss 0.00968252681195736 Validation loss 0.013362676836550236 Accuracy 0.8587999939918518\n",
      "Iteration 7130 Training loss 0.009620919823646545 Validation loss 0.013257965445518494 Accuracy 0.8597000241279602\n",
      "Iteration 7140 Training loss 0.012260998599231243 Validation loss 0.013110408559441566 Accuracy 0.8614000082015991\n",
      "Iteration 7150 Training loss 0.011938600800931454 Validation loss 0.01320758555084467 Accuracy 0.86080002784729\n",
      "Iteration 7160 Training loss 0.008278501220047474 Validation loss 0.013641811907291412 Accuracy 0.8560000061988831\n",
      "Iteration 7170 Training loss 0.00788620114326477 Validation loss 0.013199987821280956 Accuracy 0.8611000180244446\n",
      "Iteration 7180 Training loss 0.011794486083090305 Validation loss 0.013199323788285255 Accuracy 0.8608999848365784\n",
      "Iteration 7190 Training loss 0.012349898926913738 Validation loss 0.013388445600867271 Accuracy 0.8592000007629395\n",
      "Iteration 7200 Training loss 0.009722610004246235 Validation loss 0.01338941976428032 Accuracy 0.8587999939918518\n",
      "Iteration 7210 Training loss 0.010645397007465363 Validation loss 0.013755189254879951 Accuracy 0.853600025177002\n",
      "Iteration 7220 Training loss 0.010412996634840965 Validation loss 0.013260787352919579 Accuracy 0.8603000044822693\n",
      "Iteration 7230 Training loss 0.009179537184536457 Validation loss 0.013696773909032345 Accuracy 0.855400025844574\n",
      "Iteration 7240 Training loss 0.009104463271796703 Validation loss 0.01317973155528307 Accuracy 0.8615999817848206\n",
      "Iteration 7250 Training loss 0.010299427434802055 Validation loss 0.013816759921610355 Accuracy 0.855400025844574\n",
      "Iteration 7260 Training loss 0.011066182516515255 Validation loss 0.013722160831093788 Accuracy 0.855400025844574\n",
      "Iteration 7270 Training loss 0.008184166625142097 Validation loss 0.01312453206628561 Accuracy 0.8608999848365784\n",
      "Iteration 7280 Training loss 0.010322525165975094 Validation loss 0.013000927865505219 Accuracy 0.8622999787330627\n",
      "Iteration 7290 Training loss 0.010020140558481216 Validation loss 0.012725016102194786 Accuracy 0.8647000193595886\n",
      "Iteration 7300 Training loss 0.009775172919034958 Validation loss 0.013173071667551994 Accuracy 0.8610000014305115\n",
      "Iteration 7310 Training loss 0.01020924374461174 Validation loss 0.013075178489089012 Accuracy 0.8621000051498413\n",
      "Iteration 7320 Training loss 0.009812804870307446 Validation loss 0.013040544465184212 Accuracy 0.8618000149726868\n",
      "Iteration 7330 Training loss 0.011494717560708523 Validation loss 0.013744546100497246 Accuracy 0.8549000024795532\n",
      "Iteration 7340 Training loss 0.010548113845288754 Validation loss 0.013264586217701435 Accuracy 0.8593000173568726\n",
      "Iteration 7350 Training loss 0.012597735039889812 Validation loss 0.014936035498976707 Accuracy 0.8418999910354614\n",
      "Iteration 7360 Training loss 0.008300197310745716 Validation loss 0.013092842884361744 Accuracy 0.8618999719619751\n",
      "Iteration 7370 Training loss 0.011330142617225647 Validation loss 0.013224748894572258 Accuracy 0.8593999743461609\n",
      "Iteration 7380 Training loss 0.011903700418770313 Validation loss 0.012816091068089008 Accuracy 0.8634999990463257\n",
      "Iteration 7390 Training loss 0.008607188239693642 Validation loss 0.013140352442860603 Accuracy 0.8610000014305115\n",
      "Iteration 7400 Training loss 0.011459354311227798 Validation loss 0.013318619690835476 Accuracy 0.8597999811172485\n",
      "Iteration 7410 Training loss 0.00898788869380951 Validation loss 0.01292386744171381 Accuracy 0.8628000020980835\n",
      "Iteration 7420 Training loss 0.00946817733347416 Validation loss 0.014229709282517433 Accuracy 0.8489000201225281\n",
      "Iteration 7430 Training loss 0.013108969666063786 Validation loss 0.014244756661355495 Accuracy 0.8493000268936157\n",
      "Iteration 7440 Training loss 0.01111501082777977 Validation loss 0.012972096912562847 Accuracy 0.862500011920929\n",
      "Iteration 7450 Training loss 0.010143755003809929 Validation loss 0.013276316225528717 Accuracy 0.8597999811172485\n",
      "Iteration 7460 Training loss 0.007849187590181828 Validation loss 0.012789614498615265 Accuracy 0.8648999929428101\n",
      "Iteration 7470 Training loss 0.01020459458231926 Validation loss 0.012944300659000874 Accuracy 0.8621000051498413\n",
      "Iteration 7480 Training loss 0.01090958621352911 Validation loss 0.0133422976359725 Accuracy 0.8590999841690063\n",
      "Iteration 7490 Training loss 0.009289815090596676 Validation loss 0.013142851181328297 Accuracy 0.8618999719619751\n",
      "Iteration 7500 Training loss 0.008686759509146214 Validation loss 0.013549486175179482 Accuracy 0.8550999760627747\n",
      "Iteration 7510 Training loss 0.0071966033428907394 Validation loss 0.012891377322375774 Accuracy 0.8632000088691711\n",
      "Iteration 7520 Training loss 0.009843919426202774 Validation loss 0.012835628353059292 Accuracy 0.8636999726295471\n",
      "Iteration 7530 Training loss 0.011631559580564499 Validation loss 0.013467526994645596 Accuracy 0.8569999933242798\n",
      "Iteration 7540 Training loss 0.0068893395364284515 Validation loss 0.012968749739229679 Accuracy 0.8618000149726868\n",
      "Iteration 7550 Training loss 0.011586948297917843 Validation loss 0.013276159763336182 Accuracy 0.8597999811172485\n",
      "Iteration 7560 Training loss 0.008120804093778133 Validation loss 0.013481123372912407 Accuracy 0.857699990272522\n",
      "Iteration 7570 Training loss 0.01138708833605051 Validation loss 0.012957221828401089 Accuracy 0.8622999787330627\n",
      "Iteration 7580 Training loss 0.011075301095843315 Validation loss 0.012971164658665657 Accuracy 0.863099992275238\n",
      "Iteration 7590 Training loss 0.011202791705727577 Validation loss 0.013408235274255276 Accuracy 0.8579999804496765\n",
      "Iteration 7600 Training loss 0.012015542015433311 Validation loss 0.014169223606586456 Accuracy 0.8514999747276306\n",
      "Iteration 7610 Training loss 0.01086416095495224 Validation loss 0.012680635787546635 Accuracy 0.8655999898910522\n",
      "Iteration 7620 Training loss 0.0102327149361372 Validation loss 0.013023043982684612 Accuracy 0.8622999787330627\n",
      "Iteration 7630 Training loss 0.011643532663583755 Validation loss 0.013675414957106113 Accuracy 0.8551999926567078\n",
      "Iteration 7640 Training loss 0.009868119843304157 Validation loss 0.013071706518530846 Accuracy 0.8614000082015991\n",
      "Iteration 7650 Training loss 0.010171692818403244 Validation loss 0.013327102176845074 Accuracy 0.8600000143051147\n",
      "Iteration 7660 Training loss 0.009859969839453697 Validation loss 0.014303319156169891 Accuracy 0.8489000201225281\n",
      "Iteration 7670 Training loss 0.010656251572072506 Validation loss 0.012811611406505108 Accuracy 0.8640000224113464\n",
      "Iteration 7680 Training loss 0.008172506466507912 Validation loss 0.012919126078486443 Accuracy 0.8633000254631042\n",
      "Iteration 7690 Training loss 0.010722114704549313 Validation loss 0.01335117407143116 Accuracy 0.8578000068664551\n",
      "Iteration 7700 Training loss 0.011028526350855827 Validation loss 0.013152829371392727 Accuracy 0.8607000112533569\n",
      "Iteration 7710 Training loss 0.008265642449259758 Validation loss 0.01245537307113409 Accuracy 0.8682000041007996\n",
      "Iteration 7720 Training loss 0.008332489989697933 Validation loss 0.012903906404972076 Accuracy 0.8634999990463257\n",
      "Iteration 7730 Training loss 0.008138353005051613 Validation loss 0.012692934833467007 Accuracy 0.8658999800682068\n",
      "Iteration 7740 Training loss 0.009406904689967632 Validation loss 0.012876470573246479 Accuracy 0.8636000156402588\n",
      "Iteration 7750 Training loss 0.011679752729833126 Validation loss 0.01383401732891798 Accuracy 0.8515999913215637\n",
      "Iteration 7760 Training loss 0.01010316051542759 Validation loss 0.013338596560060978 Accuracy 0.859000027179718\n",
      "Iteration 7770 Training loss 0.009387427940964699 Validation loss 0.01275188010185957 Accuracy 0.8641999959945679\n",
      "Iteration 7780 Training loss 0.00850242655724287 Validation loss 0.013744493015110493 Accuracy 0.8540999889373779\n",
      "Iteration 7790 Training loss 0.00825503095984459 Validation loss 0.012698588892817497 Accuracy 0.8648999929428101\n",
      "Iteration 7800 Training loss 0.011867714114487171 Validation loss 0.01313154585659504 Accuracy 0.8605999946594238\n",
      "Iteration 7810 Training loss 0.008220132440328598 Validation loss 0.012832500040531158 Accuracy 0.8641999959945679\n",
      "Iteration 7820 Training loss 0.010134818032383919 Validation loss 0.012784888036549091 Accuracy 0.8633999824523926\n",
      "Iteration 7830 Training loss 0.009440196678042412 Validation loss 0.013117022812366486 Accuracy 0.8608999848365784\n",
      "Iteration 7840 Training loss 0.0077570159919559956 Validation loss 0.013041405938565731 Accuracy 0.8618999719619751\n",
      "Iteration 7850 Training loss 0.011579141020774841 Validation loss 0.012729211710393429 Accuracy 0.864300012588501\n",
      "Iteration 7860 Training loss 0.00855813454836607 Validation loss 0.013020540587604046 Accuracy 0.8623999953269958\n",
      "Iteration 7870 Training loss 0.010499530471861362 Validation loss 0.012815616093575954 Accuracy 0.8647000193595886\n",
      "Iteration 7880 Training loss 0.008960175327956676 Validation loss 0.012949907220900059 Accuracy 0.8628000020980835\n",
      "Iteration 7890 Training loss 0.007899930700659752 Validation loss 0.01289895735681057 Accuracy 0.8640000224113464\n",
      "Iteration 7900 Training loss 0.009282881394028664 Validation loss 0.01291376631706953 Accuracy 0.8629999756813049\n",
      "Iteration 7910 Training loss 0.009135718457400799 Validation loss 0.012701025232672691 Accuracy 0.8658999800682068\n",
      "Iteration 7920 Training loss 0.008769802749156952 Validation loss 0.013136561028659344 Accuracy 0.8614000082015991\n",
      "Iteration 7930 Training loss 0.009941927157342434 Validation loss 0.01287499163299799 Accuracy 0.8634999990463257\n",
      "Iteration 7940 Training loss 0.00935150496661663 Validation loss 0.012982703745365143 Accuracy 0.8608999848365784\n",
      "Iteration 7950 Training loss 0.010180162265896797 Validation loss 0.012586778961122036 Accuracy 0.8673999905586243\n",
      "Iteration 7960 Training loss 0.010142543353140354 Validation loss 0.012550080195069313 Accuracy 0.8676000237464905\n",
      "Iteration 7970 Training loss 0.007599490694701672 Validation loss 0.01265275664627552 Accuracy 0.8661999702453613\n",
      "Iteration 7980 Training loss 0.009085095487535 Validation loss 0.013088508509099483 Accuracy 0.8608999848365784\n",
      "Iteration 7990 Training loss 0.008139533922076225 Validation loss 0.013346719555556774 Accuracy 0.8593000173568726\n",
      "Iteration 8000 Training loss 0.009992005303502083 Validation loss 0.012915288098156452 Accuracy 0.8632000088691711\n",
      "Iteration 8010 Training loss 0.008951160125434399 Validation loss 0.01404721662402153 Accuracy 0.8514999747276306\n",
      "Iteration 8020 Training loss 0.008560016751289368 Validation loss 0.012436588294804096 Accuracy 0.8679999709129333\n",
      "Iteration 8030 Training loss 0.010104943066835403 Validation loss 0.014067491516470909 Accuracy 0.8504999876022339\n",
      "Iteration 8040 Training loss 0.00799801480025053 Validation loss 0.01281999982893467 Accuracy 0.8636000156402588\n",
      "Iteration 8050 Training loss 0.010302831418812275 Validation loss 0.013202611356973648 Accuracy 0.8604999780654907\n",
      "Iteration 8060 Training loss 0.008271719329059124 Validation loss 0.012950201518833637 Accuracy 0.8628000020980835\n",
      "Iteration 8070 Training loss 0.007833597250282764 Validation loss 0.012595581822097301 Accuracy 0.866100013256073\n",
      "Iteration 8080 Training loss 0.008715661242604256 Validation loss 0.01287749595940113 Accuracy 0.8637999892234802\n",
      "Iteration 8090 Training loss 0.0105853620916605 Validation loss 0.01248152181506157 Accuracy 0.868399977684021\n",
      "Iteration 8100 Training loss 0.009283889085054398 Validation loss 0.012547153048217297 Accuracy 0.8672000169754028\n",
      "Iteration 8110 Training loss 0.009608946740627289 Validation loss 0.01297408901154995 Accuracy 0.8626000285148621\n",
      "Iteration 8120 Training loss 0.007831812836229801 Validation loss 0.01257706806063652 Accuracy 0.8662999868392944\n",
      "Iteration 8130 Training loss 0.008323091082274914 Validation loss 0.012673967517912388 Accuracy 0.8651000261306763\n",
      "Iteration 8140 Training loss 0.008156300522387028 Validation loss 0.0124823534861207 Accuracy 0.8668000102043152\n",
      "Iteration 8150 Training loss 0.006776794325560331 Validation loss 0.012833724729716778 Accuracy 0.8632000088691711\n",
      "Iteration 8160 Training loss 0.009775392711162567 Validation loss 0.012901789508759975 Accuracy 0.8626000285148621\n",
      "Iteration 8170 Training loss 0.007929924875497818 Validation loss 0.01231400016695261 Accuracy 0.8701000213623047\n",
      "Iteration 8180 Training loss 0.009495904669165611 Validation loss 0.012218584306538105 Accuracy 0.870199978351593\n",
      "Iteration 8190 Training loss 0.010203825309872627 Validation loss 0.012787623330950737 Accuracy 0.8647000193595886\n",
      "Iteration 8200 Training loss 0.008234980516135693 Validation loss 0.013642814010381699 Accuracy 0.8553000092506409\n",
      "Iteration 8210 Training loss 0.009331626817584038 Validation loss 0.012769929133355618 Accuracy 0.8640000224113464\n",
      "Iteration 8220 Training loss 0.007910733111202717 Validation loss 0.012444594874978065 Accuracy 0.8672000169754028\n",
      "Iteration 8230 Training loss 0.007639279589056969 Validation loss 0.012465031817555428 Accuracy 0.8676999807357788\n",
      "Iteration 8240 Training loss 0.012206757441163063 Validation loss 0.013338852673768997 Accuracy 0.8579000234603882\n",
      "Iteration 8250 Training loss 0.01065297145396471 Validation loss 0.012977548874914646 Accuracy 0.8623999953269958\n",
      "Iteration 8260 Training loss 0.0086798295378685 Validation loss 0.013204230926930904 Accuracy 0.8598999977111816\n",
      "Iteration 8270 Training loss 0.007978273555636406 Validation loss 0.013106157071888447 Accuracy 0.861299991607666\n",
      "Iteration 8280 Training loss 0.008205045014619827 Validation loss 0.012660731561481953 Accuracy 0.8654999732971191\n",
      "Iteration 8290 Training loss 0.009463793598115444 Validation loss 0.013463256880640984 Accuracy 0.8571000099182129\n",
      "Iteration 8300 Training loss 0.00851061288267374 Validation loss 0.012479810975492 Accuracy 0.8686000108718872\n",
      "Iteration 8310 Training loss 0.006737860385328531 Validation loss 0.01261135283857584 Accuracy 0.8654000163078308\n",
      "Iteration 8320 Training loss 0.010198654606938362 Validation loss 0.012763308361172676 Accuracy 0.8639000058174133\n",
      "Iteration 8330 Training loss 0.008835743181407452 Validation loss 0.012668673880398273 Accuracy 0.8648999929428101\n",
      "Iteration 8340 Training loss 0.00965089537203312 Validation loss 0.01253685262054205 Accuracy 0.8646000027656555\n",
      "Iteration 8350 Training loss 0.009334244765341282 Validation loss 0.013260242529213428 Accuracy 0.8593999743461609\n",
      "Iteration 8360 Training loss 0.00796134490519762 Validation loss 0.012305016629397869 Accuracy 0.8684999942779541\n",
      "Iteration 8370 Training loss 0.011377605609595776 Validation loss 0.01244201697409153 Accuracy 0.8686000108718872\n",
      "Iteration 8380 Training loss 0.010232037864625454 Validation loss 0.012726978398859501 Accuracy 0.8654999732971191\n",
      "Iteration 8390 Training loss 0.009454315528273582 Validation loss 0.01266918983310461 Accuracy 0.8651999831199646\n",
      "Iteration 8400 Training loss 0.006543975789099932 Validation loss 0.01242778915911913 Accuracy 0.8680999875068665\n",
      "Iteration 8410 Training loss 0.010469797067344189 Validation loss 0.012439106591045856 Accuracy 0.8676000237464905\n",
      "Iteration 8420 Training loss 0.00816389825195074 Validation loss 0.01319202035665512 Accuracy 0.8607000112533569\n",
      "Iteration 8430 Training loss 0.00893356092274189 Validation loss 0.012730535119771957 Accuracy 0.8647000193595886\n",
      "Iteration 8440 Training loss 0.00944883655756712 Validation loss 0.012282131239771843 Accuracy 0.8693000078201294\n",
      "Iteration 8450 Training loss 0.008098125457763672 Validation loss 0.012530961073935032 Accuracy 0.8671000003814697\n",
      "Iteration 8460 Training loss 0.007899359799921513 Validation loss 0.01284821331501007 Accuracy 0.8622000217437744\n",
      "Iteration 8470 Training loss 0.0076672653667628765 Validation loss 0.012666315771639347 Accuracy 0.8651999831199646\n",
      "Iteration 8480 Training loss 0.00835892278701067 Validation loss 0.012978598475456238 Accuracy 0.8611999750137329\n",
      "Iteration 8490 Training loss 0.010451456531882286 Validation loss 0.012826143763959408 Accuracy 0.8629000186920166\n",
      "Iteration 8500 Training loss 0.008098330348730087 Validation loss 0.01241808757185936 Accuracy 0.8669000267982483\n",
      "Iteration 8510 Training loss 0.00957880076020956 Validation loss 0.013629629276692867 Accuracy 0.8561000227928162\n",
      "Iteration 8520 Training loss 0.009335380047559738 Validation loss 0.013008704409003258 Accuracy 0.8618000149726868\n",
      "Iteration 8530 Training loss 0.010076385922729969 Validation loss 0.012895197607576847 Accuracy 0.8633999824523926\n",
      "Iteration 8540 Training loss 0.0085326312109828 Validation loss 0.012372808530926704 Accuracy 0.8683000206947327\n",
      "Iteration 8550 Training loss 0.009888498112559319 Validation loss 0.012525166384875774 Accuracy 0.8666999936103821\n",
      "Iteration 8560 Training loss 0.008766045793890953 Validation loss 0.01277044415473938 Accuracy 0.8650000095367432\n",
      "Iteration 8570 Training loss 0.009370503947138786 Validation loss 0.012908424250781536 Accuracy 0.8633000254631042\n",
      "Iteration 8580 Training loss 0.008340899832546711 Validation loss 0.013227947056293488 Accuracy 0.8593000173568726\n",
      "Iteration 8590 Training loss 0.008732020854949951 Validation loss 0.012634115293622017 Accuracy 0.8655999898910522\n",
      "Iteration 8600 Training loss 0.008679615333676338 Validation loss 0.012736826203763485 Accuracy 0.8654999732971191\n",
      "Iteration 8610 Training loss 0.007484467700123787 Validation loss 0.012585370801389217 Accuracy 0.8668000102043152\n",
      "Iteration 8620 Training loss 0.008713268674910069 Validation loss 0.012585802003741264 Accuracy 0.8664000034332275\n",
      "Iteration 8630 Training loss 0.007064232602715492 Validation loss 0.013017878867685795 Accuracy 0.8618000149726868\n",
      "Iteration 8640 Training loss 0.009325383231043816 Validation loss 0.012629607692360878 Accuracy 0.8659999966621399\n",
      "Iteration 8650 Training loss 0.011240113526582718 Validation loss 0.012491386383771896 Accuracy 0.8671000003814697\n",
      "Iteration 8660 Training loss 0.00946908351033926 Validation loss 0.01254356186836958 Accuracy 0.8676000237464905\n",
      "Iteration 8670 Training loss 0.009177079424262047 Validation loss 0.01296520046889782 Accuracy 0.862500011920929\n",
      "Iteration 8680 Training loss 0.00843492615967989 Validation loss 0.012747908011078835 Accuracy 0.8651000261306763\n",
      "Iteration 8690 Training loss 0.007980217225849628 Validation loss 0.01283323485404253 Accuracy 0.8636999726295471\n",
      "Iteration 8700 Training loss 0.00906201172620058 Validation loss 0.012380881235003471 Accuracy 0.8693000078201294\n",
      "Iteration 8710 Training loss 0.008560668677091599 Validation loss 0.012383184395730495 Accuracy 0.8690999746322632\n",
      "Iteration 8720 Training loss 0.008455192670226097 Validation loss 0.012612402439117432 Accuracy 0.8662999868392944\n",
      "Iteration 8730 Training loss 0.009354252368211746 Validation loss 0.012874340638518333 Accuracy 0.8621000051498413\n",
      "Iteration 8740 Training loss 0.009764393791556358 Validation loss 0.01223121490329504 Accuracy 0.8707000017166138\n",
      "Iteration 8750 Training loss 0.00823935866355896 Validation loss 0.012941204942762852 Accuracy 0.8618000149726868\n",
      "Iteration 8760 Training loss 0.006781206000596285 Validation loss 0.012816173955798149 Accuracy 0.8640000224113464\n",
      "Iteration 8770 Training loss 0.00830839853733778 Validation loss 0.012808461673557758 Accuracy 0.8639000058174133\n",
      "Iteration 8780 Training loss 0.00987114105373621 Validation loss 0.012614804320037365 Accuracy 0.8664000034332275\n",
      "Iteration 8790 Training loss 0.007458256557583809 Validation loss 0.012418394908308983 Accuracy 0.8672000169754028\n",
      "Iteration 8800 Training loss 0.008829084224998951 Validation loss 0.012962128035724163 Accuracy 0.8611000180244446\n",
      "Iteration 8810 Training loss 0.00838003121316433 Validation loss 0.012562611140310764 Accuracy 0.8658000230789185\n",
      "Iteration 8820 Training loss 0.00842898990958929 Validation loss 0.0127397570759058 Accuracy 0.8650000095367432\n",
      "Iteration 8830 Training loss 0.008744915947318077 Validation loss 0.012271370738744736 Accuracy 0.8698999881744385\n",
      "Iteration 8840 Training loss 0.008388496935367584 Validation loss 0.012454546988010406 Accuracy 0.8666999936103821\n",
      "Iteration 8850 Training loss 0.009100721217691898 Validation loss 0.01290003303438425 Accuracy 0.8639000058174133\n",
      "Iteration 8860 Training loss 0.00860452838242054 Validation loss 0.012741819024085999 Accuracy 0.8655999898910522\n",
      "Iteration 8870 Training loss 0.008178608492016792 Validation loss 0.012894212268292904 Accuracy 0.8633000254631042\n",
      "Iteration 8880 Training loss 0.00913645513355732 Validation loss 0.012399043887853622 Accuracy 0.8682000041007996\n",
      "Iteration 8890 Training loss 0.008831343613564968 Validation loss 0.012522924691438675 Accuracy 0.8661999702453613\n",
      "Iteration 8900 Training loss 0.005994495935738087 Validation loss 0.012559101916849613 Accuracy 0.8662999868392944\n",
      "Iteration 8910 Training loss 0.008888649754226208 Validation loss 0.012579611502587795 Accuracy 0.8664000034332275\n",
      "Iteration 8920 Training loss 0.009380301460623741 Validation loss 0.0124994907528162 Accuracy 0.8671000003814697\n",
      "Iteration 8930 Training loss 0.010270552709698677 Validation loss 0.012853330001235008 Accuracy 0.8633000254631042\n",
      "Iteration 8940 Training loss 0.008860933594405651 Validation loss 0.013044293969869614 Accuracy 0.863099992275238\n",
      "Iteration 8950 Training loss 0.008493500761687756 Validation loss 0.012400556355714798 Accuracy 0.867900013923645\n",
      "Iteration 8960 Training loss 0.0081720482558012 Validation loss 0.01277331542223692 Accuracy 0.8644999861717224\n",
      "Iteration 8970 Training loss 0.009202638640999794 Validation loss 0.012654864229261875 Accuracy 0.8647000193595886\n",
      "Iteration 8980 Training loss 0.00810175109654665 Validation loss 0.013677436858415604 Accuracy 0.8546000123023987\n",
      "Iteration 8990 Training loss 0.006343326065689325 Validation loss 0.01299277599900961 Accuracy 0.862500011920929\n",
      "Iteration 9000 Training loss 0.007965689525008202 Validation loss 0.01215063314884901 Accuracy 0.8712000250816345\n",
      "Iteration 9010 Training loss 0.008925631642341614 Validation loss 0.012785135768353939 Accuracy 0.8646000027656555\n",
      "Iteration 9020 Training loss 0.010207952931523323 Validation loss 0.012056361883878708 Accuracy 0.8708999752998352\n",
      "Iteration 9030 Training loss 0.007685324177145958 Validation loss 0.01239293348044157 Accuracy 0.8690000176429749\n",
      "Iteration 9040 Training loss 0.008181092329323292 Validation loss 0.0120869604870677 Accuracy 0.8709999918937683\n",
      "Iteration 9050 Training loss 0.008023957721889019 Validation loss 0.012721900828182697 Accuracy 0.8637999892234802\n",
      "Iteration 9060 Training loss 0.008343735709786415 Validation loss 0.01223475020378828 Accuracy 0.8698999881744385\n",
      "Iteration 9070 Training loss 0.009871170856058598 Validation loss 0.012694001197814941 Accuracy 0.8648999929428101\n",
      "Iteration 9080 Training loss 0.007777067366987467 Validation loss 0.012057812884449959 Accuracy 0.8707000017166138\n",
      "Iteration 9090 Training loss 0.008404156193137169 Validation loss 0.012067776173353195 Accuracy 0.8722000122070312\n",
      "Iteration 9100 Training loss 0.007229101378470659 Validation loss 0.012183310464024544 Accuracy 0.8704000115394592\n",
      "Iteration 9110 Training loss 0.007529808674007654 Validation loss 0.012095651589334011 Accuracy 0.8712999820709229\n",
      "Iteration 9120 Training loss 0.007117909844964743 Validation loss 0.012662788853049278 Accuracy 0.8640999794006348\n",
      "Iteration 9130 Training loss 0.007607873994857073 Validation loss 0.01215224340558052 Accuracy 0.8705000281333923\n",
      "Iteration 9140 Training loss 0.008234228938817978 Validation loss 0.012124082073569298 Accuracy 0.8707000017166138\n",
      "Iteration 9150 Training loss 0.009370788931846619 Validation loss 0.011988616548478603 Accuracy 0.8716999888420105\n",
      "Iteration 9160 Training loss 0.0077057029120624065 Validation loss 0.012326318770647049 Accuracy 0.8677999973297119\n",
      "Iteration 9170 Training loss 0.010873237624764442 Validation loss 0.013050432316958904 Accuracy 0.8600999712944031\n",
      "Iteration 9180 Training loss 0.009048514999449253 Validation loss 0.01288194116204977 Accuracy 0.8615000247955322\n",
      "Iteration 9190 Training loss 0.009038197807967663 Validation loss 0.011804507113993168 Accuracy 0.8734999895095825\n",
      "Iteration 9200 Training loss 0.008201397955417633 Validation loss 0.012448911555111408 Accuracy 0.8675000071525574\n",
      "Iteration 9210 Training loss 0.008158198557794094 Validation loss 0.012196523137390614 Accuracy 0.869700014591217\n",
      "Iteration 9220 Training loss 0.007269762456417084 Validation loss 0.012257133610546589 Accuracy 0.8690000176429749\n",
      "Iteration 9230 Training loss 0.009132149629294872 Validation loss 0.012569316662847996 Accuracy 0.866100013256073\n",
      "Iteration 9240 Training loss 0.010931327007710934 Validation loss 0.01320053543895483 Accuracy 0.8605999946594238\n",
      "Iteration 9250 Training loss 0.006947899702936411 Validation loss 0.012220588512718678 Accuracy 0.8687000274658203\n",
      "Iteration 9260 Training loss 0.007833356037735939 Validation loss 0.012219826690852642 Accuracy 0.8694999814033508\n",
      "Iteration 9270 Training loss 0.009913047775626183 Validation loss 0.01251868437975645 Accuracy 0.8652999997138977\n",
      "Iteration 9280 Training loss 0.009772123768925667 Validation loss 0.011899486184120178 Accuracy 0.8730999827384949\n",
      "Iteration 9290 Training loss 0.008156719617545605 Validation loss 0.012049942277371883 Accuracy 0.8712000250816345\n",
      "Iteration 9300 Training loss 0.0076072667725384235 Validation loss 0.012516341172158718 Accuracy 0.866100013256073\n",
      "Iteration 9310 Training loss 0.007566868327558041 Validation loss 0.012415406294167042 Accuracy 0.8672999739646912\n",
      "Iteration 9320 Training loss 0.008833860978484154 Validation loss 0.012310363352298737 Accuracy 0.8690999746322632\n",
      "Iteration 9330 Training loss 0.009272168390452862 Validation loss 0.012191124260425568 Accuracy 0.8701000213623047\n",
      "Iteration 9340 Training loss 0.010640115477144718 Validation loss 0.012197770178318024 Accuracy 0.870199978351593\n",
      "Iteration 9350 Training loss 0.009662322700023651 Validation loss 0.012183301150798798 Accuracy 0.8701000213623047\n",
      "Iteration 9360 Training loss 0.009562334977090359 Validation loss 0.012562199495732784 Accuracy 0.8654999732971191\n",
      "Iteration 9370 Training loss 0.006653037387877703 Validation loss 0.012712299823760986 Accuracy 0.8644999861717224\n",
      "Iteration 9380 Training loss 0.008513599634170532 Validation loss 0.012175838463008404 Accuracy 0.8697999715805054\n",
      "Iteration 9390 Training loss 0.009616372175514698 Validation loss 0.01258455216884613 Accuracy 0.8659999966621399\n",
      "Iteration 9400 Training loss 0.007881101220846176 Validation loss 0.012351175770163536 Accuracy 0.8676999807357788\n",
      "Iteration 9410 Training loss 0.006724319886416197 Validation loss 0.012165488675236702 Accuracy 0.8693000078201294\n",
      "Iteration 9420 Training loss 0.009509717114269733 Validation loss 0.012390325777232647 Accuracy 0.8671000003814697\n",
      "Iteration 9430 Training loss 0.00876886211335659 Validation loss 0.012465916574001312 Accuracy 0.866100013256073\n",
      "Iteration 9440 Training loss 0.008140450343489647 Validation loss 0.012247134931385517 Accuracy 0.8691999912261963\n",
      "Iteration 9450 Training loss 0.007427147589623928 Validation loss 0.012141606770455837 Accuracy 0.8702999949455261\n",
      "Iteration 9460 Training loss 0.006894740276038647 Validation loss 0.012551093474030495 Accuracy 0.8659999966621399\n",
      "Iteration 9470 Training loss 0.008213953115046024 Validation loss 0.012069666758179665 Accuracy 0.8709999918937683\n",
      "Iteration 9480 Training loss 0.007604995276778936 Validation loss 0.012316868640482426 Accuracy 0.868399977684021\n",
      "Iteration 9490 Training loss 0.009078261442482471 Validation loss 0.012112122029066086 Accuracy 0.8705999851226807\n",
      "Iteration 9500 Training loss 0.00926830805838108 Validation loss 0.01221613772213459 Accuracy 0.8694999814033508\n",
      "Iteration 9510 Training loss 0.00670472951605916 Validation loss 0.012079250067472458 Accuracy 0.8705999851226807\n",
      "Iteration 9520 Training loss 0.007821558974683285 Validation loss 0.012228041887283325 Accuracy 0.8694000244140625\n",
      "Iteration 9530 Training loss 0.008697282522916794 Validation loss 0.01273463387042284 Accuracy 0.8636000156402588\n",
      "Iteration 9540 Training loss 0.009054464288055897 Validation loss 0.012519572861492634 Accuracy 0.8665000200271606\n",
      "Iteration 9550 Training loss 0.010712211951613426 Validation loss 0.012342585250735283 Accuracy 0.8669999837875366\n",
      "Iteration 9560 Training loss 0.007534487638622522 Validation loss 0.012105478905141354 Accuracy 0.8708999752998352\n",
      "Iteration 9570 Training loss 0.009109115228056908 Validation loss 0.012117013335227966 Accuracy 0.8705999851226807\n",
      "Iteration 9580 Training loss 0.00822040718048811 Validation loss 0.01243100967258215 Accuracy 0.866599977016449\n",
      "Iteration 9590 Training loss 0.006301156245172024 Validation loss 0.01241809967905283 Accuracy 0.8664000034332275\n",
      "Iteration 9600 Training loss 0.007425941061228514 Validation loss 0.012619808316230774 Accuracy 0.8640999794006348\n",
      "Iteration 9610 Training loss 0.008605611510574818 Validation loss 0.012014745734632015 Accuracy 0.8722000122070312\n",
      "Iteration 9620 Training loss 0.007957477122545242 Validation loss 0.01237453892827034 Accuracy 0.8687000274658203\n",
      "Iteration 9630 Training loss 0.009848223999142647 Validation loss 0.011940261349081993 Accuracy 0.8718000054359436\n",
      "Iteration 9640 Training loss 0.008238867856562138 Validation loss 0.012052260339260101 Accuracy 0.8712999820709229\n",
      "Iteration 9650 Training loss 0.005648395512253046 Validation loss 0.012051040306687355 Accuracy 0.8704000115394592\n",
      "Iteration 9660 Training loss 0.010166339576244354 Validation loss 0.012515447102487087 Accuracy 0.8658999800682068\n",
      "Iteration 9670 Training loss 0.005376977380365133 Validation loss 0.01247299462556839 Accuracy 0.8673999905586243\n",
      "Iteration 9680 Training loss 0.008830999955534935 Validation loss 0.012394038029015064 Accuracy 0.8669000267982483\n",
      "Iteration 9690 Training loss 0.008438372984528542 Validation loss 0.012172906659543514 Accuracy 0.8687000274658203\n",
      "Iteration 9700 Training loss 0.008752132765948772 Validation loss 0.012266009114682674 Accuracy 0.8687999844551086\n",
      "Iteration 9710 Training loss 0.009567027911543846 Validation loss 0.012349475175142288 Accuracy 0.868399977684021\n",
      "Iteration 9720 Training loss 0.0066091991029679775 Validation loss 0.011933433823287487 Accuracy 0.8727999925613403\n",
      "Iteration 9730 Training loss 0.009981867857277393 Validation loss 0.012910451740026474 Accuracy 0.8622000217437744\n",
      "Iteration 9740 Training loss 0.008915090933442116 Validation loss 0.012732872739434242 Accuracy 0.8651999831199646\n",
      "Iteration 9750 Training loss 0.008970357477664948 Validation loss 0.012987532652914524 Accuracy 0.861299991607666\n",
      "Iteration 9760 Training loss 0.007329319603741169 Validation loss 0.012212727218866348 Accuracy 0.8690000176429749\n",
      "Iteration 9770 Training loss 0.00996759906411171 Validation loss 0.01374509185552597 Accuracy 0.8533999919891357\n",
      "Iteration 9780 Training loss 0.007336576469242573 Validation loss 0.012606528587639332 Accuracy 0.8648999929428101\n",
      "Iteration 9790 Training loss 0.006218135356903076 Validation loss 0.012403352186083794 Accuracy 0.8679999709129333\n",
      "Iteration 9800 Training loss 0.007637078408151865 Validation loss 0.012219239957630634 Accuracy 0.868399977684021\n",
      "Iteration 9810 Training loss 0.009795786812901497 Validation loss 0.01207292452454567 Accuracy 0.8705999851226807\n",
      "Iteration 9820 Training loss 0.007371468935161829 Validation loss 0.012277466244995594 Accuracy 0.8691999912261963\n",
      "Iteration 9830 Training loss 0.009820850566029549 Validation loss 0.012083802372217178 Accuracy 0.8712999820709229\n",
      "Iteration 9840 Training loss 0.0075777131132781506 Validation loss 0.012199669145047665 Accuracy 0.8687000274658203\n",
      "Iteration 9850 Training loss 0.008144848048686981 Validation loss 0.012327780947089195 Accuracy 0.8690000176429749\n",
      "Iteration 9860 Training loss 0.007813920266926289 Validation loss 0.011991149745881557 Accuracy 0.8718000054359436\n",
      "Iteration 9870 Training loss 0.00897532794624567 Validation loss 0.01249195821583271 Accuracy 0.8669999837875366\n",
      "Iteration 9880 Training loss 0.0053911046124994755 Validation loss 0.01235603354871273 Accuracy 0.8669999837875366\n",
      "Iteration 9890 Training loss 0.007702227681875229 Validation loss 0.012127228081226349 Accuracy 0.8698999881744385\n",
      "Iteration 9900 Training loss 0.008697018958628178 Validation loss 0.012219301424920559 Accuracy 0.8695999979972839\n",
      "Iteration 9910 Training loss 0.007883470505475998 Validation loss 0.012156989425420761 Accuracy 0.8702999949455261\n",
      "Iteration 9920 Training loss 0.008793908171355724 Validation loss 0.012210422195494175 Accuracy 0.8691999912261963\n",
      "Iteration 9930 Training loss 0.008263031020760536 Validation loss 0.012273943983018398 Accuracy 0.8690000176429749\n",
      "Iteration 9940 Training loss 0.007827472873032093 Validation loss 0.01210433803498745 Accuracy 0.870199978351593\n",
      "Iteration 9950 Training loss 0.007764053996652365 Validation loss 0.012210754677653313 Accuracy 0.8697999715805054\n",
      "Iteration 9960 Training loss 0.010507398284971714 Validation loss 0.013235279358923435 Accuracy 0.8578000068664551\n",
      "Iteration 9970 Training loss 0.009106011129915714 Validation loss 0.012354208156466484 Accuracy 0.8672000169754028\n",
      "Iteration 9980 Training loss 0.006177222356200218 Validation loss 0.011999242939054966 Accuracy 0.8726000189781189\n",
      "Iteration 9990 Training loss 0.009121577255427837 Validation loss 0.012517890892922878 Accuracy 0.8669000267982483\n",
      "Iteration 10000 Training loss 0.00832213182002306 Validation loss 0.012290619313716888 Accuracy 0.8672999739646912\n",
      "Iteration 10010 Training loss 0.00654942262917757 Validation loss 0.012073409743607044 Accuracy 0.8712000250816345\n",
      "Iteration 10020 Training loss 0.008273487910628319 Validation loss 0.011918826028704643 Accuracy 0.8723999857902527\n",
      "Iteration 10030 Training loss 0.008681404404342175 Validation loss 0.011956342495977879 Accuracy 0.8716999888420105\n",
      "Iteration 10040 Training loss 0.008452700451016426 Validation loss 0.012529156170785427 Accuracy 0.866599977016449\n",
      "Iteration 10050 Training loss 0.006939954124391079 Validation loss 0.012239152565598488 Accuracy 0.8691999912261963\n",
      "Iteration 10060 Training loss 0.008564508520066738 Validation loss 0.012176989577710629 Accuracy 0.8702999949455261\n",
      "Iteration 10070 Training loss 0.008345785550773144 Validation loss 0.012413479387760162 Accuracy 0.8673999905586243\n",
      "Iteration 10080 Training loss 0.009217293933033943 Validation loss 0.01212692353874445 Accuracy 0.8712000250816345\n",
      "Iteration 10090 Training loss 0.007322126533836126 Validation loss 0.011786896735429764 Accuracy 0.8729000091552734\n",
      "Iteration 10100 Training loss 0.008049961179494858 Validation loss 0.011877528391778469 Accuracy 0.8733000159263611\n",
      "Iteration 10110 Training loss 0.007186552044004202 Validation loss 0.012036229483783245 Accuracy 0.8705000281333923\n",
      "Iteration 10120 Training loss 0.007729944307357073 Validation loss 0.012403186410665512 Accuracy 0.8658999800682068\n",
      "Iteration 10130 Training loss 0.009077794849872589 Validation loss 0.01240390632301569 Accuracy 0.868399977684021\n",
      "Iteration 10140 Training loss 0.009501230902969837 Validation loss 0.012248403392732143 Accuracy 0.868399977684021\n",
      "Iteration 10150 Training loss 0.008180437609553337 Validation loss 0.012267297133803368 Accuracy 0.8682000041007996\n",
      "Iteration 10160 Training loss 0.008936299942433834 Validation loss 0.012138267047703266 Accuracy 0.8698999881744385\n",
      "Iteration 10170 Training loss 0.006640119478106499 Validation loss 0.012199071235954762 Accuracy 0.8687999844551086\n",
      "Iteration 10180 Training loss 0.008570156060159206 Validation loss 0.011863837949931622 Accuracy 0.8711000084877014\n",
      "Iteration 10190 Training loss 0.007789301685988903 Validation loss 0.011563698761165142 Accuracy 0.8755999803543091\n",
      "Iteration 10200 Training loss 0.007099025882780552 Validation loss 0.012032519094645977 Accuracy 0.8707000017166138\n",
      "Iteration 10210 Training loss 0.0065755220130085945 Validation loss 0.012325156480073929 Accuracy 0.8669999837875366\n",
      "Iteration 10220 Training loss 0.004517671186476946 Validation loss 0.012182390317320824 Accuracy 0.8690999746322632\n",
      "Iteration 10230 Training loss 0.007271268405020237 Validation loss 0.012408454902470112 Accuracy 0.8676999807357788\n",
      "Iteration 10240 Training loss 0.008061612956225872 Validation loss 0.011883427388966084 Accuracy 0.8726000189781189\n",
      "Iteration 10250 Training loss 0.008145712316036224 Validation loss 0.011757939122617245 Accuracy 0.8727999925613403\n",
      "Iteration 10260 Training loss 0.007906910963356495 Validation loss 0.012096107937395573 Accuracy 0.8695999979972839\n",
      "Iteration 10270 Training loss 0.008512960746884346 Validation loss 0.011836745776236057 Accuracy 0.8722000122070312\n",
      "Iteration 10280 Training loss 0.007206321228295565 Validation loss 0.012150456197559834 Accuracy 0.8689000010490417\n",
      "Iteration 10290 Training loss 0.00922989472746849 Validation loss 0.01199809554964304 Accuracy 0.871399998664856\n",
      "Iteration 10300 Training loss 0.007325768005102873 Validation loss 0.012015356682240963 Accuracy 0.8715000152587891\n",
      "Iteration 10310 Training loss 0.008116370067000389 Validation loss 0.012877421453595161 Accuracy 0.8615999817848206\n",
      "Iteration 10320 Training loss 0.00820681918412447 Validation loss 0.012206040322780609 Accuracy 0.8680999875068665\n",
      "Iteration 10330 Training loss 0.008698798716068268 Validation loss 0.01223741564899683 Accuracy 0.8683000206947327\n",
      "Iteration 10340 Training loss 0.008329009637236595 Validation loss 0.012104764580726624 Accuracy 0.870199978351593\n",
      "Iteration 10350 Training loss 0.00871309731155634 Validation loss 0.01196818146854639 Accuracy 0.8716999888420105\n",
      "Iteration 10360 Training loss 0.007026742212474346 Validation loss 0.011950705200433731 Accuracy 0.8709999918937683\n",
      "Iteration 10370 Training loss 0.009140809997916222 Validation loss 0.012766530737280846 Accuracy 0.8633000254631042\n",
      "Iteration 10380 Training loss 0.006189438048750162 Validation loss 0.01204011682420969 Accuracy 0.8705000281333923\n",
      "Iteration 10390 Training loss 0.006028512958437204 Validation loss 0.01199160423129797 Accuracy 0.8716999888420105\n",
      "Iteration 10400 Training loss 0.008846600539982319 Validation loss 0.012002230621874332 Accuracy 0.871399998664856\n",
      "Iteration 10410 Training loss 0.007206820882856846 Validation loss 0.01261704321950674 Accuracy 0.864799976348877\n",
      "Iteration 10420 Training loss 0.007690505124628544 Validation loss 0.012580259703099728 Accuracy 0.8647000193595886\n",
      "Iteration 10430 Training loss 0.00819926243275404 Validation loss 0.012021899223327637 Accuracy 0.8708000183105469\n",
      "Iteration 10440 Training loss 0.006727275438606739 Validation loss 0.011945185251533985 Accuracy 0.8723000288009644\n",
      "Iteration 10450 Training loss 0.009715957567095757 Validation loss 0.012240788899362087 Accuracy 0.8677999973297119\n",
      "Iteration 10460 Training loss 0.0068265292793512344 Validation loss 0.011885843239724636 Accuracy 0.8719000220298767\n",
      "Iteration 10470 Training loss 0.008529718965291977 Validation loss 0.012529697269201279 Accuracy 0.866100013256073\n",
      "Iteration 10480 Training loss 0.010768295265734196 Validation loss 0.012435552664101124 Accuracy 0.8668000102043152\n",
      "Iteration 10490 Training loss 0.008680111728608608 Validation loss 0.012020030058920383 Accuracy 0.8707000017166138\n",
      "Iteration 10500 Training loss 0.007016174960881472 Validation loss 0.012088592164218426 Accuracy 0.8708000183105469\n",
      "Iteration 10510 Training loss 0.005859812255948782 Validation loss 0.011964516714215279 Accuracy 0.8712999820709229\n",
      "Iteration 10520 Training loss 0.007227547466754913 Validation loss 0.011949238367378712 Accuracy 0.8708000183105469\n",
      "Iteration 10530 Training loss 0.004749331623315811 Validation loss 0.01179154496639967 Accuracy 0.8736000061035156\n",
      "Iteration 10540 Training loss 0.006634127348661423 Validation loss 0.012232034467160702 Accuracy 0.8683000206947327\n",
      "Iteration 10550 Training loss 0.006315026432275772 Validation loss 0.012120739556849003 Accuracy 0.8708999752998352\n",
      "Iteration 10560 Training loss 0.00690907659009099 Validation loss 0.01201553177088499 Accuracy 0.8718000054359436\n",
      "Iteration 10570 Training loss 0.00655076140537858 Validation loss 0.01236952655017376 Accuracy 0.8679999709129333\n",
      "Iteration 10580 Training loss 0.008608327247202396 Validation loss 0.012545512989163399 Accuracy 0.8652999997138977\n",
      "Iteration 10590 Training loss 0.005171901546418667 Validation loss 0.011972310021519661 Accuracy 0.8712000250816345\n",
      "Iteration 10600 Training loss 0.008699449710547924 Validation loss 0.012049076147377491 Accuracy 0.8707000017166138\n",
      "Iteration 10610 Training loss 0.007610836066305637 Validation loss 0.01183656882494688 Accuracy 0.8736000061035156\n",
      "Iteration 10620 Training loss 0.006551121361553669 Validation loss 0.01181504875421524 Accuracy 0.8727999925613403\n",
      "Iteration 10630 Training loss 0.007309842389076948 Validation loss 0.011885217390954494 Accuracy 0.8712000250816345\n",
      "Iteration 10640 Training loss 0.007292268797755241 Validation loss 0.012017002329230309 Accuracy 0.8712000250816345\n",
      "Iteration 10650 Training loss 0.007048505824059248 Validation loss 0.011930329725146294 Accuracy 0.8716999888420105\n",
      "Iteration 10660 Training loss 0.00790394190698862 Validation loss 0.011971068568527699 Accuracy 0.8711000084877014\n",
      "Iteration 10670 Training loss 0.007034835405647755 Validation loss 0.012028016149997711 Accuracy 0.8709999918937683\n",
      "Iteration 10680 Training loss 0.006116275675594807 Validation loss 0.011904408223927021 Accuracy 0.8723000288009644\n",
      "Iteration 10690 Training loss 0.008185415528714657 Validation loss 0.012302507646381855 Accuracy 0.8689000010490417\n",
      "Iteration 10700 Training loss 0.007102860603481531 Validation loss 0.012311172671616077 Accuracy 0.8682000041007996\n",
      "Iteration 10710 Training loss 0.007778366096317768 Validation loss 0.011879485100507736 Accuracy 0.8711000084877014\n",
      "Iteration 10720 Training loss 0.00893894862383604 Validation loss 0.012124931439757347 Accuracy 0.870199978351593\n",
      "Iteration 10730 Training loss 0.00801155623048544 Validation loss 0.011895474046468735 Accuracy 0.8726999759674072\n",
      "Iteration 10740 Training loss 0.008721765130758286 Validation loss 0.01191424299031496 Accuracy 0.8718000054359436\n",
      "Iteration 10750 Training loss 0.007406735792756081 Validation loss 0.011968949809670448 Accuracy 0.8705999851226807\n",
      "Iteration 10760 Training loss 0.006026136223226786 Validation loss 0.012016057968139648 Accuracy 0.8708000183105469\n",
      "Iteration 10770 Training loss 0.005636132787913084 Validation loss 0.012155967764556408 Accuracy 0.8690999746322632\n",
      "Iteration 10780 Training loss 0.006458526011556387 Validation loss 0.012228171341121197 Accuracy 0.8690000176429749\n",
      "Iteration 10790 Training loss 0.0076265581883490086 Validation loss 0.01258119661360979 Accuracy 0.8651999831199646\n",
      "Iteration 10800 Training loss 0.006873090285807848 Validation loss 0.012501383200287819 Accuracy 0.866599977016449\n",
      "Iteration 10810 Training loss 0.007501078303903341 Validation loss 0.011928731575608253 Accuracy 0.8705999851226807\n",
      "Iteration 10820 Training loss 0.007905458100140095 Validation loss 0.011786974966526031 Accuracy 0.8729000091552734\n",
      "Iteration 10830 Training loss 0.007188612129539251 Validation loss 0.011796172708272934 Accuracy 0.8729000091552734\n",
      "Iteration 10840 Training loss 0.009006759151816368 Validation loss 0.012000927701592445 Accuracy 0.8712000250816345\n",
      "Iteration 10850 Training loss 0.006728701293468475 Validation loss 0.011872988194227219 Accuracy 0.871399998664856\n",
      "Iteration 10860 Training loss 0.008641302585601807 Validation loss 0.011763094924390316 Accuracy 0.8733999729156494\n",
      "Iteration 10870 Training loss 0.007189301773905754 Validation loss 0.011811438947916031 Accuracy 0.8730000257492065\n",
      "Iteration 10880 Training loss 0.0070059942081570625 Validation loss 0.011778178624808788 Accuracy 0.8736000061035156\n",
      "Iteration 10890 Training loss 0.007985423319041729 Validation loss 0.011786669492721558 Accuracy 0.873199999332428\n",
      "Iteration 10900 Training loss 0.00803198479115963 Validation loss 0.012111046351492405 Accuracy 0.8700000047683716\n",
      "Iteration 10910 Training loss 0.008021053858101368 Validation loss 0.011925756931304932 Accuracy 0.8718000054359436\n",
      "Iteration 10920 Training loss 0.00816968735307455 Validation loss 0.011734023690223694 Accuracy 0.8743000030517578\n",
      "Iteration 10930 Training loss 0.007183792069554329 Validation loss 0.011926542036235332 Accuracy 0.8736000061035156\n",
      "Iteration 10940 Training loss 0.008619356900453568 Validation loss 0.012164942920207977 Accuracy 0.8708000183105469\n",
      "Iteration 10950 Training loss 0.010173295624554157 Validation loss 0.011769460514187813 Accuracy 0.8733000159263611\n",
      "Iteration 10960 Training loss 0.00870439875870943 Validation loss 0.01189196016639471 Accuracy 0.8722000122070312\n",
      "Iteration 10970 Training loss 0.006396830081939697 Validation loss 0.011951318010687828 Accuracy 0.8723999857902527\n",
      "Iteration 10980 Training loss 0.007757423911243677 Validation loss 0.011960787698626518 Accuracy 0.8725000023841858\n",
      "Iteration 10990 Training loss 0.0065391166135668755 Validation loss 0.011869586072862148 Accuracy 0.8729000091552734\n",
      "Iteration 11000 Training loss 0.007150371093302965 Validation loss 0.01172612328082323 Accuracy 0.8743000030517578\n",
      "Iteration 11010 Training loss 0.006754979491233826 Validation loss 0.012104949913918972 Accuracy 0.8693000078201294\n",
      "Iteration 11020 Training loss 0.007684782147407532 Validation loss 0.011884806677699089 Accuracy 0.8730000257492065\n",
      "Iteration 11030 Training loss 0.006399039179086685 Validation loss 0.012183479964733124 Accuracy 0.8693000078201294\n",
      "Iteration 11040 Training loss 0.008042844943702221 Validation loss 0.01227369625121355 Accuracy 0.867900013923645\n",
      "Iteration 11050 Training loss 0.00857973750680685 Validation loss 0.011961743235588074 Accuracy 0.8716999888420105\n",
      "Iteration 11060 Training loss 0.007562684360891581 Validation loss 0.01186610758304596 Accuracy 0.8734999895095825\n",
      "Iteration 11070 Training loss 0.005003851372748613 Validation loss 0.011719699949026108 Accuracy 0.8743000030517578\n",
      "Iteration 11080 Training loss 0.008424390107393265 Validation loss 0.011922077275812626 Accuracy 0.8723000288009644\n",
      "Iteration 11090 Training loss 0.00637033162638545 Validation loss 0.011741186492145061 Accuracy 0.8747000098228455\n",
      "Iteration 11100 Training loss 0.008275487460196018 Validation loss 0.011703554540872574 Accuracy 0.8748000264167786\n",
      "Iteration 11110 Training loss 0.008218876086175442 Validation loss 0.012644954025745392 Accuracy 0.8637999892234802\n",
      "Iteration 11120 Training loss 0.007756268605589867 Validation loss 0.011681356467306614 Accuracy 0.8740000128746033\n",
      "Iteration 11130 Training loss 0.007161399815231562 Validation loss 0.012385092675685883 Accuracy 0.8669999837875366\n",
      "Iteration 11140 Training loss 0.0058511109091341496 Validation loss 0.011658813804388046 Accuracy 0.8755000233650208\n",
      "Iteration 11150 Training loss 0.007318573538213968 Validation loss 0.011887868866324425 Accuracy 0.871999979019165\n",
      "Iteration 11160 Training loss 0.007105056196451187 Validation loss 0.011670518666505814 Accuracy 0.8740000128746033\n",
      "Iteration 11170 Training loss 0.005060649011284113 Validation loss 0.01193884015083313 Accuracy 0.8723999857902527\n",
      "Iteration 11180 Training loss 0.008824030868709087 Validation loss 0.012031146325170994 Accuracy 0.871399998664856\n",
      "Iteration 11190 Training loss 0.008102482184767723 Validation loss 0.012512990273535252 Accuracy 0.8654000163078308\n",
      "Iteration 11200 Training loss 0.007735572289675474 Validation loss 0.012016236782073975 Accuracy 0.8690999746322632\n",
      "Iteration 11210 Training loss 0.006091863848268986 Validation loss 0.012165872380137444 Accuracy 0.8687999844551086\n",
      "Iteration 11220 Training loss 0.006712216883897781 Validation loss 0.012064575217664242 Accuracy 0.8711000084877014\n",
      "Iteration 11230 Training loss 0.006964510772377253 Validation loss 0.01179595198482275 Accuracy 0.8734999895095825\n",
      "Iteration 11240 Training loss 0.006253290921449661 Validation loss 0.011628968641161919 Accuracy 0.8755000233650208\n",
      "Iteration 11250 Training loss 0.007588865701109171 Validation loss 0.011799739673733711 Accuracy 0.8725000023841858\n",
      "Iteration 11260 Training loss 0.004309314303100109 Validation loss 0.011876284144818783 Accuracy 0.871999979019165\n",
      "Iteration 11270 Training loss 0.0054120393469929695 Validation loss 0.011857347562909126 Accuracy 0.8716999888420105\n",
      "Iteration 11280 Training loss 0.007846929132938385 Validation loss 0.011955839581787586 Accuracy 0.8711000084877014\n",
      "Iteration 11290 Training loss 0.007469486445188522 Validation loss 0.012004992924630642 Accuracy 0.8712999820709229\n",
      "Iteration 11300 Training loss 0.006934275384992361 Validation loss 0.011846724897623062 Accuracy 0.8725000023841858\n",
      "Iteration 11310 Training loss 0.007660966366529465 Validation loss 0.011904372833669186 Accuracy 0.8719000220298767\n",
      "Iteration 11320 Training loss 0.00612389761954546 Validation loss 0.011793915182352066 Accuracy 0.8723000288009644\n",
      "Iteration 11330 Training loss 0.007657113950699568 Validation loss 0.012182849459350109 Accuracy 0.8689000010490417\n",
      "Iteration 11340 Training loss 0.007965706288814545 Validation loss 0.011917469091713428 Accuracy 0.8708999752998352\n",
      "Iteration 11350 Training loss 0.008179007098078728 Validation loss 0.011922082863748074 Accuracy 0.8715999722480774\n",
      "Iteration 11360 Training loss 0.008270103484392166 Validation loss 0.012375213205814362 Accuracy 0.8672000169754028\n",
      "Iteration 11370 Training loss 0.00891593936830759 Validation loss 0.011936897411942482 Accuracy 0.8705999851226807\n",
      "Iteration 11380 Training loss 0.006937747821211815 Validation loss 0.011930719949305058 Accuracy 0.8709999918937683\n",
      "Iteration 11390 Training loss 0.006698850076645613 Validation loss 0.011446766555309296 Accuracy 0.8755999803543091\n",
      "Iteration 11400 Training loss 0.0069206212647259235 Validation loss 0.011456717737019062 Accuracy 0.8763999938964844\n",
      "Iteration 11410 Training loss 0.006809389218688011 Validation loss 0.011729560792446136 Accuracy 0.8741999864578247\n",
      "Iteration 11420 Training loss 0.006915602367371321 Validation loss 0.011755666695535183 Accuracy 0.8723000288009644\n",
      "Iteration 11430 Training loss 0.006219001021236181 Validation loss 0.011685686185956001 Accuracy 0.873199999332428\n",
      "Iteration 11440 Training loss 0.007519330829381943 Validation loss 0.011897120624780655 Accuracy 0.8722000122070312\n",
      "Iteration 11450 Training loss 0.008563997223973274 Validation loss 0.012334831990301609 Accuracy 0.866599977016449\n",
      "Iteration 11460 Training loss 0.007509306538850069 Validation loss 0.01272557582706213 Accuracy 0.8622999787330627\n",
      "Iteration 11470 Training loss 0.005871293600648642 Validation loss 0.012072930112481117 Accuracy 0.8694000244140625\n",
      "Iteration 11480 Training loss 0.006206914316862822 Validation loss 0.012148317880928516 Accuracy 0.8684999942779541\n",
      "Iteration 11490 Training loss 0.007006031461060047 Validation loss 0.01170493382960558 Accuracy 0.8751000165939331\n",
      "Iteration 11500 Training loss 0.007085885386914015 Validation loss 0.011804556474089622 Accuracy 0.8715999722480774\n",
      "Iteration 11510 Training loss 0.007521449588239193 Validation loss 0.01194204855710268 Accuracy 0.8715000152587891\n",
      "Iteration 11520 Training loss 0.007787697948515415 Validation loss 0.012105641886591911 Accuracy 0.8693000078201294\n",
      "Iteration 11530 Training loss 0.00653447350487113 Validation loss 0.011827528476715088 Accuracy 0.8730999827384949\n",
      "Iteration 11540 Training loss 0.007235201075673103 Validation loss 0.011701826937496662 Accuracy 0.873199999332428\n",
      "Iteration 11550 Training loss 0.008095704950392246 Validation loss 0.012123498134315014 Accuracy 0.8683000206947327\n",
      "Iteration 11560 Training loss 0.006083581130951643 Validation loss 0.011960302479565144 Accuracy 0.8712000250816345\n",
      "Iteration 11570 Training loss 0.006528760306537151 Validation loss 0.0117149343714118 Accuracy 0.8733999729156494\n",
      "Iteration 11580 Training loss 0.005073881708085537 Validation loss 0.011663618497550488 Accuracy 0.8737999796867371\n",
      "Iteration 11590 Training loss 0.007664721924811602 Validation loss 0.011637506075203419 Accuracy 0.8748999834060669\n",
      "Iteration 11600 Training loss 0.006947621237486601 Validation loss 0.011664973571896553 Accuracy 0.8737000226974487\n",
      "Iteration 11610 Training loss 0.007744109258055687 Validation loss 0.011758951470255852 Accuracy 0.8730999827384949\n",
      "Iteration 11620 Training loss 0.007172934710979462 Validation loss 0.01221238449215889 Accuracy 0.8679999709129333\n",
      "Iteration 11630 Training loss 0.0076338062062859535 Validation loss 0.011712290346622467 Accuracy 0.8727999925613403\n",
      "Iteration 11640 Training loss 0.007905375212430954 Validation loss 0.01159325148910284 Accuracy 0.8748999834060669\n",
      "Iteration 11650 Training loss 0.007569448556751013 Validation loss 0.01191914826631546 Accuracy 0.8716999888420105\n",
      "Iteration 11660 Training loss 0.007337114308029413 Validation loss 0.011840314604341984 Accuracy 0.8715999722480774\n",
      "Iteration 11670 Training loss 0.004979233723133802 Validation loss 0.011453584767878056 Accuracy 0.8758000135421753\n",
      "Iteration 11680 Training loss 0.006933861877769232 Validation loss 0.011603849940001965 Accuracy 0.8748000264167786\n",
      "Iteration 11690 Training loss 0.005827653221786022 Validation loss 0.011829485185444355 Accuracy 0.8705000281333923\n",
      "Iteration 11700 Training loss 0.006525889504700899 Validation loss 0.011725915595889091 Accuracy 0.8745999932289124\n",
      "Iteration 11710 Training loss 0.007246299646794796 Validation loss 0.011595969088375568 Accuracy 0.8744999766349792\n",
      "Iteration 11720 Training loss 0.008234248496592045 Validation loss 0.011638105846941471 Accuracy 0.8733000159263611\n",
      "Iteration 11730 Training loss 0.006183401681482792 Validation loss 0.01153149176388979 Accuracy 0.8748000264167786\n",
      "Iteration 11740 Training loss 0.008515463210642338 Validation loss 0.011862917803227901 Accuracy 0.8718000054359436\n",
      "Iteration 11750 Training loss 0.00557744549587369 Validation loss 0.011818945407867432 Accuracy 0.8716999888420105\n",
      "Iteration 11760 Training loss 0.008519787341356277 Validation loss 0.013029832392930984 Accuracy 0.8593000173568726\n",
      "Iteration 11770 Training loss 0.006692576687783003 Validation loss 0.011850912123918533 Accuracy 0.8711000084877014\n",
      "Iteration 11780 Training loss 0.005766076501458883 Validation loss 0.011666069738566875 Accuracy 0.8740000128746033\n",
      "Iteration 11790 Training loss 0.00819245632737875 Validation loss 0.011572450399398804 Accuracy 0.8758000135421753\n",
      "Iteration 11800 Training loss 0.005664131138473749 Validation loss 0.011607258580625057 Accuracy 0.8748999834060669\n",
      "Iteration 11810 Training loss 0.007678074296563864 Validation loss 0.012180709280073643 Accuracy 0.8694999814033508\n",
      "Iteration 11820 Training loss 0.007632887456566095 Validation loss 0.011999384500086308 Accuracy 0.8704000115394592\n",
      "Iteration 11830 Training loss 0.007991118356585503 Validation loss 0.01224454864859581 Accuracy 0.8676000237464905\n",
      "Iteration 11840 Training loss 0.0063797105103731155 Validation loss 0.012262629345059395 Accuracy 0.8675000071525574\n",
      "Iteration 11850 Training loss 0.008436243049800396 Validation loss 0.011445491574704647 Accuracy 0.876800000667572\n",
      "Iteration 11860 Training loss 0.006381671410053968 Validation loss 0.011977607384324074 Accuracy 0.8709999918937683\n",
      "Iteration 11870 Training loss 0.006519485265016556 Validation loss 0.011807942762970924 Accuracy 0.8701000213623047\n",
      "Iteration 11880 Training loss 0.008248802274465561 Validation loss 0.011957382783293724 Accuracy 0.8698999881744385\n",
      "Iteration 11890 Training loss 0.007059823255985975 Validation loss 0.011663616634905338 Accuracy 0.873199999332428\n",
      "Iteration 11900 Training loss 0.006185899954289198 Validation loss 0.011463694274425507 Accuracy 0.8758999705314636\n",
      "Iteration 11910 Training loss 0.00707244873046875 Validation loss 0.012080660089850426 Accuracy 0.8694999814033508\n",
      "Iteration 11920 Training loss 0.008943057619035244 Validation loss 0.01136819925159216 Accuracy 0.8769000172615051\n",
      "Iteration 11930 Training loss 0.007702464237809181 Validation loss 0.011865169741213322 Accuracy 0.871999979019165\n",
      "Iteration 11940 Training loss 0.00757741741836071 Validation loss 0.011593758128583431 Accuracy 0.8754000067710876\n",
      "Iteration 11950 Training loss 0.005881760735064745 Validation loss 0.011459870263934135 Accuracy 0.8759999871253967\n",
      "Iteration 11960 Training loss 0.006158358883112669 Validation loss 0.011619358323514462 Accuracy 0.8745999932289124\n",
      "Iteration 11970 Training loss 0.007557359058409929 Validation loss 0.011864050291478634 Accuracy 0.8726000189781189\n",
      "Iteration 11980 Training loss 0.006656700745224953 Validation loss 0.01154475100338459 Accuracy 0.8744000196456909\n",
      "Iteration 11990 Training loss 0.007903915829956532 Validation loss 0.01188335195183754 Accuracy 0.8723999857902527\n",
      "Iteration 12000 Training loss 0.00842416100203991 Validation loss 0.011813369579613209 Accuracy 0.8722000122070312\n",
      "Iteration 12010 Training loss 0.005330265965312719 Validation loss 0.011797216720879078 Accuracy 0.8723000288009644\n",
      "Iteration 12020 Training loss 0.00653692614287138 Validation loss 0.01160019263625145 Accuracy 0.8748000264167786\n",
      "Iteration 12030 Training loss 0.007161691784858704 Validation loss 0.011784530244767666 Accuracy 0.8715999722480774\n",
      "Iteration 12040 Training loss 0.005589333362877369 Validation loss 0.011491918936371803 Accuracy 0.8765000104904175\n",
      "Iteration 12050 Training loss 0.005914092529565096 Validation loss 0.011726317927241325 Accuracy 0.8715000152587891\n",
      "Iteration 12060 Training loss 0.008345996029675007 Validation loss 0.011739024892449379 Accuracy 0.8726000189781189\n",
      "Iteration 12070 Training loss 0.007474238518625498 Validation loss 0.01201406866312027 Accuracy 0.8701000213623047\n",
      "Iteration 12080 Training loss 0.005445712246000767 Validation loss 0.011871607974171638 Accuracy 0.8705000281333923\n",
      "Iteration 12090 Training loss 0.00927625223994255 Validation loss 0.011583829298615456 Accuracy 0.8733000159263611\n",
      "Iteration 12100 Training loss 0.009519205428659916 Validation loss 0.011647170409560204 Accuracy 0.8741000294685364\n",
      "Iteration 12110 Training loss 0.005002378020435572 Validation loss 0.011630941182374954 Accuracy 0.8741000294685364\n",
      "Iteration 12120 Training loss 0.007856463082134724 Validation loss 0.011565710417926311 Accuracy 0.8755000233650208\n",
      "Iteration 12130 Training loss 0.006807003170251846 Validation loss 0.011558949947357178 Accuracy 0.8743000030517578\n",
      "Iteration 12140 Training loss 0.00813260767608881 Validation loss 0.011826937086880207 Accuracy 0.8723999857902527\n",
      "Iteration 12150 Training loss 0.010083590634167194 Validation loss 0.011631306260824203 Accuracy 0.8740000128746033\n",
      "Iteration 12160 Training loss 0.005405372474342585 Validation loss 0.011420277878642082 Accuracy 0.8755999803543091\n",
      "Iteration 12170 Training loss 0.006322418339550495 Validation loss 0.011913441121578217 Accuracy 0.8712000250816345\n",
      "Iteration 12180 Training loss 0.006438614334911108 Validation loss 0.011773178353905678 Accuracy 0.8726000189781189\n",
      "Iteration 12190 Training loss 0.006509080994874239 Validation loss 0.012025698088109493 Accuracy 0.869700014591217\n",
      "Iteration 12200 Training loss 0.005967023316770792 Validation loss 0.011542396619915962 Accuracy 0.8747000098228455\n",
      "Iteration 12210 Training loss 0.006032140925526619 Validation loss 0.011526765301823616 Accuracy 0.8748999834060669\n",
      "Iteration 12220 Training loss 0.0071439361199736595 Validation loss 0.011456440202891827 Accuracy 0.8758999705314636\n",
      "Iteration 12230 Training loss 0.005586252082139254 Validation loss 0.011523671448230743 Accuracy 0.8754000067710876\n",
      "Iteration 12240 Training loss 0.0067233191803097725 Validation loss 0.011399357579648495 Accuracy 0.876800000667572\n",
      "Iteration 12250 Training loss 0.00640157051384449 Validation loss 0.011848055757582188 Accuracy 0.8715000152587891\n",
      "Iteration 12260 Training loss 0.008451939560472965 Validation loss 0.01149110496044159 Accuracy 0.8747000098228455\n",
      "Iteration 12270 Training loss 0.007035023998469114 Validation loss 0.011611560359597206 Accuracy 0.8748999834060669\n",
      "Iteration 12280 Training loss 0.007037149276584387 Validation loss 0.011572878807783127 Accuracy 0.8730000257492065\n",
      "Iteration 12290 Training loss 0.005606306716799736 Validation loss 0.011813771910965443 Accuracy 0.8716999888420105\n",
      "Iteration 12300 Training loss 0.006810822989791632 Validation loss 0.011978491209447384 Accuracy 0.8691999912261963\n",
      "Iteration 12310 Training loss 0.00951929111033678 Validation loss 0.011915461160242558 Accuracy 0.8715000152587891\n",
      "Iteration 12320 Training loss 0.00714764092117548 Validation loss 0.01186478789895773 Accuracy 0.8726000189781189\n",
      "Iteration 12330 Training loss 0.00815482996404171 Validation loss 0.011874224059283733 Accuracy 0.8705999851226807\n",
      "Iteration 12340 Training loss 0.00794617086648941 Validation loss 0.011719944886863232 Accuracy 0.8725000023841858\n",
      "Iteration 12350 Training loss 0.005774610675871372 Validation loss 0.011965534649789333 Accuracy 0.8708000183105469\n",
      "Iteration 12360 Training loss 0.008743787184357643 Validation loss 0.011598327197134495 Accuracy 0.8745999932289124\n",
      "Iteration 12370 Training loss 0.0072023943066596985 Validation loss 0.011668318882584572 Accuracy 0.8736000061035156\n",
      "Iteration 12380 Training loss 0.007594219874590635 Validation loss 0.011603454127907753 Accuracy 0.8734999895095825\n",
      "Iteration 12390 Training loss 0.006554029881954193 Validation loss 0.011673306114971638 Accuracy 0.8726999759674072\n",
      "Iteration 12400 Training loss 0.006942282430827618 Validation loss 0.011525051668286324 Accuracy 0.8744000196456909\n",
      "Iteration 12410 Training loss 0.005739313084632158 Validation loss 0.011448071338236332 Accuracy 0.8754000067710876\n",
      "Iteration 12420 Training loss 0.006806597579270601 Validation loss 0.011718112975358963 Accuracy 0.8727999925613403\n",
      "Iteration 12430 Training loss 0.007026965264230967 Validation loss 0.011456790380179882 Accuracy 0.8754000067710876\n",
      "Iteration 12440 Training loss 0.006655197590589523 Validation loss 0.011974869295954704 Accuracy 0.8707000017166138\n",
      "Iteration 12450 Training loss 0.0055261654779314995 Validation loss 0.011395808309316635 Accuracy 0.8765000104904175\n",
      "Iteration 12460 Training loss 0.006345337722450495 Validation loss 0.01125747337937355 Accuracy 0.8773000240325928\n",
      "Iteration 12470 Training loss 0.007641149684786797 Validation loss 0.011390929110348225 Accuracy 0.8770999908447266\n",
      "Iteration 12480 Training loss 0.00608713086694479 Validation loss 0.011276574805378914 Accuracy 0.8773000240325928\n",
      "Iteration 12490 Training loss 0.006930129136890173 Validation loss 0.01145879179239273 Accuracy 0.8748999834060669\n",
      "Iteration 12500 Training loss 0.008659896440804005 Validation loss 0.01148305181413889 Accuracy 0.8758999705314636\n",
      "Iteration 12510 Training loss 0.0066997939720749855 Validation loss 0.011623990722000599 Accuracy 0.8741000294685364\n",
      "Iteration 12520 Training loss 0.006381717510521412 Validation loss 0.011819833889603615 Accuracy 0.8727999925613403\n",
      "Iteration 12530 Training loss 0.006009394768625498 Validation loss 0.011660145595669746 Accuracy 0.875\n",
      "Iteration 12540 Training loss 0.00585829745978117 Validation loss 0.01162942498922348 Accuracy 0.8743000030517578\n",
      "Iteration 12550 Training loss 0.005617073271423578 Validation loss 0.011622034944593906 Accuracy 0.8747000098228455\n",
      "Iteration 12560 Training loss 0.007545670960098505 Validation loss 0.011792724952101707 Accuracy 0.8716999888420105\n",
      "Iteration 12570 Training loss 0.009090142324566841 Validation loss 0.011546323075890541 Accuracy 0.8745999932289124\n",
      "Iteration 12580 Training loss 0.006027051247656345 Validation loss 0.01163721177726984 Accuracy 0.8744999766349792\n",
      "Iteration 12590 Training loss 0.005053194705396891 Validation loss 0.01147404219955206 Accuracy 0.875\n",
      "Iteration 12600 Training loss 0.007575012277811766 Validation loss 0.011983891949057579 Accuracy 0.8700000047683716\n",
      "Iteration 12610 Training loss 0.006137868855148554 Validation loss 0.011666125617921352 Accuracy 0.8743000030517578\n",
      "Iteration 12620 Training loss 0.006412623915821314 Validation loss 0.011375190690159798 Accuracy 0.8766000270843506\n",
      "Iteration 12630 Training loss 0.006801453419029713 Validation loss 0.011478823609650135 Accuracy 0.8754000067710876\n",
      "Iteration 12640 Training loss 0.00632622791454196 Validation loss 0.011469479650259018 Accuracy 0.875\n",
      "Iteration 12650 Training loss 0.006795787252485752 Validation loss 0.012171657755970955 Accuracy 0.8687000274658203\n",
      "Iteration 12660 Training loss 0.0071187433786690235 Validation loss 0.011956259608268738 Accuracy 0.8705000281333923\n",
      "Iteration 12670 Training loss 0.004412774927914143 Validation loss 0.01130363903939724 Accuracy 0.8770999908447266\n",
      "Iteration 12680 Training loss 0.0072761960327625275 Validation loss 0.01171260979026556 Accuracy 0.873199999332428\n",
      "Iteration 12690 Training loss 0.008548511192202568 Validation loss 0.01163832563906908 Accuracy 0.8733999729156494\n",
      "Iteration 12700 Training loss 0.006692938972264528 Validation loss 0.011489062570035458 Accuracy 0.8758999705314636\n",
      "Iteration 12710 Training loss 0.006439657416194677 Validation loss 0.011649291962385178 Accuracy 0.8733000159263611\n",
      "Iteration 12720 Training loss 0.00557544082403183 Validation loss 0.011786766350269318 Accuracy 0.871999979019165\n",
      "Iteration 12730 Training loss 0.006445018574595451 Validation loss 0.011947104707360268 Accuracy 0.8702999949455261\n",
      "Iteration 12740 Training loss 0.006814323831349611 Validation loss 0.011643167585134506 Accuracy 0.8741000294685364\n",
      "Iteration 12750 Training loss 0.007639197167009115 Validation loss 0.011540177278220654 Accuracy 0.8752999901771545\n",
      "Iteration 12760 Training loss 0.004750980995595455 Validation loss 0.01148837897926569 Accuracy 0.8759999871253967\n",
      "Iteration 12770 Training loss 0.00725601939484477 Validation loss 0.011540625244379044 Accuracy 0.8733999729156494\n",
      "Iteration 12780 Training loss 0.007047764025628567 Validation loss 0.011301069520413876 Accuracy 0.8769999742507935\n",
      "Iteration 12790 Training loss 0.004380644299089909 Validation loss 0.011549706570804119 Accuracy 0.8740000128746033\n",
      "Iteration 12800 Training loss 0.005716737825423479 Validation loss 0.011592697352170944 Accuracy 0.8741000294685364\n",
      "Iteration 12810 Training loss 0.0066894665360450745 Validation loss 0.011250861920416355 Accuracy 0.8777999877929688\n",
      "Iteration 12820 Training loss 0.0059756990522146225 Validation loss 0.011757171712815762 Accuracy 0.8712999820709229\n",
      "Iteration 12830 Training loss 0.006090165581554174 Validation loss 0.01132858544588089 Accuracy 0.8779000043869019\n",
      "Iteration 12840 Training loss 0.008317077532410622 Validation loss 0.011878598481416702 Accuracy 0.870199978351593\n",
      "Iteration 12850 Training loss 0.0049594249576330185 Validation loss 0.011434529908001423 Accuracy 0.8765000104904175\n",
      "Iteration 12860 Training loss 0.006614314392209053 Validation loss 0.01165795885026455 Accuracy 0.8741999864578247\n",
      "Iteration 12870 Training loss 0.006088728085160255 Validation loss 0.011711671017110348 Accuracy 0.8726999759674072\n",
      "Iteration 12880 Training loss 0.0049672494642436504 Validation loss 0.011511066928505898 Accuracy 0.8754000067710876\n",
      "Iteration 12890 Training loss 0.006351178977638483 Validation loss 0.01123195793479681 Accuracy 0.8780999779701233\n",
      "Iteration 12900 Training loss 0.006844347808510065 Validation loss 0.011539923027157784 Accuracy 0.875\n",
      "Iteration 12910 Training loss 0.0070984079502522945 Validation loss 0.011398603208363056 Accuracy 0.8762999773025513\n",
      "Iteration 12920 Training loss 0.007690394297242165 Validation loss 0.011732192710042 Accuracy 0.8726000189781189\n",
      "Iteration 12930 Training loss 0.005216136109083891 Validation loss 0.01153428852558136 Accuracy 0.8744000196456909\n",
      "Iteration 12940 Training loss 0.0059822723269462585 Validation loss 0.011486168019473553 Accuracy 0.8744000196456909\n",
      "Iteration 12950 Training loss 0.004846170544624329 Validation loss 0.011382543481886387 Accuracy 0.8755999803543091\n",
      "Iteration 12960 Training loss 0.006638368126004934 Validation loss 0.011585644446313381 Accuracy 0.8737999796867371\n",
      "Iteration 12970 Training loss 0.006790784653276205 Validation loss 0.01199492160230875 Accuracy 0.8695999979972839\n",
      "Iteration 12980 Training loss 0.008181286044418812 Validation loss 0.011301059275865555 Accuracy 0.8770999908447266\n",
      "Iteration 12990 Training loss 0.00769864721223712 Validation loss 0.011510815471410751 Accuracy 0.8748999834060669\n",
      "Iteration 13000 Training loss 0.00788392685353756 Validation loss 0.011417027562856674 Accuracy 0.8755999803543091\n",
      "Iteration 13010 Training loss 0.007158662658184767 Validation loss 0.011522912420332432 Accuracy 0.8748999834060669\n",
      "Iteration 13020 Training loss 0.007648962549865246 Validation loss 0.011442739516496658 Accuracy 0.8762999773025513\n",
      "Iteration 13030 Training loss 0.005922522861510515 Validation loss 0.011570563539862633 Accuracy 0.8736000061035156\n",
      "Iteration 13040 Training loss 0.007255342323333025 Validation loss 0.011721327900886536 Accuracy 0.8723999857902527\n",
      "Iteration 13050 Training loss 0.0077202110551297665 Validation loss 0.011701444163918495 Accuracy 0.8743000030517578\n",
      "Iteration 13060 Training loss 0.007530315313488245 Validation loss 0.012008659541606903 Accuracy 0.8697999715805054\n",
      "Iteration 13070 Training loss 0.00593872694298625 Validation loss 0.01134257111698389 Accuracy 0.8759999871253967\n",
      "Iteration 13080 Training loss 0.006380843929946423 Validation loss 0.011243904940783978 Accuracy 0.8787999749183655\n",
      "Iteration 13090 Training loss 0.003753009717911482 Validation loss 0.011591569520533085 Accuracy 0.8733000159263611\n",
      "Iteration 13100 Training loss 0.00682347547262907 Validation loss 0.011480116285383701 Accuracy 0.8737999796867371\n",
      "Iteration 13110 Training loss 0.006254591513425112 Validation loss 0.011264096945524216 Accuracy 0.8776000142097473\n",
      "Iteration 13120 Training loss 0.007684723939746618 Validation loss 0.0112442746758461 Accuracy 0.8769999742507935\n",
      "Iteration 13130 Training loss 0.00537506490945816 Validation loss 0.011457272805273533 Accuracy 0.8755999803543091\n",
      "Iteration 13140 Training loss 0.007018127478659153 Validation loss 0.012270879000425339 Accuracy 0.8675000071525574\n",
      "Iteration 13150 Training loss 0.005402370356023312 Validation loss 0.011319643817842007 Accuracy 0.8779000043869019\n",
      "Iteration 13160 Training loss 0.005246692802757025 Validation loss 0.011334839276969433 Accuracy 0.8761000037193298\n",
      "Iteration 13170 Training loss 0.0073289694264531136 Validation loss 0.011743937619030476 Accuracy 0.8729000091552734\n",
      "Iteration 13180 Training loss 0.005231363698840141 Validation loss 0.011492221616208553 Accuracy 0.8752999901771545\n",
      "Iteration 13190 Training loss 0.0077661145478487015 Validation loss 0.011313598603010178 Accuracy 0.8776000142097473\n",
      "Iteration 13200 Training loss 0.00792078860104084 Validation loss 0.011278790421783924 Accuracy 0.8788999915122986\n",
      "Iteration 13210 Training loss 0.006321214139461517 Validation loss 0.011900214478373528 Accuracy 0.8687999844551086\n",
      "Iteration 13220 Training loss 0.005414851475507021 Validation loss 0.011288147419691086 Accuracy 0.8773999810218811\n",
      "Iteration 13230 Training loss 0.005981845315545797 Validation loss 0.011434455402195454 Accuracy 0.8756999969482422\n",
      "Iteration 13240 Training loss 0.005957268178462982 Validation loss 0.011611130088567734 Accuracy 0.8744999766349792\n",
      "Iteration 13250 Training loss 0.005851703695952892 Validation loss 0.011451125144958496 Accuracy 0.8741000294685364\n",
      "Iteration 13260 Training loss 0.00437368405982852 Validation loss 0.01146891713142395 Accuracy 0.8756999969482422\n",
      "Iteration 13270 Training loss 0.005550066474825144 Validation loss 0.011255188845098019 Accuracy 0.8774999976158142\n",
      "Iteration 13280 Training loss 0.0060591502115130424 Validation loss 0.01140538603067398 Accuracy 0.8762000203132629\n",
      "Iteration 13290 Training loss 0.005446902476251125 Validation loss 0.011396653018891811 Accuracy 0.8756999969482422\n",
      "Iteration 13300 Training loss 0.005831949412822723 Validation loss 0.011892742477357388 Accuracy 0.8716999888420105\n",
      "Iteration 13310 Training loss 0.006500030402094126 Validation loss 0.01138247549533844 Accuracy 0.8761000037193298\n",
      "Iteration 13320 Training loss 0.007588087115436792 Validation loss 0.01132956426590681 Accuracy 0.8766000270843506\n",
      "Iteration 13330 Training loss 0.005387352779507637 Validation loss 0.011578365229070187 Accuracy 0.8751999735832214\n",
      "Iteration 13340 Training loss 0.0044808462262153625 Validation loss 0.011301043443381786 Accuracy 0.8776000142097473\n",
      "Iteration 13350 Training loss 0.006758551113307476 Validation loss 0.011588571593165398 Accuracy 0.8741999864578247\n",
      "Iteration 13360 Training loss 0.007355515845119953 Validation loss 0.012660456821322441 Accuracy 0.863099992275238\n",
      "Iteration 13370 Training loss 0.006703299004584551 Validation loss 0.011804889887571335 Accuracy 0.8715000152587891\n",
      "Iteration 13380 Training loss 0.007168994750827551 Validation loss 0.011350695975124836 Accuracy 0.8774999976158142\n",
      "Iteration 13390 Training loss 0.004817761946469545 Validation loss 0.011430390179157257 Accuracy 0.8770999908447266\n",
      "Iteration 13400 Training loss 0.005948333535343409 Validation loss 0.011498453095555305 Accuracy 0.8758000135421753\n",
      "Iteration 13410 Training loss 0.007786432281136513 Validation loss 0.011461698450148106 Accuracy 0.8766000270843506\n",
      "Iteration 13420 Training loss 0.006130765192210674 Validation loss 0.011291616596281528 Accuracy 0.8773999810218811\n",
      "Iteration 13430 Training loss 0.006174745038151741 Validation loss 0.011479374021291733 Accuracy 0.8752999901771545\n",
      "Iteration 13440 Training loss 0.0065580690279603004 Validation loss 0.011761010624468327 Accuracy 0.8722000122070312\n",
      "Iteration 13450 Training loss 0.005994015838950872 Validation loss 0.011622053571045399 Accuracy 0.8741000294685364\n",
      "Iteration 13460 Training loss 0.00766698969528079 Validation loss 0.011730830185115337 Accuracy 0.8737000226974487\n",
      "Iteration 13470 Training loss 0.007404868956655264 Validation loss 0.011813330464065075 Accuracy 0.8709999918937683\n",
      "Iteration 13480 Training loss 0.007632641587406397 Validation loss 0.011247063055634499 Accuracy 0.8780999779701233\n",
      "Iteration 13490 Training loss 0.00423984881490469 Validation loss 0.0113930469378829 Accuracy 0.8762999773025513\n",
      "Iteration 13500 Training loss 0.0064452881924808025 Validation loss 0.011277985759079456 Accuracy 0.8773000240325928\n",
      "Iteration 13510 Training loss 0.003933004103600979 Validation loss 0.011550177820026875 Accuracy 0.8736000061035156\n",
      "Iteration 13520 Training loss 0.005648867692798376 Validation loss 0.011618126183748245 Accuracy 0.871999979019165\n",
      "Iteration 13530 Training loss 0.006009801756590605 Validation loss 0.011610493063926697 Accuracy 0.8744000196456909\n",
      "Iteration 13540 Training loss 0.006056230049580336 Validation loss 0.01156782265752554 Accuracy 0.8745999932289124\n",
      "Iteration 13550 Training loss 0.005439573898911476 Validation loss 0.011624511331319809 Accuracy 0.8747000098228455\n",
      "Iteration 13560 Training loss 0.005758770741522312 Validation loss 0.011366980150341988 Accuracy 0.8762000203132629\n",
      "Iteration 13570 Training loss 0.006757287308573723 Validation loss 0.011402414180338383 Accuracy 0.8763999938964844\n",
      "Iteration 13580 Training loss 0.005757179111242294 Validation loss 0.011502046138048172 Accuracy 0.8751000165939331\n",
      "Iteration 13590 Training loss 0.006137563847005367 Validation loss 0.011374035850167274 Accuracy 0.8755999803543091\n",
      "Iteration 13600 Training loss 0.007883499376475811 Validation loss 0.011723596602678299 Accuracy 0.8738999962806702\n",
      "Iteration 13610 Training loss 0.0055696978233754635 Validation loss 0.011546372435986996 Accuracy 0.8741000294685364\n",
      "Iteration 13620 Training loss 0.0065341065637767315 Validation loss 0.011504904367029667 Accuracy 0.8751999735832214\n",
      "Iteration 13630 Training loss 0.005669294856488705 Validation loss 0.011402691714465618 Accuracy 0.8765000104904175\n",
      "Iteration 13640 Training loss 0.006883523426949978 Validation loss 0.011465278454124928 Accuracy 0.8740000128746033\n",
      "Iteration 13650 Training loss 0.00509449141100049 Validation loss 0.011290352791547775 Accuracy 0.8770999908447266\n",
      "Iteration 13660 Training loss 0.005739583168178797 Validation loss 0.011597303673624992 Accuracy 0.8737000226974487\n",
      "Iteration 13670 Training loss 0.0061276862397789955 Validation loss 0.01173923909664154 Accuracy 0.8729000091552734\n",
      "Iteration 13680 Training loss 0.006409639958292246 Validation loss 0.011597922071814537 Accuracy 0.8741000294685364\n",
      "Iteration 13690 Training loss 0.005000600125640631 Validation loss 0.011357194744050503 Accuracy 0.8756999969482422\n",
      "Iteration 13700 Training loss 0.006708203814923763 Validation loss 0.011429732665419579 Accuracy 0.8765000104904175\n",
      "Iteration 13710 Training loss 0.006900765933096409 Validation loss 0.011551468633115292 Accuracy 0.8755999803543091\n",
      "Iteration 13720 Training loss 0.006682033650577068 Validation loss 0.011510997079312801 Accuracy 0.8736000061035156\n",
      "Iteration 13730 Training loss 0.006328039336949587 Validation loss 0.011250435374677181 Accuracy 0.8772000074386597\n",
      "Iteration 13740 Training loss 0.0064448704943060875 Validation loss 0.011277363635599613 Accuracy 0.8765000104904175\n",
      "Iteration 13750 Training loss 0.005723175592720509 Validation loss 0.01118826400488615 Accuracy 0.8792999982833862\n",
      "Iteration 13760 Training loss 0.006990923546254635 Validation loss 0.011564194224774837 Accuracy 0.8741999864578247\n",
      "Iteration 13770 Training loss 0.007564904633909464 Validation loss 0.011532413773238659 Accuracy 0.8751000165939331\n",
      "Iteration 13780 Training loss 0.005690495949238539 Validation loss 0.01168366800993681 Accuracy 0.8723000288009644\n",
      "Iteration 13790 Training loss 0.007162795402109623 Validation loss 0.012085738591849804 Accuracy 0.8690000176429749\n",
      "Iteration 13800 Training loss 0.004672898445278406 Validation loss 0.011529662646353245 Accuracy 0.8748000264167786\n",
      "Iteration 13810 Training loss 0.006238168105483055 Validation loss 0.011269770562648773 Accuracy 0.8787000179290771\n",
      "Iteration 13820 Training loss 0.005181886721402407 Validation loss 0.011599843390285969 Accuracy 0.8726000189781189\n",
      "Iteration 13830 Training loss 0.005143364891409874 Validation loss 0.011381641961634159 Accuracy 0.8755999803543091\n",
      "Iteration 13840 Training loss 0.005618011578917503 Validation loss 0.011258122511208057 Accuracy 0.8761000037193298\n",
      "Iteration 13850 Training loss 0.004888760857284069 Validation loss 0.011274022050201893 Accuracy 0.8769000172615051\n",
      "Iteration 13860 Training loss 0.007452953141182661 Validation loss 0.011423934251070023 Accuracy 0.8758000135421753\n",
      "Iteration 13870 Training loss 0.006530210375785828 Validation loss 0.01148540060967207 Accuracy 0.8745999932289124\n",
      "Iteration 13880 Training loss 0.006509965285658836 Validation loss 0.011309660971164703 Accuracy 0.8763999938964844\n",
      "Iteration 13890 Training loss 0.007098555099219084 Validation loss 0.011785327456891537 Accuracy 0.8701000213623047\n",
      "Iteration 13900 Training loss 0.00734025239944458 Validation loss 0.012089175172150135 Accuracy 0.8668000102043152\n",
      "Iteration 13910 Training loss 0.006233454216271639 Validation loss 0.01137637160718441 Accuracy 0.8765000104904175\n",
      "Iteration 13920 Training loss 0.004835996776819229 Validation loss 0.011336852796375751 Accuracy 0.8748000264167786\n",
      "Iteration 13930 Training loss 0.008203493431210518 Validation loss 0.011777150444686413 Accuracy 0.8708999752998352\n",
      "Iteration 13940 Training loss 0.006037514191120863 Validation loss 0.011559482663869858 Accuracy 0.873199999332428\n",
      "Iteration 13950 Training loss 0.0060417866334319115 Validation loss 0.011608901433646679 Accuracy 0.8733000159263611\n",
      "Iteration 13960 Training loss 0.004883793648332357 Validation loss 0.011403052136301994 Accuracy 0.8763999938964844\n",
      "Iteration 13970 Training loss 0.006878757383674383 Validation loss 0.011416885070502758 Accuracy 0.8756999969482422\n",
      "Iteration 13980 Training loss 0.006174970418214798 Validation loss 0.011891606263816357 Accuracy 0.8693000078201294\n",
      "Iteration 13990 Training loss 0.004568255972117186 Validation loss 0.011590062640607357 Accuracy 0.8741000294685364\n",
      "Iteration 14000 Training loss 0.0065098972991108894 Validation loss 0.011661713942885399 Accuracy 0.871999979019165\n",
      "Iteration 14010 Training loss 0.00397313479334116 Validation loss 0.011372305452823639 Accuracy 0.8741000294685364\n",
      "Iteration 14020 Training loss 0.006098979152739048 Validation loss 0.011334327049553394 Accuracy 0.875\n",
      "Iteration 14030 Training loss 0.004267937503755093 Validation loss 0.011313874274492264 Accuracy 0.8766000270843506\n",
      "Iteration 14040 Training loss 0.005189003422856331 Validation loss 0.01152871921658516 Accuracy 0.8737000226974487\n",
      "Iteration 14050 Training loss 0.0053339749574661255 Validation loss 0.011677414178848267 Accuracy 0.8723999857902527\n",
      "Iteration 14060 Training loss 0.0037792669609189034 Validation loss 0.011551733128726482 Accuracy 0.8741000294685364\n",
      "Iteration 14070 Training loss 0.004200413823127747 Validation loss 0.01124136708676815 Accuracy 0.8769000172615051\n",
      "Iteration 14080 Training loss 0.0064785354770720005 Validation loss 0.011072702705860138 Accuracy 0.8787999749183655\n",
      "Iteration 14090 Training loss 0.0060951621271669865 Validation loss 0.011128476820886135 Accuracy 0.8780999779701233\n",
      "Iteration 14100 Training loss 0.007185876835137606 Validation loss 0.01159394159913063 Accuracy 0.8729000091552734\n",
      "Iteration 14110 Training loss 0.005879271309822798 Validation loss 0.011569540947675705 Accuracy 0.8745999932289124\n",
      "Iteration 14120 Training loss 0.005352223291993141 Validation loss 0.011428085155785084 Accuracy 0.8743000030517578\n",
      "Iteration 14130 Training loss 0.004894075449556112 Validation loss 0.011240722611546516 Accuracy 0.8762000203132629\n",
      "Iteration 14140 Training loss 0.0049588605761528015 Validation loss 0.010949254035949707 Accuracy 0.8794999718666077\n",
      "Iteration 14150 Training loss 0.00572007754817605 Validation loss 0.011174914427101612 Accuracy 0.8779000043869019\n",
      "Iteration 14160 Training loss 0.006192784756422043 Validation loss 0.01149049773812294 Accuracy 0.8748999834060669\n",
      "Iteration 14170 Training loss 0.005405437666922808 Validation loss 0.011360223405063152 Accuracy 0.8762999773025513\n",
      "Iteration 14180 Training loss 0.006131228990852833 Validation loss 0.011209269985556602 Accuracy 0.8780999779701233\n",
      "Iteration 14190 Training loss 0.006185479927808046 Validation loss 0.011586897075176239 Accuracy 0.8727999925613403\n",
      "Iteration 14200 Training loss 0.00475868908688426 Validation loss 0.011265619657933712 Accuracy 0.878000020980835\n",
      "Iteration 14210 Training loss 0.005672902334481478 Validation loss 0.011233408004045486 Accuracy 0.8763999938964844\n",
      "Iteration 14220 Training loss 0.006541776470839977 Validation loss 0.011166490614414215 Accuracy 0.8776999711990356\n",
      "Iteration 14230 Training loss 0.005676431581377983 Validation loss 0.011108716018497944 Accuracy 0.880299985408783\n",
      "Iteration 14240 Training loss 0.004615411628037691 Validation loss 0.011248002760112286 Accuracy 0.8769000172615051\n",
      "Iteration 14250 Training loss 0.005661703646183014 Validation loss 0.011403584852814674 Accuracy 0.8758999705314636\n",
      "Iteration 14260 Training loss 0.00603675888851285 Validation loss 0.011436467058956623 Accuracy 0.8756999969482422\n",
      "Iteration 14270 Training loss 0.006963781546801329 Validation loss 0.011713308282196522 Accuracy 0.8722000122070312\n",
      "Iteration 14280 Training loss 0.006705734878778458 Validation loss 0.011118298396468163 Accuracy 0.8790000081062317\n",
      "Iteration 14290 Training loss 0.005837020464241505 Validation loss 0.011104626581072807 Accuracy 0.8798999786376953\n",
      "Iteration 14300 Training loss 0.005065736826509237 Validation loss 0.011324869468808174 Accuracy 0.8755000233650208\n",
      "Iteration 14310 Training loss 0.0062018767930567265 Validation loss 0.011344484984874725 Accuracy 0.8752999901771545\n",
      "Iteration 14320 Training loss 0.005286392755806446 Validation loss 0.011326892301440239 Accuracy 0.8758000135421753\n",
      "Iteration 14330 Training loss 0.005968562327325344 Validation loss 0.01146751455962658 Accuracy 0.8741000294685364\n",
      "Iteration 14340 Training loss 0.006469490472227335 Validation loss 0.011516680009663105 Accuracy 0.8740000128746033\n",
      "Iteration 14350 Training loss 0.005930636078119278 Validation loss 0.011364785023033619 Accuracy 0.8755000233650208\n",
      "Iteration 14360 Training loss 0.005811406299471855 Validation loss 0.011463268660008907 Accuracy 0.8758000135421753\n",
      "Iteration 14370 Training loss 0.005669937934726477 Validation loss 0.011217114515602589 Accuracy 0.8769000172615051\n",
      "Iteration 14380 Training loss 0.004980879370123148 Validation loss 0.011133592575788498 Accuracy 0.8791999816894531\n",
      "Iteration 14390 Training loss 0.005301712546497583 Validation loss 0.011231906712055206 Accuracy 0.8773000240325928\n",
      "Iteration 14400 Training loss 0.004700611345469952 Validation loss 0.011194789782166481 Accuracy 0.8769999742507935\n",
      "Iteration 14410 Training loss 0.006223832257091999 Validation loss 0.011351365596055984 Accuracy 0.875\n",
      "Iteration 14420 Training loss 0.005187174770981073 Validation loss 0.011201515793800354 Accuracy 0.8766000270843506\n",
      "Iteration 14430 Training loss 0.0058591472916305065 Validation loss 0.011142788454890251 Accuracy 0.8784000277519226\n",
      "Iteration 14440 Training loss 0.006062142085283995 Validation loss 0.011304566636681557 Accuracy 0.8762999773025513\n",
      "Iteration 14450 Training loss 0.007345682941377163 Validation loss 0.011529822833836079 Accuracy 0.8730000257492065\n",
      "Iteration 14460 Training loss 0.006269055884331465 Validation loss 0.011321944184601307 Accuracy 0.8754000067710876\n",
      "Iteration 14470 Training loss 0.006719321012496948 Validation loss 0.011338909156620502 Accuracy 0.8761000037193298\n",
      "Iteration 14480 Training loss 0.0072685731574893 Validation loss 0.011439356952905655 Accuracy 0.8748999834060669\n",
      "Iteration 14490 Training loss 0.005485848058015108 Validation loss 0.011397731490433216 Accuracy 0.8751000165939331\n",
      "Iteration 14500 Training loss 0.006232194136828184 Validation loss 0.011294559575617313 Accuracy 0.8762000203132629\n",
      "Iteration 14510 Training loss 0.006137802265584469 Validation loss 0.01139883417636156 Accuracy 0.8755999803543091\n",
      "Iteration 14520 Training loss 0.005335374269634485 Validation loss 0.011461506597697735 Accuracy 0.8744999766349792\n",
      "Iteration 14530 Training loss 0.005737908184528351 Validation loss 0.011405883356928825 Accuracy 0.8744999766349792\n",
      "Iteration 14540 Training loss 0.005896507762372494 Validation loss 0.011524121277034283 Accuracy 0.8741000294685364\n",
      "Iteration 14550 Training loss 0.0056457496248185635 Validation loss 0.011360006406903267 Accuracy 0.8766000270843506\n",
      "Iteration 14560 Training loss 0.0077387504279613495 Validation loss 0.01119933370500803 Accuracy 0.8773999810218811\n",
      "Iteration 14570 Training loss 0.005189911462366581 Validation loss 0.011705351993441582 Accuracy 0.8723999857902527\n",
      "Iteration 14580 Training loss 0.006814203690737486 Validation loss 0.011133517138659954 Accuracy 0.8773000240325928\n",
      "Iteration 14590 Training loss 0.006946198176592588 Validation loss 0.011244882829487324 Accuracy 0.8758999705314636\n",
      "Iteration 14600 Training loss 0.006681534461677074 Validation loss 0.011221412569284439 Accuracy 0.8770999908447266\n",
      "Iteration 14610 Training loss 0.007779470644891262 Validation loss 0.01155328843742609 Accuracy 0.8740000128746033\n",
      "Iteration 14620 Training loss 0.006512857973575592 Validation loss 0.011130552738904953 Accuracy 0.8781999945640564\n",
      "Iteration 14630 Training loss 0.004387832246720791 Validation loss 0.011296582408249378 Accuracy 0.8758999705314636\n",
      "Iteration 14640 Training loss 0.003739455481991172 Validation loss 0.01128519419580698 Accuracy 0.8762000203132629\n",
      "Iteration 14650 Training loss 0.0058339680545032024 Validation loss 0.011397769674658775 Accuracy 0.8740000128746033\n",
      "Iteration 14660 Training loss 0.005621970631182194 Validation loss 0.01134413480758667 Accuracy 0.8751999735832214\n",
      "Iteration 14670 Training loss 0.006278002634644508 Validation loss 0.011326288804411888 Accuracy 0.8766000270843506\n",
      "Iteration 14680 Training loss 0.005392396356910467 Validation loss 0.011446667835116386 Accuracy 0.8733999729156494\n",
      "Iteration 14690 Training loss 0.005218042526394129 Validation loss 0.011216538026928902 Accuracy 0.8773999810218811\n",
      "Iteration 14700 Training loss 0.007377213798463345 Validation loss 0.011216883547604084 Accuracy 0.8772000074386597\n",
      "Iteration 14710 Training loss 0.004936068784445524 Validation loss 0.011118288151919842 Accuracy 0.8794000148773193\n",
      "Iteration 14720 Training loss 0.007108432240784168 Validation loss 0.01122354343533516 Accuracy 0.8774999976158142\n",
      "Iteration 14730 Training loss 0.0065559884533286095 Validation loss 0.011275243945419788 Accuracy 0.8766000270843506\n",
      "Iteration 14740 Training loss 0.005117177031934261 Validation loss 0.011146101169288158 Accuracy 0.8770999908447266\n",
      "Iteration 14750 Training loss 0.004779049661010504 Validation loss 0.01124015636742115 Accuracy 0.8766999840736389\n",
      "Iteration 14760 Training loss 0.007467507850378752 Validation loss 0.010999063961207867 Accuracy 0.8794999718666077\n",
      "Iteration 14770 Training loss 0.005482149310410023 Validation loss 0.011373224668204784 Accuracy 0.8752999901771545\n",
      "Iteration 14780 Training loss 0.004575610626488924 Validation loss 0.011273923330008984 Accuracy 0.8752999901771545\n",
      "Iteration 14790 Training loss 0.00538800610229373 Validation loss 0.011562060564756393 Accuracy 0.8726999759674072\n",
      "Iteration 14800 Training loss 0.005554845090955496 Validation loss 0.011228092946112156 Accuracy 0.8773000240325928\n",
      "Iteration 14810 Training loss 0.005167301744222641 Validation loss 0.011497583240270615 Accuracy 0.8744999766349792\n",
      "Iteration 14820 Training loss 0.0051965718157589436 Validation loss 0.011762280017137527 Accuracy 0.8709999918937683\n",
      "Iteration 14830 Training loss 0.006731131114065647 Validation loss 0.011551900766789913 Accuracy 0.8727999925613403\n",
      "Iteration 14840 Training loss 0.005325006321072578 Validation loss 0.011312457732856274 Accuracy 0.8756999969482422\n",
      "Iteration 14850 Training loss 0.004314110614359379 Validation loss 0.011545766144990921 Accuracy 0.8741999864578247\n",
      "Iteration 14860 Training loss 0.004497770220041275 Validation loss 0.011346780695021152 Accuracy 0.8755999803543091\n",
      "Iteration 14870 Training loss 0.007102105300873518 Validation loss 0.011468332260847092 Accuracy 0.8736000061035156\n",
      "Iteration 14880 Training loss 0.006384757347404957 Validation loss 0.011391989886760712 Accuracy 0.8751999735832214\n",
      "Iteration 14890 Training loss 0.004338379483669996 Validation loss 0.011451028287410736 Accuracy 0.8723000288009644\n",
      "Iteration 14900 Training loss 0.005819091107696295 Validation loss 0.011511079967021942 Accuracy 0.8726999759674072\n",
      "Iteration 14910 Training loss 0.00408522691577673 Validation loss 0.01191179919987917 Accuracy 0.8690000176429749\n",
      "Iteration 14920 Training loss 0.005693770945072174 Validation loss 0.011484235525131226 Accuracy 0.8751999735832214\n",
      "Iteration 14930 Training loss 0.005823825020343065 Validation loss 0.011527929455041885 Accuracy 0.8748999834060669\n",
      "Iteration 14940 Training loss 0.006051176227629185 Validation loss 0.011596652679145336 Accuracy 0.8733999729156494\n",
      "Iteration 14950 Training loss 0.005633464083075523 Validation loss 0.011292822659015656 Accuracy 0.8762999773025513\n",
      "Iteration 14960 Training loss 0.006752575747668743 Validation loss 0.011549802497029305 Accuracy 0.8736000061035156\n",
      "Iteration 14970 Training loss 0.0062015424482524395 Validation loss 0.01152827125042677 Accuracy 0.8730000257492065\n",
      "Iteration 14980 Training loss 0.005231228191405535 Validation loss 0.011344273574650288 Accuracy 0.8765000104904175\n",
      "Iteration 14990 Training loss 0.004563756752759218 Validation loss 0.011355959810316563 Accuracy 0.8751999735832214\n",
      "Iteration 15000 Training loss 0.007176073733717203 Validation loss 0.011516226455569267 Accuracy 0.8738999962806702\n",
      "Iteration 15010 Training loss 0.005600306671112776 Validation loss 0.011377178132534027 Accuracy 0.8745999932289124\n",
      "Iteration 15020 Training loss 0.005633292719721794 Validation loss 0.01134973019361496 Accuracy 0.875\n",
      "Iteration 15030 Training loss 0.007024343591183424 Validation loss 0.0114737618714571 Accuracy 0.8743000030517578\n",
      "Iteration 15040 Training loss 0.005343475379049778 Validation loss 0.011282814666628838 Accuracy 0.8766000270843506\n",
      "Iteration 15050 Training loss 0.005923236720263958 Validation loss 0.011263629421591759 Accuracy 0.8755000233650208\n",
      "Iteration 15060 Training loss 0.00658185500651598 Validation loss 0.011512147262692451 Accuracy 0.8737999796867371\n",
      "Iteration 15070 Training loss 0.005363586358726025 Validation loss 0.011325670406222343 Accuracy 0.8759999871253967\n",
      "Iteration 15080 Training loss 0.005538796074688435 Validation loss 0.011156308464705944 Accuracy 0.8776999711990356\n",
      "Iteration 15090 Training loss 0.006580476649105549 Validation loss 0.011256667785346508 Accuracy 0.8766999840736389\n",
      "Iteration 15100 Training loss 0.0063561140559613705 Validation loss 0.01141916960477829 Accuracy 0.8751999735832214\n",
      "Iteration 15110 Training loss 0.005487706977874041 Validation loss 0.0112894456833601 Accuracy 0.8758999705314636\n",
      "Iteration 15120 Training loss 0.006149229593575001 Validation loss 0.01137287076562643 Accuracy 0.8756999969482422\n",
      "Iteration 15130 Training loss 0.006576095707714558 Validation loss 0.011382016353309155 Accuracy 0.8759999871253967\n",
      "Iteration 15140 Training loss 0.005121008027344942 Validation loss 0.011162413284182549 Accuracy 0.8772000074386597\n",
      "Iteration 15150 Training loss 0.005338153336197138 Validation loss 0.011404229328036308 Accuracy 0.8737999796867371\n",
      "Iteration 15160 Training loss 0.006060718558728695 Validation loss 0.011393946595489979 Accuracy 0.8755999803543091\n",
      "Iteration 15170 Training loss 0.005620519630610943 Validation loss 0.011341476812958717 Accuracy 0.8748000264167786\n",
      "Iteration 15180 Training loss 0.0056338985450565815 Validation loss 0.011241261847317219 Accuracy 0.8761000037193298\n",
      "Iteration 15190 Training loss 0.0053600952960550785 Validation loss 0.011345927603542805 Accuracy 0.875\n",
      "Iteration 15200 Training loss 0.0060774642042815685 Validation loss 0.01134162675589323 Accuracy 0.8769000172615051\n",
      "Iteration 15210 Training loss 0.005462565924972296 Validation loss 0.011167524382472038 Accuracy 0.876800000667572\n",
      "Iteration 15220 Training loss 0.005338563118129969 Validation loss 0.011337334290146828 Accuracy 0.875\n",
      "Iteration 15230 Training loss 0.005150304641574621 Validation loss 0.011260337196290493 Accuracy 0.8762000203132629\n",
      "Iteration 15240 Training loss 0.006217343732714653 Validation loss 0.011256391182541847 Accuracy 0.8762000203132629\n",
      "Iteration 15250 Training loss 0.004910538904368877 Validation loss 0.01130956131964922 Accuracy 0.8761000037193298\n",
      "Iteration 15260 Training loss 0.005907620303332806 Validation loss 0.011323089711368084 Accuracy 0.8761000037193298\n",
      "Iteration 15270 Training loss 0.005376758519560099 Validation loss 0.011400769464671612 Accuracy 0.8744000196456909\n",
      "Iteration 15280 Training loss 0.007236457895487547 Validation loss 0.011128397658467293 Accuracy 0.8774999976158142\n",
      "Iteration 15290 Training loss 0.005613140761852264 Validation loss 0.01103790570050478 Accuracy 0.8791000247001648\n",
      "Iteration 15300 Training loss 0.006111781578511 Validation loss 0.011159311980009079 Accuracy 0.8784000277519226\n",
      "Iteration 15310 Training loss 0.0059844800271093845 Validation loss 0.011409326456487179 Accuracy 0.875\n",
      "Iteration 15320 Training loss 0.005881812423467636 Validation loss 0.011420100927352905 Accuracy 0.8752999901771545\n",
      "Iteration 15330 Training loss 0.005584981292486191 Validation loss 0.011357610113918781 Accuracy 0.8751000165939331\n",
      "Iteration 15340 Training loss 0.004929392598569393 Validation loss 0.011328748427331448 Accuracy 0.8748000264167786\n",
      "Iteration 15350 Training loss 0.005851035937666893 Validation loss 0.011341644451022148 Accuracy 0.8744999766349792\n",
      "Iteration 15360 Training loss 0.004996342118829489 Validation loss 0.011132122948765755 Accuracy 0.878600001335144\n",
      "Iteration 15370 Training loss 0.00712130032479763 Validation loss 0.011677553877234459 Accuracy 0.8707000017166138\n",
      "Iteration 15380 Training loss 0.005707496777176857 Validation loss 0.011730130761861801 Accuracy 0.8708999752998352\n",
      "Iteration 15390 Training loss 0.003661635797470808 Validation loss 0.011377684772014618 Accuracy 0.8748000264167786\n",
      "Iteration 15400 Training loss 0.00457895640283823 Validation loss 0.011333472095429897 Accuracy 0.8745999932289124\n",
      "Iteration 15410 Training loss 0.004647566005587578 Validation loss 0.01116874534636736 Accuracy 0.8788999915122986\n",
      "Iteration 15420 Training loss 0.007213605102151632 Validation loss 0.011332171969115734 Accuracy 0.8758999705314636\n",
      "Iteration 15430 Training loss 0.005068404134362936 Validation loss 0.011136400513350964 Accuracy 0.8770999908447266\n",
      "Iteration 15440 Training loss 0.0032042618840932846 Validation loss 0.011196253821253777 Accuracy 0.8774999976158142\n",
      "Iteration 15450 Training loss 0.005831283982843161 Validation loss 0.011322575621306896 Accuracy 0.875\n",
      "Iteration 15460 Training loss 0.005220188293606043 Validation loss 0.011141300201416016 Accuracy 0.878600001335144\n",
      "Iteration 15470 Training loss 0.008220317773520947 Validation loss 0.012321378104388714 Accuracy 0.8648999929428101\n",
      "Iteration 15480 Training loss 0.00650057103484869 Validation loss 0.011236521415412426 Accuracy 0.8766000270843506\n",
      "Iteration 15490 Training loss 0.0035644343588501215 Validation loss 0.011279257945716381 Accuracy 0.8766999840736389\n",
      "Iteration 15500 Training loss 0.00664491904899478 Validation loss 0.011725841090083122 Accuracy 0.8709999918937683\n",
      "Iteration 15510 Training loss 0.003767202142626047 Validation loss 0.011164533905684948 Accuracy 0.876800000667572\n",
      "Iteration 15520 Training loss 0.005488301627337933 Validation loss 0.011173401027917862 Accuracy 0.8773000240325928\n",
      "Iteration 15530 Training loss 0.006311654578894377 Validation loss 0.01139525230973959 Accuracy 0.8759999871253967\n",
      "Iteration 15540 Training loss 0.006018360611051321 Validation loss 0.011530573479831219 Accuracy 0.8727999925613403\n",
      "Iteration 15550 Training loss 0.008515262976288795 Validation loss 0.012595195323228836 Accuracy 0.8614000082015991\n",
      "Iteration 15560 Training loss 0.0055921184830367565 Validation loss 0.011365637183189392 Accuracy 0.8762000203132629\n",
      "Iteration 15570 Training loss 0.005308795720338821 Validation loss 0.011560029350221157 Accuracy 0.8745999932289124\n",
      "Iteration 15580 Training loss 0.003642699448391795 Validation loss 0.01136542484164238 Accuracy 0.8748000264167786\n",
      "Iteration 15590 Training loss 0.0053624967113137245 Validation loss 0.011219059117138386 Accuracy 0.8762999773025513\n",
      "Iteration 15600 Training loss 0.00502703245729208 Validation loss 0.011272608302533627 Accuracy 0.8774999976158142\n",
      "Iteration 15610 Training loss 0.004903086926788092 Validation loss 0.011130725033581257 Accuracy 0.8773000240325928\n",
      "Iteration 15620 Training loss 0.0047960360534489155 Validation loss 0.01139451377093792 Accuracy 0.875\n",
      "Iteration 15630 Training loss 0.005993733648210764 Validation loss 0.011227683164179325 Accuracy 0.876800000667572\n",
      "Iteration 15640 Training loss 0.006945183500647545 Validation loss 0.011336728930473328 Accuracy 0.8755999803543091\n",
      "Iteration 15650 Training loss 0.005367693491280079 Validation loss 0.011116706766188145 Accuracy 0.8773000240325928\n",
      "Iteration 15660 Training loss 0.003562985686585307 Validation loss 0.011232134886085987 Accuracy 0.8761000037193298\n",
      "Iteration 15670 Training loss 0.005473760422319174 Validation loss 0.011221462860703468 Accuracy 0.8773999810218811\n",
      "Iteration 15680 Training loss 0.00360263348557055 Validation loss 0.011369106359779835 Accuracy 0.876800000667572\n",
      "Iteration 15690 Training loss 0.004873320925980806 Validation loss 0.011203293688595295 Accuracy 0.878000020980835\n",
      "Iteration 15700 Training loss 0.0043858313001692295 Validation loss 0.0111341942101717 Accuracy 0.8783000111579895\n",
      "Iteration 15710 Training loss 0.005612397566437721 Validation loss 0.011305811814963818 Accuracy 0.8762999773025513\n",
      "Iteration 15720 Training loss 0.005382485222071409 Validation loss 0.011520802974700928 Accuracy 0.8737999796867371\n",
      "Iteration 15730 Training loss 0.0056015788577497005 Validation loss 0.011142614297568798 Accuracy 0.8772000074386597\n",
      "Iteration 15740 Training loss 0.005498926155269146 Validation loss 0.011224846355617046 Accuracy 0.8779000043869019\n",
      "Iteration 15750 Training loss 0.005758299026638269 Validation loss 0.011203436180949211 Accuracy 0.8770999908447266\n",
      "Iteration 15760 Training loss 0.007082123309373856 Validation loss 0.011226414702832699 Accuracy 0.8758000135421753\n",
      "Iteration 15770 Training loss 0.004974032286554575 Validation loss 0.011276756413280964 Accuracy 0.8755999803543091\n",
      "Iteration 15780 Training loss 0.004521260038018227 Validation loss 0.011223439127206802 Accuracy 0.8762999773025513\n",
      "Iteration 15790 Training loss 0.005285278428345919 Validation loss 0.01120438240468502 Accuracy 0.8762000203132629\n",
      "Iteration 15800 Training loss 0.005649430677294731 Validation loss 0.011221406050026417 Accuracy 0.878000020980835\n",
      "Iteration 15810 Training loss 0.006067249458283186 Validation loss 0.011177927255630493 Accuracy 0.876800000667572\n",
      "Iteration 15820 Training loss 0.005030053202062845 Validation loss 0.011340610682964325 Accuracy 0.8761000037193298\n",
      "Iteration 15830 Training loss 0.005116503220051527 Validation loss 0.0114574721083045 Accuracy 0.8723000288009644\n",
      "Iteration 15840 Training loss 0.004466570448130369 Validation loss 0.011063538491725922 Accuracy 0.8765000104904175\n",
      "Iteration 15850 Training loss 0.004416824784129858 Validation loss 0.011155043728649616 Accuracy 0.8781999945640564\n",
      "Iteration 15860 Training loss 0.004652394447475672 Validation loss 0.011279282160103321 Accuracy 0.8751999735832214\n",
      "Iteration 15870 Training loss 0.0061846282333135605 Validation loss 0.011253139935433865 Accuracy 0.876800000667572\n",
      "Iteration 15880 Training loss 0.004652624949812889 Validation loss 0.01114877313375473 Accuracy 0.878000020980835\n",
      "Iteration 15890 Training loss 0.007495303638279438 Validation loss 0.011500050313770771 Accuracy 0.8730000257492065\n",
      "Iteration 15900 Training loss 0.007359915412962437 Validation loss 0.011566818691790104 Accuracy 0.8737000226974487\n",
      "Iteration 15910 Training loss 0.0051627252250909805 Validation loss 0.01107779797166586 Accuracy 0.8777999877929688\n",
      "Iteration 15920 Training loss 0.006505855359137058 Validation loss 0.011295173317193985 Accuracy 0.876800000667572\n",
      "Iteration 15930 Training loss 0.005646426696330309 Validation loss 0.011183495633304119 Accuracy 0.8777999877929688\n",
      "Iteration 15940 Training loss 0.004568710457533598 Validation loss 0.011203497648239136 Accuracy 0.8769999742507935\n",
      "Iteration 15950 Training loss 0.005353923887014389 Validation loss 0.011135796085000038 Accuracy 0.8784000277519226\n",
      "Iteration 15960 Training loss 0.0071944622322916985 Validation loss 0.011599801480770111 Accuracy 0.8726000189781189\n",
      "Iteration 15970 Training loss 0.005619012750685215 Validation loss 0.01142827421426773 Accuracy 0.8743000030517578\n",
      "Iteration 15980 Training loss 0.004104799125343561 Validation loss 0.011119716800749302 Accuracy 0.8763999938964844\n",
      "Iteration 15990 Training loss 0.005926722660660744 Validation loss 0.011385281570255756 Accuracy 0.8751000165939331\n",
      "Iteration 16000 Training loss 0.004797168076038361 Validation loss 0.011022460646927357 Accuracy 0.8798999786376953\n",
      "Iteration 16010 Training loss 0.004624243825674057 Validation loss 0.011080285534262657 Accuracy 0.8773000240325928\n",
      "Iteration 16020 Training loss 0.004238623660057783 Validation loss 0.011217683553695679 Accuracy 0.8748999834060669\n",
      "Iteration 16030 Training loss 0.005029082763940096 Validation loss 0.011278538964688778 Accuracy 0.8755999803543091\n",
      "Iteration 16040 Training loss 0.006754739675670862 Validation loss 0.011327396146953106 Accuracy 0.8751000165939331\n",
      "Iteration 16050 Training loss 0.0058477832935750484 Validation loss 0.011078131385147572 Accuracy 0.8777999877929688\n",
      "Iteration 16060 Training loss 0.005652827676385641 Validation loss 0.011178309097886086 Accuracy 0.8765000104904175\n",
      "Iteration 16070 Training loss 0.007032835856080055 Validation loss 0.011192687787115574 Accuracy 0.8772000074386597\n",
      "Iteration 16080 Training loss 0.004424265120178461 Validation loss 0.011154753156006336 Accuracy 0.8766000270843506\n",
      "Iteration 16090 Training loss 0.0045386007986962795 Validation loss 0.011261191219091415 Accuracy 0.8769000172615051\n",
      "Iteration 16100 Training loss 0.00558039965108037 Validation loss 0.011394032277166843 Accuracy 0.8754000067710876\n",
      "Iteration 16110 Training loss 0.006190760992467403 Validation loss 0.011482601054012775 Accuracy 0.8743000030517578\n",
      "Iteration 16120 Training loss 0.005786340218037367 Validation loss 0.011488253250718117 Accuracy 0.8730999827384949\n",
      "Iteration 16130 Training loss 0.005731198005378246 Validation loss 0.011464947834610939 Accuracy 0.8755000233650208\n",
      "Iteration 16140 Training loss 0.004764318931847811 Validation loss 0.01127508282661438 Accuracy 0.8759999871253967\n",
      "Iteration 16150 Training loss 0.006224334239959717 Validation loss 0.011293686926364899 Accuracy 0.8745999932289124\n",
      "Iteration 16160 Training loss 0.006560803856700659 Validation loss 0.011257053352892399 Accuracy 0.8773000240325928\n",
      "Iteration 16170 Training loss 0.006285887211561203 Validation loss 0.011180860921740532 Accuracy 0.876800000667572\n",
      "Iteration 16180 Training loss 0.005714166443794966 Validation loss 0.011234663426876068 Accuracy 0.8769999742507935\n",
      "Iteration 16190 Training loss 0.004877104889601469 Validation loss 0.011105002835392952 Accuracy 0.8773999810218811\n",
      "Iteration 16200 Training loss 0.00627352437004447 Validation loss 0.011133022606372833 Accuracy 0.876800000667572\n",
      "Iteration 16210 Training loss 0.005512747913599014 Validation loss 0.011446589604020119 Accuracy 0.873199999332428\n",
      "Iteration 16220 Training loss 0.005945106036961079 Validation loss 0.011451653204858303 Accuracy 0.8738999962806702\n",
      "Iteration 16230 Training loss 0.006148481275886297 Validation loss 0.011157014407217503 Accuracy 0.8773999810218811\n",
      "Iteration 16240 Training loss 0.004942098632454872 Validation loss 0.011179613880813122 Accuracy 0.876800000667572\n",
      "Iteration 16250 Training loss 0.005238373298197985 Validation loss 0.011171318590641022 Accuracy 0.8781999945640564\n",
      "Iteration 16260 Training loss 0.005551437847316265 Validation loss 0.011265547014772892 Accuracy 0.8766000270843506\n",
      "Iteration 16270 Training loss 0.005869470536708832 Validation loss 0.011765990406274796 Accuracy 0.8712999820709229\n",
      "Iteration 16280 Training loss 0.005318731535226107 Validation loss 0.011215221136808395 Accuracy 0.8758999705314636\n",
      "Iteration 16290 Training loss 0.004774569068104029 Validation loss 0.01113899052143097 Accuracy 0.878000020980835\n",
      "Iteration 16300 Training loss 0.004197348840534687 Validation loss 0.011579228565096855 Accuracy 0.8726999759674072\n",
      "Iteration 16310 Training loss 0.004808138124644756 Validation loss 0.011096884496510029 Accuracy 0.8774999976158142\n",
      "Iteration 16320 Training loss 0.0034858162980526686 Validation loss 0.01124305184930563 Accuracy 0.8770999908447266\n",
      "Iteration 16330 Training loss 0.004201754927635193 Validation loss 0.011309021152555943 Accuracy 0.8762000203132629\n",
      "Iteration 16340 Training loss 0.0044605424627661705 Validation loss 0.011201592162251472 Accuracy 0.8765000104904175\n",
      "Iteration 16350 Training loss 0.00528299156576395 Validation loss 0.011260668747127056 Accuracy 0.876800000667572\n",
      "Iteration 16360 Training loss 0.004516796208918095 Validation loss 0.01125241443514824 Accuracy 0.8758999705314636\n",
      "Iteration 16370 Training loss 0.00509325135499239 Validation loss 0.011104363948106766 Accuracy 0.8783000111579895\n",
      "Iteration 16380 Training loss 0.0052504525519907475 Validation loss 0.011175195686519146 Accuracy 0.8759999871253967\n",
      "Iteration 16390 Training loss 0.004508384503424168 Validation loss 0.011169958859682083 Accuracy 0.8770999908447266\n",
      "Iteration 16400 Training loss 0.0059964642859995365 Validation loss 0.011216447688639164 Accuracy 0.8761000037193298\n",
      "Iteration 16410 Training loss 0.00474558537825942 Validation loss 0.011331710033118725 Accuracy 0.8743000030517578\n",
      "Iteration 16420 Training loss 0.004590086173266172 Validation loss 0.011222323402762413 Accuracy 0.8759999871253967\n",
      "Iteration 16430 Training loss 0.005536332260817289 Validation loss 0.011323089711368084 Accuracy 0.8766000270843506\n",
      "Iteration 16440 Training loss 0.0043024830520153046 Validation loss 0.011278454214334488 Accuracy 0.8748000264167786\n",
      "Iteration 16450 Training loss 0.00633715046569705 Validation loss 0.011582701466977596 Accuracy 0.8722000122070312\n",
      "Iteration 16460 Training loss 0.005989386234432459 Validation loss 0.01148779783397913 Accuracy 0.8730000257492065\n",
      "Iteration 16470 Training loss 0.004857566207647324 Validation loss 0.01129056140780449 Accuracy 0.8755000233650208\n",
      "Iteration 16480 Training loss 0.0043049221858382225 Validation loss 0.011144129559397697 Accuracy 0.8784000277519226\n",
      "Iteration 16490 Training loss 0.005260826554149389 Validation loss 0.011381009593605995 Accuracy 0.8741000294685364\n",
      "Iteration 16500 Training loss 0.005109313875436783 Validation loss 0.011321856640279293 Accuracy 0.8755999803543091\n",
      "Iteration 16510 Training loss 0.004889378324151039 Validation loss 0.011172459460794926 Accuracy 0.8769000172615051\n",
      "Iteration 16520 Training loss 0.00497772078961134 Validation loss 0.01116911880671978 Accuracy 0.8758999705314636\n",
      "Iteration 16530 Training loss 0.005192834418267012 Validation loss 0.011196412146091461 Accuracy 0.8748999834060669\n",
      "Iteration 16540 Training loss 0.00384760950691998 Validation loss 0.011360212229192257 Accuracy 0.8745999932289124\n",
      "Iteration 16550 Training loss 0.005774021614342928 Validation loss 0.011725966818630695 Accuracy 0.869700014591217\n",
      "Iteration 16560 Training loss 0.0040461220778524876 Validation loss 0.01149754598736763 Accuracy 0.8737000226974487\n",
      "Iteration 16570 Training loss 0.003849888453260064 Validation loss 0.011367975734174252 Accuracy 0.8734999895095825\n",
      "Iteration 16580 Training loss 0.005239230580627918 Validation loss 0.011539162136614323 Accuracy 0.871399998664856\n",
      "Iteration 16590 Training loss 0.005715892184525728 Validation loss 0.011440183036029339 Accuracy 0.8748999834060669\n",
      "Iteration 16600 Training loss 0.004020813852548599 Validation loss 0.011065540835261345 Accuracy 0.8787000179290771\n",
      "Iteration 16610 Training loss 0.004651973955333233 Validation loss 0.011342016980051994 Accuracy 0.8754000067710876\n",
      "Iteration 16620 Training loss 0.004844856448471546 Validation loss 0.011392188258469105 Accuracy 0.8727999925613403\n",
      "Iteration 16630 Training loss 0.005025897175073624 Validation loss 0.011162684299051762 Accuracy 0.8784999847412109\n",
      "Iteration 16640 Training loss 0.005043428856879473 Validation loss 0.01123285386711359 Accuracy 0.8762999773025513\n",
      "Iteration 16650 Training loss 0.004261819180101156 Validation loss 0.011086617596447468 Accuracy 0.8790000081062317\n",
      "Iteration 16660 Training loss 0.004082555882632732 Validation loss 0.011003637686371803 Accuracy 0.8794000148773193\n",
      "Iteration 16670 Training loss 0.004708587191998959 Validation loss 0.011210048571228981 Accuracy 0.875\n",
      "Iteration 16680 Training loss 0.00652121240273118 Validation loss 0.01109418086707592 Accuracy 0.8766000270843506\n",
      "Iteration 16690 Training loss 0.004512903746217489 Validation loss 0.011002026498317719 Accuracy 0.8787999749183655\n",
      "Iteration 16700 Training loss 0.004558147396892309 Validation loss 0.01100683119148016 Accuracy 0.8794999718666077\n",
      "Iteration 16710 Training loss 0.00612891698256135 Validation loss 0.011191182769834995 Accuracy 0.8758999705314636\n",
      "Iteration 16720 Training loss 0.007041988894343376 Validation loss 0.011438918299973011 Accuracy 0.8733999729156494\n",
      "Iteration 16730 Training loss 0.003181220730766654 Validation loss 0.011113696731626987 Accuracy 0.8763999938964844\n",
      "Iteration 16740 Training loss 0.005329384468495846 Validation loss 0.011715221218764782 Accuracy 0.8707000017166138\n",
      "Iteration 16750 Training loss 0.004477029200643301 Validation loss 0.011216248385608196 Accuracy 0.8769999742507935\n",
      "Iteration 16760 Training loss 0.005234324838966131 Validation loss 0.011177907697856426 Accuracy 0.8791999816894531\n",
      "Iteration 16770 Training loss 0.0035307162906974554 Validation loss 0.011325758881866932 Accuracy 0.8751000165939331\n",
      "Iteration 16780 Training loss 0.005598489660769701 Validation loss 0.011245466768741608 Accuracy 0.8756999969482422\n",
      "Iteration 16790 Training loss 0.006158972624689341 Validation loss 0.011139476671814919 Accuracy 0.8776000142097473\n",
      "Iteration 16800 Training loss 0.00424154382199049 Validation loss 0.011136445216834545 Accuracy 0.8762999773025513\n",
      "Iteration 16810 Training loss 0.004356094170361757 Validation loss 0.011180696077644825 Accuracy 0.8773999810218811\n",
      "Iteration 16820 Training loss 0.004236988723278046 Validation loss 0.011164200492203236 Accuracy 0.8769000172615051\n",
      "Iteration 16830 Training loss 0.006101246923208237 Validation loss 0.011181583628058434 Accuracy 0.8763999938964844\n",
      "Iteration 16840 Training loss 0.006263006944209337 Validation loss 0.010946739464998245 Accuracy 0.8792999982833862\n",
      "Iteration 16850 Training loss 0.005500101950019598 Validation loss 0.011125241406261921 Accuracy 0.8766000270843506\n",
      "Iteration 16860 Training loss 0.0071588559076189995 Validation loss 0.011081279255449772 Accuracy 0.8791000247001648\n",
      "Iteration 16870 Training loss 0.005618159659206867 Validation loss 0.011107382364571095 Accuracy 0.8780999779701233\n",
      "Iteration 16880 Training loss 0.0032552657648921013 Validation loss 0.011074741370975971 Accuracy 0.8776000142097473\n",
      "Iteration 16890 Training loss 0.004970681853592396 Validation loss 0.011118688620626926 Accuracy 0.8769999742507935\n",
      "Iteration 16900 Training loss 0.006415451876819134 Validation loss 0.011296740733087063 Accuracy 0.8766000270843506\n",
      "Iteration 16910 Training loss 0.005428722593933344 Validation loss 0.011376763693988323 Accuracy 0.8754000067710876\n",
      "Iteration 16920 Training loss 0.004923206754028797 Validation loss 0.01104789786040783 Accuracy 0.8787999749183655\n",
      "Iteration 16930 Training loss 0.004736533388495445 Validation loss 0.01134879607707262 Accuracy 0.8755000233650208\n",
      "Iteration 16940 Training loss 0.0046095577999949455 Validation loss 0.01154262199997902 Accuracy 0.8730999827384949\n",
      "Iteration 16950 Training loss 0.00560004310682416 Validation loss 0.011176932603120804 Accuracy 0.8766000270843506\n",
      "Iteration 16960 Training loss 0.004663373809307814 Validation loss 0.01137719489634037 Accuracy 0.8758999705314636\n",
      "Iteration 16970 Training loss 0.004220994655042887 Validation loss 0.011232978664338589 Accuracy 0.8766999840736389\n",
      "Iteration 16980 Training loss 0.006033505778759718 Validation loss 0.011098355986177921 Accuracy 0.8776000142097473\n",
      "Iteration 16990 Training loss 0.003216456389054656 Validation loss 0.011047850362956524 Accuracy 0.8777999877929688\n",
      "Iteration 17000 Training loss 0.005059650633484125 Validation loss 0.010952243581414223 Accuracy 0.8805999755859375\n",
      "Iteration 17010 Training loss 0.00411280058324337 Validation loss 0.011275836266577244 Accuracy 0.8758999705314636\n",
      "Iteration 17020 Training loss 0.003237506141886115 Validation loss 0.011141614057123661 Accuracy 0.8776000142097473\n",
      "Iteration 17030 Training loss 0.005826705135405064 Validation loss 0.011301943100988865 Accuracy 0.8747000098228455\n",
      "Iteration 17040 Training loss 0.004987356252968311 Validation loss 0.011190507560968399 Accuracy 0.8766999840736389\n",
      "Iteration 17050 Training loss 0.005371252540498972 Validation loss 0.011031726375222206 Accuracy 0.8794999718666077\n",
      "Iteration 17060 Training loss 0.004642448853701353 Validation loss 0.010995288379490376 Accuracy 0.8787000179290771\n",
      "Iteration 17070 Training loss 0.004506124649196863 Validation loss 0.011427477933466434 Accuracy 0.8740000128746033\n",
      "Iteration 17080 Training loss 0.004332040436565876 Validation loss 0.010987485758960247 Accuracy 0.8791000247001648\n",
      "Iteration 17090 Training loss 0.005778531543910503 Validation loss 0.011148235760629177 Accuracy 0.8774999976158142\n",
      "Iteration 17100 Training loss 0.004918110091239214 Validation loss 0.011053817346692085 Accuracy 0.8788999915122986\n",
      "Iteration 17110 Training loss 0.005864900536835194 Validation loss 0.011286163702607155 Accuracy 0.8756999969482422\n",
      "Iteration 17120 Training loss 0.005826852284371853 Validation loss 0.011104519478976727 Accuracy 0.8773000240325928\n",
      "Iteration 17130 Training loss 0.004398990422487259 Validation loss 0.010922510176897049 Accuracy 0.8804000020027161\n",
      "Iteration 17140 Training loss 0.003810824127867818 Validation loss 0.010855438187718391 Accuracy 0.8809000253677368\n",
      "Iteration 17150 Training loss 0.0035016117617487907 Validation loss 0.01089357677847147 Accuracy 0.879800021648407\n",
      "Iteration 17160 Training loss 0.003783612046390772 Validation loss 0.011424914933741093 Accuracy 0.8725000023841858\n",
      "Iteration 17170 Training loss 0.004862541798502207 Validation loss 0.01125737838447094 Accuracy 0.8752999901771545\n",
      "Iteration 17180 Training loss 0.005243565421551466 Validation loss 0.010908120311796665 Accuracy 0.8808000087738037\n",
      "Iteration 17190 Training loss 0.005026486236602068 Validation loss 0.010938874445855618 Accuracy 0.8794999718666077\n",
      "Iteration 17200 Training loss 0.005859715864062309 Validation loss 0.011011403985321522 Accuracy 0.8790000081062317\n",
      "Iteration 17210 Training loss 0.00580371031537652 Validation loss 0.01141924038529396 Accuracy 0.8751000165939331\n",
      "Iteration 17220 Training loss 0.00443077040836215 Validation loss 0.010995685122907162 Accuracy 0.8787000179290771\n",
      "Iteration 17230 Training loss 0.0032056167256087065 Validation loss 0.011163742281496525 Accuracy 0.8773000240325928\n",
      "Iteration 17240 Training loss 0.0031813113018870354 Validation loss 0.011398828588426113 Accuracy 0.8744999766349792\n",
      "Iteration 17250 Training loss 0.006256766617298126 Validation loss 0.011324786581099033 Accuracy 0.8748999834060669\n",
      "Iteration 17260 Training loss 0.00492733484134078 Validation loss 0.010996991768479347 Accuracy 0.8779000043869019\n",
      "Iteration 17270 Training loss 0.0037339741829782724 Validation loss 0.01108519546687603 Accuracy 0.8787000179290771\n",
      "Iteration 17280 Training loss 0.004098338074982166 Validation loss 0.011018582619726658 Accuracy 0.8791000247001648\n",
      "Iteration 17290 Training loss 0.00528537156060338 Validation loss 0.01167221274226904 Accuracy 0.8709999918937683\n",
      "Iteration 17300 Training loss 0.004818746820092201 Validation loss 0.011141582392156124 Accuracy 0.8784000277519226\n",
      "Iteration 17310 Training loss 0.005314284469932318 Validation loss 0.011236313730478287 Accuracy 0.8765000104904175\n",
      "Iteration 17320 Training loss 0.004302192013710737 Validation loss 0.01133640669286251 Accuracy 0.8744999766349792\n",
      "Iteration 17330 Training loss 0.004450465086847544 Validation loss 0.011017926037311554 Accuracy 0.878000020980835\n",
      "Iteration 17340 Training loss 0.003921131137758493 Validation loss 0.010980424471199512 Accuracy 0.8783000111579895\n",
      "Iteration 17350 Training loss 0.004989456385374069 Validation loss 0.011330542154610157 Accuracy 0.8740000128746033\n",
      "Iteration 17360 Training loss 0.004497436340898275 Validation loss 0.011057121679186821 Accuracy 0.8790000081062317\n",
      "Iteration 17370 Training loss 0.005019879899919033 Validation loss 0.011426001787185669 Accuracy 0.8748999834060669\n",
      "Iteration 17380 Training loss 0.0036385818384587765 Validation loss 0.011127660050988197 Accuracy 0.8784999847412109\n",
      "Iteration 17390 Training loss 0.004226697143167257 Validation loss 0.011091616004705429 Accuracy 0.8776000142097473\n",
      "Iteration 17400 Training loss 0.003713431069627404 Validation loss 0.01116118859499693 Accuracy 0.876800000667572\n",
      "Iteration 17410 Training loss 0.004957826808094978 Validation loss 0.011236860416829586 Accuracy 0.8758999705314636\n",
      "Iteration 17420 Training loss 0.0038102841936051846 Validation loss 0.011339901946485043 Accuracy 0.8755000233650208\n",
      "Iteration 17430 Training loss 0.004439023323357105 Validation loss 0.010944100096821785 Accuracy 0.8787000179290771\n",
      "Iteration 17440 Training loss 0.003357079578563571 Validation loss 0.011119099333882332 Accuracy 0.8769999742507935\n",
      "Iteration 17450 Training loss 0.004125663079321384 Validation loss 0.011330361478030682 Accuracy 0.8741000294685364\n",
      "Iteration 17460 Training loss 0.004706516861915588 Validation loss 0.010955613106489182 Accuracy 0.8783000111579895\n",
      "Iteration 17470 Training loss 0.007472250610589981 Validation loss 0.011164050549268723 Accuracy 0.8773999810218811\n",
      "Iteration 17480 Training loss 0.0055563924834132195 Validation loss 0.011094722896814346 Accuracy 0.8794000148773193\n",
      "Iteration 17490 Training loss 0.0038700406439602375 Validation loss 0.011331728659570217 Accuracy 0.8748000264167786\n",
      "Iteration 17500 Training loss 0.004243772942572832 Validation loss 0.010943922214210033 Accuracy 0.8794999718666077\n",
      "Iteration 17510 Training loss 0.00451698200777173 Validation loss 0.010908324271440506 Accuracy 0.878600001335144\n",
      "Iteration 17520 Training loss 0.004988241475075483 Validation loss 0.011038445867598057 Accuracy 0.8776999711990356\n",
      "Iteration 17530 Training loss 0.004482638090848923 Validation loss 0.011100812815129757 Accuracy 0.8776999711990356\n",
      "Iteration 17540 Training loss 0.004301580600440502 Validation loss 0.010924055241048336 Accuracy 0.8810999989509583\n",
      "Iteration 17550 Training loss 0.003644809126853943 Validation loss 0.011040900833904743 Accuracy 0.8794999718666077\n",
      "Iteration 17560 Training loss 0.004121028818190098 Validation loss 0.011106987483799458 Accuracy 0.8779000043869019\n",
      "Iteration 17570 Training loss 0.005654511041939259 Validation loss 0.011236073449254036 Accuracy 0.8761000037193298\n",
      "Iteration 17580 Training loss 0.00478727463632822 Validation loss 0.0112913204357028 Accuracy 0.8761000037193298\n",
      "Iteration 17590 Training loss 0.004301054868847132 Validation loss 0.01103147678077221 Accuracy 0.8790000081062317\n",
      "Iteration 17600 Training loss 0.005059519782662392 Validation loss 0.01138691883534193 Accuracy 0.8738999962806702\n",
      "Iteration 17610 Training loss 0.004776443354785442 Validation loss 0.011325654573738575 Accuracy 0.8747000098228455\n",
      "Iteration 17620 Training loss 0.004813250154256821 Validation loss 0.011229428462684155 Accuracy 0.8762999773025513\n",
      "Iteration 17630 Training loss 0.006956419441848993 Validation loss 0.011770226992666721 Accuracy 0.8691999912261963\n",
      "Iteration 17640 Training loss 0.004322033375501633 Validation loss 0.010968673042953014 Accuracy 0.8794999718666077\n",
      "Iteration 17650 Training loss 0.005281292367726564 Validation loss 0.01097952015697956 Accuracy 0.8794000148773193\n",
      "Iteration 17660 Training loss 0.005271213594824076 Validation loss 0.010962877422571182 Accuracy 0.8790000081062317\n",
      "Iteration 17670 Training loss 0.005713821388781071 Validation loss 0.011093081906437874 Accuracy 0.8777999877929688\n",
      "Iteration 17680 Training loss 0.003903069766238332 Validation loss 0.011033598333597183 Accuracy 0.8783000111579895\n",
      "Iteration 17690 Training loss 0.004298739600926638 Validation loss 0.011035914532840252 Accuracy 0.8780999779701233\n",
      "Iteration 17700 Training loss 0.005397244356572628 Validation loss 0.010892757214605808 Accuracy 0.8808000087738037\n",
      "Iteration 17710 Training loss 0.004291332792490721 Validation loss 0.011498869396746159 Accuracy 0.8734999895095825\n",
      "Iteration 17720 Training loss 0.00523487851023674 Validation loss 0.011184428818523884 Accuracy 0.8774999976158142\n",
      "Iteration 17730 Training loss 0.004912314936518669 Validation loss 0.011217059567570686 Accuracy 0.8766999840736389\n",
      "Iteration 17740 Training loss 0.0039084479212760925 Validation loss 0.0111565962433815 Accuracy 0.878600001335144\n",
      "Iteration 17750 Training loss 0.004534131847321987 Validation loss 0.01106222439557314 Accuracy 0.8784000277519226\n",
      "Iteration 17760 Training loss 0.005817769560962915 Validation loss 0.011050600558519363 Accuracy 0.8784000277519226\n",
      "Iteration 17770 Training loss 0.0038841154891997576 Validation loss 0.011003677733242512 Accuracy 0.878600001335144\n",
      "Iteration 17780 Training loss 0.005192545708268881 Validation loss 0.011168127879500389 Accuracy 0.8769999742507935\n",
      "Iteration 17790 Training loss 0.005533376708626747 Validation loss 0.011128395795822144 Accuracy 0.876800000667572\n",
      "Iteration 17800 Training loss 0.005010598339140415 Validation loss 0.011272822506725788 Accuracy 0.8761000037193298\n",
      "Iteration 17810 Training loss 0.005526965018361807 Validation loss 0.011171451769769192 Accuracy 0.8758999705314636\n",
      "Iteration 17820 Training loss 0.005163467489182949 Validation loss 0.011116549372673035 Accuracy 0.8772000074386597\n",
      "Iteration 17830 Training loss 0.004133004695177078 Validation loss 0.011134905740618706 Accuracy 0.8776000142097473\n",
      "Iteration 17840 Training loss 0.004682009108364582 Validation loss 0.011149585247039795 Accuracy 0.8759999871253967\n",
      "Iteration 17850 Training loss 0.005162524059414864 Validation loss 0.01109935063868761 Accuracy 0.8770999908447266\n",
      "Iteration 17860 Training loss 0.004474209155887365 Validation loss 0.011069443076848984 Accuracy 0.8777999877929688\n",
      "Iteration 17870 Training loss 0.005058309994637966 Validation loss 0.011064698919653893 Accuracy 0.8772000074386597\n",
      "Iteration 17880 Training loss 0.003623436903581023 Validation loss 0.011082855984568596 Accuracy 0.8784999847412109\n",
      "Iteration 17890 Training loss 0.004249699413776398 Validation loss 0.011038968339562416 Accuracy 0.8787999749183655\n",
      "Iteration 17900 Training loss 0.006408595480024815 Validation loss 0.011094325222074986 Accuracy 0.8776999711990356\n",
      "Iteration 17910 Training loss 0.004020450636744499 Validation loss 0.011081840842962265 Accuracy 0.878600001335144\n",
      "Iteration 17920 Training loss 0.0037550097331404686 Validation loss 0.011030363850295544 Accuracy 0.8769000172615051\n",
      "Iteration 17930 Training loss 0.00487393606454134 Validation loss 0.011073140427470207 Accuracy 0.8788999915122986\n",
      "Iteration 17940 Training loss 0.004835604690015316 Validation loss 0.011040921323001385 Accuracy 0.8788999915122986\n",
      "Iteration 17950 Training loss 0.005892095156013966 Validation loss 0.011123962700366974 Accuracy 0.8763999938964844\n",
      "Iteration 17960 Training loss 0.003520167898386717 Validation loss 0.010964570567011833 Accuracy 0.8784999847412109\n",
      "Iteration 17970 Training loss 0.0037507270462810993 Validation loss 0.01107755210250616 Accuracy 0.8773999810218811\n",
      "Iteration 17980 Training loss 0.004671999718993902 Validation loss 0.010972193442285061 Accuracy 0.8776999711990356\n",
      "Iteration 17990 Training loss 0.005449929740279913 Validation loss 0.01121295802295208 Accuracy 0.8751000165939331\n",
      "Iteration 18000 Training loss 0.004543977323919535 Validation loss 0.011121941730380058 Accuracy 0.8781999945640564\n",
      "Iteration 18010 Training loss 0.004089721944183111 Validation loss 0.011180616915225983 Accuracy 0.876800000667572\n",
      "Iteration 18020 Training loss 0.0046140095219016075 Validation loss 0.011097355745732784 Accuracy 0.8773999810218811\n",
      "Iteration 18030 Training loss 0.0037545389495790005 Validation loss 0.01100621372461319 Accuracy 0.8776999711990356\n",
      "Iteration 18040 Training loss 0.0050347950309515 Validation loss 0.011216784827411175 Accuracy 0.8769000172615051\n",
      "Iteration 18050 Training loss 0.005668104160577059 Validation loss 0.011055350303649902 Accuracy 0.8776000142097473\n",
      "Iteration 18060 Training loss 0.005227025132626295 Validation loss 0.011377768591046333 Accuracy 0.8747000098228455\n",
      "Iteration 18070 Training loss 0.005422060843557119 Validation loss 0.010947316884994507 Accuracy 0.8787999749183655\n",
      "Iteration 18080 Training loss 0.00445793429389596 Validation loss 0.011099263094365597 Accuracy 0.8777999877929688\n",
      "Iteration 18090 Training loss 0.0043646967969834805 Validation loss 0.010980782099068165 Accuracy 0.8784000277519226\n",
      "Iteration 18100 Training loss 0.003967614378780127 Validation loss 0.011227859184145927 Accuracy 0.8761000037193298\n",
      "Iteration 18110 Training loss 0.00667180772870779 Validation loss 0.011329551227390766 Accuracy 0.8748000264167786\n",
      "Iteration 18120 Training loss 0.005734182894229889 Validation loss 0.01109661627560854 Accuracy 0.8781999945640564\n",
      "Iteration 18130 Training loss 0.005499797873198986 Validation loss 0.011283857747912407 Accuracy 0.8747000098228455\n",
      "Iteration 18140 Training loss 0.004791001323610544 Validation loss 0.010899611748754978 Accuracy 0.879800021648407\n",
      "Iteration 18150 Training loss 0.0045916493982076645 Validation loss 0.010915180668234825 Accuracy 0.8801000118255615\n",
      "Iteration 18160 Training loss 0.004830662626773119 Validation loss 0.011023656465113163 Accuracy 0.8781999945640564\n",
      "Iteration 18170 Training loss 0.0036916539538651705 Validation loss 0.011215539649128914 Accuracy 0.8773000240325928\n",
      "Iteration 18180 Training loss 0.004621794912964106 Validation loss 0.010908545926213264 Accuracy 0.8794000148773193\n",
      "Iteration 18190 Training loss 0.00466688210144639 Validation loss 0.010997316800057888 Accuracy 0.8791000247001648\n",
      "Iteration 18200 Training loss 0.004487798549234867 Validation loss 0.010962316766381264 Accuracy 0.8794000148773193\n",
      "Iteration 18210 Training loss 0.005555455572903156 Validation loss 0.010729118250310421 Accuracy 0.8805000185966492\n",
      "Iteration 18220 Training loss 0.004643512889742851 Validation loss 0.010807720012962818 Accuracy 0.8806999921798706\n",
      "Iteration 18230 Training loss 0.004569153767079115 Validation loss 0.01116294413805008 Accuracy 0.8773999810218811\n",
      "Iteration 18240 Training loss 0.00402295496314764 Validation loss 0.010990318842232227 Accuracy 0.8784999847412109\n",
      "Iteration 18250 Training loss 0.0030304426327347755 Validation loss 0.010852483101189137 Accuracy 0.8809000253677368\n",
      "Iteration 18260 Training loss 0.003368655452504754 Validation loss 0.010892437770962715 Accuracy 0.8794999718666077\n",
      "Iteration 18270 Training loss 0.005024832207709551 Validation loss 0.010939599946141243 Accuracy 0.8805999755859375\n",
      "Iteration 18280 Training loss 0.004794861655682325 Validation loss 0.010955031961202621 Accuracy 0.8798999786376953\n",
      "Iteration 18290 Training loss 0.004954469855874777 Validation loss 0.010977408848702908 Accuracy 0.879800021648407\n",
      "Iteration 18300 Training loss 0.005850458052009344 Validation loss 0.011049670167267323 Accuracy 0.878600001335144\n",
      "Iteration 18310 Training loss 0.004519554786384106 Validation loss 0.010968586429953575 Accuracy 0.8798999786376953\n",
      "Iteration 18320 Training loss 0.0055661797523498535 Validation loss 0.010933661833405495 Accuracy 0.8792999982833862\n",
      "Iteration 18330 Training loss 0.005508347414433956 Validation loss 0.010974946431815624 Accuracy 0.8791999816894531\n",
      "Iteration 18340 Training loss 0.004813448991626501 Validation loss 0.011075098067522049 Accuracy 0.8776999711990356\n",
      "Iteration 18350 Training loss 0.004225982818752527 Validation loss 0.011083235032856464 Accuracy 0.8783000111579895\n",
      "Iteration 18360 Training loss 0.003350913990288973 Validation loss 0.011002616956830025 Accuracy 0.8787999749183655\n",
      "Iteration 18370 Training loss 0.0033566101919859648 Validation loss 0.011107741855084896 Accuracy 0.8770999908447266\n",
      "Iteration 18380 Training loss 0.006352822761982679 Validation loss 0.01106464583426714 Accuracy 0.8776999711990356\n",
      "Iteration 18390 Training loss 0.004171718377619982 Validation loss 0.010906444862484932 Accuracy 0.8794999718666077\n",
      "Iteration 18400 Training loss 0.005652799736708403 Validation loss 0.011032624170184135 Accuracy 0.8773000240325928\n",
      "Iteration 18410 Training loss 0.003995210863649845 Validation loss 0.01099532563239336 Accuracy 0.8779000043869019\n",
      "Iteration 18420 Training loss 0.0046754879876971245 Validation loss 0.01112883910536766 Accuracy 0.8776999711990356\n",
      "Iteration 18430 Training loss 0.005175333470106125 Validation loss 0.011058556847274303 Accuracy 0.8769000172615051\n",
      "Iteration 18440 Training loss 0.004430500790476799 Validation loss 0.010991919785737991 Accuracy 0.8779000043869019\n",
      "Iteration 18450 Training loss 0.005001202691346407 Validation loss 0.010994810611009598 Accuracy 0.8790000081062317\n",
      "Iteration 18460 Training loss 0.00538107194006443 Validation loss 0.011314035393297672 Accuracy 0.8745999932289124\n",
      "Iteration 18470 Training loss 0.004430670291185379 Validation loss 0.011136600747704506 Accuracy 0.8773999810218811\n",
      "Iteration 18480 Training loss 0.003647813107818365 Validation loss 0.011051367036998272 Accuracy 0.8784000277519226\n",
      "Iteration 18490 Training loss 0.003373948624357581 Validation loss 0.01122699212282896 Accuracy 0.8762999773025513\n",
      "Iteration 18500 Training loss 0.0061207786202430725 Validation loss 0.01100695040076971 Accuracy 0.8788999915122986\n",
      "Iteration 18510 Training loss 0.004546539392322302 Validation loss 0.011093685403466225 Accuracy 0.8784999847412109\n",
      "Iteration 18520 Training loss 0.006277458276599646 Validation loss 0.011024330742657185 Accuracy 0.8795999884605408\n",
      "Iteration 18530 Training loss 0.00394492968916893 Validation loss 0.0110703706741333 Accuracy 0.8779000043869019\n",
      "Iteration 18540 Training loss 0.003532582661136985 Validation loss 0.01119154505431652 Accuracy 0.8752999901771545\n",
      "Iteration 18550 Training loss 0.0033348039723932743 Validation loss 0.011048786342144012 Accuracy 0.8783000111579895\n",
      "Iteration 18560 Training loss 0.004177127964794636 Validation loss 0.011010904796421528 Accuracy 0.8776000142097473\n",
      "Iteration 18570 Training loss 0.0035909179132431746 Validation loss 0.010981950908899307 Accuracy 0.8773999810218811\n",
      "Iteration 18580 Training loss 0.004511440172791481 Validation loss 0.011128525249660015 Accuracy 0.8770999908447266\n",
      "Iteration 18590 Training loss 0.0036272697616368532 Validation loss 0.011174993589520454 Accuracy 0.8774999976158142\n",
      "Iteration 18600 Training loss 0.00296110101044178 Validation loss 0.011049967259168625 Accuracy 0.8784999847412109\n",
      "Iteration 18610 Training loss 0.004419973585754633 Validation loss 0.010931193828582764 Accuracy 0.8797000050544739\n",
      "Iteration 18620 Training loss 0.0032493153121322393 Validation loss 0.011035171337425709 Accuracy 0.8766999840736389\n",
      "Iteration 18630 Training loss 0.00437666242942214 Validation loss 0.01109875738620758 Accuracy 0.8763999938964844\n",
      "Iteration 18640 Training loss 0.004301606677472591 Validation loss 0.011209113523364067 Accuracy 0.8762999773025513\n",
      "Iteration 18650 Training loss 0.0049759275279939175 Validation loss 0.010880503803491592 Accuracy 0.880299985408783\n",
      "Iteration 18660 Training loss 0.003677735570818186 Validation loss 0.01105427648872137 Accuracy 0.8770999908447266\n",
      "Iteration 18670 Training loss 0.005275166593492031 Validation loss 0.011220568791031837 Accuracy 0.8754000067710876\n",
      "Iteration 18680 Training loss 0.005403048358857632 Validation loss 0.01116472389549017 Accuracy 0.8762999773025513\n",
      "Iteration 18690 Training loss 0.0047621410340070724 Validation loss 0.010964690707623959 Accuracy 0.8787000179290771\n",
      "Iteration 18700 Training loss 0.0029100547544658184 Validation loss 0.010949724353849888 Accuracy 0.8787000179290771\n",
      "Iteration 18710 Training loss 0.00480008777230978 Validation loss 0.010828190483152866 Accuracy 0.8794999718666077\n",
      "Iteration 18720 Training loss 0.006087972782552242 Validation loss 0.011377881281077862 Accuracy 0.8729000091552734\n",
      "Iteration 18730 Training loss 0.003189471550285816 Validation loss 0.011104014702141285 Accuracy 0.878000020980835\n",
      "Iteration 18740 Training loss 0.003639609320089221 Validation loss 0.010959029197692871 Accuracy 0.8792999982833862\n",
      "Iteration 18750 Training loss 0.004497500602155924 Validation loss 0.011298291385173798 Accuracy 0.8744000196456909\n",
      "Iteration 18760 Training loss 0.004718944430351257 Validation loss 0.011064577847719193 Accuracy 0.8769999742507935\n",
      "Iteration 18770 Training loss 0.005246826913207769 Validation loss 0.011175408028066158 Accuracy 0.8755000233650208\n",
      "Iteration 18780 Training loss 0.006545153446495533 Validation loss 0.011167527176439762 Accuracy 0.8773000240325928\n",
      "Iteration 18790 Training loss 0.0041884202510118484 Validation loss 0.011005017906427383 Accuracy 0.8788999915122986\n",
      "Iteration 18800 Training loss 0.005002496764063835 Validation loss 0.011130241677165031 Accuracy 0.8773000240325928\n",
      "Iteration 18810 Training loss 0.0039020974654704332 Validation loss 0.011108260601758957 Accuracy 0.8773000240325928\n",
      "Iteration 18820 Training loss 0.005538019351661205 Validation loss 0.010923135094344616 Accuracy 0.8792999982833862\n",
      "Iteration 18830 Training loss 0.003756618592888117 Validation loss 0.010955248959362507 Accuracy 0.8805999755859375\n",
      "Iteration 18840 Training loss 0.004334252327680588 Validation loss 0.010871046222746372 Accuracy 0.8804000020027161\n",
      "Iteration 18850 Training loss 0.0036942020524293184 Validation loss 0.011081515811383724 Accuracy 0.8788999915122986\n",
      "Iteration 18860 Training loss 0.004394624847918749 Validation loss 0.010939051397144794 Accuracy 0.8795999884605408\n",
      "Iteration 18870 Training loss 0.004356818273663521 Validation loss 0.01074209250509739 Accuracy 0.8817999958992004\n",
      "Iteration 18880 Training loss 0.004722935147583485 Validation loss 0.011003934778273106 Accuracy 0.8787000179290771\n",
      "Iteration 18890 Training loss 0.00395704573020339 Validation loss 0.010988068766891956 Accuracy 0.8787000179290771\n",
      "Iteration 18900 Training loss 0.004051700700074434 Validation loss 0.01102155540138483 Accuracy 0.8792999982833862\n",
      "Iteration 18910 Training loss 0.0032597272656857967 Validation loss 0.011003631167113781 Accuracy 0.8777999877929688\n",
      "Iteration 18920 Training loss 0.004217032343149185 Validation loss 0.011027541942894459 Accuracy 0.8791000247001648\n",
      "Iteration 18930 Training loss 0.004960417281836271 Validation loss 0.011042458936572075 Accuracy 0.8784000277519226\n",
      "Iteration 18940 Training loss 0.003527127206325531 Validation loss 0.011056728661060333 Accuracy 0.8780999779701233\n",
      "Iteration 18950 Training loss 0.006114519666880369 Validation loss 0.011308255605399609 Accuracy 0.875\n",
      "Iteration 18960 Training loss 0.003869046922773123 Validation loss 0.011300571262836456 Accuracy 0.8762999773025513\n",
      "Iteration 18970 Training loss 0.003673275699838996 Validation loss 0.010949413292109966 Accuracy 0.878600001335144\n",
      "Iteration 18980 Training loss 0.005035376641899347 Validation loss 0.011105628684163094 Accuracy 0.8765000104904175\n",
      "Iteration 18990 Training loss 0.004938699770718813 Validation loss 0.011150146834552288 Accuracy 0.8772000074386597\n",
      "Iteration 19000 Training loss 0.002801191294565797 Validation loss 0.011034083552658558 Accuracy 0.8794999718666077\n",
      "Iteration 19010 Training loss 0.004477249458432198 Validation loss 0.011139622889459133 Accuracy 0.876800000667572\n",
      "Iteration 19020 Training loss 0.003358477959409356 Validation loss 0.010890054516494274 Accuracy 0.8802000284194946\n",
      "Iteration 19030 Training loss 0.005720641929656267 Validation loss 0.010956762358546257 Accuracy 0.8784999847412109\n",
      "Iteration 19040 Training loss 0.004383041989058256 Validation loss 0.011056367307901382 Accuracy 0.8779000043869019\n",
      "Iteration 19050 Training loss 0.004187413956969976 Validation loss 0.010998758487403393 Accuracy 0.8787000179290771\n",
      "Iteration 19060 Training loss 0.004410597030073404 Validation loss 0.01103784330189228 Accuracy 0.8773000240325928\n",
      "Iteration 19070 Training loss 0.005053735338151455 Validation loss 0.011264738626778126 Accuracy 0.8741999864578247\n",
      "Iteration 19080 Training loss 0.006469661835581064 Validation loss 0.011148234829306602 Accuracy 0.8762999773025513\n",
      "Iteration 19090 Training loss 0.004250374622642994 Validation loss 0.011112096719443798 Accuracy 0.8781999945640564\n",
      "Iteration 19100 Training loss 0.004138575401157141 Validation loss 0.011106767691671848 Accuracy 0.8776999711990356\n",
      "Iteration 19110 Training loss 0.005195379722863436 Validation loss 0.011158032342791557 Accuracy 0.8769999742507935\n",
      "Iteration 19120 Training loss 0.004735265858471394 Validation loss 0.011224012821912766 Accuracy 0.8756999969482422\n",
      "Iteration 19130 Training loss 0.00331149366684258 Validation loss 0.011086023412644863 Accuracy 0.8774999976158142\n",
      "Iteration 19140 Training loss 0.004070275463163853 Validation loss 0.01110012922435999 Accuracy 0.8770999908447266\n",
      "Iteration 19150 Training loss 0.0025893973652273417 Validation loss 0.010956223122775555 Accuracy 0.879800021648407\n",
      "Iteration 19160 Training loss 0.0047624544240534306 Validation loss 0.011107231490314007 Accuracy 0.8765000104904175\n",
      "Iteration 19170 Training loss 0.004160172306001186 Validation loss 0.011060941033065319 Accuracy 0.8776999711990356\n",
      "Iteration 19180 Training loss 0.005064279772341251 Validation loss 0.011411145329475403 Accuracy 0.8736000061035156\n",
      "Iteration 19190 Training loss 0.004499547183513641 Validation loss 0.011203018017113209 Accuracy 0.8756999969482422\n",
      "Iteration 19200 Training loss 0.006581033114343882 Validation loss 0.010906750336289406 Accuracy 0.880299985408783\n",
      "Iteration 19210 Training loss 0.003487078007310629 Validation loss 0.010886467061936855 Accuracy 0.8808000087738037\n",
      "Iteration 19220 Training loss 0.00437566265463829 Validation loss 0.01148189976811409 Accuracy 0.8725000023841858\n",
      "Iteration 19230 Training loss 0.003718089312314987 Validation loss 0.011081152595579624 Accuracy 0.878000020980835\n",
      "Iteration 19240 Training loss 0.005542228929698467 Validation loss 0.01088770292699337 Accuracy 0.8809000253677368\n",
      "Iteration 19250 Training loss 0.004597108345478773 Validation loss 0.011196094565093517 Accuracy 0.8756999969482422\n",
      "Iteration 19260 Training loss 0.003573167137801647 Validation loss 0.011010469868779182 Accuracy 0.8784000277519226\n",
      "Iteration 19270 Training loss 0.0041093407198786736 Validation loss 0.011041129939258099 Accuracy 0.8794000148773193\n",
      "Iteration 19280 Training loss 0.004144454840570688 Validation loss 0.010963533073663712 Accuracy 0.8798999786376953\n",
      "Iteration 19290 Training loss 0.0048858895897865295 Validation loss 0.010870602913200855 Accuracy 0.880299985408783\n",
      "Iteration 19300 Training loss 0.0034462683834135532 Validation loss 0.010907583869993687 Accuracy 0.8798999786376953\n",
      "Iteration 19310 Training loss 0.0048925140872597694 Validation loss 0.010862661525607109 Accuracy 0.8792999982833862\n",
      "Iteration 19320 Training loss 0.0040007829666137695 Validation loss 0.0109328031539917 Accuracy 0.8802000284194946\n",
      "Iteration 19330 Training loss 0.003245139727368951 Validation loss 0.010901253670454025 Accuracy 0.8795999884605408\n",
      "Iteration 19340 Training loss 0.0061089820228517056 Validation loss 0.011445406824350357 Accuracy 0.8723999857902527\n",
      "Iteration 19350 Training loss 0.0028895363211631775 Validation loss 0.011018767952919006 Accuracy 0.8773000240325928\n",
      "Iteration 19360 Training loss 0.004168928600847721 Validation loss 0.010937627404928207 Accuracy 0.8791000247001648\n",
      "Iteration 19370 Training loss 0.006482744123786688 Validation loss 0.010949128307402134 Accuracy 0.8791999816894531\n",
      "Iteration 19380 Training loss 0.0036281009670346975 Validation loss 0.011125756427645683 Accuracy 0.8766999840736389\n",
      "Iteration 19390 Training loss 0.005517428740859032 Validation loss 0.011126137338578701 Accuracy 0.8751000165939331\n",
      "Iteration 19400 Training loss 0.003367966739460826 Validation loss 0.010966107249259949 Accuracy 0.8787000179290771\n",
      "Iteration 19410 Training loss 0.00468915980309248 Validation loss 0.01094667986035347 Accuracy 0.8773000240325928\n",
      "Iteration 19420 Training loss 0.00493589648976922 Validation loss 0.011189179494976997 Accuracy 0.8762999773025513\n",
      "Iteration 19430 Training loss 0.005784741137176752 Validation loss 0.01118019875138998 Accuracy 0.8762000203132629\n",
      "Iteration 19440 Training loss 0.004754221066832542 Validation loss 0.011116298846900463 Accuracy 0.8762000203132629\n",
      "Iteration 19450 Training loss 0.00480341911315918 Validation loss 0.010843957774341106 Accuracy 0.8797000050544739\n",
      "Iteration 19460 Training loss 0.00349804456345737 Validation loss 0.010874553583562374 Accuracy 0.8801000118255615\n",
      "Iteration 19470 Training loss 0.006104779429733753 Validation loss 0.011080591008067131 Accuracy 0.8773000240325928\n",
      "Iteration 19480 Training loss 0.005050580482929945 Validation loss 0.010844761505723 Accuracy 0.8792999982833862\n",
      "Iteration 19490 Training loss 0.0038924310356378555 Validation loss 0.01102098636329174 Accuracy 0.878000020980835\n",
      "Iteration 19500 Training loss 0.005501645617187023 Validation loss 0.01096404530107975 Accuracy 0.8784999847412109\n",
      "Iteration 19510 Training loss 0.004989445209503174 Validation loss 0.010862425900995731 Accuracy 0.8798999786376953\n",
      "Iteration 19520 Training loss 0.005186513066291809 Validation loss 0.011040763929486275 Accuracy 0.8769000172615051\n",
      "Iteration 19530 Training loss 0.004014550242573023 Validation loss 0.010841972194612026 Accuracy 0.8810999989509583\n",
      "Iteration 19540 Training loss 0.0032815083395689726 Validation loss 0.01082584634423256 Accuracy 0.8792999982833862\n",
      "Iteration 19550 Training loss 0.004005545750260353 Validation loss 0.01097191497683525 Accuracy 0.8792999982833862\n",
      "Iteration 19560 Training loss 0.005171479657292366 Validation loss 0.010939828120172024 Accuracy 0.878000020980835\n",
      "Iteration 19570 Training loss 0.00422216672450304 Validation loss 0.010915994644165039 Accuracy 0.8794000148773193\n",
      "Iteration 19580 Training loss 0.004465667065232992 Validation loss 0.010861959308385849 Accuracy 0.8809999823570251\n",
      "Iteration 19590 Training loss 0.0024345233105123043 Validation loss 0.010951567441225052 Accuracy 0.8777999877929688\n",
      "Iteration 19600 Training loss 0.005059430375695229 Validation loss 0.011059381067752838 Accuracy 0.8783000111579895\n",
      "Iteration 19610 Training loss 0.004350463394075632 Validation loss 0.010916883125901222 Accuracy 0.8787000179290771\n",
      "Iteration 19620 Training loss 0.0034926822409033775 Validation loss 0.01119903102517128 Accuracy 0.8758000135421753\n",
      "Iteration 19630 Training loss 0.005290825851261616 Validation loss 0.010944897308945656 Accuracy 0.8783000111579895\n",
      "Iteration 19640 Training loss 0.0037994678132236004 Validation loss 0.011174168437719345 Accuracy 0.8769999742507935\n",
      "Iteration 19650 Training loss 0.004616384394466877 Validation loss 0.01098448596894741 Accuracy 0.8792999982833862\n",
      "Iteration 19660 Training loss 0.00508818170055747 Validation loss 0.011285170912742615 Accuracy 0.8747000098228455\n",
      "Iteration 19670 Training loss 0.0038969602901488543 Validation loss 0.011029358021914959 Accuracy 0.878000020980835\n",
      "Iteration 19680 Training loss 0.003773418487980962 Validation loss 0.011027224361896515 Accuracy 0.8777999877929688\n",
      "Iteration 19690 Training loss 0.004472501575946808 Validation loss 0.010947552509605885 Accuracy 0.8790000081062317\n",
      "Iteration 19700 Training loss 0.004243561532348394 Validation loss 0.010991116054356098 Accuracy 0.8774999976158142\n",
      "Iteration 19710 Training loss 0.005710653495043516 Validation loss 0.010897863656282425 Accuracy 0.8788999915122986\n",
      "Iteration 19720 Training loss 0.005623606499284506 Validation loss 0.011103122495114803 Accuracy 0.8776999711990356\n",
      "Iteration 19730 Training loss 0.003407594282180071 Validation loss 0.010962137021124363 Accuracy 0.8784000277519226\n",
      "Iteration 19740 Training loss 0.005802241154015064 Validation loss 0.01092731673270464 Accuracy 0.8801000118255615\n",
      "Iteration 19750 Training loss 0.005015546455979347 Validation loss 0.011067187413573265 Accuracy 0.8762999773025513\n",
      "Iteration 19760 Training loss 0.0038364753127098083 Validation loss 0.0109080970287323 Accuracy 0.879800021648407\n",
      "Iteration 19770 Training loss 0.0042183236218988895 Validation loss 0.011011998169124126 Accuracy 0.8773000240325928\n",
      "Iteration 19780 Training loss 0.0035703766625374556 Validation loss 0.011048718355596066 Accuracy 0.8772000074386597\n",
      "Iteration 19790 Training loss 0.004323485307395458 Validation loss 0.011091536842286587 Accuracy 0.8762999773025513\n",
      "Iteration 19800 Training loss 0.003831863170489669 Validation loss 0.010864710435271263 Accuracy 0.878600001335144\n",
      "Iteration 19810 Training loss 0.004732233472168446 Validation loss 0.010961015708744526 Accuracy 0.8788999915122986\n",
      "Iteration 19820 Training loss 0.003965374082326889 Validation loss 0.01093375775963068 Accuracy 0.8790000081062317\n",
      "Iteration 19830 Training loss 0.005365800112485886 Validation loss 0.011160150170326233 Accuracy 0.8748999834060669\n",
      "Iteration 19840 Training loss 0.004790337290614843 Validation loss 0.011036171577870846 Accuracy 0.8773999810218811\n",
      "Iteration 19850 Training loss 0.0037953320425003767 Validation loss 0.010984541848301888 Accuracy 0.8777999877929688\n",
      "Iteration 19860 Training loss 0.003163793357089162 Validation loss 0.01098222378641367 Accuracy 0.8780999779701233\n",
      "Iteration 19870 Training loss 0.005302405450493097 Validation loss 0.010983710177242756 Accuracy 0.878000020980835\n",
      "Iteration 19880 Training loss 0.003078335663303733 Validation loss 0.010852310806512833 Accuracy 0.8794999718666077\n",
      "Iteration 19890 Training loss 0.0036412086337804794 Validation loss 0.010961715131998062 Accuracy 0.8791999816894531\n",
      "Iteration 19900 Training loss 0.00487314211204648 Validation loss 0.010922690853476524 Accuracy 0.8787000179290771\n",
      "Iteration 19910 Training loss 0.0028886052314192057 Validation loss 0.011146255768835545 Accuracy 0.8774999976158142\n",
      "Iteration 19920 Training loss 0.004261891823261976 Validation loss 0.011160519905388355 Accuracy 0.8769000172615051\n",
      "Iteration 19930 Training loss 0.005395584739744663 Validation loss 0.011019010096788406 Accuracy 0.8776000142097473\n",
      "Iteration 19940 Training loss 0.0037723402492702007 Validation loss 0.010794030502438545 Accuracy 0.879800021648407\n",
      "Iteration 19950 Training loss 0.004439766053110361 Validation loss 0.01092688087373972 Accuracy 0.8783000111579895\n",
      "Iteration 19960 Training loss 0.004722127690911293 Validation loss 0.0108651053160429 Accuracy 0.8798999786376953\n",
      "Iteration 19970 Training loss 0.00368738011457026 Validation loss 0.010998197831213474 Accuracy 0.8776000142097473\n",
      "Iteration 19980 Training loss 0.003365756943821907 Validation loss 0.01088782213628292 Accuracy 0.8798999786376953\n",
      "Iteration 19990 Training loss 0.005127901677042246 Validation loss 0.01105717197060585 Accuracy 0.8777999877929688\n",
      "Iteration 20000 Training loss 0.00369237270206213 Validation loss 0.011216104961931705 Accuracy 0.8752999901771545\n",
      "Iteration 20010 Training loss 0.004341859370470047 Validation loss 0.010879582725465298 Accuracy 0.8799999952316284\n",
      "Iteration 20020 Training loss 0.004879457410424948 Validation loss 0.011008545756340027 Accuracy 0.8776999711990356\n",
      "Iteration 20030 Training loss 0.003613273845985532 Validation loss 0.010830393061041832 Accuracy 0.8809999823570251\n",
      "Iteration 20040 Training loss 0.005522103048861027 Validation loss 0.011079255491495132 Accuracy 0.8766999840736389\n",
      "Iteration 20050 Training loss 0.00392599031329155 Validation loss 0.01111434493213892 Accuracy 0.8763999938964844\n",
      "Iteration 20060 Training loss 0.003923684824258089 Validation loss 0.011013027280569077 Accuracy 0.878600001335144\n",
      "Iteration 20070 Training loss 0.00318800937384367 Validation loss 0.011076104827225208 Accuracy 0.8762999773025513\n",
      "Iteration 20080 Training loss 0.005301114171743393 Validation loss 0.01115213893353939 Accuracy 0.8762999773025513\n",
      "Iteration 20090 Training loss 0.00409019086509943 Validation loss 0.010833336971700191 Accuracy 0.8802000284194946\n",
      "Iteration 20100 Training loss 0.004193602129817009 Validation loss 0.011120019480586052 Accuracy 0.8773000240325928\n",
      "Iteration 20110 Training loss 0.0046794540248811245 Validation loss 0.010954485274851322 Accuracy 0.8772000074386597\n",
      "Iteration 20120 Training loss 0.004336283542215824 Validation loss 0.011381056159734726 Accuracy 0.8744999766349792\n",
      "Iteration 20130 Training loss 0.0040211607702076435 Validation loss 0.010771055705845356 Accuracy 0.8812000155448914\n",
      "Iteration 20140 Training loss 0.0032532752957195044 Validation loss 0.010902090929448605 Accuracy 0.8787000179290771\n",
      "Iteration 20150 Training loss 0.005783516447991133 Validation loss 0.011165241710841656 Accuracy 0.8774999976158142\n",
      "Iteration 20160 Training loss 0.004672408569604158 Validation loss 0.01117935311049223 Accuracy 0.8759999871253967\n",
      "Iteration 20170 Training loss 0.003941555507481098 Validation loss 0.011205053888261318 Accuracy 0.8758999705314636\n",
      "Iteration 20180 Training loss 0.004101897589862347 Validation loss 0.010794485919177532 Accuracy 0.8812999725341797\n",
      "Iteration 20190 Training loss 0.0056901536881923676 Validation loss 0.011064877733588219 Accuracy 0.8776000142097473\n",
      "Iteration 20200 Training loss 0.004894787911325693 Validation loss 0.010947523638606071 Accuracy 0.878000020980835\n",
      "Iteration 20210 Training loss 0.00560626620426774 Validation loss 0.010944091714918613 Accuracy 0.8788999915122986\n",
      "Iteration 20220 Training loss 0.005277986638247967 Validation loss 0.011136200278997421 Accuracy 0.8772000074386597\n",
      "Iteration 20230 Training loss 0.0038527389988303185 Validation loss 0.01119783241301775 Accuracy 0.8741999864578247\n",
      "Iteration 20240 Training loss 0.0035808798857033253 Validation loss 0.011138715781271458 Accuracy 0.8776999711990356\n",
      "Iteration 20250 Training loss 0.004746879916638136 Validation loss 0.011116718873381615 Accuracy 0.8766999840736389\n",
      "Iteration 20260 Training loss 0.0038767321966588497 Validation loss 0.0107137905433774 Accuracy 0.8830000162124634\n",
      "Iteration 20270 Training loss 0.0046858717687428 Validation loss 0.010851878672838211 Accuracy 0.8798999786376953\n",
      "Iteration 20280 Training loss 0.0040110512636601925 Validation loss 0.011062417179346085 Accuracy 0.8769999742507935\n",
      "Iteration 20290 Training loss 0.005666431505233049 Validation loss 0.010860721580684185 Accuracy 0.879800021648407\n",
      "Iteration 20300 Training loss 0.00436405511572957 Validation loss 0.01098203007131815 Accuracy 0.8794999718666077\n",
      "Iteration 20310 Training loss 0.002547718584537506 Validation loss 0.010818525217473507 Accuracy 0.8805999755859375\n",
      "Iteration 20320 Training loss 0.0031879572197794914 Validation loss 0.01111842505633831 Accuracy 0.8761000037193298\n",
      "Iteration 20330 Training loss 0.005844526458531618 Validation loss 0.010954301804304123 Accuracy 0.8773999810218811\n",
      "Iteration 20340 Training loss 0.004612745717167854 Validation loss 0.011057600378990173 Accuracy 0.8762000203132629\n",
      "Iteration 20350 Training loss 0.004227809142321348 Validation loss 0.010844262316823006 Accuracy 0.8790000081062317\n",
      "Iteration 20360 Training loss 0.004215139430016279 Validation loss 0.010752701200544834 Accuracy 0.8801000118255615\n",
      "Iteration 20370 Training loss 0.004950650967657566 Validation loss 0.010949299670755863 Accuracy 0.8795999884605408\n",
      "Iteration 20380 Training loss 0.0036125758197158575 Validation loss 0.010816918686032295 Accuracy 0.8806999921798706\n",
      "Iteration 20390 Training loss 0.004504325333982706 Validation loss 0.011030278168618679 Accuracy 0.8773999810218811\n",
      "Iteration 20400 Training loss 0.005689613521099091 Validation loss 0.011601629666984081 Accuracy 0.8711000084877014\n",
      "Iteration 20410 Training loss 0.0036060011480003595 Validation loss 0.011047638952732086 Accuracy 0.8774999976158142\n",
      "Iteration 20420 Training loss 0.004533288534730673 Validation loss 0.010853354819118977 Accuracy 0.8787000179290771\n",
      "Iteration 20430 Training loss 0.00528554106131196 Validation loss 0.011004457250237465 Accuracy 0.8780999779701233\n",
      "Iteration 20440 Training loss 0.0031628855504095554 Validation loss 0.010969596914947033 Accuracy 0.8784000277519226\n",
      "Iteration 20450 Training loss 0.0031053924467414618 Validation loss 0.010876950807869434 Accuracy 0.8780999779701233\n",
      "Iteration 20460 Training loss 0.0028244787827134132 Validation loss 0.01087978482246399 Accuracy 0.8777999877929688\n",
      "Iteration 20470 Training loss 0.005099536385387182 Validation loss 0.011032529175281525 Accuracy 0.8763999938964844\n",
      "Iteration 20480 Training loss 0.004770517814904451 Validation loss 0.010906643234193325 Accuracy 0.8773999810218811\n",
      "Iteration 20490 Training loss 0.005207891575992107 Validation loss 0.011145845986902714 Accuracy 0.8765000104904175\n",
      "Iteration 20500 Training loss 0.0040299007669091225 Validation loss 0.011008266359567642 Accuracy 0.8773999810218811\n",
      "Iteration 20510 Training loss 0.005005870945751667 Validation loss 0.011014323681592941 Accuracy 0.8776999711990356\n",
      "Iteration 20520 Training loss 0.0025297733955085278 Validation loss 0.01087894570082426 Accuracy 0.8788999915122986\n",
      "Iteration 20530 Training loss 0.003248818451538682 Validation loss 0.0107811214402318 Accuracy 0.8805000185966492\n",
      "Iteration 20540 Training loss 0.0037098496686667204 Validation loss 0.010929599404335022 Accuracy 0.8776000142097473\n",
      "Iteration 20550 Training loss 0.004466884303838015 Validation loss 0.010902472771704197 Accuracy 0.8784000277519226\n",
      "Iteration 20560 Training loss 0.0034079309552907944 Validation loss 0.010917678475379944 Accuracy 0.8784000277519226\n",
      "Iteration 20570 Training loss 0.004277740605175495 Validation loss 0.010969534516334534 Accuracy 0.8784000277519226\n",
      "Iteration 20580 Training loss 0.004541206173598766 Validation loss 0.011031490750610828 Accuracy 0.8776999711990356\n",
      "Iteration 20590 Training loss 0.003894431982189417 Validation loss 0.010900280438363552 Accuracy 0.8799999952316284\n",
      "Iteration 20600 Training loss 0.00475127249956131 Validation loss 0.011008713394403458 Accuracy 0.8769999742507935\n",
      "Iteration 20610 Training loss 0.003975306637585163 Validation loss 0.010933860205113888 Accuracy 0.8776000142097473\n",
      "Iteration 20620 Training loss 0.005522227380424738 Validation loss 0.010795727372169495 Accuracy 0.8795999884605408\n",
      "Iteration 20630 Training loss 0.004373562056571245 Validation loss 0.010816545225679874 Accuracy 0.8791999816894531\n",
      "Iteration 20640 Training loss 0.005570525303483009 Validation loss 0.011194827035069466 Accuracy 0.8769000172615051\n",
      "Iteration 20650 Training loss 0.003223286010324955 Validation loss 0.01079423539340496 Accuracy 0.8809000253677368\n",
      "Iteration 20660 Training loss 0.0042522684670984745 Validation loss 0.010846531018614769 Accuracy 0.8783000111579895\n",
      "Iteration 20670 Training loss 0.0031978264451026917 Validation loss 0.010795299895107746 Accuracy 0.8816999793052673\n",
      "Iteration 20680 Training loss 0.002702720696106553 Validation loss 0.010807941667735577 Accuracy 0.8798999786376953\n",
      "Iteration 20690 Training loss 0.0032932183239609003 Validation loss 0.010856624692678452 Accuracy 0.8799999952316284\n",
      "Iteration 20700 Training loss 0.0044347913935780525 Validation loss 0.010813599452376366 Accuracy 0.8805000185966492\n",
      "Iteration 20710 Training loss 0.003784959204494953 Validation loss 0.010842278599739075 Accuracy 0.8792999982833862\n",
      "Iteration 20720 Training loss 0.005126986186951399 Validation loss 0.011067033745348454 Accuracy 0.876800000667572\n",
      "Iteration 20730 Training loss 0.004117151256650686 Validation loss 0.010916503146290779 Accuracy 0.8787000179290771\n",
      "Iteration 20740 Training loss 0.0043582795187830925 Validation loss 0.011166170239448547 Accuracy 0.8762000203132629\n",
      "Iteration 20750 Training loss 0.0033857724629342556 Validation loss 0.010902144014835358 Accuracy 0.8809000253677368\n",
      "Iteration 20760 Training loss 0.004483378957957029 Validation loss 0.01090674102306366 Accuracy 0.8787999749183655\n",
      "Iteration 20770 Training loss 0.003996346145868301 Validation loss 0.010786731727421284 Accuracy 0.8791000247001648\n",
      "Iteration 20780 Training loss 0.003530933056026697 Validation loss 0.01083282195031643 Accuracy 0.8792999982833862\n",
      "Iteration 20790 Training loss 0.003249469678848982 Validation loss 0.011043907143175602 Accuracy 0.8773000240325928\n",
      "Iteration 20800 Training loss 0.004580900073051453 Validation loss 0.011183192953467369 Accuracy 0.8762999773025513\n",
      "Iteration 20810 Training loss 0.004376889206469059 Validation loss 0.010940290987491608 Accuracy 0.8779000043869019\n",
      "Iteration 20820 Training loss 0.004356262739747763 Validation loss 0.010905593633651733 Accuracy 0.8784000277519226\n",
      "Iteration 20830 Training loss 0.004019924905151129 Validation loss 0.010778769850730896 Accuracy 0.8808000087738037\n",
      "Iteration 20840 Training loss 0.004110146779567003 Validation loss 0.010793586261570454 Accuracy 0.8798999786376953\n",
      "Iteration 20850 Training loss 0.002188581507652998 Validation loss 0.01091140229254961 Accuracy 0.8794999718666077\n",
      "Iteration 20860 Training loss 0.0048776669427752495 Validation loss 0.01085242535918951 Accuracy 0.8788999915122986\n",
      "Iteration 20870 Training loss 0.005117231514304876 Validation loss 0.010904829017817974 Accuracy 0.8784000277519226\n",
      "Iteration 20880 Training loss 0.0033408855088055134 Validation loss 0.010883900336921215 Accuracy 0.8783000111579895\n",
      "Iteration 20890 Training loss 0.005199936684221029 Validation loss 0.010998832993209362 Accuracy 0.8770999908447266\n",
      "Iteration 20900 Training loss 0.004883674439042807 Validation loss 0.010985826142132282 Accuracy 0.8776000142097473\n",
      "Iteration 20910 Training loss 0.004393475130200386 Validation loss 0.010847466066479683 Accuracy 0.879800021648407\n",
      "Iteration 20920 Training loss 0.0033087257761508226 Validation loss 0.010896585881710052 Accuracy 0.8790000081062317\n",
      "Iteration 20930 Training loss 0.0035964546259492636 Validation loss 0.01075388677418232 Accuracy 0.8813999891281128\n",
      "Iteration 20940 Training loss 0.004013603087514639 Validation loss 0.010931342840194702 Accuracy 0.878600001335144\n",
      "Iteration 20950 Training loss 0.003700933186337352 Validation loss 0.010866183787584305 Accuracy 0.8794000148773193\n",
      "Iteration 20960 Training loss 0.004515013191848993 Validation loss 0.010873760096728802 Accuracy 0.8787999749183655\n",
      "Iteration 20970 Training loss 0.004228092264384031 Validation loss 0.01088325958698988 Accuracy 0.8797000050544739\n",
      "Iteration 20980 Training loss 0.002919729333370924 Validation loss 0.010917644016444683 Accuracy 0.8776000142097473\n",
      "Iteration 20990 Training loss 0.00444650137796998 Validation loss 0.011198977008461952 Accuracy 0.8754000067710876\n",
      "Iteration 21000 Training loss 0.0051839775405824184 Validation loss 0.011239414103329182 Accuracy 0.8769000172615051\n",
      "Iteration 21010 Training loss 0.005359957460314035 Validation loss 0.011234370060265064 Accuracy 0.8748000264167786\n",
      "Iteration 21020 Training loss 0.004015573766082525 Validation loss 0.010913215577602386 Accuracy 0.8791999816894531\n",
      "Iteration 21030 Training loss 0.003807689296081662 Validation loss 0.010900129564106464 Accuracy 0.8790000081062317\n",
      "Iteration 21040 Training loss 0.0032043198589235544 Validation loss 0.010938476771116257 Accuracy 0.8787000179290771\n",
      "Iteration 21050 Training loss 0.003701185341924429 Validation loss 0.010945846326649189 Accuracy 0.8780999779701233\n",
      "Iteration 21060 Training loss 0.0057387687265872955 Validation loss 0.010978424921631813 Accuracy 0.878600001335144\n",
      "Iteration 21070 Training loss 0.004858576226979494 Validation loss 0.010871845297515392 Accuracy 0.8797000050544739\n",
      "Iteration 21080 Training loss 0.002544144634157419 Validation loss 0.011283745989203453 Accuracy 0.8748000264167786\n",
      "Iteration 21090 Training loss 0.004799377638846636 Validation loss 0.01081946399062872 Accuracy 0.880299985408783\n",
      "Iteration 21100 Training loss 0.00323828705586493 Validation loss 0.010724409483373165 Accuracy 0.8801000118255615\n",
      "Iteration 21110 Training loss 0.004432664252817631 Validation loss 0.010853108949959278 Accuracy 0.8797000050544739\n",
      "Iteration 21120 Training loss 0.003907185047864914 Validation loss 0.010848339647054672 Accuracy 0.8794000148773193\n",
      "Iteration 21130 Training loss 0.0023544703144580126 Validation loss 0.010811418294906616 Accuracy 0.880299985408783\n",
      "Iteration 21140 Training loss 0.00228419853374362 Validation loss 0.011027198284864426 Accuracy 0.8770999908447266\n",
      "Iteration 21150 Training loss 0.003186923684552312 Validation loss 0.010906647890806198 Accuracy 0.8791000247001648\n",
      "Iteration 21160 Training loss 0.004345362540334463 Validation loss 0.010873313061892986 Accuracy 0.8780999779701233\n",
      "Iteration 21170 Training loss 0.0032642316073179245 Validation loss 0.011064385995268822 Accuracy 0.8772000074386597\n",
      "Iteration 21180 Training loss 0.0037751169875264168 Validation loss 0.01118993666023016 Accuracy 0.8748000264167786\n",
      "Iteration 21190 Training loss 0.0043872082605957985 Validation loss 0.01097647100687027 Accuracy 0.8779000043869019\n",
      "Iteration 21200 Training loss 0.005167591385543346 Validation loss 0.010764334350824356 Accuracy 0.8801000118255615\n",
      "Iteration 21210 Training loss 0.0036491164937615395 Validation loss 0.011106274090707302 Accuracy 0.8763999938964844\n",
      "Iteration 21220 Training loss 0.0036128743086010218 Validation loss 0.011112915351986885 Accuracy 0.8765000104904175\n",
      "Iteration 21230 Training loss 0.002798579167574644 Validation loss 0.010810215026140213 Accuracy 0.8812000155448914\n",
      "Iteration 21240 Training loss 0.00433397339656949 Validation loss 0.011154301464557648 Accuracy 0.8756999969482422\n",
      "Iteration 21250 Training loss 0.003155855229124427 Validation loss 0.010924468748271465 Accuracy 0.878600001335144\n",
      "Iteration 21260 Training loss 0.004774631466716528 Validation loss 0.010883538983762264 Accuracy 0.8787999749183655\n",
      "Iteration 21270 Training loss 0.003918860107660294 Validation loss 0.011011780239641666 Accuracy 0.8776000142097473\n",
      "Iteration 21280 Training loss 0.003731692675501108 Validation loss 0.010756585747003555 Accuracy 0.8805000185966492\n",
      "Iteration 21290 Training loss 0.0035473606549203396 Validation loss 0.011135567910969257 Accuracy 0.8762000203132629\n",
      "Iteration 21300 Training loss 0.004111284390091896 Validation loss 0.010891635902225971 Accuracy 0.8783000111579895\n",
      "Iteration 21310 Training loss 0.003630115184932947 Validation loss 0.010956447571516037 Accuracy 0.8787999749183655\n",
      "Iteration 21320 Training loss 0.004587545525282621 Validation loss 0.011026737280189991 Accuracy 0.8770999908447266\n",
      "Iteration 21330 Training loss 0.002965562278404832 Validation loss 0.010959186591207981 Accuracy 0.878600001335144\n",
      "Iteration 21340 Training loss 0.005452883429825306 Validation loss 0.01092060562223196 Accuracy 0.8788999915122986\n",
      "Iteration 21350 Training loss 0.003224619198590517 Validation loss 0.010920648463070393 Accuracy 0.8784999847412109\n",
      "Iteration 21360 Training loss 0.004242954310029745 Validation loss 0.010891208425164223 Accuracy 0.8790000081062317\n",
      "Iteration 21370 Training loss 0.0039108144119381905 Validation loss 0.011050961911678314 Accuracy 0.8765000104904175\n",
      "Iteration 21380 Training loss 0.00427841255441308 Validation loss 0.01083783246576786 Accuracy 0.8791999816894531\n",
      "Iteration 21390 Training loss 0.0029617834370583296 Validation loss 0.011086401529610157 Accuracy 0.8758000135421753\n",
      "Iteration 21400 Training loss 0.004023849498480558 Validation loss 0.010899250395596027 Accuracy 0.8787000179290771\n",
      "Iteration 21410 Training loss 0.0033288178965449333 Validation loss 0.010924034751951694 Accuracy 0.8777999877929688\n",
      "Iteration 21420 Training loss 0.004376558121293783 Validation loss 0.0110120614990592 Accuracy 0.8773000240325928\n",
      "Iteration 21430 Training loss 0.0047204927541315556 Validation loss 0.010967597365379333 Accuracy 0.8774999976158142\n",
      "Iteration 21440 Training loss 0.003799614030867815 Validation loss 0.010896416381001472 Accuracy 0.8792999982833862\n",
      "Iteration 21450 Training loss 0.0031085005030035973 Validation loss 0.010836911387741566 Accuracy 0.8805999755859375\n",
      "Iteration 21460 Training loss 0.004250553902238607 Validation loss 0.01094732154160738 Accuracy 0.8790000081062317\n",
      "Iteration 21470 Training loss 0.004258356522768736 Validation loss 0.010962210595607758 Accuracy 0.8799999952316284\n",
      "Iteration 21480 Training loss 0.005472744815051556 Validation loss 0.01096141617745161 Accuracy 0.8787999749183655\n",
      "Iteration 21490 Training loss 0.002822150243446231 Validation loss 0.011310440488159657 Accuracy 0.8736000061035156\n",
      "Iteration 21500 Training loss 0.004184970166534185 Validation loss 0.010835983790457249 Accuracy 0.8792999982833862\n",
      "Iteration 21510 Training loss 0.0036755679175257683 Validation loss 0.010872221551835537 Accuracy 0.8798999786376953\n",
      "Iteration 21520 Training loss 0.004630534444004297 Validation loss 0.01093672402203083 Accuracy 0.8777999877929688\n",
      "Iteration 21530 Training loss 0.0033299801871180534 Validation loss 0.010846920311450958 Accuracy 0.8798999786376953\n",
      "Iteration 21540 Training loss 0.003534651128575206 Validation loss 0.011134006083011627 Accuracy 0.8773999810218811\n",
      "Iteration 21550 Training loss 0.005377135705202818 Validation loss 0.010953040793538094 Accuracy 0.8776000142097473\n",
      "Iteration 21560 Training loss 0.005674078594893217 Validation loss 0.011739582754671574 Accuracy 0.8689000010490417\n",
      "Iteration 21570 Training loss 0.0033071911893785 Validation loss 0.010838121175765991 Accuracy 0.8801000118255615\n",
      "Iteration 21580 Training loss 0.004911766387522221 Validation loss 0.010955150239169598 Accuracy 0.8787999749183655\n",
      "Iteration 21590 Training loss 0.003578348783776164 Validation loss 0.010962910950183868 Accuracy 0.8781999945640564\n",
      "Iteration 21600 Training loss 0.0034180444199591875 Validation loss 0.010997206903994083 Accuracy 0.8784999847412109\n",
      "Iteration 21610 Training loss 0.0057621547020971775 Validation loss 0.01097938884049654 Accuracy 0.8787000179290771\n",
      "Iteration 21620 Training loss 0.0016779520083218813 Validation loss 0.011065537109971046 Accuracy 0.8766000270843506\n",
      "Iteration 21630 Training loss 0.0027717812918126583 Validation loss 0.010885465890169144 Accuracy 0.8792999982833862\n",
      "Iteration 21640 Training loss 0.0049086823128163815 Validation loss 0.010751635767519474 Accuracy 0.8791000247001648\n",
      "Iteration 21650 Training loss 0.0038330790121108294 Validation loss 0.010745099745690823 Accuracy 0.8805999755859375\n",
      "Iteration 21660 Training loss 0.002532302401959896 Validation loss 0.010856644250452518 Accuracy 0.880299985408783\n",
      "Iteration 21670 Training loss 0.004619259852916002 Validation loss 0.011035063304007053 Accuracy 0.878000020980835\n",
      "Iteration 21680 Training loss 0.003390936180949211 Validation loss 0.010948307812213898 Accuracy 0.8791000247001648\n",
      "Iteration 21690 Training loss 0.004233130719512701 Validation loss 0.010954814031720161 Accuracy 0.8799999952316284\n",
      "Iteration 21700 Training loss 0.003763638436794281 Validation loss 0.01080553513020277 Accuracy 0.880299985408783\n",
      "Iteration 21710 Training loss 0.004109119530767202 Validation loss 0.011056147515773773 Accuracy 0.8774999976158142\n",
      "Iteration 21720 Training loss 0.005089086014777422 Validation loss 0.011269445531070232 Accuracy 0.8744999766349792\n",
      "Iteration 21730 Training loss 0.0030610745307058096 Validation loss 0.010915886610746384 Accuracy 0.878600001335144\n",
      "Iteration 21740 Training loss 0.003419250715523958 Validation loss 0.010887282900512218 Accuracy 0.8787999749183655\n",
      "Iteration 21750 Training loss 0.0025485630612820387 Validation loss 0.011014539748430252 Accuracy 0.8773999810218811\n",
      "Iteration 21760 Training loss 0.004582513589411974 Validation loss 0.01087680272758007 Accuracy 0.878600001335144\n",
      "Iteration 21770 Training loss 0.0039619822055101395 Validation loss 0.010903132148087025 Accuracy 0.8788999915122986\n",
      "Iteration 21780 Training loss 0.004526890814304352 Validation loss 0.010843979194760323 Accuracy 0.8808000087738037\n",
      "Iteration 21790 Training loss 0.005209827329963446 Validation loss 0.010886534117162228 Accuracy 0.8788999915122986\n",
      "Iteration 21800 Training loss 0.003165723057463765 Validation loss 0.011083289980888367 Accuracy 0.876800000667572\n",
      "Iteration 21810 Training loss 0.004382748156785965 Validation loss 0.010968566872179508 Accuracy 0.8777999877929688\n",
      "Iteration 21820 Training loss 0.004699374083429575 Validation loss 0.010828777216374874 Accuracy 0.879800021648407\n",
      "Iteration 21830 Training loss 0.004291980993002653 Validation loss 0.011009677313268185 Accuracy 0.8773000240325928\n",
      "Iteration 21840 Training loss 0.00360273034311831 Validation loss 0.010776862502098083 Accuracy 0.8798999786376953\n",
      "Iteration 21850 Training loss 0.003642034251242876 Validation loss 0.010928446426987648 Accuracy 0.8779000043869019\n",
      "Iteration 21860 Training loss 0.0038066809065639973 Validation loss 0.010807138867676258 Accuracy 0.8801000118255615\n",
      "Iteration 21870 Training loss 0.00163687311578542 Validation loss 0.010830989107489586 Accuracy 0.880299985408783\n",
      "Iteration 21880 Training loss 0.004531220532953739 Validation loss 0.01107264868915081 Accuracy 0.8759999871253967\n",
      "Iteration 21890 Training loss 0.0045771244913339615 Validation loss 0.01094814296811819 Accuracy 0.8788999915122986\n",
      "Iteration 21900 Training loss 0.0031295171938836575 Validation loss 0.01095524337142706 Accuracy 0.878600001335144\n",
      "Iteration 21910 Training loss 0.0040140338242053986 Validation loss 0.010984239168465137 Accuracy 0.8781999945640564\n",
      "Iteration 21920 Training loss 0.003400468500331044 Validation loss 0.010973312892019749 Accuracy 0.8794999718666077\n",
      "Iteration 21930 Training loss 0.003933954983949661 Validation loss 0.010746154934167862 Accuracy 0.8812000155448914\n",
      "Iteration 21940 Training loss 0.003462085034698248 Validation loss 0.010757890529930592 Accuracy 0.8805999755859375\n",
      "Iteration 21950 Training loss 0.003996740560978651 Validation loss 0.011176764965057373 Accuracy 0.8756999969482422\n",
      "Iteration 21960 Training loss 0.005719550419598818 Validation loss 0.010977206751704216 Accuracy 0.8766999840736389\n",
      "Iteration 21970 Training loss 0.003355359425768256 Validation loss 0.010681086219847202 Accuracy 0.8819000124931335\n",
      "Iteration 21980 Training loss 0.004684479907155037 Validation loss 0.011007026769220829 Accuracy 0.8790000081062317\n",
      "Iteration 21990 Training loss 0.0038404418155550957 Validation loss 0.010842567309737206 Accuracy 0.879800021648407\n",
      "Iteration 22000 Training loss 0.0038444818928837776 Validation loss 0.010782578028738499 Accuracy 0.879800021648407\n",
      "Iteration 22010 Training loss 0.005002129822969437 Validation loss 0.010834526270627975 Accuracy 0.8805000185966492\n",
      "Iteration 22020 Training loss 0.003565547289326787 Validation loss 0.010801258496940136 Accuracy 0.8794000148773193\n",
      "Iteration 22030 Training loss 0.005324266385287046 Validation loss 0.010962733067572117 Accuracy 0.8780999779701233\n",
      "Iteration 22040 Training loss 0.0032969843596220016 Validation loss 0.010835275985300541 Accuracy 0.8791000247001648\n",
      "Iteration 22050 Training loss 0.004726123530417681 Validation loss 0.010672023519873619 Accuracy 0.881600022315979\n",
      "Iteration 22060 Training loss 0.0036302972584962845 Validation loss 0.010747561231255531 Accuracy 0.8819000124931335\n",
      "Iteration 22070 Training loss 0.003364486386999488 Validation loss 0.011025854386389256 Accuracy 0.876800000667572\n",
      "Iteration 22080 Training loss 0.003619208000600338 Validation loss 0.01083073765039444 Accuracy 0.8792999982833862\n",
      "Iteration 22090 Training loss 0.004028817638754845 Validation loss 0.010854722000658512 Accuracy 0.8799999952316284\n",
      "Iteration 22100 Training loss 0.003578402567654848 Validation loss 0.01080858614295721 Accuracy 0.8798999786376953\n",
      "Iteration 22110 Training loss 0.004471173509955406 Validation loss 0.010716110467910767 Accuracy 0.8797000050544739\n",
      "Iteration 22120 Training loss 0.0026983495336025953 Validation loss 0.010745334438979626 Accuracy 0.8805999755859375\n",
      "Iteration 22130 Training loss 0.0025734053924679756 Validation loss 0.010654131881892681 Accuracy 0.8819000124931335\n",
      "Iteration 22140 Training loss 0.002977196127176285 Validation loss 0.010658998973667622 Accuracy 0.8820000290870667\n",
      "Iteration 22150 Training loss 0.005277927499264479 Validation loss 0.010668689385056496 Accuracy 0.8827000260353088\n",
      "Iteration 22160 Training loss 0.0030313110910356045 Validation loss 0.010730566456913948 Accuracy 0.8804000020027161\n",
      "Iteration 22170 Training loss 0.004555431194603443 Validation loss 0.010481919161975384 Accuracy 0.8834999799728394\n",
      "Iteration 22180 Training loss 0.0032877675257623196 Validation loss 0.010672139003872871 Accuracy 0.8813999891281128\n",
      "Iteration 22190 Training loss 0.003917664755135775 Validation loss 0.010777505114674568 Accuracy 0.8791999816894531\n",
      "Iteration 22200 Training loss 0.003358361776918173 Validation loss 0.010797150433063507 Accuracy 0.8790000081062317\n",
      "Iteration 22210 Training loss 0.003509614383801818 Validation loss 0.010878152213990688 Accuracy 0.8787000179290771\n",
      "Iteration 22220 Training loss 0.0047034211456775665 Validation loss 0.010878286324441433 Accuracy 0.8777999877929688\n",
      "Iteration 22230 Training loss 0.004799906630069017 Validation loss 0.010694255121052265 Accuracy 0.8808000087738037\n",
      "Iteration 22240 Training loss 0.0033181076869368553 Validation loss 0.010834679007530212 Accuracy 0.8805000185966492\n",
      "Iteration 22250 Training loss 0.003504296997562051 Validation loss 0.01085701398551464 Accuracy 0.8795999884605408\n",
      "Iteration 22260 Training loss 0.0038046021945774555 Validation loss 0.010799326933920383 Accuracy 0.8791999816894531\n",
      "Iteration 22270 Training loss 0.004540988244116306 Validation loss 0.010846170596778393 Accuracy 0.8794999718666077\n",
      "Iteration 22280 Training loss 0.0045770322903990746 Validation loss 0.010805157013237476 Accuracy 0.879800021648407\n",
      "Iteration 22290 Training loss 0.0030589234083890915 Validation loss 0.011095755733549595 Accuracy 0.8765000104904175\n",
      "Iteration 22300 Training loss 0.004058849066495895 Validation loss 0.011047272011637688 Accuracy 0.8769999742507935\n",
      "Iteration 22310 Training loss 0.0029399553313851357 Validation loss 0.010792231187224388 Accuracy 0.879800021648407\n",
      "Iteration 22320 Training loss 0.004289254080504179 Validation loss 0.010785944759845734 Accuracy 0.8798999786376953\n",
      "Iteration 22330 Training loss 0.00349595258012414 Validation loss 0.010872956365346909 Accuracy 0.8791000247001648\n",
      "Iteration 22340 Training loss 0.004856953397393227 Validation loss 0.010889334604144096 Accuracy 0.8783000111579895\n",
      "Iteration 22350 Training loss 0.004226214252412319 Validation loss 0.0108548728749156 Accuracy 0.8798999786376953\n",
      "Iteration 22360 Training loss 0.0038679910358041525 Validation loss 0.010874291881918907 Accuracy 0.8787999749183655\n",
      "Iteration 22370 Training loss 0.004920457489788532 Validation loss 0.010941706597805023 Accuracy 0.878000020980835\n",
      "Iteration 22380 Training loss 0.0030424045398831367 Validation loss 0.01083520706743002 Accuracy 0.8791999816894531\n",
      "Iteration 22390 Training loss 0.003284099977463484 Validation loss 0.010979601182043552 Accuracy 0.8780999779701233\n",
      "Iteration 22400 Training loss 0.0039705317467451096 Validation loss 0.010724020190536976 Accuracy 0.8798999786376953\n",
      "Iteration 22410 Training loss 0.003529117675498128 Validation loss 0.010645084083080292 Accuracy 0.8823000192642212\n",
      "Iteration 22420 Training loss 0.0029495309572666883 Validation loss 0.010830420069396496 Accuracy 0.8801000118255615\n",
      "Iteration 22430 Training loss 0.0033777281641960144 Validation loss 0.010685854591429234 Accuracy 0.8812000155448914\n",
      "Iteration 22440 Training loss 0.00443592295050621 Validation loss 0.010867039673030376 Accuracy 0.8794000148773193\n",
      "Iteration 22450 Training loss 0.004134746268391609 Validation loss 0.011104670353233814 Accuracy 0.8748000264167786\n",
      "Iteration 22460 Training loss 0.0027604547794908285 Validation loss 0.011043083854019642 Accuracy 0.8770999908447266\n",
      "Iteration 22470 Training loss 0.005571306217461824 Validation loss 0.010939953848719597 Accuracy 0.8787000179290771\n",
      "Iteration 22480 Training loss 0.005708158947527409 Validation loss 0.011035963892936707 Accuracy 0.8772000074386597\n",
      "Iteration 22490 Training loss 0.003819277510046959 Validation loss 0.010981127619743347 Accuracy 0.8780999779701233\n",
      "Iteration 22500 Training loss 0.004646058194339275 Validation loss 0.01089214626699686 Accuracy 0.8795999884605408\n",
      "Iteration 22510 Training loss 0.003382885828614235 Validation loss 0.010803158394992352 Accuracy 0.8806999921798706\n",
      "Iteration 22520 Training loss 0.004482055548578501 Validation loss 0.010782616212964058 Accuracy 0.8808000087738037\n",
      "Iteration 22530 Training loss 0.003995644859969616 Validation loss 0.010727379471063614 Accuracy 0.8813999891281128\n",
      "Iteration 22540 Training loss 0.0026018309872597456 Validation loss 0.010788502171635628 Accuracy 0.880299985408783\n",
      "Iteration 22550 Training loss 0.0024775906931608915 Validation loss 0.010858988389372826 Accuracy 0.8791000247001648\n",
      "Iteration 22560 Training loss 0.004413577727973461 Validation loss 0.010904349386692047 Accuracy 0.8781999945640564\n",
      "Iteration 22570 Training loss 0.004405361600220203 Validation loss 0.010813852772116661 Accuracy 0.8805000185966492\n",
      "Iteration 22580 Training loss 0.003444185946136713 Validation loss 0.011015010066330433 Accuracy 0.8774999976158142\n",
      "Iteration 22590 Training loss 0.00457254983484745 Validation loss 0.01072770543396473 Accuracy 0.8804000020027161\n",
      "Iteration 22600 Training loss 0.0026354845613241196 Validation loss 0.010807951912283897 Accuracy 0.8788999915122986\n",
      "Iteration 22610 Training loss 0.003162476932629943 Validation loss 0.010807895101606846 Accuracy 0.8794999718666077\n",
      "Iteration 22620 Training loss 0.0026896472554653883 Validation loss 0.010877115651965141 Accuracy 0.8788999915122986\n",
      "Iteration 22630 Training loss 0.004571352154016495 Validation loss 0.010795832611620426 Accuracy 0.8805999755859375\n",
      "Iteration 22640 Training loss 0.0037641350645571947 Validation loss 0.010737244039773941 Accuracy 0.8804000020027161\n",
      "Iteration 22650 Training loss 0.003309338353574276 Validation loss 0.010708525776863098 Accuracy 0.879800021648407\n",
      "Iteration 22660 Training loss 0.004516442772001028 Validation loss 0.010860718786716461 Accuracy 0.8788999915122986\n",
      "Iteration 22670 Training loss 0.004706047475337982 Validation loss 0.010827840305864811 Accuracy 0.8801000118255615\n",
      "Iteration 22680 Training loss 0.002789582358673215 Validation loss 0.010858340188860893 Accuracy 0.8788999915122986\n",
      "Iteration 22690 Training loss 0.004137459211051464 Validation loss 0.010817931033670902 Accuracy 0.879800021648407\n",
      "Iteration 22700 Training loss 0.003554730210453272 Validation loss 0.010961468331515789 Accuracy 0.8773999810218811\n",
      "Iteration 22710 Training loss 0.003917793743312359 Validation loss 0.010857973247766495 Accuracy 0.8794000148773193\n",
      "Iteration 22720 Training loss 0.003244070103392005 Validation loss 0.010891011916100979 Accuracy 0.8773999810218811\n",
      "Iteration 22730 Training loss 0.003596542403101921 Validation loss 0.01074755098670721 Accuracy 0.8805999755859375\n",
      "Iteration 22740 Training loss 0.004497826099395752 Validation loss 0.010915156453847885 Accuracy 0.8781999945640564\n",
      "Iteration 22750 Training loss 0.0028238713275641203 Validation loss 0.010805496014654636 Accuracy 0.8773999810218811\n",
      "Iteration 22760 Training loss 0.0022074547596275806 Validation loss 0.010821138508617878 Accuracy 0.8780999779701233\n",
      "Iteration 22770 Training loss 0.002769986167550087 Validation loss 0.010904297232627869 Accuracy 0.8792999982833862\n",
      "Iteration 22780 Training loss 0.0049374038353562355 Validation loss 0.010957433842122555 Accuracy 0.8773999810218811\n",
      "Iteration 22790 Training loss 0.004718812182545662 Validation loss 0.010866598226130009 Accuracy 0.8773000240325928\n",
      "Iteration 22800 Training loss 0.0027041276916861534 Validation loss 0.010983557440340519 Accuracy 0.8766000270843506\n",
      "Iteration 22810 Training loss 0.0036164044868201017 Validation loss 0.011273384094238281 Accuracy 0.8748999834060669\n",
      "Iteration 22820 Training loss 0.003221088321879506 Validation loss 0.010778311640024185 Accuracy 0.8810999989509583\n",
      "Iteration 22830 Training loss 0.005154827143996954 Validation loss 0.010984189808368683 Accuracy 0.8770999908447266\n",
      "Iteration 22840 Training loss 0.004288547672331333 Validation loss 0.01084848865866661 Accuracy 0.8784999847412109\n",
      "Iteration 22850 Training loss 0.003063453594222665 Validation loss 0.010801730677485466 Accuracy 0.8792999982833862\n",
      "Iteration 22860 Training loss 0.0034146595280617476 Validation loss 0.010871930047869682 Accuracy 0.8783000111579895\n",
      "Iteration 22870 Training loss 0.0042644464410841465 Validation loss 0.011188926175236702 Accuracy 0.875\n",
      "Iteration 22880 Training loss 0.00277205603197217 Validation loss 0.010915231890976429 Accuracy 0.878000020980835\n",
      "Iteration 22890 Training loss 0.004376193042844534 Validation loss 0.01085313968360424 Accuracy 0.8787999749183655\n",
      "Iteration 22900 Training loss 0.0033034258522093296 Validation loss 0.01082746870815754 Accuracy 0.878000020980835\n",
      "Iteration 22910 Training loss 0.002762822201475501 Validation loss 0.010838329792022705 Accuracy 0.8790000081062317\n",
      "Iteration 22920 Training loss 0.002966043772175908 Validation loss 0.010763581842184067 Accuracy 0.8805999755859375\n",
      "Iteration 22930 Training loss 0.0029143686406314373 Validation loss 0.010723955929279327 Accuracy 0.8805999755859375\n",
      "Iteration 22940 Training loss 0.003774079727008939 Validation loss 0.010746811516582966 Accuracy 0.8805000185966492\n",
      "Iteration 22950 Training loss 0.0029411143623292446 Validation loss 0.010774913243949413 Accuracy 0.8798999786376953\n",
      "Iteration 22960 Training loss 0.00375003507360816 Validation loss 0.01074338611215353 Accuracy 0.8808000087738037\n",
      "Iteration 22970 Training loss 0.0041666836477816105 Validation loss 0.010726465843617916 Accuracy 0.8804000020027161\n",
      "Iteration 22980 Training loss 0.005008236970752478 Validation loss 0.010835825465619564 Accuracy 0.8794000148773193\n",
      "Iteration 22990 Training loss 0.0038227445911616087 Validation loss 0.010788329876959324 Accuracy 0.8798999786376953\n",
      "Iteration 23000 Training loss 0.004522310569882393 Validation loss 0.01086176186800003 Accuracy 0.8791000247001648\n",
      "Iteration 23010 Training loss 0.004148947540670633 Validation loss 0.011300814338028431 Accuracy 0.8748000264167786\n",
      "Iteration 23020 Training loss 0.004261419177055359 Validation loss 0.010880440473556519 Accuracy 0.878000020980835\n",
      "Iteration 23030 Training loss 0.0036155683919787407 Validation loss 0.01082760002464056 Accuracy 0.8791000247001648\n",
      "Iteration 23040 Training loss 0.0025962903164327145 Validation loss 0.010777223855257034 Accuracy 0.8795999884605408\n",
      "Iteration 23050 Training loss 0.0017813083250075579 Validation loss 0.010855757631361485 Accuracy 0.8777999877929688\n",
      "Iteration 23060 Training loss 0.004107071086764336 Validation loss 0.010868487879633904 Accuracy 0.8805000185966492\n",
      "Iteration 23070 Training loss 0.0033142659813165665 Validation loss 0.010799305513501167 Accuracy 0.8784999847412109\n",
      "Iteration 23080 Training loss 0.0032491942401975393 Validation loss 0.010896952822804451 Accuracy 0.8773000240325928\n",
      "Iteration 23090 Training loss 0.0028296317905187607 Validation loss 0.01070404052734375 Accuracy 0.8815000057220459\n",
      "Iteration 23100 Training loss 0.0033623066265136003 Validation loss 0.01079203374683857 Accuracy 0.8790000081062317\n",
      "Iteration 23110 Training loss 0.0023300580214709044 Validation loss 0.010808674618601799 Accuracy 0.8799999952316284\n",
      "Iteration 23120 Training loss 0.003591765183955431 Validation loss 0.010936046950519085 Accuracy 0.8776999711990356\n",
      "Iteration 23130 Training loss 0.0024688129778951406 Validation loss 0.010852932929992676 Accuracy 0.8797000050544739\n",
      "Iteration 23140 Training loss 0.003667885670438409 Validation loss 0.010870931670069695 Accuracy 0.8792999982833862\n",
      "Iteration 23150 Training loss 0.0052338773384690285 Validation loss 0.010871458798646927 Accuracy 0.878600001335144\n",
      "Iteration 23160 Training loss 0.005125950090587139 Validation loss 0.010699973441660404 Accuracy 0.8809000253677368\n",
      "Iteration 23170 Training loss 0.004563957918435335 Validation loss 0.01082582026720047 Accuracy 0.8779000043869019\n",
      "Iteration 23180 Training loss 0.0030544623732566833 Validation loss 0.010766840539872646 Accuracy 0.8802000284194946\n",
      "Iteration 23190 Training loss 0.0033166599459946156 Validation loss 0.010958584025502205 Accuracy 0.8769000172615051\n",
      "Iteration 23200 Training loss 0.004385525826364756 Validation loss 0.010977302677929401 Accuracy 0.8766000270843506\n",
      "Iteration 23210 Training loss 0.0038196202367544174 Validation loss 0.010970162227749825 Accuracy 0.8779000043869019\n",
      "Iteration 23220 Training loss 0.003105605486780405 Validation loss 0.010761473327875137 Accuracy 0.8787999749183655\n",
      "Iteration 23230 Training loss 0.0025343368761241436 Validation loss 0.010836765170097351 Accuracy 0.8791000247001648\n",
      "Iteration 23240 Training loss 0.00408771401271224 Validation loss 0.010689390823245049 Accuracy 0.8806999921798706\n",
      "Iteration 23250 Training loss 0.003241420490667224 Validation loss 0.010692031122744083 Accuracy 0.8798999786376953\n",
      "Iteration 23260 Training loss 0.003401120426133275 Validation loss 0.010811745189130306 Accuracy 0.8787999749183655\n",
      "Iteration 23270 Training loss 0.0036976265255361795 Validation loss 0.010744302533566952 Accuracy 0.8805999755859375\n",
      "Iteration 23280 Training loss 0.004081014543771744 Validation loss 0.010796226561069489 Accuracy 0.8805999755859375\n",
      "Iteration 23290 Training loss 0.002644493244588375 Validation loss 0.010955095291137695 Accuracy 0.8773999810218811\n",
      "Iteration 23300 Training loss 0.003456750186160207 Validation loss 0.010792396031320095 Accuracy 0.8787000179290771\n",
      "Iteration 23310 Training loss 0.00382618373259902 Validation loss 0.010644977912306786 Accuracy 0.8813999891281128\n",
      "Iteration 23320 Training loss 0.003424231894314289 Validation loss 0.010881369933485985 Accuracy 0.8787999749183655\n",
      "Iteration 23330 Training loss 0.003137976862490177 Validation loss 0.010886386968195438 Accuracy 0.8791000247001648\n",
      "Iteration 23340 Training loss 0.0034597183112055063 Validation loss 0.010813934728503227 Accuracy 0.8787000179290771\n",
      "Iteration 23350 Training loss 0.0037194646429270506 Validation loss 0.010887262411415577 Accuracy 0.8783000111579895\n",
      "Iteration 23360 Training loss 0.003991143312305212 Validation loss 0.010881842114031315 Accuracy 0.8779000043869019\n",
      "Iteration 23370 Training loss 0.0054305363446474075 Validation loss 0.010699479840695858 Accuracy 0.8809000253677368\n",
      "Iteration 23380 Training loss 0.004505838267505169 Validation loss 0.010646459646522999 Accuracy 0.8804000020027161\n",
      "Iteration 23390 Training loss 0.003412905614823103 Validation loss 0.010809081606566906 Accuracy 0.8791999816894531\n",
      "Iteration 23400 Training loss 0.0037821929436177015 Validation loss 0.010807442478835583 Accuracy 0.8791999816894531\n",
      "Iteration 23410 Training loss 0.003420826978981495 Validation loss 0.010695063509047031 Accuracy 0.8809999823570251\n",
      "Iteration 23420 Training loss 0.003707943484187126 Validation loss 0.010779639706015587 Accuracy 0.8798999786376953\n",
      "Iteration 23430 Training loss 0.004364130087196827 Validation loss 0.01057909894734621 Accuracy 0.8815000057220459\n",
      "Iteration 23440 Training loss 0.003132202196866274 Validation loss 0.010708731599152088 Accuracy 0.8805000185966492\n",
      "Iteration 23450 Training loss 0.003681470174342394 Validation loss 0.010804431512951851 Accuracy 0.879800021648407\n",
      "Iteration 23460 Training loss 0.004153801128268242 Validation loss 0.010763047263026237 Accuracy 0.8804000020027161\n",
      "Iteration 23470 Training loss 0.0047377850860357285 Validation loss 0.010760003700852394 Accuracy 0.8787999749183655\n",
      "Iteration 23480 Training loss 0.00382998283021152 Validation loss 0.010877243243157864 Accuracy 0.8784999847412109\n",
      "Iteration 23490 Training loss 0.0035702937748283148 Validation loss 0.010699599981307983 Accuracy 0.880299985408783\n",
      "Iteration 23500 Training loss 0.0038422513753175735 Validation loss 0.010773376561701298 Accuracy 0.8791999816894531\n",
      "Iteration 23510 Training loss 0.0027424138970673084 Validation loss 0.010735484771430492 Accuracy 0.8791999816894531\n",
      "Iteration 23520 Training loss 0.002579102758318186 Validation loss 0.010812925174832344 Accuracy 0.8787000179290771\n",
      "Iteration 23530 Training loss 0.004583112895488739 Validation loss 0.011045795865356922 Accuracy 0.8779000043869019\n",
      "Iteration 23540 Training loss 0.004850414115935564 Validation loss 0.010659745894372463 Accuracy 0.8808000087738037\n",
      "Iteration 23550 Training loss 0.0019492771243676543 Validation loss 0.010780330747365952 Accuracy 0.8795999884605408\n",
      "Iteration 23560 Training loss 0.004304978530853987 Validation loss 0.01089213415980339 Accuracy 0.879800021648407\n",
      "Iteration 23570 Training loss 0.0031476328149437904 Validation loss 0.010869350284337997 Accuracy 0.8794999718666077\n",
      "Iteration 23580 Training loss 0.003958424553275108 Validation loss 0.010951272211968899 Accuracy 0.8776000142097473\n",
      "Iteration 23590 Training loss 0.004577849991619587 Validation loss 0.010679597966372967 Accuracy 0.8809000253677368\n",
      "Iteration 23600 Training loss 0.0030973635148257017 Validation loss 0.01082961168140173 Accuracy 0.8804000020027161\n",
      "Iteration 23610 Training loss 0.004595312289893627 Validation loss 0.010960823856294155 Accuracy 0.878000020980835\n",
      "Iteration 23620 Training loss 0.003997138235718012 Validation loss 0.010822849348187447 Accuracy 0.8797000050544739\n",
      "Iteration 23630 Training loss 0.004416638985276222 Validation loss 0.010861950926482677 Accuracy 0.8788999915122986\n",
      "Iteration 23640 Training loss 0.00398763082921505 Validation loss 0.010852187871932983 Accuracy 0.8794000148773193\n",
      "Iteration 23650 Training loss 0.0021059783175587654 Validation loss 0.010738666169345379 Accuracy 0.8816999793052673\n",
      "Iteration 23660 Training loss 0.002853300189599395 Validation loss 0.01083409134298563 Accuracy 0.8794000148773193\n",
      "Iteration 23670 Training loss 0.003263748250901699 Validation loss 0.010742612183094025 Accuracy 0.8812999725341797\n",
      "Iteration 23680 Training loss 0.005073326174169779 Validation loss 0.01150856539607048 Accuracy 0.875\n",
      "Iteration 23690 Training loss 0.004736397881060839 Validation loss 0.010747154243290424 Accuracy 0.8808000087738037\n",
      "Iteration 23700 Training loss 0.00264829327352345 Validation loss 0.010748225264251232 Accuracy 0.8805999755859375\n",
      "Iteration 23710 Training loss 0.004600380081683397 Validation loss 0.010756469331681728 Accuracy 0.8801000118255615\n",
      "Iteration 23720 Training loss 0.0042285132221877575 Validation loss 0.011069091968238354 Accuracy 0.8777999877929688\n",
      "Iteration 23730 Training loss 0.0036162345204502344 Validation loss 0.010746358893811703 Accuracy 0.8801000118255615\n",
      "Iteration 23740 Training loss 0.0035955056082457304 Validation loss 0.010710547678172588 Accuracy 0.8809000253677368\n",
      "Iteration 23750 Training loss 0.003710393328219652 Validation loss 0.01082488615065813 Accuracy 0.8787000179290771\n",
      "Iteration 23760 Training loss 0.0034331055358052254 Validation loss 0.010745099745690823 Accuracy 0.8810999989509583\n",
      "Iteration 23770 Training loss 0.0034247131552547216 Validation loss 0.010747510939836502 Accuracy 0.8799999952316284\n",
      "Iteration 23780 Training loss 0.00438917288556695 Validation loss 0.010797063820064068 Accuracy 0.8795999884605408\n",
      "Iteration 23790 Training loss 0.004072363022714853 Validation loss 0.010657304897904396 Accuracy 0.8805999755859375\n",
      "Iteration 23800 Training loss 0.004198153503239155 Validation loss 0.010643012821674347 Accuracy 0.8805000185966492\n",
      "Iteration 23810 Training loss 0.002952872309833765 Validation loss 0.010804329067468643 Accuracy 0.879800021648407\n",
      "Iteration 23820 Training loss 0.004286220762878656 Validation loss 0.010591736063361168 Accuracy 0.8824999928474426\n",
      "Iteration 23830 Training loss 0.004548145458102226 Validation loss 0.010876530781388283 Accuracy 0.8792999982833862\n",
      "Iteration 23840 Training loss 0.0035950709134340286 Validation loss 0.010639768093824387 Accuracy 0.8804000020027161\n",
      "Iteration 23850 Training loss 0.004172082524746656 Validation loss 0.010690930299460888 Accuracy 0.8805999755859375\n",
      "Iteration 23860 Training loss 0.003932197578251362 Validation loss 0.010797742754220963 Accuracy 0.8798999786376953\n",
      "Iteration 23870 Training loss 0.0034272244665771723 Validation loss 0.010711638256907463 Accuracy 0.8812999725341797\n",
      "Iteration 23880 Training loss 0.002581017091870308 Validation loss 0.010763444937765598 Accuracy 0.8798999786376953\n",
      "Iteration 23890 Training loss 0.002595332218334079 Validation loss 0.010814527980983257 Accuracy 0.8804000020027161\n",
      "Iteration 23900 Training loss 0.0038429105188697577 Validation loss 0.0107216015458107 Accuracy 0.8805000185966492\n",
      "Iteration 23910 Training loss 0.0031848226208239794 Validation loss 0.010777687653899193 Accuracy 0.8794999718666077\n",
      "Iteration 23920 Training loss 0.0028156531043350697 Validation loss 0.010873442515730858 Accuracy 0.8773999810218811\n",
      "Iteration 23930 Training loss 0.0038825967349112034 Validation loss 0.01083671860396862 Accuracy 0.8795999884605408\n",
      "Iteration 23940 Training loss 0.0029575051739811897 Validation loss 0.010674355551600456 Accuracy 0.8809000253677368\n",
      "Iteration 23950 Training loss 0.0029740151949226856 Validation loss 0.010797131806612015 Accuracy 0.8797000050544739\n",
      "Iteration 23960 Training loss 0.0042793103493750095 Validation loss 0.010826081968843937 Accuracy 0.8808000087738037\n",
      "Iteration 23970 Training loss 0.004120844881981611 Validation loss 0.01063199806958437 Accuracy 0.8813999891281128\n",
      "Iteration 23980 Training loss 0.002708488842472434 Validation loss 0.01070241816341877 Accuracy 0.8805000185966492\n",
      "Iteration 23990 Training loss 0.003416491439566016 Validation loss 0.010791933164000511 Accuracy 0.8791000247001648\n",
      "Iteration 24000 Training loss 0.003608092200011015 Validation loss 0.010947258211672306 Accuracy 0.8776000142097473\n",
      "Iteration 24010 Training loss 0.003981349524110556 Validation loss 0.010712784714996815 Accuracy 0.8805000185966492\n",
      "Iteration 24020 Training loss 0.002158741233870387 Validation loss 0.010679894126951694 Accuracy 0.881600022315979\n",
      "Iteration 24030 Training loss 0.0038268540520220995 Validation loss 0.011003971099853516 Accuracy 0.8777999877929688\n",
      "Iteration 24040 Training loss 0.003466653171926737 Validation loss 0.010883898474276066 Accuracy 0.8791000247001648\n",
      "Iteration 24050 Training loss 0.003219977021217346 Validation loss 0.010814137756824493 Accuracy 0.879800021648407\n",
      "Iteration 24060 Training loss 0.0025601719971746206 Validation loss 0.01071814727038145 Accuracy 0.8802000284194946\n",
      "Iteration 24070 Training loss 0.003988346550613642 Validation loss 0.010808251798152924 Accuracy 0.8792999982833862\n",
      "Iteration 24080 Training loss 0.0037871859967708588 Validation loss 0.010840730741620064 Accuracy 0.8794000148773193\n",
      "Iteration 24090 Training loss 0.0026271489914506674 Validation loss 0.010826287791132927 Accuracy 0.8791999816894531\n",
      "Iteration 24100 Training loss 0.0024133382830768824 Validation loss 0.01111017819494009 Accuracy 0.8748999834060669\n",
      "Iteration 24110 Training loss 0.0023582412395626307 Validation loss 0.010721137747168541 Accuracy 0.8812999725341797\n",
      "Iteration 24120 Training loss 0.0036446433514356613 Validation loss 0.010813982225954533 Accuracy 0.8784999847412109\n",
      "Iteration 24130 Training loss 0.004712159279733896 Validation loss 0.010713769122958183 Accuracy 0.879800021648407\n",
      "Iteration 24140 Training loss 0.004097608849406242 Validation loss 0.010879280045628548 Accuracy 0.8780999779701233\n",
      "Iteration 24150 Training loss 0.0026083020493388176 Validation loss 0.010713249444961548 Accuracy 0.8795999884605408\n",
      "Iteration 24160 Training loss 0.003478375030681491 Validation loss 0.01062766369432211 Accuracy 0.8826000094413757\n",
      "Iteration 24170 Training loss 0.003272009314969182 Validation loss 0.010905866511166096 Accuracy 0.8787999749183655\n",
      "Iteration 24180 Training loss 0.0032854629680514336 Validation loss 0.010842270217835903 Accuracy 0.8792999982833862\n",
      "Iteration 24190 Training loss 0.003674842184409499 Validation loss 0.01073453202843666 Accuracy 0.880299985408783\n",
      "Iteration 24200 Training loss 0.005796148907393217 Validation loss 0.01094429474323988 Accuracy 0.8792999982833862\n",
      "Iteration 24210 Training loss 0.0035059964284300804 Validation loss 0.01075769029557705 Accuracy 0.8791999816894531\n",
      "Iteration 24220 Training loss 0.003968439530581236 Validation loss 0.010785252787172794 Accuracy 0.8799999952316284\n",
      "Iteration 24230 Training loss 0.0030975681729614735 Validation loss 0.010788110084831715 Accuracy 0.880299985408783\n",
      "Iteration 24240 Training loss 0.002925816224887967 Validation loss 0.010758177377283573 Accuracy 0.8806999921798706\n",
      "Iteration 24250 Training loss 0.005084556993097067 Validation loss 0.010674354620277882 Accuracy 0.8804000020027161\n",
      "Iteration 24260 Training loss 0.004024500027298927 Validation loss 0.010815287008881569 Accuracy 0.8787000179290771\n",
      "Iteration 24270 Training loss 0.0038891006261110306 Validation loss 0.010786442086100578 Accuracy 0.8784000277519226\n",
      "Iteration 24280 Training loss 0.0029997886158525944 Validation loss 0.010729345493018627 Accuracy 0.8802000284194946\n",
      "Iteration 24290 Training loss 0.003167266957461834 Validation loss 0.01092088408768177 Accuracy 0.878000020980835\n",
      "Iteration 24300 Training loss 0.004229766316711903 Validation loss 0.010881434194743633 Accuracy 0.8784000277519226\n",
      "Iteration 24310 Training loss 0.0030226882081478834 Validation loss 0.010847707279026508 Accuracy 0.8776999711990356\n",
      "Iteration 24320 Training loss 0.003946712706238031 Validation loss 0.010841606184840202 Accuracy 0.8804000020027161\n",
      "Iteration 24330 Training loss 0.004032287746667862 Validation loss 0.010819770395755768 Accuracy 0.8788999915122986\n",
      "Iteration 24340 Training loss 0.0034494763240218163 Validation loss 0.010891522280871868 Accuracy 0.878000020980835\n",
      "Iteration 24350 Training loss 0.00298857968300581 Validation loss 0.01070054154843092 Accuracy 0.8806999921798706\n",
      "Iteration 24360 Training loss 0.0032606336753815413 Validation loss 0.01078470703214407 Accuracy 0.8788999915122986\n",
      "Iteration 24370 Training loss 0.002925993176177144 Validation loss 0.010755583643913269 Accuracy 0.880299985408783\n",
      "Iteration 24380 Training loss 0.002729669911786914 Validation loss 0.010796572081744671 Accuracy 0.8784999847412109\n",
      "Iteration 24390 Training loss 0.0039062181022018194 Validation loss 0.010792351327836514 Accuracy 0.8797000050544739\n",
      "Iteration 24400 Training loss 0.002903005573898554 Validation loss 0.010657595470547676 Accuracy 0.8812000155448914\n",
      "Iteration 24410 Training loss 0.0029946479480713606 Validation loss 0.010711242444813251 Accuracy 0.8805999755859375\n",
      "Iteration 24420 Training loss 0.005271223373711109 Validation loss 0.010679133236408234 Accuracy 0.8812000155448914\n",
      "Iteration 24430 Training loss 0.0030429495964199305 Validation loss 0.01068718358874321 Accuracy 0.8822000026702881\n",
      "Iteration 24440 Training loss 0.0034689060412347317 Validation loss 0.010609395802021027 Accuracy 0.8819000124931335\n",
      "Iteration 24450 Training loss 0.003400649642571807 Validation loss 0.010868454352021217 Accuracy 0.8794000148773193\n",
      "Iteration 24460 Training loss 0.0024691272992640734 Validation loss 0.010677918791770935 Accuracy 0.8805000185966492\n",
      "Iteration 24470 Training loss 0.003542784135788679 Validation loss 0.010596481151878834 Accuracy 0.8827999830245972\n",
      "Iteration 24480 Training loss 0.002747954800724983 Validation loss 0.010706182569265366 Accuracy 0.8806999921798706\n",
      "Iteration 24490 Training loss 0.005387795623391867 Validation loss 0.010599566623568535 Accuracy 0.8812000155448914\n",
      "Iteration 24500 Training loss 0.002279446227476001 Validation loss 0.010624995455145836 Accuracy 0.8816999793052673\n",
      "Iteration 24510 Training loss 0.003828097600489855 Validation loss 0.010630471631884575 Accuracy 0.8812999725341797\n",
      "Iteration 24520 Training loss 0.0031056483276188374 Validation loss 0.010744067840278149 Accuracy 0.8797000050544739\n",
      "Iteration 24530 Training loss 0.003340363735333085 Validation loss 0.010739346034824848 Accuracy 0.8804000020027161\n",
      "Iteration 24540 Training loss 0.0028869095258414745 Validation loss 0.010851611383259296 Accuracy 0.8787999749183655\n",
      "Iteration 24550 Training loss 0.003681113012135029 Validation loss 0.010760719887912273 Accuracy 0.879800021648407\n",
      "Iteration 24560 Training loss 0.003967453725636005 Validation loss 0.010722700506448746 Accuracy 0.8812999725341797\n",
      "Iteration 24570 Training loss 0.003427986055612564 Validation loss 0.010754062794148922 Accuracy 0.8794000148773193\n",
      "Iteration 24580 Training loss 0.0027617579326033592 Validation loss 0.010710857808589935 Accuracy 0.8801000118255615\n",
      "Iteration 24590 Training loss 0.003507847199216485 Validation loss 0.010662946850061417 Accuracy 0.8810999989509583\n",
      "Iteration 24600 Training loss 0.0029955212958157063 Validation loss 0.010719968006014824 Accuracy 0.8805000185966492\n",
      "Iteration 24610 Training loss 0.0031736295204609632 Validation loss 0.010707629844546318 Accuracy 0.880299985408783\n",
      "Iteration 24620 Training loss 0.0035522289108484983 Validation loss 0.010717295110225677 Accuracy 0.8784999847412109\n",
      "Iteration 24630 Training loss 0.004521049093455076 Validation loss 0.010593397542834282 Accuracy 0.8823000192642212\n",
      "Iteration 24640 Training loss 0.0024420879781246185 Validation loss 0.010592252016067505 Accuracy 0.881600022315979\n",
      "Iteration 24650 Training loss 0.0024032904766499996 Validation loss 0.010768103413283825 Accuracy 0.8790000081062317\n",
      "Iteration 24660 Training loss 0.00334758753888309 Validation loss 0.010708571411669254 Accuracy 0.8798999786376953\n",
      "Iteration 24670 Training loss 0.004774150904268026 Validation loss 0.0107260225340724 Accuracy 0.8791000247001648\n",
      "Iteration 24680 Training loss 0.003635901492089033 Validation loss 0.010869587771594524 Accuracy 0.8773000240325928\n",
      "Iteration 24690 Training loss 0.00313681922852993 Validation loss 0.010616409592330456 Accuracy 0.8812000155448914\n",
      "Iteration 24700 Training loss 0.004428262822329998 Validation loss 0.010992757976055145 Accuracy 0.8779000043869019\n",
      "Iteration 24710 Training loss 0.0028183269314467907 Validation loss 0.010724248364567757 Accuracy 0.8808000087738037\n",
      "Iteration 24720 Training loss 0.0055013312958180904 Validation loss 0.010781367309391499 Accuracy 0.8794000148773193\n",
      "Iteration 24730 Training loss 0.0037071502301841974 Validation loss 0.01083089504390955 Accuracy 0.8791999816894531\n",
      "Iteration 24740 Training loss 0.0030932719819247723 Validation loss 0.010696989484131336 Accuracy 0.8808000087738037\n",
      "Iteration 24750 Training loss 0.0041766418144106865 Validation loss 0.010694427415728569 Accuracy 0.880299985408783\n",
      "Iteration 24760 Training loss 0.003285446437075734 Validation loss 0.010680067352950573 Accuracy 0.8815000057220459\n",
      "Iteration 24770 Training loss 0.00342704844661057 Validation loss 0.011077076196670532 Accuracy 0.8754000067710876\n",
      "Iteration 24780 Training loss 0.0023902314715087414 Validation loss 0.011116266250610352 Accuracy 0.8751000165939331\n",
      "Iteration 24790 Training loss 0.003333030268549919 Validation loss 0.01069458294659853 Accuracy 0.8810999989509583\n",
      "Iteration 24800 Training loss 0.003620981005951762 Validation loss 0.010659524239599705 Accuracy 0.8812000155448914\n",
      "Iteration 24810 Training loss 0.003283466910943389 Validation loss 0.010690362192690372 Accuracy 0.8812999725341797\n",
      "Iteration 24820 Training loss 0.00398355582728982 Validation loss 0.010972117073833942 Accuracy 0.8781999945640564\n",
      "Iteration 24830 Training loss 0.005139235872775316 Validation loss 0.01069231703877449 Accuracy 0.8813999891281128\n",
      "Iteration 24840 Training loss 0.0028791199438273907 Validation loss 0.010845810174942017 Accuracy 0.8783000111579895\n",
      "Iteration 24850 Training loss 0.0029087152797728777 Validation loss 0.01075013168156147 Accuracy 0.879800021648407\n",
      "Iteration 24860 Training loss 0.003828479442745447 Validation loss 0.010810970328748226 Accuracy 0.8784999847412109\n",
      "Iteration 24870 Training loss 0.005431991536170244 Validation loss 0.011079829186201096 Accuracy 0.8748000264167786\n",
      "Iteration 24880 Training loss 0.003497025463730097 Validation loss 0.010894250124692917 Accuracy 0.8777999877929688\n",
      "Iteration 24890 Training loss 0.004115761257708073 Validation loss 0.010951437056064606 Accuracy 0.8779000043869019\n",
      "Iteration 24900 Training loss 0.0032864646054804325 Validation loss 0.010873477905988693 Accuracy 0.8784000277519226\n",
      "Iteration 24910 Training loss 0.003154119709506631 Validation loss 0.01085379533469677 Accuracy 0.8784999847412109\n",
      "Iteration 24920 Training loss 0.0035164994187653065 Validation loss 0.010848618112504482 Accuracy 0.8784000277519226\n",
      "Iteration 24930 Training loss 0.0032824671361595392 Validation loss 0.010942377150058746 Accuracy 0.8766999840736389\n",
      "Iteration 24940 Training loss 0.0034618943464010954 Validation loss 0.010728714987635612 Accuracy 0.8802000284194946\n",
      "Iteration 24950 Training loss 0.002366953995078802 Validation loss 0.010698105208575726 Accuracy 0.880299985408783\n",
      "Iteration 24960 Training loss 0.0033349506556987762 Validation loss 0.010929240845143795 Accuracy 0.8773000240325928\n",
      "Iteration 24970 Training loss 0.003916224464774132 Validation loss 0.01088155247271061 Accuracy 0.8791000247001648\n",
      "Iteration 24980 Training loss 0.003036786336451769 Validation loss 0.011018846184015274 Accuracy 0.876800000667572\n",
      "Iteration 24990 Training loss 0.004051358439028263 Validation loss 0.010825442150235176 Accuracy 0.8805000185966492\n",
      "Iteration 25000 Training loss 0.0026305054780095816 Validation loss 0.010836198925971985 Accuracy 0.8792999982833862\n",
      "Iteration 25010 Training loss 0.002715596929192543 Validation loss 0.010817861184477806 Accuracy 0.879800021648407\n",
      "Iteration 25020 Training loss 0.0049916598945856094 Validation loss 0.010731014423072338 Accuracy 0.8817999958992004\n",
      "Iteration 25030 Training loss 0.0027560098096728325 Validation loss 0.01073309313505888 Accuracy 0.8809000253677368\n",
      "Iteration 25040 Training loss 0.0036029876209795475 Validation loss 0.010754367336630821 Accuracy 0.8809000253677368\n",
      "Iteration 25050 Training loss 0.0029308979865163565 Validation loss 0.010778897441923618 Accuracy 0.8801000118255615\n",
      "Iteration 25060 Training loss 0.00266623473726213 Validation loss 0.01075421180576086 Accuracy 0.8799999952316284\n",
      "Iteration 25070 Training loss 0.002709638327360153 Validation loss 0.0107526034116745 Accuracy 0.8801000118255615\n",
      "Iteration 25080 Training loss 0.0022613569162786007 Validation loss 0.010777754709124565 Accuracy 0.8798999786376953\n",
      "Iteration 25090 Training loss 0.003978780936449766 Validation loss 0.010749495588243008 Accuracy 0.8802000284194946\n",
      "Iteration 25100 Training loss 0.0019611248280853033 Validation loss 0.01075306162238121 Accuracy 0.8794999718666077\n",
      "Iteration 25110 Training loss 0.003396723186597228 Validation loss 0.010655022226274014 Accuracy 0.8809999823570251\n",
      "Iteration 25120 Training loss 0.0027980150189250708 Validation loss 0.010791818611323833 Accuracy 0.8794000148773193\n",
      "Iteration 25130 Training loss 0.00344852264970541 Validation loss 0.010710585862398148 Accuracy 0.8802000284194946\n",
      "Iteration 25140 Training loss 0.00297727738507092 Validation loss 0.010626171715557575 Accuracy 0.8830999732017517\n",
      "Iteration 25150 Training loss 0.0021992928814142942 Validation loss 0.010704862885177135 Accuracy 0.8804000020027161\n",
      "Iteration 25160 Training loss 0.003387472126632929 Validation loss 0.010745563544332981 Accuracy 0.879800021648407\n",
      "Iteration 25170 Training loss 0.004150535445660353 Validation loss 0.01075688935816288 Accuracy 0.8810999989509583\n",
      "Iteration 25180 Training loss 0.0028456563595682383 Validation loss 0.010754606686532497 Accuracy 0.8790000081062317\n",
      "Iteration 25190 Training loss 0.0039001295808702707 Validation loss 0.010775384493172169 Accuracy 0.8802000284194946\n",
      "Iteration 25200 Training loss 0.0031498721800744534 Validation loss 0.010749580338597298 Accuracy 0.8806999921798706\n",
      "Iteration 25210 Training loss 0.002555638551712036 Validation loss 0.010758635587990284 Accuracy 0.8801000118255615\n",
      "Iteration 25220 Training loss 0.0031173592433333397 Validation loss 0.010866519995033741 Accuracy 0.8787000179290771\n",
      "Iteration 25230 Training loss 0.003299399046227336 Validation loss 0.0108131542801857 Accuracy 0.8787999749183655\n",
      "Iteration 25240 Training loss 0.003720414126291871 Validation loss 0.010677574202418327 Accuracy 0.8805999755859375\n",
      "Iteration 25250 Training loss 0.0033146401401609182 Validation loss 0.010669149458408356 Accuracy 0.8812999725341797\n",
      "Iteration 25260 Training loss 0.004019261337816715 Validation loss 0.010672674514353275 Accuracy 0.8808000087738037\n",
      "Iteration 25270 Training loss 0.0020642809104174376 Validation loss 0.010763509199023247 Accuracy 0.8801000118255615\n",
      "Iteration 25280 Training loss 0.0025561160873621702 Validation loss 0.010665429756045341 Accuracy 0.8813999891281128\n",
      "Iteration 25290 Training loss 0.003573597874492407 Validation loss 0.010733854956924915 Accuracy 0.8808000087738037\n",
      "Iteration 25300 Training loss 0.0038453801535069942 Validation loss 0.010737672448158264 Accuracy 0.880299985408783\n",
      "Iteration 25310 Training loss 0.002890166360884905 Validation loss 0.010752670466899872 Accuracy 0.8808000087738037\n",
      "Iteration 25320 Training loss 0.003007755381986499 Validation loss 0.010916311293840408 Accuracy 0.8776000142097473\n",
      "Iteration 25330 Training loss 0.003325483063235879 Validation loss 0.010655902326107025 Accuracy 0.8805999755859375\n",
      "Iteration 25340 Training loss 0.0033045015297830105 Validation loss 0.010912194848060608 Accuracy 0.878000020980835\n",
      "Iteration 25350 Training loss 0.003551242407411337 Validation loss 0.011038276366889477 Accuracy 0.8765000104904175\n",
      "Iteration 25360 Training loss 0.0041513689793646336 Validation loss 0.010911875404417515 Accuracy 0.8772000074386597\n",
      "Iteration 25370 Training loss 0.00227625435218215 Validation loss 0.010776028037071228 Accuracy 0.879800021648407\n",
      "Iteration 25380 Training loss 0.003905311692506075 Validation loss 0.010888039134442806 Accuracy 0.8788999915122986\n",
      "Iteration 25390 Training loss 0.003033116925507784 Validation loss 0.010738412849605083 Accuracy 0.8794000148773193\n",
      "Iteration 25400 Training loss 0.0025691722985357046 Validation loss 0.010739845223724842 Accuracy 0.8797000050544739\n",
      "Iteration 25410 Training loss 0.0031261942349374294 Validation loss 0.010711044073104858 Accuracy 0.8805000185966492\n",
      "Iteration 25420 Training loss 0.0032731196843087673 Validation loss 0.010765958577394485 Accuracy 0.8804000020027161\n",
      "Iteration 25430 Training loss 0.0029956987127661705 Validation loss 0.01078114379197359 Accuracy 0.8791999816894531\n",
      "Iteration 25440 Training loss 0.003502438310533762 Validation loss 0.010851385071873665 Accuracy 0.8787000179290771\n",
      "Iteration 25450 Training loss 0.002917087869718671 Validation loss 0.010717292316257954 Accuracy 0.8801000118255615\n",
      "Iteration 25460 Training loss 0.0025524424854665995 Validation loss 0.010889451019465923 Accuracy 0.8781999945640564\n",
      "Iteration 25470 Training loss 0.0026337625458836555 Validation loss 0.010743164457380772 Accuracy 0.8797000050544739\n",
      "Iteration 25480 Training loss 0.003126518800854683 Validation loss 0.010746395215392113 Accuracy 0.8808000087738037\n",
      "Iteration 25490 Training loss 0.0028355824761092663 Validation loss 0.010751999914646149 Accuracy 0.8801000118255615\n",
      "Iteration 25500 Training loss 0.0029594767838716507 Validation loss 0.010724387131631374 Accuracy 0.8812000155448914\n",
      "Iteration 25510 Training loss 0.0034564968664199114 Validation loss 0.010714239440858364 Accuracy 0.8809999823570251\n",
      "Iteration 25520 Training loss 0.0030325711704790592 Validation loss 0.010771401226520538 Accuracy 0.8797000050544739\n",
      "Iteration 25530 Training loss 0.003427335061132908 Validation loss 0.010713758878409863 Accuracy 0.8805000185966492\n",
      "Iteration 25540 Training loss 0.0039383405819535255 Validation loss 0.010867157950997353 Accuracy 0.8783000111579895\n",
      "Iteration 25550 Training loss 0.0026525547727942467 Validation loss 0.010694651864469051 Accuracy 0.8808000087738037\n",
      "Iteration 25560 Training loss 0.003556638490408659 Validation loss 0.010723797604441643 Accuracy 0.8812000155448914\n",
      "Iteration 25570 Training loss 0.004125694744288921 Validation loss 0.010851262137293816 Accuracy 0.8772000074386597\n",
      "Iteration 25580 Training loss 0.0029130997136235237 Validation loss 0.010683408007025719 Accuracy 0.8809999823570251\n",
      "Iteration 25590 Training loss 0.004261084366589785 Validation loss 0.010627275332808495 Accuracy 0.8812000155448914\n",
      "Iteration 25600 Training loss 0.0035974716302007437 Validation loss 0.010781025514006615 Accuracy 0.878000020980835\n",
      "Iteration 25610 Training loss 0.0033466338645666838 Validation loss 0.010725622996687889 Accuracy 0.8809000253677368\n",
      "Iteration 25620 Training loss 0.0044292486272752285 Validation loss 0.01073482632637024 Accuracy 0.8809999823570251\n",
      "Iteration 25630 Training loss 0.0020824261009693146 Validation loss 0.0108276242390275 Accuracy 0.8799999952316284\n",
      "Iteration 25640 Training loss 0.003409788478165865 Validation loss 0.010785241611301899 Accuracy 0.8804000020027161\n",
      "Iteration 25650 Training loss 0.0024215837474912405 Validation loss 0.010682770982384682 Accuracy 0.881600022315979\n",
      "Iteration 25660 Training loss 0.00445959810167551 Validation loss 0.01089923083782196 Accuracy 0.8787999749183655\n",
      "Iteration 25670 Training loss 0.0034203720279037952 Validation loss 0.010849058628082275 Accuracy 0.8794999718666077\n",
      "Iteration 25680 Training loss 0.0032230226788669825 Validation loss 0.010733095929026604 Accuracy 0.8799999952316284\n",
      "Iteration 25690 Training loss 0.0032474270556122065 Validation loss 0.010872088372707367 Accuracy 0.878000020980835\n",
      "Iteration 25700 Training loss 0.004380593542009592 Validation loss 0.01079495344310999 Accuracy 0.8790000081062317\n",
      "Iteration 25710 Training loss 0.004393252078443766 Validation loss 0.010716941207647324 Accuracy 0.8816999793052673\n",
      "Iteration 25720 Training loss 0.0046542854979634285 Validation loss 0.010928485542535782 Accuracy 0.8780999779701233\n",
      "Iteration 25730 Training loss 0.0028455364517867565 Validation loss 0.010791977867484093 Accuracy 0.8797000050544739\n",
      "Iteration 25740 Training loss 0.00327383354306221 Validation loss 0.010789881460368633 Accuracy 0.8806999921798706\n",
      "Iteration 25750 Training loss 0.003717562882229686 Validation loss 0.01085400115698576 Accuracy 0.8784999847412109\n",
      "Iteration 25760 Training loss 0.0024613551795482635 Validation loss 0.011040168814361095 Accuracy 0.8763999938964844\n",
      "Iteration 25770 Training loss 0.0026108711026608944 Validation loss 0.010928647592663765 Accuracy 0.8779000043869019\n",
      "Iteration 25780 Training loss 0.0034896815195679665 Validation loss 0.01086627971380949 Accuracy 0.8784000277519226\n",
      "Iteration 25790 Training loss 0.002944026608020067 Validation loss 0.01072630099952221 Accuracy 0.8809999823570251\n",
      "Iteration 25800 Training loss 0.00319773331284523 Validation loss 0.010764524340629578 Accuracy 0.8787000179290771\n",
      "Iteration 25810 Training loss 0.003867332823574543 Validation loss 0.010727046057581902 Accuracy 0.8804000020027161\n",
      "Iteration 25820 Training loss 0.0030921073630452156 Validation loss 0.011067025363445282 Accuracy 0.878000020980835\n",
      "Iteration 25830 Training loss 0.0032077226787805557 Validation loss 0.010722138918936253 Accuracy 0.8795999884605408\n",
      "Iteration 25840 Training loss 0.002735293935984373 Validation loss 0.010772879235446453 Accuracy 0.8801000118255615\n",
      "Iteration 25850 Training loss 0.002929960610345006 Validation loss 0.0107467882335186 Accuracy 0.8805999755859375\n",
      "Iteration 25860 Training loss 0.003631609259173274 Validation loss 0.010748185217380524 Accuracy 0.879800021648407\n",
      "Iteration 25870 Training loss 0.0032668656203895807 Validation loss 0.010731750167906284 Accuracy 0.880299985408783\n",
      "Iteration 25880 Training loss 0.00327982846647501 Validation loss 0.010800471529364586 Accuracy 0.8787000179290771\n",
      "Iteration 25890 Training loss 0.003381428774446249 Validation loss 0.01063032727688551 Accuracy 0.8810999989509583\n",
      "Iteration 25900 Training loss 0.004602017812430859 Validation loss 0.010679946281015873 Accuracy 0.8806999921798706\n",
      "Iteration 25910 Training loss 0.0028017570730298758 Validation loss 0.010947013273835182 Accuracy 0.8790000081062317\n",
      "Iteration 25920 Training loss 0.0033614321146160364 Validation loss 0.01089530996978283 Accuracy 0.8772000074386597\n",
      "Iteration 25930 Training loss 0.0033461053390055895 Validation loss 0.010712264105677605 Accuracy 0.880299985408783\n",
      "Iteration 25940 Training loss 0.0035039493814110756 Validation loss 0.010705964639782906 Accuracy 0.8816999793052673\n",
      "Iteration 25950 Training loss 0.0031787545885890722 Validation loss 0.010762746445834637 Accuracy 0.8792999982833862\n",
      "Iteration 25960 Training loss 0.0036052612122148275 Validation loss 0.010762383230030537 Accuracy 0.8791999816894531\n",
      "Iteration 25970 Training loss 0.0035501664970070124 Validation loss 0.010914048179984093 Accuracy 0.8783000111579895\n",
      "Iteration 25980 Training loss 0.004111853428184986 Validation loss 0.011079500429332256 Accuracy 0.8763999938964844\n",
      "Iteration 25990 Training loss 0.0036255510058254004 Validation loss 0.010804631747305393 Accuracy 0.8799999952316284\n",
      "Iteration 26000 Training loss 0.0038488213904201984 Validation loss 0.010746108368039131 Accuracy 0.8794999718666077\n",
      "Iteration 26010 Training loss 0.0025462235789746046 Validation loss 0.010783971287310123 Accuracy 0.879800021648407\n",
      "Iteration 26020 Training loss 0.003153023310005665 Validation loss 0.010991360992193222 Accuracy 0.8765000104904175\n",
      "Iteration 26030 Training loss 0.0030582742765545845 Validation loss 0.010741284117102623 Accuracy 0.8798999786376953\n",
      "Iteration 26040 Training loss 0.0034424057230353355 Validation loss 0.010770456865429878 Accuracy 0.8788999915122986\n",
      "Iteration 26050 Training loss 0.002529220189899206 Validation loss 0.010944907553493977 Accuracy 0.8774999976158142\n",
      "Iteration 26060 Training loss 0.004126726184040308 Validation loss 0.01078284066170454 Accuracy 0.8802000284194946\n",
      "Iteration 26070 Training loss 0.003841887228190899 Validation loss 0.010848846286535263 Accuracy 0.8781999945640564\n",
      "Iteration 26080 Training loss 0.002028334653005004 Validation loss 0.010729682631790638 Accuracy 0.8805000185966492\n",
      "Iteration 26090 Training loss 0.0033754666801542044 Validation loss 0.01073822844773531 Accuracy 0.8809999823570251\n",
      "Iteration 26100 Training loss 0.003205101238563657 Validation loss 0.010748631320893764 Accuracy 0.8795999884605408\n",
      "Iteration 26110 Training loss 0.003405942814424634 Validation loss 0.010729731060564518 Accuracy 0.8809999823570251\n",
      "Iteration 26120 Training loss 0.003581054974347353 Validation loss 0.010817182250320911 Accuracy 0.8787999749183655\n",
      "Iteration 26130 Training loss 0.0029854190070182085 Validation loss 0.010774084366858006 Accuracy 0.8794999718666077\n",
      "Iteration 26140 Training loss 0.0030491603538393974 Validation loss 0.01070939376950264 Accuracy 0.8815000057220459\n",
      "Iteration 26150 Training loss 0.003676321357488632 Validation loss 0.01079890038818121 Accuracy 0.8792999982833862\n",
      "Iteration 26160 Training loss 0.0034200982190668583 Validation loss 0.010762670077383518 Accuracy 0.8809000253677368\n",
      "Iteration 26170 Training loss 0.0040186201222240925 Validation loss 0.010857678018510342 Accuracy 0.8788999915122986\n",
      "Iteration 26180 Training loss 0.0026303718332201242 Validation loss 0.010699460282921791 Accuracy 0.8809999823570251\n",
      "Iteration 26190 Training loss 0.002526952186599374 Validation loss 0.01090955175459385 Accuracy 0.8776000142097473\n",
      "Iteration 26200 Training loss 0.003422734560444951 Validation loss 0.010778787545859814 Accuracy 0.8790000081062317\n",
      "Iteration 26210 Training loss 0.00396739924326539 Validation loss 0.010767793282866478 Accuracy 0.879800021648407\n",
      "Iteration 26220 Training loss 0.0032644907478243113 Validation loss 0.010599665343761444 Accuracy 0.8817999958992004\n",
      "Iteration 26230 Training loss 0.002620716579258442 Validation loss 0.010749557986855507 Accuracy 0.8802000284194946\n",
      "Iteration 26240 Training loss 0.0028359091375023127 Validation loss 0.010828333906829357 Accuracy 0.878000020980835\n",
      "Iteration 26250 Training loss 0.0031792125664651394 Validation loss 0.01073927991092205 Accuracy 0.8792999982833862\n",
      "Iteration 26260 Training loss 0.003130960511043668 Validation loss 0.010933210141956806 Accuracy 0.8772000074386597\n",
      "Iteration 26270 Training loss 0.004146746825426817 Validation loss 0.010772155597805977 Accuracy 0.8798999786376953\n",
      "Iteration 26280 Training loss 0.0021002741996198893 Validation loss 0.010767574422061443 Accuracy 0.8797000050544739\n",
      "Iteration 26290 Training loss 0.0029135234653949738 Validation loss 0.01081257313489914 Accuracy 0.8788999915122986\n",
      "Iteration 26300 Training loss 0.003703565802425146 Validation loss 0.01079318393021822 Accuracy 0.8794000148773193\n",
      "Iteration 26310 Training loss 0.003123904811218381 Validation loss 0.010771318338811398 Accuracy 0.8804000020027161\n",
      "Iteration 26320 Training loss 0.004171864595264196 Validation loss 0.010781249031424522 Accuracy 0.8792999982833862\n",
      "Iteration 26330 Training loss 0.00497790239751339 Validation loss 0.01070495881140232 Accuracy 0.8806999921798706\n",
      "Iteration 26340 Training loss 0.0028271982446312904 Validation loss 0.010869688354432583 Accuracy 0.8783000111579895\n",
      "Iteration 26350 Training loss 0.0036283445078879595 Validation loss 0.010917456820607185 Accuracy 0.8773000240325928\n",
      "Iteration 26360 Training loss 0.003630266524851322 Validation loss 0.010757934302091599 Accuracy 0.8784999847412109\n",
      "Iteration 26370 Training loss 0.0036636688746511936 Validation loss 0.010681958869099617 Accuracy 0.879800021648407\n",
      "Iteration 26380 Training loss 0.0024330951273441315 Validation loss 0.010787531733512878 Accuracy 0.8788999915122986\n",
      "Iteration 26390 Training loss 0.003230736358091235 Validation loss 0.010709969326853752 Accuracy 0.8802000284194946\n",
      "Iteration 26400 Training loss 0.003974112682044506 Validation loss 0.01059884112328291 Accuracy 0.8812000155448914\n",
      "Iteration 26410 Training loss 0.003549005836248398 Validation loss 0.01068731676787138 Accuracy 0.8799999952316284\n",
      "Iteration 26420 Training loss 0.0029394642915576696 Validation loss 0.010647596791386604 Accuracy 0.882099986076355\n",
      "Iteration 26430 Training loss 0.002906834241002798 Validation loss 0.010627887211740017 Accuracy 0.8798999786376953\n",
      "Iteration 26440 Training loss 0.002840079367160797 Validation loss 0.011020826175808907 Accuracy 0.8755000233650208\n",
      "Iteration 26450 Training loss 0.004420832265168428 Validation loss 0.010726331733167171 Accuracy 0.8806999921798706\n",
      "Iteration 26460 Training loss 0.0023559851106256247 Validation loss 0.010895036160945892 Accuracy 0.8776999711990356\n",
      "Iteration 26470 Training loss 0.0030741370283067226 Validation loss 0.010802133940160275 Accuracy 0.8799999952316284\n",
      "Iteration 26480 Training loss 0.004480879288166761 Validation loss 0.010811567306518555 Accuracy 0.878600001335144\n",
      "Iteration 26490 Training loss 0.0019689344335347414 Validation loss 0.010741039179265499 Accuracy 0.880299985408783\n",
      "Iteration 26500 Training loss 0.004197755828499794 Validation loss 0.010812211781740189 Accuracy 0.8784000277519226\n",
      "Iteration 26510 Training loss 0.003139516105875373 Validation loss 0.010810199193656445 Accuracy 0.8797000050544739\n",
      "Iteration 26520 Training loss 0.0037832818925380707 Validation loss 0.01064495649188757 Accuracy 0.8815000057220459\n",
      "Iteration 26530 Training loss 0.0030276933684945107 Validation loss 0.010772841982543468 Accuracy 0.8794999718666077\n",
      "Iteration 26540 Training loss 0.004097117111086845 Validation loss 0.011035057716071606 Accuracy 0.8765000104904175\n",
      "Iteration 26550 Training loss 0.00362567906267941 Validation loss 0.01060785073786974 Accuracy 0.8810999989509583\n",
      "Iteration 26560 Training loss 0.0016122879460453987 Validation loss 0.010744563303887844 Accuracy 0.8795999884605408\n",
      "Iteration 26570 Training loss 0.003693912411108613 Validation loss 0.010746493004262447 Accuracy 0.8794999718666077\n",
      "Iteration 26580 Training loss 0.0018932310631498694 Validation loss 0.010763539932668209 Accuracy 0.8808000087738037\n",
      "Iteration 26590 Training loss 0.004470575135201216 Validation loss 0.010843056254088879 Accuracy 0.8790000081062317\n",
      "Iteration 26600 Training loss 0.002890899544581771 Validation loss 0.011033148504793644 Accuracy 0.8784000277519226\n",
      "Iteration 26610 Training loss 0.0027714555617421865 Validation loss 0.010612561367452145 Accuracy 0.8812999725341797\n",
      "Iteration 26620 Training loss 0.002632871037349105 Validation loss 0.010759780183434486 Accuracy 0.8794999718666077\n",
      "Iteration 26630 Training loss 0.0020727613009512424 Validation loss 0.0108408248052001 Accuracy 0.8798999786376953\n",
      "Iteration 26640 Training loss 0.0035400434862822294 Validation loss 0.010936847887933254 Accuracy 0.8780999779701233\n",
      "Iteration 26650 Training loss 0.0030661392956972122 Validation loss 0.011066928505897522 Accuracy 0.8756999969482422\n",
      "Iteration 26660 Training loss 0.00254654954187572 Validation loss 0.01075183879584074 Accuracy 0.8797000050544739\n",
      "Iteration 26670 Training loss 0.0027026431635022163 Validation loss 0.01085420697927475 Accuracy 0.8794000148773193\n",
      "Iteration 26680 Training loss 0.004199527204036713 Validation loss 0.010890505276620388 Accuracy 0.8784000277519226\n",
      "Iteration 26690 Training loss 0.0037008756771683693 Validation loss 0.010883107781410217 Accuracy 0.8797000050544739\n",
      "Iteration 26700 Training loss 0.0021671452559530735 Validation loss 0.010585510171949863 Accuracy 0.881600022315979\n",
      "Iteration 26710 Training loss 0.002746746176853776 Validation loss 0.010708398185670376 Accuracy 0.8812999725341797\n",
      "Iteration 26720 Training loss 0.0023474653717130423 Validation loss 0.010751855559647083 Accuracy 0.8795999884605408\n",
      "Iteration 26730 Training loss 0.002063764026388526 Validation loss 0.010858963243663311 Accuracy 0.878600001335144\n",
      "Iteration 26740 Training loss 0.0036124559119343758 Validation loss 0.010860378853976727 Accuracy 0.8788999915122986\n",
      "Iteration 26750 Training loss 0.004963088780641556 Validation loss 0.010626948438584805 Accuracy 0.8812000155448914\n",
      "Iteration 26760 Training loss 0.003156482707709074 Validation loss 0.010886773467063904 Accuracy 0.8787999749183655\n",
      "Iteration 26770 Training loss 0.0035702730529010296 Validation loss 0.01080734096467495 Accuracy 0.878600001335144\n",
      "Iteration 26780 Training loss 0.0037083574570715427 Validation loss 0.01083251740783453 Accuracy 0.8794000148773193\n",
      "Iteration 26790 Training loss 0.004378332756459713 Validation loss 0.010762921534478664 Accuracy 0.8798999786376953\n",
      "Iteration 26800 Training loss 0.0035617153625935316 Validation loss 0.01082959957420826 Accuracy 0.879800021648407\n",
      "Iteration 26810 Training loss 0.0028205334674566984 Validation loss 0.010767944157123566 Accuracy 0.8790000081062317\n",
      "Iteration 26820 Training loss 0.00334519287571311 Validation loss 0.010856274515390396 Accuracy 0.8783000111579895\n",
      "Iteration 26830 Training loss 0.002986780134961009 Validation loss 0.010819458402693272 Accuracy 0.878000020980835\n",
      "Iteration 26840 Training loss 0.0037795172538608313 Validation loss 0.010692041367292404 Accuracy 0.8809999823570251\n",
      "Iteration 26850 Training loss 0.00269499234855175 Validation loss 0.010740046389400959 Accuracy 0.8804000020027161\n",
      "Iteration 26860 Training loss 0.001874741748906672 Validation loss 0.010866784490644932 Accuracy 0.8787999749183655\n",
      "Iteration 26870 Training loss 0.0028560555074363947 Validation loss 0.010798901319503784 Accuracy 0.8805000185966492\n",
      "Iteration 26880 Training loss 0.0040265959687530994 Validation loss 0.010812487453222275 Accuracy 0.8787999749183655\n",
      "Iteration 26890 Training loss 0.004300286527723074 Validation loss 0.010885383002460003 Accuracy 0.8774999976158142\n",
      "Iteration 26900 Training loss 0.004111441783607006 Validation loss 0.01083726529031992 Accuracy 0.8781999945640564\n",
      "Iteration 26910 Training loss 0.003266360843554139 Validation loss 0.010695368982851505 Accuracy 0.8810999989509583\n",
      "Iteration 26920 Training loss 0.004564045928418636 Validation loss 0.010817530564963818 Accuracy 0.8780999779701233\n",
      "Iteration 26930 Training loss 0.0034645055420696735 Validation loss 0.010779301635921001 Accuracy 0.8805999755859375\n",
      "Iteration 26940 Training loss 0.0032190673518925905 Validation loss 0.01078985258936882 Accuracy 0.8806999921798706\n",
      "Iteration 26950 Training loss 0.0033846485894173384 Validation loss 0.010862142778933048 Accuracy 0.8784999847412109\n",
      "Iteration 26960 Training loss 0.0030914396047592163 Validation loss 0.010759714059531689 Accuracy 0.8802000284194946\n",
      "Iteration 26970 Training loss 0.002844718284904957 Validation loss 0.01075000874698162 Accuracy 0.879800021648407\n",
      "Iteration 26980 Training loss 0.0038293530233204365 Validation loss 0.010826962999999523 Accuracy 0.8791000247001648\n",
      "Iteration 26990 Training loss 0.002786874771118164 Validation loss 0.01068984903395176 Accuracy 0.8820000290870667\n",
      "Iteration 27000 Training loss 0.0024976953864097595 Validation loss 0.010877038352191448 Accuracy 0.878000020980835\n",
      "Iteration 27010 Training loss 0.0035778898745775223 Validation loss 0.010789044201374054 Accuracy 0.8802000284194946\n",
      "Iteration 27020 Training loss 0.0034311411436647177 Validation loss 0.010744858533143997 Accuracy 0.8797000050544739\n",
      "Iteration 27030 Training loss 0.0040818932466208935 Validation loss 0.010717091150581837 Accuracy 0.8813999891281128\n",
      "Iteration 27040 Training loss 0.003418420208618045 Validation loss 0.010719452984631062 Accuracy 0.8802000284194946\n",
      "Iteration 27050 Training loss 0.003643162315711379 Validation loss 0.010784911923110485 Accuracy 0.8787000179290771\n",
      "Iteration 27060 Training loss 0.0026676312554627657 Validation loss 0.01088295690715313 Accuracy 0.8779000043869019\n",
      "Iteration 27070 Training loss 0.003843131009489298 Validation loss 0.010774040594696999 Accuracy 0.8792999982833862\n",
      "Iteration 27080 Training loss 0.0023799161426723003 Validation loss 0.010745038278400898 Accuracy 0.8787999749183655\n",
      "Iteration 27090 Training loss 0.004563974216580391 Validation loss 0.010847845114767551 Accuracy 0.8784999847412109\n",
      "Iteration 27100 Training loss 0.003237270750105381 Validation loss 0.010805399157106876 Accuracy 0.8802000284194946\n",
      "Iteration 27110 Training loss 0.0028878345619887114 Validation loss 0.010998867452144623 Accuracy 0.876800000667572\n",
      "Iteration 27120 Training loss 0.0038588624447584152 Validation loss 0.01081074494868517 Accuracy 0.8784000277519226\n",
      "Iteration 27130 Training loss 0.0037361751310527325 Validation loss 0.010644462890923023 Accuracy 0.8815000057220459\n",
      "Iteration 27140 Training loss 0.0025632274337112904 Validation loss 0.010858328081667423 Accuracy 0.8776999711990356\n",
      "Iteration 27150 Training loss 0.00302274152636528 Validation loss 0.010668695904314518 Accuracy 0.8810999989509583\n",
      "Iteration 27160 Training loss 0.0023191121872514486 Validation loss 0.010765858925879002 Accuracy 0.8790000081062317\n",
      "Iteration 27170 Training loss 0.0035252482630312443 Validation loss 0.010873312130570412 Accuracy 0.8776000142097473\n",
      "Iteration 27180 Training loss 0.003607773454859853 Validation loss 0.010774804279208183 Accuracy 0.8792999982833862\n",
      "Iteration 27190 Training loss 0.002952669048681855 Validation loss 0.010707274079322815 Accuracy 0.8802000284194946\n",
      "Iteration 27200 Training loss 0.004070900846272707 Validation loss 0.010786183178424835 Accuracy 0.8802000284194946\n",
      "Iteration 27210 Training loss 0.0034783275332301855 Validation loss 0.01080577913671732 Accuracy 0.8784999847412109\n",
      "Iteration 27220 Training loss 0.003811369650065899 Validation loss 0.010849752463400364 Accuracy 0.8777999877929688\n",
      "Iteration 27230 Training loss 0.003333162982016802 Validation loss 0.010760471224784851 Accuracy 0.880299985408783\n",
      "Iteration 27240 Training loss 0.0030763254035264254 Validation loss 0.010791334323585033 Accuracy 0.8790000081062317\n",
      "Iteration 27250 Training loss 0.0038111393805593252 Validation loss 0.010904199443757534 Accuracy 0.8779000043869019\n",
      "Iteration 27260 Training loss 0.001938085537403822 Validation loss 0.010767579078674316 Accuracy 0.879800021648407\n",
      "Iteration 27270 Training loss 0.003078408306464553 Validation loss 0.010772312059998512 Accuracy 0.8798999786376953\n",
      "Iteration 27280 Training loss 0.00349904946051538 Validation loss 0.010708516463637352 Accuracy 0.8801000118255615\n",
      "Iteration 27290 Training loss 0.0023854048922657967 Validation loss 0.010848520323634148 Accuracy 0.8791000247001648\n",
      "Iteration 27300 Training loss 0.0020387449767440557 Validation loss 0.01069872360676527 Accuracy 0.8805000185966492\n",
      "Iteration 27310 Training loss 0.0028549174312502146 Validation loss 0.010772948153316975 Accuracy 0.8783000111579895\n",
      "Iteration 27320 Training loss 0.002490303246304393 Validation loss 0.010665412992238998 Accuracy 0.8810999989509583\n",
      "Iteration 27330 Training loss 0.002780279843136668 Validation loss 0.0107782743871212 Accuracy 0.8797000050544739\n",
      "Iteration 27340 Training loss 0.002371004316955805 Validation loss 0.010809123516082764 Accuracy 0.8787999749183655\n",
      "Iteration 27350 Training loss 0.0031413466203957796 Validation loss 0.010749085806310177 Accuracy 0.8791999816894531\n",
      "Iteration 27360 Training loss 0.003915970213711262 Validation loss 0.010742293670773506 Accuracy 0.8794000148773193\n",
      "Iteration 27370 Training loss 0.0035440754145383835 Validation loss 0.010870813392102718 Accuracy 0.8781999945640564\n",
      "Iteration 27380 Training loss 0.0037685974966734648 Validation loss 0.011470858938992023 Accuracy 0.8705999851226807\n",
      "Iteration 27390 Training loss 0.0032267405185848475 Validation loss 0.011008068919181824 Accuracy 0.8758000135421753\n",
      "Iteration 27400 Training loss 0.002433279063552618 Validation loss 0.010829519480466843 Accuracy 0.8790000081062317\n",
      "Iteration 27410 Training loss 0.0036926704924553633 Validation loss 0.010815884917974472 Accuracy 0.8791000247001648\n",
      "Iteration 27420 Training loss 0.0023080904502421618 Validation loss 0.010649211704730988 Accuracy 0.8802000284194946\n",
      "Iteration 27430 Training loss 0.0027385917492210865 Validation loss 0.010805467143654823 Accuracy 0.8794000148773193\n",
      "Iteration 27440 Training loss 0.0028551663272082806 Validation loss 0.01070787850767374 Accuracy 0.8802000284194946\n",
      "Iteration 27450 Training loss 0.0032753387931734324 Validation loss 0.010738860815763474 Accuracy 0.880299985408783\n",
      "Iteration 27460 Training loss 0.002760169096291065 Validation loss 0.010955351404845715 Accuracy 0.8779000043869019\n",
      "Iteration 27470 Training loss 0.002351498231291771 Validation loss 0.010759348049759865 Accuracy 0.879800021648407\n",
      "Iteration 27480 Training loss 0.002560871420428157 Validation loss 0.010909907519817352 Accuracy 0.878000020980835\n",
      "Iteration 27490 Training loss 0.002545745810493827 Validation loss 0.010638203471899033 Accuracy 0.8809999823570251\n",
      "Iteration 27500 Training loss 0.0039080483838915825 Validation loss 0.010961146093904972 Accuracy 0.8769999742507935\n",
      "Iteration 27510 Training loss 0.002779230708256364 Validation loss 0.010882453992962837 Accuracy 0.8783000111579895\n",
      "Iteration 27520 Training loss 0.002285873517394066 Validation loss 0.010822519659996033 Accuracy 0.8784999847412109\n",
      "Iteration 27530 Training loss 0.003940710332244635 Validation loss 0.010766025632619858 Accuracy 0.8794000148773193\n",
      "Iteration 27540 Training loss 0.0030840602703392506 Validation loss 0.010991932824254036 Accuracy 0.8769999742507935\n",
      "Iteration 27550 Training loss 0.0025234485510736704 Validation loss 0.010796509683132172 Accuracy 0.8788999915122986\n",
      "Iteration 27560 Training loss 0.004782769829034805 Validation loss 0.010723564773797989 Accuracy 0.8798999786376953\n",
      "Iteration 27570 Training loss 0.0036137434653937817 Validation loss 0.010816937312483788 Accuracy 0.8777999877929688\n",
      "Iteration 27580 Training loss 0.0034463312476873398 Validation loss 0.010707292705774307 Accuracy 0.879800021648407\n",
      "Iteration 27590 Training loss 0.0022128678392618895 Validation loss 0.010669594630599022 Accuracy 0.8812999725341797\n",
      "Iteration 27600 Training loss 0.002867714036256075 Validation loss 0.010687138885259628 Accuracy 0.8808000087738037\n",
      "Iteration 27610 Training loss 0.0034201866947114468 Validation loss 0.01073175948113203 Accuracy 0.8804000020027161\n",
      "Iteration 27620 Training loss 0.002399768913164735 Validation loss 0.010743141174316406 Accuracy 0.8801000118255615\n",
      "Iteration 27630 Training loss 0.002419352764263749 Validation loss 0.01074258703738451 Accuracy 0.8797000050544739\n",
      "Iteration 27640 Training loss 0.0029532082844525576 Validation loss 0.010720773600041866 Accuracy 0.8804000020027161\n",
      "Iteration 27650 Training loss 0.0033632211852818727 Validation loss 0.010687107220292091 Accuracy 0.8809000253677368\n",
      "Iteration 27660 Training loss 0.002702877623960376 Validation loss 0.010710390284657478 Accuracy 0.8805000185966492\n",
      "Iteration 27670 Training loss 0.004338313825428486 Validation loss 0.010724703781306744 Accuracy 0.879800021648407\n",
      "Iteration 27680 Training loss 0.003840164514258504 Validation loss 0.010607806034386158 Accuracy 0.8816999793052673\n",
      "Iteration 27690 Training loss 0.0030632521957159042 Validation loss 0.010625021532177925 Accuracy 0.8815000057220459\n",
      "Iteration 27700 Training loss 0.003172456519678235 Validation loss 0.010687391273677349 Accuracy 0.8794999718666077\n",
      "Iteration 27710 Training loss 0.003543286584317684 Validation loss 0.010791704058647156 Accuracy 0.8779000043869019\n",
      "Iteration 27720 Training loss 0.002728536492213607 Validation loss 0.010720686987042427 Accuracy 0.8805000185966492\n",
      "Iteration 27730 Training loss 0.002674523741006851 Validation loss 0.010894897393882275 Accuracy 0.876800000667572\n",
      "Iteration 27740 Training loss 0.0026610055938363075 Validation loss 0.01078042946755886 Accuracy 0.8790000081062317\n",
      "Iteration 27750 Training loss 0.0038263651076704264 Validation loss 0.01079721562564373 Accuracy 0.8790000081062317\n",
      "Iteration 27760 Training loss 0.0036444510333240032 Validation loss 0.010742277838289738 Accuracy 0.8787999749183655\n",
      "Iteration 27770 Training loss 0.003965424839407206 Validation loss 0.010946542955935001 Accuracy 0.8774999976158142\n",
      "Iteration 27780 Training loss 0.002592989709228277 Validation loss 0.010781164281070232 Accuracy 0.8794000148773193\n",
      "Iteration 27790 Training loss 0.002886858070269227 Validation loss 0.010930102318525314 Accuracy 0.8762999773025513\n",
      "Iteration 27800 Training loss 0.0022394629195332527 Validation loss 0.010774079710245132 Accuracy 0.8777999877929688\n",
      "Iteration 27810 Training loss 0.002914765616878867 Validation loss 0.010816755704581738 Accuracy 0.8790000081062317\n",
      "Iteration 27820 Training loss 0.002978511154651642 Validation loss 0.01078993920236826 Accuracy 0.8799999952316284\n",
      "Iteration 27830 Training loss 0.004000833258032799 Validation loss 0.0107821524143219 Accuracy 0.8797000050544739\n",
      "Iteration 27840 Training loss 0.0036536341067403555 Validation loss 0.010845750570297241 Accuracy 0.8779000043869019\n",
      "Iteration 27850 Training loss 0.0019412619294598699 Validation loss 0.010769999586045742 Accuracy 0.8787000179290771\n",
      "Iteration 27860 Training loss 0.003166138892993331 Validation loss 0.010843770578503609 Accuracy 0.8783000111579895\n",
      "Iteration 27870 Training loss 0.002802623435854912 Validation loss 0.010942858643829823 Accuracy 0.8758999705314636\n",
      "Iteration 27880 Training loss 0.004209274426102638 Validation loss 0.010729031637310982 Accuracy 0.8794000148773193\n",
      "Iteration 27890 Training loss 0.0026275983545929193 Validation loss 0.010713756084442139 Accuracy 0.8797000050544739\n",
      "Iteration 27900 Training loss 0.0032049401197582483 Validation loss 0.010742631740868092 Accuracy 0.8791000247001648\n",
      "Iteration 27910 Training loss 0.002810764592140913 Validation loss 0.010775231756269932 Accuracy 0.8791999816894531\n",
      "Iteration 27920 Training loss 0.0029247032944113016 Validation loss 0.01077970489859581 Accuracy 0.8792999982833862\n",
      "Iteration 27930 Training loss 0.003233557101339102 Validation loss 0.010676524601876736 Accuracy 0.8801000118255615\n",
      "Iteration 27940 Training loss 0.002517219167202711 Validation loss 0.01072926539927721 Accuracy 0.8804000020027161\n",
      "Iteration 27950 Training loss 0.0032360197510570288 Validation loss 0.010690961964428425 Accuracy 0.8804000020027161\n",
      "Iteration 27960 Training loss 0.0027192742563784122 Validation loss 0.010776722803711891 Accuracy 0.8788999915122986\n",
      "Iteration 27970 Training loss 0.004027631599456072 Validation loss 0.010765422135591507 Accuracy 0.8779000043869019\n",
      "Iteration 27980 Training loss 0.0034052866976708174 Validation loss 0.010721507482230663 Accuracy 0.8797000050544739\n",
      "Iteration 27990 Training loss 0.0026304644998162985 Validation loss 0.010776623152196407 Accuracy 0.8792999982833862\n",
      "Iteration 28000 Training loss 0.0017897890647873282 Validation loss 0.010790717788040638 Accuracy 0.8790000081062317\n",
      "Iteration 28010 Training loss 0.003301940392702818 Validation loss 0.010696913115680218 Accuracy 0.8802000284194946\n",
      "Iteration 28020 Training loss 0.003395877545699477 Validation loss 0.010720337741076946 Accuracy 0.8784000277519226\n",
      "Iteration 28030 Training loss 0.002962704049423337 Validation loss 0.010721352882683277 Accuracy 0.8790000081062317\n",
      "Iteration 28040 Training loss 0.0020722607150673866 Validation loss 0.010863241739571095 Accuracy 0.876800000667572\n",
      "Iteration 28050 Training loss 0.0037112445570528507 Validation loss 0.010741237550973892 Accuracy 0.8790000081062317\n",
      "Iteration 28060 Training loss 0.0029068004805594683 Validation loss 0.010761075653135777 Accuracy 0.8787999749183655\n",
      "Iteration 28070 Training loss 0.004375164862722158 Validation loss 0.010909108445048332 Accuracy 0.8770999908447266\n",
      "Iteration 28080 Training loss 0.0034967137034982443 Validation loss 0.011004905216395855 Accuracy 0.8763999938964844\n",
      "Iteration 28090 Training loss 0.0028236035723239183 Validation loss 0.010802472941577435 Accuracy 0.8797000050544739\n",
      "Iteration 28100 Training loss 0.004144603852182627 Validation loss 0.010831042192876339 Accuracy 0.8787999749183655\n",
      "Iteration 28110 Training loss 0.0030990727245807648 Validation loss 0.010796635411679745 Accuracy 0.8799999952316284\n",
      "Iteration 28120 Training loss 0.003585933707654476 Validation loss 0.010693679563701153 Accuracy 0.8799999952316284\n",
      "Iteration 28130 Training loss 0.003987448289990425 Validation loss 0.010822679847478867 Accuracy 0.8777999877929688\n",
      "Iteration 28140 Training loss 0.0026154869701713324 Validation loss 0.010741657577455044 Accuracy 0.8791000247001648\n",
      "Iteration 28150 Training loss 0.0017634087707847357 Validation loss 0.01070531364530325 Accuracy 0.8805999755859375\n",
      "Iteration 28160 Training loss 0.003114348743110895 Validation loss 0.010740376077592373 Accuracy 0.8792999982833862\n",
      "Iteration 28170 Training loss 0.004345027729868889 Validation loss 0.010742595419287682 Accuracy 0.8802000284194946\n",
      "Iteration 28180 Training loss 0.002364296233281493 Validation loss 0.010759808123111725 Accuracy 0.8787999749183655\n",
      "Iteration 28190 Training loss 0.002412232803180814 Validation loss 0.010765067301690578 Accuracy 0.8784000277519226\n",
      "Iteration 28200 Training loss 0.002579411491751671 Validation loss 0.01069837249815464 Accuracy 0.8801000118255615\n",
      "Iteration 28210 Training loss 0.0033826224971562624 Validation loss 0.010647574439644814 Accuracy 0.8806999921798706\n",
      "Iteration 28220 Training loss 0.0021107299253344536 Validation loss 0.010671736672520638 Accuracy 0.8808000087738037\n",
      "Iteration 28230 Training loss 0.0018914989195764065 Validation loss 0.01074904389679432 Accuracy 0.878600001335144\n",
      "Iteration 28240 Training loss 0.003895107191056013 Validation loss 0.01068138424307108 Accuracy 0.8805000185966492\n",
      "Iteration 28250 Training loss 0.004325751215219498 Validation loss 0.01073713880032301 Accuracy 0.879800021648407\n",
      "Iteration 28260 Training loss 0.0014717084122821689 Validation loss 0.010750964283943176 Accuracy 0.8794999718666077\n",
      "Iteration 28270 Training loss 0.0031476905569434166 Validation loss 0.01072385348379612 Accuracy 0.8795999884605408\n",
      "Iteration 28280 Training loss 0.0029464438557624817 Validation loss 0.010786467231810093 Accuracy 0.8788999915122986\n",
      "Iteration 28290 Training loss 0.003857779549434781 Validation loss 0.010917671024799347 Accuracy 0.8766000270843506\n",
      "Iteration 28300 Training loss 0.0029203544836491346 Validation loss 0.010689057409763336 Accuracy 0.881600022315979\n",
      "Iteration 28310 Training loss 0.002030988223850727 Validation loss 0.010690328665077686 Accuracy 0.8805999755859375\n",
      "Iteration 28320 Training loss 0.0018026356119662523 Validation loss 0.010692696087062359 Accuracy 0.8798999786376953\n",
      "Iteration 28330 Training loss 0.0031340119894593954 Validation loss 0.010701232589781284 Accuracy 0.8806999921798706\n",
      "Iteration 28340 Training loss 0.003024822333827615 Validation loss 0.010678202845156193 Accuracy 0.8809000253677368\n",
      "Iteration 28350 Training loss 0.0027251653373241425 Validation loss 0.010655025951564312 Accuracy 0.8801000118255615\n",
      "Iteration 28360 Training loss 0.004342596512287855 Validation loss 0.010685720480978489 Accuracy 0.8805000185966492\n",
      "Iteration 28370 Training loss 0.0027939211577177048 Validation loss 0.01079228799790144 Accuracy 0.8795999884605408\n",
      "Iteration 28380 Training loss 0.0030931897927075624 Validation loss 0.010755973868072033 Accuracy 0.8810999989509583\n",
      "Iteration 28390 Training loss 0.0028427785728126764 Validation loss 0.010802709497511387 Accuracy 0.8787999749183655\n",
      "Iteration 28400 Training loss 0.00210475642234087 Validation loss 0.010795317590236664 Accuracy 0.879800021648407\n",
      "Iteration 28410 Training loss 0.0017669741064310074 Validation loss 0.010721118189394474 Accuracy 0.8791999816894531\n",
      "Iteration 28420 Training loss 0.003009985201060772 Validation loss 0.010754551738500595 Accuracy 0.8799999952316284\n",
      "Iteration 28430 Training loss 0.0020446605049073696 Validation loss 0.010761811397969723 Accuracy 0.8794999718666077\n",
      "Iteration 28440 Training loss 0.0018604163778945804 Validation loss 0.010754801332950592 Accuracy 0.879800021648407\n",
      "Iteration 28450 Training loss 0.0037612973246723413 Validation loss 0.0107281394302845 Accuracy 0.8799999952316284\n",
      "Iteration 28460 Training loss 0.0036356898490339518 Validation loss 0.010718117468059063 Accuracy 0.8805000185966492\n",
      "Iteration 28470 Training loss 0.001730922027491033 Validation loss 0.010670776478946209 Accuracy 0.8801000118255615\n",
      "Iteration 28480 Training loss 0.00357990013435483 Validation loss 0.010655355639755726 Accuracy 0.8798999786376953\n",
      "Iteration 28490 Training loss 0.0025696400552988052 Validation loss 0.01062808744609356 Accuracy 0.8804000020027161\n",
      "Iteration 28500 Training loss 0.003906101919710636 Validation loss 0.010644128546118736 Accuracy 0.880299985408783\n",
      "Iteration 28510 Training loss 0.002808503108099103 Validation loss 0.010631444863975048 Accuracy 0.8801000118255615\n",
      "Iteration 28520 Training loss 0.002404461381956935 Validation loss 0.010743447579443455 Accuracy 0.8794999718666077\n",
      "Iteration 28530 Training loss 0.0032808659598231316 Validation loss 0.010719314217567444 Accuracy 0.8801000118255615\n",
      "Iteration 28540 Training loss 0.002957815071567893 Validation loss 0.010702229104936123 Accuracy 0.8794999718666077\n",
      "Iteration 28550 Training loss 0.0034553706645965576 Validation loss 0.010838823392987251 Accuracy 0.8784000277519226\n",
      "Iteration 28560 Training loss 0.002531873295083642 Validation loss 0.01081096287816763 Accuracy 0.8788999915122986\n",
      "Iteration 28570 Training loss 0.002991125453263521 Validation loss 0.010662763379514217 Accuracy 0.8804000020027161\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[205]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_3_trained_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[195]\u001b[39m\u001b[32m, line 191\u001b[39m, in \u001b[36mthree_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, coef_iter, lr, reg1, reg2, reg3, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2, train_layer_3)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28mself\u001b[39m.training_loss_trajectory.append(training_loss.item())\n\u001b[32m    190\u001b[39m \u001b[38;5;28mself\u001b[39m.validation_loss_trajectory.append(validation_loss.item())\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m accuracy = torch.mean((torch.argmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m], dim=\u001b[32m1\u001b[39m) == torch.argmax(y_valid, dim=\u001b[32m1\u001b[39m)).to(dtype))\n\u001b[32m    192\u001b[39m \u001b[38;5;28mself\u001b[39m.accuracy_trajectory.append(accuracy.item())\n\u001b[32m    193\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIteration\u001b[39m\u001b[33m\"\u001b[39m, i, \u001b[33m\"\u001b[39m\u001b[33mTraining loss\u001b[39m\u001b[33m\"\u001b[39m, training_loss.item(), \u001b[33m\"\u001b[39m\u001b[33mValidation loss\u001b[39m\u001b[33m\"\u001b[39m, validation_loss.item(), \u001b[33m\"\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m\"\u001b[39m, accuracy.item())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[195]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36mthree_layer_NN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     z1 = (\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m.b1).t() \u001b[38;5;66;03m# shape (n_data, hidden_1_size) # logits layer 1\u001b[39;00m\n\u001b[32m    151\u001b[39m     h1 = ReLU(z1)  \u001b[38;5;66;03m# hidden neurons layer 1\u001b[39;00m\n\u001b[32m    152\u001b[39m     z2 = (torch.mm(\u001b[38;5;28mself\u001b[39m.W2, h1.t()) + \u001b[38;5;28mself\u001b[39m.b2).t() \u001b[38;5;66;03m# shape (n_data, number_of_classes ) # logits layer 2\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 1, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 512, 256, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter 614656\n",
      "Iteration 0 Training loss 0.09681342542171478 Validation loss 0.09679381549358368 Accuracy 0.01549999974668026\n",
      "Iteration 10 Training loss 0.07025577127933502 Validation loss 0.071634441614151 Accuracy 0.27709999680519104\n",
      "Iteration 20 Training loss 0.07483749836683273 Validation loss 0.07187289744615555 Accuracy 0.2770000100135803\n",
      "Iteration 30 Training loss 0.07151298224925995 Validation loss 0.0703793466091156 Accuracy 0.2897999882698059\n",
      "Iteration 40 Training loss 0.0717371255159378 Validation loss 0.07081842422485352 Accuracy 0.2879999876022339\n",
      "Iteration 50 Training loss 0.06561345607042313 Validation loss 0.06775342673063278 Accuracy 0.31869998574256897\n",
      "Iteration 60 Training loss 0.06806135922670364 Validation loss 0.06635008752346039 Accuracy 0.33340001106262207\n",
      "Iteration 70 Training loss 0.06591656804084778 Validation loss 0.06590680032968521 Accuracy 0.3375000059604645\n",
      "Iteration 80 Training loss 0.06448914855718613 Validation loss 0.06582719832658768 Accuracy 0.33809998631477356\n",
      "Iteration 90 Training loss 0.06510696560144424 Validation loss 0.0650632306933403 Accuracy 0.3458999991416931\n",
      "Iteration 100 Training loss 0.06481432914733887 Validation loss 0.06516330689191818 Accuracy 0.3456000089645386\n",
      "Iteration 110 Training loss 0.06519254297018051 Validation loss 0.06454440206289291 Accuracy 0.3513999879360199\n",
      "Iteration 120 Training loss 0.06610730290412903 Validation loss 0.06434834003448486 Accuracy 0.3531000018119812\n",
      "Iteration 130 Training loss 0.06447216123342514 Validation loss 0.0640874132514 Accuracy 0.35589998960494995\n",
      "Iteration 140 Training loss 0.06356823444366455 Validation loss 0.06365334987640381 Accuracy 0.3594000041484833\n",
      "Iteration 150 Training loss 0.06104898452758789 Validation loss 0.06423423439264297 Accuracy 0.35420000553131104\n",
      "Iteration 160 Training loss 0.06154386326670647 Validation loss 0.06378281116485596 Accuracy 0.358599990606308\n",
      "Iteration 170 Training loss 0.06062667816877365 Validation loss 0.06293276697397232 Accuracy 0.36719998717308044\n",
      "Iteration 180 Training loss 0.0633554607629776 Validation loss 0.06275607645511627 Accuracy 0.36959999799728394\n",
      "Iteration 190 Training loss 0.06360737979412079 Validation loss 0.06424403935670853 Accuracy 0.3547999858856201\n",
      "Iteration 200 Training loss 0.05998288840055466 Validation loss 0.06259393692016602 Accuracy 0.3716000020503998\n",
      "Iteration 210 Training loss 0.0656139999628067 Validation loss 0.06475847959518433 Accuracy 0.3483999967575073\n",
      "Iteration 220 Training loss 0.06648310273885727 Validation loss 0.06541035324335098 Accuracy 0.34139999747276306\n",
      "Iteration 230 Training loss 0.06485000252723694 Validation loss 0.06238383427262306 Accuracy 0.3734999895095825\n",
      "Iteration 240 Training loss 0.0656300038099289 Validation loss 0.06331808865070343 Accuracy 0.36329999566078186\n",
      "Iteration 250 Training loss 0.06266553699970245 Validation loss 0.06295252591371536 Accuracy 0.36649999022483826\n",
      "Iteration 260 Training loss 0.06390517950057983 Validation loss 0.06367454677820206 Accuracy 0.35910001397132874\n",
      "Iteration 270 Training loss 0.0644538402557373 Validation loss 0.06320903450250626 Accuracy 0.3652999997138977\n",
      "Iteration 280 Training loss 0.06047629192471504 Validation loss 0.06344085186719894 Accuracy 0.3612000048160553\n",
      "Iteration 290 Training loss 0.0646301731467247 Validation loss 0.06482428312301636 Accuracy 0.3490000069141388\n",
      "Iteration 300 Training loss 0.06605573743581772 Validation loss 0.06283316016197205 Accuracy 0.3677000105381012\n",
      "Iteration 310 Training loss 0.06213518604636192 Validation loss 0.06228671967983246 Accuracy 0.3736000061035156\n",
      "Iteration 320 Training loss 0.06148162856698036 Validation loss 0.06318891793489456 Accuracy 0.3646000027656555\n",
      "Iteration 330 Training loss 0.05920996516942978 Validation loss 0.0643068328499794 Accuracy 0.3538999855518341\n",
      "Iteration 340 Training loss 0.06010616570711136 Validation loss 0.06321743130683899 Accuracy 0.36469998955726624\n",
      "Iteration 350 Training loss 0.06509632617235184 Validation loss 0.06263940781354904 Accuracy 0.3700999915599823\n",
      "Iteration 360 Training loss 0.061316974461078644 Validation loss 0.06286469846963882 Accuracy 0.367900013923645\n",
      "Iteration 370 Training loss 0.06366414576768875 Validation loss 0.06373517960309982 Accuracy 0.3594000041484833\n",
      "Iteration 380 Training loss 0.06134548783302307 Validation loss 0.06400471925735474 Accuracy 0.3564999997615814\n",
      "Iteration 390 Training loss 0.06272302567958832 Validation loss 0.06237966939806938 Accuracy 0.37310001254081726\n",
      "Iteration 400 Training loss 0.059231650084257126 Validation loss 0.06262415647506714 Accuracy 0.3709000051021576\n",
      "Iteration 410 Training loss 0.06532244384288788 Validation loss 0.06252743303775787 Accuracy 0.3723999857902527\n",
      "Iteration 420 Training loss 0.06218881160020828 Validation loss 0.0625888854265213 Accuracy 0.3709999918937683\n",
      "Iteration 430 Training loss 0.06218549236655235 Validation loss 0.06220751628279686 Accuracy 0.3750999867916107\n",
      "Iteration 440 Training loss 0.06652753055095673 Validation loss 0.06284452229738235 Accuracy 0.367900013923645\n",
      "Iteration 450 Training loss 0.06017130985856056 Validation loss 0.06209343299269676 Accuracy 0.37540000677108765\n",
      "Iteration 460 Training loss 0.0617203488945961 Validation loss 0.06283801048994064 Accuracy 0.3668999969959259\n",
      "Iteration 470 Training loss 0.06010906398296356 Validation loss 0.06288861483335495 Accuracy 0.3675000071525574\n",
      "Iteration 480 Training loss 0.061435308307409286 Validation loss 0.06316553056240082 Accuracy 0.364300012588501\n",
      "Iteration 490 Training loss 0.06466055661439896 Validation loss 0.06286845356225967 Accuracy 0.3684999942779541\n",
      "Iteration 500 Training loss 0.06289128959178925 Validation loss 0.06275241076946259 Accuracy 0.36980000138282776\n",
      "Iteration 510 Training loss 0.0617629699409008 Validation loss 0.062480755150318146 Accuracy 0.3720000088214874\n",
      "Iteration 520 Training loss 0.06264029443264008 Validation loss 0.06282860040664673 Accuracy 0.3686999976634979\n",
      "Iteration 530 Training loss 0.062437184154987335 Validation loss 0.06236303225159645 Accuracy 0.3734000027179718\n",
      "Iteration 540 Training loss 0.06613387912511826 Validation loss 0.06208719685673714 Accuracy 0.3765000104904175\n",
      "Iteration 550 Training loss 0.06119377538561821 Validation loss 0.06271062791347504 Accuracy 0.36970001459121704\n",
      "Iteration 560 Training loss 0.062253449112176895 Validation loss 0.06212647631764412 Accuracy 0.37560001015663147\n",
      "Iteration 570 Training loss 0.06301126629114151 Validation loss 0.06194628030061722 Accuracy 0.37790000438690186\n",
      "Iteration 580 Training loss 0.06043562665581703 Validation loss 0.062120601534843445 Accuracy 0.37560001015663147\n",
      "Iteration 590 Training loss 0.0637250691652298 Validation loss 0.062290165573358536 Accuracy 0.373199999332428\n",
      "Iteration 600 Training loss 0.06191181018948555 Validation loss 0.06286054849624634 Accuracy 0.36820000410079956\n",
      "Iteration 610 Training loss 0.06025319918990135 Validation loss 0.062223710119724274 Accuracy 0.37470000982284546\n",
      "Iteration 620 Training loss 0.06293023377656937 Validation loss 0.062183961272239685 Accuracy 0.3750999867916107\n",
      "Iteration 630 Training loss 0.06181155890226364 Validation loss 0.06220300868153572 Accuracy 0.3749000132083893\n",
      "Iteration 640 Training loss 0.06459978967905045 Validation loss 0.06254703551530838 Accuracy 0.3714999854564667\n",
      "Iteration 650 Training loss 0.06413652747869492 Validation loss 0.06268174201250076 Accuracy 0.3700999915599823\n",
      "Iteration 660 Training loss 0.06447205692529678 Validation loss 0.06246007978916168 Accuracy 0.3720000088214874\n",
      "Iteration 670 Training loss 0.06083574518561363 Validation loss 0.06211043894290924 Accuracy 0.3756999969482422\n",
      "Iteration 680 Training loss 0.06180707737803459 Validation loss 0.06286086142063141 Accuracy 0.3668000102043152\n",
      "Iteration 690 Training loss 0.057589344680309296 Validation loss 0.0625247061252594 Accuracy 0.37059998512268066\n",
      "Iteration 700 Training loss 0.060938481241464615 Validation loss 0.06207214668393135 Accuracy 0.3767000138759613\n",
      "Iteration 710 Training loss 0.0646825060248375 Validation loss 0.06237010657787323 Accuracy 0.37369999289512634\n",
      "Iteration 720 Training loss 0.060752999037504196 Validation loss 0.06210925430059433 Accuracy 0.3767000138759613\n",
      "Iteration 730 Training loss 0.061758872121572495 Validation loss 0.06268032640218735 Accuracy 0.3702000081539154\n",
      "Iteration 740 Training loss 0.06176965683698654 Validation loss 0.061889901757240295 Accuracy 0.3785000145435333\n",
      "Iteration 750 Training loss 0.061893559992313385 Validation loss 0.062388911843299866 Accuracy 0.3732999861240387\n",
      "Iteration 760 Training loss 0.062049541622400284 Validation loss 0.06251653283834457 Accuracy 0.3727000057697296\n",
      "Iteration 770 Training loss 0.0604417622089386 Validation loss 0.062453486025333405 Accuracy 0.3734000027179718\n",
      "Iteration 780 Training loss 0.05962628126144409 Validation loss 0.0621010921895504 Accuracy 0.37560001015663147\n",
      "Iteration 790 Training loss 0.0626043826341629 Validation loss 0.06215652450919151 Accuracy 0.37560001015663147\n",
      "Iteration 800 Training loss 0.05884283408522606 Validation loss 0.06202397868037224 Accuracy 0.37770000100135803\n",
      "Iteration 810 Training loss 0.060616277158260345 Validation loss 0.06207827478647232 Accuracy 0.37709999084472656\n",
      "Iteration 820 Training loss 0.06222010403871536 Validation loss 0.062292519956827164 Accuracy 0.374099999666214\n",
      "Iteration 830 Training loss 0.061014916747808456 Validation loss 0.06199661269783974 Accuracy 0.3781000077724457\n",
      "Iteration 840 Training loss 0.05932649224996567 Validation loss 0.061900898814201355 Accuracy 0.37860000133514404\n",
      "Iteration 850 Training loss 0.06261584162712097 Validation loss 0.06210209056735039 Accuracy 0.37619999051094055\n",
      "Iteration 860 Training loss 0.061600930988788605 Validation loss 0.06190915033221245 Accuracy 0.3783000111579895\n",
      "Iteration 870 Training loss 0.05751967430114746 Validation loss 0.06202961876988411 Accuracy 0.37689998745918274\n",
      "Iteration 880 Training loss 0.0625913217663765 Validation loss 0.06268950551748276 Accuracy 0.3700000047683716\n",
      "Iteration 890 Training loss 0.06287931650876999 Validation loss 0.06184432655572891 Accuracy 0.37880000472068787\n",
      "Iteration 900 Training loss 0.06369531899690628 Validation loss 0.062277939170598984 Accuracy 0.374099999666214\n",
      "Iteration 910 Training loss 0.06556003540754318 Validation loss 0.0627472922205925 Accuracy 0.3700999915599823\n",
      "Iteration 920 Training loss 0.0628931000828743 Validation loss 0.06324949115514755 Accuracy 0.3643999993801117\n",
      "Iteration 930 Training loss 0.06273963302373886 Validation loss 0.06186412647366524 Accuracy 0.37869998812675476\n",
      "Iteration 940 Training loss 0.05984930321574211 Validation loss 0.06223617121577263 Accuracy 0.3749000132083893\n",
      "Iteration 950 Training loss 0.06440544873476028 Validation loss 0.06194903701543808 Accuracy 0.3781999945640564\n",
      "Iteration 960 Training loss 0.05955662578344345 Validation loss 0.06224231421947479 Accuracy 0.3749000132083893\n",
      "Iteration 970 Training loss 0.06602150201797485 Validation loss 0.06267150491476059 Accuracy 0.3700000047683716\n",
      "Iteration 980 Training loss 0.05995794013142586 Validation loss 0.06201562657952309 Accuracy 0.3772999942302704\n",
      "Iteration 990 Training loss 0.06294853240251541 Validation loss 0.0621044859290123 Accuracy 0.37619999051094055\n",
      "Iteration 1000 Training loss 0.061380136758089066 Validation loss 0.0619208887219429 Accuracy 0.37779998779296875\n",
      "Iteration 1010 Training loss 0.061633627861738205 Validation loss 0.06326096504926682 Accuracy 0.36489999294281006\n",
      "Iteration 1020 Training loss 0.06015864387154579 Validation loss 0.06190413609147072 Accuracy 0.37860000133514404\n",
      "Iteration 1030 Training loss 0.061387691646814346 Validation loss 0.06205940246582031 Accuracy 0.37779998779296875\n",
      "Iteration 1040 Training loss 0.0609537772834301 Validation loss 0.0625554621219635 Accuracy 0.3720000088214874\n",
      "Iteration 1050 Training loss 0.06281513720750809 Validation loss 0.06246418133378029 Accuracy 0.373199999332428\n",
      "Iteration 1060 Training loss 0.06479346007108688 Validation loss 0.061937324702739716 Accuracy 0.37869998812675476\n",
      "Iteration 1070 Training loss 0.060900188982486725 Validation loss 0.06255699694156647 Accuracy 0.3725000023841858\n",
      "Iteration 1080 Training loss 0.06415856629610062 Validation loss 0.06296602636575699 Accuracy 0.3675000071525574\n",
      "Iteration 1090 Training loss 0.05909064784646034 Validation loss 0.0622369721531868 Accuracy 0.375\n",
      "Iteration 1100 Training loss 0.0630667433142662 Validation loss 0.061819612979888916 Accuracy 0.3797000050544739\n",
      "Iteration 1110 Training loss 0.05822118744254112 Validation loss 0.061746254563331604 Accuracy 0.3797999918460846\n",
      "Iteration 1120 Training loss 0.060731835663318634 Validation loss 0.06212953105568886 Accuracy 0.375900000333786\n",
      "Iteration 1130 Training loss 0.06291725486516953 Validation loss 0.061894744634628296 Accuracy 0.3785000145435333\n",
      "Iteration 1140 Training loss 0.06097548454999924 Validation loss 0.06227165833115578 Accuracy 0.3752000033855438\n",
      "Iteration 1150 Training loss 0.059418465942144394 Validation loss 0.062348369508981705 Accuracy 0.37439998984336853\n",
      "Iteration 1160 Training loss 0.06130143627524376 Validation loss 0.062135957181453705 Accuracy 0.3767000138759613\n",
      "Iteration 1170 Training loss 0.0625862181186676 Validation loss 0.061887823045253754 Accuracy 0.3790999948978424\n",
      "Iteration 1180 Training loss 0.0627128928899765 Validation loss 0.06349476426839828 Accuracy 0.3614000082015991\n",
      "Iteration 1190 Training loss 0.0602848045527935 Validation loss 0.062426190823316574 Accuracy 0.3732999861240387\n",
      "Iteration 1200 Training loss 0.06369350850582123 Validation loss 0.06255663931369781 Accuracy 0.3720000088214874\n",
      "Iteration 1210 Training loss 0.061896856874227524 Validation loss 0.062060967087745667 Accuracy 0.3767000138759613\n",
      "Iteration 1220 Training loss 0.06137664616107941 Validation loss 0.061866000294685364 Accuracy 0.3790999948978424\n",
      "Iteration 1230 Training loss 0.06167910620570183 Validation loss 0.062080904841423035 Accuracy 0.3772999942302704\n",
      "Iteration 1240 Training loss 0.06282886862754822 Validation loss 0.06198464334011078 Accuracy 0.3781999945640564\n",
      "Iteration 1250 Training loss 0.06169695034623146 Validation loss 0.06180540844798088 Accuracy 0.3797000050544739\n",
      "Iteration 1260 Training loss 0.06558238714933395 Validation loss 0.06267766654491425 Accuracy 0.37059998512268066\n",
      "Iteration 1270 Training loss 0.062185607850551605 Validation loss 0.06184086948633194 Accuracy 0.37929999828338623\n",
      "Iteration 1280 Training loss 0.06265450268983841 Validation loss 0.06176377832889557 Accuracy 0.3799999952316284\n",
      "Iteration 1290 Training loss 0.06103607639670372 Validation loss 0.06175724044442177 Accuracy 0.3797999918460846\n",
      "Iteration 1300 Training loss 0.05991778522729874 Validation loss 0.06177166849374771 Accuracy 0.37929999828338623\n",
      "Iteration 1310 Training loss 0.06014111265540123 Validation loss 0.061939191073179245 Accuracy 0.3781000077724457\n",
      "Iteration 1320 Training loss 0.06209991127252579 Validation loss 0.061830535531044006 Accuracy 0.3790999948978424\n",
      "Iteration 1330 Training loss 0.061673495918512344 Validation loss 0.061578720808029175 Accuracy 0.3815999925136566\n",
      "Iteration 1340 Training loss 0.060814838856458664 Validation loss 0.06170663982629776 Accuracy 0.3801000118255615\n",
      "Iteration 1350 Training loss 0.06037862226366997 Validation loss 0.06235765293240547 Accuracy 0.3734000027179718\n",
      "Iteration 1360 Training loss 0.06127375364303589 Validation loss 0.06217449530959129 Accuracy 0.37560001015663147\n",
      "Iteration 1370 Training loss 0.06531934440135956 Validation loss 0.06185457110404968 Accuracy 0.37860000133514404\n",
      "Iteration 1380 Training loss 0.06181265413761139 Validation loss 0.062007732689380646 Accuracy 0.3772999942302704\n",
      "Iteration 1390 Training loss 0.062173351645469666 Validation loss 0.06161178648471832 Accuracy 0.38089999556541443\n",
      "Iteration 1400 Training loss 0.059231456369161606 Validation loss 0.0617138147354126 Accuracy 0.3797999918460846\n",
      "Iteration 1410 Training loss 0.059042949229478836 Validation loss 0.06176178529858589 Accuracy 0.3790999948978424\n",
      "Iteration 1420 Training loss 0.0599893182516098 Validation loss 0.06219211965799332 Accuracy 0.375\n",
      "Iteration 1430 Training loss 0.058331046253442764 Validation loss 0.061721380800008774 Accuracy 0.38019999861717224\n",
      "Iteration 1440 Training loss 0.0593782439827919 Validation loss 0.06172199547290802 Accuracy 0.38040000200271606\n",
      "Iteration 1450 Training loss 0.06390766054391861 Validation loss 0.061895161867141724 Accuracy 0.3779999911785126\n",
      "Iteration 1460 Training loss 0.060080233961343765 Validation loss 0.06167173013091087 Accuracy 0.38029998540878296\n",
      "Iteration 1470 Training loss 0.05911700427532196 Validation loss 0.06196862459182739 Accuracy 0.37779998779296875\n",
      "Iteration 1480 Training loss 0.06402168422937393 Validation loss 0.061901167035102844 Accuracy 0.3792000114917755\n",
      "Iteration 1490 Training loss 0.06355020403862 Validation loss 0.06229879707098007 Accuracy 0.3749000132083893\n",
      "Iteration 1500 Training loss 0.058934442698955536 Validation loss 0.06172914430499077 Accuracy 0.3808000087738037\n",
      "Iteration 1510 Training loss 0.05904282629489899 Validation loss 0.06169610470533371 Accuracy 0.3806000053882599\n",
      "Iteration 1520 Training loss 0.06179070100188255 Validation loss 0.06166471540927887 Accuracy 0.3808000087738037\n",
      "Iteration 1530 Training loss 0.06307137757539749 Validation loss 0.06171708181500435 Accuracy 0.38040000200271606\n",
      "Iteration 1540 Training loss 0.06336259841918945 Validation loss 0.06205367296934128 Accuracy 0.3767000138759613\n",
      "Iteration 1550 Training loss 0.05933809280395508 Validation loss 0.06164063885807991 Accuracy 0.3804999887943268\n",
      "Iteration 1560 Training loss 0.06473420560359955 Validation loss 0.06222657859325409 Accuracy 0.37560001015663147\n",
      "Iteration 1570 Training loss 0.0591685026884079 Validation loss 0.06173645332455635 Accuracy 0.38040000200271606\n",
      "Iteration 1580 Training loss 0.05901150405406952 Validation loss 0.0619392916560173 Accuracy 0.3776000142097473\n",
      "Iteration 1590 Training loss 0.060888759791851044 Validation loss 0.06176875904202461 Accuracy 0.37869998812675476\n",
      "Iteration 1600 Training loss 0.05935519561171532 Validation loss 0.06072639673948288 Accuracy 0.3894999921321869\n",
      "Iteration 1610 Training loss 0.06045062094926834 Validation loss 0.058611832559108734 Accuracy 0.40880000591278076\n",
      "Iteration 1620 Training loss 0.05558205023407936 Validation loss 0.0575638972222805 Accuracy 0.4205999970436096\n",
      "Iteration 1630 Training loss 0.05631983280181885 Validation loss 0.05797625333070755 Accuracy 0.41519999504089355\n",
      "Iteration 1640 Training loss 0.06258013844490051 Validation loss 0.06198025494813919 Accuracy 0.37549999356269836\n",
      "Iteration 1650 Training loss 0.05675927549600601 Validation loss 0.057030726224184036 Accuracy 0.42559999227523804\n",
      "Iteration 1660 Training loss 0.06031040474772453 Validation loss 0.0590578131377697 Accuracy 0.4050999879837036\n",
      "Iteration 1670 Training loss 0.055133309215307236 Validation loss 0.05646289139986038 Accuracy 0.42989999055862427\n",
      "Iteration 1680 Training loss 0.0588606521487236 Validation loss 0.056920427829027176 Accuracy 0.42590001225471497\n",
      "Iteration 1690 Training loss 0.05912543833255768 Validation loss 0.05788334086537361 Accuracy 0.41690000891685486\n",
      "Iteration 1700 Training loss 0.05947735160589218 Validation loss 0.05974578112363815 Accuracy 0.39890000224113464\n",
      "Iteration 1710 Training loss 0.0567125640809536 Validation loss 0.056831251829862595 Accuracy 0.4268999993801117\n",
      "Iteration 1720 Training loss 0.052931323647499084 Validation loss 0.05677441880106926 Accuracy 0.42879998683929443\n",
      "Iteration 1730 Training loss 0.05660775303840637 Validation loss 0.05672219395637512 Accuracy 0.42879998683929443\n",
      "Iteration 1740 Training loss 0.05498869717121124 Validation loss 0.05639907345175743 Accuracy 0.4323999881744385\n",
      "Iteration 1750 Training loss 0.05533900484442711 Validation loss 0.05632994323968887 Accuracy 0.4334999918937683\n",
      "Iteration 1760 Training loss 0.05565182864665985 Validation loss 0.05655048415064812 Accuracy 0.4311000108718872\n",
      "Iteration 1770 Training loss 0.05010263994336128 Validation loss 0.05674254521727562 Accuracy 0.4284999966621399\n",
      "Iteration 1780 Training loss 0.05797722935676575 Validation loss 0.05739886686205864 Accuracy 0.42100000381469727\n",
      "Iteration 1790 Training loss 0.05971166118979454 Validation loss 0.05829674005508423 Accuracy 0.41370001435279846\n",
      "Iteration 1800 Training loss 0.057684533298015594 Validation loss 0.056557171046733856 Accuracy 0.4300999939441681\n",
      "Iteration 1810 Training loss 0.05811777710914612 Validation loss 0.057937342673540115 Accuracy 0.41690000891685486\n",
      "Iteration 1820 Training loss 0.057223957031965256 Validation loss 0.05601673945784569 Accuracy 0.43650001287460327\n",
      "Iteration 1830 Training loss 0.05981170013546944 Validation loss 0.05583691969513893 Accuracy 0.4381999969482422\n",
      "Iteration 1840 Training loss 0.05503953620791435 Validation loss 0.056170083582401276 Accuracy 0.43459999561309814\n",
      "Iteration 1850 Training loss 0.05841994285583496 Validation loss 0.056207235902547836 Accuracy 0.43389999866485596\n",
      "Iteration 1860 Training loss 0.057846587151288986 Validation loss 0.05668039992451668 Accuracy 0.428600013256073\n",
      "Iteration 1870 Training loss 0.05526128038764 Validation loss 0.05597739666700363 Accuracy 0.4359000027179718\n",
      "Iteration 1880 Training loss 0.05632885918021202 Validation loss 0.056309882551431656 Accuracy 0.4339999854564667\n",
      "Iteration 1890 Training loss 0.05904441326856613 Validation loss 0.056376297026872635 Accuracy 0.43320000171661377\n",
      "Iteration 1900 Training loss 0.058366354554891586 Validation loss 0.05593843385577202 Accuracy 0.43700000643730164\n",
      "Iteration 1910 Training loss 0.062030330300331116 Validation loss 0.05847742035984993 Accuracy 0.41200000047683716\n",
      "Iteration 1920 Training loss 0.059502094984054565 Validation loss 0.05790695175528526 Accuracy 0.41670000553131104\n",
      "Iteration 1930 Training loss 0.05958199128508568 Validation loss 0.05749788135290146 Accuracy 0.421099990606308\n",
      "Iteration 1940 Training loss 0.05645892396569252 Validation loss 0.05630125105381012 Accuracy 0.43470001220703125\n",
      "Iteration 1950 Training loss 0.05438944324851036 Validation loss 0.05622018873691559 Accuracy 0.4343000054359436\n",
      "Iteration 1960 Training loss 0.05662993714213371 Validation loss 0.05664047971367836 Accuracy 0.4307999908924103\n",
      "Iteration 1970 Training loss 0.05723773315548897 Validation loss 0.05585369095206261 Accuracy 0.4381999969482422\n",
      "Iteration 1980 Training loss 0.05555761605501175 Validation loss 0.05601835995912552 Accuracy 0.4375999867916107\n",
      "Iteration 1990 Training loss 0.05726765841245651 Validation loss 0.05586440488696098 Accuracy 0.4374000132083893\n",
      "Iteration 2000 Training loss 0.053905852138996124 Validation loss 0.056283555924892426 Accuracy 0.4334999918937683\n",
      "Iteration 2010 Training loss 0.05641685426235199 Validation loss 0.055549491196870804 Accuracy 0.44119998812675476\n",
      "Iteration 2020 Training loss 0.050950534641742706 Validation loss 0.0556502640247345 Accuracy 0.4390000104904175\n",
      "Iteration 2030 Training loss 0.05339117348194122 Validation loss 0.05556530877947807 Accuracy 0.4408999979496002\n",
      "Iteration 2040 Training loss 0.055564995855093 Validation loss 0.05563634634017944 Accuracy 0.4390999972820282\n",
      "Iteration 2050 Training loss 0.05284656956791878 Validation loss 0.05580419301986694 Accuracy 0.4388999938964844\n",
      "Iteration 2060 Training loss 0.053316883742809296 Validation loss 0.05521368235349655 Accuracy 0.44449999928474426\n",
      "Iteration 2070 Training loss 0.05583469569683075 Validation loss 0.05529552325606346 Accuracy 0.44269999861717224\n",
      "Iteration 2080 Training loss 0.05304306373000145 Validation loss 0.055366773158311844 Accuracy 0.44190001487731934\n",
      "Iteration 2090 Training loss 0.057887427508831024 Validation loss 0.055467505007982254 Accuracy 0.4413999915122986\n",
      "Iteration 2100 Training loss 0.05613572150468826 Validation loss 0.05608311668038368 Accuracy 0.43540000915527344\n",
      "Iteration 2110 Training loss 0.053454626351594925 Validation loss 0.055503156036138535 Accuracy 0.4408999979496002\n",
      "Iteration 2120 Training loss 0.05666256695985794 Validation loss 0.055833037942647934 Accuracy 0.4374000132083893\n",
      "Iteration 2130 Training loss 0.053359776735305786 Validation loss 0.05557942017912865 Accuracy 0.439300000667572\n",
      "Iteration 2140 Training loss 0.05553872138261795 Validation loss 0.0554560124874115 Accuracy 0.4406000077724457\n",
      "Iteration 2150 Training loss 0.05195426940917969 Validation loss 0.05550091713666916 Accuracy 0.4399999976158142\n",
      "Iteration 2160 Training loss 0.05465629696846008 Validation loss 0.05535714328289032 Accuracy 0.44279998540878296\n",
      "Iteration 2170 Training loss 0.05261315777897835 Validation loss 0.05531243234872818 Accuracy 0.44359999895095825\n",
      "Iteration 2180 Training loss 0.05393799766898155 Validation loss 0.05502184480428696 Accuracy 0.44620001316070557\n",
      "Iteration 2190 Training loss 0.053789302706718445 Validation loss 0.05501680076122284 Accuracy 0.445499986410141\n",
      "Iteration 2200 Training loss 0.05562414601445198 Validation loss 0.05634184554219246 Accuracy 0.43309998512268066\n",
      "Iteration 2210 Training loss 0.057565268129110336 Validation loss 0.055620938539505005 Accuracy 0.4399000108242035\n",
      "Iteration 2220 Training loss 0.05501357093453407 Validation loss 0.057439375668764114 Accuracy 0.4221999943256378\n",
      "Iteration 2230 Training loss 0.05254219099879265 Validation loss 0.05548849701881409 Accuracy 0.44029998779296875\n",
      "Iteration 2240 Training loss 0.05342170596122742 Validation loss 0.055713292211294174 Accuracy 0.4388999938964844\n",
      "Iteration 2250 Training loss 0.05287841707468033 Validation loss 0.055505961179733276 Accuracy 0.44020000100135803\n",
      "Iteration 2260 Training loss 0.05483367294073105 Validation loss 0.0558260940015316 Accuracy 0.43799999356269836\n",
      "Iteration 2270 Training loss 0.05839034169912338 Validation loss 0.05727849528193474 Accuracy 0.4235000014305115\n",
      "Iteration 2280 Training loss 0.05308527499437332 Validation loss 0.05595827475190163 Accuracy 0.43560001254081726\n",
      "Iteration 2290 Training loss 0.05459930747747421 Validation loss 0.056183621287345886 Accuracy 0.4343999922275543\n",
      "Iteration 2300 Training loss 0.059234537184238434 Validation loss 0.05595783144235611 Accuracy 0.436599999666214\n",
      "Iteration 2310 Training loss 0.05421716347336769 Validation loss 0.05689706653356552 Accuracy 0.42730000615119934\n",
      "Iteration 2320 Training loss 0.05844414606690407 Validation loss 0.06097175180912018 Accuracy 0.3871000111103058\n",
      "Iteration 2330 Training loss 0.05418481305241585 Validation loss 0.056551553308963776 Accuracy 0.429500013589859\n",
      "Iteration 2340 Training loss 0.05935806408524513 Validation loss 0.05930960923433304 Accuracy 0.40389999747276306\n",
      "Iteration 2350 Training loss 0.053808193653821945 Validation loss 0.05584479495882988 Accuracy 0.43869999051094055\n",
      "Iteration 2360 Training loss 0.05644891411066055 Validation loss 0.05640029162168503 Accuracy 0.43209999799728394\n",
      "Iteration 2370 Training loss 0.05607861652970314 Validation loss 0.055502355098724365 Accuracy 0.4417000114917755\n",
      "Iteration 2380 Training loss 0.05532708391547203 Validation loss 0.055409569293260574 Accuracy 0.44279998540878296\n",
      "Iteration 2390 Training loss 0.05559522658586502 Validation loss 0.05546758696436882 Accuracy 0.4415000081062317\n",
      "Iteration 2400 Training loss 0.05419417470693588 Validation loss 0.05590716004371643 Accuracy 0.43639999628067017\n",
      "Iteration 2410 Training loss 0.05364556238055229 Validation loss 0.055545419454574585 Accuracy 0.4406000077724457\n",
      "Iteration 2420 Training loss 0.05309043824672699 Validation loss 0.05605926364660263 Accuracy 0.4361000061035156\n",
      "Iteration 2430 Training loss 0.05822058022022247 Validation loss 0.056669168174266815 Accuracy 0.429500013589859\n",
      "Iteration 2440 Training loss 0.05312274768948555 Validation loss 0.054911259561777115 Accuracy 0.4465999901294708\n",
      "Iteration 2450 Training loss 0.05731205642223358 Validation loss 0.05556602403521538 Accuracy 0.4390999972820282\n",
      "Iteration 2460 Training loss 0.05215172469615936 Validation loss 0.05491403490304947 Accuracy 0.4465000033378601\n",
      "Iteration 2470 Training loss 0.05552433803677559 Validation loss 0.05503276363015175 Accuracy 0.4442000091075897\n",
      "Iteration 2480 Training loss 0.05434206500649452 Validation loss 0.05502137541770935 Accuracy 0.4449999928474426\n",
      "Iteration 2490 Training loss 0.05376952514052391 Validation loss 0.054629936814308167 Accuracy 0.4487000107765198\n",
      "Iteration 2500 Training loss 0.05266096070408821 Validation loss 0.05486157163977623 Accuracy 0.44690001010894775\n",
      "Iteration 2510 Training loss 0.05593973398208618 Validation loss 0.05556381866335869 Accuracy 0.4390000104904175\n",
      "Iteration 2520 Training loss 0.05931929126381874 Validation loss 0.055297963321208954 Accuracy 0.44200000166893005\n",
      "Iteration 2530 Training loss 0.06353919208049774 Validation loss 0.05871252343058586 Accuracy 0.4090999960899353\n",
      "Iteration 2540 Training loss 0.0555129200220108 Validation loss 0.05470564588904381 Accuracy 0.44769999384880066\n",
      "Iteration 2550 Training loss 0.053561124950647354 Validation loss 0.054931122809648514 Accuracy 0.445499986410141\n",
      "Iteration 2560 Training loss 0.054825592786073685 Validation loss 0.054847411811351776 Accuracy 0.4471000134944916\n",
      "Iteration 2570 Training loss 0.053289443254470825 Validation loss 0.058217357844114304 Accuracy 0.41440001130104065\n",
      "Iteration 2580 Training loss 0.05486907437443733 Validation loss 0.054979223757982254 Accuracy 0.4458000063896179\n",
      "Iteration 2590 Training loss 0.056001994758844376 Validation loss 0.05548009276390076 Accuracy 0.44029998779296875\n",
      "Iteration 2600 Training loss 0.05372694879770279 Validation loss 0.054831214249134064 Accuracy 0.4471000134944916\n",
      "Iteration 2610 Training loss 0.05711250752210617 Validation loss 0.05486198142170906 Accuracy 0.4465999901294708\n",
      "Iteration 2620 Training loss 0.055193960666656494 Validation loss 0.055246781557798386 Accuracy 0.4440000057220459\n",
      "Iteration 2630 Training loss 0.05561231076717377 Validation loss 0.05598120018839836 Accuracy 0.43560001254081726\n",
      "Iteration 2640 Training loss 0.055078644305467606 Validation loss 0.055355582386255264 Accuracy 0.44119998812675476\n",
      "Iteration 2650 Training loss 0.05390990152955055 Validation loss 0.0560438372194767 Accuracy 0.43459999561309814\n",
      "Iteration 2660 Training loss 0.057524729520082474 Validation loss 0.05641891434788704 Accuracy 0.43140000104904175\n",
      "Iteration 2670 Training loss 0.05238236486911774 Validation loss 0.05489317327737808 Accuracy 0.4462999999523163\n",
      "Iteration 2680 Training loss 0.052173394709825516 Validation loss 0.05497540906071663 Accuracy 0.44609999656677246\n",
      "Iteration 2690 Training loss 0.054164860397577286 Validation loss 0.057027120143175125 Accuracy 0.4259999990463257\n",
      "Iteration 2700 Training loss 0.05241182819008827 Validation loss 0.05644179508090019 Accuracy 0.4320000112056732\n",
      "Iteration 2710 Training loss 0.05391900986433029 Validation loss 0.05492126941680908 Accuracy 0.4471000134944916\n",
      "Iteration 2720 Training loss 0.0580863393843174 Validation loss 0.058050040155649185 Accuracy 0.41510000824928284\n",
      "Iteration 2730 Training loss 0.054163508117198944 Validation loss 0.05545484274625778 Accuracy 0.4413999915122986\n",
      "Iteration 2740 Training loss 0.05610337853431702 Validation loss 0.05634354054927826 Accuracy 0.43160000443458557\n",
      "Iteration 2750 Training loss 0.057712361216545105 Validation loss 0.0567793995141983 Accuracy 0.4284999966621399\n",
      "Iteration 2760 Training loss 0.05298057943582535 Validation loss 0.05604426935315132 Accuracy 0.4359999895095825\n",
      "Iteration 2770 Training loss 0.05291668325662613 Validation loss 0.05490009859204292 Accuracy 0.44749999046325684\n",
      "Iteration 2780 Training loss 0.056012146174907684 Validation loss 0.05576939508318901 Accuracy 0.4374000132083893\n",
      "Iteration 2790 Training loss 0.05676017329096794 Validation loss 0.055477071553468704 Accuracy 0.44130000472068787\n",
      "Iteration 2800 Training loss 0.0559583343565464 Validation loss 0.054641127586364746 Accuracy 0.4494999945163727\n",
      "Iteration 2810 Training loss 0.057229746133089066 Validation loss 0.0547509603202343 Accuracy 0.44920000433921814\n",
      "Iteration 2820 Training loss 0.05146649107336998 Validation loss 0.054740577936172485 Accuracy 0.44850000739097595\n",
      "Iteration 2830 Training loss 0.05894329771399498 Validation loss 0.05594464763998985 Accuracy 0.43650001287460327\n",
      "Iteration 2840 Training loss 0.05171032249927521 Validation loss 0.05484247952699661 Accuracy 0.4478999972343445\n",
      "Iteration 2850 Training loss 0.054459135979413986 Validation loss 0.05509067326784134 Accuracy 0.445499986410141\n",
      "Iteration 2860 Training loss 0.05648050829768181 Validation loss 0.05525227636098862 Accuracy 0.44350001215934753\n",
      "Iteration 2870 Training loss 0.05522684380412102 Validation loss 0.05477322265505791 Accuracy 0.4487999975681305\n",
      "Iteration 2880 Training loss 0.051413483917713165 Validation loss 0.05506075546145439 Accuracy 0.4465000033378601\n",
      "Iteration 2890 Training loss 0.05316930636763573 Validation loss 0.05489254370331764 Accuracy 0.4465000033378601\n",
      "Iteration 2900 Training loss 0.05809022858738899 Validation loss 0.05544672906398773 Accuracy 0.4417000114917755\n",
      "Iteration 2910 Training loss 0.05558585003018379 Validation loss 0.05505264550447464 Accuracy 0.4453999996185303\n",
      "Iteration 2920 Training loss 0.05833388864994049 Validation loss 0.05495034158229828 Accuracy 0.44589999318122864\n",
      "Iteration 2930 Training loss 0.055487263947725296 Validation loss 0.055935729295015335 Accuracy 0.43639999628067017\n",
      "Iteration 2940 Training loss 0.051012538373470306 Validation loss 0.0553755946457386 Accuracy 0.4410000145435333\n",
      "Iteration 2950 Training loss 0.05474366247653961 Validation loss 0.05644431710243225 Accuracy 0.43050000071525574\n",
      "Iteration 2960 Training loss 0.05328270420432091 Validation loss 0.054842300713062286 Accuracy 0.4478999972343445\n",
      "Iteration 2970 Training loss 0.057180169969797134 Validation loss 0.056227002292871475 Accuracy 0.43380001187324524\n",
      "Iteration 2980 Training loss 0.053066518157720566 Validation loss 0.055089741945266724 Accuracy 0.445499986410141\n",
      "Iteration 2990 Training loss 0.05042378231883049 Validation loss 0.055437322705984116 Accuracy 0.4415999948978424\n",
      "Iteration 3000 Training loss 0.058456551283597946 Validation loss 0.05629725009202957 Accuracy 0.43220001459121704\n",
      "Iteration 3010 Training loss 0.0583682656288147 Validation loss 0.05480996146798134 Accuracy 0.4478999972343445\n",
      "Iteration 3020 Training loss 0.05525930970907211 Validation loss 0.055697325617074966 Accuracy 0.43939998745918274\n",
      "Iteration 3030 Training loss 0.05593903735280037 Validation loss 0.054740142077207565 Accuracy 0.4487000107765198\n",
      "Iteration 3040 Training loss 0.053727660328149796 Validation loss 0.054979950189590454 Accuracy 0.4456999897956848\n",
      "Iteration 3050 Training loss 0.05472901463508606 Validation loss 0.05485042557120323 Accuracy 0.4474000036716461\n",
      "Iteration 3060 Training loss 0.051951512694358826 Validation loss 0.056123167276382446 Accuracy 0.4350999891757965\n",
      "Iteration 3070 Training loss 0.05040765553712845 Validation loss 0.05616011843085289 Accuracy 0.43459999561309814\n",
      "Iteration 3080 Training loss 0.05454004555940628 Validation loss 0.0550306998193264 Accuracy 0.4456999897956848\n",
      "Iteration 3090 Training loss 0.05436265096068382 Validation loss 0.05500180646777153 Accuracy 0.446399986743927\n",
      "Iteration 3100 Training loss 0.055364832282066345 Validation loss 0.05512596666812897 Accuracy 0.4440000057220459\n",
      "Iteration 3110 Training loss 0.04971221461892128 Validation loss 0.05500450357794762 Accuracy 0.44620001316070557\n",
      "Iteration 3120 Training loss 0.05364793911576271 Validation loss 0.05481753498315811 Accuracy 0.4472000002861023\n",
      "Iteration 3130 Training loss 0.057424478232860565 Validation loss 0.0553094781935215 Accuracy 0.4433000087738037\n",
      "Iteration 3140 Training loss 0.05123699828982353 Validation loss 0.054682664573192596 Accuracy 0.44839999079704285\n",
      "Iteration 3150 Training loss 0.05740734934806824 Validation loss 0.05502662435173988 Accuracy 0.4456999897956848\n",
      "Iteration 3160 Training loss 0.053436096757650375 Validation loss 0.05561276897788048 Accuracy 0.4404999911785126\n",
      "Iteration 3170 Training loss 0.05339905247092247 Validation loss 0.055680032819509506 Accuracy 0.4390000104904175\n",
      "Iteration 3180 Training loss 0.05499517172574997 Validation loss 0.0548875592648983 Accuracy 0.44679999351501465\n",
      "Iteration 3190 Training loss 0.053755443543195724 Validation loss 0.05612635239958763 Accuracy 0.43389999866485596\n",
      "Iteration 3200 Training loss 0.054944250732660294 Validation loss 0.05468980595469475 Accuracy 0.44780001044273376\n",
      "Iteration 3210 Training loss 0.05327816680073738 Validation loss 0.05469268560409546 Accuracy 0.4487000107765198\n",
      "Iteration 3220 Training loss 0.050257373601198196 Validation loss 0.05506641045212746 Accuracy 0.44510000944137573\n",
      "Iteration 3230 Training loss 0.05584624782204628 Validation loss 0.05492228642106056 Accuracy 0.446399986743927\n",
      "Iteration 3240 Training loss 0.05287133902311325 Validation loss 0.055302694439888 Accuracy 0.44290000200271606\n",
      "Iteration 3250 Training loss 0.05338742956519127 Validation loss 0.054964564740657806 Accuracy 0.446399986743927\n",
      "Iteration 3260 Training loss 0.056280817836523056 Validation loss 0.05551480874419212 Accuracy 0.4399999976158142\n",
      "Iteration 3270 Training loss 0.054195791482925415 Validation loss 0.054933320730924606 Accuracy 0.4462999999523163\n",
      "Iteration 3280 Training loss 0.0532212033867836 Validation loss 0.055069562047719955 Accuracy 0.4447999894618988\n",
      "Iteration 3290 Training loss 0.05427850782871246 Validation loss 0.05572719871997833 Accuracy 0.43860000371932983\n",
      "Iteration 3300 Training loss 0.05511344224214554 Validation loss 0.05653556063771248 Accuracy 0.4311000108718872\n",
      "Iteration 3310 Training loss 0.05587419122457504 Validation loss 0.05472251772880554 Accuracy 0.44839999079704285\n",
      "Iteration 3320 Training loss 0.055214155465364456 Validation loss 0.056845106184482574 Accuracy 0.42719998955726624\n",
      "Iteration 3330 Training loss 0.053736694157123566 Validation loss 0.055079832673072815 Accuracy 0.4449999928474426\n",
      "Iteration 3340 Training loss 0.05351737141609192 Validation loss 0.05476948246359825 Accuracy 0.4478999972343445\n",
      "Iteration 3350 Training loss 0.0564374178647995 Validation loss 0.05498974025249481 Accuracy 0.445499986410141\n",
      "Iteration 3360 Training loss 0.05796762555837631 Validation loss 0.05490731820464134 Accuracy 0.44620001316070557\n",
      "Iteration 3370 Training loss 0.05299818888306618 Validation loss 0.05467680096626282 Accuracy 0.44850000739097595\n",
      "Iteration 3380 Training loss 0.05441605672240257 Validation loss 0.055285919457674026 Accuracy 0.44269999861717224\n",
      "Iteration 3390 Training loss 0.05590998753905296 Validation loss 0.05556746944785118 Accuracy 0.4397999942302704\n",
      "Iteration 3400 Training loss 0.0544302724301815 Validation loss 0.05493846535682678 Accuracy 0.4458000063896179\n",
      "Iteration 3410 Training loss 0.05662493407726288 Validation loss 0.05505324527621269 Accuracy 0.44609999656677246\n",
      "Iteration 3420 Training loss 0.058687154203653336 Validation loss 0.054661355912685394 Accuracy 0.4489000141620636\n",
      "Iteration 3430 Training loss 0.05344857648015022 Validation loss 0.05485031381249428 Accuracy 0.44670000672340393\n",
      "Iteration 3440 Training loss 0.055621352046728134 Validation loss 0.055025435984134674 Accuracy 0.4456999897956848\n",
      "Iteration 3450 Training loss 0.05389128252863884 Validation loss 0.05313534289598465 Accuracy 0.4625000059604645\n",
      "Iteration 3460 Training loss 0.045640427619218826 Validation loss 0.04806606471538544 Accuracy 0.5153999924659729\n",
      "Iteration 3470 Training loss 0.046214308589696884 Validation loss 0.04679155349731445 Accuracy 0.5278000235557556\n",
      "Iteration 3480 Training loss 0.0447048656642437 Validation loss 0.04657392576336861 Accuracy 0.5299999713897705\n",
      "Iteration 3490 Training loss 0.04602164775133133 Validation loss 0.047459930181503296 Accuracy 0.5199000239372253\n",
      "Iteration 3500 Training loss 0.046175118535757065 Validation loss 0.046607762575149536 Accuracy 0.5296000242233276\n",
      "Iteration 3510 Training loss 0.04830547422170639 Validation loss 0.04834168031811714 Accuracy 0.5117999911308289\n",
      "Iteration 3520 Training loss 0.04194526746869087 Validation loss 0.04657702520489693 Accuracy 0.5291000008583069\n",
      "Iteration 3530 Training loss 0.04756215587258339 Validation loss 0.046522852033376694 Accuracy 0.5303999781608582\n",
      "Iteration 3540 Training loss 0.047181397676467896 Validation loss 0.046226222068071365 Accuracy 0.5343999862670898\n",
      "Iteration 3550 Training loss 0.048866551369428635 Validation loss 0.047340232878923416 Accuracy 0.5224999785423279\n",
      "Iteration 3560 Training loss 0.045706409960985184 Validation loss 0.0467194989323616 Accuracy 0.5281999707221985\n",
      "Iteration 3570 Training loss 0.0432804599404335 Validation loss 0.04670353978872299 Accuracy 0.527999997138977\n",
      "Iteration 3580 Training loss 0.0468498095870018 Validation loss 0.04801972582936287 Accuracy 0.5156999826431274\n",
      "Iteration 3590 Training loss 0.05039020627737045 Validation loss 0.04669889062643051 Accuracy 0.5285000205039978\n",
      "Iteration 3600 Training loss 0.04446551948785782 Validation loss 0.048052381724119186 Accuracy 0.5157999992370605\n",
      "Iteration 3610 Training loss 0.0451464019715786 Validation loss 0.046867698431015015 Accuracy 0.5274999737739563\n",
      "Iteration 3620 Training loss 0.048086997121572495 Validation loss 0.04724973067641258 Accuracy 0.5231999754905701\n",
      "Iteration 3630 Training loss 0.04820461571216583 Validation loss 0.047692716121673584 Accuracy 0.5182999968528748\n",
      "Iteration 3640 Training loss 0.042485881596803665 Validation loss 0.046329230070114136 Accuracy 0.5325000286102295\n",
      "Iteration 3650 Training loss 0.04883720353245735 Validation loss 0.046252183616161346 Accuracy 0.532800018787384\n",
      "Iteration 3660 Training loss 0.045628633350133896 Validation loss 0.046360332518815994 Accuracy 0.5325000286102295\n",
      "Iteration 3670 Training loss 0.04594044014811516 Validation loss 0.0465967059135437 Accuracy 0.5289999842643738\n",
      "Iteration 3680 Training loss 0.04084449261426926 Validation loss 0.046557020395994186 Accuracy 0.5307000279426575\n",
      "Iteration 3690 Training loss 0.045784905552864075 Validation loss 0.04610034078359604 Accuracy 0.535099983215332\n",
      "Iteration 3700 Training loss 0.040251944214105606 Validation loss 0.04620997980237007 Accuracy 0.5339000225067139\n",
      "Iteration 3710 Training loss 0.04665154963731766 Validation loss 0.045989710837602615 Accuracy 0.5368000268936157\n",
      "Iteration 3720 Training loss 0.04352801665663719 Validation loss 0.046671006828546524 Accuracy 0.5286999940872192\n",
      "Iteration 3730 Training loss 0.0415714867413044 Validation loss 0.04594672843813896 Accuracy 0.5375999808311462\n",
      "Iteration 3740 Training loss 0.04612518474459648 Validation loss 0.04594206064939499 Accuracy 0.5374000072479248\n",
      "Iteration 3750 Training loss 0.0459812730550766 Validation loss 0.04732513800263405 Accuracy 0.5223000049591064\n",
      "Iteration 3760 Training loss 0.04784121364355087 Validation loss 0.04599780589342117 Accuracy 0.536899983882904\n",
      "Iteration 3770 Training loss 0.04331885278224945 Validation loss 0.04594377055764198 Accuracy 0.5378999710083008\n",
      "Iteration 3780 Training loss 0.04819504916667938 Validation loss 0.04915360361337662 Accuracy 0.5052000284194946\n",
      "Iteration 3790 Training loss 0.04617506265640259 Validation loss 0.046667955815792084 Accuracy 0.531000018119812\n",
      "Iteration 3800 Training loss 0.043423883616924286 Validation loss 0.046185169368982315 Accuracy 0.535099983215332\n",
      "Iteration 3810 Training loss 0.04770764708518982 Validation loss 0.04677534103393555 Accuracy 0.5293999910354614\n",
      "Iteration 3820 Training loss 0.044723913073539734 Validation loss 0.046280570328235626 Accuracy 0.5342000126838684\n",
      "Iteration 3830 Training loss 0.0474068857729435 Validation loss 0.04618477448821068 Accuracy 0.5347999930381775\n",
      "Iteration 3840 Training loss 0.04418954998254776 Validation loss 0.04679042100906372 Accuracy 0.5281000137329102\n",
      "Iteration 3850 Training loss 0.045714110136032104 Validation loss 0.04603152722120285 Accuracy 0.5356000065803528\n",
      "Iteration 3860 Training loss 0.04419029876589775 Validation loss 0.04632524773478508 Accuracy 0.5321999788284302\n",
      "Iteration 3870 Training loss 0.04707110673189163 Validation loss 0.04744545370340347 Accuracy 0.5205000042915344\n",
      "Iteration 3880 Training loss 0.04698878154158592 Validation loss 0.04623296856880188 Accuracy 0.5338000059127808\n",
      "Iteration 3890 Training loss 0.04279119893908501 Validation loss 0.04699956625699997 Accuracy 0.5253999829292297\n",
      "Iteration 3900 Training loss 0.042559318244457245 Validation loss 0.046666219830513 Accuracy 0.5288000106811523\n",
      "Iteration 3910 Training loss 0.04807441309094429 Validation loss 0.04633725434541702 Accuracy 0.5322999954223633\n",
      "Iteration 3920 Training loss 0.046826109290122986 Validation loss 0.04654359444975853 Accuracy 0.5306000113487244\n",
      "Iteration 3930 Training loss 0.045253414660692215 Validation loss 0.046533361077308655 Accuracy 0.531000018119812\n",
      "Iteration 3940 Training loss 0.043054576963186264 Validation loss 0.04704307019710541 Accuracy 0.525600016117096\n",
      "Iteration 3950 Training loss 0.045796968042850494 Validation loss 0.04654330387711525 Accuracy 0.5307000279426575\n",
      "Iteration 3960 Training loss 0.04393087327480316 Validation loss 0.04715302586555481 Accuracy 0.5250999927520752\n",
      "Iteration 3970 Training loss 0.04830889031291008 Validation loss 0.04645702615380287 Accuracy 0.5329999923706055\n",
      "Iteration 3980 Training loss 0.04352797567844391 Validation loss 0.04636676236987114 Accuracy 0.5332000255584717\n",
      "Iteration 3990 Training loss 0.04494578391313553 Validation loss 0.04650949314236641 Accuracy 0.5317000150680542\n",
      "Iteration 4000 Training loss 0.04728654399514198 Validation loss 0.046663686633110046 Accuracy 0.5297999978065491\n",
      "Iteration 4010 Training loss 0.04194524139165878 Validation loss 0.046347443014383316 Accuracy 0.5336999893188477\n",
      "Iteration 4020 Training loss 0.04807509481906891 Validation loss 0.046323612332344055 Accuracy 0.53329998254776\n",
      "Iteration 4030 Training loss 0.0478643998503685 Validation loss 0.047759998589754105 Accuracy 0.5184000134468079\n",
      "Iteration 4040 Training loss 0.047514669597148895 Validation loss 0.04695591703057289 Accuracy 0.5263000130653381\n",
      "Iteration 4050 Training loss 0.04658851772546768 Validation loss 0.046649180352687836 Accuracy 0.5300999879837036\n",
      "Iteration 4060 Training loss 0.049059249460697174 Validation loss 0.04790056496858597 Accuracy 0.5170000195503235\n",
      "Iteration 4070 Training loss 0.044186558574438095 Validation loss 0.0460488460958004 Accuracy 0.5360999703407288\n",
      "Iteration 4080 Training loss 0.043439317494630814 Validation loss 0.04579363763332367 Accuracy 0.5385000109672546\n",
      "Iteration 4090 Training loss 0.04236816242337227 Validation loss 0.04579095542430878 Accuracy 0.5376999974250793\n",
      "Iteration 4100 Training loss 0.0441659651696682 Validation loss 0.045775752514600754 Accuracy 0.5378999710083008\n",
      "Iteration 4110 Training loss 0.048629481345415115 Validation loss 0.046198826283216476 Accuracy 0.5332000255584717\n",
      "Iteration 4120 Training loss 0.04491962864995003 Validation loss 0.046087704598903656 Accuracy 0.5350000262260437\n",
      "Iteration 4130 Training loss 0.04641347378492355 Validation loss 0.04610559716820717 Accuracy 0.5354999899864197\n",
      "Iteration 4140 Training loss 0.04758240655064583 Validation loss 0.046343978494405746 Accuracy 0.5324000120162964\n",
      "Iteration 4150 Training loss 0.047817163169384 Validation loss 0.04640282317996025 Accuracy 0.5310999751091003\n",
      "Iteration 4160 Training loss 0.045433446764945984 Validation loss 0.046082742512226105 Accuracy 0.5356000065803528\n",
      "Iteration 4170 Training loss 0.044427067041397095 Validation loss 0.045914068818092346 Accuracy 0.5364999771118164\n",
      "Iteration 4180 Training loss 0.04737664759159088 Validation loss 0.046015672385692596 Accuracy 0.536300003528595\n",
      "Iteration 4190 Training loss 0.045566823333501816 Validation loss 0.04577399417757988 Accuracy 0.5385000109672546\n",
      "Iteration 4200 Training loss 0.040958553552627563 Validation loss 0.046174366027116776 Accuracy 0.5335000157356262\n",
      "Iteration 4210 Training loss 0.04209234192967415 Validation loss 0.046006493270397186 Accuracy 0.5360000133514404\n",
      "Iteration 4220 Training loss 0.043176598846912384 Validation loss 0.046152401715517044 Accuracy 0.5343000292778015\n",
      "Iteration 4230 Training loss 0.04913904890418053 Validation loss 0.046633556485176086 Accuracy 0.5293999910354614\n",
      "Iteration 4240 Training loss 0.04310718923807144 Validation loss 0.045715633779764175 Accuracy 0.5385000109672546\n",
      "Iteration 4250 Training loss 0.04945015907287598 Validation loss 0.046252042055130005 Accuracy 0.5328999757766724\n",
      "Iteration 4260 Training loss 0.043658025562763214 Validation loss 0.045611847192049026 Accuracy 0.5394999980926514\n",
      "Iteration 4270 Training loss 0.04517119750380516 Validation loss 0.04621577262878418 Accuracy 0.5329999923706055\n",
      "Iteration 4280 Training loss 0.04320931434631348 Validation loss 0.0462297648191452 Accuracy 0.5338000059127808\n",
      "Iteration 4290 Training loss 0.04826868698000908 Validation loss 0.04650897905230522 Accuracy 0.5307000279426575\n",
      "Iteration 4300 Training loss 0.04284623637795448 Validation loss 0.04636330157518387 Accuracy 0.5317999720573425\n",
      "Iteration 4310 Training loss 0.045329783111810684 Validation loss 0.045882269740104675 Accuracy 0.5367000102996826\n",
      "Iteration 4320 Training loss 0.044308483600616455 Validation loss 0.04630160331726074 Accuracy 0.5320000052452087\n",
      "Iteration 4330 Training loss 0.043598927557468414 Validation loss 0.04595079645514488 Accuracy 0.5357000231742859\n",
      "Iteration 4340 Training loss 0.04428258165717125 Validation loss 0.04592558741569519 Accuracy 0.5364999771118164\n",
      "Iteration 4350 Training loss 0.04787905886769295 Validation loss 0.04562452808022499 Accuracy 0.5392000079154968\n",
      "Iteration 4360 Training loss 0.043308548629283905 Validation loss 0.047211937606334686 Accuracy 0.5236999988555908\n",
      "Iteration 4370 Training loss 0.04335102438926697 Validation loss 0.045914433896541595 Accuracy 0.5360000133514404\n",
      "Iteration 4380 Training loss 0.047665271908044815 Validation loss 0.047226645052433014 Accuracy 0.5235999822616577\n",
      "Iteration 4390 Training loss 0.04476457089185715 Validation loss 0.04701286181807518 Accuracy 0.5260999798774719\n",
      "Iteration 4400 Training loss 0.043373145163059235 Validation loss 0.04572933912277222 Accuracy 0.5389000177383423\n",
      "Iteration 4410 Training loss 0.04471248760819435 Validation loss 0.04667587950825691 Accuracy 0.5289999842643738\n",
      "Iteration 4420 Training loss 0.045403316617012024 Validation loss 0.047232430428266525 Accuracy 0.5231999754905701\n",
      "Iteration 4430 Training loss 0.04363656044006348 Validation loss 0.04664510115981102 Accuracy 0.52920001745224\n",
      "Iteration 4440 Training loss 0.04753749072551727 Validation loss 0.04570652171969414 Accuracy 0.5385000109672546\n",
      "Iteration 4450 Training loss 0.04621824622154236 Validation loss 0.04567962512373924 Accuracy 0.5389999747276306\n",
      "Iteration 4460 Training loss 0.046256277710199356 Validation loss 0.046685393899679184 Accuracy 0.5284000039100647\n",
      "Iteration 4470 Training loss 0.04469846561551094 Validation loss 0.046258095651865005 Accuracy 0.5324000120162964\n",
      "Iteration 4480 Training loss 0.044963765889406204 Validation loss 0.046098291873931885 Accuracy 0.5339999794960022\n",
      "Iteration 4490 Training loss 0.040713515132665634 Validation loss 0.04594545438885689 Accuracy 0.5364000201225281\n",
      "Iteration 4500 Training loss 0.05012768879532814 Validation loss 0.04853179678320885 Accuracy 0.5102999806404114\n",
      "Iteration 4510 Training loss 0.044019490480422974 Validation loss 0.045849405229091644 Accuracy 0.5368000268936157\n",
      "Iteration 4520 Training loss 0.04691718891263008 Validation loss 0.046326134353876114 Accuracy 0.5321999788284302\n",
      "Iteration 4530 Training loss 0.04294847697019577 Validation loss 0.04586956650018692 Accuracy 0.5368000268936157\n",
      "Iteration 4540 Training loss 0.048522207885980606 Validation loss 0.045750971883535385 Accuracy 0.5378999710083008\n",
      "Iteration 4550 Training loss 0.045490019023418427 Validation loss 0.046404879540205 Accuracy 0.5321000218391418\n",
      "Iteration 4560 Training loss 0.0424322746694088 Validation loss 0.045798271894454956 Accuracy 0.5375000238418579\n",
      "Iteration 4570 Training loss 0.04574092477560043 Validation loss 0.045758433640003204 Accuracy 0.538100004196167\n",
      "Iteration 4580 Training loss 0.04358919709920883 Validation loss 0.04599476605653763 Accuracy 0.5357999801635742\n",
      "Iteration 4590 Training loss 0.0432392843067646 Validation loss 0.04616544395685196 Accuracy 0.5342000126838684\n",
      "Iteration 4600 Training loss 0.043800435960292816 Validation loss 0.04603064805269241 Accuracy 0.5350000262260437\n",
      "Iteration 4610 Training loss 0.04628415405750275 Validation loss 0.04643252491950989 Accuracy 0.5309000015258789\n",
      "Iteration 4620 Training loss 0.04324645921587944 Validation loss 0.04557670280337334 Accuracy 0.5396999716758728\n",
      "Iteration 4630 Training loss 0.044502899050712585 Validation loss 0.04576177895069122 Accuracy 0.5382999777793884\n",
      "Iteration 4640 Training loss 0.04293050244450569 Validation loss 0.04594302177429199 Accuracy 0.536300003528595\n",
      "Iteration 4650 Training loss 0.04681261256337166 Validation loss 0.047871798276901245 Accuracy 0.5177000164985657\n",
      "Iteration 4660 Training loss 0.0416945144534111 Validation loss 0.046010781079530716 Accuracy 0.5356000065803528\n",
      "Iteration 4670 Training loss 0.04529518634080887 Validation loss 0.046936046332120895 Accuracy 0.5260999798774719\n",
      "Iteration 4680 Training loss 0.03978893160820007 Validation loss 0.04603708162903786 Accuracy 0.5354999899864197\n",
      "Iteration 4690 Training loss 0.04656823351979256 Validation loss 0.046161290258169174 Accuracy 0.5343999862670898\n",
      "Iteration 4700 Training loss 0.046427905559539795 Validation loss 0.045521073043346405 Accuracy 0.5404000282287598\n",
      "Iteration 4710 Training loss 0.04700947180390358 Validation loss 0.045721277594566345 Accuracy 0.538100004196167\n",
      "Iteration 4720 Training loss 0.046916183084249496 Validation loss 0.04649703949689865 Accuracy 0.5304999947547913\n",
      "Iteration 4730 Training loss 0.040986914187669754 Validation loss 0.04595144838094711 Accuracy 0.5368000268936157\n",
      "Iteration 4740 Training loss 0.041182953864336014 Validation loss 0.04653569310903549 Accuracy 0.5307999849319458\n",
      "Iteration 4750 Training loss 0.047118183225393295 Validation loss 0.04688860848546028 Accuracy 0.527400016784668\n",
      "Iteration 4760 Training loss 0.04188172519207001 Validation loss 0.045903321355581284 Accuracy 0.5364999771118164\n",
      "Iteration 4770 Training loss 0.046592652797698975 Validation loss 0.04593096673488617 Accuracy 0.5368000268936157\n",
      "Iteration 4780 Training loss 0.04437550902366638 Validation loss 0.04612579941749573 Accuracy 0.5351999998092651\n",
      "Iteration 4790 Training loss 0.04627988860011101 Validation loss 0.04551786929368973 Accuracy 0.5403000116348267\n",
      "Iteration 4800 Training loss 0.04739648476243019 Validation loss 0.04570940509438515 Accuracy 0.5386000275611877\n",
      "Iteration 4810 Training loss 0.04577295482158661 Validation loss 0.0458313412964344 Accuracy 0.5371000170707703\n",
      "Iteration 4820 Training loss 0.04019559547305107 Validation loss 0.04592360183596611 Accuracy 0.5357000231742859\n",
      "Iteration 4830 Training loss 0.045119430869817734 Validation loss 0.04633410647511482 Accuracy 0.5324000120162964\n",
      "Iteration 4840 Training loss 0.039628248661756516 Validation loss 0.046420883387327194 Accuracy 0.5311999917030334\n",
      "Iteration 4850 Training loss 0.04795478284358978 Validation loss 0.04588630050420761 Accuracy 0.5368000268936157\n",
      "Iteration 4860 Training loss 0.0501779168844223 Validation loss 0.04748564958572388 Accuracy 0.5206000208854675\n",
      "Iteration 4870 Training loss 0.04165395721793175 Validation loss 0.04570898413658142 Accuracy 0.5389000177383423\n",
      "Iteration 4880 Training loss 0.039832815527915955 Validation loss 0.04550550878047943 Accuracy 0.5407000184059143\n",
      "Iteration 4890 Training loss 0.046236541122198105 Validation loss 0.045405395328998566 Accuracy 0.5414999723434448\n",
      "Iteration 4900 Training loss 0.04455500841140747 Validation loss 0.04554525017738342 Accuracy 0.5408999919891357\n",
      "Iteration 4910 Training loss 0.045167308300733566 Validation loss 0.04556230083107948 Accuracy 0.5397999882698059\n",
      "Iteration 4920 Training loss 0.04428265988826752 Validation loss 0.04595312848687172 Accuracy 0.5360999703407288\n",
      "Iteration 4930 Training loss 0.04312077909708023 Validation loss 0.04608847200870514 Accuracy 0.534500002861023\n",
      "Iteration 4940 Training loss 0.042643509805202484 Validation loss 0.04552961885929108 Accuracy 0.5407000184059143\n",
      "Iteration 4950 Training loss 0.04852665588259697 Validation loss 0.045960355550050735 Accuracy 0.5364000201225281\n",
      "Iteration 4960 Training loss 0.047879695892333984 Validation loss 0.04561135172843933 Accuracy 0.539900004863739\n",
      "Iteration 4970 Training loss 0.04673076048493385 Validation loss 0.04586729034781456 Accuracy 0.5372999906539917\n",
      "Iteration 4980 Training loss 0.0476207360625267 Validation loss 0.04619457945227623 Accuracy 0.5339000225067139\n",
      "Iteration 4990 Training loss 0.045145489275455475 Validation loss 0.04566282406449318 Accuracy 0.5400000214576721\n",
      "Iteration 5000 Training loss 0.04254237189888954 Validation loss 0.045733071863651276 Accuracy 0.5383999943733215\n",
      "Iteration 5010 Training loss 0.04362357035279274 Validation loss 0.04544103890657425 Accuracy 0.5414000153541565\n",
      "Iteration 5020 Training loss 0.04624564200639725 Validation loss 0.04562983289361 Accuracy 0.5400999784469604\n",
      "Iteration 5030 Training loss 0.04628795012831688 Validation loss 0.04598385840654373 Accuracy 0.5353999733924866\n",
      "Iteration 5040 Training loss 0.044956427067518234 Validation loss 0.04559875652194023 Accuracy 0.5393000245094299\n",
      "Iteration 5050 Training loss 0.048497524112463 Validation loss 0.046817731112241745 Accuracy 0.527400016784668\n",
      "Iteration 5060 Training loss 0.045810211449861526 Validation loss 0.045589957386255264 Accuracy 0.5388000011444092\n",
      "Iteration 5070 Training loss 0.04503699392080307 Validation loss 0.04557975009083748 Accuracy 0.5396999716758728\n",
      "Iteration 5080 Training loss 0.04337959736585617 Validation loss 0.04590737447142601 Accuracy 0.5371000170707703\n",
      "Iteration 5090 Training loss 0.041519664227962494 Validation loss 0.045596033334732056 Accuracy 0.5400000214576721\n",
      "Iteration 5100 Training loss 0.043523192405700684 Validation loss 0.04570747911930084 Accuracy 0.5396000146865845\n",
      "Iteration 5110 Training loss 0.04590233415365219 Validation loss 0.04546515271067619 Accuracy 0.5411999821662903\n",
      "Iteration 5120 Training loss 0.04529055207967758 Validation loss 0.045767053961753845 Accuracy 0.5372999906539917\n",
      "Iteration 5130 Training loss 0.044219065457582474 Validation loss 0.046602144837379456 Accuracy 0.5295000076293945\n",
      "Iteration 5140 Training loss 0.045087117701768875 Validation loss 0.04536716639995575 Accuracy 0.5418999791145325\n",
      "Iteration 5150 Training loss 0.04107310622930527 Validation loss 0.045747313648462296 Accuracy 0.5388000011444092\n",
      "Iteration 5160 Training loss 0.049202512949705124 Validation loss 0.04576049745082855 Accuracy 0.5376999974250793\n",
      "Iteration 5170 Training loss 0.042780980467796326 Validation loss 0.045710403472185135 Accuracy 0.538100004196167\n",
      "Iteration 5180 Training loss 0.0417419895529747 Validation loss 0.04561220854520798 Accuracy 0.5400000214576721\n",
      "Iteration 5190 Training loss 0.04731881245970726 Validation loss 0.046527616679668427 Accuracy 0.5303999781608582\n",
      "Iteration 5200 Training loss 0.043365947902202606 Validation loss 0.04560248181223869 Accuracy 0.5389999747276306\n",
      "Iteration 5210 Training loss 0.042624954134225845 Validation loss 0.04559609293937683 Accuracy 0.5393999814987183\n",
      "Iteration 5220 Training loss 0.04590712860226631 Validation loss 0.04731280729174614 Accuracy 0.5224999785423279\n",
      "Iteration 5230 Training loss 0.04480484873056412 Validation loss 0.045981839299201965 Accuracy 0.5364999771118164\n",
      "Iteration 5240 Training loss 0.04452592134475708 Validation loss 0.045408837497234344 Accuracy 0.541100025177002\n",
      "Iteration 5250 Training loss 0.04839823395013809 Validation loss 0.045722030103206635 Accuracy 0.5389000177383423\n",
      "Iteration 5260 Training loss 0.04698924347758293 Validation loss 0.045437078922986984 Accuracy 0.5419999957084656\n",
      "Iteration 5270 Training loss 0.045139480382204056 Validation loss 0.04565272107720375 Accuracy 0.5385000109672546\n",
      "Iteration 5280 Training loss 0.04760333150625229 Validation loss 0.04615860804915428 Accuracy 0.5332000255584717\n",
      "Iteration 5290 Training loss 0.044132050126791 Validation loss 0.04550101235508919 Accuracy 0.5404000282287598\n",
      "Iteration 5300 Training loss 0.043904099613428116 Validation loss 0.04559577256441116 Accuracy 0.5396000146865845\n",
      "Iteration 5310 Training loss 0.04486541077494621 Validation loss 0.045990291982889175 Accuracy 0.5357000231742859\n",
      "Iteration 5320 Training loss 0.044794950634241104 Validation loss 0.04651627317070961 Accuracy 0.5306000113487244\n",
      "Iteration 5330 Training loss 0.044248729944229126 Validation loss 0.046973273158073425 Accuracy 0.5263000130653381\n",
      "Iteration 5340 Training loss 0.045419640839099884 Validation loss 0.04610449820756912 Accuracy 0.534600019454956\n",
      "Iteration 5350 Training loss 0.0406491756439209 Validation loss 0.04577994719147682 Accuracy 0.5378999710083008\n",
      "Iteration 5360 Training loss 0.04363163188099861 Validation loss 0.04594235494732857 Accuracy 0.5360999703407288\n",
      "Iteration 5370 Training loss 0.046981681138277054 Validation loss 0.04629051685333252 Accuracy 0.5329999923706055\n",
      "Iteration 5380 Training loss 0.045793671160936356 Validation loss 0.0456274040043354 Accuracy 0.5396000146865845\n",
      "Iteration 5390 Training loss 0.045185551047325134 Validation loss 0.04558105394244194 Accuracy 0.539900004863739\n",
      "Iteration 5400 Training loss 0.045668669044971466 Validation loss 0.04568808898329735 Accuracy 0.5389999747276306\n",
      "Iteration 5410 Training loss 0.04367804527282715 Validation loss 0.04590769484639168 Accuracy 0.5358999967575073\n",
      "Iteration 5420 Training loss 0.04534188285470009 Validation loss 0.04566014185547829 Accuracy 0.5386000275611877\n",
      "Iteration 5430 Training loss 0.043536216020584106 Validation loss 0.045479364693164825 Accuracy 0.5394999980926514\n",
      "Iteration 5440 Training loss 0.045795366168022156 Validation loss 0.04671061038970947 Accuracy 0.527899980545044\n",
      "Iteration 5450 Training loss 0.04149743169546127 Validation loss 0.04640261083841324 Accuracy 0.5313000082969666\n",
      "Iteration 5460 Training loss 0.049437910318374634 Validation loss 0.04598841443657875 Accuracy 0.535099983215332\n",
      "Iteration 5470 Training loss 0.04162225499749184 Validation loss 0.04684259369969368 Accuracy 0.5260000228881836\n",
      "Iteration 5480 Training loss 0.046025507152080536 Validation loss 0.0457635335624218 Accuracy 0.5358999967575073\n",
      "Iteration 5490 Training loss 0.04714280366897583 Validation loss 0.04547303542494774 Accuracy 0.539900004863739\n",
      "Iteration 5500 Training loss 0.0502595528960228 Validation loss 0.04561812803149223 Accuracy 0.5394999980926514\n",
      "Iteration 5510 Training loss 0.04566206410527229 Validation loss 0.04592788219451904 Accuracy 0.5358999967575073\n",
      "Iteration 5520 Training loss 0.0448114387691021 Validation loss 0.04546072706580162 Accuracy 0.5400000214576721\n",
      "Iteration 5530 Training loss 0.042785268276929855 Validation loss 0.045333221554756165 Accuracy 0.5418000221252441\n",
      "Iteration 5540 Training loss 0.04439253360033035 Validation loss 0.045714542269706726 Accuracy 0.5372999906539917\n",
      "Iteration 5550 Training loss 0.04399658739566803 Validation loss 0.045860741287469864 Accuracy 0.5361999869346619\n",
      "Iteration 5560 Training loss 0.047887444496154785 Validation loss 0.04581634700298309 Accuracy 0.5365999937057495\n",
      "Iteration 5570 Training loss 0.042143721133470535 Validation loss 0.04536447674036026 Accuracy 0.5415999889373779\n",
      "Iteration 5580 Training loss 0.046403415501117706 Validation loss 0.04537542164325714 Accuracy 0.5414000153541565\n",
      "Iteration 5590 Training loss 0.05128428712487221 Validation loss 0.04750695452094078 Accuracy 0.5199000239372253\n",
      "Iteration 5600 Training loss 0.043770529329776764 Validation loss 0.045450545847415924 Accuracy 0.5400999784469604\n",
      "Iteration 5610 Training loss 0.04397796466946602 Validation loss 0.04530816525220871 Accuracy 0.5424000024795532\n",
      "Iteration 5620 Training loss 0.04836483672261238 Validation loss 0.045457661151885986 Accuracy 0.5404999852180481\n",
      "Iteration 5630 Training loss 0.046734996140003204 Validation loss 0.045353926718235016 Accuracy 0.5414999723434448\n",
      "Iteration 5640 Training loss 0.04311584308743477 Validation loss 0.04596257954835892 Accuracy 0.5364000201225281\n",
      "Iteration 5650 Training loss 0.045425042510032654 Validation loss 0.04573395475745201 Accuracy 0.5375000238418579\n",
      "Iteration 5660 Training loss 0.04552118852734566 Validation loss 0.045395541936159134 Accuracy 0.541100025177002\n",
      "Iteration 5670 Training loss 0.04730531945824623 Validation loss 0.04561059921979904 Accuracy 0.539900004863739\n",
      "Iteration 5680 Training loss 0.0463058203458786 Validation loss 0.0462941974401474 Accuracy 0.5325999855995178\n",
      "Iteration 5690 Training loss 0.042027391493320465 Validation loss 0.045668091624975204 Accuracy 0.5379999876022339\n",
      "Iteration 5700 Training loss 0.04421508312225342 Validation loss 0.045359570533037186 Accuracy 0.5414000153541565\n",
      "Iteration 5710 Training loss 0.04448387771844864 Validation loss 0.04566977173089981 Accuracy 0.5386999845504761\n",
      "Iteration 5720 Training loss 0.04465675354003906 Validation loss 0.045755404978990555 Accuracy 0.5382000207901001\n",
      "Iteration 5730 Training loss 0.04243195056915283 Validation loss 0.04560095816850662 Accuracy 0.5393000245094299\n",
      "Iteration 5740 Training loss 0.044084370136260986 Validation loss 0.04528891295194626 Accuracy 0.5414999723434448\n",
      "Iteration 5750 Training loss 0.04392740875482559 Validation loss 0.04530754312872887 Accuracy 0.5415999889373779\n",
      "Iteration 5760 Training loss 0.045446496456861496 Validation loss 0.04550094157457352 Accuracy 0.5394999980926514\n",
      "Iteration 5770 Training loss 0.04415452107787132 Validation loss 0.04562176391482353 Accuracy 0.5382999777793884\n",
      "Iteration 5780 Training loss 0.04183942824602127 Validation loss 0.045839305967092514 Accuracy 0.536899983882904\n",
      "Iteration 5790 Training loss 0.048281315714120865 Validation loss 0.045731209218502045 Accuracy 0.5379999876022339\n",
      "Iteration 5800 Training loss 0.04020015895366669 Validation loss 0.04531257972121239 Accuracy 0.541700005531311\n",
      "Iteration 5810 Training loss 0.043702106922864914 Validation loss 0.045676928013563156 Accuracy 0.5378000140190125\n",
      "Iteration 5820 Training loss 0.04174353927373886 Validation loss 0.045529190450906754 Accuracy 0.5393000245094299\n",
      "Iteration 5830 Training loss 0.04695278778672218 Validation loss 0.045123424381017685 Accuracy 0.5425999760627747\n",
      "Iteration 5840 Training loss 0.04234878718852997 Validation loss 0.04524298757314682 Accuracy 0.5418999791145325\n",
      "Iteration 5850 Training loss 0.045865751802921295 Validation loss 0.04596153274178505 Accuracy 0.5342000126838684\n",
      "Iteration 5860 Training loss 0.044452037662267685 Validation loss 0.04545115306973457 Accuracy 0.5400999784469604\n",
      "Iteration 5870 Training loss 0.04412613809108734 Validation loss 0.046378377825021744 Accuracy 0.5303999781608582\n",
      "Iteration 5880 Training loss 0.04205348342657089 Validation loss 0.04548145458102226 Accuracy 0.5393000245094299\n",
      "Iteration 5890 Training loss 0.041698798537254333 Validation loss 0.04621624946594238 Accuracy 0.5322999954223633\n",
      "Iteration 5900 Training loss 0.04628322273492813 Validation loss 0.045482147485017776 Accuracy 0.5400999784469604\n",
      "Iteration 5910 Training loss 0.04789193719625473 Validation loss 0.045163094997406006 Accuracy 0.5422000288963318\n",
      "Iteration 5920 Training loss 0.044106632471084595 Validation loss 0.045392733067274094 Accuracy 0.5401999950408936\n",
      "Iteration 5930 Training loss 0.04483772814273834 Validation loss 0.045449160039424896 Accuracy 0.5401999950408936\n",
      "Iteration 5940 Training loss 0.042564164847135544 Validation loss 0.04533218592405319 Accuracy 0.541700005531311\n",
      "Iteration 5950 Training loss 0.0471365861594677 Validation loss 0.04546734318137169 Accuracy 0.539900004863739\n",
      "Iteration 5960 Training loss 0.04402095451951027 Validation loss 0.04575469717383385 Accuracy 0.5375000238418579\n",
      "Iteration 5970 Training loss 0.04381730780005455 Validation loss 0.04516470059752464 Accuracy 0.542900025844574\n",
      "Iteration 5980 Training loss 0.047557149082422256 Validation loss 0.04607541859149933 Accuracy 0.53329998254776\n",
      "Iteration 5990 Training loss 0.042484283447265625 Validation loss 0.04529527202248573 Accuracy 0.5415999889373779\n",
      "Iteration 6000 Training loss 0.0466848760843277 Validation loss 0.04529713839292526 Accuracy 0.5419999957084656\n",
      "Iteration 6010 Training loss 0.04374919459223747 Validation loss 0.04532827064394951 Accuracy 0.541700005531311\n",
      "Iteration 6020 Training loss 0.043197449296712875 Validation loss 0.04529861360788345 Accuracy 0.542900025844574\n",
      "Iteration 6030 Training loss 0.047696553170681 Validation loss 0.04523536190390587 Accuracy 0.5425000190734863\n",
      "Iteration 6040 Training loss 0.04649639502167702 Validation loss 0.04534247890114784 Accuracy 0.5415999889373779\n",
      "Iteration 6050 Training loss 0.04186903312802315 Validation loss 0.045284539461135864 Accuracy 0.5419999957084656\n",
      "Iteration 6060 Training loss 0.04181946441531181 Validation loss 0.04537118971347809 Accuracy 0.5407000184059143\n",
      "Iteration 6070 Training loss 0.04137560725212097 Validation loss 0.04611213877797127 Accuracy 0.5346999764442444\n",
      "Iteration 6080 Training loss 0.044936832040548325 Validation loss 0.04609338194131851 Accuracy 0.5335000157356262\n",
      "Iteration 6090 Training loss 0.044440388679504395 Validation loss 0.0457625687122345 Accuracy 0.5375999808311462\n",
      "Iteration 6100 Training loss 0.04623449593782425 Validation loss 0.04526829719543457 Accuracy 0.5412999987602234\n",
      "Iteration 6110 Training loss 0.04407599940896034 Validation loss 0.04529452323913574 Accuracy 0.5415999889373779\n",
      "Iteration 6120 Training loss 0.04295339435338974 Validation loss 0.045285750180482864 Accuracy 0.5422000288963318\n",
      "Iteration 6130 Training loss 0.04309754818677902 Validation loss 0.045126285403966904 Accuracy 0.5436999797821045\n",
      "Iteration 6140 Training loss 0.04497389867901802 Validation loss 0.045191649347543716 Accuracy 0.5430999994277954\n",
      "Iteration 6150 Training loss 0.043528784066438675 Validation loss 0.04567747935652733 Accuracy 0.5388000011444092\n",
      "Iteration 6160 Training loss 0.04254719614982605 Validation loss 0.045696958899497986 Accuracy 0.5374000072479248\n",
      "Iteration 6170 Training loss 0.04421889781951904 Validation loss 0.045861005783081055 Accuracy 0.5360999703407288\n",
      "Iteration 6180 Training loss 0.043686140328645706 Validation loss 0.0454927496612072 Accuracy 0.5396999716758728\n",
      "Iteration 6190 Training loss 0.04575047269463539 Validation loss 0.045338403433561325 Accuracy 0.5412999987602234\n",
      "Iteration 6200 Training loss 0.04138333722949028 Validation loss 0.0417390875518322 Accuracy 0.574999988079071\n",
      "Iteration 6210 Training loss 0.03936281055212021 Validation loss 0.040256284177303314 Accuracy 0.5922999978065491\n",
      "Iteration 6220 Training loss 0.03609223663806915 Validation loss 0.0390118844807148 Accuracy 0.6039000153541565\n",
      "Iteration 6230 Training loss 0.03789432346820831 Validation loss 0.03957964852452278 Accuracy 0.5967000126838684\n",
      "Iteration 6240 Training loss 0.041900649666786194 Validation loss 0.0387648306787014 Accuracy 0.6061000227928162\n",
      "Iteration 6250 Training loss 0.03628271073102951 Validation loss 0.03831746056675911 Accuracy 0.6101999878883362\n",
      "Iteration 6260 Training loss 0.0375056155025959 Validation loss 0.038338299840688705 Accuracy 0.6104000210762024\n",
      "Iteration 6270 Training loss 0.034877002239227295 Validation loss 0.03854889050126076 Accuracy 0.6082000136375427\n",
      "Iteration 6280 Training loss 0.039781149476766586 Validation loss 0.03860641270875931 Accuracy 0.6075000166893005\n",
      "Iteration 6290 Training loss 0.034579843282699585 Validation loss 0.03799467533826828 Accuracy 0.6141999959945679\n",
      "Iteration 6300 Training loss 0.03750484809279442 Validation loss 0.03885013982653618 Accuracy 0.6061999797821045\n",
      "Iteration 6310 Training loss 0.037944942712783813 Validation loss 0.038472194224596024 Accuracy 0.609499990940094\n",
      "Iteration 6320 Training loss 0.03768099099397659 Validation loss 0.03797557204961777 Accuracy 0.6139000058174133\n",
      "Iteration 6330 Training loss 0.03632369637489319 Validation loss 0.038100868463516235 Accuracy 0.6128000020980835\n",
      "Iteration 6340 Training loss 0.0386507548391819 Validation loss 0.037972718477249146 Accuracy 0.6144000291824341\n",
      "Iteration 6350 Training loss 0.03736235201358795 Validation loss 0.03816702589392662 Accuracy 0.6129999756813049\n",
      "Iteration 6360 Training loss 0.03581296652555466 Validation loss 0.03794652223587036 Accuracy 0.6151000261306763\n",
      "Iteration 6370 Training loss 0.03359013795852661 Validation loss 0.03799152001738548 Accuracy 0.6141999959945679\n",
      "Iteration 6380 Training loss 0.03803149238228798 Validation loss 0.039486899971961975 Accuracy 0.5989000201225281\n",
      "Iteration 6390 Training loss 0.03779472038149834 Validation loss 0.03885616362094879 Accuracy 0.6046000123023987\n",
      "Iteration 6400 Training loss 0.037146538496017456 Validation loss 0.03804556280374527 Accuracy 0.6136000156402588\n",
      "Iteration 6410 Training loss 0.03584780916571617 Validation loss 0.03813893720507622 Accuracy 0.6126000285148621\n",
      "Iteration 6420 Training loss 0.03556743264198303 Validation loss 0.037876565009355545 Accuracy 0.6158000230789185\n",
      "Iteration 6430 Training loss 0.032966602593660355 Validation loss 0.037678565829992294 Accuracy 0.6164000034332275\n",
      "Iteration 6440 Training loss 0.03734958916902542 Validation loss 0.037469539791345596 Accuracy 0.6190999746322632\n",
      "Iteration 6450 Training loss 0.03708414360880852 Validation loss 0.037559833377599716 Accuracy 0.6182000041007996\n",
      "Iteration 6460 Training loss 0.03748561069369316 Validation loss 0.03708755970001221 Accuracy 0.6230999827384949\n",
      "Iteration 6470 Training loss 0.033703532069921494 Validation loss 0.038098204880952835 Accuracy 0.6136000156402588\n",
      "Iteration 6480 Training loss 0.03492060303688049 Validation loss 0.037694212049245834 Accuracy 0.617900013923645\n",
      "Iteration 6490 Training loss 0.033087652176618576 Validation loss 0.03710592910647392 Accuracy 0.6225000023841858\n",
      "Iteration 6500 Training loss 0.03526950627565384 Validation loss 0.038091011345386505 Accuracy 0.6136000156402588\n",
      "Iteration 6510 Training loss 0.03893541172146797 Validation loss 0.03842420503497124 Accuracy 0.6104999780654907\n",
      "Iteration 6520 Training loss 0.03744465857744217 Validation loss 0.03717055171728134 Accuracy 0.6212999820709229\n",
      "Iteration 6530 Training loss 0.03512667864561081 Validation loss 0.037393901497125626 Accuracy 0.6189000010490417\n",
      "Iteration 6540 Training loss 0.029761260375380516 Validation loss 0.03753941133618355 Accuracy 0.6182000041007996\n",
      "Iteration 6550 Training loss 0.03270895779132843 Validation loss 0.03824261203408241 Accuracy 0.6116999983787537\n",
      "Iteration 6560 Training loss 0.037035416811704636 Validation loss 0.0378643274307251 Accuracy 0.6150000095367432\n",
      "Iteration 6570 Training loss 0.035868994891643524 Validation loss 0.03732728585600853 Accuracy 0.6204000115394592\n",
      "Iteration 6580 Training loss 0.035399820655584335 Validation loss 0.03713390231132507 Accuracy 0.6223000288009644\n",
      "Iteration 6590 Training loss 0.033297933638095856 Validation loss 0.03764058277010918 Accuracy 0.616599977016449\n",
      "Iteration 6600 Training loss 0.03872586786746979 Validation loss 0.03677567467093468 Accuracy 0.6251999735832214\n",
      "Iteration 6610 Training loss 0.034971170127391815 Validation loss 0.03734899312257767 Accuracy 0.620199978351593\n",
      "Iteration 6620 Training loss 0.03879651054739952 Validation loss 0.037256717681884766 Accuracy 0.6212999820709229\n",
      "Iteration 6630 Training loss 0.03441505506634712 Validation loss 0.038044918328523636 Accuracy 0.6115000247955322\n",
      "Iteration 6640 Training loss 0.030712418258190155 Validation loss 0.031008301302790642 Accuracy 0.6826000213623047\n",
      "Iteration 6650 Training loss 0.02762564644217491 Validation loss 0.0290566086769104 Accuracy 0.7032999992370605\n",
      "Iteration 6660 Training loss 0.027927517890930176 Validation loss 0.029197346419095993 Accuracy 0.7013999819755554\n",
      "Iteration 6670 Training loss 0.025801951065659523 Validation loss 0.02907497063279152 Accuracy 0.7037000060081482\n",
      "Iteration 6680 Training loss 0.030986933037638664 Validation loss 0.028755083680152893 Accuracy 0.7063999772071838\n",
      "Iteration 6690 Training loss 0.02495456486940384 Validation loss 0.028377827256917953 Accuracy 0.7103999853134155\n",
      "Iteration 6700 Training loss 0.02850833162665367 Validation loss 0.028676873072981834 Accuracy 0.70660001039505\n",
      "Iteration 6710 Training loss 0.026343664154410362 Validation loss 0.028350913897156715 Accuracy 0.7106999754905701\n",
      "Iteration 6720 Training loss 0.024705680087208748 Validation loss 0.02860136702656746 Accuracy 0.7077999711036682\n",
      "Iteration 6730 Training loss 0.026086682453751564 Validation loss 0.028660938143730164 Accuracy 0.7071999907493591\n",
      "Iteration 6740 Training loss 0.027823608368635178 Validation loss 0.02552354894578457 Accuracy 0.7382000088691711\n",
      "Iteration 6750 Training loss 0.018369805067777634 Validation loss 0.02194308675825596 Accuracy 0.7749000191688538\n",
      "Iteration 6760 Training loss 0.01791570335626602 Validation loss 0.020300056785345078 Accuracy 0.7915999889373779\n",
      "Iteration 6770 Training loss 0.016320398077368736 Validation loss 0.021206654608249664 Accuracy 0.7832000255584717\n",
      "Iteration 6780 Training loss 0.019398443400859833 Validation loss 0.020188869908452034 Accuracy 0.7919999957084656\n",
      "Iteration 6790 Training loss 0.019442010670900345 Validation loss 0.01999525912106037 Accuracy 0.7950000166893005\n",
      "Iteration 6800 Training loss 0.020134273916482925 Validation loss 0.019790759310126305 Accuracy 0.7975000143051147\n",
      "Iteration 6810 Training loss 0.018311655148863792 Validation loss 0.02016344852745533 Accuracy 0.7932999730110168\n",
      "Iteration 6820 Training loss 0.019970513880252838 Validation loss 0.02008557878434658 Accuracy 0.7955999970436096\n",
      "Iteration 6830 Training loss 0.019058456644415855 Validation loss 0.019736550748348236 Accuracy 0.798799991607666\n",
      "Iteration 6840 Training loss 0.016570812091231346 Validation loss 0.01931164413690567 Accuracy 0.8021000027656555\n",
      "Iteration 6850 Training loss 0.01824420690536499 Validation loss 0.019398923963308334 Accuracy 0.8011999726295471\n",
      "Iteration 6860 Training loss 0.020000087097287178 Validation loss 0.020816568285226822 Accuracy 0.786899983882904\n",
      "Iteration 6870 Training loss 0.018766913563013077 Validation loss 0.01914536952972412 Accuracy 0.8039000034332275\n",
      "Iteration 6880 Training loss 0.01904531940817833 Validation loss 0.019868627190589905 Accuracy 0.7972999811172485\n",
      "Iteration 6890 Training loss 0.018593233078718185 Validation loss 0.01941865123808384 Accuracy 0.8011000156402588\n",
      "Iteration 6900 Training loss 0.015500420704483986 Validation loss 0.01945573277771473 Accuracy 0.8011999726295471\n",
      "Iteration 6910 Training loss 0.01883900910615921 Validation loss 0.01997421123087406 Accuracy 0.7955999970436096\n",
      "Iteration 6920 Training loss 0.01820121519267559 Validation loss 0.018880026414990425 Accuracy 0.8065999746322632\n",
      "Iteration 6930 Training loss 0.018225396052002907 Validation loss 0.018915371969342232 Accuracy 0.807200014591217\n",
      "Iteration 6940 Training loss 0.0196000337600708 Validation loss 0.019048992544412613 Accuracy 0.8048999905586243\n",
      "Iteration 6950 Training loss 0.017298007383942604 Validation loss 0.019776098430156708 Accuracy 0.7975999712944031\n",
      "Iteration 6960 Training loss 0.01805269904434681 Validation loss 0.019237391650676727 Accuracy 0.8036999702453613\n",
      "Iteration 6970 Training loss 0.016855264082551003 Validation loss 0.01974407024681568 Accuracy 0.7982000112533569\n",
      "Iteration 6980 Training loss 0.01806861162185669 Validation loss 0.018983498215675354 Accuracy 0.8054999709129333\n",
      "Iteration 6990 Training loss 0.015896638855338097 Validation loss 0.019565599039196968 Accuracy 0.8001999855041504\n",
      "Iteration 7000 Training loss 0.017580002546310425 Validation loss 0.0191965214908123 Accuracy 0.8037999868392944\n",
      "Iteration 7010 Training loss 0.01670735701918602 Validation loss 0.018774153664708138 Accuracy 0.8087999820709229\n",
      "Iteration 7020 Training loss 0.015072759240865707 Validation loss 0.018647316843271255 Accuracy 0.8095999956130981\n",
      "Iteration 7030 Training loss 0.014962870627641678 Validation loss 0.019116073846817017 Accuracy 0.8051000237464905\n",
      "Iteration 7040 Training loss 0.016275009140372276 Validation loss 0.018498430028557777 Accuracy 0.8102999925613403\n",
      "Iteration 7050 Training loss 0.016361264511942863 Validation loss 0.01944371871650219 Accuracy 0.8011000156402588\n",
      "Iteration 7060 Training loss 0.016269341111183167 Validation loss 0.01910783350467682 Accuracy 0.8047999739646912\n",
      "Iteration 7070 Training loss 0.01649634726345539 Validation loss 0.019636401906609535 Accuracy 0.798799991607666\n",
      "Iteration 7080 Training loss 0.023159228265285492 Validation loss 0.019527200609445572 Accuracy 0.7998999953269958\n",
      "Iteration 7090 Training loss 0.020237833261489868 Validation loss 0.018745778128504753 Accuracy 0.8076000213623047\n",
      "Iteration 7100 Training loss 0.018030542880296707 Validation loss 0.018452700227499008 Accuracy 0.8116000294685364\n",
      "Iteration 7110 Training loss 0.021385598927736282 Validation loss 0.018890725448727608 Accuracy 0.8068000078201294\n",
      "Iteration 7120 Training loss 0.014380506239831448 Validation loss 0.01857946254312992 Accuracy 0.8101999759674072\n",
      "Iteration 7130 Training loss 0.016235169023275375 Validation loss 0.019010284915566444 Accuracy 0.8065000176429749\n",
      "Iteration 7140 Training loss 0.0182461179792881 Validation loss 0.019816696643829346 Accuracy 0.7972999811172485\n",
      "Iteration 7150 Training loss 0.020302999764680862 Validation loss 0.018934084102511406 Accuracy 0.805899977684021\n",
      "Iteration 7160 Training loss 0.019612926989793777 Validation loss 0.019643910229206085 Accuracy 0.7982000112533569\n",
      "Iteration 7170 Training loss 0.02194703370332718 Validation loss 0.01864282786846161 Accuracy 0.808899998664856\n",
      "Iteration 7180 Training loss 0.017562532797455788 Validation loss 0.018944082781672478 Accuracy 0.8066999912261963\n",
      "Iteration 7190 Training loss 0.017082417383790016 Validation loss 0.01906917430460453 Accuracy 0.8057000041007996\n",
      "Iteration 7200 Training loss 0.016820257529616356 Validation loss 0.018653061240911484 Accuracy 0.809499979019165\n",
      "Iteration 7210 Training loss 0.017247460782527924 Validation loss 0.018524933606386185 Accuracy 0.8105000257492065\n",
      "Iteration 7220 Training loss 0.018814479932188988 Validation loss 0.018516337499022484 Accuracy 0.8108999729156494\n",
      "Iteration 7230 Training loss 0.018329650163650513 Validation loss 0.019192509353160858 Accuracy 0.8043000102043152\n",
      "Iteration 7240 Training loss 0.017439059913158417 Validation loss 0.0185987651348114 Accuracy 0.8105999827384949\n",
      "Iteration 7250 Training loss 0.017986144870519638 Validation loss 0.01845061220228672 Accuracy 0.8116999864578247\n",
      "Iteration 7260 Training loss 0.01932886429131031 Validation loss 0.018555762246251106 Accuracy 0.8109999895095825\n",
      "Iteration 7270 Training loss 0.017618633806705475 Validation loss 0.01882695034146309 Accuracy 0.8073999881744385\n",
      "Iteration 7280 Training loss 0.014179212972521782 Validation loss 0.018581805750727654 Accuracy 0.809499979019165\n",
      "Iteration 7290 Training loss 0.018014511093497276 Validation loss 0.018569311127066612 Accuracy 0.8105000257492065\n",
      "Iteration 7300 Training loss 0.018594855442643166 Validation loss 0.018347665667533875 Accuracy 0.8123999834060669\n",
      "Iteration 7310 Training loss 0.01777130551636219 Validation loss 0.01935630477964878 Accuracy 0.8025000095367432\n",
      "Iteration 7320 Training loss 0.017485104501247406 Validation loss 0.018419316038489342 Accuracy 0.8119000196456909\n",
      "Iteration 7330 Training loss 0.017957083880901337 Validation loss 0.018768077716231346 Accuracy 0.8082000017166138\n",
      "Iteration 7340 Training loss 0.014832503162324429 Validation loss 0.01854727976024151 Accuracy 0.8105999827384949\n",
      "Iteration 7350 Training loss 0.01615707203745842 Validation loss 0.018550192937254906 Accuracy 0.8101999759674072\n",
      "Iteration 7360 Training loss 0.016929075121879578 Validation loss 0.01835591532289982 Accuracy 0.8122000098228455\n",
      "Iteration 7370 Training loss 0.014901638962328434 Validation loss 0.018564671277999878 Accuracy 0.8111000061035156\n",
      "Iteration 7380 Training loss 0.016125569120049477 Validation loss 0.01825978234410286 Accuracy 0.8136000037193298\n",
      "Iteration 7390 Training loss 0.01637938618659973 Validation loss 0.01953325979411602 Accuracy 0.7998999953269958\n",
      "Iteration 7400 Training loss 0.01608794741332531 Validation loss 0.01825576275587082 Accuracy 0.8140000104904175\n",
      "Iteration 7410 Training loss 0.016095789149403572 Validation loss 0.018633000552654266 Accuracy 0.809499979019165\n",
      "Iteration 7420 Training loss 0.015708304941654205 Validation loss 0.018644843250513077 Accuracy 0.8100000023841858\n",
      "Iteration 7430 Training loss 0.015819383785128593 Validation loss 0.01834813319146633 Accuracy 0.8119999766349792\n",
      "Iteration 7440 Training loss 0.0164786446839571 Validation loss 0.018366849049925804 Accuracy 0.8127999901771545\n",
      "Iteration 7450 Training loss 0.01693863980472088 Validation loss 0.018336649984121323 Accuracy 0.8116000294685364\n",
      "Iteration 7460 Training loss 0.018223058432340622 Validation loss 0.018305091187357903 Accuracy 0.8126999735832214\n",
      "Iteration 7470 Training loss 0.014955058693885803 Validation loss 0.018373336642980576 Accuracy 0.8115000128746033\n",
      "Iteration 7480 Training loss 0.017337892204523087 Validation loss 0.0190049447119236 Accuracy 0.8051999807357788\n",
      "Iteration 7490 Training loss 0.016858944669365883 Validation loss 0.01862337440252304 Accuracy 0.8095999956130981\n",
      "Iteration 7500 Training loss 0.016229983419179916 Validation loss 0.018263809382915497 Accuracy 0.8126000165939331\n",
      "Iteration 7510 Training loss 0.01637854613363743 Validation loss 0.018041355535387993 Accuracy 0.8159000277519226\n",
      "Iteration 7520 Training loss 0.016449902206659317 Validation loss 0.018234577029943466 Accuracy 0.8130000233650208\n",
      "Iteration 7530 Training loss 0.015704350546002388 Validation loss 0.01856217347085476 Accuracy 0.8098999857902527\n",
      "Iteration 7540 Training loss 0.018169313669204712 Validation loss 0.018323520198464394 Accuracy 0.8131999969482422\n",
      "Iteration 7550 Training loss 0.0181288979947567 Validation loss 0.019397005438804626 Accuracy 0.8015000224113464\n",
      "Iteration 7560 Training loss 0.017815006896853447 Validation loss 0.01844068616628647 Accuracy 0.8123000264167786\n",
      "Iteration 7570 Training loss 0.016879180446267128 Validation loss 0.018586507067084312 Accuracy 0.8101999759674072\n",
      "Iteration 7580 Training loss 0.017559129744768143 Validation loss 0.019662635400891304 Accuracy 0.7993999719619751\n",
      "Iteration 7590 Training loss 0.01625322550535202 Validation loss 0.018803494051098824 Accuracy 0.8069000244140625\n",
      "Iteration 7600 Training loss 0.01581965759396553 Validation loss 0.018497169017791748 Accuracy 0.8111000061035156\n",
      "Iteration 7610 Training loss 0.019450252875685692 Validation loss 0.01874789409339428 Accuracy 0.8082000017166138\n",
      "Iteration 7620 Training loss 0.017172489315271378 Validation loss 0.019010823220014572 Accuracy 0.8054999709129333\n",
      "Iteration 7630 Training loss 0.017124900594353676 Validation loss 0.018503911793231964 Accuracy 0.8101000189781189\n",
      "Iteration 7640 Training loss 0.016130654141306877 Validation loss 0.018131162971258163 Accuracy 0.8149999976158142\n",
      "Iteration 7650 Training loss 0.02042272500693798 Validation loss 0.01835632137954235 Accuracy 0.8112999796867371\n",
      "Iteration 7660 Training loss 0.013505632989108562 Validation loss 0.018075013533234596 Accuracy 0.8154000043869019\n",
      "Iteration 7670 Training loss 0.01638224348425865 Validation loss 0.018224116414785385 Accuracy 0.8141000270843506\n",
      "Iteration 7680 Training loss 0.018729614093899727 Validation loss 0.019421890377998352 Accuracy 0.8012999892234802\n",
      "Iteration 7690 Training loss 0.018323849886655807 Validation loss 0.01878882758319378 Accuracy 0.8080000281333923\n",
      "Iteration 7700 Training loss 0.013757748529314995 Validation loss 0.01841307058930397 Accuracy 0.8112000226974487\n",
      "Iteration 7710 Training loss 0.01758243516087532 Validation loss 0.018738890066742897 Accuracy 0.8080000281333923\n",
      "Iteration 7720 Training loss 0.015533402562141418 Validation loss 0.01797107420861721 Accuracy 0.8162000179290771\n",
      "Iteration 7730 Training loss 0.015153463929891586 Validation loss 0.018648048862814903 Accuracy 0.8091999888420105\n",
      "Iteration 7740 Training loss 0.01574036478996277 Validation loss 0.019273927435278893 Accuracy 0.8023999929428101\n",
      "Iteration 7750 Training loss 0.015302425250411034 Validation loss 0.018280671909451485 Accuracy 0.8133999705314636\n",
      "Iteration 7760 Training loss 0.017353059723973274 Validation loss 0.018985973671078682 Accuracy 0.8064000010490417\n",
      "Iteration 7770 Training loss 0.016567492857575417 Validation loss 0.018425356596708298 Accuracy 0.8112000226974487\n",
      "Iteration 7780 Training loss 0.01506426278501749 Validation loss 0.018147742375731468 Accuracy 0.8145999908447266\n",
      "Iteration 7790 Training loss 0.015308814123272896 Validation loss 0.01810372993350029 Accuracy 0.8141999840736389\n",
      "Iteration 7800 Training loss 0.015817349776625633 Validation loss 0.0186014361679554 Accuracy 0.809499979019165\n",
      "Iteration 7810 Training loss 0.01906469650566578 Validation loss 0.018322821706533432 Accuracy 0.8126000165939331\n",
      "Iteration 7820 Training loss 0.01522278692573309 Validation loss 0.018397338688373566 Accuracy 0.8109999895095825\n",
      "Iteration 7830 Training loss 0.01533754076808691 Validation loss 0.018382053822278976 Accuracy 0.8111000061035156\n",
      "Iteration 7840 Training loss 0.015818852931261063 Validation loss 0.018497856333851814 Accuracy 0.810699999332428\n",
      "Iteration 7850 Training loss 0.016800034791231155 Validation loss 0.018559541553258896 Accuracy 0.8101000189781189\n",
      "Iteration 7860 Training loss 0.017987696453928947 Validation loss 0.01845366880297661 Accuracy 0.810699999332428\n",
      "Iteration 7870 Training loss 0.01664678007364273 Validation loss 0.018107984215021133 Accuracy 0.8151000142097473\n",
      "Iteration 7880 Training loss 0.014900759793817997 Validation loss 0.01802436076104641 Accuracy 0.815500020980835\n",
      "Iteration 7890 Training loss 0.014466112479567528 Validation loss 0.01807464472949505 Accuracy 0.8148999810218811\n",
      "Iteration 7900 Training loss 0.015170490369200706 Validation loss 0.019179638475179672 Accuracy 0.8030999898910522\n",
      "Iteration 7910 Training loss 0.01672937348484993 Validation loss 0.017994331195950508 Accuracy 0.8154000043869019\n",
      "Iteration 7920 Training loss 0.017367152497172356 Validation loss 0.019175875931978226 Accuracy 0.8034999966621399\n",
      "Iteration 7930 Training loss 0.017258629202842712 Validation loss 0.018609678372740746 Accuracy 0.8090999722480774\n",
      "Iteration 7940 Training loss 0.016267672181129456 Validation loss 0.01805426925420761 Accuracy 0.8138999938964844\n",
      "Iteration 7950 Training loss 0.017137521877884865 Validation loss 0.01910961978137493 Accuracy 0.8044000267982483\n",
      "Iteration 7960 Training loss 0.01801295205950737 Validation loss 0.019124776124954224 Accuracy 0.8047999739646912\n",
      "Iteration 7970 Training loss 0.017074016854166985 Validation loss 0.018376782536506653 Accuracy 0.8123000264167786\n",
      "Iteration 7980 Training loss 0.0174273531883955 Validation loss 0.018654990941286087 Accuracy 0.8105000257492065\n",
      "Iteration 7990 Training loss 0.01637943461537361 Validation loss 0.018427295610308647 Accuracy 0.8118000030517578\n",
      "Iteration 8000 Training loss 0.01817825809121132 Validation loss 0.019250623881816864 Accuracy 0.8030999898910522\n",
      "Iteration 8010 Training loss 0.015162070281803608 Validation loss 0.018107544630765915 Accuracy 0.8151999711990356\n",
      "Iteration 8020 Training loss 0.017917053773999214 Validation loss 0.018041515722870827 Accuracy 0.8152999877929688\n",
      "Iteration 8030 Training loss 0.015016406774520874 Validation loss 0.018017850816249847 Accuracy 0.8152999877929688\n",
      "Iteration 8040 Training loss 0.016798362135887146 Validation loss 0.017990076914429665 Accuracy 0.8162000179290771\n",
      "Iteration 8050 Training loss 0.01551125105470419 Validation loss 0.01866813935339451 Accuracy 0.8098000288009644\n",
      "Iteration 8060 Training loss 0.016880959272384644 Validation loss 0.018384087830781937 Accuracy 0.8119000196456909\n",
      "Iteration 8070 Training loss 0.016075845807790756 Validation loss 0.01991315372288227 Accuracy 0.796500027179718\n",
      "Iteration 8080 Training loss 0.018995877355337143 Validation loss 0.018170462921261787 Accuracy 0.8141999840736389\n",
      "Iteration 8090 Training loss 0.01622416079044342 Validation loss 0.01806614175438881 Accuracy 0.8149999976158142\n",
      "Iteration 8100 Training loss 0.016397546976804733 Validation loss 0.018067004159092903 Accuracy 0.8152999877929688\n",
      "Iteration 8110 Training loss 0.019313829019665718 Validation loss 0.01910504139959812 Accuracy 0.8046000003814697\n",
      "Iteration 8120 Training loss 0.016855422407388687 Validation loss 0.01827552169561386 Accuracy 0.8131999969482422\n",
      "Iteration 8130 Training loss 0.017205487936735153 Validation loss 0.018245600163936615 Accuracy 0.8126999735832214\n",
      "Iteration 8140 Training loss 0.013992696069180965 Validation loss 0.01846766658127308 Accuracy 0.8108000159263611\n",
      "Iteration 8150 Training loss 0.013824010267853737 Validation loss 0.018292153254151344 Accuracy 0.8126999735832214\n",
      "Iteration 8160 Training loss 0.016023896634578705 Validation loss 0.017857659608125687 Accuracy 0.8176000118255615\n",
      "Iteration 8170 Training loss 0.014611471444368362 Validation loss 0.018077561631798744 Accuracy 0.8144999742507935\n",
      "Iteration 8180 Training loss 0.016116028651595116 Validation loss 0.01836586371064186 Accuracy 0.8116000294685364\n",
      "Iteration 8190 Training loss 0.014139874838292599 Validation loss 0.01805487461388111 Accuracy 0.8151000142097473\n",
      "Iteration 8200 Training loss 0.017400003969669342 Validation loss 0.017967958003282547 Accuracy 0.8166000247001648\n",
      "Iteration 8210 Training loss 0.017238078638911247 Validation loss 0.018059253692626953 Accuracy 0.8152999877929688\n",
      "Iteration 8220 Training loss 0.01817639172077179 Validation loss 0.017879409715533257 Accuracy 0.8170999884605408\n",
      "Iteration 8230 Training loss 0.015221831388771534 Validation loss 0.01807307079434395 Accuracy 0.8155999779701233\n",
      "Iteration 8240 Training loss 0.016787797212600708 Validation loss 0.01846775785088539 Accuracy 0.8112000226974487\n",
      "Iteration 8250 Training loss 0.01724635250866413 Validation loss 0.017881371080875397 Accuracy 0.8165000081062317\n",
      "Iteration 8260 Training loss 0.017740679904818535 Validation loss 0.018194522708654404 Accuracy 0.8141000270843506\n",
      "Iteration 8270 Training loss 0.01680327206850052 Validation loss 0.01802123710513115 Accuracy 0.816100001335144\n",
      "Iteration 8280 Training loss 0.014230536296963692 Validation loss 0.017857616767287254 Accuracy 0.8179000020027161\n",
      "Iteration 8290 Training loss 0.01188082154840231 Validation loss 0.01798839122056961 Accuracy 0.8158000111579895\n",
      "Iteration 8300 Training loss 0.017427699640393257 Validation loss 0.01835382729768753 Accuracy 0.8116000294685364\n",
      "Iteration 8310 Training loss 0.018478287383913994 Validation loss 0.018822522833943367 Accuracy 0.8080999851226807\n",
      "Iteration 8320 Training loss 0.01649652048945427 Validation loss 0.01813463121652603 Accuracy 0.8136000037193298\n",
      "Iteration 8330 Training loss 0.015212364494800568 Validation loss 0.018021926283836365 Accuracy 0.8158000111579895\n",
      "Iteration 8340 Training loss 0.016308702528476715 Validation loss 0.018224302679300308 Accuracy 0.8137999773025513\n",
      "Iteration 8350 Training loss 0.01771199144423008 Validation loss 0.01744047924876213 Accuracy 0.8216999769210815\n",
      "Iteration 8360 Training loss 0.015210019424557686 Validation loss 0.018637675791978836 Accuracy 0.810699999332428\n",
      "Iteration 8370 Training loss 0.01624361425638199 Validation loss 0.01854327693581581 Accuracy 0.8101000189781189\n",
      "Iteration 8380 Training loss 0.01566726341843605 Validation loss 0.01850808411836624 Accuracy 0.8116999864578247\n",
      "Iteration 8390 Training loss 0.014230811037123203 Validation loss 0.01754545234143734 Accuracy 0.8205000162124634\n",
      "Iteration 8400 Training loss 0.017709258943796158 Validation loss 0.01838967576622963 Accuracy 0.8108000159263611\n",
      "Iteration 8410 Training loss 0.016376690939068794 Validation loss 0.017826832830905914 Accuracy 0.817799985408783\n",
      "Iteration 8420 Training loss 0.015814486891031265 Validation loss 0.017721137031912804 Accuracy 0.8190000057220459\n",
      "Iteration 8430 Training loss 0.017902858555316925 Validation loss 0.018019849434494972 Accuracy 0.8154000043869019\n",
      "Iteration 8440 Training loss 0.015261078253388405 Validation loss 0.018220096826553345 Accuracy 0.8140000104904175\n",
      "Iteration 8450 Training loss 0.015557307749986649 Validation loss 0.01752658188343048 Accuracy 0.821399986743927\n",
      "Iteration 8460 Training loss 0.01489297579973936 Validation loss 0.017721645534038544 Accuracy 0.8195000290870667\n",
      "Iteration 8470 Training loss 0.01847299002110958 Validation loss 0.017742741852998734 Accuracy 0.8187000155448914\n",
      "Iteration 8480 Training loss 0.016586828976869583 Validation loss 0.01788092404603958 Accuracy 0.8172000050544739\n",
      "Iteration 8490 Training loss 0.016077401116490364 Validation loss 0.0176315288990736 Accuracy 0.819100022315979\n",
      "Iteration 8500 Training loss 0.014374065212905407 Validation loss 0.017676934599876404 Accuracy 0.819100022315979\n",
      "Iteration 8510 Training loss 0.01608315482735634 Validation loss 0.01764259859919548 Accuracy 0.819599986076355\n",
      "Iteration 8520 Training loss 0.01717217080295086 Validation loss 0.018315058201551437 Accuracy 0.8122000098228455\n",
      "Iteration 8530 Training loss 0.013608458451926708 Validation loss 0.017546214163303375 Accuracy 0.8206999897956848\n",
      "Iteration 8540 Training loss 0.01534512359648943 Validation loss 0.01777632161974907 Accuracy 0.8183000087738037\n",
      "Iteration 8550 Training loss 0.016570191830396652 Validation loss 0.018013007938861847 Accuracy 0.8159000277519226\n",
      "Iteration 8560 Training loss 0.016595302149653435 Validation loss 0.01743847317993641 Accuracy 0.8213000297546387\n",
      "Iteration 8570 Training loss 0.015668541193008423 Validation loss 0.018824024125933647 Accuracy 0.8075000047683716\n",
      "Iteration 8580 Training loss 0.014494710601866245 Validation loss 0.017795002087950706 Accuracy 0.8180000185966492\n",
      "Iteration 8590 Training loss 0.016573771834373474 Validation loss 0.017580831423401833 Accuracy 0.8198999762535095\n",
      "Iteration 8600 Training loss 0.016967549920082092 Validation loss 0.01819651760160923 Accuracy 0.8145999908447266\n",
      "Iteration 8610 Training loss 0.015275430865585804 Validation loss 0.017611220479011536 Accuracy 0.8202000260353088\n",
      "Iteration 8620 Training loss 0.013051524758338928 Validation loss 0.018010834231972694 Accuracy 0.8159000277519226\n",
      "Iteration 8630 Training loss 0.018485605716705322 Validation loss 0.01762683130800724 Accuracy 0.8192999958992004\n",
      "Iteration 8640 Training loss 0.018022364005446434 Validation loss 0.019096102565526962 Accuracy 0.8047000169754028\n",
      "Iteration 8650 Training loss 0.01625187136232853 Validation loss 0.018146734684705734 Accuracy 0.814300000667572\n",
      "Iteration 8660 Training loss 0.01818251609802246 Validation loss 0.018103566020727158 Accuracy 0.815500020980835\n",
      "Iteration 8670 Training loss 0.0140247056260705 Validation loss 0.017736397683620453 Accuracy 0.8185999989509583\n",
      "Iteration 8680 Training loss 0.01731056720018387 Validation loss 0.017734678462147713 Accuracy 0.8184000253677368\n",
      "Iteration 8690 Training loss 0.01521612424403429 Validation loss 0.018308302387595177 Accuracy 0.8127999901771545\n",
      "Iteration 8700 Training loss 0.014105870388448238 Validation loss 0.017764603719115257 Accuracy 0.817799985408783\n",
      "Iteration 8710 Training loss 0.015618843026459217 Validation loss 0.01784573309123516 Accuracy 0.8169000148773193\n",
      "Iteration 8720 Training loss 0.016659878194332123 Validation loss 0.017597917467355728 Accuracy 0.8195000290870667\n",
      "Iteration 8730 Training loss 0.01757912151515484 Validation loss 0.01744735799729824 Accuracy 0.8212000131607056\n",
      "Iteration 8740 Training loss 0.015815632417798042 Validation loss 0.01858443208038807 Accuracy 0.808899998664856\n",
      "Iteration 8750 Training loss 0.01433545257896185 Validation loss 0.01807251013815403 Accuracy 0.815500020980835\n",
      "Iteration 8760 Training loss 0.013611330650746822 Validation loss 0.0184186939150095 Accuracy 0.8122000098228455\n",
      "Iteration 8770 Training loss 0.014980114065110683 Validation loss 0.017878033220767975 Accuracy 0.8169000148773193\n",
      "Iteration 8780 Training loss 0.013943112455308437 Validation loss 0.017673460766673088 Accuracy 0.8190000057220459\n",
      "Iteration 8790 Training loss 0.014908784069120884 Validation loss 0.01810963824391365 Accuracy 0.8141000270843506\n",
      "Iteration 8800 Training loss 0.016421707347035408 Validation loss 0.017751866951584816 Accuracy 0.817799985408783\n",
      "Iteration 8810 Training loss 0.015173632651567459 Validation loss 0.01771947182714939 Accuracy 0.8188999891281128\n",
      "Iteration 8820 Training loss 0.01589128188788891 Validation loss 0.018306726589798927 Accuracy 0.8123000264167786\n",
      "Iteration 8830 Training loss 0.017819436267018318 Validation loss 0.018160667270421982 Accuracy 0.8148000240325928\n",
      "Iteration 8840 Training loss 0.016976872459053993 Validation loss 0.01789214462041855 Accuracy 0.8162000179290771\n",
      "Iteration 8850 Training loss 0.01553831435739994 Validation loss 0.017933448776602745 Accuracy 0.8158000111579895\n",
      "Iteration 8860 Training loss 0.012561981566250324 Validation loss 0.017721781507134438 Accuracy 0.8183000087738037\n",
      "Iteration 8870 Training loss 0.014647497795522213 Validation loss 0.018002824857831 Accuracy 0.8155999779701233\n",
      "Iteration 8880 Training loss 0.014963414520025253 Validation loss 0.01783742755651474 Accuracy 0.8176000118255615\n",
      "Iteration 8890 Training loss 0.013732888735830784 Validation loss 0.018078457564115524 Accuracy 0.8144999742507935\n",
      "Iteration 8900 Training loss 0.017069082707166672 Validation loss 0.0192070584744215 Accuracy 0.804099977016449\n",
      "Iteration 8910 Training loss 0.014988362789154053 Validation loss 0.01771995797753334 Accuracy 0.8184000253677368\n",
      "Iteration 8920 Training loss 0.013660567812621593 Validation loss 0.01764955371618271 Accuracy 0.8201000094413757\n",
      "Iteration 8930 Training loss 0.014870546758174896 Validation loss 0.017903277650475502 Accuracy 0.8169000148773193\n",
      "Iteration 8940 Training loss 0.019469892606139183 Validation loss 0.017858076840639114 Accuracy 0.8176000118255615\n",
      "Iteration 8950 Training loss 0.015702813863754272 Validation loss 0.017990585416555405 Accuracy 0.8162999749183655\n",
      "Iteration 8960 Training loss 0.015541790053248405 Validation loss 0.018364178016781807 Accuracy 0.8122000098228455\n",
      "Iteration 8970 Training loss 0.015161403454840183 Validation loss 0.01791316829621792 Accuracy 0.8162999749183655\n",
      "Iteration 8980 Training loss 0.016016805544495583 Validation loss 0.017710478976368904 Accuracy 0.8183000087738037\n",
      "Iteration 8990 Training loss 0.015911683440208435 Validation loss 0.01740487478673458 Accuracy 0.8215000033378601\n",
      "Iteration 9000 Training loss 0.0138280363753438 Validation loss 0.017488578334450722 Accuracy 0.8205999732017517\n",
      "Iteration 9010 Training loss 0.015430509112775326 Validation loss 0.017583392560482025 Accuracy 0.8198999762535095\n",
      "Iteration 9020 Training loss 0.013529473915696144 Validation loss 0.017740044742822647 Accuracy 0.819100022315979\n",
      "Iteration 9030 Training loss 0.01407918892800808 Validation loss 0.01774514652788639 Accuracy 0.8187000155448914\n",
      "Iteration 9040 Training loss 0.013245731592178345 Validation loss 0.017548512667417526 Accuracy 0.8205999732017517\n",
      "Iteration 9050 Training loss 0.016106780618429184 Validation loss 0.01753091625869274 Accuracy 0.8192999958992004\n",
      "Iteration 9060 Training loss 0.016515575349330902 Validation loss 0.017522508278489113 Accuracy 0.8206999897956848\n",
      "Iteration 9070 Training loss 0.014996490441262722 Validation loss 0.018017109483480453 Accuracy 0.8159999847412109\n",
      "Iteration 9080 Training loss 0.014993837103247643 Validation loss 0.017729222774505615 Accuracy 0.8181999921798706\n",
      "Iteration 9090 Training loss 0.014006642624735832 Validation loss 0.017384013161063194 Accuracy 0.8220000267028809\n",
      "Iteration 9100 Training loss 0.01672322116792202 Validation loss 0.017340784892439842 Accuracy 0.8222000002861023\n",
      "Iteration 9110 Training loss 0.016185201704502106 Validation loss 0.01804250478744507 Accuracy 0.8152999877929688\n",
      "Iteration 9120 Training loss 0.017496751621365547 Validation loss 0.017785726115107536 Accuracy 0.8181999921798706\n",
      "Iteration 9130 Training loss 0.01700395531952381 Validation loss 0.017503850162029266 Accuracy 0.820900022983551\n",
      "Iteration 9140 Training loss 0.012294397689402103 Validation loss 0.01745353825390339 Accuracy 0.8208000063896179\n",
      "Iteration 9150 Training loss 0.016713593155145645 Validation loss 0.017992498353123665 Accuracy 0.8151000142097473\n",
      "Iteration 9160 Training loss 0.018655166029930115 Validation loss 0.017741644755005836 Accuracy 0.8183000087738037\n",
      "Iteration 9170 Training loss 0.01772543042898178 Validation loss 0.01779814064502716 Accuracy 0.8177000284194946\n",
      "Iteration 9180 Training loss 0.01639423333108425 Validation loss 0.017361482605338097 Accuracy 0.821399986743927\n",
      "Iteration 9190 Training loss 0.0135006383061409 Validation loss 0.017696691676974297 Accuracy 0.8180000185966492\n",
      "Iteration 9200 Training loss 0.016076749190688133 Validation loss 0.01765744574368 Accuracy 0.8190000057220459\n",
      "Iteration 9210 Training loss 0.013458308763802052 Validation loss 0.017656110227108 Accuracy 0.8187000155448914\n",
      "Iteration 9220 Training loss 0.013953118585050106 Validation loss 0.0186949260532856 Accuracy 0.8090999722480774\n",
      "Iteration 9230 Training loss 0.016400881111621857 Validation loss 0.01819632202386856 Accuracy 0.8141999840736389\n",
      "Iteration 9240 Training loss 0.012902948074042797 Validation loss 0.017624035477638245 Accuracy 0.8201000094413757\n",
      "Iteration 9250 Training loss 0.016587642952799797 Validation loss 0.018309958279132843 Accuracy 0.8126000165939331\n",
      "Iteration 9260 Training loss 0.014941107481718063 Validation loss 0.0174785777926445 Accuracy 0.8203999996185303\n",
      "Iteration 9270 Training loss 0.01650070771574974 Validation loss 0.01776188239455223 Accuracy 0.8180999755859375\n",
      "Iteration 9280 Training loss 0.013731783255934715 Validation loss 0.01791009120643139 Accuracy 0.8166999816894531\n",
      "Iteration 9290 Training loss 0.013510029762983322 Validation loss 0.017438817769289017 Accuracy 0.8209999799728394\n",
      "Iteration 9300 Training loss 0.01555335707962513 Validation loss 0.017284072935581207 Accuracy 0.8230000138282776\n",
      "Iteration 9310 Training loss 0.015174130909144878 Validation loss 0.017318295314908028 Accuracy 0.8227999806404114\n",
      "Iteration 9320 Training loss 0.01637098379433155 Validation loss 0.017858030274510384 Accuracy 0.8173999786376953\n",
      "Iteration 9330 Training loss 0.016453517600893974 Validation loss 0.01792571134865284 Accuracy 0.817300021648407\n",
      "Iteration 9340 Training loss 0.014438897371292114 Validation loss 0.017435777932405472 Accuracy 0.8222000002861023\n",
      "Iteration 9350 Training loss 0.015018119476735592 Validation loss 0.01825612783432007 Accuracy 0.8129000067710876\n",
      "Iteration 9360 Training loss 0.016936207190155983 Validation loss 0.018103159964084625 Accuracy 0.8148999810218811\n",
      "Iteration 9370 Training loss 0.01589248701930046 Validation loss 0.017420116811990738 Accuracy 0.8222000002861023\n",
      "Iteration 9380 Training loss 0.017833607271313667 Validation loss 0.017964856699109077 Accuracy 0.815500020980835\n",
      "Iteration 9390 Training loss 0.015926094725728035 Validation loss 0.018274057656526566 Accuracy 0.8133999705314636\n",
      "Iteration 9400 Training loss 0.016765324398875237 Validation loss 0.018222084268927574 Accuracy 0.8130999803543091\n",
      "Iteration 9410 Training loss 0.014923855662345886 Validation loss 0.017621519044041634 Accuracy 0.8201000094413757\n",
      "Iteration 9420 Training loss 0.013369069434702396 Validation loss 0.0176271740347147 Accuracy 0.8195000290870667\n",
      "Iteration 9430 Training loss 0.015299835242331028 Validation loss 0.01783287711441517 Accuracy 0.8174999952316284\n",
      "Iteration 9440 Training loss 0.01379435695707798 Validation loss 0.017680805176496506 Accuracy 0.8185999989509583\n",
      "Iteration 9450 Training loss 0.014914752915501595 Validation loss 0.0174433421343565 Accuracy 0.8212000131607056\n",
      "Iteration 9460 Training loss 0.014828132465481758 Validation loss 0.01751210168004036 Accuracy 0.8199999928474426\n",
      "Iteration 9470 Training loss 0.014794196002185345 Validation loss 0.01758354716002941 Accuracy 0.8187999725341797\n",
      "Iteration 9480 Training loss 0.014596093446016312 Validation loss 0.017539996653795242 Accuracy 0.8202000260353088\n",
      "Iteration 9490 Training loss 0.017599591985344887 Validation loss 0.018268156796693802 Accuracy 0.8127999901771545\n",
      "Iteration 9500 Training loss 0.014825981110334396 Validation loss 0.018779398873448372 Accuracy 0.8076000213623047\n",
      "Iteration 9510 Training loss 0.013758119195699692 Validation loss 0.017509333789348602 Accuracy 0.8205999732017517\n",
      "Iteration 9520 Training loss 0.013689803890883923 Validation loss 0.01772264763712883 Accuracy 0.8190000057220459\n",
      "Iteration 9530 Training loss 0.015711141750216484 Validation loss 0.018904775381088257 Accuracy 0.8064000010490417\n",
      "Iteration 9540 Training loss 0.015113599598407745 Validation loss 0.01855326257646084 Accuracy 0.8105999827384949\n",
      "Iteration 9550 Training loss 0.016897646710276604 Validation loss 0.01782562956213951 Accuracy 0.8176000118255615\n",
      "Iteration 9560 Training loss 0.01312668900936842 Validation loss 0.017589418217539787 Accuracy 0.8198999762535095\n",
      "Iteration 9570 Training loss 0.013169145211577415 Validation loss 0.01739531382918358 Accuracy 0.8215000033378601\n",
      "Iteration 9580 Training loss 0.018665911629796028 Validation loss 0.018469257280230522 Accuracy 0.8109999895095825\n",
      "Iteration 9590 Training loss 0.014896577224135399 Validation loss 0.017299043014645576 Accuracy 0.8228999972343445\n",
      "Iteration 9600 Training loss 0.01446120347827673 Validation loss 0.01742207072675228 Accuracy 0.8215000033378601\n",
      "Iteration 9610 Training loss 0.014745518565177917 Validation loss 0.018311019986867905 Accuracy 0.8123999834060669\n",
      "Iteration 9620 Training loss 0.01660262979567051 Validation loss 0.017247701063752174 Accuracy 0.8228999972343445\n",
      "Iteration 9630 Training loss 0.019507912918925285 Validation loss 0.017320357263088226 Accuracy 0.8219000101089478\n",
      "Iteration 9640 Training loss 0.01715427078306675 Validation loss 0.01694994606077671 Accuracy 0.8266000151634216\n",
      "Iteration 9650 Training loss 0.016608282923698425 Validation loss 0.017101502045989037 Accuracy 0.8251000046730042\n",
      "Iteration 9660 Training loss 0.013897405937314034 Validation loss 0.018220990896224976 Accuracy 0.8130000233650208\n",
      "Iteration 9670 Training loss 0.016456464305520058 Validation loss 0.01747787371277809 Accuracy 0.8216000199317932\n",
      "Iteration 9680 Training loss 0.01686353236436844 Validation loss 0.017998816445469856 Accuracy 0.8166000247001648\n",
      "Iteration 9690 Training loss 0.014043992385268211 Validation loss 0.01743120141327381 Accuracy 0.8216000199317932\n",
      "Iteration 9700 Training loss 0.013430219143629074 Validation loss 0.01752275601029396 Accuracy 0.8199999928474426\n",
      "Iteration 9710 Training loss 0.012033341452479362 Validation loss 0.017199795693159103 Accuracy 0.823199987411499\n",
      "Iteration 9720 Training loss 0.017187412828207016 Validation loss 0.017804592847824097 Accuracy 0.8177000284194946\n",
      "Iteration 9730 Training loss 0.014822931960225105 Validation loss 0.018151341006159782 Accuracy 0.8130000233650208\n",
      "Iteration 9740 Training loss 0.016619015485048294 Validation loss 0.017380662262439728 Accuracy 0.8216999769210815\n",
      "Iteration 9750 Training loss 0.015055212192237377 Validation loss 0.017324745655059814 Accuracy 0.8224999904632568\n",
      "Iteration 9760 Training loss 0.015274152159690857 Validation loss 0.017350424081087112 Accuracy 0.8217999935150146\n",
      "Iteration 9770 Training loss 0.015789344906806946 Validation loss 0.017460940405726433 Accuracy 0.821399986743927\n",
      "Iteration 9780 Training loss 0.015437624417245388 Validation loss 0.017419693991541862 Accuracy 0.8217999935150146\n",
      "Iteration 9790 Training loss 0.014657918363809586 Validation loss 0.01732744462788105 Accuracy 0.8223000168800354\n",
      "Iteration 9800 Training loss 0.01638292334973812 Validation loss 0.017288511618971825 Accuracy 0.822700023651123\n",
      "Iteration 9810 Training loss 0.014171780087053776 Validation loss 0.017696434631943703 Accuracy 0.8188999891281128\n",
      "Iteration 9820 Training loss 0.014285707846283913 Validation loss 0.017586620524525642 Accuracy 0.819599986076355\n",
      "Iteration 9830 Training loss 0.015595314092934132 Validation loss 0.01779043860733509 Accuracy 0.8184000253677368\n",
      "Iteration 9840 Training loss 0.01611504703760147 Validation loss 0.018524227663874626 Accuracy 0.8101999759674072\n",
      "Iteration 9850 Training loss 0.01721729338169098 Validation loss 0.017483647912740707 Accuracy 0.8205999732017517\n",
      "Iteration 9860 Training loss 0.013813981786370277 Validation loss 0.017334572970867157 Accuracy 0.8233000040054321\n",
      "Iteration 9870 Training loss 0.016179513186216354 Validation loss 0.018424997106194496 Accuracy 0.8119999766349792\n",
      "Iteration 9880 Training loss 0.014719256199896336 Validation loss 0.01860978454351425 Accuracy 0.8091999888420105\n",
      "Iteration 9890 Training loss 0.015553489327430725 Validation loss 0.017786022275686264 Accuracy 0.8184000253677368\n",
      "Iteration 9900 Training loss 0.015115977264940739 Validation loss 0.017283622175455093 Accuracy 0.8228999972343445\n",
      "Iteration 9910 Training loss 0.01682092249393463 Validation loss 0.017265891656279564 Accuracy 0.8230000138282776\n",
      "Iteration 9920 Training loss 0.013779204338788986 Validation loss 0.017453322187066078 Accuracy 0.820900022983551\n",
      "Iteration 9930 Training loss 0.017614783719182014 Validation loss 0.01707153022289276 Accuracy 0.8253999948501587\n",
      "Iteration 9940 Training loss 0.015024815686047077 Validation loss 0.016887977719306946 Accuracy 0.8271999955177307\n",
      "Iteration 9950 Training loss 0.016505030915141106 Validation loss 0.017419269308447838 Accuracy 0.8215000033378601\n",
      "Iteration 9960 Training loss 0.015233014710247517 Validation loss 0.018393704667687416 Accuracy 0.8116000294685364\n",
      "Iteration 9970 Training loss 0.014447608031332493 Validation loss 0.017461439594626427 Accuracy 0.8217999935150146\n",
      "Iteration 9980 Training loss 0.017643040046095848 Validation loss 0.017712276428937912 Accuracy 0.8180000185966492\n",
      "Iteration 9990 Training loss 0.01791379600763321 Validation loss 0.018095888197422028 Accuracy 0.8151999711990356\n",
      "Iteration 10000 Training loss 0.013407982885837555 Validation loss 0.01735059730708599 Accuracy 0.8220000267028809\n",
      "Iteration 10010 Training loss 0.015281500294804573 Validation loss 0.01772977225482464 Accuracy 0.8184000253677368\n",
      "Iteration 10020 Training loss 0.013889309950172901 Validation loss 0.017721062526106834 Accuracy 0.8188999891281128\n",
      "Iteration 10030 Training loss 0.017644552513957024 Validation loss 0.017812874168157578 Accuracy 0.8177000284194946\n",
      "Iteration 10040 Training loss 0.01514752209186554 Validation loss 0.017540525645017624 Accuracy 0.8198999762535095\n",
      "Iteration 10050 Training loss 0.012286691926419735 Validation loss 0.017829161137342453 Accuracy 0.817799985408783\n",
      "Iteration 10060 Training loss 0.01741373911499977 Validation loss 0.01730792038142681 Accuracy 0.8228999972343445\n",
      "Iteration 10070 Training loss 0.015837298706173897 Validation loss 0.017956137657165527 Accuracy 0.8159999847412109\n",
      "Iteration 10080 Training loss 0.014623771421611309 Validation loss 0.01836228556931019 Accuracy 0.8116000294685364\n",
      "Iteration 10090 Training loss 0.015228082425892353 Validation loss 0.017768487334251404 Accuracy 0.8183000087738037\n",
      "Iteration 10100 Training loss 0.013327832333743572 Validation loss 0.017349641770124435 Accuracy 0.8217999935150146\n",
      "Iteration 10110 Training loss 0.0135244345292449 Validation loss 0.017136961221694946 Accuracy 0.824400007724762\n",
      "Iteration 10120 Training loss 0.013132457621395588 Validation loss 0.017509490251541138 Accuracy 0.8212000131607056\n",
      "Iteration 10130 Training loss 0.0152866430580616 Validation loss 0.01705971173942089 Accuracy 0.8256000280380249\n",
      "Iteration 10140 Training loss 0.016515111550688744 Validation loss 0.01770113967359066 Accuracy 0.8188999891281128\n",
      "Iteration 10150 Training loss 0.015115411952137947 Validation loss 0.017566058784723282 Accuracy 0.8199999928474426\n",
      "Iteration 10160 Training loss 0.015945157036185265 Validation loss 0.017714394256472588 Accuracy 0.8190000057220459\n",
      "Iteration 10170 Training loss 0.017517780885100365 Validation loss 0.017270514741539955 Accuracy 0.8224999904632568\n",
      "Iteration 10180 Training loss 0.015261593274772167 Validation loss 0.017646988853812218 Accuracy 0.8185999989509583\n",
      "Iteration 10190 Training loss 0.016028597950935364 Validation loss 0.01750139705836773 Accuracy 0.820900022983551\n",
      "Iteration 10200 Training loss 0.015448608435690403 Validation loss 0.017356110736727715 Accuracy 0.8226000070571899\n",
      "Iteration 10210 Training loss 0.014681183733046055 Validation loss 0.017701953649520874 Accuracy 0.8194000124931335\n",
      "Iteration 10220 Training loss 0.013578341342508793 Validation loss 0.01800328679382801 Accuracy 0.8151000142097473\n",
      "Iteration 10230 Training loss 0.01474919356405735 Validation loss 0.017286088317632675 Accuracy 0.8220000267028809\n",
      "Iteration 10240 Training loss 0.014031047001481056 Validation loss 0.017698772251605988 Accuracy 0.8184000253677368\n",
      "Iteration 10250 Training loss 0.015221220441162586 Validation loss 0.01835106872022152 Accuracy 0.8113999962806702\n",
      "Iteration 10260 Training loss 0.014719735831022263 Validation loss 0.0176620502024889 Accuracy 0.8184000253677368\n",
      "Iteration 10270 Training loss 0.01366046816110611 Validation loss 0.017858820036053658 Accuracy 0.8172000050544739\n",
      "Iteration 10280 Training loss 0.016564808785915375 Validation loss 0.018640222027897835 Accuracy 0.807200014591217\n",
      "Iteration 10290 Training loss 0.01509972382336855 Validation loss 0.0173367727547884 Accuracy 0.8219000101089478\n",
      "Iteration 10300 Training loss 0.015314637683331966 Validation loss 0.017570236697793007 Accuracy 0.8198999762535095\n",
      "Iteration 10310 Training loss 0.013904268853366375 Validation loss 0.01704905927181244 Accuracy 0.8248999714851379\n",
      "Iteration 10320 Training loss 0.01323274988681078 Validation loss 0.01706382818520069 Accuracy 0.8256000280380249\n",
      "Iteration 10330 Training loss 0.014926673844456673 Validation loss 0.017377866432070732 Accuracy 0.8213000297546387\n",
      "Iteration 10340 Training loss 0.012953273952007294 Validation loss 0.017419643700122833 Accuracy 0.820900022983551\n",
      "Iteration 10350 Training loss 0.015991264954209328 Validation loss 0.01750493235886097 Accuracy 0.8202999830245972\n",
      "Iteration 10360 Training loss 0.015453212894499302 Validation loss 0.018245315179228783 Accuracy 0.8113999962806702\n",
      "Iteration 10370 Training loss 0.014607551507651806 Validation loss 0.017227618023753166 Accuracy 0.8233000040054321\n",
      "Iteration 10380 Training loss 0.01295219175517559 Validation loss 0.017852431163191795 Accuracy 0.8159999847412109\n",
      "Iteration 10390 Training loss 0.014045044779777527 Validation loss 0.017156239598989487 Accuracy 0.8241000175476074\n",
      "Iteration 10400 Training loss 0.01897243969142437 Validation loss 0.018040718510746956 Accuracy 0.8148999810218811\n",
      "Iteration 10410 Training loss 0.016852376982569695 Validation loss 0.017765598371624947 Accuracy 0.8179000020027161\n",
      "Iteration 10420 Training loss 0.014012598432600498 Validation loss 0.017489101737737656 Accuracy 0.8208000063896179\n",
      "Iteration 10430 Training loss 0.013199363835155964 Validation loss 0.017300786450505257 Accuracy 0.8222000002861023\n",
      "Iteration 10440 Training loss 0.014681275933980942 Validation loss 0.016965048387646675 Accuracy 0.8256999850273132\n",
      "Iteration 10450 Training loss 0.01220998726785183 Validation loss 0.017194077372550964 Accuracy 0.8234999775886536\n",
      "Iteration 10460 Training loss 0.015828125178813934 Validation loss 0.01796003431081772 Accuracy 0.815500020980835\n",
      "Iteration 10470 Training loss 0.016085460782051086 Validation loss 0.017623526975512505 Accuracy 0.8191999793052673\n",
      "Iteration 10480 Training loss 0.015756933018565178 Validation loss 0.017196638509631157 Accuracy 0.8235999941825867\n",
      "Iteration 10490 Training loss 0.012884385883808136 Validation loss 0.017075229436159134 Accuracy 0.8241999745368958\n",
      "Iteration 10500 Training loss 0.017338333651423454 Validation loss 0.017201147973537445 Accuracy 0.8234000205993652\n",
      "Iteration 10510 Training loss 0.015131236985325813 Validation loss 0.017891226336359978 Accuracy 0.8159000277519226\n",
      "Iteration 10520 Training loss 0.01650690846145153 Validation loss 0.018206197768449783 Accuracy 0.8119999766349792\n",
      "Iteration 10530 Training loss 0.014487044885754585 Validation loss 0.016980132088065147 Accuracy 0.826200008392334\n",
      "Iteration 10540 Training loss 0.01450001634657383 Validation loss 0.017240844666957855 Accuracy 0.8230000138282776\n",
      "Iteration 10550 Training loss 0.01558752916753292 Validation loss 0.017568891867995262 Accuracy 0.8195000290870667\n",
      "Iteration 10560 Training loss 0.015083360485732555 Validation loss 0.017705217003822327 Accuracy 0.8183000087738037\n",
      "Iteration 10570 Training loss 0.013223415240645409 Validation loss 0.017197392880916595 Accuracy 0.8237000107765198\n",
      "Iteration 10580 Training loss 0.013735217973589897 Validation loss 0.017268115654587746 Accuracy 0.8226000070571899\n",
      "Iteration 10590 Training loss 0.01613220013678074 Validation loss 0.017282485961914062 Accuracy 0.823199987411499\n",
      "Iteration 10600 Training loss 0.01545813400298357 Validation loss 0.0170510895550251 Accuracy 0.8248000144958496\n",
      "Iteration 10610 Training loss 0.016763964667916298 Validation loss 0.017309270799160004 Accuracy 0.8216999769210815\n",
      "Iteration 10620 Training loss 0.01476304978132248 Validation loss 0.017123084515333176 Accuracy 0.824400007724762\n",
      "Iteration 10630 Training loss 0.013344130478799343 Validation loss 0.017379239201545715 Accuracy 0.8227999806404114\n",
      "Iteration 10640 Training loss 0.015963781625032425 Validation loss 0.017554810270667076 Accuracy 0.8203999996185303\n",
      "Iteration 10650 Training loss 0.016143232583999634 Validation loss 0.017223550006747246 Accuracy 0.8224999904632568\n",
      "Iteration 10660 Training loss 0.013700917363166809 Validation loss 0.017255304381251335 Accuracy 0.8224999904632568\n",
      "Iteration 10670 Training loss 0.015028472058475018 Validation loss 0.017247062176465988 Accuracy 0.8223999738693237\n",
      "Iteration 10680 Training loss 0.016253866255283356 Validation loss 0.01914738491177559 Accuracy 0.8019000291824341\n",
      "Iteration 10690 Training loss 0.014374122954905033 Validation loss 0.017116187140345573 Accuracy 0.8245999813079834\n",
      "Iteration 10700 Training loss 0.012259973213076591 Validation loss 0.017609721049666405 Accuracy 0.8184999823570251\n",
      "Iteration 10710 Training loss 0.015512467361986637 Validation loss 0.018551675602793694 Accuracy 0.8101000189781189\n",
      "Iteration 10720 Training loss 0.014729726128280163 Validation loss 0.01724827103316784 Accuracy 0.8228999972343445\n",
      "Iteration 10730 Training loss 0.015448999591171741 Validation loss 0.017109733074903488 Accuracy 0.8238000273704529\n",
      "Iteration 10740 Training loss 0.014011817052960396 Validation loss 0.016868000850081444 Accuracy 0.8273000121116638\n",
      "Iteration 10750 Training loss 0.01612374372780323 Validation loss 0.017517564818263054 Accuracy 0.8187000155448914\n",
      "Iteration 10760 Training loss 0.014137827791273594 Validation loss 0.017553551122546196 Accuracy 0.8201000094413757\n",
      "Iteration 10770 Training loss 0.014488794840872288 Validation loss 0.016977936029434204 Accuracy 0.8255000114440918\n",
      "Iteration 10780 Training loss 0.013229585252702236 Validation loss 0.017031989991664886 Accuracy 0.8241999745368958\n",
      "Iteration 10790 Training loss 0.016485029831528664 Validation loss 0.01770685985684395 Accuracy 0.8177000284194946\n",
      "Iteration 10800 Training loss 0.01473168097436428 Validation loss 0.016998929902911186 Accuracy 0.8248999714851379\n",
      "Iteration 10810 Training loss 0.014490200206637383 Validation loss 0.017778798937797546 Accuracy 0.8169999718666077\n",
      "Iteration 10820 Training loss 0.014515114948153496 Validation loss 0.017570555210113525 Accuracy 0.8181999921798706\n",
      "Iteration 10830 Training loss 0.013454859144985676 Validation loss 0.017630698159337044 Accuracy 0.8159999847412109\n",
      "Iteration 10840 Training loss 0.012721618637442589 Validation loss 0.016689354553818703 Accuracy 0.8270000219345093\n",
      "Iteration 10850 Training loss 0.012700543738901615 Validation loss 0.015822554007172585 Accuracy 0.8345000147819519\n",
      "Iteration 10860 Training loss 0.012420054525136948 Validation loss 0.015742536634206772 Accuracy 0.8363999724388123\n",
      "Iteration 10870 Training loss 0.012060122564435005 Validation loss 0.016274960711598396 Accuracy 0.8307999968528748\n",
      "Iteration 10880 Training loss 0.008847487159073353 Validation loss 0.015159372240304947 Accuracy 0.8424999713897705\n",
      "Iteration 10890 Training loss 0.01498487964272499 Validation loss 0.0171187911182642 Accuracy 0.8230999708175659\n",
      "Iteration 10900 Training loss 0.01279741246253252 Validation loss 0.015580947510898113 Accuracy 0.8385000228881836\n",
      "Iteration 10910 Training loss 0.014029383659362793 Validation loss 0.015680214390158653 Accuracy 0.8371000289916992\n",
      "Iteration 10920 Training loss 0.011356967501342297 Validation loss 0.014878175221383572 Accuracy 0.8450000286102295\n",
      "Iteration 10930 Training loss 0.012848705053329468 Validation loss 0.014899208210408688 Accuracy 0.845300018787384\n",
      "Iteration 10940 Training loss 0.012009940110147 Validation loss 0.015532148070633411 Accuracy 0.8389000296592712\n",
      "Iteration 10950 Training loss 0.01765858568251133 Validation loss 0.020756714046001434 Accuracy 0.7846999764442444\n",
      "Iteration 10960 Training loss 0.014630181714892387 Validation loss 0.015755286440253258 Accuracy 0.8378999829292297\n",
      "Iteration 10970 Training loss 0.014339908957481384 Validation loss 0.015105785802006721 Accuracy 0.8435999751091003\n",
      "Iteration 10980 Training loss 0.013458112254738808 Validation loss 0.015816686674952507 Accuracy 0.8356000185012817\n",
      "Iteration 10990 Training loss 0.008649109862744808 Validation loss 0.016004204750061035 Accuracy 0.8335999846458435\n",
      "Iteration 11000 Training loss 0.009865348227322102 Validation loss 0.014789959415793419 Accuracy 0.8478999733924866\n",
      "Iteration 11010 Training loss 0.010998162440955639 Validation loss 0.015118812210857868 Accuracy 0.8424000144004822\n",
      "Iteration 11020 Training loss 0.012201334349811077 Validation loss 0.014351025223731995 Accuracy 0.8504999876022339\n",
      "Iteration 11030 Training loss 0.01677003689110279 Validation loss 0.016462333500385284 Accuracy 0.829200029373169\n",
      "Iteration 11040 Training loss 0.012752288952469826 Validation loss 0.014715385623276234 Accuracy 0.8476999998092651\n",
      "Iteration 11050 Training loss 0.012069332413375378 Validation loss 0.014383632689714432 Accuracy 0.8514999747276306\n",
      "Iteration 11060 Training loss 0.011815741658210754 Validation loss 0.014678138308227062 Accuracy 0.8467000126838684\n",
      "Iteration 11070 Training loss 0.013783845119178295 Validation loss 0.015050649642944336 Accuracy 0.8449000120162964\n",
      "Iteration 11080 Training loss 0.012386047281324863 Validation loss 0.01549755409359932 Accuracy 0.8389999866485596\n",
      "Iteration 11090 Training loss 0.01169199775904417 Validation loss 0.014281542971730232 Accuracy 0.8518000245094299\n",
      "Iteration 11100 Training loss 0.011307230219244957 Validation loss 0.014821222983300686 Accuracy 0.8454999923706055\n",
      "Iteration 11110 Training loss 0.011826887726783752 Validation loss 0.014526685699820518 Accuracy 0.8482000231742859\n",
      "Iteration 11120 Training loss 0.013148785568773746 Validation loss 0.015176412649452686 Accuracy 0.8431000113487244\n",
      "Iteration 11130 Training loss 0.014457267709076405 Validation loss 0.016451630741357803 Accuracy 0.8299000263214111\n",
      "Iteration 11140 Training loss 0.014231301844120026 Validation loss 0.014102472923696041 Accuracy 0.8533999919891357\n",
      "Iteration 11150 Training loss 0.009564591571688652 Validation loss 0.014538364484906197 Accuracy 0.8495000004768372\n",
      "Iteration 11160 Training loss 0.01329794805496931 Validation loss 0.015620769001543522 Accuracy 0.8384000062942505\n",
      "Iteration 11170 Training loss 0.01038798876106739 Validation loss 0.0147471372038126 Accuracy 0.847599983215332\n",
      "Iteration 11180 Training loss 0.009937458671629429 Validation loss 0.014234964735805988 Accuracy 0.8518999814987183\n",
      "Iteration 11190 Training loss 0.011969786137342453 Validation loss 0.014935553073883057 Accuracy 0.8454999923706055\n",
      "Iteration 11200 Training loss 0.012325826101005077 Validation loss 0.014860107563436031 Accuracy 0.845300018787384\n",
      "Iteration 11210 Training loss 0.009731425903737545 Validation loss 0.014405892230570316 Accuracy 0.8500999808311462\n",
      "Iteration 11220 Training loss 0.01501370221376419 Validation loss 0.01641298644244671 Accuracy 0.8281999826431274\n",
      "Iteration 11230 Training loss 0.012284486554563046 Validation loss 0.014587851241230965 Accuracy 0.8485999703407288\n",
      "Iteration 11240 Training loss 0.014096175320446491 Validation loss 0.015047329477965832 Accuracy 0.8424000144004822\n",
      "Iteration 11250 Training loss 0.011094333603978157 Validation loss 0.014319051057100296 Accuracy 0.8503999710083008\n",
      "Iteration 11260 Training loss 0.01232822798192501 Validation loss 0.014013481326401234 Accuracy 0.853600025177002\n",
      "Iteration 11270 Training loss 0.013173433020710945 Validation loss 0.014977610670030117 Accuracy 0.8436999917030334\n",
      "Iteration 11280 Training loss 0.012395903468132019 Validation loss 0.014976167120039463 Accuracy 0.8435999751091003\n",
      "Iteration 11290 Training loss 0.009188113734126091 Validation loss 0.01429253164678812 Accuracy 0.8507000207901001\n",
      "Iteration 11300 Training loss 0.012797819450497627 Validation loss 0.014033500105142593 Accuracy 0.853600025177002\n",
      "Iteration 11310 Training loss 0.013082548975944519 Validation loss 0.014388272538781166 Accuracy 0.8503999710083008\n",
      "Iteration 11320 Training loss 0.011398605071008205 Validation loss 0.014021501876413822 Accuracy 0.8539999723434448\n",
      "Iteration 11330 Training loss 0.011795924976468086 Validation loss 0.014971345663070679 Accuracy 0.8443999886512756\n",
      "Iteration 11340 Training loss 0.011317330412566662 Validation loss 0.014106702990829945 Accuracy 0.8532999753952026\n",
      "Iteration 11350 Training loss 0.011929651722311974 Validation loss 0.013730259612202644 Accuracy 0.8561999797821045\n",
      "Iteration 11360 Training loss 0.01229116041213274 Validation loss 0.01393708772957325 Accuracy 0.8561999797821045\n",
      "Iteration 11370 Training loss 0.011884897015988827 Validation loss 0.014136010780930519 Accuracy 0.8531000018119812\n",
      "Iteration 11380 Training loss 0.017533639445900917 Validation loss 0.019854657351970673 Accuracy 0.7940999865531921\n",
      "Iteration 11390 Training loss 0.010789094492793083 Validation loss 0.013835998252034187 Accuracy 0.8551999926567078\n",
      "Iteration 11400 Training loss 0.011029167100787163 Validation loss 0.015040654689073563 Accuracy 0.8443999886512756\n",
      "Iteration 11410 Training loss 0.010287294164299965 Validation loss 0.014337689615786076 Accuracy 0.8522999882698059\n",
      "Iteration 11420 Training loss 0.011218889616429806 Validation loss 0.013976257294416428 Accuracy 0.853600025177002\n",
      "Iteration 11430 Training loss 0.011358438059687614 Validation loss 0.014882741495966911 Accuracy 0.8457000255584717\n",
      "Iteration 11440 Training loss 0.010926020331680775 Validation loss 0.014029773883521557 Accuracy 0.8536999821662903\n",
      "Iteration 11450 Training loss 0.01060725748538971 Validation loss 0.013892510905861855 Accuracy 0.8568999767303467\n",
      "Iteration 11460 Training loss 0.013263924047350883 Validation loss 0.015939634293317795 Accuracy 0.8349000215530396\n",
      "Iteration 11470 Training loss 0.01130292471498251 Validation loss 0.013758811168372631 Accuracy 0.857200026512146\n",
      "Iteration 11480 Training loss 0.010973217897117138 Validation loss 0.013709771446883678 Accuracy 0.8589000105857849\n",
      "Iteration 11490 Training loss 0.013501699082553387 Validation loss 0.014623884111642838 Accuracy 0.8482000231742859\n",
      "Iteration 11500 Training loss 0.012888752855360508 Validation loss 0.01584857888519764 Accuracy 0.8352000117301941\n",
      "Iteration 11510 Training loss 0.010556197725236416 Validation loss 0.014135582372546196 Accuracy 0.8529000282287598\n",
      "Iteration 11520 Training loss 0.010690247640013695 Validation loss 0.013548480346798897 Accuracy 0.8589000105857849\n",
      "Iteration 11530 Training loss 0.012499523349106312 Validation loss 0.01419101282954216 Accuracy 0.8525999784469604\n",
      "Iteration 11540 Training loss 0.011331119574606419 Validation loss 0.014075541868805885 Accuracy 0.8535000085830688\n",
      "Iteration 11550 Training loss 0.010979725047945976 Validation loss 0.013802762143313885 Accuracy 0.8568999767303467\n",
      "Iteration 11560 Training loss 0.010102211497724056 Validation loss 0.015259540639817715 Accuracy 0.840399980545044\n",
      "Iteration 11570 Training loss 0.011340911500155926 Validation loss 0.014699318446218967 Accuracy 0.847000002861023\n",
      "Iteration 11580 Training loss 0.009341485798358917 Validation loss 0.014618675224483013 Accuracy 0.8474000096321106\n",
      "Iteration 11590 Training loss 0.014056313782930374 Validation loss 0.014251725748181343 Accuracy 0.8521999716758728\n",
      "Iteration 11600 Training loss 0.010458358563482761 Validation loss 0.014200644567608833 Accuracy 0.8518999814987183\n",
      "Iteration 11610 Training loss 0.012465758249163628 Validation loss 0.01387315709143877 Accuracy 0.855400025844574\n",
      "Iteration 11620 Training loss 0.011974970810115337 Validation loss 0.013948922976851463 Accuracy 0.8551999926567078\n",
      "Iteration 11630 Training loss 0.010574464686214924 Validation loss 0.013770746998488903 Accuracy 0.8569999933242798\n",
      "Iteration 11640 Training loss 0.011491778306663036 Validation loss 0.014997310005128384 Accuracy 0.8446000218391418\n",
      "Iteration 11650 Training loss 0.012231016531586647 Validation loss 0.01358872652053833 Accuracy 0.8589000105857849\n",
      "Iteration 11660 Training loss 0.012414713390171528 Validation loss 0.013692986220121384 Accuracy 0.8583999872207642\n",
      "Iteration 11670 Training loss 0.010942799039185047 Validation loss 0.013621353544294834 Accuracy 0.8585000038146973\n",
      "Iteration 11680 Training loss 0.010715029202401638 Validation loss 0.014341697096824646 Accuracy 0.8508999943733215\n",
      "Iteration 11690 Training loss 0.010956555604934692 Validation loss 0.013855601660907269 Accuracy 0.85589998960495\n",
      "Iteration 11700 Training loss 0.009440172463655472 Validation loss 0.013661965727806091 Accuracy 0.8575000166893005\n",
      "Iteration 11710 Training loss 0.011329126544296741 Validation loss 0.013776064850389957 Accuracy 0.85589998960495\n",
      "Iteration 11720 Training loss 0.011307226493954659 Validation loss 0.013947825878858566 Accuracy 0.8546000123023987\n",
      "Iteration 11730 Training loss 0.010882802307605743 Validation loss 0.01343483291566372 Accuracy 0.8596000075340271\n",
      "Iteration 11740 Training loss 0.00942111387848854 Validation loss 0.013401910662651062 Accuracy 0.86080002784729\n",
      "Iteration 11750 Training loss 0.008008310571312904 Validation loss 0.01312203612178564 Accuracy 0.8634999990463257\n",
      "Iteration 11760 Training loss 0.011022916994988918 Validation loss 0.013584259897470474 Accuracy 0.8593000173568726\n",
      "Iteration 11770 Training loss 0.00971875712275505 Validation loss 0.013663542456924915 Accuracy 0.8571000099182129\n",
      "Iteration 11780 Training loss 0.013011802919209003 Validation loss 0.013556436635553837 Accuracy 0.8579000234603882\n",
      "Iteration 11790 Training loss 0.010579624213278294 Validation loss 0.013686297461390495 Accuracy 0.8575999736785889\n",
      "Iteration 11800 Training loss 0.013756765983998775 Validation loss 0.014861419796943665 Accuracy 0.8454999923706055\n",
      "Iteration 11810 Training loss 0.011984209530055523 Validation loss 0.013990788720548153 Accuracy 0.8540999889373779\n",
      "Iteration 11820 Training loss 0.010744472965598106 Validation loss 0.01410836074501276 Accuracy 0.852400004863739\n",
      "Iteration 11830 Training loss 0.010318389162421227 Validation loss 0.01391636859625578 Accuracy 0.8539000153541565\n",
      "Iteration 11840 Training loss 0.010053006932139397 Validation loss 0.014592103660106659 Accuracy 0.8472999930381775\n",
      "Iteration 11850 Training loss 0.01091020554304123 Validation loss 0.013575073331594467 Accuracy 0.859000027179718\n",
      "Iteration 11860 Training loss 0.009067006409168243 Validation loss 0.01367434673011303 Accuracy 0.8561999797821045\n",
      "Iteration 11870 Training loss 0.010882346890866756 Validation loss 0.015178665518760681 Accuracy 0.842199981212616\n",
      "Iteration 11880 Training loss 0.010325923562049866 Validation loss 0.013488470576703548 Accuracy 0.8597000241279602\n",
      "Iteration 11890 Training loss 0.013138054870069027 Validation loss 0.013757090084254742 Accuracy 0.8571000099182129\n",
      "Iteration 11900 Training loss 0.010254266671836376 Validation loss 0.01360363606363535 Accuracy 0.8589000105857849\n",
      "Iteration 11910 Training loss 0.010085233487188816 Validation loss 0.014413919299840927 Accuracy 0.8507000207901001\n",
      "Iteration 11920 Training loss 0.01084098219871521 Validation loss 0.01503479853272438 Accuracy 0.8429999947547913\n",
      "Iteration 11930 Training loss 0.010692227631807327 Validation loss 0.014244237914681435 Accuracy 0.8518999814987183\n",
      "Iteration 11940 Training loss 0.011290661059319973 Validation loss 0.013500083237886429 Accuracy 0.8593999743461609\n",
      "Iteration 11950 Training loss 0.010767940431833267 Validation loss 0.01396371703594923 Accuracy 0.8549000024795532\n",
      "Iteration 11960 Training loss 0.01060535665601492 Validation loss 0.013781423680484295 Accuracy 0.8557000160217285\n",
      "Iteration 11970 Training loss 0.008814556524157524 Validation loss 0.013386111706495285 Accuracy 0.8601999878883362\n",
      "Iteration 11980 Training loss 0.010596835054457188 Validation loss 0.01324050035327673 Accuracy 0.8619999885559082\n",
      "Iteration 11990 Training loss 0.01290880423039198 Validation loss 0.014310233294963837 Accuracy 0.8511999845504761\n",
      "Iteration 12000 Training loss 0.010304690338671207 Validation loss 0.013685031794011593 Accuracy 0.857699990272522\n",
      "Iteration 12010 Training loss 0.01065656915307045 Validation loss 0.013449539430439472 Accuracy 0.8598999977111816\n",
      "Iteration 12020 Training loss 0.01061293389648199 Validation loss 0.014089076779782772 Accuracy 0.8532000184059143\n",
      "Iteration 12030 Training loss 0.01052951905876398 Validation loss 0.01303925458341837 Accuracy 0.8634999990463257\n",
      "Iteration 12040 Training loss 0.011315196752548218 Validation loss 0.014788702130317688 Accuracy 0.8460000157356262\n",
      "Iteration 12050 Training loss 0.010339015163481236 Validation loss 0.014211506582796574 Accuracy 0.8508999943733215\n",
      "Iteration 12060 Training loss 0.010227886028587818 Validation loss 0.014214254915714264 Accuracy 0.8518999814987183\n",
      "Iteration 12070 Training loss 0.010722066275775433 Validation loss 0.013673834502696991 Accuracy 0.8569999933242798\n",
      "Iteration 12080 Training loss 0.013561878353357315 Validation loss 0.014516505412757397 Accuracy 0.8507000207901001\n",
      "Iteration 12090 Training loss 0.010702657513320446 Validation loss 0.013869201764464378 Accuracy 0.85589998960495\n",
      "Iteration 12100 Training loss 0.009290044195950031 Validation loss 0.013240501284599304 Accuracy 0.8616999983787537\n",
      "Iteration 12110 Training loss 0.010957866907119751 Validation loss 0.014085342176258564 Accuracy 0.8529000282287598\n",
      "Iteration 12120 Training loss 0.01042620837688446 Validation loss 0.013871703296899796 Accuracy 0.855400025844574\n",
      "Iteration 12130 Training loss 0.010544225573539734 Validation loss 0.013637428171932697 Accuracy 0.8582000136375427\n",
      "Iteration 12140 Training loss 0.01328634936362505 Validation loss 0.014915063045918941 Accuracy 0.843999981880188\n",
      "Iteration 12150 Training loss 0.008925346657633781 Validation loss 0.013913237489759922 Accuracy 0.8553000092506409\n",
      "Iteration 12160 Training loss 0.010544825345277786 Validation loss 0.014181536622345448 Accuracy 0.8515999913215637\n",
      "Iteration 12170 Training loss 0.010029599070549011 Validation loss 0.013721391558647156 Accuracy 0.8572999835014343\n",
      "Iteration 12180 Training loss 0.012341871857643127 Validation loss 0.01407947763800621 Accuracy 0.8529000282287598\n",
      "Iteration 12190 Training loss 0.00955029297620058 Validation loss 0.013314609415829182 Accuracy 0.8615000247955322\n",
      "Iteration 12200 Training loss 0.014194950461387634 Validation loss 0.013911196030676365 Accuracy 0.8540999889373779\n",
      "Iteration 12210 Training loss 0.011758006177842617 Validation loss 0.013842237181961536 Accuracy 0.8547000288963318\n",
      "Iteration 12220 Training loss 0.008903264999389648 Validation loss 0.013326730579137802 Accuracy 0.8605999946594238\n",
      "Iteration 12230 Training loss 0.00839434564113617 Validation loss 0.013226068578660488 Accuracy 0.8614000082015991\n",
      "Iteration 12240 Training loss 0.008509067818522453 Validation loss 0.013361893594264984 Accuracy 0.8608999848365784\n",
      "Iteration 12250 Training loss 0.009701129049062729 Validation loss 0.013402379117906094 Accuracy 0.8601999878883362\n",
      "Iteration 12260 Training loss 0.009437812492251396 Validation loss 0.014104150235652924 Accuracy 0.8522999882698059\n",
      "Iteration 12270 Training loss 0.009005029685795307 Validation loss 0.013718968257308006 Accuracy 0.8578000068664551\n",
      "Iteration 12280 Training loss 0.008784551173448563 Validation loss 0.01375856064260006 Accuracy 0.8557000160217285\n",
      "Iteration 12290 Training loss 0.009932813234627247 Validation loss 0.013197476044297218 Accuracy 0.8618999719619751\n",
      "Iteration 12300 Training loss 0.010655059479176998 Validation loss 0.013951962813735008 Accuracy 0.8537999987602234\n",
      "Iteration 12310 Training loss 0.01120088342577219 Validation loss 0.01408262737095356 Accuracy 0.8521999716758728\n",
      "Iteration 12320 Training loss 0.010159341618418694 Validation loss 0.013806682080030441 Accuracy 0.8553000092506409\n",
      "Iteration 12330 Training loss 0.008007671684026718 Validation loss 0.013692214153707027 Accuracy 0.8579000234603882\n",
      "Iteration 12340 Training loss 0.009586912579834461 Validation loss 0.013329562731087208 Accuracy 0.86080002784729\n",
      "Iteration 12350 Training loss 0.011743348091840744 Validation loss 0.014211294241249561 Accuracy 0.8513000011444092\n",
      "Iteration 12360 Training loss 0.010795815847814083 Validation loss 0.01355524081736803 Accuracy 0.8585000038146973\n",
      "Iteration 12370 Training loss 0.008703254163265228 Validation loss 0.013467253185808659 Accuracy 0.859000027179718\n",
      "Iteration 12380 Training loss 0.014473493210971355 Validation loss 0.013736902736127377 Accuracy 0.8561000227928162\n",
      "Iteration 12390 Training loss 0.01120260450989008 Validation loss 0.013502472080290318 Accuracy 0.8585000038146973\n",
      "Iteration 12400 Training loss 0.014092613011598587 Validation loss 0.014916503801941872 Accuracy 0.8458999991416931\n",
      "Iteration 12410 Training loss 0.012261216528713703 Validation loss 0.014323711395263672 Accuracy 0.8518000245094299\n",
      "Iteration 12420 Training loss 0.008638511411845684 Validation loss 0.013214834034442902 Accuracy 0.8622000217437744\n",
      "Iteration 12430 Training loss 0.01055500004440546 Validation loss 0.013238298706710339 Accuracy 0.8607000112533569\n",
      "Iteration 12440 Training loss 0.008493983186781406 Validation loss 0.013703932985663414 Accuracy 0.8569999933242798\n",
      "Iteration 12450 Training loss 0.009336047805845737 Validation loss 0.013667947612702847 Accuracy 0.8575000166893005\n",
      "Iteration 12460 Training loss 0.012994074262678623 Validation loss 0.014264457859098911 Accuracy 0.8507999777793884\n",
      "Iteration 12470 Training loss 0.010187258012592793 Validation loss 0.014276726171374321 Accuracy 0.8517000079154968\n",
      "Iteration 12480 Training loss 0.009861735627055168 Validation loss 0.013636835850775242 Accuracy 0.8580999970436096\n",
      "Iteration 12490 Training loss 0.008539749309420586 Validation loss 0.013618162833154202 Accuracy 0.8575999736785889\n",
      "Iteration 12500 Training loss 0.007977405562996864 Validation loss 0.013572309166193008 Accuracy 0.858299970626831\n",
      "Iteration 12510 Training loss 0.012454618699848652 Validation loss 0.013173045590519905 Accuracy 0.862500011920929\n",
      "Iteration 12520 Training loss 0.01143848430365324 Validation loss 0.013959460891783237 Accuracy 0.8543999791145325\n",
      "Iteration 12530 Training loss 0.010730287060141563 Validation loss 0.013008026406168938 Accuracy 0.8640000224113464\n",
      "Iteration 12540 Training loss 0.011636040173470974 Validation loss 0.013342919759452343 Accuracy 0.8605999946594238\n",
      "Iteration 12550 Training loss 0.01105561200529337 Validation loss 0.013427715748548508 Accuracy 0.8598999977111816\n",
      "Iteration 12560 Training loss 0.010026211850345135 Validation loss 0.013191900216042995 Accuracy 0.8622000217437744\n",
      "Iteration 12570 Training loss 0.011934899725019932 Validation loss 0.01312718354165554 Accuracy 0.8637999892234802\n",
      "Iteration 12580 Training loss 0.009999509900808334 Validation loss 0.013831946067512035 Accuracy 0.8554999828338623\n",
      "Iteration 12590 Training loss 0.00879006739705801 Validation loss 0.013639026321470737 Accuracy 0.8575999736785889\n",
      "Iteration 12600 Training loss 0.009553509764373302 Validation loss 0.01307682879269123 Accuracy 0.8634999990463257\n",
      "Iteration 12610 Training loss 0.010411938652396202 Validation loss 0.013654697686433792 Accuracy 0.8586999773979187\n",
      "Iteration 12620 Training loss 0.01160508207976818 Validation loss 0.01389361172914505 Accuracy 0.8560000061988831\n",
      "Iteration 12630 Training loss 0.00949034933000803 Validation loss 0.013573074713349342 Accuracy 0.8574000000953674\n",
      "Iteration 12640 Training loss 0.011179815046489239 Validation loss 0.013062471523880959 Accuracy 0.8636999726295471\n",
      "Iteration 12650 Training loss 0.008891156874597073 Validation loss 0.013947440311312675 Accuracy 0.8540999889373779\n",
      "Iteration 12660 Training loss 0.01194806955754757 Validation loss 0.013966781087219715 Accuracy 0.8555999994277954\n",
      "Iteration 12670 Training loss 0.011687065474689007 Validation loss 0.01393108069896698 Accuracy 0.8547000288963318\n",
      "Iteration 12680 Training loss 0.010390626266598701 Validation loss 0.01327754557132721 Accuracy 0.8615999817848206\n",
      "Iteration 12690 Training loss 0.009756242856383324 Validation loss 0.013336154632270336 Accuracy 0.86080002784729\n",
      "Iteration 12700 Training loss 0.01104667130857706 Validation loss 0.013616802170872688 Accuracy 0.8572999835014343\n",
      "Iteration 12710 Training loss 0.01107136532664299 Validation loss 0.013230479322373867 Accuracy 0.8621000051498413\n",
      "Iteration 12720 Training loss 0.010095269419252872 Validation loss 0.01332019455730915 Accuracy 0.86080002784729\n",
      "Iteration 12730 Training loss 0.009760198183357716 Validation loss 0.013179924339056015 Accuracy 0.8626999855041504\n",
      "Iteration 12740 Training loss 0.009608644060790539 Validation loss 0.013264413923025131 Accuracy 0.861299991607666\n",
      "Iteration 12750 Training loss 0.00903412140905857 Validation loss 0.013646071776747704 Accuracy 0.8567000031471252\n",
      "Iteration 12760 Training loss 0.012340583838522434 Validation loss 0.014053034596145153 Accuracy 0.8533999919891357\n",
      "Iteration 12770 Training loss 0.012623277492821217 Validation loss 0.013350402005016804 Accuracy 0.8605999946594238\n",
      "Iteration 12780 Training loss 0.00846821628510952 Validation loss 0.013289722613990307 Accuracy 0.8607000112533569\n",
      "Iteration 12790 Training loss 0.009153960272669792 Validation loss 0.013262524269521236 Accuracy 0.8616999983787537\n",
      "Iteration 12800 Training loss 0.009691637940704823 Validation loss 0.013435917906463146 Accuracy 0.8593999743461609\n",
      "Iteration 12810 Training loss 0.007753025274723768 Validation loss 0.012876886874437332 Accuracy 0.8665000200271606\n",
      "Iteration 12820 Training loss 0.009622075594961643 Validation loss 0.01279687974601984 Accuracy 0.8669000267982483\n",
      "Iteration 12830 Training loss 0.010585635900497437 Validation loss 0.014241254888474941 Accuracy 0.8511999845504761\n",
      "Iteration 12840 Training loss 0.010855835862457752 Validation loss 0.013091319240629673 Accuracy 0.8626999855041504\n",
      "Iteration 12850 Training loss 0.009295159950852394 Validation loss 0.012900810688734055 Accuracy 0.8651000261306763\n",
      "Iteration 12860 Training loss 0.00942218117415905 Validation loss 0.013701913878321648 Accuracy 0.8567000031471252\n",
      "Iteration 12870 Training loss 0.011390791274607182 Validation loss 0.013416905887424946 Accuracy 0.8600000143051147\n",
      "Iteration 12880 Training loss 0.010026502422988415 Validation loss 0.013746770098805428 Accuracy 0.8564000129699707\n",
      "Iteration 12890 Training loss 0.009849202819168568 Validation loss 0.013363132253289223 Accuracy 0.8601999878883362\n",
      "Iteration 12900 Training loss 0.009080114774405956 Validation loss 0.013020327314734459 Accuracy 0.8633000254631042\n",
      "Iteration 12910 Training loss 0.008447468280792236 Validation loss 0.01309851836413145 Accuracy 0.8633999824523926\n",
      "Iteration 12920 Training loss 0.008180608972907066 Validation loss 0.013334118761122227 Accuracy 0.8610000014305115\n",
      "Iteration 12930 Training loss 0.00959023367613554 Validation loss 0.01337339822202921 Accuracy 0.8597999811172485\n",
      "Iteration 12940 Training loss 0.010427320376038551 Validation loss 0.013073215261101723 Accuracy 0.8636999726295471\n",
      "Iteration 12950 Training loss 0.008425792679190636 Validation loss 0.013863674364984035 Accuracy 0.8553000092506409\n",
      "Iteration 12960 Training loss 0.01088303979486227 Validation loss 0.013126148842275143 Accuracy 0.8628000020980835\n",
      "Iteration 12970 Training loss 0.010010634548962116 Validation loss 0.013378739356994629 Accuracy 0.8597999811172485\n",
      "Iteration 12980 Training loss 0.009607240557670593 Validation loss 0.013582365587353706 Accuracy 0.8593000173568726\n",
      "Iteration 12990 Training loss 0.01103441882878542 Validation loss 0.012851779349148273 Accuracy 0.8654000163078308\n",
      "Iteration 13000 Training loss 0.008478955365717411 Validation loss 0.013288809917867184 Accuracy 0.8608999848365784\n",
      "Iteration 13010 Training loss 0.00888838991522789 Validation loss 0.013124467805027962 Accuracy 0.8623999953269958\n",
      "Iteration 13020 Training loss 0.008426336571574211 Validation loss 0.012776206247508526 Accuracy 0.8676000237464905\n",
      "Iteration 13030 Training loss 0.009580726735293865 Validation loss 0.013087117113173008 Accuracy 0.8628000020980835\n",
      "Iteration 13040 Training loss 0.009238209575414658 Validation loss 0.012982160784304142 Accuracy 0.8639000058174133\n",
      "Iteration 13050 Training loss 0.009413189254701138 Validation loss 0.01312008872628212 Accuracy 0.8626000285148621\n",
      "Iteration 13060 Training loss 0.009111848659813404 Validation loss 0.013322318904101849 Accuracy 0.8611999750137329\n",
      "Iteration 13070 Training loss 0.009649277664721012 Validation loss 0.01356544904410839 Accuracy 0.858299970626831\n",
      "Iteration 13080 Training loss 0.009002532809972763 Validation loss 0.013198446482419968 Accuracy 0.8619999885559082\n",
      "Iteration 13090 Training loss 0.00801966991275549 Validation loss 0.013175024650990963 Accuracy 0.8619999885559082\n",
      "Iteration 13100 Training loss 0.00898942444473505 Validation loss 0.012875061482191086 Accuracy 0.8654000163078308\n",
      "Iteration 13110 Training loss 0.009212438948452473 Validation loss 0.01308268029242754 Accuracy 0.8626000285148621\n",
      "Iteration 13120 Training loss 0.009003423154354095 Validation loss 0.013387752696871758 Accuracy 0.8597000241279602\n",
      "Iteration 13130 Training loss 0.013010995462536812 Validation loss 0.014060859568417072 Accuracy 0.8528000116348267\n",
      "Iteration 13140 Training loss 0.01149036642163992 Validation loss 0.01284540630877018 Accuracy 0.8655999898910522\n",
      "Iteration 13150 Training loss 0.008495657704770565 Validation loss 0.012753022834658623 Accuracy 0.8669000267982483\n",
      "Iteration 13160 Training loss 0.008524361066520214 Validation loss 0.013317910023033619 Accuracy 0.8611999750137329\n",
      "Iteration 13170 Training loss 0.012859726324677467 Validation loss 0.014308501034975052 Accuracy 0.8504999876022339\n",
      "Iteration 13180 Training loss 0.009396580047905445 Validation loss 0.013634427450597286 Accuracy 0.8575999736785889\n",
      "Iteration 13190 Training loss 0.008349649608135223 Validation loss 0.014934713952243328 Accuracy 0.8446000218391418\n",
      "Iteration 13200 Training loss 0.011257154867053032 Validation loss 0.013649131171405315 Accuracy 0.858299970626831\n",
      "Iteration 13210 Training loss 0.01059824787080288 Validation loss 0.012868298217654228 Accuracy 0.8658999800682068\n",
      "Iteration 13220 Training loss 0.009485503658652306 Validation loss 0.012620418332517147 Accuracy 0.8680999875068665\n",
      "Iteration 13230 Training loss 0.010938049294054508 Validation loss 0.013420147821307182 Accuracy 0.8597000241279602\n",
      "Iteration 13240 Training loss 0.009801129810512066 Validation loss 0.013136046938598156 Accuracy 0.8628000020980835\n",
      "Iteration 13250 Training loss 0.008589851669967175 Validation loss 0.013507598079741001 Accuracy 0.8597000241279602\n",
      "Iteration 13260 Training loss 0.009152032434940338 Validation loss 0.012996265664696693 Accuracy 0.8639000058174133\n",
      "Iteration 13270 Training loss 0.009064656682312489 Validation loss 0.012676622718572617 Accuracy 0.8677999973297119\n",
      "Iteration 13280 Training loss 0.0105780940502882 Validation loss 0.013392317108809948 Accuracy 0.859499990940094\n",
      "Iteration 13290 Training loss 0.010619065724313259 Validation loss 0.013297056779265404 Accuracy 0.8621000051498413\n",
      "Iteration 13300 Training loss 0.014943533577024937 Validation loss 0.0156359001994133 Accuracy 0.8378999829292297\n",
      "Iteration 13310 Training loss 0.012614967301487923 Validation loss 0.014127839356660843 Accuracy 0.8521000146865845\n",
      "Iteration 13320 Training loss 0.010439778678119183 Validation loss 0.012802778743207455 Accuracy 0.8668000102043152\n",
      "Iteration 13330 Training loss 0.010604998096823692 Validation loss 0.012795088812708855 Accuracy 0.8658999800682068\n",
      "Iteration 13340 Training loss 0.00898290891200304 Validation loss 0.012758960016071796 Accuracy 0.8672000169754028\n",
      "Iteration 13350 Training loss 0.010016538202762604 Validation loss 0.013012148439884186 Accuracy 0.8640000224113464\n",
      "Iteration 13360 Training loss 0.010105947032570839 Validation loss 0.012827526777982712 Accuracy 0.8666999936103821\n",
      "Iteration 13370 Training loss 0.008467643521726131 Validation loss 0.012796832248568535 Accuracy 0.8658000230789185\n",
      "Iteration 13380 Training loss 0.009464865550398827 Validation loss 0.012845369055867195 Accuracy 0.8657000064849854\n",
      "Iteration 13390 Training loss 0.010919453576207161 Validation loss 0.013743935152888298 Accuracy 0.855400025844574\n",
      "Iteration 13400 Training loss 0.008801600895822048 Validation loss 0.013963552191853523 Accuracy 0.8525999784469604\n",
      "Iteration 13410 Training loss 0.009209229610860348 Validation loss 0.013014473952353 Accuracy 0.8648999929428101\n",
      "Iteration 13420 Training loss 0.007443420588970184 Validation loss 0.012711809016764164 Accuracy 0.8669000267982483\n",
      "Iteration 13430 Training loss 0.012002470903098583 Validation loss 0.012990759685635567 Accuracy 0.864300012588501\n",
      "Iteration 13440 Training loss 0.008662247098982334 Validation loss 0.012888161465525627 Accuracy 0.8658000230789185\n",
      "Iteration 13450 Training loss 0.01247029472142458 Validation loss 0.013637225143611431 Accuracy 0.8585000038146973\n",
      "Iteration 13460 Training loss 0.007516286335885525 Validation loss 0.012818731367588043 Accuracy 0.8662999868392944\n",
      "Iteration 13470 Training loss 0.010949374176561832 Validation loss 0.012847162783145905 Accuracy 0.8650000095367432\n",
      "Iteration 13480 Training loss 0.010523870587348938 Validation loss 0.014084008522331715 Accuracy 0.853600025177002\n",
      "Iteration 13490 Training loss 0.009606795385479927 Validation loss 0.012907787226140499 Accuracy 0.8650000095367432\n",
      "Iteration 13500 Training loss 0.009722786024212837 Validation loss 0.012696507386863232 Accuracy 0.8675000071525574\n",
      "Iteration 13510 Training loss 0.013547108508646488 Validation loss 0.013186916708946228 Accuracy 0.8626999855041504\n",
      "Iteration 13520 Training loss 0.009053393267095089 Validation loss 0.014483005739748478 Accuracy 0.8482999801635742\n",
      "Iteration 13530 Training loss 0.01067165657877922 Validation loss 0.012877192348241806 Accuracy 0.8651000261306763\n",
      "Iteration 13540 Training loss 0.008363436907529831 Validation loss 0.012663875706493855 Accuracy 0.8673999905586243\n",
      "Iteration 13550 Training loss 0.008414783515036106 Validation loss 0.012592120096087456 Accuracy 0.8684999942779541\n",
      "Iteration 13560 Training loss 0.007600985001772642 Validation loss 0.01330797653645277 Accuracy 0.8616999983787537\n",
      "Iteration 13570 Training loss 0.009248554706573486 Validation loss 0.013835049234330654 Accuracy 0.85589998960495\n",
      "Iteration 13580 Training loss 0.010283219628036022 Validation loss 0.01278078742325306 Accuracy 0.8668000102043152\n",
      "Iteration 13590 Training loss 0.008741378784179688 Validation loss 0.013037465512752533 Accuracy 0.8636999726295471\n",
      "Iteration 13600 Training loss 0.010121076367795467 Validation loss 0.013193706050515175 Accuracy 0.8628000020980835\n",
      "Iteration 13610 Training loss 0.010592089034616947 Validation loss 0.01349599752575159 Accuracy 0.859000027179718\n",
      "Iteration 13620 Training loss 0.010082810185849667 Validation loss 0.014265302568674088 Accuracy 0.8514000177383423\n",
      "Iteration 13630 Training loss 0.010021495632827282 Validation loss 0.013021976687014103 Accuracy 0.8641999959945679\n",
      "Iteration 13640 Training loss 0.00911661610007286 Validation loss 0.012765380553901196 Accuracy 0.8665000200271606\n",
      "Iteration 13650 Training loss 0.010125134140253067 Validation loss 0.012957214377820492 Accuracy 0.8639000058174133\n",
      "Iteration 13660 Training loss 0.010766684077680111 Validation loss 0.013288432732224464 Accuracy 0.8611999750137329\n",
      "Iteration 13670 Training loss 0.01023817714303732 Validation loss 0.01277946773916483 Accuracy 0.8666999936103821\n",
      "Iteration 13680 Training loss 0.01088255550712347 Validation loss 0.012744294479489326 Accuracy 0.8658000230789185\n",
      "Iteration 13690 Training loss 0.009767581708729267 Validation loss 0.012672331184148788 Accuracy 0.867900013923645\n",
      "Iteration 13700 Training loss 0.00819661095738411 Validation loss 0.013240553438663483 Accuracy 0.8618999719619751\n",
      "Iteration 13710 Training loss 0.007690729107707739 Validation loss 0.012863293290138245 Accuracy 0.8664000034332275\n",
      "Iteration 13720 Training loss 0.010079470463097095 Validation loss 0.013145411387085915 Accuracy 0.8619999885559082\n",
      "Iteration 13730 Training loss 0.00848659873008728 Validation loss 0.013835722580552101 Accuracy 0.8567000031471252\n",
      "Iteration 13740 Training loss 0.008390657603740692 Validation loss 0.013199507258832455 Accuracy 0.8621000051498413\n",
      "Iteration 13750 Training loss 0.01095964852720499 Validation loss 0.014027253724634647 Accuracy 0.8535000085830688\n",
      "Iteration 13760 Training loss 0.008956079371273518 Validation loss 0.012963556684553623 Accuracy 0.864799976348877\n",
      "Iteration 13770 Training loss 0.009463587775826454 Validation loss 0.013275210745632648 Accuracy 0.8615000247955322\n",
      "Iteration 13780 Training loss 0.010714137926697731 Validation loss 0.012912615202367306 Accuracy 0.864799976348877\n",
      "Iteration 13790 Training loss 0.008036159910261631 Validation loss 0.013015522621572018 Accuracy 0.8652999997138977\n",
      "Iteration 13800 Training loss 0.010795910842716694 Validation loss 0.013442333787679672 Accuracy 0.8597999811172485\n",
      "Iteration 13810 Training loss 0.011083323508501053 Validation loss 0.012998774647712708 Accuracy 0.8644999861717224\n",
      "Iteration 13820 Training loss 0.0075886789709329605 Validation loss 0.013493208214640617 Accuracy 0.8582000136375427\n",
      "Iteration 13830 Training loss 0.013521934859454632 Validation loss 0.014796389266848564 Accuracy 0.8460999727249146\n",
      "Iteration 13840 Training loss 0.010145656764507294 Validation loss 0.014195800758898258 Accuracy 0.8518000245094299\n",
      "Iteration 13850 Training loss 0.00871123094111681 Validation loss 0.012982543557882309 Accuracy 0.8639000058174133\n",
      "Iteration 13860 Training loss 0.01001080870628357 Validation loss 0.012802732177078724 Accuracy 0.8657000064849854\n",
      "Iteration 13870 Training loss 0.008667214773595333 Validation loss 0.012732314877212048 Accuracy 0.8672999739646912\n",
      "Iteration 13880 Training loss 0.007983773946762085 Validation loss 0.012549162842333317 Accuracy 0.8687999844551086\n",
      "Iteration 13890 Training loss 0.011152681894600391 Validation loss 0.013341833837330341 Accuracy 0.8615000247955322\n",
      "Iteration 13900 Training loss 0.009746584109961987 Validation loss 0.013269958086311817 Accuracy 0.8621000051498413\n",
      "Iteration 13910 Training loss 0.010545482859015465 Validation loss 0.014461105689406395 Accuracy 0.8489999771118164\n",
      "Iteration 13920 Training loss 0.013182674534618855 Validation loss 0.015223522670567036 Accuracy 0.8407999873161316\n",
      "Iteration 13930 Training loss 0.009593705646693707 Validation loss 0.012891844846308231 Accuracy 0.8650000095367432\n",
      "Iteration 13940 Training loss 0.00811835378408432 Validation loss 0.013013736344873905 Accuracy 0.8650000095367432\n",
      "Iteration 13950 Training loss 0.01115812174975872 Validation loss 0.012872175313532352 Accuracy 0.8651999831199646\n",
      "Iteration 13960 Training loss 0.009130111895501614 Validation loss 0.013262826949357986 Accuracy 0.8614000082015991\n",
      "Iteration 13970 Training loss 0.010150152258574963 Validation loss 0.014077777974307537 Accuracy 0.8529000282287598\n",
      "Iteration 13980 Training loss 0.007105807773768902 Validation loss 0.013115636073052883 Accuracy 0.8639000058174133\n",
      "Iteration 13990 Training loss 0.008087866008281708 Validation loss 0.01275075227022171 Accuracy 0.8672999739646912\n",
      "Iteration 14000 Training loss 0.010135176591575146 Validation loss 0.01299852505326271 Accuracy 0.8640000224113464\n",
      "Iteration 14010 Training loss 0.009557683020830154 Validation loss 0.012883213348686695 Accuracy 0.8650000095367432\n",
      "Iteration 14020 Training loss 0.010632200166583061 Validation loss 0.012924079783260822 Accuracy 0.8640000224113464\n",
      "Iteration 14030 Training loss 0.009183275513350964 Validation loss 0.01271993387490511 Accuracy 0.8668000102043152\n",
      "Iteration 14040 Training loss 0.009380199946463108 Validation loss 0.012687836773693562 Accuracy 0.8672999739646912\n",
      "Iteration 14050 Training loss 0.00849135871976614 Validation loss 0.012966807000339031 Accuracy 0.8650000095367432\n",
      "Iteration 14060 Training loss 0.009002515114843845 Validation loss 0.013086768798530102 Accuracy 0.8628000020980835\n",
      "Iteration 14070 Training loss 0.008861418813467026 Validation loss 0.012734354473650455 Accuracy 0.866599977016449\n",
      "Iteration 14080 Training loss 0.0068453471176326275 Validation loss 0.012955973856151104 Accuracy 0.8647000193595886\n",
      "Iteration 14090 Training loss 0.006919853389263153 Validation loss 0.01271784957498312 Accuracy 0.8682000041007996\n",
      "Iteration 14100 Training loss 0.008808227255940437 Validation loss 0.013424898497760296 Accuracy 0.8604999780654907\n",
      "Iteration 14110 Training loss 0.007293203379958868 Validation loss 0.013029869645833969 Accuracy 0.8637999892234802\n",
      "Iteration 14120 Training loss 0.009237336926162243 Validation loss 0.012609213590621948 Accuracy 0.867900013923645\n",
      "Iteration 14130 Training loss 0.010515905916690826 Validation loss 0.01326716784387827 Accuracy 0.8614000082015991\n",
      "Iteration 14140 Training loss 0.009367472492158413 Validation loss 0.012780877761542797 Accuracy 0.8661999702453613\n",
      "Iteration 14150 Training loss 0.009096633642911911 Validation loss 0.012622734531760216 Accuracy 0.8691999912261963\n",
      "Iteration 14160 Training loss 0.01036001741886139 Validation loss 0.012854743748903275 Accuracy 0.8658000230789185\n",
      "Iteration 14170 Training loss 0.010196560993790627 Validation loss 0.013011554256081581 Accuracy 0.8641999959945679\n",
      "Iteration 14180 Training loss 0.009859634563326836 Validation loss 0.013788663782179356 Accuracy 0.8560000061988831\n",
      "Iteration 14190 Training loss 0.007305957842618227 Validation loss 0.012901319190859795 Accuracy 0.866100013256073\n",
      "Iteration 14200 Training loss 0.007948633283376694 Validation loss 0.012612930499017239 Accuracy 0.8683000206947327\n",
      "Iteration 14210 Training loss 0.009324450977146626 Validation loss 0.013038135133683681 Accuracy 0.8651000261306763\n",
      "Iteration 14220 Training loss 0.008169608190655708 Validation loss 0.012794975191354752 Accuracy 0.8671000003814697\n",
      "Iteration 14230 Training loss 0.008213836699724197 Validation loss 0.013021089136600494 Accuracy 0.8626000285148621\n",
      "Iteration 14240 Training loss 0.008837560191750526 Validation loss 0.012867055833339691 Accuracy 0.8658000230789185\n",
      "Iteration 14250 Training loss 0.006909471470862627 Validation loss 0.012722772546112537 Accuracy 0.8673999905586243\n",
      "Iteration 14260 Training loss 0.009871315211057663 Validation loss 0.012716994620859623 Accuracy 0.8661999702453613\n",
      "Iteration 14270 Training loss 0.008985739201307297 Validation loss 0.012825565412640572 Accuracy 0.8654000163078308\n"
     ]
    }
   ],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 1, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = True\n",
    "date = \"_27_04_25\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = model_3_trained_layer\n",
    "    model_name = \"model_3_layer_trained_save_1\"\n",
    "    save_path = \"Classifiers/\" + model_name + date + \"/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"datas\\models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "tensor(0.8692)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "model_name = \"model_2_layer_trained_save_1\"\n",
    "assessed_model = torch.load(\"datas/models/model_2_layer_save_1.pt\", weights_only=False)\n",
    "\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plots of performances\n",
    "accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(accuracy)\n",
    "kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_accuracy)\n",
    "loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(loss)\n",
    "kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_loss)\n",
    "plt.show()\n",
    "\n",
    "# Import datas\n",
    "accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).to(dtype)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "\n",
    "\n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, esp_init = 1, fraction_batch=0.01):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = esp_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = esp_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # # hidden neurons layer 2\n",
    "        z3 = torch.mm(self.W3, h2.t() + self.b3)\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, coef_iter = 1, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=0.2, fraction_batch=1e-2, train_layer1=True, train_layer2=True, train_layer3=True):\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        kappa_max = 1 + coef_iter\n",
    "        max_iter = self.input_dimension**(kappa_max)\n",
    "        print(\"max_iter\", max_iter)\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_minibatches = int(max_iter / self.fraction_batch) # Nombre de minibatches utilisés pour l'apprentissage de la première couche\n",
    "        for i in range(N_minibatches):\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            # Suivi de l'apprentissage\n",
    "            if i % 100 == 0:\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2, dim=0) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                overall_training_loss = torch.mean(training_loss,dim=0)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2, dim=0)\n",
    "                overall_validation_loss = torch.mean(validation_loss,dim=0)\n",
    "                self.training_loss_trajectory.append(overall_training_loss.item())\n",
    "                self.validation_loss_trajectory.append(overall_validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", overall_training_loss.item(), \"Validation loss\", overall_validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0] # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = torch.mean(grad_z1, dim=0).unsqueeze(1) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0] # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = torch.mean(grad_z2, dim=0).unsqueeze(1)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            self.W1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/self.eps_init**2 + self.reg1*self.W1) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "            self.b1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/self.eps_init**2 + self.reg1*self.b1)\n",
    "            self.W2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/self.eps_init**2 +self.reg2*self.W2)\n",
    "            self.b2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/self.eps_init**2 + self.reg2*self.b2)\n",
    "        return \"Training done\"\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
