{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from utils import mnist_reader\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 18\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 10]) torch.Size([10000, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36000, 784]) torch.Size([36000, 1]) tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]) torch.Size([6000, 784]) torch.Size([6000, 1])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "# Importation des données\n",
    "x_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "x_valid, y_valid = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "x_train, y_train_raw, x_valid, y_valid_raw = torch.tensor(x_train, dtype=dtype), torch.tensor(y_train, dtype=dtype), torch.tensor(x_valid, dtype=dtype), torch.tensor(y_valid,dtype=dtype)\n",
    "\n",
    "# Modification du format des données shape (n_data,1) -> (n_data, n_classes)\n",
    "y_train = torch.zeros(y_train_raw.shape[0], 10)\n",
    "for i,y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros(y_valid_raw.shape[0], 10)\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "print(y_train.shape, y_valid.shape)\n",
    "\n",
    "# Binary reduction of the classes # To avoid using softmax, we regroup classes in two classes\n",
    "class_binary_reduction = True\n",
    "determination_des_classes = True\n",
    "if class_binary_reduction :\n",
    "    if determination_des_classes :\n",
    "        # Determination des classes\n",
    "        class_list = []\n",
    "        class_index = 0\n",
    "        for i in range (x_train.shape[0]):\n",
    "            if y_train[i, class_index] == 1:\n",
    "                class_list.append(x_train[i])\n",
    "                class_index += 1\n",
    "            if len(class_list) == len(y_train[0]):\n",
    "                break\n",
    "            \n",
    "        # For square images\n",
    "        class_list = [x.reshape(int(np.sqrt(len(x))),int(np.sqrt(len(x)))) for x in class_list]\n",
    "        for i, x in enumerate(class_list) :\n",
    "            plt.subplot(2, len(class_list)//2+1, i+1)\n",
    "            plt.imshow(x, cmap='gray')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "            \n",
    "    classe1 = [0, 2, 4]\n",
    "    classe2 = [5, 7, 9]\n",
    "\n",
    "    # Création des masques pour les échantillons appartenant à ces classes\n",
    "    mask_classe1_train = y_train[:, classe1].sum(dim=1) > 0  # True si appartient à classe1\n",
    "    mask_classe2_train = y_train[:, classe2].sum(dim=1) > 0  # True si appartient à classe2\n",
    "    \n",
    "    mask_classe1_valid = y_valid[:, classe1].sum(dim=1) > 0\n",
    "    mask_classe2_valid = y_valid[:, classe2].sum(dim=1) > 0\n",
    "    \n",
    "    # Filtrage des exemples concernés\n",
    "    mask_train = torch.logical_or(mask_classe1_train, mask_classe2_train)\n",
    "    mask_valid = torch.logical_or(mask_classe1_valid, mask_classe2_valid)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid]\n",
    "\n",
    "    # Création du vecteur de labels binaires (1 pour classe1, 0 pour classe2)\n",
    "    y_train = (y_train[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    y_valid = (y_valid[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    \n",
    "    # Avec tanh\n",
    "    \n",
    "    # x_train = 2*(x_train-0.5)\n",
    "    # y_train = 2*(y_train-0.5)\n",
    "    # x_valid = 2*(x_valid-0.5)\n",
    "    # y_valid = 2*(y_valid-0.5)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_train[0:10], x_valid.shape, y_valid.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45060724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on :  mps\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Computing on : \", device)\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0).to(device),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).type(dtype).to(device)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    s = s.to(device)\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return torch.exp(-x)/((1 + torch.exp(-x))**2)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (torch.tanh(x)**2)).to(device)\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output); grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = (torch.einsum('no,noz->nz',grad_output,softmax_derivative(output))).to(dtype) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "\n",
    "class binary_classification_two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(1, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = torch.sigmoid(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            #print(output[0:5], z2[0:5], h1[0:5], z1[0:5])\n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                print(\"Output\", output[20:22])\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = grad_output*sigmoid_derivative(z2); grad_z2  = grad_z2.to(dtype) # shape(n_data, 1)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            if i == 250:\n",
    "                break\n",
    "            # print(\"grad_output\" ,grad_output[0:2,0])            \n",
    "            # print(\"grad_z2\",grad_z2[0:2,0]) \n",
    "            # print(\"grad_h1\",grad_h1[0:2,0])\n",
    "            # print(\"grad_z1\", grad_z1[0:2,0] )\n",
    "            # print(\"x_minibatch\", torch.mean(x_minibatch[0]))\n",
    "            # print(\"grad_W1\",grad_W1[0:2,0])\n",
    "            # print(\"mean W1\", torch.mean(grad_W1))            \n",
    "            # print(\"grad_b1\",grad_b1[0:2,0])\n",
    "            # print(\"grad_W2\",grad_W2[0:2,0])\n",
    "            # print(\"grad_b2\",grad_b2[0:2,0])\n",
    "            # print(\"W1\",self.W1[0:2,0])\n",
    "            # print(\"W2\",self.W2[0:2,0])\n",
    "            # print(\"b1\",self.b1[0:2,0])\n",
    "            # print(\"b2\",self.b2[0:2,0])\n",
    "            # break\n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "class binary_classification_three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(1, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, 1)\n",
    "        output = sigmoid(z3) # output layer # shape (n_data, 1)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = grad_output*sigmoid_derivative(z3).to(dtype) # shape (n_data, 1) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "            \n",
    "            if i == 250:\n",
    "                break\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36584c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2_layer = binary_classification_two_layer_NN(784, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57236d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 61465600.0 and the number of iterations is 170737.\n",
      "Output tensor([[1.0000],\n",
      "        [1.0000]], device='mps:0')\n",
      "Iteration 0 Training loss 0.20281709730625153 Validation loss 0.186473086476326 Accuracy 0.5703333020210266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.9923],\n",
      "        [0.2855]], device='mps:0')\n",
      "Iteration 10 Training loss 0.12413476407527924 Validation loss 0.11930113285779953 Accuracy 0.7108333110809326\n",
      "Output tensor([[0.8593],\n",
      "        [0.4324]], device='mps:0')\n",
      "Iteration 20 Training loss 0.07317319512367249 Validation loss 0.0809139832854271 Accuracy 0.7906666398048401\n",
      "Output tensor([[1.4475e-05],\n",
      "        [8.0679e-01]], device='mps:0')\n",
      "Iteration 30 Training loss 0.05583755671977997 Validation loss 0.0645996704697609 Accuracy 0.82833331823349\n",
      "Output tensor([[9.0023e-01],\n",
      "        [9.7533e-05]], device='mps:0')\n",
      "Iteration 40 Training loss 0.05612010136246681 Validation loss 0.052558429539203644 Accuracy 0.859333336353302\n",
      "Output tensor([[0.9952],\n",
      "        [0.9893]], device='mps:0')\n",
      "Iteration 50 Training loss 0.03668433800339699 Validation loss 0.04470488429069519 Accuracy 0.878333330154419\n",
      "Output tensor([[1.0000],\n",
      "        [0.8596]], device='mps:0')\n",
      "Iteration 60 Training loss 0.042913809418678284 Validation loss 0.03850286453962326 Accuracy 0.8958333134651184\n",
      "Output tensor([[1.0000],\n",
      "        [0.0274]], device='mps:0')\n",
      "Iteration 70 Training loss 0.026939138770103455 Validation loss 0.033964525908231735 Accuracy 0.9079999923706055\n",
      "Output tensor([[1.9275e-04],\n",
      "        [9.6506e-01]], device='mps:0')\n",
      "Iteration 80 Training loss 0.02837701514363289 Validation loss 0.030337486416101456 Accuracy 0.9206666350364685\n",
      "Output tensor([[1.0000],\n",
      "        [1.0000]], device='mps:0')\n",
      "Iteration 90 Training loss 0.026656480506062508 Validation loss 0.027730001136660576 Accuracy 0.9284999966621399\n",
      "Output tensor([[0.7214],\n",
      "        [1.0000]], device='mps:0')\n",
      "Iteration 100 Training loss 0.02574678510427475 Validation loss 0.02543150819838047 Accuracy 0.934499979019165\n",
      "Output tensor([[0.9802],\n",
      "        [0.9720]], device='mps:0')\n",
      "Iteration 110 Training loss 0.03273630142211914 Validation loss 0.023626957088708878 Accuracy 0.9413332939147949\n",
      "Output tensor([[0.4357],\n",
      "        [0.6937]], device='mps:0')\n",
      "Iteration 120 Training loss 0.03403836861252785 Validation loss 0.02231898531317711 Accuracy 0.9438333511352539\n",
      "Output tensor([[1.0000e+00],\n",
      "        [7.3656e-05]], device='mps:0')\n",
      "Iteration 130 Training loss 0.02128453366458416 Validation loss 0.02072998508810997 Accuracy 0.9488333463668823\n",
      "Output tensor([[0.1738],\n",
      "        [0.8730]], device='mps:0')\n",
      "Iteration 140 Training loss 0.02865976095199585 Validation loss 0.019772395491600037 Accuracy 0.9513333439826965\n",
      "Output tensor([[3.7141e-04],\n",
      "        [9.9914e-01]], device='mps:0')\n",
      "Iteration 150 Training loss 0.023854531347751617 Validation loss 0.01875484734773636 Accuracy 0.9528332948684692\n",
      "Output tensor([[0.9986],\n",
      "        [0.0135]], device='mps:0')\n",
      "Iteration 160 Training loss 0.020005658268928528 Validation loss 0.01775236800312996 Accuracy 0.956166684627533\n",
      "Output tensor([[0.0006],\n",
      "        [0.0001]], device='mps:0')\n",
      "Iteration 170 Training loss 0.01867508888244629 Validation loss 0.01700545847415924 Accuracy 0.9576666355133057\n",
      "Output tensor([[0.9922],\n",
      "        [1.0000]], device='mps:0')\n",
      "Iteration 180 Training loss 0.013600713573396206 Validation loss 0.01633819378912449 Accuracy 0.9591666460037231\n",
      "Output tensor([[3.3839e-03],\n",
      "        [4.5822e-05]], device='mps:0')\n",
      "Iteration 190 Training loss 0.016784366220235825 Validation loss 0.015779253095388412 Accuracy 0.9599999785423279\n",
      "Output tensor([[1.0481e-07],\n",
      "        [6.1841e-02]], device='mps:0')\n",
      "Iteration 200 Training loss 0.015755336731672287 Validation loss 0.015149615705013275 Accuracy 0.9618332982063293\n",
      "Output tensor([[9.9717e-01],\n",
      "        [1.8699e-05]], device='mps:0')\n",
      "Iteration 210 Training loss 0.019894251599907875 Validation loss 0.014785475097596645 Accuracy 0.9626666307449341\n",
      "Output tensor([[8.8906e-01],\n",
      "        [4.0397e-06]], device='mps:0')\n",
      "Iteration 220 Training loss 0.018273821100592613 Validation loss 0.014332635328173637 Accuracy 0.9641666412353516\n",
      "Output tensor([[9.9999e-01],\n",
      "        [4.8892e-06]], device='mps:0')\n",
      "Iteration 230 Training loss 0.00966034084558487 Validation loss 0.01376545149832964 Accuracy 0.9651666283607483\n",
      "Output tensor([[0.0016],\n",
      "        [0.9552]], device='mps:0')\n",
      "Iteration 240 Training loss 0.008997898548841476 Validation loss 0.013263329863548279 Accuracy 0.9664999842643738\n",
      "Output tensor([[2.7565e-05],\n",
      "        [1.0000e+00]], device='mps:0')\n",
      "Iteration 250 Training loss 0.010620291344821453 Validation loss 0.012936823070049286 Accuracy 0.9673333168029785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-7, 0, 0, 1e-1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08bfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer = binary_classification_three_layer_NN(784, 512, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c674f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 61465600.0 and the number of iterations is 170737.\n",
      "Iteration 0 Training loss 0.12495290488004684 Validation loss 0.1248823031783104 Accuracy 0.5214999914169312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 Training loss 0.04008433222770691 Validation loss 0.040252674371004105 Accuracy 0.9894999861717224\n",
      "Iteration 20 Training loss 0.013041185215115547 Validation loss 0.010698629543185234 Accuracy 0.9950000047683716\n",
      "Iteration 30 Training loss 0.005574205424636602 Validation loss 0.006406205240637064 Accuracy 0.9965000152587891\n",
      "Iteration 40 Training loss 0.003577133873477578 Validation loss 0.00480898329988122 Accuracy 0.9963333010673523\n",
      "Iteration 50 Training loss 0.0028957505710422993 Validation loss 0.003870221320539713 Accuracy 0.996666669845581\n",
      "Iteration 60 Training loss 0.0036164692137390375 Validation loss 0.003331567160785198 Accuracy 0.996999979019165\n",
      "Iteration 70 Training loss 0.0032030888833105564 Validation loss 0.002939242869615555 Accuracy 0.996999979019165\n",
      "Iteration 80 Training loss 0.0026082745753228664 Validation loss 0.0026421956717967987 Accuracy 0.996999979019165\n",
      "Iteration 90 Training loss 0.003172689350321889 Validation loss 0.002382013713940978 Accuracy 0.9973333477973938\n",
      "Iteration 100 Training loss 0.0045167203061282635 Validation loss 0.002204470569267869 Accuracy 0.997166633605957\n",
      "Iteration 110 Training loss 0.0016995889600366354 Validation loss 0.0020300874020904303 Accuracy 0.9975000023841858\n",
      "Iteration 120 Training loss 0.0029233263339847326 Validation loss 0.0018842731369659305 Accuracy 0.9976666569709778\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbinary_model_3_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 544\u001b[39m, in \u001b[36mbinary_classification_three_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, reg1, reg2, reg3, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2, train_layer_3)\u001b[39m\n\u001b[32m    542\u001b[39m grad_output = (output - y_minibatch).to(dtype)\n\u001b[32m    543\u001b[39m grad_z3 = grad_output*sigmoid_derivative(z3).to(dtype) \u001b[38;5;66;03m# shape (n_data, 1) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m grad_h2 = (\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_z3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW3\u001b[49m\u001b[43m)\u001b[49m).to(dtype) \u001b[38;5;66;03m# shape (n_data, hidden_2_size)\u001b[39;00m\n\u001b[32m    545\u001b[39m grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) \u001b[38;5;66;03m# shape(n_data, hidden_2_size)         \u001b[39;00m\n\u001b[32m    546\u001b[39m grad_h1 = (torch.mm(grad_z2, \u001b[38;5;28mself\u001b[39m.W2)).to(dtype)  \u001b[38;5;66;03m# shape (n_data, hidden_1_size)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "binary_model_3_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(784,2048,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 61465600.0 and the number of iterations is 102442.\n",
      "Iteration 0 Training loss 0.09000594913959503 Validation loss 0.08966609090566635 Accuracy 0.086669921875\n",
      "Iteration 10 Training loss 0.0809607058763504 Validation loss 0.0812806636095047 Accuracy 0.17529296875\n",
      "Iteration 20 Training loss 0.07912186533212662 Validation loss 0.07966550439596176 Accuracy 0.19140625\n",
      "Iteration 30 Training loss 0.07924842089414597 Validation loss 0.07813602685928345 Accuracy 0.208251953125\n",
      "Iteration 40 Training loss 0.07869761437177658 Validation loss 0.07787581533193588 Accuracy 0.2109375\n",
      "Iteration 50 Training loss 0.07642209529876709 Validation loss 0.07636221498250961 Accuracy 0.2259521484375\n",
      "Iteration 60 Training loss 0.07404225319623947 Validation loss 0.07514816522598267 Accuracy 0.24072265625\n",
      "Iteration 70 Training loss 0.07221750915050507 Validation loss 0.07405910640954971 Accuracy 0.251708984375\n",
      "Iteration 80 Training loss 0.07117284834384918 Validation loss 0.07238618284463882 Accuracy 0.268310546875\n",
      "Iteration 90 Training loss 0.07001210749149323 Validation loss 0.07067995518445969 Accuracy 0.28662109375\n",
      "Iteration 100 Training loss 0.07494407147169113 Validation loss 0.07119035720825195 Accuracy 0.28271484375\n",
      "Iteration 110 Training loss 0.06957150995731354 Validation loss 0.07076562941074371 Accuracy 0.287109375\n",
      "Iteration 120 Training loss 0.06395421922206879 Validation loss 0.06799498945474625 Accuracy 0.314697265625\n",
      "Iteration 130 Training loss 0.06471017748117447 Validation loss 0.06379541009664536 Accuracy 0.356201171875\n",
      "Iteration 140 Training loss 0.0601494275033474 Validation loss 0.06309593468904495 Accuracy 0.364501953125\n",
      "Iteration 150 Training loss 0.06413625180721283 Validation loss 0.062430527061223984 Accuracy 0.37060546875\n",
      "Iteration 160 Training loss 0.06511334329843521 Validation loss 0.0632990300655365 Accuracy 0.362548828125\n",
      "Iteration 170 Training loss 0.06479749828577042 Validation loss 0.06403496861457825 Accuracy 0.35546875\n",
      "Iteration 180 Training loss 0.061866678297519684 Validation loss 0.062486432492733 Accuracy 0.370361328125\n",
      "Iteration 190 Training loss 0.06300345063209534 Validation loss 0.061877306550741196 Accuracy 0.375732421875\n",
      "Iteration 200 Training loss 0.06354387104511261 Validation loss 0.062284570187330246 Accuracy 0.373291015625\n",
      "Iteration 210 Training loss 0.05889119580388069 Validation loss 0.06069916486740112 Accuracy 0.38818359375\n",
      "Iteration 220 Training loss 0.06657999753952026 Validation loss 0.06085781008005142 Accuracy 0.38720703125\n",
      "Iteration 230 Training loss 0.05894234776496887 Validation loss 0.06011826917529106 Accuracy 0.394287109375\n",
      "Iteration 240 Training loss 0.05995061248540878 Validation loss 0.06043045595288277 Accuracy 0.391357421875\n",
      "Iteration 250 Training loss 0.05850105732679367 Validation loss 0.05962572991847992 Accuracy 0.398681640625\n",
      "Iteration 260 Training loss 0.058196865022182465 Validation loss 0.05991358309984207 Accuracy 0.39599609375\n",
      "Iteration 270 Training loss 0.06075524911284447 Validation loss 0.06088883802294731 Accuracy 0.386962890625\n",
      "Iteration 280 Training loss 0.05770648643374443 Validation loss 0.05971242114901543 Accuracy 0.399169921875\n",
      "Iteration 290 Training loss 0.05992398038506508 Validation loss 0.05946393311023712 Accuracy 0.400390625\n",
      "Iteration 300 Training loss 0.058211322873830795 Validation loss 0.05886451154947281 Accuracy 0.406982421875\n",
      "Iteration 310 Training loss 0.0597735196352005 Validation loss 0.0593661405146122 Accuracy 0.402587890625\n",
      "Iteration 320 Training loss 0.05600550398230553 Validation loss 0.05907803401350975 Accuracy 0.40478515625\n",
      "Iteration 330 Training loss 0.055961973965168 Validation loss 0.05880654975771904 Accuracy 0.407958984375\n",
      "Iteration 340 Training loss 0.05606001615524292 Validation loss 0.059129320085048676 Accuracy 0.4052734375\n",
      "Iteration 350 Training loss 0.05975470319390297 Validation loss 0.05845730006694794 Accuracy 0.4111328125\n",
      "Iteration 360 Training loss 0.06172051280736923 Validation loss 0.05885596573352814 Accuracy 0.40673828125\n",
      "Iteration 370 Training loss 0.05805492773652077 Validation loss 0.05837656185030937 Accuracy 0.412353515625\n",
      "Iteration 380 Training loss 0.06166232377290726 Validation loss 0.058727771043777466 Accuracy 0.408935546875\n",
      "Iteration 390 Training loss 0.059444598853588104 Validation loss 0.0599256232380867 Accuracy 0.397216796875\n",
      "Iteration 400 Training loss 0.057039275765419006 Validation loss 0.058583471924066544 Accuracy 0.41015625\n",
      "Iteration 410 Training loss 0.055334024131298065 Validation loss 0.05834074690937996 Accuracy 0.4130859375\n",
      "Iteration 420 Training loss 0.05829929932951927 Validation loss 0.05809934809803963 Accuracy 0.41552734375\n",
      "Iteration 430 Training loss 0.055226609110832214 Validation loss 0.05822547525167465 Accuracy 0.413818359375\n",
      "Iteration 440 Training loss 0.05624333769083023 Validation loss 0.05825023353099823 Accuracy 0.4140625\n",
      "Iteration 450 Training loss 0.05793467536568642 Validation loss 0.05806450918316841 Accuracy 0.41552734375\n",
      "Iteration 460 Training loss 0.060604460537433624 Validation loss 0.0603061206638813 Accuracy 0.392578125\n",
      "Iteration 470 Training loss 0.05576633661985397 Validation loss 0.058654189109802246 Accuracy 0.409912109375\n",
      "Iteration 480 Training loss 0.057547010481357574 Validation loss 0.057696640491485596 Accuracy 0.41943359375\n",
      "Iteration 490 Training loss 0.05666466802358627 Validation loss 0.05780808627605438 Accuracy 0.41796875\n",
      "Iteration 500 Training loss 0.057327840477228165 Validation loss 0.05854593962430954 Accuracy 0.411376953125\n",
      "Iteration 510 Training loss 0.05771595612168312 Validation loss 0.05896542966365814 Accuracy 0.406982421875\n",
      "Iteration 520 Training loss 0.055225566029548645 Validation loss 0.057512350380420685 Accuracy 0.4208984375\n",
      "Iteration 530 Training loss 0.05418699234724045 Validation loss 0.057485681027173996 Accuracy 0.42138671875\n",
      "Iteration 540 Training loss 0.058718226850032806 Validation loss 0.05758391320705414 Accuracy 0.420654296875\n",
      "Iteration 550 Training loss 0.055817559361457825 Validation loss 0.05793629214167595 Accuracy 0.4169921875\n",
      "Iteration 560 Training loss 0.056998856365680695 Validation loss 0.057710912078619 Accuracy 0.418701171875\n",
      "Iteration 570 Training loss 0.05623771995306015 Validation loss 0.05768042430281639 Accuracy 0.418701171875\n",
      "Iteration 580 Training loss 0.058195557445287704 Validation loss 0.05817696452140808 Accuracy 0.41455078125\n",
      "Iteration 590 Training loss 0.0574512742459774 Validation loss 0.05794534087181091 Accuracy 0.4169921875\n",
      "Iteration 600 Training loss 0.05533440038561821 Validation loss 0.057346414774656296 Accuracy 0.4228515625\n",
      "Iteration 610 Training loss 0.056626301258802414 Validation loss 0.05760525166988373 Accuracy 0.42041015625\n",
      "Iteration 620 Training loss 0.05809423699975014 Validation loss 0.05726359784603119 Accuracy 0.423095703125\n",
      "Iteration 630 Training loss 0.055786050856113434 Validation loss 0.05715867131948471 Accuracy 0.42431640625\n",
      "Iteration 640 Training loss 0.05681564286351204 Validation loss 0.057295817881822586 Accuracy 0.4228515625\n",
      "Iteration 650 Training loss 0.05564839765429497 Validation loss 0.05724968761205673 Accuracy 0.423583984375\n",
      "Iteration 660 Training loss 0.05630582198500633 Validation loss 0.058512136340141296 Accuracy 0.411865234375\n",
      "Iteration 670 Training loss 0.05507215857505798 Validation loss 0.05712433531880379 Accuracy 0.4248046875\n",
      "Iteration 680 Training loss 0.0548098050057888 Validation loss 0.05743345618247986 Accuracy 0.422119140625\n",
      "Iteration 690 Training loss 0.05348016321659088 Validation loss 0.0582183301448822 Accuracy 0.413818359375\n",
      "Iteration 700 Training loss 0.05838795378804207 Validation loss 0.05735453963279724 Accuracy 0.423095703125\n",
      "Iteration 710 Training loss 0.05253516510128975 Validation loss 0.05683182552456856 Accuracy 0.4287109375\n",
      "Iteration 720 Training loss 0.05574605241417885 Validation loss 0.05700826644897461 Accuracy 0.426513671875\n",
      "Iteration 730 Training loss 0.05493118241429329 Validation loss 0.05686434358358383 Accuracy 0.42724609375\n",
      "Iteration 740 Training loss 0.058842889964580536 Validation loss 0.05721501260995865 Accuracy 0.4248046875\n",
      "Iteration 750 Training loss 0.05635467916727066 Validation loss 0.05793489143252373 Accuracy 0.417724609375\n",
      "Iteration 760 Training loss 0.05363029986619949 Validation loss 0.05684973672032356 Accuracy 0.42724609375\n",
      "Iteration 770 Training loss 0.05519244819879532 Validation loss 0.05669492110610008 Accuracy 0.429443359375\n",
      "Iteration 780 Training loss 0.05414879322052002 Validation loss 0.05707843229174614 Accuracy 0.425537109375\n",
      "Iteration 790 Training loss 0.05780107527971268 Validation loss 0.05731058493256569 Accuracy 0.4228515625\n",
      "Iteration 800 Training loss 0.054711658507585526 Validation loss 0.056728798896074295 Accuracy 0.4287109375\n",
      "Iteration 810 Training loss 0.058338407427072525 Validation loss 0.057401515543460846 Accuracy 0.42236328125\n",
      "Iteration 820 Training loss 0.05385717749595642 Validation loss 0.05711491033434868 Accuracy 0.4248046875\n",
      "Iteration 830 Training loss 0.057334981858730316 Validation loss 0.05663083866238594 Accuracy 0.43017578125\n",
      "Iteration 840 Training loss 0.05830051749944687 Validation loss 0.05816200003027916 Accuracy 0.4150390625\n",
      "Iteration 850 Training loss 0.05475854501128197 Validation loss 0.05649714544415474 Accuracy 0.431396484375\n",
      "Iteration 860 Training loss 0.05395885184407234 Validation loss 0.056670162826776505 Accuracy 0.429931640625\n",
      "Iteration 870 Training loss 0.052751556038856506 Validation loss 0.056267376989126205 Accuracy 0.433837890625\n",
      "Iteration 880 Training loss 0.054656412452459335 Validation loss 0.05642785504460335 Accuracy 0.432373046875\n",
      "Iteration 890 Training loss 0.05779882147908211 Validation loss 0.057197682559490204 Accuracy 0.42431640625\n",
      "Iteration 900 Training loss 0.05517015606164932 Validation loss 0.0566839762032032 Accuracy 0.42919921875\n",
      "Iteration 910 Training loss 0.05863066390156746 Validation loss 0.057088982313871384 Accuracy 0.425537109375\n",
      "Iteration 920 Training loss 0.057627175003290176 Validation loss 0.05622250959277153 Accuracy 0.43408203125\n",
      "Iteration 930 Training loss 0.056076034903526306 Validation loss 0.056324850767850876 Accuracy 0.432861328125\n",
      "Iteration 940 Training loss 0.05550359562039375 Validation loss 0.05625298619270325 Accuracy 0.434326171875\n",
      "Iteration 950 Training loss 0.05484713613986969 Validation loss 0.05634511634707451 Accuracy 0.433349609375\n",
      "Iteration 960 Training loss 0.057661645114421844 Validation loss 0.058396030217409134 Accuracy 0.41259765625\n",
      "Iteration 970 Training loss 0.05070656165480614 Validation loss 0.05620064213871956 Accuracy 0.4345703125\n",
      "Iteration 980 Training loss 0.0601193942129612 Validation loss 0.06104360520839691 Accuracy 0.386474609375\n",
      "Iteration 990 Training loss 0.05482439696788788 Validation loss 0.0565391406416893 Accuracy 0.431640625\n",
      "Iteration 1000 Training loss 0.056299883872270584 Validation loss 0.056757330894470215 Accuracy 0.4296875\n",
      "Iteration 1010 Training loss 0.05957414582371712 Validation loss 0.05710960924625397 Accuracy 0.425537109375\n",
      "Iteration 1020 Training loss 0.05469314008951187 Validation loss 0.05619310587644577 Accuracy 0.434326171875\n",
      "Iteration 1030 Training loss 0.05591033399105072 Validation loss 0.05740780010819435 Accuracy 0.42236328125\n",
      "Iteration 1040 Training loss 0.05317080765962601 Validation loss 0.05691983178257942 Accuracy 0.42724609375\n",
      "Iteration 1050 Training loss 0.054880715906620026 Validation loss 0.05677061900496483 Accuracy 0.428955078125\n",
      "Iteration 1060 Training loss 0.05564083904027939 Validation loss 0.056814003735780716 Accuracy 0.4287109375\n",
      "Iteration 1070 Training loss 0.05831969156861305 Validation loss 0.056450892239809036 Accuracy 0.432373046875\n",
      "Iteration 1080 Training loss 0.05694722756743431 Validation loss 0.05631284415721893 Accuracy 0.4326171875\n",
      "Iteration 1090 Training loss 0.05758979916572571 Validation loss 0.05645236745476723 Accuracy 0.431884765625\n",
      "Iteration 1100 Training loss 0.05697954073548317 Validation loss 0.05638374760746956 Accuracy 0.432861328125\n",
      "Iteration 1110 Training loss 0.05839000269770622 Validation loss 0.05603562295436859 Accuracy 0.436279296875\n",
      "Iteration 1120 Training loss 0.05526873096823692 Validation loss 0.056021418422460556 Accuracy 0.43603515625\n",
      "Iteration 1130 Training loss 0.053887613117694855 Validation loss 0.05687262490391731 Accuracy 0.42724609375\n",
      "Iteration 1140 Training loss 0.0538899265229702 Validation loss 0.05617901310324669 Accuracy 0.434814453125\n",
      "Iteration 1150 Training loss 0.054190244525671005 Validation loss 0.056721072643995285 Accuracy 0.429443359375\n",
      "Iteration 1160 Training loss 0.0548427440226078 Validation loss 0.05693493038415909 Accuracy 0.42724609375\n",
      "Iteration 1170 Training loss 0.052808959037065506 Validation loss 0.05617551505565643 Accuracy 0.434814453125\n",
      "Iteration 1180 Training loss 0.05670967698097229 Validation loss 0.056158702820539474 Accuracy 0.435546875\n",
      "Iteration 1190 Training loss 0.05440656468272209 Validation loss 0.056161969900131226 Accuracy 0.435546875\n",
      "Iteration 1200 Training loss 0.054937876760959625 Validation loss 0.05602346360683441 Accuracy 0.436767578125\n",
      "Iteration 1210 Training loss 0.05494062229990959 Validation loss 0.0558931902050972 Accuracy 0.437744140625\n",
      "Iteration 1220 Training loss 0.05636204406619072 Validation loss 0.05593850463628769 Accuracy 0.43701171875\n",
      "Iteration 1230 Training loss 0.056467242538928986 Validation loss 0.05748506262898445 Accuracy 0.42236328125\n",
      "Iteration 1240 Training loss 0.0531596876680851 Validation loss 0.056315530091524124 Accuracy 0.43359375\n",
      "Iteration 1250 Training loss 0.0550035685300827 Validation loss 0.05598771944642067 Accuracy 0.436279296875\n",
      "Iteration 1260 Training loss 0.05325740575790405 Validation loss 0.056142956018447876 Accuracy 0.435791015625\n",
      "Iteration 1270 Training loss 0.059021372348070145 Validation loss 0.056031011044979095 Accuracy 0.436767578125\n",
      "Iteration 1280 Training loss 0.05253477022051811 Validation loss 0.05614440143108368 Accuracy 0.434814453125\n",
      "Iteration 1290 Training loss 0.05889345332980156 Validation loss 0.05943479761481285 Accuracy 0.40234375\n",
      "Iteration 1300 Training loss 0.056524697691202164 Validation loss 0.05609450489282608 Accuracy 0.435302734375\n",
      "Iteration 1310 Training loss 0.05523001030087471 Validation loss 0.05711335316300392 Accuracy 0.424560546875\n",
      "Iteration 1320 Training loss 0.0562821663916111 Validation loss 0.056208476424217224 Accuracy 0.43408203125\n",
      "Iteration 1330 Training loss 0.053925588726997375 Validation loss 0.05617411807179451 Accuracy 0.434814453125\n",
      "Iteration 1340 Training loss 0.052557963877916336 Validation loss 0.056635912507772446 Accuracy 0.429443359375\n",
      "Iteration 1350 Training loss 0.055017150938510895 Validation loss 0.0559723898768425 Accuracy 0.4365234375\n",
      "Iteration 1360 Training loss 0.05560119450092316 Validation loss 0.0562715120613575 Accuracy 0.43408203125\n",
      "Iteration 1370 Training loss 0.054096680134534836 Validation loss 0.05653829500079155 Accuracy 0.43115234375\n",
      "Iteration 1380 Training loss 0.05399470403790474 Validation loss 0.05629787966609001 Accuracy 0.4326171875\n",
      "Iteration 1390 Training loss 0.05994100496172905 Validation loss 0.05633864179253578 Accuracy 0.4326171875\n",
      "Iteration 1400 Training loss 0.05462457239627838 Validation loss 0.05661044642329216 Accuracy 0.430419921875\n",
      "Iteration 1410 Training loss 0.05423545092344284 Validation loss 0.05675261467695236 Accuracy 0.4287109375\n",
      "Iteration 1420 Training loss 0.051948901265859604 Validation loss 0.05602291226387024 Accuracy 0.43603515625\n",
      "Iteration 1430 Training loss 0.05424203351140022 Validation loss 0.055845774710178375 Accuracy 0.438232421875\n",
      "Iteration 1440 Training loss 0.05523017793893814 Validation loss 0.056634534150362015 Accuracy 0.429443359375\n",
      "Iteration 1450 Training loss 0.05914301052689552 Validation loss 0.05783369392156601 Accuracy 0.418701171875\n",
      "Iteration 1460 Training loss 0.05373886600136757 Validation loss 0.05604849383234978 Accuracy 0.43603515625\n",
      "Iteration 1470 Training loss 0.05567055195569992 Validation loss 0.05587141588330269 Accuracy 0.438232421875\n",
      "Iteration 1480 Training loss 0.05110376328229904 Validation loss 0.05582226440310478 Accuracy 0.43798828125\n",
      "Iteration 1490 Training loss 0.0582784004509449 Validation loss 0.055926766246557236 Accuracy 0.436767578125\n",
      "Iteration 1500 Training loss 0.053847961127758026 Validation loss 0.05616815388202667 Accuracy 0.434814453125\n",
      "Iteration 1510 Training loss 0.05295248329639435 Validation loss 0.05593816190958023 Accuracy 0.437255859375\n",
      "Iteration 1520 Training loss 0.055695246905088425 Validation loss 0.055794086307287216 Accuracy 0.439208984375\n",
      "Iteration 1530 Training loss 0.0545334555208683 Validation loss 0.05612786114215851 Accuracy 0.435302734375\n",
      "Iteration 1540 Training loss 0.05402250215411186 Validation loss 0.05634767562150955 Accuracy 0.432861328125\n",
      "Iteration 1550 Training loss 0.054815102368593216 Validation loss 0.05579577013850212 Accuracy 0.438720703125\n",
      "Iteration 1560 Training loss 0.05552435666322708 Validation loss 0.056037478148937225 Accuracy 0.436279296875\n",
      "Iteration 1570 Training loss 0.05616259574890137 Validation loss 0.05632129684090614 Accuracy 0.433349609375\n",
      "Iteration 1580 Training loss 0.05786454305052757 Validation loss 0.05574069172143936 Accuracy 0.439208984375\n",
      "Iteration 1590 Training loss 0.057023629546165466 Validation loss 0.05641495808959007 Accuracy 0.431884765625\n",
      "Iteration 1600 Training loss 0.05585656687617302 Validation loss 0.056125082075595856 Accuracy 0.435302734375\n",
      "Iteration 1610 Training loss 0.05020789057016373 Validation loss 0.05590057373046875 Accuracy 0.43701171875\n",
      "Iteration 1620 Training loss 0.05491766333580017 Validation loss 0.05589596927165985 Accuracy 0.43701171875\n",
      "Iteration 1630 Training loss 0.05351031571626663 Validation loss 0.05606268346309662 Accuracy 0.435302734375\n",
      "Iteration 1640 Training loss 0.053204212337732315 Validation loss 0.05602768063545227 Accuracy 0.435546875\n",
      "Iteration 1650 Training loss 0.05437985062599182 Validation loss 0.055933788418769836 Accuracy 0.436767578125\n",
      "Iteration 1660 Training loss 0.05514714866876602 Validation loss 0.05635802447795868 Accuracy 0.431884765625\n",
      "Iteration 1670 Training loss 0.053133439272642136 Validation loss 0.05559687316417694 Accuracy 0.439697265625\n",
      "Iteration 1680 Training loss 0.055626560002565384 Validation loss 0.055984314531087875 Accuracy 0.4375\n",
      "Iteration 1690 Training loss 0.0522211417555809 Validation loss 0.056021805852651596 Accuracy 0.435302734375\n",
      "Iteration 1700 Training loss 0.053758494555950165 Validation loss 0.056053679436445236 Accuracy 0.435546875\n",
      "Iteration 1710 Training loss 0.05575559288263321 Validation loss 0.05598585680127144 Accuracy 0.43603515625\n",
      "Iteration 1720 Training loss 0.05655151233077049 Validation loss 0.05657258629798889 Accuracy 0.430419921875\n",
      "Iteration 1730 Training loss 0.05341937020421028 Validation loss 0.05555596575140953 Accuracy 0.44091796875\n",
      "Iteration 1740 Training loss 0.055015482008457184 Validation loss 0.055805642157793045 Accuracy 0.43798828125\n",
      "Iteration 1750 Training loss 0.055177707225084305 Validation loss 0.05580655112862587 Accuracy 0.4384765625\n",
      "Iteration 1760 Training loss 0.050618864595890045 Validation loss 0.05548302084207535 Accuracy 0.44189453125\n",
      "Iteration 1770 Training loss 0.05717183277010918 Validation loss 0.05623805522918701 Accuracy 0.43408203125\n",
      "Iteration 1780 Training loss 0.05403819680213928 Validation loss 0.055749211460351944 Accuracy 0.439208984375\n",
      "Iteration 1790 Training loss 0.05549997836351395 Validation loss 0.05605060234665871 Accuracy 0.435546875\n",
      "Iteration 1800 Training loss 0.05383343622088432 Validation loss 0.055896513164043427 Accuracy 0.4375\n",
      "Iteration 1810 Training loss 0.05512334033846855 Validation loss 0.05561721697449684 Accuracy 0.440185546875\n",
      "Iteration 1820 Training loss 0.05936837196350098 Validation loss 0.05615588650107384 Accuracy 0.4345703125\n",
      "Iteration 1830 Training loss 0.0551384761929512 Validation loss 0.05563140660524368 Accuracy 0.440185546875\n",
      "Iteration 1840 Training loss 0.050277262926101685 Validation loss 0.05570889636874199 Accuracy 0.439208984375\n",
      "Iteration 1850 Training loss 0.05473218858242035 Validation loss 0.05607641488313675 Accuracy 0.435791015625\n",
      "Iteration 1860 Training loss 0.05354836955666542 Validation loss 0.05698851868510246 Accuracy 0.4267578125\n",
      "Iteration 1870 Training loss 0.05510256811976433 Validation loss 0.05607173591852188 Accuracy 0.435791015625\n",
      "Iteration 1880 Training loss 0.05895790457725525 Validation loss 0.05553838610649109 Accuracy 0.44140625\n",
      "Iteration 1890 Training loss 0.05334990844130516 Validation loss 0.05589476600289345 Accuracy 0.4375\n",
      "Iteration 1900 Training loss 0.0564095564186573 Validation loss 0.05601826682686806 Accuracy 0.43603515625\n",
      "Iteration 1910 Training loss 0.054547250270843506 Validation loss 0.056554120033979416 Accuracy 0.43115234375\n",
      "Iteration 1920 Training loss 0.05270685628056526 Validation loss 0.05568726733326912 Accuracy 0.439453125\n",
      "Iteration 1930 Training loss 0.057338275015354156 Validation loss 0.055717434734106064 Accuracy 0.439208984375\n",
      "Iteration 1940 Training loss 0.05048704892396927 Validation loss 0.05558428540825844 Accuracy 0.440673828125\n",
      "Iteration 1950 Training loss 0.05260974541306496 Validation loss 0.05554310977458954 Accuracy 0.44140625\n",
      "Iteration 1960 Training loss 0.05786190181970596 Validation loss 0.05676593631505966 Accuracy 0.4287109375\n",
      "Iteration 1970 Training loss 0.05512741208076477 Validation loss 0.055578261613845825 Accuracy 0.44091796875\n",
      "Iteration 1980 Training loss 0.05744381621479988 Validation loss 0.05566271394491196 Accuracy 0.44091796875\n",
      "Iteration 1990 Training loss 0.05480897054076195 Validation loss 0.0555434413254261 Accuracy 0.44140625\n",
      "Iteration 2000 Training loss 0.056116484105587006 Validation loss 0.05574031546711922 Accuracy 0.438720703125\n",
      "Iteration 2010 Training loss 0.054855916649103165 Validation loss 0.05578872933983803 Accuracy 0.43798828125\n",
      "Iteration 2020 Training loss 0.05645781755447388 Validation loss 0.05584295094013214 Accuracy 0.43798828125\n",
      "Iteration 2030 Training loss 0.05573026463389397 Validation loss 0.05544253811240196 Accuracy 0.44140625\n",
      "Iteration 2040 Training loss 0.05582543835043907 Validation loss 0.05607239156961441 Accuracy 0.435546875\n",
      "Iteration 2050 Training loss 0.05297986418008804 Validation loss 0.05675533041357994 Accuracy 0.428466796875\n",
      "Iteration 2060 Training loss 0.05692075565457344 Validation loss 0.05573964864015579 Accuracy 0.438720703125\n",
      "Iteration 2070 Training loss 0.056462086737155914 Validation loss 0.05559471622109413 Accuracy 0.43994140625\n",
      "Iteration 2080 Training loss 0.0527748204767704 Validation loss 0.055594321340322495 Accuracy 0.4404296875\n",
      "Iteration 2090 Training loss 0.056428212672472 Validation loss 0.055419325828552246 Accuracy 0.442138671875\n",
      "Iteration 2100 Training loss 0.054062340408563614 Validation loss 0.05728907883167267 Accuracy 0.423583984375\n",
      "Iteration 2110 Training loss 0.05664445832371712 Validation loss 0.055371951311826706 Accuracy 0.442626953125\n",
      "Iteration 2120 Training loss 0.05509568750858307 Validation loss 0.05540315806865692 Accuracy 0.442626953125\n",
      "Iteration 2130 Training loss 0.051866598427295685 Validation loss 0.05549757555127144 Accuracy 0.44189453125\n",
      "Iteration 2140 Training loss 0.0569482296705246 Validation loss 0.0574008971452713 Accuracy 0.4228515625\n",
      "Iteration 2150 Training loss 0.05596836283802986 Validation loss 0.05563020333647728 Accuracy 0.44091796875\n",
      "Iteration 2160 Training loss 0.05771949887275696 Validation loss 0.055539071559906006 Accuracy 0.441650390625\n",
      "Iteration 2170 Training loss 0.05386282876133919 Validation loss 0.05541913956403732 Accuracy 0.44189453125\n",
      "Iteration 2180 Training loss 0.055134184658527374 Validation loss 0.05528877675533295 Accuracy 0.44384765625\n",
      "Iteration 2190 Training loss 0.05566719174385071 Validation loss 0.056032899767160416 Accuracy 0.43603515625\n",
      "Iteration 2200 Training loss 0.05308275669813156 Validation loss 0.05586561560630798 Accuracy 0.43798828125\n",
      "Iteration 2210 Training loss 0.05159931257367134 Validation loss 0.055650848895311356 Accuracy 0.440185546875\n",
      "Iteration 2220 Training loss 0.0558817982673645 Validation loss 0.05562595650553703 Accuracy 0.440673828125\n",
      "Iteration 2230 Training loss 0.054358579218387604 Validation loss 0.05546651780605316 Accuracy 0.4423828125\n",
      "Iteration 2240 Training loss 0.05334426835179329 Validation loss 0.055386364459991455 Accuracy 0.4423828125\n",
      "Iteration 2250 Training loss 0.053556155413389206 Validation loss 0.05733543261885643 Accuracy 0.423095703125\n",
      "Iteration 2260 Training loss 0.05650768429040909 Validation loss 0.055774979293346405 Accuracy 0.43896484375\n",
      "Iteration 2270 Training loss 0.05347605422139168 Validation loss 0.05631188303232193 Accuracy 0.43359375\n",
      "Iteration 2280 Training loss 0.05460161343216896 Validation loss 0.05544112250208855 Accuracy 0.442138671875\n",
      "Iteration 2290 Training loss 0.052546653896570206 Validation loss 0.05545056611299515 Accuracy 0.44189453125\n",
      "Iteration 2300 Training loss 0.058198943734169006 Validation loss 0.055812787264585495 Accuracy 0.438720703125\n",
      "Iteration 2310 Training loss 0.056951720267534256 Validation loss 0.05549517273902893 Accuracy 0.44140625\n",
      "Iteration 2320 Training loss 0.05492981895804405 Validation loss 0.055641308426856995 Accuracy 0.43994140625\n",
      "Iteration 2330 Training loss 0.05687670782208443 Validation loss 0.05602514371275902 Accuracy 0.435791015625\n",
      "Iteration 2340 Training loss 0.05464712530374527 Validation loss 0.056045155972242355 Accuracy 0.435791015625\n",
      "Iteration 2350 Training loss 0.05690309405326843 Validation loss 0.05636397376656532 Accuracy 0.432861328125\n",
      "Iteration 2360 Training loss 0.0567169226706028 Validation loss 0.055638112127780914 Accuracy 0.43994140625\n",
      "Iteration 2370 Training loss 0.05432672053575516 Validation loss 0.05553879216313362 Accuracy 0.44091796875\n",
      "Iteration 2380 Training loss 0.05679735168814659 Validation loss 0.05531506612896919 Accuracy 0.443359375\n",
      "Iteration 2390 Training loss 0.0564250573515892 Validation loss 0.05586135759949684 Accuracy 0.4375\n",
      "Iteration 2400 Training loss 0.05604639649391174 Validation loss 0.055469006299972534 Accuracy 0.441650390625\n",
      "Iteration 2410 Training loss 0.058024290949106216 Validation loss 0.055591702461242676 Accuracy 0.440673828125\n",
      "Iteration 2420 Training loss 0.054434601217508316 Validation loss 0.05581323057413101 Accuracy 0.438232421875\n",
      "Iteration 2430 Training loss 0.0523732453584671 Validation loss 0.05672283098101616 Accuracy 0.42919921875\n",
      "Iteration 2440 Training loss 0.05256914719939232 Validation loss 0.055489618331193924 Accuracy 0.44091796875\n",
      "Iteration 2450 Training loss 0.05226423218846321 Validation loss 0.055787160992622375 Accuracy 0.4384765625\n",
      "Iteration 2460 Training loss 0.05665407329797745 Validation loss 0.05521410331130028 Accuracy 0.444091796875\n",
      "Iteration 2470 Training loss 0.05186011642217636 Validation loss 0.05540844425559044 Accuracy 0.442626953125\n",
      "Iteration 2480 Training loss 0.056549880653619766 Validation loss 0.055770859122276306 Accuracy 0.438232421875\n",
      "Iteration 2490 Training loss 0.05592994764447212 Validation loss 0.05557125806808472 Accuracy 0.440673828125\n",
      "Iteration 2500 Training loss 0.05482598394155502 Validation loss 0.05563387647271156 Accuracy 0.439697265625\n",
      "Iteration 2510 Training loss 0.05316796898841858 Validation loss 0.05543040484189987 Accuracy 0.44189453125\n",
      "Iteration 2520 Training loss 0.05363944545388222 Validation loss 0.05627980828285217 Accuracy 0.43310546875\n",
      "Iteration 2530 Training loss 0.05557573214173317 Validation loss 0.05541189759969711 Accuracy 0.44189453125\n",
      "Iteration 2540 Training loss 0.05430728942155838 Validation loss 0.055602408945560455 Accuracy 0.440185546875\n",
      "Iteration 2550 Training loss 0.05291293188929558 Validation loss 0.05536595731973648 Accuracy 0.44189453125\n",
      "Iteration 2560 Training loss 0.05363288149237633 Validation loss 0.055490441620349884 Accuracy 0.44140625\n",
      "Iteration 2570 Training loss 0.05055364966392517 Validation loss 0.05548311397433281 Accuracy 0.44140625\n",
      "Iteration 2580 Training loss 0.05913901701569557 Validation loss 0.056046731770038605 Accuracy 0.435791015625\n",
      "Iteration 2590 Training loss 0.055558521300554276 Validation loss 0.055452194064855576 Accuracy 0.441650390625\n",
      "Iteration 2600 Training loss 0.05366489663720131 Validation loss 0.05574197694659233 Accuracy 0.438232421875\n",
      "Iteration 2610 Training loss 0.053743280470371246 Validation loss 0.05542333796620369 Accuracy 0.4423828125\n",
      "Iteration 2620 Training loss 0.05807714909315109 Validation loss 0.05537237226963043 Accuracy 0.4423828125\n",
      "Iteration 2630 Training loss 0.052074071019887924 Validation loss 0.055418651551008224 Accuracy 0.441650390625\n",
      "Iteration 2640 Training loss 0.05491055175662041 Validation loss 0.055260781198740005 Accuracy 0.443603515625\n",
      "Iteration 2650 Training loss 0.054141998291015625 Validation loss 0.0555984266102314 Accuracy 0.43994140625\n",
      "Iteration 2660 Training loss 0.05414554849267006 Validation loss 0.056684158742427826 Accuracy 0.4287109375\n",
      "Iteration 2670 Training loss 0.05228813737630844 Validation loss 0.055992353707551956 Accuracy 0.43603515625\n",
      "Iteration 2680 Training loss 0.050199270248413086 Validation loss 0.055479228496551514 Accuracy 0.44091796875\n",
      "Iteration 2690 Training loss 0.053364817053079605 Validation loss 0.05536385625600815 Accuracy 0.4423828125\n",
      "Iteration 2700 Training loss 0.053202152252197266 Validation loss 0.05529257282614708 Accuracy 0.44287109375\n",
      "Iteration 2710 Training loss 0.05123639851808548 Validation loss 0.05525650084018707 Accuracy 0.443603515625\n",
      "Iteration 2720 Training loss 0.05524304136633873 Validation loss 0.055984560400247574 Accuracy 0.4365234375\n",
      "Iteration 2730 Training loss 0.05534055456519127 Validation loss 0.05532010644674301 Accuracy 0.442626953125\n",
      "Iteration 2740 Training loss 0.053283028304576874 Validation loss 0.055217817425727844 Accuracy 0.4443359375\n",
      "Iteration 2750 Training loss 0.05380893871188164 Validation loss 0.05547722056508064 Accuracy 0.44140625\n",
      "Iteration 2760 Training loss 0.05581577494740486 Validation loss 0.05527288839221001 Accuracy 0.443603515625\n",
      "Iteration 2770 Training loss 0.058788929134607315 Validation loss 0.05520627647638321 Accuracy 0.44384765625\n",
      "Iteration 2780 Training loss 0.05212290957570076 Validation loss 0.05542150139808655 Accuracy 0.442138671875\n",
      "Iteration 2790 Training loss 0.05466032773256302 Validation loss 0.05563995614647865 Accuracy 0.440673828125\n",
      "Iteration 2800 Training loss 0.0542152002453804 Validation loss 0.05532145872712135 Accuracy 0.443359375\n",
      "Iteration 2810 Training loss 0.05051513761281967 Validation loss 0.055552881211042404 Accuracy 0.44140625\n",
      "Iteration 2820 Training loss 0.052442148327827454 Validation loss 0.055378396064043045 Accuracy 0.4423828125\n",
      "Iteration 2830 Training loss 0.05589602515101433 Validation loss 0.05524903163313866 Accuracy 0.44384765625\n",
      "Iteration 2840 Training loss 0.05452646687626839 Validation loss 0.05580182373523712 Accuracy 0.4384765625\n",
      "Iteration 2850 Training loss 0.0524100735783577 Validation loss 0.05606440454721451 Accuracy 0.435546875\n",
      "Iteration 2860 Training loss 0.05290542170405388 Validation loss 0.055289413779973984 Accuracy 0.443603515625\n",
      "Iteration 2870 Training loss 0.05314702168107033 Validation loss 0.05567234382033348 Accuracy 0.440185546875\n",
      "Iteration 2880 Training loss 0.05155322700738907 Validation loss 0.05521136522293091 Accuracy 0.444580078125\n",
      "Iteration 2890 Training loss 0.05905414745211601 Validation loss 0.05625588074326515 Accuracy 0.43408203125\n",
      "Iteration 2900 Training loss 0.054345615208148956 Validation loss 0.055413611233234406 Accuracy 0.441650390625\n",
      "Iteration 2910 Training loss 0.053914789110422134 Validation loss 0.05540258064866066 Accuracy 0.4423828125\n",
      "Iteration 2920 Training loss 0.05317343771457672 Validation loss 0.05544811859726906 Accuracy 0.44140625\n",
      "Iteration 2930 Training loss 0.05257085710763931 Validation loss 0.05598088726401329 Accuracy 0.4365234375\n",
      "Iteration 2940 Training loss 0.05705123767256737 Validation loss 0.05516621470451355 Accuracy 0.444580078125\n",
      "Iteration 2950 Training loss 0.05162007734179497 Validation loss 0.05550156906247139 Accuracy 0.441162109375\n",
      "Iteration 2960 Training loss 0.05540257319808006 Validation loss 0.05581759661436081 Accuracy 0.438232421875\n",
      "Iteration 2970 Training loss 0.05512542650103569 Validation loss 0.055345579981803894 Accuracy 0.4423828125\n",
      "Iteration 2980 Training loss 0.05307076871395111 Validation loss 0.055914685130119324 Accuracy 0.43701171875\n",
      "Iteration 2990 Training loss 0.053760405629873276 Validation loss 0.05559220537543297 Accuracy 0.440185546875\n",
      "Iteration 3000 Training loss 0.05741957575082779 Validation loss 0.05606560409069061 Accuracy 0.435791015625\n",
      "Iteration 3010 Training loss 0.05240974947810173 Validation loss 0.055235058069229126 Accuracy 0.443359375\n",
      "Iteration 3020 Training loss 0.05584951862692833 Validation loss 0.05552097409963608 Accuracy 0.44140625\n",
      "Iteration 3030 Training loss 0.05507643520832062 Validation loss 0.0551772341132164 Accuracy 0.4443359375\n",
      "Iteration 3040 Training loss 0.05060931295156479 Validation loss 0.0552624873816967 Accuracy 0.443115234375\n",
      "Iteration 3050 Training loss 0.05312315374612808 Validation loss 0.05545676499605179 Accuracy 0.441650390625\n",
      "Iteration 3060 Training loss 0.05460461229085922 Validation loss 0.05594349652528763 Accuracy 0.43701171875\n",
      "Iteration 3070 Training loss 0.05218222364783287 Validation loss 0.055172547698020935 Accuracy 0.4443359375\n",
      "Iteration 3080 Training loss 0.053105734288692474 Validation loss 0.05514642596244812 Accuracy 0.44482421875\n",
      "Iteration 3090 Training loss 0.05371476337313652 Validation loss 0.055255185812711716 Accuracy 0.44384765625\n",
      "Iteration 3100 Training loss 0.05253344774246216 Validation loss 0.05555330961942673 Accuracy 0.4404296875\n",
      "Iteration 3110 Training loss 0.054091501981019974 Validation loss 0.05527392029762268 Accuracy 0.44384765625\n",
      "Iteration 3120 Training loss 0.056614287197589874 Validation loss 0.055186279118061066 Accuracy 0.444580078125\n",
      "Iteration 3130 Training loss 0.053498148918151855 Validation loss 0.05531776323914528 Accuracy 0.443115234375\n",
      "Iteration 3140 Training loss 0.05059070885181427 Validation loss 0.055179569870233536 Accuracy 0.444580078125\n",
      "Iteration 3150 Training loss 0.052827637642621994 Validation loss 0.05579746887087822 Accuracy 0.438232421875\n",
      "Iteration 3160 Training loss 0.05601639673113823 Validation loss 0.055763885378837585 Accuracy 0.4384765625\n",
      "Iteration 3170 Training loss 0.05443665012717247 Validation loss 0.05539221689105034 Accuracy 0.4423828125\n",
      "Iteration 3180 Training loss 0.055616676807403564 Validation loss 0.055173225700855255 Accuracy 0.445068359375\n",
      "Iteration 3190 Training loss 0.05451482534408569 Validation loss 0.055178314447402954 Accuracy 0.444580078125\n",
      "Iteration 3200 Training loss 0.05415618419647217 Validation loss 0.05516562983393669 Accuracy 0.4443359375\n",
      "Iteration 3210 Training loss 0.05374886095523834 Validation loss 0.05510969087481499 Accuracy 0.4443359375\n",
      "Iteration 3220 Training loss 0.05025679990649223 Validation loss 0.055224910378456116 Accuracy 0.44384765625\n",
      "Iteration 3230 Training loss 0.05664340779185295 Validation loss 0.05534670129418373 Accuracy 0.443115234375\n",
      "Iteration 3240 Training loss 0.05659967660903931 Validation loss 0.05550447106361389 Accuracy 0.44140625\n",
      "Iteration 3250 Training loss 0.05909375473856926 Validation loss 0.05525006726384163 Accuracy 0.443359375\n",
      "Iteration 3260 Training loss 0.057284075766801834 Validation loss 0.05552186071872711 Accuracy 0.440673828125\n",
      "Iteration 3270 Training loss 0.05486755445599556 Validation loss 0.05523981153964996 Accuracy 0.443603515625\n",
      "Iteration 3280 Training loss 0.0569751039147377 Validation loss 0.0558917261660099 Accuracy 0.437255859375\n",
      "Iteration 3290 Training loss 0.05313749611377716 Validation loss 0.05506262555718422 Accuracy 0.4453125\n",
      "Iteration 3300 Training loss 0.05753049999475479 Validation loss 0.056595154106616974 Accuracy 0.430419921875\n",
      "Iteration 3310 Training loss 0.055589497089385986 Validation loss 0.0550784170627594 Accuracy 0.44482421875\n",
      "Iteration 3320 Training loss 0.054701052606105804 Validation loss 0.05532144010066986 Accuracy 0.44287109375\n",
      "Iteration 3330 Training loss 0.05243947356939316 Validation loss 0.0551237091422081 Accuracy 0.44482421875\n",
      "Iteration 3340 Training loss 0.054453328251838684 Validation loss 0.05518008768558502 Accuracy 0.4443359375\n",
      "Iteration 3350 Training loss 0.05559911951422691 Validation loss 0.055810775607824326 Accuracy 0.437744140625\n",
      "Iteration 3360 Training loss 0.05496172979474068 Validation loss 0.05510272458195686 Accuracy 0.445068359375\n",
      "Iteration 3370 Training loss 0.05583779886364937 Validation loss 0.05525779724121094 Accuracy 0.444091796875\n",
      "Iteration 3380 Training loss 0.053673043847084045 Validation loss 0.05531887710094452 Accuracy 0.442626953125\n",
      "Iteration 3390 Training loss 0.05372706428170204 Validation loss 0.05581203103065491 Accuracy 0.438232421875\n",
      "Iteration 3400 Training loss 0.054375674575567245 Validation loss 0.05533992126584053 Accuracy 0.44287109375\n",
      "Iteration 3410 Training loss 0.05207068845629692 Validation loss 0.055369000881910324 Accuracy 0.442626953125\n",
      "Iteration 3420 Training loss 0.05136829614639282 Validation loss 0.05520617961883545 Accuracy 0.44384765625\n",
      "Iteration 3430 Training loss 0.05259658396244049 Validation loss 0.05502127483487129 Accuracy 0.446044921875\n",
      "Iteration 3440 Training loss 0.0517013743519783 Validation loss 0.05519476532936096 Accuracy 0.444091796875\n",
      "Iteration 3450 Training loss 0.053852252662181854 Validation loss 0.05533084645867348 Accuracy 0.442626953125\n",
      "Iteration 3460 Training loss 0.054505061358213425 Validation loss 0.0554388053715229 Accuracy 0.441650390625\n",
      "Iteration 3470 Training loss 0.05531185492873192 Validation loss 0.05509486794471741 Accuracy 0.444580078125\n",
      "Iteration 3480 Training loss 0.05305026099085808 Validation loss 0.05513698607683182 Accuracy 0.444091796875\n",
      "Iteration 3490 Training loss 0.05369003117084503 Validation loss 0.055398281663656235 Accuracy 0.442138671875\n",
      "Iteration 3500 Training loss 0.05381511524319649 Validation loss 0.05581069365143776 Accuracy 0.43798828125\n",
      "Iteration 3510 Training loss 0.052728764712810516 Validation loss 0.05547501891851425 Accuracy 0.44091796875\n",
      "Iteration 3520 Training loss 0.0558050312101841 Validation loss 0.05594230815768242 Accuracy 0.437255859375\n",
      "Iteration 3530 Training loss 0.055659279227256775 Validation loss 0.05566774681210518 Accuracy 0.439453125\n",
      "Iteration 3540 Training loss 0.05259249359369278 Validation loss 0.055769216269254684 Accuracy 0.43798828125\n",
      "Iteration 3550 Training loss 0.051246535032987595 Validation loss 0.055047646164894104 Accuracy 0.445556640625\n",
      "Iteration 3560 Training loss 0.05372065305709839 Validation loss 0.05567144975066185 Accuracy 0.439697265625\n",
      "Iteration 3570 Training loss 0.05063368007540703 Validation loss 0.05518424138426781 Accuracy 0.44384765625\n",
      "Iteration 3580 Training loss 0.050875600427389145 Validation loss 0.055266473442316055 Accuracy 0.443115234375\n",
      "Iteration 3590 Training loss 0.053860656917095184 Validation loss 0.05520832911133766 Accuracy 0.444091796875\n",
      "Iteration 3600 Training loss 0.05929780378937721 Validation loss 0.05621987581253052 Accuracy 0.43408203125\n",
      "Iteration 3610 Training loss 0.055348750203847885 Validation loss 0.05555877462029457 Accuracy 0.440673828125\n",
      "Iteration 3620 Training loss 0.05449574068188667 Validation loss 0.05543643981218338 Accuracy 0.441162109375\n",
      "Iteration 3630 Training loss 0.05324144288897514 Validation loss 0.05576379597187042 Accuracy 0.438720703125\n",
      "Iteration 3640 Training loss 0.05371735990047455 Validation loss 0.056210603564977646 Accuracy 0.434326171875\n",
      "Iteration 3650 Training loss 0.053678225725889206 Validation loss 0.05495927110314369 Accuracy 0.4462890625\n",
      "Iteration 3660 Training loss 0.050947535783052444 Validation loss 0.05512210726737976 Accuracy 0.4443359375\n",
      "Iteration 3670 Training loss 0.053399357944726944 Validation loss 0.05510770529508591 Accuracy 0.445068359375\n",
      "Iteration 3680 Training loss 0.05015704408288002 Validation loss 0.05500136688351631 Accuracy 0.445556640625\n",
      "Iteration 3690 Training loss 0.05247693881392479 Validation loss 0.05554116144776344 Accuracy 0.440185546875\n",
      "Iteration 3700 Training loss 0.05023318901658058 Validation loss 0.055279962718486786 Accuracy 0.443359375\n",
      "Iteration 3710 Training loss 0.05413183942437172 Validation loss 0.05502757057547569 Accuracy 0.4462890625\n",
      "Iteration 3720 Training loss 0.05648733302950859 Validation loss 0.05503576993942261 Accuracy 0.4462890625\n",
      "Iteration 3730 Training loss 0.05576945096254349 Validation loss 0.05500006675720215 Accuracy 0.446044921875\n",
      "Iteration 3740 Training loss 0.05302071571350098 Validation loss 0.055187419056892395 Accuracy 0.4443359375\n",
      "Iteration 3750 Training loss 0.053058065474033356 Validation loss 0.055654626339673996 Accuracy 0.439453125\n",
      "Iteration 3760 Training loss 0.05497122183442116 Validation loss 0.05662703514099121 Accuracy 0.43017578125\n",
      "Iteration 3770 Training loss 0.05400069057941437 Validation loss 0.055592987686395645 Accuracy 0.440185546875\n",
      "Iteration 3780 Training loss 0.052410464733839035 Validation loss 0.0551859512925148 Accuracy 0.4443359375\n",
      "Iteration 3790 Training loss 0.055267155170440674 Validation loss 0.0549454391002655 Accuracy 0.447021484375\n",
      "Iteration 3800 Training loss 0.057584792375564575 Validation loss 0.0557108148932457 Accuracy 0.438720703125\n",
      "Iteration 3810 Training loss 0.053305741399526596 Validation loss 0.05552717670798302 Accuracy 0.441162109375\n",
      "Iteration 3820 Training loss 0.05278816819190979 Validation loss 0.05512814223766327 Accuracy 0.445068359375\n",
      "Iteration 3830 Training loss 0.05185457319021225 Validation loss 0.055124230682849884 Accuracy 0.444580078125\n",
      "Iteration 3840 Training loss 0.05264552682638168 Validation loss 0.055265653878450394 Accuracy 0.443115234375\n",
      "Iteration 3850 Training loss 0.055308155715465546 Validation loss 0.054947108030319214 Accuracy 0.446533203125\n",
      "Iteration 3860 Training loss 0.057089969515800476 Validation loss 0.05500619858503342 Accuracy 0.446044921875\n",
      "Iteration 3870 Training loss 0.05124564468860626 Validation loss 0.055085957050323486 Accuracy 0.4453125\n",
      "Iteration 3880 Training loss 0.05446435511112213 Validation loss 0.054887499660253525 Accuracy 0.44677734375\n",
      "Iteration 3890 Training loss 0.05514528974890709 Validation loss 0.05505760759115219 Accuracy 0.44580078125\n",
      "Iteration 3900 Training loss 0.05748169869184494 Validation loss 0.05664287135004997 Accuracy 0.430419921875\n",
      "Iteration 3910 Training loss 0.05504941940307617 Validation loss 0.05539129674434662 Accuracy 0.44140625\n",
      "Iteration 3920 Training loss 0.05463361740112305 Validation loss 0.05510597676038742 Accuracy 0.444580078125\n",
      "Iteration 3930 Training loss 0.0535183846950531 Validation loss 0.05522417649626732 Accuracy 0.443603515625\n",
      "Iteration 3940 Training loss 0.05098197981715202 Validation loss 0.055015888065099716 Accuracy 0.44580078125\n",
      "Iteration 3950 Training loss 0.05070681869983673 Validation loss 0.05502678453922272 Accuracy 0.44580078125\n",
      "Iteration 3960 Training loss 0.05383269116282463 Validation loss 0.05521215498447418 Accuracy 0.444091796875\n",
      "Iteration 3970 Training loss 0.05466494336724281 Validation loss 0.05479367449879646 Accuracy 0.44775390625\n",
      "Iteration 3980 Training loss 0.05858546867966652 Validation loss 0.055712517350912094 Accuracy 0.439697265625\n",
      "Iteration 3990 Training loss 0.05169309303164482 Validation loss 0.05532435327768326 Accuracy 0.443603515625\n",
      "Iteration 4000 Training loss 0.056719422340393066 Validation loss 0.05512052774429321 Accuracy 0.44482421875\n",
      "Iteration 4010 Training loss 0.05274885147809982 Validation loss 0.05489026755094528 Accuracy 0.447021484375\n",
      "Iteration 4020 Training loss 0.05316048115491867 Validation loss 0.05502767115831375 Accuracy 0.4462890625\n",
      "Iteration 4030 Training loss 0.054489873349666595 Validation loss 0.054939765483140945 Accuracy 0.44677734375\n",
      "Iteration 4040 Training loss 0.053970687091350555 Validation loss 0.05490383505821228 Accuracy 0.446533203125\n",
      "Iteration 4050 Training loss 0.05744867026805878 Validation loss 0.055704470723867416 Accuracy 0.439453125\n",
      "Iteration 4060 Training loss 0.054021768271923065 Validation loss 0.05541152507066727 Accuracy 0.44189453125\n",
      "Iteration 4070 Training loss 0.05134839192032814 Validation loss 0.054961852729320526 Accuracy 0.447021484375\n",
      "Iteration 4080 Training loss 0.058686330914497375 Validation loss 0.05511162802577019 Accuracy 0.444580078125\n",
      "Iteration 4090 Training loss 0.05336760729551315 Validation loss 0.05513308569788933 Accuracy 0.445068359375\n",
      "Iteration 4100 Training loss 0.05400167778134346 Validation loss 0.054989367723464966 Accuracy 0.446044921875\n",
      "Iteration 4110 Training loss 0.057269707322120667 Validation loss 0.056064262986183167 Accuracy 0.435791015625\n",
      "Iteration 4120 Training loss 0.051859136670827866 Validation loss 0.055318690836429596 Accuracy 0.443603515625\n",
      "Iteration 4130 Training loss 0.053783975541591644 Validation loss 0.05500015616416931 Accuracy 0.44580078125\n",
      "Iteration 4140 Training loss 0.05174649506807327 Validation loss 0.05538418889045715 Accuracy 0.442138671875\n",
      "Iteration 4150 Training loss 0.05640437826514244 Validation loss 0.055202219635248184 Accuracy 0.44384765625\n",
      "Iteration 4160 Training loss 0.056046273559331894 Validation loss 0.055117059499025345 Accuracy 0.444580078125\n",
      "Iteration 4170 Training loss 0.05122040584683418 Validation loss 0.05522533133625984 Accuracy 0.444091796875\n",
      "Iteration 4180 Training loss 0.05491279065608978 Validation loss 0.0550147145986557 Accuracy 0.446044921875\n",
      "Iteration 4190 Training loss 0.052524566650390625 Validation loss 0.055044643580913544 Accuracy 0.4462890625\n",
      "Iteration 4200 Training loss 0.0518423356115818 Validation loss 0.05513177067041397 Accuracy 0.44482421875\n",
      "Iteration 4210 Training loss 0.053396616131067276 Validation loss 0.055107008665800095 Accuracy 0.444580078125\n",
      "Iteration 4220 Training loss 0.05258876457810402 Validation loss 0.055222850292921066 Accuracy 0.443603515625\n",
      "Iteration 4230 Training loss 0.05569108948111534 Validation loss 0.055260468274354935 Accuracy 0.443359375\n",
      "Iteration 4240 Training loss 0.055031199008226395 Validation loss 0.05521547794342041 Accuracy 0.443359375\n",
      "Iteration 4250 Training loss 0.05293585732579231 Validation loss 0.05499398335814476 Accuracy 0.4462890625\n",
      "Iteration 4260 Training loss 0.05352163687348366 Validation loss 0.054920319467782974 Accuracy 0.44677734375\n",
      "Iteration 4270 Training loss 0.05789545550942421 Validation loss 0.054863981902599335 Accuracy 0.447509765625\n",
      "Iteration 4280 Training loss 0.05101073160767555 Validation loss 0.05490686371922493 Accuracy 0.447265625\n",
      "Iteration 4290 Training loss 0.05356168746948242 Validation loss 0.05486905202269554 Accuracy 0.447265625\n",
      "Iteration 4300 Training loss 0.056072529405355453 Validation loss 0.056165143847465515 Accuracy 0.43408203125\n",
      "Iteration 4310 Training loss 0.05159491300582886 Validation loss 0.05503014475107193 Accuracy 0.44580078125\n",
      "Iteration 4320 Training loss 0.05473578721284866 Validation loss 0.0558689720928669 Accuracy 0.437744140625\n",
      "Iteration 4330 Training loss 0.0496077686548233 Validation loss 0.05543681979179382 Accuracy 0.442138671875\n",
      "Iteration 4340 Training loss 0.053029149770736694 Validation loss 0.05491071194410324 Accuracy 0.447509765625\n",
      "Iteration 4350 Training loss 0.05319921672344208 Validation loss 0.05503929406404495 Accuracy 0.44580078125\n",
      "Iteration 4360 Training loss 0.05254888907074928 Validation loss 0.05507007986307144 Accuracy 0.4453125\n",
      "Iteration 4370 Training loss 0.05380040034651756 Validation loss 0.05503087863326073 Accuracy 0.4462890625\n",
      "Iteration 4380 Training loss 0.05614444985985756 Validation loss 0.05511040985584259 Accuracy 0.445556640625\n",
      "Iteration 4390 Training loss 0.05526510626077652 Validation loss 0.05558604374527931 Accuracy 0.440673828125\n",
      "Iteration 4400 Training loss 0.056734658777713776 Validation loss 0.05613222345709801 Accuracy 0.43505859375\n",
      "Iteration 4410 Training loss 0.054548513144254684 Validation loss 0.05490250512957573 Accuracy 0.447265625\n",
      "Iteration 4420 Training loss 0.05568886175751686 Validation loss 0.054996516555547714 Accuracy 0.446533203125\n",
      "Iteration 4430 Training loss 0.05566829442977905 Validation loss 0.05523281916975975 Accuracy 0.44384765625\n",
      "Iteration 4440 Training loss 0.05342628061771393 Validation loss 0.05525502935051918 Accuracy 0.443603515625\n",
      "Iteration 4450 Training loss 0.05463004484772682 Validation loss 0.056366413831710815 Accuracy 0.432373046875\n",
      "Iteration 4460 Training loss 0.05384134501218796 Validation loss 0.05529370158910751 Accuracy 0.443603515625\n",
      "Iteration 4470 Training loss 0.05412526801228523 Validation loss 0.0560135543346405 Accuracy 0.435791015625\n",
      "Iteration 4480 Training loss 0.05428491160273552 Validation loss 0.055118776857852936 Accuracy 0.4453125\n",
      "Iteration 4490 Training loss 0.05101289972662926 Validation loss 0.05494528263807297 Accuracy 0.447021484375\n",
      "Iteration 4500 Training loss 0.051745519042015076 Validation loss 0.05513981357216835 Accuracy 0.44482421875\n",
      "Iteration 4510 Training loss 0.05305982381105423 Validation loss 0.05500970035791397 Accuracy 0.446044921875\n",
      "Iteration 4520 Training loss 0.05230966582894325 Validation loss 0.05495689809322357 Accuracy 0.447021484375\n",
      "Iteration 4530 Training loss 0.054336316883563995 Validation loss 0.05505228415131569 Accuracy 0.446044921875\n",
      "Iteration 4540 Training loss 0.05466296151280403 Validation loss 0.0551324188709259 Accuracy 0.44482421875\n",
      "Iteration 4550 Training loss 0.050950318574905396 Validation loss 0.05508152022957802 Accuracy 0.44580078125\n",
      "Iteration 4560 Training loss 0.053171344101428986 Validation loss 0.05503121018409729 Accuracy 0.4462890625\n",
      "Iteration 4570 Training loss 0.0565255843102932 Validation loss 0.05521117150783539 Accuracy 0.44482421875\n",
      "Iteration 4580 Training loss 0.05175364017486572 Validation loss 0.05500934273004532 Accuracy 0.44677734375\n",
      "Iteration 4590 Training loss 0.0551033616065979 Validation loss 0.0553351454436779 Accuracy 0.44384765625\n",
      "Iteration 4600 Training loss 0.05328774079680443 Validation loss 0.05540540814399719 Accuracy 0.442626953125\n",
      "Iteration 4610 Training loss 0.05307769030332565 Validation loss 0.05504337325692177 Accuracy 0.446044921875\n",
      "Iteration 4620 Training loss 0.04921988770365715 Validation loss 0.0549188032746315 Accuracy 0.447509765625\n",
      "Iteration 4630 Training loss 0.057642098516225815 Validation loss 0.05523355305194855 Accuracy 0.44384765625\n",
      "Iteration 4640 Training loss 0.05454644560813904 Validation loss 0.055487968027591705 Accuracy 0.441650390625\n",
      "Iteration 4650 Training loss 0.05419696867465973 Validation loss 0.055241659283638 Accuracy 0.444091796875\n",
      "Iteration 4660 Training loss 0.057289596647024155 Validation loss 0.05641108378767967 Accuracy 0.4326171875\n",
      "Iteration 4670 Training loss 0.05593262240290642 Validation loss 0.055283475667238235 Accuracy 0.44384765625\n",
      "Iteration 4680 Training loss 0.05533796176314354 Validation loss 0.05503103509545326 Accuracy 0.4462890625\n",
      "Iteration 4690 Training loss 0.055963627994060516 Validation loss 0.054818447679281235 Accuracy 0.4482421875\n",
      "Iteration 4700 Training loss 0.05193498358130455 Validation loss 0.055607929825782776 Accuracy 0.441162109375\n",
      "Iteration 4710 Training loss 0.0554141141474247 Validation loss 0.055657558143138885 Accuracy 0.440185546875\n",
      "Iteration 4720 Training loss 0.056082833558321 Validation loss 0.05561687797307968 Accuracy 0.439208984375\n",
      "Iteration 4730 Training loss 0.05573076382279396 Validation loss 0.055232077836990356 Accuracy 0.443115234375\n",
      "Iteration 4740 Training loss 0.055308062583208084 Validation loss 0.055058278143405914 Accuracy 0.44580078125\n",
      "Iteration 4750 Training loss 0.05339488759636879 Validation loss 0.05523594096302986 Accuracy 0.4443359375\n",
      "Iteration 4760 Training loss 0.05600105598568916 Validation loss 0.055625468492507935 Accuracy 0.4404296875\n",
      "Iteration 4770 Training loss 0.0547456368803978 Validation loss 0.05548563227057457 Accuracy 0.441650390625\n",
      "Iteration 4780 Training loss 0.05271202698349953 Validation loss 0.05544228479266167 Accuracy 0.44140625\n",
      "Iteration 4790 Training loss 0.05471815913915634 Validation loss 0.05506332963705063 Accuracy 0.44580078125\n",
      "Iteration 4800 Training loss 0.053246427327394485 Validation loss 0.055106762796640396 Accuracy 0.445068359375\n",
      "Iteration 4810 Training loss 0.054183512926101685 Validation loss 0.05481254681944847 Accuracy 0.447998046875\n",
      "Iteration 4820 Training loss 0.05484308674931526 Validation loss 0.054958585649728775 Accuracy 0.447021484375\n",
      "Iteration 4830 Training loss 0.05248263478279114 Validation loss 0.0547674261033535 Accuracy 0.448974609375\n",
      "Iteration 4840 Training loss 0.055078621953725815 Validation loss 0.0553860142827034 Accuracy 0.442138671875\n",
      "Iteration 4850 Training loss 0.05130480229854584 Validation loss 0.05478639528155327 Accuracy 0.4482421875\n",
      "Iteration 4860 Training loss 0.05532701686024666 Validation loss 0.05483245104551315 Accuracy 0.447509765625\n",
      "Iteration 4870 Training loss 0.05246548354625702 Validation loss 0.05475477874279022 Accuracy 0.448486328125\n",
      "Iteration 4880 Training loss 0.051891811192035675 Validation loss 0.05507002770900726 Accuracy 0.4453125\n",
      "Iteration 4890 Training loss 0.05495651811361313 Validation loss 0.055276889353990555 Accuracy 0.443359375\n",
      "Iteration 4900 Training loss 0.054406408220529556 Validation loss 0.054714374244213104 Accuracy 0.44873046875\n",
      "Iteration 4910 Training loss 0.054317839443683624 Validation loss 0.0551522821187973 Accuracy 0.444091796875\n",
      "Iteration 4920 Training loss 0.05242699757218361 Validation loss 0.0551970899105072 Accuracy 0.44384765625\n",
      "Iteration 4930 Training loss 0.05351269617676735 Validation loss 0.05506235361099243 Accuracy 0.445556640625\n",
      "Iteration 4940 Training loss 0.0526866540312767 Validation loss 0.054882731288671494 Accuracy 0.447509765625\n",
      "Iteration 4950 Training loss 0.04990311712026596 Validation loss 0.05493376776576042 Accuracy 0.447265625\n",
      "Iteration 4960 Training loss 0.05473736673593521 Validation loss 0.054925497621297836 Accuracy 0.4462890625\n",
      "Iteration 4970 Training loss 0.05464399233460426 Validation loss 0.05475194752216339 Accuracy 0.448486328125\n",
      "Iteration 4980 Training loss 0.05586572736501694 Validation loss 0.055005043745040894 Accuracy 0.446044921875\n",
      "Iteration 4990 Training loss 0.051727112382650375 Validation loss 0.05497945100069046 Accuracy 0.446044921875\n",
      "Iteration 5000 Training loss 0.05144787207245827 Validation loss 0.05508842691779137 Accuracy 0.4453125\n",
      "Iteration 5010 Training loss 0.056429579854011536 Validation loss 0.05526243895292282 Accuracy 0.443603515625\n",
      "Iteration 5020 Training loss 0.05466323718428612 Validation loss 0.05499355494976044 Accuracy 0.4462890625\n",
      "Iteration 5030 Training loss 0.05738865211606026 Validation loss 0.05513286963105202 Accuracy 0.4453125\n",
      "Iteration 5040 Training loss 0.0535503551363945 Validation loss 0.054701536893844604 Accuracy 0.448974609375\n",
      "Iteration 5050 Training loss 0.05516739934682846 Validation loss 0.054857611656188965 Accuracy 0.44775390625\n",
      "Iteration 5060 Training loss 0.05228758603334427 Validation loss 0.055005498230457306 Accuracy 0.4462890625\n",
      "Iteration 5070 Training loss 0.05231538414955139 Validation loss 0.05503121018409729 Accuracy 0.44580078125\n",
      "Iteration 5080 Training loss 0.056581050157547 Validation loss 0.055425211787223816 Accuracy 0.44140625\n",
      "Iteration 5090 Training loss 0.053770266473293304 Validation loss 0.05495346710085869 Accuracy 0.446533203125\n",
      "Iteration 5100 Training loss 0.052333515137434006 Validation loss 0.05504893511533737 Accuracy 0.4453125\n",
      "Iteration 5110 Training loss 0.05439722537994385 Validation loss 0.054839882999658585 Accuracy 0.447998046875\n",
      "Iteration 5120 Training loss 0.0526583157479763 Validation loss 0.05483526363968849 Accuracy 0.44775390625\n",
      "Iteration 5130 Training loss 0.054370176047086716 Validation loss 0.054955944418907166 Accuracy 0.446533203125\n",
      "Iteration 5140 Training loss 0.054170601069927216 Validation loss 0.05485668405890465 Accuracy 0.447509765625\n",
      "Iteration 5150 Training loss 0.05642930045723915 Validation loss 0.05474051460623741 Accuracy 0.448486328125\n",
      "Iteration 5160 Training loss 0.052062638103961945 Validation loss 0.054916296154260635 Accuracy 0.44677734375\n",
      "Iteration 5170 Training loss 0.05627782270312309 Validation loss 0.05483279004693031 Accuracy 0.448486328125\n",
      "Iteration 5180 Training loss 0.05642516911029816 Validation loss 0.05507587268948555 Accuracy 0.4453125\n",
      "Iteration 5190 Training loss 0.051822345703840256 Validation loss 0.05469809100031853 Accuracy 0.44970703125\n",
      "Iteration 5200 Training loss 0.053129974752664566 Validation loss 0.0551963746547699 Accuracy 0.444091796875\n",
      "Iteration 5210 Training loss 0.052715759724378586 Validation loss 0.054871562868356705 Accuracy 0.447265625\n",
      "Iteration 5220 Training loss 0.0529169924557209 Validation loss 0.05470772460103035 Accuracy 0.448974609375\n",
      "Iteration 5230 Training loss 0.05342695116996765 Validation loss 0.05493348836898804 Accuracy 0.44677734375\n",
      "Iteration 5240 Training loss 0.05226978659629822 Validation loss 0.05485127866268158 Accuracy 0.447509765625\n",
      "Iteration 5250 Training loss 0.05561380088329315 Validation loss 0.05489470437169075 Accuracy 0.447021484375\n",
      "Iteration 5260 Training loss 0.05499958246946335 Validation loss 0.054674576967954636 Accuracy 0.44921875\n",
      "Iteration 5270 Training loss 0.056144922971725464 Validation loss 0.05479399114847183 Accuracy 0.447998046875\n",
      "Iteration 5280 Training loss 0.054680921137332916 Validation loss 0.054646678268909454 Accuracy 0.44921875\n",
      "Iteration 5290 Training loss 0.05565223470330238 Validation loss 0.05476316437125206 Accuracy 0.44873046875\n",
      "Iteration 5300 Training loss 0.05146031826734543 Validation loss 0.05470709875226021 Accuracy 0.448974609375\n",
      "Iteration 5310 Training loss 0.053891561925411224 Validation loss 0.05503490939736366 Accuracy 0.446044921875\n",
      "Iteration 5320 Training loss 0.053555626422166824 Validation loss 0.0552663616836071 Accuracy 0.443359375\n",
      "Iteration 5330 Training loss 0.05434971675276756 Validation loss 0.054805878549814224 Accuracy 0.44775390625\n",
      "Iteration 5340 Training loss 0.05514902621507645 Validation loss 0.054739248007535934 Accuracy 0.4482421875\n",
      "Iteration 5350 Training loss 0.05583763122558594 Validation loss 0.055237654596567154 Accuracy 0.443115234375\n",
      "Iteration 5360 Training loss 0.05522064492106438 Validation loss 0.055574215948581696 Accuracy 0.440673828125\n",
      "Iteration 5370 Training loss 0.05453154072165489 Validation loss 0.054927192628383636 Accuracy 0.4462890625\n",
      "Iteration 5380 Training loss 0.056100696325302124 Validation loss 0.05543763190507889 Accuracy 0.441162109375\n",
      "Iteration 5390 Training loss 0.05303162708878517 Validation loss 0.054784782230854034 Accuracy 0.447998046875\n",
      "Iteration 5400 Training loss 0.05392191931605339 Validation loss 0.054915159940719604 Accuracy 0.447021484375\n",
      "Iteration 5410 Training loss 0.050744667649269104 Validation loss 0.05510397255420685 Accuracy 0.445068359375\n",
      "Iteration 5420 Training loss 0.05516783520579338 Validation loss 0.05587002635002136 Accuracy 0.437255859375\n",
      "Iteration 5430 Training loss 0.05670753866434097 Validation loss 0.05580991506576538 Accuracy 0.437744140625\n",
      "Iteration 5440 Training loss 0.05209898203611374 Validation loss 0.05504138395190239 Accuracy 0.445068359375\n",
      "Iteration 5450 Training loss 0.05456477776169777 Validation loss 0.05487882345914841 Accuracy 0.447509765625\n",
      "Iteration 5460 Training loss 0.055066660046577454 Validation loss 0.05470557510852814 Accuracy 0.448486328125\n",
      "Iteration 5470 Training loss 0.0532061904668808 Validation loss 0.05489632859826088 Accuracy 0.447509765625\n",
      "Iteration 5480 Training loss 0.05562300607562065 Validation loss 0.05470218509435654 Accuracy 0.448486328125\n",
      "Iteration 5490 Training loss 0.053857795894145966 Validation loss 0.05496544390916824 Accuracy 0.446533203125\n",
      "Iteration 5500 Training loss 0.05476250499486923 Validation loss 0.05526350438594818 Accuracy 0.443603515625\n",
      "Iteration 5510 Training loss 0.05473197251558304 Validation loss 0.05485915765166283 Accuracy 0.447265625\n",
      "Iteration 5520 Training loss 0.0498250387609005 Validation loss 0.05540170893073082 Accuracy 0.44189453125\n",
      "Iteration 5530 Training loss 0.0550265908241272 Validation loss 0.05480993539094925 Accuracy 0.44775390625\n",
      "Iteration 5540 Training loss 0.052148252725601196 Validation loss 0.05475000664591789 Accuracy 0.4482421875\n",
      "Iteration 5550 Training loss 0.05642177164554596 Validation loss 0.05471593514084816 Accuracy 0.447998046875\n",
      "Iteration 5560 Training loss 0.05400819703936577 Validation loss 0.05472269281744957 Accuracy 0.448486328125\n",
      "Iteration 5570 Training loss 0.05017772689461708 Validation loss 0.054838456213474274 Accuracy 0.447021484375\n",
      "Iteration 5580 Training loss 0.051348406821489334 Validation loss 0.0547887347638607 Accuracy 0.44775390625\n",
      "Iteration 5590 Training loss 0.05621260404586792 Validation loss 0.056611161679029465 Accuracy 0.429931640625\n",
      "Iteration 5600 Training loss 0.05428896099328995 Validation loss 0.05538473650813103 Accuracy 0.441650390625\n",
      "Iteration 5610 Training loss 0.05482945591211319 Validation loss 0.055028147995471954 Accuracy 0.445556640625\n",
      "Iteration 5620 Training loss 0.05733970180153847 Validation loss 0.05515386909246445 Accuracy 0.4443359375\n",
      "Iteration 5630 Training loss 0.054178934544324875 Validation loss 0.05474554002285004 Accuracy 0.448486328125\n",
      "Iteration 5640 Training loss 0.05354839935898781 Validation loss 0.05558733642101288 Accuracy 0.439697265625\n",
      "Iteration 5650 Training loss 0.057589661329984665 Validation loss 0.05496978387236595 Accuracy 0.446044921875\n",
      "Iteration 5660 Training loss 0.053408294916152954 Validation loss 0.0548936203122139 Accuracy 0.44677734375\n",
      "Iteration 5670 Training loss 0.05240897461771965 Validation loss 0.054652854800224304 Accuracy 0.448974609375\n",
      "Iteration 5680 Training loss 0.05940116196870804 Validation loss 0.05597571283578873 Accuracy 0.435791015625\n",
      "Iteration 5690 Training loss 0.05068478360772133 Validation loss 0.05478605255484581 Accuracy 0.447998046875\n",
      "Iteration 5700 Training loss 0.055743325501680374 Validation loss 0.05473373457789421 Accuracy 0.448486328125\n",
      "Iteration 5710 Training loss 0.0517803356051445 Validation loss 0.05481620505452156 Accuracy 0.44775390625\n",
      "Iteration 5720 Training loss 0.05729985982179642 Validation loss 0.055899158120155334 Accuracy 0.436279296875\n",
      "Iteration 5730 Training loss 0.051502104848623276 Validation loss 0.05505019426345825 Accuracy 0.44482421875\n",
      "Iteration 5740 Training loss 0.054157692939043045 Validation loss 0.05492803454399109 Accuracy 0.446044921875\n",
      "Iteration 5750 Training loss 0.053528737276792526 Validation loss 0.054957274347543716 Accuracy 0.4462890625\n",
      "Iteration 5760 Training loss 0.05276527255773544 Validation loss 0.05475423112511635 Accuracy 0.448486328125\n",
      "Iteration 5770 Training loss 0.050418414175510406 Validation loss 0.05495793744921684 Accuracy 0.446044921875\n",
      "Iteration 5780 Training loss 0.05244014784693718 Validation loss 0.05614612251520157 Accuracy 0.434326171875\n",
      "Iteration 5790 Training loss 0.05335721746087074 Validation loss 0.05486782640218735 Accuracy 0.44677734375\n",
      "Iteration 5800 Training loss 0.052834656089544296 Validation loss 0.054610475897789 Accuracy 0.448974609375\n",
      "Iteration 5810 Training loss 0.05033457279205322 Validation loss 0.0546121671795845 Accuracy 0.44970703125\n",
      "Iteration 5820 Training loss 0.05707133933901787 Validation loss 0.05593718960881233 Accuracy 0.4365234375\n",
      "Iteration 5830 Training loss 0.05576033517718315 Validation loss 0.05615302547812462 Accuracy 0.433837890625\n",
      "Iteration 5840 Training loss 0.053148895502090454 Validation loss 0.055199138820171356 Accuracy 0.443115234375\n",
      "Iteration 5850 Training loss 0.05589015781879425 Validation loss 0.05489417910575867 Accuracy 0.44775390625\n",
      "Iteration 5860 Training loss 0.053174398839473724 Validation loss 0.05477038025856018 Accuracy 0.447998046875\n",
      "Iteration 5870 Training loss 0.054669421166181564 Validation loss 0.054783161729574203 Accuracy 0.44775390625\n",
      "Iteration 5880 Training loss 0.05552404373884201 Validation loss 0.05463710427284241 Accuracy 0.44921875\n",
      "Iteration 5890 Training loss 0.05376453697681427 Validation loss 0.05539228394627571 Accuracy 0.441650390625\n",
      "Iteration 5900 Training loss 0.050648488104343414 Validation loss 0.055288221687078476 Accuracy 0.443359375\n",
      "Iteration 5910 Training loss 0.05369958281517029 Validation loss 0.05511955916881561 Accuracy 0.444580078125\n",
      "Iteration 5920 Training loss 0.05710228905081749 Validation loss 0.0558650903403759 Accuracy 0.4375\n",
      "Iteration 5930 Training loss 0.051615457981824875 Validation loss 0.054782669991254807 Accuracy 0.4482421875\n",
      "Iteration 5940 Training loss 0.05380963161587715 Validation loss 0.05479740723967552 Accuracy 0.447998046875\n",
      "Iteration 5950 Training loss 0.05594538152217865 Validation loss 0.05476406216621399 Accuracy 0.4482421875\n",
      "Iteration 5960 Training loss 0.05394607409834862 Validation loss 0.05525635555386543 Accuracy 0.443603515625\n",
      "Iteration 5970 Training loss 0.0541415698826313 Validation loss 0.05469473823904991 Accuracy 0.448974609375\n",
      "Iteration 5980 Training loss 0.052036065608263016 Validation loss 0.05477691441774368 Accuracy 0.44775390625\n",
      "Iteration 5990 Training loss 0.04990178346633911 Validation loss 0.05466875433921814 Accuracy 0.449462890625\n",
      "Iteration 6000 Training loss 0.052989501506090164 Validation loss 0.05463528633117676 Accuracy 0.44921875\n",
      "Iteration 6010 Training loss 0.05304193124175072 Validation loss 0.05469314008951187 Accuracy 0.4482421875\n",
      "Iteration 6020 Training loss 0.05397042632102966 Validation loss 0.05512962117791176 Accuracy 0.444091796875\n",
      "Iteration 6030 Training loss 0.052461057901382446 Validation loss 0.0548318549990654 Accuracy 0.447509765625\n",
      "Iteration 6040 Training loss 0.05464557930827141 Validation loss 0.05470900610089302 Accuracy 0.447998046875\n",
      "Iteration 6050 Training loss 0.053497765213251114 Validation loss 0.05469077080488205 Accuracy 0.44873046875\n",
      "Iteration 6060 Training loss 0.055476926267147064 Validation loss 0.054849471896886826 Accuracy 0.44677734375\n",
      "Iteration 6070 Training loss 0.0540376678109169 Validation loss 0.054793328046798706 Accuracy 0.447021484375\n",
      "Iteration 6080 Training loss 0.05431661009788513 Validation loss 0.054956305772066116 Accuracy 0.446044921875\n",
      "Iteration 6090 Training loss 0.052575916051864624 Validation loss 0.054770682007074356 Accuracy 0.448486328125\n",
      "Iteration 6100 Training loss 0.053995005786418915 Validation loss 0.0548020601272583 Accuracy 0.447509765625\n",
      "Iteration 6110 Training loss 0.05598635971546173 Validation loss 0.05476165935397148 Accuracy 0.447998046875\n",
      "Iteration 6120 Training loss 0.05895213410258293 Validation loss 0.055507536977529526 Accuracy 0.441162109375\n",
      "Iteration 6130 Training loss 0.05470891296863556 Validation loss 0.05468496307730675 Accuracy 0.44873046875\n",
      "Iteration 6140 Training loss 0.050715118646621704 Validation loss 0.05467610061168671 Accuracy 0.44921875\n",
      "Iteration 6150 Training loss 0.055443573743104935 Validation loss 0.0547906830906868 Accuracy 0.447509765625\n",
      "Iteration 6160 Training loss 0.05441431701183319 Validation loss 0.05475054681301117 Accuracy 0.44775390625\n",
      "Iteration 6170 Training loss 0.0535416342318058 Validation loss 0.054581113159656525 Accuracy 0.449951171875\n",
      "Iteration 6180 Training loss 0.05444562807679176 Validation loss 0.05477360263466835 Accuracy 0.447509765625\n",
      "Iteration 6190 Training loss 0.05438904091715813 Validation loss 0.05554138496518135 Accuracy 0.44091796875\n",
      "Iteration 6200 Training loss 0.05339622497558594 Validation loss 0.05712703987956047 Accuracy 0.42529296875\n",
      "Iteration 6210 Training loss 0.055543843656778336 Validation loss 0.05479247495532036 Accuracy 0.447509765625\n",
      "Iteration 6220 Training loss 0.05721871927380562 Validation loss 0.054764579981565475 Accuracy 0.447998046875\n",
      "Iteration 6230 Training loss 0.055236220359802246 Validation loss 0.0548383966088295 Accuracy 0.447509765625\n",
      "Iteration 6240 Training loss 0.05600697919726372 Validation loss 0.0551048144698143 Accuracy 0.444091796875\n",
      "Iteration 6250 Training loss 0.05463571101427078 Validation loss 0.05596471205353737 Accuracy 0.435302734375\n",
      "Iteration 6260 Training loss 0.049890320748090744 Validation loss 0.054645221680402756 Accuracy 0.448974609375\n",
      "Iteration 6270 Training loss 0.050619445741176605 Validation loss 0.055253591388463974 Accuracy 0.4423828125\n",
      "Iteration 6280 Training loss 0.055228669196367264 Validation loss 0.05500398948788643 Accuracy 0.445556640625\n",
      "Iteration 6290 Training loss 0.05276108905673027 Validation loss 0.054702963680028915 Accuracy 0.448486328125\n",
      "Iteration 6300 Training loss 0.0508151538670063 Validation loss 0.054807331413030624 Accuracy 0.447509765625\n",
      "Iteration 6310 Training loss 0.05407755821943283 Validation loss 0.05472857877612114 Accuracy 0.44775390625\n",
      "Iteration 6320 Training loss 0.05086057633161545 Validation loss 0.054932449012994766 Accuracy 0.44580078125\n",
      "Iteration 6330 Training loss 0.056023258715867996 Validation loss 0.054559435695409775 Accuracy 0.4501953125\n",
      "Iteration 6340 Training loss 0.052952688187360764 Validation loss 0.05500141531229019 Accuracy 0.44580078125\n",
      "Iteration 6350 Training loss 0.05374843254685402 Validation loss 0.05463346466422081 Accuracy 0.449462890625\n",
      "Iteration 6360 Training loss 0.05363670736551285 Validation loss 0.05479414016008377 Accuracy 0.4482421875\n",
      "Iteration 6370 Training loss 0.05449345335364342 Validation loss 0.054867226630449295 Accuracy 0.447265625\n",
      "Iteration 6380 Training loss 0.05470459535717964 Validation loss 0.05455179512500763 Accuracy 0.4501953125\n",
      "Iteration 6390 Training loss 0.050071440637111664 Validation loss 0.05461376905441284 Accuracy 0.44921875\n",
      "Iteration 6400 Training loss 0.05623142048716545 Validation loss 0.05491120368242264 Accuracy 0.447509765625\n",
      "Iteration 6410 Training loss 0.05388907715678215 Validation loss 0.055126406252384186 Accuracy 0.444580078125\n",
      "Iteration 6420 Training loss 0.05576292425394058 Validation loss 0.0547216534614563 Accuracy 0.4482421875\n",
      "Iteration 6430 Training loss 0.05474326014518738 Validation loss 0.05450882762670517 Accuracy 0.4501953125\n",
      "Iteration 6440 Training loss 0.05231170728802681 Validation loss 0.05484914779663086 Accuracy 0.447509765625\n",
      "Iteration 6450 Training loss 0.050267357379198074 Validation loss 0.05476823076605797 Accuracy 0.447998046875\n",
      "Iteration 6460 Training loss 0.04925430193543434 Validation loss 0.054679255932569504 Accuracy 0.448486328125\n",
      "Iteration 6470 Training loss 0.05048321932554245 Validation loss 0.055073268711566925 Accuracy 0.445068359375\n",
      "Iteration 6480 Training loss 0.05873460695147514 Validation loss 0.0546659491956234 Accuracy 0.44873046875\n",
      "Iteration 6490 Training loss 0.05277865380048752 Validation loss 0.05455666780471802 Accuracy 0.448974609375\n",
      "Iteration 6500 Training loss 0.05684458464384079 Validation loss 0.054875995963811874 Accuracy 0.4462890625\n",
      "Iteration 6510 Training loss 0.05100511759519577 Validation loss 0.05464513599872589 Accuracy 0.448974609375\n",
      "Iteration 6520 Training loss 0.05688918009400368 Validation loss 0.05480893328785896 Accuracy 0.44775390625\n",
      "Iteration 6530 Training loss 0.055642884224653244 Validation loss 0.05479918792843819 Accuracy 0.44775390625\n",
      "Iteration 6540 Training loss 0.05153156816959381 Validation loss 0.054673679172992706 Accuracy 0.448974609375\n",
      "Iteration 6550 Training loss 0.05595473572611809 Validation loss 0.05527474358677864 Accuracy 0.4423828125\n",
      "Iteration 6560 Training loss 0.05452747642993927 Validation loss 0.05479556322097778 Accuracy 0.447509765625\n",
      "Iteration 6570 Training loss 0.05363257974386215 Validation loss 0.05505957454442978 Accuracy 0.445068359375\n",
      "Iteration 6580 Training loss 0.054558929055929184 Validation loss 0.054884668439626694 Accuracy 0.4462890625\n",
      "Iteration 6590 Training loss 0.05441604554653168 Validation loss 0.055007703602313995 Accuracy 0.445556640625\n",
      "Iteration 6600 Training loss 0.053018707782030106 Validation loss 0.054839979857206345 Accuracy 0.447265625\n",
      "Iteration 6610 Training loss 0.05579886958003044 Validation loss 0.054541315883398056 Accuracy 0.449951171875\n",
      "Iteration 6620 Training loss 0.05693653225898743 Validation loss 0.05489378795027733 Accuracy 0.446533203125\n",
      "Iteration 6630 Training loss 0.055054716765880585 Validation loss 0.055154863744974136 Accuracy 0.443603515625\n",
      "Iteration 6640 Training loss 0.05369935557246208 Validation loss 0.05464295670390129 Accuracy 0.44970703125\n",
      "Iteration 6650 Training loss 0.05412725731730461 Validation loss 0.054791297763586044 Accuracy 0.447265625\n",
      "Iteration 6660 Training loss 0.05285300686955452 Validation loss 0.05479240044951439 Accuracy 0.447509765625\n",
      "Iteration 6670 Training loss 0.05601301044225693 Validation loss 0.05470702052116394 Accuracy 0.4482421875\n",
      "Iteration 6680 Training loss 0.052798207849264145 Validation loss 0.054663073271512985 Accuracy 0.448974609375\n",
      "Iteration 6690 Training loss 0.05289715901017189 Validation loss 0.054900724440813065 Accuracy 0.447021484375\n",
      "Iteration 6700 Training loss 0.05535214766860008 Validation loss 0.055150967091321945 Accuracy 0.444580078125\n",
      "Iteration 6710 Training loss 0.0546017661690712 Validation loss 0.054709140211343765 Accuracy 0.448974609375\n",
      "Iteration 6720 Training loss 0.052864715456962585 Validation loss 0.05515844747424126 Accuracy 0.44482421875\n",
      "Iteration 6730 Training loss 0.051623161882162094 Validation loss 0.054607827216386795 Accuracy 0.4501953125\n",
      "Iteration 6740 Training loss 0.05598480999469757 Validation loss 0.05526486039161682 Accuracy 0.44384765625\n",
      "Iteration 6750 Training loss 0.05276627093553543 Validation loss 0.055105917155742645 Accuracy 0.444580078125\n",
      "Iteration 6760 Training loss 0.05691702291369438 Validation loss 0.055718544870615005 Accuracy 0.4384765625\n",
      "Iteration 6770 Training loss 0.05569500848650932 Validation loss 0.054626159369945526 Accuracy 0.44970703125\n",
      "Iteration 6780 Training loss 0.05396813899278641 Validation loss 0.05465961620211601 Accuracy 0.44921875\n",
      "Iteration 6790 Training loss 0.05535213649272919 Validation loss 0.05486086755990982 Accuracy 0.44677734375\n",
      "Iteration 6800 Training loss 0.05622078478336334 Validation loss 0.05557820573449135 Accuracy 0.4404296875\n",
      "Iteration 6810 Training loss 0.058003123849630356 Validation loss 0.0548543818295002 Accuracy 0.447021484375\n",
      "Iteration 6820 Training loss 0.05744417756795883 Validation loss 0.05490482226014137 Accuracy 0.446533203125\n",
      "Iteration 6830 Training loss 0.05407651141285896 Validation loss 0.05451606214046478 Accuracy 0.450439453125\n",
      "Iteration 6840 Training loss 0.05318642780184746 Validation loss 0.055084023624658585 Accuracy 0.444091796875\n",
      "Iteration 6850 Training loss 0.05503220483660698 Validation loss 0.05461089685559273 Accuracy 0.44970703125\n",
      "Iteration 6860 Training loss 0.05245530232787132 Validation loss 0.054801810532808304 Accuracy 0.447998046875\n",
      "Iteration 6870 Training loss 0.05453679338097572 Validation loss 0.05562140420079231 Accuracy 0.439697265625\n",
      "Iteration 6880 Training loss 0.05302000790834427 Validation loss 0.05463289096951485 Accuracy 0.44921875\n",
      "Iteration 6890 Training loss 0.05373276770114899 Validation loss 0.05492959916591644 Accuracy 0.4462890625\n",
      "Iteration 6900 Training loss 0.05375878885388374 Validation loss 0.05527376011013985 Accuracy 0.44287109375\n",
      "Iteration 6910 Training loss 0.054159343242645264 Validation loss 0.054687194526195526 Accuracy 0.448486328125\n",
      "Iteration 6920 Training loss 0.053747162222862244 Validation loss 0.054781582206487656 Accuracy 0.447998046875\n",
      "Iteration 6930 Training loss 0.05343153700232506 Validation loss 0.054433632642030716 Accuracy 0.45068359375\n",
      "Iteration 6940 Training loss 0.05178716033697128 Validation loss 0.05471038073301315 Accuracy 0.448486328125\n",
      "Iteration 6950 Training loss 0.05691753700375557 Validation loss 0.05457330122590065 Accuracy 0.4501953125\n",
      "Iteration 6960 Training loss 0.05311139300465584 Validation loss 0.05462835729122162 Accuracy 0.44873046875\n",
      "Iteration 6970 Training loss 0.05456507205963135 Validation loss 0.05457310378551483 Accuracy 0.4501953125\n",
      "Iteration 6980 Training loss 0.05386253446340561 Validation loss 0.05531585216522217 Accuracy 0.443359375\n",
      "Iteration 6990 Training loss 0.05315179005265236 Validation loss 0.05457088723778725 Accuracy 0.44970703125\n",
      "Iteration 7000 Training loss 0.05534931272268295 Validation loss 0.05444852262735367 Accuracy 0.45068359375\n",
      "Iteration 7010 Training loss 0.053357940167188644 Validation loss 0.05466945841908455 Accuracy 0.44873046875\n",
      "Iteration 7020 Training loss 0.05575203523039818 Validation loss 0.05455542728304863 Accuracy 0.449462890625\n",
      "Iteration 7030 Training loss 0.051905207335948944 Validation loss 0.054444681853055954 Accuracy 0.450927734375\n",
      "Iteration 7040 Training loss 0.05625659227371216 Validation loss 0.05496533215045929 Accuracy 0.44580078125\n",
      "Iteration 7050 Training loss 0.05205204337835312 Validation loss 0.055061325430870056 Accuracy 0.44580078125\n",
      "Iteration 7060 Training loss 0.05588443577289581 Validation loss 0.05454738065600395 Accuracy 0.4501953125\n",
      "Iteration 7070 Training loss 0.05482925474643707 Validation loss 0.05472298711538315 Accuracy 0.44873046875\n",
      "Iteration 7080 Training loss 0.04908563569188118 Validation loss 0.054521575570106506 Accuracy 0.450439453125\n",
      "Iteration 7090 Training loss 0.053506460040807724 Validation loss 0.05543651059269905 Accuracy 0.44091796875\n",
      "Iteration 7100 Training loss 0.05308343097567558 Validation loss 0.05499792844057083 Accuracy 0.4453125\n",
      "Iteration 7110 Training loss 0.05167442187666893 Validation loss 0.05486894026398659 Accuracy 0.44677734375\n",
      "Iteration 7120 Training loss 0.05303404852747917 Validation loss 0.05462224408984184 Accuracy 0.44921875\n",
      "Iteration 7130 Training loss 0.05335244536399841 Validation loss 0.05483229458332062 Accuracy 0.447265625\n",
      "Iteration 7140 Training loss 0.051102831959724426 Validation loss 0.05481225997209549 Accuracy 0.447021484375\n",
      "Iteration 7150 Training loss 0.05383935570716858 Validation loss 0.054416291415691376 Accuracy 0.450927734375\n",
      "Iteration 7160 Training loss 0.0536336712539196 Validation loss 0.05450417846441269 Accuracy 0.4501953125\n",
      "Iteration 7170 Training loss 0.05401049926877022 Validation loss 0.05455191433429718 Accuracy 0.4501953125\n",
      "Iteration 7180 Training loss 0.05423664301633835 Validation loss 0.05456147342920303 Accuracy 0.4501953125\n",
      "Iteration 7190 Training loss 0.05892401188611984 Validation loss 0.05447518453001976 Accuracy 0.45068359375\n",
      "Iteration 7200 Training loss 0.052118875086307526 Validation loss 0.05461796745657921 Accuracy 0.44970703125\n",
      "Iteration 7210 Training loss 0.0544414259493351 Validation loss 0.05455984175205231 Accuracy 0.449951171875\n",
      "Iteration 7220 Training loss 0.05479638651013374 Validation loss 0.05495108291506767 Accuracy 0.446533203125\n",
      "Iteration 7230 Training loss 0.05388737842440605 Validation loss 0.05457675829529762 Accuracy 0.4501953125\n",
      "Iteration 7240 Training loss 0.05222362279891968 Validation loss 0.05451543629169464 Accuracy 0.449951171875\n",
      "Iteration 7250 Training loss 0.053588006645441055 Validation loss 0.05485304445028305 Accuracy 0.44677734375\n",
      "Iteration 7260 Training loss 0.05130447447299957 Validation loss 0.054765935987234116 Accuracy 0.447998046875\n",
      "Iteration 7270 Training loss 0.05094955489039421 Validation loss 0.05468978360295296 Accuracy 0.44873046875\n",
      "Iteration 7280 Training loss 0.05569276958703995 Validation loss 0.055082060396671295 Accuracy 0.445556640625\n",
      "Iteration 7290 Training loss 0.05345041677355766 Validation loss 0.054619211703538895 Accuracy 0.448974609375\n",
      "Iteration 7300 Training loss 0.05186714977025986 Validation loss 0.0545952133834362 Accuracy 0.449462890625\n",
      "Iteration 7310 Training loss 0.05317499861121178 Validation loss 0.0547296404838562 Accuracy 0.447998046875\n",
      "Iteration 7320 Training loss 0.052973825484514236 Validation loss 0.05457402765750885 Accuracy 0.449951171875\n",
      "Iteration 7330 Training loss 0.052519120275974274 Validation loss 0.054422128945589066 Accuracy 0.450927734375\n",
      "Iteration 7340 Training loss 0.05230333283543587 Validation loss 0.05443773418664932 Accuracy 0.45068359375\n",
      "Iteration 7350 Training loss 0.0560404397547245 Validation loss 0.05548334866762161 Accuracy 0.44140625\n",
      "Iteration 7360 Training loss 0.05238647758960724 Validation loss 0.05479409173130989 Accuracy 0.447509765625\n",
      "Iteration 7370 Training loss 0.05373515188694 Validation loss 0.0545085184276104 Accuracy 0.44921875\n",
      "Iteration 7380 Training loss 0.05570757016539574 Validation loss 0.056078020483255386 Accuracy 0.4345703125\n",
      "Iteration 7390 Training loss 0.056081369519233704 Validation loss 0.0543924979865551 Accuracy 0.450927734375\n",
      "Iteration 7400 Training loss 0.052481070160865784 Validation loss 0.05514928698539734 Accuracy 0.44384765625\n",
      "Iteration 7410 Training loss 0.05668771639466286 Validation loss 0.05442829802632332 Accuracy 0.451416015625\n",
      "Iteration 7420 Training loss 0.05700017139315605 Validation loss 0.054770808666944504 Accuracy 0.447509765625\n",
      "Iteration 7430 Training loss 0.05382806435227394 Validation loss 0.05480577051639557 Accuracy 0.447509765625\n",
      "Iteration 7440 Training loss 0.05341038480401039 Validation loss 0.05510321259498596 Accuracy 0.444580078125\n",
      "Iteration 7450 Training loss 0.05658314377069473 Validation loss 0.054684560745954514 Accuracy 0.44873046875\n",
      "Iteration 7460 Training loss 0.053817782551050186 Validation loss 0.0548064298927784 Accuracy 0.447998046875\n",
      "Iteration 7470 Training loss 0.054984189569950104 Validation loss 0.05463963747024536 Accuracy 0.44873046875\n",
      "Iteration 7480 Training loss 0.054543476551771164 Validation loss 0.05469880625605583 Accuracy 0.44873046875\n",
      "Iteration 7490 Training loss 0.0532284677028656 Validation loss 0.054654598236083984 Accuracy 0.44873046875\n",
      "Iteration 7500 Training loss 0.04972875490784645 Validation loss 0.054380301386117935 Accuracy 0.450439453125\n",
      "Iteration 7510 Training loss 0.05320997163653374 Validation loss 0.054472967982292175 Accuracy 0.45068359375\n",
      "Iteration 7520 Training loss 0.05267373099923134 Validation loss 0.05472061410546303 Accuracy 0.448486328125\n",
      "Iteration 7530 Training loss 0.0553593784570694 Validation loss 0.05455160140991211 Accuracy 0.44970703125\n",
      "Iteration 7540 Training loss 0.053434956818819046 Validation loss 0.054772768169641495 Accuracy 0.44775390625\n",
      "Iteration 7550 Training loss 0.05411737039685249 Validation loss 0.05458579584956169 Accuracy 0.44970703125\n",
      "Iteration 7560 Training loss 0.05439586192369461 Validation loss 0.05462997034192085 Accuracy 0.448974609375\n",
      "Iteration 7570 Training loss 0.05057203024625778 Validation loss 0.05444875732064247 Accuracy 0.4501953125\n",
      "Iteration 7580 Training loss 0.05546202510595322 Validation loss 0.05519898608326912 Accuracy 0.443603515625\n",
      "Iteration 7590 Training loss 0.0493600107729435 Validation loss 0.05449029058218002 Accuracy 0.45068359375\n",
      "Iteration 7600 Training loss 0.05711380019783974 Validation loss 0.05477418750524521 Accuracy 0.44775390625\n",
      "Iteration 7610 Training loss 0.05336863547563553 Validation loss 0.05469633638858795 Accuracy 0.447998046875\n",
      "Iteration 7620 Training loss 0.05339416489005089 Validation loss 0.05483613163232803 Accuracy 0.447509765625\n",
      "Iteration 7630 Training loss 0.0550592802464962 Validation loss 0.05491303652524948 Accuracy 0.446533203125\n",
      "Iteration 7640 Training loss 0.05154610052704811 Validation loss 0.05450543016195297 Accuracy 0.44970703125\n",
      "Iteration 7650 Training loss 0.05024862661957741 Validation loss 0.054646726697683334 Accuracy 0.448974609375\n",
      "Iteration 7660 Training loss 0.05497952178120613 Validation loss 0.05448049679398537 Accuracy 0.450439453125\n",
      "Iteration 7670 Training loss 0.05784333124756813 Validation loss 0.05492284148931503 Accuracy 0.4462890625\n",
      "Iteration 7680 Training loss 0.057050954550504684 Validation loss 0.05558393523097038 Accuracy 0.43896484375\n",
      "Iteration 7690 Training loss 0.05269474908709526 Validation loss 0.054747287184000015 Accuracy 0.448486328125\n",
      "Iteration 7700 Training loss 0.052759766578674316 Validation loss 0.05481487140059471 Accuracy 0.447021484375\n",
      "Iteration 7710 Training loss 0.05224835127592087 Validation loss 0.05451511591672897 Accuracy 0.449951171875\n",
      "Iteration 7720 Training loss 0.05091795697808266 Validation loss 0.05455746874213219 Accuracy 0.449951171875\n",
      "Iteration 7730 Training loss 0.04996464028954506 Validation loss 0.05470605939626694 Accuracy 0.448486328125\n",
      "Iteration 7740 Training loss 0.054352015256881714 Validation loss 0.05456669628620148 Accuracy 0.449951171875\n",
      "Iteration 7750 Training loss 0.05230787768959999 Validation loss 0.054441049695014954 Accuracy 0.450439453125\n",
      "Iteration 7760 Training loss 0.05558750405907631 Validation loss 0.055260077118873596 Accuracy 0.44384765625\n",
      "Iteration 7770 Training loss 0.053356148302555084 Validation loss 0.05519074946641922 Accuracy 0.443603515625\n",
      "Iteration 7780 Training loss 0.05184498056769371 Validation loss 0.05460619926452637 Accuracy 0.449951171875\n",
      "Iteration 7790 Training loss 0.052416421473026276 Validation loss 0.05465494096279144 Accuracy 0.44921875\n",
      "Iteration 7800 Training loss 0.055495187640190125 Validation loss 0.05465066432952881 Accuracy 0.449462890625\n",
      "Iteration 7810 Training loss 0.05371802672743797 Validation loss 0.05485689640045166 Accuracy 0.447021484375\n",
      "Iteration 7820 Training loss 0.052784036844968796 Validation loss 0.05468514934182167 Accuracy 0.44873046875\n",
      "Iteration 7830 Training loss 0.05227581784129143 Validation loss 0.05452318862080574 Accuracy 0.44970703125\n",
      "Iteration 7840 Training loss 0.05337570980191231 Validation loss 0.05435076355934143 Accuracy 0.451416015625\n",
      "Iteration 7850 Training loss 0.05616988241672516 Validation loss 0.054740067571401596 Accuracy 0.448486328125\n",
      "Iteration 7860 Training loss 0.05571453645825386 Validation loss 0.05468881130218506 Accuracy 0.448486328125\n",
      "Iteration 7870 Training loss 0.05261284485459328 Validation loss 0.05519857257604599 Accuracy 0.44384765625\n",
      "Iteration 7880 Training loss 0.05525129660964012 Validation loss 0.05492812395095825 Accuracy 0.446533203125\n",
      "Iteration 7890 Training loss 0.05594126507639885 Validation loss 0.05527644231915474 Accuracy 0.4423828125\n",
      "Iteration 7900 Training loss 0.051494915038347244 Validation loss 0.054410420358181 Accuracy 0.451416015625\n",
      "Iteration 7910 Training loss 0.05536032095551491 Validation loss 0.05458902195096016 Accuracy 0.4501953125\n",
      "Iteration 7920 Training loss 0.052259642630815506 Validation loss 0.0545973926782608 Accuracy 0.449462890625\n",
      "Iteration 7930 Training loss 0.05712010711431503 Validation loss 0.054679080843925476 Accuracy 0.448486328125\n",
      "Iteration 7940 Training loss 0.05640018731355667 Validation loss 0.0545848049223423 Accuracy 0.4501953125\n",
      "Iteration 7950 Training loss 0.055203408002853394 Validation loss 0.054885417222976685 Accuracy 0.447265625\n",
      "Iteration 7960 Training loss 0.05144932121038437 Validation loss 0.054441019892692566 Accuracy 0.451171875\n",
      "Iteration 7970 Training loss 0.054061319679021835 Validation loss 0.05440933629870415 Accuracy 0.450927734375\n",
      "Iteration 7980 Training loss 0.05172913521528244 Validation loss 0.05516324192285538 Accuracy 0.44482421875\n",
      "Iteration 7990 Training loss 0.054300617426633835 Validation loss 0.0545615628361702 Accuracy 0.44970703125\n",
      "Iteration 8000 Training loss 0.054951414465904236 Validation loss 0.054855167865753174 Accuracy 0.447265625\n",
      "Iteration 8010 Training loss 0.056047528982162476 Validation loss 0.054596420377492905 Accuracy 0.44970703125\n",
      "Iteration 8020 Training loss 0.053084056824445724 Validation loss 0.05460173636674881 Accuracy 0.4501953125\n",
      "Iteration 8030 Training loss 0.05292539298534393 Validation loss 0.05441498011350632 Accuracy 0.451904296875\n",
      "Iteration 8040 Training loss 0.056997690349817276 Validation loss 0.0545855276286602 Accuracy 0.4501953125\n",
      "Iteration 8050 Training loss 0.053391601890325546 Validation loss 0.054441824555397034 Accuracy 0.451416015625\n",
      "Iteration 8060 Training loss 0.05122557654976845 Validation loss 0.055262234061956406 Accuracy 0.44287109375\n",
      "Iteration 8070 Training loss 0.05459356680512428 Validation loss 0.054422467947006226 Accuracy 0.451904296875\n",
      "Iteration 8080 Training loss 0.0523129366338253 Validation loss 0.055566128343343735 Accuracy 0.440673828125\n",
      "Iteration 8090 Training loss 0.050159208476543427 Validation loss 0.05457547307014465 Accuracy 0.44970703125\n",
      "Iteration 8100 Training loss 0.05074619501829147 Validation loss 0.05443216487765312 Accuracy 0.451416015625\n",
      "Iteration 8110 Training loss 0.05275508016347885 Validation loss 0.05448077991604805 Accuracy 0.450927734375\n",
      "Iteration 8120 Training loss 0.05287926271557808 Validation loss 0.05522117018699646 Accuracy 0.444580078125\n",
      "Iteration 8130 Training loss 0.05376029387116432 Validation loss 0.05450441315770149 Accuracy 0.450927734375\n",
      "Iteration 8140 Training loss 0.05486088991165161 Validation loss 0.05459367111325264 Accuracy 0.4501953125\n",
      "Iteration 8150 Training loss 0.05151066929101944 Validation loss 0.054839882999658585 Accuracy 0.447509765625\n",
      "Iteration 8160 Training loss 0.052914030849933624 Validation loss 0.05498324707150459 Accuracy 0.44580078125\n",
      "Iteration 8170 Training loss 0.05151718109846115 Validation loss 0.0545310340821743 Accuracy 0.450439453125\n",
      "Iteration 8180 Training loss 0.057340189814567566 Validation loss 0.054803088307380676 Accuracy 0.44775390625\n",
      "Iteration 8190 Training loss 0.053945962339639664 Validation loss 0.0556357204914093 Accuracy 0.439697265625\n",
      "Iteration 8200 Training loss 0.05425248295068741 Validation loss 0.05446137860417366 Accuracy 0.450927734375\n",
      "Iteration 8210 Training loss 0.056140534579753876 Validation loss 0.05510275065898895 Accuracy 0.444580078125\n",
      "Iteration 8220 Training loss 0.05337176471948624 Validation loss 0.05488502234220505 Accuracy 0.44677734375\n",
      "Iteration 8230 Training loss 0.049528155475854874 Validation loss 0.055269256234169006 Accuracy 0.442626953125\n",
      "Iteration 8240 Training loss 0.05233628675341606 Validation loss 0.054505687206983566 Accuracy 0.450927734375\n",
      "Iteration 8250 Training loss 0.052013009786605835 Validation loss 0.054486896842718124 Accuracy 0.450927734375\n",
      "Iteration 8260 Training loss 0.05198463052511215 Validation loss 0.05497957766056061 Accuracy 0.44580078125\n",
      "Iteration 8270 Training loss 0.05286990478634834 Validation loss 0.05474323406815529 Accuracy 0.4482421875\n",
      "Iteration 8280 Training loss 0.04884379357099533 Validation loss 0.05472378805279732 Accuracy 0.448974609375\n",
      "Iteration 8290 Training loss 0.05462273955345154 Validation loss 0.05502621829509735 Accuracy 0.44580078125\n",
      "Iteration 8300 Training loss 0.05287434905767441 Validation loss 0.0543488934636116 Accuracy 0.451904296875\n",
      "Iteration 8310 Training loss 0.052847687155008316 Validation loss 0.05506797879934311 Accuracy 0.4453125\n",
      "Iteration 8320 Training loss 0.054490670561790466 Validation loss 0.05489491671323776 Accuracy 0.44677734375\n",
      "Iteration 8330 Training loss 0.054158248007297516 Validation loss 0.054746296256780624 Accuracy 0.44775390625\n",
      "Iteration 8340 Training loss 0.052871450781822205 Validation loss 0.05506819859147072 Accuracy 0.4453125\n",
      "Iteration 8350 Training loss 0.05253233760595322 Validation loss 0.054888032376766205 Accuracy 0.4462890625\n",
      "Iteration 8360 Training loss 0.05429939553141594 Validation loss 0.054645244032144547 Accuracy 0.44970703125\n",
      "Iteration 8370 Training loss 0.0541837140917778 Validation loss 0.05443178489804268 Accuracy 0.451904296875\n",
      "Iteration 8380 Training loss 0.0544775128364563 Validation loss 0.05495217815041542 Accuracy 0.44677734375\n",
      "Iteration 8390 Training loss 0.05394693464040756 Validation loss 0.05469833314418793 Accuracy 0.44873046875\n",
      "Iteration 8400 Training loss 0.053270481526851654 Validation loss 0.054627303034067154 Accuracy 0.44921875\n",
      "Iteration 8410 Training loss 0.05350560322403908 Validation loss 0.05459108576178551 Accuracy 0.449951171875\n",
      "Iteration 8420 Training loss 0.054592233151197433 Validation loss 0.05456205829977989 Accuracy 0.45068359375\n",
      "Iteration 8430 Training loss 0.05554044619202614 Validation loss 0.05454022437334061 Accuracy 0.450439453125\n",
      "Iteration 8440 Training loss 0.05204751715064049 Validation loss 0.05441909655928612 Accuracy 0.451416015625\n",
      "Iteration 8450 Training loss 0.05198954790830612 Validation loss 0.05462052673101425 Accuracy 0.44970703125\n",
      "Iteration 8460 Training loss 0.05116720125079155 Validation loss 0.054449662566185 Accuracy 0.45166015625\n",
      "Iteration 8470 Training loss 0.055513422936201096 Validation loss 0.05449290946125984 Accuracy 0.45068359375\n",
      "Iteration 8480 Training loss 0.05092647671699524 Validation loss 0.05467652529478073 Accuracy 0.449462890625\n",
      "Iteration 8490 Training loss 0.05490075796842575 Validation loss 0.05462208017706871 Accuracy 0.449462890625\n",
      "Iteration 8500 Training loss 0.055472493171691895 Validation loss 0.05475688353180885 Accuracy 0.447998046875\n",
      "Iteration 8510 Training loss 0.04948233813047409 Validation loss 0.05474204942584038 Accuracy 0.4482421875\n",
      "Iteration 8520 Training loss 0.04954460635781288 Validation loss 0.05478760600090027 Accuracy 0.447998046875\n",
      "Iteration 8530 Training loss 0.05569232255220413 Validation loss 0.054359547793865204 Accuracy 0.452392578125\n",
      "Iteration 8540 Training loss 0.056048452854156494 Validation loss 0.05420892313122749 Accuracy 0.45361328125\n",
      "Iteration 8550 Training loss 0.054548848420381546 Validation loss 0.05531088262796402 Accuracy 0.442138671875\n",
      "Iteration 8560 Training loss 0.05565346032381058 Validation loss 0.05514605715870857 Accuracy 0.44384765625\n",
      "Iteration 8570 Training loss 0.053494229912757874 Validation loss 0.055549778044223785 Accuracy 0.43994140625\n",
      "Iteration 8580 Training loss 0.050936259329319 Validation loss 0.054638031870126724 Accuracy 0.449462890625\n",
      "Iteration 8590 Training loss 0.052185606211423874 Validation loss 0.05425851419568062 Accuracy 0.452880859375\n",
      "Iteration 8600 Training loss 0.056626882404088974 Validation loss 0.054856665432453156 Accuracy 0.4462890625\n",
      "Iteration 8610 Training loss 0.05618806928396225 Validation loss 0.05546218529343605 Accuracy 0.4404296875\n",
      "Iteration 8620 Training loss 0.05334493890404701 Validation loss 0.054654255509376526 Accuracy 0.44921875\n",
      "Iteration 8630 Training loss 0.05238235369324684 Validation loss 0.05458345636725426 Accuracy 0.4501953125\n",
      "Iteration 8640 Training loss 0.05126457288861275 Validation loss 0.05456883832812309 Accuracy 0.4501953125\n",
      "Iteration 8650 Training loss 0.05553564429283142 Validation loss 0.0547020398080349 Accuracy 0.44873046875\n",
      "Iteration 8660 Training loss 0.049317698925733566 Validation loss 0.05444589629769325 Accuracy 0.45068359375\n",
      "Iteration 8670 Training loss 0.0544845387339592 Validation loss 0.054395996034145355 Accuracy 0.451416015625\n",
      "Iteration 8680 Training loss 0.052240680903196335 Validation loss 0.05446356162428856 Accuracy 0.451171875\n",
      "Iteration 8690 Training loss 0.05383172631263733 Validation loss 0.05446016043424606 Accuracy 0.45068359375\n",
      "Iteration 8700 Training loss 0.0556296743452549 Validation loss 0.05459083989262581 Accuracy 0.449951171875\n",
      "Iteration 8710 Training loss 0.05431792140007019 Validation loss 0.05460924282670021 Accuracy 0.448974609375\n",
      "Iteration 8720 Training loss 0.05673784390091896 Validation loss 0.054532792419195175 Accuracy 0.45068359375\n",
      "Iteration 8730 Training loss 0.05271701514720917 Validation loss 0.05434083193540573 Accuracy 0.4521484375\n",
      "Iteration 8740 Training loss 0.05731958895921707 Validation loss 0.05499167740345001 Accuracy 0.44580078125\n",
      "Iteration 8750 Training loss 0.056228432804346085 Validation loss 0.05430154129862785 Accuracy 0.45263671875\n",
      "Iteration 8760 Training loss 0.051931705325841904 Validation loss 0.05464991554617882 Accuracy 0.44921875\n",
      "Iteration 8770 Training loss 0.05206666141748428 Validation loss 0.054533787071704865 Accuracy 0.4501953125\n",
      "Iteration 8780 Training loss 0.05358400568366051 Validation loss 0.05447884649038315 Accuracy 0.451171875\n",
      "Iteration 8790 Training loss 0.05315970629453659 Validation loss 0.05513952672481537 Accuracy 0.444580078125\n",
      "Iteration 8800 Training loss 0.05280183255672455 Validation loss 0.054676029831171036 Accuracy 0.44970703125\n",
      "Iteration 8810 Training loss 0.05457974225282669 Validation loss 0.054695915430784225 Accuracy 0.44921875\n",
      "Iteration 8820 Training loss 0.054654449224472046 Validation loss 0.05444394424557686 Accuracy 0.451416015625\n",
      "Iteration 8830 Training loss 0.05268286168575287 Validation loss 0.05476558580994606 Accuracy 0.447998046875\n",
      "Iteration 8840 Training loss 0.05377596616744995 Validation loss 0.054823026061058044 Accuracy 0.447265625\n",
      "Iteration 8850 Training loss 0.05216584354639053 Validation loss 0.05503081902861595 Accuracy 0.44580078125\n",
      "Iteration 8860 Training loss 0.0548868365585804 Validation loss 0.05456843599677086 Accuracy 0.44970703125\n",
      "Iteration 8870 Training loss 0.05321456864476204 Validation loss 0.05444762483239174 Accuracy 0.45068359375\n",
      "Iteration 8880 Training loss 0.05750550702214241 Validation loss 0.05427611991763115 Accuracy 0.452880859375\n",
      "Iteration 8890 Training loss 0.05516105145215988 Validation loss 0.05471773445606232 Accuracy 0.448486328125\n",
      "Iteration 8900 Training loss 0.05320294573903084 Validation loss 0.05461357906460762 Accuracy 0.449462890625\n",
      "Iteration 8910 Training loss 0.053747568279504776 Validation loss 0.05479617044329643 Accuracy 0.447998046875\n",
      "Iteration 8920 Training loss 0.0578249953687191 Validation loss 0.05484643578529358 Accuracy 0.447021484375\n",
      "Iteration 8930 Training loss 0.05198381468653679 Validation loss 0.054545145481824875 Accuracy 0.450439453125\n",
      "Iteration 8940 Training loss 0.053817667067050934 Validation loss 0.05440094694495201 Accuracy 0.451904296875\n",
      "Iteration 8950 Training loss 0.05659746751189232 Validation loss 0.056529730558395386 Accuracy 0.4306640625\n",
      "Iteration 8960 Training loss 0.05196453630924225 Validation loss 0.054515283554792404 Accuracy 0.45068359375\n",
      "Iteration 8970 Training loss 0.055565860122442245 Validation loss 0.054430458694696426 Accuracy 0.45166015625\n",
      "Iteration 8980 Training loss 0.05232373997569084 Validation loss 0.054493408650159836 Accuracy 0.4501953125\n",
      "Iteration 8990 Training loss 0.051877349615097046 Validation loss 0.05448086932301521 Accuracy 0.45068359375\n",
      "Iteration 9000 Training loss 0.0529438778758049 Validation loss 0.05456225574016571 Accuracy 0.449951171875\n",
      "Iteration 9010 Training loss 0.053565703332424164 Validation loss 0.05431215837597847 Accuracy 0.45263671875\n",
      "Iteration 9020 Training loss 0.04953472688794136 Validation loss 0.05439424142241478 Accuracy 0.45166015625\n",
      "Iteration 9030 Training loss 0.057084567844867706 Validation loss 0.05516262724995613 Accuracy 0.443115234375\n",
      "Iteration 9040 Training loss 0.05517945438623428 Validation loss 0.054557282477617264 Accuracy 0.450439453125\n",
      "Iteration 9050 Training loss 0.058968719094991684 Validation loss 0.05461188033223152 Accuracy 0.44970703125\n",
      "Iteration 9060 Training loss 0.056323565542697906 Validation loss 0.05492729693651199 Accuracy 0.44580078125\n",
      "Iteration 9070 Training loss 0.05333402380347252 Validation loss 0.05553895980119705 Accuracy 0.439697265625\n",
      "Iteration 9080 Training loss 0.054308440536260605 Validation loss 0.054348524659872055 Accuracy 0.4521484375\n",
      "Iteration 9090 Training loss 0.051674265414476395 Validation loss 0.05447530001401901 Accuracy 0.45068359375\n",
      "Iteration 9100 Training loss 0.05392178148031235 Validation loss 0.05436163395643234 Accuracy 0.4521484375\n",
      "Iteration 9110 Training loss 0.05121636763215065 Validation loss 0.054314035922288895 Accuracy 0.451904296875\n",
      "Iteration 9120 Training loss 0.05511452630162239 Validation loss 0.054334040731191635 Accuracy 0.45166015625\n",
      "Iteration 9130 Training loss 0.05230867862701416 Validation loss 0.05473112314939499 Accuracy 0.4482421875\n",
      "Iteration 9140 Training loss 0.05298660323023796 Validation loss 0.0545073039829731 Accuracy 0.4501953125\n",
      "Iteration 9150 Training loss 0.05795609578490257 Validation loss 0.05509137734770775 Accuracy 0.444580078125\n",
      "Iteration 9160 Training loss 0.05537039041519165 Validation loss 0.05450265482068062 Accuracy 0.451416015625\n",
      "Iteration 9170 Training loss 0.05438237264752388 Validation loss 0.05465427413582802 Accuracy 0.44970703125\n",
      "Iteration 9180 Training loss 0.05367090180516243 Validation loss 0.054332125931978226 Accuracy 0.451904296875\n",
      "Iteration 9190 Training loss 0.053906746208667755 Validation loss 0.05540847033262253 Accuracy 0.44287109375\n",
      "Iteration 9200 Training loss 0.05544675514101982 Validation loss 0.05447714403271675 Accuracy 0.451416015625\n",
      "Iteration 9210 Training loss 0.05716345086693764 Validation loss 0.05455079302191734 Accuracy 0.450439453125\n",
      "Iteration 9220 Training loss 0.05176495388150215 Validation loss 0.054852575063705444 Accuracy 0.447021484375\n",
      "Iteration 9230 Training loss 0.053239401429891586 Validation loss 0.0545555017888546 Accuracy 0.45068359375\n",
      "Iteration 9240 Training loss 0.05224309116601944 Validation loss 0.0543489009141922 Accuracy 0.452392578125\n",
      "Iteration 9250 Training loss 0.05227041617035866 Validation loss 0.05499096214771271 Accuracy 0.446044921875\n",
      "Iteration 9260 Training loss 0.051664773374795914 Validation loss 0.0543399415910244 Accuracy 0.45263671875\n",
      "Iteration 9270 Training loss 0.05347433313727379 Validation loss 0.05438164994120598 Accuracy 0.45263671875\n",
      "Iteration 9280 Training loss 0.0522008053958416 Validation loss 0.05444303900003433 Accuracy 0.4521484375\n",
      "Iteration 9290 Training loss 0.05316644906997681 Validation loss 0.054347675293684006 Accuracy 0.452392578125\n",
      "Iteration 9300 Training loss 0.051878441125154495 Validation loss 0.05430012196302414 Accuracy 0.453125\n",
      "Iteration 9310 Training loss 0.05343575030565262 Validation loss 0.054541975259780884 Accuracy 0.4501953125\n",
      "Iteration 9320 Training loss 0.055352579802274704 Validation loss 0.054189492017030716 Accuracy 0.453369140625\n",
      "Iteration 9330 Training loss 0.053395841270685196 Validation loss 0.05422857403755188 Accuracy 0.453369140625\n",
      "Iteration 9340 Training loss 0.05536619573831558 Validation loss 0.05445532128214836 Accuracy 0.451904296875\n",
      "Iteration 9350 Training loss 0.05265815183520317 Validation loss 0.05442994832992554 Accuracy 0.451904296875\n",
      "Iteration 9360 Training loss 0.04755097255110741 Validation loss 0.05419445037841797 Accuracy 0.45361328125\n",
      "Iteration 9370 Training loss 0.051608119159936905 Validation loss 0.054705940186977386 Accuracy 0.447998046875\n",
      "Iteration 9380 Training loss 0.055003855377435684 Validation loss 0.054401155561208725 Accuracy 0.451904296875\n",
      "Iteration 9390 Training loss 0.053141310811042786 Validation loss 0.05419265478849411 Accuracy 0.45361328125\n",
      "Iteration 9400 Training loss 0.05287652090191841 Validation loss 0.05427328497171402 Accuracy 0.45263671875\n",
      "Iteration 9410 Training loss 0.053810760378837585 Validation loss 0.05465441197156906 Accuracy 0.448974609375\n",
      "Iteration 9420 Training loss 0.0500333234667778 Validation loss 0.05434532091021538 Accuracy 0.45263671875\n",
      "Iteration 9430 Training loss 0.05457335710525513 Validation loss 0.054559968411922455 Accuracy 0.44970703125\n",
      "Iteration 9440 Training loss 0.05516451969742775 Validation loss 0.05447005853056908 Accuracy 0.45068359375\n",
      "Iteration 9450 Training loss 0.050985515117645264 Validation loss 0.054400525987148285 Accuracy 0.451904296875\n",
      "Iteration 9460 Training loss 0.05191199854016304 Validation loss 0.054615359753370285 Accuracy 0.44921875\n",
      "Iteration 9470 Training loss 0.05373471602797508 Validation loss 0.05439845100045204 Accuracy 0.451416015625\n",
      "Iteration 9480 Training loss 0.05135316029191017 Validation loss 0.05461438372731209 Accuracy 0.449951171875\n",
      "Iteration 9490 Training loss 0.05308579280972481 Validation loss 0.05427846685051918 Accuracy 0.4521484375\n",
      "Iteration 9500 Training loss 0.05353446304798126 Validation loss 0.05427763611078262 Accuracy 0.4521484375\n",
      "Iteration 9510 Training loss 0.051186010241508484 Validation loss 0.05513991042971611 Accuracy 0.443603515625\n",
      "Iteration 9520 Training loss 0.05621779337525368 Validation loss 0.054574254900217056 Accuracy 0.44970703125\n",
      "Iteration 9530 Training loss 0.05481000244617462 Validation loss 0.05478510633111 Accuracy 0.447265625\n",
      "Iteration 9540 Training loss 0.05635532736778259 Validation loss 0.05475962907075882 Accuracy 0.447265625\n",
      "Iteration 9550 Training loss 0.04871262237429619 Validation loss 0.05491869896650314 Accuracy 0.4453125\n",
      "Iteration 9560 Training loss 0.050403203815221786 Validation loss 0.05417861044406891 Accuracy 0.453125\n",
      "Iteration 9570 Training loss 0.05347108840942383 Validation loss 0.05429728701710701 Accuracy 0.451904296875\n",
      "Iteration 9580 Training loss 0.05328315123915672 Validation loss 0.05507200211286545 Accuracy 0.444580078125\n",
      "Iteration 9590 Training loss 0.05488485097885132 Validation loss 0.05422130599617958 Accuracy 0.453369140625\n",
      "Iteration 9600 Training loss 0.05419156700372696 Validation loss 0.054883453994989395 Accuracy 0.44580078125\n",
      "Iteration 9610 Training loss 0.05380437150597572 Validation loss 0.05421224981546402 Accuracy 0.45263671875\n",
      "Iteration 9620 Training loss 0.05473225191235542 Validation loss 0.05454391613602638 Accuracy 0.4501953125\n",
      "Iteration 9630 Training loss 0.05411570519208908 Validation loss 0.054323870688676834 Accuracy 0.451904296875\n",
      "Iteration 9640 Training loss 0.051271453499794006 Validation loss 0.05422573164105415 Accuracy 0.453125\n",
      "Iteration 9650 Training loss 0.0552232451736927 Validation loss 0.05436895415186882 Accuracy 0.451416015625\n",
      "Iteration 9660 Training loss 0.05387083813548088 Validation loss 0.054866574704647064 Accuracy 0.446533203125\n",
      "Iteration 9670 Training loss 0.05391734093427658 Validation loss 0.05439532920718193 Accuracy 0.451904296875\n",
      "Iteration 9680 Training loss 0.05328059941530228 Validation loss 0.05435946211218834 Accuracy 0.451416015625\n",
      "Iteration 9690 Training loss 0.05607631430029869 Validation loss 0.05528632551431656 Accuracy 0.4423828125\n",
      "Iteration 9700 Training loss 0.05569528788328171 Validation loss 0.054286424070596695 Accuracy 0.45263671875\n",
      "Iteration 9710 Training loss 0.05375070124864578 Validation loss 0.056024979799985886 Accuracy 0.434814453125\n",
      "Iteration 9720 Training loss 0.055965255945920944 Validation loss 0.05433044955134392 Accuracy 0.451904296875\n",
      "Iteration 9730 Training loss 0.050501372665166855 Validation loss 0.0542525090277195 Accuracy 0.452880859375\n",
      "Iteration 9740 Training loss 0.053942564874887466 Validation loss 0.054730720818042755 Accuracy 0.447998046875\n",
      "Iteration 9750 Training loss 0.053580496460199356 Validation loss 0.05513881519436836 Accuracy 0.444091796875\n",
      "Iteration 9760 Training loss 0.05412079766392708 Validation loss 0.05420118197798729 Accuracy 0.453369140625\n",
      "Iteration 9770 Training loss 0.05230680853128433 Validation loss 0.05450064688920975 Accuracy 0.45068359375\n",
      "Iteration 9780 Training loss 0.05142482370138168 Validation loss 0.05484117195010185 Accuracy 0.447265625\n",
      "Iteration 9790 Training loss 0.05100123956799507 Validation loss 0.05457581952214241 Accuracy 0.44970703125\n",
      "Iteration 9800 Training loss 0.05486295372247696 Validation loss 0.05470769479870796 Accuracy 0.448974609375\n",
      "Iteration 9810 Training loss 0.05759584531188011 Validation loss 0.05446556583046913 Accuracy 0.45068359375\n",
      "Iteration 9820 Training loss 0.05141454562544823 Validation loss 0.0546724796295166 Accuracy 0.448486328125\n",
      "Iteration 9830 Training loss 0.05145036801695824 Validation loss 0.05452793836593628 Accuracy 0.4501953125\n",
      "Iteration 9840 Training loss 0.05042015388607979 Validation loss 0.05456739291548729 Accuracy 0.449951171875\n",
      "Iteration 9850 Training loss 0.054707009345293045 Validation loss 0.05494634807109833 Accuracy 0.446044921875\n",
      "Iteration 9860 Training loss 0.0551326684653759 Validation loss 0.054853543639183044 Accuracy 0.447265625\n",
      "Iteration 9870 Training loss 0.053096164017915726 Validation loss 0.054379694163799286 Accuracy 0.451904296875\n",
      "Iteration 9880 Training loss 0.053500834852457047 Validation loss 0.05462299659848213 Accuracy 0.44921875\n",
      "Iteration 9890 Training loss 0.05549042671918869 Validation loss 0.054953210055828094 Accuracy 0.44580078125\n",
      "Iteration 9900 Training loss 0.051927972584962845 Validation loss 0.05427565425634384 Accuracy 0.452392578125\n",
      "Iteration 9910 Training loss 0.052082356065511703 Validation loss 0.054293371737003326 Accuracy 0.452392578125\n",
      "Iteration 9920 Training loss 0.05256756395101547 Validation loss 0.05456586927175522 Accuracy 0.44970703125\n",
      "Iteration 9930 Training loss 0.054411377757787704 Validation loss 0.05436750873923302 Accuracy 0.45166015625\n",
      "Iteration 9940 Training loss 0.055071961134672165 Validation loss 0.05470554158091545 Accuracy 0.448486328125\n",
      "Iteration 9950 Training loss 0.052513398230075836 Validation loss 0.05459590628743172 Accuracy 0.449462890625\n",
      "Iteration 9960 Training loss 0.051963403820991516 Validation loss 0.05428396165370941 Accuracy 0.453125\n",
      "Iteration 9970 Training loss 0.05534978583455086 Validation loss 0.0541960746049881 Accuracy 0.453369140625\n",
      "Iteration 9980 Training loss 0.05217829346656799 Validation loss 0.05419027805328369 Accuracy 0.45361328125\n",
      "Iteration 9990 Training loss 0.05482739582657814 Validation loss 0.05473501235246658 Accuracy 0.448486328125\n",
      "Iteration 10000 Training loss 0.050513263791799545 Validation loss 0.05436282977461815 Accuracy 0.451416015625\n",
      "Iteration 10010 Training loss 0.05389038100838661 Validation loss 0.05451151356101036 Accuracy 0.449951171875\n",
      "Iteration 10020 Training loss 0.054309599101543427 Validation loss 0.054429832845926285 Accuracy 0.45166015625\n",
      "Iteration 10030 Training loss 0.0516153909265995 Validation loss 0.05427232384681702 Accuracy 0.453125\n",
      "Iteration 10040 Training loss 0.051637083292007446 Validation loss 0.05460420250892639 Accuracy 0.449462890625\n",
      "Iteration 10050 Training loss 0.054506994783878326 Validation loss 0.05436831712722778 Accuracy 0.45166015625\n",
      "Iteration 10060 Training loss 0.0555536188185215 Validation loss 0.055465515702962875 Accuracy 0.4404296875\n",
      "Iteration 10070 Training loss 0.052810754626989365 Validation loss 0.05420481786131859 Accuracy 0.452880859375\n",
      "Iteration 10080 Training loss 0.055273182690143585 Validation loss 0.05452683940529823 Accuracy 0.45068359375\n",
      "Iteration 10090 Training loss 0.05309324711561203 Validation loss 0.054392941296100616 Accuracy 0.452392578125\n",
      "Iteration 10100 Training loss 0.05315222963690758 Validation loss 0.05473048612475395 Accuracy 0.44873046875\n",
      "Iteration 10110 Training loss 0.05359800159931183 Validation loss 0.05455895513296127 Accuracy 0.449462890625\n",
      "Iteration 10120 Training loss 0.04949279874563217 Validation loss 0.05446755141019821 Accuracy 0.451416015625\n",
      "Iteration 10130 Training loss 0.05555889755487442 Validation loss 0.0544687882065773 Accuracy 0.450439453125\n",
      "Iteration 10140 Training loss 0.052102670073509216 Validation loss 0.054222702980041504 Accuracy 0.45361328125\n",
      "Iteration 10150 Training loss 0.05443504825234413 Validation loss 0.054097097367048264 Accuracy 0.454833984375\n",
      "Iteration 10160 Training loss 0.05408543348312378 Validation loss 0.05410773307085037 Accuracy 0.45458984375\n",
      "Iteration 10170 Training loss 0.05574607104063034 Validation loss 0.05424213781952858 Accuracy 0.453369140625\n",
      "Iteration 10180 Training loss 0.052081186324357986 Validation loss 0.05460532009601593 Accuracy 0.448974609375\n",
      "Iteration 10190 Training loss 0.05302727594971657 Validation loss 0.05419475585222244 Accuracy 0.453369140625\n",
      "Iteration 10200 Training loss 0.05325239151716232 Validation loss 0.05438912287354469 Accuracy 0.451416015625\n",
      "Iteration 10210 Training loss 0.051203664392232895 Validation loss 0.05461829900741577 Accuracy 0.44873046875\n",
      "Iteration 10220 Training loss 0.05340051278471947 Validation loss 0.05441927909851074 Accuracy 0.45068359375\n",
      "Iteration 10230 Training loss 0.0532478429377079 Validation loss 0.05405706912279129 Accuracy 0.45556640625\n",
      "Iteration 10240 Training loss 0.052728261798620224 Validation loss 0.054234690964221954 Accuracy 0.45361328125\n",
      "Iteration 10250 Training loss 0.054545916616916656 Validation loss 0.05522119998931885 Accuracy 0.44384765625\n",
      "Iteration 10260 Training loss 0.04953019693493843 Validation loss 0.05415613576769829 Accuracy 0.453857421875\n",
      "Iteration 10270 Training loss 0.05714685097336769 Validation loss 0.054408587515354156 Accuracy 0.451416015625\n",
      "Iteration 10280 Training loss 0.05240140110254288 Validation loss 0.054325517266988754 Accuracy 0.452880859375\n",
      "Iteration 10290 Training loss 0.05212678015232086 Validation loss 0.05413935333490372 Accuracy 0.454833984375\n",
      "Iteration 10300 Training loss 0.05524677410721779 Validation loss 0.054682645946741104 Accuracy 0.4482421875\n",
      "Iteration 10310 Training loss 0.05368185043334961 Validation loss 0.05428459867835045 Accuracy 0.452392578125\n",
      "Iteration 10320 Training loss 0.05329526588320732 Validation loss 0.05432469770312309 Accuracy 0.452392578125\n",
      "Iteration 10330 Training loss 0.056781116873025894 Validation loss 0.05430145189166069 Accuracy 0.452880859375\n",
      "Iteration 10340 Training loss 0.05497577786445618 Validation loss 0.05460778996348381 Accuracy 0.449951171875\n",
      "Iteration 10350 Training loss 0.05513796955347061 Validation loss 0.05447617545723915 Accuracy 0.450927734375\n",
      "Iteration 10360 Training loss 0.05534464493393898 Validation loss 0.054227106273174286 Accuracy 0.453369140625\n",
      "Iteration 10370 Training loss 0.053478434681892395 Validation loss 0.054326023906469345 Accuracy 0.452392578125\n",
      "Iteration 10380 Training loss 0.050117574632167816 Validation loss 0.05481928214430809 Accuracy 0.447509765625\n",
      "Iteration 10390 Training loss 0.0507182851433754 Validation loss 0.05404852330684662 Accuracy 0.454833984375\n",
      "Iteration 10400 Training loss 0.05490943416953087 Validation loss 0.054177410900592804 Accuracy 0.4541015625\n",
      "Iteration 10410 Training loss 0.05446483939886093 Validation loss 0.05445992201566696 Accuracy 0.450927734375\n",
      "Iteration 10420 Training loss 0.05063097923994064 Validation loss 0.05459722876548767 Accuracy 0.448974609375\n",
      "Iteration 10430 Training loss 0.05528935790061951 Validation loss 0.05437355488538742 Accuracy 0.451904296875\n",
      "Iteration 10440 Training loss 0.05210858955979347 Validation loss 0.05409052222967148 Accuracy 0.45458984375\n",
      "Iteration 10450 Training loss 0.05497520789504051 Validation loss 0.055323611944913864 Accuracy 0.4423828125\n",
      "Iteration 10460 Training loss 0.053597502410411835 Validation loss 0.05418800562620163 Accuracy 0.45361328125\n",
      "Iteration 10470 Training loss 0.05052347853779793 Validation loss 0.05440055578947067 Accuracy 0.451416015625\n",
      "Iteration 10480 Training loss 0.05508491024374962 Validation loss 0.05430762097239494 Accuracy 0.452392578125\n",
      "Iteration 10490 Training loss 0.051642365753650665 Validation loss 0.05422442406415939 Accuracy 0.452880859375\n",
      "Iteration 10500 Training loss 0.05212647467851639 Validation loss 0.054236918687820435 Accuracy 0.453369140625\n",
      "Iteration 10510 Training loss 0.05499298498034477 Validation loss 0.05455245077610016 Accuracy 0.4501953125\n",
      "Iteration 10520 Training loss 0.05272109434008598 Validation loss 0.054161325097084045 Accuracy 0.4541015625\n",
      "Iteration 10530 Training loss 0.05221841484308243 Validation loss 0.05434861034154892 Accuracy 0.452392578125\n",
      "Iteration 10540 Training loss 0.05587682127952576 Validation loss 0.054495006799697876 Accuracy 0.44970703125\n",
      "Iteration 10550 Training loss 0.05740034952759743 Validation loss 0.05413059517741203 Accuracy 0.453857421875\n",
      "Iteration 10560 Training loss 0.052025217562913895 Validation loss 0.05435213819146156 Accuracy 0.45166015625\n",
      "Iteration 10570 Training loss 0.05284253880381584 Validation loss 0.054784148931503296 Accuracy 0.447265625\n",
      "Iteration 10580 Training loss 0.05309721827507019 Validation loss 0.054358649998903275 Accuracy 0.4521484375\n",
      "Iteration 10590 Training loss 0.055738210678100586 Validation loss 0.054186709225177765 Accuracy 0.453125\n",
      "Iteration 10600 Training loss 0.05261876434087753 Validation loss 0.05430908128619194 Accuracy 0.453125\n",
      "Iteration 10610 Training loss 0.05286003649234772 Validation loss 0.05423654988408089 Accuracy 0.453369140625\n",
      "Iteration 10620 Training loss 0.049161672592163086 Validation loss 0.05405803397297859 Accuracy 0.455078125\n",
      "Iteration 10630 Training loss 0.051305774599313736 Validation loss 0.05429184436798096 Accuracy 0.4521484375\n",
      "Iteration 10640 Training loss 0.052882708609104156 Validation loss 0.05407495051622391 Accuracy 0.454833984375\n",
      "Iteration 10650 Training loss 0.055762093514204025 Validation loss 0.05442937836050987 Accuracy 0.45068359375\n",
      "Iteration 10660 Training loss 0.052043020725250244 Validation loss 0.05423801764845848 Accuracy 0.452880859375\n",
      "Iteration 10670 Training loss 0.057007186114788055 Validation loss 0.055687133222818375 Accuracy 0.43798828125\n",
      "Iteration 10680 Training loss 0.05421953275799751 Validation loss 0.05421029031276703 Accuracy 0.45361328125\n",
      "Iteration 10690 Training loss 0.05675693228840828 Validation loss 0.05419516563415527 Accuracy 0.453857421875\n",
      "Iteration 10700 Training loss 0.05669868737459183 Validation loss 0.05430320277810097 Accuracy 0.4521484375\n",
      "Iteration 10710 Training loss 0.053641144186258316 Validation loss 0.05424902215600014 Accuracy 0.45263671875\n",
      "Iteration 10720 Training loss 0.052913859486579895 Validation loss 0.054205793887376785 Accuracy 0.45361328125\n",
      "Iteration 10730 Training loss 0.05069093406200409 Validation loss 0.0551699697971344 Accuracy 0.443359375\n",
      "Iteration 10740 Training loss 0.05099054053425789 Validation loss 0.05472463369369507 Accuracy 0.448974609375\n",
      "Iteration 10750 Training loss 0.052360232919454575 Validation loss 0.05465143546462059 Accuracy 0.4482421875\n",
      "Iteration 10760 Training loss 0.05495205521583557 Validation loss 0.05535688251256943 Accuracy 0.442138671875\n",
      "Iteration 10770 Training loss 0.0530698299407959 Validation loss 0.05440937355160713 Accuracy 0.4521484375\n",
      "Iteration 10780 Training loss 0.05341891571879387 Validation loss 0.05422425642609596 Accuracy 0.452880859375\n",
      "Iteration 10790 Training loss 0.05390016734600067 Validation loss 0.05411802604794502 Accuracy 0.4541015625\n",
      "Iteration 10800 Training loss 0.05028792843222618 Validation loss 0.05415356159210205 Accuracy 0.4541015625\n",
      "Iteration 10810 Training loss 0.05287070572376251 Validation loss 0.05415719002485275 Accuracy 0.4541015625\n",
      "Iteration 10820 Training loss 0.054109640419483185 Validation loss 0.0543050616979599 Accuracy 0.452392578125\n",
      "Iteration 10830 Training loss 0.051473699510097504 Validation loss 0.054819297045469284 Accuracy 0.447021484375\n",
      "Iteration 10840 Training loss 0.05157182738184929 Validation loss 0.05423193797469139 Accuracy 0.453369140625\n",
      "Iteration 10850 Training loss 0.05100531876087189 Validation loss 0.054231077432632446 Accuracy 0.453125\n",
      "Iteration 10860 Training loss 0.05714437738060951 Validation loss 0.054242927581071854 Accuracy 0.453369140625\n",
      "Iteration 10870 Training loss 0.052481770515441895 Validation loss 0.05428880825638771 Accuracy 0.452880859375\n",
      "Iteration 10880 Training loss 0.055702921003103256 Validation loss 0.05425937846302986 Accuracy 0.452880859375\n",
      "Iteration 10890 Training loss 0.052113767713308334 Validation loss 0.05425305664539337 Accuracy 0.453125\n",
      "Iteration 10900 Training loss 0.056039001792669296 Validation loss 0.05420032888650894 Accuracy 0.453369140625\n",
      "Iteration 10910 Training loss 0.049692727625370026 Validation loss 0.05438327044248581 Accuracy 0.45166015625\n",
      "Iteration 10920 Training loss 0.05335703119635582 Validation loss 0.055545270442962646 Accuracy 0.439697265625\n",
      "Iteration 10930 Training loss 0.05328645929694176 Validation loss 0.05429515615105629 Accuracy 0.451904296875\n",
      "Iteration 10940 Training loss 0.055260028690099716 Validation loss 0.054536640644073486 Accuracy 0.44970703125\n",
      "Iteration 10950 Training loss 0.05313124507665634 Validation loss 0.054262831807136536 Accuracy 0.452392578125\n",
      "Iteration 10960 Training loss 0.05650351941585541 Validation loss 0.05461713671684265 Accuracy 0.449462890625\n",
      "Iteration 10970 Training loss 0.053489603102207184 Validation loss 0.05417262017726898 Accuracy 0.45361328125\n",
      "Iteration 10980 Training loss 0.05184272304177284 Validation loss 0.05438566207885742 Accuracy 0.451416015625\n",
      "Iteration 10990 Training loss 0.05385022982954979 Validation loss 0.05425352230668068 Accuracy 0.452392578125\n",
      "Iteration 11000 Training loss 0.048062216490507126 Validation loss 0.05433765798807144 Accuracy 0.4521484375\n",
      "Iteration 11010 Training loss 0.051225125789642334 Validation loss 0.05417601391673088 Accuracy 0.453125\n",
      "Iteration 11020 Training loss 0.05071580037474632 Validation loss 0.05451907590031624 Accuracy 0.4501953125\n",
      "Iteration 11030 Training loss 0.05003241077065468 Validation loss 0.05423041060566902 Accuracy 0.452880859375\n",
      "Iteration 11040 Training loss 0.05371039733290672 Validation loss 0.05409972369670868 Accuracy 0.453857421875\n",
      "Iteration 11050 Training loss 0.052431777119636536 Validation loss 0.05435311421751976 Accuracy 0.451416015625\n",
      "Iteration 11060 Training loss 0.053418975323438644 Validation loss 0.054222334176301956 Accuracy 0.452392578125\n",
      "Iteration 11070 Training loss 0.049436431378126144 Validation loss 0.05414428934454918 Accuracy 0.4541015625\n",
      "Iteration 11080 Training loss 0.05118472874164581 Validation loss 0.05441763624548912 Accuracy 0.451171875\n",
      "Iteration 11090 Training loss 0.05212845653295517 Validation loss 0.05474509298801422 Accuracy 0.44775390625\n",
      "Iteration 11100 Training loss 0.05460319295525551 Validation loss 0.05433005839586258 Accuracy 0.4521484375\n",
      "Iteration 11110 Training loss 0.05273013189435005 Validation loss 0.054450273513793945 Accuracy 0.4501953125\n",
      "Iteration 11120 Training loss 0.058066003024578094 Validation loss 0.05454033240675926 Accuracy 0.450439453125\n",
      "Iteration 11130 Training loss 0.05283229425549507 Validation loss 0.05434414744377136 Accuracy 0.45166015625\n",
      "Iteration 11140 Training loss 0.05543508008122444 Validation loss 0.05418380722403526 Accuracy 0.45361328125\n",
      "Iteration 11150 Training loss 0.05115998908877373 Validation loss 0.05480156093835831 Accuracy 0.447509765625\n",
      "Iteration 11160 Training loss 0.05222634971141815 Validation loss 0.054441653192043304 Accuracy 0.451171875\n",
      "Iteration 11170 Training loss 0.05431865528225899 Validation loss 0.054356373846530914 Accuracy 0.451904296875\n",
      "Iteration 11180 Training loss 0.05165254697203636 Validation loss 0.05436864495277405 Accuracy 0.451904296875\n",
      "Iteration 11190 Training loss 0.05063074082136154 Validation loss 0.05446283891797066 Accuracy 0.450927734375\n",
      "Iteration 11200 Training loss 0.05163539573550224 Validation loss 0.05422079563140869 Accuracy 0.453125\n",
      "Iteration 11210 Training loss 0.05227711796760559 Validation loss 0.05436338856816292 Accuracy 0.451904296875\n",
      "Iteration 11220 Training loss 0.05614451691508293 Validation loss 0.05437230318784714 Accuracy 0.45166015625\n",
      "Iteration 11230 Training loss 0.05090450868010521 Validation loss 0.054326336830854416 Accuracy 0.45166015625\n",
      "Iteration 11240 Training loss 0.05301205441355705 Validation loss 0.05475928634405136 Accuracy 0.44775390625\n",
      "Iteration 11250 Training loss 0.055435966700315475 Validation loss 0.05451280251145363 Accuracy 0.450439453125\n",
      "Iteration 11260 Training loss 0.05116352438926697 Validation loss 0.054370731115341187 Accuracy 0.451416015625\n",
      "Iteration 11270 Training loss 0.05090129375457764 Validation loss 0.05563029274344444 Accuracy 0.439697265625\n",
      "Iteration 11280 Training loss 0.05151086300611496 Validation loss 0.05418477579951286 Accuracy 0.454345703125\n",
      "Iteration 11290 Training loss 0.05213271081447601 Validation loss 0.05430568754673004 Accuracy 0.451904296875\n",
      "Iteration 11300 Training loss 0.050371184945106506 Validation loss 0.054459501057863235 Accuracy 0.450439453125\n",
      "Iteration 11310 Training loss 0.05048501491546631 Validation loss 0.05451061949133873 Accuracy 0.449951171875\n",
      "Iteration 11320 Training loss 0.05272739753127098 Validation loss 0.054108310490846634 Accuracy 0.454345703125\n",
      "Iteration 11330 Training loss 0.05299822986125946 Validation loss 0.05425894260406494 Accuracy 0.452392578125\n",
      "Iteration 11340 Training loss 0.052358418703079224 Validation loss 0.0543891005218029 Accuracy 0.451904296875\n",
      "Iteration 11350 Training loss 0.05524878203868866 Validation loss 0.0541658028960228 Accuracy 0.4541015625\n",
      "Iteration 11360 Training loss 0.05174534022808075 Validation loss 0.05437625199556351 Accuracy 0.45166015625\n",
      "Iteration 11370 Training loss 0.05107561871409416 Validation loss 0.05497397854924202 Accuracy 0.4453125\n",
      "Iteration 11380 Training loss 0.05412455275654793 Validation loss 0.05410514026880264 Accuracy 0.4541015625\n",
      "Iteration 11390 Training loss 0.05262498930096626 Validation loss 0.054569266736507416 Accuracy 0.44970703125\n",
      "Iteration 11400 Training loss 0.0491115003824234 Validation loss 0.05416693538427353 Accuracy 0.45361328125\n",
      "Iteration 11410 Training loss 0.05441781505942345 Validation loss 0.05417684465646744 Accuracy 0.453369140625\n",
      "Iteration 11420 Training loss 0.05417989194393158 Validation loss 0.054967913776636124 Accuracy 0.446044921875\n",
      "Iteration 11430 Training loss 0.05393792688846588 Validation loss 0.05428958684206009 Accuracy 0.452392578125\n",
      "Iteration 11440 Training loss 0.053533170372247696 Validation loss 0.054406970739364624 Accuracy 0.45166015625\n",
      "Iteration 11450 Training loss 0.05503785237669945 Validation loss 0.05533795803785324 Accuracy 0.442626953125\n",
      "Iteration 11460 Training loss 0.05382276326417923 Validation loss 0.05457528680562973 Accuracy 0.44970703125\n",
      "Iteration 11470 Training loss 0.05444980785250664 Validation loss 0.054380882531404495 Accuracy 0.452392578125\n",
      "Iteration 11480 Training loss 0.053255852311849594 Validation loss 0.054293304681777954 Accuracy 0.452392578125\n",
      "Iteration 11490 Training loss 0.054726723581552505 Validation loss 0.05414874851703644 Accuracy 0.453857421875\n",
      "Iteration 11500 Training loss 0.05026566982269287 Validation loss 0.05441124364733696 Accuracy 0.451904296875\n",
      "Iteration 11510 Training loss 0.05522241070866585 Validation loss 0.05421501025557518 Accuracy 0.453369140625\n",
      "Iteration 11520 Training loss 0.05085750296711922 Validation loss 0.054258935153484344 Accuracy 0.453125\n",
      "Iteration 11530 Training loss 0.05095871910452843 Validation loss 0.05436680465936661 Accuracy 0.451171875\n",
      "Iteration 11540 Training loss 0.053434066474437714 Validation loss 0.05408051237463951 Accuracy 0.454833984375\n",
      "Iteration 11550 Training loss 0.05246276408433914 Validation loss 0.05430174246430397 Accuracy 0.452392578125\n",
      "Iteration 11560 Training loss 0.05439577251672745 Validation loss 0.054084256291389465 Accuracy 0.4541015625\n",
      "Iteration 11570 Training loss 0.054118093103170395 Validation loss 0.05446373298764229 Accuracy 0.450439453125\n",
      "Iteration 11580 Training loss 0.051139943301677704 Validation loss 0.05422714352607727 Accuracy 0.45361328125\n",
      "Iteration 11590 Training loss 0.051812127232551575 Validation loss 0.05429091677069664 Accuracy 0.452880859375\n",
      "Iteration 11600 Training loss 0.05202974006533623 Validation loss 0.05432121828198433 Accuracy 0.4521484375\n",
      "Iteration 11610 Training loss 0.05091872066259384 Validation loss 0.054322294890880585 Accuracy 0.451904296875\n",
      "Iteration 11620 Training loss 0.05367550626397133 Validation loss 0.054391924291849136 Accuracy 0.4521484375\n",
      "Iteration 11630 Training loss 0.052804697304964066 Validation loss 0.054416853934526443 Accuracy 0.451904296875\n",
      "Iteration 11640 Training loss 0.0558621920645237 Validation loss 0.054336804896593094 Accuracy 0.4521484375\n",
      "Iteration 11650 Training loss 0.055701129138469696 Validation loss 0.05445683002471924 Accuracy 0.451416015625\n",
      "Iteration 11660 Training loss 0.056796662509441376 Validation loss 0.054405778646469116 Accuracy 0.45068359375\n",
      "Iteration 11670 Training loss 0.051562342792749405 Validation loss 0.05422568321228027 Accuracy 0.45263671875\n",
      "Iteration 11680 Training loss 0.053981807082891464 Validation loss 0.05423637479543686 Accuracy 0.45263671875\n",
      "Iteration 11690 Training loss 0.05222640559077263 Validation loss 0.05410096421837807 Accuracy 0.45458984375\n",
      "Iteration 11700 Training loss 0.04939685016870499 Validation loss 0.05445370450615883 Accuracy 0.451416015625\n",
      "Iteration 11710 Training loss 0.05167573317885399 Validation loss 0.05414576083421707 Accuracy 0.453857421875\n",
      "Iteration 11720 Training loss 0.04990636184811592 Validation loss 0.05433565378189087 Accuracy 0.45263671875\n",
      "Iteration 11730 Training loss 0.0518612265586853 Validation loss 0.054287172853946686 Accuracy 0.453369140625\n",
      "Iteration 11740 Training loss 0.05153454467654228 Validation loss 0.054238222539424896 Accuracy 0.45361328125\n",
      "Iteration 11750 Training loss 0.053933266550302505 Validation loss 0.05446566268801689 Accuracy 0.450927734375\n",
      "Iteration 11760 Training loss 0.051439061760902405 Validation loss 0.05448570102453232 Accuracy 0.450439453125\n",
      "Iteration 11770 Training loss 0.0511079803109169 Validation loss 0.05450081825256348 Accuracy 0.450439453125\n",
      "Iteration 11780 Training loss 0.05081481859087944 Validation loss 0.05454563349485397 Accuracy 0.450439453125\n",
      "Iteration 11790 Training loss 0.05507555231451988 Validation loss 0.054429177194833755 Accuracy 0.450927734375\n",
      "Iteration 11800 Training loss 0.0508880652487278 Validation loss 0.05426386743783951 Accuracy 0.451904296875\n",
      "Iteration 11810 Training loss 0.05439938232302666 Validation loss 0.05423860251903534 Accuracy 0.4521484375\n",
      "Iteration 11820 Training loss 0.05414317175745964 Validation loss 0.05409502610564232 Accuracy 0.454345703125\n",
      "Iteration 11830 Training loss 0.05141725763678551 Validation loss 0.054704450070858 Accuracy 0.447509765625\n",
      "Iteration 11840 Training loss 0.05099958926439285 Validation loss 0.05420995503664017 Accuracy 0.452880859375\n",
      "Iteration 11850 Training loss 0.05364261940121651 Validation loss 0.05439133197069168 Accuracy 0.451904296875\n",
      "Iteration 11860 Training loss 0.053747572004795074 Validation loss 0.05439307540655136 Accuracy 0.4521484375\n",
      "Iteration 11870 Training loss 0.05644341558218002 Validation loss 0.05453527346253395 Accuracy 0.450439453125\n",
      "Iteration 11880 Training loss 0.057389501482248306 Validation loss 0.05545768886804581 Accuracy 0.44140625\n",
      "Iteration 11890 Training loss 0.05119962617754936 Validation loss 0.05428256094455719 Accuracy 0.45166015625\n",
      "Iteration 11900 Training loss 0.05468394234776497 Validation loss 0.05498003587126732 Accuracy 0.4453125\n",
      "Iteration 11910 Training loss 0.051717955619096756 Validation loss 0.054305028170347214 Accuracy 0.45263671875\n",
      "Iteration 11920 Training loss 0.05521158128976822 Validation loss 0.05408620834350586 Accuracy 0.454345703125\n",
      "Iteration 11930 Training loss 0.052774544805288315 Validation loss 0.05451131612062454 Accuracy 0.4501953125\n",
      "Iteration 11940 Training loss 0.053526315838098526 Validation loss 0.054061394184827805 Accuracy 0.45458984375\n",
      "Iteration 11950 Training loss 0.0528254434466362 Validation loss 0.05433155223727226 Accuracy 0.452392578125\n",
      "Iteration 11960 Training loss 0.052808165550231934 Validation loss 0.054093871265649796 Accuracy 0.45458984375\n",
      "Iteration 11970 Training loss 0.05499807745218277 Validation loss 0.05416392907500267 Accuracy 0.45361328125\n",
      "Iteration 11980 Training loss 0.055710047483444214 Validation loss 0.054390449076890945 Accuracy 0.451904296875\n",
      "Iteration 11990 Training loss 0.04913484305143356 Validation loss 0.0542634055018425 Accuracy 0.452880859375\n",
      "Iteration 12000 Training loss 0.049922000616788864 Validation loss 0.054795682430267334 Accuracy 0.447998046875\n",
      "Iteration 12010 Training loss 0.04978112876415253 Validation loss 0.05412084981799126 Accuracy 0.45458984375\n",
      "Iteration 12020 Training loss 0.05364184454083443 Validation loss 0.054564379155635834 Accuracy 0.450439453125\n",
      "Iteration 12030 Training loss 0.052687566727399826 Validation loss 0.05408874526619911 Accuracy 0.4541015625\n",
      "Iteration 12040 Training loss 0.0487050861120224 Validation loss 0.05442942678928375 Accuracy 0.4521484375\n",
      "Iteration 12050 Training loss 0.052577607333660126 Validation loss 0.054148491472005844 Accuracy 0.4541015625\n",
      "Iteration 12060 Training loss 0.05402284115552902 Validation loss 0.05436558276414871 Accuracy 0.4521484375\n",
      "Iteration 12070 Training loss 0.048680804669857025 Validation loss 0.05398479849100113 Accuracy 0.455322265625\n",
      "Iteration 12080 Training loss 0.05356166511774063 Validation loss 0.0544675774872303 Accuracy 0.451416015625\n",
      "Iteration 12090 Training loss 0.05530368164181709 Validation loss 0.054270487278699875 Accuracy 0.452392578125\n",
      "Iteration 12100 Training loss 0.0546409897506237 Validation loss 0.0541735477745533 Accuracy 0.453125\n",
      "Iteration 12110 Training loss 0.05101262405514717 Validation loss 0.05459705367684364 Accuracy 0.448974609375\n",
      "Iteration 12120 Training loss 0.05044500529766083 Validation loss 0.05413311719894409 Accuracy 0.4541015625\n",
      "Iteration 12130 Training loss 0.05390740931034088 Validation loss 0.05417770892381668 Accuracy 0.452880859375\n",
      "Iteration 12140 Training loss 0.055219218134880066 Validation loss 0.054387614130973816 Accuracy 0.45263671875\n",
      "Iteration 12150 Training loss 0.05026146396994591 Validation loss 0.05478133633732796 Accuracy 0.448486328125\n",
      "Iteration 12160 Training loss 0.051827024668455124 Validation loss 0.055842868983745575 Accuracy 0.4365234375\n",
      "Iteration 12170 Training loss 0.052950769662857056 Validation loss 0.055802445858716965 Accuracy 0.43798828125\n",
      "Iteration 12180 Training loss 0.05195601284503937 Validation loss 0.05478820204734802 Accuracy 0.447509765625\n",
      "Iteration 12190 Training loss 0.05054624378681183 Validation loss 0.054294928908348083 Accuracy 0.453369140625\n",
      "Iteration 12200 Training loss 0.05106108635663986 Validation loss 0.05440165847539902 Accuracy 0.451904296875\n",
      "Iteration 12210 Training loss 0.05065199360251427 Validation loss 0.054404404014348984 Accuracy 0.451904296875\n",
      "Iteration 12220 Training loss 0.05012836307287216 Validation loss 0.054350148886442184 Accuracy 0.451171875\n",
      "Iteration 12230 Training loss 0.053229205310344696 Validation loss 0.0543619766831398 Accuracy 0.4521484375\n",
      "Iteration 12240 Training loss 0.05247025564312935 Validation loss 0.05405331775546074 Accuracy 0.454833984375\n",
      "Iteration 12250 Training loss 0.05129428952932358 Validation loss 0.05414958298206329 Accuracy 0.453857421875\n",
      "Iteration 12260 Training loss 0.05380731076002121 Validation loss 0.054545141756534576 Accuracy 0.451416015625\n",
      "Iteration 12270 Training loss 0.05358641594648361 Validation loss 0.05514401197433472 Accuracy 0.444580078125\n",
      "Iteration 12280 Training loss 0.0530901774764061 Validation loss 0.05461569130420685 Accuracy 0.44970703125\n",
      "Iteration 12290 Training loss 0.05434059724211693 Validation loss 0.05439673364162445 Accuracy 0.452392578125\n",
      "Iteration 12300 Training loss 0.04816066473722458 Validation loss 0.05406726524233818 Accuracy 0.45556640625\n",
      "Iteration 12310 Training loss 0.04975707083940506 Validation loss 0.05430690199136734 Accuracy 0.453125\n",
      "Iteration 12320 Training loss 0.056515052914619446 Validation loss 0.05417637154459953 Accuracy 0.4541015625\n",
      "Iteration 12330 Training loss 0.05410303547978401 Validation loss 0.05445868894457817 Accuracy 0.451171875\n",
      "Iteration 12340 Training loss 0.048912160098552704 Validation loss 0.054120905697345734 Accuracy 0.45458984375\n",
      "Iteration 12350 Training loss 0.05307597666978836 Validation loss 0.05457412824034691 Accuracy 0.45068359375\n",
      "Iteration 12360 Training loss 0.05170106887817383 Validation loss 0.05433660000562668 Accuracy 0.452880859375\n",
      "Iteration 12370 Training loss 0.053552936762571335 Validation loss 0.054474540054798126 Accuracy 0.45068359375\n",
      "Iteration 12380 Training loss 0.05385414510965347 Validation loss 0.054377295076847076 Accuracy 0.452880859375\n",
      "Iteration 12390 Training loss 0.05100063607096672 Validation loss 0.05414991080760956 Accuracy 0.4541015625\n",
      "Iteration 12400 Training loss 0.05319426953792572 Validation loss 0.05458717793226242 Accuracy 0.44970703125\n",
      "Iteration 12410 Training loss 0.055342771112918854 Validation loss 0.05492527037858963 Accuracy 0.447509765625\n",
      "Iteration 12420 Training loss 0.053119901567697525 Validation loss 0.054583337157964706 Accuracy 0.449951171875\n",
      "Iteration 12430 Training loss 0.05529497563838959 Validation loss 0.05480336770415306 Accuracy 0.447265625\n",
      "Iteration 12440 Training loss 0.05277818813920021 Validation loss 0.05413418263196945 Accuracy 0.455322265625\n",
      "Iteration 12450 Training loss 0.052522413432598114 Validation loss 0.05418041720986366 Accuracy 0.45458984375\n",
      "Iteration 12460 Training loss 0.052447523921728134 Validation loss 0.05415503308176994 Accuracy 0.455322265625\n",
      "Iteration 12470 Training loss 0.05569825693964958 Validation loss 0.054241396486759186 Accuracy 0.4541015625\n",
      "Iteration 12480 Training loss 0.05376972258090973 Validation loss 0.054130058735609055 Accuracy 0.454833984375\n",
      "Iteration 12490 Training loss 0.05372465029358864 Validation loss 0.05432846397161484 Accuracy 0.45263671875\n",
      "Iteration 12500 Training loss 0.05556671693921089 Validation loss 0.05421309545636177 Accuracy 0.45458984375\n",
      "Iteration 12510 Training loss 0.05303238704800606 Validation loss 0.054592035710811615 Accuracy 0.44921875\n",
      "Iteration 12520 Training loss 0.051913656294345856 Validation loss 0.05416920781135559 Accuracy 0.4541015625\n",
      "Iteration 12530 Training loss 0.0575392059981823 Validation loss 0.05479898303747177 Accuracy 0.448486328125\n",
      "Iteration 12540 Training loss 0.05651479959487915 Validation loss 0.05488099530339241 Accuracy 0.447509765625\n",
      "Iteration 12550 Training loss 0.05554790049791336 Validation loss 0.05628368631005287 Accuracy 0.432861328125\n",
      "Iteration 12560 Training loss 0.050302594900131226 Validation loss 0.05429584160447121 Accuracy 0.452880859375\n",
      "Iteration 12570 Training loss 0.05361431464552879 Validation loss 0.05413239076733589 Accuracy 0.455078125\n",
      "Iteration 12580 Training loss 0.050690196454524994 Validation loss 0.05422843247652054 Accuracy 0.45361328125\n",
      "Iteration 12590 Training loss 0.054161589592695236 Validation loss 0.05455551669001579 Accuracy 0.44970703125\n",
      "Iteration 12600 Training loss 0.053927235305309296 Validation loss 0.054327499121427536 Accuracy 0.453369140625\n",
      "Iteration 12610 Training loss 0.05329146608710289 Validation loss 0.0541263073682785 Accuracy 0.454833984375\n",
      "Iteration 12620 Training loss 0.05362646281719208 Validation loss 0.054287344217300415 Accuracy 0.452880859375\n",
      "Iteration 12630 Training loss 0.057047225534915924 Validation loss 0.055047135800123215 Accuracy 0.444091796875\n",
      "Iteration 12640 Training loss 0.05139036104083061 Validation loss 0.05431633070111275 Accuracy 0.451171875\n",
      "Iteration 12650 Training loss 0.0508294552564621 Validation loss 0.05448714643716812 Accuracy 0.450927734375\n",
      "Iteration 12660 Training loss 0.051267266273498535 Validation loss 0.05414756387472153 Accuracy 0.452880859375\n",
      "Iteration 12670 Training loss 0.050409067422151566 Validation loss 0.05449311435222626 Accuracy 0.451171875\n",
      "Iteration 12680 Training loss 0.05604337155818939 Validation loss 0.05402059480547905 Accuracy 0.455078125\n",
      "Iteration 12690 Training loss 0.053278807550668716 Validation loss 0.054266396909952164 Accuracy 0.452880859375\n",
      "Iteration 12700 Training loss 0.056403324007987976 Validation loss 0.0543142668902874 Accuracy 0.451416015625\n",
      "Iteration 12710 Training loss 0.05048808827996254 Validation loss 0.05412124842405319 Accuracy 0.45458984375\n",
      "Iteration 12720 Training loss 0.055810846388339996 Validation loss 0.05476227030158043 Accuracy 0.44873046875\n",
      "Iteration 12730 Training loss 0.05436988174915314 Validation loss 0.05419411137700081 Accuracy 0.4541015625\n",
      "Iteration 12740 Training loss 0.05223551765084267 Validation loss 0.05500105395913124 Accuracy 0.445068359375\n",
      "Iteration 12750 Training loss 0.05273319408297539 Validation loss 0.05545775219798088 Accuracy 0.44189453125\n",
      "Iteration 12760 Training loss 0.05321657285094261 Validation loss 0.055025383830070496 Accuracy 0.44482421875\n",
      "Iteration 12770 Training loss 0.05005241185426712 Validation loss 0.05424922704696655 Accuracy 0.451416015625\n",
      "Iteration 12780 Training loss 0.052672646939754486 Validation loss 0.054270125925540924 Accuracy 0.45166015625\n",
      "Iteration 12790 Training loss 0.05378636717796326 Validation loss 0.055549465119838715 Accuracy 0.441162109375\n",
      "Iteration 12800 Training loss 0.05544175207614899 Validation loss 0.05411859229207039 Accuracy 0.454833984375\n",
      "Iteration 12810 Training loss 0.0525156669318676 Validation loss 0.05426691100001335 Accuracy 0.452880859375\n",
      "Iteration 12820 Training loss 0.05449759215116501 Validation loss 0.05430716648697853 Accuracy 0.45361328125\n",
      "Iteration 12830 Training loss 0.04957644268870354 Validation loss 0.0551457554101944 Accuracy 0.4453125\n",
      "Iteration 12840 Training loss 0.0533900149166584 Validation loss 0.054401785135269165 Accuracy 0.451416015625\n",
      "Iteration 12850 Training loss 0.0548156313598156 Validation loss 0.05406969040632248 Accuracy 0.455322265625\n",
      "Iteration 12860 Training loss 0.050844866782426834 Validation loss 0.05409830063581467 Accuracy 0.454833984375\n",
      "Iteration 12870 Training loss 0.052551642060279846 Validation loss 0.054367005825042725 Accuracy 0.45263671875\n",
      "Iteration 12880 Training loss 0.05410534143447876 Validation loss 0.054062917828559875 Accuracy 0.455078125\n",
      "Iteration 12890 Training loss 0.05446050316095352 Validation loss 0.05447576195001602 Accuracy 0.449951171875\n",
      "Iteration 12900 Training loss 0.053973134607076645 Validation loss 0.05437885969877243 Accuracy 0.4521484375\n",
      "Iteration 12910 Training loss 0.05281545966863632 Validation loss 0.05396191403269768 Accuracy 0.455810546875\n",
      "Iteration 12920 Training loss 0.052133552730083466 Validation loss 0.05407676473259926 Accuracy 0.45458984375\n",
      "Iteration 12930 Training loss 0.0522017702460289 Validation loss 0.05412076413631439 Accuracy 0.4541015625\n",
      "Iteration 12940 Training loss 0.05103546753525734 Validation loss 0.05446828901767731 Accuracy 0.449951171875\n",
      "Iteration 12950 Training loss 0.050097182393074036 Validation loss 0.054043009877204895 Accuracy 0.454345703125\n",
      "Iteration 12960 Training loss 0.049677371978759766 Validation loss 0.054034240543842316 Accuracy 0.4541015625\n",
      "Iteration 12970 Training loss 0.05296701565384865 Validation loss 0.05436130613088608 Accuracy 0.45166015625\n",
      "Iteration 12980 Training loss 0.05321288853883743 Validation loss 0.05445634946227074 Accuracy 0.450927734375\n",
      "Iteration 12990 Training loss 0.054011858999729156 Validation loss 0.05438663065433502 Accuracy 0.451416015625\n",
      "Iteration 13000 Training loss 0.05106796324253082 Validation loss 0.05416252836585045 Accuracy 0.45361328125\n",
      "Iteration 13010 Training loss 0.05678979307413101 Validation loss 0.05429583042860031 Accuracy 0.45263671875\n",
      "Iteration 13020 Training loss 0.05278840661048889 Validation loss 0.05455273389816284 Accuracy 0.44970703125\n",
      "Iteration 13030 Training loss 0.05551393702626228 Validation loss 0.05407234653830528 Accuracy 0.45458984375\n",
      "Iteration 13040 Training loss 0.051936712116003036 Validation loss 0.05428902059793472 Accuracy 0.452880859375\n",
      "Iteration 13050 Training loss 0.05740193650126457 Validation loss 0.054900310933589935 Accuracy 0.4462890625\n",
      "Iteration 13060 Training loss 0.05129756033420563 Validation loss 0.05431264638900757 Accuracy 0.451904296875\n",
      "Iteration 13070 Training loss 0.05281881242990494 Validation loss 0.05418972671031952 Accuracy 0.453369140625\n",
      "Iteration 13080 Training loss 0.05083282291889191 Validation loss 0.054025448858737946 Accuracy 0.454345703125\n",
      "Iteration 13090 Training loss 0.05005047097802162 Validation loss 0.054218634963035583 Accuracy 0.45361328125\n",
      "Iteration 13100 Training loss 0.05364483222365379 Validation loss 0.0541294626891613 Accuracy 0.45361328125\n",
      "Iteration 13110 Training loss 0.050591886043548584 Validation loss 0.05411985144019127 Accuracy 0.453857421875\n",
      "Iteration 13120 Training loss 0.05039039999246597 Validation loss 0.05411215499043465 Accuracy 0.453857421875\n",
      "Iteration 13130 Training loss 0.05456806346774101 Validation loss 0.054175302386283875 Accuracy 0.453369140625\n",
      "Iteration 13140 Training loss 0.05254731699824333 Validation loss 0.0545208565890789 Accuracy 0.45068359375\n",
      "Iteration 13150 Training loss 0.05362594500184059 Validation loss 0.054675810039043427 Accuracy 0.447998046875\n",
      "Iteration 13160 Training loss 0.05474255234003067 Validation loss 0.055370695888996124 Accuracy 0.44189453125\n",
      "Iteration 13170 Training loss 0.052515529096126556 Validation loss 0.054278504103422165 Accuracy 0.452880859375\n",
      "Iteration 13180 Training loss 0.052990976721048355 Validation loss 0.05461566150188446 Accuracy 0.449462890625\n",
      "Iteration 13190 Training loss 0.053518880158662796 Validation loss 0.05449160933494568 Accuracy 0.4501953125\n",
      "Iteration 13200 Training loss 0.05146753787994385 Validation loss 0.054264552891254425 Accuracy 0.452392578125\n",
      "Iteration 13210 Training loss 0.05464809015393257 Validation loss 0.054093558341264725 Accuracy 0.454345703125\n",
      "Iteration 13220 Training loss 0.054425884038209915 Validation loss 0.05425778776407242 Accuracy 0.452880859375\n",
      "Iteration 13230 Training loss 0.054342955350875854 Validation loss 0.05467141419649124 Accuracy 0.448486328125\n",
      "Iteration 13240 Training loss 0.052073683589696884 Validation loss 0.05460848659276962 Accuracy 0.448974609375\n",
      "Iteration 13250 Training loss 0.05172649025917053 Validation loss 0.05414612218737602 Accuracy 0.45458984375\n",
      "Iteration 13260 Training loss 0.053136810660362244 Validation loss 0.05442364141345024 Accuracy 0.450927734375\n",
      "Iteration 13270 Training loss 0.053256805986166 Validation loss 0.054227858781814575 Accuracy 0.453125\n",
      "Iteration 13280 Training loss 0.05196903273463249 Validation loss 0.054129138588905334 Accuracy 0.45361328125\n",
      "Iteration 13290 Training loss 0.051063284277915955 Validation loss 0.05424877628684044 Accuracy 0.45263671875\n",
      "Iteration 13300 Training loss 0.05057362839579582 Validation loss 0.054390858858823776 Accuracy 0.45166015625\n",
      "Iteration 13310 Training loss 0.05265546962618828 Validation loss 0.05423350632190704 Accuracy 0.452392578125\n",
      "Iteration 13320 Training loss 0.0516122430562973 Validation loss 0.054128024727106094 Accuracy 0.453857421875\n",
      "Iteration 13330 Training loss 0.053852614015340805 Validation loss 0.0540991947054863 Accuracy 0.454345703125\n",
      "Iteration 13340 Training loss 0.04703165590763092 Validation loss 0.0540497824549675 Accuracy 0.45458984375\n",
      "Iteration 13350 Training loss 0.05019538477063179 Validation loss 0.054465096443891525 Accuracy 0.4501953125\n",
      "Iteration 13360 Training loss 0.05483047291636467 Validation loss 0.05423950031399727 Accuracy 0.452392578125\n",
      "Iteration 13370 Training loss 0.05115264281630516 Validation loss 0.05421528220176697 Accuracy 0.452392578125\n",
      "Iteration 13380 Training loss 0.04961049184203148 Validation loss 0.05494091659784317 Accuracy 0.44580078125\n",
      "Iteration 13390 Training loss 0.05673861503601074 Validation loss 0.05440495163202286 Accuracy 0.451416015625\n",
      "Iteration 13400 Training loss 0.052788808941841125 Validation loss 0.05408941209316254 Accuracy 0.4541015625\n",
      "Iteration 13410 Training loss 0.05525374412536621 Validation loss 0.054237302392721176 Accuracy 0.452880859375\n",
      "Iteration 13420 Training loss 0.054300662130117416 Validation loss 0.054335013031959534 Accuracy 0.452392578125\n",
      "Iteration 13430 Training loss 0.05271431803703308 Validation loss 0.05425034835934639 Accuracy 0.4521484375\n",
      "Iteration 13440 Training loss 0.04976378381252289 Validation loss 0.05412125959992409 Accuracy 0.452880859375\n",
      "Iteration 13450 Training loss 0.05419740080833435 Validation loss 0.05440973863005638 Accuracy 0.45068359375\n",
      "Iteration 13460 Training loss 0.05280977860093117 Validation loss 0.05457767844200134 Accuracy 0.449462890625\n",
      "Iteration 13470 Training loss 0.0541551448404789 Validation loss 0.05431998893618584 Accuracy 0.45166015625\n",
      "Iteration 13480 Training loss 0.055123403668403625 Validation loss 0.0543682798743248 Accuracy 0.450439453125\n",
      "Iteration 13490 Training loss 0.052588388323783875 Validation loss 0.05438327044248581 Accuracy 0.45068359375\n",
      "Iteration 13500 Training loss 0.053824532777071 Validation loss 0.054313212633132935 Accuracy 0.452392578125\n",
      "Iteration 13510 Training loss 0.054475899785757065 Validation loss 0.0541105754673481 Accuracy 0.45361328125\n",
      "Iteration 13520 Training loss 0.05359746888279915 Validation loss 0.05421828106045723 Accuracy 0.45361328125\n",
      "Iteration 13530 Training loss 0.050005409866571426 Validation loss 0.05454159900546074 Accuracy 0.449951171875\n",
      "Iteration 13540 Training loss 0.051891352981328964 Validation loss 0.05430257320404053 Accuracy 0.451416015625\n",
      "Iteration 13550 Training loss 0.051811277866363525 Validation loss 0.05411595478653908 Accuracy 0.4541015625\n",
      "Iteration 13560 Training loss 0.05524167791008949 Validation loss 0.05415189266204834 Accuracy 0.45361328125\n",
      "Iteration 13570 Training loss 0.05325191095471382 Validation loss 0.054135363548994064 Accuracy 0.454345703125\n",
      "Iteration 13580 Training loss 0.04968617111444473 Validation loss 0.054782312363386154 Accuracy 0.44677734375\n",
      "Iteration 13590 Training loss 0.051870524883270264 Validation loss 0.05414603278040886 Accuracy 0.454345703125\n",
      "Iteration 13600 Training loss 0.05986249819397926 Validation loss 0.055507928133010864 Accuracy 0.4404296875\n",
      "Iteration 13610 Training loss 0.051299355924129486 Validation loss 0.05421676114201546 Accuracy 0.45361328125\n",
      "Iteration 13620 Training loss 0.05002659931778908 Validation loss 0.05414680019021034 Accuracy 0.4541015625\n",
      "Iteration 13630 Training loss 0.05253800377249718 Validation loss 0.054051660001277924 Accuracy 0.454345703125\n",
      "Iteration 13640 Training loss 0.05908003821969032 Validation loss 0.05411962792277336 Accuracy 0.454345703125\n",
      "Iteration 13650 Training loss 0.055044736713171005 Validation loss 0.05440362170338631 Accuracy 0.45166015625\n",
      "Iteration 13660 Training loss 0.05098043754696846 Validation loss 0.054405659437179565 Accuracy 0.451171875\n",
      "Iteration 13670 Training loss 0.05176447331905365 Validation loss 0.05432264134287834 Accuracy 0.4521484375\n",
      "Iteration 13680 Training loss 0.04756917431950569 Validation loss 0.05414006859064102 Accuracy 0.45458984375\n",
      "Iteration 13690 Training loss 0.05222439020872116 Validation loss 0.0542987696826458 Accuracy 0.45263671875\n",
      "Iteration 13700 Training loss 0.05481269210577011 Validation loss 0.054980576038360596 Accuracy 0.445556640625\n",
      "Iteration 13710 Training loss 0.05324641242623329 Validation loss 0.05453791841864586 Accuracy 0.449951171875\n",
      "Iteration 13720 Training loss 0.05214216187596321 Validation loss 0.054368212819099426 Accuracy 0.451416015625\n",
      "Iteration 13730 Training loss 0.05280841514468193 Validation loss 0.054068610072135925 Accuracy 0.454345703125\n",
      "Iteration 13740 Training loss 0.05131712928414345 Validation loss 0.05464750528335571 Accuracy 0.448974609375\n",
      "Iteration 13750 Training loss 0.05203947797417641 Validation loss 0.05414890870451927 Accuracy 0.453857421875\n",
      "Iteration 13760 Training loss 0.05268272012472153 Validation loss 0.05462750792503357 Accuracy 0.44873046875\n",
      "Iteration 13770 Training loss 0.056860197335481644 Validation loss 0.05432962626218796 Accuracy 0.452392578125\n",
      "Iteration 13780 Training loss 0.05499746650457382 Validation loss 0.05438242107629776 Accuracy 0.450927734375\n",
      "Iteration 13790 Training loss 0.04713462293148041 Validation loss 0.054132379591464996 Accuracy 0.453857421875\n",
      "Iteration 13800 Training loss 0.052525706589221954 Validation loss 0.05404060333967209 Accuracy 0.455078125\n",
      "Iteration 13810 Training loss 0.05415632948279381 Validation loss 0.054107118397951126 Accuracy 0.453857421875\n",
      "Iteration 13820 Training loss 0.05352200195193291 Validation loss 0.05434747785329819 Accuracy 0.451416015625\n",
      "Iteration 13830 Training loss 0.050594452768564224 Validation loss 0.05404891446232796 Accuracy 0.454833984375\n",
      "Iteration 13840 Training loss 0.051750559359788895 Validation loss 0.05432681366801262 Accuracy 0.45166015625\n",
      "Iteration 13850 Training loss 0.055896416306495667 Validation loss 0.054445959627628326 Accuracy 0.45068359375\n",
      "Iteration 13860 Training loss 0.05406012758612633 Validation loss 0.054332226514816284 Accuracy 0.451904296875\n",
      "Iteration 13870 Training loss 0.05510856956243515 Validation loss 0.055461883544921875 Accuracy 0.440673828125\n",
      "Iteration 13880 Training loss 0.05208220332860947 Validation loss 0.05451662093400955 Accuracy 0.45068359375\n",
      "Iteration 13890 Training loss 0.052372537553310394 Validation loss 0.0544220507144928 Accuracy 0.451416015625\n",
      "Iteration 13900 Training loss 0.05626910179853439 Validation loss 0.05580209940671921 Accuracy 0.438232421875\n",
      "Iteration 13910 Training loss 0.05384264141321182 Validation loss 0.05436510965228081 Accuracy 0.451904296875\n",
      "Iteration 13920 Training loss 0.05040058493614197 Validation loss 0.05451961234211922 Accuracy 0.448974609375\n",
      "Iteration 13930 Training loss 0.05227428302168846 Validation loss 0.054456766694784164 Accuracy 0.451171875\n",
      "Iteration 13940 Training loss 0.0489078052341938 Validation loss 0.05403796583414078 Accuracy 0.454833984375\n",
      "Iteration 13950 Training loss 0.05252109840512276 Validation loss 0.05452141910791397 Accuracy 0.4501953125\n",
      "Iteration 13960 Training loss 0.04849029332399368 Validation loss 0.05416136234998703 Accuracy 0.453125\n",
      "Iteration 13970 Training loss 0.052157703787088394 Validation loss 0.054840754717588425 Accuracy 0.447021484375\n",
      "Iteration 13980 Training loss 0.050753578543663025 Validation loss 0.05460715666413307 Accuracy 0.4482421875\n",
      "Iteration 13990 Training loss 0.05296941474080086 Validation loss 0.05414688214659691 Accuracy 0.4541015625\n",
      "Iteration 14000 Training loss 0.05647037550806999 Validation loss 0.054112810641527176 Accuracy 0.45361328125\n",
      "Iteration 14010 Training loss 0.054765187203884125 Validation loss 0.054102733731269836 Accuracy 0.45361328125\n",
      "Iteration 14020 Training loss 0.052986402064561844 Validation loss 0.05439899116754532 Accuracy 0.450439453125\n",
      "Iteration 14030 Training loss 0.05051632970571518 Validation loss 0.05406120792031288 Accuracy 0.45361328125\n",
      "Iteration 14040 Training loss 0.053924232721328735 Validation loss 0.055127400904893875 Accuracy 0.44384765625\n",
      "Iteration 14050 Training loss 0.05396845564246178 Validation loss 0.054002802819013596 Accuracy 0.454345703125\n",
      "Iteration 14060 Training loss 0.04989103972911835 Validation loss 0.0540521964430809 Accuracy 0.45458984375\n",
      "Iteration 14070 Training loss 0.05443373695015907 Validation loss 0.05409754812717438 Accuracy 0.45458984375\n",
      "Iteration 14080 Training loss 0.05315328761935234 Validation loss 0.05412209406495094 Accuracy 0.45458984375\n",
      "Iteration 14090 Training loss 0.052730295807123184 Validation loss 0.054267942905426025 Accuracy 0.452880859375\n",
      "Iteration 14100 Training loss 0.05285511538386345 Validation loss 0.05407160148024559 Accuracy 0.454345703125\n",
      "Iteration 14110 Training loss 0.05373050272464752 Validation loss 0.05401104316115379 Accuracy 0.454345703125\n",
      "Iteration 14120 Training loss 0.054357223212718964 Validation loss 0.05438293144106865 Accuracy 0.45166015625\n",
      "Iteration 14130 Training loss 0.053291015326976776 Validation loss 0.0541825070977211 Accuracy 0.453369140625\n",
      "Iteration 14140 Training loss 0.05229419469833374 Validation loss 0.054205961525440216 Accuracy 0.45263671875\n",
      "Iteration 14150 Training loss 0.04923058673739433 Validation loss 0.054238446056842804 Accuracy 0.452392578125\n",
      "Iteration 14160 Training loss 0.05143648386001587 Validation loss 0.05410556122660637 Accuracy 0.45361328125\n",
      "Iteration 14170 Training loss 0.05449284240603447 Validation loss 0.054348915815353394 Accuracy 0.4521484375\n",
      "Iteration 14180 Training loss 0.05494334176182747 Validation loss 0.054196882992982864 Accuracy 0.453125\n",
      "Iteration 14190 Training loss 0.0544869527220726 Validation loss 0.05398260056972504 Accuracy 0.45556640625\n",
      "Iteration 14200 Training loss 0.053874094039201736 Validation loss 0.05400225520133972 Accuracy 0.455078125\n",
      "Iteration 14210 Training loss 0.05415692925453186 Validation loss 0.05401228368282318 Accuracy 0.454833984375\n",
      "Iteration 14220 Training loss 0.05287020653486252 Validation loss 0.054028190672397614 Accuracy 0.455322265625\n",
      "Iteration 14230 Training loss 0.05101945623755455 Validation loss 0.053982317447662354 Accuracy 0.455322265625\n",
      "Iteration 14240 Training loss 0.050427477806806564 Validation loss 0.05444099009037018 Accuracy 0.45068359375\n",
      "Iteration 14250 Training loss 0.05423685535788536 Validation loss 0.054356515407562256 Accuracy 0.452392578125\n",
      "Iteration 14260 Training loss 0.05223073437809944 Validation loss 0.055143553763628006 Accuracy 0.443603515625\n",
      "Iteration 14270 Training loss 0.05065589398145676 Validation loss 0.05466174706816673 Accuracy 0.4482421875\n",
      "Iteration 14280 Training loss 0.05105941742658615 Validation loss 0.05411948263645172 Accuracy 0.454345703125\n",
      "Iteration 14290 Training loss 0.051287941634655 Validation loss 0.05422521382570267 Accuracy 0.452880859375\n",
      "Iteration 14300 Training loss 0.0511443130671978 Validation loss 0.05411718785762787 Accuracy 0.454833984375\n",
      "Iteration 14310 Training loss 0.05155159533023834 Validation loss 0.05415596440434456 Accuracy 0.4541015625\n",
      "Iteration 14320 Training loss 0.05275535583496094 Validation loss 0.05411382019519806 Accuracy 0.45458984375\n",
      "Iteration 14330 Training loss 0.05545772239565849 Validation loss 0.05557726323604584 Accuracy 0.439208984375\n",
      "Iteration 14340 Training loss 0.05457289516925812 Validation loss 0.05427216738462448 Accuracy 0.4521484375\n",
      "Iteration 14350 Training loss 0.05463011562824249 Validation loss 0.055135902017354965 Accuracy 0.44287109375\n",
      "Iteration 14360 Training loss 0.05148059502243996 Validation loss 0.05408629775047302 Accuracy 0.45458984375\n",
      "Iteration 14370 Training loss 0.051404356956481934 Validation loss 0.054051365703344345 Accuracy 0.455078125\n",
      "Iteration 14380 Training loss 0.05583580583333969 Validation loss 0.054236747324466705 Accuracy 0.453369140625\n",
      "Iteration 14390 Training loss 0.0523308664560318 Validation loss 0.054225388914346695 Accuracy 0.452880859375\n",
      "Iteration 14400 Training loss 0.05086352303624153 Validation loss 0.05411754921078682 Accuracy 0.45361328125\n",
      "Iteration 14410 Training loss 0.05469029024243355 Validation loss 0.05407702922821045 Accuracy 0.454833984375\n",
      "Iteration 14420 Training loss 0.05626141279935837 Validation loss 0.05413281172513962 Accuracy 0.45361328125\n",
      "Iteration 14430 Training loss 0.05612998828291893 Validation loss 0.054159507155418396 Accuracy 0.455078125\n",
      "Iteration 14440 Training loss 0.0509926937520504 Validation loss 0.0541849210858345 Accuracy 0.453857421875\n",
      "Iteration 14450 Training loss 0.05497812479734421 Validation loss 0.05486074090003967 Accuracy 0.447265625\n",
      "Iteration 14460 Training loss 0.053086597472429276 Validation loss 0.054037902504205704 Accuracy 0.454345703125\n",
      "Iteration 14470 Training loss 0.05375877767801285 Validation loss 0.05399790778756142 Accuracy 0.45458984375\n",
      "Iteration 14480 Training loss 0.054804727435112 Validation loss 0.054245010018348694 Accuracy 0.453857421875\n",
      "Iteration 14490 Training loss 0.05378439277410507 Validation loss 0.05410858616232872 Accuracy 0.454833984375\n",
      "Iteration 14500 Training loss 0.053870461881160736 Validation loss 0.054105859249830246 Accuracy 0.455078125\n",
      "Iteration 14510 Training loss 0.05414343625307083 Validation loss 0.054112620651721954 Accuracy 0.45458984375\n",
      "Iteration 14520 Training loss 0.04787355661392212 Validation loss 0.05436364933848381 Accuracy 0.451904296875\n",
      "Iteration 14530 Training loss 0.05306354537606239 Validation loss 0.054560139775276184 Accuracy 0.4501953125\n",
      "Iteration 14540 Training loss 0.05046429485082626 Validation loss 0.054386042058467865 Accuracy 0.451904296875\n",
      "Iteration 14550 Training loss 0.05032585561275482 Validation loss 0.054169218987226486 Accuracy 0.453857421875\n",
      "Iteration 14560 Training loss 0.0554802343249321 Validation loss 0.054566819220781326 Accuracy 0.45068359375\n",
      "Iteration 14570 Training loss 0.05323171615600586 Validation loss 0.05404883250594139 Accuracy 0.45458984375\n",
      "Iteration 14580 Training loss 0.053458064794540405 Validation loss 0.054169245064258575 Accuracy 0.4541015625\n",
      "Iteration 14590 Training loss 0.05249461904168129 Validation loss 0.0540367066860199 Accuracy 0.455078125\n",
      "Iteration 14600 Training loss 0.05015387013554573 Validation loss 0.05420100316405296 Accuracy 0.453369140625\n",
      "Iteration 14610 Training loss 0.04919145256280899 Validation loss 0.05429479479789734 Accuracy 0.452880859375\n",
      "Iteration 14620 Training loss 0.04984012246131897 Validation loss 0.054428622126579285 Accuracy 0.45068359375\n",
      "Iteration 14630 Training loss 0.05428832769393921 Validation loss 0.054233066737651825 Accuracy 0.452880859375\n",
      "Iteration 14640 Training loss 0.05354049801826477 Validation loss 0.054247040301561356 Accuracy 0.451904296875\n",
      "Iteration 14650 Training loss 0.05380655825138092 Validation loss 0.0544775165617466 Accuracy 0.4501953125\n",
      "Iteration 14660 Training loss 0.05380062758922577 Validation loss 0.05410962924361229 Accuracy 0.4541015625\n",
      "Iteration 14670 Training loss 0.052646830677986145 Validation loss 0.054565444588661194 Accuracy 0.4501953125\n",
      "Iteration 14680 Training loss 0.05559999495744705 Validation loss 0.05413055419921875 Accuracy 0.45361328125\n",
      "Iteration 14690 Training loss 0.05213660001754761 Validation loss 0.05403616651892662 Accuracy 0.45458984375\n",
      "Iteration 14700 Training loss 0.05135420337319374 Validation loss 0.05426403135061264 Accuracy 0.452392578125\n",
      "Iteration 14710 Training loss 0.055948421359062195 Validation loss 0.05583875998854637 Accuracy 0.43603515625\n",
      "Iteration 14720 Training loss 0.05168868973851204 Validation loss 0.054673027247190475 Accuracy 0.448974609375\n",
      "Iteration 14730 Training loss 0.05220268666744232 Validation loss 0.05412396788597107 Accuracy 0.4541015625\n",
      "Iteration 14740 Training loss 0.04758281260728836 Validation loss 0.05417706072330475 Accuracy 0.453125\n",
      "Iteration 14750 Training loss 0.05508298799395561 Validation loss 0.05416315421462059 Accuracy 0.454345703125\n",
      "Iteration 14760 Training loss 0.05253204330801964 Validation loss 0.05440344661474228 Accuracy 0.45166015625\n",
      "Iteration 14770 Training loss 0.05116749554872513 Validation loss 0.05412929877638817 Accuracy 0.45458984375\n",
      "Iteration 14780 Training loss 0.0517440140247345 Validation loss 0.05429404601454735 Accuracy 0.4521484375\n",
      "Iteration 14790 Training loss 0.050371114164590836 Validation loss 0.05418925732374191 Accuracy 0.453369140625\n",
      "Iteration 14800 Training loss 0.052442651242017746 Validation loss 0.054664451628923416 Accuracy 0.448486328125\n",
      "Iteration 14810 Training loss 0.05226028338074684 Validation loss 0.05457468703389168 Accuracy 0.44921875\n",
      "Iteration 14820 Training loss 0.05599147453904152 Validation loss 0.054804880172014236 Accuracy 0.447998046875\n",
      "Iteration 14830 Training loss 0.05363725125789642 Validation loss 0.05402073264122009 Accuracy 0.45458984375\n",
      "Iteration 14840 Training loss 0.049911078065633774 Validation loss 0.05451154336333275 Accuracy 0.4501953125\n",
      "Iteration 14850 Training loss 0.0511503592133522 Validation loss 0.05410204827785492 Accuracy 0.4541015625\n",
      "Iteration 14860 Training loss 0.05420438200235367 Validation loss 0.05411335453391075 Accuracy 0.453857421875\n",
      "Iteration 14870 Training loss 0.051904480904340744 Validation loss 0.05421062186360359 Accuracy 0.452880859375\n",
      "Iteration 14880 Training loss 0.05259111523628235 Validation loss 0.05432666838169098 Accuracy 0.45166015625\n",
      "Iteration 14890 Training loss 0.05582104250788689 Validation loss 0.054831840097904205 Accuracy 0.44677734375\n",
      "Iteration 14900 Training loss 0.05475853756070137 Validation loss 0.05438891425728798 Accuracy 0.45068359375\n",
      "Iteration 14910 Training loss 0.05651362985372543 Validation loss 0.05426390841603279 Accuracy 0.452392578125\n",
      "Iteration 14920 Training loss 0.0541500449180603 Validation loss 0.05415752902626991 Accuracy 0.45361328125\n",
      "Iteration 14930 Training loss 0.047907840460538864 Validation loss 0.05415811389684677 Accuracy 0.45361328125\n",
      "Iteration 14940 Training loss 0.05632936581969261 Validation loss 0.055314403027296066 Accuracy 0.441650390625\n",
      "Iteration 14950 Training loss 0.051488276571035385 Validation loss 0.05447003245353699 Accuracy 0.44970703125\n",
      "Iteration 14960 Training loss 0.05249183624982834 Validation loss 0.0543661005795002 Accuracy 0.451416015625\n",
      "Iteration 14970 Training loss 0.05436313897371292 Validation loss 0.05456706881523132 Accuracy 0.44921875\n",
      "Iteration 14980 Training loss 0.05476374924182892 Validation loss 0.05458502471446991 Accuracy 0.449462890625\n",
      "Iteration 14990 Training loss 0.054776642471551895 Validation loss 0.05671730265021324 Accuracy 0.42822265625\n",
      "Iteration 15000 Training loss 0.05334782227873802 Validation loss 0.0541701540350914 Accuracy 0.452880859375\n",
      "Iteration 15010 Training loss 0.05424237623810768 Validation loss 0.05412755906581879 Accuracy 0.4541015625\n",
      "Iteration 15020 Training loss 0.05340810492634773 Validation loss 0.0543806292116642 Accuracy 0.45068359375\n",
      "Iteration 15030 Training loss 0.05405111610889435 Validation loss 0.054584089666604996 Accuracy 0.44921875\n",
      "Iteration 15040 Training loss 0.05181385576725006 Validation loss 0.054210226982831955 Accuracy 0.452880859375\n",
      "Iteration 15050 Training loss 0.050281330943107605 Validation loss 0.05401809886097908 Accuracy 0.45556640625\n",
      "Iteration 15060 Training loss 0.054940927773714066 Validation loss 0.053931064903736115 Accuracy 0.455810546875\n",
      "Iteration 15070 Training loss 0.051775917410850525 Validation loss 0.0545971654355526 Accuracy 0.449462890625\n",
      "Iteration 15080 Training loss 0.05807836353778839 Validation loss 0.05393299087882042 Accuracy 0.456298828125\n",
      "Iteration 15090 Training loss 0.050539519637823105 Validation loss 0.054129861295223236 Accuracy 0.45361328125\n",
      "Iteration 15100 Training loss 0.05138035863637924 Validation loss 0.054163608700037 Accuracy 0.4541015625\n",
      "Iteration 15110 Training loss 0.04857294633984566 Validation loss 0.05434877797961235 Accuracy 0.4521484375\n",
      "Iteration 15120 Training loss 0.05271759629249573 Validation loss 0.05423528328537941 Accuracy 0.452880859375\n",
      "Iteration 15130 Training loss 0.052144862711429596 Validation loss 0.05395621061325073 Accuracy 0.455810546875\n",
      "Iteration 15140 Training loss 0.05260566994547844 Validation loss 0.05429902300238609 Accuracy 0.4521484375\n",
      "Iteration 15150 Training loss 0.05443108454346657 Validation loss 0.05411970615386963 Accuracy 0.4541015625\n",
      "Iteration 15160 Training loss 0.05543072894215584 Validation loss 0.054239559918642044 Accuracy 0.45361328125\n",
      "Iteration 15170 Training loss 0.05574248731136322 Validation loss 0.05425070971250534 Accuracy 0.45361328125\n",
      "Iteration 15180 Training loss 0.052325326949357986 Validation loss 0.054153479635715485 Accuracy 0.45458984375\n",
      "Iteration 15190 Training loss 0.05488019809126854 Validation loss 0.05430317297577858 Accuracy 0.450927734375\n",
      "Iteration 15200 Training loss 0.05242544040083885 Validation loss 0.05501669645309448 Accuracy 0.444580078125\n",
      "Iteration 15210 Training loss 0.052532587200403214 Validation loss 0.0546242818236351 Accuracy 0.448486328125\n",
      "Iteration 15220 Training loss 0.05173955112695694 Validation loss 0.0541483499109745 Accuracy 0.45361328125\n",
      "Iteration 15230 Training loss 0.05256817489862442 Validation loss 0.0540035143494606 Accuracy 0.45556640625\n",
      "Iteration 15240 Training loss 0.052148766815662384 Validation loss 0.05411505326628685 Accuracy 0.453369140625\n",
      "Iteration 15250 Training loss 0.051323045045137405 Validation loss 0.05410784110426903 Accuracy 0.4541015625\n",
      "Iteration 15260 Training loss 0.05140428617596626 Validation loss 0.05415328964591026 Accuracy 0.453369140625\n",
      "Iteration 15270 Training loss 0.052134815603494644 Validation loss 0.05413608253002167 Accuracy 0.453857421875\n",
      "Iteration 15280 Training loss 0.049082059413194656 Validation loss 0.0541827417910099 Accuracy 0.453369140625\n",
      "Iteration 15290 Training loss 0.051967743784189224 Validation loss 0.054261982440948486 Accuracy 0.452392578125\n",
      "Iteration 15300 Training loss 0.053329162299633026 Validation loss 0.05436469614505768 Accuracy 0.451171875\n",
      "Iteration 15310 Training loss 0.04938909783959389 Validation loss 0.054134100675582886 Accuracy 0.453857421875\n",
      "Iteration 15320 Training loss 0.05455845966935158 Validation loss 0.05452769994735718 Accuracy 0.449462890625\n",
      "Iteration 15330 Training loss 0.05479026213288307 Validation loss 0.054681941866874695 Accuracy 0.448486328125\n",
      "Iteration 15340 Training loss 0.051516152918338776 Validation loss 0.05494038760662079 Accuracy 0.444580078125\n",
      "Iteration 15350 Training loss 0.05025394633412361 Validation loss 0.05399103835225105 Accuracy 0.455078125\n",
      "Iteration 15360 Training loss 0.05274038761854172 Validation loss 0.054176926612854004 Accuracy 0.45361328125\n",
      "Iteration 15370 Training loss 0.05431213229894638 Validation loss 0.054136019200086594 Accuracy 0.453369140625\n",
      "Iteration 15380 Training loss 0.05252565070986748 Validation loss 0.05429566651582718 Accuracy 0.4521484375\n",
      "Iteration 15390 Training loss 0.05283350124955177 Validation loss 0.05433295667171478 Accuracy 0.451171875\n",
      "Iteration 15400 Training loss 0.05337214097380638 Validation loss 0.054272040724754333 Accuracy 0.4521484375\n",
      "Iteration 15410 Training loss 0.05281398445367813 Validation loss 0.05431343615055084 Accuracy 0.451416015625\n",
      "Iteration 15420 Training loss 0.05419604107737541 Validation loss 0.054170507937669754 Accuracy 0.45361328125\n",
      "Iteration 15430 Training loss 0.05114658549427986 Validation loss 0.05414117872714996 Accuracy 0.452880859375\n",
      "Iteration 15440 Training loss 0.054495301097631454 Validation loss 0.05413477495312691 Accuracy 0.453857421875\n",
      "Iteration 15450 Training loss 0.05283823609352112 Validation loss 0.05405894294381142 Accuracy 0.45361328125\n",
      "Iteration 15460 Training loss 0.05145876109600067 Validation loss 0.05463840067386627 Accuracy 0.44921875\n",
      "Iteration 15470 Training loss 0.055798258632421494 Validation loss 0.05488565191626549 Accuracy 0.446533203125\n",
      "Iteration 15480 Training loss 0.05355564132332802 Validation loss 0.054336145520210266 Accuracy 0.451416015625\n",
      "Iteration 15490 Training loss 0.05459684133529663 Validation loss 0.054814696311950684 Accuracy 0.447021484375\n",
      "Iteration 15500 Training loss 0.05184943601489067 Validation loss 0.05416017025709152 Accuracy 0.45263671875\n",
      "Iteration 15510 Training loss 0.05094267055392265 Validation loss 0.05449143424630165 Accuracy 0.450439453125\n",
      "Iteration 15520 Training loss 0.05163300409913063 Validation loss 0.05423999950289726 Accuracy 0.452392578125\n",
      "Iteration 15530 Training loss 0.05553845316171646 Validation loss 0.05407993867993355 Accuracy 0.4541015625\n",
      "Iteration 15540 Training loss 0.05308491364121437 Validation loss 0.054078176617622375 Accuracy 0.45361328125\n",
      "Iteration 15550 Training loss 0.05341682210564613 Validation loss 0.05416480451822281 Accuracy 0.45361328125\n",
      "Iteration 15560 Training loss 0.054521817713975906 Validation loss 0.054054293781518936 Accuracy 0.454345703125\n",
      "Iteration 15570 Training loss 0.05478740483522415 Validation loss 0.05423470586538315 Accuracy 0.452392578125\n",
      "Iteration 15580 Training loss 0.05061064660549164 Validation loss 0.05413605272769928 Accuracy 0.453125\n",
      "Iteration 15590 Training loss 0.05521475151181221 Validation loss 0.05406733229756355 Accuracy 0.45458984375\n",
      "Iteration 15600 Training loss 0.05009045451879501 Validation loss 0.054092343896627426 Accuracy 0.45361328125\n",
      "Iteration 15610 Training loss 0.052461013197898865 Validation loss 0.05399983748793602 Accuracy 0.454833984375\n",
      "Iteration 15620 Training loss 0.04991496726870537 Validation loss 0.053989212960004807 Accuracy 0.45458984375\n",
      "Iteration 15630 Training loss 0.05160759389400482 Validation loss 0.0543145015835762 Accuracy 0.451904296875\n",
      "Iteration 15640 Training loss 0.05151006579399109 Validation loss 0.05410638079047203 Accuracy 0.453369140625\n",
      "Iteration 15650 Training loss 0.04966254159808159 Validation loss 0.05406945198774338 Accuracy 0.45458984375\n",
      "Iteration 15660 Training loss 0.052393682301044464 Validation loss 0.0541999489068985 Accuracy 0.453125\n",
      "Iteration 15670 Training loss 0.05305569991469383 Validation loss 0.05413396283984184 Accuracy 0.45361328125\n",
      "Iteration 15680 Training loss 0.050040699541568756 Validation loss 0.054246868938207626 Accuracy 0.452392578125\n",
      "Iteration 15690 Training loss 0.05508003756403923 Validation loss 0.054169755429029465 Accuracy 0.45361328125\n",
      "Iteration 15700 Training loss 0.05250724405050278 Validation loss 0.054066162556409836 Accuracy 0.454345703125\n",
      "Iteration 15710 Training loss 0.05284871906042099 Validation loss 0.05420070141553879 Accuracy 0.452392578125\n",
      "Iteration 15720 Training loss 0.0515914261341095 Validation loss 0.054477185010910034 Accuracy 0.4501953125\n",
      "Iteration 15730 Training loss 0.05390993878245354 Validation loss 0.054213717579841614 Accuracy 0.452880859375\n",
      "Iteration 15740 Training loss 0.05602605268359184 Validation loss 0.05422305688261986 Accuracy 0.453369140625\n",
      "Iteration 15750 Training loss 0.055752869695425034 Validation loss 0.054196547716856 Accuracy 0.452880859375\n",
      "Iteration 15760 Training loss 0.05507867410778999 Validation loss 0.05403721705079079 Accuracy 0.455078125\n",
      "Iteration 15770 Training loss 0.053868066519498825 Validation loss 0.054407089948654175 Accuracy 0.45068359375\n",
      "Iteration 15780 Training loss 0.05446583405137062 Validation loss 0.05430107191205025 Accuracy 0.451904296875\n",
      "Iteration 15790 Training loss 0.053511686623096466 Validation loss 0.054098110646009445 Accuracy 0.454833984375\n",
      "Iteration 15800 Training loss 0.05193790793418884 Validation loss 0.05412985384464264 Accuracy 0.45361328125\n",
      "Iteration 15810 Training loss 0.0516427606344223 Validation loss 0.054468944668769836 Accuracy 0.450927734375\n",
      "Iteration 15820 Training loss 0.05459609255194664 Validation loss 0.05416899919509888 Accuracy 0.452880859375\n",
      "Iteration 15830 Training loss 0.05305081978440285 Validation loss 0.05440155789256096 Accuracy 0.451904296875\n",
      "Iteration 15840 Training loss 0.05526254326105118 Validation loss 0.056287381798028946 Accuracy 0.433349609375\n",
      "Iteration 15850 Training loss 0.05239633098244667 Validation loss 0.05418560281395912 Accuracy 0.45361328125\n",
      "Iteration 15860 Training loss 0.054480619728565216 Validation loss 0.05433182790875435 Accuracy 0.45166015625\n",
      "Iteration 15870 Training loss 0.05146040767431259 Validation loss 0.05420456826686859 Accuracy 0.452880859375\n",
      "Iteration 15880 Training loss 0.05141054838895798 Validation loss 0.05419887602329254 Accuracy 0.453369140625\n",
      "Iteration 15890 Training loss 0.053428154438734055 Validation loss 0.05423771217465401 Accuracy 0.4521484375\n",
      "Iteration 15900 Training loss 0.05462007597088814 Validation loss 0.054178714752197266 Accuracy 0.452392578125\n",
      "Iteration 15910 Training loss 0.0520683191716671 Validation loss 0.054297201335430145 Accuracy 0.451904296875\n",
      "Iteration 15920 Training loss 0.05883607268333435 Validation loss 0.05408279225230217 Accuracy 0.454345703125\n",
      "Iteration 15930 Training loss 0.05248760059475899 Validation loss 0.054121848195791245 Accuracy 0.4541015625\n",
      "Iteration 15940 Training loss 0.05491698160767555 Validation loss 0.05486489459872246 Accuracy 0.446044921875\n",
      "Iteration 15950 Training loss 0.051956720650196075 Validation loss 0.053993161767721176 Accuracy 0.455078125\n",
      "Iteration 15960 Training loss 0.05070902407169342 Validation loss 0.05431832745671272 Accuracy 0.452392578125\n",
      "Iteration 15970 Training loss 0.05129436030983925 Validation loss 0.05510491132736206 Accuracy 0.443603515625\n",
      "Iteration 15980 Training loss 0.05013910308480263 Validation loss 0.05420643463730812 Accuracy 0.452880859375\n",
      "Iteration 15990 Training loss 0.05244116112589836 Validation loss 0.054091766476631165 Accuracy 0.4541015625\n",
      "Iteration 16000 Training loss 0.05283602699637413 Validation loss 0.05418384447693825 Accuracy 0.45361328125\n",
      "Iteration 16010 Training loss 0.05399942025542259 Validation loss 0.054264895617961884 Accuracy 0.45166015625\n",
      "Iteration 16020 Training loss 0.05272438004612923 Validation loss 0.054962508380413055 Accuracy 0.445068359375\n",
      "Iteration 16030 Training loss 0.05300745740532875 Validation loss 0.05409693345427513 Accuracy 0.45361328125\n",
      "Iteration 16040 Training loss 0.054356832057237625 Validation loss 0.0541909784078598 Accuracy 0.452880859375\n",
      "Iteration 16050 Training loss 0.056063342839479446 Validation loss 0.0540030263364315 Accuracy 0.45458984375\n",
      "Iteration 16060 Training loss 0.05667557194828987 Validation loss 0.05470002815127373 Accuracy 0.447265625\n",
      "Iteration 16070 Training loss 0.04907859116792679 Validation loss 0.054080039262771606 Accuracy 0.4541015625\n",
      "Iteration 16080 Training loss 0.049981627613306046 Validation loss 0.05434069782495499 Accuracy 0.451416015625\n",
      "Iteration 16090 Training loss 0.05383384972810745 Validation loss 0.054244209080934525 Accuracy 0.451416015625\n",
      "Iteration 16100 Training loss 0.05414191633462906 Validation loss 0.054634470492601395 Accuracy 0.447998046875\n",
      "Iteration 16110 Training loss 0.05039876326918602 Validation loss 0.05407895892858505 Accuracy 0.4541015625\n",
      "Iteration 16120 Training loss 0.0570094920694828 Validation loss 0.05405561253428459 Accuracy 0.4541015625\n",
      "Iteration 16130 Training loss 0.05323514714837074 Validation loss 0.05431322008371353 Accuracy 0.450927734375\n",
      "Iteration 16140 Training loss 0.05264919996261597 Validation loss 0.05414894223213196 Accuracy 0.453125\n",
      "Iteration 16150 Training loss 0.05227683484554291 Validation loss 0.054193414747714996 Accuracy 0.453125\n",
      "Iteration 16160 Training loss 0.05415980890393257 Validation loss 0.05420581251382828 Accuracy 0.45263671875\n",
      "Iteration 16170 Training loss 0.052520185708999634 Validation loss 0.05418849363923073 Accuracy 0.452392578125\n",
      "Iteration 16180 Training loss 0.054954588413238525 Validation loss 0.05449962615966797 Accuracy 0.449951171875\n",
      "Iteration 16190 Training loss 0.052413471043109894 Validation loss 0.054225094616413116 Accuracy 0.452392578125\n",
      "Iteration 16200 Training loss 0.05192406103014946 Validation loss 0.05475523695349693 Accuracy 0.447021484375\n",
      "Iteration 16210 Training loss 0.0494825579226017 Validation loss 0.05426035076379776 Accuracy 0.45263671875\n",
      "Iteration 16220 Training loss 0.05482938513159752 Validation loss 0.054202865809202194 Accuracy 0.452880859375\n",
      "Iteration 16230 Training loss 0.05106348544359207 Validation loss 0.054038695991039276 Accuracy 0.45458984375\n",
      "Iteration 16240 Training loss 0.05248481035232544 Validation loss 0.05430636554956436 Accuracy 0.4521484375\n",
      "Iteration 16250 Training loss 0.05297979339957237 Validation loss 0.0541885644197464 Accuracy 0.453369140625\n",
      "Iteration 16260 Training loss 0.05092087760567665 Validation loss 0.05453638359904289 Accuracy 0.45068359375\n",
      "Iteration 16270 Training loss 0.05225181579589844 Validation loss 0.0540333054959774 Accuracy 0.4541015625\n",
      "Iteration 16280 Training loss 0.05548139289021492 Validation loss 0.05449584871530533 Accuracy 0.4501953125\n",
      "Iteration 16290 Training loss 0.052558571100234985 Validation loss 0.05419198051095009 Accuracy 0.453369140625\n",
      "Iteration 16300 Training loss 0.05099429190158844 Validation loss 0.05446571111679077 Accuracy 0.451171875\n",
      "Iteration 16310 Training loss 0.054184503853321075 Validation loss 0.05414937436580658 Accuracy 0.452880859375\n",
      "Iteration 16320 Training loss 0.05351962894201279 Validation loss 0.05439773201942444 Accuracy 0.450439453125\n",
      "Iteration 16330 Training loss 0.05228130519390106 Validation loss 0.05412086844444275 Accuracy 0.45361328125\n",
      "Iteration 16340 Training loss 0.0529666431248188 Validation loss 0.05409342795610428 Accuracy 0.453857421875\n",
      "Iteration 16350 Training loss 0.05193593353033066 Validation loss 0.054157182574272156 Accuracy 0.45361328125\n",
      "Iteration 16360 Training loss 0.05509549379348755 Validation loss 0.05429977923631668 Accuracy 0.451416015625\n",
      "Iteration 16370 Training loss 0.053652092814445496 Validation loss 0.05414431169629097 Accuracy 0.453125\n",
      "Iteration 16380 Training loss 0.05595378950238228 Validation loss 0.054131630808115005 Accuracy 0.4541015625\n",
      "Iteration 16390 Training loss 0.05296952277421951 Validation loss 0.05459137633442879 Accuracy 0.449462890625\n",
      "Iteration 16400 Training loss 0.05104171857237816 Validation loss 0.05411612242460251 Accuracy 0.453125\n",
      "Iteration 16410 Training loss 0.05003613233566284 Validation loss 0.05438440665602684 Accuracy 0.450439453125\n",
      "Iteration 16420 Training loss 0.05220222473144531 Validation loss 0.0539889894425869 Accuracy 0.454833984375\n",
      "Iteration 16430 Training loss 0.05251443013548851 Validation loss 0.05403614789247513 Accuracy 0.45458984375\n",
      "Iteration 16440 Training loss 0.049180448055267334 Validation loss 0.0540667288005352 Accuracy 0.45458984375\n",
      "Iteration 16450 Training loss 0.048998717218637466 Validation loss 0.05444837734103203 Accuracy 0.45068359375\n",
      "Iteration 16460 Training loss 0.05126209557056427 Validation loss 0.0542161799967289 Accuracy 0.452392578125\n",
      "Iteration 16470 Training loss 0.05286146327853203 Validation loss 0.05425293743610382 Accuracy 0.453369140625\n",
      "Iteration 16480 Training loss 0.0572415366768837 Validation loss 0.05441935732960701 Accuracy 0.451171875\n",
      "Iteration 16490 Training loss 0.053063683211803436 Validation loss 0.05454336851835251 Accuracy 0.44970703125\n",
      "Iteration 16500 Training loss 0.051424916833639145 Validation loss 0.05406457930803299 Accuracy 0.454833984375\n",
      "Iteration 16510 Training loss 0.054307978600263596 Validation loss 0.05399806424975395 Accuracy 0.454833984375\n",
      "Iteration 16520 Training loss 0.05124210938811302 Validation loss 0.05429701507091522 Accuracy 0.452392578125\n",
      "Iteration 16530 Training loss 0.052684396505355835 Validation loss 0.05487515404820442 Accuracy 0.4462890625\n",
      "Iteration 16540 Training loss 0.05018669739365578 Validation loss 0.05402544140815735 Accuracy 0.454345703125\n",
      "Iteration 16550 Training loss 0.04994099959731102 Validation loss 0.05414446070790291 Accuracy 0.45361328125\n",
      "Iteration 16560 Training loss 0.05118789151310921 Validation loss 0.054059721529483795 Accuracy 0.454833984375\n",
      "Iteration 16570 Training loss 0.050837986171245575 Validation loss 0.05397908389568329 Accuracy 0.455078125\n",
      "Iteration 16580 Training loss 0.052320096641778946 Validation loss 0.05388083681464195 Accuracy 0.455810546875\n",
      "Iteration 16590 Training loss 0.049207743257284164 Validation loss 0.05413138493895531 Accuracy 0.453857421875\n",
      "Iteration 16600 Training loss 0.05098031088709831 Validation loss 0.05402141064405441 Accuracy 0.45458984375\n",
      "Iteration 16610 Training loss 0.05282548815011978 Validation loss 0.055231161415576935 Accuracy 0.4423828125\n",
      "Iteration 16620 Training loss 0.05118289962410927 Validation loss 0.0540010929107666 Accuracy 0.45458984375\n",
      "Iteration 16630 Training loss 0.052367258816957474 Validation loss 0.05409052222967148 Accuracy 0.452880859375\n",
      "Iteration 16640 Training loss 0.0523001104593277 Validation loss 0.054271772503852844 Accuracy 0.45263671875\n",
      "Iteration 16650 Training loss 0.0540275014936924 Validation loss 0.05415661633014679 Accuracy 0.45361328125\n",
      "Iteration 16660 Training loss 0.04982695356011391 Validation loss 0.05408850312232971 Accuracy 0.454833984375\n",
      "Iteration 16670 Training loss 0.054378144443035126 Validation loss 0.05403805524110794 Accuracy 0.45458984375\n",
      "Iteration 16680 Training loss 0.04956132918596268 Validation loss 0.05412662774324417 Accuracy 0.4541015625\n",
      "Iteration 16690 Training loss 0.05553935095667839 Validation loss 0.05405420809984207 Accuracy 0.454345703125\n",
      "Iteration 16700 Training loss 0.05490594357252121 Validation loss 0.055215898901224136 Accuracy 0.4423828125\n",
      "Iteration 16710 Training loss 0.050800759345293045 Validation loss 0.05411461740732193 Accuracy 0.45361328125\n",
      "Iteration 16720 Training loss 0.05438494682312012 Validation loss 0.055446747690439224 Accuracy 0.440673828125\n",
      "Iteration 16730 Training loss 0.053324125707149506 Validation loss 0.05463556572794914 Accuracy 0.44921875\n",
      "Iteration 16740 Training loss 0.05534776300191879 Validation loss 0.053948625922203064 Accuracy 0.455322265625\n",
      "Iteration 16750 Training loss 0.05309483781456947 Validation loss 0.05421169847249985 Accuracy 0.45263671875\n",
      "Iteration 16760 Training loss 0.051939234137535095 Validation loss 0.05411434546113014 Accuracy 0.45361328125\n",
      "Iteration 16770 Training loss 0.05576593056321144 Validation loss 0.054188817739486694 Accuracy 0.453369140625\n",
      "Iteration 16780 Training loss 0.0530264787375927 Validation loss 0.054098885506391525 Accuracy 0.453857421875\n",
      "Iteration 16790 Training loss 0.056266285479068756 Validation loss 0.05492653325200081 Accuracy 0.445068359375\n",
      "Iteration 16800 Training loss 0.05379582196474075 Validation loss 0.05446207523345947 Accuracy 0.45068359375\n",
      "Iteration 16810 Training loss 0.05240746960043907 Validation loss 0.05402873083949089 Accuracy 0.455078125\n",
      "Iteration 16820 Training loss 0.049632538110017776 Validation loss 0.05464259535074234 Accuracy 0.448974609375\n",
      "Iteration 16830 Training loss 0.05338848754763603 Validation loss 0.05404795706272125 Accuracy 0.45458984375\n",
      "Iteration 16840 Training loss 0.051397085189819336 Validation loss 0.054710693657398224 Accuracy 0.448486328125\n",
      "Iteration 16850 Training loss 0.053565505892038345 Validation loss 0.054244089871644974 Accuracy 0.4521484375\n",
      "Iteration 16860 Training loss 0.05277308076620102 Validation loss 0.05407361313700676 Accuracy 0.4541015625\n",
      "Iteration 16870 Training loss 0.04796484485268593 Validation loss 0.054222602397203445 Accuracy 0.45361328125\n",
      "Iteration 16880 Training loss 0.05341898649930954 Validation loss 0.05489826947450638 Accuracy 0.445556640625\n",
      "Iteration 16890 Training loss 0.05355197191238403 Validation loss 0.05446041747927666 Accuracy 0.4501953125\n",
      "Iteration 16900 Training loss 0.05190590023994446 Validation loss 0.05416163057088852 Accuracy 0.453857421875\n",
      "Iteration 16910 Training loss 0.05321623757481575 Validation loss 0.05424990504980087 Accuracy 0.45361328125\n",
      "Iteration 16920 Training loss 0.05147910490632057 Validation loss 0.05437678098678589 Accuracy 0.451904296875\n",
      "Iteration 16930 Training loss 0.05356765538454056 Validation loss 0.05407540500164032 Accuracy 0.454833984375\n",
      "Iteration 16940 Training loss 0.05388181656599045 Validation loss 0.0555734857916832 Accuracy 0.43896484375\n",
      "Iteration 16950 Training loss 0.049087945371866226 Validation loss 0.054037656635046005 Accuracy 0.455078125\n",
      "Iteration 16960 Training loss 0.05237244442105293 Validation loss 0.05471455305814743 Accuracy 0.447509765625\n",
      "Iteration 16970 Training loss 0.0535060316324234 Validation loss 0.05421855300664902 Accuracy 0.452880859375\n",
      "Iteration 16980 Training loss 0.05350590869784355 Validation loss 0.05416326969861984 Accuracy 0.453369140625\n",
      "Iteration 16990 Training loss 0.05246191471815109 Validation loss 0.0545208640396595 Accuracy 0.44970703125\n",
      "Iteration 17000 Training loss 0.056143466383218765 Validation loss 0.053974661976099014 Accuracy 0.455078125\n",
      "Iteration 17010 Training loss 0.05354807898402214 Validation loss 0.05419262498617172 Accuracy 0.452880859375\n",
      "Iteration 17020 Training loss 0.053878795355558395 Validation loss 0.05455130711197853 Accuracy 0.449951171875\n",
      "Iteration 17030 Training loss 0.05405130982398987 Validation loss 0.05459681525826454 Accuracy 0.448486328125\n",
      "Iteration 17040 Training loss 0.05645180121064186 Validation loss 0.054042283445596695 Accuracy 0.4541015625\n",
      "Iteration 17050 Training loss 0.05351835489273071 Validation loss 0.054154228419065475 Accuracy 0.452880859375\n",
      "Iteration 17060 Training loss 0.0565539225935936 Validation loss 0.054359763860702515 Accuracy 0.450439453125\n",
      "Iteration 17070 Training loss 0.05348961427807808 Validation loss 0.05408642441034317 Accuracy 0.454345703125\n",
      "Iteration 17080 Training loss 0.04971566051244736 Validation loss 0.054870788007974625 Accuracy 0.44580078125\n",
      "Iteration 17090 Training loss 0.05529977008700371 Validation loss 0.05419255793094635 Accuracy 0.452880859375\n",
      "Iteration 17100 Training loss 0.051396533846855164 Validation loss 0.054206620901823044 Accuracy 0.452392578125\n",
      "Iteration 17110 Training loss 0.049579206854104996 Validation loss 0.05451791360974312 Accuracy 0.449462890625\n",
      "Iteration 17120 Training loss 0.05513227358460426 Validation loss 0.054168473929166794 Accuracy 0.4541015625\n",
      "Iteration 17130 Training loss 0.0520920492708683 Validation loss 0.054070618003606796 Accuracy 0.45458984375\n",
      "Iteration 17140 Training loss 0.054121702909469604 Validation loss 0.05400535464286804 Accuracy 0.45458984375\n",
      "Iteration 17150 Training loss 0.056057482957839966 Validation loss 0.05402161926031113 Accuracy 0.455078125\n",
      "Iteration 17160 Training loss 0.05137842148542404 Validation loss 0.05420701950788498 Accuracy 0.452880859375\n",
      "Iteration 17170 Training loss 0.05224466323852539 Validation loss 0.05434483289718628 Accuracy 0.452392578125\n",
      "Iteration 17180 Training loss 0.05242875590920448 Validation loss 0.05404310300946236 Accuracy 0.45458984375\n",
      "Iteration 17190 Training loss 0.05365310609340668 Validation loss 0.054368700832128525 Accuracy 0.45166015625\n",
      "Iteration 17200 Training loss 0.048758458346128464 Validation loss 0.054102614521980286 Accuracy 0.453857421875\n",
      "Iteration 17210 Training loss 0.05231478437781334 Validation loss 0.05397754907608032 Accuracy 0.455322265625\n",
      "Iteration 17220 Training loss 0.052523232996463776 Validation loss 0.05426499992609024 Accuracy 0.452880859375\n",
      "Iteration 17230 Training loss 0.05285472050309181 Validation loss 0.05420348420739174 Accuracy 0.452880859375\n",
      "Iteration 17240 Training loss 0.05264348164200783 Validation loss 0.05423503741621971 Accuracy 0.4521484375\n",
      "Iteration 17250 Training loss 0.054283224046230316 Validation loss 0.05416557565331459 Accuracy 0.453857421875\n",
      "Iteration 17260 Training loss 0.05294276773929596 Validation loss 0.054232385009527206 Accuracy 0.452392578125\n",
      "Iteration 17270 Training loss 0.0528857484459877 Validation loss 0.05411119386553764 Accuracy 0.454833984375\n",
      "Iteration 17280 Training loss 0.05601160228252411 Validation loss 0.054480575025081635 Accuracy 0.44970703125\n",
      "Iteration 17290 Training loss 0.05480986088514328 Validation loss 0.05415289103984833 Accuracy 0.45361328125\n",
      "Iteration 17300 Training loss 0.05555484816431999 Validation loss 0.054471902549266815 Accuracy 0.45068359375\n",
      "Iteration 17310 Training loss 0.053812701255083084 Validation loss 0.054629694670438766 Accuracy 0.44970703125\n",
      "Iteration 17320 Training loss 0.05171138420701027 Validation loss 0.05419269576668739 Accuracy 0.45361328125\n",
      "Iteration 17330 Training loss 0.05632546916604042 Validation loss 0.05598505586385727 Accuracy 0.435546875\n",
      "Iteration 17340 Training loss 0.05323118716478348 Validation loss 0.054040633141994476 Accuracy 0.4541015625\n",
      "Iteration 17350 Training loss 0.054630786180496216 Validation loss 0.05566428229212761 Accuracy 0.438232421875\n",
      "Iteration 17360 Training loss 0.05158020928502083 Validation loss 0.05429506674408913 Accuracy 0.45263671875\n",
      "Iteration 17370 Training loss 0.050918061286211014 Validation loss 0.054453834891319275 Accuracy 0.4501953125\n",
      "Iteration 17380 Training loss 0.05041894316673279 Validation loss 0.05411992594599724 Accuracy 0.453369140625\n",
      "Iteration 17390 Training loss 0.052692752331495285 Validation loss 0.054274145513772964 Accuracy 0.45263671875\n",
      "Iteration 17400 Training loss 0.054080940783023834 Validation loss 0.05438642576336861 Accuracy 0.45068359375\n",
      "Iteration 17410 Training loss 0.0513836108148098 Validation loss 0.05424867197871208 Accuracy 0.45263671875\n",
      "Iteration 17420 Training loss 0.05353190004825592 Validation loss 0.054052043706178665 Accuracy 0.45458984375\n",
      "Iteration 17430 Training loss 0.056893717497587204 Validation loss 0.054772958159446716 Accuracy 0.447509765625\n",
      "Iteration 17440 Training loss 0.054486989974975586 Validation loss 0.0547989197075367 Accuracy 0.447998046875\n",
      "Iteration 17450 Training loss 0.04982830584049225 Validation loss 0.05414706468582153 Accuracy 0.453857421875\n",
      "Iteration 17460 Training loss 0.053785014897584915 Validation loss 0.054198212921619415 Accuracy 0.453125\n",
      "Iteration 17470 Training loss 0.051020070910453796 Validation loss 0.054757822304964066 Accuracy 0.447021484375\n",
      "Iteration 17480 Training loss 0.05335667356848717 Validation loss 0.054321423172950745 Accuracy 0.452392578125\n",
      "Iteration 17490 Training loss 0.05103601887822151 Validation loss 0.054152123630046844 Accuracy 0.452880859375\n",
      "Iteration 17500 Training loss 0.055954694747924805 Validation loss 0.05511213839054108 Accuracy 0.444091796875\n",
      "Iteration 17510 Training loss 0.05494868755340576 Validation loss 0.054356567561626434 Accuracy 0.45166015625\n",
      "Iteration 17520 Training loss 0.053856730461120605 Validation loss 0.05425310507416725 Accuracy 0.45263671875\n",
      "Iteration 17530 Training loss 0.048900503665208817 Validation loss 0.05407579615712166 Accuracy 0.45458984375\n",
      "Iteration 17540 Training loss 0.05245586112141609 Validation loss 0.054853055626153946 Accuracy 0.4462890625\n",
      "Iteration 17550 Training loss 0.05511626973748207 Validation loss 0.05402155965566635 Accuracy 0.45458984375\n",
      "Iteration 17560 Training loss 0.054763805121183395 Validation loss 0.054188381880521774 Accuracy 0.452880859375\n",
      "Iteration 17570 Training loss 0.05246647074818611 Validation loss 0.05391886457800865 Accuracy 0.45556640625\n",
      "Iteration 17580 Training loss 0.05287758260965347 Validation loss 0.05434395372867584 Accuracy 0.45166015625\n",
      "Iteration 17590 Training loss 0.053203023970127106 Validation loss 0.054021816700696945 Accuracy 0.454833984375\n",
      "Iteration 17600 Training loss 0.054198939353227615 Validation loss 0.05430879816412926 Accuracy 0.452880859375\n",
      "Iteration 17610 Training loss 0.050792962312698364 Validation loss 0.054607342928647995 Accuracy 0.449951171875\n",
      "Iteration 17620 Training loss 0.05322541669011116 Validation loss 0.05408145487308502 Accuracy 0.45458984375\n",
      "Iteration 17630 Training loss 0.053378086537122726 Validation loss 0.054188329726457596 Accuracy 0.452880859375\n",
      "Iteration 17640 Training loss 0.053175002336502075 Validation loss 0.05403553694486618 Accuracy 0.454833984375\n",
      "Iteration 17650 Training loss 0.05307760462164879 Validation loss 0.05433147773146629 Accuracy 0.451904296875\n",
      "Iteration 17660 Training loss 0.052757907658815384 Validation loss 0.054143026471138 Accuracy 0.45361328125\n",
      "Iteration 17670 Training loss 0.0516669824719429 Validation loss 0.05413725599646568 Accuracy 0.45361328125\n",
      "Iteration 17680 Training loss 0.051893625408411026 Validation loss 0.05417463555932045 Accuracy 0.45361328125\n",
      "Iteration 17690 Training loss 0.0485236719250679 Validation loss 0.054005615413188934 Accuracy 0.454833984375\n",
      "Iteration 17700 Training loss 0.051919322460889816 Validation loss 0.054423220455646515 Accuracy 0.4501953125\n",
      "Iteration 17710 Training loss 0.04969072341918945 Validation loss 0.05395789071917534 Accuracy 0.455078125\n",
      "Iteration 17720 Training loss 0.0552825927734375 Validation loss 0.054099902510643005 Accuracy 0.4541015625\n",
      "Iteration 17730 Training loss 0.05174650996923447 Validation loss 0.054489392787218094 Accuracy 0.4501953125\n",
      "Iteration 17740 Training loss 0.05432421714067459 Validation loss 0.054503679275512695 Accuracy 0.4501953125\n",
      "Iteration 17750 Training loss 0.05385764688253403 Validation loss 0.05418244004249573 Accuracy 0.45263671875\n",
      "Iteration 17760 Training loss 0.054636310786008835 Validation loss 0.054163891822099686 Accuracy 0.452392578125\n",
      "Iteration 17770 Training loss 0.05349535495042801 Validation loss 0.054281022399663925 Accuracy 0.452392578125\n",
      "Iteration 17780 Training loss 0.05273393541574478 Validation loss 0.054081108421087265 Accuracy 0.4541015625\n",
      "Iteration 17790 Training loss 0.05066787824034691 Validation loss 0.05403680354356766 Accuracy 0.4541015625\n",
      "Iteration 17800 Training loss 0.052436914294958115 Validation loss 0.054341766983270645 Accuracy 0.45068359375\n",
      "Iteration 17810 Training loss 0.052820831537246704 Validation loss 0.05399572104215622 Accuracy 0.45458984375\n",
      "Iteration 17820 Training loss 0.05186400189995766 Validation loss 0.05399126932024956 Accuracy 0.45556640625\n",
      "Iteration 17830 Training loss 0.055320266634225845 Validation loss 0.05401935800909996 Accuracy 0.4541015625\n",
      "Iteration 17840 Training loss 0.049998585134744644 Validation loss 0.054008278995752335 Accuracy 0.453857421875\n",
      "Iteration 17850 Training loss 0.05387149378657341 Validation loss 0.05398431047797203 Accuracy 0.454345703125\n",
      "Iteration 17860 Training loss 0.05507278814911842 Validation loss 0.05419972911477089 Accuracy 0.45361328125\n",
      "Iteration 17870 Training loss 0.051171813160181046 Validation loss 0.054153427481651306 Accuracy 0.453125\n",
      "Iteration 17880 Training loss 0.05546155571937561 Validation loss 0.053974494338035583 Accuracy 0.45458984375\n",
      "Iteration 17890 Training loss 0.051990408450365067 Validation loss 0.05408790707588196 Accuracy 0.453857421875\n",
      "Iteration 17900 Training loss 0.05322042852640152 Validation loss 0.054398857057094574 Accuracy 0.45068359375\n",
      "Iteration 17910 Training loss 0.05140168219804764 Validation loss 0.05423518270254135 Accuracy 0.453125\n",
      "Iteration 17920 Training loss 0.05507621169090271 Validation loss 0.05432392284274101 Accuracy 0.4521484375\n",
      "Iteration 17930 Training loss 0.05091077461838722 Validation loss 0.05464419350028038 Accuracy 0.448486328125\n",
      "Iteration 17940 Training loss 0.0526374876499176 Validation loss 0.054217591881752014 Accuracy 0.45263671875\n",
      "Iteration 17950 Training loss 0.05409706383943558 Validation loss 0.05411549285054207 Accuracy 0.4541015625\n",
      "Iteration 17960 Training loss 0.047720134258270264 Validation loss 0.05409269407391548 Accuracy 0.45361328125\n",
      "Iteration 17970 Training loss 0.04994745925068855 Validation loss 0.054032180458307266 Accuracy 0.45458984375\n",
      "Iteration 17980 Training loss 0.056613754481077194 Validation loss 0.05452880635857582 Accuracy 0.448974609375\n",
      "Iteration 17990 Training loss 0.04994268715381622 Validation loss 0.0545915849506855 Accuracy 0.44921875\n",
      "Iteration 18000 Training loss 0.05031078681349754 Validation loss 0.053975846618413925 Accuracy 0.45458984375\n",
      "Iteration 18010 Training loss 0.05232797563076019 Validation loss 0.05402015894651413 Accuracy 0.455322265625\n",
      "Iteration 18020 Training loss 0.05722477287054062 Validation loss 0.054435618221759796 Accuracy 0.45166015625\n",
      "Iteration 18030 Training loss 0.05295795574784279 Validation loss 0.05393258482217789 Accuracy 0.45556640625\n",
      "Iteration 18040 Training loss 0.053118422627449036 Validation loss 0.05398896709084511 Accuracy 0.455322265625\n",
      "Iteration 18050 Training loss 0.05448479205369949 Validation loss 0.05402632802724838 Accuracy 0.45458984375\n",
      "Iteration 18060 Training loss 0.055144768208265305 Validation loss 0.05444728210568428 Accuracy 0.449951171875\n",
      "Iteration 18070 Training loss 0.05655565485358238 Validation loss 0.054422859102487564 Accuracy 0.450927734375\n",
      "Iteration 18080 Training loss 0.05214643478393555 Validation loss 0.053870800882577896 Accuracy 0.456787109375\n",
      "Iteration 18090 Training loss 0.05505777895450592 Validation loss 0.054279398173093796 Accuracy 0.45166015625\n",
      "Iteration 18100 Training loss 0.05380580574274063 Validation loss 0.05411843955516815 Accuracy 0.45361328125\n",
      "Iteration 18110 Training loss 0.052819203585386276 Validation loss 0.05436669662594795 Accuracy 0.4501953125\n",
      "Iteration 18120 Training loss 0.05392810329794884 Validation loss 0.05471048504114151 Accuracy 0.448486328125\n",
      "Iteration 18130 Training loss 0.05320969596505165 Validation loss 0.053855638951063156 Accuracy 0.455810546875\n",
      "Iteration 18140 Training loss 0.05053767189383507 Validation loss 0.05415333807468414 Accuracy 0.453369140625\n",
      "Iteration 18150 Training loss 0.05447393283247948 Validation loss 0.05405541881918907 Accuracy 0.45458984375\n",
      "Iteration 18160 Training loss 0.052933841943740845 Validation loss 0.0541054867208004 Accuracy 0.453369140625\n",
      "Iteration 18170 Training loss 0.05207080394029617 Validation loss 0.054336123168468475 Accuracy 0.451416015625\n",
      "Iteration 18180 Training loss 0.05273456871509552 Validation loss 0.05400644242763519 Accuracy 0.455078125\n",
      "Iteration 18190 Training loss 0.05568893253803253 Validation loss 0.054005835205316544 Accuracy 0.45458984375\n",
      "Iteration 18200 Training loss 0.05220047011971474 Validation loss 0.05394319072365761 Accuracy 0.455078125\n",
      "Iteration 18210 Training loss 0.055277738720178604 Validation loss 0.054368894547224045 Accuracy 0.45068359375\n",
      "Iteration 18220 Training loss 0.052635349333286285 Validation loss 0.053989119827747345 Accuracy 0.454345703125\n",
      "Iteration 18230 Training loss 0.05784836784005165 Validation loss 0.054722338914871216 Accuracy 0.447265625\n",
      "Iteration 18240 Training loss 0.051163651049137115 Validation loss 0.05405433848500252 Accuracy 0.45361328125\n",
      "Iteration 18250 Training loss 0.05131668969988823 Validation loss 0.054229460656642914 Accuracy 0.45263671875\n",
      "Iteration 18260 Training loss 0.05219187214970589 Validation loss 0.05409427732229233 Accuracy 0.45361328125\n",
      "Iteration 18270 Training loss 0.05178792402148247 Validation loss 0.05396028980612755 Accuracy 0.45458984375\n",
      "Iteration 18280 Training loss 0.053028836846351624 Validation loss 0.054397761821746826 Accuracy 0.450439453125\n",
      "Iteration 18290 Training loss 0.05383770540356636 Validation loss 0.054119426757097244 Accuracy 0.4541015625\n",
      "Iteration 18300 Training loss 0.051586080342531204 Validation loss 0.05427587777376175 Accuracy 0.45166015625\n",
      "Iteration 18310 Training loss 0.05422693490982056 Validation loss 0.05396149680018425 Accuracy 0.454833984375\n",
      "Iteration 18320 Training loss 0.05635847523808479 Validation loss 0.05453580990433693 Accuracy 0.448974609375\n",
      "Iteration 18330 Training loss 0.05448002740740776 Validation loss 0.053903158754110336 Accuracy 0.455322265625\n",
      "Iteration 18340 Training loss 0.05179478973150253 Validation loss 0.054313965141773224 Accuracy 0.45166015625\n",
      "Iteration 18350 Training loss 0.05003371462225914 Validation loss 0.05400511249899864 Accuracy 0.45458984375\n",
      "Iteration 18360 Training loss 0.05390311777591705 Validation loss 0.05476900190114975 Accuracy 0.44677734375\n",
      "Iteration 18370 Training loss 0.05407242849469185 Validation loss 0.054254092276096344 Accuracy 0.4521484375\n",
      "Iteration 18380 Training loss 0.0522327683866024 Validation loss 0.05522818863391876 Accuracy 0.4423828125\n",
      "Iteration 18390 Training loss 0.050007060170173645 Validation loss 0.05396807938814163 Accuracy 0.455322265625\n",
      "Iteration 18400 Training loss 0.05184071883559227 Validation loss 0.05384879931807518 Accuracy 0.45654296875\n",
      "Iteration 18410 Training loss 0.05689924955368042 Validation loss 0.05467468500137329 Accuracy 0.448974609375\n",
      "Iteration 18420 Training loss 0.05074991285800934 Validation loss 0.053908251225948334 Accuracy 0.45556640625\n",
      "Iteration 18430 Training loss 0.053437523543834686 Validation loss 0.053856704384088516 Accuracy 0.45654296875\n",
      "Iteration 18440 Training loss 0.05287497863173485 Validation loss 0.054063666611909866 Accuracy 0.45458984375\n",
      "Iteration 18450 Training loss 0.0544954314827919 Validation loss 0.053971048444509506 Accuracy 0.455810546875\n",
      "Iteration 18460 Training loss 0.054702986031770706 Validation loss 0.05398700013756752 Accuracy 0.454345703125\n",
      "Iteration 18470 Training loss 0.0536370687186718 Validation loss 0.054011013358831406 Accuracy 0.454833984375\n",
      "Iteration 18480 Training loss 0.05035986378788948 Validation loss 0.0542888268828392 Accuracy 0.452392578125\n",
      "Iteration 18490 Training loss 0.052618712186813354 Validation loss 0.054019782692193985 Accuracy 0.454833984375\n",
      "Iteration 18500 Training loss 0.055037371814250946 Validation loss 0.05410154163837433 Accuracy 0.454345703125\n",
      "Iteration 18510 Training loss 0.052302855998277664 Validation loss 0.05398647487163544 Accuracy 0.454833984375\n",
      "Iteration 18520 Training loss 0.05271591246128082 Validation loss 0.05403904989361763 Accuracy 0.4541015625\n",
      "Iteration 18530 Training loss 0.054571978747844696 Validation loss 0.05419739708304405 Accuracy 0.453125\n",
      "Iteration 18540 Training loss 0.05460278317332268 Validation loss 0.05404283106327057 Accuracy 0.454345703125\n",
      "Iteration 18550 Training loss 0.054391760379076004 Validation loss 0.054227303713560104 Accuracy 0.451904296875\n",
      "Iteration 18560 Training loss 0.052624937146902084 Validation loss 0.05430474504828453 Accuracy 0.451171875\n",
      "Iteration 18570 Training loss 0.05344942584633827 Validation loss 0.05444175750017166 Accuracy 0.45068359375\n",
      "Iteration 18580 Training loss 0.0510379783809185 Validation loss 0.05399121344089508 Accuracy 0.455078125\n",
      "Iteration 18590 Training loss 0.052058301866054535 Validation loss 0.0541682206094265 Accuracy 0.453125\n",
      "Iteration 18600 Training loss 0.050895098596811295 Validation loss 0.05392508581280708 Accuracy 0.4560546875\n",
      "Iteration 18610 Training loss 0.05248728394508362 Validation loss 0.054804861545562744 Accuracy 0.447265625\n",
      "Iteration 18620 Training loss 0.05274883285164833 Validation loss 0.053929638117551804 Accuracy 0.455810546875\n",
      "Iteration 18630 Training loss 0.05353204905986786 Validation loss 0.053873442113399506 Accuracy 0.456298828125\n",
      "Iteration 18640 Training loss 0.052139703184366226 Validation loss 0.05375117063522339 Accuracy 0.45703125\n",
      "Iteration 18650 Training loss 0.050569817423820496 Validation loss 0.05390652269124985 Accuracy 0.455810546875\n",
      "Iteration 18660 Training loss 0.05223335325717926 Validation loss 0.05401093140244484 Accuracy 0.455078125\n",
      "Iteration 18670 Training loss 0.05158410593867302 Validation loss 0.054561879485845566 Accuracy 0.448974609375\n",
      "Iteration 18680 Training loss 0.05271253362298012 Validation loss 0.05406420677900314 Accuracy 0.454345703125\n",
      "Iteration 18690 Training loss 0.052432626485824585 Validation loss 0.05386477708816528 Accuracy 0.456298828125\n",
      "Iteration 18700 Training loss 0.05190674960613251 Validation loss 0.05391504615545273 Accuracy 0.455810546875\n",
      "Iteration 18710 Training loss 0.05357503890991211 Validation loss 0.056558191776275635 Accuracy 0.4296875\n",
      "Iteration 18720 Training loss 0.05290168151259422 Validation loss 0.054255954921245575 Accuracy 0.4521484375\n",
      "Iteration 18730 Training loss 0.054077453911304474 Validation loss 0.053968075662851334 Accuracy 0.45458984375\n",
      "Iteration 18740 Training loss 0.052399639040231705 Validation loss 0.05509793385863304 Accuracy 0.44384765625\n",
      "Iteration 18750 Training loss 0.04862828552722931 Validation loss 0.05406409874558449 Accuracy 0.45458984375\n",
      "Iteration 18760 Training loss 0.053302306681871414 Validation loss 0.05453119054436684 Accuracy 0.448486328125\n",
      "Iteration 18770 Training loss 0.05415613576769829 Validation loss 0.05412418395280838 Accuracy 0.4541015625\n",
      "Iteration 18780 Training loss 0.05128106474876404 Validation loss 0.05407746508717537 Accuracy 0.454345703125\n",
      "Iteration 18790 Training loss 0.05044596642255783 Validation loss 0.05410334840416908 Accuracy 0.4541015625\n",
      "Iteration 18800 Training loss 0.0553566999733448 Validation loss 0.05397200211882591 Accuracy 0.45458984375\n",
      "Iteration 18810 Training loss 0.05293627455830574 Validation loss 0.05458961799740791 Accuracy 0.448974609375\n",
      "Iteration 18820 Training loss 0.05333210155367851 Validation loss 0.05441732332110405 Accuracy 0.45068359375\n",
      "Iteration 18830 Training loss 0.05401047691702843 Validation loss 0.05429362505674362 Accuracy 0.451904296875\n",
      "Iteration 18840 Training loss 0.05096189305186272 Validation loss 0.053907278925180435 Accuracy 0.45556640625\n",
      "Iteration 18850 Training loss 0.05260268971323967 Validation loss 0.05422188341617584 Accuracy 0.4521484375\n",
      "Iteration 18860 Training loss 0.051636263728141785 Validation loss 0.05418512970209122 Accuracy 0.4541015625\n",
      "Iteration 18870 Training loss 0.05475210025906563 Validation loss 0.05403013527393341 Accuracy 0.45458984375\n",
      "Iteration 18880 Training loss 0.0528942309319973 Validation loss 0.0539802610874176 Accuracy 0.45458984375\n",
      "Iteration 18890 Training loss 0.05036088079214096 Validation loss 0.054328449070453644 Accuracy 0.451416015625\n",
      "Iteration 18900 Training loss 0.052778441458940506 Validation loss 0.053923383355140686 Accuracy 0.455810546875\n",
      "Iteration 18910 Training loss 0.05584552884101868 Validation loss 0.05452544242143631 Accuracy 0.4501953125\n",
      "Iteration 18920 Training loss 0.0535423569381237 Validation loss 0.054108064621686935 Accuracy 0.453857421875\n",
      "Iteration 18930 Training loss 0.04866350069642067 Validation loss 0.05391447991132736 Accuracy 0.455810546875\n",
      "Iteration 18940 Training loss 0.052695564925670624 Validation loss 0.054043836891651154 Accuracy 0.455810546875\n",
      "Iteration 18950 Training loss 0.05112123116850853 Validation loss 0.05408739298582077 Accuracy 0.45361328125\n",
      "Iteration 18960 Training loss 0.0500873364508152 Validation loss 0.05400713160634041 Accuracy 0.454345703125\n",
      "Iteration 18970 Training loss 0.057267963886260986 Validation loss 0.054041966795921326 Accuracy 0.45458984375\n",
      "Iteration 18980 Training loss 0.05087397247552872 Validation loss 0.05392151698470116 Accuracy 0.454833984375\n",
      "Iteration 18990 Training loss 0.052740469574928284 Validation loss 0.05392937362194061 Accuracy 0.45458984375\n",
      "Iteration 19000 Training loss 0.04965633153915405 Validation loss 0.05392365902662277 Accuracy 0.455810546875\n",
      "Iteration 19010 Training loss 0.055133555084466934 Validation loss 0.054625511169433594 Accuracy 0.4482421875\n",
      "Iteration 19020 Training loss 0.05161639302968979 Validation loss 0.05432916805148125 Accuracy 0.451171875\n",
      "Iteration 19030 Training loss 0.050074949860572815 Validation loss 0.0539197139441967 Accuracy 0.456298828125\n",
      "Iteration 19040 Training loss 0.0569586344063282 Validation loss 0.053956981748342514 Accuracy 0.4560546875\n",
      "Iteration 19050 Training loss 0.05295465141534805 Validation loss 0.05537418648600578 Accuracy 0.4404296875\n",
      "Iteration 19060 Training loss 0.05383281037211418 Validation loss 0.054025594145059586 Accuracy 0.4541015625\n",
      "Iteration 19070 Training loss 0.04954687878489494 Validation loss 0.05392538011074066 Accuracy 0.45458984375\n",
      "Iteration 19080 Training loss 0.055693428963422775 Validation loss 0.05405980348587036 Accuracy 0.454345703125\n",
      "Iteration 19090 Training loss 0.049984972923994064 Validation loss 0.05419057980179787 Accuracy 0.453369140625\n",
      "Iteration 19100 Training loss 0.05379941686987877 Validation loss 0.05419863015413284 Accuracy 0.451904296875\n",
      "Iteration 19110 Training loss 0.05325179919600487 Validation loss 0.05498582497239113 Accuracy 0.445556640625\n",
      "Iteration 19120 Training loss 0.05026981979608536 Validation loss 0.054130468517541885 Accuracy 0.45361328125\n",
      "Iteration 19130 Training loss 0.05223449692130089 Validation loss 0.05423298850655556 Accuracy 0.453125\n",
      "Iteration 19140 Training loss 0.054242201149463654 Validation loss 0.053916823118925095 Accuracy 0.454833984375\n",
      "Iteration 19150 Training loss 0.054006773978471756 Validation loss 0.054005905985832214 Accuracy 0.45458984375\n",
      "Iteration 19160 Training loss 0.051030803471803665 Validation loss 0.054208558052778244 Accuracy 0.453125\n",
      "Iteration 19170 Training loss 0.05526374652981758 Validation loss 0.05439780279994011 Accuracy 0.451416015625\n",
      "Iteration 19180 Training loss 0.04852135106921196 Validation loss 0.05419779196381569 Accuracy 0.45263671875\n",
      "Iteration 19190 Training loss 0.05149113014340401 Validation loss 0.05403392016887665 Accuracy 0.454345703125\n",
      "Iteration 19200 Training loss 0.048626892268657684 Validation loss 0.054914433509111404 Accuracy 0.446044921875\n",
      "Iteration 19210 Training loss 0.05179233103990555 Validation loss 0.053954705595970154 Accuracy 0.455322265625\n",
      "Iteration 19220 Training loss 0.053347837179899216 Validation loss 0.05432187393307686 Accuracy 0.4521484375\n",
      "Iteration 19230 Training loss 0.05181217938661575 Validation loss 0.0540771409869194 Accuracy 0.4541015625\n",
      "Iteration 19240 Training loss 0.054044779390096664 Validation loss 0.053935009986162186 Accuracy 0.455078125\n",
      "Iteration 19250 Training loss 0.05316919833421707 Validation loss 0.053926628082990646 Accuracy 0.455078125\n",
      "Iteration 19260 Training loss 0.047718364745378494 Validation loss 0.05409221723675728 Accuracy 0.453857421875\n",
      "Iteration 19270 Training loss 0.04948083311319351 Validation loss 0.05386752635240555 Accuracy 0.456298828125\n",
      "Iteration 19280 Training loss 0.053767405450344086 Validation loss 0.0538165457546711 Accuracy 0.456298828125\n",
      "Iteration 19290 Training loss 0.05185539275407791 Validation loss 0.054243747144937515 Accuracy 0.452392578125\n",
      "Iteration 19300 Training loss 0.055506955832242966 Validation loss 0.05407177656888962 Accuracy 0.454345703125\n",
      "Iteration 19310 Training loss 0.0526055283844471 Validation loss 0.0542038269340992 Accuracy 0.452392578125\n",
      "Iteration 19320 Training loss 0.05307389423251152 Validation loss 0.05454566702246666 Accuracy 0.44873046875\n",
      "Iteration 19330 Training loss 0.05156947299838066 Validation loss 0.05414038524031639 Accuracy 0.453125\n",
      "Iteration 19340 Training loss 0.054583679884672165 Validation loss 0.05423136055469513 Accuracy 0.452392578125\n",
      "Iteration 19350 Training loss 0.05495307222008705 Validation loss 0.05440250039100647 Accuracy 0.450927734375\n",
      "Iteration 19360 Training loss 0.054917480796575546 Validation loss 0.05597403272986412 Accuracy 0.436279296875\n",
      "Iteration 19370 Training loss 0.05393550544977188 Validation loss 0.054168812930583954 Accuracy 0.453125\n",
      "Iteration 19380 Training loss 0.05020132288336754 Validation loss 0.05381748452782631 Accuracy 0.455810546875\n",
      "Iteration 19390 Training loss 0.052598267793655396 Validation loss 0.0541345588862896 Accuracy 0.45361328125\n",
      "Iteration 19400 Training loss 0.05532316118478775 Validation loss 0.05398187413811684 Accuracy 0.455078125\n",
      "Iteration 19410 Training loss 0.051599618047475815 Validation loss 0.05380362272262573 Accuracy 0.456298828125\n",
      "Iteration 19420 Training loss 0.049812596291303635 Validation loss 0.05390564724802971 Accuracy 0.45556640625\n",
      "Iteration 19430 Training loss 0.05221080407500267 Validation loss 0.054034363478422165 Accuracy 0.454345703125\n",
      "Iteration 19440 Training loss 0.05358768627047539 Validation loss 0.05401192232966423 Accuracy 0.454345703125\n",
      "Iteration 19450 Training loss 0.05309922248125076 Validation loss 0.054178427904844284 Accuracy 0.45361328125\n",
      "Iteration 19460 Training loss 0.05557712912559509 Validation loss 0.05392797291278839 Accuracy 0.455810546875\n",
      "Iteration 19470 Training loss 0.052614726126194 Validation loss 0.05400115251541138 Accuracy 0.45458984375\n",
      "Iteration 19480 Training loss 0.05358753353357315 Validation loss 0.05390014499425888 Accuracy 0.45556640625\n",
      "Iteration 19490 Training loss 0.05281447619199753 Validation loss 0.05396723374724388 Accuracy 0.45458984375\n",
      "Iteration 19500 Training loss 0.05050789937376976 Validation loss 0.0541144423186779 Accuracy 0.45361328125\n",
      "Iteration 19510 Training loss 0.05623633414506912 Validation loss 0.054682407528162 Accuracy 0.447265625\n",
      "Iteration 19520 Training loss 0.053647857159376144 Validation loss 0.05395437777042389 Accuracy 0.454833984375\n",
      "Iteration 19530 Training loss 0.05739792808890343 Validation loss 0.05523386597633362 Accuracy 0.4423828125\n",
      "Iteration 19540 Training loss 0.05263140797615051 Validation loss 0.054183535277843475 Accuracy 0.453125\n",
      "Iteration 19550 Training loss 0.05219573900103569 Validation loss 0.05411992222070694 Accuracy 0.45361328125\n",
      "Iteration 19560 Training loss 0.053677938878536224 Validation loss 0.05437109246850014 Accuracy 0.451904296875\n",
      "Iteration 19570 Training loss 0.05178482457995415 Validation loss 0.053875211626291275 Accuracy 0.45556640625\n",
      "Iteration 19580 Training loss 0.05316445976495743 Validation loss 0.05417069047689438 Accuracy 0.45361328125\n",
      "Iteration 19590 Training loss 0.054302096366882324 Validation loss 0.05475714057683945 Accuracy 0.447509765625\n",
      "Iteration 19600 Training loss 0.04932176694273949 Validation loss 0.053853947669267654 Accuracy 0.45654296875\n",
      "Iteration 19610 Training loss 0.054544225335121155 Validation loss 0.05413128435611725 Accuracy 0.453857421875\n",
      "Iteration 19620 Training loss 0.05407298728823662 Validation loss 0.05416501685976982 Accuracy 0.45361328125\n",
      "Iteration 19630 Training loss 0.05234719440340996 Validation loss 0.05589992552995682 Accuracy 0.435791015625\n",
      "Iteration 19640 Training loss 0.05348874256014824 Validation loss 0.053966645151376724 Accuracy 0.455078125\n",
      "Iteration 19650 Training loss 0.054122742265462875 Validation loss 0.05433548614382744 Accuracy 0.451904296875\n",
      "Iteration 19660 Training loss 0.05620682239532471 Validation loss 0.05388132855296135 Accuracy 0.455322265625\n",
      "Iteration 19670 Training loss 0.05414077639579773 Validation loss 0.05539609119296074 Accuracy 0.44140625\n",
      "Iteration 19680 Training loss 0.05076073482632637 Validation loss 0.05388001725077629 Accuracy 0.45654296875\n",
      "Iteration 19690 Training loss 0.05635474994778633 Validation loss 0.05420996621251106 Accuracy 0.453125\n",
      "Iteration 19700 Training loss 0.04917750135064125 Validation loss 0.053872283548116684 Accuracy 0.4560546875\n",
      "Iteration 19710 Training loss 0.05246993899345398 Validation loss 0.05388861522078514 Accuracy 0.455810546875\n",
      "Iteration 19720 Training loss 0.05207161232829094 Validation loss 0.053948383778333664 Accuracy 0.455810546875\n",
      "Iteration 19730 Training loss 0.053427353501319885 Validation loss 0.05406536906957626 Accuracy 0.454345703125\n",
      "Iteration 19740 Training loss 0.05001477152109146 Validation loss 0.05377994477748871 Accuracy 0.456298828125\n",
      "Iteration 19750 Training loss 0.056186430156230927 Validation loss 0.054359812289476395 Accuracy 0.4521484375\n",
      "Iteration 19760 Training loss 0.05154004320502281 Validation loss 0.05385219678282738 Accuracy 0.456298828125\n",
      "Iteration 19770 Training loss 0.05354512110352516 Validation loss 0.05411398410797119 Accuracy 0.45361328125\n",
      "Iteration 19780 Training loss 0.05403589829802513 Validation loss 0.05383557453751564 Accuracy 0.45654296875\n",
      "Iteration 19790 Training loss 0.05255996808409691 Validation loss 0.05449112132191658 Accuracy 0.45068359375\n",
      "Iteration 19800 Training loss 0.0537545345723629 Validation loss 0.05380795896053314 Accuracy 0.456298828125\n",
      "Iteration 19810 Training loss 0.053155019879341125 Validation loss 0.05386023223400116 Accuracy 0.4560546875\n",
      "Iteration 19820 Training loss 0.050421807914972305 Validation loss 0.05390070006251335 Accuracy 0.45556640625\n",
      "Iteration 19830 Training loss 0.052458830177783966 Validation loss 0.053891316056251526 Accuracy 0.455078125\n",
      "Iteration 19840 Training loss 0.05053984001278877 Validation loss 0.05407661572098732 Accuracy 0.454345703125\n",
      "Iteration 19850 Training loss 0.05653204768896103 Validation loss 0.054992128163576126 Accuracy 0.44482421875\n",
      "Iteration 19860 Training loss 0.05425974354147911 Validation loss 0.05402020364999771 Accuracy 0.454345703125\n",
      "Iteration 19870 Training loss 0.05444975569844246 Validation loss 0.05427535995841026 Accuracy 0.452392578125\n",
      "Iteration 19880 Training loss 0.05120309069752693 Validation loss 0.05423712730407715 Accuracy 0.45361328125\n",
      "Iteration 19890 Training loss 0.04901467263698578 Validation loss 0.05393737554550171 Accuracy 0.455078125\n",
      "Iteration 19900 Training loss 0.052186280488967896 Validation loss 0.054013535380363464 Accuracy 0.454345703125\n",
      "Iteration 19910 Training loss 0.05173639953136444 Validation loss 0.054457370191812515 Accuracy 0.448974609375\n",
      "Iteration 19920 Training loss 0.05276888981461525 Validation loss 0.053884148597717285 Accuracy 0.45556640625\n",
      "Iteration 19930 Training loss 0.04632568359375 Validation loss 0.05391915887594223 Accuracy 0.455078125\n",
      "Iteration 19940 Training loss 0.054860614240169525 Validation loss 0.05383112281560898 Accuracy 0.456787109375\n",
      "Iteration 19950 Training loss 0.05290718004107475 Validation loss 0.053902629762887955 Accuracy 0.455078125\n",
      "Iteration 19960 Training loss 0.05090833455324173 Validation loss 0.05393839627504349 Accuracy 0.45556640625\n",
      "Iteration 19970 Training loss 0.050780557096004486 Validation loss 0.053862497210502625 Accuracy 0.455810546875\n",
      "Iteration 19980 Training loss 0.05099140852689743 Validation loss 0.05423677712678909 Accuracy 0.452880859375\n",
      "Iteration 19990 Training loss 0.0478922463953495 Validation loss 0.05397966131567955 Accuracy 0.454345703125\n",
      "Iteration 20000 Training loss 0.05125533044338226 Validation loss 0.05386221408843994 Accuracy 0.45654296875\n",
      "Iteration 20010 Training loss 0.05154380202293396 Validation loss 0.05387695133686066 Accuracy 0.456298828125\n",
      "Iteration 20020 Training loss 0.049846161156892776 Validation loss 0.05384868010878563 Accuracy 0.455810546875\n",
      "Iteration 20030 Training loss 0.05564320832490921 Validation loss 0.053916364908218384 Accuracy 0.455322265625\n",
      "Iteration 20040 Training loss 0.05398869886994362 Validation loss 0.05380465090274811 Accuracy 0.456298828125\n",
      "Iteration 20050 Training loss 0.0528222993016243 Validation loss 0.05386057868599892 Accuracy 0.456787109375\n",
      "Iteration 20060 Training loss 0.051664505153894424 Validation loss 0.054031357169151306 Accuracy 0.455322265625\n",
      "Iteration 20070 Training loss 0.053250353783369064 Validation loss 0.05408095568418503 Accuracy 0.454833984375\n",
      "Iteration 20080 Training loss 0.051466118544340134 Validation loss 0.05378182604908943 Accuracy 0.457275390625\n",
      "Iteration 20090 Training loss 0.052233871072530746 Validation loss 0.05406591668725014 Accuracy 0.455078125\n",
      "Iteration 20100 Training loss 0.053076740354299545 Validation loss 0.05405023321509361 Accuracy 0.454345703125\n",
      "Iteration 20110 Training loss 0.05190540850162506 Validation loss 0.0541532002389431 Accuracy 0.45361328125\n",
      "Iteration 20120 Training loss 0.04835250973701477 Validation loss 0.054590754210948944 Accuracy 0.449462890625\n",
      "Iteration 20130 Training loss 0.05345924198627472 Validation loss 0.05400257185101509 Accuracy 0.454833984375\n",
      "Iteration 20140 Training loss 0.052722156047821045 Validation loss 0.05387077108025551 Accuracy 0.456298828125\n",
      "Iteration 20150 Training loss 0.051065776497125626 Validation loss 0.053967028856277466 Accuracy 0.455322265625\n",
      "Iteration 20160 Training loss 0.050745878368616104 Validation loss 0.05447438359260559 Accuracy 0.451171875\n",
      "Iteration 20170 Training loss 0.05164923891425133 Validation loss 0.05393299087882042 Accuracy 0.455078125\n",
      "Iteration 20180 Training loss 0.05088987201452255 Validation loss 0.053793035447597504 Accuracy 0.457275390625\n",
      "Iteration 20190 Training loss 0.04986008629202843 Validation loss 0.05414482206106186 Accuracy 0.453857421875\n",
      "Iteration 20200 Training loss 0.055191583931446075 Validation loss 0.05402493104338646 Accuracy 0.453857421875\n",
      "Iteration 20210 Training loss 0.051443058997392654 Validation loss 0.05386006832122803 Accuracy 0.455810546875\n",
      "Iteration 20220 Training loss 0.05636964365839958 Validation loss 0.05389834940433502 Accuracy 0.455078125\n",
      "Iteration 20230 Training loss 0.05040280520915985 Validation loss 0.053810134530067444 Accuracy 0.455810546875\n",
      "Iteration 20240 Training loss 0.05349995940923691 Validation loss 0.05396197736263275 Accuracy 0.456298828125\n",
      "Iteration 20250 Training loss 0.053417667746543884 Validation loss 0.05406879633665085 Accuracy 0.454833984375\n",
      "Iteration 20260 Training loss 0.05221141129732132 Validation loss 0.05437179654836655 Accuracy 0.451904296875\n",
      "Iteration 20270 Training loss 0.05664166063070297 Validation loss 0.054177284240722656 Accuracy 0.4541015625\n",
      "Iteration 20280 Training loss 0.04885883629322052 Validation loss 0.05563552677631378 Accuracy 0.438720703125\n",
      "Iteration 20290 Training loss 0.05080973729491234 Validation loss 0.05403007194399834 Accuracy 0.454345703125\n",
      "Iteration 20300 Training loss 0.05583060160279274 Validation loss 0.053953494876623154 Accuracy 0.454833984375\n",
      "Iteration 20310 Training loss 0.05612136796116829 Validation loss 0.05411004647612572 Accuracy 0.4541015625\n",
      "Iteration 20320 Training loss 0.048939239233732224 Validation loss 0.0540030375123024 Accuracy 0.455322265625\n",
      "Iteration 20330 Training loss 0.05304344743490219 Validation loss 0.05383767560124397 Accuracy 0.457275390625\n",
      "Iteration 20340 Training loss 0.050427183508872986 Validation loss 0.05394403263926506 Accuracy 0.4560546875\n",
      "Iteration 20350 Training loss 0.05148671939969063 Validation loss 0.05377396196126938 Accuracy 0.45703125\n",
      "Iteration 20360 Training loss 0.04888562858104706 Validation loss 0.05368519946932793 Accuracy 0.45751953125\n",
      "Iteration 20370 Training loss 0.05356211215257645 Validation loss 0.05394419655203819 Accuracy 0.455810546875\n",
      "Iteration 20380 Training loss 0.05744810029864311 Validation loss 0.05420096963644028 Accuracy 0.453369140625\n",
      "Iteration 20390 Training loss 0.049522724002599716 Validation loss 0.053718630224466324 Accuracy 0.45703125\n",
      "Iteration 20400 Training loss 0.05089784786105156 Validation loss 0.053992342203855515 Accuracy 0.45458984375\n",
      "Iteration 20410 Training loss 0.05439801141619682 Validation loss 0.05376507714390755 Accuracy 0.45703125\n",
      "Iteration 20420 Training loss 0.054503366351127625 Validation loss 0.05461311340332031 Accuracy 0.448486328125\n",
      "Iteration 20430 Training loss 0.0525805726647377 Validation loss 0.054077062755823135 Accuracy 0.454345703125\n",
      "Iteration 20440 Training loss 0.05164259299635887 Validation loss 0.05385920777916908 Accuracy 0.4560546875\n",
      "Iteration 20450 Training loss 0.05198017880320549 Validation loss 0.0538298636674881 Accuracy 0.456787109375\n",
      "Iteration 20460 Training loss 0.052490413188934326 Validation loss 0.053797926753759384 Accuracy 0.45703125\n",
      "Iteration 20470 Training loss 0.05284051597118378 Validation loss 0.053912751376628876 Accuracy 0.45703125\n",
      "Iteration 20480 Training loss 0.05090492591261864 Validation loss 0.05387166887521744 Accuracy 0.456787109375\n",
      "Iteration 20490 Training loss 0.053301066160202026 Validation loss 0.05375776067376137 Accuracy 0.457275390625\n",
      "Iteration 20500 Training loss 0.05062360689043999 Validation loss 0.05381390079855919 Accuracy 0.456298828125\n",
      "Iteration 20510 Training loss 0.0532802939414978 Validation loss 0.0545818917453289 Accuracy 0.448486328125\n",
      "Iteration 20520 Training loss 0.049085505306720734 Validation loss 0.05382798612117767 Accuracy 0.457275390625\n",
      "Iteration 20530 Training loss 0.05213651433587074 Validation loss 0.05383826419711113 Accuracy 0.456298828125\n",
      "Iteration 20540 Training loss 0.05302923172712326 Validation loss 0.05386827513575554 Accuracy 0.45654296875\n",
      "Iteration 20550 Training loss 0.049387115985155106 Validation loss 0.053781893104314804 Accuracy 0.45654296875\n",
      "Iteration 20560 Training loss 0.04776262491941452 Validation loss 0.05384395271539688 Accuracy 0.45654296875\n",
      "Iteration 20570 Training loss 0.05391763150691986 Validation loss 0.05385447293519974 Accuracy 0.45556640625\n",
      "Iteration 20580 Training loss 0.05362526327371597 Validation loss 0.0551055446267128 Accuracy 0.443603515625\n",
      "Iteration 20590 Training loss 0.05353117361664772 Validation loss 0.05437067151069641 Accuracy 0.45068359375\n",
      "Iteration 20600 Training loss 0.050409045070409775 Validation loss 0.053784988820552826 Accuracy 0.45703125\n",
      "Iteration 20610 Training loss 0.04913347214460373 Validation loss 0.05392741411924362 Accuracy 0.45458984375\n",
      "Iteration 20620 Training loss 0.04970923438668251 Validation loss 0.05374287813901901 Accuracy 0.457763671875\n",
      "Iteration 20630 Training loss 0.052206315100193024 Validation loss 0.05372827872633934 Accuracy 0.45751953125\n",
      "Iteration 20640 Training loss 0.05441071838140488 Validation loss 0.05431656539440155 Accuracy 0.451904296875\n",
      "Iteration 20650 Training loss 0.05428864061832428 Validation loss 0.05376945808529854 Accuracy 0.457275390625\n",
      "Iteration 20660 Training loss 0.05401960015296936 Validation loss 0.05399451032280922 Accuracy 0.454345703125\n",
      "Iteration 20670 Training loss 0.055743779987096786 Validation loss 0.0541081540286541 Accuracy 0.454345703125\n",
      "Iteration 20680 Training loss 0.05441763252019882 Validation loss 0.054621677845716476 Accuracy 0.449951171875\n",
      "Iteration 20690 Training loss 0.05432981252670288 Validation loss 0.0540141798555851 Accuracy 0.455078125\n",
      "Iteration 20700 Training loss 0.0538051538169384 Validation loss 0.053940389305353165 Accuracy 0.454833984375\n",
      "Iteration 20710 Training loss 0.05250801146030426 Validation loss 0.05387504771351814 Accuracy 0.45556640625\n",
      "Iteration 20720 Training loss 0.05271369218826294 Validation loss 0.054046373814344406 Accuracy 0.455078125\n",
      "Iteration 20730 Training loss 0.05356276035308838 Validation loss 0.05403882637619972 Accuracy 0.45361328125\n",
      "Iteration 20740 Training loss 0.04947881028056145 Validation loss 0.05416572093963623 Accuracy 0.452880859375\n",
      "Iteration 20750 Training loss 0.05222572758793831 Validation loss 0.053758587688207626 Accuracy 0.456787109375\n",
      "Iteration 20760 Training loss 0.053982287645339966 Validation loss 0.05399126932024956 Accuracy 0.455322265625\n",
      "Iteration 20770 Training loss 0.05540037900209427 Validation loss 0.05403764173388481 Accuracy 0.454833984375\n",
      "Iteration 20780 Training loss 0.05132044479250908 Validation loss 0.05400936305522919 Accuracy 0.454345703125\n",
      "Iteration 20790 Training loss 0.052921000868082047 Validation loss 0.05443193390965462 Accuracy 0.449951171875\n",
      "Iteration 20800 Training loss 0.050052981823682785 Validation loss 0.05374133959412575 Accuracy 0.457275390625\n",
      "Iteration 20810 Training loss 0.0532483272254467 Validation loss 0.05376839637756348 Accuracy 0.45751953125\n",
      "Iteration 20820 Training loss 0.0545247383415699 Validation loss 0.05381093919277191 Accuracy 0.45654296875\n",
      "Iteration 20830 Training loss 0.05619677156209946 Validation loss 0.05394342169165611 Accuracy 0.45654296875\n",
      "Iteration 20840 Training loss 0.05446866899728775 Validation loss 0.05388248339295387 Accuracy 0.45458984375\n",
      "Iteration 20850 Training loss 0.0511372834444046 Validation loss 0.05447477474808693 Accuracy 0.45068359375\n",
      "Iteration 20860 Training loss 0.05504760146141052 Validation loss 0.05380577594041824 Accuracy 0.456298828125\n",
      "Iteration 20870 Training loss 0.051359985023736954 Validation loss 0.05377615988254547 Accuracy 0.45751953125\n",
      "Iteration 20880 Training loss 0.05078591778874397 Validation loss 0.053702499717473984 Accuracy 0.457763671875\n",
      "Iteration 20890 Training loss 0.05708068609237671 Validation loss 0.05382542684674263 Accuracy 0.456298828125\n",
      "Iteration 20900 Training loss 0.052999719977378845 Validation loss 0.05422794818878174 Accuracy 0.45361328125\n",
      "Iteration 20910 Training loss 0.053317438811063766 Validation loss 0.053790781646966934 Accuracy 0.457275390625\n",
      "Iteration 20920 Training loss 0.05159399285912514 Validation loss 0.05388709902763367 Accuracy 0.4560546875\n",
      "Iteration 20930 Training loss 0.05111465975642204 Validation loss 0.05407371371984482 Accuracy 0.454345703125\n",
      "Iteration 20940 Training loss 0.052786935120821 Validation loss 0.053945399820804596 Accuracy 0.455078125\n",
      "Iteration 20950 Training loss 0.05531955510377884 Validation loss 0.05534299835562706 Accuracy 0.441650390625\n",
      "Iteration 20960 Training loss 0.054468099027872086 Validation loss 0.053849607706069946 Accuracy 0.45654296875\n",
      "Iteration 20970 Training loss 0.05240871012210846 Validation loss 0.05411496385931969 Accuracy 0.4541015625\n",
      "Iteration 20980 Training loss 0.052494749426841736 Validation loss 0.053884707391262054 Accuracy 0.456298828125\n",
      "Iteration 20990 Training loss 0.051472440361976624 Validation loss 0.05411471426486969 Accuracy 0.45361328125\n",
      "Iteration 21000 Training loss 0.050974272191524506 Validation loss 0.05395319312810898 Accuracy 0.455322265625\n",
      "Iteration 21010 Training loss 0.05037349462509155 Validation loss 0.05390077084302902 Accuracy 0.4560546875\n",
      "Iteration 21020 Training loss 0.05270075052976608 Validation loss 0.05405912920832634 Accuracy 0.453369140625\n",
      "Iteration 21030 Training loss 0.05553445219993591 Validation loss 0.05413255840539932 Accuracy 0.453857421875\n",
      "Iteration 21040 Training loss 0.0561998076736927 Validation loss 0.0540800578892231 Accuracy 0.453857421875\n",
      "Iteration 21050 Training loss 0.04961032047867775 Validation loss 0.05385700985789299 Accuracy 0.455810546875\n",
      "Iteration 21060 Training loss 0.05166279524564743 Validation loss 0.05385307967662811 Accuracy 0.456298828125\n",
      "Iteration 21070 Training loss 0.048826914280653 Validation loss 0.0537232980132103 Accuracy 0.45654296875\n",
      "Iteration 21080 Training loss 0.051535867154598236 Validation loss 0.05409204959869385 Accuracy 0.453369140625\n",
      "Iteration 21090 Training loss 0.05157715827226639 Validation loss 0.053945813328027725 Accuracy 0.455810546875\n",
      "Iteration 21100 Training loss 0.051176976412534714 Validation loss 0.05390940234065056 Accuracy 0.455322265625\n",
      "Iteration 21110 Training loss 0.05246313288807869 Validation loss 0.053925398737192154 Accuracy 0.45556640625\n",
      "Iteration 21120 Training loss 0.05298128351569176 Validation loss 0.054095398634672165 Accuracy 0.45458984375\n",
      "Iteration 21130 Training loss 0.04969005659222603 Validation loss 0.053744032979011536 Accuracy 0.456787109375\n",
      "Iteration 21140 Training loss 0.052471667528152466 Validation loss 0.0540170781314373 Accuracy 0.4541015625\n",
      "Iteration 21150 Training loss 0.0548764169216156 Validation loss 0.054049666970968246 Accuracy 0.453857421875\n",
      "Iteration 21160 Training loss 0.05346444249153137 Validation loss 0.053878847509622574 Accuracy 0.45556640625\n",
      "Iteration 21170 Training loss 0.05651414766907692 Validation loss 0.05405339598655701 Accuracy 0.453857421875\n",
      "Iteration 21180 Training loss 0.05134419724345207 Validation loss 0.05425557494163513 Accuracy 0.45166015625\n",
      "Iteration 21190 Training loss 0.05035904794931412 Validation loss 0.05384831875562668 Accuracy 0.456298828125\n",
      "Iteration 21200 Training loss 0.05031231790781021 Validation loss 0.054094575345516205 Accuracy 0.453125\n",
      "Iteration 21210 Training loss 0.05547522380948067 Validation loss 0.05371362715959549 Accuracy 0.45703125\n",
      "Iteration 21220 Training loss 0.05170723423361778 Validation loss 0.05373232811689377 Accuracy 0.457275390625\n",
      "Iteration 21230 Training loss 0.050741855055093765 Validation loss 0.05375516414642334 Accuracy 0.456787109375\n",
      "Iteration 21240 Training loss 0.05176196247339249 Validation loss 0.05383378639817238 Accuracy 0.45556640625\n",
      "Iteration 21250 Training loss 0.05352884903550148 Validation loss 0.05382728949189186 Accuracy 0.45654296875\n",
      "Iteration 21260 Training loss 0.05291574075818062 Validation loss 0.054770320653915405 Accuracy 0.447021484375\n",
      "Iteration 21270 Training loss 0.05295903608202934 Validation loss 0.05421691760420799 Accuracy 0.452392578125\n",
      "Iteration 21280 Training loss 0.0534682422876358 Validation loss 0.05387125536799431 Accuracy 0.455810546875\n",
      "Iteration 21290 Training loss 0.049616698175668716 Validation loss 0.0541081540286541 Accuracy 0.4541015625\n",
      "Iteration 21300 Training loss 0.05193227156996727 Validation loss 0.05374355986714363 Accuracy 0.456787109375\n",
      "Iteration 21310 Training loss 0.05367283150553703 Validation loss 0.05400888994336128 Accuracy 0.4541015625\n",
      "Iteration 21320 Training loss 0.053488895297050476 Validation loss 0.05425113067030907 Accuracy 0.451904296875\n",
      "Iteration 21330 Training loss 0.05558967590332031 Validation loss 0.053824666887521744 Accuracy 0.4560546875\n",
      "Iteration 21340 Training loss 0.05293663591146469 Validation loss 0.054056160151958466 Accuracy 0.455078125\n",
      "Iteration 21350 Training loss 0.05128941684961319 Validation loss 0.05383697524666786 Accuracy 0.457275390625\n",
      "Iteration 21360 Training loss 0.053336162120103836 Validation loss 0.05407492443919182 Accuracy 0.453857421875\n",
      "Iteration 21370 Training loss 0.051383066922426224 Validation loss 0.05399872735142708 Accuracy 0.45458984375\n",
      "Iteration 21380 Training loss 0.056139372289180756 Validation loss 0.05403866246342659 Accuracy 0.4541015625\n",
      "Iteration 21390 Training loss 0.05258530005812645 Validation loss 0.05400118976831436 Accuracy 0.453857421875\n",
      "Iteration 21400 Training loss 0.054147299379110336 Validation loss 0.054388947784900665 Accuracy 0.450439453125\n",
      "Iteration 21410 Training loss 0.05005877465009689 Validation loss 0.05397576838731766 Accuracy 0.455078125\n",
      "Iteration 21420 Training loss 0.051650501787662506 Validation loss 0.054806120693683624 Accuracy 0.44677734375\n",
      "Iteration 21430 Training loss 0.054398152977228165 Validation loss 0.054475829005241394 Accuracy 0.450439453125\n",
      "Iteration 21440 Training loss 0.05338595062494278 Validation loss 0.05464552715420723 Accuracy 0.447509765625\n",
      "Iteration 21450 Training loss 0.05151338502764702 Validation loss 0.05413602292537689 Accuracy 0.453125\n",
      "Iteration 21460 Training loss 0.05250069499015808 Validation loss 0.053758904337882996 Accuracy 0.456787109375\n",
      "Iteration 21470 Training loss 0.05434093251824379 Validation loss 0.05413280054926872 Accuracy 0.453125\n",
      "Iteration 21480 Training loss 0.05431806296110153 Validation loss 0.05374341830611229 Accuracy 0.45751953125\n",
      "Iteration 21490 Training loss 0.050784435123205185 Validation loss 0.05390797555446625 Accuracy 0.455322265625\n",
      "Iteration 21500 Training loss 0.049510400742292404 Validation loss 0.05388263612985611 Accuracy 0.456298828125\n",
      "Iteration 21510 Training loss 0.05270146578550339 Validation loss 0.05401134490966797 Accuracy 0.454833984375\n",
      "Iteration 21520 Training loss 0.051901865750551224 Validation loss 0.05385730788111687 Accuracy 0.45556640625\n",
      "Iteration 21530 Training loss 0.05785292387008667 Validation loss 0.05509864538908005 Accuracy 0.442626953125\n",
      "Iteration 21540 Training loss 0.05488813668489456 Validation loss 0.05395570024847984 Accuracy 0.454345703125\n",
      "Iteration 21550 Training loss 0.052652888000011444 Validation loss 0.054001159965991974 Accuracy 0.454345703125\n",
      "Iteration 21560 Training loss 0.05272174999117851 Validation loss 0.054003093391656876 Accuracy 0.454345703125\n",
      "Iteration 21570 Training loss 0.05484433099627495 Validation loss 0.05399300530552864 Accuracy 0.454345703125\n",
      "Iteration 21580 Training loss 0.05213366821408272 Validation loss 0.05387335270643234 Accuracy 0.455078125\n",
      "Iteration 21590 Training loss 0.05257239192724228 Validation loss 0.05434370040893555 Accuracy 0.451904296875\n",
      "Iteration 21600 Training loss 0.049246855080127716 Validation loss 0.05398990958929062 Accuracy 0.455078125\n",
      "Iteration 21610 Training loss 0.04836602881550789 Validation loss 0.05405224487185478 Accuracy 0.453857421875\n",
      "Iteration 21620 Training loss 0.051073506474494934 Validation loss 0.05390613153576851 Accuracy 0.455810546875\n",
      "Iteration 21630 Training loss 0.05241403728723526 Validation loss 0.054167721420526505 Accuracy 0.453369140625\n",
      "Iteration 21640 Training loss 0.05557945743203163 Validation loss 0.05403071269392967 Accuracy 0.4541015625\n",
      "Iteration 21650 Training loss 0.055410388857126236 Validation loss 0.05410025268793106 Accuracy 0.4541015625\n",
      "Iteration 21660 Training loss 0.05161099135875702 Validation loss 0.054330721497535706 Accuracy 0.45068359375\n",
      "Iteration 21670 Training loss 0.053686633706092834 Validation loss 0.05420401319861412 Accuracy 0.45361328125\n",
      "Iteration 21680 Training loss 0.05063784122467041 Validation loss 0.053801052272319794 Accuracy 0.45654296875\n",
      "Iteration 21690 Training loss 0.05424611270427704 Validation loss 0.05394540727138519 Accuracy 0.455078125\n",
      "Iteration 21700 Training loss 0.05540815740823746 Validation loss 0.05425254628062248 Accuracy 0.45166015625\n",
      "Iteration 21710 Training loss 0.05222507193684578 Validation loss 0.053985945880413055 Accuracy 0.454833984375\n",
      "Iteration 21720 Training loss 0.05286574363708496 Validation loss 0.05392665043473244 Accuracy 0.455810546875\n",
      "Iteration 21730 Training loss 0.05468953773379326 Validation loss 0.05433809384703636 Accuracy 0.451416015625\n",
      "Iteration 21740 Training loss 0.0513288788497448 Validation loss 0.05407388508319855 Accuracy 0.452880859375\n",
      "Iteration 21750 Training loss 0.05310036614537239 Validation loss 0.053759392350912094 Accuracy 0.45654296875\n",
      "Iteration 21760 Training loss 0.05268503352999687 Validation loss 0.05418828874826431 Accuracy 0.45263671875\n",
      "Iteration 21770 Training loss 0.0540967620909214 Validation loss 0.05431383103132248 Accuracy 0.451904296875\n",
      "Iteration 21780 Training loss 0.052571237087249756 Validation loss 0.05403788760304451 Accuracy 0.454345703125\n",
      "Iteration 21790 Training loss 0.05017981305718422 Validation loss 0.05388541892170906 Accuracy 0.455322265625\n",
      "Iteration 21800 Training loss 0.05006098747253418 Validation loss 0.05377877131104469 Accuracy 0.456298828125\n",
      "Iteration 21810 Training loss 0.05052964761853218 Validation loss 0.0544457733631134 Accuracy 0.449951171875\n",
      "Iteration 21820 Training loss 0.05179115757346153 Validation loss 0.05462944135069847 Accuracy 0.447509765625\n",
      "Iteration 21830 Training loss 0.052340585738420486 Validation loss 0.05456607788801193 Accuracy 0.447998046875\n",
      "Iteration 21840 Training loss 0.05060669034719467 Validation loss 0.05402863770723343 Accuracy 0.45458984375\n",
      "Iteration 21850 Training loss 0.0512099415063858 Validation loss 0.05384690687060356 Accuracy 0.455078125\n",
      "Iteration 21860 Training loss 0.04921603947877884 Validation loss 0.053881507366895676 Accuracy 0.455810546875\n",
      "Iteration 21870 Training loss 0.05234934389591217 Validation loss 0.05383847653865814 Accuracy 0.455322265625\n",
      "Iteration 21880 Training loss 0.05176807940006256 Validation loss 0.05402887612581253 Accuracy 0.45361328125\n",
      "Iteration 21890 Training loss 0.052748776972293854 Validation loss 0.05423920601606369 Accuracy 0.452392578125\n",
      "Iteration 21900 Training loss 0.052252527326345444 Validation loss 0.054752133786678314 Accuracy 0.4462890625\n",
      "Iteration 21910 Training loss 0.052202410995960236 Validation loss 0.0543823167681694 Accuracy 0.45166015625\n",
      "Iteration 21920 Training loss 0.054562948644161224 Validation loss 0.054054539650678635 Accuracy 0.45361328125\n",
      "Iteration 21930 Training loss 0.053121402859687805 Validation loss 0.05467601493000984 Accuracy 0.447021484375\n",
      "Iteration 21940 Training loss 0.0519416481256485 Validation loss 0.054064709693193436 Accuracy 0.45361328125\n",
      "Iteration 21950 Training loss 0.046411264687776566 Validation loss 0.05389027297496796 Accuracy 0.454833984375\n",
      "Iteration 21960 Training loss 0.04814738407731056 Validation loss 0.054127346724271774 Accuracy 0.454345703125\n",
      "Iteration 21970 Training loss 0.05295185372233391 Validation loss 0.05490900203585625 Accuracy 0.445556640625\n",
      "Iteration 21980 Training loss 0.054976724088191986 Validation loss 0.0541268065571785 Accuracy 0.45361328125\n",
      "Iteration 21990 Training loss 0.050742898136377335 Validation loss 0.05390821024775505 Accuracy 0.455322265625\n",
      "Iteration 22000 Training loss 0.0515090636909008 Validation loss 0.054746292531490326 Accuracy 0.446533203125\n",
      "Iteration 22010 Training loss 0.05306267365813255 Validation loss 0.054886892437934875 Accuracy 0.44482421875\n",
      "Iteration 22020 Training loss 0.05431711673736572 Validation loss 0.05396503955125809 Accuracy 0.455078125\n",
      "Iteration 22030 Training loss 0.055201441049575806 Validation loss 0.054004378616809845 Accuracy 0.454833984375\n",
      "Iteration 22040 Training loss 0.050892170518636703 Validation loss 0.05392386019229889 Accuracy 0.45556640625\n",
      "Iteration 22050 Training loss 0.050319090485572815 Validation loss 0.053828977048397064 Accuracy 0.45556640625\n",
      "Iteration 22060 Training loss 0.053799498826265335 Validation loss 0.053941287100315094 Accuracy 0.455078125\n",
      "Iteration 22070 Training loss 0.04846901074051857 Validation loss 0.0540040023624897 Accuracy 0.45556640625\n",
      "Iteration 22080 Training loss 0.05340489745140076 Validation loss 0.05382860079407692 Accuracy 0.456298828125\n",
      "Iteration 22090 Training loss 0.05181372910737991 Validation loss 0.05381906032562256 Accuracy 0.456298828125\n",
      "Iteration 22100 Training loss 0.051774729043245316 Validation loss 0.053965695202350616 Accuracy 0.455078125\n",
      "Iteration 22110 Training loss 0.05344508960843086 Validation loss 0.05385735258460045 Accuracy 0.4560546875\n",
      "Iteration 22120 Training loss 0.05225750058889389 Validation loss 0.05372412130236626 Accuracy 0.457275390625\n",
      "Iteration 22130 Training loss 0.051362477242946625 Validation loss 0.0538528636097908 Accuracy 0.4560546875\n",
      "Iteration 22140 Training loss 0.05005941540002823 Validation loss 0.05394627898931503 Accuracy 0.455078125\n",
      "Iteration 22150 Training loss 0.048985838890075684 Validation loss 0.05410197749733925 Accuracy 0.453857421875\n",
      "Iteration 22160 Training loss 0.0497298389673233 Validation loss 0.05402158945798874 Accuracy 0.45458984375\n",
      "Iteration 22170 Training loss 0.05428347364068031 Validation loss 0.054636429995298386 Accuracy 0.4482421875\n",
      "Iteration 22180 Training loss 0.05385991185903549 Validation loss 0.05385938286781311 Accuracy 0.456298828125\n",
      "Iteration 22190 Training loss 0.0541367270052433 Validation loss 0.05392228439450264 Accuracy 0.455810546875\n",
      "Iteration 22200 Training loss 0.05411553382873535 Validation loss 0.054042018949985504 Accuracy 0.454345703125\n",
      "Iteration 22210 Training loss 0.05052804574370384 Validation loss 0.0540473535656929 Accuracy 0.453857421875\n",
      "Iteration 22220 Training loss 0.0505240261554718 Validation loss 0.05383538454771042 Accuracy 0.456298828125\n",
      "Iteration 22230 Training loss 0.05545263737440109 Validation loss 0.05389391630887985 Accuracy 0.4541015625\n",
      "Iteration 22240 Training loss 0.052230045199394226 Validation loss 0.05402025207877159 Accuracy 0.453857421875\n",
      "Iteration 22250 Training loss 0.05127786099910736 Validation loss 0.0538645014166832 Accuracy 0.455810546875\n",
      "Iteration 22260 Training loss 0.05312688648700714 Validation loss 0.05387168750166893 Accuracy 0.4560546875\n",
      "Iteration 22270 Training loss 0.052817970514297485 Validation loss 0.05408503860235214 Accuracy 0.454345703125\n",
      "Iteration 22280 Training loss 0.053386881947517395 Validation loss 0.05378787964582443 Accuracy 0.45654296875\n",
      "Iteration 22290 Training loss 0.05343871936202049 Validation loss 0.054021164774894714 Accuracy 0.454345703125\n",
      "Iteration 22300 Training loss 0.05398314446210861 Validation loss 0.053984157741069794 Accuracy 0.45361328125\n",
      "Iteration 22310 Training loss 0.049738284200429916 Validation loss 0.0537903793156147 Accuracy 0.456298828125\n",
      "Iteration 22320 Training loss 0.05430518463253975 Validation loss 0.05383189395070076 Accuracy 0.45556640625\n",
      "Iteration 22330 Training loss 0.05210978165268898 Validation loss 0.053934451192617416 Accuracy 0.455078125\n",
      "Iteration 22340 Training loss 0.052032653242349625 Validation loss 0.053729310631752014 Accuracy 0.45703125\n",
      "Iteration 22350 Training loss 0.05289674922823906 Validation loss 0.0541178360581398 Accuracy 0.45263671875\n",
      "Iteration 22360 Training loss 0.05071975290775299 Validation loss 0.05401841178536415 Accuracy 0.45458984375\n",
      "Iteration 22370 Training loss 0.05017480254173279 Validation loss 0.05387407913804054 Accuracy 0.456787109375\n",
      "Iteration 22380 Training loss 0.050578415393829346 Validation loss 0.05393216758966446 Accuracy 0.455810546875\n",
      "Iteration 22390 Training loss 0.05076877772808075 Validation loss 0.054076023399829865 Accuracy 0.454345703125\n",
      "Iteration 22400 Training loss 0.05091236159205437 Validation loss 0.053842440247535706 Accuracy 0.456787109375\n",
      "Iteration 22410 Training loss 0.05470666289329529 Validation loss 0.05409907177090645 Accuracy 0.453125\n",
      "Iteration 22420 Training loss 0.05219652131199837 Validation loss 0.05381481349468231 Accuracy 0.45654296875\n",
      "Iteration 22430 Training loss 0.05406234785914421 Validation loss 0.05375036597251892 Accuracy 0.45654296875\n",
      "Iteration 22440 Training loss 0.05072246119379997 Validation loss 0.05391756817698479 Accuracy 0.455810546875\n",
      "Iteration 22450 Training loss 0.052738871425390244 Validation loss 0.05381355807185173 Accuracy 0.456787109375\n",
      "Iteration 22460 Training loss 0.051703840494155884 Validation loss 0.05406700074672699 Accuracy 0.4541015625\n",
      "Iteration 22470 Training loss 0.05059918016195297 Validation loss 0.054167453199625015 Accuracy 0.453369140625\n",
      "Iteration 22480 Training loss 0.05449262261390686 Validation loss 0.053767770528793335 Accuracy 0.45703125\n",
      "Iteration 22490 Training loss 0.05316980928182602 Validation loss 0.05426671728491783 Accuracy 0.450927734375\n",
      "Iteration 22500 Training loss 0.05216718465089798 Validation loss 0.053820740431547165 Accuracy 0.4560546875\n",
      "Iteration 22510 Training loss 0.05345652252435684 Validation loss 0.05367841199040413 Accuracy 0.458251953125\n",
      "Iteration 22520 Training loss 0.05149229243397713 Validation loss 0.05384964868426323 Accuracy 0.456298828125\n",
      "Iteration 22530 Training loss 0.0501382015645504 Validation loss 0.0541844516992569 Accuracy 0.452880859375\n",
      "Iteration 22540 Training loss 0.04933396354317665 Validation loss 0.05391041934490204 Accuracy 0.455322265625\n",
      "Iteration 22550 Training loss 0.05360232666134834 Validation loss 0.05425003916025162 Accuracy 0.45263671875\n",
      "Iteration 22560 Training loss 0.051506612449884415 Validation loss 0.053958382457494736 Accuracy 0.45556640625\n",
      "Iteration 22570 Training loss 0.05105667933821678 Validation loss 0.05442892387509346 Accuracy 0.449951171875\n",
      "Iteration 22580 Training loss 0.05262666940689087 Validation loss 0.054048825055360794 Accuracy 0.454345703125\n",
      "Iteration 22590 Training loss 0.05422121286392212 Validation loss 0.05378826707601547 Accuracy 0.45654296875\n",
      "Iteration 22600 Training loss 0.05172068253159523 Validation loss 0.05401911959052086 Accuracy 0.45458984375\n",
      "Iteration 22610 Training loss 0.053431760519742966 Validation loss 0.0540565624833107 Accuracy 0.453369140625\n",
      "Iteration 22620 Training loss 0.052751414477825165 Validation loss 0.05404253676533699 Accuracy 0.455078125\n",
      "Iteration 22630 Training loss 0.051171962171792984 Validation loss 0.05446586757898331 Accuracy 0.44970703125\n",
      "Iteration 22640 Training loss 0.0540856271982193 Validation loss 0.0545809268951416 Accuracy 0.44921875\n",
      "Iteration 22650 Training loss 0.05074247345328331 Validation loss 0.05385453999042511 Accuracy 0.45654296875\n",
      "Iteration 22660 Training loss 0.05192684754729271 Validation loss 0.055183231830596924 Accuracy 0.443115234375\n",
      "Iteration 22670 Training loss 0.053474389016628265 Validation loss 0.0541607029736042 Accuracy 0.45361328125\n",
      "Iteration 22680 Training loss 0.053462304174900055 Validation loss 0.05456887185573578 Accuracy 0.44970703125\n",
      "Iteration 22690 Training loss 0.05250107869505882 Validation loss 0.05391443148255348 Accuracy 0.45458984375\n",
      "Iteration 22700 Training loss 0.05053938180208206 Validation loss 0.05390758812427521 Accuracy 0.455810546875\n",
      "Iteration 22710 Training loss 0.05220614746212959 Validation loss 0.05389055982232094 Accuracy 0.456298828125\n",
      "Iteration 22720 Training loss 0.04854941740632057 Validation loss 0.053687434643507004 Accuracy 0.45751953125\n",
      "Iteration 22730 Training loss 0.05289589986205101 Validation loss 0.05377970263361931 Accuracy 0.456298828125\n",
      "Iteration 22740 Training loss 0.05094146728515625 Validation loss 0.05407055467367172 Accuracy 0.45458984375\n",
      "Iteration 22750 Training loss 0.05077895149588585 Validation loss 0.053836889564991 Accuracy 0.456298828125\n",
      "Iteration 22760 Training loss 0.05554066225886345 Validation loss 0.05491744354367256 Accuracy 0.445556640625\n",
      "Iteration 22770 Training loss 0.04944879561662674 Validation loss 0.05385538563132286 Accuracy 0.456298828125\n",
      "Iteration 22780 Training loss 0.05133090168237686 Validation loss 0.05390440300107002 Accuracy 0.456298828125\n",
      "Iteration 22790 Training loss 0.05122893303632736 Validation loss 0.053967565298080444 Accuracy 0.4541015625\n",
      "Iteration 22800 Training loss 0.05224113538861275 Validation loss 0.054099638015031815 Accuracy 0.453125\n",
      "Iteration 22810 Training loss 0.05260177701711655 Validation loss 0.05373091995716095 Accuracy 0.45703125\n",
      "Iteration 22820 Training loss 0.053069550544023514 Validation loss 0.05420088395476341 Accuracy 0.453125\n",
      "Iteration 22830 Training loss 0.05271679535508156 Validation loss 0.053706999868154526 Accuracy 0.457275390625\n",
      "Iteration 22840 Training loss 0.05266686528921127 Validation loss 0.05372309312224388 Accuracy 0.458251953125\n",
      "Iteration 22850 Training loss 0.04992014542222023 Validation loss 0.05375354364514351 Accuracy 0.457763671875\n",
      "Iteration 22860 Training loss 0.05504626780748367 Validation loss 0.05439160019159317 Accuracy 0.451904296875\n",
      "Iteration 22870 Training loss 0.05218362435698509 Validation loss 0.054310452193021774 Accuracy 0.451171875\n",
      "Iteration 22880 Training loss 0.05100962892174721 Validation loss 0.053951650857925415 Accuracy 0.45556640625\n",
      "Iteration 22890 Training loss 0.05410955846309662 Validation loss 0.053811587393283844 Accuracy 0.456298828125\n",
      "Iteration 22900 Training loss 0.05408293753862381 Validation loss 0.053907059133052826 Accuracy 0.455078125\n",
      "Iteration 22910 Training loss 0.05494404211640358 Validation loss 0.05410522222518921 Accuracy 0.452392578125\n",
      "Iteration 22920 Training loss 0.05579657107591629 Validation loss 0.05377452075481415 Accuracy 0.456787109375\n",
      "Iteration 22930 Training loss 0.05144638568162918 Validation loss 0.05362783372402191 Accuracy 0.458740234375\n",
      "Iteration 22940 Training loss 0.05481461435556412 Validation loss 0.054591111838817596 Accuracy 0.44970703125\n",
      "Iteration 22950 Training loss 0.05467342212796211 Validation loss 0.05371862277388573 Accuracy 0.45751953125\n",
      "Iteration 22960 Training loss 0.0559389628469944 Validation loss 0.05376696214079857 Accuracy 0.456787109375\n",
      "Iteration 22970 Training loss 0.05275307223200798 Validation loss 0.05376822128891945 Accuracy 0.45751953125\n",
      "Iteration 22980 Training loss 0.05232439562678337 Validation loss 0.053803153336048126 Accuracy 0.457763671875\n",
      "Iteration 22990 Training loss 0.046440958976745605 Validation loss 0.05390358716249466 Accuracy 0.455810546875\n",
      "Iteration 23000 Training loss 0.05344846472144127 Validation loss 0.054681356996297836 Accuracy 0.448486328125\n",
      "Iteration 23010 Training loss 0.05368726700544357 Validation loss 0.05380202457308769 Accuracy 0.456298828125\n",
      "Iteration 23020 Training loss 0.05143751949071884 Validation loss 0.053881365805864334 Accuracy 0.4560546875\n",
      "Iteration 23030 Training loss 0.05010461434721947 Validation loss 0.05377371981739998 Accuracy 0.45703125\n",
      "Iteration 23040 Training loss 0.05112210661172867 Validation loss 0.05380893871188164 Accuracy 0.45751953125\n",
      "Iteration 23050 Training loss 0.053564880043268204 Validation loss 0.05388951674103737 Accuracy 0.455810546875\n",
      "Iteration 23060 Training loss 0.05347161740064621 Validation loss 0.05392542853951454 Accuracy 0.45556640625\n",
      "Iteration 23070 Training loss 0.0510355643928051 Validation loss 0.05391811206936836 Accuracy 0.4560546875\n",
      "Iteration 23080 Training loss 0.05268797650933266 Validation loss 0.053822219371795654 Accuracy 0.45654296875\n",
      "Iteration 23090 Training loss 0.050829801708459854 Validation loss 0.05374188348650932 Accuracy 0.45654296875\n",
      "Iteration 23100 Training loss 0.05564384534955025 Validation loss 0.053957775235176086 Accuracy 0.454833984375\n",
      "Iteration 23110 Training loss 0.05165620893239975 Validation loss 0.05395447090268135 Accuracy 0.454833984375\n",
      "Iteration 23120 Training loss 0.05119484290480614 Validation loss 0.05429558828473091 Accuracy 0.45263671875\n",
      "Iteration 23130 Training loss 0.05305476859211922 Validation loss 0.054191287606954575 Accuracy 0.4521484375\n",
      "Iteration 23140 Training loss 0.05051907151937485 Validation loss 0.053682565689086914 Accuracy 0.45751953125\n",
      "Iteration 23150 Training loss 0.0517999529838562 Validation loss 0.05381651222705841 Accuracy 0.456787109375\n",
      "Iteration 23160 Training loss 0.05145161226391792 Validation loss 0.054231543093919754 Accuracy 0.451904296875\n",
      "Iteration 23170 Training loss 0.05352932587265968 Validation loss 0.05361304432153702 Accuracy 0.45849609375\n",
      "Iteration 23180 Training loss 0.05044707655906677 Validation loss 0.05386161804199219 Accuracy 0.4560546875\n",
      "Iteration 23190 Training loss 0.05424489825963974 Validation loss 0.05378914996981621 Accuracy 0.456787109375\n",
      "Iteration 23200 Training loss 0.05422293022274971 Validation loss 0.053741395473480225 Accuracy 0.45654296875\n",
      "Iteration 23210 Training loss 0.053759027272462845 Validation loss 0.053750086575746536 Accuracy 0.456787109375\n",
      "Iteration 23220 Training loss 0.05085412412881851 Validation loss 0.05389882251620293 Accuracy 0.4560546875\n",
      "Iteration 23230 Training loss 0.05458691343665123 Validation loss 0.053744617849588394 Accuracy 0.457275390625\n",
      "Iteration 23240 Training loss 0.05520239844918251 Validation loss 0.0540684275329113 Accuracy 0.453857421875\n",
      "Iteration 23250 Training loss 0.04850428178906441 Validation loss 0.05368192121386528 Accuracy 0.45703125\n",
      "Iteration 23260 Training loss 0.05295220762491226 Validation loss 0.053744636476039886 Accuracy 0.457275390625\n",
      "Iteration 23270 Training loss 0.053279221057891846 Validation loss 0.05401163548231125 Accuracy 0.453369140625\n",
      "Iteration 23280 Training loss 0.05354297161102295 Validation loss 0.053636711090803146 Accuracy 0.457763671875\n",
      "Iteration 23290 Training loss 0.051030129194259644 Validation loss 0.053916268050670624 Accuracy 0.45556640625\n",
      "Iteration 23300 Training loss 0.05202110856771469 Validation loss 0.053953543305397034 Accuracy 0.455078125\n",
      "Iteration 23310 Training loss 0.053255289793014526 Validation loss 0.0544116348028183 Accuracy 0.45068359375\n",
      "Iteration 23320 Training loss 0.05206361785531044 Validation loss 0.05395759269595146 Accuracy 0.454345703125\n",
      "Iteration 23330 Training loss 0.0521969310939312 Validation loss 0.053814101964235306 Accuracy 0.457763671875\n",
      "Iteration 23340 Training loss 0.05145420879125595 Validation loss 0.054288797080516815 Accuracy 0.453125\n",
      "Iteration 23350 Training loss 0.05352853983640671 Validation loss 0.05372035130858421 Accuracy 0.45751953125\n",
      "Iteration 23360 Training loss 0.053757842630147934 Validation loss 0.05444217100739479 Accuracy 0.450439453125\n",
      "Iteration 23370 Training loss 0.05429036542773247 Validation loss 0.053876444697380066 Accuracy 0.455810546875\n",
      "Iteration 23380 Training loss 0.05712538957595825 Validation loss 0.05405713617801666 Accuracy 0.45361328125\n",
      "Iteration 23390 Training loss 0.052923258394002914 Validation loss 0.05387169122695923 Accuracy 0.456787109375\n",
      "Iteration 23400 Training loss 0.053552113473415375 Validation loss 0.05400243028998375 Accuracy 0.45556640625\n",
      "Iteration 23410 Training loss 0.05303501337766647 Validation loss 0.05463985353708267 Accuracy 0.449462890625\n",
      "Iteration 23420 Training loss 0.05080996826291084 Validation loss 0.05382479727268219 Accuracy 0.4560546875\n",
      "Iteration 23430 Training loss 0.04930397495627403 Validation loss 0.05370346084237099 Accuracy 0.457275390625\n",
      "Iteration 23440 Training loss 0.05454535782337189 Validation loss 0.053618837147951126 Accuracy 0.457275390625\n",
      "Iteration 23450 Training loss 0.05045114457607269 Validation loss 0.0536775179207325 Accuracy 0.45751953125\n",
      "Iteration 23460 Training loss 0.052957434207201004 Validation loss 0.05445447191596031 Accuracy 0.44970703125\n",
      "Iteration 23470 Training loss 0.050874702632427216 Validation loss 0.054027702659368515 Accuracy 0.4541015625\n",
      "Iteration 23480 Training loss 0.04956010729074478 Validation loss 0.053617849946022034 Accuracy 0.457763671875\n",
      "Iteration 23490 Training loss 0.05031133070588112 Validation loss 0.0536840595304966 Accuracy 0.45703125\n",
      "Iteration 23500 Training loss 0.053033556789159775 Validation loss 0.053805507719516754 Accuracy 0.45703125\n",
      "Iteration 23510 Training loss 0.05166659504175186 Validation loss 0.0539485327899456 Accuracy 0.454833984375\n",
      "Iteration 23520 Training loss 0.04939935356378555 Validation loss 0.05376657098531723 Accuracy 0.457275390625\n",
      "Iteration 23530 Training loss 0.05333369970321655 Validation loss 0.0537881925702095 Accuracy 0.45703125\n",
      "Iteration 23540 Training loss 0.05251714959740639 Validation loss 0.05396806821227074 Accuracy 0.455810546875\n",
      "Iteration 23550 Training loss 0.050438955426216125 Validation loss 0.05380730330944061 Accuracy 0.456787109375\n",
      "Iteration 23560 Training loss 0.05117690935730934 Validation loss 0.05364641547203064 Accuracy 0.45751953125\n",
      "Iteration 23570 Training loss 0.05132810398936272 Validation loss 0.05366231128573418 Accuracy 0.457763671875\n",
      "Iteration 23580 Training loss 0.05323038622736931 Validation loss 0.053666017949581146 Accuracy 0.457763671875\n",
      "Iteration 23590 Training loss 0.05342669039964676 Validation loss 0.05405055359005928 Accuracy 0.454345703125\n",
      "Iteration 23600 Training loss 0.05280383676290512 Validation loss 0.05401163920760155 Accuracy 0.45556640625\n",
      "Iteration 23610 Training loss 0.05455474928021431 Validation loss 0.054006483405828476 Accuracy 0.45654296875\n",
      "Iteration 23620 Training loss 0.05580317601561546 Validation loss 0.05385923758149147 Accuracy 0.45556640625\n",
      "Iteration 23630 Training loss 0.05524823069572449 Validation loss 0.05365739017724991 Accuracy 0.457763671875\n",
      "Iteration 23640 Training loss 0.05158725753426552 Validation loss 0.053819384425878525 Accuracy 0.456787109375\n",
      "Iteration 23650 Training loss 0.049958210438489914 Validation loss 0.05397693067789078 Accuracy 0.455322265625\n",
      "Iteration 23660 Training loss 0.054079197347164154 Validation loss 0.054434653371572495 Accuracy 0.450927734375\n",
      "Iteration 23670 Training loss 0.05683191865682602 Validation loss 0.05456835404038429 Accuracy 0.449462890625\n",
      "Iteration 23680 Training loss 0.05226130038499832 Validation loss 0.05369579792022705 Accuracy 0.457275390625\n",
      "Iteration 23690 Training loss 0.05244904011487961 Validation loss 0.05391466245055199 Accuracy 0.455322265625\n",
      "Iteration 23700 Training loss 0.05526570603251457 Validation loss 0.05445774644613266 Accuracy 0.448974609375\n",
      "Iteration 23710 Training loss 0.053700052201747894 Validation loss 0.053812723606824875 Accuracy 0.456787109375\n",
      "Iteration 23720 Training loss 0.05514143407344818 Validation loss 0.053909022361040115 Accuracy 0.455810546875\n",
      "Iteration 23730 Training loss 0.054424747824668884 Validation loss 0.0537649504840374 Accuracy 0.456787109375\n",
      "Iteration 23740 Training loss 0.05379897356033325 Validation loss 0.05430911108851433 Accuracy 0.451904296875\n",
      "Iteration 23750 Training loss 0.05238201841711998 Validation loss 0.05390343442559242 Accuracy 0.4560546875\n",
      "Iteration 23760 Training loss 0.05192156881093979 Validation loss 0.05406653508543968 Accuracy 0.452880859375\n",
      "Iteration 23770 Training loss 0.05313080921769142 Validation loss 0.05379137396812439 Accuracy 0.456298828125\n",
      "Iteration 23780 Training loss 0.05166219547390938 Validation loss 0.05376036465167999 Accuracy 0.4580078125\n",
      "Iteration 23790 Training loss 0.051265861839056015 Validation loss 0.0538937933743 Accuracy 0.455322265625\n",
      "Iteration 23800 Training loss 0.05039601027965546 Validation loss 0.05392081290483475 Accuracy 0.456298828125\n",
      "Iteration 23810 Training loss 0.05204435810446739 Validation loss 0.05364615097641945 Accuracy 0.457763671875\n",
      "Iteration 23820 Training loss 0.050108652561903 Validation loss 0.05368083715438843 Accuracy 0.458251953125\n",
      "Iteration 23830 Training loss 0.05194526165723801 Validation loss 0.053977131843566895 Accuracy 0.454833984375\n",
      "Iteration 23840 Training loss 0.05032650753855705 Validation loss 0.05385810509324074 Accuracy 0.456298828125\n",
      "Iteration 23850 Training loss 0.05096577852964401 Validation loss 0.053767576813697815 Accuracy 0.4560546875\n",
      "Iteration 23860 Training loss 0.05084460973739624 Validation loss 0.053740244358778 Accuracy 0.4580078125\n",
      "Iteration 23870 Training loss 0.051652658730745316 Validation loss 0.053775470703840256 Accuracy 0.456298828125\n",
      "Iteration 23880 Training loss 0.05255214869976044 Validation loss 0.053803425282239914 Accuracy 0.456787109375\n",
      "Iteration 23890 Training loss 0.05211998522281647 Validation loss 0.053915124386548996 Accuracy 0.455078125\n",
      "Iteration 23900 Training loss 0.0495205894112587 Validation loss 0.053843945264816284 Accuracy 0.455810546875\n",
      "Iteration 23910 Training loss 0.052875954657793045 Validation loss 0.05402200669050217 Accuracy 0.453125\n",
      "Iteration 23920 Training loss 0.05302324518561363 Validation loss 0.05386672541499138 Accuracy 0.456787109375\n",
      "Iteration 23930 Training loss 0.05508465692400932 Validation loss 0.0541299544274807 Accuracy 0.45361328125\n",
      "Iteration 23940 Training loss 0.056366585195064545 Validation loss 0.053829826414585114 Accuracy 0.456298828125\n",
      "Iteration 23950 Training loss 0.05143490433692932 Validation loss 0.053812425583601 Accuracy 0.4560546875\n",
      "Iteration 23960 Training loss 0.053956832736730576 Validation loss 0.054279934614896774 Accuracy 0.451904296875\n",
      "Iteration 23970 Training loss 0.051839679479599 Validation loss 0.05371136590838432 Accuracy 0.457275390625\n",
      "Iteration 23980 Training loss 0.05237569287419319 Validation loss 0.053913045674562454 Accuracy 0.4541015625\n",
      "Iteration 23990 Training loss 0.05388317629694939 Validation loss 0.053717490285634995 Accuracy 0.456787109375\n",
      "Iteration 24000 Training loss 0.05109803378582001 Validation loss 0.05401729792356491 Accuracy 0.45458984375\n",
      "Iteration 24010 Training loss 0.05247708410024643 Validation loss 0.05359145253896713 Accuracy 0.458740234375\n",
      "Iteration 24020 Training loss 0.05091965198516846 Validation loss 0.05421437323093414 Accuracy 0.453125\n",
      "Iteration 24030 Training loss 0.05054802447557449 Validation loss 0.05367295444011688 Accuracy 0.457763671875\n",
      "Iteration 24040 Training loss 0.05671089515089989 Validation loss 0.0558335967361927 Accuracy 0.435791015625\n",
      "Iteration 24050 Training loss 0.05275491252541542 Validation loss 0.053707897663116455 Accuracy 0.45751953125\n",
      "Iteration 24060 Training loss 0.05131550878286362 Validation loss 0.053640663623809814 Accuracy 0.45849609375\n",
      "Iteration 24070 Training loss 0.05137307569384575 Validation loss 0.05377967655658722 Accuracy 0.45751953125\n",
      "Iteration 24080 Training loss 0.05509132146835327 Validation loss 0.05393671616911888 Accuracy 0.454345703125\n",
      "Iteration 24090 Training loss 0.04838204011321068 Validation loss 0.0535544715821743 Accuracy 0.45849609375\n",
      "Iteration 24100 Training loss 0.05279260128736496 Validation loss 0.05411640554666519 Accuracy 0.453125\n",
      "Iteration 24110 Training loss 0.05337357148528099 Validation loss 0.05376124009490013 Accuracy 0.456787109375\n",
      "Iteration 24120 Training loss 0.052030302584171295 Validation loss 0.05382123962044716 Accuracy 0.45703125\n",
      "Iteration 24130 Training loss 0.053301818668842316 Validation loss 0.05374235287308693 Accuracy 0.45703125\n",
      "Iteration 24140 Training loss 0.050800200551748276 Validation loss 0.05377133563160896 Accuracy 0.45654296875\n",
      "Iteration 24150 Training loss 0.05010759457945824 Validation loss 0.053455691784620285 Accuracy 0.459228515625\n",
      "Iteration 24160 Training loss 0.05430902913212776 Validation loss 0.05372146517038345 Accuracy 0.45703125\n",
      "Iteration 24170 Training loss 0.05416727811098099 Validation loss 0.05376356467604637 Accuracy 0.456298828125\n",
      "Iteration 24180 Training loss 0.0538729690015316 Validation loss 0.05368874967098236 Accuracy 0.457275390625\n",
      "Iteration 24190 Training loss 0.054926514625549316 Validation loss 0.053666215389966965 Accuracy 0.4580078125\n",
      "Iteration 24200 Training loss 0.05489726737141609 Validation loss 0.05412905290722847 Accuracy 0.453857421875\n",
      "Iteration 24210 Training loss 0.057220909744501114 Validation loss 0.053825993090867996 Accuracy 0.4560546875\n",
      "Iteration 24220 Training loss 0.05249430239200592 Validation loss 0.05430806428194046 Accuracy 0.4521484375\n",
      "Iteration 24230 Training loss 0.04916008934378624 Validation loss 0.053710609674453735 Accuracy 0.4580078125\n",
      "Iteration 24240 Training loss 0.04874206334352493 Validation loss 0.0537237673997879 Accuracy 0.457275390625\n",
      "Iteration 24250 Training loss 0.05073278769850731 Validation loss 0.05371037870645523 Accuracy 0.45703125\n",
      "Iteration 24260 Training loss 0.053281519562006 Validation loss 0.05379258468747139 Accuracy 0.456787109375\n",
      "Iteration 24270 Training loss 0.056235674768686295 Validation loss 0.05382632836699486 Accuracy 0.45654296875\n",
      "Iteration 24280 Training loss 0.04869280010461807 Validation loss 0.05397525429725647 Accuracy 0.455810546875\n",
      "Iteration 24290 Training loss 0.0529843233525753 Validation loss 0.0540723092854023 Accuracy 0.454345703125\n",
      "Iteration 24300 Training loss 0.049951497465372086 Validation loss 0.053775571286678314 Accuracy 0.456787109375\n",
      "Iteration 24310 Training loss 0.05105576664209366 Validation loss 0.05385386943817139 Accuracy 0.455810546875\n",
      "Iteration 24320 Training loss 0.052983738481998444 Validation loss 0.05405809357762337 Accuracy 0.454345703125\n",
      "Iteration 24330 Training loss 0.053214721381664276 Validation loss 0.05359290912747383 Accuracy 0.458740234375\n",
      "Iteration 24340 Training loss 0.05018904432654381 Validation loss 0.0537668913602829 Accuracy 0.45751953125\n",
      "Iteration 24350 Training loss 0.051496826112270355 Validation loss 0.053993258625268936 Accuracy 0.4541015625\n",
      "Iteration 24360 Training loss 0.05038243532180786 Validation loss 0.05362020432949066 Accuracy 0.458740234375\n",
      "Iteration 24370 Training loss 0.04953361302614212 Validation loss 0.05375337228178978 Accuracy 0.457275390625\n",
      "Iteration 24380 Training loss 0.05641200765967369 Validation loss 0.05511721596121788 Accuracy 0.443603515625\n",
      "Iteration 24390 Training loss 0.05252455174922943 Validation loss 0.05369231849908829 Accuracy 0.457763671875\n",
      "Iteration 24400 Training loss 0.05023946240544319 Validation loss 0.053642965853214264 Accuracy 0.4580078125\n",
      "Iteration 24410 Training loss 0.04925147444009781 Validation loss 0.0536976158618927 Accuracy 0.456787109375\n",
      "Iteration 24420 Training loss 0.052366551011800766 Validation loss 0.054095759987831116 Accuracy 0.4541015625\n",
      "Iteration 24430 Training loss 0.04936716705560684 Validation loss 0.053609296679496765 Accuracy 0.458251953125\n",
      "Iteration 24440 Training loss 0.053907427936792374 Validation loss 0.05351251736283302 Accuracy 0.45947265625\n",
      "Iteration 24450 Training loss 0.04744288697838783 Validation loss 0.053651388734579086 Accuracy 0.4580078125\n",
      "Iteration 24460 Training loss 0.051728393882513046 Validation loss 0.05405836179852486 Accuracy 0.45361328125\n",
      "Iteration 24470 Training loss 0.05022575706243515 Validation loss 0.054030537605285645 Accuracy 0.45458984375\n",
      "Iteration 24480 Training loss 0.05085447058081627 Validation loss 0.05387100577354431 Accuracy 0.455810546875\n",
      "Iteration 24490 Training loss 0.05329647660255432 Validation loss 0.05384482443332672 Accuracy 0.4560546875\n",
      "Iteration 24500 Training loss 0.049855101853609085 Validation loss 0.05403776839375496 Accuracy 0.453857421875\n",
      "Iteration 24510 Training loss 0.052260082215070724 Validation loss 0.05373696610331535 Accuracy 0.457275390625\n",
      "Iteration 24520 Training loss 0.04931607097387314 Validation loss 0.05399835482239723 Accuracy 0.45556640625\n",
      "Iteration 24530 Training loss 0.051843270659446716 Validation loss 0.054273299872875214 Accuracy 0.452880859375\n",
      "Iteration 24540 Training loss 0.04986162111163139 Validation loss 0.053765565156936646 Accuracy 0.45703125\n",
      "Iteration 24550 Training loss 0.051871489733457565 Validation loss 0.053823329508304596 Accuracy 0.457275390625\n",
      "Iteration 24560 Training loss 0.05201166868209839 Validation loss 0.053764648735523224 Accuracy 0.4580078125\n",
      "Iteration 24570 Training loss 0.05282501503825188 Validation loss 0.0535581149160862 Accuracy 0.458984375\n",
      "Iteration 24580 Training loss 0.05563299357891083 Validation loss 0.05438867583870888 Accuracy 0.449951171875\n",
      "Iteration 24590 Training loss 0.050294503569602966 Validation loss 0.05392918363213539 Accuracy 0.454833984375\n",
      "Iteration 24600 Training loss 0.0512298122048378 Validation loss 0.054101523011922836 Accuracy 0.453369140625\n",
      "Iteration 24610 Training loss 0.052092764526605606 Validation loss 0.05361924692988396 Accuracy 0.458251953125\n",
      "Iteration 24620 Training loss 0.05087345838546753 Validation loss 0.05360644310712814 Accuracy 0.458251953125\n",
      "Iteration 24630 Training loss 0.0492597259581089 Validation loss 0.053619422018527985 Accuracy 0.45849609375\n",
      "Iteration 24640 Training loss 0.055547405034303665 Validation loss 0.054954905062913895 Accuracy 0.4453125\n",
      "Iteration 24650 Training loss 0.05232037231326103 Validation loss 0.05365895479917526 Accuracy 0.45849609375\n",
      "Iteration 24660 Training loss 0.05240776389837265 Validation loss 0.054231561720371246 Accuracy 0.451904296875\n",
      "Iteration 24670 Training loss 0.052816346287727356 Validation loss 0.053604040294885635 Accuracy 0.45849609375\n",
      "Iteration 24680 Training loss 0.05009632930159569 Validation loss 0.05374152213335037 Accuracy 0.45703125\n",
      "Iteration 24690 Training loss 0.04759383946657181 Validation loss 0.05351642891764641 Accuracy 0.45947265625\n",
      "Iteration 24700 Training loss 0.05376086011528969 Validation loss 0.054025936871767044 Accuracy 0.45458984375\n",
      "Iteration 24710 Training loss 0.05450914055109024 Validation loss 0.0539696030318737 Accuracy 0.454833984375\n",
      "Iteration 24720 Training loss 0.051987141370773315 Validation loss 0.05370648205280304 Accuracy 0.457275390625\n",
      "Iteration 24730 Training loss 0.05094543471932411 Validation loss 0.05366557464003563 Accuracy 0.45751953125\n",
      "Iteration 24740 Training loss 0.054740116000175476 Validation loss 0.05379799008369446 Accuracy 0.4560546875\n",
      "Iteration 24750 Training loss 0.0477951243519783 Validation loss 0.05366987735033035 Accuracy 0.45751953125\n",
      "Iteration 24760 Training loss 0.04997120052576065 Validation loss 0.05357271805405617 Accuracy 0.45849609375\n",
      "Iteration 24770 Training loss 0.05087602511048317 Validation loss 0.053630463778972626 Accuracy 0.457763671875\n",
      "Iteration 24780 Training loss 0.05158676579594612 Validation loss 0.0536842867732048 Accuracy 0.4580078125\n",
      "Iteration 24790 Training loss 0.05308361351490021 Validation loss 0.053627047687768936 Accuracy 0.45849609375\n",
      "Iteration 24800 Training loss 0.05362631008028984 Validation loss 0.05495667830109596 Accuracy 0.443603515625\n",
      "Iteration 24810 Training loss 0.053836360573768616 Validation loss 0.05362239107489586 Accuracy 0.4580078125\n",
      "Iteration 24820 Training loss 0.05285731330513954 Validation loss 0.053678564727306366 Accuracy 0.457763671875\n",
      "Iteration 24830 Training loss 0.055021584033966064 Validation loss 0.054025281220674515 Accuracy 0.455322265625\n",
      "Iteration 24840 Training loss 0.05798604339361191 Validation loss 0.05397068336606026 Accuracy 0.454833984375\n",
      "Iteration 24850 Training loss 0.04985852912068367 Validation loss 0.053734783083200455 Accuracy 0.457275390625\n",
      "Iteration 24860 Training loss 0.05361638218164444 Validation loss 0.053886234760284424 Accuracy 0.456298828125\n",
      "Iteration 24870 Training loss 0.05499610677361488 Validation loss 0.05363079532980919 Accuracy 0.4580078125\n",
      "Iteration 24880 Training loss 0.0546296164393425 Validation loss 0.05414821207523346 Accuracy 0.453857421875\n",
      "Iteration 24890 Training loss 0.057943787425756454 Validation loss 0.05406477302312851 Accuracy 0.4541015625\n",
      "Iteration 24900 Training loss 0.053328271955251694 Validation loss 0.05372624844312668 Accuracy 0.45751953125\n",
      "Iteration 24910 Training loss 0.050860799849033356 Validation loss 0.054090339690446854 Accuracy 0.453857421875\n",
      "Iteration 24920 Training loss 0.055273499339818954 Validation loss 0.05379779264330864 Accuracy 0.456787109375\n",
      "Iteration 24930 Training loss 0.04768165946006775 Validation loss 0.05380132794380188 Accuracy 0.45556640625\n",
      "Iteration 24940 Training loss 0.05453243479132652 Validation loss 0.053930722177028656 Accuracy 0.45654296875\n",
      "Iteration 24950 Training loss 0.052166350185871124 Validation loss 0.05407353490591049 Accuracy 0.453369140625\n",
      "Iteration 24960 Training loss 0.05114053934812546 Validation loss 0.054328180849552155 Accuracy 0.450927734375\n",
      "Iteration 24970 Training loss 0.05184435099363327 Validation loss 0.053822021931409836 Accuracy 0.456787109375\n",
      "Iteration 24980 Training loss 0.05761013552546501 Validation loss 0.05386538431048393 Accuracy 0.4560546875\n",
      "Iteration 24990 Training loss 0.05094162002205849 Validation loss 0.05363527685403824 Accuracy 0.458251953125\n",
      "Iteration 25000 Training loss 0.04856686294078827 Validation loss 0.053821858018636703 Accuracy 0.456298828125\n",
      "Iteration 25010 Training loss 0.049072396010160446 Validation loss 0.05375070497393608 Accuracy 0.45751953125\n",
      "Iteration 25020 Training loss 0.053320832550525665 Validation loss 0.0536317452788353 Accuracy 0.45751953125\n",
      "Iteration 25030 Training loss 0.053462620824575424 Validation loss 0.05368217080831528 Accuracy 0.458984375\n",
      "Iteration 25040 Training loss 0.054027870297431946 Validation loss 0.05368303880095482 Accuracy 0.458251953125\n",
      "Iteration 25050 Training loss 0.05254393815994263 Validation loss 0.054245106875896454 Accuracy 0.452880859375\n",
      "Iteration 25060 Training loss 0.05527675151824951 Validation loss 0.05371762439608574 Accuracy 0.458251953125\n",
      "Iteration 25070 Training loss 0.05553264915943146 Validation loss 0.053865086287260056 Accuracy 0.456298828125\n",
      "Iteration 25080 Training loss 0.05302774906158447 Validation loss 0.05376539006829262 Accuracy 0.4560546875\n",
      "Iteration 25090 Training loss 0.04999997466802597 Validation loss 0.053592000156641006 Accuracy 0.458984375\n",
      "Iteration 25100 Training loss 0.05215185508131981 Validation loss 0.053642820566892624 Accuracy 0.4580078125\n",
      "Iteration 25110 Training loss 0.05412137880921364 Validation loss 0.054837167263031006 Accuracy 0.4453125\n",
      "Iteration 25120 Training loss 0.05132533609867096 Validation loss 0.05404118075966835 Accuracy 0.455078125\n",
      "Iteration 25130 Training loss 0.05393405631184578 Validation loss 0.05368459224700928 Accuracy 0.457763671875\n",
      "Iteration 25140 Training loss 0.052835896611213684 Validation loss 0.05368537828326225 Accuracy 0.45849609375\n",
      "Iteration 25150 Training loss 0.051776375621557236 Validation loss 0.05456700921058655 Accuracy 0.44873046875\n",
      "Iteration 25160 Training loss 0.0508049912750721 Validation loss 0.053756218403577805 Accuracy 0.457275390625\n",
      "Iteration 25170 Training loss 0.04975457116961479 Validation loss 0.05369722843170166 Accuracy 0.45751953125\n",
      "Iteration 25180 Training loss 0.050544749945402145 Validation loss 0.0537196584045887 Accuracy 0.45751953125\n",
      "Iteration 25190 Training loss 0.05212194845080376 Validation loss 0.05369340628385544 Accuracy 0.4580078125\n",
      "Iteration 25200 Training loss 0.04794669151306152 Validation loss 0.05396231263875961 Accuracy 0.45458984375\n",
      "Iteration 25210 Training loss 0.054091550409793854 Validation loss 0.054845280945301056 Accuracy 0.44580078125\n",
      "Iteration 25220 Training loss 0.05370444059371948 Validation loss 0.053801219910383224 Accuracy 0.457275390625\n",
      "Iteration 25230 Training loss 0.05075716972351074 Validation loss 0.05383883789181709 Accuracy 0.4560546875\n",
      "Iteration 25240 Training loss 0.05320126935839653 Validation loss 0.05373307317495346 Accuracy 0.4580078125\n",
      "Iteration 25250 Training loss 0.050508107990026474 Validation loss 0.05435715615749359 Accuracy 0.450927734375\n",
      "Iteration 25260 Training loss 0.0484439879655838 Validation loss 0.053741443902254105 Accuracy 0.45751953125\n",
      "Iteration 25270 Training loss 0.051448460668325424 Validation loss 0.054158929735422134 Accuracy 0.452880859375\n",
      "Iteration 25280 Training loss 0.054394811391830444 Validation loss 0.05372801050543785 Accuracy 0.45703125\n",
      "Iteration 25290 Training loss 0.051428765058517456 Validation loss 0.054311759769916534 Accuracy 0.451171875\n",
      "Iteration 25300 Training loss 0.05245592072606087 Validation loss 0.0536554679274559 Accuracy 0.45849609375\n",
      "Iteration 25310 Training loss 0.05112503841519356 Validation loss 0.053699128329753876 Accuracy 0.458740234375\n",
      "Iteration 25320 Training loss 0.05422873795032501 Validation loss 0.05466603487730026 Accuracy 0.447265625\n",
      "Iteration 25330 Training loss 0.051021117717027664 Validation loss 0.05411509796977043 Accuracy 0.45458984375\n",
      "Iteration 25340 Training loss 0.05243070423603058 Validation loss 0.053680419921875 Accuracy 0.457275390625\n",
      "Iteration 25350 Training loss 0.051488589495420456 Validation loss 0.053714875131845474 Accuracy 0.457275390625\n",
      "Iteration 25360 Training loss 0.0484013557434082 Validation loss 0.05364297702908516 Accuracy 0.457763671875\n",
      "Iteration 25370 Training loss 0.05029015988111496 Validation loss 0.05372112989425659 Accuracy 0.45703125\n",
      "Iteration 25380 Training loss 0.052223678678274155 Validation loss 0.05364326015114784 Accuracy 0.457763671875\n",
      "Iteration 25390 Training loss 0.05147542804479599 Validation loss 0.05430421605706215 Accuracy 0.452392578125\n",
      "Iteration 25400 Training loss 0.05233915522694588 Validation loss 0.053702980279922485 Accuracy 0.457763671875\n",
      "Iteration 25410 Training loss 0.05260900780558586 Validation loss 0.05365587770938873 Accuracy 0.45751953125\n",
      "Iteration 25420 Training loss 0.04923349991440773 Validation loss 0.0537169948220253 Accuracy 0.45703125\n",
      "Iteration 25430 Training loss 0.051391974091529846 Validation loss 0.053679775446653366 Accuracy 0.45751953125\n",
      "Iteration 25440 Training loss 0.04737014323472977 Validation loss 0.05367957800626755 Accuracy 0.457763671875\n",
      "Iteration 25450 Training loss 0.05196990817785263 Validation loss 0.05361054465174675 Accuracy 0.45751953125\n",
      "Iteration 25460 Training loss 0.05355013906955719 Validation loss 0.05390971526503563 Accuracy 0.456787109375\n",
      "Iteration 25470 Training loss 0.05322444066405296 Validation loss 0.05395082011818886 Accuracy 0.455322265625\n",
      "Iteration 25480 Training loss 0.05072909966111183 Validation loss 0.05370335653424263 Accuracy 0.456787109375\n",
      "Iteration 25490 Training loss 0.05144396051764488 Validation loss 0.05415349826216698 Accuracy 0.451904296875\n",
      "Iteration 25500 Training loss 0.05321773141622543 Validation loss 0.05371765047311783 Accuracy 0.457275390625\n",
      "Iteration 25510 Training loss 0.05378720909357071 Validation loss 0.05364573001861572 Accuracy 0.457763671875\n",
      "Iteration 25520 Training loss 0.05169805884361267 Validation loss 0.05473628640174866 Accuracy 0.447021484375\n",
      "Iteration 25530 Training loss 0.051669128239154816 Validation loss 0.05374938249588013 Accuracy 0.456787109375\n",
      "Iteration 25540 Training loss 0.052958499640226364 Validation loss 0.05369243398308754 Accuracy 0.45751953125\n",
      "Iteration 25550 Training loss 0.05249851569533348 Validation loss 0.05365387722849846 Accuracy 0.457275390625\n",
      "Iteration 25560 Training loss 0.05011812224984169 Validation loss 0.05368826165795326 Accuracy 0.45751953125\n",
      "Iteration 25570 Training loss 0.05471928417682648 Validation loss 0.05405775085091591 Accuracy 0.453369140625\n",
      "Iteration 25580 Training loss 0.05533301830291748 Validation loss 0.053778670728206635 Accuracy 0.4560546875\n",
      "Iteration 25590 Training loss 0.05308655649423599 Validation loss 0.053794652223587036 Accuracy 0.4560546875\n",
      "Iteration 25600 Training loss 0.048845693469047546 Validation loss 0.05373544618487358 Accuracy 0.45703125\n",
      "Iteration 25610 Training loss 0.04991982877254486 Validation loss 0.053777433931827545 Accuracy 0.455078125\n",
      "Iteration 25620 Training loss 0.051443807780742645 Validation loss 0.05373840779066086 Accuracy 0.456787109375\n",
      "Iteration 25630 Training loss 0.04957886040210724 Validation loss 0.05399000272154808 Accuracy 0.4541015625\n",
      "Iteration 25640 Training loss 0.056535106152296066 Validation loss 0.054144930094480515 Accuracy 0.45361328125\n",
      "Iteration 25650 Training loss 0.05295522138476372 Validation loss 0.05396752804517746 Accuracy 0.45556640625\n",
      "Iteration 25660 Training loss 0.0541607141494751 Validation loss 0.05422176793217659 Accuracy 0.452880859375\n",
      "Iteration 25670 Training loss 0.05205991491675377 Validation loss 0.053923219442367554 Accuracy 0.45458984375\n",
      "Iteration 25680 Training loss 0.051096636801958084 Validation loss 0.05393538624048233 Accuracy 0.45458984375\n",
      "Iteration 25690 Training loss 0.05209077522158623 Validation loss 0.05384625494480133 Accuracy 0.455322265625\n",
      "Iteration 25700 Training loss 0.05040537565946579 Validation loss 0.054001662880182266 Accuracy 0.45361328125\n",
      "Iteration 25710 Training loss 0.05292786657810211 Validation loss 0.05388636142015457 Accuracy 0.454345703125\n",
      "Iteration 25720 Training loss 0.050743468105793 Validation loss 0.05387964844703674 Accuracy 0.455810546875\n",
      "Iteration 25730 Training loss 0.05813136324286461 Validation loss 0.054291386157274246 Accuracy 0.4521484375\n",
      "Iteration 25740 Training loss 0.05444437637925148 Validation loss 0.053807612508535385 Accuracy 0.456298828125\n",
      "Iteration 25750 Training loss 0.05213034152984619 Validation loss 0.05390793830156326 Accuracy 0.455810546875\n",
      "Iteration 25760 Training loss 0.052187103778123856 Validation loss 0.05408352240920067 Accuracy 0.4541015625\n",
      "Iteration 25770 Training loss 0.04912940040230751 Validation loss 0.05457568168640137 Accuracy 0.447998046875\n",
      "Iteration 25780 Training loss 0.051936037838459015 Validation loss 0.05358106642961502 Accuracy 0.4580078125\n",
      "Iteration 25790 Training loss 0.05200384557247162 Validation loss 0.05402843654155731 Accuracy 0.45361328125\n",
      "Iteration 25800 Training loss 0.05187655985355377 Validation loss 0.05379030480980873 Accuracy 0.45654296875\n",
      "Iteration 25810 Training loss 0.05049401894211769 Validation loss 0.05402647331357002 Accuracy 0.454345703125\n",
      "Iteration 25820 Training loss 0.053029123693704605 Validation loss 0.053793441504240036 Accuracy 0.4560546875\n",
      "Iteration 25830 Training loss 0.0531732514500618 Validation loss 0.055230267345905304 Accuracy 0.442626953125\n",
      "Iteration 25840 Training loss 0.05483996495604515 Validation loss 0.05376964062452316 Accuracy 0.45654296875\n",
      "Iteration 25850 Training loss 0.05119457095861435 Validation loss 0.05426410958170891 Accuracy 0.451416015625\n",
      "Iteration 25860 Training loss 0.049249738454818726 Validation loss 0.05368290841579437 Accuracy 0.458251953125\n",
      "Iteration 25870 Training loss 0.05145798623561859 Validation loss 0.05361123010516167 Accuracy 0.4580078125\n",
      "Iteration 25880 Training loss 0.05049610510468483 Validation loss 0.05368459224700928 Accuracy 0.45751953125\n",
      "Iteration 25890 Training loss 0.055483121424913406 Validation loss 0.05366566404700279 Accuracy 0.4580078125\n",
      "Iteration 25900 Training loss 0.050543975085020065 Validation loss 0.054801374673843384 Accuracy 0.446044921875\n",
      "Iteration 25910 Training loss 0.051848527044057846 Validation loss 0.053775887936353683 Accuracy 0.45751953125\n",
      "Iteration 25920 Training loss 0.05125207453966141 Validation loss 0.05391691252589226 Accuracy 0.455078125\n",
      "Iteration 25930 Training loss 0.05326913297176361 Validation loss 0.054079070687294006 Accuracy 0.45361328125\n",
      "Iteration 25940 Training loss 0.046902481466531754 Validation loss 0.05389269441366196 Accuracy 0.45556640625\n",
      "Iteration 25950 Training loss 0.05340554937720299 Validation loss 0.05371870845556259 Accuracy 0.45703125\n",
      "Iteration 25960 Training loss 0.05221595615148544 Validation loss 0.053586386144161224 Accuracy 0.45849609375\n",
      "Iteration 25970 Training loss 0.05252672731876373 Validation loss 0.053868796676397324 Accuracy 0.454833984375\n",
      "Iteration 25980 Training loss 0.04875555261969566 Validation loss 0.05415385961532593 Accuracy 0.452880859375\n",
      "Iteration 25990 Training loss 0.05071750283241272 Validation loss 0.05378260463476181 Accuracy 0.456298828125\n",
      "Iteration 26000 Training loss 0.048189759254455566 Validation loss 0.053664304316043854 Accuracy 0.457275390625\n",
      "Iteration 26010 Training loss 0.04596559703350067 Validation loss 0.05364236235618591 Accuracy 0.45751953125\n",
      "Iteration 26020 Training loss 0.05056782811880112 Validation loss 0.05380004644393921 Accuracy 0.45556640625\n",
      "Iteration 26030 Training loss 0.05202397704124451 Validation loss 0.05427584424614906 Accuracy 0.451904296875\n",
      "Iteration 26040 Training loss 0.052365392446517944 Validation loss 0.053846053779125214 Accuracy 0.45556640625\n",
      "Iteration 26050 Training loss 0.053003713488578796 Validation loss 0.054594989866018295 Accuracy 0.447998046875\n",
      "Iteration 26060 Training loss 0.05226387828588486 Validation loss 0.05409745126962662 Accuracy 0.45263671875\n",
      "Iteration 26070 Training loss 0.05440327897667885 Validation loss 0.054033588618040085 Accuracy 0.454345703125\n",
      "Iteration 26080 Training loss 0.04925551265478134 Validation loss 0.05362309515476227 Accuracy 0.45849609375\n",
      "Iteration 26090 Training loss 0.04947695508599281 Validation loss 0.054042473435401917 Accuracy 0.455322265625\n",
      "Iteration 26100 Training loss 0.054176341742277145 Validation loss 0.053729645907878876 Accuracy 0.45751953125\n",
      "Iteration 26110 Training loss 0.05306289717555046 Validation loss 0.05391249805688858 Accuracy 0.45458984375\n",
      "Iteration 26120 Training loss 0.05278570577502251 Validation loss 0.05362401157617569 Accuracy 0.458251953125\n",
      "Iteration 26130 Training loss 0.05189528316259384 Validation loss 0.053658921271562576 Accuracy 0.456298828125\n",
      "Iteration 26140 Training loss 0.05355183780193329 Validation loss 0.053741998970508575 Accuracy 0.45654296875\n",
      "Iteration 26150 Training loss 0.054270192980766296 Validation loss 0.05356957018375397 Accuracy 0.4580078125\n",
      "Iteration 26160 Training loss 0.05434320122003555 Validation loss 0.0536830760538578 Accuracy 0.456298828125\n",
      "Iteration 26170 Training loss 0.05295911058783531 Validation loss 0.054008591920137405 Accuracy 0.453857421875\n",
      "Iteration 26180 Training loss 0.0497036837041378 Validation loss 0.053680505603551865 Accuracy 0.457763671875\n",
      "Iteration 26190 Training loss 0.054260462522506714 Validation loss 0.05379503592848778 Accuracy 0.455810546875\n",
      "Iteration 26200 Training loss 0.05346722900867462 Validation loss 0.05378423631191254 Accuracy 0.4560546875\n",
      "Iteration 26210 Training loss 0.05358275771141052 Validation loss 0.05366465821862221 Accuracy 0.457763671875\n",
      "Iteration 26220 Training loss 0.05261838808655739 Validation loss 0.0537383146584034 Accuracy 0.45751953125\n",
      "Iteration 26230 Training loss 0.053543820977211 Validation loss 0.05364382639527321 Accuracy 0.457275390625\n",
      "Iteration 26240 Training loss 0.05397837609052658 Validation loss 0.054335277527570724 Accuracy 0.450439453125\n",
      "Iteration 26250 Training loss 0.05097451061010361 Validation loss 0.05367071554064751 Accuracy 0.45703125\n",
      "Iteration 26260 Training loss 0.05197771266102791 Validation loss 0.05366373807191849 Accuracy 0.457275390625\n",
      "Iteration 26270 Training loss 0.049880869686603546 Validation loss 0.0541137233376503 Accuracy 0.454345703125\n",
      "Iteration 26280 Training loss 0.051371920853853226 Validation loss 0.05395624414086342 Accuracy 0.455078125\n",
      "Iteration 26290 Training loss 0.05045279860496521 Validation loss 0.053684525191783905 Accuracy 0.45751953125\n",
      "Iteration 26300 Training loss 0.053737375885248184 Validation loss 0.053721677511930466 Accuracy 0.456787109375\n",
      "Iteration 26310 Training loss 0.05270487815141678 Validation loss 0.053963739424943924 Accuracy 0.454833984375\n",
      "Iteration 26320 Training loss 0.05499598756432533 Validation loss 0.053733982145786285 Accuracy 0.456298828125\n",
      "Iteration 26330 Training loss 0.050711993128061295 Validation loss 0.05379832908511162 Accuracy 0.45751953125\n",
      "Iteration 26340 Training loss 0.0534689836204052 Validation loss 0.05375104397535324 Accuracy 0.45703125\n",
      "Iteration 26350 Training loss 0.05074426159262657 Validation loss 0.05422298610210419 Accuracy 0.453125\n",
      "Iteration 26360 Training loss 0.047324929386377335 Validation loss 0.05385759472846985 Accuracy 0.456298828125\n",
      "Iteration 26370 Training loss 0.05301492288708687 Validation loss 0.05378848314285278 Accuracy 0.456298828125\n",
      "Iteration 26380 Training loss 0.052476510405540466 Validation loss 0.053828269243240356 Accuracy 0.45654296875\n",
      "Iteration 26390 Training loss 0.05408221483230591 Validation loss 0.05389181524515152 Accuracy 0.45458984375\n",
      "Iteration 26400 Training loss 0.05093410611152649 Validation loss 0.053797099739313126 Accuracy 0.456298828125\n",
      "Iteration 26410 Training loss 0.05359243229031563 Validation loss 0.05390652269124985 Accuracy 0.456298828125\n",
      "Iteration 26420 Training loss 0.04946349933743477 Validation loss 0.05366211757063866 Accuracy 0.457275390625\n",
      "Iteration 26430 Training loss 0.0565512590110302 Validation loss 0.05381819233298302 Accuracy 0.456787109375\n",
      "Iteration 26440 Training loss 0.05416136607527733 Validation loss 0.05375850573182106 Accuracy 0.45703125\n",
      "Iteration 26450 Training loss 0.048818036913871765 Validation loss 0.05376739054918289 Accuracy 0.45703125\n",
      "Iteration 26460 Training loss 0.052736010402441025 Validation loss 0.05369444191455841 Accuracy 0.456787109375\n",
      "Iteration 26470 Training loss 0.05050010234117508 Validation loss 0.05358375981450081 Accuracy 0.45849609375\n",
      "Iteration 26480 Training loss 0.050500448793172836 Validation loss 0.05369774252176285 Accuracy 0.457275390625\n",
      "Iteration 26490 Training loss 0.05375734344124794 Validation loss 0.05382612347602844 Accuracy 0.456298828125\n",
      "Iteration 26500 Training loss 0.04828094318509102 Validation loss 0.05422612279653549 Accuracy 0.4521484375\n",
      "Iteration 26510 Training loss 0.049561161547899246 Validation loss 0.05363420397043228 Accuracy 0.45751953125\n",
      "Iteration 26520 Training loss 0.05319908633828163 Validation loss 0.05350557342171669 Accuracy 0.458251953125\n",
      "Iteration 26530 Training loss 0.052098631858825684 Validation loss 0.05377312749624252 Accuracy 0.456298828125\n",
      "Iteration 26540 Training loss 0.05173826962709427 Validation loss 0.05369218438863754 Accuracy 0.4580078125\n",
      "Iteration 26550 Training loss 0.04929793253540993 Validation loss 0.05351760610938072 Accuracy 0.458984375\n",
      "Iteration 26560 Training loss 0.05358324199914932 Validation loss 0.053962331265211105 Accuracy 0.454833984375\n",
      "Iteration 26570 Training loss 0.053587716072797775 Validation loss 0.05416066199541092 Accuracy 0.45263671875\n",
      "Iteration 26580 Training loss 0.0488072894513607 Validation loss 0.05360375717282295 Accuracy 0.45751953125\n",
      "Iteration 26590 Training loss 0.0535949282348156 Validation loss 0.054117653518915176 Accuracy 0.45361328125\n",
      "Iteration 26600 Training loss 0.0505959652364254 Validation loss 0.0542246550321579 Accuracy 0.451904296875\n",
      "Iteration 26610 Training loss 0.05158558115363121 Validation loss 0.0537136010825634 Accuracy 0.457275390625\n",
      "Iteration 26620 Training loss 0.052128441631793976 Validation loss 0.05367293208837509 Accuracy 0.457275390625\n",
      "Iteration 26630 Training loss 0.051030371338129044 Validation loss 0.05373818799853325 Accuracy 0.456298828125\n",
      "Iteration 26640 Training loss 0.05168474093079567 Validation loss 0.05365002155303955 Accuracy 0.4580078125\n",
      "Iteration 26650 Training loss 0.05178827419877052 Validation loss 0.05394316092133522 Accuracy 0.455322265625\n",
      "Iteration 26660 Training loss 0.051440466195344925 Validation loss 0.0535089448094368 Accuracy 0.458984375\n",
      "Iteration 26670 Training loss 0.05051537603139877 Validation loss 0.054110556840896606 Accuracy 0.451171875\n",
      "Iteration 26680 Training loss 0.05217193439602852 Validation loss 0.05363386496901512 Accuracy 0.45654296875\n",
      "Iteration 26690 Training loss 0.05283021554350853 Validation loss 0.053791288286447525 Accuracy 0.456298828125\n",
      "Iteration 26700 Training loss 0.052987176924943924 Validation loss 0.0537581592798233 Accuracy 0.45556640625\n",
      "Iteration 26710 Training loss 0.051827896386384964 Validation loss 0.05361489951610565 Accuracy 0.457763671875\n",
      "Iteration 26720 Training loss 0.055775437504053116 Validation loss 0.0537760928273201 Accuracy 0.4560546875\n",
      "Iteration 26730 Training loss 0.051860008388757706 Validation loss 0.05356753617525101 Accuracy 0.4580078125\n",
      "Iteration 26740 Training loss 0.052532095462083817 Validation loss 0.05364092066884041 Accuracy 0.45751953125\n",
      "Iteration 26750 Training loss 0.051860228180885315 Validation loss 0.05357997119426727 Accuracy 0.45751953125\n",
      "Iteration 26760 Training loss 0.053592242300510406 Validation loss 0.055047109723091125 Accuracy 0.444580078125\n",
      "Iteration 26770 Training loss 0.05423007532954216 Validation loss 0.054947465658187866 Accuracy 0.44580078125\n",
      "Iteration 26780 Training loss 0.05456035956740379 Validation loss 0.05400498956441879 Accuracy 0.454833984375\n",
      "Iteration 26790 Training loss 0.053920578211545944 Validation loss 0.053688857704401016 Accuracy 0.4580078125\n",
      "Iteration 26800 Training loss 0.05346441641449928 Validation loss 0.05357274413108826 Accuracy 0.45849609375\n",
      "Iteration 26810 Training loss 0.05405391380190849 Validation loss 0.05362503603100777 Accuracy 0.457763671875\n",
      "Iteration 26820 Training loss 0.05300499498844147 Validation loss 0.05366148427128792 Accuracy 0.45751953125\n",
      "Iteration 26830 Training loss 0.05117417499423027 Validation loss 0.05363927781581879 Accuracy 0.458740234375\n",
      "Iteration 26840 Training loss 0.05224999412894249 Validation loss 0.053754955530166626 Accuracy 0.456787109375\n",
      "Iteration 26850 Training loss 0.05066091939806938 Validation loss 0.05486486107110977 Accuracy 0.444580078125\n",
      "Iteration 26860 Training loss 0.0513346791267395 Validation loss 0.053658973425626755 Accuracy 0.458251953125\n",
      "Iteration 26870 Training loss 0.05276411026716232 Validation loss 0.05389959737658501 Accuracy 0.455810546875\n",
      "Iteration 26880 Training loss 0.05410585552453995 Validation loss 0.053788214921951294 Accuracy 0.4560546875\n",
      "Iteration 26890 Training loss 0.05024588853120804 Validation loss 0.05365812033414841 Accuracy 0.45751953125\n",
      "Iteration 26900 Training loss 0.052554838359355927 Validation loss 0.05370975285768509 Accuracy 0.457763671875\n",
      "Iteration 26910 Training loss 0.05344757065176964 Validation loss 0.054086823016405106 Accuracy 0.453125\n",
      "Iteration 26920 Training loss 0.050461772829294205 Validation loss 0.05386941134929657 Accuracy 0.45556640625\n",
      "Iteration 26930 Training loss 0.05141705274581909 Validation loss 0.05372884124517441 Accuracy 0.4580078125\n",
      "Iteration 26940 Training loss 0.053628407418727875 Validation loss 0.053746648132801056 Accuracy 0.458251953125\n",
      "Iteration 26950 Training loss 0.05467291548848152 Validation loss 0.05410940945148468 Accuracy 0.451904296875\n",
      "Iteration 26960 Training loss 0.05523296073079109 Validation loss 0.053797293454408646 Accuracy 0.456298828125\n",
      "Iteration 26970 Training loss 0.05024418607354164 Validation loss 0.05365683138370514 Accuracy 0.45703125\n",
      "Iteration 26980 Training loss 0.04993657395243645 Validation loss 0.05363893508911133 Accuracy 0.4580078125\n",
      "Iteration 26990 Training loss 0.0548049658536911 Validation loss 0.05439130589365959 Accuracy 0.4501953125\n",
      "Iteration 27000 Training loss 0.05332206189632416 Validation loss 0.05392669141292572 Accuracy 0.455810546875\n",
      "Iteration 27010 Training loss 0.04772805795073509 Validation loss 0.0537615604698658 Accuracy 0.455810546875\n",
      "Iteration 27020 Training loss 0.049537450075149536 Validation loss 0.053642429411411285 Accuracy 0.45751953125\n",
      "Iteration 27030 Training loss 0.04876408725976944 Validation loss 0.05369257181882858 Accuracy 0.457275390625\n",
      "Iteration 27040 Training loss 0.053671158850193024 Validation loss 0.053665682673454285 Accuracy 0.4580078125\n",
      "Iteration 27050 Training loss 0.05530291795730591 Validation loss 0.054154038429260254 Accuracy 0.452392578125\n",
      "Iteration 27060 Training loss 0.05279375612735748 Validation loss 0.05372915044426918 Accuracy 0.45751953125\n",
      "Iteration 27070 Training loss 0.05360151082277298 Validation loss 0.05362857133150101 Accuracy 0.458984375\n",
      "Iteration 27080 Training loss 0.05227530375123024 Validation loss 0.054015520960092545 Accuracy 0.4541015625\n",
      "Iteration 27090 Training loss 0.05418812856078148 Validation loss 0.05379083752632141 Accuracy 0.456787109375\n",
      "Iteration 27100 Training loss 0.051770832389593124 Validation loss 0.05356355011463165 Accuracy 0.45751953125\n",
      "Iteration 27110 Training loss 0.05502406880259514 Validation loss 0.05385386571288109 Accuracy 0.45556640625\n",
      "Iteration 27120 Training loss 0.052342548966407776 Validation loss 0.05369744822382927 Accuracy 0.45654296875\n",
      "Iteration 27130 Training loss 0.05330827459692955 Validation loss 0.05375094711780548 Accuracy 0.45654296875\n",
      "Iteration 27140 Training loss 0.050846632570028305 Validation loss 0.05356179177761078 Accuracy 0.4580078125\n",
      "Iteration 27150 Training loss 0.05311385914683342 Validation loss 0.05365287512540817 Accuracy 0.45751953125\n",
      "Iteration 27160 Training loss 0.0506763719022274 Validation loss 0.053695034235715866 Accuracy 0.45751953125\n",
      "Iteration 27170 Training loss 0.051463596522808075 Validation loss 0.05361786484718323 Accuracy 0.45703125\n",
      "Iteration 27180 Training loss 0.0516461618244648 Validation loss 0.053806282579898834 Accuracy 0.456787109375\n",
      "Iteration 27190 Training loss 0.0539003387093544 Validation loss 0.054046425968408585 Accuracy 0.45556640625\n",
      "Iteration 27200 Training loss 0.0540001317858696 Validation loss 0.05409982427954674 Accuracy 0.453857421875\n",
      "Iteration 27210 Training loss 0.05135810375213623 Validation loss 0.0537383034825325 Accuracy 0.456298828125\n",
      "Iteration 27220 Training loss 0.056080933660268784 Validation loss 0.053828444331884384 Accuracy 0.455810546875\n",
      "Iteration 27230 Training loss 0.05658354237675667 Validation loss 0.0536908283829689 Accuracy 0.45751953125\n",
      "Iteration 27240 Training loss 0.049010343849658966 Validation loss 0.05409052222967148 Accuracy 0.4541015625\n",
      "Iteration 27250 Training loss 0.05442069470882416 Validation loss 0.05369532108306885 Accuracy 0.4580078125\n",
      "Iteration 27260 Training loss 0.054497577250003815 Validation loss 0.053870346397161484 Accuracy 0.455078125\n",
      "Iteration 27270 Training loss 0.049099523574113846 Validation loss 0.05379839241504669 Accuracy 0.45751953125\n",
      "Iteration 27280 Training loss 0.048981133848428726 Validation loss 0.05367341265082359 Accuracy 0.4580078125\n",
      "Iteration 27290 Training loss 0.05290457606315613 Validation loss 0.054353997111320496 Accuracy 0.45068359375\n",
      "Iteration 27300 Training loss 0.05289538577198982 Validation loss 0.053913284093141556 Accuracy 0.455078125\n",
      "Iteration 27310 Training loss 0.05578472465276718 Validation loss 0.053676292300224304 Accuracy 0.45849609375\n",
      "Iteration 27320 Training loss 0.05203141272068024 Validation loss 0.05367238074541092 Accuracy 0.45751953125\n",
      "Iteration 27330 Training loss 0.05760716646909714 Validation loss 0.053740497678518295 Accuracy 0.45703125\n",
      "Iteration 27340 Training loss 0.051288146525621414 Validation loss 0.05380462855100632 Accuracy 0.4560546875\n",
      "Iteration 27350 Training loss 0.05225943401455879 Validation loss 0.05366608127951622 Accuracy 0.45703125\n",
      "Iteration 27360 Training loss 0.051170628517866135 Validation loss 0.053584784269332886 Accuracy 0.45849609375\n",
      "Iteration 27370 Training loss 0.05338359624147415 Validation loss 0.05388244614005089 Accuracy 0.455810546875\n",
      "Iteration 27380 Training loss 0.050766926258802414 Validation loss 0.05361808463931084 Accuracy 0.458740234375\n",
      "Iteration 27390 Training loss 0.053094904869794846 Validation loss 0.05402889847755432 Accuracy 0.453857421875\n",
      "Iteration 27400 Training loss 0.05337103083729744 Validation loss 0.05417527258396149 Accuracy 0.452880859375\n",
      "Iteration 27410 Training loss 0.05083194002509117 Validation loss 0.05426010489463806 Accuracy 0.4501953125\n",
      "Iteration 27420 Training loss 0.05300770327448845 Validation loss 0.05399579927325249 Accuracy 0.453857421875\n",
      "Iteration 27430 Training loss 0.05175686627626419 Validation loss 0.05365011468529701 Accuracy 0.4580078125\n",
      "Iteration 27440 Training loss 0.05117194727063179 Validation loss 0.05379991605877876 Accuracy 0.4560546875\n",
      "Iteration 27450 Training loss 0.052712585777044296 Validation loss 0.053580641746520996 Accuracy 0.457763671875\n",
      "Iteration 27460 Training loss 0.052634097635746 Validation loss 0.05420714244246483 Accuracy 0.453369140625\n",
      "Iteration 27470 Training loss 0.05338486656546593 Validation loss 0.05361476540565491 Accuracy 0.45751953125\n",
      "Iteration 27480 Training loss 0.0532829575240612 Validation loss 0.054625608026981354 Accuracy 0.4482421875\n",
      "Iteration 27490 Training loss 0.05077657848596573 Validation loss 0.05367397889494896 Accuracy 0.45751953125\n",
      "Iteration 27500 Training loss 0.04752931743860245 Validation loss 0.05390051752328873 Accuracy 0.4560546875\n",
      "Iteration 27510 Training loss 0.05538180097937584 Validation loss 0.05369322746992111 Accuracy 0.45751953125\n",
      "Iteration 27520 Training loss 0.05512408912181854 Validation loss 0.05361946299672127 Accuracy 0.45849609375\n",
      "Iteration 27530 Training loss 0.050943780690431595 Validation loss 0.05359611287713051 Accuracy 0.457763671875\n",
      "Iteration 27540 Training loss 0.05222432315349579 Validation loss 0.05382290855050087 Accuracy 0.455322265625\n",
      "Iteration 27550 Training loss 0.05431860685348511 Validation loss 0.0539889819920063 Accuracy 0.454345703125\n",
      "Iteration 27560 Training loss 0.046370502561330795 Validation loss 0.05353999882936478 Accuracy 0.45849609375\n",
      "Iteration 27570 Training loss 0.04999062046408653 Validation loss 0.05350121110677719 Accuracy 0.458740234375\n",
      "Iteration 27580 Training loss 0.05035398155450821 Validation loss 0.053669992834329605 Accuracy 0.456787109375\n",
      "Iteration 27590 Training loss 0.05144209787249565 Validation loss 0.05362568795681 Accuracy 0.457763671875\n",
      "Iteration 27600 Training loss 0.054543834179639816 Validation loss 0.05361030995845795 Accuracy 0.458251953125\n",
      "Iteration 27610 Training loss 0.0522192120552063 Validation loss 0.053480468690395355 Accuracy 0.4580078125\n",
      "Iteration 27620 Training loss 0.05045917257666588 Validation loss 0.05377128720283508 Accuracy 0.455810546875\n",
      "Iteration 27630 Training loss 0.053068723529577255 Validation loss 0.05381833761930466 Accuracy 0.45458984375\n",
      "Iteration 27640 Training loss 0.051326312124729156 Validation loss 0.05363394320011139 Accuracy 0.45751953125\n",
      "Iteration 27650 Training loss 0.05296774208545685 Validation loss 0.05371782183647156 Accuracy 0.45751953125\n",
      "Iteration 27660 Training loss 0.0529625341296196 Validation loss 0.053524501621723175 Accuracy 0.45849609375\n",
      "Iteration 27670 Training loss 0.05513787642121315 Validation loss 0.05437162518501282 Accuracy 0.450927734375\n",
      "Iteration 27680 Training loss 0.05226484686136246 Validation loss 0.05367293953895569 Accuracy 0.45751953125\n",
      "Iteration 27690 Training loss 0.053052011877298355 Validation loss 0.05386682599782944 Accuracy 0.4560546875\n",
      "Iteration 27700 Training loss 0.04965513199567795 Validation loss 0.053954217582941055 Accuracy 0.45458984375\n",
      "Iteration 27710 Training loss 0.053029876202344894 Validation loss 0.054117489606142044 Accuracy 0.452880859375\n",
      "Iteration 27720 Training loss 0.046968087553977966 Validation loss 0.05357733368873596 Accuracy 0.457763671875\n",
      "Iteration 27730 Training loss 0.053156498819589615 Validation loss 0.05361298844218254 Accuracy 0.4580078125\n",
      "Iteration 27740 Training loss 0.05322081595659256 Validation loss 0.05363740026950836 Accuracy 0.457275390625\n",
      "Iteration 27750 Training loss 0.050751831382513046 Validation loss 0.053684819489717484 Accuracy 0.45751953125\n",
      "Iteration 27760 Training loss 0.04856899008154869 Validation loss 0.05386849492788315 Accuracy 0.455078125\n",
      "Iteration 27770 Training loss 0.053148966282606125 Validation loss 0.054249051958322525 Accuracy 0.451416015625\n",
      "Iteration 27780 Training loss 0.05376523733139038 Validation loss 0.05426904186606407 Accuracy 0.451171875\n",
      "Iteration 27790 Training loss 0.05144261568784714 Validation loss 0.053594790399074554 Accuracy 0.458740234375\n",
      "Iteration 27800 Training loss 0.05122694373130798 Validation loss 0.0535527765750885 Accuracy 0.458251953125\n",
      "Iteration 27810 Training loss 0.05321546271443367 Validation loss 0.053923867642879486 Accuracy 0.455078125\n",
      "Iteration 27820 Training loss 0.05342212691903114 Validation loss 0.053727272897958755 Accuracy 0.45654296875\n",
      "Iteration 27830 Training loss 0.053708262741565704 Validation loss 0.0534435398876667 Accuracy 0.4599609375\n",
      "Iteration 27840 Training loss 0.05244913697242737 Validation loss 0.05346568301320076 Accuracy 0.459228515625\n",
      "Iteration 27850 Training loss 0.05080358311533928 Validation loss 0.053491849452257156 Accuracy 0.458984375\n",
      "Iteration 27860 Training loss 0.05383125692605972 Validation loss 0.05392710492014885 Accuracy 0.455078125\n",
      "Iteration 27870 Training loss 0.052875980734825134 Validation loss 0.053643420338630676 Accuracy 0.457763671875\n",
      "Iteration 27880 Training loss 0.05548950284719467 Validation loss 0.05389709025621414 Accuracy 0.456298828125\n",
      "Iteration 27890 Training loss 0.05364709347486496 Validation loss 0.0534636452794075 Accuracy 0.458984375\n",
      "Iteration 27900 Training loss 0.052376799285411835 Validation loss 0.05362352356314659 Accuracy 0.458251953125\n",
      "Iteration 27910 Training loss 0.050807978957891464 Validation loss 0.05376647412776947 Accuracy 0.456787109375\n",
      "Iteration 27920 Training loss 0.05245942249894142 Validation loss 0.05361113324761391 Accuracy 0.45751953125\n",
      "Iteration 27930 Training loss 0.05181584507226944 Validation loss 0.05371377617120743 Accuracy 0.456298828125\n",
      "Iteration 27940 Training loss 0.052619583904743195 Validation loss 0.053725503385066986 Accuracy 0.457275390625\n",
      "Iteration 27950 Training loss 0.05268371105194092 Validation loss 0.05385325103998184 Accuracy 0.456787109375\n",
      "Iteration 27960 Training loss 0.0484808124601841 Validation loss 0.05405956879258156 Accuracy 0.4541015625\n",
      "Iteration 27970 Training loss 0.05238176882266998 Validation loss 0.05371154472231865 Accuracy 0.45703125\n",
      "Iteration 27980 Training loss 0.04925420507788658 Validation loss 0.053680043667554855 Accuracy 0.45751953125\n",
      "Iteration 27990 Training loss 0.05345720425248146 Validation loss 0.05375252664089203 Accuracy 0.45703125\n",
      "Iteration 28000 Training loss 0.053105540573596954 Validation loss 0.05415076017379761 Accuracy 0.45263671875\n",
      "Iteration 28010 Training loss 0.05123047158122063 Validation loss 0.053788911551237106 Accuracy 0.456298828125\n",
      "Iteration 28020 Training loss 0.05214882269501686 Validation loss 0.054036013782024384 Accuracy 0.454345703125\n",
      "Iteration 28030 Training loss 0.05131557211279869 Validation loss 0.05392860621213913 Accuracy 0.45556640625\n",
      "Iteration 28040 Training loss 0.05571644380688667 Validation loss 0.05355117470026016 Accuracy 0.45849609375\n",
      "Iteration 28050 Training loss 0.05784260481595993 Validation loss 0.05393926054239273 Accuracy 0.455078125\n",
      "Iteration 28060 Training loss 0.04954686015844345 Validation loss 0.05358419939875603 Accuracy 0.458740234375\n",
      "Iteration 28070 Training loss 0.05252515524625778 Validation loss 0.053873732686042786 Accuracy 0.455322265625\n",
      "Iteration 28080 Training loss 0.05051357299089432 Validation loss 0.05354049801826477 Accuracy 0.45849609375\n",
      "Iteration 28090 Training loss 0.05340990051627159 Validation loss 0.05341389402747154 Accuracy 0.4599609375\n",
      "Iteration 28100 Training loss 0.05123294144868851 Validation loss 0.05358651652932167 Accuracy 0.45849609375\n",
      "Iteration 28110 Training loss 0.05080313980579376 Validation loss 0.05356844142079353 Accuracy 0.458740234375\n",
      "Iteration 28120 Training loss 0.05114814266562462 Validation loss 0.05363095551729202 Accuracy 0.456787109375\n",
      "Iteration 28130 Training loss 0.05562673509120941 Validation loss 0.05353112146258354 Accuracy 0.458984375\n",
      "Iteration 28140 Training loss 0.052909109741449356 Validation loss 0.053703535348176956 Accuracy 0.45751953125\n",
      "Iteration 28150 Training loss 0.0526682548224926 Validation loss 0.05391780659556389 Accuracy 0.4541015625\n",
      "Iteration 28160 Training loss 0.05364909768104553 Validation loss 0.05414624884724617 Accuracy 0.45263671875\n",
      "Iteration 28170 Training loss 0.050695739686489105 Validation loss 0.053773533552885056 Accuracy 0.45654296875\n",
      "Iteration 28180 Training loss 0.05589599534869194 Validation loss 0.05345350131392479 Accuracy 0.4599609375\n",
      "Iteration 28190 Training loss 0.05402190610766411 Validation loss 0.05371321737766266 Accuracy 0.457763671875\n",
      "Iteration 28200 Training loss 0.055232707411050797 Validation loss 0.05368988215923309 Accuracy 0.457275390625\n",
      "Iteration 28210 Training loss 0.04983917251229286 Validation loss 0.05374378710985184 Accuracy 0.4580078125\n",
      "Iteration 28220 Training loss 0.05063893646001816 Validation loss 0.05355702340602875 Accuracy 0.45849609375\n",
      "Iteration 28230 Training loss 0.050816141068935394 Validation loss 0.05372936278581619 Accuracy 0.457275390625\n",
      "Iteration 28240 Training loss 0.053869668394327164 Validation loss 0.05377684533596039 Accuracy 0.456298828125\n",
      "Iteration 28250 Training loss 0.05229552090167999 Validation loss 0.05368485301733017 Accuracy 0.45703125\n",
      "Iteration 28260 Training loss 0.04791939631104469 Validation loss 0.05358707532286644 Accuracy 0.4580078125\n",
      "Iteration 28270 Training loss 0.0513106994330883 Validation loss 0.053473152220249176 Accuracy 0.45947265625\n",
      "Iteration 28280 Training loss 0.055294279009103775 Validation loss 0.05359986051917076 Accuracy 0.4580078125\n",
      "Iteration 28290 Training loss 0.051434293389320374 Validation loss 0.05347747728228569 Accuracy 0.4599609375\n",
      "Iteration 28300 Training loss 0.051849059760570526 Validation loss 0.05418218672275543 Accuracy 0.45166015625\n",
      "Iteration 28310 Training loss 0.05228447541594505 Validation loss 0.05361310392618179 Accuracy 0.45751953125\n",
      "Iteration 28320 Training loss 0.05009031295776367 Validation loss 0.053549300879240036 Accuracy 0.458251953125\n",
      "Iteration 28330 Training loss 0.05279956012964249 Validation loss 0.05423981323838234 Accuracy 0.45263671875\n",
      "Iteration 28340 Training loss 0.05642581731081009 Validation loss 0.053999871015548706 Accuracy 0.45458984375\n",
      "Iteration 28350 Training loss 0.054939962923526764 Validation loss 0.05370742455124855 Accuracy 0.45751953125\n",
      "Iteration 28360 Training loss 0.05345340073108673 Validation loss 0.05361053720116615 Accuracy 0.457763671875\n",
      "Iteration 28370 Training loss 0.05585688352584839 Validation loss 0.05357559025287628 Accuracy 0.45849609375\n",
      "Iteration 28380 Training loss 0.05178748443722725 Validation loss 0.05378344655036926 Accuracy 0.455810546875\n",
      "Iteration 28390 Training loss 0.052789345383644104 Validation loss 0.05399676784873009 Accuracy 0.454345703125\n",
      "Iteration 28400 Training loss 0.05245492607355118 Validation loss 0.05371847376227379 Accuracy 0.457275390625\n",
      "Iteration 28410 Training loss 0.05388692021369934 Validation loss 0.05403498560190201 Accuracy 0.455078125\n",
      "Iteration 28420 Training loss 0.05114226043224335 Validation loss 0.0534856952726841 Accuracy 0.459228515625\n",
      "Iteration 28430 Training loss 0.0531686507165432 Validation loss 0.05350398272275925 Accuracy 0.458740234375\n",
      "Iteration 28440 Training loss 0.05605153739452362 Validation loss 0.054365530610084534 Accuracy 0.449951171875\n",
      "Iteration 28450 Training loss 0.05212635174393654 Validation loss 0.05347246304154396 Accuracy 0.45849609375\n",
      "Iteration 28460 Training loss 0.05209232494235039 Validation loss 0.05364791303873062 Accuracy 0.45654296875\n",
      "Iteration 28470 Training loss 0.05243385583162308 Validation loss 0.0542677640914917 Accuracy 0.451416015625\n",
      "Iteration 28480 Training loss 0.05313334986567497 Validation loss 0.053742665797472 Accuracy 0.45703125\n",
      "Iteration 28490 Training loss 0.05327190086245537 Validation loss 0.05350986495614052 Accuracy 0.458251953125\n",
      "Iteration 28500 Training loss 0.053849782794713974 Validation loss 0.05424190312623978 Accuracy 0.452392578125\n",
      "Iteration 28510 Training loss 0.052503347396850586 Validation loss 0.0537128709256649 Accuracy 0.45751953125\n",
      "Iteration 28520 Training loss 0.05262398719787598 Validation loss 0.0536213181912899 Accuracy 0.45849609375\n",
      "Iteration 28530 Training loss 0.055291131138801575 Validation loss 0.05380716174840927 Accuracy 0.4560546875\n",
      "Iteration 28540 Training loss 0.05333774909377098 Validation loss 0.05376683548092842 Accuracy 0.457275390625\n",
      "Iteration 28550 Training loss 0.0517612099647522 Validation loss 0.05357804894447327 Accuracy 0.458984375\n",
      "Iteration 28560 Training loss 0.05231829360127449 Validation loss 0.053963154554367065 Accuracy 0.4541015625\n",
      "Iteration 28570 Training loss 0.05139404162764549 Validation loss 0.053645920008420944 Accuracy 0.45849609375\n",
      "Iteration 28580 Training loss 0.05293382704257965 Validation loss 0.05430452898144722 Accuracy 0.451904296875\n",
      "Iteration 28590 Training loss 0.05107922852039337 Validation loss 0.05364839732646942 Accuracy 0.458251953125\n",
      "Iteration 28600 Training loss 0.05401716008782387 Validation loss 0.05375158041715622 Accuracy 0.45703125\n",
      "Iteration 28610 Training loss 0.05096171051263809 Validation loss 0.053696054965257645 Accuracy 0.45751953125\n",
      "Iteration 28620 Training loss 0.053236380219459534 Validation loss 0.05391373857855797 Accuracy 0.454345703125\n",
      "Iteration 28630 Training loss 0.049811653792858124 Validation loss 0.053640395402908325 Accuracy 0.4580078125\n",
      "Iteration 28640 Training loss 0.05267825350165367 Validation loss 0.05369466543197632 Accuracy 0.45703125\n",
      "Iteration 28650 Training loss 0.05121472105383873 Validation loss 0.053716741502285004 Accuracy 0.457275390625\n",
      "Iteration 28660 Training loss 0.05193037912249565 Validation loss 0.053968943655490875 Accuracy 0.4541015625\n",
      "Iteration 28670 Training loss 0.05018509551882744 Validation loss 0.05349438264966011 Accuracy 0.458984375\n",
      "Iteration 28680 Training loss 0.05632999539375305 Validation loss 0.05363074690103531 Accuracy 0.45751953125\n",
      "Iteration 28690 Training loss 0.04975343868136406 Validation loss 0.05393774434924126 Accuracy 0.45361328125\n",
      "Iteration 28700 Training loss 0.04953845217823982 Validation loss 0.05367254093289375 Accuracy 0.45703125\n",
      "Iteration 28710 Training loss 0.05381841957569122 Validation loss 0.053670451045036316 Accuracy 0.45703125\n",
      "Iteration 28720 Training loss 0.051216937601566315 Validation loss 0.053618837147951126 Accuracy 0.457763671875\n",
      "Iteration 28730 Training loss 0.05245925486087799 Validation loss 0.05416466295719147 Accuracy 0.45166015625\n",
      "Iteration 28740 Training loss 0.0537450946867466 Validation loss 0.05384046584367752 Accuracy 0.454833984375\n",
      "Iteration 28750 Training loss 0.04810541123151779 Validation loss 0.053533606231212616 Accuracy 0.458251953125\n",
      "Iteration 28760 Training loss 0.0497184582054615 Validation loss 0.054017119109630585 Accuracy 0.45361328125\n",
      "Iteration 28770 Training loss 0.05168073996901512 Validation loss 0.05427870154380798 Accuracy 0.453125\n",
      "Iteration 28780 Training loss 0.05377080291509628 Validation loss 0.053662631660699844 Accuracy 0.45751953125\n",
      "Iteration 28790 Training loss 0.0520068034529686 Validation loss 0.05376468971371651 Accuracy 0.45703125\n",
      "Iteration 28800 Training loss 0.048713263124227524 Validation loss 0.05370062217116356 Accuracy 0.45703125\n",
      "Iteration 28810 Training loss 0.05084015801548958 Validation loss 0.0545257106423378 Accuracy 0.44921875\n",
      "Iteration 28820 Training loss 0.049622200429439545 Validation loss 0.05362659692764282 Accuracy 0.458251953125\n",
      "Iteration 28830 Training loss 0.055170100182294846 Validation loss 0.05388141795992851 Accuracy 0.455810546875\n",
      "Iteration 28840 Training loss 0.05007938668131828 Validation loss 0.05423564836382866 Accuracy 0.451904296875\n",
      "Iteration 28850 Training loss 0.05670950189232826 Validation loss 0.05383450537919998 Accuracy 0.45654296875\n",
      "Iteration 28860 Training loss 0.0538841187953949 Validation loss 0.05377791449427605 Accuracy 0.45703125\n",
      "Iteration 28870 Training loss 0.05272432416677475 Validation loss 0.05356839671730995 Accuracy 0.458984375\n",
      "Iteration 28880 Training loss 0.05049716681241989 Validation loss 0.054015547037124634 Accuracy 0.455078125\n",
      "Iteration 28890 Training loss 0.05112336203455925 Validation loss 0.05375179648399353 Accuracy 0.457275390625\n",
      "Iteration 28900 Training loss 0.05196017399430275 Validation loss 0.053658030927181244 Accuracy 0.45751953125\n",
      "Iteration 28910 Training loss 0.050781555473804474 Validation loss 0.05364420264959335 Accuracy 0.458984375\n",
      "Iteration 28920 Training loss 0.050074122846126556 Validation loss 0.053727343678474426 Accuracy 0.456787109375\n",
      "Iteration 28930 Training loss 0.049784209579229355 Validation loss 0.053812555968761444 Accuracy 0.456298828125\n",
      "Iteration 28940 Training loss 0.052615564316511154 Validation loss 0.05455358326435089 Accuracy 0.449462890625\n",
      "Iteration 28950 Training loss 0.05367344245314598 Validation loss 0.053703274577856064 Accuracy 0.45703125\n",
      "Iteration 28960 Training loss 0.05561736971139908 Validation loss 0.05365193635225296 Accuracy 0.45751953125\n",
      "Iteration 28970 Training loss 0.055441610515117645 Validation loss 0.05493449047207832 Accuracy 0.443359375\n",
      "Iteration 28980 Training loss 0.04979785159230232 Validation loss 0.054010964930057526 Accuracy 0.453125\n",
      "Iteration 28990 Training loss 0.056219927966594696 Validation loss 0.05360502749681473 Accuracy 0.45751953125\n",
      "Iteration 29000 Training loss 0.059081993997097015 Validation loss 0.05403023213148117 Accuracy 0.453125\n",
      "Iteration 29010 Training loss 0.04892639070749283 Validation loss 0.05373379588127136 Accuracy 0.456787109375\n",
      "Iteration 29020 Training loss 0.049715228378772736 Validation loss 0.053778026252985 Accuracy 0.455322265625\n",
      "Iteration 29030 Training loss 0.05044654384255409 Validation loss 0.05351843684911728 Accuracy 0.45849609375\n",
      "Iteration 29040 Training loss 0.053078748285770416 Validation loss 0.05358907952904701 Accuracy 0.458251953125\n",
      "Iteration 29050 Training loss 0.05484199896454811 Validation loss 0.053727298974990845 Accuracy 0.4560546875\n",
      "Iteration 29060 Training loss 0.05094427615404129 Validation loss 0.053683988749980927 Accuracy 0.45751953125\n",
      "Iteration 29070 Training loss 0.052888207137584686 Validation loss 0.053549617528915405 Accuracy 0.458740234375\n",
      "Iteration 29080 Training loss 0.05074506625533104 Validation loss 0.05365497246384621 Accuracy 0.45751953125\n",
      "Iteration 29090 Training loss 0.05220848694443703 Validation loss 0.0536959171295166 Accuracy 0.457275390625\n",
      "Iteration 29100 Training loss 0.05076122656464577 Validation loss 0.05376410111784935 Accuracy 0.45751953125\n",
      "Iteration 29110 Training loss 0.05059093236923218 Validation loss 0.053693562746047974 Accuracy 0.45751953125\n",
      "Iteration 29120 Training loss 0.05048830062150955 Validation loss 0.05357911065220833 Accuracy 0.45947265625\n",
      "Iteration 29130 Training loss 0.05010586977005005 Validation loss 0.053605951368808746 Accuracy 0.45849609375\n",
      "Iteration 29140 Training loss 0.054410479962825775 Validation loss 0.05354909971356392 Accuracy 0.45849609375\n",
      "Iteration 29150 Training loss 0.04955260828137398 Validation loss 0.054029710590839386 Accuracy 0.453857421875\n",
      "Iteration 29160 Training loss 0.054346486926078796 Validation loss 0.05426539108157158 Accuracy 0.45166015625\n",
      "Iteration 29170 Training loss 0.05070967599749565 Validation loss 0.053535234183073044 Accuracy 0.4580078125\n",
      "Iteration 29180 Training loss 0.04701210930943489 Validation loss 0.05360899120569229 Accuracy 0.4580078125\n",
      "Iteration 29190 Training loss 0.05306011810898781 Validation loss 0.053496167063713074 Accuracy 0.458251953125\n",
      "Iteration 29200 Training loss 0.051510803401470184 Validation loss 0.053610388189554214 Accuracy 0.458740234375\n",
      "Iteration 29210 Training loss 0.05294503644108772 Validation loss 0.05380583181977272 Accuracy 0.4560546875\n",
      "Iteration 29220 Training loss 0.05444169417023659 Validation loss 0.05399676784873009 Accuracy 0.453369140625\n",
      "Iteration 29230 Training loss 0.05314089357852936 Validation loss 0.05360553413629532 Accuracy 0.458740234375\n",
      "Iteration 29240 Training loss 0.05588985234498978 Validation loss 0.053714364767074585 Accuracy 0.456787109375\n",
      "Iteration 29250 Training loss 0.054007284343242645 Validation loss 0.05364863947033882 Accuracy 0.45751953125\n",
      "Iteration 29260 Training loss 0.05053964629769325 Validation loss 0.05355704948306084 Accuracy 0.45849609375\n",
      "Iteration 29270 Training loss 0.05268321558833122 Validation loss 0.05362691357731819 Accuracy 0.45849609375\n",
      "Iteration 29280 Training loss 0.052106618881225586 Validation loss 0.05411103740334511 Accuracy 0.45263671875\n",
      "Iteration 29290 Training loss 0.05587473511695862 Validation loss 0.05453922599554062 Accuracy 0.449462890625\n",
      "Iteration 29300 Training loss 0.05305982008576393 Validation loss 0.05359506607055664 Accuracy 0.4580078125\n",
      "Iteration 29310 Training loss 0.0539277121424675 Validation loss 0.05358143895864487 Accuracy 0.45947265625\n",
      "Iteration 29320 Training loss 0.05193585157394409 Validation loss 0.054013822227716446 Accuracy 0.45361328125\n",
      "Iteration 29330 Training loss 0.054242756217718124 Validation loss 0.053578075021505356 Accuracy 0.458251953125\n",
      "Iteration 29340 Training loss 0.05174176022410393 Validation loss 0.05373451113700867 Accuracy 0.45654296875\n",
      "Iteration 29350 Training loss 0.049757491797208786 Validation loss 0.053494442254304886 Accuracy 0.458984375\n",
      "Iteration 29360 Training loss 0.05581875517964363 Validation loss 0.05359642580151558 Accuracy 0.45849609375\n",
      "Iteration 29370 Training loss 0.04877174273133278 Validation loss 0.053933218121528625 Accuracy 0.454833984375\n",
      "Iteration 29380 Training loss 0.048613931983709335 Validation loss 0.053563445806503296 Accuracy 0.45849609375\n",
      "Iteration 29390 Training loss 0.05262710526585579 Validation loss 0.05379161983728409 Accuracy 0.4560546875\n",
      "Iteration 29400 Training loss 0.056738197803497314 Validation loss 0.05552620068192482 Accuracy 0.4404296875\n",
      "Iteration 29410 Training loss 0.055948708206415176 Validation loss 0.05353960394859314 Accuracy 0.458984375\n",
      "Iteration 29420 Training loss 0.051788654178380966 Validation loss 0.05393434315919876 Accuracy 0.45458984375\n",
      "Iteration 29430 Training loss 0.050294991582632065 Validation loss 0.05366438254714012 Accuracy 0.45849609375\n",
      "Iteration 29440 Training loss 0.05040265992283821 Validation loss 0.05371296778321266 Accuracy 0.456787109375\n",
      "Iteration 29450 Training loss 0.04901650920510292 Validation loss 0.05379323288798332 Accuracy 0.455810546875\n",
      "Iteration 29460 Training loss 0.05251352861523628 Validation loss 0.053686924278736115 Accuracy 0.458251953125\n",
      "Iteration 29470 Training loss 0.053425133228302 Validation loss 0.05417585000395775 Accuracy 0.451904296875\n",
      "Iteration 29480 Training loss 0.049167003482580185 Validation loss 0.05358567833900452 Accuracy 0.458251953125\n",
      "Iteration 29490 Training loss 0.0477353036403656 Validation loss 0.05386987328529358 Accuracy 0.455810546875\n",
      "Iteration 29500 Training loss 0.05179766193032265 Validation loss 0.053785182535648346 Accuracy 0.45703125\n",
      "Iteration 29510 Training loss 0.049446068704128265 Validation loss 0.053850896656513214 Accuracy 0.4560546875\n",
      "Iteration 29520 Training loss 0.05153792351484299 Validation loss 0.05365713685750961 Accuracy 0.457763671875\n",
      "Iteration 29530 Training loss 0.054061491042375565 Validation loss 0.053760819137096405 Accuracy 0.456298828125\n",
      "Iteration 29540 Training loss 0.049598805606365204 Validation loss 0.05365836247801781 Accuracy 0.4580078125\n",
      "Iteration 29550 Training loss 0.052286479622125626 Validation loss 0.05415116250514984 Accuracy 0.453369140625\n",
      "Iteration 29560 Training loss 0.05098271742463112 Validation loss 0.05435921624302864 Accuracy 0.45068359375\n",
      "Iteration 29570 Training loss 0.05015771463513374 Validation loss 0.05340617895126343 Accuracy 0.459716796875\n",
      "Iteration 29580 Training loss 0.048696521669626236 Validation loss 0.053669970482587814 Accuracy 0.45751953125\n",
      "Iteration 29590 Training loss 0.0561961829662323 Validation loss 0.05368109419941902 Accuracy 0.457275390625\n",
      "Iteration 29600 Training loss 0.05104450881481171 Validation loss 0.053752582520246506 Accuracy 0.456787109375\n",
      "Iteration 29610 Training loss 0.054378289729356766 Validation loss 0.05378660559654236 Accuracy 0.45751953125\n",
      "Iteration 29620 Training loss 0.05505317077040672 Validation loss 0.054258231073617935 Accuracy 0.452392578125\n",
      "Iteration 29630 Training loss 0.052576515823602676 Validation loss 0.05380804464221001 Accuracy 0.456787109375\n",
      "Iteration 29640 Training loss 0.054374296218156815 Validation loss 0.05356696993112564 Accuracy 0.459228515625\n",
      "Iteration 29650 Training loss 0.04931039363145828 Validation loss 0.0535234697163105 Accuracy 0.458251953125\n",
      "Iteration 29660 Training loss 0.053134985268116 Validation loss 0.053999774158000946 Accuracy 0.45361328125\n",
      "Iteration 29670 Training loss 0.04911349341273308 Validation loss 0.05364008992910385 Accuracy 0.458251953125\n",
      "Iteration 29680 Training loss 0.04964074492454529 Validation loss 0.05384368821978569 Accuracy 0.455810546875\n",
      "Iteration 29690 Training loss 0.05221892148256302 Validation loss 0.05423978716135025 Accuracy 0.45068359375\n",
      "Iteration 29700 Training loss 0.05639755725860596 Validation loss 0.053524427115917206 Accuracy 0.45849609375\n",
      "Iteration 29710 Training loss 0.05144326761364937 Validation loss 0.054251380264759064 Accuracy 0.452392578125\n",
      "Iteration 29720 Training loss 0.05053889751434326 Validation loss 0.053480349481105804 Accuracy 0.45947265625\n",
      "Iteration 29730 Training loss 0.0544905960559845 Validation loss 0.05405587702989578 Accuracy 0.45361328125\n",
      "Iteration 29740 Training loss 0.05130723491311073 Validation loss 0.05375063419342041 Accuracy 0.457275390625\n",
      "Iteration 29750 Training loss 0.055065277963876724 Validation loss 0.05370583385229111 Accuracy 0.457275390625\n",
      "Iteration 29760 Training loss 0.04989085718989372 Validation loss 0.05377115681767464 Accuracy 0.45703125\n",
      "Iteration 29770 Training loss 0.05467570945620537 Validation loss 0.05368207395076752 Accuracy 0.4580078125\n",
      "Iteration 29780 Training loss 0.056869037449359894 Validation loss 0.05462202802300453 Accuracy 0.448486328125\n",
      "Iteration 29790 Training loss 0.05704741179943085 Validation loss 0.05368403345346451 Accuracy 0.45849609375\n",
      "Iteration 29800 Training loss 0.051005806773900986 Validation loss 0.054350219666957855 Accuracy 0.451904296875\n",
      "Iteration 29810 Training loss 0.0496748685836792 Validation loss 0.05356187745928764 Accuracy 0.458984375\n",
      "Iteration 29820 Training loss 0.05054505541920662 Validation loss 0.053837791085243225 Accuracy 0.45703125\n",
      "Iteration 29830 Training loss 0.051649946719408035 Validation loss 0.05406142398715019 Accuracy 0.453125\n",
      "Iteration 29840 Training loss 0.048326630145311356 Validation loss 0.0536300465464592 Accuracy 0.45849609375\n",
      "Iteration 29850 Training loss 0.04815981164574623 Validation loss 0.0537947416305542 Accuracy 0.455322265625\n",
      "Iteration 29860 Training loss 0.05284881591796875 Validation loss 0.05399075895547867 Accuracy 0.4541015625\n",
      "Iteration 29870 Training loss 0.050568245351314545 Validation loss 0.05422757565975189 Accuracy 0.4521484375\n",
      "Iteration 29880 Training loss 0.05060374364256859 Validation loss 0.053900282829999924 Accuracy 0.45458984375\n",
      "Iteration 29890 Training loss 0.04910837858915329 Validation loss 0.05358722433447838 Accuracy 0.45849609375\n",
      "Iteration 29900 Training loss 0.05308401584625244 Validation loss 0.05402030050754547 Accuracy 0.45458984375\n",
      "Iteration 29910 Training loss 0.0527469776570797 Validation loss 0.05358976498246193 Accuracy 0.457763671875\n",
      "Iteration 29920 Training loss 0.054948318749666214 Validation loss 0.05347847193479538 Accuracy 0.458984375\n",
      "Iteration 29930 Training loss 0.049744993448257446 Validation loss 0.05353103205561638 Accuracy 0.458740234375\n",
      "Iteration 29940 Training loss 0.047272052615880966 Validation loss 0.05363147333264351 Accuracy 0.458251953125\n",
      "Iteration 29950 Training loss 0.05471618101000786 Validation loss 0.053749945014715195 Accuracy 0.457275390625\n",
      "Iteration 29960 Training loss 0.05066370964050293 Validation loss 0.053745344281196594 Accuracy 0.456298828125\n",
      "Iteration 29970 Training loss 0.051082875579595566 Validation loss 0.05362485349178314 Accuracy 0.457763671875\n",
      "Iteration 29980 Training loss 0.05136778578162193 Validation loss 0.05364939197897911 Accuracy 0.45751953125\n",
      "Iteration 29990 Training loss 0.05046198517084122 Validation loss 0.053680986166000366 Accuracy 0.45751953125\n",
      "Iteration 30000 Training loss 0.05105762556195259 Validation loss 0.0535316988825798 Accuracy 0.4580078125\n",
      "Iteration 30010 Training loss 0.04984194412827492 Validation loss 0.053718991577625275 Accuracy 0.4560546875\n",
      "Iteration 30020 Training loss 0.05117504298686981 Validation loss 0.05368336662650108 Accuracy 0.457763671875\n",
      "Iteration 30030 Training loss 0.053142327815294266 Validation loss 0.05376071110367775 Accuracy 0.45751953125\n",
      "Iteration 30040 Training loss 0.052493683993816376 Validation loss 0.05444809049367905 Accuracy 0.4501953125\n",
      "Iteration 30050 Training loss 0.05275023356080055 Validation loss 0.053595781326293945 Accuracy 0.458251953125\n",
      "Iteration 30060 Training loss 0.054200612008571625 Validation loss 0.05366555601358414 Accuracy 0.45751953125\n",
      "Iteration 30070 Training loss 0.05324093624949455 Validation loss 0.05432077869772911 Accuracy 0.451416015625\n",
      "Iteration 30080 Training loss 0.051601309329271317 Validation loss 0.053539738059043884 Accuracy 0.4580078125\n",
      "Iteration 30090 Training loss 0.05377852916717529 Validation loss 0.05374238267540932 Accuracy 0.45751953125\n",
      "Iteration 30100 Training loss 0.0546918548643589 Validation loss 0.05368075519800186 Accuracy 0.45654296875\n",
      "Iteration 30110 Training loss 0.05132414028048515 Validation loss 0.053472768515348434 Accuracy 0.4580078125\n",
      "Iteration 30120 Training loss 0.05150459334254265 Validation loss 0.05357537046074867 Accuracy 0.458740234375\n",
      "Iteration 30130 Training loss 0.0541871041059494 Validation loss 0.05368779972195625 Accuracy 0.45751953125\n",
      "Iteration 30140 Training loss 0.05435679107904434 Validation loss 0.05351371318101883 Accuracy 0.45849609375\n",
      "Iteration 30150 Training loss 0.05006865784525871 Validation loss 0.05396483093500137 Accuracy 0.453857421875\n",
      "Iteration 30160 Training loss 0.05284842848777771 Validation loss 0.053566824644804 Accuracy 0.457763671875\n",
      "Iteration 30170 Training loss 0.05355517193675041 Validation loss 0.053613923490047455 Accuracy 0.45751953125\n",
      "Iteration 30180 Training loss 0.0539064034819603 Validation loss 0.05417577549815178 Accuracy 0.450927734375\n",
      "Iteration 30190 Training loss 0.05210932344198227 Validation loss 0.05404160916805267 Accuracy 0.4541015625\n",
      "Iteration 30200 Training loss 0.053509775549173355 Validation loss 0.05353792384266853 Accuracy 0.458740234375\n",
      "Iteration 30210 Training loss 0.05076177045702934 Validation loss 0.05364082008600235 Accuracy 0.456787109375\n",
      "Iteration 30220 Training loss 0.04858752340078354 Validation loss 0.053710974752902985 Accuracy 0.456787109375\n",
      "Iteration 30230 Training loss 0.053672026842832565 Validation loss 0.0535813607275486 Accuracy 0.457763671875\n",
      "Iteration 30240 Training loss 0.050032977014780045 Validation loss 0.05365830287337303 Accuracy 0.456787109375\n",
      "Iteration 30250 Training loss 0.05386952683329582 Validation loss 0.053587254136800766 Accuracy 0.45751953125\n",
      "Iteration 30260 Training loss 0.05362224206328392 Validation loss 0.05347706377506256 Accuracy 0.45849609375\n",
      "Iteration 30270 Training loss 0.05233687162399292 Validation loss 0.05410042777657509 Accuracy 0.452880859375\n",
      "Iteration 30280 Training loss 0.05257394537329674 Validation loss 0.053415585309267044 Accuracy 0.458740234375\n",
      "Iteration 30290 Training loss 0.05165858939290047 Validation loss 0.0534868985414505 Accuracy 0.45849609375\n",
      "Iteration 30300 Training loss 0.050960127264261246 Validation loss 0.05363763868808746 Accuracy 0.4580078125\n",
      "Iteration 30310 Training loss 0.052630502730607986 Validation loss 0.05401679500937462 Accuracy 0.452880859375\n",
      "Iteration 30320 Training loss 0.051367949694395065 Validation loss 0.05358511209487915 Accuracy 0.45751953125\n",
      "Iteration 30330 Training loss 0.049399737268686295 Validation loss 0.05349665880203247 Accuracy 0.458984375\n",
      "Iteration 30340 Training loss 0.05332651734352112 Validation loss 0.053574226796627045 Accuracy 0.458251953125\n",
      "Iteration 30350 Training loss 0.052311211824417114 Validation loss 0.05444087088108063 Accuracy 0.44970703125\n",
      "Iteration 30360 Training loss 0.052805230021476746 Validation loss 0.05351271480321884 Accuracy 0.459228515625\n",
      "Iteration 30370 Training loss 0.054495230317115784 Validation loss 0.053617194294929504 Accuracy 0.45849609375\n",
      "Iteration 30380 Training loss 0.051155101507902145 Validation loss 0.053440481424331665 Accuracy 0.459228515625\n",
      "Iteration 30390 Training loss 0.05193031206727028 Validation loss 0.05379542335867882 Accuracy 0.456787109375\n",
      "Iteration 30400 Training loss 0.04828432574868202 Validation loss 0.05374768003821373 Accuracy 0.456787109375\n",
      "Iteration 30410 Training loss 0.052225466817617416 Validation loss 0.05357804894447327 Accuracy 0.457763671875\n",
      "Iteration 30420 Training loss 0.05346836894750595 Validation loss 0.05383485555648804 Accuracy 0.454833984375\n",
      "Iteration 30430 Training loss 0.04992246627807617 Validation loss 0.053887851536273956 Accuracy 0.455810546875\n",
      "Iteration 30440 Training loss 0.055472034960985184 Validation loss 0.05349106341600418 Accuracy 0.458984375\n",
      "Iteration 30450 Training loss 0.05167738348245621 Validation loss 0.053902264684438705 Accuracy 0.454345703125\n",
      "Iteration 30460 Training loss 0.05248411372303963 Validation loss 0.05357816815376282 Accuracy 0.458984375\n",
      "Iteration 30470 Training loss 0.053951650857925415 Validation loss 0.0537247397005558 Accuracy 0.45654296875\n",
      "Iteration 30480 Training loss 0.05278836935758591 Validation loss 0.05375734344124794 Accuracy 0.456787109375\n",
      "Iteration 30490 Training loss 0.05500071123242378 Validation loss 0.053559865802526474 Accuracy 0.458740234375\n",
      "Iteration 30500 Training loss 0.05341292917728424 Validation loss 0.05345717817544937 Accuracy 0.458984375\n",
      "Iteration 30510 Training loss 0.0527479462325573 Validation loss 0.05363544821739197 Accuracy 0.45751953125\n",
      "Iteration 30520 Training loss 0.05116661638021469 Validation loss 0.05390122905373573 Accuracy 0.455810546875\n",
      "Iteration 30530 Training loss 0.05511654168367386 Validation loss 0.053630731999874115 Accuracy 0.4580078125\n",
      "Iteration 30540 Training loss 0.0519816093146801 Validation loss 0.05362740159034729 Accuracy 0.4580078125\n",
      "Iteration 30550 Training loss 0.05306670069694519 Validation loss 0.05392668768763542 Accuracy 0.454833984375\n",
      "Iteration 30560 Training loss 0.05140215530991554 Validation loss 0.05367160588502884 Accuracy 0.456787109375\n",
      "Iteration 30570 Training loss 0.05402672290802002 Validation loss 0.05372237041592598 Accuracy 0.456298828125\n",
      "Iteration 30580 Training loss 0.05496399849653244 Validation loss 0.053614843636751175 Accuracy 0.4580078125\n",
      "Iteration 30590 Training loss 0.05450185015797615 Validation loss 0.05356099456548691 Accuracy 0.458251953125\n",
      "Iteration 30600 Training loss 0.05573779344558716 Validation loss 0.054001931101083755 Accuracy 0.45458984375\n",
      "Iteration 30610 Training loss 0.05451019108295441 Validation loss 0.0535614937543869 Accuracy 0.4580078125\n",
      "Iteration 30620 Training loss 0.05420827120542526 Validation loss 0.05420574173331261 Accuracy 0.452880859375\n",
      "Iteration 30630 Training loss 0.05340026319026947 Validation loss 0.0536077544093132 Accuracy 0.458984375\n",
      "Iteration 30640 Training loss 0.04982629790902138 Validation loss 0.05360964685678482 Accuracy 0.458251953125\n",
      "Iteration 30650 Training loss 0.05302609130740166 Validation loss 0.053813982754945755 Accuracy 0.454833984375\n",
      "Iteration 30660 Training loss 0.05215822160243988 Validation loss 0.053953465074300766 Accuracy 0.455322265625\n",
      "Iteration 30670 Training loss 0.05270546302199364 Validation loss 0.05390439182519913 Accuracy 0.455810546875\n",
      "Iteration 30680 Training loss 0.056374408304691315 Validation loss 0.053974006325006485 Accuracy 0.454833984375\n",
      "Iteration 30690 Training loss 0.05014857277274132 Validation loss 0.053803447633981705 Accuracy 0.457275390625\n",
      "Iteration 30700 Training loss 0.05420783534646034 Validation loss 0.05457501858472824 Accuracy 0.44921875\n",
      "Iteration 30710 Training loss 0.050140198320150375 Validation loss 0.05367745831608772 Accuracy 0.458251953125\n",
      "Iteration 30720 Training loss 0.054682884365320206 Validation loss 0.05405881255865097 Accuracy 0.45458984375\n",
      "Iteration 30730 Training loss 0.05187583714723587 Validation loss 0.053823407739400864 Accuracy 0.45654296875\n",
      "Iteration 30740 Training loss 0.05586551874876022 Validation loss 0.0541122704744339 Accuracy 0.45361328125\n",
      "Iteration 30750 Training loss 0.04955057054758072 Validation loss 0.05355029180645943 Accuracy 0.4580078125\n",
      "Iteration 30760 Training loss 0.051663294434547424 Validation loss 0.05373922362923622 Accuracy 0.456298828125\n",
      "Iteration 30770 Training loss 0.05722924694418907 Validation loss 0.054097458720207214 Accuracy 0.453369140625\n",
      "Iteration 30780 Training loss 0.05185369402170181 Validation loss 0.0539400652050972 Accuracy 0.455810546875\n",
      "Iteration 30790 Training loss 0.056565165519714355 Validation loss 0.05377170443534851 Accuracy 0.4560546875\n",
      "Iteration 30800 Training loss 0.05506856366991997 Validation loss 0.05387750267982483 Accuracy 0.45556640625\n",
      "Iteration 30810 Training loss 0.04931936413049698 Validation loss 0.053900111466646194 Accuracy 0.45458984375\n",
      "Iteration 30820 Training loss 0.053724098950624466 Validation loss 0.05397079139947891 Accuracy 0.45458984375\n",
      "Iteration 30830 Training loss 0.050090063363313675 Validation loss 0.0536276251077652 Accuracy 0.4580078125\n",
      "Iteration 30840 Training loss 0.049026764929294586 Validation loss 0.05362248793244362 Accuracy 0.4580078125\n",
      "Iteration 30850 Training loss 0.05128198862075806 Validation loss 0.05356636270880699 Accuracy 0.458251953125\n",
      "Iteration 30860 Training loss 0.054705336689949036 Validation loss 0.0543258860707283 Accuracy 0.450439453125\n",
      "Iteration 30870 Training loss 0.04738068953156471 Validation loss 0.05368167906999588 Accuracy 0.456298828125\n",
      "Iteration 30880 Training loss 0.05004600062966347 Validation loss 0.05368714779615402 Accuracy 0.45751953125\n",
      "Iteration 30890 Training loss 0.05203932151198387 Validation loss 0.05415823683142662 Accuracy 0.453125\n",
      "Iteration 30900 Training loss 0.05196116492152214 Validation loss 0.054085247218608856 Accuracy 0.4541015625\n",
      "Iteration 30910 Training loss 0.05010514333844185 Validation loss 0.05376644432544708 Accuracy 0.455078125\n",
      "Iteration 30920 Training loss 0.05390137806534767 Validation loss 0.05361311510205269 Accuracy 0.457275390625\n",
      "Iteration 30930 Training loss 0.048490237444639206 Validation loss 0.0538397952914238 Accuracy 0.45458984375\n",
      "Iteration 30940 Training loss 0.05148785188794136 Validation loss 0.05377579852938652 Accuracy 0.4560546875\n",
      "Iteration 30950 Training loss 0.05299386382102966 Validation loss 0.05350639671087265 Accuracy 0.458740234375\n",
      "Iteration 30960 Training loss 0.05265772342681885 Validation loss 0.05367498844861984 Accuracy 0.4580078125\n",
      "Iteration 30970 Training loss 0.052050504833459854 Validation loss 0.053532760590314865 Accuracy 0.45849609375\n",
      "Iteration 30980 Training loss 0.050310954451560974 Validation loss 0.05361802503466606 Accuracy 0.4580078125\n",
      "Iteration 30990 Training loss 0.05535551533102989 Validation loss 0.054187264293432236 Accuracy 0.453369140625\n",
      "Iteration 31000 Training loss 0.05510266125202179 Validation loss 0.053883422166109085 Accuracy 0.455322265625\n",
      "Iteration 31010 Training loss 0.057635944336652756 Validation loss 0.05520971491932869 Accuracy 0.44189453125\n",
      "Iteration 31020 Training loss 0.054792750626802444 Validation loss 0.053655486553907394 Accuracy 0.457275390625\n",
      "Iteration 31030 Training loss 0.051359161734580994 Validation loss 0.05418655648827553 Accuracy 0.452392578125\n",
      "Iteration 31040 Training loss 0.05206132307648659 Validation loss 0.05349607765674591 Accuracy 0.45947265625\n",
      "Iteration 31050 Training loss 0.05080972611904144 Validation loss 0.05353681370615959 Accuracy 0.459228515625\n",
      "Iteration 31060 Training loss 0.051950208842754364 Validation loss 0.053941771388053894 Accuracy 0.455322265625\n",
      "Iteration 31070 Training loss 0.051096975803375244 Validation loss 0.05350007861852646 Accuracy 0.458740234375\n",
      "Iteration 31080 Training loss 0.0492585152387619 Validation loss 0.053654931485652924 Accuracy 0.456787109375\n",
      "Iteration 31090 Training loss 0.0509607270359993 Validation loss 0.0537121556699276 Accuracy 0.457763671875\n",
      "Iteration 31100 Training loss 0.05264703184366226 Validation loss 0.05371209233999252 Accuracy 0.456787109375\n",
      "Iteration 31110 Training loss 0.05303947255015373 Validation loss 0.053758054971694946 Accuracy 0.456787109375\n",
      "Iteration 31120 Training loss 0.05288342759013176 Validation loss 0.05359797552227974 Accuracy 0.458251953125\n",
      "Iteration 31130 Training loss 0.051151372492313385 Validation loss 0.05368569865822792 Accuracy 0.45751953125\n",
      "Iteration 31140 Training loss 0.0516229085624218 Validation loss 0.05374668911099434 Accuracy 0.456298828125\n",
      "Iteration 31150 Training loss 0.052120961248874664 Validation loss 0.05367983132600784 Accuracy 0.457275390625\n",
      "Iteration 31160 Training loss 0.04982397332787514 Validation loss 0.053893063217401505 Accuracy 0.45458984375\n",
      "Iteration 31170 Training loss 0.05346506088972092 Validation loss 0.05396207422018051 Accuracy 0.455322265625\n",
      "Iteration 31180 Training loss 0.05516817048192024 Validation loss 0.05402208864688873 Accuracy 0.455810546875\n",
      "Iteration 31190 Training loss 0.049868546426296234 Validation loss 0.05404379591345787 Accuracy 0.452392578125\n",
      "Iteration 31200 Training loss 0.05307985469698906 Validation loss 0.053847815841436386 Accuracy 0.45556640625\n",
      "Iteration 31210 Training loss 0.047434452921152115 Validation loss 0.0552741140127182 Accuracy 0.44091796875\n",
      "Iteration 31220 Training loss 0.0524311363697052 Validation loss 0.05399540439248085 Accuracy 0.45361328125\n",
      "Iteration 31230 Training loss 0.05098581314086914 Validation loss 0.05368153005838394 Accuracy 0.45703125\n",
      "Iteration 31240 Training loss 0.0538773350417614 Validation loss 0.05373694375157356 Accuracy 0.456298828125\n",
      "Iteration 31250 Training loss 0.05091143772006035 Validation loss 0.05357309430837631 Accuracy 0.457763671875\n",
      "Iteration 31260 Training loss 0.05444984883069992 Validation loss 0.054318562150001526 Accuracy 0.4521484375\n",
      "Iteration 31270 Training loss 0.05099908262491226 Validation loss 0.05364224314689636 Accuracy 0.45751953125\n",
      "Iteration 31280 Training loss 0.05310450866818428 Validation loss 0.053702570497989655 Accuracy 0.457275390625\n",
      "Iteration 31290 Training loss 0.05136417597532272 Validation loss 0.05353350564837456 Accuracy 0.4580078125\n",
      "Iteration 31300 Training loss 0.05086620897054672 Validation loss 0.05367745831608772 Accuracy 0.457275390625\n",
      "Iteration 31310 Training loss 0.055442601442337036 Validation loss 0.054064277559518814 Accuracy 0.455322265625\n",
      "Iteration 31320 Training loss 0.054506007581949234 Validation loss 0.05373692139983177 Accuracy 0.457275390625\n",
      "Iteration 31330 Training loss 0.05483759567141533 Validation loss 0.05445915460586548 Accuracy 0.448486328125\n",
      "Iteration 31340 Training loss 0.049394071102142334 Validation loss 0.05354690179228783 Accuracy 0.460205078125\n",
      "Iteration 31350 Training loss 0.04930874705314636 Validation loss 0.05372681841254234 Accuracy 0.457275390625\n",
      "Iteration 31360 Training loss 0.05027197301387787 Validation loss 0.053550366312265396 Accuracy 0.459228515625\n",
      "Iteration 31370 Training loss 0.04967055097222328 Validation loss 0.05367178097367287 Accuracy 0.45849609375\n",
      "Iteration 31380 Training loss 0.0503215529024601 Validation loss 0.05451376736164093 Accuracy 0.4482421875\n",
      "Iteration 31390 Training loss 0.05436626076698303 Validation loss 0.05400203913450241 Accuracy 0.45263671875\n",
      "Iteration 31400 Training loss 0.05229593440890312 Validation loss 0.053622644394636154 Accuracy 0.457275390625\n",
      "Iteration 31410 Training loss 0.052154749631881714 Validation loss 0.053797680884599686 Accuracy 0.4560546875\n",
      "Iteration 31420 Training loss 0.05321629345417023 Validation loss 0.053944941610097885 Accuracy 0.454345703125\n",
      "Iteration 31430 Training loss 0.0541967935860157 Validation loss 0.054340481758117676 Accuracy 0.4501953125\n",
      "Iteration 31440 Training loss 0.04983707517385483 Validation loss 0.05372103303670883 Accuracy 0.455810546875\n",
      "Iteration 31450 Training loss 0.054872043430805206 Validation loss 0.054135993123054504 Accuracy 0.452392578125\n",
      "Iteration 31460 Training loss 0.052274540066719055 Validation loss 0.05344388633966446 Accuracy 0.45849609375\n",
      "Iteration 31470 Training loss 0.05327796936035156 Validation loss 0.053448308259248734 Accuracy 0.458984375\n",
      "Iteration 31480 Training loss 0.05276766046881676 Validation loss 0.053366512060165405 Accuracy 0.46044921875\n",
      "Iteration 31490 Training loss 0.04684718698263168 Validation loss 0.053495924919843674 Accuracy 0.45849609375\n",
      "Iteration 31500 Training loss 0.051093120127916336 Validation loss 0.0534241683781147 Accuracy 0.458740234375\n",
      "Iteration 31510 Training loss 0.05102499946951866 Validation loss 0.05342371389269829 Accuracy 0.458984375\n",
      "Iteration 31520 Training loss 0.053793955594301224 Validation loss 0.05398184433579445 Accuracy 0.455078125\n",
      "Iteration 31530 Training loss 0.05131460353732109 Validation loss 0.053431738168001175 Accuracy 0.458740234375\n",
      "Iteration 31540 Training loss 0.052745889872312546 Validation loss 0.05393543839454651 Accuracy 0.455322265625\n",
      "Iteration 31550 Training loss 0.053031209856271744 Validation loss 0.05344662815332413 Accuracy 0.459716796875\n",
      "Iteration 31560 Training loss 0.053661834448575974 Validation loss 0.053627751767635345 Accuracy 0.457763671875\n",
      "Iteration 31570 Training loss 0.04900621622800827 Validation loss 0.05344841256737709 Accuracy 0.45849609375\n",
      "Iteration 31580 Training loss 0.05069742724299431 Validation loss 0.05354152247309685 Accuracy 0.458740234375\n",
      "Iteration 31590 Training loss 0.052610430866479874 Validation loss 0.05345376953482628 Accuracy 0.458740234375\n",
      "Iteration 31600 Training loss 0.05461292341351509 Validation loss 0.05367632210254669 Accuracy 0.456787109375\n",
      "Iteration 31610 Training loss 0.05661746487021446 Validation loss 0.0535111203789711 Accuracy 0.45947265625\n",
      "Iteration 31620 Training loss 0.04865621030330658 Validation loss 0.0534607395529747 Accuracy 0.458740234375\n",
      "Iteration 31630 Training loss 0.05349548161029816 Validation loss 0.05366860330104828 Accuracy 0.45654296875\n",
      "Iteration 31640 Training loss 0.05130996182560921 Validation loss 0.053743522614240646 Accuracy 0.45751953125\n",
      "Iteration 31650 Training loss 0.05031672492623329 Validation loss 0.05372720956802368 Accuracy 0.45703125\n",
      "Iteration 31660 Training loss 0.053634703159332275 Validation loss 0.05414414033293724 Accuracy 0.451416015625\n",
      "Iteration 31670 Training loss 0.05171506106853485 Validation loss 0.05424804240465164 Accuracy 0.451416015625\n",
      "Iteration 31680 Training loss 0.05296644940972328 Validation loss 0.05373354256153107 Accuracy 0.456787109375\n",
      "Iteration 31690 Training loss 0.05417286977171898 Validation loss 0.05357051640748978 Accuracy 0.457275390625\n",
      "Iteration 31700 Training loss 0.05337315797805786 Validation loss 0.05400718003511429 Accuracy 0.452880859375\n",
      "Iteration 31710 Training loss 0.05279769375920296 Validation loss 0.05376391112804413 Accuracy 0.455810546875\n",
      "Iteration 31720 Training loss 0.05311175435781479 Validation loss 0.053765758872032166 Accuracy 0.457275390625\n",
      "Iteration 31730 Training loss 0.05142941325902939 Validation loss 0.05352079123258591 Accuracy 0.459716796875\n",
      "Iteration 31740 Training loss 0.052231963723897934 Validation loss 0.05380543693900108 Accuracy 0.455322265625\n",
      "Iteration 31750 Training loss 0.05159342288970947 Validation loss 0.05350581929087639 Accuracy 0.458984375\n",
      "Iteration 31760 Training loss 0.052055295556783676 Validation loss 0.053464580327272415 Accuracy 0.45947265625\n",
      "Iteration 31770 Training loss 0.05086692422628403 Validation loss 0.05368071049451828 Accuracy 0.457275390625\n",
      "Iteration 31780 Training loss 0.05006697401404381 Validation loss 0.05349661037325859 Accuracy 0.45947265625\n",
      "Iteration 31790 Training loss 0.05333228036761284 Validation loss 0.05351573973894119 Accuracy 0.459228515625\n",
      "Iteration 31800 Training loss 0.05107058212161064 Validation loss 0.05382600426673889 Accuracy 0.455810546875\n",
      "Iteration 31810 Training loss 0.05677137151360512 Validation loss 0.05449690669775009 Accuracy 0.44970703125\n",
      "Iteration 31820 Training loss 0.049152079969644547 Validation loss 0.05347403138875961 Accuracy 0.459716796875\n",
      "Iteration 31830 Training loss 0.05377323925495148 Validation loss 0.05356486141681671 Accuracy 0.457763671875\n",
      "Iteration 31840 Training loss 0.05264227092266083 Validation loss 0.05363176763057709 Accuracy 0.4580078125\n",
      "Iteration 31850 Training loss 0.049431513994932175 Validation loss 0.05397208034992218 Accuracy 0.453857421875\n",
      "Iteration 31860 Training loss 0.0552525632083416 Validation loss 0.053675487637519836 Accuracy 0.45703125\n",
      "Iteration 31870 Training loss 0.05173999071121216 Validation loss 0.05362913757562637 Accuracy 0.4580078125\n",
      "Iteration 31880 Training loss 0.05132606625556946 Validation loss 0.053468007594347 Accuracy 0.45947265625\n",
      "Iteration 31890 Training loss 0.050328329205513 Validation loss 0.05386509746313095 Accuracy 0.455810546875\n",
      "Iteration 31900 Training loss 0.0495724119246006 Validation loss 0.05363059788942337 Accuracy 0.457275390625\n",
      "Iteration 31910 Training loss 0.051472920924425125 Validation loss 0.053955454379320145 Accuracy 0.455078125\n",
      "Iteration 31920 Training loss 0.05373305827379227 Validation loss 0.05382557585835457 Accuracy 0.455078125\n",
      "Iteration 31930 Training loss 0.0537540577352047 Validation loss 0.05370229855179787 Accuracy 0.456787109375\n",
      "Iteration 31940 Training loss 0.051786869764328 Validation loss 0.053434472531080246 Accuracy 0.458984375\n",
      "Iteration 31950 Training loss 0.05627221614122391 Validation loss 0.05347079038619995 Accuracy 0.45947265625\n",
      "Iteration 31960 Training loss 0.04798290506005287 Validation loss 0.05357784032821655 Accuracy 0.458984375\n",
      "Iteration 31970 Training loss 0.05444162338972092 Validation loss 0.054021015763282776 Accuracy 0.453369140625\n",
      "Iteration 31980 Training loss 0.04900528863072395 Validation loss 0.053796228021383286 Accuracy 0.4560546875\n",
      "Iteration 31990 Training loss 0.05135601386427879 Validation loss 0.05344649776816368 Accuracy 0.459228515625\n",
      "Iteration 32000 Training loss 0.050458796322345734 Validation loss 0.05356641486287117 Accuracy 0.4580078125\n",
      "Iteration 32010 Training loss 0.05186295509338379 Validation loss 0.0539192259311676 Accuracy 0.455078125\n",
      "Iteration 32020 Training loss 0.05489221587777138 Validation loss 0.053708016872406006 Accuracy 0.45751953125\n",
      "Iteration 32030 Training loss 0.05151032283902168 Validation loss 0.0534907691180706 Accuracy 0.459228515625\n",
      "Iteration 32040 Training loss 0.053101200610399246 Validation loss 0.053519997745752335 Accuracy 0.458984375\n",
      "Iteration 32050 Training loss 0.049958694726228714 Validation loss 0.053510136902332306 Accuracy 0.45849609375\n",
      "Iteration 32060 Training loss 0.053728457540273666 Validation loss 0.05372304469347 Accuracy 0.4560546875\n",
      "Iteration 32070 Training loss 0.05236183479428291 Validation loss 0.05362430587410927 Accuracy 0.45849609375\n",
      "Iteration 32080 Training loss 0.05066060274839401 Validation loss 0.053650643676519394 Accuracy 0.458251953125\n",
      "Iteration 32090 Training loss 0.05135868862271309 Validation loss 0.054388079792261124 Accuracy 0.451416015625\n",
      "Iteration 32100 Training loss 0.05399463698267937 Validation loss 0.05360783636569977 Accuracy 0.4580078125\n",
      "Iteration 32110 Training loss 0.05055399611592293 Validation loss 0.05370740219950676 Accuracy 0.457275390625\n",
      "Iteration 32120 Training loss 0.05408239737153053 Validation loss 0.05403374508023262 Accuracy 0.453857421875\n",
      "Iteration 32130 Training loss 0.052899859845638275 Validation loss 0.05362895876169205 Accuracy 0.457763671875\n",
      "Iteration 32140 Training loss 0.05260687693953514 Validation loss 0.05362255871295929 Accuracy 0.458251953125\n",
      "Iteration 32150 Training loss 0.05205441266298294 Validation loss 0.054222654551267624 Accuracy 0.453125\n",
      "Iteration 32160 Training loss 0.054787393659353256 Validation loss 0.05358283221721649 Accuracy 0.4580078125\n",
      "Iteration 32170 Training loss 0.05378660932183266 Validation loss 0.054658640176057816 Accuracy 0.447265625\n",
      "Iteration 32180 Training loss 0.05263618752360344 Validation loss 0.05359996482729912 Accuracy 0.4580078125\n",
      "Iteration 32190 Training loss 0.05128483474254608 Validation loss 0.05368601530790329 Accuracy 0.45751953125\n",
      "Iteration 32200 Training loss 0.05155311897397041 Validation loss 0.05362122878432274 Accuracy 0.457763671875\n",
      "Iteration 32210 Training loss 0.05094705522060394 Validation loss 0.053962353616952896 Accuracy 0.4541015625\n",
      "Iteration 32220 Training loss 0.05364120379090309 Validation loss 0.053481414914131165 Accuracy 0.458984375\n",
      "Iteration 32230 Training loss 0.0495908185839653 Validation loss 0.053623832762241364 Accuracy 0.4580078125\n",
      "Iteration 32240 Training loss 0.051253337413072586 Validation loss 0.05370927229523659 Accuracy 0.45703125\n",
      "Iteration 32250 Training loss 0.050508636981248856 Validation loss 0.0540916845202446 Accuracy 0.451904296875\n",
      "Iteration 32260 Training loss 0.053751882165670395 Validation loss 0.05365590751171112 Accuracy 0.456787109375\n",
      "Iteration 32270 Training loss 0.054583437740802765 Validation loss 0.05364581570029259 Accuracy 0.45751953125\n",
      "Iteration 32280 Training loss 0.05694131925702095 Validation loss 0.05383763462305069 Accuracy 0.45654296875\n",
      "Iteration 32290 Training loss 0.05343392863869667 Validation loss 0.05369282886385918 Accuracy 0.45703125\n",
      "Iteration 32300 Training loss 0.053562261164188385 Validation loss 0.054023295640945435 Accuracy 0.452880859375\n",
      "Iteration 32310 Training loss 0.050608355551958084 Validation loss 0.05365263670682907 Accuracy 0.45703125\n",
      "Iteration 32320 Training loss 0.0498468317091465 Validation loss 0.0546482615172863 Accuracy 0.447998046875\n",
      "Iteration 32330 Training loss 0.053923215717077255 Validation loss 0.053677067160606384 Accuracy 0.45751953125\n",
      "Iteration 32340 Training loss 0.05416426435112953 Validation loss 0.053686752915382385 Accuracy 0.45654296875\n",
      "Iteration 32350 Training loss 0.05443336069583893 Validation loss 0.05375179648399353 Accuracy 0.45703125\n",
      "Iteration 32360 Training loss 0.05282391235232353 Validation loss 0.0539366789162159 Accuracy 0.45458984375\n",
      "Iteration 32370 Training loss 0.05480809137225151 Validation loss 0.054034534841775894 Accuracy 0.4541015625\n",
      "Iteration 32380 Training loss 0.05595126748085022 Validation loss 0.05356775224208832 Accuracy 0.458984375\n",
      "Iteration 32390 Training loss 0.04903776943683624 Validation loss 0.05352234095335007 Accuracy 0.4580078125\n",
      "Iteration 32400 Training loss 0.05460178479552269 Validation loss 0.05358581990003586 Accuracy 0.45849609375\n",
      "Iteration 32410 Training loss 0.04900970309972763 Validation loss 0.053577668964862823 Accuracy 0.45849609375\n",
      "Iteration 32420 Training loss 0.04933541640639305 Validation loss 0.053662944585084915 Accuracy 0.458251953125\n",
      "Iteration 32430 Training loss 0.049643877893686295 Validation loss 0.05361421778798103 Accuracy 0.459228515625\n",
      "Iteration 32440 Training loss 0.05085710436105728 Validation loss 0.053711920976638794 Accuracy 0.45751953125\n",
      "Iteration 32450 Training loss 0.052335526794195175 Validation loss 0.05422888696193695 Accuracy 0.451171875\n",
      "Iteration 32460 Training loss 0.052389517426490784 Validation loss 0.05369358882308006 Accuracy 0.45751953125\n",
      "Iteration 32470 Training loss 0.05426490306854248 Validation loss 0.05360593646764755 Accuracy 0.45849609375\n",
      "Iteration 32480 Training loss 0.05402056872844696 Validation loss 0.05369744822382927 Accuracy 0.457275390625\n",
      "Iteration 32490 Training loss 0.05258613079786301 Validation loss 0.05371304601430893 Accuracy 0.45703125\n",
      "Iteration 32500 Training loss 0.05686037242412567 Validation loss 0.053610291332006454 Accuracy 0.45751953125\n",
      "Iteration 32510 Training loss 0.05100405961275101 Validation loss 0.053784482181072235 Accuracy 0.45703125\n",
      "Iteration 32520 Training loss 0.05174659192562103 Validation loss 0.053809382021427155 Accuracy 0.45703125\n",
      "Iteration 32530 Training loss 0.053452108055353165 Validation loss 0.05383262410759926 Accuracy 0.454833984375\n",
      "Iteration 32540 Training loss 0.05013473331928253 Validation loss 0.053869012743234634 Accuracy 0.456298828125\n",
      "Iteration 32550 Training loss 0.047195542603731155 Validation loss 0.053624287247657776 Accuracy 0.45751953125\n",
      "Iteration 32560 Training loss 0.05083365738391876 Validation loss 0.05356279015541077 Accuracy 0.45849609375\n",
      "Iteration 32570 Training loss 0.0498136505484581 Validation loss 0.05358541011810303 Accuracy 0.457275390625\n",
      "Iteration 32580 Training loss 0.052063435316085815 Validation loss 0.0537860281765461 Accuracy 0.456298828125\n",
      "Iteration 32590 Training loss 0.052886366844177246 Validation loss 0.05365115404129028 Accuracy 0.456787109375\n",
      "Iteration 32600 Training loss 0.05204026773571968 Validation loss 0.05352982133626938 Accuracy 0.458740234375\n",
      "Iteration 32610 Training loss 0.05351754650473595 Validation loss 0.05346475541591644 Accuracy 0.458984375\n",
      "Iteration 32620 Training loss 0.05255156382918358 Validation loss 0.05379872024059296 Accuracy 0.4560546875\n",
      "Iteration 32630 Training loss 0.050061605870723724 Validation loss 0.053583525121212006 Accuracy 0.458251953125\n",
      "Iteration 32640 Training loss 0.0517117865383625 Validation loss 0.05345551297068596 Accuracy 0.458740234375\n",
      "Iteration 32650 Training loss 0.050940658897161484 Validation loss 0.05362889915704727 Accuracy 0.45703125\n",
      "Iteration 32660 Training loss 0.05364054813981056 Validation loss 0.05413932353258133 Accuracy 0.452392578125\n",
      "Iteration 32670 Training loss 0.05043473839759827 Validation loss 0.053609613329172134 Accuracy 0.45751953125\n",
      "Iteration 32680 Training loss 0.0511908195912838 Validation loss 0.05392445623874664 Accuracy 0.455810546875\n",
      "Iteration 32690 Training loss 0.051925428211688995 Validation loss 0.05393385514616966 Accuracy 0.455078125\n",
      "Iteration 32700 Training loss 0.05168871581554413 Validation loss 0.05410027131438255 Accuracy 0.4541015625\n",
      "Iteration 32710 Training loss 0.051802702248096466 Validation loss 0.053496118634939194 Accuracy 0.4580078125\n",
      "Iteration 32720 Training loss 0.049760852009058 Validation loss 0.053523797541856766 Accuracy 0.45849609375\n",
      "Iteration 32730 Training loss 0.052106231451034546 Validation loss 0.05368082970380783 Accuracy 0.45751953125\n",
      "Iteration 32740 Training loss 0.05360293388366699 Validation loss 0.053883254528045654 Accuracy 0.45458984375\n",
      "Iteration 32750 Training loss 0.05205825716257095 Validation loss 0.05402597412467003 Accuracy 0.454833984375\n",
      "Iteration 32760 Training loss 0.05125566944479942 Validation loss 0.0535275861620903 Accuracy 0.45849609375\n",
      "Iteration 32770 Training loss 0.04589398205280304 Validation loss 0.05360354483127594 Accuracy 0.458740234375\n",
      "Iteration 32780 Training loss 0.04911694675683975 Validation loss 0.05357865244150162 Accuracy 0.457763671875\n",
      "Iteration 32790 Training loss 0.05268608406186104 Validation loss 0.05350533127784729 Accuracy 0.4580078125\n",
      "Iteration 32800 Training loss 0.051126208156347275 Validation loss 0.05382336676120758 Accuracy 0.456787109375\n",
      "Iteration 32810 Training loss 0.047573648393154144 Validation loss 0.05365243926644325 Accuracy 0.45751953125\n",
      "Iteration 32820 Training loss 0.05256650596857071 Validation loss 0.05357109382748604 Accuracy 0.458740234375\n",
      "Iteration 32830 Training loss 0.05221422389149666 Validation loss 0.05347428470849991 Accuracy 0.459228515625\n",
      "Iteration 32840 Training loss 0.05457184836268425 Validation loss 0.05372212082147598 Accuracy 0.456787109375\n",
      "Iteration 32850 Training loss 0.051694273948669434 Validation loss 0.05348189175128937 Accuracy 0.459228515625\n",
      "Iteration 32860 Training loss 0.05089973285794258 Validation loss 0.05354716628789902 Accuracy 0.45849609375\n",
      "Iteration 32870 Training loss 0.051760394126176834 Validation loss 0.05363515391945839 Accuracy 0.458251953125\n",
      "Iteration 32880 Training loss 0.051290836185216904 Validation loss 0.05400668829679489 Accuracy 0.454345703125\n",
      "Iteration 32890 Training loss 0.05417396500706673 Validation loss 0.05363037809729576 Accuracy 0.457763671875\n",
      "Iteration 32900 Training loss 0.048363097012043 Validation loss 0.05357694998383522 Accuracy 0.457763671875\n",
      "Iteration 32910 Training loss 0.0517091266810894 Validation loss 0.05379130691289902 Accuracy 0.45703125\n",
      "Iteration 32920 Training loss 0.05174308270215988 Validation loss 0.053765568882226944 Accuracy 0.45654296875\n",
      "Iteration 32930 Training loss 0.049986280500888824 Validation loss 0.053872834891080856 Accuracy 0.45458984375\n",
      "Iteration 32940 Training loss 0.0495244599878788 Validation loss 0.05343778058886528 Accuracy 0.458740234375\n",
      "Iteration 32950 Training loss 0.050552621483802795 Validation loss 0.05345163121819496 Accuracy 0.45849609375\n",
      "Iteration 32960 Training loss 0.052868474274873734 Validation loss 0.0535876639187336 Accuracy 0.45703125\n",
      "Iteration 32970 Training loss 0.04996486380696297 Validation loss 0.05355152115225792 Accuracy 0.45751953125\n",
      "Iteration 32980 Training loss 0.050314173102378845 Validation loss 0.05358276143670082 Accuracy 0.4580078125\n",
      "Iteration 32990 Training loss 0.04807758703827858 Validation loss 0.05353701114654541 Accuracy 0.457763671875\n",
      "Iteration 33000 Training loss 0.051439423114061356 Validation loss 0.053622111678123474 Accuracy 0.457275390625\n",
      "Iteration 33010 Training loss 0.052018240094184875 Validation loss 0.05393020063638687 Accuracy 0.4541015625\n",
      "Iteration 33020 Training loss 0.04959876462817192 Validation loss 0.05344850569963455 Accuracy 0.45849609375\n",
      "Iteration 33030 Training loss 0.053980834782123566 Validation loss 0.05349389463663101 Accuracy 0.45849609375\n",
      "Iteration 33040 Training loss 0.052325814962387085 Validation loss 0.05411000922322273 Accuracy 0.452880859375\n",
      "Iteration 33050 Training loss 0.05173060670495033 Validation loss 0.05363713577389717 Accuracy 0.45751953125\n",
      "Iteration 33060 Training loss 0.05202120915055275 Validation loss 0.05388966575264931 Accuracy 0.455810546875\n",
      "Iteration 33070 Training loss 0.05676655098795891 Validation loss 0.05369441211223602 Accuracy 0.45751953125\n",
      "Iteration 33080 Training loss 0.05242850258946419 Validation loss 0.05372554436326027 Accuracy 0.456787109375\n",
      "Iteration 33090 Training loss 0.047756776213645935 Validation loss 0.05339492857456207 Accuracy 0.458984375\n",
      "Iteration 33100 Training loss 0.05514128506183624 Validation loss 0.053709160536527634 Accuracy 0.456298828125\n",
      "Iteration 33110 Training loss 0.04751763865351677 Validation loss 0.053551144897937775 Accuracy 0.4580078125\n",
      "Iteration 33120 Training loss 0.05265216901898384 Validation loss 0.05381979048252106 Accuracy 0.45703125\n",
      "Iteration 33130 Training loss 0.052205681800842285 Validation loss 0.05358489230275154 Accuracy 0.45849609375\n",
      "Iteration 33140 Training loss 0.054200414568185806 Validation loss 0.05352921783924103 Accuracy 0.45849609375\n",
      "Iteration 33150 Training loss 0.05563265085220337 Validation loss 0.053533680737018585 Accuracy 0.458251953125\n",
      "Iteration 33160 Training loss 0.04961147904396057 Validation loss 0.05344300717115402 Accuracy 0.459228515625\n",
      "Iteration 33170 Training loss 0.049535080790519714 Validation loss 0.053911738097667694 Accuracy 0.454345703125\n",
      "Iteration 33180 Training loss 0.05344671756029129 Validation loss 0.05394335463643074 Accuracy 0.4541015625\n",
      "Iteration 33190 Training loss 0.04821844398975372 Validation loss 0.05348866060376167 Accuracy 0.458740234375\n",
      "Iteration 33200 Training loss 0.052578601986169815 Validation loss 0.05364290252327919 Accuracy 0.457275390625\n",
      "Iteration 33210 Training loss 0.05116632208228111 Validation loss 0.053614817559719086 Accuracy 0.4580078125\n",
      "Iteration 33220 Training loss 0.05077061802148819 Validation loss 0.053675126284360886 Accuracy 0.45751953125\n",
      "Iteration 33230 Training loss 0.05178745090961456 Validation loss 0.05392087623476982 Accuracy 0.454833984375\n",
      "Iteration 33240 Training loss 0.05264850705862045 Validation loss 0.05375639349222183 Accuracy 0.456787109375\n",
      "Iteration 33250 Training loss 0.05371243134140968 Validation loss 0.05401849001646042 Accuracy 0.452880859375\n",
      "Iteration 33260 Training loss 0.0519789382815361 Validation loss 0.05364685505628586 Accuracy 0.4580078125\n",
      "Iteration 33270 Training loss 0.0489584244787693 Validation loss 0.05365125834941864 Accuracy 0.457763671875\n",
      "Iteration 33280 Training loss 0.055117297917604446 Validation loss 0.053663939237594604 Accuracy 0.457763671875\n",
      "Iteration 33290 Training loss 0.05141172185540199 Validation loss 0.05372363328933716 Accuracy 0.455078125\n",
      "Iteration 33300 Training loss 0.05285164713859558 Validation loss 0.05355488508939743 Accuracy 0.45751953125\n",
      "Iteration 33310 Training loss 0.052055440843105316 Validation loss 0.05378525331616402 Accuracy 0.455810546875\n",
      "Iteration 33320 Training loss 0.05142827332019806 Validation loss 0.05347959324717522 Accuracy 0.458740234375\n",
      "Iteration 33330 Training loss 0.05024005472660065 Validation loss 0.05375147983431816 Accuracy 0.456787109375\n",
      "Iteration 33340 Training loss 0.05092078447341919 Validation loss 0.05371499061584473 Accuracy 0.456787109375\n",
      "Iteration 33350 Training loss 0.04809783399105072 Validation loss 0.053715985268354416 Accuracy 0.45751953125\n",
      "Iteration 33360 Training loss 0.05701281502842903 Validation loss 0.053913213312625885 Accuracy 0.454833984375\n",
      "Iteration 33370 Training loss 0.052224669605493546 Validation loss 0.053588174283504486 Accuracy 0.45849609375\n",
      "Iteration 33380 Training loss 0.05084024369716644 Validation loss 0.053581103682518005 Accuracy 0.45751953125\n",
      "Iteration 33390 Training loss 0.051502496004104614 Validation loss 0.05359196662902832 Accuracy 0.4580078125\n",
      "Iteration 33400 Training loss 0.05441338196396828 Validation loss 0.05364770069718361 Accuracy 0.45703125\n",
      "Iteration 33410 Training loss 0.052333373576402664 Validation loss 0.053572915494441986 Accuracy 0.458984375\n",
      "Iteration 33420 Training loss 0.050212834030389786 Validation loss 0.053521424531936646 Accuracy 0.457763671875\n",
      "Iteration 33430 Training loss 0.05018240213394165 Validation loss 0.053583044558763504 Accuracy 0.457763671875\n",
      "Iteration 33440 Training loss 0.051466383039951324 Validation loss 0.05380965769290924 Accuracy 0.45654296875\n",
      "Iteration 33450 Training loss 0.052705876529216766 Validation loss 0.0542752631008625 Accuracy 0.45068359375\n",
      "Iteration 33460 Training loss 0.05403819680213928 Validation loss 0.05376950651407242 Accuracy 0.4560546875\n",
      "Iteration 33470 Training loss 0.05328497290611267 Validation loss 0.05395291745662689 Accuracy 0.454833984375\n",
      "Iteration 33480 Training loss 0.051979370415210724 Validation loss 0.053651273250579834 Accuracy 0.457275390625\n",
      "Iteration 33490 Training loss 0.054364655166864395 Validation loss 0.05362782999873161 Accuracy 0.45751953125\n",
      "Iteration 33500 Training loss 0.05170513689517975 Validation loss 0.053815558552742004 Accuracy 0.45556640625\n",
      "Iteration 33510 Training loss 0.04798973724246025 Validation loss 0.05365931615233421 Accuracy 0.4580078125\n",
      "Iteration 33520 Training loss 0.048972081393003464 Validation loss 0.053531404584646225 Accuracy 0.458740234375\n",
      "Iteration 33530 Training loss 0.05360656604170799 Validation loss 0.05398924648761749 Accuracy 0.4541015625\n",
      "Iteration 33540 Training loss 0.05275188013911247 Validation loss 0.0534682422876358 Accuracy 0.459228515625\n",
      "Iteration 33550 Training loss 0.053704071789979935 Validation loss 0.053470637649297714 Accuracy 0.458251953125\n",
      "Iteration 33560 Training loss 0.05219437554478645 Validation loss 0.05363760143518448 Accuracy 0.4580078125\n",
      "Iteration 33570 Training loss 0.051226988434791565 Validation loss 0.054109569638967514 Accuracy 0.45263671875\n",
      "Iteration 33580 Training loss 0.05077913776040077 Validation loss 0.05340523272752762 Accuracy 0.458984375\n",
      "Iteration 33590 Training loss 0.05349956080317497 Validation loss 0.05336609110236168 Accuracy 0.4599609375\n",
      "Iteration 33600 Training loss 0.051331568509340286 Validation loss 0.05356457456946373 Accuracy 0.45849609375\n",
      "Iteration 33610 Training loss 0.052979856729507446 Validation loss 0.05374791473150253 Accuracy 0.458251953125\n",
      "Iteration 33620 Training loss 0.051061004400253296 Validation loss 0.05354698747396469 Accuracy 0.458984375\n",
      "Iteration 33630 Training loss 0.05102423205971718 Validation loss 0.053444989025592804 Accuracy 0.458984375\n",
      "Iteration 33640 Training loss 0.051782529801130295 Validation loss 0.05342961475253105 Accuracy 0.45947265625\n",
      "Iteration 33650 Training loss 0.053464122116565704 Validation loss 0.053537167608737946 Accuracy 0.458984375\n",
      "Iteration 33660 Training loss 0.049127593636512756 Validation loss 0.053951434791088104 Accuracy 0.454345703125\n",
      "Iteration 33670 Training loss 0.05126393586397171 Validation loss 0.053625207394361496 Accuracy 0.45849609375\n",
      "Iteration 33680 Training loss 0.056884367018938065 Validation loss 0.05359441787004471 Accuracy 0.45849609375\n",
      "Iteration 33690 Training loss 0.053451940417289734 Validation loss 0.05364280194044113 Accuracy 0.4580078125\n",
      "Iteration 33700 Training loss 0.051604922860860825 Validation loss 0.05363905802369118 Accuracy 0.458984375\n",
      "Iteration 33710 Training loss 0.04988821968436241 Validation loss 0.05400633066892624 Accuracy 0.453857421875\n",
      "Iteration 33720 Training loss 0.051200225949287415 Validation loss 0.053680770099163055 Accuracy 0.457763671875\n",
      "Iteration 33730 Training loss 0.05160166323184967 Validation loss 0.053732141852378845 Accuracy 0.45751953125\n",
      "Iteration 33740 Training loss 0.05123420059680939 Validation loss 0.05345733091235161 Accuracy 0.458740234375\n",
      "Iteration 33750 Training loss 0.05373023450374603 Validation loss 0.05380958318710327 Accuracy 0.455810546875\n",
      "Iteration 33760 Training loss 0.05015047267079353 Validation loss 0.05349046736955643 Accuracy 0.459228515625\n",
      "Iteration 33770 Training loss 0.05544758960604668 Validation loss 0.05363943800330162 Accuracy 0.458740234375\n",
      "Iteration 33780 Training loss 0.049258388578891754 Validation loss 0.05415261909365654 Accuracy 0.45361328125\n",
      "Iteration 33790 Training loss 0.0529034361243248 Validation loss 0.053569577634334564 Accuracy 0.45947265625\n",
      "Iteration 33800 Training loss 0.050469186156988144 Validation loss 0.05366954952478409 Accuracy 0.4580078125\n",
      "Iteration 33810 Training loss 0.05214560404419899 Validation loss 0.05336824059486389 Accuracy 0.45947265625\n",
      "Iteration 33820 Training loss 0.04961641505360603 Validation loss 0.05355388671159744 Accuracy 0.45947265625\n",
      "Iteration 33830 Training loss 0.05403991788625717 Validation loss 0.05500185489654541 Accuracy 0.444091796875\n",
      "Iteration 33840 Training loss 0.054034460335969925 Validation loss 0.05365147441625595 Accuracy 0.456787109375\n",
      "Iteration 33850 Training loss 0.05306319147348404 Validation loss 0.05355420708656311 Accuracy 0.45849609375\n",
      "Iteration 33860 Training loss 0.049286697059869766 Validation loss 0.05357687920331955 Accuracy 0.458984375\n",
      "Iteration 33870 Training loss 0.05442630872130394 Validation loss 0.053801845759153366 Accuracy 0.455322265625\n",
      "Iteration 33880 Training loss 0.05372767150402069 Validation loss 0.05347982794046402 Accuracy 0.45947265625\n",
      "Iteration 33890 Training loss 0.05633419752120972 Validation loss 0.053771696984767914 Accuracy 0.45654296875\n",
      "Iteration 33900 Training loss 0.053700804710388184 Validation loss 0.05346353352069855 Accuracy 0.458984375\n",
      "Iteration 33910 Training loss 0.05170807987451553 Validation loss 0.053940627723932266 Accuracy 0.45361328125\n",
      "Iteration 33920 Training loss 0.049913544207811356 Validation loss 0.05352194234728813 Accuracy 0.458984375\n",
      "Iteration 33930 Training loss 0.051073938608169556 Validation loss 0.05338481441140175 Accuracy 0.459228515625\n",
      "Iteration 33940 Training loss 0.05386526510119438 Validation loss 0.05359458923339844 Accuracy 0.4580078125\n",
      "Iteration 33950 Training loss 0.05409306287765503 Validation loss 0.053446266800165176 Accuracy 0.458984375\n",
      "Iteration 33960 Training loss 0.05095917731523514 Validation loss 0.05389915779232979 Accuracy 0.455810546875\n",
      "Iteration 33970 Training loss 0.053780242800712585 Validation loss 0.05353612080216408 Accuracy 0.45947265625\n",
      "Iteration 33980 Training loss 0.05458769574761391 Validation loss 0.053677644580602646 Accuracy 0.45751953125\n",
      "Iteration 33990 Training loss 0.05172576755285263 Validation loss 0.05347933992743492 Accuracy 0.458984375\n",
      "Iteration 34000 Training loss 0.05302589759230614 Validation loss 0.05373410880565643 Accuracy 0.45751953125\n",
      "Iteration 34010 Training loss 0.05204419046640396 Validation loss 0.054466165602207184 Accuracy 0.44873046875\n",
      "Iteration 34020 Training loss 0.050290144979953766 Validation loss 0.05372467264533043 Accuracy 0.456787109375\n",
      "Iteration 34030 Training loss 0.051729120314121246 Validation loss 0.05355013534426689 Accuracy 0.457763671875\n",
      "Iteration 34040 Training loss 0.053185898810625076 Validation loss 0.05366070196032524 Accuracy 0.457763671875\n",
      "Iteration 34050 Training loss 0.05264009162783623 Validation loss 0.053514931350946426 Accuracy 0.458984375\n",
      "Iteration 34060 Training loss 0.052789896726608276 Validation loss 0.05359887704253197 Accuracy 0.45849609375\n",
      "Iteration 34070 Training loss 0.051362670958042145 Validation loss 0.053737133741378784 Accuracy 0.458251953125\n",
      "Iteration 34080 Training loss 0.053019143640995026 Validation loss 0.05365344509482384 Accuracy 0.459228515625\n",
      "Iteration 34090 Training loss 0.05009450390934944 Validation loss 0.05371565744280815 Accuracy 0.45849609375\n",
      "Iteration 34100 Training loss 0.04869420453906059 Validation loss 0.05342241749167442 Accuracy 0.45947265625\n",
      "Iteration 34110 Training loss 0.0546872541308403 Validation loss 0.05521339178085327 Accuracy 0.44091796875\n",
      "Iteration 34120 Training loss 0.051666196435689926 Validation loss 0.054071664810180664 Accuracy 0.4541015625\n",
      "Iteration 34130 Training loss 0.048992544412612915 Validation loss 0.053477413952350616 Accuracy 0.458740234375\n",
      "Iteration 34140 Training loss 0.05055653676390648 Validation loss 0.053982384502887726 Accuracy 0.45361328125\n",
      "Iteration 34150 Training loss 0.051749441772699356 Validation loss 0.053465936332941055 Accuracy 0.459228515625\n",
      "Iteration 34160 Training loss 0.05033986270427704 Validation loss 0.05358413979411125 Accuracy 0.45751953125\n",
      "Iteration 34170 Training loss 0.051991891115903854 Validation loss 0.0537576824426651 Accuracy 0.45751953125\n",
      "Iteration 34180 Training loss 0.050510913133621216 Validation loss 0.054345812648534775 Accuracy 0.45068359375\n",
      "Iteration 34190 Training loss 0.051620207726955414 Validation loss 0.05350314453244209 Accuracy 0.4580078125\n",
      "Iteration 34200 Training loss 0.051177218556404114 Validation loss 0.05346449092030525 Accuracy 0.458251953125\n",
      "Iteration 34210 Training loss 0.049204468727111816 Validation loss 0.05344786122441292 Accuracy 0.459716796875\n",
      "Iteration 34220 Training loss 0.05095311999320984 Validation loss 0.05387113243341446 Accuracy 0.4560546875\n",
      "Iteration 34230 Training loss 0.05153597891330719 Validation loss 0.05354209989309311 Accuracy 0.458984375\n",
      "Iteration 34240 Training loss 0.048529062420129776 Validation loss 0.05343092978000641 Accuracy 0.458984375\n",
      "Iteration 34250 Training loss 0.0485881045460701 Validation loss 0.053670331835746765 Accuracy 0.45654296875\n",
      "Iteration 34260 Training loss 0.05094550922513008 Validation loss 0.05343858152627945 Accuracy 0.459228515625\n",
      "Iteration 34270 Training loss 0.05260069668292999 Validation loss 0.053708288818597794 Accuracy 0.45654296875\n",
      "Iteration 34280 Training loss 0.05235922709107399 Validation loss 0.05390162020921707 Accuracy 0.455322265625\n",
      "Iteration 34290 Training loss 0.05214259773492813 Validation loss 0.05363307148218155 Accuracy 0.4580078125\n",
      "Iteration 34300 Training loss 0.05075189843773842 Validation loss 0.05363587662577629 Accuracy 0.45751953125\n",
      "Iteration 34310 Training loss 0.050295766443014145 Validation loss 0.053344521671533585 Accuracy 0.45947265625\n",
      "Iteration 34320 Training loss 0.055725570768117905 Validation loss 0.05452926084399223 Accuracy 0.44921875\n",
      "Iteration 34330 Training loss 0.052161723375320435 Validation loss 0.05383696034550667 Accuracy 0.455322265625\n",
      "Iteration 34340 Training loss 0.051893193274736404 Validation loss 0.054010774940252304 Accuracy 0.455322265625\n",
      "Iteration 34350 Training loss 0.05076860636472702 Validation loss 0.05364631116390228 Accuracy 0.4580078125\n",
      "Iteration 34360 Training loss 0.05278381332755089 Validation loss 0.05355244129896164 Accuracy 0.458251953125\n",
      "Iteration 34370 Training loss 0.04995741695165634 Validation loss 0.05334937199950218 Accuracy 0.459228515625\n",
      "Iteration 34380 Training loss 0.05322602763772011 Validation loss 0.05366255342960358 Accuracy 0.4560546875\n",
      "Iteration 34390 Training loss 0.0516226701438427 Validation loss 0.05360027030110359 Accuracy 0.456298828125\n",
      "Iteration 34400 Training loss 0.05225273221731186 Validation loss 0.05409689620137215 Accuracy 0.453857421875\n",
      "Iteration 34410 Training loss 0.05306945741176605 Validation loss 0.053636375814676285 Accuracy 0.456787109375\n",
      "Iteration 34420 Training loss 0.055757202208042145 Validation loss 0.05382532253861427 Accuracy 0.456298828125\n",
      "Iteration 34430 Training loss 0.05142886936664581 Validation loss 0.05355486646294594 Accuracy 0.459228515625\n",
      "Iteration 34440 Training loss 0.05118027701973915 Validation loss 0.053586434572935104 Accuracy 0.45849609375\n",
      "Iteration 34450 Training loss 0.04987253621220589 Validation loss 0.05438041314482689 Accuracy 0.45068359375\n",
      "Iteration 34460 Training loss 0.04960086941719055 Validation loss 0.053598709404468536 Accuracy 0.4580078125\n",
      "Iteration 34470 Training loss 0.050844695419073105 Validation loss 0.0539633184671402 Accuracy 0.45361328125\n",
      "Iteration 34480 Training loss 0.04865677282214165 Validation loss 0.053794026374816895 Accuracy 0.45654296875\n",
      "Iteration 34490 Training loss 0.04801492393016815 Validation loss 0.05356418713927269 Accuracy 0.457275390625\n",
      "Iteration 34500 Training loss 0.051565613597631454 Validation loss 0.05356229469180107 Accuracy 0.4580078125\n",
      "Iteration 34510 Training loss 0.05216902121901512 Validation loss 0.05357571691274643 Accuracy 0.45751953125\n",
      "Iteration 34520 Training loss 0.05006180703639984 Validation loss 0.05364038795232773 Accuracy 0.45751953125\n",
      "Iteration 34530 Training loss 0.050527509301900864 Validation loss 0.05364016443490982 Accuracy 0.456298828125\n",
      "Iteration 34540 Training loss 0.055516283959150314 Validation loss 0.053821660578250885 Accuracy 0.4560546875\n",
      "Iteration 34550 Training loss 0.05193153768777847 Validation loss 0.053843844681978226 Accuracy 0.45556640625\n",
      "Iteration 34560 Training loss 0.053657326847314835 Validation loss 0.05362952873110771 Accuracy 0.4580078125\n",
      "Iteration 34570 Training loss 0.0527774952352047 Validation loss 0.053664881736040115 Accuracy 0.457763671875\n",
      "Iteration 34580 Training loss 0.0522269643843174 Validation loss 0.053688663989305496 Accuracy 0.457275390625\n",
      "Iteration 34590 Training loss 0.05132918059825897 Validation loss 0.05394856259226799 Accuracy 0.4541015625\n",
      "Iteration 34600 Training loss 0.05299844220280647 Validation loss 0.05429307371377945 Accuracy 0.4521484375\n",
      "Iteration 34610 Training loss 0.05203651264309883 Validation loss 0.05358992889523506 Accuracy 0.45849609375\n",
      "Iteration 34620 Training loss 0.050991885364055634 Validation loss 0.0536850281059742 Accuracy 0.45751953125\n",
      "Iteration 34630 Training loss 0.05236281454563141 Validation loss 0.053623754531145096 Accuracy 0.4580078125\n",
      "Iteration 34640 Training loss 0.05417238175868988 Validation loss 0.054483767598867416 Accuracy 0.449462890625\n",
      "Iteration 34650 Training loss 0.051409561187028885 Validation loss 0.05385309085249901 Accuracy 0.455810546875\n",
      "Iteration 34660 Training loss 0.053585782647132874 Validation loss 0.05376799777150154 Accuracy 0.457763671875\n",
      "Iteration 34670 Training loss 0.051906466484069824 Validation loss 0.05429995805025101 Accuracy 0.45263671875\n",
      "Iteration 34680 Training loss 0.05305586755275726 Validation loss 0.053704604506492615 Accuracy 0.457275390625\n",
      "Iteration 34690 Training loss 0.050141047686338425 Validation loss 0.053784944117069244 Accuracy 0.457763671875\n",
      "Iteration 34700 Training loss 0.05165526270866394 Validation loss 0.0539589524269104 Accuracy 0.455810546875\n",
      "Iteration 34710 Training loss 0.049717191606760025 Validation loss 0.05346909910440445 Accuracy 0.458740234375\n",
      "Iteration 34720 Training loss 0.05245120823383331 Validation loss 0.053508855402469635 Accuracy 0.458984375\n",
      "Iteration 34730 Training loss 0.04923958703875542 Validation loss 0.05368392542004585 Accuracy 0.458251953125\n",
      "Iteration 34740 Training loss 0.053521670401096344 Validation loss 0.053518034517765045 Accuracy 0.458740234375\n",
      "Iteration 34750 Training loss 0.053765084594488144 Validation loss 0.054557327181100845 Accuracy 0.44921875\n",
      "Iteration 34760 Training loss 0.05355595797300339 Validation loss 0.05360439792275429 Accuracy 0.45751953125\n",
      "Iteration 34770 Training loss 0.05160391330718994 Validation loss 0.054273832589387894 Accuracy 0.451416015625\n",
      "Iteration 34780 Training loss 0.05147169902920723 Validation loss 0.05372611805796623 Accuracy 0.45703125\n",
      "Iteration 34790 Training loss 0.04984091967344284 Validation loss 0.05357808619737625 Accuracy 0.45751953125\n",
      "Iteration 34800 Training loss 0.055055636912584305 Validation loss 0.05343163013458252 Accuracy 0.459228515625\n",
      "Iteration 34810 Training loss 0.051287028938531876 Validation loss 0.05352051183581352 Accuracy 0.45849609375\n",
      "Iteration 34820 Training loss 0.0531524233520031 Validation loss 0.053922977298498154 Accuracy 0.453857421875\n",
      "Iteration 34830 Training loss 0.05164599418640137 Validation loss 0.053473085165023804 Accuracy 0.458984375\n",
      "Iteration 34840 Training loss 0.05452854186296463 Validation loss 0.053424108773469925 Accuracy 0.459228515625\n",
      "Iteration 34850 Training loss 0.049211472272872925 Validation loss 0.05355444550514221 Accuracy 0.4580078125\n",
      "Iteration 34860 Training loss 0.04858992621302605 Validation loss 0.05355396494269371 Accuracy 0.458984375\n",
      "Iteration 34870 Training loss 0.05154457688331604 Validation loss 0.053578924387693405 Accuracy 0.45751953125\n",
      "Iteration 34880 Training loss 0.04762161150574684 Validation loss 0.0535711795091629 Accuracy 0.4580078125\n",
      "Iteration 34890 Training loss 0.051422119140625 Validation loss 0.05349309742450714 Accuracy 0.45849609375\n",
      "Iteration 34900 Training loss 0.055901963263750076 Validation loss 0.05471833050251007 Accuracy 0.447998046875\n",
      "Iteration 34910 Training loss 0.055440135300159454 Validation loss 0.05384433642029762 Accuracy 0.457275390625\n",
      "Iteration 34920 Training loss 0.051792047917842865 Validation loss 0.05366523191332817 Accuracy 0.45751953125\n",
      "Iteration 34930 Training loss 0.05360850319266319 Validation loss 0.05355321988463402 Accuracy 0.4580078125\n",
      "Iteration 34940 Training loss 0.05368266627192497 Validation loss 0.05362752452492714 Accuracy 0.4580078125\n",
      "Iteration 34950 Training loss 0.050753336399793625 Validation loss 0.05349580943584442 Accuracy 0.4580078125\n",
      "Iteration 34960 Training loss 0.05177946388721466 Validation loss 0.05343005061149597 Accuracy 0.459228515625\n",
      "Iteration 34970 Training loss 0.05342124402523041 Validation loss 0.054215580224990845 Accuracy 0.45263671875\n",
      "Iteration 34980 Training loss 0.04929360747337341 Validation loss 0.05355726554989815 Accuracy 0.458984375\n",
      "Iteration 34990 Training loss 0.05040086805820465 Validation loss 0.05412866547703743 Accuracy 0.452392578125\n",
      "Iteration 35000 Training loss 0.052987758070230484 Validation loss 0.053908661007881165 Accuracy 0.4560546875\n",
      "Iteration 35010 Training loss 0.05274505540728569 Validation loss 0.05375127121806145 Accuracy 0.45703125\n",
      "Iteration 35020 Training loss 0.05018439516425133 Validation loss 0.05393499508500099 Accuracy 0.45458984375\n",
      "Iteration 35030 Training loss 0.05199535936117172 Validation loss 0.05387435108423233 Accuracy 0.456787109375\n",
      "Iteration 35040 Training loss 0.0496184378862381 Validation loss 0.05346333980560303 Accuracy 0.45947265625\n",
      "Iteration 35050 Training loss 0.04878575727343559 Validation loss 0.05344345420598984 Accuracy 0.45849609375\n",
      "Iteration 35060 Training loss 0.05198836699128151 Validation loss 0.053649235516786575 Accuracy 0.457275390625\n",
      "Iteration 35070 Training loss 0.05057341605424881 Validation loss 0.05347207933664322 Accuracy 0.459228515625\n",
      "Iteration 35080 Training loss 0.05172719433903694 Validation loss 0.05368155986070633 Accuracy 0.457275390625\n",
      "Iteration 35090 Training loss 0.050363413989543915 Validation loss 0.05343877896666527 Accuracy 0.458740234375\n",
      "Iteration 35100 Training loss 0.050475310534238815 Validation loss 0.05345052480697632 Accuracy 0.459228515625\n",
      "Iteration 35110 Training loss 0.05541134253144264 Validation loss 0.05341654643416405 Accuracy 0.45947265625\n",
      "Iteration 35120 Training loss 0.04913076385855675 Validation loss 0.05360861122608185 Accuracy 0.458251953125\n",
      "Iteration 35130 Training loss 0.052141495048999786 Validation loss 0.053436484187841415 Accuracy 0.458984375\n",
      "Iteration 35140 Training loss 0.05092146620154381 Validation loss 0.053370751440525055 Accuracy 0.460205078125\n",
      "Iteration 35150 Training loss 0.05420859903097153 Validation loss 0.053542811423540115 Accuracy 0.458984375\n",
      "Iteration 35160 Training loss 0.05049927905201912 Validation loss 0.053401973098516464 Accuracy 0.459716796875\n",
      "Iteration 35170 Training loss 0.04799515753984451 Validation loss 0.05345558375120163 Accuracy 0.458984375\n",
      "Iteration 35180 Training loss 0.05245115980505943 Validation loss 0.053421344608068466 Accuracy 0.458984375\n",
      "Iteration 35190 Training loss 0.05238441005349159 Validation loss 0.053976237773895264 Accuracy 0.454833984375\n",
      "Iteration 35200 Training loss 0.05256460979580879 Validation loss 0.05347372591495514 Accuracy 0.458984375\n",
      "Iteration 35210 Training loss 0.050786398351192474 Validation loss 0.053413327783346176 Accuracy 0.45947265625\n",
      "Iteration 35220 Training loss 0.04715469479560852 Validation loss 0.05339787155389786 Accuracy 0.4599609375\n",
      "Iteration 35230 Training loss 0.05292697995901108 Validation loss 0.053383875638246536 Accuracy 0.460205078125\n",
      "Iteration 35240 Training loss 0.05514016002416611 Validation loss 0.05348822474479675 Accuracy 0.45849609375\n",
      "Iteration 35250 Training loss 0.050145793706178665 Validation loss 0.0537019707262516 Accuracy 0.457275390625\n",
      "Iteration 35260 Training loss 0.05369814857840538 Validation loss 0.05358222499489784 Accuracy 0.45849609375\n",
      "Iteration 35270 Training loss 0.05115969851613045 Validation loss 0.05356234312057495 Accuracy 0.4580078125\n",
      "Iteration 35280 Training loss 0.05330948904156685 Validation loss 0.05344934016466141 Accuracy 0.459228515625\n",
      "Iteration 35290 Training loss 0.05213867500424385 Validation loss 0.0534261055290699 Accuracy 0.4599609375\n",
      "Iteration 35300 Training loss 0.050173912197351456 Validation loss 0.053417470306158066 Accuracy 0.45947265625\n",
      "Iteration 35310 Training loss 0.052600372582674026 Validation loss 0.05350467562675476 Accuracy 0.458740234375\n",
      "Iteration 35320 Training loss 0.0503079928457737 Validation loss 0.0535220168530941 Accuracy 0.458984375\n",
      "Iteration 35330 Training loss 0.04977348819375038 Validation loss 0.053475428372621536 Accuracy 0.45849609375\n",
      "Iteration 35340 Training loss 0.05361985042691231 Validation loss 0.053491707891225815 Accuracy 0.45849609375\n",
      "Iteration 35350 Training loss 0.05355297401547432 Validation loss 0.053613562136888504 Accuracy 0.45751953125\n",
      "Iteration 35360 Training loss 0.0527467243373394 Validation loss 0.05337882041931152 Accuracy 0.458740234375\n",
      "Iteration 35370 Training loss 0.05738032981753349 Validation loss 0.05429281294345856 Accuracy 0.4501953125\n",
      "Iteration 35380 Training loss 0.051494110375642776 Validation loss 0.05348939448595047 Accuracy 0.45849609375\n",
      "Iteration 35390 Training loss 0.054258596152067184 Validation loss 0.0539739616215229 Accuracy 0.454833984375\n",
      "Iteration 35400 Training loss 0.053580231964588165 Validation loss 0.053560204803943634 Accuracy 0.45751953125\n",
      "Iteration 35410 Training loss 0.05419542267918587 Validation loss 0.05372557416558266 Accuracy 0.457275390625\n",
      "Iteration 35420 Training loss 0.053194642066955566 Validation loss 0.05365634709596634 Accuracy 0.456298828125\n",
      "Iteration 35430 Training loss 0.05246850475668907 Validation loss 0.053863249719142914 Accuracy 0.453857421875\n",
      "Iteration 35440 Training loss 0.05246300250291824 Validation loss 0.05342242121696472 Accuracy 0.45849609375\n",
      "Iteration 35450 Training loss 0.05053871497511864 Validation loss 0.05369551479816437 Accuracy 0.4580078125\n",
      "Iteration 35460 Training loss 0.05344408005475998 Validation loss 0.05464872345328331 Accuracy 0.447021484375\n",
      "Iteration 35470 Training loss 0.052994467318058014 Validation loss 0.05383048206567764 Accuracy 0.4560546875\n",
      "Iteration 35480 Training loss 0.0553402304649353 Validation loss 0.05396856367588043 Accuracy 0.452880859375\n",
      "Iteration 35490 Training loss 0.05279175192117691 Validation loss 0.05355334281921387 Accuracy 0.457763671875\n",
      "Iteration 35500 Training loss 0.04971258342266083 Validation loss 0.0539541095495224 Accuracy 0.45361328125\n",
      "Iteration 35510 Training loss 0.05000431835651398 Validation loss 0.05368051305413246 Accuracy 0.45703125\n",
      "Iteration 35520 Training loss 0.051859546452760696 Validation loss 0.053542762994766235 Accuracy 0.457275390625\n",
      "Iteration 35530 Training loss 0.05038103833794594 Validation loss 0.05368112772703171 Accuracy 0.457275390625\n",
      "Iteration 35540 Training loss 0.05317195504903793 Validation loss 0.05427062511444092 Accuracy 0.45068359375\n",
      "Iteration 35550 Training loss 0.05328791216015816 Validation loss 0.05340975522994995 Accuracy 0.458984375\n",
      "Iteration 35560 Training loss 0.051109425723552704 Validation loss 0.05345027148723602 Accuracy 0.459228515625\n",
      "Iteration 35570 Training loss 0.056061938405036926 Validation loss 0.053502392023801804 Accuracy 0.458251953125\n",
      "Iteration 35580 Training loss 0.05396299809217453 Validation loss 0.053459372371435165 Accuracy 0.45947265625\n",
      "Iteration 35590 Training loss 0.05007083714008331 Validation loss 0.05372270196676254 Accuracy 0.456787109375\n",
      "Iteration 35600 Training loss 0.0505802296102047 Validation loss 0.05369560047984123 Accuracy 0.4560546875\n",
      "Iteration 35610 Training loss 0.048455167561769485 Validation loss 0.05339536443352699 Accuracy 0.45947265625\n",
      "Iteration 35620 Training loss 0.052253227680921555 Validation loss 0.05373270809650421 Accuracy 0.45654296875\n",
      "Iteration 35630 Training loss 0.050760477781295776 Validation loss 0.053913380950689316 Accuracy 0.455078125\n",
      "Iteration 35640 Training loss 0.05274481698870659 Validation loss 0.05364551022648811 Accuracy 0.457763671875\n",
      "Iteration 35650 Training loss 0.05351875722408295 Validation loss 0.05412225425243378 Accuracy 0.4541015625\n",
      "Iteration 35660 Training loss 0.05152365192770958 Validation loss 0.05373920127749443 Accuracy 0.45654296875\n",
      "Iteration 35670 Training loss 0.05441544950008392 Validation loss 0.05359499901533127 Accuracy 0.4580078125\n",
      "Iteration 35680 Training loss 0.0524502769112587 Validation loss 0.05384968966245651 Accuracy 0.456787109375\n",
      "Iteration 35690 Training loss 0.054286692291498184 Validation loss 0.05357561260461807 Accuracy 0.458251953125\n",
      "Iteration 35700 Training loss 0.052375178784132004 Validation loss 0.05361715704202652 Accuracy 0.45751953125\n",
      "Iteration 35710 Training loss 0.050368234515190125 Validation loss 0.054065920412540436 Accuracy 0.453125\n",
      "Iteration 35720 Training loss 0.052402984350919724 Validation loss 0.05351205915212631 Accuracy 0.45849609375\n",
      "Iteration 35730 Training loss 0.05500878766179085 Validation loss 0.05385169759392738 Accuracy 0.455810546875\n",
      "Iteration 35740 Training loss 0.048042021691799164 Validation loss 0.05366629734635353 Accuracy 0.45703125\n",
      "Iteration 35750 Training loss 0.048467665910720825 Validation loss 0.05372311547398567 Accuracy 0.456298828125\n",
      "Iteration 35760 Training loss 0.05258375033736229 Validation loss 0.05438597500324249 Accuracy 0.44970703125\n",
      "Iteration 35770 Training loss 0.057131633162498474 Validation loss 0.053547780960798264 Accuracy 0.45849609375\n",
      "Iteration 35780 Training loss 0.05460236966609955 Validation loss 0.053878191858530045 Accuracy 0.45458984375\n",
      "Iteration 35790 Training loss 0.05435778573155403 Validation loss 0.05352845415472984 Accuracy 0.459228515625\n",
      "Iteration 35800 Training loss 0.054600074887275696 Validation loss 0.05344637483358383 Accuracy 0.458740234375\n",
      "Iteration 35810 Training loss 0.052794646471738815 Validation loss 0.05387446656823158 Accuracy 0.45458984375\n",
      "Iteration 35820 Training loss 0.05299615487456322 Validation loss 0.05364447087049484 Accuracy 0.456298828125\n",
      "Iteration 35830 Training loss 0.05135194957256317 Validation loss 0.053500134497880936 Accuracy 0.45849609375\n",
      "Iteration 35840 Training loss 0.05077236518263817 Validation loss 0.05344054102897644 Accuracy 0.45849609375\n",
      "Iteration 35850 Training loss 0.05179167538881302 Validation loss 0.053527675569057465 Accuracy 0.458740234375\n",
      "Iteration 35860 Training loss 0.05095257610082626 Validation loss 0.053715310990810394 Accuracy 0.456298828125\n",
      "Iteration 35870 Training loss 0.055217791348695755 Validation loss 0.05353645235300064 Accuracy 0.458740234375\n",
      "Iteration 35880 Training loss 0.05004994571208954 Validation loss 0.05355098843574524 Accuracy 0.457275390625\n",
      "Iteration 35890 Training loss 0.048190888017416 Validation loss 0.053352247923612595 Accuracy 0.459716796875\n",
      "Iteration 35900 Training loss 0.053986113518476486 Validation loss 0.05348185449838638 Accuracy 0.45849609375\n",
      "Iteration 35910 Training loss 0.0491822212934494 Validation loss 0.05351601541042328 Accuracy 0.45947265625\n",
      "Iteration 35920 Training loss 0.050249774008989334 Validation loss 0.05364208668470383 Accuracy 0.45751953125\n",
      "Iteration 35930 Training loss 0.049357421696186066 Validation loss 0.05354306101799011 Accuracy 0.458984375\n",
      "Iteration 35940 Training loss 0.05402854457497597 Validation loss 0.05367177724838257 Accuracy 0.457763671875\n",
      "Iteration 35950 Training loss 0.04933282360434532 Validation loss 0.05372942239046097 Accuracy 0.456787109375\n",
      "Iteration 35960 Training loss 0.046663470566272736 Validation loss 0.054328788071870804 Accuracy 0.4501953125\n",
      "Iteration 35970 Training loss 0.05611500144004822 Validation loss 0.053351063281297684 Accuracy 0.459716796875\n",
      "Iteration 35980 Training loss 0.05321608483791351 Validation loss 0.053644582629203796 Accuracy 0.458251953125\n",
      "Iteration 35990 Training loss 0.047747887670993805 Validation loss 0.053575705736875534 Accuracy 0.458984375\n",
      "Iteration 36000 Training loss 0.05192117393016815 Validation loss 0.054059501737356186 Accuracy 0.454833984375\n",
      "Iteration 36010 Training loss 0.04946910962462425 Validation loss 0.053463999181985855 Accuracy 0.4599609375\n",
      "Iteration 36020 Training loss 0.04944430664181709 Validation loss 0.05434918403625488 Accuracy 0.450927734375\n",
      "Iteration 36030 Training loss 0.05513197183609009 Validation loss 0.05368031933903694 Accuracy 0.457275390625\n",
      "Iteration 36040 Training loss 0.05432180315256119 Validation loss 0.053765248507261276 Accuracy 0.456298828125\n",
      "Iteration 36050 Training loss 0.04851381853222847 Validation loss 0.05371391773223877 Accuracy 0.45751953125\n",
      "Iteration 36060 Training loss 0.05538708344101906 Validation loss 0.05406608432531357 Accuracy 0.45458984375\n",
      "Iteration 36070 Training loss 0.050707995891571045 Validation loss 0.05338055640459061 Accuracy 0.460205078125\n",
      "Iteration 36080 Training loss 0.051262419670820236 Validation loss 0.05402787774801254 Accuracy 0.45458984375\n",
      "Iteration 36090 Training loss 0.05166560783982277 Validation loss 0.05349571257829666 Accuracy 0.458984375\n",
      "Iteration 36100 Training loss 0.051167938858270645 Validation loss 0.053516071289777756 Accuracy 0.458740234375\n",
      "Iteration 36110 Training loss 0.0497552827000618 Validation loss 0.05363844707608223 Accuracy 0.45654296875\n",
      "Iteration 36120 Training loss 0.05090615898370743 Validation loss 0.05405408889055252 Accuracy 0.453125\n",
      "Iteration 36130 Training loss 0.049903176724910736 Validation loss 0.05357853323221207 Accuracy 0.45751953125\n",
      "Iteration 36140 Training loss 0.053329579532146454 Validation loss 0.05372731387615204 Accuracy 0.45556640625\n",
      "Iteration 36150 Training loss 0.05066033825278282 Validation loss 0.053462374955415726 Accuracy 0.4580078125\n",
      "Iteration 36160 Training loss 0.05023818835616112 Validation loss 0.053668759763240814 Accuracy 0.455078125\n",
      "Iteration 36170 Training loss 0.04928431659936905 Validation loss 0.05342186987400055 Accuracy 0.459716796875\n",
      "Iteration 36180 Training loss 0.04886212199926376 Validation loss 0.053443145006895065 Accuracy 0.4599609375\n",
      "Iteration 36190 Training loss 0.048922326415777206 Validation loss 0.05342164263129234 Accuracy 0.458740234375\n",
      "Iteration 36200 Training loss 0.04993753135204315 Validation loss 0.053477946668863297 Accuracy 0.45947265625\n",
      "Iteration 36210 Training loss 0.05330551043152809 Validation loss 0.054462265223264694 Accuracy 0.4501953125\n",
      "Iteration 36220 Training loss 0.05189847946166992 Validation loss 0.05401741713285446 Accuracy 0.454345703125\n",
      "Iteration 36230 Training loss 0.04917697235941887 Validation loss 0.05348297208547592 Accuracy 0.4599609375\n",
      "Iteration 36240 Training loss 0.05068464204668999 Validation loss 0.053780730813741684 Accuracy 0.45703125\n",
      "Iteration 36250 Training loss 0.05029236152768135 Validation loss 0.05414078012108803 Accuracy 0.452880859375\n",
      "Iteration 36260 Training loss 0.05221554636955261 Validation loss 0.05338497832417488 Accuracy 0.460205078125\n",
      "Iteration 36270 Training loss 0.05022979527711868 Validation loss 0.05353774130344391 Accuracy 0.457763671875\n",
      "Iteration 36280 Training loss 0.053273025900125504 Validation loss 0.05359790474176407 Accuracy 0.4580078125\n",
      "Iteration 36290 Training loss 0.056038688868284225 Validation loss 0.053726594895124435 Accuracy 0.456787109375\n",
      "Iteration 36300 Training loss 0.051710132509469986 Validation loss 0.053557440638542175 Accuracy 0.45751953125\n",
      "Iteration 36310 Training loss 0.05377868190407753 Validation loss 0.0536654032766819 Accuracy 0.45947265625\n",
      "Iteration 36320 Training loss 0.05333680287003517 Validation loss 0.05554075911641121 Accuracy 0.439453125\n",
      "Iteration 36330 Training loss 0.05330343917012215 Validation loss 0.054280564188957214 Accuracy 0.45263671875\n",
      "Iteration 36340 Training loss 0.05336296558380127 Validation loss 0.05364058539271355 Accuracy 0.45849609375\n",
      "Iteration 36350 Training loss 0.05361710488796234 Validation loss 0.05385127663612366 Accuracy 0.45654296875\n",
      "Iteration 36360 Training loss 0.04729636758565903 Validation loss 0.05367780104279518 Accuracy 0.457763671875\n",
      "Iteration 36370 Training loss 0.05723350867629051 Validation loss 0.05358836427330971 Accuracy 0.458984375\n",
      "Iteration 36380 Training loss 0.05311058834195137 Validation loss 0.05347947031259537 Accuracy 0.459228515625\n",
      "Iteration 36390 Training loss 0.05196317657828331 Validation loss 0.05361446738243103 Accuracy 0.45849609375\n",
      "Iteration 36400 Training loss 0.052847784012556076 Validation loss 0.0536230243742466 Accuracy 0.458251953125\n",
      "Iteration 36410 Training loss 0.05091208964586258 Validation loss 0.05362661927938461 Accuracy 0.456298828125\n",
      "Iteration 36420 Training loss 0.05555524677038193 Validation loss 0.05342664569616318 Accuracy 0.45849609375\n",
      "Iteration 36430 Training loss 0.05076234042644501 Validation loss 0.05352391302585602 Accuracy 0.4580078125\n",
      "Iteration 36440 Training loss 0.04816707223653793 Validation loss 0.05366842821240425 Accuracy 0.45751953125\n",
      "Iteration 36450 Training loss 0.050842128694057465 Validation loss 0.05349598452448845 Accuracy 0.45751953125\n",
      "Iteration 36460 Training loss 0.05625800043344498 Validation loss 0.053752243518829346 Accuracy 0.4560546875\n",
      "Iteration 36470 Training loss 0.05060406029224396 Validation loss 0.05342011898756027 Accuracy 0.458984375\n",
      "Iteration 36480 Training loss 0.051902465522289276 Validation loss 0.053439538925886154 Accuracy 0.458984375\n",
      "Iteration 36490 Training loss 0.05178358778357506 Validation loss 0.05367696285247803 Accuracy 0.45751953125\n",
      "Iteration 36500 Training loss 0.05080738663673401 Validation loss 0.05368736758828163 Accuracy 0.456298828125\n",
      "Iteration 36510 Training loss 0.05161002650856972 Validation loss 0.05428708717226982 Accuracy 0.450927734375\n",
      "Iteration 36520 Training loss 0.05244668945670128 Validation loss 0.053648192435503006 Accuracy 0.456787109375\n",
      "Iteration 36530 Training loss 0.05061657726764679 Validation loss 0.05352557450532913 Accuracy 0.458984375\n",
      "Iteration 36540 Training loss 0.05178019031882286 Validation loss 0.05346904322504997 Accuracy 0.459716796875\n",
      "Iteration 36550 Training loss 0.055384546518325806 Validation loss 0.05354292690753937 Accuracy 0.458740234375\n",
      "Iteration 36560 Training loss 0.051080718636512756 Validation loss 0.05393970385193825 Accuracy 0.455322265625\n",
      "Iteration 36570 Training loss 0.05399961769580841 Validation loss 0.054317258298397064 Accuracy 0.45068359375\n",
      "Iteration 36580 Training loss 0.05277617275714874 Validation loss 0.05361141636967659 Accuracy 0.4580078125\n",
      "Iteration 36590 Training loss 0.05330520495772362 Validation loss 0.053961869329214096 Accuracy 0.454833984375\n",
      "Iteration 36600 Training loss 0.05134458467364311 Validation loss 0.053708769381046295 Accuracy 0.456787109375\n",
      "Iteration 36610 Training loss 0.05328185483813286 Validation loss 0.05378872901201248 Accuracy 0.45703125\n",
      "Iteration 36620 Training loss 0.051803283393383026 Validation loss 0.05355024337768555 Accuracy 0.459716796875\n",
      "Iteration 36630 Training loss 0.05361219123005867 Validation loss 0.05408118665218353 Accuracy 0.4541015625\n",
      "Iteration 36640 Training loss 0.048097360879182816 Validation loss 0.053502317517995834 Accuracy 0.458984375\n",
      "Iteration 36650 Training loss 0.05124536529183388 Validation loss 0.05354715511202812 Accuracy 0.458984375\n",
      "Iteration 36660 Training loss 0.05384306609630585 Validation loss 0.053555067628622055 Accuracy 0.458251953125\n",
      "Iteration 36670 Training loss 0.05117441713809967 Validation loss 0.05340729281306267 Accuracy 0.458740234375\n",
      "Iteration 36680 Training loss 0.053725793957710266 Validation loss 0.05337310954928398 Accuracy 0.459716796875\n",
      "Iteration 36690 Training loss 0.052363332360982895 Validation loss 0.053879521787166595 Accuracy 0.45556640625\n",
      "Iteration 36700 Training loss 0.052194319665431976 Validation loss 0.053572192788124084 Accuracy 0.4580078125\n",
      "Iteration 36710 Training loss 0.05115548521280289 Validation loss 0.053822264075279236 Accuracy 0.456787109375\n",
      "Iteration 36720 Training loss 0.050224628299474716 Validation loss 0.05375383794307709 Accuracy 0.456787109375\n",
      "Iteration 36730 Training loss 0.052421364933252335 Validation loss 0.05359775200486183 Accuracy 0.457275390625\n",
      "Iteration 36740 Training loss 0.05059860274195671 Validation loss 0.05421825870871544 Accuracy 0.4521484375\n",
      "Iteration 36750 Training loss 0.0514611154794693 Validation loss 0.05356849357485771 Accuracy 0.458251953125\n",
      "Iteration 36760 Training loss 0.05210055410861969 Validation loss 0.054155390709638596 Accuracy 0.4521484375\n",
      "Iteration 36770 Training loss 0.0557502843439579 Validation loss 0.05406732112169266 Accuracy 0.45361328125\n",
      "Iteration 36780 Training loss 0.054185450077056885 Validation loss 0.05364319309592247 Accuracy 0.45849609375\n",
      "Iteration 36790 Training loss 0.05100509524345398 Validation loss 0.05407925695180893 Accuracy 0.454345703125\n",
      "Iteration 36800 Training loss 0.05371500179171562 Validation loss 0.05391228199005127 Accuracy 0.456298828125\n",
      "Iteration 36810 Training loss 0.0509919747710228 Validation loss 0.05388456583023071 Accuracy 0.45654296875\n",
      "Iteration 36820 Training loss 0.05142125487327576 Validation loss 0.053585875779390335 Accuracy 0.459228515625\n",
      "Iteration 36830 Training loss 0.050977472215890884 Validation loss 0.05379713699221611 Accuracy 0.455810546875\n",
      "Iteration 36840 Training loss 0.05310598760843277 Validation loss 0.05350283160805702 Accuracy 0.458740234375\n",
      "Iteration 36850 Training loss 0.05204291269183159 Validation loss 0.053830210119485855 Accuracy 0.455322265625\n",
      "Iteration 36860 Training loss 0.052633848041296005 Validation loss 0.05345458537340164 Accuracy 0.45947265625\n",
      "Iteration 36870 Training loss 0.05210237577557564 Validation loss 0.05372629687190056 Accuracy 0.45654296875\n",
      "Iteration 36880 Training loss 0.05349455773830414 Validation loss 0.05338127911090851 Accuracy 0.45947265625\n",
      "Iteration 36890 Training loss 0.05145980790257454 Validation loss 0.05359087884426117 Accuracy 0.45751953125\n",
      "Iteration 36900 Training loss 0.04954150319099426 Validation loss 0.05337298661470413 Accuracy 0.460205078125\n",
      "Iteration 36910 Training loss 0.05400047078728676 Validation loss 0.05346038565039635 Accuracy 0.458740234375\n",
      "Iteration 36920 Training loss 0.050775349140167236 Validation loss 0.05357898399233818 Accuracy 0.45703125\n",
      "Iteration 36930 Training loss 0.05333508178591728 Validation loss 0.054110296070575714 Accuracy 0.453369140625\n",
      "Iteration 36940 Training loss 0.0501561164855957 Validation loss 0.05369192361831665 Accuracy 0.457763671875\n",
      "Iteration 36950 Training loss 0.05067270249128342 Validation loss 0.05370519310235977 Accuracy 0.457275390625\n",
      "Iteration 36960 Training loss 0.04853789880871773 Validation loss 0.053427793085575104 Accuracy 0.458984375\n",
      "Iteration 36970 Training loss 0.05591193586587906 Validation loss 0.053493525832891464 Accuracy 0.458984375\n",
      "Iteration 36980 Training loss 0.054745323956012726 Validation loss 0.05359911918640137 Accuracy 0.457763671875\n",
      "Iteration 36990 Training loss 0.0475461408495903 Validation loss 0.05375686287879944 Accuracy 0.45751953125\n",
      "Iteration 37000 Training loss 0.053810007870197296 Validation loss 0.053687941282987595 Accuracy 0.45751953125\n",
      "Iteration 37010 Training loss 0.05044792965054512 Validation loss 0.053412675857543945 Accuracy 0.45947265625\n",
      "Iteration 37020 Training loss 0.05130473151803017 Validation loss 0.05355270206928253 Accuracy 0.45751953125\n",
      "Iteration 37030 Training loss 0.05170557647943497 Validation loss 0.05365779995918274 Accuracy 0.4580078125\n",
      "Iteration 37040 Training loss 0.05310538783669472 Validation loss 0.05382496118545532 Accuracy 0.45556640625\n",
      "Iteration 37050 Training loss 0.05648406967520714 Validation loss 0.05418745055794716 Accuracy 0.451904296875\n",
      "Iteration 37060 Training loss 0.052817702293395996 Validation loss 0.05363155156373978 Accuracy 0.456298828125\n",
      "Iteration 37070 Training loss 0.05204479768872261 Validation loss 0.053502462804317474 Accuracy 0.458740234375\n",
      "Iteration 37080 Training loss 0.052491821348667145 Validation loss 0.05334778130054474 Accuracy 0.458984375\n",
      "Iteration 37090 Training loss 0.05135420709848404 Validation loss 0.05358478054404259 Accuracy 0.45849609375\n",
      "Iteration 37100 Training loss 0.051432251930236816 Validation loss 0.05395688861608505 Accuracy 0.455810546875\n",
      "Iteration 37110 Training loss 0.0516720786690712 Validation loss 0.053555749356746674 Accuracy 0.45849609375\n",
      "Iteration 37120 Training loss 0.055683035403490067 Validation loss 0.05364874005317688 Accuracy 0.4560546875\n",
      "Iteration 37130 Training loss 0.05476246029138565 Validation loss 0.0535559356212616 Accuracy 0.459228515625\n",
      "Iteration 37140 Training loss 0.04965292289853096 Validation loss 0.05337098613381386 Accuracy 0.4599609375\n",
      "Iteration 37150 Training loss 0.052378829568624496 Validation loss 0.05342179909348488 Accuracy 0.459716796875\n",
      "Iteration 37160 Training loss 0.04978526756167412 Validation loss 0.0534149594604969 Accuracy 0.459716796875\n",
      "Iteration 37170 Training loss 0.054246366024017334 Validation loss 0.05349615216255188 Accuracy 0.459228515625\n",
      "Iteration 37180 Training loss 0.054669372737407684 Validation loss 0.05336187407374382 Accuracy 0.45947265625\n",
      "Iteration 37190 Training loss 0.050597142428159714 Validation loss 0.05359470471739769 Accuracy 0.4580078125\n",
      "Iteration 37200 Training loss 0.05242735892534256 Validation loss 0.05335679650306702 Accuracy 0.459716796875\n",
      "Iteration 37210 Training loss 0.0519995279610157 Validation loss 0.053882528096437454 Accuracy 0.456298828125\n",
      "Iteration 37220 Training loss 0.051577504724264145 Validation loss 0.053485672920942307 Accuracy 0.458984375\n",
      "Iteration 37230 Training loss 0.052324920892715454 Validation loss 0.05333809554576874 Accuracy 0.460205078125\n",
      "Iteration 37240 Training loss 0.050017766654491425 Validation loss 0.05341965705156326 Accuracy 0.459716796875\n",
      "Iteration 37250 Training loss 0.0538351908326149 Validation loss 0.05348805710673332 Accuracy 0.45849609375\n",
      "Iteration 37260 Training loss 0.05425076186656952 Validation loss 0.054620642215013504 Accuracy 0.447509765625\n",
      "Iteration 37270 Training loss 0.05243788659572601 Validation loss 0.05362638458609581 Accuracy 0.45703125\n",
      "Iteration 37280 Training loss 0.05492856726050377 Validation loss 0.05345745012164116 Accuracy 0.459228515625\n",
      "Iteration 37290 Training loss 0.05323034152388573 Validation loss 0.05539163947105408 Accuracy 0.439697265625\n",
      "Iteration 37300 Training loss 0.05157874524593353 Validation loss 0.05387388914823532 Accuracy 0.455078125\n",
      "Iteration 37310 Training loss 0.04948512092232704 Validation loss 0.05353677645325661 Accuracy 0.458251953125\n",
      "Iteration 37320 Training loss 0.05358017981052399 Validation loss 0.053587596863508224 Accuracy 0.45751953125\n",
      "Iteration 37330 Training loss 0.05269408971071243 Validation loss 0.05398474633693695 Accuracy 0.45361328125\n",
      "Iteration 37340 Training loss 0.051328469067811966 Validation loss 0.05363538861274719 Accuracy 0.456787109375\n",
      "Iteration 37350 Training loss 0.051918063312768936 Validation loss 0.05356335639953613 Accuracy 0.458251953125\n",
      "Iteration 37360 Training loss 0.05275725573301315 Validation loss 0.05360834300518036 Accuracy 0.457275390625\n",
      "Iteration 37370 Training loss 0.05192170292139053 Validation loss 0.05398757755756378 Accuracy 0.454833984375\n",
      "Iteration 37380 Training loss 0.0542188324034214 Validation loss 0.0536833293735981 Accuracy 0.4580078125\n",
      "Iteration 37390 Training loss 0.05109916254878044 Validation loss 0.05367007106542587 Accuracy 0.457275390625\n",
      "Iteration 37400 Training loss 0.052269794046878815 Validation loss 0.0534341037273407 Accuracy 0.458984375\n",
      "Iteration 37410 Training loss 0.05282202735543251 Validation loss 0.05388422682881355 Accuracy 0.4560546875\n",
      "Iteration 37420 Training loss 0.05243649333715439 Validation loss 0.053550057113170624 Accuracy 0.458984375\n",
      "Iteration 37430 Training loss 0.053681448101997375 Validation loss 0.05384107679128647 Accuracy 0.45556640625\n",
      "Iteration 37440 Training loss 0.054062411189079285 Validation loss 0.05412337929010391 Accuracy 0.454345703125\n",
      "Iteration 37450 Training loss 0.0523453988134861 Validation loss 0.053724609315395355 Accuracy 0.458251953125\n",
      "Iteration 37460 Training loss 0.053866397589445114 Validation loss 0.05540483072400093 Accuracy 0.4404296875\n",
      "Iteration 37470 Training loss 0.0497673861682415 Validation loss 0.05336293205618858 Accuracy 0.459716796875\n",
      "Iteration 37480 Training loss 0.05205851048231125 Validation loss 0.05344456806778908 Accuracy 0.459228515625\n",
      "Iteration 37490 Training loss 0.050563711673021317 Validation loss 0.0534837543964386 Accuracy 0.458984375\n",
      "Iteration 37500 Training loss 0.04954645410180092 Validation loss 0.053931593894958496 Accuracy 0.45458984375\n",
      "Iteration 37510 Training loss 0.05445190519094467 Validation loss 0.05348644033074379 Accuracy 0.45849609375\n",
      "Iteration 37520 Training loss 0.05297229811549187 Validation loss 0.05358549579977989 Accuracy 0.4580078125\n",
      "Iteration 37530 Training loss 0.04889499023556709 Validation loss 0.05351850017905235 Accuracy 0.4580078125\n",
      "Iteration 37540 Training loss 0.050869353115558624 Validation loss 0.053888268768787384 Accuracy 0.4541015625\n",
      "Iteration 37550 Training loss 0.05222777649760246 Validation loss 0.053492285311222076 Accuracy 0.4580078125\n",
      "Iteration 37560 Training loss 0.05005175247788429 Validation loss 0.053764525800943375 Accuracy 0.456787109375\n",
      "Iteration 37570 Training loss 0.051192983984947205 Validation loss 0.05359402298927307 Accuracy 0.458251953125\n",
      "Iteration 37580 Training loss 0.053126394748687744 Validation loss 0.05351785942912102 Accuracy 0.458740234375\n",
      "Iteration 37590 Training loss 0.047961678355932236 Validation loss 0.05364873260259628 Accuracy 0.457763671875\n",
      "Iteration 37600 Training loss 0.054753683507442474 Validation loss 0.05411963164806366 Accuracy 0.453125\n",
      "Iteration 37610 Training loss 0.051612552255392075 Validation loss 0.05359754338860512 Accuracy 0.4580078125\n",
      "Iteration 37620 Training loss 0.05286971852183342 Validation loss 0.05345391109585762 Accuracy 0.459228515625\n",
      "Iteration 37630 Training loss 0.05242849513888359 Validation loss 0.05379350483417511 Accuracy 0.456787109375\n",
      "Iteration 37640 Training loss 0.049740761518478394 Validation loss 0.053870394825935364 Accuracy 0.454833984375\n",
      "Iteration 37650 Training loss 0.05411328002810478 Validation loss 0.05361233651638031 Accuracy 0.45703125\n",
      "Iteration 37660 Training loss 0.05005381256341934 Validation loss 0.05364123359322548 Accuracy 0.4580078125\n",
      "Iteration 37670 Training loss 0.05039004981517792 Validation loss 0.05410301685333252 Accuracy 0.452880859375\n",
      "Iteration 37680 Training loss 0.052991341799497604 Validation loss 0.05358634144067764 Accuracy 0.45849609375\n",
      "Iteration 37690 Training loss 0.05248693749308586 Validation loss 0.05377170443534851 Accuracy 0.456787109375\n",
      "Iteration 37700 Training loss 0.05369066819548607 Validation loss 0.05351622775197029 Accuracy 0.45849609375\n",
      "Iteration 37710 Training loss 0.051953189074993134 Validation loss 0.05359301716089249 Accuracy 0.45703125\n",
      "Iteration 37720 Training loss 0.05295334756374359 Validation loss 0.05429641157388687 Accuracy 0.449462890625\n",
      "Iteration 37730 Training loss 0.051941417157649994 Validation loss 0.053471364080905914 Accuracy 0.459228515625\n",
      "Iteration 37740 Training loss 0.05154487490653992 Validation loss 0.05357082933187485 Accuracy 0.457275390625\n",
      "Iteration 37750 Training loss 0.05129765346646309 Validation loss 0.053409792482852936 Accuracy 0.459716796875\n",
      "Iteration 37760 Training loss 0.05339353531599045 Validation loss 0.05371446534991264 Accuracy 0.456787109375\n",
      "Iteration 37770 Training loss 0.051145315170288086 Validation loss 0.053539715707302094 Accuracy 0.45849609375\n",
      "Iteration 37780 Training loss 0.05140495300292969 Validation loss 0.05380025878548622 Accuracy 0.45556640625\n",
      "Iteration 37790 Training loss 0.053354665637016296 Validation loss 0.0544627346098423 Accuracy 0.44970703125\n",
      "Iteration 37800 Training loss 0.05542045086622238 Validation loss 0.05355057492852211 Accuracy 0.458740234375\n",
      "Iteration 37810 Training loss 0.05163673684000969 Validation loss 0.05344734340906143 Accuracy 0.45947265625\n",
      "Iteration 37820 Training loss 0.05380344018340111 Validation loss 0.05352500453591347 Accuracy 0.45849609375\n",
      "Iteration 37830 Training loss 0.051394347101449966 Validation loss 0.05349435284733772 Accuracy 0.459228515625\n",
      "Iteration 37840 Training loss 0.04868339002132416 Validation loss 0.05429024249315262 Accuracy 0.451171875\n",
      "Iteration 37850 Training loss 0.05257885530591011 Validation loss 0.053819216787815094 Accuracy 0.45703125\n",
      "Iteration 37860 Training loss 0.0497964471578598 Validation loss 0.053860828280448914 Accuracy 0.45556640625\n",
      "Iteration 37870 Training loss 0.04917627573013306 Validation loss 0.05351581051945686 Accuracy 0.45849609375\n",
      "Iteration 37880 Training loss 0.051869943737983704 Validation loss 0.05372464656829834 Accuracy 0.457275390625\n",
      "Iteration 37890 Training loss 0.050653666257858276 Validation loss 0.053445376455783844 Accuracy 0.459228515625\n",
      "Iteration 37900 Training loss 0.05203443765640259 Validation loss 0.05372735112905502 Accuracy 0.45654296875\n",
      "Iteration 37910 Training loss 0.05425448715686798 Validation loss 0.053810350596904755 Accuracy 0.45556640625\n",
      "Iteration 37920 Training loss 0.05035845562815666 Validation loss 0.053606823086738586 Accuracy 0.45849609375\n",
      "Iteration 37930 Training loss 0.051690541207790375 Validation loss 0.05389244481921196 Accuracy 0.45361328125\n",
      "Iteration 37940 Training loss 0.05357889086008072 Validation loss 0.05350624769926071 Accuracy 0.45849609375\n",
      "Iteration 37950 Training loss 0.053183700889348984 Validation loss 0.053409114480018616 Accuracy 0.459716796875\n",
      "Iteration 37960 Training loss 0.05279695615172386 Validation loss 0.05353919789195061 Accuracy 0.458984375\n",
      "Iteration 37970 Training loss 0.05139223113656044 Validation loss 0.05385085195302963 Accuracy 0.45654296875\n",
      "Iteration 37980 Training loss 0.05407736077904701 Validation loss 0.05345207825303078 Accuracy 0.45849609375\n",
      "Iteration 37990 Training loss 0.056464578956365585 Validation loss 0.05346367135643959 Accuracy 0.458740234375\n",
      "Iteration 38000 Training loss 0.05409775674343109 Validation loss 0.05374620854854584 Accuracy 0.456787109375\n",
      "Iteration 38010 Training loss 0.04659713804721832 Validation loss 0.053681667894124985 Accuracy 0.455810546875\n",
      "Iteration 38020 Training loss 0.052703965455293655 Validation loss 0.05339030176401138 Accuracy 0.458740234375\n",
      "Iteration 38030 Training loss 0.05244610458612442 Validation loss 0.05367065221071243 Accuracy 0.457275390625\n",
      "Iteration 38040 Training loss 0.05143570899963379 Validation loss 0.054007720202207565 Accuracy 0.454345703125\n",
      "Iteration 38050 Training loss 0.05405154451727867 Validation loss 0.0535489059984684 Accuracy 0.45751953125\n",
      "Iteration 38060 Training loss 0.05272849649190903 Validation loss 0.053339749574661255 Accuracy 0.45947265625\n",
      "Iteration 38070 Training loss 0.05369239300489426 Validation loss 0.053938284516334534 Accuracy 0.45458984375\n",
      "Iteration 38080 Training loss 0.047777608036994934 Validation loss 0.05348748713731766 Accuracy 0.458984375\n",
      "Iteration 38090 Training loss 0.05065792798995972 Validation loss 0.053411923348903656 Accuracy 0.459716796875\n",
      "Iteration 38100 Training loss 0.05040903761982918 Validation loss 0.05345812812447548 Accuracy 0.459716796875\n",
      "Iteration 38110 Training loss 0.05545135587453842 Validation loss 0.05348251014947891 Accuracy 0.45849609375\n",
      "Iteration 38120 Training loss 0.04929126426577568 Validation loss 0.05375843495130539 Accuracy 0.45849609375\n",
      "Iteration 38130 Training loss 0.04899777099490166 Validation loss 0.053762275725603104 Accuracy 0.456298828125\n",
      "Iteration 38140 Training loss 0.048680804669857025 Validation loss 0.053693488240242004 Accuracy 0.458251953125\n",
      "Iteration 38150 Training loss 0.05448730289936066 Validation loss 0.053667496889829636 Accuracy 0.45703125\n",
      "Iteration 38160 Training loss 0.05400041490793228 Validation loss 0.05346247926354408 Accuracy 0.459228515625\n",
      "Iteration 38170 Training loss 0.049585286527872086 Validation loss 0.053400877863168716 Accuracy 0.458984375\n",
      "Iteration 38180 Training loss 0.055504463613033295 Validation loss 0.053688641637563705 Accuracy 0.457763671875\n",
      "Iteration 38190 Training loss 0.05064188688993454 Validation loss 0.05351434275507927 Accuracy 0.4580078125\n",
      "Iteration 38200 Training loss 0.048974160104990005 Validation loss 0.05427969992160797 Accuracy 0.449951171875\n",
      "Iteration 38210 Training loss 0.04924299195408821 Validation loss 0.05384008586406708 Accuracy 0.45654296875\n",
      "Iteration 38220 Training loss 0.05354033410549164 Validation loss 0.0534231960773468 Accuracy 0.459228515625\n",
      "Iteration 38230 Training loss 0.05222463607788086 Validation loss 0.05341270938515663 Accuracy 0.460205078125\n",
      "Iteration 38240 Training loss 0.05257318913936615 Validation loss 0.05343370884656906 Accuracy 0.458251953125\n",
      "Iteration 38250 Training loss 0.05312180519104004 Validation loss 0.053773317486047745 Accuracy 0.45654296875\n",
      "Iteration 38260 Training loss 0.05628586933016777 Validation loss 0.0533011257648468 Accuracy 0.458984375\n",
      "Iteration 38270 Training loss 0.05040062591433525 Validation loss 0.05353835970163345 Accuracy 0.459228515625\n",
      "Iteration 38280 Training loss 0.05184223875403404 Validation loss 0.05370432510972023 Accuracy 0.45849609375\n",
      "Iteration 38290 Training loss 0.051238566637039185 Validation loss 0.05337212607264519 Accuracy 0.45947265625\n",
      "Iteration 38300 Training loss 0.05182903632521629 Validation loss 0.05342734605073929 Accuracy 0.458740234375\n",
      "Iteration 38310 Training loss 0.05288190767168999 Validation loss 0.05369236320257187 Accuracy 0.45654296875\n",
      "Iteration 38320 Training loss 0.05196700990200043 Validation loss 0.05338164418935776 Accuracy 0.458984375\n",
      "Iteration 38330 Training loss 0.04988912492990494 Validation loss 0.053706541657447815 Accuracy 0.45751953125\n",
      "Iteration 38340 Training loss 0.05551373213529587 Validation loss 0.05368887633085251 Accuracy 0.45654296875\n",
      "Iteration 38350 Training loss 0.051102619618177414 Validation loss 0.05333374813199043 Accuracy 0.45849609375\n",
      "Iteration 38360 Training loss 0.051205407828092575 Validation loss 0.053428519517183304 Accuracy 0.460693359375\n",
      "Iteration 38370 Training loss 0.050412893295288086 Validation loss 0.05371427163481712 Accuracy 0.457275390625\n",
      "Iteration 38380 Training loss 0.04513511434197426 Validation loss 0.053508151322603226 Accuracy 0.459228515625\n",
      "Iteration 38390 Training loss 0.0518031008541584 Validation loss 0.05379297211766243 Accuracy 0.45654296875\n",
      "Iteration 38400 Training loss 0.05211551859974861 Validation loss 0.053371865302324295 Accuracy 0.459716796875\n",
      "Iteration 38410 Training loss 0.05233610048890114 Validation loss 0.053497157990932465 Accuracy 0.458984375\n",
      "Iteration 38420 Training loss 0.05080335959792137 Validation loss 0.0534626729786396 Accuracy 0.45849609375\n",
      "Iteration 38430 Training loss 0.05278261378407478 Validation loss 0.05356357619166374 Accuracy 0.458984375\n",
      "Iteration 38440 Training loss 0.05232705548405647 Validation loss 0.05348214879631996 Accuracy 0.45947265625\n",
      "Iteration 38450 Training loss 0.049373261630535126 Validation loss 0.05370939150452614 Accuracy 0.457275390625\n",
      "Iteration 38460 Training loss 0.05317506939172745 Validation loss 0.054253291338682175 Accuracy 0.451416015625\n",
      "Iteration 38470 Training loss 0.05053151398897171 Validation loss 0.053693123161792755 Accuracy 0.455810546875\n",
      "Iteration 38480 Training loss 0.05262894183397293 Validation loss 0.05349346995353699 Accuracy 0.45849609375\n",
      "Iteration 38490 Training loss 0.05415117368102074 Validation loss 0.053483668714761734 Accuracy 0.458984375\n",
      "Iteration 38500 Training loss 0.050536710768938065 Validation loss 0.05376141518354416 Accuracy 0.455322265625\n",
      "Iteration 38510 Training loss 0.054972708225250244 Validation loss 0.05353496968746185 Accuracy 0.459228515625\n",
      "Iteration 38520 Training loss 0.05358552932739258 Validation loss 0.053436294198036194 Accuracy 0.458984375\n",
      "Iteration 38530 Training loss 0.05027269572019577 Validation loss 0.0534929558634758 Accuracy 0.459228515625\n",
      "Iteration 38540 Training loss 0.05237101763486862 Validation loss 0.05357442423701286 Accuracy 0.456787109375\n",
      "Iteration 38550 Training loss 0.05200888589024544 Validation loss 0.05359817296266556 Accuracy 0.456298828125\n",
      "Iteration 38560 Training loss 0.05029070004820824 Validation loss 0.05368619039654732 Accuracy 0.455810546875\n",
      "Iteration 38570 Training loss 0.050489313900470734 Validation loss 0.05392569303512573 Accuracy 0.4541015625\n",
      "Iteration 38580 Training loss 0.052348650991916656 Validation loss 0.053523991256952286 Accuracy 0.458740234375\n",
      "Iteration 38590 Training loss 0.05378374084830284 Validation loss 0.05356253683567047 Accuracy 0.458251953125\n",
      "Iteration 38600 Training loss 0.05058693885803223 Validation loss 0.05357931926846504 Accuracy 0.456787109375\n",
      "Iteration 38610 Training loss 0.053044091910123825 Validation loss 0.05381079018115997 Accuracy 0.455810546875\n",
      "Iteration 38620 Training loss 0.0543922483921051 Validation loss 0.053408537060022354 Accuracy 0.458251953125\n",
      "Iteration 38630 Training loss 0.05237099155783653 Validation loss 0.05414659157395363 Accuracy 0.452880859375\n",
      "Iteration 38640 Training loss 0.0528506375849247 Validation loss 0.05341485142707825 Accuracy 0.458984375\n",
      "Iteration 38650 Training loss 0.05368303507566452 Validation loss 0.05379485338926315 Accuracy 0.455810546875\n",
      "Iteration 38660 Training loss 0.052909452468156815 Validation loss 0.05349753797054291 Accuracy 0.4580078125\n",
      "Iteration 38670 Training loss 0.05587977170944214 Validation loss 0.05385333299636841 Accuracy 0.4560546875\n",
      "Iteration 38680 Training loss 0.04992389306426048 Validation loss 0.05345192179083824 Accuracy 0.45849609375\n",
      "Iteration 38690 Training loss 0.05292944237589836 Validation loss 0.05358095467090607 Accuracy 0.4580078125\n",
      "Iteration 38700 Training loss 0.0540778785943985 Validation loss 0.05379428714513779 Accuracy 0.457275390625\n",
      "Iteration 38710 Training loss 0.05421816557645798 Validation loss 0.053459543734788895 Accuracy 0.459716796875\n",
      "Iteration 38720 Training loss 0.05076690763235092 Validation loss 0.0534755177795887 Accuracy 0.459716796875\n",
      "Iteration 38730 Training loss 0.05253293737769127 Validation loss 0.05385487526655197 Accuracy 0.45458984375\n",
      "Iteration 38740 Training loss 0.05698888748884201 Validation loss 0.05356487259268761 Accuracy 0.4580078125\n",
      "Iteration 38750 Training loss 0.05375966802239418 Validation loss 0.053541943430900574 Accuracy 0.45703125\n",
      "Iteration 38760 Training loss 0.05266664922237396 Validation loss 0.05347228795289993 Accuracy 0.45849609375\n",
      "Iteration 38770 Training loss 0.04857209324836731 Validation loss 0.05370404198765755 Accuracy 0.4560546875\n",
      "Iteration 38780 Training loss 0.055112630128860474 Validation loss 0.054176393896341324 Accuracy 0.45166015625\n",
      "Iteration 38790 Training loss 0.05162898823618889 Validation loss 0.053565170615911484 Accuracy 0.458740234375\n",
      "Iteration 38800 Training loss 0.05214841291308403 Validation loss 0.05333230271935463 Accuracy 0.45947265625\n",
      "Iteration 38810 Training loss 0.0511600635945797 Validation loss 0.05349016934633255 Accuracy 0.457763671875\n",
      "Iteration 38820 Training loss 0.049669403582811356 Validation loss 0.05343599617481232 Accuracy 0.458251953125\n",
      "Iteration 38830 Training loss 0.05204353854060173 Validation loss 0.05425311252474785 Accuracy 0.450439453125\n",
      "Iteration 38840 Training loss 0.0543399341404438 Validation loss 0.05377265065908432 Accuracy 0.456787109375\n",
      "Iteration 38850 Training loss 0.05003344640135765 Validation loss 0.05368003621697426 Accuracy 0.456298828125\n",
      "Iteration 38860 Training loss 0.05321192368865013 Validation loss 0.05382801592350006 Accuracy 0.45458984375\n",
      "Iteration 38870 Training loss 0.05030125752091408 Validation loss 0.05357545614242554 Accuracy 0.45703125\n",
      "Iteration 38880 Training loss 0.05216246470808983 Validation loss 0.05366059020161629 Accuracy 0.458251953125\n",
      "Iteration 38890 Training loss 0.04896485432982445 Validation loss 0.05426744371652603 Accuracy 0.4501953125\n",
      "Iteration 38900 Training loss 0.05198728293180466 Validation loss 0.053393974900245667 Accuracy 0.458984375\n",
      "Iteration 38910 Training loss 0.051239777356386185 Validation loss 0.05343584716320038 Accuracy 0.458740234375\n",
      "Iteration 38920 Training loss 0.055806346237659454 Validation loss 0.05369202047586441 Accuracy 0.457275390625\n",
      "Iteration 38930 Training loss 0.05180937051773071 Validation loss 0.053570475429296494 Accuracy 0.458251953125\n",
      "Iteration 38940 Training loss 0.05302182957530022 Validation loss 0.053605545312166214 Accuracy 0.458740234375\n",
      "Iteration 38950 Training loss 0.05491282045841217 Validation loss 0.05367385596036911 Accuracy 0.45751953125\n",
      "Iteration 38960 Training loss 0.048701923340559006 Validation loss 0.05356944724917412 Accuracy 0.45849609375\n",
      "Iteration 38970 Training loss 0.05503875017166138 Validation loss 0.05353114753961563 Accuracy 0.4580078125\n",
      "Iteration 38980 Training loss 0.049814168363809586 Validation loss 0.05347547307610512 Accuracy 0.45947265625\n",
      "Iteration 38990 Training loss 0.05011819303035736 Validation loss 0.053582508116960526 Accuracy 0.458984375\n",
      "Iteration 39000 Training loss 0.05245349183678627 Validation loss 0.05348297581076622 Accuracy 0.4599609375\n",
      "Iteration 39010 Training loss 0.04954589903354645 Validation loss 0.053612034767866135 Accuracy 0.457275390625\n",
      "Iteration 39020 Training loss 0.052837010473012924 Validation loss 0.05350378155708313 Accuracy 0.45849609375\n",
      "Iteration 39030 Training loss 0.05251085013151169 Validation loss 0.05365569144487381 Accuracy 0.457275390625\n",
      "Iteration 39040 Training loss 0.04752834513783455 Validation loss 0.053975336253643036 Accuracy 0.453857421875\n",
      "Iteration 39050 Training loss 0.05035947635769844 Validation loss 0.053775906562805176 Accuracy 0.456298828125\n",
      "Iteration 39060 Training loss 0.054774463176727295 Validation loss 0.05377921089529991 Accuracy 0.456298828125\n",
      "Iteration 39070 Training loss 0.04868870973587036 Validation loss 0.05382797122001648 Accuracy 0.456787109375\n",
      "Iteration 39080 Training loss 0.05244660750031471 Validation loss 0.05357540026307106 Accuracy 0.45849609375\n",
      "Iteration 39090 Training loss 0.05179355666041374 Validation loss 0.05349980294704437 Accuracy 0.458984375\n",
      "Iteration 39100 Training loss 0.05451788380742073 Validation loss 0.053498417139053345 Accuracy 0.45751953125\n",
      "Iteration 39110 Training loss 0.052327580749988556 Validation loss 0.05398038029670715 Accuracy 0.45361328125\n",
      "Iteration 39120 Training loss 0.04795822128653526 Validation loss 0.05352315306663513 Accuracy 0.45849609375\n",
      "Iteration 39130 Training loss 0.05361516401171684 Validation loss 0.05352777615189552 Accuracy 0.45849609375\n",
      "Iteration 39140 Training loss 0.050573840737342834 Validation loss 0.05354180559515953 Accuracy 0.459228515625\n",
      "Iteration 39150 Training loss 0.05306526646018028 Validation loss 0.05346086248755455 Accuracy 0.458984375\n",
      "Iteration 39160 Training loss 0.05062118545174599 Validation loss 0.053757235407829285 Accuracy 0.45654296875\n",
      "Iteration 39170 Training loss 0.051131561398506165 Validation loss 0.053585875779390335 Accuracy 0.4580078125\n",
      "Iteration 39180 Training loss 0.04850080981850624 Validation loss 0.053375065326690674 Accuracy 0.458740234375\n",
      "Iteration 39190 Training loss 0.04936623200774193 Validation loss 0.05362490937113762 Accuracy 0.457275390625\n",
      "Iteration 39200 Training loss 0.05076717957854271 Validation loss 0.05339472368359566 Accuracy 0.458984375\n",
      "Iteration 39210 Training loss 0.05094212293624878 Validation loss 0.05348404124379158 Accuracy 0.45751953125\n",
      "Iteration 39220 Training loss 0.05197906494140625 Validation loss 0.05335788056254387 Accuracy 0.458984375\n",
      "Iteration 39230 Training loss 0.053959254175424576 Validation loss 0.05354718491435051 Accuracy 0.457763671875\n",
      "Iteration 39240 Training loss 0.052183184772729874 Validation loss 0.053376972675323486 Accuracy 0.4599609375\n",
      "Iteration 39250 Training loss 0.05018431693315506 Validation loss 0.053594157099723816 Accuracy 0.458984375\n",
      "Iteration 39260 Training loss 0.05203138291835785 Validation loss 0.053433503955602646 Accuracy 0.458740234375\n",
      "Iteration 39270 Training loss 0.05702783167362213 Validation loss 0.05342091619968414 Accuracy 0.459228515625\n",
      "Iteration 39280 Training loss 0.05070754885673523 Validation loss 0.05385208874940872 Accuracy 0.45654296875\n",
      "Iteration 39290 Training loss 0.04938748478889465 Validation loss 0.05344783514738083 Accuracy 0.458740234375\n",
      "Iteration 39300 Training loss 0.049913398921489716 Validation loss 0.05338321626186371 Accuracy 0.459228515625\n",
      "Iteration 39310 Training loss 0.05145817995071411 Validation loss 0.054276224225759506 Accuracy 0.449951171875\n",
      "Iteration 39320 Training loss 0.05328785628080368 Validation loss 0.053318899124860764 Accuracy 0.460205078125\n",
      "Iteration 39330 Training loss 0.053342629224061966 Validation loss 0.05366579070687294 Accuracy 0.456787109375\n",
      "Iteration 39340 Training loss 0.05415218695998192 Validation loss 0.054037440568208694 Accuracy 0.453857421875\n",
      "Iteration 39350 Training loss 0.05254216119647026 Validation loss 0.05349114537239075 Accuracy 0.4580078125\n",
      "Iteration 39360 Training loss 0.05005791783332825 Validation loss 0.0540127232670784 Accuracy 0.453857421875\n",
      "Iteration 39370 Training loss 0.050987597554922104 Validation loss 0.05406226962804794 Accuracy 0.4541015625\n",
      "Iteration 39380 Training loss 0.05265490710735321 Validation loss 0.05334161967039108 Accuracy 0.45947265625\n",
      "Iteration 39390 Training loss 0.052920348942279816 Validation loss 0.053537216037511826 Accuracy 0.457275390625\n",
      "Iteration 39400 Training loss 0.05469139665365219 Validation loss 0.053408171981573105 Accuracy 0.459228515625\n",
      "Iteration 39410 Training loss 0.049698323011398315 Validation loss 0.053699616342782974 Accuracy 0.45556640625\n",
      "Iteration 39420 Training loss 0.05209275335073471 Validation loss 0.05342138931155205 Accuracy 0.458740234375\n",
      "Iteration 39430 Training loss 0.051525816321372986 Validation loss 0.0535469613969326 Accuracy 0.457275390625\n",
      "Iteration 39440 Training loss 0.05061456188559532 Validation loss 0.053341396152973175 Accuracy 0.45947265625\n",
      "Iteration 39450 Training loss 0.04971637949347496 Validation loss 0.05338221415877342 Accuracy 0.459716796875\n",
      "Iteration 39460 Training loss 0.05296853929758072 Validation loss 0.05344567075371742 Accuracy 0.45849609375\n",
      "Iteration 39470 Training loss 0.05378507077693939 Validation loss 0.05335080996155739 Accuracy 0.458984375\n",
      "Iteration 39480 Training loss 0.05252698063850403 Validation loss 0.05366940796375275 Accuracy 0.4580078125\n",
      "Iteration 39490 Training loss 0.050168294459581375 Validation loss 0.053372859954833984 Accuracy 0.458740234375\n",
      "Iteration 39500 Training loss 0.049752674996852875 Validation loss 0.053418055176734924 Accuracy 0.458740234375\n",
      "Iteration 39510 Training loss 0.050954487174749374 Validation loss 0.053543251007795334 Accuracy 0.456787109375\n",
      "Iteration 39520 Training loss 0.05053643882274628 Validation loss 0.05345707759261131 Accuracy 0.458740234375\n",
      "Iteration 39530 Training loss 0.048270680010318756 Validation loss 0.053454745560884476 Accuracy 0.45751953125\n",
      "Iteration 39540 Training loss 0.05447598546743393 Validation loss 0.05360542982816696 Accuracy 0.45703125\n",
      "Iteration 39550 Training loss 0.050439126789569855 Validation loss 0.05327856168150902 Accuracy 0.45947265625\n",
      "Iteration 39560 Training loss 0.04993556812405586 Validation loss 0.0536765530705452 Accuracy 0.45654296875\n",
      "Iteration 39570 Training loss 0.05271649733185768 Validation loss 0.05354378744959831 Accuracy 0.4580078125\n",
      "Iteration 39580 Training loss 0.050835784524679184 Validation loss 0.053687870502471924 Accuracy 0.45703125\n",
      "Iteration 39590 Training loss 0.05122573301196098 Validation loss 0.05351204052567482 Accuracy 0.459228515625\n",
      "Iteration 39600 Training loss 0.054708994925022125 Validation loss 0.05369186773896217 Accuracy 0.455322265625\n",
      "Iteration 39610 Training loss 0.04960740730166435 Validation loss 0.05342080071568489 Accuracy 0.4580078125\n",
      "Iteration 39620 Training loss 0.05090492591261864 Validation loss 0.05355597659945488 Accuracy 0.45751953125\n",
      "Iteration 39630 Training loss 0.05205598846077919 Validation loss 0.0535413958132267 Accuracy 0.4580078125\n",
      "Iteration 39640 Training loss 0.05046826973557472 Validation loss 0.05339064821600914 Accuracy 0.458740234375\n",
      "Iteration 39650 Training loss 0.05238470435142517 Validation loss 0.053596992045640945 Accuracy 0.458740234375\n",
      "Iteration 39660 Training loss 0.05024436488747597 Validation loss 0.0535944364964962 Accuracy 0.456298828125\n",
      "Iteration 39670 Training loss 0.055217377841472626 Validation loss 0.05345834791660309 Accuracy 0.459228515625\n",
      "Iteration 39680 Training loss 0.05340254679322243 Validation loss 0.05381714552640915 Accuracy 0.457275390625\n",
      "Iteration 39690 Training loss 0.05037476494908333 Validation loss 0.053675126284360886 Accuracy 0.45751953125\n",
      "Iteration 39700 Training loss 0.05318787693977356 Validation loss 0.05357800051569939 Accuracy 0.45849609375\n",
      "Iteration 39710 Training loss 0.05062761530280113 Validation loss 0.05355795845389366 Accuracy 0.45849609375\n",
      "Iteration 39720 Training loss 0.04853794723749161 Validation loss 0.05340540036559105 Accuracy 0.45849609375\n",
      "Iteration 39730 Training loss 0.051168300211429596 Validation loss 0.05340088903903961 Accuracy 0.458740234375\n",
      "Iteration 39740 Training loss 0.053305692970752716 Validation loss 0.05362872779369354 Accuracy 0.45703125\n",
      "Iteration 39750 Training loss 0.051733989268541336 Validation loss 0.0534011647105217 Accuracy 0.458984375\n",
      "Iteration 39760 Training loss 0.0506066232919693 Validation loss 0.05335357412695885 Accuracy 0.458984375\n",
      "Iteration 39770 Training loss 0.050859566777944565 Validation loss 0.053574588149785995 Accuracy 0.457763671875\n",
      "Iteration 39780 Training loss 0.05155676603317261 Validation loss 0.05363436043262482 Accuracy 0.456787109375\n",
      "Iteration 39790 Training loss 0.05068133398890495 Validation loss 0.053353339433670044 Accuracy 0.459228515625\n",
      "Iteration 39800 Training loss 0.04860137775540352 Validation loss 0.053331393748521805 Accuracy 0.45947265625\n",
      "Iteration 39810 Training loss 0.05340182036161423 Validation loss 0.05357830598950386 Accuracy 0.45751953125\n",
      "Iteration 39820 Training loss 0.04871407151222229 Validation loss 0.05344967544078827 Accuracy 0.45849609375\n",
      "Iteration 39830 Training loss 0.05395347997546196 Validation loss 0.053416911512613297 Accuracy 0.458984375\n",
      "Iteration 39840 Training loss 0.05247993394732475 Validation loss 0.053417861461639404 Accuracy 0.458984375\n",
      "Iteration 39850 Training loss 0.04998641833662987 Validation loss 0.05340103805065155 Accuracy 0.459228515625\n",
      "Iteration 39860 Training loss 0.053092099726200104 Validation loss 0.053331159055233 Accuracy 0.45947265625\n",
      "Iteration 39870 Training loss 0.05081804469227791 Validation loss 0.05354205518960953 Accuracy 0.458740234375\n",
      "Iteration 39880 Training loss 0.05169476941227913 Validation loss 0.05365731939673424 Accuracy 0.45849609375\n",
      "Iteration 39890 Training loss 0.0535149946808815 Validation loss 0.05373099446296692 Accuracy 0.45703125\n",
      "Iteration 39900 Training loss 0.051374662667512894 Validation loss 0.05350947007536888 Accuracy 0.4580078125\n",
      "Iteration 39910 Training loss 0.05506730079650879 Validation loss 0.05364719405770302 Accuracy 0.455810546875\n",
      "Iteration 39920 Training loss 0.050421092659235 Validation loss 0.05364599823951721 Accuracy 0.455810546875\n",
      "Iteration 39930 Training loss 0.052518222481012344 Validation loss 0.053543347865343094 Accuracy 0.456787109375\n",
      "Iteration 39940 Training loss 0.049924738705158234 Validation loss 0.053442005068063736 Accuracy 0.458740234375\n",
      "Iteration 39950 Training loss 0.0531141459941864 Validation loss 0.05347667261958122 Accuracy 0.458984375\n",
      "Iteration 39960 Training loss 0.052604980766773224 Validation loss 0.05343401059508324 Accuracy 0.458251953125\n",
      "Iteration 39970 Training loss 0.0521855503320694 Validation loss 0.05346933379769325 Accuracy 0.457763671875\n",
      "Iteration 39980 Training loss 0.04996061325073242 Validation loss 0.05328642576932907 Accuracy 0.459716796875\n",
      "Iteration 39990 Training loss 0.05372711271047592 Validation loss 0.0536331906914711 Accuracy 0.45751953125\n",
      "Iteration 40000 Training loss 0.05209224298596382 Validation loss 0.05348699167370796 Accuracy 0.45703125\n",
      "Iteration 40010 Training loss 0.04937354102730751 Validation loss 0.053547874093055725 Accuracy 0.456298828125\n",
      "Iteration 40020 Training loss 0.0544121153652668 Validation loss 0.053392402827739716 Accuracy 0.459228515625\n",
      "Iteration 40030 Training loss 0.05107209086418152 Validation loss 0.053399764001369476 Accuracy 0.45849609375\n",
      "Iteration 40040 Training loss 0.052851226180791855 Validation loss 0.05324244126677513 Accuracy 0.4599609375\n",
      "Iteration 40050 Training loss 0.05358225479722023 Validation loss 0.05331014469265938 Accuracy 0.459228515625\n",
      "Iteration 40060 Training loss 0.051960479468107224 Validation loss 0.053320351988077164 Accuracy 0.45947265625\n",
      "Iteration 40070 Training loss 0.05150676518678665 Validation loss 0.05334199219942093 Accuracy 0.459716796875\n",
      "Iteration 40080 Training loss 0.053746286779642105 Validation loss 0.0545182004570961 Accuracy 0.447998046875\n",
      "Iteration 40090 Training loss 0.05339207872748375 Validation loss 0.053750090301036835 Accuracy 0.45654296875\n",
      "Iteration 40100 Training loss 0.05181097239255905 Validation loss 0.053438104689121246 Accuracy 0.45849609375\n",
      "Iteration 40110 Training loss 0.05213009566068649 Validation loss 0.0535992756485939 Accuracy 0.45654296875\n",
      "Iteration 40120 Training loss 0.050627391785383224 Validation loss 0.053292348980903625 Accuracy 0.460205078125\n",
      "Iteration 40130 Training loss 0.05362536385655403 Validation loss 0.05331100896000862 Accuracy 0.459716796875\n",
      "Iteration 40140 Training loss 0.04986686259508133 Validation loss 0.05325762555003166 Accuracy 0.4599609375\n",
      "Iteration 40150 Training loss 0.05048731341958046 Validation loss 0.05334686487913132 Accuracy 0.458984375\n",
      "Iteration 40160 Training loss 0.0527040995657444 Validation loss 0.05325081944465637 Accuracy 0.460205078125\n",
      "Iteration 40170 Training loss 0.05526144802570343 Validation loss 0.05382334813475609 Accuracy 0.455322265625\n",
      "Iteration 40180 Training loss 0.05076814815402031 Validation loss 0.053730905055999756 Accuracy 0.457275390625\n",
      "Iteration 40190 Training loss 0.05207120254635811 Validation loss 0.0536494143307209 Accuracy 0.456787109375\n",
      "Iteration 40200 Training loss 0.05007543787360191 Validation loss 0.053811755031347275 Accuracy 0.45556640625\n",
      "Iteration 40210 Training loss 0.049380045384168625 Validation loss 0.05332125350832939 Accuracy 0.459716796875\n",
      "Iteration 40220 Training loss 0.05329306051135063 Validation loss 0.05344120040535927 Accuracy 0.45751953125\n",
      "Iteration 40230 Training loss 0.05106025189161301 Validation loss 0.05345877259969711 Accuracy 0.459228515625\n",
      "Iteration 40240 Training loss 0.05156606063246727 Validation loss 0.05330674350261688 Accuracy 0.4599609375\n",
      "Iteration 40250 Training loss 0.05361396074295044 Validation loss 0.05355987697839737 Accuracy 0.458251953125\n",
      "Iteration 40260 Training loss 0.05080857127904892 Validation loss 0.05323037877678871 Accuracy 0.4599609375\n",
      "Iteration 40270 Training loss 0.05141480639576912 Validation loss 0.05325774848461151 Accuracy 0.4609375\n",
      "Iteration 40280 Training loss 0.05403108522295952 Validation loss 0.05348633974790573 Accuracy 0.45703125\n",
      "Iteration 40290 Training loss 0.053338658064603806 Validation loss 0.0535149984061718 Accuracy 0.457275390625\n",
      "Iteration 40300 Training loss 0.050910111516714096 Validation loss 0.05351578816771507 Accuracy 0.45849609375\n",
      "Iteration 40310 Training loss 0.05432608351111412 Validation loss 0.05354355275630951 Accuracy 0.45751953125\n",
      "Iteration 40320 Training loss 0.051325224339962006 Validation loss 0.0541231669485569 Accuracy 0.453857421875\n",
      "Iteration 40330 Training loss 0.0496952086687088 Validation loss 0.053500402718782425 Accuracy 0.45849609375\n",
      "Iteration 40340 Training loss 0.05237589403986931 Validation loss 0.05332767963409424 Accuracy 0.459716796875\n",
      "Iteration 40350 Training loss 0.05365782231092453 Validation loss 0.0534052848815918 Accuracy 0.459228515625\n",
      "Iteration 40360 Training loss 0.05114565044641495 Validation loss 0.0535668283700943 Accuracy 0.456787109375\n",
      "Iteration 40370 Training loss 0.05119098722934723 Validation loss 0.05353110283613205 Accuracy 0.4580078125\n",
      "Iteration 40380 Training loss 0.05281129851937294 Validation loss 0.053723130375146866 Accuracy 0.4560546875\n",
      "Iteration 40390 Training loss 0.053332842886447906 Validation loss 0.0534246489405632 Accuracy 0.459716796875\n",
      "Iteration 40400 Training loss 0.052036888897418976 Validation loss 0.053497496992349625 Accuracy 0.45849609375\n",
      "Iteration 40410 Training loss 0.05702761933207512 Validation loss 0.053557127714157104 Accuracy 0.458984375\n",
      "Iteration 40420 Training loss 0.049769409000873566 Validation loss 0.05329924821853638 Accuracy 0.458984375\n",
      "Iteration 40430 Training loss 0.05166112259030342 Validation loss 0.05367882549762726 Accuracy 0.45703125\n",
      "Iteration 40440 Training loss 0.0495220348238945 Validation loss 0.053508684039115906 Accuracy 0.458251953125\n",
      "Iteration 40450 Training loss 0.04954107478260994 Validation loss 0.05327899381518364 Accuracy 0.460205078125\n",
      "Iteration 40460 Training loss 0.05071917176246643 Validation loss 0.05372094735503197 Accuracy 0.4560546875\n",
      "Iteration 40470 Training loss 0.05044860765337944 Validation loss 0.05332830920815468 Accuracy 0.4599609375\n",
      "Iteration 40480 Training loss 0.04828351363539696 Validation loss 0.05333497002720833 Accuracy 0.459716796875\n",
      "Iteration 40490 Training loss 0.05174177139997482 Validation loss 0.05379645526409149 Accuracy 0.45703125\n",
      "Iteration 40500 Training loss 0.051590994000434875 Validation loss 0.05326587334275246 Accuracy 0.4599609375\n",
      "Iteration 40510 Training loss 0.053285226225852966 Validation loss 0.053244058042764664 Accuracy 0.4599609375\n",
      "Iteration 40520 Training loss 0.05143743380904198 Validation loss 0.053497135639190674 Accuracy 0.459716796875\n",
      "Iteration 40530 Training loss 0.050648730248212814 Validation loss 0.053777292370796204 Accuracy 0.4560546875\n",
      "Iteration 40540 Training loss 0.05289354547858238 Validation loss 0.05370466783642769 Accuracy 0.457275390625\n",
      "Iteration 40550 Training loss 0.05309031531214714 Validation loss 0.05359943211078644 Accuracy 0.458984375\n",
      "Iteration 40560 Training loss 0.05225289240479469 Validation loss 0.053267598152160645 Accuracy 0.4599609375\n",
      "Iteration 40570 Training loss 0.05185787007212639 Validation loss 0.05396093428134918 Accuracy 0.45458984375\n",
      "Iteration 40580 Training loss 0.051062051206827164 Validation loss 0.053292278200387955 Accuracy 0.459228515625\n",
      "Iteration 40590 Training loss 0.05371961370110512 Validation loss 0.05354829132556915 Accuracy 0.457763671875\n",
      "Iteration 40600 Training loss 0.0491534024477005 Validation loss 0.05347956344485283 Accuracy 0.45947265625\n",
      "Iteration 40610 Training loss 0.051029857248067856 Validation loss 0.053241606801748276 Accuracy 0.460693359375\n",
      "Iteration 40620 Training loss 0.05406644567847252 Validation loss 0.0535782128572464 Accuracy 0.45849609375\n",
      "Iteration 40630 Training loss 0.05219608172774315 Validation loss 0.05375305563211441 Accuracy 0.455322265625\n",
      "Iteration 40640 Training loss 0.050622761249542236 Validation loss 0.053817011415958405 Accuracy 0.45458984375\n",
      "Iteration 40650 Training loss 0.05138520151376724 Validation loss 0.05348199978470802 Accuracy 0.45703125\n",
      "Iteration 40660 Training loss 0.04990449920296669 Validation loss 0.05409099534153938 Accuracy 0.453857421875\n",
      "Iteration 40670 Training loss 0.05109583958983421 Validation loss 0.05347301438450813 Accuracy 0.45947265625\n",
      "Iteration 40680 Training loss 0.05478496104478836 Validation loss 0.05338079854846001 Accuracy 0.45947265625\n",
      "Iteration 40690 Training loss 0.05128313973546028 Validation loss 0.0535590723156929 Accuracy 0.457763671875\n",
      "Iteration 40700 Training loss 0.05432124808430672 Validation loss 0.05342763662338257 Accuracy 0.458984375\n",
      "Iteration 40710 Training loss 0.0543833002448082 Validation loss 0.05318315327167511 Accuracy 0.460693359375\n",
      "Iteration 40720 Training loss 0.053347330540418625 Validation loss 0.053735774010419846 Accuracy 0.456787109375\n",
      "Iteration 40730 Training loss 0.05172322690486908 Validation loss 0.05338076874613762 Accuracy 0.4599609375\n",
      "Iteration 40740 Training loss 0.05154622346162796 Validation loss 0.05349954962730408 Accuracy 0.458740234375\n",
      "Iteration 40750 Training loss 0.048817239701747894 Validation loss 0.05333529785275459 Accuracy 0.459716796875\n",
      "Iteration 40760 Training loss 0.05244763195514679 Validation loss 0.05370161309838295 Accuracy 0.455322265625\n",
      "Iteration 40770 Training loss 0.051269855350255966 Validation loss 0.05344640463590622 Accuracy 0.458740234375\n",
      "Iteration 40780 Training loss 0.05297395959496498 Validation loss 0.053401537239551544 Accuracy 0.458251953125\n",
      "Iteration 40790 Training loss 0.05221477150917053 Validation loss 0.053700245916843414 Accuracy 0.45654296875\n",
      "Iteration 40800 Training loss 0.05044990032911301 Validation loss 0.05384247750043869 Accuracy 0.455810546875\n",
      "Iteration 40810 Training loss 0.052132222801446915 Validation loss 0.05336008593440056 Accuracy 0.458740234375\n",
      "Iteration 40820 Training loss 0.050435975193977356 Validation loss 0.053351759910583496 Accuracy 0.460205078125\n",
      "Iteration 40830 Training loss 0.05327724292874336 Validation loss 0.05340862646698952 Accuracy 0.459228515625\n",
      "Iteration 40840 Training loss 0.05437281355261803 Validation loss 0.053403161466121674 Accuracy 0.458984375\n",
      "Iteration 40850 Training loss 0.05311264470219612 Validation loss 0.05387260019779205 Accuracy 0.45458984375\n",
      "Iteration 40860 Training loss 0.05359949544072151 Validation loss 0.053359221667051315 Accuracy 0.459716796875\n",
      "Iteration 40870 Training loss 0.053434599190950394 Validation loss 0.05345204100012779 Accuracy 0.458984375\n",
      "Iteration 40880 Training loss 0.05065770819783211 Validation loss 0.05338519439101219 Accuracy 0.45947265625\n",
      "Iteration 40890 Training loss 0.054770518094301224 Validation loss 0.053649626672267914 Accuracy 0.456787109375\n",
      "Iteration 40900 Training loss 0.051528893411159515 Validation loss 0.053371913731098175 Accuracy 0.458984375\n",
      "Iteration 40910 Training loss 0.05328335985541344 Validation loss 0.05332950875163078 Accuracy 0.4599609375\n",
      "Iteration 40920 Training loss 0.052243225276470184 Validation loss 0.05330728366971016 Accuracy 0.459716796875\n",
      "Iteration 40930 Training loss 0.05133117735385895 Validation loss 0.053508806973695755 Accuracy 0.45849609375\n",
      "Iteration 40940 Training loss 0.05134543776512146 Validation loss 0.05344002693891525 Accuracy 0.458984375\n",
      "Iteration 40950 Training loss 0.04988419637084007 Validation loss 0.05339875444769859 Accuracy 0.459228515625\n",
      "Iteration 40960 Training loss 0.051387134939432144 Validation loss 0.054106324911117554 Accuracy 0.45458984375\n",
      "Iteration 40970 Training loss 0.05321355164051056 Validation loss 0.05342833697795868 Accuracy 0.45849609375\n",
      "Iteration 40980 Training loss 0.04900190234184265 Validation loss 0.05339823290705681 Accuracy 0.458984375\n",
      "Iteration 40990 Training loss 0.05182153359055519 Validation loss 0.05324608087539673 Accuracy 0.46044921875\n",
      "Iteration 41000 Training loss 0.05272766947746277 Validation loss 0.05316222086548805 Accuracy 0.460693359375\n",
      "Iteration 41010 Training loss 0.04993652179837227 Validation loss 0.05355345457792282 Accuracy 0.457763671875\n",
      "Iteration 41020 Training loss 0.0519220270216465 Validation loss 0.053322918713092804 Accuracy 0.460205078125\n",
      "Iteration 41030 Training loss 0.05150758847594261 Validation loss 0.05316855385899544 Accuracy 0.4599609375\n",
      "Iteration 41040 Training loss 0.053292326629161835 Validation loss 0.05384979397058487 Accuracy 0.456298828125\n",
      "Iteration 41050 Training loss 0.05690339580178261 Validation loss 0.05369006097316742 Accuracy 0.4560546875\n",
      "Iteration 41060 Training loss 0.05561870336532593 Validation loss 0.05325712263584137 Accuracy 0.461181640625\n",
      "Iteration 41070 Training loss 0.05418696627020836 Validation loss 0.05340377241373062 Accuracy 0.458984375\n",
      "Iteration 41080 Training loss 0.049390505999326706 Validation loss 0.05354039743542671 Accuracy 0.458740234375\n",
      "Iteration 41090 Training loss 0.05340133607387543 Validation loss 0.0534970797598362 Accuracy 0.458984375\n",
      "Iteration 41100 Training loss 0.05328706279397011 Validation loss 0.0538334958255291 Accuracy 0.455322265625\n",
      "Iteration 41110 Training loss 0.0495779812335968 Validation loss 0.05389568209648132 Accuracy 0.45263671875\n",
      "Iteration 41120 Training loss 0.05173692852258682 Validation loss 0.05338076874613762 Accuracy 0.4599609375\n",
      "Iteration 41130 Training loss 0.05131520703434944 Validation loss 0.053411051630973816 Accuracy 0.45947265625\n",
      "Iteration 41140 Training loss 0.05225926637649536 Validation loss 0.05358368158340454 Accuracy 0.45654296875\n",
      "Iteration 41150 Training loss 0.051357101649045944 Validation loss 0.05365259200334549 Accuracy 0.456298828125\n",
      "Iteration 41160 Training loss 0.05106351152062416 Validation loss 0.053524427115917206 Accuracy 0.45849609375\n",
      "Iteration 41170 Training loss 0.05010431259870529 Validation loss 0.05362704023718834 Accuracy 0.456787109375\n",
      "Iteration 41180 Training loss 0.04816744476556778 Validation loss 0.053385138511657715 Accuracy 0.45947265625\n",
      "Iteration 41190 Training loss 0.05345115438103676 Validation loss 0.05335583910346031 Accuracy 0.460205078125\n",
      "Iteration 41200 Training loss 0.04759199917316437 Validation loss 0.05340772494673729 Accuracy 0.459228515625\n",
      "Iteration 41210 Training loss 0.05457156151533127 Validation loss 0.05338121950626373 Accuracy 0.45947265625\n",
      "Iteration 41220 Training loss 0.05645669624209404 Validation loss 0.05354743078351021 Accuracy 0.456787109375\n",
      "Iteration 41230 Training loss 0.051992226392030716 Validation loss 0.053338173776865005 Accuracy 0.460205078125\n",
      "Iteration 41240 Training loss 0.04923044890165329 Validation loss 0.05334608256816864 Accuracy 0.4599609375\n",
      "Iteration 41250 Training loss 0.052675556391477585 Validation loss 0.05368397384881973 Accuracy 0.45703125\n",
      "Iteration 41260 Training loss 0.048880260437726974 Validation loss 0.053303711116313934 Accuracy 0.4599609375\n",
      "Iteration 41270 Training loss 0.0536758117377758 Validation loss 0.05333822965621948 Accuracy 0.458984375\n",
      "Iteration 41280 Training loss 0.050144437700510025 Validation loss 0.053193021565675735 Accuracy 0.460205078125\n",
      "Iteration 41290 Training loss 0.05195377394556999 Validation loss 0.0538434162735939 Accuracy 0.456298828125\n",
      "Iteration 41300 Training loss 0.051075734198093414 Validation loss 0.05339641496539116 Accuracy 0.459228515625\n",
      "Iteration 41310 Training loss 0.053994033485651016 Validation loss 0.05326702073216438 Accuracy 0.459716796875\n",
      "Iteration 41320 Training loss 0.04806365445256233 Validation loss 0.05314004793763161 Accuracy 0.461669921875\n",
      "Iteration 41330 Training loss 0.05422617495059967 Validation loss 0.05334166809916496 Accuracy 0.460693359375\n",
      "Iteration 41340 Training loss 0.05286126583814621 Validation loss 0.05387450382113457 Accuracy 0.453857421875\n",
      "Iteration 41350 Training loss 0.050819527357816696 Validation loss 0.05345311388373375 Accuracy 0.45947265625\n",
      "Iteration 41360 Training loss 0.04986593499779701 Validation loss 0.053638942539691925 Accuracy 0.458251953125\n",
      "Iteration 41370 Training loss 0.051781971007585526 Validation loss 0.053586266934871674 Accuracy 0.458984375\n",
      "Iteration 41380 Training loss 0.05534609779715538 Validation loss 0.05341029167175293 Accuracy 0.459716796875\n",
      "Iteration 41390 Training loss 0.053357481956481934 Validation loss 0.05347553640604019 Accuracy 0.4580078125\n",
      "Iteration 41400 Training loss 0.054058268666267395 Validation loss 0.0532744824886322 Accuracy 0.4609375\n",
      "Iteration 41410 Training loss 0.05202281475067139 Validation loss 0.053607240319252014 Accuracy 0.455810546875\n",
      "Iteration 41420 Training loss 0.05109268054366112 Validation loss 0.05320749431848526 Accuracy 0.460693359375\n",
      "Iteration 41430 Training loss 0.056071873754262924 Validation loss 0.05364617705345154 Accuracy 0.455810546875\n",
      "Iteration 41440 Training loss 0.05226251855492592 Validation loss 0.053840383887290955 Accuracy 0.455078125\n",
      "Iteration 41450 Training loss 0.05112522840499878 Validation loss 0.053556401282548904 Accuracy 0.457275390625\n",
      "Iteration 41460 Training loss 0.05066412687301636 Validation loss 0.05332562327384949 Accuracy 0.460205078125\n",
      "Iteration 41470 Training loss 0.05210365727543831 Validation loss 0.0533674880862236 Accuracy 0.460205078125\n",
      "Iteration 41480 Training loss 0.04954542592167854 Validation loss 0.05353643745183945 Accuracy 0.457763671875\n",
      "Iteration 41490 Training loss 0.050373345613479614 Validation loss 0.05324918404221535 Accuracy 0.460693359375\n",
      "Iteration 41500 Training loss 0.05200614407658577 Validation loss 0.053455546498298645 Accuracy 0.458740234375\n",
      "Iteration 41510 Training loss 0.04802780598402023 Validation loss 0.05340220034122467 Accuracy 0.458984375\n",
      "Iteration 41520 Training loss 0.055040497332811356 Validation loss 0.05339325964450836 Accuracy 0.459228515625\n",
      "Iteration 41530 Training loss 0.05337992310523987 Validation loss 0.0533178448677063 Accuracy 0.4599609375\n",
      "Iteration 41540 Training loss 0.045971743762493134 Validation loss 0.05326146259903908 Accuracy 0.460205078125\n",
      "Iteration 41550 Training loss 0.05241069570183754 Validation loss 0.0538104772567749 Accuracy 0.45654296875\n",
      "Iteration 41560 Training loss 0.049061112105846405 Validation loss 0.05326620116829872 Accuracy 0.4599609375\n",
      "Iteration 41570 Training loss 0.04847440868616104 Validation loss 0.053399134427309036 Accuracy 0.458740234375\n",
      "Iteration 41580 Training loss 0.05005931109189987 Validation loss 0.05437289550900459 Accuracy 0.44970703125\n",
      "Iteration 41590 Training loss 0.05181536823511124 Validation loss 0.0536094605922699 Accuracy 0.45751953125\n",
      "Iteration 41600 Training loss 0.05121084675192833 Validation loss 0.05333811417222023 Accuracy 0.459716796875\n",
      "Iteration 41610 Training loss 0.051394231617450714 Validation loss 0.05339369177818298 Accuracy 0.459228515625\n",
      "Iteration 41620 Training loss 0.052394621074199677 Validation loss 0.05381733924150467 Accuracy 0.455078125\n",
      "Iteration 41630 Training loss 0.049891699105501175 Validation loss 0.053542524576187134 Accuracy 0.45849609375\n",
      "Iteration 41640 Training loss 0.05120934918522835 Validation loss 0.053351759910583496 Accuracy 0.45947265625\n",
      "Iteration 41650 Training loss 0.048552703112363815 Validation loss 0.053933173418045044 Accuracy 0.453125\n",
      "Iteration 41660 Training loss 0.05346493422985077 Validation loss 0.05328293889760971 Accuracy 0.459716796875\n",
      "Iteration 41670 Training loss 0.048102233558893204 Validation loss 0.05368172749876976 Accuracy 0.456787109375\n",
      "Iteration 41680 Training loss 0.051073141396045685 Validation loss 0.053267430514097214 Accuracy 0.459716796875\n",
      "Iteration 41690 Training loss 0.050902109593153 Validation loss 0.053513303399086 Accuracy 0.458984375\n",
      "Iteration 41700 Training loss 0.050736501812934875 Validation loss 0.05335783213376999 Accuracy 0.46044921875\n",
      "Iteration 41710 Training loss 0.05248698964715004 Validation loss 0.05329049751162529 Accuracy 0.45947265625\n",
      "Iteration 41720 Training loss 0.053124308586120605 Validation loss 0.05337240546941757 Accuracy 0.459716796875\n",
      "Iteration 41730 Training loss 0.05194074288010597 Validation loss 0.05392250046133995 Accuracy 0.45458984375\n",
      "Iteration 41740 Training loss 0.05489320680499077 Validation loss 0.05357927456498146 Accuracy 0.457763671875\n",
      "Iteration 41750 Training loss 0.048923343420028687 Validation loss 0.053222235292196274 Accuracy 0.460205078125\n",
      "Iteration 41760 Training loss 0.04966042935848236 Validation loss 0.05322690308094025 Accuracy 0.461181640625\n",
      "Iteration 41770 Training loss 0.0531153604388237 Validation loss 0.053327526897192 Accuracy 0.45947265625\n",
      "Iteration 41780 Training loss 0.04929438605904579 Validation loss 0.05339037999510765 Accuracy 0.4599609375\n",
      "Iteration 41790 Training loss 0.04582015052437782 Validation loss 0.05401165038347244 Accuracy 0.45263671875\n",
      "Iteration 41800 Training loss 0.05519205704331398 Validation loss 0.05358679220080376 Accuracy 0.4580078125\n",
      "Iteration 41810 Training loss 0.0516691729426384 Validation loss 0.05336889624595642 Accuracy 0.459716796875\n",
      "Iteration 41820 Training loss 0.05192181468009949 Validation loss 0.05403662472963333 Accuracy 0.453369140625\n",
      "Iteration 41830 Training loss 0.05181725695729256 Validation loss 0.053391218185424805 Accuracy 0.460205078125\n",
      "Iteration 41840 Training loss 0.05251380801200867 Validation loss 0.05360320582985878 Accuracy 0.457763671875\n",
      "Iteration 41850 Training loss 0.05288010463118553 Validation loss 0.053388506174087524 Accuracy 0.45947265625\n",
      "Iteration 41860 Training loss 0.05284663289785385 Validation loss 0.05338079482316971 Accuracy 0.45947265625\n",
      "Iteration 41870 Training loss 0.05346783250570297 Validation loss 0.05330822244286537 Accuracy 0.459716796875\n",
      "Iteration 41880 Training loss 0.047774769365787506 Validation loss 0.05322937294840813 Accuracy 0.4609375\n",
      "Iteration 41890 Training loss 0.049828093498945236 Validation loss 0.05423099547624588 Accuracy 0.4521484375\n",
      "Iteration 41900 Training loss 0.05244278907775879 Validation loss 0.053248390555381775 Accuracy 0.459228515625\n",
      "Iteration 41910 Training loss 0.053696706891059875 Validation loss 0.053989969193935394 Accuracy 0.453125\n",
      "Iteration 41920 Training loss 0.04818442836403847 Validation loss 0.05331882834434509 Accuracy 0.460205078125\n",
      "Iteration 41930 Training loss 0.05609647184610367 Validation loss 0.05392134562134743 Accuracy 0.454345703125\n",
      "Iteration 41940 Training loss 0.05004189535975456 Validation loss 0.05342106148600578 Accuracy 0.45947265625\n",
      "Iteration 41950 Training loss 0.05243503674864769 Validation loss 0.053783781826496124 Accuracy 0.4560546875\n",
      "Iteration 41960 Training loss 0.05172981694340706 Validation loss 0.05326732620596886 Accuracy 0.459716796875\n",
      "Iteration 41970 Training loss 0.05363991856575012 Validation loss 0.05343439802527428 Accuracy 0.459716796875\n",
      "Iteration 41980 Training loss 0.05036933347582817 Validation loss 0.05342144891619682 Accuracy 0.45849609375\n",
      "Iteration 41990 Training loss 0.0503116101026535 Validation loss 0.05337274447083473 Accuracy 0.458984375\n",
      "Iteration 42000 Training loss 0.0530409999191761 Validation loss 0.05360298231244087 Accuracy 0.45751953125\n",
      "Iteration 42010 Training loss 0.05514296516776085 Validation loss 0.05332639440894127 Accuracy 0.46044921875\n",
      "Iteration 42020 Training loss 0.05534827709197998 Validation loss 0.05345871299505234 Accuracy 0.459228515625\n",
      "Iteration 42030 Training loss 0.05263088271021843 Validation loss 0.05345667898654938 Accuracy 0.458251953125\n",
      "Iteration 42040 Training loss 0.05263286456465721 Validation loss 0.0536893829703331 Accuracy 0.45556640625\n",
      "Iteration 42050 Training loss 0.052151620388031006 Validation loss 0.053169138729572296 Accuracy 0.4609375\n",
      "Iteration 42060 Training loss 0.05254340544342995 Validation loss 0.053334448486566544 Accuracy 0.460693359375\n",
      "Iteration 42070 Training loss 0.04752151295542717 Validation loss 0.05335802957415581 Accuracy 0.45947265625\n",
      "Iteration 42080 Training loss 0.04814983904361725 Validation loss 0.053170498460531235 Accuracy 0.460693359375\n",
      "Iteration 42090 Training loss 0.050328899174928665 Validation loss 0.05323244631290436 Accuracy 0.460693359375\n",
      "Iteration 42100 Training loss 0.052245862782001495 Validation loss 0.05407286807894707 Accuracy 0.45361328125\n",
      "Iteration 42110 Training loss 0.04972127079963684 Validation loss 0.05339580029249191 Accuracy 0.459716796875\n",
      "Iteration 42120 Training loss 0.05000593885779381 Validation loss 0.05348619446158409 Accuracy 0.457763671875\n",
      "Iteration 42130 Training loss 0.05071051046252251 Validation loss 0.0535888746380806 Accuracy 0.457275390625\n",
      "Iteration 42140 Training loss 0.05402620881795883 Validation loss 0.053204793483018875 Accuracy 0.460693359375\n",
      "Iteration 42150 Training loss 0.05302465707063675 Validation loss 0.053621985018253326 Accuracy 0.458251953125\n",
      "Iteration 42160 Training loss 0.05295441672205925 Validation loss 0.05342044681310654 Accuracy 0.458984375\n",
      "Iteration 42170 Training loss 0.052581317722797394 Validation loss 0.053552865982055664 Accuracy 0.458984375\n",
      "Iteration 42180 Training loss 0.05019866302609444 Validation loss 0.05340488255023956 Accuracy 0.458984375\n",
      "Iteration 42190 Training loss 0.05095954239368439 Validation loss 0.05362625792622566 Accuracy 0.45751953125\n",
      "Iteration 42200 Training loss 0.050990547984838486 Validation loss 0.05343446135520935 Accuracy 0.45947265625\n",
      "Iteration 42210 Training loss 0.05597329139709473 Validation loss 0.0535300187766552 Accuracy 0.458251953125\n",
      "Iteration 42220 Training loss 0.04894937947392464 Validation loss 0.053237106651067734 Accuracy 0.460693359375\n",
      "Iteration 42230 Training loss 0.052900902926921844 Validation loss 0.053230151534080505 Accuracy 0.4599609375\n",
      "Iteration 42240 Training loss 0.0498369038105011 Validation loss 0.05345667898654938 Accuracy 0.458984375\n",
      "Iteration 42250 Training loss 0.05146782472729683 Validation loss 0.053516194224357605 Accuracy 0.45849609375\n",
      "Iteration 42260 Training loss 0.05379660055041313 Validation loss 0.053422946482896805 Accuracy 0.458740234375\n",
      "Iteration 42270 Training loss 0.048439666628837585 Validation loss 0.05333084985613823 Accuracy 0.459716796875\n",
      "Iteration 42280 Training loss 0.05478624254465103 Validation loss 0.05345793440937996 Accuracy 0.458740234375\n",
      "Iteration 42290 Training loss 0.05228462070226669 Validation loss 0.05342457816004753 Accuracy 0.459716796875\n",
      "Iteration 42300 Training loss 0.052035629749298096 Validation loss 0.053201183676719666 Accuracy 0.46044921875\n",
      "Iteration 42310 Training loss 0.051671069115400314 Validation loss 0.05343202129006386 Accuracy 0.458984375\n",
      "Iteration 42320 Training loss 0.05237995460629463 Validation loss 0.05354525148868561 Accuracy 0.459228515625\n",
      "Iteration 42330 Training loss 0.05083004757761955 Validation loss 0.05356805399060249 Accuracy 0.458984375\n",
      "Iteration 42340 Training loss 0.0501222088932991 Validation loss 0.05333280190825462 Accuracy 0.460205078125\n",
      "Iteration 42350 Training loss 0.04735735058784485 Validation loss 0.05362687259912491 Accuracy 0.45751953125\n",
      "Iteration 42360 Training loss 0.05083511397242546 Validation loss 0.053376972675323486 Accuracy 0.459716796875\n",
      "Iteration 42370 Training loss 0.04903784021735191 Validation loss 0.05320940166711807 Accuracy 0.460205078125\n",
      "Iteration 42380 Training loss 0.05413353443145752 Validation loss 0.05328121781349182 Accuracy 0.4599609375\n",
      "Iteration 42390 Training loss 0.05381759628653526 Validation loss 0.053525038063526154 Accuracy 0.458984375\n",
      "Iteration 42400 Training loss 0.052819591015577316 Validation loss 0.05318845063447952 Accuracy 0.460693359375\n",
      "Iteration 42410 Training loss 0.051057830452919006 Validation loss 0.053529705852270126 Accuracy 0.45751953125\n",
      "Iteration 42420 Training loss 0.051910508424043655 Validation loss 0.053199637681245804 Accuracy 0.46044921875\n",
      "Iteration 42430 Training loss 0.04931668937206268 Validation loss 0.05339657887816429 Accuracy 0.45849609375\n",
      "Iteration 42440 Training loss 0.05105777084827423 Validation loss 0.0533241406083107 Accuracy 0.460205078125\n",
      "Iteration 42450 Training loss 0.05143537372350693 Validation loss 0.05359896272420883 Accuracy 0.4580078125\n",
      "Iteration 42460 Training loss 0.05209406465291977 Validation loss 0.05350925773382187 Accuracy 0.45849609375\n",
      "Iteration 42470 Training loss 0.053933411836624146 Validation loss 0.05388045683503151 Accuracy 0.454345703125\n",
      "Iteration 42480 Training loss 0.05028814822435379 Validation loss 0.05314278230071068 Accuracy 0.46044921875\n",
      "Iteration 42490 Training loss 0.05250968039035797 Validation loss 0.053287480026483536 Accuracy 0.459716796875\n",
      "Iteration 42500 Training loss 0.048818156123161316 Validation loss 0.053462546318769455 Accuracy 0.459228515625\n",
      "Iteration 42510 Training loss 0.05411243811249733 Validation loss 0.05320490151643753 Accuracy 0.4599609375\n",
      "Iteration 42520 Training loss 0.05047754943370819 Validation loss 0.0531967468559742 Accuracy 0.460693359375\n",
      "Iteration 42530 Training loss 0.05122048035264015 Validation loss 0.0533934161067009 Accuracy 0.459716796875\n",
      "Iteration 42540 Training loss 0.048827655613422394 Validation loss 0.053455766290426254 Accuracy 0.45947265625\n",
      "Iteration 42550 Training loss 0.04818200692534447 Validation loss 0.053411003202199936 Accuracy 0.4599609375\n",
      "Iteration 42560 Training loss 0.050676651298999786 Validation loss 0.053458184003829956 Accuracy 0.457763671875\n",
      "Iteration 42570 Training loss 0.053626056760549545 Validation loss 0.05379838868975639 Accuracy 0.45556640625\n",
      "Iteration 42580 Training loss 0.048064593225717545 Validation loss 0.053122229874134064 Accuracy 0.460693359375\n",
      "Iteration 42590 Training loss 0.05228346586227417 Validation loss 0.05316659063100815 Accuracy 0.460693359375\n",
      "Iteration 42600 Training loss 0.05479023605585098 Validation loss 0.053392380475997925 Accuracy 0.4599609375\n",
      "Iteration 42610 Training loss 0.05157449468970299 Validation loss 0.05352863296866417 Accuracy 0.458740234375\n",
      "Iteration 42620 Training loss 0.048615071922540665 Validation loss 0.05381833016872406 Accuracy 0.455078125\n",
      "Iteration 42630 Training loss 0.05212221294641495 Validation loss 0.05344071239233017 Accuracy 0.4580078125\n",
      "Iteration 42640 Training loss 0.048801716417074203 Validation loss 0.05329369381070137 Accuracy 0.459716796875\n",
      "Iteration 42650 Training loss 0.053315091878175735 Validation loss 0.053602930158376694 Accuracy 0.45654296875\n",
      "Iteration 42660 Training loss 0.05194258689880371 Validation loss 0.05350906774401665 Accuracy 0.458251953125\n",
      "Iteration 42670 Training loss 0.054250482469797134 Validation loss 0.05388261005282402 Accuracy 0.45556640625\n",
      "Iteration 42680 Training loss 0.053493380546569824 Validation loss 0.05319304019212723 Accuracy 0.4609375\n",
      "Iteration 42690 Training loss 0.05143170803785324 Validation loss 0.053419552743434906 Accuracy 0.45751953125\n",
      "Iteration 42700 Training loss 0.05282529816031456 Validation loss 0.054131850600242615 Accuracy 0.45166015625\n",
      "Iteration 42710 Training loss 0.052738942205905914 Validation loss 0.05325903370976448 Accuracy 0.4609375\n",
      "Iteration 42720 Training loss 0.054244134575128555 Validation loss 0.05367141589522362 Accuracy 0.456298828125\n",
      "Iteration 42730 Training loss 0.053463224321603775 Validation loss 0.05343792960047722 Accuracy 0.459228515625\n",
      "Iteration 42740 Training loss 0.04617242515087128 Validation loss 0.05349961295723915 Accuracy 0.457763671875\n",
      "Iteration 42750 Training loss 0.04788203909993172 Validation loss 0.05346594750881195 Accuracy 0.459228515625\n",
      "Iteration 42760 Training loss 0.05403575673699379 Validation loss 0.054205603897571564 Accuracy 0.45263671875\n",
      "Iteration 42770 Training loss 0.051826298236846924 Validation loss 0.05341027304530144 Accuracy 0.458984375\n",
      "Iteration 42780 Training loss 0.05190926045179367 Validation loss 0.053388405591249466 Accuracy 0.459228515625\n",
      "Iteration 42790 Training loss 0.047998636960983276 Validation loss 0.053542010486125946 Accuracy 0.458740234375\n",
      "Iteration 42800 Training loss 0.053065959364175797 Validation loss 0.053573932498693466 Accuracy 0.4580078125\n",
      "Iteration 42810 Training loss 0.0509321391582489 Validation loss 0.05344695597887039 Accuracy 0.45751953125\n",
      "Iteration 42820 Training loss 0.05006201192736626 Validation loss 0.05347352474927902 Accuracy 0.45751953125\n",
      "Iteration 42830 Training loss 0.05147300288081169 Validation loss 0.05330101400613785 Accuracy 0.458984375\n",
      "Iteration 42840 Training loss 0.0502120666205883 Validation loss 0.053538549691438675 Accuracy 0.4580078125\n",
      "Iteration 42850 Training loss 0.04901319742202759 Validation loss 0.053358547389507294 Accuracy 0.45751953125\n",
      "Iteration 42860 Training loss 0.05306382104754448 Validation loss 0.053350597620010376 Accuracy 0.458984375\n",
      "Iteration 42870 Training loss 0.05653279647231102 Validation loss 0.05446038767695427 Accuracy 0.447998046875\n",
      "Iteration 42880 Training loss 0.05283260717988014 Validation loss 0.053211383521556854 Accuracy 0.459716796875\n",
      "Iteration 42890 Training loss 0.055547405034303665 Validation loss 0.054144393652677536 Accuracy 0.453369140625\n",
      "Iteration 42900 Training loss 0.05414096638560295 Validation loss 0.05345390364527702 Accuracy 0.45849609375\n",
      "Iteration 42910 Training loss 0.05167270824313164 Validation loss 0.0532170832157135 Accuracy 0.460205078125\n",
      "Iteration 42920 Training loss 0.05225338786840439 Validation loss 0.053232040256261826 Accuracy 0.460205078125\n",
      "Iteration 42930 Training loss 0.053177010267972946 Validation loss 0.05354543402791023 Accuracy 0.457763671875\n",
      "Iteration 42940 Training loss 0.0482974536716938 Validation loss 0.053371068090200424 Accuracy 0.46044921875\n",
      "Iteration 42950 Training loss 0.05445883050560951 Validation loss 0.05340836942195892 Accuracy 0.45947265625\n",
      "Iteration 42960 Training loss 0.05032315105199814 Validation loss 0.05340449512004852 Accuracy 0.45947265625\n",
      "Iteration 42970 Training loss 0.05556904524564743 Validation loss 0.05327610298991203 Accuracy 0.459716796875\n",
      "Iteration 42980 Training loss 0.050570812076330185 Validation loss 0.05334058403968811 Accuracy 0.459716796875\n",
      "Iteration 42990 Training loss 0.052848752588033676 Validation loss 0.0535501167178154 Accuracy 0.457763671875\n",
      "Iteration 43000 Training loss 0.05009167268872261 Validation loss 0.05337892472743988 Accuracy 0.459716796875\n",
      "Iteration 43010 Training loss 0.05148116871714592 Validation loss 0.05364272743463516 Accuracy 0.456787109375\n",
      "Iteration 43020 Training loss 0.04948605224490166 Validation loss 0.05346211418509483 Accuracy 0.45947265625\n",
      "Iteration 43030 Training loss 0.04756416752934456 Validation loss 0.05326548591256142 Accuracy 0.45947265625\n",
      "Iteration 43040 Training loss 0.053361982107162476 Validation loss 0.05347863584756851 Accuracy 0.45947265625\n",
      "Iteration 43050 Training loss 0.053763262927532196 Validation loss 0.0534452423453331 Accuracy 0.45947265625\n",
      "Iteration 43060 Training loss 0.05354516580700874 Validation loss 0.05357152223587036 Accuracy 0.4580078125\n",
      "Iteration 43070 Training loss 0.054568927735090256 Validation loss 0.05357297882437706 Accuracy 0.45947265625\n",
      "Iteration 43080 Training loss 0.052645690739154816 Validation loss 0.053358469158411026 Accuracy 0.459228515625\n",
      "Iteration 43090 Training loss 0.049654439091682434 Validation loss 0.05340186879038811 Accuracy 0.460205078125\n",
      "Iteration 43100 Training loss 0.053104277700185776 Validation loss 0.05334581807255745 Accuracy 0.45947265625\n",
      "Iteration 43110 Training loss 0.04964549094438553 Validation loss 0.053512558341026306 Accuracy 0.458740234375\n",
      "Iteration 43120 Training loss 0.05055965110659599 Validation loss 0.053265951573848724 Accuracy 0.460205078125\n",
      "Iteration 43130 Training loss 0.052621882408857346 Validation loss 0.05375282093882561 Accuracy 0.456787109375\n",
      "Iteration 43140 Training loss 0.051654841750860214 Validation loss 0.0532412976026535 Accuracy 0.45947265625\n",
      "Iteration 43150 Training loss 0.055926669389009476 Validation loss 0.053269218653440475 Accuracy 0.460205078125\n",
      "Iteration 43160 Training loss 0.05166416987776756 Validation loss 0.05369684472680092 Accuracy 0.4560546875\n",
      "Iteration 43170 Training loss 0.05477878823876381 Validation loss 0.05421726405620575 Accuracy 0.452392578125\n",
      "Iteration 43180 Training loss 0.04766801744699478 Validation loss 0.05341005697846413 Accuracy 0.459228515625\n",
      "Iteration 43190 Training loss 0.05090547725558281 Validation loss 0.05378052592277527 Accuracy 0.45556640625\n",
      "Iteration 43200 Training loss 0.053519509732723236 Validation loss 0.053161971271038055 Accuracy 0.4609375\n",
      "Iteration 43210 Training loss 0.05252094566822052 Validation loss 0.05370008572936058 Accuracy 0.4560546875\n",
      "Iteration 43220 Training loss 0.05256686359643936 Validation loss 0.05323295295238495 Accuracy 0.46044921875\n",
      "Iteration 43230 Training loss 0.055606964975595474 Validation loss 0.05327058956027031 Accuracy 0.460205078125\n",
      "Iteration 43240 Training loss 0.05227189138531685 Validation loss 0.05333686247467995 Accuracy 0.4599609375\n",
      "Iteration 43250 Training loss 0.053152017295360565 Validation loss 0.053446635603904724 Accuracy 0.460205078125\n",
      "Iteration 43260 Training loss 0.04983353242278099 Validation loss 0.05359410494565964 Accuracy 0.458740234375\n",
      "Iteration 43270 Training loss 0.05193737894296646 Validation loss 0.05343649908900261 Accuracy 0.459228515625\n",
      "Iteration 43280 Training loss 0.05159532651305199 Validation loss 0.053838711231946945 Accuracy 0.45556640625\n",
      "Iteration 43290 Training loss 0.05153806507587433 Validation loss 0.05334961414337158 Accuracy 0.45947265625\n",
      "Iteration 43300 Training loss 0.05225064605474472 Validation loss 0.05359126999974251 Accuracy 0.457763671875\n",
      "Iteration 43310 Training loss 0.052470508962869644 Validation loss 0.05350165814161301 Accuracy 0.45849609375\n",
      "Iteration 43320 Training loss 0.051727090030908585 Validation loss 0.05329710617661476 Accuracy 0.460693359375\n",
      "Iteration 43330 Training loss 0.052235208451747894 Validation loss 0.053596608340740204 Accuracy 0.45751953125\n",
      "Iteration 43340 Training loss 0.04931626468896866 Validation loss 0.053316015750169754 Accuracy 0.458984375\n",
      "Iteration 43350 Training loss 0.04912672936916351 Validation loss 0.053579431027173996 Accuracy 0.456787109375\n",
      "Iteration 43360 Training loss 0.050656188279390335 Validation loss 0.05351652204990387 Accuracy 0.4580078125\n",
      "Iteration 43370 Training loss 0.05079372972249985 Validation loss 0.053341276943683624 Accuracy 0.458740234375\n",
      "Iteration 43380 Training loss 0.05237630754709244 Validation loss 0.05333692207932472 Accuracy 0.4599609375\n",
      "Iteration 43390 Training loss 0.04991336166858673 Validation loss 0.05362610146403313 Accuracy 0.459228515625\n",
      "Iteration 43400 Training loss 0.052798304706811905 Validation loss 0.053501155227422714 Accuracy 0.46044921875\n",
      "Iteration 43410 Training loss 0.05177747458219528 Validation loss 0.05353259667754173 Accuracy 0.457763671875\n",
      "Iteration 43420 Training loss 0.05068798363208771 Validation loss 0.05356554687023163 Accuracy 0.457763671875\n",
      "Iteration 43430 Training loss 0.051393333822488785 Validation loss 0.05387911945581436 Accuracy 0.4560546875\n",
      "Iteration 43440 Training loss 0.047767553478479385 Validation loss 0.05328957736492157 Accuracy 0.45947265625\n",
      "Iteration 43450 Training loss 0.049923766404390335 Validation loss 0.053458232432603836 Accuracy 0.458740234375\n",
      "Iteration 43460 Training loss 0.049306370317935944 Validation loss 0.05325841158628464 Accuracy 0.460693359375\n",
      "Iteration 43470 Training loss 0.048928312957286835 Validation loss 0.053315237164497375 Accuracy 0.458740234375\n",
      "Iteration 43480 Training loss 0.051239725202322006 Validation loss 0.053238749504089355 Accuracy 0.4609375\n",
      "Iteration 43490 Training loss 0.04930969327688217 Validation loss 0.05335977301001549 Accuracy 0.459716796875\n",
      "Iteration 43500 Training loss 0.05370057001709938 Validation loss 0.05360383540391922 Accuracy 0.456787109375\n",
      "Iteration 43510 Training loss 0.05056321248412132 Validation loss 0.05348243936896324 Accuracy 0.4580078125\n",
      "Iteration 43520 Training loss 0.05228019505739212 Validation loss 0.05342922359704971 Accuracy 0.45849609375\n",
      "Iteration 43530 Training loss 0.05245290696620941 Validation loss 0.05350973829627037 Accuracy 0.457275390625\n",
      "Iteration 43540 Training loss 0.04879327490925789 Validation loss 0.05354068800806999 Accuracy 0.45849609375\n",
      "Iteration 43550 Training loss 0.05052200332283974 Validation loss 0.05335699021816254 Accuracy 0.460205078125\n",
      "Iteration 43560 Training loss 0.047417089343070984 Validation loss 0.053352631628513336 Accuracy 0.459716796875\n",
      "Iteration 43570 Training loss 0.05293342098593712 Validation loss 0.0532185323536396 Accuracy 0.460693359375\n",
      "Iteration 43580 Training loss 0.05583278462290764 Validation loss 0.053744345903396606 Accuracy 0.4541015625\n",
      "Iteration 43590 Training loss 0.051393695175647736 Validation loss 0.053840916603803635 Accuracy 0.455322265625\n",
      "Iteration 43600 Training loss 0.049543969333171844 Validation loss 0.053111594170331955 Accuracy 0.460693359375\n",
      "Iteration 43610 Training loss 0.05361410975456238 Validation loss 0.05318322032690048 Accuracy 0.4609375\n",
      "Iteration 43620 Training loss 0.0505414642393589 Validation loss 0.05324298143386841 Accuracy 0.460205078125\n",
      "Iteration 43630 Training loss 0.0508972629904747 Validation loss 0.053259626030921936 Accuracy 0.461181640625\n",
      "Iteration 43640 Training loss 0.049763549119234085 Validation loss 0.05331391468644142 Accuracy 0.46044921875\n",
      "Iteration 43650 Training loss 0.04801636189222336 Validation loss 0.05344580113887787 Accuracy 0.458740234375\n",
      "Iteration 43660 Training loss 0.05165495723485947 Validation loss 0.053316351026296616 Accuracy 0.4599609375\n",
      "Iteration 43670 Training loss 0.05275329574942589 Validation loss 0.05335831269621849 Accuracy 0.459716796875\n",
      "Iteration 43680 Training loss 0.05208348482847214 Validation loss 0.053337518125772476 Accuracy 0.458740234375\n",
      "Iteration 43690 Training loss 0.0510055236518383 Validation loss 0.05339200049638748 Accuracy 0.459716796875\n",
      "Iteration 43700 Training loss 0.05484410375356674 Validation loss 0.053339697420597076 Accuracy 0.459716796875\n",
      "Iteration 43710 Training loss 0.050830282270908356 Validation loss 0.0535861998796463 Accuracy 0.45849609375\n",
      "Iteration 43720 Training loss 0.05383002758026123 Validation loss 0.05352753773331642 Accuracy 0.45751953125\n",
      "Iteration 43730 Training loss 0.05267168954014778 Validation loss 0.05309081822633743 Accuracy 0.4619140625\n",
      "Iteration 43740 Training loss 0.051755391061306 Validation loss 0.05328278988599777 Accuracy 0.460205078125\n",
      "Iteration 43750 Training loss 0.048980265855789185 Validation loss 0.05329414829611778 Accuracy 0.4599609375\n",
      "Iteration 43760 Training loss 0.05323072895407677 Validation loss 0.05327732115983963 Accuracy 0.4599609375\n",
      "Iteration 43770 Training loss 0.054015472531318665 Validation loss 0.05343632772564888 Accuracy 0.45947265625\n",
      "Iteration 43780 Training loss 0.04966185614466667 Validation loss 0.05315962806344032 Accuracy 0.461181640625\n",
      "Iteration 43790 Training loss 0.049696724861860275 Validation loss 0.053413257002830505 Accuracy 0.4580078125\n",
      "Iteration 43800 Training loss 0.05424223840236664 Validation loss 0.053328242152929306 Accuracy 0.45947265625\n",
      "Iteration 43810 Training loss 0.052114907652139664 Validation loss 0.05358768254518509 Accuracy 0.45849609375\n",
      "Iteration 43820 Training loss 0.05095016583800316 Validation loss 0.05354735255241394 Accuracy 0.457763671875\n",
      "Iteration 43830 Training loss 0.05484901741147041 Validation loss 0.05326042324304581 Accuracy 0.460205078125\n",
      "Iteration 43840 Training loss 0.04850611463189125 Validation loss 0.053343191742897034 Accuracy 0.459716796875\n",
      "Iteration 43850 Training loss 0.05320178344845772 Validation loss 0.053783223032951355 Accuracy 0.455078125\n",
      "Iteration 43860 Training loss 0.0495588593184948 Validation loss 0.05351029708981514 Accuracy 0.459228515625\n",
      "Iteration 43870 Training loss 0.05373173952102661 Validation loss 0.05332246050238609 Accuracy 0.4599609375\n",
      "Iteration 43880 Training loss 0.05089978501200676 Validation loss 0.053416844457387924 Accuracy 0.46044921875\n",
      "Iteration 43890 Training loss 0.0523109957575798 Validation loss 0.05361806973814964 Accuracy 0.456787109375\n",
      "Iteration 43900 Training loss 0.05039446800947189 Validation loss 0.053270068019628525 Accuracy 0.460205078125\n",
      "Iteration 43910 Training loss 0.050262533128261566 Validation loss 0.053216297179460526 Accuracy 0.460205078125\n",
      "Iteration 43920 Training loss 0.055279865860939026 Validation loss 0.053535476326942444 Accuracy 0.45751953125\n",
      "Iteration 43930 Training loss 0.0503966361284256 Validation loss 0.05331311374902725 Accuracy 0.46044921875\n",
      "Iteration 43940 Training loss 0.05520473048090935 Validation loss 0.05329294130206108 Accuracy 0.459716796875\n",
      "Iteration 43950 Training loss 0.05463305488228798 Validation loss 0.05316656082868576 Accuracy 0.4599609375\n",
      "Iteration 43960 Training loss 0.04910898953676224 Validation loss 0.05328526720404625 Accuracy 0.459716796875\n",
      "Iteration 43970 Training loss 0.05434376001358032 Validation loss 0.05334152281284332 Accuracy 0.459716796875\n",
      "Iteration 43980 Training loss 0.049985822290182114 Validation loss 0.05326848849654198 Accuracy 0.459228515625\n",
      "Iteration 43990 Training loss 0.0529567152261734 Validation loss 0.05369554087519646 Accuracy 0.455810546875\n",
      "Iteration 44000 Training loss 0.04822738096117973 Validation loss 0.053349316120147705 Accuracy 0.45849609375\n",
      "Iteration 44010 Training loss 0.055087290704250336 Validation loss 0.053540173918008804 Accuracy 0.4580078125\n",
      "Iteration 44020 Training loss 0.04906705766916275 Validation loss 0.05362667888402939 Accuracy 0.456298828125\n",
      "Iteration 44030 Training loss 0.05192575976252556 Validation loss 0.05341923236846924 Accuracy 0.458984375\n",
      "Iteration 44040 Training loss 0.052178408950567245 Validation loss 0.0534738227725029 Accuracy 0.45849609375\n",
      "Iteration 44050 Training loss 0.05167458578944206 Validation loss 0.0532984733581543 Accuracy 0.45947265625\n",
      "Iteration 44060 Training loss 0.05184492841362953 Validation loss 0.05379030480980873 Accuracy 0.453857421875\n",
      "Iteration 44070 Training loss 0.05121830105781555 Validation loss 0.05323619022965431 Accuracy 0.461181640625\n",
      "Iteration 44080 Training loss 0.04683862626552582 Validation loss 0.053456906229257584 Accuracy 0.45947265625\n",
      "Iteration 44090 Training loss 0.05312465876340866 Validation loss 0.05350804701447487 Accuracy 0.458984375\n",
      "Iteration 44100 Training loss 0.05249645560979843 Validation loss 0.05342445522546768 Accuracy 0.45947265625\n",
      "Iteration 44110 Training loss 0.051705896854400635 Validation loss 0.05404664948582649 Accuracy 0.45458984375\n",
      "Iteration 44120 Training loss 0.05078928545117378 Validation loss 0.05348396301269531 Accuracy 0.459716796875\n",
      "Iteration 44130 Training loss 0.05001969262957573 Validation loss 0.05368679389357567 Accuracy 0.45751953125\n",
      "Iteration 44140 Training loss 0.05246385186910629 Validation loss 0.05340741202235222 Accuracy 0.457763671875\n",
      "Iteration 44150 Training loss 0.04982621222734451 Validation loss 0.053144004195928574 Accuracy 0.46044921875\n",
      "Iteration 44160 Training loss 0.0522782988846302 Validation loss 0.05338352546095848 Accuracy 0.457763671875\n",
      "Iteration 44170 Training loss 0.04702266305685043 Validation loss 0.05350077152252197 Accuracy 0.458984375\n",
      "Iteration 44180 Training loss 0.050353147089481354 Validation loss 0.0535968542098999 Accuracy 0.45751953125\n",
      "Iteration 44190 Training loss 0.05398010089993477 Validation loss 0.05345707759261131 Accuracy 0.458251953125\n",
      "Iteration 44200 Training loss 0.04993482306599617 Validation loss 0.05378427729010582 Accuracy 0.455078125\n",
      "Iteration 44210 Training loss 0.05185575410723686 Validation loss 0.05375668779015541 Accuracy 0.4541015625\n",
      "Iteration 44220 Training loss 0.046622518450021744 Validation loss 0.05325132608413696 Accuracy 0.460205078125\n",
      "Iteration 44230 Training loss 0.04802730679512024 Validation loss 0.053853221237659454 Accuracy 0.455078125\n",
      "Iteration 44240 Training loss 0.054227616637945175 Validation loss 0.05379954352974892 Accuracy 0.455322265625\n",
      "Iteration 44250 Training loss 0.05071970447897911 Validation loss 0.05364350974559784 Accuracy 0.45703125\n",
      "Iteration 44260 Training loss 0.04920794442296028 Validation loss 0.05364314466714859 Accuracy 0.455810546875\n",
      "Iteration 44270 Training loss 0.05438283085823059 Validation loss 0.05329621955752373 Accuracy 0.458740234375\n",
      "Iteration 44280 Training loss 0.04957142099738121 Validation loss 0.05409866198897362 Accuracy 0.45361328125\n",
      "Iteration 44290 Training loss 0.047301035374403 Validation loss 0.053464628756046295 Accuracy 0.45849609375\n",
      "Iteration 44300 Training loss 0.05033288896083832 Validation loss 0.05339212343096733 Accuracy 0.45947265625\n",
      "Iteration 44310 Training loss 0.05233300104737282 Validation loss 0.053790077567100525 Accuracy 0.456298828125\n",
      "Iteration 44320 Training loss 0.05215708538889885 Validation loss 0.053296852856874466 Accuracy 0.459228515625\n",
      "Iteration 44330 Training loss 0.054338112473487854 Validation loss 0.05344017967581749 Accuracy 0.458984375\n",
      "Iteration 44340 Training loss 0.05201531946659088 Validation loss 0.0533372163772583 Accuracy 0.459716796875\n",
      "Iteration 44350 Training loss 0.04786262288689613 Validation loss 0.05324278771877289 Accuracy 0.460205078125\n",
      "Iteration 44360 Training loss 0.051267318427562714 Validation loss 0.053575024008750916 Accuracy 0.457763671875\n",
      "Iteration 44370 Training loss 0.05269455909729004 Validation loss 0.05327397957444191 Accuracy 0.459716796875\n",
      "Iteration 44380 Training loss 0.05301682651042938 Validation loss 0.05340292304754257 Accuracy 0.460205078125\n",
      "Iteration 44390 Training loss 0.051939141005277634 Validation loss 0.05335826054215431 Accuracy 0.46044921875\n",
      "Iteration 44400 Training loss 0.053726110607385635 Validation loss 0.053713247179985046 Accuracy 0.456298828125\n",
      "Iteration 44410 Training loss 0.05115850642323494 Validation loss 0.05334658920764923 Accuracy 0.458984375\n",
      "Iteration 44420 Training loss 0.05185377597808838 Validation loss 0.05329049751162529 Accuracy 0.46044921875\n",
      "Iteration 44430 Training loss 0.05610646679997444 Validation loss 0.053497932851314545 Accuracy 0.45947265625\n",
      "Iteration 44440 Training loss 0.05099669471383095 Validation loss 0.053280144929885864 Accuracy 0.45947265625\n",
      "Iteration 44450 Training loss 0.04907083511352539 Validation loss 0.05320096015930176 Accuracy 0.460693359375\n",
      "Iteration 44460 Training loss 0.0534871481359005 Validation loss 0.053235750645399094 Accuracy 0.4599609375\n",
      "Iteration 44470 Training loss 0.05622533708810806 Validation loss 0.05361069366335869 Accuracy 0.457275390625\n",
      "Iteration 44480 Training loss 0.05032720789313316 Validation loss 0.053927771747112274 Accuracy 0.455810546875\n",
      "Iteration 44490 Training loss 0.049532484263181686 Validation loss 0.05324316397309303 Accuracy 0.459228515625\n",
      "Iteration 44500 Training loss 0.05536217242479324 Validation loss 0.05437406152486801 Accuracy 0.44970703125\n",
      "Iteration 44510 Training loss 0.05425930395722389 Validation loss 0.05339325964450836 Accuracy 0.458740234375\n",
      "Iteration 44520 Training loss 0.04984349012374878 Validation loss 0.053414128720760345 Accuracy 0.458740234375\n",
      "Iteration 44530 Training loss 0.05371975526213646 Validation loss 0.0536363422870636 Accuracy 0.4580078125\n",
      "Iteration 44540 Training loss 0.050898436456918716 Validation loss 0.053419213742017746 Accuracy 0.45849609375\n",
      "Iteration 44550 Training loss 0.04914489760994911 Validation loss 0.053567737340927124 Accuracy 0.45849609375\n",
      "Iteration 44560 Training loss 0.05336136743426323 Validation loss 0.05358199030160904 Accuracy 0.457763671875\n",
      "Iteration 44570 Training loss 0.051251500844955444 Validation loss 0.053322117775678635 Accuracy 0.459716796875\n",
      "Iteration 44580 Training loss 0.05239875242114067 Validation loss 0.05363849923014641 Accuracy 0.45703125\n",
      "Iteration 44590 Training loss 0.052435532212257385 Validation loss 0.05346861109137535 Accuracy 0.459228515625\n",
      "Iteration 44600 Training loss 0.05180847644805908 Validation loss 0.05322786048054695 Accuracy 0.460205078125\n",
      "Iteration 44610 Training loss 0.053019508719444275 Validation loss 0.05337335914373398 Accuracy 0.459716796875\n",
      "Iteration 44620 Training loss 0.05285118520259857 Validation loss 0.05359715223312378 Accuracy 0.456298828125\n",
      "Iteration 44630 Training loss 0.051431410014629364 Validation loss 0.053266819566488266 Accuracy 0.45849609375\n",
      "Iteration 44640 Training loss 0.05094200745224953 Validation loss 0.05334234610199928 Accuracy 0.459716796875\n",
      "Iteration 44650 Training loss 0.05260111391544342 Validation loss 0.05360514670610428 Accuracy 0.45654296875\n",
      "Iteration 44660 Training loss 0.05270196869969368 Validation loss 0.05355706065893173 Accuracy 0.457275390625\n",
      "Iteration 44670 Training loss 0.052436716854572296 Validation loss 0.05368078872561455 Accuracy 0.456787109375\n",
      "Iteration 44680 Training loss 0.05023279786109924 Validation loss 0.05338890478014946 Accuracy 0.4580078125\n",
      "Iteration 44690 Training loss 0.05177785083651543 Validation loss 0.053364988416433334 Accuracy 0.4580078125\n",
      "Iteration 44700 Training loss 0.05299023538827896 Validation loss 0.05336344987154007 Accuracy 0.459716796875\n",
      "Iteration 44710 Training loss 0.05025380477309227 Validation loss 0.053225573152303696 Accuracy 0.45947265625\n",
      "Iteration 44720 Training loss 0.04845013469457626 Validation loss 0.053243476897478104 Accuracy 0.460205078125\n",
      "Iteration 44730 Training loss 0.051411986351013184 Validation loss 0.05334904044866562 Accuracy 0.460205078125\n",
      "Iteration 44740 Training loss 0.049755219370126724 Validation loss 0.05338470637798309 Accuracy 0.458984375\n",
      "Iteration 44750 Training loss 0.04599795490503311 Validation loss 0.05355047807097435 Accuracy 0.4580078125\n",
      "Iteration 44760 Training loss 0.05375267565250397 Validation loss 0.05349532887339592 Accuracy 0.458984375\n",
      "Iteration 44770 Training loss 0.04851394519209862 Validation loss 0.053896307945251465 Accuracy 0.456298828125\n",
      "Iteration 44780 Training loss 0.05522008240222931 Validation loss 0.05362731218338013 Accuracy 0.4580078125\n",
      "Iteration 44790 Training loss 0.05194622650742531 Validation loss 0.05331913381814957 Accuracy 0.459716796875\n",
      "Iteration 44800 Training loss 0.04996949061751366 Validation loss 0.05351586639881134 Accuracy 0.457763671875\n",
      "Iteration 44810 Training loss 0.05274279788136482 Validation loss 0.053365956991910934 Accuracy 0.459228515625\n",
      "Iteration 44820 Training loss 0.05248386040329933 Validation loss 0.0535389743745327 Accuracy 0.459228515625\n",
      "Iteration 44830 Training loss 0.049475885927677155 Validation loss 0.05348769575357437 Accuracy 0.45703125\n",
      "Iteration 44840 Training loss 0.05230392515659332 Validation loss 0.05352450907230377 Accuracy 0.4580078125\n",
      "Iteration 44850 Training loss 0.05279944837093353 Validation loss 0.05326452851295471 Accuracy 0.460205078125\n",
      "Iteration 44860 Training loss 0.05321667343378067 Validation loss 0.05332868918776512 Accuracy 0.458984375\n",
      "Iteration 44870 Training loss 0.05005442351102829 Validation loss 0.0533938966691494 Accuracy 0.459716796875\n",
      "Iteration 44880 Training loss 0.05206000804901123 Validation loss 0.0533377043902874 Accuracy 0.460205078125\n",
      "Iteration 44890 Training loss 0.052146464586257935 Validation loss 0.053339436650276184 Accuracy 0.459716796875\n",
      "Iteration 44900 Training loss 0.05138503015041351 Validation loss 0.05482577905058861 Accuracy 0.4462890625\n",
      "Iteration 44910 Training loss 0.0506754033267498 Validation loss 0.05343133583664894 Accuracy 0.459228515625\n",
      "Iteration 44920 Training loss 0.05342909321188927 Validation loss 0.053558193147182465 Accuracy 0.458740234375\n",
      "Iteration 44930 Training loss 0.05339377373456955 Validation loss 0.05369926616549492 Accuracy 0.457763671875\n",
      "Iteration 44940 Training loss 0.05151296406984329 Validation loss 0.05348421260714531 Accuracy 0.45849609375\n",
      "Iteration 44950 Training loss 0.05152347311377525 Validation loss 0.05335071310400963 Accuracy 0.4599609375\n",
      "Iteration 44960 Training loss 0.048484645783901215 Validation loss 0.0535239651799202 Accuracy 0.4580078125\n",
      "Iteration 44970 Training loss 0.05122843384742737 Validation loss 0.05343889817595482 Accuracy 0.458984375\n",
      "Iteration 44980 Training loss 0.05185499042272568 Validation loss 0.05322129651904106 Accuracy 0.460205078125\n",
      "Iteration 44990 Training loss 0.048350270837545395 Validation loss 0.053945720195770264 Accuracy 0.455078125\n",
      "Iteration 45000 Training loss 0.05407828092575073 Validation loss 0.05332488194108009 Accuracy 0.460205078125\n",
      "Iteration 45010 Training loss 0.04923612251877785 Validation loss 0.054021093994379044 Accuracy 0.45361328125\n",
      "Iteration 45020 Training loss 0.052290525287389755 Validation loss 0.05336177721619606 Accuracy 0.460205078125\n",
      "Iteration 45030 Training loss 0.05345385521650314 Validation loss 0.05335159972310066 Accuracy 0.459716796875\n",
      "Iteration 45040 Training loss 0.05239712446928024 Validation loss 0.05342676118016243 Accuracy 0.458740234375\n",
      "Iteration 45050 Training loss 0.05423194169998169 Validation loss 0.053531039506196976 Accuracy 0.456787109375\n",
      "Iteration 45060 Training loss 0.0517006441950798 Validation loss 0.05321237072348595 Accuracy 0.460205078125\n",
      "Iteration 45070 Training loss 0.05135767534375191 Validation loss 0.05334973335266113 Accuracy 0.45947265625\n",
      "Iteration 45080 Training loss 0.04868810996413231 Validation loss 0.05379369854927063 Accuracy 0.455078125\n",
      "Iteration 45090 Training loss 0.05227681249380112 Validation loss 0.053354017436504364 Accuracy 0.459716796875\n",
      "Iteration 45100 Training loss 0.05166973918676376 Validation loss 0.05336252972483635 Accuracy 0.458740234375\n",
      "Iteration 45110 Training loss 0.04833412542939186 Validation loss 0.053902268409729004 Accuracy 0.454345703125\n",
      "Iteration 45120 Training loss 0.05051780119538307 Validation loss 0.05332452058792114 Accuracy 0.45947265625\n",
      "Iteration 45130 Training loss 0.05388317629694939 Validation loss 0.05367918312549591 Accuracy 0.45751953125\n",
      "Iteration 45140 Training loss 0.050167255103588104 Validation loss 0.05341682583093643 Accuracy 0.459228515625\n",
      "Iteration 45150 Training loss 0.052267495542764664 Validation loss 0.05350467190146446 Accuracy 0.458740234375\n",
      "Iteration 45160 Training loss 0.050456054508686066 Validation loss 0.05346386507153511 Accuracy 0.458251953125\n",
      "Iteration 45170 Training loss 0.05364621803164482 Validation loss 0.05345519259572029 Accuracy 0.459228515625\n",
      "Iteration 45180 Training loss 0.05172479525208473 Validation loss 0.05359669774770737 Accuracy 0.458251953125\n",
      "Iteration 45190 Training loss 0.05102663114666939 Validation loss 0.053516361862421036 Accuracy 0.45751953125\n",
      "Iteration 45200 Training loss 0.0501028448343277 Validation loss 0.05329562723636627 Accuracy 0.45947265625\n",
      "Iteration 45210 Training loss 0.05502203851938248 Validation loss 0.054090164601802826 Accuracy 0.451904296875\n",
      "Iteration 45220 Training loss 0.054247111082077026 Validation loss 0.05412425473332405 Accuracy 0.452392578125\n",
      "Iteration 45230 Training loss 0.05000773072242737 Validation loss 0.053695835173130035 Accuracy 0.45703125\n",
      "Iteration 45240 Training loss 0.05347278714179993 Validation loss 0.05335383489727974 Accuracy 0.4580078125\n",
      "Iteration 45250 Training loss 0.05344441533088684 Validation loss 0.053604207932949066 Accuracy 0.4580078125\n",
      "Iteration 45260 Training loss 0.052051395177841187 Validation loss 0.053569450974464417 Accuracy 0.4580078125\n",
      "Iteration 45270 Training loss 0.04903919994831085 Validation loss 0.05354257673025131 Accuracy 0.457763671875\n",
      "Iteration 45280 Training loss 0.05388162285089493 Validation loss 0.05328508839011192 Accuracy 0.4599609375\n",
      "Iteration 45290 Training loss 0.04827059805393219 Validation loss 0.05348338559269905 Accuracy 0.45849609375\n",
      "Iteration 45300 Training loss 0.05526205524802208 Validation loss 0.053478993475437164 Accuracy 0.45849609375\n",
      "Iteration 45310 Training loss 0.05013004317879677 Validation loss 0.05329820141196251 Accuracy 0.459716796875\n",
      "Iteration 45320 Training loss 0.05515970662236214 Validation loss 0.05370965600013733 Accuracy 0.456298828125\n",
      "Iteration 45330 Training loss 0.051783036440610886 Validation loss 0.053633447736501694 Accuracy 0.45703125\n",
      "Iteration 45340 Training loss 0.052087899297475815 Validation loss 0.05323634669184685 Accuracy 0.460693359375\n",
      "Iteration 45350 Training loss 0.04752960056066513 Validation loss 0.05327877774834633 Accuracy 0.459716796875\n",
      "Iteration 45360 Training loss 0.05547671392560005 Validation loss 0.05316701903939247 Accuracy 0.46044921875\n",
      "Iteration 45370 Training loss 0.04905404523015022 Validation loss 0.053384069353342056 Accuracy 0.458984375\n",
      "Iteration 45380 Training loss 0.05256907269358635 Validation loss 0.05356866866350174 Accuracy 0.45849609375\n",
      "Iteration 45390 Training loss 0.051345035433769226 Validation loss 0.0533318929374218 Accuracy 0.4599609375\n",
      "Iteration 45400 Training loss 0.04935093969106674 Validation loss 0.053316276520490646 Accuracy 0.459228515625\n",
      "Iteration 45410 Training loss 0.05063200742006302 Validation loss 0.053579289466142654 Accuracy 0.4580078125\n",
      "Iteration 45420 Training loss 0.05086129158735275 Validation loss 0.0537213459610939 Accuracy 0.456787109375\n",
      "Iteration 45430 Training loss 0.054987721145153046 Validation loss 0.053267527371644974 Accuracy 0.4599609375\n",
      "Iteration 45440 Training loss 0.04621478170156479 Validation loss 0.05339350923895836 Accuracy 0.459716796875\n",
      "Iteration 45450 Training loss 0.05264297500252724 Validation loss 0.05341164022684097 Accuracy 0.458984375\n",
      "Iteration 45460 Training loss 0.05262104794383049 Validation loss 0.05337138473987579 Accuracy 0.459228515625\n",
      "Iteration 45470 Training loss 0.04850093647837639 Validation loss 0.053328707814216614 Accuracy 0.4599609375\n",
      "Iteration 45480 Training loss 0.052871957421302795 Validation loss 0.05391351878643036 Accuracy 0.453369140625\n",
      "Iteration 45490 Training loss 0.04917021468281746 Validation loss 0.05328216776251793 Accuracy 0.45947265625\n",
      "Iteration 45500 Training loss 0.04909799247980118 Validation loss 0.05350923538208008 Accuracy 0.456298828125\n",
      "Iteration 45510 Training loss 0.04941274970769882 Validation loss 0.05353643000125885 Accuracy 0.458251953125\n",
      "Iteration 45520 Training loss 0.049724556505680084 Validation loss 0.05335028097033501 Accuracy 0.459716796875\n",
      "Iteration 45530 Training loss 0.05253273993730545 Validation loss 0.05339135602116585 Accuracy 0.459228515625\n",
      "Iteration 45540 Training loss 0.05076999217271805 Validation loss 0.05332270264625549 Accuracy 0.458984375\n",
      "Iteration 45550 Training loss 0.05309702083468437 Validation loss 0.05326325073838234 Accuracy 0.46044921875\n",
      "Iteration 45560 Training loss 0.052830200642347336 Validation loss 0.05364996939897537 Accuracy 0.456298828125\n",
      "Iteration 45570 Training loss 0.049350034445524216 Validation loss 0.05341244861483574 Accuracy 0.458251953125\n",
      "Iteration 45580 Training loss 0.0507500059902668 Validation loss 0.05373775586485863 Accuracy 0.455078125\n",
      "Iteration 45590 Training loss 0.052610695362091064 Validation loss 0.05325399339199066 Accuracy 0.459716796875\n",
      "Iteration 45600 Training loss 0.04992621764540672 Validation loss 0.05332184582948685 Accuracy 0.460205078125\n",
      "Iteration 45610 Training loss 0.051276497542858124 Validation loss 0.05322141945362091 Accuracy 0.459716796875\n",
      "Iteration 45620 Training loss 0.05409850925207138 Validation loss 0.05342017486691475 Accuracy 0.458740234375\n",
      "Iteration 45630 Training loss 0.050539590418338776 Validation loss 0.05357600376009941 Accuracy 0.45751953125\n",
      "Iteration 45640 Training loss 0.05080718547105789 Validation loss 0.05319109559059143 Accuracy 0.460205078125\n",
      "Iteration 45650 Training loss 0.05155109614133835 Validation loss 0.05344770476222038 Accuracy 0.45849609375\n",
      "Iteration 45660 Training loss 0.0524432510137558 Validation loss 0.05353245139122009 Accuracy 0.457763671875\n",
      "Iteration 45670 Training loss 0.050166115164756775 Validation loss 0.05366341769695282 Accuracy 0.456298828125\n",
      "Iteration 45680 Training loss 0.04845292121171951 Validation loss 0.053405195474624634 Accuracy 0.4580078125\n",
      "Iteration 45690 Training loss 0.0504438690841198 Validation loss 0.05354298651218414 Accuracy 0.45751953125\n",
      "Iteration 45700 Training loss 0.04872536659240723 Validation loss 0.053378406912088394 Accuracy 0.459228515625\n",
      "Iteration 45710 Training loss 0.04837414622306824 Validation loss 0.05332418158650398 Accuracy 0.459716796875\n",
      "Iteration 45720 Training loss 0.054350219666957855 Validation loss 0.053721897304058075 Accuracy 0.455810546875\n",
      "Iteration 45730 Training loss 0.04919592663645744 Validation loss 0.05326060950756073 Accuracy 0.460693359375\n",
      "Iteration 45740 Training loss 0.05318345129489899 Validation loss 0.05428886413574219 Accuracy 0.450927734375\n",
      "Iteration 45750 Training loss 0.053487274795770645 Validation loss 0.05365515127778053 Accuracy 0.45849609375\n",
      "Iteration 45760 Training loss 0.05332505702972412 Validation loss 0.053229209035634995 Accuracy 0.4599609375\n",
      "Iteration 45770 Training loss 0.05201077088713646 Validation loss 0.05363694205880165 Accuracy 0.456298828125\n",
      "Iteration 45780 Training loss 0.05244117230176926 Validation loss 0.053451161831617355 Accuracy 0.458984375\n",
      "Iteration 45790 Training loss 0.04459923133254051 Validation loss 0.05344225838780403 Accuracy 0.45849609375\n",
      "Iteration 45800 Training loss 0.05136985331773758 Validation loss 0.053759824484586716 Accuracy 0.455078125\n",
      "Iteration 45810 Training loss 0.0520164929330349 Validation loss 0.053450897336006165 Accuracy 0.458984375\n",
      "Iteration 45820 Training loss 0.049983225762844086 Validation loss 0.05369378626346588 Accuracy 0.4560546875\n",
      "Iteration 45830 Training loss 0.051575418561697006 Validation loss 0.053381405770778656 Accuracy 0.460205078125\n",
      "Iteration 45840 Training loss 0.054094307124614716 Validation loss 0.05336786061525345 Accuracy 0.460205078125\n",
      "Iteration 45850 Training loss 0.05110391229391098 Validation loss 0.054259754717350006 Accuracy 0.4501953125\n",
      "Iteration 45860 Training loss 0.05029728636145592 Validation loss 0.053341299295425415 Accuracy 0.458984375\n",
      "Iteration 45870 Training loss 0.0533909797668457 Validation loss 0.053300224244594574 Accuracy 0.460205078125\n",
      "Iteration 45880 Training loss 0.05583398416638374 Validation loss 0.053421638906002045 Accuracy 0.45849609375\n",
      "Iteration 45890 Training loss 0.05106798931956291 Validation loss 0.05361180379986763 Accuracy 0.455078125\n",
      "Iteration 45900 Training loss 0.05269838497042656 Validation loss 0.05374535918235779 Accuracy 0.4560546875\n",
      "Iteration 45910 Training loss 0.05257399007678032 Validation loss 0.05367689207196236 Accuracy 0.45751953125\n",
      "Iteration 45920 Training loss 0.0528816357254982 Validation loss 0.05379565432667732 Accuracy 0.456787109375\n",
      "Iteration 45930 Training loss 0.051123250275850296 Validation loss 0.054192233830690384 Accuracy 0.451171875\n",
      "Iteration 45940 Training loss 0.05535542592406273 Validation loss 0.053197380155324936 Accuracy 0.460205078125\n",
      "Iteration 45950 Training loss 0.05235588923096657 Validation loss 0.05349748581647873 Accuracy 0.458984375\n",
      "Iteration 45960 Training loss 0.053094685077667236 Validation loss 0.053378574550151825 Accuracy 0.45849609375\n",
      "Iteration 45970 Training loss 0.05439622327685356 Validation loss 0.05332336947321892 Accuracy 0.460205078125\n",
      "Iteration 45980 Training loss 0.0556645393371582 Validation loss 0.053973469883203506 Accuracy 0.4541015625\n",
      "Iteration 45990 Training loss 0.05325052887201309 Validation loss 0.05348837375640869 Accuracy 0.458984375\n",
      "Iteration 46000 Training loss 0.05192430689930916 Validation loss 0.053622107952833176 Accuracy 0.457763671875\n",
      "Iteration 46010 Training loss 0.05209498107433319 Validation loss 0.05331019312143326 Accuracy 0.460205078125\n",
      "Iteration 46020 Training loss 0.053804412484169006 Validation loss 0.0535515695810318 Accuracy 0.455322265625\n",
      "Iteration 46030 Training loss 0.04949034005403519 Validation loss 0.053301215171813965 Accuracy 0.460205078125\n",
      "Iteration 46040 Training loss 0.05052575096487999 Validation loss 0.053471703082323074 Accuracy 0.457763671875\n",
      "Iteration 46050 Training loss 0.05497635155916214 Validation loss 0.05332936346530914 Accuracy 0.45947265625\n",
      "Iteration 46060 Training loss 0.05142044648528099 Validation loss 0.05339037999510765 Accuracy 0.459716796875\n",
      "Iteration 46070 Training loss 0.048675648868083954 Validation loss 0.053248826414346695 Accuracy 0.459716796875\n",
      "Iteration 46080 Training loss 0.04993806779384613 Validation loss 0.05360506847500801 Accuracy 0.45703125\n",
      "Iteration 46090 Training loss 0.050849515944719315 Validation loss 0.05353134498000145 Accuracy 0.45751953125\n",
      "Iteration 46100 Training loss 0.0538465678691864 Validation loss 0.05343508720397949 Accuracy 0.458984375\n",
      "Iteration 46110 Training loss 0.051725685596466064 Validation loss 0.05361068248748779 Accuracy 0.45849609375\n",
      "Iteration 46120 Training loss 0.05083172395825386 Validation loss 0.0533599853515625 Accuracy 0.459716796875\n",
      "Iteration 46130 Training loss 0.052905742079019547 Validation loss 0.05390414595603943 Accuracy 0.45361328125\n",
      "Iteration 46140 Training loss 0.04900161549448967 Validation loss 0.0533716082572937 Accuracy 0.459716796875\n",
      "Iteration 46150 Training loss 0.04949997738003731 Validation loss 0.05346478521823883 Accuracy 0.45947265625\n",
      "Iteration 46160 Training loss 0.05469491332769394 Validation loss 0.05356623977422714 Accuracy 0.456298828125\n",
      "Iteration 46170 Training loss 0.05127489194273949 Validation loss 0.05374148115515709 Accuracy 0.456298828125\n",
      "Iteration 46180 Training loss 0.0531444288790226 Validation loss 0.053597766906023026 Accuracy 0.456298828125\n",
      "Iteration 46190 Training loss 0.05151911452412605 Validation loss 0.05364591255784035 Accuracy 0.45751953125\n",
      "Iteration 46200 Training loss 0.052514586597681046 Validation loss 0.053263094276189804 Accuracy 0.4609375\n",
      "Iteration 46210 Training loss 0.05152295529842377 Validation loss 0.053390055894851685 Accuracy 0.458740234375\n",
      "Iteration 46220 Training loss 0.050854068249464035 Validation loss 0.053668465465307236 Accuracy 0.45654296875\n",
      "Iteration 46230 Training loss 0.05169728398323059 Validation loss 0.05353112518787384 Accuracy 0.45751953125\n",
      "Iteration 46240 Training loss 0.052155304700136185 Validation loss 0.053519606590270996 Accuracy 0.458740234375\n",
      "Iteration 46250 Training loss 0.04979586601257324 Validation loss 0.05332588776946068 Accuracy 0.460205078125\n",
      "Iteration 46260 Training loss 0.053053975105285645 Validation loss 0.053287263959646225 Accuracy 0.45849609375\n",
      "Iteration 46270 Training loss 0.05132914334535599 Validation loss 0.05329044908285141 Accuracy 0.4599609375\n",
      "Iteration 46280 Training loss 0.04977115988731384 Validation loss 0.053603846579790115 Accuracy 0.457275390625\n",
      "Iteration 46290 Training loss 0.05154447257518768 Validation loss 0.05327973514795303 Accuracy 0.459716796875\n",
      "Iteration 46300 Training loss 0.05055190995335579 Validation loss 0.053677674382925034 Accuracy 0.455078125\n",
      "Iteration 46310 Training loss 0.05356181785464287 Validation loss 0.05348021164536476 Accuracy 0.458984375\n",
      "Iteration 46320 Training loss 0.05048147588968277 Validation loss 0.05375460907816887 Accuracy 0.458251953125\n",
      "Iteration 46330 Training loss 0.050505079329013824 Validation loss 0.053933802992105484 Accuracy 0.453857421875\n",
      "Iteration 46340 Training loss 0.05000539496541023 Validation loss 0.053356848657131195 Accuracy 0.459228515625\n",
      "Iteration 46350 Training loss 0.05274524912238121 Validation loss 0.053421083837747574 Accuracy 0.45947265625\n",
      "Iteration 46360 Training loss 0.050230417400598526 Validation loss 0.053505636751651764 Accuracy 0.45849609375\n",
      "Iteration 46370 Training loss 0.05250156670808792 Validation loss 0.053584471344947815 Accuracy 0.45703125\n",
      "Iteration 46380 Training loss 0.051138054579496384 Validation loss 0.053345244377851486 Accuracy 0.45947265625\n",
      "Iteration 46390 Training loss 0.04916766658425331 Validation loss 0.053228579461574554 Accuracy 0.460693359375\n",
      "Iteration 46400 Training loss 0.05319836363196373 Validation loss 0.053541820496320724 Accuracy 0.458251953125\n",
      "Iteration 46410 Training loss 0.05540822446346283 Validation loss 0.05341456085443497 Accuracy 0.45849609375\n",
      "Iteration 46420 Training loss 0.04986847937107086 Validation loss 0.05331030115485191 Accuracy 0.460205078125\n",
      "Iteration 46430 Training loss 0.05063679814338684 Validation loss 0.05379084497690201 Accuracy 0.45556640625\n",
      "Iteration 46440 Training loss 0.051069386303424835 Validation loss 0.0536927804350853 Accuracy 0.456787109375\n",
      "Iteration 46450 Training loss 0.053817372769117355 Validation loss 0.05362999066710472 Accuracy 0.457275390625\n",
      "Iteration 46460 Training loss 0.04994979873299599 Validation loss 0.053485143929719925 Accuracy 0.458984375\n",
      "Iteration 46470 Training loss 0.04922819882631302 Validation loss 0.05334413796663284 Accuracy 0.458984375\n",
      "Iteration 46480 Training loss 0.051102690398693085 Validation loss 0.053275272250175476 Accuracy 0.4599609375\n",
      "Iteration 46490 Training loss 0.05227504298090935 Validation loss 0.05351481959223747 Accuracy 0.4580078125\n",
      "Iteration 46500 Training loss 0.05589129030704498 Validation loss 0.05353540554642677 Accuracy 0.458984375\n",
      "Iteration 46510 Training loss 0.050708089023828506 Validation loss 0.05345245450735092 Accuracy 0.458740234375\n",
      "Iteration 46520 Training loss 0.050649143755435944 Validation loss 0.05366668477654457 Accuracy 0.456298828125\n",
      "Iteration 46530 Training loss 0.05072686821222305 Validation loss 0.053400203585624695 Accuracy 0.45947265625\n",
      "Iteration 46540 Training loss 0.051335327327251434 Validation loss 0.05370897427201271 Accuracy 0.4580078125\n",
      "Iteration 46550 Training loss 0.04914391040802002 Validation loss 0.0534035824239254 Accuracy 0.459716796875\n",
      "Iteration 46560 Training loss 0.049854278564453125 Validation loss 0.053320854902267456 Accuracy 0.460205078125\n",
      "Iteration 46570 Training loss 0.052750252187252045 Validation loss 0.05434957891702652 Accuracy 0.4501953125\n",
      "Iteration 46580 Training loss 0.049197256565093994 Validation loss 0.053679823875427246 Accuracy 0.45654296875\n",
      "Iteration 46590 Training loss 0.053753871470689774 Validation loss 0.05418776348233223 Accuracy 0.45166015625\n",
      "Iteration 46600 Training loss 0.05079081282019615 Validation loss 0.05371320992708206 Accuracy 0.456298828125\n",
      "Iteration 46610 Training loss 0.05175704509019852 Validation loss 0.05330074951052666 Accuracy 0.460205078125\n",
      "Iteration 46620 Training loss 0.051480334252119064 Validation loss 0.05363599583506584 Accuracy 0.457275390625\n",
      "Iteration 46630 Training loss 0.0516875796020031 Validation loss 0.05331788957118988 Accuracy 0.460205078125\n",
      "Iteration 46640 Training loss 0.05356762558221817 Validation loss 0.05552833527326584 Accuracy 0.43798828125\n",
      "Iteration 46650 Training loss 0.04946838691830635 Validation loss 0.053455859422683716 Accuracy 0.458251953125\n",
      "Iteration 46660 Training loss 0.05385719612240791 Validation loss 0.0532652921974659 Accuracy 0.45947265625\n",
      "Iteration 46670 Training loss 0.049390994012355804 Validation loss 0.0533100962638855 Accuracy 0.460693359375\n",
      "Iteration 46680 Training loss 0.049783192574977875 Validation loss 0.05348286032676697 Accuracy 0.45849609375\n",
      "Iteration 46690 Training loss 0.0498049296438694 Validation loss 0.05348239466547966 Accuracy 0.4580078125\n",
      "Iteration 46700 Training loss 0.05106309428811073 Validation loss 0.05331093445420265 Accuracy 0.459716796875\n",
      "Iteration 46710 Training loss 0.051233548671007156 Validation loss 0.05342530086636543 Accuracy 0.45849609375\n",
      "Iteration 46720 Training loss 0.05013797804713249 Validation loss 0.053232595324516296 Accuracy 0.45947265625\n",
      "Iteration 46730 Training loss 0.048965465277433395 Validation loss 0.05384951829910278 Accuracy 0.455078125\n",
      "Iteration 46740 Training loss 0.05146954208612442 Validation loss 0.053730156272649765 Accuracy 0.457275390625\n",
      "Iteration 46750 Training loss 0.050165802240371704 Validation loss 0.0534803681075573 Accuracy 0.45947265625\n",
      "Iteration 46760 Training loss 0.05191349238157272 Validation loss 0.05375932529568672 Accuracy 0.45556640625\n",
      "Iteration 46770 Training loss 0.05096077546477318 Validation loss 0.0535019114613533 Accuracy 0.458984375\n",
      "Iteration 46780 Training loss 0.05564140900969505 Validation loss 0.054076023399829865 Accuracy 0.45263671875\n",
      "Iteration 46790 Training loss 0.05108404532074928 Validation loss 0.05353314429521561 Accuracy 0.45849609375\n",
      "Iteration 46800 Training loss 0.05135815218091011 Validation loss 0.05352878198027611 Accuracy 0.4580078125\n",
      "Iteration 46810 Training loss 0.05237589031457901 Validation loss 0.053288474678993225 Accuracy 0.459228515625\n",
      "Iteration 46820 Training loss 0.04993319511413574 Validation loss 0.0534336157143116 Accuracy 0.458740234375\n",
      "Iteration 46830 Training loss 0.05246194452047348 Validation loss 0.053448572754859924 Accuracy 0.458984375\n",
      "Iteration 46840 Training loss 0.053025949746370316 Validation loss 0.053208135068416595 Accuracy 0.460693359375\n",
      "Iteration 46850 Training loss 0.05596975237131119 Validation loss 0.053254470229148865 Accuracy 0.4599609375\n",
      "Iteration 46860 Training loss 0.05077807977795601 Validation loss 0.053389228880405426 Accuracy 0.458984375\n",
      "Iteration 46870 Training loss 0.053983673453330994 Validation loss 0.053370505571365356 Accuracy 0.459228515625\n",
      "Iteration 46880 Training loss 0.052531301975250244 Validation loss 0.053688500076532364 Accuracy 0.456787109375\n",
      "Iteration 46890 Training loss 0.0486324168741703 Validation loss 0.053324345499277115 Accuracy 0.459716796875\n",
      "Iteration 46900 Training loss 0.048859380185604095 Validation loss 0.05352529138326645 Accuracy 0.4560546875\n",
      "Iteration 46910 Training loss 0.048794910311698914 Validation loss 0.0532880537211895 Accuracy 0.46044921875\n",
      "Iteration 46920 Training loss 0.04782373458147049 Validation loss 0.053200721740722656 Accuracy 0.460693359375\n",
      "Iteration 46930 Training loss 0.048786360770463943 Validation loss 0.05339377745985985 Accuracy 0.458740234375\n",
      "Iteration 46940 Training loss 0.05253037065267563 Validation loss 0.053351618349552155 Accuracy 0.45947265625\n",
      "Iteration 46950 Training loss 0.053028084337711334 Validation loss 0.05314646288752556 Accuracy 0.46142578125\n",
      "Iteration 46960 Training loss 0.05039776489138603 Validation loss 0.053169719874858856 Accuracy 0.46044921875\n",
      "Iteration 46970 Training loss 0.055308640003204346 Validation loss 0.05351471155881882 Accuracy 0.459228515625\n",
      "Iteration 46980 Training loss 0.04955650120973587 Validation loss 0.05336429551243782 Accuracy 0.458984375\n",
      "Iteration 46990 Training loss 0.05068377032876015 Validation loss 0.05343954637646675 Accuracy 0.4580078125\n",
      "Iteration 47000 Training loss 0.05036133900284767 Validation loss 0.0534251369535923 Accuracy 0.459716796875\n",
      "Iteration 47010 Training loss 0.05146964639425278 Validation loss 0.053695354610681534 Accuracy 0.456298828125\n",
      "Iteration 47020 Training loss 0.049256376922130585 Validation loss 0.053364306688308716 Accuracy 0.4599609375\n",
      "Iteration 47030 Training loss 0.050747670233249664 Validation loss 0.05349448695778847 Accuracy 0.45947265625\n",
      "Iteration 47040 Training loss 0.053483396768569946 Validation loss 0.05334468558430672 Accuracy 0.45947265625\n",
      "Iteration 47050 Training loss 0.05265503376722336 Validation loss 0.05354961380362511 Accuracy 0.457763671875\n",
      "Iteration 47060 Training loss 0.051171984523534775 Validation loss 0.053665272891521454 Accuracy 0.45654296875\n",
      "Iteration 47070 Training loss 0.0501459538936615 Validation loss 0.05324868485331535 Accuracy 0.460205078125\n",
      "Iteration 47080 Training loss 0.05489201843738556 Validation loss 0.05392676964402199 Accuracy 0.45556640625\n",
      "Iteration 47090 Training loss 0.05205751582980156 Validation loss 0.05310364067554474 Accuracy 0.461181640625\n",
      "Iteration 47100 Training loss 0.05511033907532692 Validation loss 0.0534011609852314 Accuracy 0.4580078125\n",
      "Iteration 47110 Training loss 0.050172749906778336 Validation loss 0.053343504667282104 Accuracy 0.45947265625\n",
      "Iteration 47120 Training loss 0.04860644042491913 Validation loss 0.053379081189632416 Accuracy 0.457275390625\n",
      "Iteration 47130 Training loss 0.05218818038702011 Validation loss 0.05309591814875603 Accuracy 0.460693359375\n",
      "Iteration 47140 Training loss 0.05286961421370506 Validation loss 0.05322428420186043 Accuracy 0.4609375\n",
      "Iteration 47150 Training loss 0.05121206119656563 Validation loss 0.05319151282310486 Accuracy 0.460205078125\n",
      "Iteration 47160 Training loss 0.04764324054121971 Validation loss 0.0531972199678421 Accuracy 0.459716796875\n",
      "Iteration 47170 Training loss 0.05060984194278717 Validation loss 0.05332975462079048 Accuracy 0.459716796875\n",
      "Iteration 47180 Training loss 0.057231880724430084 Validation loss 0.0532013364136219 Accuracy 0.459716796875\n",
      "Iteration 47190 Training loss 0.05190201848745346 Validation loss 0.05339024215936661 Accuracy 0.458984375\n",
      "Iteration 47200 Training loss 0.05186158046126366 Validation loss 0.05315670371055603 Accuracy 0.46044921875\n",
      "Iteration 47210 Training loss 0.05347167328000069 Validation loss 0.05341225862503052 Accuracy 0.460205078125\n",
      "Iteration 47220 Training loss 0.04933395981788635 Validation loss 0.05335089564323425 Accuracy 0.45849609375\n",
      "Iteration 47230 Training loss 0.050613027065992355 Validation loss 0.05324038118124008 Accuracy 0.4599609375\n",
      "Iteration 47240 Training loss 0.05306369438767433 Validation loss 0.05369557440280914 Accuracy 0.456298828125\n",
      "Iteration 47250 Training loss 0.04884761944413185 Validation loss 0.05317682400345802 Accuracy 0.46044921875\n",
      "Iteration 47260 Training loss 0.04836636409163475 Validation loss 0.05334668979048729 Accuracy 0.45947265625\n",
      "Iteration 47270 Training loss 0.053509023040533066 Validation loss 0.05334976315498352 Accuracy 0.458984375\n",
      "Iteration 47280 Training loss 0.05471919849514961 Validation loss 0.05385838821530342 Accuracy 0.455078125\n",
      "Iteration 47290 Training loss 0.05045701190829277 Validation loss 0.053428374230861664 Accuracy 0.4599609375\n",
      "Iteration 47300 Training loss 0.05385806784033775 Validation loss 0.053757280111312866 Accuracy 0.4580078125\n",
      "Iteration 47310 Training loss 0.050267159938812256 Validation loss 0.05341480299830437 Accuracy 0.459228515625\n",
      "Iteration 47320 Training loss 0.05264218524098396 Validation loss 0.05354813486337662 Accuracy 0.458251953125\n",
      "Iteration 47330 Training loss 0.05332016944885254 Validation loss 0.05375639349222183 Accuracy 0.45654296875\n",
      "Iteration 47340 Training loss 0.052073247730731964 Validation loss 0.05355473980307579 Accuracy 0.458984375\n",
      "Iteration 47350 Training loss 0.04952586814761162 Validation loss 0.05345578119158745 Accuracy 0.45849609375\n",
      "Iteration 47360 Training loss 0.051074426621198654 Validation loss 0.05340335890650749 Accuracy 0.459716796875\n",
      "Iteration 47370 Training loss 0.05180218443274498 Validation loss 0.053270526230335236 Accuracy 0.459716796875\n",
      "Iteration 47380 Training loss 0.05075152963399887 Validation loss 0.053192175924777985 Accuracy 0.4599609375\n",
      "Iteration 47390 Training loss 0.05040591210126877 Validation loss 0.05409407243132591 Accuracy 0.451904296875\n",
      "Iteration 47400 Training loss 0.05468282103538513 Validation loss 0.05343133583664894 Accuracy 0.45849609375\n",
      "Iteration 47410 Training loss 0.05112834274768829 Validation loss 0.05351998284459114 Accuracy 0.457763671875\n",
      "Iteration 47420 Training loss 0.049645137041807175 Validation loss 0.053814198821783066 Accuracy 0.4541015625\n",
      "Iteration 47430 Training loss 0.04996893182396889 Validation loss 0.05363433435559273 Accuracy 0.457275390625\n",
      "Iteration 47440 Training loss 0.051289152354002 Validation loss 0.053162265568971634 Accuracy 0.4609375\n",
      "Iteration 47450 Training loss 0.049304354935884476 Validation loss 0.053198158740997314 Accuracy 0.460693359375\n",
      "Iteration 47460 Training loss 0.04850596934556961 Validation loss 0.053384311497211456 Accuracy 0.458984375\n",
      "Iteration 47470 Training loss 0.054689921438694 Validation loss 0.053108055144548416 Accuracy 0.46142578125\n",
      "Iteration 47480 Training loss 0.04761447012424469 Validation loss 0.053392548114061356 Accuracy 0.45947265625\n",
      "Iteration 47490 Training loss 0.05060784891247749 Validation loss 0.0536528117954731 Accuracy 0.457763671875\n",
      "Iteration 47500 Training loss 0.050675902515649796 Validation loss 0.05336277186870575 Accuracy 0.459716796875\n",
      "Iteration 47510 Training loss 0.05158780887722969 Validation loss 0.05348696559667587 Accuracy 0.457275390625\n",
      "Iteration 47520 Training loss 0.052082691341638565 Validation loss 0.05323256552219391 Accuracy 0.461181640625\n",
      "Iteration 47530 Training loss 0.05051323398947716 Validation loss 0.05353505164384842 Accuracy 0.458251953125\n",
      "Iteration 47540 Training loss 0.052679646760225296 Validation loss 0.05336100980639458 Accuracy 0.459228515625\n",
      "Iteration 47550 Training loss 0.055162373930215836 Validation loss 0.05328560248017311 Accuracy 0.45947265625\n",
      "Iteration 47560 Training loss 0.0520927868783474 Validation loss 0.05337168276309967 Accuracy 0.4580078125\n",
      "Iteration 47570 Training loss 0.053068261593580246 Validation loss 0.05342347174882889 Accuracy 0.45947265625\n",
      "Iteration 47580 Training loss 0.050923917442560196 Validation loss 0.053383830934762955 Accuracy 0.459716796875\n",
      "Iteration 47590 Training loss 0.04842163994908333 Validation loss 0.0533355250954628 Accuracy 0.45947265625\n",
      "Iteration 47600 Training loss 0.05052963271737099 Validation loss 0.05345063656568527 Accuracy 0.459716796875\n",
      "Iteration 47610 Training loss 0.05167895928025246 Validation loss 0.053349804133176804 Accuracy 0.459228515625\n",
      "Iteration 47620 Training loss 0.05184531584382057 Validation loss 0.05356399342417717 Accuracy 0.4580078125\n",
      "Iteration 47630 Training loss 0.05092629790306091 Validation loss 0.053718291223049164 Accuracy 0.45654296875\n",
      "Iteration 47640 Training loss 0.050399862229824066 Validation loss 0.053226929157972336 Accuracy 0.4609375\n",
      "Iteration 47650 Training loss 0.051808685064315796 Validation loss 0.05334228277206421 Accuracy 0.458984375\n",
      "Iteration 47660 Training loss 0.05236232280731201 Validation loss 0.05322711914777756 Accuracy 0.459716796875\n",
      "Iteration 47670 Training loss 0.05299456790089607 Validation loss 0.05316157266497612 Accuracy 0.461181640625\n",
      "Iteration 47680 Training loss 0.04799331724643707 Validation loss 0.053458020091056824 Accuracy 0.459228515625\n",
      "Iteration 47690 Training loss 0.05298514664173126 Validation loss 0.05314236134290695 Accuracy 0.460205078125\n",
      "Iteration 47700 Training loss 0.05287344381213188 Validation loss 0.05331750214099884 Accuracy 0.4599609375\n",
      "Iteration 47710 Training loss 0.053679827600717545 Validation loss 0.053110651671886444 Accuracy 0.460693359375\n",
      "Iteration 47720 Training loss 0.05415205657482147 Validation loss 0.05418989062309265 Accuracy 0.452880859375\n",
      "Iteration 47730 Training loss 0.05156656354665756 Validation loss 0.05386979505419731 Accuracy 0.4541015625\n",
      "Iteration 47740 Training loss 0.04875700920820236 Validation loss 0.053256805986166 Accuracy 0.459716796875\n",
      "Iteration 47750 Training loss 0.0490550734102726 Validation loss 0.053413718938827515 Accuracy 0.458740234375\n",
      "Iteration 47760 Training loss 0.05284818634390831 Validation loss 0.05329558625817299 Accuracy 0.459228515625\n",
      "Iteration 47770 Training loss 0.04855887219309807 Validation loss 0.05315154790878296 Accuracy 0.460205078125\n",
      "Iteration 47780 Training loss 0.05415383353829384 Validation loss 0.05358939245343208 Accuracy 0.458251953125\n",
      "Iteration 47790 Training loss 0.04794524982571602 Validation loss 0.053499650210142136 Accuracy 0.456787109375\n",
      "Iteration 47800 Training loss 0.04986032098531723 Validation loss 0.05314929410815239 Accuracy 0.461181640625\n",
      "Iteration 47810 Training loss 0.05045272782444954 Validation loss 0.05344058573246002 Accuracy 0.4580078125\n",
      "Iteration 47820 Training loss 0.05022585391998291 Validation loss 0.05324995517730713 Accuracy 0.460205078125\n",
      "Iteration 47830 Training loss 0.05100477114319801 Validation loss 0.053420599550008774 Accuracy 0.45947265625\n",
      "Iteration 47840 Training loss 0.053225863724946976 Validation loss 0.053416457027196884 Accuracy 0.458984375\n",
      "Iteration 47850 Training loss 0.05046490952372551 Validation loss 0.05333786830306053 Accuracy 0.45947265625\n",
      "Iteration 47860 Training loss 0.04863125458359718 Validation loss 0.05334091559052467 Accuracy 0.458740234375\n",
      "Iteration 47870 Training loss 0.05355692654848099 Validation loss 0.053437601774930954 Accuracy 0.457275390625\n",
      "Iteration 47880 Training loss 0.053297270089387894 Validation loss 0.05326292663812637 Accuracy 0.459228515625\n",
      "Iteration 47890 Training loss 0.05383304879069328 Validation loss 0.053196121007204056 Accuracy 0.46044921875\n",
      "Iteration 47900 Training loss 0.05309848487377167 Validation loss 0.05309535190463066 Accuracy 0.459716796875\n",
      "Iteration 47910 Training loss 0.05457313358783722 Validation loss 0.05388602241873741 Accuracy 0.456298828125\n",
      "Iteration 47920 Training loss 0.05726846307516098 Validation loss 0.05386107787489891 Accuracy 0.4560546875\n",
      "Iteration 47930 Training loss 0.04840929061174393 Validation loss 0.05326152220368385 Accuracy 0.46044921875\n",
      "Iteration 47940 Training loss 0.053876280784606934 Validation loss 0.05349329859018326 Accuracy 0.456787109375\n",
      "Iteration 47950 Training loss 0.052413541823625565 Validation loss 0.05361470207571983 Accuracy 0.4560546875\n",
      "Iteration 47960 Training loss 0.04969942569732666 Validation loss 0.05315927416086197 Accuracy 0.460693359375\n",
      "Iteration 47970 Training loss 0.051114603877067566 Validation loss 0.05332053452730179 Accuracy 0.4599609375\n",
      "Iteration 47980 Training loss 0.04777270928025246 Validation loss 0.05318586900830269 Accuracy 0.460693359375\n",
      "Iteration 47990 Training loss 0.05129942297935486 Validation loss 0.05368630215525627 Accuracy 0.45751953125\n",
      "Iteration 48000 Training loss 0.049469541758298874 Validation loss 0.0531480647623539 Accuracy 0.46142578125\n",
      "Iteration 48010 Training loss 0.053513191640377045 Validation loss 0.053497347980737686 Accuracy 0.457763671875\n",
      "Iteration 48020 Training loss 0.0506259948015213 Validation loss 0.05354807525873184 Accuracy 0.458984375\n",
      "Iteration 48030 Training loss 0.05347695201635361 Validation loss 0.05350930616259575 Accuracy 0.458984375\n",
      "Iteration 48040 Training loss 0.05114869028329849 Validation loss 0.05338368937373161 Accuracy 0.460205078125\n",
      "Iteration 48050 Training loss 0.04972044751048088 Validation loss 0.053700193762779236 Accuracy 0.454833984375\n",
      "Iteration 48060 Training loss 0.05235784500837326 Validation loss 0.05322551727294922 Accuracy 0.459716796875\n",
      "Iteration 48070 Training loss 0.05238215625286102 Validation loss 0.05325399339199066 Accuracy 0.460693359375\n",
      "Iteration 48080 Training loss 0.052712392061948776 Validation loss 0.05352381616830826 Accuracy 0.458984375\n",
      "Iteration 48090 Training loss 0.05050289258360863 Validation loss 0.05371846631169319 Accuracy 0.455322265625\n",
      "Iteration 48100 Training loss 0.049659352749586105 Validation loss 0.05355239287018776 Accuracy 0.458740234375\n",
      "Iteration 48110 Training loss 0.05021835118532181 Validation loss 0.05353657901287079 Accuracy 0.456787109375\n",
      "Iteration 48120 Training loss 0.052583806216716766 Validation loss 0.0539204366505146 Accuracy 0.455810546875\n",
      "Iteration 48130 Training loss 0.05081070959568024 Validation loss 0.05327508971095085 Accuracy 0.4599609375\n",
      "Iteration 48140 Training loss 0.05384431034326553 Validation loss 0.0534605048596859 Accuracy 0.4580078125\n",
      "Iteration 48150 Training loss 0.050851039588451385 Validation loss 0.053436897695064545 Accuracy 0.458251953125\n",
      "Iteration 48160 Training loss 0.048791248351335526 Validation loss 0.05342148244380951 Accuracy 0.458251953125\n",
      "Iteration 48170 Training loss 0.05415307730436325 Validation loss 0.05316414311528206 Accuracy 0.460693359375\n",
      "Iteration 48180 Training loss 0.05078122764825821 Validation loss 0.05328923836350441 Accuracy 0.458984375\n",
      "Iteration 48190 Training loss 0.04953564330935478 Validation loss 0.053290095180273056 Accuracy 0.460693359375\n",
      "Iteration 48200 Training loss 0.05160647630691528 Validation loss 0.053319770842790604 Accuracy 0.459716796875\n",
      "Iteration 48210 Training loss 0.05092310905456543 Validation loss 0.053145233541727066 Accuracy 0.460693359375\n",
      "Iteration 48220 Training loss 0.05329139158129692 Validation loss 0.05332048609852791 Accuracy 0.458740234375\n",
      "Iteration 48230 Training loss 0.05343671515583992 Validation loss 0.05356772243976593 Accuracy 0.458251953125\n",
      "Iteration 48240 Training loss 0.05373680964112282 Validation loss 0.053515251725912094 Accuracy 0.457763671875\n",
      "Iteration 48250 Training loss 0.052902769297361374 Validation loss 0.05320991203188896 Accuracy 0.460205078125\n",
      "Iteration 48260 Training loss 0.051527298986911774 Validation loss 0.053413357585668564 Accuracy 0.458984375\n",
      "Iteration 48270 Training loss 0.04980151355266571 Validation loss 0.05341655761003494 Accuracy 0.458984375\n",
      "Iteration 48280 Training loss 0.05225638672709465 Validation loss 0.053665656596422195 Accuracy 0.45556640625\n",
      "Iteration 48290 Training loss 0.05195409059524536 Validation loss 0.05324793979525566 Accuracy 0.4609375\n",
      "Iteration 48300 Training loss 0.04853005334734917 Validation loss 0.0531308688223362 Accuracy 0.460205078125\n",
      "Iteration 48310 Training loss 0.055214762687683105 Validation loss 0.05428091809153557 Accuracy 0.4501953125\n",
      "Iteration 48320 Training loss 0.05237486585974693 Validation loss 0.05454693362116814 Accuracy 0.447509765625\n",
      "Iteration 48330 Training loss 0.052097517997026443 Validation loss 0.05311932414770126 Accuracy 0.460693359375\n",
      "Iteration 48340 Training loss 0.05560120940208435 Validation loss 0.05329680070281029 Accuracy 0.458984375\n",
      "Iteration 48350 Training loss 0.05086737498641014 Validation loss 0.053108636289834976 Accuracy 0.461181640625\n",
      "Iteration 48360 Training loss 0.0498904213309288 Validation loss 0.053441982716321945 Accuracy 0.459228515625\n",
      "Iteration 48370 Training loss 0.051393087953329086 Validation loss 0.05347074940800667 Accuracy 0.45703125\n",
      "Iteration 48380 Training loss 0.051136892288923264 Validation loss 0.053413767367601395 Accuracy 0.458984375\n",
      "Iteration 48390 Training loss 0.05549110844731331 Validation loss 0.05352048575878143 Accuracy 0.457763671875\n",
      "Iteration 48400 Training loss 0.0531894713640213 Validation loss 0.05364426225423813 Accuracy 0.45703125\n",
      "Iteration 48410 Training loss 0.05259581282734871 Validation loss 0.05314387381076813 Accuracy 0.461181640625\n",
      "Iteration 48420 Training loss 0.05438913404941559 Validation loss 0.053552307188510895 Accuracy 0.456298828125\n",
      "Iteration 48430 Training loss 0.04869047552347183 Validation loss 0.05345817282795906 Accuracy 0.460205078125\n",
      "Iteration 48440 Training loss 0.047979217022657394 Validation loss 0.05319187790155411 Accuracy 0.460205078125\n",
      "Iteration 48450 Training loss 0.05101662129163742 Validation loss 0.053115926682949066 Accuracy 0.461669921875\n",
      "Iteration 48460 Training loss 0.053389985114336014 Validation loss 0.05336569994688034 Accuracy 0.4580078125\n",
      "Iteration 48470 Training loss 0.05273791775107384 Validation loss 0.053373392671346664 Accuracy 0.459228515625\n",
      "Iteration 48480 Training loss 0.05572256073355675 Validation loss 0.05349377915263176 Accuracy 0.4580078125\n",
      "Iteration 48490 Training loss 0.05083613097667694 Validation loss 0.05332371965050697 Accuracy 0.46044921875\n",
      "Iteration 48500 Training loss 0.04918272793292999 Validation loss 0.0536232590675354 Accuracy 0.456298828125\n",
      "Iteration 48510 Training loss 0.051366183906793594 Validation loss 0.05385236069560051 Accuracy 0.45556640625\n",
      "Iteration 48520 Training loss 0.05033831670880318 Validation loss 0.053186044096946716 Accuracy 0.46044921875\n",
      "Iteration 48530 Training loss 0.05295206606388092 Validation loss 0.053177107125520706 Accuracy 0.460693359375\n",
      "Iteration 48540 Training loss 0.05024309083819389 Validation loss 0.05318928137421608 Accuracy 0.460205078125\n",
      "Iteration 48550 Training loss 0.051465556025505066 Validation loss 0.0532279834151268 Accuracy 0.4599609375\n",
      "Iteration 48560 Training loss 0.051302626729011536 Validation loss 0.05337929725646973 Accuracy 0.458740234375\n",
      "Iteration 48570 Training loss 0.051309071481227875 Validation loss 0.05305825173854828 Accuracy 0.461181640625\n",
      "Iteration 48580 Training loss 0.05611701309680939 Validation loss 0.05326583981513977 Accuracy 0.459228515625\n",
      "Iteration 48590 Training loss 0.04772935435175896 Validation loss 0.0533439926803112 Accuracy 0.45947265625\n",
      "Iteration 48600 Training loss 0.049952778965234756 Validation loss 0.053199853748083115 Accuracy 0.4609375\n",
      "Iteration 48610 Training loss 0.04871787503361702 Validation loss 0.05342022329568863 Accuracy 0.459228515625\n",
      "Iteration 48620 Training loss 0.05009548366069794 Validation loss 0.053359560668468475 Accuracy 0.46044921875\n",
      "Iteration 48630 Training loss 0.050386637449264526 Validation loss 0.05330852046608925 Accuracy 0.45849609375\n",
      "Iteration 48640 Training loss 0.04757082462310791 Validation loss 0.053049277514219284 Accuracy 0.460693359375\n",
      "Iteration 48650 Training loss 0.05347249284386635 Validation loss 0.05320417881011963 Accuracy 0.4599609375\n",
      "Iteration 48660 Training loss 0.05169301480054855 Validation loss 0.05350731313228607 Accuracy 0.45849609375\n",
      "Iteration 48670 Training loss 0.051055923104286194 Validation loss 0.05333338677883148 Accuracy 0.458740234375\n",
      "Iteration 48680 Training loss 0.04981711879372597 Validation loss 0.05332125723361969 Accuracy 0.45849609375\n",
      "Iteration 48690 Training loss 0.05156074836850166 Validation loss 0.05334882810711861 Accuracy 0.458740234375\n",
      "Iteration 48700 Training loss 0.0508560948073864 Validation loss 0.05319702997803688 Accuracy 0.4609375\n",
      "Iteration 48710 Training loss 0.053946610540151596 Validation loss 0.05314163863658905 Accuracy 0.4599609375\n",
      "Iteration 48720 Training loss 0.04674290865659714 Validation loss 0.05368111655116081 Accuracy 0.456298828125\n",
      "Iteration 48730 Training loss 0.0521043986082077 Validation loss 0.05352506786584854 Accuracy 0.45751953125\n",
      "Iteration 48740 Training loss 0.05418584495782852 Validation loss 0.054249294102191925 Accuracy 0.449951171875\n",
      "Iteration 48750 Training loss 0.05537402257323265 Validation loss 0.05370781943202019 Accuracy 0.455810546875\n",
      "Iteration 48760 Training loss 0.05183674022555351 Validation loss 0.05334532633423805 Accuracy 0.459228515625\n",
      "Iteration 48770 Training loss 0.05041321739554405 Validation loss 0.0536300428211689 Accuracy 0.45751953125\n",
      "Iteration 48780 Training loss 0.05259319022297859 Validation loss 0.053309667855501175 Accuracy 0.459716796875\n",
      "Iteration 48790 Training loss 0.04878947511315346 Validation loss 0.053498465567827225 Accuracy 0.4580078125\n",
      "Iteration 48800 Training loss 0.05139923468232155 Validation loss 0.053514618426561356 Accuracy 0.4580078125\n",
      "Iteration 48810 Training loss 0.051161009818315506 Validation loss 0.05319038778543472 Accuracy 0.4599609375\n",
      "Iteration 48820 Training loss 0.050344791263341904 Validation loss 0.05325496941804886 Accuracy 0.458984375\n",
      "Iteration 48830 Training loss 0.05302730202674866 Validation loss 0.05421169847249985 Accuracy 0.45263671875\n",
      "Iteration 48840 Training loss 0.05099158734083176 Validation loss 0.053323231637477875 Accuracy 0.45947265625\n",
      "Iteration 48850 Training loss 0.04983813315629959 Validation loss 0.053495343774557114 Accuracy 0.458251953125\n",
      "Iteration 48860 Training loss 0.05183832347393036 Validation loss 0.053185511380434036 Accuracy 0.4609375\n",
      "Iteration 48870 Training loss 0.05052245035767555 Validation loss 0.05320470407605171 Accuracy 0.4599609375\n",
      "Iteration 48880 Training loss 0.05091071501374245 Validation loss 0.05360189452767372 Accuracy 0.45751953125\n",
      "Iteration 48890 Training loss 0.050291188061237335 Validation loss 0.0534781739115715 Accuracy 0.458740234375\n",
      "Iteration 48900 Training loss 0.05075133219361305 Validation loss 0.0533851757645607 Accuracy 0.459228515625\n",
      "Iteration 48910 Training loss 0.05377620458602905 Validation loss 0.05322848632931709 Accuracy 0.4599609375\n",
      "Iteration 48920 Training loss 0.05302900820970535 Validation loss 0.05314938351511955 Accuracy 0.460205078125\n",
      "Iteration 48930 Training loss 0.05280543863773346 Validation loss 0.05337878689169884 Accuracy 0.4580078125\n",
      "Iteration 48940 Training loss 0.052647266536951065 Validation loss 0.0535152330994606 Accuracy 0.457763671875\n",
      "Iteration 48950 Training loss 0.0574650876224041 Validation loss 0.05334094539284706 Accuracy 0.458984375\n",
      "Iteration 48960 Training loss 0.04990953579545021 Validation loss 0.053139686584472656 Accuracy 0.46044921875\n",
      "Iteration 48970 Training loss 0.05200406163930893 Validation loss 0.05377541482448578 Accuracy 0.454833984375\n",
      "Iteration 48980 Training loss 0.05122515186667442 Validation loss 0.05316365510225296 Accuracy 0.460205078125\n",
      "Iteration 48990 Training loss 0.05488619953393936 Validation loss 0.05327502265572548 Accuracy 0.459228515625\n",
      "Iteration 49000 Training loss 0.05266178399324417 Validation loss 0.0532163642346859 Accuracy 0.46044921875\n",
      "Iteration 49010 Training loss 0.05115669593214989 Validation loss 0.05343940109014511 Accuracy 0.45751953125\n",
      "Iteration 49020 Training loss 0.053703270852565765 Validation loss 0.05315246060490608 Accuracy 0.460205078125\n",
      "Iteration 49030 Training loss 0.05358516424894333 Validation loss 0.05314856767654419 Accuracy 0.460693359375\n",
      "Iteration 49040 Training loss 0.05501231178641319 Validation loss 0.053473591804504395 Accuracy 0.4580078125\n",
      "Iteration 49050 Training loss 0.052770547568798065 Validation loss 0.05340384319424629 Accuracy 0.456787109375\n",
      "Iteration 49060 Training loss 0.05174893140792847 Validation loss 0.053351592272520065 Accuracy 0.460205078125\n",
      "Iteration 49070 Training loss 0.05274602025747299 Validation loss 0.05346965789794922 Accuracy 0.4580078125\n",
      "Iteration 49080 Training loss 0.05348820984363556 Validation loss 0.05353960767388344 Accuracy 0.45654296875\n",
      "Iteration 49090 Training loss 0.05282341316342354 Validation loss 0.05334196984767914 Accuracy 0.459716796875\n",
      "Iteration 49100 Training loss 0.0500730536878109 Validation loss 0.0533621571958065 Accuracy 0.459716796875\n",
      "Iteration 49110 Training loss 0.051946964114904404 Validation loss 0.0534711517393589 Accuracy 0.45849609375\n",
      "Iteration 49120 Training loss 0.05353555828332901 Validation loss 0.053901687264442444 Accuracy 0.45556640625\n",
      "Iteration 49130 Training loss 0.049357593059539795 Validation loss 0.05317777395248413 Accuracy 0.460693359375\n",
      "Iteration 49140 Training loss 0.052046895027160645 Validation loss 0.053302034735679626 Accuracy 0.45849609375\n",
      "Iteration 49150 Training loss 0.05569889768958092 Validation loss 0.05337768420577049 Accuracy 0.4599609375\n",
      "Iteration 49160 Training loss 0.05364784598350525 Validation loss 0.05377015471458435 Accuracy 0.455078125\n",
      "Iteration 49170 Training loss 0.05288821458816528 Validation loss 0.0532299168407917 Accuracy 0.459716796875\n",
      "Iteration 49180 Training loss 0.05178390070796013 Validation loss 0.05384613201022148 Accuracy 0.4541015625\n",
      "Iteration 49190 Training loss 0.05336589366197586 Validation loss 0.053557831794023514 Accuracy 0.456787109375\n",
      "Iteration 49200 Training loss 0.04985221475362778 Validation loss 0.05348488315939903 Accuracy 0.458251953125\n",
      "Iteration 49210 Training loss 0.052271757274866104 Validation loss 0.05336279794573784 Accuracy 0.45947265625\n",
      "Iteration 49220 Training loss 0.044697631150484085 Validation loss 0.05322888493537903 Accuracy 0.460205078125\n",
      "Iteration 49230 Training loss 0.05307275801897049 Validation loss 0.05336971580982208 Accuracy 0.458984375\n",
      "Iteration 49240 Training loss 0.05591337010264397 Validation loss 0.05514153093099594 Accuracy 0.444580078125\n",
      "Iteration 49250 Training loss 0.0502859391272068 Validation loss 0.05339382588863373 Accuracy 0.45849609375\n",
      "Iteration 49260 Training loss 0.05187683925032616 Validation loss 0.05316349118947983 Accuracy 0.460205078125\n",
      "Iteration 49270 Training loss 0.05443734675645828 Validation loss 0.05329059809446335 Accuracy 0.458984375\n",
      "Iteration 49280 Training loss 0.04948500916361809 Validation loss 0.053395334631204605 Accuracy 0.460205078125\n",
      "Iteration 49290 Training loss 0.05176426097750664 Validation loss 0.05332821607589722 Accuracy 0.459228515625\n",
      "Iteration 49300 Training loss 0.052157022058963776 Validation loss 0.05342148244380951 Accuracy 0.45703125\n",
      "Iteration 49310 Training loss 0.0510263554751873 Validation loss 0.05351248010993004 Accuracy 0.458251953125\n",
      "Iteration 49320 Training loss 0.05141616240143776 Validation loss 0.05331248790025711 Accuracy 0.458740234375\n",
      "Iteration 49330 Training loss 0.05182164907455444 Validation loss 0.05344467610120773 Accuracy 0.459716796875\n",
      "Iteration 49340 Training loss 0.051828496158123016 Validation loss 0.053294580429792404 Accuracy 0.458251953125\n",
      "Iteration 49350 Training loss 0.05175652354955673 Validation loss 0.053547244518995285 Accuracy 0.45751953125\n",
      "Iteration 49360 Training loss 0.050883010029792786 Validation loss 0.05351496860384941 Accuracy 0.45849609375\n",
      "Iteration 49370 Training loss 0.05129272863268852 Validation loss 0.053428493440151215 Accuracy 0.458984375\n",
      "Iteration 49380 Training loss 0.049960989505052567 Validation loss 0.05342138558626175 Accuracy 0.458740234375\n",
      "Iteration 49390 Training loss 0.050210051238536835 Validation loss 0.05357996001839638 Accuracy 0.45849609375\n",
      "Iteration 49400 Training loss 0.050767526030540466 Validation loss 0.05342058092355728 Accuracy 0.458740234375\n",
      "Iteration 49410 Training loss 0.050130829215049744 Validation loss 0.05329892039299011 Accuracy 0.458984375\n",
      "Iteration 49420 Training loss 0.05013623833656311 Validation loss 0.053359877318143845 Accuracy 0.458251953125\n",
      "Iteration 49430 Training loss 0.050097208470106125 Validation loss 0.053554460406303406 Accuracy 0.45751953125\n",
      "Iteration 49440 Training loss 0.04931172728538513 Validation loss 0.053706321865320206 Accuracy 0.457275390625\n",
      "Iteration 49450 Training loss 0.04985242709517479 Validation loss 0.05339248850941658 Accuracy 0.45849609375\n",
      "Iteration 49460 Training loss 0.050085894763469696 Validation loss 0.05360274761915207 Accuracy 0.4560546875\n",
      "Iteration 49470 Training loss 0.04875153675675392 Validation loss 0.05328476428985596 Accuracy 0.45849609375\n",
      "Iteration 49480 Training loss 0.05072278156876564 Validation loss 0.05323657765984535 Accuracy 0.458984375\n",
      "Iteration 49490 Training loss 0.05169660225510597 Validation loss 0.053348422050476074 Accuracy 0.458984375\n",
      "Iteration 49500 Training loss 0.052122119814157486 Validation loss 0.05337296426296234 Accuracy 0.45947265625\n",
      "Iteration 49510 Training loss 0.04988936707377434 Validation loss 0.05379024147987366 Accuracy 0.455322265625\n",
      "Iteration 49520 Training loss 0.04957899823784828 Validation loss 0.05371788889169693 Accuracy 0.456298828125\n",
      "Iteration 49530 Training loss 0.053708791732788086 Validation loss 0.053410641849040985 Accuracy 0.458984375\n",
      "Iteration 49540 Training loss 0.053902581334114075 Validation loss 0.053642790764570236 Accuracy 0.45654296875\n",
      "Iteration 49550 Training loss 0.05261315405368805 Validation loss 0.05341283977031708 Accuracy 0.45849609375\n",
      "Iteration 49560 Training loss 0.05330248922109604 Validation loss 0.05360914766788483 Accuracy 0.456787109375\n",
      "Iteration 49570 Training loss 0.050937261432409286 Validation loss 0.053366970270872116 Accuracy 0.459228515625\n",
      "Iteration 49580 Training loss 0.0475672110915184 Validation loss 0.05324012413620949 Accuracy 0.458740234375\n",
      "Iteration 49590 Training loss 0.052971936762332916 Validation loss 0.05347682908177376 Accuracy 0.45751953125\n",
      "Iteration 49600 Training loss 0.049170978367328644 Validation loss 0.05321716517210007 Accuracy 0.459716796875\n",
      "Iteration 49610 Training loss 0.05181289464235306 Validation loss 0.053208641707897186 Accuracy 0.458984375\n",
      "Iteration 49620 Training loss 0.05777590349316597 Validation loss 0.05370345711708069 Accuracy 0.45751953125\n",
      "Iteration 49630 Training loss 0.051864296197891235 Validation loss 0.05337738245725632 Accuracy 0.458984375\n",
      "Iteration 49640 Training loss 0.05111689120531082 Validation loss 0.053354255855083466 Accuracy 0.4599609375\n",
      "Iteration 49650 Training loss 0.04765281453728676 Validation loss 0.05335138365626335 Accuracy 0.459228515625\n",
      "Iteration 49660 Training loss 0.05266919359564781 Validation loss 0.05328228324651718 Accuracy 0.458740234375\n",
      "Iteration 49670 Training loss 0.0488087423145771 Validation loss 0.053485333919525146 Accuracy 0.456787109375\n",
      "Iteration 49680 Training loss 0.05054700747132301 Validation loss 0.05346110835671425 Accuracy 0.459228515625\n",
      "Iteration 49690 Training loss 0.04818250611424446 Validation loss 0.05322995036840439 Accuracy 0.46044921875\n",
      "Iteration 49700 Training loss 0.05231880024075508 Validation loss 0.05385007709264755 Accuracy 0.453125\n",
      "Iteration 49710 Training loss 0.053333181887865067 Validation loss 0.05326279625296593 Accuracy 0.4599609375\n",
      "Iteration 49720 Training loss 0.05354432761669159 Validation loss 0.053335487842559814 Accuracy 0.460205078125\n",
      "Iteration 49730 Training loss 0.04974941909313202 Validation loss 0.05320246145129204 Accuracy 0.459716796875\n",
      "Iteration 49740 Training loss 0.051003988832235336 Validation loss 0.053418729454278946 Accuracy 0.459228515625\n",
      "Iteration 49750 Training loss 0.050256162881851196 Validation loss 0.053574226796627045 Accuracy 0.45849609375\n",
      "Iteration 49760 Training loss 0.050199076533317566 Validation loss 0.05377793684601784 Accuracy 0.45654296875\n",
      "Iteration 49770 Training loss 0.051638517528772354 Validation loss 0.05325218662619591 Accuracy 0.459228515625\n",
      "Iteration 49780 Training loss 0.053309112787246704 Validation loss 0.05349870026111603 Accuracy 0.458251953125\n",
      "Iteration 49790 Training loss 0.051276564598083496 Validation loss 0.05323580279946327 Accuracy 0.457763671875\n",
      "Iteration 49800 Training loss 0.052243445068597794 Validation loss 0.053245242685079575 Accuracy 0.459228515625\n",
      "Iteration 49810 Training loss 0.049771443009376526 Validation loss 0.05338187888264656 Accuracy 0.458740234375\n",
      "Iteration 49820 Training loss 0.05071812868118286 Validation loss 0.05355760455131531 Accuracy 0.45703125\n",
      "Iteration 49830 Training loss 0.053467921912670135 Validation loss 0.05364936217665672 Accuracy 0.45654296875\n",
      "Iteration 49840 Training loss 0.05161624774336815 Validation loss 0.05349293723702431 Accuracy 0.458251953125\n",
      "Iteration 49850 Training loss 0.050348035991191864 Validation loss 0.0532856248319149 Accuracy 0.458251953125\n",
      "Iteration 49860 Training loss 0.053769566118717194 Validation loss 0.0532531812787056 Accuracy 0.458740234375\n",
      "Iteration 49870 Training loss 0.05152914300560951 Validation loss 0.053518787026405334 Accuracy 0.45751953125\n",
      "Iteration 49880 Training loss 0.05255061388015747 Validation loss 0.05324847623705864 Accuracy 0.459716796875\n",
      "Iteration 49890 Training loss 0.05049199238419533 Validation loss 0.05322954058647156 Accuracy 0.459716796875\n",
      "Iteration 49900 Training loss 0.05184389650821686 Validation loss 0.05360328033566475 Accuracy 0.45751953125\n",
      "Iteration 49910 Training loss 0.05283224582672119 Validation loss 0.05320369824767113 Accuracy 0.461181640625\n",
      "Iteration 49920 Training loss 0.049967482686042786 Validation loss 0.05347811430692673 Accuracy 0.4580078125\n",
      "Iteration 49930 Training loss 0.05465691164135933 Validation loss 0.05323276296257973 Accuracy 0.460205078125\n",
      "Iteration 49940 Training loss 0.0486791729927063 Validation loss 0.053275126963853836 Accuracy 0.460205078125\n",
      "Iteration 49950 Training loss 0.0522562712430954 Validation loss 0.0534907765686512 Accuracy 0.458740234375\n",
      "Iteration 49960 Training loss 0.05240527167916298 Validation loss 0.05361505597829819 Accuracy 0.457275390625\n",
      "Iteration 49970 Training loss 0.053307462483644485 Validation loss 0.05361014977097511 Accuracy 0.456298828125\n",
      "Iteration 49980 Training loss 0.0516386441886425 Validation loss 0.053317517042160034 Accuracy 0.458740234375\n",
      "Iteration 49990 Training loss 0.05402486026287079 Validation loss 0.054080765694379807 Accuracy 0.451904296875\n",
      "Iteration 50000 Training loss 0.05038804933428764 Validation loss 0.05381787568330765 Accuracy 0.455078125\n",
      "Iteration 50010 Training loss 0.05128150433301926 Validation loss 0.05364536494016647 Accuracy 0.456298828125\n",
      "Iteration 50020 Training loss 0.049631357192993164 Validation loss 0.0532107912003994 Accuracy 0.460205078125\n",
      "Iteration 50030 Training loss 0.052511297166347504 Validation loss 0.053254980593919754 Accuracy 0.458740234375\n",
      "Iteration 50040 Training loss 0.05096694827079773 Validation loss 0.053510960191488266 Accuracy 0.4580078125\n",
      "Iteration 50050 Training loss 0.0499405711889267 Validation loss 0.053099099546670914 Accuracy 0.46044921875\n",
      "Iteration 50060 Training loss 0.052313536405563354 Validation loss 0.053683631122112274 Accuracy 0.457275390625\n",
      "Iteration 50070 Training loss 0.0470934733748436 Validation loss 0.053341180086135864 Accuracy 0.4599609375\n",
      "Iteration 50080 Training loss 0.052848704159259796 Validation loss 0.05311952903866768 Accuracy 0.461181640625\n",
      "Iteration 50090 Training loss 0.05103686451911926 Validation loss 0.05345854535698891 Accuracy 0.45849609375\n",
      "Iteration 50100 Training loss 0.049152933061122894 Validation loss 0.053454138338565826 Accuracy 0.458740234375\n",
      "Iteration 50110 Training loss 0.052862461656332016 Validation loss 0.05310966074466705 Accuracy 0.46142578125\n",
      "Iteration 50120 Training loss 0.05348340794444084 Validation loss 0.0529920868575573 Accuracy 0.46142578125\n",
      "Iteration 50130 Training loss 0.05275960639119148 Validation loss 0.053307026624679565 Accuracy 0.459716796875\n",
      "Iteration 50140 Training loss 0.05369381606578827 Validation loss 0.05345678701996803 Accuracy 0.458740234375\n",
      "Iteration 50150 Training loss 0.04961186647415161 Validation loss 0.053394071757793427 Accuracy 0.458251953125\n",
      "Iteration 50160 Training loss 0.054625336080789566 Validation loss 0.05332879349589348 Accuracy 0.45947265625\n",
      "Iteration 50170 Training loss 0.05092678964138031 Validation loss 0.053066469728946686 Accuracy 0.460693359375\n",
      "Iteration 50180 Training loss 0.05221361666917801 Validation loss 0.05324665457010269 Accuracy 0.460693359375\n",
      "Iteration 50190 Training loss 0.048939451575279236 Validation loss 0.05338466539978981 Accuracy 0.458984375\n",
      "Iteration 50200 Training loss 0.04948623105883598 Validation loss 0.0531395822763443 Accuracy 0.4609375\n",
      "Iteration 50210 Training loss 0.053917430341243744 Validation loss 0.053405117243528366 Accuracy 0.458984375\n",
      "Iteration 50220 Training loss 0.05267221853137016 Validation loss 0.05370490625500679 Accuracy 0.457763671875\n",
      "Iteration 50230 Training loss 0.05288095399737358 Validation loss 0.05366097390651703 Accuracy 0.45654296875\n",
      "Iteration 50240 Training loss 0.0527629479765892 Validation loss 0.05371212214231491 Accuracy 0.453857421875\n",
      "Iteration 50250 Training loss 0.0548006035387516 Validation loss 0.05355481430888176 Accuracy 0.4580078125\n",
      "Iteration 50260 Training loss 0.048211365938186646 Validation loss 0.053125154227018356 Accuracy 0.460693359375\n",
      "Iteration 50270 Training loss 0.05358036234974861 Validation loss 0.053401023149490356 Accuracy 0.457763671875\n",
      "Iteration 50280 Training loss 0.04773075133562088 Validation loss 0.05338585749268532 Accuracy 0.45849609375\n",
      "Iteration 50290 Training loss 0.048003245145082474 Validation loss 0.0532725565135479 Accuracy 0.460693359375\n",
      "Iteration 50300 Training loss 0.050370749086141586 Validation loss 0.053753580898046494 Accuracy 0.455810546875\n",
      "Iteration 50310 Training loss 0.049408864229917526 Validation loss 0.053818222135305405 Accuracy 0.454345703125\n",
      "Iteration 50320 Training loss 0.05484181270003319 Validation loss 0.053677093237638474 Accuracy 0.457275390625\n",
      "Iteration 50330 Training loss 0.0505916066467762 Validation loss 0.05385593697428703 Accuracy 0.45556640625\n",
      "Iteration 50340 Training loss 0.048624929040670395 Validation loss 0.05407807603478432 Accuracy 0.453369140625\n",
      "Iteration 50350 Training loss 0.0536373071372509 Validation loss 0.05317364260554314 Accuracy 0.4599609375\n",
      "Iteration 50360 Training loss 0.05240166187286377 Validation loss 0.0534764789044857 Accuracy 0.458984375\n",
      "Iteration 50370 Training loss 0.053260333836078644 Validation loss 0.05313025414943695 Accuracy 0.46044921875\n",
      "Iteration 50380 Training loss 0.05581693351268768 Validation loss 0.05388099327683449 Accuracy 0.45458984375\n",
      "Iteration 50390 Training loss 0.053098179399967194 Validation loss 0.05348806083202362 Accuracy 0.4580078125\n",
      "Iteration 50400 Training loss 0.04882531613111496 Validation loss 0.05302434414625168 Accuracy 0.461669921875\n",
      "Iteration 50410 Training loss 0.05076102167367935 Validation loss 0.053228698670864105 Accuracy 0.4599609375\n",
      "Iteration 50420 Training loss 0.04598661884665489 Validation loss 0.05316601321101189 Accuracy 0.461181640625\n",
      "Iteration 50430 Training loss 0.05338640138506889 Validation loss 0.05330774188041687 Accuracy 0.459716796875\n",
      "Iteration 50440 Training loss 0.05282478779554367 Validation loss 0.053401004523038864 Accuracy 0.458984375\n",
      "Iteration 50450 Training loss 0.053157880902290344 Validation loss 0.05345211923122406 Accuracy 0.4580078125\n",
      "Iteration 50460 Training loss 0.054433632642030716 Validation loss 0.053553514182567596 Accuracy 0.45703125\n",
      "Iteration 50470 Training loss 0.05164186656475067 Validation loss 0.0531766340136528 Accuracy 0.459716796875\n",
      "Iteration 50480 Training loss 0.05293808504939079 Validation loss 0.05324838310480118 Accuracy 0.459716796875\n",
      "Iteration 50490 Training loss 0.053893979638814926 Validation loss 0.053557947278022766 Accuracy 0.458251953125\n",
      "Iteration 50500 Training loss 0.051697827875614166 Validation loss 0.05319686233997345 Accuracy 0.460205078125\n",
      "Iteration 50510 Training loss 0.04968704283237457 Validation loss 0.05312400311231613 Accuracy 0.4599609375\n",
      "Iteration 50520 Training loss 0.051953352987766266 Validation loss 0.053308356553316116 Accuracy 0.4599609375\n",
      "Iteration 50530 Training loss 0.05008220300078392 Validation loss 0.05312070995569229 Accuracy 0.4609375\n",
      "Iteration 50540 Training loss 0.05472089722752571 Validation loss 0.053640734404325485 Accuracy 0.4580078125\n",
      "Iteration 50550 Training loss 0.05552651733160019 Validation loss 0.05367543175816536 Accuracy 0.454345703125\n",
      "Iteration 50560 Training loss 0.05118216946721077 Validation loss 0.05349382758140564 Accuracy 0.45849609375\n",
      "Iteration 50570 Training loss 0.0485723651945591 Validation loss 0.05330716818571091 Accuracy 0.459716796875\n",
      "Iteration 50580 Training loss 0.04963257908821106 Validation loss 0.053232673555612564 Accuracy 0.459716796875\n",
      "Iteration 50590 Training loss 0.05158618465065956 Validation loss 0.05337873846292496 Accuracy 0.4599609375\n",
      "Iteration 50600 Training loss 0.05359414592385292 Validation loss 0.05352583900094032 Accuracy 0.456787109375\n",
      "Iteration 50610 Training loss 0.05236101150512695 Validation loss 0.05317372828722 Accuracy 0.460205078125\n",
      "Iteration 50620 Training loss 0.05280042439699173 Validation loss 0.053269680589437485 Accuracy 0.460205078125\n",
      "Iteration 50630 Training loss 0.05172361806035042 Validation loss 0.053311366587877274 Accuracy 0.460693359375\n",
      "Iteration 50640 Training loss 0.0495438389480114 Validation loss 0.05344550311565399 Accuracy 0.45947265625\n",
      "Iteration 50650 Training loss 0.05408906564116478 Validation loss 0.05329020321369171 Accuracy 0.460205078125\n",
      "Iteration 50660 Training loss 0.053537093102931976 Validation loss 0.05351424589753151 Accuracy 0.45849609375\n",
      "Iteration 50670 Training loss 0.05181567370891571 Validation loss 0.05325587838888168 Accuracy 0.460693359375\n",
      "Iteration 50680 Training loss 0.0508800745010376 Validation loss 0.053325776010751724 Accuracy 0.46044921875\n",
      "Iteration 50690 Training loss 0.0505225732922554 Validation loss 0.053156062960624695 Accuracy 0.46044921875\n",
      "Iteration 50700 Training loss 0.05114638805389404 Validation loss 0.0533401258289814 Accuracy 0.458251953125\n",
      "Iteration 50710 Training loss 0.051013264805078506 Validation loss 0.05417291447520256 Accuracy 0.451416015625\n",
      "Iteration 50720 Training loss 0.05032094195485115 Validation loss 0.05332520231604576 Accuracy 0.4599609375\n",
      "Iteration 50730 Training loss 0.05250444635748863 Validation loss 0.053372252732515335 Accuracy 0.45849609375\n",
      "Iteration 50740 Training loss 0.05126742646098137 Validation loss 0.05347957834601402 Accuracy 0.45751953125\n",
      "Iteration 50750 Training loss 0.04962809011340141 Validation loss 0.05350906774401665 Accuracy 0.458740234375\n",
      "Iteration 50760 Training loss 0.05357125401496887 Validation loss 0.053567733615636826 Accuracy 0.45751953125\n",
      "Iteration 50770 Training loss 0.051208510994911194 Validation loss 0.05340328812599182 Accuracy 0.45947265625\n",
      "Iteration 50780 Training loss 0.052096351981163025 Validation loss 0.053844891488552094 Accuracy 0.45458984375\n",
      "Iteration 50790 Training loss 0.053898002952337265 Validation loss 0.0534944087266922 Accuracy 0.458740234375\n",
      "Iteration 50800 Training loss 0.05048654228448868 Validation loss 0.05321275070309639 Accuracy 0.460205078125\n",
      "Iteration 50810 Training loss 0.04942777007818222 Validation loss 0.05341776832938194 Accuracy 0.45849609375\n",
      "Iteration 50820 Training loss 0.05154644325375557 Validation loss 0.05314953997731209 Accuracy 0.460693359375\n",
      "Iteration 50830 Training loss 0.050416234880685806 Validation loss 0.0533394031226635 Accuracy 0.460205078125\n",
      "Iteration 50840 Training loss 0.0518857017159462 Validation loss 0.05341550335288048 Accuracy 0.458740234375\n",
      "Iteration 50850 Training loss 0.051706429570913315 Validation loss 0.05396507680416107 Accuracy 0.455078125\n",
      "Iteration 50860 Training loss 0.04996570199728012 Validation loss 0.053092021495103836 Accuracy 0.460205078125\n",
      "Iteration 50870 Training loss 0.05187564715743065 Validation loss 0.05344047397375107 Accuracy 0.459228515625\n",
      "Iteration 50880 Training loss 0.05476221814751625 Validation loss 0.053505729883909225 Accuracy 0.456787109375\n",
      "Iteration 50890 Training loss 0.051000189036130905 Validation loss 0.05369485169649124 Accuracy 0.45654296875\n",
      "Iteration 50900 Training loss 0.05116904526948929 Validation loss 0.05315270274877548 Accuracy 0.459716796875\n",
      "Iteration 50910 Training loss 0.05090416967868805 Validation loss 0.05300218239426613 Accuracy 0.460693359375\n",
      "Iteration 50920 Training loss 0.04867446795105934 Validation loss 0.05329175665974617 Accuracy 0.45947265625\n",
      "Iteration 50930 Training loss 0.05049784108996391 Validation loss 0.053173016756772995 Accuracy 0.458984375\n",
      "Iteration 50940 Training loss 0.05003130063414574 Validation loss 0.05311174690723419 Accuracy 0.461181640625\n",
      "Iteration 50950 Training loss 0.051487985998392105 Validation loss 0.05352645739912987 Accuracy 0.458740234375\n",
      "Iteration 50960 Training loss 0.04859359189867973 Validation loss 0.053386904299259186 Accuracy 0.45849609375\n",
      "Iteration 50970 Training loss 0.05015593022108078 Validation loss 0.05314619094133377 Accuracy 0.460693359375\n",
      "Iteration 50980 Training loss 0.052457116544246674 Validation loss 0.053328078240156174 Accuracy 0.459716796875\n",
      "Iteration 50990 Training loss 0.05202696844935417 Validation loss 0.05335770547389984 Accuracy 0.459228515625\n",
      "Iteration 51000 Training loss 0.05097058042883873 Validation loss 0.053222011774778366 Accuracy 0.4599609375\n",
      "Iteration 51010 Training loss 0.04672408103942871 Validation loss 0.05319647490978241 Accuracy 0.460205078125\n",
      "Iteration 51020 Training loss 0.05476199463009834 Validation loss 0.05319857597351074 Accuracy 0.46044921875\n",
      "Iteration 51030 Training loss 0.05428439378738403 Validation loss 0.05329994112253189 Accuracy 0.46044921875\n",
      "Iteration 51040 Training loss 0.05531340464949608 Validation loss 0.05377739295363426 Accuracy 0.45654296875\n",
      "Iteration 51050 Training loss 0.05317433550953865 Validation loss 0.05385637655854225 Accuracy 0.45556640625\n",
      "Iteration 51060 Training loss 0.04793338105082512 Validation loss 0.05317734554409981 Accuracy 0.460205078125\n",
      "Iteration 51070 Training loss 0.05431094765663147 Validation loss 0.05400920286774635 Accuracy 0.4541015625\n",
      "Iteration 51080 Training loss 0.05101802945137024 Validation loss 0.053381141275167465 Accuracy 0.458984375\n",
      "Iteration 51090 Training loss 0.051450226455926895 Validation loss 0.05304010584950447 Accuracy 0.461181640625\n",
      "Iteration 51100 Training loss 0.046809062361717224 Validation loss 0.053583092987537384 Accuracy 0.458740234375\n",
      "Iteration 51110 Training loss 0.05576885864138603 Validation loss 0.05352813005447388 Accuracy 0.458251953125\n",
      "Iteration 51120 Training loss 0.05068992078304291 Validation loss 0.05308258533477783 Accuracy 0.4609375\n",
      "Iteration 51130 Training loss 0.04851970449090004 Validation loss 0.0534072145819664 Accuracy 0.458984375\n",
      "Iteration 51140 Training loss 0.053060926496982574 Validation loss 0.05311296880245209 Accuracy 0.460205078125\n",
      "Iteration 51150 Training loss 0.05223874747753143 Validation loss 0.05323553457856178 Accuracy 0.458984375\n",
      "Iteration 51160 Training loss 0.04953869432210922 Validation loss 0.05340784043073654 Accuracy 0.457275390625\n",
      "Iteration 51170 Training loss 0.049404725432395935 Validation loss 0.05384436249732971 Accuracy 0.454833984375\n",
      "Iteration 51180 Training loss 0.053555041551589966 Validation loss 0.0532042421400547 Accuracy 0.460693359375\n",
      "Iteration 51190 Training loss 0.04974003881216049 Validation loss 0.05344614014029503 Accuracy 0.458251953125\n",
      "Iteration 51200 Training loss 0.05383799225091934 Validation loss 0.05319608375430107 Accuracy 0.46044921875\n",
      "Iteration 51210 Training loss 0.04976982995867729 Validation loss 0.05357665941119194 Accuracy 0.456787109375\n",
      "Iteration 51220 Training loss 0.05481424555182457 Validation loss 0.05427580326795578 Accuracy 0.450927734375\n",
      "Iteration 51230 Training loss 0.04807377979159355 Validation loss 0.053373679518699646 Accuracy 0.459716796875\n",
      "Iteration 51240 Training loss 0.055468883365392685 Validation loss 0.05357269570231438 Accuracy 0.456787109375\n",
      "Iteration 51250 Training loss 0.05594069883227348 Validation loss 0.0531686507165432 Accuracy 0.460693359375\n",
      "Iteration 51260 Training loss 0.05217237398028374 Validation loss 0.05317717045545578 Accuracy 0.460693359375\n",
      "Iteration 51270 Training loss 0.05030297860503197 Validation loss 0.05380720645189285 Accuracy 0.4541015625\n",
      "Iteration 51280 Training loss 0.05049942806363106 Validation loss 0.05309682339429855 Accuracy 0.460693359375\n",
      "Iteration 51290 Training loss 0.050438668578863144 Validation loss 0.0533176027238369 Accuracy 0.4580078125\n",
      "Iteration 51300 Training loss 0.05320339277386665 Validation loss 0.053562019020318985 Accuracy 0.456787109375\n",
      "Iteration 51310 Training loss 0.050864581018686295 Validation loss 0.0538378581404686 Accuracy 0.455078125\n",
      "Iteration 51320 Training loss 0.052851445972919464 Validation loss 0.053492654114961624 Accuracy 0.456787109375\n",
      "Iteration 51330 Training loss 0.04839419573545456 Validation loss 0.05351231247186661 Accuracy 0.45751953125\n",
      "Iteration 51340 Training loss 0.04858094081282616 Validation loss 0.053133077919483185 Accuracy 0.461181640625\n",
      "Iteration 51350 Training loss 0.05392802879214287 Validation loss 0.05325433611869812 Accuracy 0.461181640625\n",
      "Iteration 51360 Training loss 0.050748977810144424 Validation loss 0.053136903792619705 Accuracy 0.462158203125\n",
      "Iteration 51370 Training loss 0.052319739013910294 Validation loss 0.053327858448028564 Accuracy 0.4609375\n",
      "Iteration 51380 Training loss 0.05229246988892555 Validation loss 0.0530790314078331 Accuracy 0.46142578125\n",
      "Iteration 51390 Training loss 0.050340332090854645 Validation loss 0.054317910224199295 Accuracy 0.44921875\n",
      "Iteration 51400 Training loss 0.04939386621117592 Validation loss 0.05328544229269028 Accuracy 0.460205078125\n",
      "Iteration 51410 Training loss 0.05161289498209953 Validation loss 0.05320572480559349 Accuracy 0.460693359375\n",
      "Iteration 51420 Training loss 0.053332626819610596 Validation loss 0.05357178673148155 Accuracy 0.4580078125\n",
      "Iteration 51430 Training loss 0.050343338400125504 Validation loss 0.05328197032213211 Accuracy 0.46044921875\n",
      "Iteration 51440 Training loss 0.050838880240917206 Validation loss 0.05325419455766678 Accuracy 0.461669921875\n",
      "Iteration 51450 Training loss 0.053186673671007156 Validation loss 0.053316667675971985 Accuracy 0.460205078125\n",
      "Iteration 51460 Training loss 0.053855061531066895 Validation loss 0.05320131778717041 Accuracy 0.459716796875\n",
      "Iteration 51470 Training loss 0.052239272743463516 Validation loss 0.053186122328042984 Accuracy 0.459716796875\n",
      "Iteration 51480 Training loss 0.049775172024965286 Validation loss 0.053244154900312424 Accuracy 0.460693359375\n",
      "Iteration 51490 Training loss 0.04901140183210373 Validation loss 0.053207751363515854 Accuracy 0.460205078125\n",
      "Iteration 51500 Training loss 0.053252968937158585 Validation loss 0.05367785692214966 Accuracy 0.45556640625\n",
      "Iteration 51510 Training loss 0.05285193771123886 Validation loss 0.05353863164782524 Accuracy 0.458251953125\n",
      "Iteration 51520 Training loss 0.04605666548013687 Validation loss 0.053730741143226624 Accuracy 0.45849609375\n",
      "Iteration 51530 Training loss 0.05106246843934059 Validation loss 0.053227175027132034 Accuracy 0.4609375\n",
      "Iteration 51540 Training loss 0.051906418055295944 Validation loss 0.053599774837493896 Accuracy 0.4580078125\n",
      "Iteration 51550 Training loss 0.04964040219783783 Validation loss 0.053533148020505905 Accuracy 0.458251953125\n",
      "Iteration 51560 Training loss 0.05196402594447136 Validation loss 0.053621698170900345 Accuracy 0.456787109375\n",
      "Iteration 51570 Training loss 0.04934954643249512 Validation loss 0.053104765713214874 Accuracy 0.460693359375\n",
      "Iteration 51580 Training loss 0.05075569078326225 Validation loss 0.053764089941978455 Accuracy 0.456787109375\n",
      "Iteration 51590 Training loss 0.051808807998895645 Validation loss 0.05323706939816475 Accuracy 0.459228515625\n",
      "Iteration 51600 Training loss 0.051937758922576904 Validation loss 0.05318266525864601 Accuracy 0.4599609375\n",
      "Iteration 51610 Training loss 0.04838884249329567 Validation loss 0.05326994135975838 Accuracy 0.45947265625\n",
      "Iteration 51620 Training loss 0.04902855306863785 Validation loss 0.053165022283792496 Accuracy 0.460205078125\n",
      "Iteration 51630 Training loss 0.056611742824316025 Validation loss 0.05373001843690872 Accuracy 0.456787109375\n",
      "Iteration 51640 Training loss 0.05214596912264824 Validation loss 0.05320169776678085 Accuracy 0.460693359375\n",
      "Iteration 51650 Training loss 0.05020570755004883 Validation loss 0.053300388157367706 Accuracy 0.45947265625\n",
      "Iteration 51660 Training loss 0.050423912703990936 Validation loss 0.05340385437011719 Accuracy 0.459228515625\n",
      "Iteration 51670 Training loss 0.05099146440625191 Validation loss 0.053590141236782074 Accuracy 0.456787109375\n",
      "Iteration 51680 Training loss 0.05223905295133591 Validation loss 0.05324335768818855 Accuracy 0.460205078125\n",
      "Iteration 51690 Training loss 0.0454883836209774 Validation loss 0.05364757776260376 Accuracy 0.4560546875\n",
      "Iteration 51700 Training loss 0.053449664264917374 Validation loss 0.05367906764149666 Accuracy 0.455810546875\n",
      "Iteration 51710 Training loss 0.05040765181183815 Validation loss 0.053183864802122116 Accuracy 0.461181640625\n",
      "Iteration 51720 Training loss 0.05319038778543472 Validation loss 0.05346199497580528 Accuracy 0.458740234375\n",
      "Iteration 51730 Training loss 0.04880063235759735 Validation loss 0.05329318717122078 Accuracy 0.4580078125\n",
      "Iteration 51740 Training loss 0.0506247952580452 Validation loss 0.05324423685669899 Accuracy 0.4609375\n",
      "Iteration 51750 Training loss 0.05154336616396904 Validation loss 0.05402910336852074 Accuracy 0.45361328125\n",
      "Iteration 51760 Training loss 0.05006399005651474 Validation loss 0.05341958999633789 Accuracy 0.458984375\n",
      "Iteration 51770 Training loss 0.05175967514514923 Validation loss 0.05329612269997597 Accuracy 0.458984375\n",
      "Iteration 51780 Training loss 0.04989227280020714 Validation loss 0.05317298695445061 Accuracy 0.460693359375\n",
      "Iteration 51790 Training loss 0.05121566355228424 Validation loss 0.053345970809459686 Accuracy 0.458984375\n",
      "Iteration 51800 Training loss 0.050179895013570786 Validation loss 0.05373002216219902 Accuracy 0.45751953125\n",
      "Iteration 51810 Training loss 0.05446140840649605 Validation loss 0.05378720909357071 Accuracy 0.455810546875\n",
      "Iteration 51820 Training loss 0.052810169756412506 Validation loss 0.0534687303006649 Accuracy 0.458984375\n",
      "Iteration 51830 Training loss 0.05479644611477852 Validation loss 0.05368064343929291 Accuracy 0.454345703125\n",
      "Iteration 51840 Training loss 0.050383616238832474 Validation loss 0.05323464795947075 Accuracy 0.45947265625\n",
      "Iteration 51850 Training loss 0.05203092843294144 Validation loss 0.054120540618896484 Accuracy 0.4521484375\n",
      "Iteration 51860 Training loss 0.05212792754173279 Validation loss 0.05308416858315468 Accuracy 0.460693359375\n",
      "Iteration 51870 Training loss 0.05118383839726448 Validation loss 0.0535062775015831 Accuracy 0.458740234375\n",
      "Iteration 51880 Training loss 0.04764970764517784 Validation loss 0.05340790003538132 Accuracy 0.458251953125\n",
      "Iteration 51890 Training loss 0.05121888220310211 Validation loss 0.0535091795027256 Accuracy 0.458740234375\n",
      "Iteration 51900 Training loss 0.049649596214294434 Validation loss 0.05328421667218208 Accuracy 0.45849609375\n",
      "Iteration 51910 Training loss 0.051349762827157974 Validation loss 0.05367637425661087 Accuracy 0.45654296875\n",
      "Iteration 51920 Training loss 0.05130904167890549 Validation loss 0.053219567984342575 Accuracy 0.4599609375\n",
      "Iteration 51930 Training loss 0.04890235885977745 Validation loss 0.0532364547252655 Accuracy 0.46044921875\n",
      "Iteration 51940 Training loss 0.05163615942001343 Validation loss 0.05340699106454849 Accuracy 0.458984375\n",
      "Iteration 51950 Training loss 0.05124590918421745 Validation loss 0.05314600467681885 Accuracy 0.4599609375\n",
      "Iteration 51960 Training loss 0.051599353551864624 Validation loss 0.053260643035173416 Accuracy 0.45947265625\n",
      "Iteration 51970 Training loss 0.056026436388492584 Validation loss 0.05320674553513527 Accuracy 0.4609375\n",
      "Iteration 51980 Training loss 0.05036791041493416 Validation loss 0.05335821956396103 Accuracy 0.45849609375\n",
      "Iteration 51990 Training loss 0.05154707655310631 Validation loss 0.05318845435976982 Accuracy 0.46044921875\n",
      "Iteration 52000 Training loss 0.04995213821530342 Validation loss 0.05327727273106575 Accuracy 0.45849609375\n",
      "Iteration 52010 Training loss 0.0491243377327919 Validation loss 0.053253572434186935 Accuracy 0.458984375\n",
      "Iteration 52020 Training loss 0.05113799124956131 Validation loss 0.05313945561647415 Accuracy 0.4609375\n",
      "Iteration 52030 Training loss 0.05335098132491112 Validation loss 0.05339095741510391 Accuracy 0.45947265625\n",
      "Iteration 52040 Training loss 0.047714732587337494 Validation loss 0.0533093586564064 Accuracy 0.458984375\n",
      "Iteration 52050 Training loss 0.05219953879714012 Validation loss 0.05321802943944931 Accuracy 0.460693359375\n",
      "Iteration 52060 Training loss 0.04978359863162041 Validation loss 0.053419023752212524 Accuracy 0.458740234375\n",
      "Iteration 52070 Training loss 0.05239139124751091 Validation loss 0.05315648764371872 Accuracy 0.45947265625\n",
      "Iteration 52080 Training loss 0.049141641706228256 Validation loss 0.05340086668729782 Accuracy 0.458984375\n",
      "Iteration 52090 Training loss 0.05034956708550453 Validation loss 0.053289901465177536 Accuracy 0.459228515625\n",
      "Iteration 52100 Training loss 0.05292950198054314 Validation loss 0.05335346236824989 Accuracy 0.459228515625\n",
      "Iteration 52110 Training loss 0.05042112618684769 Validation loss 0.053208813071250916 Accuracy 0.460205078125\n",
      "Iteration 52120 Training loss 0.04718748852610588 Validation loss 0.05338142812252045 Accuracy 0.459716796875\n",
      "Iteration 52130 Training loss 0.055600810796022415 Validation loss 0.053551506251096725 Accuracy 0.458251953125\n",
      "Iteration 52140 Training loss 0.05096302926540375 Validation loss 0.05324685573577881 Accuracy 0.46142578125\n",
      "Iteration 52150 Training loss 0.056284524500370026 Validation loss 0.05364520847797394 Accuracy 0.45556640625\n",
      "Iteration 52160 Training loss 0.05563312768936157 Validation loss 0.05318376421928406 Accuracy 0.46044921875\n",
      "Iteration 52170 Training loss 0.05081694573163986 Validation loss 0.05356760695576668 Accuracy 0.457763671875\n",
      "Iteration 52180 Training loss 0.05241735652089119 Validation loss 0.053470954298973083 Accuracy 0.458251953125\n",
      "Iteration 52190 Training loss 0.0486912727355957 Validation loss 0.052964285016059875 Accuracy 0.4619140625\n",
      "Iteration 52200 Training loss 0.05661238729953766 Validation loss 0.05347030609846115 Accuracy 0.458984375\n",
      "Iteration 52210 Training loss 0.05049135908484459 Validation loss 0.05403217673301697 Accuracy 0.45361328125\n",
      "Iteration 52220 Training loss 0.052327223122119904 Validation loss 0.05412006378173828 Accuracy 0.451171875\n",
      "Iteration 52230 Training loss 0.05577985569834709 Validation loss 0.05318324267864227 Accuracy 0.459716796875\n",
      "Iteration 52240 Training loss 0.05270182713866234 Validation loss 0.05329848453402519 Accuracy 0.459716796875\n",
      "Iteration 52250 Training loss 0.05348005145788193 Validation loss 0.05344587191939354 Accuracy 0.456787109375\n",
      "Iteration 52260 Training loss 0.05149234086275101 Validation loss 0.05319574102759361 Accuracy 0.459716796875\n",
      "Iteration 52270 Training loss 0.051736876368522644 Validation loss 0.05356287956237793 Accuracy 0.457275390625\n",
      "Iteration 52280 Training loss 0.05358000099658966 Validation loss 0.05318518728017807 Accuracy 0.459228515625\n",
      "Iteration 52290 Training loss 0.05114762485027313 Validation loss 0.05321730673313141 Accuracy 0.459716796875\n",
      "Iteration 52300 Training loss 0.050878167152404785 Validation loss 0.05367156118154526 Accuracy 0.456298828125\n",
      "Iteration 52310 Training loss 0.0534195639193058 Validation loss 0.053382545709609985 Accuracy 0.458251953125\n",
      "Iteration 52320 Training loss 0.053503673523664474 Validation loss 0.053558796644210815 Accuracy 0.456787109375\n",
      "Iteration 52330 Training loss 0.05296356603503227 Validation loss 0.053433287888765335 Accuracy 0.45849609375\n",
      "Iteration 52340 Training loss 0.05290926992893219 Validation loss 0.053413715213537216 Accuracy 0.459228515625\n",
      "Iteration 52350 Training loss 0.051438216120004654 Validation loss 0.05341312289237976 Accuracy 0.459228515625\n",
      "Iteration 52360 Training loss 0.05015888437628746 Validation loss 0.05363909527659416 Accuracy 0.457763671875\n",
      "Iteration 52370 Training loss 0.05355876684188843 Validation loss 0.0534982793033123 Accuracy 0.458984375\n",
      "Iteration 52380 Training loss 0.05175263434648514 Validation loss 0.05332491546869278 Accuracy 0.460693359375\n",
      "Iteration 52390 Training loss 0.05738062411546707 Validation loss 0.0534932017326355 Accuracy 0.45849609375\n",
      "Iteration 52400 Training loss 0.04942938685417175 Validation loss 0.05328678712248802 Accuracy 0.458984375\n",
      "Iteration 52410 Training loss 0.049538470804691315 Validation loss 0.05311949923634529 Accuracy 0.46142578125\n",
      "Iteration 52420 Training loss 0.0508618988096714 Validation loss 0.05326349660754204 Accuracy 0.458984375\n",
      "Iteration 52430 Training loss 0.04834472015500069 Validation loss 0.05312233790755272 Accuracy 0.460205078125\n",
      "Iteration 52440 Training loss 0.054317738860845566 Validation loss 0.05428570508956909 Accuracy 0.44921875\n",
      "Iteration 52450 Training loss 0.05340282991528511 Validation loss 0.05346556007862091 Accuracy 0.45947265625\n",
      "Iteration 52460 Training loss 0.05168867111206055 Validation loss 0.053111717104911804 Accuracy 0.460693359375\n",
      "Iteration 52470 Training loss 0.052670612931251526 Validation loss 0.053089689463377 Accuracy 0.460693359375\n",
      "Iteration 52480 Training loss 0.04696787893772125 Validation loss 0.05314343795180321 Accuracy 0.459716796875\n",
      "Iteration 52490 Training loss 0.05098303034901619 Validation loss 0.053178440779447556 Accuracy 0.46142578125\n",
      "Iteration 52500 Training loss 0.05259808152914047 Validation loss 0.05319780111312866 Accuracy 0.460693359375\n",
      "Iteration 52510 Training loss 0.05303240939974785 Validation loss 0.05344695225358009 Accuracy 0.457763671875\n",
      "Iteration 52520 Training loss 0.05064534395933151 Validation loss 0.053677987307310104 Accuracy 0.455810546875\n",
      "Iteration 52530 Training loss 0.050831399857997894 Validation loss 0.053172606974840164 Accuracy 0.46044921875\n",
      "Iteration 52540 Training loss 0.049041301012039185 Validation loss 0.05303429439663887 Accuracy 0.460693359375\n",
      "Iteration 52550 Training loss 0.05197839438915253 Validation loss 0.0532575286924839 Accuracy 0.459716796875\n",
      "Iteration 52560 Training loss 0.0534222237765789 Validation loss 0.05330795422196388 Accuracy 0.458984375\n",
      "Iteration 52570 Training loss 0.049351319670677185 Validation loss 0.05336182564496994 Accuracy 0.4599609375\n",
      "Iteration 52580 Training loss 0.05084936320781708 Validation loss 0.05311215668916702 Accuracy 0.4599609375\n",
      "Iteration 52590 Training loss 0.05292419716715813 Validation loss 0.05323391035199165 Accuracy 0.459228515625\n",
      "Iteration 52600 Training loss 0.05289914831519127 Validation loss 0.05317539721727371 Accuracy 0.45947265625\n",
      "Iteration 52610 Training loss 0.05206901952624321 Validation loss 0.053159162402153015 Accuracy 0.45947265625\n",
      "Iteration 52620 Training loss 0.05272746831178665 Validation loss 0.05302630737423897 Accuracy 0.460693359375\n",
      "Iteration 52630 Training loss 0.04967343062162399 Validation loss 0.05330035090446472 Accuracy 0.4599609375\n",
      "Iteration 52640 Training loss 0.05037284269928932 Validation loss 0.05315742641687393 Accuracy 0.46044921875\n",
      "Iteration 52650 Training loss 0.05806383118033409 Validation loss 0.05353565141558647 Accuracy 0.45751953125\n",
      "Iteration 52660 Training loss 0.053589172661304474 Validation loss 0.053178705275058746 Accuracy 0.459716796875\n",
      "Iteration 52670 Training loss 0.050660744309425354 Validation loss 0.053097374737262726 Accuracy 0.461181640625\n",
      "Iteration 52680 Training loss 0.05489650368690491 Validation loss 0.05303673818707466 Accuracy 0.461669921875\n",
      "Iteration 52690 Training loss 0.05486733093857765 Validation loss 0.05327258259057999 Accuracy 0.460205078125\n",
      "Iteration 52700 Training loss 0.0519159734249115 Validation loss 0.053412020206451416 Accuracy 0.45849609375\n",
      "Iteration 52710 Training loss 0.050851721316576004 Validation loss 0.05306630581617355 Accuracy 0.4609375\n",
      "Iteration 52720 Training loss 0.05142216011881828 Validation loss 0.05352797731757164 Accuracy 0.45849609375\n",
      "Iteration 52730 Training loss 0.04540227726101875 Validation loss 0.05314266309142113 Accuracy 0.459716796875\n",
      "Iteration 52740 Training loss 0.05363283306360245 Validation loss 0.053260769695043564 Accuracy 0.458740234375\n",
      "Iteration 52750 Training loss 0.05258866399526596 Validation loss 0.05340595915913582 Accuracy 0.4580078125\n",
      "Iteration 52760 Training loss 0.04927316680550575 Validation loss 0.05318265035748482 Accuracy 0.46044921875\n",
      "Iteration 52770 Training loss 0.05219672620296478 Validation loss 0.05339203029870987 Accuracy 0.45947265625\n",
      "Iteration 52780 Training loss 0.05483771860599518 Validation loss 0.05354031175374985 Accuracy 0.458984375\n",
      "Iteration 52790 Training loss 0.05098298192024231 Validation loss 0.053307756781578064 Accuracy 0.46044921875\n",
      "Iteration 52800 Training loss 0.05395514890551567 Validation loss 0.053941234946250916 Accuracy 0.45458984375\n",
      "Iteration 52810 Training loss 0.05313875153660774 Validation loss 0.05329596623778343 Accuracy 0.458984375\n",
      "Iteration 52820 Training loss 0.05103354901075363 Validation loss 0.053080298006534576 Accuracy 0.46142578125\n",
      "Iteration 52830 Training loss 0.05262896791100502 Validation loss 0.05299324914813042 Accuracy 0.461669921875\n",
      "Iteration 52840 Training loss 0.05369335785508156 Validation loss 0.05331505835056305 Accuracy 0.45751953125\n",
      "Iteration 52850 Training loss 0.054135918617248535 Validation loss 0.053405970335006714 Accuracy 0.458984375\n",
      "Iteration 52860 Training loss 0.046248551458120346 Validation loss 0.05313503369688988 Accuracy 0.4599609375\n",
      "Iteration 52870 Training loss 0.05229794234037399 Validation loss 0.05314181372523308 Accuracy 0.461669921875\n",
      "Iteration 52880 Training loss 0.05369164049625397 Validation loss 0.05332144349813461 Accuracy 0.460205078125\n",
      "Iteration 52890 Training loss 0.0516912005841732 Validation loss 0.05333356186747551 Accuracy 0.4599609375\n",
      "Iteration 52900 Training loss 0.054827865213155746 Validation loss 0.05363178625702858 Accuracy 0.458251953125\n",
      "Iteration 52910 Training loss 0.047211673110723495 Validation loss 0.053148336708545685 Accuracy 0.460693359375\n",
      "Iteration 52920 Training loss 0.052145831286907196 Validation loss 0.05312864109873772 Accuracy 0.4609375\n",
      "Iteration 52930 Training loss 0.055144552141427994 Validation loss 0.05422397330403328 Accuracy 0.449951171875\n",
      "Iteration 52940 Training loss 0.04874644801020622 Validation loss 0.05310800299048424 Accuracy 0.46044921875\n",
      "Iteration 52950 Training loss 0.053292132914066315 Validation loss 0.053064730018377304 Accuracy 0.461181640625\n",
      "Iteration 52960 Training loss 0.0507032684981823 Validation loss 0.053882673382759094 Accuracy 0.4541015625\n",
      "Iteration 52970 Training loss 0.05385769158601761 Validation loss 0.05333861708641052 Accuracy 0.458984375\n",
      "Iteration 52980 Training loss 0.05094611644744873 Validation loss 0.053615864366292953 Accuracy 0.4560546875\n",
      "Iteration 52990 Training loss 0.05288492888212204 Validation loss 0.05350256338715553 Accuracy 0.458984375\n",
      "Iteration 53000 Training loss 0.04810701310634613 Validation loss 0.0532769039273262 Accuracy 0.458251953125\n",
      "Iteration 53010 Training loss 0.05140318721532822 Validation loss 0.05369511991739273 Accuracy 0.456787109375\n",
      "Iteration 53020 Training loss 0.053058359771966934 Validation loss 0.054111771285533905 Accuracy 0.451171875\n",
      "Iteration 53030 Training loss 0.05176929011940956 Validation loss 0.053480129688978195 Accuracy 0.45849609375\n",
      "Iteration 53040 Training loss 0.05023075267672539 Validation loss 0.053627848625183105 Accuracy 0.458251953125\n",
      "Iteration 53050 Training loss 0.0520700067281723 Validation loss 0.05333448201417923 Accuracy 0.4599609375\n",
      "Iteration 53060 Training loss 0.04893932119011879 Validation loss 0.053287606686353683 Accuracy 0.45947265625\n",
      "Iteration 53070 Training loss 0.04747709259390831 Validation loss 0.05329173058271408 Accuracy 0.458984375\n",
      "Iteration 53080 Training loss 0.05377204343676567 Validation loss 0.05353669822216034 Accuracy 0.4580078125\n",
      "Iteration 53090 Training loss 0.049716439098119736 Validation loss 0.05326914042234421 Accuracy 0.460693359375\n",
      "Iteration 53100 Training loss 0.05345737561583519 Validation loss 0.05374300107359886 Accuracy 0.455810546875\n",
      "Iteration 53110 Training loss 0.05289853364229202 Validation loss 0.053567755967378616 Accuracy 0.4580078125\n",
      "Iteration 53120 Training loss 0.05217072367668152 Validation loss 0.053268857300281525 Accuracy 0.461181640625\n",
      "Iteration 53130 Training loss 0.05161524564027786 Validation loss 0.05322710797190666 Accuracy 0.460693359375\n",
      "Iteration 53140 Training loss 0.05041048675775528 Validation loss 0.05327498912811279 Accuracy 0.4599609375\n",
      "Iteration 53150 Training loss 0.051246341317892075 Validation loss 0.053412504494190216 Accuracy 0.458251953125\n",
      "Iteration 53160 Training loss 0.049732472747564316 Validation loss 0.053121596574783325 Accuracy 0.460693359375\n",
      "Iteration 53170 Training loss 0.051885101944208145 Validation loss 0.05372495949268341 Accuracy 0.455810546875\n",
      "Iteration 53180 Training loss 0.05014480650424957 Validation loss 0.05307455360889435 Accuracy 0.46044921875\n",
      "Iteration 53190 Training loss 0.052248451858758926 Validation loss 0.05333689972758293 Accuracy 0.45947265625\n",
      "Iteration 53200 Training loss 0.05073976516723633 Validation loss 0.05328839644789696 Accuracy 0.459716796875\n",
      "Iteration 53210 Training loss 0.05596424266695976 Validation loss 0.053488876670598984 Accuracy 0.459716796875\n",
      "Iteration 53220 Training loss 0.05088033527135849 Validation loss 0.05342039093375206 Accuracy 0.459716796875\n",
      "Iteration 53230 Training loss 0.05138318985700607 Validation loss 0.05344588682055473 Accuracy 0.4580078125\n",
      "Iteration 53240 Training loss 0.05508696287870407 Validation loss 0.053599726408720016 Accuracy 0.45751953125\n",
      "Iteration 53250 Training loss 0.05101947858929634 Validation loss 0.053537752479314804 Accuracy 0.45654296875\n",
      "Iteration 53260 Training loss 0.04612195864319801 Validation loss 0.053411815315485 Accuracy 0.45751953125\n",
      "Iteration 53270 Training loss 0.052042458206415176 Validation loss 0.05342628434300423 Accuracy 0.45849609375\n",
      "Iteration 53280 Training loss 0.05353517457842827 Validation loss 0.05325789749622345 Accuracy 0.459716796875\n",
      "Iteration 53290 Training loss 0.05248973146080971 Validation loss 0.05322088673710823 Accuracy 0.459716796875\n",
      "Iteration 53300 Training loss 0.050116781145334244 Validation loss 0.05315172299742699 Accuracy 0.459716796875\n",
      "Iteration 53310 Training loss 0.05137869343161583 Validation loss 0.053566593676805496 Accuracy 0.45654296875\n",
      "Iteration 53320 Training loss 0.04885243624448776 Validation loss 0.05387911945581436 Accuracy 0.453125\n",
      "Iteration 53330 Training loss 0.052652012556791306 Validation loss 0.05342589318752289 Accuracy 0.458251953125\n",
      "Iteration 53340 Training loss 0.05010377988219261 Validation loss 0.053348518908023834 Accuracy 0.458984375\n",
      "Iteration 53350 Training loss 0.04962228238582611 Validation loss 0.053160615265369415 Accuracy 0.459716796875\n",
      "Iteration 53360 Training loss 0.05070914700627327 Validation loss 0.05303732305765152 Accuracy 0.460693359375\n",
      "Iteration 53370 Training loss 0.05489226058125496 Validation loss 0.05317431688308716 Accuracy 0.459228515625\n",
      "Iteration 53380 Training loss 0.052897557616233826 Validation loss 0.053679969161748886 Accuracy 0.456298828125\n",
      "Iteration 53390 Training loss 0.05203666538000107 Validation loss 0.05340519919991493 Accuracy 0.45947265625\n",
      "Iteration 53400 Training loss 0.0558425597846508 Validation loss 0.053352244198322296 Accuracy 0.4580078125\n",
      "Iteration 53410 Training loss 0.04831734672188759 Validation loss 0.05345534533262253 Accuracy 0.457763671875\n",
      "Iteration 53420 Training loss 0.05177619308233261 Validation loss 0.05330340191721916 Accuracy 0.45947265625\n",
      "Iteration 53430 Training loss 0.05104382336139679 Validation loss 0.053449515253305435 Accuracy 0.45849609375\n",
      "Iteration 53440 Training loss 0.0504157617688179 Validation loss 0.05337445065379143 Accuracy 0.459716796875\n",
      "Iteration 53450 Training loss 0.04762022942304611 Validation loss 0.053143903613090515 Accuracy 0.46044921875\n",
      "Iteration 53460 Training loss 0.05015924945473671 Validation loss 0.0537521094083786 Accuracy 0.45654296875\n",
      "Iteration 53470 Training loss 0.053275976330041885 Validation loss 0.05315414443612099 Accuracy 0.46044921875\n",
      "Iteration 53480 Training loss 0.04705005884170532 Validation loss 0.053139232099056244 Accuracy 0.460205078125\n",
      "Iteration 53490 Training loss 0.05237429589033127 Validation loss 0.05346529185771942 Accuracy 0.456298828125\n",
      "Iteration 53500 Training loss 0.05433109402656555 Validation loss 0.05349532514810562 Accuracy 0.45849609375\n",
      "Iteration 53510 Training loss 0.05323264002799988 Validation loss 0.053117625415325165 Accuracy 0.46044921875\n",
      "Iteration 53520 Training loss 0.053698424249887466 Validation loss 0.053349364548921585 Accuracy 0.45849609375\n",
      "Iteration 53530 Training loss 0.05386458709836006 Validation loss 0.05315389484167099 Accuracy 0.45947265625\n",
      "Iteration 53540 Training loss 0.05324166268110275 Validation loss 0.053343698382377625 Accuracy 0.457763671875\n",
      "Iteration 53550 Training loss 0.050597138702869415 Validation loss 0.05324895307421684 Accuracy 0.460205078125\n",
      "Iteration 53560 Training loss 0.05211125686764717 Validation loss 0.053170766681432724 Accuracy 0.46044921875\n",
      "Iteration 53570 Training loss 0.053185392171144485 Validation loss 0.0532170869410038 Accuracy 0.459716796875\n",
      "Iteration 53580 Training loss 0.050286997109651566 Validation loss 0.05344631150364876 Accuracy 0.45849609375\n",
      "Iteration 53590 Training loss 0.054254092276096344 Validation loss 0.05361831933259964 Accuracy 0.45654296875\n",
      "Iteration 53600 Training loss 0.05269340053200722 Validation loss 0.05318082869052887 Accuracy 0.458984375\n",
      "Iteration 53610 Training loss 0.05123895779252052 Validation loss 0.05332421883940697 Accuracy 0.460205078125\n",
      "Iteration 53620 Training loss 0.05185772478580475 Validation loss 0.05304444208741188 Accuracy 0.460693359375\n",
      "Iteration 53630 Training loss 0.05317164584994316 Validation loss 0.052990179508924484 Accuracy 0.4609375\n",
      "Iteration 53640 Training loss 0.05008344352245331 Validation loss 0.0532810240983963 Accuracy 0.459228515625\n",
      "Iteration 53650 Training loss 0.050117868930101395 Validation loss 0.05310704931616783 Accuracy 0.4609375\n",
      "Iteration 53660 Training loss 0.05186643451452255 Validation loss 0.05358845740556717 Accuracy 0.457275390625\n",
      "Iteration 53670 Training loss 0.05179766193032265 Validation loss 0.05356692150235176 Accuracy 0.457763671875\n",
      "Iteration 53680 Training loss 0.049934662878513336 Validation loss 0.054445136338472366 Accuracy 0.450439453125\n",
      "Iteration 53690 Training loss 0.05039571225643158 Validation loss 0.05441688001155853 Accuracy 0.448486328125\n",
      "Iteration 53700 Training loss 0.050882428884506226 Validation loss 0.05320350453257561 Accuracy 0.458984375\n",
      "Iteration 53710 Training loss 0.046460479497909546 Validation loss 0.05330679565668106 Accuracy 0.459228515625\n",
      "Iteration 53720 Training loss 0.05355049669742584 Validation loss 0.05323709920048714 Accuracy 0.460205078125\n",
      "Iteration 53730 Training loss 0.04870212823152542 Validation loss 0.05312185734510422 Accuracy 0.460205078125\n",
      "Iteration 53740 Training loss 0.04871343821287155 Validation loss 0.05326151102781296 Accuracy 0.459716796875\n",
      "Iteration 53750 Training loss 0.05333418771624565 Validation loss 0.05371125414967537 Accuracy 0.45556640625\n",
      "Iteration 53760 Training loss 0.05130796134471893 Validation loss 0.053086038678884506 Accuracy 0.4599609375\n",
      "Iteration 53770 Training loss 0.04533088579773903 Validation loss 0.05397330969572067 Accuracy 0.453125\n",
      "Iteration 53780 Training loss 0.04735720157623291 Validation loss 0.053083933889865875 Accuracy 0.460693359375\n",
      "Iteration 53790 Training loss 0.051458485424518585 Validation loss 0.053103089332580566 Accuracy 0.460693359375\n",
      "Iteration 53800 Training loss 0.05002787336707115 Validation loss 0.05334978550672531 Accuracy 0.45849609375\n",
      "Iteration 53810 Training loss 0.052101220935583115 Validation loss 0.053127825260162354 Accuracy 0.460205078125\n",
      "Iteration 53820 Training loss 0.05125843361020088 Validation loss 0.05351271480321884 Accuracy 0.45751953125\n",
      "Iteration 53830 Training loss 0.053481899201869965 Validation loss 0.053295981138944626 Accuracy 0.458740234375\n",
      "Iteration 53840 Training loss 0.05296875908970833 Validation loss 0.05309830978512764 Accuracy 0.459716796875\n",
      "Iteration 53850 Training loss 0.05281245708465576 Validation loss 0.053229689598083496 Accuracy 0.459228515625\n",
      "Iteration 53860 Training loss 0.051740191876888275 Validation loss 0.05337289348244667 Accuracy 0.458251953125\n",
      "Iteration 53870 Training loss 0.05250410735607147 Validation loss 0.05330894514918327 Accuracy 0.457763671875\n",
      "Iteration 53880 Training loss 0.047007523477077484 Validation loss 0.05322318151593208 Accuracy 0.459716796875\n",
      "Iteration 53890 Training loss 0.05396291986107826 Validation loss 0.053140364587306976 Accuracy 0.4599609375\n",
      "Iteration 53900 Training loss 0.051440346986055374 Validation loss 0.05315541476011276 Accuracy 0.4599609375\n",
      "Iteration 53910 Training loss 0.05017850548028946 Validation loss 0.05320746824145317 Accuracy 0.459716796875\n",
      "Iteration 53920 Training loss 0.049513570964336395 Validation loss 0.05325276404619217 Accuracy 0.459228515625\n",
      "Iteration 53930 Training loss 0.05363450571894646 Validation loss 0.053779423236846924 Accuracy 0.4560546875\n",
      "Iteration 53940 Training loss 0.052021902054548264 Validation loss 0.05408212170004845 Accuracy 0.4541015625\n",
      "Iteration 53950 Training loss 0.05128633975982666 Validation loss 0.053195737302303314 Accuracy 0.458984375\n",
      "Iteration 53960 Training loss 0.05040985718369484 Validation loss 0.053189538419246674 Accuracy 0.460205078125\n",
      "Iteration 53970 Training loss 0.04942146688699722 Validation loss 0.0532500296831131 Accuracy 0.459716796875\n",
      "Iteration 53980 Training loss 0.053819648921489716 Validation loss 0.05326465144753456 Accuracy 0.460205078125\n",
      "Iteration 53990 Training loss 0.05362631008028984 Validation loss 0.05334475636482239 Accuracy 0.4580078125\n",
      "Iteration 54000 Training loss 0.04986952617764473 Validation loss 0.05334067717194557 Accuracy 0.458740234375\n",
      "Iteration 54010 Training loss 0.05057969689369202 Validation loss 0.053742967545986176 Accuracy 0.456787109375\n",
      "Iteration 54020 Training loss 0.0510694682598114 Validation loss 0.05360540747642517 Accuracy 0.45654296875\n",
      "Iteration 54030 Training loss 0.05219024419784546 Validation loss 0.05334172770380974 Accuracy 0.459228515625\n",
      "Iteration 54040 Training loss 0.048435285687446594 Validation loss 0.053404707461595535 Accuracy 0.45849609375\n",
      "Iteration 54050 Training loss 0.051664408296346664 Validation loss 0.053291335701942444 Accuracy 0.459228515625\n",
      "Iteration 54060 Training loss 0.051327209919691086 Validation loss 0.053459081798791885 Accuracy 0.4580078125\n",
      "Iteration 54070 Training loss 0.05367356166243553 Validation loss 0.05361362174153328 Accuracy 0.45703125\n",
      "Iteration 54080 Training loss 0.05022350326180458 Validation loss 0.05328565463423729 Accuracy 0.4599609375\n",
      "Iteration 54090 Training loss 0.05109701305627823 Validation loss 0.05321744084358215 Accuracy 0.45849609375\n",
      "Iteration 54100 Training loss 0.04935870319604874 Validation loss 0.05335551127791405 Accuracy 0.459716796875\n",
      "Iteration 54110 Training loss 0.04889940842986107 Validation loss 0.05323046073317528 Accuracy 0.459228515625\n",
      "Iteration 54120 Training loss 0.05181780084967613 Validation loss 0.05311495065689087 Accuracy 0.460693359375\n",
      "Iteration 54130 Training loss 0.05048780143260956 Validation loss 0.05336754769086838 Accuracy 0.458984375\n",
      "Iteration 54140 Training loss 0.05296909436583519 Validation loss 0.05324076861143112 Accuracy 0.45849609375\n",
      "Iteration 54150 Training loss 0.049522221088409424 Validation loss 0.0532744899392128 Accuracy 0.459228515625\n",
      "Iteration 54160 Training loss 0.05110717564821243 Validation loss 0.05352577939629555 Accuracy 0.4580078125\n",
      "Iteration 54170 Training loss 0.05029863491654396 Validation loss 0.053406886756420135 Accuracy 0.45947265625\n",
      "Iteration 54180 Training loss 0.05099387466907501 Validation loss 0.05378066748380661 Accuracy 0.455078125\n",
      "Iteration 54190 Training loss 0.04900708422064781 Validation loss 0.0530574806034565 Accuracy 0.46044921875\n",
      "Iteration 54200 Training loss 0.048684995621442795 Validation loss 0.053027693182229996 Accuracy 0.460693359375\n",
      "Iteration 54210 Training loss 0.05093755945563316 Validation loss 0.053339723497629166 Accuracy 0.460205078125\n",
      "Iteration 54220 Training loss 0.05185013636946678 Validation loss 0.053184159100055695 Accuracy 0.4599609375\n",
      "Iteration 54230 Training loss 0.05160576477646828 Validation loss 0.053215641528367996 Accuracy 0.458984375\n",
      "Iteration 54240 Training loss 0.04962649568915367 Validation loss 0.05322544276714325 Accuracy 0.459228515625\n",
      "Iteration 54250 Training loss 0.048250872641801834 Validation loss 0.05323844775557518 Accuracy 0.459228515625\n",
      "Iteration 54260 Training loss 0.04907156899571419 Validation loss 0.05333773419260979 Accuracy 0.458740234375\n",
      "Iteration 54270 Training loss 0.04802684858441353 Validation loss 0.053735408931970596 Accuracy 0.45654296875\n",
      "Iteration 54280 Training loss 0.05174190178513527 Validation loss 0.05387177690863609 Accuracy 0.45458984375\n",
      "Iteration 54290 Training loss 0.04843248426914215 Validation loss 0.05308407172560692 Accuracy 0.46044921875\n",
      "Iteration 54300 Training loss 0.05084039270877838 Validation loss 0.05330787971615791 Accuracy 0.459228515625\n",
      "Iteration 54310 Training loss 0.049530211836099625 Validation loss 0.053468309342861176 Accuracy 0.45751953125\n",
      "Iteration 54320 Training loss 0.052553705871105194 Validation loss 0.05311131104826927 Accuracy 0.460693359375\n",
      "Iteration 54330 Training loss 0.054142169654369354 Validation loss 0.05363721400499344 Accuracy 0.457275390625\n",
      "Iteration 54340 Training loss 0.04927533492445946 Validation loss 0.053350407630205154 Accuracy 0.45849609375\n",
      "Iteration 54350 Training loss 0.05384132266044617 Validation loss 0.05326094478368759 Accuracy 0.45849609375\n",
      "Iteration 54360 Training loss 0.05335533991456032 Validation loss 0.053330983966588974 Accuracy 0.45849609375\n",
      "Iteration 54370 Training loss 0.05411529913544655 Validation loss 0.05319027230143547 Accuracy 0.459716796875\n",
      "Iteration 54380 Training loss 0.05522470921278 Validation loss 0.05401010438799858 Accuracy 0.45361328125\n",
      "Iteration 54390 Training loss 0.05539683997631073 Validation loss 0.053767163306474686 Accuracy 0.456787109375\n",
      "Iteration 54400 Training loss 0.050998035818338394 Validation loss 0.05322536826133728 Accuracy 0.458984375\n",
      "Iteration 54410 Training loss 0.054452020674943924 Validation loss 0.053426310420036316 Accuracy 0.458984375\n",
      "Iteration 54420 Training loss 0.050410907715559006 Validation loss 0.05330660939216614 Accuracy 0.458984375\n",
      "Iteration 54430 Training loss 0.05094202607870102 Validation loss 0.053248561918735504 Accuracy 0.460205078125\n",
      "Iteration 54440 Training loss 0.05164477601647377 Validation loss 0.05415503680706024 Accuracy 0.45068359375\n",
      "Iteration 54450 Training loss 0.049271464347839355 Validation loss 0.05329078808426857 Accuracy 0.459716796875\n",
      "Iteration 54460 Training loss 0.04867062717676163 Validation loss 0.053072184324264526 Accuracy 0.461181640625\n",
      "Iteration 54470 Training loss 0.051305387169122696 Validation loss 0.05349277704954147 Accuracy 0.45849609375\n",
      "Iteration 54480 Training loss 0.05080455541610718 Validation loss 0.05309979245066643 Accuracy 0.46044921875\n",
      "Iteration 54490 Training loss 0.05246962979435921 Validation loss 0.05333121865987778 Accuracy 0.458251953125\n",
      "Iteration 54500 Training loss 0.053665485233068466 Validation loss 0.05329946056008339 Accuracy 0.458984375\n",
      "Iteration 54510 Training loss 0.05243228003382683 Validation loss 0.053047064691782 Accuracy 0.460205078125\n",
      "Iteration 54520 Training loss 0.04977288842201233 Validation loss 0.053475722670555115 Accuracy 0.457275390625\n",
      "Iteration 54530 Training loss 0.05155820772051811 Validation loss 0.053339455276727676 Accuracy 0.45947265625\n",
      "Iteration 54540 Training loss 0.05332598835229874 Validation loss 0.053385887295007706 Accuracy 0.458984375\n",
      "Iteration 54550 Training loss 0.05084320530295372 Validation loss 0.053155574947595596 Accuracy 0.460205078125\n",
      "Iteration 54560 Training loss 0.05046829208731651 Validation loss 0.05361657962203026 Accuracy 0.4580078125\n",
      "Iteration 54570 Training loss 0.050646182149648666 Validation loss 0.05312180146574974 Accuracy 0.4609375\n",
      "Iteration 54580 Training loss 0.048217374831438065 Validation loss 0.05344124883413315 Accuracy 0.45654296875\n",
      "Iteration 54590 Training loss 0.04968384653329849 Validation loss 0.05309631675481796 Accuracy 0.460693359375\n",
      "Iteration 54600 Training loss 0.048776187002658844 Validation loss 0.053327761590480804 Accuracy 0.4599609375\n",
      "Iteration 54610 Training loss 0.05190742015838623 Validation loss 0.05336909368634224 Accuracy 0.4599609375\n",
      "Iteration 54620 Training loss 0.052884455770254135 Validation loss 0.05339768901467323 Accuracy 0.45849609375\n",
      "Iteration 54630 Training loss 0.05279894918203354 Validation loss 0.05335688218474388 Accuracy 0.459716796875\n",
      "Iteration 54640 Training loss 0.052565015852451324 Validation loss 0.0533563606441021 Accuracy 0.459228515625\n",
      "Iteration 54650 Training loss 0.05152146518230438 Validation loss 0.05326640605926514 Accuracy 0.458984375\n",
      "Iteration 54660 Training loss 0.0494377501308918 Validation loss 0.053165633231401443 Accuracy 0.460693359375\n",
      "Iteration 54670 Training loss 0.05149286985397339 Validation loss 0.05402373895049095 Accuracy 0.453857421875\n",
      "Iteration 54680 Training loss 0.05414814129471779 Validation loss 0.05350424721837044 Accuracy 0.458740234375\n",
      "Iteration 54690 Training loss 0.05185329169034958 Validation loss 0.053169045597314835 Accuracy 0.45849609375\n",
      "Iteration 54700 Training loss 0.056726571172475815 Validation loss 0.05347144976258278 Accuracy 0.45703125\n",
      "Iteration 54710 Training loss 0.05410996824502945 Validation loss 0.053191158920526505 Accuracy 0.459716796875\n",
      "Iteration 54720 Training loss 0.05176481604576111 Validation loss 0.05336768552660942 Accuracy 0.4580078125\n",
      "Iteration 54730 Training loss 0.05258302390575409 Validation loss 0.05352763459086418 Accuracy 0.45654296875\n",
      "Iteration 54740 Training loss 0.05318658798933029 Validation loss 0.0537271648645401 Accuracy 0.4541015625\n",
      "Iteration 54750 Training loss 0.05434205383062363 Validation loss 0.05374429374933243 Accuracy 0.45556640625\n",
      "Iteration 54760 Training loss 0.04813992232084274 Validation loss 0.05328493192791939 Accuracy 0.45849609375\n",
      "Iteration 54770 Training loss 0.0506608672440052 Validation loss 0.05367487296462059 Accuracy 0.455078125\n",
      "Iteration 54780 Training loss 0.04877857491374016 Validation loss 0.053201157599687576 Accuracy 0.4599609375\n",
      "Iteration 54790 Training loss 0.05081452429294586 Validation loss 0.053344082087278366 Accuracy 0.458984375\n",
      "Iteration 54800 Training loss 0.050651915371418 Validation loss 0.053437575697898865 Accuracy 0.4580078125\n",
      "Iteration 54810 Training loss 0.05516568571329117 Validation loss 0.05369370058178902 Accuracy 0.45458984375\n",
      "Iteration 54820 Training loss 0.05030769854784012 Validation loss 0.05315312370657921 Accuracy 0.46044921875\n",
      "Iteration 54830 Training loss 0.0503271222114563 Validation loss 0.053332820534706116 Accuracy 0.458740234375\n",
      "Iteration 54840 Training loss 0.0478052981197834 Validation loss 0.05354556813836098 Accuracy 0.45849609375\n",
      "Iteration 54850 Training loss 0.04724759981036186 Validation loss 0.05332101881504059 Accuracy 0.459716796875\n",
      "Iteration 54860 Training loss 0.05239533260464668 Validation loss 0.05321921780705452 Accuracy 0.459716796875\n",
      "Iteration 54870 Training loss 0.05038115382194519 Validation loss 0.0535246767103672 Accuracy 0.45703125\n",
      "Iteration 54880 Training loss 0.05170729011297226 Validation loss 0.05368558317422867 Accuracy 0.455810546875\n",
      "Iteration 54890 Training loss 0.05164435878396034 Validation loss 0.05326557904481888 Accuracy 0.460693359375\n",
      "Iteration 54900 Training loss 0.05542060360312462 Validation loss 0.053199879825115204 Accuracy 0.460205078125\n",
      "Iteration 54910 Training loss 0.05304093286395073 Validation loss 0.05391960218548775 Accuracy 0.452880859375\n",
      "Iteration 54920 Training loss 0.05029919371008873 Validation loss 0.053305864334106445 Accuracy 0.459228515625\n",
      "Iteration 54930 Training loss 0.04822167009115219 Validation loss 0.05316808447241783 Accuracy 0.460205078125\n",
      "Iteration 54940 Training loss 0.05027217045426369 Validation loss 0.053530652076005936 Accuracy 0.4560546875\n",
      "Iteration 54950 Training loss 0.05640348419547081 Validation loss 0.05342533066868782 Accuracy 0.459228515625\n",
      "Iteration 54960 Training loss 0.05355173721909523 Validation loss 0.05326225981116295 Accuracy 0.459228515625\n",
      "Iteration 54970 Training loss 0.05085035040974617 Validation loss 0.053285397589206696 Accuracy 0.45947265625\n",
      "Iteration 54980 Training loss 0.05086992308497429 Validation loss 0.05339628830552101 Accuracy 0.4580078125\n",
      "Iteration 54990 Training loss 0.0517665334045887 Validation loss 0.05318274348974228 Accuracy 0.459716796875\n",
      "Iteration 55000 Training loss 0.05521560460329056 Validation loss 0.053269162774086 Accuracy 0.45849609375\n",
      "Iteration 55010 Training loss 0.05162417143583298 Validation loss 0.05340598523616791 Accuracy 0.45751953125\n",
      "Iteration 55020 Training loss 0.05324086546897888 Validation loss 0.053854987025260925 Accuracy 0.45458984375\n",
      "Iteration 55030 Training loss 0.054123710840940475 Validation loss 0.05322650074958801 Accuracy 0.459716796875\n",
      "Iteration 55040 Training loss 0.053222063928842545 Validation loss 0.05346149206161499 Accuracy 0.458251953125\n",
      "Iteration 55050 Training loss 0.04798441380262375 Validation loss 0.05316711217164993 Accuracy 0.460205078125\n",
      "Iteration 55060 Training loss 0.051130421459674835 Validation loss 0.05343339592218399 Accuracy 0.45849609375\n",
      "Iteration 55070 Training loss 0.05334239825606346 Validation loss 0.05327155068516731 Accuracy 0.46044921875\n",
      "Iteration 55080 Training loss 0.0497511625289917 Validation loss 0.05387270450592041 Accuracy 0.455078125\n",
      "Iteration 55090 Training loss 0.05013043060898781 Validation loss 0.05383269861340523 Accuracy 0.45556640625\n",
      "Iteration 55100 Training loss 0.0543532520532608 Validation loss 0.05327761545777321 Accuracy 0.459228515625\n",
      "Iteration 55110 Training loss 0.05124460905790329 Validation loss 0.05340416729450226 Accuracy 0.4580078125\n",
      "Iteration 55120 Training loss 0.04868844896554947 Validation loss 0.0535539835691452 Accuracy 0.456787109375\n",
      "Iteration 55130 Training loss 0.053680308163166046 Validation loss 0.0534883588552475 Accuracy 0.45703125\n",
      "Iteration 55140 Training loss 0.05223459750413895 Validation loss 0.053314805030822754 Accuracy 0.458740234375\n",
      "Iteration 55150 Training loss 0.05069690942764282 Validation loss 0.05320705845952034 Accuracy 0.459716796875\n",
      "Iteration 55160 Training loss 0.04841932654380798 Validation loss 0.05345722660422325 Accuracy 0.45849609375\n",
      "Iteration 55170 Training loss 0.05023232847452164 Validation loss 0.053596410900354385 Accuracy 0.456298828125\n",
      "Iteration 55180 Training loss 0.05157362297177315 Validation loss 0.053233467042446136 Accuracy 0.458984375\n",
      "Iteration 55190 Training loss 0.05547519400715828 Validation loss 0.05340906232595444 Accuracy 0.458984375\n",
      "Iteration 55200 Training loss 0.05248080566525459 Validation loss 0.053292643278837204 Accuracy 0.45849609375\n",
      "Iteration 55210 Training loss 0.05169260501861572 Validation loss 0.05333448573946953 Accuracy 0.45751953125\n",
      "Iteration 55220 Training loss 0.05141886696219444 Validation loss 0.05327008664608002 Accuracy 0.45947265625\n",
      "Iteration 55230 Training loss 0.052767571061849594 Validation loss 0.05354193225502968 Accuracy 0.4560546875\n",
      "Iteration 55240 Training loss 0.04838336259126663 Validation loss 0.0532531775534153 Accuracy 0.45849609375\n",
      "Iteration 55250 Training loss 0.05444002151489258 Validation loss 0.053676821291446686 Accuracy 0.45361328125\n",
      "Iteration 55260 Training loss 0.05253918468952179 Validation loss 0.05315186828374863 Accuracy 0.46044921875\n",
      "Iteration 55270 Training loss 0.04950868710875511 Validation loss 0.05317138135433197 Accuracy 0.459228515625\n",
      "Iteration 55280 Training loss 0.0480562224984169 Validation loss 0.05320668965578079 Accuracy 0.459716796875\n",
      "Iteration 55290 Training loss 0.053439103066921234 Validation loss 0.05341258645057678 Accuracy 0.459228515625\n",
      "Iteration 55300 Training loss 0.051746468991041183 Validation loss 0.053151004016399384 Accuracy 0.459716796875\n",
      "Iteration 55310 Training loss 0.050341639667749405 Validation loss 0.05337612330913544 Accuracy 0.4580078125\n",
      "Iteration 55320 Training loss 0.05166473239660263 Validation loss 0.05331352725625038 Accuracy 0.458984375\n",
      "Iteration 55330 Training loss 0.052761390805244446 Validation loss 0.05332047864794731 Accuracy 0.458740234375\n",
      "Iteration 55340 Training loss 0.05179747939109802 Validation loss 0.05347849801182747 Accuracy 0.459228515625\n",
      "Iteration 55350 Training loss 0.049186863005161285 Validation loss 0.05344018340110779 Accuracy 0.458740234375\n",
      "Iteration 55360 Training loss 0.05170423910021782 Validation loss 0.05377187207341194 Accuracy 0.455322265625\n",
      "Iteration 55370 Training loss 0.05096844956278801 Validation loss 0.05317641794681549 Accuracy 0.46044921875\n",
      "Iteration 55380 Training loss 0.05118057131767273 Validation loss 0.053216274827718735 Accuracy 0.45947265625\n",
      "Iteration 55390 Training loss 0.051562439650297165 Validation loss 0.05319668725132942 Accuracy 0.46044921875\n",
      "Iteration 55400 Training loss 0.049774445593357086 Validation loss 0.05363749340176582 Accuracy 0.456787109375\n",
      "Iteration 55410 Training loss 0.051788248121738434 Validation loss 0.05317677557468414 Accuracy 0.459716796875\n",
      "Iteration 55420 Training loss 0.05044419318437576 Validation loss 0.053830407559871674 Accuracy 0.455810546875\n",
      "Iteration 55430 Training loss 0.0557246133685112 Validation loss 0.05392148718237877 Accuracy 0.453125\n",
      "Iteration 55440 Training loss 0.05092112347483635 Validation loss 0.05328451842069626 Accuracy 0.458984375\n",
      "Iteration 55450 Training loss 0.054516494274139404 Validation loss 0.053437333554029465 Accuracy 0.4580078125\n",
      "Iteration 55460 Training loss 0.049814410507678986 Validation loss 0.05348144844174385 Accuracy 0.458251953125\n",
      "Iteration 55470 Training loss 0.04986076429486275 Validation loss 0.05362924560904503 Accuracy 0.45654296875\n",
      "Iteration 55480 Training loss 0.04954496771097183 Validation loss 0.05326777696609497 Accuracy 0.459716796875\n",
      "Iteration 55490 Training loss 0.050501931458711624 Validation loss 0.053174931555986404 Accuracy 0.45947265625\n",
      "Iteration 55500 Training loss 0.0488157793879509 Validation loss 0.053278710693120956 Accuracy 0.459228515625\n",
      "Iteration 55510 Training loss 0.05214361846446991 Validation loss 0.05321161076426506 Accuracy 0.4599609375\n",
      "Iteration 55520 Training loss 0.05602847412228584 Validation loss 0.05342845991253853 Accuracy 0.459228515625\n",
      "Iteration 55530 Training loss 0.05077357962727547 Validation loss 0.053070321679115295 Accuracy 0.460205078125\n",
      "Iteration 55540 Training loss 0.054615944623947144 Validation loss 0.053639695048332214 Accuracy 0.457763671875\n",
      "Iteration 55550 Training loss 0.05091458931565285 Validation loss 0.05326716601848602 Accuracy 0.458984375\n",
      "Iteration 55560 Training loss 0.0561378076672554 Validation loss 0.05327318236231804 Accuracy 0.459228515625\n",
      "Iteration 55570 Training loss 0.05027228593826294 Validation loss 0.05377167835831642 Accuracy 0.454833984375\n",
      "Iteration 55580 Training loss 0.051741983741521835 Validation loss 0.05312371999025345 Accuracy 0.461669921875\n",
      "Iteration 55590 Training loss 0.05397156625986099 Validation loss 0.053152214735746384 Accuracy 0.46142578125\n",
      "Iteration 55600 Training loss 0.04798300191760063 Validation loss 0.05316019430756569 Accuracy 0.461669921875\n",
      "Iteration 55610 Training loss 0.05156950652599335 Validation loss 0.05361386761069298 Accuracy 0.456787109375\n",
      "Iteration 55620 Training loss 0.05014045163989067 Validation loss 0.053206492215394974 Accuracy 0.46044921875\n",
      "Iteration 55630 Training loss 0.05296250432729721 Validation loss 0.053122684359550476 Accuracy 0.461181640625\n",
      "Iteration 55640 Training loss 0.051594216376543045 Validation loss 0.054939154535532 Accuracy 0.444580078125\n",
      "Iteration 55650 Training loss 0.04848482832312584 Validation loss 0.053167909383773804 Accuracy 0.459716796875\n",
      "Iteration 55660 Training loss 0.05158790200948715 Validation loss 0.05372554436326027 Accuracy 0.45556640625\n",
      "Iteration 55670 Training loss 0.052262693643569946 Validation loss 0.0531524121761322 Accuracy 0.45947265625\n",
      "Iteration 55680 Training loss 0.0499085895717144 Validation loss 0.05373386666178703 Accuracy 0.457275390625\n",
      "Iteration 55690 Training loss 0.05151161178946495 Validation loss 0.05369536951184273 Accuracy 0.45751953125\n",
      "Iteration 55700 Training loss 0.05013943091034889 Validation loss 0.05367668345570564 Accuracy 0.455078125\n",
      "Iteration 55710 Training loss 0.050135936588048935 Validation loss 0.05345701426267624 Accuracy 0.458984375\n",
      "Iteration 55720 Training loss 0.05343512073159218 Validation loss 0.05322840064764023 Accuracy 0.459716796875\n",
      "Iteration 55730 Training loss 0.052706919610500336 Validation loss 0.05321258306503296 Accuracy 0.4599609375\n",
      "Iteration 55740 Training loss 0.051181111484766006 Validation loss 0.05350438505411148 Accuracy 0.458251953125\n",
      "Iteration 55750 Training loss 0.04790996387600899 Validation loss 0.05322861298918724 Accuracy 0.4599609375\n",
      "Iteration 55760 Training loss 0.05098545178771019 Validation loss 0.053336821496486664 Accuracy 0.458740234375\n",
      "Iteration 55770 Training loss 0.05355328321456909 Validation loss 0.053610555827617645 Accuracy 0.45703125\n",
      "Iteration 55780 Training loss 0.05288814380764961 Validation loss 0.05334155261516571 Accuracy 0.45947265625\n",
      "Iteration 55790 Training loss 0.05159212648868561 Validation loss 0.05332455039024353 Accuracy 0.460205078125\n",
      "Iteration 55800 Training loss 0.05194169655442238 Validation loss 0.05405590683221817 Accuracy 0.453369140625\n",
      "Iteration 55810 Training loss 0.048450179398059845 Validation loss 0.05311599001288414 Accuracy 0.460693359375\n",
      "Iteration 55820 Training loss 0.05145791918039322 Validation loss 0.053220998495817184 Accuracy 0.4599609375\n",
      "Iteration 55830 Training loss 0.05343525484204292 Validation loss 0.05324125662446022 Accuracy 0.460205078125\n",
      "Iteration 55840 Training loss 0.04563615843653679 Validation loss 0.05301832780241966 Accuracy 0.46142578125\n",
      "Iteration 55850 Training loss 0.04975763335824013 Validation loss 0.053291160613298416 Accuracy 0.460693359375\n",
      "Iteration 55860 Training loss 0.050559528172016144 Validation loss 0.05321362242102623 Accuracy 0.460693359375\n",
      "Iteration 55870 Training loss 0.051695093512535095 Validation loss 0.053257547318935394 Accuracy 0.459716796875\n",
      "Iteration 55880 Training loss 0.05242462083697319 Validation loss 0.053267884999513626 Accuracy 0.459716796875\n",
      "Iteration 55890 Training loss 0.053819529712200165 Validation loss 0.05323127284646034 Accuracy 0.46044921875\n",
      "Iteration 55900 Training loss 0.050443585962057114 Validation loss 0.05312861129641533 Accuracy 0.460693359375\n",
      "Iteration 55910 Training loss 0.05330368131399155 Validation loss 0.053545087575912476 Accuracy 0.4580078125\n",
      "Iteration 55920 Training loss 0.050475891679525375 Validation loss 0.053596898913383484 Accuracy 0.457275390625\n",
      "Iteration 55930 Training loss 0.05158144235610962 Validation loss 0.053112536668777466 Accuracy 0.46044921875\n",
      "Iteration 55940 Training loss 0.04905291646718979 Validation loss 0.05316583812236786 Accuracy 0.46044921875\n",
      "Iteration 55950 Training loss 0.049117278307676315 Validation loss 0.05310729891061783 Accuracy 0.460693359375\n",
      "Iteration 55960 Training loss 0.04976906627416611 Validation loss 0.05355260521173477 Accuracy 0.4580078125\n",
      "Iteration 55970 Training loss 0.04835251718759537 Validation loss 0.05352901294827461 Accuracy 0.458984375\n",
      "Iteration 55980 Training loss 0.05063221603631973 Validation loss 0.05357795208692551 Accuracy 0.457763671875\n",
      "Iteration 55990 Training loss 0.050444286316633224 Validation loss 0.05338778346776962 Accuracy 0.458984375\n",
      "Iteration 56000 Training loss 0.052169524133205414 Validation loss 0.053417809307575226 Accuracy 0.45849609375\n",
      "Iteration 56010 Training loss 0.04848418012261391 Validation loss 0.05319852754473686 Accuracy 0.45947265625\n",
      "Iteration 56020 Training loss 0.048876091837882996 Validation loss 0.05308985337615013 Accuracy 0.460205078125\n",
      "Iteration 56030 Training loss 0.05182841792702675 Validation loss 0.053182996809482574 Accuracy 0.459716796875\n",
      "Iteration 56040 Training loss 0.049006231129169464 Validation loss 0.053086355328559875 Accuracy 0.460693359375\n",
      "Iteration 56050 Training loss 0.04939072206616402 Validation loss 0.05312955006957054 Accuracy 0.461181640625\n",
      "Iteration 56060 Training loss 0.05103261396288872 Validation loss 0.05300859734416008 Accuracy 0.461181640625\n",
      "Iteration 56070 Training loss 0.05385945737361908 Validation loss 0.05323271453380585 Accuracy 0.459716796875\n",
      "Iteration 56080 Training loss 0.05232653766870499 Validation loss 0.0531887486577034 Accuracy 0.458984375\n",
      "Iteration 56090 Training loss 0.04924268648028374 Validation loss 0.05326754227280617 Accuracy 0.458251953125\n",
      "Iteration 56100 Training loss 0.05336485803127289 Validation loss 0.054590802639722824 Accuracy 0.44677734375\n",
      "Iteration 56110 Training loss 0.050619497895240784 Validation loss 0.053085487335920334 Accuracy 0.4599609375\n",
      "Iteration 56120 Training loss 0.051107533276081085 Validation loss 0.053827449679374695 Accuracy 0.4541015625\n",
      "Iteration 56130 Training loss 0.04953165724873543 Validation loss 0.05334086716175079 Accuracy 0.46044921875\n",
      "Iteration 56140 Training loss 0.05065131559967995 Validation loss 0.05404078960418701 Accuracy 0.45361328125\n",
      "Iteration 56150 Training loss 0.05042634159326553 Validation loss 0.053329948335886 Accuracy 0.458984375\n",
      "Iteration 56160 Training loss 0.05137927085161209 Validation loss 0.053386230021715164 Accuracy 0.46044921875\n",
      "Iteration 56170 Training loss 0.052025724202394485 Validation loss 0.05334226414561272 Accuracy 0.459716796875\n",
      "Iteration 56180 Training loss 0.05050014331936836 Validation loss 0.053492698818445206 Accuracy 0.459716796875\n",
      "Iteration 56190 Training loss 0.04987381771206856 Validation loss 0.0531538650393486 Accuracy 0.460205078125\n",
      "Iteration 56200 Training loss 0.053993918001651764 Validation loss 0.054481141269207 Accuracy 0.4482421875\n",
      "Iteration 56210 Training loss 0.048821866512298584 Validation loss 0.05327950045466423 Accuracy 0.459228515625\n",
      "Iteration 56220 Training loss 0.052634552121162415 Validation loss 0.053146664053201675 Accuracy 0.46044921875\n",
      "Iteration 56230 Training loss 0.05150771141052246 Validation loss 0.05332661047577858 Accuracy 0.458740234375\n",
      "Iteration 56240 Training loss 0.05096373334527016 Validation loss 0.053152065724134445 Accuracy 0.460205078125\n",
      "Iteration 56250 Training loss 0.05464479699730873 Validation loss 0.05352462828159332 Accuracy 0.4560546875\n",
      "Iteration 56260 Training loss 0.05116742476820946 Validation loss 0.052979011088609695 Accuracy 0.460693359375\n",
      "Iteration 56270 Training loss 0.0538921095430851 Validation loss 0.053405094891786575 Accuracy 0.458740234375\n",
      "Iteration 56280 Training loss 0.05091250687837601 Validation loss 0.05299462378025055 Accuracy 0.460693359375\n",
      "Iteration 56290 Training loss 0.05012132227420807 Validation loss 0.053146447986364365 Accuracy 0.4599609375\n",
      "Iteration 56300 Training loss 0.051848266273736954 Validation loss 0.05322343111038208 Accuracy 0.458740234375\n",
      "Iteration 56310 Training loss 0.05367282032966614 Validation loss 0.053679559379816055 Accuracy 0.45556640625\n",
      "Iteration 56320 Training loss 0.05115745961666107 Validation loss 0.05344986170530319 Accuracy 0.4580078125\n",
      "Iteration 56330 Training loss 0.05264253169298172 Validation loss 0.05338452383875847 Accuracy 0.4580078125\n",
      "Iteration 56340 Training loss 0.05438413470983505 Validation loss 0.053560588508844376 Accuracy 0.457763671875\n",
      "Iteration 56350 Training loss 0.052489906549453735 Validation loss 0.053631242364645004 Accuracy 0.45458984375\n",
      "Iteration 56360 Training loss 0.045773833990097046 Validation loss 0.053094614297151566 Accuracy 0.460205078125\n",
      "Iteration 56370 Training loss 0.05605696886777878 Validation loss 0.053271014243364334 Accuracy 0.459716796875\n",
      "Iteration 56380 Training loss 0.05484094098210335 Validation loss 0.05362663045525551 Accuracy 0.456787109375\n",
      "Iteration 56390 Training loss 0.05282285436987877 Validation loss 0.05382019281387329 Accuracy 0.455078125\n",
      "Iteration 56400 Training loss 0.051080960780382156 Validation loss 0.053415995091199875 Accuracy 0.45849609375\n",
      "Iteration 56410 Training loss 0.05080171674489975 Validation loss 0.053491171449422836 Accuracy 0.45849609375\n",
      "Iteration 56420 Training loss 0.05075383558869362 Validation loss 0.05330456420779228 Accuracy 0.45947265625\n",
      "Iteration 56430 Training loss 0.05244436487555504 Validation loss 0.053268201649188995 Accuracy 0.45947265625\n",
      "Iteration 56440 Training loss 0.05388960242271423 Validation loss 0.05359130725264549 Accuracy 0.4580078125\n",
      "Iteration 56450 Training loss 0.05211770161986351 Validation loss 0.05337350443005562 Accuracy 0.458740234375\n",
      "Iteration 56460 Training loss 0.0563984178006649 Validation loss 0.053487468510866165 Accuracy 0.45751953125\n",
      "Iteration 56470 Training loss 0.05124092474579811 Validation loss 0.05323092266917229 Accuracy 0.459228515625\n",
      "Iteration 56480 Training loss 0.05023233965039253 Validation loss 0.05317138135433197 Accuracy 0.459716796875\n",
      "Iteration 56490 Training loss 0.05245165899395943 Validation loss 0.053252458572387695 Accuracy 0.458251953125\n",
      "Iteration 56500 Training loss 0.05591871961951256 Validation loss 0.053300537168979645 Accuracy 0.458740234375\n",
      "Iteration 56510 Training loss 0.050967030227184296 Validation loss 0.05367858335375786 Accuracy 0.45751953125\n",
      "Iteration 56520 Training loss 0.04897436499595642 Validation loss 0.05332217738032341 Accuracy 0.458740234375\n",
      "Iteration 56530 Training loss 0.050852954387664795 Validation loss 0.05321744829416275 Accuracy 0.459716796875\n",
      "Iteration 56540 Training loss 0.05231652408838272 Validation loss 0.053226545453071594 Accuracy 0.459228515625\n",
      "Iteration 56550 Training loss 0.04998129606246948 Validation loss 0.05325314402580261 Accuracy 0.460205078125\n",
      "Iteration 56560 Training loss 0.05040794983506203 Validation loss 0.053393129259347916 Accuracy 0.459228515625\n",
      "Iteration 56570 Training loss 0.047925736755132675 Validation loss 0.053374871611595154 Accuracy 0.4580078125\n",
      "Iteration 56580 Training loss 0.04781648889183998 Validation loss 0.0532880537211895 Accuracy 0.459716796875\n",
      "Iteration 56590 Training loss 0.05175117030739784 Validation loss 0.05299651622772217 Accuracy 0.460693359375\n",
      "Iteration 56600 Training loss 0.052429940551519394 Validation loss 0.053597621619701385 Accuracy 0.4560546875\n",
      "Iteration 56610 Training loss 0.05407870560884476 Validation loss 0.053171347826719284 Accuracy 0.460205078125\n",
      "Iteration 56620 Training loss 0.05133435130119324 Validation loss 0.05366827920079231 Accuracy 0.457763671875\n",
      "Iteration 56630 Training loss 0.048811379820108414 Validation loss 0.05360901355743408 Accuracy 0.4580078125\n",
      "Iteration 56640 Training loss 0.050131622701883316 Validation loss 0.053298965096473694 Accuracy 0.459716796875\n",
      "Iteration 56650 Training loss 0.04937664791941643 Validation loss 0.05328696221113205 Accuracy 0.45947265625\n",
      "Iteration 56660 Training loss 0.04932178556919098 Validation loss 0.05322525277733803 Accuracy 0.45947265625\n",
      "Iteration 56670 Training loss 0.05104721337556839 Validation loss 0.05332176759839058 Accuracy 0.459716796875\n",
      "Iteration 56680 Training loss 0.05416116863489151 Validation loss 0.05368608236312866 Accuracy 0.456787109375\n",
      "Iteration 56690 Training loss 0.049181777983903885 Validation loss 0.05309063196182251 Accuracy 0.460205078125\n",
      "Iteration 56700 Training loss 0.05454491078853607 Validation loss 0.05322347953915596 Accuracy 0.45849609375\n",
      "Iteration 56710 Training loss 0.05087912082672119 Validation loss 0.053197529166936874 Accuracy 0.459716796875\n",
      "Iteration 56720 Training loss 0.054054468870162964 Validation loss 0.053359922021627426 Accuracy 0.458251953125\n",
      "Iteration 56730 Training loss 0.05209147930145264 Validation loss 0.053281717002391815 Accuracy 0.459228515625\n",
      "Iteration 56740 Training loss 0.04988471046090126 Validation loss 0.05337844043970108 Accuracy 0.458984375\n",
      "Iteration 56750 Training loss 0.05338173732161522 Validation loss 0.05373683571815491 Accuracy 0.45751953125\n",
      "Iteration 56760 Training loss 0.04997249320149422 Validation loss 0.053284697234630585 Accuracy 0.458251953125\n",
      "Iteration 56770 Training loss 0.0514964759349823 Validation loss 0.05312558636069298 Accuracy 0.4599609375\n",
      "Iteration 56780 Training loss 0.052620865404605865 Validation loss 0.05371561273932457 Accuracy 0.455078125\n",
      "Iteration 56790 Training loss 0.05167857185006142 Validation loss 0.05299030989408493 Accuracy 0.461181640625\n",
      "Iteration 56800 Training loss 0.0504622757434845 Validation loss 0.053351569920778275 Accuracy 0.4580078125\n",
      "Iteration 56810 Training loss 0.047672566026449203 Validation loss 0.053333621472120285 Accuracy 0.459228515625\n",
      "Iteration 56820 Training loss 0.04982372373342514 Validation loss 0.053429387509822845 Accuracy 0.45849609375\n",
      "Iteration 56830 Training loss 0.05242520943284035 Validation loss 0.05322057008743286 Accuracy 0.45947265625\n",
      "Iteration 56840 Training loss 0.05039457976818085 Validation loss 0.053193412721157074 Accuracy 0.4599609375\n",
      "Iteration 56850 Training loss 0.04950987547636032 Validation loss 0.05325572192668915 Accuracy 0.45947265625\n",
      "Iteration 56860 Training loss 0.05123431235551834 Validation loss 0.05335626006126404 Accuracy 0.4599609375\n",
      "Iteration 56870 Training loss 0.0512467660009861 Validation loss 0.05353786051273346 Accuracy 0.458251953125\n",
      "Iteration 56880 Training loss 0.05458778142929077 Validation loss 0.052975185215473175 Accuracy 0.461669921875\n",
      "Iteration 56890 Training loss 0.05210140347480774 Validation loss 0.05313715711236 Accuracy 0.461181640625\n",
      "Iteration 56900 Training loss 0.0538162924349308 Validation loss 0.053177107125520706 Accuracy 0.461181640625\n",
      "Iteration 56910 Training loss 0.05046107992529869 Validation loss 0.05373263359069824 Accuracy 0.456298828125\n",
      "Iteration 56920 Training loss 0.053192637860774994 Validation loss 0.053117815405130386 Accuracy 0.460205078125\n",
      "Iteration 56930 Training loss 0.051846444606781006 Validation loss 0.05342498794198036 Accuracy 0.457763671875\n",
      "Iteration 56940 Training loss 0.05338961258530617 Validation loss 0.0531606487929821 Accuracy 0.461181640625\n",
      "Iteration 56950 Training loss 0.050554096698760986 Validation loss 0.053166020661592484 Accuracy 0.460693359375\n",
      "Iteration 56960 Training loss 0.04825710877776146 Validation loss 0.053172893822193146 Accuracy 0.460205078125\n",
      "Iteration 56970 Training loss 0.053492091596126556 Validation loss 0.05318700894713402 Accuracy 0.460205078125\n",
      "Iteration 56980 Training loss 0.05142722651362419 Validation loss 0.05340087413787842 Accuracy 0.458740234375\n",
      "Iteration 56990 Training loss 0.054660309106111526 Validation loss 0.05311746895313263 Accuracy 0.460693359375\n",
      "Iteration 57000 Training loss 0.04795039817690849 Validation loss 0.05350145697593689 Accuracy 0.458251953125\n",
      "Iteration 57010 Training loss 0.04960549250245094 Validation loss 0.05312635004520416 Accuracy 0.460205078125\n",
      "Iteration 57020 Training loss 0.05123112350702286 Validation loss 0.05299939215183258 Accuracy 0.4609375\n",
      "Iteration 57030 Training loss 0.05047920346260071 Validation loss 0.05300910025835037 Accuracy 0.4609375\n",
      "Iteration 57040 Training loss 0.05500642955303192 Validation loss 0.05313974618911743 Accuracy 0.459716796875\n",
      "Iteration 57050 Training loss 0.05339367687702179 Validation loss 0.05326205864548683 Accuracy 0.4609375\n",
      "Iteration 57060 Training loss 0.05252936854958534 Validation loss 0.05322742089629173 Accuracy 0.460205078125\n",
      "Iteration 57070 Training loss 0.051505573093891144 Validation loss 0.05337625741958618 Accuracy 0.4580078125\n",
      "Iteration 57080 Training loss 0.051375921815633774 Validation loss 0.05315849557518959 Accuracy 0.460693359375\n",
      "Iteration 57090 Training loss 0.053356923162937164 Validation loss 0.053397465497255325 Accuracy 0.45849609375\n",
      "Iteration 57100 Training loss 0.053109947592020035 Validation loss 0.05348284915089607 Accuracy 0.458251953125\n",
      "Iteration 57110 Training loss 0.05185837671160698 Validation loss 0.0534069649875164 Accuracy 0.459228515625\n",
      "Iteration 57120 Training loss 0.05304911360144615 Validation loss 0.05333474650979042 Accuracy 0.459716796875\n",
      "Iteration 57130 Training loss 0.0484401173889637 Validation loss 0.05328160896897316 Accuracy 0.458984375\n",
      "Iteration 57140 Training loss 0.05383860319852829 Validation loss 0.05312642827630043 Accuracy 0.460205078125\n",
      "Iteration 57150 Training loss 0.05296725034713745 Validation loss 0.053886719048023224 Accuracy 0.455078125\n",
      "Iteration 57160 Training loss 0.05325103551149368 Validation loss 0.05337400361895561 Accuracy 0.459716796875\n",
      "Iteration 57170 Training loss 0.050793975591659546 Validation loss 0.05321939289569855 Accuracy 0.459228515625\n",
      "Iteration 57180 Training loss 0.050192367285490036 Validation loss 0.05341142416000366 Accuracy 0.458984375\n",
      "Iteration 57190 Training loss 0.04996616393327713 Validation loss 0.05363444611430168 Accuracy 0.4580078125\n",
      "Iteration 57200 Training loss 0.054494500160217285 Validation loss 0.0534815788269043 Accuracy 0.45849609375\n",
      "Iteration 57210 Training loss 0.050609927624464035 Validation loss 0.05331004783511162 Accuracy 0.458251953125\n",
      "Iteration 57220 Training loss 0.048205625265836716 Validation loss 0.05330151319503784 Accuracy 0.45947265625\n",
      "Iteration 57230 Training loss 0.052199166268110275 Validation loss 0.053449030965566635 Accuracy 0.45751953125\n",
      "Iteration 57240 Training loss 0.05465609207749367 Validation loss 0.05298389494419098 Accuracy 0.461669921875\n",
      "Iteration 57250 Training loss 0.05204470083117485 Validation loss 0.053284790366888046 Accuracy 0.459228515625\n",
      "Iteration 57260 Training loss 0.05219569802284241 Validation loss 0.05336148291826248 Accuracy 0.458984375\n",
      "Iteration 57270 Training loss 0.05189697816967964 Validation loss 0.054186638444662094 Accuracy 0.45166015625\n",
      "Iteration 57280 Training loss 0.0499708466231823 Validation loss 0.05310796573758125 Accuracy 0.460205078125\n",
      "Iteration 57290 Training loss 0.051251765340566635 Validation loss 0.05332161486148834 Accuracy 0.46044921875\n",
      "Iteration 57300 Training loss 0.048723023384809494 Validation loss 0.053190458565950394 Accuracy 0.459716796875\n",
      "Iteration 57310 Training loss 0.04919826239347458 Validation loss 0.053694870322942734 Accuracy 0.45703125\n",
      "Iteration 57320 Training loss 0.054266784340143204 Validation loss 0.05333230271935463 Accuracy 0.45849609375\n",
      "Iteration 57330 Training loss 0.0478639230132103 Validation loss 0.05338844656944275 Accuracy 0.458251953125\n",
      "Iteration 57340 Training loss 0.053503599017858505 Validation loss 0.0536477230489254 Accuracy 0.45751953125\n",
      "Iteration 57350 Training loss 0.053215280175209045 Validation loss 0.05323692411184311 Accuracy 0.459716796875\n",
      "Iteration 57360 Training loss 0.04950616508722305 Validation loss 0.05350598320364952 Accuracy 0.455810546875\n",
      "Iteration 57370 Training loss 0.05037133768200874 Validation loss 0.05302108824253082 Accuracy 0.4609375\n",
      "Iteration 57380 Training loss 0.05226245895028114 Validation loss 0.053670693188905716 Accuracy 0.45556640625\n",
      "Iteration 57390 Training loss 0.04995659366250038 Validation loss 0.05348072201013565 Accuracy 0.45849609375\n",
      "Iteration 57400 Training loss 0.05383913218975067 Validation loss 0.05334557592868805 Accuracy 0.459716796875\n",
      "Iteration 57410 Training loss 0.051825348287820816 Validation loss 0.05322172865271568 Accuracy 0.460205078125\n",
      "Iteration 57420 Training loss 0.053430572152137756 Validation loss 0.05331144481897354 Accuracy 0.459716796875\n",
      "Iteration 57430 Training loss 0.05080894008278847 Validation loss 0.05324916914105415 Accuracy 0.45849609375\n",
      "Iteration 57440 Training loss 0.049242712557315826 Validation loss 0.05331619828939438 Accuracy 0.457763671875\n",
      "Iteration 57450 Training loss 0.0497320182621479 Validation loss 0.053343191742897034 Accuracy 0.4580078125\n",
      "Iteration 57460 Training loss 0.04959765449166298 Validation loss 0.053068216890096664 Accuracy 0.4609375\n",
      "Iteration 57470 Training loss 0.05031704530119896 Validation loss 0.05327574908733368 Accuracy 0.458984375\n",
      "Iteration 57480 Training loss 0.053873054683208466 Validation loss 0.05348949134349823 Accuracy 0.45849609375\n",
      "Iteration 57490 Training loss 0.05346794053912163 Validation loss 0.05305909365415573 Accuracy 0.46142578125\n",
      "Iteration 57500 Training loss 0.05214397609233856 Validation loss 0.05307968705892563 Accuracy 0.460693359375\n",
      "Iteration 57510 Training loss 0.05169648304581642 Validation loss 0.05385010689496994 Accuracy 0.455078125\n",
      "Iteration 57520 Training loss 0.05441228300333023 Validation loss 0.053330931812524796 Accuracy 0.45849609375\n",
      "Iteration 57530 Training loss 0.05111726000905037 Validation loss 0.05349668487906456 Accuracy 0.458251953125\n",
      "Iteration 57540 Training loss 0.05254722014069557 Validation loss 0.053112614899873734 Accuracy 0.4599609375\n",
      "Iteration 57550 Training loss 0.05189929157495499 Validation loss 0.053249750286340714 Accuracy 0.459716796875\n",
      "Iteration 57560 Training loss 0.05150454491376877 Validation loss 0.053187157958745956 Accuracy 0.4609375\n",
      "Iteration 57570 Training loss 0.05587271600961685 Validation loss 0.05311337858438492 Accuracy 0.461181640625\n",
      "Iteration 57580 Training loss 0.04790627956390381 Validation loss 0.05363516882061958 Accuracy 0.4580078125\n",
      "Iteration 57590 Training loss 0.04744475707411766 Validation loss 0.05329245328903198 Accuracy 0.458984375\n",
      "Iteration 57600 Training loss 0.051058828830718994 Validation loss 0.05300608277320862 Accuracy 0.4609375\n",
      "Iteration 57610 Training loss 0.04631257429718971 Validation loss 0.05320370942354202 Accuracy 0.4599609375\n",
      "Iteration 57620 Training loss 0.05626024305820465 Validation loss 0.053162138909101486 Accuracy 0.459716796875\n",
      "Iteration 57630 Training loss 0.04903493821620941 Validation loss 0.053395599126815796 Accuracy 0.458740234375\n",
      "Iteration 57640 Training loss 0.050654102116823196 Validation loss 0.05312902107834816 Accuracy 0.46044921875\n",
      "Iteration 57650 Training loss 0.04807734116911888 Validation loss 0.05337921902537346 Accuracy 0.45849609375\n",
      "Iteration 57660 Training loss 0.04939652606844902 Validation loss 0.05321095511317253 Accuracy 0.4599609375\n",
      "Iteration 57670 Training loss 0.05323762446641922 Validation loss 0.05340844765305519 Accuracy 0.458984375\n",
      "Iteration 57680 Training loss 0.05042463168501854 Validation loss 0.05321810394525528 Accuracy 0.459716796875\n",
      "Iteration 57690 Training loss 0.05046328529715538 Validation loss 0.053218573331832886 Accuracy 0.4599609375\n",
      "Iteration 57700 Training loss 0.049409858882427216 Validation loss 0.053159549832344055 Accuracy 0.460205078125\n",
      "Iteration 57710 Training loss 0.050567299127578735 Validation loss 0.05307479202747345 Accuracy 0.460693359375\n",
      "Iteration 57720 Training loss 0.05414954200387001 Validation loss 0.053694069385528564 Accuracy 0.456298828125\n",
      "Iteration 57730 Training loss 0.05259731039404869 Validation loss 0.053333260118961334 Accuracy 0.45947265625\n",
      "Iteration 57740 Training loss 0.05128243938088417 Validation loss 0.05291944369673729 Accuracy 0.461181640625\n",
      "Iteration 57750 Training loss 0.05414671450853348 Validation loss 0.05299971625208855 Accuracy 0.46142578125\n",
      "Iteration 57760 Training loss 0.05153155326843262 Validation loss 0.05314858257770538 Accuracy 0.4599609375\n",
      "Iteration 57770 Training loss 0.050948768854141235 Validation loss 0.05306345596909523 Accuracy 0.4599609375\n",
      "Iteration 57780 Training loss 0.05155593901872635 Validation loss 0.05374494940042496 Accuracy 0.4560546875\n",
      "Iteration 57790 Training loss 0.05455460026860237 Validation loss 0.05320620536804199 Accuracy 0.46044921875\n",
      "Iteration 57800 Training loss 0.05239023268222809 Validation loss 0.053695522248744965 Accuracy 0.455078125\n",
      "Iteration 57810 Training loss 0.05065223574638367 Validation loss 0.05293670669198036 Accuracy 0.461181640625\n",
      "Iteration 57820 Training loss 0.05338042974472046 Validation loss 0.05330759659409523 Accuracy 0.460693359375\n",
      "Iteration 57830 Training loss 0.05422494560480118 Validation loss 0.05323590710759163 Accuracy 0.458984375\n",
      "Iteration 57840 Training loss 0.05252143368124962 Validation loss 0.05353228375315666 Accuracy 0.456787109375\n",
      "Iteration 57850 Training loss 0.05281849578022957 Validation loss 0.053184885531663895 Accuracy 0.458740234375\n",
      "Iteration 57860 Training loss 0.05269888415932655 Validation loss 0.05338740721344948 Accuracy 0.4580078125\n",
      "Iteration 57870 Training loss 0.05105539783835411 Validation loss 0.05342867225408554 Accuracy 0.458984375\n",
      "Iteration 57880 Training loss 0.054284535348415375 Validation loss 0.05308377742767334 Accuracy 0.460205078125\n",
      "Iteration 57890 Training loss 0.05085751786828041 Validation loss 0.05333079397678375 Accuracy 0.458740234375\n",
      "Iteration 57900 Training loss 0.053657546639442444 Validation loss 0.053227927535772324 Accuracy 0.459228515625\n",
      "Iteration 57910 Training loss 0.05369970574975014 Validation loss 0.053124524652957916 Accuracy 0.4599609375\n",
      "Iteration 57920 Training loss 0.05481787025928497 Validation loss 0.05346408486366272 Accuracy 0.45849609375\n",
      "Iteration 57930 Training loss 0.052326902747154236 Validation loss 0.05325759947299957 Accuracy 0.459716796875\n",
      "Iteration 57940 Training loss 0.04998819902539253 Validation loss 0.053532958030700684 Accuracy 0.45654296875\n",
      "Iteration 57950 Training loss 0.048232950270175934 Validation loss 0.05325751751661301 Accuracy 0.45849609375\n",
      "Iteration 57960 Training loss 0.051335833966732025 Validation loss 0.054323747754096985 Accuracy 0.450439453125\n",
      "Iteration 57970 Training loss 0.04987763240933418 Validation loss 0.05332062393426895 Accuracy 0.459716796875\n",
      "Iteration 57980 Training loss 0.049644745886325836 Validation loss 0.053374335169792175 Accuracy 0.458984375\n",
      "Iteration 57990 Training loss 0.050018951296806335 Validation loss 0.05320916324853897 Accuracy 0.460205078125\n",
      "Iteration 58000 Training loss 0.052740681916475296 Validation loss 0.05316203832626343 Accuracy 0.460205078125\n",
      "Iteration 58010 Training loss 0.05289715155959129 Validation loss 0.0530698336660862 Accuracy 0.460205078125\n",
      "Iteration 58020 Training loss 0.05409436300396919 Validation loss 0.05324625223875046 Accuracy 0.459228515625\n",
      "Iteration 58030 Training loss 0.056070320308208466 Validation loss 0.05313844233751297 Accuracy 0.459716796875\n",
      "Iteration 58040 Training loss 0.05018840357661247 Validation loss 0.05315537005662918 Accuracy 0.459716796875\n",
      "Iteration 58050 Training loss 0.05263075605034828 Validation loss 0.053420424461364746 Accuracy 0.4580078125\n",
      "Iteration 58060 Training loss 0.05333653837442398 Validation loss 0.05327031761407852 Accuracy 0.45849609375\n",
      "Iteration 58070 Training loss 0.054951369762420654 Validation loss 0.053469784557819366 Accuracy 0.45751953125\n",
      "Iteration 58080 Training loss 0.04825310781598091 Validation loss 0.05332629755139351 Accuracy 0.459228515625\n",
      "Iteration 58090 Training loss 0.0502299964427948 Validation loss 0.05330692231655121 Accuracy 0.459228515625\n",
      "Iteration 58100 Training loss 0.052230771631002426 Validation loss 0.05372608080506325 Accuracy 0.4541015625\n",
      "Iteration 58110 Training loss 0.05247364193201065 Validation loss 0.053272370249032974 Accuracy 0.458251953125\n",
      "Iteration 58120 Training loss 0.05418067425489426 Validation loss 0.05346067249774933 Accuracy 0.459228515625\n",
      "Iteration 58130 Training loss 0.0501951202750206 Validation loss 0.05320415273308754 Accuracy 0.45849609375\n",
      "Iteration 58140 Training loss 0.051001399755477905 Validation loss 0.053425438702106476 Accuracy 0.458984375\n",
      "Iteration 58150 Training loss 0.04814470186829567 Validation loss 0.053505588322877884 Accuracy 0.457763671875\n",
      "Iteration 58160 Training loss 0.05331873148679733 Validation loss 0.05322930961847305 Accuracy 0.458251953125\n",
      "Iteration 58170 Training loss 0.04817524179816246 Validation loss 0.05323990061879158 Accuracy 0.460205078125\n",
      "Iteration 58180 Training loss 0.05312572047114372 Validation loss 0.05333063751459122 Accuracy 0.458740234375\n",
      "Iteration 58190 Training loss 0.05088227614760399 Validation loss 0.05313386768102646 Accuracy 0.45947265625\n",
      "Iteration 58200 Training loss 0.05211223289370537 Validation loss 0.053604915738105774 Accuracy 0.457275390625\n",
      "Iteration 58210 Training loss 0.05078805610537529 Validation loss 0.0533488467335701 Accuracy 0.45849609375\n",
      "Iteration 58220 Training loss 0.05159282311797142 Validation loss 0.05334429442882538 Accuracy 0.457763671875\n",
      "Iteration 58230 Training loss 0.05053647235035896 Validation loss 0.05315253138542175 Accuracy 0.460205078125\n",
      "Iteration 58240 Training loss 0.04961308091878891 Validation loss 0.0533333383500576 Accuracy 0.45947265625\n",
      "Iteration 58250 Training loss 0.04996667057275772 Validation loss 0.052901141345500946 Accuracy 0.4619140625\n",
      "Iteration 58260 Training loss 0.05323833227157593 Validation loss 0.053527895361185074 Accuracy 0.458251953125\n",
      "Iteration 58270 Training loss 0.049569204449653625 Validation loss 0.05319751426577568 Accuracy 0.459716796875\n",
      "Iteration 58280 Training loss 0.049452077597379684 Validation loss 0.05342671647667885 Accuracy 0.4580078125\n",
      "Iteration 58290 Training loss 0.04947073385119438 Validation loss 0.05356152355670929 Accuracy 0.458984375\n",
      "Iteration 58300 Training loss 0.04755682498216629 Validation loss 0.05348488315939903 Accuracy 0.45849609375\n",
      "Iteration 58310 Training loss 0.04979618266224861 Validation loss 0.05321047827601433 Accuracy 0.459716796875\n",
      "Iteration 58320 Training loss 0.05116092413663864 Validation loss 0.05326634645462036 Accuracy 0.458740234375\n",
      "Iteration 58330 Training loss 0.05212514102458954 Validation loss 0.05312406271696091 Accuracy 0.460693359375\n",
      "Iteration 58340 Training loss 0.05122363567352295 Validation loss 0.05349601432681084 Accuracy 0.4580078125\n",
      "Iteration 58350 Training loss 0.05137276649475098 Validation loss 0.05331389978528023 Accuracy 0.45947265625\n",
      "Iteration 58360 Training loss 0.05301365256309509 Validation loss 0.053158923983573914 Accuracy 0.46044921875\n",
      "Iteration 58370 Training loss 0.054844215512275696 Validation loss 0.05324029177427292 Accuracy 0.458984375\n",
      "Iteration 58380 Training loss 0.05258815363049507 Validation loss 0.053289745002985 Accuracy 0.457763671875\n",
      "Iteration 58390 Training loss 0.05229591950774193 Validation loss 0.05378701910376549 Accuracy 0.455078125\n",
      "Iteration 58400 Training loss 0.05001499503850937 Validation loss 0.05340598523616791 Accuracy 0.45849609375\n",
      "Iteration 58410 Training loss 0.04963890463113785 Validation loss 0.05334506556391716 Accuracy 0.4580078125\n",
      "Iteration 58420 Training loss 0.050980690866708755 Validation loss 0.053050149232149124 Accuracy 0.46142578125\n",
      "Iteration 58430 Training loss 0.0512937530875206 Validation loss 0.05292169749736786 Accuracy 0.4619140625\n",
      "Iteration 58440 Training loss 0.049076538532972336 Validation loss 0.05313641577959061 Accuracy 0.460693359375\n",
      "Iteration 58450 Training loss 0.05051897093653679 Validation loss 0.05313803628087044 Accuracy 0.460205078125\n",
      "Iteration 58460 Training loss 0.053282156586647034 Validation loss 0.053834836930036545 Accuracy 0.455078125\n",
      "Iteration 58470 Training loss 0.052812520414590836 Validation loss 0.0534270778298378 Accuracy 0.4580078125\n",
      "Iteration 58480 Training loss 0.050253551453351974 Validation loss 0.053172171115875244 Accuracy 0.4609375\n",
      "Iteration 58490 Training loss 0.049042414873838425 Validation loss 0.05305122956633568 Accuracy 0.46142578125\n",
      "Iteration 58500 Training loss 0.050221048295497894 Validation loss 0.05320074036717415 Accuracy 0.460205078125\n",
      "Iteration 58510 Training loss 0.05284464359283447 Validation loss 0.05311698094010353 Accuracy 0.460693359375\n",
      "Iteration 58520 Training loss 0.04665292426943779 Validation loss 0.05306881666183472 Accuracy 0.461669921875\n",
      "Iteration 58530 Training loss 0.04892410337924957 Validation loss 0.0533725880086422 Accuracy 0.458984375\n",
      "Iteration 58540 Training loss 0.053565237671136856 Validation loss 0.05341348052024841 Accuracy 0.4580078125\n",
      "Iteration 58550 Training loss 0.05595891922712326 Validation loss 0.05301399156451225 Accuracy 0.460205078125\n",
      "Iteration 58560 Training loss 0.04996905475854874 Validation loss 0.053196750581264496 Accuracy 0.460205078125\n",
      "Iteration 58570 Training loss 0.0490741990506649 Validation loss 0.0531773716211319 Accuracy 0.45947265625\n",
      "Iteration 58580 Training loss 0.051192332059144974 Validation loss 0.053164564073085785 Accuracy 0.460205078125\n",
      "Iteration 58590 Training loss 0.052296482026576996 Validation loss 0.05313211679458618 Accuracy 0.4609375\n",
      "Iteration 58600 Training loss 0.05252249538898468 Validation loss 0.05311856418848038 Accuracy 0.460693359375\n",
      "Iteration 58610 Training loss 0.05222855508327484 Validation loss 0.05335071310400963 Accuracy 0.458251953125\n",
      "Iteration 58620 Training loss 0.051173411309719086 Validation loss 0.05300215631723404 Accuracy 0.460693359375\n",
      "Iteration 58630 Training loss 0.05090382695198059 Validation loss 0.05369158461689949 Accuracy 0.454833984375\n",
      "Iteration 58640 Training loss 0.05133926495909691 Validation loss 0.05317571014165878 Accuracy 0.46044921875\n",
      "Iteration 58650 Training loss 0.048163726925849915 Validation loss 0.05312628671526909 Accuracy 0.45947265625\n",
      "Iteration 58660 Training loss 0.05049445480108261 Validation loss 0.05321390554308891 Accuracy 0.459228515625\n",
      "Iteration 58670 Training loss 0.048004209995269775 Validation loss 0.05307198688387871 Accuracy 0.460205078125\n",
      "Iteration 58680 Training loss 0.048026617616415024 Validation loss 0.053309258073568344 Accuracy 0.45849609375\n",
      "Iteration 58690 Training loss 0.0515095517039299 Validation loss 0.053007058799266815 Accuracy 0.4609375\n",
      "Iteration 58700 Training loss 0.05153794214129448 Validation loss 0.05357074737548828 Accuracy 0.45654296875\n",
      "Iteration 58710 Training loss 0.05184108391404152 Validation loss 0.05325934290885925 Accuracy 0.45849609375\n",
      "Iteration 58720 Training loss 0.049017343670129776 Validation loss 0.05329468473792076 Accuracy 0.45849609375\n",
      "Iteration 58730 Training loss 0.04752226173877716 Validation loss 0.05339111015200615 Accuracy 0.4580078125\n",
      "Iteration 58740 Training loss 0.05095081403851509 Validation loss 0.05290636420249939 Accuracy 0.461669921875\n",
      "Iteration 58750 Training loss 0.0492447093129158 Validation loss 0.05364745110273361 Accuracy 0.457275390625\n",
      "Iteration 58760 Training loss 0.05286751687526703 Validation loss 0.0530770979821682 Accuracy 0.459716796875\n",
      "Iteration 58770 Training loss 0.05121031031012535 Validation loss 0.05350532755255699 Accuracy 0.458984375\n",
      "Iteration 58780 Training loss 0.05003737285733223 Validation loss 0.05319838598370552 Accuracy 0.46044921875\n",
      "Iteration 58790 Training loss 0.052262939512729645 Validation loss 0.05316133052110672 Accuracy 0.459716796875\n",
      "Iteration 58800 Training loss 0.05137034133076668 Validation loss 0.052959688007831573 Accuracy 0.461181640625\n",
      "Iteration 58810 Training loss 0.051947180181741714 Validation loss 0.05342668667435646 Accuracy 0.457763671875\n",
      "Iteration 58820 Training loss 0.05092724412679672 Validation loss 0.05308986082673073 Accuracy 0.46142578125\n",
      "Iteration 58830 Training loss 0.0504605695605278 Validation loss 0.053207509219646454 Accuracy 0.459716796875\n",
      "Iteration 58840 Training loss 0.052494924515485764 Validation loss 0.05311296135187149 Accuracy 0.46044921875\n",
      "Iteration 58850 Training loss 0.05165433883666992 Validation loss 0.0536637157201767 Accuracy 0.45361328125\n",
      "Iteration 58860 Training loss 0.05076100677251816 Validation loss 0.05315793678164482 Accuracy 0.45947265625\n",
      "Iteration 58870 Training loss 0.05007974058389664 Validation loss 0.0533549003303051 Accuracy 0.4580078125\n",
      "Iteration 58880 Training loss 0.052070800215005875 Validation loss 0.053234513849020004 Accuracy 0.4599609375\n",
      "Iteration 58890 Training loss 0.051985982805490494 Validation loss 0.05370175093412399 Accuracy 0.455322265625\n",
      "Iteration 58900 Training loss 0.0500311478972435 Validation loss 0.0532679446041584 Accuracy 0.459716796875\n",
      "Iteration 58910 Training loss 0.05299178510904312 Validation loss 0.052966006100177765 Accuracy 0.460693359375\n",
      "Iteration 58920 Training loss 0.05270129442214966 Validation loss 0.05334622040390968 Accuracy 0.460205078125\n",
      "Iteration 58930 Training loss 0.051574595272541046 Validation loss 0.0538179911673069 Accuracy 0.455078125\n",
      "Iteration 58940 Training loss 0.05394560843706131 Validation loss 0.05309643596410751 Accuracy 0.4599609375\n",
      "Iteration 58950 Training loss 0.05100527033209801 Validation loss 0.05314449593424797 Accuracy 0.459716796875\n",
      "Iteration 58960 Training loss 0.052139170467853546 Validation loss 0.053045809268951416 Accuracy 0.46044921875\n",
      "Iteration 58970 Training loss 0.056140974164009094 Validation loss 0.05359867960214615 Accuracy 0.458251953125\n",
      "Iteration 58980 Training loss 0.051952462643384933 Validation loss 0.05320919677615166 Accuracy 0.45849609375\n",
      "Iteration 58990 Training loss 0.05339053273200989 Validation loss 0.05345721170306206 Accuracy 0.458251953125\n",
      "Iteration 59000 Training loss 0.05027458071708679 Validation loss 0.053227975964546204 Accuracy 0.460205078125\n",
      "Iteration 59010 Training loss 0.049999892711639404 Validation loss 0.0530976727604866 Accuracy 0.461669921875\n",
      "Iteration 59020 Training loss 0.051416199654340744 Validation loss 0.05324694514274597 Accuracy 0.4609375\n",
      "Iteration 59030 Training loss 0.05102188512682915 Validation loss 0.053227249532938004 Accuracy 0.45947265625\n",
      "Iteration 59040 Training loss 0.04696902260184288 Validation loss 0.053080204874277115 Accuracy 0.461181640625\n",
      "Iteration 59050 Training loss 0.04943939670920372 Validation loss 0.05378872528672218 Accuracy 0.453857421875\n",
      "Iteration 59060 Training loss 0.05617179721593857 Validation loss 0.05363040044903755 Accuracy 0.455078125\n",
      "Iteration 59070 Training loss 0.04775533080101013 Validation loss 0.05324011668562889 Accuracy 0.459228515625\n",
      "Iteration 59080 Training loss 0.052058495581150055 Validation loss 0.05297928676009178 Accuracy 0.4609375\n",
      "Iteration 59090 Training loss 0.055959511548280716 Validation loss 0.053109388798475266 Accuracy 0.459716796875\n",
      "Iteration 59100 Training loss 0.048266422003507614 Validation loss 0.053354039788246155 Accuracy 0.45849609375\n",
      "Iteration 59110 Training loss 0.05300969257950783 Validation loss 0.05344722047448158 Accuracy 0.4580078125\n",
      "Iteration 59120 Training loss 0.05214501917362213 Validation loss 0.0532328225672245 Accuracy 0.45947265625\n",
      "Iteration 59130 Training loss 0.05063877999782562 Validation loss 0.05316047742962837 Accuracy 0.460205078125\n",
      "Iteration 59140 Training loss 0.05087674781680107 Validation loss 0.05334681645035744 Accuracy 0.45947265625\n",
      "Iteration 59150 Training loss 0.04921700060367584 Validation loss 0.0531432218849659 Accuracy 0.460205078125\n",
      "Iteration 59160 Training loss 0.05306881666183472 Validation loss 0.05309843644499779 Accuracy 0.459716796875\n",
      "Iteration 59170 Training loss 0.052443843334913254 Validation loss 0.05309140309691429 Accuracy 0.4619140625\n",
      "Iteration 59180 Training loss 0.05592407286167145 Validation loss 0.0537145659327507 Accuracy 0.45556640625\n",
      "Iteration 59190 Training loss 0.05119332671165466 Validation loss 0.053100910037755966 Accuracy 0.461181640625\n",
      "Iteration 59200 Training loss 0.05299080163240433 Validation loss 0.05324377864599228 Accuracy 0.45947265625\n",
      "Iteration 59210 Training loss 0.05069437623023987 Validation loss 0.053119104355573654 Accuracy 0.460693359375\n",
      "Iteration 59220 Training loss 0.052213847637176514 Validation loss 0.053327396512031555 Accuracy 0.459228515625\n",
      "Iteration 59230 Training loss 0.05094683915376663 Validation loss 0.053280409425497055 Accuracy 0.45947265625\n",
      "Iteration 59240 Training loss 0.05021084472537041 Validation loss 0.05333518981933594 Accuracy 0.460205078125\n",
      "Iteration 59250 Training loss 0.049479663372039795 Validation loss 0.0529768280684948 Accuracy 0.461669921875\n",
      "Iteration 59260 Training loss 0.05246227979660034 Validation loss 0.05303831398487091 Accuracy 0.461181640625\n",
      "Iteration 59270 Training loss 0.05395955219864845 Validation loss 0.053063053637742996 Accuracy 0.46044921875\n",
      "Iteration 59280 Training loss 0.05373130366206169 Validation loss 0.053317807614803314 Accuracy 0.4599609375\n",
      "Iteration 59290 Training loss 0.04920615628361702 Validation loss 0.05288810282945633 Accuracy 0.46240234375\n",
      "Iteration 59300 Training loss 0.05211480334401131 Validation loss 0.05309075862169266 Accuracy 0.4599609375\n",
      "Iteration 59310 Training loss 0.04892157018184662 Validation loss 0.053245022892951965 Accuracy 0.460205078125\n",
      "Iteration 59320 Training loss 0.051336802542209625 Validation loss 0.05313834175467491 Accuracy 0.460693359375\n",
      "Iteration 59330 Training loss 0.05349479243159294 Validation loss 0.05316973477602005 Accuracy 0.460205078125\n",
      "Iteration 59340 Training loss 0.04743200168013573 Validation loss 0.0529431588947773 Accuracy 0.461181640625\n",
      "Iteration 59350 Training loss 0.04934091120958328 Validation loss 0.053277477622032166 Accuracy 0.460693359375\n",
      "Iteration 59360 Training loss 0.05127060040831566 Validation loss 0.05318664759397507 Accuracy 0.4599609375\n",
      "Iteration 59370 Training loss 0.04981658235192299 Validation loss 0.053374338895082474 Accuracy 0.458984375\n",
      "Iteration 59380 Training loss 0.050635915249586105 Validation loss 0.05321021005511284 Accuracy 0.45947265625\n",
      "Iteration 59390 Training loss 0.04965078830718994 Validation loss 0.05313873291015625 Accuracy 0.460205078125\n",
      "Iteration 59400 Training loss 0.04968167468905449 Validation loss 0.053113896399736404 Accuracy 0.45947265625\n",
      "Iteration 59410 Training loss 0.05081932619214058 Validation loss 0.05385977402329445 Accuracy 0.455078125\n",
      "Iteration 59420 Training loss 0.05515913665294647 Validation loss 0.05435590073466301 Accuracy 0.44775390625\n",
      "Iteration 59430 Training loss 0.04993468523025513 Validation loss 0.053105536848306656 Accuracy 0.46044921875\n",
      "Iteration 59440 Training loss 0.05029298737645149 Validation loss 0.05308761075139046 Accuracy 0.461181640625\n",
      "Iteration 59450 Training loss 0.051823362708091736 Validation loss 0.05304255336523056 Accuracy 0.4619140625\n",
      "Iteration 59460 Training loss 0.05105241760611534 Validation loss 0.05306955799460411 Accuracy 0.461181640625\n",
      "Iteration 59470 Training loss 0.04785453900694847 Validation loss 0.0529695600271225 Accuracy 0.4609375\n",
      "Iteration 59480 Training loss 0.051059428602457047 Validation loss 0.05306628718972206 Accuracy 0.461181640625\n",
      "Iteration 59490 Training loss 0.04974878206849098 Validation loss 0.05292051285505295 Accuracy 0.46142578125\n",
      "Iteration 59500 Training loss 0.05487325042486191 Validation loss 0.05340119078755379 Accuracy 0.458740234375\n",
      "Iteration 59510 Training loss 0.051641907542943954 Validation loss 0.053461767733097076 Accuracy 0.45849609375\n",
      "Iteration 59520 Training loss 0.05506366491317749 Validation loss 0.05316440388560295 Accuracy 0.460205078125\n",
      "Iteration 59530 Training loss 0.051263466477394104 Validation loss 0.053220994770526886 Accuracy 0.45849609375\n",
      "Iteration 59540 Training loss 0.053017374128103256 Validation loss 0.05332371965050697 Accuracy 0.459716796875\n",
      "Iteration 59550 Training loss 0.052674055099487305 Validation loss 0.05309347063302994 Accuracy 0.460205078125\n",
      "Iteration 59560 Training loss 0.052077729254961014 Validation loss 0.05350201204419136 Accuracy 0.4580078125\n",
      "Iteration 59570 Training loss 0.05127986520528793 Validation loss 0.05319042503833771 Accuracy 0.459716796875\n",
      "Iteration 59580 Training loss 0.05360347032546997 Validation loss 0.05336834862828255 Accuracy 0.459228515625\n",
      "Iteration 59590 Training loss 0.04913843795657158 Validation loss 0.05329076945781708 Accuracy 0.459716796875\n",
      "Iteration 59600 Training loss 0.04930833727121353 Validation loss 0.053271155804395676 Accuracy 0.45849609375\n",
      "Iteration 59610 Training loss 0.05128052830696106 Validation loss 0.05296409875154495 Accuracy 0.4619140625\n",
      "Iteration 59620 Training loss 0.051480598747730255 Validation loss 0.053311701864004135 Accuracy 0.459228515625\n",
      "Iteration 59630 Training loss 0.05011632293462753 Validation loss 0.05318554863333702 Accuracy 0.459716796875\n",
      "Iteration 59640 Training loss 0.05318520963191986 Validation loss 0.05361371487379074 Accuracy 0.45751953125\n",
      "Iteration 59650 Training loss 0.05206047371029854 Validation loss 0.05313628911972046 Accuracy 0.45947265625\n",
      "Iteration 59660 Training loss 0.052947498857975006 Validation loss 0.05460687354207039 Accuracy 0.446533203125\n",
      "Iteration 59670 Training loss 0.04974614456295967 Validation loss 0.05316914990544319 Accuracy 0.460205078125\n",
      "Iteration 59680 Training loss 0.05007864162325859 Validation loss 0.05381880700588226 Accuracy 0.454345703125\n",
      "Iteration 59690 Training loss 0.049966126680374146 Validation loss 0.05317094177007675 Accuracy 0.4609375\n",
      "Iteration 59700 Training loss 0.05188316851854324 Validation loss 0.053385887295007706 Accuracy 0.45849609375\n",
      "Iteration 59710 Training loss 0.05255149304866791 Validation loss 0.05296749994158745 Accuracy 0.460693359375\n",
      "Iteration 59720 Training loss 0.04674651101231575 Validation loss 0.053198158740997314 Accuracy 0.4599609375\n",
      "Iteration 59730 Training loss 0.050837259739637375 Validation loss 0.05337686464190483 Accuracy 0.458984375\n",
      "Iteration 59740 Training loss 0.05180121958255768 Validation loss 0.05321282893419266 Accuracy 0.459228515625\n",
      "Iteration 59750 Training loss 0.052009113132953644 Validation loss 0.053362660109996796 Accuracy 0.45849609375\n",
      "Iteration 59760 Training loss 0.04932306706905365 Validation loss 0.05351053550839424 Accuracy 0.45751953125\n",
      "Iteration 59770 Training loss 0.05123134329915047 Validation loss 0.05348097160458565 Accuracy 0.45751953125\n",
      "Iteration 59780 Training loss 0.05054811015725136 Validation loss 0.05301240086555481 Accuracy 0.459228515625\n",
      "Iteration 59790 Training loss 0.05002305656671524 Validation loss 0.05325031280517578 Accuracy 0.4599609375\n",
      "Iteration 59800 Training loss 0.04951748624444008 Validation loss 0.05304959416389465 Accuracy 0.460693359375\n",
      "Iteration 59810 Training loss 0.04835706576704979 Validation loss 0.053097546100616455 Accuracy 0.4619140625\n",
      "Iteration 59820 Training loss 0.049577269703149796 Validation loss 0.053423408418893814 Accuracy 0.458984375\n",
      "Iteration 59830 Training loss 0.050946034491062164 Validation loss 0.0535343699157238 Accuracy 0.456787109375\n",
      "Iteration 59840 Training loss 0.048592519015073776 Validation loss 0.05300144478678703 Accuracy 0.46142578125\n",
      "Iteration 59850 Training loss 0.04768623784184456 Validation loss 0.05306234210729599 Accuracy 0.459716796875\n",
      "Iteration 59860 Training loss 0.04922216758131981 Validation loss 0.053056854754686356 Accuracy 0.459716796875\n",
      "Iteration 59870 Training loss 0.051195982843637466 Validation loss 0.05322312191128731 Accuracy 0.460205078125\n",
      "Iteration 59880 Training loss 0.0530884750187397 Validation loss 0.053035300225019455 Accuracy 0.460205078125\n",
      "Iteration 59890 Training loss 0.05271453782916069 Validation loss 0.053160037845373154 Accuracy 0.460693359375\n",
      "Iteration 59900 Training loss 0.051316037774086 Validation loss 0.053754545748233795 Accuracy 0.456298828125\n",
      "Iteration 59910 Training loss 0.049710460007190704 Validation loss 0.053049758076667786 Accuracy 0.460205078125\n",
      "Iteration 59920 Training loss 0.05016014724969864 Validation loss 0.053081903606653214 Accuracy 0.460693359375\n",
      "Iteration 59930 Training loss 0.053749654442071915 Validation loss 0.05408408120274544 Accuracy 0.454345703125\n",
      "Iteration 59940 Training loss 0.051021136343479156 Validation loss 0.05322624742984772 Accuracy 0.4580078125\n",
      "Iteration 59950 Training loss 0.048175740987062454 Validation loss 0.05317414924502373 Accuracy 0.45947265625\n",
      "Iteration 59960 Training loss 0.05027981847524643 Validation loss 0.05324156954884529 Accuracy 0.458740234375\n",
      "Iteration 59970 Training loss 0.050407424569129944 Validation loss 0.05324416235089302 Accuracy 0.459228515625\n",
      "Iteration 59980 Training loss 0.04835350438952446 Validation loss 0.05305518954992294 Accuracy 0.460205078125\n",
      "Iteration 59990 Training loss 0.04859668016433716 Validation loss 0.05290191248059273 Accuracy 0.460693359375\n",
      "Iteration 60000 Training loss 0.052235301584005356 Validation loss 0.05303744599223137 Accuracy 0.460693359375\n",
      "Iteration 60010 Training loss 0.05088824778795242 Validation loss 0.05320039764046669 Accuracy 0.460693359375\n",
      "Iteration 60020 Training loss 0.04848098009824753 Validation loss 0.053365208208560944 Accuracy 0.4580078125\n",
      "Iteration 60030 Training loss 0.05205640569329262 Validation loss 0.05314193293452263 Accuracy 0.459716796875\n",
      "Iteration 60040 Training loss 0.04964737966656685 Validation loss 0.05307679623365402 Accuracy 0.4609375\n",
      "Iteration 60050 Training loss 0.05235452204942703 Validation loss 0.05316317081451416 Accuracy 0.460205078125\n",
      "Iteration 60060 Training loss 0.05455758050084114 Validation loss 0.05316389352083206 Accuracy 0.4580078125\n",
      "Iteration 60070 Training loss 0.04766848683357239 Validation loss 0.05326914042234421 Accuracy 0.458740234375\n",
      "Iteration 60080 Training loss 0.053660888224840164 Validation loss 0.05388295277953148 Accuracy 0.4521484375\n",
      "Iteration 60090 Training loss 0.05230974778532982 Validation loss 0.05306319147348404 Accuracy 0.461181640625\n",
      "Iteration 60100 Training loss 0.05111578479409218 Validation loss 0.05359824746847153 Accuracy 0.45654296875\n",
      "Iteration 60110 Training loss 0.05050583928823471 Validation loss 0.053199294954538345 Accuracy 0.45947265625\n",
      "Iteration 60120 Training loss 0.046826910227537155 Validation loss 0.05305764451622963 Accuracy 0.4609375\n",
      "Iteration 60130 Training loss 0.04964939132332802 Validation loss 0.054054953157901764 Accuracy 0.451416015625\n",
      "Iteration 60140 Training loss 0.050277043133974075 Validation loss 0.05369675159454346 Accuracy 0.455078125\n",
      "Iteration 60150 Training loss 0.0511137880384922 Validation loss 0.0532773993909359 Accuracy 0.459228515625\n",
      "Iteration 60160 Training loss 0.04947308823466301 Validation loss 0.053077567368745804 Accuracy 0.461181640625\n",
      "Iteration 60170 Training loss 0.05115549638867378 Validation loss 0.05347387120127678 Accuracy 0.45849609375\n",
      "Iteration 60180 Training loss 0.05399757996201515 Validation loss 0.05345616117119789 Accuracy 0.45849609375\n",
      "Iteration 60190 Training loss 0.051992643624544144 Validation loss 0.05333077162504196 Accuracy 0.460693359375\n",
      "Iteration 60200 Training loss 0.05052904039621353 Validation loss 0.05355400592088699 Accuracy 0.45703125\n",
      "Iteration 60210 Training loss 0.05108672007918358 Validation loss 0.05340900644659996 Accuracy 0.45849609375\n",
      "Iteration 60220 Training loss 0.05297854542732239 Validation loss 0.053029123693704605 Accuracy 0.46142578125\n",
      "Iteration 60230 Training loss 0.04825901240110397 Validation loss 0.053400926291942596 Accuracy 0.458251953125\n",
      "Iteration 60240 Training loss 0.0479462668299675 Validation loss 0.053073685616254807 Accuracy 0.460693359375\n",
      "Iteration 60250 Training loss 0.05216651409864426 Validation loss 0.053216032683849335 Accuracy 0.45947265625\n",
      "Iteration 60260 Training loss 0.05186013877391815 Validation loss 0.05315818265080452 Accuracy 0.460205078125\n",
      "Iteration 60270 Training loss 0.05016990005970001 Validation loss 0.053268611431121826 Accuracy 0.458740234375\n",
      "Iteration 60280 Training loss 0.05194266885519028 Validation loss 0.05323413014411926 Accuracy 0.459228515625\n",
      "Iteration 60290 Training loss 0.053102485835552216 Validation loss 0.05299275368452072 Accuracy 0.46044921875\n",
      "Iteration 60300 Training loss 0.052939463406801224 Validation loss 0.053067851811647415 Accuracy 0.460205078125\n",
      "Iteration 60310 Training loss 0.05125412344932556 Validation loss 0.053164560347795486 Accuracy 0.4599609375\n",
      "Iteration 60320 Training loss 0.05273982137441635 Validation loss 0.053311366587877274 Accuracy 0.459228515625\n",
      "Iteration 60330 Training loss 0.051796987652778625 Validation loss 0.05313354358077049 Accuracy 0.460693359375\n",
      "Iteration 60340 Training loss 0.05193071439862251 Validation loss 0.053221579641103745 Accuracy 0.459716796875\n",
      "Iteration 60350 Training loss 0.050921909511089325 Validation loss 0.0531584657728672 Accuracy 0.45947265625\n",
      "Iteration 60360 Training loss 0.05047024041414261 Validation loss 0.05312493070960045 Accuracy 0.461181640625\n",
      "Iteration 60370 Training loss 0.04746793955564499 Validation loss 0.05296950042247772 Accuracy 0.4609375\n",
      "Iteration 60380 Training loss 0.049691569060087204 Validation loss 0.053006235510110855 Accuracy 0.461181640625\n",
      "Iteration 60390 Training loss 0.05221662297844887 Validation loss 0.05307267606258392 Accuracy 0.461181640625\n",
      "Iteration 60400 Training loss 0.05530911311507225 Validation loss 0.05312880873680115 Accuracy 0.46142578125\n",
      "Iteration 60410 Training loss 0.052374545484781265 Validation loss 0.05398701876401901 Accuracy 0.45361328125\n",
      "Iteration 60420 Training loss 0.04744173213839531 Validation loss 0.053274717181921005 Accuracy 0.458984375\n",
      "Iteration 60430 Training loss 0.05186961963772774 Validation loss 0.05355014652013779 Accuracy 0.45751953125\n",
      "Iteration 60440 Training loss 0.05459639057517052 Validation loss 0.05298322066664696 Accuracy 0.4609375\n",
      "Iteration 60450 Training loss 0.05124066025018692 Validation loss 0.0532013364136219 Accuracy 0.4599609375\n",
      "Iteration 60460 Training loss 0.05093327909708023 Validation loss 0.05288319289684296 Accuracy 0.4619140625\n",
      "Iteration 60470 Training loss 0.051024749875068665 Validation loss 0.05400516837835312 Accuracy 0.4521484375\n",
      "Iteration 60480 Training loss 0.055708616971969604 Validation loss 0.05433599650859833 Accuracy 0.4482421875\n",
      "Iteration 60490 Training loss 0.048557911068201065 Validation loss 0.0529995895922184 Accuracy 0.46142578125\n",
      "Iteration 60500 Training loss 0.05244367942214012 Validation loss 0.05298199877142906 Accuracy 0.46142578125\n",
      "Iteration 60510 Training loss 0.05192386731505394 Validation loss 0.05303218588232994 Accuracy 0.460205078125\n",
      "Iteration 60520 Training loss 0.04875917360186577 Validation loss 0.05296727269887924 Accuracy 0.460205078125\n",
      "Iteration 60530 Training loss 0.05311689153313637 Validation loss 0.05304412916302681 Accuracy 0.460693359375\n",
      "Iteration 60540 Training loss 0.05322186276316643 Validation loss 0.05304533988237381 Accuracy 0.4609375\n",
      "Iteration 60550 Training loss 0.050071630626916885 Validation loss 0.05330175533890724 Accuracy 0.458984375\n",
      "Iteration 60560 Training loss 0.049563802778720856 Validation loss 0.05332960933446884 Accuracy 0.458251953125\n",
      "Iteration 60570 Training loss 0.05216722935438156 Validation loss 0.053214676678180695 Accuracy 0.459716796875\n",
      "Iteration 60580 Training loss 0.049844104796648026 Validation loss 0.05297870561480522 Accuracy 0.461181640625\n",
      "Iteration 60590 Training loss 0.05026308074593544 Validation loss 0.05310320481657982 Accuracy 0.460205078125\n",
      "Iteration 60600 Training loss 0.04854096472263336 Validation loss 0.05319811403751373 Accuracy 0.458984375\n",
      "Iteration 60610 Training loss 0.04838941991329193 Validation loss 0.05305241048336029 Accuracy 0.460205078125\n",
      "Iteration 60620 Training loss 0.053923964500427246 Validation loss 0.05317851901054382 Accuracy 0.460693359375\n",
      "Iteration 60630 Training loss 0.0522889606654644 Validation loss 0.05333580821752548 Accuracy 0.459228515625\n",
      "Iteration 60640 Training loss 0.05004832521080971 Validation loss 0.05383206903934479 Accuracy 0.455810546875\n",
      "Iteration 60650 Training loss 0.050441864877939224 Validation loss 0.0531175471842289 Accuracy 0.461181640625\n",
      "Iteration 60660 Training loss 0.051303379237651825 Validation loss 0.05317123234272003 Accuracy 0.458984375\n",
      "Iteration 60670 Training loss 0.051962655037641525 Validation loss 0.05376925691962242 Accuracy 0.45458984375\n",
      "Iteration 60680 Training loss 0.05031202733516693 Validation loss 0.05322211608290672 Accuracy 0.459716796875\n",
      "Iteration 60690 Training loss 0.052229706197977066 Validation loss 0.05318798869848251 Accuracy 0.460205078125\n",
      "Iteration 60700 Training loss 0.05071960762143135 Validation loss 0.053147681057453156 Accuracy 0.4609375\n",
      "Iteration 60710 Training loss 0.05151937156915665 Validation loss 0.0531771145761013 Accuracy 0.460693359375\n",
      "Iteration 60720 Training loss 0.051033541560173035 Validation loss 0.05286416411399841 Accuracy 0.461181640625\n",
      "Iteration 60730 Training loss 0.04924570769071579 Validation loss 0.053472332656383514 Accuracy 0.45849609375\n",
      "Iteration 60740 Training loss 0.05075399577617645 Validation loss 0.05362275242805481 Accuracy 0.457763671875\n",
      "Iteration 60750 Training loss 0.0511515811085701 Validation loss 0.05284906178712845 Accuracy 0.4609375\n",
      "Iteration 60760 Training loss 0.05424729362130165 Validation loss 0.05309038981795311 Accuracy 0.461181640625\n",
      "Iteration 60770 Training loss 0.04782288894057274 Validation loss 0.05336041375994682 Accuracy 0.45947265625\n",
      "Iteration 60780 Training loss 0.05243990942835808 Validation loss 0.05337812379002571 Accuracy 0.45849609375\n",
      "Iteration 60790 Training loss 0.052877187728881836 Validation loss 0.053354743868112564 Accuracy 0.459716796875\n",
      "Iteration 60800 Training loss 0.05485687032341957 Validation loss 0.05312370881438255 Accuracy 0.460205078125\n",
      "Iteration 60810 Training loss 0.05457412824034691 Validation loss 0.05368266999721527 Accuracy 0.454833984375\n",
      "Iteration 60820 Training loss 0.05333330109715462 Validation loss 0.053445905447006226 Accuracy 0.455810546875\n",
      "Iteration 60830 Training loss 0.047585379332304 Validation loss 0.05294050648808479 Accuracy 0.461181640625\n",
      "Iteration 60840 Training loss 0.0521230474114418 Validation loss 0.05306116119027138 Accuracy 0.45947265625\n",
      "Iteration 60850 Training loss 0.05404314026236534 Validation loss 0.053516872227191925 Accuracy 0.4580078125\n",
      "Iteration 60860 Training loss 0.053093139082193375 Validation loss 0.05343194305896759 Accuracy 0.458740234375\n",
      "Iteration 60870 Training loss 0.05109049752354622 Validation loss 0.05297297239303589 Accuracy 0.461181640625\n",
      "Iteration 60880 Training loss 0.05118570849299431 Validation loss 0.053297072649002075 Accuracy 0.460693359375\n",
      "Iteration 60890 Training loss 0.050411295145750046 Validation loss 0.05297486111521721 Accuracy 0.461669921875\n",
      "Iteration 60900 Training loss 0.04954080283641815 Validation loss 0.053752899169921875 Accuracy 0.455810546875\n",
      "Iteration 60910 Training loss 0.05027506873011589 Validation loss 0.05327475443482399 Accuracy 0.459228515625\n",
      "Iteration 60920 Training loss 0.053395070135593414 Validation loss 0.05345427244901657 Accuracy 0.457275390625\n",
      "Iteration 60930 Training loss 0.048649199306964874 Validation loss 0.053431667387485504 Accuracy 0.4580078125\n",
      "Iteration 60940 Training loss 0.05270608514547348 Validation loss 0.05315499007701874 Accuracy 0.459228515625\n",
      "Iteration 60950 Training loss 0.05444544926285744 Validation loss 0.05315352976322174 Accuracy 0.4599609375\n",
      "Iteration 60960 Training loss 0.050292376428842545 Validation loss 0.053448889404535294 Accuracy 0.45849609375\n",
      "Iteration 60970 Training loss 0.046742990612983704 Validation loss 0.05346488952636719 Accuracy 0.4580078125\n",
      "Iteration 60980 Training loss 0.05143895372748375 Validation loss 0.05321824923157692 Accuracy 0.458984375\n",
      "Iteration 60990 Training loss 0.05110475793480873 Validation loss 0.05313783511519432 Accuracy 0.4609375\n",
      "Iteration 61000 Training loss 0.05249445512890816 Validation loss 0.0532664991915226 Accuracy 0.45947265625\n",
      "Iteration 61010 Training loss 0.04851880669593811 Validation loss 0.053269557654857635 Accuracy 0.46044921875\n",
      "Iteration 61020 Training loss 0.0472855381667614 Validation loss 0.05323003605008125 Accuracy 0.459716796875\n",
      "Iteration 61030 Training loss 0.05112074315547943 Validation loss 0.05282670632004738 Accuracy 0.462158203125\n",
      "Iteration 61040 Training loss 0.05164606124162674 Validation loss 0.05328020453453064 Accuracy 0.45849609375\n",
      "Iteration 61050 Training loss 0.052424270659685135 Validation loss 0.05327654257416725 Accuracy 0.458984375\n",
      "Iteration 61060 Training loss 0.0509151853621006 Validation loss 0.05308457836508751 Accuracy 0.461181640625\n",
      "Iteration 61070 Training loss 0.052628256380558014 Validation loss 0.05332571640610695 Accuracy 0.458984375\n",
      "Iteration 61080 Training loss 0.05180362984538078 Validation loss 0.05326142534613609 Accuracy 0.46044921875\n",
      "Iteration 61090 Training loss 0.04919148609042168 Validation loss 0.0532488115131855 Accuracy 0.459716796875\n",
      "Iteration 61100 Training loss 0.051231760531663895 Validation loss 0.05305939167737961 Accuracy 0.460205078125\n",
      "Iteration 61110 Training loss 0.052558258175849915 Validation loss 0.0531143918633461 Accuracy 0.46044921875\n",
      "Iteration 61120 Training loss 0.05224618315696716 Validation loss 0.05309125781059265 Accuracy 0.459716796875\n",
      "Iteration 61130 Training loss 0.05300033465027809 Validation loss 0.05335098132491112 Accuracy 0.458740234375\n",
      "Iteration 61140 Training loss 0.054967913776636124 Validation loss 0.053146205842494965 Accuracy 0.45947265625\n",
      "Iteration 61150 Training loss 0.050818149000406265 Validation loss 0.05381947010755539 Accuracy 0.456787109375\n",
      "Iteration 61160 Training loss 0.05305706337094307 Validation loss 0.05304081737995148 Accuracy 0.461181640625\n",
      "Iteration 61170 Training loss 0.051022958010435104 Validation loss 0.05307800695300102 Accuracy 0.461181640625\n",
      "Iteration 61180 Training loss 0.047414712607860565 Validation loss 0.053062666207551956 Accuracy 0.462158203125\n",
      "Iteration 61190 Training loss 0.05349578335881233 Validation loss 0.05316084250807762 Accuracy 0.461669921875\n",
      "Iteration 61200 Training loss 0.04898075386881828 Validation loss 0.0529228076338768 Accuracy 0.46240234375\n",
      "Iteration 61210 Training loss 0.0513061098754406 Validation loss 0.05304195359349251 Accuracy 0.461669921875\n",
      "Iteration 61220 Training loss 0.053046051412820816 Validation loss 0.05322558060288429 Accuracy 0.45849609375\n",
      "Iteration 61230 Training loss 0.053790364414453506 Validation loss 0.053305014967918396 Accuracy 0.460205078125\n",
      "Iteration 61240 Training loss 0.05258876085281372 Validation loss 0.05325450375676155 Accuracy 0.459228515625\n",
      "Iteration 61250 Training loss 0.051924265921115875 Validation loss 0.053066760301589966 Accuracy 0.460693359375\n",
      "Iteration 61260 Training loss 0.050313614308834076 Validation loss 0.053251344710588455 Accuracy 0.460205078125\n",
      "Iteration 61270 Training loss 0.05060046166181564 Validation loss 0.05325925722718239 Accuracy 0.4609375\n",
      "Iteration 61280 Training loss 0.05505109578371048 Validation loss 0.05312969163060188 Accuracy 0.461181640625\n",
      "Iteration 61290 Training loss 0.05050103738903999 Validation loss 0.05352817103266716 Accuracy 0.457275390625\n",
      "Iteration 61300 Training loss 0.0491250604391098 Validation loss 0.052962224930524826 Accuracy 0.46240234375\n",
      "Iteration 61310 Training loss 0.050831206142902374 Validation loss 0.053014274686574936 Accuracy 0.4619140625\n",
      "Iteration 61320 Training loss 0.05149298533797264 Validation loss 0.05335765331983566 Accuracy 0.46044921875\n",
      "Iteration 61330 Training loss 0.0503639280796051 Validation loss 0.05329263210296631 Accuracy 0.458984375\n",
      "Iteration 61340 Training loss 0.049239784479141235 Validation loss 0.05310492217540741 Accuracy 0.45947265625\n",
      "Iteration 61350 Training loss 0.051140304654836655 Validation loss 0.052874207496643066 Accuracy 0.46240234375\n",
      "Iteration 61360 Training loss 0.054233286529779434 Validation loss 0.05314416065812111 Accuracy 0.459716796875\n",
      "Iteration 61370 Training loss 0.04977532848715782 Validation loss 0.05311570316553116 Accuracy 0.46044921875\n",
      "Iteration 61380 Training loss 0.05465506389737129 Validation loss 0.05300386622548103 Accuracy 0.461181640625\n",
      "Iteration 61390 Training loss 0.04974322393536568 Validation loss 0.05297455936670303 Accuracy 0.461181640625\n",
      "Iteration 61400 Training loss 0.05060561001300812 Validation loss 0.053272370249032974 Accuracy 0.45947265625\n",
      "Iteration 61410 Training loss 0.05152248963713646 Validation loss 0.05326315388083458 Accuracy 0.458984375\n",
      "Iteration 61420 Training loss 0.04944435507059097 Validation loss 0.05297846719622612 Accuracy 0.46044921875\n",
      "Iteration 61430 Training loss 0.052352726459503174 Validation loss 0.053366150707006454 Accuracy 0.45703125\n",
      "Iteration 61440 Training loss 0.054578155279159546 Validation loss 0.053101230412721634 Accuracy 0.460205078125\n",
      "Iteration 61450 Training loss 0.05242986977100372 Validation loss 0.05322466045618057 Accuracy 0.45849609375\n",
      "Iteration 61460 Training loss 0.051811572164297104 Validation loss 0.05338258668780327 Accuracy 0.458251953125\n",
      "Iteration 61470 Training loss 0.05156691372394562 Validation loss 0.05317891389131546 Accuracy 0.45849609375\n",
      "Iteration 61480 Training loss 0.05067254602909088 Validation loss 0.05316441133618355 Accuracy 0.45947265625\n",
      "Iteration 61490 Training loss 0.050565946847200394 Validation loss 0.05295378714799881 Accuracy 0.461181640625\n",
      "Iteration 61500 Training loss 0.050898004323244095 Validation loss 0.05306321009993553 Accuracy 0.460205078125\n",
      "Iteration 61510 Training loss 0.04846293106675148 Validation loss 0.05347582697868347 Accuracy 0.458251953125\n",
      "Iteration 61520 Training loss 0.05128113180398941 Validation loss 0.053241439163684845 Accuracy 0.45947265625\n",
      "Iteration 61530 Training loss 0.0512361042201519 Validation loss 0.053142860531806946 Accuracy 0.461669921875\n",
      "Iteration 61540 Training loss 0.050090789794921875 Validation loss 0.05313148722052574 Accuracy 0.4599609375\n",
      "Iteration 61550 Training loss 0.051651835441589355 Validation loss 0.05290134251117706 Accuracy 0.461669921875\n",
      "Iteration 61560 Training loss 0.05109761655330658 Validation loss 0.053764745593070984 Accuracy 0.4541015625\n",
      "Iteration 61570 Training loss 0.05037997290492058 Validation loss 0.05347318574786186 Accuracy 0.458740234375\n",
      "Iteration 61580 Training loss 0.053605061024427414 Validation loss 0.05293984338641167 Accuracy 0.46044921875\n",
      "Iteration 61590 Training loss 0.04857442155480385 Validation loss 0.05312558636069298 Accuracy 0.460693359375\n",
      "Iteration 61600 Training loss 0.05393585190176964 Validation loss 0.05350663885474205 Accuracy 0.45751953125\n",
      "Iteration 61610 Training loss 0.04958520457148552 Validation loss 0.05292394012212753 Accuracy 0.46142578125\n",
      "Iteration 61620 Training loss 0.051702823489904404 Validation loss 0.05313142389059067 Accuracy 0.4599609375\n",
      "Iteration 61630 Training loss 0.050467781722545624 Validation loss 0.052843865007162094 Accuracy 0.4619140625\n",
      "Iteration 61640 Training loss 0.052787646651268005 Validation loss 0.05310501903295517 Accuracy 0.460693359375\n",
      "Iteration 61650 Training loss 0.050159454345703125 Validation loss 0.05336898937821388 Accuracy 0.458984375\n",
      "Iteration 61660 Training loss 0.05035446211695671 Validation loss 0.053041789680719376 Accuracy 0.461181640625\n",
      "Iteration 61670 Training loss 0.053903233259916306 Validation loss 0.05330280587077141 Accuracy 0.46044921875\n",
      "Iteration 61680 Training loss 0.05548983812332153 Validation loss 0.054407693445682526 Accuracy 0.4482421875\n",
      "Iteration 61690 Training loss 0.053385186940431595 Validation loss 0.052926693111658096 Accuracy 0.4619140625\n",
      "Iteration 61700 Training loss 0.053928785026073456 Validation loss 0.053399279713630676 Accuracy 0.458984375\n",
      "Iteration 61710 Training loss 0.050368405878543854 Validation loss 0.05299101024866104 Accuracy 0.4619140625\n",
      "Iteration 61720 Training loss 0.048836614936590195 Validation loss 0.05303911864757538 Accuracy 0.460205078125\n",
      "Iteration 61730 Training loss 0.049563124775886536 Validation loss 0.05355111509561539 Accuracy 0.457763671875\n",
      "Iteration 61740 Training loss 0.04902622476220131 Validation loss 0.053539764136075974 Accuracy 0.456787109375\n",
      "Iteration 61750 Training loss 0.05418119207024574 Validation loss 0.05298183485865593 Accuracy 0.46142578125\n",
      "Iteration 61760 Training loss 0.05757541209459305 Validation loss 0.05297350510954857 Accuracy 0.461181640625\n",
      "Iteration 61770 Training loss 0.04958534240722656 Validation loss 0.05374060571193695 Accuracy 0.456298828125\n",
      "Iteration 61780 Training loss 0.05278899148106575 Validation loss 0.05324755236506462 Accuracy 0.45849609375\n",
      "Iteration 61790 Training loss 0.05181164667010307 Validation loss 0.05346217751502991 Accuracy 0.45654296875\n",
      "Iteration 61800 Training loss 0.05120013281702995 Validation loss 0.053292274475097656 Accuracy 0.458740234375\n",
      "Iteration 61810 Training loss 0.047531742602586746 Validation loss 0.05324932932853699 Accuracy 0.4599609375\n",
      "Iteration 61820 Training loss 0.049946852028369904 Validation loss 0.053085580468177795 Accuracy 0.461181640625\n",
      "Iteration 61830 Training loss 0.048142191022634506 Validation loss 0.05349103361368179 Accuracy 0.457275390625\n",
      "Iteration 61840 Training loss 0.05188402906060219 Validation loss 0.053540438413619995 Accuracy 0.45849609375\n",
      "Iteration 61850 Training loss 0.04709894210100174 Validation loss 0.05312449112534523 Accuracy 0.461669921875\n",
      "Iteration 61860 Training loss 0.052552927285432816 Validation loss 0.053159065544605255 Accuracy 0.4609375\n",
      "Iteration 61870 Training loss 0.050587646663188934 Validation loss 0.05351521447300911 Accuracy 0.458251953125\n",
      "Iteration 61880 Training loss 0.050251927226781845 Validation loss 0.05316866934299469 Accuracy 0.460205078125\n",
      "Iteration 61890 Training loss 0.049413491040468216 Validation loss 0.0534021258354187 Accuracy 0.458251953125\n",
      "Iteration 61900 Training loss 0.04892390966415405 Validation loss 0.05314254388213158 Accuracy 0.460693359375\n",
      "Iteration 61910 Training loss 0.050499510020017624 Validation loss 0.053001608699560165 Accuracy 0.460693359375\n",
      "Iteration 61920 Training loss 0.0510978177189827 Validation loss 0.05308496952056885 Accuracy 0.460693359375\n",
      "Iteration 61930 Training loss 0.048528045415878296 Validation loss 0.05327562242746353 Accuracy 0.45947265625\n",
      "Iteration 61940 Training loss 0.04897662252187729 Validation loss 0.05279027298092842 Accuracy 0.4619140625\n",
      "Iteration 61950 Training loss 0.0510367825627327 Validation loss 0.05345403775572777 Accuracy 0.458251953125\n",
      "Iteration 61960 Training loss 0.05534280464053154 Validation loss 0.054098132997751236 Accuracy 0.45263671875\n",
      "Iteration 61970 Training loss 0.04890414699912071 Validation loss 0.053075917065143585 Accuracy 0.460693359375\n",
      "Iteration 61980 Training loss 0.05677105486392975 Validation loss 0.05304279178380966 Accuracy 0.461181640625\n",
      "Iteration 61990 Training loss 0.04991395026445389 Validation loss 0.05330384522676468 Accuracy 0.4599609375\n",
      "Iteration 62000 Training loss 0.050908178091049194 Validation loss 0.05327514559030533 Accuracy 0.458984375\n",
      "Iteration 62010 Training loss 0.051112283021211624 Validation loss 0.0530582033097744 Accuracy 0.4599609375\n",
      "Iteration 62020 Training loss 0.052114684134721756 Validation loss 0.053206268697977066 Accuracy 0.458984375\n",
      "Iteration 62030 Training loss 0.05041785165667534 Validation loss 0.05296846479177475 Accuracy 0.46142578125\n",
      "Iteration 62040 Training loss 0.052590616047382355 Validation loss 0.052952900528907776 Accuracy 0.46240234375\n",
      "Iteration 62050 Training loss 0.05146420747041702 Validation loss 0.05333404242992401 Accuracy 0.459716796875\n",
      "Iteration 62060 Training loss 0.04945268854498863 Validation loss 0.05311138555407524 Accuracy 0.461181640625\n",
      "Iteration 62070 Training loss 0.04911140725016594 Validation loss 0.05353819206357002 Accuracy 0.4580078125\n",
      "Iteration 62080 Training loss 0.05037164315581322 Validation loss 0.053073231130838394 Accuracy 0.4619140625\n",
      "Iteration 62090 Training loss 0.0485934242606163 Validation loss 0.05349094420671463 Accuracy 0.458984375\n",
      "Iteration 62100 Training loss 0.051045436412096024 Validation loss 0.053147297352552414 Accuracy 0.460205078125\n",
      "Iteration 62110 Training loss 0.0496501550078392 Validation loss 0.05310872942209244 Accuracy 0.461181640625\n",
      "Iteration 62120 Training loss 0.05418222397565842 Validation loss 0.05359150096774101 Accuracy 0.4560546875\n",
      "Iteration 62130 Training loss 0.050210632383823395 Validation loss 0.0531294122338295 Accuracy 0.459716796875\n",
      "Iteration 62140 Training loss 0.05054417997598648 Validation loss 0.053307849913835526 Accuracy 0.457763671875\n",
      "Iteration 62150 Training loss 0.05473790317773819 Validation loss 0.05296698212623596 Accuracy 0.461181640625\n",
      "Iteration 62160 Training loss 0.04989296570420265 Validation loss 0.05340278148651123 Accuracy 0.458740234375\n",
      "Iteration 62170 Training loss 0.056369565427303314 Validation loss 0.05322114750742912 Accuracy 0.460205078125\n",
      "Iteration 62180 Training loss 0.0528397299349308 Validation loss 0.053210336714982986 Accuracy 0.4599609375\n",
      "Iteration 62190 Training loss 0.05015102028846741 Validation loss 0.05343601480126381 Accuracy 0.4580078125\n",
      "Iteration 62200 Training loss 0.05113058537244797 Validation loss 0.053128935396671295 Accuracy 0.460205078125\n",
      "Iteration 62210 Training loss 0.05120990797877312 Validation loss 0.05294451117515564 Accuracy 0.462158203125\n",
      "Iteration 62220 Training loss 0.050957418978214264 Validation loss 0.05320744961500168 Accuracy 0.459716796875\n",
      "Iteration 62230 Training loss 0.05192697048187256 Validation loss 0.05319264531135559 Accuracy 0.4609375\n",
      "Iteration 62240 Training loss 0.05273093283176422 Validation loss 0.05313747748732567 Accuracy 0.460205078125\n",
      "Iteration 62250 Training loss 0.04927213490009308 Validation loss 0.053425632417201996 Accuracy 0.45751953125\n",
      "Iteration 62260 Training loss 0.05256740748882294 Validation loss 0.053367968648672104 Accuracy 0.45654296875\n",
      "Iteration 62270 Training loss 0.053091514855623245 Validation loss 0.05333343520760536 Accuracy 0.45947265625\n",
      "Iteration 62280 Training loss 0.047011494636535645 Validation loss 0.053289562463760376 Accuracy 0.458984375\n",
      "Iteration 62290 Training loss 0.051128797233104706 Validation loss 0.053214553743600845 Accuracy 0.460693359375\n",
      "Iteration 62300 Training loss 0.04680812358856201 Validation loss 0.053021181374788284 Accuracy 0.461669921875\n",
      "Iteration 62310 Training loss 0.04929336905479431 Validation loss 0.05324948951601982 Accuracy 0.460205078125\n",
      "Iteration 62320 Training loss 0.05245028808712959 Validation loss 0.053173474967479706 Accuracy 0.46044921875\n",
      "Iteration 62330 Training loss 0.05148300155997276 Validation loss 0.052799541503190994 Accuracy 0.462158203125\n",
      "Iteration 62340 Training loss 0.05369296297430992 Validation loss 0.05298696085810661 Accuracy 0.460693359375\n",
      "Iteration 62350 Training loss 0.05272960290312767 Validation loss 0.05299869179725647 Accuracy 0.460205078125\n",
      "Iteration 62360 Training loss 0.049522705376148224 Validation loss 0.05303273722529411 Accuracy 0.460205078125\n",
      "Iteration 62370 Training loss 0.04736259952187538 Validation loss 0.05295662581920624 Accuracy 0.4609375\n",
      "Iteration 62380 Training loss 0.0517604686319828 Validation loss 0.05332246422767639 Accuracy 0.458251953125\n",
      "Iteration 62390 Training loss 0.0522935688495636 Validation loss 0.05341304466128349 Accuracy 0.45751953125\n",
      "Iteration 62400 Training loss 0.04973010718822479 Validation loss 0.05341145023703575 Accuracy 0.458251953125\n",
      "Iteration 62410 Training loss 0.05481322109699249 Validation loss 0.05311570316553116 Accuracy 0.46142578125\n",
      "Iteration 62420 Training loss 0.05318004637956619 Validation loss 0.05298992991447449 Accuracy 0.4619140625\n",
      "Iteration 62430 Training loss 0.050724223256111145 Validation loss 0.05292629823088646 Accuracy 0.46142578125\n",
      "Iteration 62440 Training loss 0.05053950101137161 Validation loss 0.05312688276171684 Accuracy 0.46044921875\n",
      "Iteration 62450 Training loss 0.05412664636969566 Validation loss 0.0536157600581646 Accuracy 0.45703125\n",
      "Iteration 62460 Training loss 0.053490202873945236 Validation loss 0.05297334864735603 Accuracy 0.46044921875\n",
      "Iteration 62470 Training loss 0.050178445875644684 Validation loss 0.05318311229348183 Accuracy 0.462158203125\n",
      "Iteration 62480 Training loss 0.05288507044315338 Validation loss 0.05326830968260765 Accuracy 0.458984375\n",
      "Iteration 62490 Training loss 0.05232514813542366 Validation loss 0.0535491406917572 Accuracy 0.457275390625\n",
      "Iteration 62500 Training loss 0.05220929905772209 Validation loss 0.053092628717422485 Accuracy 0.460205078125\n",
      "Iteration 62510 Training loss 0.04980180412530899 Validation loss 0.053454529494047165 Accuracy 0.45849609375\n",
      "Iteration 62520 Training loss 0.04753376170992851 Validation loss 0.05304347351193428 Accuracy 0.460693359375\n",
      "Iteration 62530 Training loss 0.04697059094905853 Validation loss 0.053270019590854645 Accuracy 0.4599609375\n",
      "Iteration 62540 Training loss 0.0465417355298996 Validation loss 0.052978742867708206 Accuracy 0.461669921875\n",
      "Iteration 62550 Training loss 0.04946522414684296 Validation loss 0.05293569341301918 Accuracy 0.461669921875\n",
      "Iteration 62560 Training loss 0.049435995519161224 Validation loss 0.05309073254466057 Accuracy 0.461181640625\n",
      "Iteration 62570 Training loss 0.04704081639647484 Validation loss 0.05300605669617653 Accuracy 0.461181640625\n",
      "Iteration 62580 Training loss 0.048784442245960236 Validation loss 0.052844393998384476 Accuracy 0.46142578125\n",
      "Iteration 62590 Training loss 0.05035918205976486 Validation loss 0.05296429619193077 Accuracy 0.46044921875\n",
      "Iteration 62600 Training loss 0.05069063603878021 Validation loss 0.053375244140625 Accuracy 0.45703125\n",
      "Iteration 62610 Training loss 0.05120379477739334 Validation loss 0.05324997752904892 Accuracy 0.459228515625\n",
      "Iteration 62620 Training loss 0.05187813192605972 Validation loss 0.0529370941221714 Accuracy 0.461669921875\n",
      "Iteration 62630 Training loss 0.049181029200553894 Validation loss 0.053197722882032394 Accuracy 0.45947265625\n",
      "Iteration 62640 Training loss 0.05101266875863075 Validation loss 0.053116556257009506 Accuracy 0.4599609375\n",
      "Iteration 62650 Training loss 0.05390581116080284 Validation loss 0.05322563275694847 Accuracy 0.460205078125\n",
      "Iteration 62660 Training loss 0.05281398072838783 Validation loss 0.05322844535112381 Accuracy 0.45849609375\n",
      "Iteration 62670 Training loss 0.050402525812387466 Validation loss 0.05319454148411751 Accuracy 0.4609375\n",
      "Iteration 62680 Training loss 0.04887009039521217 Validation loss 0.05328779295086861 Accuracy 0.459716796875\n",
      "Iteration 62690 Training loss 0.050897642970085144 Validation loss 0.05326905474066734 Accuracy 0.460205078125\n",
      "Iteration 62700 Training loss 0.04898449778556824 Validation loss 0.05334952846169472 Accuracy 0.4580078125\n",
      "Iteration 62710 Training loss 0.04941071197390556 Validation loss 0.053143709897994995 Accuracy 0.460205078125\n",
      "Iteration 62720 Training loss 0.049697987735271454 Validation loss 0.05332572013139725 Accuracy 0.458251953125\n",
      "Iteration 62730 Training loss 0.05454188585281372 Validation loss 0.05373683571815491 Accuracy 0.455810546875\n",
      "Iteration 62740 Training loss 0.05142980068922043 Validation loss 0.05342783033847809 Accuracy 0.458740234375\n",
      "Iteration 62750 Training loss 0.05309749394655228 Validation loss 0.05319296568632126 Accuracy 0.4599609375\n",
      "Iteration 62760 Training loss 0.05346905440092087 Validation loss 0.05325479805469513 Accuracy 0.460205078125\n",
      "Iteration 62770 Training loss 0.053497858345508575 Validation loss 0.05300768464803696 Accuracy 0.461181640625\n",
      "Iteration 62780 Training loss 0.051920756697654724 Validation loss 0.05319251865148544 Accuracy 0.458984375\n",
      "Iteration 62790 Training loss 0.05097091943025589 Validation loss 0.053308285772800446 Accuracy 0.459228515625\n",
      "Iteration 62800 Training loss 0.05224822089076042 Validation loss 0.053449198603630066 Accuracy 0.458984375\n",
      "Iteration 62810 Training loss 0.05201242119073868 Validation loss 0.05356224626302719 Accuracy 0.45849609375\n",
      "Iteration 62820 Training loss 0.05159936472773552 Validation loss 0.053380563855171204 Accuracy 0.457275390625\n",
      "Iteration 62830 Training loss 0.050772786140441895 Validation loss 0.05288378894329071 Accuracy 0.460693359375\n",
      "Iteration 62840 Training loss 0.04815535247325897 Validation loss 0.05297118052840233 Accuracy 0.461669921875\n",
      "Iteration 62850 Training loss 0.04993017017841339 Validation loss 0.05301714316010475 Accuracy 0.4609375\n",
      "Iteration 62860 Training loss 0.05128920078277588 Validation loss 0.05299241095781326 Accuracy 0.4619140625\n",
      "Iteration 62870 Training loss 0.05297751724720001 Validation loss 0.05288268253207207 Accuracy 0.461181640625\n",
      "Iteration 62880 Training loss 0.05289129540324211 Validation loss 0.0532446950674057 Accuracy 0.46044921875\n",
      "Iteration 62890 Training loss 0.050583112984895706 Validation loss 0.05310435593128204 Accuracy 0.460693359375\n",
      "Iteration 62900 Training loss 0.052203450351953506 Validation loss 0.053025051951408386 Accuracy 0.461181640625\n",
      "Iteration 62910 Training loss 0.0492299385368824 Validation loss 0.05324558541178703 Accuracy 0.45751953125\n",
      "Iteration 62920 Training loss 0.052103299647569656 Validation loss 0.05315154790878296 Accuracy 0.46044921875\n",
      "Iteration 62930 Training loss 0.05110054463148117 Validation loss 0.05407875403761864 Accuracy 0.45068359375\n",
      "Iteration 62940 Training loss 0.053199056535959244 Validation loss 0.05322025343775749 Accuracy 0.4599609375\n",
      "Iteration 62950 Training loss 0.05269482359290123 Validation loss 0.05315170809626579 Accuracy 0.458984375\n",
      "Iteration 62960 Training loss 0.0506509393453598 Validation loss 0.053248099982738495 Accuracy 0.458251953125\n",
      "Iteration 62970 Training loss 0.05246419087052345 Validation loss 0.05317997932434082 Accuracy 0.459716796875\n",
      "Iteration 62980 Training loss 0.05092545971274376 Validation loss 0.052992794662714005 Accuracy 0.461181640625\n",
      "Iteration 62990 Training loss 0.04945635423064232 Validation loss 0.053760379552841187 Accuracy 0.4560546875\n",
      "Iteration 63000 Training loss 0.049688633531332016 Validation loss 0.05333761125802994 Accuracy 0.45849609375\n",
      "Iteration 63010 Training loss 0.04929310455918312 Validation loss 0.05307003855705261 Accuracy 0.460693359375\n",
      "Iteration 63020 Training loss 0.051359012722969055 Validation loss 0.052890632301568985 Accuracy 0.46142578125\n",
      "Iteration 63030 Training loss 0.05071865767240524 Validation loss 0.05308078974485397 Accuracy 0.461181640625\n",
      "Iteration 63040 Training loss 0.04716920852661133 Validation loss 0.05336456745862961 Accuracy 0.45849609375\n",
      "Iteration 63050 Training loss 0.050249356776475906 Validation loss 0.0536690317094326 Accuracy 0.454345703125\n",
      "Iteration 63060 Training loss 0.046572502702474594 Validation loss 0.05302407220005989 Accuracy 0.461181640625\n",
      "Iteration 63070 Training loss 0.05490521714091301 Validation loss 0.054006293416023254 Accuracy 0.452880859375\n",
      "Iteration 63080 Training loss 0.04875340685248375 Validation loss 0.053150732070207596 Accuracy 0.459716796875\n",
      "Iteration 63090 Training loss 0.050934385508298874 Validation loss 0.05318838357925415 Accuracy 0.45849609375\n",
      "Iteration 63100 Training loss 0.05444372445344925 Validation loss 0.053250838071107864 Accuracy 0.457763671875\n",
      "Iteration 63110 Training loss 0.051523879170417786 Validation loss 0.05295078828930855 Accuracy 0.4599609375\n",
      "Iteration 63120 Training loss 0.05411255732178688 Validation loss 0.053043924272060394 Accuracy 0.460205078125\n",
      "Iteration 63130 Training loss 0.05030963197350502 Validation loss 0.05344853550195694 Accuracy 0.458984375\n",
      "Iteration 63140 Training loss 0.04989159107208252 Validation loss 0.05310247838497162 Accuracy 0.459228515625\n",
      "Iteration 63150 Training loss 0.05184075981378555 Validation loss 0.05314769223332405 Accuracy 0.459716796875\n",
      "Iteration 63160 Training loss 0.050577037036418915 Validation loss 0.053098924458026886 Accuracy 0.46044921875\n",
      "Iteration 63170 Training loss 0.051641784608364105 Validation loss 0.053324174135923386 Accuracy 0.45947265625\n",
      "Iteration 63180 Training loss 0.04905325174331665 Validation loss 0.053056493401527405 Accuracy 0.460693359375\n",
      "Iteration 63190 Training loss 0.05231257900595665 Validation loss 0.05336819589138031 Accuracy 0.45947265625\n",
      "Iteration 63200 Training loss 0.05280869081616402 Validation loss 0.053124334663152695 Accuracy 0.460205078125\n",
      "Iteration 63210 Training loss 0.04759366810321808 Validation loss 0.053140390664339066 Accuracy 0.459228515625\n",
      "Iteration 63220 Training loss 0.046468570828437805 Validation loss 0.05307583883404732 Accuracy 0.4609375\n",
      "Iteration 63230 Training loss 0.05003798380494118 Validation loss 0.05320586636662483 Accuracy 0.4599609375\n",
      "Iteration 63240 Training loss 0.04895881190896034 Validation loss 0.05319495126605034 Accuracy 0.4599609375\n",
      "Iteration 63250 Training loss 0.05096558481454849 Validation loss 0.053376976400613785 Accuracy 0.458984375\n",
      "Iteration 63260 Training loss 0.05214564874768257 Validation loss 0.05340585485100746 Accuracy 0.45849609375\n",
      "Iteration 63270 Training loss 0.049069441854953766 Validation loss 0.05333758145570755 Accuracy 0.457763671875\n",
      "Iteration 63280 Training loss 0.052364349365234375 Validation loss 0.05335494130849838 Accuracy 0.45849609375\n",
      "Iteration 63290 Training loss 0.049871645867824554 Validation loss 0.053310491144657135 Accuracy 0.4580078125\n",
      "Iteration 63300 Training loss 0.0501069538295269 Validation loss 0.05320875719189644 Accuracy 0.460205078125\n",
      "Iteration 63310 Training loss 0.04884222894906998 Validation loss 0.052983999252319336 Accuracy 0.460205078125\n",
      "Iteration 63320 Training loss 0.05173571780323982 Validation loss 0.053123436868190765 Accuracy 0.460205078125\n",
      "Iteration 63330 Training loss 0.05357799679040909 Validation loss 0.053431689739227295 Accuracy 0.45654296875\n",
      "Iteration 63340 Training loss 0.052406106144189835 Validation loss 0.05396009609103203 Accuracy 0.4541015625\n",
      "Iteration 63350 Training loss 0.05174534022808075 Validation loss 0.05304470658302307 Accuracy 0.460693359375\n",
      "Iteration 63360 Training loss 0.053159669041633606 Validation loss 0.05347740650177002 Accuracy 0.457763671875\n",
      "Iteration 63370 Training loss 0.04738064855337143 Validation loss 0.053322337567806244 Accuracy 0.45849609375\n",
      "Iteration 63380 Training loss 0.05303172767162323 Validation loss 0.05321711674332619 Accuracy 0.459716796875\n",
      "Iteration 63390 Training loss 0.05225575342774391 Validation loss 0.05339030176401138 Accuracy 0.4580078125\n",
      "Iteration 63400 Training loss 0.05471740663051605 Validation loss 0.05320218577980995 Accuracy 0.459228515625\n",
      "Iteration 63410 Training loss 0.04961053282022476 Validation loss 0.05332352966070175 Accuracy 0.4599609375\n",
      "Iteration 63420 Training loss 0.051055699586868286 Validation loss 0.052973899990320206 Accuracy 0.45947265625\n",
      "Iteration 63430 Training loss 0.05420241877436638 Validation loss 0.05338018387556076 Accuracy 0.45849609375\n",
      "Iteration 63440 Training loss 0.05022050440311432 Validation loss 0.05316591262817383 Accuracy 0.459716796875\n",
      "Iteration 63450 Training loss 0.04980580508708954 Validation loss 0.053222961723804474 Accuracy 0.457275390625\n",
      "Iteration 63460 Training loss 0.04895079508423805 Validation loss 0.05309808626770973 Accuracy 0.46044921875\n",
      "Iteration 63470 Training loss 0.049556899815797806 Validation loss 0.05314340814948082 Accuracy 0.460205078125\n",
      "Iteration 63480 Training loss 0.050213705748319626 Validation loss 0.053661707788705826 Accuracy 0.4580078125\n",
      "Iteration 63490 Training loss 0.05072379484772682 Validation loss 0.05344373732805252 Accuracy 0.458251953125\n",
      "Iteration 63500 Training loss 0.051633819937705994 Validation loss 0.053387727588415146 Accuracy 0.45849609375\n",
      "Iteration 63510 Training loss 0.051021967083215714 Validation loss 0.05321406200528145 Accuracy 0.459228515625\n",
      "Iteration 63520 Training loss 0.05111115425825119 Validation loss 0.053178779780864716 Accuracy 0.460693359375\n",
      "Iteration 63530 Training loss 0.048357829451560974 Validation loss 0.05342636629939079 Accuracy 0.458251953125\n",
      "Iteration 63540 Training loss 0.05128296837210655 Validation loss 0.05287659540772438 Accuracy 0.461181640625\n",
      "Iteration 63550 Training loss 0.05021338909864426 Validation loss 0.05367061495780945 Accuracy 0.45654296875\n",
      "Iteration 63560 Training loss 0.049812424927949905 Validation loss 0.05348627641797066 Accuracy 0.45556640625\n",
      "Iteration 63570 Training loss 0.05039393901824951 Validation loss 0.053188715130090714 Accuracy 0.46044921875\n",
      "Iteration 63580 Training loss 0.04547201842069626 Validation loss 0.05308995023369789 Accuracy 0.45947265625\n",
      "Iteration 63590 Training loss 0.056524790823459625 Validation loss 0.05299511179327965 Accuracy 0.459716796875\n",
      "Iteration 63600 Training loss 0.050223760306835175 Validation loss 0.052878208458423615 Accuracy 0.461181640625\n",
      "Iteration 63610 Training loss 0.049907244741916656 Validation loss 0.05330429598689079 Accuracy 0.458740234375\n",
      "Iteration 63620 Training loss 0.05109455808997154 Validation loss 0.05313946306705475 Accuracy 0.459716796875\n",
      "Iteration 63630 Training loss 0.05043363571166992 Validation loss 0.05340614914894104 Accuracy 0.458251953125\n",
      "Iteration 63640 Training loss 0.05011969059705734 Validation loss 0.05364331975579262 Accuracy 0.45703125\n",
      "Iteration 63650 Training loss 0.04799365624785423 Validation loss 0.053544480353593826 Accuracy 0.458251953125\n",
      "Iteration 63660 Training loss 0.050689972937107086 Validation loss 0.05322820320725441 Accuracy 0.460205078125\n",
      "Iteration 63670 Training loss 0.049509964883327484 Validation loss 0.05332925543189049 Accuracy 0.45947265625\n",
      "Iteration 63680 Training loss 0.05265527218580246 Validation loss 0.053606364876031876 Accuracy 0.4560546875\n",
      "Iteration 63690 Training loss 0.052962981164455414 Validation loss 0.05317528173327446 Accuracy 0.459716796875\n",
      "Iteration 63700 Training loss 0.051264744251966476 Validation loss 0.0531165711581707 Accuracy 0.460205078125\n",
      "Iteration 63710 Training loss 0.04929303377866745 Validation loss 0.053115446120500565 Accuracy 0.459716796875\n",
      "Iteration 63720 Training loss 0.05235287919640541 Validation loss 0.053424060344696045 Accuracy 0.46044921875\n",
      "Iteration 63730 Training loss 0.05172114074230194 Validation loss 0.05319605767726898 Accuracy 0.458984375\n",
      "Iteration 63740 Training loss 0.048241231590509415 Validation loss 0.05307428911328316 Accuracy 0.460205078125\n",
      "Iteration 63750 Training loss 0.05077110603451729 Validation loss 0.052902158349752426 Accuracy 0.4609375\n",
      "Iteration 63760 Training loss 0.0492829903960228 Validation loss 0.05332863703370094 Accuracy 0.459228515625\n",
      "Iteration 63770 Training loss 0.050939515233039856 Validation loss 0.05355624482035637 Accuracy 0.456787109375\n",
      "Iteration 63780 Training loss 0.05260283872485161 Validation loss 0.05374963581562042 Accuracy 0.455078125\n",
      "Iteration 63790 Training loss 0.049768153578042984 Validation loss 0.05376451462507248 Accuracy 0.454345703125\n",
      "Iteration 63800 Training loss 0.052103545516729355 Validation loss 0.05332671105861664 Accuracy 0.4580078125\n",
      "Iteration 63810 Training loss 0.050454579293727875 Validation loss 0.053099144250154495 Accuracy 0.458984375\n",
      "Iteration 63820 Training loss 0.049436476081609726 Validation loss 0.05313200503587723 Accuracy 0.460205078125\n",
      "Iteration 63830 Training loss 0.05016361549496651 Validation loss 0.053110476583242416 Accuracy 0.460693359375\n",
      "Iteration 63840 Training loss 0.05476543679833412 Validation loss 0.05331949517130852 Accuracy 0.459716796875\n",
      "Iteration 63850 Training loss 0.05071601644158363 Validation loss 0.0534140020608902 Accuracy 0.45751953125\n",
      "Iteration 63860 Training loss 0.05304072052240372 Validation loss 0.053421903401613235 Accuracy 0.45751953125\n",
      "Iteration 63870 Training loss 0.052746351808309555 Validation loss 0.05322619155049324 Accuracy 0.4599609375\n",
      "Iteration 63880 Training loss 0.051621004939079285 Validation loss 0.053342096507549286 Accuracy 0.46044921875\n",
      "Iteration 63890 Training loss 0.054064828902482986 Validation loss 0.05339648202061653 Accuracy 0.458984375\n",
      "Iteration 63900 Training loss 0.04999392479658127 Validation loss 0.053464021533727646 Accuracy 0.4580078125\n",
      "Iteration 63910 Training loss 0.05097726359963417 Validation loss 0.05342002958059311 Accuracy 0.45751953125\n",
      "Iteration 63920 Training loss 0.050610799342393875 Validation loss 0.05327260494232178 Accuracy 0.45849609375\n",
      "Iteration 63930 Training loss 0.052072957158088684 Validation loss 0.053120944648981094 Accuracy 0.459716796875\n",
      "Iteration 63940 Training loss 0.051676299422979355 Validation loss 0.05356845632195473 Accuracy 0.457275390625\n",
      "Iteration 63950 Training loss 0.04836761951446533 Validation loss 0.053139373660087585 Accuracy 0.460693359375\n",
      "Iteration 63960 Training loss 0.05055384710431099 Validation loss 0.0531761609017849 Accuracy 0.459228515625\n",
      "Iteration 63970 Training loss 0.05262080207467079 Validation loss 0.05331997945904732 Accuracy 0.458740234375\n",
      "Iteration 63980 Training loss 0.05325856804847717 Validation loss 0.053297605365514755 Accuracy 0.45849609375\n",
      "Iteration 63990 Training loss 0.05062272027134895 Validation loss 0.05335768312215805 Accuracy 0.4580078125\n",
      "Iteration 64000 Training loss 0.056533657014369965 Validation loss 0.05309620872139931 Accuracy 0.460205078125\n",
      "Iteration 64010 Training loss 0.04811835289001465 Validation loss 0.05308988690376282 Accuracy 0.460205078125\n",
      "Iteration 64020 Training loss 0.04799937456846237 Validation loss 0.0531688816845417 Accuracy 0.459716796875\n",
      "Iteration 64030 Training loss 0.05285470560193062 Validation loss 0.05333241447806358 Accuracy 0.459716796875\n",
      "Iteration 64040 Training loss 0.05252975970506668 Validation loss 0.05323115736246109 Accuracy 0.459716796875\n",
      "Iteration 64050 Training loss 0.04988880455493927 Validation loss 0.05322635546326637 Accuracy 0.459228515625\n",
      "Iteration 64060 Training loss 0.05336274579167366 Validation loss 0.05325416848063469 Accuracy 0.459716796875\n",
      "Iteration 64070 Training loss 0.04931826889514923 Validation loss 0.05354650318622589 Accuracy 0.458251953125\n",
      "Iteration 64080 Training loss 0.05165731906890869 Validation loss 0.05334514006972313 Accuracy 0.458984375\n",
      "Iteration 64090 Training loss 0.05157384276390076 Validation loss 0.05346381664276123 Accuracy 0.456787109375\n",
      "Iteration 64100 Training loss 0.04986695200204849 Validation loss 0.053092263638973236 Accuracy 0.4599609375\n",
      "Iteration 64110 Training loss 0.05117829516530037 Validation loss 0.05338841676712036 Accuracy 0.4580078125\n",
      "Iteration 64120 Training loss 0.05333754047751427 Validation loss 0.0532548651099205 Accuracy 0.4599609375\n",
      "Iteration 64130 Training loss 0.05117262899875641 Validation loss 0.05335112661123276 Accuracy 0.4580078125\n",
      "Iteration 64140 Training loss 0.05345918610692024 Validation loss 0.05332992970943451 Accuracy 0.458984375\n",
      "Iteration 64150 Training loss 0.05227292701601982 Validation loss 0.05318467691540718 Accuracy 0.45947265625\n",
      "Iteration 64160 Training loss 0.05154476687312126 Validation loss 0.053056396543979645 Accuracy 0.460693359375\n",
      "Iteration 64170 Training loss 0.04986054450273514 Validation loss 0.05293138697743416 Accuracy 0.46240234375\n",
      "Iteration 64180 Training loss 0.05177567899227142 Validation loss 0.05298488959670067 Accuracy 0.460205078125\n",
      "Iteration 64190 Training loss 0.047821044921875 Validation loss 0.05317090079188347 Accuracy 0.458984375\n",
      "Iteration 64200 Training loss 0.05199072137475014 Validation loss 0.05328057333827019 Accuracy 0.4599609375\n",
      "Iteration 64210 Training loss 0.04807380214333534 Validation loss 0.05319231376051903 Accuracy 0.460693359375\n",
      "Iteration 64220 Training loss 0.05186131224036217 Validation loss 0.05320766940712929 Accuracy 0.45947265625\n",
      "Iteration 64230 Training loss 0.05227141082286835 Validation loss 0.05293010547757149 Accuracy 0.4609375\n",
      "Iteration 64240 Training loss 0.047878447920084 Validation loss 0.052909474819898605 Accuracy 0.460693359375\n",
      "Iteration 64250 Training loss 0.05167723447084427 Validation loss 0.053821317851543427 Accuracy 0.455322265625\n",
      "Iteration 64260 Training loss 0.051981545984745026 Validation loss 0.05298655107617378 Accuracy 0.4609375\n",
      "Iteration 64270 Training loss 0.05722019448876381 Validation loss 0.05320831760764122 Accuracy 0.4599609375\n",
      "Iteration 64280 Training loss 0.050289448350667953 Validation loss 0.05306443199515343 Accuracy 0.460205078125\n",
      "Iteration 64290 Training loss 0.05116613581776619 Validation loss 0.053404733538627625 Accuracy 0.457275390625\n",
      "Iteration 64300 Training loss 0.05159199982881546 Validation loss 0.052854832261800766 Accuracy 0.461181640625\n",
      "Iteration 64310 Training loss 0.05157705023884773 Validation loss 0.05300586670637131 Accuracy 0.4609375\n",
      "Iteration 64320 Training loss 0.051675375550985336 Validation loss 0.052880458533763885 Accuracy 0.4609375\n",
      "Iteration 64330 Training loss 0.05386829003691673 Validation loss 0.05312320217490196 Accuracy 0.4609375\n",
      "Iteration 64340 Training loss 0.04939310625195503 Validation loss 0.05345264449715614 Accuracy 0.4580078125\n",
      "Iteration 64350 Training loss 0.05189305543899536 Validation loss 0.05399457737803459 Accuracy 0.451904296875\n",
      "Iteration 64360 Training loss 0.05017342418432236 Validation loss 0.05322170630097389 Accuracy 0.4599609375\n",
      "Iteration 64370 Training loss 0.04986438900232315 Validation loss 0.05342627689242363 Accuracy 0.45849609375\n",
      "Iteration 64380 Training loss 0.04907349497079849 Validation loss 0.05288281291723251 Accuracy 0.46142578125\n",
      "Iteration 64390 Training loss 0.05382629483938217 Validation loss 0.053185638040304184 Accuracy 0.458984375\n",
      "Iteration 64400 Training loss 0.0542752668261528 Validation loss 0.05309585481882095 Accuracy 0.45849609375\n",
      "Iteration 64410 Training loss 0.051554977893829346 Validation loss 0.053010642528533936 Accuracy 0.461181640625\n",
      "Iteration 64420 Training loss 0.05059454217553139 Validation loss 0.05317573994398117 Accuracy 0.46044921875\n",
      "Iteration 64430 Training loss 0.05229796841740608 Validation loss 0.05444832891225815 Accuracy 0.4482421875\n",
      "Iteration 64440 Training loss 0.04859934374690056 Validation loss 0.05310721695423126 Accuracy 0.460693359375\n",
      "Iteration 64450 Training loss 0.05028901994228363 Validation loss 0.052979253232479095 Accuracy 0.461181640625\n",
      "Iteration 64460 Training loss 0.05382067337632179 Validation loss 0.05317145213484764 Accuracy 0.460205078125\n",
      "Iteration 64470 Training loss 0.05064705014228821 Validation loss 0.05328233167529106 Accuracy 0.458984375\n",
      "Iteration 64480 Training loss 0.05167490616440773 Validation loss 0.05329765006899834 Accuracy 0.45947265625\n",
      "Iteration 64490 Training loss 0.05142977833747864 Validation loss 0.05302993580698967 Accuracy 0.4599609375\n",
      "Iteration 64500 Training loss 0.053932398557662964 Validation loss 0.053258661180734634 Accuracy 0.459228515625\n",
      "Iteration 64510 Training loss 0.049241483211517334 Validation loss 0.05308106169104576 Accuracy 0.460693359375\n",
      "Iteration 64520 Training loss 0.05159991234540939 Validation loss 0.053271424025297165 Accuracy 0.458740234375\n",
      "Iteration 64530 Training loss 0.05176810175180435 Validation loss 0.05340096727013588 Accuracy 0.456787109375\n",
      "Iteration 64540 Training loss 0.048553988337516785 Validation loss 0.052984461188316345 Accuracy 0.460693359375\n",
      "Iteration 64550 Training loss 0.05164012685418129 Validation loss 0.05351204052567482 Accuracy 0.458740234375\n",
      "Iteration 64560 Training loss 0.04936090111732483 Validation loss 0.053202155977487564 Accuracy 0.461181640625\n",
      "Iteration 64570 Training loss 0.05379632115364075 Validation loss 0.05389019846916199 Accuracy 0.455322265625\n",
      "Iteration 64580 Training loss 0.0532694086432457 Validation loss 0.053366269916296005 Accuracy 0.4599609375\n",
      "Iteration 64590 Training loss 0.050699736922979355 Validation loss 0.05332285538315773 Accuracy 0.458740234375\n",
      "Iteration 64600 Training loss 0.05492522194981575 Validation loss 0.0538581907749176 Accuracy 0.455322265625\n",
      "Iteration 64610 Training loss 0.052695952355861664 Validation loss 0.052946947515010834 Accuracy 0.460693359375\n",
      "Iteration 64620 Training loss 0.05156436935067177 Validation loss 0.0533771812915802 Accuracy 0.45849609375\n",
      "Iteration 64630 Training loss 0.053143542259931564 Validation loss 0.05336258187890053 Accuracy 0.4580078125\n",
      "Iteration 64640 Training loss 0.045692864805459976 Validation loss 0.05357222631573677 Accuracy 0.45751953125\n",
      "Iteration 64650 Training loss 0.05452216416597366 Validation loss 0.053346898406744 Accuracy 0.46044921875\n",
      "Iteration 64660 Training loss 0.0521063394844532 Validation loss 0.05370260030031204 Accuracy 0.45556640625\n",
      "Iteration 64670 Training loss 0.04600389674305916 Validation loss 0.053068168461322784 Accuracy 0.460205078125\n",
      "Iteration 64680 Training loss 0.04914882406592369 Validation loss 0.05299447476863861 Accuracy 0.46044921875\n",
      "Iteration 64690 Training loss 0.05020550638437271 Validation loss 0.053274016827344894 Accuracy 0.4599609375\n",
      "Iteration 64700 Training loss 0.05320880934596062 Validation loss 0.05345408245921135 Accuracy 0.45751953125\n",
      "Iteration 64710 Training loss 0.05625486001372337 Validation loss 0.05339278280735016 Accuracy 0.45751953125\n",
      "Iteration 64720 Training loss 0.04806894809007645 Validation loss 0.05299992114305496 Accuracy 0.4599609375\n",
      "Iteration 64730 Training loss 0.049926575273275375 Validation loss 0.053128305822610855 Accuracy 0.461181640625\n",
      "Iteration 64740 Training loss 0.05242595449090004 Validation loss 0.05351760983467102 Accuracy 0.45751953125\n",
      "Iteration 64750 Training loss 0.04924078285694122 Validation loss 0.053249701857566833 Accuracy 0.460205078125\n",
      "Iteration 64760 Training loss 0.05203171819448471 Validation loss 0.05360957980155945 Accuracy 0.456787109375\n",
      "Iteration 64770 Training loss 0.046583108603954315 Validation loss 0.0530962198972702 Accuracy 0.45947265625\n",
      "Iteration 64780 Training loss 0.050912242382764816 Validation loss 0.053151991218328476 Accuracy 0.45947265625\n",
      "Iteration 64790 Training loss 0.053177427500486374 Validation loss 0.05304349586367607 Accuracy 0.461181640625\n",
      "Iteration 64800 Training loss 0.055242929607629776 Validation loss 0.054562587291002274 Accuracy 0.4482421875\n",
      "Iteration 64810 Training loss 0.05147042125463486 Validation loss 0.053306445479393005 Accuracy 0.459716796875\n",
      "Iteration 64820 Training loss 0.05283055081963539 Validation loss 0.05301778018474579 Accuracy 0.460693359375\n",
      "Iteration 64830 Training loss 0.05108116567134857 Validation loss 0.05349458009004593 Accuracy 0.456298828125\n",
      "Iteration 64840 Training loss 0.04767389968037605 Validation loss 0.05326930060982704 Accuracy 0.4609375\n",
      "Iteration 64850 Training loss 0.05263214930891991 Validation loss 0.05336831882596016 Accuracy 0.459716796875\n",
      "Iteration 64860 Training loss 0.050296906381845474 Validation loss 0.05301113799214363 Accuracy 0.461669921875\n",
      "Iteration 64870 Training loss 0.0521218404173851 Validation loss 0.05338761582970619 Accuracy 0.4599609375\n",
      "Iteration 64880 Training loss 0.04761819541454315 Validation loss 0.053583525121212006 Accuracy 0.45654296875\n",
      "Iteration 64890 Training loss 0.050501592457294464 Validation loss 0.0530947782099247 Accuracy 0.4609375\n",
      "Iteration 64900 Training loss 0.05677046626806259 Validation loss 0.05328934267163277 Accuracy 0.460205078125\n",
      "Iteration 64910 Training loss 0.05088791996240616 Validation loss 0.05375361070036888 Accuracy 0.45556640625\n",
      "Iteration 64920 Training loss 0.05094325542449951 Validation loss 0.05320141091942787 Accuracy 0.458984375\n",
      "Iteration 64930 Training loss 0.04894619807600975 Validation loss 0.05323340743780136 Accuracy 0.4599609375\n",
      "Iteration 64940 Training loss 0.04921415075659752 Validation loss 0.053178612142801285 Accuracy 0.459228515625\n",
      "Iteration 64950 Training loss 0.05156844109296799 Validation loss 0.0530511736869812 Accuracy 0.46044921875\n",
      "Iteration 64960 Training loss 0.05156615749001503 Validation loss 0.05350739136338234 Accuracy 0.45703125\n",
      "Iteration 64970 Training loss 0.0491197407245636 Validation loss 0.053066037595272064 Accuracy 0.460205078125\n",
      "Iteration 64980 Training loss 0.051937103271484375 Validation loss 0.05326123908162117 Accuracy 0.460205078125\n",
      "Iteration 64990 Training loss 0.04918012022972107 Validation loss 0.05303783714771271 Accuracy 0.46044921875\n",
      "Iteration 65000 Training loss 0.04959506168961525 Validation loss 0.05325599014759064 Accuracy 0.45947265625\n",
      "Iteration 65010 Training loss 0.050498951226472855 Validation loss 0.05328616127371788 Accuracy 0.45849609375\n",
      "Iteration 65020 Training loss 0.05140857771039009 Validation loss 0.05307750403881073 Accuracy 0.4609375\n",
      "Iteration 65030 Training loss 0.05166216939687729 Validation loss 0.05310100317001343 Accuracy 0.460205078125\n",
      "Iteration 65040 Training loss 0.04876646399497986 Validation loss 0.053347013890743256 Accuracy 0.458984375\n",
      "Iteration 65050 Training loss 0.05240464210510254 Validation loss 0.05349230393767357 Accuracy 0.45849609375\n",
      "Iteration 65060 Training loss 0.050148580223321915 Validation loss 0.05338738486170769 Accuracy 0.458984375\n",
      "Iteration 65070 Training loss 0.0514962412416935 Validation loss 0.053349681198596954 Accuracy 0.45849609375\n",
      "Iteration 65080 Training loss 0.05115374177694321 Validation loss 0.053580477833747864 Accuracy 0.456298828125\n",
      "Iteration 65090 Training loss 0.05138416588306427 Validation loss 0.0532202385365963 Accuracy 0.460205078125\n",
      "Iteration 65100 Training loss 0.051148273050785065 Validation loss 0.053338706493377686 Accuracy 0.458984375\n",
      "Iteration 65110 Training loss 0.05070530250668526 Validation loss 0.05315149948000908 Accuracy 0.46044921875\n",
      "Iteration 65120 Training loss 0.050433848053216934 Validation loss 0.0532771535217762 Accuracy 0.458984375\n",
      "Iteration 65130 Training loss 0.05073035508394241 Validation loss 0.0534183569252491 Accuracy 0.4580078125\n",
      "Iteration 65140 Training loss 0.04945698007941246 Validation loss 0.05315663293004036 Accuracy 0.459716796875\n",
      "Iteration 65150 Training loss 0.05120197311043739 Validation loss 0.053055185824632645 Accuracy 0.459716796875\n",
      "Iteration 65160 Training loss 0.05189298093318939 Validation loss 0.05325189232826233 Accuracy 0.458251953125\n",
      "Iteration 65170 Training loss 0.05346408486366272 Validation loss 0.05294927582144737 Accuracy 0.460693359375\n",
      "Iteration 65180 Training loss 0.053316254168748856 Validation loss 0.0533854216337204 Accuracy 0.4580078125\n",
      "Iteration 65190 Training loss 0.05091220140457153 Validation loss 0.05321120098233223 Accuracy 0.45947265625\n",
      "Iteration 65200 Training loss 0.050736941397190094 Validation loss 0.05363553389906883 Accuracy 0.45654296875\n",
      "Iteration 65210 Training loss 0.046929311007261276 Validation loss 0.05347886681556702 Accuracy 0.4580078125\n",
      "Iteration 65220 Training loss 0.05025622621178627 Validation loss 0.053344275802373886 Accuracy 0.458251953125\n",
      "Iteration 65230 Training loss 0.05194905027747154 Validation loss 0.05311960726976395 Accuracy 0.458984375\n",
      "Iteration 65240 Training loss 0.05237126722931862 Validation loss 0.053166404366493225 Accuracy 0.458740234375\n",
      "Iteration 65250 Training loss 0.055309440940618515 Validation loss 0.05337565392255783 Accuracy 0.457763671875\n",
      "Iteration 65260 Training loss 0.049846358597278595 Validation loss 0.053267702460289 Accuracy 0.460205078125\n",
      "Iteration 65270 Training loss 0.051090057939291 Validation loss 0.05333109200000763 Accuracy 0.458740234375\n",
      "Iteration 65280 Training loss 0.048317667096853256 Validation loss 0.053032275289297104 Accuracy 0.4599609375\n",
      "Iteration 65290 Training loss 0.05170333757996559 Validation loss 0.05326463654637337 Accuracy 0.458984375\n",
      "Iteration 65300 Training loss 0.04875649884343147 Validation loss 0.05349705368280411 Accuracy 0.457763671875\n",
      "Iteration 65310 Training loss 0.04843749478459358 Validation loss 0.052971698343753815 Accuracy 0.460693359375\n",
      "Iteration 65320 Training loss 0.05406834930181503 Validation loss 0.05347711592912674 Accuracy 0.45849609375\n",
      "Iteration 65330 Training loss 0.04437803104519844 Validation loss 0.05339903011918068 Accuracy 0.45947265625\n",
      "Iteration 65340 Training loss 0.051691990345716476 Validation loss 0.05293089523911476 Accuracy 0.460205078125\n",
      "Iteration 65350 Training loss 0.05339376628398895 Validation loss 0.05339863523840904 Accuracy 0.458984375\n",
      "Iteration 65360 Training loss 0.048951029777526855 Validation loss 0.05344190448522568 Accuracy 0.460205078125\n",
      "Iteration 65370 Training loss 0.05158722773194313 Validation loss 0.053237367421388626 Accuracy 0.45947265625\n",
      "Iteration 65380 Training loss 0.050035521388053894 Validation loss 0.053596701472997665 Accuracy 0.456298828125\n",
      "Iteration 65390 Training loss 0.05112575739622116 Validation loss 0.05381918326020241 Accuracy 0.45458984375\n",
      "Iteration 65400 Training loss 0.05293659493327141 Validation loss 0.053098056465387344 Accuracy 0.458984375\n",
      "Iteration 65410 Training loss 0.05011071637272835 Validation loss 0.053123585879802704 Accuracy 0.458740234375\n",
      "Iteration 65420 Training loss 0.05256001651287079 Validation loss 0.05338582769036293 Accuracy 0.458251953125\n",
      "Iteration 65430 Training loss 0.05207699164748192 Validation loss 0.053023561835289 Accuracy 0.45947265625\n",
      "Iteration 65440 Training loss 0.05249326676130295 Validation loss 0.053059980273246765 Accuracy 0.461181640625\n",
      "Iteration 65450 Training loss 0.0566643662750721 Validation loss 0.05363374948501587 Accuracy 0.455810546875\n",
      "Iteration 65460 Training loss 0.050823189318180084 Validation loss 0.05340922623872757 Accuracy 0.458984375\n",
      "Iteration 65470 Training loss 0.04704202339053154 Validation loss 0.05292453616857529 Accuracy 0.461181640625\n",
      "Iteration 65480 Training loss 0.04843602329492569 Validation loss 0.05293995141983032 Accuracy 0.460205078125\n",
      "Iteration 65490 Training loss 0.05272865667939186 Validation loss 0.05339998006820679 Accuracy 0.458740234375\n",
      "Iteration 65500 Training loss 0.05372631922364235 Validation loss 0.053225815296173096 Accuracy 0.460205078125\n",
      "Iteration 65510 Training loss 0.05259969085454941 Validation loss 0.053536444902420044 Accuracy 0.456298828125\n",
      "Iteration 65520 Training loss 0.050439197570085526 Validation loss 0.053172558546066284 Accuracy 0.4599609375\n",
      "Iteration 65530 Training loss 0.052172910422086716 Validation loss 0.053487859666347504 Accuracy 0.45751953125\n",
      "Iteration 65540 Training loss 0.0508975014090538 Validation loss 0.05332210287451744 Accuracy 0.4580078125\n",
      "Iteration 65550 Training loss 0.04899347946047783 Validation loss 0.05302833765745163 Accuracy 0.4599609375\n",
      "Iteration 65560 Training loss 0.051392991095781326 Validation loss 0.05291016772389412 Accuracy 0.461181640625\n",
      "Iteration 65570 Training loss 0.05574129521846771 Validation loss 0.05350007861852646 Accuracy 0.45751953125\n",
      "Iteration 65580 Training loss 0.05437299609184265 Validation loss 0.05321788787841797 Accuracy 0.459716796875\n",
      "Iteration 65590 Training loss 0.05259688198566437 Validation loss 0.05294033885002136 Accuracy 0.461669921875\n",
      "Iteration 65600 Training loss 0.04984809458255768 Validation loss 0.05309125781059265 Accuracy 0.459716796875\n",
      "Iteration 65610 Training loss 0.05416296795010567 Validation loss 0.053335029631853104 Accuracy 0.458984375\n",
      "Iteration 65620 Training loss 0.05012388899922371 Validation loss 0.05314890667796135 Accuracy 0.459228515625\n",
      "Iteration 65630 Training loss 0.051695678383111954 Validation loss 0.05330704152584076 Accuracy 0.457763671875\n",
      "Iteration 65640 Training loss 0.05300747603178024 Validation loss 0.053331077098846436 Accuracy 0.45849609375\n",
      "Iteration 65650 Training loss 0.050342850387096405 Validation loss 0.05305217579007149 Accuracy 0.459716796875\n",
      "Iteration 65660 Training loss 0.05110954865813255 Validation loss 0.052961815148591995 Accuracy 0.4609375\n",
      "Iteration 65670 Training loss 0.05331888422369957 Validation loss 0.05315876752138138 Accuracy 0.460205078125\n",
      "Iteration 65680 Training loss 0.05116880312561989 Validation loss 0.05296870321035385 Accuracy 0.4609375\n",
      "Iteration 65690 Training loss 0.050640933215618134 Validation loss 0.05356431379914284 Accuracy 0.4580078125\n",
      "Iteration 65700 Training loss 0.05330422520637512 Validation loss 0.053020648658275604 Accuracy 0.4599609375\n",
      "Iteration 65710 Training loss 0.051419224590063095 Validation loss 0.053511444479227066 Accuracy 0.456787109375\n",
      "Iteration 65720 Training loss 0.05096033588051796 Validation loss 0.05359921604394913 Accuracy 0.456787109375\n",
      "Iteration 65730 Training loss 0.051086537539958954 Validation loss 0.05350888520479202 Accuracy 0.45703125\n",
      "Iteration 65740 Training loss 0.04828762635588646 Validation loss 0.05304872989654541 Accuracy 0.461669921875\n",
      "Iteration 65750 Training loss 0.05408443510532379 Validation loss 0.053312573581933975 Accuracy 0.459716796875\n",
      "Iteration 65760 Training loss 0.05026146396994591 Validation loss 0.05316098406910896 Accuracy 0.46142578125\n",
      "Iteration 65770 Training loss 0.050712909549474716 Validation loss 0.0530342161655426 Accuracy 0.46044921875\n",
      "Iteration 65780 Training loss 0.05267813429236412 Validation loss 0.053313493728637695 Accuracy 0.459228515625\n",
      "Iteration 65790 Training loss 0.046880390495061874 Validation loss 0.05301453918218613 Accuracy 0.460693359375\n",
      "Iteration 65800 Training loss 0.05087811127305031 Validation loss 0.05335211753845215 Accuracy 0.460205078125\n",
      "Iteration 65810 Training loss 0.05115668848156929 Validation loss 0.05286586657166481 Accuracy 0.46142578125\n",
      "Iteration 65820 Training loss 0.05308464542031288 Validation loss 0.053193315863609314 Accuracy 0.45947265625\n",
      "Iteration 65830 Training loss 0.05048247054219246 Validation loss 0.05320562794804573 Accuracy 0.459716796875\n",
      "Iteration 65840 Training loss 0.05109100788831711 Validation loss 0.05311301723122597 Accuracy 0.459716796875\n",
      "Iteration 65850 Training loss 0.04904927685856819 Validation loss 0.05297688767313957 Accuracy 0.461669921875\n",
      "Iteration 65860 Training loss 0.05287773534655571 Validation loss 0.05310668796300888 Accuracy 0.461181640625\n",
      "Iteration 65870 Training loss 0.048106107860803604 Validation loss 0.05308341979980469 Accuracy 0.4599609375\n",
      "Iteration 65880 Training loss 0.05139777436852455 Validation loss 0.053292807191610336 Accuracy 0.457763671875\n",
      "Iteration 65890 Training loss 0.05139858275651932 Validation loss 0.052850183099508286 Accuracy 0.461669921875\n",
      "Iteration 65900 Training loss 0.05320478603243828 Validation loss 0.0534108467400074 Accuracy 0.45849609375\n",
      "Iteration 65910 Training loss 0.05079265311360359 Validation loss 0.05299915000796318 Accuracy 0.460693359375\n",
      "Iteration 65920 Training loss 0.05105040594935417 Validation loss 0.05296389013528824 Accuracy 0.461181640625\n",
      "Iteration 65930 Training loss 0.05418743938207626 Validation loss 0.05445099622011185 Accuracy 0.44873046875\n",
      "Iteration 65940 Training loss 0.05028041452169418 Validation loss 0.052873626351356506 Accuracy 0.46142578125\n",
      "Iteration 65950 Training loss 0.0494152307510376 Validation loss 0.053028613328933716 Accuracy 0.4609375\n",
      "Iteration 65960 Training loss 0.05213402584195137 Validation loss 0.05284521356225014 Accuracy 0.46240234375\n",
      "Iteration 65970 Training loss 0.05144314467906952 Validation loss 0.05310408025979996 Accuracy 0.46044921875\n",
      "Iteration 65980 Training loss 0.05389980599284172 Validation loss 0.053289007395505905 Accuracy 0.4599609375\n",
      "Iteration 65990 Training loss 0.05163605883717537 Validation loss 0.05368238687515259 Accuracy 0.45458984375\n",
      "Iteration 66000 Training loss 0.051757924258708954 Validation loss 0.05298442766070366 Accuracy 0.46240234375\n",
      "Iteration 66010 Training loss 0.050878915935754776 Validation loss 0.05354789271950722 Accuracy 0.45849609375\n",
      "Iteration 66020 Training loss 0.05354645848274231 Validation loss 0.0529281347990036 Accuracy 0.46044921875\n",
      "Iteration 66030 Training loss 0.05203292891383171 Validation loss 0.05331467092037201 Accuracy 0.458740234375\n",
      "Iteration 66040 Training loss 0.05068868398666382 Validation loss 0.05326078087091446 Accuracy 0.45947265625\n",
      "Iteration 66050 Training loss 0.05053723230957985 Validation loss 0.05303048714995384 Accuracy 0.461181640625\n",
      "Iteration 66060 Training loss 0.053021568804979324 Validation loss 0.05288844555616379 Accuracy 0.461181640625\n",
      "Iteration 66070 Training loss 0.04921640828251839 Validation loss 0.05343559384346008 Accuracy 0.45849609375\n",
      "Iteration 66080 Training loss 0.05348769947886467 Validation loss 0.053428709506988525 Accuracy 0.4580078125\n",
      "Iteration 66090 Training loss 0.051140569150447845 Validation loss 0.0529286153614521 Accuracy 0.460693359375\n",
      "Iteration 66100 Training loss 0.052681293338537216 Validation loss 0.05304974317550659 Accuracy 0.46044921875\n",
      "Iteration 66110 Training loss 0.047084394842386246 Validation loss 0.05307497829198837 Accuracy 0.4609375\n",
      "Iteration 66120 Training loss 0.05279126763343811 Validation loss 0.053128719329833984 Accuracy 0.460205078125\n",
      "Iteration 66130 Training loss 0.04896564781665802 Validation loss 0.05309934914112091 Accuracy 0.4599609375\n",
      "Iteration 66140 Training loss 0.05389155074954033 Validation loss 0.053061582148075104 Accuracy 0.460205078125\n",
      "Iteration 66150 Training loss 0.049664996564388275 Validation loss 0.053349051624536514 Accuracy 0.457275390625\n",
      "Iteration 66160 Training loss 0.05126557499170303 Validation loss 0.053601186722517014 Accuracy 0.4580078125\n",
      "Iteration 66170 Training loss 0.05183371901512146 Validation loss 0.0531504824757576 Accuracy 0.4599609375\n",
      "Iteration 66180 Training loss 0.050772085785865784 Validation loss 0.05359143763780594 Accuracy 0.456787109375\n",
      "Iteration 66190 Training loss 0.04981640726327896 Validation loss 0.05304737016558647 Accuracy 0.45947265625\n",
      "Iteration 66200 Training loss 0.048501722514629364 Validation loss 0.05288727581501007 Accuracy 0.4609375\n",
      "Iteration 66210 Training loss 0.05167597904801369 Validation loss 0.05314934253692627 Accuracy 0.46044921875\n",
      "Iteration 66220 Training loss 0.049984466284513474 Validation loss 0.05301722511649132 Accuracy 0.4609375\n",
      "Iteration 66230 Training loss 0.050828125327825546 Validation loss 0.0529995895922184 Accuracy 0.460693359375\n",
      "Iteration 66240 Training loss 0.0515093132853508 Validation loss 0.0528634637594223 Accuracy 0.46142578125\n",
      "Iteration 66250 Training loss 0.0486048087477684 Validation loss 0.0530393086373806 Accuracy 0.4609375\n",
      "Iteration 66260 Training loss 0.05109649524092674 Validation loss 0.053022805601358414 Accuracy 0.462158203125\n",
      "Iteration 66270 Training loss 0.0478026308119297 Validation loss 0.052926063537597656 Accuracy 0.461669921875\n",
      "Iteration 66280 Training loss 0.05057268589735031 Validation loss 0.05358234420418739 Accuracy 0.45654296875\n",
      "Iteration 66290 Training loss 0.05073133111000061 Validation loss 0.05279391631484032 Accuracy 0.4619140625\n",
      "Iteration 66300 Training loss 0.05341477319598198 Validation loss 0.0533134900033474 Accuracy 0.45849609375\n",
      "Iteration 66310 Training loss 0.05114630237221718 Validation loss 0.053756844252347946 Accuracy 0.455810546875\n",
      "Iteration 66320 Training loss 0.05372222885489464 Validation loss 0.05491054430603981 Accuracy 0.4453125\n",
      "Iteration 66330 Training loss 0.04928063973784447 Validation loss 0.05289863049983978 Accuracy 0.461181640625\n",
      "Iteration 66340 Training loss 0.05079945921897888 Validation loss 0.053736936300992966 Accuracy 0.45263671875\n",
      "Iteration 66350 Training loss 0.049536917358636856 Validation loss 0.05278894305229187 Accuracy 0.461669921875\n",
      "Iteration 66360 Training loss 0.0470786988735199 Validation loss 0.05341336131095886 Accuracy 0.4580078125\n",
      "Iteration 66370 Training loss 0.05341087654232979 Validation loss 0.05286509543657303 Accuracy 0.461181640625\n",
      "Iteration 66380 Training loss 0.050645582377910614 Validation loss 0.05304950103163719 Accuracy 0.4609375\n",
      "Iteration 66390 Training loss 0.050601400434970856 Validation loss 0.05286463350057602 Accuracy 0.46142578125\n",
      "Iteration 66400 Training loss 0.05166478827595711 Validation loss 0.05298586189746857 Accuracy 0.46240234375\n",
      "Iteration 66410 Training loss 0.04927019774913788 Validation loss 0.05310393497347832 Accuracy 0.459716796875\n",
      "Iteration 66420 Training loss 0.0514867790043354 Validation loss 0.052760083228349686 Accuracy 0.461669921875\n",
      "Iteration 66430 Training loss 0.04797530919313431 Validation loss 0.053034570068120956 Accuracy 0.460693359375\n",
      "Iteration 66440 Training loss 0.05121711641550064 Validation loss 0.05369420722126961 Accuracy 0.456787109375\n",
      "Iteration 66450 Training loss 0.0495464988052845 Validation loss 0.052813056856393814 Accuracy 0.4619140625\n",
      "Iteration 66460 Training loss 0.05477546155452728 Validation loss 0.0528380461037159 Accuracy 0.4619140625\n",
      "Iteration 66470 Training loss 0.04979792237281799 Validation loss 0.05305862054228783 Accuracy 0.46142578125\n",
      "Iteration 66480 Training loss 0.053543709218502045 Validation loss 0.0531652607023716 Accuracy 0.4619140625\n",
      "Iteration 66490 Training loss 0.0515233650803566 Validation loss 0.05309232324361801 Accuracy 0.460205078125\n",
      "Iteration 66500 Training loss 0.04975120350718498 Validation loss 0.0530446358025074 Accuracy 0.460205078125\n",
      "Iteration 66510 Training loss 0.050570953637361526 Validation loss 0.053136203438043594 Accuracy 0.459716796875\n",
      "Iteration 66520 Training loss 0.04647958278656006 Validation loss 0.053254134953022 Accuracy 0.460205078125\n",
      "Iteration 66530 Training loss 0.05244774371385574 Validation loss 0.0541970394551754 Accuracy 0.451171875\n",
      "Iteration 66540 Training loss 0.05371993035078049 Validation loss 0.053446341305971146 Accuracy 0.458984375\n",
      "Iteration 66550 Training loss 0.04721895232796669 Validation loss 0.05327153205871582 Accuracy 0.460693359375\n",
      "Iteration 66560 Training loss 0.05159543827176094 Validation loss 0.05289516970515251 Accuracy 0.46240234375\n",
      "Iteration 66570 Training loss 0.0527426041662693 Validation loss 0.05312425643205643 Accuracy 0.4599609375\n",
      "Iteration 66580 Training loss 0.050783753395080566 Validation loss 0.05331838130950928 Accuracy 0.45751953125\n",
      "Iteration 66590 Training loss 0.05371807888150215 Validation loss 0.05337128788232803 Accuracy 0.458251953125\n",
      "Iteration 66600 Training loss 0.054307516664266586 Validation loss 0.053676992654800415 Accuracy 0.455078125\n",
      "Iteration 66610 Training loss 0.051283176988363266 Validation loss 0.053092021495103836 Accuracy 0.460205078125\n",
      "Iteration 66620 Training loss 0.05383114516735077 Validation loss 0.05348366126418114 Accuracy 0.45947265625\n",
      "Iteration 66630 Training loss 0.04870358109474182 Validation loss 0.05313189700245857 Accuracy 0.459716796875\n",
      "Iteration 66640 Training loss 0.050166595727205276 Validation loss 0.05307508632540703 Accuracy 0.460693359375\n",
      "Iteration 66650 Training loss 0.05231504887342453 Validation loss 0.05290018394589424 Accuracy 0.4609375\n",
      "Iteration 66660 Training loss 0.05055437609553337 Validation loss 0.05292178690433502 Accuracy 0.461669921875\n",
      "Iteration 66670 Training loss 0.05566847696900368 Validation loss 0.053809754550457 Accuracy 0.455078125\n",
      "Iteration 66680 Training loss 0.04912767559289932 Validation loss 0.052696727216243744 Accuracy 0.46240234375\n",
      "Iteration 66690 Training loss 0.0492321215569973 Validation loss 0.053070198744535446 Accuracy 0.461181640625\n",
      "Iteration 66700 Training loss 0.050123970955610275 Validation loss 0.053192947059869766 Accuracy 0.460693359375\n",
      "Iteration 66710 Training loss 0.05238945409655571 Validation loss 0.05353807657957077 Accuracy 0.458984375\n",
      "Iteration 66720 Training loss 0.05113536864519119 Validation loss 0.052944596856832504 Accuracy 0.460205078125\n",
      "Iteration 66730 Training loss 0.05058516561985016 Validation loss 0.052986029535532 Accuracy 0.460693359375\n",
      "Iteration 66740 Training loss 0.05195792764425278 Validation loss 0.05327202379703522 Accuracy 0.45947265625\n",
      "Iteration 66750 Training loss 0.0519716776907444 Validation loss 0.05276601016521454 Accuracy 0.4619140625\n",
      "Iteration 66760 Training loss 0.051921285688877106 Validation loss 0.05335775390267372 Accuracy 0.46044921875\n",
      "Iteration 66770 Training loss 0.051558755338191986 Validation loss 0.05353126674890518 Accuracy 0.45849609375\n",
      "Iteration 66780 Training loss 0.04876488074660301 Validation loss 0.053386248648166656 Accuracy 0.45556640625\n",
      "Iteration 66790 Training loss 0.05158882588148117 Validation loss 0.053017981350421906 Accuracy 0.4619140625\n",
      "Iteration 66800 Training loss 0.05162999406456947 Validation loss 0.052797820419073105 Accuracy 0.4619140625\n",
      "Iteration 66810 Training loss 0.05176742002367973 Validation loss 0.05305255204439163 Accuracy 0.46044921875\n",
      "Iteration 66820 Training loss 0.05335019528865814 Validation loss 0.053048957139253616 Accuracy 0.46142578125\n",
      "Iteration 66830 Training loss 0.05264887586236 Validation loss 0.05303972586989403 Accuracy 0.460205078125\n",
      "Iteration 66840 Training loss 0.05149132385849953 Validation loss 0.053336795419454575 Accuracy 0.4580078125\n",
      "Iteration 66850 Training loss 0.05213687941431999 Validation loss 0.0529232993721962 Accuracy 0.461181640625\n",
      "Iteration 66860 Training loss 0.04940037429332733 Validation loss 0.05322324484586716 Accuracy 0.460205078125\n",
      "Iteration 66870 Training loss 0.05334216356277466 Validation loss 0.05336781218647957 Accuracy 0.458251953125\n",
      "Iteration 66880 Training loss 0.05044189840555191 Validation loss 0.05301268398761749 Accuracy 0.46044921875\n",
      "Iteration 66890 Training loss 0.05179959163069725 Validation loss 0.05305826663970947 Accuracy 0.46044921875\n",
      "Iteration 66900 Training loss 0.05003849044442177 Validation loss 0.05329304561018944 Accuracy 0.458984375\n",
      "Iteration 66910 Training loss 0.0537264309823513 Validation loss 0.053205184638500214 Accuracy 0.458251953125\n",
      "Iteration 66920 Training loss 0.05104425176978111 Validation loss 0.05317112058401108 Accuracy 0.4599609375\n",
      "Iteration 66930 Training loss 0.05151861160993576 Validation loss 0.05305857211351395 Accuracy 0.460205078125\n",
      "Iteration 66940 Training loss 0.0513109527528286 Validation loss 0.053445231169462204 Accuracy 0.4580078125\n",
      "Iteration 66950 Training loss 0.05485031008720398 Validation loss 0.05343399941921234 Accuracy 0.45751953125\n",
      "Iteration 66960 Training loss 0.051001232117414474 Validation loss 0.053540680557489395 Accuracy 0.45751953125\n",
      "Iteration 66970 Training loss 0.05207303166389465 Validation loss 0.053967952728271484 Accuracy 0.454345703125\n",
      "Iteration 66980 Training loss 0.05509694293141365 Validation loss 0.0535554438829422 Accuracy 0.456787109375\n",
      "Iteration 66990 Training loss 0.05506255477666855 Validation loss 0.05322602391242981 Accuracy 0.460205078125\n",
      "Iteration 67000 Training loss 0.05213358625769615 Validation loss 0.05302618071436882 Accuracy 0.459716796875\n",
      "Iteration 67010 Training loss 0.04837658628821373 Validation loss 0.0534290187060833 Accuracy 0.4580078125\n",
      "Iteration 67020 Training loss 0.048778045922517776 Validation loss 0.053189970552921295 Accuracy 0.459716796875\n",
      "Iteration 67030 Training loss 0.05069488286972046 Validation loss 0.05312531068921089 Accuracy 0.4609375\n",
      "Iteration 67040 Training loss 0.052826691418886185 Validation loss 0.053412362933158875 Accuracy 0.4599609375\n",
      "Iteration 67050 Training loss 0.05108340457081795 Validation loss 0.053296640515327454 Accuracy 0.459716796875\n",
      "Iteration 67060 Training loss 0.05167008563876152 Validation loss 0.05322348326444626 Accuracy 0.45947265625\n",
      "Iteration 67070 Training loss 0.04837663844227791 Validation loss 0.053449541330337524 Accuracy 0.4560546875\n",
      "Iteration 67080 Training loss 0.04945341497659683 Validation loss 0.05289731174707413 Accuracy 0.4609375\n",
      "Iteration 67090 Training loss 0.050729915499687195 Validation loss 0.05275348946452141 Accuracy 0.4619140625\n",
      "Iteration 67100 Training loss 0.05374063178896904 Validation loss 0.053141869604587555 Accuracy 0.460693359375\n",
      "Iteration 67110 Training loss 0.05211819335818291 Validation loss 0.052959948778152466 Accuracy 0.4609375\n",
      "Iteration 67120 Training loss 0.04999290779232979 Validation loss 0.0528179295361042 Accuracy 0.4619140625\n",
      "Iteration 67130 Training loss 0.051786672323942184 Validation loss 0.05318906158208847 Accuracy 0.4599609375\n",
      "Iteration 67140 Training loss 0.05274920538067818 Validation loss 0.05339790880680084 Accuracy 0.457763671875\n",
      "Iteration 67150 Training loss 0.05080506205558777 Validation loss 0.053051307797431946 Accuracy 0.4599609375\n",
      "Iteration 67160 Training loss 0.05054642632603645 Validation loss 0.05314633995294571 Accuracy 0.459716796875\n",
      "Iteration 67170 Training loss 0.048719096928834915 Validation loss 0.053283385932445526 Accuracy 0.460205078125\n",
      "Iteration 67180 Training loss 0.0487503856420517 Validation loss 0.05292950198054314 Accuracy 0.461181640625\n",
      "Iteration 67190 Training loss 0.0498056523501873 Validation loss 0.05335603281855583 Accuracy 0.457763671875\n",
      "Iteration 67200 Training loss 0.05094883218407631 Validation loss 0.05299798771739006 Accuracy 0.461181640625\n",
      "Iteration 67210 Training loss 0.05117437615990639 Validation loss 0.053010281175374985 Accuracy 0.46044921875\n",
      "Iteration 67220 Training loss 0.04832299426198006 Validation loss 0.05301623046398163 Accuracy 0.4619140625\n",
      "Iteration 67230 Training loss 0.053228139877319336 Validation loss 0.053344450891017914 Accuracy 0.45947265625\n",
      "Iteration 67240 Training loss 0.05182570219039917 Validation loss 0.05291685461997986 Accuracy 0.4599609375\n",
      "Iteration 67250 Training loss 0.05310934782028198 Validation loss 0.053041838109493256 Accuracy 0.460693359375\n",
      "Iteration 67260 Training loss 0.05294915661215782 Validation loss 0.052892982959747314 Accuracy 0.4619140625\n",
      "Iteration 67270 Training loss 0.05110390856862068 Validation loss 0.05306654050946236 Accuracy 0.460205078125\n",
      "Iteration 67280 Training loss 0.05432058498263359 Validation loss 0.05352051183581352 Accuracy 0.458251953125\n",
      "Iteration 67290 Training loss 0.05269454047083855 Validation loss 0.05295787379145622 Accuracy 0.461181640625\n",
      "Iteration 67300 Training loss 0.054691415280103683 Validation loss 0.05325805023312569 Accuracy 0.45849609375\n",
      "Iteration 67310 Training loss 0.052251871675252914 Validation loss 0.05290832743048668 Accuracy 0.460205078125\n",
      "Iteration 67320 Training loss 0.04810326173901558 Validation loss 0.052766889333724976 Accuracy 0.4619140625\n",
      "Iteration 67330 Training loss 0.050030238926410675 Validation loss 0.05291422829031944 Accuracy 0.4609375\n",
      "Iteration 67340 Training loss 0.04922246187925339 Validation loss 0.05312684550881386 Accuracy 0.458740234375\n",
      "Iteration 67350 Training loss 0.04965605214238167 Validation loss 0.05294215679168701 Accuracy 0.459716796875\n",
      "Iteration 67360 Training loss 0.05339200422167778 Validation loss 0.053412653505802155 Accuracy 0.456787109375\n",
      "Iteration 67370 Training loss 0.05155017971992493 Validation loss 0.05341942235827446 Accuracy 0.456787109375\n",
      "Iteration 67380 Training loss 0.05189962685108185 Validation loss 0.05336441099643707 Accuracy 0.45751953125\n",
      "Iteration 67390 Training loss 0.052897363901138306 Validation loss 0.053017400205135345 Accuracy 0.459716796875\n",
      "Iteration 67400 Training loss 0.05056704208254814 Validation loss 0.053214333951473236 Accuracy 0.45947265625\n",
      "Iteration 67410 Training loss 0.053303904831409454 Validation loss 0.053081903606653214 Accuracy 0.460693359375\n",
      "Iteration 67420 Training loss 0.051990408450365067 Validation loss 0.05324200540781021 Accuracy 0.4609375\n",
      "Iteration 67430 Training loss 0.048468515276908875 Validation loss 0.05355872958898544 Accuracy 0.455078125\n",
      "Iteration 67440 Training loss 0.04922538250684738 Validation loss 0.05316588282585144 Accuracy 0.458740234375\n",
      "Iteration 67450 Training loss 0.05286883935332298 Validation loss 0.05321560800075531 Accuracy 0.459716796875\n",
      "Iteration 67460 Training loss 0.04997973516583443 Validation loss 0.05306620895862579 Accuracy 0.4599609375\n",
      "Iteration 67470 Training loss 0.05006645992398262 Validation loss 0.053037263453006744 Accuracy 0.460693359375\n",
      "Iteration 67480 Training loss 0.05002971366047859 Validation loss 0.05310380831360817 Accuracy 0.4599609375\n",
      "Iteration 67490 Training loss 0.05137624964118004 Validation loss 0.05299351364374161 Accuracy 0.45947265625\n",
      "Iteration 67500 Training loss 0.050303537398576736 Validation loss 0.05306321755051613 Accuracy 0.460693359375\n",
      "Iteration 67510 Training loss 0.05139097943902016 Validation loss 0.05316539108753204 Accuracy 0.458984375\n",
      "Iteration 67520 Training loss 0.04968048632144928 Validation loss 0.05314485356211662 Accuracy 0.4619140625\n",
      "Iteration 67530 Training loss 0.05081122741103172 Validation loss 0.053170379251241684 Accuracy 0.459716796875\n",
      "Iteration 67540 Training loss 0.04860217124223709 Validation loss 0.05345922335982323 Accuracy 0.45849609375\n",
      "Iteration 67550 Training loss 0.054274749010801315 Validation loss 0.05317610129714012 Accuracy 0.459716796875\n",
      "Iteration 67560 Training loss 0.05229572579264641 Validation loss 0.053095392882823944 Accuracy 0.460693359375\n",
      "Iteration 67570 Training loss 0.0506514273583889 Validation loss 0.053007934242486954 Accuracy 0.46044921875\n",
      "Iteration 67580 Training loss 0.04937975853681564 Validation loss 0.053480036556720734 Accuracy 0.45947265625\n",
      "Iteration 67590 Training loss 0.05294063314795494 Validation loss 0.05358650162816048 Accuracy 0.457275390625\n",
      "Iteration 67600 Training loss 0.05304039269685745 Validation loss 0.053313225507736206 Accuracy 0.458984375\n",
      "Iteration 67610 Training loss 0.04957594722509384 Validation loss 0.05384630709886551 Accuracy 0.455078125\n",
      "Iteration 67620 Training loss 0.05052381381392479 Validation loss 0.05315416306257248 Accuracy 0.460693359375\n",
      "Iteration 67630 Training loss 0.0520210936665535 Validation loss 0.05321899428963661 Accuracy 0.459716796875\n",
      "Iteration 67640 Training loss 0.050213102251291275 Validation loss 0.05309323966503143 Accuracy 0.4599609375\n",
      "Iteration 67650 Training loss 0.05216395854949951 Validation loss 0.05305448919534683 Accuracy 0.460693359375\n",
      "Iteration 67660 Training loss 0.04989948496222496 Validation loss 0.0530431792140007 Accuracy 0.460205078125\n",
      "Iteration 67670 Training loss 0.0493272989988327 Validation loss 0.053590428084135056 Accuracy 0.457275390625\n",
      "Iteration 67680 Training loss 0.05056395009160042 Validation loss 0.05296942591667175 Accuracy 0.459716796875\n",
      "Iteration 67690 Training loss 0.049297865480184555 Validation loss 0.05288868397474289 Accuracy 0.46142578125\n",
      "Iteration 67700 Training loss 0.05110352486371994 Validation loss 0.05306653305888176 Accuracy 0.4609375\n",
      "Iteration 67710 Training loss 0.04905929043889046 Validation loss 0.05297023802995682 Accuracy 0.459228515625\n",
      "Iteration 67720 Training loss 0.05256184935569763 Validation loss 0.05341791734099388 Accuracy 0.4580078125\n",
      "Iteration 67730 Training loss 0.05164099112153053 Validation loss 0.05352144315838814 Accuracy 0.4580078125\n",
      "Iteration 67740 Training loss 0.04937257990241051 Validation loss 0.0530676543712616 Accuracy 0.4599609375\n",
      "Iteration 67750 Training loss 0.047217171639204025 Validation loss 0.0530315600335598 Accuracy 0.46142578125\n",
      "Iteration 67760 Training loss 0.04969673603773117 Validation loss 0.053469471633434296 Accuracy 0.458984375\n",
      "Iteration 67770 Training loss 0.04932874068617821 Validation loss 0.05299452319741249 Accuracy 0.4599609375\n",
      "Iteration 67780 Training loss 0.05137907713651657 Validation loss 0.053818803280591965 Accuracy 0.4541015625\n",
      "Iteration 67790 Training loss 0.05000259727239609 Validation loss 0.053648367524147034 Accuracy 0.4580078125\n",
      "Iteration 67800 Training loss 0.050137460231781006 Validation loss 0.05316602438688278 Accuracy 0.46044921875\n",
      "Iteration 67810 Training loss 0.05541611835360527 Validation loss 0.053398698568344116 Accuracy 0.459716796875\n",
      "Iteration 67820 Training loss 0.05112925171852112 Validation loss 0.05277566239237785 Accuracy 0.462158203125\n",
      "Iteration 67830 Training loss 0.04783722013235092 Validation loss 0.05307763069868088 Accuracy 0.459228515625\n",
      "Iteration 67840 Training loss 0.05100692808628082 Validation loss 0.053010813891887665 Accuracy 0.459716796875\n",
      "Iteration 67850 Training loss 0.051145732402801514 Validation loss 0.053022049367427826 Accuracy 0.4609375\n",
      "Iteration 67860 Training loss 0.04663509130477905 Validation loss 0.05309051275253296 Accuracy 0.46044921875\n",
      "Iteration 67870 Training loss 0.05128363147377968 Validation loss 0.05323859304189682 Accuracy 0.46044921875\n",
      "Iteration 67880 Training loss 0.047160279005765915 Validation loss 0.05313907936215401 Accuracy 0.458251953125\n",
      "Iteration 67890 Training loss 0.0538240447640419 Validation loss 0.05333932861685753 Accuracy 0.457275390625\n",
      "Iteration 67900 Training loss 0.05431952700018883 Validation loss 0.052839260548353195 Accuracy 0.461181640625\n",
      "Iteration 67910 Training loss 0.05165215954184532 Validation loss 0.05326518043875694 Accuracy 0.45849609375\n",
      "Iteration 67920 Training loss 0.04904697462916374 Validation loss 0.0529722161591053 Accuracy 0.458984375\n",
      "Iteration 67930 Training loss 0.049791086465120316 Validation loss 0.05341664329171181 Accuracy 0.45849609375\n",
      "Iteration 67940 Training loss 0.0526549369096756 Validation loss 0.05291563272476196 Accuracy 0.46044921875\n",
      "Iteration 67950 Training loss 0.05116994306445122 Validation loss 0.05338776111602783 Accuracy 0.45703125\n",
      "Iteration 67960 Training loss 0.05507948622107506 Validation loss 0.05283660814166069 Accuracy 0.461669921875\n",
      "Iteration 67970 Training loss 0.051501017063856125 Validation loss 0.05332780256867409 Accuracy 0.458740234375\n",
      "Iteration 67980 Training loss 0.05073132738471031 Validation loss 0.05319015309214592 Accuracy 0.458984375\n",
      "Iteration 67990 Training loss 0.0535992830991745 Validation loss 0.05295151099562645 Accuracy 0.460693359375\n",
      "Iteration 68000 Training loss 0.054224103689193726 Validation loss 0.05328194797039032 Accuracy 0.458984375\n",
      "Iteration 68010 Training loss 0.05462006479501724 Validation loss 0.05319029092788696 Accuracy 0.459716796875\n",
      "Iteration 68020 Training loss 0.048407409340143204 Validation loss 0.0529402419924736 Accuracy 0.460693359375\n",
      "Iteration 68030 Training loss 0.05160166695713997 Validation loss 0.05377478525042534 Accuracy 0.4541015625\n",
      "Iteration 68040 Training loss 0.05075832083821297 Validation loss 0.053255345672369 Accuracy 0.4599609375\n",
      "Iteration 68050 Training loss 0.04912761226296425 Validation loss 0.05297466740012169 Accuracy 0.460693359375\n",
      "Iteration 68060 Training loss 0.05006060749292374 Validation loss 0.0531674288213253 Accuracy 0.459228515625\n",
      "Iteration 68070 Training loss 0.053063105791807175 Validation loss 0.0529937781393528 Accuracy 0.45947265625\n",
      "Iteration 68080 Training loss 0.049447864294052124 Validation loss 0.05312354862689972 Accuracy 0.45849609375\n",
      "Iteration 68090 Training loss 0.051519062370061874 Validation loss 0.053144749253988266 Accuracy 0.4599609375\n",
      "Iteration 68100 Training loss 0.048383139073848724 Validation loss 0.05330470949411392 Accuracy 0.456787109375\n",
      "Iteration 68110 Training loss 0.05011656507849693 Validation loss 0.0530054047703743 Accuracy 0.460205078125\n",
      "Iteration 68120 Training loss 0.04990324005484581 Validation loss 0.053123436868190765 Accuracy 0.460205078125\n",
      "Iteration 68130 Training loss 0.051223259419202805 Validation loss 0.05330271273851395 Accuracy 0.458740234375\n",
      "Iteration 68140 Training loss 0.04953679442405701 Validation loss 0.05310378596186638 Accuracy 0.459716796875\n",
      "Iteration 68150 Training loss 0.05354728177189827 Validation loss 0.05418913811445236 Accuracy 0.450927734375\n",
      "Iteration 68160 Training loss 0.05250496417284012 Validation loss 0.0534113273024559 Accuracy 0.45654296875\n",
      "Iteration 68170 Training loss 0.049807943403720856 Validation loss 0.05283481255173683 Accuracy 0.461669921875\n",
      "Iteration 68180 Training loss 0.052421946078538895 Validation loss 0.053219277411699295 Accuracy 0.457763671875\n",
      "Iteration 68190 Training loss 0.05284322425723076 Validation loss 0.05308188498020172 Accuracy 0.458984375\n",
      "Iteration 68200 Training loss 0.050772614777088165 Validation loss 0.05300343036651611 Accuracy 0.459716796875\n",
      "Iteration 68210 Training loss 0.0553126223385334 Validation loss 0.05332109332084656 Accuracy 0.45947265625\n",
      "Iteration 68220 Training loss 0.04995693266391754 Validation loss 0.05314233899116516 Accuracy 0.4609375\n",
      "Iteration 68230 Training loss 0.05097080394625664 Validation loss 0.05312827602028847 Accuracy 0.461181640625\n",
      "Iteration 68240 Training loss 0.051097217947244644 Validation loss 0.0531938411295414 Accuracy 0.46044921875\n",
      "Iteration 68250 Training loss 0.049547575414180756 Validation loss 0.05311896279454231 Accuracy 0.460205078125\n",
      "Iteration 68260 Training loss 0.053567200899124146 Validation loss 0.05304529517889023 Accuracy 0.460693359375\n",
      "Iteration 68270 Training loss 0.04944661259651184 Validation loss 0.053205545991659164 Accuracy 0.4599609375\n",
      "Iteration 68280 Training loss 0.05526498705148697 Validation loss 0.05353879928588867 Accuracy 0.4560546875\n",
      "Iteration 68290 Training loss 0.05025375261902809 Validation loss 0.05302882567048073 Accuracy 0.460693359375\n",
      "Iteration 68300 Training loss 0.04808191955089569 Validation loss 0.05296970531344414 Accuracy 0.460205078125\n",
      "Iteration 68310 Training loss 0.04945097863674164 Validation loss 0.05332111194729805 Accuracy 0.459228515625\n",
      "Iteration 68320 Training loss 0.05235128849744797 Validation loss 0.05321888625621796 Accuracy 0.4599609375\n",
      "Iteration 68330 Training loss 0.045893117785453796 Validation loss 0.053037311881780624 Accuracy 0.4609375\n",
      "Iteration 68340 Training loss 0.05286741256713867 Validation loss 0.05328694358468056 Accuracy 0.459716796875\n",
      "Iteration 68350 Training loss 0.05151945352554321 Validation loss 0.05316880717873573 Accuracy 0.46044921875\n",
      "Iteration 68360 Training loss 0.05235857143998146 Validation loss 0.053231559693813324 Accuracy 0.459716796875\n",
      "Iteration 68370 Training loss 0.04867618903517723 Validation loss 0.05290771275758743 Accuracy 0.4609375\n",
      "Iteration 68380 Training loss 0.04702095314860344 Validation loss 0.053233202546834946 Accuracy 0.459716796875\n",
      "Iteration 68390 Training loss 0.05021306127309799 Validation loss 0.053895723074674606 Accuracy 0.453125\n",
      "Iteration 68400 Training loss 0.04554338380694389 Validation loss 0.05322177708148956 Accuracy 0.45849609375\n",
      "Iteration 68410 Training loss 0.05420640856027603 Validation loss 0.05342130735516548 Accuracy 0.45947265625\n",
      "Iteration 68420 Training loss 0.04912744089961052 Validation loss 0.05365733802318573 Accuracy 0.4560546875\n",
      "Iteration 68430 Training loss 0.05410465598106384 Validation loss 0.05313650146126747 Accuracy 0.458984375\n",
      "Iteration 68440 Training loss 0.04939214512705803 Validation loss 0.05318192392587662 Accuracy 0.45947265625\n",
      "Iteration 68450 Training loss 0.052011631429195404 Validation loss 0.05323184281587601 Accuracy 0.459716796875\n",
      "Iteration 68460 Training loss 0.04992303252220154 Validation loss 0.05332695320248604 Accuracy 0.459716796875\n",
      "Iteration 68470 Training loss 0.05275840312242508 Validation loss 0.05304515361785889 Accuracy 0.459716796875\n",
      "Iteration 68480 Training loss 0.05189354345202446 Validation loss 0.053629070520401 Accuracy 0.45654296875\n",
      "Iteration 68490 Training loss 0.05137091875076294 Validation loss 0.05321577191352844 Accuracy 0.45849609375\n",
      "Iteration 68500 Training loss 0.048572614789009094 Validation loss 0.05292605981230736 Accuracy 0.460693359375\n",
      "Iteration 68510 Training loss 0.056056778877973557 Validation loss 0.052924931049346924 Accuracy 0.461181640625\n",
      "Iteration 68520 Training loss 0.048580992966890335 Validation loss 0.05372633785009384 Accuracy 0.456298828125\n",
      "Iteration 68530 Training loss 0.05053652822971344 Validation loss 0.05346295237541199 Accuracy 0.45703125\n",
      "Iteration 68540 Training loss 0.05180971696972847 Validation loss 0.05298919603228569 Accuracy 0.4599609375\n",
      "Iteration 68550 Training loss 0.047684021294116974 Validation loss 0.05316987261176109 Accuracy 0.460693359375\n",
      "Iteration 68560 Training loss 0.05336703360080719 Validation loss 0.05316835641860962 Accuracy 0.460205078125\n",
      "Iteration 68570 Training loss 0.04920284077525139 Validation loss 0.05308646336197853 Accuracy 0.4599609375\n",
      "Iteration 68580 Training loss 0.05155160650610924 Validation loss 0.05301562324166298 Accuracy 0.46142578125\n",
      "Iteration 68590 Training loss 0.04902750998735428 Validation loss 0.05337715521454811 Accuracy 0.458740234375\n",
      "Iteration 68600 Training loss 0.054378703236579895 Validation loss 0.053485047072172165 Accuracy 0.45703125\n",
      "Iteration 68610 Training loss 0.052014321088790894 Validation loss 0.05316244810819626 Accuracy 0.458984375\n",
      "Iteration 68620 Training loss 0.056784749031066895 Validation loss 0.0530085526406765 Accuracy 0.4609375\n",
      "Iteration 68630 Training loss 0.05349605157971382 Validation loss 0.053046103566884995 Accuracy 0.459716796875\n",
      "Iteration 68640 Training loss 0.04708606004714966 Validation loss 0.053165681660175323 Accuracy 0.460205078125\n",
      "Iteration 68650 Training loss 0.049905817955732346 Validation loss 0.05312613025307655 Accuracy 0.460693359375\n",
      "Iteration 68660 Training loss 0.050320595502853394 Validation loss 0.052957259118556976 Accuracy 0.46044921875\n",
      "Iteration 68670 Training loss 0.05238728225231171 Validation loss 0.053072117269039154 Accuracy 0.460693359375\n",
      "Iteration 68680 Training loss 0.0516471229493618 Validation loss 0.053312573581933975 Accuracy 0.457275390625\n",
      "Iteration 68690 Training loss 0.05269790440797806 Validation loss 0.053145892918109894 Accuracy 0.460205078125\n",
      "Iteration 68700 Training loss 0.0497494712471962 Validation loss 0.05343038588762283 Accuracy 0.45849609375\n",
      "Iteration 68710 Training loss 0.0499265231192112 Validation loss 0.053393974900245667 Accuracy 0.456298828125\n",
      "Iteration 68720 Training loss 0.05265726149082184 Validation loss 0.05280786007642746 Accuracy 0.46142578125\n",
      "Iteration 68730 Training loss 0.05105727165937424 Validation loss 0.053331952542066574 Accuracy 0.45849609375\n",
      "Iteration 68740 Training loss 0.0494365431368351 Validation loss 0.05334492027759552 Accuracy 0.46044921875\n",
      "Iteration 68750 Training loss 0.04754875972867012 Validation loss 0.05304428189992905 Accuracy 0.460205078125\n",
      "Iteration 68760 Training loss 0.05083832889795303 Validation loss 0.05358933284878731 Accuracy 0.45556640625\n",
      "Iteration 68770 Training loss 0.052367374300956726 Validation loss 0.053112439811229706 Accuracy 0.4609375\n",
      "Iteration 68780 Training loss 0.047521449625492096 Validation loss 0.05333777144551277 Accuracy 0.45849609375\n",
      "Iteration 68790 Training loss 0.049671173095703125 Validation loss 0.05349736288189888 Accuracy 0.4580078125\n",
      "Iteration 68800 Training loss 0.04958033189177513 Validation loss 0.05334751680493355 Accuracy 0.457763671875\n",
      "Iteration 68810 Training loss 0.05044341832399368 Validation loss 0.05317053943872452 Accuracy 0.459228515625\n",
      "Iteration 68820 Training loss 0.05233711004257202 Validation loss 0.05358792841434479 Accuracy 0.45556640625\n",
      "Iteration 68830 Training loss 0.04935303330421448 Validation loss 0.0533168651163578 Accuracy 0.45849609375\n",
      "Iteration 68840 Training loss 0.05015196651220322 Validation loss 0.0531187579035759 Accuracy 0.460205078125\n",
      "Iteration 68850 Training loss 0.05236426368355751 Validation loss 0.05329843610525131 Accuracy 0.45849609375\n",
      "Iteration 68860 Training loss 0.04955977201461792 Validation loss 0.053490594029426575 Accuracy 0.45849609375\n",
      "Iteration 68870 Training loss 0.05141202360391617 Validation loss 0.052946120500564575 Accuracy 0.46044921875\n",
      "Iteration 68880 Training loss 0.051033392548561096 Validation loss 0.052970752120018005 Accuracy 0.46044921875\n",
      "Iteration 68890 Training loss 0.05125414952635765 Validation loss 0.05287869647145271 Accuracy 0.461181640625\n",
      "Iteration 68900 Training loss 0.050414055585861206 Validation loss 0.05290444195270538 Accuracy 0.460693359375\n",
      "Iteration 68910 Training loss 0.050306353718042374 Validation loss 0.05307848006486893 Accuracy 0.459228515625\n",
      "Iteration 68920 Training loss 0.051555562764406204 Validation loss 0.05320331081748009 Accuracy 0.45849609375\n",
      "Iteration 68930 Training loss 0.05209285393357277 Validation loss 0.05304202437400818 Accuracy 0.4599609375\n",
      "Iteration 68940 Training loss 0.05381420999765396 Validation loss 0.0532427616417408 Accuracy 0.45947265625\n",
      "Iteration 68950 Training loss 0.053855400532484055 Validation loss 0.05290395766496658 Accuracy 0.4619140625\n",
      "Iteration 68960 Training loss 0.050646912306547165 Validation loss 0.05303781107068062 Accuracy 0.460205078125\n",
      "Iteration 68970 Training loss 0.049567319452762604 Validation loss 0.05294221639633179 Accuracy 0.459716796875\n",
      "Iteration 68980 Training loss 0.05257239565253258 Validation loss 0.053059156984090805 Accuracy 0.460693359375\n",
      "Iteration 68990 Training loss 0.052368879318237305 Validation loss 0.05330164358019829 Accuracy 0.45947265625\n",
      "Iteration 69000 Training loss 0.04951023682951927 Validation loss 0.05315744876861572 Accuracy 0.459716796875\n",
      "Iteration 69010 Training loss 0.05148160830140114 Validation loss 0.053299739956855774 Accuracy 0.458740234375\n",
      "Iteration 69020 Training loss 0.0502496212720871 Validation loss 0.05303673818707466 Accuracy 0.461181640625\n",
      "Iteration 69030 Training loss 0.05388392135500908 Validation loss 0.05330750718712807 Accuracy 0.458251953125\n",
      "Iteration 69040 Training loss 0.050667326897382736 Validation loss 0.053679775446653366 Accuracy 0.456298828125\n",
      "Iteration 69050 Training loss 0.050744444131851196 Validation loss 0.05305435508489609 Accuracy 0.45947265625\n",
      "Iteration 69060 Training loss 0.05142974108457565 Validation loss 0.05298392474651337 Accuracy 0.461181640625\n",
      "Iteration 69070 Training loss 0.050557371228933334 Validation loss 0.052980661392211914 Accuracy 0.46044921875\n",
      "Iteration 69080 Training loss 0.04933615028858185 Validation loss 0.052932899445295334 Accuracy 0.46044921875\n",
      "Iteration 69090 Training loss 0.050022613257169724 Validation loss 0.05326421186327934 Accuracy 0.45849609375\n",
      "Iteration 69100 Training loss 0.0488111786544323 Validation loss 0.05305224657058716 Accuracy 0.4599609375\n",
      "Iteration 69110 Training loss 0.0524764247238636 Validation loss 0.05318279564380646 Accuracy 0.458984375\n",
      "Iteration 69120 Training loss 0.04864748194813728 Validation loss 0.05332079902291298 Accuracy 0.4580078125\n",
      "Iteration 69130 Training loss 0.05182536691427231 Validation loss 0.05305098369717598 Accuracy 0.4599609375\n",
      "Iteration 69140 Training loss 0.051351796835660934 Validation loss 0.05292930454015732 Accuracy 0.460205078125\n",
      "Iteration 69150 Training loss 0.04903949424624443 Validation loss 0.053289420902729034 Accuracy 0.45751953125\n",
      "Iteration 69160 Training loss 0.050795428454875946 Validation loss 0.053243644535541534 Accuracy 0.460205078125\n",
      "Iteration 69170 Training loss 0.05137982964515686 Validation loss 0.05318305641412735 Accuracy 0.458984375\n",
      "Iteration 69180 Training loss 0.05142280459403992 Validation loss 0.053388811647892 Accuracy 0.460205078125\n",
      "Iteration 69190 Training loss 0.048575326800346375 Validation loss 0.05306196212768555 Accuracy 0.460693359375\n",
      "Iteration 69200 Training loss 0.05039677768945694 Validation loss 0.0532967671751976 Accuracy 0.4580078125\n",
      "Iteration 69210 Training loss 0.04940899461507797 Validation loss 0.05287181958556175 Accuracy 0.461181640625\n",
      "Iteration 69220 Training loss 0.051928307861089706 Validation loss 0.05331457033753395 Accuracy 0.45849609375\n",
      "Iteration 69230 Training loss 0.049817007035017014 Validation loss 0.05300164595246315 Accuracy 0.4599609375\n",
      "Iteration 69240 Training loss 0.05171234533190727 Validation loss 0.05317803472280502 Accuracy 0.45849609375\n",
      "Iteration 69250 Training loss 0.051522668451070786 Validation loss 0.053266238421201706 Accuracy 0.458984375\n",
      "Iteration 69260 Training loss 0.04891686886548996 Validation loss 0.0530855655670166 Accuracy 0.46044921875\n",
      "Iteration 69270 Training loss 0.05093672126531601 Validation loss 0.0531281903386116 Accuracy 0.460693359375\n",
      "Iteration 69280 Training loss 0.050397034734487534 Validation loss 0.05356745794415474 Accuracy 0.45849609375\n",
      "Iteration 69290 Training loss 0.048359669744968414 Validation loss 0.052879784256219864 Accuracy 0.461669921875\n",
      "Iteration 69300 Training loss 0.05014197155833244 Validation loss 0.05341397225856781 Accuracy 0.45751953125\n",
      "Iteration 69310 Training loss 0.0533769465982914 Validation loss 0.05317360535264015 Accuracy 0.45947265625\n",
      "Iteration 69320 Training loss 0.05251722037792206 Validation loss 0.052974529564380646 Accuracy 0.461181640625\n",
      "Iteration 69330 Training loss 0.05078433081507683 Validation loss 0.05304481461644173 Accuracy 0.460693359375\n",
      "Iteration 69340 Training loss 0.04901980981230736 Validation loss 0.05319700017571449 Accuracy 0.460693359375\n",
      "Iteration 69350 Training loss 0.04775794595479965 Validation loss 0.053140003234148026 Accuracy 0.45849609375\n",
      "Iteration 69360 Training loss 0.0494278259575367 Validation loss 0.05316862836480141 Accuracy 0.458740234375\n",
      "Iteration 69370 Training loss 0.04694435000419617 Validation loss 0.05308014526963234 Accuracy 0.459716796875\n",
      "Iteration 69380 Training loss 0.05246657133102417 Validation loss 0.05308490991592407 Accuracy 0.459716796875\n",
      "Iteration 69390 Training loss 0.04898752644658089 Validation loss 0.05290796235203743 Accuracy 0.460205078125\n",
      "Iteration 69400 Training loss 0.055247195065021515 Validation loss 0.05353127792477608 Accuracy 0.45703125\n",
      "Iteration 69410 Training loss 0.05178080126643181 Validation loss 0.05307137593626976 Accuracy 0.458984375\n",
      "Iteration 69420 Training loss 0.04900207370519638 Validation loss 0.05345383659005165 Accuracy 0.456787109375\n",
      "Iteration 69430 Training loss 0.05258817598223686 Validation loss 0.05333409458398819 Accuracy 0.46044921875\n",
      "Iteration 69440 Training loss 0.05417299270629883 Validation loss 0.053348712623119354 Accuracy 0.45703125\n",
      "Iteration 69450 Training loss 0.053714074194431305 Validation loss 0.0532892607152462 Accuracy 0.457763671875\n",
      "Iteration 69460 Training loss 0.05191729590296745 Validation loss 0.05325642600655556 Accuracy 0.459228515625\n",
      "Iteration 69470 Training loss 0.05573946237564087 Validation loss 0.05328664928674698 Accuracy 0.458984375\n",
      "Iteration 69480 Training loss 0.05031359940767288 Validation loss 0.05302996560931206 Accuracy 0.461181640625\n",
      "Iteration 69490 Training loss 0.05061660706996918 Validation loss 0.05302468687295914 Accuracy 0.4619140625\n",
      "Iteration 69500 Training loss 0.05169353261590004 Validation loss 0.05368991196155548 Accuracy 0.456298828125\n",
      "Iteration 69510 Training loss 0.05493635684251785 Validation loss 0.05352827161550522 Accuracy 0.45703125\n",
      "Iteration 69520 Training loss 0.05111779645085335 Validation loss 0.05324704945087433 Accuracy 0.459228515625\n",
      "Iteration 69530 Training loss 0.04970915615558624 Validation loss 0.05342772975564003 Accuracy 0.459716796875\n",
      "Iteration 69540 Training loss 0.04898686334490776 Validation loss 0.05307945981621742 Accuracy 0.459716796875\n",
      "Iteration 69550 Training loss 0.05348777025938034 Validation loss 0.05306853726506233 Accuracy 0.459716796875\n",
      "Iteration 69560 Training loss 0.04863959550857544 Validation loss 0.0535736009478569 Accuracy 0.457275390625\n",
      "Iteration 69570 Training loss 0.05306589603424072 Validation loss 0.053584836423397064 Accuracy 0.458251953125\n",
      "Iteration 69580 Training loss 0.05275356024503708 Validation loss 0.05312296748161316 Accuracy 0.46044921875\n",
      "Iteration 69590 Training loss 0.04742252081632614 Validation loss 0.05303305387496948 Accuracy 0.460205078125\n",
      "Iteration 69600 Training loss 0.051069606095552444 Validation loss 0.05368388071656227 Accuracy 0.455078125\n",
      "Iteration 69610 Training loss 0.05436746031045914 Validation loss 0.05313098803162575 Accuracy 0.45947265625\n",
      "Iteration 69620 Training loss 0.05215150862932205 Validation loss 0.05347348377108574 Accuracy 0.456787109375\n",
      "Iteration 69630 Training loss 0.05166671797633171 Validation loss 0.053053583949804306 Accuracy 0.460205078125\n",
      "Iteration 69640 Training loss 0.04994781315326691 Validation loss 0.05290944129228592 Accuracy 0.46044921875\n",
      "Iteration 69650 Training loss 0.05152473226189613 Validation loss 0.05324329063296318 Accuracy 0.4599609375\n",
      "Iteration 69660 Training loss 0.050656288862228394 Validation loss 0.053125377744436264 Accuracy 0.46142578125\n",
      "Iteration 69670 Training loss 0.05121960863471031 Validation loss 0.05337078124284744 Accuracy 0.4580078125\n",
      "Iteration 69680 Training loss 0.05316460132598877 Validation loss 0.05477286875247955 Accuracy 0.44580078125\n",
      "Iteration 69690 Training loss 0.05342791602015495 Validation loss 0.053363386541604996 Accuracy 0.4580078125\n",
      "Iteration 69700 Training loss 0.04868239909410477 Validation loss 0.053154490888118744 Accuracy 0.458740234375\n",
      "Iteration 69710 Training loss 0.052722733467817307 Validation loss 0.05327958986163139 Accuracy 0.45849609375\n",
      "Iteration 69720 Training loss 0.049681954085826874 Validation loss 0.05316316708922386 Accuracy 0.46044921875\n",
      "Iteration 69730 Training loss 0.05240507051348686 Validation loss 0.053294528275728226 Accuracy 0.459716796875\n",
      "Iteration 69740 Training loss 0.05335930734872818 Validation loss 0.05300416052341461 Accuracy 0.461181640625\n",
      "Iteration 69750 Training loss 0.048475563526153564 Validation loss 0.053092289716005325 Accuracy 0.46044921875\n",
      "Iteration 69760 Training loss 0.048594918102025986 Validation loss 0.0529596172273159 Accuracy 0.4609375\n",
      "Iteration 69770 Training loss 0.052741847932338715 Validation loss 0.053415682166814804 Accuracy 0.457275390625\n",
      "Iteration 69780 Training loss 0.04919092729687691 Validation loss 0.053240444511175156 Accuracy 0.4599609375\n",
      "Iteration 69790 Training loss 0.04976256936788559 Validation loss 0.05333947762846947 Accuracy 0.458984375\n",
      "Iteration 69800 Training loss 0.050637099891901016 Validation loss 0.05322340503334999 Accuracy 0.459716796875\n",
      "Iteration 69810 Training loss 0.05069397762417793 Validation loss 0.05291750282049179 Accuracy 0.460693359375\n",
      "Iteration 69820 Training loss 0.04941289871931076 Validation loss 0.05302264541387558 Accuracy 0.460205078125\n",
      "Iteration 69830 Training loss 0.05374929681420326 Validation loss 0.053420741111040115 Accuracy 0.45849609375\n",
      "Iteration 69840 Training loss 0.048006050288677216 Validation loss 0.05297377333045006 Accuracy 0.460693359375\n",
      "Iteration 69850 Training loss 0.05203006789088249 Validation loss 0.052911609411239624 Accuracy 0.4609375\n",
      "Iteration 69860 Training loss 0.05113263800740242 Validation loss 0.05331690236926079 Accuracy 0.458984375\n",
      "Iteration 69870 Training loss 0.05153968930244446 Validation loss 0.053192850202322006 Accuracy 0.45849609375\n",
      "Iteration 69880 Training loss 0.050755422562360764 Validation loss 0.0530206561088562 Accuracy 0.46044921875\n",
      "Iteration 69890 Training loss 0.05205971375107765 Validation loss 0.05298491567373276 Accuracy 0.46142578125\n",
      "Iteration 69900 Training loss 0.05319811776280403 Validation loss 0.052836403250694275 Accuracy 0.46142578125\n",
      "Iteration 69910 Training loss 0.051749639213085175 Validation loss 0.05426964536309242 Accuracy 0.45068359375\n",
      "Iteration 69920 Training loss 0.05203879252076149 Validation loss 0.05319252237677574 Accuracy 0.45947265625\n",
      "Iteration 69930 Training loss 0.04921761155128479 Validation loss 0.05339795723557472 Accuracy 0.458740234375\n",
      "Iteration 69940 Training loss 0.04735954850912094 Validation loss 0.053152184933423996 Accuracy 0.45947265625\n",
      "Iteration 69950 Training loss 0.053156398236751556 Validation loss 0.05311281979084015 Accuracy 0.45947265625\n",
      "Iteration 69960 Training loss 0.051350004971027374 Validation loss 0.05385901406407356 Accuracy 0.453125\n",
      "Iteration 69970 Training loss 0.054658789187669754 Validation loss 0.053864285349845886 Accuracy 0.45361328125\n",
      "Iteration 69980 Training loss 0.05208966135978699 Validation loss 0.05321474373340607 Accuracy 0.4599609375\n",
      "Iteration 69990 Training loss 0.05460590496659279 Validation loss 0.05298458784818649 Accuracy 0.460205078125\n",
      "Iteration 70000 Training loss 0.05669257417321205 Validation loss 0.053533073514699936 Accuracy 0.45849609375\n",
      "Iteration 70010 Training loss 0.05143895000219345 Validation loss 0.05292443558573723 Accuracy 0.460205078125\n",
      "Iteration 70020 Training loss 0.05191105976700783 Validation loss 0.053145986050367355 Accuracy 0.460693359375\n",
      "Iteration 70030 Training loss 0.05463656038045883 Validation loss 0.05353718250989914 Accuracy 0.4541015625\n",
      "Iteration 70040 Training loss 0.04842313751578331 Validation loss 0.053043466061353683 Accuracy 0.45849609375\n",
      "Iteration 70050 Training loss 0.050827812403440475 Validation loss 0.053227946162223816 Accuracy 0.4599609375\n",
      "Iteration 70060 Training loss 0.05047748610377312 Validation loss 0.053209297358989716 Accuracy 0.459716796875\n",
      "Iteration 70070 Training loss 0.05052214488387108 Validation loss 0.053032126277685165 Accuracy 0.46044921875\n",
      "Iteration 70080 Training loss 0.051561370491981506 Validation loss 0.05343708023428917 Accuracy 0.458740234375\n",
      "Iteration 70090 Training loss 0.05332491919398308 Validation loss 0.052836354821920395 Accuracy 0.461669921875\n",
      "Iteration 70100 Training loss 0.05021211877465248 Validation loss 0.05301728472113609 Accuracy 0.460693359375\n",
      "Iteration 70110 Training loss 0.04687848687171936 Validation loss 0.05323884263634682 Accuracy 0.45947265625\n",
      "Iteration 70120 Training loss 0.056220442056655884 Validation loss 0.053592342883348465 Accuracy 0.456787109375\n",
      "Iteration 70130 Training loss 0.05079496279358864 Validation loss 0.053415555506944656 Accuracy 0.458251953125\n",
      "Iteration 70140 Training loss 0.05145256593823433 Validation loss 0.05347855016589165 Accuracy 0.457763671875\n",
      "Iteration 70150 Training loss 0.05175097659230232 Validation loss 0.05281480401754379 Accuracy 0.4619140625\n",
      "Iteration 70160 Training loss 0.04807177558541298 Validation loss 0.053464360535144806 Accuracy 0.458740234375\n",
      "Iteration 70170 Training loss 0.05315764248371124 Validation loss 0.053475771099328995 Accuracy 0.458984375\n",
      "Iteration 70180 Training loss 0.053337179124355316 Validation loss 0.05301965773105621 Accuracy 0.45947265625\n",
      "Iteration 70190 Training loss 0.0522962249815464 Validation loss 0.05299913138151169 Accuracy 0.46142578125\n",
      "Iteration 70200 Training loss 0.04884493350982666 Validation loss 0.05318337678909302 Accuracy 0.460205078125\n",
      "Iteration 70210 Training loss 0.05182547867298126 Validation loss 0.0535118542611599 Accuracy 0.458251953125\n",
      "Iteration 70220 Training loss 0.04799295961856842 Validation loss 0.05344897508621216 Accuracy 0.457763671875\n",
      "Iteration 70230 Training loss 0.05187693238258362 Validation loss 0.05295860394835472 Accuracy 0.46044921875\n",
      "Iteration 70240 Training loss 0.04956680163741112 Validation loss 0.052949901670217514 Accuracy 0.460205078125\n",
      "Iteration 70250 Training loss 0.05203499272465706 Validation loss 0.05320237576961517 Accuracy 0.458740234375\n",
      "Iteration 70260 Training loss 0.05094785615801811 Validation loss 0.05322056636214256 Accuracy 0.45849609375\n",
      "Iteration 70270 Training loss 0.0532645657658577 Validation loss 0.053126703947782516 Accuracy 0.458984375\n",
      "Iteration 70280 Training loss 0.048642050474882126 Validation loss 0.05290424823760986 Accuracy 0.4609375\n",
      "Iteration 70290 Training loss 0.05201735720038414 Validation loss 0.0537581741809845 Accuracy 0.454833984375\n",
      "Iteration 70300 Training loss 0.05033385753631592 Validation loss 0.05315552279353142 Accuracy 0.460205078125\n",
      "Iteration 70310 Training loss 0.05120813846588135 Validation loss 0.053812939673662186 Accuracy 0.4521484375\n",
      "Iteration 70320 Training loss 0.05257132276892662 Validation loss 0.053540632128715515 Accuracy 0.45703125\n",
      "Iteration 70330 Training loss 0.0560636892914772 Validation loss 0.053488630801439285 Accuracy 0.4580078125\n",
      "Iteration 70340 Training loss 0.04585688188672066 Validation loss 0.05286870896816254 Accuracy 0.461669921875\n",
      "Iteration 70350 Training loss 0.04994111508131027 Validation loss 0.05292664095759392 Accuracy 0.460205078125\n",
      "Iteration 70360 Training loss 0.04923269525170326 Validation loss 0.05338152125477791 Accuracy 0.45703125\n",
      "Iteration 70370 Training loss 0.05244963616132736 Validation loss 0.05386412888765335 Accuracy 0.452392578125\n",
      "Iteration 70380 Training loss 0.05242616683244705 Validation loss 0.05354950204491615 Accuracy 0.457763671875\n",
      "Iteration 70390 Training loss 0.046044137328863144 Validation loss 0.05371342599391937 Accuracy 0.456298828125\n",
      "Iteration 70400 Training loss 0.050245799124240875 Validation loss 0.05284026265144348 Accuracy 0.460693359375\n",
      "Iteration 70410 Training loss 0.05404497683048248 Validation loss 0.05331330746412277 Accuracy 0.4599609375\n",
      "Iteration 70420 Training loss 0.04764249548316002 Validation loss 0.05307244136929512 Accuracy 0.46044921875\n",
      "Iteration 70430 Training loss 0.05044253543019295 Validation loss 0.05312061309814453 Accuracy 0.45849609375\n",
      "Iteration 70440 Training loss 0.052168313413858414 Validation loss 0.05312972888350487 Accuracy 0.45849609375\n",
      "Iteration 70450 Training loss 0.05186500400304794 Validation loss 0.053317293524742126 Accuracy 0.458984375\n",
      "Iteration 70460 Training loss 0.05353625491261482 Validation loss 0.05312312766909599 Accuracy 0.460693359375\n",
      "Iteration 70470 Training loss 0.04906991124153137 Validation loss 0.053325995802879333 Accuracy 0.45703125\n",
      "Iteration 70480 Training loss 0.0534142330288887 Validation loss 0.05328746512532234 Accuracy 0.459716796875\n",
      "Iteration 70490 Training loss 0.05163410305976868 Validation loss 0.05322625860571861 Accuracy 0.459716796875\n",
      "Iteration 70500 Training loss 0.05110485851764679 Validation loss 0.05325135216116905 Accuracy 0.46044921875\n",
      "Iteration 70510 Training loss 0.049821339547634125 Validation loss 0.05352907255291939 Accuracy 0.456787109375\n",
      "Iteration 70520 Training loss 0.05138762295246124 Validation loss 0.05321137607097626 Accuracy 0.45947265625\n",
      "Iteration 70530 Training loss 0.0487697534263134 Validation loss 0.052942197769880295 Accuracy 0.460205078125\n",
      "Iteration 70540 Training loss 0.05063120648264885 Validation loss 0.05345530062913895 Accuracy 0.45751953125\n",
      "Iteration 70550 Training loss 0.05174420028924942 Validation loss 0.05329088866710663 Accuracy 0.458251953125\n",
      "Iteration 70560 Training loss 0.04912034049630165 Validation loss 0.052955057471990585 Accuracy 0.460693359375\n",
      "Iteration 70570 Training loss 0.05322631448507309 Validation loss 0.05347401276230812 Accuracy 0.457763671875\n",
      "Iteration 70580 Training loss 0.05208083614706993 Validation loss 0.05277414992451668 Accuracy 0.461669921875\n",
      "Iteration 70590 Training loss 0.051372792571783066 Validation loss 0.05343613028526306 Accuracy 0.457763671875\n",
      "Iteration 70600 Training loss 0.049502670764923096 Validation loss 0.053121406584978104 Accuracy 0.460693359375\n",
      "Iteration 70610 Training loss 0.048714976757764816 Validation loss 0.05288121849298477 Accuracy 0.4609375\n",
      "Iteration 70620 Training loss 0.049263060092926025 Validation loss 0.0532134547829628 Accuracy 0.458984375\n",
      "Iteration 70630 Training loss 0.055436328053474426 Validation loss 0.053058065474033356 Accuracy 0.458740234375\n",
      "Iteration 70640 Training loss 0.05252731591463089 Validation loss 0.0531502328813076 Accuracy 0.45849609375\n",
      "Iteration 70650 Training loss 0.05215208977460861 Validation loss 0.05304309353232384 Accuracy 0.460693359375\n",
      "Iteration 70660 Training loss 0.052435822784900665 Validation loss 0.052825678139925 Accuracy 0.4609375\n",
      "Iteration 70670 Training loss 0.049976199865341187 Validation loss 0.05350378155708313 Accuracy 0.457763671875\n",
      "Iteration 70680 Training loss 0.049990538507699966 Validation loss 0.05319572985172272 Accuracy 0.459716796875\n",
      "Iteration 70690 Training loss 0.049716103821992874 Validation loss 0.05335467681288719 Accuracy 0.459228515625\n",
      "Iteration 70700 Training loss 0.0510060116648674 Validation loss 0.05316745862364769 Accuracy 0.4599609375\n",
      "Iteration 70710 Training loss 0.04965181276202202 Validation loss 0.05361894518136978 Accuracy 0.455078125\n",
      "Iteration 70720 Training loss 0.05128319188952446 Validation loss 0.05305596441030502 Accuracy 0.461181640625\n",
      "Iteration 70730 Training loss 0.048817262053489685 Validation loss 0.053139589726924896 Accuracy 0.461181640625\n",
      "Iteration 70740 Training loss 0.05049484223127365 Validation loss 0.05304689705371857 Accuracy 0.459716796875\n",
      "Iteration 70750 Training loss 0.05060027539730072 Validation loss 0.05333629623055458 Accuracy 0.45849609375\n",
      "Iteration 70760 Training loss 0.05085351690649986 Validation loss 0.05312550812959671 Accuracy 0.45947265625\n",
      "Iteration 70770 Training loss 0.05317996069788933 Validation loss 0.053708743304014206 Accuracy 0.456298828125\n",
      "Iteration 70780 Training loss 0.05108503997325897 Validation loss 0.053150203078985214 Accuracy 0.45849609375\n",
      "Iteration 70790 Training loss 0.05181421712040901 Validation loss 0.0529622919857502 Accuracy 0.4609375\n",
      "Iteration 70800 Training loss 0.04903793707489967 Validation loss 0.053218767046928406 Accuracy 0.459228515625\n",
      "Iteration 70810 Training loss 0.05459259822964668 Validation loss 0.05324247479438782 Accuracy 0.458740234375\n",
      "Iteration 70820 Training loss 0.048678576946258545 Validation loss 0.05313122272491455 Accuracy 0.460205078125\n",
      "Iteration 70830 Training loss 0.05263437330722809 Validation loss 0.05329342186450958 Accuracy 0.458984375\n",
      "Iteration 70840 Training loss 0.05384132266044617 Validation loss 0.05287869647145271 Accuracy 0.461669921875\n",
      "Iteration 70850 Training loss 0.053402893245220184 Validation loss 0.053995516151189804 Accuracy 0.45263671875\n",
      "Iteration 70860 Training loss 0.05215100571513176 Validation loss 0.05299606919288635 Accuracy 0.4609375\n",
      "Iteration 70870 Training loss 0.05104178190231323 Validation loss 0.053033728152513504 Accuracy 0.460205078125\n",
      "Iteration 70880 Training loss 0.05302669107913971 Validation loss 0.0529310517013073 Accuracy 0.460693359375\n",
      "Iteration 70890 Training loss 0.05074981972575188 Validation loss 0.05295015498995781 Accuracy 0.461669921875\n",
      "Iteration 70900 Training loss 0.04786026105284691 Validation loss 0.05344214662909508 Accuracy 0.458984375\n",
      "Iteration 70910 Training loss 0.05170426517724991 Validation loss 0.053067512810230255 Accuracy 0.45947265625\n",
      "Iteration 70920 Training loss 0.051702696830034256 Validation loss 0.05302108824253082 Accuracy 0.461181640625\n",
      "Iteration 70930 Training loss 0.0498180128633976 Validation loss 0.053087469190359116 Accuracy 0.46044921875\n",
      "Iteration 70940 Training loss 0.04887637868523598 Validation loss 0.05331248790025711 Accuracy 0.459228515625\n",
      "Iteration 70950 Training loss 0.05461918190121651 Validation loss 0.05322376638650894 Accuracy 0.45947265625\n",
      "Iteration 70960 Training loss 0.048260558396577835 Validation loss 0.0529727041721344 Accuracy 0.460693359375\n",
      "Iteration 70970 Training loss 0.05407128855586052 Validation loss 0.05369759723544121 Accuracy 0.45654296875\n",
      "Iteration 70980 Training loss 0.049803923815488815 Validation loss 0.053045663982629776 Accuracy 0.460693359375\n",
      "Iteration 70990 Training loss 0.04908496141433716 Validation loss 0.05317504703998566 Accuracy 0.460205078125\n",
      "Iteration 71000 Training loss 0.05335484817624092 Validation loss 0.05371560528874397 Accuracy 0.457763671875\n",
      "Iteration 71010 Training loss 0.051288310438394547 Validation loss 0.05317305400967598 Accuracy 0.4599609375\n",
      "Iteration 71020 Training loss 0.0505763478577137 Validation loss 0.05293748900294304 Accuracy 0.461181640625\n",
      "Iteration 71030 Training loss 0.05117914825677872 Validation loss 0.05367296189069748 Accuracy 0.455810546875\n",
      "Iteration 71040 Training loss 0.049289438873529434 Validation loss 0.05286109074950218 Accuracy 0.4609375\n",
      "Iteration 71050 Training loss 0.05308603495359421 Validation loss 0.05295355245471001 Accuracy 0.459228515625\n",
      "Iteration 71060 Training loss 0.05149616301059723 Validation loss 0.053501784801483154 Accuracy 0.458251953125\n",
      "Iteration 71070 Training loss 0.05190553888678551 Validation loss 0.05303565412759781 Accuracy 0.4609375\n",
      "Iteration 71080 Training loss 0.0512431375682354 Validation loss 0.05275333672761917 Accuracy 0.46240234375\n",
      "Iteration 71090 Training loss 0.048789020627737045 Validation loss 0.05312084034085274 Accuracy 0.45947265625\n",
      "Iteration 71100 Training loss 0.04840994253754616 Validation loss 0.05330276116728783 Accuracy 0.45751953125\n",
      "Iteration 71110 Training loss 0.05225197970867157 Validation loss 0.05292525142431259 Accuracy 0.461181640625\n",
      "Iteration 71120 Training loss 0.051905445754528046 Validation loss 0.05341339111328125 Accuracy 0.459716796875\n",
      "Iteration 71130 Training loss 0.051098667085170746 Validation loss 0.05285503715276718 Accuracy 0.461669921875\n",
      "Iteration 71140 Training loss 0.04933023080229759 Validation loss 0.05303686857223511 Accuracy 0.459716796875\n",
      "Iteration 71150 Training loss 0.051707204431295395 Validation loss 0.053333621472120285 Accuracy 0.457763671875\n",
      "Iteration 71160 Training loss 0.04983107000589371 Validation loss 0.05368750914931297 Accuracy 0.456298828125\n",
      "Iteration 71170 Training loss 0.05251229181885719 Validation loss 0.053816501051187515 Accuracy 0.4560546875\n",
      "Iteration 71180 Training loss 0.04825759306550026 Validation loss 0.053187910467386246 Accuracy 0.459716796875\n",
      "Iteration 71190 Training loss 0.05043553560972214 Validation loss 0.052770182490348816 Accuracy 0.461669921875\n",
      "Iteration 71200 Training loss 0.049917224794626236 Validation loss 0.05275295674800873 Accuracy 0.46240234375\n",
      "Iteration 71210 Training loss 0.0520540252327919 Validation loss 0.05298170819878578 Accuracy 0.461669921875\n",
      "Iteration 71220 Training loss 0.048115748912096024 Validation loss 0.05276629701256752 Accuracy 0.46240234375\n",
      "Iteration 71230 Training loss 0.05014811083674431 Validation loss 0.05287900194525719 Accuracy 0.461669921875\n",
      "Iteration 71240 Training loss 0.05194982886314392 Validation loss 0.05282639339566231 Accuracy 0.4619140625\n",
      "Iteration 71250 Training loss 0.052864398807287216 Validation loss 0.05296666547656059 Accuracy 0.460693359375\n",
      "Iteration 71260 Training loss 0.05306708812713623 Validation loss 0.05305100977420807 Accuracy 0.459716796875\n",
      "Iteration 71270 Training loss 0.051316384226083755 Validation loss 0.05285489186644554 Accuracy 0.46240234375\n",
      "Iteration 71280 Training loss 0.049363452941179276 Validation loss 0.05307731777429581 Accuracy 0.4599609375\n",
      "Iteration 71290 Training loss 0.049906518310308456 Validation loss 0.05275505408644676 Accuracy 0.46240234375\n",
      "Iteration 71300 Training loss 0.050747547298669815 Validation loss 0.053322527557611465 Accuracy 0.4580078125\n",
      "Iteration 71310 Training loss 0.04968111589550972 Validation loss 0.05288907140493393 Accuracy 0.461669921875\n",
      "Iteration 71320 Training loss 0.04945439100265503 Validation loss 0.05338827893137932 Accuracy 0.45751953125\n",
      "Iteration 71330 Training loss 0.049344226717948914 Validation loss 0.052895888686180115 Accuracy 0.461181640625\n",
      "Iteration 71340 Training loss 0.05274447426199913 Validation loss 0.05341634526848793 Accuracy 0.458251953125\n",
      "Iteration 71350 Training loss 0.04836488887667656 Validation loss 0.053010910749435425 Accuracy 0.46044921875\n",
      "Iteration 71360 Training loss 0.046524256467819214 Validation loss 0.05286449193954468 Accuracy 0.462158203125\n",
      "Iteration 71370 Training loss 0.04949318245053291 Validation loss 0.053054746240377426 Accuracy 0.461181640625\n",
      "Iteration 71380 Training loss 0.04954413324594498 Validation loss 0.05295611172914505 Accuracy 0.461181640625\n",
      "Iteration 71390 Training loss 0.04975133761763573 Validation loss 0.05318312719464302 Accuracy 0.45947265625\n",
      "Iteration 71400 Training loss 0.05290280655026436 Validation loss 0.05303732678294182 Accuracy 0.460693359375\n",
      "Iteration 71410 Training loss 0.04971860721707344 Validation loss 0.0530732199549675 Accuracy 0.46142578125\n",
      "Iteration 71420 Training loss 0.05152619257569313 Validation loss 0.05303196981549263 Accuracy 0.4599609375\n",
      "Iteration 71430 Training loss 0.04997101426124573 Validation loss 0.05283592268824577 Accuracy 0.462158203125\n",
      "Iteration 71440 Training loss 0.05306306853890419 Validation loss 0.05322711914777756 Accuracy 0.459228515625\n",
      "Iteration 71450 Training loss 0.04937560856342316 Validation loss 0.05284729227423668 Accuracy 0.46142578125\n",
      "Iteration 71460 Training loss 0.053270526230335236 Validation loss 0.05296732112765312 Accuracy 0.4609375\n",
      "Iteration 71470 Training loss 0.050608620047569275 Validation loss 0.05284334719181061 Accuracy 0.461181640625\n",
      "Iteration 71480 Training loss 0.05153026431798935 Validation loss 0.053356923162937164 Accuracy 0.456787109375\n",
      "Iteration 71490 Training loss 0.05020487308502197 Validation loss 0.05331104248762131 Accuracy 0.4580078125\n",
      "Iteration 71500 Training loss 0.046248383820056915 Validation loss 0.053408559411764145 Accuracy 0.4580078125\n",
      "Iteration 71510 Training loss 0.051426783204078674 Validation loss 0.05300340801477432 Accuracy 0.458984375\n",
      "Iteration 71520 Training loss 0.05375644937157631 Validation loss 0.05355103313922882 Accuracy 0.456787109375\n",
      "Iteration 71530 Training loss 0.05162879452109337 Validation loss 0.053102146834135056 Accuracy 0.45947265625\n",
      "Iteration 71540 Training loss 0.04903016239404678 Validation loss 0.05338554456830025 Accuracy 0.45751953125\n",
      "Iteration 71550 Training loss 0.050054553896188736 Validation loss 0.0531676784157753 Accuracy 0.45849609375\n",
      "Iteration 71560 Training loss 0.048877205699682236 Validation loss 0.05301498994231224 Accuracy 0.46142578125\n",
      "Iteration 71570 Training loss 0.04751541092991829 Validation loss 0.05336913838982582 Accuracy 0.459228515625\n",
      "Iteration 71580 Training loss 0.051767248660326004 Validation loss 0.05363374203443527 Accuracy 0.455810546875\n",
      "Iteration 71590 Training loss 0.050393249839544296 Validation loss 0.05329299718141556 Accuracy 0.458740234375\n",
      "Iteration 71600 Training loss 0.051931366324424744 Validation loss 0.05288795754313469 Accuracy 0.46142578125\n",
      "Iteration 71610 Training loss 0.053025003522634506 Validation loss 0.05306030064821243 Accuracy 0.4599609375\n",
      "Iteration 71620 Training loss 0.05556922405958176 Validation loss 0.05271850898861885 Accuracy 0.46240234375\n",
      "Iteration 71630 Training loss 0.05237775668501854 Validation loss 0.0533173531293869 Accuracy 0.457763671875\n",
      "Iteration 71640 Training loss 0.050280969589948654 Validation loss 0.053111374378204346 Accuracy 0.460693359375\n",
      "Iteration 71650 Training loss 0.049627237021923065 Validation loss 0.05330553650856018 Accuracy 0.459228515625\n",
      "Iteration 71660 Training loss 0.04859764501452446 Validation loss 0.05321621522307396 Accuracy 0.458984375\n",
      "Iteration 71670 Training loss 0.053786102682352066 Validation loss 0.05333544313907623 Accuracy 0.459228515625\n",
      "Iteration 71680 Training loss 0.051010310649871826 Validation loss 0.053309205919504166 Accuracy 0.45751953125\n",
      "Iteration 71690 Training loss 0.05373614281415939 Validation loss 0.053242702037096024 Accuracy 0.459716796875\n",
      "Iteration 71700 Training loss 0.05155770853161812 Validation loss 0.05356823652982712 Accuracy 0.4560546875\n",
      "Iteration 71710 Training loss 0.0513780452311039 Validation loss 0.053134284913539886 Accuracy 0.460205078125\n",
      "Iteration 71720 Training loss 0.053451020270586014 Validation loss 0.053019121289253235 Accuracy 0.460205078125\n",
      "Iteration 71730 Training loss 0.05061229318380356 Validation loss 0.053109392523765564 Accuracy 0.45849609375\n",
      "Iteration 71740 Training loss 0.047998134046792984 Validation loss 0.05299456790089607 Accuracy 0.46044921875\n",
      "Iteration 71750 Training loss 0.052509237080812454 Validation loss 0.05313556641340256 Accuracy 0.459228515625\n",
      "Iteration 71760 Training loss 0.05488002300262451 Validation loss 0.05339193344116211 Accuracy 0.459716796875\n",
      "Iteration 71770 Training loss 0.0479818731546402 Validation loss 0.05323578044772148 Accuracy 0.458740234375\n",
      "Iteration 71780 Training loss 0.05093454569578171 Validation loss 0.05304829776287079 Accuracy 0.460205078125\n",
      "Iteration 71790 Training loss 0.0509163923561573 Validation loss 0.05309557914733887 Accuracy 0.458984375\n",
      "Iteration 71800 Training loss 0.05166010931134224 Validation loss 0.0530933178961277 Accuracy 0.459716796875\n",
      "Iteration 71810 Training loss 0.05189098045229912 Validation loss 0.05291583016514778 Accuracy 0.4599609375\n",
      "Iteration 71820 Training loss 0.05156870558857918 Validation loss 0.052974604070186615 Accuracy 0.46044921875\n",
      "Iteration 71830 Training loss 0.05293268337845802 Validation loss 0.05302177742123604 Accuracy 0.4599609375\n",
      "Iteration 71840 Training loss 0.050579216331243515 Validation loss 0.05292167887091637 Accuracy 0.461181640625\n",
      "Iteration 71850 Training loss 0.05096807703375816 Validation loss 0.05316263437271118 Accuracy 0.461181640625\n",
      "Iteration 71860 Training loss 0.05093316733837128 Validation loss 0.05322103947401047 Accuracy 0.461181640625\n",
      "Iteration 71870 Training loss 0.05384315922856331 Validation loss 0.05299888923764229 Accuracy 0.461669921875\n",
      "Iteration 71880 Training loss 0.050723470747470856 Validation loss 0.053590428084135056 Accuracy 0.4580078125\n",
      "Iteration 71890 Training loss 0.049134235829114914 Validation loss 0.05308866128325462 Accuracy 0.458251953125\n",
      "Iteration 71900 Training loss 0.05113642290234566 Validation loss 0.05295615643262863 Accuracy 0.46142578125\n",
      "Iteration 71910 Training loss 0.051454056054353714 Validation loss 0.05364907160401344 Accuracy 0.457275390625\n",
      "Iteration 71920 Training loss 0.05096012353897095 Validation loss 0.053087782114744186 Accuracy 0.460205078125\n",
      "Iteration 71930 Training loss 0.04945003613829613 Validation loss 0.053145669400691986 Accuracy 0.45849609375\n",
      "Iteration 71940 Training loss 0.04859159141778946 Validation loss 0.05298147350549698 Accuracy 0.45947265625\n",
      "Iteration 71950 Training loss 0.04983988404273987 Validation loss 0.05303732305765152 Accuracy 0.458984375\n",
      "Iteration 71960 Training loss 0.04798539727926254 Validation loss 0.05296764522790909 Accuracy 0.461669921875\n",
      "Iteration 71970 Training loss 0.04933641478419304 Validation loss 0.053105875849723816 Accuracy 0.45947265625\n",
      "Iteration 71980 Training loss 0.050203755497932434 Validation loss 0.05315416306257248 Accuracy 0.4609375\n",
      "Iteration 71990 Training loss 0.054151151329278946 Validation loss 0.05365600064396858 Accuracy 0.456298828125\n",
      "Iteration 72000 Training loss 0.05133238807320595 Validation loss 0.053300537168979645 Accuracy 0.459716796875\n",
      "Iteration 72010 Training loss 0.051319193094968796 Validation loss 0.05328284204006195 Accuracy 0.46044921875\n",
      "Iteration 72020 Training loss 0.0509435199201107 Validation loss 0.05324658006429672 Accuracy 0.459716796875\n",
      "Iteration 72030 Training loss 0.05075886473059654 Validation loss 0.05319082736968994 Accuracy 0.457763671875\n",
      "Iteration 72040 Training loss 0.05193014070391655 Validation loss 0.052902646362781525 Accuracy 0.4609375\n",
      "Iteration 72050 Training loss 0.054603394120931625 Validation loss 0.05342387035489082 Accuracy 0.45751953125\n",
      "Iteration 72060 Training loss 0.047576893121004105 Validation loss 0.05344536155462265 Accuracy 0.45751953125\n",
      "Iteration 72070 Training loss 0.05072760954499245 Validation loss 0.05287014693021774 Accuracy 0.461181640625\n",
      "Iteration 72080 Training loss 0.049668390303850174 Validation loss 0.05342290550470352 Accuracy 0.458251953125\n",
      "Iteration 72090 Training loss 0.05531361326575279 Validation loss 0.053490832448005676 Accuracy 0.45947265625\n",
      "Iteration 72100 Training loss 0.051513757556676865 Validation loss 0.05324941873550415 Accuracy 0.458740234375\n",
      "Iteration 72110 Training loss 0.05200878158211708 Validation loss 0.053382840007543564 Accuracy 0.459716796875\n",
      "Iteration 72120 Training loss 0.051159996539354324 Validation loss 0.0536663644015789 Accuracy 0.45703125\n",
      "Iteration 72130 Training loss 0.05249917507171631 Validation loss 0.05298912152647972 Accuracy 0.4619140625\n",
      "Iteration 72140 Training loss 0.0528416745364666 Validation loss 0.05413619056344032 Accuracy 0.451416015625\n",
      "Iteration 72150 Training loss 0.04970391467213631 Validation loss 0.05344121903181076 Accuracy 0.458984375\n",
      "Iteration 72160 Training loss 0.05327158421278 Validation loss 0.053218014538288116 Accuracy 0.459228515625\n",
      "Iteration 72170 Training loss 0.053313203155994415 Validation loss 0.053234558552503586 Accuracy 0.45849609375\n",
      "Iteration 72180 Training loss 0.05074092373251915 Validation loss 0.0530136302113533 Accuracy 0.45947265625\n",
      "Iteration 72190 Training loss 0.0468912236392498 Validation loss 0.05285729840397835 Accuracy 0.4609375\n",
      "Iteration 72200 Training loss 0.04990103095769882 Validation loss 0.05284284055233002 Accuracy 0.462158203125\n",
      "Iteration 72210 Training loss 0.05110017955303192 Validation loss 0.05329610034823418 Accuracy 0.4560546875\n",
      "Iteration 72220 Training loss 0.052939340472221375 Validation loss 0.05294966697692871 Accuracy 0.461669921875\n",
      "Iteration 72230 Training loss 0.05341409891843796 Validation loss 0.05327144265174866 Accuracy 0.45849609375\n",
      "Iteration 72240 Training loss 0.0499379001557827 Validation loss 0.053012821823358536 Accuracy 0.460205078125\n",
      "Iteration 72250 Training loss 0.05277162790298462 Validation loss 0.053252264857292175 Accuracy 0.458740234375\n",
      "Iteration 72260 Training loss 0.049710724502801895 Validation loss 0.05307271331548691 Accuracy 0.459716796875\n",
      "Iteration 72270 Training loss 0.0490121953189373 Validation loss 0.053206540644168854 Accuracy 0.459228515625\n",
      "Iteration 72280 Training loss 0.05025891214609146 Validation loss 0.05328790470957756 Accuracy 0.45654296875\n",
      "Iteration 72290 Training loss 0.0493013970553875 Validation loss 0.05298691242933273 Accuracy 0.461181640625\n",
      "Iteration 72300 Training loss 0.0534575916826725 Validation loss 0.053092848509550095 Accuracy 0.46044921875\n",
      "Iteration 72310 Training loss 0.05056693032383919 Validation loss 0.05288594588637352 Accuracy 0.460205078125\n",
      "Iteration 72320 Training loss 0.05155960097908974 Validation loss 0.05296247825026512 Accuracy 0.4599609375\n",
      "Iteration 72330 Training loss 0.051157113164663315 Validation loss 0.05332773178815842 Accuracy 0.45751953125\n",
      "Iteration 72340 Training loss 0.05055350810289383 Validation loss 0.05320952460169792 Accuracy 0.459228515625\n",
      "Iteration 72350 Training loss 0.05170012265443802 Validation loss 0.05354154109954834 Accuracy 0.45751953125\n",
      "Iteration 72360 Training loss 0.05178432539105415 Validation loss 0.05300285667181015 Accuracy 0.460693359375\n",
      "Iteration 72370 Training loss 0.05316539108753204 Validation loss 0.052864354103803635 Accuracy 0.459716796875\n",
      "Iteration 72380 Training loss 0.050614774227142334 Validation loss 0.053172167390584946 Accuracy 0.458984375\n",
      "Iteration 72390 Training loss 0.05110863223671913 Validation loss 0.052861288189888 Accuracy 0.461181640625\n",
      "Iteration 72400 Training loss 0.05064457654953003 Validation loss 0.053024228662252426 Accuracy 0.458984375\n",
      "Iteration 72410 Training loss 0.05164935067296028 Validation loss 0.05290598422288895 Accuracy 0.460693359375\n",
      "Iteration 72420 Training loss 0.05131901055574417 Validation loss 0.053763508796691895 Accuracy 0.456298828125\n",
      "Iteration 72430 Training loss 0.051893509924411774 Validation loss 0.05318043380975723 Accuracy 0.45849609375\n",
      "Iteration 72440 Training loss 0.05145859718322754 Validation loss 0.05298667028546333 Accuracy 0.461181640625\n",
      "Iteration 72450 Training loss 0.050616923719644547 Validation loss 0.053183045238256454 Accuracy 0.460205078125\n",
      "Iteration 72460 Training loss 0.0457601398229599 Validation loss 0.05310206487774849 Accuracy 0.458251953125\n",
      "Iteration 72470 Training loss 0.050594329833984375 Validation loss 0.053063251078128815 Accuracy 0.459228515625\n",
      "Iteration 72480 Training loss 0.04827216640114784 Validation loss 0.0528995543718338 Accuracy 0.459716796875\n",
      "Iteration 72490 Training loss 0.049772560596466064 Validation loss 0.053261756896972656 Accuracy 0.45947265625\n",
      "Iteration 72500 Training loss 0.05334068834781647 Validation loss 0.05277733877301216 Accuracy 0.46142578125\n",
      "Iteration 72510 Training loss 0.049958087503910065 Validation loss 0.05309521406888962 Accuracy 0.459228515625\n",
      "Iteration 72520 Training loss 0.04834266006946564 Validation loss 0.05288025364279747 Accuracy 0.461181640625\n",
      "Iteration 72530 Training loss 0.050160013139247894 Validation loss 0.05310509726405144 Accuracy 0.459716796875\n",
      "Iteration 72540 Training loss 0.04825664311647415 Validation loss 0.05309559404850006 Accuracy 0.460205078125\n",
      "Iteration 72550 Training loss 0.052254416048526764 Validation loss 0.05299600586295128 Accuracy 0.461181640625\n",
      "Iteration 72560 Training loss 0.05288043990731239 Validation loss 0.05346827954053879 Accuracy 0.457763671875\n",
      "Iteration 72570 Training loss 0.05283379927277565 Validation loss 0.05289747938513756 Accuracy 0.459228515625\n",
      "Iteration 72580 Training loss 0.05335991829633713 Validation loss 0.052821580320596695 Accuracy 0.460693359375\n",
      "Iteration 72590 Training loss 0.05217744782567024 Validation loss 0.05301844701170921 Accuracy 0.460693359375\n",
      "Iteration 72600 Training loss 0.054125819355249405 Validation loss 0.053141020238399506 Accuracy 0.45849609375\n",
      "Iteration 72610 Training loss 0.050294727087020874 Validation loss 0.052998434752225876 Accuracy 0.459716796875\n",
      "Iteration 72620 Training loss 0.05068276822566986 Validation loss 0.05311084911227226 Accuracy 0.459716796875\n",
      "Iteration 72630 Training loss 0.04682362824678421 Validation loss 0.05311653017997742 Accuracy 0.460205078125\n",
      "Iteration 72640 Training loss 0.05065188929438591 Validation loss 0.052920158952474594 Accuracy 0.4609375\n",
      "Iteration 72650 Training loss 0.050770070403814316 Validation loss 0.05317595601081848 Accuracy 0.45947265625\n",
      "Iteration 72660 Training loss 0.05061861500144005 Validation loss 0.052891556173563004 Accuracy 0.460693359375\n",
      "Iteration 72670 Training loss 0.05116716027259827 Validation loss 0.05308583006262779 Accuracy 0.46044921875\n",
      "Iteration 72680 Training loss 0.05195270851254463 Validation loss 0.053358837962150574 Accuracy 0.458984375\n",
      "Iteration 72690 Training loss 0.05490005388855934 Validation loss 0.05322076380252838 Accuracy 0.459228515625\n",
      "Iteration 72700 Training loss 0.051928289234638214 Validation loss 0.05327357351779938 Accuracy 0.458984375\n",
      "Iteration 72710 Training loss 0.049836862832307816 Validation loss 0.05351901054382324 Accuracy 0.454345703125\n",
      "Iteration 72720 Training loss 0.05280420556664467 Validation loss 0.053187429904937744 Accuracy 0.459716796875\n",
      "Iteration 72730 Training loss 0.05177462100982666 Validation loss 0.053051337599754333 Accuracy 0.460205078125\n",
      "Iteration 72740 Training loss 0.05002469941973686 Validation loss 0.052774280309677124 Accuracy 0.461669921875\n",
      "Iteration 72750 Training loss 0.04913890361785889 Validation loss 0.05333113670349121 Accuracy 0.457275390625\n",
      "Iteration 72760 Training loss 0.050221700221300125 Validation loss 0.05285104364156723 Accuracy 0.460693359375\n",
      "Iteration 72770 Training loss 0.05453715845942497 Validation loss 0.053021520376205444 Accuracy 0.46044921875\n",
      "Iteration 72780 Training loss 0.048761025071144104 Validation loss 0.05295829102396965 Accuracy 0.4619140625\n",
      "Iteration 72790 Training loss 0.05363790690898895 Validation loss 0.05306949466466904 Accuracy 0.46044921875\n",
      "Iteration 72800 Training loss 0.05276131257414818 Validation loss 0.053664080798625946 Accuracy 0.457275390625\n",
      "Iteration 72810 Training loss 0.05276487022638321 Validation loss 0.052829910069704056 Accuracy 0.46142578125\n",
      "Iteration 72820 Training loss 0.05285387486219406 Validation loss 0.052865784615278244 Accuracy 0.460693359375\n",
      "Iteration 72830 Training loss 0.04960194230079651 Validation loss 0.053552448749542236 Accuracy 0.4560546875\n",
      "Iteration 72840 Training loss 0.051471877843141556 Validation loss 0.053344909101724625 Accuracy 0.458984375\n",
      "Iteration 72850 Training loss 0.05487958714365959 Validation loss 0.05317385494709015 Accuracy 0.459716796875\n",
      "Iteration 72860 Training loss 0.051423877477645874 Validation loss 0.0535341277718544 Accuracy 0.45751953125\n",
      "Iteration 72870 Training loss 0.05406062304973602 Validation loss 0.05320175737142563 Accuracy 0.458740234375\n",
      "Iteration 72880 Training loss 0.05117243528366089 Validation loss 0.05298478901386261 Accuracy 0.459716796875\n",
      "Iteration 72890 Training loss 0.052616771310567856 Validation loss 0.05289514362812042 Accuracy 0.46044921875\n",
      "Iteration 72900 Training loss 0.05180808901786804 Validation loss 0.05299530178308487 Accuracy 0.46044921875\n",
      "Iteration 72910 Training loss 0.049870651215314865 Validation loss 0.053048525005578995 Accuracy 0.459228515625\n",
      "Iteration 72920 Training loss 0.0525909923017025 Validation loss 0.053152479231357574 Accuracy 0.4599609375\n",
      "Iteration 72930 Training loss 0.049557141959667206 Validation loss 0.05300430580973625 Accuracy 0.46044921875\n",
      "Iteration 72940 Training loss 0.051917947828769684 Validation loss 0.05353355407714844 Accuracy 0.4580078125\n",
      "Iteration 72950 Training loss 0.05217822268605232 Validation loss 0.05333104357123375 Accuracy 0.459716796875\n",
      "Iteration 72960 Training loss 0.051343414932489395 Validation loss 0.053020913153886795 Accuracy 0.460205078125\n",
      "Iteration 72970 Training loss 0.050903309136629105 Validation loss 0.053325798362493515 Accuracy 0.45751953125\n",
      "Iteration 72980 Training loss 0.051281657069921494 Validation loss 0.053011950105428696 Accuracy 0.46044921875\n",
      "Iteration 72990 Training loss 0.04936743155121803 Validation loss 0.053254157304763794 Accuracy 0.459716796875\n",
      "Iteration 73000 Training loss 0.05336262285709381 Validation loss 0.05375971645116806 Accuracy 0.45458984375\n",
      "Iteration 73010 Training loss 0.053493399173021317 Validation loss 0.053263530135154724 Accuracy 0.45849609375\n",
      "Iteration 73020 Training loss 0.055931903421878815 Validation loss 0.05364290252327919 Accuracy 0.45751953125\n",
      "Iteration 73030 Training loss 0.05081215500831604 Validation loss 0.05300133675336838 Accuracy 0.460693359375\n",
      "Iteration 73040 Training loss 0.05046762526035309 Validation loss 0.053153857588768005 Accuracy 0.458740234375\n",
      "Iteration 73050 Training loss 0.05169129744172096 Validation loss 0.053169313818216324 Accuracy 0.458740234375\n",
      "Iteration 73060 Training loss 0.04883192852139473 Validation loss 0.0531463660299778 Accuracy 0.460205078125\n",
      "Iteration 73070 Training loss 0.04978484660387039 Validation loss 0.052924491465091705 Accuracy 0.459716796875\n",
      "Iteration 73080 Training loss 0.05500457063317299 Validation loss 0.053302690386772156 Accuracy 0.4580078125\n",
      "Iteration 73090 Training loss 0.04461688548326492 Validation loss 0.052833352237939835 Accuracy 0.460693359375\n",
      "Iteration 73100 Training loss 0.052177976816892624 Validation loss 0.052934225648641586 Accuracy 0.460693359375\n",
      "Iteration 73110 Training loss 0.051159173250198364 Validation loss 0.05329165235161781 Accuracy 0.459716796875\n",
      "Iteration 73120 Training loss 0.049422845244407654 Validation loss 0.05309215188026428 Accuracy 0.460205078125\n",
      "Iteration 73130 Training loss 0.05153409764170647 Validation loss 0.05319352447986603 Accuracy 0.46142578125\n",
      "Iteration 73140 Training loss 0.049470022320747375 Validation loss 0.05305056646466255 Accuracy 0.45947265625\n",
      "Iteration 73150 Training loss 0.050592366605997086 Validation loss 0.05315685272216797 Accuracy 0.459228515625\n",
      "Iteration 73160 Training loss 0.053735848516225815 Validation loss 0.05295673757791519 Accuracy 0.46044921875\n",
      "Iteration 73170 Training loss 0.05097658187150955 Validation loss 0.05274504795670509 Accuracy 0.46240234375\n",
      "Iteration 73180 Training loss 0.051160216331481934 Validation loss 0.05289479345083237 Accuracy 0.46142578125\n",
      "Iteration 73190 Training loss 0.05252084136009216 Validation loss 0.05292993038892746 Accuracy 0.46142578125\n",
      "Iteration 73200 Training loss 0.050094299018383026 Validation loss 0.052982147783041 Accuracy 0.46142578125\n",
      "Iteration 73210 Training loss 0.04895125702023506 Validation loss 0.0537840910255909 Accuracy 0.453125\n",
      "Iteration 73220 Training loss 0.05207841098308563 Validation loss 0.053036000579595566 Accuracy 0.46044921875\n",
      "Iteration 73230 Training loss 0.05149248242378235 Validation loss 0.05293217673897743 Accuracy 0.46142578125\n",
      "Iteration 73240 Training loss 0.05060836300253868 Validation loss 0.05307770520448685 Accuracy 0.46044921875\n",
      "Iteration 73250 Training loss 0.048935506492853165 Validation loss 0.05293510481715202 Accuracy 0.460693359375\n",
      "Iteration 73260 Training loss 0.04948301985859871 Validation loss 0.05307462811470032 Accuracy 0.459716796875\n",
      "Iteration 73270 Training loss 0.056045226752758026 Validation loss 0.052862830460071564 Accuracy 0.46142578125\n",
      "Iteration 73280 Training loss 0.05404416471719742 Validation loss 0.05310893431305885 Accuracy 0.460205078125\n",
      "Iteration 73290 Training loss 0.05200282856822014 Validation loss 0.05298399180173874 Accuracy 0.461669921875\n",
      "Iteration 73300 Training loss 0.052084606140851974 Validation loss 0.05330764129757881 Accuracy 0.4580078125\n",
      "Iteration 73310 Training loss 0.051452647894620895 Validation loss 0.053076110780239105 Accuracy 0.45947265625\n",
      "Iteration 73320 Training loss 0.05549965426325798 Validation loss 0.053282421082258224 Accuracy 0.458984375\n",
      "Iteration 73330 Training loss 0.05103247985243797 Validation loss 0.053148459643125534 Accuracy 0.458984375\n",
      "Iteration 73340 Training loss 0.04775775223970413 Validation loss 0.05327991396188736 Accuracy 0.459716796875\n",
      "Iteration 73350 Training loss 0.05143722519278526 Validation loss 0.052919480949640274 Accuracy 0.461181640625\n",
      "Iteration 73360 Training loss 0.05217006802558899 Validation loss 0.052993591874837875 Accuracy 0.4609375\n",
      "Iteration 73370 Training loss 0.05157877504825592 Validation loss 0.05302134528756142 Accuracy 0.460693359375\n",
      "Iteration 73380 Training loss 0.05181784927845001 Validation loss 0.05321675166487694 Accuracy 0.458984375\n",
      "Iteration 73390 Training loss 0.048788391053676605 Validation loss 0.05290303751826286 Accuracy 0.4609375\n",
      "Iteration 73400 Training loss 0.04688594490289688 Validation loss 0.05289510637521744 Accuracy 0.458984375\n",
      "Iteration 73410 Training loss 0.05086490884423256 Validation loss 0.053316108882427216 Accuracy 0.4599609375\n",
      "Iteration 73420 Training loss 0.05275999754667282 Validation loss 0.05344729870557785 Accuracy 0.45751953125\n",
      "Iteration 73430 Training loss 0.05210443213582039 Validation loss 0.05293959006667137 Accuracy 0.4599609375\n",
      "Iteration 73440 Training loss 0.0526699535548687 Validation loss 0.05341915786266327 Accuracy 0.4580078125\n",
      "Iteration 73450 Training loss 0.050915494561195374 Validation loss 0.052763503044843674 Accuracy 0.461181640625\n",
      "Iteration 73460 Training loss 0.0514555349946022 Validation loss 0.053442489355802536 Accuracy 0.4580078125\n",
      "Iteration 73470 Training loss 0.05110250413417816 Validation loss 0.053034763783216476 Accuracy 0.459716796875\n",
      "Iteration 73480 Training loss 0.05208316072821617 Validation loss 0.05330336466431618 Accuracy 0.459228515625\n",
      "Iteration 73490 Training loss 0.048629023134708405 Validation loss 0.05316084995865822 Accuracy 0.459716796875\n",
      "Iteration 73500 Training loss 0.05223415419459343 Validation loss 0.05317655950784683 Accuracy 0.459716796875\n",
      "Iteration 73510 Training loss 0.05278000980615616 Validation loss 0.05363438278436661 Accuracy 0.45751953125\n",
      "Iteration 73520 Training loss 0.051097676157951355 Validation loss 0.05299961939454079 Accuracy 0.459716796875\n",
      "Iteration 73530 Training loss 0.04983244091272354 Validation loss 0.053253620862960815 Accuracy 0.460205078125\n",
      "Iteration 73540 Training loss 0.050520651042461395 Validation loss 0.05287229269742966 Accuracy 0.46142578125\n",
      "Iteration 73550 Training loss 0.054703932255506516 Validation loss 0.052938103675842285 Accuracy 0.460693359375\n",
      "Iteration 73560 Training loss 0.05005333945155144 Validation loss 0.05349043011665344 Accuracy 0.45703125\n",
      "Iteration 73570 Training loss 0.05099667236208916 Validation loss 0.052901580929756165 Accuracy 0.46044921875\n",
      "Iteration 73580 Training loss 0.048593997955322266 Validation loss 0.05310588702559471 Accuracy 0.458984375\n",
      "Iteration 73590 Training loss 0.053287189453840256 Validation loss 0.05307972431182861 Accuracy 0.460205078125\n",
      "Iteration 73600 Training loss 0.0519130639731884 Validation loss 0.05340481922030449 Accuracy 0.45849609375\n",
      "Iteration 73610 Training loss 0.05190443992614746 Validation loss 0.053252723067998886 Accuracy 0.4580078125\n",
      "Iteration 73620 Training loss 0.046819671988487244 Validation loss 0.05313011631369591 Accuracy 0.459228515625\n",
      "Iteration 73630 Training loss 0.048742685467004776 Validation loss 0.0527469627559185 Accuracy 0.461669921875\n",
      "Iteration 73640 Training loss 0.05251164734363556 Validation loss 0.052962254732847214 Accuracy 0.4609375\n",
      "Iteration 73650 Training loss 0.04977497458457947 Validation loss 0.05322965607047081 Accuracy 0.459228515625\n",
      "Iteration 73660 Training loss 0.052192945033311844 Validation loss 0.05355247110128403 Accuracy 0.455810546875\n",
      "Iteration 73670 Training loss 0.049761250615119934 Validation loss 0.05313926190137863 Accuracy 0.459228515625\n",
      "Iteration 73680 Training loss 0.053231820464134216 Validation loss 0.05316763371229172 Accuracy 0.458740234375\n",
      "Iteration 73690 Training loss 0.049197662621736526 Validation loss 0.05315787345170975 Accuracy 0.458984375\n",
      "Iteration 73700 Training loss 0.05255002900958061 Validation loss 0.05312288925051689 Accuracy 0.458984375\n",
      "Iteration 73710 Training loss 0.05013139918446541 Validation loss 0.052835047245025635 Accuracy 0.4609375\n",
      "Iteration 73720 Training loss 0.04769987612962723 Validation loss 0.05307478457689285 Accuracy 0.45947265625\n",
      "Iteration 73730 Training loss 0.05153319239616394 Validation loss 0.05330223590135574 Accuracy 0.459228515625\n",
      "Iteration 73740 Training loss 0.05015821382403374 Validation loss 0.05388638377189636 Accuracy 0.453125\n",
      "Iteration 73750 Training loss 0.05385557562112808 Validation loss 0.0531282052397728 Accuracy 0.460693359375\n",
      "Iteration 73760 Training loss 0.051159102469682693 Validation loss 0.053033169358968735 Accuracy 0.460205078125\n",
      "Iteration 73770 Training loss 0.049992091953754425 Validation loss 0.05300702154636383 Accuracy 0.45947265625\n",
      "Iteration 73780 Training loss 0.0490141324698925 Validation loss 0.05337382107973099 Accuracy 0.4580078125\n",
      "Iteration 73790 Training loss 0.05382665991783142 Validation loss 0.05333637446165085 Accuracy 0.457763671875\n",
      "Iteration 73800 Training loss 0.04640218988060951 Validation loss 0.053155072033405304 Accuracy 0.459716796875\n",
      "Iteration 73810 Training loss 0.05109212175011635 Validation loss 0.053137585520744324 Accuracy 0.45947265625\n",
      "Iteration 73820 Training loss 0.04988432303071022 Validation loss 0.05344061926007271 Accuracy 0.457275390625\n",
      "Iteration 73830 Training loss 0.05118807777762413 Validation loss 0.05323250964283943 Accuracy 0.458984375\n",
      "Iteration 73840 Training loss 0.051057230681180954 Validation loss 0.05325733870267868 Accuracy 0.457275390625\n",
      "Iteration 73850 Training loss 0.05040757358074188 Validation loss 0.053147077560424805 Accuracy 0.460693359375\n",
      "Iteration 73860 Training loss 0.05219430848956108 Validation loss 0.05300263687968254 Accuracy 0.4599609375\n",
      "Iteration 73870 Training loss 0.049452319741249084 Validation loss 0.052990954369306564 Accuracy 0.458740234375\n",
      "Iteration 73880 Training loss 0.05204835534095764 Validation loss 0.05302044376730919 Accuracy 0.458984375\n",
      "Iteration 73890 Training loss 0.048083044588565826 Validation loss 0.05298257991671562 Accuracy 0.460693359375\n",
      "Iteration 73900 Training loss 0.04789629206061363 Validation loss 0.05297764390707016 Accuracy 0.46142578125\n",
      "Iteration 73910 Training loss 0.05017031729221344 Validation loss 0.05309482291340828 Accuracy 0.459228515625\n",
      "Iteration 73920 Training loss 0.04900658130645752 Validation loss 0.05296647176146507 Accuracy 0.459716796875\n",
      "Iteration 73930 Training loss 0.05038605257868767 Validation loss 0.05330922082066536 Accuracy 0.458984375\n",
      "Iteration 73940 Training loss 0.05307197943329811 Validation loss 0.053796473890542984 Accuracy 0.45458984375\n",
      "Iteration 73950 Training loss 0.050620727241039276 Validation loss 0.05301821976900101 Accuracy 0.46142578125\n",
      "Iteration 73960 Training loss 0.05161413550376892 Validation loss 0.05318652093410492 Accuracy 0.45947265625\n",
      "Iteration 73970 Training loss 0.052560221403837204 Validation loss 0.05311055853962898 Accuracy 0.460205078125\n",
      "Iteration 73980 Training loss 0.055747050791978836 Validation loss 0.053305741399526596 Accuracy 0.456787109375\n",
      "Iteration 73990 Training loss 0.051842547953128815 Validation loss 0.05365508794784546 Accuracy 0.454345703125\n",
      "Iteration 74000 Training loss 0.047982409596443176 Validation loss 0.05288923531770706 Accuracy 0.461669921875\n",
      "Iteration 74010 Training loss 0.05098753795027733 Validation loss 0.0528748519718647 Accuracy 0.4599609375\n",
      "Iteration 74020 Training loss 0.051353275775909424 Validation loss 0.05297350510954857 Accuracy 0.4599609375\n",
      "Iteration 74030 Training loss 0.0493265800178051 Validation loss 0.052968986332416534 Accuracy 0.460205078125\n",
      "Iteration 74040 Training loss 0.04860701411962509 Validation loss 0.053596317768096924 Accuracy 0.4541015625\n",
      "Iteration 74050 Training loss 0.047790080308914185 Validation loss 0.05309168994426727 Accuracy 0.459716796875\n",
      "Iteration 74060 Training loss 0.05170793458819389 Validation loss 0.05323578044772148 Accuracy 0.458251953125\n",
      "Iteration 74070 Training loss 0.054089803248643875 Validation loss 0.053604643791913986 Accuracy 0.45556640625\n",
      "Iteration 74080 Training loss 0.04884761571884155 Validation loss 0.05316999927163124 Accuracy 0.458984375\n",
      "Iteration 74090 Training loss 0.05107912793755531 Validation loss 0.05327162891626358 Accuracy 0.459716796875\n",
      "Iteration 74100 Training loss 0.05149419978260994 Validation loss 0.053015515208244324 Accuracy 0.45947265625\n",
      "Iteration 74110 Training loss 0.04930601269006729 Validation loss 0.05320523679256439 Accuracy 0.45947265625\n",
      "Iteration 74120 Training loss 0.0512768030166626 Validation loss 0.05353214591741562 Accuracy 0.455078125\n",
      "Iteration 74130 Training loss 0.0519188717007637 Validation loss 0.053477756679058075 Accuracy 0.456298828125\n",
      "Iteration 74140 Training loss 0.050521112978458405 Validation loss 0.0534406341612339 Accuracy 0.457763671875\n",
      "Iteration 74150 Training loss 0.05189383774995804 Validation loss 0.05320478975772858 Accuracy 0.456298828125\n",
      "Iteration 74160 Training loss 0.0523483008146286 Validation loss 0.053534895181655884 Accuracy 0.456298828125\n",
      "Iteration 74170 Training loss 0.053394217044115067 Validation loss 0.05301687493920326 Accuracy 0.45849609375\n",
      "Iteration 74180 Training loss 0.047958698123693466 Validation loss 0.05334916338324547 Accuracy 0.458984375\n",
      "Iteration 74190 Training loss 0.05107753351330757 Validation loss 0.052953366190195084 Accuracy 0.459716796875\n",
      "Iteration 74200 Training loss 0.050737228244543076 Validation loss 0.053212668746709824 Accuracy 0.4580078125\n",
      "Iteration 74210 Training loss 0.04973350837826729 Validation loss 0.052867889404296875 Accuracy 0.461181640625\n",
      "Iteration 74220 Training loss 0.05029122903943062 Validation loss 0.05305236577987671 Accuracy 0.45947265625\n",
      "Iteration 74230 Training loss 0.05064830556511879 Validation loss 0.05327479913830757 Accuracy 0.45849609375\n",
      "Iteration 74240 Training loss 0.05067238211631775 Validation loss 0.05328219756484032 Accuracy 0.458740234375\n",
      "Iteration 74250 Training loss 0.05146629735827446 Validation loss 0.053103476762771606 Accuracy 0.458740234375\n",
      "Iteration 74260 Training loss 0.04990248382091522 Validation loss 0.05289929732680321 Accuracy 0.461181640625\n",
      "Iteration 74270 Training loss 0.05305159464478493 Validation loss 0.05329209193587303 Accuracy 0.458740234375\n",
      "Iteration 74280 Training loss 0.053457483649253845 Validation loss 0.053581878542900085 Accuracy 0.45703125\n",
      "Iteration 74290 Training loss 0.05037470534443855 Validation loss 0.05307454988360405 Accuracy 0.458984375\n",
      "Iteration 74300 Training loss 0.052728116512298584 Validation loss 0.05340313911437988 Accuracy 0.4560546875\n",
      "Iteration 74310 Training loss 0.050795093178749084 Validation loss 0.052936676889657974 Accuracy 0.46044921875\n",
      "Iteration 74320 Training loss 0.05185028910636902 Validation loss 0.05311557650566101 Accuracy 0.459716796875\n",
      "Iteration 74330 Training loss 0.05151214078068733 Validation loss 0.053171005100011826 Accuracy 0.45947265625\n",
      "Iteration 74340 Training loss 0.04945770278573036 Validation loss 0.0528489388525486 Accuracy 0.460205078125\n",
      "Iteration 74350 Training loss 0.054525360465049744 Validation loss 0.05288132652640343 Accuracy 0.46044921875\n",
      "Iteration 74360 Training loss 0.04964565485715866 Validation loss 0.052930671721696854 Accuracy 0.4599609375\n",
      "Iteration 74370 Training loss 0.05192464590072632 Validation loss 0.053170740604400635 Accuracy 0.458984375\n",
      "Iteration 74380 Training loss 0.052795205265283585 Validation loss 0.05294867977499962 Accuracy 0.459716796875\n",
      "Iteration 74390 Training loss 0.05104144290089607 Validation loss 0.053898610174655914 Accuracy 0.455078125\n",
      "Iteration 74400 Training loss 0.049663420766592026 Validation loss 0.053135935217142105 Accuracy 0.46044921875\n",
      "Iteration 74410 Training loss 0.049775030463933945 Validation loss 0.05339500308036804 Accuracy 0.457763671875\n",
      "Iteration 74420 Training loss 0.051753830164670944 Validation loss 0.05308616906404495 Accuracy 0.459716796875\n",
      "Iteration 74430 Training loss 0.05196652561426163 Validation loss 0.053119923919439316 Accuracy 0.460205078125\n",
      "Iteration 74440 Training loss 0.051649127155542374 Validation loss 0.052839651703834534 Accuracy 0.46044921875\n",
      "Iteration 74450 Training loss 0.05016086623072624 Validation loss 0.05289735272526741 Accuracy 0.460205078125\n",
      "Iteration 74460 Training loss 0.04871460050344467 Validation loss 0.05318935588002205 Accuracy 0.458984375\n",
      "Iteration 74470 Training loss 0.05382021889090538 Validation loss 0.053241971880197525 Accuracy 0.4599609375\n",
      "Iteration 74480 Training loss 0.05191351845860481 Validation loss 0.05298430472612381 Accuracy 0.459716796875\n",
      "Iteration 74490 Training loss 0.050117913633584976 Validation loss 0.05296342819929123 Accuracy 0.461181640625\n",
      "Iteration 74500 Training loss 0.052737899124622345 Validation loss 0.053188446909189224 Accuracy 0.45849609375\n",
      "Iteration 74510 Training loss 0.05014156922698021 Validation loss 0.05301954597234726 Accuracy 0.460205078125\n",
      "Iteration 74520 Training loss 0.053002338856458664 Validation loss 0.05317683517932892 Accuracy 0.4609375\n",
      "Iteration 74530 Training loss 0.05195346102118492 Validation loss 0.052764881402254105 Accuracy 0.461181640625\n",
      "Iteration 74540 Training loss 0.04718596488237381 Validation loss 0.05304431542754173 Accuracy 0.4609375\n",
      "Iteration 74550 Training loss 0.053325895220041275 Validation loss 0.053194306790828705 Accuracy 0.456787109375\n",
      "Iteration 74560 Training loss 0.04847513884305954 Validation loss 0.053714826703071594 Accuracy 0.452392578125\n",
      "Iteration 74570 Training loss 0.05196763947606087 Validation loss 0.05295299366116524 Accuracy 0.460693359375\n",
      "Iteration 74580 Training loss 0.05173030495643616 Validation loss 0.05286116153001785 Accuracy 0.460693359375\n",
      "Iteration 74590 Training loss 0.05086425319314003 Validation loss 0.0533793605864048 Accuracy 0.459228515625\n",
      "Iteration 74600 Training loss 0.04867773875594139 Validation loss 0.05327572301030159 Accuracy 0.458984375\n",
      "Iteration 74610 Training loss 0.051388904452323914 Validation loss 0.053288739174604416 Accuracy 0.458984375\n",
      "Iteration 74620 Training loss 0.05318372696638107 Validation loss 0.05324254930019379 Accuracy 0.459716796875\n",
      "Iteration 74630 Training loss 0.053260497748851776 Validation loss 0.05286026373505592 Accuracy 0.460693359375\n",
      "Iteration 74640 Training loss 0.04933435842394829 Validation loss 0.053387634456157684 Accuracy 0.458984375\n",
      "Iteration 74650 Training loss 0.05030619725584984 Validation loss 0.053042128682136536 Accuracy 0.458984375\n",
      "Iteration 74660 Training loss 0.05191124230623245 Validation loss 0.05299215763807297 Accuracy 0.4609375\n",
      "Iteration 74670 Training loss 0.04697434604167938 Validation loss 0.053067468106746674 Accuracy 0.460205078125\n",
      "Iteration 74680 Training loss 0.053376320749521255 Validation loss 0.05324667692184448 Accuracy 0.459228515625\n",
      "Iteration 74690 Training loss 0.051353223621845245 Validation loss 0.05321632698178291 Accuracy 0.459228515625\n",
      "Iteration 74700 Training loss 0.05009955167770386 Validation loss 0.0532170869410038 Accuracy 0.459716796875\n",
      "Iteration 74710 Training loss 0.049290914088487625 Validation loss 0.05279289558529854 Accuracy 0.461181640625\n",
      "Iteration 74720 Training loss 0.05037585273385048 Validation loss 0.05310717225074768 Accuracy 0.459716796875\n",
      "Iteration 74730 Training loss 0.05080223083496094 Validation loss 0.05270695313811302 Accuracy 0.4619140625\n",
      "Iteration 74740 Training loss 0.05226021260023117 Validation loss 0.0531231053173542 Accuracy 0.458984375\n",
      "Iteration 74750 Training loss 0.051830992102622986 Validation loss 0.05296255648136139 Accuracy 0.459228515625\n",
      "Iteration 74760 Training loss 0.04928210377693176 Validation loss 0.052887219935655594 Accuracy 0.460693359375\n",
      "Iteration 74770 Training loss 0.0528101846575737 Validation loss 0.05278531461954117 Accuracy 0.4609375\n",
      "Iteration 74780 Training loss 0.050657693296670914 Validation loss 0.0533982515335083 Accuracy 0.457275390625\n",
      "Iteration 74790 Training loss 0.05135402828454971 Validation loss 0.052939463406801224 Accuracy 0.461669921875\n",
      "Iteration 74800 Training loss 0.051094405353069305 Validation loss 0.05298679694533348 Accuracy 0.460205078125\n",
      "Iteration 74810 Training loss 0.05260950326919556 Validation loss 0.05310861021280289 Accuracy 0.460693359375\n",
      "Iteration 74820 Training loss 0.0511549711227417 Validation loss 0.05342501401901245 Accuracy 0.458984375\n",
      "Iteration 74830 Training loss 0.04896941035985947 Validation loss 0.05347689241170883 Accuracy 0.45751953125\n",
      "Iteration 74840 Training loss 0.05111173540353775 Validation loss 0.052731458097696304 Accuracy 0.46142578125\n",
      "Iteration 74850 Training loss 0.052987609058618546 Validation loss 0.05308975279331207 Accuracy 0.460205078125\n",
      "Iteration 74860 Training loss 0.04704290255904198 Validation loss 0.05307665467262268 Accuracy 0.45947265625\n",
      "Iteration 74870 Training loss 0.05110982060432434 Validation loss 0.052912674844264984 Accuracy 0.4599609375\n",
      "Iteration 74880 Training loss 0.05148350074887276 Validation loss 0.05309182405471802 Accuracy 0.460205078125\n",
      "Iteration 74890 Training loss 0.050515249371528625 Validation loss 0.05309315398335457 Accuracy 0.460693359375\n",
      "Iteration 74900 Training loss 0.04978293180465698 Validation loss 0.052831586450338364 Accuracy 0.461181640625\n",
      "Iteration 74910 Training loss 0.05059470608830452 Validation loss 0.053350672125816345 Accuracy 0.458984375\n",
      "Iteration 74920 Training loss 0.05235955864191055 Validation loss 0.052858300507068634 Accuracy 0.461181640625\n",
      "Iteration 74930 Training loss 0.050136689096689224 Validation loss 0.05282684415578842 Accuracy 0.461181640625\n",
      "Iteration 74940 Training loss 0.051291339099407196 Validation loss 0.053564876317977905 Accuracy 0.456298828125\n",
      "Iteration 74950 Training loss 0.054302677512168884 Validation loss 0.05343882367014885 Accuracy 0.45703125\n",
      "Iteration 74960 Training loss 0.051971327513456345 Validation loss 0.05280772224068642 Accuracy 0.4619140625\n",
      "Iteration 74970 Training loss 0.05035459250211716 Validation loss 0.05330405756831169 Accuracy 0.458984375\n",
      "Iteration 74980 Training loss 0.05233864113688469 Validation loss 0.05298691615462303 Accuracy 0.460205078125\n",
      "Iteration 74990 Training loss 0.04986019805073738 Validation loss 0.0538272000849247 Accuracy 0.453369140625\n",
      "Iteration 75000 Training loss 0.049788396805524826 Validation loss 0.05281095951795578 Accuracy 0.4619140625\n",
      "Iteration 75010 Training loss 0.05093136057257652 Validation loss 0.05281434580683708 Accuracy 0.461181640625\n",
      "Iteration 75020 Training loss 0.048337869346141815 Validation loss 0.0533665306866169 Accuracy 0.458251953125\n",
      "Iteration 75030 Training loss 0.05282178521156311 Validation loss 0.053165238350629807 Accuracy 0.460205078125\n",
      "Iteration 75040 Training loss 0.05128147453069687 Validation loss 0.05311622843146324 Accuracy 0.460205078125\n",
      "Iteration 75050 Training loss 0.05388416349887848 Validation loss 0.05339321121573448 Accuracy 0.4580078125\n",
      "Iteration 75060 Training loss 0.05096886307001114 Validation loss 0.05330387130379677 Accuracy 0.458984375\n",
      "Iteration 75070 Training loss 0.05055266246199608 Validation loss 0.05318286642432213 Accuracy 0.459716796875\n",
      "Iteration 75080 Training loss 0.051188189536333084 Validation loss 0.05295285955071449 Accuracy 0.45947265625\n",
      "Iteration 75090 Training loss 0.053047239780426025 Validation loss 0.05356918275356293 Accuracy 0.4580078125\n",
      "Iteration 75100 Training loss 0.05193546414375305 Validation loss 0.053082410246133804 Accuracy 0.460205078125\n",
      "Iteration 75110 Training loss 0.04999026283621788 Validation loss 0.05332035571336746 Accuracy 0.458984375\n",
      "Iteration 75120 Training loss 0.051210056990385056 Validation loss 0.05317099019885063 Accuracy 0.460205078125\n",
      "Iteration 75130 Training loss 0.05482165515422821 Validation loss 0.05293412134051323 Accuracy 0.46044921875\n",
      "Iteration 75140 Training loss 0.04783810302615166 Validation loss 0.05296321213245392 Accuracy 0.4599609375\n",
      "Iteration 75150 Training loss 0.04881848022341728 Validation loss 0.053110722452402115 Accuracy 0.45947265625\n",
      "Iteration 75160 Training loss 0.05184575915336609 Validation loss 0.05327104777097702 Accuracy 0.460205078125\n",
      "Iteration 75170 Training loss 0.04885992407798767 Validation loss 0.053256258368492126 Accuracy 0.459716796875\n",
      "Iteration 75180 Training loss 0.04905184358358383 Validation loss 0.0531863272190094 Accuracy 0.459716796875\n",
      "Iteration 75190 Training loss 0.05491182953119278 Validation loss 0.05320001393556595 Accuracy 0.456787109375\n",
      "Iteration 75200 Training loss 0.048884809017181396 Validation loss 0.05305592715740204 Accuracy 0.458984375\n",
      "Iteration 75210 Training loss 0.052664387971162796 Validation loss 0.05349620804190636 Accuracy 0.458740234375\n",
      "Iteration 75220 Training loss 0.04963894933462143 Validation loss 0.05353591963648796 Accuracy 0.4560546875\n",
      "Iteration 75230 Training loss 0.05167781189084053 Validation loss 0.053446535021066666 Accuracy 0.45654296875\n",
      "Iteration 75240 Training loss 0.05042945221066475 Validation loss 0.05337651073932648 Accuracy 0.457275390625\n",
      "Iteration 75250 Training loss 0.049385033547878265 Validation loss 0.05303730443120003 Accuracy 0.460205078125\n",
      "Iteration 75260 Training loss 0.05363225191831589 Validation loss 0.0533423125743866 Accuracy 0.456787109375\n",
      "Iteration 75270 Training loss 0.048288628458976746 Validation loss 0.05287865549325943 Accuracy 0.460205078125\n",
      "Iteration 75280 Training loss 0.05211279168725014 Validation loss 0.05412575602531433 Accuracy 0.452392578125\n",
      "Iteration 75290 Training loss 0.04749200493097305 Validation loss 0.05318888649344444 Accuracy 0.45947265625\n",
      "Iteration 75300 Training loss 0.05056615546345711 Validation loss 0.05303921923041344 Accuracy 0.45849609375\n",
      "Iteration 75310 Training loss 0.051371123641729355 Validation loss 0.05320916697382927 Accuracy 0.45947265625\n",
      "Iteration 75320 Training loss 0.05344171077013016 Validation loss 0.053414177149534225 Accuracy 0.4580078125\n",
      "Iteration 75330 Training loss 0.04808180034160614 Validation loss 0.0527513362467289 Accuracy 0.462158203125\n",
      "Iteration 75340 Training loss 0.04764946922659874 Validation loss 0.05328524112701416 Accuracy 0.45947265625\n",
      "Iteration 75350 Training loss 0.04830830916762352 Validation loss 0.05267295613884926 Accuracy 0.462158203125\n",
      "Iteration 75360 Training loss 0.051916997879743576 Validation loss 0.0529724545776844 Accuracy 0.460693359375\n",
      "Iteration 75370 Training loss 0.04973762854933739 Validation loss 0.05290057510137558 Accuracy 0.460693359375\n",
      "Iteration 75380 Training loss 0.05056612193584442 Validation loss 0.05346669629216194 Accuracy 0.45556640625\n",
      "Iteration 75390 Training loss 0.0526585690677166 Validation loss 0.053065065294504166 Accuracy 0.459716796875\n",
      "Iteration 75400 Training loss 0.05150835961103439 Validation loss 0.053406618535518646 Accuracy 0.457763671875\n",
      "Iteration 75410 Training loss 0.050786927342414856 Validation loss 0.053227607160806656 Accuracy 0.459228515625\n",
      "Iteration 75420 Training loss 0.04970947280526161 Validation loss 0.05301450192928314 Accuracy 0.460693359375\n",
      "Iteration 75430 Training loss 0.0506831556558609 Validation loss 0.05330095440149307 Accuracy 0.4580078125\n",
      "Iteration 75440 Training loss 0.051690444350242615 Validation loss 0.05375690385699272 Accuracy 0.45458984375\n",
      "Iteration 75450 Training loss 0.05242371931672096 Validation loss 0.0533948577940464 Accuracy 0.45849609375\n",
      "Iteration 75460 Training loss 0.04788713902235031 Validation loss 0.05287700146436691 Accuracy 0.460693359375\n",
      "Iteration 75470 Training loss 0.05082565173506737 Validation loss 0.053115516901016235 Accuracy 0.459716796875\n",
      "Iteration 75480 Training loss 0.048756226897239685 Validation loss 0.05320289731025696 Accuracy 0.45849609375\n",
      "Iteration 75490 Training loss 0.05152798816561699 Validation loss 0.05288567394018173 Accuracy 0.45947265625\n",
      "Iteration 75500 Training loss 0.049632538110017776 Validation loss 0.052925072610378265 Accuracy 0.4599609375\n",
      "Iteration 75510 Training loss 0.049317367374897 Validation loss 0.05313612148165703 Accuracy 0.46044921875\n",
      "Iteration 75520 Training loss 0.054127734154462814 Validation loss 0.05315651372075081 Accuracy 0.459716796875\n",
      "Iteration 75530 Training loss 0.05365942418575287 Validation loss 0.05355402082204819 Accuracy 0.456787109375\n",
      "Iteration 75540 Training loss 0.0510164350271225 Validation loss 0.052889637649059296 Accuracy 0.461181640625\n",
      "Iteration 75550 Training loss 0.04746530205011368 Validation loss 0.05304776132106781 Accuracy 0.459716796875\n",
      "Iteration 75560 Training loss 0.053203556686639786 Validation loss 0.053122907876968384 Accuracy 0.460205078125\n",
      "Iteration 75570 Training loss 0.04804704338312149 Validation loss 0.05320098623633385 Accuracy 0.45751953125\n",
      "Iteration 75580 Training loss 0.05016489699482918 Validation loss 0.05330974608659744 Accuracy 0.460205078125\n",
      "Iteration 75590 Training loss 0.05208146944642067 Validation loss 0.05327175557613373 Accuracy 0.459716796875\n",
      "Iteration 75600 Training loss 0.0525086373090744 Validation loss 0.05306418240070343 Accuracy 0.45947265625\n",
      "Iteration 75610 Training loss 0.05050617828965187 Validation loss 0.053106267005205154 Accuracy 0.459228515625\n",
      "Iteration 75620 Training loss 0.04999665170907974 Validation loss 0.053134020417928696 Accuracy 0.459228515625\n",
      "Iteration 75630 Training loss 0.049269694834947586 Validation loss 0.052818622440099716 Accuracy 0.461181640625\n",
      "Iteration 75640 Training loss 0.050581272691488266 Validation loss 0.05298076197504997 Accuracy 0.460693359375\n",
      "Iteration 75650 Training loss 0.04758254066109657 Validation loss 0.052797090262174606 Accuracy 0.460693359375\n",
      "Iteration 75660 Training loss 0.05124518647789955 Validation loss 0.05317931994795799 Accuracy 0.459228515625\n",
      "Iteration 75670 Training loss 0.050301045179367065 Validation loss 0.053082991391420364 Accuracy 0.461181640625\n",
      "Iteration 75680 Training loss 0.05209995433688164 Validation loss 0.05306542292237282 Accuracy 0.46044921875\n",
      "Iteration 75690 Training loss 0.04896128922700882 Validation loss 0.05355554446578026 Accuracy 0.456787109375\n",
      "Iteration 75700 Training loss 0.052458133548498154 Validation loss 0.05302418768405914 Accuracy 0.461181640625\n",
      "Iteration 75710 Training loss 0.05158768594264984 Validation loss 0.05300743132829666 Accuracy 0.460693359375\n",
      "Iteration 75720 Training loss 0.05147572606801987 Validation loss 0.05292561277747154 Accuracy 0.461181640625\n",
      "Iteration 75730 Training loss 0.048484645783901215 Validation loss 0.05312472581863403 Accuracy 0.45849609375\n",
      "Iteration 75740 Training loss 0.04988589510321617 Validation loss 0.05284017324447632 Accuracy 0.460205078125\n",
      "Iteration 75750 Training loss 0.0530531220138073 Validation loss 0.05297563225030899 Accuracy 0.460205078125\n",
      "Iteration 75760 Training loss 0.048875946551561356 Validation loss 0.05291364714503288 Accuracy 0.460693359375\n",
      "Iteration 75770 Training loss 0.052195459604263306 Validation loss 0.053150419145822525 Accuracy 0.458984375\n",
      "Iteration 75780 Training loss 0.049458302557468414 Validation loss 0.05318581312894821 Accuracy 0.459716796875\n",
      "Iteration 75790 Training loss 0.05235084146261215 Validation loss 0.05314941331744194 Accuracy 0.45947265625\n",
      "Iteration 75800 Training loss 0.05100516602396965 Validation loss 0.05289807543158531 Accuracy 0.4609375\n",
      "Iteration 75810 Training loss 0.05132666975259781 Validation loss 0.052930574864149094 Accuracy 0.459716796875\n",
      "Iteration 75820 Training loss 0.05308955907821655 Validation loss 0.053186140954494476 Accuracy 0.4599609375\n",
      "Iteration 75830 Training loss 0.05135653540492058 Validation loss 0.053445398807525635 Accuracy 0.45849609375\n",
      "Iteration 75840 Training loss 0.05264044180512428 Validation loss 0.052883513271808624 Accuracy 0.460205078125\n",
      "Iteration 75850 Training loss 0.049678150564432144 Validation loss 0.05297449231147766 Accuracy 0.4599609375\n",
      "Iteration 75860 Training loss 0.048152994364500046 Validation loss 0.053047917783260345 Accuracy 0.45947265625\n",
      "Iteration 75870 Training loss 0.049234624952077866 Validation loss 0.05283356457948685 Accuracy 0.4609375\n",
      "Iteration 75880 Training loss 0.05029919371008873 Validation loss 0.05346100032329559 Accuracy 0.45849609375\n",
      "Iteration 75890 Training loss 0.050527140498161316 Validation loss 0.05271584913134575 Accuracy 0.461669921875\n",
      "Iteration 75900 Training loss 0.05096814036369324 Validation loss 0.05289394035935402 Accuracy 0.460693359375\n",
      "Iteration 75910 Training loss 0.04771047085523605 Validation loss 0.05329545587301254 Accuracy 0.459716796875\n",
      "Iteration 75920 Training loss 0.05101907253265381 Validation loss 0.05300755798816681 Accuracy 0.458984375\n",
      "Iteration 75930 Training loss 0.049955274909734726 Validation loss 0.0530194528400898 Accuracy 0.459716796875\n",
      "Iteration 75940 Training loss 0.05284683778882027 Validation loss 0.05280616879463196 Accuracy 0.46142578125\n",
      "Iteration 75950 Training loss 0.05285395681858063 Validation loss 0.05292526260018349 Accuracy 0.460693359375\n",
      "Iteration 75960 Training loss 0.05016731843352318 Validation loss 0.053146395832300186 Accuracy 0.459228515625\n",
      "Iteration 75970 Training loss 0.05018962174654007 Validation loss 0.053221073001623154 Accuracy 0.460693359375\n",
      "Iteration 75980 Training loss 0.050950560718774796 Validation loss 0.05353328958153725 Accuracy 0.454833984375\n",
      "Iteration 75990 Training loss 0.04761887714266777 Validation loss 0.053399305790662766 Accuracy 0.45703125\n",
      "Iteration 76000 Training loss 0.049834005534648895 Validation loss 0.05296238511800766 Accuracy 0.460205078125\n",
      "Iteration 76010 Training loss 0.049610741436481476 Validation loss 0.05335337668657303 Accuracy 0.456298828125\n",
      "Iteration 76020 Training loss 0.051822222769260406 Validation loss 0.05275612697005272 Accuracy 0.461181640625\n",
      "Iteration 76030 Training loss 0.04905586317181587 Validation loss 0.05306117236614227 Accuracy 0.459716796875\n",
      "Iteration 76040 Training loss 0.04885353147983551 Validation loss 0.052837781608104706 Accuracy 0.460693359375\n",
      "Iteration 76050 Training loss 0.05113949626684189 Validation loss 0.05287642776966095 Accuracy 0.461669921875\n",
      "Iteration 76060 Training loss 0.0518149808049202 Validation loss 0.05279009789228439 Accuracy 0.46142578125\n",
      "Iteration 76070 Training loss 0.05136314779520035 Validation loss 0.053322166204452515 Accuracy 0.45849609375\n",
      "Iteration 76080 Training loss 0.05128597840666771 Validation loss 0.05283502861857414 Accuracy 0.460205078125\n",
      "Iteration 76090 Training loss 0.0503043606877327 Validation loss 0.053090900182724 Accuracy 0.45947265625\n",
      "Iteration 76100 Training loss 0.05275913327932358 Validation loss 0.05333233252167702 Accuracy 0.457763671875\n",
      "Iteration 76110 Training loss 0.05156403407454491 Validation loss 0.05304129421710968 Accuracy 0.460205078125\n",
      "Iteration 76120 Training loss 0.04960000887513161 Validation loss 0.05308074131608009 Accuracy 0.460205078125\n",
      "Iteration 76130 Training loss 0.05495259538292885 Validation loss 0.05319643393158913 Accuracy 0.459716796875\n",
      "Iteration 76140 Training loss 0.05342472344636917 Validation loss 0.05307798832654953 Accuracy 0.460693359375\n",
      "Iteration 76150 Training loss 0.04964090883731842 Validation loss 0.053280457854270935 Accuracy 0.458984375\n",
      "Iteration 76160 Training loss 0.05108724534511566 Validation loss 0.05345320329070091 Accuracy 0.455810546875\n",
      "Iteration 76170 Training loss 0.05087454617023468 Validation loss 0.05310271307826042 Accuracy 0.459716796875\n",
      "Iteration 76180 Training loss 0.0498858243227005 Validation loss 0.05360540747642517 Accuracy 0.457275390625\n",
      "Iteration 76190 Training loss 0.048312533646821976 Validation loss 0.05310117080807686 Accuracy 0.460693359375\n",
      "Iteration 76200 Training loss 0.05416559800505638 Validation loss 0.0536411888897419 Accuracy 0.455322265625\n",
      "Iteration 76210 Training loss 0.05156007781624794 Validation loss 0.05311908200383186 Accuracy 0.45947265625\n",
      "Iteration 76220 Training loss 0.05293167009949684 Validation loss 0.053022757172584534 Accuracy 0.461669921875\n",
      "Iteration 76230 Training loss 0.05233721807599068 Validation loss 0.05345401540398598 Accuracy 0.4560546875\n",
      "Iteration 76240 Training loss 0.04920974001288414 Validation loss 0.05295230448246002 Accuracy 0.461181640625\n",
      "Iteration 76250 Training loss 0.05105119198560715 Validation loss 0.053106654435396194 Accuracy 0.460205078125\n",
      "Iteration 76260 Training loss 0.0539361871778965 Validation loss 0.05395039916038513 Accuracy 0.453125\n",
      "Iteration 76270 Training loss 0.05349552258849144 Validation loss 0.05388268455862999 Accuracy 0.449462890625\n",
      "Iteration 76280 Training loss 0.049584656953811646 Validation loss 0.053025100380182266 Accuracy 0.46044921875\n",
      "Iteration 76290 Training loss 0.04960574954748154 Validation loss 0.053426072001457214 Accuracy 0.457763671875\n",
      "Iteration 76300 Training loss 0.04874145984649658 Validation loss 0.05292355641722679 Accuracy 0.460205078125\n",
      "Iteration 76310 Training loss 0.05264684930443764 Validation loss 0.05278346687555313 Accuracy 0.462158203125\n",
      "Iteration 76320 Training loss 0.053321726620197296 Validation loss 0.052891530096530914 Accuracy 0.46044921875\n",
      "Iteration 76330 Training loss 0.05131065845489502 Validation loss 0.05321046710014343 Accuracy 0.45947265625\n",
      "Iteration 76340 Training loss 0.05291333422064781 Validation loss 0.05359257757663727 Accuracy 0.455322265625\n",
      "Iteration 76350 Training loss 0.05103239417076111 Validation loss 0.053595710545778275 Accuracy 0.4580078125\n",
      "Iteration 76360 Training loss 0.04961470142006874 Validation loss 0.053184375166893005 Accuracy 0.456787109375\n",
      "Iteration 76370 Training loss 0.05020561069250107 Validation loss 0.053093548864126205 Accuracy 0.460205078125\n",
      "Iteration 76380 Training loss 0.05117524042725563 Validation loss 0.05358409136533737 Accuracy 0.455810546875\n",
      "Iteration 76390 Training loss 0.04929755628108978 Validation loss 0.053172655403614044 Accuracy 0.45751953125\n",
      "Iteration 76400 Training loss 0.050056811422109604 Validation loss 0.05313877761363983 Accuracy 0.46044921875\n",
      "Iteration 76410 Training loss 0.04947507008910179 Validation loss 0.05325605720281601 Accuracy 0.458740234375\n",
      "Iteration 76420 Training loss 0.04858896881341934 Validation loss 0.053378913551568985 Accuracy 0.45849609375\n",
      "Iteration 76430 Training loss 0.05122813209891319 Validation loss 0.05339251831173897 Accuracy 0.457763671875\n",
      "Iteration 76440 Training loss 0.050004053860902786 Validation loss 0.05300440266728401 Accuracy 0.46044921875\n",
      "Iteration 76450 Training loss 0.052854228764772415 Validation loss 0.05312519893050194 Accuracy 0.4599609375\n",
      "Iteration 76460 Training loss 0.05363461375236511 Validation loss 0.05308588221669197 Accuracy 0.460693359375\n",
      "Iteration 76470 Training loss 0.04900483042001724 Validation loss 0.053189706057310104 Accuracy 0.45751953125\n",
      "Iteration 76480 Training loss 0.05256155505776405 Validation loss 0.05331335961818695 Accuracy 0.45849609375\n",
      "Iteration 76490 Training loss 0.05173279717564583 Validation loss 0.05303533002734184 Accuracy 0.46044921875\n",
      "Iteration 76500 Training loss 0.05061972513794899 Validation loss 0.05315644294023514 Accuracy 0.460205078125\n",
      "Iteration 76510 Training loss 0.053525879979133606 Validation loss 0.05380960926413536 Accuracy 0.453369140625\n",
      "Iteration 76520 Training loss 0.0518667995929718 Validation loss 0.05320851504802704 Accuracy 0.45751953125\n",
      "Iteration 76530 Training loss 0.04988129809498787 Validation loss 0.05321299657225609 Accuracy 0.460693359375\n",
      "Iteration 76540 Training loss 0.04784354940056801 Validation loss 0.053089968860149384 Accuracy 0.460205078125\n",
      "Iteration 76550 Training loss 0.050865232944488525 Validation loss 0.05318659543991089 Accuracy 0.460693359375\n",
      "Iteration 76560 Training loss 0.04977831989526749 Validation loss 0.05310036987066269 Accuracy 0.46142578125\n",
      "Iteration 76570 Training loss 0.05036057159304619 Validation loss 0.05320952460169792 Accuracy 0.4599609375\n",
      "Iteration 76580 Training loss 0.050350021570920944 Validation loss 0.053116243332624435 Accuracy 0.460693359375\n",
      "Iteration 76590 Training loss 0.050458766520023346 Validation loss 0.053055018186569214 Accuracy 0.460205078125\n",
      "Iteration 76600 Training loss 0.052173808217048645 Validation loss 0.05307576060295105 Accuracy 0.458251953125\n",
      "Iteration 76610 Training loss 0.04821370542049408 Validation loss 0.05309708043932915 Accuracy 0.4599609375\n",
      "Iteration 76620 Training loss 0.05036157742142677 Validation loss 0.05304272472858429 Accuracy 0.460693359375\n",
      "Iteration 76630 Training loss 0.051819391548633575 Validation loss 0.05319462716579437 Accuracy 0.458984375\n",
      "Iteration 76640 Training loss 0.0511249378323555 Validation loss 0.052787043154239655 Accuracy 0.4619140625\n",
      "Iteration 76650 Training loss 0.05249638855457306 Validation loss 0.05358252674341202 Accuracy 0.455810546875\n",
      "Iteration 76660 Training loss 0.052709970623254776 Validation loss 0.052824560552835464 Accuracy 0.4609375\n",
      "Iteration 76670 Training loss 0.04811806604266167 Validation loss 0.05306582897901535 Accuracy 0.4580078125\n",
      "Iteration 76680 Training loss 0.05144314467906952 Validation loss 0.05310504883527756 Accuracy 0.460205078125\n",
      "Iteration 76690 Training loss 0.050561826676130295 Validation loss 0.05284944176673889 Accuracy 0.460693359375\n",
      "Iteration 76700 Training loss 0.05169859156012535 Validation loss 0.05321095883846283 Accuracy 0.456787109375\n",
      "Iteration 76710 Training loss 0.05214103311300278 Validation loss 0.0532815121114254 Accuracy 0.460205078125\n",
      "Iteration 76720 Training loss 0.057569365948438644 Validation loss 0.053353339433670044 Accuracy 0.459228515625\n",
      "Iteration 76730 Training loss 0.048380717635154724 Validation loss 0.053355418145656586 Accuracy 0.45751953125\n",
      "Iteration 76740 Training loss 0.053302012383937836 Validation loss 0.053106069564819336 Accuracy 0.4609375\n",
      "Iteration 76750 Training loss 0.04737081378698349 Validation loss 0.05315544456243515 Accuracy 0.459716796875\n",
      "Iteration 76760 Training loss 0.04574718326330185 Validation loss 0.05282578617334366 Accuracy 0.461181640625\n",
      "Iteration 76770 Training loss 0.04881763085722923 Validation loss 0.05290072038769722 Accuracy 0.4609375\n",
      "Iteration 76780 Training loss 0.05257048457860947 Validation loss 0.05358840152621269 Accuracy 0.4541015625\n",
      "Iteration 76790 Training loss 0.05028777942061424 Validation loss 0.053189851343631744 Accuracy 0.4599609375\n",
      "Iteration 76800 Training loss 0.05156905949115753 Validation loss 0.05315549671649933 Accuracy 0.458984375\n",
      "Iteration 76810 Training loss 0.05291198194026947 Validation loss 0.05316217616200447 Accuracy 0.460205078125\n",
      "Iteration 76820 Training loss 0.051084909588098526 Validation loss 0.05347338691353798 Accuracy 0.45947265625\n",
      "Iteration 76830 Training loss 0.04890711233019829 Validation loss 0.05312078073620796 Accuracy 0.46044921875\n",
      "Iteration 76840 Training loss 0.05187099799513817 Validation loss 0.05292019248008728 Accuracy 0.459716796875\n",
      "Iteration 76850 Training loss 0.05174608901143074 Validation loss 0.05320647358894348 Accuracy 0.459716796875\n",
      "Iteration 76860 Training loss 0.051399942487478256 Validation loss 0.05306658148765564 Accuracy 0.45947265625\n",
      "Iteration 76870 Training loss 0.051276884973049164 Validation loss 0.0530032180249691 Accuracy 0.46044921875\n",
      "Iteration 76880 Training loss 0.049157146364450455 Validation loss 0.05286705493927002 Accuracy 0.461181640625\n",
      "Iteration 76890 Training loss 0.049965936690568924 Validation loss 0.05292584374547005 Accuracy 0.4609375\n",
      "Iteration 76900 Training loss 0.052195433527231216 Validation loss 0.05283380672335625 Accuracy 0.461669921875\n",
      "Iteration 76910 Training loss 0.04692696034908295 Validation loss 0.05334484204649925 Accuracy 0.45947265625\n",
      "Iteration 76920 Training loss 0.050553638488054276 Validation loss 0.05309151113033295 Accuracy 0.458984375\n",
      "Iteration 76930 Training loss 0.05047876760363579 Validation loss 0.05288733169436455 Accuracy 0.46142578125\n",
      "Iteration 76940 Training loss 0.049508851021528244 Validation loss 0.052811190485954285 Accuracy 0.46142578125\n",
      "Iteration 76950 Training loss 0.05114086717367172 Validation loss 0.053127869963645935 Accuracy 0.458740234375\n",
      "Iteration 76960 Training loss 0.047781217843294144 Validation loss 0.053037963807582855 Accuracy 0.459716796875\n",
      "Iteration 76970 Training loss 0.048365045338869095 Validation loss 0.05309788137674332 Accuracy 0.458740234375\n",
      "Iteration 76980 Training loss 0.05084040388464928 Validation loss 0.052969105541706085 Accuracy 0.461181640625\n",
      "Iteration 76990 Training loss 0.04906214773654938 Validation loss 0.053033750504255295 Accuracy 0.459716796875\n",
      "Iteration 77000 Training loss 0.053256694227457047 Validation loss 0.05334840714931488 Accuracy 0.459228515625\n",
      "Iteration 77010 Training loss 0.05076805502176285 Validation loss 0.053016386926174164 Accuracy 0.458984375\n",
      "Iteration 77020 Training loss 0.04645801708102226 Validation loss 0.05262983217835426 Accuracy 0.462646484375\n",
      "Iteration 77030 Training loss 0.05090387910604477 Validation loss 0.05285348370671272 Accuracy 0.462158203125\n",
      "Iteration 77040 Training loss 0.049560170620679855 Validation loss 0.05312022939324379 Accuracy 0.459716796875\n",
      "Iteration 77050 Training loss 0.050586502999067307 Validation loss 0.05282177776098251 Accuracy 0.4609375\n",
      "Iteration 77060 Training loss 0.05313611403107643 Validation loss 0.05387909710407257 Accuracy 0.452880859375\n",
      "Iteration 77070 Training loss 0.053796183317899704 Validation loss 0.052952829748392105 Accuracy 0.461181640625\n",
      "Iteration 77080 Training loss 0.04925842583179474 Validation loss 0.0529981292784214 Accuracy 0.458984375\n",
      "Iteration 77090 Training loss 0.048783719539642334 Validation loss 0.0529366098344326 Accuracy 0.461669921875\n",
      "Iteration 77100 Training loss 0.04649430140852928 Validation loss 0.05299244821071625 Accuracy 0.460205078125\n",
      "Iteration 77110 Training loss 0.04943642020225525 Validation loss 0.05281267315149307 Accuracy 0.460693359375\n",
      "Iteration 77120 Training loss 0.050230808556079865 Validation loss 0.05320093780755997 Accuracy 0.458740234375\n",
      "Iteration 77130 Training loss 0.04862360656261444 Validation loss 0.05290081351995468 Accuracy 0.4599609375\n",
      "Iteration 77140 Training loss 0.05620953440666199 Validation loss 0.052852846682071686 Accuracy 0.45947265625\n",
      "Iteration 77150 Training loss 0.05213000252842903 Validation loss 0.05282272771000862 Accuracy 0.461669921875\n",
      "Iteration 77160 Training loss 0.04900547116994858 Validation loss 0.052846845239400864 Accuracy 0.460693359375\n",
      "Iteration 77170 Training loss 0.05203426629304886 Validation loss 0.05296291783452034 Accuracy 0.461181640625\n",
      "Iteration 77180 Training loss 0.05117782950401306 Validation loss 0.05310475453734398 Accuracy 0.458984375\n",
      "Iteration 77190 Training loss 0.050947561860084534 Validation loss 0.05373230203986168 Accuracy 0.45556640625\n",
      "Iteration 77200 Training loss 0.050781309604644775 Validation loss 0.05315937474370003 Accuracy 0.45947265625\n",
      "Iteration 77210 Training loss 0.05247873812913895 Validation loss 0.05353136733174324 Accuracy 0.4541015625\n",
      "Iteration 77220 Training loss 0.054526910185813904 Validation loss 0.05284459516406059 Accuracy 0.461181640625\n",
      "Iteration 77230 Training loss 0.05130269378423691 Validation loss 0.05316350609064102 Accuracy 0.458251953125\n",
      "Iteration 77240 Training loss 0.05012938007712364 Validation loss 0.053027860820293427 Accuracy 0.460205078125\n",
      "Iteration 77250 Training loss 0.05347485840320587 Validation loss 0.05327953025698662 Accuracy 0.457275390625\n",
      "Iteration 77260 Training loss 0.05104391276836395 Validation loss 0.05294615030288696 Accuracy 0.459716796875\n",
      "Iteration 77270 Training loss 0.050631601363420486 Validation loss 0.05322635918855667 Accuracy 0.4580078125\n",
      "Iteration 77280 Training loss 0.0529349111020565 Validation loss 0.053194060921669006 Accuracy 0.4599609375\n",
      "Iteration 77290 Training loss 0.053373854607343674 Validation loss 0.053195029497146606 Accuracy 0.4580078125\n",
      "Iteration 77300 Training loss 0.05481930077075958 Validation loss 0.05287270247936249 Accuracy 0.461669921875\n",
      "Iteration 77310 Training loss 0.05202357470989227 Validation loss 0.05295354500412941 Accuracy 0.460205078125\n",
      "Iteration 77320 Training loss 0.04880029335618019 Validation loss 0.053518425673246384 Accuracy 0.456787109375\n",
      "Iteration 77330 Training loss 0.05091027915477753 Validation loss 0.0531574971973896 Accuracy 0.460205078125\n",
      "Iteration 77340 Training loss 0.04700222611427307 Validation loss 0.05347876250743866 Accuracy 0.45654296875\n",
      "Iteration 77350 Training loss 0.05077317729592323 Validation loss 0.05320030078291893 Accuracy 0.45947265625\n",
      "Iteration 77360 Training loss 0.04966217651963234 Validation loss 0.05329958721995354 Accuracy 0.45654296875\n",
      "Iteration 77370 Training loss 0.04583059623837471 Validation loss 0.052950046956539154 Accuracy 0.459716796875\n",
      "Iteration 77380 Training loss 0.05129915475845337 Validation loss 0.05315793305635452 Accuracy 0.46044921875\n",
      "Iteration 77390 Training loss 0.04716330021619797 Validation loss 0.05299961194396019 Accuracy 0.461181640625\n",
      "Iteration 77400 Training loss 0.04805676266551018 Validation loss 0.05332768335938454 Accuracy 0.457275390625\n",
      "Iteration 77410 Training loss 0.05382509157061577 Validation loss 0.05295303463935852 Accuracy 0.460693359375\n",
      "Iteration 77420 Training loss 0.050393011420965195 Validation loss 0.053049519658088684 Accuracy 0.460693359375\n",
      "Iteration 77430 Training loss 0.05158195272088051 Validation loss 0.05285698175430298 Accuracy 0.462158203125\n",
      "Iteration 77440 Training loss 0.05251876637339592 Validation loss 0.05299665778875351 Accuracy 0.461181640625\n",
      "Iteration 77450 Training loss 0.04670342803001404 Validation loss 0.0529179573059082 Accuracy 0.46044921875\n",
      "Iteration 77460 Training loss 0.04944631829857826 Validation loss 0.05308404564857483 Accuracy 0.46044921875\n",
      "Iteration 77470 Training loss 0.053119879215955734 Validation loss 0.0529131181538105 Accuracy 0.460205078125\n",
      "Iteration 77480 Training loss 0.050751153379678726 Validation loss 0.05269647017121315 Accuracy 0.462158203125\n",
      "Iteration 77490 Training loss 0.04942914843559265 Validation loss 0.05265253782272339 Accuracy 0.462890625\n",
      "Iteration 77500 Training loss 0.0527762845158577 Validation loss 0.05297338590025902 Accuracy 0.4609375\n",
      "Iteration 77510 Training loss 0.04950990900397301 Validation loss 0.05282247066497803 Accuracy 0.460693359375\n",
      "Iteration 77520 Training loss 0.05141894891858101 Validation loss 0.05293682590126991 Accuracy 0.460205078125\n",
      "Iteration 77530 Training loss 0.05134574696421623 Validation loss 0.05287395417690277 Accuracy 0.4619140625\n",
      "Iteration 77540 Training loss 0.051030904054641724 Validation loss 0.052907444536685944 Accuracy 0.45947265625\n",
      "Iteration 77550 Training loss 0.04943079128861427 Validation loss 0.05314254388213158 Accuracy 0.45947265625\n",
      "Iteration 77560 Training loss 0.05163462087512016 Validation loss 0.05340727046132088 Accuracy 0.45849609375\n",
      "Iteration 77570 Training loss 0.05231891945004463 Validation loss 0.05338862165808678 Accuracy 0.458251953125\n",
      "Iteration 77580 Training loss 0.05382116511464119 Validation loss 0.05296451970934868 Accuracy 0.459716796875\n",
      "Iteration 77590 Training loss 0.049257393926382065 Validation loss 0.0532439649105072 Accuracy 0.45849609375\n",
      "Iteration 77600 Training loss 0.05077316239476204 Validation loss 0.05316777527332306 Accuracy 0.45849609375\n",
      "Iteration 77610 Training loss 0.05058111622929573 Validation loss 0.053220730274915695 Accuracy 0.45849609375\n",
      "Iteration 77620 Training loss 0.053116969764232635 Validation loss 0.054675374180078506 Accuracy 0.44677734375\n",
      "Iteration 77630 Training loss 0.04977373033761978 Validation loss 0.053502216935157776 Accuracy 0.456298828125\n",
      "Iteration 77640 Training loss 0.05332391336560249 Validation loss 0.05312882363796234 Accuracy 0.458984375\n",
      "Iteration 77650 Training loss 0.05292603373527527 Validation loss 0.05348533019423485 Accuracy 0.4560546875\n",
      "Iteration 77660 Training loss 0.049584828317165375 Validation loss 0.053051650524139404 Accuracy 0.459716796875\n",
      "Iteration 77670 Training loss 0.05055948346853256 Validation loss 0.05310995876789093 Accuracy 0.46142578125\n",
      "Iteration 77680 Training loss 0.048751216381788254 Validation loss 0.053875986486673355 Accuracy 0.4541015625\n",
      "Iteration 77690 Training loss 0.04848827049136162 Validation loss 0.05291033163666725 Accuracy 0.46044921875\n",
      "Iteration 77700 Training loss 0.05166580155491829 Validation loss 0.05306002125144005 Accuracy 0.4609375\n",
      "Iteration 77710 Training loss 0.04830775409936905 Validation loss 0.05309204012155533 Accuracy 0.461181640625\n",
      "Iteration 77720 Training loss 0.05096644535660744 Validation loss 0.05314726382493973 Accuracy 0.457763671875\n",
      "Iteration 77730 Training loss 0.04961944371461868 Validation loss 0.053142715245485306 Accuracy 0.461181640625\n",
      "Iteration 77740 Training loss 0.046667180955410004 Validation loss 0.052939608693122864 Accuracy 0.46044921875\n",
      "Iteration 77750 Training loss 0.04920274391770363 Validation loss 0.053045641630887985 Accuracy 0.459228515625\n",
      "Iteration 77760 Training loss 0.050397634506225586 Validation loss 0.053139179944992065 Accuracy 0.459716796875\n",
      "Iteration 77770 Training loss 0.04833672195672989 Validation loss 0.05284002795815468 Accuracy 0.461669921875\n",
      "Iteration 77780 Training loss 0.05139743909239769 Validation loss 0.05305692180991173 Accuracy 0.459716796875\n",
      "Iteration 77790 Training loss 0.05050494894385338 Validation loss 0.05292556434869766 Accuracy 0.46142578125\n",
      "Iteration 77800 Training loss 0.04848914593458176 Validation loss 0.0527348630130291 Accuracy 0.461669921875\n",
      "Iteration 77810 Training loss 0.04876188933849335 Validation loss 0.05384673550724983 Accuracy 0.455322265625\n",
      "Iteration 77820 Training loss 0.05451826751232147 Validation loss 0.0530066192150116 Accuracy 0.46044921875\n",
      "Iteration 77830 Training loss 0.052694015204906464 Validation loss 0.05348294600844383 Accuracy 0.4580078125\n",
      "Iteration 77840 Training loss 0.05067843571305275 Validation loss 0.05323215574026108 Accuracy 0.458984375\n",
      "Iteration 77850 Training loss 0.04981173202395439 Validation loss 0.05348126217722893 Accuracy 0.456298828125\n",
      "Iteration 77860 Training loss 0.04938789829611778 Validation loss 0.052858203649520874 Accuracy 0.46044921875\n",
      "Iteration 77870 Training loss 0.05073423311114311 Validation loss 0.05328647419810295 Accuracy 0.459228515625\n",
      "Iteration 77880 Training loss 0.0556441992521286 Validation loss 0.05346665903925896 Accuracy 0.457275390625\n",
      "Iteration 77890 Training loss 0.048778872936964035 Validation loss 0.05305621027946472 Accuracy 0.459228515625\n",
      "Iteration 77900 Training loss 0.046748097985982895 Validation loss 0.05316321551799774 Accuracy 0.458984375\n",
      "Iteration 77910 Training loss 0.0498429574072361 Validation loss 0.053164735436439514 Accuracy 0.458984375\n",
      "Iteration 77920 Training loss 0.054293956607580185 Validation loss 0.05339651182293892 Accuracy 0.456787109375\n",
      "Iteration 77930 Training loss 0.04813457280397415 Validation loss 0.05274404212832451 Accuracy 0.461181640625\n",
      "Iteration 77940 Training loss 0.054010383784770966 Validation loss 0.05297624692320824 Accuracy 0.4599609375\n",
      "Iteration 77950 Training loss 0.052439723163843155 Validation loss 0.053067076951265335 Accuracy 0.460205078125\n",
      "Iteration 77960 Training loss 0.04959794133901596 Validation loss 0.05304054543375969 Accuracy 0.460693359375\n",
      "Iteration 77970 Training loss 0.05044857785105705 Validation loss 0.0531574971973896 Accuracy 0.457763671875\n",
      "Iteration 77980 Training loss 0.0492173470556736 Validation loss 0.05310217663645744 Accuracy 0.4599609375\n",
      "Iteration 77990 Training loss 0.05418464168906212 Validation loss 0.053310319781303406 Accuracy 0.459716796875\n",
      "Iteration 78000 Training loss 0.050863660871982574 Validation loss 0.05293131619691849 Accuracy 0.461181640625\n",
      "Iteration 78010 Training loss 0.05173119157552719 Validation loss 0.05304073542356491 Accuracy 0.460693359375\n",
      "Iteration 78020 Training loss 0.05606291815638542 Validation loss 0.05341372266411781 Accuracy 0.45849609375\n",
      "Iteration 78030 Training loss 0.04759456217288971 Validation loss 0.05411494895815849 Accuracy 0.450439453125\n",
      "Iteration 78040 Training loss 0.049124110490083694 Validation loss 0.05301733314990997 Accuracy 0.461181640625\n",
      "Iteration 78050 Training loss 0.05179884657263756 Validation loss 0.05283229425549507 Accuracy 0.461669921875\n",
      "Iteration 78060 Training loss 0.05093913897871971 Validation loss 0.0529618039727211 Accuracy 0.4599609375\n",
      "Iteration 78070 Training loss 0.0545271597802639 Validation loss 0.05289507657289505 Accuracy 0.460693359375\n",
      "Iteration 78080 Training loss 0.05522414669394493 Validation loss 0.053402937948703766 Accuracy 0.45751953125\n",
      "Iteration 78090 Training loss 0.0517631433904171 Validation loss 0.053030796349048615 Accuracy 0.4599609375\n",
      "Iteration 78100 Training loss 0.05246478319168091 Validation loss 0.05278615280985832 Accuracy 0.46142578125\n",
      "Iteration 78110 Training loss 0.0516977421939373 Validation loss 0.05279909446835518 Accuracy 0.46142578125\n",
      "Iteration 78120 Training loss 0.05089634284377098 Validation loss 0.053110696375370026 Accuracy 0.45947265625\n",
      "Iteration 78130 Training loss 0.049383413046598434 Validation loss 0.053041767328977585 Accuracy 0.461181640625\n",
      "Iteration 78140 Training loss 0.05341407284140587 Validation loss 0.052865613251924515 Accuracy 0.461181640625\n",
      "Iteration 78150 Training loss 0.04882502183318138 Validation loss 0.052769970148801804 Accuracy 0.462158203125\n",
      "Iteration 78160 Training loss 0.050154462456703186 Validation loss 0.05288136005401611 Accuracy 0.4609375\n",
      "Iteration 78170 Training loss 0.05327849090099335 Validation loss 0.052924539893865585 Accuracy 0.461181640625\n",
      "Iteration 78180 Training loss 0.0487932413816452 Validation loss 0.052995260804891586 Accuracy 0.460693359375\n",
      "Iteration 78190 Training loss 0.04992963746190071 Validation loss 0.05305267870426178 Accuracy 0.4599609375\n",
      "Iteration 78200 Training loss 0.046725787222385406 Validation loss 0.05273288115859032 Accuracy 0.462158203125\n",
      "Iteration 78210 Training loss 0.04958556219935417 Validation loss 0.05347045883536339 Accuracy 0.458251953125\n",
      "Iteration 78220 Training loss 0.05123253911733627 Validation loss 0.05257701128721237 Accuracy 0.462646484375\n",
      "Iteration 78230 Training loss 0.049397602677345276 Validation loss 0.052817557007074356 Accuracy 0.4619140625\n",
      "Iteration 78240 Training loss 0.05337034910917282 Validation loss 0.0531231127679348 Accuracy 0.458984375\n",
      "Iteration 78250 Training loss 0.048845767974853516 Validation loss 0.05315697565674782 Accuracy 0.45849609375\n",
      "Iteration 78260 Training loss 0.051348358392715454 Validation loss 0.05309662967920303 Accuracy 0.46044921875\n",
      "Iteration 78270 Training loss 0.04959138110280037 Validation loss 0.05303202569484711 Accuracy 0.4599609375\n",
      "Iteration 78280 Training loss 0.05274749547243118 Validation loss 0.05276747792959213 Accuracy 0.462646484375\n",
      "Iteration 78290 Training loss 0.04916694015264511 Validation loss 0.052890583872795105 Accuracy 0.4609375\n",
      "Iteration 78300 Training loss 0.0488167330622673 Validation loss 0.05273823440074921 Accuracy 0.461669921875\n",
      "Iteration 78310 Training loss 0.05088271200656891 Validation loss 0.05319729447364807 Accuracy 0.45849609375\n",
      "Iteration 78320 Training loss 0.05011318251490593 Validation loss 0.053011029958724976 Accuracy 0.461181640625\n",
      "Iteration 78330 Training loss 0.053952258080244064 Validation loss 0.05298089236021042 Accuracy 0.4599609375\n",
      "Iteration 78340 Training loss 0.054613251239061356 Validation loss 0.05336814373731613 Accuracy 0.456787109375\n",
      "Iteration 78350 Training loss 0.053101908415555954 Validation loss 0.05282207950949669 Accuracy 0.46142578125\n",
      "Iteration 78360 Training loss 0.05151539668440819 Validation loss 0.05303490161895752 Accuracy 0.459716796875\n",
      "Iteration 78370 Training loss 0.05235840380191803 Validation loss 0.05343318358063698 Accuracy 0.45849609375\n",
      "Iteration 78380 Training loss 0.04879277944564819 Validation loss 0.05289291962981224 Accuracy 0.46142578125\n",
      "Iteration 78390 Training loss 0.045284636318683624 Validation loss 0.05279257148504257 Accuracy 0.462890625\n",
      "Iteration 78400 Training loss 0.05042523145675659 Validation loss 0.05342060327529907 Accuracy 0.45458984375\n",
      "Iteration 78410 Training loss 0.05549251288175583 Validation loss 0.05316779017448425 Accuracy 0.458984375\n",
      "Iteration 78420 Training loss 0.04968583211302757 Validation loss 0.05294674634933472 Accuracy 0.460205078125\n",
      "Iteration 78430 Training loss 0.04815271124243736 Validation loss 0.05330783128738403 Accuracy 0.458984375\n",
      "Iteration 78440 Training loss 0.046981893479824066 Validation loss 0.05279160663485527 Accuracy 0.4619140625\n",
      "Iteration 78450 Training loss 0.05292839556932449 Validation loss 0.053900428116321564 Accuracy 0.453369140625\n",
      "Iteration 78460 Training loss 0.050512734800577164 Validation loss 0.05279580503702164 Accuracy 0.461181640625\n",
      "Iteration 78470 Training loss 0.04798879846930504 Validation loss 0.053368035703897476 Accuracy 0.458984375\n",
      "Iteration 78480 Training loss 0.0515073761343956 Validation loss 0.0530259944498539 Accuracy 0.461181640625\n",
      "Iteration 78490 Training loss 0.05367643013596535 Validation loss 0.053266484290361404 Accuracy 0.4580078125\n",
      "Iteration 78500 Training loss 0.05355655774474144 Validation loss 0.05314350500702858 Accuracy 0.46044921875\n",
      "Iteration 78510 Training loss 0.04988650605082512 Validation loss 0.053061507642269135 Accuracy 0.46044921875\n",
      "Iteration 78520 Training loss 0.05157412588596344 Validation loss 0.05306044965982437 Accuracy 0.4599609375\n",
      "Iteration 78530 Training loss 0.049656860530376434 Validation loss 0.05293383076786995 Accuracy 0.459716796875\n",
      "Iteration 78540 Training loss 0.05250762775540352 Validation loss 0.052880868315696716 Accuracy 0.462158203125\n",
      "Iteration 78550 Training loss 0.05194216966629028 Validation loss 0.05320403724908829 Accuracy 0.4599609375\n",
      "Iteration 78560 Training loss 0.048926036804914474 Validation loss 0.053340084850788116 Accuracy 0.456298828125\n",
      "Iteration 78570 Training loss 0.052480366080999374 Validation loss 0.05321751907467842 Accuracy 0.460205078125\n",
      "Iteration 78580 Training loss 0.05138959735631943 Validation loss 0.05261312797665596 Accuracy 0.46240234375\n",
      "Iteration 78590 Training loss 0.05350257828831673 Validation loss 0.05287785083055496 Accuracy 0.461181640625\n",
      "Iteration 78600 Training loss 0.05047978088259697 Validation loss 0.05299137160181999 Accuracy 0.460693359375\n",
      "Iteration 78610 Training loss 0.05104141682386398 Validation loss 0.0526779480278492 Accuracy 0.461669921875\n",
      "Iteration 78620 Training loss 0.049559980630874634 Validation loss 0.053425632417201996 Accuracy 0.45703125\n",
      "Iteration 78630 Training loss 0.05345093831419945 Validation loss 0.05352611839771271 Accuracy 0.456787109375\n",
      "Iteration 78640 Training loss 0.05079847574234009 Validation loss 0.0528152771294117 Accuracy 0.46044921875\n",
      "Iteration 78650 Training loss 0.054532911628484726 Validation loss 0.05308954045176506 Accuracy 0.459716796875\n",
      "Iteration 78660 Training loss 0.05124825984239578 Validation loss 0.05294552445411682 Accuracy 0.4619140625\n",
      "Iteration 78670 Training loss 0.049790337681770325 Validation loss 0.05298088118433952 Accuracy 0.459716796875\n",
      "Iteration 78680 Training loss 0.0509994812309742 Validation loss 0.0531124584376812 Accuracy 0.461181640625\n",
      "Iteration 78690 Training loss 0.04831833764910698 Validation loss 0.05330001190304756 Accuracy 0.458740234375\n",
      "Iteration 78700 Training loss 0.05374116823077202 Validation loss 0.05342454835772514 Accuracy 0.459716796875\n",
      "Iteration 78710 Training loss 0.052191633731126785 Validation loss 0.05344075709581375 Accuracy 0.458251953125\n",
      "Iteration 78720 Training loss 0.05090804398059845 Validation loss 0.0529940128326416 Accuracy 0.461181640625\n",
      "Iteration 78730 Training loss 0.053144097328186035 Validation loss 0.0532279871404171 Accuracy 0.45947265625\n",
      "Iteration 78740 Training loss 0.04756459593772888 Validation loss 0.05320581793785095 Accuracy 0.45947265625\n",
      "Iteration 78750 Training loss 0.04631904885172844 Validation loss 0.05272963270545006 Accuracy 0.46142578125\n",
      "Iteration 78760 Training loss 0.049636177718639374 Validation loss 0.05266578868031502 Accuracy 0.462890625\n",
      "Iteration 78770 Training loss 0.0513959638774395 Validation loss 0.05278461426496506 Accuracy 0.4619140625\n",
      "Iteration 78780 Training loss 0.051112305372953415 Validation loss 0.053042273968458176 Accuracy 0.4580078125\n",
      "Iteration 78790 Training loss 0.05107154697179794 Validation loss 0.052988115698099136 Accuracy 0.4599609375\n",
      "Iteration 78800 Training loss 0.05265877768397331 Validation loss 0.05273618921637535 Accuracy 0.462890625\n",
      "Iteration 78810 Training loss 0.053452637046575546 Validation loss 0.0529516376554966 Accuracy 0.45947265625\n",
      "Iteration 78820 Training loss 0.05267437547445297 Validation loss 0.05265026167035103 Accuracy 0.4619140625\n",
      "Iteration 78830 Training loss 0.05038681998848915 Validation loss 0.05294298380613327 Accuracy 0.461669921875\n",
      "Iteration 78840 Training loss 0.05041288211941719 Validation loss 0.05300000309944153 Accuracy 0.459716796875\n",
      "Iteration 78850 Training loss 0.05391160026192665 Validation loss 0.053202420473098755 Accuracy 0.459716796875\n",
      "Iteration 78860 Training loss 0.05256669223308563 Validation loss 0.05370070040225983 Accuracy 0.456298828125\n",
      "Iteration 78870 Training loss 0.052037011831998825 Validation loss 0.05321808531880379 Accuracy 0.45947265625\n",
      "Iteration 78880 Training loss 0.05300528556108475 Validation loss 0.05283856391906738 Accuracy 0.462158203125\n",
      "Iteration 78890 Training loss 0.05353422090411186 Validation loss 0.05272530019283295 Accuracy 0.461669921875\n",
      "Iteration 78900 Training loss 0.0506095364689827 Validation loss 0.05361008644104004 Accuracy 0.455322265625\n",
      "Iteration 78910 Training loss 0.04871709272265434 Validation loss 0.05299316719174385 Accuracy 0.462158203125\n",
      "Iteration 78920 Training loss 0.053247567266225815 Validation loss 0.05299987271428108 Accuracy 0.46044921875\n",
      "Iteration 78930 Training loss 0.05097045376896858 Validation loss 0.05278581753373146 Accuracy 0.461669921875\n",
      "Iteration 78940 Training loss 0.04882366210222244 Validation loss 0.052881501615047455 Accuracy 0.461181640625\n",
      "Iteration 78950 Training loss 0.051848407834768295 Validation loss 0.0530601441860199 Accuracy 0.460693359375\n",
      "Iteration 78960 Training loss 0.051330577582120895 Validation loss 0.05319619178771973 Accuracy 0.459716796875\n",
      "Iteration 78970 Training loss 0.051924820989370346 Validation loss 0.0528835766017437 Accuracy 0.461669921875\n",
      "Iteration 78980 Training loss 0.04785936325788498 Validation loss 0.053040534257888794 Accuracy 0.46044921875\n",
      "Iteration 78990 Training loss 0.05129057168960571 Validation loss 0.052979160100221634 Accuracy 0.45947265625\n",
      "Iteration 79000 Training loss 0.05206451937556267 Validation loss 0.05274548381567001 Accuracy 0.461669921875\n",
      "Iteration 79010 Training loss 0.054410420358181 Validation loss 0.05351792648434639 Accuracy 0.4541015625\n",
      "Iteration 79020 Training loss 0.05111321061849594 Validation loss 0.05319375917315483 Accuracy 0.460205078125\n",
      "Iteration 79030 Training loss 0.050478287041187286 Validation loss 0.052752215415239334 Accuracy 0.4619140625\n",
      "Iteration 79040 Training loss 0.05251087248325348 Validation loss 0.053246673196554184 Accuracy 0.460205078125\n",
      "Iteration 79050 Training loss 0.04980020970106125 Validation loss 0.05297836288809776 Accuracy 0.459716796875\n",
      "Iteration 79060 Training loss 0.0516556017100811 Validation loss 0.052981045097112656 Accuracy 0.4619140625\n",
      "Iteration 79070 Training loss 0.047960925847291946 Validation loss 0.05311887711286545 Accuracy 0.460205078125\n",
      "Iteration 79080 Training loss 0.048581693321466446 Validation loss 0.053221024572849274 Accuracy 0.458740234375\n",
      "Iteration 79090 Training loss 0.05198545381426811 Validation loss 0.05292455106973648 Accuracy 0.460205078125\n",
      "Iteration 79100 Training loss 0.04733486846089363 Validation loss 0.05279184877872467 Accuracy 0.462158203125\n",
      "Iteration 79110 Training loss 0.04998137429356575 Validation loss 0.05312328785657883 Accuracy 0.460205078125\n",
      "Iteration 79120 Training loss 0.05014796927571297 Validation loss 0.05273836478590965 Accuracy 0.461669921875\n",
      "Iteration 79130 Training loss 0.04868648573756218 Validation loss 0.053049638867378235 Accuracy 0.46044921875\n",
      "Iteration 79140 Training loss 0.0483848974108696 Validation loss 0.05315084382891655 Accuracy 0.458740234375\n",
      "Iteration 79150 Training loss 0.053237732499837875 Validation loss 0.05294158682227135 Accuracy 0.45849609375\n",
      "Iteration 79160 Training loss 0.05107304826378822 Validation loss 0.052996307611465454 Accuracy 0.458984375\n",
      "Iteration 79170 Training loss 0.05168452486395836 Validation loss 0.05353887379169464 Accuracy 0.45751953125\n",
      "Iteration 79180 Training loss 0.051758941262960434 Validation loss 0.05290522426366806 Accuracy 0.460205078125\n",
      "Iteration 79190 Training loss 0.049741681665182114 Validation loss 0.053094927221536636 Accuracy 0.459716796875\n",
      "Iteration 79200 Training loss 0.05506978929042816 Validation loss 0.05285009741783142 Accuracy 0.46044921875\n",
      "Iteration 79210 Training loss 0.04845577850937843 Validation loss 0.05359506607055664 Accuracy 0.456787109375\n",
      "Iteration 79220 Training loss 0.05019194632768631 Validation loss 0.05302724987268448 Accuracy 0.460205078125\n",
      "Iteration 79230 Training loss 0.05212176963686943 Validation loss 0.0538511797785759 Accuracy 0.455078125\n",
      "Iteration 79240 Training loss 0.051935166120529175 Validation loss 0.052902646362781525 Accuracy 0.461181640625\n",
      "Iteration 79250 Training loss 0.05372519791126251 Validation loss 0.0532422736287117 Accuracy 0.45849609375\n",
      "Iteration 79260 Training loss 0.054581161588430405 Validation loss 0.05343503877520561 Accuracy 0.456298828125\n",
      "Iteration 79270 Training loss 0.0492185577750206 Validation loss 0.05270417034626007 Accuracy 0.461181640625\n",
      "Iteration 79280 Training loss 0.051991015672683716 Validation loss 0.05310579761862755 Accuracy 0.459716796875\n",
      "Iteration 79290 Training loss 0.05481269955635071 Validation loss 0.05285380780696869 Accuracy 0.460693359375\n",
      "Iteration 79300 Training loss 0.05087066441774368 Validation loss 0.05299936234951019 Accuracy 0.461181640625\n",
      "Iteration 79310 Training loss 0.049186598509550095 Validation loss 0.05322723463177681 Accuracy 0.459716796875\n",
      "Iteration 79320 Training loss 0.051849476993083954 Validation loss 0.05423250421881676 Accuracy 0.450439453125\n",
      "Iteration 79330 Training loss 0.04948500543832779 Validation loss 0.05292803421616554 Accuracy 0.458984375\n",
      "Iteration 79340 Training loss 0.04768116772174835 Validation loss 0.052826669067144394 Accuracy 0.46142578125\n",
      "Iteration 79350 Training loss 0.05094175040721893 Validation loss 0.05286053195595741 Accuracy 0.4609375\n",
      "Iteration 79360 Training loss 0.05014696344733238 Validation loss 0.052866630256175995 Accuracy 0.460205078125\n",
      "Iteration 79370 Training loss 0.04983198642730713 Validation loss 0.05294583737850189 Accuracy 0.461181640625\n",
      "Iteration 79380 Training loss 0.04616978019475937 Validation loss 0.05280805379152298 Accuracy 0.4609375\n",
      "Iteration 79390 Training loss 0.052630409598350525 Validation loss 0.052829597145318985 Accuracy 0.461181640625\n",
      "Iteration 79400 Training loss 0.051375143229961395 Validation loss 0.052953027188777924 Accuracy 0.459716796875\n",
      "Iteration 79410 Training loss 0.049386605620384216 Validation loss 0.0530819408595562 Accuracy 0.458740234375\n",
      "Iteration 79420 Training loss 0.05119902640581131 Validation loss 0.05315307155251503 Accuracy 0.458984375\n",
      "Iteration 79430 Training loss 0.050917450338602066 Validation loss 0.053066350519657135 Accuracy 0.459716796875\n",
      "Iteration 79440 Training loss 0.048230692744255066 Validation loss 0.05287690460681915 Accuracy 0.461181640625\n",
      "Iteration 79450 Training loss 0.050722379237413406 Validation loss 0.05356214568018913 Accuracy 0.4560546875\n",
      "Iteration 79460 Training loss 0.050918735563755035 Validation loss 0.05289828032255173 Accuracy 0.4599609375\n",
      "Iteration 79470 Training loss 0.04866570979356766 Validation loss 0.05289479345083237 Accuracy 0.460693359375\n",
      "Iteration 79480 Training loss 0.048888690769672394 Validation loss 0.05284208431839943 Accuracy 0.45947265625\n",
      "Iteration 79490 Training loss 0.04978909343481064 Validation loss 0.05282348394393921 Accuracy 0.4609375\n",
      "Iteration 79500 Training loss 0.04640632122755051 Validation loss 0.0531463548541069 Accuracy 0.460205078125\n",
      "Iteration 79510 Training loss 0.05028336122632027 Validation loss 0.05277906730771065 Accuracy 0.461181640625\n",
      "Iteration 79520 Training loss 0.04974202439188957 Validation loss 0.052761878818273544 Accuracy 0.4609375\n",
      "Iteration 79530 Training loss 0.050138987600803375 Validation loss 0.05300694331526756 Accuracy 0.459228515625\n",
      "Iteration 79540 Training loss 0.050249531865119934 Validation loss 0.05327680706977844 Accuracy 0.456787109375\n",
      "Iteration 79550 Training loss 0.04987217113375664 Validation loss 0.05327344685792923 Accuracy 0.458740234375\n",
      "Iteration 79560 Training loss 0.04953290894627571 Validation loss 0.05396927520632744 Accuracy 0.452880859375\n",
      "Iteration 79570 Training loss 0.05202314630150795 Validation loss 0.05299701169133186 Accuracy 0.460205078125\n",
      "Iteration 79580 Training loss 0.05438839644193649 Validation loss 0.053335849195718765 Accuracy 0.458740234375\n",
      "Iteration 79590 Training loss 0.05020017549395561 Validation loss 0.05299177020788193 Accuracy 0.4599609375\n",
      "Iteration 79600 Training loss 0.051097217947244644 Validation loss 0.0529978908598423 Accuracy 0.4609375\n",
      "Iteration 79610 Training loss 0.05315921828150749 Validation loss 0.05317556485533714 Accuracy 0.4609375\n",
      "Iteration 79620 Training loss 0.048744916915893555 Validation loss 0.053282663226127625 Accuracy 0.459228515625\n",
      "Iteration 79630 Training loss 0.05087198689579964 Validation loss 0.05317402631044388 Accuracy 0.460205078125\n",
      "Iteration 79640 Training loss 0.05537378787994385 Validation loss 0.05306798592209816 Accuracy 0.459716796875\n",
      "Iteration 79650 Training loss 0.052229441702365875 Validation loss 0.05279340595006943 Accuracy 0.46044921875\n",
      "Iteration 79660 Training loss 0.05174730345606804 Validation loss 0.05322489142417908 Accuracy 0.457763671875\n",
      "Iteration 79670 Training loss 0.05302273482084274 Validation loss 0.05303064361214638 Accuracy 0.459716796875\n",
      "Iteration 79680 Training loss 0.049663521349430084 Validation loss 0.05300780013203621 Accuracy 0.459716796875\n",
      "Iteration 79690 Training loss 0.05045223981142044 Validation loss 0.05294292792677879 Accuracy 0.460205078125\n",
      "Iteration 79700 Training loss 0.05293868109583855 Validation loss 0.053280506283044815 Accuracy 0.458740234375\n",
      "Iteration 79710 Training loss 0.051979079842567444 Validation loss 0.05289051681756973 Accuracy 0.4599609375\n",
      "Iteration 79720 Training loss 0.049411121755838394 Validation loss 0.05297604203224182 Accuracy 0.4599609375\n",
      "Iteration 79730 Training loss 0.04970896244049072 Validation loss 0.05270783603191376 Accuracy 0.461669921875\n",
      "Iteration 79740 Training loss 0.05162869021296501 Validation loss 0.05298226326704025 Accuracy 0.459716796875\n",
      "Iteration 79750 Training loss 0.053564731031656265 Validation loss 0.052771784365177155 Accuracy 0.4609375\n",
      "Iteration 79760 Training loss 0.05259770527482033 Validation loss 0.05294812470674515 Accuracy 0.46044921875\n",
      "Iteration 79770 Training loss 0.05185514688491821 Validation loss 0.05379033461213112 Accuracy 0.455078125\n",
      "Iteration 79780 Training loss 0.05362672731280327 Validation loss 0.0529417023062706 Accuracy 0.4619140625\n",
      "Iteration 79790 Training loss 0.05255110561847687 Validation loss 0.052564676851034164 Accuracy 0.46240234375\n",
      "Iteration 79800 Training loss 0.051579397171735764 Validation loss 0.05311274155974388 Accuracy 0.45947265625\n",
      "Iteration 79810 Training loss 0.05154219642281532 Validation loss 0.052564505487680435 Accuracy 0.46240234375\n",
      "Iteration 79820 Training loss 0.0524318590760231 Validation loss 0.05335678905248642 Accuracy 0.455322265625\n",
      "Iteration 79830 Training loss 0.054826200008392334 Validation loss 0.05299558490514755 Accuracy 0.460205078125\n",
      "Iteration 79840 Training loss 0.053554315119981766 Validation loss 0.05336441099643707 Accuracy 0.458984375\n",
      "Iteration 79850 Training loss 0.0521969199180603 Validation loss 0.05288565158843994 Accuracy 0.46142578125\n",
      "Iteration 79860 Training loss 0.05058079957962036 Validation loss 0.05317344516515732 Accuracy 0.457763671875\n",
      "Iteration 79870 Training loss 0.05157516524195671 Validation loss 0.05309572443366051 Accuracy 0.460693359375\n",
      "Iteration 79880 Training loss 0.053209107369184494 Validation loss 0.05301506817340851 Accuracy 0.45849609375\n",
      "Iteration 79890 Training loss 0.05259380862116814 Validation loss 0.05297801271080971 Accuracy 0.4619140625\n",
      "Iteration 79900 Training loss 0.051560018211603165 Validation loss 0.052744876593351364 Accuracy 0.4609375\n",
      "Iteration 79910 Training loss 0.04964572563767433 Validation loss 0.05272843688726425 Accuracy 0.461181640625\n",
      "Iteration 79920 Training loss 0.05237642303109169 Validation loss 0.052973564714193344 Accuracy 0.460693359375\n",
      "Iteration 79930 Training loss 0.04814191535115242 Validation loss 0.05310488119721413 Accuracy 0.458251953125\n",
      "Iteration 79940 Training loss 0.04878613352775574 Validation loss 0.05278998240828514 Accuracy 0.460693359375\n",
      "Iteration 79950 Training loss 0.04797922819852829 Validation loss 0.053108640015125275 Accuracy 0.460205078125\n",
      "Iteration 79960 Training loss 0.04706704989075661 Validation loss 0.052771203219890594 Accuracy 0.461181640625\n",
      "Iteration 79970 Training loss 0.05324789509177208 Validation loss 0.052879661321640015 Accuracy 0.4599609375\n",
      "Iteration 79980 Training loss 0.04973984137177467 Validation loss 0.05321692302823067 Accuracy 0.458984375\n",
      "Iteration 79990 Training loss 0.05233629420399666 Validation loss 0.052689842879772186 Accuracy 0.4609375\n",
      "Iteration 80000 Training loss 0.05282243713736534 Validation loss 0.052924416959285736 Accuracy 0.461181640625\n",
      "Iteration 80010 Training loss 0.05125303566455841 Validation loss 0.05298536643385887 Accuracy 0.460205078125\n",
      "Iteration 80020 Training loss 0.05212428793311119 Validation loss 0.052606433629989624 Accuracy 0.46240234375\n",
      "Iteration 80030 Training loss 0.05135231092572212 Validation loss 0.05277762934565544 Accuracy 0.461181640625\n",
      "Iteration 80040 Training loss 0.05157674103975296 Validation loss 0.05277165025472641 Accuracy 0.461181640625\n",
      "Iteration 80050 Training loss 0.056252460926771164 Validation loss 0.05296872928738594 Accuracy 0.460693359375\n",
      "Iteration 80060 Training loss 0.05528169125318527 Validation loss 0.053064167499542236 Accuracy 0.459716796875\n",
      "Iteration 80070 Training loss 0.050044167786836624 Validation loss 0.053186073899269104 Accuracy 0.459716796875\n",
      "Iteration 80080 Training loss 0.053088415414094925 Validation loss 0.05360187962651253 Accuracy 0.458251953125\n",
      "Iteration 80090 Training loss 0.05134209245443344 Validation loss 0.05271261930465698 Accuracy 0.4599609375\n",
      "Iteration 80100 Training loss 0.052929360419511795 Validation loss 0.052970994263887405 Accuracy 0.460205078125\n",
      "Iteration 80110 Training loss 0.048922184854745865 Validation loss 0.053376175463199615 Accuracy 0.458740234375\n",
      "Iteration 80120 Training loss 0.052318911999464035 Validation loss 0.053707268089056015 Accuracy 0.454833984375\n",
      "Iteration 80130 Training loss 0.049955662339925766 Validation loss 0.05312832072377205 Accuracy 0.458984375\n",
      "Iteration 80140 Training loss 0.04961554333567619 Validation loss 0.0528438463807106 Accuracy 0.460693359375\n",
      "Iteration 80150 Training loss 0.04961816221475601 Validation loss 0.053020961582660675 Accuracy 0.460693359375\n",
      "Iteration 80160 Training loss 0.054510995745658875 Validation loss 0.052967824041843414 Accuracy 0.460693359375\n",
      "Iteration 80170 Training loss 0.04795229062438011 Validation loss 0.052726034075021744 Accuracy 0.461181640625\n",
      "Iteration 80180 Training loss 0.05448680743575096 Validation loss 0.052962396293878555 Accuracy 0.460693359375\n",
      "Iteration 80190 Training loss 0.05077224597334862 Validation loss 0.053054653108119965 Accuracy 0.458984375\n",
      "Iteration 80200 Training loss 0.04703336954116821 Validation loss 0.05342642590403557 Accuracy 0.45751953125\n",
      "Iteration 80210 Training loss 0.05307977274060249 Validation loss 0.052687741816043854 Accuracy 0.4619140625\n",
      "Iteration 80220 Training loss 0.053889334201812744 Validation loss 0.05309243127703667 Accuracy 0.4599609375\n",
      "Iteration 80230 Training loss 0.050969429314136505 Validation loss 0.052803944796323776 Accuracy 0.4609375\n",
      "Iteration 80240 Training loss 0.0515681654214859 Validation loss 0.05284743010997772 Accuracy 0.459716796875\n",
      "Iteration 80250 Training loss 0.05027110129594803 Validation loss 0.053445760160684586 Accuracy 0.457275390625\n",
      "Iteration 80260 Training loss 0.05005674064159393 Validation loss 0.05276065319776535 Accuracy 0.460693359375\n",
      "Iteration 80270 Training loss 0.052550170570611954 Validation loss 0.05329209193587303 Accuracy 0.458251953125\n",
      "Iteration 80280 Training loss 0.048573192209005356 Validation loss 0.05292879790067673 Accuracy 0.46044921875\n",
      "Iteration 80290 Training loss 0.05068730190396309 Validation loss 0.052787184715270996 Accuracy 0.461181640625\n",
      "Iteration 80300 Training loss 0.05063144862651825 Validation loss 0.05291885510087013 Accuracy 0.461181640625\n",
      "Iteration 80310 Training loss 0.048927001655101776 Validation loss 0.05292130261659622 Accuracy 0.46142578125\n",
      "Iteration 80320 Training loss 0.05166403204202652 Validation loss 0.053084153681993484 Accuracy 0.458984375\n",
      "Iteration 80330 Training loss 0.04959481582045555 Validation loss 0.053201574832201004 Accuracy 0.459716796875\n",
      "Iteration 80340 Training loss 0.051580168306827545 Validation loss 0.05302611365914345 Accuracy 0.458740234375\n",
      "Iteration 80350 Training loss 0.051327452063560486 Validation loss 0.05292820185422897 Accuracy 0.461181640625\n",
      "Iteration 80360 Training loss 0.053658608347177505 Validation loss 0.052633825689554214 Accuracy 0.4619140625\n",
      "Iteration 80370 Training loss 0.052154622972011566 Validation loss 0.05296431481838226 Accuracy 0.45947265625\n",
      "Iteration 80380 Training loss 0.04979519546031952 Validation loss 0.05328957736492157 Accuracy 0.458740234375\n",
      "Iteration 80390 Training loss 0.05237901583313942 Validation loss 0.05330560356378555 Accuracy 0.4580078125\n",
      "Iteration 80400 Training loss 0.04954526200890541 Validation loss 0.05278826877474785 Accuracy 0.461181640625\n",
      "Iteration 80410 Training loss 0.05068349093198776 Validation loss 0.05297555401921272 Accuracy 0.460205078125\n",
      "Iteration 80420 Training loss 0.05358545854687691 Validation loss 0.052898380905389786 Accuracy 0.4599609375\n",
      "Iteration 80430 Training loss 0.05213365703821182 Validation loss 0.05269399285316467 Accuracy 0.4619140625\n",
      "Iteration 80440 Training loss 0.05121095851063728 Validation loss 0.052775830030441284 Accuracy 0.46142578125\n",
      "Iteration 80450 Training loss 0.04929036274552345 Validation loss 0.05321136489510536 Accuracy 0.45751953125\n",
      "Iteration 80460 Training loss 0.04861260578036308 Validation loss 0.052709437906742096 Accuracy 0.46240234375\n",
      "Iteration 80470 Training loss 0.05278283730149269 Validation loss 0.05317927524447441 Accuracy 0.45849609375\n",
      "Iteration 80480 Training loss 0.051257893443107605 Validation loss 0.052959807217121124 Accuracy 0.461181640625\n",
      "Iteration 80490 Training loss 0.049378763884305954 Validation loss 0.05292590707540512 Accuracy 0.460205078125\n",
      "Iteration 80500 Training loss 0.05519510805606842 Validation loss 0.05275612324476242 Accuracy 0.460693359375\n",
      "Iteration 80510 Training loss 0.04461785778403282 Validation loss 0.052598826587200165 Accuracy 0.462646484375\n",
      "Iteration 80520 Training loss 0.0522255040705204 Validation loss 0.05250917747616768 Accuracy 0.463134765625\n",
      "Iteration 80530 Training loss 0.050614360719919205 Validation loss 0.052955642342567444 Accuracy 0.460205078125\n",
      "Iteration 80540 Training loss 0.051960401237010956 Validation loss 0.05262148380279541 Accuracy 0.4619140625\n",
      "Iteration 80550 Training loss 0.04827238991856575 Validation loss 0.05312376841902733 Accuracy 0.46044921875\n",
      "Iteration 80560 Training loss 0.05017169192433357 Validation loss 0.052825067192316055 Accuracy 0.4609375\n",
      "Iteration 80570 Training loss 0.05257309973239899 Validation loss 0.05292338877916336 Accuracy 0.460693359375\n",
      "Iteration 80580 Training loss 0.05315445736050606 Validation loss 0.05329513177275658 Accuracy 0.45849609375\n",
      "Iteration 80590 Training loss 0.052839066833257675 Validation loss 0.05336339771747589 Accuracy 0.457763671875\n",
      "Iteration 80600 Training loss 0.05486505106091499 Validation loss 0.053810007870197296 Accuracy 0.452880859375\n",
      "Iteration 80610 Training loss 0.05227313190698624 Validation loss 0.05284867063164711 Accuracy 0.4619140625\n",
      "Iteration 80620 Training loss 0.05105544999241829 Validation loss 0.05286441370844841 Accuracy 0.462158203125\n",
      "Iteration 80630 Training loss 0.05012880265712738 Validation loss 0.05328107252717018 Accuracy 0.457275390625\n",
      "Iteration 80640 Training loss 0.04855869337916374 Validation loss 0.05263013392686844 Accuracy 0.462890625\n",
      "Iteration 80650 Training loss 0.04945291206240654 Validation loss 0.05302534997463226 Accuracy 0.46142578125\n",
      "Iteration 80660 Training loss 0.05324539169669151 Validation loss 0.05305278301239014 Accuracy 0.459716796875\n",
      "Iteration 80670 Training loss 0.04925258457660675 Validation loss 0.052568212151527405 Accuracy 0.463623046875\n",
      "Iteration 80680 Training loss 0.046896785497665405 Validation loss 0.05267507582902908 Accuracy 0.462890625\n",
      "Iteration 80690 Training loss 0.049250777810811996 Validation loss 0.052717480808496475 Accuracy 0.463134765625\n",
      "Iteration 80700 Training loss 0.05428317189216614 Validation loss 0.052693698555231094 Accuracy 0.462646484375\n",
      "Iteration 80710 Training loss 0.047046955674886703 Validation loss 0.05317215994000435 Accuracy 0.45947265625\n",
      "Iteration 80720 Training loss 0.052286915481090546 Validation loss 0.05283650383353233 Accuracy 0.46142578125\n",
      "Iteration 80730 Training loss 0.052361901849508286 Validation loss 0.05269727483391762 Accuracy 0.461669921875\n",
      "Iteration 80740 Training loss 0.051902588456869125 Validation loss 0.052849750965833664 Accuracy 0.462158203125\n",
      "Iteration 80750 Training loss 0.04812903702259064 Validation loss 0.05317677929997444 Accuracy 0.459228515625\n",
      "Iteration 80760 Training loss 0.05106572434306145 Validation loss 0.05271044746041298 Accuracy 0.461669921875\n",
      "Iteration 80770 Training loss 0.05306575447320938 Validation loss 0.0528811477124691 Accuracy 0.462158203125\n",
      "Iteration 80780 Training loss 0.05169475078582764 Validation loss 0.05281177535653114 Accuracy 0.46142578125\n",
      "Iteration 80790 Training loss 0.05096379667520523 Validation loss 0.05286870151758194 Accuracy 0.4619140625\n",
      "Iteration 80800 Training loss 0.04814273118972778 Validation loss 0.05265546590089798 Accuracy 0.4619140625\n",
      "Iteration 80810 Training loss 0.048618510365486145 Validation loss 0.05304180458188057 Accuracy 0.458984375\n",
      "Iteration 80820 Training loss 0.050586357712745667 Validation loss 0.05291663110256195 Accuracy 0.461181640625\n",
      "Iteration 80830 Training loss 0.05047211796045303 Validation loss 0.053088460117578506 Accuracy 0.4619140625\n",
      "Iteration 80840 Training loss 0.04960118606686592 Validation loss 0.052963435649871826 Accuracy 0.461181640625\n",
      "Iteration 80850 Training loss 0.052944619208574295 Validation loss 0.05287635698914528 Accuracy 0.46044921875\n",
      "Iteration 80860 Training loss 0.05257505178451538 Validation loss 0.05330928415060043 Accuracy 0.458740234375\n",
      "Iteration 80870 Training loss 0.04874199256300926 Validation loss 0.05320369079709053 Accuracy 0.459716796875\n",
      "Iteration 80880 Training loss 0.04957672208547592 Validation loss 0.052673377096652985 Accuracy 0.461669921875\n",
      "Iteration 80890 Training loss 0.049161382019519806 Validation loss 0.05310511589050293 Accuracy 0.45654296875\n",
      "Iteration 80900 Training loss 0.05295320600271225 Validation loss 0.052849601954221725 Accuracy 0.4609375\n",
      "Iteration 80910 Training loss 0.05405427888035774 Validation loss 0.05312421917915344 Accuracy 0.460205078125\n",
      "Iteration 80920 Training loss 0.053469400852918625 Validation loss 0.05266303941607475 Accuracy 0.46142578125\n",
      "Iteration 80930 Training loss 0.053457509726285934 Validation loss 0.052826687693595886 Accuracy 0.461669921875\n",
      "Iteration 80940 Training loss 0.049369927495718 Validation loss 0.05287255346775055 Accuracy 0.461181640625\n",
      "Iteration 80950 Training loss 0.04994222894310951 Validation loss 0.053585320711135864 Accuracy 0.457275390625\n",
      "Iteration 80960 Training loss 0.04954872652888298 Validation loss 0.05260695144534111 Accuracy 0.462646484375\n",
      "Iteration 80970 Training loss 0.04869707301259041 Validation loss 0.05296197533607483 Accuracy 0.46044921875\n",
      "Iteration 80980 Training loss 0.04852334037423134 Validation loss 0.05282047763466835 Accuracy 0.4619140625\n",
      "Iteration 80990 Training loss 0.05176728218793869 Validation loss 0.05298833176493645 Accuracy 0.4609375\n",
      "Iteration 81000 Training loss 0.052836038172245026 Validation loss 0.053194086998701096 Accuracy 0.4580078125\n",
      "Iteration 81010 Training loss 0.04889141768217087 Validation loss 0.05298878625035286 Accuracy 0.4599609375\n",
      "Iteration 81020 Training loss 0.052328288555145264 Validation loss 0.052767783403396606 Accuracy 0.460693359375\n",
      "Iteration 81030 Training loss 0.051282115280628204 Validation loss 0.05276046320796013 Accuracy 0.462646484375\n",
      "Iteration 81040 Training loss 0.05182318016886711 Validation loss 0.05313558503985405 Accuracy 0.4599609375\n",
      "Iteration 81050 Training loss 0.05231776461005211 Validation loss 0.053318314254283905 Accuracy 0.45947265625\n",
      "Iteration 81060 Training loss 0.05026378482580185 Validation loss 0.05306660383939743 Accuracy 0.459228515625\n",
      "Iteration 81070 Training loss 0.05179180949926376 Validation loss 0.0533040314912796 Accuracy 0.458984375\n",
      "Iteration 81080 Training loss 0.048163071274757385 Validation loss 0.05273200944066048 Accuracy 0.460205078125\n",
      "Iteration 81090 Training loss 0.051554303616285324 Validation loss 0.052706483751535416 Accuracy 0.460693359375\n",
      "Iteration 81100 Training loss 0.04997144639492035 Validation loss 0.053010132163763046 Accuracy 0.4609375\n",
      "Iteration 81110 Training loss 0.05076538026332855 Validation loss 0.05283487215638161 Accuracy 0.461669921875\n",
      "Iteration 81120 Training loss 0.05185600742697716 Validation loss 0.05356874689459801 Accuracy 0.455322265625\n",
      "Iteration 81130 Training loss 0.04958494007587433 Validation loss 0.05314776301383972 Accuracy 0.4580078125\n",
      "Iteration 81140 Training loss 0.053367551416158676 Validation loss 0.05333781987428665 Accuracy 0.45703125\n",
      "Iteration 81150 Training loss 0.05152271315455437 Validation loss 0.05333970487117767 Accuracy 0.45703125\n",
      "Iteration 81160 Training loss 0.0506107471883297 Validation loss 0.05266949161887169 Accuracy 0.46142578125\n",
      "Iteration 81170 Training loss 0.05184617638587952 Validation loss 0.0532245971262455 Accuracy 0.45703125\n",
      "Iteration 81180 Training loss 0.05363839492201805 Validation loss 0.05330651253461838 Accuracy 0.45751953125\n",
      "Iteration 81190 Training loss 0.05173132196068764 Validation loss 0.05285610258579254 Accuracy 0.4609375\n",
      "Iteration 81200 Training loss 0.05088568106293678 Validation loss 0.052825942635536194 Accuracy 0.459716796875\n",
      "Iteration 81210 Training loss 0.054778918623924255 Validation loss 0.05298730731010437 Accuracy 0.460205078125\n",
      "Iteration 81220 Training loss 0.05375547334551811 Validation loss 0.05395674705505371 Accuracy 0.452392578125\n",
      "Iteration 81230 Training loss 0.05283680558204651 Validation loss 0.052964188158512115 Accuracy 0.460205078125\n",
      "Iteration 81240 Training loss 0.05007021129131317 Validation loss 0.05272757261991501 Accuracy 0.460693359375\n",
      "Iteration 81250 Training loss 0.053376056253910065 Validation loss 0.053885117173194885 Accuracy 0.452392578125\n",
      "Iteration 81260 Training loss 0.05393686890602112 Validation loss 0.05308607220649719 Accuracy 0.459716796875\n",
      "Iteration 81270 Training loss 0.048784926533699036 Validation loss 0.05302578955888748 Accuracy 0.461181640625\n",
      "Iteration 81280 Training loss 0.0518806092441082 Validation loss 0.052956800907850266 Accuracy 0.4599609375\n",
      "Iteration 81290 Training loss 0.04571134224534035 Validation loss 0.05330919474363327 Accuracy 0.45751953125\n",
      "Iteration 81300 Training loss 0.05101938173174858 Validation loss 0.05286021530628204 Accuracy 0.460693359375\n",
      "Iteration 81310 Training loss 0.05326370149850845 Validation loss 0.053102340549230576 Accuracy 0.458740234375\n",
      "Iteration 81320 Training loss 0.050761763006448746 Validation loss 0.05325676500797272 Accuracy 0.4580078125\n",
      "Iteration 81330 Training loss 0.048175517469644547 Validation loss 0.05293717980384827 Accuracy 0.460693359375\n",
      "Iteration 81340 Training loss 0.051376014947891235 Validation loss 0.05301956459879875 Accuracy 0.46044921875\n",
      "Iteration 81350 Training loss 0.04821513965725899 Validation loss 0.0527765154838562 Accuracy 0.46142578125\n",
      "Iteration 81360 Training loss 0.04907922446727753 Validation loss 0.05287949740886688 Accuracy 0.4609375\n",
      "Iteration 81370 Training loss 0.053134892135858536 Validation loss 0.05277999863028526 Accuracy 0.460693359375\n",
      "Iteration 81380 Training loss 0.0489375926554203 Validation loss 0.05287580564618111 Accuracy 0.460693359375\n",
      "Iteration 81390 Training loss 0.050930388271808624 Validation loss 0.052652668207883835 Accuracy 0.4619140625\n",
      "Iteration 81400 Training loss 0.05000226944684982 Validation loss 0.0529683418571949 Accuracy 0.460693359375\n",
      "Iteration 81410 Training loss 0.050101663917303085 Validation loss 0.05300263687968254 Accuracy 0.458984375\n",
      "Iteration 81420 Training loss 0.045432403683662415 Validation loss 0.05279538780450821 Accuracy 0.460693359375\n",
      "Iteration 81430 Training loss 0.053135864436626434 Validation loss 0.053418420255184174 Accuracy 0.456298828125\n",
      "Iteration 81440 Training loss 0.050142038613557816 Validation loss 0.05316655710339546 Accuracy 0.457763671875\n",
      "Iteration 81450 Training loss 0.0505354106426239 Validation loss 0.05260021239519119 Accuracy 0.46142578125\n",
      "Iteration 81460 Training loss 0.04580735042691231 Validation loss 0.05288781225681305 Accuracy 0.460693359375\n",
      "Iteration 81470 Training loss 0.050167493522167206 Validation loss 0.05286155268549919 Accuracy 0.4609375\n",
      "Iteration 81480 Training loss 0.05117972940206528 Validation loss 0.052723415195941925 Accuracy 0.461669921875\n",
      "Iteration 81490 Training loss 0.0525432825088501 Validation loss 0.0532764233648777 Accuracy 0.46044921875\n",
      "Iteration 81500 Training loss 0.0484001561999321 Validation loss 0.05329114943742752 Accuracy 0.459228515625\n",
      "Iteration 81510 Training loss 0.05200077220797539 Validation loss 0.05275744944810867 Accuracy 0.461181640625\n",
      "Iteration 81520 Training loss 0.05214397981762886 Validation loss 0.05296555906534195 Accuracy 0.4599609375\n",
      "Iteration 81530 Training loss 0.0529208667576313 Validation loss 0.05288846418261528 Accuracy 0.4609375\n",
      "Iteration 81540 Training loss 0.05194615572690964 Validation loss 0.05275507643818855 Accuracy 0.461181640625\n",
      "Iteration 81550 Training loss 0.05270392820239067 Validation loss 0.05291297659277916 Accuracy 0.459228515625\n",
      "Iteration 81560 Training loss 0.05318097397685051 Validation loss 0.0527169369161129 Accuracy 0.460693359375\n",
      "Iteration 81570 Training loss 0.04850884899497032 Validation loss 0.05305994674563408 Accuracy 0.458984375\n",
      "Iteration 81580 Training loss 0.05017775297164917 Validation loss 0.05266513675451279 Accuracy 0.461181640625\n",
      "Iteration 81590 Training loss 0.04683564230799675 Validation loss 0.052783749997615814 Accuracy 0.46240234375\n",
      "Iteration 81600 Training loss 0.052558302879333496 Validation loss 0.05270294472575188 Accuracy 0.4619140625\n",
      "Iteration 81610 Training loss 0.051412615925073624 Validation loss 0.05284205079078674 Accuracy 0.460693359375\n",
      "Iteration 81620 Training loss 0.049563467502593994 Validation loss 0.0529661700129509 Accuracy 0.460693359375\n",
      "Iteration 81630 Training loss 0.054389677941799164 Validation loss 0.053462915122509 Accuracy 0.455810546875\n",
      "Iteration 81640 Training loss 0.0476565808057785 Validation loss 0.052763622254133224 Accuracy 0.46142578125\n",
      "Iteration 81650 Training loss 0.050378087908029556 Validation loss 0.0533396452665329 Accuracy 0.4599609375\n",
      "Iteration 81660 Training loss 0.05323343724012375 Validation loss 0.05352737009525299 Accuracy 0.455810546875\n",
      "Iteration 81670 Training loss 0.04919563606381416 Validation loss 0.05284304544329643 Accuracy 0.461669921875\n",
      "Iteration 81680 Training loss 0.0470990389585495 Validation loss 0.05273851752281189 Accuracy 0.461181640625\n",
      "Iteration 81690 Training loss 0.053978219628334045 Validation loss 0.053223300725221634 Accuracy 0.458984375\n",
      "Iteration 81700 Training loss 0.051125772297382355 Validation loss 0.052926525473594666 Accuracy 0.46240234375\n",
      "Iteration 81710 Training loss 0.04806883633136749 Validation loss 0.05272615700960159 Accuracy 0.4609375\n",
      "Iteration 81720 Training loss 0.05206365883350372 Validation loss 0.05295870825648308 Accuracy 0.461669921875\n",
      "Iteration 81730 Training loss 0.05002951994538307 Validation loss 0.052796658128499985 Accuracy 0.4619140625\n",
      "Iteration 81740 Training loss 0.0492384135723114 Validation loss 0.05278893560171127 Accuracy 0.461669921875\n",
      "Iteration 81750 Training loss 0.053717389702796936 Validation loss 0.05316668748855591 Accuracy 0.458740234375\n",
      "Iteration 81760 Training loss 0.052341584116220474 Validation loss 0.05314505845308304 Accuracy 0.459228515625\n",
      "Iteration 81770 Training loss 0.04843746870756149 Validation loss 0.05290704593062401 Accuracy 0.46142578125\n",
      "Iteration 81780 Training loss 0.05039588361978531 Validation loss 0.05281147360801697 Accuracy 0.461181640625\n",
      "Iteration 81790 Training loss 0.049263499677181244 Validation loss 0.052964240312576294 Accuracy 0.46044921875\n",
      "Iteration 81800 Training loss 0.051571398973464966 Validation loss 0.052987780421972275 Accuracy 0.459716796875\n",
      "Iteration 81810 Training loss 0.052090905606746674 Validation loss 0.05273476243019104 Accuracy 0.46142578125\n",
      "Iteration 81820 Training loss 0.05046910047531128 Validation loss 0.05291852355003357 Accuracy 0.462158203125\n",
      "Iteration 81830 Training loss 0.05063256621360779 Validation loss 0.05327371507883072 Accuracy 0.459228515625\n",
      "Iteration 81840 Training loss 0.049541179090738297 Validation loss 0.052932947874069214 Accuracy 0.460205078125\n",
      "Iteration 81850 Training loss 0.05124267563223839 Validation loss 0.05289759486913681 Accuracy 0.459716796875\n",
      "Iteration 81860 Training loss 0.05172078311443329 Validation loss 0.052511222660541534 Accuracy 0.462646484375\n",
      "Iteration 81870 Training loss 0.047856565564870834 Validation loss 0.052615322172641754 Accuracy 0.4619140625\n",
      "Iteration 81880 Training loss 0.049008436501026154 Validation loss 0.05271415784955025 Accuracy 0.46240234375\n",
      "Iteration 81890 Training loss 0.04736948013305664 Validation loss 0.05293232202529907 Accuracy 0.4599609375\n",
      "Iteration 81900 Training loss 0.049210529774427414 Validation loss 0.0529380701482296 Accuracy 0.45849609375\n",
      "Iteration 81910 Training loss 0.05112139508128166 Validation loss 0.0526631623506546 Accuracy 0.462890625\n",
      "Iteration 81920 Training loss 0.05213766545057297 Validation loss 0.05320952832698822 Accuracy 0.460693359375\n",
      "Iteration 81930 Training loss 0.05186958238482475 Validation loss 0.05255965515971184 Accuracy 0.462890625\n",
      "Iteration 81940 Training loss 0.04913124442100525 Validation loss 0.05275703966617584 Accuracy 0.46142578125\n",
      "Iteration 81950 Training loss 0.05194723606109619 Validation loss 0.05286402255296707 Accuracy 0.461181640625\n",
      "Iteration 81960 Training loss 0.04999254271388054 Validation loss 0.05272587388753891 Accuracy 0.462890625\n",
      "Iteration 81970 Training loss 0.05184949189424515 Validation loss 0.053061842918395996 Accuracy 0.461181640625\n",
      "Iteration 81980 Training loss 0.05166913941502571 Validation loss 0.052724823355674744 Accuracy 0.4619140625\n",
      "Iteration 81990 Training loss 0.05292093753814697 Validation loss 0.052890144288539886 Accuracy 0.460693359375\n",
      "Iteration 82000 Training loss 0.052791137248277664 Validation loss 0.05328479781746864 Accuracy 0.4599609375\n",
      "Iteration 82010 Training loss 0.049510203301906586 Validation loss 0.05296040698885918 Accuracy 0.4619140625\n",
      "Iteration 82020 Training loss 0.05081206187605858 Validation loss 0.053077589720487595 Accuracy 0.4580078125\n",
      "Iteration 82030 Training loss 0.0539434440433979 Validation loss 0.05339091643691063 Accuracy 0.45703125\n",
      "Iteration 82040 Training loss 0.051307544112205505 Validation loss 0.05272950604557991 Accuracy 0.46142578125\n",
      "Iteration 82050 Training loss 0.0461861789226532 Validation loss 0.052972082048654556 Accuracy 0.4599609375\n",
      "Iteration 82060 Training loss 0.05269159749150276 Validation loss 0.05253726989030838 Accuracy 0.4619140625\n",
      "Iteration 82070 Training loss 0.050058215856552124 Validation loss 0.052970487624406815 Accuracy 0.4609375\n",
      "Iteration 82080 Training loss 0.05207342654466629 Validation loss 0.052820343524217606 Accuracy 0.459716796875\n",
      "Iteration 82090 Training loss 0.04964352399110794 Validation loss 0.05362888425588608 Accuracy 0.456787109375\n",
      "Iteration 82100 Training loss 0.0464177243411541 Validation loss 0.05259003862738609 Accuracy 0.462890625\n",
      "Iteration 82110 Training loss 0.047870952636003494 Validation loss 0.05303886532783508 Accuracy 0.4599609375\n",
      "Iteration 82120 Training loss 0.051581431180238724 Validation loss 0.05280197039246559 Accuracy 0.46044921875\n",
      "Iteration 82130 Training loss 0.04799969494342804 Validation loss 0.052831150591373444 Accuracy 0.460693359375\n",
      "Iteration 82140 Training loss 0.05196633189916611 Validation loss 0.05306590721011162 Accuracy 0.45947265625\n",
      "Iteration 82150 Training loss 0.05023655295372009 Validation loss 0.052682049572467804 Accuracy 0.461181640625\n",
      "Iteration 82160 Training loss 0.05144764110445976 Validation loss 0.05277850478887558 Accuracy 0.4609375\n",
      "Iteration 82170 Training loss 0.047778189182281494 Validation loss 0.05256456509232521 Accuracy 0.46240234375\n",
      "Iteration 82180 Training loss 0.05243873968720436 Validation loss 0.05327440798282623 Accuracy 0.460205078125\n",
      "Iteration 82190 Training loss 0.050689391791820526 Validation loss 0.052830521017313004 Accuracy 0.4619140625\n",
      "Iteration 82200 Training loss 0.0471537783741951 Validation loss 0.052899070084095 Accuracy 0.4609375\n",
      "Iteration 82210 Training loss 0.04897312447428703 Validation loss 0.0530216209590435 Accuracy 0.459228515625\n",
      "Iteration 82220 Training loss 0.0540941059589386 Validation loss 0.05316835641860962 Accuracy 0.459716796875\n",
      "Iteration 82230 Training loss 0.0528130829334259 Validation loss 0.05332205817103386 Accuracy 0.457275390625\n",
      "Iteration 82240 Training loss 0.05079346522688866 Validation loss 0.05313101038336754 Accuracy 0.458984375\n",
      "Iteration 82250 Training loss 0.04921108856797218 Validation loss 0.053012341260910034 Accuracy 0.460205078125\n",
      "Iteration 82260 Training loss 0.0500309132039547 Validation loss 0.05276019871234894 Accuracy 0.4609375\n",
      "Iteration 82270 Training loss 0.051833536475896835 Validation loss 0.052908968180418015 Accuracy 0.46142578125\n",
      "Iteration 82280 Training loss 0.04885237663984299 Validation loss 0.052698925137519836 Accuracy 0.4619140625\n",
      "Iteration 82290 Training loss 0.046567343175411224 Validation loss 0.05277736112475395 Accuracy 0.46142578125\n",
      "Iteration 82300 Training loss 0.048964131623506546 Validation loss 0.05262979492545128 Accuracy 0.462158203125\n",
      "Iteration 82310 Training loss 0.053919270634651184 Validation loss 0.05281492695212364 Accuracy 0.46240234375\n",
      "Iteration 82320 Training loss 0.05351932719349861 Validation loss 0.052640896290540695 Accuracy 0.4619140625\n",
      "Iteration 82330 Training loss 0.05176417902112007 Validation loss 0.05255090072751045 Accuracy 0.461669921875\n",
      "Iteration 82340 Training loss 0.04582059755921364 Validation loss 0.05281369015574455 Accuracy 0.4619140625\n",
      "Iteration 82350 Training loss 0.050993673503398895 Validation loss 0.05293228477239609 Accuracy 0.4609375\n",
      "Iteration 82360 Training loss 0.0508141964673996 Validation loss 0.05350706726312637 Accuracy 0.457275390625\n",
      "Iteration 82370 Training loss 0.04926859959959984 Validation loss 0.05288975313305855 Accuracy 0.4609375\n",
      "Iteration 82380 Training loss 0.05181962996721268 Validation loss 0.052707329392433167 Accuracy 0.461669921875\n",
      "Iteration 82390 Training loss 0.04966690391302109 Validation loss 0.0530802384018898 Accuracy 0.46044921875\n",
      "Iteration 82400 Training loss 0.04975498840212822 Validation loss 0.0530814602971077 Accuracy 0.458984375\n",
      "Iteration 82410 Training loss 0.05013294517993927 Validation loss 0.053139813244342804 Accuracy 0.459716796875\n",
      "Iteration 82420 Training loss 0.05320221930742264 Validation loss 0.05365092307329178 Accuracy 0.455322265625\n",
      "Iteration 82430 Training loss 0.04903911426663399 Validation loss 0.0530017726123333 Accuracy 0.460205078125\n",
      "Iteration 82440 Training loss 0.05169233679771423 Validation loss 0.053127676248550415 Accuracy 0.46044921875\n",
      "Iteration 82450 Training loss 0.05041376128792763 Validation loss 0.052950311452150345 Accuracy 0.460693359375\n",
      "Iteration 82460 Training loss 0.05062761530280113 Validation loss 0.05364179611206055 Accuracy 0.455322265625\n",
      "Iteration 82470 Training loss 0.048381682485342026 Validation loss 0.052689746022224426 Accuracy 0.4619140625\n",
      "Iteration 82480 Training loss 0.049932338297367096 Validation loss 0.052548848092556 Accuracy 0.4619140625\n",
      "Iteration 82490 Training loss 0.04930299147963524 Validation loss 0.0527738556265831 Accuracy 0.460693359375\n",
      "Iteration 82500 Training loss 0.05193816125392914 Validation loss 0.052708644419908524 Accuracy 0.462890625\n",
      "Iteration 82510 Training loss 0.044423237442970276 Validation loss 0.052924901247024536 Accuracy 0.461181640625\n",
      "Iteration 82520 Training loss 0.054479777812957764 Validation loss 0.053094785660505295 Accuracy 0.4599609375\n",
      "Iteration 82530 Training loss 0.04708496108651161 Validation loss 0.05286934971809387 Accuracy 0.4609375\n",
      "Iteration 82540 Training loss 0.05232900008559227 Validation loss 0.05314949154853821 Accuracy 0.459716796875\n",
      "Iteration 82550 Training loss 0.053069598972797394 Validation loss 0.0529060922563076 Accuracy 0.460693359375\n",
      "Iteration 82560 Training loss 0.05172499641776085 Validation loss 0.052990663796663284 Accuracy 0.45947265625\n",
      "Iteration 82570 Training loss 0.05259737744927406 Validation loss 0.05295497551560402 Accuracy 0.460693359375\n",
      "Iteration 82580 Training loss 0.04962313920259476 Validation loss 0.05333559960126877 Accuracy 0.45654296875\n",
      "Iteration 82590 Training loss 0.051485173404216766 Validation loss 0.05340560898184776 Accuracy 0.458984375\n",
      "Iteration 82600 Training loss 0.05120546743273735 Validation loss 0.05290183052420616 Accuracy 0.460693359375\n",
      "Iteration 82610 Training loss 0.05285683274269104 Validation loss 0.05296288803219795 Accuracy 0.46142578125\n",
      "Iteration 82620 Training loss 0.052110396325588226 Validation loss 0.053108230233192444 Accuracy 0.4599609375\n",
      "Iteration 82630 Training loss 0.05244774371385574 Validation loss 0.05334614962339401 Accuracy 0.458740234375\n",
      "Iteration 82640 Training loss 0.05325113981962204 Validation loss 0.05348336696624756 Accuracy 0.45458984375\n",
      "Iteration 82650 Training loss 0.04907488077878952 Validation loss 0.05284715071320534 Accuracy 0.460693359375\n",
      "Iteration 82660 Training loss 0.052171289920806885 Validation loss 0.0535154864192009 Accuracy 0.457275390625\n",
      "Iteration 82670 Training loss 0.04803447425365448 Validation loss 0.05296360328793526 Accuracy 0.460693359375\n",
      "Iteration 82680 Training loss 0.049532316625118256 Validation loss 0.053009629249572754 Accuracy 0.459716796875\n",
      "Iteration 82690 Training loss 0.052716873586177826 Validation loss 0.05355892330408096 Accuracy 0.457275390625\n",
      "Iteration 82700 Training loss 0.052440907806158066 Validation loss 0.05284711718559265 Accuracy 0.462158203125\n",
      "Iteration 82710 Training loss 0.051209188997745514 Validation loss 0.05286261439323425 Accuracy 0.4619140625\n",
      "Iteration 82720 Training loss 0.050077665597200394 Validation loss 0.052722252905368805 Accuracy 0.4619140625\n",
      "Iteration 82730 Training loss 0.05046512931585312 Validation loss 0.05292459949851036 Accuracy 0.460205078125\n",
      "Iteration 82740 Training loss 0.05428946018218994 Validation loss 0.052692532539367676 Accuracy 0.461181640625\n",
      "Iteration 82750 Training loss 0.0469597764313221 Validation loss 0.052943333983421326 Accuracy 0.4599609375\n",
      "Iteration 82760 Training loss 0.04960370808839798 Validation loss 0.05276702344417572 Accuracy 0.460693359375\n",
      "Iteration 82770 Training loss 0.05232399329543114 Validation loss 0.05311507731676102 Accuracy 0.459228515625\n",
      "Iteration 82780 Training loss 0.05310934782028198 Validation loss 0.053267527371644974 Accuracy 0.457275390625\n",
      "Iteration 82790 Training loss 0.053290657699108124 Validation loss 0.05283920839428902 Accuracy 0.460205078125\n",
      "Iteration 82800 Training loss 0.05337982997298241 Validation loss 0.05290258303284645 Accuracy 0.459716796875\n",
      "Iteration 82810 Training loss 0.05226949602365494 Validation loss 0.052771810442209244 Accuracy 0.461181640625\n",
      "Iteration 82820 Training loss 0.049339380115270615 Validation loss 0.052606210112571716 Accuracy 0.4619140625\n",
      "Iteration 82830 Training loss 0.05045607313513756 Validation loss 0.05327717587351799 Accuracy 0.458984375\n",
      "Iteration 82840 Training loss 0.051413170993328094 Validation loss 0.05315106362104416 Accuracy 0.458740234375\n",
      "Iteration 82850 Training loss 0.05121729150414467 Validation loss 0.053361084312200546 Accuracy 0.4599609375\n",
      "Iteration 82860 Training loss 0.0498683862388134 Validation loss 0.05292774736881256 Accuracy 0.46044921875\n",
      "Iteration 82870 Training loss 0.052190836519002914 Validation loss 0.05322950705885887 Accuracy 0.4560546875\n",
      "Iteration 82880 Training loss 0.050209879875183105 Validation loss 0.05297207832336426 Accuracy 0.4619140625\n",
      "Iteration 82890 Training loss 0.04933081567287445 Validation loss 0.05300743877887726 Accuracy 0.46142578125\n",
      "Iteration 82900 Training loss 0.05088945850729942 Validation loss 0.052625872194767 Accuracy 0.461181640625\n",
      "Iteration 82910 Training loss 0.04934074357151985 Validation loss 0.05327693372964859 Accuracy 0.458740234375\n",
      "Iteration 82920 Training loss 0.0470484122633934 Validation loss 0.05285318195819855 Accuracy 0.460693359375\n",
      "Iteration 82930 Training loss 0.05030545964837074 Validation loss 0.05334293842315674 Accuracy 0.456298828125\n",
      "Iteration 82940 Training loss 0.049183521419763565 Validation loss 0.05324079096317291 Accuracy 0.4580078125\n",
      "Iteration 82950 Training loss 0.050087060779333115 Validation loss 0.052875276654958725 Accuracy 0.461181640625\n",
      "Iteration 82960 Training loss 0.051548294723033905 Validation loss 0.052958808839321136 Accuracy 0.4609375\n",
      "Iteration 82970 Training loss 0.052166275680065155 Validation loss 0.05350270867347717 Accuracy 0.455078125\n",
      "Iteration 82980 Training loss 0.0518648661673069 Validation loss 0.05287882685661316 Accuracy 0.462158203125\n",
      "Iteration 82990 Training loss 0.05115393549203873 Validation loss 0.053185153752565384 Accuracy 0.45703125\n",
      "Iteration 83000 Training loss 0.05115833505988121 Validation loss 0.052747443318367004 Accuracy 0.4619140625\n",
      "Iteration 83010 Training loss 0.049399733543395996 Validation loss 0.052806299179792404 Accuracy 0.46044921875\n",
      "Iteration 83020 Training loss 0.04795383661985397 Validation loss 0.05322061479091644 Accuracy 0.460693359375\n",
      "Iteration 83030 Training loss 0.05366120487451553 Validation loss 0.05335115268826485 Accuracy 0.458740234375\n",
      "Iteration 83040 Training loss 0.04668194055557251 Validation loss 0.053042780607938766 Accuracy 0.45947265625\n",
      "Iteration 83050 Training loss 0.05262903869152069 Validation loss 0.05306365713477135 Accuracy 0.459716796875\n",
      "Iteration 83060 Training loss 0.05315467715263367 Validation loss 0.05273125320672989 Accuracy 0.461669921875\n",
      "Iteration 83070 Training loss 0.05119229108095169 Validation loss 0.05318988114595413 Accuracy 0.458740234375\n",
      "Iteration 83080 Training loss 0.0510309636592865 Validation loss 0.052824027836322784 Accuracy 0.4609375\n",
      "Iteration 83090 Training loss 0.05219853296875954 Validation loss 0.0528809130191803 Accuracy 0.460693359375\n",
      "Iteration 83100 Training loss 0.05035321041941643 Validation loss 0.052915964275598526 Accuracy 0.462158203125\n",
      "Iteration 83110 Training loss 0.049763232469558716 Validation loss 0.05299730971455574 Accuracy 0.459716796875\n",
      "Iteration 83120 Training loss 0.05628181993961334 Validation loss 0.052660971879959106 Accuracy 0.461181640625\n",
      "Iteration 83130 Training loss 0.05262356624007225 Validation loss 0.05321859195828438 Accuracy 0.459228515625\n",
      "Iteration 83140 Training loss 0.0496034137904644 Validation loss 0.053085487335920334 Accuracy 0.45947265625\n",
      "Iteration 83150 Training loss 0.052115172147750854 Validation loss 0.05299372971057892 Accuracy 0.459716796875\n",
      "Iteration 83160 Training loss 0.04896020516753197 Validation loss 0.05302336439490318 Accuracy 0.46240234375\n",
      "Iteration 83170 Training loss 0.05278016999363899 Validation loss 0.05274265632033348 Accuracy 0.46240234375\n",
      "Iteration 83180 Training loss 0.05299869552254677 Validation loss 0.052747245877981186 Accuracy 0.461181640625\n",
      "Iteration 83190 Training loss 0.05077405646443367 Validation loss 0.053165726363658905 Accuracy 0.458984375\n",
      "Iteration 83200 Training loss 0.05007653683423996 Validation loss 0.05288843438029289 Accuracy 0.459716796875\n",
      "Iteration 83210 Training loss 0.05371885001659393 Validation loss 0.05285126715898514 Accuracy 0.460693359375\n",
      "Iteration 83220 Training loss 0.05041128396987915 Validation loss 0.053122639656066895 Accuracy 0.45947265625\n",
      "Iteration 83230 Training loss 0.05527453124523163 Validation loss 0.0531441867351532 Accuracy 0.460693359375\n",
      "Iteration 83240 Training loss 0.054918695241212845 Validation loss 0.05364125967025757 Accuracy 0.453857421875\n",
      "Iteration 83250 Training loss 0.053190816193819046 Validation loss 0.05260571092367172 Accuracy 0.462158203125\n",
      "Iteration 83260 Training loss 0.051270950585603714 Validation loss 0.05281345546245575 Accuracy 0.461181640625\n",
      "Iteration 83270 Training loss 0.05138606205582619 Validation loss 0.05271967127919197 Accuracy 0.46240234375\n",
      "Iteration 83280 Training loss 0.05102818086743355 Validation loss 0.052657775580883026 Accuracy 0.4619140625\n",
      "Iteration 83290 Training loss 0.05042606592178345 Validation loss 0.052843522280454636 Accuracy 0.461181640625\n",
      "Iteration 83300 Training loss 0.051580555737018585 Validation loss 0.05331595987081528 Accuracy 0.45703125\n",
      "Iteration 83310 Training loss 0.04936516284942627 Validation loss 0.05276060104370117 Accuracy 0.4609375\n",
      "Iteration 83320 Training loss 0.047622695565223694 Validation loss 0.052867624908685684 Accuracy 0.459716796875\n",
      "Iteration 83330 Training loss 0.05184977129101753 Validation loss 0.05280071124434471 Accuracy 0.461669921875\n",
      "Iteration 83340 Training loss 0.053009048104286194 Validation loss 0.053208984434604645 Accuracy 0.458984375\n",
      "Iteration 83350 Training loss 0.048302315175533295 Validation loss 0.05274687707424164 Accuracy 0.4619140625\n",
      "Iteration 83360 Training loss 0.05071989446878433 Validation loss 0.05275654047727585 Accuracy 0.4619140625\n",
      "Iteration 83370 Training loss 0.05089137330651283 Validation loss 0.052810847759246826 Accuracy 0.4619140625\n",
      "Iteration 83380 Training loss 0.05206617712974548 Validation loss 0.05301772430539131 Accuracy 0.46044921875\n",
      "Iteration 83390 Training loss 0.047874726355075836 Validation loss 0.05300804600119591 Accuracy 0.45751953125\n",
      "Iteration 83400 Training loss 0.051690153777599335 Validation loss 0.0528254397213459 Accuracy 0.461181640625\n",
      "Iteration 83410 Training loss 0.053461965173482895 Validation loss 0.053201619535684586 Accuracy 0.45849609375\n",
      "Iteration 83420 Training loss 0.05020296201109886 Validation loss 0.05285171791911125 Accuracy 0.459716796875\n",
      "Iteration 83430 Training loss 0.05053648725152016 Validation loss 0.052993543446063995 Accuracy 0.4609375\n",
      "Iteration 83440 Training loss 0.05078430473804474 Validation loss 0.05279457941651344 Accuracy 0.4619140625\n",
      "Iteration 83450 Training loss 0.053251445293426514 Validation loss 0.05289461836218834 Accuracy 0.460205078125\n",
      "Iteration 83460 Training loss 0.0499105229973793 Validation loss 0.05270306393504143 Accuracy 0.4609375\n",
      "Iteration 83470 Training loss 0.047974273562431335 Validation loss 0.05280376970767975 Accuracy 0.46142578125\n",
      "Iteration 83480 Training loss 0.05044012516736984 Validation loss 0.052919499576091766 Accuracy 0.460693359375\n",
      "Iteration 83490 Training loss 0.04916467145085335 Validation loss 0.052690815180540085 Accuracy 0.460693359375\n",
      "Iteration 83500 Training loss 0.05193781107664108 Validation loss 0.052720263600349426 Accuracy 0.461669921875\n",
      "Iteration 83510 Training loss 0.046361733227968216 Validation loss 0.05288200452923775 Accuracy 0.461669921875\n",
      "Iteration 83520 Training loss 0.054056014865636826 Validation loss 0.05452969670295715 Accuracy 0.4462890625\n",
      "Iteration 83530 Training loss 0.05302068963646889 Validation loss 0.05279986560344696 Accuracy 0.461181640625\n",
      "Iteration 83540 Training loss 0.04747006669640541 Validation loss 0.053114134818315506 Accuracy 0.4580078125\n",
      "Iteration 83550 Training loss 0.05093954876065254 Validation loss 0.052905935794115067 Accuracy 0.458740234375\n",
      "Iteration 83560 Training loss 0.04476232826709747 Validation loss 0.05298749729990959 Accuracy 0.4609375\n",
      "Iteration 83570 Training loss 0.05506221577525139 Validation loss 0.05339691415429115 Accuracy 0.45703125\n",
      "Iteration 83580 Training loss 0.04764695093035698 Validation loss 0.05304061248898506 Accuracy 0.458984375\n",
      "Iteration 83590 Training loss 0.04898255690932274 Validation loss 0.05348477512598038 Accuracy 0.456298828125\n",
      "Iteration 83600 Training loss 0.05035349354147911 Validation loss 0.05270775407552719 Accuracy 0.4609375\n",
      "Iteration 83610 Training loss 0.047901563346385956 Validation loss 0.053172312676906586 Accuracy 0.46044921875\n",
      "Iteration 83620 Training loss 0.05293135344982147 Validation loss 0.0532195158302784 Accuracy 0.45751953125\n",
      "Iteration 83630 Training loss 0.05091600492596626 Validation loss 0.0531509667634964 Accuracy 0.458984375\n",
      "Iteration 83640 Training loss 0.05132894217967987 Validation loss 0.0528515949845314 Accuracy 0.461181640625\n",
      "Iteration 83650 Training loss 0.051318079233169556 Validation loss 0.053513944149017334 Accuracy 0.45751953125\n",
      "Iteration 83660 Training loss 0.047233376652002335 Validation loss 0.05294255539774895 Accuracy 0.461181640625\n",
      "Iteration 83670 Training loss 0.05125243961811066 Validation loss 0.05314940959215164 Accuracy 0.45947265625\n",
      "Iteration 83680 Training loss 0.05032164603471756 Validation loss 0.05302418768405914 Accuracy 0.4599609375\n",
      "Iteration 83690 Training loss 0.05132826045155525 Validation loss 0.05282306671142578 Accuracy 0.458984375\n",
      "Iteration 83700 Training loss 0.05102751776576042 Validation loss 0.05355655029416084 Accuracy 0.453125\n",
      "Iteration 83710 Training loss 0.04904897138476372 Validation loss 0.05299857631325722 Accuracy 0.460693359375\n",
      "Iteration 83720 Training loss 0.051690369844436646 Validation loss 0.05324193090200424 Accuracy 0.45654296875\n",
      "Iteration 83730 Training loss 0.05093386396765709 Validation loss 0.0529886856675148 Accuracy 0.458984375\n",
      "Iteration 83740 Training loss 0.05116710811853409 Validation loss 0.05340006574988365 Accuracy 0.456298828125\n",
      "Iteration 83750 Training loss 0.05204835906624794 Validation loss 0.05288100615143776 Accuracy 0.4619140625\n",
      "Iteration 83760 Training loss 0.05223015323281288 Validation loss 0.052783701568841934 Accuracy 0.46240234375\n",
      "Iteration 83770 Training loss 0.051429301500320435 Validation loss 0.052782539278268814 Accuracy 0.462158203125\n",
      "Iteration 83780 Training loss 0.050745125859975815 Validation loss 0.05344437435269356 Accuracy 0.454345703125\n",
      "Iteration 83790 Training loss 0.05285147950053215 Validation loss 0.05295894294977188 Accuracy 0.4609375\n",
      "Iteration 83800 Training loss 0.051420584321022034 Validation loss 0.052789490669965744 Accuracy 0.462890625\n",
      "Iteration 83810 Training loss 0.04603792726993561 Validation loss 0.052760861814022064 Accuracy 0.462646484375\n",
      "Iteration 83820 Training loss 0.047909777611494064 Validation loss 0.053213391453027725 Accuracy 0.460693359375\n",
      "Iteration 83830 Training loss 0.05216025561094284 Validation loss 0.05301651358604431 Accuracy 0.460205078125\n",
      "Iteration 83840 Training loss 0.05351642891764641 Validation loss 0.05257439240813255 Accuracy 0.462646484375\n",
      "Iteration 83850 Training loss 0.050301238894462585 Validation loss 0.05337521433830261 Accuracy 0.458251953125\n",
      "Iteration 83860 Training loss 0.04768427461385727 Validation loss 0.05283074080944061 Accuracy 0.461181640625\n",
      "Iteration 83870 Training loss 0.048599012196063995 Validation loss 0.052766092121601105 Accuracy 0.461669921875\n",
      "Iteration 83880 Training loss 0.05152438208460808 Validation loss 0.05271915718913078 Accuracy 0.460693359375\n",
      "Iteration 83890 Training loss 0.04754715785384178 Validation loss 0.052632834762334824 Accuracy 0.462158203125\n",
      "Iteration 83900 Training loss 0.05194791033864021 Validation loss 0.052652206271886826 Accuracy 0.462646484375\n",
      "Iteration 83910 Training loss 0.05110638216137886 Validation loss 0.052814219146966934 Accuracy 0.46142578125\n",
      "Iteration 83920 Training loss 0.05140332132577896 Validation loss 0.053370602428913116 Accuracy 0.4560546875\n",
      "Iteration 83930 Training loss 0.05319930985569954 Validation loss 0.053303152322769165 Accuracy 0.457275390625\n",
      "Iteration 83940 Training loss 0.05268285050988197 Validation loss 0.05319388583302498 Accuracy 0.460205078125\n",
      "Iteration 83950 Training loss 0.05418688803911209 Validation loss 0.05355851352214813 Accuracy 0.45361328125\n",
      "Iteration 83960 Training loss 0.0524362288415432 Validation loss 0.0531882718205452 Accuracy 0.459228515625\n",
      "Iteration 83970 Training loss 0.05210331827402115 Validation loss 0.05293373018503189 Accuracy 0.4619140625\n",
      "Iteration 83980 Training loss 0.05108830705285072 Validation loss 0.05316042900085449 Accuracy 0.4599609375\n",
      "Iteration 83990 Training loss 0.05166544392704964 Validation loss 0.05277787148952484 Accuracy 0.462646484375\n",
      "Iteration 84000 Training loss 0.05287729203701019 Validation loss 0.05327005684375763 Accuracy 0.4609375\n",
      "Iteration 84010 Training loss 0.05039804056286812 Validation loss 0.05288539454340935 Accuracy 0.4609375\n",
      "Iteration 84020 Training loss 0.049766428768634796 Validation loss 0.05281425639986992 Accuracy 0.462890625\n",
      "Iteration 84030 Training loss 0.05123214051127434 Validation loss 0.05357849970459938 Accuracy 0.4560546875\n",
      "Iteration 84040 Training loss 0.05126725137233734 Validation loss 0.05311684310436249 Accuracy 0.4599609375\n",
      "Iteration 84050 Training loss 0.05124781280755997 Validation loss 0.053300641477108 Accuracy 0.459716796875\n",
      "Iteration 84060 Training loss 0.048730332404375076 Validation loss 0.05307276174426079 Accuracy 0.460693359375\n",
      "Iteration 84070 Training loss 0.04785570129752159 Validation loss 0.05285398289561272 Accuracy 0.462158203125\n",
      "Iteration 84080 Training loss 0.05349491164088249 Validation loss 0.05273667722940445 Accuracy 0.46240234375\n",
      "Iteration 84090 Training loss 0.04858735203742981 Validation loss 0.052807897329330444 Accuracy 0.4619140625\n",
      "Iteration 84100 Training loss 0.053848255425691605 Validation loss 0.05293897166848183 Accuracy 0.461181640625\n",
      "Iteration 84110 Training loss 0.05003504827618599 Validation loss 0.054170094430446625 Accuracy 0.44873046875\n",
      "Iteration 84120 Training loss 0.0520118847489357 Validation loss 0.052869684994220734 Accuracy 0.460205078125\n",
      "Iteration 84130 Training loss 0.04978375881910324 Validation loss 0.05306151136755943 Accuracy 0.4599609375\n",
      "Iteration 84140 Training loss 0.050797779113054276 Validation loss 0.053142987191677094 Accuracy 0.461669921875\n",
      "Iteration 84150 Training loss 0.05585421621799469 Validation loss 0.05282969772815704 Accuracy 0.462890625\n",
      "Iteration 84160 Training loss 0.04917558655142784 Validation loss 0.05312827602028847 Accuracy 0.460205078125\n",
      "Iteration 84170 Training loss 0.052008140832185745 Validation loss 0.053168345242738724 Accuracy 0.4609375\n",
      "Iteration 84180 Training loss 0.048992615193128586 Validation loss 0.0527777336537838 Accuracy 0.46142578125\n",
      "Iteration 84190 Training loss 0.055445555597543716 Validation loss 0.0528070405125618 Accuracy 0.46044921875\n",
      "Iteration 84200 Training loss 0.048190176486968994 Validation loss 0.05333646014332771 Accuracy 0.458984375\n",
      "Iteration 84210 Training loss 0.04966586083173752 Validation loss 0.05281996354460716 Accuracy 0.461669921875\n",
      "Iteration 84220 Training loss 0.047588642686605453 Validation loss 0.052683647722005844 Accuracy 0.46240234375\n",
      "Iteration 84230 Training loss 0.050361521542072296 Validation loss 0.05318642780184746 Accuracy 0.4599609375\n",
      "Iteration 84240 Training loss 0.05199049785733223 Validation loss 0.05311816930770874 Accuracy 0.460205078125\n",
      "Iteration 84250 Training loss 0.04897865653038025 Validation loss 0.05286107212305069 Accuracy 0.462646484375\n",
      "Iteration 84260 Training loss 0.04891153797507286 Validation loss 0.053176149725914 Accuracy 0.4599609375\n",
      "Iteration 84270 Training loss 0.051156383007764816 Validation loss 0.052797555923461914 Accuracy 0.461669921875\n",
      "Iteration 84280 Training loss 0.05143120884895325 Validation loss 0.052977029234170914 Accuracy 0.4609375\n",
      "Iteration 84290 Training loss 0.051666855812072754 Validation loss 0.053011082112789154 Accuracy 0.461181640625\n",
      "Iteration 84300 Training loss 0.0524911992251873 Validation loss 0.05337408557534218 Accuracy 0.458740234375\n",
      "Iteration 84310 Training loss 0.0452103354036808 Validation loss 0.05318174138665199 Accuracy 0.45849609375\n",
      "Iteration 84320 Training loss 0.050781331956386566 Validation loss 0.05344105511903763 Accuracy 0.455078125\n",
      "Iteration 84330 Training loss 0.04917427524924278 Validation loss 0.05273738130927086 Accuracy 0.4619140625\n",
      "Iteration 84340 Training loss 0.051831502467393875 Validation loss 0.053338855504989624 Accuracy 0.45751953125\n",
      "Iteration 84350 Training loss 0.04890439659357071 Validation loss 0.052821408957242966 Accuracy 0.46044921875\n",
      "Iteration 84360 Training loss 0.04726061969995499 Validation loss 0.05321959778666496 Accuracy 0.45751953125\n",
      "Iteration 84370 Training loss 0.05242261663079262 Validation loss 0.053173113614320755 Accuracy 0.45751953125\n",
      "Iteration 84380 Training loss 0.04657510668039322 Validation loss 0.05292036384344101 Accuracy 0.460693359375\n",
      "Iteration 84390 Training loss 0.05378671735525131 Validation loss 0.05326710641384125 Accuracy 0.459228515625\n",
      "Iteration 84400 Training loss 0.05152685567736626 Validation loss 0.05332193151116371 Accuracy 0.456787109375\n",
      "Iteration 84410 Training loss 0.054291076958179474 Validation loss 0.0529460646212101 Accuracy 0.45947265625\n",
      "Iteration 84420 Training loss 0.053864769637584686 Validation loss 0.05302489921450615 Accuracy 0.460693359375\n",
      "Iteration 84430 Training loss 0.04819656163454056 Validation loss 0.05306568369269371 Accuracy 0.460693359375\n",
      "Iteration 84440 Training loss 0.050825800746679306 Validation loss 0.05291737988591194 Accuracy 0.460693359375\n",
      "Iteration 84450 Training loss 0.04749182239174843 Validation loss 0.052748944610357285 Accuracy 0.461669921875\n",
      "Iteration 84460 Training loss 0.051951467990875244 Validation loss 0.05313776805996895 Accuracy 0.4609375\n",
      "Iteration 84470 Training loss 0.051212433725595474 Validation loss 0.05340628698468208 Accuracy 0.456298828125\n",
      "Iteration 84480 Training loss 0.04961712658405304 Validation loss 0.05290428549051285 Accuracy 0.460693359375\n",
      "Iteration 84490 Training loss 0.052552662789821625 Validation loss 0.05283180624246597 Accuracy 0.459228515625\n",
      "Iteration 84500 Training loss 0.05307335406541824 Validation loss 0.052923645824193954 Accuracy 0.46044921875\n",
      "Iteration 84510 Training loss 0.04859587177634239 Validation loss 0.053081072866916656 Accuracy 0.4599609375\n",
      "Iteration 84520 Training loss 0.05485456809401512 Validation loss 0.05313948914408684 Accuracy 0.458740234375\n",
      "Iteration 84530 Training loss 0.04971441626548767 Validation loss 0.05309672653675079 Accuracy 0.45703125\n",
      "Iteration 84540 Training loss 0.05207330361008644 Validation loss 0.05301040783524513 Accuracy 0.461181640625\n",
      "Iteration 84550 Training loss 0.05003267526626587 Validation loss 0.05267006531357765 Accuracy 0.461181640625\n",
      "Iteration 84560 Training loss 0.05173920467495918 Validation loss 0.05289078876376152 Accuracy 0.46044921875\n",
      "Iteration 84570 Training loss 0.051519010215997696 Validation loss 0.053204696625471115 Accuracy 0.458251953125\n",
      "Iteration 84580 Training loss 0.05273473635315895 Validation loss 0.053227975964546204 Accuracy 0.456787109375\n",
      "Iteration 84590 Training loss 0.05111372470855713 Validation loss 0.05377795919775963 Accuracy 0.452880859375\n",
      "Iteration 84600 Training loss 0.049718160182237625 Validation loss 0.05323507636785507 Accuracy 0.456298828125\n",
      "Iteration 84610 Training loss 0.05199766904115677 Validation loss 0.05335027351975441 Accuracy 0.458984375\n",
      "Iteration 84620 Training loss 0.050811488181352615 Validation loss 0.052670419216156006 Accuracy 0.461181640625\n",
      "Iteration 84630 Training loss 0.048344023525714874 Validation loss 0.053140789270401 Accuracy 0.45849609375\n",
      "Iteration 84640 Training loss 0.04873715341091156 Validation loss 0.053258318454027176 Accuracy 0.458984375\n",
      "Iteration 84650 Training loss 0.04945439472794533 Validation loss 0.053051795810461044 Accuracy 0.460205078125\n",
      "Iteration 84660 Training loss 0.05251758173108101 Validation loss 0.053301118314266205 Accuracy 0.45849609375\n",
      "Iteration 84670 Training loss 0.05086127296090126 Validation loss 0.05280301719903946 Accuracy 0.460205078125\n",
      "Iteration 84680 Training loss 0.050939690321683884 Validation loss 0.05338944122195244 Accuracy 0.456787109375\n",
      "Iteration 84690 Training loss 0.051206886768341064 Validation loss 0.05306440219283104 Accuracy 0.459716796875\n",
      "Iteration 84700 Training loss 0.050129830837249756 Validation loss 0.052561093121767044 Accuracy 0.4619140625\n",
      "Iteration 84710 Training loss 0.049726251512765884 Validation loss 0.05280790850520134 Accuracy 0.45947265625\n",
      "Iteration 84720 Training loss 0.04857365041971207 Validation loss 0.05272683873772621 Accuracy 0.46044921875\n",
      "Iteration 84730 Training loss 0.052350565791130066 Validation loss 0.05327868089079857 Accuracy 0.457763671875\n",
      "Iteration 84740 Training loss 0.04785088077187538 Validation loss 0.052806854248046875 Accuracy 0.46044921875\n",
      "Iteration 84750 Training loss 0.05129682272672653 Validation loss 0.05350809544324875 Accuracy 0.454345703125\n",
      "Iteration 84760 Training loss 0.0517110675573349 Validation loss 0.05332621932029724 Accuracy 0.45751953125\n",
      "Iteration 84770 Training loss 0.04797216132283211 Validation loss 0.052735574543476105 Accuracy 0.461181640625\n",
      "Iteration 84780 Training loss 0.04914284124970436 Validation loss 0.053090278059244156 Accuracy 0.460205078125\n",
      "Iteration 84790 Training loss 0.04868560656905174 Validation loss 0.05280197039246559 Accuracy 0.45947265625\n",
      "Iteration 84800 Training loss 0.04746268317103386 Validation loss 0.05286100506782532 Accuracy 0.46044921875\n",
      "Iteration 84810 Training loss 0.04963965713977814 Validation loss 0.05302171781659126 Accuracy 0.459716796875\n",
      "Iteration 84820 Training loss 0.04993879050016403 Validation loss 0.05353261157870293 Accuracy 0.45556640625\n",
      "Iteration 84830 Training loss 0.047617025673389435 Validation loss 0.053075917065143585 Accuracy 0.460205078125\n",
      "Iteration 84840 Training loss 0.050405461341142654 Validation loss 0.053020551800727844 Accuracy 0.460205078125\n",
      "Iteration 84850 Training loss 0.05191139504313469 Validation loss 0.0527564212679863 Accuracy 0.4609375\n",
      "Iteration 84860 Training loss 0.05270751565694809 Validation loss 0.053159210830926895 Accuracy 0.460693359375\n",
      "Iteration 84870 Training loss 0.046919528394937515 Validation loss 0.052830398082733154 Accuracy 0.460205078125\n",
      "Iteration 84880 Training loss 0.04858029633760452 Validation loss 0.05286845564842224 Accuracy 0.461181640625\n",
      "Iteration 84890 Training loss 0.04968320578336716 Validation loss 0.05299735069274902 Accuracy 0.4619140625\n",
      "Iteration 84900 Training loss 0.050583988428115845 Validation loss 0.053020380437374115 Accuracy 0.46044921875\n",
      "Iteration 84910 Training loss 0.048836108297109604 Validation loss 0.05276070162653923 Accuracy 0.461181640625\n",
      "Iteration 84920 Training loss 0.05302976444363594 Validation loss 0.052649252116680145 Accuracy 0.4619140625\n",
      "Iteration 84930 Training loss 0.05301910266280174 Validation loss 0.0532977320253849 Accuracy 0.459716796875\n",
      "Iteration 84940 Training loss 0.0513896681368351 Validation loss 0.053153954446315765 Accuracy 0.458984375\n",
      "Iteration 84950 Training loss 0.05137716233730316 Validation loss 0.05278104543685913 Accuracy 0.46240234375\n",
      "Iteration 84960 Training loss 0.051095712929964066 Validation loss 0.05267508700489998 Accuracy 0.461669921875\n",
      "Iteration 84970 Training loss 0.05284079536795616 Validation loss 0.05305418372154236 Accuracy 0.459228515625\n",
      "Iteration 84980 Training loss 0.05220073461532593 Validation loss 0.0530170314013958 Accuracy 0.459716796875\n",
      "Iteration 84990 Training loss 0.05258013680577278 Validation loss 0.05289657041430473 Accuracy 0.4609375\n",
      "Iteration 85000 Training loss 0.05088302493095398 Validation loss 0.0529940202832222 Accuracy 0.459228515625\n",
      "Iteration 85010 Training loss 0.04877893999218941 Validation loss 0.053436409682035446 Accuracy 0.4580078125\n",
      "Iteration 85020 Training loss 0.047279417514801025 Validation loss 0.052947305142879486 Accuracy 0.460693359375\n",
      "Iteration 85030 Training loss 0.051837258040905 Validation loss 0.05299718677997589 Accuracy 0.461181640625\n",
      "Iteration 85040 Training loss 0.04640434682369232 Validation loss 0.05311204865574837 Accuracy 0.458251953125\n",
      "Iteration 85050 Training loss 0.05262865498661995 Validation loss 0.05295950174331665 Accuracy 0.459716796875\n",
      "Iteration 85060 Training loss 0.051898419857025146 Validation loss 0.053071726113557816 Accuracy 0.458984375\n",
      "Iteration 85070 Training loss 0.0510227344930172 Validation loss 0.05362704023718834 Accuracy 0.454833984375\n",
      "Iteration 85080 Training loss 0.047707390040159225 Validation loss 0.05267304554581642 Accuracy 0.4609375\n",
      "Iteration 85090 Training loss 0.0528801865875721 Validation loss 0.05336738005280495 Accuracy 0.456787109375\n",
      "Iteration 85100 Training loss 0.05393939092755318 Validation loss 0.052684236317873 Accuracy 0.461181640625\n",
      "Iteration 85110 Training loss 0.0481136329472065 Validation loss 0.052795588970184326 Accuracy 0.461181640625\n",
      "Iteration 85120 Training loss 0.050207123160362244 Validation loss 0.05290451645851135 Accuracy 0.460205078125\n",
      "Iteration 85130 Training loss 0.05301550775766373 Validation loss 0.05259012430906296 Accuracy 0.4619140625\n",
      "Iteration 85140 Training loss 0.049047328531742096 Validation loss 0.05312156677246094 Accuracy 0.46044921875\n",
      "Iteration 85150 Training loss 0.04641043022274971 Validation loss 0.0527469664812088 Accuracy 0.462158203125\n",
      "Iteration 85160 Training loss 0.04955444112420082 Validation loss 0.052828554064035416 Accuracy 0.462158203125\n",
      "Iteration 85170 Training loss 0.05024071782827377 Validation loss 0.05301447957754135 Accuracy 0.46044921875\n",
      "Iteration 85180 Training loss 0.05228686332702637 Validation loss 0.053038544952869415 Accuracy 0.4609375\n",
      "Iteration 85190 Training loss 0.0513634979724884 Validation loss 0.0530463270843029 Accuracy 0.4599609375\n",
      "Iteration 85200 Training loss 0.05449577793478966 Validation loss 0.0529823824763298 Accuracy 0.46044921875\n",
      "Iteration 85210 Training loss 0.048773784190416336 Validation loss 0.05266336724162102 Accuracy 0.462158203125\n",
      "Iteration 85220 Training loss 0.051161427050828934 Validation loss 0.052709050476551056 Accuracy 0.46142578125\n",
      "Iteration 85230 Training loss 0.05244476720690727 Validation loss 0.053078681230545044 Accuracy 0.4599609375\n",
      "Iteration 85240 Training loss 0.053750209510326385 Validation loss 0.053517576307058334 Accuracy 0.45751953125\n",
      "Iteration 85250 Training loss 0.051107268780469894 Validation loss 0.05281369015574455 Accuracy 0.461181640625\n",
      "Iteration 85260 Training loss 0.04965662583708763 Validation loss 0.05291236191987991 Accuracy 0.460693359375\n",
      "Iteration 85270 Training loss 0.05221312493085861 Validation loss 0.052661068737506866 Accuracy 0.46142578125\n",
      "Iteration 85280 Training loss 0.05005751550197601 Validation loss 0.05274064466357231 Accuracy 0.46142578125\n",
      "Iteration 85290 Training loss 0.05228118970990181 Validation loss 0.052542392164468765 Accuracy 0.462890625\n",
      "Iteration 85300 Training loss 0.050674956291913986 Validation loss 0.05315899848937988 Accuracy 0.459228515625\n",
      "Iteration 85310 Training loss 0.050364766269922256 Validation loss 0.05278565362095833 Accuracy 0.4609375\n",
      "Iteration 85320 Training loss 0.04867757111787796 Validation loss 0.053248997777700424 Accuracy 0.45849609375\n",
      "Iteration 85330 Training loss 0.051900409162044525 Validation loss 0.05265307426452637 Accuracy 0.4619140625\n",
      "Iteration 85340 Training loss 0.05376782640814781 Validation loss 0.05259805917739868 Accuracy 0.4619140625\n",
      "Iteration 85350 Training loss 0.04687552899122238 Validation loss 0.05302468687295914 Accuracy 0.459228515625\n",
      "Iteration 85360 Training loss 0.050543904304504395 Validation loss 0.05256234109401703 Accuracy 0.462890625\n",
      "Iteration 85370 Training loss 0.05182144045829773 Validation loss 0.052800051867961884 Accuracy 0.462158203125\n",
      "Iteration 85380 Training loss 0.05208493024110794 Validation loss 0.05265023186802864 Accuracy 0.461669921875\n",
      "Iteration 85390 Training loss 0.05182792246341705 Validation loss 0.05288461595773697 Accuracy 0.46044921875\n",
      "Iteration 85400 Training loss 0.05145546793937683 Validation loss 0.05305788293480873 Accuracy 0.4599609375\n",
      "Iteration 85410 Training loss 0.05199741944670677 Validation loss 0.05323951691389084 Accuracy 0.460693359375\n",
      "Iteration 85420 Training loss 0.05345892161130905 Validation loss 0.053679656237363815 Accuracy 0.45556640625\n",
      "Iteration 85430 Training loss 0.05194900929927826 Validation loss 0.05308730900287628 Accuracy 0.4580078125\n",
      "Iteration 85440 Training loss 0.04987626522779465 Validation loss 0.052750490605831146 Accuracy 0.462158203125\n",
      "Iteration 85450 Training loss 0.051840320229530334 Validation loss 0.0527813546359539 Accuracy 0.46044921875\n",
      "Iteration 85460 Training loss 0.051118042320013046 Validation loss 0.05298015475273132 Accuracy 0.461181640625\n",
      "Iteration 85470 Training loss 0.05177973210811615 Validation loss 0.053006649017333984 Accuracy 0.462158203125\n",
      "Iteration 85480 Training loss 0.05239548534154892 Validation loss 0.05302165076136589 Accuracy 0.46142578125\n",
      "Iteration 85490 Training loss 0.04739910736680031 Validation loss 0.05280565470457077 Accuracy 0.461669921875\n",
      "Iteration 85500 Training loss 0.05184923857450485 Validation loss 0.053338102996349335 Accuracy 0.457275390625\n",
      "Iteration 85510 Training loss 0.04918357729911804 Validation loss 0.05326027050614357 Accuracy 0.45751953125\n",
      "Iteration 85520 Training loss 0.04963696748018265 Validation loss 0.0527641698718071 Accuracy 0.461181640625\n",
      "Iteration 85530 Training loss 0.04730788245797157 Validation loss 0.052723776549100876 Accuracy 0.4619140625\n",
      "Iteration 85540 Training loss 0.053377531468868256 Validation loss 0.05296718701720238 Accuracy 0.460205078125\n",
      "Iteration 85550 Training loss 0.05362417548894882 Validation loss 0.05280293896794319 Accuracy 0.461669921875\n",
      "Iteration 85560 Training loss 0.052613839507102966 Validation loss 0.052751459181308746 Accuracy 0.460693359375\n",
      "Iteration 85570 Training loss 0.051703501492738724 Validation loss 0.05330418795347214 Accuracy 0.4580078125\n",
      "Iteration 85580 Training loss 0.05218076705932617 Validation loss 0.05281706526875496 Accuracy 0.46142578125\n",
      "Iteration 85590 Training loss 0.049634210765361786 Validation loss 0.052512332797050476 Accuracy 0.462646484375\n",
      "Iteration 85600 Training loss 0.052181676030159 Validation loss 0.05261887237429619 Accuracy 0.462646484375\n",
      "Iteration 85610 Training loss 0.053242117166519165 Validation loss 0.05242845416069031 Accuracy 0.463623046875\n",
      "Iteration 85620 Training loss 0.052494924515485764 Validation loss 0.0527026392519474 Accuracy 0.46240234375\n",
      "Iteration 85630 Training loss 0.04928646981716156 Validation loss 0.05316714569926262 Accuracy 0.45751953125\n",
      "Iteration 85640 Training loss 0.053834278136491776 Validation loss 0.05283202975988388 Accuracy 0.461181640625\n",
      "Iteration 85650 Training loss 0.05178005248308182 Validation loss 0.052646733820438385 Accuracy 0.4619140625\n",
      "Iteration 85660 Training loss 0.0505952462553978 Validation loss 0.05331495776772499 Accuracy 0.455810546875\n",
      "Iteration 85670 Training loss 0.049641065299510956 Validation loss 0.05275878682732582 Accuracy 0.462158203125\n",
      "Iteration 85680 Training loss 0.05217009410262108 Validation loss 0.053130485117435455 Accuracy 0.460205078125\n",
      "Iteration 85690 Training loss 0.05109386518597603 Validation loss 0.053090739995241165 Accuracy 0.459228515625\n",
      "Iteration 85700 Training loss 0.05254453048110008 Validation loss 0.05286600440740585 Accuracy 0.461181640625\n",
      "Iteration 85710 Training loss 0.04966125264763832 Validation loss 0.05293838307261467 Accuracy 0.460693359375\n",
      "Iteration 85720 Training loss 0.05023690313100815 Validation loss 0.05270630866289139 Accuracy 0.4619140625\n",
      "Iteration 85730 Training loss 0.04827781021595001 Validation loss 0.05286701023578644 Accuracy 0.460693359375\n",
      "Iteration 85740 Training loss 0.04628727212548256 Validation loss 0.05270279198884964 Accuracy 0.461181640625\n",
      "Iteration 85750 Training loss 0.05424132198095322 Validation loss 0.05281606689095497 Accuracy 0.46142578125\n",
      "Iteration 85760 Training loss 0.053619738668203354 Validation loss 0.05293227359652519 Accuracy 0.45947265625\n",
      "Iteration 85770 Training loss 0.054226797074079514 Validation loss 0.052746113389730453 Accuracy 0.461181640625\n",
      "Iteration 85780 Training loss 0.051231201738119125 Validation loss 0.052896250039339066 Accuracy 0.461181640625\n",
      "Iteration 85790 Training loss 0.05096840485930443 Validation loss 0.05282965674996376 Accuracy 0.4619140625\n",
      "Iteration 85800 Training loss 0.05311759561300278 Validation loss 0.05298734828829765 Accuracy 0.4619140625\n",
      "Iteration 85810 Training loss 0.050377167761325836 Validation loss 0.0528569221496582 Accuracy 0.460205078125\n",
      "Iteration 85820 Training loss 0.05186331644654274 Validation loss 0.052807655185461044 Accuracy 0.462158203125\n",
      "Iteration 85830 Training loss 0.05111433193087578 Validation loss 0.05258556082844734 Accuracy 0.463134765625\n",
      "Iteration 85840 Training loss 0.050014082342386246 Validation loss 0.05312565714120865 Accuracy 0.4599609375\n",
      "Iteration 85850 Training loss 0.04618961364030838 Validation loss 0.05257672816514969 Accuracy 0.462158203125\n",
      "Iteration 85860 Training loss 0.04829690605401993 Validation loss 0.05264946073293686 Accuracy 0.46240234375\n",
      "Iteration 85870 Training loss 0.050807829946279526 Validation loss 0.05256359279155731 Accuracy 0.46240234375\n",
      "Iteration 85880 Training loss 0.05300086364150047 Validation loss 0.05408089607954025 Accuracy 0.450439453125\n",
      "Iteration 85890 Training loss 0.05163412541151047 Validation loss 0.05272132530808449 Accuracy 0.46142578125\n",
      "Iteration 85900 Training loss 0.05435957759618759 Validation loss 0.052553143352270126 Accuracy 0.4619140625\n",
      "Iteration 85910 Training loss 0.047963619232177734 Validation loss 0.05252986028790474 Accuracy 0.462158203125\n",
      "Iteration 85920 Training loss 0.04937656596302986 Validation loss 0.053195852786302567 Accuracy 0.46044921875\n",
      "Iteration 85930 Training loss 0.04958917200565338 Validation loss 0.0530424490571022 Accuracy 0.4599609375\n",
      "Iteration 85940 Training loss 0.05073150619864464 Validation loss 0.052775342017412186 Accuracy 0.46142578125\n",
      "Iteration 85950 Training loss 0.053135376423597336 Validation loss 0.05291740968823433 Accuracy 0.461181640625\n",
      "Iteration 85960 Training loss 0.050365835428237915 Validation loss 0.052716486155986786 Accuracy 0.461669921875\n",
      "Iteration 85970 Training loss 0.050157580524683 Validation loss 0.05333685874938965 Accuracy 0.4599609375\n",
      "Iteration 85980 Training loss 0.05087185278534889 Validation loss 0.05268026888370514 Accuracy 0.461181640625\n",
      "Iteration 85990 Training loss 0.04966602101922035 Validation loss 0.05262041836977005 Accuracy 0.462158203125\n",
      "Iteration 86000 Training loss 0.04971297085285187 Validation loss 0.052557963877916336 Accuracy 0.4619140625\n",
      "Iteration 86010 Training loss 0.04937422648072243 Validation loss 0.05267662927508354 Accuracy 0.4619140625\n",
      "Iteration 86020 Training loss 0.0540962778031826 Validation loss 0.05299947038292885 Accuracy 0.460693359375\n",
      "Iteration 86030 Training loss 0.04663269966840744 Validation loss 0.05369090661406517 Accuracy 0.452392578125\n",
      "Iteration 86040 Training loss 0.04935498535633087 Validation loss 0.052991174161434174 Accuracy 0.4619140625\n",
      "Iteration 86050 Training loss 0.05071946233510971 Validation loss 0.052808113396167755 Accuracy 0.461181640625\n",
      "Iteration 86060 Training loss 0.05212925001978874 Validation loss 0.05284445732831955 Accuracy 0.461669921875\n",
      "Iteration 86070 Training loss 0.05082520470023155 Validation loss 0.05290841683745384 Accuracy 0.460693359375\n",
      "Iteration 86080 Training loss 0.05288684368133545 Validation loss 0.05291919782757759 Accuracy 0.460693359375\n",
      "Iteration 86090 Training loss 0.0505983829498291 Validation loss 0.052678730338811874 Accuracy 0.4609375\n",
      "Iteration 86100 Training loss 0.05285542830824852 Validation loss 0.052741244435310364 Accuracy 0.4619140625\n",
      "Iteration 86110 Training loss 0.052109699696302414 Validation loss 0.05271688476204872 Accuracy 0.46142578125\n",
      "Iteration 86120 Training loss 0.05038369074463844 Validation loss 0.053000178188085556 Accuracy 0.459228515625\n",
      "Iteration 86130 Training loss 0.05251632630825043 Validation loss 0.053023211658000946 Accuracy 0.45947265625\n",
      "Iteration 86140 Training loss 0.05043720453977585 Validation loss 0.05328146368265152 Accuracy 0.4599609375\n",
      "Iteration 86150 Training loss 0.05358295515179634 Validation loss 0.05302809551358223 Accuracy 0.45849609375\n",
      "Iteration 86160 Training loss 0.05405179783701897 Validation loss 0.0527738593518734 Accuracy 0.46142578125\n",
      "Iteration 86170 Training loss 0.05279175192117691 Validation loss 0.05294320359826088 Accuracy 0.4619140625\n",
      "Iteration 86180 Training loss 0.050534967333078384 Validation loss 0.052882902324199677 Accuracy 0.4619140625\n",
      "Iteration 86190 Training loss 0.046314842998981476 Validation loss 0.052658211439847946 Accuracy 0.461181640625\n",
      "Iteration 86200 Training loss 0.0509168803691864 Validation loss 0.053028710186481476 Accuracy 0.460693359375\n",
      "Iteration 86210 Training loss 0.049831073731184006 Validation loss 0.05296093598008156 Accuracy 0.461181640625\n",
      "Iteration 86220 Training loss 0.047744013369083405 Validation loss 0.0528804175555706 Accuracy 0.460693359375\n",
      "Iteration 86230 Training loss 0.0513739250600338 Validation loss 0.052899476140737534 Accuracy 0.46240234375\n",
      "Iteration 86240 Training loss 0.0506662093102932 Validation loss 0.05292179435491562 Accuracy 0.46142578125\n",
      "Iteration 86250 Training loss 0.05012683942914009 Validation loss 0.05272342637181282 Accuracy 0.4609375\n",
      "Iteration 86260 Training loss 0.052553340792655945 Validation loss 0.0528394915163517 Accuracy 0.461181640625\n",
      "Iteration 86270 Training loss 0.048911213874816895 Validation loss 0.05285727232694626 Accuracy 0.4609375\n",
      "Iteration 86280 Training loss 0.05047804117202759 Validation loss 0.052737850695848465 Accuracy 0.461669921875\n",
      "Iteration 86290 Training loss 0.04624795541167259 Validation loss 0.052810341119766235 Accuracy 0.461181640625\n",
      "Iteration 86300 Training loss 0.05033838748931885 Validation loss 0.052990544587373734 Accuracy 0.458740234375\n",
      "Iteration 86310 Training loss 0.05259541794657707 Validation loss 0.05269644409418106 Accuracy 0.460693359375\n",
      "Iteration 86320 Training loss 0.052993111312389374 Validation loss 0.05303779989480972 Accuracy 0.4609375\n",
      "Iteration 86330 Training loss 0.04883427917957306 Validation loss 0.053378645330667496 Accuracy 0.457275390625\n",
      "Iteration 86340 Training loss 0.05047174543142319 Validation loss 0.052751198410987854 Accuracy 0.461181640625\n",
      "Iteration 86350 Training loss 0.053538475185632706 Validation loss 0.05322566255927086 Accuracy 0.458984375\n",
      "Iteration 86360 Training loss 0.04866715520620346 Validation loss 0.052591901272535324 Accuracy 0.462158203125\n",
      "Iteration 86370 Training loss 0.05038910731673241 Validation loss 0.05303547531366348 Accuracy 0.460205078125\n",
      "Iteration 86380 Training loss 0.04693887382745743 Validation loss 0.052856702357530594 Accuracy 0.460693359375\n",
      "Iteration 86390 Training loss 0.04979121312499046 Validation loss 0.05282023176550865 Accuracy 0.46044921875\n",
      "Iteration 86400 Training loss 0.05424882844090462 Validation loss 0.053086914122104645 Accuracy 0.4599609375\n",
      "Iteration 86410 Training loss 0.050106022506952286 Validation loss 0.05304062366485596 Accuracy 0.46044921875\n",
      "Iteration 86420 Training loss 0.055074941366910934 Validation loss 0.053105078637599945 Accuracy 0.45947265625\n",
      "Iteration 86430 Training loss 0.051822077482938766 Validation loss 0.05285234749317169 Accuracy 0.46142578125\n",
      "Iteration 86440 Training loss 0.0488845631480217 Validation loss 0.052783407270908356 Accuracy 0.4609375\n",
      "Iteration 86450 Training loss 0.05069981887936592 Validation loss 0.05275360122323036 Accuracy 0.461181640625\n",
      "Iteration 86460 Training loss 0.05492258816957474 Validation loss 0.052772216498851776 Accuracy 0.460205078125\n",
      "Iteration 86470 Training loss 0.04794089123606682 Validation loss 0.053742263466119766 Accuracy 0.455078125\n",
      "Iteration 86480 Training loss 0.052837397903203964 Validation loss 0.05288083478808403 Accuracy 0.460693359375\n",
      "Iteration 86490 Training loss 0.047158319503068924 Validation loss 0.052747029811143875 Accuracy 0.461181640625\n",
      "Iteration 86500 Training loss 0.0496523380279541 Validation loss 0.052863266319036484 Accuracy 0.461181640625\n",
      "Iteration 86510 Training loss 0.04918774217367172 Validation loss 0.05324338749051094 Accuracy 0.45849609375\n",
      "Iteration 86520 Training loss 0.048905741423368454 Validation loss 0.052852947264909744 Accuracy 0.4609375\n",
      "Iteration 86530 Training loss 0.048682138323783875 Validation loss 0.052647337317466736 Accuracy 0.461181640625\n",
      "Iteration 86540 Training loss 0.05362281575798988 Validation loss 0.05300525203347206 Accuracy 0.460693359375\n",
      "Iteration 86550 Training loss 0.05091109499335289 Validation loss 0.052954938262701035 Accuracy 0.4609375\n",
      "Iteration 86560 Training loss 0.048572469502687454 Validation loss 0.05277072265744209 Accuracy 0.461181640625\n",
      "Iteration 86570 Training loss 0.05182529613375664 Validation loss 0.05276418849825859 Accuracy 0.4609375\n",
      "Iteration 86580 Training loss 0.05035891756415367 Validation loss 0.05286838859319687 Accuracy 0.460205078125\n",
      "Iteration 86590 Training loss 0.052504636347293854 Validation loss 0.05308162048459053 Accuracy 0.4599609375\n",
      "Iteration 86600 Training loss 0.051383260637521744 Validation loss 0.05333677679300308 Accuracy 0.455810546875\n",
      "Iteration 86610 Training loss 0.051905784755945206 Validation loss 0.053356219083070755 Accuracy 0.457763671875\n",
      "Iteration 86620 Training loss 0.051230426877737045 Validation loss 0.052959565073251724 Accuracy 0.460205078125\n",
      "Iteration 86630 Training loss 0.04989834874868393 Validation loss 0.05289668217301369 Accuracy 0.460693359375\n",
      "Iteration 86640 Training loss 0.055109404027462006 Validation loss 0.053408198058605194 Accuracy 0.45654296875\n",
      "Iteration 86650 Training loss 0.0516706146299839 Validation loss 0.05274271219968796 Accuracy 0.4619140625\n",
      "Iteration 86660 Training loss 0.050309959799051285 Validation loss 0.05295119434595108 Accuracy 0.4609375\n",
      "Iteration 86670 Training loss 0.05273249372839928 Validation loss 0.052788954228162766 Accuracy 0.461181640625\n",
      "Iteration 86680 Training loss 0.051486507058143616 Validation loss 0.05303169786930084 Accuracy 0.46044921875\n",
      "Iteration 86690 Training loss 0.0509362630546093 Validation loss 0.05313751846551895 Accuracy 0.458984375\n",
      "Iteration 86700 Training loss 0.045931991189718246 Validation loss 0.05282832309603691 Accuracy 0.461181640625\n",
      "Iteration 86710 Training loss 0.05176950991153717 Validation loss 0.05306323245167732 Accuracy 0.458984375\n",
      "Iteration 86720 Training loss 0.05349152535200119 Validation loss 0.0532282292842865 Accuracy 0.456787109375\n",
      "Iteration 86730 Training loss 0.04992443695664406 Validation loss 0.05300718545913696 Accuracy 0.4599609375\n",
      "Iteration 86740 Training loss 0.051289353519678116 Validation loss 0.053240954875946045 Accuracy 0.455322265625\n",
      "Iteration 86750 Training loss 0.04715192690491676 Validation loss 0.05294575169682503 Accuracy 0.4599609375\n",
      "Iteration 86760 Training loss 0.054853204637765884 Validation loss 0.05356607213616371 Accuracy 0.454345703125\n",
      "Iteration 86770 Training loss 0.05270769074559212 Validation loss 0.052812669426202774 Accuracy 0.461669921875\n",
      "Iteration 86780 Training loss 0.050854939967393875 Validation loss 0.05334177240729332 Accuracy 0.460205078125\n",
      "Iteration 86790 Training loss 0.04999367147684097 Validation loss 0.05298331752419472 Accuracy 0.4599609375\n",
      "Iteration 86800 Training loss 0.050154250115156174 Validation loss 0.05273062363266945 Accuracy 0.461181640625\n",
      "Iteration 86810 Training loss 0.04918914660811424 Validation loss 0.05310439318418503 Accuracy 0.45947265625\n",
      "Iteration 86820 Training loss 0.04905078187584877 Validation loss 0.052599675953388214 Accuracy 0.4609375\n",
      "Iteration 86830 Training loss 0.05351673439145088 Validation loss 0.053856898099184036 Accuracy 0.4541015625\n",
      "Iteration 86840 Training loss 0.047953374683856964 Validation loss 0.05273689329624176 Accuracy 0.4599609375\n",
      "Iteration 86850 Training loss 0.05141489580273628 Validation loss 0.05287273973226547 Accuracy 0.4599609375\n",
      "Iteration 86860 Training loss 0.05163541063666344 Validation loss 0.053186655044555664 Accuracy 0.4580078125\n",
      "Iteration 86870 Training loss 0.051611337810754776 Validation loss 0.05316808074712753 Accuracy 0.460205078125\n",
      "Iteration 86880 Training loss 0.055455826222896576 Validation loss 0.05277547612786293 Accuracy 0.4599609375\n",
      "Iteration 86890 Training loss 0.04899825155735016 Validation loss 0.052783749997615814 Accuracy 0.461181640625\n",
      "Iteration 86900 Training loss 0.054746951907873154 Validation loss 0.053084325045347214 Accuracy 0.456787109375\n",
      "Iteration 86910 Training loss 0.0518905334174633 Validation loss 0.05308840796351433 Accuracy 0.4599609375\n",
      "Iteration 86920 Training loss 0.045256681740283966 Validation loss 0.05298697575926781 Accuracy 0.461181640625\n",
      "Iteration 86930 Training loss 0.053327951580286026 Validation loss 0.05300581827759743 Accuracy 0.45947265625\n",
      "Iteration 86940 Training loss 0.05173958092927933 Validation loss 0.052570462226867676 Accuracy 0.461181640625\n",
      "Iteration 86950 Training loss 0.05078347772359848 Validation loss 0.05374384671449661 Accuracy 0.452392578125\n",
      "Iteration 86960 Training loss 0.05331337824463844 Validation loss 0.05292241647839546 Accuracy 0.459228515625\n",
      "Iteration 86970 Training loss 0.04876689985394478 Validation loss 0.053015198558568954 Accuracy 0.4599609375\n",
      "Iteration 86980 Training loss 0.04917237162590027 Validation loss 0.052901726216077805 Accuracy 0.461181640625\n",
      "Iteration 86990 Training loss 0.05053264647722244 Validation loss 0.05278654024004936 Accuracy 0.461181640625\n",
      "Iteration 87000 Training loss 0.050231512635946274 Validation loss 0.05299343541264534 Accuracy 0.460693359375\n",
      "Iteration 87010 Training loss 0.050893064588308334 Validation loss 0.05287855491042137 Accuracy 0.46044921875\n",
      "Iteration 87020 Training loss 0.05137939751148224 Validation loss 0.052952587604522705 Accuracy 0.461669921875\n",
      "Iteration 87030 Training loss 0.051497332751750946 Validation loss 0.05313792824745178 Accuracy 0.458740234375\n",
      "Iteration 87040 Training loss 0.051911212503910065 Validation loss 0.053012147545814514 Accuracy 0.460693359375\n",
      "Iteration 87050 Training loss 0.050486501306295395 Validation loss 0.05335010215640068 Accuracy 0.458251953125\n",
      "Iteration 87060 Training loss 0.046625975519418716 Validation loss 0.05295861139893532 Accuracy 0.46044921875\n",
      "Iteration 87070 Training loss 0.04932354763150215 Validation loss 0.052627917379140854 Accuracy 0.461669921875\n",
      "Iteration 87080 Training loss 0.05324652045965195 Validation loss 0.05316194146871567 Accuracy 0.45849609375\n",
      "Iteration 87090 Training loss 0.051920246332883835 Validation loss 0.05275610834360123 Accuracy 0.460693359375\n",
      "Iteration 87100 Training loss 0.053590577095746994 Validation loss 0.05304504930973053 Accuracy 0.45947265625\n",
      "Iteration 87110 Training loss 0.052352163940668106 Validation loss 0.05269642546772957 Accuracy 0.4609375\n",
      "Iteration 87120 Training loss 0.04933532327413559 Validation loss 0.05345046892762184 Accuracy 0.4580078125\n",
      "Iteration 87130 Training loss 0.051250774413347244 Validation loss 0.0529085136950016 Accuracy 0.460205078125\n",
      "Iteration 87140 Training loss 0.05228319391608238 Validation loss 0.052918441593647 Accuracy 0.460693359375\n",
      "Iteration 87150 Training loss 0.05250084772706032 Validation loss 0.05281422287225723 Accuracy 0.459716796875\n",
      "Iteration 87160 Training loss 0.04980270192027092 Validation loss 0.05273564159870148 Accuracy 0.4609375\n",
      "Iteration 87170 Training loss 0.050629135221242905 Validation loss 0.05318208038806915 Accuracy 0.4609375\n",
      "Iteration 87180 Training loss 0.050260771065950394 Validation loss 0.052874065935611725 Accuracy 0.460205078125\n",
      "Iteration 87190 Training loss 0.050766777247190475 Validation loss 0.052809592336416245 Accuracy 0.4609375\n",
      "Iteration 87200 Training loss 0.054287269711494446 Validation loss 0.055014755576848984 Accuracy 0.44189453125\n",
      "Iteration 87210 Training loss 0.04967709258198738 Validation loss 0.05309607833623886 Accuracy 0.459228515625\n",
      "Iteration 87220 Training loss 0.05247435346245766 Validation loss 0.05303457751870155 Accuracy 0.45947265625\n",
      "Iteration 87230 Training loss 0.0503632128238678 Validation loss 0.05340290814638138 Accuracy 0.45703125\n",
      "Iteration 87240 Training loss 0.048796407878398895 Validation loss 0.05292852222919464 Accuracy 0.460693359375\n",
      "Iteration 87250 Training loss 0.0491950660943985 Validation loss 0.05291946232318878 Accuracy 0.45947265625\n",
      "Iteration 87260 Training loss 0.05572635307908058 Validation loss 0.05316591262817383 Accuracy 0.460205078125\n",
      "Iteration 87270 Training loss 0.05053386837244034 Validation loss 0.052781518548727036 Accuracy 0.460693359375\n",
      "Iteration 87280 Training loss 0.05012164264917374 Validation loss 0.05276396498084068 Accuracy 0.46240234375\n",
      "Iteration 87290 Training loss 0.05210211127996445 Validation loss 0.053251396864652634 Accuracy 0.460693359375\n",
      "Iteration 87300 Training loss 0.05343615636229515 Validation loss 0.052975643426179886 Accuracy 0.45751953125\n",
      "Iteration 87310 Training loss 0.052723757922649384 Validation loss 0.0537063367664814 Accuracy 0.453857421875\n",
      "Iteration 87320 Training loss 0.051023516803979874 Validation loss 0.05417013540863991 Accuracy 0.45068359375\n",
      "Iteration 87330 Training loss 0.047420863062143326 Validation loss 0.052722230553627014 Accuracy 0.462646484375\n",
      "Iteration 87340 Training loss 0.04815903678536415 Validation loss 0.05326183885335922 Accuracy 0.45849609375\n",
      "Iteration 87350 Training loss 0.05326797068119049 Validation loss 0.05269763618707657 Accuracy 0.462158203125\n",
      "Iteration 87360 Training loss 0.049571823328733444 Validation loss 0.05285593494772911 Accuracy 0.4609375\n",
      "Iteration 87370 Training loss 0.04965398460626602 Validation loss 0.05301855877041817 Accuracy 0.46240234375\n",
      "Iteration 87380 Training loss 0.05056251958012581 Validation loss 0.05262265354394913 Accuracy 0.46240234375\n",
      "Iteration 87390 Training loss 0.05200238153338432 Validation loss 0.0525631345808506 Accuracy 0.4619140625\n",
      "Iteration 87400 Training loss 0.05292029678821564 Validation loss 0.05276568606495857 Accuracy 0.461181640625\n",
      "Iteration 87410 Training loss 0.05077061802148819 Validation loss 0.05289221927523613 Accuracy 0.46044921875\n",
      "Iteration 87420 Training loss 0.05139613896608353 Validation loss 0.05319461598992348 Accuracy 0.458740234375\n",
      "Iteration 87430 Training loss 0.05261000618338585 Validation loss 0.0528743639588356 Accuracy 0.460693359375\n",
      "Iteration 87440 Training loss 0.04844345152378082 Validation loss 0.05255049839615822 Accuracy 0.462158203125\n",
      "Iteration 87450 Training loss 0.04865456745028496 Validation loss 0.05286814272403717 Accuracy 0.461181640625\n",
      "Iteration 87460 Training loss 0.04922011122107506 Validation loss 0.05285869911313057 Accuracy 0.460205078125\n",
      "Iteration 87470 Training loss 0.052217233926057816 Validation loss 0.05266079306602478 Accuracy 0.46142578125\n",
      "Iteration 87480 Training loss 0.04877575486898422 Validation loss 0.05341304466128349 Accuracy 0.457763671875\n",
      "Iteration 87490 Training loss 0.0486469566822052 Validation loss 0.052595898509025574 Accuracy 0.4619140625\n",
      "Iteration 87500 Training loss 0.05285573750734329 Validation loss 0.05304441601037979 Accuracy 0.4609375\n",
      "Iteration 87510 Training loss 0.04844287410378456 Validation loss 0.05291784927248955 Accuracy 0.462158203125\n",
      "Iteration 87520 Training loss 0.0519520565867424 Validation loss 0.05443772301077843 Accuracy 0.448486328125\n",
      "Iteration 87530 Training loss 0.05047192424535751 Validation loss 0.05291333794593811 Accuracy 0.460693359375\n",
      "Iteration 87540 Training loss 0.048756618052721024 Validation loss 0.05319689214229584 Accuracy 0.458251953125\n",
      "Iteration 87550 Training loss 0.05103352293372154 Validation loss 0.052858591079711914 Accuracy 0.46142578125\n",
      "Iteration 87560 Training loss 0.05281028151512146 Validation loss 0.05321907252073288 Accuracy 0.45849609375\n",
      "Iteration 87570 Training loss 0.048446785658597946 Validation loss 0.0529489740729332 Accuracy 0.461181640625\n",
      "Iteration 87580 Training loss 0.051742564886808395 Validation loss 0.05296596512198448 Accuracy 0.46044921875\n",
      "Iteration 87590 Training loss 0.04768609628081322 Validation loss 0.052915140986442566 Accuracy 0.4609375\n",
      "Iteration 87600 Training loss 0.052896030247211456 Validation loss 0.05282343924045563 Accuracy 0.461181640625\n",
      "Iteration 87610 Training loss 0.050814054906368256 Validation loss 0.053315456956624985 Accuracy 0.458984375\n",
      "Iteration 87620 Training loss 0.05276809632778168 Validation loss 0.053080637007951736 Accuracy 0.458984375\n",
      "Iteration 87630 Training loss 0.04877977445721626 Validation loss 0.05299423635005951 Accuracy 0.460693359375\n",
      "Iteration 87640 Training loss 0.050122182816267014 Validation loss 0.05280269309878349 Accuracy 0.46142578125\n",
      "Iteration 87650 Training loss 0.05091773346066475 Validation loss 0.05309323966503143 Accuracy 0.458984375\n",
      "Iteration 87660 Training loss 0.05289454013109207 Validation loss 0.05350445210933685 Accuracy 0.45703125\n",
      "Iteration 87670 Training loss 0.05057074874639511 Validation loss 0.05335264652967453 Accuracy 0.455810546875\n",
      "Iteration 87680 Training loss 0.05285825952887535 Validation loss 0.05278005823493004 Accuracy 0.460693359375\n",
      "Iteration 87690 Training loss 0.04959847405552864 Validation loss 0.05302514508366585 Accuracy 0.459228515625\n",
      "Iteration 87700 Training loss 0.05191805958747864 Validation loss 0.05293780937790871 Accuracy 0.461181640625\n",
      "Iteration 87710 Training loss 0.05044112727046013 Validation loss 0.053181491792201996 Accuracy 0.458740234375\n",
      "Iteration 87720 Training loss 0.049950338900089264 Validation loss 0.052960656583309174 Accuracy 0.4609375\n",
      "Iteration 87730 Training loss 0.049691129475831985 Validation loss 0.05291077122092247 Accuracy 0.46142578125\n",
      "Iteration 87740 Training loss 0.05074669420719147 Validation loss 0.05283083766698837 Accuracy 0.4619140625\n",
      "Iteration 87750 Training loss 0.05097821354866028 Validation loss 0.05295499041676521 Accuracy 0.4609375\n",
      "Iteration 87760 Training loss 0.049322474747896194 Validation loss 0.053031161427497864 Accuracy 0.460693359375\n",
      "Iteration 87770 Training loss 0.051803071051836014 Validation loss 0.052871014922857285 Accuracy 0.4619140625\n",
      "Iteration 87780 Training loss 0.05130961164832115 Validation loss 0.05300036817789078 Accuracy 0.460205078125\n",
      "Iteration 87790 Training loss 0.04858030006289482 Validation loss 0.05277630314230919 Accuracy 0.459228515625\n",
      "Iteration 87800 Training loss 0.050879668444395065 Validation loss 0.053068261593580246 Accuracy 0.46142578125\n",
      "Iteration 87810 Training loss 0.051405709236860275 Validation loss 0.05295785889029503 Accuracy 0.460693359375\n",
      "Iteration 87820 Training loss 0.04975561425089836 Validation loss 0.05268189683556557 Accuracy 0.462646484375\n",
      "Iteration 87830 Training loss 0.05053942650556564 Validation loss 0.052797332406044006 Accuracy 0.462646484375\n",
      "Iteration 87840 Training loss 0.04836690053343773 Validation loss 0.052462395280599594 Accuracy 0.462646484375\n",
      "Iteration 87850 Training loss 0.051921598613262177 Validation loss 0.05337075889110565 Accuracy 0.45703125\n",
      "Iteration 87860 Training loss 0.048503387719392776 Validation loss 0.052644647657871246 Accuracy 0.462890625\n",
      "Iteration 87870 Training loss 0.0544007271528244 Validation loss 0.053223803639411926 Accuracy 0.4580078125\n",
      "Iteration 87880 Training loss 0.05318416282534599 Validation loss 0.05265522748231888 Accuracy 0.461669921875\n",
      "Iteration 87890 Training loss 0.052780404686927795 Validation loss 0.052774421870708466 Accuracy 0.461181640625\n",
      "Iteration 87900 Training loss 0.045676618814468384 Validation loss 0.053396373987197876 Accuracy 0.457763671875\n",
      "Iteration 87910 Training loss 0.05675497278571129 Validation loss 0.05289386957883835 Accuracy 0.461669921875\n",
      "Iteration 87920 Training loss 0.05033006891608238 Validation loss 0.05259421095252037 Accuracy 0.462646484375\n",
      "Iteration 87930 Training loss 0.051712024956941605 Validation loss 0.053145330399274826 Accuracy 0.46044921875\n",
      "Iteration 87940 Training loss 0.05409691855311394 Validation loss 0.052562370896339417 Accuracy 0.46240234375\n",
      "Iteration 87950 Training loss 0.04779920354485512 Validation loss 0.05278106406331062 Accuracy 0.46142578125\n",
      "Iteration 87960 Training loss 0.05435745790600777 Validation loss 0.05267078056931496 Accuracy 0.462158203125\n",
      "Iteration 87970 Training loss 0.05178035423159599 Validation loss 0.05349918454885483 Accuracy 0.4580078125\n",
      "Iteration 87980 Training loss 0.05585246905684471 Validation loss 0.05267782136797905 Accuracy 0.46240234375\n",
      "Iteration 87990 Training loss 0.052626628428697586 Validation loss 0.05282605439424515 Accuracy 0.462646484375\n",
      "Iteration 88000 Training loss 0.05394524708390236 Validation loss 0.05295099318027496 Accuracy 0.4609375\n",
      "Iteration 88010 Training loss 0.046158935874700546 Validation loss 0.052675750106573105 Accuracy 0.46240234375\n",
      "Iteration 88020 Training loss 0.05140811949968338 Validation loss 0.053068626672029495 Accuracy 0.459716796875\n",
      "Iteration 88030 Training loss 0.04761124774813652 Validation loss 0.052995193749666214 Accuracy 0.458984375\n",
      "Iteration 88040 Training loss 0.051776859909296036 Validation loss 0.053112611174583435 Accuracy 0.458984375\n",
      "Iteration 88050 Training loss 0.0497235506772995 Validation loss 0.05279533192515373 Accuracy 0.46044921875\n",
      "Iteration 88060 Training loss 0.04946034774184227 Validation loss 0.05269147455692291 Accuracy 0.462646484375\n",
      "Iteration 88070 Training loss 0.05144419148564339 Validation loss 0.052846066653728485 Accuracy 0.46240234375\n",
      "Iteration 88080 Training loss 0.05187253654003143 Validation loss 0.05291426181793213 Accuracy 0.462158203125\n",
      "Iteration 88090 Training loss 0.049204178154468536 Validation loss 0.05280562490224838 Accuracy 0.461181640625\n",
      "Iteration 88100 Training loss 0.054344795644283295 Validation loss 0.05313803255558014 Accuracy 0.45751953125\n",
      "Iteration 88110 Training loss 0.05252501741051674 Validation loss 0.05322970449924469 Accuracy 0.45849609375\n",
      "Iteration 88120 Training loss 0.049340978264808655 Validation loss 0.05273544415831566 Accuracy 0.46240234375\n",
      "Iteration 88130 Training loss 0.04952089488506317 Validation loss 0.05295288935303688 Accuracy 0.461181640625\n",
      "Iteration 88140 Training loss 0.05142657458782196 Validation loss 0.052956078201532364 Accuracy 0.460693359375\n",
      "Iteration 88150 Training loss 0.04879224672913551 Validation loss 0.053124040365219116 Accuracy 0.45947265625\n",
      "Iteration 88160 Training loss 0.05212051421403885 Validation loss 0.0525776632130146 Accuracy 0.463134765625\n",
      "Iteration 88170 Training loss 0.053546417504549026 Validation loss 0.05300644785165787 Accuracy 0.4599609375\n",
      "Iteration 88180 Training loss 0.051417961716651917 Validation loss 0.05281795933842659 Accuracy 0.46142578125\n",
      "Iteration 88190 Training loss 0.04852679371833801 Validation loss 0.05267921835184097 Accuracy 0.462890625\n",
      "Iteration 88200 Training loss 0.05127313360571861 Validation loss 0.05273382365703583 Accuracy 0.4599609375\n",
      "Iteration 88210 Training loss 0.05147556588053703 Validation loss 0.05287603288888931 Accuracy 0.46142578125\n",
      "Iteration 88220 Training loss 0.05305441841483116 Validation loss 0.053099773824214935 Accuracy 0.459716796875\n",
      "Iteration 88230 Training loss 0.04715771600604057 Validation loss 0.05281725898385048 Accuracy 0.461181640625\n",
      "Iteration 88240 Training loss 0.05285978317260742 Validation loss 0.05257883667945862 Accuracy 0.4619140625\n",
      "Iteration 88250 Training loss 0.04874785989522934 Validation loss 0.05289866030216217 Accuracy 0.462158203125\n",
      "Iteration 88260 Training loss 0.05266324430704117 Validation loss 0.05291420593857765 Accuracy 0.46044921875\n",
      "Iteration 88270 Training loss 0.05447063595056534 Validation loss 0.05326419696211815 Accuracy 0.460205078125\n",
      "Iteration 88280 Training loss 0.04985670745372772 Validation loss 0.05256710946559906 Accuracy 0.463134765625\n",
      "Iteration 88290 Training loss 0.05292942747473717 Validation loss 0.05284745991230011 Accuracy 0.460205078125\n",
      "Iteration 88300 Training loss 0.050900232046842575 Validation loss 0.052713118493556976 Accuracy 0.462158203125\n",
      "Iteration 88310 Training loss 0.04910324141383171 Validation loss 0.053514983505010605 Accuracy 0.4580078125\n",
      "Iteration 88320 Training loss 0.05102216452360153 Validation loss 0.0527663454413414 Accuracy 0.4599609375\n",
      "Iteration 88330 Training loss 0.050470683723688126 Validation loss 0.05271751806139946 Accuracy 0.46240234375\n",
      "Iteration 88340 Training loss 0.04671790450811386 Validation loss 0.052543044090270996 Accuracy 0.46240234375\n",
      "Iteration 88350 Training loss 0.05342387780547142 Validation loss 0.05286697298288345 Accuracy 0.460693359375\n",
      "Iteration 88360 Training loss 0.0521974079310894 Validation loss 0.05273223668336868 Accuracy 0.4638671875\n",
      "Iteration 88370 Training loss 0.051124848425388336 Validation loss 0.05261233448982239 Accuracy 0.46240234375\n",
      "Iteration 88380 Training loss 0.04881351813673973 Validation loss 0.05245086923241615 Accuracy 0.462890625\n",
      "Iteration 88390 Training loss 0.04935088008642197 Validation loss 0.05249229818582535 Accuracy 0.463134765625\n",
      "Iteration 88400 Training loss 0.050804708153009415 Validation loss 0.053552333265542984 Accuracy 0.456298828125\n",
      "Iteration 88410 Training loss 0.05389450863003731 Validation loss 0.05290254205465317 Accuracy 0.459228515625\n",
      "Iteration 88420 Training loss 0.05151922628283501 Validation loss 0.052841562777757645 Accuracy 0.4619140625\n",
      "Iteration 88430 Training loss 0.05030432716012001 Validation loss 0.052917517721652985 Accuracy 0.46142578125\n",
      "Iteration 88440 Training loss 0.05027243494987488 Validation loss 0.053227465599775314 Accuracy 0.458251953125\n",
      "Iteration 88450 Training loss 0.04859371483325958 Validation loss 0.05273682624101639 Accuracy 0.461181640625\n",
      "Iteration 88460 Training loss 0.05338447168469429 Validation loss 0.05285077542066574 Accuracy 0.461181640625\n",
      "Iteration 88470 Training loss 0.049413569271564484 Validation loss 0.05289182439446449 Accuracy 0.460693359375\n",
      "Iteration 88480 Training loss 0.04816993325948715 Validation loss 0.05326190963387489 Accuracy 0.459716796875\n",
      "Iteration 88490 Training loss 0.051980189979076385 Validation loss 0.05278053507208824 Accuracy 0.4619140625\n",
      "Iteration 88500 Training loss 0.04971129447221756 Validation loss 0.052637919783592224 Accuracy 0.463134765625\n",
      "Iteration 88510 Training loss 0.0516381673514843 Validation loss 0.05294721573591232 Accuracy 0.461181640625\n",
      "Iteration 88520 Training loss 0.05278192460536957 Validation loss 0.05253903567790985 Accuracy 0.462646484375\n",
      "Iteration 88530 Training loss 0.052590787410736084 Validation loss 0.05276443064212799 Accuracy 0.462646484375\n",
      "Iteration 88540 Training loss 0.05205782130360603 Validation loss 0.05255048722028732 Accuracy 0.462890625\n",
      "Iteration 88550 Training loss 0.05133340507745743 Validation loss 0.052722468972206116 Accuracy 0.46142578125\n",
      "Iteration 88560 Training loss 0.0506722554564476 Validation loss 0.05274263396859169 Accuracy 0.461669921875\n",
      "Iteration 88570 Training loss 0.05426975339651108 Validation loss 0.053400568664073944 Accuracy 0.4560546875\n",
      "Iteration 88580 Training loss 0.05266251415014267 Validation loss 0.05281692370772362 Accuracy 0.4609375\n",
      "Iteration 88590 Training loss 0.04804210364818573 Validation loss 0.05273589864373207 Accuracy 0.462890625\n",
      "Iteration 88600 Training loss 0.053123462945222855 Validation loss 0.05266899615526199 Accuracy 0.4619140625\n",
      "Iteration 88610 Training loss 0.05050456523895264 Validation loss 0.05324291065335274 Accuracy 0.458740234375\n",
      "Iteration 88620 Training loss 0.055526889860630035 Validation loss 0.05279157683253288 Accuracy 0.461669921875\n",
      "Iteration 88630 Training loss 0.05080036446452141 Validation loss 0.052468232810497284 Accuracy 0.462646484375\n",
      "Iteration 88640 Training loss 0.05498160794377327 Validation loss 0.05284874513745308 Accuracy 0.462158203125\n",
      "Iteration 88650 Training loss 0.049674443900585175 Validation loss 0.05266610160470009 Accuracy 0.461669921875\n",
      "Iteration 88660 Training loss 0.0566154383122921 Validation loss 0.0533549040555954 Accuracy 0.457275390625\n",
      "Iteration 88670 Training loss 0.05177104100584984 Validation loss 0.052692048251628876 Accuracy 0.462158203125\n",
      "Iteration 88680 Training loss 0.05224187299609184 Validation loss 0.053477879613637924 Accuracy 0.4541015625\n",
      "Iteration 88690 Training loss 0.051150597631931305 Validation loss 0.05318183824419975 Accuracy 0.4599609375\n",
      "Iteration 88700 Training loss 0.04827636107802391 Validation loss 0.052961308509111404 Accuracy 0.461181640625\n",
      "Iteration 88710 Training loss 0.05076248571276665 Validation loss 0.053231608122587204 Accuracy 0.45751953125\n",
      "Iteration 88720 Training loss 0.04945068433880806 Validation loss 0.05271117016673088 Accuracy 0.461669921875\n",
      "Iteration 88730 Training loss 0.05133657157421112 Validation loss 0.05259813740849495 Accuracy 0.46240234375\n",
      "Iteration 88740 Training loss 0.04978920519351959 Validation loss 0.05267863720655441 Accuracy 0.4609375\n",
      "Iteration 88750 Training loss 0.050873707979917526 Validation loss 0.05308988317847252 Accuracy 0.459716796875\n",
      "Iteration 88760 Training loss 0.046679746359586716 Validation loss 0.0526595301926136 Accuracy 0.46044921875\n",
      "Iteration 88770 Training loss 0.051924578845500946 Validation loss 0.05312975123524666 Accuracy 0.46044921875\n",
      "Iteration 88780 Training loss 0.05199988931417465 Validation loss 0.05271872878074646 Accuracy 0.4619140625\n",
      "Iteration 88790 Training loss 0.04896284267306328 Validation loss 0.05263257771730423 Accuracy 0.462158203125\n",
      "Iteration 88800 Training loss 0.05172285437583923 Validation loss 0.05270131677389145 Accuracy 0.46240234375\n",
      "Iteration 88810 Training loss 0.04780443757772446 Validation loss 0.05269274115562439 Accuracy 0.4619140625\n",
      "Iteration 88820 Training loss 0.049598753452301025 Validation loss 0.05314165726304054 Accuracy 0.4609375\n",
      "Iteration 88830 Training loss 0.04690242558717728 Validation loss 0.052866511046886444 Accuracy 0.461181640625\n",
      "Iteration 88840 Training loss 0.052950162440538406 Validation loss 0.05263702943921089 Accuracy 0.46240234375\n",
      "Iteration 88850 Training loss 0.048019908368587494 Validation loss 0.05290980264544487 Accuracy 0.461181640625\n",
      "Iteration 88860 Training loss 0.048331376165151596 Validation loss 0.05268050730228424 Accuracy 0.46142578125\n",
      "Iteration 88870 Training loss 0.04893602430820465 Validation loss 0.05249860882759094 Accuracy 0.462646484375\n",
      "Iteration 88880 Training loss 0.05149773135781288 Validation loss 0.05296031013131142 Accuracy 0.459716796875\n",
      "Iteration 88890 Training loss 0.05112685263156891 Validation loss 0.052687011659145355 Accuracy 0.462646484375\n",
      "Iteration 88900 Training loss 0.05131664127111435 Validation loss 0.05394342541694641 Accuracy 0.4541015625\n",
      "Iteration 88910 Training loss 0.05276698246598244 Validation loss 0.05298157408833504 Accuracy 0.460205078125\n",
      "Iteration 88920 Training loss 0.053456034511327744 Validation loss 0.05371217802166939 Accuracy 0.45556640625\n",
      "Iteration 88930 Training loss 0.04696958512067795 Validation loss 0.05265409126877785 Accuracy 0.463134765625\n",
      "Iteration 88940 Training loss 0.05082908645272255 Validation loss 0.0527791865170002 Accuracy 0.462890625\n",
      "Iteration 88950 Training loss 0.04861783981323242 Validation loss 0.05289916694164276 Accuracy 0.460693359375\n",
      "Iteration 88960 Training loss 0.054042134433984756 Validation loss 0.05284622311592102 Accuracy 0.461181640625\n",
      "Iteration 88970 Training loss 0.052287694066762924 Validation loss 0.052659936249256134 Accuracy 0.462890625\n",
      "Iteration 88980 Training loss 0.05079994723200798 Validation loss 0.05267566442489624 Accuracy 0.4619140625\n",
      "Iteration 88990 Training loss 0.05244533345103264 Validation loss 0.05264059081673622 Accuracy 0.4619140625\n",
      "Iteration 89000 Training loss 0.05083869770169258 Validation loss 0.05281345546245575 Accuracy 0.4609375\n",
      "Iteration 89010 Training loss 0.04862654581665993 Validation loss 0.0528368204832077 Accuracy 0.460205078125\n",
      "Iteration 89020 Training loss 0.055230285972356796 Validation loss 0.05322638526558876 Accuracy 0.457763671875\n",
      "Iteration 89030 Training loss 0.05286518111824989 Validation loss 0.05316517502069473 Accuracy 0.4599609375\n",
      "Iteration 89040 Training loss 0.05130288749933243 Validation loss 0.05252860113978386 Accuracy 0.462158203125\n",
      "Iteration 89050 Training loss 0.05024111270904541 Validation loss 0.05337273329496384 Accuracy 0.45849609375\n",
      "Iteration 89060 Training loss 0.04985310882329941 Validation loss 0.05257456749677658 Accuracy 0.462890625\n",
      "Iteration 89070 Training loss 0.04891565814614296 Validation loss 0.05283001810312271 Accuracy 0.461181640625\n",
      "Iteration 89080 Training loss 0.04975811764597893 Validation loss 0.052974265068769455 Accuracy 0.4599609375\n",
      "Iteration 89090 Training loss 0.052135638892650604 Validation loss 0.05296586453914642 Accuracy 0.45751953125\n",
      "Iteration 89100 Training loss 0.05158429965376854 Validation loss 0.05276494100689888 Accuracy 0.461181640625\n",
      "Iteration 89110 Training loss 0.050812218338251114 Validation loss 0.05258595198392868 Accuracy 0.46240234375\n",
      "Iteration 89120 Training loss 0.04998956248164177 Validation loss 0.05301380902528763 Accuracy 0.4599609375\n",
      "Iteration 89130 Training loss 0.052763890475034714 Validation loss 0.05300721898674965 Accuracy 0.46044921875\n",
      "Iteration 89140 Training loss 0.04969152808189392 Validation loss 0.05275441333651543 Accuracy 0.462158203125\n",
      "Iteration 89150 Training loss 0.04941293224692345 Validation loss 0.05255585163831711 Accuracy 0.462158203125\n",
      "Iteration 89160 Training loss 0.05053962022066116 Validation loss 0.05250062420964241 Accuracy 0.462158203125\n",
      "Iteration 89170 Training loss 0.04993714392185211 Validation loss 0.053102392703294754 Accuracy 0.4599609375\n",
      "Iteration 89180 Training loss 0.049790479242801666 Validation loss 0.053100910037755966 Accuracy 0.459716796875\n",
      "Iteration 89190 Training loss 0.05281374603509903 Validation loss 0.05276557058095932 Accuracy 0.46240234375\n",
      "Iteration 89200 Training loss 0.05224863812327385 Validation loss 0.05290818214416504 Accuracy 0.46044921875\n",
      "Iteration 89210 Training loss 0.05427652597427368 Validation loss 0.052769459784030914 Accuracy 0.462646484375\n",
      "Iteration 89220 Training loss 0.051624927669763565 Validation loss 0.05322851985692978 Accuracy 0.45751953125\n",
      "Iteration 89230 Training loss 0.0501972921192646 Validation loss 0.052651189267635345 Accuracy 0.46240234375\n",
      "Iteration 89240 Training loss 0.04996447637677193 Validation loss 0.0528993234038353 Accuracy 0.46044921875\n",
      "Iteration 89250 Training loss 0.05074255168437958 Validation loss 0.0527140311896801 Accuracy 0.460205078125\n",
      "Iteration 89260 Training loss 0.05144301801919937 Validation loss 0.05342487990856171 Accuracy 0.45458984375\n",
      "Iteration 89270 Training loss 0.049874693155288696 Validation loss 0.05305228382349014 Accuracy 0.4619140625\n",
      "Iteration 89280 Training loss 0.04742259532213211 Validation loss 0.053071171045303345 Accuracy 0.460693359375\n",
      "Iteration 89290 Training loss 0.05046188458800316 Validation loss 0.05297913774847984 Accuracy 0.4580078125\n",
      "Iteration 89300 Training loss 0.05141393095254898 Validation loss 0.053535204380750656 Accuracy 0.45703125\n",
      "Iteration 89310 Training loss 0.05262712389230728 Validation loss 0.052731484174728394 Accuracy 0.461181640625\n",
      "Iteration 89320 Training loss 0.05361456423997879 Validation loss 0.05325935408473015 Accuracy 0.45947265625\n",
      "Iteration 89330 Training loss 0.05178330838680267 Validation loss 0.05263668671250343 Accuracy 0.462890625\n",
      "Iteration 89340 Training loss 0.05009068176150322 Validation loss 0.05329187214374542 Accuracy 0.454345703125\n",
      "Iteration 89350 Training loss 0.047621678560972214 Validation loss 0.05255943164229393 Accuracy 0.461181640625\n",
      "Iteration 89360 Training loss 0.05033153295516968 Validation loss 0.05316821113228798 Accuracy 0.459228515625\n",
      "Iteration 89370 Training loss 0.051289696246385574 Validation loss 0.05332200229167938 Accuracy 0.458740234375\n",
      "Iteration 89380 Training loss 0.04926585406064987 Validation loss 0.05241754651069641 Accuracy 0.462646484375\n",
      "Iteration 89390 Training loss 0.05075853317975998 Validation loss 0.052591823041439056 Accuracy 0.462158203125\n",
      "Iteration 89400 Training loss 0.049963582307100296 Validation loss 0.05304192379117012 Accuracy 0.45947265625\n",
      "Iteration 89410 Training loss 0.052471473813056946 Validation loss 0.05285381153225899 Accuracy 0.4609375\n",
      "Iteration 89420 Training loss 0.05065058916807175 Validation loss 0.05262727290391922 Accuracy 0.461669921875\n",
      "Iteration 89430 Training loss 0.05113878473639488 Validation loss 0.05254954844713211 Accuracy 0.462646484375\n",
      "Iteration 89440 Training loss 0.04935634881258011 Validation loss 0.05283553898334503 Accuracy 0.461181640625\n",
      "Iteration 89450 Training loss 0.05135999619960785 Validation loss 0.053314775228500366 Accuracy 0.45703125\n",
      "Iteration 89460 Training loss 0.050471413880586624 Validation loss 0.05269056558609009 Accuracy 0.4609375\n",
      "Iteration 89470 Training loss 0.04561338573694229 Validation loss 0.05292271822690964 Accuracy 0.46044921875\n",
      "Iteration 89480 Training loss 0.04618563875555992 Validation loss 0.0525171123445034 Accuracy 0.462646484375\n",
      "Iteration 89490 Training loss 0.04978156462311745 Validation loss 0.052616287022829056 Accuracy 0.462158203125\n",
      "Iteration 89500 Training loss 0.0514676608145237 Validation loss 0.0528007373213768 Accuracy 0.461669921875\n",
      "Iteration 89510 Training loss 0.04860010743141174 Validation loss 0.052991874516010284 Accuracy 0.459716796875\n",
      "Iteration 89520 Training loss 0.04996486380696297 Validation loss 0.05309046432375908 Accuracy 0.45751953125\n",
      "Iteration 89530 Training loss 0.04967579245567322 Validation loss 0.05307061970233917 Accuracy 0.4599609375\n",
      "Iteration 89540 Training loss 0.05312971770763397 Validation loss 0.052651483565568924 Accuracy 0.4619140625\n",
      "Iteration 89550 Training loss 0.04746599122881889 Validation loss 0.05257211625576019 Accuracy 0.46142578125\n",
      "Iteration 89560 Training loss 0.04869737848639488 Validation loss 0.05265270918607712 Accuracy 0.462158203125\n",
      "Iteration 89570 Training loss 0.04901973903179169 Validation loss 0.0528697706758976 Accuracy 0.4619140625\n",
      "Iteration 89580 Training loss 0.049968622624874115 Validation loss 0.05343415215611458 Accuracy 0.45849609375\n",
      "Iteration 89590 Training loss 0.049200743436813354 Validation loss 0.05317973345518112 Accuracy 0.45849609375\n",
      "Iteration 89600 Training loss 0.05231669917702675 Validation loss 0.05286382511258125 Accuracy 0.4619140625\n",
      "Iteration 89610 Training loss 0.05421602725982666 Validation loss 0.05260560289025307 Accuracy 0.460693359375\n",
      "Iteration 89620 Training loss 0.04935750737786293 Validation loss 0.05294133722782135 Accuracy 0.46142578125\n",
      "Iteration 89630 Training loss 0.047679007053375244 Validation loss 0.052770379930734634 Accuracy 0.462158203125\n",
      "Iteration 89640 Training loss 0.05043831467628479 Validation loss 0.052632130682468414 Accuracy 0.4619140625\n",
      "Iteration 89650 Training loss 0.049226611852645874 Validation loss 0.05273997038602829 Accuracy 0.4619140625\n",
      "Iteration 89660 Training loss 0.05177457630634308 Validation loss 0.053466130048036575 Accuracy 0.45751953125\n",
      "Iteration 89670 Training loss 0.053306568413972855 Validation loss 0.053246092051267624 Accuracy 0.4560546875\n",
      "Iteration 89680 Training loss 0.05391289293766022 Validation loss 0.05317787826061249 Accuracy 0.458984375\n",
      "Iteration 89690 Training loss 0.05481983348727226 Validation loss 0.05322372913360596 Accuracy 0.458984375\n",
      "Iteration 89700 Training loss 0.04621529206633568 Validation loss 0.05267447978258133 Accuracy 0.462158203125\n",
      "Iteration 89710 Training loss 0.05183783546090126 Validation loss 0.05282324180006981 Accuracy 0.46240234375\n",
      "Iteration 89720 Training loss 0.04974566772580147 Validation loss 0.052747692912817 Accuracy 0.462158203125\n",
      "Iteration 89730 Training loss 0.0480525903403759 Validation loss 0.05302295461297035 Accuracy 0.460205078125\n",
      "Iteration 89740 Training loss 0.04932026192545891 Validation loss 0.05266134813427925 Accuracy 0.4619140625\n",
      "Iteration 89750 Training loss 0.04681152477860451 Validation loss 0.05324932560324669 Accuracy 0.46044921875\n",
      "Iteration 89760 Training loss 0.05127597600221634 Validation loss 0.052755750715732574 Accuracy 0.4619140625\n",
      "Iteration 89770 Training loss 0.05194356292486191 Validation loss 0.052883464843034744 Accuracy 0.460693359375\n",
      "Iteration 89780 Training loss 0.05344677343964577 Validation loss 0.05315452441573143 Accuracy 0.458984375\n",
      "Iteration 89790 Training loss 0.052603963762521744 Validation loss 0.05320444330573082 Accuracy 0.459228515625\n",
      "Iteration 89800 Training loss 0.05434471741318703 Validation loss 0.052710842341184616 Accuracy 0.460205078125\n",
      "Iteration 89810 Training loss 0.05055484548211098 Validation loss 0.05267362296581268 Accuracy 0.461181640625\n",
      "Iteration 89820 Training loss 0.052096180617809296 Validation loss 0.05263770744204521 Accuracy 0.462158203125\n",
      "Iteration 89830 Training loss 0.04702148213982582 Validation loss 0.053430862724781036 Accuracy 0.457275390625\n",
      "Iteration 89840 Training loss 0.05040007084608078 Validation loss 0.05306140333414078 Accuracy 0.459716796875\n",
      "Iteration 89850 Training loss 0.04963885620236397 Validation loss 0.053430214524269104 Accuracy 0.4580078125\n",
      "Iteration 89860 Training loss 0.05300142243504524 Validation loss 0.05471395328640938 Accuracy 0.4443359375\n",
      "Iteration 89870 Training loss 0.050726134330034256 Validation loss 0.052835240960121155 Accuracy 0.459228515625\n",
      "Iteration 89880 Training loss 0.051796458661556244 Validation loss 0.052744340151548386 Accuracy 0.460205078125\n",
      "Iteration 89890 Training loss 0.051490575075149536 Validation loss 0.05268849432468414 Accuracy 0.46240234375\n",
      "Iteration 89900 Training loss 0.05217357352375984 Validation loss 0.05261322110891342 Accuracy 0.462158203125\n",
      "Iteration 89910 Training loss 0.052446238696575165 Validation loss 0.052992429584264755 Accuracy 0.4609375\n",
      "Iteration 89920 Training loss 0.050766173750162125 Validation loss 0.05345631390810013 Accuracy 0.456298828125\n",
      "Iteration 89930 Training loss 0.05039208009839058 Validation loss 0.05269060283899307 Accuracy 0.4619140625\n",
      "Iteration 89940 Training loss 0.04970259964466095 Validation loss 0.05253488942980766 Accuracy 0.461669921875\n",
      "Iteration 89950 Training loss 0.04888254404067993 Validation loss 0.05265592783689499 Accuracy 0.462646484375\n",
      "Iteration 89960 Training loss 0.051692672073841095 Validation loss 0.05427328124642372 Accuracy 0.451416015625\n",
      "Iteration 89970 Training loss 0.05136238411068916 Validation loss 0.05280318111181259 Accuracy 0.4609375\n",
      "Iteration 89980 Training loss 0.05338612198829651 Validation loss 0.053158603608608246 Accuracy 0.459228515625\n",
      "Iteration 89990 Training loss 0.050804950296878815 Validation loss 0.052676066756248474 Accuracy 0.46240234375\n",
      "Iteration 90000 Training loss 0.04866725578904152 Validation loss 0.052996307611465454 Accuracy 0.458740234375\n",
      "Iteration 90010 Training loss 0.049084704369306564 Validation loss 0.052455779165029526 Accuracy 0.46240234375\n",
      "Iteration 90020 Training loss 0.04746311530470848 Validation loss 0.05286908149719238 Accuracy 0.459228515625\n",
      "Iteration 90030 Training loss 0.04925006628036499 Validation loss 0.052797723561525345 Accuracy 0.460693359375\n",
      "Iteration 90040 Training loss 0.053430814296007156 Validation loss 0.05253680422902107 Accuracy 0.46240234375\n",
      "Iteration 90050 Training loss 0.05280476436018944 Validation loss 0.05294250324368477 Accuracy 0.459716796875\n",
      "Iteration 90060 Training loss 0.05144631862640381 Validation loss 0.052578166127204895 Accuracy 0.460693359375\n",
      "Iteration 90070 Training loss 0.051139261573553085 Validation loss 0.05308032035827637 Accuracy 0.4599609375\n",
      "Iteration 90080 Training loss 0.04779236763715744 Validation loss 0.0526827909052372 Accuracy 0.46240234375\n",
      "Iteration 90090 Training loss 0.0481451191008091 Validation loss 0.05255180597305298 Accuracy 0.46142578125\n",
      "Iteration 90100 Training loss 0.052960071712732315 Validation loss 0.05316857248544693 Accuracy 0.45947265625\n",
      "Iteration 90110 Training loss 0.04827166721224785 Validation loss 0.05306971073150635 Accuracy 0.460693359375\n",
      "Iteration 90120 Training loss 0.04677114635705948 Validation loss 0.05285101756453514 Accuracy 0.46142578125\n",
      "Iteration 90130 Training loss 0.051392048597335815 Validation loss 0.0531112402677536 Accuracy 0.458740234375\n",
      "Iteration 90140 Training loss 0.052505165338516235 Validation loss 0.05294083431363106 Accuracy 0.4609375\n",
      "Iteration 90150 Training loss 0.051672305911779404 Validation loss 0.05299941450357437 Accuracy 0.461181640625\n",
      "Iteration 90160 Training loss 0.04614097997546196 Validation loss 0.0527980662882328 Accuracy 0.462158203125\n",
      "Iteration 90170 Training loss 0.05328057333827019 Validation loss 0.05300973355770111 Accuracy 0.460205078125\n",
      "Iteration 90180 Training loss 0.04968741908669472 Validation loss 0.05309305712580681 Accuracy 0.45947265625\n",
      "Iteration 90190 Training loss 0.049733348190784454 Validation loss 0.05284849926829338 Accuracy 0.461669921875\n",
      "Iteration 90200 Training loss 0.05049634724855423 Validation loss 0.052502986043691635 Accuracy 0.461669921875\n",
      "Iteration 90210 Training loss 0.04960580915212631 Validation loss 0.052616823464632034 Accuracy 0.46142578125\n",
      "Iteration 90220 Training loss 0.05194777250289917 Validation loss 0.05273313447833061 Accuracy 0.4619140625\n",
      "Iteration 90230 Training loss 0.05345812067389488 Validation loss 0.05268678441643715 Accuracy 0.4609375\n",
      "Iteration 90240 Training loss 0.052734363824129105 Validation loss 0.052666522562503815 Accuracy 0.461669921875\n",
      "Iteration 90250 Training loss 0.04735510051250458 Validation loss 0.052976712584495544 Accuracy 0.461181640625\n",
      "Iteration 90260 Training loss 0.05238218232989311 Validation loss 0.05263407900929451 Accuracy 0.4609375\n",
      "Iteration 90270 Training loss 0.04907861724495888 Validation loss 0.05279816314578056 Accuracy 0.462158203125\n",
      "Iteration 90280 Training loss 0.04880031198263168 Validation loss 0.053190432488918304 Accuracy 0.4599609375\n",
      "Iteration 90290 Training loss 0.052563928067684174 Validation loss 0.05299082770943642 Accuracy 0.46044921875\n",
      "Iteration 90300 Training loss 0.05300583690404892 Validation loss 0.05316993594169617 Accuracy 0.4580078125\n",
      "Iteration 90310 Training loss 0.048695191740989685 Validation loss 0.05303309112787247 Accuracy 0.461181640625\n",
      "Iteration 90320 Training loss 0.053325630724430084 Validation loss 0.05299418792128563 Accuracy 0.4599609375\n",
      "Iteration 90330 Training loss 0.051457133144140244 Validation loss 0.05281689763069153 Accuracy 0.460205078125\n",
      "Iteration 90340 Training loss 0.04729875177145004 Validation loss 0.052872560918331146 Accuracy 0.461181640625\n",
      "Iteration 90350 Training loss 0.04929954558610916 Validation loss 0.0528569333255291 Accuracy 0.4599609375\n",
      "Iteration 90360 Training loss 0.052206914871931076 Validation loss 0.053706858307123184 Accuracy 0.456787109375\n",
      "Iteration 90370 Training loss 0.04875810816884041 Validation loss 0.052793122828006744 Accuracy 0.4619140625\n",
      "Iteration 90380 Training loss 0.04836833477020264 Validation loss 0.05303069204092026 Accuracy 0.45849609375\n",
      "Iteration 90390 Training loss 0.05002328380942345 Validation loss 0.052975185215473175 Accuracy 0.4609375\n",
      "Iteration 90400 Training loss 0.04970080405473709 Validation loss 0.05271933972835541 Accuracy 0.4619140625\n",
      "Iteration 90410 Training loss 0.052497655153274536 Validation loss 0.053329482674598694 Accuracy 0.455810546875\n",
      "Iteration 90420 Training loss 0.05257527530193329 Validation loss 0.053218092769384384 Accuracy 0.458740234375\n",
      "Iteration 90430 Training loss 0.048487018793821335 Validation loss 0.05288871005177498 Accuracy 0.459716796875\n",
      "Iteration 90440 Training loss 0.0475502535700798 Validation loss 0.05281985178589821 Accuracy 0.460205078125\n",
      "Iteration 90450 Training loss 0.052205801010131836 Validation loss 0.05275319144129753 Accuracy 0.46044921875\n",
      "Iteration 90460 Training loss 0.05244877561926842 Validation loss 0.052758876234292984 Accuracy 0.461181640625\n",
      "Iteration 90470 Training loss 0.05602690950036049 Validation loss 0.052868831902742386 Accuracy 0.460693359375\n",
      "Iteration 90480 Training loss 0.05039095878601074 Validation loss 0.052755504846572876 Accuracy 0.460693359375\n",
      "Iteration 90490 Training loss 0.04908541962504387 Validation loss 0.05324897915124893 Accuracy 0.45849609375\n",
      "Iteration 90500 Training loss 0.05360880494117737 Validation loss 0.05270933359861374 Accuracy 0.46240234375\n",
      "Iteration 90510 Training loss 0.05003120005130768 Validation loss 0.052548035979270935 Accuracy 0.46240234375\n",
      "Iteration 90520 Training loss 0.04820838198065758 Validation loss 0.05260481685400009 Accuracy 0.4619140625\n",
      "Iteration 90530 Training loss 0.05105825886130333 Validation loss 0.05274822935461998 Accuracy 0.459716796875\n",
      "Iteration 90540 Training loss 0.04991946369409561 Validation loss 0.05285599082708359 Accuracy 0.460205078125\n",
      "Iteration 90550 Training loss 0.05202457681298256 Validation loss 0.05328143388032913 Accuracy 0.460205078125\n",
      "Iteration 90560 Training loss 0.04801343381404877 Validation loss 0.053090449422597885 Accuracy 0.457275390625\n",
      "Iteration 90570 Training loss 0.04953653737902641 Validation loss 0.052678242325782776 Accuracy 0.4609375\n",
      "Iteration 90580 Training loss 0.051902394741773605 Validation loss 0.052989035844802856 Accuracy 0.459716796875\n",
      "Iteration 90590 Training loss 0.05205642804503441 Validation loss 0.05316498503088951 Accuracy 0.4599609375\n",
      "Iteration 90600 Training loss 0.04983816295862198 Validation loss 0.05271054059267044 Accuracy 0.460693359375\n",
      "Iteration 90610 Training loss 0.052782490849494934 Validation loss 0.05300712212920189 Accuracy 0.4609375\n",
      "Iteration 90620 Training loss 0.050966765731573105 Validation loss 0.05246439203619957 Accuracy 0.462890625\n",
      "Iteration 90630 Training loss 0.05505501106381416 Validation loss 0.0529242679476738 Accuracy 0.4609375\n",
      "Iteration 90640 Training loss 0.04989562928676605 Validation loss 0.052469950169324875 Accuracy 0.463134765625\n",
      "Iteration 90650 Training loss 0.05215561017394066 Validation loss 0.05345866084098816 Accuracy 0.455810546875\n",
      "Iteration 90660 Training loss 0.052119120955467224 Validation loss 0.052712332457304 Accuracy 0.4619140625\n",
      "Iteration 90670 Training loss 0.048401378095149994 Validation loss 0.05263733118772507 Accuracy 0.4619140625\n",
      "Iteration 90680 Training loss 0.0533556304872036 Validation loss 0.05271114036440849 Accuracy 0.461181640625\n",
      "Iteration 90690 Training loss 0.054084666073322296 Validation loss 0.05292424559593201 Accuracy 0.4619140625\n",
      "Iteration 90700 Training loss 0.050872109830379486 Validation loss 0.05303537845611572 Accuracy 0.45947265625\n",
      "Iteration 90710 Training loss 0.052971065044403076 Validation loss 0.0528155080974102 Accuracy 0.462158203125\n",
      "Iteration 90720 Training loss 0.052285972982645035 Validation loss 0.05262047052383423 Accuracy 0.46142578125\n",
      "Iteration 90730 Training loss 0.05151045694947243 Validation loss 0.0528898723423481 Accuracy 0.461181640625\n",
      "Iteration 90740 Training loss 0.05151107907295227 Validation loss 0.05321142449975014 Accuracy 0.460205078125\n",
      "Iteration 90750 Training loss 0.04959862679243088 Validation loss 0.05280901864171028 Accuracy 0.461181640625\n",
      "Iteration 90760 Training loss 0.047302234917879105 Validation loss 0.05336820334196091 Accuracy 0.455810546875\n",
      "Iteration 90770 Training loss 0.05425434932112694 Validation loss 0.053041744977235794 Accuracy 0.456298828125\n",
      "Iteration 90780 Training loss 0.04767266660928726 Validation loss 0.052973270416259766 Accuracy 0.4609375\n",
      "Iteration 90790 Training loss 0.04830477014183998 Validation loss 0.05279270559549332 Accuracy 0.4609375\n",
      "Iteration 90800 Training loss 0.04890798032283783 Validation loss 0.05316973850131035 Accuracy 0.45751953125\n",
      "Iteration 90810 Training loss 0.0504615418612957 Validation loss 0.05314136669039726 Accuracy 0.454345703125\n",
      "Iteration 90820 Training loss 0.05027392506599426 Validation loss 0.05271383747458458 Accuracy 0.4609375\n",
      "Iteration 90830 Training loss 0.05144818499684334 Validation loss 0.05316927656531334 Accuracy 0.45849609375\n",
      "Iteration 90840 Training loss 0.049108292907476425 Validation loss 0.05268703028559685 Accuracy 0.462646484375\n",
      "Iteration 90850 Training loss 0.049049682915210724 Validation loss 0.05283164978027344 Accuracy 0.46044921875\n",
      "Iteration 90860 Training loss 0.046989232301712036 Validation loss 0.053229622542858124 Accuracy 0.45947265625\n",
      "Iteration 90870 Training loss 0.052123576402664185 Validation loss 0.052609216421842575 Accuracy 0.461669921875\n",
      "Iteration 90880 Training loss 0.05035987123847008 Validation loss 0.05283987522125244 Accuracy 0.460693359375\n",
      "Iteration 90890 Training loss 0.04936602711677551 Validation loss 0.053207240998744965 Accuracy 0.460693359375\n",
      "Iteration 90900 Training loss 0.048575159162282944 Validation loss 0.05274232104420662 Accuracy 0.460693359375\n",
      "Iteration 90910 Training loss 0.05238696560263634 Validation loss 0.0528794601559639 Accuracy 0.46044921875\n",
      "Iteration 90920 Training loss 0.05017785727977753 Validation loss 0.053207073360681534 Accuracy 0.457763671875\n",
      "Iteration 90930 Training loss 0.05365888401865959 Validation loss 0.05266433581709862 Accuracy 0.462158203125\n",
      "Iteration 90940 Training loss 0.05028365179896355 Validation loss 0.05378657579421997 Accuracy 0.452880859375\n",
      "Iteration 90950 Training loss 0.04987090826034546 Validation loss 0.05332880839705467 Accuracy 0.4580078125\n",
      "Iteration 90960 Training loss 0.04977419972419739 Validation loss 0.052694790065288544 Accuracy 0.461669921875\n",
      "Iteration 90970 Training loss 0.05019261687994003 Validation loss 0.0527677908539772 Accuracy 0.4609375\n",
      "Iteration 90980 Training loss 0.05150486156344414 Validation loss 0.0529249906539917 Accuracy 0.459716796875\n",
      "Iteration 90990 Training loss 0.05105973407626152 Validation loss 0.05264356732368469 Accuracy 0.46044921875\n",
      "Iteration 91000 Training loss 0.04947587102651596 Validation loss 0.05265311524271965 Accuracy 0.46044921875\n",
      "Iteration 91010 Training loss 0.05263102054595947 Validation loss 0.05307836830615997 Accuracy 0.458984375\n",
      "Iteration 91020 Training loss 0.04979320615530014 Validation loss 0.0531185045838356 Accuracy 0.4599609375\n",
      "Iteration 91030 Training loss 0.05025795102119446 Validation loss 0.05298314988613129 Accuracy 0.459716796875\n",
      "Iteration 91040 Training loss 0.053114499896764755 Validation loss 0.05351390317082405 Accuracy 0.45166015625\n",
      "Iteration 91050 Training loss 0.05271613597869873 Validation loss 0.05307343602180481 Accuracy 0.459228515625\n",
      "Iteration 91060 Training loss 0.051773712038993835 Validation loss 0.05279427766799927 Accuracy 0.460693359375\n",
      "Iteration 91070 Training loss 0.05216887220740318 Validation loss 0.052670370787382126 Accuracy 0.462158203125\n",
      "Iteration 91080 Training loss 0.0472719632089138 Validation loss 0.05294782668352127 Accuracy 0.460205078125\n",
      "Iteration 91090 Training loss 0.05438509210944176 Validation loss 0.05289377272129059 Accuracy 0.46044921875\n",
      "Iteration 91100 Training loss 0.05039377138018608 Validation loss 0.052816033363342285 Accuracy 0.4619140625\n",
      "Iteration 91110 Training loss 0.04918881133198738 Validation loss 0.05362089350819588 Accuracy 0.45458984375\n",
      "Iteration 91120 Training loss 0.04998846352100372 Validation loss 0.05284915864467621 Accuracy 0.46044921875\n",
      "Iteration 91130 Training loss 0.04738103225827217 Validation loss 0.05301249027252197 Accuracy 0.4599609375\n",
      "Iteration 91140 Training loss 0.04493875429034233 Validation loss 0.05267573148012161 Accuracy 0.461181640625\n",
      "Iteration 91150 Training loss 0.05279698595404625 Validation loss 0.053131986409425735 Accuracy 0.459716796875\n",
      "Iteration 91160 Training loss 0.05031326413154602 Validation loss 0.05292892828583717 Accuracy 0.460693359375\n",
      "Iteration 91170 Training loss 0.050088174641132355 Validation loss 0.052722904831171036 Accuracy 0.461181640625\n",
      "Iteration 91180 Training loss 0.05057262256741524 Validation loss 0.052507080137729645 Accuracy 0.461669921875\n",
      "Iteration 91190 Training loss 0.05038423463702202 Validation loss 0.05259324982762337 Accuracy 0.462646484375\n",
      "Iteration 91200 Training loss 0.05260404571890831 Validation loss 0.052980199456214905 Accuracy 0.461181640625\n",
      "Iteration 91210 Training loss 0.0545709952712059 Validation loss 0.052914056926965714 Accuracy 0.459228515625\n",
      "Iteration 91220 Training loss 0.05240554362535477 Validation loss 0.05273081734776497 Accuracy 0.462646484375\n",
      "Iteration 91230 Training loss 0.04943719878792763 Validation loss 0.05322995036840439 Accuracy 0.457763671875\n",
      "Iteration 91240 Training loss 0.05280192941427231 Validation loss 0.05271291360259056 Accuracy 0.460693359375\n",
      "Iteration 91250 Training loss 0.04881085827946663 Validation loss 0.052728455513715744 Accuracy 0.461669921875\n",
      "Iteration 91260 Training loss 0.052424248307943344 Validation loss 0.05295516550540924 Accuracy 0.460693359375\n",
      "Iteration 91270 Training loss 0.049914006143808365 Validation loss 0.0527910515666008 Accuracy 0.46142578125\n",
      "Iteration 91280 Training loss 0.04976431280374527 Validation loss 0.05290951579809189 Accuracy 0.460693359375\n",
      "Iteration 91290 Training loss 0.05522279813885689 Validation loss 0.05287418141961098 Accuracy 0.4599609375\n",
      "Iteration 91300 Training loss 0.05446798726916313 Validation loss 0.05308559536933899 Accuracy 0.458984375\n",
      "Iteration 91310 Training loss 0.050612956285476685 Validation loss 0.05262494087219238 Accuracy 0.461181640625\n",
      "Iteration 91320 Training loss 0.05266745388507843 Validation loss 0.05271660163998604 Accuracy 0.4609375\n",
      "Iteration 91330 Training loss 0.05228612571954727 Validation loss 0.05286835879087448 Accuracy 0.460205078125\n",
      "Iteration 91340 Training loss 0.0503227524459362 Validation loss 0.05325492471456528 Accuracy 0.456298828125\n",
      "Iteration 91350 Training loss 0.053690582513809204 Validation loss 0.05289136618375778 Accuracy 0.460693359375\n",
      "Iteration 91360 Training loss 0.05041450262069702 Validation loss 0.05319613590836525 Accuracy 0.458984375\n",
      "Iteration 91370 Training loss 0.051659032702445984 Validation loss 0.05308349430561066 Accuracy 0.459716796875\n",
      "Iteration 91380 Training loss 0.05004702880978584 Validation loss 0.05291850492358208 Accuracy 0.460693359375\n",
      "Iteration 91390 Training loss 0.04866492748260498 Validation loss 0.052840713411569595 Accuracy 0.461181640625\n",
      "Iteration 91400 Training loss 0.0458240807056427 Validation loss 0.05274580046534538 Accuracy 0.462158203125\n",
      "Iteration 91410 Training loss 0.04861122742295265 Validation loss 0.05284668877720833 Accuracy 0.461181640625\n",
      "Iteration 91420 Training loss 0.05309233069419861 Validation loss 0.05295614153146744 Accuracy 0.4609375\n",
      "Iteration 91430 Training loss 0.04996516928076744 Validation loss 0.052760038524866104 Accuracy 0.461181640625\n",
      "Iteration 91440 Training loss 0.05179845169186592 Validation loss 0.05293014645576477 Accuracy 0.461181640625\n",
      "Iteration 91450 Training loss 0.046346861869096756 Validation loss 0.05284593999385834 Accuracy 0.461181640625\n",
      "Iteration 91460 Training loss 0.04882420226931572 Validation loss 0.052718259394168854 Accuracy 0.460205078125\n",
      "Iteration 91470 Training loss 0.05407964438199997 Validation loss 0.05270491912961006 Accuracy 0.4619140625\n",
      "Iteration 91480 Training loss 0.05373833701014519 Validation loss 0.053483132272958755 Accuracy 0.456787109375\n",
      "Iteration 91490 Training loss 0.05014101043343544 Validation loss 0.053162604570388794 Accuracy 0.459228515625\n",
      "Iteration 91500 Training loss 0.05139080807566643 Validation loss 0.05328888073563576 Accuracy 0.4580078125\n",
      "Iteration 91510 Training loss 0.05196103826165199 Validation loss 0.05279207229614258 Accuracy 0.460693359375\n",
      "Iteration 91520 Training loss 0.05148087441921234 Validation loss 0.052665047347545624 Accuracy 0.46142578125\n",
      "Iteration 91530 Training loss 0.05012394115328789 Validation loss 0.0527411624789238 Accuracy 0.460693359375\n",
      "Iteration 91540 Training loss 0.0491735003888607 Validation loss 0.052651502192020416 Accuracy 0.461181640625\n",
      "Iteration 91550 Training loss 0.051068518310785294 Validation loss 0.05284040793776512 Accuracy 0.46142578125\n",
      "Iteration 91560 Training loss 0.05564724653959274 Validation loss 0.05280667170882225 Accuracy 0.45947265625\n",
      "Iteration 91570 Training loss 0.056102629750967026 Validation loss 0.05295810475945473 Accuracy 0.459228515625\n",
      "Iteration 91580 Training loss 0.050686802715063095 Validation loss 0.05298852548003197 Accuracy 0.460693359375\n",
      "Iteration 91590 Training loss 0.04941728711128235 Validation loss 0.053071532398462296 Accuracy 0.458251953125\n",
      "Iteration 91600 Training loss 0.05078570172190666 Validation loss 0.05306010693311691 Accuracy 0.45947265625\n",
      "Iteration 91610 Training loss 0.051022302359342575 Validation loss 0.0529915988445282 Accuracy 0.458984375\n",
      "Iteration 91620 Training loss 0.05227229371666908 Validation loss 0.05345308408141136 Accuracy 0.458251953125\n",
      "Iteration 91630 Training loss 0.05127891153097153 Validation loss 0.052935898303985596 Accuracy 0.459228515625\n",
      "Iteration 91640 Training loss 0.04830121248960495 Validation loss 0.05281702056527138 Accuracy 0.460205078125\n",
      "Iteration 91650 Training loss 0.05508027970790863 Validation loss 0.05309559404850006 Accuracy 0.460205078125\n",
      "Iteration 91660 Training loss 0.050947751849889755 Validation loss 0.05273808538913727 Accuracy 0.461181640625\n",
      "Iteration 91670 Training loss 0.04862775653600693 Validation loss 0.05299396440386772 Accuracy 0.460205078125\n",
      "Iteration 91680 Training loss 0.05042296275496483 Validation loss 0.052713148295879364 Accuracy 0.460693359375\n",
      "Iteration 91690 Training loss 0.049294549971818924 Validation loss 0.05267779156565666 Accuracy 0.461181640625\n",
      "Iteration 91700 Training loss 0.05360238254070282 Validation loss 0.0529506579041481 Accuracy 0.4599609375\n",
      "Iteration 91710 Training loss 0.04952837899327278 Validation loss 0.05315246433019638 Accuracy 0.457763671875\n",
      "Iteration 91720 Training loss 0.05056033656001091 Validation loss 0.053277693688869476 Accuracy 0.45654296875\n",
      "Iteration 91730 Training loss 0.052145831286907196 Validation loss 0.05270977318286896 Accuracy 0.46142578125\n",
      "Iteration 91740 Training loss 0.05147729441523552 Validation loss 0.05303044617176056 Accuracy 0.458740234375\n",
      "Iteration 91750 Training loss 0.0499391071498394 Validation loss 0.05303673818707466 Accuracy 0.460205078125\n",
      "Iteration 91760 Training loss 0.049854859709739685 Validation loss 0.05279068276286125 Accuracy 0.460205078125\n",
      "Iteration 91770 Training loss 0.0486740805208683 Validation loss 0.05288344621658325 Accuracy 0.460693359375\n",
      "Iteration 91780 Training loss 0.052962202578783035 Validation loss 0.052701208740472794 Accuracy 0.461181640625\n",
      "Iteration 91790 Training loss 0.05419398844242096 Validation loss 0.05272381752729416 Accuracy 0.46142578125\n",
      "Iteration 91800 Training loss 0.05026085302233696 Validation loss 0.052865710109472275 Accuracy 0.461669921875\n",
      "Iteration 91810 Training loss 0.05119500309228897 Validation loss 0.053050555288791656 Accuracy 0.460205078125\n",
      "Iteration 91820 Training loss 0.0475909523665905 Validation loss 0.05287499725818634 Accuracy 0.461181640625\n",
      "Iteration 91830 Training loss 0.05467858910560608 Validation loss 0.053600013256073 Accuracy 0.45458984375\n",
      "Iteration 91840 Training loss 0.04883144050836563 Validation loss 0.05292711406946182 Accuracy 0.46044921875\n",
      "Iteration 91850 Training loss 0.05159620940685272 Validation loss 0.05318044126033783 Accuracy 0.458740234375\n",
      "Iteration 91860 Training loss 0.04736026003956795 Validation loss 0.052515558898448944 Accuracy 0.4619140625\n",
      "Iteration 91870 Training loss 0.05023913457989693 Validation loss 0.05310183763504028 Accuracy 0.458740234375\n",
      "Iteration 91880 Training loss 0.054410021752119064 Validation loss 0.053553055971860886 Accuracy 0.45654296875\n",
      "Iteration 91890 Training loss 0.051042236387729645 Validation loss 0.053054943680763245 Accuracy 0.46142578125\n",
      "Iteration 91900 Training loss 0.05022217705845833 Validation loss 0.053199607878923416 Accuracy 0.458251953125\n",
      "Iteration 91910 Training loss 0.05159478634595871 Validation loss 0.052589863538742065 Accuracy 0.462158203125\n",
      "Iteration 91920 Training loss 0.04851739481091499 Validation loss 0.05257607251405716 Accuracy 0.46240234375\n",
      "Iteration 91930 Training loss 0.05015363544225693 Validation loss 0.05269838124513626 Accuracy 0.4609375\n",
      "Iteration 91940 Training loss 0.05239184573292732 Validation loss 0.052578218281269073 Accuracy 0.461181640625\n",
      "Iteration 91950 Training loss 0.04797838255763054 Validation loss 0.05340605229139328 Accuracy 0.4541015625\n",
      "Iteration 91960 Training loss 0.05095841363072395 Validation loss 0.05267542973160744 Accuracy 0.4609375\n",
      "Iteration 91970 Training loss 0.05139359459280968 Validation loss 0.05293967202305794 Accuracy 0.4609375\n",
      "Iteration 91980 Training loss 0.05162797123193741 Validation loss 0.053444381803274155 Accuracy 0.458740234375\n",
      "Iteration 91990 Training loss 0.04963458329439163 Validation loss 0.053130410611629486 Accuracy 0.458740234375\n",
      "Iteration 92000 Training loss 0.052686724811792374 Validation loss 0.05307826027274132 Accuracy 0.4599609375\n",
      "Iteration 92010 Training loss 0.04992467164993286 Validation loss 0.05329694226384163 Accuracy 0.4580078125\n",
      "Iteration 92020 Training loss 0.04964517056941986 Validation loss 0.052819352596998215 Accuracy 0.4599609375\n",
      "Iteration 92030 Training loss 0.051250338554382324 Validation loss 0.05302370339632034 Accuracy 0.458251953125\n",
      "Iteration 92040 Training loss 0.054529666900634766 Validation loss 0.0529833547770977 Accuracy 0.459716796875\n",
      "Iteration 92050 Training loss 0.051807280629873276 Validation loss 0.053044453263282776 Accuracy 0.459716796875\n",
      "Iteration 92060 Training loss 0.05006243288516998 Validation loss 0.05343226343393326 Accuracy 0.457275390625\n",
      "Iteration 92070 Training loss 0.049624837934970856 Validation loss 0.05325082689523697 Accuracy 0.4580078125\n",
      "Iteration 92080 Training loss 0.051942043006420135 Validation loss 0.053072307258844376 Accuracy 0.4609375\n",
      "Iteration 92090 Training loss 0.04929209500551224 Validation loss 0.05266723409295082 Accuracy 0.46142578125\n",
      "Iteration 92100 Training loss 0.050974078476428986 Validation loss 0.05259593948721886 Accuracy 0.461181640625\n",
      "Iteration 92110 Training loss 0.04929399490356445 Validation loss 0.052846748381853104 Accuracy 0.4619140625\n",
      "Iteration 92120 Training loss 0.05151082202792168 Validation loss 0.05298301577568054 Accuracy 0.45849609375\n",
      "Iteration 92130 Training loss 0.04925651475787163 Validation loss 0.053057000041007996 Accuracy 0.46044921875\n",
      "Iteration 92140 Training loss 0.04776504635810852 Validation loss 0.052814461290836334 Accuracy 0.461181640625\n",
      "Iteration 92150 Training loss 0.048852019011974335 Validation loss 0.05249033868312836 Accuracy 0.462158203125\n",
      "Iteration 92160 Training loss 0.04935222491621971 Validation loss 0.053112175315618515 Accuracy 0.45947265625\n",
      "Iteration 92170 Training loss 0.05420119687914848 Validation loss 0.05254899710416794 Accuracy 0.4619140625\n",
      "Iteration 92180 Training loss 0.05024941638112068 Validation loss 0.05319942161440849 Accuracy 0.458984375\n",
      "Iteration 92190 Training loss 0.048384908586740494 Validation loss 0.05266782268881798 Accuracy 0.462890625\n",
      "Iteration 92200 Training loss 0.048644427210092545 Validation loss 0.05273319035768509 Accuracy 0.462646484375\n",
      "Iteration 92210 Training loss 0.05326215550303459 Validation loss 0.05268707871437073 Accuracy 0.46240234375\n",
      "Iteration 92220 Training loss 0.05283696576952934 Validation loss 0.05277212709188461 Accuracy 0.460693359375\n",
      "Iteration 92230 Training loss 0.04937880486249924 Validation loss 0.052853669971227646 Accuracy 0.461669921875\n",
      "Iteration 92240 Training loss 0.04899561405181885 Validation loss 0.05265463888645172 Accuracy 0.462646484375\n",
      "Iteration 92250 Training loss 0.051462799310684204 Validation loss 0.0527663417160511 Accuracy 0.462890625\n",
      "Iteration 92260 Training loss 0.053404856473207474 Validation loss 0.05298599600791931 Accuracy 0.462158203125\n",
      "Iteration 92270 Training loss 0.052210088819265366 Validation loss 0.0532328300178051 Accuracy 0.4560546875\n",
      "Iteration 92280 Training loss 0.05262065678834915 Validation loss 0.05249658599495888 Accuracy 0.462890625\n",
      "Iteration 92290 Training loss 0.05165614187717438 Validation loss 0.052758295089006424 Accuracy 0.461669921875\n",
      "Iteration 92300 Training loss 0.05293215811252594 Validation loss 0.05399136617779732 Accuracy 0.452392578125\n",
      "Iteration 92310 Training loss 0.05273519828915596 Validation loss 0.053278952836990356 Accuracy 0.459228515625\n",
      "Iteration 92320 Training loss 0.053631823509931564 Validation loss 0.05279044806957245 Accuracy 0.46240234375\n",
      "Iteration 92330 Training loss 0.052504658699035645 Validation loss 0.05275978520512581 Accuracy 0.461669921875\n",
      "Iteration 92340 Training loss 0.05233323946595192 Validation loss 0.0530414842069149 Accuracy 0.4580078125\n",
      "Iteration 92350 Training loss 0.05344456806778908 Validation loss 0.0534684844315052 Accuracy 0.455078125\n",
      "Iteration 92360 Training loss 0.0489252507686615 Validation loss 0.052639592438936234 Accuracy 0.46240234375\n",
      "Iteration 92370 Training loss 0.049522776156663895 Validation loss 0.052636414766311646 Accuracy 0.4619140625\n",
      "Iteration 92380 Training loss 0.04948574677109718 Validation loss 0.05289367958903313 Accuracy 0.4619140625\n",
      "Iteration 92390 Training loss 0.046710241585969925 Validation loss 0.0527682788670063 Accuracy 0.461181640625\n",
      "Iteration 92400 Training loss 0.05265945941209793 Validation loss 0.05291144549846649 Accuracy 0.45947265625\n",
      "Iteration 92410 Training loss 0.05117814242839813 Validation loss 0.05279948189854622 Accuracy 0.46142578125\n",
      "Iteration 92420 Training loss 0.04811634123325348 Validation loss 0.05340656265616417 Accuracy 0.45654296875\n",
      "Iteration 92430 Training loss 0.052315015345811844 Validation loss 0.0537303127348423 Accuracy 0.455078125\n",
      "Iteration 92440 Training loss 0.05077613145112991 Validation loss 0.052819959819316864 Accuracy 0.461181640625\n",
      "Iteration 92450 Training loss 0.051127947866916656 Validation loss 0.0527719222009182 Accuracy 0.462158203125\n",
      "Iteration 92460 Training loss 0.046359892934560776 Validation loss 0.05284486338496208 Accuracy 0.460205078125\n",
      "Iteration 92470 Training loss 0.04751290753483772 Validation loss 0.052641455084085464 Accuracy 0.461181640625\n",
      "Iteration 92480 Training loss 0.05035722628235817 Validation loss 0.05272869020700455 Accuracy 0.46044921875\n",
      "Iteration 92490 Training loss 0.04971125349402428 Validation loss 0.05279351770877838 Accuracy 0.4619140625\n",
      "Iteration 92500 Training loss 0.05378780886530876 Validation loss 0.052983034402132034 Accuracy 0.4599609375\n",
      "Iteration 92510 Training loss 0.050227414816617966 Validation loss 0.05289696156978607 Accuracy 0.4609375\n",
      "Iteration 92520 Training loss 0.04954329878091812 Validation loss 0.05266191437840462 Accuracy 0.461181640625\n",
      "Iteration 92530 Training loss 0.049800608307123184 Validation loss 0.052670981734991074 Accuracy 0.4599609375\n",
      "Iteration 92540 Training loss 0.05112055316567421 Validation loss 0.05246522277593613 Accuracy 0.461181640625\n",
      "Iteration 92550 Training loss 0.052399180829524994 Validation loss 0.05286356434226036 Accuracy 0.4619140625\n",
      "Iteration 92560 Training loss 0.048540517687797546 Validation loss 0.05268387496471405 Accuracy 0.461669921875\n",
      "Iteration 92570 Training loss 0.05123653635382652 Validation loss 0.05295977741479874 Accuracy 0.460205078125\n",
      "Iteration 92580 Training loss 0.049321431666612625 Validation loss 0.05273864045739174 Accuracy 0.461181640625\n",
      "Iteration 92590 Training loss 0.04980109632015228 Validation loss 0.05328771844506264 Accuracy 0.457763671875\n",
      "Iteration 92600 Training loss 0.046828195452690125 Validation loss 0.052942026406526566 Accuracy 0.4619140625\n",
      "Iteration 92610 Training loss 0.05097838118672371 Validation loss 0.05321653187274933 Accuracy 0.45849609375\n",
      "Iteration 92620 Training loss 0.04840277135372162 Validation loss 0.05256516486406326 Accuracy 0.462158203125\n",
      "Iteration 92630 Training loss 0.04835870862007141 Validation loss 0.0531047061085701 Accuracy 0.45947265625\n",
      "Iteration 92640 Training loss 0.048582885414361954 Validation loss 0.05267836153507233 Accuracy 0.4609375\n",
      "Iteration 92650 Training loss 0.0480538010597229 Validation loss 0.05347098037600517 Accuracy 0.455078125\n",
      "Iteration 92660 Training loss 0.052793752402067184 Validation loss 0.053477440029382706 Accuracy 0.45703125\n",
      "Iteration 92670 Training loss 0.05206283554434776 Validation loss 0.05318991094827652 Accuracy 0.458740234375\n",
      "Iteration 92680 Training loss 0.05007627233862877 Validation loss 0.052398014813661575 Accuracy 0.4619140625\n",
      "Iteration 92690 Training loss 0.052325379103422165 Validation loss 0.052970755845308304 Accuracy 0.46142578125\n",
      "Iteration 92700 Training loss 0.04729046672582626 Validation loss 0.05301929637789726 Accuracy 0.459716796875\n",
      "Iteration 92710 Training loss 0.05044475197792053 Validation loss 0.052645791321992874 Accuracy 0.462646484375\n",
      "Iteration 92720 Training loss 0.04884226247668266 Validation loss 0.05270956829190254 Accuracy 0.46142578125\n",
      "Iteration 92730 Training loss 0.05414435267448425 Validation loss 0.05234944075345993 Accuracy 0.463134765625\n",
      "Iteration 92740 Training loss 0.04746410995721817 Validation loss 0.05245010554790497 Accuracy 0.46240234375\n",
      "Iteration 92750 Training loss 0.04987017810344696 Validation loss 0.05249762535095215 Accuracy 0.46240234375\n",
      "Iteration 92760 Training loss 0.05287535488605499 Validation loss 0.05262238532304764 Accuracy 0.461181640625\n",
      "Iteration 92770 Training loss 0.05002420023083687 Validation loss 0.052610110491514206 Accuracy 0.46240234375\n",
      "Iteration 92780 Training loss 0.04848687723278999 Validation loss 0.05256921797990799 Accuracy 0.46337890625\n",
      "Iteration 92790 Training loss 0.04691486060619354 Validation loss 0.052388083189725876 Accuracy 0.462890625\n",
      "Iteration 92800 Training loss 0.047257352620363235 Validation loss 0.05293066427111626 Accuracy 0.4619140625\n",
      "Iteration 92810 Training loss 0.04804205149412155 Validation loss 0.05246707424521446 Accuracy 0.461181640625\n",
      "Iteration 92820 Training loss 0.049192748963832855 Validation loss 0.0522843599319458 Accuracy 0.462890625\n",
      "Iteration 92830 Training loss 0.05295589193701744 Validation loss 0.05281991884112358 Accuracy 0.461181640625\n",
      "Iteration 92840 Training loss 0.050662241876125336 Validation loss 0.052775390446186066 Accuracy 0.461669921875\n",
      "Iteration 92850 Training loss 0.050649575889110565 Validation loss 0.05281278118491173 Accuracy 0.461181640625\n",
      "Iteration 92860 Training loss 0.04737187549471855 Validation loss 0.052757009863853455 Accuracy 0.461669921875\n",
      "Iteration 92870 Training loss 0.052498966455459595 Validation loss 0.05354435741901398 Accuracy 0.458740234375\n",
      "Iteration 92880 Training loss 0.04728350788354874 Validation loss 0.05276298522949219 Accuracy 0.461181640625\n",
      "Iteration 92890 Training loss 0.04994755983352661 Validation loss 0.053060080856084824 Accuracy 0.4599609375\n",
      "Iteration 92900 Training loss 0.05044974759221077 Validation loss 0.05274467542767525 Accuracy 0.458740234375\n",
      "Iteration 92910 Training loss 0.04927162453532219 Validation loss 0.052721086889505386 Accuracy 0.4619140625\n",
      "Iteration 92920 Training loss 0.05137930065393448 Validation loss 0.052691128104925156 Accuracy 0.462646484375\n",
      "Iteration 92930 Training loss 0.04735745117068291 Validation loss 0.0523960143327713 Accuracy 0.461181640625\n",
      "Iteration 92940 Training loss 0.05289972946047783 Validation loss 0.05271949991583824 Accuracy 0.460205078125\n",
      "Iteration 92950 Training loss 0.0482851043343544 Validation loss 0.05272194370627403 Accuracy 0.4619140625\n",
      "Iteration 92960 Training loss 0.05148375406861305 Validation loss 0.05255115032196045 Accuracy 0.461181640625\n",
      "Iteration 92970 Training loss 0.04555898904800415 Validation loss 0.052677810192108154 Accuracy 0.462158203125\n",
      "Iteration 92980 Training loss 0.0519382618367672 Validation loss 0.053747937083244324 Accuracy 0.452392578125\n",
      "Iteration 92990 Training loss 0.05160245671868324 Validation loss 0.053192224353551865 Accuracy 0.458984375\n",
      "Iteration 93000 Training loss 0.04643922299146652 Validation loss 0.05334042012691498 Accuracy 0.458740234375\n",
      "Iteration 93010 Training loss 0.05130342021584511 Validation loss 0.05304620787501335 Accuracy 0.459716796875\n",
      "Iteration 93020 Training loss 0.04903947934508324 Validation loss 0.0526946522295475 Accuracy 0.4619140625\n",
      "Iteration 93030 Training loss 0.05100705847144127 Validation loss 0.053472280502319336 Accuracy 0.456787109375\n",
      "Iteration 93040 Training loss 0.049997150897979736 Validation loss 0.05279611051082611 Accuracy 0.460205078125\n",
      "Iteration 93050 Training loss 0.05154842138290405 Validation loss 0.05279204994440079 Accuracy 0.461669921875\n",
      "Iteration 93060 Training loss 0.051636602729558945 Validation loss 0.0531451590359211 Accuracy 0.459716796875\n",
      "Iteration 93070 Training loss 0.048780396580696106 Validation loss 0.0526605099439621 Accuracy 0.46240234375\n",
      "Iteration 93080 Training loss 0.0482022687792778 Validation loss 0.05253659188747406 Accuracy 0.46240234375\n",
      "Iteration 93090 Training loss 0.04875987023115158 Validation loss 0.05307809263467789 Accuracy 0.45751953125\n",
      "Iteration 93100 Training loss 0.04934229329228401 Validation loss 0.05281059071421623 Accuracy 0.461181640625\n",
      "Iteration 93110 Training loss 0.05408088117837906 Validation loss 0.05288790911436081 Accuracy 0.460693359375\n",
      "Iteration 93120 Training loss 0.046084411442279816 Validation loss 0.052684150636196136 Accuracy 0.462158203125\n",
      "Iteration 93130 Training loss 0.04679298773407936 Validation loss 0.053018875420093536 Accuracy 0.460205078125\n",
      "Iteration 93140 Training loss 0.05411373823881149 Validation loss 0.05302976444363594 Accuracy 0.4619140625\n",
      "Iteration 93150 Training loss 0.05194798856973648 Validation loss 0.05258400738239288 Accuracy 0.463134765625\n",
      "Iteration 93160 Training loss 0.052237655967473984 Validation loss 0.05253737047314644 Accuracy 0.4619140625\n",
      "Iteration 93170 Training loss 0.04828304052352905 Validation loss 0.05259538069367409 Accuracy 0.4619140625\n",
      "Iteration 93180 Training loss 0.05133244767785072 Validation loss 0.05259482190012932 Accuracy 0.461181640625\n",
      "Iteration 93190 Training loss 0.050851501524448395 Validation loss 0.05277647450566292 Accuracy 0.460693359375\n",
      "Iteration 93200 Training loss 0.04958021640777588 Validation loss 0.0529063381254673 Accuracy 0.4580078125\n",
      "Iteration 93210 Training loss 0.051019396632909775 Validation loss 0.05321703851222992 Accuracy 0.45849609375\n",
      "Iteration 93220 Training loss 0.05146332085132599 Validation loss 0.053023967891931534 Accuracy 0.458984375\n",
      "Iteration 93230 Training loss 0.04826875776052475 Validation loss 0.05293811485171318 Accuracy 0.460205078125\n",
      "Iteration 93240 Training loss 0.04798312485218048 Validation loss 0.0528155080974102 Accuracy 0.460693359375\n",
      "Iteration 93250 Training loss 0.048352472484111786 Validation loss 0.05283503234386444 Accuracy 0.46142578125\n",
      "Iteration 93260 Training loss 0.053235508501529694 Validation loss 0.05317799001932144 Accuracy 0.46044921875\n",
      "Iteration 93270 Training loss 0.05229164659976959 Validation loss 0.052815068513154984 Accuracy 0.4619140625\n",
      "Iteration 93280 Training loss 0.05115211755037308 Validation loss 0.05275638401508331 Accuracy 0.4609375\n",
      "Iteration 93290 Training loss 0.052883509546518326 Validation loss 0.05300086736679077 Accuracy 0.460693359375\n",
      "Iteration 93300 Training loss 0.05078041926026344 Validation loss 0.05305168777704239 Accuracy 0.459716796875\n",
      "Iteration 93310 Training loss 0.05188477411866188 Validation loss 0.05271241441369057 Accuracy 0.461669921875\n",
      "Iteration 93320 Training loss 0.051005665212869644 Validation loss 0.052985213696956635 Accuracy 0.4599609375\n",
      "Iteration 93330 Training loss 0.052304890006780624 Validation loss 0.052543532103300095 Accuracy 0.461669921875\n",
      "Iteration 93340 Training loss 0.050695620477199554 Validation loss 0.0524301752448082 Accuracy 0.462158203125\n",
      "Iteration 93350 Training loss 0.051287759095430374 Validation loss 0.053181033581495285 Accuracy 0.45703125\n",
      "Iteration 93360 Training loss 0.04843611642718315 Validation loss 0.05285893380641937 Accuracy 0.459716796875\n",
      "Iteration 93370 Training loss 0.049110498279333115 Validation loss 0.05285542458295822 Accuracy 0.459228515625\n",
      "Iteration 93380 Training loss 0.04897674545645714 Validation loss 0.05251618102192879 Accuracy 0.4619140625\n",
      "Iteration 93390 Training loss 0.05173474922776222 Validation loss 0.05283037945628166 Accuracy 0.460205078125\n",
      "Iteration 93400 Training loss 0.04899805784225464 Validation loss 0.05280974507331848 Accuracy 0.460693359375\n",
      "Iteration 93410 Training loss 0.049759771674871445 Validation loss 0.05288830026984215 Accuracy 0.460693359375\n",
      "Iteration 93420 Training loss 0.052880655974149704 Validation loss 0.052543602883815765 Accuracy 0.4619140625\n",
      "Iteration 93430 Training loss 0.048220809549093246 Validation loss 0.05328449234366417 Accuracy 0.456298828125\n",
      "Iteration 93440 Training loss 0.050963498651981354 Validation loss 0.05259688198566437 Accuracy 0.462158203125\n",
      "Iteration 93450 Training loss 0.04825857654213905 Validation loss 0.05323014408349991 Accuracy 0.456298828125\n",
      "Iteration 93460 Training loss 0.050155747681856155 Validation loss 0.05294375866651535 Accuracy 0.459716796875\n",
      "Iteration 93470 Training loss 0.05006130784749985 Validation loss 0.052770599722862244 Accuracy 0.4619140625\n",
      "Iteration 93480 Training loss 0.05247307941317558 Validation loss 0.05292496085166931 Accuracy 0.459228515625\n",
      "Iteration 93490 Training loss 0.05122583359479904 Validation loss 0.05306358262896538 Accuracy 0.458740234375\n",
      "Iteration 93500 Training loss 0.05378299579024315 Validation loss 0.05292542651295662 Accuracy 0.46044921875\n",
      "Iteration 93510 Training loss 0.051370564848184586 Validation loss 0.05336880683898926 Accuracy 0.455078125\n",
      "Iteration 93520 Training loss 0.05332382768392563 Validation loss 0.05379358306527138 Accuracy 0.4501953125\n",
      "Iteration 93530 Training loss 0.05274824798107147 Validation loss 0.052719596773386 Accuracy 0.46142578125\n",
      "Iteration 93540 Training loss 0.05137713626027107 Validation loss 0.05313102528452873 Accuracy 0.460205078125\n",
      "Iteration 93550 Training loss 0.05056767165660858 Validation loss 0.0526740625500679 Accuracy 0.4609375\n",
      "Iteration 93560 Training loss 0.049832411110401154 Validation loss 0.052739765495061874 Accuracy 0.461181640625\n",
      "Iteration 93570 Training loss 0.04864312335848808 Validation loss 0.052757907658815384 Accuracy 0.462158203125\n",
      "Iteration 93580 Training loss 0.050526637583971024 Validation loss 0.05273044854402542 Accuracy 0.46142578125\n",
      "Iteration 93590 Training loss 0.05270976200699806 Validation loss 0.05266047641634941 Accuracy 0.4609375\n",
      "Iteration 93600 Training loss 0.04942798241972923 Validation loss 0.05257861316204071 Accuracy 0.463623046875\n",
      "Iteration 93610 Training loss 0.05074364319443703 Validation loss 0.0526299886405468 Accuracy 0.460693359375\n",
      "Iteration 93620 Training loss 0.05039982125163078 Validation loss 0.05267325043678284 Accuracy 0.460693359375\n",
      "Iteration 93630 Training loss 0.04959115386009216 Validation loss 0.05325985327363014 Accuracy 0.45849609375\n",
      "Iteration 93640 Training loss 0.04945332929491997 Validation loss 0.05311625823378563 Accuracy 0.459716796875\n",
      "Iteration 93650 Training loss 0.05228189006447792 Validation loss 0.05268899351358414 Accuracy 0.462646484375\n",
      "Iteration 93660 Training loss 0.05159273371100426 Validation loss 0.05296611040830612 Accuracy 0.459716796875\n",
      "Iteration 93670 Training loss 0.049306076020002365 Validation loss 0.053024403750896454 Accuracy 0.461181640625\n",
      "Iteration 93680 Training loss 0.04972364380955696 Validation loss 0.05279600992798805 Accuracy 0.461181640625\n",
      "Iteration 93690 Training loss 0.050293926149606705 Validation loss 0.05282997339963913 Accuracy 0.461669921875\n",
      "Iteration 93700 Training loss 0.04980538785457611 Validation loss 0.05307824909687042 Accuracy 0.458251953125\n",
      "Iteration 93710 Training loss 0.05008292570710182 Validation loss 0.05310874059796333 Accuracy 0.458740234375\n",
      "Iteration 93720 Training loss 0.04814897105097771 Validation loss 0.052619900554418564 Accuracy 0.461669921875\n",
      "Iteration 93730 Training loss 0.05461962893605232 Validation loss 0.05308413878083229 Accuracy 0.459716796875\n",
      "Iteration 93740 Training loss 0.050653908401727676 Validation loss 0.05314802750945091 Accuracy 0.460205078125\n",
      "Iteration 93750 Training loss 0.050239197909832 Validation loss 0.05295613780617714 Accuracy 0.462646484375\n",
      "Iteration 93760 Training loss 0.04933566227555275 Validation loss 0.053015097975730896 Accuracy 0.45947265625\n",
      "Iteration 93770 Training loss 0.049522336572408676 Validation loss 0.05271105468273163 Accuracy 0.462158203125\n",
      "Iteration 93780 Training loss 0.05053392052650452 Validation loss 0.052457816898822784 Accuracy 0.46240234375\n",
      "Iteration 93790 Training loss 0.047055959701538086 Validation loss 0.0528288371860981 Accuracy 0.461669921875\n",
      "Iteration 93800 Training loss 0.04767250642180443 Validation loss 0.0529392771422863 Accuracy 0.460205078125\n",
      "Iteration 93810 Training loss 0.05294066295027733 Validation loss 0.05268271267414093 Accuracy 0.461669921875\n",
      "Iteration 93820 Training loss 0.053555529564619064 Validation loss 0.05334814265370369 Accuracy 0.4580078125\n",
      "Iteration 93830 Training loss 0.04927995428442955 Validation loss 0.05313322693109512 Accuracy 0.460693359375\n",
      "Iteration 93840 Training loss 0.05100126564502716 Validation loss 0.052643515169620514 Accuracy 0.46240234375\n",
      "Iteration 93850 Training loss 0.04735735058784485 Validation loss 0.05286509543657303 Accuracy 0.4609375\n",
      "Iteration 93860 Training loss 0.05020565912127495 Validation loss 0.052572254091501236 Accuracy 0.461669921875\n",
      "Iteration 93870 Training loss 0.05087597668170929 Validation loss 0.05254765599966049 Accuracy 0.4619140625\n",
      "Iteration 93880 Training loss 0.050321731716394424 Validation loss 0.05274474620819092 Accuracy 0.4619140625\n",
      "Iteration 93890 Training loss 0.04754605144262314 Validation loss 0.052604518830776215 Accuracy 0.461669921875\n",
      "Iteration 93900 Training loss 0.04990384355187416 Validation loss 0.05242539569735527 Accuracy 0.46337890625\n",
      "Iteration 93910 Training loss 0.04875887930393219 Validation loss 0.052859965711832047 Accuracy 0.461181640625\n",
      "Iteration 93920 Training loss 0.05213421583175659 Validation loss 0.05291289836168289 Accuracy 0.45703125\n",
      "Iteration 93930 Training loss 0.04877008870244026 Validation loss 0.05260181427001953 Accuracy 0.462890625\n",
      "Iteration 93940 Training loss 0.0487043559551239 Validation loss 0.05288712680339813 Accuracy 0.45703125\n",
      "Iteration 93950 Training loss 0.05208063870668411 Validation loss 0.053220540285110474 Accuracy 0.455810546875\n",
      "Iteration 93960 Training loss 0.05079085752367973 Validation loss 0.05285901203751564 Accuracy 0.45947265625\n",
      "Iteration 93970 Training loss 0.048805296421051025 Validation loss 0.05286528170108795 Accuracy 0.458984375\n",
      "Iteration 93980 Training loss 0.04795936122536659 Validation loss 0.053343452513217926 Accuracy 0.456787109375\n",
      "Iteration 93990 Training loss 0.052612803876399994 Validation loss 0.052961770445108414 Accuracy 0.458251953125\n",
      "Iteration 94000 Training loss 0.04955681413412094 Validation loss 0.05265618860721588 Accuracy 0.462890625\n",
      "Iteration 94010 Training loss 0.05277840048074722 Validation loss 0.053000546991825104 Accuracy 0.45849609375\n",
      "Iteration 94020 Training loss 0.05086128041148186 Validation loss 0.05280238017439842 Accuracy 0.460205078125\n",
      "Iteration 94030 Training loss 0.05454146862030029 Validation loss 0.05273698270320892 Accuracy 0.4599609375\n",
      "Iteration 94040 Training loss 0.05267167091369629 Validation loss 0.05340953543782234 Accuracy 0.4560546875\n",
      "Iteration 94050 Training loss 0.050461556762456894 Validation loss 0.052970752120018005 Accuracy 0.461181640625\n",
      "Iteration 94060 Training loss 0.052741292864084244 Validation loss 0.05277281627058983 Accuracy 0.46142578125\n",
      "Iteration 94070 Training loss 0.05226530507206917 Validation loss 0.053033944219350815 Accuracy 0.4599609375\n",
      "Iteration 94080 Training loss 0.052628256380558014 Validation loss 0.053048085421323776 Accuracy 0.458984375\n",
      "Iteration 94090 Training loss 0.052004922181367874 Validation loss 0.052519526332616806 Accuracy 0.462890625\n",
      "Iteration 94100 Training loss 0.04816928133368492 Validation loss 0.05274808779358864 Accuracy 0.4619140625\n",
      "Iteration 94110 Training loss 0.05046587064862251 Validation loss 0.0529114231467247 Accuracy 0.461669921875\n",
      "Iteration 94120 Training loss 0.05217159911990166 Validation loss 0.05298910662531853 Accuracy 0.460693359375\n",
      "Iteration 94130 Training loss 0.04931575432419777 Validation loss 0.052908845245838165 Accuracy 0.459716796875\n",
      "Iteration 94140 Training loss 0.05725295841693878 Validation loss 0.05258744955062866 Accuracy 0.4619140625\n",
      "Iteration 94150 Training loss 0.049072396010160446 Validation loss 0.05253428965806961 Accuracy 0.46240234375\n",
      "Iteration 94160 Training loss 0.04934897646307945 Validation loss 0.054180119186639786 Accuracy 0.44873046875\n",
      "Iteration 94170 Training loss 0.04999048635363579 Validation loss 0.05294184759259224 Accuracy 0.4609375\n",
      "Iteration 94180 Training loss 0.05165877565741539 Validation loss 0.05378706753253937 Accuracy 0.44970703125\n",
      "Iteration 94190 Training loss 0.05154460668563843 Validation loss 0.052885156124830246 Accuracy 0.461181640625\n",
      "Iteration 94200 Training loss 0.04809217154979706 Validation loss 0.05251440405845642 Accuracy 0.462158203125\n",
      "Iteration 94210 Training loss 0.05129086598753929 Validation loss 0.05274811387062073 Accuracy 0.4619140625\n",
      "Iteration 94220 Training loss 0.0505513995885849 Validation loss 0.05317673832178116 Accuracy 0.459228515625\n",
      "Iteration 94230 Training loss 0.046461399644613266 Validation loss 0.05266215652227402 Accuracy 0.46240234375\n",
      "Iteration 94240 Training loss 0.049380335956811905 Validation loss 0.05293117091059685 Accuracy 0.458984375\n",
      "Iteration 94250 Training loss 0.05069055035710335 Validation loss 0.05277340114116669 Accuracy 0.459228515625\n",
      "Iteration 94260 Training loss 0.048882242292165756 Validation loss 0.052538417279720306 Accuracy 0.461181640625\n",
      "Iteration 94270 Training loss 0.051121681928634644 Validation loss 0.05279453843832016 Accuracy 0.46142578125\n",
      "Iteration 94280 Training loss 0.046021223068237305 Validation loss 0.05272367596626282 Accuracy 0.460205078125\n",
      "Iteration 94290 Training loss 0.05151091888546944 Validation loss 0.0530572347342968 Accuracy 0.4609375\n",
      "Iteration 94300 Training loss 0.04991292208433151 Validation loss 0.053192753344774246 Accuracy 0.459716796875\n",
      "Iteration 94310 Training loss 0.04880014434456825 Validation loss 0.052764132618904114 Accuracy 0.4609375\n",
      "Iteration 94320 Training loss 0.04836804047226906 Validation loss 0.05281057581305504 Accuracy 0.4599609375\n",
      "Iteration 94330 Training loss 0.050661850720644 Validation loss 0.052651748061180115 Accuracy 0.46240234375\n",
      "Iteration 94340 Training loss 0.04945685714483261 Validation loss 0.05361637473106384 Accuracy 0.4560546875\n",
      "Iteration 94350 Training loss 0.051130980253219604 Validation loss 0.05369071289896965 Accuracy 0.454833984375\n",
      "Iteration 94360 Training loss 0.05378982052206993 Validation loss 0.052826981991529465 Accuracy 0.461181640625\n",
      "Iteration 94370 Training loss 0.048783231526613235 Validation loss 0.05282016098499298 Accuracy 0.46142578125\n",
      "Iteration 94380 Training loss 0.05038569122552872 Validation loss 0.05268484726548195 Accuracy 0.461181640625\n",
      "Iteration 94390 Training loss 0.05130018666386604 Validation loss 0.052900251001119614 Accuracy 0.459716796875\n",
      "Iteration 94400 Training loss 0.04819362238049507 Validation loss 0.052719440311193466 Accuracy 0.4599609375\n",
      "Iteration 94410 Training loss 0.051628462970256805 Validation loss 0.05297967791557312 Accuracy 0.458984375\n",
      "Iteration 94420 Training loss 0.05121966078877449 Validation loss 0.053343210369348526 Accuracy 0.45703125\n",
      "Iteration 94430 Training loss 0.05019737780094147 Validation loss 0.05267620459198952 Accuracy 0.4609375\n",
      "Iteration 94440 Training loss 0.048325616866350174 Validation loss 0.05254179611802101 Accuracy 0.4619140625\n",
      "Iteration 94450 Training loss 0.05033330246806145 Validation loss 0.052414603531360626 Accuracy 0.462890625\n",
      "Iteration 94460 Training loss 0.05163915082812309 Validation loss 0.05350403115153313 Accuracy 0.456787109375\n",
      "Iteration 94470 Training loss 0.04777338728308678 Validation loss 0.05345707759261131 Accuracy 0.456298828125\n",
      "Iteration 94480 Training loss 0.05306762456893921 Validation loss 0.05315845459699631 Accuracy 0.45849609375\n",
      "Iteration 94490 Training loss 0.0478326752781868 Validation loss 0.05305759236216545 Accuracy 0.45849609375\n",
      "Iteration 94500 Training loss 0.04991896450519562 Validation loss 0.05264180526137352 Accuracy 0.461181640625\n",
      "Iteration 94510 Training loss 0.05172443762421608 Validation loss 0.052661996334791183 Accuracy 0.460693359375\n",
      "Iteration 94520 Training loss 0.04961464926600456 Validation loss 0.05290687456727028 Accuracy 0.460693359375\n",
      "Iteration 94530 Training loss 0.05251708999276161 Validation loss 0.05327621102333069 Accuracy 0.45849609375\n",
      "Iteration 94540 Training loss 0.0523790568113327 Validation loss 0.05337781459093094 Accuracy 0.45703125\n",
      "Iteration 94550 Training loss 0.051499221473932266 Validation loss 0.05268201604485512 Accuracy 0.462890625\n",
      "Iteration 94560 Training loss 0.050369374454021454 Validation loss 0.05366005748510361 Accuracy 0.45458984375\n",
      "Iteration 94570 Training loss 0.05201466381549835 Validation loss 0.05263407155871391 Accuracy 0.4619140625\n",
      "Iteration 94580 Training loss 0.050602853298187256 Validation loss 0.05268779769539833 Accuracy 0.4609375\n",
      "Iteration 94590 Training loss 0.04997754842042923 Validation loss 0.052435290068387985 Accuracy 0.46240234375\n",
      "Iteration 94600 Training loss 0.047230158001184464 Validation loss 0.052814871072769165 Accuracy 0.4599609375\n",
      "Iteration 94610 Training loss 0.050473570823669434 Validation loss 0.05330107361078262 Accuracy 0.457763671875\n",
      "Iteration 94620 Training loss 0.05143716558814049 Validation loss 0.05310704931616783 Accuracy 0.459228515625\n",
      "Iteration 94630 Training loss 0.04988450929522514 Validation loss 0.05305702984333038 Accuracy 0.459716796875\n",
      "Iteration 94640 Training loss 0.04841452091932297 Validation loss 0.05254156515002251 Accuracy 0.462890625\n",
      "Iteration 94650 Training loss 0.052912674844264984 Validation loss 0.05264367535710335 Accuracy 0.46240234375\n",
      "Iteration 94660 Training loss 0.049591757357120514 Validation loss 0.05278044193983078 Accuracy 0.4609375\n",
      "Iteration 94670 Training loss 0.04951401799917221 Validation loss 0.0524933859705925 Accuracy 0.4619140625\n",
      "Iteration 94680 Training loss 0.051353972405195236 Validation loss 0.05306721106171608 Accuracy 0.4599609375\n",
      "Iteration 94690 Training loss 0.050300952047109604 Validation loss 0.05272134765982628 Accuracy 0.458984375\n",
      "Iteration 94700 Training loss 0.04980975389480591 Validation loss 0.05267305672168732 Accuracy 0.4599609375\n",
      "Iteration 94710 Training loss 0.04731025546789169 Validation loss 0.05303115025162697 Accuracy 0.4599609375\n",
      "Iteration 94720 Training loss 0.05187990888953209 Validation loss 0.052426356822252274 Accuracy 0.462158203125\n",
      "Iteration 94730 Training loss 0.050159748643636703 Validation loss 0.0526217557489872 Accuracy 0.4619140625\n",
      "Iteration 94740 Training loss 0.050028275698423386 Validation loss 0.05272195115685463 Accuracy 0.46044921875\n",
      "Iteration 94750 Training loss 0.04548296704888344 Validation loss 0.05313357338309288 Accuracy 0.459716796875\n",
      "Iteration 94760 Training loss 0.05160493403673172 Validation loss 0.052845705300569534 Accuracy 0.462158203125\n",
      "Iteration 94770 Training loss 0.050711486488580704 Validation loss 0.053176794201135635 Accuracy 0.458984375\n",
      "Iteration 94780 Training loss 0.04968355596065521 Validation loss 0.0526413656771183 Accuracy 0.462646484375\n",
      "Iteration 94790 Training loss 0.05104389786720276 Validation loss 0.05277382582426071 Accuracy 0.462158203125\n",
      "Iteration 94800 Training loss 0.047691360116004944 Validation loss 0.0531572625041008 Accuracy 0.459716796875\n",
      "Iteration 94810 Training loss 0.05206461623311043 Validation loss 0.05324574559926987 Accuracy 0.460205078125\n",
      "Iteration 94820 Training loss 0.054320402443408966 Validation loss 0.05311065912246704 Accuracy 0.458740234375\n",
      "Iteration 94830 Training loss 0.04989730939269066 Validation loss 0.05294434353709221 Accuracy 0.460205078125\n",
      "Iteration 94840 Training loss 0.04895105957984924 Validation loss 0.05267621949315071 Accuracy 0.462158203125\n",
      "Iteration 94850 Training loss 0.05001248046755791 Validation loss 0.052941933274269104 Accuracy 0.460205078125\n",
      "Iteration 94860 Training loss 0.04946482181549072 Validation loss 0.05250299349427223 Accuracy 0.462646484375\n",
      "Iteration 94870 Training loss 0.049803558737039566 Validation loss 0.05314963683485985 Accuracy 0.4609375\n",
      "Iteration 94880 Training loss 0.04981958493590355 Validation loss 0.05260432884097099 Accuracy 0.462646484375\n",
      "Iteration 94890 Training loss 0.05241108685731888 Validation loss 0.05290538817644119 Accuracy 0.461669921875\n",
      "Iteration 94900 Training loss 0.05264183506369591 Validation loss 0.052633874118328094 Accuracy 0.4619140625\n",
      "Iteration 94910 Training loss 0.04876702278852463 Validation loss 0.052824001759290695 Accuracy 0.461669921875\n",
      "Iteration 94920 Training loss 0.05139647796750069 Validation loss 0.05299145355820656 Accuracy 0.459228515625\n",
      "Iteration 94930 Training loss 0.05304868519306183 Validation loss 0.0526672899723053 Accuracy 0.46240234375\n",
      "Iteration 94940 Training loss 0.05352425202727318 Validation loss 0.0529722236096859 Accuracy 0.4599609375\n",
      "Iteration 94950 Training loss 0.04993036016821861 Validation loss 0.05246075242757797 Accuracy 0.462158203125\n",
      "Iteration 94960 Training loss 0.05096907541155815 Validation loss 0.05268416553735733 Accuracy 0.461181640625\n",
      "Iteration 94970 Training loss 0.05363060534000397 Validation loss 0.053213659673929214 Accuracy 0.45849609375\n",
      "Iteration 94980 Training loss 0.05143740400671959 Validation loss 0.05319563299417496 Accuracy 0.45849609375\n",
      "Iteration 94990 Training loss 0.04929662123322487 Validation loss 0.0531175471842289 Accuracy 0.459228515625\n",
      "Iteration 95000 Training loss 0.0475083626806736 Validation loss 0.0530904196202755 Accuracy 0.45849609375\n",
      "Iteration 95010 Training loss 0.05052635073661804 Validation loss 0.053007226437330246 Accuracy 0.460205078125\n",
      "Iteration 95020 Training loss 0.0501975379884243 Validation loss 0.05323578044772148 Accuracy 0.458984375\n",
      "Iteration 95030 Training loss 0.05259392783045769 Validation loss 0.052564144134521484 Accuracy 0.4619140625\n",
      "Iteration 95040 Training loss 0.053494710475206375 Validation loss 0.053123075515031815 Accuracy 0.458984375\n",
      "Iteration 95050 Training loss 0.051164764910936356 Validation loss 0.0525481142103672 Accuracy 0.460693359375\n",
      "Iteration 95060 Training loss 0.05185471475124359 Validation loss 0.053112342953681946 Accuracy 0.458984375\n",
      "Iteration 95070 Training loss 0.053939346224069595 Validation loss 0.05263297259807587 Accuracy 0.4619140625\n",
      "Iteration 95080 Training loss 0.050945669412612915 Validation loss 0.05289937183260918 Accuracy 0.461181640625\n",
      "Iteration 95090 Training loss 0.05061972513794899 Validation loss 0.053011566400527954 Accuracy 0.46044921875\n",
      "Iteration 95100 Training loss 0.05004638433456421 Validation loss 0.05288376659154892 Accuracy 0.4609375\n",
      "Iteration 95110 Training loss 0.0523124597966671 Validation loss 0.05275987833738327 Accuracy 0.46044921875\n",
      "Iteration 95120 Training loss 0.05409190431237221 Validation loss 0.05247202888131142 Accuracy 0.46240234375\n",
      "Iteration 95130 Training loss 0.049333687871694565 Validation loss 0.05272446200251579 Accuracy 0.46240234375\n",
      "Iteration 95140 Training loss 0.0496475026011467 Validation loss 0.05266726762056351 Accuracy 0.4609375\n",
      "Iteration 95150 Training loss 0.04850856587290764 Validation loss 0.05263913795351982 Accuracy 0.46240234375\n",
      "Iteration 95160 Training loss 0.04759325087070465 Validation loss 0.052541229873895645 Accuracy 0.46240234375\n",
      "Iteration 95170 Training loss 0.04911515861749649 Validation loss 0.052838459610939026 Accuracy 0.461669921875\n",
      "Iteration 95180 Training loss 0.049556367099285126 Validation loss 0.05317693203687668 Accuracy 0.4580078125\n",
      "Iteration 95190 Training loss 0.048507384955883026 Validation loss 0.05263783037662506 Accuracy 0.462158203125\n",
      "Iteration 95200 Training loss 0.05082133784890175 Validation loss 0.05275968462228775 Accuracy 0.461669921875\n",
      "Iteration 95210 Training loss 0.049613870680332184 Validation loss 0.05298888683319092 Accuracy 0.459716796875\n",
      "Iteration 95220 Training loss 0.05151745676994324 Validation loss 0.053024422377347946 Accuracy 0.4580078125\n",
      "Iteration 95230 Training loss 0.05365150049328804 Validation loss 0.05286617949604988 Accuracy 0.46142578125\n",
      "Iteration 95240 Training loss 0.05442679300904274 Validation loss 0.05308191105723381 Accuracy 0.460693359375\n",
      "Iteration 95250 Training loss 0.050635721534490585 Validation loss 0.05316466838121414 Accuracy 0.4599609375\n",
      "Iteration 95260 Training loss 0.047434985637664795 Validation loss 0.053148429840803146 Accuracy 0.4599609375\n",
      "Iteration 95270 Training loss 0.04957914352416992 Validation loss 0.0530451163649559 Accuracy 0.461181640625\n",
      "Iteration 95280 Training loss 0.04962491989135742 Validation loss 0.05245427042245865 Accuracy 0.46240234375\n",
      "Iteration 95290 Training loss 0.052174754440784454 Validation loss 0.05258382856845856 Accuracy 0.461181640625\n",
      "Iteration 95300 Training loss 0.0512731559574604 Validation loss 0.052608929574489594 Accuracy 0.460693359375\n",
      "Iteration 95310 Training loss 0.05099041759967804 Validation loss 0.052523016929626465 Accuracy 0.46240234375\n",
      "Iteration 95320 Training loss 0.048399198800325394 Validation loss 0.05282754823565483 Accuracy 0.459716796875\n",
      "Iteration 95330 Training loss 0.04756161943078041 Validation loss 0.05298422649502754 Accuracy 0.46142578125\n",
      "Iteration 95340 Training loss 0.04873520880937576 Validation loss 0.05285663902759552 Accuracy 0.461181640625\n",
      "Iteration 95350 Training loss 0.05118211358785629 Validation loss 0.052693117409944534 Accuracy 0.462646484375\n",
      "Iteration 95360 Training loss 0.050726063549518585 Validation loss 0.052516136318445206 Accuracy 0.46142578125\n",
      "Iteration 95370 Training loss 0.05613211914896965 Validation loss 0.053498271852731705 Accuracy 0.453857421875\n",
      "Iteration 95380 Training loss 0.05046818405389786 Validation loss 0.05294952169060707 Accuracy 0.46044921875\n",
      "Iteration 95390 Training loss 0.05202406644821167 Validation loss 0.05243643745779991 Accuracy 0.462158203125\n",
      "Iteration 95400 Training loss 0.05095585435628891 Validation loss 0.053006693720817566 Accuracy 0.4609375\n",
      "Iteration 95410 Training loss 0.05162511020898819 Validation loss 0.05290159955620766 Accuracy 0.46044921875\n",
      "Iteration 95420 Training loss 0.05210483819246292 Validation loss 0.05285133793950081 Accuracy 0.460205078125\n",
      "Iteration 95430 Training loss 0.050764068961143494 Validation loss 0.05273153632879257 Accuracy 0.46142578125\n",
      "Iteration 95440 Training loss 0.04765661060810089 Validation loss 0.05281342566013336 Accuracy 0.461181640625\n",
      "Iteration 95450 Training loss 0.05002656951546669 Validation loss 0.05269276350736618 Accuracy 0.46044921875\n",
      "Iteration 95460 Training loss 0.050965316593647 Validation loss 0.05264782905578613 Accuracy 0.463134765625\n",
      "Iteration 95470 Training loss 0.04876958206295967 Validation loss 0.05272872745990753 Accuracy 0.4599609375\n",
      "Iteration 95480 Training loss 0.04743766039609909 Validation loss 0.05270982161164284 Accuracy 0.460693359375\n",
      "Iteration 95490 Training loss 0.05417357757687569 Validation loss 0.05292541906237602 Accuracy 0.461669921875\n",
      "Iteration 95500 Training loss 0.05029239505529404 Validation loss 0.05256105214357376 Accuracy 0.462158203125\n",
      "Iteration 95510 Training loss 0.05086173862218857 Validation loss 0.052585799247026443 Accuracy 0.46240234375\n",
      "Iteration 95520 Training loss 0.05490858852863312 Validation loss 0.053294915705919266 Accuracy 0.4580078125\n",
      "Iteration 95530 Training loss 0.05060319975018501 Validation loss 0.05283985286951065 Accuracy 0.46142578125\n",
      "Iteration 95540 Training loss 0.05235203355550766 Validation loss 0.053110234439373016 Accuracy 0.4580078125\n",
      "Iteration 95550 Training loss 0.05094079673290253 Validation loss 0.053627729415893555 Accuracy 0.4541015625\n",
      "Iteration 95560 Training loss 0.04879019036889076 Validation loss 0.05269631743431091 Accuracy 0.463134765625\n",
      "Iteration 95570 Training loss 0.05096626281738281 Validation loss 0.05275503918528557 Accuracy 0.4619140625\n",
      "Iteration 95580 Training loss 0.04878389090299606 Validation loss 0.05261603742837906 Accuracy 0.4619140625\n",
      "Iteration 95590 Training loss 0.05080747604370117 Validation loss 0.05318527668714523 Accuracy 0.45849609375\n",
      "Iteration 95600 Training loss 0.052093565464019775 Validation loss 0.052684489637613297 Accuracy 0.461181640625\n",
      "Iteration 95610 Training loss 0.05317147821187973 Validation loss 0.05284528061747551 Accuracy 0.4619140625\n",
      "Iteration 95620 Training loss 0.048621974885463715 Validation loss 0.05264970660209656 Accuracy 0.46142578125\n",
      "Iteration 95630 Training loss 0.04756131023168564 Validation loss 0.05264072120189667 Accuracy 0.462158203125\n",
      "Iteration 95640 Training loss 0.05167710781097412 Validation loss 0.05274006724357605 Accuracy 0.4619140625\n",
      "Iteration 95650 Training loss 0.05043986067175865 Validation loss 0.05456114187836647 Accuracy 0.445556640625\n",
      "Iteration 95660 Training loss 0.047062937170267105 Validation loss 0.052489347755908966 Accuracy 0.460693359375\n",
      "Iteration 95670 Training loss 0.05275683104991913 Validation loss 0.05270468443632126 Accuracy 0.460205078125\n",
      "Iteration 95680 Training loss 0.052940987050533295 Validation loss 0.05305864289402962 Accuracy 0.45703125\n",
      "Iteration 95690 Training loss 0.053100138902664185 Validation loss 0.0528399683535099 Accuracy 0.460205078125\n",
      "Iteration 95700 Training loss 0.05161022022366524 Validation loss 0.053415682166814804 Accuracy 0.456787109375\n",
      "Iteration 95710 Training loss 0.05002247542142868 Validation loss 0.052414704114198685 Accuracy 0.4619140625\n",
      "Iteration 95720 Training loss 0.05037601292133331 Validation loss 0.05302180349826813 Accuracy 0.459716796875\n",
      "Iteration 95730 Training loss 0.05169672518968582 Validation loss 0.05318678915500641 Accuracy 0.4599609375\n",
      "Iteration 95740 Training loss 0.050885822623968124 Validation loss 0.052855927497148514 Accuracy 0.461669921875\n",
      "Iteration 95750 Training loss 0.0517609640955925 Validation loss 0.05286925658583641 Accuracy 0.461669921875\n",
      "Iteration 95760 Training loss 0.052197497338056564 Validation loss 0.0527883917093277 Accuracy 0.459228515625\n",
      "Iteration 95770 Training loss 0.05160525441169739 Validation loss 0.05245853215456009 Accuracy 0.462646484375\n",
      "Iteration 95780 Training loss 0.04508791118860245 Validation loss 0.05297677591443062 Accuracy 0.461181640625\n",
      "Iteration 95790 Training loss 0.05250762030482292 Validation loss 0.05295277759432793 Accuracy 0.460205078125\n",
      "Iteration 95800 Training loss 0.05015293136239052 Validation loss 0.05241335183382034 Accuracy 0.46240234375\n",
      "Iteration 95810 Training loss 0.05196923390030861 Validation loss 0.05236531049013138 Accuracy 0.462646484375\n",
      "Iteration 95820 Training loss 0.047957468777894974 Validation loss 0.053094979375600815 Accuracy 0.45947265625\n",
      "Iteration 95830 Training loss 0.05109291523694992 Validation loss 0.05338187888264656 Accuracy 0.45849609375\n",
      "Iteration 95840 Training loss 0.05297933518886566 Validation loss 0.05291108414530754 Accuracy 0.4599609375\n",
      "Iteration 95850 Training loss 0.04767083749175072 Validation loss 0.05263461917638779 Accuracy 0.461181640625\n",
      "Iteration 95860 Training loss 0.050336968153715134 Validation loss 0.05309600383043289 Accuracy 0.460693359375\n",
      "Iteration 95870 Training loss 0.05251409858465195 Validation loss 0.05286680534482002 Accuracy 0.461669921875\n",
      "Iteration 95880 Training loss 0.05322166904807091 Validation loss 0.052500877529382706 Accuracy 0.462646484375\n",
      "Iteration 95890 Training loss 0.050587158650159836 Validation loss 0.05268107354640961 Accuracy 0.462646484375\n",
      "Iteration 95900 Training loss 0.048259083181619644 Validation loss 0.05273830518126488 Accuracy 0.46240234375\n",
      "Iteration 95910 Training loss 0.05034656077623367 Validation loss 0.05325734242796898 Accuracy 0.458740234375\n",
      "Iteration 95920 Training loss 0.051276303827762604 Validation loss 0.05237878859043121 Accuracy 0.462646484375\n",
      "Iteration 95930 Training loss 0.054641012102365494 Validation loss 0.05320271477103233 Accuracy 0.459716796875\n",
      "Iteration 95940 Training loss 0.0496450737118721 Validation loss 0.05295601114630699 Accuracy 0.4599609375\n",
      "Iteration 95950 Training loss 0.05184254050254822 Validation loss 0.05257882550358772 Accuracy 0.46240234375\n",
      "Iteration 95960 Training loss 0.05171642825007439 Validation loss 0.05310346558690071 Accuracy 0.459716796875\n",
      "Iteration 95970 Training loss 0.05537138879299164 Validation loss 0.052560146898031235 Accuracy 0.461669921875\n",
      "Iteration 95980 Training loss 0.04885224997997284 Validation loss 0.05284394323825836 Accuracy 0.461181640625\n",
      "Iteration 95990 Training loss 0.05169311910867691 Validation loss 0.05312425643205643 Accuracy 0.46044921875\n",
      "Iteration 96000 Training loss 0.04874621704220772 Validation loss 0.05289676785469055 Accuracy 0.459716796875\n",
      "Iteration 96010 Training loss 0.04590529203414917 Validation loss 0.052900683134794235 Accuracy 0.460693359375\n",
      "Iteration 96020 Training loss 0.05101535841822624 Validation loss 0.05274633318185806 Accuracy 0.461181640625\n",
      "Iteration 96030 Training loss 0.05057393014431 Validation loss 0.05274118110537529 Accuracy 0.4609375\n",
      "Iteration 96040 Training loss 0.04790673777461052 Validation loss 0.05271840840578079 Accuracy 0.460693359375\n",
      "Iteration 96050 Training loss 0.05475956201553345 Validation loss 0.053448572754859924 Accuracy 0.456787109375\n",
      "Iteration 96060 Training loss 0.0497652068734169 Validation loss 0.05267397314310074 Accuracy 0.45947265625\n",
      "Iteration 96070 Training loss 0.05247115343809128 Validation loss 0.05264884978532791 Accuracy 0.462158203125\n",
      "Iteration 96080 Training loss 0.04654667153954506 Validation loss 0.05254809930920601 Accuracy 0.462890625\n",
      "Iteration 96090 Training loss 0.04741300269961357 Validation loss 0.05271395295858383 Accuracy 0.461669921875\n",
      "Iteration 96100 Training loss 0.04710493981838226 Validation loss 0.052869606763124466 Accuracy 0.45947265625\n",
      "Iteration 96110 Training loss 0.05021402984857559 Validation loss 0.05327367037534714 Accuracy 0.4580078125\n",
      "Iteration 96120 Training loss 0.05010867118835449 Validation loss 0.05272411182522774 Accuracy 0.460693359375\n",
      "Iteration 96130 Training loss 0.049474142491817474 Validation loss 0.05306427553296089 Accuracy 0.461181640625\n",
      "Iteration 96140 Training loss 0.050473712384700775 Validation loss 0.052592478692531586 Accuracy 0.461669921875\n",
      "Iteration 96150 Training loss 0.04892120882868767 Validation loss 0.05325784161686897 Accuracy 0.45751953125\n",
      "Iteration 96160 Training loss 0.04673558101058006 Validation loss 0.052462127059698105 Accuracy 0.46240234375\n",
      "Iteration 96170 Training loss 0.049111805856227875 Validation loss 0.05284970626235008 Accuracy 0.460693359375\n",
      "Iteration 96180 Training loss 0.05265418067574501 Validation loss 0.05310210958123207 Accuracy 0.460205078125\n",
      "Iteration 96190 Training loss 0.04785195738077164 Validation loss 0.05275457352399826 Accuracy 0.460205078125\n",
      "Iteration 96200 Training loss 0.0540754534304142 Validation loss 0.05288232862949371 Accuracy 0.461181640625\n",
      "Iteration 96210 Training loss 0.04961347579956055 Validation loss 0.052763063460588455 Accuracy 0.462646484375\n",
      "Iteration 96220 Training loss 0.049078959971666336 Validation loss 0.05281335860490799 Accuracy 0.45849609375\n",
      "Iteration 96230 Training loss 0.05075720325112343 Validation loss 0.052577923983335495 Accuracy 0.462646484375\n",
      "Iteration 96240 Training loss 0.05165140703320503 Validation loss 0.05300571769475937 Accuracy 0.460205078125\n",
      "Iteration 96250 Training loss 0.046194110065698624 Validation loss 0.05277913063764572 Accuracy 0.4609375\n",
      "Iteration 96260 Training loss 0.04886322468519211 Validation loss 0.05324413254857063 Accuracy 0.45947265625\n",
      "Iteration 96270 Training loss 0.05058116465806961 Validation loss 0.05281125754117966 Accuracy 0.462158203125\n",
      "Iteration 96280 Training loss 0.05004441738128662 Validation loss 0.05262020230293274 Accuracy 0.462158203125\n",
      "Iteration 96290 Training loss 0.05173486843705177 Validation loss 0.052898798137903214 Accuracy 0.460693359375\n",
      "Iteration 96300 Training loss 0.05135525017976761 Validation loss 0.05287730321288109 Accuracy 0.46044921875\n",
      "Iteration 96310 Training loss 0.05185876041650772 Validation loss 0.05278843641281128 Accuracy 0.461669921875\n",
      "Iteration 96320 Training loss 0.04768585041165352 Validation loss 0.05268611013889313 Accuracy 0.46142578125\n",
      "Iteration 96330 Training loss 0.0515596978366375 Validation loss 0.05294473096728325 Accuracy 0.461181640625\n",
      "Iteration 96340 Training loss 0.05215352401137352 Validation loss 0.05269956961274147 Accuracy 0.46142578125\n",
      "Iteration 96350 Training loss 0.052333179861307144 Validation loss 0.053549882024526596 Accuracy 0.455078125\n",
      "Iteration 96360 Training loss 0.05018738657236099 Validation loss 0.05263330787420273 Accuracy 0.4609375\n",
      "Iteration 96370 Training loss 0.05171860009431839 Validation loss 0.05281591787934303 Accuracy 0.4609375\n",
      "Iteration 96380 Training loss 0.050982944667339325 Validation loss 0.0526970699429512 Accuracy 0.46142578125\n",
      "Iteration 96390 Training loss 0.0462607815861702 Validation loss 0.05255043879151344 Accuracy 0.46240234375\n",
      "Iteration 96400 Training loss 0.05088232457637787 Validation loss 0.05286741256713867 Accuracy 0.4619140625\n",
      "Iteration 96410 Training loss 0.04959573969244957 Validation loss 0.05270035192370415 Accuracy 0.461181640625\n",
      "Iteration 96420 Training loss 0.049284789711236954 Validation loss 0.05255217105150223 Accuracy 0.461669921875\n",
      "Iteration 96430 Training loss 0.050978709012269974 Validation loss 0.05283859744668007 Accuracy 0.4609375\n",
      "Iteration 96440 Training loss 0.054603926837444305 Validation loss 0.05321498215198517 Accuracy 0.458251953125\n",
      "Iteration 96450 Training loss 0.05161502957344055 Validation loss 0.052905697375535965 Accuracy 0.462158203125\n",
      "Iteration 96460 Training loss 0.05052709951996803 Validation loss 0.052721209824085236 Accuracy 0.461669921875\n",
      "Iteration 96470 Training loss 0.0518813356757164 Validation loss 0.0528603121638298 Accuracy 0.46044921875\n",
      "Iteration 96480 Training loss 0.05103334039449692 Validation loss 0.052801914513111115 Accuracy 0.460693359375\n",
      "Iteration 96490 Training loss 0.048996277153491974 Validation loss 0.05283482372760773 Accuracy 0.46142578125\n",
      "Iteration 96500 Training loss 0.04797971621155739 Validation loss 0.052947595715522766 Accuracy 0.4599609375\n",
      "Iteration 96510 Training loss 0.04719378426671028 Validation loss 0.0525602325797081 Accuracy 0.460693359375\n",
      "Iteration 96520 Training loss 0.04804454371333122 Validation loss 0.05257447063922882 Accuracy 0.46044921875\n",
      "Iteration 96530 Training loss 0.048886094242334366 Validation loss 0.053941868245601654 Accuracy 0.449951171875\n",
      "Iteration 96540 Training loss 0.04749848321080208 Validation loss 0.052682604640722275 Accuracy 0.461181640625\n",
      "Iteration 96550 Training loss 0.05337625741958618 Validation loss 0.05315029248595238 Accuracy 0.455810546875\n",
      "Iteration 96560 Training loss 0.05027999356389046 Validation loss 0.05286036431789398 Accuracy 0.459716796875\n",
      "Iteration 96570 Training loss 0.05016690865159035 Validation loss 0.052838992327451706 Accuracy 0.4609375\n",
      "Iteration 96580 Training loss 0.04814520105719566 Validation loss 0.05254647508263588 Accuracy 0.46142578125\n",
      "Iteration 96590 Training loss 0.049094218760728836 Validation loss 0.052920691668987274 Accuracy 0.4609375\n",
      "Iteration 96600 Training loss 0.04825138300657272 Validation loss 0.05269291251897812 Accuracy 0.460693359375\n",
      "Iteration 96610 Training loss 0.0512879341840744 Validation loss 0.0530577078461647 Accuracy 0.460693359375\n",
      "Iteration 96620 Training loss 0.05355580151081085 Validation loss 0.05310957506299019 Accuracy 0.458984375\n",
      "Iteration 96630 Training loss 0.05119960382580757 Validation loss 0.05294473096728325 Accuracy 0.46044921875\n",
      "Iteration 96640 Training loss 0.050519511103630066 Validation loss 0.05350891128182411 Accuracy 0.455078125\n",
      "Iteration 96650 Training loss 0.05010049045085907 Validation loss 0.0531872995197773 Accuracy 0.457275390625\n",
      "Iteration 96660 Training loss 0.048304736614227295 Validation loss 0.0534554049372673 Accuracy 0.45654296875\n",
      "Iteration 96670 Training loss 0.04770229011774063 Validation loss 0.05261008068919182 Accuracy 0.4619140625\n",
      "Iteration 96680 Training loss 0.05146760866045952 Validation loss 0.05277525261044502 Accuracy 0.46142578125\n",
      "Iteration 96690 Training loss 0.05028670281171799 Validation loss 0.0531277135014534 Accuracy 0.46044921875\n",
      "Iteration 96700 Training loss 0.04947822913527489 Validation loss 0.0524771586060524 Accuracy 0.463134765625\n",
      "Iteration 96710 Training loss 0.051123879849910736 Validation loss 0.05275946110486984 Accuracy 0.459228515625\n",
      "Iteration 96720 Training loss 0.05170679837465286 Validation loss 0.052732717245817184 Accuracy 0.461181640625\n",
      "Iteration 96730 Training loss 0.05198001116514206 Validation loss 0.053178004920482635 Accuracy 0.45654296875\n",
      "Iteration 96740 Training loss 0.047764088958501816 Validation loss 0.05251913517713547 Accuracy 0.4609375\n",
      "Iteration 96750 Training loss 0.050138335675001144 Validation loss 0.052843689918518066 Accuracy 0.460205078125\n",
      "Iteration 96760 Training loss 0.05150286853313446 Validation loss 0.05275965854525566 Accuracy 0.460693359375\n",
      "Iteration 96770 Training loss 0.04815901815891266 Validation loss 0.053227126598358154 Accuracy 0.45849609375\n",
      "Iteration 96780 Training loss 0.05251164734363556 Validation loss 0.053055379539728165 Accuracy 0.458251953125\n",
      "Iteration 96790 Training loss 0.04899930953979492 Validation loss 0.05271047353744507 Accuracy 0.46240234375\n",
      "Iteration 96800 Training loss 0.05133386701345444 Validation loss 0.052940208464860916 Accuracy 0.45947265625\n",
      "Iteration 96810 Training loss 0.05392182990908623 Validation loss 0.05289183557033539 Accuracy 0.4599609375\n",
      "Iteration 96820 Training loss 0.04951457679271698 Validation loss 0.05258959159255028 Accuracy 0.462646484375\n",
      "Iteration 96830 Training loss 0.051462095230817795 Validation loss 0.052778664976358414 Accuracy 0.4599609375\n",
      "Iteration 96840 Training loss 0.04847327247262001 Validation loss 0.05288867652416229 Accuracy 0.46044921875\n",
      "Iteration 96850 Training loss 0.04805166646838188 Validation loss 0.05317581072449684 Accuracy 0.456298828125\n",
      "Iteration 96860 Training loss 0.05129831284284592 Validation loss 0.0529337152838707 Accuracy 0.4619140625\n",
      "Iteration 96870 Training loss 0.05025995895266533 Validation loss 0.05301831290125847 Accuracy 0.460205078125\n",
      "Iteration 96880 Training loss 0.051364049315452576 Validation loss 0.053022682666778564 Accuracy 0.4609375\n",
      "Iteration 96890 Training loss 0.047943126410245895 Validation loss 0.05267912149429321 Accuracy 0.460693359375\n",
      "Iteration 96900 Training loss 0.053333830088377 Validation loss 0.05309625342488289 Accuracy 0.458984375\n",
      "Iteration 96910 Training loss 0.051365483552217484 Validation loss 0.05270254611968994 Accuracy 0.462646484375\n",
      "Iteration 96920 Training loss 0.049256086349487305 Validation loss 0.05266854912042618 Accuracy 0.4619140625\n",
      "Iteration 96930 Training loss 0.050068143755197525 Validation loss 0.052820149809122086 Accuracy 0.459228515625\n",
      "Iteration 96940 Training loss 0.05090823397040367 Validation loss 0.05319022759795189 Accuracy 0.45849609375\n",
      "Iteration 96950 Training loss 0.04912488907575607 Validation loss 0.053156159818172455 Accuracy 0.45947265625\n",
      "Iteration 96960 Training loss 0.04953540116548538 Validation loss 0.052516087889671326 Accuracy 0.4619140625\n",
      "Iteration 96970 Training loss 0.0501830019056797 Validation loss 0.052935000509023666 Accuracy 0.459716796875\n",
      "Iteration 96980 Training loss 0.048970192670822144 Validation loss 0.05265093594789505 Accuracy 0.461181640625\n",
      "Iteration 96990 Training loss 0.05122045800089836 Validation loss 0.05268006771802902 Accuracy 0.462158203125\n",
      "Iteration 97000 Training loss 0.04743284732103348 Validation loss 0.05251640826463699 Accuracy 0.462158203125\n",
      "Iteration 97010 Training loss 0.050377704203128815 Validation loss 0.052486877888441086 Accuracy 0.462646484375\n",
      "Iteration 97020 Training loss 0.047539763152599335 Validation loss 0.05242956057190895 Accuracy 0.462646484375\n",
      "Iteration 97030 Training loss 0.050094686448574066 Validation loss 0.05274403095245361 Accuracy 0.46142578125\n",
      "Iteration 97040 Training loss 0.05104707181453705 Validation loss 0.053366631269454956 Accuracy 0.45947265625\n",
      "Iteration 97050 Training loss 0.05137133598327637 Validation loss 0.05246562883257866 Accuracy 0.462646484375\n",
      "Iteration 97060 Training loss 0.04677850380539894 Validation loss 0.05267331004142761 Accuracy 0.461181640625\n",
      "Iteration 97070 Training loss 0.051661692559719086 Validation loss 0.052801553159952164 Accuracy 0.461181640625\n",
      "Iteration 97080 Training loss 0.05045589059591293 Validation loss 0.05304555594921112 Accuracy 0.458984375\n",
      "Iteration 97090 Training loss 0.05118035897612572 Validation loss 0.053289804607629776 Accuracy 0.458984375\n",
      "Iteration 97100 Training loss 0.048683907836675644 Validation loss 0.05299631878733635 Accuracy 0.460205078125\n",
      "Iteration 97110 Training loss 0.05129136145114899 Validation loss 0.05338411033153534 Accuracy 0.4580078125\n",
      "Iteration 97120 Training loss 0.04953639581799507 Validation loss 0.052544597536325455 Accuracy 0.461669921875\n",
      "Iteration 97130 Training loss 0.049684084951877594 Validation loss 0.05326728895306587 Accuracy 0.45751953125\n",
      "Iteration 97140 Training loss 0.05143028870224953 Validation loss 0.05269581824541092 Accuracy 0.45947265625\n",
      "Iteration 97150 Training loss 0.04899249225854874 Validation loss 0.05260919779539108 Accuracy 0.462158203125\n",
      "Iteration 97160 Training loss 0.05264399200677872 Validation loss 0.0528499111533165 Accuracy 0.460693359375\n",
      "Iteration 97170 Training loss 0.05001052841544151 Validation loss 0.05310439318418503 Accuracy 0.458251953125\n",
      "Iteration 97180 Training loss 0.05211236700415611 Validation loss 0.053064215928316116 Accuracy 0.459228515625\n",
      "Iteration 97190 Training loss 0.05037599802017212 Validation loss 0.05279623344540596 Accuracy 0.460205078125\n",
      "Iteration 97200 Training loss 0.05197441205382347 Validation loss 0.052741166204214096 Accuracy 0.460693359375\n",
      "Iteration 97210 Training loss 0.05289875343441963 Validation loss 0.05250569060444832 Accuracy 0.4619140625\n",
      "Iteration 97220 Training loss 0.04891591519117355 Validation loss 0.053404852747917175 Accuracy 0.455322265625\n",
      "Iteration 97230 Training loss 0.04889772832393646 Validation loss 0.05304146185517311 Accuracy 0.46044921875\n",
      "Iteration 97240 Training loss 0.05135837569832802 Validation loss 0.0528816394507885 Accuracy 0.46044921875\n",
      "Iteration 97250 Training loss 0.04905278608202934 Validation loss 0.05307360738515854 Accuracy 0.459716796875\n",
      "Iteration 97260 Training loss 0.05282912030816078 Validation loss 0.0529525950551033 Accuracy 0.460205078125\n",
      "Iteration 97270 Training loss 0.049881335347890854 Validation loss 0.053098708391189575 Accuracy 0.4599609375\n",
      "Iteration 97280 Training loss 0.05189497768878937 Validation loss 0.052710797637701035 Accuracy 0.460693359375\n",
      "Iteration 97290 Training loss 0.04801523685455322 Validation loss 0.05285665765404701 Accuracy 0.458984375\n",
      "Iteration 97300 Training loss 0.04767827317118645 Validation loss 0.05272235721349716 Accuracy 0.459716796875\n",
      "Iteration 97310 Training loss 0.05145025625824928 Validation loss 0.05338669940829277 Accuracy 0.45654296875\n",
      "Iteration 97320 Training loss 0.05267271026968956 Validation loss 0.05267886444926262 Accuracy 0.461181640625\n",
      "Iteration 97330 Training loss 0.050709690898656845 Validation loss 0.052889421582221985 Accuracy 0.4609375\n",
      "Iteration 97340 Training loss 0.054084792733192444 Validation loss 0.052612099796533585 Accuracy 0.461181640625\n",
      "Iteration 97350 Training loss 0.04889988526701927 Validation loss 0.052559807896614075 Accuracy 0.460205078125\n",
      "Iteration 97360 Training loss 0.0490250438451767 Validation loss 0.05285006761550903 Accuracy 0.459228515625\n",
      "Iteration 97370 Training loss 0.04869275912642479 Validation loss 0.05283592641353607 Accuracy 0.461669921875\n",
      "Iteration 97380 Training loss 0.050639841705560684 Validation loss 0.05308401584625244 Accuracy 0.45751953125\n",
      "Iteration 97390 Training loss 0.05106440186500549 Validation loss 0.05288507789373398 Accuracy 0.460205078125\n",
      "Iteration 97400 Training loss 0.05198752135038376 Validation loss 0.05290956795215607 Accuracy 0.459228515625\n",
      "Iteration 97410 Training loss 0.04902823269367218 Validation loss 0.05263279005885124 Accuracy 0.4609375\n",
      "Iteration 97420 Training loss 0.05919082462787628 Validation loss 0.05399776250123978 Accuracy 0.451171875\n",
      "Iteration 97430 Training loss 0.051966343075037 Validation loss 0.05303409695625305 Accuracy 0.45654296875\n",
      "Iteration 97440 Training loss 0.050459492951631546 Validation loss 0.05279017612338066 Accuracy 0.45947265625\n",
      "Iteration 97450 Training loss 0.04942838475108147 Validation loss 0.052852436900138855 Accuracy 0.45947265625\n",
      "Iteration 97460 Training loss 0.05149297043681145 Validation loss 0.052656855434179306 Accuracy 0.460693359375\n",
      "Iteration 97470 Training loss 0.04936488717794418 Validation loss 0.05244871601462364 Accuracy 0.4619140625\n",
      "Iteration 97480 Training loss 0.04988820478320122 Validation loss 0.05277330055832863 Accuracy 0.460693359375\n",
      "Iteration 97490 Training loss 0.04920969158411026 Validation loss 0.05349099636077881 Accuracy 0.455078125\n",
      "Iteration 97500 Training loss 0.05097411945462227 Validation loss 0.05284367874264717 Accuracy 0.462158203125\n",
      "Iteration 97510 Training loss 0.05063413456082344 Validation loss 0.05271404981613159 Accuracy 0.460693359375\n",
      "Iteration 97520 Training loss 0.0500270240008831 Validation loss 0.05256476253271103 Accuracy 0.4619140625\n",
      "Iteration 97530 Training loss 0.050340570509433746 Validation loss 0.05297965556383133 Accuracy 0.459716796875\n",
      "Iteration 97540 Training loss 0.04988384619355202 Validation loss 0.05263861268758774 Accuracy 0.460693359375\n",
      "Iteration 97550 Training loss 0.049921665340662 Validation loss 0.05314192175865173 Accuracy 0.459716796875\n",
      "Iteration 97560 Training loss 0.05174398794770241 Validation loss 0.05301991105079651 Accuracy 0.458251953125\n",
      "Iteration 97570 Training loss 0.04972844570875168 Validation loss 0.05318004637956619 Accuracy 0.45849609375\n",
      "Iteration 97580 Training loss 0.04969105124473572 Validation loss 0.053344421088695526 Accuracy 0.459228515625\n",
      "Iteration 97590 Training loss 0.05012218654155731 Validation loss 0.05277002975344658 Accuracy 0.460205078125\n",
      "Iteration 97600 Training loss 0.05111657828092575 Validation loss 0.053058698773384094 Accuracy 0.459228515625\n",
      "Iteration 97610 Training loss 0.04727470502257347 Validation loss 0.05285140499472618 Accuracy 0.461669921875\n",
      "Iteration 97620 Training loss 0.049224335700273514 Validation loss 0.05256103351712227 Accuracy 0.46142578125\n",
      "Iteration 97630 Training loss 0.05080017074942589 Validation loss 0.052930522710084915 Accuracy 0.46142578125\n",
      "Iteration 97640 Training loss 0.05189775675535202 Validation loss 0.052897900342941284 Accuracy 0.460693359375\n",
      "Iteration 97650 Training loss 0.052947912365198135 Validation loss 0.05262115225195885 Accuracy 0.460693359375\n",
      "Iteration 97660 Training loss 0.048346586525440216 Validation loss 0.05295819044113159 Accuracy 0.45947265625\n",
      "Iteration 97670 Training loss 0.0538274310529232 Validation loss 0.052456311881542206 Accuracy 0.4619140625\n",
      "Iteration 97680 Training loss 0.05165810137987137 Validation loss 0.05288346856832504 Accuracy 0.459716796875\n",
      "Iteration 97690 Training loss 0.051686473190784454 Validation loss 0.05331483110785484 Accuracy 0.45849609375\n",
      "Iteration 97700 Training loss 0.04687466472387314 Validation loss 0.05301356315612793 Accuracy 0.459716796875\n",
      "Iteration 97710 Training loss 0.05134480819106102 Validation loss 0.05295446515083313 Accuracy 0.45947265625\n",
      "Iteration 97720 Training loss 0.05075211077928543 Validation loss 0.05317433550953865 Accuracy 0.459228515625\n",
      "Iteration 97730 Training loss 0.04822937026619911 Validation loss 0.05251210182905197 Accuracy 0.462158203125\n",
      "Iteration 97740 Training loss 0.04918089881539345 Validation loss 0.052642155438661575 Accuracy 0.4609375\n",
      "Iteration 97750 Training loss 0.047887083142995834 Validation loss 0.0525798425078392 Accuracy 0.46240234375\n",
      "Iteration 97760 Training loss 0.051727671176195145 Validation loss 0.052784524857997894 Accuracy 0.460693359375\n",
      "Iteration 97770 Training loss 0.048398684710264206 Validation loss 0.05307755619287491 Accuracy 0.459716796875\n",
      "Iteration 97780 Training loss 0.05114024877548218 Validation loss 0.05263557657599449 Accuracy 0.46240234375\n",
      "Iteration 97790 Training loss 0.0473550520837307 Validation loss 0.052610546350479126 Accuracy 0.462158203125\n",
      "Iteration 97800 Training loss 0.04988687485456467 Validation loss 0.05269100144505501 Accuracy 0.461181640625\n",
      "Iteration 97810 Training loss 0.05008206516504288 Validation loss 0.052448298782110214 Accuracy 0.461181640625\n",
      "Iteration 97820 Training loss 0.04896094650030136 Validation loss 0.052708689123392105 Accuracy 0.46142578125\n",
      "Iteration 97830 Training loss 0.051677457988262177 Validation loss 0.052673857659101486 Accuracy 0.4619140625\n",
      "Iteration 97840 Training loss 0.053152646869421005 Validation loss 0.05305822193622589 Accuracy 0.4580078125\n",
      "Iteration 97850 Training loss 0.051416005939245224 Validation loss 0.052938226610422134 Accuracy 0.460693359375\n",
      "Iteration 97860 Training loss 0.05499274656176567 Validation loss 0.05312946066260338 Accuracy 0.460693359375\n",
      "Iteration 97870 Training loss 0.050412800163030624 Validation loss 0.05250955745577812 Accuracy 0.4619140625\n",
      "Iteration 97880 Training loss 0.05374731868505478 Validation loss 0.0529521219432354 Accuracy 0.4609375\n",
      "Iteration 97890 Training loss 0.049724455922842026 Validation loss 0.05279164761304855 Accuracy 0.4599609375\n",
      "Iteration 97900 Training loss 0.04760026931762695 Validation loss 0.05288200080394745 Accuracy 0.460693359375\n",
      "Iteration 97910 Training loss 0.052885640412569046 Validation loss 0.052462633699178696 Accuracy 0.463134765625\n",
      "Iteration 97920 Training loss 0.04976516589522362 Validation loss 0.052748698741197586 Accuracy 0.462158203125\n",
      "Iteration 97930 Training loss 0.05124170705676079 Validation loss 0.052856381982564926 Accuracy 0.46044921875\n",
      "Iteration 97940 Training loss 0.05070462077856064 Validation loss 0.05256107822060585 Accuracy 0.4619140625\n",
      "Iteration 97950 Training loss 0.04981706663966179 Validation loss 0.05249255895614624 Accuracy 0.462158203125\n",
      "Iteration 97960 Training loss 0.04860104247927666 Validation loss 0.053467489778995514 Accuracy 0.458740234375\n",
      "Iteration 97970 Training loss 0.0496605820953846 Validation loss 0.052585065364837646 Accuracy 0.4619140625\n",
      "Iteration 97980 Training loss 0.0489211231470108 Validation loss 0.05291875824332237 Accuracy 0.460693359375\n",
      "Iteration 97990 Training loss 0.05153883993625641 Validation loss 0.05307759717106819 Accuracy 0.458740234375\n",
      "Iteration 98000 Training loss 0.04936479777097702 Validation loss 0.0528220497071743 Accuracy 0.460693359375\n",
      "Iteration 98010 Training loss 0.05023808777332306 Validation loss 0.05276628956198692 Accuracy 0.4619140625\n",
      "Iteration 98020 Training loss 0.048537638038396835 Validation loss 0.052928827702999115 Accuracy 0.460205078125\n",
      "Iteration 98030 Training loss 0.050808995962142944 Validation loss 0.05267376825213432 Accuracy 0.459228515625\n",
      "Iteration 98040 Training loss 0.05223434045910835 Validation loss 0.05270972475409508 Accuracy 0.4609375\n",
      "Iteration 98050 Training loss 0.05017847567796707 Validation loss 0.05278906226158142 Accuracy 0.460693359375\n",
      "Iteration 98060 Training loss 0.04886909946799278 Validation loss 0.05303923785686493 Accuracy 0.4599609375\n",
      "Iteration 98070 Training loss 0.05003432184457779 Validation loss 0.05261605232954025 Accuracy 0.46142578125\n",
      "Iteration 98080 Training loss 0.04953593760728836 Validation loss 0.05272575095295906 Accuracy 0.460693359375\n",
      "Iteration 98090 Training loss 0.046829596161842346 Validation loss 0.05323826149106026 Accuracy 0.45751953125\n",
      "Iteration 98100 Training loss 0.05001738294959068 Validation loss 0.05316371098160744 Accuracy 0.45947265625\n",
      "Iteration 98110 Training loss 0.049090418964624405 Validation loss 0.05320620536804199 Accuracy 0.458984375\n",
      "Iteration 98120 Training loss 0.05129120871424675 Validation loss 0.05266875773668289 Accuracy 0.462158203125\n",
      "Iteration 98130 Training loss 0.05657797306776047 Validation loss 0.05282184109091759 Accuracy 0.460693359375\n",
      "Iteration 98140 Training loss 0.054667022079229355 Validation loss 0.05275064334273338 Accuracy 0.461181640625\n",
      "Iteration 98150 Training loss 0.05181035399436951 Validation loss 0.05318635702133179 Accuracy 0.4580078125\n",
      "Iteration 98160 Training loss 0.05037348344922066 Validation loss 0.0525481253862381 Accuracy 0.461669921875\n",
      "Iteration 98170 Training loss 0.05156958848237991 Validation loss 0.052671484649181366 Accuracy 0.462646484375\n",
      "Iteration 98180 Training loss 0.051619261503219604 Validation loss 0.053148191422224045 Accuracy 0.45849609375\n",
      "Iteration 98190 Training loss 0.047596968710422516 Validation loss 0.052606433629989624 Accuracy 0.4609375\n",
      "Iteration 98200 Training loss 0.049420326948165894 Validation loss 0.05242111161351204 Accuracy 0.46240234375\n",
      "Iteration 98210 Training loss 0.05416586250066757 Validation loss 0.05295488238334656 Accuracy 0.4599609375\n",
      "Iteration 98220 Training loss 0.05137968808412552 Validation loss 0.05298690125346184 Accuracy 0.458984375\n",
      "Iteration 98230 Training loss 0.05066913738846779 Validation loss 0.052880287170410156 Accuracy 0.458251953125\n",
      "Iteration 98240 Training loss 0.04729931429028511 Validation loss 0.053008973598480225 Accuracy 0.460205078125\n",
      "Iteration 98250 Training loss 0.05357304960489273 Validation loss 0.05257536843419075 Accuracy 0.460693359375\n",
      "Iteration 98260 Training loss 0.04977897182106972 Validation loss 0.05326380580663681 Accuracy 0.45751953125\n",
      "Iteration 98270 Training loss 0.05141565948724747 Validation loss 0.052691854536533356 Accuracy 0.4619140625\n",
      "Iteration 98280 Training loss 0.051145218312740326 Validation loss 0.05298008769750595 Accuracy 0.461669921875\n",
      "Iteration 98290 Training loss 0.05006188899278641 Validation loss 0.05274784192442894 Accuracy 0.462158203125\n",
      "Iteration 98300 Training loss 0.05082552880048752 Validation loss 0.05295773223042488 Accuracy 0.4609375\n",
      "Iteration 98310 Training loss 0.04680758714675903 Validation loss 0.05246099457144737 Accuracy 0.46240234375\n",
      "Iteration 98320 Training loss 0.052464574575424194 Validation loss 0.05349109321832657 Accuracy 0.45556640625\n",
      "Iteration 98330 Training loss 0.046652667224407196 Validation loss 0.05269778519868851 Accuracy 0.45849609375\n",
      "Iteration 98340 Training loss 0.046954162418842316 Validation loss 0.05271335318684578 Accuracy 0.4609375\n",
      "Iteration 98350 Training loss 0.04836076870560646 Validation loss 0.052545614540576935 Accuracy 0.461181640625\n",
      "Iteration 98360 Training loss 0.04720271751284599 Validation loss 0.05269552767276764 Accuracy 0.460205078125\n",
      "Iteration 98370 Training loss 0.049683552235364914 Validation loss 0.05295196548104286 Accuracy 0.4609375\n",
      "Iteration 98380 Training loss 0.04978521540760994 Validation loss 0.052769094705581665 Accuracy 0.461181640625\n",
      "Iteration 98390 Training loss 0.046757590025663376 Validation loss 0.05255916714668274 Accuracy 0.4619140625\n",
      "Iteration 98400 Training loss 0.04771909862756729 Validation loss 0.052340202033519745 Accuracy 0.462646484375\n",
      "Iteration 98410 Training loss 0.05232277512550354 Validation loss 0.05256206914782524 Accuracy 0.461181640625\n",
      "Iteration 98420 Training loss 0.05145495384931564 Validation loss 0.05362619087100029 Accuracy 0.45263671875\n",
      "Iteration 98430 Training loss 0.04920891672372818 Validation loss 0.05287153273820877 Accuracy 0.4580078125\n",
      "Iteration 98440 Training loss 0.04822135344147682 Validation loss 0.05310538783669472 Accuracy 0.459228515625\n",
      "Iteration 98450 Training loss 0.05065755173563957 Validation loss 0.0533783957362175 Accuracy 0.45654296875\n",
      "Iteration 98460 Training loss 0.0507027693092823 Validation loss 0.05292153358459473 Accuracy 0.461669921875\n",
      "Iteration 98470 Training loss 0.052299030125141144 Validation loss 0.05285771191120148 Accuracy 0.461181640625\n",
      "Iteration 98480 Training loss 0.04801385477185249 Validation loss 0.052659761160612106 Accuracy 0.460205078125\n",
      "Iteration 98490 Training loss 0.04760970175266266 Validation loss 0.052263494580984116 Accuracy 0.46337890625\n",
      "Iteration 98500 Training loss 0.05004625394940376 Validation loss 0.053168997168540955 Accuracy 0.45849609375\n",
      "Iteration 98510 Training loss 0.05137084051966667 Validation loss 0.0526028610765934 Accuracy 0.461669921875\n",
      "Iteration 98520 Training loss 0.05165573209524155 Validation loss 0.052656471729278564 Accuracy 0.4609375\n",
      "Iteration 98530 Training loss 0.05128050222992897 Validation loss 0.05283935368061066 Accuracy 0.461669921875\n",
      "Iteration 98540 Training loss 0.05120299383997917 Validation loss 0.05239395052194595 Accuracy 0.46240234375\n",
      "Iteration 98550 Training loss 0.05199258774518967 Validation loss 0.053362734615802765 Accuracy 0.458740234375\n",
      "Iteration 98560 Training loss 0.04627586156129837 Validation loss 0.05270810425281525 Accuracy 0.4599609375\n",
      "Iteration 98570 Training loss 0.04952951893210411 Validation loss 0.05271469056606293 Accuracy 0.462158203125\n",
      "Iteration 98580 Training loss 0.0525982566177845 Validation loss 0.052830468863248825 Accuracy 0.46142578125\n",
      "Iteration 98590 Training loss 0.05232799053192139 Validation loss 0.052565138787031174 Accuracy 0.4619140625\n",
      "Iteration 98600 Training loss 0.04988212138414383 Validation loss 0.05279051512479782 Accuracy 0.460205078125\n",
      "Iteration 98610 Training loss 0.051207538694143295 Validation loss 0.052796728909015656 Accuracy 0.460693359375\n",
      "Iteration 98620 Training loss 0.052357807755470276 Validation loss 0.053535595536231995 Accuracy 0.455322265625\n",
      "Iteration 98630 Training loss 0.04827776178717613 Validation loss 0.052664030343294144 Accuracy 0.4609375\n",
      "Iteration 98640 Training loss 0.04791060462594032 Validation loss 0.05285819247364998 Accuracy 0.4599609375\n",
      "Iteration 98650 Training loss 0.05322236567735672 Validation loss 0.052603770047426224 Accuracy 0.4599609375\n",
      "Iteration 98660 Training loss 0.050418607890605927 Validation loss 0.05312695354223251 Accuracy 0.458251953125\n",
      "Iteration 98670 Training loss 0.05118413269519806 Validation loss 0.05250174179673195 Accuracy 0.4609375\n",
      "Iteration 98680 Training loss 0.049787454307079315 Validation loss 0.05303703993558884 Accuracy 0.458984375\n",
      "Iteration 98690 Training loss 0.04886182025074959 Validation loss 0.05256185308098793 Accuracy 0.461669921875\n",
      "Iteration 98700 Training loss 0.051924798637628555 Validation loss 0.05290945991873741 Accuracy 0.46044921875\n",
      "Iteration 98710 Training loss 0.047108013182878494 Validation loss 0.05279497057199478 Accuracy 0.46142578125\n",
      "Iteration 98720 Training loss 0.04889446869492531 Validation loss 0.05275281146168709 Accuracy 0.4599609375\n",
      "Iteration 98730 Training loss 0.050951212644577026 Validation loss 0.05299944058060646 Accuracy 0.460693359375\n",
      "Iteration 98740 Training loss 0.046944182366132736 Validation loss 0.0531817264854908 Accuracy 0.45751953125\n",
      "Iteration 98750 Training loss 0.054512858390808105 Validation loss 0.05301827937364578 Accuracy 0.459716796875\n",
      "Iteration 98760 Training loss 0.0529370978474617 Validation loss 0.053074911236763 Accuracy 0.456787109375\n",
      "Iteration 98770 Training loss 0.0508861280977726 Validation loss 0.052599191665649414 Accuracy 0.4609375\n",
      "Iteration 98780 Training loss 0.049587640911340714 Validation loss 0.05284722521901131 Accuracy 0.4619140625\n",
      "Iteration 98790 Training loss 0.05378250032663345 Validation loss 0.05338241904973984 Accuracy 0.456787109375\n",
      "Iteration 98800 Training loss 0.04778880625963211 Validation loss 0.05261719599366188 Accuracy 0.460205078125\n",
      "Iteration 98810 Training loss 0.050882142037153244 Validation loss 0.05297905206680298 Accuracy 0.4580078125\n",
      "Iteration 98820 Training loss 0.048893172293901443 Validation loss 0.05289727449417114 Accuracy 0.458984375\n",
      "Iteration 98830 Training loss 0.04966672137379646 Validation loss 0.05274160951375961 Accuracy 0.4609375\n",
      "Iteration 98840 Training loss 0.0517142191529274 Validation loss 0.05358774587512016 Accuracy 0.456298828125\n",
      "Iteration 98850 Training loss 0.05277995765209198 Validation loss 0.05273263156414032 Accuracy 0.46044921875\n",
      "Iteration 98860 Training loss 0.04934491962194443 Validation loss 0.05305870622396469 Accuracy 0.457275390625\n",
      "Iteration 98870 Training loss 0.049169983714818954 Validation loss 0.05263713374733925 Accuracy 0.461181640625\n",
      "Iteration 98880 Training loss 0.05412213131785393 Validation loss 0.05401962623000145 Accuracy 0.44970703125\n",
      "Iteration 98890 Training loss 0.05194225534796715 Validation loss 0.053068142384290695 Accuracy 0.45947265625\n",
      "Iteration 98900 Training loss 0.05151728168129921 Validation loss 0.052684567868709564 Accuracy 0.458740234375\n",
      "Iteration 98910 Training loss 0.05168106034398079 Validation loss 0.052879221737384796 Accuracy 0.460693359375\n",
      "Iteration 98920 Training loss 0.048703838139772415 Validation loss 0.05285131558775902 Accuracy 0.46044921875\n",
      "Iteration 98930 Training loss 0.04929027333855629 Validation loss 0.05301349610090256 Accuracy 0.458984375\n",
      "Iteration 98940 Training loss 0.05065557733178139 Validation loss 0.052897434681653976 Accuracy 0.459716796875\n",
      "Iteration 98950 Training loss 0.050965625792741776 Validation loss 0.05304398760199547 Accuracy 0.459228515625\n",
      "Iteration 98960 Training loss 0.05045534297823906 Validation loss 0.05290335789322853 Accuracy 0.45947265625\n",
      "Iteration 98970 Training loss 0.04951906204223633 Validation loss 0.053305596113204956 Accuracy 0.458251953125\n",
      "Iteration 98980 Training loss 0.05099242180585861 Validation loss 0.053158294409513474 Accuracy 0.458251953125\n",
      "Iteration 98990 Training loss 0.051484644412994385 Validation loss 0.052808232605457306 Accuracy 0.459716796875\n",
      "Iteration 99000 Training loss 0.05088994279503822 Validation loss 0.05307161062955856 Accuracy 0.459716796875\n",
      "Iteration 99010 Training loss 0.04534400254487991 Validation loss 0.05279000848531723 Accuracy 0.460693359375\n",
      "Iteration 99020 Training loss 0.050648391246795654 Validation loss 0.052824363112449646 Accuracy 0.46142578125\n",
      "Iteration 99030 Training loss 0.05212925001978874 Validation loss 0.05336679145693779 Accuracy 0.458740234375\n",
      "Iteration 99040 Training loss 0.051518138498067856 Validation loss 0.052559010684490204 Accuracy 0.461669921875\n",
      "Iteration 99050 Training loss 0.05329158529639244 Validation loss 0.052712682634592056 Accuracy 0.46044921875\n",
      "Iteration 99060 Training loss 0.05124557390809059 Validation loss 0.05272293835878372 Accuracy 0.460693359375\n",
      "Iteration 99070 Training loss 0.05140111222863197 Validation loss 0.053261805325746536 Accuracy 0.458984375\n",
      "Iteration 99080 Training loss 0.04988499730825424 Validation loss 0.053466834127902985 Accuracy 0.456787109375\n",
      "Iteration 99090 Training loss 0.04836108162999153 Validation loss 0.0528438575565815 Accuracy 0.45947265625\n",
      "Iteration 99100 Training loss 0.050618138164281845 Validation loss 0.052871741354465485 Accuracy 0.4580078125\n",
      "Iteration 99110 Training loss 0.049630288034677505 Validation loss 0.05250903591513634 Accuracy 0.461669921875\n",
      "Iteration 99120 Training loss 0.05002192035317421 Validation loss 0.05245888605713844 Accuracy 0.46142578125\n",
      "Iteration 99130 Training loss 0.05177890136837959 Validation loss 0.05253593623638153 Accuracy 0.461181640625\n",
      "Iteration 99140 Training loss 0.04706911742687225 Validation loss 0.05290752649307251 Accuracy 0.458251953125\n",
      "Iteration 99150 Training loss 0.05152926594018936 Validation loss 0.05307614058256149 Accuracy 0.458984375\n",
      "Iteration 99160 Training loss 0.046722620725631714 Validation loss 0.05285808444023132 Accuracy 0.460693359375\n",
      "Iteration 99170 Training loss 0.05284646525979042 Validation loss 0.05338219553232193 Accuracy 0.456787109375\n",
      "Iteration 99180 Training loss 0.050610341131687164 Validation loss 0.05326421558856964 Accuracy 0.45703125\n",
      "Iteration 99190 Training loss 0.04882737249135971 Validation loss 0.05316341295838356 Accuracy 0.45703125\n",
      "Iteration 99200 Training loss 0.051142171025276184 Validation loss 0.05311145260930061 Accuracy 0.4580078125\n",
      "Iteration 99210 Training loss 0.05125029385089874 Validation loss 0.053058139979839325 Accuracy 0.460205078125\n",
      "Iteration 99220 Training loss 0.050748810172080994 Validation loss 0.0529988557100296 Accuracy 0.45947265625\n",
      "Iteration 99230 Training loss 0.052697956562042236 Validation loss 0.053380087018013 Accuracy 0.45458984375\n",
      "Iteration 99240 Training loss 0.04783328250050545 Validation loss 0.05299445614218712 Accuracy 0.45849609375\n",
      "Iteration 99250 Training loss 0.053136177361011505 Validation loss 0.053609609603881836 Accuracy 0.452392578125\n",
      "Iteration 99260 Training loss 0.04875429347157478 Validation loss 0.05278291925787926 Accuracy 0.460693359375\n",
      "Iteration 99270 Training loss 0.051989469677209854 Validation loss 0.052764296531677246 Accuracy 0.4599609375\n",
      "Iteration 99280 Training loss 0.05360865592956543 Validation loss 0.0525532104074955 Accuracy 0.461669921875\n",
      "Iteration 99290 Training loss 0.049896202981472015 Validation loss 0.05258785933256149 Accuracy 0.460693359375\n",
      "Iteration 99300 Training loss 0.049661893397569656 Validation loss 0.052582453936338425 Accuracy 0.461669921875\n",
      "Iteration 99310 Training loss 0.0485648587346077 Validation loss 0.05275207385420799 Accuracy 0.460693359375\n",
      "Iteration 99320 Training loss 0.05120277777314186 Validation loss 0.05258171260356903 Accuracy 0.461669921875\n",
      "Iteration 99330 Training loss 0.05104580521583557 Validation loss 0.05287390574812889 Accuracy 0.458984375\n",
      "Iteration 99340 Training loss 0.05349695309996605 Validation loss 0.052659228444099426 Accuracy 0.4619140625\n",
      "Iteration 99350 Training loss 0.050409622490406036 Validation loss 0.053402021527290344 Accuracy 0.45556640625\n",
      "Iteration 99360 Training loss 0.048615701496601105 Validation loss 0.052702646702528 Accuracy 0.459716796875\n",
      "Iteration 99370 Training loss 0.05372873321175575 Validation loss 0.05262541398406029 Accuracy 0.460205078125\n",
      "Iteration 99380 Training loss 0.05119694024324417 Validation loss 0.05340796336531639 Accuracy 0.456787109375\n",
      "Iteration 99390 Training loss 0.049804553389549255 Validation loss 0.053562626242637634 Accuracy 0.451904296875\n",
      "Iteration 99400 Training loss 0.05047411099076271 Validation loss 0.05260378122329712 Accuracy 0.46142578125\n",
      "Iteration 99410 Training loss 0.04862213134765625 Validation loss 0.0525539331138134 Accuracy 0.461669921875\n",
      "Iteration 99420 Training loss 0.04951687157154083 Validation loss 0.053599562495946884 Accuracy 0.45361328125\n",
      "Iteration 99430 Training loss 0.049093637615442276 Validation loss 0.05304151400923729 Accuracy 0.45947265625\n",
      "Iteration 99440 Training loss 0.049023374915122986 Validation loss 0.053076229989528656 Accuracy 0.459716796875\n",
      "Iteration 99450 Training loss 0.047445595264434814 Validation loss 0.05270524322986603 Accuracy 0.462158203125\n",
      "Iteration 99460 Training loss 0.054800812155008316 Validation loss 0.05296396464109421 Accuracy 0.461181640625\n",
      "Iteration 99470 Training loss 0.051787443459033966 Validation loss 0.05286039039492607 Accuracy 0.460693359375\n",
      "Iteration 99480 Training loss 0.05101917311549187 Validation loss 0.053057119250297546 Accuracy 0.460205078125\n",
      "Iteration 99490 Training loss 0.04666762053966522 Validation loss 0.052550703287124634 Accuracy 0.46142578125\n",
      "Iteration 99500 Training loss 0.04956591874361038 Validation loss 0.0529366098344326 Accuracy 0.4609375\n",
      "Iteration 99510 Training loss 0.04872335121035576 Validation loss 0.05287296697497368 Accuracy 0.46044921875\n",
      "Iteration 99520 Training loss 0.0528997965157032 Validation loss 0.05318235233426094 Accuracy 0.458251953125\n",
      "Iteration 99530 Training loss 0.04993478208780289 Validation loss 0.05270339548587799 Accuracy 0.460205078125\n",
      "Iteration 99540 Training loss 0.05083254352211952 Validation loss 0.05255662649869919 Accuracy 0.46142578125\n",
      "Iteration 99550 Training loss 0.05126696079969406 Validation loss 0.05262685939669609 Accuracy 0.461181640625\n",
      "Iteration 99560 Training loss 0.04758830368518829 Validation loss 0.052932631224393845 Accuracy 0.460693359375\n",
      "Iteration 99570 Training loss 0.05447724461555481 Validation loss 0.052881356328725815 Accuracy 0.46044921875\n",
      "Iteration 99580 Training loss 0.053217172622680664 Validation loss 0.052692707628011703 Accuracy 0.461181640625\n",
      "Iteration 99590 Training loss 0.04786611348390579 Validation loss 0.05265970528125763 Accuracy 0.461669921875\n",
      "Iteration 99600 Training loss 0.05459912493824959 Validation loss 0.052738163620233536 Accuracy 0.460205078125\n",
      "Iteration 99610 Training loss 0.052747659385204315 Validation loss 0.05289251729846001 Accuracy 0.46044921875\n",
      "Iteration 99620 Training loss 0.04903367906808853 Validation loss 0.052602313458919525 Accuracy 0.4609375\n",
      "Iteration 99630 Training loss 0.05356184393167496 Validation loss 0.05274242162704468 Accuracy 0.46142578125\n",
      "Iteration 99640 Training loss 0.05116308480501175 Validation loss 0.05309883505105972 Accuracy 0.458740234375\n",
      "Iteration 99650 Training loss 0.04717937111854553 Validation loss 0.052906569093465805 Accuracy 0.460693359375\n",
      "Iteration 99660 Training loss 0.05210237205028534 Validation loss 0.05259087309241295 Accuracy 0.462890625\n",
      "Iteration 99670 Training loss 0.04844255372881889 Validation loss 0.05249391123652458 Accuracy 0.4619140625\n",
      "Iteration 99680 Training loss 0.052448708564043045 Validation loss 0.05272175744175911 Accuracy 0.46142578125\n",
      "Iteration 99690 Training loss 0.051300983875989914 Validation loss 0.052786462008953094 Accuracy 0.460693359375\n",
      "Iteration 99700 Training loss 0.052305225282907486 Validation loss 0.052931033074855804 Accuracy 0.459716796875\n",
      "Iteration 99710 Training loss 0.0480455607175827 Validation loss 0.0526682510972023 Accuracy 0.4619140625\n",
      "Iteration 99720 Training loss 0.04903959110379219 Validation loss 0.05249800533056259 Accuracy 0.462890625\n",
      "Iteration 99730 Training loss 0.04832962527871132 Validation loss 0.05318408086895943 Accuracy 0.45751953125\n",
      "Iteration 99740 Training loss 0.04587726667523384 Validation loss 0.05266508832573891 Accuracy 0.462646484375\n",
      "Iteration 99750 Training loss 0.05284546688199043 Validation loss 0.05254480242729187 Accuracy 0.4619140625\n",
      "Iteration 99760 Training loss 0.04936863109469414 Validation loss 0.05261730030179024 Accuracy 0.46142578125\n",
      "Iteration 99770 Training loss 0.05486495420336723 Validation loss 0.05321541801095009 Accuracy 0.4560546875\n",
      "Iteration 99780 Training loss 0.04937119409441948 Validation loss 0.05266548693180084 Accuracy 0.46044921875\n",
      "Iteration 99790 Training loss 0.04660428687930107 Validation loss 0.05315997824072838 Accuracy 0.4580078125\n",
      "Iteration 99800 Training loss 0.05009278655052185 Validation loss 0.05278565362095833 Accuracy 0.4609375\n",
      "Iteration 99810 Training loss 0.04944003373384476 Validation loss 0.052539627999067307 Accuracy 0.46142578125\n",
      "Iteration 99820 Training loss 0.05189111456274986 Validation loss 0.05299367010593414 Accuracy 0.459716796875\n",
      "Iteration 99830 Training loss 0.050350889563560486 Validation loss 0.05265897512435913 Accuracy 0.461181640625\n",
      "Iteration 99840 Training loss 0.049029987305402756 Validation loss 0.05288085713982582 Accuracy 0.458984375\n",
      "Iteration 99850 Training loss 0.04773685708642006 Validation loss 0.05287774279713631 Accuracy 0.460205078125\n",
      "Iteration 99860 Training loss 0.05020924657583237 Validation loss 0.05287829041481018 Accuracy 0.459716796875\n",
      "Iteration 99870 Training loss 0.05245084688067436 Validation loss 0.05277176573872566 Accuracy 0.459716796875\n",
      "Iteration 99880 Training loss 0.04973556473851204 Validation loss 0.05324115976691246 Accuracy 0.459716796875\n",
      "Iteration 99890 Training loss 0.052433013916015625 Validation loss 0.05314858257770538 Accuracy 0.4599609375\n",
      "Iteration 99900 Training loss 0.05050518736243248 Validation loss 0.05309906229376793 Accuracy 0.458251953125\n",
      "Iteration 99910 Training loss 0.04792853817343712 Validation loss 0.052543412894010544 Accuracy 0.4619140625\n",
      "Iteration 99920 Training loss 0.04985690116882324 Validation loss 0.052866559475660324 Accuracy 0.4609375\n",
      "Iteration 99930 Training loss 0.0477447509765625 Validation loss 0.052880242466926575 Accuracy 0.461669921875\n",
      "Iteration 99940 Training loss 0.047638971358537674 Validation loss 0.052622850984334946 Accuracy 0.4619140625\n",
      "Iteration 99950 Training loss 0.050141263753175735 Validation loss 0.05248044803738594 Accuracy 0.461669921875\n",
      "Iteration 99960 Training loss 0.04872488975524902 Validation loss 0.05311930924654007 Accuracy 0.459716796875\n",
      "Iteration 99970 Training loss 0.05037807673215866 Validation loss 0.0534498505294323 Accuracy 0.451904296875\n",
      "Iteration 99980 Training loss 0.04895474761724472 Validation loss 0.05250108242034912 Accuracy 0.46240234375\n",
      "Iteration 99990 Training loss 0.05144700035452843 Validation loss 0.052710723131895065 Accuracy 0.458984375\n",
      "Iteration 100000 Training loss 0.050891973078250885 Validation loss 0.052616726607084274 Accuracy 0.46240234375\n",
      "Iteration 100010 Training loss 0.04783209040760994 Validation loss 0.052564237266778946 Accuracy 0.4609375\n",
      "Iteration 100020 Training loss 0.046482279896736145 Validation loss 0.05287856236100197 Accuracy 0.4619140625\n",
      "Iteration 100030 Training loss 0.053637176752090454 Validation loss 0.05276201665401459 Accuracy 0.45947265625\n",
      "Iteration 100040 Training loss 0.04847443476319313 Validation loss 0.052660055458545685 Accuracy 0.461181640625\n",
      "Iteration 100050 Training loss 0.05066384747624397 Validation loss 0.052942387759685516 Accuracy 0.461181640625\n",
      "Iteration 100060 Training loss 0.0536913238465786 Validation loss 0.05247512087225914 Accuracy 0.462646484375\n",
      "Iteration 100070 Training loss 0.05358157306909561 Validation loss 0.05308812856674194 Accuracy 0.46044921875\n",
      "Iteration 100080 Training loss 0.049046438187360764 Validation loss 0.052513640373945236 Accuracy 0.462158203125\n",
      "Iteration 100090 Training loss 0.04958031326532364 Validation loss 0.05231260508298874 Accuracy 0.46240234375\n",
      "Iteration 100100 Training loss 0.051918793469667435 Validation loss 0.052531562745571136 Accuracy 0.4619140625\n",
      "Iteration 100110 Training loss 0.052143365144729614 Validation loss 0.05243814364075661 Accuracy 0.462890625\n",
      "Iteration 100120 Training loss 0.05207386240363121 Validation loss 0.05329161882400513 Accuracy 0.45849609375\n",
      "Iteration 100130 Training loss 0.04997701570391655 Validation loss 0.053348686546087265 Accuracy 0.457275390625\n",
      "Iteration 100140 Training loss 0.05099490284919739 Validation loss 0.052588894963264465 Accuracy 0.4619140625\n",
      "Iteration 100150 Training loss 0.051302552223205566 Validation loss 0.05244093015789986 Accuracy 0.46240234375\n",
      "Iteration 100160 Training loss 0.04907191917300224 Validation loss 0.05258147418498993 Accuracy 0.4619140625\n",
      "Iteration 100170 Training loss 0.053401459008455276 Validation loss 0.05305885151028633 Accuracy 0.458740234375\n",
      "Iteration 100180 Training loss 0.05193272978067398 Validation loss 0.0528874397277832 Accuracy 0.460693359375\n",
      "Iteration 100190 Training loss 0.050931304693222046 Validation loss 0.05246026813983917 Accuracy 0.462890625\n",
      "Iteration 100200 Training loss 0.04967387393116951 Validation loss 0.05247575417160988 Accuracy 0.464111328125\n",
      "Iteration 100210 Training loss 0.04982800409197807 Validation loss 0.05289468169212341 Accuracy 0.46142578125\n",
      "Iteration 100220 Training loss 0.047550495713949203 Validation loss 0.0525142177939415 Accuracy 0.461669921875\n",
      "Iteration 100230 Training loss 0.04955592378973961 Validation loss 0.052835751324892044 Accuracy 0.45947265625\n",
      "Iteration 100240 Training loss 0.05214159935712814 Validation loss 0.052704066038131714 Accuracy 0.4609375\n",
      "Iteration 100250 Training loss 0.050220031291246414 Validation loss 0.05305631086230278 Accuracy 0.4580078125\n",
      "Iteration 100260 Training loss 0.05140078812837601 Validation loss 0.052652113139629364 Accuracy 0.46044921875\n",
      "Iteration 100270 Training loss 0.04765688627958298 Validation loss 0.05285641551017761 Accuracy 0.461669921875\n",
      "Iteration 100280 Training loss 0.05138779804110527 Validation loss 0.05251585692167282 Accuracy 0.460205078125\n",
      "Iteration 100290 Training loss 0.05041441693902016 Validation loss 0.05263077840209007 Accuracy 0.462646484375\n",
      "Iteration 100300 Training loss 0.04629162698984146 Validation loss 0.05250370502471924 Accuracy 0.46240234375\n",
      "Iteration 100310 Training loss 0.0499330535531044 Validation loss 0.05235876515507698 Accuracy 0.463623046875\n",
      "Iteration 100320 Training loss 0.05251501500606537 Validation loss 0.053174443542957306 Accuracy 0.458740234375\n",
      "Iteration 100330 Training loss 0.04939998313784599 Validation loss 0.05272556468844414 Accuracy 0.460693359375\n",
      "Iteration 100340 Training loss 0.04598977044224739 Validation loss 0.052697885781526566 Accuracy 0.46142578125\n",
      "Iteration 100350 Training loss 0.04808223247528076 Validation loss 0.05257539078593254 Accuracy 0.4619140625\n",
      "Iteration 100360 Training loss 0.049033503979444504 Validation loss 0.052545517683029175 Accuracy 0.461669921875\n",
      "Iteration 100370 Training loss 0.050822582095861435 Validation loss 0.0528489351272583 Accuracy 0.461669921875\n",
      "Iteration 100380 Training loss 0.04845869541168213 Validation loss 0.05326806381344795 Accuracy 0.45703125\n",
      "Iteration 100390 Training loss 0.048222869634628296 Validation loss 0.05347118154168129 Accuracy 0.455810546875\n",
      "Iteration 100400 Training loss 0.048650674521923065 Validation loss 0.053624965250492096 Accuracy 0.45361328125\n",
      "Iteration 100410 Training loss 0.05223718285560608 Validation loss 0.052841730415821075 Accuracy 0.4609375\n",
      "Iteration 100420 Training loss 0.04828675463795662 Validation loss 0.05345100536942482 Accuracy 0.455322265625\n",
      "Iteration 100430 Training loss 0.052222996950149536 Validation loss 0.052856650203466415 Accuracy 0.461669921875\n",
      "Iteration 100440 Training loss 0.05217888206243515 Validation loss 0.05280239135026932 Accuracy 0.461669921875\n",
      "Iteration 100450 Training loss 0.05023616552352905 Validation loss 0.052532292902469635 Accuracy 0.462158203125\n",
      "Iteration 100460 Training loss 0.054721567779779434 Validation loss 0.05262824520468712 Accuracy 0.461669921875\n",
      "Iteration 100470 Training loss 0.04914594441652298 Validation loss 0.05269072204828262 Accuracy 0.46240234375\n",
      "Iteration 100480 Training loss 0.05083813518285751 Validation loss 0.05276654288172722 Accuracy 0.459228515625\n",
      "Iteration 100490 Training loss 0.053972695022821426 Validation loss 0.05254369601607323 Accuracy 0.46240234375\n",
      "Iteration 100500 Training loss 0.05097378417849541 Validation loss 0.05298817157745361 Accuracy 0.458740234375\n",
      "Iteration 100510 Training loss 0.05171182379126549 Validation loss 0.05261526256799698 Accuracy 0.4609375\n",
      "Iteration 100520 Training loss 0.05295204743742943 Validation loss 0.052690185606479645 Accuracy 0.462646484375\n",
      "Iteration 100530 Training loss 0.04603656008839607 Validation loss 0.05266178399324417 Accuracy 0.4609375\n",
      "Iteration 100540 Training loss 0.05148373171687126 Validation loss 0.052686452865600586 Accuracy 0.462158203125\n",
      "Iteration 100550 Training loss 0.05211412534117699 Validation loss 0.0545123927295208 Accuracy 0.44580078125\n",
      "Iteration 100560 Training loss 0.05148034542798996 Validation loss 0.052628979086875916 Accuracy 0.4609375\n",
      "Iteration 100570 Training loss 0.04527851939201355 Validation loss 0.05264478549361229 Accuracy 0.461669921875\n",
      "Iteration 100580 Training loss 0.0493927039206028 Validation loss 0.052711371332407 Accuracy 0.4619140625\n",
      "Iteration 100590 Training loss 0.05044243484735489 Validation loss 0.05305672809481621 Accuracy 0.45947265625\n",
      "Iteration 100600 Training loss 0.047849081456661224 Validation loss 0.05235197767615318 Accuracy 0.462646484375\n",
      "Iteration 100610 Training loss 0.05081399530172348 Validation loss 0.05250966548919678 Accuracy 0.462158203125\n",
      "Iteration 100620 Training loss 0.05005211383104324 Validation loss 0.05308079347014427 Accuracy 0.458251953125\n",
      "Iteration 100630 Training loss 0.04858171567320824 Validation loss 0.05301358923316002 Accuracy 0.460205078125\n",
      "Iteration 100640 Training loss 0.05028793215751648 Validation loss 0.05277162045240402 Accuracy 0.46044921875\n",
      "Iteration 100650 Training loss 0.05039781704545021 Validation loss 0.05246710777282715 Accuracy 0.462158203125\n",
      "Iteration 100660 Training loss 0.05083460360765457 Validation loss 0.05305780470371246 Accuracy 0.4599609375\n",
      "Iteration 100670 Training loss 0.04741377755999565 Validation loss 0.052714500576257706 Accuracy 0.462158203125\n",
      "Iteration 100680 Training loss 0.048506949096918106 Validation loss 0.05330701172351837 Accuracy 0.455810546875\n",
      "Iteration 100690 Training loss 0.048499878495931625 Validation loss 0.05241973325610161 Accuracy 0.462158203125\n",
      "Iteration 100700 Training loss 0.04940939322113991 Validation loss 0.053087323904037476 Accuracy 0.458740234375\n",
      "Iteration 100710 Training loss 0.050695404410362244 Validation loss 0.052686404436826706 Accuracy 0.4609375\n",
      "Iteration 100720 Training loss 0.04821986332535744 Validation loss 0.052484460175037384 Accuracy 0.4619140625\n",
      "Iteration 100730 Training loss 0.05121832713484764 Validation loss 0.053371842950582504 Accuracy 0.4560546875\n",
      "Iteration 100740 Training loss 0.050556741654872894 Validation loss 0.05265052989125252 Accuracy 0.461669921875\n",
      "Iteration 100750 Training loss 0.04977860301733017 Validation loss 0.05289909988641739 Accuracy 0.46044921875\n",
      "Iteration 100760 Training loss 0.05141064524650574 Validation loss 0.05292116478085518 Accuracy 0.460205078125\n",
      "Iteration 100770 Training loss 0.049790047109127045 Validation loss 0.052573684602975845 Accuracy 0.4609375\n",
      "Iteration 100780 Training loss 0.04800594970583916 Validation loss 0.05289129912853241 Accuracy 0.460205078125\n",
      "Iteration 100790 Training loss 0.053567200899124146 Validation loss 0.05361088737845421 Accuracy 0.4541015625\n",
      "Iteration 100800 Training loss 0.05048907548189163 Validation loss 0.05258466303348541 Accuracy 0.4619140625\n",
      "Iteration 100810 Training loss 0.048553045839071274 Validation loss 0.05278598144650459 Accuracy 0.460205078125\n",
      "Iteration 100820 Training loss 0.05011258274316788 Validation loss 0.05279569327831268 Accuracy 0.46044921875\n",
      "Iteration 100830 Training loss 0.04832536354660988 Validation loss 0.05341200530529022 Accuracy 0.45556640625\n",
      "Iteration 100840 Training loss 0.049570389091968536 Validation loss 0.05273604765534401 Accuracy 0.462646484375\n",
      "Iteration 100850 Training loss 0.051347315311431885 Validation loss 0.05256248265504837 Accuracy 0.4609375\n",
      "Iteration 100860 Training loss 0.05211795121431351 Validation loss 0.0536792017519474 Accuracy 0.455078125\n",
      "Iteration 100870 Training loss 0.05319308862090111 Validation loss 0.05296609178185463 Accuracy 0.45849609375\n",
      "Iteration 100880 Training loss 0.051398567855358124 Validation loss 0.05329744890332222 Accuracy 0.456787109375\n",
      "Iteration 100890 Training loss 0.04903768002986908 Validation loss 0.052648574113845825 Accuracy 0.46142578125\n",
      "Iteration 100900 Training loss 0.05260825902223587 Validation loss 0.05326984077692032 Accuracy 0.456787109375\n",
      "Iteration 100910 Training loss 0.05103036388754845 Validation loss 0.0527382493019104 Accuracy 0.460205078125\n",
      "Iteration 100920 Training loss 0.0529097244143486 Validation loss 0.05244697630405426 Accuracy 0.461669921875\n",
      "Iteration 100930 Training loss 0.050739482045173645 Validation loss 0.05309923738241196 Accuracy 0.45947265625\n",
      "Iteration 100940 Training loss 0.0479583665728569 Validation loss 0.052672043442726135 Accuracy 0.4619140625\n",
      "Iteration 100950 Training loss 0.046538207679986954 Validation loss 0.05341506376862526 Accuracy 0.45703125\n",
      "Iteration 100960 Training loss 0.05153833329677582 Validation loss 0.052595678716897964 Accuracy 0.461669921875\n",
      "Iteration 100970 Training loss 0.05057738721370697 Validation loss 0.05297525227069855 Accuracy 0.45947265625\n",
      "Iteration 100980 Training loss 0.04545924812555313 Validation loss 0.05253579840064049 Accuracy 0.46240234375\n",
      "Iteration 100990 Training loss 0.05226912349462509 Validation loss 0.05321343243122101 Accuracy 0.459716796875\n",
      "Iteration 101000 Training loss 0.05472829192876816 Validation loss 0.05252504348754883 Accuracy 0.461181640625\n",
      "Iteration 101010 Training loss 0.053581368178129196 Validation loss 0.05292883142828941 Accuracy 0.46044921875\n",
      "Iteration 101020 Training loss 0.050495248287916183 Validation loss 0.05283602327108383 Accuracy 0.460693359375\n",
      "Iteration 101030 Training loss 0.0494421161711216 Validation loss 0.053133249282836914 Accuracy 0.45849609375\n",
      "Iteration 101040 Training loss 0.04682852700352669 Validation loss 0.05313082039356232 Accuracy 0.45751953125\n",
      "Iteration 101050 Training loss 0.04841840639710426 Validation loss 0.0525350421667099 Accuracy 0.4609375\n",
      "Iteration 101060 Training loss 0.05374058336019516 Validation loss 0.052955739200115204 Accuracy 0.459716796875\n",
      "Iteration 101070 Training loss 0.051164884120225906 Validation loss 0.05294673517346382 Accuracy 0.457763671875\n",
      "Iteration 101080 Training loss 0.0467434823513031 Validation loss 0.05279341712594032 Accuracy 0.45947265625\n",
      "Iteration 101090 Training loss 0.05184704437851906 Validation loss 0.05353013426065445 Accuracy 0.453369140625\n",
      "Iteration 101100 Training loss 0.05078563466668129 Validation loss 0.052852410823106766 Accuracy 0.460693359375\n",
      "Iteration 101110 Training loss 0.05050289258360863 Validation loss 0.052692167460918427 Accuracy 0.460693359375\n",
      "Iteration 101120 Training loss 0.05134351924061775 Validation loss 0.05301306024193764 Accuracy 0.456787109375\n",
      "Iteration 101130 Training loss 0.05077765882015228 Validation loss 0.053016968071460724 Accuracy 0.45849609375\n",
      "Iteration 101140 Training loss 0.05092502757906914 Validation loss 0.053166769444942474 Accuracy 0.4599609375\n",
      "Iteration 101150 Training loss 0.04750506952404976 Validation loss 0.05275799334049225 Accuracy 0.460205078125\n",
      "Iteration 101160 Training loss 0.04848310351371765 Validation loss 0.052942924201488495 Accuracy 0.460205078125\n",
      "Iteration 101170 Training loss 0.05442216992378235 Validation loss 0.05333046615123749 Accuracy 0.457763671875\n",
      "Iteration 101180 Training loss 0.04861602187156677 Validation loss 0.0526001900434494 Accuracy 0.460693359375\n",
      "Iteration 101190 Training loss 0.05052078887820244 Validation loss 0.05311417952179909 Accuracy 0.458984375\n",
      "Iteration 101200 Training loss 0.0509377047419548 Validation loss 0.05265376716852188 Accuracy 0.461181640625\n",
      "Iteration 101210 Training loss 0.05084345489740372 Validation loss 0.05300340801477432 Accuracy 0.457275390625\n",
      "Iteration 101220 Training loss 0.05399096757173538 Validation loss 0.05264276638627052 Accuracy 0.460205078125\n",
      "Iteration 101230 Training loss 0.04858998954296112 Validation loss 0.05265553668141365 Accuracy 0.46142578125\n",
      "Iteration 101240 Training loss 0.05276750773191452 Validation loss 0.05272425711154938 Accuracy 0.4609375\n",
      "Iteration 101250 Training loss 0.04932788386940956 Validation loss 0.0526701882481575 Accuracy 0.459716796875\n",
      "Iteration 101260 Training loss 0.05142274498939514 Validation loss 0.053169723600149155 Accuracy 0.456298828125\n",
      "Iteration 101270 Training loss 0.05090970918536186 Validation loss 0.05331820994615555 Accuracy 0.455810546875\n",
      "Iteration 101280 Training loss 0.05079450458288193 Validation loss 0.052836306393146515 Accuracy 0.45947265625\n",
      "Iteration 101290 Training loss 0.05082761123776436 Validation loss 0.05246901139616966 Accuracy 0.461669921875\n",
      "Iteration 101300 Training loss 0.049179110676050186 Validation loss 0.05275087431073189 Accuracy 0.4599609375\n",
      "Iteration 101310 Training loss 0.049122512340545654 Validation loss 0.052973486483097076 Accuracy 0.4580078125\n",
      "Iteration 101320 Training loss 0.05229497328400612 Validation loss 0.05306779965758324 Accuracy 0.45947265625\n",
      "Iteration 101330 Training loss 0.05044760927557945 Validation loss 0.05286833271384239 Accuracy 0.4599609375\n",
      "Iteration 101340 Training loss 0.04945845156908035 Validation loss 0.05296928063035011 Accuracy 0.459228515625\n",
      "Iteration 101350 Training loss 0.05048785358667374 Validation loss 0.05274506285786629 Accuracy 0.46142578125\n",
      "Iteration 101360 Training loss 0.04674533009529114 Validation loss 0.05294206365942955 Accuracy 0.45751953125\n",
      "Iteration 101370 Training loss 0.051846109330654144 Validation loss 0.052921321243047714 Accuracy 0.459228515625\n",
      "Iteration 101380 Training loss 0.05143469199538231 Validation loss 0.052742596715688705 Accuracy 0.4609375\n",
      "Iteration 101390 Training loss 0.054158829152584076 Validation loss 0.05344565212726593 Accuracy 0.4560546875\n",
      "Iteration 101400 Training loss 0.04949810355901718 Validation loss 0.0527164526283741 Accuracy 0.458984375\n",
      "Iteration 101410 Training loss 0.04605039954185486 Validation loss 0.0529252327978611 Accuracy 0.459228515625\n",
      "Iteration 101420 Training loss 0.053973399102687836 Validation loss 0.05286150425672531 Accuracy 0.459716796875\n",
      "Iteration 101430 Training loss 0.047723859548568726 Validation loss 0.05273335427045822 Accuracy 0.461181640625\n",
      "Iteration 101440 Training loss 0.047421641647815704 Validation loss 0.0525711253285408 Accuracy 0.461181640625\n",
      "Iteration 101450 Training loss 0.047513529658317566 Validation loss 0.05262744054198265 Accuracy 0.4619140625\n",
      "Iteration 101460 Training loss 0.051728505641222 Validation loss 0.05310838669538498 Accuracy 0.4580078125\n",
      "Iteration 101470 Training loss 0.05020375922322273 Validation loss 0.05319664999842644 Accuracy 0.457763671875\n",
      "Iteration 101480 Training loss 0.0486474446952343 Validation loss 0.05285291001200676 Accuracy 0.4609375\n",
      "Iteration 101490 Training loss 0.04920556768774986 Validation loss 0.053291287273168564 Accuracy 0.458984375\n",
      "Iteration 101500 Training loss 0.05349871143698692 Validation loss 0.05273330956697464 Accuracy 0.460693359375\n",
      "Iteration 101510 Training loss 0.05119043588638306 Validation loss 0.05330876633524895 Accuracy 0.4580078125\n",
      "Iteration 101520 Training loss 0.047998007386922836 Validation loss 0.052736300975084305 Accuracy 0.461181640625\n",
      "Iteration 101530 Training loss 0.04904518648982048 Validation loss 0.052881404757499695 Accuracy 0.461181640625\n",
      "Iteration 101540 Training loss 0.04972580820322037 Validation loss 0.05244952440261841 Accuracy 0.4619140625\n",
      "Iteration 101550 Training loss 0.05207400023937225 Validation loss 0.05238206312060356 Accuracy 0.461181640625\n",
      "Iteration 101560 Training loss 0.05197877064347267 Validation loss 0.05246124789118767 Accuracy 0.461669921875\n",
      "Iteration 101570 Training loss 0.04644955322146416 Validation loss 0.052656687796115875 Accuracy 0.4609375\n",
      "Iteration 101580 Training loss 0.047777216881513596 Validation loss 0.05309271812438965 Accuracy 0.458251953125\n",
      "Iteration 101590 Training loss 0.05040187016129494 Validation loss 0.05255741626024246 Accuracy 0.461181640625\n",
      "Iteration 101600 Training loss 0.050036411732435226 Validation loss 0.052541621029376984 Accuracy 0.46240234375\n",
      "Iteration 101610 Training loss 0.04949145391583443 Validation loss 0.05252806469798088 Accuracy 0.46142578125\n",
      "Iteration 101620 Training loss 0.04762531816959381 Validation loss 0.052411530166864395 Accuracy 0.4619140625\n",
      "Iteration 101630 Training loss 0.052245132625103 Validation loss 0.05328405648469925 Accuracy 0.45556640625\n",
      "Iteration 101640 Training loss 0.052218418568372726 Validation loss 0.052833981812000275 Accuracy 0.4609375\n",
      "Iteration 101650 Training loss 0.05125309154391289 Validation loss 0.05308341607451439 Accuracy 0.458984375\n",
      "Iteration 101660 Training loss 0.05206646770238876 Validation loss 0.05269334837794304 Accuracy 0.461181640625\n",
      "Iteration 101670 Training loss 0.05177389085292816 Validation loss 0.0525130070745945 Accuracy 0.462646484375\n",
      "Iteration 101680 Training loss 0.045691050589084625 Validation loss 0.05253942683339119 Accuracy 0.4619140625\n",
      "Iteration 101690 Training loss 0.051373645663261414 Validation loss 0.05264117196202278 Accuracy 0.462646484375\n",
      "Iteration 101700 Training loss 0.054189033806324005 Validation loss 0.05245484411716461 Accuracy 0.46240234375\n",
      "Iteration 101710 Training loss 0.04968053102493286 Validation loss 0.052673935890197754 Accuracy 0.459228515625\n",
      "Iteration 101720 Training loss 0.04990127682685852 Validation loss 0.053088173270225525 Accuracy 0.458251953125\n",
      "Iteration 101730 Training loss 0.049927204847335815 Validation loss 0.05320372432470322 Accuracy 0.45654296875\n",
      "Iteration 101740 Training loss 0.049874842166900635 Validation loss 0.052739717066287994 Accuracy 0.46142578125\n",
      "Iteration 101750 Training loss 0.04859711229801178 Validation loss 0.0525849387049675 Accuracy 0.46240234375\n",
      "Iteration 101760 Training loss 0.048878636211156845 Validation loss 0.0525570847094059 Accuracy 0.46142578125\n",
      "Iteration 101770 Training loss 0.05258728936314583 Validation loss 0.0525386705994606 Accuracy 0.463134765625\n",
      "Iteration 101780 Training loss 0.049230240285396576 Validation loss 0.052605584263801575 Accuracy 0.46142578125\n",
      "Iteration 101790 Training loss 0.04761367663741112 Validation loss 0.05285550281405449 Accuracy 0.460693359375\n",
      "Iteration 101800 Training loss 0.04979155585169792 Validation loss 0.05278107151389122 Accuracy 0.46240234375\n",
      "Iteration 101810 Training loss 0.049748022109270096 Validation loss 0.05298154056072235 Accuracy 0.461181640625\n",
      "Iteration 101820 Training loss 0.04850590601563454 Validation loss 0.05269163101911545 Accuracy 0.458740234375\n",
      "Iteration 101830 Training loss 0.05031536892056465 Validation loss 0.05309218540787697 Accuracy 0.4580078125\n",
      "Iteration 101840 Training loss 0.051237791776657104 Validation loss 0.052540477365255356 Accuracy 0.461181640625\n",
      "Iteration 101850 Training loss 0.049230851233005524 Validation loss 0.052744656801223755 Accuracy 0.46142578125\n",
      "Iteration 101860 Training loss 0.04897790402173996 Validation loss 0.052680887281894684 Accuracy 0.461669921875\n",
      "Iteration 101870 Training loss 0.04777710512280464 Validation loss 0.052302177995443344 Accuracy 0.462158203125\n",
      "Iteration 101880 Training loss 0.0472722165286541 Validation loss 0.052551668137311935 Accuracy 0.46142578125\n",
      "Iteration 101890 Training loss 0.04924818500876427 Validation loss 0.05321143940091133 Accuracy 0.458984375\n",
      "Iteration 101900 Training loss 0.05148865655064583 Validation loss 0.05263669416308403 Accuracy 0.461669921875\n",
      "Iteration 101910 Training loss 0.04955412819981575 Validation loss 0.05290742591023445 Accuracy 0.459716796875\n",
      "Iteration 101920 Training loss 0.051145732402801514 Validation loss 0.052478231489658356 Accuracy 0.461669921875\n",
      "Iteration 101930 Training loss 0.053084492683410645 Validation loss 0.053043827414512634 Accuracy 0.458251953125\n",
      "Iteration 101940 Training loss 0.05084671080112457 Validation loss 0.05271352455019951 Accuracy 0.46142578125\n",
      "Iteration 101950 Training loss 0.05153390392661095 Validation loss 0.05351544916629791 Accuracy 0.45458984375\n",
      "Iteration 101960 Training loss 0.05454013869166374 Validation loss 0.05282619968056679 Accuracy 0.461181640625\n",
      "Iteration 101970 Training loss 0.05427103489637375 Validation loss 0.05291476473212242 Accuracy 0.459228515625\n",
      "Iteration 101980 Training loss 0.05193055793642998 Validation loss 0.052550505846738815 Accuracy 0.4619140625\n",
      "Iteration 101990 Training loss 0.04986025020480156 Validation loss 0.053124383091926575 Accuracy 0.455078125\n",
      "Iteration 102000 Training loss 0.04953419417142868 Validation loss 0.05270608887076378 Accuracy 0.46044921875\n",
      "Iteration 102010 Training loss 0.0543653666973114 Validation loss 0.05241042748093605 Accuracy 0.4619140625\n",
      "Iteration 102020 Training loss 0.05063818395137787 Validation loss 0.05271429196000099 Accuracy 0.460693359375\n",
      "Iteration 102030 Training loss 0.05146779492497444 Validation loss 0.052531614899635315 Accuracy 0.461181640625\n",
      "Iteration 102040 Training loss 0.04962243512272835 Validation loss 0.052527908235788345 Accuracy 0.46240234375\n",
      "Iteration 102050 Training loss 0.05090336501598358 Validation loss 0.052797336131334305 Accuracy 0.46142578125\n",
      "Iteration 102060 Training loss 0.04948830232024193 Validation loss 0.05303284153342247 Accuracy 0.459716796875\n",
      "Iteration 102070 Training loss 0.0475374311208725 Validation loss 0.052746668457984924 Accuracy 0.46142578125\n",
      "Iteration 102080 Training loss 0.050982359796762466 Validation loss 0.05261814221739769 Accuracy 0.461669921875\n",
      "Iteration 102090 Training loss 0.05302945896983147 Validation loss 0.05258573964238167 Accuracy 0.4619140625\n",
      "Iteration 102100 Training loss 0.049316056072711945 Validation loss 0.053217511624097824 Accuracy 0.45751953125\n",
      "Iteration 102110 Training loss 0.05185113474726677 Validation loss 0.052397143095731735 Accuracy 0.462890625\n",
      "Iteration 102120 Training loss 0.0509418323636055 Validation loss 0.05303502827882767 Accuracy 0.45751953125\n",
      "Iteration 102130 Training loss 0.04997028037905693 Validation loss 0.053378112614154816 Accuracy 0.4580078125\n",
      "Iteration 102140 Training loss 0.050597961992025375 Validation loss 0.05294255539774895 Accuracy 0.458984375\n",
      "Iteration 102150 Training loss 0.04991999268531799 Validation loss 0.05270027369260788 Accuracy 0.460693359375\n",
      "Iteration 102160 Training loss 0.05167001113295555 Validation loss 0.05260872095823288 Accuracy 0.460693359375\n",
      "Iteration 102170 Training loss 0.04905872419476509 Validation loss 0.0532991886138916 Accuracy 0.458251953125\n",
      "Iteration 102180 Training loss 0.05098188668489456 Validation loss 0.052554842084646225 Accuracy 0.4619140625\n",
      "Iteration 102190 Training loss 0.04987514019012451 Validation loss 0.05292346701025963 Accuracy 0.459716796875\n",
      "Iteration 102200 Training loss 0.0482599176466465 Validation loss 0.053012553602457047 Accuracy 0.45849609375\n",
      "Iteration 102210 Training loss 0.05090658366680145 Validation loss 0.05263998359441757 Accuracy 0.461181640625\n",
      "Iteration 102220 Training loss 0.04986192286014557 Validation loss 0.052683696150779724 Accuracy 0.4609375\n",
      "Iteration 102230 Training loss 0.0499647855758667 Validation loss 0.05260168015956879 Accuracy 0.460693359375\n",
      "Iteration 102240 Training loss 0.05111807584762573 Validation loss 0.05323081091046333 Accuracy 0.460205078125\n",
      "Iteration 102250 Training loss 0.050235357135534286 Validation loss 0.05262398347258568 Accuracy 0.4619140625\n",
      "Iteration 102260 Training loss 0.05241332948207855 Validation loss 0.0532400980591774 Accuracy 0.456787109375\n",
      "Iteration 102270 Training loss 0.05308620259165764 Validation loss 0.05284658074378967 Accuracy 0.458740234375\n",
      "Iteration 102280 Training loss 0.054580505937337875 Validation loss 0.052602797746658325 Accuracy 0.4609375\n",
      "Iteration 102290 Training loss 0.05212515592575073 Validation loss 0.0525275282561779 Accuracy 0.461181640625\n",
      "Iteration 102300 Training loss 0.05035996437072754 Validation loss 0.05245792493224144 Accuracy 0.461669921875\n",
      "Iteration 102310 Training loss 0.04882121831178665 Validation loss 0.05292927473783493 Accuracy 0.459228515625\n",
      "Iteration 102320 Training loss 0.04997267574071884 Validation loss 0.052399177104234695 Accuracy 0.462158203125\n",
      "Iteration 102330 Training loss 0.05046624690294266 Validation loss 0.05270820111036301 Accuracy 0.4609375\n",
      "Iteration 102340 Training loss 0.05103827267885208 Validation loss 0.05276667699217796 Accuracy 0.46142578125\n",
      "Iteration 102350 Training loss 0.05141502991318703 Validation loss 0.052617382258176804 Accuracy 0.461669921875\n",
      "Iteration 102360 Training loss 0.05223774164915085 Validation loss 0.0530412532389164 Accuracy 0.459228515625\n",
      "Iteration 102370 Training loss 0.05083348602056503 Validation loss 0.05262940749526024 Accuracy 0.461181640625\n",
      "Iteration 102380 Training loss 0.04926520586013794 Validation loss 0.05254090577363968 Accuracy 0.4619140625\n",
      "Iteration 102390 Training loss 0.050306059420108795 Validation loss 0.052934881299734116 Accuracy 0.457763671875\n",
      "Iteration 102400 Training loss 0.05105484277009964 Validation loss 0.05300137400627136 Accuracy 0.45703125\n",
      "Iteration 102410 Training loss 0.05033735930919647 Validation loss 0.0528484508395195 Accuracy 0.459716796875\n",
      "Iteration 102420 Training loss 0.05065057426691055 Validation loss 0.052770838141441345 Accuracy 0.4609375\n",
      "Iteration 102430 Training loss 0.05221007764339447 Validation loss 0.05274311453104019 Accuracy 0.461669921875\n",
      "Iteration 102440 Training loss 0.05397423356771469 Validation loss 0.05279260128736496 Accuracy 0.46044921875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-4, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c253240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 61465600.0 and the number of iterations is 102442.\n",
      "Iteration 0 Training loss 0.08748721331357956 Validation loss 0.08817725628614426 Accuracy 0.102294921875\n",
      "Iteration 10 Training loss 0.07844120264053345 Validation loss 0.07942960411310196 Accuracy 0.200927734375\n",
      "Iteration 20 Training loss 0.08213479816913605 Validation loss 0.08172647655010223 Accuracy 0.1795654296875\n",
      "Iteration 30 Training loss 0.07555239647626877 Validation loss 0.07497331500053406 Accuracy 0.2467041015625\n",
      "Iteration 40 Training loss 0.07391677051782608 Validation loss 0.07393834739923477 Accuracy 0.257080078125\n",
      "Iteration 50 Training loss 0.07477433979511261 Validation loss 0.07315880805253983 Accuracy 0.265869140625\n",
      "Iteration 60 Training loss 0.0741143673658371 Validation loss 0.0726979598402977 Accuracy 0.2705078125\n",
      "Iteration 70 Training loss 0.06965716183185577 Validation loss 0.07211826741695404 Accuracy 0.273681640625\n",
      "Iteration 80 Training loss 0.07196424156427383 Validation loss 0.0720154345035553 Accuracy 0.275634765625\n",
      "Iteration 90 Training loss 0.07721260190010071 Validation loss 0.07507993280887604 Accuracy 0.246826171875\n",
      "Iteration 100 Training loss 0.0759267807006836 Validation loss 0.07473825663328171 Accuracy 0.2491455078125\n",
      "Iteration 110 Training loss 0.0717630386352539 Validation loss 0.07257895171642303 Accuracy 0.272216796875\n",
      "Iteration 120 Training loss 0.07214990258216858 Validation loss 0.07182168960571289 Accuracy 0.279296875\n",
      "Iteration 130 Training loss 0.07019497454166412 Validation loss 0.07167469710111618 Accuracy 0.281494140625\n",
      "Iteration 140 Training loss 0.07342763245105743 Validation loss 0.07225783169269562 Accuracy 0.27490234375\n",
      "Iteration 150 Training loss 0.07062767446041107 Validation loss 0.07140801101922989 Accuracy 0.283935546875\n",
      "Iteration 160 Training loss 0.0714464783668518 Validation loss 0.07155710458755493 Accuracy 0.28271484375\n",
      "Iteration 170 Training loss 0.06770876795053482 Validation loss 0.07163205742835999 Accuracy 0.280517578125\n",
      "Iteration 180 Training loss 0.07008950412273407 Validation loss 0.07130321860313416 Accuracy 0.284912109375\n",
      "Iteration 190 Training loss 0.07315066456794739 Validation loss 0.07129567116498947 Accuracy 0.2841796875\n",
      "Iteration 200 Training loss 0.07338987290859222 Validation loss 0.07246636599302292 Accuracy 0.272216796875\n",
      "Iteration 210 Training loss 0.07204689085483551 Validation loss 0.07138799130916595 Accuracy 0.283935546875\n",
      "Iteration 220 Training loss 0.07235988229513168 Validation loss 0.0716899186372757 Accuracy 0.280517578125\n",
      "Iteration 230 Training loss 0.06942839920520782 Validation loss 0.07127534598112106 Accuracy 0.285400390625\n",
      "Iteration 240 Training loss 0.07183083146810532 Validation loss 0.07111244648694992 Accuracy 0.286865234375\n",
      "Iteration 250 Training loss 0.07132838666439056 Validation loss 0.07122338563203812 Accuracy 0.285888671875\n",
      "Iteration 260 Training loss 0.07041209191083908 Validation loss 0.07107365131378174 Accuracy 0.287353515625\n",
      "Iteration 270 Training loss 0.07304098457098007 Validation loss 0.07182393223047256 Accuracy 0.2802734375\n",
      "Iteration 280 Training loss 0.07284821569919586 Validation loss 0.07115738838911057 Accuracy 0.287109375\n",
      "Iteration 290 Training loss 0.06719397753477097 Validation loss 0.07122351229190826 Accuracy 0.2861328125\n",
      "Iteration 300 Training loss 0.06866003572940826 Validation loss 0.07116074860095978 Accuracy 0.28662109375\n",
      "Iteration 310 Training loss 0.07362893968820572 Validation loss 0.07191783934831619 Accuracy 0.278564453125\n",
      "Iteration 320 Training loss 0.07060062885284424 Validation loss 0.07154465466737747 Accuracy 0.28271484375\n",
      "Iteration 330 Training loss 0.07253018766641617 Validation loss 0.0712813064455986 Accuracy 0.285400390625\n",
      "Iteration 340 Training loss 0.0675342008471489 Validation loss 0.07087622582912445 Accuracy 0.2890625\n",
      "Iteration 350 Training loss 0.07122082263231277 Validation loss 0.07136025279760361 Accuracy 0.284423828125\n",
      "Iteration 360 Training loss 0.07279221713542938 Validation loss 0.07099083065986633 Accuracy 0.2880859375\n",
      "Iteration 370 Training loss 0.07295078784227371 Validation loss 0.07121231406927109 Accuracy 0.28662109375\n",
      "Iteration 380 Training loss 0.0708935558795929 Validation loss 0.07141188532114029 Accuracy 0.284423828125\n",
      "Iteration 390 Training loss 0.07062819600105286 Validation loss 0.07096616178750992 Accuracy 0.288330078125\n",
      "Iteration 400 Training loss 0.07079897075891495 Validation loss 0.07133126258850098 Accuracy 0.28515625\n",
      "Iteration 410 Training loss 0.0764952301979065 Validation loss 0.07111677527427673 Accuracy 0.286865234375\n",
      "Iteration 420 Training loss 0.07374773919582367 Validation loss 0.0709817185997963 Accuracy 0.288330078125\n",
      "Iteration 430 Training loss 0.069486603140831 Validation loss 0.07096990197896957 Accuracy 0.28857421875\n",
      "Iteration 440 Training loss 0.07080136984586716 Validation loss 0.07135660201311111 Accuracy 0.284912109375\n",
      "Iteration 450 Training loss 0.07041679322719574 Validation loss 0.07168243080377579 Accuracy 0.28173828125\n",
      "Iteration 460 Training loss 0.06918210536241531 Validation loss 0.07151027768850327 Accuracy 0.2841796875\n",
      "Iteration 470 Training loss 0.07236537337303162 Validation loss 0.07149641215801239 Accuracy 0.2841796875\n",
      "Iteration 480 Training loss 0.07356106489896774 Validation loss 0.07383649051189423 Accuracy 0.259033203125\n",
      "Iteration 490 Training loss 0.07197108864784241 Validation loss 0.07144664227962494 Accuracy 0.283935546875\n",
      "Iteration 500 Training loss 0.07025496661663055 Validation loss 0.07100322097539902 Accuracy 0.28857421875\n",
      "Iteration 510 Training loss 0.06805302947759628 Validation loss 0.07116653025150299 Accuracy 0.286865234375\n",
      "Iteration 520 Training loss 0.07254565507173538 Validation loss 0.07102245837450027 Accuracy 0.288818359375\n",
      "Iteration 530 Training loss 0.0699809119105339 Validation loss 0.0709109976887703 Accuracy 0.28955078125\n",
      "Iteration 540 Training loss 0.07113824039697647 Validation loss 0.07132300734519958 Accuracy 0.285400390625\n",
      "Iteration 550 Training loss 0.0713396817445755 Validation loss 0.07096821069717407 Accuracy 0.288818359375\n",
      "Iteration 560 Training loss 0.06839148700237274 Validation loss 0.07093196362257004 Accuracy 0.289306640625\n",
      "Iteration 570 Training loss 0.07086930423974991 Validation loss 0.07089254260063171 Accuracy 0.289794921875\n",
      "Iteration 580 Training loss 0.07126834243535995 Validation loss 0.07098757475614548 Accuracy 0.288330078125\n",
      "Iteration 590 Training loss 0.06976868957281113 Validation loss 0.07094131410121918 Accuracy 0.288818359375\n",
      "Iteration 600 Training loss 0.06865645945072174 Validation loss 0.0709078311920166 Accuracy 0.2890625\n",
      "Iteration 610 Training loss 0.069943867623806 Validation loss 0.07087381929159164 Accuracy 0.2900390625\n",
      "Iteration 620 Training loss 0.07164662331342697 Validation loss 0.07086215913295746 Accuracy 0.289794921875\n",
      "Iteration 630 Training loss 0.07246143370866776 Validation loss 0.07170584052801132 Accuracy 0.281005859375\n",
      "Iteration 640 Training loss 0.07280461490154266 Validation loss 0.07087419927120209 Accuracy 0.289794921875\n",
      "Iteration 650 Training loss 0.07073314487934113 Validation loss 0.0709058940410614 Accuracy 0.28955078125\n",
      "Iteration 660 Training loss 0.0718618780374527 Validation loss 0.07140098512172699 Accuracy 0.284423828125\n",
      "Iteration 670 Training loss 0.06944730132818222 Validation loss 0.07085523754358292 Accuracy 0.290283203125\n",
      "Iteration 680 Training loss 0.06835298240184784 Validation loss 0.070886991918087 Accuracy 0.2900390625\n",
      "Iteration 690 Training loss 0.06835570186376572 Validation loss 0.0709516704082489 Accuracy 0.288818359375\n",
      "Iteration 700 Training loss 0.07111096382141113 Validation loss 0.0710340365767479 Accuracy 0.287841796875\n",
      "Iteration 710 Training loss 0.0693669468164444 Validation loss 0.07090948522090912 Accuracy 0.289306640625\n",
      "Iteration 720 Training loss 0.07367297261953354 Validation loss 0.0709872841835022 Accuracy 0.288818359375\n",
      "Iteration 730 Training loss 0.07023213803768158 Validation loss 0.07189958542585373 Accuracy 0.279541015625\n",
      "Iteration 740 Training loss 0.0705447718501091 Validation loss 0.07095008343458176 Accuracy 0.289306640625\n",
      "Iteration 750 Training loss 0.07101698964834213 Validation loss 0.07118270546197891 Accuracy 0.28662109375\n",
      "Iteration 760 Training loss 0.07190178334712982 Validation loss 0.07092393189668655 Accuracy 0.288818359375\n",
      "Iteration 770 Training loss 0.07203580439090729 Validation loss 0.07095949351787567 Accuracy 0.2890625\n",
      "Iteration 780 Training loss 0.07244843989610672 Validation loss 0.07166728377342224 Accuracy 0.281494140625\n",
      "Iteration 790 Training loss 0.06828044354915619 Validation loss 0.07156912237405777 Accuracy 0.282958984375\n",
      "Iteration 800 Training loss 0.06984912604093552 Validation loss 0.0709584653377533 Accuracy 0.289306640625\n",
      "Iteration 810 Training loss 0.0709235817193985 Validation loss 0.07130976021289825 Accuracy 0.28564453125\n",
      "Iteration 820 Training loss 0.06978222727775574 Validation loss 0.07090403884649277 Accuracy 0.2900390625\n",
      "Iteration 830 Training loss 0.0666196420788765 Validation loss 0.07110550254583359 Accuracy 0.287109375\n",
      "Iteration 840 Training loss 0.07090701162815094 Validation loss 0.07107207924127579 Accuracy 0.28759765625\n",
      "Iteration 850 Training loss 0.0703013613820076 Validation loss 0.07100392878055573 Accuracy 0.287841796875\n",
      "Iteration 860 Training loss 0.06607306748628616 Validation loss 0.07084790617227554 Accuracy 0.290283203125\n",
      "Iteration 870 Training loss 0.06924082338809967 Validation loss 0.070888452231884 Accuracy 0.28955078125\n",
      "Iteration 880 Training loss 0.07218343764543533 Validation loss 0.07095890492200851 Accuracy 0.2890625\n",
      "Iteration 890 Training loss 0.07241138815879822 Validation loss 0.07146570086479187 Accuracy 0.283935546875\n",
      "Iteration 900 Training loss 0.07275329530239105 Validation loss 0.07120218127965927 Accuracy 0.28662109375\n",
      "Iteration 910 Training loss 0.06899648904800415 Validation loss 0.07096637785434723 Accuracy 0.289306640625\n",
      "Iteration 920 Training loss 0.07266399264335632 Validation loss 0.07096680998802185 Accuracy 0.288818359375\n",
      "Iteration 930 Training loss 0.07198655605316162 Validation loss 0.07093052566051483 Accuracy 0.28955078125\n",
      "Iteration 940 Training loss 0.07313224673271179 Validation loss 0.07088159769773483 Accuracy 0.289794921875\n",
      "Iteration 950 Training loss 0.07198300957679749 Validation loss 0.07088141143321991 Accuracy 0.289794921875\n",
      "Iteration 960 Training loss 0.07069901376962662 Validation loss 0.07090603560209274 Accuracy 0.289306640625\n",
      "Iteration 970 Training loss 0.068463996052742 Validation loss 0.07124479115009308 Accuracy 0.28564453125\n",
      "Iteration 980 Training loss 0.07002169638872147 Validation loss 0.07087346166372299 Accuracy 0.2900390625\n",
      "Iteration 990 Training loss 0.07251188158988953 Validation loss 0.07091904431581497 Accuracy 0.288818359375\n",
      "Iteration 1000 Training loss 0.07140665501356125 Validation loss 0.07091684639453888 Accuracy 0.289794921875\n",
      "Iteration 1010 Training loss 0.07076683640480042 Validation loss 0.07089675217866898 Accuracy 0.2900390625\n",
      "Iteration 1020 Training loss 0.07277538627386093 Validation loss 0.0711304098367691 Accuracy 0.28759765625\n",
      "Iteration 1030 Training loss 0.06998053193092346 Validation loss 0.07096902281045914 Accuracy 0.2890625\n",
      "Iteration 1040 Training loss 0.07045979052782059 Validation loss 0.07197359204292297 Accuracy 0.279052734375\n",
      "Iteration 1050 Training loss 0.0690397322177887 Validation loss 0.0712752640247345 Accuracy 0.286376953125\n",
      "Iteration 1060 Training loss 0.06954051554203033 Validation loss 0.0708736777305603 Accuracy 0.2900390625\n",
      "Iteration 1070 Training loss 0.07047631591558456 Validation loss 0.07087226957082748 Accuracy 0.2900390625\n",
      "Iteration 1080 Training loss 0.07157787680625916 Validation loss 0.07090029865503311 Accuracy 0.2900390625\n",
      "Iteration 1090 Training loss 0.07485942542552948 Validation loss 0.07091999053955078 Accuracy 0.2900390625\n",
      "Iteration 1100 Training loss 0.06777343153953552 Validation loss 0.07096996158361435 Accuracy 0.289306640625\n",
      "Iteration 1110 Training loss 0.07133302092552185 Validation loss 0.07100218534469604 Accuracy 0.28857421875\n",
      "Iteration 1120 Training loss 0.07427632808685303 Validation loss 0.07083120197057724 Accuracy 0.290771484375\n",
      "Iteration 1130 Training loss 0.07330437004566193 Validation loss 0.07113571465015411 Accuracy 0.28759765625\n",
      "Iteration 1140 Training loss 0.07542432844638824 Validation loss 0.07089205086231232 Accuracy 0.289794921875\n",
      "Iteration 1150 Training loss 0.07413903623819351 Validation loss 0.07091964036226273 Accuracy 0.2900390625\n",
      "Iteration 1160 Training loss 0.0711735188961029 Validation loss 0.07084916532039642 Accuracy 0.2900390625\n",
      "Iteration 1170 Training loss 0.0750732347369194 Validation loss 0.07110694795846939 Accuracy 0.287841796875\n",
      "Iteration 1180 Training loss 0.0701955258846283 Validation loss 0.07074986398220062 Accuracy 0.291259765625\n",
      "Iteration 1190 Training loss 0.06984959542751312 Validation loss 0.07078535854816437 Accuracy 0.290771484375\n",
      "Iteration 1200 Training loss 0.06958358734846115 Validation loss 0.07079155743122101 Accuracy 0.290771484375\n",
      "Iteration 1210 Training loss 0.06942287087440491 Validation loss 0.07082700729370117 Accuracy 0.290283203125\n",
      "Iteration 1220 Training loss 0.0716485008597374 Validation loss 0.07092670351266861 Accuracy 0.289794921875\n",
      "Iteration 1230 Training loss 0.07133103907108307 Validation loss 0.07110080868005753 Accuracy 0.2880859375\n",
      "Iteration 1240 Training loss 0.07077404856681824 Validation loss 0.07095834612846375 Accuracy 0.28955078125\n",
      "Iteration 1250 Training loss 0.07206308096647263 Validation loss 0.07091616094112396 Accuracy 0.289306640625\n",
      "Iteration 1260 Training loss 0.07053090631961823 Validation loss 0.07075399905443192 Accuracy 0.291015625\n",
      "Iteration 1270 Training loss 0.07068249583244324 Validation loss 0.07090498507022858 Accuracy 0.28857421875\n",
      "Iteration 1280 Training loss 0.0731329470872879 Validation loss 0.07084915786981583 Accuracy 0.2900390625\n",
      "Iteration 1290 Training loss 0.06880169361829758 Validation loss 0.07100631296634674 Accuracy 0.28759765625\n",
      "Iteration 1300 Training loss 0.0708022266626358 Validation loss 0.07068739086389542 Accuracy 0.290771484375\n",
      "Iteration 1310 Training loss 0.07250919193029404 Validation loss 0.07072808593511581 Accuracy 0.2900390625\n",
      "Iteration 1320 Training loss 0.06867685914039612 Validation loss 0.07213373482227325 Accuracy 0.276123046875\n",
      "Iteration 1330 Training loss 0.07291387021541595 Validation loss 0.07077855616807938 Accuracy 0.2900390625\n",
      "Iteration 1340 Training loss 0.073598712682724 Validation loss 0.07077180594205856 Accuracy 0.2900390625\n",
      "Iteration 1350 Training loss 0.07080293446779251 Validation loss 0.07067251950502396 Accuracy 0.290771484375\n",
      "Iteration 1360 Training loss 0.07300635427236557 Validation loss 0.07115969806909561 Accuracy 0.287353515625\n",
      "Iteration 1370 Training loss 0.06955400854349136 Validation loss 0.07073092460632324 Accuracy 0.291259765625\n",
      "Iteration 1380 Training loss 0.07030471414327621 Validation loss 0.07099571079015732 Accuracy 0.288330078125\n",
      "Iteration 1390 Training loss 0.07154569774866104 Validation loss 0.0710015669465065 Accuracy 0.288330078125\n",
      "Iteration 1400 Training loss 0.07042553275823593 Validation loss 0.07072373479604721 Accuracy 0.291748046875\n",
      "Iteration 1410 Training loss 0.07128766179084778 Validation loss 0.07076279819011688 Accuracy 0.290771484375\n",
      "Iteration 1420 Training loss 0.0720587745308876 Validation loss 0.07083477079868317 Accuracy 0.28857421875\n",
      "Iteration 1430 Training loss 0.07040541619062424 Validation loss 0.07071482390165329 Accuracy 0.29052734375\n",
      "Iteration 1440 Training loss 0.07119150459766388 Validation loss 0.07074024528265 Accuracy 0.289794921875\n",
      "Iteration 1450 Training loss 0.06773757189512253 Validation loss 0.07065847516059875 Accuracy 0.2919921875\n",
      "Iteration 1460 Training loss 0.07127724587917328 Validation loss 0.07126982510089874 Accuracy 0.284912109375\n",
      "Iteration 1470 Training loss 0.06687890738248825 Validation loss 0.07065518200397491 Accuracy 0.290771484375\n",
      "Iteration 1480 Training loss 0.07309740781784058 Validation loss 0.07085776329040527 Accuracy 0.2900390625\n",
      "Iteration 1490 Training loss 0.07057816535234451 Validation loss 0.07064719498157501 Accuracy 0.291259765625\n",
      "Iteration 1500 Training loss 0.07122515887022018 Validation loss 0.07073195278644562 Accuracy 0.290283203125\n",
      "Iteration 1510 Training loss 0.07100459188222885 Validation loss 0.07064064592123032 Accuracy 0.2919921875\n",
      "Iteration 1520 Training loss 0.0718037411570549 Validation loss 0.07067393511533737 Accuracy 0.291748046875\n",
      "Iteration 1530 Training loss 0.06797703355550766 Validation loss 0.07073825597763062 Accuracy 0.291015625\n",
      "Iteration 1540 Training loss 0.07115545868873596 Validation loss 0.07071293145418167 Accuracy 0.291015625\n",
      "Iteration 1550 Training loss 0.06913604587316513 Validation loss 0.07074557989835739 Accuracy 0.291259765625\n",
      "Iteration 1560 Training loss 0.07267960906028748 Validation loss 0.07196936011314392 Accuracy 0.278564453125\n",
      "Iteration 1570 Training loss 0.07157102227210999 Validation loss 0.07077410817146301 Accuracy 0.291015625\n",
      "Iteration 1580 Training loss 0.06872744113206863 Validation loss 0.07076103985309601 Accuracy 0.29150390625\n",
      "Iteration 1590 Training loss 0.07011581212282181 Validation loss 0.07077451795339584 Accuracy 0.29150390625\n",
      "Iteration 1600 Training loss 0.07277750223875046 Validation loss 0.07133348286151886 Accuracy 0.285888671875\n",
      "Iteration 1610 Training loss 0.07265085726976395 Validation loss 0.07070106267929077 Accuracy 0.2919921875\n",
      "Iteration 1620 Training loss 0.06966511905193329 Validation loss 0.07074091583490372 Accuracy 0.29150390625\n",
      "Iteration 1630 Training loss 0.06842321902513504 Validation loss 0.07075219601392746 Accuracy 0.29150390625\n",
      "Iteration 1640 Training loss 0.07073908299207687 Validation loss 0.07097158581018448 Accuracy 0.2890625\n",
      "Iteration 1650 Training loss 0.07242466509342194 Validation loss 0.07067615538835526 Accuracy 0.292236328125\n",
      "Iteration 1660 Training loss 0.06931308656930923 Validation loss 0.07064443081617355 Accuracy 0.29150390625\n",
      "Iteration 1670 Training loss 0.07115614414215088 Validation loss 0.07066036760807037 Accuracy 0.291259765625\n",
      "Iteration 1680 Training loss 0.06997215747833252 Validation loss 0.07067224383354187 Accuracy 0.290771484375\n",
      "Iteration 1690 Training loss 0.0707758367061615 Validation loss 0.07077130675315857 Accuracy 0.29150390625\n",
      "Iteration 1700 Training loss 0.06885051727294922 Validation loss 0.07067126035690308 Accuracy 0.291259765625\n",
      "Iteration 1710 Training loss 0.06856928765773773 Validation loss 0.07082043588161469 Accuracy 0.289306640625\n",
      "Iteration 1720 Training loss 0.07103782892227173 Validation loss 0.07075110077857971 Accuracy 0.29150390625\n",
      "Iteration 1730 Training loss 0.06994456797838211 Validation loss 0.07081338763237 Accuracy 0.291015625\n",
      "Iteration 1740 Training loss 0.07197965681552887 Validation loss 0.07089214026927948 Accuracy 0.2900390625\n",
      "Iteration 1750 Training loss 0.07030081003904343 Validation loss 0.07095678895711899 Accuracy 0.289794921875\n",
      "Iteration 1760 Training loss 0.06868743151426315 Validation loss 0.07078579068183899 Accuracy 0.29052734375\n",
      "Iteration 1770 Training loss 0.06928679347038269 Validation loss 0.07105005532503128 Accuracy 0.28857421875\n",
      "Iteration 1780 Training loss 0.06943286955356598 Validation loss 0.07084399461746216 Accuracy 0.29052734375\n",
      "Iteration 1790 Training loss 0.06840281933546066 Validation loss 0.07082416862249374 Accuracy 0.291015625\n",
      "Iteration 1800 Training loss 0.06697490066289902 Validation loss 0.07107087969779968 Accuracy 0.287841796875\n",
      "Iteration 1810 Training loss 0.07013625651597977 Validation loss 0.07086192816495895 Accuracy 0.288818359375\n",
      "Iteration 1820 Training loss 0.07050022482872009 Validation loss 0.0708223432302475 Accuracy 0.289794921875\n",
      "Iteration 1830 Training loss 0.07275490462779999 Validation loss 0.07095272094011307 Accuracy 0.287353515625\n",
      "Iteration 1840 Training loss 0.06845035403966904 Validation loss 0.0706239566206932 Accuracy 0.29150390625\n",
      "Iteration 1850 Training loss 0.06880553066730499 Validation loss 0.07112187892198563 Accuracy 0.28515625\n",
      "Iteration 1860 Training loss 0.06951829791069031 Validation loss 0.07079421728849411 Accuracy 0.2890625\n",
      "Iteration 1870 Training loss 0.0690208151936531 Validation loss 0.07135221362113953 Accuracy 0.28271484375\n",
      "Iteration 1880 Training loss 0.07079115509986877 Validation loss 0.07131745666265488 Accuracy 0.283447265625\n",
      "Iteration 1890 Training loss 0.07010744512081146 Validation loss 0.07114146649837494 Accuracy 0.28564453125\n",
      "Iteration 1900 Training loss 0.0745733305811882 Validation loss 0.07071832567453384 Accuracy 0.28955078125\n",
      "Iteration 1910 Training loss 0.07295206934213638 Validation loss 0.07064103335142136 Accuracy 0.289794921875\n",
      "Iteration 1920 Training loss 0.07357163727283478 Validation loss 0.07076605409383774 Accuracy 0.289306640625\n",
      "Iteration 1930 Training loss 0.07053698599338531 Validation loss 0.07105719298124313 Accuracy 0.28564453125\n",
      "Iteration 1940 Training loss 0.071494922041893 Validation loss 0.07091862708330154 Accuracy 0.2880859375\n",
      "Iteration 1950 Training loss 0.07048382610082626 Validation loss 0.0708765983581543 Accuracy 0.287841796875\n",
      "Iteration 1960 Training loss 0.07481282204389572 Validation loss 0.07083059102296829 Accuracy 0.28857421875\n",
      "Iteration 1970 Training loss 0.07017706334590912 Validation loss 0.07056481391191483 Accuracy 0.29248046875\n",
      "Iteration 1980 Training loss 0.07019739598035812 Validation loss 0.07067269086837769 Accuracy 0.290771484375\n",
      "Iteration 1990 Training loss 0.06763412058353424 Validation loss 0.07069166004657745 Accuracy 0.291015625\n",
      "Iteration 2000 Training loss 0.0715768039226532 Validation loss 0.07068493217229843 Accuracy 0.29150390625\n",
      "Iteration 2010 Training loss 0.0712374597787857 Validation loss 0.07066895067691803 Accuracy 0.29248046875\n",
      "Iteration 2020 Training loss 0.0695357546210289 Validation loss 0.07060419768095016 Accuracy 0.29248046875\n",
      "Iteration 2030 Training loss 0.07241848856210709 Validation loss 0.07063541561365128 Accuracy 0.29248046875\n",
      "Iteration 2040 Training loss 0.07348804920911789 Validation loss 0.07060158252716064 Accuracy 0.29248046875\n",
      "Iteration 2050 Training loss 0.0712938904762268 Validation loss 0.07121356576681137 Accuracy 0.2861328125\n",
      "Iteration 2060 Training loss 0.07210507988929749 Validation loss 0.07067672908306122 Accuracy 0.29150390625\n",
      "Iteration 2070 Training loss 0.06989893317222595 Validation loss 0.07065939903259277 Accuracy 0.29248046875\n",
      "Iteration 2080 Training loss 0.07113947719335556 Validation loss 0.07099207490682602 Accuracy 0.288818359375\n",
      "Iteration 2090 Training loss 0.0723721832036972 Validation loss 0.0707317441701889 Accuracy 0.29150390625\n",
      "Iteration 2100 Training loss 0.06872650980949402 Validation loss 0.0706641748547554 Accuracy 0.29248046875\n",
      "Iteration 2110 Training loss 0.06982405483722687 Validation loss 0.07068432122468948 Accuracy 0.2919921875\n",
      "Iteration 2120 Training loss 0.07035789638757706 Validation loss 0.0706649050116539 Accuracy 0.290771484375\n",
      "Iteration 2130 Training loss 0.07042589038610458 Validation loss 0.07068021595478058 Accuracy 0.291259765625\n",
      "Iteration 2140 Training loss 0.0722566619515419 Validation loss 0.0706423744559288 Accuracy 0.29052734375\n",
      "Iteration 2150 Training loss 0.07101913541555405 Validation loss 0.07063884288072586 Accuracy 0.2919921875\n",
      "Iteration 2160 Training loss 0.06970888376235962 Validation loss 0.07066420465707779 Accuracy 0.2919921875\n",
      "Iteration 2170 Training loss 0.0715966522693634 Validation loss 0.07069651037454605 Accuracy 0.291748046875\n",
      "Iteration 2180 Training loss 0.06930733472108841 Validation loss 0.0706821084022522 Accuracy 0.2919921875\n",
      "Iteration 2190 Training loss 0.0718834325671196 Validation loss 0.07062046229839325 Accuracy 0.292236328125\n",
      "Iteration 2200 Training loss 0.06940855830907822 Validation loss 0.07065702229738235 Accuracy 0.29150390625\n",
      "Iteration 2210 Training loss 0.06970285624265671 Validation loss 0.07123170793056488 Accuracy 0.284423828125\n",
      "Iteration 2220 Training loss 0.06686464697122574 Validation loss 0.07082045823335648 Accuracy 0.289306640625\n",
      "Iteration 2230 Training loss 0.06926864385604858 Validation loss 0.0708613395690918 Accuracy 0.288818359375\n",
      "Iteration 2240 Training loss 0.06798002868890762 Validation loss 0.07064370810985565 Accuracy 0.290771484375\n",
      "Iteration 2250 Training loss 0.0706840232014656 Validation loss 0.07077696174383163 Accuracy 0.2900390625\n",
      "Iteration 2260 Training loss 0.07095770537853241 Validation loss 0.07077760249376297 Accuracy 0.290283203125\n",
      "Iteration 2270 Training loss 0.07245387881994247 Validation loss 0.07065604627132416 Accuracy 0.29150390625\n",
      "Iteration 2280 Training loss 0.07010240852832794 Validation loss 0.07079929113388062 Accuracy 0.2900390625\n",
      "Iteration 2290 Training loss 0.07222370058298111 Validation loss 0.07076942175626755 Accuracy 0.290283203125\n",
      "Iteration 2300 Training loss 0.07244838029146194 Validation loss 0.0712137371301651 Accuracy 0.28564453125\n",
      "Iteration 2310 Training loss 0.06916630268096924 Validation loss 0.07069013267755508 Accuracy 0.291015625\n",
      "Iteration 2320 Training loss 0.07256174832582474 Validation loss 0.07079630345106125 Accuracy 0.2900390625\n",
      "Iteration 2330 Training loss 0.06882895529270172 Validation loss 0.07068304717540741 Accuracy 0.2919921875\n",
      "Iteration 2340 Training loss 0.06989791989326477 Validation loss 0.07068132609128952 Accuracy 0.291015625\n",
      "Iteration 2350 Training loss 0.07419371604919434 Validation loss 0.07086748629808426 Accuracy 0.2900390625\n",
      "Iteration 2360 Training loss 0.07168794423341751 Validation loss 0.07079755514860153 Accuracy 0.290771484375\n",
      "Iteration 2370 Training loss 0.07276266068220139 Validation loss 0.07078531384468079 Accuracy 0.291015625\n",
      "Iteration 2380 Training loss 0.06886181235313416 Validation loss 0.07063354551792145 Accuracy 0.2919921875\n",
      "Iteration 2390 Training loss 0.06901255995035172 Validation loss 0.07060328125953674 Accuracy 0.29150390625\n",
      "Iteration 2400 Training loss 0.07289545238018036 Validation loss 0.07106277346611023 Accuracy 0.287109375\n",
      "Iteration 2410 Training loss 0.0697309672832489 Validation loss 0.07063058018684387 Accuracy 0.2919921875\n",
      "Iteration 2420 Training loss 0.06965501606464386 Validation loss 0.0710323303937912 Accuracy 0.288818359375\n",
      "Iteration 2430 Training loss 0.07050737738609314 Validation loss 0.07064785808324814 Accuracy 0.292236328125\n",
      "Iteration 2440 Training loss 0.06561043113470078 Validation loss 0.07064936310052872 Accuracy 0.29248046875\n",
      "Iteration 2450 Training loss 0.07214679569005966 Validation loss 0.07063011825084686 Accuracy 0.291015625\n",
      "Iteration 2460 Training loss 0.07411666959524155 Validation loss 0.07073312997817993 Accuracy 0.2890625\n",
      "Iteration 2470 Training loss 0.0713767558336258 Validation loss 0.07230707257986069 Accuracy 0.27490234375\n",
      "Iteration 2480 Training loss 0.07014410197734833 Validation loss 0.07052874565124512 Accuracy 0.29150390625\n",
      "Iteration 2490 Training loss 0.07055525481700897 Validation loss 0.07196377962827682 Accuracy 0.278076171875\n",
      "Iteration 2500 Training loss 0.07023564726114273 Validation loss 0.07086565345525742 Accuracy 0.2880859375\n",
      "Iteration 2510 Training loss 0.07208947837352753 Validation loss 0.0707261860370636 Accuracy 0.289306640625\n",
      "Iteration 2520 Training loss 0.07183244824409485 Validation loss 0.06986680626869202 Accuracy 0.298095703125\n",
      "Iteration 2530 Training loss 0.06352957338094711 Validation loss 0.06436977535486221 Accuracy 0.352783203125\n",
      "Iteration 2540 Training loss 0.06372959166765213 Validation loss 0.06253422051668167 Accuracy 0.368896484375\n",
      "Iteration 2550 Training loss 0.06333152204751968 Validation loss 0.06245812028646469 Accuracy 0.37060546875\n",
      "Iteration 2560 Training loss 0.058828845620155334 Validation loss 0.06276752054691315 Accuracy 0.369384765625\n",
      "Iteration 2570 Training loss 0.061252329498529434 Validation loss 0.06163516268134117 Accuracy 0.37744140625\n",
      "Iteration 2580 Training loss 0.059775106608867645 Validation loss 0.06322302669286728 Accuracy 0.36328125\n",
      "Iteration 2590 Training loss 0.06182854250073433 Validation loss 0.06204444169998169 Accuracy 0.377197265625\n",
      "Iteration 2600 Training loss 0.06367304176092148 Validation loss 0.06188160181045532 Accuracy 0.377197265625\n",
      "Iteration 2610 Training loss 0.05975241959095001 Validation loss 0.061960820108652115 Accuracy 0.376220703125\n",
      "Iteration 2620 Training loss 0.0621224045753479 Validation loss 0.06307647377252579 Accuracy 0.3671875\n",
      "Iteration 2630 Training loss 0.06122095510363579 Validation loss 0.061656318604946136 Accuracy 0.381103515625\n",
      "Iteration 2640 Training loss 0.05953725799918175 Validation loss 0.06144162267446518 Accuracy 0.382080078125\n",
      "Iteration 2650 Training loss 0.06492391228675842 Validation loss 0.06219768524169922 Accuracy 0.374267578125\n",
      "Iteration 2660 Training loss 0.06497340649366379 Validation loss 0.061347831040620804 Accuracy 0.3828125\n",
      "Iteration 2670 Training loss 0.06079782545566559 Validation loss 0.062002670019865036 Accuracy 0.375732421875\n",
      "Iteration 2680 Training loss 0.06257936358451843 Validation loss 0.06122655048966408 Accuracy 0.3828125\n",
      "Iteration 2690 Training loss 0.058907561004161835 Validation loss 0.06129096448421478 Accuracy 0.382568359375\n",
      "Iteration 2700 Training loss 0.06277383118867874 Validation loss 0.0619521290063858 Accuracy 0.37548828125\n",
      "Iteration 2710 Training loss 0.06104732304811478 Validation loss 0.06258106976747513 Accuracy 0.370361328125\n",
      "Iteration 2720 Training loss 0.05796518921852112 Validation loss 0.06137348338961601 Accuracy 0.382568359375\n",
      "Iteration 2730 Training loss 0.059293508529663086 Validation loss 0.061555009335279465 Accuracy 0.38037109375\n",
      "Iteration 2740 Training loss 0.061566539108753204 Validation loss 0.061494167894124985 Accuracy 0.382568359375\n",
      "Iteration 2750 Training loss 0.05821249261498451 Validation loss 0.0615902915596962 Accuracy 0.38134765625\n",
      "Iteration 2760 Training loss 0.0614095963537693 Validation loss 0.062975212931633 Accuracy 0.36767578125\n",
      "Iteration 2770 Training loss 0.06117112189531326 Validation loss 0.06176271662116051 Accuracy 0.378662109375\n",
      "Iteration 2780 Training loss 0.05855633318424225 Validation loss 0.0615900419652462 Accuracy 0.38037109375\n",
      "Iteration 2790 Training loss 0.0639413371682167 Validation loss 0.06248094514012337 Accuracy 0.373291015625\n",
      "Iteration 2800 Training loss 0.06260446459054947 Validation loss 0.06253065913915634 Accuracy 0.371337890625\n",
      "Iteration 2810 Training loss 0.06112275272607803 Validation loss 0.06161097064614296 Accuracy 0.3818359375\n",
      "Iteration 2820 Training loss 0.06475833058357239 Validation loss 0.061128851026296616 Accuracy 0.385009765625\n",
      "Iteration 2830 Training loss 0.060731496661901474 Validation loss 0.06122457981109619 Accuracy 0.3857421875\n",
      "Iteration 2840 Training loss 0.06510581076145172 Validation loss 0.06162354350090027 Accuracy 0.38232421875\n",
      "Iteration 2850 Training loss 0.05939977988600731 Validation loss 0.06167266517877579 Accuracy 0.3818359375\n",
      "Iteration 2860 Training loss 0.06048210710287094 Validation loss 0.06147334724664688 Accuracy 0.3837890625\n",
      "Iteration 2870 Training loss 0.05802134796977043 Validation loss 0.06121779978275299 Accuracy 0.3857421875\n",
      "Iteration 2880 Training loss 0.062283292412757874 Validation loss 0.0611722394824028 Accuracy 0.385498046875\n",
      "Iteration 2890 Training loss 0.06415450572967529 Validation loss 0.061778221279382706 Accuracy 0.381103515625\n",
      "Iteration 2900 Training loss 0.058627452701330185 Validation loss 0.06118112802505493 Accuracy 0.384521484375\n",
      "Iteration 2910 Training loss 0.06334862858057022 Validation loss 0.06154220551252365 Accuracy 0.381591796875\n",
      "Iteration 2920 Training loss 0.06068137660622597 Validation loss 0.06144697591662407 Accuracy 0.38330078125\n",
      "Iteration 2930 Training loss 0.06126343831419945 Validation loss 0.06243954598903656 Accuracy 0.372802734375\n",
      "Iteration 2940 Training loss 0.05811047926545143 Validation loss 0.061227887868881226 Accuracy 0.38427734375\n",
      "Iteration 2950 Training loss 0.06203186884522438 Validation loss 0.061082873493433 Accuracy 0.385986328125\n",
      "Iteration 2960 Training loss 0.05871753767132759 Validation loss 0.06127851828932762 Accuracy 0.385009765625\n",
      "Iteration 2970 Training loss 0.06067468225955963 Validation loss 0.06135697662830353 Accuracy 0.3828125\n",
      "Iteration 2980 Training loss 0.06110954284667969 Validation loss 0.061166733503341675 Accuracy 0.385009765625\n",
      "Iteration 2990 Training loss 0.062137421220541 Validation loss 0.061334967613220215 Accuracy 0.3828125\n",
      "Iteration 3000 Training loss 0.06444761902093887 Validation loss 0.06132418289780617 Accuracy 0.38232421875\n",
      "Iteration 3010 Training loss 0.05919363722205162 Validation loss 0.06109168380498886 Accuracy 0.38525390625\n",
      "Iteration 3020 Training loss 0.06118636578321457 Validation loss 0.061317961663007736 Accuracy 0.38330078125\n",
      "Iteration 3030 Training loss 0.06192416325211525 Validation loss 0.06166740134358406 Accuracy 0.380126953125\n",
      "Iteration 3040 Training loss 0.061675842851400375 Validation loss 0.06171690300107002 Accuracy 0.379638671875\n",
      "Iteration 3050 Training loss 0.06495697796344757 Validation loss 0.06126938387751579 Accuracy 0.38427734375\n",
      "Iteration 3060 Training loss 0.06437855213880539 Validation loss 0.06116185337305069 Accuracy 0.385009765625\n",
      "Iteration 3070 Training loss 0.05759882554411888 Validation loss 0.06118816137313843 Accuracy 0.384521484375\n",
      "Iteration 3080 Training loss 0.05655236169695854 Validation loss 0.06118180602788925 Accuracy 0.385009765625\n",
      "Iteration 3090 Training loss 0.06350190192461014 Validation loss 0.06157363951206207 Accuracy 0.382080078125\n",
      "Iteration 3100 Training loss 0.06134757027029991 Validation loss 0.06130237132310867 Accuracy 0.384521484375\n",
      "Iteration 3110 Training loss 0.058159951120615005 Validation loss 0.061371296644210815 Accuracy 0.383056640625\n",
      "Iteration 3120 Training loss 0.0593370720744133 Validation loss 0.06138097494840622 Accuracy 0.384033203125\n",
      "Iteration 3130 Training loss 0.06415412575006485 Validation loss 0.061461396515369415 Accuracy 0.38330078125\n",
      "Iteration 3140 Training loss 0.06351548433303833 Validation loss 0.06124526262283325 Accuracy 0.38525390625\n",
      "Iteration 3150 Training loss 0.06018265709280968 Validation loss 0.06129898875951767 Accuracy 0.385009765625\n",
      "Iteration 3160 Training loss 0.06176214665174484 Validation loss 0.061200857162475586 Accuracy 0.384765625\n",
      "Iteration 3170 Training loss 0.0614425353705883 Validation loss 0.0612839013338089 Accuracy 0.385009765625\n",
      "Iteration 3180 Training loss 0.062528096139431 Validation loss 0.06145220994949341 Accuracy 0.38330078125\n",
      "Iteration 3190 Training loss 0.06122031435370445 Validation loss 0.06169099360704422 Accuracy 0.381591796875\n",
      "Iteration 3200 Training loss 0.06384416669607162 Validation loss 0.06130821257829666 Accuracy 0.3837890625\n",
      "Iteration 3210 Training loss 0.06099637970328331 Validation loss 0.061116356402635574 Accuracy 0.385498046875\n",
      "Iteration 3220 Training loss 0.0570160448551178 Validation loss 0.06128521263599396 Accuracy 0.385009765625\n",
      "Iteration 3230 Training loss 0.06174303963780403 Validation loss 0.06152782961726189 Accuracy 0.382080078125\n",
      "Iteration 3240 Training loss 0.05811203271150589 Validation loss 0.06134258955717087 Accuracy 0.3837890625\n",
      "Iteration 3250 Training loss 0.05781476944684982 Validation loss 0.061189550906419754 Accuracy 0.3857421875\n",
      "Iteration 3260 Training loss 0.0613967590034008 Validation loss 0.06182558834552765 Accuracy 0.378662109375\n",
      "Iteration 3270 Training loss 0.06017503887414932 Validation loss 0.06104778125882149 Accuracy 0.386962890625\n",
      "Iteration 3280 Training loss 0.06051269918680191 Validation loss 0.06146244332194328 Accuracy 0.381103515625\n",
      "Iteration 3290 Training loss 0.059136763215065 Validation loss 0.061065368354320526 Accuracy 0.38671875\n",
      "Iteration 3300 Training loss 0.06211116537451744 Validation loss 0.06130781024694443 Accuracy 0.385009765625\n",
      "Iteration 3310 Training loss 0.06197291612625122 Validation loss 0.061330828815698624 Accuracy 0.383056640625\n",
      "Iteration 3320 Training loss 0.06278657913208008 Validation loss 0.06268342584371567 Accuracy 0.36962890625\n",
      "Iteration 3330 Training loss 0.06093983352184296 Validation loss 0.061167653650045395 Accuracy 0.384765625\n",
      "Iteration 3340 Training loss 0.0646175816655159 Validation loss 0.06167543679475784 Accuracy 0.382080078125\n",
      "Iteration 3350 Training loss 0.0627027302980423 Validation loss 0.061459802091121674 Accuracy 0.383544921875\n",
      "Iteration 3360 Training loss 0.056548237800598145 Validation loss 0.05519186332821846 Accuracy 0.445068359375\n",
      "Iteration 3370 Training loss 0.057088445872068405 Validation loss 0.05489534139633179 Accuracy 0.44873046875\n",
      "Iteration 3380 Training loss 0.050530821084976196 Validation loss 0.054285019636154175 Accuracy 0.45458984375\n",
      "Iteration 3390 Training loss 0.05284414067864418 Validation loss 0.05354375019669533 Accuracy 0.4619140625\n",
      "Iteration 3400 Training loss 0.055678486824035645 Validation loss 0.05345776677131653 Accuracy 0.463623046875\n",
      "Iteration 3410 Training loss 0.05499023199081421 Validation loss 0.0532764308154583 Accuracy 0.464599609375\n",
      "Iteration 3420 Training loss 0.053867045789957047 Validation loss 0.05345506593585014 Accuracy 0.4638671875\n",
      "Iteration 3430 Training loss 0.056030649691820145 Validation loss 0.05341716110706329 Accuracy 0.463134765625\n",
      "Iteration 3440 Training loss 0.05506303161382675 Validation loss 0.05301117151975632 Accuracy 0.467529296875\n",
      "Iteration 3450 Training loss 0.05523405969142914 Validation loss 0.052840203046798706 Accuracy 0.469482421875\n",
      "Iteration 3460 Training loss 0.052478112280368805 Validation loss 0.05403773486614227 Accuracy 0.4580078125\n",
      "Iteration 3470 Training loss 0.052560094743967056 Validation loss 0.05269533395767212 Accuracy 0.470703125\n",
      "Iteration 3480 Training loss 0.05548984557390213 Validation loss 0.05272790789604187 Accuracy 0.469970703125\n",
      "Iteration 3490 Training loss 0.050412822514772415 Validation loss 0.05276774242520332 Accuracy 0.470703125\n",
      "Iteration 3500 Training loss 0.05237458273768425 Validation loss 0.0526011623442173 Accuracy 0.4716796875\n",
      "Iteration 3510 Training loss 0.05185665935277939 Validation loss 0.05291905999183655 Accuracy 0.46923828125\n",
      "Iteration 3520 Training loss 0.050958652049303055 Validation loss 0.053050920367240906 Accuracy 0.46728515625\n",
      "Iteration 3530 Training loss 0.054816700518131256 Validation loss 0.052696920931339264 Accuracy 0.470947265625\n",
      "Iteration 3540 Training loss 0.0566725879907608 Validation loss 0.05424826219677925 Accuracy 0.456298828125\n",
      "Iteration 3550 Training loss 0.051826801151037216 Validation loss 0.05311167240142822 Accuracy 0.466796875\n",
      "Iteration 3560 Training loss 0.05051837116479874 Validation loss 0.05274757370352745 Accuracy 0.47021484375\n",
      "Iteration 3570 Training loss 0.052720438688993454 Validation loss 0.05273052677512169 Accuracy 0.470703125\n",
      "Iteration 3580 Training loss 0.05037475377321243 Validation loss 0.052974577993154526 Accuracy 0.46875\n",
      "Iteration 3590 Training loss 0.051652852445840836 Validation loss 0.0525863841176033 Accuracy 0.471923828125\n",
      "Iteration 3600 Training loss 0.05157936364412308 Validation loss 0.053051162511110306 Accuracy 0.468505859375\n",
      "Iteration 3610 Training loss 0.05120137333869934 Validation loss 0.0534822940826416 Accuracy 0.4619140625\n",
      "Iteration 3620 Training loss 0.050834767520427704 Validation loss 0.05340787023305893 Accuracy 0.4638671875\n",
      "Iteration 3630 Training loss 0.05305740237236023 Validation loss 0.053399767726659775 Accuracy 0.464111328125\n",
      "Iteration 3640 Training loss 0.05327731370925903 Validation loss 0.052648499608039856 Accuracy 0.471435546875\n",
      "Iteration 3650 Training loss 0.05721479281783104 Validation loss 0.05257878825068474 Accuracy 0.47216796875\n",
      "Iteration 3660 Training loss 0.053512927144765854 Validation loss 0.05262576416134834 Accuracy 0.47119140625\n",
      "Iteration 3670 Training loss 0.05068053677678108 Validation loss 0.05350151285529137 Accuracy 0.46337890625\n",
      "Iteration 3680 Training loss 0.05127726122736931 Validation loss 0.052599236369132996 Accuracy 0.471923828125\n",
      "Iteration 3690 Training loss 0.05427267402410507 Validation loss 0.05280442163348198 Accuracy 0.469970703125\n",
      "Iteration 3700 Training loss 0.05497928708791733 Validation loss 0.05351787060499191 Accuracy 0.462890625\n",
      "Iteration 3710 Training loss 0.05343614146113396 Validation loss 0.052809812128543854 Accuracy 0.469970703125\n",
      "Iteration 3720 Training loss 0.048992425203323364 Validation loss 0.05274173244833946 Accuracy 0.470703125\n",
      "Iteration 3730 Training loss 0.05305762216448784 Validation loss 0.05331265553832054 Accuracy 0.464599609375\n",
      "Iteration 3740 Training loss 0.051560670137405396 Validation loss 0.05467524752020836 Accuracy 0.44921875\n",
      "Iteration 3750 Training loss 0.05420340597629547 Validation loss 0.05263923108577728 Accuracy 0.4716796875\n",
      "Iteration 3760 Training loss 0.05111518129706383 Validation loss 0.052976787090301514 Accuracy 0.468505859375\n",
      "Iteration 3770 Training loss 0.05155232176184654 Validation loss 0.05314444378018379 Accuracy 0.46630859375\n",
      "Iteration 3780 Training loss 0.0533936470746994 Validation loss 0.05254673212766647 Accuracy 0.471923828125\n",
      "Iteration 3790 Training loss 0.05198604241013527 Validation loss 0.05253998935222626 Accuracy 0.472900390625\n",
      "Iteration 3800 Training loss 0.052153680473566055 Validation loss 0.05265237018465996 Accuracy 0.4716796875\n",
      "Iteration 3810 Training loss 0.052593130618333817 Validation loss 0.052528899163007736 Accuracy 0.472900390625\n",
      "Iteration 3820 Training loss 0.052614323794841766 Validation loss 0.05250333249568939 Accuracy 0.472900390625\n",
      "Iteration 3830 Training loss 0.04858184605836868 Validation loss 0.05281458795070648 Accuracy 0.46923828125\n",
      "Iteration 3840 Training loss 0.04830420762300491 Validation loss 0.05272859334945679 Accuracy 0.469970703125\n",
      "Iteration 3850 Training loss 0.05227158963680267 Validation loss 0.05295512452721596 Accuracy 0.46923828125\n",
      "Iteration 3860 Training loss 0.05023953318595886 Validation loss 0.05293605476617813 Accuracy 0.46875\n",
      "Iteration 3870 Training loss 0.05184418708086014 Validation loss 0.052608367055654526 Accuracy 0.4716796875\n",
      "Iteration 3880 Training loss 0.0523306168615818 Validation loss 0.05340433493256569 Accuracy 0.464599609375\n",
      "Iteration 3890 Training loss 0.054719533771276474 Validation loss 0.05307382717728615 Accuracy 0.467041015625\n",
      "Iteration 3900 Training loss 0.052548542618751526 Validation loss 0.05296742171049118 Accuracy 0.468017578125\n",
      "Iteration 3910 Training loss 0.052355121821165085 Validation loss 0.052690066397190094 Accuracy 0.47021484375\n",
      "Iteration 3920 Training loss 0.05431956797838211 Validation loss 0.05240446329116821 Accuracy 0.473388671875\n",
      "Iteration 3930 Training loss 0.04939424619078636 Validation loss 0.052569642663002014 Accuracy 0.470947265625\n",
      "Iteration 3940 Training loss 0.05484160780906677 Validation loss 0.05330298840999603 Accuracy 0.464599609375\n",
      "Iteration 3950 Training loss 0.0531114786863327 Validation loss 0.05286020413041115 Accuracy 0.468994140625\n",
      "Iteration 3960 Training loss 0.049692098051309586 Validation loss 0.053208135068416595 Accuracy 0.464599609375\n",
      "Iteration 3970 Training loss 0.05126228928565979 Validation loss 0.05277349427342415 Accuracy 0.469970703125\n",
      "Iteration 3980 Training loss 0.046081166714429855 Validation loss 0.0525723434984684 Accuracy 0.4716796875\n",
      "Iteration 3990 Training loss 0.05033772066235542 Validation loss 0.05284598469734192 Accuracy 0.46875\n",
      "Iteration 4000 Training loss 0.0527382418513298 Validation loss 0.052434902638196945 Accuracy 0.473388671875\n",
      "Iteration 4010 Training loss 0.0532485656440258 Validation loss 0.05239573121070862 Accuracy 0.473388671875\n",
      "Iteration 4020 Training loss 0.05038578808307648 Validation loss 0.05299913138151169 Accuracy 0.468505859375\n",
      "Iteration 4030 Training loss 0.05262576416134834 Validation loss 0.053071923553943634 Accuracy 0.4677734375\n",
      "Iteration 4040 Training loss 0.05268166959285736 Validation loss 0.05263007804751396 Accuracy 0.470703125\n",
      "Iteration 4050 Training loss 0.05395613610744476 Validation loss 0.052913401275873184 Accuracy 0.4677734375\n",
      "Iteration 4060 Training loss 0.05115882307291031 Validation loss 0.052639275789260864 Accuracy 0.470703125\n",
      "Iteration 4070 Training loss 0.05744246393442154 Validation loss 0.05243806168437004 Accuracy 0.472412109375\n",
      "Iteration 4080 Training loss 0.053704190999269485 Validation loss 0.05251167714595795 Accuracy 0.47119140625\n",
      "Iteration 4090 Training loss 0.0471007265150547 Validation loss 0.05266042798757553 Accuracy 0.470458984375\n",
      "Iteration 4100 Training loss 0.052115269005298615 Validation loss 0.053041722625494 Accuracy 0.46630859375\n",
      "Iteration 4110 Training loss 0.0509251207113266 Validation loss 0.05229213833808899 Accuracy 0.473876953125\n",
      "Iteration 4120 Training loss 0.05605437234044075 Validation loss 0.052179910242557526 Accuracy 0.474609375\n",
      "Iteration 4130 Training loss 0.05459189787507057 Validation loss 0.052677225321531296 Accuracy 0.47119140625\n",
      "Iteration 4140 Training loss 0.05179424583911896 Validation loss 0.053095944225788116 Accuracy 0.467041015625\n",
      "Iteration 4150 Training loss 0.05127905681729317 Validation loss 0.05312662944197655 Accuracy 0.465087890625\n",
      "Iteration 4160 Training loss 0.054529231041669846 Validation loss 0.05248827114701271 Accuracy 0.470703125\n",
      "Iteration 4170 Training loss 0.048797424882650375 Validation loss 0.05259234830737114 Accuracy 0.469970703125\n",
      "Iteration 4180 Training loss 0.05837306007742882 Validation loss 0.05242489278316498 Accuracy 0.472412109375\n",
      "Iteration 4190 Training loss 0.054320327937603 Validation loss 0.05234629288315773 Accuracy 0.47216796875\n",
      "Iteration 4200 Training loss 0.0518704317510128 Validation loss 0.052289873361587524 Accuracy 0.472900390625\n",
      "Iteration 4210 Training loss 0.05297376587986946 Validation loss 0.05226224660873413 Accuracy 0.473388671875\n",
      "Iteration 4220 Training loss 0.05505865439772606 Validation loss 0.052348241209983826 Accuracy 0.47216796875\n",
      "Iteration 4230 Training loss 0.05167749151587486 Validation loss 0.05212812498211861 Accuracy 0.474609375\n",
      "Iteration 4240 Training loss 0.057543493807315826 Validation loss 0.05237344279885292 Accuracy 0.47265625\n",
      "Iteration 4250 Training loss 0.050895243883132935 Validation loss 0.05302318185567856 Accuracy 0.466796875\n",
      "Iteration 4260 Training loss 0.054175201803445816 Validation loss 0.05356704443693161 Accuracy 0.461181640625\n",
      "Iteration 4270 Training loss 0.05243002250790596 Validation loss 0.052914608269929886 Accuracy 0.468505859375\n",
      "Iteration 4280 Training loss 0.054698407649993896 Validation loss 0.052845779806375504 Accuracy 0.46923828125\n",
      "Iteration 4290 Training loss 0.04992816224694252 Validation loss 0.052664823830127716 Accuracy 0.470703125\n",
      "Iteration 4300 Training loss 0.052543818950653076 Validation loss 0.052468299865722656 Accuracy 0.471923828125\n",
      "Iteration 4310 Training loss 0.05477188900113106 Validation loss 0.052420180290937424 Accuracy 0.4716796875\n",
      "Iteration 4320 Training loss 0.05128263309597969 Validation loss 0.053399648517370224 Accuracy 0.461181640625\n",
      "Iteration 4330 Training loss 0.05045231059193611 Validation loss 0.052583884447813034 Accuracy 0.469970703125\n",
      "Iteration 4340 Training loss 0.052052877843379974 Validation loss 0.05279266834259033 Accuracy 0.468017578125\n",
      "Iteration 4350 Training loss 0.05218592658638954 Validation loss 0.05271751806139946 Accuracy 0.46923828125\n",
      "Iteration 4360 Training loss 0.04954244941473007 Validation loss 0.05286579951643944 Accuracy 0.468017578125\n",
      "Iteration 4370 Training loss 0.050787150859832764 Validation loss 0.05232495069503784 Accuracy 0.473388671875\n",
      "Iteration 4380 Training loss 0.054525554180145264 Validation loss 0.05230918899178505 Accuracy 0.4736328125\n",
      "Iteration 4390 Training loss 0.055639490485191345 Validation loss 0.05211132764816284 Accuracy 0.47509765625\n",
      "Iteration 4400 Training loss 0.051810137927532196 Validation loss 0.05231517553329468 Accuracy 0.473388671875\n",
      "Iteration 4410 Training loss 0.04859361797571182 Validation loss 0.052168577909469604 Accuracy 0.474853515625\n",
      "Iteration 4420 Training loss 0.05466519668698311 Validation loss 0.05200117453932762 Accuracy 0.47607421875\n",
      "Iteration 4430 Training loss 0.05326671898365021 Validation loss 0.05212608724832535 Accuracy 0.474609375\n",
      "Iteration 4440 Training loss 0.05520514026284218 Validation loss 0.05242707207798958 Accuracy 0.472900390625\n",
      "Iteration 4450 Training loss 0.053288258612155914 Validation loss 0.05315761640667915 Accuracy 0.465087890625\n",
      "Iteration 4460 Training loss 0.054502081125974655 Validation loss 0.053067516535520554 Accuracy 0.46533203125\n",
      "Iteration 4470 Training loss 0.051012735813856125 Validation loss 0.05223380774259567 Accuracy 0.473388671875\n",
      "Iteration 4480 Training loss 0.055893804877996445 Validation loss 0.053449928760528564 Accuracy 0.46240234375\n",
      "Iteration 4490 Training loss 0.05228769779205322 Validation loss 0.0521930456161499 Accuracy 0.474853515625\n",
      "Iteration 4500 Training loss 0.05445140600204468 Validation loss 0.05332063511013985 Accuracy 0.464599609375\n",
      "Iteration 4510 Training loss 0.05013288930058479 Validation loss 0.05228138715028763 Accuracy 0.472412109375\n",
      "Iteration 4520 Training loss 0.054805830121040344 Validation loss 0.05254334211349487 Accuracy 0.47021484375\n",
      "Iteration 4530 Training loss 0.05085491016507149 Validation loss 0.05210375413298607 Accuracy 0.4755859375\n",
      "Iteration 4540 Training loss 0.05327468737959862 Validation loss 0.05227573961019516 Accuracy 0.47412109375\n",
      "Iteration 4550 Training loss 0.05318427458405495 Validation loss 0.052643466740846634 Accuracy 0.470703125\n",
      "Iteration 4560 Training loss 0.05049952119588852 Validation loss 0.05240784212946892 Accuracy 0.47265625\n",
      "Iteration 4570 Training loss 0.052858274430036545 Validation loss 0.05310681089758873 Accuracy 0.465576171875\n",
      "Iteration 4580 Training loss 0.05491029471158981 Validation loss 0.05265739932656288 Accuracy 0.470458984375\n",
      "Iteration 4590 Training loss 0.05096408352255821 Validation loss 0.052212443202733994 Accuracy 0.474365234375\n",
      "Iteration 4600 Training loss 0.05127161741256714 Validation loss 0.0522400364279747 Accuracy 0.47412109375\n",
      "Iteration 4610 Training loss 0.05261845886707306 Validation loss 0.05273928493261337 Accuracy 0.469970703125\n",
      "Iteration 4620 Training loss 0.052480533719062805 Validation loss 0.052174162119627 Accuracy 0.474853515625\n",
      "Iteration 4630 Training loss 0.052191488444805145 Validation loss 0.05318928137421608 Accuracy 0.46630859375\n",
      "Iteration 4640 Training loss 0.05201161652803421 Validation loss 0.05259501934051514 Accuracy 0.471923828125\n",
      "Iteration 4650 Training loss 0.05215970799326897 Validation loss 0.05262136459350586 Accuracy 0.4716796875\n",
      "Iteration 4660 Training loss 0.05208619311451912 Validation loss 0.052606355398893356 Accuracy 0.4716796875\n",
      "Iteration 4670 Training loss 0.051966842263936996 Validation loss 0.05221100524067879 Accuracy 0.474853515625\n",
      "Iteration 4680 Training loss 0.04947218671441078 Validation loss 0.05226042866706848 Accuracy 0.47509765625\n",
      "Iteration 4690 Training loss 0.053752634674310684 Validation loss 0.05233931168913841 Accuracy 0.472900390625\n",
      "Iteration 4700 Training loss 0.052171315997838974 Validation loss 0.052368368953466415 Accuracy 0.472900390625\n",
      "Iteration 4710 Training loss 0.052296243607997894 Validation loss 0.052771247923374176 Accuracy 0.469482421875\n",
      "Iteration 4720 Training loss 0.0528574101626873 Validation loss 0.05294567719101906 Accuracy 0.46630859375\n",
      "Iteration 4730 Training loss 0.04949810355901718 Validation loss 0.05234783887863159 Accuracy 0.4736328125\n",
      "Iteration 4740 Training loss 0.04436717554926872 Validation loss 0.04539083316922188 Accuracy 0.54150390625\n",
      "Iteration 4750 Training loss 0.04086260497570038 Validation loss 0.04555503651499748 Accuracy 0.5419921875\n",
      "Iteration 4760 Training loss 0.046451445668935776 Validation loss 0.04470350593328476 Accuracy 0.54833984375\n",
      "Iteration 4770 Training loss 0.04604858532547951 Validation loss 0.04548073932528496 Accuracy 0.5390625\n",
      "Iteration 4780 Training loss 0.04953023046255112 Validation loss 0.04571780189871788 Accuracy 0.5390625\n",
      "Iteration 4790 Training loss 0.04583849012851715 Validation loss 0.044544901698827744 Accuracy 0.55029296875\n",
      "Iteration 4800 Training loss 0.045205626636743546 Validation loss 0.04455706104636192 Accuracy 0.55126953125\n",
      "Iteration 4810 Training loss 0.041058141738176346 Validation loss 0.04438895359635353 Accuracy 0.55126953125\n",
      "Iteration 4820 Training loss 0.046654753386974335 Validation loss 0.044307079166173935 Accuracy 0.552734375\n",
      "Iteration 4830 Training loss 0.04175594449043274 Validation loss 0.044625137001276016 Accuracy 0.55078125\n",
      "Iteration 4840 Training loss 0.04435684531927109 Validation loss 0.04410974681377411 Accuracy 0.55419921875\n",
      "Iteration 4850 Training loss 0.04125373810529709 Validation loss 0.0454762727022171 Accuracy 0.5419921875\n",
      "Iteration 4860 Training loss 0.048088982701301575 Validation loss 0.04456474632024765 Accuracy 0.55078125\n",
      "Iteration 4870 Training loss 0.04446323961019516 Validation loss 0.044587742537260056 Accuracy 0.55029296875\n",
      "Iteration 4880 Training loss 0.04419764503836632 Validation loss 0.045454081147909164 Accuracy 0.54296875\n",
      "Iteration 4890 Training loss 0.04271752014756203 Validation loss 0.04454898461699486 Accuracy 0.5498046875\n",
      "Iteration 4900 Training loss 0.04475310072302818 Validation loss 0.04466373100876808 Accuracy 0.54931640625\n",
      "Iteration 4910 Training loss 0.04666561260819435 Validation loss 0.04419964551925659 Accuracy 0.55517578125\n",
      "Iteration 4920 Training loss 0.03989559784531593 Validation loss 0.04449980705976486 Accuracy 0.5517578125\n",
      "Iteration 4930 Training loss 0.04630608484148979 Validation loss 0.044227972626686096 Accuracy 0.55419921875\n",
      "Iteration 4940 Training loss 0.044582489877939224 Validation loss 0.04469374567270279 Accuracy 0.548828125\n",
      "Iteration 4950 Training loss 0.04359535127878189 Validation loss 0.044262856245040894 Accuracy 0.55419921875\n",
      "Iteration 4960 Training loss 0.046470507979393005 Validation loss 0.04418860748410225 Accuracy 0.5556640625\n",
      "Iteration 4970 Training loss 0.04023756831884384 Validation loss 0.04438367113471031 Accuracy 0.55224609375\n",
      "Iteration 4980 Training loss 0.04739046096801758 Validation loss 0.044231776148080826 Accuracy 0.55224609375\n",
      "Iteration 4990 Training loss 0.04557628184556961 Validation loss 0.043782565742731094 Accuracy 0.5576171875\n",
      "Iteration 5000 Training loss 0.04471297189593315 Validation loss 0.04453069344162941 Accuracy 0.55126953125\n",
      "Iteration 5010 Training loss 0.04478570073843002 Validation loss 0.043945878744125366 Accuracy 0.55615234375\n",
      "Iteration 5020 Training loss 0.04588751494884491 Validation loss 0.0441003181040287 Accuracy 0.55419921875\n",
      "Iteration 5030 Training loss 0.04764940217137337 Validation loss 0.04525403305888176 Accuracy 0.544921875\n",
      "Iteration 5040 Training loss 0.04361152648925781 Validation loss 0.04372304677963257 Accuracy 0.55908203125\n",
      "Iteration 5050 Training loss 0.04379141330718994 Validation loss 0.04501995071768761 Accuracy 0.54541015625\n",
      "Iteration 5060 Training loss 0.0445907823741436 Validation loss 0.04418480768799782 Accuracy 0.55419921875\n",
      "Iteration 5070 Training loss 0.04253649711608887 Validation loss 0.0442747101187706 Accuracy 0.55322265625\n",
      "Iteration 5080 Training loss 0.048708364367485046 Validation loss 0.04444570094347 Accuracy 0.55224609375\n",
      "Iteration 5090 Training loss 0.04433014988899231 Validation loss 0.044603895395994186 Accuracy 0.54931640625\n",
      "Iteration 5100 Training loss 0.04466957226395607 Validation loss 0.043983057141304016 Accuracy 0.556640625\n",
      "Iteration 5110 Training loss 0.04661884903907776 Validation loss 0.044799432158470154 Accuracy 0.54931640625\n",
      "Iteration 5120 Training loss 0.041263382881879807 Validation loss 0.044520653784275055 Accuracy 0.55078125\n",
      "Iteration 5130 Training loss 0.04116268455982208 Validation loss 0.04378310590982437 Accuracy 0.55859375\n",
      "Iteration 5140 Training loss 0.041770417243242264 Validation loss 0.043895844370126724 Accuracy 0.55810546875\n",
      "Iteration 5150 Training loss 0.04401552304625511 Validation loss 0.04395119845867157 Accuracy 0.556640625\n",
      "Iteration 5160 Training loss 0.04312827065587044 Validation loss 0.043677255511283875 Accuracy 0.55908203125\n",
      "Iteration 5170 Training loss 0.043992094695568085 Validation loss 0.04356935992836952 Accuracy 0.56005859375\n",
      "Iteration 5180 Training loss 0.040891848504543304 Validation loss 0.04421674832701683 Accuracy 0.5546875\n",
      "Iteration 5190 Training loss 0.046239059418439865 Validation loss 0.04352511465549469 Accuracy 0.56005859375\n",
      "Iteration 5200 Training loss 0.0417119637131691 Validation loss 0.04415270313620567 Accuracy 0.556640625\n",
      "Iteration 5210 Training loss 0.04512948542833328 Validation loss 0.043989941477775574 Accuracy 0.5576171875\n",
      "Iteration 5220 Training loss 0.04038636013865471 Validation loss 0.04412702098488808 Accuracy 0.556640625\n",
      "Iteration 5230 Training loss 0.04728583246469498 Validation loss 0.0435740128159523 Accuracy 0.56103515625\n",
      "Iteration 5240 Training loss 0.038041722029447556 Validation loss 0.04369714856147766 Accuracy 0.560546875\n",
      "Iteration 5250 Training loss 0.04405684396624565 Validation loss 0.0437377393245697 Accuracy 0.5595703125\n",
      "Iteration 5260 Training loss 0.044598549604415894 Validation loss 0.04447448253631592 Accuracy 0.5517578125\n",
      "Iteration 5270 Training loss 0.04182332754135132 Validation loss 0.044219374656677246 Accuracy 0.552734375\n",
      "Iteration 5280 Training loss 0.04247516766190529 Validation loss 0.04350894317030907 Accuracy 0.5615234375\n",
      "Iteration 5290 Training loss 0.04334248974919319 Validation loss 0.043865811079740524 Accuracy 0.55712890625\n",
      "Iteration 5300 Training loss 0.040974561125040054 Validation loss 0.04432303458452225 Accuracy 0.55419921875\n",
      "Iteration 5310 Training loss 0.04236410930752754 Validation loss 0.044356297701597214 Accuracy 0.55322265625\n",
      "Iteration 5320 Training loss 0.04407939687371254 Validation loss 0.044364530593156815 Accuracy 0.55322265625\n",
      "Iteration 5330 Training loss 0.04262394830584526 Validation loss 0.043736692517995834 Accuracy 0.5595703125\n",
      "Iteration 5340 Training loss 0.044470809400081635 Validation loss 0.043863240629434586 Accuracy 0.55810546875\n",
      "Iteration 5350 Training loss 0.0431334413588047 Validation loss 0.04346437007188797 Accuracy 0.5625\n",
      "Iteration 5360 Training loss 0.04219779372215271 Validation loss 0.04466306418180466 Accuracy 0.5498046875\n",
      "Iteration 5370 Training loss 0.04603058472275734 Validation loss 0.04488428682088852 Accuracy 0.548828125\n",
      "Iteration 5380 Training loss 0.043323494493961334 Validation loss 0.04412408173084259 Accuracy 0.55517578125\n",
      "Iteration 5390 Training loss 0.04377451539039612 Validation loss 0.043885715305805206 Accuracy 0.5576171875\n",
      "Iteration 5400 Training loss 0.046182312071323395 Validation loss 0.04438774287700653 Accuracy 0.552734375\n",
      "Iteration 5410 Training loss 0.04381490498781204 Validation loss 0.043799128383398056 Accuracy 0.55859375\n",
      "Iteration 5420 Training loss 0.04196873679757118 Validation loss 0.04411875829100609 Accuracy 0.5546875\n",
      "Iteration 5430 Training loss 0.045073602348566055 Validation loss 0.04380885139107704 Accuracy 0.55859375\n",
      "Iteration 5440 Training loss 0.042943622916936874 Validation loss 0.04447697103023529 Accuracy 0.55126953125\n",
      "Iteration 5450 Training loss 0.0413016639649868 Validation loss 0.044040314853191376 Accuracy 0.556640625\n",
      "Iteration 5460 Training loss 0.04333651065826416 Validation loss 0.043805334717035294 Accuracy 0.55810546875\n",
      "Iteration 5470 Training loss 0.04132108390331268 Validation loss 0.043536774814128876 Accuracy 0.56103515625\n",
      "Iteration 5480 Training loss 0.04255499690771103 Validation loss 0.04368407651782036 Accuracy 0.56005859375\n",
      "Iteration 5490 Training loss 0.04265180230140686 Validation loss 0.044084686785936356 Accuracy 0.55712890625\n",
      "Iteration 5500 Training loss 0.043997183442115784 Validation loss 0.04457565397024155 Accuracy 0.5517578125\n",
      "Iteration 5510 Training loss 0.04347763955593109 Validation loss 0.04367905110120773 Accuracy 0.55810546875\n",
      "Iteration 5520 Training loss 0.038476791232824326 Validation loss 0.04368794709444046 Accuracy 0.55859375\n",
      "Iteration 5530 Training loss 0.0482686348259449 Validation loss 0.04360039532184601 Accuracy 0.56005859375\n",
      "Iteration 5540 Training loss 0.04202372208237648 Validation loss 0.0433630608022213 Accuracy 0.5634765625\n",
      "Iteration 5550 Training loss 0.04350965842604637 Validation loss 0.04341650754213333 Accuracy 0.56298828125\n",
      "Iteration 5560 Training loss 0.043888550251722336 Validation loss 0.04325864091515541 Accuracy 0.56396484375\n",
      "Iteration 5570 Training loss 0.04138987883925438 Validation loss 0.04370856657624245 Accuracy 0.560546875\n",
      "Iteration 5580 Training loss 0.04515417292714119 Validation loss 0.043584052473306656 Accuracy 0.56201171875\n",
      "Iteration 5590 Training loss 0.04412772133946419 Validation loss 0.04382222518324852 Accuracy 0.5595703125\n",
      "Iteration 5600 Training loss 0.044861018657684326 Validation loss 0.04460776224732399 Accuracy 0.55078125\n",
      "Iteration 5610 Training loss 0.04132606461644173 Validation loss 0.044145915657281876 Accuracy 0.5556640625\n",
      "Iteration 5620 Training loss 0.04238157719373703 Validation loss 0.04395214840769768 Accuracy 0.55810546875\n",
      "Iteration 5630 Training loss 0.038971614092588425 Validation loss 0.04374897480010986 Accuracy 0.56005859375\n",
      "Iteration 5640 Training loss 0.04401645436882973 Validation loss 0.04381973668932915 Accuracy 0.55810546875\n",
      "Iteration 5650 Training loss 0.0396357923746109 Validation loss 0.043960947543382645 Accuracy 0.55615234375\n",
      "Iteration 5660 Training loss 0.04375302046537399 Validation loss 0.04348530247807503 Accuracy 0.56103515625\n",
      "Iteration 5670 Training loss 0.042186032980680466 Validation loss 0.043570443987846375 Accuracy 0.56201171875\n",
      "Iteration 5680 Training loss 0.04019151255488396 Validation loss 0.043562039732933044 Accuracy 0.56201171875\n",
      "Iteration 5690 Training loss 0.041603896766901016 Validation loss 0.04368690401315689 Accuracy 0.56005859375\n",
      "Iteration 5700 Training loss 0.04172369837760925 Validation loss 0.043451398611068726 Accuracy 0.5615234375\n",
      "Iteration 5710 Training loss 0.044425129890441895 Validation loss 0.04386981204152107 Accuracy 0.55712890625\n",
      "Iteration 5720 Training loss 0.0452781580388546 Validation loss 0.04344608262181282 Accuracy 0.56201171875\n",
      "Iteration 5730 Training loss 0.042325060814619064 Validation loss 0.043849147856235504 Accuracy 0.55810546875\n",
      "Iteration 5740 Training loss 0.043193668127059937 Validation loss 0.043524011969566345 Accuracy 0.56201171875\n",
      "Iteration 5750 Training loss 0.04155987501144409 Validation loss 0.043192069977521896 Accuracy 0.56396484375\n",
      "Iteration 5760 Training loss 0.042884133756160736 Validation loss 0.0433632954955101 Accuracy 0.56201171875\n",
      "Iteration 5770 Training loss 0.04203333333134651 Validation loss 0.04419220983982086 Accuracy 0.55517578125\n",
      "Iteration 5780 Training loss 0.04088945686817169 Validation loss 0.04332950711250305 Accuracy 0.56201171875\n",
      "Iteration 5790 Training loss 0.04360754415392876 Validation loss 0.043744370341300964 Accuracy 0.5595703125\n",
      "Iteration 5800 Training loss 0.04444760084152222 Validation loss 0.04373414069414139 Accuracy 0.5595703125\n",
      "Iteration 5810 Training loss 0.04451167583465576 Validation loss 0.043534960597753525 Accuracy 0.56103515625\n",
      "Iteration 5820 Training loss 0.04220427945256233 Validation loss 0.04331497102975845 Accuracy 0.56201171875\n",
      "Iteration 5830 Training loss 0.04350663349032402 Validation loss 0.043109871447086334 Accuracy 0.564453125\n",
      "Iteration 5840 Training loss 0.04298347607254982 Validation loss 0.043922461569309235 Accuracy 0.55810546875\n",
      "Iteration 5850 Training loss 0.0457274466753006 Validation loss 0.043401919305324554 Accuracy 0.5625\n",
      "Iteration 5860 Training loss 0.042126163840293884 Validation loss 0.043735381215810776 Accuracy 0.55859375\n",
      "Iteration 5870 Training loss 0.03963400796055794 Validation loss 0.043689701706171036 Accuracy 0.56005859375\n",
      "Iteration 5880 Training loss 0.04662974923849106 Validation loss 0.043467726558446884 Accuracy 0.56103515625\n",
      "Iteration 5890 Training loss 0.04625697806477547 Validation loss 0.043412577360868454 Accuracy 0.5615234375\n",
      "Iteration 5900 Training loss 0.04073823615908623 Validation loss 0.043788231909275055 Accuracy 0.5576171875\n",
      "Iteration 5910 Training loss 0.040241602808237076 Validation loss 0.04365519806742668 Accuracy 0.56005859375\n",
      "Iteration 5920 Training loss 0.04236207902431488 Validation loss 0.04401221498847008 Accuracy 0.55712890625\n",
      "Iteration 5930 Training loss 0.04318821802735329 Validation loss 0.04372284188866615 Accuracy 0.55859375\n",
      "Iteration 5940 Training loss 0.04121096432209015 Validation loss 0.043393418192863464 Accuracy 0.5615234375\n",
      "Iteration 5950 Training loss 0.0399416908621788 Validation loss 0.043370071798563004 Accuracy 0.56298828125\n",
      "Iteration 5960 Training loss 0.03984919562935829 Validation loss 0.043442077934741974 Accuracy 0.56201171875\n",
      "Iteration 5970 Training loss 0.0420224592089653 Validation loss 0.04318287968635559 Accuracy 0.56396484375\n",
      "Iteration 5980 Training loss 0.03865104913711548 Validation loss 0.04348977282643318 Accuracy 0.5615234375\n",
      "Iteration 5990 Training loss 0.040319304913282394 Validation loss 0.043897248804569244 Accuracy 0.55712890625\n",
      "Iteration 6000 Training loss 0.041340455412864685 Validation loss 0.043406255543231964 Accuracy 0.56201171875\n",
      "Iteration 6010 Training loss 0.04188816621899605 Validation loss 0.0431048609316349 Accuracy 0.56494140625\n",
      "Iteration 6020 Training loss 0.04473474621772766 Validation loss 0.04347003623843193 Accuracy 0.56103515625\n",
      "Iteration 6030 Training loss 0.04235639423131943 Validation loss 0.043317411094903946 Accuracy 0.5634765625\n",
      "Iteration 6040 Training loss 0.041518017649650574 Validation loss 0.04322236403822899 Accuracy 0.564453125\n",
      "Iteration 6050 Training loss 0.04028113931417465 Validation loss 0.04329052194952965 Accuracy 0.5634765625\n",
      "Iteration 6060 Training loss 0.04361194372177124 Validation loss 0.04336625710129738 Accuracy 0.56201171875\n",
      "Iteration 6070 Training loss 0.04182208701968193 Validation loss 0.04340977594256401 Accuracy 0.5615234375\n",
      "Iteration 6080 Training loss 0.0445362813770771 Validation loss 0.04424098879098892 Accuracy 0.55322265625\n",
      "Iteration 6090 Training loss 0.04441653564572334 Validation loss 0.04387187212705612 Accuracy 0.55810546875\n",
      "Iteration 6100 Training loss 0.041888296604156494 Validation loss 0.0429471954703331 Accuracy 0.56591796875\n",
      "Iteration 6110 Training loss 0.04101664200425148 Validation loss 0.04316054657101631 Accuracy 0.56396484375\n",
      "Iteration 6120 Training loss 0.044852934777736664 Validation loss 0.04452717676758766 Accuracy 0.5517578125\n",
      "Iteration 6130 Training loss 0.042047761380672455 Validation loss 0.0435156375169754 Accuracy 0.56201171875\n",
      "Iteration 6140 Training loss 0.045292530208826065 Validation loss 0.04328888654708862 Accuracy 0.5634765625\n",
      "Iteration 6150 Training loss 0.03871461749076843 Validation loss 0.04314723610877991 Accuracy 0.5634765625\n",
      "Iteration 6160 Training loss 0.04588177427649498 Validation loss 0.0436297170817852 Accuracy 0.560546875\n",
      "Iteration 6170 Training loss 0.04354516416788101 Validation loss 0.04290192946791649 Accuracy 0.56689453125\n",
      "Iteration 6180 Training loss 0.039368774741888046 Validation loss 0.042939167469739914 Accuracy 0.56591796875\n",
      "Iteration 6190 Training loss 0.042626772075891495 Validation loss 0.043153081089258194 Accuracy 0.564453125\n",
      "Iteration 6200 Training loss 0.03933604434132576 Validation loss 0.04323357343673706 Accuracy 0.56494140625\n",
      "Iteration 6210 Training loss 0.04624100774526596 Validation loss 0.04470814764499664 Accuracy 0.548828125\n",
      "Iteration 6220 Training loss 0.04621167108416557 Validation loss 0.04320153221487999 Accuracy 0.56494140625\n",
      "Iteration 6230 Training loss 0.04617409035563469 Validation loss 0.04517027735710144 Accuracy 0.54248046875\n",
      "Iteration 6240 Training loss 0.04167252406477928 Validation loss 0.04288971424102783 Accuracy 0.56689453125\n",
      "Iteration 6250 Training loss 0.04258580505847931 Validation loss 0.04323044419288635 Accuracy 0.56201171875\n",
      "Iteration 6260 Training loss 0.042443446815013885 Validation loss 0.043160490691661835 Accuracy 0.56396484375\n",
      "Iteration 6270 Training loss 0.04317058250308037 Validation loss 0.042869798839092255 Accuracy 0.56689453125\n",
      "Iteration 6280 Training loss 0.04341118782758713 Validation loss 0.04355233907699585 Accuracy 0.560546875\n",
      "Iteration 6290 Training loss 0.038740646094083786 Validation loss 0.043162256479263306 Accuracy 0.56494140625\n",
      "Iteration 6300 Training loss 0.043905749917030334 Validation loss 0.04329195246100426 Accuracy 0.56201171875\n",
      "Iteration 6310 Training loss 0.03743337094783783 Validation loss 0.04349600523710251 Accuracy 0.56103515625\n",
      "Iteration 6320 Training loss 0.04104763641953468 Validation loss 0.04309528321027756 Accuracy 0.56396484375\n",
      "Iteration 6330 Training loss 0.042157094925642014 Validation loss 0.04308914765715599 Accuracy 0.564453125\n",
      "Iteration 6340 Training loss 0.04117630422115326 Validation loss 0.042824968695640564 Accuracy 0.56689453125\n",
      "Iteration 6350 Training loss 0.043375276029109955 Validation loss 0.043041981756687164 Accuracy 0.56494140625\n",
      "Iteration 6360 Training loss 0.0386161133646965 Validation loss 0.04343939945101738 Accuracy 0.5634765625\n",
      "Iteration 6370 Training loss 0.04488645866513252 Validation loss 0.04308917745947838 Accuracy 0.5654296875\n",
      "Iteration 6380 Training loss 0.04212073236703873 Validation loss 0.0432111956179142 Accuracy 0.56396484375\n",
      "Iteration 6390 Training loss 0.039986077696084976 Validation loss 0.043169055134058 Accuracy 0.564453125\n",
      "Iteration 6400 Training loss 0.04170433431863785 Validation loss 0.04295781999826431 Accuracy 0.56640625\n",
      "Iteration 6410 Training loss 0.04411070793867111 Validation loss 0.04367193207144737 Accuracy 0.5595703125\n",
      "Iteration 6420 Training loss 0.04209805279970169 Validation loss 0.04348618537187576 Accuracy 0.56103515625\n",
      "Iteration 6430 Training loss 0.04156726971268654 Validation loss 0.044741082936525345 Accuracy 0.54931640625\n",
      "Iteration 6440 Training loss 0.03967254236340523 Validation loss 0.04291696846485138 Accuracy 0.5654296875\n",
      "Iteration 6450 Training loss 0.042862262576818466 Validation loss 0.0428335964679718 Accuracy 0.56591796875\n",
      "Iteration 6460 Training loss 0.04069620370864868 Validation loss 0.04381635785102844 Accuracy 0.55615234375\n",
      "Iteration 6470 Training loss 0.04430701583623886 Validation loss 0.042863085865974426 Accuracy 0.5654296875\n",
      "Iteration 6480 Training loss 0.04002414271235466 Validation loss 0.04313961789011955 Accuracy 0.56298828125\n",
      "Iteration 6490 Training loss 0.04466401785612106 Validation loss 0.04343390837311745 Accuracy 0.560546875\n",
      "Iteration 6500 Training loss 0.039653435349464417 Validation loss 0.04312979802489281 Accuracy 0.5634765625\n",
      "Iteration 6510 Training loss 0.043340060859918594 Validation loss 0.043732572346925735 Accuracy 0.55859375\n",
      "Iteration 6520 Training loss 0.04110690578818321 Validation loss 0.043522078543901443 Accuracy 0.56103515625\n",
      "Iteration 6530 Training loss 0.04521657153964043 Validation loss 0.043248601257801056 Accuracy 0.5625\n",
      "Iteration 6540 Training loss 0.04300128296017647 Validation loss 0.04315996170043945 Accuracy 0.56396484375\n",
      "Iteration 6550 Training loss 0.041494570672512054 Validation loss 0.04298385605216026 Accuracy 0.5634765625\n",
      "Iteration 6560 Training loss 0.043101489543914795 Validation loss 0.04313408583402634 Accuracy 0.56298828125\n",
      "Iteration 6570 Training loss 0.043951984494924545 Validation loss 0.044341396540403366 Accuracy 0.55224609375\n",
      "Iteration 6580 Training loss 0.04700702801346779 Validation loss 0.044739462435245514 Accuracy 0.54736328125\n",
      "Iteration 6590 Training loss 0.043274667114019394 Validation loss 0.04326137527823448 Accuracy 0.56298828125\n",
      "Iteration 6600 Training loss 0.046821512281894684 Validation loss 0.043103743344545364 Accuracy 0.5634765625\n",
      "Iteration 6610 Training loss 0.04088238999247551 Validation loss 0.04315781965851784 Accuracy 0.5625\n",
      "Iteration 6620 Training loss 0.043268971145153046 Validation loss 0.04301685467362404 Accuracy 0.5634765625\n",
      "Iteration 6630 Training loss 0.040940120816230774 Validation loss 0.043549053370952606 Accuracy 0.5595703125\n",
      "Iteration 6640 Training loss 0.04749494791030884 Validation loss 0.04411771520972252 Accuracy 0.5537109375\n",
      "Iteration 6650 Training loss 0.04341449588537216 Validation loss 0.043597009032964706 Accuracy 0.56005859375\n",
      "Iteration 6660 Training loss 0.03828553855419159 Validation loss 0.04306074231863022 Accuracy 0.56298828125\n",
      "Iteration 6670 Training loss 0.04031846299767494 Validation loss 0.042817845940589905 Accuracy 0.56494140625\n",
      "Iteration 6680 Training loss 0.04305786266922951 Validation loss 0.0429244264960289 Accuracy 0.56298828125\n",
      "Iteration 6690 Training loss 0.04074978828430176 Validation loss 0.04307801276445389 Accuracy 0.56396484375\n",
      "Iteration 6700 Training loss 0.04309828579425812 Validation loss 0.04319782927632332 Accuracy 0.56201171875\n",
      "Iteration 6710 Training loss 0.037474196404218674 Validation loss 0.042943958193063736 Accuracy 0.56396484375\n",
      "Iteration 6720 Training loss 0.04165874049067497 Validation loss 0.0434265173971653 Accuracy 0.5595703125\n",
      "Iteration 6730 Training loss 0.04340570420026779 Validation loss 0.04304700717329979 Accuracy 0.5634765625\n",
      "Iteration 6740 Training loss 0.04501083493232727 Validation loss 0.042778052389621735 Accuracy 0.56689453125\n",
      "Iteration 6750 Training loss 0.04397913068532944 Validation loss 0.0428374707698822 Accuracy 0.564453125\n",
      "Iteration 6760 Training loss 0.04138360545039177 Validation loss 0.043040964752435684 Accuracy 0.56298828125\n",
      "Iteration 6770 Training loss 0.04495478421449661 Validation loss 0.04290890693664551 Accuracy 0.56494140625\n",
      "Iteration 6780 Training loss 0.0415494330227375 Validation loss 0.04283913969993591 Accuracy 0.56494140625\n",
      "Iteration 6790 Training loss 0.042234864085912704 Validation loss 0.04294973611831665 Accuracy 0.564453125\n",
      "Iteration 6800 Training loss 0.043568648397922516 Validation loss 0.04337430000305176 Accuracy 0.56103515625\n",
      "Iteration 6810 Training loss 0.04046300798654556 Validation loss 0.0436185784637928 Accuracy 0.55810546875\n",
      "Iteration 6820 Training loss 0.04413788393139839 Validation loss 0.043768513947725296 Accuracy 0.55859375\n",
      "Iteration 6830 Training loss 0.04211042821407318 Validation loss 0.043202586472034454 Accuracy 0.5634765625\n",
      "Iteration 6840 Training loss 0.04325202479958534 Validation loss 0.04307729750871658 Accuracy 0.56494140625\n",
      "Iteration 6850 Training loss 0.04259369522333145 Validation loss 0.043607622385025024 Accuracy 0.5576171875\n",
      "Iteration 6860 Training loss 0.04247035086154938 Validation loss 0.042739976197481155 Accuracy 0.56591796875\n",
      "Iteration 6870 Training loss 0.03775252401828766 Validation loss 0.04046507552266121 Accuracy 0.58935546875\n",
      "Iteration 6880 Training loss 0.038283221423625946 Validation loss 0.03858131170272827 Accuracy 0.61083984375\n",
      "Iteration 6890 Training loss 0.039179619401693344 Validation loss 0.039352476596832275 Accuracy 0.603515625\n",
      "Iteration 6900 Training loss 0.037432774901390076 Validation loss 0.03835182636976242 Accuracy 0.6123046875\n",
      "Iteration 6910 Training loss 0.036878764629364014 Validation loss 0.038489021360874176 Accuracy 0.61181640625\n",
      "Iteration 6920 Training loss 0.03406573459506035 Validation loss 0.038141585886478424 Accuracy 0.6142578125\n",
      "Iteration 6930 Training loss 0.04093441367149353 Validation loss 0.03878796845674515 Accuracy 0.60791015625\n",
      "Iteration 6940 Training loss 0.03620689734816551 Validation loss 0.03830622881650925 Accuracy 0.6123046875\n",
      "Iteration 6950 Training loss 0.03643743321299553 Validation loss 0.03800332918763161 Accuracy 0.6162109375\n",
      "Iteration 6960 Training loss 0.03385801240801811 Validation loss 0.03847789391875267 Accuracy 0.61181640625\n",
      "Iteration 6970 Training loss 0.03821370378136635 Validation loss 0.03782813251018524 Accuracy 0.619140625\n",
      "Iteration 6980 Training loss 0.037497542798519135 Validation loss 0.03772379830479622 Accuracy 0.6201171875\n",
      "Iteration 6990 Training loss 0.03737882524728775 Validation loss 0.037914153188467026 Accuracy 0.61767578125\n",
      "Iteration 7000 Training loss 0.03569438308477402 Validation loss 0.039185862988233566 Accuracy 0.60400390625\n",
      "Iteration 7010 Training loss 0.034768395125865936 Validation loss 0.038318246603012085 Accuracy 0.61328125\n",
      "Iteration 7020 Training loss 0.03704338148236275 Validation loss 0.03809596970677376 Accuracy 0.615234375\n",
      "Iteration 7030 Training loss 0.03565242141485214 Validation loss 0.037568144500255585 Accuracy 0.6201171875\n",
      "Iteration 7040 Training loss 0.034876152873039246 Validation loss 0.03765837103128433 Accuracy 0.619140625\n",
      "Iteration 7050 Training loss 0.033220816403627396 Validation loss 0.037637535482645035 Accuracy 0.61962890625\n",
      "Iteration 7060 Training loss 0.03813345730304718 Validation loss 0.03821149840950966 Accuracy 0.61376953125\n",
      "Iteration 7070 Training loss 0.03632979094982147 Validation loss 0.038784418255090714 Accuracy 0.60791015625\n",
      "Iteration 7080 Training loss 0.036943767219781876 Validation loss 0.03797085955739021 Accuracy 0.6171875\n",
      "Iteration 7090 Training loss 0.03819011524319649 Validation loss 0.03788083419203758 Accuracy 0.6171875\n",
      "Iteration 7100 Training loss 0.03511688485741615 Validation loss 0.03757105767726898 Accuracy 0.62060546875\n",
      "Iteration 7110 Training loss 0.036379374563694 Validation loss 0.03752145543694496 Accuracy 0.62158203125\n",
      "Iteration 7120 Training loss 0.03373634070158005 Validation loss 0.037289947271347046 Accuracy 0.62255859375\n",
      "Iteration 7130 Training loss 0.035163167864084244 Validation loss 0.03749127686023712 Accuracy 0.62060546875\n",
      "Iteration 7140 Training loss 0.03754248097538948 Validation loss 0.037879716604948044 Accuracy 0.61767578125\n",
      "Iteration 7150 Training loss 0.0378950759768486 Validation loss 0.04067229852080345 Accuracy 0.587890625\n",
      "Iteration 7160 Training loss 0.03952379524707794 Validation loss 0.037682224065065384 Accuracy 0.6181640625\n",
      "Iteration 7170 Training loss 0.035323843359947205 Validation loss 0.03789205104112625 Accuracy 0.6171875\n",
      "Iteration 7180 Training loss 0.036871835589408875 Validation loss 0.03739852085709572 Accuracy 0.6220703125\n",
      "Iteration 7190 Training loss 0.037185292690992355 Validation loss 0.038240544497966766 Accuracy 0.6142578125\n",
      "Iteration 7200 Training loss 0.03856479004025459 Validation loss 0.037697482854127884 Accuracy 0.619140625\n",
      "Iteration 7210 Training loss 0.03295856714248657 Validation loss 0.0374123752117157 Accuracy 0.62158203125\n",
      "Iteration 7220 Training loss 0.03806131333112717 Validation loss 0.03742905706167221 Accuracy 0.62109375\n",
      "Iteration 7230 Training loss 0.03358935937285423 Validation loss 0.03732769936323166 Accuracy 0.62353515625\n",
      "Iteration 7240 Training loss 0.03878827020525932 Validation loss 0.03713660314679146 Accuracy 0.625\n",
      "Iteration 7250 Training loss 0.0379573330283165 Validation loss 0.0375845804810524 Accuracy 0.6201171875\n",
      "Iteration 7260 Training loss 0.03233569115400314 Validation loss 0.037892282009124756 Accuracy 0.6162109375\n",
      "Iteration 7270 Training loss 0.03518793731927872 Validation loss 0.03797009959816933 Accuracy 0.61767578125\n",
      "Iteration 7280 Training loss 0.034458644688129425 Validation loss 0.0383189357817173 Accuracy 0.61083984375\n",
      "Iteration 7290 Training loss 0.03340741619467735 Validation loss 0.03811540827155113 Accuracy 0.61474609375\n",
      "Iteration 7300 Training loss 0.037296194583177567 Validation loss 0.03734392300248146 Accuracy 0.6220703125\n",
      "Iteration 7310 Training loss 0.032555364072322845 Validation loss 0.03732119873166084 Accuracy 0.62255859375\n",
      "Iteration 7320 Training loss 0.032429106533527374 Validation loss 0.037336062639951706 Accuracy 0.62158203125\n",
      "Iteration 7330 Training loss 0.04019375890493393 Validation loss 0.03745678812265396 Accuracy 0.61962890625\n",
      "Iteration 7340 Training loss 0.032286886125802994 Validation loss 0.03687680885195732 Accuracy 0.626953125\n",
      "Iteration 7350 Training loss 0.03360145539045334 Validation loss 0.03712394833564758 Accuracy 0.625\n",
      "Iteration 7360 Training loss 0.03343786671757698 Validation loss 0.03524588420987129 Accuracy 0.64208984375\n",
      "Iteration 7370 Training loss 0.03220345824956894 Validation loss 0.034041840583086014 Accuracy 0.65234375\n",
      "Iteration 7380 Training loss 0.029278788715600967 Validation loss 0.03376345708966255 Accuracy 0.65771484375\n",
      "Iteration 7390 Training loss 0.033759456127882004 Validation loss 0.03464064747095108 Accuracy 0.64892578125\n",
      "Iteration 7400 Training loss 0.03286134824156761 Validation loss 0.033610664308071136 Accuracy 0.65869140625\n",
      "Iteration 7410 Training loss 0.032502010464668274 Validation loss 0.03358250483870506 Accuracy 0.65771484375\n",
      "Iteration 7420 Training loss 0.037126876413822174 Validation loss 0.03318164870142937 Accuracy 0.6630859375\n",
      "Iteration 7430 Training loss 0.033108294010162354 Validation loss 0.03418900817632675 Accuracy 0.65380859375\n",
      "Iteration 7440 Training loss 0.028470413759350777 Validation loss 0.031698863953351974 Accuracy 0.67724609375\n",
      "Iteration 7450 Training loss 0.033968839794397354 Validation loss 0.03330113738775253 Accuracy 0.66064453125\n",
      "Iteration 7460 Training loss 0.028713079169392586 Validation loss 0.03099757805466652 Accuracy 0.68359375\n",
      "Iteration 7470 Training loss 0.029699889943003654 Validation loss 0.031686145812273026 Accuracy 0.67724609375\n",
      "Iteration 7480 Training loss 0.027318140491843224 Validation loss 0.03162771090865135 Accuracy 0.677734375\n",
      "Iteration 7490 Training loss 0.027173316106200218 Validation loss 0.030854957178235054 Accuracy 0.68505859375\n",
      "Iteration 7500 Training loss 0.032075557857751846 Validation loss 0.03326309472322464 Accuracy 0.66259765625\n",
      "Iteration 7510 Training loss 0.030811116099357605 Validation loss 0.031119821593165398 Accuracy 0.68310546875\n",
      "Iteration 7520 Training loss 0.030891217291355133 Validation loss 0.031120875850319862 Accuracy 0.68408203125\n",
      "Iteration 7530 Training loss 0.033219851553440094 Validation loss 0.031463924795389175 Accuracy 0.6796875\n",
      "Iteration 7540 Training loss 0.03144798427820206 Validation loss 0.032331738620996475 Accuracy 0.671875\n",
      "Iteration 7550 Training loss 0.027094395831227303 Validation loss 0.0316031388938427 Accuracy 0.67919921875\n",
      "Iteration 7560 Training loss 0.02940979227423668 Validation loss 0.03203275427222252 Accuracy 0.6748046875\n",
      "Iteration 7570 Training loss 0.029634205624461174 Validation loss 0.03194455802440643 Accuracy 0.67529296875\n",
      "Iteration 7580 Training loss 0.032321497797966 Validation loss 0.03164574131369591 Accuracy 0.6787109375\n",
      "Iteration 7590 Training loss 0.030712706968188286 Validation loss 0.03126341104507446 Accuracy 0.6826171875\n",
      "Iteration 7600 Training loss 0.030713897198438644 Validation loss 0.030526602640748024 Accuracy 0.68896484375\n",
      "Iteration 7610 Training loss 0.028760986402630806 Validation loss 0.031717170029878616 Accuracy 0.677734375\n",
      "Iteration 7620 Training loss 0.033041827380657196 Validation loss 0.03797266632318497 Accuracy 0.61572265625\n",
      "Iteration 7630 Training loss 0.030204974114894867 Validation loss 0.03124387562274933 Accuracy 0.681640625\n",
      "Iteration 7640 Training loss 0.028805037960410118 Validation loss 0.03240704536437988 Accuracy 0.67138671875\n",
      "Iteration 7650 Training loss 0.027818839997053146 Validation loss 0.03213616833090782 Accuracy 0.6728515625\n",
      "Iteration 7660 Training loss 0.0356982946395874 Validation loss 0.035040441900491714 Accuracy 0.64697265625\n",
      "Iteration 7670 Training loss 0.03485509380698204 Validation loss 0.03442079946398735 Accuracy 0.65087890625\n",
      "Iteration 7680 Training loss 0.03019058145582676 Validation loss 0.03266562521457672 Accuracy 0.66796875\n",
      "Iteration 7690 Training loss 0.026003343984484673 Validation loss 0.03064189851284027 Accuracy 0.6875\n",
      "Iteration 7700 Training loss 0.029086997732520103 Validation loss 0.03095114603638649 Accuracy 0.685546875\n",
      "Iteration 7710 Training loss 0.030623784288764 Validation loss 0.03255688026547432 Accuracy 0.669921875\n",
      "Iteration 7720 Training loss 0.030068090185523033 Validation loss 0.03227539360523224 Accuracy 0.6728515625\n",
      "Iteration 7730 Training loss 0.0307490024715662 Validation loss 0.030534248799085617 Accuracy 0.68896484375\n",
      "Iteration 7740 Training loss 0.035712145268917084 Validation loss 0.03445374220609665 Accuracy 0.65087890625\n",
      "Iteration 7750 Training loss 0.030761225149035454 Validation loss 0.030957113951444626 Accuracy 0.6845703125\n",
      "Iteration 7760 Training loss 0.02935541234910488 Validation loss 0.0324554480612278 Accuracy 0.6708984375\n",
      "Iteration 7770 Training loss 0.032240718603134155 Validation loss 0.030582888051867485 Accuracy 0.68798828125\n",
      "Iteration 7780 Training loss 0.03155471384525299 Validation loss 0.0315869115293026 Accuracy 0.6787109375\n",
      "Iteration 7790 Training loss 0.03228555619716644 Validation loss 0.031028812751173973 Accuracy 0.68310546875\n",
      "Iteration 7800 Training loss 0.030906086787581444 Validation loss 0.030719716101884842 Accuracy 0.6875\n",
      "Iteration 7810 Training loss 0.02900327555835247 Validation loss 0.030723944306373596 Accuracy 0.68701171875\n",
      "Iteration 7820 Training loss 0.029982414096593857 Validation loss 0.030575644224882126 Accuracy 0.6884765625\n",
      "Iteration 7830 Training loss 0.032904159277677536 Validation loss 0.031683601438999176 Accuracy 0.67724609375\n",
      "Iteration 7840 Training loss 0.02872474119067192 Validation loss 0.03027024306356907 Accuracy 0.69091796875\n",
      "Iteration 7850 Training loss 0.03148789331316948 Validation loss 0.034235887229442596 Accuracy 0.6533203125\n",
      "Iteration 7860 Training loss 0.030920017510652542 Validation loss 0.030929232016205788 Accuracy 0.68408203125\n",
      "Iteration 7870 Training loss 0.029127538204193115 Validation loss 0.03065752610564232 Accuracy 0.6875\n",
      "Iteration 7880 Training loss 0.033522699028253555 Validation loss 0.03318967670202255 Accuracy 0.66162109375\n",
      "Iteration 7890 Training loss 0.02755344659090042 Validation loss 0.032235004007816315 Accuracy 0.67333984375\n",
      "Iteration 7900 Training loss 0.032539743930101395 Validation loss 0.03307776525616646 Accuracy 0.66357421875\n",
      "Iteration 7910 Training loss 0.03323785960674286 Validation loss 0.03453819826245308 Accuracy 0.6494140625\n",
      "Iteration 7920 Training loss 0.03316228836774826 Validation loss 0.03185245022177696 Accuracy 0.6767578125\n",
      "Iteration 7930 Training loss 0.02726142294704914 Validation loss 0.030723214149475098 Accuracy 0.68701171875\n",
      "Iteration 7940 Training loss 0.03230014443397522 Validation loss 0.031748734414577484 Accuracy 0.6767578125\n",
      "Iteration 7950 Training loss 0.02994055487215519 Validation loss 0.030470507219433784 Accuracy 0.68994140625\n",
      "Iteration 7960 Training loss 0.02857290394604206 Validation loss 0.031623829156160355 Accuracy 0.6767578125\n",
      "Iteration 7970 Training loss 0.028205877169966698 Validation loss 0.031162962317466736 Accuracy 0.68359375\n",
      "Iteration 7980 Training loss 0.031480222940444946 Validation loss 0.03362598642706871 Accuracy 0.6572265625\n",
      "Iteration 7990 Training loss 0.027466878294944763 Validation loss 0.031036188825964928 Accuracy 0.68310546875\n",
      "Iteration 8000 Training loss 0.02760910801589489 Validation loss 0.030939489603042603 Accuracy 0.68505859375\n",
      "Iteration 8010 Training loss 0.029840240254998207 Validation loss 0.03156331554055214 Accuracy 0.67919921875\n",
      "Iteration 8020 Training loss 0.027991607785224915 Validation loss 0.030782576650381088 Accuracy 0.68603515625\n",
      "Iteration 8030 Training loss 0.031075583770871162 Validation loss 0.030914828181266785 Accuracy 0.6865234375\n",
      "Iteration 8040 Training loss 0.03060149773955345 Validation loss 0.0315147303044796 Accuracy 0.67919921875\n",
      "Iteration 8050 Training loss 0.027119887992739677 Validation loss 0.03038814291357994 Accuracy 0.69091796875\n",
      "Iteration 8060 Training loss 0.029621507972478867 Validation loss 0.030599545687437057 Accuracy 0.6865234375\n",
      "Iteration 8070 Training loss 0.030200021341443062 Validation loss 0.030441349372267723 Accuracy 0.69091796875\n",
      "Iteration 8080 Training loss 0.028628524392843246 Validation loss 0.03008476085960865 Accuracy 0.69287109375\n",
      "Iteration 8090 Training loss 0.027430996298789978 Validation loss 0.02993793413043022 Accuracy 0.6943359375\n",
      "Iteration 8100 Training loss 0.02897854521870613 Validation loss 0.031288836151361465 Accuracy 0.6806640625\n",
      "Iteration 8110 Training loss 0.03095744363963604 Validation loss 0.029854169115424156 Accuracy 0.6953125\n",
      "Iteration 8120 Training loss 0.029749775305390358 Validation loss 0.03047586791217327 Accuracy 0.6904296875\n",
      "Iteration 8130 Training loss 0.03345712274312973 Validation loss 0.030806556344032288 Accuracy 0.68603515625\n",
      "Iteration 8140 Training loss 0.03163331001996994 Validation loss 0.0322258397936821 Accuracy 0.67333984375\n",
      "Iteration 8150 Training loss 0.030553199350833893 Validation loss 0.03068581409752369 Accuracy 0.6875\n",
      "Iteration 8160 Training loss 0.029314231127500534 Validation loss 0.030666569247841835 Accuracy 0.6865234375\n",
      "Iteration 8170 Training loss 0.0298884529620409 Validation loss 0.030046286061406136 Accuracy 0.69384765625\n",
      "Iteration 8180 Training loss 0.029464660212397575 Validation loss 0.03117876872420311 Accuracy 0.681640625\n",
      "Iteration 8190 Training loss 0.027353202924132347 Validation loss 0.02998408116400242 Accuracy 0.69482421875\n",
      "Iteration 8200 Training loss 0.03176011145114899 Validation loss 0.0310454573482275 Accuracy 0.68359375\n",
      "Iteration 8210 Training loss 0.028336601331830025 Validation loss 0.02968381531536579 Accuracy 0.6962890625\n",
      "Iteration 8220 Training loss 0.028204889968037605 Validation loss 0.02981581911444664 Accuracy 0.6953125\n",
      "Iteration 8230 Training loss 0.02869001217186451 Validation loss 0.03054213710129261 Accuracy 0.68701171875\n",
      "Iteration 8240 Training loss 0.03293171897530556 Validation loss 0.031052889302372932 Accuracy 0.6826171875\n",
      "Iteration 8250 Training loss 0.030993977561593056 Validation loss 0.03099105879664421 Accuracy 0.68408203125\n",
      "Iteration 8260 Training loss 0.027679352089762688 Validation loss 0.030614973977208138 Accuracy 0.68701171875\n",
      "Iteration 8270 Training loss 0.030518589541316032 Validation loss 0.03088081069290638 Accuracy 0.6845703125\n",
      "Iteration 8280 Training loss 0.028561405837535858 Validation loss 0.03177553787827492 Accuracy 0.67724609375\n",
      "Iteration 8290 Training loss 0.027356678619980812 Validation loss 0.029447108507156372 Accuracy 0.69921875\n",
      "Iteration 8300 Training loss 0.027408065274357796 Validation loss 0.029687367379665375 Accuracy 0.69677734375\n",
      "Iteration 8310 Training loss 0.028598245233297348 Validation loss 0.03004773147404194 Accuracy 0.6943359375\n",
      "Iteration 8320 Training loss 0.03157990053296089 Validation loss 0.029928764328360558 Accuracy 0.6943359375\n",
      "Iteration 8330 Training loss 0.029087601229548454 Validation loss 0.030691862106323242 Accuracy 0.6865234375\n",
      "Iteration 8340 Training loss 0.026144886389374733 Validation loss 0.030074454843997955 Accuracy 0.69482421875\n",
      "Iteration 8350 Training loss 0.03245946392416954 Validation loss 0.030903836712241173 Accuracy 0.6865234375\n",
      "Iteration 8360 Training loss 0.031065212562680244 Validation loss 0.032765842974185944 Accuracy 0.66650390625\n",
      "Iteration 8370 Training loss 0.027054153382778168 Validation loss 0.02982155606150627 Accuracy 0.69580078125\n",
      "Iteration 8380 Training loss 0.026127511635422707 Validation loss 0.030082840472459793 Accuracy 0.69189453125\n",
      "Iteration 8390 Training loss 0.03137262910604477 Validation loss 0.031043803319334984 Accuracy 0.68408203125\n",
      "Iteration 8400 Training loss 0.030089057981967926 Validation loss 0.030587665736675262 Accuracy 0.68701171875\n",
      "Iteration 8410 Training loss 0.033270660787820816 Validation loss 0.031841229647397995 Accuracy 0.6748046875\n",
      "Iteration 8420 Training loss 0.02832123078405857 Validation loss 0.02962687984108925 Accuracy 0.6982421875\n",
      "Iteration 8430 Training loss 0.024137739092111588 Validation loss 0.03009822592139244 Accuracy 0.69384765625\n",
      "Iteration 8440 Training loss 0.030798105522990227 Validation loss 0.03077869489789009 Accuracy 0.68603515625\n",
      "Iteration 8450 Training loss 0.030032318085432053 Validation loss 0.030000779777765274 Accuracy 0.69482421875\n",
      "Iteration 8460 Training loss 0.02776009403169155 Validation loss 0.029673870652914047 Accuracy 0.697265625\n",
      "Iteration 8470 Training loss 0.029149316251277924 Validation loss 0.030021199956536293 Accuracy 0.69384765625\n",
      "Iteration 8480 Training loss 0.028339294716715813 Validation loss 0.03003263846039772 Accuracy 0.69482421875\n",
      "Iteration 8490 Training loss 0.02799369767308235 Validation loss 0.030177753418684006 Accuracy 0.69287109375\n",
      "Iteration 8500 Training loss 0.02711164765059948 Validation loss 0.029717791825532913 Accuracy 0.6982421875\n",
      "Iteration 8510 Training loss 0.030663060024380684 Validation loss 0.029920805245637894 Accuracy 0.69482421875\n",
      "Iteration 8520 Training loss 0.026353389024734497 Validation loss 0.02976119890809059 Accuracy 0.6953125\n",
      "Iteration 8530 Training loss 0.02791457064449787 Validation loss 0.02978183515369892 Accuracy 0.6962890625\n",
      "Iteration 8540 Training loss 0.028641102835536003 Validation loss 0.030048448592424393 Accuracy 0.69140625\n",
      "Iteration 8550 Training loss 0.02722369320690632 Validation loss 0.030396994203329086 Accuracy 0.689453125\n",
      "Iteration 8560 Training loss 0.027523808181285858 Validation loss 0.03181976079940796 Accuracy 0.6767578125\n",
      "Iteration 8570 Training loss 0.028438441455364227 Validation loss 0.029314013198018074 Accuracy 0.701171875\n",
      "Iteration 8580 Training loss 0.02933187037706375 Validation loss 0.030178045853972435 Accuracy 0.6904296875\n",
      "Iteration 8590 Training loss 0.030586516484618187 Validation loss 0.029643623158335686 Accuracy 0.697265625\n",
      "Iteration 8600 Training loss 0.024357527494430542 Validation loss 0.029189366847276688 Accuracy 0.70166015625\n",
      "Iteration 8610 Training loss 0.026375889778137207 Validation loss 0.029680706560611725 Accuracy 0.6962890625\n",
      "Iteration 8620 Training loss 0.030062003061175346 Validation loss 0.02942056581377983 Accuracy 0.69921875\n",
      "Iteration 8630 Training loss 0.025989441201090813 Validation loss 0.02979840710759163 Accuracy 0.6962890625\n",
      "Iteration 8640 Training loss 0.027811802923679352 Validation loss 0.02942643314599991 Accuracy 0.69921875\n",
      "Iteration 8650 Training loss 0.02825791947543621 Validation loss 0.02950294315814972 Accuracy 0.697265625\n",
      "Iteration 8660 Training loss 0.025022059679031372 Validation loss 0.029241686686873436 Accuracy 0.7001953125\n",
      "Iteration 8670 Training loss 0.025942591950297356 Validation loss 0.030022071674466133 Accuracy 0.69189453125\n",
      "Iteration 8680 Training loss 0.02876291424036026 Validation loss 0.02987469732761383 Accuracy 0.6943359375\n",
      "Iteration 8690 Training loss 0.02937520481646061 Validation loss 0.029197128489613533 Accuracy 0.70068359375\n",
      "Iteration 8700 Training loss 0.027515435591340065 Validation loss 0.030047211796045303 Accuracy 0.69189453125\n",
      "Iteration 8710 Training loss 0.02617878094315529 Validation loss 0.028978437185287476 Accuracy 0.70263671875\n",
      "Iteration 8720 Training loss 0.027401473373174667 Validation loss 0.029347263276576996 Accuracy 0.69873046875\n",
      "Iteration 8730 Training loss 0.025678085163235664 Validation loss 0.02925904281437397 Accuracy 0.70068359375\n",
      "Iteration 8740 Training loss 0.026251979172229767 Validation loss 0.029250599443912506 Accuracy 0.70068359375\n",
      "Iteration 8750 Training loss 0.03151731193065643 Validation loss 0.02976394258439541 Accuracy 0.6953125\n",
      "Iteration 8760 Training loss 0.027701212093234062 Validation loss 0.03023066371679306 Accuracy 0.69091796875\n",
      "Iteration 8770 Training loss 0.025969650596380234 Validation loss 0.029061131179332733 Accuracy 0.7021484375\n",
      "Iteration 8780 Training loss 0.03169579803943634 Validation loss 0.03272654861211777 Accuracy 0.66650390625\n",
      "Iteration 8790 Training loss 0.025839243084192276 Validation loss 0.029890624806284904 Accuracy 0.6943359375\n",
      "Iteration 8800 Training loss 0.02646080031991005 Validation loss 0.02950049191713333 Accuracy 0.697265625\n",
      "Iteration 8810 Training loss 0.026502853259444237 Validation loss 0.03037901781499386 Accuracy 0.68798828125\n",
      "Iteration 8820 Training loss 0.02927199751138687 Validation loss 0.0291096493601799 Accuracy 0.70166015625\n",
      "Iteration 8830 Training loss 0.02813032455742359 Validation loss 0.029219970107078552 Accuracy 0.701171875\n",
      "Iteration 8840 Training loss 0.028032083064317703 Validation loss 0.0288620013743639 Accuracy 0.70458984375\n",
      "Iteration 8850 Training loss 0.02814548648893833 Validation loss 0.029667314141988754 Accuracy 0.69482421875\n",
      "Iteration 8860 Training loss 0.025713711977005005 Validation loss 0.02894379384815693 Accuracy 0.7021484375\n",
      "Iteration 8870 Training loss 0.019214250147342682 Validation loss 0.020636746659874916 Accuracy 0.7880859375\n",
      "Iteration 8880 Training loss 0.015699809417128563 Validation loss 0.02158716507256031 Accuracy 0.779296875\n",
      "Iteration 8890 Training loss 0.02006777748465538 Validation loss 0.020622026175260544 Accuracy 0.7890625\n",
      "Iteration 8900 Training loss 0.014146855100989342 Validation loss 0.020985662937164307 Accuracy 0.78515625\n",
      "Iteration 8910 Training loss 0.0213426873087883 Validation loss 0.02188922092318535 Accuracy 0.7763671875\n",
      "Iteration 8920 Training loss 0.02018379233777523 Validation loss 0.02092329040169716 Accuracy 0.7861328125\n",
      "Iteration 8930 Training loss 0.021284906193614006 Validation loss 0.0218126829713583 Accuracy 0.77587890625\n",
      "Iteration 8940 Training loss 0.020098775625228882 Validation loss 0.0228671133518219 Accuracy 0.765625\n",
      "Iteration 8950 Training loss 0.019963782280683517 Validation loss 0.021019956097006798 Accuracy 0.7841796875\n",
      "Iteration 8960 Training loss 0.01951751485466957 Validation loss 0.02054251916706562 Accuracy 0.78955078125\n",
      "Iteration 8970 Training loss 0.021432770416140556 Validation loss 0.022679701447486877 Accuracy 0.76806640625\n",
      "Iteration 8980 Training loss 0.018202079460024834 Validation loss 0.020428303629159927 Accuracy 0.79150390625\n",
      "Iteration 8990 Training loss 0.019375959411263466 Validation loss 0.020913900807499886 Accuracy 0.7861328125\n",
      "Iteration 9000 Training loss 0.020389942452311516 Validation loss 0.02030777372419834 Accuracy 0.7919921875\n",
      "Iteration 9010 Training loss 0.01635976880788803 Validation loss 0.020106585696339607 Accuracy 0.7939453125\n",
      "Iteration 9020 Training loss 0.016196349635720253 Validation loss 0.02015765942633152 Accuracy 0.79296875\n",
      "Iteration 9030 Training loss 0.01375990454107523 Validation loss 0.020162619650363922 Accuracy 0.79296875\n",
      "Iteration 9040 Training loss 0.016564449295401573 Validation loss 0.019780900329351425 Accuracy 0.796875\n",
      "Iteration 9050 Training loss 0.019248150289058685 Validation loss 0.022151855751872063 Accuracy 0.7734375\n",
      "Iteration 9060 Training loss 0.017804928123950958 Validation loss 0.022300785407423973 Accuracy 0.77197265625\n",
      "Iteration 9070 Training loss 0.02157003805041313 Validation loss 0.020584603771567345 Accuracy 0.7890625\n",
      "Iteration 9080 Training loss 0.0162399522960186 Validation loss 0.01999194547533989 Accuracy 0.79443359375\n",
      "Iteration 9090 Training loss 0.01806027628481388 Validation loss 0.02085449919104576 Accuracy 0.7861328125\n",
      "Iteration 9100 Training loss 0.01570982113480568 Validation loss 0.020790379494428635 Accuracy 0.78662109375\n",
      "Iteration 9110 Training loss 0.017088372260332108 Validation loss 0.020760685205459595 Accuracy 0.7880859375\n",
      "Iteration 9120 Training loss 0.017419613897800446 Validation loss 0.020000005140900612 Accuracy 0.79541015625\n",
      "Iteration 9130 Training loss 0.017558086663484573 Validation loss 0.020191827788949013 Accuracy 0.79443359375\n",
      "Iteration 9140 Training loss 0.018046004697680473 Validation loss 0.020005598664283752 Accuracy 0.7958984375\n",
      "Iteration 9150 Training loss 0.020836804062128067 Validation loss 0.020257335156202316 Accuracy 0.79296875\n",
      "Iteration 9160 Training loss 0.020600933581590652 Validation loss 0.01996024139225483 Accuracy 0.7958984375\n",
      "Iteration 9170 Training loss 0.020328020676970482 Validation loss 0.01979345642030239 Accuracy 0.796875\n",
      "Iteration 9180 Training loss 0.018792353570461273 Validation loss 0.020219571888446808 Accuracy 0.79248046875\n",
      "Iteration 9190 Training loss 0.015445144847035408 Validation loss 0.020085565745830536 Accuracy 0.79443359375\n",
      "Iteration 9200 Training loss 0.018876586109399796 Validation loss 0.020236898213624954 Accuracy 0.79248046875\n",
      "Iteration 9210 Training loss 0.01862042397260666 Validation loss 0.019837195053696632 Accuracy 0.796875\n",
      "Iteration 9220 Training loss 0.01693749614059925 Validation loss 0.020405223593115807 Accuracy 0.791015625\n",
      "Iteration 9230 Training loss 0.018039321526885033 Validation loss 0.020466135814785957 Accuracy 0.7900390625\n",
      "Iteration 9240 Training loss 0.01776171661913395 Validation loss 0.02035635896027088 Accuracy 0.79296875\n",
      "Iteration 9250 Training loss 0.01905910298228264 Validation loss 0.01995561458170414 Accuracy 0.7958984375\n",
      "Iteration 9260 Training loss 0.01891482062637806 Validation loss 0.020512495189905167 Accuracy 0.7890625\n",
      "Iteration 9270 Training loss 0.018384814262390137 Validation loss 0.021349556744098663 Accuracy 0.78173828125\n",
      "Iteration 9280 Training loss 0.019415847957134247 Validation loss 0.021009251475334167 Accuracy 0.78564453125\n",
      "Iteration 9290 Training loss 0.018765782937407494 Validation loss 0.02205444686114788 Accuracy 0.7744140625\n",
      "Iteration 9300 Training loss 0.014420815743505955 Validation loss 0.02023608237504959 Accuracy 0.79296875\n",
      "Iteration 9310 Training loss 0.01912388764321804 Validation loss 0.020623158663511276 Accuracy 0.7890625\n",
      "Iteration 9320 Training loss 0.019495347514748573 Validation loss 0.020414847880601883 Accuracy 0.79150390625\n",
      "Iteration 9330 Training loss 0.020493071526288986 Validation loss 0.020773084834218025 Accuracy 0.78662109375\n",
      "Iteration 9340 Training loss 0.015596074052155018 Validation loss 0.020144445821642876 Accuracy 0.79443359375\n",
      "Iteration 9350 Training loss 0.019433697685599327 Validation loss 0.02083783969283104 Accuracy 0.78662109375\n",
      "Iteration 9360 Training loss 0.016844727098941803 Validation loss 0.01997091807425022 Accuracy 0.794921875\n",
      "Iteration 9370 Training loss 0.01865839958190918 Validation loss 0.020519418641924858 Accuracy 0.78955078125\n",
      "Iteration 9380 Training loss 0.01772906817495823 Validation loss 0.01970367506146431 Accuracy 0.7978515625\n",
      "Iteration 9390 Training loss 0.017125368118286133 Validation loss 0.020826486870646477 Accuracy 0.78759765625\n",
      "Iteration 9400 Training loss 0.018425190821290016 Validation loss 0.019831791520118713 Accuracy 0.79736328125\n",
      "Iteration 9410 Training loss 0.017906898632645607 Validation loss 0.020858339965343475 Accuracy 0.78662109375\n",
      "Iteration 9420 Training loss 0.01639486849308014 Validation loss 0.02074403502047062 Accuracy 0.7880859375\n",
      "Iteration 9430 Training loss 0.015937915071845055 Validation loss 0.020885344594717026 Accuracy 0.78662109375\n",
      "Iteration 9440 Training loss 0.020476611331105232 Validation loss 0.019876297563314438 Accuracy 0.79638671875\n",
      "Iteration 9450 Training loss 0.01788383163511753 Validation loss 0.019601253792643547 Accuracy 0.79931640625\n",
      "Iteration 9460 Training loss 0.0184605922549963 Validation loss 0.021680055186152458 Accuracy 0.77880859375\n",
      "Iteration 9470 Training loss 0.017935074865818024 Validation loss 0.019800756126642227 Accuracy 0.79736328125\n",
      "Iteration 9480 Training loss 0.01778441295027733 Validation loss 0.02109544537961483 Accuracy 0.78369140625\n",
      "Iteration 9490 Training loss 0.01742994226515293 Validation loss 0.019674869254231453 Accuracy 0.79833984375\n",
      "Iteration 9500 Training loss 0.018666358664631844 Validation loss 0.019832324236631393 Accuracy 0.796875\n",
      "Iteration 9510 Training loss 0.02220427617430687 Validation loss 0.02217697538435459 Accuracy 0.77392578125\n",
      "Iteration 9520 Training loss 0.01910226419568062 Validation loss 0.021455636247992516 Accuracy 0.78173828125\n",
      "Iteration 9530 Training loss 0.018697915598750114 Validation loss 0.020076416432857513 Accuracy 0.7939453125\n",
      "Iteration 9540 Training loss 0.01780695654451847 Validation loss 0.021741138771176338 Accuracy 0.77783203125\n",
      "Iteration 9550 Training loss 0.020359808579087257 Validation loss 0.02033735252916813 Accuracy 0.791015625\n",
      "Iteration 9560 Training loss 0.01647556945681572 Validation loss 0.01941700652241707 Accuracy 0.80126953125\n",
      "Iteration 9570 Training loss 0.016892826184630394 Validation loss 0.019478194415569305 Accuracy 0.7998046875\n",
      "Iteration 9580 Training loss 0.017929384484887123 Validation loss 0.019910836592316628 Accuracy 0.7958984375\n",
      "Iteration 9590 Training loss 0.018898630514740944 Validation loss 0.020899245515465736 Accuracy 0.78564453125\n",
      "Iteration 9600 Training loss 0.01879434660077095 Validation loss 0.019453490152955055 Accuracy 0.80078125\n",
      "Iteration 9610 Training loss 0.01960163563489914 Validation loss 0.01933608017861843 Accuracy 0.80224609375\n",
      "Iteration 9620 Training loss 0.019744819030165672 Validation loss 0.020466124638915062 Accuracy 0.7900390625\n",
      "Iteration 9630 Training loss 0.019070053473114967 Validation loss 0.020970890298485756 Accuracy 0.78515625\n",
      "Iteration 9640 Training loss 0.017876043915748596 Validation loss 0.01993369869887829 Accuracy 0.79541015625\n",
      "Iteration 9650 Training loss 0.01712799072265625 Validation loss 0.019426686689257622 Accuracy 0.80126953125\n",
      "Iteration 9660 Training loss 0.01691288687288761 Validation loss 0.019894830882549286 Accuracy 0.796875\n",
      "Iteration 9670 Training loss 0.017892617732286453 Validation loss 0.020475640892982483 Accuracy 0.79052734375\n",
      "Iteration 9680 Training loss 0.018314389511942863 Validation loss 0.019494466483592987 Accuracy 0.80078125\n",
      "Iteration 9690 Training loss 0.0177454873919487 Validation loss 0.01928243786096573 Accuracy 0.8017578125\n",
      "Iteration 9700 Training loss 0.01980256848037243 Validation loss 0.019835643470287323 Accuracy 0.796875\n",
      "Iteration 9710 Training loss 0.020435186102986336 Validation loss 0.021783065050840378 Accuracy 0.77783203125\n",
      "Iteration 9720 Training loss 0.016193941235542297 Validation loss 0.01944672130048275 Accuracy 0.80078125\n",
      "Iteration 9730 Training loss 0.017865855246782303 Validation loss 0.01946433074772358 Accuracy 0.80078125\n",
      "Iteration 9740 Training loss 0.015822598710656166 Validation loss 0.019420647993683815 Accuracy 0.80126953125\n",
      "Iteration 9750 Training loss 0.014625894837081432 Validation loss 0.019726421684026718 Accuracy 0.7978515625\n",
      "Iteration 9760 Training loss 0.017617762088775635 Validation loss 0.019357826560735703 Accuracy 0.8017578125\n",
      "Iteration 9770 Training loss 0.017024453729391098 Validation loss 0.01923290267586708 Accuracy 0.80224609375\n",
      "Iteration 9780 Training loss 0.018639422953128815 Validation loss 0.02057894878089428 Accuracy 0.78955078125\n",
      "Iteration 9790 Training loss 0.01724919117987156 Validation loss 0.01975051686167717 Accuracy 0.79736328125\n",
      "Iteration 9800 Training loss 0.018024155870079994 Validation loss 0.019452720880508423 Accuracy 0.7998046875\n",
      "Iteration 9810 Training loss 0.014598206616938114 Validation loss 0.02079935558140278 Accuracy 0.78662109375\n",
      "Iteration 9820 Training loss 0.015976065769791603 Validation loss 0.020123090595006943 Accuracy 0.79345703125\n",
      "Iteration 9830 Training loss 0.01702173426747322 Validation loss 0.019513139501214027 Accuracy 0.7998046875\n",
      "Iteration 9840 Training loss 0.01790929026901722 Validation loss 0.019764602184295654 Accuracy 0.79736328125\n",
      "Iteration 9850 Training loss 0.017543917521834373 Validation loss 0.019498132169246674 Accuracy 0.7998046875\n",
      "Iteration 9860 Training loss 0.018206896260380745 Validation loss 0.01921425200998783 Accuracy 0.802734375\n",
      "Iteration 9870 Training loss 0.017670800909399986 Validation loss 0.019972914829850197 Accuracy 0.7958984375\n",
      "Iteration 9880 Training loss 0.0208203736692667 Validation loss 0.023435750976204872 Accuracy 0.759765625\n",
      "Iteration 9890 Training loss 0.019403720274567604 Validation loss 0.019460024312138557 Accuracy 0.80029296875\n",
      "Iteration 9900 Training loss 0.020212389528751373 Validation loss 0.019460679963231087 Accuracy 0.7998046875\n",
      "Iteration 9910 Training loss 0.015538250096142292 Validation loss 0.020079340785741806 Accuracy 0.79345703125\n",
      "Iteration 9920 Training loss 0.018667254596948624 Validation loss 0.01974157616496086 Accuracy 0.7978515625\n",
      "Iteration 9930 Training loss 0.018532155081629753 Validation loss 0.019408227875828743 Accuracy 0.80029296875\n",
      "Iteration 9940 Training loss 0.019107159227132797 Validation loss 0.019255900755524635 Accuracy 0.802734375\n",
      "Iteration 9950 Training loss 0.019315578043460846 Validation loss 0.020188653841614723 Accuracy 0.79296875\n",
      "Iteration 9960 Training loss 0.015822719782590866 Validation loss 0.0191703662276268 Accuracy 0.8037109375\n",
      "Iteration 9970 Training loss 0.016268381848931313 Validation loss 0.019786197692155838 Accuracy 0.79736328125\n",
      "Iteration 9980 Training loss 0.02072531171143055 Validation loss 0.020063426345586777 Accuracy 0.7939453125\n",
      "Iteration 9990 Training loss 0.016730522736907005 Validation loss 0.019278258085250854 Accuracy 0.80126953125\n",
      "Iteration 10000 Training loss 0.01993531733751297 Validation loss 0.02012518420815468 Accuracy 0.79248046875\n",
      "Iteration 10010 Training loss 0.020878713577985764 Validation loss 0.020316001027822495 Accuracy 0.7919921875\n",
      "Iteration 10020 Training loss 0.018675176426768303 Validation loss 0.019745493307709694 Accuracy 0.7978515625\n",
      "Iteration 10030 Training loss 0.018480196595191956 Validation loss 0.019538938999176025 Accuracy 0.80029296875\n",
      "Iteration 10040 Training loss 0.018736733123660088 Validation loss 0.019408516585826874 Accuracy 0.80078125\n",
      "Iteration 10050 Training loss 0.01774699054658413 Validation loss 0.01993601769208908 Accuracy 0.7958984375\n",
      "Iteration 10060 Training loss 0.019305473193526268 Validation loss 0.01951269432902336 Accuracy 0.80078125\n",
      "Iteration 10070 Training loss 0.018417958170175552 Validation loss 0.019859762862324715 Accuracy 0.79638671875\n",
      "Iteration 10080 Training loss 0.01708434522151947 Validation loss 0.021060211583971977 Accuracy 0.78466796875\n",
      "Iteration 10090 Training loss 0.01717841997742653 Validation loss 0.018970241770148277 Accuracy 0.80517578125\n",
      "Iteration 10100 Training loss 0.0179500263184309 Validation loss 0.019279126077890396 Accuracy 0.80224609375\n",
      "Iteration 10110 Training loss 0.016610272228717804 Validation loss 0.020082036033272743 Accuracy 0.79443359375\n",
      "Iteration 10120 Training loss 0.017145950347185135 Validation loss 0.01947970502078533 Accuracy 0.80029296875\n",
      "Iteration 10130 Training loss 0.018037224188447 Validation loss 0.019358763471245766 Accuracy 0.8017578125\n",
      "Iteration 10140 Training loss 0.02255971170961857 Validation loss 0.0213957317173481 Accuracy 0.78125\n",
      "Iteration 10150 Training loss 0.017608147114515305 Validation loss 0.019034184515476227 Accuracy 0.8046875\n",
      "Iteration 10160 Training loss 0.014996535144746304 Validation loss 0.019295262172818184 Accuracy 0.8017578125\n",
      "Iteration 10170 Training loss 0.01786603033542633 Validation loss 0.01934295892715454 Accuracy 0.80126953125\n",
      "Iteration 10180 Training loss 0.01755378395318985 Validation loss 0.019686300307512283 Accuracy 0.79833984375\n",
      "Iteration 10190 Training loss 0.017384273931384087 Validation loss 0.019118409603834152 Accuracy 0.8037109375\n",
      "Iteration 10200 Training loss 0.016212813556194305 Validation loss 0.01950869895517826 Accuracy 0.79931640625\n",
      "Iteration 10210 Training loss 0.016926856711506844 Validation loss 0.01918199099600315 Accuracy 0.80224609375\n",
      "Iteration 10220 Training loss 0.01799914985895157 Validation loss 0.019573846831917763 Accuracy 0.79931640625\n",
      "Iteration 10230 Training loss 0.019422970712184906 Validation loss 0.019525058567523956 Accuracy 0.79931640625\n",
      "Iteration 10240 Training loss 0.015944629907608032 Validation loss 0.0195353701710701 Accuracy 0.79931640625\n",
      "Iteration 10250 Training loss 0.01756981573998928 Validation loss 0.019539305940270424 Accuracy 0.7998046875\n",
      "Iteration 10260 Training loss 0.018867967650294304 Validation loss 0.01968155801296234 Accuracy 0.79931640625\n",
      "Iteration 10270 Training loss 0.018498245626688004 Validation loss 0.01932128146290779 Accuracy 0.80126953125\n",
      "Iteration 10280 Training loss 0.015109439380466938 Validation loss 0.019675571471452713 Accuracy 0.79736328125\n",
      "Iteration 10290 Training loss 0.024870721623301506 Validation loss 0.0268386397510767 Accuracy 0.72705078125\n",
      "Iteration 10300 Training loss 0.01893800124526024 Validation loss 0.01915230229496956 Accuracy 0.8037109375\n",
      "Iteration 10310 Training loss 0.01670254021883011 Validation loss 0.019324783235788345 Accuracy 0.8017578125\n",
      "Iteration 10320 Training loss 0.018446436151862144 Validation loss 0.020007580518722534 Accuracy 0.794921875\n",
      "Iteration 10330 Training loss 0.015599957667291164 Validation loss 0.019266044721007347 Accuracy 0.80224609375\n",
      "Iteration 10340 Training loss 0.020415741950273514 Validation loss 0.019397128373384476 Accuracy 0.80126953125\n",
      "Iteration 10350 Training loss 0.01861044578254223 Validation loss 0.019884610548615456 Accuracy 0.7958984375\n",
      "Iteration 10360 Training loss 0.015756715089082718 Validation loss 0.020224522799253464 Accuracy 0.79248046875\n",
      "Iteration 10370 Training loss 0.017127973958849907 Validation loss 0.020935770124197006 Accuracy 0.7861328125\n",
      "Iteration 10380 Training loss 0.01911725290119648 Validation loss 0.02146914042532444 Accuracy 0.77978515625\n",
      "Iteration 10390 Training loss 0.017604148015379906 Validation loss 0.018856346607208252 Accuracy 0.806640625\n",
      "Iteration 10400 Training loss 0.018199317157268524 Validation loss 0.01970786042511463 Accuracy 0.79736328125\n",
      "Iteration 10410 Training loss 0.01569247618317604 Validation loss 0.019451148808002472 Accuracy 0.7998046875\n",
      "Iteration 10420 Training loss 0.016818825155496597 Validation loss 0.01952175237238407 Accuracy 0.7998046875\n",
      "Iteration 10430 Training loss 0.01587350107729435 Validation loss 0.018995795398950577 Accuracy 0.8056640625\n",
      "Iteration 10440 Training loss 0.0184959564357996 Validation loss 0.019261792302131653 Accuracy 0.80322265625\n",
      "Iteration 10450 Training loss 0.01663435623049736 Validation loss 0.01920098066329956 Accuracy 0.80322265625\n",
      "Iteration 10460 Training loss 0.015983330085873604 Validation loss 0.019753633067011833 Accuracy 0.79736328125\n",
      "Iteration 10470 Training loss 0.016911586746573448 Validation loss 0.019374441355466843 Accuracy 0.80029296875\n",
      "Iteration 10480 Training loss 0.017192255705595016 Validation loss 0.019185271114110947 Accuracy 0.80322265625\n",
      "Iteration 10490 Training loss 0.01567317172884941 Validation loss 0.01940545253455639 Accuracy 0.80078125\n",
      "Iteration 10500 Training loss 0.017133677378296852 Validation loss 0.019226817414164543 Accuracy 0.80322265625\n",
      "Iteration 10510 Training loss 0.01912142150104046 Validation loss 0.019615471363067627 Accuracy 0.798828125\n",
      "Iteration 10520 Training loss 0.016600847244262695 Validation loss 0.01978563517332077 Accuracy 0.79638671875\n",
      "Iteration 10530 Training loss 0.012696768157184124 Validation loss 0.01953066512942314 Accuracy 0.7998046875\n",
      "Iteration 10540 Training loss 0.015796523541212082 Validation loss 0.01990526355803013 Accuracy 0.79541015625\n",
      "Iteration 10550 Training loss 0.015625767409801483 Validation loss 0.019680766388773918 Accuracy 0.79736328125\n",
      "Iteration 10560 Training loss 0.017381813377141953 Validation loss 0.019811686128377914 Accuracy 0.796875\n",
      "Iteration 10570 Training loss 0.016921836882829666 Validation loss 0.019146161153912544 Accuracy 0.80322265625\n",
      "Iteration 10580 Training loss 0.020052917301654816 Validation loss 0.020775234326720238 Accuracy 0.7880859375\n",
      "Iteration 10590 Training loss 0.01459770929068327 Validation loss 0.020068859681487083 Accuracy 0.7939453125\n",
      "Iteration 10600 Training loss 0.017218811437487602 Validation loss 0.019052058458328247 Accuracy 0.80419921875\n",
      "Iteration 10610 Training loss 0.01865246705710888 Validation loss 0.019544100388884544 Accuracy 0.7998046875\n",
      "Iteration 10620 Training loss 0.017856549471616745 Validation loss 0.019987083971500397 Accuracy 0.79443359375\n",
      "Iteration 10630 Training loss 0.020060617476701736 Validation loss 0.019361378625035286 Accuracy 0.80126953125\n",
      "Iteration 10640 Training loss 0.01777247153222561 Validation loss 0.0203799270093441 Accuracy 0.791015625\n",
      "Iteration 10650 Training loss 0.017489118501544 Validation loss 0.01944335736334324 Accuracy 0.7998046875\n",
      "Iteration 10660 Training loss 0.016936006024479866 Validation loss 0.02042994648218155 Accuracy 0.791015625\n",
      "Iteration 10670 Training loss 0.019688505679368973 Validation loss 0.019700780510902405 Accuracy 0.79736328125\n",
      "Iteration 10680 Training loss 0.018780160695314407 Validation loss 0.019322536885738373 Accuracy 0.8017578125\n",
      "Iteration 10690 Training loss 0.015973486006259918 Validation loss 0.01902632974088192 Accuracy 0.80419921875\n",
      "Iteration 10700 Training loss 0.017027705907821655 Validation loss 0.019408948719501495 Accuracy 0.80078125\n",
      "Iteration 10710 Training loss 0.015592320822179317 Validation loss 0.019580185413360596 Accuracy 0.79931640625\n",
      "Iteration 10720 Training loss 0.014749188907444477 Validation loss 0.019716499373316765 Accuracy 0.79736328125\n",
      "Iteration 10730 Training loss 0.016695888713002205 Validation loss 0.019252153113484383 Accuracy 0.802734375\n",
      "Iteration 10740 Training loss 0.019246093928813934 Validation loss 0.018985988572239876 Accuracy 0.8046875\n",
      "Iteration 10750 Training loss 0.018996916711330414 Validation loss 0.020628366619348526 Accuracy 0.7890625\n",
      "Iteration 10760 Training loss 0.016740664839744568 Validation loss 0.019442329183220863 Accuracy 0.80126953125\n",
      "Iteration 10770 Training loss 0.01755458489060402 Validation loss 0.018955668434500694 Accuracy 0.8046875\n",
      "Iteration 10780 Training loss 0.01777089387178421 Validation loss 0.01988648995757103 Accuracy 0.7958984375\n",
      "Iteration 10790 Training loss 0.017468875274062157 Validation loss 0.019289741292595863 Accuracy 0.80126953125\n",
      "Iteration 10800 Training loss 0.017258981242775917 Validation loss 0.01907425746321678 Accuracy 0.8037109375\n",
      "Iteration 10810 Training loss 0.016080159693956375 Validation loss 0.019236918538808823 Accuracy 0.802734375\n",
      "Iteration 10820 Training loss 0.018485797569155693 Validation loss 0.01995115727186203 Accuracy 0.7958984375\n",
      "Iteration 10830 Training loss 0.019037986174225807 Validation loss 0.01978325843811035 Accuracy 0.79541015625\n",
      "Iteration 10840 Training loss 0.014031333848834038 Validation loss 0.01922329142689705 Accuracy 0.80224609375\n",
      "Iteration 10850 Training loss 0.018455075100064278 Validation loss 0.01920267939567566 Accuracy 0.8017578125\n",
      "Iteration 10860 Training loss 0.01641135849058628 Validation loss 0.019116198644042015 Accuracy 0.80419921875\n",
      "Iteration 10870 Training loss 0.01641050912439823 Validation loss 0.01939520426094532 Accuracy 0.80078125\n",
      "Iteration 10880 Training loss 0.019042156636714935 Validation loss 0.019257333129644394 Accuracy 0.802734375\n",
      "Iteration 10890 Training loss 0.01703675091266632 Validation loss 0.01967458613216877 Accuracy 0.79833984375\n",
      "Iteration 10900 Training loss 0.01405761856585741 Validation loss 0.01949797011911869 Accuracy 0.7998046875\n",
      "Iteration 10910 Training loss 0.017682000994682312 Validation loss 0.019380131736397743 Accuracy 0.80126953125\n",
      "Iteration 10920 Training loss 0.013316141441464424 Validation loss 0.01947961188852787 Accuracy 0.7998046875\n",
      "Iteration 10930 Training loss 0.017075780779123306 Validation loss 0.018788745626807213 Accuracy 0.8076171875\n",
      "Iteration 10940 Training loss 0.01955261453986168 Validation loss 0.0190445389598608 Accuracy 0.8046875\n",
      "Iteration 10950 Training loss 0.01655837520956993 Validation loss 0.01920902542769909 Accuracy 0.80322265625\n",
      "Iteration 10960 Training loss 0.015675140544772148 Validation loss 0.02089403010904789 Accuracy 0.78662109375\n",
      "Iteration 10970 Training loss 0.01752210035920143 Validation loss 0.01931513287127018 Accuracy 0.8017578125\n",
      "Iteration 10980 Training loss 0.018221639096736908 Validation loss 0.0203991886228323 Accuracy 0.7919921875\n",
      "Iteration 10990 Training loss 0.02087813802063465 Validation loss 0.023198995739221573 Accuracy 0.76416015625\n",
      "Iteration 11000 Training loss 0.016288967803120613 Validation loss 0.019312938675284386 Accuracy 0.8017578125\n",
      "Iteration 11010 Training loss 0.01549972128123045 Validation loss 0.019526103511452675 Accuracy 0.80029296875\n",
      "Iteration 11020 Training loss 0.019322330132126808 Validation loss 0.019126281142234802 Accuracy 0.80322265625\n",
      "Iteration 11030 Training loss 0.016084136441349983 Validation loss 0.018998563289642334 Accuracy 0.80615234375\n",
      "Iteration 11040 Training loss 0.01699438877403736 Validation loss 0.019397050142288208 Accuracy 0.80078125\n",
      "Iteration 11050 Training loss 0.018962102010846138 Validation loss 0.019650476053357124 Accuracy 0.7978515625\n",
      "Iteration 11060 Training loss 0.017347341403365135 Validation loss 0.019421042874455452 Accuracy 0.7998046875\n",
      "Iteration 11070 Training loss 0.016548361629247665 Validation loss 0.0193682461977005 Accuracy 0.80078125\n",
      "Iteration 11080 Training loss 0.015867656096816063 Validation loss 0.019568519666790962 Accuracy 0.79833984375\n",
      "Iteration 11090 Training loss 0.01752907782793045 Validation loss 0.019171418622136116 Accuracy 0.80224609375\n",
      "Iteration 11100 Training loss 0.01616874523460865 Validation loss 0.019006889313459396 Accuracy 0.80517578125\n",
      "Iteration 11110 Training loss 0.020415719598531723 Validation loss 0.018669748678803444 Accuracy 0.80810546875\n",
      "Iteration 11120 Training loss 0.019485928118228912 Validation loss 0.019510148093104362 Accuracy 0.7998046875\n",
      "Iteration 11130 Training loss 0.01509378757327795 Validation loss 0.019007869064807892 Accuracy 0.8046875\n",
      "Iteration 11140 Training loss 0.016513461247086525 Validation loss 0.01918088272213936 Accuracy 0.8037109375\n",
      "Iteration 11150 Training loss 0.017033269628882408 Validation loss 0.019935360178351402 Accuracy 0.796875\n",
      "Iteration 11160 Training loss 0.01639353670179844 Validation loss 0.019634855911135674 Accuracy 0.798828125\n",
      "Iteration 11170 Training loss 0.01655144803225994 Validation loss 0.018889667466282845 Accuracy 0.80615234375\n",
      "Iteration 11180 Training loss 0.016590608283877373 Validation loss 0.019422387704253197 Accuracy 0.80029296875\n",
      "Iteration 11190 Training loss 0.02093949168920517 Validation loss 0.019812002778053284 Accuracy 0.79541015625\n",
      "Iteration 11200 Training loss 0.013415832072496414 Validation loss 0.01902903988957405 Accuracy 0.80517578125\n",
      "Iteration 11210 Training loss 0.016369732096791267 Validation loss 0.019137509167194366 Accuracy 0.80419921875\n",
      "Iteration 11220 Training loss 0.01588122732937336 Validation loss 0.019540756940841675 Accuracy 0.7998046875\n",
      "Iteration 11230 Training loss 0.019484924152493477 Validation loss 0.01899307779967785 Accuracy 0.8046875\n",
      "Iteration 11240 Training loss 0.01733790896832943 Validation loss 0.018864983692765236 Accuracy 0.8056640625\n",
      "Iteration 11250 Training loss 0.016177136451005936 Validation loss 0.01877153292298317 Accuracy 0.80712890625\n",
      "Iteration 11260 Training loss 0.013047341257333755 Validation loss 0.01917962357401848 Accuracy 0.802734375\n",
      "Iteration 11270 Training loss 0.015201589092612267 Validation loss 0.01906314119696617 Accuracy 0.80322265625\n",
      "Iteration 11280 Training loss 0.017147734761238098 Validation loss 0.019248835742473602 Accuracy 0.8017578125\n",
      "Iteration 11290 Training loss 0.015617633238434792 Validation loss 0.019156619906425476 Accuracy 0.80322265625\n",
      "Iteration 11300 Training loss 0.01586667262017727 Validation loss 0.019254952669143677 Accuracy 0.8017578125\n",
      "Iteration 11310 Training loss 0.01598214916884899 Validation loss 0.019225645810365677 Accuracy 0.80224609375\n",
      "Iteration 11320 Training loss 0.017189135774970055 Validation loss 0.01900475099682808 Accuracy 0.80419921875\n",
      "Iteration 11330 Training loss 0.018844593316316605 Validation loss 0.019959252327680588 Accuracy 0.79541015625\n",
      "Iteration 11340 Training loss 0.016949234530329704 Validation loss 0.019108669832348824 Accuracy 0.8037109375\n",
      "Iteration 11350 Training loss 0.018481263890862465 Validation loss 0.018702073022723198 Accuracy 0.8076171875\n",
      "Iteration 11360 Training loss 0.02260185219347477 Validation loss 0.020872605964541435 Accuracy 0.78564453125\n",
      "Iteration 11370 Training loss 0.016078991815447807 Validation loss 0.01881127990782261 Accuracy 0.806640625\n",
      "Iteration 11380 Training loss 0.019877588376402855 Validation loss 0.019923407584428787 Accuracy 0.7958984375\n",
      "Iteration 11390 Training loss 0.01847863756120205 Validation loss 0.018884483724832535 Accuracy 0.8056640625\n",
      "Iteration 11400 Training loss 0.014756014570593834 Validation loss 0.018709855154156685 Accuracy 0.80810546875\n",
      "Iteration 11410 Training loss 0.017988057807087898 Validation loss 0.019014891237020493 Accuracy 0.80517578125\n",
      "Iteration 11420 Training loss 0.01672234758734703 Validation loss 0.018890375271439552 Accuracy 0.8056640625\n",
      "Iteration 11430 Training loss 0.01652885414659977 Validation loss 0.018856890499591827 Accuracy 0.8056640625\n",
      "Iteration 11440 Training loss 0.018199708312749863 Validation loss 0.019278710708022118 Accuracy 0.8017578125\n",
      "Iteration 11450 Training loss 0.01668773591518402 Validation loss 0.019611943513154984 Accuracy 0.79833984375\n",
      "Iteration 11460 Training loss 0.018052106723189354 Validation loss 0.018875043839216232 Accuracy 0.80615234375\n",
      "Iteration 11470 Training loss 0.020095720887184143 Validation loss 0.02053925208747387 Accuracy 0.78955078125\n",
      "Iteration 11480 Training loss 0.01625349372625351 Validation loss 0.019033562391996384 Accuracy 0.80419921875\n",
      "Iteration 11490 Training loss 0.018466049805283546 Validation loss 0.020032357424497604 Accuracy 0.79443359375\n",
      "Iteration 11500 Training loss 0.016674522310495377 Validation loss 0.019166437909007072 Accuracy 0.80322265625\n",
      "Iteration 11510 Training loss 0.014070598408579826 Validation loss 0.018930448219180107 Accuracy 0.80517578125\n",
      "Iteration 11520 Training loss 0.01580420881509781 Validation loss 0.018890641629695892 Accuracy 0.80517578125\n",
      "Iteration 11530 Training loss 0.016671955585479736 Validation loss 0.019382033497095108 Accuracy 0.80029296875\n",
      "Iteration 11540 Training loss 0.019578730687499046 Validation loss 0.01986413449048996 Accuracy 0.79541015625\n",
      "Iteration 11550 Training loss 0.0157410129904747 Validation loss 0.019225606694817543 Accuracy 0.80224609375\n",
      "Iteration 11560 Training loss 0.015443655662238598 Validation loss 0.01923125609755516 Accuracy 0.80224609375\n",
      "Iteration 11570 Training loss 0.018458882346749306 Validation loss 0.019878821447491646 Accuracy 0.79638671875\n",
      "Iteration 11580 Training loss 0.016137229278683662 Validation loss 0.019339824095368385 Accuracy 0.80126953125\n",
      "Iteration 11590 Training loss 0.014291430823504925 Validation loss 0.019076241180300713 Accuracy 0.802734375\n",
      "Iteration 11600 Training loss 0.01896396465599537 Validation loss 0.02025717869400978 Accuracy 0.79296875\n",
      "Iteration 11610 Training loss 0.015293828211724758 Validation loss 0.018927814438939095 Accuracy 0.80615234375\n",
      "Iteration 11620 Training loss 0.015051581896841526 Validation loss 0.018959227949380875 Accuracy 0.80615234375\n",
      "Iteration 11630 Training loss 0.016376730054616928 Validation loss 0.019679659977555275 Accuracy 0.79833984375\n",
      "Iteration 11640 Training loss 0.01808128133416176 Validation loss 0.018991859629750252 Accuracy 0.80419921875\n",
      "Iteration 11650 Training loss 0.018884776160120964 Validation loss 0.01945531740784645 Accuracy 0.7998046875\n",
      "Iteration 11660 Training loss 0.01519758440554142 Validation loss 0.01885291188955307 Accuracy 0.8046875\n",
      "Iteration 11670 Training loss 0.01551747415214777 Validation loss 0.02033834718167782 Accuracy 0.79150390625\n",
      "Iteration 11680 Training loss 0.01596435159444809 Validation loss 0.019192537292838097 Accuracy 0.8017578125\n",
      "Iteration 11690 Training loss 0.01692468672990799 Validation loss 0.01995108835399151 Accuracy 0.79443359375\n",
      "Iteration 11700 Training loss 0.01774645783007145 Validation loss 0.019748693332076073 Accuracy 0.796875\n",
      "Iteration 11710 Training loss 0.017093565315008163 Validation loss 0.019321149215102196 Accuracy 0.80126953125\n",
      "Iteration 11720 Training loss 0.018206441774964333 Validation loss 0.019297299906611443 Accuracy 0.802734375\n",
      "Iteration 11730 Training loss 0.016064560040831566 Validation loss 0.01917239837348461 Accuracy 0.80419921875\n",
      "Iteration 11740 Training loss 0.017194753512740135 Validation loss 0.01861945539712906 Accuracy 0.80908203125\n",
      "Iteration 11750 Training loss 0.01554478146135807 Validation loss 0.019328352063894272 Accuracy 0.80029296875\n",
      "Iteration 11760 Training loss 0.015561078675091267 Validation loss 0.019124969840049744 Accuracy 0.80322265625\n",
      "Iteration 11770 Training loss 0.018524976447224617 Validation loss 0.01898115500807762 Accuracy 0.8046875\n",
      "Iteration 11780 Training loss 0.01581263542175293 Validation loss 0.01869904436171055 Accuracy 0.8076171875\n",
      "Iteration 11790 Training loss 0.016401465982198715 Validation loss 0.019352586939930916 Accuracy 0.80078125\n",
      "Iteration 11800 Training loss 0.014739476144313812 Validation loss 0.01927519403398037 Accuracy 0.8017578125\n",
      "Iteration 11810 Training loss 0.0156302098184824 Validation loss 0.01935766451060772 Accuracy 0.80078125\n",
      "Iteration 11820 Training loss 0.01619757153093815 Validation loss 0.018782351166009903 Accuracy 0.806640625\n",
      "Iteration 11830 Training loss 0.017441468313336372 Validation loss 0.019033662974834442 Accuracy 0.80419921875\n",
      "Iteration 11840 Training loss 0.02043784223496914 Validation loss 0.01897379569709301 Accuracy 0.8046875\n",
      "Iteration 11850 Training loss 0.017381705343723297 Validation loss 0.020395753905177116 Accuracy 0.79150390625\n",
      "Iteration 11860 Training loss 0.0181315615773201 Validation loss 0.019037971273064613 Accuracy 0.80419921875\n",
      "Iteration 11870 Training loss 0.016847483813762665 Validation loss 0.01901852898299694 Accuracy 0.80517578125\n",
      "Iteration 11880 Training loss 0.018133552744984627 Validation loss 0.019550535827875137 Accuracy 0.7998046875\n",
      "Iteration 11890 Training loss 0.014912881888449192 Validation loss 0.018987229093909264 Accuracy 0.8037109375\n",
      "Iteration 11900 Training loss 0.014587732963263988 Validation loss 0.019287750124931335 Accuracy 0.8017578125\n",
      "Iteration 11910 Training loss 0.016495641320943832 Validation loss 0.019081346690654755 Accuracy 0.80419921875\n",
      "Iteration 11920 Training loss 0.015503793954849243 Validation loss 0.0200611911714077 Accuracy 0.79296875\n",
      "Iteration 11930 Training loss 0.016712281852960587 Validation loss 0.01921766810119152 Accuracy 0.80126953125\n",
      "Iteration 11940 Training loss 0.015025581233203411 Validation loss 0.018762987107038498 Accuracy 0.80712890625\n",
      "Iteration 11950 Training loss 0.016804512590169907 Validation loss 0.01891457289457321 Accuracy 0.80517578125\n",
      "Iteration 11960 Training loss 0.016472412273287773 Validation loss 0.018889809027314186 Accuracy 0.8056640625\n",
      "Iteration 11970 Training loss 0.018141621723771095 Validation loss 0.01905881054699421 Accuracy 0.80419921875\n",
      "Iteration 11980 Training loss 0.018943417817354202 Validation loss 0.018974894657731056 Accuracy 0.80517578125\n",
      "Iteration 11990 Training loss 0.01830288954079151 Validation loss 0.019246995449066162 Accuracy 0.80322265625\n",
      "Iteration 12000 Training loss 0.017315737903118134 Validation loss 0.019176801666617393 Accuracy 0.8017578125\n",
      "Iteration 12010 Training loss 0.01632680743932724 Validation loss 0.018992243334650993 Accuracy 0.8037109375\n",
      "Iteration 12020 Training loss 0.019277451559901237 Validation loss 0.019661039113998413 Accuracy 0.7978515625\n",
      "Iteration 12030 Training loss 0.014997283928096294 Validation loss 0.01844244822859764 Accuracy 0.81005859375\n",
      "Iteration 12040 Training loss 0.01837490126490593 Validation loss 0.018785642459988594 Accuracy 0.8056640625\n",
      "Iteration 12050 Training loss 0.014767097309231758 Validation loss 0.018734591081738472 Accuracy 0.80712890625\n",
      "Iteration 12060 Training loss 0.015712643042206764 Validation loss 0.018956545740365982 Accuracy 0.80517578125\n",
      "Iteration 12070 Training loss 0.01634032651782036 Validation loss 0.01890505850315094 Accuracy 0.806640625\n",
      "Iteration 12080 Training loss 0.014822385273873806 Validation loss 0.019052905961871147 Accuracy 0.80419921875\n",
      "Iteration 12090 Training loss 0.017878934741020203 Validation loss 0.018968364223837852 Accuracy 0.80517578125\n",
      "Iteration 12100 Training loss 0.01769719459116459 Validation loss 0.01892508566379547 Accuracy 0.8046875\n",
      "Iteration 12110 Training loss 0.015890026465058327 Validation loss 0.019091736525297165 Accuracy 0.802734375\n",
      "Iteration 12120 Training loss 0.012853463180363178 Validation loss 0.01909632608294487 Accuracy 0.80322265625\n",
      "Iteration 12130 Training loss 0.015301528386771679 Validation loss 0.020004916936159134 Accuracy 0.79443359375\n",
      "Iteration 12140 Training loss 0.017393426969647408 Validation loss 0.019626840949058533 Accuracy 0.79833984375\n",
      "Iteration 12150 Training loss 0.014649108983576298 Validation loss 0.01874413713812828 Accuracy 0.80810546875\n",
      "Iteration 12160 Training loss 0.015498371794819832 Validation loss 0.018937626853585243 Accuracy 0.80517578125\n",
      "Iteration 12170 Training loss 0.01759394258260727 Validation loss 0.019528286531567574 Accuracy 0.79931640625\n",
      "Iteration 12180 Training loss 0.017637303099036217 Validation loss 0.018977336585521698 Accuracy 0.8046875\n",
      "Iteration 12190 Training loss 0.01735331118106842 Validation loss 0.018667226657271385 Accuracy 0.80810546875\n",
      "Iteration 12200 Training loss 0.01845472864806652 Validation loss 0.019674818962812424 Accuracy 0.7978515625\n",
      "Iteration 12210 Training loss 0.015749836340546608 Validation loss 0.018853839486837387 Accuracy 0.806640625\n",
      "Iteration 12220 Training loss 0.015326235443353653 Validation loss 0.019137291237711906 Accuracy 0.80322265625\n",
      "Iteration 12230 Training loss 0.015702642500400543 Validation loss 0.018987232819199562 Accuracy 0.8056640625\n",
      "Iteration 12240 Training loss 0.01495231781154871 Validation loss 0.018756940960884094 Accuracy 0.806640625\n",
      "Iteration 12250 Training loss 0.02117742970585823 Validation loss 0.01897892728447914 Accuracy 0.8037109375\n",
      "Iteration 12260 Training loss 0.017022959887981415 Validation loss 0.018710697069764137 Accuracy 0.80712890625\n",
      "Iteration 12270 Training loss 0.017509382218122482 Validation loss 0.019131362438201904 Accuracy 0.802734375\n",
      "Iteration 12280 Training loss 0.01515077706426382 Validation loss 0.01923222839832306 Accuracy 0.80126953125\n",
      "Iteration 12290 Training loss 0.018237732350826263 Validation loss 0.018964603543281555 Accuracy 0.80517578125\n",
      "Iteration 12300 Training loss 0.016856443136930466 Validation loss 0.018771832808852196 Accuracy 0.8076171875\n",
      "Iteration 12310 Training loss 0.018235519528388977 Validation loss 0.019059013575315475 Accuracy 0.80419921875\n",
      "Iteration 12320 Training loss 0.017244936898350716 Validation loss 0.019150981679558754 Accuracy 0.80322265625\n",
      "Iteration 12330 Training loss 0.017191242426633835 Validation loss 0.019018199294805527 Accuracy 0.8046875\n",
      "Iteration 12340 Training loss 0.017366452142596245 Validation loss 0.01893913559615612 Accuracy 0.80517578125\n",
      "Iteration 12350 Training loss 0.014103055000305176 Validation loss 0.01883726939558983 Accuracy 0.806640625\n",
      "Iteration 12360 Training loss 0.016085902228951454 Validation loss 0.019089404493570328 Accuracy 0.8046875\n",
      "Iteration 12370 Training loss 0.014380858279764652 Validation loss 0.019010724499821663 Accuracy 0.8046875\n",
      "Iteration 12380 Training loss 0.01281630527228117 Validation loss 0.019110407680273056 Accuracy 0.8046875\n",
      "Iteration 12390 Training loss 0.015960106626152992 Validation loss 0.020255232229828835 Accuracy 0.79248046875\n",
      "Iteration 12400 Training loss 0.01565876044332981 Validation loss 0.01937939040362835 Accuracy 0.80078125\n",
      "Iteration 12410 Training loss 0.016110029071569443 Validation loss 0.018904339522123337 Accuracy 0.8056640625\n",
      "Iteration 12420 Training loss 0.017377212643623352 Validation loss 0.01929723657667637 Accuracy 0.80224609375\n",
      "Iteration 12430 Training loss 0.01646154187619686 Validation loss 0.01861540786921978 Accuracy 0.80908203125\n",
      "Iteration 12440 Training loss 0.015557807870209217 Validation loss 0.018453752622008324 Accuracy 0.810546875\n",
      "Iteration 12450 Training loss 0.018476752564311028 Validation loss 0.01896478980779648 Accuracy 0.8046875\n",
      "Iteration 12460 Training loss 0.019222361966967583 Validation loss 0.018939141184091568 Accuracy 0.80517578125\n",
      "Iteration 12470 Training loss 0.01450270600616932 Validation loss 0.018958818167448044 Accuracy 0.80419921875\n",
      "Iteration 12480 Training loss 0.01649964042007923 Validation loss 0.018545180559158325 Accuracy 0.8095703125\n",
      "Iteration 12490 Training loss 0.01586175337433815 Validation loss 0.01870667189359665 Accuracy 0.80615234375\n",
      "Iteration 12500 Training loss 0.013331093825399876 Validation loss 0.018975187093019485 Accuracy 0.8046875\n",
      "Iteration 12510 Training loss 0.015091652981936932 Validation loss 0.019690409302711487 Accuracy 0.79833984375\n",
      "Iteration 12520 Training loss 0.016573650762438774 Validation loss 0.01831621490418911 Accuracy 0.81103515625\n",
      "Iteration 12530 Training loss 0.016296232119202614 Validation loss 0.018777789548039436 Accuracy 0.80615234375\n",
      "Iteration 12540 Training loss 0.018388040363788605 Validation loss 0.01848602294921875 Accuracy 0.8095703125\n",
      "Iteration 12550 Training loss 0.016348106786608696 Validation loss 0.018952567130327225 Accuracy 0.80322265625\n",
      "Iteration 12560 Training loss 0.018468813970685005 Validation loss 0.01961517333984375 Accuracy 0.79736328125\n",
      "Iteration 12570 Training loss 0.014949322678148746 Validation loss 0.018743284046649933 Accuracy 0.8056640625\n",
      "Iteration 12580 Training loss 0.01707957126200199 Validation loss 0.019590994343161583 Accuracy 0.79833984375\n",
      "Iteration 12590 Training loss 0.016642460599541664 Validation loss 0.018751440569758415 Accuracy 0.80712890625\n",
      "Iteration 12600 Training loss 0.01666734740138054 Validation loss 0.01885570026934147 Accuracy 0.80712890625\n",
      "Iteration 12610 Training loss 0.014808962121605873 Validation loss 0.018594535067677498 Accuracy 0.8076171875\n",
      "Iteration 12620 Training loss 0.01654234156012535 Validation loss 0.018925746902823448 Accuracy 0.80419921875\n",
      "Iteration 12630 Training loss 0.018335215747356415 Validation loss 0.01843925565481186 Accuracy 0.8095703125\n",
      "Iteration 12640 Training loss 0.018737606704235077 Validation loss 0.019238295033574104 Accuracy 0.8017578125\n",
      "Iteration 12650 Training loss 0.01787818782031536 Validation loss 0.018874621018767357 Accuracy 0.80517578125\n",
      "Iteration 12660 Training loss 0.018294962123036385 Validation loss 0.019266659393906593 Accuracy 0.80126953125\n",
      "Iteration 12670 Training loss 0.0173883568495512 Validation loss 0.019602132961153984 Accuracy 0.7978515625\n",
      "Iteration 12680 Training loss 0.017988627776503563 Validation loss 0.019577516242861748 Accuracy 0.7978515625\n",
      "Iteration 12690 Training loss 0.017427757382392883 Validation loss 0.018841952085494995 Accuracy 0.8056640625\n",
      "Iteration 12700 Training loss 0.01434249710291624 Validation loss 0.018720535561442375 Accuracy 0.8076171875\n",
      "Iteration 12710 Training loss 0.016362905502319336 Validation loss 0.018727390095591545 Accuracy 0.80712890625\n",
      "Iteration 12720 Training loss 0.014394573867321014 Validation loss 0.018584182485938072 Accuracy 0.80908203125\n",
      "Iteration 12730 Training loss 0.01501868199557066 Validation loss 0.018954738974571228 Accuracy 0.80419921875\n",
      "Iteration 12740 Training loss 0.01571366935968399 Validation loss 0.018745746463537216 Accuracy 0.80712890625\n",
      "Iteration 12750 Training loss 0.015107415616512299 Validation loss 0.018793698400259018 Accuracy 0.8056640625\n",
      "Iteration 12760 Training loss 0.013517135754227638 Validation loss 0.018707670271396637 Accuracy 0.80615234375\n",
      "Iteration 12770 Training loss 0.01708320714533329 Validation loss 0.019556129351258278 Accuracy 0.798828125\n",
      "Iteration 12780 Training loss 0.016909008845686913 Validation loss 0.018905578181147575 Accuracy 0.80615234375\n",
      "Iteration 12790 Training loss 0.013690761290490627 Validation loss 0.018905162811279297 Accuracy 0.80615234375\n",
      "Iteration 12800 Training loss 0.01427381206303835 Validation loss 0.01852688379585743 Accuracy 0.8095703125\n",
      "Iteration 12810 Training loss 0.01772174797952175 Validation loss 0.01899479702115059 Accuracy 0.8046875\n",
      "Iteration 12820 Training loss 0.018249517306685448 Validation loss 0.01899007335305214 Accuracy 0.8046875\n",
      "Iteration 12830 Training loss 0.015784693881869316 Validation loss 0.018694883212447166 Accuracy 0.80810546875\n",
      "Iteration 12840 Training loss 0.015676017850637436 Validation loss 0.018431102856993675 Accuracy 0.81005859375\n",
      "Iteration 12850 Training loss 0.01736716739833355 Validation loss 0.01850145123898983 Accuracy 0.80908203125\n",
      "Iteration 12860 Training loss 0.02066030353307724 Validation loss 0.01972450688481331 Accuracy 0.7958984375\n",
      "Iteration 12870 Training loss 0.01631971448659897 Validation loss 0.01899918168783188 Accuracy 0.8037109375\n",
      "Iteration 12880 Training loss 0.015923112630844116 Validation loss 0.019032157957553864 Accuracy 0.80322265625\n",
      "Iteration 12890 Training loss 0.01925630122423172 Validation loss 0.019111357629299164 Accuracy 0.80322265625\n",
      "Iteration 12900 Training loss 0.0163718294352293 Validation loss 0.0188551414757967 Accuracy 0.80517578125\n",
      "Iteration 12910 Training loss 0.01942463591694832 Validation loss 0.019726771861314774 Accuracy 0.79736328125\n",
      "Iteration 12920 Training loss 0.014025884680449963 Validation loss 0.01904181018471718 Accuracy 0.80419921875\n",
      "Iteration 12930 Training loss 0.016672076657414436 Validation loss 0.018560728058218956 Accuracy 0.80810546875\n",
      "Iteration 12940 Training loss 0.01668175496160984 Validation loss 0.01856847107410431 Accuracy 0.80712890625\n",
      "Iteration 12950 Training loss 0.0135949170216918 Validation loss 0.019065529108047485 Accuracy 0.80322265625\n",
      "Iteration 12960 Training loss 0.01374827604740858 Validation loss 0.01889747381210327 Accuracy 0.80419921875\n",
      "Iteration 12970 Training loss 0.017223022878170013 Validation loss 0.01953713409602642 Accuracy 0.7978515625\n",
      "Iteration 12980 Training loss 0.015409957617521286 Validation loss 0.01942463405430317 Accuracy 0.7998046875\n",
      "Iteration 12990 Training loss 0.01568642631173134 Validation loss 0.01887470670044422 Accuracy 0.8046875\n",
      "Iteration 13000 Training loss 0.017520802095532417 Validation loss 0.018838876858353615 Accuracy 0.80517578125\n",
      "Iteration 13010 Training loss 0.014026926830410957 Validation loss 0.018715379759669304 Accuracy 0.806640625\n",
      "Iteration 13020 Training loss 0.014062142930924892 Validation loss 0.018815500661730766 Accuracy 0.80712890625\n",
      "Iteration 13030 Training loss 0.013959714211523533 Validation loss 0.018566595390439034 Accuracy 0.80859375\n",
      "Iteration 13040 Training loss 0.014665690250694752 Validation loss 0.01914331316947937 Accuracy 0.8037109375\n",
      "Iteration 13050 Training loss 0.016405818983912468 Validation loss 0.018680226057767868 Accuracy 0.80810546875\n",
      "Iteration 13060 Training loss 0.014370208606123924 Validation loss 0.018893815577030182 Accuracy 0.80419921875\n",
      "Iteration 13070 Training loss 0.017990674823522568 Validation loss 0.018923116847872734 Accuracy 0.8037109375\n",
      "Iteration 13080 Training loss 0.016379840672016144 Validation loss 0.018537182360887527 Accuracy 0.80859375\n",
      "Iteration 13090 Training loss 0.015004641376435757 Validation loss 0.01868295669555664 Accuracy 0.80810546875\n",
      "Iteration 13100 Training loss 0.015699047595262527 Validation loss 0.018794022500514984 Accuracy 0.8056640625\n",
      "Iteration 13110 Training loss 0.0161670483648777 Validation loss 0.018627025187015533 Accuracy 0.8076171875\n",
      "Iteration 13120 Training loss 0.01745646260678768 Validation loss 0.01879294402897358 Accuracy 0.8056640625\n",
      "Iteration 13130 Training loss 0.013534445315599442 Validation loss 0.018450958654284477 Accuracy 0.80908203125\n",
      "Iteration 13140 Training loss 0.016773724928498268 Validation loss 0.018854239955544472 Accuracy 0.8037109375\n",
      "Iteration 13150 Training loss 0.016500147059559822 Validation loss 0.01869514398276806 Accuracy 0.80712890625\n",
      "Iteration 13160 Training loss 0.017394106835126877 Validation loss 0.018524926155805588 Accuracy 0.80908203125\n",
      "Iteration 13170 Training loss 0.017586389556527138 Validation loss 0.01955721713602543 Accuracy 0.79736328125\n",
      "Iteration 13180 Training loss 0.017683615908026695 Validation loss 0.018828140571713448 Accuracy 0.8056640625\n",
      "Iteration 13190 Training loss 0.015501691028475761 Validation loss 0.01872045174241066 Accuracy 0.8056640625\n",
      "Iteration 13200 Training loss 0.019230496138334274 Validation loss 0.019318940117955208 Accuracy 0.8017578125\n",
      "Iteration 13210 Training loss 0.02018718607723713 Validation loss 0.019442692399024963 Accuracy 0.80029296875\n",
      "Iteration 13220 Training loss 0.016663312911987305 Validation loss 0.019013117998838425 Accuracy 0.80419921875\n",
      "Iteration 13230 Training loss 0.016411345452070236 Validation loss 0.01884370669722557 Accuracy 0.806640625\n",
      "Iteration 13240 Training loss 0.018692374229431152 Validation loss 0.019108224660158157 Accuracy 0.802734375\n",
      "Iteration 13250 Training loss 0.014481175690889359 Validation loss 0.018675094470381737 Accuracy 0.8076171875\n",
      "Iteration 13260 Training loss 0.01925336942076683 Validation loss 0.018425431102514267 Accuracy 0.8095703125\n",
      "Iteration 13270 Training loss 0.019906604662537575 Validation loss 0.019717292860150337 Accuracy 0.7978515625\n",
      "Iteration 13280 Training loss 0.01661224663257599 Validation loss 0.01897607557475567 Accuracy 0.80419921875\n",
      "Iteration 13290 Training loss 0.017111051827669144 Validation loss 0.018437987193465233 Accuracy 0.81005859375\n",
      "Iteration 13300 Training loss 0.016006002202630043 Validation loss 0.018866868689656258 Accuracy 0.80517578125\n",
      "Iteration 13310 Training loss 0.014460115693509579 Validation loss 0.018283652141690254 Accuracy 0.8115234375\n",
      "Iteration 13320 Training loss 0.0150214284658432 Validation loss 0.019117828458547592 Accuracy 0.802734375\n",
      "Iteration 13330 Training loss 0.01668880134820938 Validation loss 0.018868666142225266 Accuracy 0.80419921875\n",
      "Iteration 13340 Training loss 0.015364769846200943 Validation loss 0.018779562786221504 Accuracy 0.80517578125\n",
      "Iteration 13350 Training loss 0.016415180638432503 Validation loss 0.019189627841114998 Accuracy 0.8017578125\n",
      "Iteration 13360 Training loss 0.013906191103160381 Validation loss 0.01864372007548809 Accuracy 0.80810546875\n",
      "Iteration 13370 Training loss 0.014977316372096539 Validation loss 0.018596995621919632 Accuracy 0.8076171875\n",
      "Iteration 13380 Training loss 0.017422489821910858 Validation loss 0.01853039488196373 Accuracy 0.8076171875\n",
      "Iteration 13390 Training loss 0.01441926322877407 Validation loss 0.01827274262905121 Accuracy 0.81103515625\n",
      "Iteration 13400 Training loss 0.01759110577404499 Validation loss 0.018603673204779625 Accuracy 0.8076171875\n",
      "Iteration 13410 Training loss 0.015415745787322521 Validation loss 0.018489260226488113 Accuracy 0.80859375\n",
      "Iteration 13420 Training loss 0.016791507601737976 Validation loss 0.018819527700543404 Accuracy 0.8056640625\n",
      "Iteration 13430 Training loss 0.015316522680222988 Validation loss 0.018946362659335136 Accuracy 0.8046875\n",
      "Iteration 13440 Training loss 0.014647398144006729 Validation loss 0.018584439530968666 Accuracy 0.8076171875\n",
      "Iteration 13450 Training loss 0.017183659598231316 Validation loss 0.019358297809958458 Accuracy 0.798828125\n",
      "Iteration 13460 Training loss 0.01523649599403143 Validation loss 0.01878480426967144 Accuracy 0.80615234375\n",
      "Iteration 13470 Training loss 0.01436314545571804 Validation loss 0.018413253128528595 Accuracy 0.81005859375\n",
      "Iteration 13480 Training loss 0.014857240952551365 Validation loss 0.018621258437633514 Accuracy 0.80712890625\n",
      "Iteration 13490 Training loss 0.012752347625792027 Validation loss 0.018741395324468613 Accuracy 0.8056640625\n",
      "Iteration 13500 Training loss 0.0164723489433527 Validation loss 0.0184037983417511 Accuracy 0.8095703125\n",
      "Iteration 13510 Training loss 0.02000049129128456 Validation loss 0.01891011744737625 Accuracy 0.80322265625\n",
      "Iteration 13520 Training loss 0.015680935233831406 Validation loss 0.01818283647298813 Accuracy 0.81103515625\n",
      "Iteration 13530 Training loss 0.016353605315089226 Validation loss 0.01839986816048622 Accuracy 0.8095703125\n",
      "Iteration 13540 Training loss 0.01362114679068327 Validation loss 0.018698837608098984 Accuracy 0.80615234375\n",
      "Iteration 13550 Training loss 0.012263988144695759 Validation loss 0.018440265208482742 Accuracy 0.810546875\n",
      "Iteration 13560 Training loss 0.016285637393593788 Validation loss 0.018588555976748466 Accuracy 0.8076171875\n",
      "Iteration 13570 Training loss 0.016626780852675438 Validation loss 0.018443433567881584 Accuracy 0.80859375\n",
      "Iteration 13580 Training loss 0.013534177094697952 Validation loss 0.018489720299839973 Accuracy 0.80810546875\n",
      "Iteration 13590 Training loss 0.014129010029137135 Validation loss 0.018194444477558136 Accuracy 0.81103515625\n",
      "Iteration 13600 Training loss 0.01578105427324772 Validation loss 0.01872534118592739 Accuracy 0.80517578125\n",
      "Iteration 13610 Training loss 0.016209792345762253 Validation loss 0.01867685467004776 Accuracy 0.80615234375\n",
      "Iteration 13620 Training loss 0.016713207587599754 Validation loss 0.01831366866827011 Accuracy 0.810546875\n",
      "Iteration 13630 Training loss 0.01654166541993618 Validation loss 0.018834685906767845 Accuracy 0.8037109375\n",
      "Iteration 13640 Training loss 0.01436953991651535 Validation loss 0.01858573593199253 Accuracy 0.80810546875\n",
      "Iteration 13650 Training loss 0.016490301117300987 Validation loss 0.01850230246782303 Accuracy 0.80859375\n",
      "Iteration 13660 Training loss 0.01773110032081604 Validation loss 0.019018063321709633 Accuracy 0.80322265625\n",
      "Iteration 13670 Training loss 0.016465120017528534 Validation loss 0.019403226673603058 Accuracy 0.798828125\n",
      "Iteration 13680 Training loss 0.016803346574306488 Validation loss 0.018636727705597878 Accuracy 0.8076171875\n",
      "Iteration 13690 Training loss 0.015230707824230194 Validation loss 0.018586572259664536 Accuracy 0.80810546875\n",
      "Iteration 13700 Training loss 0.01637698896229267 Validation loss 0.019733134657144547 Accuracy 0.79736328125\n",
      "Iteration 13710 Training loss 0.015869975090026855 Validation loss 0.01852625235915184 Accuracy 0.80859375\n",
      "Iteration 13720 Training loss 0.0187546256929636 Validation loss 0.01992400363087654 Accuracy 0.79443359375\n",
      "Iteration 13730 Training loss 0.017314068973064423 Validation loss 0.018756888806819916 Accuracy 0.8056640625\n",
      "Iteration 13740 Training loss 0.016823623329401016 Validation loss 0.018672989681363106 Accuracy 0.80712890625\n",
      "Iteration 13750 Training loss 0.01613643206655979 Validation loss 0.01847204752266407 Accuracy 0.8095703125\n",
      "Iteration 13760 Training loss 0.017346220090985298 Validation loss 0.018643591552972794 Accuracy 0.80712890625\n",
      "Iteration 13770 Training loss 0.015959784388542175 Validation loss 0.01880570687353611 Accuracy 0.80615234375\n",
      "Iteration 13780 Training loss 0.01779499463737011 Validation loss 0.018670879304409027 Accuracy 0.80615234375\n",
      "Iteration 13790 Training loss 0.014200927689671516 Validation loss 0.01863757334649563 Accuracy 0.806640625\n",
      "Iteration 13800 Training loss 0.015797242522239685 Validation loss 0.018935687839984894 Accuracy 0.80419921875\n",
      "Iteration 13810 Training loss 0.015002377331256866 Validation loss 0.018546951934695244 Accuracy 0.80859375\n",
      "Iteration 13820 Training loss 0.014326670207083225 Validation loss 0.01869778148829937 Accuracy 0.80615234375\n",
      "Iteration 13830 Training loss 0.016470085829496384 Validation loss 0.01842041127383709 Accuracy 0.81005859375\n",
      "Iteration 13840 Training loss 0.015101665630936623 Validation loss 0.018459906801581383 Accuracy 0.80810546875\n",
      "Iteration 13850 Training loss 0.015412088483572006 Validation loss 0.0185107234865427 Accuracy 0.80859375\n",
      "Iteration 13860 Training loss 0.016969729214906693 Validation loss 0.018521703779697418 Accuracy 0.80859375\n",
      "Iteration 13870 Training loss 0.014716732315719128 Validation loss 0.018694385886192322 Accuracy 0.80712890625\n",
      "Iteration 13880 Training loss 0.017758948728442192 Validation loss 0.018740326166152954 Accuracy 0.8056640625\n",
      "Iteration 13890 Training loss 0.015952302142977715 Validation loss 0.018529493361711502 Accuracy 0.80908203125\n",
      "Iteration 13900 Training loss 0.019297994673252106 Validation loss 0.018661413341760635 Accuracy 0.80712890625\n",
      "Iteration 13910 Training loss 0.016652705147862434 Validation loss 0.018378395587205887 Accuracy 0.80908203125\n",
      "Iteration 13920 Training loss 0.016807029023766518 Validation loss 0.01841118186712265 Accuracy 0.81005859375\n",
      "Iteration 13930 Training loss 0.01760607212781906 Validation loss 0.01839854009449482 Accuracy 0.81005859375\n",
      "Iteration 13940 Training loss 0.015362787991762161 Validation loss 0.018360160291194916 Accuracy 0.8095703125\n",
      "Iteration 13950 Training loss 0.013484136201441288 Validation loss 0.01839120127260685 Accuracy 0.8095703125\n",
      "Iteration 13960 Training loss 0.0156979039311409 Validation loss 0.018495140597224236 Accuracy 0.80810546875\n",
      "Iteration 13970 Training loss 0.016927585005760193 Validation loss 0.019358597695827484 Accuracy 0.7998046875\n",
      "Iteration 13980 Training loss 0.014789922162890434 Validation loss 0.019326385110616684 Accuracy 0.798828125\n",
      "Iteration 13990 Training loss 0.018493741750717163 Validation loss 0.019015345722436905 Accuracy 0.802734375\n",
      "Iteration 14000 Training loss 0.017146853730082512 Validation loss 0.019108377397060394 Accuracy 0.8037109375\n",
      "Iteration 14010 Training loss 0.015042525716125965 Validation loss 0.018634630367159843 Accuracy 0.80712890625\n",
      "Iteration 14020 Training loss 0.014638606458902359 Validation loss 0.018689002841711044 Accuracy 0.806640625\n",
      "Iteration 14030 Training loss 0.01599322259426117 Validation loss 0.01834707520902157 Accuracy 0.8095703125\n",
      "Iteration 14040 Training loss 0.015781601890921593 Validation loss 0.019521109759807587 Accuracy 0.79833984375\n",
      "Iteration 14050 Training loss 0.018572745844721794 Validation loss 0.01847890391945839 Accuracy 0.80859375\n",
      "Iteration 14060 Training loss 0.014468975365161896 Validation loss 0.01862333156168461 Accuracy 0.806640625\n",
      "Iteration 14070 Training loss 0.0157243050634861 Validation loss 0.01889663003385067 Accuracy 0.802734375\n",
      "Iteration 14080 Training loss 0.01439580786973238 Validation loss 0.018926825374364853 Accuracy 0.80322265625\n",
      "Iteration 14090 Training loss 0.014562695287168026 Validation loss 0.018510473892092705 Accuracy 0.8076171875\n",
      "Iteration 14100 Training loss 0.016077451407909393 Validation loss 0.018785575404763222 Accuracy 0.8046875\n",
      "Iteration 14110 Training loss 0.013928454369306564 Validation loss 0.018488457426428795 Accuracy 0.80810546875\n",
      "Iteration 14120 Training loss 0.012959431856870651 Validation loss 0.018564406782388687 Accuracy 0.8076171875\n",
      "Iteration 14130 Training loss 0.01547912321984768 Validation loss 0.01863142102956772 Accuracy 0.806640625\n",
      "Iteration 14140 Training loss 0.017700886353850365 Validation loss 0.018652236089110374 Accuracy 0.80615234375\n",
      "Iteration 14150 Training loss 0.013529034331440926 Validation loss 0.018383346498012543 Accuracy 0.80908203125\n",
      "Iteration 14160 Training loss 0.014973591081798077 Validation loss 0.018632791936397552 Accuracy 0.80712890625\n",
      "Iteration 14170 Training loss 0.012576424516737461 Validation loss 0.018486903980374336 Accuracy 0.80908203125\n",
      "Iteration 14180 Training loss 0.012893694452941418 Validation loss 0.018356474116444588 Accuracy 0.8095703125\n",
      "Iteration 14190 Training loss 0.016360612586140633 Validation loss 0.01879573054611683 Accuracy 0.8046875\n",
      "Iteration 14200 Training loss 0.014117174781858921 Validation loss 0.018005361780524254 Accuracy 0.8134765625\n",
      "Iteration 14210 Training loss 0.015215090475976467 Validation loss 0.0184367373585701 Accuracy 0.80810546875\n",
      "Iteration 14220 Training loss 0.01605016365647316 Validation loss 0.019131137058138847 Accuracy 0.8017578125\n",
      "Iteration 14230 Training loss 0.014560192823410034 Validation loss 0.01904805190861225 Accuracy 0.802734375\n",
      "Iteration 14240 Training loss 0.018755266442894936 Validation loss 0.01933952420949936 Accuracy 0.7998046875\n",
      "Iteration 14250 Training loss 0.013271176256239414 Validation loss 0.018447093665599823 Accuracy 0.80859375\n",
      "Iteration 14260 Training loss 0.014421643689274788 Validation loss 0.018482357263565063 Accuracy 0.80810546875\n",
      "Iteration 14270 Training loss 0.01721552386879921 Validation loss 0.018120737746357918 Accuracy 0.8125\n",
      "Iteration 14280 Training loss 0.01563955470919609 Validation loss 0.018666110932826996 Accuracy 0.806640625\n",
      "Iteration 14290 Training loss 0.017333708703517914 Validation loss 0.018345976248383522 Accuracy 0.8095703125\n",
      "Iteration 14300 Training loss 0.016509991139173508 Validation loss 0.018703032284975052 Accuracy 0.80615234375\n",
      "Iteration 14310 Training loss 0.016309354454278946 Validation loss 0.018739525228738785 Accuracy 0.8056640625\n",
      "Iteration 14320 Training loss 0.017002349719405174 Validation loss 0.018160507082939148 Accuracy 0.8125\n",
      "Iteration 14330 Training loss 0.016066523268818855 Validation loss 0.018622487783432007 Accuracy 0.80712890625\n",
      "Iteration 14340 Training loss 0.014841010794043541 Validation loss 0.018285058438777924 Accuracy 0.81103515625\n",
      "Iteration 14350 Training loss 0.01595268025994301 Validation loss 0.018444128334522247 Accuracy 0.80810546875\n",
      "Iteration 14360 Training loss 0.014564508572220802 Validation loss 0.01863083243370056 Accuracy 0.806640625\n",
      "Iteration 14370 Training loss 0.015435920096933842 Validation loss 0.018297895789146423 Accuracy 0.81005859375\n",
      "Iteration 14380 Training loss 0.016655806452035904 Validation loss 0.01811337284743786 Accuracy 0.8115234375\n",
      "Iteration 14390 Training loss 0.014962023124098778 Validation loss 0.018060065805912018 Accuracy 0.8125\n",
      "Iteration 14400 Training loss 0.015105511993169785 Validation loss 0.01826884225010872 Accuracy 0.810546875\n",
      "Iteration 14410 Training loss 0.012575769796967506 Validation loss 0.019060563296079636 Accuracy 0.8017578125\n",
      "Iteration 14420 Training loss 0.014781690202653408 Validation loss 0.018429776653647423 Accuracy 0.81005859375\n",
      "Iteration 14430 Training loss 0.01567974127829075 Validation loss 0.0185557771474123 Accuracy 0.80810546875\n",
      "Iteration 14440 Training loss 0.016713600605726242 Validation loss 0.01868453249335289 Accuracy 0.80712890625\n",
      "Iteration 14450 Training loss 0.018167296424508095 Validation loss 0.018253348767757416 Accuracy 0.81005859375\n",
      "Iteration 14460 Training loss 0.014897636137902737 Validation loss 0.018277497962117195 Accuracy 0.80908203125\n",
      "Iteration 14470 Training loss 0.014433883130550385 Validation loss 0.01860233023762703 Accuracy 0.80712890625\n",
      "Iteration 14480 Training loss 0.016116760671138763 Validation loss 0.01845964603126049 Accuracy 0.80859375\n",
      "Iteration 14490 Training loss 0.018249046057462692 Validation loss 0.01829397864639759 Accuracy 0.81005859375\n",
      "Iteration 14500 Training loss 0.01699681766331196 Validation loss 0.018303420394659042 Accuracy 0.810546875\n",
      "Iteration 14510 Training loss 0.013047505170106888 Validation loss 0.018434759229421616 Accuracy 0.80908203125\n",
      "Iteration 14520 Training loss 0.015212373808026314 Validation loss 0.01861557550728321 Accuracy 0.80712890625\n",
      "Iteration 14530 Training loss 0.01488039642572403 Validation loss 0.01829581893980503 Accuracy 0.81005859375\n",
      "Iteration 14540 Training loss 0.018544290214776993 Validation loss 0.019039997830986977 Accuracy 0.802734375\n",
      "Iteration 14550 Training loss 0.016803249716758728 Validation loss 0.01832122541964054 Accuracy 0.81005859375\n",
      "Iteration 14560 Training loss 0.015242879278957844 Validation loss 0.018409280106425285 Accuracy 0.80859375\n",
      "Iteration 14570 Training loss 0.014840856194496155 Validation loss 0.01831831783056259 Accuracy 0.8095703125\n",
      "Iteration 14580 Training loss 0.016471482813358307 Validation loss 0.018808653578162193 Accuracy 0.80419921875\n",
      "Iteration 14590 Training loss 0.014268750324845314 Validation loss 0.01819480024278164 Accuracy 0.81103515625\n",
      "Iteration 14600 Training loss 0.016110597178339958 Validation loss 0.01844807155430317 Accuracy 0.8076171875\n",
      "Iteration 14610 Training loss 0.018064089119434357 Validation loss 0.020094096660614014 Accuracy 0.79150390625\n",
      "Iteration 14620 Training loss 0.01646418683230877 Validation loss 0.018464788794517517 Accuracy 0.80859375\n",
      "Iteration 14630 Training loss 0.01569165103137493 Validation loss 0.01802739128470421 Accuracy 0.8125\n",
      "Iteration 14640 Training loss 0.010912188328802586 Validation loss 0.014726641587913036 Accuracy 0.845703125\n",
      "Iteration 14650 Training loss 0.010517633520066738 Validation loss 0.014506022445857525 Accuracy 0.84814453125\n",
      "Iteration 14660 Training loss 0.012877007946372032 Validation loss 0.015610819682478905 Accuracy 0.83740234375\n",
      "Iteration 14670 Training loss 0.009510007686913013 Validation loss 0.014740981161594391 Accuracy 0.84619140625\n",
      "Iteration 14680 Training loss 0.012846323661506176 Validation loss 0.014133657328784466 Accuracy 0.8525390625\n",
      "Iteration 14690 Training loss 0.010261187329888344 Validation loss 0.014883366413414478 Accuracy 0.8447265625\n",
      "Iteration 14700 Training loss 0.011169067583978176 Validation loss 0.014142709784209728 Accuracy 0.8525390625\n",
      "Iteration 14710 Training loss 0.008866585791110992 Validation loss 0.01371508277952671 Accuracy 0.8544921875\n",
      "Iteration 14720 Training loss 0.011911292560398579 Validation loss 0.01375108864158392 Accuracy 0.857421875\n",
      "Iteration 14730 Training loss 0.012028200551867485 Validation loss 0.013522477820515633 Accuracy 0.85888671875\n",
      "Iteration 14740 Training loss 0.010760308243334293 Validation loss 0.013510571792721748 Accuracy 0.85888671875\n",
      "Iteration 14750 Training loss 0.01401557121425867 Validation loss 0.015424384735524654 Accuracy 0.83837890625\n",
      "Iteration 14760 Training loss 0.011701817624270916 Validation loss 0.014534229412674904 Accuracy 0.84716796875\n",
      "Iteration 14770 Training loss 0.01020811963826418 Validation loss 0.014247670769691467 Accuracy 0.8515625\n",
      "Iteration 14780 Training loss 0.010956293903291225 Validation loss 0.013388622552156448 Accuracy 0.86083984375\n",
      "Iteration 14790 Training loss 0.009049015119671822 Validation loss 0.013299201615154743 Accuracy 0.861328125\n",
      "Iteration 14800 Training loss 0.009298079647123814 Validation loss 0.013126823119819164 Accuracy 0.86328125\n",
      "Iteration 14810 Training loss 0.011429807171225548 Validation loss 0.014179423451423645 Accuracy 0.8525390625\n",
      "Iteration 14820 Training loss 0.011617721058428288 Validation loss 0.013869289308786392 Accuracy 0.85498046875\n",
      "Iteration 14830 Training loss 0.011694811284542084 Validation loss 0.014235073700547218 Accuracy 0.85107421875\n",
      "Iteration 14840 Training loss 0.011726335622370243 Validation loss 0.014009950682520866 Accuracy 0.853515625\n",
      "Iteration 14850 Training loss 0.01175269391387701 Validation loss 0.013547985814511776 Accuracy 0.8583984375\n",
      "Iteration 14860 Training loss 0.009136194363236427 Validation loss 0.01296147145330906 Accuracy 0.865234375\n",
      "Iteration 14870 Training loss 0.010298590175807476 Validation loss 0.014062987640500069 Accuracy 0.8525390625\n",
      "Iteration 14880 Training loss 0.010818498209118843 Validation loss 0.013688365928828716 Accuracy 0.85693359375\n",
      "Iteration 14890 Training loss 0.01174042746424675 Validation loss 0.01372595690190792 Accuracy 0.857421875\n",
      "Iteration 14900 Training loss 0.011778337880969048 Validation loss 0.014428644441068172 Accuracy 0.849609375\n",
      "Iteration 14910 Training loss 0.010950678028166294 Validation loss 0.013868283480405807 Accuracy 0.85595703125\n",
      "Iteration 14920 Training loss 0.008451879024505615 Validation loss 0.013625108636915684 Accuracy 0.85791015625\n",
      "Iteration 14930 Training loss 0.011644861660897732 Validation loss 0.01356505136936903 Accuracy 0.859375\n",
      "Iteration 14940 Training loss 0.009832755662500858 Validation loss 0.013300632126629353 Accuracy 0.861328125\n",
      "Iteration 14950 Training loss 0.008285013027489185 Validation loss 0.013698039576411247 Accuracy 0.857421875\n",
      "Iteration 14960 Training loss 0.01066703163087368 Validation loss 0.01420533936470747 Accuracy 0.85205078125\n",
      "Iteration 14970 Training loss 0.008930251933634281 Validation loss 0.013482457958161831 Accuracy 0.85986328125\n",
      "Iteration 14980 Training loss 0.010162466205656528 Validation loss 0.013689755462110043 Accuracy 0.85791015625\n",
      "Iteration 14990 Training loss 0.007794716861099005 Validation loss 0.012977750040590763 Accuracy 0.86572265625\n",
      "Iteration 15000 Training loss 0.010708709247410297 Validation loss 0.01311374083161354 Accuracy 0.86376953125\n",
      "Iteration 15010 Training loss 0.009220589883625507 Validation loss 0.01335434801876545 Accuracy 0.86083984375\n",
      "Iteration 15020 Training loss 0.009859371930360794 Validation loss 0.013312953524291515 Accuracy 0.861328125\n",
      "Iteration 15030 Training loss 0.010782492347061634 Validation loss 0.013285180553793907 Accuracy 0.86181640625\n",
      "Iteration 15040 Training loss 0.011038286611437798 Validation loss 0.012864772230386734 Accuracy 0.8662109375\n",
      "Iteration 15050 Training loss 0.009377744980156422 Validation loss 0.012679245322942734 Accuracy 0.8681640625\n",
      "Iteration 15060 Training loss 0.007932690903544426 Validation loss 0.012795394286513329 Accuracy 0.8662109375\n",
      "Iteration 15070 Training loss 0.00888761319220066 Validation loss 0.01301631424576044 Accuracy 0.8642578125\n",
      "Iteration 15080 Training loss 0.009489121846854687 Validation loss 0.013086005114018917 Accuracy 0.8642578125\n",
      "Iteration 15090 Training loss 0.010592013597488403 Validation loss 0.012981530278921127 Accuracy 0.865234375\n",
      "Iteration 15100 Training loss 0.010883376002311707 Validation loss 0.014378041960299015 Accuracy 0.84912109375\n",
      "Iteration 15110 Training loss 0.011893551796674728 Validation loss 0.013234131969511509 Accuracy 0.86279296875\n",
      "Iteration 15120 Training loss 0.013321688398718834 Validation loss 0.017167173326015472 Accuracy 0.822265625\n",
      "Iteration 15130 Training loss 0.012225860729813576 Validation loss 0.01487198006361723 Accuracy 0.84521484375\n",
      "Iteration 15140 Training loss 0.009443646296858788 Validation loss 0.013164984993636608 Accuracy 0.86328125\n",
      "Iteration 15150 Training loss 0.01130435150116682 Validation loss 0.013126139529049397 Accuracy 0.8623046875\n",
      "Iteration 15160 Training loss 0.011256901547312737 Validation loss 0.013051887974143028 Accuracy 0.86376953125\n",
      "Iteration 15170 Training loss 0.011159513145685196 Validation loss 0.014256244525313377 Accuracy 0.8525390625\n",
      "Iteration 15180 Training loss 0.013602552935481071 Validation loss 0.01575990580022335 Accuracy 0.83544921875\n",
      "Iteration 15190 Training loss 0.009647564962506294 Validation loss 0.012965505011379719 Accuracy 0.8642578125\n",
      "Iteration 15200 Training loss 0.009072056040167809 Validation loss 0.01329717505723238 Accuracy 0.86083984375\n",
      "Iteration 15210 Training loss 0.008868159726262093 Validation loss 0.013511992059648037 Accuracy 0.85791015625\n",
      "Iteration 15220 Training loss 0.013794147409498692 Validation loss 0.015714704990386963 Accuracy 0.8369140625\n",
      "Iteration 15230 Training loss 0.009213426150381565 Validation loss 0.013171089813113213 Accuracy 0.86181640625\n",
      "Iteration 15240 Training loss 0.009686785750091076 Validation loss 0.012925783172249794 Accuracy 0.86474609375\n",
      "Iteration 15250 Training loss 0.009710525162518024 Validation loss 0.012887179851531982 Accuracy 0.865234375\n",
      "Iteration 15260 Training loss 0.012509406544268131 Validation loss 0.014966132119297981 Accuracy 0.84326171875\n",
      "Iteration 15270 Training loss 0.009718041867017746 Validation loss 0.013415313325822353 Accuracy 0.85986328125\n",
      "Iteration 15280 Training loss 0.008435082621872425 Validation loss 0.01290966011583805 Accuracy 0.86572265625\n",
      "Iteration 15290 Training loss 0.009978200308978558 Validation loss 0.012890475802123547 Accuracy 0.86572265625\n",
      "Iteration 15300 Training loss 0.00804896280169487 Validation loss 0.012986703775823116 Accuracy 0.8642578125\n",
      "Iteration 15310 Training loss 0.00911190640181303 Validation loss 0.013949920423328876 Accuracy 0.85400390625\n",
      "Iteration 15320 Training loss 0.01074172742664814 Validation loss 0.013224294409155846 Accuracy 0.861328125\n",
      "Iteration 15330 Training loss 0.009128909558057785 Validation loss 0.013699489645659924 Accuracy 0.857421875\n",
      "Iteration 15340 Training loss 0.007241987157613039 Validation loss 0.013157021254301071 Accuracy 0.8623046875\n",
      "Iteration 15350 Training loss 0.009636270813643932 Validation loss 0.012730513699352741 Accuracy 0.8671875\n",
      "Iteration 15360 Training loss 0.00842283759266138 Validation loss 0.012739906087517738 Accuracy 0.8662109375\n",
      "Iteration 15370 Training loss 0.009574447758495808 Validation loss 0.012592155486345291 Accuracy 0.8681640625\n",
      "Iteration 15380 Training loss 0.010986607521772385 Validation loss 0.01345137134194374 Accuracy 0.85986328125\n",
      "Iteration 15390 Training loss 0.008344556204974651 Validation loss 0.013171061873435974 Accuracy 0.86279296875\n",
      "Iteration 15400 Training loss 0.010377734899520874 Validation loss 0.013465077616274357 Accuracy 0.85986328125\n",
      "Iteration 15410 Training loss 0.009664642624557018 Validation loss 0.013357043266296387 Accuracy 0.85986328125\n",
      "Iteration 15420 Training loss 0.009547661989927292 Validation loss 0.012660844251513481 Accuracy 0.8681640625\n",
      "Iteration 15430 Training loss 0.010995735414326191 Validation loss 0.012876858003437519 Accuracy 0.865234375\n",
      "Iteration 15440 Training loss 0.011383275501430035 Validation loss 0.013673617504537106 Accuracy 0.8564453125\n",
      "Iteration 15450 Training loss 0.010420218110084534 Validation loss 0.01327365543693304 Accuracy 0.8623046875\n",
      "Iteration 15460 Training loss 0.010574573650956154 Validation loss 0.01309913583099842 Accuracy 0.86328125\n",
      "Iteration 15470 Training loss 0.007695936597883701 Validation loss 0.012806104496121407 Accuracy 0.865234375\n",
      "Iteration 15480 Training loss 0.009850955568253994 Validation loss 0.012858648784458637 Accuracy 0.865234375\n",
      "Iteration 15490 Training loss 0.007194054313004017 Validation loss 0.012488010339438915 Accuracy 0.86962890625\n",
      "Iteration 15500 Training loss 0.007604797836393118 Validation loss 0.01256110891699791 Accuracy 0.8681640625\n",
      "Iteration 15510 Training loss 0.008996589109301567 Validation loss 0.012991823256015778 Accuracy 0.8642578125\n",
      "Iteration 15520 Training loss 0.009684378281235695 Validation loss 0.013310873880982399 Accuracy 0.8603515625\n",
      "Iteration 15530 Training loss 0.00918505247682333 Validation loss 0.013212003745138645 Accuracy 0.861328125\n",
      "Iteration 15540 Training loss 0.008557593449950218 Validation loss 0.013133120723068714 Accuracy 0.86328125\n",
      "Iteration 15550 Training loss 0.009751803241670132 Validation loss 0.012926936149597168 Accuracy 0.86572265625\n",
      "Iteration 15560 Training loss 0.00824015587568283 Validation loss 0.012577560730278492 Accuracy 0.869140625\n",
      "Iteration 15570 Training loss 0.00950371753424406 Validation loss 0.014023425988852978 Accuracy 0.8544921875\n",
      "Iteration 15580 Training loss 0.008735926821827888 Validation loss 0.012834088876843452 Accuracy 0.8662109375\n",
      "Iteration 15590 Training loss 0.009622820653021336 Validation loss 0.012656408362090588 Accuracy 0.8681640625\n",
      "Iteration 15600 Training loss 0.012811682187020779 Validation loss 0.015666987746953964 Accuracy 0.83642578125\n",
      "Iteration 15610 Training loss 0.009475312195718288 Validation loss 0.013207084499299526 Accuracy 0.86181640625\n",
      "Iteration 15620 Training loss 0.008423499763011932 Validation loss 0.013004770502448082 Accuracy 0.86474609375\n",
      "Iteration 15630 Training loss 0.00759802246466279 Validation loss 0.01283557340502739 Accuracy 0.86572265625\n",
      "Iteration 15640 Training loss 0.008718674071133137 Validation loss 0.013191452249884605 Accuracy 0.86279296875\n",
      "Iteration 15650 Training loss 0.00991471204906702 Validation loss 0.013715161010622978 Accuracy 0.8564453125\n",
      "Iteration 15660 Training loss 0.008391711860895157 Validation loss 0.013452745974063873 Accuracy 0.85986328125\n",
      "Iteration 15670 Training loss 0.008755099959671497 Validation loss 0.012810208834707737 Accuracy 0.8662109375\n",
      "Iteration 15680 Training loss 0.010849326848983765 Validation loss 0.01273118332028389 Accuracy 0.8671875\n",
      "Iteration 15690 Training loss 0.010114212520420551 Validation loss 0.01285307202488184 Accuracy 0.865234375\n",
      "Iteration 15700 Training loss 0.009661003015935421 Validation loss 0.013028646819293499 Accuracy 0.8623046875\n",
      "Iteration 15710 Training loss 0.008906549774110317 Validation loss 0.012913540005683899 Accuracy 0.8642578125\n",
      "Iteration 15720 Training loss 0.012105347588658333 Validation loss 0.01529866922646761 Accuracy 0.83984375\n",
      "Iteration 15730 Training loss 0.00880027562379837 Validation loss 0.012308643199503422 Accuracy 0.87158203125\n",
      "Iteration 15740 Training loss 0.008352442644536495 Validation loss 0.012976394034922123 Accuracy 0.86376953125\n",
      "Iteration 15750 Training loss 0.011778129264712334 Validation loss 0.013670983724296093 Accuracy 0.85693359375\n",
      "Iteration 15760 Training loss 0.00729397451505065 Validation loss 0.012551247142255306 Accuracy 0.86865234375\n",
      "Iteration 15770 Training loss 0.006911237724125385 Validation loss 0.01280741672962904 Accuracy 0.8662109375\n",
      "Iteration 15780 Training loss 0.008304670453071594 Validation loss 0.012840420007705688 Accuracy 0.86572265625\n",
      "Iteration 15790 Training loss 0.006739966105669737 Validation loss 0.013017436489462852 Accuracy 0.8642578125\n",
      "Iteration 15800 Training loss 0.008376228623092175 Validation loss 0.012818015180528164 Accuracy 0.86669921875\n",
      "Iteration 15810 Training loss 0.007938949391245842 Validation loss 0.012120477855205536 Accuracy 0.87353515625\n",
      "Iteration 15820 Training loss 0.008257615379989147 Validation loss 0.01285752933472395 Accuracy 0.865234375\n",
      "Iteration 15830 Training loss 0.008238697424530983 Validation loss 0.01223689690232277 Accuracy 0.8720703125\n",
      "Iteration 15840 Training loss 0.0075705512426793575 Validation loss 0.01340350229293108 Accuracy 0.859375\n",
      "Iteration 15850 Training loss 0.01029575802385807 Validation loss 0.013268161565065384 Accuracy 0.861328125\n",
      "Iteration 15860 Training loss 0.00966846477240324 Validation loss 0.012162513099610806 Accuracy 0.87353515625\n",
      "Iteration 15870 Training loss 0.009936577640473843 Validation loss 0.012546162120997906 Accuracy 0.8681640625\n",
      "Iteration 15880 Training loss 0.007531074341386557 Validation loss 0.012554384768009186 Accuracy 0.8681640625\n",
      "Iteration 15890 Training loss 0.011631464585661888 Validation loss 0.013005864806473255 Accuracy 0.86474609375\n",
      "Iteration 15900 Training loss 0.010156561620533466 Validation loss 0.012288479134440422 Accuracy 0.87109375\n",
      "Iteration 15910 Training loss 0.008197205141186714 Validation loss 0.012582577764987946 Accuracy 0.8681640625\n",
      "Iteration 15920 Training loss 0.007098095957189798 Validation loss 0.012577297165989876 Accuracy 0.869140625\n",
      "Iteration 15930 Training loss 0.008987155742943287 Validation loss 0.012039633467793465 Accuracy 0.875\n",
      "Iteration 15940 Training loss 0.009244582615792751 Validation loss 0.012287970632314682 Accuracy 0.8720703125\n",
      "Iteration 15950 Training loss 0.008584403432905674 Validation loss 0.012977343052625656 Accuracy 0.86474609375\n",
      "Iteration 15960 Training loss 0.010066743940114975 Validation loss 0.012825684621930122 Accuracy 0.865234375\n",
      "Iteration 15970 Training loss 0.010216385126113892 Validation loss 0.01358543150126934 Accuracy 0.85986328125\n",
      "Iteration 15980 Training loss 0.008565082214772701 Validation loss 0.012793922796845436 Accuracy 0.86669921875\n",
      "Iteration 15990 Training loss 0.010324256494641304 Validation loss 0.012443935498595238 Accuracy 0.86962890625\n",
      "Iteration 16000 Training loss 0.008029273711144924 Validation loss 0.012547746300697327 Accuracy 0.86962890625\n",
      "Iteration 16010 Training loss 0.009192746132612228 Validation loss 0.012244799174368382 Accuracy 0.87109375\n",
      "Iteration 16020 Training loss 0.0073545933701097965 Validation loss 0.01274361927062273 Accuracy 0.86865234375\n",
      "Iteration 16030 Training loss 0.010984476655721664 Validation loss 0.013141484931111336 Accuracy 0.8623046875\n",
      "Iteration 16040 Training loss 0.006456204690039158 Validation loss 0.012257883325219154 Accuracy 0.8720703125\n",
      "Iteration 16050 Training loss 0.0092574842274189 Validation loss 0.012928777374327183 Accuracy 0.865234375\n",
      "Iteration 16060 Training loss 0.009759387001395226 Validation loss 0.013192879036068916 Accuracy 0.861328125\n",
      "Iteration 16070 Training loss 0.008007718250155449 Validation loss 0.012605089694261551 Accuracy 0.86669921875\n",
      "Iteration 16080 Training loss 0.0076903486624360085 Validation loss 0.012093329802155495 Accuracy 0.87255859375\n",
      "Iteration 16090 Training loss 0.008426693268120289 Validation loss 0.012036618776619434 Accuracy 0.8740234375\n",
      "Iteration 16100 Training loss 0.007469514850527048 Validation loss 0.012490151450037956 Accuracy 0.869140625\n",
      "Iteration 16110 Training loss 0.009312246926128864 Validation loss 0.013179682195186615 Accuracy 0.861328125\n",
      "Iteration 16120 Training loss 0.007299148943275213 Validation loss 0.013479425571858883 Accuracy 0.8583984375\n",
      "Iteration 16130 Training loss 0.011877697892487049 Validation loss 0.013123217038810253 Accuracy 0.86328125\n",
      "Iteration 16140 Training loss 0.008694848045706749 Validation loss 0.012479116208851337 Accuracy 0.8701171875\n",
      "Iteration 16150 Training loss 0.007498151157051325 Validation loss 0.012544454075396061 Accuracy 0.86865234375\n",
      "Iteration 16160 Training loss 0.0066994475200772285 Validation loss 0.012790456414222717 Accuracy 0.8662109375\n",
      "Iteration 16170 Training loss 0.005987110082060099 Validation loss 0.01246443297713995 Accuracy 0.86962890625\n",
      "Iteration 16180 Training loss 0.010190349072217941 Validation loss 0.01257797610014677 Accuracy 0.869140625\n",
      "Iteration 16190 Training loss 0.009119792841374874 Validation loss 0.012318495661020279 Accuracy 0.87109375\n",
      "Iteration 16200 Training loss 0.007119846064597368 Validation loss 0.01219567097723484 Accuracy 0.8720703125\n",
      "Iteration 16210 Training loss 0.010338831692934036 Validation loss 0.01243955735117197 Accuracy 0.8701171875\n",
      "Iteration 16220 Training loss 0.010989496484398842 Validation loss 0.01328026968985796 Accuracy 0.86083984375\n",
      "Iteration 16230 Training loss 0.00917789340019226 Validation loss 0.013183292001485825 Accuracy 0.861328125\n",
      "Iteration 16240 Training loss 0.008644004352390766 Validation loss 0.012627423740923405 Accuracy 0.86767578125\n",
      "Iteration 16250 Training loss 0.008577785454690456 Validation loss 0.012972009368240833 Accuracy 0.8642578125\n",
      "Iteration 16260 Training loss 0.009298074059188366 Validation loss 0.012552578002214432 Accuracy 0.8681640625\n",
      "Iteration 16270 Training loss 0.008389213122427464 Validation loss 0.012147168628871441 Accuracy 0.8720703125\n",
      "Iteration 16280 Training loss 0.010803950019180775 Validation loss 0.012634553015232086 Accuracy 0.8681640625\n",
      "Iteration 16290 Training loss 0.011316019110381603 Validation loss 0.012863132171332836 Accuracy 0.8662109375\n",
      "Iteration 16300 Training loss 0.007554051000624895 Validation loss 0.012290474958717823 Accuracy 0.87158203125\n",
      "Iteration 16310 Training loss 0.010149591602385044 Validation loss 0.012742002494633198 Accuracy 0.8662109375\n",
      "Iteration 16320 Training loss 0.010271706618368626 Validation loss 0.013129918836057186 Accuracy 0.86181640625\n",
      "Iteration 16330 Training loss 0.007210333365947008 Validation loss 0.01258821040391922 Accuracy 0.86865234375\n",
      "Iteration 16340 Training loss 0.007438191212713718 Validation loss 0.012342612259089947 Accuracy 0.87109375\n",
      "Iteration 16350 Training loss 0.010466252453625202 Validation loss 0.013237688690423965 Accuracy 0.86083984375\n",
      "Iteration 16360 Training loss 0.007485757581889629 Validation loss 0.012406840920448303 Accuracy 0.86962890625\n",
      "Iteration 16370 Training loss 0.00943309348076582 Validation loss 0.013300793245434761 Accuracy 0.861328125\n",
      "Iteration 16380 Training loss 0.009948907420039177 Validation loss 0.012855501845479012 Accuracy 0.86572265625\n",
      "Iteration 16390 Training loss 0.009419601410627365 Validation loss 0.012661966495215893 Accuracy 0.86767578125\n",
      "Iteration 16400 Training loss 0.008154837414622307 Validation loss 0.012276223860681057 Accuracy 0.87158203125\n",
      "Iteration 16410 Training loss 0.007297352887690067 Validation loss 0.012199175544083118 Accuracy 0.87255859375\n",
      "Iteration 16420 Training loss 0.00941611547023058 Validation loss 0.01228010468184948 Accuracy 0.87060546875\n",
      "Iteration 16430 Training loss 0.008527428843080997 Validation loss 0.012334347702562809 Accuracy 0.8701171875\n",
      "Iteration 16440 Training loss 0.010806003585457802 Validation loss 0.012852320447564125 Accuracy 0.86572265625\n",
      "Iteration 16450 Training loss 0.007931315340101719 Validation loss 0.012019484303891659 Accuracy 0.8740234375\n",
      "Iteration 16460 Training loss 0.009325521998107433 Validation loss 0.012388289906084538 Accuracy 0.87060546875\n",
      "Iteration 16470 Training loss 0.009360849857330322 Validation loss 0.012094123288989067 Accuracy 0.87353515625\n",
      "Iteration 16480 Training loss 0.00856911763548851 Validation loss 0.01256994716823101 Accuracy 0.8671875\n",
      "Iteration 16490 Training loss 0.008562438189983368 Validation loss 0.012606486678123474 Accuracy 0.86767578125\n",
      "Iteration 16500 Training loss 0.009840044192969799 Validation loss 0.0124092698097229 Accuracy 0.87060546875\n",
      "Iteration 16510 Training loss 0.011333869770169258 Validation loss 0.0125571945682168 Accuracy 0.869140625\n",
      "Iteration 16520 Training loss 0.008615516126155853 Validation loss 0.012946883216500282 Accuracy 0.86328125\n",
      "Iteration 16530 Training loss 0.00822633970528841 Validation loss 0.01325264573097229 Accuracy 0.86083984375\n",
      "Iteration 16540 Training loss 0.007586903870105743 Validation loss 0.012363173067569733 Accuracy 0.8701171875\n",
      "Iteration 16550 Training loss 0.008038287051022053 Validation loss 0.01214252132922411 Accuracy 0.87353515625\n",
      "Iteration 16560 Training loss 0.012347614392638206 Validation loss 0.014448351226747036 Accuracy 0.84912109375\n",
      "Iteration 16570 Training loss 0.010543017648160458 Validation loss 0.012897985056042671 Accuracy 0.8642578125\n",
      "Iteration 16580 Training loss 0.00900206994265318 Validation loss 0.012048550881445408 Accuracy 0.87353515625\n",
      "Iteration 16590 Training loss 0.010732121765613556 Validation loss 0.012203981168568134 Accuracy 0.8720703125\n",
      "Iteration 16600 Training loss 0.009327670559287071 Validation loss 0.01231563463807106 Accuracy 0.87158203125\n",
      "Iteration 16610 Training loss 0.008072937838733196 Validation loss 0.012235412374138832 Accuracy 0.87158203125\n",
      "Iteration 16620 Training loss 0.009663554839789867 Validation loss 0.01268539298325777 Accuracy 0.86669921875\n",
      "Iteration 16630 Training loss 0.008121966384351254 Validation loss 0.01249082013964653 Accuracy 0.86962890625\n",
      "Iteration 16640 Training loss 0.008841491304337978 Validation loss 0.012287761084735394 Accuracy 0.8720703125\n",
      "Iteration 16650 Training loss 0.008307745680212975 Validation loss 0.012113512493669987 Accuracy 0.87255859375\n",
      "Iteration 16660 Training loss 0.010361053049564362 Validation loss 0.012875927612185478 Accuracy 0.86572265625\n",
      "Iteration 16670 Training loss 0.00785602442920208 Validation loss 0.011963929980993271 Accuracy 0.87451171875\n",
      "Iteration 16680 Training loss 0.008656112477183342 Validation loss 0.012411614879965782 Accuracy 0.87109375\n",
      "Iteration 16690 Training loss 0.007889355532824993 Validation loss 0.01236882247030735 Accuracy 0.86962890625\n",
      "Iteration 16700 Training loss 0.0076001472771167755 Validation loss 0.01233151089400053 Accuracy 0.87060546875\n",
      "Iteration 16710 Training loss 0.006917861755937338 Validation loss 0.01249720435589552 Accuracy 0.869140625\n",
      "Iteration 16720 Training loss 0.007949942722916603 Validation loss 0.012258455157279968 Accuracy 0.87158203125\n",
      "Iteration 16730 Training loss 0.010192982852458954 Validation loss 0.012461886741220951 Accuracy 0.86865234375\n",
      "Iteration 16740 Training loss 0.007943804375827312 Validation loss 0.012453261762857437 Accuracy 0.87060546875\n",
      "Iteration 16750 Training loss 0.009096014313399792 Validation loss 0.01206530723720789 Accuracy 0.87353515625\n",
      "Iteration 16760 Training loss 0.009190039709210396 Validation loss 0.012366468086838722 Accuracy 0.87109375\n",
      "Iteration 16770 Training loss 0.007953677326440811 Validation loss 0.012076755054295063 Accuracy 0.87353515625\n",
      "Iteration 16780 Training loss 0.008548232726752758 Validation loss 0.013360419310629368 Accuracy 0.85986328125\n",
      "Iteration 16790 Training loss 0.006554177962243557 Validation loss 0.012160909362137318 Accuracy 0.8720703125\n",
      "Iteration 16800 Training loss 0.00893909577280283 Validation loss 0.012338008731603622 Accuracy 0.87060546875\n",
      "Iteration 16810 Training loss 0.00938087236136198 Validation loss 0.012193773873150349 Accuracy 0.873046875\n",
      "Iteration 16820 Training loss 0.009047501720488071 Validation loss 0.012031168676912785 Accuracy 0.87451171875\n",
      "Iteration 16830 Training loss 0.008762169629335403 Validation loss 0.012856453657150269 Accuracy 0.86572265625\n",
      "Iteration 16840 Training loss 0.007564541418105364 Validation loss 0.012040780857205391 Accuracy 0.8740234375\n",
      "Iteration 16850 Training loss 0.007029613945633173 Validation loss 0.01204263512045145 Accuracy 0.87353515625\n",
      "Iteration 16860 Training loss 0.007998130284249783 Validation loss 0.011911685578525066 Accuracy 0.87548828125\n",
      "Iteration 16870 Training loss 0.007255412172526121 Validation loss 0.012068038806319237 Accuracy 0.87353515625\n",
      "Iteration 16880 Training loss 0.007389638107270002 Validation loss 0.012060346081852913 Accuracy 0.87353515625\n",
      "Iteration 16890 Training loss 0.00795253086835146 Validation loss 0.012241852469742298 Accuracy 0.8720703125\n",
      "Iteration 16900 Training loss 0.007608878891915083 Validation loss 0.01309204287827015 Accuracy 0.8623046875\n",
      "Iteration 16910 Training loss 0.010184643790125847 Validation loss 0.012109627015888691 Accuracy 0.873046875\n",
      "Iteration 16920 Training loss 0.006308667361736298 Validation loss 0.012795965187251568 Accuracy 0.86669921875\n",
      "Iteration 16930 Training loss 0.008098463527858257 Validation loss 0.01228122878819704 Accuracy 0.87060546875\n",
      "Iteration 16940 Training loss 0.009699725545942783 Validation loss 0.012235491536557674 Accuracy 0.87255859375\n",
      "Iteration 16950 Training loss 0.010709172114729881 Validation loss 0.013162338174879551 Accuracy 0.8623046875\n",
      "Iteration 16960 Training loss 0.007585454266518354 Validation loss 0.011969550512731075 Accuracy 0.875\n",
      "Iteration 16970 Training loss 0.009061826393008232 Validation loss 0.012944230809807777 Accuracy 0.86328125\n",
      "Iteration 16980 Training loss 0.007026189006865025 Validation loss 0.012291367165744305 Accuracy 0.87109375\n",
      "Iteration 16990 Training loss 0.00997874978929758 Validation loss 0.012385230511426926 Accuracy 0.87060546875\n",
      "Iteration 17000 Training loss 0.010898260399699211 Validation loss 0.01292564906179905 Accuracy 0.8642578125\n",
      "Iteration 17010 Training loss 0.008877642452716827 Validation loss 0.013046948239207268 Accuracy 0.86279296875\n",
      "Iteration 17020 Training loss 0.007747103460133076 Validation loss 0.0122302807867527 Accuracy 0.8720703125\n",
      "Iteration 17030 Training loss 0.006897935178130865 Validation loss 0.012427294626832008 Accuracy 0.86962890625\n",
      "Iteration 17040 Training loss 0.007821155712008476 Validation loss 0.012399539351463318 Accuracy 0.86962890625\n",
      "Iteration 17050 Training loss 0.009056692942976952 Validation loss 0.012548825703561306 Accuracy 0.8681640625\n",
      "Iteration 17060 Training loss 0.008017509244382381 Validation loss 0.0124978581443429 Accuracy 0.869140625\n",
      "Iteration 17070 Training loss 0.007689122576266527 Validation loss 0.012217008508741856 Accuracy 0.87158203125\n",
      "Iteration 17080 Training loss 0.007785280235111713 Validation loss 0.012165150605142117 Accuracy 0.87255859375\n",
      "Iteration 17090 Training loss 0.011128829792141914 Validation loss 0.012158242054283619 Accuracy 0.87353515625\n",
      "Iteration 17100 Training loss 0.007611299864947796 Validation loss 0.012189987115561962 Accuracy 0.87255859375\n",
      "Iteration 17110 Training loss 0.009271802380681038 Validation loss 0.012231830507516861 Accuracy 0.87255859375\n",
      "Iteration 17120 Training loss 0.00744756031781435 Validation loss 0.012091065756976604 Accuracy 0.873046875\n",
      "Iteration 17130 Training loss 0.007683014031499624 Validation loss 0.012656640261411667 Accuracy 0.8671875\n",
      "Iteration 17140 Training loss 0.009637040086090565 Validation loss 0.012447553686797619 Accuracy 0.87109375\n",
      "Iteration 17150 Training loss 0.007494731340557337 Validation loss 0.012340519577264786 Accuracy 0.87158203125\n",
      "Iteration 17160 Training loss 0.007916323840618134 Validation loss 0.012227265164256096 Accuracy 0.87158203125\n",
      "Iteration 17170 Training loss 0.009616601280868053 Validation loss 0.012142272666096687 Accuracy 0.873046875\n",
      "Iteration 17180 Training loss 0.006846939213573933 Validation loss 0.012102242559194565 Accuracy 0.87353515625\n",
      "Iteration 17190 Training loss 0.009517182596027851 Validation loss 0.012894738465547562 Accuracy 0.865234375\n",
      "Iteration 17200 Training loss 0.009259947575628757 Validation loss 0.012958308681845665 Accuracy 0.865234375\n",
      "Iteration 17210 Training loss 0.009029882028698921 Validation loss 0.012522551231086254 Accuracy 0.86962890625\n",
      "Iteration 17220 Training loss 0.00862072128802538 Validation loss 0.012855482287704945 Accuracy 0.86669921875\n",
      "Iteration 17230 Training loss 0.01047752145677805 Validation loss 0.01217186264693737 Accuracy 0.8720703125\n",
      "Iteration 17240 Training loss 0.007202975917607546 Validation loss 0.01199689693748951 Accuracy 0.87451171875\n",
      "Iteration 17250 Training loss 0.008606605231761932 Validation loss 0.012060514651238918 Accuracy 0.8740234375\n",
      "Iteration 17260 Training loss 0.010324847884476185 Validation loss 0.013046498410403728 Accuracy 0.86376953125\n",
      "Iteration 17270 Training loss 0.011141130700707436 Validation loss 0.012029238976538181 Accuracy 0.8740234375\n",
      "Iteration 17280 Training loss 0.00932281743735075 Validation loss 0.012042751535773277 Accuracy 0.8740234375\n",
      "Iteration 17290 Training loss 0.008401528932154179 Validation loss 0.011979672126471996 Accuracy 0.87451171875\n",
      "Iteration 17300 Training loss 0.007632842753082514 Validation loss 0.012566948309540749 Accuracy 0.86865234375\n",
      "Iteration 17310 Training loss 0.00956827774643898 Validation loss 0.013531851582229137 Accuracy 0.85888671875\n",
      "Iteration 17320 Training loss 0.009121722541749477 Validation loss 0.012262721545994282 Accuracy 0.8720703125\n",
      "Iteration 17330 Training loss 0.008622588589787483 Validation loss 0.012070422992110252 Accuracy 0.873046875\n",
      "Iteration 17340 Training loss 0.007617984898388386 Validation loss 0.012114237993955612 Accuracy 0.8740234375\n",
      "Iteration 17350 Training loss 0.007224349305033684 Validation loss 0.011822253465652466 Accuracy 0.87548828125\n",
      "Iteration 17360 Training loss 0.00731930136680603 Validation loss 0.012745697051286697 Accuracy 0.86669921875\n",
      "Iteration 17370 Training loss 0.00950920395553112 Validation loss 0.014003981836140156 Accuracy 0.853515625\n",
      "Iteration 17380 Training loss 0.010750571265816689 Validation loss 0.012805228121578693 Accuracy 0.8662109375\n",
      "Iteration 17390 Training loss 0.008360170759260654 Validation loss 0.012629449367523193 Accuracy 0.86865234375\n",
      "Iteration 17400 Training loss 0.007206539157778025 Validation loss 0.012512258253991604 Accuracy 0.8681640625\n",
      "Iteration 17410 Training loss 0.009232074953615665 Validation loss 0.013112055137753487 Accuracy 0.8623046875\n",
      "Iteration 17420 Training loss 0.01075277291238308 Validation loss 0.01254324335604906 Accuracy 0.8681640625\n",
      "Iteration 17430 Training loss 0.00823536328971386 Validation loss 0.012066977098584175 Accuracy 0.8740234375\n",
      "Iteration 17440 Training loss 0.009529651142656803 Validation loss 0.01282102707773447 Accuracy 0.8662109375\n",
      "Iteration 17450 Training loss 0.008008904755115509 Validation loss 0.012566127814352512 Accuracy 0.86767578125\n",
      "Iteration 17460 Training loss 0.007027417421340942 Validation loss 0.012476591393351555 Accuracy 0.8701171875\n",
      "Iteration 17470 Training loss 0.007967226207256317 Validation loss 0.012097069062292576 Accuracy 0.87353515625\n",
      "Iteration 17480 Training loss 0.006999021861702204 Validation loss 0.012679965235292912 Accuracy 0.8671875\n",
      "Iteration 17490 Training loss 0.0078254584223032 Validation loss 0.012403750792145729 Accuracy 0.86962890625\n",
      "Iteration 17500 Training loss 0.00763087859377265 Validation loss 0.012381386011838913 Accuracy 0.8701171875\n",
      "Iteration 17510 Training loss 0.00835593044757843 Validation loss 0.01253406424075365 Accuracy 0.86865234375\n",
      "Iteration 17520 Training loss 0.007481062319129705 Validation loss 0.012172311544418335 Accuracy 0.87255859375\n",
      "Iteration 17530 Training loss 0.00866605993360281 Validation loss 0.012320253998041153 Accuracy 0.87060546875\n",
      "Iteration 17540 Training loss 0.008853031322360039 Validation loss 0.012385976500809193 Accuracy 0.87060546875\n",
      "Iteration 17550 Training loss 0.008403206244111061 Validation loss 0.01196147408336401 Accuracy 0.875\n",
      "Iteration 17560 Training loss 0.009548647329211235 Validation loss 0.012113328091800213 Accuracy 0.8740234375\n",
      "Iteration 17570 Training loss 0.008404407650232315 Validation loss 0.011891700327396393 Accuracy 0.875\n",
      "Iteration 17580 Training loss 0.009005511179566383 Validation loss 0.012313609011471272 Accuracy 0.87158203125\n",
      "Iteration 17590 Training loss 0.010484926402568817 Validation loss 0.012437905184924603 Accuracy 0.8701171875\n",
      "Iteration 17600 Training loss 0.007984085008502007 Validation loss 0.013076890259981155 Accuracy 0.86328125\n",
      "Iteration 17610 Training loss 0.01003616489470005 Validation loss 0.012426985427737236 Accuracy 0.87109375\n",
      "Iteration 17620 Training loss 0.00881833303719759 Validation loss 0.01194668747484684 Accuracy 0.875\n",
      "Iteration 17630 Training loss 0.008400388062000275 Validation loss 0.01217428594827652 Accuracy 0.87158203125\n",
      "Iteration 17640 Training loss 0.008267819881439209 Validation loss 0.012082831934094429 Accuracy 0.87353515625\n",
      "Iteration 17650 Training loss 0.00790893193334341 Validation loss 0.012154902331531048 Accuracy 0.873046875\n",
      "Iteration 17660 Training loss 0.007388266269117594 Validation loss 0.01220572367310524 Accuracy 0.87255859375\n",
      "Iteration 17670 Training loss 0.00929209589958191 Validation loss 0.012491585686802864 Accuracy 0.8701171875\n",
      "Iteration 17680 Training loss 0.007551889400929213 Validation loss 0.012401185929775238 Accuracy 0.87109375\n",
      "Iteration 17690 Training loss 0.008293851278722286 Validation loss 0.012086117640137672 Accuracy 0.87353515625\n",
      "Iteration 17700 Training loss 0.008255751803517342 Validation loss 0.012050453573465347 Accuracy 0.8740234375\n",
      "Iteration 17710 Training loss 0.0073249503038823605 Validation loss 0.011799799278378487 Accuracy 0.876953125\n",
      "Iteration 17720 Training loss 0.00736484257504344 Validation loss 0.012358500622212887 Accuracy 0.87060546875\n",
      "Iteration 17730 Training loss 0.010131895542144775 Validation loss 0.013097197748720646 Accuracy 0.86279296875\n",
      "Iteration 17740 Training loss 0.0075784414075315 Validation loss 0.012318598106503487 Accuracy 0.87109375\n",
      "Iteration 17750 Training loss 0.005978664383292198 Validation loss 0.011770601384341717 Accuracy 0.87646484375\n",
      "Iteration 17760 Training loss 0.007737824693322182 Validation loss 0.011909511871635914 Accuracy 0.87548828125\n",
      "Iteration 17770 Training loss 0.007146931253373623 Validation loss 0.011965488083660603 Accuracy 0.8740234375\n",
      "Iteration 17780 Training loss 0.006724728271365166 Validation loss 0.012135080061852932 Accuracy 0.87158203125\n",
      "Iteration 17790 Training loss 0.010652828961610794 Validation loss 0.012383642606437206 Accuracy 0.86962890625\n",
      "Iteration 17800 Training loss 0.007352277170866728 Validation loss 0.012026410549879074 Accuracy 0.873046875\n",
      "Iteration 17810 Training loss 0.007759288884699345 Validation loss 0.012415889650583267 Accuracy 0.869140625\n",
      "Iteration 17820 Training loss 0.0056677754037082195 Validation loss 0.01220118347555399 Accuracy 0.87255859375\n",
      "Iteration 17830 Training loss 0.009126885794103146 Validation loss 0.01237899623811245 Accuracy 0.8701171875\n",
      "Iteration 17840 Training loss 0.007870843634009361 Validation loss 0.012250326573848724 Accuracy 0.87158203125\n",
      "Iteration 17850 Training loss 0.008034514263272285 Validation loss 0.012483011931180954 Accuracy 0.869140625\n",
      "Iteration 17860 Training loss 0.007146541494876146 Validation loss 0.012022458016872406 Accuracy 0.87451171875\n",
      "Iteration 17870 Training loss 0.009393214248120785 Validation loss 0.012034470215439796 Accuracy 0.8740234375\n",
      "Iteration 17880 Training loss 0.007535846438258886 Validation loss 0.012049969285726547 Accuracy 0.8740234375\n",
      "Iteration 17890 Training loss 0.007788159418851137 Validation loss 0.012374097481369972 Accuracy 0.87109375\n",
      "Iteration 17900 Training loss 0.006119538564234972 Validation loss 0.012482991442084312 Accuracy 0.86962890625\n",
      "Iteration 17910 Training loss 0.008001819252967834 Validation loss 0.012652176432311535 Accuracy 0.8681640625\n",
      "Iteration 17920 Training loss 0.009259996004402637 Validation loss 0.011853749863803387 Accuracy 0.87646484375\n",
      "Iteration 17930 Training loss 0.010046213865280151 Validation loss 0.011818856000900269 Accuracy 0.8759765625\n",
      "Iteration 17940 Training loss 0.006777065806090832 Validation loss 0.012953581288456917 Accuracy 0.865234375\n",
      "Iteration 17950 Training loss 0.006108947563916445 Validation loss 0.012441486120223999 Accuracy 0.8701171875\n",
      "Iteration 17960 Training loss 0.008185870945453644 Validation loss 0.013031239621341228 Accuracy 0.86328125\n",
      "Iteration 17970 Training loss 0.008081172592937946 Validation loss 0.0121424850076437 Accuracy 0.8720703125\n",
      "Iteration 17980 Training loss 0.008401292376220226 Validation loss 0.012194096110761166 Accuracy 0.8720703125\n",
      "Iteration 17990 Training loss 0.008295070379972458 Validation loss 0.011837858706712723 Accuracy 0.87548828125\n",
      "Iteration 18000 Training loss 0.005906211212277412 Validation loss 0.012022646144032478 Accuracy 0.8740234375\n",
      "Iteration 18010 Training loss 0.007015631999820471 Validation loss 0.011866770684719086 Accuracy 0.8759765625\n",
      "Iteration 18020 Training loss 0.009427153505384922 Validation loss 0.011805597692728043 Accuracy 0.87548828125\n",
      "Iteration 18030 Training loss 0.007418048568069935 Validation loss 0.011900250799953938 Accuracy 0.8759765625\n",
      "Iteration 18040 Training loss 0.006197401788085699 Validation loss 0.012838916853070259 Accuracy 0.86474609375\n",
      "Iteration 18050 Training loss 0.008540241979062557 Validation loss 0.01176405232399702 Accuracy 0.8759765625\n",
      "Iteration 18060 Training loss 0.008283022791147232 Validation loss 0.011763494461774826 Accuracy 0.87646484375\n",
      "Iteration 18070 Training loss 0.007220031227916479 Validation loss 0.011692040599882603 Accuracy 0.8759765625\n",
      "Iteration 18080 Training loss 0.007091033738106489 Validation loss 0.012312542647123337 Accuracy 0.87158203125\n",
      "Iteration 18090 Training loss 0.007665902841836214 Validation loss 0.011848466470837593 Accuracy 0.875\n",
      "Iteration 18100 Training loss 0.0082173440605402 Validation loss 0.012023900635540485 Accuracy 0.87353515625\n",
      "Iteration 18110 Training loss 0.008027750998735428 Validation loss 0.012495091184973717 Accuracy 0.869140625\n",
      "Iteration 18120 Training loss 0.008181721903383732 Validation loss 0.012426537461578846 Accuracy 0.86962890625\n",
      "Iteration 18130 Training loss 0.008990374393761158 Validation loss 0.012272033840417862 Accuracy 0.87109375\n",
      "Iteration 18140 Training loss 0.007747245486825705 Validation loss 0.011739542707800865 Accuracy 0.87646484375\n",
      "Iteration 18150 Training loss 0.0079546719789505 Validation loss 0.0117444833740592 Accuracy 0.87646484375\n",
      "Iteration 18160 Training loss 0.006859526038169861 Validation loss 0.012401437386870384 Accuracy 0.8701171875\n",
      "Iteration 18170 Training loss 0.0070600975304841995 Validation loss 0.011886547319591045 Accuracy 0.87548828125\n",
      "Iteration 18180 Training loss 0.007074906025081873 Validation loss 0.012003026902675629 Accuracy 0.87353515625\n",
      "Iteration 18190 Training loss 0.007680376525968313 Validation loss 0.011860900558531284 Accuracy 0.87548828125\n",
      "Iteration 18200 Training loss 0.007672120351344347 Validation loss 0.011768299154937267 Accuracy 0.876953125\n",
      "Iteration 18210 Training loss 0.0066177258267998695 Validation loss 0.01173307839781046 Accuracy 0.87646484375\n",
      "Iteration 18220 Training loss 0.007035407237708569 Validation loss 0.012111213058233261 Accuracy 0.8720703125\n",
      "Iteration 18230 Training loss 0.008169718086719513 Validation loss 0.011752692982554436 Accuracy 0.8759765625\n",
      "Iteration 18240 Training loss 0.0078125 Validation loss 0.011797379702329636 Accuracy 0.8759765625\n",
      "Iteration 18250 Training loss 0.0068857562728226185 Validation loss 0.01194449421018362 Accuracy 0.8740234375\n",
      "Iteration 18260 Training loss 0.006885294336825609 Validation loss 0.012602180242538452 Accuracy 0.8671875\n",
      "Iteration 18270 Training loss 0.00910615362226963 Validation loss 0.013319894671440125 Accuracy 0.86083984375\n",
      "Iteration 18280 Training loss 0.008034538477659225 Validation loss 0.012135075405240059 Accuracy 0.873046875\n",
      "Iteration 18290 Training loss 0.006801027338951826 Validation loss 0.011752571910619736 Accuracy 0.87646484375\n",
      "Iteration 18300 Training loss 0.009568736888468266 Validation loss 0.011796632781624794 Accuracy 0.876953125\n",
      "Iteration 18310 Training loss 0.005910410080105066 Validation loss 0.011609064415097237 Accuracy 0.87841796875\n",
      "Iteration 18320 Training loss 0.006909174378961325 Validation loss 0.012862292118370533 Accuracy 0.8662109375\n",
      "Iteration 18330 Training loss 0.0066212196834385395 Validation loss 0.011631371453404427 Accuracy 0.87890625\n",
      "Iteration 18340 Training loss 0.008220324292778969 Validation loss 0.011935580521821976 Accuracy 0.87451171875\n",
      "Iteration 18350 Training loss 0.006179032381623983 Validation loss 0.011674482375383377 Accuracy 0.87744140625\n",
      "Iteration 18360 Training loss 0.008706141263246536 Validation loss 0.011878417804837227 Accuracy 0.875\n",
      "Iteration 18370 Training loss 0.009014464914798737 Validation loss 0.01196647435426712 Accuracy 0.8740234375\n",
      "Iteration 18380 Training loss 0.008069084025919437 Validation loss 0.01205923780798912 Accuracy 0.87255859375\n",
      "Iteration 18390 Training loss 0.008259657770395279 Validation loss 0.011774053797125816 Accuracy 0.87646484375\n",
      "Iteration 18400 Training loss 0.006334988866001368 Validation loss 0.0117386095225811 Accuracy 0.8759765625\n",
      "Iteration 18410 Training loss 0.010531652718782425 Validation loss 0.012075529433786869 Accuracy 0.87255859375\n",
      "Iteration 18420 Training loss 0.00827227532863617 Validation loss 0.011882644146680832 Accuracy 0.8759765625\n",
      "Iteration 18430 Training loss 0.007283117156475782 Validation loss 0.011871681548655033 Accuracy 0.875\n",
      "Iteration 18440 Training loss 0.009508652612566948 Validation loss 0.012298507615923882 Accuracy 0.8701171875\n",
      "Iteration 18450 Training loss 0.007674511522054672 Validation loss 0.01169058121740818 Accuracy 0.87744140625\n",
      "Iteration 18460 Training loss 0.008678282611072063 Validation loss 0.012228273786604404 Accuracy 0.87158203125\n",
      "Iteration 18470 Training loss 0.007969620637595654 Validation loss 0.0117736104875803 Accuracy 0.876953125\n",
      "Iteration 18480 Training loss 0.0069429995492100716 Validation loss 0.012099777348339558 Accuracy 0.8740234375\n",
      "Iteration 18490 Training loss 0.00835790578275919 Validation loss 0.011981461197137833 Accuracy 0.87451171875\n",
      "Iteration 18500 Training loss 0.008073294535279274 Validation loss 0.012096303515136242 Accuracy 0.873046875\n",
      "Iteration 18510 Training loss 0.007405128329992294 Validation loss 0.011914951726794243 Accuracy 0.875\n",
      "Iteration 18520 Training loss 0.0074620600789785385 Validation loss 0.011729689314961433 Accuracy 0.8779296875\n",
      "Iteration 18530 Training loss 0.008675595745444298 Validation loss 0.01252862997353077 Accuracy 0.86962890625\n",
      "Iteration 18540 Training loss 0.0075951386243104935 Validation loss 0.011644147336483002 Accuracy 0.876953125\n",
      "Iteration 18550 Training loss 0.008378718979656696 Validation loss 0.011945459060370922 Accuracy 0.87451171875\n",
      "Iteration 18560 Training loss 0.0064194099977612495 Validation loss 0.012004769407212734 Accuracy 0.87353515625\n",
      "Iteration 18570 Training loss 0.008063341490924358 Validation loss 0.011722290888428688 Accuracy 0.87646484375\n",
      "Iteration 18580 Training loss 0.006853972561657429 Validation loss 0.011879938654601574 Accuracy 0.87548828125\n",
      "Iteration 18590 Training loss 0.00751544488593936 Validation loss 0.012217424809932709 Accuracy 0.8720703125\n",
      "Iteration 18600 Training loss 0.0068860347382724285 Validation loss 0.012116232886910439 Accuracy 0.87353515625\n",
      "Iteration 18610 Training loss 0.00802088063210249 Validation loss 0.012084435671567917 Accuracy 0.873046875\n",
      "Iteration 18620 Training loss 0.009090578183531761 Validation loss 0.012310079298913479 Accuracy 0.87109375\n",
      "Iteration 18630 Training loss 0.007941159419715405 Validation loss 0.011930477805435658 Accuracy 0.8740234375\n",
      "Iteration 18640 Training loss 0.007355549372732639 Validation loss 0.011724605225026608 Accuracy 0.876953125\n",
      "Iteration 18650 Training loss 0.006484985817223787 Validation loss 0.011800155974924564 Accuracy 0.8759765625\n",
      "Iteration 18660 Training loss 0.009784079156816006 Validation loss 0.0123407281935215 Accuracy 0.87060546875\n",
      "Iteration 18670 Training loss 0.007342070806771517 Validation loss 0.011765590868890285 Accuracy 0.87646484375\n",
      "Iteration 18680 Training loss 0.00796285830438137 Validation loss 0.012524472549557686 Accuracy 0.86865234375\n",
      "Iteration 18690 Training loss 0.010457471013069153 Validation loss 0.012294970452785492 Accuracy 0.87158203125\n",
      "Iteration 18700 Training loss 0.008706298656761646 Validation loss 0.012815441936254501 Accuracy 0.8662109375\n",
      "Iteration 18710 Training loss 0.00903693400323391 Validation loss 0.012075487524271011 Accuracy 0.873046875\n",
      "Iteration 18720 Training loss 0.0068754879757761955 Validation loss 0.011754550039768219 Accuracy 0.8759765625\n",
      "Iteration 18730 Training loss 0.00871837418526411 Validation loss 0.012804626487195492 Accuracy 0.86572265625\n",
      "Iteration 18740 Training loss 0.007221119944006205 Validation loss 0.011492013931274414 Accuracy 0.87890625\n",
      "Iteration 18750 Training loss 0.007065075449645519 Validation loss 0.011574339121580124 Accuracy 0.8779296875\n",
      "Iteration 18760 Training loss 0.008453615941107273 Validation loss 0.011963846161961555 Accuracy 0.87451171875\n",
      "Iteration 18770 Training loss 0.0070107802748680115 Validation loss 0.012177504599094391 Accuracy 0.8720703125\n",
      "Iteration 18780 Training loss 0.006266261916607618 Validation loss 0.012043551541864872 Accuracy 0.87353515625\n",
      "Iteration 18790 Training loss 0.007479474414139986 Validation loss 0.011577568016946316 Accuracy 0.8779296875\n",
      "Iteration 18800 Training loss 0.007389547303318977 Validation loss 0.011568473652005196 Accuracy 0.87890625\n",
      "Iteration 18810 Training loss 0.007608184590935707 Validation loss 0.011806382797658443 Accuracy 0.87646484375\n",
      "Iteration 18820 Training loss 0.009100827388465405 Validation loss 0.011893603019416332 Accuracy 0.8740234375\n",
      "Iteration 18830 Training loss 0.007191701792180538 Validation loss 0.011668730527162552 Accuracy 0.87744140625\n",
      "Iteration 18840 Training loss 0.006246963515877724 Validation loss 0.011748671531677246 Accuracy 0.87744140625\n",
      "Iteration 18850 Training loss 0.0066574434749782085 Validation loss 0.012033598497509956 Accuracy 0.87353515625\n",
      "Iteration 18860 Training loss 0.005308076273649931 Validation loss 0.012037908658385277 Accuracy 0.87353515625\n",
      "Iteration 18870 Training loss 0.00841104332357645 Validation loss 0.01158563420176506 Accuracy 0.87841796875\n",
      "Iteration 18880 Training loss 0.006703255698084831 Validation loss 0.011906718835234642 Accuracy 0.87451171875\n",
      "Iteration 18890 Training loss 0.0066161383874714375 Validation loss 0.012018579989671707 Accuracy 0.87353515625\n",
      "Iteration 18900 Training loss 0.008351899683475494 Validation loss 0.011839966289699078 Accuracy 0.875\n",
      "Iteration 18910 Training loss 0.009633899666368961 Validation loss 0.012416188605129719 Accuracy 0.86865234375\n",
      "Iteration 18920 Training loss 0.004647617693990469 Validation loss 0.012132811360061169 Accuracy 0.873046875\n",
      "Iteration 18930 Training loss 0.007018357049673796 Validation loss 0.011780700646340847 Accuracy 0.8759765625\n",
      "Iteration 18940 Training loss 0.00656375614926219 Validation loss 0.011882215738296509 Accuracy 0.87548828125\n",
      "Iteration 18950 Training loss 0.008174218237400055 Validation loss 0.012903050519526005 Accuracy 0.86474609375\n",
      "Iteration 18960 Training loss 0.0077726515009999275 Validation loss 0.012487977743148804 Accuracy 0.8681640625\n",
      "Iteration 18970 Training loss 0.006332061719149351 Validation loss 0.011886530555784702 Accuracy 0.87451171875\n",
      "Iteration 18980 Training loss 0.0070452382788062096 Validation loss 0.011939474381506443 Accuracy 0.87451171875\n",
      "Iteration 18990 Training loss 0.005691764876246452 Validation loss 0.011927219107747078 Accuracy 0.87451171875\n",
      "Iteration 19000 Training loss 0.00784788466989994 Validation loss 0.012420278042554855 Accuracy 0.8701171875\n",
      "Iteration 19010 Training loss 0.007722118403762579 Validation loss 0.01212526299059391 Accuracy 0.87255859375\n",
      "Iteration 19020 Training loss 0.007969746366143227 Validation loss 0.011951153166592121 Accuracy 0.875\n",
      "Iteration 19030 Training loss 0.006764683406800032 Validation loss 0.011867714114487171 Accuracy 0.875\n",
      "Iteration 19040 Training loss 0.007536106742918491 Validation loss 0.01234267558902502 Accuracy 0.869140625\n",
      "Iteration 19050 Training loss 0.008762981742620468 Validation loss 0.01217910461127758 Accuracy 0.8720703125\n",
      "Iteration 19060 Training loss 0.009591375477612019 Validation loss 0.012453930452466011 Accuracy 0.86865234375\n",
      "Iteration 19070 Training loss 0.009099331684410572 Validation loss 0.012080961838364601 Accuracy 0.873046875\n",
      "Iteration 19080 Training loss 0.008079187013208866 Validation loss 0.012567245401442051 Accuracy 0.86865234375\n",
      "Iteration 19090 Training loss 0.007200323045253754 Validation loss 0.011798180639743805 Accuracy 0.87646484375\n",
      "Iteration 19100 Training loss 0.006351714953780174 Validation loss 0.011950796470046043 Accuracy 0.8740234375\n",
      "Iteration 19110 Training loss 0.006351579446345568 Validation loss 0.012014943175017834 Accuracy 0.8740234375\n",
      "Iteration 19120 Training loss 0.008815468288958073 Validation loss 0.012005140073597431 Accuracy 0.87353515625\n",
      "Iteration 19130 Training loss 0.00851703155785799 Validation loss 0.011773434467613697 Accuracy 0.876953125\n",
      "Iteration 19140 Training loss 0.008383977226912975 Validation loss 0.011916120536625385 Accuracy 0.875\n",
      "Iteration 19150 Training loss 0.007401794195175171 Validation loss 0.011883101426064968 Accuracy 0.87548828125\n",
      "Iteration 19160 Training loss 0.007772975135594606 Validation loss 0.011843200773000717 Accuracy 0.875\n",
      "Iteration 19170 Training loss 0.009675289504230022 Validation loss 0.01203538291156292 Accuracy 0.87353515625\n",
      "Iteration 19180 Training loss 0.008984844200313091 Validation loss 0.01222524419426918 Accuracy 0.8720703125\n",
      "Iteration 19190 Training loss 0.007823798805475235 Validation loss 0.012048301286995411 Accuracy 0.8720703125\n",
      "Iteration 19200 Training loss 0.007891477085649967 Validation loss 0.012080871500074863 Accuracy 0.8720703125\n",
      "Iteration 19210 Training loss 0.005092563573271036 Validation loss 0.011401988565921783 Accuracy 0.8798828125\n",
      "Iteration 19220 Training loss 0.007646540179848671 Validation loss 0.011744786985218525 Accuracy 0.87646484375\n",
      "Iteration 19230 Training loss 0.008512710221111774 Validation loss 0.011913646943867207 Accuracy 0.875\n",
      "Iteration 19240 Training loss 0.007695774082094431 Validation loss 0.011683649383485317 Accuracy 0.8779296875\n",
      "Iteration 19250 Training loss 0.006910438649356365 Validation loss 0.011788374744355679 Accuracy 0.87646484375\n",
      "Iteration 19260 Training loss 0.007820641621947289 Validation loss 0.012193683534860611 Accuracy 0.8720703125\n",
      "Iteration 19270 Training loss 0.007775815203785896 Validation loss 0.012103567831218243 Accuracy 0.87255859375\n",
      "Iteration 19280 Training loss 0.008685164153575897 Validation loss 0.012096766382455826 Accuracy 0.87353515625\n",
      "Iteration 19290 Training loss 0.006349537987262011 Validation loss 0.012505356222391129 Accuracy 0.8681640625\n",
      "Iteration 19300 Training loss 0.004512937273830175 Validation loss 0.012296756729483604 Accuracy 0.87109375\n",
      "Iteration 19310 Training loss 0.006832540035247803 Validation loss 0.01154386904090643 Accuracy 0.87841796875\n",
      "Iteration 19320 Training loss 0.006574404425919056 Validation loss 0.012075236067175865 Accuracy 0.8720703125\n",
      "Iteration 19330 Training loss 0.007648416329175234 Validation loss 0.011720250360667706 Accuracy 0.87646484375\n",
      "Iteration 19340 Training loss 0.006021862383931875 Validation loss 0.011638158932328224 Accuracy 0.8779296875\n",
      "Iteration 19350 Training loss 0.008048615418374538 Validation loss 0.011713996529579163 Accuracy 0.87646484375\n",
      "Iteration 19360 Training loss 0.00750813540071249 Validation loss 0.011676506139338017 Accuracy 0.87646484375\n",
      "Iteration 19370 Training loss 0.005637718830257654 Validation loss 0.011948478408157825 Accuracy 0.8740234375\n",
      "Iteration 19380 Training loss 0.009861159138381481 Validation loss 0.012294615618884563 Accuracy 0.87060546875\n",
      "Iteration 19390 Training loss 0.0068413023836910725 Validation loss 0.011735102161765099 Accuracy 0.87646484375\n",
      "Iteration 19400 Training loss 0.008043664507567883 Validation loss 0.011823190376162529 Accuracy 0.87548828125\n",
      "Iteration 19410 Training loss 0.009220954962074757 Validation loss 0.01276672724634409 Accuracy 0.8662109375\n",
      "Iteration 19420 Training loss 0.006435157265514135 Validation loss 0.012070500291883945 Accuracy 0.87255859375\n",
      "Iteration 19430 Training loss 0.007664923090487719 Validation loss 0.01184156071394682 Accuracy 0.875\n",
      "Iteration 19440 Training loss 0.00689526554197073 Validation loss 0.01192951388657093 Accuracy 0.87451171875\n",
      "Iteration 19450 Training loss 0.008298822678625584 Validation loss 0.011705761775374413 Accuracy 0.87646484375\n",
      "Iteration 19460 Training loss 0.006978444289416075 Validation loss 0.011709809303283691 Accuracy 0.87744140625\n",
      "Iteration 19470 Training loss 0.006688236258924007 Validation loss 0.011955702677369118 Accuracy 0.875\n",
      "Iteration 19480 Training loss 0.008562058210372925 Validation loss 0.011790317483246326 Accuracy 0.875\n",
      "Iteration 19490 Training loss 0.009514672681689262 Validation loss 0.012304704636335373 Accuracy 0.87060546875\n",
      "Iteration 19500 Training loss 0.00872746016830206 Validation loss 0.011751098558306694 Accuracy 0.8759765625\n",
      "Iteration 19510 Training loss 0.006777188740670681 Validation loss 0.012036584317684174 Accuracy 0.8720703125\n",
      "Iteration 19520 Training loss 0.007811355404555798 Validation loss 0.012635739520192146 Accuracy 0.86669921875\n",
      "Iteration 19530 Training loss 0.0055406889878213406 Validation loss 0.011937489733099937 Accuracy 0.8740234375\n",
      "Iteration 19540 Training loss 0.008162273094058037 Validation loss 0.012090831995010376 Accuracy 0.8740234375\n",
      "Iteration 19550 Training loss 0.007501406129449606 Validation loss 0.012089838273823261 Accuracy 0.87255859375\n",
      "Iteration 19560 Training loss 0.006496045272797346 Validation loss 0.011513185687363148 Accuracy 0.87939453125\n",
      "Iteration 19570 Training loss 0.007779345847666264 Validation loss 0.012248864397406578 Accuracy 0.87060546875\n",
      "Iteration 19580 Training loss 0.007253782823681831 Validation loss 0.012166646309196949 Accuracy 0.8720703125\n",
      "Iteration 19590 Training loss 0.006576459389179945 Validation loss 0.011901875026524067 Accuracy 0.875\n",
      "Iteration 19600 Training loss 0.006442081183195114 Validation loss 0.011720802634954453 Accuracy 0.87744140625\n",
      "Iteration 19610 Training loss 0.010024857707321644 Validation loss 0.012746283784508705 Accuracy 0.8662109375\n",
      "Iteration 19620 Training loss 0.006269244942814112 Validation loss 0.011902897618710995 Accuracy 0.87451171875\n",
      "Iteration 19630 Training loss 0.006492486223578453 Validation loss 0.01207934319972992 Accuracy 0.87353515625\n",
      "Iteration 19640 Training loss 0.007371225859969854 Validation loss 0.011886459775269032 Accuracy 0.87451171875\n",
      "Iteration 19650 Training loss 0.006269598379731178 Validation loss 0.01243369746953249 Accuracy 0.8701171875\n",
      "Iteration 19660 Training loss 0.010413940995931625 Validation loss 0.01223849318921566 Accuracy 0.87109375\n",
      "Iteration 19670 Training loss 0.008840561844408512 Validation loss 0.011875711381435394 Accuracy 0.875\n",
      "Iteration 19680 Training loss 0.007725599687546492 Validation loss 0.01192252617329359 Accuracy 0.87548828125\n",
      "Iteration 19690 Training loss 0.005518291611224413 Validation loss 0.011417378671467304 Accuracy 0.8798828125\n",
      "Iteration 19700 Training loss 0.010946174152195454 Validation loss 0.011766435578465462 Accuracy 0.87744140625\n",
      "Iteration 19710 Training loss 0.007583736442029476 Validation loss 0.01159963384270668 Accuracy 0.8779296875\n",
      "Iteration 19720 Training loss 0.008416157215833664 Validation loss 0.011338203214108944 Accuracy 0.88134765625\n",
      "Iteration 19730 Training loss 0.006259313318878412 Validation loss 0.011905219405889511 Accuracy 0.875\n",
      "Iteration 19740 Training loss 0.007983994670212269 Validation loss 0.012357831932604313 Accuracy 0.87060546875\n",
      "Iteration 19750 Training loss 0.005252046510577202 Validation loss 0.011866888031363487 Accuracy 0.875\n",
      "Iteration 19760 Training loss 0.008044615387916565 Validation loss 0.01230328343808651 Accuracy 0.87109375\n",
      "Iteration 19770 Training loss 0.00727566285058856 Validation loss 0.011668302118778229 Accuracy 0.87744140625\n",
      "Iteration 19780 Training loss 0.007453483995050192 Validation loss 0.011805463582277298 Accuracy 0.875\n",
      "Iteration 19790 Training loss 0.006250898819416761 Validation loss 0.011798289604485035 Accuracy 0.87548828125\n",
      "Iteration 19800 Training loss 0.004783414304256439 Validation loss 0.01160119567066431 Accuracy 0.87841796875\n",
      "Iteration 19810 Training loss 0.0066330875270068645 Validation loss 0.011585869826376438 Accuracy 0.87841796875\n",
      "Iteration 19820 Training loss 0.0060520414263010025 Validation loss 0.011819418519735336 Accuracy 0.8759765625\n",
      "Iteration 19830 Training loss 0.005465890280902386 Validation loss 0.011541547253727913 Accuracy 0.87890625\n",
      "Iteration 19840 Training loss 0.00895907822996378 Validation loss 0.011606015264987946 Accuracy 0.87744140625\n",
      "Iteration 19850 Training loss 0.007196292281150818 Validation loss 0.011756069026887417 Accuracy 0.875\n",
      "Iteration 19860 Training loss 0.0070069339126348495 Validation loss 0.011581989005208015 Accuracy 0.87890625\n",
      "Iteration 19870 Training loss 0.009105204604566097 Validation loss 0.012462271377444267 Accuracy 0.86865234375\n",
      "Iteration 19880 Training loss 0.00758237624540925 Validation loss 0.011660898104310036 Accuracy 0.87744140625\n",
      "Iteration 19890 Training loss 0.007116631604731083 Validation loss 0.011980768293142319 Accuracy 0.87353515625\n",
      "Iteration 19900 Training loss 0.0074494159780442715 Validation loss 0.01188630610704422 Accuracy 0.87548828125\n",
      "Iteration 19910 Training loss 0.005803808569908142 Validation loss 0.011528229340910912 Accuracy 0.8779296875\n",
      "Iteration 19920 Training loss 0.006519509479403496 Validation loss 0.011808296665549278 Accuracy 0.875\n",
      "Iteration 19930 Training loss 0.006135799456387758 Validation loss 0.011886940337717533 Accuracy 0.87451171875\n",
      "Iteration 19940 Training loss 0.007838474586606026 Validation loss 0.011574533767998219 Accuracy 0.87841796875\n",
      "Iteration 19950 Training loss 0.0060670520178973675 Validation loss 0.011607257649302483 Accuracy 0.87744140625\n",
      "Iteration 19960 Training loss 0.006034668069332838 Validation loss 0.011533461511135101 Accuracy 0.87939453125\n",
      "Iteration 19970 Training loss 0.007691734004765749 Validation loss 0.011621088720858097 Accuracy 0.8779296875\n",
      "Iteration 19980 Training loss 0.005817682947963476 Validation loss 0.011771904304623604 Accuracy 0.876953125\n",
      "Iteration 19990 Training loss 0.008314733393490314 Validation loss 0.012217792682349682 Accuracy 0.8720703125\n",
      "Iteration 20000 Training loss 0.006352282129228115 Validation loss 0.011400651186704636 Accuracy 0.87939453125\n",
      "Iteration 20010 Training loss 0.005363464821130037 Validation loss 0.012062731198966503 Accuracy 0.87353515625\n",
      "Iteration 20020 Training loss 0.007568892557173967 Validation loss 0.01145137194544077 Accuracy 0.87939453125\n",
      "Iteration 20030 Training loss 0.008243635296821594 Validation loss 0.01169281080365181 Accuracy 0.87646484375\n",
      "Iteration 20040 Training loss 0.007052088156342506 Validation loss 0.011893988586962223 Accuracy 0.87451171875\n",
      "Iteration 20050 Training loss 0.007121838629245758 Validation loss 0.011749987490475178 Accuracy 0.8759765625\n",
      "Iteration 20060 Training loss 0.007856156677007675 Validation loss 0.011401528492569923 Accuracy 0.88037109375\n",
      "Iteration 20070 Training loss 0.005072084255516529 Validation loss 0.011406774632632732 Accuracy 0.87939453125\n",
      "Iteration 20080 Training loss 0.006982236634939909 Validation loss 0.012004205957055092 Accuracy 0.87353515625\n",
      "Iteration 20090 Training loss 0.006063316483050585 Validation loss 0.011508197523653507 Accuracy 0.87890625\n",
      "Iteration 20100 Training loss 0.010139591060578823 Validation loss 0.012349067255854607 Accuracy 0.869140625\n",
      "Iteration 20110 Training loss 0.00888932403177023 Validation loss 0.011512376368045807 Accuracy 0.87890625\n",
      "Iteration 20120 Training loss 0.006830182857811451 Validation loss 0.012074630707502365 Accuracy 0.87255859375\n",
      "Iteration 20130 Training loss 0.005717205815017223 Validation loss 0.011604768224060535 Accuracy 0.87744140625\n",
      "Iteration 20140 Training loss 0.007941663265228271 Validation loss 0.01178357470780611 Accuracy 0.87744140625\n",
      "Iteration 20150 Training loss 0.006918226834386587 Validation loss 0.012080592103302479 Accuracy 0.87353515625\n",
      "Iteration 20160 Training loss 0.007973626255989075 Validation loss 0.011754176579415798 Accuracy 0.87646484375\n",
      "Iteration 20170 Training loss 0.006810352206230164 Validation loss 0.011801783926784992 Accuracy 0.87646484375\n",
      "Iteration 20180 Training loss 0.005919657181948423 Validation loss 0.012067156843841076 Accuracy 0.87353515625\n",
      "Iteration 20190 Training loss 0.008270172402262688 Validation loss 0.012132614850997925 Accuracy 0.873046875\n",
      "Iteration 20200 Training loss 0.004626660142093897 Validation loss 0.011674684472382069 Accuracy 0.87744140625\n",
      "Iteration 20210 Training loss 0.0072489515878260136 Validation loss 0.011887497268617153 Accuracy 0.87451171875\n",
      "Iteration 20220 Training loss 0.006479061674326658 Validation loss 0.012056464329361916 Accuracy 0.8720703125\n",
      "Iteration 20230 Training loss 0.006829905789345503 Validation loss 0.011648600921034813 Accuracy 0.876953125\n",
      "Iteration 20240 Training loss 0.006996419280767441 Validation loss 0.011545702815055847 Accuracy 0.87841796875\n",
      "Iteration 20250 Training loss 0.0060584088787436485 Validation loss 0.01240853127092123 Accuracy 0.86962890625\n",
      "Iteration 20260 Training loss 0.007705168332904577 Validation loss 0.012117165140807629 Accuracy 0.873046875\n",
      "Iteration 20270 Training loss 0.006214364431798458 Validation loss 0.011664699763059616 Accuracy 0.876953125\n",
      "Iteration 20280 Training loss 0.009027284570038319 Validation loss 0.011612013913691044 Accuracy 0.8779296875\n",
      "Iteration 20290 Training loss 0.006399883423000574 Validation loss 0.01171521469950676 Accuracy 0.876953125\n",
      "Iteration 20300 Training loss 0.008812684565782547 Validation loss 0.012079733423888683 Accuracy 0.87255859375\n",
      "Iteration 20310 Training loss 0.00748441182076931 Validation loss 0.012082833796739578 Accuracy 0.873046875\n",
      "Iteration 20320 Training loss 0.005073782987892628 Validation loss 0.011570774018764496 Accuracy 0.87841796875\n",
      "Iteration 20330 Training loss 0.007123475428670645 Validation loss 0.011925782077014446 Accuracy 0.87451171875\n",
      "Iteration 20340 Training loss 0.0054375589825212955 Validation loss 0.011713105253875256 Accuracy 0.87744140625\n",
      "Iteration 20350 Training loss 0.00860959105193615 Validation loss 0.01148940622806549 Accuracy 0.87890625\n",
      "Iteration 20360 Training loss 0.008760361932218075 Validation loss 0.012100549414753914 Accuracy 0.873046875\n",
      "Iteration 20370 Training loss 0.006704936269670725 Validation loss 0.011802866123616695 Accuracy 0.8759765625\n",
      "Iteration 20380 Training loss 0.0076678963378071785 Validation loss 0.011626798659563065 Accuracy 0.8779296875\n",
      "Iteration 20390 Training loss 0.005967423785477877 Validation loss 0.011376645416021347 Accuracy 0.88037109375\n",
      "Iteration 20400 Training loss 0.006379289086908102 Validation loss 0.011609865352511406 Accuracy 0.8779296875\n",
      "Iteration 20410 Training loss 0.00796347577124834 Validation loss 0.011598652228713036 Accuracy 0.87744140625\n",
      "Iteration 20420 Training loss 0.006154251750558615 Validation loss 0.011466510593891144 Accuracy 0.87890625\n",
      "Iteration 20430 Training loss 0.009873430244624615 Validation loss 0.012921166606247425 Accuracy 0.86376953125\n",
      "Iteration 20440 Training loss 0.006806838326156139 Validation loss 0.011430758982896805 Accuracy 0.8798828125\n",
      "Iteration 20450 Training loss 0.010488289408385754 Validation loss 0.012465540319681168 Accuracy 0.869140625\n",
      "Iteration 20460 Training loss 0.007955145090818405 Validation loss 0.01214428711682558 Accuracy 0.873046875\n",
      "Iteration 20470 Training loss 0.005833802279084921 Validation loss 0.012573298066854477 Accuracy 0.8671875\n",
      "Iteration 20480 Training loss 0.007533869240432978 Validation loss 0.011973352171480656 Accuracy 0.873046875\n",
      "Iteration 20490 Training loss 0.007126061711460352 Validation loss 0.011385058052837849 Accuracy 0.87939453125\n",
      "Iteration 20500 Training loss 0.006382415536791086 Validation loss 0.012005601078271866 Accuracy 0.87451171875\n",
      "Iteration 20510 Training loss 0.0075521813705563545 Validation loss 0.011913737282156944 Accuracy 0.875\n",
      "Iteration 20520 Training loss 0.006485556252300739 Validation loss 0.01167260855436325 Accuracy 0.8779296875\n",
      "Iteration 20530 Training loss 0.004726841580122709 Validation loss 0.011610455811023712 Accuracy 0.8779296875\n",
      "Iteration 20540 Training loss 0.006911533419042826 Validation loss 0.011857946403324604 Accuracy 0.8759765625\n",
      "Iteration 20550 Training loss 0.00712865125387907 Validation loss 0.011642700992524624 Accuracy 0.87646484375\n",
      "Iteration 20560 Training loss 0.009192044846713543 Validation loss 0.011784793809056282 Accuracy 0.875\n",
      "Iteration 20570 Training loss 0.008674520999193192 Validation loss 0.012487963773310184 Accuracy 0.869140625\n",
      "Iteration 20580 Training loss 0.006225828547030687 Validation loss 0.01167727168649435 Accuracy 0.87744140625\n",
      "Iteration 20590 Training loss 0.005467962007969618 Validation loss 0.012015873566269875 Accuracy 0.873046875\n",
      "Iteration 20600 Training loss 0.004976952914148569 Validation loss 0.011588489636778831 Accuracy 0.87841796875\n",
      "Iteration 20610 Training loss 0.005116116721183062 Validation loss 0.011591327376663685 Accuracy 0.87841796875\n",
      "Iteration 20620 Training loss 0.008408918976783752 Validation loss 0.01216015126556158 Accuracy 0.8720703125\n",
      "Iteration 20630 Training loss 0.005438619758933783 Validation loss 0.011856178753077984 Accuracy 0.875\n",
      "Iteration 20640 Training loss 0.0070256562903523445 Validation loss 0.011838178150355816 Accuracy 0.87548828125\n",
      "Iteration 20650 Training loss 0.006804533768445253 Validation loss 0.011631252244114876 Accuracy 0.87744140625\n",
      "Iteration 20660 Training loss 0.009836097247898579 Validation loss 0.01186366192996502 Accuracy 0.87548828125\n",
      "Iteration 20670 Training loss 0.00551576679572463 Validation loss 0.011444096453487873 Accuracy 0.8798828125\n",
      "Iteration 20680 Training loss 0.004408481065183878 Validation loss 0.011965048499405384 Accuracy 0.87451171875\n",
      "Iteration 20690 Training loss 0.0075209173373878 Validation loss 0.01202132273465395 Accuracy 0.8720703125\n",
      "Iteration 20700 Training loss 0.005143304355442524 Validation loss 0.011659030802547932 Accuracy 0.87841796875\n",
      "Iteration 20710 Training loss 0.008225549943745136 Validation loss 0.011925749480724335 Accuracy 0.875\n",
      "Iteration 20720 Training loss 0.004957871045917273 Validation loss 0.011811408214271069 Accuracy 0.8759765625\n",
      "Iteration 20730 Training loss 0.006920411251485348 Validation loss 0.0118557158857584 Accuracy 0.875\n",
      "Iteration 20740 Training loss 0.00619810214266181 Validation loss 0.011818023398518562 Accuracy 0.87548828125\n",
      "Iteration 20750 Training loss 0.007119977846741676 Validation loss 0.012181092984974384 Accuracy 0.87158203125\n",
      "Iteration 20760 Training loss 0.007822506129741669 Validation loss 0.012569306418299675 Accuracy 0.86767578125\n",
      "Iteration 20770 Training loss 0.007530753966420889 Validation loss 0.01206537801772356 Accuracy 0.87353515625\n",
      "Iteration 20780 Training loss 0.006740823853760958 Validation loss 0.011646205559372902 Accuracy 0.8779296875\n",
      "Iteration 20790 Training loss 0.008442948572337627 Validation loss 0.012385229580104351 Accuracy 0.86962890625\n",
      "Iteration 20800 Training loss 0.005634988192468882 Validation loss 0.011610081419348717 Accuracy 0.87841796875\n",
      "Iteration 20810 Training loss 0.0071283914148807526 Validation loss 0.011805739253759384 Accuracy 0.8759765625\n",
      "Iteration 20820 Training loss 0.0063687036745250225 Validation loss 0.01178622804582119 Accuracy 0.87646484375\n",
      "Iteration 20830 Training loss 0.006866881158202887 Validation loss 0.01184119563549757 Accuracy 0.8759765625\n",
      "Iteration 20840 Training loss 0.006431824993342161 Validation loss 0.011819939129054546 Accuracy 0.87548828125\n",
      "Iteration 20850 Training loss 0.006137824151664972 Validation loss 0.011289136484265327 Accuracy 0.88134765625\n",
      "Iteration 20860 Training loss 0.006795684807002544 Validation loss 0.011264531873166561 Accuracy 0.880859375\n",
      "Iteration 20870 Training loss 0.006099455524235964 Validation loss 0.011662842705845833 Accuracy 0.87744140625\n",
      "Iteration 20880 Training loss 0.007454556878656149 Validation loss 0.011416415683925152 Accuracy 0.8798828125\n",
      "Iteration 20890 Training loss 0.008346877060830593 Validation loss 0.012279821559786797 Accuracy 0.87060546875\n",
      "Iteration 20900 Training loss 0.004572523757815361 Validation loss 0.011728779412806034 Accuracy 0.8759765625\n",
      "Iteration 20910 Training loss 0.008064857684075832 Validation loss 0.011677619069814682 Accuracy 0.876953125\n",
      "Iteration 20920 Training loss 0.005522776860743761 Validation loss 0.011394632048904896 Accuracy 0.8798828125\n",
      "Iteration 20930 Training loss 0.00529453344643116 Validation loss 0.011206453666090965 Accuracy 0.8818359375\n",
      "Iteration 20940 Training loss 0.00609762966632843 Validation loss 0.011474044993519783 Accuracy 0.87939453125\n",
      "Iteration 20950 Training loss 0.008936544880270958 Validation loss 0.011994938366115093 Accuracy 0.873046875\n",
      "Iteration 20960 Training loss 0.007445622701197863 Validation loss 0.011633068323135376 Accuracy 0.8779296875\n",
      "Iteration 20970 Training loss 0.0063543617725372314 Validation loss 0.011690656654536724 Accuracy 0.87646484375\n",
      "Iteration 20980 Training loss 0.007134765386581421 Validation loss 0.011937580071389675 Accuracy 0.87353515625\n",
      "Iteration 20990 Training loss 0.0070445905439555645 Validation loss 0.011394601315259933 Accuracy 0.8798828125\n",
      "Iteration 21000 Training loss 0.005740034859627485 Validation loss 0.011730806902050972 Accuracy 0.87548828125\n",
      "Iteration 21010 Training loss 0.006967534311115742 Validation loss 0.011381813324987888 Accuracy 0.8798828125\n",
      "Iteration 21020 Training loss 0.005727672949433327 Validation loss 0.012485815212130547 Accuracy 0.8681640625\n",
      "Iteration 21030 Training loss 0.005685076117515564 Validation loss 0.011473161168396473 Accuracy 0.87890625\n",
      "Iteration 21040 Training loss 0.00592609541490674 Validation loss 0.01148407906293869 Accuracy 0.8798828125\n",
      "Iteration 21050 Training loss 0.007339395582675934 Validation loss 0.011627458967268467 Accuracy 0.87744140625\n",
      "Iteration 21060 Training loss 0.009000862017273903 Validation loss 0.011921325698494911 Accuracy 0.875\n",
      "Iteration 21070 Training loss 0.007437131367623806 Validation loss 0.011686638928949833 Accuracy 0.876953125\n",
      "Iteration 21080 Training loss 0.008262427523732185 Validation loss 0.01165554579347372 Accuracy 0.8779296875\n",
      "Iteration 21090 Training loss 0.007801157422363758 Validation loss 0.01160862110555172 Accuracy 0.8779296875\n",
      "Iteration 21100 Training loss 0.005707354750484228 Validation loss 0.011384177021682262 Accuracy 0.88037109375\n",
      "Iteration 21110 Training loss 0.007214664947241545 Validation loss 0.012255944311618805 Accuracy 0.87109375\n",
      "Iteration 21120 Training loss 0.007766688708215952 Validation loss 0.011530219577252865 Accuracy 0.87841796875\n",
      "Iteration 21130 Training loss 0.006327853538095951 Validation loss 0.011719447560608387 Accuracy 0.8759765625\n",
      "Iteration 21140 Training loss 0.0056572542525827885 Validation loss 0.011361398734152317 Accuracy 0.880859375\n",
      "Iteration 21150 Training loss 0.006688543129712343 Validation loss 0.01147528551518917 Accuracy 0.87939453125\n",
      "Iteration 21160 Training loss 0.006722385995090008 Validation loss 0.01124102994799614 Accuracy 0.88134765625\n",
      "Iteration 21170 Training loss 0.007178655359894037 Validation loss 0.011479929089546204 Accuracy 0.8798828125\n",
      "Iteration 21180 Training loss 0.008383908309042454 Validation loss 0.011329533532261848 Accuracy 0.880859375\n",
      "Iteration 21190 Training loss 0.007698180619627237 Validation loss 0.011944944970309734 Accuracy 0.87451171875\n",
      "Iteration 21200 Training loss 0.007838508114218712 Validation loss 0.012314794585108757 Accuracy 0.87060546875\n",
      "Iteration 21210 Training loss 0.007203333545476198 Validation loss 0.011670436710119247 Accuracy 0.876953125\n",
      "Iteration 21220 Training loss 0.008291076868772507 Validation loss 0.012074189260601997 Accuracy 0.87255859375\n",
      "Iteration 21230 Training loss 0.007538847625255585 Validation loss 0.01218628603965044 Accuracy 0.87060546875\n",
      "Iteration 21240 Training loss 0.006944649387151003 Validation loss 0.012081527151167393 Accuracy 0.87255859375\n",
      "Iteration 21250 Training loss 0.005592842120677233 Validation loss 0.011538260616362095 Accuracy 0.87939453125\n",
      "Iteration 21260 Training loss 0.008474904112517834 Validation loss 0.011937406845390797 Accuracy 0.8740234375\n",
      "Iteration 21270 Training loss 0.0066657885909080505 Validation loss 0.011547885835170746 Accuracy 0.87890625\n",
      "Iteration 21280 Training loss 0.0058568790555000305 Validation loss 0.011631330475211143 Accuracy 0.87744140625\n",
      "Iteration 21290 Training loss 0.006352369673550129 Validation loss 0.0112537182867527 Accuracy 0.8818359375\n",
      "Iteration 21300 Training loss 0.0057732099667191505 Validation loss 0.01134719792753458 Accuracy 0.880859375\n",
      "Iteration 21310 Training loss 0.007108307909220457 Validation loss 0.01185719296336174 Accuracy 0.87548828125\n",
      "Iteration 21320 Training loss 0.006698837038129568 Validation loss 0.011733978986740112 Accuracy 0.87646484375\n",
      "Iteration 21330 Training loss 0.006191717926412821 Validation loss 0.01209142804145813 Accuracy 0.873046875\n",
      "Iteration 21340 Training loss 0.005807476118206978 Validation loss 0.011378027498722076 Accuracy 0.8798828125\n",
      "Iteration 21350 Training loss 0.00559152290225029 Validation loss 0.011430710554122925 Accuracy 0.87939453125\n",
      "Iteration 21360 Training loss 0.009190069511532784 Validation loss 0.011940049938857555 Accuracy 0.87353515625\n",
      "Iteration 21370 Training loss 0.0059542362578213215 Validation loss 0.011360879987478256 Accuracy 0.8798828125\n",
      "Iteration 21380 Training loss 0.007296995725482702 Validation loss 0.01200046855956316 Accuracy 0.8740234375\n",
      "Iteration 21390 Training loss 0.008269188925623894 Validation loss 0.011766967363655567 Accuracy 0.87646484375\n",
      "Iteration 21400 Training loss 0.006903535220772028 Validation loss 0.011571744456887245 Accuracy 0.87939453125\n",
      "Iteration 21410 Training loss 0.006398222409188747 Validation loss 0.011590500362217426 Accuracy 0.8779296875\n",
      "Iteration 21420 Training loss 0.0056906635873019695 Validation loss 0.011765656061470509 Accuracy 0.8759765625\n",
      "Iteration 21430 Training loss 0.00477992556989193 Validation loss 0.011624060571193695 Accuracy 0.87744140625\n",
      "Iteration 21440 Training loss 0.007037846837192774 Validation loss 0.011979104951024055 Accuracy 0.875\n",
      "Iteration 21450 Training loss 0.005939450580626726 Validation loss 0.011719190515577793 Accuracy 0.876953125\n",
      "Iteration 21460 Training loss 0.005497191566973925 Validation loss 0.011609903536736965 Accuracy 0.8779296875\n",
      "Iteration 21470 Training loss 0.008904078043997288 Validation loss 0.01274939626455307 Accuracy 0.86572265625\n",
      "Iteration 21480 Training loss 0.005493914242833853 Validation loss 0.011202709749341011 Accuracy 0.88232421875\n",
      "Iteration 21490 Training loss 0.004859123378992081 Validation loss 0.01114602293819189 Accuracy 0.88232421875\n",
      "Iteration 21500 Training loss 0.008249271661043167 Validation loss 0.013582802377641201 Accuracy 0.85693359375\n",
      "Iteration 21510 Training loss 0.004936514422297478 Validation loss 0.0114812720566988 Accuracy 0.8779296875\n",
      "Iteration 21520 Training loss 0.006892061326652765 Validation loss 0.011535807512700558 Accuracy 0.87890625\n",
      "Iteration 21530 Training loss 0.006398672703653574 Validation loss 0.011441564187407494 Accuracy 0.87939453125\n",
      "Iteration 21540 Training loss 0.00628669373691082 Validation loss 0.012311508879065514 Accuracy 0.87158203125\n",
      "Iteration 21550 Training loss 0.006962586659938097 Validation loss 0.011891821399331093 Accuracy 0.87548828125\n",
      "Iteration 21560 Training loss 0.005905600264668465 Validation loss 0.012177892960608006 Accuracy 0.87158203125\n",
      "Iteration 21570 Training loss 0.006108326371759176 Validation loss 0.011433462612330914 Accuracy 0.8798828125\n",
      "Iteration 21580 Training loss 0.006224223878234625 Validation loss 0.011861502192914486 Accuracy 0.875\n",
      "Iteration 21590 Training loss 0.004973383154720068 Validation loss 0.011333060450851917 Accuracy 0.880859375\n",
      "Iteration 21600 Training loss 0.006169362459331751 Validation loss 0.011632567271590233 Accuracy 0.87744140625\n",
      "Iteration 21610 Training loss 0.005914566107094288 Validation loss 0.011227443814277649 Accuracy 0.8818359375\n",
      "Iteration 21620 Training loss 0.007495852652937174 Validation loss 0.011364434845745564 Accuracy 0.88037109375\n",
      "Iteration 21630 Training loss 0.006840101908892393 Validation loss 0.011607302352786064 Accuracy 0.87841796875\n",
      "Iteration 21640 Training loss 0.008340508677065372 Validation loss 0.011332012712955475 Accuracy 0.88037109375\n",
      "Iteration 21650 Training loss 0.008813257329165936 Validation loss 0.011347094550728798 Accuracy 0.8798828125\n",
      "Iteration 21660 Training loss 0.007692382670938969 Validation loss 0.011539997532963753 Accuracy 0.87841796875\n",
      "Iteration 21670 Training loss 0.006021992303431034 Validation loss 0.011526759713888168 Accuracy 0.87890625\n",
      "Iteration 21680 Training loss 0.005285362713038921 Validation loss 0.0113442437723279 Accuracy 0.88037109375\n",
      "Iteration 21690 Training loss 0.006025717128068209 Validation loss 0.01135936751961708 Accuracy 0.88037109375\n",
      "Iteration 21700 Training loss 0.0070494296960532665 Validation loss 0.01139053050428629 Accuracy 0.880859375\n",
      "Iteration 21710 Training loss 0.008107578381896019 Validation loss 0.011833393014967442 Accuracy 0.87451171875\n",
      "Iteration 21720 Training loss 0.006121294107288122 Validation loss 0.011819287203252316 Accuracy 0.875\n",
      "Iteration 21730 Training loss 0.0102214515209198 Validation loss 0.011714433319866657 Accuracy 0.87744140625\n",
      "Iteration 21740 Training loss 0.005816176068037748 Validation loss 0.011501840315759182 Accuracy 0.87890625\n",
      "Iteration 21750 Training loss 0.007188587915152311 Validation loss 0.011669527739286423 Accuracy 0.87646484375\n",
      "Iteration 21760 Training loss 0.007332461420446634 Validation loss 0.011405065655708313 Accuracy 0.87939453125\n",
      "Iteration 21770 Training loss 0.006010090932250023 Validation loss 0.011551725678145885 Accuracy 0.8779296875\n",
      "Iteration 21780 Training loss 0.005773287266492844 Validation loss 0.011566807515919209 Accuracy 0.87890625\n",
      "Iteration 21790 Training loss 0.00789420586079359 Validation loss 0.012297326698899269 Accuracy 0.8701171875\n",
      "Iteration 21800 Training loss 0.007521314080804586 Validation loss 0.0116495992988348 Accuracy 0.87841796875\n",
      "Iteration 21810 Training loss 0.006071270443499088 Validation loss 0.011575681157410145 Accuracy 0.87890625\n",
      "Iteration 21820 Training loss 0.006763548590242863 Validation loss 0.011901666410267353 Accuracy 0.8740234375\n",
      "Iteration 21830 Training loss 0.007111165206879377 Validation loss 0.011635707691311836 Accuracy 0.87744140625\n",
      "Iteration 21840 Training loss 0.007064598146826029 Validation loss 0.011684870347380638 Accuracy 0.87744140625\n",
      "Iteration 21850 Training loss 0.006900899112224579 Validation loss 0.011435167863965034 Accuracy 0.8798828125\n",
      "Iteration 21860 Training loss 0.006984742358326912 Validation loss 0.011311415582895279 Accuracy 0.880859375\n",
      "Iteration 21870 Training loss 0.007495645899325609 Validation loss 0.012120900675654411 Accuracy 0.87255859375\n",
      "Iteration 21880 Training loss 0.006689752917736769 Validation loss 0.01150477398186922 Accuracy 0.87841796875\n",
      "Iteration 21890 Training loss 0.007782142609357834 Validation loss 0.012052162550389767 Accuracy 0.8740234375\n",
      "Iteration 21900 Training loss 0.0060417549684643745 Validation loss 0.011261441744863987 Accuracy 0.88134765625\n",
      "Iteration 21910 Training loss 0.006607873365283012 Validation loss 0.011874807067215443 Accuracy 0.87451171875\n",
      "Iteration 21920 Training loss 0.007052567787468433 Validation loss 0.011614159680902958 Accuracy 0.87841796875\n",
      "Iteration 21930 Training loss 0.00917354691773653 Validation loss 0.012731580063700676 Accuracy 0.86669921875\n",
      "Iteration 21940 Training loss 0.004525294061750174 Validation loss 0.011275977827608585 Accuracy 0.8818359375\n",
      "Iteration 21950 Training loss 0.006018310319632292 Validation loss 0.011506891809403896 Accuracy 0.87890625\n",
      "Iteration 21960 Training loss 0.008158715441823006 Validation loss 0.011697784066200256 Accuracy 0.87646484375\n",
      "Iteration 21970 Training loss 0.006711573339998722 Validation loss 0.012067957781255245 Accuracy 0.87158203125\n",
      "Iteration 21980 Training loss 0.007310068234801292 Validation loss 0.011813928373157978 Accuracy 0.8759765625\n",
      "Iteration 21990 Training loss 0.005067609250545502 Validation loss 0.011575629934668541 Accuracy 0.8779296875\n",
      "Iteration 22000 Training loss 0.007485285401344299 Validation loss 0.01186361163854599 Accuracy 0.8740234375\n",
      "Iteration 22010 Training loss 0.007853452116250992 Validation loss 0.01178718265146017 Accuracy 0.8759765625\n",
      "Iteration 22020 Training loss 0.006173360161483288 Validation loss 0.01170973852276802 Accuracy 0.8759765625\n",
      "Iteration 22030 Training loss 0.006322323344647884 Validation loss 0.01171646174043417 Accuracy 0.875\n",
      "Iteration 22040 Training loss 0.00730010075494647 Validation loss 0.011354584246873856 Accuracy 0.87890625\n",
      "Iteration 22050 Training loss 0.006703244522213936 Validation loss 0.011686627753078938 Accuracy 0.87646484375\n",
      "Iteration 22060 Training loss 0.0071447547525167465 Validation loss 0.011411169543862343 Accuracy 0.8798828125\n",
      "Iteration 22070 Training loss 0.0038456039037555456 Validation loss 0.011142470873892307 Accuracy 0.88232421875\n",
      "Iteration 22080 Training loss 0.00509046483784914 Validation loss 0.011343107558786869 Accuracy 0.88037109375\n",
      "Iteration 22090 Training loss 0.0051116603426635265 Validation loss 0.011781793087720871 Accuracy 0.8759765625\n",
      "Iteration 22100 Training loss 0.008982863277196884 Validation loss 0.011820285581052303 Accuracy 0.87548828125\n",
      "Iteration 22110 Training loss 0.006782062351703644 Validation loss 0.011629397049546242 Accuracy 0.876953125\n",
      "Iteration 22120 Training loss 0.006461183074861765 Validation loss 0.011962086893618107 Accuracy 0.87353515625\n",
      "Iteration 22130 Training loss 0.005151183810085058 Validation loss 0.011531169526278973 Accuracy 0.87890625\n",
      "Iteration 22140 Training loss 0.009500261396169662 Validation loss 0.011932404711842537 Accuracy 0.87451171875\n",
      "Iteration 22150 Training loss 0.007461948320269585 Validation loss 0.011115227825939655 Accuracy 0.88232421875\n",
      "Iteration 22160 Training loss 0.0064300899393856525 Validation loss 0.012319513596594334 Accuracy 0.86962890625\n",
      "Iteration 22170 Training loss 0.005360629875212908 Validation loss 0.011281519196927547 Accuracy 0.880859375\n",
      "Iteration 22180 Training loss 0.008816815912723541 Validation loss 0.012223134748637676 Accuracy 0.87109375\n",
      "Iteration 22190 Training loss 0.006340446416288614 Validation loss 0.01136691588908434 Accuracy 0.8798828125\n",
      "Iteration 22200 Training loss 0.006024359259754419 Validation loss 0.011318706907331944 Accuracy 0.880859375\n",
      "Iteration 22210 Training loss 0.007271686568856239 Validation loss 0.011705739423632622 Accuracy 0.8759765625\n",
      "Iteration 22220 Training loss 0.009577176533639431 Validation loss 0.013255920261144638 Accuracy 0.85986328125\n",
      "Iteration 22230 Training loss 0.006197106558829546 Validation loss 0.011490714736282825 Accuracy 0.87841796875\n",
      "Iteration 22240 Training loss 0.006731685716658831 Validation loss 0.011214599013328552 Accuracy 0.88232421875\n",
      "Iteration 22250 Training loss 0.006036300677806139 Validation loss 0.011538698337972164 Accuracy 0.8779296875\n",
      "Iteration 22260 Training loss 0.007353529334068298 Validation loss 0.01129046268761158 Accuracy 0.88134765625\n",
      "Iteration 22270 Training loss 0.006563976872712374 Validation loss 0.011248906143009663 Accuracy 0.88134765625\n",
      "Iteration 22280 Training loss 0.006198596209287643 Validation loss 0.011362974531948566 Accuracy 0.88037109375\n",
      "Iteration 22290 Training loss 0.007464478723704815 Validation loss 0.011573593132197857 Accuracy 0.8779296875\n",
      "Iteration 22300 Training loss 0.0067467051558196545 Validation loss 0.01142838690429926 Accuracy 0.8798828125\n",
      "Iteration 22310 Training loss 0.0051316265016794205 Validation loss 0.011598212644457817 Accuracy 0.87744140625\n",
      "Iteration 22320 Training loss 0.00551370345056057 Validation loss 0.01141311228275299 Accuracy 0.8798828125\n",
      "Iteration 22330 Training loss 0.006915983743965626 Validation loss 0.011278152465820312 Accuracy 0.88134765625\n",
      "Iteration 22340 Training loss 0.004718095064163208 Validation loss 0.011136362329125404 Accuracy 0.88232421875\n",
      "Iteration 22350 Training loss 0.005332102999091148 Validation loss 0.011324831284582615 Accuracy 0.880859375\n",
      "Iteration 22360 Training loss 0.007257335353642702 Validation loss 0.011408663354814053 Accuracy 0.8798828125\n",
      "Iteration 22370 Training loss 0.007481763605028391 Validation loss 0.011461556889116764 Accuracy 0.87939453125\n",
      "Iteration 22380 Training loss 0.008754092268645763 Validation loss 0.01153438538312912 Accuracy 0.87890625\n",
      "Iteration 22390 Training loss 0.007146343123167753 Validation loss 0.011664772406220436 Accuracy 0.876953125\n",
      "Iteration 22400 Training loss 0.004988780245184898 Validation loss 0.011369766667485237 Accuracy 0.88037109375\n",
      "Iteration 22410 Training loss 0.005092863459140062 Validation loss 0.011277562938630581 Accuracy 0.88037109375\n",
      "Iteration 22420 Training loss 0.006883824709802866 Validation loss 0.01118677482008934 Accuracy 0.88134765625\n",
      "Iteration 22430 Training loss 0.00687751779332757 Validation loss 0.011540091596543789 Accuracy 0.8779296875\n",
      "Iteration 22440 Training loss 0.005839757155627012 Validation loss 0.011335006915032864 Accuracy 0.87939453125\n",
      "Iteration 22450 Training loss 0.005835818592458963 Validation loss 0.011806783266365528 Accuracy 0.87548828125\n",
      "Iteration 22460 Training loss 0.008365200832486153 Validation loss 0.012186619453132153 Accuracy 0.87109375\n",
      "Iteration 22470 Training loss 0.006461500655859709 Validation loss 0.011473177000880241 Accuracy 0.87841796875\n",
      "Iteration 22480 Training loss 0.00695995707064867 Validation loss 0.011601803824305534 Accuracy 0.87890625\n",
      "Iteration 22490 Training loss 0.00519119156524539 Validation loss 0.011561362072825432 Accuracy 0.8779296875\n",
      "Iteration 22500 Training loss 0.006442566402256489 Validation loss 0.011596680618822575 Accuracy 0.8779296875\n",
      "Iteration 22510 Training loss 0.005283366423100233 Validation loss 0.011778061278164387 Accuracy 0.8759765625\n",
      "Iteration 22520 Training loss 0.007112703286111355 Validation loss 0.011455628089606762 Accuracy 0.87939453125\n",
      "Iteration 22530 Training loss 0.00718674948439002 Validation loss 0.011516178958117962 Accuracy 0.87841796875\n",
      "Iteration 22540 Training loss 0.007472315337508917 Validation loss 0.012553123757243156 Accuracy 0.8671875\n",
      "Iteration 22550 Training loss 0.008036904968321323 Validation loss 0.011236213147640228 Accuracy 0.8828125\n",
      "Iteration 22560 Training loss 0.003784920321777463 Validation loss 0.011383556760847569 Accuracy 0.8779296875\n",
      "Iteration 22570 Training loss 0.006931766401976347 Validation loss 0.011308911256492138 Accuracy 0.87939453125\n",
      "Iteration 22580 Training loss 0.006544387899339199 Validation loss 0.011534023098647594 Accuracy 0.87841796875\n",
      "Iteration 22590 Training loss 0.007689665537327528 Validation loss 0.011193196289241314 Accuracy 0.8818359375\n",
      "Iteration 22600 Training loss 0.005699389148503542 Validation loss 0.011508532799780369 Accuracy 0.87841796875\n",
      "Iteration 22610 Training loss 0.0046998788602650166 Validation loss 0.011302340775728226 Accuracy 0.88037109375\n",
      "Iteration 22620 Training loss 0.005305654369294643 Validation loss 0.011278335005044937 Accuracy 0.880859375\n",
      "Iteration 22630 Training loss 0.0060925609432160854 Validation loss 0.011773314327001572 Accuracy 0.8759765625\n",
      "Iteration 22640 Training loss 0.005587513092905283 Validation loss 0.01154824998229742 Accuracy 0.8779296875\n",
      "Iteration 22650 Training loss 0.003790859831497073 Validation loss 0.011328835971653461 Accuracy 0.8798828125\n",
      "Iteration 22660 Training loss 0.0069801090285182 Validation loss 0.011367116123437881 Accuracy 0.88037109375\n",
      "Iteration 22670 Training loss 0.005758407525718212 Validation loss 0.011732607148587704 Accuracy 0.87548828125\n",
      "Iteration 22680 Training loss 0.008139865472912788 Validation loss 0.011408703401684761 Accuracy 0.87939453125\n",
      "Iteration 22690 Training loss 0.009560180827975273 Validation loss 0.014180552214384079 Accuracy 0.8515625\n",
      "Iteration 22700 Training loss 0.00577856320887804 Validation loss 0.011616818606853485 Accuracy 0.8779296875\n",
      "Iteration 22710 Training loss 0.005522649735212326 Validation loss 0.011347794905304909 Accuracy 0.88037109375\n",
      "Iteration 22720 Training loss 0.006405968684703112 Validation loss 0.011438670568168163 Accuracy 0.87939453125\n",
      "Iteration 22730 Training loss 0.005879055708646774 Validation loss 0.011477958410978317 Accuracy 0.87890625\n",
      "Iteration 22740 Training loss 0.006773513741791248 Validation loss 0.01164157409220934 Accuracy 0.87744140625\n",
      "Iteration 22750 Training loss 0.006224830634891987 Validation loss 0.011731859296560287 Accuracy 0.876953125\n",
      "Iteration 22760 Training loss 0.004398013930767775 Validation loss 0.011562065221369267 Accuracy 0.8779296875\n",
      "Iteration 22770 Training loss 0.004872904159128666 Validation loss 0.011762261390686035 Accuracy 0.87548828125\n",
      "Iteration 22780 Training loss 0.006891642697155476 Validation loss 0.011401491239666939 Accuracy 0.87939453125\n",
      "Iteration 22790 Training loss 0.005543590523302555 Validation loss 0.011326666921377182 Accuracy 0.88037109375\n",
      "Iteration 22800 Training loss 0.006571026984602213 Validation loss 0.011340485885739326 Accuracy 0.88037109375\n",
      "Iteration 22810 Training loss 0.006080931052565575 Validation loss 0.011500908061861992 Accuracy 0.87939453125\n",
      "Iteration 22820 Training loss 0.0073065729811787605 Validation loss 0.011315295472741127 Accuracy 0.880859375\n",
      "Iteration 22830 Training loss 0.006641559302806854 Validation loss 0.011746341362595558 Accuracy 0.8759765625\n",
      "Iteration 22840 Training loss 0.004758570343255997 Validation loss 0.011570378206670284 Accuracy 0.87841796875\n",
      "Iteration 22850 Training loss 0.00753673305734992 Validation loss 0.01170413289219141 Accuracy 0.87646484375\n",
      "Iteration 22860 Training loss 0.006437886040657759 Validation loss 0.011352374218404293 Accuracy 0.87939453125\n",
      "Iteration 22870 Training loss 0.006129962392151356 Validation loss 0.011511021293699741 Accuracy 0.87890625\n",
      "Iteration 22880 Training loss 0.005415989551693201 Validation loss 0.011207293719053268 Accuracy 0.8818359375\n",
      "Iteration 22890 Training loss 0.005920459516346455 Validation loss 0.011308925226330757 Accuracy 0.8818359375\n",
      "Iteration 22900 Training loss 0.005421197973191738 Validation loss 0.011631619185209274 Accuracy 0.87744140625\n",
      "Iteration 22910 Training loss 0.004064490087330341 Validation loss 0.01171969249844551 Accuracy 0.87646484375\n",
      "Iteration 22920 Training loss 0.005881635472178459 Validation loss 0.011272462084889412 Accuracy 0.88037109375\n",
      "Iteration 22930 Training loss 0.005036083981394768 Validation loss 0.011475580744445324 Accuracy 0.87939453125\n",
      "Iteration 22940 Training loss 0.005985477473586798 Validation loss 0.011367897503077984 Accuracy 0.87939453125\n",
      "Iteration 22950 Training loss 0.007647308986634016 Validation loss 0.011772875674068928 Accuracy 0.8759765625\n",
      "Iteration 22960 Training loss 0.0063715120777487755 Validation loss 0.011843744665384293 Accuracy 0.87548828125\n",
      "Iteration 22970 Training loss 0.008070160634815693 Validation loss 0.011594901792705059 Accuracy 0.8779296875\n",
      "Iteration 22980 Training loss 0.006006844807416201 Validation loss 0.011520031839609146 Accuracy 0.87744140625\n",
      "Iteration 22990 Training loss 0.008404811844229698 Validation loss 0.011704184114933014 Accuracy 0.8759765625\n",
      "Iteration 23000 Training loss 0.00778901157900691 Validation loss 0.011764715425670147 Accuracy 0.87646484375\n",
      "Iteration 23010 Training loss 0.004564139526337385 Validation loss 0.011437159962952137 Accuracy 0.87939453125\n",
      "Iteration 23020 Training loss 0.006847096607089043 Validation loss 0.011690954677760601 Accuracy 0.87646484375\n",
      "Iteration 23030 Training loss 0.008566939271986485 Validation loss 0.011344840750098228 Accuracy 0.880859375\n",
      "Iteration 23040 Training loss 0.006311634089797735 Validation loss 0.011628687381744385 Accuracy 0.87841796875\n",
      "Iteration 23050 Training loss 0.0059472001157701015 Validation loss 0.01159678678959608 Accuracy 0.87890625\n",
      "Iteration 23060 Training loss 0.0043376656249165535 Validation loss 0.01140008494257927 Accuracy 0.87841796875\n",
      "Iteration 23070 Training loss 0.004712016321718693 Validation loss 0.011600231751799583 Accuracy 0.87548828125\n",
      "Iteration 23080 Training loss 0.006567493546754122 Validation loss 0.011214987374842167 Accuracy 0.8818359375\n",
      "Iteration 23090 Training loss 0.006772542372345924 Validation loss 0.011332379654049873 Accuracy 0.88037109375\n",
      "Iteration 23100 Training loss 0.004667033441364765 Validation loss 0.011153693310916424 Accuracy 0.8818359375\n",
      "Iteration 23110 Training loss 0.007460144814103842 Validation loss 0.011195347644388676 Accuracy 0.8828125\n",
      "Iteration 23120 Training loss 0.006997985299676657 Validation loss 0.011189958080649376 Accuracy 0.88232421875\n",
      "Iteration 23130 Training loss 0.006013388279825449 Validation loss 0.011289757676422596 Accuracy 0.880859375\n",
      "Iteration 23140 Training loss 0.005314499139785767 Validation loss 0.011327018961310387 Accuracy 0.880859375\n",
      "Iteration 23150 Training loss 0.007265964988619089 Validation loss 0.01141269039362669 Accuracy 0.87890625\n",
      "Iteration 23160 Training loss 0.007708193268626928 Validation loss 0.011346716433763504 Accuracy 0.88037109375\n",
      "Iteration 23170 Training loss 0.004403707571327686 Validation loss 0.011246266774833202 Accuracy 0.8818359375\n",
      "Iteration 23180 Training loss 0.006600277964025736 Validation loss 0.01237691380083561 Accuracy 0.86962890625\n",
      "Iteration 23190 Training loss 0.0051328083500266075 Validation loss 0.01166363526135683 Accuracy 0.87646484375\n",
      "Iteration 23200 Training loss 0.00818631425499916 Validation loss 0.012051326222717762 Accuracy 0.87255859375\n",
      "Iteration 23210 Training loss 0.006191923748701811 Validation loss 0.011803208850324154 Accuracy 0.87548828125\n",
      "Iteration 23220 Training loss 0.0060570817440748215 Validation loss 0.011330763809382915 Accuracy 0.88037109375\n",
      "Iteration 23230 Training loss 0.005168805830180645 Validation loss 0.011340558528900146 Accuracy 0.87939453125\n",
      "Iteration 23240 Training loss 0.005250024609267712 Validation loss 0.01154242642223835 Accuracy 0.8779296875\n",
      "Iteration 23250 Training loss 0.005953578744083643 Validation loss 0.011515275575220585 Accuracy 0.87841796875\n",
      "Iteration 23260 Training loss 0.004194584675133228 Validation loss 0.01124765444546938 Accuracy 0.880859375\n",
      "Iteration 23270 Training loss 0.003955502063035965 Validation loss 0.011198215186595917 Accuracy 0.8818359375\n",
      "Iteration 23280 Training loss 0.006012941710650921 Validation loss 0.011666483245790005 Accuracy 0.8759765625\n",
      "Iteration 23290 Training loss 0.0059739635325968266 Validation loss 0.011427517980337143 Accuracy 0.87890625\n",
      "Iteration 23300 Training loss 0.005972014274448156 Validation loss 0.012090911157429218 Accuracy 0.87158203125\n",
      "Iteration 23310 Training loss 0.004844483453780413 Validation loss 0.011553146876394749 Accuracy 0.876953125\n",
      "Iteration 23320 Training loss 0.007928543724119663 Validation loss 0.01144914049655199 Accuracy 0.87890625\n",
      "Iteration 23330 Training loss 0.006347058340907097 Validation loss 0.01122279278934002 Accuracy 0.88134765625\n",
      "Iteration 23340 Training loss 0.003964012023061514 Validation loss 0.011337313801050186 Accuracy 0.87939453125\n",
      "Iteration 23350 Training loss 0.006573000457137823 Validation loss 0.011642209254205227 Accuracy 0.876953125\n",
      "Iteration 23360 Training loss 0.005135662388056517 Validation loss 0.011208991520106792 Accuracy 0.88134765625\n",
      "Iteration 23370 Training loss 0.008536768145859241 Validation loss 0.011725779622793198 Accuracy 0.87548828125\n",
      "Iteration 23380 Training loss 0.007211880758404732 Validation loss 0.01166341919451952 Accuracy 0.876953125\n",
      "Iteration 23390 Training loss 0.0049354941584169865 Validation loss 0.011201185174286366 Accuracy 0.880859375\n",
      "Iteration 23400 Training loss 0.007386564742773771 Validation loss 0.012160449288785458 Accuracy 0.87158203125\n",
      "Iteration 23410 Training loss 0.006168826948851347 Validation loss 0.011344562284648418 Accuracy 0.8798828125\n",
      "Iteration 23420 Training loss 0.004290047101676464 Validation loss 0.011353116482496262 Accuracy 0.87939453125\n",
      "Iteration 23430 Training loss 0.005188977345824242 Validation loss 0.01151090394705534 Accuracy 0.87890625\n",
      "Iteration 23440 Training loss 0.004756886046379805 Validation loss 0.011369041167199612 Accuracy 0.87939453125\n",
      "Iteration 23450 Training loss 0.0058000716380774975 Validation loss 0.011343976482748985 Accuracy 0.880859375\n",
      "Iteration 23460 Training loss 0.005199664272367954 Validation loss 0.011467025615274906 Accuracy 0.8798828125\n",
      "Iteration 23470 Training loss 0.006534601096063852 Validation loss 0.011190755292773247 Accuracy 0.88134765625\n",
      "Iteration 23480 Training loss 0.006754857487976551 Validation loss 0.011778159067034721 Accuracy 0.87548828125\n",
      "Iteration 23490 Training loss 0.005117663647979498 Validation loss 0.011203691363334656 Accuracy 0.8828125\n",
      "Iteration 23500 Training loss 0.0075325132347643375 Validation loss 0.01140100322663784 Accuracy 0.87841796875\n",
      "Iteration 23510 Training loss 0.005691662896424532 Validation loss 0.011482122354209423 Accuracy 0.87841796875\n",
      "Iteration 23520 Training loss 0.0058796461671590805 Validation loss 0.011750828474760056 Accuracy 0.87548828125\n",
      "Iteration 23530 Training loss 0.005703585222363472 Validation loss 0.011203168891370296 Accuracy 0.88134765625\n",
      "Iteration 23540 Training loss 0.007033621426671743 Validation loss 0.011309808120131493 Accuracy 0.88037109375\n",
      "Iteration 23550 Training loss 0.004774114117026329 Validation loss 0.011084910482168198 Accuracy 0.88232421875\n",
      "Iteration 23560 Training loss 0.008030342869460583 Validation loss 0.011959929019212723 Accuracy 0.87451171875\n",
      "Iteration 23570 Training loss 0.005448942072689533 Validation loss 0.011593746952712536 Accuracy 0.8759765625\n",
      "Iteration 23580 Training loss 0.0052994596771895885 Validation loss 0.01115456409752369 Accuracy 0.8818359375\n",
      "Iteration 23590 Training loss 0.004599258303642273 Validation loss 0.011422308161854744 Accuracy 0.87890625\n",
      "Iteration 23600 Training loss 0.007297387346625328 Validation loss 0.012015104293823242 Accuracy 0.873046875\n",
      "Iteration 23610 Training loss 0.007341365329921246 Validation loss 0.01137690432369709 Accuracy 0.87939453125\n",
      "Iteration 23620 Training loss 0.007268806453794241 Validation loss 0.011259239166975021 Accuracy 0.88037109375\n",
      "Iteration 23630 Training loss 0.006423856131732464 Validation loss 0.01135995052754879 Accuracy 0.87939453125\n",
      "Iteration 23640 Training loss 0.004320111125707626 Validation loss 0.011463739909231663 Accuracy 0.87890625\n",
      "Iteration 23650 Training loss 0.00487100426107645 Validation loss 0.011425409466028214 Accuracy 0.8798828125\n",
      "Iteration 23660 Training loss 0.006625285837799311 Validation loss 0.011447597295045853 Accuracy 0.87841796875\n",
      "Iteration 23670 Training loss 0.0062606073915958405 Validation loss 0.011387133039534092 Accuracy 0.87939453125\n",
      "Iteration 23680 Training loss 0.005451090633869171 Validation loss 0.011278693564236164 Accuracy 0.88037109375\n",
      "Iteration 23690 Training loss 0.006203757133334875 Validation loss 0.011793903075158596 Accuracy 0.87548828125\n",
      "Iteration 23700 Training loss 0.005534776020795107 Validation loss 0.011384783312678337 Accuracy 0.8798828125\n",
      "Iteration 23710 Training loss 0.0050302487798035145 Validation loss 0.011410290375351906 Accuracy 0.87939453125\n",
      "Iteration 23720 Training loss 0.005219542887061834 Validation loss 0.011386530473828316 Accuracy 0.8798828125\n",
      "Iteration 23730 Training loss 0.0055654626339674 Validation loss 0.01152213104069233 Accuracy 0.8779296875\n",
      "Iteration 23740 Training loss 0.007366048637777567 Validation loss 0.01175668928772211 Accuracy 0.875\n",
      "Iteration 23750 Training loss 0.006382203195244074 Validation loss 0.011330908164381981 Accuracy 0.8798828125\n",
      "Iteration 23760 Training loss 0.005933166481554508 Validation loss 0.011697525158524513 Accuracy 0.8759765625\n",
      "Iteration 23770 Training loss 0.004572635050863028 Validation loss 0.011859237216413021 Accuracy 0.87451171875\n",
      "Iteration 23780 Training loss 0.0053463527001440525 Validation loss 0.011266598477959633 Accuracy 0.8818359375\n",
      "Iteration 23790 Training loss 0.007053145207464695 Validation loss 0.011848862282931805 Accuracy 0.87451171875\n",
      "Iteration 23800 Training loss 0.004727391991764307 Validation loss 0.011241584084928036 Accuracy 0.88037109375\n",
      "Iteration 23810 Training loss 0.0055258674547076225 Validation loss 0.011493390426039696 Accuracy 0.8779296875\n",
      "Iteration 23820 Training loss 0.003462341846898198 Validation loss 0.011372409760951996 Accuracy 0.880859375\n",
      "Iteration 23830 Training loss 0.006048567593097687 Validation loss 0.011503560468554497 Accuracy 0.87841796875\n",
      "Iteration 23840 Training loss 0.0056546833366155624 Validation loss 0.011398116126656532 Accuracy 0.87939453125\n",
      "Iteration 23850 Training loss 0.006019349209964275 Validation loss 0.011301911436021328 Accuracy 0.880859375\n",
      "Iteration 23860 Training loss 0.004974715411663055 Validation loss 0.011605174280703068 Accuracy 0.8779296875\n",
      "Iteration 23870 Training loss 0.007296114694327116 Validation loss 0.01123534794896841 Accuracy 0.880859375\n",
      "Iteration 23880 Training loss 0.007011507637798786 Validation loss 0.011377601884305477 Accuracy 0.8798828125\n",
      "Iteration 23890 Training loss 0.005189636722207069 Validation loss 0.010970031842589378 Accuracy 0.884765625\n",
      "Iteration 23900 Training loss 0.004924365784972906 Validation loss 0.011244083754718304 Accuracy 0.88134765625\n",
      "Iteration 23910 Training loss 0.004892606753855944 Validation loss 0.01113271713256836 Accuracy 0.88134765625\n",
      "Iteration 23920 Training loss 0.006860149092972279 Validation loss 0.01133290957659483 Accuracy 0.88037109375\n",
      "Iteration 23930 Training loss 0.00637606019154191 Validation loss 0.011268538422882557 Accuracy 0.880859375\n",
      "Iteration 23940 Training loss 0.005752298515290022 Validation loss 0.011028463952243328 Accuracy 0.8828125\n",
      "Iteration 23950 Training loss 0.007160963024944067 Validation loss 0.011098158545792103 Accuracy 0.8828125\n",
      "Iteration 23960 Training loss 0.005478359293192625 Validation loss 0.01132172904908657 Accuracy 0.88037109375\n",
      "Iteration 23970 Training loss 0.006752896588295698 Validation loss 0.011695807799696922 Accuracy 0.87646484375\n",
      "Iteration 23980 Training loss 0.006746791768819094 Validation loss 0.011419896967709064 Accuracy 0.87890625\n",
      "Iteration 23990 Training loss 0.00644800066947937 Validation loss 0.011848402209579945 Accuracy 0.87548828125\n",
      "Iteration 24000 Training loss 0.004600547719746828 Validation loss 0.011591295711696148 Accuracy 0.8779296875\n",
      "Iteration 24010 Training loss 0.005347772501409054 Validation loss 0.011154791340231895 Accuracy 0.8818359375\n",
      "Iteration 24020 Training loss 0.007756727747619152 Validation loss 0.011960526928305626 Accuracy 0.8740234375\n",
      "Iteration 24030 Training loss 0.005816158372908831 Validation loss 0.011452562175691128 Accuracy 0.87890625\n",
      "Iteration 24040 Training loss 0.005318488925695419 Validation loss 0.011430365964770317 Accuracy 0.87939453125\n",
      "Iteration 24050 Training loss 0.006230452563613653 Validation loss 0.011641977354884148 Accuracy 0.876953125\n",
      "Iteration 24060 Training loss 0.00643776124343276 Validation loss 0.01125511433929205 Accuracy 0.88037109375\n",
      "Iteration 24070 Training loss 0.004783631768077612 Validation loss 0.011243323795497417 Accuracy 0.880859375\n",
      "Iteration 24080 Training loss 0.005814108531922102 Validation loss 0.011286857537925243 Accuracy 0.87939453125\n",
      "Iteration 24090 Training loss 0.0073339669033885 Validation loss 0.011347121559083462 Accuracy 0.8798828125\n",
      "Iteration 24100 Training loss 0.005229657981544733 Validation loss 0.011327543295919895 Accuracy 0.8798828125\n",
      "Iteration 24110 Training loss 0.006173672154545784 Validation loss 0.011108212172985077 Accuracy 0.88330078125\n",
      "Iteration 24120 Training loss 0.0052602761425077915 Validation loss 0.010860887356102467 Accuracy 0.88623046875\n",
      "Iteration 24130 Training loss 0.004802832845598459 Validation loss 0.011321193538606167 Accuracy 0.88037109375\n",
      "Iteration 24140 Training loss 0.006070921663194895 Validation loss 0.011300421319901943 Accuracy 0.88037109375\n",
      "Iteration 24150 Training loss 0.00474745174869895 Validation loss 0.011455628089606762 Accuracy 0.87890625\n",
      "Iteration 24160 Training loss 0.005945087876170874 Validation loss 0.011613612063229084 Accuracy 0.8759765625\n",
      "Iteration 24170 Training loss 0.005535897333174944 Validation loss 0.011692618019878864 Accuracy 0.876953125\n",
      "Iteration 24180 Training loss 0.006502246949821711 Validation loss 0.0110784862190485 Accuracy 0.8828125\n",
      "Iteration 24190 Training loss 0.005791733507066965 Validation loss 0.01093988586217165 Accuracy 0.8837890625\n",
      "Iteration 24200 Training loss 0.003597254166379571 Validation loss 0.011180775240063667 Accuracy 0.880859375\n",
      "Iteration 24210 Training loss 0.006683776620775461 Validation loss 0.011190441437065601 Accuracy 0.88134765625\n",
      "Iteration 24220 Training loss 0.0065296851098537445 Validation loss 0.011382775381207466 Accuracy 0.8798828125\n",
      "Iteration 24230 Training loss 0.005770583637058735 Validation loss 0.010939422063529491 Accuracy 0.8837890625\n",
      "Iteration 24240 Training loss 0.00565903028473258 Validation loss 0.011198075488209724 Accuracy 0.88134765625\n",
      "Iteration 24250 Training loss 0.004757055547088385 Validation loss 0.011925515718758106 Accuracy 0.8740234375\n",
      "Iteration 24260 Training loss 0.0059519256465137005 Validation loss 0.011331713758409023 Accuracy 0.88037109375\n",
      "Iteration 24270 Training loss 0.0052511016838252544 Validation loss 0.01155292708426714 Accuracy 0.8779296875\n",
      "Iteration 24280 Training loss 0.005878634750843048 Validation loss 0.011467362754046917 Accuracy 0.87841796875\n",
      "Iteration 24290 Training loss 0.006556034553796053 Validation loss 0.011107392609119415 Accuracy 0.8828125\n",
      "Iteration 24300 Training loss 0.006433095782995224 Validation loss 0.011909694410860538 Accuracy 0.8740234375\n",
      "Iteration 24310 Training loss 0.004876669030636549 Validation loss 0.011190858669579029 Accuracy 0.88232421875\n",
      "Iteration 24320 Training loss 0.006349456496536732 Validation loss 0.011711006984114647 Accuracy 0.87744140625\n",
      "Iteration 24330 Training loss 0.006381376646459103 Validation loss 0.011255266144871712 Accuracy 0.880859375\n",
      "Iteration 24340 Training loss 0.0050158314406871796 Validation loss 0.011337419040501118 Accuracy 0.88037109375\n",
      "Iteration 24350 Training loss 0.006359962280839682 Validation loss 0.011160004884004593 Accuracy 0.8818359375\n",
      "Iteration 24360 Training loss 0.006078408565372229 Validation loss 0.01124454103410244 Accuracy 0.8818359375\n",
      "Iteration 24370 Training loss 0.005143311340361834 Validation loss 0.011175860650837421 Accuracy 0.8818359375\n",
      "Iteration 24380 Training loss 0.004692140966653824 Validation loss 0.010857343673706055 Accuracy 0.88623046875\n",
      "Iteration 24390 Training loss 0.005107086151838303 Validation loss 0.010933826677501202 Accuracy 0.884765625\n",
      "Iteration 24400 Training loss 0.0071876090951263905 Validation loss 0.011418454349040985 Accuracy 0.88037109375\n",
      "Iteration 24410 Training loss 0.005629583727568388 Validation loss 0.01110456045717001 Accuracy 0.880859375\n",
      "Iteration 24420 Training loss 0.0069784140214324 Validation loss 0.011559340171515942 Accuracy 0.8779296875\n",
      "Iteration 24430 Training loss 0.005936130881309509 Validation loss 0.011250772513449192 Accuracy 0.880859375\n",
      "Iteration 24440 Training loss 0.005385276861488819 Validation loss 0.011202231980860233 Accuracy 0.880859375\n",
      "Iteration 24450 Training loss 0.0034586472902446985 Validation loss 0.011250670999288559 Accuracy 0.8818359375\n",
      "Iteration 24460 Training loss 0.006620398722589016 Validation loss 0.011058148927986622 Accuracy 0.8828125\n",
      "Iteration 24470 Training loss 0.006630444899201393 Validation loss 0.011320756748318672 Accuracy 0.88037109375\n",
      "Iteration 24480 Training loss 0.005375719629228115 Validation loss 0.011251899413764477 Accuracy 0.880859375\n",
      "Iteration 24490 Training loss 0.006084328982979059 Validation loss 0.011305754072964191 Accuracy 0.88037109375\n",
      "Iteration 24500 Training loss 0.005001165438443422 Validation loss 0.011503363959491253 Accuracy 0.8779296875\n",
      "Iteration 24510 Training loss 0.004705867730081081 Validation loss 0.011132074519991875 Accuracy 0.8818359375\n",
      "Iteration 24520 Training loss 0.004893324337899685 Validation loss 0.010910222306847572 Accuracy 0.88427734375\n",
      "Iteration 24530 Training loss 0.006098881829530001 Validation loss 0.011230863630771637 Accuracy 0.880859375\n",
      "Iteration 24540 Training loss 0.007306706625968218 Validation loss 0.01096031442284584 Accuracy 0.8837890625\n",
      "Iteration 24550 Training loss 0.007234023418277502 Validation loss 0.010881594382226467 Accuracy 0.8837890625\n",
      "Iteration 24560 Training loss 0.00581017741933465 Validation loss 0.011154109612107277 Accuracy 0.880859375\n",
      "Iteration 24570 Training loss 0.00519912876188755 Validation loss 0.011261220090091228 Accuracy 0.88037109375\n",
      "Iteration 24580 Training loss 0.006934393662959337 Validation loss 0.011216940358281136 Accuracy 0.880859375\n",
      "Iteration 24590 Training loss 0.007538145408034325 Validation loss 0.011973625048995018 Accuracy 0.87255859375\n",
      "Iteration 24600 Training loss 0.005467874929308891 Validation loss 0.011123981326818466 Accuracy 0.8818359375\n",
      "Iteration 24610 Training loss 0.007115209475159645 Validation loss 0.011251840740442276 Accuracy 0.880859375\n",
      "Iteration 24620 Training loss 0.004677434451878071 Validation loss 0.011349259875714779 Accuracy 0.87890625\n",
      "Iteration 24630 Training loss 0.006456897594034672 Validation loss 0.01152073498815298 Accuracy 0.87890625\n",
      "Iteration 24640 Training loss 0.005051294341683388 Validation loss 0.011365729384124279 Accuracy 0.87939453125\n",
      "Iteration 24650 Training loss 0.005621397402137518 Validation loss 0.011439790017902851 Accuracy 0.87890625\n",
      "Iteration 24660 Training loss 0.00570332258939743 Validation loss 0.011264672502875328 Accuracy 0.880859375\n",
      "Iteration 24670 Training loss 0.004387849010527134 Validation loss 0.011243246495723724 Accuracy 0.88134765625\n",
      "Iteration 24680 Training loss 0.004713596310466528 Validation loss 0.011182179674506187 Accuracy 0.8818359375\n",
      "Iteration 24690 Training loss 0.0059768143109977245 Validation loss 0.010827950201928616 Accuracy 0.88525390625\n",
      "Iteration 24700 Training loss 0.006374781019985676 Validation loss 0.011497315019369125 Accuracy 0.87890625\n",
      "Iteration 24710 Training loss 0.005403994116932154 Validation loss 0.010956876911222935 Accuracy 0.884765625\n",
      "Iteration 24720 Training loss 0.0069570583291351795 Validation loss 0.01170971617102623 Accuracy 0.8759765625\n",
      "Iteration 24730 Training loss 0.00513302581384778 Validation loss 0.011206962168216705 Accuracy 0.88134765625\n",
      "Iteration 24740 Training loss 0.006398025434464216 Validation loss 0.011233515106141567 Accuracy 0.88134765625\n",
      "Iteration 24750 Training loss 0.00634120823815465 Validation loss 0.011496801860630512 Accuracy 0.87841796875\n",
      "Iteration 24760 Training loss 0.0066696940921247005 Validation loss 0.011293971911072731 Accuracy 0.88037109375\n",
      "Iteration 24770 Training loss 0.0068624489940702915 Validation loss 0.01143475342541933 Accuracy 0.87890625\n",
      "Iteration 24780 Training loss 0.004690384026616812 Validation loss 0.011169237084686756 Accuracy 0.8818359375\n",
      "Iteration 24790 Training loss 0.004987700842320919 Validation loss 0.011510219424962997 Accuracy 0.8779296875\n",
      "Iteration 24800 Training loss 0.006561496760696173 Validation loss 0.011158288456499577 Accuracy 0.880859375\n",
      "Iteration 24810 Training loss 0.004457561299204826 Validation loss 0.011249467730522156 Accuracy 0.8798828125\n",
      "Iteration 24820 Training loss 0.004281299654394388 Validation loss 0.011469386518001556 Accuracy 0.87744140625\n",
      "Iteration 24830 Training loss 0.005192657001316547 Validation loss 0.011403136886656284 Accuracy 0.87939453125\n",
      "Iteration 24840 Training loss 0.0049584731459617615 Validation loss 0.011020155623555183 Accuracy 0.88330078125\n",
      "Iteration 24850 Training loss 0.007756985258311033 Validation loss 0.01181689091026783 Accuracy 0.87451171875\n",
      "Iteration 24860 Training loss 0.0060426462441682816 Validation loss 0.011345912702381611 Accuracy 0.87939453125\n",
      "Iteration 24870 Training loss 0.0065607032738626 Validation loss 0.011302878148853779 Accuracy 0.8798828125\n",
      "Iteration 24880 Training loss 0.006784564815461636 Validation loss 0.011342334561049938 Accuracy 0.8798828125\n",
      "Iteration 24890 Training loss 0.0064873467199504375 Validation loss 0.012021845206618309 Accuracy 0.87255859375\n",
      "Iteration 24900 Training loss 0.00394819863140583 Validation loss 0.01116734929382801 Accuracy 0.88134765625\n",
      "Iteration 24910 Training loss 0.006282932125031948 Validation loss 0.011067809537053108 Accuracy 0.88330078125\n",
      "Iteration 24920 Training loss 0.005817696917802095 Validation loss 0.01136422622948885 Accuracy 0.87890625\n",
      "Iteration 24930 Training loss 0.005348519422113895 Validation loss 0.011214353144168854 Accuracy 0.88134765625\n",
      "Iteration 24940 Training loss 0.006258323788642883 Validation loss 0.011654943227767944 Accuracy 0.87646484375\n",
      "Iteration 24950 Training loss 0.005790241993963718 Validation loss 0.01099538803100586 Accuracy 0.88232421875\n",
      "Iteration 24960 Training loss 0.0064511289820075035 Validation loss 0.010946615599095821 Accuracy 0.884765625\n",
      "Iteration 24970 Training loss 0.005893938709050417 Validation loss 0.011129393242299557 Accuracy 0.8818359375\n",
      "Iteration 24980 Training loss 0.005687374621629715 Validation loss 0.011199058964848518 Accuracy 0.88134765625\n",
      "Iteration 24990 Training loss 0.007528623566031456 Validation loss 0.011249669827520847 Accuracy 0.880859375\n",
      "Iteration 25000 Training loss 0.006372474599629641 Validation loss 0.011518171988427639 Accuracy 0.87744140625\n",
      "Iteration 25010 Training loss 0.005812598392367363 Validation loss 0.010896340012550354 Accuracy 0.8837890625\n",
      "Iteration 25020 Training loss 0.005201444029808044 Validation loss 0.011020154692232609 Accuracy 0.8828125\n",
      "Iteration 25030 Training loss 0.005161607638001442 Validation loss 0.011217092163860798 Accuracy 0.8798828125\n",
      "Iteration 25040 Training loss 0.00522985216230154 Validation loss 0.01132748182862997 Accuracy 0.8798828125\n",
      "Iteration 25050 Training loss 0.005143207497894764 Validation loss 0.010864474810659885 Accuracy 0.88427734375\n",
      "Iteration 25060 Training loss 0.004263146780431271 Validation loss 0.01152748428285122 Accuracy 0.87744140625\n",
      "Iteration 25070 Training loss 0.005575637798756361 Validation loss 0.011916354298591614 Accuracy 0.8740234375\n",
      "Iteration 25080 Training loss 0.005352507345378399 Validation loss 0.011151948943734169 Accuracy 0.8818359375\n",
      "Iteration 25090 Training loss 0.007237199228256941 Validation loss 0.011078915558755398 Accuracy 0.88232421875\n",
      "Iteration 25100 Training loss 0.006141869816929102 Validation loss 0.011096864007413387 Accuracy 0.88330078125\n",
      "Iteration 25110 Training loss 0.005540969781577587 Validation loss 0.01104117650538683 Accuracy 0.88330078125\n",
      "Iteration 25120 Training loss 0.005363295786082745 Validation loss 0.010961120016872883 Accuracy 0.88330078125\n",
      "Iteration 25130 Training loss 0.0031245702411979437 Validation loss 0.011252478696405888 Accuracy 0.880859375\n",
      "Iteration 25140 Training loss 0.00584949366748333 Validation loss 0.011077575385570526 Accuracy 0.8818359375\n",
      "Iteration 25150 Training loss 0.005403559189289808 Validation loss 0.010997326113283634 Accuracy 0.8837890625\n",
      "Iteration 25160 Training loss 0.005666601937264204 Validation loss 0.0118342200294137 Accuracy 0.87451171875\n",
      "Iteration 25170 Training loss 0.005334780085831881 Validation loss 0.011360883712768555 Accuracy 0.8798828125\n",
      "Iteration 25180 Training loss 0.005386426113545895 Validation loss 0.011340227909386158 Accuracy 0.87939453125\n",
      "Iteration 25190 Training loss 0.0068260664120316505 Validation loss 0.011340086348354816 Accuracy 0.8798828125\n",
      "Iteration 25200 Training loss 0.005912848748266697 Validation loss 0.011365678161382675 Accuracy 0.87939453125\n",
      "Iteration 25210 Training loss 0.004970676265656948 Validation loss 0.01136048510670662 Accuracy 0.87841796875\n",
      "Iteration 25220 Training loss 0.005916281137615442 Validation loss 0.011178278364241123 Accuracy 0.8818359375\n",
      "Iteration 25230 Training loss 0.005204773973673582 Validation loss 0.012113810516893864 Accuracy 0.8720703125\n",
      "Iteration 25240 Training loss 0.004768876358866692 Validation loss 0.01114142406731844 Accuracy 0.88330078125\n",
      "Iteration 25250 Training loss 0.007157471030950546 Validation loss 0.011250726878643036 Accuracy 0.8818359375\n",
      "Iteration 25260 Training loss 0.004990353714674711 Validation loss 0.011426370590925217 Accuracy 0.87939453125\n",
      "Iteration 25270 Training loss 0.006404608488082886 Validation loss 0.011563141830265522 Accuracy 0.8779296875\n",
      "Iteration 25280 Training loss 0.004443135112524033 Validation loss 0.011299023404717445 Accuracy 0.88134765625\n",
      "Iteration 25290 Training loss 0.006757747381925583 Validation loss 0.011300874873995781 Accuracy 0.88037109375\n",
      "Iteration 25300 Training loss 0.004425133112818003 Validation loss 0.011265002191066742 Accuracy 0.88037109375\n",
      "Iteration 25310 Training loss 0.0048441109247505665 Validation loss 0.011090031825006008 Accuracy 0.8828125\n",
      "Iteration 25320 Training loss 0.007293534930795431 Validation loss 0.010879768058657646 Accuracy 0.88525390625\n",
      "Iteration 25330 Training loss 0.00572267547249794 Validation loss 0.011220204643905163 Accuracy 0.8818359375\n",
      "Iteration 25340 Training loss 0.004110709298402071 Validation loss 0.011122814379632473 Accuracy 0.8828125\n",
      "Iteration 25350 Training loss 0.004745895508676767 Validation loss 0.011193771846592426 Accuracy 0.8818359375\n",
      "Iteration 25360 Training loss 0.004625602625310421 Validation loss 0.010953725315630436 Accuracy 0.88427734375\n",
      "Iteration 25370 Training loss 0.005491004791110754 Validation loss 0.011294455267488956 Accuracy 0.880859375\n",
      "Iteration 25380 Training loss 0.005148274824023247 Validation loss 0.01135250460356474 Accuracy 0.8798828125\n",
      "Iteration 25390 Training loss 0.006250485777854919 Validation loss 0.011068466119468212 Accuracy 0.8828125\n",
      "Iteration 25400 Training loss 0.005206177476793528 Validation loss 0.011273227632045746 Accuracy 0.880859375\n",
      "Iteration 25410 Training loss 0.006654779426753521 Validation loss 0.01134247612208128 Accuracy 0.88037109375\n",
      "Iteration 25420 Training loss 0.005071216728538275 Validation loss 0.011544787324965 Accuracy 0.87744140625\n",
      "Iteration 25430 Training loss 0.004198166541755199 Validation loss 0.01134578324854374 Accuracy 0.87939453125\n",
      "Iteration 25440 Training loss 0.004843488801270723 Validation loss 0.011750133708119392 Accuracy 0.87451171875\n",
      "Iteration 25450 Training loss 0.0077122291550040245 Validation loss 0.011586007662117481 Accuracy 0.876953125\n",
      "Iteration 25460 Training loss 0.004576416220515966 Validation loss 0.011207750998437405 Accuracy 0.880859375\n",
      "Iteration 25470 Training loss 0.004688891116529703 Validation loss 0.011004908010363579 Accuracy 0.8828125\n",
      "Iteration 25480 Training loss 0.0047404649667441845 Validation loss 0.010985798202455044 Accuracy 0.8828125\n",
      "Iteration 25490 Training loss 0.005313475616276264 Validation loss 0.01116293203085661 Accuracy 0.88037109375\n",
      "Iteration 25500 Training loss 0.007574823219329119 Validation loss 0.011430444195866585 Accuracy 0.8798828125\n",
      "Iteration 25510 Training loss 0.005887867417186499 Validation loss 0.011178957298398018 Accuracy 0.8818359375\n",
      "Iteration 25520 Training loss 0.005171956494450569 Validation loss 0.010913505218923092 Accuracy 0.88427734375\n",
      "Iteration 25530 Training loss 0.005009529180824757 Validation loss 0.011064340360462666 Accuracy 0.88232421875\n",
      "Iteration 25540 Training loss 0.006971403490751982 Validation loss 0.01147589460015297 Accuracy 0.8779296875\n",
      "Iteration 25550 Training loss 0.005972057115286589 Validation loss 0.011288043111562729 Accuracy 0.8798828125\n",
      "Iteration 25560 Training loss 0.005165891721844673 Validation loss 0.011239930987358093 Accuracy 0.88134765625\n",
      "Iteration 25570 Training loss 0.006078050471842289 Validation loss 0.01115898322314024 Accuracy 0.88134765625\n",
      "Iteration 25580 Training loss 0.005242117680609226 Validation loss 0.01095866784453392 Accuracy 0.8837890625\n",
      "Iteration 25590 Training loss 0.006490897387266159 Validation loss 0.011137858964502811 Accuracy 0.8818359375\n",
      "Iteration 25600 Training loss 0.004758454393595457 Validation loss 0.011024235747754574 Accuracy 0.8818359375\n",
      "Iteration 25610 Training loss 0.006550464779138565 Validation loss 0.011048915795981884 Accuracy 0.88232421875\n",
      "Iteration 25620 Training loss 0.005693239625543356 Validation loss 0.011000175960361958 Accuracy 0.88330078125\n",
      "Iteration 25630 Training loss 0.006360976956784725 Validation loss 0.010835091583430767 Accuracy 0.88427734375\n",
      "Iteration 25640 Training loss 0.005718565545976162 Validation loss 0.011215523816645145 Accuracy 0.8798828125\n",
      "Iteration 25650 Training loss 0.004883980844169855 Validation loss 0.011080966331064701 Accuracy 0.8818359375\n",
      "Iteration 25660 Training loss 0.006239467766135931 Validation loss 0.011225695721805096 Accuracy 0.88037109375\n",
      "Iteration 25670 Training loss 0.005310805048793554 Validation loss 0.010907917283475399 Accuracy 0.8837890625\n",
      "Iteration 25680 Training loss 0.0059577603824436665 Validation loss 0.011557218618690968 Accuracy 0.876953125\n",
      "Iteration 25690 Training loss 0.005767494905740023 Validation loss 0.011411735787987709 Accuracy 0.87841796875\n",
      "Iteration 25700 Training loss 0.008686322718858719 Validation loss 0.011248796246945858 Accuracy 0.880859375\n",
      "Iteration 25710 Training loss 0.0072362045757472515 Validation loss 0.011104545556008816 Accuracy 0.8828125\n",
      "Iteration 25720 Training loss 0.004662193823605776 Validation loss 0.011301998980343342 Accuracy 0.8798828125\n",
      "Iteration 25730 Training loss 0.006207944359630346 Validation loss 0.012193085625767708 Accuracy 0.8701171875\n",
      "Iteration 25740 Training loss 0.005679024383425713 Validation loss 0.011097900569438934 Accuracy 0.8818359375\n",
      "Iteration 25750 Training loss 0.004614075180143118 Validation loss 0.011132831685245037 Accuracy 0.88232421875\n",
      "Iteration 25760 Training loss 0.005683566443622112 Validation loss 0.01104439701884985 Accuracy 0.88330078125\n",
      "Iteration 25770 Training loss 0.007741156034171581 Validation loss 0.011660926043987274 Accuracy 0.876953125\n",
      "Iteration 25780 Training loss 0.005223811138421297 Validation loss 0.01092491764575243 Accuracy 0.88330078125\n",
      "Iteration 25790 Training loss 0.006336522288620472 Validation loss 0.011233210563659668 Accuracy 0.880859375\n",
      "Iteration 25800 Training loss 0.005423230584710836 Validation loss 0.011585223488509655 Accuracy 0.87890625\n",
      "Iteration 25810 Training loss 0.0053069400601089 Validation loss 0.01092914491891861 Accuracy 0.88427734375\n",
      "Iteration 25820 Training loss 0.00593979237601161 Validation loss 0.011164813302457333 Accuracy 0.88134765625\n",
      "Iteration 25830 Training loss 0.006608627270907164 Validation loss 0.011214611120522022 Accuracy 0.88037109375\n",
      "Iteration 25840 Training loss 0.005395283456891775 Validation loss 0.011462738737463951 Accuracy 0.8779296875\n",
      "Iteration 25850 Training loss 0.00550390500575304 Validation loss 0.011051804758608341 Accuracy 0.8818359375\n",
      "Iteration 25860 Training loss 0.0049636103212833405 Validation loss 0.011079523712396622 Accuracy 0.88330078125\n",
      "Iteration 25870 Training loss 0.00479381438344717 Validation loss 0.010934868827462196 Accuracy 0.8828125\n",
      "Iteration 25880 Training loss 0.005202895496040583 Validation loss 0.010883975774049759 Accuracy 0.884765625\n",
      "Iteration 25890 Training loss 0.0049512977711856365 Validation loss 0.010916030965745449 Accuracy 0.88330078125\n",
      "Iteration 25900 Training loss 0.005549461115151644 Validation loss 0.010928820818662643 Accuracy 0.8837890625\n",
      "Iteration 25910 Training loss 0.0046484763734042645 Validation loss 0.010838341899216175 Accuracy 0.88525390625\n",
      "Iteration 25920 Training loss 0.004641344770789146 Validation loss 0.01102191861718893 Accuracy 0.88330078125\n",
      "Iteration 25930 Training loss 0.002886800328269601 Validation loss 0.010960434563457966 Accuracy 0.8837890625\n",
      "Iteration 25940 Training loss 0.004673364106565714 Validation loss 0.011259675025939941 Accuracy 0.8798828125\n",
      "Iteration 25950 Training loss 0.004886422771960497 Validation loss 0.01127680018544197 Accuracy 0.87890625\n",
      "Iteration 25960 Training loss 0.006545765325427055 Validation loss 0.011181039735674858 Accuracy 0.88037109375\n",
      "Iteration 25970 Training loss 0.005239682272076607 Validation loss 0.01093112863600254 Accuracy 0.8837890625\n",
      "Iteration 25980 Training loss 0.004520911257714033 Validation loss 0.010932750068604946 Accuracy 0.8837890625\n",
      "Iteration 25990 Training loss 0.004779407288879156 Validation loss 0.011195268481969833 Accuracy 0.88134765625\n",
      "Iteration 26000 Training loss 0.005349168088287115 Validation loss 0.01106173824518919 Accuracy 0.88232421875\n",
      "Iteration 26010 Training loss 0.006670372094959021 Validation loss 0.012107728980481625 Accuracy 0.87255859375\n",
      "Iteration 26020 Training loss 0.005841778591275215 Validation loss 0.011413670144975185 Accuracy 0.87841796875\n",
      "Iteration 26030 Training loss 0.004814543295651674 Validation loss 0.011244622059166431 Accuracy 0.88037109375\n",
      "Iteration 26040 Training loss 0.0056967418640851974 Validation loss 0.011079628951847553 Accuracy 0.8818359375\n",
      "Iteration 26050 Training loss 0.0052225785329937935 Validation loss 0.011811915785074234 Accuracy 0.87451171875\n",
      "Iteration 26060 Training loss 0.0051148422062397 Validation loss 0.011514213867485523 Accuracy 0.87841796875\n",
      "Iteration 26070 Training loss 0.004699349869042635 Validation loss 0.010955854319036007 Accuracy 0.88330078125\n",
      "Iteration 26080 Training loss 0.006000572349876165 Validation loss 0.01122818049043417 Accuracy 0.880859375\n",
      "Iteration 26090 Training loss 0.005173432175070047 Validation loss 0.011257362551987171 Accuracy 0.880859375\n",
      "Iteration 26100 Training loss 0.004693891387432814 Validation loss 0.011377746239304543 Accuracy 0.87939453125\n",
      "Iteration 26110 Training loss 0.004803462885320187 Validation loss 0.011311247013509274 Accuracy 0.87890625\n",
      "Iteration 26120 Training loss 0.0056985169649124146 Validation loss 0.01113925501704216 Accuracy 0.88232421875\n",
      "Iteration 26130 Training loss 0.003786541521549225 Validation loss 0.011175709776580334 Accuracy 0.88134765625\n",
      "Iteration 26140 Training loss 0.005770216695964336 Validation loss 0.011321243830025196 Accuracy 0.8798828125\n",
      "Iteration 26150 Training loss 0.006366393994539976 Validation loss 0.011412855237722397 Accuracy 0.87939453125\n",
      "Iteration 26160 Training loss 0.006235078908503056 Validation loss 0.011517436243593693 Accuracy 0.87744140625\n",
      "Iteration 26170 Training loss 0.005567519925534725 Validation loss 0.011060371063649654 Accuracy 0.8828125\n",
      "Iteration 26180 Training loss 0.00487801618874073 Validation loss 0.010997067205607891 Accuracy 0.88232421875\n",
      "Iteration 26190 Training loss 0.007212381809949875 Validation loss 0.011869979090988636 Accuracy 0.87255859375\n",
      "Iteration 26200 Training loss 0.005756725091487169 Validation loss 0.011708999052643776 Accuracy 0.875\n",
      "Iteration 26210 Training loss 0.0043949284590780735 Validation loss 0.010839594528079033 Accuracy 0.884765625\n",
      "Iteration 26220 Training loss 0.0035611381754279137 Validation loss 0.01086411066353321 Accuracy 0.8837890625\n",
      "Iteration 26230 Training loss 0.004852428566664457 Validation loss 0.011061306111514568 Accuracy 0.88330078125\n",
      "Iteration 26240 Training loss 0.005589671898633242 Validation loss 0.011142644099891186 Accuracy 0.880859375\n",
      "Iteration 26250 Training loss 0.005856398027390242 Validation loss 0.010986877605319023 Accuracy 0.8837890625\n",
      "Iteration 26260 Training loss 0.004433934576809406 Validation loss 0.011140938848257065 Accuracy 0.880859375\n",
      "Iteration 26270 Training loss 0.006011616438627243 Validation loss 0.010876638814806938 Accuracy 0.88427734375\n",
      "Iteration 26280 Training loss 0.0053406828083097935 Validation loss 0.011171817779541016 Accuracy 0.88134765625\n",
      "Iteration 26290 Training loss 0.003331353422254324 Validation loss 0.011308128014206886 Accuracy 0.87939453125\n",
      "Iteration 26300 Training loss 0.008427945896983147 Validation loss 0.011318236589431763 Accuracy 0.8798828125\n",
      "Iteration 26310 Training loss 0.007620662450790405 Validation loss 0.011141443625092506 Accuracy 0.8818359375\n",
      "Iteration 26320 Training loss 0.006797126494348049 Validation loss 0.011182731948792934 Accuracy 0.88037109375\n",
      "Iteration 26330 Training loss 0.0061798300594091415 Validation loss 0.011286723427474499 Accuracy 0.8798828125\n",
      "Iteration 26340 Training loss 0.0050215995870530605 Validation loss 0.011398936621844769 Accuracy 0.87744140625\n",
      "Iteration 26350 Training loss 0.005323693621903658 Validation loss 0.011195468716323376 Accuracy 0.8798828125\n",
      "Iteration 26360 Training loss 0.004961507394909859 Validation loss 0.011187968775629997 Accuracy 0.880859375\n",
      "Iteration 26370 Training loss 0.004943726118654013 Validation loss 0.011540137231349945 Accuracy 0.876953125\n",
      "Iteration 26380 Training loss 0.006112988572567701 Validation loss 0.011080997996032238 Accuracy 0.88232421875\n",
      "Iteration 26390 Training loss 0.005097391549497843 Validation loss 0.011430683545768261 Accuracy 0.87841796875\n",
      "Iteration 26400 Training loss 0.007065360434353352 Validation loss 0.011772455647587776 Accuracy 0.87548828125\n",
      "Iteration 26410 Training loss 0.007034109905362129 Validation loss 0.01185954175889492 Accuracy 0.87353515625\n",
      "Iteration 26420 Training loss 0.004274987615644932 Validation loss 0.011066201142966747 Accuracy 0.88330078125\n",
      "Iteration 26430 Training loss 0.004919034894555807 Validation loss 0.011898425407707691 Accuracy 0.87353515625\n",
      "Iteration 26440 Training loss 0.00426126504316926 Validation loss 0.011066913604736328 Accuracy 0.8828125\n",
      "Iteration 26450 Training loss 0.0050368355587124825 Validation loss 0.010944782756268978 Accuracy 0.8837890625\n",
      "Iteration 26460 Training loss 0.003908409271389246 Validation loss 0.011206689290702343 Accuracy 0.880859375\n",
      "Iteration 26470 Training loss 0.006517508532851934 Validation loss 0.011129946447908878 Accuracy 0.8818359375\n",
      "Iteration 26480 Training loss 0.005975072272121906 Validation loss 0.011092898435890675 Accuracy 0.88134765625\n",
      "Iteration 26490 Training loss 0.007658636197447777 Validation loss 0.01162275392562151 Accuracy 0.875\n",
      "Iteration 26500 Training loss 0.005411970894783735 Validation loss 0.010874217376112938 Accuracy 0.88525390625\n",
      "Iteration 26510 Training loss 0.006285542622208595 Validation loss 0.011132277548313141 Accuracy 0.88134765625\n",
      "Iteration 26520 Training loss 0.006442971061915159 Validation loss 0.011190295219421387 Accuracy 0.88037109375\n",
      "Iteration 26530 Training loss 0.005426185671240091 Validation loss 0.011123810894787312 Accuracy 0.880859375\n",
      "Iteration 26540 Training loss 0.005539478734135628 Validation loss 0.011117157526314259 Accuracy 0.88037109375\n",
      "Iteration 26550 Training loss 0.006138913333415985 Validation loss 0.011281827464699745 Accuracy 0.87939453125\n",
      "Iteration 26560 Training loss 0.004705038852989674 Validation loss 0.011069275438785553 Accuracy 0.88330078125\n",
      "Iteration 26570 Training loss 0.0055884006433188915 Validation loss 0.011570102535188198 Accuracy 0.8779296875\n",
      "Iteration 26580 Training loss 0.004799301270395517 Validation loss 0.011275463737547398 Accuracy 0.880859375\n",
      "Iteration 26590 Training loss 0.005234428681433201 Validation loss 0.011014132760465145 Accuracy 0.88330078125\n",
      "Iteration 26600 Training loss 0.004423188976943493 Validation loss 0.011148888617753983 Accuracy 0.8818359375\n",
      "Iteration 26610 Training loss 0.0053662522695958614 Validation loss 0.011947043240070343 Accuracy 0.87353515625\n",
      "Iteration 26620 Training loss 0.005244264379143715 Validation loss 0.01114291325211525 Accuracy 0.88134765625\n",
      "Iteration 26630 Training loss 0.005721372086554766 Validation loss 0.011548295617103577 Accuracy 0.87744140625\n",
      "Iteration 26640 Training loss 0.006223181262612343 Validation loss 0.010978435166180134 Accuracy 0.88330078125\n",
      "Iteration 26650 Training loss 0.005184420850127935 Validation loss 0.011186005547642708 Accuracy 0.880859375\n",
      "Iteration 26660 Training loss 0.004155660048127174 Validation loss 0.011230690404772758 Accuracy 0.880859375\n",
      "Iteration 26670 Training loss 0.004797295201569796 Validation loss 0.011290272697806358 Accuracy 0.87939453125\n",
      "Iteration 26680 Training loss 0.004770590458065271 Validation loss 0.011385010555386543 Accuracy 0.87841796875\n",
      "Iteration 26690 Training loss 0.005952535662800074 Validation loss 0.01135337632149458 Accuracy 0.87939453125\n",
      "Iteration 26700 Training loss 0.005431478377431631 Validation loss 0.011691935360431671 Accuracy 0.8759765625\n",
      "Iteration 26710 Training loss 0.003603110322728753 Validation loss 0.011023161001503468 Accuracy 0.88330078125\n",
      "Iteration 26720 Training loss 0.006787446793168783 Validation loss 0.011798310093581676 Accuracy 0.87451171875\n",
      "Iteration 26730 Training loss 0.007615386042743921 Validation loss 0.011051176115870476 Accuracy 0.8828125\n",
      "Iteration 26740 Training loss 0.004404344130307436 Validation loss 0.011212228797376156 Accuracy 0.88037109375\n",
      "Iteration 26750 Training loss 0.004830149933695793 Validation loss 0.01119996514171362 Accuracy 0.88134765625\n",
      "Iteration 26760 Training loss 0.0059267678298056126 Validation loss 0.010880974121391773 Accuracy 0.8837890625\n",
      "Iteration 26770 Training loss 0.004239608068019152 Validation loss 0.011011655442416668 Accuracy 0.88330078125\n",
      "Iteration 26780 Training loss 0.006265608593821526 Validation loss 0.011213457211852074 Accuracy 0.87939453125\n",
      "Iteration 26790 Training loss 0.004752474371343851 Validation loss 0.010767311789095402 Accuracy 0.8857421875\n",
      "Iteration 26800 Training loss 0.004697691183537245 Validation loss 0.011776800267398357 Accuracy 0.87548828125\n",
      "Iteration 26810 Training loss 0.004457315895706415 Validation loss 0.010918735526502132 Accuracy 0.88427734375\n",
      "Iteration 26820 Training loss 0.006168060004711151 Validation loss 0.01109224371612072 Accuracy 0.88232421875\n",
      "Iteration 26830 Training loss 0.00661698542535305 Validation loss 0.010917961597442627 Accuracy 0.88427734375\n",
      "Iteration 26840 Training loss 0.0064738779328763485 Validation loss 0.010850315913558006 Accuracy 0.88525390625\n",
      "Iteration 26850 Training loss 0.005506459623575211 Validation loss 0.010834179818630219 Accuracy 0.88427734375\n",
      "Iteration 26860 Training loss 0.004544908180832863 Validation loss 0.010954523459076881 Accuracy 0.88330078125\n",
      "Iteration 26870 Training loss 0.00404321076348424 Validation loss 0.010952741838991642 Accuracy 0.88427734375\n",
      "Iteration 26880 Training loss 0.005955576431006193 Validation loss 0.01110438909381628 Accuracy 0.8828125\n",
      "Iteration 26890 Training loss 0.005757007747888565 Validation loss 0.010824843309819698 Accuracy 0.8857421875\n",
      "Iteration 26900 Training loss 0.005885799881070852 Validation loss 0.010801387950778008 Accuracy 0.884765625\n",
      "Iteration 26910 Training loss 0.003625662764534354 Validation loss 0.010888608172535896 Accuracy 0.88427734375\n",
      "Iteration 26920 Training loss 0.005448435433208942 Validation loss 0.010702180676162243 Accuracy 0.88623046875\n",
      "Iteration 26930 Training loss 0.004670523572713137 Validation loss 0.011225228197872639 Accuracy 0.880859375\n",
      "Iteration 26940 Training loss 0.00635526655241847 Validation loss 0.011310659348964691 Accuracy 0.8798828125\n",
      "Iteration 26950 Training loss 0.004254254978150129 Validation loss 0.010821818374097347 Accuracy 0.884765625\n",
      "Iteration 26960 Training loss 0.006223613861948252 Validation loss 0.011018476448953152 Accuracy 0.88330078125\n",
      "Iteration 26970 Training loss 0.005465326365083456 Validation loss 0.011132234707474709 Accuracy 0.880859375\n",
      "Iteration 26980 Training loss 0.004806065931916237 Validation loss 0.011069882661104202 Accuracy 0.88232421875\n",
      "Iteration 26990 Training loss 0.006174550857394934 Validation loss 0.011331393383443356 Accuracy 0.8798828125\n",
      "Iteration 27000 Training loss 0.004043081775307655 Validation loss 0.011375895701348782 Accuracy 0.87890625\n",
      "Iteration 27010 Training loss 0.0036227498203516006 Validation loss 0.011232323944568634 Accuracy 0.88134765625\n",
      "Iteration 27020 Training loss 0.007316750939935446 Validation loss 0.011655513197183609 Accuracy 0.87451171875\n",
      "Iteration 27030 Training loss 0.005680416245013475 Validation loss 0.011230438016355038 Accuracy 0.88134765625\n",
      "Iteration 27040 Training loss 0.0054906392470002174 Validation loss 0.011038051918148994 Accuracy 0.88232421875\n",
      "Iteration 27050 Training loss 0.005374914035201073 Validation loss 0.011346293613314629 Accuracy 0.87939453125\n",
      "Iteration 27060 Training loss 0.004872848745435476 Validation loss 0.010876083746552467 Accuracy 0.88427734375\n",
      "Iteration 27070 Training loss 0.006061345338821411 Validation loss 0.010996844619512558 Accuracy 0.8828125\n",
      "Iteration 27080 Training loss 0.005417818669229746 Validation loss 0.011091078631579876 Accuracy 0.88232421875\n",
      "Iteration 27090 Training loss 0.00513048842549324 Validation loss 0.011384009383618832 Accuracy 0.87890625\n",
      "Iteration 27100 Training loss 0.0042753443121910095 Validation loss 0.011224107816815376 Accuracy 0.88037109375\n",
      "Iteration 27110 Training loss 0.0061539942398667336 Validation loss 0.011227519251406193 Accuracy 0.880859375\n",
      "Iteration 27120 Training loss 0.005360075738281012 Validation loss 0.01106232963502407 Accuracy 0.88232421875\n",
      "Iteration 27130 Training loss 0.005106134805828333 Validation loss 0.011134248226881027 Accuracy 0.8818359375\n",
      "Iteration 27140 Training loss 0.007028433494269848 Validation loss 0.012339483015239239 Accuracy 0.86962890625\n",
      "Iteration 27150 Training loss 0.004150213673710823 Validation loss 0.010780123993754387 Accuracy 0.88427734375\n",
      "Iteration 27160 Training loss 0.0056368084624409676 Validation loss 0.01111684087663889 Accuracy 0.8818359375\n",
      "Iteration 27170 Training loss 0.003547891043126583 Validation loss 0.010864026844501495 Accuracy 0.884765625\n",
      "Iteration 27180 Training loss 0.002749307546764612 Validation loss 0.011048378422856331 Accuracy 0.8818359375\n",
      "Iteration 27190 Training loss 0.00540131377056241 Validation loss 0.011053097434341908 Accuracy 0.88232421875\n",
      "Iteration 27200 Training loss 0.0038916263729333878 Validation loss 0.010987850837409496 Accuracy 0.88330078125\n",
      "Iteration 27210 Training loss 0.005352489184588194 Validation loss 0.011209296993911266 Accuracy 0.8798828125\n",
      "Iteration 27220 Training loss 0.005256050731986761 Validation loss 0.011115597561001778 Accuracy 0.88330078125\n",
      "Iteration 27230 Training loss 0.005541648715734482 Validation loss 0.010898921638727188 Accuracy 0.88427734375\n",
      "Iteration 27240 Training loss 0.0046874526888132095 Validation loss 0.011208419688045979 Accuracy 0.88134765625\n",
      "Iteration 27250 Training loss 0.00525695038959384 Validation loss 0.010832848958671093 Accuracy 0.884765625\n",
      "Iteration 27260 Training loss 0.005780231207609177 Validation loss 0.011212844401597977 Accuracy 0.88037109375\n",
      "Iteration 27270 Training loss 0.005729508586227894 Validation loss 0.011359131895005703 Accuracy 0.8798828125\n",
      "Iteration 27280 Training loss 0.005358690395951271 Validation loss 0.010939416475594044 Accuracy 0.88427734375\n",
      "Iteration 27290 Training loss 0.005854861345142126 Validation loss 0.011313016526401043 Accuracy 0.8798828125\n",
      "Iteration 27300 Training loss 0.00635361485183239 Validation loss 0.01208356861025095 Accuracy 0.8720703125\n",
      "Iteration 27310 Training loss 0.004889746196568012 Validation loss 0.011041130870580673 Accuracy 0.88232421875\n",
      "Iteration 27320 Training loss 0.0047821709886193275 Validation loss 0.011360586620867252 Accuracy 0.87939453125\n",
      "Iteration 27330 Training loss 0.004295523278415203 Validation loss 0.01107199676334858 Accuracy 0.88232421875\n",
      "Iteration 27340 Training loss 0.005111014470458031 Validation loss 0.011295346543192863 Accuracy 0.88037109375\n",
      "Iteration 27350 Training loss 0.004597911611199379 Validation loss 0.011341147124767303 Accuracy 0.87939453125\n",
      "Iteration 27360 Training loss 0.00632822047919035 Validation loss 0.01196176279336214 Accuracy 0.8740234375\n",
      "Iteration 27370 Training loss 0.004603330045938492 Validation loss 0.011000003665685654 Accuracy 0.8828125\n",
      "Iteration 27380 Training loss 0.003877770621329546 Validation loss 0.010993754491209984 Accuracy 0.8837890625\n",
      "Iteration 27390 Training loss 0.0051673599518835545 Validation loss 0.01089017279446125 Accuracy 0.884765625\n",
      "Iteration 27400 Training loss 0.0062776184640824795 Validation loss 0.011085270904004574 Accuracy 0.88134765625\n",
      "Iteration 27410 Training loss 0.005701085552573204 Validation loss 0.012014348059892654 Accuracy 0.8720703125\n",
      "Iteration 27420 Training loss 0.0065283579751849174 Validation loss 0.01117488369345665 Accuracy 0.88037109375\n",
      "Iteration 27430 Training loss 0.005520826205611229 Validation loss 0.010920969769358635 Accuracy 0.8837890625\n",
      "Iteration 27440 Training loss 0.004792462568730116 Validation loss 0.01106999721378088 Accuracy 0.8818359375\n",
      "Iteration 27450 Training loss 0.006613409146666527 Validation loss 0.011157792992889881 Accuracy 0.8818359375\n",
      "Iteration 27460 Training loss 0.004457365721464157 Validation loss 0.011219008825719357 Accuracy 0.88037109375\n",
      "Iteration 27470 Training loss 0.004604043439030647 Validation loss 0.010810856707394123 Accuracy 0.88525390625\n",
      "Iteration 27480 Training loss 0.006729340646415949 Validation loss 0.01146544050425291 Accuracy 0.8779296875\n",
      "Iteration 27490 Training loss 0.0044885254465043545 Validation loss 0.010883903130888939 Accuracy 0.884765625\n",
      "Iteration 27500 Training loss 0.004241100512444973 Validation loss 0.010866941884160042 Accuracy 0.88427734375\n",
      "Iteration 27510 Training loss 0.007794239092618227 Validation loss 0.011388509534299374 Accuracy 0.87841796875\n",
      "Iteration 27520 Training loss 0.003915691748261452 Validation loss 0.010976816527545452 Accuracy 0.8837890625\n",
      "Iteration 27530 Training loss 0.005472322925925255 Validation loss 0.011446710675954819 Accuracy 0.8779296875\n",
      "Iteration 27540 Training loss 0.005256186239421368 Validation loss 0.0112908398732543 Accuracy 0.87939453125\n",
      "Iteration 27550 Training loss 0.004151526372879744 Validation loss 0.011005544103682041 Accuracy 0.88232421875\n",
      "Iteration 27560 Training loss 0.005835341289639473 Validation loss 0.011228090152144432 Accuracy 0.88232421875\n",
      "Iteration 27570 Training loss 0.005724437069147825 Validation loss 0.011000090278685093 Accuracy 0.88232421875\n",
      "Iteration 27580 Training loss 0.005856883712112904 Validation loss 0.011162499897181988 Accuracy 0.88037109375\n",
      "Iteration 27590 Training loss 0.004410797730088234 Validation loss 0.011299439705908298 Accuracy 0.87939453125\n",
      "Iteration 27600 Training loss 0.005565624218434095 Validation loss 0.010929304175078869 Accuracy 0.8837890625\n",
      "Iteration 27610 Training loss 0.004124315455555916 Validation loss 0.01089389342814684 Accuracy 0.88330078125\n",
      "Iteration 27620 Training loss 0.005253514740616083 Validation loss 0.011051164008677006 Accuracy 0.88330078125\n",
      "Iteration 27630 Training loss 0.006374063901603222 Validation loss 0.011945605278015137 Accuracy 0.873046875\n",
      "Iteration 27640 Training loss 0.005329711362719536 Validation loss 0.011268211528658867 Accuracy 0.880859375\n",
      "Iteration 27650 Training loss 0.004259663168340921 Validation loss 0.011256217025220394 Accuracy 0.87939453125\n",
      "Iteration 27660 Training loss 0.0037612037267535925 Validation loss 0.010773449204862118 Accuracy 0.88525390625\n",
      "Iteration 27670 Training loss 0.0036457062233239412 Validation loss 0.01072259247303009 Accuracy 0.88525390625\n",
      "Iteration 27680 Training loss 0.006772899534553289 Validation loss 0.011214192025363445 Accuracy 0.88037109375\n",
      "Iteration 27690 Training loss 0.003748226212337613 Validation loss 0.011019157245755196 Accuracy 0.88232421875\n",
      "Iteration 27700 Training loss 0.005580116994678974 Validation loss 0.01149632129818201 Accuracy 0.87841796875\n",
      "Iteration 27710 Training loss 0.003924737684428692 Validation loss 0.011003187857568264 Accuracy 0.8837890625\n",
      "Iteration 27720 Training loss 0.0051092179492115974 Validation loss 0.010987192392349243 Accuracy 0.88232421875\n",
      "Iteration 27730 Training loss 0.005013908259570599 Validation loss 0.011298705823719501 Accuracy 0.87890625\n",
      "Iteration 27740 Training loss 0.004851021803915501 Validation loss 0.010982717387378216 Accuracy 0.8828125\n",
      "Iteration 27750 Training loss 0.005994326435029507 Validation loss 0.011205244809389114 Accuracy 0.8798828125\n",
      "Iteration 27760 Training loss 0.005382436793297529 Validation loss 0.010918458923697472 Accuracy 0.88330078125\n",
      "Iteration 27770 Training loss 0.005357707850635052 Validation loss 0.01154684741050005 Accuracy 0.87646484375\n",
      "Iteration 27780 Training loss 0.005860883742570877 Validation loss 0.011013959534466267 Accuracy 0.88232421875\n",
      "Iteration 27790 Training loss 0.004486911930143833 Validation loss 0.011073002591729164 Accuracy 0.8818359375\n",
      "Iteration 27800 Training loss 0.004481895361095667 Validation loss 0.011067553423345089 Accuracy 0.88232421875\n",
      "Iteration 27810 Training loss 0.004418271128088236 Validation loss 0.011140900664031506 Accuracy 0.8818359375\n",
      "Iteration 27820 Training loss 0.00449117599055171 Validation loss 0.011079687625169754 Accuracy 0.8818359375\n",
      "Iteration 27830 Training loss 0.005535340402275324 Validation loss 0.011278863064944744 Accuracy 0.8798828125\n",
      "Iteration 27840 Training loss 0.005227589979767799 Validation loss 0.010974748060107231 Accuracy 0.8818359375\n",
      "Iteration 27850 Training loss 0.004351702984422445 Validation loss 0.011171088553965092 Accuracy 0.88134765625\n",
      "Iteration 27860 Training loss 0.005266491789370775 Validation loss 0.011005955748260021 Accuracy 0.8828125\n",
      "Iteration 27870 Training loss 0.004911038558930159 Validation loss 0.0113276531919837 Accuracy 0.87939453125\n",
      "Iteration 27880 Training loss 0.004202147945761681 Validation loss 0.010962736792862415 Accuracy 0.8837890625\n",
      "Iteration 27890 Training loss 0.0035464500542730093 Validation loss 0.01094979327172041 Accuracy 0.8828125\n",
      "Iteration 27900 Training loss 0.004510644357651472 Validation loss 0.0109309833496809 Accuracy 0.8837890625\n",
      "Iteration 27910 Training loss 0.004688594024628401 Validation loss 0.01106491219252348 Accuracy 0.88232421875\n",
      "Iteration 27920 Training loss 0.004745951388031244 Validation loss 0.010911569930613041 Accuracy 0.88427734375\n",
      "Iteration 27930 Training loss 0.00429244851693511 Validation loss 0.011031122878193855 Accuracy 0.8828125\n",
      "Iteration 27940 Training loss 0.004964754916727543 Validation loss 0.011195134371519089 Accuracy 0.880859375\n",
      "Iteration 27950 Training loss 0.005853645969182253 Validation loss 0.010790802538394928 Accuracy 0.88525390625\n",
      "Iteration 27960 Training loss 0.004652021918445826 Validation loss 0.010973210446536541 Accuracy 0.88232421875\n",
      "Iteration 27970 Training loss 0.0052507189102470875 Validation loss 0.010758544318377972 Accuracy 0.884765625\n",
      "Iteration 27980 Training loss 0.005317081231623888 Validation loss 0.010765896178781986 Accuracy 0.884765625\n",
      "Iteration 27990 Training loss 0.005901432130485773 Validation loss 0.01106290239840746 Accuracy 0.8828125\n",
      "Iteration 28000 Training loss 0.004947939421981573 Validation loss 0.01110240537673235 Accuracy 0.8818359375\n",
      "Iteration 28010 Training loss 0.005793644115328789 Validation loss 0.011381011456251144 Accuracy 0.8798828125\n",
      "Iteration 28020 Training loss 0.004379419144243002 Validation loss 0.0112595334649086 Accuracy 0.88037109375\n",
      "Iteration 28030 Training loss 0.005813155323266983 Validation loss 0.010902036912739277 Accuracy 0.8837890625\n",
      "Iteration 28040 Training loss 0.004822810646146536 Validation loss 0.011099305003881454 Accuracy 0.88232421875\n",
      "Iteration 28050 Training loss 0.003922098316252232 Validation loss 0.011043607257306576 Accuracy 0.88232421875\n",
      "Iteration 28060 Training loss 0.004898310638964176 Validation loss 0.011005409061908722 Accuracy 0.88330078125\n",
      "Iteration 28070 Training loss 0.00511307455599308 Validation loss 0.011050600558519363 Accuracy 0.88232421875\n",
      "Iteration 28080 Training loss 0.00618847506120801 Validation loss 0.010923256166279316 Accuracy 0.88427734375\n",
      "Iteration 28090 Training loss 0.004431604407727718 Validation loss 0.011022782884538174 Accuracy 0.8828125\n",
      "Iteration 28100 Training loss 0.005681330803781748 Validation loss 0.010950685478746891 Accuracy 0.88232421875\n",
      "Iteration 28110 Training loss 0.004399665631353855 Validation loss 0.011131836101412773 Accuracy 0.880859375\n",
      "Iteration 28120 Training loss 0.005556171294301748 Validation loss 0.011213310062885284 Accuracy 0.88037109375\n",
      "Iteration 28130 Training loss 0.00454375334084034 Validation loss 0.010932880453765392 Accuracy 0.8828125\n",
      "Iteration 28140 Training loss 0.006109870038926601 Validation loss 0.011248082853853703 Accuracy 0.880859375\n",
      "Iteration 28150 Training loss 0.0045554605312645435 Validation loss 0.010897265747189522 Accuracy 0.88427734375\n",
      "Iteration 28160 Training loss 0.003792235627770424 Validation loss 0.010876552201807499 Accuracy 0.8837890625\n",
      "Iteration 28170 Training loss 0.0042882137931883335 Validation loss 0.01110800914466381 Accuracy 0.8818359375\n",
      "Iteration 28180 Training loss 0.005446983501315117 Validation loss 0.01131710410118103 Accuracy 0.87890625\n",
      "Iteration 28190 Training loss 0.004809068515896797 Validation loss 0.010946808382868767 Accuracy 0.88232421875\n",
      "Iteration 28200 Training loss 0.005283923353999853 Validation loss 0.011033608578145504 Accuracy 0.8828125\n",
      "Iteration 28210 Training loss 0.005391646642237902 Validation loss 0.010751032270491123 Accuracy 0.88427734375\n",
      "Iteration 28220 Training loss 0.006467424798756838 Validation loss 0.010868747718632221 Accuracy 0.8837890625\n",
      "Iteration 28230 Training loss 0.004291364457458258 Validation loss 0.010971041396260262 Accuracy 0.88525390625\n",
      "Iteration 28240 Training loss 0.004259697627276182 Validation loss 0.011063042096793652 Accuracy 0.8828125\n",
      "Iteration 28250 Training loss 0.005431088153272867 Validation loss 0.011144401505589485 Accuracy 0.88232421875\n",
      "Iteration 28260 Training loss 0.004618443548679352 Validation loss 0.011346996761858463 Accuracy 0.87939453125\n",
      "Iteration 28270 Training loss 0.005095801781862974 Validation loss 0.011075767688453197 Accuracy 0.88232421875\n",
      "Iteration 28280 Training loss 0.005697701591998339 Validation loss 0.011569526046514511 Accuracy 0.87548828125\n",
      "Iteration 28290 Training loss 0.0041577741503715515 Validation loss 0.011385414749383926 Accuracy 0.87841796875\n",
      "Iteration 28300 Training loss 0.0040849014185369015 Validation loss 0.011097882874310017 Accuracy 0.880859375\n",
      "Iteration 28310 Training loss 0.005493483040481806 Validation loss 0.01126483827829361 Accuracy 0.87939453125\n",
      "Iteration 28320 Training loss 0.004801207687705755 Validation loss 0.011183654889464378 Accuracy 0.88037109375\n",
      "Iteration 28330 Training loss 0.004165909253060818 Validation loss 0.010990846902132034 Accuracy 0.8828125\n",
      "Iteration 28340 Training loss 0.0066460659727454185 Validation loss 0.01104779914021492 Accuracy 0.88232421875\n",
      "Iteration 28350 Training loss 0.004406424704939127 Validation loss 0.011405756697058678 Accuracy 0.87890625\n",
      "Iteration 28360 Training loss 0.005086932796984911 Validation loss 0.011426940560340881 Accuracy 0.87890625\n",
      "Iteration 28370 Training loss 0.005164043977856636 Validation loss 0.010907694697380066 Accuracy 0.88427734375\n",
      "Iteration 28380 Training loss 0.005159869324415922 Validation loss 0.010996767319738865 Accuracy 0.88330078125\n",
      "Iteration 28390 Training loss 0.003983297385275364 Validation loss 0.01123387273401022 Accuracy 0.88037109375\n",
      "Iteration 28400 Training loss 0.005207187030464411 Validation loss 0.01106530986726284 Accuracy 0.88232421875\n",
      "Iteration 28410 Training loss 0.005449590738862753 Validation loss 0.011303380131721497 Accuracy 0.8798828125\n",
      "Iteration 28420 Training loss 0.004256652668118477 Validation loss 0.010951826348900795 Accuracy 0.88330078125\n",
      "Iteration 28430 Training loss 0.00470580393448472 Validation loss 0.010917318053543568 Accuracy 0.88330078125\n",
      "Iteration 28440 Training loss 0.004615080542862415 Validation loss 0.011006530374288559 Accuracy 0.88232421875\n",
      "Iteration 28450 Training loss 0.0051872399635612965 Validation loss 0.01109373476356268 Accuracy 0.880859375\n",
      "Iteration 28460 Training loss 0.0049665640108287334 Validation loss 0.010885195806622505 Accuracy 0.8828125\n",
      "Iteration 28470 Training loss 0.005640088114887476 Validation loss 0.010919333435595036 Accuracy 0.8837890625\n",
      "Iteration 28480 Training loss 0.003942679613828659 Validation loss 0.010907504707574844 Accuracy 0.8837890625\n",
      "Iteration 28490 Training loss 0.005104838404804468 Validation loss 0.01122439093887806 Accuracy 0.88037109375\n",
      "Iteration 28500 Training loss 0.00361830135807395 Validation loss 0.010841213166713715 Accuracy 0.88525390625\n",
      "Iteration 28510 Training loss 0.005151445511728525 Validation loss 0.011667444370687008 Accuracy 0.8759765625\n",
      "Iteration 28520 Training loss 0.004602709319442511 Validation loss 0.011019177734851837 Accuracy 0.8828125\n",
      "Iteration 28530 Training loss 0.006860754452645779 Validation loss 0.010986250825226307 Accuracy 0.8828125\n",
      "Iteration 28540 Training loss 0.005520194303244352 Validation loss 0.010988626629114151 Accuracy 0.8818359375\n",
      "Iteration 28550 Training loss 0.004905024543404579 Validation loss 0.011045871302485466 Accuracy 0.88134765625\n",
      "Iteration 28560 Training loss 0.0035077754873782396 Validation loss 0.011047961190342903 Accuracy 0.8818359375\n",
      "Iteration 28570 Training loss 0.004861171822994947 Validation loss 0.01137405727058649 Accuracy 0.8779296875\n",
      "Iteration 28580 Training loss 0.004120498429983854 Validation loss 0.010854172520339489 Accuracy 0.8837890625\n",
      "Iteration 28590 Training loss 0.00541830575093627 Validation loss 0.010963872075080872 Accuracy 0.88232421875\n",
      "Iteration 28600 Training loss 0.004125514999032021 Validation loss 0.010748371481895447 Accuracy 0.884765625\n",
      "Iteration 28610 Training loss 0.00545946229249239 Validation loss 0.010816274210810661 Accuracy 0.884765625\n",
      "Iteration 28620 Training loss 0.00480540469288826 Validation loss 0.010952609591186047 Accuracy 0.88232421875\n",
      "Iteration 28630 Training loss 0.004811848048120737 Validation loss 0.01089537050575018 Accuracy 0.88330078125\n",
      "Iteration 28640 Training loss 0.004644846543669701 Validation loss 0.01098434254527092 Accuracy 0.88232421875\n",
      "Iteration 28650 Training loss 0.004090689122676849 Validation loss 0.011200145818293095 Accuracy 0.88037109375\n",
      "Iteration 28660 Training loss 0.004809171427041292 Validation loss 0.010865497402846813 Accuracy 0.88427734375\n",
      "Iteration 28670 Training loss 0.005298596806824207 Validation loss 0.0110007980838418 Accuracy 0.8818359375\n",
      "Iteration 28680 Training loss 0.00327684567309916 Validation loss 0.01102948933839798 Accuracy 0.8818359375\n",
      "Iteration 28690 Training loss 0.0057816957123577595 Validation loss 0.011192354373633862 Accuracy 0.880859375\n",
      "Iteration 28700 Training loss 0.0036620839964598417 Validation loss 0.010762382298707962 Accuracy 0.884765625\n",
      "Iteration 28710 Training loss 0.004014180041849613 Validation loss 0.010743862017989159 Accuracy 0.88427734375\n",
      "Iteration 28720 Training loss 0.004961386322975159 Validation loss 0.010765431448817253 Accuracy 0.88525390625\n",
      "Iteration 28730 Training loss 0.006151181645691395 Validation loss 0.011344509199261665 Accuracy 0.87939453125\n",
      "Iteration 28740 Training loss 0.00397305004298687 Validation loss 0.010827829129993916 Accuracy 0.88427734375\n",
      "Iteration 28750 Training loss 0.0036100351717323065 Validation loss 0.011050558649003506 Accuracy 0.88232421875\n",
      "Iteration 28760 Training loss 0.005684088449925184 Validation loss 0.011155177839100361 Accuracy 0.88134765625\n",
      "Iteration 28770 Training loss 0.006331870798021555 Validation loss 0.010996829718351364 Accuracy 0.88232421875\n",
      "Iteration 28780 Training loss 0.0036540997680276632 Validation loss 0.011133622378110886 Accuracy 0.880859375\n",
      "Iteration 28790 Training loss 0.006142125464975834 Validation loss 0.01129031553864479 Accuracy 0.87939453125\n",
      "Iteration 28800 Training loss 0.004134297836571932 Validation loss 0.010892106220126152 Accuracy 0.88330078125\n",
      "Iteration 28810 Training loss 0.003876802511513233 Validation loss 0.011134113185107708 Accuracy 0.880859375\n",
      "Iteration 28820 Training loss 0.004829861223697662 Validation loss 0.011199726723134518 Accuracy 0.88037109375\n",
      "Iteration 28830 Training loss 0.004177686758339405 Validation loss 0.011115841567516327 Accuracy 0.8818359375\n",
      "Iteration 28840 Training loss 0.0041363537311553955 Validation loss 0.011281400918960571 Accuracy 0.87890625\n",
      "Iteration 28850 Training loss 0.005697344895452261 Validation loss 0.011146562173962593 Accuracy 0.8818359375\n",
      "Iteration 28860 Training loss 0.004641234874725342 Validation loss 0.011197532527148724 Accuracy 0.88037109375\n",
      "Iteration 28870 Training loss 0.003934988286346197 Validation loss 0.011141098104417324 Accuracy 0.880859375\n",
      "Iteration 28880 Training loss 0.0068487077951431274 Validation loss 0.011490408331155777 Accuracy 0.8779296875\n",
      "Iteration 28890 Training loss 0.006351771764457226 Validation loss 0.01150039304047823 Accuracy 0.8759765625\n",
      "Iteration 28900 Training loss 0.004801206290721893 Validation loss 0.011047950014472008 Accuracy 0.8818359375\n",
      "Iteration 28910 Training loss 0.006008657161146402 Validation loss 0.0113834822550416 Accuracy 0.87744140625\n",
      "Iteration 28920 Training loss 0.00555424252524972 Validation loss 0.01095940638333559 Accuracy 0.88232421875\n",
      "Iteration 28930 Training loss 0.003813267918303609 Validation loss 0.010777898132801056 Accuracy 0.88427734375\n",
      "Iteration 28940 Training loss 0.003948742989450693 Validation loss 0.011138440109789371 Accuracy 0.880859375\n",
      "Iteration 28950 Training loss 0.004271712154150009 Validation loss 0.011200137436389923 Accuracy 0.8798828125\n",
      "Iteration 28960 Training loss 0.005235549062490463 Validation loss 0.011177253909409046 Accuracy 0.880859375\n",
      "Iteration 28970 Training loss 0.005456211045384407 Validation loss 0.011564718559384346 Accuracy 0.87744140625\n",
      "Iteration 28980 Training loss 0.005672745406627655 Validation loss 0.01091886218637228 Accuracy 0.88330078125\n",
      "Iteration 28990 Training loss 0.0036323030944913626 Validation loss 0.011132352985441685 Accuracy 0.880859375\n",
      "Iteration 29000 Training loss 0.00405650632455945 Validation loss 0.010766101069748402 Accuracy 0.88427734375\n",
      "Iteration 29010 Training loss 0.003314228728413582 Validation loss 0.011183436959981918 Accuracy 0.880859375\n",
      "Iteration 29020 Training loss 0.005099175032228231 Validation loss 0.010903107933700085 Accuracy 0.8837890625\n",
      "Iteration 29030 Training loss 0.00535815441980958 Validation loss 0.011292116716504097 Accuracy 0.87939453125\n",
      "Iteration 29040 Training loss 0.005914564710110426 Validation loss 0.010891476646065712 Accuracy 0.8828125\n",
      "Iteration 29050 Training loss 0.005682336166501045 Validation loss 0.011236955411732197 Accuracy 0.88037109375\n",
      "Iteration 29060 Training loss 0.0036802678368985653 Validation loss 0.011135797016322613 Accuracy 0.8818359375\n",
      "Iteration 29070 Training loss 0.004031247925013304 Validation loss 0.011079326272010803 Accuracy 0.8818359375\n",
      "Iteration 29080 Training loss 0.0025805537588894367 Validation loss 0.011080103926360607 Accuracy 0.8818359375\n",
      "Iteration 29090 Training loss 0.003921968396753073 Validation loss 0.01097810734063387 Accuracy 0.8818359375\n",
      "Iteration 29100 Training loss 0.004872637335211039 Validation loss 0.011127748526632786 Accuracy 0.88134765625\n",
      "Iteration 29110 Training loss 0.0037942505441606045 Validation loss 0.011075041256844997 Accuracy 0.8828125\n",
      "Iteration 29120 Training loss 0.005017537623643875 Validation loss 0.011194157414138317 Accuracy 0.88037109375\n",
      "Iteration 29130 Training loss 0.004899764433503151 Validation loss 0.011385851539671421 Accuracy 0.87890625\n",
      "Iteration 29140 Training loss 0.004166432190686464 Validation loss 0.010790873318910599 Accuracy 0.884765625\n",
      "Iteration 29150 Training loss 0.0037413793615996838 Validation loss 0.011075391434133053 Accuracy 0.8818359375\n",
      "Iteration 29160 Training loss 0.005044138059020042 Validation loss 0.011265900917351246 Accuracy 0.8798828125\n",
      "Iteration 29170 Training loss 0.00462509086355567 Validation loss 0.010872473008930683 Accuracy 0.884765625\n",
      "Iteration 29180 Training loss 0.003961421083658934 Validation loss 0.011081557720899582 Accuracy 0.880859375\n",
      "Iteration 29190 Training loss 0.005435576196759939 Validation loss 0.011047432199120522 Accuracy 0.8818359375\n",
      "Iteration 29200 Training loss 0.0048913415521383286 Validation loss 0.011151987127959728 Accuracy 0.880859375\n",
      "Iteration 29210 Training loss 0.004297333303838968 Validation loss 0.010988808237016201 Accuracy 0.8828125\n",
      "Iteration 29220 Training loss 0.004203586373478174 Validation loss 0.011151708662509918 Accuracy 0.88037109375\n",
      "Iteration 29230 Training loss 0.004407237749546766 Validation loss 0.011103703640401363 Accuracy 0.88232421875\n",
      "Iteration 29240 Training loss 0.004879770800471306 Validation loss 0.010969365015625954 Accuracy 0.88232421875\n",
      "Iteration 29250 Training loss 0.005294289905577898 Validation loss 0.011200662702322006 Accuracy 0.87939453125\n",
      "Iteration 29260 Training loss 0.006267928518354893 Validation loss 0.011595477350056171 Accuracy 0.87548828125\n",
      "Iteration 29270 Training loss 0.005323545541614294 Validation loss 0.010993454605340958 Accuracy 0.8828125\n",
      "Iteration 29280 Training loss 0.003677181201055646 Validation loss 0.011054587550461292 Accuracy 0.88232421875\n",
      "Iteration 29290 Training loss 0.003999378066509962 Validation loss 0.011251210235059261 Accuracy 0.8798828125\n",
      "Iteration 29300 Training loss 0.003266130341216922 Validation loss 0.010731403715908527 Accuracy 0.8857421875\n",
      "Iteration 29310 Training loss 0.0038174211513251066 Validation loss 0.010893022641539574 Accuracy 0.88330078125\n",
      "Iteration 29320 Training loss 0.00438288738951087 Validation loss 0.010947063565254211 Accuracy 0.8828125\n",
      "Iteration 29330 Training loss 0.004472789820283651 Validation loss 0.010976869612932205 Accuracy 0.8828125\n",
      "Iteration 29340 Training loss 0.00496105058118701 Validation loss 0.01067350897938013 Accuracy 0.8857421875\n",
      "Iteration 29350 Training loss 0.004346616566181183 Validation loss 0.010921883396804333 Accuracy 0.8828125\n",
      "Iteration 29360 Training loss 0.0046560377813875675 Validation loss 0.011087138205766678 Accuracy 0.8818359375\n",
      "Iteration 29370 Training loss 0.005729490891098976 Validation loss 0.011270038783550262 Accuracy 0.87939453125\n",
      "Iteration 29380 Training loss 0.005988226737827063 Validation loss 0.011153627187013626 Accuracy 0.8818359375\n",
      "Iteration 29390 Training loss 0.0051888711750507355 Validation loss 0.01084721926599741 Accuracy 0.884765625\n",
      "Iteration 29400 Training loss 0.004870621953159571 Validation loss 0.010841774754226208 Accuracy 0.884765625\n",
      "Iteration 29410 Training loss 0.004820794332772493 Validation loss 0.010760756209492683 Accuracy 0.88525390625\n",
      "Iteration 29420 Training loss 0.0048086936585605145 Validation loss 0.01137343980371952 Accuracy 0.87939453125\n",
      "Iteration 29430 Training loss 0.00460700923576951 Validation loss 0.01082959771156311 Accuracy 0.88427734375\n",
      "Iteration 29440 Training loss 0.004617720376700163 Validation loss 0.010839556343853474 Accuracy 0.884765625\n",
      "Iteration 29450 Training loss 0.006499655544757843 Validation loss 0.010664314031600952 Accuracy 0.88623046875\n",
      "Iteration 29460 Training loss 0.0033231403212994337 Validation loss 0.010824068449437618 Accuracy 0.8837890625\n",
      "Iteration 29470 Training loss 0.0034663076512515545 Validation loss 0.010899035260081291 Accuracy 0.88330078125\n",
      "Iteration 29480 Training loss 0.005978791508823633 Validation loss 0.010706898756325245 Accuracy 0.88720703125\n",
      "Iteration 29490 Training loss 0.005441118963062763 Validation loss 0.0108480928465724 Accuracy 0.8837890625\n",
      "Iteration 29500 Training loss 0.004265658091753721 Validation loss 0.010906178504228592 Accuracy 0.8837890625\n",
      "Iteration 29510 Training loss 0.004104589577764273 Validation loss 0.010968572460114956 Accuracy 0.88330078125\n",
      "Iteration 29520 Training loss 0.004649971146136522 Validation loss 0.010766535066068172 Accuracy 0.884765625\n",
      "Iteration 29530 Training loss 0.005391914863139391 Validation loss 0.010820573195815086 Accuracy 0.884765625\n",
      "Iteration 29540 Training loss 0.00465340306982398 Validation loss 0.011075317859649658 Accuracy 0.8818359375\n",
      "Iteration 29550 Training loss 0.0035346876829862595 Validation loss 0.010991334915161133 Accuracy 0.8818359375\n",
      "Iteration 29560 Training loss 0.0040864297188818455 Validation loss 0.010767140425741673 Accuracy 0.8857421875\n",
      "Iteration 29570 Training loss 0.0035432903096079826 Validation loss 0.01096492912620306 Accuracy 0.88330078125\n",
      "Iteration 29580 Training loss 0.005223878659307957 Validation loss 0.011111181229352951 Accuracy 0.88037109375\n",
      "Iteration 29590 Training loss 0.004685277119278908 Validation loss 0.01076614111661911 Accuracy 0.8857421875\n",
      "Iteration 29600 Training loss 0.0036129786167293787 Validation loss 0.010852540843188763 Accuracy 0.884765625\n",
      "Iteration 29610 Training loss 0.00498345447704196 Validation loss 0.010827071033418179 Accuracy 0.8837890625\n",
      "Iteration 29620 Training loss 0.005173817276954651 Validation loss 0.010839779861271381 Accuracy 0.884765625\n",
      "Iteration 29630 Training loss 0.004763668868690729 Validation loss 0.011029540561139584 Accuracy 0.88330078125\n",
      "Iteration 29640 Training loss 0.006248741876333952 Validation loss 0.011001154780387878 Accuracy 0.88232421875\n",
      "Iteration 29650 Training loss 0.004749090876430273 Validation loss 0.010977184399962425 Accuracy 0.8828125\n",
      "Iteration 29660 Training loss 0.0037389148492366076 Validation loss 0.011077712289988995 Accuracy 0.88134765625\n",
      "Iteration 29670 Training loss 0.00371455866843462 Validation loss 0.011109672486782074 Accuracy 0.8818359375\n",
      "Iteration 29680 Training loss 0.005847801920026541 Validation loss 0.011416954919695854 Accuracy 0.87841796875\n",
      "Iteration 29690 Training loss 0.004723937716335058 Validation loss 0.010961068794131279 Accuracy 0.8837890625\n",
      "Iteration 29700 Training loss 0.004809125792235136 Validation loss 0.011259269900619984 Accuracy 0.880859375\n",
      "Iteration 29710 Training loss 0.0028604702092707157 Validation loss 0.010846535675227642 Accuracy 0.88427734375\n",
      "Iteration 29720 Training loss 0.005786539521068335 Validation loss 0.010702221654355526 Accuracy 0.88623046875\n",
      "Iteration 29730 Training loss 0.00378051376901567 Validation loss 0.010837818495929241 Accuracy 0.88427734375\n",
      "Iteration 29740 Training loss 0.0045422702096402645 Validation loss 0.010702555999159813 Accuracy 0.88623046875\n",
      "Iteration 29750 Training loss 0.003934090957045555 Validation loss 0.011055574752390385 Accuracy 0.88037109375\n",
      "Iteration 29760 Training loss 0.005159594584256411 Validation loss 0.010703378356993198 Accuracy 0.88623046875\n",
      "Iteration 29770 Training loss 0.00542704900726676 Validation loss 0.010880931280553341 Accuracy 0.8837890625\n",
      "Iteration 29780 Training loss 0.0037199973594397306 Validation loss 0.01085752248764038 Accuracy 0.8837890625\n",
      "Iteration 29790 Training loss 0.004398445598781109 Validation loss 0.010981088504195213 Accuracy 0.88232421875\n",
      "Iteration 29800 Training loss 0.005824640393257141 Validation loss 0.011087482795119286 Accuracy 0.88232421875\n",
      "Iteration 29810 Training loss 0.005424892529845238 Validation loss 0.0108941113576293 Accuracy 0.88427734375\n",
      "Iteration 29820 Training loss 0.0056408606469631195 Validation loss 0.011320584453642368 Accuracy 0.88037109375\n",
      "Iteration 29830 Training loss 0.006112379487603903 Validation loss 0.011063388548791409 Accuracy 0.88232421875\n",
      "Iteration 29840 Training loss 0.005272905807942152 Validation loss 0.011249138042330742 Accuracy 0.8798828125\n",
      "Iteration 29850 Training loss 0.004919779486954212 Validation loss 0.010901418514549732 Accuracy 0.8837890625\n",
      "Iteration 29860 Training loss 0.004383999388664961 Validation loss 0.010956364683806896 Accuracy 0.8828125\n",
      "Iteration 29870 Training loss 0.004110144451260567 Validation loss 0.010914365760982037 Accuracy 0.88232421875\n",
      "Iteration 29880 Training loss 0.0033323729876428843 Validation loss 0.010741925798356533 Accuracy 0.8837890625\n",
      "Iteration 29890 Training loss 0.004348321817815304 Validation loss 0.01072611752897501 Accuracy 0.884765625\n",
      "Iteration 29900 Training loss 0.005062439013272524 Validation loss 0.0107295336201787 Accuracy 0.8857421875\n",
      "Iteration 29910 Training loss 0.005188846029341221 Validation loss 0.010807372629642487 Accuracy 0.884765625\n",
      "Iteration 29920 Training loss 0.005294200498610735 Validation loss 0.010649084113538265 Accuracy 0.88623046875\n",
      "Iteration 29930 Training loss 0.004860466346144676 Validation loss 0.010644831694662571 Accuracy 0.88525390625\n",
      "Iteration 29940 Training loss 0.0028164274990558624 Validation loss 0.01081504113972187 Accuracy 0.88427734375\n",
      "Iteration 29950 Training loss 0.004983908962458372 Validation loss 0.011016580276191235 Accuracy 0.88134765625\n",
      "Iteration 29960 Training loss 0.004544044379144907 Validation loss 0.011117368936538696 Accuracy 0.880859375\n",
      "Iteration 29970 Training loss 0.0036456845700740814 Validation loss 0.01147610042244196 Accuracy 0.8779296875\n",
      "Iteration 29980 Training loss 0.003942081239074469 Validation loss 0.01095989253371954 Accuracy 0.8828125\n",
      "Iteration 29990 Training loss 0.005808466579765081 Validation loss 0.011275500059127808 Accuracy 0.87890625\n",
      "Iteration 30000 Training loss 0.0055571370758116245 Validation loss 0.011869830079376698 Accuracy 0.87353515625\n",
      "Iteration 30010 Training loss 0.005714822094887495 Validation loss 0.0111030712723732 Accuracy 0.880859375\n",
      "Iteration 30020 Training loss 0.004373088013380766 Validation loss 0.010882353410124779 Accuracy 0.8837890625\n",
      "Iteration 30030 Training loss 0.004284994211047888 Validation loss 0.010833202861249447 Accuracy 0.8857421875\n",
      "Iteration 30040 Training loss 0.003500918624922633 Validation loss 0.011037897318601608 Accuracy 0.8818359375\n",
      "Iteration 30050 Training loss 0.0030660470947623253 Validation loss 0.01085782703012228 Accuracy 0.8837890625\n",
      "Iteration 30060 Training loss 0.004104133695363998 Validation loss 0.010947726666927338 Accuracy 0.88330078125\n",
      "Iteration 30070 Training loss 0.004852416459470987 Validation loss 0.01101107057183981 Accuracy 0.88232421875\n",
      "Iteration 30080 Training loss 0.003367512719705701 Validation loss 0.010608881711959839 Accuracy 0.88720703125\n",
      "Iteration 30090 Training loss 0.00459663849323988 Validation loss 0.010999553836882114 Accuracy 0.88232421875\n",
      "Iteration 30100 Training loss 0.004271337296813726 Validation loss 0.01069981511682272 Accuracy 0.884765625\n",
      "Iteration 30110 Training loss 0.005261733662337065 Validation loss 0.01085656601935625 Accuracy 0.88427734375\n",
      "Iteration 30120 Training loss 0.0056141954846680164 Validation loss 0.010841130279004574 Accuracy 0.88427734375\n",
      "Iteration 30130 Training loss 0.004444010555744171 Validation loss 0.010769783519208431 Accuracy 0.8857421875\n",
      "Iteration 30140 Training loss 0.007100529968738556 Validation loss 0.011194844730198383 Accuracy 0.880859375\n",
      "Iteration 30150 Training loss 0.004325644578784704 Validation loss 0.010955717414617538 Accuracy 0.8828125\n",
      "Iteration 30160 Training loss 0.005576052237302065 Validation loss 0.01101616770029068 Accuracy 0.8818359375\n",
      "Iteration 30170 Training loss 0.004341470543295145 Validation loss 0.010674107819795609 Accuracy 0.88671875\n",
      "Iteration 30180 Training loss 0.005846785847097635 Validation loss 0.011184696108102798 Accuracy 0.880859375\n",
      "Iteration 30190 Training loss 0.004787237849086523 Validation loss 0.011059770360589027 Accuracy 0.8818359375\n",
      "Iteration 30200 Training loss 0.005385789088904858 Validation loss 0.011527501046657562 Accuracy 0.87646484375\n",
      "Iteration 30210 Training loss 0.004955342970788479 Validation loss 0.01089449692517519 Accuracy 0.8837890625\n",
      "Iteration 30220 Training loss 0.003965455107390881 Validation loss 0.01071945484727621 Accuracy 0.8857421875\n",
      "Iteration 30230 Training loss 0.003954870626330376 Validation loss 0.010996975004673004 Accuracy 0.88330078125\n",
      "Iteration 30240 Training loss 0.003976465202867985 Validation loss 0.010825044475495815 Accuracy 0.88330078125\n",
      "Iteration 30250 Training loss 0.003460913896560669 Validation loss 0.010758018121123314 Accuracy 0.884765625\n",
      "Iteration 30260 Training loss 0.003097300184890628 Validation loss 0.01087441947311163 Accuracy 0.88525390625\n",
      "Iteration 30270 Training loss 0.004805384203791618 Validation loss 0.01066604070365429 Accuracy 0.8857421875\n",
      "Iteration 30280 Training loss 0.004483513999730349 Validation loss 0.010979191400110722 Accuracy 0.88330078125\n",
      "Iteration 30290 Training loss 0.005656997673213482 Validation loss 0.010975339449942112 Accuracy 0.88232421875\n",
      "Iteration 30300 Training loss 0.0047288015484809875 Validation loss 0.010857603512704372 Accuracy 0.884765625\n",
      "Iteration 30310 Training loss 0.005536925513297319 Validation loss 0.010691729374229908 Accuracy 0.88671875\n",
      "Iteration 30320 Training loss 0.005858779884874821 Validation loss 0.0110005559399724 Accuracy 0.88330078125\n",
      "Iteration 30330 Training loss 0.004621861036866903 Validation loss 0.010810361243784428 Accuracy 0.884765625\n",
      "Iteration 30340 Training loss 0.005277101881802082 Validation loss 0.010860146023333073 Accuracy 0.8837890625\n",
      "Iteration 30350 Training loss 0.0055312784388661385 Validation loss 0.010860557667911053 Accuracy 0.88427734375\n",
      "Iteration 30360 Training loss 0.005863516591489315 Validation loss 0.011123303323984146 Accuracy 0.880859375\n",
      "Iteration 30370 Training loss 0.0047205109149217606 Validation loss 0.010982565581798553 Accuracy 0.8828125\n",
      "Iteration 30380 Training loss 0.004967504646629095 Validation loss 0.010915413498878479 Accuracy 0.8837890625\n",
      "Iteration 30390 Training loss 0.004217574838548899 Validation loss 0.01079352293163538 Accuracy 0.88427734375\n",
      "Iteration 30400 Training loss 0.004842076450586319 Validation loss 0.010903866961598396 Accuracy 0.88330078125\n",
      "Iteration 30410 Training loss 0.0041956715285778046 Validation loss 0.010712802410125732 Accuracy 0.8837890625\n",
      "Iteration 30420 Training loss 0.004978125914931297 Validation loss 0.010816018097102642 Accuracy 0.88330078125\n",
      "Iteration 30430 Training loss 0.004496362060308456 Validation loss 0.010592514649033546 Accuracy 0.88671875\n",
      "Iteration 30440 Training loss 0.006684437859803438 Validation loss 0.011133923195302486 Accuracy 0.88134765625\n",
      "Iteration 30450 Training loss 0.004554123617708683 Validation loss 0.011086962185800076 Accuracy 0.880859375\n",
      "Iteration 30460 Training loss 0.003396997693926096 Validation loss 0.010990693233907223 Accuracy 0.8828125\n",
      "Iteration 30470 Training loss 0.004300704225897789 Validation loss 0.010630575940012932 Accuracy 0.8857421875\n",
      "Iteration 30480 Training loss 0.0048478394746780396 Validation loss 0.010890546254813671 Accuracy 0.8837890625\n",
      "Iteration 30490 Training loss 0.004392225760966539 Validation loss 0.010654906742274761 Accuracy 0.88525390625\n",
      "Iteration 30500 Training loss 0.0038435962051153183 Validation loss 0.010886437259614468 Accuracy 0.8837890625\n",
      "Iteration 30510 Training loss 0.005944414529949427 Validation loss 0.010833202861249447 Accuracy 0.8857421875\n",
      "Iteration 30520 Training loss 0.0024626993108540773 Validation loss 0.010895822197198868 Accuracy 0.88330078125\n",
      "Iteration 30530 Training loss 0.0038705470506101847 Validation loss 0.01062572468072176 Accuracy 0.88623046875\n",
      "Iteration 30540 Training loss 0.003690283978357911 Validation loss 0.010708806104958057 Accuracy 0.88671875\n",
      "Iteration 30550 Training loss 0.004745991434901953 Validation loss 0.010683168657124043 Accuracy 0.88623046875\n",
      "Iteration 30560 Training loss 0.00585869699716568 Validation loss 0.010802575387060642 Accuracy 0.884765625\n",
      "Iteration 30570 Training loss 0.00424750242382288 Validation loss 0.011023999191820621 Accuracy 0.8837890625\n",
      "Iteration 30580 Training loss 0.006565842777490616 Validation loss 0.010855128988623619 Accuracy 0.884765625\n",
      "Iteration 30590 Training loss 0.0036851107142865658 Validation loss 0.010819641873240471 Accuracy 0.88330078125\n",
      "Iteration 30600 Training loss 0.005634370259940624 Validation loss 0.011338585987687111 Accuracy 0.8779296875\n",
      "Iteration 30610 Training loss 0.0038047574926167727 Validation loss 0.010940180160105228 Accuracy 0.88330078125\n",
      "Iteration 30620 Training loss 0.003904378740116954 Validation loss 0.01082814671099186 Accuracy 0.88427734375\n",
      "Iteration 30630 Training loss 0.004116291645914316 Validation loss 0.010960829444229603 Accuracy 0.88330078125\n",
      "Iteration 30640 Training loss 0.004044858738780022 Validation loss 0.010911151766777039 Accuracy 0.8837890625\n",
      "Iteration 30650 Training loss 0.004144128877669573 Validation loss 0.010751012712717056 Accuracy 0.884765625\n",
      "Iteration 30660 Training loss 0.005892462562769651 Validation loss 0.011613293550908566 Accuracy 0.876953125\n",
      "Iteration 30670 Training loss 0.0033771656453609467 Validation loss 0.011076798662543297 Accuracy 0.88134765625\n",
      "Iteration 30680 Training loss 0.005098687019199133 Validation loss 0.010712088085711002 Accuracy 0.88623046875\n",
      "Iteration 30690 Training loss 0.0035050385631620884 Validation loss 0.011135891079902649 Accuracy 0.88037109375\n",
      "Iteration 30700 Training loss 0.004170111380517483 Validation loss 0.010837907902896404 Accuracy 0.8837890625\n",
      "Iteration 30710 Training loss 0.005583995021879673 Validation loss 0.010946308262646198 Accuracy 0.88232421875\n",
      "Iteration 30720 Training loss 0.0042596557177603245 Validation loss 0.011245915666222572 Accuracy 0.8798828125\n",
      "Iteration 30730 Training loss 0.004634143318980932 Validation loss 0.011512823402881622 Accuracy 0.87646484375\n",
      "Iteration 30740 Training loss 0.002066435292363167 Validation loss 0.010797392576932907 Accuracy 0.88427734375\n",
      "Iteration 30750 Training loss 0.0051337555050849915 Validation loss 0.01083255372941494 Accuracy 0.8837890625\n",
      "Iteration 30760 Training loss 0.0041565606370568275 Validation loss 0.010752313770353794 Accuracy 0.8857421875\n",
      "Iteration 30770 Training loss 0.0049967519007623196 Validation loss 0.010758311487734318 Accuracy 0.8857421875\n",
      "Iteration 30780 Training loss 0.004947444424033165 Validation loss 0.010804432444274426 Accuracy 0.88330078125\n",
      "Iteration 30790 Training loss 0.0029918975196778774 Validation loss 0.01086256094276905 Accuracy 0.8837890625\n",
      "Iteration 30800 Training loss 0.004376294557005167 Validation loss 0.011281008832156658 Accuracy 0.88037109375\n",
      "Iteration 30810 Training loss 0.0034801498986780643 Validation loss 0.010800573043525219 Accuracy 0.884765625\n",
      "Iteration 30820 Training loss 0.00543979974463582 Validation loss 0.010690995492041111 Accuracy 0.8857421875\n",
      "Iteration 30830 Training loss 0.00448132399469614 Validation loss 0.010658873245120049 Accuracy 0.88623046875\n",
      "Iteration 30840 Training loss 0.0048216562718153 Validation loss 0.010716731660068035 Accuracy 0.88525390625\n",
      "Iteration 30850 Training loss 0.004078918602317572 Validation loss 0.010751930996775627 Accuracy 0.884765625\n",
      "Iteration 30860 Training loss 0.004335327539592981 Validation loss 0.011265846900641918 Accuracy 0.8798828125\n",
      "Iteration 30870 Training loss 0.00432689068838954 Validation loss 0.010787195526063442 Accuracy 0.8837890625\n",
      "Iteration 30880 Training loss 0.004119638353586197 Validation loss 0.010681398212909698 Accuracy 0.88671875\n",
      "Iteration 30890 Training loss 0.004441136494278908 Validation loss 0.01093828584998846 Accuracy 0.88330078125\n",
      "Iteration 30900 Training loss 0.0041491081938147545 Validation loss 0.010844831354916096 Accuracy 0.8837890625\n",
      "Iteration 30910 Training loss 0.004953906871378422 Validation loss 0.011216589249670506 Accuracy 0.8798828125\n",
      "Iteration 30920 Training loss 0.004574904218316078 Validation loss 0.01067846268415451 Accuracy 0.88623046875\n",
      "Iteration 30930 Training loss 0.003986324183642864 Validation loss 0.010831418447196484 Accuracy 0.88427734375\n",
      "Iteration 30940 Training loss 0.003992181736975908 Validation loss 0.010819591581821442 Accuracy 0.88427734375\n",
      "Iteration 30950 Training loss 0.003853201400488615 Validation loss 0.010920140892267227 Accuracy 0.88232421875\n",
      "Iteration 30960 Training loss 0.006228045094758272 Validation loss 0.010808904655277729 Accuracy 0.884765625\n",
      "Iteration 30970 Training loss 0.0036003892309963703 Validation loss 0.011134328320622444 Accuracy 0.88037109375\n",
      "Iteration 30980 Training loss 0.0031995840836316347 Validation loss 0.010926916263997555 Accuracy 0.8828125\n",
      "Iteration 30990 Training loss 0.005498308688402176 Validation loss 0.010971122421324253 Accuracy 0.88232421875\n",
      "Iteration 31000 Training loss 0.0033174988348037004 Validation loss 0.011191142722964287 Accuracy 0.8798828125\n",
      "Iteration 31010 Training loss 0.005351154133677483 Validation loss 0.011119348928332329 Accuracy 0.8818359375\n",
      "Iteration 31020 Training loss 0.005897365976125002 Validation loss 0.01095669623464346 Accuracy 0.8837890625\n",
      "Iteration 31030 Training loss 0.00344702391885221 Validation loss 0.010747460648417473 Accuracy 0.88623046875\n",
      "Iteration 31040 Training loss 0.004480310250073671 Validation loss 0.010975395329296589 Accuracy 0.8828125\n",
      "Iteration 31050 Training loss 0.004880865570157766 Validation loss 0.010812930762767792 Accuracy 0.884765625\n",
      "Iteration 31060 Training loss 0.00512280547991395 Validation loss 0.010911780409514904 Accuracy 0.8837890625\n",
      "Iteration 31070 Training loss 0.004191204905509949 Validation loss 0.010958444327116013 Accuracy 0.88330078125\n",
      "Iteration 31080 Training loss 0.0055156610906124115 Validation loss 0.010645892471075058 Accuracy 0.88623046875\n",
      "Iteration 31090 Training loss 0.0042665572836995125 Validation loss 0.011440690606832504 Accuracy 0.87939453125\n",
      "Iteration 31100 Training loss 0.0053774514235556126 Validation loss 0.01100255362689495 Accuracy 0.88232421875\n",
      "Iteration 31110 Training loss 0.00381491519510746 Validation loss 0.010801617987453938 Accuracy 0.884765625\n",
      "Iteration 31120 Training loss 0.003640042617917061 Validation loss 0.011076935566961765 Accuracy 0.8818359375\n",
      "Iteration 31130 Training loss 0.005033995024859905 Validation loss 0.010526652447879314 Accuracy 0.88720703125\n",
      "Iteration 31140 Training loss 0.00453717028722167 Validation loss 0.01061245333403349 Accuracy 0.88720703125\n",
      "Iteration 31150 Training loss 0.005006662104278803 Validation loss 0.010877924971282482 Accuracy 0.8828125\n",
      "Iteration 31160 Training loss 0.0036060635466128588 Validation loss 0.010730107314884663 Accuracy 0.88525390625\n",
      "Iteration 31170 Training loss 0.003551291534677148 Validation loss 0.011271169409155846 Accuracy 0.87939453125\n",
      "Iteration 31180 Training loss 0.003935503773391247 Validation loss 0.010549189522862434 Accuracy 0.8857421875\n",
      "Iteration 31190 Training loss 0.0044927699491381645 Validation loss 0.011083110235631466 Accuracy 0.88232421875\n",
      "Iteration 31200 Training loss 0.004403355065733194 Validation loss 0.010840934701263905 Accuracy 0.8837890625\n",
      "Iteration 31210 Training loss 0.004344445653259754 Validation loss 0.010786877013742924 Accuracy 0.88427734375\n",
      "Iteration 31220 Training loss 0.005160504020750523 Validation loss 0.010979914106428623 Accuracy 0.88232421875\n",
      "Iteration 31230 Training loss 0.0049318671226501465 Validation loss 0.011100558564066887 Accuracy 0.88134765625\n",
      "Iteration 31240 Training loss 0.004139300901442766 Validation loss 0.010806004516780376 Accuracy 0.88525390625\n",
      "Iteration 31250 Training loss 0.005071793217211962 Validation loss 0.011119483038783073 Accuracy 0.88134765625\n",
      "Iteration 31260 Training loss 0.005756495054811239 Validation loss 0.011098467744886875 Accuracy 0.8818359375\n",
      "Iteration 31270 Training loss 0.003790505463257432 Validation loss 0.010680925101041794 Accuracy 0.884765625\n",
      "Iteration 31280 Training loss 0.0039028720930218697 Validation loss 0.011047233827412128 Accuracy 0.8828125\n",
      "Iteration 31290 Training loss 0.004551181569695473 Validation loss 0.011069483123719692 Accuracy 0.8818359375\n",
      "Iteration 31300 Training loss 0.006047310307621956 Validation loss 0.011002499610185623 Accuracy 0.88232421875\n",
      "Iteration 31310 Training loss 0.006276139989495277 Validation loss 0.010976062156260014 Accuracy 0.8837890625\n",
      "Iteration 31320 Training loss 0.004292989149689674 Validation loss 0.010937334038317204 Accuracy 0.8828125\n",
      "Iteration 31330 Training loss 0.0033729188144207 Validation loss 0.010959445498883724 Accuracy 0.88330078125\n",
      "Iteration 31340 Training loss 0.005148808006197214 Validation loss 0.010937614366412163 Accuracy 0.88232421875\n",
      "Iteration 31350 Training loss 0.004703248851001263 Validation loss 0.011143231764435768 Accuracy 0.88134765625\n",
      "Iteration 31360 Training loss 0.005174565594643354 Validation loss 0.011216484010219574 Accuracy 0.88037109375\n",
      "Iteration 31370 Training loss 0.005361303687095642 Validation loss 0.011249919421970844 Accuracy 0.8798828125\n",
      "Iteration 31380 Training loss 0.004873988218605518 Validation loss 0.01100040040910244 Accuracy 0.8818359375\n",
      "Iteration 31390 Training loss 0.004949276335537434 Validation loss 0.011036536656320095 Accuracy 0.88134765625\n",
      "Iteration 31400 Training loss 0.005103531293570995 Validation loss 0.010822062380611897 Accuracy 0.8837890625\n",
      "Iteration 31410 Training loss 0.004201948177069426 Validation loss 0.010819323360919952 Accuracy 0.88525390625\n",
      "Iteration 31420 Training loss 0.005579747259616852 Validation loss 0.011144174262881279 Accuracy 0.880859375\n",
      "Iteration 31430 Training loss 0.0034258426167070866 Validation loss 0.010769755579531193 Accuracy 0.88671875\n",
      "Iteration 31440 Training loss 0.002971055917441845 Validation loss 0.010963900946080685 Accuracy 0.88330078125\n",
      "Iteration 31450 Training loss 0.006104466505348682 Validation loss 0.011081363074481487 Accuracy 0.8818359375\n",
      "Iteration 31460 Training loss 0.005927270278334618 Validation loss 0.010842588730156422 Accuracy 0.8837890625\n",
      "Iteration 31470 Training loss 0.0046914624981582165 Validation loss 0.011046637780964375 Accuracy 0.8818359375\n",
      "Iteration 31480 Training loss 0.005100517999380827 Validation loss 0.011243682354688644 Accuracy 0.8798828125\n",
      "Iteration 31490 Training loss 0.004168997053056955 Validation loss 0.011024422012269497 Accuracy 0.8828125\n",
      "Iteration 31500 Training loss 0.004140152595937252 Validation loss 0.010946877300739288 Accuracy 0.8828125\n",
      "Iteration 31510 Training loss 0.005562166683375835 Validation loss 0.010935490019619465 Accuracy 0.8837890625\n",
      "Iteration 31520 Training loss 0.004180236253887415 Validation loss 0.010804391466081142 Accuracy 0.88427734375\n",
      "Iteration 31530 Training loss 0.0056005967780947685 Validation loss 0.010894659906625748 Accuracy 0.88330078125\n",
      "Iteration 31540 Training loss 0.004716651979833841 Validation loss 0.010644704103469849 Accuracy 0.88671875\n",
      "Iteration 31550 Training loss 0.005273770075291395 Validation loss 0.01086992397904396 Accuracy 0.8828125\n",
      "Iteration 31560 Training loss 0.0051124864257872105 Validation loss 0.010757612995803356 Accuracy 0.8857421875\n",
      "Iteration 31570 Training loss 0.0037599713541567326 Validation loss 0.01075740810483694 Accuracy 0.8857421875\n",
      "Iteration 31580 Training loss 0.003972606733441353 Validation loss 0.01064188964664936 Accuracy 0.88671875\n",
      "Iteration 31590 Training loss 0.005405975505709648 Validation loss 0.011043749749660492 Accuracy 0.88232421875\n",
      "Iteration 31600 Training loss 0.004232839681208134 Validation loss 0.011001062579452991 Accuracy 0.88232421875\n",
      "Iteration 31610 Training loss 0.005694124381989241 Validation loss 0.010914214886724949 Accuracy 0.8837890625\n",
      "Iteration 31620 Training loss 0.005886517930775881 Validation loss 0.010937709361314774 Accuracy 0.8837890625\n",
      "Iteration 31630 Training loss 0.004115065094083548 Validation loss 0.011238094419240952 Accuracy 0.880859375\n",
      "Iteration 31640 Training loss 0.005061344243586063 Validation loss 0.010696006007492542 Accuracy 0.8857421875\n",
      "Iteration 31650 Training loss 0.00429181195795536 Validation loss 0.010979472659528255 Accuracy 0.8828125\n",
      "Iteration 31660 Training loss 0.002146426821127534 Validation loss 0.010686774738132954 Accuracy 0.88623046875\n",
      "Iteration 31670 Training loss 0.006371852941811085 Validation loss 0.01157389860600233 Accuracy 0.8759765625\n",
      "Iteration 31680 Training loss 0.005767158232629299 Validation loss 0.011585701256990433 Accuracy 0.876953125\n",
      "Iteration 31690 Training loss 0.0047712200321257114 Validation loss 0.010905612260103226 Accuracy 0.88330078125\n",
      "Iteration 31700 Training loss 0.0038234605453908443 Validation loss 0.010984336026012897 Accuracy 0.8837890625\n",
      "Iteration 31710 Training loss 0.003551566507667303 Validation loss 0.011469691060483456 Accuracy 0.87744140625\n",
      "Iteration 31720 Training loss 0.003553427755832672 Validation loss 0.010931521654129028 Accuracy 0.88330078125\n",
      "Iteration 31730 Training loss 0.004531043581664562 Validation loss 0.011172887869179249 Accuracy 0.88037109375\n",
      "Iteration 31740 Training loss 0.0045265681110322475 Validation loss 0.010943254455924034 Accuracy 0.88330078125\n",
      "Iteration 31750 Training loss 0.004396943841129541 Validation loss 0.011124537326395512 Accuracy 0.88232421875\n",
      "Iteration 31760 Training loss 0.00582940923050046 Validation loss 0.010824312455952168 Accuracy 0.884765625\n",
      "Iteration 31770 Training loss 0.005281899590045214 Validation loss 0.010935906320810318 Accuracy 0.88330078125\n",
      "Iteration 31780 Training loss 0.003330484963953495 Validation loss 0.010982739739120007 Accuracy 0.8828125\n",
      "Iteration 31790 Training loss 0.0043899305164813995 Validation loss 0.010623305104672909 Accuracy 0.88623046875\n",
      "Iteration 31800 Training loss 0.0028585351537913084 Validation loss 0.010832448489964008 Accuracy 0.88427734375\n",
      "Iteration 31810 Training loss 0.002778795547783375 Validation loss 0.010834981687366962 Accuracy 0.88427734375\n",
      "Iteration 31820 Training loss 0.004609460011124611 Validation loss 0.011016464792191982 Accuracy 0.8818359375\n",
      "Iteration 31830 Training loss 0.004451681859791279 Validation loss 0.010761188343167305 Accuracy 0.8857421875\n",
      "Iteration 31840 Training loss 0.005183685105293989 Validation loss 0.010913964360952377 Accuracy 0.8837890625\n",
      "Iteration 31850 Training loss 0.0030396778602153063 Validation loss 0.011103067547082901 Accuracy 0.880859375\n",
      "Iteration 31860 Training loss 0.001838846830651164 Validation loss 0.01120965089648962 Accuracy 0.88037109375\n",
      "Iteration 31870 Training loss 0.004107271321117878 Validation loss 0.010671098716557026 Accuracy 0.88525390625\n",
      "Iteration 31880 Training loss 0.004328918643295765 Validation loss 0.010706956498324871 Accuracy 0.88525390625\n",
      "Iteration 31890 Training loss 0.005884375423192978 Validation loss 0.010717513971030712 Accuracy 0.88623046875\n",
      "Iteration 31900 Training loss 0.0038470663130283356 Validation loss 0.011075077578425407 Accuracy 0.88232421875\n",
      "Iteration 31910 Training loss 0.004034124314785004 Validation loss 0.011058583855628967 Accuracy 0.88134765625\n",
      "Iteration 31920 Training loss 0.004646408837288618 Validation loss 0.01088693831115961 Accuracy 0.8837890625\n",
      "Iteration 31930 Training loss 0.004672906827181578 Validation loss 0.010793942958116531 Accuracy 0.88525390625\n",
      "Iteration 31940 Training loss 0.003896568203344941 Validation loss 0.010746069252490997 Accuracy 0.884765625\n",
      "Iteration 31950 Training loss 0.004347424954175949 Validation loss 0.010620578192174435 Accuracy 0.88623046875\n",
      "Iteration 31960 Training loss 0.004475774709135294 Validation loss 0.010604835115373135 Accuracy 0.88623046875\n",
      "Iteration 31970 Training loss 0.005243619438260794 Validation loss 0.010564272291958332 Accuracy 0.88671875\n",
      "Iteration 31980 Training loss 0.004397282842546701 Validation loss 0.011307724751532078 Accuracy 0.87890625\n",
      "Iteration 31990 Training loss 0.004720652941614389 Validation loss 0.011217696592211723 Accuracy 0.88037109375\n",
      "Iteration 32000 Training loss 0.0030243650544434786 Validation loss 0.010939721018075943 Accuracy 0.8828125\n",
      "Iteration 32010 Training loss 0.003833167953416705 Validation loss 0.011027218773961067 Accuracy 0.8818359375\n",
      "Iteration 32020 Training loss 0.0051598562858998775 Validation loss 0.010839283466339111 Accuracy 0.884765625\n",
      "Iteration 32030 Training loss 0.004009635653346777 Validation loss 0.010837763547897339 Accuracy 0.88427734375\n",
      "Iteration 32040 Training loss 0.0032982202246785164 Validation loss 0.0108168451115489 Accuracy 0.88427734375\n",
      "Iteration 32050 Training loss 0.0031099007464945316 Validation loss 0.010658476501703262 Accuracy 0.88671875\n",
      "Iteration 32060 Training loss 0.005254153162240982 Validation loss 0.010820351541042328 Accuracy 0.8837890625\n",
      "Iteration 32070 Training loss 0.00428271247074008 Validation loss 0.010645834729075432 Accuracy 0.88623046875\n",
      "Iteration 32080 Training loss 0.0037310628686100245 Validation loss 0.010670309886336327 Accuracy 0.88671875\n",
      "Iteration 32090 Training loss 0.004065487068146467 Validation loss 0.01103740930557251 Accuracy 0.88232421875\n",
      "Iteration 32100 Training loss 0.0032278571743518114 Validation loss 0.010674002580344677 Accuracy 0.88623046875\n",
      "Iteration 32110 Training loss 0.00591536657884717 Validation loss 0.01060159970074892 Accuracy 0.88623046875\n",
      "Iteration 32120 Training loss 0.004613233730196953 Validation loss 0.010773643851280212 Accuracy 0.884765625\n",
      "Iteration 32130 Training loss 0.004273300990462303 Validation loss 0.011098110117018223 Accuracy 0.88037109375\n",
      "Iteration 32140 Training loss 0.0032055103220045567 Validation loss 0.010970944538712502 Accuracy 0.8828125\n",
      "Iteration 32150 Training loss 0.004330867901444435 Validation loss 0.010842876508831978 Accuracy 0.88427734375\n",
      "Iteration 32160 Training loss 0.005151910707354546 Validation loss 0.011028554290533066 Accuracy 0.8828125\n",
      "Iteration 32170 Training loss 0.0042187818326056 Validation loss 0.010905536822974682 Accuracy 0.88232421875\n",
      "Iteration 32180 Training loss 0.0053675477392971516 Validation loss 0.010987752117216587 Accuracy 0.88232421875\n",
      "Iteration 32190 Training loss 0.0043939449824392796 Validation loss 0.010901431553065777 Accuracy 0.8818359375\n",
      "Iteration 32200 Training loss 0.004317714832723141 Validation loss 0.010707603767514229 Accuracy 0.8857421875\n",
      "Iteration 32210 Training loss 0.004662804771214724 Validation loss 0.010733617469668388 Accuracy 0.884765625\n",
      "Iteration 32220 Training loss 0.005536725278943777 Validation loss 0.010583207942545414 Accuracy 0.88671875\n",
      "Iteration 32230 Training loss 0.004609308205544949 Validation loss 0.010855416767299175 Accuracy 0.88427734375\n",
      "Iteration 32240 Training loss 0.00491061108186841 Validation loss 0.011078180745244026 Accuracy 0.8818359375\n",
      "Iteration 32250 Training loss 0.004225555807352066 Validation loss 0.011333678849041462 Accuracy 0.87890625\n",
      "Iteration 32260 Training loss 0.003920887131243944 Validation loss 0.01071077398955822 Accuracy 0.8857421875\n",
      "Iteration 32270 Training loss 0.004407515749335289 Validation loss 0.010547948069870472 Accuracy 0.8876953125\n",
      "Iteration 32280 Training loss 0.006479048170149326 Validation loss 0.011335753835737705 Accuracy 0.8798828125\n",
      "Iteration 32290 Training loss 0.00478538079187274 Validation loss 0.01086372323334217 Accuracy 0.88427734375\n",
      "Iteration 32300 Training loss 0.006258392706513405 Validation loss 0.011302500031888485 Accuracy 0.87939453125\n",
      "Iteration 32310 Training loss 0.0043335240334272385 Validation loss 0.011119249276816845 Accuracy 0.8818359375\n",
      "Iteration 32320 Training loss 0.0026934235356748104 Validation loss 0.010931984521448612 Accuracy 0.88232421875\n",
      "Iteration 32330 Training loss 0.0046277763321995735 Validation loss 0.010832516476511955 Accuracy 0.8837890625\n",
      "Iteration 32340 Training loss 0.004722808953374624 Validation loss 0.01085522398352623 Accuracy 0.88427734375\n",
      "Iteration 32350 Training loss 0.004049745388329029 Validation loss 0.010831057094037533 Accuracy 0.884765625\n",
      "Iteration 32360 Training loss 0.0038746611680835485 Validation loss 0.01123164501041174 Accuracy 0.87939453125\n",
      "Iteration 32370 Training loss 0.004919133614748716 Validation loss 0.011224939487874508 Accuracy 0.88037109375\n",
      "Iteration 32380 Training loss 0.003970598801970482 Validation loss 0.01081396546214819 Accuracy 0.8837890625\n",
      "Iteration 32390 Training loss 0.005789207294583321 Validation loss 0.010752072557806969 Accuracy 0.884765625\n",
      "Iteration 32400 Training loss 0.004466718528419733 Validation loss 0.011045686900615692 Accuracy 0.88232421875\n",
      "Iteration 32410 Training loss 0.004132927395403385 Validation loss 0.010875711217522621 Accuracy 0.8828125\n",
      "Iteration 32420 Training loss 0.002871833508834243 Validation loss 0.010747932828962803 Accuracy 0.884765625\n",
      "Iteration 32430 Training loss 0.0033283112570643425 Validation loss 0.010711432434618473 Accuracy 0.88525390625\n",
      "Iteration 32440 Training loss 0.00601611565798521 Validation loss 0.011057589203119278 Accuracy 0.8818359375\n",
      "Iteration 32450 Training loss 0.005084620323032141 Validation loss 0.011022454127669334 Accuracy 0.8818359375\n",
      "Iteration 32460 Training loss 0.004398518707603216 Validation loss 0.01092294417321682 Accuracy 0.88330078125\n",
      "Iteration 32470 Training loss 0.004408855922520161 Validation loss 0.01081507746130228 Accuracy 0.88427734375\n",
      "Iteration 32480 Training loss 0.004215228836983442 Validation loss 0.010681762360036373 Accuracy 0.88671875\n",
      "Iteration 32490 Training loss 0.0050824303179979324 Validation loss 0.010984404012560844 Accuracy 0.88330078125\n",
      "Iteration 32500 Training loss 0.003947945777326822 Validation loss 0.01078780461102724 Accuracy 0.884765625\n",
      "Iteration 32510 Training loss 0.00476955808699131 Validation loss 0.010938703082501888 Accuracy 0.8828125\n",
      "Iteration 32520 Training loss 0.004354530479758978 Validation loss 0.011225280351936817 Accuracy 0.880859375\n",
      "Iteration 32530 Training loss 0.0044415900483727455 Validation loss 0.010677271522581577 Accuracy 0.8857421875\n",
      "Iteration 32540 Training loss 0.003996229264885187 Validation loss 0.010909689590334892 Accuracy 0.88427734375\n",
      "Iteration 32550 Training loss 0.003482840722426772 Validation loss 0.010619720444083214 Accuracy 0.88671875\n",
      "Iteration 32560 Training loss 0.00462913466617465 Validation loss 0.0106849679723382 Accuracy 0.88671875\n",
      "Iteration 32570 Training loss 0.004471719264984131 Validation loss 0.010913119651377201 Accuracy 0.88330078125\n",
      "Iteration 32580 Training loss 0.0037855191621929407 Validation loss 0.01047446858137846 Accuracy 0.88916015625\n",
      "Iteration 32590 Training loss 0.004434030503034592 Validation loss 0.010901901870965958 Accuracy 0.88330078125\n",
      "Iteration 32600 Training loss 0.004555427003651857 Validation loss 0.011055037379264832 Accuracy 0.8818359375\n",
      "Iteration 32610 Training loss 0.004556297790259123 Validation loss 0.010918264277279377 Accuracy 0.8828125\n",
      "Iteration 32620 Training loss 0.003432704834267497 Validation loss 0.010929894633591175 Accuracy 0.8828125\n",
      "Iteration 32630 Training loss 0.005237719044089317 Validation loss 0.010951080359518528 Accuracy 0.8828125\n",
      "Iteration 32640 Training loss 0.0061220149509608746 Validation loss 0.010952351614832878 Accuracy 0.8837890625\n",
      "Iteration 32650 Training loss 0.005278280004858971 Validation loss 0.01081344299018383 Accuracy 0.88330078125\n",
      "Iteration 32660 Training loss 0.004018843173980713 Validation loss 0.011071174405515194 Accuracy 0.88134765625\n",
      "Iteration 32670 Training loss 0.0043284217827022076 Validation loss 0.010993212461471558 Accuracy 0.8818359375\n",
      "Iteration 32680 Training loss 0.004761618562042713 Validation loss 0.011094614863395691 Accuracy 0.88232421875\n",
      "Iteration 32690 Training loss 0.004370864015072584 Validation loss 0.010781490243971348 Accuracy 0.884765625\n",
      "Iteration 32700 Training loss 0.003274446353316307 Validation loss 0.011022888123989105 Accuracy 0.8818359375\n",
      "Iteration 32710 Training loss 0.004327783361077309 Validation loss 0.010973017662763596 Accuracy 0.8828125\n",
      "Iteration 32720 Training loss 0.005031533073633909 Validation loss 0.01131435763090849 Accuracy 0.8798828125\n",
      "Iteration 32730 Training loss 0.0034304093569517136 Validation loss 0.010806691832840443 Accuracy 0.884765625\n",
      "Iteration 32740 Training loss 0.005746681243181229 Validation loss 0.01109572034329176 Accuracy 0.8818359375\n",
      "Iteration 32750 Training loss 0.005088745150715113 Validation loss 0.010988202877342701 Accuracy 0.88232421875\n",
      "Iteration 32760 Training loss 0.004674089141190052 Validation loss 0.011225550435483456 Accuracy 0.8798828125\n",
      "Iteration 32770 Training loss 0.00602866243571043 Validation loss 0.011137118563055992 Accuracy 0.880859375\n",
      "Iteration 32780 Training loss 0.0038157710805535316 Validation loss 0.010998928919434547 Accuracy 0.88330078125\n",
      "Iteration 32790 Training loss 0.004067955072969198 Validation loss 0.011027832515537739 Accuracy 0.88232421875\n",
      "Iteration 32800 Training loss 0.0054297661408782005 Validation loss 0.011040979996323586 Accuracy 0.88330078125\n",
      "Iteration 32810 Training loss 0.003435928840190172 Validation loss 0.010787682607769966 Accuracy 0.88427734375\n",
      "Iteration 32820 Training loss 0.0046866838820278645 Validation loss 0.010835767723619938 Accuracy 0.8837890625\n",
      "Iteration 32830 Training loss 0.005198596976697445 Validation loss 0.011068952269852161 Accuracy 0.88232421875\n",
      "Iteration 32840 Training loss 0.0037758664693683386 Validation loss 0.011029308661818504 Accuracy 0.88232421875\n",
      "Iteration 32850 Training loss 0.003656294196844101 Validation loss 0.01070279348641634 Accuracy 0.8857421875\n",
      "Iteration 32860 Training loss 0.003252635942772031 Validation loss 0.011233613826334476 Accuracy 0.87890625\n",
      "Iteration 32870 Training loss 0.003752944292500615 Validation loss 0.01064382866024971 Accuracy 0.88623046875\n",
      "Iteration 32880 Training loss 0.004136998206377029 Validation loss 0.010831503197550774 Accuracy 0.88427734375\n",
      "Iteration 32890 Training loss 0.0038380834739655256 Validation loss 0.010724467225372791 Accuracy 0.88525390625\n",
      "Iteration 32900 Training loss 0.004727063234895468 Validation loss 0.010746709071099758 Accuracy 0.88427734375\n",
      "Iteration 32910 Training loss 0.004604453220963478 Validation loss 0.011143451556563377 Accuracy 0.880859375\n",
      "Iteration 32920 Training loss 0.004631541669368744 Validation loss 0.010793864727020264 Accuracy 0.884765625\n",
      "Iteration 32930 Training loss 0.003627423895522952 Validation loss 0.01108162198215723 Accuracy 0.8818359375\n",
      "Iteration 32940 Training loss 0.003569595282897353 Validation loss 0.010862451046705246 Accuracy 0.8837890625\n",
      "Iteration 32950 Training loss 0.004242564085870981 Validation loss 0.010836588218808174 Accuracy 0.884765625\n",
      "Iteration 32960 Training loss 0.0038186025340110064 Validation loss 0.010680194944143295 Accuracy 0.88525390625\n",
      "Iteration 32970 Training loss 0.003839680226519704 Validation loss 0.010763873346149921 Accuracy 0.88525390625\n",
      "Iteration 32980 Training loss 0.0033206010702997446 Validation loss 0.0107376454398036 Accuracy 0.88623046875\n",
      "Iteration 32990 Training loss 0.003950855694711208 Validation loss 0.010628225281834602 Accuracy 0.88671875\n",
      "Iteration 33000 Training loss 0.0024702849332243204 Validation loss 0.010701566003262997 Accuracy 0.88427734375\n",
      "Iteration 33010 Training loss 0.003848726861178875 Validation loss 0.011030180379748344 Accuracy 0.8837890625\n",
      "Iteration 33020 Training loss 0.00515358243137598 Validation loss 0.010735942982137203 Accuracy 0.88525390625\n",
      "Iteration 33030 Training loss 0.00373170361854136 Validation loss 0.01097936648875475 Accuracy 0.88232421875\n",
      "Iteration 33040 Training loss 0.0046373396180570126 Validation loss 0.011101544834673405 Accuracy 0.880859375\n",
      "Iteration 33050 Training loss 0.005802375730127096 Validation loss 0.011002451181411743 Accuracy 0.88232421875\n",
      "Iteration 33060 Training loss 0.0035446512047201395 Validation loss 0.010946481488645077 Accuracy 0.8828125\n",
      "Iteration 33070 Training loss 0.0039151799865067005 Validation loss 0.010595700703561306 Accuracy 0.88720703125\n",
      "Iteration 33080 Training loss 0.003070816630497575 Validation loss 0.010642502456903458 Accuracy 0.8857421875\n",
      "Iteration 33090 Training loss 0.006220775656402111 Validation loss 0.010798564180731773 Accuracy 0.88427734375\n",
      "Iteration 33100 Training loss 0.0028962958604097366 Validation loss 0.010961931198835373 Accuracy 0.88330078125\n",
      "Iteration 33110 Training loss 0.0028196556959301233 Validation loss 0.010956325568258762 Accuracy 0.88232421875\n",
      "Iteration 33120 Training loss 0.0028297773096710443 Validation loss 0.010607575066387653 Accuracy 0.88623046875\n",
      "Iteration 33130 Training loss 0.003209441900253296 Validation loss 0.010991591960191727 Accuracy 0.8818359375\n",
      "Iteration 33140 Training loss 0.0029535649809986353 Validation loss 0.011105154640972614 Accuracy 0.88134765625\n",
      "Iteration 33150 Training loss 0.0040715476498007774 Validation loss 0.010852055624127388 Accuracy 0.884765625\n",
      "Iteration 33160 Training loss 0.004994231276214123 Validation loss 0.011163346469402313 Accuracy 0.88134765625\n",
      "Iteration 33170 Training loss 0.005246736574918032 Validation loss 0.011387697421014309 Accuracy 0.87890625\n",
      "Iteration 33180 Training loss 0.0037345504388213158 Validation loss 0.01078566163778305 Accuracy 0.884765625\n",
      "Iteration 33190 Training loss 0.0033912425860762596 Validation loss 0.010744764469563961 Accuracy 0.884765625\n",
      "Iteration 33200 Training loss 0.003615268040448427 Validation loss 0.010980380699038506 Accuracy 0.88232421875\n",
      "Iteration 33210 Training loss 0.004025888629257679 Validation loss 0.010692574083805084 Accuracy 0.8857421875\n",
      "Iteration 33220 Training loss 0.00352338794618845 Validation loss 0.010929650627076626 Accuracy 0.8837890625\n",
      "Iteration 33230 Training loss 0.003633905667811632 Validation loss 0.010690322145819664 Accuracy 0.8857421875\n",
      "Iteration 33240 Training loss 0.003989413380622864 Validation loss 0.010916011407971382 Accuracy 0.88330078125\n",
      "Iteration 33250 Training loss 0.0045518456026911736 Validation loss 0.010860924609005451 Accuracy 0.8837890625\n",
      "Iteration 33260 Training loss 0.003612356260418892 Validation loss 0.010630689561367035 Accuracy 0.88720703125\n",
      "Iteration 33270 Training loss 0.002819906920194626 Validation loss 0.01087268814444542 Accuracy 0.8837890625\n",
      "Iteration 33280 Training loss 0.004695784766227007 Validation loss 0.010846245102584362 Accuracy 0.8837890625\n",
      "Iteration 33290 Training loss 0.0039842319674789906 Validation loss 0.01081534381955862 Accuracy 0.8837890625\n",
      "Iteration 33300 Training loss 0.005377980414777994 Validation loss 0.010784359648823738 Accuracy 0.8837890625\n",
      "Iteration 33310 Training loss 0.0042716036550700665 Validation loss 0.011012404225766659 Accuracy 0.8818359375\n",
      "Iteration 33320 Training loss 0.004128015134483576 Validation loss 0.010639316402375698 Accuracy 0.8857421875\n",
      "Iteration 33330 Training loss 0.005490334704518318 Validation loss 0.010733290575444698 Accuracy 0.8857421875\n",
      "Iteration 33340 Training loss 0.004314555320888758 Validation loss 0.010697780176997185 Accuracy 0.884765625\n",
      "Iteration 33350 Training loss 0.003423704532906413 Validation loss 0.010586815886199474 Accuracy 0.88671875\n",
      "Iteration 33360 Training loss 0.0035565050784498453 Validation loss 0.010712162591516972 Accuracy 0.884765625\n",
      "Iteration 33370 Training loss 0.0022400866728276014 Validation loss 0.010749934241175652 Accuracy 0.884765625\n",
      "Iteration 33380 Training loss 0.0033880623523145914 Validation loss 0.010682749561965466 Accuracy 0.8857421875\n",
      "Iteration 33390 Training loss 0.004072363488376141 Validation loss 0.010782182216644287 Accuracy 0.88525390625\n",
      "Iteration 33400 Training loss 0.0034743594005703926 Validation loss 0.010934404097497463 Accuracy 0.8837890625\n",
      "Iteration 33410 Training loss 0.004012517165392637 Validation loss 0.010658390820026398 Accuracy 0.88671875\n",
      "Iteration 33420 Training loss 0.003746998030692339 Validation loss 0.010522580705583096 Accuracy 0.88720703125\n",
      "Iteration 33430 Training loss 0.0034578489139676094 Validation loss 0.010604627430438995 Accuracy 0.88720703125\n",
      "Iteration 33440 Training loss 0.00491076335310936 Validation loss 0.010810215026140213 Accuracy 0.88427734375\n",
      "Iteration 33450 Training loss 0.0031140141654759645 Validation loss 0.011228105053305626 Accuracy 0.87939453125\n",
      "Iteration 33460 Training loss 0.005564397666603327 Validation loss 0.010912366211414337 Accuracy 0.8828125\n",
      "Iteration 33470 Training loss 0.003958546556532383 Validation loss 0.010699967853724957 Accuracy 0.884765625\n",
      "Iteration 33480 Training loss 0.00397328520193696 Validation loss 0.010783437639474869 Accuracy 0.8837890625\n",
      "Iteration 33490 Training loss 0.004665007349103689 Validation loss 0.010900049470365047 Accuracy 0.88330078125\n",
      "Iteration 33500 Training loss 0.0027594028506428003 Validation loss 0.010959292761981487 Accuracy 0.8837890625\n",
      "Iteration 33510 Training loss 0.0031212009489536285 Validation loss 0.010982140898704529 Accuracy 0.88232421875\n",
      "Iteration 33520 Training loss 0.004973439034074545 Validation loss 0.010950585827231407 Accuracy 0.88232421875\n",
      "Iteration 33530 Training loss 0.004183343145996332 Validation loss 0.010821832343935966 Accuracy 0.88330078125\n",
      "Iteration 33540 Training loss 0.00406405096873641 Validation loss 0.010776513256132603 Accuracy 0.884765625\n",
      "Iteration 33550 Training loss 0.0049546463415026665 Validation loss 0.010931716300547123 Accuracy 0.88330078125\n",
      "Iteration 33560 Training loss 0.00440972251817584 Validation loss 0.010842653922736645 Accuracy 0.88427734375\n",
      "Iteration 33570 Training loss 0.00421183230355382 Validation loss 0.010635441169142723 Accuracy 0.88671875\n",
      "Iteration 33580 Training loss 0.005340487230569124 Validation loss 0.01090560108423233 Accuracy 0.8828125\n",
      "Iteration 33590 Training loss 0.003312312066555023 Validation loss 0.010805658996105194 Accuracy 0.88525390625\n",
      "Iteration 33600 Training loss 0.0039109098725020885 Validation loss 0.010908647440373898 Accuracy 0.8837890625\n",
      "Iteration 33610 Training loss 0.003929703030735254 Validation loss 0.010831701569259167 Accuracy 0.8837890625\n",
      "Iteration 33620 Training loss 0.003934184554964304 Validation loss 0.010723410174250603 Accuracy 0.8857421875\n",
      "Iteration 33630 Training loss 0.00501468637958169 Validation loss 0.010816257447004318 Accuracy 0.8837890625\n",
      "Iteration 33640 Training loss 0.005272608250379562 Validation loss 0.01111269649118185 Accuracy 0.880859375\n",
      "Iteration 33650 Training loss 0.004048841539770365 Validation loss 0.010752038098871708 Accuracy 0.88525390625\n",
      "Iteration 33660 Training loss 0.0039907703176140785 Validation loss 0.010730346664786339 Accuracy 0.88525390625\n",
      "Iteration 33670 Training loss 0.004992042668163776 Validation loss 0.011036170646548271 Accuracy 0.8818359375\n",
      "Iteration 33680 Training loss 0.003280091565102339 Validation loss 0.010715551674365997 Accuracy 0.88623046875\n",
      "Iteration 33690 Training loss 0.004729371517896652 Validation loss 0.010923155583441257 Accuracy 0.88330078125\n",
      "Iteration 33700 Training loss 0.0033117153216153383 Validation loss 0.010649400763213634 Accuracy 0.8857421875\n",
      "Iteration 33710 Training loss 0.003945090342313051 Validation loss 0.010694246739149094 Accuracy 0.8857421875\n",
      "Iteration 33720 Training loss 0.003367058001458645 Validation loss 0.010661757551133633 Accuracy 0.88623046875\n",
      "Iteration 33730 Training loss 0.0062856064178049564 Validation loss 0.011172765865921974 Accuracy 0.8818359375\n",
      "Iteration 33740 Training loss 0.0036620476748794317 Validation loss 0.010825887322425842 Accuracy 0.88427734375\n",
      "Iteration 33750 Training loss 0.003660490270704031 Validation loss 0.010839630849659443 Accuracy 0.8837890625\n",
      "Iteration 33760 Training loss 0.004040485247969627 Validation loss 0.0106351962313056 Accuracy 0.88623046875\n",
      "Iteration 33770 Training loss 0.003917057532817125 Validation loss 0.0108854491263628 Accuracy 0.8828125\n",
      "Iteration 33780 Training loss 0.004948843736201525 Validation loss 0.010818704962730408 Accuracy 0.884765625\n",
      "Iteration 33790 Training loss 0.004474584013223648 Validation loss 0.011019296944141388 Accuracy 0.8818359375\n",
      "Iteration 33800 Training loss 0.004848725162446499 Validation loss 0.01060874480754137 Accuracy 0.88818359375\n",
      "Iteration 33810 Training loss 0.004237025510519743 Validation loss 0.01072351448237896 Accuracy 0.8857421875\n",
      "Iteration 33820 Training loss 0.004263861104846001 Validation loss 0.010829446837306023 Accuracy 0.88427734375\n",
      "Iteration 33830 Training loss 0.0051608942449092865 Validation loss 0.01068209670484066 Accuracy 0.8857421875\n",
      "Iteration 33840 Training loss 0.004095019306987524 Validation loss 0.01072494313120842 Accuracy 0.88427734375\n",
      "Iteration 33850 Training loss 0.0028011936228722334 Validation loss 0.010833901353180408 Accuracy 0.88427734375\n",
      "Iteration 33860 Training loss 0.004279653541743755 Validation loss 0.010736840777099133 Accuracy 0.884765625\n",
      "Iteration 33870 Training loss 0.003970600198954344 Validation loss 0.011204764246940613 Accuracy 0.88037109375\n",
      "Iteration 33880 Training loss 0.0027757962234318256 Validation loss 0.010641010478138924 Accuracy 0.8857421875\n",
      "Iteration 33890 Training loss 0.004497618414461613 Validation loss 0.010746045038104057 Accuracy 0.8857421875\n",
      "Iteration 33900 Training loss 0.003988051787018776 Validation loss 0.01073980238288641 Accuracy 0.88525390625\n",
      "Iteration 33910 Training loss 0.002602776512503624 Validation loss 0.010684389621019363 Accuracy 0.88525390625\n",
      "Iteration 33920 Training loss 0.0033453062642365694 Validation loss 0.010843398049473763 Accuracy 0.8837890625\n",
      "Iteration 33930 Training loss 0.00425497954711318 Validation loss 0.011124026030302048 Accuracy 0.880859375\n",
      "Iteration 33940 Training loss 0.004085863009095192 Validation loss 0.010922165587544441 Accuracy 0.88427734375\n",
      "Iteration 33950 Training loss 0.004977871663868427 Validation loss 0.011244389228522778 Accuracy 0.88037109375\n",
      "Iteration 33960 Training loss 0.004596530459821224 Validation loss 0.010808886028826237 Accuracy 0.88427734375\n",
      "Iteration 33970 Training loss 0.003196943551301956 Validation loss 0.010860572569072247 Accuracy 0.88427734375\n",
      "Iteration 33980 Training loss 0.0043840110301971436 Validation loss 0.010746188461780548 Accuracy 0.88525390625\n",
      "Iteration 33990 Training loss 0.004258597269654274 Validation loss 0.011094927787780762 Accuracy 0.88232421875\n",
      "Iteration 34000 Training loss 0.0028519846964627504 Validation loss 0.010766090825200081 Accuracy 0.88525390625\n",
      "Iteration 34010 Training loss 0.0036055666860193014 Validation loss 0.010777665302157402 Accuracy 0.884765625\n",
      "Iteration 34020 Training loss 0.0017296220175921917 Validation loss 0.010636847466230392 Accuracy 0.88623046875\n",
      "Iteration 34030 Training loss 0.00436015147715807 Validation loss 0.010872932150959969 Accuracy 0.8837890625\n",
      "Iteration 34040 Training loss 0.00415085582062602 Validation loss 0.010770458728075027 Accuracy 0.8837890625\n",
      "Iteration 34050 Training loss 0.0037489801179617643 Validation loss 0.011028881184756756 Accuracy 0.88232421875\n",
      "Iteration 34060 Training loss 0.0027568244840949774 Validation loss 0.010524805635213852 Accuracy 0.88818359375\n",
      "Iteration 34070 Training loss 0.003652723506093025 Validation loss 0.01081712357699871 Accuracy 0.88427734375\n",
      "Iteration 34080 Training loss 0.00505864666774869 Validation loss 0.010964835993945599 Accuracy 0.88330078125\n",
      "Iteration 34090 Training loss 0.003291698405519128 Validation loss 0.01071025338023901 Accuracy 0.8857421875\n",
      "Iteration 34100 Training loss 0.0033863417338579893 Validation loss 0.010668774135410786 Accuracy 0.88427734375\n",
      "Iteration 34110 Training loss 0.0031702760607004166 Validation loss 0.010485044680535793 Accuracy 0.88720703125\n",
      "Iteration 34120 Training loss 0.004793370608240366 Validation loss 0.010672868229448795 Accuracy 0.88525390625\n",
      "Iteration 34130 Training loss 0.004195982590317726 Validation loss 0.010607192292809486 Accuracy 0.88671875\n",
      "Iteration 34140 Training loss 0.004721298813819885 Validation loss 0.011131828650832176 Accuracy 0.88037109375\n",
      "Iteration 34150 Training loss 0.003983092028647661 Validation loss 0.010526093654334545 Accuracy 0.88720703125\n",
      "Iteration 34160 Training loss 0.0034550291020423174 Validation loss 0.010598180815577507 Accuracy 0.88720703125\n",
      "Iteration 34170 Training loss 0.004159938544034958 Validation loss 0.010663808323442936 Accuracy 0.88525390625\n",
      "Iteration 34180 Training loss 0.0037573804147541523 Validation loss 0.010661930777132511 Accuracy 0.88671875\n",
      "Iteration 34190 Training loss 0.004542081151157618 Validation loss 0.010516955517232418 Accuracy 0.88720703125\n",
      "Iteration 34200 Training loss 0.001858071656897664 Validation loss 0.01075170747935772 Accuracy 0.88427734375\n",
      "Iteration 34210 Training loss 0.004519709385931492 Validation loss 0.010605229996144772 Accuracy 0.88671875\n",
      "Iteration 34220 Training loss 0.003032634500414133 Validation loss 0.010625412687659264 Accuracy 0.8857421875\n",
      "Iteration 34230 Training loss 0.003985768184065819 Validation loss 0.010761383920907974 Accuracy 0.88525390625\n",
      "Iteration 34240 Training loss 0.004193683620542288 Validation loss 0.010441751219332218 Accuracy 0.88916015625\n",
      "Iteration 34250 Training loss 0.00462412741035223 Validation loss 0.01086872536689043 Accuracy 0.8828125\n",
      "Iteration 34260 Training loss 0.0032750011887401342 Validation loss 0.010779890231788158 Accuracy 0.884765625\n",
      "Iteration 34270 Training loss 0.004586648195981979 Validation loss 0.010794973000884056 Accuracy 0.88427734375\n",
      "Iteration 34280 Training loss 0.003568991320207715 Validation loss 0.010900571011006832 Accuracy 0.88330078125\n",
      "Iteration 34290 Training loss 0.0024436863604933023 Validation loss 0.010609986260533333 Accuracy 0.88623046875\n",
      "Iteration 34300 Training loss 0.0033922067377716303 Validation loss 0.010955603793263435 Accuracy 0.8828125\n",
      "Iteration 34310 Training loss 0.004014408215880394 Validation loss 0.01072665210813284 Accuracy 0.884765625\n",
      "Iteration 34320 Training loss 0.003992513753473759 Validation loss 0.01090798806399107 Accuracy 0.88330078125\n",
      "Iteration 34330 Training loss 0.00444211158901453 Validation loss 0.010740506462752819 Accuracy 0.8857421875\n",
      "Iteration 34340 Training loss 0.0037417407147586346 Validation loss 0.010817743837833405 Accuracy 0.88525390625\n",
      "Iteration 34350 Training loss 0.0027181352488696575 Validation loss 0.011095789261162281 Accuracy 0.8818359375\n",
      "Iteration 34360 Training loss 0.003975201863795519 Validation loss 0.011006074957549572 Accuracy 0.88232421875\n",
      "Iteration 34370 Training loss 0.00428090849891305 Validation loss 0.010656053200364113 Accuracy 0.88720703125\n",
      "Iteration 34380 Training loss 0.0040893107652664185 Validation loss 0.010834538377821445 Accuracy 0.88427734375\n",
      "Iteration 34390 Training loss 0.0050042057409882545 Validation loss 0.010896110907196999 Accuracy 0.8828125\n",
      "Iteration 34400 Training loss 0.0040376088581979275 Validation loss 0.010442324914038181 Accuracy 0.88818359375\n",
      "Iteration 34410 Training loss 0.0038953889161348343 Validation loss 0.01076117530465126 Accuracy 0.8857421875\n",
      "Iteration 34420 Training loss 0.0038531608879566193 Validation loss 0.010653247125446796 Accuracy 0.88671875\n",
      "Iteration 34430 Training loss 0.0032801583874970675 Validation loss 0.010821598581969738 Accuracy 0.884765625\n",
      "Iteration 34440 Training loss 0.0048348503187298775 Validation loss 0.01056670118123293 Accuracy 0.88671875\n",
      "Iteration 34450 Training loss 0.003610792802646756 Validation loss 0.010480890981853008 Accuracy 0.8876953125\n",
      "Iteration 34460 Training loss 0.006085254717618227 Validation loss 0.01067467499524355 Accuracy 0.88671875\n",
      "Iteration 34470 Training loss 0.005653237458318472 Validation loss 0.01096299383789301 Accuracy 0.8837890625\n",
      "Iteration 34480 Training loss 0.003356215776875615 Validation loss 0.01052769459784031 Accuracy 0.88671875\n",
      "Iteration 34490 Training loss 0.004163626115769148 Validation loss 0.010639604181051254 Accuracy 0.88671875\n",
      "Iteration 34500 Training loss 0.001967987045645714 Validation loss 0.010869953781366348 Accuracy 0.88427734375\n",
      "Iteration 34510 Training loss 0.003410096513107419 Validation loss 0.010427983477711678 Accuracy 0.8876953125\n",
      "Iteration 34520 Training loss 0.004115809220820665 Validation loss 0.010609985329210758 Accuracy 0.88623046875\n",
      "Iteration 34530 Training loss 0.004317977465689182 Validation loss 0.010569603182375431 Accuracy 0.88623046875\n",
      "Iteration 34540 Training loss 0.005076994653791189 Validation loss 0.010714193806052208 Accuracy 0.8857421875\n",
      "Iteration 34550 Training loss 0.003717706073075533 Validation loss 0.010690978728234768 Accuracy 0.8857421875\n",
      "Iteration 34560 Training loss 0.005011768080294132 Validation loss 0.010934468358755112 Accuracy 0.8828125\n",
      "Iteration 34570 Training loss 0.004589442629367113 Validation loss 0.01077751163393259 Accuracy 0.88427734375\n",
      "Iteration 34580 Training loss 0.004808942321687937 Validation loss 0.010731556452810764 Accuracy 0.88525390625\n",
      "Iteration 34590 Training loss 0.004740843083709478 Validation loss 0.010504860430955887 Accuracy 0.88720703125\n",
      "Iteration 34600 Training loss 0.00416801730170846 Validation loss 0.010722805745899677 Accuracy 0.8857421875\n",
      "Iteration 34610 Training loss 0.005309318657964468 Validation loss 0.010738274082541466 Accuracy 0.8857421875\n",
      "Iteration 34620 Training loss 0.003459185129031539 Validation loss 0.010609938763082027 Accuracy 0.88671875\n",
      "Iteration 34630 Training loss 0.0034425207413733006 Validation loss 0.010495198890566826 Accuracy 0.88720703125\n",
      "Iteration 34640 Training loss 0.002729948377236724 Validation loss 0.010597281157970428 Accuracy 0.88671875\n",
      "Iteration 34650 Training loss 0.004304047673940659 Validation loss 0.010657568462193012 Accuracy 0.88671875\n",
      "Iteration 34660 Training loss 0.0035735988058149815 Validation loss 0.01092507317662239 Accuracy 0.88427734375\n",
      "Iteration 34670 Training loss 0.004146484192460775 Validation loss 0.01076589897274971 Accuracy 0.884765625\n",
      "Iteration 34680 Training loss 0.004020096268504858 Validation loss 0.010549260303378105 Accuracy 0.88623046875\n",
      "Iteration 34690 Training loss 0.004308838397264481 Validation loss 0.010702072642743587 Accuracy 0.88525390625\n",
      "Iteration 34700 Training loss 0.004919493570923805 Validation loss 0.010620733723044395 Accuracy 0.88623046875\n",
      "Iteration 34710 Training loss 0.004176286514848471 Validation loss 0.010652289725840092 Accuracy 0.8857421875\n",
      "Iteration 34720 Training loss 0.005780432373285294 Validation loss 0.010713648051023483 Accuracy 0.88427734375\n",
      "Iteration 34730 Training loss 0.003936357796192169 Validation loss 0.01089477725327015 Accuracy 0.8837890625\n",
      "Iteration 34740 Training loss 0.003492276882752776 Validation loss 0.010961857624351978 Accuracy 0.88330078125\n",
      "Iteration 34750 Training loss 0.003210330381989479 Validation loss 0.010633722878992558 Accuracy 0.88671875\n",
      "Iteration 34760 Training loss 0.0035892436280846596 Validation loss 0.010918903164565563 Accuracy 0.8828125\n",
      "Iteration 34770 Training loss 0.0035944196861237288 Validation loss 0.010764162056148052 Accuracy 0.884765625\n",
      "Iteration 34780 Training loss 0.004044190514832735 Validation loss 0.01085766963660717 Accuracy 0.88330078125\n",
      "Iteration 34790 Training loss 0.0030972864478826523 Validation loss 0.010578930377960205 Accuracy 0.88720703125\n",
      "Iteration 34800 Training loss 0.0044835819862782955 Validation loss 0.010903632268309593 Accuracy 0.8828125\n",
      "Iteration 34810 Training loss 0.003939968068152666 Validation loss 0.010705915279686451 Accuracy 0.8857421875\n",
      "Iteration 34820 Training loss 0.002311276737600565 Validation loss 0.010807658545672894 Accuracy 0.8837890625\n",
      "Iteration 34830 Training loss 0.004708480555564165 Validation loss 0.010857131332159042 Accuracy 0.8837890625\n",
      "Iteration 34840 Training loss 0.0028771287761628628 Validation loss 0.010608497075736523 Accuracy 0.88671875\n",
      "Iteration 34850 Training loss 0.0038815808948129416 Validation loss 0.010982723906636238 Accuracy 0.8828125\n",
      "Iteration 34860 Training loss 0.0033305182587355375 Validation loss 0.010748068802058697 Accuracy 0.8857421875\n",
      "Iteration 34870 Training loss 0.004664006642997265 Validation loss 0.010744498111307621 Accuracy 0.88525390625\n",
      "Iteration 34880 Training loss 0.0038612298667430878 Validation loss 0.010767120867967606 Accuracy 0.884765625\n",
      "Iteration 34890 Training loss 0.0039131250232458115 Validation loss 0.010638583451509476 Accuracy 0.88525390625\n",
      "Iteration 34900 Training loss 0.005134129896759987 Validation loss 0.010812672786414623 Accuracy 0.8857421875\n",
      "Iteration 34910 Training loss 0.0021646814420819283 Validation loss 0.010512595064938068 Accuracy 0.88818359375\n",
      "Iteration 34920 Training loss 0.003420647932216525 Validation loss 0.010934839025139809 Accuracy 0.88330078125\n",
      "Iteration 34930 Training loss 0.005173624958842993 Validation loss 0.011104047298431396 Accuracy 0.88037109375\n",
      "Iteration 34940 Training loss 0.004802670329809189 Validation loss 0.01128776092082262 Accuracy 0.87939453125\n",
      "Iteration 34950 Training loss 0.0044008283875882626 Validation loss 0.010984046384692192 Accuracy 0.8828125\n",
      "Iteration 34960 Training loss 0.0032494785264134407 Validation loss 0.01063966192305088 Accuracy 0.8857421875\n",
      "Iteration 34970 Training loss 0.004094126168638468 Validation loss 0.01080322265625 Accuracy 0.884765625\n",
      "Iteration 34980 Training loss 0.002725728088989854 Validation loss 0.01106314454227686 Accuracy 0.88232421875\n",
      "Iteration 34990 Training loss 0.003907849546521902 Validation loss 0.010877592489123344 Accuracy 0.88427734375\n",
      "Iteration 35000 Training loss 0.003981495276093483 Validation loss 0.01068701408803463 Accuracy 0.88623046875\n",
      "Iteration 35010 Training loss 0.0036080130375921726 Validation loss 0.010718139819800854 Accuracy 0.88623046875\n",
      "Iteration 35020 Training loss 0.00280218874104321 Validation loss 0.010534345172345638 Accuracy 0.88818359375\n",
      "Iteration 35030 Training loss 0.003842848353087902 Validation loss 0.010842076502740383 Accuracy 0.88427734375\n",
      "Iteration 35040 Training loss 0.00641426257789135 Validation loss 0.011155541986227036 Accuracy 0.88134765625\n",
      "Iteration 35050 Training loss 0.003977867774665356 Validation loss 0.011250981129705906 Accuracy 0.88037109375\n",
      "Iteration 35060 Training loss 0.0031793073285371065 Validation loss 0.010710970498621464 Accuracy 0.884765625\n",
      "Iteration 35070 Training loss 0.0036270199343562126 Validation loss 0.010585074312984943 Accuracy 0.88623046875\n",
      "Iteration 35080 Training loss 0.003745259018614888 Validation loss 0.010890553705394268 Accuracy 0.8828125\n",
      "Iteration 35090 Training loss 0.002470561536028981 Validation loss 0.01086230669170618 Accuracy 0.8837890625\n",
      "Iteration 35100 Training loss 0.003844399005174637 Validation loss 0.01087123341858387 Accuracy 0.8837890625\n",
      "Iteration 35110 Training loss 0.002966483822092414 Validation loss 0.010751388035714626 Accuracy 0.884765625\n",
      "Iteration 35120 Training loss 0.0035536193754523993 Validation loss 0.010607787407934666 Accuracy 0.88720703125\n",
      "Iteration 35130 Training loss 0.00529605895280838 Validation loss 0.010611995123326778 Accuracy 0.88671875\n",
      "Iteration 35140 Training loss 0.004225662909448147 Validation loss 0.010425186716020107 Accuracy 0.888671875\n",
      "Iteration 35150 Training loss 0.004727164749056101 Validation loss 0.010662255808711052 Accuracy 0.88623046875\n",
      "Iteration 35160 Training loss 0.003032383508980274 Validation loss 0.010470635257661343 Accuracy 0.8876953125\n",
      "Iteration 35170 Training loss 0.004634442739188671 Validation loss 0.01043753232806921 Accuracy 0.888671875\n",
      "Iteration 35180 Training loss 0.004464397206902504 Validation loss 0.010544857010245323 Accuracy 0.88623046875\n",
      "Iteration 35190 Training loss 0.0053275274112820625 Validation loss 0.010654345154762268 Accuracy 0.88671875\n",
      "Iteration 35200 Training loss 0.003310216823592782 Validation loss 0.010842653922736645 Accuracy 0.8837890625\n",
      "Iteration 35210 Training loss 0.0031639747321605682 Validation loss 0.01067531667649746 Accuracy 0.88671875\n",
      "Iteration 35220 Training loss 0.003978177439421415 Validation loss 0.010682487860321999 Accuracy 0.88525390625\n",
      "Iteration 35230 Training loss 0.0035616550594568253 Validation loss 0.010660006664693356 Accuracy 0.88623046875\n",
      "Iteration 35240 Training loss 0.003831845475360751 Validation loss 0.01059463620185852 Accuracy 0.88623046875\n",
      "Iteration 35250 Training loss 0.0029408843256533146 Validation loss 0.0105547234416008 Accuracy 0.8876953125\n",
      "Iteration 35260 Training loss 0.0040196385234594345 Validation loss 0.010698825120925903 Accuracy 0.8857421875\n",
      "Iteration 35270 Training loss 0.004745462443679571 Validation loss 0.01077356655150652 Accuracy 0.884765625\n",
      "Iteration 35280 Training loss 0.0028672395274043083 Validation loss 0.01065797172486782 Accuracy 0.88525390625\n",
      "Iteration 35290 Training loss 0.003915665205568075 Validation loss 0.010874169878661633 Accuracy 0.88427734375\n",
      "Iteration 35300 Training loss 0.002381540834903717 Validation loss 0.010798361152410507 Accuracy 0.88427734375\n",
      "Iteration 35310 Training loss 0.003231555223464966 Validation loss 0.01090123038738966 Accuracy 0.8837890625\n",
      "Iteration 35320 Training loss 0.0034168274141848087 Validation loss 0.01069610845297575 Accuracy 0.88623046875\n",
      "Iteration 35330 Training loss 0.004374855197966099 Validation loss 0.010688964277505875 Accuracy 0.8857421875\n",
      "Iteration 35340 Training loss 0.0033879552502185106 Validation loss 0.011181759648025036 Accuracy 0.88037109375\n",
      "Iteration 35350 Training loss 0.0035274671390652657 Validation loss 0.01053603459149599 Accuracy 0.8857421875\n",
      "Iteration 35360 Training loss 0.0034938398748636246 Validation loss 0.010507708415389061 Accuracy 0.88720703125\n",
      "Iteration 35370 Training loss 0.003633936634287238 Validation loss 0.010785551741719246 Accuracy 0.884765625\n",
      "Iteration 35380 Training loss 0.004308589734137058 Validation loss 0.010527340695261955 Accuracy 0.88720703125\n",
      "Iteration 35390 Training loss 0.003659340785816312 Validation loss 0.010855874046683311 Accuracy 0.8837890625\n",
      "Iteration 35400 Training loss 0.004242760129272938 Validation loss 0.010735002346336842 Accuracy 0.88427734375\n",
      "Iteration 35410 Training loss 0.0028201246168464422 Validation loss 0.010495348833501339 Accuracy 0.8876953125\n",
      "Iteration 35420 Training loss 0.0035618040710687637 Validation loss 0.010648906230926514 Accuracy 0.88671875\n",
      "Iteration 35430 Training loss 0.004679340403527021 Validation loss 0.010602914728224277 Accuracy 0.88623046875\n",
      "Iteration 35440 Training loss 0.003342516254633665 Validation loss 0.010743907652795315 Accuracy 0.8857421875\n",
      "Iteration 35450 Training loss 0.004330274648964405 Validation loss 0.010674314573407173 Accuracy 0.88671875\n",
      "Iteration 35460 Training loss 0.003273705020546913 Validation loss 0.010626726783812046 Accuracy 0.88623046875\n",
      "Iteration 35470 Training loss 0.00408944021910429 Validation loss 0.010707796551287174 Accuracy 0.8837890625\n",
      "Iteration 35480 Training loss 0.003012055065482855 Validation loss 0.010656788945198059 Accuracy 0.88623046875\n",
      "Iteration 35490 Training loss 0.004463179036974907 Validation loss 0.010697314515709877 Accuracy 0.884765625\n",
      "Iteration 35500 Training loss 0.0031552205327898264 Validation loss 0.010452832095324993 Accuracy 0.88720703125\n",
      "Iteration 35510 Training loss 0.004248297773301601 Validation loss 0.010600988753139973 Accuracy 0.88623046875\n",
      "Iteration 35520 Training loss 0.003913385793566704 Validation loss 0.010506799444556236 Accuracy 0.88720703125\n",
      "Iteration 35530 Training loss 0.003164831083267927 Validation loss 0.010568317957222462 Accuracy 0.88623046875\n",
      "Iteration 35540 Training loss 0.00512858759611845 Validation loss 0.010705034248530865 Accuracy 0.884765625\n",
      "Iteration 35550 Training loss 0.002965874271467328 Validation loss 0.01047747302800417 Accuracy 0.8876953125\n",
      "Iteration 35560 Training loss 0.0036661310587078333 Validation loss 0.010669810697436333 Accuracy 0.88525390625\n",
      "Iteration 35570 Training loss 0.0030068291816860437 Validation loss 0.010879010893404484 Accuracy 0.88330078125\n",
      "Iteration 35580 Training loss 0.003227442502975464 Validation loss 0.010730606503784657 Accuracy 0.88623046875\n",
      "Iteration 35590 Training loss 0.004942281637340784 Validation loss 0.01073923148214817 Accuracy 0.88525390625\n",
      "Iteration 35600 Training loss 0.0043551078997552395 Validation loss 0.010582407005131245 Accuracy 0.88671875\n",
      "Iteration 35610 Training loss 0.004343243781477213 Validation loss 0.010911609046161175 Accuracy 0.8837890625\n",
      "Iteration 35620 Training loss 0.004098216537386179 Validation loss 0.011025781743228436 Accuracy 0.88232421875\n",
      "Iteration 35630 Training loss 0.003904384560883045 Validation loss 0.010722331702709198 Accuracy 0.884765625\n",
      "Iteration 35640 Training loss 0.003676195628941059 Validation loss 0.0110397320240736 Accuracy 0.8818359375\n",
      "Iteration 35650 Training loss 0.004716707859188318 Validation loss 0.01089246105402708 Accuracy 0.8837890625\n",
      "Iteration 35660 Training loss 0.00428858632221818 Validation loss 0.010914078913629055 Accuracy 0.8837890625\n",
      "Iteration 35670 Training loss 0.003790342016145587 Validation loss 0.010810400359332561 Accuracy 0.8837890625\n",
      "Iteration 35680 Training loss 0.004402414429932833 Validation loss 0.010782519355416298 Accuracy 0.88427734375\n",
      "Iteration 35690 Training loss 0.004254714585840702 Validation loss 0.010564088821411133 Accuracy 0.88623046875\n",
      "Iteration 35700 Training loss 0.004516506567597389 Validation loss 0.010675751604139805 Accuracy 0.88525390625\n",
      "Iteration 35710 Training loss 0.003223998239263892 Validation loss 0.010823879390954971 Accuracy 0.88330078125\n",
      "Iteration 35720 Training loss 0.004135398659855127 Validation loss 0.0107726464048028 Accuracy 0.8837890625\n",
      "Iteration 35730 Training loss 0.0037541782949119806 Validation loss 0.010815690271556377 Accuracy 0.88427734375\n",
      "Iteration 35740 Training loss 0.003243156475946307 Validation loss 0.010712952353060246 Accuracy 0.88525390625\n",
      "Iteration 35750 Training loss 0.0035531367175281048 Validation loss 0.010860729962587357 Accuracy 0.8837890625\n",
      "Iteration 35760 Training loss 0.003567994339391589 Validation loss 0.010809347964823246 Accuracy 0.884765625\n",
      "Iteration 35770 Training loss 0.003509103087708354 Validation loss 0.010738871991634369 Accuracy 0.88525390625\n",
      "Iteration 35780 Training loss 0.002556792227551341 Validation loss 0.010679779574275017 Accuracy 0.88623046875\n",
      "Iteration 35790 Training loss 0.0034927462693303823 Validation loss 0.011097094044089317 Accuracy 0.8818359375\n",
      "Iteration 35800 Training loss 0.00456753745675087 Validation loss 0.010815047658979893 Accuracy 0.8837890625\n",
      "Iteration 35810 Training loss 0.0046477061696350574 Validation loss 0.010682868771255016 Accuracy 0.88525390625\n",
      "Iteration 35820 Training loss 0.0033061103895306587 Validation loss 0.010949073359370232 Accuracy 0.8837890625\n",
      "Iteration 35830 Training loss 0.0036126377526670694 Validation loss 0.010448700748383999 Accuracy 0.88916015625\n",
      "Iteration 35840 Training loss 0.0023154255468398333 Validation loss 0.01053767092525959 Accuracy 0.8876953125\n",
      "Iteration 35850 Training loss 0.0031691938638687134 Validation loss 0.01044089812785387 Accuracy 0.88818359375\n",
      "Iteration 35860 Training loss 0.0027825774159282446 Validation loss 0.010731462389230728 Accuracy 0.88427734375\n",
      "Iteration 35870 Training loss 0.003668418852612376 Validation loss 0.010799895040690899 Accuracy 0.88330078125\n",
      "Iteration 35880 Training loss 0.00515328673645854 Validation loss 0.010698813013732433 Accuracy 0.8857421875\n",
      "Iteration 35890 Training loss 0.002477058907970786 Validation loss 0.010473818518221378 Accuracy 0.88720703125\n",
      "Iteration 35900 Training loss 0.0043205395340919495 Validation loss 0.010753671638667583 Accuracy 0.884765625\n",
      "Iteration 35910 Training loss 0.0032395769376307726 Validation loss 0.010964426212012768 Accuracy 0.88232421875\n",
      "Iteration 35920 Training loss 0.004010110627859831 Validation loss 0.010741855017840862 Accuracy 0.88525390625\n",
      "Iteration 35930 Training loss 0.0024640585761517286 Validation loss 0.010532980784773827 Accuracy 0.88671875\n",
      "Iteration 35940 Training loss 0.002332933945581317 Validation loss 0.010450766421854496 Accuracy 0.88818359375\n",
      "Iteration 35950 Training loss 0.00326215079985559 Validation loss 0.010395695455372334 Accuracy 0.88818359375\n",
      "Iteration 35960 Training loss 0.004498028662055731 Validation loss 0.010735532268881798 Accuracy 0.88525390625\n",
      "Iteration 35970 Training loss 0.003636040957644582 Validation loss 0.010647078976035118 Accuracy 0.88623046875\n",
      "Iteration 35980 Training loss 0.002964960178360343 Validation loss 0.010583302937448025 Accuracy 0.88623046875\n",
      "Iteration 35990 Training loss 0.004759072791785002 Validation loss 0.010514536872506142 Accuracy 0.88818359375\n",
      "Iteration 36000 Training loss 0.0031433654949069023 Validation loss 0.010600373148918152 Accuracy 0.8876953125\n",
      "Iteration 36010 Training loss 0.002913164673373103 Validation loss 0.010679539293050766 Accuracy 0.88525390625\n",
      "Iteration 36020 Training loss 0.002736894879490137 Validation loss 0.01075492613017559 Accuracy 0.88427734375\n",
      "Iteration 36030 Training loss 0.005297939293086529 Validation loss 0.011054903268814087 Accuracy 0.8828125\n",
      "Iteration 36040 Training loss 0.0038161328993737698 Validation loss 0.011012006551027298 Accuracy 0.88134765625\n",
      "Iteration 36050 Training loss 0.00426826998591423 Validation loss 0.010664111003279686 Accuracy 0.88671875\n",
      "Iteration 36060 Training loss 0.004576284438371658 Validation loss 0.010954853147268295 Accuracy 0.88232421875\n",
      "Iteration 36070 Training loss 0.004718192853033543 Validation loss 0.010662108659744263 Accuracy 0.8857421875\n",
      "Iteration 36080 Training loss 0.001706258743070066 Validation loss 0.01060121413320303 Accuracy 0.88720703125\n",
      "Iteration 36090 Training loss 0.004346083849668503 Validation loss 0.010720604099333286 Accuracy 0.8857421875\n",
      "Iteration 36100 Training loss 0.0040605501271784306 Validation loss 0.010774074122309685 Accuracy 0.88427734375\n",
      "Iteration 36110 Training loss 0.004487066064029932 Validation loss 0.01092839241027832 Accuracy 0.8828125\n",
      "Iteration 36120 Training loss 0.004545832052826881 Validation loss 0.010882368311285973 Accuracy 0.8828125\n",
      "Iteration 36130 Training loss 0.003543820697814226 Validation loss 0.01064027938991785 Accuracy 0.884765625\n",
      "Iteration 36140 Training loss 0.00334048829972744 Validation loss 0.010553374886512756 Accuracy 0.88623046875\n",
      "Iteration 36150 Training loss 0.0036991785746067762 Validation loss 0.01085322443395853 Accuracy 0.8837890625\n",
      "Iteration 36160 Training loss 0.002913795178756118 Validation loss 0.010701720602810383 Accuracy 0.8857421875\n",
      "Iteration 36170 Training loss 0.0021013172809034586 Validation loss 0.010498707182705402 Accuracy 0.88818359375\n",
      "Iteration 36180 Training loss 0.003141904715448618 Validation loss 0.010562984272837639 Accuracy 0.8876953125\n",
      "Iteration 36190 Training loss 0.0038991656620055437 Validation loss 0.010567864403128624 Accuracy 0.8876953125\n",
      "Iteration 36200 Training loss 0.002814892213791609 Validation loss 0.01072164811193943 Accuracy 0.8857421875\n",
      "Iteration 36210 Training loss 0.003918673377484083 Validation loss 0.010664449073374271 Accuracy 0.88671875\n",
      "Iteration 36220 Training loss 0.00341430539265275 Validation loss 0.010689537972211838 Accuracy 0.8857421875\n",
      "Iteration 36230 Training loss 0.0042053209617733955 Validation loss 0.010679694823920727 Accuracy 0.8857421875\n",
      "Iteration 36240 Training loss 0.003965457901358604 Validation loss 0.010473400354385376 Accuracy 0.88720703125\n",
      "Iteration 36250 Training loss 0.004900349769741297 Validation loss 0.010853617452085018 Accuracy 0.8837890625\n",
      "Iteration 36260 Training loss 0.0032997499220073223 Validation loss 0.010549607686698437 Accuracy 0.8876953125\n",
      "Iteration 36270 Training loss 0.004997747018933296 Validation loss 0.010669332928955555 Accuracy 0.884765625\n",
      "Iteration 36280 Training loss 0.003384241135790944 Validation loss 0.010688884183764458 Accuracy 0.88671875\n",
      "Iteration 36290 Training loss 0.0052446285262703896 Validation loss 0.010696444660425186 Accuracy 0.88525390625\n",
      "Iteration 36300 Training loss 0.004410683177411556 Validation loss 0.010656972415745258 Accuracy 0.88623046875\n",
      "Iteration 36310 Training loss 0.003532655304297805 Validation loss 0.010616518557071686 Accuracy 0.88671875\n",
      "Iteration 36320 Training loss 0.004749819170683622 Validation loss 0.010588652454316616 Accuracy 0.88720703125\n",
      "Iteration 36330 Training loss 0.0020876999478787184 Validation loss 0.010667634196579456 Accuracy 0.88525390625\n",
      "Iteration 36340 Training loss 0.004750593099743128 Validation loss 0.010665018111467361 Accuracy 0.88623046875\n",
      "Iteration 36350 Training loss 0.0038364154752343893 Validation loss 0.010970763862133026 Accuracy 0.88232421875\n",
      "Iteration 36360 Training loss 0.003897078800946474 Validation loss 0.010768495500087738 Accuracy 0.88525390625\n",
      "Iteration 36370 Training loss 0.002745910082012415 Validation loss 0.010764742270112038 Accuracy 0.8837890625\n",
      "Iteration 36380 Training loss 0.005975375883281231 Validation loss 0.010946597903966904 Accuracy 0.8837890625\n",
      "Iteration 36390 Training loss 0.004983499646186829 Validation loss 0.01054054033011198 Accuracy 0.88818359375\n",
      "Iteration 36400 Training loss 0.0034635858610272408 Validation loss 0.010764660313725471 Accuracy 0.8837890625\n",
      "Iteration 36410 Training loss 0.005143146030604839 Validation loss 0.010645104572176933 Accuracy 0.88671875\n",
      "Iteration 36420 Training loss 0.0028082123026251793 Validation loss 0.010534721426665783 Accuracy 0.88720703125\n",
      "Iteration 36430 Training loss 0.004235481843352318 Validation loss 0.010585547424852848 Accuracy 0.8857421875\n",
      "Iteration 36440 Training loss 0.0032279007136821747 Validation loss 0.010467428714036942 Accuracy 0.88720703125\n",
      "Iteration 36450 Training loss 0.0049466597847640514 Validation loss 0.010805011726915836 Accuracy 0.884765625\n",
      "Iteration 36460 Training loss 0.0033675094600766897 Validation loss 0.010595371015369892 Accuracy 0.88623046875\n",
      "Iteration 36470 Training loss 0.0038405098021030426 Validation loss 0.010674513876438141 Accuracy 0.8857421875\n",
      "Iteration 36480 Training loss 0.003650421043857932 Validation loss 0.010452516376972198 Accuracy 0.88818359375\n",
      "Iteration 36490 Training loss 0.0035870710853487253 Validation loss 0.01053654309362173 Accuracy 0.88671875\n",
      "Iteration 36500 Training loss 0.0029753046110272408 Validation loss 0.01063003670424223 Accuracy 0.88720703125\n",
      "Iteration 36510 Training loss 0.0028548771515488625 Validation loss 0.010499925352633 Accuracy 0.88671875\n",
      "Iteration 36520 Training loss 0.0026308957021683455 Validation loss 0.010522028431296349 Accuracy 0.88818359375\n",
      "Iteration 36530 Training loss 0.0035645095631480217 Validation loss 0.010447812266647816 Accuracy 0.88818359375\n",
      "Iteration 36540 Training loss 0.0025277745444327593 Validation loss 0.010452847927808762 Accuracy 0.8876953125\n",
      "Iteration 36550 Training loss 0.003618543967604637 Validation loss 0.010542514733970165 Accuracy 0.88720703125\n",
      "Iteration 36560 Training loss 0.005518861580640078 Validation loss 0.010784158483147621 Accuracy 0.884765625\n",
      "Iteration 36570 Training loss 0.005025338847190142 Validation loss 0.010609987191855907 Accuracy 0.8876953125\n",
      "Iteration 36580 Training loss 0.004529902711510658 Validation loss 0.01079582516103983 Accuracy 0.88427734375\n",
      "Iteration 36590 Training loss 0.0038801743648946285 Validation loss 0.010475724004209042 Accuracy 0.88720703125\n",
      "Iteration 36600 Training loss 0.003690374316647649 Validation loss 0.0105948681011796 Accuracy 0.88623046875\n",
      "Iteration 36610 Training loss 0.002808949211612344 Validation loss 0.010572419501841068 Accuracy 0.8876953125\n",
      "Iteration 36620 Training loss 0.003008126514032483 Validation loss 0.01048513688147068 Accuracy 0.88720703125\n",
      "Iteration 36630 Training loss 0.004726713988929987 Validation loss 0.010451732203364372 Accuracy 0.88818359375\n",
      "Iteration 36640 Training loss 0.0021559062879532576 Validation loss 0.010587800294160843 Accuracy 0.88671875\n",
      "Iteration 36650 Training loss 0.0031345500610768795 Validation loss 0.010567289777100086 Accuracy 0.88671875\n",
      "Iteration 36660 Training loss 0.003288974752649665 Validation loss 0.010645541362464428 Accuracy 0.88671875\n",
      "Iteration 36670 Training loss 0.0036397892981767654 Validation loss 0.010549964383244514 Accuracy 0.88623046875\n",
      "Iteration 36680 Training loss 0.0030620836187154055 Validation loss 0.010647309012711048 Accuracy 0.88623046875\n",
      "Iteration 36690 Training loss 0.0024291719309985638 Validation loss 0.01060700137168169 Accuracy 0.88525390625\n",
      "Iteration 36700 Training loss 0.004054227378219366 Validation loss 0.01116152759641409 Accuracy 0.880859375\n",
      "Iteration 36710 Training loss 0.0037848185747861862 Validation loss 0.010802402161061764 Accuracy 0.88330078125\n",
      "Iteration 36720 Training loss 0.003934698179364204 Validation loss 0.010786386206746101 Accuracy 0.8837890625\n",
      "Iteration 36730 Training loss 0.0032127241138368845 Validation loss 0.011233046650886536 Accuracy 0.87939453125\n",
      "Iteration 36740 Training loss 0.003515304531902075 Validation loss 0.010683493688702583 Accuracy 0.88525390625\n",
      "Iteration 36750 Training loss 0.0031018052250146866 Validation loss 0.010721257887780666 Accuracy 0.8857421875\n",
      "Iteration 36760 Training loss 0.004504823125898838 Validation loss 0.01087319664657116 Accuracy 0.8837890625\n",
      "Iteration 36770 Training loss 0.0031502211932092905 Validation loss 0.010936353355646133 Accuracy 0.8818359375\n",
      "Iteration 36780 Training loss 0.002643260871991515 Validation loss 0.010778583586215973 Accuracy 0.88330078125\n",
      "Iteration 36790 Training loss 0.0035762032493948936 Validation loss 0.010596193373203278 Accuracy 0.8876953125\n",
      "Iteration 36800 Training loss 0.005096543580293655 Validation loss 0.010439193807542324 Accuracy 0.88720703125\n",
      "Iteration 36810 Training loss 0.0031807583291083574 Validation loss 0.010508796200156212 Accuracy 0.88818359375\n",
      "Iteration 36820 Training loss 0.0036184126511216164 Validation loss 0.010533151216804981 Accuracy 0.88720703125\n",
      "Iteration 36830 Training loss 0.0034766909666359425 Validation loss 0.010723583400249481 Accuracy 0.8857421875\n",
      "Iteration 36840 Training loss 0.0035039717331528664 Validation loss 0.01037876307964325 Accuracy 0.88818359375\n",
      "Iteration 36850 Training loss 0.00441662035882473 Validation loss 0.011686046607792377 Accuracy 0.875\n",
      "Iteration 36860 Training loss 0.0033856555819511414 Validation loss 0.01110110804438591 Accuracy 0.88134765625\n",
      "Iteration 36870 Training loss 0.0046646250411868095 Validation loss 0.010948696173727512 Accuracy 0.88330078125\n",
      "Iteration 36880 Training loss 0.002405497245490551 Validation loss 0.010484604164958 Accuracy 0.88818359375\n",
      "Iteration 36890 Training loss 0.004893817938864231 Validation loss 0.01047083456069231 Accuracy 0.8876953125\n",
      "Iteration 36900 Training loss 0.0040029073134064674 Validation loss 0.010579860769212246 Accuracy 0.8857421875\n",
      "Iteration 36910 Training loss 0.00433652987703681 Validation loss 0.010711058974266052 Accuracy 0.88427734375\n",
      "Iteration 36920 Training loss 0.002956278854981065 Validation loss 0.010560316033661366 Accuracy 0.88623046875\n",
      "Iteration 36930 Training loss 0.0018117781728506088 Validation loss 0.010504266247153282 Accuracy 0.88671875\n",
      "Iteration 36940 Training loss 0.0033471330534666777 Validation loss 0.010594572871923447 Accuracy 0.88671875\n",
      "Iteration 36950 Training loss 0.003323320997878909 Validation loss 0.0105109428986907 Accuracy 0.8876953125\n",
      "Iteration 36960 Training loss 0.003281545592471957 Validation loss 0.010426892898976803 Accuracy 0.88818359375\n",
      "Iteration 36970 Training loss 0.00432689068838954 Validation loss 0.010522001422941685 Accuracy 0.88720703125\n",
      "Iteration 36980 Training loss 0.004034731071442366 Validation loss 0.010490244254469872 Accuracy 0.88818359375\n",
      "Iteration 36990 Training loss 0.003364389296621084 Validation loss 0.010779025964438915 Accuracy 0.88427734375\n",
      "Iteration 37000 Training loss 0.003961008507758379 Validation loss 0.010713068768382072 Accuracy 0.884765625\n",
      "Iteration 37010 Training loss 0.004997149109840393 Validation loss 0.010623627342283726 Accuracy 0.88525390625\n",
      "Iteration 37020 Training loss 0.003535287221893668 Validation loss 0.010609726421535015 Accuracy 0.88623046875\n",
      "Iteration 37030 Training loss 0.003262878395617008 Validation loss 0.010532796382904053 Accuracy 0.8876953125\n",
      "Iteration 37040 Training loss 0.002706727012991905 Validation loss 0.010819421149790287 Accuracy 0.88330078125\n",
      "Iteration 37050 Training loss 0.00424817344173789 Validation loss 0.010573591105639935 Accuracy 0.8876953125\n",
      "Iteration 37060 Training loss 0.003662639996036887 Validation loss 0.010624023154377937 Accuracy 0.88525390625\n",
      "Iteration 37070 Training loss 0.0024082388263195753 Validation loss 0.010544675402343273 Accuracy 0.88720703125\n",
      "Iteration 37080 Training loss 0.0046987091191112995 Validation loss 0.010640563443303108 Accuracy 0.88525390625\n",
      "Iteration 37090 Training loss 0.003791200928390026 Validation loss 0.01094920001924038 Accuracy 0.88232421875\n",
      "Iteration 37100 Training loss 0.003659999230876565 Validation loss 0.01080382987856865 Accuracy 0.88427734375\n",
      "Iteration 37110 Training loss 0.0039057445246726274 Validation loss 0.010600383393466473 Accuracy 0.88525390625\n",
      "Iteration 37120 Training loss 0.0031272820197045803 Validation loss 0.01057686097919941 Accuracy 0.8857421875\n",
      "Iteration 37130 Training loss 0.0036498294211924076 Validation loss 0.010810297913849354 Accuracy 0.88427734375\n",
      "Iteration 37140 Training loss 0.004003135487437248 Validation loss 0.01063950452953577 Accuracy 0.884765625\n",
      "Iteration 37150 Training loss 0.004610702395439148 Validation loss 0.010564287193119526 Accuracy 0.88623046875\n",
      "Iteration 37160 Training loss 0.004232617095112801 Validation loss 0.010647531598806381 Accuracy 0.8857421875\n",
      "Iteration 37170 Training loss 0.004002900328487158 Validation loss 0.010456358082592487 Accuracy 0.88818359375\n",
      "Iteration 37180 Training loss 0.003917937632650137 Validation loss 0.010878002271056175 Accuracy 0.88330078125\n",
      "Iteration 37190 Training loss 0.0051751951687037945 Validation loss 0.010550129227340221 Accuracy 0.88720703125\n",
      "Iteration 37200 Training loss 0.0026402289513498545 Validation loss 0.010836860164999962 Accuracy 0.884765625\n",
      "Iteration 37210 Training loss 0.003246481530368328 Validation loss 0.010548161342740059 Accuracy 0.888671875\n",
      "Iteration 37220 Training loss 0.0032244049943983555 Validation loss 0.010507483035326004 Accuracy 0.88818359375\n",
      "Iteration 37230 Training loss 0.004416974261403084 Validation loss 0.010845121927559376 Accuracy 0.8828125\n",
      "Iteration 37240 Training loss 0.003535239491611719 Validation loss 0.010452455841004848 Accuracy 0.88671875\n",
      "Iteration 37250 Training loss 0.0031541213393211365 Validation loss 0.010538851842284203 Accuracy 0.88720703125\n",
      "Iteration 37260 Training loss 0.003965701442211866 Validation loss 0.010694440454244614 Accuracy 0.884765625\n",
      "Iteration 37270 Training loss 0.004006276372820139 Validation loss 0.010652021504938602 Accuracy 0.88525390625\n",
      "Iteration 37280 Training loss 0.004383698105812073 Validation loss 0.011011259630322456 Accuracy 0.88232421875\n",
      "Iteration 37290 Training loss 0.0043901847675442696 Validation loss 0.010709855705499649 Accuracy 0.8857421875\n",
      "Iteration 37300 Training loss 0.0039505017921328545 Validation loss 0.010536826215684414 Accuracy 0.88671875\n",
      "Iteration 37310 Training loss 0.004203529097139835 Validation loss 0.010738427750766277 Accuracy 0.88525390625\n",
      "Iteration 37320 Training loss 0.0041161589324474335 Validation loss 0.010642286390066147 Accuracy 0.8857421875\n",
      "Iteration 37330 Training loss 0.0037394026294350624 Validation loss 0.010708007961511612 Accuracy 0.88623046875\n",
      "Iteration 37340 Training loss 0.0032404428347945213 Validation loss 0.010768307372927666 Accuracy 0.8837890625\n",
      "Iteration 37350 Training loss 0.0036064572632312775 Validation loss 0.010630921460688114 Accuracy 0.88671875\n",
      "Iteration 37360 Training loss 0.003570991102606058 Validation loss 0.010744232684373856 Accuracy 0.884765625\n",
      "Iteration 37370 Training loss 0.0038987458683550358 Validation loss 0.011299557983875275 Accuracy 0.87890625\n",
      "Iteration 37380 Training loss 0.004890033509582281 Validation loss 0.010729709640145302 Accuracy 0.88427734375\n",
      "Iteration 37390 Training loss 0.0032577801030129194 Validation loss 0.010982939042150974 Accuracy 0.88330078125\n",
      "Iteration 37400 Training loss 0.0037068582605570555 Validation loss 0.010750843212008476 Accuracy 0.884765625\n",
      "Iteration 37410 Training loss 0.0036586569622159004 Validation loss 0.010470363311469555 Accuracy 0.88720703125\n",
      "Iteration 37420 Training loss 0.003995047416538 Validation loss 0.010513920336961746 Accuracy 0.88818359375\n",
      "Iteration 37430 Training loss 0.004433594178408384 Validation loss 0.01068514958024025 Accuracy 0.884765625\n",
      "Iteration 37440 Training loss 0.003727738745510578 Validation loss 0.010722717270255089 Accuracy 0.88525390625\n",
      "Iteration 37450 Training loss 0.0034856058191508055 Validation loss 0.01058153249323368 Accuracy 0.88671875\n",
      "Iteration 37460 Training loss 0.003018500516191125 Validation loss 0.010678612627089024 Accuracy 0.88525390625\n",
      "Iteration 37470 Training loss 0.004217133857309818 Validation loss 0.010907838121056557 Accuracy 0.88232421875\n",
      "Iteration 37480 Training loss 0.0035242519807070494 Validation loss 0.010516567155718803 Accuracy 0.88720703125\n",
      "Iteration 37490 Training loss 0.0029933967161923647 Validation loss 0.010775861330330372 Accuracy 0.8837890625\n",
      "Iteration 37500 Training loss 0.0037544993683695793 Validation loss 0.010646282695233822 Accuracy 0.88671875\n",
      "Iteration 37510 Training loss 0.003993472550064325 Validation loss 0.010481511242687702 Accuracy 0.888671875\n",
      "Iteration 37520 Training loss 0.00447850814089179 Validation loss 0.010606725700199604 Accuracy 0.88671875\n",
      "Iteration 37530 Training loss 0.0056058610789477825 Validation loss 0.010582651011645794 Accuracy 0.8857421875\n",
      "Iteration 37540 Training loss 0.002821657108142972 Validation loss 0.01064027938991785 Accuracy 0.8857421875\n",
      "Iteration 37550 Training loss 0.004122632090002298 Validation loss 0.010801437310874462 Accuracy 0.88330078125\n",
      "Iteration 37560 Training loss 0.00398651510477066 Validation loss 0.010767362080514431 Accuracy 0.88427734375\n",
      "Iteration 37570 Training loss 0.003146166680380702 Validation loss 0.010675412602722645 Accuracy 0.88525390625\n",
      "Iteration 37580 Training loss 0.0038809317629784346 Validation loss 0.010751105844974518 Accuracy 0.88427734375\n",
      "Iteration 37590 Training loss 0.002578369341790676 Validation loss 0.010658244602382183 Accuracy 0.88720703125\n",
      "Iteration 37600 Training loss 0.0032942993566393852 Validation loss 0.010631526820361614 Accuracy 0.8857421875\n",
      "Iteration 37610 Training loss 0.004140867386013269 Validation loss 0.010680011473596096 Accuracy 0.884765625\n",
      "Iteration 37620 Training loss 0.0036600609309971333 Validation loss 0.01060886774212122 Accuracy 0.8857421875\n",
      "Iteration 37630 Training loss 0.003338885260745883 Validation loss 0.010487094521522522 Accuracy 0.88720703125\n",
      "Iteration 37640 Training loss 0.0036303100641816854 Validation loss 0.010422454215586185 Accuracy 0.88720703125\n",
      "Iteration 37650 Training loss 0.004365050233900547 Validation loss 0.010446845553815365 Accuracy 0.88720703125\n",
      "Iteration 37660 Training loss 0.004356499295681715 Validation loss 0.01054504606872797 Accuracy 0.8876953125\n",
      "Iteration 37670 Training loss 0.004602814093232155 Validation loss 0.010723900981247425 Accuracy 0.884765625\n",
      "Iteration 37680 Training loss 0.002125704428181052 Validation loss 0.010815300047397614 Accuracy 0.88427734375\n",
      "Iteration 37690 Training loss 0.0035123019479215145 Validation loss 0.010619091801345348 Accuracy 0.88623046875\n",
      "Iteration 37700 Training loss 0.004286566283553839 Validation loss 0.010620457120239735 Accuracy 0.8857421875\n",
      "Iteration 37710 Training loss 0.0034852097742259502 Validation loss 0.010747582651674747 Accuracy 0.8857421875\n",
      "Iteration 37720 Training loss 0.0038563834968954325 Validation loss 0.010925800539553165 Accuracy 0.88232421875\n",
      "Iteration 37730 Training loss 0.0051120249554514885 Validation loss 0.01070313062518835 Accuracy 0.88623046875\n",
      "Iteration 37740 Training loss 0.0036035366356372833 Validation loss 0.010756725445389748 Accuracy 0.88427734375\n",
      "Iteration 37750 Training loss 0.003705583745613694 Validation loss 0.010596963576972485 Accuracy 0.88623046875\n",
      "Iteration 37760 Training loss 0.003117319429293275 Validation loss 0.01092598121613264 Accuracy 0.88330078125\n",
      "Iteration 37770 Training loss 0.003037021029740572 Validation loss 0.010520190000534058 Accuracy 0.88720703125\n",
      "Iteration 37780 Training loss 0.004702832084149122 Validation loss 0.010448740795254707 Accuracy 0.8876953125\n",
      "Iteration 37790 Training loss 0.0042391205206513405 Validation loss 0.010627382434904575 Accuracy 0.8857421875\n",
      "Iteration 37800 Training loss 0.003088713390752673 Validation loss 0.010625869035720825 Accuracy 0.88720703125\n",
      "Iteration 37810 Training loss 0.0031027754303067923 Validation loss 0.010796216316521168 Accuracy 0.88525390625\n",
      "Iteration 37820 Training loss 0.0031421356834471226 Validation loss 0.01059165969491005 Accuracy 0.88671875\n",
      "Iteration 37830 Training loss 0.004808933939784765 Validation loss 0.010522907599806786 Accuracy 0.888671875\n",
      "Iteration 37840 Training loss 0.0036423015408217907 Validation loss 0.010661057196557522 Accuracy 0.88623046875\n",
      "Iteration 37850 Training loss 0.0045562186278402805 Validation loss 0.011422641575336456 Accuracy 0.8779296875\n",
      "Iteration 37860 Training loss 0.004710226319730282 Validation loss 0.010683572851121426 Accuracy 0.884765625\n",
      "Iteration 37870 Training loss 0.003971883561462164 Validation loss 0.011019467376172543 Accuracy 0.88330078125\n",
      "Iteration 37880 Training loss 0.002300113206729293 Validation loss 0.010607216507196426 Accuracy 0.88671875\n",
      "Iteration 37890 Training loss 0.0032912814058363438 Validation loss 0.010562614537775517 Accuracy 0.8857421875\n",
      "Iteration 37900 Training loss 0.0041782367043197155 Validation loss 0.010669996030628681 Accuracy 0.8857421875\n",
      "Iteration 37910 Training loss 0.004652644041925669 Validation loss 0.011046161875128746 Accuracy 0.88232421875\n",
      "Iteration 37920 Training loss 0.004467313177883625 Validation loss 0.01059491466730833 Accuracy 0.88671875\n",
      "Iteration 37930 Training loss 0.0020586741156876087 Validation loss 0.010609319433569908 Accuracy 0.88671875\n",
      "Iteration 37940 Training loss 0.0020685617346316576 Validation loss 0.010604549199342728 Accuracy 0.88623046875\n",
      "Iteration 37950 Training loss 0.004252976272255182 Validation loss 0.010703889653086662 Accuracy 0.8857421875\n",
      "Iteration 37960 Training loss 0.0030208893585950136 Validation loss 0.010787751525640488 Accuracy 0.88427734375\n",
      "Iteration 37970 Training loss 0.003691550809890032 Validation loss 0.010842448100447655 Accuracy 0.8828125\n",
      "Iteration 37980 Training loss 0.0033977653365582228 Validation loss 0.010632358491420746 Accuracy 0.88671875\n",
      "Iteration 37990 Training loss 0.0033776273485273123 Validation loss 0.010558822192251682 Accuracy 0.88720703125\n",
      "Iteration 38000 Training loss 0.003283450612798333 Validation loss 0.010550686158239841 Accuracy 0.88525390625\n",
      "Iteration 38010 Training loss 0.003166444832459092 Validation loss 0.01064386684447527 Accuracy 0.88525390625\n",
      "Iteration 38020 Training loss 0.0030150546226650476 Validation loss 0.010587207041680813 Accuracy 0.88671875\n",
      "Iteration 38030 Training loss 0.003317242953926325 Validation loss 0.010710072703659534 Accuracy 0.88427734375\n",
      "Iteration 38040 Training loss 0.0025404023472219706 Validation loss 0.010833099484443665 Accuracy 0.884765625\n",
      "Iteration 38050 Training loss 0.0038750043604522943 Validation loss 0.01073953602463007 Accuracy 0.88525390625\n",
      "Iteration 38060 Training loss 0.004087282344698906 Validation loss 0.010910236276686192 Accuracy 0.88330078125\n",
      "Iteration 38070 Training loss 0.002324976958334446 Validation loss 0.010686825960874557 Accuracy 0.8857421875\n",
      "Iteration 38080 Training loss 0.003634524531662464 Validation loss 0.010820417664945126 Accuracy 0.88427734375\n",
      "Iteration 38090 Training loss 0.0035761012695729733 Validation loss 0.011039015837013721 Accuracy 0.88232421875\n",
      "Iteration 38100 Training loss 0.003947321325540543 Validation loss 0.010912487283349037 Accuracy 0.8828125\n",
      "Iteration 38110 Training loss 0.0038529480807483196 Validation loss 0.01060129888355732 Accuracy 0.88671875\n",
      "Iteration 38120 Training loss 0.0036317382473498583 Validation loss 0.010663461871445179 Accuracy 0.8857421875\n",
      "Iteration 38130 Training loss 0.0036366386339068413 Validation loss 0.011160634458065033 Accuracy 0.8798828125\n",
      "Iteration 38140 Training loss 0.002436799695715308 Validation loss 0.010523030534386635 Accuracy 0.88623046875\n",
      "Iteration 38150 Training loss 0.0027501494623720646 Validation loss 0.010603019967675209 Accuracy 0.88671875\n",
      "Iteration 38160 Training loss 0.0026678680442273617 Validation loss 0.010580339469015598 Accuracy 0.88720703125\n",
      "Iteration 38170 Training loss 0.004192854277789593 Validation loss 0.01053448673337698 Accuracy 0.8876953125\n",
      "Iteration 38180 Training loss 0.002716552698984742 Validation loss 0.01053442619740963 Accuracy 0.88720703125\n",
      "Iteration 38190 Training loss 0.003653552383184433 Validation loss 0.01074920129030943 Accuracy 0.88525390625\n",
      "Iteration 38200 Training loss 0.0038314450066536665 Validation loss 0.010472873225808144 Accuracy 0.88818359375\n",
      "Iteration 38210 Training loss 0.003355196909978986 Validation loss 0.010836632922291756 Accuracy 0.88330078125\n",
      "Iteration 38220 Training loss 0.005099277477711439 Validation loss 0.011005991138517857 Accuracy 0.8818359375\n",
      "Iteration 38230 Training loss 0.0018167493399232626 Validation loss 0.010674321092665195 Accuracy 0.88525390625\n",
      "Iteration 38240 Training loss 0.004565126728266478 Validation loss 0.010943290777504444 Accuracy 0.88232421875\n",
      "Iteration 38250 Training loss 0.003393748542293906 Validation loss 0.01074210088700056 Accuracy 0.8837890625\n",
      "Iteration 38260 Training loss 0.0034681595861911774 Validation loss 0.010858545079827309 Accuracy 0.8828125\n",
      "Iteration 38270 Training loss 0.003275701543316245 Validation loss 0.010813955217599869 Accuracy 0.8837890625\n",
      "Iteration 38280 Training loss 0.0030336726922541857 Validation loss 0.01096712239086628 Accuracy 0.88232421875\n",
      "Iteration 38290 Training loss 0.00290341442450881 Validation loss 0.010724847204983234 Accuracy 0.8857421875\n",
      "Iteration 38300 Training loss 0.0040395124815404415 Validation loss 0.010860797949135303 Accuracy 0.8837890625\n",
      "Iteration 38310 Training loss 0.0031119021587073803 Validation loss 0.010670582763850689 Accuracy 0.88525390625\n",
      "Iteration 38320 Training loss 0.004061958286911249 Validation loss 0.010509251616895199 Accuracy 0.88720703125\n",
      "Iteration 38330 Training loss 0.0039876773953437805 Validation loss 0.010517570190131664 Accuracy 0.88818359375\n",
      "Iteration 38340 Training loss 0.003558716969564557 Validation loss 0.01064886711537838 Accuracy 0.88525390625\n",
      "Iteration 38350 Training loss 0.002662437269464135 Validation loss 0.0106767937541008 Accuracy 0.8857421875\n",
      "Iteration 38360 Training loss 0.004326925612986088 Validation loss 0.010579402558505535 Accuracy 0.88671875\n",
      "Iteration 38370 Training loss 0.003725383197888732 Validation loss 0.010482389479875565 Accuracy 0.888671875\n",
      "Iteration 38380 Training loss 0.003114475170150399 Validation loss 0.010791247710585594 Accuracy 0.8857421875\n",
      "Iteration 38390 Training loss 0.003494354197755456 Validation loss 0.010810251347720623 Accuracy 0.88427734375\n",
      "Iteration 38400 Training loss 0.003845602972432971 Validation loss 0.01064449641853571 Accuracy 0.8857421875\n",
      "Iteration 38410 Training loss 0.004366748034954071 Validation loss 0.010467593558132648 Accuracy 0.88720703125\n",
      "Iteration 38420 Training loss 0.0029694270342588425 Validation loss 0.01035957783460617 Accuracy 0.888671875\n",
      "Iteration 38430 Training loss 0.002380231162533164 Validation loss 0.010467697866261005 Accuracy 0.8876953125\n",
      "Iteration 38440 Training loss 0.003615049412474036 Validation loss 0.010628278367221355 Accuracy 0.88623046875\n",
      "Iteration 38450 Training loss 0.0020861581433564425 Validation loss 0.010760807432234287 Accuracy 0.8837890625\n",
      "Iteration 38460 Training loss 0.0046247257851064205 Validation loss 0.01082330197095871 Accuracy 0.8837890625\n",
      "Iteration 38470 Training loss 0.0028113338630646467 Validation loss 0.010721527971327305 Accuracy 0.88427734375\n",
      "Iteration 38480 Training loss 0.005195183213800192 Validation loss 0.01072840578854084 Accuracy 0.884765625\n",
      "Iteration 38490 Training loss 0.0038014843594282866 Validation loss 0.010922367684543133 Accuracy 0.88330078125\n",
      "Iteration 38500 Training loss 0.0027657002210617065 Validation loss 0.010704504325985909 Accuracy 0.88525390625\n",
      "Iteration 38510 Training loss 0.004537297412753105 Validation loss 0.010768935084342957 Accuracy 0.884765625\n",
      "Iteration 38520 Training loss 0.0036546699702739716 Validation loss 0.010759929195046425 Accuracy 0.88427734375\n",
      "Iteration 38530 Training loss 0.004576004110276699 Validation loss 0.010577838867902756 Accuracy 0.88671875\n",
      "Iteration 38540 Training loss 0.003869539825245738 Validation loss 0.010756686329841614 Accuracy 0.884765625\n",
      "Iteration 38550 Training loss 0.00416981428861618 Validation loss 0.010797380469739437 Accuracy 0.8857421875\n",
      "Iteration 38560 Training loss 0.004815931431949139 Validation loss 0.010628764517605305 Accuracy 0.88623046875\n",
      "Iteration 38570 Training loss 0.003984087612479925 Validation loss 0.010605412535369396 Accuracy 0.88671875\n",
      "Iteration 38580 Training loss 0.004251193720847368 Validation loss 0.0105888731777668 Accuracy 0.88720703125\n",
      "Iteration 38590 Training loss 0.003795205382630229 Validation loss 0.010788927786052227 Accuracy 0.8837890625\n",
      "Iteration 38600 Training loss 0.0028551616705954075 Validation loss 0.010858459398150444 Accuracy 0.8837890625\n",
      "Iteration 38610 Training loss 0.002626527799293399 Validation loss 0.010575901716947556 Accuracy 0.88623046875\n",
      "Iteration 38620 Training loss 0.0036638646852225065 Validation loss 0.01075099129229784 Accuracy 0.884765625\n",
      "Iteration 38630 Training loss 0.003492397954687476 Validation loss 0.010620256885886192 Accuracy 0.88671875\n",
      "Iteration 38640 Training loss 0.0030465442687273026 Validation loss 0.010714645497500896 Accuracy 0.884765625\n",
      "Iteration 38650 Training loss 0.0031649747397750616 Validation loss 0.010569647885859013 Accuracy 0.88671875\n",
      "Iteration 38660 Training loss 0.003720587119460106 Validation loss 0.010593099519610405 Accuracy 0.88623046875\n",
      "Iteration 38670 Training loss 0.0037909781094640493 Validation loss 0.011515643447637558 Accuracy 0.876953125\n",
      "Iteration 38680 Training loss 0.0041130841709673405 Validation loss 0.010858435183763504 Accuracy 0.884765625\n",
      "Iteration 38690 Training loss 0.003254098119214177 Validation loss 0.010687775909900665 Accuracy 0.88525390625\n",
      "Iteration 38700 Training loss 0.004475393332540989 Validation loss 0.010535904206335545 Accuracy 0.88720703125\n",
      "Iteration 38710 Training loss 0.0033167852088809013 Validation loss 0.010749168694019318 Accuracy 0.88427734375\n",
      "Iteration 38720 Training loss 0.003888703417032957 Validation loss 0.010743539780378342 Accuracy 0.8837890625\n",
      "Iteration 38730 Training loss 0.004457918927073479 Validation loss 0.010559199377894402 Accuracy 0.8857421875\n",
      "Iteration 38740 Training loss 0.0035443201195448637 Validation loss 0.010799044743180275 Accuracy 0.88330078125\n",
      "Iteration 38750 Training loss 0.0028360493015497923 Validation loss 0.010738986544311047 Accuracy 0.884765625\n",
      "Iteration 38760 Training loss 0.004884601105004549 Validation loss 0.011001838371157646 Accuracy 0.88232421875\n",
      "Iteration 38770 Training loss 0.003336676862090826 Validation loss 0.010644695721566677 Accuracy 0.88525390625\n",
      "Iteration 38780 Training loss 0.002308742143213749 Validation loss 0.010633120313286781 Accuracy 0.8857421875\n",
      "Iteration 38790 Training loss 0.0025168394204229116 Validation loss 0.010489342734217644 Accuracy 0.88818359375\n",
      "Iteration 38800 Training loss 0.0030461056157946587 Validation loss 0.010689465329051018 Accuracy 0.884765625\n",
      "Iteration 38810 Training loss 0.00353904883377254 Validation loss 0.010574016720056534 Accuracy 0.8857421875\n",
      "Iteration 38820 Training loss 0.004069805610924959 Validation loss 0.010915185324847698 Accuracy 0.88232421875\n",
      "Iteration 38830 Training loss 0.0034076543524861336 Validation loss 0.010772996582090855 Accuracy 0.884765625\n",
      "Iteration 38840 Training loss 0.003599137533456087 Validation loss 0.010883285664021969 Accuracy 0.88427734375\n",
      "Iteration 38850 Training loss 0.0033268723636865616 Validation loss 0.010726659558713436 Accuracy 0.884765625\n",
      "Iteration 38860 Training loss 0.0040077636949718 Validation loss 0.01077463012188673 Accuracy 0.8837890625\n",
      "Iteration 38870 Training loss 0.00408592913299799 Validation loss 0.010699206031858921 Accuracy 0.88525390625\n",
      "Iteration 38880 Training loss 0.005125327035784721 Validation loss 0.010648421011865139 Accuracy 0.8857421875\n",
      "Iteration 38890 Training loss 0.0027555800043046474 Validation loss 0.010689843446016312 Accuracy 0.884765625\n",
      "Iteration 38900 Training loss 0.0043998840264976025 Validation loss 0.010327682830393314 Accuracy 0.88916015625\n",
      "Iteration 38910 Training loss 0.0033967820927500725 Validation loss 0.010844760574400425 Accuracy 0.88427734375\n",
      "Iteration 38920 Training loss 0.003041981952264905 Validation loss 0.01042991690337658 Accuracy 0.888671875\n",
      "Iteration 38930 Training loss 0.0031875520944595337 Validation loss 0.010720184072852135 Accuracy 0.88427734375\n",
      "Iteration 38940 Training loss 0.0037135162856429815 Validation loss 0.010945209302008152 Accuracy 0.8818359375\n",
      "Iteration 38950 Training loss 0.003473786637187004 Validation loss 0.01051116082817316 Accuracy 0.88671875\n",
      "Iteration 38960 Training loss 0.00553105166181922 Validation loss 0.010580983944237232 Accuracy 0.8857421875\n",
      "Iteration 38970 Training loss 0.002816767431795597 Validation loss 0.01093541830778122 Accuracy 0.88232421875\n",
      "Iteration 38980 Training loss 0.00343882292509079 Validation loss 0.010486718267202377 Accuracy 0.88720703125\n",
      "Iteration 38990 Training loss 0.003202166873961687 Validation loss 0.010573266074061394 Accuracy 0.8876953125\n",
      "Iteration 39000 Training loss 0.0034555974416434765 Validation loss 0.01072537899017334 Accuracy 0.884765625\n",
      "Iteration 39010 Training loss 0.0027226749807596207 Validation loss 0.010747743770480156 Accuracy 0.88427734375\n",
      "Iteration 39020 Training loss 0.004624071065336466 Validation loss 0.010654793120920658 Accuracy 0.8857421875\n",
      "Iteration 39030 Training loss 0.0036578059662133455 Validation loss 0.010789123363792896 Accuracy 0.88330078125\n",
      "Iteration 39040 Training loss 0.002936403267085552 Validation loss 0.010695637203752995 Accuracy 0.88525390625\n",
      "Iteration 39050 Training loss 0.0029767360538244247 Validation loss 0.010833905078470707 Accuracy 0.8837890625\n",
      "Iteration 39060 Training loss 0.003586571430787444 Validation loss 0.010648217052221298 Accuracy 0.8857421875\n",
      "Iteration 39070 Training loss 0.004543783143162727 Validation loss 0.01056376937776804 Accuracy 0.88623046875\n",
      "Iteration 39080 Training loss 0.004060425329953432 Validation loss 0.01100127398967743 Accuracy 0.8828125\n",
      "Iteration 39090 Training loss 0.0038152243942022324 Validation loss 0.01053739245980978 Accuracy 0.88671875\n",
      "Iteration 39100 Training loss 0.0024856191594153643 Validation loss 0.010464340448379517 Accuracy 0.89013671875\n",
      "Iteration 39110 Training loss 0.0036447092425078154 Validation loss 0.010573217645287514 Accuracy 0.88623046875\n",
      "Iteration 39120 Training loss 0.003793301759287715 Validation loss 0.010586293414235115 Accuracy 0.88720703125\n",
      "Iteration 39130 Training loss 0.0036854168865829706 Validation loss 0.010600639507174492 Accuracy 0.88720703125\n",
      "Iteration 39140 Training loss 0.002745263045653701 Validation loss 0.010778809897601604 Accuracy 0.88427734375\n",
      "Iteration 39150 Training loss 0.004008294548839331 Validation loss 0.010910493321716785 Accuracy 0.8828125\n",
      "Iteration 39160 Training loss 0.003788375062867999 Validation loss 0.010743621736764908 Accuracy 0.884765625\n",
      "Iteration 39170 Training loss 0.00333011825568974 Validation loss 0.01057229470461607 Accuracy 0.88671875\n",
      "Iteration 39180 Training loss 0.0043601891957223415 Validation loss 0.01065811701118946 Accuracy 0.88525390625\n",
      "Iteration 39190 Training loss 0.004928035195916891 Validation loss 0.010635239072144032 Accuracy 0.88623046875\n",
      "Iteration 39200 Training loss 0.003250826383009553 Validation loss 0.010430478490889072 Accuracy 0.8876953125\n",
      "Iteration 39210 Training loss 0.00424566213041544 Validation loss 0.010688056237995625 Accuracy 0.88427734375\n",
      "Iteration 39220 Training loss 0.0038330911193042994 Validation loss 0.010556302033364773 Accuracy 0.88525390625\n",
      "Iteration 39230 Training loss 0.004229987971484661 Validation loss 0.010592155158519745 Accuracy 0.88671875\n",
      "Iteration 39240 Training loss 0.0038112178444862366 Validation loss 0.010655059479176998 Accuracy 0.88623046875\n",
      "Iteration 39250 Training loss 0.0028468307573348284 Validation loss 0.010585475713014603 Accuracy 0.88671875\n",
      "Iteration 39260 Training loss 0.005318204406648874 Validation loss 0.010628288611769676 Accuracy 0.88671875\n",
      "Iteration 39270 Training loss 0.0051565999165177345 Validation loss 0.010605390183627605 Accuracy 0.88720703125\n",
      "Iteration 39280 Training loss 0.003156309947371483 Validation loss 0.010645515285432339 Accuracy 0.88623046875\n",
      "Iteration 39290 Training loss 0.0032648316118866205 Validation loss 0.010638884268701077 Accuracy 0.8857421875\n",
      "Iteration 39300 Training loss 0.0030575015116482973 Validation loss 0.010617957450449467 Accuracy 0.8857421875\n",
      "Iteration 39310 Training loss 0.004032548051327467 Validation loss 0.010721420869231224 Accuracy 0.88427734375\n",
      "Iteration 39320 Training loss 0.002322872867807746 Validation loss 0.010511535219848156 Accuracy 0.88720703125\n",
      "Iteration 39330 Training loss 0.0045610531233251095 Validation loss 0.010931923985481262 Accuracy 0.88330078125\n",
      "Iteration 39340 Training loss 0.0021571884863078594 Validation loss 0.010676494799554348 Accuracy 0.8857421875\n",
      "Iteration 39350 Training loss 0.0030035963281989098 Validation loss 0.010579206049442291 Accuracy 0.88671875\n",
      "Iteration 39360 Training loss 0.0042579518631100655 Validation loss 0.010593881830573082 Accuracy 0.88623046875\n",
      "Iteration 39370 Training loss 0.0028196393977850676 Validation loss 0.0109392199665308 Accuracy 0.8828125\n",
      "Iteration 39380 Training loss 0.0031021670438349247 Validation loss 0.010768632404506207 Accuracy 0.884765625\n",
      "Iteration 39390 Training loss 0.002761944429948926 Validation loss 0.010646602138876915 Accuracy 0.8857421875\n",
      "Iteration 39400 Training loss 0.0038145091384649277 Validation loss 0.010596498847007751 Accuracy 0.88720703125\n",
      "Iteration 39410 Training loss 0.004161476157605648 Validation loss 0.010454026982188225 Accuracy 0.88818359375\n",
      "Iteration 39420 Training loss 0.003258609212934971 Validation loss 0.01051806379109621 Accuracy 0.88671875\n",
      "Iteration 39430 Training loss 0.003719690954312682 Validation loss 0.010469655506312847 Accuracy 0.88720703125\n",
      "Iteration 39440 Training loss 0.0031267786398530006 Validation loss 0.010482761077582836 Accuracy 0.88818359375\n",
      "Iteration 39450 Training loss 0.0036000232212245464 Validation loss 0.010636693798005581 Accuracy 0.88623046875\n",
      "Iteration 39460 Training loss 0.002500948030501604 Validation loss 0.010538190603256226 Accuracy 0.88623046875\n",
      "Iteration 39470 Training loss 0.004010354168713093 Validation loss 0.010540316812694073 Accuracy 0.8857421875\n",
      "Iteration 39480 Training loss 0.0056940470822155476 Validation loss 0.010693151503801346 Accuracy 0.88623046875\n",
      "Iteration 39490 Training loss 0.004072819370776415 Validation loss 0.010725453495979309 Accuracy 0.8837890625\n",
      "Iteration 39500 Training loss 0.003826087573543191 Validation loss 0.01074831560254097 Accuracy 0.884765625\n",
      "Iteration 39510 Training loss 0.0038278503343462944 Validation loss 0.010663913562893867 Accuracy 0.8857421875\n",
      "Iteration 39520 Training loss 0.0033505649771541357 Validation loss 0.010868347249925137 Accuracy 0.88330078125\n",
      "Iteration 39530 Training loss 0.004203429911285639 Validation loss 0.010407939553260803 Accuracy 0.88720703125\n",
      "Iteration 39540 Training loss 0.0013243848225101829 Validation loss 0.0105648348107934 Accuracy 0.88623046875\n",
      "Iteration 39550 Training loss 0.004151068162173033 Validation loss 0.010887271724641323 Accuracy 0.8828125\n",
      "Iteration 39560 Training loss 0.0037252248730510473 Validation loss 0.010571635328233242 Accuracy 0.88720703125\n",
      "Iteration 39570 Training loss 0.0037044486962258816 Validation loss 0.010334601625800133 Accuracy 0.8896484375\n",
      "Iteration 39580 Training loss 0.003945571836084127 Validation loss 0.010916116647422314 Accuracy 0.8828125\n",
      "Iteration 39590 Training loss 0.003862628946080804 Validation loss 0.010462557896971703 Accuracy 0.8876953125\n",
      "Iteration 39600 Training loss 0.004884028807282448 Validation loss 0.010620019398629665 Accuracy 0.88525390625\n",
      "Iteration 39610 Training loss 0.003414260921999812 Validation loss 0.010564906522631645 Accuracy 0.88623046875\n",
      "Iteration 39620 Training loss 0.00335154146887362 Validation loss 0.010655570775270462 Accuracy 0.88525390625\n",
      "Iteration 39630 Training loss 0.0026743023190647364 Validation loss 0.010726832784712315 Accuracy 0.884765625\n",
      "Iteration 39640 Training loss 0.0032402172219008207 Validation loss 0.010504481382668018 Accuracy 0.8876953125\n",
      "Iteration 39650 Training loss 0.003446707036346197 Validation loss 0.010590878315269947 Accuracy 0.88671875\n",
      "Iteration 39660 Training loss 0.0024602890480309725 Validation loss 0.01048998348414898 Accuracy 0.888671875\n",
      "Iteration 39670 Training loss 0.0019973537418991327 Validation loss 0.010550222359597683 Accuracy 0.8857421875\n",
      "Iteration 39680 Training loss 0.0028575139585882425 Validation loss 0.01052919402718544 Accuracy 0.88671875\n",
      "Iteration 39690 Training loss 0.0032278457656502724 Validation loss 0.010519077070057392 Accuracy 0.8876953125\n",
      "Iteration 39700 Training loss 0.004522078670561314 Validation loss 0.010529948398470879 Accuracy 0.88623046875\n",
      "Iteration 39710 Training loss 0.0021544895134866238 Validation loss 0.010518795810639858 Accuracy 0.88720703125\n",
      "Iteration 39720 Training loss 0.003415925893932581 Validation loss 0.01035167183727026 Accuracy 0.888671875\n",
      "Iteration 39730 Training loss 0.003588679013773799 Validation loss 0.010569341480731964 Accuracy 0.88671875\n",
      "Iteration 39740 Training loss 0.0023511273320764303 Validation loss 0.010707632638514042 Accuracy 0.884765625\n",
      "Iteration 39750 Training loss 0.0035697754938155413 Validation loss 0.010547717101871967 Accuracy 0.884765625\n",
      "Iteration 39760 Training loss 0.004126731306314468 Validation loss 0.010524886660277843 Accuracy 0.88623046875\n",
      "Iteration 39770 Training loss 0.003344203345477581 Validation loss 0.010578560642898083 Accuracy 0.8857421875\n",
      "Iteration 39780 Training loss 0.003594173351302743 Validation loss 0.010839146561920643 Accuracy 0.8828125\n",
      "Iteration 39790 Training loss 0.003012138418853283 Validation loss 0.010426503606140614 Accuracy 0.888671875\n",
      "Iteration 39800 Training loss 0.0030063348822295666 Validation loss 0.010553944855928421 Accuracy 0.88623046875\n",
      "Iteration 39810 Training loss 0.004232020117342472 Validation loss 0.010591225698590279 Accuracy 0.8857421875\n",
      "Iteration 39820 Training loss 0.0025132920127362013 Validation loss 0.01079030241817236 Accuracy 0.8837890625\n",
      "Iteration 39830 Training loss 0.0032627484761178493 Validation loss 0.010645166039466858 Accuracy 0.88623046875\n",
      "Iteration 39840 Training loss 0.002373314229771495 Validation loss 0.01079979445785284 Accuracy 0.88427734375\n",
      "Iteration 39850 Training loss 0.003565336111932993 Validation loss 0.0104291420429945 Accuracy 0.88916015625\n",
      "Iteration 39860 Training loss 0.00393351586535573 Validation loss 0.010583817958831787 Accuracy 0.88671875\n",
      "Iteration 39870 Training loss 0.004595127888023853 Validation loss 0.010563519783318043 Accuracy 0.8857421875\n",
      "Iteration 39880 Training loss 0.004059819038957357 Validation loss 0.010610434226691723 Accuracy 0.88623046875\n",
      "Iteration 39890 Training loss 0.0038888179697096348 Validation loss 0.010840597562491894 Accuracy 0.8837890625\n",
      "Iteration 39900 Training loss 0.003775113495066762 Validation loss 0.010638066567480564 Accuracy 0.88623046875\n",
      "Iteration 39910 Training loss 0.005087627563625574 Validation loss 0.010674345307052135 Accuracy 0.8857421875\n",
      "Iteration 39920 Training loss 0.002112498041242361 Validation loss 0.010452472604811192 Accuracy 0.88818359375\n",
      "Iteration 39930 Training loss 0.002792397513985634 Validation loss 0.010592907667160034 Accuracy 0.88623046875\n",
      "Iteration 39940 Training loss 0.002066425746306777 Validation loss 0.01062039379030466 Accuracy 0.88623046875\n",
      "Iteration 39950 Training loss 0.003885383950546384 Validation loss 0.010620763525366783 Accuracy 0.8857421875\n",
      "Iteration 39960 Training loss 0.0032437751069664955 Validation loss 0.010563774034380913 Accuracy 0.88720703125\n",
      "Iteration 39970 Training loss 0.0036304278764873743 Validation loss 0.010619589127600193 Accuracy 0.88720703125\n",
      "Iteration 39980 Training loss 0.003244666149839759 Validation loss 0.010544247925281525 Accuracy 0.88720703125\n",
      "Iteration 39990 Training loss 0.004197100177407265 Validation loss 0.010595775209367275 Accuracy 0.88623046875\n",
      "Iteration 40000 Training loss 0.002836614614352584 Validation loss 0.010639062151312828 Accuracy 0.8857421875\n",
      "Iteration 40010 Training loss 0.00427358178421855 Validation loss 0.010506569407880306 Accuracy 0.8876953125\n",
      "Iteration 40020 Training loss 0.00451639574021101 Validation loss 0.010625092312693596 Accuracy 0.88671875\n",
      "Iteration 40030 Training loss 0.004770142026245594 Validation loss 0.010964804328978062 Accuracy 0.8828125\n",
      "Iteration 40040 Training loss 0.00330475065857172 Validation loss 0.0109883863478899 Accuracy 0.8828125\n",
      "Iteration 40050 Training loss 0.0027185098733752966 Validation loss 0.01071347389370203 Accuracy 0.884765625\n",
      "Iteration 40060 Training loss 0.0045564924366772175 Validation loss 0.010518213734030724 Accuracy 0.88671875\n",
      "Iteration 40070 Training loss 0.0032919333316385746 Validation loss 0.010630810633301735 Accuracy 0.88623046875\n",
      "Iteration 40080 Training loss 0.0028452554251998663 Validation loss 0.010602802038192749 Accuracy 0.88671875\n",
      "Iteration 40090 Training loss 0.003756697988137603 Validation loss 0.010740119032561779 Accuracy 0.88525390625\n",
      "Iteration 40100 Training loss 0.003330649109557271 Validation loss 0.010701139457523823 Accuracy 0.884765625\n",
      "Iteration 40110 Training loss 0.0024682343937456608 Validation loss 0.01065431535243988 Accuracy 0.884765625\n",
      "Iteration 40120 Training loss 0.0035872445441782475 Validation loss 0.010747360996901989 Accuracy 0.88525390625\n",
      "Iteration 40130 Training loss 0.004650440067052841 Validation loss 0.010656159371137619 Accuracy 0.8857421875\n",
      "Iteration 40140 Training loss 0.0035957631189376116 Validation loss 0.01050181407481432 Accuracy 0.88720703125\n",
      "Iteration 40150 Training loss 0.0032265586778521538 Validation loss 0.010835298337042332 Accuracy 0.88427734375\n",
      "Iteration 40160 Training loss 0.002808628138154745 Validation loss 0.0106477877125144 Accuracy 0.88623046875\n",
      "Iteration 40170 Training loss 0.003752220654860139 Validation loss 0.010434753261506557 Accuracy 0.88818359375\n",
      "Iteration 40180 Training loss 0.004699429962784052 Validation loss 0.010758347809314728 Accuracy 0.88525390625\n",
      "Iteration 40190 Training loss 0.0030046964529901743 Validation loss 0.010651188902556896 Accuracy 0.8857421875\n",
      "Iteration 40200 Training loss 0.0028063070494681597 Validation loss 0.01059054397046566 Accuracy 0.88623046875\n",
      "Iteration 40210 Training loss 0.0029611866921186447 Validation loss 0.010655578225851059 Accuracy 0.88525390625\n",
      "Iteration 40220 Training loss 0.0025093546137213707 Validation loss 0.010786342434585094 Accuracy 0.88427734375\n",
      "Iteration 40230 Training loss 0.0029007066041231155 Validation loss 0.010497340932488441 Accuracy 0.88720703125\n",
      "Iteration 40240 Training loss 0.002915454562753439 Validation loss 0.010674619115889072 Accuracy 0.88525390625\n",
      "Iteration 40250 Training loss 0.0028702309355139732 Validation loss 0.010478480719029903 Accuracy 0.8876953125\n",
      "Iteration 40260 Training loss 0.003599221585318446 Validation loss 0.01053929328918457 Accuracy 0.8876953125\n",
      "Iteration 40270 Training loss 0.004830625373870134 Validation loss 0.010530691593885422 Accuracy 0.88720703125\n",
      "Iteration 40280 Training loss 0.004021535161882639 Validation loss 0.010730095207691193 Accuracy 0.8857421875\n",
      "Iteration 40290 Training loss 0.0017114918446168303 Validation loss 0.01077255792915821 Accuracy 0.88427734375\n",
      "Iteration 40300 Training loss 0.002852127654477954 Validation loss 0.010782585479319096 Accuracy 0.884765625\n",
      "Iteration 40310 Training loss 0.0022843715269118547 Validation loss 0.01080272812396288 Accuracy 0.884765625\n",
      "Iteration 40320 Training loss 0.0035613069776445627 Validation loss 0.010700619779527187 Accuracy 0.884765625\n",
      "Iteration 40330 Training loss 0.0028170673176646233 Validation loss 0.010551277548074722 Accuracy 0.88623046875\n",
      "Iteration 40340 Training loss 0.003317885100841522 Validation loss 0.010472746565937996 Accuracy 0.88818359375\n",
      "Iteration 40350 Training loss 0.0037810001522302628 Validation loss 0.010671176947653294 Accuracy 0.88525390625\n",
      "Iteration 40360 Training loss 0.0038341209292411804 Validation loss 0.010502436198294163 Accuracy 0.88720703125\n",
      "Iteration 40370 Training loss 0.0025336304679512978 Validation loss 0.010628681629896164 Accuracy 0.88623046875\n",
      "Iteration 40380 Training loss 0.0032957051880657673 Validation loss 0.01068259496241808 Accuracy 0.8857421875\n",
      "Iteration 40390 Training loss 0.002208341844379902 Validation loss 0.01046714000403881 Accuracy 0.8876953125\n",
      "Iteration 40400 Training loss 0.003410645527765155 Validation loss 0.01042325422167778 Accuracy 0.88671875\n",
      "Iteration 40410 Training loss 0.0035795026924461126 Validation loss 0.010470491833984852 Accuracy 0.8876953125\n",
      "Iteration 40420 Training loss 0.003306854283437133 Validation loss 0.010516691952943802 Accuracy 0.88720703125\n",
      "Iteration 40430 Training loss 0.003495490411296487 Validation loss 0.010598844848573208 Accuracy 0.8857421875\n",
      "Iteration 40440 Training loss 0.0038188821636140347 Validation loss 0.010490795597434044 Accuracy 0.88671875\n",
      "Iteration 40450 Training loss 0.00433260016143322 Validation loss 0.010828261263668537 Accuracy 0.88330078125\n",
      "Iteration 40460 Training loss 0.0035063812974840403 Validation loss 0.010611915960907936 Accuracy 0.88671875\n",
      "Iteration 40470 Training loss 0.0028658227529376745 Validation loss 0.010657164268195629 Accuracy 0.88623046875\n",
      "Iteration 40480 Training loss 0.003019805531948805 Validation loss 0.01059997733682394 Accuracy 0.88671875\n",
      "Iteration 40490 Training loss 0.0030364326667040586 Validation loss 0.010646759532392025 Accuracy 0.88671875\n",
      "Iteration 40500 Training loss 0.0041652568615973 Validation loss 0.010638908483088017 Accuracy 0.88525390625\n",
      "Iteration 40510 Training loss 0.0036488256882876158 Validation loss 0.010752245783805847 Accuracy 0.8837890625\n",
      "Iteration 40520 Training loss 0.00408782996237278 Validation loss 0.010624195449054241 Accuracy 0.8857421875\n",
      "Iteration 40530 Training loss 0.0031436483841389418 Validation loss 0.010763506405055523 Accuracy 0.884765625\n",
      "Iteration 40540 Training loss 0.0029381497297436 Validation loss 0.010803820565342903 Accuracy 0.88330078125\n",
      "Iteration 40550 Training loss 0.0029556022491306067 Validation loss 0.010471496731042862 Accuracy 0.88818359375\n",
      "Iteration 40560 Training loss 0.003646650817245245 Validation loss 0.010818996466696262 Accuracy 0.884765625\n",
      "Iteration 40570 Training loss 0.0034241981338709593 Validation loss 0.010528487153351307 Accuracy 0.88720703125\n",
      "Iteration 40580 Training loss 0.0052858623676002026 Validation loss 0.010693207383155823 Accuracy 0.884765625\n",
      "Iteration 40590 Training loss 0.00457375030964613 Validation loss 0.010643614456057549 Accuracy 0.88525390625\n",
      "Iteration 40600 Training loss 0.00189885008148849 Validation loss 0.010548378340899944 Accuracy 0.88623046875\n",
      "Iteration 40610 Training loss 0.002959931269288063 Validation loss 0.010514108464121819 Accuracy 0.88671875\n",
      "Iteration 40620 Training loss 0.004103674087673426 Validation loss 0.0106569929048419 Accuracy 0.88525390625\n",
      "Iteration 40630 Training loss 0.0030605217907577753 Validation loss 0.010732526890933514 Accuracy 0.88427734375\n",
      "Iteration 40640 Training loss 0.0024204482324421406 Validation loss 0.010652396827936172 Accuracy 0.8857421875\n",
      "Iteration 40650 Training loss 0.003313753055408597 Validation loss 0.010816876776516438 Accuracy 0.884765625\n",
      "Iteration 40660 Training loss 0.0033935876563191414 Validation loss 0.010613051243126392 Accuracy 0.88671875\n",
      "Iteration 40670 Training loss 0.002487583551555872 Validation loss 0.010615305975079536 Accuracy 0.88525390625\n",
      "Iteration 40680 Training loss 0.0024404097348451614 Validation loss 0.010622415691614151 Accuracy 0.88623046875\n",
      "Iteration 40690 Training loss 0.004164768382906914 Validation loss 0.010767382569611073 Accuracy 0.884765625\n",
      "Iteration 40700 Training loss 0.0034513319842517376 Validation loss 0.010797567665576935 Accuracy 0.88427734375\n",
      "Iteration 40710 Training loss 0.003384543815627694 Validation loss 0.010546527802944183 Accuracy 0.88671875\n",
      "Iteration 40720 Training loss 0.004559484776109457 Validation loss 0.010510292835533619 Accuracy 0.88671875\n",
      "Iteration 40730 Training loss 0.004016471095383167 Validation loss 0.010604765266180038 Accuracy 0.8857421875\n",
      "Iteration 40740 Training loss 0.0023098820820450783 Validation loss 0.010671067051589489 Accuracy 0.8857421875\n",
      "Iteration 40750 Training loss 0.003104679984971881 Validation loss 0.010581552051007748 Accuracy 0.88720703125\n",
      "Iteration 40760 Training loss 0.0033429490868002176 Validation loss 0.01096897479146719 Accuracy 0.8828125\n",
      "Iteration 40770 Training loss 0.003799624741077423 Validation loss 0.010695432312786579 Accuracy 0.884765625\n",
      "Iteration 40780 Training loss 0.003093745093792677 Validation loss 0.01084861345589161 Accuracy 0.884765625\n",
      "Iteration 40790 Training loss 0.0042871031910181046 Validation loss 0.01085218321532011 Accuracy 0.8837890625\n",
      "Iteration 40800 Training loss 0.0039395117200911045 Validation loss 0.010848186910152435 Accuracy 0.8837890625\n",
      "Iteration 40810 Training loss 0.0029676761478185654 Validation loss 0.01070837490260601 Accuracy 0.8857421875\n",
      "Iteration 40820 Training loss 0.003394907806068659 Validation loss 0.010784853249788284 Accuracy 0.88427734375\n",
      "Iteration 40830 Training loss 0.0037360338028520346 Validation loss 0.01069994643330574 Accuracy 0.8857421875\n",
      "Iteration 40840 Training loss 0.0021998323500156403 Validation loss 0.01058039627969265 Accuracy 0.88623046875\n",
      "Iteration 40850 Training loss 0.0032771260011941195 Validation loss 0.010606491006910801 Accuracy 0.88671875\n",
      "Iteration 40860 Training loss 0.004446040373295546 Validation loss 0.010472076013684273 Accuracy 0.8876953125\n",
      "Iteration 40870 Training loss 0.002968978602439165 Validation loss 0.01041122991591692 Accuracy 0.8876953125\n",
      "Iteration 40880 Training loss 0.0034339402336627245 Validation loss 0.010599249042570591 Accuracy 0.88671875\n",
      "Iteration 40890 Training loss 0.003742202650755644 Validation loss 0.010711646638810635 Accuracy 0.88525390625\n",
      "Iteration 40900 Training loss 0.0027755647897720337 Validation loss 0.0106497248634696 Accuracy 0.88720703125\n",
      "Iteration 40910 Training loss 0.003404265036806464 Validation loss 0.01059701107442379 Accuracy 0.88623046875\n",
      "Iteration 40920 Training loss 0.0014502271078526974 Validation loss 0.010660332627594471 Accuracy 0.884765625\n",
      "Iteration 40930 Training loss 0.0034897199366241693 Validation loss 0.0107234762981534 Accuracy 0.88525390625\n",
      "Iteration 40940 Training loss 0.0034427859354764223 Validation loss 0.010823322460055351 Accuracy 0.8837890625\n",
      "Iteration 40950 Training loss 0.0032705001067370176 Validation loss 0.010752084665000439 Accuracy 0.88525390625\n",
      "Iteration 40960 Training loss 0.002432272769510746 Validation loss 0.010663686320185661 Accuracy 0.88623046875\n",
      "Iteration 40970 Training loss 0.002293815603479743 Validation loss 0.010454664006829262 Accuracy 0.888671875\n",
      "Iteration 40980 Training loss 0.0037370878271758556 Validation loss 0.010568766854703426 Accuracy 0.88720703125\n",
      "Iteration 40990 Training loss 0.003060116432607174 Validation loss 0.01077550183981657 Accuracy 0.8837890625\n",
      "Iteration 41000 Training loss 0.0037830988876521587 Validation loss 0.010780119337141514 Accuracy 0.8837890625\n",
      "Iteration 41010 Training loss 0.0026359213516116142 Validation loss 0.01089339330792427 Accuracy 0.88330078125\n",
      "Iteration 41020 Training loss 0.0029534136410802603 Validation loss 0.010773235000669956 Accuracy 0.88427734375\n",
      "Iteration 41030 Training loss 0.004293100442737341 Validation loss 0.010685576125979424 Accuracy 0.88525390625\n",
      "Iteration 41040 Training loss 0.0045505003072321415 Validation loss 0.01068231649696827 Accuracy 0.88525390625\n",
      "Iteration 41050 Training loss 0.004169030115008354 Validation loss 0.010887746699154377 Accuracy 0.88330078125\n",
      "Iteration 41060 Training loss 0.003307654056698084 Validation loss 0.010800783522427082 Accuracy 0.884765625\n",
      "Iteration 41070 Training loss 0.0036416491493582726 Validation loss 0.010926386341452599 Accuracy 0.88232421875\n",
      "Iteration 41080 Training loss 0.002980851801112294 Validation loss 0.010659884661436081 Accuracy 0.88525390625\n",
      "Iteration 41090 Training loss 0.0030901883728802204 Validation loss 0.010712158866226673 Accuracy 0.884765625\n",
      "Iteration 41100 Training loss 0.003991662058979273 Validation loss 0.010712251998484135 Accuracy 0.88427734375\n",
      "Iteration 41110 Training loss 0.0035188954789191484 Validation loss 0.010558434762060642 Accuracy 0.88623046875\n",
      "Iteration 41120 Training loss 0.0023097421508282423 Validation loss 0.01072897668927908 Accuracy 0.8837890625\n",
      "Iteration 41130 Training loss 0.002893885364755988 Validation loss 0.010580367408692837 Accuracy 0.88623046875\n",
      "Iteration 41140 Training loss 0.0031289909966289997 Validation loss 0.010923395864665508 Accuracy 0.8837890625\n",
      "Iteration 41150 Training loss 0.002603123662993312 Validation loss 0.010592589154839516 Accuracy 0.88623046875\n",
      "Iteration 41160 Training loss 0.0028299991972744465 Validation loss 0.01059455331414938 Accuracy 0.88623046875\n",
      "Iteration 41170 Training loss 0.0031495278235524893 Validation loss 0.010694452561438084 Accuracy 0.8857421875\n",
      "Iteration 41180 Training loss 0.003720140317454934 Validation loss 0.010648438706994057 Accuracy 0.88623046875\n",
      "Iteration 41190 Training loss 0.003783336840569973 Validation loss 0.010637402534484863 Accuracy 0.88623046875\n",
      "Iteration 41200 Training loss 0.0035959421657025814 Validation loss 0.010751117952167988 Accuracy 0.88525390625\n",
      "Iteration 41210 Training loss 0.0042458935640752316 Validation loss 0.010650381445884705 Accuracy 0.8857421875\n",
      "Iteration 41220 Training loss 0.003421142464503646 Validation loss 0.010698300786316395 Accuracy 0.88525390625\n",
      "Iteration 41230 Training loss 0.0021571265533566475 Validation loss 0.010513232089579105 Accuracy 0.88671875\n",
      "Iteration 41240 Training loss 0.0022339774295687675 Validation loss 0.010744321160018444 Accuracy 0.88525390625\n",
      "Iteration 41250 Training loss 0.0021968712098896503 Validation loss 0.010637934319674969 Accuracy 0.88623046875\n",
      "Iteration 41260 Training loss 0.0031502668280154467 Validation loss 0.01075078547000885 Accuracy 0.88427734375\n",
      "Iteration 41270 Training loss 0.003044659271836281 Validation loss 0.010531813837587833 Accuracy 0.88720703125\n",
      "Iteration 41280 Training loss 0.003308027284219861 Validation loss 0.010831309482455254 Accuracy 0.8837890625\n",
      "Iteration 41290 Training loss 0.0032473027240484953 Validation loss 0.010776343755424023 Accuracy 0.8837890625\n",
      "Iteration 41300 Training loss 0.0029739532619714737 Validation loss 0.010647691786289215 Accuracy 0.88623046875\n",
      "Iteration 41310 Training loss 0.002755463821813464 Validation loss 0.010728497989475727 Accuracy 0.88427734375\n",
      "Iteration 41320 Training loss 0.003055589273571968 Validation loss 0.010764280334115028 Accuracy 0.88427734375\n",
      "Iteration 41330 Training loss 0.0035362732596695423 Validation loss 0.010864883661270142 Accuracy 0.8837890625\n",
      "Iteration 41340 Training loss 0.002687175292521715 Validation loss 0.010921475477516651 Accuracy 0.88232421875\n",
      "Iteration 41350 Training loss 0.0018192599527537823 Validation loss 0.01063476037234068 Accuracy 0.88623046875\n",
      "Iteration 41360 Training loss 0.004210844170302153 Validation loss 0.010941432788968086 Accuracy 0.88232421875\n",
      "Iteration 41370 Training loss 0.0026171961799263954 Validation loss 0.010772615671157837 Accuracy 0.884765625\n",
      "Iteration 41380 Training loss 0.003437890438362956 Validation loss 0.010796642862260342 Accuracy 0.8837890625\n",
      "Iteration 41390 Training loss 0.0031391994562000036 Validation loss 0.010748250409960747 Accuracy 0.88427734375\n",
      "Iteration 41400 Training loss 0.003350300481542945 Validation loss 0.010692162439227104 Accuracy 0.88525390625\n",
      "Iteration 41410 Training loss 0.002339232712984085 Validation loss 0.01061935257166624 Accuracy 0.884765625\n",
      "Iteration 41420 Training loss 0.0024925905745476484 Validation loss 0.010946283116936684 Accuracy 0.88232421875\n",
      "Iteration 41430 Training loss 0.0045610396191477776 Validation loss 0.010832538828253746 Accuracy 0.88427734375\n",
      "Iteration 41440 Training loss 0.0025900413747876883 Validation loss 0.010632151737809181 Accuracy 0.88525390625\n",
      "Iteration 41450 Training loss 0.0036383557599037886 Validation loss 0.010619803331792355 Accuracy 0.88720703125\n",
      "Iteration 41460 Training loss 0.0035364069044589996 Validation loss 0.010859690606594086 Accuracy 0.88427734375\n",
      "Iteration 41470 Training loss 0.003372395411133766 Validation loss 0.010672216303646564 Accuracy 0.88623046875\n",
      "Iteration 41480 Training loss 0.0022353988606482744 Validation loss 0.010494736023247242 Accuracy 0.88720703125\n",
      "Iteration 41490 Training loss 0.0023872521705925465 Validation loss 0.010720238089561462 Accuracy 0.8857421875\n",
      "Iteration 41500 Training loss 0.004415706731379032 Validation loss 0.010591485537588596 Accuracy 0.88623046875\n",
      "Iteration 41510 Training loss 0.0037292626220732927 Validation loss 0.011002550832927227 Accuracy 0.88134765625\n",
      "Iteration 41520 Training loss 0.0027780162636190653 Validation loss 0.010700507089495659 Accuracy 0.884765625\n",
      "Iteration 41530 Training loss 0.003491349518299103 Validation loss 0.010625583119690418 Accuracy 0.88525390625\n",
      "Iteration 41540 Training loss 0.0026023725513368845 Validation loss 0.010608375072479248 Accuracy 0.88671875\n",
      "Iteration 41550 Training loss 0.003959557507187128 Validation loss 0.011061913333833218 Accuracy 0.88232421875\n",
      "Iteration 41560 Training loss 0.003603869117796421 Validation loss 0.010793260298669338 Accuracy 0.8837890625\n",
      "Iteration 41570 Training loss 0.0029386768583208323 Validation loss 0.01096758060157299 Accuracy 0.8828125\n",
      "Iteration 41580 Training loss 0.00352676585316658 Validation loss 0.010614901781082153 Accuracy 0.88671875\n",
      "Iteration 41590 Training loss 0.002261608373373747 Validation loss 0.010671276599168777 Accuracy 0.88623046875\n",
      "Iteration 41600 Training loss 0.0021839295513927937 Validation loss 0.01075784396380186 Accuracy 0.8857421875\n",
      "Iteration 41610 Training loss 0.003814839059486985 Validation loss 0.010620568878948689 Accuracy 0.88671875\n",
      "Iteration 41620 Training loss 0.003440784988924861 Validation loss 0.010634960606694221 Accuracy 0.8857421875\n",
      "Iteration 41630 Training loss 0.0029181893914937973 Validation loss 0.010591474361717701 Accuracy 0.8857421875\n",
      "Iteration 41640 Training loss 0.003942484501749277 Validation loss 0.010630734264850616 Accuracy 0.8857421875\n",
      "Iteration 41650 Training loss 0.002804777817800641 Validation loss 0.010697155259549618 Accuracy 0.88427734375\n",
      "Iteration 41660 Training loss 0.0028932609129697084 Validation loss 0.010635926388204098 Accuracy 0.8857421875\n",
      "Iteration 41670 Training loss 0.002492086496204138 Validation loss 0.0106996214017272 Accuracy 0.884765625\n",
      "Iteration 41680 Training loss 0.0030134429689496756 Validation loss 0.010698098689317703 Accuracy 0.88330078125\n",
      "Iteration 41690 Training loss 0.002395401708781719 Validation loss 0.01069231890141964 Accuracy 0.884765625\n",
      "Iteration 41700 Training loss 0.002591641154140234 Validation loss 0.010476657189428806 Accuracy 0.88671875\n",
      "Iteration 41710 Training loss 0.004524221643805504 Validation loss 0.010640178807079792 Accuracy 0.88671875\n",
      "Iteration 41720 Training loss 0.002788958139717579 Validation loss 0.0106258699670434 Accuracy 0.8857421875\n",
      "Iteration 41730 Training loss 0.0035730688832700253 Validation loss 0.010821767151355743 Accuracy 0.8837890625\n",
      "Iteration 41740 Training loss 0.003704568836838007 Validation loss 0.010769772343337536 Accuracy 0.88525390625\n",
      "Iteration 41750 Training loss 0.002457505324855447 Validation loss 0.011003940366208553 Accuracy 0.8818359375\n",
      "Iteration 41760 Training loss 0.0027509257197380066 Validation loss 0.010853984393179417 Accuracy 0.88330078125\n",
      "Iteration 41770 Training loss 0.004554195795208216 Validation loss 0.011244344525039196 Accuracy 0.87939453125\n",
      "Iteration 41780 Training loss 0.004470594227313995 Validation loss 0.010769047774374485 Accuracy 0.884765625\n",
      "Iteration 41790 Training loss 0.0026572588831186295 Validation loss 0.010826320387423038 Accuracy 0.8837890625\n",
      "Iteration 41800 Training loss 0.0031262950506061316 Validation loss 0.010599174536764622 Accuracy 0.8857421875\n",
      "Iteration 41810 Training loss 0.004190958570688963 Validation loss 0.010910915210843086 Accuracy 0.88330078125\n",
      "Iteration 41820 Training loss 0.003638721536844969 Validation loss 0.01072815153747797 Accuracy 0.88525390625\n",
      "Iteration 41830 Training loss 0.00420098751783371 Validation loss 0.010802744887769222 Accuracy 0.88330078125\n",
      "Iteration 41840 Training loss 0.002288175979629159 Validation loss 0.010678809136152267 Accuracy 0.88525390625\n",
      "Iteration 41850 Training loss 0.0020791208371520042 Validation loss 0.010535912588238716 Accuracy 0.88525390625\n",
      "Iteration 41860 Training loss 0.0029612125363200903 Validation loss 0.010612688027322292 Accuracy 0.8857421875\n",
      "Iteration 41870 Training loss 0.004807450342923403 Validation loss 0.010814514011144638 Accuracy 0.8828125\n",
      "Iteration 41880 Training loss 0.003993681166321039 Validation loss 0.010639284737408161 Accuracy 0.8857421875\n",
      "Iteration 41890 Training loss 0.0032380828633904457 Validation loss 0.011091701686382294 Accuracy 0.88134765625\n",
      "Iteration 41900 Training loss 0.004058287478983402 Validation loss 0.010719245299696922 Accuracy 0.88427734375\n",
      "Iteration 41910 Training loss 0.0030420233961194754 Validation loss 0.010537548922002316 Accuracy 0.88671875\n",
      "Iteration 41920 Training loss 0.0028746582102030516 Validation loss 0.01073567382991314 Accuracy 0.8857421875\n",
      "Iteration 41930 Training loss 0.0026491691824048758 Validation loss 0.010626226663589478 Accuracy 0.8857421875\n",
      "Iteration 41940 Training loss 0.0028626041021198034 Validation loss 0.01063457503914833 Accuracy 0.88623046875\n",
      "Iteration 41950 Training loss 0.003211554139852524 Validation loss 0.01062957476824522 Accuracy 0.8857421875\n",
      "Iteration 41960 Training loss 0.002386959735304117 Validation loss 0.010677204467356205 Accuracy 0.88427734375\n",
      "Iteration 41970 Training loss 0.0036548771895468235 Validation loss 0.010738900862634182 Accuracy 0.88525390625\n",
      "Iteration 41980 Training loss 0.0028545341920107603 Validation loss 0.01066212821751833 Accuracy 0.8857421875\n",
      "Iteration 41990 Training loss 0.0035988858435302973 Validation loss 0.010422115214169025 Accuracy 0.8876953125\n",
      "Iteration 42000 Training loss 0.002936822595074773 Validation loss 0.01069143321365118 Accuracy 0.8857421875\n",
      "Iteration 42010 Training loss 0.004044622182846069 Validation loss 0.0109476363286376 Accuracy 0.8828125\n",
      "Iteration 42020 Training loss 0.00334080308675766 Validation loss 0.010759343393146992 Accuracy 0.88525390625\n",
      "Iteration 42030 Training loss 0.0033393395133316517 Validation loss 0.010883122682571411 Accuracy 0.8837890625\n",
      "Iteration 42040 Training loss 0.003142630448564887 Validation loss 0.010743080638349056 Accuracy 0.884765625\n",
      "Iteration 42050 Training loss 0.0034579080529510975 Validation loss 0.010769009590148926 Accuracy 0.8857421875\n",
      "Iteration 42060 Training loss 0.004460587166249752 Validation loss 0.010579644702374935 Accuracy 0.88623046875\n",
      "Iteration 42070 Training loss 0.004100378602743149 Validation loss 0.010672803968191147 Accuracy 0.88623046875\n",
      "Iteration 42080 Training loss 0.004099609330296516 Validation loss 0.0107363136485219 Accuracy 0.88427734375\n",
      "Iteration 42090 Training loss 0.002262985799461603 Validation loss 0.010580392554402351 Accuracy 0.88671875\n",
      "Iteration 42100 Training loss 0.003747670678421855 Validation loss 0.010652143508195877 Accuracy 0.8857421875\n",
      "Iteration 42110 Training loss 0.004047504160553217 Validation loss 0.010635686106979847 Accuracy 0.88623046875\n",
      "Iteration 42120 Training loss 0.002629151102155447 Validation loss 0.010506141930818558 Accuracy 0.8876953125\n",
      "Iteration 42130 Training loss 0.0030314859468489885 Validation loss 0.010555736720561981 Accuracy 0.88671875\n",
      "Iteration 42140 Training loss 0.0024497625418007374 Validation loss 0.010436005890369415 Accuracy 0.888671875\n",
      "Iteration 42150 Training loss 0.0025334390811622143 Validation loss 0.010473030619323254 Accuracy 0.8876953125\n",
      "Iteration 42160 Training loss 0.004409588873386383 Validation loss 0.010661615990102291 Accuracy 0.8857421875\n",
      "Iteration 42170 Training loss 0.0030782960820943117 Validation loss 0.010742985643446445 Accuracy 0.884765625\n",
      "Iteration 42180 Training loss 0.0030461647547781467 Validation loss 0.011001252569258213 Accuracy 0.880859375\n",
      "Iteration 42190 Training loss 0.0037092934362590313 Validation loss 0.010914742946624756 Accuracy 0.88330078125\n",
      "Iteration 42200 Training loss 0.0028340346179902554 Validation loss 0.010706650093197823 Accuracy 0.8837890625\n",
      "Iteration 42210 Training loss 0.003455501515418291 Validation loss 0.010740404017269611 Accuracy 0.88427734375\n",
      "Iteration 42220 Training loss 0.0025658474769443274 Validation loss 0.010552925989031792 Accuracy 0.88671875\n",
      "Iteration 42230 Training loss 0.003155280603095889 Validation loss 0.010743297636508942 Accuracy 0.88427734375\n",
      "Iteration 42240 Training loss 0.0020761892665177584 Validation loss 0.010705114342272282 Accuracy 0.88427734375\n",
      "Iteration 42250 Training loss 0.002841288223862648 Validation loss 0.010839150287210941 Accuracy 0.8837890625\n",
      "Iteration 42260 Training loss 0.0027945854235440493 Validation loss 0.010883810929954052 Accuracy 0.88330078125\n",
      "Iteration 42270 Training loss 0.0024814873468130827 Validation loss 0.010616803541779518 Accuracy 0.88623046875\n",
      "Iteration 42280 Training loss 0.0032734028063714504 Validation loss 0.010633294470608234 Accuracy 0.88525390625\n",
      "Iteration 42290 Training loss 0.002731210784986615 Validation loss 0.010697778314352036 Accuracy 0.8857421875\n",
      "Iteration 42300 Training loss 0.0035499476362019777 Validation loss 0.010647948831319809 Accuracy 0.88623046875\n",
      "Iteration 42310 Training loss 0.0030114827677607536 Validation loss 0.010818415321409702 Accuracy 0.88330078125\n",
      "Iteration 42320 Training loss 0.002440708689391613 Validation loss 0.010674175806343555 Accuracy 0.88623046875\n",
      "Iteration 42330 Training loss 0.003990080673247576 Validation loss 0.010749008506536484 Accuracy 0.884765625\n",
      "Iteration 42340 Training loss 0.0027719473000615835 Validation loss 0.010771789588034153 Accuracy 0.88427734375\n",
      "Iteration 42350 Training loss 0.0036969135981053114 Validation loss 0.010544662363827229 Accuracy 0.88671875\n",
      "Iteration 42360 Training loss 0.003427717834711075 Validation loss 0.010583911091089249 Accuracy 0.88525390625\n",
      "Iteration 42370 Training loss 0.002784150652587414 Validation loss 0.010592248290777206 Accuracy 0.88623046875\n",
      "Iteration 42380 Training loss 0.0026150369085371494 Validation loss 0.010433424264192581 Accuracy 0.8876953125\n",
      "Iteration 42390 Training loss 0.004155807662755251 Validation loss 0.01078000571578741 Accuracy 0.88427734375\n",
      "Iteration 42400 Training loss 0.0013080304488539696 Validation loss 0.010541405528783798 Accuracy 0.88623046875\n",
      "Iteration 42410 Training loss 0.003493159543722868 Validation loss 0.010928263887763023 Accuracy 0.88330078125\n",
      "Iteration 42420 Training loss 0.003518175333738327 Validation loss 0.011120337061583996 Accuracy 0.8798828125\n",
      "Iteration 42430 Training loss 0.0034513480495661497 Validation loss 0.011004838161170483 Accuracy 0.880859375\n",
      "Iteration 42440 Training loss 0.003785465843975544 Validation loss 0.010544384829699993 Accuracy 0.88671875\n",
      "Iteration 42450 Training loss 0.0029907994903624058 Validation loss 0.010696807876229286 Accuracy 0.88525390625\n",
      "Iteration 42460 Training loss 0.002635152079164982 Validation loss 0.01059782411903143 Accuracy 0.88525390625\n",
      "Iteration 42470 Training loss 0.002654146635904908 Validation loss 0.010662916116416454 Accuracy 0.88525390625\n",
      "Iteration 42480 Training loss 0.003088184632360935 Validation loss 0.01075603999197483 Accuracy 0.884765625\n",
      "Iteration 42490 Training loss 0.0027314515318721533 Validation loss 0.010793562978506088 Accuracy 0.88427734375\n",
      "Iteration 42500 Training loss 0.0034173668827861547 Validation loss 0.010674611665308475 Accuracy 0.88623046875\n",
      "Iteration 42510 Training loss 0.003754282835870981 Validation loss 0.010586857795715332 Accuracy 0.88623046875\n",
      "Iteration 42520 Training loss 0.002787795616313815 Validation loss 0.010804645717144012 Accuracy 0.88330078125\n",
      "Iteration 42530 Training loss 0.002767602214589715 Validation loss 0.010591519065201283 Accuracy 0.8857421875\n",
      "Iteration 42540 Training loss 0.003491208888590336 Validation loss 0.01067805103957653 Accuracy 0.88623046875\n",
      "Iteration 42550 Training loss 0.003669451456516981 Validation loss 0.010637974366545677 Accuracy 0.88720703125\n",
      "Iteration 42560 Training loss 0.003524972591549158 Validation loss 0.01061574462801218 Accuracy 0.88671875\n",
      "Iteration 42570 Training loss 0.0033930556382983923 Validation loss 0.01069639902561903 Accuracy 0.88525390625\n",
      "Iteration 42580 Training loss 0.003189884126186371 Validation loss 0.010728956200182438 Accuracy 0.884765625\n",
      "Iteration 42590 Training loss 0.0025037764571607113 Validation loss 0.010698121972382069 Accuracy 0.884765625\n",
      "Iteration 42600 Training loss 0.003607391845434904 Validation loss 0.010654745623469353 Accuracy 0.88623046875\n",
      "Iteration 42610 Training loss 0.0028226501308381557 Validation loss 0.010737945325672626 Accuracy 0.8857421875\n",
      "Iteration 42620 Training loss 0.002996055642142892 Validation loss 0.011030364781618118 Accuracy 0.880859375\n",
      "Iteration 42630 Training loss 0.0021531269885599613 Validation loss 0.010715317912399769 Accuracy 0.88427734375\n",
      "Iteration 42640 Training loss 0.0026533729396760464 Validation loss 0.011090063489973545 Accuracy 0.88134765625\n",
      "Iteration 42650 Training loss 0.003003116464242339 Validation loss 0.010592186823487282 Accuracy 0.88623046875\n",
      "Iteration 42660 Training loss 0.0027677242178469896 Validation loss 0.010641233995556831 Accuracy 0.88525390625\n",
      "Iteration 42670 Training loss 0.0028964527882635593 Validation loss 0.01073632575571537 Accuracy 0.88525390625\n",
      "Iteration 42680 Training loss 0.003010808490216732 Validation loss 0.010701579041779041 Accuracy 0.88427734375\n",
      "Iteration 42690 Training loss 0.0026590926572680473 Validation loss 0.010786451399326324 Accuracy 0.88232421875\n",
      "Iteration 42700 Training loss 0.003334522247314453 Validation loss 0.010747583582997322 Accuracy 0.8837890625\n",
      "Iteration 42710 Training loss 0.0031206670682877302 Validation loss 0.010584685951471329 Accuracy 0.8857421875\n",
      "Iteration 42720 Training loss 0.004616208840161562 Validation loss 0.0105296541005373 Accuracy 0.88671875\n",
      "Iteration 42730 Training loss 0.004210519138723612 Validation loss 0.010423803701996803 Accuracy 0.88720703125\n",
      "Iteration 42740 Training loss 0.0049119084142148495 Validation loss 0.010648053139448166 Accuracy 0.88623046875\n",
      "Iteration 42750 Training loss 0.002217611065134406 Validation loss 0.0106447022408247 Accuracy 0.8857421875\n",
      "Iteration 42760 Training loss 0.0018104114569723606 Validation loss 0.010841520503163338 Accuracy 0.88427734375\n",
      "Iteration 42770 Training loss 0.0033669264521449804 Validation loss 0.01062835194170475 Accuracy 0.88623046875\n",
      "Iteration 42780 Training loss 0.0016395162092521787 Validation loss 0.01054171472787857 Accuracy 0.88671875\n",
      "Iteration 42790 Training loss 0.0023689132649451494 Validation loss 0.010792125016450882 Accuracy 0.884765625\n",
      "Iteration 42800 Training loss 0.0038198058027774096 Validation loss 0.01072762068361044 Accuracy 0.88525390625\n",
      "Iteration 42810 Training loss 0.0024745273403823376 Validation loss 0.010750995948910713 Accuracy 0.8857421875\n",
      "Iteration 42820 Training loss 0.003002341603860259 Validation loss 0.010734904557466507 Accuracy 0.8857421875\n",
      "Iteration 42830 Training loss 0.0033458583056926727 Validation loss 0.010704242624342442 Accuracy 0.88525390625\n",
      "Iteration 42840 Training loss 0.002028099028393626 Validation loss 0.010544994845986366 Accuracy 0.88720703125\n",
      "Iteration 42850 Training loss 0.0023377889301627874 Validation loss 0.010570197366178036 Accuracy 0.88623046875\n",
      "Iteration 42860 Training loss 0.004820171277970076 Validation loss 0.010546262376010418 Accuracy 0.88671875\n",
      "Iteration 42870 Training loss 0.0030791577883064747 Validation loss 0.010462351143360138 Accuracy 0.88818359375\n",
      "Iteration 42880 Training loss 0.004049114417284727 Validation loss 0.010346520692110062 Accuracy 0.88916015625\n",
      "Iteration 42890 Training loss 0.0017461908282712102 Validation loss 0.01043880358338356 Accuracy 0.888671875\n",
      "Iteration 42900 Training loss 0.003010490443557501 Validation loss 0.010615898296236992 Accuracy 0.8857421875\n",
      "Iteration 42910 Training loss 0.003788809757679701 Validation loss 0.010580925270915031 Accuracy 0.88720703125\n",
      "Iteration 42920 Training loss 0.0021772682666778564 Validation loss 0.010586635209619999 Accuracy 0.88720703125\n",
      "Iteration 42930 Training loss 0.0038592801429331303 Validation loss 0.0107272919267416 Accuracy 0.88427734375\n",
      "Iteration 42940 Training loss 0.003994669299572706 Validation loss 0.010662783868610859 Accuracy 0.8857421875\n",
      "Iteration 42950 Training loss 0.0026426336262375116 Validation loss 0.010681310668587685 Accuracy 0.884765625\n",
      "Iteration 42960 Training loss 0.003272591158747673 Validation loss 0.010518956929445267 Accuracy 0.88671875\n",
      "Iteration 42970 Training loss 0.00347473518922925 Validation loss 0.010634389705955982 Accuracy 0.884765625\n",
      "Iteration 42980 Training loss 0.002759831491857767 Validation loss 0.010630137287080288 Accuracy 0.8857421875\n",
      "Iteration 42990 Training loss 0.002541032386943698 Validation loss 0.010669445618987083 Accuracy 0.88623046875\n",
      "Iteration 43000 Training loss 0.0026532700285315514 Validation loss 0.010747209191322327 Accuracy 0.88427734375\n",
      "Iteration 43010 Training loss 0.0030459905974566936 Validation loss 0.010611024685204029 Accuracy 0.88623046875\n",
      "Iteration 43020 Training loss 0.0028242364060133696 Validation loss 0.010562499985098839 Accuracy 0.88671875\n",
      "Iteration 43030 Training loss 0.002178437076508999 Validation loss 0.010680732317268848 Accuracy 0.884765625\n",
      "Iteration 43040 Training loss 0.0032406558748334646 Validation loss 0.010646463371813297 Accuracy 0.88525390625\n",
      "Iteration 43050 Training loss 0.0021759257651865482 Validation loss 0.01068168692290783 Accuracy 0.88427734375\n",
      "Iteration 43060 Training loss 0.0035285570193082094 Validation loss 0.010835709981620312 Accuracy 0.88427734375\n",
      "Iteration 43070 Training loss 0.002564008114859462 Validation loss 0.010774033144116402 Accuracy 0.88427734375\n",
      "Iteration 43080 Training loss 0.0031943332869559526 Validation loss 0.010539577342569828 Accuracy 0.8876953125\n",
      "Iteration 43090 Training loss 0.0032045592088252306 Validation loss 0.010727724060416222 Accuracy 0.88525390625\n",
      "Iteration 43100 Training loss 0.0038654538802802563 Validation loss 0.010631939396262169 Accuracy 0.8857421875\n",
      "Iteration 43110 Training loss 0.0036169083323329687 Validation loss 0.010697347111999989 Accuracy 0.8857421875\n",
      "Iteration 43120 Training loss 0.0037448813673108816 Validation loss 0.010768145322799683 Accuracy 0.88427734375\n",
      "Iteration 43130 Training loss 0.0027234998997300863 Validation loss 0.010892525315284729 Accuracy 0.8828125\n",
      "Iteration 43140 Training loss 0.0025530795101076365 Validation loss 0.01064242608845234 Accuracy 0.884765625\n",
      "Iteration 43150 Training loss 0.00262086046859622 Validation loss 0.010648391209542751 Accuracy 0.8857421875\n",
      "Iteration 43160 Training loss 0.00379352574236691 Validation loss 0.010776229202747345 Accuracy 0.88525390625\n",
      "Iteration 43170 Training loss 0.0035548838786780834 Validation loss 0.010767641477286816 Accuracy 0.88525390625\n",
      "Iteration 43180 Training loss 0.0030283050145953894 Validation loss 0.010597418062388897 Accuracy 0.8857421875\n",
      "Iteration 43190 Training loss 0.0041106888093054295 Validation loss 0.01063974667340517 Accuracy 0.88623046875\n",
      "Iteration 43200 Training loss 0.0037712426856160164 Validation loss 0.010643655434250832 Accuracy 0.88525390625\n",
      "Iteration 43210 Training loss 0.0031903653871268034 Validation loss 0.010542700067162514 Accuracy 0.88623046875\n",
      "Iteration 43220 Training loss 0.003776431316509843 Validation loss 0.010543324053287506 Accuracy 0.8876953125\n",
      "Iteration 43230 Training loss 0.002426769118756056 Validation loss 0.010701015591621399 Accuracy 0.8857421875\n",
      "Iteration 43240 Training loss 0.0036855051293969154 Validation loss 0.011320219375193119 Accuracy 0.87841796875\n",
      "Iteration 43250 Training loss 0.003759271465241909 Validation loss 0.010636474005877972 Accuracy 0.88671875\n",
      "Iteration 43260 Training loss 0.00259147840552032 Validation loss 0.010594562627375126 Accuracy 0.88525390625\n",
      "Iteration 43270 Training loss 0.0028511567506939173 Validation loss 0.010685000568628311 Accuracy 0.88427734375\n",
      "Iteration 43280 Training loss 0.003783458610996604 Validation loss 0.01057412289083004 Accuracy 0.8857421875\n",
      "Iteration 43290 Training loss 0.003392287530004978 Validation loss 0.010638942942023277 Accuracy 0.88525390625\n",
      "Iteration 43300 Training loss 0.0037529771216213703 Validation loss 0.010472983121871948 Accuracy 0.88720703125\n",
      "Iteration 43310 Training loss 0.0027059824205935 Validation loss 0.010464834049344063 Accuracy 0.88720703125\n",
      "Iteration 43320 Training loss 0.0021741287782788277 Validation loss 0.01071704551577568 Accuracy 0.8857421875\n",
      "Iteration 43330 Training loss 0.003521881066262722 Validation loss 0.010545369237661362 Accuracy 0.88720703125\n",
      "Iteration 43340 Training loss 0.0024445110466331244 Validation loss 0.010745839215815067 Accuracy 0.8857421875\n",
      "Iteration 43350 Training loss 0.0019077910110354424 Validation loss 0.010498955845832825 Accuracy 0.8876953125\n",
      "Iteration 43360 Training loss 0.002843189751729369 Validation loss 0.010414795018732548 Accuracy 0.8876953125\n",
      "Iteration 43370 Training loss 0.0022045571822673082 Validation loss 0.010650199837982655 Accuracy 0.8857421875\n",
      "Iteration 43380 Training loss 0.002639778656885028 Validation loss 0.010718943551182747 Accuracy 0.88427734375\n",
      "Iteration 43390 Training loss 0.0028830391820520163 Validation loss 0.01077308040112257 Accuracy 0.88427734375\n",
      "Iteration 43400 Training loss 0.00339674623683095 Validation loss 0.010667025111615658 Accuracy 0.88525390625\n",
      "Iteration 43410 Training loss 0.0032579945400357246 Validation loss 0.010794579982757568 Accuracy 0.88525390625\n",
      "Iteration 43420 Training loss 0.0026601182762533426 Validation loss 0.01076617743819952 Accuracy 0.88330078125\n",
      "Iteration 43430 Training loss 0.004529840312898159 Validation loss 0.010618251748383045 Accuracy 0.8857421875\n",
      "Iteration 43440 Training loss 0.004071689676493406 Validation loss 0.010673449374735355 Accuracy 0.884765625\n",
      "Iteration 43450 Training loss 0.0034798826090991497 Validation loss 0.010897309519350529 Accuracy 0.8818359375\n",
      "Iteration 43460 Training loss 0.0031682541593909264 Validation loss 0.01081271655857563 Accuracy 0.8828125\n",
      "Iteration 43470 Training loss 0.004456689581274986 Validation loss 0.01067390851676464 Accuracy 0.8857421875\n",
      "Iteration 43480 Training loss 0.0028034579008817673 Validation loss 0.010767862200737 Accuracy 0.88525390625\n",
      "Iteration 43490 Training loss 0.003684673923999071 Validation loss 0.01069034356623888 Accuracy 0.8857421875\n",
      "Iteration 43500 Training loss 0.002853497164323926 Validation loss 0.0106031633913517 Accuracy 0.88623046875\n",
      "Iteration 43510 Training loss 0.0030095456168055534 Validation loss 0.01068070251494646 Accuracy 0.884765625\n",
      "Iteration 43520 Training loss 0.0038725254125893116 Validation loss 0.010524146258831024 Accuracy 0.88720703125\n",
      "Iteration 43530 Training loss 0.0026685886550694704 Validation loss 0.010595392435789108 Accuracy 0.88623046875\n",
      "Iteration 43540 Training loss 0.002577745821326971 Validation loss 0.010759394615888596 Accuracy 0.8837890625\n",
      "Iteration 43550 Training loss 0.0028490759432315826 Validation loss 0.010505951941013336 Accuracy 0.88671875\n",
      "Iteration 43560 Training loss 0.002433009212836623 Validation loss 0.010573063977062702 Accuracy 0.88671875\n",
      "Iteration 43570 Training loss 0.003622195916250348 Validation loss 0.010596295818686485 Accuracy 0.88623046875\n",
      "Iteration 43580 Training loss 0.0034555222373455763 Validation loss 0.01067711878567934 Accuracy 0.8857421875\n",
      "Iteration 43590 Training loss 0.002529617166146636 Validation loss 0.010499111376702785 Accuracy 0.88818359375\n",
      "Iteration 43600 Training loss 0.0026781968772411346 Validation loss 0.010743198916316032 Accuracy 0.8837890625\n",
      "Iteration 43610 Training loss 0.002972551854327321 Validation loss 0.010772584937512875 Accuracy 0.8837890625\n",
      "Iteration 43620 Training loss 0.004266245290637016 Validation loss 0.010623800568282604 Accuracy 0.88623046875\n",
      "Iteration 43630 Training loss 0.0035642993170768023 Validation loss 0.010672258213162422 Accuracy 0.88427734375\n",
      "Iteration 43640 Training loss 0.0026714708656072617 Validation loss 0.010586386546492577 Accuracy 0.88623046875\n",
      "Iteration 43650 Training loss 0.0017719307215884328 Validation loss 0.010686306282877922 Accuracy 0.884765625\n",
      "Iteration 43660 Training loss 0.004111915826797485 Validation loss 0.010520871728658676 Accuracy 0.8876953125\n",
      "Iteration 43670 Training loss 0.00234472774900496 Validation loss 0.010531923733651638 Accuracy 0.88623046875\n",
      "Iteration 43680 Training loss 0.0028314103838056326 Validation loss 0.010699720121920109 Accuracy 0.884765625\n",
      "Iteration 43690 Training loss 0.0035121766850352287 Validation loss 0.010665380395948887 Accuracy 0.88671875\n",
      "Iteration 43700 Training loss 0.004745837301015854 Validation loss 0.010650424286723137 Accuracy 0.88623046875\n",
      "Iteration 43710 Training loss 0.004213080741465092 Validation loss 0.01070246659219265 Accuracy 0.8857421875\n",
      "Iteration 43720 Training loss 0.0038109777960926294 Validation loss 0.010571049526333809 Accuracy 0.88623046875\n",
      "Iteration 43730 Training loss 0.0025257703382521868 Validation loss 0.010692643001675606 Accuracy 0.88623046875\n",
      "Iteration 43740 Training loss 0.003246262203902006 Validation loss 0.010502837598323822 Accuracy 0.88720703125\n",
      "Iteration 43750 Training loss 0.0022925653029233217 Validation loss 0.01058558002114296 Accuracy 0.8857421875\n",
      "Iteration 43760 Training loss 0.0021510536316782236 Validation loss 0.010627989657223225 Accuracy 0.88671875\n",
      "Iteration 43770 Training loss 0.0036628853995352983 Validation loss 0.010687639936804771 Accuracy 0.88427734375\n",
      "Iteration 43780 Training loss 0.0028595682233572006 Validation loss 0.010640610940754414 Accuracy 0.8857421875\n",
      "Iteration 43790 Training loss 0.0032507088035345078 Validation loss 0.010623457841575146 Accuracy 0.884765625\n",
      "Iteration 43800 Training loss 0.0034427775535732508 Validation loss 0.010717760771512985 Accuracy 0.884765625\n",
      "Iteration 43810 Training loss 0.0038962687831372023 Validation loss 0.010745061561465263 Accuracy 0.88427734375\n",
      "Iteration 43820 Training loss 0.0035376872401684523 Validation loss 0.010861112736165524 Accuracy 0.88330078125\n",
      "Iteration 43830 Training loss 0.0037728403694927692 Validation loss 0.010938824154436588 Accuracy 0.8828125\n",
      "Iteration 43840 Training loss 0.002412542700767517 Validation loss 0.010694729164242744 Accuracy 0.88525390625\n",
      "Iteration 43850 Training loss 0.0036924686282873154 Validation loss 0.01066662184894085 Accuracy 0.8857421875\n",
      "Iteration 43860 Training loss 0.002308448078110814 Validation loss 0.010577099397778511 Accuracy 0.8857421875\n",
      "Iteration 43870 Training loss 0.003142639761790633 Validation loss 0.01076273899525404 Accuracy 0.884765625\n",
      "Iteration 43880 Training loss 0.0028451576363295317 Validation loss 0.010791991837322712 Accuracy 0.8837890625\n",
      "Iteration 43890 Training loss 0.002769823418930173 Validation loss 0.010699198581278324 Accuracy 0.88427734375\n",
      "Iteration 43900 Training loss 0.00337530137039721 Validation loss 0.010701064020395279 Accuracy 0.88525390625\n",
      "Iteration 43910 Training loss 0.003933677449822426 Validation loss 0.010757463052868843 Accuracy 0.88330078125\n",
      "Iteration 43920 Training loss 0.0035400851629674435 Validation loss 0.010805265977978706 Accuracy 0.88427734375\n",
      "Iteration 43930 Training loss 0.0021950984373688698 Validation loss 0.010741034522652626 Accuracy 0.884765625\n",
      "Iteration 43940 Training loss 0.003022278193384409 Validation loss 0.01087250467389822 Accuracy 0.88232421875\n",
      "Iteration 43950 Training loss 0.0022665958385914564 Validation loss 0.010695111937820911 Accuracy 0.88525390625\n",
      "Iteration 43960 Training loss 0.004359654616564512 Validation loss 0.010777927935123444 Accuracy 0.88623046875\n",
      "Iteration 43970 Training loss 0.0030180790927261114 Validation loss 0.010852071456611156 Accuracy 0.88330078125\n",
      "Iteration 43980 Training loss 0.002497021108865738 Validation loss 0.01081538014113903 Accuracy 0.88330078125\n",
      "Iteration 43990 Training loss 0.0028980127535760403 Validation loss 0.010693512856960297 Accuracy 0.88525390625\n",
      "Iteration 44000 Training loss 0.002512105042114854 Validation loss 0.010733582079410553 Accuracy 0.88525390625\n",
      "Iteration 44010 Training loss 0.0034256966318935156 Validation loss 0.010709100402891636 Accuracy 0.884765625\n",
      "Iteration 44020 Training loss 0.0036209067329764366 Validation loss 0.010648042894899845 Accuracy 0.88623046875\n",
      "Iteration 44030 Training loss 0.0032602993305772543 Validation loss 0.010763893835246563 Accuracy 0.884765625\n",
      "Iteration 44040 Training loss 0.0037086522206664085 Validation loss 0.010886513628065586 Accuracy 0.88232421875\n",
      "Iteration 44050 Training loss 0.0032856445759534836 Validation loss 0.010790151543915272 Accuracy 0.88427734375\n",
      "Iteration 44060 Training loss 0.003955204505473375 Validation loss 0.010928626172244549 Accuracy 0.8828125\n",
      "Iteration 44070 Training loss 0.0031229699961841106 Validation loss 0.010866200551390648 Accuracy 0.8828125\n",
      "Iteration 44080 Training loss 0.002830593613907695 Validation loss 0.01096384972333908 Accuracy 0.88232421875\n",
      "Iteration 44090 Training loss 0.004313889890909195 Validation loss 0.010949301533401012 Accuracy 0.8818359375\n",
      "Iteration 44100 Training loss 0.0022802178282290697 Validation loss 0.011051452718675137 Accuracy 0.8818359375\n",
      "Iteration 44110 Training loss 0.003654740983620286 Validation loss 0.010871664620935917 Accuracy 0.8828125\n",
      "Iteration 44120 Training loss 0.002663949504494667 Validation loss 0.010853917337954044 Accuracy 0.88330078125\n",
      "Iteration 44130 Training loss 0.003414709120988846 Validation loss 0.010940446518361568 Accuracy 0.8828125\n",
      "Iteration 44140 Training loss 0.0034411961678415537 Validation loss 0.010880221612751484 Accuracy 0.88330078125\n",
      "Iteration 44150 Training loss 0.0035450260620564222 Validation loss 0.010877043008804321 Accuracy 0.8837890625\n",
      "Iteration 44160 Training loss 0.003397765103727579 Validation loss 0.010882665403187275 Accuracy 0.8837890625\n",
      "Iteration 44170 Training loss 0.0035241087898612022 Validation loss 0.010874731466174126 Accuracy 0.8837890625\n",
      "Iteration 44180 Training loss 0.00297885132022202 Validation loss 0.010846978053450584 Accuracy 0.8837890625\n",
      "Iteration 44190 Training loss 0.0028962395153939724 Validation loss 0.011125299148261547 Accuracy 0.880859375\n",
      "Iteration 44200 Training loss 0.0029234387911856174 Validation loss 0.01088419184088707 Accuracy 0.88330078125\n",
      "Iteration 44210 Training loss 0.0035950113087892532 Validation loss 0.010866757482290268 Accuracy 0.8837890625\n",
      "Iteration 44220 Training loss 0.0028011782560497522 Validation loss 0.01082372572273016 Accuracy 0.8837890625\n",
      "Iteration 44230 Training loss 0.003782095853239298 Validation loss 0.010823319666087627 Accuracy 0.88330078125\n",
      "Iteration 44240 Training loss 0.0026539727114140987 Validation loss 0.010724836960434914 Accuracy 0.884765625\n",
      "Iteration 44250 Training loss 0.002717049792408943 Validation loss 0.010818066075444221 Accuracy 0.8837890625\n",
      "Iteration 44260 Training loss 0.002657763659954071 Validation loss 0.010561729781329632 Accuracy 0.88623046875\n",
      "Iteration 44270 Training loss 0.0028130861464887857 Validation loss 0.010643956251442432 Accuracy 0.8857421875\n",
      "Iteration 44280 Training loss 0.0031628322321921587 Validation loss 0.010536924935877323 Accuracy 0.88720703125\n",
      "Iteration 44290 Training loss 0.004038010723888874 Validation loss 0.010809484869241714 Accuracy 0.8837890625\n",
      "Iteration 44300 Training loss 0.0031004950869828463 Validation loss 0.010729154571890831 Accuracy 0.884765625\n",
      "Iteration 44310 Training loss 0.0033865601290017366 Validation loss 0.010621354915201664 Accuracy 0.88623046875\n",
      "Iteration 44320 Training loss 0.002455378184095025 Validation loss 0.010658567771315575 Accuracy 0.88623046875\n",
      "Iteration 44330 Training loss 0.002317163860425353 Validation loss 0.010768922977149487 Accuracy 0.8837890625\n",
      "Iteration 44340 Training loss 0.004080381244421005 Validation loss 0.010721425525844097 Accuracy 0.88427734375\n",
      "Iteration 44350 Training loss 0.003102795220911503 Validation loss 0.010734504088759422 Accuracy 0.8837890625\n",
      "Iteration 44360 Training loss 0.0030501035507768393 Validation loss 0.010931672528386116 Accuracy 0.8818359375\n",
      "Iteration 44370 Training loss 0.004327910020947456 Validation loss 0.010705417022109032 Accuracy 0.88525390625\n",
      "Iteration 44380 Training loss 0.002195813925936818 Validation loss 0.010757277719676495 Accuracy 0.88427734375\n",
      "Iteration 44390 Training loss 0.0025789164938032627 Validation loss 0.010804273188114166 Accuracy 0.8837890625\n",
      "Iteration 44400 Training loss 0.002947867615148425 Validation loss 0.010615619830787182 Accuracy 0.88671875\n",
      "Iteration 44410 Training loss 0.003809420159086585 Validation loss 0.010649366304278374 Accuracy 0.884765625\n",
      "Iteration 44420 Training loss 0.004162325523793697 Validation loss 0.010635319165885448 Accuracy 0.88427734375\n",
      "Iteration 44430 Training loss 0.002702775876969099 Validation loss 0.01056775264441967 Accuracy 0.88623046875\n",
      "Iteration 44440 Training loss 0.002284494461491704 Validation loss 0.01056625321507454 Accuracy 0.88623046875\n",
      "Iteration 44450 Training loss 0.0033547459170222282 Validation loss 0.010759560391306877 Accuracy 0.884765625\n",
      "Iteration 44460 Training loss 0.003619581228122115 Validation loss 0.010731554590165615 Accuracy 0.8837890625\n",
      "Iteration 44470 Training loss 0.0036875600926578045 Validation loss 0.010889487341046333 Accuracy 0.88232421875\n",
      "Iteration 44480 Training loss 0.003240046091377735 Validation loss 0.01062419917434454 Accuracy 0.88623046875\n",
      "Iteration 44490 Training loss 0.002787858946248889 Validation loss 0.01079415250569582 Accuracy 0.88330078125\n",
      "Iteration 44500 Training loss 0.0036897265817970037 Validation loss 0.010766876861453056 Accuracy 0.8837890625\n",
      "Iteration 44510 Training loss 0.002755426801741123 Validation loss 0.01075101736932993 Accuracy 0.88427734375\n",
      "Iteration 44520 Training loss 0.0036651541013270617 Validation loss 0.010650415904819965 Accuracy 0.884765625\n",
      "Iteration 44530 Training loss 0.0044106608256697655 Validation loss 0.01097075641155243 Accuracy 0.88232421875\n",
      "Iteration 44540 Training loss 0.0037321129348129034 Validation loss 0.010707821696996689 Accuracy 0.884765625\n",
      "Iteration 44550 Training loss 0.003033902496099472 Validation loss 0.010684213601052761 Accuracy 0.884765625\n",
      "Iteration 44560 Training loss 0.004189174622297287 Validation loss 0.010703892447054386 Accuracy 0.884765625\n",
      "Iteration 44570 Training loss 0.0024560189340263605 Validation loss 0.010707658715546131 Accuracy 0.88525390625\n",
      "Iteration 44580 Training loss 0.00387491169385612 Validation loss 0.01073025818914175 Accuracy 0.88525390625\n",
      "Iteration 44590 Training loss 0.004191360902041197 Validation loss 0.010605852119624615 Accuracy 0.88623046875\n",
      "Iteration 44600 Training loss 0.0030172092374414206 Validation loss 0.010613112710416317 Accuracy 0.8857421875\n",
      "Iteration 44610 Training loss 0.002283101435750723 Validation loss 0.010639730840921402 Accuracy 0.88525390625\n",
      "Iteration 44620 Training loss 0.0021754922345280647 Validation loss 0.010667752474546432 Accuracy 0.88525390625\n",
      "Iteration 44630 Training loss 0.0024539176374673843 Validation loss 0.010643496178090572 Accuracy 0.884765625\n",
      "Iteration 44640 Training loss 0.004407824017107487 Validation loss 0.010679470375180244 Accuracy 0.884765625\n",
      "Iteration 44650 Training loss 0.0038196074310690165 Validation loss 0.010652459226548672 Accuracy 0.88427734375\n",
      "Iteration 44660 Training loss 0.0016405553324148059 Validation loss 0.010804361663758755 Accuracy 0.88427734375\n",
      "Iteration 44670 Training loss 0.002403398510068655 Validation loss 0.010665043257176876 Accuracy 0.8857421875\n",
      "Iteration 44680 Training loss 0.0037878158036619425 Validation loss 0.010564611293375492 Accuracy 0.88623046875\n",
      "Iteration 44690 Training loss 0.0022808751091361046 Validation loss 0.010521230287849903 Accuracy 0.88720703125\n",
      "Iteration 44700 Training loss 0.003575672395527363 Validation loss 0.010667726397514343 Accuracy 0.88525390625\n",
      "Iteration 44710 Training loss 0.0036804755218327045 Validation loss 0.010742207057774067 Accuracy 0.88427734375\n",
      "Iteration 44720 Training loss 0.003103247843682766 Validation loss 0.010677658021450043 Accuracy 0.88623046875\n",
      "Iteration 44730 Training loss 0.0030299376230686903 Validation loss 0.010848430916666985 Accuracy 0.88232421875\n",
      "Iteration 44740 Training loss 0.0035578873939812183 Validation loss 0.010843581520020962 Accuracy 0.88330078125\n",
      "Iteration 44750 Training loss 0.003666250267997384 Validation loss 0.010864541865885258 Accuracy 0.88330078125\n",
      "Iteration 44760 Training loss 0.0035083817783743143 Validation loss 0.010675141587853432 Accuracy 0.8857421875\n",
      "Iteration 44770 Training loss 0.0028246999718248844 Validation loss 0.0106230853125453 Accuracy 0.88623046875\n",
      "Iteration 44780 Training loss 0.0035392700228840113 Validation loss 0.010686174035072327 Accuracy 0.88427734375\n",
      "Iteration 44790 Training loss 0.0017640263540670276 Validation loss 0.010714098811149597 Accuracy 0.884765625\n",
      "Iteration 44800 Training loss 0.0036121548619121313 Validation loss 0.010608985088765621 Accuracy 0.88671875\n",
      "Iteration 44810 Training loss 0.003244862426072359 Validation loss 0.010765539482235909 Accuracy 0.8837890625\n",
      "Iteration 44820 Training loss 0.0042574149556458 Validation loss 0.010768609121441841 Accuracy 0.884765625\n",
      "Iteration 44830 Training loss 0.002694072900339961 Validation loss 0.010656364262104034 Accuracy 0.8857421875\n",
      "Iteration 44840 Training loss 0.001963603077456355 Validation loss 0.010675644502043724 Accuracy 0.884765625\n",
      "Iteration 44850 Training loss 0.002518025226891041 Validation loss 0.0108222970739007 Accuracy 0.8837890625\n",
      "Iteration 44860 Training loss 0.0034349130000919104 Validation loss 0.010640480555593967 Accuracy 0.88671875\n",
      "Iteration 44870 Training loss 0.003440509084612131 Validation loss 0.010813465341925621 Accuracy 0.8837890625\n",
      "Iteration 44880 Training loss 0.003465634537860751 Validation loss 0.010851128026843071 Accuracy 0.8828125\n",
      "Iteration 44890 Training loss 0.001839072210714221 Validation loss 0.01072730217128992 Accuracy 0.88623046875\n",
      "Iteration 44900 Training loss 0.00318328058347106 Validation loss 0.010649381205439568 Accuracy 0.8857421875\n",
      "Iteration 44910 Training loss 0.003437418257817626 Validation loss 0.010613063350319862 Accuracy 0.88623046875\n",
      "Iteration 44920 Training loss 0.0034428329672664404 Validation loss 0.01070384494960308 Accuracy 0.884765625\n",
      "Iteration 44930 Training loss 0.0033459924161434174 Validation loss 0.010735644027590752 Accuracy 0.8837890625\n",
      "Iteration 44940 Training loss 0.0033011646009981632 Validation loss 0.01065994892269373 Accuracy 0.8857421875\n",
      "Iteration 44950 Training loss 0.0026141004636883736 Validation loss 0.010769728571176529 Accuracy 0.88427734375\n",
      "Iteration 44960 Training loss 0.003050577826797962 Validation loss 0.010837206616997719 Accuracy 0.88427734375\n",
      "Iteration 44970 Training loss 0.002783013042062521 Validation loss 0.010654189623892307 Accuracy 0.8857421875\n",
      "Iteration 44980 Training loss 0.0024897686671465635 Validation loss 0.010667516849935055 Accuracy 0.8857421875\n",
      "Iteration 44990 Training loss 0.004751409869641066 Validation loss 0.010816887021064758 Accuracy 0.8837890625\n",
      "Iteration 45000 Training loss 0.0020667097996920347 Validation loss 0.010791923850774765 Accuracy 0.884765625\n",
      "Iteration 45010 Training loss 0.002640285762026906 Validation loss 0.010835863649845123 Accuracy 0.8837890625\n",
      "Iteration 45020 Training loss 0.004318063147366047 Validation loss 0.010835262015461922 Accuracy 0.88330078125\n",
      "Iteration 45030 Training loss 0.0021837169770151377 Validation loss 0.010721943341195583 Accuracy 0.88427734375\n",
      "Iteration 45040 Training loss 0.0012258682399988174 Validation loss 0.010789615102112293 Accuracy 0.8837890625\n",
      "Iteration 45050 Training loss 0.003808677662163973 Validation loss 0.010636902414262295 Accuracy 0.8857421875\n",
      "Iteration 45060 Training loss 0.0030562984757125378 Validation loss 0.010836580768227577 Accuracy 0.8828125\n",
      "Iteration 45070 Training loss 0.0026778888422995806 Validation loss 0.010679514147341251 Accuracy 0.88525390625\n",
      "Iteration 45080 Training loss 0.002651395509019494 Validation loss 0.010790424421429634 Accuracy 0.8837890625\n",
      "Iteration 45090 Training loss 0.0027414073701947927 Validation loss 0.010990936309099197 Accuracy 0.8818359375\n",
      "Iteration 45100 Training loss 0.004281431902199984 Validation loss 0.010818964801728725 Accuracy 0.88427734375\n",
      "Iteration 45110 Training loss 0.002522441791370511 Validation loss 0.01100603025406599 Accuracy 0.8818359375\n",
      "Iteration 45120 Training loss 0.003300166456028819 Validation loss 0.010769295506179333 Accuracy 0.88427734375\n",
      "Iteration 45130 Training loss 0.00192247424274683 Validation loss 0.01064543891698122 Accuracy 0.88671875\n",
      "Iteration 45140 Training loss 0.0027269073761999607 Validation loss 0.01067738514393568 Accuracy 0.884765625\n",
      "Iteration 45150 Training loss 0.003050445578992367 Validation loss 0.010769503191113472 Accuracy 0.8837890625\n",
      "Iteration 45160 Training loss 0.0024618152529001236 Validation loss 0.010876896791160107 Accuracy 0.88330078125\n",
      "Iteration 45170 Training loss 0.0023879350628703833 Validation loss 0.010808732360601425 Accuracy 0.884765625\n",
      "Iteration 45180 Training loss 0.0032485343981534243 Validation loss 0.010633922182023525 Accuracy 0.88623046875\n",
      "Iteration 45190 Training loss 0.002670773770660162 Validation loss 0.010738571174442768 Accuracy 0.884765625\n",
      "Iteration 45200 Training loss 0.003530242945998907 Validation loss 0.010791592299938202 Accuracy 0.88427734375\n",
      "Iteration 45210 Training loss 0.0038316077552735806 Validation loss 0.01085314154624939 Accuracy 0.88330078125\n",
      "Iteration 45220 Training loss 0.0017229049699380994 Validation loss 0.010705218650400639 Accuracy 0.88427734375\n",
      "Iteration 45230 Training loss 0.003910844214260578 Validation loss 0.010783986188471317 Accuracy 0.88427734375\n",
      "Iteration 45240 Training loss 0.004503238946199417 Validation loss 0.01061108335852623 Accuracy 0.88623046875\n",
      "Iteration 45250 Training loss 0.003059124806895852 Validation loss 0.010742979124188423 Accuracy 0.88427734375\n",
      "Iteration 45260 Training loss 0.002301018452271819 Validation loss 0.010692699812352657 Accuracy 0.884765625\n",
      "Iteration 45270 Training loss 0.0021465958561748266 Validation loss 0.01067445520311594 Accuracy 0.8837890625\n",
      "Iteration 45280 Training loss 0.002993667032569647 Validation loss 0.010560528375208378 Accuracy 0.88671875\n",
      "Iteration 45290 Training loss 0.002916737226769328 Validation loss 0.01057110819965601 Accuracy 0.88623046875\n",
      "Iteration 45300 Training loss 0.003821259131655097 Validation loss 0.010783585719764233 Accuracy 0.88330078125\n",
      "Iteration 45310 Training loss 0.002457896713167429 Validation loss 0.010722039267420769 Accuracy 0.884765625\n",
      "Iteration 45320 Training loss 0.002266732743009925 Validation loss 0.01089642196893692 Accuracy 0.88232421875\n",
      "Iteration 45330 Training loss 0.0033242113422602415 Validation loss 0.010906166397035122 Accuracy 0.88232421875\n",
      "Iteration 45340 Training loss 0.002975670387968421 Validation loss 0.010599552653729916 Accuracy 0.88623046875\n",
      "Iteration 45350 Training loss 0.002264341339468956 Validation loss 0.010766196064651012 Accuracy 0.88427734375\n",
      "Iteration 45360 Training loss 0.0030062971636652946 Validation loss 0.010708678513765335 Accuracy 0.8837890625\n",
      "Iteration 45370 Training loss 0.003424611408263445 Validation loss 0.010667930357158184 Accuracy 0.88525390625\n",
      "Iteration 45380 Training loss 0.003345800330862403 Validation loss 0.010647187009453773 Accuracy 0.88525390625\n",
      "Iteration 45390 Training loss 0.0026587373577058315 Validation loss 0.010679648257791996 Accuracy 0.88330078125\n",
      "Iteration 45400 Training loss 0.0034594624303281307 Validation loss 0.010694297961890697 Accuracy 0.8857421875\n",
      "Iteration 45410 Training loss 0.003319180104881525 Validation loss 0.01060060877352953 Accuracy 0.8857421875\n",
      "Iteration 45420 Training loss 0.00329402438364923 Validation loss 0.010628610849380493 Accuracy 0.8857421875\n",
      "Iteration 45430 Training loss 0.003040400566533208 Validation loss 0.010786280035972595 Accuracy 0.88525390625\n",
      "Iteration 45440 Training loss 0.003648871323093772 Validation loss 0.010644521564245224 Accuracy 0.88623046875\n",
      "Iteration 45450 Training loss 0.0026831740979105234 Validation loss 0.010602721944451332 Accuracy 0.8857421875\n",
      "Iteration 45460 Training loss 0.0025508450344204903 Validation loss 0.010755534283816814 Accuracy 0.88427734375\n",
      "Iteration 45470 Training loss 0.0031792912632226944 Validation loss 0.01081449817866087 Accuracy 0.88427734375\n",
      "Iteration 45480 Training loss 0.0035040511284023523 Validation loss 0.01079173106700182 Accuracy 0.88427734375\n",
      "Iteration 45490 Training loss 0.0024586825165897608 Validation loss 0.010575971566140652 Accuracy 0.88671875\n",
      "Iteration 45500 Training loss 0.002722501754760742 Validation loss 0.010671116411685944 Accuracy 0.88525390625\n",
      "Iteration 45510 Training loss 0.002737971255555749 Validation loss 0.01067936047911644 Accuracy 0.884765625\n",
      "Iteration 45520 Training loss 0.0026229186914861202 Validation loss 0.010706447064876556 Accuracy 0.88427734375\n",
      "Iteration 45530 Training loss 0.003564703045412898 Validation loss 0.010726364329457283 Accuracy 0.884765625\n",
      "Iteration 45540 Training loss 0.0034086285158991814 Validation loss 0.010649469681084156 Accuracy 0.88427734375\n",
      "Iteration 45550 Training loss 0.0026155286468565464 Validation loss 0.010712064802646637 Accuracy 0.8857421875\n",
      "Iteration 45560 Training loss 0.0034470907412469387 Validation loss 0.010632108896970749 Accuracy 0.88623046875\n",
      "Iteration 45570 Training loss 0.004327406641095877 Validation loss 0.01063541229814291 Accuracy 0.8857421875\n",
      "Iteration 45580 Training loss 0.0029190084896981716 Validation loss 0.010932397097349167 Accuracy 0.88330078125\n",
      "Iteration 45590 Training loss 0.002952211769297719 Validation loss 0.010861757211387157 Accuracy 0.8828125\n",
      "Iteration 45600 Training loss 0.0024112078826874495 Validation loss 0.010618966072797775 Accuracy 0.8857421875\n",
      "Iteration 45610 Training loss 0.003372492967173457 Validation loss 0.010660534724593163 Accuracy 0.88671875\n",
      "Iteration 45620 Training loss 0.003841657657176256 Validation loss 0.0106187229976058 Accuracy 0.88623046875\n",
      "Iteration 45630 Training loss 0.003960391506552696 Validation loss 0.010616512969136238 Accuracy 0.8857421875\n",
      "Iteration 45640 Training loss 0.0029963203705847263 Validation loss 0.010677996091544628 Accuracy 0.8857421875\n",
      "Iteration 45650 Training loss 0.002840779023244977 Validation loss 0.010625075548887253 Accuracy 0.88623046875\n",
      "Iteration 45660 Training loss 0.0029201454017311335 Validation loss 0.010648192837834358 Accuracy 0.88623046875\n",
      "Iteration 45670 Training loss 0.0026001017540693283 Validation loss 0.010563777759671211 Accuracy 0.8876953125\n",
      "Iteration 45680 Training loss 0.0031783466693013906 Validation loss 0.010709631256759167 Accuracy 0.884765625\n",
      "Iteration 45690 Training loss 0.0017618306446820498 Validation loss 0.010708294808864594 Accuracy 0.8857421875\n",
      "Iteration 45700 Training loss 0.003973101731389761 Validation loss 0.010741114616394043 Accuracy 0.884765625\n",
      "Iteration 45710 Training loss 0.0036879493854939938 Validation loss 0.010700048878788948 Accuracy 0.884765625\n",
      "Iteration 45720 Training loss 0.002075828844681382 Validation loss 0.01047674659639597 Accuracy 0.8876953125\n",
      "Iteration 45730 Training loss 0.004341174848377705 Validation loss 0.010502099059522152 Accuracy 0.88720703125\n",
      "Iteration 45740 Training loss 0.0026512013282626867 Validation loss 0.010549969971179962 Accuracy 0.8857421875\n",
      "Iteration 45750 Training loss 0.0024588550440967083 Validation loss 0.010497288778424263 Accuracy 0.88671875\n",
      "Iteration 45760 Training loss 0.0018623938085511327 Validation loss 0.010470923967659473 Accuracy 0.88720703125\n",
      "Iteration 45770 Training loss 0.0019345175242051482 Validation loss 0.010656323283910751 Accuracy 0.88427734375\n",
      "Iteration 45780 Training loss 0.003353756619617343 Validation loss 0.010605888441205025 Accuracy 0.88623046875\n",
      "Iteration 45790 Training loss 0.0022538634948432446 Validation loss 0.010692003183066845 Accuracy 0.884765625\n",
      "Iteration 45800 Training loss 0.0025022891350090504 Validation loss 0.01069649402052164 Accuracy 0.884765625\n",
      "Iteration 45810 Training loss 0.0035629933699965477 Validation loss 0.010709199123084545 Accuracy 0.8857421875\n",
      "Iteration 45820 Training loss 0.0030204078648239374 Validation loss 0.010584477335214615 Accuracy 0.88623046875\n",
      "Iteration 45830 Training loss 0.0031884415075182915 Validation loss 0.010548052377998829 Accuracy 0.8857421875\n",
      "Iteration 45840 Training loss 0.004026284907013178 Validation loss 0.01072765700519085 Accuracy 0.88427734375\n",
      "Iteration 45850 Training loss 0.0030915418174117804 Validation loss 0.010641541332006454 Accuracy 0.8857421875\n",
      "Iteration 45860 Training loss 0.0026876700576394796 Validation loss 0.010976763442158699 Accuracy 0.88232421875\n",
      "Iteration 45870 Training loss 0.004040507134050131 Validation loss 0.010662609711289406 Accuracy 0.88525390625\n",
      "Iteration 45880 Training loss 0.002721525263041258 Validation loss 0.01064352411776781 Accuracy 0.8857421875\n",
      "Iteration 45890 Training loss 0.002312122378498316 Validation loss 0.01065151672810316 Accuracy 0.8857421875\n",
      "Iteration 45900 Training loss 0.0032766987569630146 Validation loss 0.010611169040203094 Accuracy 0.8857421875\n",
      "Iteration 45910 Training loss 0.003195713274180889 Validation loss 0.01057208701968193 Accuracy 0.88623046875\n",
      "Iteration 45920 Training loss 0.003126103663817048 Validation loss 0.010563737712800503 Accuracy 0.88671875\n",
      "Iteration 45930 Training loss 0.0032471735030412674 Validation loss 0.010646024718880653 Accuracy 0.88525390625\n",
      "Iteration 45940 Training loss 0.002657700562849641 Validation loss 0.010445191524922848 Accuracy 0.88623046875\n",
      "Iteration 45950 Training loss 0.0033345979172736406 Validation loss 0.010501083917915821 Accuracy 0.8876953125\n",
      "Iteration 45960 Training loss 0.003528304398059845 Validation loss 0.01062462106347084 Accuracy 0.8857421875\n",
      "Iteration 45970 Training loss 0.002732990775257349 Validation loss 0.01065275352448225 Accuracy 0.884765625\n",
      "Iteration 45980 Training loss 0.0033653220161795616 Validation loss 0.01066880114376545 Accuracy 0.88623046875\n",
      "Iteration 45990 Training loss 0.0027103631291538477 Validation loss 0.010662633925676346 Accuracy 0.88623046875\n",
      "Iteration 46000 Training loss 0.003488316433504224 Validation loss 0.010732104070484638 Accuracy 0.884765625\n",
      "Iteration 46010 Training loss 0.002805037423968315 Validation loss 0.010761374607682228 Accuracy 0.8837890625\n",
      "Iteration 46020 Training loss 0.0032350250985473394 Validation loss 0.010639560408890247 Accuracy 0.884765625\n",
      "Iteration 46030 Training loss 0.0035462912637740374 Validation loss 0.01061417069286108 Accuracy 0.88623046875\n",
      "Iteration 46040 Training loss 0.0036669180262833834 Validation loss 0.010603569447994232 Accuracy 0.8857421875\n",
      "Iteration 46050 Training loss 0.002487247809767723 Validation loss 0.010940623469650745 Accuracy 0.88330078125\n",
      "Iteration 46060 Training loss 0.0026321744080632925 Validation loss 0.010749051347374916 Accuracy 0.88525390625\n",
      "Iteration 46070 Training loss 0.0026678936555981636 Validation loss 0.010593929328024387 Accuracy 0.8857421875\n",
      "Iteration 46080 Training loss 0.0033664193470031023 Validation loss 0.010450365953147411 Accuracy 0.88720703125\n",
      "Iteration 46090 Training loss 0.0022549189161509275 Validation loss 0.01055973581969738 Accuracy 0.88623046875\n",
      "Iteration 46100 Training loss 0.0036452794447541237 Validation loss 0.010502348653972149 Accuracy 0.88720703125\n",
      "Iteration 46110 Training loss 0.002260893117636442 Validation loss 0.010546398349106312 Accuracy 0.88720703125\n",
      "Iteration 46120 Training loss 0.0018954552942886949 Validation loss 0.010546904057264328 Accuracy 0.88671875\n",
      "Iteration 46130 Training loss 0.0026301604229956865 Validation loss 0.010535228997468948 Accuracy 0.88623046875\n",
      "Iteration 46140 Training loss 0.002992387395352125 Validation loss 0.011001543141901493 Accuracy 0.88134765625\n",
      "Iteration 46150 Training loss 0.004831735510379076 Validation loss 0.010808691382408142 Accuracy 0.8837890625\n",
      "Iteration 46160 Training loss 0.00388797908090055 Validation loss 0.010725655592978 Accuracy 0.8837890625\n",
      "Iteration 46170 Training loss 0.00253412127494812 Validation loss 0.010698198340833187 Accuracy 0.88427734375\n",
      "Iteration 46180 Training loss 0.003550122957676649 Validation loss 0.010873730294406414 Accuracy 0.88232421875\n",
      "Iteration 46190 Training loss 0.003514606738463044 Validation loss 0.010757924988865852 Accuracy 0.88427734375\n",
      "Iteration 46200 Training loss 0.0038051337469369173 Validation loss 0.010595371015369892 Accuracy 0.88671875\n",
      "Iteration 46210 Training loss 0.003125250805169344 Validation loss 0.010796916671097279 Accuracy 0.88427734375\n",
      "Iteration 46220 Training loss 0.0023483692202717066 Validation loss 0.010684069246053696 Accuracy 0.88427734375\n",
      "Iteration 46230 Training loss 0.0023584100417792797 Validation loss 0.010641832835972309 Accuracy 0.88525390625\n",
      "Iteration 46240 Training loss 0.003966503776609898 Validation loss 0.010748139582574368 Accuracy 0.884765625\n",
      "Iteration 46250 Training loss 0.0032735855784267187 Validation loss 0.010559627786278725 Accuracy 0.88671875\n",
      "Iteration 46260 Training loss 0.002494300249963999 Validation loss 0.010510588064789772 Accuracy 0.88671875\n",
      "Iteration 46270 Training loss 0.0025604753755033016 Validation loss 0.010545508936047554 Accuracy 0.88671875\n",
      "Iteration 46280 Training loss 0.0029629208147525787 Validation loss 0.010622845962643623 Accuracy 0.88623046875\n",
      "Iteration 46290 Training loss 0.0034265187568962574 Validation loss 0.010725614614784718 Accuracy 0.884765625\n",
      "Iteration 46300 Training loss 0.0034205468837171793 Validation loss 0.01069931872189045 Accuracy 0.88427734375\n",
      "Iteration 46310 Training loss 0.003999061416834593 Validation loss 0.010741411708295345 Accuracy 0.88330078125\n",
      "Iteration 46320 Training loss 0.0022876497823745012 Validation loss 0.010516483336687088 Accuracy 0.88720703125\n",
      "Iteration 46330 Training loss 0.0035074881743639708 Validation loss 0.010739143006503582 Accuracy 0.8837890625\n",
      "Iteration 46340 Training loss 0.0032133820932358503 Validation loss 0.010618571192026138 Accuracy 0.8857421875\n",
      "Iteration 46350 Training loss 0.0028420593589544296 Validation loss 0.010545242577791214 Accuracy 0.88671875\n",
      "Iteration 46360 Training loss 0.002946866676211357 Validation loss 0.01053601410239935 Accuracy 0.88671875\n",
      "Iteration 46370 Training loss 0.002447958802804351 Validation loss 0.010616001673042774 Accuracy 0.88525390625\n",
      "Iteration 46380 Training loss 0.003235338255763054 Validation loss 0.01075062993913889 Accuracy 0.8837890625\n",
      "Iteration 46390 Training loss 0.0032484473194926977 Validation loss 0.010660853236913681 Accuracy 0.884765625\n",
      "Iteration 46400 Training loss 0.003626585006713867 Validation loss 0.010681159794330597 Accuracy 0.8857421875\n",
      "Iteration 46410 Training loss 0.002501749899238348 Validation loss 0.010660684667527676 Accuracy 0.88525390625\n",
      "Iteration 46420 Training loss 0.002497608307749033 Validation loss 0.010574137791991234 Accuracy 0.8857421875\n",
      "Iteration 46430 Training loss 0.0024536065757274628 Validation loss 0.010541762225329876 Accuracy 0.8876953125\n",
      "Iteration 46440 Training loss 0.004521807190030813 Validation loss 0.010699589736759663 Accuracy 0.88427734375\n",
      "Iteration 46450 Training loss 0.003609316423535347 Validation loss 0.010648617520928383 Accuracy 0.8857421875\n",
      "Iteration 46460 Training loss 0.0028879616875201464 Validation loss 0.010578102432191372 Accuracy 0.88671875\n",
      "Iteration 46470 Training loss 0.0021162747871130705 Validation loss 0.010565186850726604 Accuracy 0.88623046875\n",
      "Iteration 46480 Training loss 0.002656711498275399 Validation loss 0.010519037023186684 Accuracy 0.88720703125\n",
      "Iteration 46490 Training loss 0.0029971497133374214 Validation loss 0.010586102493107319 Accuracy 0.88623046875\n",
      "Iteration 46500 Training loss 0.003131358651444316 Validation loss 0.010579507797956467 Accuracy 0.88671875\n",
      "Iteration 46510 Training loss 0.0042274403385818005 Validation loss 0.010714779607951641 Accuracy 0.8857421875\n",
      "Iteration 46520 Training loss 0.003472070675343275 Validation loss 0.010704835876822472 Accuracy 0.8857421875\n",
      "Iteration 46530 Training loss 0.003802467370405793 Validation loss 0.01059689186513424 Accuracy 0.88623046875\n",
      "Iteration 46540 Training loss 0.002613379154354334 Validation loss 0.010789713822305202 Accuracy 0.88330078125\n",
      "Iteration 46550 Training loss 0.003062991425395012 Validation loss 0.0105734933167696 Accuracy 0.88623046875\n",
      "Iteration 46560 Training loss 0.003757453989237547 Validation loss 0.010542758740484715 Accuracy 0.88720703125\n",
      "Iteration 46570 Training loss 0.0032337941229343414 Validation loss 0.010488582774996758 Accuracy 0.88720703125\n",
      "Iteration 46580 Training loss 0.0018217687029391527 Validation loss 0.010810773819684982 Accuracy 0.8837890625\n",
      "Iteration 46590 Training loss 0.0035075212363153696 Validation loss 0.010631545446813107 Accuracy 0.8857421875\n",
      "Iteration 46600 Training loss 0.0024983463808894157 Validation loss 0.010663656517863274 Accuracy 0.884765625\n",
      "Iteration 46610 Training loss 0.002731866203248501 Validation loss 0.010554623790085316 Accuracy 0.88671875\n",
      "Iteration 46620 Training loss 0.003799266181886196 Validation loss 0.010565795935690403 Accuracy 0.8876953125\n",
      "Iteration 46630 Training loss 0.0024658117908984423 Validation loss 0.010551664978265762 Accuracy 0.88671875\n",
      "Iteration 46640 Training loss 0.002894399920478463 Validation loss 0.010612624697387218 Accuracy 0.88525390625\n",
      "Iteration 46650 Training loss 0.0013337335549294949 Validation loss 0.010593121871352196 Accuracy 0.8857421875\n",
      "Iteration 46660 Training loss 0.002382742241024971 Validation loss 0.010423600673675537 Accuracy 0.8876953125\n",
      "Iteration 46670 Training loss 0.0038594261277467012 Validation loss 0.010688337497413158 Accuracy 0.884765625\n",
      "Iteration 46680 Training loss 0.002317443024367094 Validation loss 0.01067106518894434 Accuracy 0.88427734375\n",
      "Iteration 46690 Training loss 0.003882434917613864 Validation loss 0.010683317668735981 Accuracy 0.8837890625\n",
      "Iteration 46700 Training loss 0.0043048863299191 Validation loss 0.010675163008272648 Accuracy 0.88525390625\n",
      "Iteration 46710 Training loss 0.0031312720384448767 Validation loss 0.010522919707000256 Accuracy 0.88671875\n",
      "Iteration 46720 Training loss 0.002996098715811968 Validation loss 0.010841473937034607 Accuracy 0.8828125\n",
      "Iteration 46730 Training loss 0.0028756256215274334 Validation loss 0.010534035041928291 Accuracy 0.8857421875\n",
      "Iteration 46740 Training loss 0.0036767113488167524 Validation loss 0.010586259886622429 Accuracy 0.88623046875\n",
      "Iteration 46750 Training loss 0.002315480262041092 Validation loss 0.010580104775726795 Accuracy 0.88720703125\n",
      "Iteration 46760 Training loss 0.0027745275292545557 Validation loss 0.010619941167533398 Accuracy 0.88623046875\n",
      "Iteration 46770 Training loss 0.0035703766625374556 Validation loss 0.010669920593500137 Accuracy 0.8857421875\n",
      "Iteration 46780 Training loss 0.0037643788382411003 Validation loss 0.010603436268866062 Accuracy 0.8857421875\n",
      "Iteration 46790 Training loss 0.002846835181117058 Validation loss 0.01080983504652977 Accuracy 0.88427734375\n",
      "Iteration 46800 Training loss 0.002615779871121049 Validation loss 0.010661994107067585 Accuracy 0.884765625\n",
      "Iteration 46810 Training loss 0.0029149604961276054 Validation loss 0.010542390868067741 Accuracy 0.8876953125\n",
      "Iteration 46820 Training loss 0.0026804995723068714 Validation loss 0.010503539815545082 Accuracy 0.88720703125\n",
      "Iteration 46830 Training loss 0.0026088058948516846 Validation loss 0.010755038820207119 Accuracy 0.8837890625\n",
      "Iteration 46840 Training loss 0.003157292725518346 Validation loss 0.010575531050562859 Accuracy 0.88623046875\n",
      "Iteration 46850 Training loss 0.0028719825204461813 Validation loss 0.010589104145765305 Accuracy 0.8857421875\n",
      "Iteration 46860 Training loss 0.002878683153539896 Validation loss 0.010614853352308273 Accuracy 0.8857421875\n",
      "Iteration 46870 Training loss 0.0020832130685448647 Validation loss 0.010475552640855312 Accuracy 0.88623046875\n",
      "Iteration 46880 Training loss 0.0036197120789438486 Validation loss 0.01058349572122097 Accuracy 0.8857421875\n",
      "Iteration 46890 Training loss 0.0013768603093922138 Validation loss 0.0106499707326293 Accuracy 0.88623046875\n",
      "Iteration 46900 Training loss 0.002905289875343442 Validation loss 0.010617121122777462 Accuracy 0.88623046875\n",
      "Iteration 46910 Training loss 0.0028535390738397837 Validation loss 0.010551905259490013 Accuracy 0.8857421875\n",
      "Iteration 46920 Training loss 0.003590567270293832 Validation loss 0.010659173130989075 Accuracy 0.884765625\n",
      "Iteration 46930 Training loss 0.0027361796237528324 Validation loss 0.010650619864463806 Accuracy 0.88671875\n",
      "Iteration 46940 Training loss 0.003247643820941448 Validation loss 0.010681340470910072 Accuracy 0.8857421875\n",
      "Iteration 46950 Training loss 0.002955281175673008 Validation loss 0.01042894460260868 Accuracy 0.888671875\n",
      "Iteration 46960 Training loss 0.002515546977519989 Validation loss 0.010682700201869011 Accuracy 0.884765625\n",
      "Iteration 46970 Training loss 0.001831109868362546 Validation loss 0.010723095387220383 Accuracy 0.884765625\n",
      "Iteration 46980 Training loss 0.0017731329426169395 Validation loss 0.010595527477562428 Accuracy 0.88623046875\n",
      "Iteration 46990 Training loss 0.004488333128392696 Validation loss 0.010590575635433197 Accuracy 0.88671875\n",
      "Iteration 47000 Training loss 0.00236231810413301 Validation loss 0.01051232311874628 Accuracy 0.88720703125\n",
      "Iteration 47010 Training loss 0.003586034057661891 Validation loss 0.010589852929115295 Accuracy 0.88671875\n",
      "Iteration 47020 Training loss 0.0021386388689279556 Validation loss 0.010585525073111057 Accuracy 0.88623046875\n",
      "Iteration 47030 Training loss 0.0034250481985509396 Validation loss 0.010755037888884544 Accuracy 0.88427734375\n",
      "Iteration 47040 Training loss 0.0013707205653190613 Validation loss 0.010588874109089375 Accuracy 0.88623046875\n",
      "Iteration 47050 Training loss 0.0015000645071268082 Validation loss 0.010700041428208351 Accuracy 0.884765625\n",
      "Iteration 47060 Training loss 0.0023743135388940573 Validation loss 0.010746713727712631 Accuracy 0.88427734375\n",
      "Iteration 47070 Training loss 0.003798243124037981 Validation loss 0.010601615533232689 Accuracy 0.88623046875\n",
      "Iteration 47080 Training loss 0.0027732616290450096 Validation loss 0.01054719090461731 Accuracy 0.88671875\n",
      "Iteration 47090 Training loss 0.0027381298132240772 Validation loss 0.010531044565141201 Accuracy 0.88671875\n",
      "Iteration 47100 Training loss 0.004075719974935055 Validation loss 0.010498714633286 Accuracy 0.88671875\n",
      "Iteration 47110 Training loss 0.003529016859829426 Validation loss 0.010502568446099758 Accuracy 0.88671875\n",
      "Iteration 47120 Training loss 0.003248011227697134 Validation loss 0.010609200224280357 Accuracy 0.8857421875\n",
      "Iteration 47130 Training loss 0.003752522636204958 Validation loss 0.010593860410153866 Accuracy 0.8857421875\n",
      "Iteration 47140 Training loss 0.0018243941012769938 Validation loss 0.010632709600031376 Accuracy 0.88525390625\n",
      "Iteration 47150 Training loss 0.0036640542093664408 Validation loss 0.010595622472465038 Accuracy 0.8857421875\n",
      "Iteration 47160 Training loss 0.003060293849557638 Validation loss 0.010888908058404922 Accuracy 0.8818359375\n",
      "Iteration 47170 Training loss 0.0031003712210804224 Validation loss 0.010546342469751835 Accuracy 0.88623046875\n",
      "Iteration 47180 Training loss 0.003332492895424366 Validation loss 0.010677228681743145 Accuracy 0.884765625\n",
      "Iteration 47190 Training loss 0.0017948998138308525 Validation loss 0.010610253550112247 Accuracy 0.884765625\n",
      "Iteration 47200 Training loss 0.004502652212977409 Validation loss 0.01062515377998352 Accuracy 0.8857421875\n",
      "Iteration 47210 Training loss 0.0024888371117413044 Validation loss 0.01060250960290432 Accuracy 0.8857421875\n",
      "Iteration 47220 Training loss 0.0029907021671533585 Validation loss 0.010545581579208374 Accuracy 0.88671875\n",
      "Iteration 47230 Training loss 0.0023055949714034796 Validation loss 0.010658207349479198 Accuracy 0.88525390625\n",
      "Iteration 47240 Training loss 0.003221237799152732 Validation loss 0.010485460981726646 Accuracy 0.88720703125\n",
      "Iteration 47250 Training loss 0.0032185057643800974 Validation loss 0.01064950879663229 Accuracy 0.8857421875\n",
      "Iteration 47260 Training loss 0.002472875639796257 Validation loss 0.010611738078296185 Accuracy 0.8857421875\n",
      "Iteration 47270 Training loss 0.002640959108248353 Validation loss 0.010513263754546642 Accuracy 0.88671875\n",
      "Iteration 47280 Training loss 0.0029321624897420406 Validation loss 0.010711041279137135 Accuracy 0.884765625\n",
      "Iteration 47290 Training loss 0.0033390820026397705 Validation loss 0.010569443926215172 Accuracy 0.88525390625\n",
      "Iteration 47300 Training loss 0.0031094455625861883 Validation loss 0.010713593102991581 Accuracy 0.88427734375\n",
      "Iteration 47310 Training loss 0.0033776997588574886 Validation loss 0.010710449889302254 Accuracy 0.8857421875\n",
      "Iteration 47320 Training loss 0.003083058400079608 Validation loss 0.010452842339873314 Accuracy 0.8876953125\n",
      "Iteration 47330 Training loss 0.002769644372165203 Validation loss 0.01048262882977724 Accuracy 0.8876953125\n",
      "Iteration 47340 Training loss 0.0022486152593046427 Validation loss 0.010519656352698803 Accuracy 0.88671875\n",
      "Iteration 47350 Training loss 0.0017588261980563402 Validation loss 0.010540680028498173 Accuracy 0.88671875\n",
      "Iteration 47360 Training loss 0.0038271681405603886 Validation loss 0.010619438253343105 Accuracy 0.88623046875\n",
      "Iteration 47370 Training loss 0.002448985818773508 Validation loss 0.010497893206775188 Accuracy 0.88720703125\n",
      "Iteration 47380 Training loss 0.0038184309378266335 Validation loss 0.010568750090897083 Accuracy 0.88720703125\n",
      "Iteration 47390 Training loss 0.002456821035593748 Validation loss 0.010746294632554054 Accuracy 0.88427734375\n",
      "Iteration 47400 Training loss 0.0025982793886214495 Validation loss 0.010677553713321686 Accuracy 0.8857421875\n",
      "Iteration 47410 Training loss 0.0026207375340163708 Validation loss 0.010474154725670815 Accuracy 0.88671875\n",
      "Iteration 47420 Training loss 0.0028007959481328726 Validation loss 0.010634618811309338 Accuracy 0.8857421875\n",
      "Iteration 47430 Training loss 0.0025160349905490875 Validation loss 0.010556429624557495 Accuracy 0.88623046875\n",
      "Iteration 47440 Training loss 0.002462879056110978 Validation loss 0.010707961395382881 Accuracy 0.884765625\n",
      "Iteration 47450 Training loss 0.0015315004857257009 Validation loss 0.010594573803246021 Accuracy 0.88525390625\n",
      "Iteration 47460 Training loss 0.00270366040058434 Validation loss 0.010603837668895721 Accuracy 0.88525390625\n",
      "Iteration 47470 Training loss 0.0030940035358071327 Validation loss 0.010722550563514233 Accuracy 0.88427734375\n",
      "Iteration 47480 Training loss 0.002140115248039365 Validation loss 0.010616361163556576 Accuracy 0.88720703125\n",
      "Iteration 47490 Training loss 0.003799125086516142 Validation loss 0.010561731643974781 Accuracy 0.88671875\n",
      "Iteration 47500 Training loss 0.0028730311896651983 Validation loss 0.010595292784273624 Accuracy 0.88623046875\n",
      "Iteration 47510 Training loss 0.002498872112482786 Validation loss 0.010573217645287514 Accuracy 0.88525390625\n",
      "Iteration 47520 Training loss 0.0020866079721599817 Validation loss 0.010517743416130543 Accuracy 0.88623046875\n",
      "Iteration 47530 Training loss 0.0020903090480715036 Validation loss 0.01045914739370346 Accuracy 0.88671875\n",
      "Iteration 47540 Training loss 0.0017710143001750112 Validation loss 0.01078063901513815 Accuracy 0.8837890625\n",
      "Iteration 47550 Training loss 0.0036219602916389704 Validation loss 0.010706649161875248 Accuracy 0.88427734375\n",
      "Iteration 47560 Training loss 0.0032976798247545958 Validation loss 0.010572842322289944 Accuracy 0.8857421875\n",
      "Iteration 47570 Training loss 0.003107996191829443 Validation loss 0.010650807060301304 Accuracy 0.884765625\n",
      "Iteration 47580 Training loss 0.0014343161601573229 Validation loss 0.010645254515111446 Accuracy 0.88671875\n",
      "Iteration 47590 Training loss 0.0031416791025549173 Validation loss 0.010672138072550297 Accuracy 0.8857421875\n",
      "Iteration 47600 Training loss 0.002444647252559662 Validation loss 0.01054073590785265 Accuracy 0.8876953125\n",
      "Iteration 47610 Training loss 0.0022924768272787333 Validation loss 0.010507592931389809 Accuracy 0.88818359375\n",
      "Iteration 47620 Training loss 0.003298684488981962 Validation loss 0.010686286725103855 Accuracy 0.88427734375\n",
      "Iteration 47630 Training loss 0.0024112656246870756 Validation loss 0.010650224052369595 Accuracy 0.884765625\n",
      "Iteration 47640 Training loss 0.0028595509938895702 Validation loss 0.010594514198601246 Accuracy 0.88671875\n",
      "Iteration 47650 Training loss 0.001999139552935958 Validation loss 0.010557188652455807 Accuracy 0.88720703125\n",
      "Iteration 47660 Training loss 0.003270356683060527 Validation loss 0.01060470100492239 Accuracy 0.88623046875\n",
      "Iteration 47670 Training loss 0.0035857383627444506 Validation loss 0.010805838741362095 Accuracy 0.8837890625\n",
      "Iteration 47680 Training loss 0.0020721061155200005 Validation loss 0.010614927858114243 Accuracy 0.88623046875\n",
      "Iteration 47690 Training loss 0.0036363492254167795 Validation loss 0.010633856989443302 Accuracy 0.88623046875\n",
      "Iteration 47700 Training loss 0.0035347016528248787 Validation loss 0.010666518472135067 Accuracy 0.884765625\n",
      "Iteration 47710 Training loss 0.002303775865584612 Validation loss 0.01067446917295456 Accuracy 0.884765625\n",
      "Iteration 47720 Training loss 0.0035427003167569637 Validation loss 0.010573158040642738 Accuracy 0.88623046875\n",
      "Iteration 47730 Training loss 0.003908445592969656 Validation loss 0.010683475993573666 Accuracy 0.8857421875\n",
      "Iteration 47740 Training loss 0.00277478015050292 Validation loss 0.010782185010612011 Accuracy 0.8828125\n",
      "Iteration 47750 Training loss 0.004121854901313782 Validation loss 0.010564633645117283 Accuracy 0.88623046875\n",
      "Iteration 47760 Training loss 0.0026317767333239317 Validation loss 0.010544904507696629 Accuracy 0.88720703125\n",
      "Iteration 47770 Training loss 0.002240363974124193 Validation loss 0.010538166388869286 Accuracy 0.88623046875\n",
      "Iteration 47780 Training loss 0.0032219234853982925 Validation loss 0.010631591081619263 Accuracy 0.88427734375\n",
      "Iteration 47790 Training loss 0.0037711579352617264 Validation loss 0.01063007302582264 Accuracy 0.88525390625\n",
      "Iteration 47800 Training loss 0.003347602905705571 Validation loss 0.010598713532090187 Accuracy 0.88623046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1076cb410>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47810 Training loss 0.002809042576700449 Validation loss 0.010421793907880783 Accuracy 0.88818359375\n",
      "Iteration 47820 Training loss 0.00301810703240335 Validation loss 0.010476169176399708 Accuracy 0.8876953125\n",
      "Iteration 47830 Training loss 0.002794896485283971 Validation loss 0.010643944144248962 Accuracy 0.8857421875\n",
      "Iteration 47840 Training loss 0.0017396865878254175 Validation loss 0.010600919835269451 Accuracy 0.8857421875\n",
      "Iteration 47850 Training loss 0.0034045397769659758 Validation loss 0.01056494377553463 Accuracy 0.88623046875\n",
      "Iteration 47860 Training loss 0.002637468511238694 Validation loss 0.010590159334242344 Accuracy 0.8857421875\n",
      "Iteration 47870 Training loss 0.002878663595765829 Validation loss 0.010597501881420612 Accuracy 0.88525390625\n",
      "Iteration 47880 Training loss 0.0023843084927648306 Validation loss 0.01059157308191061 Accuracy 0.88623046875\n",
      "Iteration 47890 Training loss 0.0028588958084583282 Validation loss 0.010633093304932117 Accuracy 0.88525390625\n",
      "Iteration 47900 Training loss 0.0021616281010210514 Validation loss 0.010568547993898392 Accuracy 0.88671875\n",
      "Iteration 47910 Training loss 0.002831535879522562 Validation loss 0.010472903959453106 Accuracy 0.88671875\n",
      "Iteration 47920 Training loss 0.0021815267391502857 Validation loss 0.010675854980945587 Accuracy 0.8857421875\n",
      "Iteration 47930 Training loss 0.0013649514876306057 Validation loss 0.010603578761219978 Accuracy 0.88671875\n",
      "Iteration 47940 Training loss 0.002471331274136901 Validation loss 0.010615314356982708 Accuracy 0.88623046875\n",
      "Iteration 47950 Training loss 0.0028619030490517616 Validation loss 0.01047096773982048 Accuracy 0.88818359375\n",
      "Iteration 47960 Training loss 0.0030491482466459274 Validation loss 0.01056408416479826 Accuracy 0.88671875\n",
      "Iteration 47970 Training loss 0.004335252568125725 Validation loss 0.010694711469113827 Accuracy 0.88623046875\n",
      "Iteration 47980 Training loss 0.0032203339505940676 Validation loss 0.010610492900013924 Accuracy 0.88623046875\n",
      "Iteration 47990 Training loss 0.002455121371895075 Validation loss 0.010585837066173553 Accuracy 0.8876953125\n",
      "Iteration 48000 Training loss 0.00315055251121521 Validation loss 0.010545789264142513 Accuracy 0.88720703125\n",
      "Iteration 48010 Training loss 0.002479559276252985 Validation loss 0.010605673305690289 Accuracy 0.8857421875\n",
      "Iteration 48020 Training loss 0.0035923710092902184 Validation loss 0.010660293512046337 Accuracy 0.88525390625\n",
      "Iteration 48030 Training loss 0.0028647335711866617 Validation loss 0.010672379285097122 Accuracy 0.8857421875\n",
      "Iteration 48040 Training loss 0.0032703338656574488 Validation loss 0.010683786123991013 Accuracy 0.88525390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1076cb410>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48050 Training loss 0.002422309946268797 Validation loss 0.010584050789475441 Accuracy 0.88671875\n",
      "Iteration 48060 Training loss 0.0025670321192592382 Validation loss 0.010744591243565083 Accuracy 0.884765625\n",
      "Iteration 48070 Training loss 0.0024934944231063128 Validation loss 0.010549862869083881 Accuracy 0.88818359375\n",
      "Iteration 48080 Training loss 0.004410143010318279 Validation loss 0.010523930191993713 Accuracy 0.88671875\n",
      "Iteration 48090 Training loss 0.0028228263836354017 Validation loss 0.010511358268558979 Accuracy 0.88720703125\n",
      "Iteration 48100 Training loss 0.0037939916364848614 Validation loss 0.010562456212937832 Accuracy 0.88623046875\n",
      "Iteration 48110 Training loss 0.0027133130934089422 Validation loss 0.01059702504426241 Accuracy 0.88623046875\n",
      "Iteration 48120 Training loss 0.002325055655092001 Validation loss 0.010800277814269066 Accuracy 0.88330078125\n",
      "Iteration 48130 Training loss 0.002635604003444314 Validation loss 0.010525689460337162 Accuracy 0.88623046875\n",
      "Iteration 48140 Training loss 0.0025779304560273886 Validation loss 0.010506141930818558 Accuracy 0.88720703125\n",
      "Iteration 48150 Training loss 0.001963868038728833 Validation loss 0.010509156621992588 Accuracy 0.88671875\n",
      "Iteration 48160 Training loss 0.0024368518497794867 Validation loss 0.0105843311175704 Accuracy 0.88623046875\n",
      "Iteration 48170 Training loss 0.003071711864322424 Validation loss 0.010729309171438217 Accuracy 0.88525390625\n",
      "Iteration 48180 Training loss 0.004528329707682133 Validation loss 0.010874398984014988 Accuracy 0.8837890625\n",
      "Iteration 48190 Training loss 0.001718376181088388 Validation loss 0.010500144213438034 Accuracy 0.88818359375\n",
      "Iteration 48200 Training loss 0.0025741676799952984 Validation loss 0.010527810081839561 Accuracy 0.88671875\n",
      "Iteration 48210 Training loss 0.0018373536877334118 Validation loss 0.010510922409594059 Accuracy 0.88818359375\n",
      "Iteration 48220 Training loss 0.002974704373627901 Validation loss 0.01090067345649004 Accuracy 0.88330078125\n",
      "Iteration 48230 Training loss 0.002166483551263809 Validation loss 0.010674023069441319 Accuracy 0.88720703125\n",
      "Iteration 48240 Training loss 0.003933171276003122 Validation loss 0.010626537725329399 Accuracy 0.8857421875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_3_trained_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 258\u001b[39m, in \u001b[36mthree_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, reg1, reg2, reg3, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2, train_layer_3)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\u001b[39;00m\n\u001b[32m    255\u001b[39m \n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Calcul des gradients\u001b[39;00m\n\u001b[32m    257\u001b[39m grad_output = (output - y_minibatch).to(dtype)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m grad_z3 = (torch.einsum(\u001b[33m'\u001b[39m\u001b[33mno,noz->nz\u001b[39m\u001b[33m'\u001b[39m,grad_output,\u001b[43msoftmax_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m)).to(dtype) \u001b[38;5;66;03m# shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\u001b[39;00m\n\u001b[32m    259\u001b[39m grad_h2 = (torch.mm(grad_z3, \u001b[38;5;28mself\u001b[39m.W3)).to(dtype) \u001b[38;5;66;03m# shape (n_data, hidden_2_size)\u001b[39;00m\n\u001b[32m    260\u001b[39m grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) \u001b[38;5;66;03m# shape(n_data, hidden_2_size)         \u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36msoftmax_derivative\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     22\u001b[39m jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) \u001b[38;5;66;03m# Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):  \u001b[38;5;66;03m# Pour chaque échantillon du batch, on calcule la jacobienne de softmax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     si = \u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\u001b[39;00m\n\u001b[32m     25\u001b[39m     jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) \u001b[38;5;66;03m# calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jacobians\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 2048, 2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_3_layer_1_untrained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_3_layer_1_untrained\u001b[49m.train_layers(x_train, y_train, x_valid, y_valid, \u001b[32m2\u001b[39m, \u001b[32m1e-3\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m10\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_3_layer_1_untrained' is not defined"
     ]
    }
   ],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-4, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = True\n",
    "date = \"30_04_25_\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = model_2_layer\n",
    "    model_name = \"model_(784+2048+10)\"\n",
    "    save_path = \"Classifiers/\" + date + model_name + \"/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"datas\\models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "tensor(0.8692)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "model_name = \"model_2_layer_trained_save_1\"\n",
    "assessed_model = torch.load(\"datas/models/model_2_layer_save_1.pt\", weights_only=False)\n",
    "\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plots of performances\n",
    "accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(accuracy)\n",
    "kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_accuracy)\n",
    "loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(loss)\n",
    "kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_loss)\n",
    "plt.show()\n",
    "\n",
    "# Import datas\n",
    "accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).to(dtype)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "\n",
    "\n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, esp_init = 1, fraction_batch=0.01):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = esp_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = esp_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # # hidden neurons layer 2\n",
    "        z3 = torch.mm(self.W3, h2.t() + self.b3)\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, coef_iter = 1, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=0.2, fraction_batch=1e-2, train_layer1=True, train_layer2=True, train_layer3=True):\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        kappa_max = 1 + coef_iter\n",
    "        max_iter = self.input_dimension**(kappa_max)\n",
    "        print(\"max_iter\", max_iter)\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_minibatches = int(max_iter / self.fraction_batch) # Nombre de minibatches utilisés pour l'apprentissage de la première couche\n",
    "        for i in range(N_minibatches):\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            # Suivi de l'apprentissage\n",
    "            if i % 100 == 0:\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2, dim=0) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                overall_training_loss = torch.mean(training_loss,dim=0)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2, dim=0)\n",
    "                overall_validation_loss = torch.mean(validation_loss,dim=0)\n",
    "                self.training_loss_trajectory.append(overall_training_loss.item())\n",
    "                self.validation_loss_trajectory.append(overall_validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", overall_training_loss.item(), \"Validation loss\", overall_validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0] # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = torch.mean(grad_z1, dim=0).unsqueeze(1) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0] # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = torch.mean(grad_z2, dim=0).unsqueeze(1)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            self.W1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/self.eps_init**2 + self.reg1*self.W1) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "            self.b1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/self.eps_init**2 + self.reg1*self.b1)\n",
    "            self.W2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/self.eps_init**2 +self.reg2*self.W2)\n",
    "            self.b2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/self.eps_init**2 + self.reg2*self.b2)\n",
    "        return \"Training done\"\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
