{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from utils import mnist_reader\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 18\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033bbdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float16\n",
    "\n",
    "# Importation des données\n",
    "x_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "x_valid, y_valid = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "x_train, y_train_raw, x_valid, y_valid_raw = torch.tensor(x_train, dtype=dtype), torch.tensor(y_train, dtype=dtype), torch.tensor(x_valid, dtype=dtype), torch.tensor(y_valid,dtype=dtype)\n",
    "\n",
    "# Modification du format des données shape (n_data,1) -> (n_data, n_classes)\n",
    "y_train = torch.zeros(y_train_raw.shape[0], 10)\n",
    "for i,y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros(y_valid_raw.shape[0], 10)\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "print(y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 10]) torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAGpCAYAAADY7qJlAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIfFJREFUeJzt3Q1wFPX9x/FvnkkCJAaCIASEiCgYAiMKhaEU8YEqM3RkClqlhfIgOtYqA+0wLaJ2AKeVitVacaAawJGHURAKWnVkBkqRFgEJgowiSmokhEgeCHnO/ef3m/9lCLkk7Ca3+d7d+zWz7OV2f9wvl00+t7u//W6Uz+fzCQAAykR3dAcAAAiEgAIAqERAAQBUIqAAACoRUAAAlQgoAIBKBBQAQCUCCgCgEgEFAFCJgAIAqBTr1QtVV1fLn//8Z3nttdektrZW+vTpI3/4wx/khz/8oaP/p76+XvLz86VLly4SFRUVtP4CANqfqa5XVlYm11xzjURHt7KP5PNAZWWlb/z48b7Bgwf7vvnmG/vcpk2bfHFxcXbuRF5enqkdyMTExMQkoTuZv+WtiTL/SJA9/vjj8sILL8j+/fvl1ltvbXj+Zz/7mWzbtk1yc3Olf//+V/R/lZSUSGpqahB7i1Dz3HPPOW6zatUqV6/1xRdfSDjJyspy1e4Xv/iF4zYLFixw9VoIT8XFxZKSktKxh/i+/vpr+etf/yqDBw9uFE7G9OnT5c0335RFixbJhg0bruj/47Ce99y+514Vyk9MTHTcJiYmJih9CTVu3wc37zng9O9K0AdJbNy40Z5zGj16dJNlI0eOtPMtW7ZIUVFRsLsCAAghQQ+oHTt22PmAAQOaLEtLS5PevXvbARR79+4NdlcAACEk6AF16NAhOzej9gLxn086fPhwwOVVVVVSWlraaAIAhL+gBlRlZaVcuHDBPm5uYIP/JNm5c+cCLl++fLldxz9lZGQEsccAgIgIqEvPKyUlJQXuwP+PgzdhFogZQGFG7vmnvLy8IPUWAKBJUEfxxcfHtzqiy5x/8p+PCiQhIcFOAIDIEtQ9KBM6/pAqLy9vdiy80b1792B2BQAQYqKDfY2Fuf7JMOWJAikoKLDz7OzsYHYFABBigj6K76677rLzzz77rMkyMzDCnFdKTk6WcePGBbsrAIAQEvRKErNmzZI//elPsnv37ibL9u3bZ+dTpkxpdL4KkavV4pEBNHf4uCV//OMfxQ3/qFQnTp486bhNXFyc4zaZmZmeVfs4ceKEq3aAqj2ogQMHyty5c229vcuvdcrJybElU5YsWRLsbgAAQky0V8U8b775Zpk3b558//339lPbX/7yF9m+fbusXbs2YJUJAEBk8+R+UOYc065du2Tx4sUyYsQIexjnpptukv/+978ydOhQL7oAAAgxnt2w0NxgcOXKlXYCAKA13PIdAKASAQUAUImAAgCoREABAFQioAAAKhFQAACVCCgAgEoEFAAgsi/UBYJVvLSsrMxxmy+++ELc6NSpk+M2zd2MsyWxsc5/NWtrax23+fbbb8XtrXS0ioqK8qxoLoKLPSgAgEoEFABAJQIKAKASAQUAUImAAgCoREABAFQioAAAKhFQAACVCCgAgEoEFABAJQIKAKASAQUAUImAAgCoRDVzBK3Ss1dVpVNSUhy3KS4uFjfcVE6vr6933KampkZtpXWjT58+rtoBTrAHBQBQiYACAKhEQAEAVCKgAAAqEVAAAJUIKACASgQUAEAlAgoAoBIBBQBQiYACAKhEQAEAVCKgAAAqUSwWQSn62pYis06dPXvWk6KvxoULFxy3SUxMdNwmJiZGbSFbo7q6WrTyartD8LEHBQBQiYACAKhEQAEAVCKgAAAqEVAAAJUIKACASgQUAEAlAgoAoBIBBQBQiYACAKhEQAEAVCKgAAAqeVosdsuWLXLvvfc2ef6nP/2pbNq0ycuuIIyKxV68eNFxm86dO3tWJNVNQVY3711NTY3jNgkJCY7buH0tr0RHR3tWNBdhtAe1fPnygM8vWLDAy24AAEKAZ3tQH374of20dvz48Safdq6//nqvugEACBGxXu49/e53v5MbbrjBq5cEAIQwTw7x7d+/X/7973/LN998I59//rkXLwkACHHRXu09VVZWyrx58+TGG2+UW265Rf75z3968dIAgBAV9IAqKiqSc+fOyaBBgxpuY33gwAGZOHGiPPHEE62OVqqqqpLS0tJGEwAg/AU9oLp16yb/+te/7KE9E1Z///vfpVevXnbZypUrZcmSJa3ufaWkpDRMGRkZwe4yACDShpmbgJk5c6YNqzFjxtjnnn32WTl16lSzbRYtWiQlJSUNU15enoc9BgBEVCWJrl27ys6dO6Vfv372gr+33nqr2XXN0HSz/qUTACD8dVipIxM0v//97+3jkydPdlQ3AABKdWgtvttvv71NZWcAAOGrQwPKP1hi1KhRHdkNAECkF4u93NGjR6V///4yadKkjuwGQryQprkUwanU1FTVhUhjY2M96Vt8fLy4oflyD6+KFCMM9qDML+f58+cDLjMj+F577TXXFZUBAOEr6AH1k5/8RNLT0+Xxxx+X77//3j5XWFgo8+fPl9mzZ8u4ceOC3QUAQAgK+iE+E0RnzpyR1atXS05OjowdO9ZeA2VG8KWlpQX75QEAISroAfWjH/1I/vOf/wT7ZQAAYYZbvgMAVCKgAAAqEVAAAJUIKACASgQUAEAlAgoAoBIBBQBQiYACAKjUocVigfZQXV3tuI25UaYbUVFRrtp58ToxMTGeFVYtLi4WrSgWGz7YgwIAqERAAQBUIqAAACoRUAAAlQgoAIBKBBQAQCUCCgCgEgEFAFCJgAIAqERAAQBUIqAAACoRUAAAlQgoAIBKVDNHyIuPj3fcpnPnzq5eq6SkxJPK6ZWVlY7bxMbGevLeGVQMhxfYgwIAqERAAQBUIqAAACoRUAAAlQgoAIBKBBQAQCUCCgCgEgEFAFCJgAIAqERAAQBUIqAAACoRUAAAlSgWi5B34403Om5TXl7u6rXq6uoct0lPT3fcpr6+3nGbgoICx22ioqLEjYEDB7pqBzjBHhQAQCUCCgCgEgEFAFCJgAIAqERAAQBUIqAAACoRUAAAlQgoAIBKBBQAQCUCCgCgEgEFAFCJgAIAhGex2B07dsjSpUtl7ty5MmPGjGbXO3jwoCxevFiOHTsmMTExMmXKFHnqqackMTGxrV1AkLktKOrz+cQL0dHOP2dduHDB1WuZbdepa665xpPX+fbbbx23uXjxoriRmprqqh3gyR7Upk2bZOTIkTJp0iTZt29fi+tu375dRo8eLRMmTJBTp07JJ598Inv37rVfu60qDQAIb64DasSIEbJ79+5Wy+7n5eXJAw88ILfddpvMnz/fPpeSkiJr1qyRjz/+WBYuXOi2CwCAMOY6oAYMGCAJCQkyfPjwFtd7+umnpaysTGbOnNno+UGDBsktt9wir7zyihw/ftxtNwAAYarNgyQ6derU7LKamhrZvHmzfWwO8V1u1KhR9jzF6tWr29oNAECYiQ7mCfQ9e/ZIaWmp3dPq3bt3k+VZWVl2vmvXrrZ2AwAQZoJ6y/dDhw7ZeaBwunQkUG5urr2VdqCRS1VVVXbyM4EHAAh/Qb0OqrCwsMUhqWawhFFbWyslJSUB11m+fLldzz9lZGQEsccAgIgIqKKiIjtPSkpq9fqVysrKgOssWrTIhpd/MqMCAQDhL6iH+OLj41u8YLO6urrhcVpaWsB1zPkrMwEAIktQ96B69uxp581djFtcXGznycnJLY4GBABEnqAG1NChQ+08Pz8/4PKCggI7z87ODmY3AAAhKKgBNX78eHuY7+zZs3Lu3Lkmy7/88ks7v/vuu4PZDQBACArqOaiuXbvKtGnTZN26dbYs0r333ttouanhZwZKTJ06NZjdQAgVi73qqqsct6moqHDcJtAHpivRo0cPx23279/vuI057O1Ut27dHLdpbvRsa/wjcINdALe+vt6T4sHmMheE4R6UGSLe0g94yZIl9pdt7dq1jZ4/evSorXA+e/bsVuv5AQAiT5sCynxyPXLkiH1sCr8GkpmZKatWrbK35Vi/fr197vTp0/Lggw/KmDFj5Pnnn29LFwAAYcp1QN13333SvXt3WwXCMPX0zCEGU/z1cqaa+c6dO+Xll1+2RWbNOafp06fLRx991Ow1UgCAyOb6HNSGDRscrX/HHXfYCQCAK8Et3wEAKhFQAACVCCgAgEoEFABAJQIKAKASAQUAUImAAgCoREABAFQioAAAkVfNHOHBTVVyt7Kyshy3cXOzy5qaGnEjPT3dcZtvv/3WcZsuXbo4btPcXalb4r/ljVNxcXGO2wwZMsRxG3+tT0Qm9qAAACoRUAAAlQgoAIBKBBQAQCUCCgCgEgEFAFCJgAIAqERAAQBUIqAAACoRUAAAlQgoAIBKBBQAQCWKxUJVsdjrr7/ecZvq6mrxipsirtHRzj8HxsfHO26TmprquE1dXZ24UVFR4bhNZmamJ8Vi3X5P0Ic9KACASgQUAEAlAgoAoBIBBQBQiYACAKhEQAEAVCKgAAAqEVAAAJUIKACASgQUAEAlAgoAoBIBBQBQiWKxCJqEhATHbfr06eNJYVU3BVyNTp06OW4TG+v81ywxMdGTYrFufkZuC/Smp6e7ei1ELvagAAAqEVAAAJUIKACASgQUAEAlAgoAoBIBBQBQiYACAKhEQAEAVCKgAAAqEVAAAJUIKACASgQUACA8i8Xu2LFDli5dKnPnzpUZM2a0uO6wYcPk008/bfRcVFSU5ObmypAhQ9raFSjTq1cvT4qXxsTEiGZxcXGO29TV1Tlu07lzZ0/6ZtTX1ztuk5SU5Oq1ELlc70Ft2rRJRo4cKZMmTZJ9+/a1uv7OnTubhJMxceJEwgkA0H57UCNGjJDdu3dLVlaWfPHFF62uv3z5ctmwYYNkZ2c3er5Hjx5uuwAACGOuA2rAgAF2Pnz48FYDas+ePVJTUyPTpk1z+3IAgAjT5kESV3IDt2XLltk9JXO+qqKioq0vCQCIAG0OKDPIoSWHDx+W9957T7Zv327PV1199dXyxBNPyPnz59v60gCAMBb0YeYfffSRPQzYvXt3+3VZWZmsXLnSnos6cuRIq+2rqqqktLS00QQACH9BD6j58+fLwYMH5ezZs3Zvyn8eKi8vT+68807Jz89vdXBFSkpKw5SRkRHsLgMAIulCXXMo0Ow1mZF8GzdutNeuFBQUyJNPPtliu0WLFklJSUnDZIINABD+OqSSxNSpU2XFihX28ebNm1u86M9cuNm1a9dGEwAg/HVYqaNHHnlErr32WntOqbCwsKO6AQBQqsMCypRYGTdunOsSLQCA8Bbd0bXabrrpJklOTu7IbgAAwrFYbFscPXpUHnvssY7sAoLIXPPmReHX2tradr9+rz3Fx8c7bmMqrzjl5Qc9N+9fWlpaUAoBXK6ystJxG4TpHpT/j0Nz1ZeLi4sDLjtw4ID4fD6ZNWtWW7sAAAhDbQooU7bIf7Htxx9/HDCEunXrJoMHD5b333/fPmdCyZQ8ysnJscPNo6O5JRUAoCnX6XDffffZ6hDmXk7G6tWrbRi98sorDesMHTpUHn74YTtSz5Q5Ml+b0Xtm6PiLL77IuScAQPufgzIX3F7JsfeXXnrJTgAAOMHxNQCASgQUAEAlAgoAoBIBBQBQiYACAKhEQAEAVCKgAAAqEVAAAJUIKACASh1azRzhzdxOxYvK5G7qObqtAdnS3Z+bk5SU5MnrmBJiXlRad1vN3M1r9ezZ03Gbr7/+2nEb6MQeFABAJQIKAKASAQUAUImAAgCoREABAFQioAAAKhFQAACVCCgAgEoEFABAJQIKAKASAQUAUImAAgCoRLFYBE16errjNtXV1Y7bxMTEeFYs1ufzOW7TqVMnx23Ky8sdt0lJSXHcpq6uTrwqFuvmPadYbGRjDwoAoBIBBQBQiYACAKhEQAEAVCKgAAAqEVAAAJUIKACASgQUAEAlAgoAoBIBBQBQiYACAKhEQAEAVKJYLIImNTXVcZuamhrHbRITEx23iY2N9axYbHx8vOM2ZWVljtt06dLFcZv6+nrx6n2oqKhw3Oa6665z3Objjz923AY6sQcFAFCJgAIAqERAAQBUIqAAACoRUAAAlQgoAIBKBBQAQCUCCgCgEgEFAFCJgAIAqERAAQBUIqAAACrFui0U+eqrr8rLL78sJ06ckKSkJBk7dqwsXrxYRowYEbDNwYMH7fJjx45JTEyMTJkyRZ566ilXhT4RGtLT0x23+e677xy3ueqqqxy3iY5299mstrbWcZtOnTp50j83r+OWm2KxbtpkZGQ4boPw4eq39KGHHpJ58+bJkSNH7C/s+fPnZdu2bTJ69Gh5++23m6y/fft2u2zChAly6tQp+eSTT2Tv3r326/Ly8vb4PgAAkR5Q7777rmzZskVycnKktLRUKisrZevWrfbTsrlVwsyZM+XcuXMN6+fl5ckDDzwgt912m8yfP98+l5KSImvWrLFl8RcuXNi+3xEAIDID6vXXX5cPPvhAfv7zn9v7z5j76kyePFnefPNNu9yEltmb8nv66aftvW1McF1q0KBBcsstt8grr7wix48fb4/vBQAQyQFlzjUNGzasyfPmcN3w4cPt48LCQjs3e1SbN2+2j80hvsuNGjXKHpdevXq1m74DAMKY44B69NFHm102cOBAO+/Xr5+d79mzx+5RJSQkSO/evZusn5WVZee7du1y2g0AQJhr11u+m3NPJowmTpxovz506JCdBwqnS28JnpubK3V1dXZ03+Wqqqrs5GcCDwAQ/trtOqiLFy/Kvn37ZPbs2Q3B4z/U5//6cmawhGFGApaUlARcZ/ny5XY9/8SwUwCIDO0WUOY8khk08cwzzzQ8V1RUZOfmOqnWrvUwowEDWbRokQ0v/2RGBQIAwl+7HOIzQbR06VI79DwtLa3h+fj4+BYv0Kuurm54fGm7S5lDhmYCAESWdtmDmjNnjr2eyX/uya9nz5523tzFuMXFxXaenJzs6VXwAIAICKhly5ZJ3759ZcGCBU2WDR061M7z8/MDti0oKLDz7OzstnYDABBm2hRQ69ats7X4nn/++YDLx48fbw/znT17tlF1Cb8vv/zSzu++++62dAMAEIZcn4MyNffeeecd2bBhg0RFRTVaZoaMm70mM+Ju2rRpNsh2794t9957b6P1zKg/M1Bi6tSp7r8DBJ2pFuKGm0LAbgqK+s91OhHokoYrUV9f77iNm3Oobt4Hc6jcK3FxcY7bmL8LTvXo0cNxG0T4HpSpvWcGRLzxxhtN/nidOXNGZsyYIV999ZX9esmSJfYXZ+3atY3WO3r0qK1wboal+y/wBQDAz/FHYxNKJoA6d+7c5AJcMyrP1N0ze07+QMrMzJRVq1bZNuvXr5cHH3xQTp8+bedjxoxp9vAgACCyOQqoHTt2yPTp0+3hB/8IvEDuv//+Rof9TDVzs6tu9qaefPJJe12UKR77q1/9ytXhGQBA+HMUUPfcc4+rY/DGHXfcYScAAK4Et3wHAKhEQAEAVCKgAAAqEVAAAJUIKACASgQUAEAlAgoAoBIBBQBQiYACAITvHXUR3nr16uVZxWs3Vbwvr6Z/JUwVfTfc9M/N++CGl2XDamtrHbepqalx3MaURfOiin5FRYXjNgg+9qAAACoRUAAAlQgoAIBKBBQAQCUCCgCgEgEFAFCJgAIAqERAAQBUIqAAACoRUAAAlQgoAIBKBBQAQCWKxaJVffv2ddWuvr7ekyKuXhZJ9ap/bovZevU6bn62bormxsTEOG4zYMAAx20+++wzx20QfOxBAQBUIqAAACoRUAAAlQgoAIBKBBQAQCUCCgCgEgEFAFCJgAIAqERAAQBUIqAAACoRUAAAlQgoAIBKFItFq1JTU121q6ur86Swqps2UVFR4hU3/YuNjfWksKqbNm7fv8TERMdtKioqHLfp3bu34zYUi9WJPSgAgEoEFABAJQIKAKASAQUAUImAAgCoREABAFQioAAAKhFQAACVCCgAgEoEFABAJQIKAKASAQUACJ9isT6fT1599VV5+eWX5cSJE5KUlCRjx46VxYsXy4gRI5ptN2zYMPn000+bFJ3Mzc2VIUOGuOkKPHDTTTeJ2+3EizbV1dWeFGM1qqqqHLe56qqrPPme8vLyHLdJSEgQNyorKz15LTfbw6233uq4zfvvv++4DZTuQT300EMyb948OXLkiNTW1sr58+dl27ZtMnr0aHn77bcDttm5c2eTcDImTpxIOAEA2h5Q7777rmzZskVycnKktLTUfpLaunWrpKenS01NjcycOVPOnTvXpN3y5ctlw4YNcvz48UbT+vXrnXYBABABHB/neP311+WDDz6wh+v8Jk+eLJ07d5bbb7/dhpbZm/rlL3/ZsHzPnj02vKZNm9Z+PQcAhDXHe1DmXNOl4eQ3YcIEGT58uH1cWFjYaNmyZcukR48esmPHDlc3IAMARB7HAfXoo482u2zgwIF23q9fv4bnDh8+LO+9955s375dJk2aJFdffbU88cQT9rwVAACeDDM3557MSB0z8MHvo48+sntW3bt3t1+XlZXJypUrJTs72w6yuJJRU+aw4aUTACD8tVtAXbx4Ufbt2yezZ8+W1NTUhufnz58vBw8elLNnz9q9Kf95KDMk9s4775T8/PwW/18zuCIlJaVhysjIaK8uAwAiIaBWr14tXbp0kWeeeSbgcnO9k9lrMiP5Nm7cKDExMVJQUCBPPvlki//vokWLpKSkpGFyc60HACBCA6qoqEiWLl1qh56npaW1uv7UqVNlxYoV9vHmzZulvr6+2XXNIcOuXbs2mgAA4a9dAmrOnDmycOHCRueeWvPII4/Itddea88pXT7qDwCANgeUGULet29fWbBggaN2cXFxMm7cOPvYXEMFAMCl3BUk+3/r1q2ztfjMxbtu9OrVy9Z5S05Obks3AABhyPUelKm5984778iaNWvsAIhL1dXVXdFghqNHj8pjjz3mtgsAgDDmag/K1N4zAyI2bdrUpCr0mTNn7PkoM9zcDAkvLi62o/vMqL1LHThwwFYqnjVrVtu+AwTdsWPHXLW78cYbHbfxX+ztRJ8+fRy3MdulG/7r+YLdP/N75FRmZqbjNm4v2ygvL/ekf26uezx58qTjNgiTgHrjjTdkxowZ9rxR7969m9wiwFyIazb6tWvX2hAaOXKkXHfddfLiiy/a655MKJnK5qa6hBluHh3NLakAAG0MKFNLb/r06TZkWvoEev/999vDfkOHDpWHH35Y3nrrLVvm6IYbbpAxY8bIlClTbGABANAuAXXPPfe0eM3S5eLj4+Wll16yEwAATnB8DQCgEgEFAFCJgAIAqERAAQBUIqAAACoRUAAAlQgoAIBKBBQAQCUCCgCgUpTP1C0KIaZ4ZEpKSkd3A1cgMTHRkwKzV199teM2N998s7hx+vRpx20uL5QcrGKxubm5jts88MAD4sapU6cctzFFo50ydTud+u677xy3gfdKSkpavUM6e1AAAJUIKACASgQUAEAlAgoAoBIBBQBQiYACAKhEQAEAVCKgAAAqEVAAAJUIKACASgQUAEClWAkxIVY6MKK5+VnV1dU5blNTU+O4TVVVlbhRXV3tSS0+N99TfX29Z++Dm/65ee/cfE8In78PIVcs9n//+59kZGR0dDcAAG2Ql5cnffr0Ca+AMp+o8vPzbWXkqKioRlXOTXCZb7q1CrkIf2wPuBTbgx4mcsrKyuSaa66R6Ojo8DrEZ76hllLXbHxsgPBje8Cl2B50uNJbJjFIAgCgEgEFAFApbAIqISFBlixZYucA2wMuxfYQmkJukAQAIDKEzR4UACC8EFAAAJUIKACASgQUAEAlAgoAoFJYBJQpQvnss8/KoEGDJDMzU8aNGye7d+/u6G7BIzt27JDRo0fL66+/3uJ6Bw8elHvuuUf69+8v1113nfz2t7+ViooKz/qJ4DADkVetWiXZ2dnSqVMnSUtLk8mTJ8uBAweabcO2ECJ8Ia6ystI3fvx43+DBg33ffPONfW7Tpk2+uLg4O0f42rhxo+/WW281l0nY6bXXXmt23W3btvkSEhJ8K1assF8XFxf7xowZ4/vBD37gu3Dhgoe9RnubM2dOwzYQExPT8Nj8DXjrrbearM+2EDpCPqB+/etf241x//79jZ6///77fcnJyb6vvvqqw/qG4Dp58qT9gDJw4MAWA+r06dO+Ll26+H784x83ev7zzz/3RUVF+R5++GGPeoz2tnPnTl/37t19OTk5vtLSUl9NTY1v69atvvT0dLtNdO3a1VdYWNiwPttCaAnpgDp16pQvNjbW7j0F2nDNBjpt2rQO6Ru8M3Xq1BYDatasWXZ5oD1qswdm/jAdO3bMg54iGD/7Q4cONXn+ww8/bNiTWrNmTcPzbAuhJaTPQW3cuFFqa2vt+YfLjRw50s63bNkiRUVFHdA7eMWcd2jpxnqbN2+2jwNtJ6NGjbLnMFavXh3UPiI4xo4dK8OGDWvy/IQJE2T48OH2cWFhoZ2zLYSe6FA/OW4MGDCgyTJzorR37952AMXevXs7oHfwyqX3Bbvcnj177L2ATA02sz1cLisry8537doV1D4iOB599NFmlw0cONDO+/XrZ+dsC6EnpAPq0KFDdt7c/aFSU1Pt/PDhw572C/q2kUB/kC7dRnJzc13dbh56nTt3zobRxIkT7ddsC6EnZAOqsrJSLly40GjDau6mWGZDRWTyH95pbRsxh4pLSko87RuC5+LFi7Jv3z6ZPXt2w8+ebSH0hGxAXXpeKSkpKeA6/tsJmzBDZPJvJ61tIwbbSfgw55G6dOkizzzzTMNzbAuhJ2QDKj4+vuFxc3cMMeef/OejEJn820lr24jBdhIeTBAtXbpUcnJyGv1M2RZCT8gGlNmA/BtceXl5wHWKi4vtvHv37p72DXr07NnziraR5OTkFkcDInTMmTNHFi5c2HDuyY9tIfSEbEDFxMTI4MGD7eP8/PyA6xQUFNi5KYGCyDR06FA7ZxuJDMuWLZO+ffvKggULmixjWwg9IRtQxl133WXnn332WZNlZmCEOdFpPg2Z2nyITOPHj7d72mfPng04WObLL7+087vvvrsDeof2tG7dOjlx4oQ8//zzAZezLYSekA6oWbNm2RObgQrDmhE8xpQpUxqdr0Jk6dq1q0ybNs0+bm47MdvQ1KlTO6B3aC9vv/22vPPOO7JmzZom18WZIeN5eXlsCyEopAPKXIg3d+5ce93C5dc6mROkiYmJsmTJkg7rH7xhhgUbzV27YrYBsye9du3aRs8fPXrUVrU2Q5H9F3Ui9GzdutX+vr/xxhsSGxvbaNmZM2dkxowZ8tVXX9mv2RZCjC/EmerDN998s2/kyJG+oqIiX319ve+FF17wxcfH+zZv3tzR3UOQXbx40ZeVlWXrq82ePbvZ9davX2/rNq5bt85+bSrfZ2dn2yrW5eXlHvYY7cn/c01NTfV169at0WSKwprtIiMjw/5duLwN24J+IR9QhqlibKqa9+/f35eZmembPHmy79NPP+3obiHITCHgpKSkhqKgZkpLS/P97W9/C7j++++/b2+pYLaTIUOG+J577jlfVVWV5/1G+/jHP/5hi7te+vMPNP3mN79p0pZtITREmX86ei8OAICwOgcFAAhfBBQAQCUCCgCgEgEFAFCJgAIAqERAAQBUIqAAACoRUAAAlQgoAIBKBBQAQCUCCgCgEgEFAFCJgAIAqERAAQBEo/8DhM1DbIuQgbgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAGpCAYAAADY7qJlAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAI3FJREFUeJzt3QtwVOX5x/End0IgiSFcFCICRiwaAhUNQilFasXLDB0YiY7aYrkUq/XCoB3aarwM4Eyr4tix0oFKBEcuVfCSWC+VGSiiFQkCilYNaloKJIHcyQVy/vO+899MQjYJ50327Hs238/My0l2z5t9s3uyP86e9zwnynEcRwAAsEx0uAcAAEAwBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKsV49UGNjozz55JPy/PPPy6lTp2TYsGHy2GOPyQ9/+ENXP6e5uVkOHz4s/fv3l6ioqJCNFwDQ81R1verqajnvvPMkOrqLfSTHA/X19c60adOcMWPGON9++62+bdOmTU5cXJxeulFSUqJqB9JoNBpN/NvUe3lXPAmoe+65Rw/oww8/bHP7zTff7CQlJTnFxcVn/bMqKirC/sTSaDQaTbrV1Ht52APq0KFDTmxsrN57OlNhYaEeaG5u7ln/vMrKyrA/sTQajUaTbjX1Xt6VkE+S2Lhxoz7mNGnSpHb35eTk6OWWLVukvLw81EMBAPhIyAOqoKBAL0eOHNnuvrS0NBk6dKieQLFz585QDwUA4CMhD6iioiK9VLP2gklNTdXLvXv3Br2/oaFBqqqq2jQAQOQLaUDV19dLTU1NmyA6U0pKil6WlZUFvX/FihV6nUDLyMgI4YgBAL0ioFofV+rbt2/wAfz/PHgVZsEsXbpUKisrW1pJSUmIRgsA6DUn6sbHx7d8rWYMBqOOPwWORwWTkJCgGwCgdwnpHpQKnUBI1dbWBl2noqJCL9PT00M5FACAz4Q0oGJiYmTMmDH6a1WeKJijR4/qZXZ2diiHAgDwmZDP4rvmmmv08tNPP213n5oYoY4rJSUlydSpU0M9FACAj4Q8oObNm6cnQmzfvr3dfbt27dLL2bNntzleBQBAyAMqMzNTFi5cKPv37293rlN+fr4kJiZKXl5eqIcBAPAbxwM1NTXOZZdd5uTk5Djl5eVOc3Oz8/TTTzvx8fHO5s2bXf0savHRaDSa9IpafJ5cD0odY9q2bZs8+OCDMmHCBP2R36WXXiofffSRjB071oshAAB8JkqllPiIKnUUqD4BAPAnNUEuOTm503W45DsAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASrHhHgC8FRUV5bqP4zhGjxUd7f7/P83NzeKFO++806hfcXGx6z5vvvmm6z7f//73XffZs2ePeMXm1xaRgz0oAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlaIc00qgBrZs2SKzZs1qd/uNN94omzZtOqufUVVVJSkpKWIzk0KacXFxnhTfbGpqEq/Ex8e77tPY2Oi6z7hx41z3KSoqEhO7d+/25LEWLFjguo/6O3Lrb3/7m5hISEhw3aehocGT4sZe8vDtM+JUVlZKcnKyPXtQK1asCHr7kiVLvBwGAMAHPLvcxrvvvqv/13Xw4MF2exsXXXSRV8MAAPhErJd7T7/73e/k4osv9uohAQA+5slHfB9++KG8//778u2338rnn3/uxUMCAHwu2qu9p/r6elm0aJF873vfk8svv1zeeustLx4aAOBTIQ+o8vJyKSsrk9GjR0tMTEzLTKgZM2bIfffd1+UsGDXzR83ca90AAJEv5AE1YMAA+ec//6k/2lNh9de//lXOPfdcfd/KlSslLy+vy70vNa080DIyMkI9ZACABTydZq4C5vbbb9dhNXnyZH3b448/LocOHeqwz9KlS/V8+UArKSnxcMQAgF5VSUKdnFVYWCjDhw/XJ46+/PLLHa6rpqar9Vs3AEDkC1upIxU0v//97/XXX3/9dbiGAQCwVFhr8f34xz/Wy379+oVzGAAAC4U1oAKTJSZOnBjOYQAAenMliWAOHDggI0aMkBtuuEEiiUkRV5NCmrYzKfxqYu/eva77qMk5Jk6cOOG6z7///W/XfVJTUz0r/GrCq+3V9mKsXhVE9pJJgd5QvU7RXrxZd/RHrd4knn/+eaPKyACAyBbygPrpT38qAwcOlHvvvVeOHz+ubystLZXFixfL/PnzZerUqaEeAgDAh0L+EZ8KoiNHjsjq1aslPz9fpkyZos+BUjP40tLSQv3wAACfCnlA/ehHP5J//etfoX4YAECE4ZLvAAArEVAAACsRUAAAKxFQAAArEVAAACsRUAAAKxFQAAArEVAAACtFObZXYzxDVVWVvjKv26KGtv+aM2bMcN1n7ty5rvsUFxe77vPb3/7WdZ9IdfHFF7vuc+WVV7ruo2pUAgHR0dERVyBbXSG9qwvQ2v1bAwB6LQIKAGAlAgoAYCUCCgBgJQIKAGAlAgoAYCUCCgBgJQIKAGAlAgoAYCUCCgBgJQIKAGAlAgoAYCUCCgBgJd9WM1eVzN1UMzeptmvqsccec93nnnvucd2nT58+rvvcfffdrvskJiaKiSeffFK80NDQ4LqP6fZg8lx8/fXXrvu42bYDBg0a5LpPTU2NmCgqKnLdZ8uWLZ6Mb/r06Z5UnFf+8Ic/uO6zdu1asVmUwbZnEiNUMwcA+BYBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASxWI7kZGRISYKCwtd97ngggtc92lsbPSk2GlsbKyYaGpqct3H5HXq27ev6z6nTp0SEybPuYl+/fq57lNdXe26T1JSkpiIjo725Lkz6WPyO5luDyaFVT/55BPXfRYtWuS6z+7du8VmFIsFAPgWAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwklkV0FYKCgpk2bJlsnDhQpk7d26H6+3Zs0cefPBB+eyzzyQmJkZmz54tDz/8sFHxUkXVuA11ndt7773XqF9mZqbrPqdPn3bdJz4+3nWfhoYG133q6+vFhFdFXFUBYa8K4JpscydOnPDkcUwcP37cqF9CQoInhVVN1NbWeraNmxSzHT9+vOs+b7/9tus+I0eOFBMVFRXi+z2oTZs2SU5Ojtxwww2ya9euTtd9/fXXZdKkSTJ9+nQ5dOiQfPzxx7Jz5079vcnGBACIfMYBNWHCBNm+fXuXewslJSVyyy23yFVXXSWLFy/Wt6nLZaxZs0Y++OADuf/++02HAACIYMYBpXYf1W5+V7urjzzyiL5Oze23397m9tGjR8vll18uzz33nBw8eNB0GACACNXtSRJ9+vTp9IJ1mzdv1l+rj/jONHHiRP1Z++rVq7s7DABAhOl2QHV24HPHjh36ALba0xo6dGi7+7OysvRy27Zt3R0GACDCdHsWX2eKior0Mlg4KampqXq5f/9+PYtNze4LNuus9cwzkxlbAAD/Cel5UKWlpW2C6ExqskRgarG6Pn0wK1as0OsFWkZGRghHDADoFQFVXl7e6fkw0dHRXZ6HsHTpUh1egaZmBQIAIl9IP+ILnEja0UmHrU9yS0tLC7qOOn5lclIgAMDfQroHNWTIEL3s6GTcwBnLSUlJnc4GBAD0PiENqLFjx+rl4cOHg95/9OhRvczOzg7lMAAAPhTSgJo2bZr+mO/YsWNSVlbW7v6vvvpKL6+77rpQDgMA4EMhPQaVnJwsubm5sm7dOl0WadasWW3uVzX81ESJOXPmSKh1dIyrM7/4xS+MHqu5udmTYrEmBUVNCswGm/4fqsK0JscbTZ4708KlcXFxrvuoE9a9KGbbetJRKIudmm7jJh/je/XcdTTTOBRFZjv6RKkzw4YNc92nsLBQTAQrquDbPahA9emO3iTy8vL0MaYXXnihze0HDhzQFc7nz59vVP0bABDZuhVQJ0+elH379umvVeHXYEaNGiWrVq3Sl+VYv369vu27776TW2+9VSZPnixPPfVUd4YAAIhQxgF10003SXp6uq4Coah6egMGDNDFX8+kqpmr3c1nn31WF5lVx5xuu+02ee+994yuGQQAiHzGx6A2bNjgav2rr75aNwAAzgaXfAcAWImAAgBYiYACAFiJgAIAWImAAgBYiYACAFiJgAIAWImAAgBYiYACAPS+auY2mT59umcVjqurqz2pwGxSkdukArpJtXDTyt8mVbJNK5ObMHkuTCpyJyYmihdMXqPOLkLa01XxvarQHih67cXVAUyurPC///3PdZ+srCwxocrRuVVcXCyhwB4UAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASr2mWOyNN97o2WOZFGQ1KYppUoTU5HG8fB68KvxqMjbT8Zk85ydPnnTdJyUlxZPCqqaFX00Kspo8d15u4ybPn0mB2YSEBNd9+vXrJyZ+/vOfu+6Tl5cnocAeFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEpRjmnVzDCpqqoyKopp8mvW19eLCZOimCbFN3320kVEsdi4uDjXfWpqajwpFjtkyBDXferq6sREQ0OD6z7Nzc2e/F2YFGP1ksnvdPr0ac+Kxb7//vuu+0yePNl1n8rKSklOTu50HfagAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAVort7g8oKCiQZcuWycKFC2Xu3Lmdrjtu3Dj55JNP2hUH3b9/v1xyySWuHvfaa681KtwZ6oKdpkUaTYpvUmDWH0y2U1VI09ZirKb9amtrPXnuoqOjPStSbPJYJn+DjkEfk6LVytixY8X3e1CbNm2SnJwcueGGG2TXrl1drl9YWNgunJQZM2a4DicAQOQz3oOaMGGCbN++XbKysuTLL7/scv0VK1bIhg0bJDs7u83tgwYNMh0CACCCGQfUyJEj9XL8+PFdBtSOHTukqalJcnNzTR8OANDLdHuSRJ8+fbpcZ/ny5XpPSR2vMj2uAwDoXbodUF0dXNy7d6/8/e9/l9dff10frxo8eLDcd999cuLEie4+NAAggoV8mvl7772nPwZMT0/X31dXV8vKlSv1sah9+/ad1ew2dZn31g0AEPlCHlCLFy+WPXv2yLFjx/TeVOA4VElJifzkJz+Rw4cPdzm5IiUlpaVlZGSEesgAgN50oq76KFDtNamZfBs3btTnURw9elQeeuihTvstXbpUnxMSaCrYAACRLyyVJObMmSNPPPGE/nrz5s2dnmCYkJAgycnJbRoAIPKFrdTRr371K7ngggv0MaXS0tJwDQMAYKmwBZQqYTJ16lTj0kAAgMgW1mKx5557rlx66aWSlJQUzmEAACKxWGx3HDhwQO6++26jvirY1PGpUDItrGpSQNKkz+nTp8ULpgVFvSqK6VXBTqW+vt51H5Pt1ORxTPp4+dqaFH7t27evJ9tDY2OjmDD5nbwqDN1o+DuZfKKVlpbmats52/Ngu70HFaiY29GbZUVFRdD7du/erQc6b9687g4BABCBuhVQqmxR4GTbDz74IGgIDRgwQMaMGSNvv/22vk2Fkip5lJ+fr6ebm/xvBwAQ+YzT4aabbtLVIdS1nJTVq1frMHruuefaXFfkjjvu0DP1VJkj9b2avac+8njmmWc49gQA6PljUOqE267Ex8fLn/70J90AAHCDz9cAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWCms18+4YOHCgJCYmhvQxTCs9qxqFbsXGun8pOrsScU+KiooSr3hVod2UyXNu8vyZbHsm1czPOeccMVFXV+e6j8nfq8nz7dVrZPpYJn/rSQZl4Y4fPy4mTCrIn3feea7+xj2rZg4AQCgQUAAAKxFQAAArEVAAACsRUAAAKxFQAAArEVAAACsRUAAAKxFQAAArEVAAACsRUAAAKxFQAAAr+bZYbHJycsiLxZoWYzUp2mlSDDIuLs6TIqSNjY3i1fMXHR3tyeOYFgKOj4933efUqVOu+2RkZLju8+WXX7ruk5aWJl4VFHUcx5PnzqTgsOn2YPK3YfLcVVRUeNJHSU9Pd90nMzPzrNdtamqSgwcPntW67EEBAKxEQAEArERAAQCsREABAKxEQAEArERAAQCsREABAKxEQAEArERAAQCsREABAKxEQAEArERAAQCs5NtisaoQoklRVjdiY82eHpMirrW1ta77REVFefI7mRRIVRISEjwpFmtSHNSkCKlpUVGT7bR///6u+/Tr1891n5KSEjExbNgw133q6uo8KTBr8ndhWizW5HU6fvy46z41NTWu+wwYMEC8cuWVV7r6e3jjjTfOal32oAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYyqoaqCjj+5S9/kWeffVa++OIL6du3r0yZMkUefPBBmTBhQtA+e/bs0fd/9tlnujDj7Nmz5eGHH5bExESjgauf56aIaWNjo2fFYk0KT5oU+qyurvakKK0pkwKXqgiwF6/ToEGDxIRJIdKUlBTXfZqamlz3GT58uOs+6u/RhElRX5O/C5NCxQ0NDa77mBaeNtleTV7bYQbFeU2KKJsW6L3iiitC8h5ktAf1y1/+UhYtWiT79u3TVaFPnDghr732mkyaNEleeeWVduu//vrr+r7p06fLoUOH5OOPP5adO3fq7718wwQA+IfrgHrzzTdly5Ytkp+fL1VVVfp/Hlu3bpWBAwfq/xncfvvtUlZW1qac/y233CJXXXWVLF68uOV/lGvWrJEPPvhA7r///p79jQAAvTOg1q5dK++884787Gc/09dCUR+vzJw5U1566SV9vwottTcV8Mgjj+iPolRwtTZ69Gi5/PLL5bnnnpODBw/2xO8CAOjNAaWONY0bN67d7erjuvHjx+uvS0tL9VLtUW3evFl/rT7iO9PEiRP1552rV682GTsAIIK5Dqi77rqrw/syMzPbHKzdsWOH3qNSV1YdOnRou/WzsrL0ctu2bW6HAQCIcD16yXd17EmF0YwZM/T3RUVFehksnJTU1FS93L9/v55xEmyWj5qR03pWjgo8AEDk67HzoOrq6mTXrl0yf/78luAJfNQX+L6j6bdqJmBlZWXQdVasWKHXC7SMjIyeGjIAoDcElDqOpCZNPProoy23lZeX66U6T6qrcyk6Og9h6dKlOrwCTc0KBABEvh75iE8F0bJly/TU87S0tHYn2XV04lfrk2db92tNfWSoGgCgd+mRPagFCxbo85kCx54ChgwZopcdnYwbOAs7KSlJ+vTp0xNDAQBEiG4H1PLly+X888+XJUuWtLtv7Nixenn48OGgfY8ePaqX2dnZ3R0GACDCdCug1q1bp2vxPfXUU0HvnzZtmv6Y79ixY22qSwR89dVXenndddd1ZxgAgAhkfAxK1dx79dVXZcOGDe0KaKop42qvSc24y83N1UG2fft2mTVrVpv11Kw/NVFizpw5rh+/ubnZVTFEtb5bXk5pNznOFpiE4oZJ7cNzzjlHTJgUcTUpmmvC9LU9cuSIJ8ViTQrtmhT5/Oabb8Sr19ak4KlJkWc1o9iLPqaFX9VkMi/6VBgUslW+/PJL133+8Y9/hKSYr9EelKq9pyZEvPjii+02VPUHPHfuXCkuLtbf5+Xl6WNML7zwQpv1Dhw4oCuSq2npgRN8AQAIcP3fIBVKKoDU/3TPPAFX/W9H1d1Te06BQBo1apSsWrVK91m/fr3ceuut8t133+nl5MmTO/x4EADQu7kKqIKCArntttv0Rwmd7T7efPPNbT72U9XM1fV31N7UQw89pM+LUsVjf/3rXxtd7wUAEPlcBdT1119vdCxHufrqq3UDAOBscMl3AICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKABA5F5RNxxiYmJcVVQ2uSCiqitoIi4uzqifF4/jpgJ8QGVlpZhQleq9qGZuUlnb5HlQBg8e7Mn4EhMTXfdRRZndSk1NFRMjRoxw3efMqx6EisnzYFI9vvUlg0Ld55lnnnHd58knnxQTp06dEluwBwUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALBSlOM4jvhIVVWVpKSkePJYpoU0hw0b5rpPZmam6z4XXnih6z4ZGRmu+yQkJIgJk02rvr7edZ+GhgbXfUw3+6amJqNt1q26ujpPngeTPsrJkyc9ee6OHj3qus9//vMfz4rFVlRUGPWD6CLUycnJna7DHhQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKFIsFAHiOYrEAAN8ioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAACRE1CqvuyqVaskOztb+vTpI2lpaTJz5kzZvXt3p/3GjRsnUVFRbVp0dLR8+umnpuMHAEQqx8CCBQtUBXTdYmJiWr6Oi4tzXn755aB9CgoKWtZr3a699lpXj11ZWRn059BoNBpNfNPUe3lXXAdUYWGhk56e7uTn5ztVVVVOU1OTs3XrVmfgwIH6QZOTk53S0tJ2/X7wgx84GzZscA4ePNimlZeXu3p8AopGo9HE9y0kATVnzhynqKio3e3vvvtuywOvWbOmzX3bt293cnJynJ5AQNFoNJr0ioByfQxqypQp+ljSmaZPny7jx4/XX5eWlra5b/ny5TJo0CApKCiQkydPducTSQBAb+H0ILV3pX7kSy+91HKb2ttqnZr9+/d37r33Xuf48eNGj8EeFI1Go4nvW0j2oDpTVlYmCQkJMmPGjJbb3nvvPb1nlZ6err+vrq6WlStX6hmA+/bt6/JnNjQ06Mu8t24AgF7A6SG1tbVOYmKic+eddwa9v7m52dm7d6+Tm5vbkqCDBw92/vvf/3b6c/Py8sKe9DQajUYT+ydJdOTpp592Bg0adFaz8jZu3NgyPX3evHmdrltfX69/kUArKSkJ+xNLo9FoNPFHQJWVlelwevPNN8+6z8qVK1umpZ8+ffqs+3EMikaj0aRXBFSU+qe7HxPOmjVLJk2aJEuWLDnrPk1NTXLRRRfJN998I0eOHJHBgwefVT91DColJaUbowUAhFtlZaUkJyd3uk63J0moKeTnn3++q3BS4uLiZOrUqfrrfv36dXcYAIAIE9udzuvWrZMvvvhC1q5da9T/3HPPlUsvvVSSkpK6MwwAQAQy3oN65ZVX5NVXX5U1a9booq+tnT59WkpKSrr8GQcOHJC7777bdAgAgAhmFFBbt26V/Px8efHFFyU2tu1OmDqeNHfuXCkuLtbfV1RU6MA6k6p8rg5/zZs3z3TsAIBI5ri0fv16JzY21klNTXUGDBjQpqkqEepHZmRk6POePvroIyc6Otq56KKLnLfeekv3V7e/8cYbzl133eXU1NS4fXhm8dFoNJr4v/X4NHMVLFFRUV0+8AMPPKDXb2ho0CfuDhkyRF+KIysry1m0aJHzzjvvuA4mAopGo9EkYppn08y9xDRzAPA/T6aZAwAQCgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEq+CyjHccI9BACAB+/lvguo6urqcA8BAODBe3mU47NdkubmZjl8+LD0799foqKiWm6vqqqSjIwMKSkpkeTk5LCOEeHH9oDW2B7soSJHhdN5550n0dGd7yPFis+oX2jYsGEd3q82PjZABLA9oDW2BzukpKSc1Xq++4gPANA7EFAAACtFTEAlJCRIXl6eXgJsD2iN7cGffDdJAgDQO0TMHhQAILIQUAAAKxFQAAArEVAAACsRUAAAK0VEQDU2Nsrjjz8uo0ePllGjRsnUqVNl+/bt4R4WPFJQUCCTJk2StWvXdrrenj175Prrr5cRI0bIhRdeKL/5zW/k5MmTno0ToaEmIq9atUqys7OlT58+kpaWJjNnzpTdu3d32IdtwSccn6uvr3emTZvmjBkzxvn222/1bZs2bXLi4uL0EpFr48aNzhVXXKFOk9Dt+eef73Dd1157zUlISHCeeOIJ/X1FRYUzefJk58orr3Rqamo8HDV62oIFC1q2gZiYmJav1XvAyy+/3G59tgX/8H1A3XPPPXpj/PDDD9vcfvPNNztJSUlOcXFx2MaG0Pr666/1f1AyMzM7DajvvvvO6d+/v3Pttde2uf3zzz93oqKinDvuuMOjEaOnFRYWOunp6U5+fr5TVVXlNDU1OVu3bnUGDhyot4nk5GSntLS0ZX22BX/xdUAdOnTIiY2N1XtPwTZctYHm5uaGZWzwzpw5czoNqHnz5un7g+1Rqz0w9cb02WefeTBShOK1Lyoqanf7u+++27IntWbNmpbb2Rb8xdfHoDZu3CinTp3Sxx/OlJOTo5dbtmyR8vLyMIwOXlHHHTrS1NQkmzdv1l8H204mTpyoj2GsXr06pGNEaEyZMkXGjRvX7vbp06fL+PHj9delpaV6ybbgP9F+PziujBw5st196kDp0KFD9QSKnTt3hmF08Err64KdaceOHfpaQKoGm9oezpSVlaWX27ZtC+kYERp33XVXh/dlZmbq5fDhw/WSbcF/fB1QRUVFetnR9aFSU1P1cu/evZ6OC/ZtI8HekFpvI/v375fTp097OjaEVllZmQ6jGTNm6O/ZFvzHtwFVX18vNTU1bTasji6KpTZU9E6Bj3e62kbUR8WVlZWejg2hU1dXJ7t27ZL58+e3vPZsC/7j24BqfVypb9++QdcJXE5YhRl6p8B20tU2orCdRA51HKl///7y6KOPttzGtuA/vg2o+Pj4lq87umKIOv4UOB6F3imwnXS1jShsJ5FBBdGyZcskPz+/zWvKtuA/vg0otQEFNrja2tqg61RUVOhlenq6p2ODPYYMGXJW20hSUlKnswHhHwsWLJD777+/5dhTANuC//g2oGJiYmTMmDH668OHDwdd5+jRo3qpSqCgdxo7dqxeso30DsuXL5fzzz9flixZ0u4+tgX/8W1AKddcc41efvrpp+3uUxMj1IFO9b8hVZsPvdO0adP0nvaxY8eCTpb56quv9PK6664Lw+jQk9atWydffPGFPPXUU0HvZ1vwH18H1Lx58/SBzWCFYdUMHmX27Nltjlehd0lOTpbc3Fz9dUfbidqG5syZE4bRoae88sor8uqrr8qaNWvanRenpoyXlJSwLfiQrwNKnYi3cOFCfd7Cmec6qQOkiYmJkpeXF7bxwRtqWrDS0bkrahtQe9IvvPBCm9sPHDigq1qrqciBkzrhP1u3btV/7y+++KLExsa2ue/IkSMyd+5cKS4u1t+zLfiM43Oq+vBll13m5OTkOOXl5U5zc7Pz9NNPO/Hx8c7mzZvDPTyEWF1dnZOVlaXrq82fP7/D9davX6/rNq5bt05/ryrfZ2dn6yrWtbW1Ho4YPSnwuqampjoDBgxo01RRWLVdZGRk6PeFM/uwLdjP9wGlqCrGqqr5iBEjnFGjRjkzZ850Pvnkk3APCyGmCgH37du3pSioamlpac6f//znoOu//fbb+pIKaju55JJLnD/+8Y9OQ0OD5+NGz3jjjTd0cdfWr3+w9sADD7Try7bgD1Hqn3DvxQEAEFHHoAAAkYuAAgBYiYACAFiJgAIAWImAAgBYiYACAFiJgAIAWImAAgBYiYACAFiJgAIAWImAAgBYiYACAFiJgAIAWImAAgCIjf4PomfrjhVQ4EMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36000, 784]) torch.Size([36000, 1])\n"
     ]
    }
   ],
   "source": [
    "# Binary reduction of the classes # To avoid using softmax, we regroup classes in two classes\n",
    "class_binary_reduction = True\n",
    "determination_des_classes = True\n",
    "if class_binary_reduction :\n",
    "    if determination_des_classes :\n",
    "        # Determination des classes\n",
    "        class_list = []\n",
    "        class_index = 0\n",
    "        for i in range (x_train.shape[0]):\n",
    "            if y_train[i, class_index] == 1:\n",
    "                class_list.append(x_train[i])\n",
    "                class_index += 1\n",
    "            if len(class_list) == len(y_train[0]):\n",
    "                break\n",
    "            \n",
    "        # For square images\n",
    "        class_list = [x.reshape(int(np.sqrt(len(x))),int(np.sqrt(len(x)))) for x in class_list]\n",
    "        for x in class_list :\n",
    "            plt.imshow(x, cmap='gray')\n",
    "            plt.show()\n",
    "            \n",
    "    classe1 = [0, 2, 4]\n",
    "    classe2 = [5, 7, 9]\n",
    "\n",
    "    # Création des masques pour les échantillons appartenant à ces classes\n",
    "    mask_classe1_train = y_train[:, classe1].sum(dim=1) > 0  # True si appartient à classe1\n",
    "    mask_classe2_train = y_train[:, classe2].sum(dim=1) > 0  # True si appartient à classe2\n",
    "    \n",
    "    mask_classe1_valid = y_valid[:, classe1].sum(dim=1) > 0\n",
    "    mask_classe2_valid = y_valid[:, classe2].sum(dim=1) > 0\n",
    "    \n",
    "    # Filtrage des exemples concernés\n",
    "    mask_train = torch.logical_or(mask_classe1_train, mask_classe2_train)\n",
    "    mask_valid = torch.logical_or(mask_classe1_valid, mask_classe2_valid)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid]\n",
    "\n",
    "    # Création du vecteur de labels binaires (1 pour classe1, 0 pour classe2)\n",
    "    y_train = (y_train[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    y_valid = (y_valid[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672602f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAGpCAYAAADY7qJlAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJDpJREFUeJzt3QlwVeX5x/EnCwkkZJkkIIuRzYCiYRlREIoUEUVligNTolU7IAHRUq0M2GFaxGUAp61Vu1h1oBrBKcsoKIJrpYVicAMkUXTKUoxGQ4hkI9yQkPOf953/zSTkZjlvck/ee/P9zLyem3vOy329Ockv5573PCfCcRxHAACwTGRnDwAAgEAIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJWivXqhs2fPyh//+Ed54YUXpLa2Vi688EJ57LHH5JprrnH179TV1UlhYaEkJCRIRERE0MYLAOh4qrpeRUWF9OvXTyIjWzlGcjzg8/mcyZMnO8OHD3eOHz+un9u0aZPTrVs3vXSjoKBA1Q6k0Wg0moRuU7/LW+NJQN1///16QB9++GGj52+77TYnPj7eOXr0aJv/rdLS0k5/Y2k0Go0m7Wrqd3mnB9SxY8ec6OhoffR0vh07duiBZmVltfnfKysr6/Q3lkaj0WjSrqZ+l7cm6JMkNm7cqM85jR8/vsm6sWPH6uWWLVukpKQk2EMBAISQoAfU9u3b9XLw4MFN1qWkpEj//v31BIo9e/YEeygAgBAS9IDav3+/XqpZe4EkJyfr5YEDBwKur66ulvLy8kYNABD+ghpQPp9PKisrGwXR+ZKSkvTy5MmTAdevXr1ab+Nv6enpQRwxAKBLBFTD80pxcXGBB/D/8+BVmAWybNkyKSsrq28FBQVBGi0AoMtcqBsTE1P/WM0YDESdf/KfjwokNjZWNwBA1xLUIygVOv6QOn36dMBtSktL9TItLS2YQwEAhJigBlRUVJQMHz5cP1bliQIpKirSy5EjRwZzKACAEBP0WXw33HCDXn7++edN1qmJEeq8Unx8vEyaNCnYQwEAhJCgB9S8efP0RIhdu3Y1WZebm6uXs2bNanS+CgCAoAdURkaGLFiwQPLy8ppc65STkyM9evSQFStWBHsYAIBQ43igsrLSueKKK5yxY8c6JSUlTl1dnfP00087MTExzubNm139W9Tio9FoNOkStfg8uR+UOse0c+dOWb58uYwZM0Z/5Hf55ZfLxx9/LCNGjPBiCACAEBOhUkpCiCp15K8+AQAITWqCXGJiYovbcMt3AICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVojt7AEAoiYiIcN3HcRzxQkJCgus+P/rRj4xe68033xRb3++oqCjXfWprayXcRBi8d6aCtY9zBAUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASxWIBFyIj3f9Nd+7cOdd9Lr74Ytd9srOzXfc5c+aMmDh9+rTrPj6fz3Wfjz76yOrCryYFWU32oQiD1/HyfXBToFcVlq2rq2vTthxBAQCsREABAKxEQAEArERAAQCsREABAKxEQAEArERAAQCsREABAKxEQAEArERAAQCsREABAKxEQAEArORpsdgtW7bIzJkzmzz/05/+VDZt2uTlUICgF8VsT7HYa6+91nWf6667znWfb775RkzExsa67hMXF+e6z9SpU133WbNmjes+RUVFYkIVPvVifzDRs2dPo35tLeTaUFVVlYT8EdTq1asDPr9kyRIvhwEACAGeHUG99957+q+uQ4cONSk9P3ToUK+GAQAIEdFeHj395je/kUsuucSrlwQAhDBPPuL78MMP5YMPPpDjx4/Ll19+6cVLAgBCXKRXR0/qbpoLFy6USy+9VK688kp5++23vXhpAECICnpAlZSUyMmTJ2XYsGH1M6A++eQTmTZtmjzwwAOtzoKprq6W8vLyRg0AEP6CHlCpqanyn//8R3+0p8Lq73//u/Tt21eve+qpp2TFihWtHn0lJSXVt/T09GAPGQBgAU+nmauAmTt3rg6rCRMm6Ocef/xxOXbsWLN9li1bJmVlZfWtoKDAwxEDALpUJYnExETZsWOHDBgwQGpqauSVV15pdls1NV1t37ABAMJfp5U6UkHz29/+Vj8+cuRIZw0DAGCpTq3F5y/NYlqSAwAQvjo1oPyTJcaNG9eZwwAAdPVisefLz8+XQYMGyfTp0ztzGECbnT171pPXUdcKujVw4EBPit/6S5S5ZXLt4+jRo133+d3vfue6j7r0xUReXp7rPueXe2uLq666ypN9SFFFFdzKzc1t87bq0qK2Xi4U9CMoVRn31KlTAdepGXwvvPCCUWVkAEB4C3pA3XLLLdKrVy/51a9+JT/88IN+rri4WBYvXizZ2dkyadKkYA8BABCCgv4Rnwqi77//Xt+jJScnRyZOnKivgVIz+FJSUoL98gCAEBX0gPrxj38sH330UbBfBgAQZrjlOwDASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKnVosFugsERERRv1UoUu3pk6d6rrPmDFjXPepqKhw3Sc+Pl5MDB061JM+H3/8ses+hw8fdt3H9JY/V199tes+M2fOdN2npqbGk/dOUSXo3Kqurm7ztrW1tbJ79+42bcsRFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADAShGOSXnmTlReXi5JSUmdPQxYVmXcKyY/Lnv37nXdZ+DAgWLz+60qUrt19uxZ8YLP53Pdp66uzui19u3b50m19VqD93vatGliYvDgwa779O/f33WfsrIySUxMbHEbjqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWiu7sAQANhVjt4jY5deqU6z59+/Z13efMmTOu+8TGxoqJ6Gj3vzp69uzpSeHXHj16eFYsduLEia77jB8/3nWfyEj3xxK9e/cWE2+99ZbYgiMoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKABAeBaL3b59u6xcuVIWLFggc+bMaXa7ffv2yfLly+WLL76QqKgomTVrljz88MNGhR2BUBIXF+dJcVCTPlVVVWKirKzMdZ+SkhLXfQYOHOhJweGIiAgxYfKem+wP586d86wAbnp6uoT8EdSmTZtk7NixMn36dMnNzW1x223btukKvlOmTJFjx47Jp59+Knv27NFfnz592nQIAIAwZhxQY8aMkV27dklGRkaL2xUUFMjtt98u1157rSxevFg/l5SUJGvXrpW9e/fK0qVLTYcAAAhjxgE1ePBgfS+Z0aNHt7jdI488IhUVFTJ37txGzw8bNkyuvPJKefbZZ+XQoUOmwwAAhKl2T5Lo3r17s+tqampk8+bNzd6ka9y4cfrz4jVr1rR3GACAMNPugGrp5OLu3bulvLxcH2n179+/yfrMzEy93LlzZ3uHAQAIM0G95fv+/fv1MlA4KcnJyXqZl5enZ6mo2X3nq66u1s1PBR4AIPwF9Tqo4uLiRkF0PjVZQqmtrW122urq1av1dv5m0xRIAECIBpT/uofm5v03vIbA5/MF3GbZsmU6vPxNzQoEAIS/oH7EFxMT0+KFc2fPnq1/nJKSEnAbdf5KNQBA1xLUI6g+ffroZXMX45aWluplfHx8i7MBAQBdT1ADasSIEXpZWFgYcH1RUZFejhw5MpjDAACEoKAG1OTJk/XHfCdOnJCTJ082WX/48GG9vOmmm4I5DABACArqOajExETJysqSdevW6bJIM2fObLRe1fBTEyVmz54dzGEghJgU7TQp2GlSfFPp2bOn6z79+vVz3afhpRXB7GN6frfh+eNgFqZtbgZwRxelNSng2vA8uxuqso5bSf8/49mNgwcPilf7uCp95+Znz38JUtCPoNQUcf+LBrJixQp9jumll15q9Hx+fr6ucJ6dnd1qPT8AQNfTroA6c+ZMfUqrwq+BDBkyRJ577jl9W47169fr577++mu54447ZMKECfLkk0+2ZwgAgDBlHFC33nqrpKWl6SoQiqqnl5qaqou/nk9VM9+xY4c888wzusisOud05513yvvvv298aA0ACG/G56A2bNjgavupU6fqBgBAW3DLdwCAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAQNerZg641dzdl1sSFRXlWTVzVZ3f9MadbhQXF7vu06NHD9d96urqxIQqAO1Wenq6J1XTTSq019TUiIno6GhPvk+pqamu+/z1r38VE6NGjfLkfWgLjqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWolgsrGJSdNKkoKip/Px8132qq6td9+nWrZvVRXN79+7tuo/P53Pdp6SkxJP3rnv37uJV0dxTp0657vPNN9+47vOzn/1MTPz+97933Wfv3r0SDBxBAQCsREABAKxEQAEArERAAQCsREABAKxEQAEArERAAQCsREABAKxEQAEArERAAQCsREABAKxEQAEArESx2BZEREQY9TMp2hkZGenJ+Gpqalz3qaurE6/U1taKzXbs2OG6z+nTp133OXPmjOs+MTExrvs4jiMmiouLPfm5MCniarKPm/Lq5ynK4L0bMWKEmCgrKxNbcAQFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQAIz2Kx27dvl5UrV8qCBQtkzpw5LW47atQo+eyzz5oUPM3Ly5PLLrtMgsmk2OK5c+fCsuCpza655hrXfWbNmuW6z4QJE8REVVWV6z4lJSWeFH6Njo72bB83eR9MfgZjY2M9KTBrWjTX5H0wEWOwP1RWVhq91syZM1332bZtm1h1BLVp0yYZO3asTJ8+XXJzc9tUBfr8cFKmTZsW9HACAHShI6gxY8bIrl27JDMzU/773/+2uv3q1atlw4YNMnLkyEbP9+7d23QIAIAwZhxQgwcP1svRo0e3GlC7d+/W903JysoyfTkAQBfT7kkSbfm8d9WqVfpISZ2vMrkRGwCg62l3QLV2V9cDBw7IW2+9pU+iqfNVF1xwgTzwwANy6tSp9r40ACCMBX2a+fvvv68/BkxLS9NfV1RUyFNPPaXPRR08eLDV/tXV1VJeXt6oAQDCX9ADavHixbJv3z45ceKEPpryn4cqKCiQ66+/XgoLC1udXJGUlFTf0tPTgz1kAEBXulBXfRSojprUTL6NGzfqayKKiorkoYcearHfsmXLpKysrL6pYAMAhL9OqSQxe/ZseeKJJ/TjzZs3S11dXYsX6iUmJjZqAIDw12mlju69914ZOHCgPqdUXFzcWcMAAFiq0wKqW7duMmnSJP24Z8+enTUMAIClOrVYbN++feXyyy+X+Pj4zhwGACAci8W2R35+vtx3332evJZpUUyvpKSkuO7Tr18/130yMjI8eR3TopNDhw513UddiuBWZGSkZ8VBU1NTXfdpbXZrID6fz5MipKYlys6ePeu6T1xcnOs+H3zwges+pp/imBQ3bumce3PKyspc91HVe0yMGzdOwuYIyl+5u7kAKC0tDbjuk08+0RWE582b194hAADCULsCSpUt8l9su3fv3oAhpP56HD58uLzzzjv6ORVKquRRTk6Onm5u+pcsACC8GafDrbfeqqtDqHs5KWvWrNFh9Oyzz9ZvM2LECLnnnnv0TD1V5kh9rWbvqanjf/7znzn3BADo+HNQ6oLbtny+/Ze//EU3AADc4PM1AICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlQgoAICVCCgAgJUIKACAlTq1mrmXTCr0PvbYY0av1atXL9d9kpOTPanQHhUV5bqPKvjbnkLCblRUVHhSJTsiIkJM6096UV1b3XXaLVX70q2EhAQxYVJBXt2g1AuZmZmevQ8FBQWeVMTv0aOHZxXaBwwYILbgCAoAYCUCCgBgJQIKAGAlAgoAYCUCCgBgJQIKAGAlAgoAYCUCCgBgJQIKAGAlAgoAYCUCCgBgJQIKAGClkC0WGxkZ6arg55/+9CfXr9G3b18xYVLE1aSPSdFJEzExMUb9TP6fTIqxmkhKSvKskObjjz/uyftwzz33uO5TWFgoJnw+n+s+//znP133OXr0qOs+GRkZrvukpqaKCZNCxd26dTP6fedWTU2NmCguLhZbcAQFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwUoTjOI6EkPLycl3o8/bbb3dVxNSkYOeRI0fERM+ePT3pExsbK14wKW5pWpC1oKDAk4KnvXr1EhMmRTv79Onjus8tt9ziuk/37t1d9xk4cKCYMNlfr7jiCk/6mHyPTIq+mr6WafFlt9wU027vz/u4cePavG1dXZ18++23UlZWJomJiS1uyxEUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADAStEmnVR92eeff16eeeYZ+eqrryQuLk4mTpwoy5cvlzFjxgTss2/fPr3+iy++kKioKJk1a5Y8/PDD0qNHD6OBFxcXuypqaFKENCEhQUxUV1e77mMyPpOCnSaFKlsr6NicH374wXWf48ePe/I+nDlzRkz4fD7XfWpra1332bJli+s+eXl5nhWLTUlJ8aQga2lpqes+NTU1nnyP/IVPvSjGWmfwOqbFYk1+RwwdOtTVe62KxQbtCOruu++WhQsXysGDB/WLnTp1Sl5//XUZP368vPrqq02237Ztm143ZcoUOXbsmHz66aeyZ88e/fXp06dNhgAACHOuA+rNN9/Uf93l5OToW1+ovyi3bt2qb1+g/nKZO3eunDx5stGRgbo1xrXXXiuLFy+uvw3D2rVrZe/evbJ06dKO/T8CAHTNgHrxxRfl3XfflZ///Of6I7Do6GiZMWOG/OMf/9DrVWipoym/Rx55RCoqKnRwNTRs2DC58sor5dlnn5VDhw51xP8LAKArB5Q61zRq1Kgmz6uP60aPHl1/fkhRR1SbN2/Wj9VHfIFucqXOZ61Zs8Zk7ACAMOY6oBYtWtTsuoyMDL0cMGCAXu7evVsfUak7v/bv37/J9pmZmXq5c+dOt8MAAIQ5o1l8zVHnnlQYTZs2TX+9f/9+vQwUTkpycnL97KNz587p2X2BZsQ1nBWnAg8AEP467Dqoqqoqyc3Nlezs7Prg8X/U5//6fGqyhKJmAqr70weyevVqvZ2/paend9SQAQBdIaDUeSQ1aeLRRx+tf66kpEQv1XVSAV88MrLV60uWLVumw8vfTK4XAgB00Y/4VBCtXLlSTz1veAGf/4IvNRGitQv3mrvwT31kqBoAoGvpkCOo+fPn6+uZ/Oee/Pr06aOXzV2M679KPD4+Xrp3794RQwEAhIl2B9SqVavkoosukiVLljRZN2LECL0sLCwM2LeoqEgvR44c2d5hAADCTLsCat26dboW35NPPhlw/eTJk/XHfCdOnGhUXcLv8OHDennTTTe1ZxgAgDBkfA5K1dx77bXXZMOGDU2KEqop4+qoSc24y8rK0kG2a9cumTlzZqPt1Kw/NVFi9uzZrl//u+++CzgtvTnNnQdryTfffCMm1EeWbqWlpXlSSDPQHwqt8c/GdEtVGXHL5HyjSfFN04+UTQoIN5wMFMzv06WXXuq6j2ktTJPJSqpmpxf7g8l7Z1Jg1rTIrMlr9TAoqu0/xeJWczOqWxKoeENz1GVD//73v4N3BKVq76kJES+//HKTX0Lff/+9zJkzR44ePaq/XrFihf6F/dJLLzXaLj8/X1c4V9PS/Rf4AgDg5/pPXBVKKoDULQ7OvwBXzcpTdffUkZM/kIYMGSLPPfec7rN+/Xq544475Ouvv9bLCRMmNPvxIACga3MVUNu3b5c777xTf1zW0sdLt912W6OP/VQ18969e+ujqYceekhfF6WKx/7yl780uvcIACD8uQqom2++2ejGWcrUqVN1AwCgLbjlOwDASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAwEoEFADASgQUAMBKBBQAIHzvqNsZ8vLyXFdfd+uuu+4SE83d/6ol/uK6bvh8Ptd9VA1FL6qFm1ZgNil95aaqfcOKyiZUpX63TCrpV1VVGVX492Jspu+DSXV7r/bxhnf3DvYdBUz61BhUQDeptK4MGjTIdR//vf06+r3mCAoAYCUCCgBgJQIKAGAlAgoAYCUCCgBgJQIKAGAlAgoAYCUCCgBgJQIKAGAlAgoAYCUCCgBgJQIKAGClCMe0WmQnKS8vl6SkJE9e68YbbzTqt2TJEtd9evfu7brPyZMnPSlUaVIY1LSIq0mxWJMipCZjUyIiIlz3MfkRMynQa9LH5P02fS2T986Eyeu4KXbaXibveV1dnes+ffr0ERMHDx503Wf27Nmu+5SVlUliYmKL23AEBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsFLIFotVBSHdFIU0KbbopcmTJ7vus3r1ak+K0poW542MjPSkiKtJsVjTArgmTpw44bqPyY/lt99+67qP6c9FZWWlZwV6vXjvampqjF6rqqrKk5+Ld99913WfQ4cOiYkPPvhAvECxWABAyCKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAFYioAAAViKgAABWIqAAAOFTLFZ1ef755+WZZ56Rr776SuLi4mTixImyfPlyGTNmTLP9Ro0aJZ999lnjAURESF5enlx22WWuisXCO5dccolRv7S0NNd9SktLXfe58MILXff53//+JyZMiooeOXLE6LWAcBa0YrF33323LFy4UA4ePCi1tbVy6tQpef3112X8+PHy6quvBuyzY8eOJuGkTJs2rc3hBADoOlwH1JtvvilbtmyRnJwcfTTj8/lk69at0qtXL/3X5dy5c+XkyZMBbw2xYcMGXQK+YVu/fn1H/b8AAMKI6xvpvPjii/reJOrjOr8ZM2ZIz5495brrrtOhpY6m7rrrrvr1u3fv1uGVlZXVcSMHAIQ110dQ6lxTw3DymzJliowePVo/Li4ubrRu1apV+kZ527dvlzNnzrRnvACALsJ1QC1atKjZdRkZGXo5YMCA+ucOHDggb731lmzbtk2mT58uF1xwgTzwwAP6vBUAAJ5MM1fnnmJjY/XEB7/3339fH1n5Z3RVVFTIU089JSNHjtSTLFpTXV2tPzZs2AAA4a/DAqqqqkpyc3MlOztbkpOT659fvHix7Nu3T06cOKGPpvznoQoKCuT666+XwsLCFv9dNblCTSv3t/T09I4aMgCgKwTUmjVrJCEhQR599NGA69X1TuqoSc3k27hxo0RFRUlRUZE89NBDLf67y5Yt0/Pl/U0FGwAg/HVIQJWUlMjKlSv11POUlJRWt589e7Y88cQT+vHmzZulrq6u2W3VR4bqYq6GDQAQ/jokoObPny9Lly5tdO6pNffee68MHDhQn1M6f9YfAADtDig1hfyiiy6SJUuWuOrXrVs3mTRpkn6srqECAKBdF+o2tG7dOl2LT128a6Jv375y+eWXS3x8fHuGAQAIQ8ZHUKrm3muvvSZr167VEyAaOnfuXJsmM+Tn58t9991nOgQAQDhzDGzZssX5yU9+4vh8vibrvvvuO+eOO+5w/vWvf+mvT5065dTW1jbZ7uOPP3Zuvvlm59y5c65eu6ysTFVfp9FoNJqEblO/y1vjOqDWr1/vREdHO8nJyU5qamqjlpCQoF84PT3dqaur0yEUGRnpDB061Hn77bd1f/X8G2+84SxatMiprKx0+/IEFI1Go0notw4PKBUsERERrb7wgw8+qLevrq52fvGLXzh9+vRxunXr5mRmZjoLFy503n33XdfBREDRaDSadKmAMrphYWfihoUAEPqCdsNCAACCjYACAFiJgAIAWImAAgBYiYACAFiJgAIAWImAAgBYiYACAFiJgAIAWImAAgBYiYACAFiJgAIAWImAAgBYiYACAFiJgAIAWImAAgBYiYACAFiJgAIAWImAAgBYiYACAFiJgAIAWCnkAspxnM4eAgDAg9/lIRdQFRUVnT0EAIAHv8sjnBA7JKmrq5PCwkJJSEiQiIiI+ufLy8slPT1dCgoKJDExsVPHiM7H/oCG2B/soSJHhVO/fv0kMrLlY6RoCTHqf+jCCy9sdr3a+dgB4cf+gIbYH+yQlJTUpu1C7iM+AEDXQEABAKwUNgEVGxsrK1as0EuA/QENsT+EppCbJAEA6BrC5ggKABBeCCgAgJUIKACAlQgoAICVCCgAgJXCIqDOnj0rjz/+uAwbNkyGDBkikyZNkl27dnX2sOCR7du3y/jx4+XFF19scbt9+/bJzTffLIMGDZKLL75Yfv3rX8uZM2c8GyeCQ01Efu6552TkyJHSvXt3SUlJkRkzZsgnn3zSbB/2hRDhhDifz+dMnjzZGT58uHP8+HH93KZNm5xu3brpJcLXxo0bnauuukpdJqHbCy+80Oy2r7/+uhMbG+s88cQT+uvS0lJnwoQJztVXX+1UVlZ6OGp0tPnz59fvA1FRUfWP1e+AV155pcn27AuhI+QD6v7779c744cfftjo+dtuu82Jj493jh492mljQ3AdOXJE/4GSkZHRYkB9/fXXTkJCgnPjjTc2ev7LL790IiIinHvuucejEaOj7dixw0lLS3NycnKc8vJyp6amxtm6davTq1cvvU8kJiY6xcXF9duzL4SWkA6oY8eOOdHR0froKdCOq3bQrKysThkbvDN79uwWA2revHl6faAjanUEpn4xffHFFx6MFMH43u/fv7/J8++99179kdTatWvrn2dfCC0hfQ5q48aNUltbq88/nG/s2LF6uWXLFikpKemE0cEr6rxDc2pqamTz5s36caD9ZNy4cfocxpo1a4I6RgTHxIkTZdSoUU2enzJliowePVo/Li4u1kv2hdATGeonx5XBgwc3WadOlPbv319PoNizZ08njA5eaXhfsPPt3r1b3wtI1WBT+8P5MjMz9XLnzp1BHSOCY9GiRc2uy8jI0MsBAwboJftC6AnpgNq/f79eNnd/qOTkZL08cOCAp+OCfftIoF9IDfeRvLw8OXfunKdjQ3CdPHlSh9G0adP01+wLoSdkA8rn80llZWWjHau5m2KpHRVdk//jndb2EfVRcVlZmadjQ/BUVVVJbm6uZGdn13/v2RdCT8gGVMPzSnFxcQG38d9OWIUZuib/ftLaPqKwn4QPdR4pISFBHn300frn2BdCT8gGVExMTP3j5u4Yos4/+c9HoWvy7yet7SMK+0l4UEG0cuVKycnJafQ9ZV8IPSEbUGoH8u9wp0+fDrhNaWmpXqalpXk6NtijT58+bdpH4uPjW5wNiNAxf/58Wbp0af25Jz/2hdATsgEVFRUlw4cP148LCwsDblNUVKSXqgQKuqYRI0boJftI17Bq1Sq56KKLZMmSJU3WsS+EnpANKOWGG27Qy88//7zJOjUxQp3oVH8Nqdp86JomT56sj7RPnDgRcLLM4cOH9fKmm27qhNGhI61bt06++uorefLJJwOuZ18IPSEdUPPmzdMnNgMVhlUzeJRZs2Y1Ol+FriUxMVGysrL04+b2E7UPzZ49uxNGh47y6quvymuvvSZr165tcl2cmjJeUFDAvhCCQjqg1IV4CxYs0NctnH+tkzpB2qNHD1mxYkWnjQ/eUNOCleauXVH7gDqSfumllxo9n5+fr6taq6nI/os6EXq2bt2qf95ffvlliY6ObrTu+++/lzlz5sjRo0f11+wLIcYJcar68BVXXOGMHTvWKSkpcerq6pynn37aiYmJcTZv3tzZw0OQVVVVOZmZmbq+WnZ2drPbrV+/XtdtXLdunf5aVb4fOXKkrmJ9+vRpD0eMjuT/viYnJzupqamNmioKq/aL9PR0/Xvh/D7sC/YL+YBSVBVjVdV80KBBzpAhQ5wZM2Y4n332WWcPC0GmCgHHxcXVFwVVLSUlxfnb3/4WcPt33nlH31JB7SeXXXaZ84c//MGprq72fNzoGG+88YYu7trw+x+oPfjgg036si+Ehgj1n84+igMAIKzOQQEAwhcBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQCwEgEFALASAQUAsBIBBQAQG/0fUWApeaWajZUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf2045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour éviter l'utilisation de softmax, on regroupe les classes dans 2 classes.\n",
    "class_reduction = True\n",
    "if class_reduction :\n",
    "    # Pour le fashion-MNIST, la classe 1 regroupe les t-shirts et la classe 2 regroupe les chaussures\n",
    "    classe1 = [0, 2, 4] # Classe des t-shirts\n",
    "    classe2 = [5, 7, 9] # Classe des chaussures\n",
    "    \n",
    "    # Nouveau dataset à deux classes\n",
    "    x_train_classe1 = x_train[:][0] + x_train[:][2] + x_train[:][4]\n",
    "    x_train_classe2 = x_train[:][5] + x_train[:][7] + x_train[:][9]\n",
    "    \n",
    "    x_train = torch.concatenate([x_train_classe1, x_train_classe2], dim = 1) # shape (n_data, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab5918b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [1.0, 0.0, 2.0],\n",
    "    [0.0, 0.0, 0.0],\n",
    "    [3.0, 4.0, 0.0],\n",
    "    [0.0, 0.0, 0.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45060724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on :  mps\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float16\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Computing on : \", device)\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).to(dtype)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    s = s.to(device)\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(x))\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output); grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = (torch.einsum('no,noz->nz',grad_output,softmax_derivative(output))).to(dtype) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "\n",
    "class binary_classification_two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(1, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = sigmoid(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = grad_output; grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(784,512,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 61465600.0 and the number of iterations is 102442.\n",
      "Iteration 0 Training loss 0.08873898535966873 Validation loss 0.08719004690647125 Accuracy 0.12298583984375\n",
      "Iteration 10 Training loss 0.0648888573050499 Validation loss 0.06581135094165802 Accuracy 0.338134765625\n",
      "Iteration 20 Training loss 0.06443115323781967 Validation loss 0.06473524123430252 Accuracy 0.349853515625\n",
      "Iteration 30 Training loss 0.06799623370170593 Validation loss 0.0688323825597763 Accuracy 0.30810546875\n",
      "Iteration 40 Training loss 0.06466986984014511 Validation loss 0.06249084323644638 Accuracy 0.37255859375\n",
      "Iteration 50 Training loss 0.06200340762734413 Validation loss 0.062392015010118484 Accuracy 0.3740234375\n",
      "Iteration 60 Training loss 0.06763027608394623 Validation loss 0.06684283167123795 Accuracy 0.327392578125\n",
      "Iteration 70 Training loss 0.06348197162151337 Validation loss 0.06400296092033386 Accuracy 0.357421875\n",
      "Iteration 80 Training loss 0.0618005096912384 Validation loss 0.06296548992395401 Accuracy 0.36865234375\n",
      "Iteration 90 Training loss 0.07037458568811417 Validation loss 0.06786629557609558 Accuracy 0.319091796875\n",
      "Iteration 100 Training loss 0.06501201540231705 Validation loss 0.06411509960889816 Accuracy 0.357421875\n",
      "Iteration 110 Training loss 0.06395244598388672 Validation loss 0.06571473181247711 Accuracy 0.341064453125\n",
      "Iteration 120 Training loss 0.06533896923065186 Validation loss 0.0630437508225441 Accuracy 0.367919921875\n",
      "Iteration 130 Training loss 0.06185116618871689 Validation loss 0.062253162264823914 Accuracy 0.376220703125\n",
      "Iteration 140 Training loss 0.058376897126436234 Validation loss 0.06302361935377121 Accuracy 0.3662109375\n",
      "Iteration 150 Training loss 0.06245066225528717 Validation loss 0.06244106590747833 Accuracy 0.372802734375\n",
      "Iteration 160 Training loss 0.0613066591322422 Validation loss 0.06130499765276909 Accuracy 0.384033203125\n",
      "Iteration 170 Training loss 0.06720168143510818 Validation loss 0.061470095068216324 Accuracy 0.3828125\n",
      "Iteration 180 Training loss 0.061186451464891434 Validation loss 0.060918133705854416 Accuracy 0.387451171875\n",
      "Iteration 190 Training loss 0.0613677091896534 Validation loss 0.060736317187547684 Accuracy 0.390869140625\n",
      "Iteration 200 Training loss 0.05534850060939789 Validation loss 0.05948057398200035 Accuracy 0.40234375\n",
      "Iteration 210 Training loss 0.05824742093682289 Validation loss 0.05835050716996193 Accuracy 0.4140625\n",
      "Iteration 220 Training loss 0.05319738760590553 Validation loss 0.05829503387212753 Accuracy 0.414794921875\n",
      "Iteration 230 Training loss 0.06719761341810226 Validation loss 0.06382967531681061 Accuracy 0.359130859375\n",
      "Iteration 240 Training loss 0.05687244236469269 Validation loss 0.05806879699230194 Accuracy 0.41748046875\n",
      "Iteration 250 Training loss 0.05524727329611778 Validation loss 0.058149535208940506 Accuracy 0.41650390625\n",
      "Iteration 260 Training loss 0.05478769913315773 Validation loss 0.057466473430395126 Accuracy 0.421875\n",
      "Iteration 270 Training loss 0.057803042232990265 Validation loss 0.05618630722165108 Accuracy 0.4365234375\n",
      "Iteration 280 Training loss 0.05778995528817177 Validation loss 0.05547327175736427 Accuracy 0.442138671875\n",
      "Iteration 290 Training loss 0.05411383509635925 Validation loss 0.054744891822338104 Accuracy 0.44970703125\n",
      "Iteration 300 Training loss 0.05176609382033348 Validation loss 0.05582471936941147 Accuracy 0.438232421875\n",
      "Iteration 310 Training loss 0.05636608973145485 Validation loss 0.055108506232500076 Accuracy 0.44580078125\n",
      "Iteration 320 Training loss 0.05237720161676407 Validation loss 0.05539897456765175 Accuracy 0.44384765625\n",
      "Iteration 330 Training loss 0.05363442003726959 Validation loss 0.05498915910720825 Accuracy 0.4482421875\n",
      "Iteration 340 Training loss 0.055649835616350174 Validation loss 0.05486975982785225 Accuracy 0.4501953125\n",
      "Iteration 350 Training loss 0.05319006368517876 Validation loss 0.054528046399354935 Accuracy 0.453125\n",
      "Iteration 360 Training loss 0.05288391560316086 Validation loss 0.054845891892910004 Accuracy 0.44921875\n",
      "Iteration 370 Training loss 0.05193470045924187 Validation loss 0.05467404052615166 Accuracy 0.45068359375\n",
      "Iteration 380 Training loss 0.05431530624628067 Validation loss 0.0549226738512516 Accuracy 0.448486328125\n",
      "Iteration 390 Training loss 0.0542169064283371 Validation loss 0.05474216490983963 Accuracy 0.4501953125\n",
      "Iteration 400 Training loss 0.05676981061697006 Validation loss 0.05469811335206032 Accuracy 0.450927734375\n",
      "Iteration 410 Training loss 0.05176249146461487 Validation loss 0.0548730306327343 Accuracy 0.44970703125\n",
      "Iteration 420 Training loss 0.05257350951433182 Validation loss 0.05431675538420677 Accuracy 0.455078125\n",
      "Iteration 430 Training loss 0.052408818155527115 Validation loss 0.05438200384378433 Accuracy 0.45458984375\n",
      "Iteration 440 Training loss 0.05412275716662407 Validation loss 0.05531137436628342 Accuracy 0.445068359375\n",
      "Iteration 450 Training loss 0.05782359838485718 Validation loss 0.054150864481925964 Accuracy 0.457275390625\n",
      "Iteration 460 Training loss 0.05145326629281044 Validation loss 0.0547969825565815 Accuracy 0.4501953125\n",
      "Iteration 470 Training loss 0.05418333783745766 Validation loss 0.054301608353853226 Accuracy 0.45458984375\n",
      "Iteration 480 Training loss 0.053322143852710724 Validation loss 0.054431308060884476 Accuracy 0.4541015625\n",
      "Iteration 490 Training loss 0.05789606645703316 Validation loss 0.05464451014995575 Accuracy 0.451904296875\n",
      "Iteration 500 Training loss 0.05290977284312248 Validation loss 0.054285239428281784 Accuracy 0.455810546875\n",
      "Iteration 510 Training loss 0.05232023075222969 Validation loss 0.054696325212717056 Accuracy 0.45166015625\n",
      "Iteration 520 Training loss 0.05636812373995781 Validation loss 0.05611239746212959 Accuracy 0.437255859375\n",
      "Iteration 530 Training loss 0.05245428532361984 Validation loss 0.053481269627809525 Accuracy 0.46337890625\n",
      "Iteration 540 Training loss 0.054428476840257645 Validation loss 0.054173074662685394 Accuracy 0.45751953125\n",
      "Iteration 550 Training loss 0.05455085635185242 Validation loss 0.05461525917053223 Accuracy 0.45263671875\n",
      "Iteration 560 Training loss 0.05859553813934326 Validation loss 0.053940076380968094 Accuracy 0.459228515625\n",
      "Iteration 570 Training loss 0.056227438151836395 Validation loss 0.053815316408872604 Accuracy 0.46044921875\n",
      "Iteration 580 Training loss 0.05344901606440544 Validation loss 0.05436220392584801 Accuracy 0.455078125\n",
      "Iteration 590 Training loss 0.05106673762202263 Validation loss 0.05399161949753761 Accuracy 0.45849609375\n",
      "Iteration 600 Training loss 0.05503452941775322 Validation loss 0.05435309559106827 Accuracy 0.455078125\n",
      "Iteration 610 Training loss 0.05526646226644516 Validation loss 0.054028138518333435 Accuracy 0.45849609375\n",
      "Iteration 620 Training loss 0.05533277243375778 Validation loss 0.05430673807859421 Accuracy 0.45556640625\n",
      "Iteration 630 Training loss 0.058118417859077454 Validation loss 0.055053163319826126 Accuracy 0.44677734375\n",
      "Iteration 640 Training loss 0.05423586070537567 Validation loss 0.05411215499043465 Accuracy 0.457763671875\n",
      "Iteration 650 Training loss 0.05980882793664932 Validation loss 0.054663464426994324 Accuracy 0.45068359375\n",
      "Iteration 660 Training loss 0.05600868538022041 Validation loss 0.05379796773195267 Accuracy 0.459716796875\n",
      "Iteration 670 Training loss 0.055917758494615555 Validation loss 0.054515644907951355 Accuracy 0.453369140625\n",
      "Iteration 680 Training loss 0.05136466398835182 Validation loss 0.0532400980591774 Accuracy 0.465087890625\n",
      "Iteration 690 Training loss 0.05536707490682602 Validation loss 0.0531838983297348 Accuracy 0.46630859375\n",
      "Iteration 700 Training loss 0.056784696877002716 Validation loss 0.053712595254182816 Accuracy 0.461669921875\n",
      "Iteration 710 Training loss 0.05082984268665314 Validation loss 0.05345163866877556 Accuracy 0.463134765625\n",
      "Iteration 720 Training loss 0.056556228548288345 Validation loss 0.05354202166199684 Accuracy 0.46337890625\n",
      "Iteration 730 Training loss 0.05206935107707977 Validation loss 0.05358005687594414 Accuracy 0.462890625\n",
      "Iteration 740 Training loss 0.04933498054742813 Validation loss 0.05326201021671295 Accuracy 0.465087890625\n",
      "Iteration 750 Training loss 0.05295500531792641 Validation loss 0.053054872900247574 Accuracy 0.466796875\n",
      "Iteration 760 Training loss 0.05186537280678749 Validation loss 0.05362504720687866 Accuracy 0.46142578125\n",
      "Iteration 770 Training loss 0.05093665421009064 Validation loss 0.05384475365281105 Accuracy 0.460205078125\n",
      "Iteration 780 Training loss 0.05290981009602547 Validation loss 0.05380348488688469 Accuracy 0.4609375\n",
      "Iteration 790 Training loss 0.052377037703990936 Validation loss 0.05359483137726784 Accuracy 0.462890625\n",
      "Iteration 800 Training loss 0.054275430738925934 Validation loss 0.053819362074136734 Accuracy 0.460205078125\n",
      "Iteration 810 Training loss 0.0530727244913578 Validation loss 0.05411991477012634 Accuracy 0.456787109375\n",
      "Iteration 820 Training loss 0.05230933055281639 Validation loss 0.05329689383506775 Accuracy 0.46533203125\n",
      "Iteration 830 Training loss 0.04931347444653511 Validation loss 0.05352436378598213 Accuracy 0.463623046875\n",
      "Iteration 840 Training loss 0.053158555179834366 Validation loss 0.05355779826641083 Accuracy 0.462646484375\n",
      "Iteration 850 Training loss 0.05235757306218147 Validation loss 0.05386379361152649 Accuracy 0.459228515625\n",
      "Iteration 860 Training loss 0.05272277817130089 Validation loss 0.053253427147865295 Accuracy 0.466552734375\n",
      "Iteration 870 Training loss 0.054182711988687515 Validation loss 0.053647223860025406 Accuracy 0.4619140625\n",
      "Iteration 880 Training loss 0.05238374322652817 Validation loss 0.05376311391592026 Accuracy 0.460205078125\n",
      "Iteration 890 Training loss 0.05578560382127762 Validation loss 0.05444573238492012 Accuracy 0.452392578125\n",
      "Iteration 900 Training loss 0.050764016807079315 Validation loss 0.05303240567445755 Accuracy 0.468017578125\n",
      "Iteration 910 Training loss 0.054966285824775696 Validation loss 0.05330003798007965 Accuracy 0.4658203125\n",
      "Iteration 920 Training loss 0.05565338209271431 Validation loss 0.053539469838142395 Accuracy 0.463623046875\n",
      "Iteration 930 Training loss 0.05631861463189125 Validation loss 0.053998664021492004 Accuracy 0.458740234375\n",
      "Iteration 940 Training loss 0.05612253397703171 Validation loss 0.053969647735357285 Accuracy 0.45849609375\n",
      "Iteration 950 Training loss 0.056495193392038345 Validation loss 0.0530618354678154 Accuracy 0.46630859375\n",
      "Iteration 960 Training loss 0.05191842466592789 Validation loss 0.052737634629011154 Accuracy 0.469970703125\n",
      "Iteration 970 Training loss 0.051462676376104355 Validation loss 0.054117001593112946 Accuracy 0.456787109375\n",
      "Iteration 980 Training loss 0.05051169916987419 Validation loss 0.052768524736166 Accuracy 0.469970703125\n",
      "Iteration 990 Training loss 0.0499495193362236 Validation loss 0.05352887138724327 Accuracy 0.46240234375\n",
      "Iteration 1000 Training loss 0.05258674919605255 Validation loss 0.053283654153347015 Accuracy 0.466064453125\n",
      "Iteration 1010 Training loss 0.05786912888288498 Validation loss 0.05326896905899048 Accuracy 0.464599609375\n",
      "Iteration 1020 Training loss 0.05320652201771736 Validation loss 0.05302725359797478 Accuracy 0.468017578125\n",
      "Iteration 1030 Training loss 0.05352303013205528 Validation loss 0.05285127833485603 Accuracy 0.469482421875\n",
      "Iteration 1040 Training loss 0.05315389111638069 Validation loss 0.05286652222275734 Accuracy 0.469482421875\n",
      "Iteration 1050 Training loss 0.05383773148059845 Validation loss 0.05293472483754158 Accuracy 0.46923828125\n",
      "Iteration 1060 Training loss 0.05262317880988121 Validation loss 0.053881920874118805 Accuracy 0.459716796875\n",
      "Iteration 1070 Training loss 0.0529792383313179 Validation loss 0.053313788026571274 Accuracy 0.465087890625\n",
      "Iteration 1080 Training loss 0.05315292254090309 Validation loss 0.053486619144678116 Accuracy 0.46337890625\n",
      "Iteration 1090 Training loss 0.052598804235458374 Validation loss 0.0538477785885334 Accuracy 0.45849609375\n",
      "Iteration 1100 Training loss 0.054121967405080795 Validation loss 0.053053855895996094 Accuracy 0.466796875\n",
      "Iteration 1110 Training loss 0.04912934824824333 Validation loss 0.05323474854230881 Accuracy 0.466064453125\n",
      "Iteration 1120 Training loss 0.055472370237112045 Validation loss 0.05297423526644707 Accuracy 0.46923828125\n",
      "Iteration 1130 Training loss 0.05344276502728462 Validation loss 0.0531202033162117 Accuracy 0.46728515625\n",
      "Iteration 1140 Training loss 0.053825743496418 Validation loss 0.05330861732363701 Accuracy 0.46533203125\n",
      "Iteration 1150 Training loss 0.052054889500141144 Validation loss 0.05333883687853813 Accuracy 0.464599609375\n",
      "Iteration 1160 Training loss 0.055187419056892395 Validation loss 0.053212620317935944 Accuracy 0.464599609375\n",
      "Iteration 1170 Training loss 0.05568487569689751 Validation loss 0.05340074747800827 Accuracy 0.46484375\n",
      "Iteration 1180 Training loss 0.052727892994880676 Validation loss 0.052725233137607574 Accuracy 0.470703125\n",
      "Iteration 1190 Training loss 0.054039839655160904 Validation loss 0.05277762562036514 Accuracy 0.470703125\n",
      "Iteration 1200 Training loss 0.04826386645436287 Validation loss 0.0530008040368557 Accuracy 0.46826171875\n",
      "Iteration 1210 Training loss 0.055026426911354065 Validation loss 0.05316392332315445 Accuracy 0.467041015625\n",
      "Iteration 1220 Training loss 0.05360451340675354 Validation loss 0.05317075923085213 Accuracy 0.467041015625\n",
      "Iteration 1230 Training loss 0.04966670647263527 Validation loss 0.05282680690288544 Accuracy 0.470703125\n",
      "Iteration 1240 Training loss 0.05010940507054329 Validation loss 0.05294374004006386 Accuracy 0.46875\n",
      "Iteration 1250 Training loss 0.05419950187206268 Validation loss 0.05311361327767372 Accuracy 0.467041015625\n",
      "Iteration 1260 Training loss 0.053258657455444336 Validation loss 0.05281182378530502 Accuracy 0.47021484375\n",
      "Iteration 1270 Training loss 0.053380921483039856 Validation loss 0.053134724497795105 Accuracy 0.46728515625\n",
      "Iteration 1280 Training loss 0.05365132540464401 Validation loss 0.05325046181678772 Accuracy 0.466552734375\n",
      "Iteration 1290 Training loss 0.05577857419848442 Validation loss 0.052935004234313965 Accuracy 0.469482421875\n",
      "Iteration 1300 Training loss 0.054444096982479095 Validation loss 0.05291016399860382 Accuracy 0.4697265625\n",
      "Iteration 1310 Training loss 0.05157856270670891 Validation loss 0.052821140736341476 Accuracy 0.470703125\n",
      "Iteration 1320 Training loss 0.0552549734711647 Validation loss 0.05358691141009331 Accuracy 0.463134765625\n",
      "Iteration 1330 Training loss 0.05423497036099434 Validation loss 0.05264442414045334 Accuracy 0.470703125\n",
      "Iteration 1340 Training loss 0.053775887936353683 Validation loss 0.052858173847198486 Accuracy 0.469482421875\n",
      "Iteration 1350 Training loss 0.0541607066988945 Validation loss 0.05280902609229088 Accuracy 0.470703125\n",
      "Iteration 1360 Training loss 0.053587377071380615 Validation loss 0.052731674164533615 Accuracy 0.47119140625\n",
      "Iteration 1370 Training loss 0.049794696271419525 Validation loss 0.05314163491129875 Accuracy 0.466796875\n",
      "Iteration 1380 Training loss 0.053935322910547256 Validation loss 0.05278247967362404 Accuracy 0.470947265625\n",
      "Iteration 1390 Training loss 0.056148502975702286 Validation loss 0.05372753366827965 Accuracy 0.460205078125\n",
      "Iteration 1400 Training loss 0.053070832043886185 Validation loss 0.052982743829488754 Accuracy 0.4677734375\n",
      "Iteration 1410 Training loss 0.054523371160030365 Validation loss 0.05382207781076431 Accuracy 0.4599609375\n",
      "Iteration 1420 Training loss 0.05397353693842888 Validation loss 0.05305704474449158 Accuracy 0.468505859375\n",
      "Iteration 1430 Training loss 0.054545383900403976 Validation loss 0.05344901233911514 Accuracy 0.464599609375\n",
      "Iteration 1440 Training loss 0.05341055244207382 Validation loss 0.05321412906050682 Accuracy 0.466796875\n",
      "Iteration 1450 Training loss 0.05254172161221504 Validation loss 0.05464787781238556 Accuracy 0.45166015625\n",
      "Iteration 1460 Training loss 0.04954525828361511 Validation loss 0.05313720181584358 Accuracy 0.4677734375\n",
      "Iteration 1470 Training loss 0.05731383338570595 Validation loss 0.0531938374042511 Accuracy 0.466796875\n",
      "Iteration 1480 Training loss 0.055765777826309204 Validation loss 0.05336146429181099 Accuracy 0.465087890625\n",
      "Iteration 1490 Training loss 0.053352221846580505 Validation loss 0.053734228014945984 Accuracy 0.460693359375\n",
      "Iteration 1500 Training loss 0.05334495007991791 Validation loss 0.05316542834043503 Accuracy 0.46728515625\n",
      "Iteration 1510 Training loss 0.05370621755719185 Validation loss 0.05352998897433281 Accuracy 0.463623046875\n",
      "Iteration 1520 Training loss 0.04977399483323097 Validation loss 0.0537346675992012 Accuracy 0.461181640625\n",
      "Iteration 1530 Training loss 0.05285084620118141 Validation loss 0.052890975028276443 Accuracy 0.47021484375\n",
      "Iteration 1540 Training loss 0.05098501592874527 Validation loss 0.05317552760243416 Accuracy 0.467041015625\n",
      "Iteration 1550 Training loss 0.05285869166254997 Validation loss 0.05407630652189255 Accuracy 0.458251953125\n",
      "Iteration 1560 Training loss 0.04818011447787285 Validation loss 0.054606061428785324 Accuracy 0.452392578125\n",
      "Iteration 1570 Training loss 0.051774363964796066 Validation loss 0.05279981717467308 Accuracy 0.470703125\n",
      "Iteration 1580 Training loss 0.051002006977796555 Validation loss 0.05304553732275963 Accuracy 0.468505859375\n",
      "Iteration 1590 Training loss 0.048899125307798386 Validation loss 0.05342140048742294 Accuracy 0.462890625\n",
      "Iteration 1600 Training loss 0.05129348859190941 Validation loss 0.052665602415800095 Accuracy 0.471923828125\n",
      "Iteration 1610 Training loss 0.05324862152338028 Validation loss 0.054665789008140564 Accuracy 0.45068359375\n",
      "Iteration 1620 Training loss 0.05544314160943031 Validation loss 0.05324570834636688 Accuracy 0.46630859375\n",
      "Iteration 1630 Training loss 0.05235067754983902 Validation loss 0.05312928929924965 Accuracy 0.4677734375\n",
      "Iteration 1640 Training loss 0.05552074685692787 Validation loss 0.05295884609222412 Accuracy 0.46923828125\n",
      "Iteration 1650 Training loss 0.05584166944026947 Validation loss 0.05313968285918236 Accuracy 0.46728515625\n",
      "Iteration 1660 Training loss 0.05399243161082268 Validation loss 0.05313638597726822 Accuracy 0.466796875\n",
      "Iteration 1670 Training loss 0.054414570331573486 Validation loss 0.05302705988287926 Accuracy 0.46728515625\n",
      "Iteration 1680 Training loss 0.052227091044187546 Validation loss 0.053148116916418076 Accuracy 0.466552734375\n",
      "Iteration 1690 Training loss 0.053415004163980484 Validation loss 0.053008172661066055 Accuracy 0.468505859375\n",
      "Iteration 1700 Training loss 0.05512889474630356 Validation loss 0.05301177501678467 Accuracy 0.46826171875\n",
      "Iteration 1710 Training loss 0.05392233282327652 Validation loss 0.05312889441847801 Accuracy 0.467041015625\n",
      "Iteration 1720 Training loss 0.05319473147392273 Validation loss 0.05293838679790497 Accuracy 0.46826171875\n",
      "Iteration 1730 Training loss 0.05406292527914047 Validation loss 0.05324114114046097 Accuracy 0.46484375\n",
      "Iteration 1740 Training loss 0.053735945373773575 Validation loss 0.05271979048848152 Accuracy 0.471435546875\n",
      "Iteration 1750 Training loss 0.054460540413856506 Validation loss 0.052878376096487045 Accuracy 0.47021484375\n",
      "Iteration 1760 Training loss 0.055152226239442825 Validation loss 0.05306633189320564 Accuracy 0.468017578125\n",
      "Iteration 1770 Training loss 0.055064961314201355 Validation loss 0.052858851850032806 Accuracy 0.4697265625\n",
      "Iteration 1780 Training loss 0.04857947677373886 Validation loss 0.05241221562027931 Accuracy 0.473876953125\n",
      "Iteration 1790 Training loss 0.057847585529088974 Validation loss 0.05580241233110428 Accuracy 0.439453125\n",
      "Iteration 1800 Training loss 0.05262940749526024 Validation loss 0.052591461688280106 Accuracy 0.4716796875\n",
      "Iteration 1810 Training loss 0.05454179272055626 Validation loss 0.05238979309797287 Accuracy 0.473876953125\n",
      "Iteration 1820 Training loss 0.05089756101369858 Validation loss 0.05270136892795563 Accuracy 0.470703125\n",
      "Iteration 1830 Training loss 0.05257895961403847 Validation loss 0.05363069102168083 Accuracy 0.461181640625\n",
      "Iteration 1840 Training loss 0.05292391777038574 Validation loss 0.05335060879588127 Accuracy 0.46484375\n",
      "Iteration 1850 Training loss 0.052784524857997894 Validation loss 0.05264147371053696 Accuracy 0.471923828125\n",
      "Iteration 1860 Training loss 0.05205767601728439 Validation loss 0.05307019501924515 Accuracy 0.466796875\n",
      "Iteration 1870 Training loss 0.05376218259334564 Validation loss 0.05302976444363594 Accuracy 0.468017578125\n",
      "Iteration 1880 Training loss 0.05333486944437027 Validation loss 0.052438605576753616 Accuracy 0.47314453125\n",
      "Iteration 1890 Training loss 0.04798690602183342 Validation loss 0.05338165909051895 Accuracy 0.46435546875\n",
      "Iteration 1900 Training loss 0.05188595503568649 Validation loss 0.05370244011282921 Accuracy 0.461181640625\n",
      "Iteration 1910 Training loss 0.05383896827697754 Validation loss 0.052493616938591 Accuracy 0.472900390625\n",
      "Iteration 1920 Training loss 0.051581837236881256 Validation loss 0.05264886096119881 Accuracy 0.47119140625\n",
      "Iteration 1930 Training loss 0.05134371295571327 Validation loss 0.052916064858436584 Accuracy 0.468505859375\n",
      "Iteration 1940 Training loss 0.05063670128583908 Validation loss 0.05288103222846985 Accuracy 0.46875\n",
      "Iteration 1950 Training loss 0.0531306266784668 Validation loss 0.05286925286054611 Accuracy 0.46923828125\n",
      "Iteration 1960 Training loss 0.05162947624921799 Validation loss 0.05297563225030899 Accuracy 0.468017578125\n",
      "Iteration 1970 Training loss 0.05241036415100098 Validation loss 0.053203269839286804 Accuracy 0.46630859375\n",
      "Iteration 1980 Training loss 0.05656295642256737 Validation loss 0.052827831357717514 Accuracy 0.469970703125\n",
      "Iteration 1990 Training loss 0.04963415488600731 Validation loss 0.05257592722773552 Accuracy 0.472412109375\n",
      "Iteration 2000 Training loss 0.05203556269407272 Validation loss 0.05309494957327843 Accuracy 0.467041015625\n",
      "Iteration 2010 Training loss 0.05061446130275726 Validation loss 0.05311679095029831 Accuracy 0.4677734375\n",
      "Iteration 2020 Training loss 0.04951909929513931 Validation loss 0.05259530246257782 Accuracy 0.472412109375\n",
      "Iteration 2030 Training loss 0.04752800613641739 Validation loss 0.052940018475055695 Accuracy 0.46875\n",
      "Iteration 2040 Training loss 0.055789027363061905 Validation loss 0.05268985033035278 Accuracy 0.47216796875\n",
      "Iteration 2050 Training loss 0.05439608544111252 Validation loss 0.052465349435806274 Accuracy 0.4736328125\n",
      "Iteration 2060 Training loss 0.05109844356775284 Validation loss 0.052924130111932755 Accuracy 0.469482421875\n",
      "Iteration 2070 Training loss 0.05093551054596901 Validation loss 0.052479296922683716 Accuracy 0.4736328125\n",
      "Iteration 2080 Training loss 0.0514591783285141 Validation loss 0.05269809439778328 Accuracy 0.4716796875\n",
      "Iteration 2090 Training loss 0.05383389815688133 Validation loss 0.05295654758810997 Accuracy 0.469482421875\n",
      "Iteration 2100 Training loss 0.05279750004410744 Validation loss 0.052799589931964874 Accuracy 0.47119140625\n",
      "Iteration 2110 Training loss 0.04944950342178345 Validation loss 0.05257728695869446 Accuracy 0.47314453125\n",
      "Iteration 2120 Training loss 0.0548916757106781 Validation loss 0.052730388939380646 Accuracy 0.4716796875\n",
      "Iteration 2130 Training loss 0.052572183310985565 Validation loss 0.0527842678129673 Accuracy 0.470947265625\n",
      "Iteration 2140 Training loss 0.05038655921816826 Validation loss 0.05277272313833237 Accuracy 0.47119140625\n",
      "Iteration 2150 Training loss 0.053633708506822586 Validation loss 0.05282588303089142 Accuracy 0.4697265625\n",
      "Iteration 2160 Training loss 0.05204606056213379 Validation loss 0.05245853215456009 Accuracy 0.474365234375\n",
      "Iteration 2170 Training loss 0.049259867519140244 Validation loss 0.05320606753230095 Accuracy 0.46630859375\n",
      "Iteration 2180 Training loss 0.05174567550420761 Validation loss 0.0528770387172699 Accuracy 0.47021484375\n",
      "Iteration 2190 Training loss 0.05075254291296005 Validation loss 0.052720461040735245 Accuracy 0.4716796875\n",
      "Iteration 2200 Training loss 0.05427077040076256 Validation loss 0.05327456444501877 Accuracy 0.4658203125\n",
      "Iteration 2210 Training loss 0.05139591544866562 Validation loss 0.053002674132585526 Accuracy 0.46923828125\n",
      "Iteration 2220 Training loss 0.05269908159971237 Validation loss 0.052533894777297974 Accuracy 0.473876953125\n",
      "Iteration 2230 Training loss 0.05188612639904022 Validation loss 0.05261143296957016 Accuracy 0.472900390625\n",
      "Iteration 2240 Training loss 0.05699131637811661 Validation loss 0.05250418186187744 Accuracy 0.474365234375\n",
      "Iteration 2250 Training loss 0.05461494252085686 Validation loss 0.0524241179227829 Accuracy 0.473876953125\n",
      "Iteration 2260 Training loss 0.05060024559497833 Validation loss 0.05239978805184364 Accuracy 0.47509765625\n",
      "Iteration 2270 Training loss 0.05310650169849396 Validation loss 0.052772704511880875 Accuracy 0.471435546875\n",
      "Iteration 2280 Training loss 0.05431601405143738 Validation loss 0.05306261405348778 Accuracy 0.46875\n",
      "Iteration 2290 Training loss 0.049726713448762894 Validation loss 0.052401572465896606 Accuracy 0.47509765625\n",
      "Iteration 2300 Training loss 0.048825159668922424 Validation loss 0.05249311029911041 Accuracy 0.473876953125\n",
      "Iteration 2310 Training loss 0.054422393441200256 Validation loss 0.05242621898651123 Accuracy 0.474365234375\n",
      "Iteration 2320 Training loss 0.052011217921972275 Validation loss 0.05278073251247406 Accuracy 0.47119140625\n",
      "Iteration 2330 Training loss 0.046745024621486664 Validation loss 0.05256200209259987 Accuracy 0.472900390625\n",
      "Iteration 2340 Training loss 0.052746713161468506 Validation loss 0.05262196809053421 Accuracy 0.472900390625\n",
      "Iteration 2350 Training loss 0.04990316182374954 Validation loss 0.0524284653365612 Accuracy 0.473388671875\n",
      "Iteration 2360 Training loss 0.051518235355615616 Validation loss 0.052862703800201416 Accuracy 0.4697265625\n",
      "Iteration 2370 Training loss 0.05339847877621651 Validation loss 0.052652694284915924 Accuracy 0.4716796875\n",
      "Iteration 2380 Training loss 0.05243454873561859 Validation loss 0.05326396971940994 Accuracy 0.4658203125\n",
      "Iteration 2390 Training loss 0.05317584052681923 Validation loss 0.052517324686050415 Accuracy 0.47314453125\n",
      "Iteration 2400 Training loss 0.05094875022768974 Validation loss 0.05287214368581772 Accuracy 0.47021484375\n",
      "Iteration 2410 Training loss 0.05404603108763695 Validation loss 0.0527176558971405 Accuracy 0.471923828125\n",
      "Iteration 2420 Training loss 0.053896091878414154 Validation loss 0.053431298583745956 Accuracy 0.46484375\n",
      "Iteration 2430 Training loss 0.05039006844162941 Validation loss 0.05245967209339142 Accuracy 0.47412109375\n",
      "Iteration 2440 Training loss 0.05432981997728348 Validation loss 0.05270843207836151 Accuracy 0.47119140625\n",
      "Iteration 2450 Training loss 0.054123543202877045 Validation loss 0.05259969085454941 Accuracy 0.472900390625\n",
      "Iteration 2460 Training loss 0.05387561023235321 Validation loss 0.0545460544526577 Accuracy 0.452392578125\n",
      "Iteration 2470 Training loss 0.04944222420454025 Validation loss 0.053532760590314865 Accuracy 0.46240234375\n",
      "Iteration 2480 Training loss 0.055259883403778076 Validation loss 0.05259300768375397 Accuracy 0.472412109375\n",
      "Iteration 2490 Training loss 0.052565135061740875 Validation loss 0.053276218473911285 Accuracy 0.4658203125\n",
      "Iteration 2500 Training loss 0.04996044188737869 Validation loss 0.05249463766813278 Accuracy 0.47314453125\n",
      "Iteration 2510 Training loss 0.05220412462949753 Validation loss 0.052275151014328 Accuracy 0.474853515625\n",
      "Iteration 2520 Training loss 0.05080035701394081 Validation loss 0.052561696618795395 Accuracy 0.472412109375\n",
      "Iteration 2530 Training loss 0.05176117271184921 Validation loss 0.052611298859119415 Accuracy 0.4716796875\n",
      "Iteration 2540 Training loss 0.05407468602061272 Validation loss 0.052645593881607056 Accuracy 0.47119140625\n",
      "Iteration 2550 Training loss 0.051238566637039185 Validation loss 0.05322753265500069 Accuracy 0.466796875\n",
      "Iteration 2560 Training loss 0.051269449293613434 Validation loss 0.05257749930024147 Accuracy 0.473388671875\n",
      "Iteration 2570 Training loss 0.05391676351428032 Validation loss 0.05242154747247696 Accuracy 0.47412109375\n",
      "Iteration 2580 Training loss 0.054674550890922546 Validation loss 0.05266905575990677 Accuracy 0.47216796875\n",
      "Iteration 2590 Training loss 0.050808653235435486 Validation loss 0.05301447585225105 Accuracy 0.46826171875\n",
      "Iteration 2600 Training loss 0.05082602798938751 Validation loss 0.052227556705474854 Accuracy 0.475830078125\n",
      "Iteration 2610 Training loss 0.04957563057541847 Validation loss 0.05271362140774727 Accuracy 0.471435546875\n",
      "Iteration 2620 Training loss 0.051852427423000336 Validation loss 0.05318666994571686 Accuracy 0.466552734375\n",
      "Iteration 2630 Training loss 0.05463340878486633 Validation loss 0.05344357341527939 Accuracy 0.4638671875\n",
      "Iteration 2640 Training loss 0.05177919566631317 Validation loss 0.052406493574380875 Accuracy 0.47412109375\n",
      "Iteration 2650 Training loss 0.05280063673853874 Validation loss 0.05284680053591728 Accuracy 0.469482421875\n",
      "Iteration 2660 Training loss 0.04738647863268852 Validation loss 0.05232418701052666 Accuracy 0.475341796875\n",
      "Iteration 2670 Training loss 0.053902916610240936 Validation loss 0.05274297669529915 Accuracy 0.470947265625\n",
      "Iteration 2680 Training loss 0.04761812835931778 Validation loss 0.05267032980918884 Accuracy 0.471923828125\n",
      "Iteration 2690 Training loss 0.055685896426439285 Validation loss 0.052480779588222504 Accuracy 0.4736328125\n",
      "Iteration 2700 Training loss 0.0498003214597702 Validation loss 0.0532866008579731 Accuracy 0.465087890625\n",
      "Iteration 2710 Training loss 0.04871957376599312 Validation loss 0.052355751395225525 Accuracy 0.474609375\n",
      "Iteration 2720 Training loss 0.051977843046188354 Validation loss 0.05291245877742767 Accuracy 0.469482421875\n",
      "Iteration 2730 Training loss 0.05121203139424324 Validation loss 0.052342869341373444 Accuracy 0.47509765625\n",
      "Iteration 2740 Training loss 0.051143575459718704 Validation loss 0.05225352570414543 Accuracy 0.47607421875\n",
      "Iteration 2750 Training loss 0.050720956176519394 Validation loss 0.05232321470975876 Accuracy 0.47509765625\n",
      "Iteration 2760 Training loss 0.04997842758893967 Validation loss 0.052562303841114044 Accuracy 0.472900390625\n",
      "Iteration 2770 Training loss 0.05055004730820656 Validation loss 0.05233100429177284 Accuracy 0.474609375\n",
      "Iteration 2780 Training loss 0.04903274402022362 Validation loss 0.0530959852039814 Accuracy 0.46728515625\n",
      "Iteration 2790 Training loss 0.050220564007759094 Validation loss 0.05262841656804085 Accuracy 0.4716796875\n",
      "Iteration 2800 Training loss 0.054209817200899124 Validation loss 0.052690278738737106 Accuracy 0.470947265625\n",
      "Iteration 2810 Training loss 0.053355686366558075 Validation loss 0.05330660939216614 Accuracy 0.464599609375\n",
      "Iteration 2820 Training loss 0.05140082910656929 Validation loss 0.05281586945056915 Accuracy 0.469970703125\n",
      "Iteration 2830 Training loss 0.05040191486477852 Validation loss 0.05249437317252159 Accuracy 0.47314453125\n",
      "Iteration 2840 Training loss 0.05168817937374115 Validation loss 0.053180839866399765 Accuracy 0.46533203125\n",
      "Iteration 2850 Training loss 0.050896309316158295 Validation loss 0.0522293820977211 Accuracy 0.4755859375\n",
      "Iteration 2860 Training loss 0.05173292011022568 Validation loss 0.052335452288389206 Accuracy 0.47509765625\n",
      "Iteration 2870 Training loss 0.05293027684092522 Validation loss 0.0532652884721756 Accuracy 0.46533203125\n",
      "Iteration 2880 Training loss 0.05334997549653053 Validation loss 0.05223708227276802 Accuracy 0.475341796875\n",
      "Iteration 2890 Training loss 0.053093209862709045 Validation loss 0.05378399416804314 Accuracy 0.460205078125\n",
      "Iteration 2900 Training loss 0.049372851848602295 Validation loss 0.05234698951244354 Accuracy 0.47509765625\n",
      "Iteration 2910 Training loss 0.05091119930148125 Validation loss 0.05252557620406151 Accuracy 0.47314453125\n",
      "Iteration 2920 Training loss 0.054960697889328 Validation loss 0.052737362682819366 Accuracy 0.470703125\n",
      "Iteration 2930 Training loss 0.050802528858184814 Validation loss 0.052552253007888794 Accuracy 0.472900390625\n",
      "Iteration 2940 Training loss 0.048759590834379196 Validation loss 0.052565526217222214 Accuracy 0.472412109375\n",
      "Iteration 2950 Training loss 0.05032557249069214 Validation loss 0.05245565250515938 Accuracy 0.4736328125\n",
      "Iteration 2960 Training loss 0.050954852253198624 Validation loss 0.05263062193989754 Accuracy 0.47216796875\n",
      "Iteration 2970 Training loss 0.0548858642578125 Validation loss 0.052690289914608 Accuracy 0.4716796875\n",
      "Iteration 2980 Training loss 0.052131880074739456 Validation loss 0.05303328111767769 Accuracy 0.467529296875\n",
      "Iteration 2990 Training loss 0.05387450009584427 Validation loss 0.05290208011865616 Accuracy 0.46826171875\n",
      "Iteration 3000 Training loss 0.05317768454551697 Validation loss 0.0528482049703598 Accuracy 0.469482421875\n",
      "Iteration 3010 Training loss 0.04922467842698097 Validation loss 0.0530085451900959 Accuracy 0.468017578125\n",
      "Iteration 3020 Training loss 0.053331516683101654 Validation loss 0.05257868021726608 Accuracy 0.472412109375\n",
      "Iteration 3030 Training loss 0.05222514271736145 Validation loss 0.05257522314786911 Accuracy 0.47216796875\n",
      "Iteration 3040 Training loss 0.051740437746047974 Validation loss 0.05238445848226547 Accuracy 0.474365234375\n",
      "Iteration 3050 Training loss 0.05128602683544159 Validation loss 0.052230965346097946 Accuracy 0.4755859375\n",
      "Iteration 3060 Training loss 0.05295958369970322 Validation loss 0.05256162956357002 Accuracy 0.47216796875\n",
      "Iteration 3070 Training loss 0.05083364248275757 Validation loss 0.05229732394218445 Accuracy 0.474609375\n",
      "Iteration 3080 Training loss 0.04969701170921326 Validation loss 0.05238230898976326 Accuracy 0.474365234375\n",
      "Iteration 3090 Training loss 0.05594172701239586 Validation loss 0.05265302583575249 Accuracy 0.471435546875\n",
      "Iteration 3100 Training loss 0.051495544612407684 Validation loss 0.052816689014434814 Accuracy 0.4697265625\n",
      "Iteration 3110 Training loss 0.05303110554814339 Validation loss 0.0522264763712883 Accuracy 0.475830078125\n",
      "Iteration 3120 Training loss 0.05329965800046921 Validation loss 0.05260586366057396 Accuracy 0.47216796875\n",
      "Iteration 3130 Training loss 0.05271967872977257 Validation loss 0.05243692174553871 Accuracy 0.473876953125\n",
      "Iteration 3140 Training loss 0.05152406170964241 Validation loss 0.05275534838438034 Accuracy 0.470703125\n",
      "Iteration 3150 Training loss 0.051565829664468765 Validation loss 0.05279918387532234 Accuracy 0.4697265625\n",
      "Iteration 3160 Training loss 0.05255408212542534 Validation loss 0.05275733396410942 Accuracy 0.470703125\n",
      "Iteration 3170 Training loss 0.05384072661399841 Validation loss 0.052212171256542206 Accuracy 0.475830078125\n",
      "Iteration 3180 Training loss 0.056877024471759796 Validation loss 0.05283225327730179 Accuracy 0.4697265625\n",
      "Iteration 3190 Training loss 0.05502608045935631 Validation loss 0.05224749445915222 Accuracy 0.47509765625\n",
      "Iteration 3200 Training loss 0.0493117980659008 Validation loss 0.05345897749066353 Accuracy 0.4638671875\n",
      "Iteration 3210 Training loss 0.049500420689582825 Validation loss 0.05266844481229782 Accuracy 0.4716796875\n",
      "Iteration 3220 Training loss 0.05363015830516815 Validation loss 0.052930641919374466 Accuracy 0.46875\n",
      "Iteration 3230 Training loss 0.053798858076334 Validation loss 0.05386285111308098 Accuracy 0.458984375\n",
      "Iteration 3240 Training loss 0.05198448523879051 Validation loss 0.05267194285988808 Accuracy 0.47119140625\n",
      "Iteration 3250 Training loss 0.05043194442987442 Validation loss 0.05329732224345207 Accuracy 0.46484375\n",
      "Iteration 3260 Training loss 0.05144491046667099 Validation loss 0.05235782265663147 Accuracy 0.474853515625\n",
      "Iteration 3270 Training loss 0.05392684042453766 Validation loss 0.052374694496393204 Accuracy 0.47412109375\n",
      "Iteration 3280 Training loss 0.053034279495477676 Validation loss 0.05320551618933678 Accuracy 0.4658203125\n",
      "Iteration 3290 Training loss 0.05040222778916359 Validation loss 0.052464451640844345 Accuracy 0.47314453125\n",
      "Iteration 3300 Training loss 0.054876577109098434 Validation loss 0.05339508503675461 Accuracy 0.4638671875\n",
      "Iteration 3310 Training loss 0.054373759776353836 Validation loss 0.053006190806627274 Accuracy 0.46826171875\n",
      "Iteration 3320 Training loss 0.05389973893761635 Validation loss 0.05305616557598114 Accuracy 0.46728515625\n",
      "Iteration 3330 Training loss 0.051322419196367264 Validation loss 0.0530644915997982 Accuracy 0.468017578125\n",
      "Iteration 3340 Training loss 0.05074862763285637 Validation loss 0.05303983762860298 Accuracy 0.46826171875\n",
      "Iteration 3350 Training loss 0.050346873700618744 Validation loss 0.05255279317498207 Accuracy 0.47314453125\n",
      "Iteration 3360 Training loss 0.054089684039354324 Validation loss 0.052369728684425354 Accuracy 0.474365234375\n",
      "Iteration 3370 Training loss 0.049804288893938065 Validation loss 0.05239500850439072 Accuracy 0.4736328125\n",
      "Iteration 3380 Training loss 0.05620565637946129 Validation loss 0.053427692502737045 Accuracy 0.463623046875\n",
      "Iteration 3390 Training loss 0.053084615617990494 Validation loss 0.052773650735616684 Accuracy 0.470458984375\n",
      "Iteration 3400 Training loss 0.05447709187865257 Validation loss 0.05260220170021057 Accuracy 0.471923828125\n",
      "Iteration 3410 Training loss 0.05359417200088501 Validation loss 0.05237409844994545 Accuracy 0.47412109375\n",
      "Iteration 3420 Training loss 0.051162056624889374 Validation loss 0.05249924585223198 Accuracy 0.472900390625\n",
      "Iteration 3430 Training loss 0.05108395218849182 Validation loss 0.05274789780378342 Accuracy 0.470703125\n",
      "Iteration 3440 Training loss 0.05037468299269676 Validation loss 0.05268159881234169 Accuracy 0.47119140625\n",
      "Iteration 3450 Training loss 0.05426948890089989 Validation loss 0.053053807467222214 Accuracy 0.4677734375\n",
      "Iteration 3460 Training loss 0.05354469642043114 Validation loss 0.053934089839458466 Accuracy 0.45849609375\n",
      "Iteration 3470 Training loss 0.049254436045885086 Validation loss 0.0524887777864933 Accuracy 0.472900390625\n",
      "Iteration 3480 Training loss 0.053320158272981644 Validation loss 0.05303836241364479 Accuracy 0.46826171875\n",
      "Iteration 3490 Training loss 0.05443444848060608 Validation loss 0.05299662798643112 Accuracy 0.468505859375\n",
      "Iteration 3500 Training loss 0.052879445254802704 Validation loss 0.052660636603832245 Accuracy 0.471923828125\n",
      "Iteration 3510 Training loss 0.05235559120774269 Validation loss 0.05337508022785187 Accuracy 0.46484375\n",
      "Iteration 3520 Training loss 0.05279829353094101 Validation loss 0.052740730345249176 Accuracy 0.470947265625\n",
      "Iteration 3530 Training loss 0.05194678530097008 Validation loss 0.052307985723018646 Accuracy 0.47509765625\n",
      "Iteration 3540 Training loss 0.053885553032159805 Validation loss 0.05246378108859062 Accuracy 0.473876953125\n",
      "Iteration 3550 Training loss 0.054264891892671585 Validation loss 0.05306060239672661 Accuracy 0.468017578125\n",
      "Iteration 3560 Training loss 0.058208536356687546 Validation loss 0.05263221636414528 Accuracy 0.47119140625\n",
      "Iteration 3570 Training loss 0.04868064448237419 Validation loss 0.0524446964263916 Accuracy 0.4736328125\n",
      "Iteration 3580 Training loss 0.05342140793800354 Validation loss 0.05259077996015549 Accuracy 0.472412109375\n",
      "Iteration 3590 Training loss 0.04829075187444687 Validation loss 0.05229586735367775 Accuracy 0.47509765625\n",
      "Iteration 3600 Training loss 0.04975193366408348 Validation loss 0.05293048545718193 Accuracy 0.46875\n",
      "Iteration 3610 Training loss 0.049884699285030365 Validation loss 0.05220838263630867 Accuracy 0.476318359375\n",
      "Iteration 3620 Training loss 0.05433036386966705 Validation loss 0.052780427038669586 Accuracy 0.470703125\n",
      "Iteration 3630 Training loss 0.05122696980834007 Validation loss 0.05224364250898361 Accuracy 0.47607421875\n",
      "Iteration 3640 Training loss 0.05024899169802666 Validation loss 0.052258290350437164 Accuracy 0.4755859375\n",
      "Iteration 3650 Training loss 0.05702334642410278 Validation loss 0.0522138848900795 Accuracy 0.476318359375\n",
      "Iteration 3660 Training loss 0.051733147352933884 Validation loss 0.053502488881349564 Accuracy 0.46337890625\n",
      "Iteration 3670 Training loss 0.052884552627801895 Validation loss 0.05244065448641777 Accuracy 0.473388671875\n",
      "Iteration 3680 Training loss 0.049875300377607346 Validation loss 0.05245856195688248 Accuracy 0.473388671875\n",
      "Iteration 3690 Training loss 0.051627419888973236 Validation loss 0.053135864436626434 Accuracy 0.46728515625\n",
      "Iteration 3700 Training loss 0.05161859467625618 Validation loss 0.05251341313123703 Accuracy 0.473388671875\n",
      "Iteration 3710 Training loss 0.053594086319208145 Validation loss 0.05252033472061157 Accuracy 0.473876953125\n",
      "Iteration 3720 Training loss 0.05199694260954857 Validation loss 0.05263514444231987 Accuracy 0.47265625\n",
      "Iteration 3730 Training loss 0.04706733301281929 Validation loss 0.0522393099963665 Accuracy 0.4765625\n",
      "Iteration 3740 Training loss 0.052229832857847214 Validation loss 0.05289814993739128 Accuracy 0.4697265625\n",
      "Iteration 3750 Training loss 0.049426671117544174 Validation loss 0.05238315463066101 Accuracy 0.475341796875\n",
      "Iteration 3760 Training loss 0.056761614978313446 Validation loss 0.05242294818162918 Accuracy 0.474853515625\n",
      "Iteration 3770 Training loss 0.04989326000213623 Validation loss 0.05261046811938286 Accuracy 0.472900390625\n",
      "Iteration 3780 Training loss 0.05397096648812294 Validation loss 0.05251970887184143 Accuracy 0.4736328125\n",
      "Iteration 3790 Training loss 0.054127756506204605 Validation loss 0.052550312131643295 Accuracy 0.473876953125\n",
      "Iteration 3800 Training loss 0.047720275819301605 Validation loss 0.0525037981569767 Accuracy 0.47412109375\n",
      "Iteration 3810 Training loss 0.05363541096448898 Validation loss 0.052461717277765274 Accuracy 0.474609375\n",
      "Iteration 3820 Training loss 0.052171122282743454 Validation loss 0.05267735570669174 Accuracy 0.47216796875\n",
      "Iteration 3830 Training loss 0.05279349535703659 Validation loss 0.05263159051537514 Accuracy 0.472900390625\n",
      "Iteration 3840 Training loss 0.05168649181723595 Validation loss 0.05259261652827263 Accuracy 0.472900390625\n",
      "Iteration 3850 Training loss 0.04886949062347412 Validation loss 0.052222151309251785 Accuracy 0.476318359375\n",
      "Iteration 3860 Training loss 0.05147862061858177 Validation loss 0.05254588648676872 Accuracy 0.47314453125\n",
      "Iteration 3870 Training loss 0.05615789815783501 Validation loss 0.052661631256341934 Accuracy 0.47216796875\n",
      "Iteration 3880 Training loss 0.052114713937044144 Validation loss 0.05231684446334839 Accuracy 0.4755859375\n",
      "Iteration 3890 Training loss 0.05264127627015114 Validation loss 0.052473943680524826 Accuracy 0.474365234375\n",
      "Iteration 3900 Training loss 0.049614209681749344 Validation loss 0.05303153023123741 Accuracy 0.46826171875\n",
      "Iteration 3910 Training loss 0.05377112329006195 Validation loss 0.0521867461502552 Accuracy 0.476806640625\n",
      "Iteration 3920 Training loss 0.05163165554404259 Validation loss 0.05249037966132164 Accuracy 0.473388671875\n",
      "Iteration 3930 Training loss 0.05216934531927109 Validation loss 0.05255685746669769 Accuracy 0.472900390625\n",
      "Iteration 3940 Training loss 0.053575821220874786 Validation loss 0.05286658555269241 Accuracy 0.469482421875\n",
      "Iteration 3950 Training loss 0.04938102141022682 Validation loss 0.052251044660806656 Accuracy 0.47607421875\n",
      "Iteration 3960 Training loss 0.054181456565856934 Validation loss 0.05232998728752136 Accuracy 0.47509765625\n",
      "Iteration 3970 Training loss 0.05055451765656471 Validation loss 0.05232861638069153 Accuracy 0.4755859375\n",
      "Iteration 3980 Training loss 0.05225067958235741 Validation loss 0.05224299803376198 Accuracy 0.476318359375\n",
      "Iteration 3990 Training loss 0.05181972309947014 Validation loss 0.05212783068418503 Accuracy 0.47705078125\n",
      "Iteration 4000 Training loss 0.05330002307891846 Validation loss 0.05328892543911934 Accuracy 0.46435546875\n",
      "Iteration 4010 Training loss 0.048815712332725525 Validation loss 0.05237578973174095 Accuracy 0.473876953125\n",
      "Iteration 4020 Training loss 0.04835674539208412 Validation loss 0.05226165056228638 Accuracy 0.4755859375\n",
      "Iteration 4030 Training loss 0.04870226979255676 Validation loss 0.05217602849006653 Accuracy 0.475830078125\n",
      "Iteration 4040 Training loss 0.051784172654151917 Validation loss 0.052663784474134445 Accuracy 0.47216796875\n",
      "Iteration 4050 Training loss 0.05505567416548729 Validation loss 0.05237646400928497 Accuracy 0.474853515625\n",
      "Iteration 4060 Training loss 0.05016496405005455 Validation loss 0.05221547186374664 Accuracy 0.47607421875\n",
      "Iteration 4070 Training loss 0.05148683488368988 Validation loss 0.05244772508740425 Accuracy 0.472900390625\n",
      "Iteration 4080 Training loss 0.05147567763924599 Validation loss 0.052244022488594055 Accuracy 0.47509765625\n",
      "Iteration 4090 Training loss 0.05201388895511627 Validation loss 0.05321004241704941 Accuracy 0.465576171875\n",
      "Iteration 4100 Training loss 0.04987362027168274 Validation loss 0.05210678651928902 Accuracy 0.476806640625\n",
      "Iteration 4110 Training loss 0.05641862004995346 Validation loss 0.0522645004093647 Accuracy 0.475830078125\n",
      "Iteration 4120 Training loss 0.05213961377739906 Validation loss 0.05243232473731041 Accuracy 0.473388671875\n",
      "Iteration 4130 Training loss 0.05403876677155495 Validation loss 0.05260580778121948 Accuracy 0.472412109375\n",
      "Iteration 4140 Training loss 0.05145364999771118 Validation loss 0.052295174449682236 Accuracy 0.4755859375\n",
      "Iteration 4150 Training loss 0.056660495698451996 Validation loss 0.052236102521419525 Accuracy 0.47607421875\n",
      "Iteration 4160 Training loss 0.04937798157334328 Validation loss 0.052111923694610596 Accuracy 0.476806640625\n",
      "Iteration 4170 Training loss 0.05152731388807297 Validation loss 0.05198820307850838 Accuracy 0.477783203125\n",
      "Iteration 4180 Training loss 0.049043573439121246 Validation loss 0.05234445258975029 Accuracy 0.474609375\n",
      "Iteration 4190 Training loss 0.05033496022224426 Validation loss 0.05042329430580139 Accuracy 0.493408203125\n",
      "Iteration 4200 Training loss 0.05250072851777077 Validation loss 0.05143316090106964 Accuracy 0.484375\n",
      "Iteration 4210 Training loss 0.04706805199384689 Validation loss 0.04542882740497589 Accuracy 0.54443359375\n",
      "Iteration 4220 Training loss 0.04273677617311478 Validation loss 0.04505117982625961 Accuracy 0.5478515625\n",
      "Iteration 4230 Training loss 0.042377080768346786 Validation loss 0.0444059856235981 Accuracy 0.55517578125\n",
      "Iteration 4240 Training loss 0.04319149628281593 Validation loss 0.04454720765352249 Accuracy 0.5537109375\n",
      "Iteration 4250 Training loss 0.04358481988310814 Validation loss 0.04443195089697838 Accuracy 0.55419921875\n",
      "Iteration 4260 Training loss 0.043299585580825806 Validation loss 0.0447927862405777 Accuracy 0.55029296875\n",
      "Iteration 4270 Training loss 0.042680006474256516 Validation loss 0.04392199218273163 Accuracy 0.55908203125\n",
      "Iteration 4280 Training loss 0.04537056386470795 Validation loss 0.04399711638689041 Accuracy 0.55859375\n",
      "Iteration 4290 Training loss 0.04607497528195381 Validation loss 0.044179607182741165 Accuracy 0.556640625\n",
      "Iteration 4300 Training loss 0.04336708411574364 Validation loss 0.04411714896559715 Accuracy 0.55712890625\n",
      "Iteration 4310 Training loss 0.04681112617254257 Validation loss 0.043923456221818924 Accuracy 0.55908203125\n",
      "Iteration 4320 Training loss 0.04326446354389191 Validation loss 0.04501727595925331 Accuracy 0.54736328125\n",
      "Iteration 4330 Training loss 0.043256111443042755 Validation loss 0.04388764873147011 Accuracy 0.5595703125\n",
      "Iteration 4340 Training loss 0.04187177121639252 Validation loss 0.04425156116485596 Accuracy 0.55615234375\n",
      "Iteration 4350 Training loss 0.044682927429676056 Validation loss 0.04477396234869957 Accuracy 0.55078125\n",
      "Iteration 4360 Training loss 0.04283686354756355 Validation loss 0.044917311519384384 Accuracy 0.548828125\n",
      "Iteration 4370 Training loss 0.04638366773724556 Validation loss 0.04588643088936806 Accuracy 0.5390625\n",
      "Iteration 4380 Training loss 0.042555924504995346 Validation loss 0.04383930563926697 Accuracy 0.56005859375\n",
      "Iteration 4390 Training loss 0.04494771361351013 Validation loss 0.043711502104997635 Accuracy 0.56103515625\n",
      "Iteration 4400 Training loss 0.0447433777153492 Validation loss 0.04660586640238762 Accuracy 0.5322265625\n",
      "Iteration 4410 Training loss 0.04453864321112633 Validation loss 0.04418963938951492 Accuracy 0.556640625\n",
      "Iteration 4420 Training loss 0.04259737581014633 Validation loss 0.043566830456256866 Accuracy 0.56298828125\n",
      "Iteration 4430 Training loss 0.04531313106417656 Validation loss 0.04354853555560112 Accuracy 0.56298828125\n",
      "Iteration 4440 Training loss 0.046166855841875076 Validation loss 0.04575997591018677 Accuracy 0.541015625\n",
      "Iteration 4450 Training loss 0.04056817665696144 Validation loss 0.04385727271437645 Accuracy 0.56005859375\n",
      "Iteration 4460 Training loss 0.04265114665031433 Validation loss 0.04376394301652908 Accuracy 0.560546875\n",
      "Iteration 4470 Training loss 0.04406435415148735 Validation loss 0.04383229464292526 Accuracy 0.56005859375\n",
      "Iteration 4480 Training loss 0.044267721474170685 Validation loss 0.044347625225782394 Accuracy 0.5546875\n",
      "Iteration 4490 Training loss 0.044102802872657776 Validation loss 0.04497073218226433 Accuracy 0.54833984375\n",
      "Iteration 4500 Training loss 0.04295508936047554 Validation loss 0.043633006513118744 Accuracy 0.5615234375\n",
      "Iteration 4510 Training loss 0.0422692596912384 Validation loss 0.043713416904211044 Accuracy 0.560546875\n",
      "Iteration 4520 Training loss 0.0402517169713974 Validation loss 0.0435844324529171 Accuracy 0.56298828125\n",
      "Iteration 4530 Training loss 0.041738465428352356 Validation loss 0.043869324028491974 Accuracy 0.5595703125\n",
      "Iteration 4540 Training loss 0.041290659457445145 Validation loss 0.04364457726478577 Accuracy 0.5625\n",
      "Iteration 4550 Training loss 0.0470006950199604 Validation loss 0.043543603271245956 Accuracy 0.56298828125\n",
      "Iteration 4560 Training loss 0.044648732990026474 Validation loss 0.04352429881691933 Accuracy 0.56396484375\n",
      "Iteration 4570 Training loss 0.044791869819164276 Validation loss 0.04382631182670593 Accuracy 0.56005859375\n",
      "Iteration 4580 Training loss 0.04023701325058937 Validation loss 0.043460339307785034 Accuracy 0.56396484375\n",
      "Iteration 4590 Training loss 0.04441139101982117 Validation loss 0.0438896082341671 Accuracy 0.55908203125\n",
      "Iteration 4600 Training loss 0.04475000500679016 Validation loss 0.043653152883052826 Accuracy 0.5625\n",
      "Iteration 4610 Training loss 0.045493677258491516 Validation loss 0.043653786182403564 Accuracy 0.56201171875\n",
      "Iteration 4620 Training loss 0.047206614166498184 Validation loss 0.04406554251909256 Accuracy 0.5576171875\n",
      "Iteration 4630 Training loss 0.04310586676001549 Validation loss 0.043809499591588974 Accuracy 0.560546875\n",
      "Iteration 4640 Training loss 0.04265974834561348 Validation loss 0.04329478368163109 Accuracy 0.5654296875\n",
      "Iteration 4650 Training loss 0.0449715256690979 Validation loss 0.04363056644797325 Accuracy 0.5625\n",
      "Iteration 4660 Training loss 0.041766539216041565 Validation loss 0.04332198202610016 Accuracy 0.5654296875\n",
      "Iteration 4670 Training loss 0.04442434757947922 Validation loss 0.04360392317175865 Accuracy 0.5625\n",
      "Iteration 4680 Training loss 0.04421553388237953 Validation loss 0.04353661462664604 Accuracy 0.5634765625\n",
      "Iteration 4690 Training loss 0.0437065027654171 Validation loss 0.04401277378201485 Accuracy 0.55859375\n",
      "Iteration 4700 Training loss 0.04526713117957115 Validation loss 0.04533853381872177 Accuracy 0.544921875\n",
      "Iteration 4710 Training loss 0.04374115914106369 Validation loss 0.04543004930019379 Accuracy 0.54345703125\n",
      "Iteration 4720 Training loss 0.04486660659313202 Validation loss 0.04392671585083008 Accuracy 0.5595703125\n",
      "Iteration 4730 Training loss 0.04671110957860947 Validation loss 0.044264934957027435 Accuracy 0.5556640625\n",
      "Iteration 4740 Training loss 0.042710673063993454 Validation loss 0.04363437369465828 Accuracy 0.5625\n",
      "Iteration 4750 Training loss 0.039412714540958405 Validation loss 0.04362088814377785 Accuracy 0.5625\n",
      "Iteration 4760 Training loss 0.04338572546839714 Validation loss 0.043384432792663574 Accuracy 0.56494140625\n",
      "Iteration 4770 Training loss 0.04357030242681503 Validation loss 0.04334121569991112 Accuracy 0.5654296875\n",
      "Iteration 4780 Training loss 0.04342098906636238 Validation loss 0.04327540844678879 Accuracy 0.56640625\n",
      "Iteration 4790 Training loss 0.044653575867414474 Validation loss 0.04329084977507591 Accuracy 0.5654296875\n",
      "Iteration 4800 Training loss 0.04255262017250061 Validation loss 0.04330853372812271 Accuracy 0.56494140625\n",
      "Iteration 4810 Training loss 0.04327860474586487 Validation loss 0.043582215905189514 Accuracy 0.5625\n",
      "Iteration 4820 Training loss 0.04345998913049698 Validation loss 0.04343311861157417 Accuracy 0.56396484375\n",
      "Iteration 4830 Training loss 0.04262293875217438 Validation loss 0.04344356432557106 Accuracy 0.56396484375\n",
      "Iteration 4840 Training loss 0.04431948438286781 Validation loss 0.04374313727021217 Accuracy 0.560546875\n",
      "Iteration 4850 Training loss 0.04048838093876839 Validation loss 0.04436545819044113 Accuracy 0.5546875\n",
      "Iteration 4860 Training loss 0.04240311309695244 Validation loss 0.043770644813776016 Accuracy 0.560546875\n",
      "Iteration 4870 Training loss 0.04351649433374405 Validation loss 0.04346920922398567 Accuracy 0.5634765625\n",
      "Iteration 4880 Training loss 0.04674628749489784 Validation loss 0.045182254165410995 Accuracy 0.544921875\n",
      "Iteration 4890 Training loss 0.045078713446855545 Validation loss 0.04334579035639763 Accuracy 0.5654296875\n",
      "Iteration 4900 Training loss 0.041389595717191696 Validation loss 0.044060662388801575 Accuracy 0.55810546875\n",
      "Iteration 4910 Training loss 0.041841816157102585 Validation loss 0.043309323489665985 Accuracy 0.5654296875\n",
      "Iteration 4920 Training loss 0.0472390241920948 Validation loss 0.04468248412013054 Accuracy 0.55126953125\n",
      "Iteration 4930 Training loss 0.04420042410492897 Validation loss 0.04356173053383827 Accuracy 0.56298828125\n",
      "Iteration 4940 Training loss 0.04840785264968872 Validation loss 0.04341895878314972 Accuracy 0.56396484375\n",
      "Iteration 4950 Training loss 0.03871358558535576 Validation loss 0.04326742887496948 Accuracy 0.56640625\n",
      "Iteration 4960 Training loss 0.04497191309928894 Validation loss 0.04366286098957062 Accuracy 0.5625\n",
      "Iteration 4970 Training loss 0.0430251881480217 Validation loss 0.04344736039638519 Accuracy 0.56396484375\n",
      "Iteration 4980 Training loss 0.04363798350095749 Validation loss 0.04331493377685547 Accuracy 0.56591796875\n",
      "Iteration 4990 Training loss 0.04238130897283554 Validation loss 0.04327001795172691 Accuracy 0.56591796875\n",
      "Iteration 5000 Training loss 0.03991016000509262 Validation loss 0.04328897222876549 Accuracy 0.5654296875\n",
      "Iteration 5010 Training loss 0.042485110461711884 Validation loss 0.043716952204704285 Accuracy 0.56103515625\n",
      "Iteration 5020 Training loss 0.04536684229969978 Validation loss 0.044334717094898224 Accuracy 0.5546875\n",
      "Iteration 5030 Training loss 0.045024480670690536 Validation loss 0.04527681693434715 Accuracy 0.544921875\n",
      "Iteration 5040 Training loss 0.04018640145659447 Validation loss 0.04346586763858795 Accuracy 0.56396484375\n",
      "Iteration 5050 Training loss 0.04221215844154358 Validation loss 0.04341793805360794 Accuracy 0.564453125\n",
      "Iteration 5060 Training loss 0.04355571046471596 Validation loss 0.044133320450782776 Accuracy 0.556640625\n",
      "Iteration 5070 Training loss 0.045317087322473526 Validation loss 0.04381389543414116 Accuracy 0.56005859375\n",
      "Iteration 5080 Training loss 0.040961846709251404 Validation loss 0.04369590803980827 Accuracy 0.5615234375\n",
      "Iteration 5090 Training loss 0.0442022942006588 Validation loss 0.04427643492817879 Accuracy 0.5556640625\n",
      "Iteration 5100 Training loss 0.04287683591246605 Validation loss 0.043281570076942444 Accuracy 0.5654296875\n",
      "Iteration 5110 Training loss 0.043643511831760406 Validation loss 0.043364666402339935 Accuracy 0.56494140625\n",
      "Iteration 5120 Training loss 0.04488035663962364 Validation loss 0.04317197948694229 Accuracy 0.56640625\n",
      "Iteration 5130 Training loss 0.04278538003563881 Validation loss 0.0437738262116909 Accuracy 0.56103515625\n",
      "Iteration 5140 Training loss 0.04343992471694946 Validation loss 0.043152838945388794 Accuracy 0.56689453125\n",
      "Iteration 5150 Training loss 0.044608570635318756 Validation loss 0.04387839883565903 Accuracy 0.5595703125\n",
      "Iteration 5160 Training loss 0.04378260672092438 Validation loss 0.04320918396115303 Accuracy 0.56591796875\n",
      "Iteration 5170 Training loss 0.03996406868100166 Validation loss 0.03964250534772873 Accuracy 0.599609375\n",
      "Iteration 5180 Training loss 0.03861381486058235 Validation loss 0.038222480565309525 Accuracy 0.615234375\n",
      "Iteration 5190 Training loss 0.035123493522405624 Validation loss 0.03739316388964653 Accuracy 0.623046875\n",
      "Iteration 5200 Training loss 0.03378152847290039 Validation loss 0.03730449452996254 Accuracy 0.62451171875\n",
      "Iteration 5210 Training loss 0.036743711680173874 Validation loss 0.036986906081438065 Accuracy 0.6279296875\n",
      "Iteration 5220 Training loss 0.03538276627659798 Validation loss 0.03652060031890869 Accuracy 0.6328125\n",
      "Iteration 5230 Training loss 0.0347631610929966 Validation loss 0.03616771847009659 Accuracy 0.63623046875\n",
      "Iteration 5240 Training loss 0.040200334042310715 Validation loss 0.038020115345716476 Accuracy 0.61767578125\n",
      "Iteration 5250 Training loss 0.03624705225229263 Validation loss 0.03564094752073288 Accuracy 0.64111328125\n",
      "Iteration 5260 Training loss 0.03260535001754761 Validation loss 0.03597043454647064 Accuracy 0.63818359375\n",
      "Iteration 5270 Training loss 0.036956485360860825 Validation loss 0.03695914149284363 Accuracy 0.6279296875\n",
      "Iteration 5280 Training loss 0.03619584068655968 Validation loss 0.035589754581451416 Accuracy 0.64208984375\n",
      "Iteration 5290 Training loss 0.03612149879336357 Validation loss 0.03850620239973068 Accuracy 0.61279296875\n",
      "Iteration 5300 Training loss 0.03770225867629051 Validation loss 0.035733338445425034 Accuracy 0.640625\n",
      "Iteration 5310 Training loss 0.03229586407542229 Validation loss 0.03521337732672691 Accuracy 0.64599609375\n",
      "Iteration 5320 Training loss 0.04161641001701355 Validation loss 0.041060104966163635 Accuracy 0.5869140625\n",
      "Iteration 5330 Training loss 0.033713843673467636 Validation loss 0.035407982766628265 Accuracy 0.64404296875\n",
      "Iteration 5340 Training loss 0.03166784718632698 Validation loss 0.03572783246636391 Accuracy 0.640625\n",
      "Iteration 5350 Training loss 0.03513491153717041 Validation loss 0.035304296761751175 Accuracy 0.64453125\n",
      "Iteration 5360 Training loss 0.03538081794977188 Validation loss 0.03534946218132973 Accuracy 0.64404296875\n",
      "Iteration 5370 Training loss 0.032435692846775055 Validation loss 0.034066472202539444 Accuracy 0.65625\n",
      "Iteration 5380 Training loss 0.032459694892168045 Validation loss 0.03275064006447792 Accuracy 0.669921875\n",
      "Iteration 5390 Training loss 0.03081424906849861 Validation loss 0.03159173205494881 Accuracy 0.68115234375\n",
      "Iteration 5400 Training loss 0.030113449320197105 Validation loss 0.033132072538137436 Accuracy 0.66650390625\n",
      "Iteration 5410 Training loss 0.029632896184921265 Validation loss 0.031371284276247025 Accuracy 0.68359375\n",
      "Iteration 5420 Training loss 0.03441539779305458 Validation loss 0.035296984016895294 Accuracy 0.64501953125\n",
      "Iteration 5430 Training loss 0.029212431982159615 Validation loss 0.031517159193754196 Accuracy 0.6826171875\n",
      "Iteration 5440 Training loss 0.031161420047283173 Validation loss 0.031588904559612274 Accuracy 0.681640625\n",
      "Iteration 5450 Training loss 0.032424744218587875 Validation loss 0.032541800290346146 Accuracy 0.67236328125\n",
      "Iteration 5460 Training loss 0.03315490856766701 Validation loss 0.031030062586069107 Accuracy 0.6875\n",
      "Iteration 5470 Training loss 0.02969450317323208 Validation loss 0.03132714703679085 Accuracy 0.6845703125\n",
      "Iteration 5480 Training loss 0.02918252721428871 Validation loss 0.031308747828006744 Accuracy 0.68505859375\n",
      "Iteration 5490 Training loss 0.028338108211755753 Validation loss 0.030216118320822716 Accuracy 0.6953125\n",
      "Iteration 5500 Training loss 0.033081889152526855 Validation loss 0.03213661164045334 Accuracy 0.67626953125\n",
      "Iteration 5510 Training loss 0.030517099425196648 Validation loss 0.031178729608654976 Accuracy 0.68603515625\n",
      "Iteration 5520 Training loss 0.02994033694267273 Validation loss 0.030633583664894104 Accuracy 0.69140625\n",
      "Iteration 5530 Training loss 0.03342561796307564 Validation loss 0.0311990138143301 Accuracy 0.68603515625\n",
      "Iteration 5540 Training loss 0.030101198703050613 Validation loss 0.030380062758922577 Accuracy 0.6943359375\n",
      "Iteration 5550 Training loss 0.028627360239624977 Validation loss 0.03169320151209831 Accuracy 0.68115234375\n",
      "Iteration 5560 Training loss 0.027586888521909714 Validation loss 0.030479345470666885 Accuracy 0.6923828125\n",
      "Iteration 5570 Training loss 0.030568547546863556 Validation loss 0.033965274691581726 Accuracy 0.65771484375\n",
      "Iteration 5580 Training loss 0.031124135479331017 Validation loss 0.03193320333957672 Accuracy 0.677734375\n",
      "Iteration 5590 Training loss 0.03138347715139389 Validation loss 0.03289249166846275 Accuracy 0.6689453125\n",
      "Iteration 5600 Training loss 0.03201989084482193 Validation loss 0.032956577837467194 Accuracy 0.66845703125\n",
      "Iteration 5610 Training loss 0.030271122232079506 Validation loss 0.030269676819443703 Accuracy 0.69482421875\n",
      "Iteration 5620 Training loss 0.02761850506067276 Validation loss 0.029730770736932755 Accuracy 0.7001953125\n",
      "Iteration 5630 Training loss 0.027118850499391556 Validation loss 0.030774276703596115 Accuracy 0.68994140625\n",
      "Iteration 5640 Training loss 0.02988480031490326 Validation loss 0.031424589455127716 Accuracy 0.68408203125\n",
      "Iteration 5650 Training loss 0.029065633192658424 Validation loss 0.02984905242919922 Accuracy 0.69921875\n",
      "Iteration 5660 Training loss 0.03231477737426758 Validation loss 0.03337204083800316 Accuracy 0.6650390625\n",
      "Iteration 5670 Training loss 0.029877400025725365 Validation loss 0.03164228796958923 Accuracy 0.68115234375\n",
      "Iteration 5680 Training loss 0.030339539051055908 Validation loss 0.03110457956790924 Accuracy 0.68701171875\n",
      "Iteration 5690 Training loss 0.026935426518321037 Validation loss 0.03046507202088833 Accuracy 0.69287109375\n",
      "Iteration 5700 Training loss 0.025684218853712082 Validation loss 0.029875949025154114 Accuracy 0.69921875\n",
      "Iteration 5710 Training loss 0.02765307016670704 Validation loss 0.03093564882874489 Accuracy 0.68798828125\n",
      "Iteration 5720 Training loss 0.028210600838065147 Validation loss 0.029490824788808823 Accuracy 0.7021484375\n",
      "Iteration 5730 Training loss 0.027933385223150253 Validation loss 0.029745975509285927 Accuracy 0.7001953125\n",
      "Iteration 5740 Training loss 0.02822774648666382 Validation loss 0.030224686488509178 Accuracy 0.69580078125\n",
      "Iteration 5750 Training loss 0.031755976378917694 Validation loss 0.03037504479289055 Accuracy 0.6943359375\n",
      "Iteration 5760 Training loss 0.02709871716797352 Validation loss 0.03118763118982315 Accuracy 0.68603515625\n",
      "Iteration 5770 Training loss 0.027088643983006477 Validation loss 0.030029017478227615 Accuracy 0.697265625\n",
      "Iteration 5780 Training loss 0.030566787347197533 Validation loss 0.02937517687678337 Accuracy 0.7041015625\n",
      "Iteration 5790 Training loss 0.0274215005338192 Validation loss 0.029769375920295715 Accuracy 0.7001953125\n",
      "Iteration 5800 Training loss 0.028108321130275726 Validation loss 0.029814183712005615 Accuracy 0.69970703125\n",
      "Iteration 5810 Training loss 0.028747376054525375 Validation loss 0.02991291508078575 Accuracy 0.69873046875\n",
      "Iteration 5820 Training loss 0.02908317558467388 Validation loss 0.030977828428149223 Accuracy 0.68798828125\n",
      "Iteration 5830 Training loss 0.031171897426247597 Validation loss 0.03319395333528519 Accuracy 0.66552734375\n",
      "Iteration 5840 Training loss 0.025865374132990837 Validation loss 0.030195502564311028 Accuracy 0.69580078125\n",
      "Iteration 5850 Training loss 0.03057810850441456 Validation loss 0.031529366970062256 Accuracy 0.6826171875\n",
      "Iteration 5860 Training loss 0.02931949868798256 Validation loss 0.03029685653746128 Accuracy 0.69482421875\n",
      "Iteration 5870 Training loss 0.027554700151085854 Validation loss 0.029152341187000275 Accuracy 0.7060546875\n",
      "Iteration 5880 Training loss 0.02569834142923355 Validation loss 0.029392065480351448 Accuracy 0.7041015625\n",
      "Iteration 5890 Training loss 0.02704237401485443 Validation loss 0.029279928654432297 Accuracy 0.7041015625\n",
      "Iteration 5900 Training loss 0.03305990621447563 Validation loss 0.03202502056956291 Accuracy 0.6767578125\n",
      "Iteration 5910 Training loss 0.03463262692093849 Validation loss 0.03444438800215721 Accuracy 0.65380859375\n",
      "Iteration 5920 Training loss 0.027531152591109276 Validation loss 0.03058033622801304 Accuracy 0.69189453125\n",
      "Iteration 5930 Training loss 0.027945388108491898 Validation loss 0.03028225526213646 Accuracy 0.69482421875\n",
      "Iteration 5940 Training loss 0.028305938467383385 Validation loss 0.029235346242785454 Accuracy 0.70556640625\n",
      "Iteration 5950 Training loss 0.02764134481549263 Validation loss 0.030581824481487274 Accuracy 0.69189453125\n",
      "Iteration 5960 Training loss 0.029977360740303993 Validation loss 0.029348060488700867 Accuracy 0.70458984375\n",
      "Iteration 5970 Training loss 0.02695121243596077 Validation loss 0.029319828376173973 Accuracy 0.70458984375\n",
      "Iteration 5980 Training loss 0.029284674674272537 Validation loss 0.029554303735494614 Accuracy 0.70166015625\n",
      "Iteration 5990 Training loss 0.03354443237185478 Validation loss 0.03020225465297699 Accuracy 0.6953125\n",
      "Iteration 6000 Training loss 0.029872052371501923 Validation loss 0.031857676804065704 Accuracy 0.67919921875\n",
      "Iteration 6010 Training loss 0.02645936608314514 Validation loss 0.029362719506025314 Accuracy 0.70361328125\n",
      "Iteration 6020 Training loss 0.02788546122610569 Validation loss 0.029353730380535126 Accuracy 0.70458984375\n",
      "Iteration 6030 Training loss 0.029720911756157875 Validation loss 0.029174931347370148 Accuracy 0.70654296875\n",
      "Iteration 6040 Training loss 0.02876649983227253 Validation loss 0.030271992087364197 Accuracy 0.6953125\n",
      "Iteration 6050 Training loss 0.02837340347468853 Validation loss 0.029276015236973763 Accuracy 0.705078125\n",
      "Iteration 6060 Training loss 0.028085054829716682 Validation loss 0.03001447394490242 Accuracy 0.697265625\n",
      "Iteration 6070 Training loss 0.030733658000826836 Validation loss 0.02939504384994507 Accuracy 0.7041015625\n",
      "Iteration 6080 Training loss 0.03286033123731613 Validation loss 0.03045344166457653 Accuracy 0.693359375\n",
      "Iteration 6090 Training loss 0.03018512949347496 Validation loss 0.0303141251206398 Accuracy 0.6943359375\n",
      "Iteration 6100 Training loss 0.028128396719694138 Validation loss 0.028978876769542694 Accuracy 0.7080078125\n",
      "Iteration 6110 Training loss 0.028606262058019638 Validation loss 0.029330609366297722 Accuracy 0.70458984375\n",
      "Iteration 6120 Training loss 0.029448281973600388 Validation loss 0.029142936691641808 Accuracy 0.70654296875\n",
      "Iteration 6130 Training loss 0.028671590611338615 Validation loss 0.030046304687857628 Accuracy 0.69775390625\n",
      "Iteration 6140 Training loss 0.028472762554883957 Validation loss 0.03007822483778 Accuracy 0.697265625\n",
      "Iteration 6150 Training loss 0.02803625725209713 Validation loss 0.030025726184248924 Accuracy 0.69775390625\n",
      "Iteration 6160 Training loss 0.027908094227313995 Validation loss 0.028956321999430656 Accuracy 0.7080078125\n",
      "Iteration 6170 Training loss 0.030728913843631744 Validation loss 0.029541702941060066 Accuracy 0.70263671875\n",
      "Iteration 6180 Training loss 0.02727344259619713 Validation loss 0.029067641124129295 Accuracy 0.70751953125\n",
      "Iteration 6190 Training loss 0.027106378227472305 Validation loss 0.029573773965239525 Accuracy 0.70263671875\n",
      "Iteration 6200 Training loss 0.027722157537937164 Validation loss 0.030487824231386185 Accuracy 0.69287109375\n",
      "Iteration 6210 Training loss 0.028722023591399193 Validation loss 0.028817279264330864 Accuracy 0.7099609375\n",
      "Iteration 6220 Training loss 0.029303640127182007 Validation loss 0.029856044799089432 Accuracy 0.69921875\n",
      "Iteration 6230 Training loss 0.02677162177860737 Validation loss 0.029560882598161697 Accuracy 0.70263671875\n",
      "Iteration 6240 Training loss 0.025818228721618652 Validation loss 0.02910655178129673 Accuracy 0.70703125\n",
      "Iteration 6250 Training loss 0.025459863245487213 Validation loss 0.030308956280350685 Accuracy 0.69482421875\n",
      "Iteration 6260 Training loss 0.02930688112974167 Validation loss 0.031957536935806274 Accuracy 0.677734375\n",
      "Iteration 6270 Training loss 0.027306945994496346 Validation loss 0.029355086386203766 Accuracy 0.7041015625\n",
      "Iteration 6280 Training loss 0.028055451810359955 Validation loss 0.02916555106639862 Accuracy 0.7060546875\n",
      "Iteration 6290 Training loss 0.027095016092061996 Validation loss 0.029538631439208984 Accuracy 0.70263671875\n",
      "Iteration 6300 Training loss 0.02874065563082695 Validation loss 0.029331376776099205 Accuracy 0.70458984375\n",
      "Iteration 6310 Training loss 0.030240880325436592 Validation loss 0.03139384463429451 Accuracy 0.68359375\n",
      "Iteration 6320 Training loss 0.026661783456802368 Validation loss 0.029170570895075798 Accuracy 0.7060546875\n",
      "Iteration 6330 Training loss 0.027593668550252914 Validation loss 0.029604213312268257 Accuracy 0.7021484375\n",
      "Iteration 6340 Training loss 0.028099512681365013 Validation loss 0.03030102699995041 Accuracy 0.69482421875\n",
      "Iteration 6350 Training loss 0.03165339678525925 Validation loss 0.03040960803627968 Accuracy 0.693359375\n",
      "Iteration 6360 Training loss 0.027810048311948776 Validation loss 0.028915496543049812 Accuracy 0.70849609375\n",
      "Iteration 6370 Training loss 0.02605513483285904 Validation loss 0.02921363152563572 Accuracy 0.7060546875\n",
      "Iteration 6380 Training loss 0.030674656853079796 Validation loss 0.028578678146004677 Accuracy 0.71240234375\n",
      "Iteration 6390 Training loss 0.02997947670519352 Validation loss 0.02866271138191223 Accuracy 0.71142578125\n",
      "Iteration 6400 Training loss 0.024509327486157417 Validation loss 0.028683319687843323 Accuracy 0.71142578125\n",
      "Iteration 6410 Training loss 0.027182575315237045 Validation loss 0.0290411077439785 Accuracy 0.70751953125\n",
      "Iteration 6420 Training loss 0.026624521240592003 Validation loss 0.02910878323018551 Accuracy 0.7060546875\n",
      "Iteration 6430 Training loss 0.024012433364987373 Validation loss 0.028413672000169754 Accuracy 0.71337890625\n",
      "Iteration 6440 Training loss 0.02721422351896763 Validation loss 0.029243847355246544 Accuracy 0.70556640625\n",
      "Iteration 6450 Training loss 0.027122672647237778 Validation loss 0.029089758172631264 Accuracy 0.70703125\n",
      "Iteration 6460 Training loss 0.02850981056690216 Validation loss 0.029302658513188362 Accuracy 0.705078125\n",
      "Iteration 6470 Training loss 0.02833673544228077 Validation loss 0.02990490198135376 Accuracy 0.69873046875\n",
      "Iteration 6480 Training loss 0.030249664559960365 Validation loss 0.030684934929013252 Accuracy 0.69091796875\n",
      "Iteration 6490 Training loss 0.02539544180035591 Validation loss 0.02850322239100933 Accuracy 0.712890625\n",
      "Iteration 6500 Training loss 0.030270304530858994 Validation loss 0.029786037281155586 Accuracy 0.69970703125\n",
      "Iteration 6510 Training loss 0.026794003322720528 Validation loss 0.02951548807322979 Accuracy 0.703125\n",
      "Iteration 6520 Training loss 0.0284931231290102 Validation loss 0.030188165605068207 Accuracy 0.6962890625\n",
      "Iteration 6530 Training loss 0.023136353120207787 Validation loss 0.026036644354462624 Accuracy 0.7373046875\n",
      "Iteration 6540 Training loss 0.01926654577255249 Validation loss 0.021154334768652916 Accuracy 0.7861328125\n",
      "Iteration 6550 Training loss 0.01842181384563446 Validation loss 0.02142918109893799 Accuracy 0.783203125\n",
      "Iteration 6560 Training loss 0.01897898130118847 Validation loss 0.02123316563665867 Accuracy 0.78564453125\n",
      "Iteration 6570 Training loss 0.02015092223882675 Validation loss 0.020649565383791924 Accuracy 0.7919921875\n",
      "Iteration 6580 Training loss 0.01948888972401619 Validation loss 0.02090197242796421 Accuracy 0.7890625\n",
      "Iteration 6590 Training loss 0.02061338722705841 Validation loss 0.02366972155869007 Accuracy 0.7607421875\n",
      "Iteration 6600 Training loss 0.019933752715587616 Validation loss 0.020884335041046143 Accuracy 0.7890625\n",
      "Iteration 6610 Training loss 0.02049664594233036 Validation loss 0.021725760772824287 Accuracy 0.78076171875\n",
      "Iteration 6620 Training loss 0.019721422344446182 Validation loss 0.020951274782419205 Accuracy 0.78857421875\n",
      "Iteration 6630 Training loss 0.021434616297483444 Validation loss 0.021582545712590218 Accuracy 0.7822265625\n",
      "Iteration 6640 Training loss 0.020327594131231308 Validation loss 0.020296750590205193 Accuracy 0.7958984375\n",
      "Iteration 6650 Training loss 0.019113987684249878 Validation loss 0.02037680521607399 Accuracy 0.7939453125\n",
      "Iteration 6660 Training loss 0.020518984645605087 Validation loss 0.02031487226486206 Accuracy 0.7958984375\n",
      "Iteration 6670 Training loss 0.019837969914078712 Validation loss 0.019778352230787277 Accuracy 0.80078125\n",
      "Iteration 6680 Training loss 0.018742386251688004 Validation loss 0.01979726180434227 Accuracy 0.80078125\n",
      "Iteration 6690 Training loss 0.019823024049401283 Validation loss 0.020001396536827087 Accuracy 0.79833984375\n",
      "Iteration 6700 Training loss 0.019109001383185387 Validation loss 0.02064449153840542 Accuracy 0.79150390625\n",
      "Iteration 6710 Training loss 0.020694535225629807 Validation loss 0.021598195657134056 Accuracy 0.78125\n",
      "Iteration 6720 Training loss 0.020914863795042038 Validation loss 0.020175274461507797 Accuracy 0.796875\n",
      "Iteration 6730 Training loss 0.02349778264760971 Validation loss 0.022099129855632782 Accuracy 0.77685546875\n",
      "Iteration 6740 Training loss 0.019080951809883118 Validation loss 0.020167609676718712 Accuracy 0.7958984375\n",
      "Iteration 6750 Training loss 0.020586075261235237 Validation loss 0.02038983628153801 Accuracy 0.79443359375\n",
      "Iteration 6760 Training loss 0.020116278901696205 Validation loss 0.02043418399989605 Accuracy 0.79345703125\n",
      "Iteration 6770 Training loss 0.019794512540102005 Validation loss 0.02067050151526928 Accuracy 0.79150390625\n",
      "Iteration 6780 Training loss 0.022208400070667267 Validation loss 0.020380888134241104 Accuracy 0.7939453125\n",
      "Iteration 6790 Training loss 0.021158669143915176 Validation loss 0.020437706261873245 Accuracy 0.7939453125\n",
      "Iteration 6800 Training loss 0.014878639951348305 Validation loss 0.019531618803739548 Accuracy 0.80322265625\n",
      "Iteration 6810 Training loss 0.018652023747563362 Validation loss 0.020480649545788765 Accuracy 0.79345703125\n",
      "Iteration 6820 Training loss 0.019275598227977753 Validation loss 0.019744543358683586 Accuracy 0.80078125\n",
      "Iteration 6830 Training loss 0.018619246780872345 Validation loss 0.019659940153360367 Accuracy 0.8017578125\n",
      "Iteration 6840 Training loss 0.022574642673134804 Validation loss 0.02085372433066368 Accuracy 0.78955078125\n",
      "Iteration 6850 Training loss 0.01776847243309021 Validation loss 0.019948890432715416 Accuracy 0.798828125\n",
      "Iteration 6860 Training loss 0.015856582671403885 Validation loss 0.020773909986019135 Accuracy 0.79052734375\n",
      "Iteration 6870 Training loss 0.018515029922127724 Validation loss 0.02002883329987526 Accuracy 0.79736328125\n",
      "Iteration 6880 Training loss 0.019885072484612465 Validation loss 0.020707884803414345 Accuracy 0.79150390625\n",
      "Iteration 6890 Training loss 0.01734686829149723 Validation loss 0.020108943805098534 Accuracy 0.79736328125\n",
      "Iteration 6900 Training loss 0.02033797651529312 Validation loss 0.020944369956851006 Accuracy 0.78955078125\n",
      "Iteration 6910 Training loss 0.02136414311826229 Validation loss 0.020072396844625473 Accuracy 0.79736328125\n",
      "Iteration 6920 Training loss 0.018317941576242447 Validation loss 0.01975742168724537 Accuracy 0.80126953125\n",
      "Iteration 6930 Training loss 0.019106505438685417 Validation loss 0.021306535229086876 Accuracy 0.78466796875\n",
      "Iteration 6940 Training loss 0.01845884881913662 Validation loss 0.019372494891285896 Accuracy 0.8046875\n",
      "Iteration 6950 Training loss 0.01688878796994686 Validation loss 0.019636159762740135 Accuracy 0.8017578125\n",
      "Iteration 6960 Training loss 0.018238887190818787 Validation loss 0.02012200467288494 Accuracy 0.796875\n",
      "Iteration 6970 Training loss 0.020551778376102448 Validation loss 0.02183394506573677 Accuracy 0.7802734375\n",
      "Iteration 6980 Training loss 0.0172562375664711 Validation loss 0.019291916862130165 Accuracy 0.80517578125\n",
      "Iteration 6990 Training loss 0.019387539476156235 Validation loss 0.019475355744361877 Accuracy 0.8037109375\n",
      "Iteration 7000 Training loss 0.014940841123461723 Validation loss 0.019607720896601677 Accuracy 0.802734375\n",
      "Iteration 7010 Training loss 0.01931813359260559 Validation loss 0.019884323701262474 Accuracy 0.79931640625\n",
      "Iteration 7020 Training loss 0.01715705730021 Validation loss 0.020136786624789238 Accuracy 0.79736328125\n",
      "Iteration 7030 Training loss 0.020096516236662865 Validation loss 0.02054816670715809 Accuracy 0.7919921875\n",
      "Iteration 7040 Training loss 0.016952266916632652 Validation loss 0.020140450447797775 Accuracy 0.79638671875\n",
      "Iteration 7050 Training loss 0.018641963601112366 Validation loss 0.019456099718809128 Accuracy 0.80322265625\n",
      "Iteration 7060 Training loss 0.019577238708734512 Validation loss 0.019789159297943115 Accuracy 0.7998046875\n",
      "Iteration 7070 Training loss 0.01770615577697754 Validation loss 0.019396400079131126 Accuracy 0.8046875\n",
      "Iteration 7080 Training loss 0.019783547148108482 Validation loss 0.018961872905492783 Accuracy 0.80908203125\n",
      "Iteration 7090 Training loss 0.01646803505718708 Validation loss 0.019723815843462944 Accuracy 0.80078125\n",
      "Iteration 7100 Training loss 0.02196039818227291 Validation loss 0.02093450166285038 Accuracy 0.78857421875\n",
      "Iteration 7110 Training loss 0.01868554577231407 Validation loss 0.020234478637576103 Accuracy 0.7958984375\n",
      "Iteration 7120 Training loss 0.019134102389216423 Validation loss 0.019520573318004608 Accuracy 0.80322265625\n",
      "Iteration 7130 Training loss 0.01783595234155655 Validation loss 0.019745631143450737 Accuracy 0.80078125\n",
      "Iteration 7140 Training loss 0.014917691238224506 Validation loss 0.019297616556286812 Accuracy 0.80517578125\n",
      "Iteration 7150 Training loss 0.01844705641269684 Validation loss 0.02048800326883793 Accuracy 0.79296875\n",
      "Iteration 7160 Training loss 0.020558806136250496 Validation loss 0.022069741040468216 Accuracy 0.77783203125\n",
      "Iteration 7170 Training loss 0.023303424939513206 Validation loss 0.021093444898724556 Accuracy 0.78759765625\n",
      "Iteration 7180 Training loss 0.017436698079109192 Validation loss 0.020001767203211784 Accuracy 0.79833984375\n",
      "Iteration 7190 Training loss 0.017579304054379463 Validation loss 0.019547458738088608 Accuracy 0.802734375\n",
      "Iteration 7200 Training loss 0.017547953873872757 Validation loss 0.01929323747754097 Accuracy 0.80517578125\n",
      "Iteration 7210 Training loss 0.01954985037446022 Validation loss 0.01941256783902645 Accuracy 0.80419921875\n",
      "Iteration 7220 Training loss 0.019025813788175583 Validation loss 0.02044769749045372 Accuracy 0.7939453125\n",
      "Iteration 7230 Training loss 0.019289901480078697 Validation loss 0.019651398062705994 Accuracy 0.8017578125\n",
      "Iteration 7240 Training loss 0.01570228859782219 Validation loss 0.018997956067323685 Accuracy 0.80810546875\n",
      "Iteration 7250 Training loss 0.018773969262838364 Validation loss 0.01984497345983982 Accuracy 0.80029296875\n",
      "Iteration 7260 Training loss 0.01875302940607071 Validation loss 0.020794225856661797 Accuracy 0.7900390625\n",
      "Iteration 7270 Training loss 0.020916864275932312 Validation loss 0.020062170922756195 Accuracy 0.7978515625\n",
      "Iteration 7280 Training loss 0.018915312364697456 Validation loss 0.018878279253840446 Accuracy 0.8095703125\n",
      "Iteration 7290 Training loss 0.018994785845279694 Validation loss 0.01953909359872341 Accuracy 0.80322265625\n",
      "Iteration 7300 Training loss 0.018359260633587837 Validation loss 0.02064989134669304 Accuracy 0.7919921875\n",
      "Iteration 7310 Training loss 0.017933260649442673 Validation loss 0.019145172089338303 Accuracy 0.80712890625\n",
      "Iteration 7320 Training loss 0.021562321111559868 Validation loss 0.021017959341406822 Accuracy 0.78759765625\n",
      "Iteration 7330 Training loss 0.017436407506465912 Validation loss 0.01909349486231804 Accuracy 0.8076171875\n",
      "Iteration 7340 Training loss 0.01615540124475956 Validation loss 0.01898207515478134 Accuracy 0.80908203125\n",
      "Iteration 7350 Training loss 0.017903858795762062 Validation loss 0.02022366411983967 Accuracy 0.79541015625\n",
      "Iteration 7360 Training loss 0.018309950828552246 Validation loss 0.020279861986637115 Accuracy 0.7958984375\n",
      "Iteration 7370 Training loss 0.016713019460439682 Validation loss 0.019249582663178444 Accuracy 0.80615234375\n",
      "Iteration 7380 Training loss 0.020074734464287758 Validation loss 0.01987108774483204 Accuracy 0.79931640625\n",
      "Iteration 7390 Training loss 0.01829635538160801 Validation loss 0.019499866291880608 Accuracy 0.80322265625\n",
      "Iteration 7400 Training loss 0.0190750602632761 Validation loss 0.019441263750195503 Accuracy 0.80419921875\n",
      "Iteration 7410 Training loss 0.01633663848042488 Validation loss 0.0192601028829813 Accuracy 0.80615234375\n",
      "Iteration 7420 Training loss 0.017206016927957535 Validation loss 0.01998196914792061 Accuracy 0.79833984375\n",
      "Iteration 7430 Training loss 0.016461635008454323 Validation loss 0.01932397671043873 Accuracy 0.80517578125\n",
      "Iteration 7440 Training loss 0.015587257221341133 Validation loss 0.019537173211574554 Accuracy 0.802734375\n",
      "Iteration 7450 Training loss 0.01731627620756626 Validation loss 0.019443988800048828 Accuracy 0.8037109375\n",
      "Iteration 7460 Training loss 0.017233720049262047 Validation loss 0.01934760995209217 Accuracy 0.80517578125\n",
      "Iteration 7470 Training loss 0.015057657845318317 Validation loss 0.01914873719215393 Accuracy 0.806640625\n",
      "Iteration 7480 Training loss 0.020875364542007446 Validation loss 0.019098687916994095 Accuracy 0.80712890625\n",
      "Iteration 7490 Training loss 0.01814846321940422 Validation loss 0.0190426018089056 Accuracy 0.8076171875\n",
      "Iteration 7500 Training loss 0.020740771666169167 Validation loss 0.019381776452064514 Accuracy 0.80517578125\n",
      "Iteration 7510 Training loss 0.01726452447474003 Validation loss 0.018998928368091583 Accuracy 0.80810546875\n",
      "Iteration 7520 Training loss 0.019692961126565933 Validation loss 0.019145283848047256 Accuracy 0.80615234375\n",
      "Iteration 7530 Training loss 0.019383734092116356 Validation loss 0.019250797107815742 Accuracy 0.8056640625\n",
      "Iteration 7540 Training loss 0.02060389518737793 Validation loss 0.019901037216186523 Accuracy 0.79931640625\n",
      "Iteration 7550 Training loss 0.018163487315177917 Validation loss 0.018701445311307907 Accuracy 0.81103515625\n",
      "Iteration 7560 Training loss 0.014957956969738007 Validation loss 0.019235368818044662 Accuracy 0.80615234375\n",
      "Iteration 7570 Training loss 0.01610487513244152 Validation loss 0.018727973103523254 Accuracy 0.81103515625\n",
      "Iteration 7580 Training loss 0.017288217321038246 Validation loss 0.01882993057370186 Accuracy 0.810546875\n",
      "Iteration 7590 Training loss 0.015791337937116623 Validation loss 0.018690597265958786 Accuracy 0.8115234375\n",
      "Iteration 7600 Training loss 0.015182116068899632 Validation loss 0.018791450187563896 Accuracy 0.81005859375\n",
      "Iteration 7610 Training loss 0.019382767379283905 Validation loss 0.01882362924516201 Accuracy 0.81005859375\n",
      "Iteration 7620 Training loss 0.019783517345786095 Validation loss 0.018832627683877945 Accuracy 0.81005859375\n",
      "Iteration 7630 Training loss 0.01713264174759388 Validation loss 0.018558228388428688 Accuracy 0.81298828125\n",
      "Iteration 7640 Training loss 0.021815365180373192 Validation loss 0.019257057458162308 Accuracy 0.8056640625\n",
      "Iteration 7650 Training loss 0.01786387898027897 Validation loss 0.01868033967912197 Accuracy 0.8115234375\n",
      "Iteration 7660 Training loss 0.019413601607084274 Validation loss 0.02038600668311119 Accuracy 0.794921875\n",
      "Iteration 7670 Training loss 0.01729530096054077 Validation loss 0.01897852122783661 Accuracy 0.80810546875\n",
      "Iteration 7680 Training loss 0.01894982159137726 Validation loss 0.020142577588558197 Accuracy 0.796875\n",
      "Iteration 7690 Training loss 0.018174227327108383 Validation loss 0.019357750192284584 Accuracy 0.8046875\n",
      "Iteration 7700 Training loss 0.020673349499702454 Validation loss 0.020286567509174347 Accuracy 0.79541015625\n",
      "Iteration 7710 Training loss 0.01654140278697014 Validation loss 0.019394587725400925 Accuracy 0.8046875\n",
      "Iteration 7720 Training loss 0.01950525864958763 Validation loss 0.01980985514819622 Accuracy 0.80029296875\n",
      "Iteration 7730 Training loss 0.017931584268808365 Validation loss 0.0189982783049345 Accuracy 0.80810546875\n",
      "Iteration 7740 Training loss 0.018922843039035797 Validation loss 0.01922418735921383 Accuracy 0.8056640625\n",
      "Iteration 7750 Training loss 0.01853705383837223 Validation loss 0.020119262859225273 Accuracy 0.796875\n",
      "Iteration 7760 Training loss 0.017765268683433533 Validation loss 0.018677087500691414 Accuracy 0.8115234375\n",
      "Iteration 7770 Training loss 0.01717647723853588 Validation loss 0.018762031570076942 Accuracy 0.81103515625\n",
      "Iteration 7780 Training loss 0.018917294219136238 Validation loss 0.019442467018961906 Accuracy 0.80322265625\n",
      "Iteration 7790 Training loss 0.018117718398571014 Validation loss 0.019021889194846153 Accuracy 0.8076171875\n",
      "Iteration 7800 Training loss 0.01722140982747078 Validation loss 0.019594881683588028 Accuracy 0.8017578125\n",
      "Iteration 7810 Training loss 0.018001938238739967 Validation loss 0.020154083147644997 Accuracy 0.796875\n",
      "Iteration 7820 Training loss 0.015845540910959244 Validation loss 0.018650474026799202 Accuracy 0.81201171875\n",
      "Iteration 7830 Training loss 0.018318571150302887 Validation loss 0.019436804577708244 Accuracy 0.8037109375\n",
      "Iteration 7840 Training loss 0.015258921310305595 Validation loss 0.0192109402269125 Accuracy 0.8056640625\n",
      "Iteration 7850 Training loss 0.01920764148235321 Validation loss 0.018867328763008118 Accuracy 0.8095703125\n",
      "Iteration 7860 Training loss 0.0181196928024292 Validation loss 0.019640818238258362 Accuracy 0.80224609375\n",
      "Iteration 7870 Training loss 0.019924582913517952 Validation loss 0.018550070002675056 Accuracy 0.81298828125\n",
      "Iteration 7880 Training loss 0.015910759568214417 Validation loss 0.01916281133890152 Accuracy 0.806640625\n",
      "Iteration 7890 Training loss 0.02010725624859333 Validation loss 0.021499907597899437 Accuracy 0.78271484375\n",
      "Iteration 7900 Training loss 0.01765202358365059 Validation loss 0.0189225971698761 Accuracy 0.80908203125\n",
      "Iteration 7910 Training loss 0.01779029332101345 Validation loss 0.018503572791814804 Accuracy 0.8134765625\n",
      "Iteration 7920 Training loss 0.01477621216326952 Validation loss 0.01876148395240307 Accuracy 0.81103515625\n",
      "Iteration 7930 Training loss 0.016661377623677254 Validation loss 0.018539031967520714 Accuracy 0.81298828125\n",
      "Iteration 7940 Training loss 0.018758900463581085 Validation loss 0.01915108598768711 Accuracy 0.806640625\n",
      "Iteration 7950 Training loss 0.01646777242422104 Validation loss 0.019181732088327408 Accuracy 0.806640625\n",
      "Iteration 7960 Training loss 0.01994389295578003 Validation loss 0.019719058647751808 Accuracy 0.80078125\n",
      "Iteration 7970 Training loss 0.016949206590652466 Validation loss 0.019512049853801727 Accuracy 0.80322265625\n",
      "Iteration 7980 Training loss 0.02025238610804081 Validation loss 0.019647955894470215 Accuracy 0.8017578125\n",
      "Iteration 7990 Training loss 0.019321968778967857 Validation loss 0.019080180674791336 Accuracy 0.80712890625\n",
      "Iteration 8000 Training loss 0.0155866714194417 Validation loss 0.019145317375659943 Accuracy 0.806640625\n",
      "Iteration 8010 Training loss 0.017064590007066727 Validation loss 0.01859954185783863 Accuracy 0.81298828125\n",
      "Iteration 8020 Training loss 0.0212459284812212 Validation loss 0.019993150606751442 Accuracy 0.79833984375\n",
      "Iteration 8030 Training loss 0.016355201601982117 Validation loss 0.01895121857523918 Accuracy 0.80908203125\n",
      "Iteration 8040 Training loss 0.014284268952906132 Validation loss 0.018955156207084656 Accuracy 0.80859375\n",
      "Iteration 8050 Training loss 0.016569601371884346 Validation loss 0.019110646098852158 Accuracy 0.8076171875\n",
      "Iteration 8060 Training loss 0.01745343580842018 Validation loss 0.021039091050624847 Accuracy 0.7880859375\n",
      "Iteration 8070 Training loss 0.019882306456565857 Validation loss 0.019070003181695938 Accuracy 0.8076171875\n",
      "Iteration 8080 Training loss 0.017138591036200523 Validation loss 0.018742568790912628 Accuracy 0.81103515625\n",
      "Iteration 8090 Training loss 0.01887611672282219 Validation loss 0.019880758598446846 Accuracy 0.7998046875\n",
      "Iteration 8100 Training loss 0.02017439715564251 Validation loss 0.019517401233315468 Accuracy 0.80322265625\n",
      "Iteration 8110 Training loss 0.018740059807896614 Validation loss 0.019818656146526337 Accuracy 0.79931640625\n",
      "Iteration 8120 Training loss 0.01659727841615677 Validation loss 0.020294075831770897 Accuracy 0.79541015625\n",
      "Iteration 8130 Training loss 0.01419861800968647 Validation loss 0.018711792305111885 Accuracy 0.81103515625\n",
      "Iteration 8140 Training loss 0.016712594777345657 Validation loss 0.018790360540151596 Accuracy 0.81005859375\n",
      "Iteration 8150 Training loss 0.01600375398993492 Validation loss 0.01908455789089203 Accuracy 0.80712890625\n",
      "Iteration 8160 Training loss 0.017166923731565475 Validation loss 0.019472291693091393 Accuracy 0.80419921875\n",
      "Iteration 8170 Training loss 0.018538961187005043 Validation loss 0.019372738897800446 Accuracy 0.8046875\n",
      "Iteration 8180 Training loss 0.01861640438437462 Validation loss 0.019063491374254227 Accuracy 0.80712890625\n",
      "Iteration 8190 Training loss 0.01690969429910183 Validation loss 0.018459031358361244 Accuracy 0.8134765625\n",
      "Iteration 8200 Training loss 0.016707323491573334 Validation loss 0.018516818061470985 Accuracy 0.81298828125\n",
      "Iteration 8210 Training loss 0.017895856872200966 Validation loss 0.018923098221421242 Accuracy 0.8095703125\n",
      "Iteration 8220 Training loss 0.016415633261203766 Validation loss 0.018428869545459747 Accuracy 0.81396484375\n",
      "Iteration 8230 Training loss 0.019056107848882675 Validation loss 0.018695976585149765 Accuracy 0.81103515625\n",
      "Iteration 8240 Training loss 0.02066913992166519 Validation loss 0.02032180316746235 Accuracy 0.79443359375\n",
      "Iteration 8250 Training loss 0.018740342929959297 Validation loss 0.01955532655119896 Accuracy 0.802734375\n",
      "Iteration 8260 Training loss 0.016816461458802223 Validation loss 0.0185355506837368 Accuracy 0.8125\n",
      "Iteration 8270 Training loss 0.017315872013568878 Validation loss 0.01935221068561077 Accuracy 0.80419921875\n",
      "Iteration 8280 Training loss 0.019000424072146416 Validation loss 0.019157622009515762 Accuracy 0.806640625\n",
      "Iteration 8290 Training loss 0.01650751568377018 Validation loss 0.019611380994319916 Accuracy 0.80224609375\n",
      "Iteration 8300 Training loss 0.016718020662665367 Validation loss 0.01852342300117016 Accuracy 0.8134765625\n",
      "Iteration 8310 Training loss 0.019253524020314217 Validation loss 0.018570629879832268 Accuracy 0.8125\n",
      "Iteration 8320 Training loss 0.01755708083510399 Validation loss 0.019323358312249184 Accuracy 0.8046875\n",
      "Iteration 8330 Training loss 0.018771780654788017 Validation loss 0.019919417798519135 Accuracy 0.79931640625\n",
      "Iteration 8340 Training loss 0.017480410635471344 Validation loss 0.01926703006029129 Accuracy 0.80517578125\n",
      "Iteration 8350 Training loss 0.020396674051880836 Validation loss 0.0186381284147501 Accuracy 0.81201171875\n",
      "Iteration 8360 Training loss 0.016232704743742943 Validation loss 0.018434880301356316 Accuracy 0.81396484375\n",
      "Iteration 8370 Training loss 0.016964301466941833 Validation loss 0.018461549654603004 Accuracy 0.81396484375\n",
      "Iteration 8380 Training loss 0.016489580273628235 Validation loss 0.018152086064219475 Accuracy 0.81640625\n",
      "Iteration 8390 Training loss 0.016314931213855743 Validation loss 0.018568120896816254 Accuracy 0.81298828125\n",
      "Iteration 8400 Training loss 0.017730440944433212 Validation loss 0.0204861368983984 Accuracy 0.7939453125\n",
      "Iteration 8410 Training loss 0.017743723466992378 Validation loss 0.01865813136100769 Accuracy 0.81201171875\n",
      "Iteration 8420 Training loss 0.022571319714188576 Validation loss 0.020545097067952156 Accuracy 0.79296875\n",
      "Iteration 8430 Training loss 0.019238097593188286 Validation loss 0.018907442688941956 Accuracy 0.80908203125\n",
      "Iteration 8440 Training loss 0.014394392259418964 Validation loss 0.01929861307144165 Accuracy 0.80517578125\n",
      "Iteration 8450 Training loss 0.01843998208642006 Validation loss 0.02021549828350544 Accuracy 0.79638671875\n",
      "Iteration 8460 Training loss 0.01503821462392807 Validation loss 0.01837746612727642 Accuracy 0.814453125\n",
      "Iteration 8470 Training loss 0.017649080604314804 Validation loss 0.018375568091869354 Accuracy 0.81396484375\n",
      "Iteration 8480 Training loss 0.016341792419552803 Validation loss 0.018574664369225502 Accuracy 0.81201171875\n",
      "Iteration 8490 Training loss 0.018320243805646896 Validation loss 0.019798994064331055 Accuracy 0.80029296875\n",
      "Iteration 8500 Training loss 0.015312056988477707 Validation loss 0.018407341092824936 Accuracy 0.81396484375\n",
      "Iteration 8510 Training loss 0.01570030488073826 Validation loss 0.018347755074501038 Accuracy 0.81396484375\n",
      "Iteration 8520 Training loss 0.017230475321412086 Validation loss 0.019014574587345123 Accuracy 0.80859375\n",
      "Iteration 8530 Training loss 0.01650308445096016 Validation loss 0.018353447318077087 Accuracy 0.81494140625\n",
      "Iteration 8540 Training loss 0.019031159579753876 Validation loss 0.018204815685749054 Accuracy 0.81591796875\n",
      "Iteration 8550 Training loss 0.021220380440354347 Validation loss 0.019864773377776146 Accuracy 0.7998046875\n",
      "Iteration 8560 Training loss 0.015509100630879402 Validation loss 0.018312733620405197 Accuracy 0.8154296875\n",
      "Iteration 8570 Training loss 0.016163287684321404 Validation loss 0.018569711595773697 Accuracy 0.8125\n",
      "Iteration 8580 Training loss 0.014174791984260082 Validation loss 0.018193207681179047 Accuracy 0.81640625\n",
      "Iteration 8590 Training loss 0.019171681255102158 Validation loss 0.018320539966225624 Accuracy 0.8154296875\n",
      "Iteration 8600 Training loss 0.011975695379078388 Validation loss 0.01857510395348072 Accuracy 0.8125\n",
      "Iteration 8610 Training loss 0.015138580463826656 Validation loss 0.01865202747285366 Accuracy 0.81201171875\n",
      "Iteration 8620 Training loss 0.01567709818482399 Validation loss 0.018384067341685295 Accuracy 0.814453125\n",
      "Iteration 8630 Training loss 0.0188725758343935 Validation loss 0.018396060913801193 Accuracy 0.814453125\n",
      "Iteration 8640 Training loss 0.019038738682866096 Validation loss 0.018650036305189133 Accuracy 0.8115234375\n",
      "Iteration 8650 Training loss 0.01580313965678215 Validation loss 0.018429942429065704 Accuracy 0.81396484375\n",
      "Iteration 8660 Training loss 0.015594357624650002 Validation loss 0.019189616665244102 Accuracy 0.80615234375\n",
      "Iteration 8670 Training loss 0.018969839438796043 Validation loss 0.01826281100511551 Accuracy 0.81591796875\n",
      "Iteration 8680 Training loss 0.018899638205766678 Validation loss 0.018528111279010773 Accuracy 0.81298828125\n",
      "Iteration 8690 Training loss 0.018179183825850487 Validation loss 0.018698574975132942 Accuracy 0.81103515625\n",
      "Iteration 8700 Training loss 0.01893479935824871 Validation loss 0.01941918209195137 Accuracy 0.80419921875\n",
      "Iteration 8710 Training loss 0.019297214224934578 Validation loss 0.018531110137701035 Accuracy 0.81298828125\n",
      "Iteration 8720 Training loss 0.02099027670919895 Validation loss 0.022381912916898727 Accuracy 0.77490234375\n",
      "Iteration 8730 Training loss 0.02020632103085518 Validation loss 0.018756616860628128 Accuracy 0.81103515625\n",
      "Iteration 8740 Training loss 0.017933879047632217 Validation loss 0.018922533839941025 Accuracy 0.8095703125\n",
      "Iteration 8750 Training loss 0.01808730885386467 Validation loss 0.018423616886138916 Accuracy 0.81396484375\n",
      "Iteration 8760 Training loss 0.016298899427056313 Validation loss 0.018883608281612396 Accuracy 0.80908203125\n",
      "Iteration 8770 Training loss 0.0172681026160717 Validation loss 0.018857261165976524 Accuracy 0.8095703125\n",
      "Iteration 8780 Training loss 0.018808456137776375 Validation loss 0.01895168609917164 Accuracy 0.80908203125\n",
      "Iteration 8790 Training loss 0.01686641201376915 Validation loss 0.018421199172735214 Accuracy 0.81396484375\n",
      "Iteration 8800 Training loss 0.015968522056937218 Validation loss 0.019146278500556946 Accuracy 0.806640625\n",
      "Iteration 8810 Training loss 0.01639900915324688 Validation loss 0.01887473277747631 Accuracy 0.80908203125\n",
      "Iteration 8820 Training loss 0.020197806879878044 Validation loss 0.018386926501989365 Accuracy 0.814453125\n",
      "Iteration 8830 Training loss 0.015048029832541943 Validation loss 0.018106769770383835 Accuracy 0.8173828125\n",
      "Iteration 8840 Training loss 0.015173237770795822 Validation loss 0.01837761141359806 Accuracy 0.814453125\n",
      "Iteration 8850 Training loss 0.01832547038793564 Validation loss 0.018764197826385498 Accuracy 0.81103515625\n",
      "Iteration 8860 Training loss 0.01783638633787632 Validation loss 0.018458114936947823 Accuracy 0.8134765625\n",
      "Iteration 8870 Training loss 0.018096379935741425 Validation loss 0.018794799223542213 Accuracy 0.81005859375\n",
      "Iteration 8880 Training loss 0.020996354520320892 Validation loss 0.020611993968486786 Accuracy 0.79150390625\n",
      "Iteration 8890 Training loss 0.017909375950694084 Validation loss 0.019489364698529243 Accuracy 0.80322265625\n",
      "Iteration 8900 Training loss 0.017911577597260475 Validation loss 0.018934613093733788 Accuracy 0.80908203125\n",
      "Iteration 8910 Training loss 0.02110450156033039 Validation loss 0.01854354701936245 Accuracy 0.8134765625\n",
      "Iteration 8920 Training loss 0.017388038337230682 Validation loss 0.018172981217503548 Accuracy 0.81640625\n",
      "Iteration 8930 Training loss 0.017633182927966118 Validation loss 0.019641300663352013 Accuracy 0.8017578125\n",
      "Iteration 8940 Training loss 0.015614441595971584 Validation loss 0.018916383385658264 Accuracy 0.80908203125\n",
      "Iteration 8950 Training loss 0.01872737519443035 Validation loss 0.01853887550532818 Accuracy 0.81298828125\n",
      "Iteration 8960 Training loss 0.01815912127494812 Validation loss 0.018545523285865784 Accuracy 0.81298828125\n",
      "Iteration 8970 Training loss 0.01817820593714714 Validation loss 0.018640778958797455 Accuracy 0.81201171875\n",
      "Iteration 8980 Training loss 0.01699952408671379 Validation loss 0.018514463678002357 Accuracy 0.81298828125\n",
      "Iteration 8990 Training loss 0.015141606330871582 Validation loss 0.018607040867209435 Accuracy 0.81201171875\n",
      "Iteration 9000 Training loss 0.01629292219877243 Validation loss 0.018581852316856384 Accuracy 0.8125\n",
      "Iteration 9010 Training loss 0.016865938901901245 Validation loss 0.018281808122992516 Accuracy 0.81591796875\n",
      "Iteration 9020 Training loss 0.01884298399090767 Validation loss 0.01833857223391533 Accuracy 0.8154296875\n",
      "Iteration 9030 Training loss 0.017758764326572418 Validation loss 0.018328197300434113 Accuracy 0.814453125\n",
      "Iteration 9040 Training loss 0.017446571961045265 Validation loss 0.019864056259393692 Accuracy 0.7998046875\n",
      "Iteration 9050 Training loss 0.015667127445340157 Validation loss 0.018512006849050522 Accuracy 0.81298828125\n",
      "Iteration 9060 Training loss 0.01781553588807583 Validation loss 0.018388720229268074 Accuracy 0.81396484375\n",
      "Iteration 9070 Training loss 0.015447975136339664 Validation loss 0.018042249605059624 Accuracy 0.818359375\n",
      "Iteration 9080 Training loss 0.021353721618652344 Validation loss 0.01901528239250183 Accuracy 0.8076171875\n",
      "Iteration 9090 Training loss 0.016914544627070427 Validation loss 0.01877809315919876 Accuracy 0.810546875\n",
      "Iteration 9100 Training loss 0.01734384335577488 Validation loss 0.018271544948220253 Accuracy 0.8154296875\n",
      "Iteration 9110 Training loss 0.016329467296600342 Validation loss 0.018262945115566254 Accuracy 0.81494140625\n",
      "Iteration 9120 Training loss 0.014780381694436073 Validation loss 0.01889352686703205 Accuracy 0.80908203125\n",
      "Iteration 9130 Training loss 0.016505233943462372 Validation loss 0.01945488341152668 Accuracy 0.8037109375\n",
      "Iteration 9140 Training loss 0.018849899992346764 Validation loss 0.018362607806921005 Accuracy 0.81494140625\n",
      "Iteration 9150 Training loss 0.01755055971443653 Validation loss 0.018759755417704582 Accuracy 0.810546875\n",
      "Iteration 9160 Training loss 0.016479145735502243 Validation loss 0.018490299582481384 Accuracy 0.81298828125\n",
      "Iteration 9170 Training loss 0.019140835851430893 Validation loss 0.01919943280518055 Accuracy 0.806640625\n",
      "Iteration 9180 Training loss 0.0179721862077713 Validation loss 0.018016798421740532 Accuracy 0.818359375\n",
      "Iteration 9190 Training loss 0.022819465026259422 Validation loss 0.02113012596964836 Accuracy 0.78662109375\n",
      "Iteration 9200 Training loss 0.019170867279171944 Validation loss 0.02072484791278839 Accuracy 0.7900390625\n",
      "Iteration 9210 Training loss 0.016227930784225464 Validation loss 0.01885823719203472 Accuracy 0.8095703125\n",
      "Iteration 9220 Training loss 0.017299247905611992 Validation loss 0.01874454878270626 Accuracy 0.8115234375\n",
      "Iteration 9230 Training loss 0.01673329621553421 Validation loss 0.018548257648944855 Accuracy 0.8125\n",
      "Iteration 9240 Training loss 0.016865314915776253 Validation loss 0.018946373835206032 Accuracy 0.80908203125\n",
      "Iteration 9250 Training loss 0.016415191814303398 Validation loss 0.018296219408512115 Accuracy 0.8154296875\n",
      "Iteration 9260 Training loss 0.01683671586215496 Validation loss 0.018256505951285362 Accuracy 0.81640625\n",
      "Iteration 9270 Training loss 0.016044991090893745 Validation loss 0.018623163923621178 Accuracy 0.81201171875\n",
      "Iteration 9280 Training loss 0.016404027119278908 Validation loss 0.018267374485731125 Accuracy 0.81591796875\n",
      "Iteration 9290 Training loss 0.0143317561596632 Validation loss 0.018738074228167534 Accuracy 0.81103515625\n",
      "Iteration 9300 Training loss 0.014253903180360794 Validation loss 0.01869547739624977 Accuracy 0.8115234375\n",
      "Iteration 9310 Training loss 0.01614362560212612 Validation loss 0.018965711817145348 Accuracy 0.80810546875\n",
      "Iteration 9320 Training loss 0.01794220134615898 Validation loss 0.018488790839910507 Accuracy 0.8134765625\n",
      "Iteration 9330 Training loss 0.019869981333613396 Validation loss 0.018615102395415306 Accuracy 0.8125\n",
      "Iteration 9340 Training loss 0.015253203921020031 Validation loss 0.018398204818367958 Accuracy 0.814453125\n",
      "Iteration 9350 Training loss 0.01790347322821617 Validation loss 0.019617391750216484 Accuracy 0.8017578125\n",
      "Iteration 9360 Training loss 0.01612149365246296 Validation loss 0.018158238381147385 Accuracy 0.81640625\n",
      "Iteration 9370 Training loss 0.01655653677880764 Validation loss 0.018706433475017548 Accuracy 0.8115234375\n",
      "Iteration 9380 Training loss 0.017301691696047783 Validation loss 0.018394002690911293 Accuracy 0.81396484375\n",
      "Iteration 9390 Training loss 0.013992668129503727 Validation loss 0.018333693966269493 Accuracy 0.8154296875\n",
      "Iteration 9400 Training loss 0.016538839787244797 Validation loss 0.018197432160377502 Accuracy 0.81640625\n",
      "Iteration 9410 Training loss 0.017889101058244705 Validation loss 0.018152037635445595 Accuracy 0.81689453125\n",
      "Iteration 9420 Training loss 0.01621447689831257 Validation loss 0.018294699490070343 Accuracy 0.8154296875\n",
      "Iteration 9430 Training loss 0.01843852549791336 Validation loss 0.01939360983669758 Accuracy 0.8046875\n",
      "Iteration 9440 Training loss 0.01634962670505047 Validation loss 0.018730808049440384 Accuracy 0.810546875\n",
      "Iteration 9450 Training loss 0.01632787100970745 Validation loss 0.01853780634701252 Accuracy 0.8125\n",
      "Iteration 9460 Training loss 0.017103418707847595 Validation loss 0.01845591701567173 Accuracy 0.81396484375\n",
      "Iteration 9470 Training loss 0.01465561892837286 Validation loss 0.018007583916187286 Accuracy 0.818359375\n",
      "Iteration 9480 Training loss 0.024603063240647316 Validation loss 0.021959057077765465 Accuracy 0.7783203125\n",
      "Iteration 9490 Training loss 0.017102111130952835 Validation loss 0.018994461745023727 Accuracy 0.80810546875\n",
      "Iteration 9500 Training loss 0.01692383736371994 Validation loss 0.01851707510650158 Accuracy 0.8125\n",
      "Iteration 9510 Training loss 0.0147194629535079 Validation loss 0.018352936953306198 Accuracy 0.814453125\n",
      "Iteration 9520 Training loss 0.01621970161795616 Validation loss 0.01914258673787117 Accuracy 0.80712890625\n",
      "Iteration 9530 Training loss 0.017592350021004677 Validation loss 0.018172582611441612 Accuracy 0.81689453125\n",
      "Iteration 9540 Training loss 0.019051793962717056 Validation loss 0.017955143004655838 Accuracy 0.81884765625\n",
      "Iteration 9550 Training loss 0.016108741983771324 Validation loss 0.018957484513521194 Accuracy 0.80908203125\n",
      "Iteration 9560 Training loss 0.01648162491619587 Validation loss 0.01843579113483429 Accuracy 0.81396484375\n",
      "Iteration 9570 Training loss 0.016061782836914062 Validation loss 0.017993152141571045 Accuracy 0.818359375\n",
      "Iteration 9580 Training loss 0.017784103751182556 Validation loss 0.017939118668437004 Accuracy 0.8193359375\n",
      "Iteration 9590 Training loss 0.014016552828252316 Validation loss 0.018619567155838013 Accuracy 0.8115234375\n",
      "Iteration 9600 Training loss 0.014852120541036129 Validation loss 0.01830860786139965 Accuracy 0.81494140625\n",
      "Iteration 9610 Training loss 0.016695421189069748 Validation loss 0.018453028053045273 Accuracy 0.8134765625\n",
      "Iteration 9620 Training loss 0.016996636986732483 Validation loss 0.018754586577415466 Accuracy 0.810546875\n",
      "Iteration 9630 Training loss 0.0205089058727026 Validation loss 0.019632160663604736 Accuracy 0.8017578125\n",
      "Iteration 9640 Training loss 0.018632320687174797 Validation loss 0.017937269061803818 Accuracy 0.81884765625\n",
      "Iteration 9650 Training loss 0.02011752314865589 Validation loss 0.018621329218149185 Accuracy 0.8125\n",
      "Iteration 9660 Training loss 0.01710692048072815 Validation loss 0.01832493580877781 Accuracy 0.81494140625\n",
      "Iteration 9670 Training loss 0.015727190300822258 Validation loss 0.01825283095240593 Accuracy 0.81640625\n",
      "Iteration 9680 Training loss 0.016862863674759865 Validation loss 0.018634572625160217 Accuracy 0.81201171875\n",
      "Iteration 9690 Training loss 0.015315599739551544 Validation loss 0.0188971608877182 Accuracy 0.8095703125\n",
      "Iteration 9700 Training loss 0.016081709414720535 Validation loss 0.018046196550130844 Accuracy 0.81787109375\n",
      "Iteration 9710 Training loss 0.014962924644351006 Validation loss 0.018715040758252144 Accuracy 0.81103515625\n",
      "Iteration 9720 Training loss 0.020562339574098587 Validation loss 0.019616231322288513 Accuracy 0.80224609375\n",
      "Iteration 9730 Training loss 0.0161055326461792 Validation loss 0.01809692569077015 Accuracy 0.81689453125\n",
      "Iteration 9740 Training loss 0.017994733527302742 Validation loss 0.019055824726819992 Accuracy 0.80810546875\n",
      "Iteration 9750 Training loss 0.020435474812984467 Validation loss 0.018891027197241783 Accuracy 0.8095703125\n",
      "Iteration 9760 Training loss 0.017907744273543358 Validation loss 0.018168680369853973 Accuracy 0.81689453125\n",
      "Iteration 9770 Training loss 0.01582713983952999 Validation loss 0.018533039838075638 Accuracy 0.81298828125\n",
      "Iteration 9780 Training loss 0.01700887270271778 Validation loss 0.01826988160610199 Accuracy 0.81591796875\n",
      "Iteration 9790 Training loss 0.01848544366657734 Validation loss 0.0190410278737545 Accuracy 0.80810546875\n",
      "Iteration 9800 Training loss 0.015405290760099888 Validation loss 0.018240204080939293 Accuracy 0.81591796875\n",
      "Iteration 9810 Training loss 0.017982076853513718 Validation loss 0.018357975408434868 Accuracy 0.814453125\n",
      "Iteration 9820 Training loss 0.019430190324783325 Validation loss 0.0185269545763731 Accuracy 0.81298828125\n",
      "Iteration 9830 Training loss 0.016727376729249954 Validation loss 0.01837397739291191 Accuracy 0.81396484375\n",
      "Iteration 9840 Training loss 0.01569500006735325 Validation loss 0.01825575716793537 Accuracy 0.8154296875\n",
      "Iteration 9850 Training loss 0.016110029071569443 Validation loss 0.01812753453850746 Accuracy 0.8173828125\n",
      "Iteration 9860 Training loss 0.017963815480470657 Validation loss 0.019169248640537262 Accuracy 0.806640625\n",
      "Iteration 9870 Training loss 0.01754968799650669 Validation loss 0.017901206389069557 Accuracy 0.8193359375\n",
      "Iteration 9880 Training loss 0.018169676885008812 Validation loss 0.01953624188899994 Accuracy 0.802734375\n",
      "Iteration 9890 Training loss 0.018190938979387283 Validation loss 0.01832442171871662 Accuracy 0.8154296875\n",
      "Iteration 9900 Training loss 0.01637071929872036 Validation loss 0.0181422121822834 Accuracy 0.81689453125\n",
      "Iteration 9910 Training loss 0.017437798902392387 Validation loss 0.01822977513074875 Accuracy 0.81591796875\n",
      "Iteration 9920 Training loss 0.014953061006963253 Validation loss 0.018005063757300377 Accuracy 0.818359375\n",
      "Iteration 9930 Training loss 0.015648359432816505 Validation loss 0.018154846504330635 Accuracy 0.81640625\n",
      "Iteration 9940 Training loss 0.01814068667590618 Validation loss 0.018077127635478973 Accuracy 0.81787109375\n",
      "Iteration 9950 Training loss 0.01443888247013092 Validation loss 0.018197614699602127 Accuracy 0.81640625\n",
      "Iteration 9960 Training loss 0.017513953149318695 Validation loss 0.01866023615002632 Accuracy 0.8115234375\n",
      "Iteration 9970 Training loss 0.013319751247763634 Validation loss 0.018118003383278847 Accuracy 0.81640625\n",
      "Iteration 9980 Training loss 0.01646203175187111 Validation loss 0.017978301271796227 Accuracy 0.81884765625\n",
      "Iteration 9990 Training loss 0.015143618918955326 Validation loss 0.018268229439854622 Accuracy 0.8154296875\n",
      "Iteration 10000 Training loss 0.01446568313986063 Validation loss 0.017996927723288536 Accuracy 0.81884765625\n",
      "Iteration 10010 Training loss 0.017160188406705856 Validation loss 0.018012352287769318 Accuracy 0.818359375\n",
      "Iteration 10020 Training loss 0.01729440502822399 Validation loss 0.018506132066249847 Accuracy 0.8134765625\n",
      "Iteration 10030 Training loss 0.017275376245379448 Validation loss 0.018143877387046814 Accuracy 0.81640625\n",
      "Iteration 10040 Training loss 0.015308546833693981 Validation loss 0.01890406385064125 Accuracy 0.8095703125\n",
      "Iteration 10050 Training loss 0.016286976635456085 Validation loss 0.019002145156264305 Accuracy 0.80810546875\n",
      "Iteration 10060 Training loss 0.01707637682557106 Validation loss 0.018287478014826775 Accuracy 0.8154296875\n",
      "Iteration 10070 Training loss 0.015654105693101883 Validation loss 0.018979398533701897 Accuracy 0.80859375\n",
      "Iteration 10080 Training loss 0.015093239024281502 Validation loss 0.018087178468704224 Accuracy 0.8173828125\n",
      "Iteration 10090 Training loss 0.015894923359155655 Validation loss 0.018080417066812515 Accuracy 0.8173828125\n",
      "Iteration 10100 Training loss 0.01816214993596077 Validation loss 0.018253512680530548 Accuracy 0.81591796875\n",
      "Iteration 10110 Training loss 0.01870553009212017 Validation loss 0.01839618757367134 Accuracy 0.814453125\n",
      "Iteration 10120 Training loss 0.015215644612908363 Validation loss 0.018593190237879753 Accuracy 0.8125\n",
      "Iteration 10130 Training loss 0.016506796702742577 Validation loss 0.018224865198135376 Accuracy 0.81591796875\n",
      "Iteration 10140 Training loss 0.020085634663701057 Validation loss 0.018672259524464607 Accuracy 0.8115234375\n",
      "Iteration 10150 Training loss 0.016756892204284668 Validation loss 0.019121993333101273 Accuracy 0.80712890625\n",
      "Iteration 10160 Training loss 0.017521949484944344 Validation loss 0.01802687533199787 Accuracy 0.81787109375\n",
      "Iteration 10170 Training loss 0.01640448160469532 Validation loss 0.018086591735482216 Accuracy 0.8173828125\n",
      "Iteration 10180 Training loss 0.014354221522808075 Validation loss 0.01813446916639805 Accuracy 0.8173828125\n",
      "Iteration 10190 Training loss 0.019599691033363342 Validation loss 0.018493859097361565 Accuracy 0.8134765625\n",
      "Iteration 10200 Training loss 0.017709894105792046 Validation loss 0.01808304525911808 Accuracy 0.8173828125\n",
      "Iteration 10210 Training loss 0.017940599471330643 Validation loss 0.018358884379267693 Accuracy 0.81494140625\n",
      "Iteration 10220 Training loss 0.01745625212788582 Validation loss 0.018288467079401016 Accuracy 0.81494140625\n",
      "Iteration 10230 Training loss 0.01388105470687151 Validation loss 0.01803756132721901 Accuracy 0.81787109375\n",
      "Iteration 10240 Training loss 0.019982242956757545 Validation loss 0.01805410161614418 Accuracy 0.818359375\n",
      "Iteration 10250 Training loss 0.01607494428753853 Validation loss 0.01793854869902134 Accuracy 0.8193359375\n",
      "Iteration 10260 Training loss 0.015911519527435303 Validation loss 0.018785133957862854 Accuracy 0.81005859375\n",
      "Iteration 10270 Training loss 0.018175462260842323 Validation loss 0.01983811892569065 Accuracy 0.80029296875\n",
      "Iteration 10280 Training loss 0.017193321138620377 Validation loss 0.01931082084774971 Accuracy 0.80419921875\n",
      "Iteration 10290 Training loss 0.014604405499994755 Validation loss 0.018589965999126434 Accuracy 0.8125\n",
      "Iteration 10300 Training loss 0.01559238787740469 Validation loss 0.018056340515613556 Accuracy 0.81787109375\n",
      "Iteration 10310 Training loss 0.015180016867816448 Validation loss 0.018763216212391853 Accuracy 0.810546875\n",
      "Iteration 10320 Training loss 0.016246968880295753 Validation loss 0.0180745217949152 Accuracy 0.81787109375\n",
      "Iteration 10330 Training loss 0.01986207254230976 Validation loss 0.018440987914800644 Accuracy 0.8134765625\n",
      "Iteration 10340 Training loss 0.017243199050426483 Validation loss 0.0179657693952322 Accuracy 0.81884765625\n",
      "Iteration 10350 Training loss 0.014993351884186268 Validation loss 0.017745213583111763 Accuracy 0.82080078125\n",
      "Iteration 10360 Training loss 0.018101656809449196 Validation loss 0.01787860505282879 Accuracy 0.8193359375\n",
      "Iteration 10370 Training loss 0.016039405018091202 Validation loss 0.018067961558699608 Accuracy 0.8173828125\n",
      "Iteration 10380 Training loss 0.014856952242553234 Validation loss 0.018172737210989 Accuracy 0.81689453125\n",
      "Iteration 10390 Training loss 0.017454922199249268 Validation loss 0.018415985628962517 Accuracy 0.81396484375\n",
      "Iteration 10400 Training loss 0.016437772661447525 Validation loss 0.0181360375136137 Accuracy 0.81689453125\n",
      "Iteration 10410 Training loss 0.01617535576224327 Validation loss 0.01807340607047081 Accuracy 0.81787109375\n",
      "Iteration 10420 Training loss 0.017609501257538795 Validation loss 0.017933692783117294 Accuracy 0.818359375\n",
      "Iteration 10430 Training loss 0.017223728820681572 Validation loss 0.018599288538098335 Accuracy 0.81201171875\n",
      "Iteration 10440 Training loss 0.015249628573656082 Validation loss 0.01821756735444069 Accuracy 0.81591796875\n",
      "Iteration 10450 Training loss 0.015721602365374565 Validation loss 0.018074188381433487 Accuracy 0.81787109375\n",
      "Iteration 10460 Training loss 0.01783219538629055 Validation loss 0.018383312970399857 Accuracy 0.814453125\n",
      "Iteration 10470 Training loss 0.016864681616425514 Validation loss 0.01814476028084755 Accuracy 0.81689453125\n",
      "Iteration 10480 Training loss 0.01518088299781084 Validation loss 0.017970118671655655 Accuracy 0.81787109375\n",
      "Iteration 10490 Training loss 0.015608551912009716 Validation loss 0.017910238355398178 Accuracy 0.81884765625\n",
      "Iteration 10500 Training loss 0.01728232018649578 Validation loss 0.01781206950545311 Accuracy 0.81982421875\n",
      "Iteration 10510 Training loss 0.017264872789382935 Validation loss 0.01799432560801506 Accuracy 0.818359375\n",
      "Iteration 10520 Training loss 0.019293470308184624 Validation loss 0.018099751323461533 Accuracy 0.81689453125\n",
      "Iteration 10530 Training loss 0.01483619399368763 Validation loss 0.017962336540222168 Accuracy 0.818359375\n",
      "Iteration 10540 Training loss 0.017035381868481636 Validation loss 0.018045766279101372 Accuracy 0.81787109375\n",
      "Iteration 10550 Training loss 0.016568774357438087 Validation loss 0.018995415419340134 Accuracy 0.80859375\n",
      "Iteration 10560 Training loss 0.014460472390055656 Validation loss 0.0177314430475235 Accuracy 0.8212890625\n",
      "Iteration 10570 Training loss 0.018275458365678787 Validation loss 0.018931996077299118 Accuracy 0.8095703125\n",
      "Iteration 10580 Training loss 0.016421431675553322 Validation loss 0.017784513533115387 Accuracy 0.8203125\n",
      "Iteration 10590 Training loss 0.016235414892435074 Validation loss 0.017784519121050835 Accuracy 0.82080078125\n",
      "Iteration 10600 Training loss 0.0126033378764987 Validation loss 0.018043112009763718 Accuracy 0.81787109375\n",
      "Iteration 10610 Training loss 0.018262870609760284 Validation loss 0.0185887198895216 Accuracy 0.81201171875\n",
      "Iteration 10620 Training loss 0.017656361684203148 Validation loss 0.01891033537685871 Accuracy 0.80908203125\n",
      "Iteration 10630 Training loss 0.016251537948846817 Validation loss 0.01848256029188633 Accuracy 0.814453125\n",
      "Iteration 10640 Training loss 0.017396988347172737 Validation loss 0.017956532537937164 Accuracy 0.81884765625\n",
      "Iteration 10650 Training loss 0.016072887927293777 Validation loss 0.01805158145725727 Accuracy 0.81787109375\n",
      "Iteration 10660 Training loss 0.018042048439383507 Validation loss 0.017830951139330864 Accuracy 0.8203125\n",
      "Iteration 10670 Training loss 0.015981236472725868 Validation loss 0.018078502267599106 Accuracy 0.8173828125\n",
      "Iteration 10680 Training loss 0.021281659603118896 Validation loss 0.01804017648100853 Accuracy 0.818359375\n",
      "Iteration 10690 Training loss 0.01689012721180916 Validation loss 0.018252991139888763 Accuracy 0.81591796875\n",
      "Iteration 10700 Training loss 0.01589859277009964 Validation loss 0.017993565648794174 Accuracy 0.81884765625\n",
      "Iteration 10710 Training loss 0.01753356121480465 Validation loss 0.01851678267121315 Accuracy 0.81298828125\n",
      "Iteration 10720 Training loss 0.017567194998264313 Validation loss 0.017676787450909615 Accuracy 0.8212890625\n",
      "Iteration 10730 Training loss 0.01927792839705944 Validation loss 0.018463602289557457 Accuracy 0.81396484375\n",
      "Iteration 10740 Training loss 0.015224471688270569 Validation loss 0.01778697781264782 Accuracy 0.82080078125\n",
      "Iteration 10750 Training loss 0.014139319770038128 Validation loss 0.01798436790704727 Accuracy 0.818359375\n",
      "Iteration 10760 Training loss 0.01711990311741829 Validation loss 0.01828763633966446 Accuracy 0.8154296875\n",
      "Iteration 10770 Training loss 0.020550450310111046 Validation loss 0.01980266533792019 Accuracy 0.80029296875\n",
      "Iteration 10780 Training loss 0.014939229004085064 Validation loss 0.018335694447159767 Accuracy 0.814453125\n",
      "Iteration 10790 Training loss 0.014926684089004993 Validation loss 0.019301440566778183 Accuracy 0.80517578125\n",
      "Iteration 10800 Training loss 0.015540770255029202 Validation loss 0.017950478941202164 Accuracy 0.8193359375\n",
      "Iteration 10810 Training loss 0.013588955625891685 Validation loss 0.017785893753170967 Accuracy 0.82080078125\n",
      "Iteration 10820 Training loss 0.017179032787680626 Validation loss 0.01824069209396839 Accuracy 0.81591796875\n",
      "Iteration 10830 Training loss 0.01863677054643631 Validation loss 0.018148597329854965 Accuracy 0.81689453125\n",
      "Iteration 10840 Training loss 0.01576070673763752 Validation loss 0.01819750852882862 Accuracy 0.81640625\n",
      "Iteration 10850 Training loss 0.01777981035411358 Validation loss 0.018840212374925613 Accuracy 0.8095703125\n",
      "Iteration 10860 Training loss 0.015822483226656914 Validation loss 0.0179742481559515 Accuracy 0.81884765625\n",
      "Iteration 10870 Training loss 0.015545682050287724 Validation loss 0.018152080476284027 Accuracy 0.8173828125\n",
      "Iteration 10880 Training loss 0.01776461862027645 Validation loss 0.017969777807593346 Accuracy 0.81884765625\n",
      "Iteration 10890 Training loss 0.017290065065026283 Validation loss 0.01785113848745823 Accuracy 0.8193359375\n",
      "Iteration 10900 Training loss 0.017085738480091095 Validation loss 0.017662495374679565 Accuracy 0.8212890625\n",
      "Iteration 10910 Training loss 0.015989618375897408 Validation loss 0.01881132833659649 Accuracy 0.810546875\n",
      "Iteration 10920 Training loss 0.021424928680062294 Validation loss 0.019579755142331123 Accuracy 0.80224609375\n",
      "Iteration 10930 Training loss 0.018316863104701042 Validation loss 0.018456166610121727 Accuracy 0.81298828125\n",
      "Iteration 10940 Training loss 0.01836434006690979 Validation loss 0.018178611993789673 Accuracy 0.81591796875\n",
      "Iteration 10950 Training loss 0.01795070432126522 Validation loss 0.01863699220120907 Accuracy 0.81201171875\n",
      "Iteration 10960 Training loss 0.01340144220739603 Validation loss 0.017713679000735283 Accuracy 0.82177734375\n",
      "Iteration 10970 Training loss 0.015228459611535072 Validation loss 0.01844242587685585 Accuracy 0.8134765625\n",
      "Iteration 10980 Training loss 0.01577727310359478 Validation loss 0.018036630004644394 Accuracy 0.81787109375\n",
      "Iteration 10990 Training loss 0.01877918280661106 Validation loss 0.01844746060669422 Accuracy 0.81396484375\n",
      "Iteration 11000 Training loss 0.016307063400745392 Validation loss 0.017674356698989868 Accuracy 0.82177734375\n",
      "Iteration 11010 Training loss 0.01712506264448166 Validation loss 0.01763012818992138 Accuracy 0.822265625\n",
      "Iteration 11020 Training loss 0.0182159636169672 Validation loss 0.018538303673267365 Accuracy 0.8125\n",
      "Iteration 11030 Training loss 0.016088353469967842 Validation loss 0.01817040517926216 Accuracy 0.81640625\n",
      "Iteration 11040 Training loss 0.014165399596095085 Validation loss 0.01791256107389927 Accuracy 0.81884765625\n",
      "Iteration 11050 Training loss 0.016684237867593765 Validation loss 0.018974997103214264 Accuracy 0.80859375\n",
      "Iteration 11060 Training loss 0.01584254391491413 Validation loss 0.017785226926207542 Accuracy 0.81982421875\n",
      "Iteration 11070 Training loss 0.017665596678853035 Validation loss 0.018773315474390984 Accuracy 0.81005859375\n",
      "Iteration 11080 Training loss 0.015706190839409828 Validation loss 0.017689434811472893 Accuracy 0.8212890625\n",
      "Iteration 11090 Training loss 0.015499720349907875 Validation loss 0.0182015523314476 Accuracy 0.81591796875\n",
      "Iteration 11100 Training loss 0.01859506219625473 Validation loss 0.01838056929409504 Accuracy 0.814453125\n",
      "Iteration 11110 Training loss 0.01685309037566185 Validation loss 0.017919136211276054 Accuracy 0.81884765625\n",
      "Iteration 11120 Training loss 0.016857612878084183 Validation loss 0.017840681597590446 Accuracy 0.8203125\n",
      "Iteration 11130 Training loss 0.019807929173111916 Validation loss 0.01979786902666092 Accuracy 0.80029296875\n",
      "Iteration 11140 Training loss 0.017382381483912468 Validation loss 0.017963765189051628 Accuracy 0.818359375\n",
      "Iteration 11150 Training loss 0.013418244197964668 Validation loss 0.017916297540068626 Accuracy 0.81884765625\n",
      "Iteration 11160 Training loss 0.014629248529672623 Validation loss 0.01828332059085369 Accuracy 0.81591796875\n",
      "Iteration 11170 Training loss 0.01662573777139187 Validation loss 0.01845659874379635 Accuracy 0.81298828125\n",
      "Iteration 11180 Training loss 0.015340899117290974 Validation loss 0.018068833276629448 Accuracy 0.8173828125\n",
      "Iteration 11190 Training loss 0.016537576913833618 Validation loss 0.0181962251663208 Accuracy 0.81591796875\n",
      "Iteration 11200 Training loss 0.016097333282232285 Validation loss 0.018059050664305687 Accuracy 0.81787109375\n",
      "Iteration 11210 Training loss 0.018714433535933495 Validation loss 0.01821514591574669 Accuracy 0.8154296875\n",
      "Iteration 11220 Training loss 0.017017796635627747 Validation loss 0.01783444918692112 Accuracy 0.81982421875\n",
      "Iteration 11230 Training loss 0.013295505195856094 Validation loss 0.017879042774438858 Accuracy 0.8193359375\n",
      "Iteration 11240 Training loss 0.01479889452457428 Validation loss 0.01870615966618061 Accuracy 0.81103515625\n",
      "Iteration 11250 Training loss 0.015337798744440079 Validation loss 0.018782230094075203 Accuracy 0.81005859375\n",
      "Iteration 11260 Training loss 0.01876436546444893 Validation loss 0.019221894443035126 Accuracy 0.80615234375\n",
      "Iteration 11270 Training loss 0.016903221607208252 Validation loss 0.018328245729207993 Accuracy 0.81494140625\n",
      "Iteration 11280 Training loss 0.015782250091433525 Validation loss 0.017935670912265778 Accuracy 0.818359375\n",
      "Iteration 11290 Training loss 0.018382739275693893 Validation loss 0.018559210002422333 Accuracy 0.81298828125\n",
      "Iteration 11300 Training loss 0.016963571310043335 Validation loss 0.018752029165625572 Accuracy 0.810546875\n",
      "Iteration 11310 Training loss 0.018065854907035828 Validation loss 0.01800183579325676 Accuracy 0.818359375\n",
      "Iteration 11320 Training loss 0.015055837109684944 Validation loss 0.017890362069010735 Accuracy 0.8193359375\n",
      "Iteration 11330 Training loss 0.017152559012174606 Validation loss 0.018962206318974495 Accuracy 0.80810546875\n",
      "Iteration 11340 Training loss 0.018430739641189575 Validation loss 0.01783757098019123 Accuracy 0.8193359375\n",
      "Iteration 11350 Training loss 0.017566706985235214 Validation loss 0.017727546393871307 Accuracy 0.8212890625\n",
      "Iteration 11360 Training loss 0.015514564700424671 Validation loss 0.01827802136540413 Accuracy 0.8154296875\n",
      "Iteration 11370 Training loss 0.016232412308454514 Validation loss 0.019590646028518677 Accuracy 0.8017578125\n",
      "Iteration 11380 Training loss 0.015234487131237984 Validation loss 0.01804335229098797 Accuracy 0.818359375\n",
      "Iteration 11390 Training loss 0.013314919546246529 Validation loss 0.01787855103611946 Accuracy 0.81982421875\n",
      "Iteration 11400 Training loss 0.012777038849890232 Validation loss 0.017925458028912544 Accuracy 0.81884765625\n",
      "Iteration 11410 Training loss 0.01966613158583641 Validation loss 0.017895983532071114 Accuracy 0.8193359375\n",
      "Iteration 11420 Training loss 0.017505042254924774 Validation loss 0.01802035979926586 Accuracy 0.81787109375\n",
      "Iteration 11430 Training loss 0.016755366697907448 Validation loss 0.017974184826016426 Accuracy 0.818359375\n",
      "Iteration 11440 Training loss 0.01737622730433941 Validation loss 0.017551515251398087 Accuracy 0.82275390625\n",
      "Iteration 11450 Training loss 0.01803136244416237 Validation loss 0.018205637112259865 Accuracy 0.81591796875\n",
      "Iteration 11460 Training loss 0.01526722963899374 Validation loss 0.017672276124358177 Accuracy 0.8212890625\n",
      "Iteration 11470 Training loss 0.014833541586995125 Validation loss 0.017684582620859146 Accuracy 0.82080078125\n",
      "Iteration 11480 Training loss 0.016598032787442207 Validation loss 0.01786859519779682 Accuracy 0.8193359375\n",
      "Iteration 11490 Training loss 0.014443982392549515 Validation loss 0.017832793295383453 Accuracy 0.81982421875\n",
      "Iteration 11500 Training loss 0.01851651445031166 Validation loss 0.017700672149658203 Accuracy 0.8212890625\n",
      "Iteration 11510 Training loss 0.01688428409397602 Validation loss 0.01797465607523918 Accuracy 0.818359375\n",
      "Iteration 11520 Training loss 0.014334093779325485 Validation loss 0.017562102526426315 Accuracy 0.8232421875\n",
      "Iteration 11530 Training loss 0.015344960615038872 Validation loss 0.01768467202782631 Accuracy 0.82177734375\n",
      "Iteration 11540 Training loss 0.015692662447690964 Validation loss 0.018110021948814392 Accuracy 0.81640625\n",
      "Iteration 11550 Training loss 0.018160322681069374 Validation loss 0.01886088401079178 Accuracy 0.80908203125\n",
      "Iteration 11560 Training loss 0.015809426084160805 Validation loss 0.017533238977193832 Accuracy 0.82275390625\n",
      "Iteration 11570 Training loss 0.01833656243979931 Validation loss 0.01766311191022396 Accuracy 0.82177734375\n",
      "Iteration 11580 Training loss 0.01602853089570999 Validation loss 0.01776091195642948 Accuracy 0.82080078125\n",
      "Iteration 11590 Training loss 0.013635149225592613 Validation loss 0.017621492967009544 Accuracy 0.82177734375\n",
      "Iteration 11600 Training loss 0.017946256324648857 Validation loss 0.017975453287363052 Accuracy 0.818359375\n",
      "Iteration 11610 Training loss 0.016319941729307175 Validation loss 0.017687443643808365 Accuracy 0.82177734375\n",
      "Iteration 11620 Training loss 0.01725183241069317 Validation loss 0.01776192896068096 Accuracy 0.8203125\n",
      "Iteration 11630 Training loss 0.015485895797610283 Validation loss 0.017997823655605316 Accuracy 0.818359375\n",
      "Iteration 11640 Training loss 0.018805036321282387 Validation loss 0.018474066630005836 Accuracy 0.81396484375\n",
      "Iteration 11650 Training loss 0.013241451233625412 Validation loss 0.017706111073493958 Accuracy 0.82177734375\n",
      "Iteration 11660 Training loss 0.01551486924290657 Validation loss 0.01776372268795967 Accuracy 0.82080078125\n",
      "Iteration 11670 Training loss 0.01925875060260296 Validation loss 0.017625248059630394 Accuracy 0.822265625\n",
      "Iteration 11680 Training loss 0.01816450245678425 Validation loss 0.017969805747270584 Accuracy 0.818359375\n",
      "Iteration 11690 Training loss 0.013753745704889297 Validation loss 0.017920782789587975 Accuracy 0.81884765625\n",
      "Iteration 11700 Training loss 0.018228236585855484 Validation loss 0.018086334690451622 Accuracy 0.8173828125\n",
      "Iteration 11710 Training loss 0.01677706092596054 Validation loss 0.019047679379582405 Accuracy 0.80712890625\n",
      "Iteration 11720 Training loss 0.016784125939011574 Validation loss 0.018046380952000618 Accuracy 0.81787109375\n",
      "Iteration 11730 Training loss 0.013660120777785778 Validation loss 0.017580578103661537 Accuracy 0.82275390625\n",
      "Iteration 11740 Training loss 0.017689989879727364 Validation loss 0.018179602921009064 Accuracy 0.81640625\n",
      "Iteration 11750 Training loss 0.01310441829264164 Validation loss 0.017703749239444733 Accuracy 0.8212890625\n",
      "Iteration 11760 Training loss 0.016480669379234314 Validation loss 0.01783016324043274 Accuracy 0.81982421875\n",
      "Iteration 11770 Training loss 0.0166665930300951 Validation loss 0.017285099253058434 Accuracy 0.8251953125\n",
      "Iteration 11780 Training loss 0.015826310962438583 Validation loss 0.017328577116131783 Accuracy 0.8251953125\n",
      "Iteration 11790 Training loss 0.01644802652299404 Validation loss 0.017745908349752426 Accuracy 0.82080078125\n",
      "Iteration 11800 Training loss 0.017926739528775215 Validation loss 0.017731808125972748 Accuracy 0.8203125\n",
      "Iteration 11810 Training loss 0.015742242336273193 Validation loss 0.017733419314026833 Accuracy 0.82080078125\n",
      "Iteration 11820 Training loss 0.015411998145282269 Validation loss 0.018683360889554024 Accuracy 0.81103515625\n",
      "Iteration 11830 Training loss 0.015246263705193996 Validation loss 0.018291857093572617 Accuracy 0.8154296875\n",
      "Iteration 11840 Training loss 0.01647060737013817 Validation loss 0.018725885078310966 Accuracy 0.81103515625\n",
      "Iteration 11850 Training loss 0.015938451513648033 Validation loss 0.018027089536190033 Accuracy 0.8173828125\n",
      "Iteration 11860 Training loss 0.016530897468328476 Validation loss 0.01817399635910988 Accuracy 0.81591796875\n",
      "Iteration 11870 Training loss 0.01452283188700676 Validation loss 0.017488226294517517 Accuracy 0.82373046875\n",
      "Iteration 11880 Training loss 0.016125846654176712 Validation loss 0.01785397157073021 Accuracy 0.81982421875\n",
      "Iteration 11890 Training loss 0.013591781258583069 Validation loss 0.01780114509165287 Accuracy 0.8203125\n",
      "Iteration 11900 Training loss 0.015310399234294891 Validation loss 0.017493827268481255 Accuracy 0.82373046875\n",
      "Iteration 11910 Training loss 0.014792246744036674 Validation loss 0.017886746674776077 Accuracy 0.81884765625\n",
      "Iteration 11920 Training loss 0.01671517826616764 Validation loss 0.017653485760092735 Accuracy 0.82177734375\n",
      "Iteration 11930 Training loss 0.018385788425803185 Validation loss 0.018480325117707253 Accuracy 0.81298828125\n",
      "Iteration 11940 Training loss 0.015803972259163857 Validation loss 0.01766621135175228 Accuracy 0.8212890625\n",
      "Iteration 11950 Training loss 0.017164286226034164 Validation loss 0.01833583414554596 Accuracy 0.81494140625\n",
      "Iteration 11960 Training loss 0.01576850563287735 Validation loss 0.01765829510986805 Accuracy 0.8212890625\n",
      "Iteration 11970 Training loss 0.017225084826350212 Validation loss 0.017672469839453697 Accuracy 0.8212890625\n",
      "Iteration 11980 Training loss 0.016346804797649384 Validation loss 0.017615243792533875 Accuracy 0.822265625\n",
      "Iteration 11990 Training loss 0.02037803828716278 Validation loss 0.01885777711868286 Accuracy 0.80908203125\n",
      "Iteration 12000 Training loss 0.015960346907377243 Validation loss 0.01845640316605568 Accuracy 0.8134765625\n",
      "Iteration 12010 Training loss 0.015405278652906418 Validation loss 0.01751718483865261 Accuracy 0.8232421875\n",
      "Iteration 12020 Training loss 0.015148228965699673 Validation loss 0.01746448129415512 Accuracy 0.82373046875\n",
      "Iteration 12030 Training loss 0.014942605048418045 Validation loss 0.017720725387334824 Accuracy 0.8203125\n",
      "Iteration 12040 Training loss 0.014929967001080513 Validation loss 0.017523277550935745 Accuracy 0.8232421875\n",
      "Iteration 12050 Training loss 0.019673092290759087 Validation loss 0.018491849303245544 Accuracy 0.81298828125\n",
      "Iteration 12060 Training loss 0.017167961224913597 Validation loss 0.01813894882798195 Accuracy 0.81689453125\n",
      "Iteration 12070 Training loss 0.01308425609022379 Validation loss 0.017903834581375122 Accuracy 0.8193359375\n",
      "Iteration 12080 Training loss 0.015515140257775784 Validation loss 0.017657140269875526 Accuracy 0.82177734375\n",
      "Iteration 12090 Training loss 0.0177532397210598 Validation loss 0.01911137066781521 Accuracy 0.80615234375\n",
      "Iteration 12100 Training loss 0.01626967266201973 Validation loss 0.017549987882375717 Accuracy 0.822265625\n",
      "Iteration 12110 Training loss 0.01593112386763096 Validation loss 0.017759358510375023 Accuracy 0.82080078125\n",
      "Iteration 12120 Training loss 0.018204044550657272 Validation loss 0.018152175471186638 Accuracy 0.81640625\n",
      "Iteration 12130 Training loss 0.01926477812230587 Validation loss 0.01756748929619789 Accuracy 0.82275390625\n",
      "Iteration 12140 Training loss 0.01767147146165371 Validation loss 0.017797814682126045 Accuracy 0.8203125\n",
      "Iteration 12150 Training loss 0.015569210983812809 Validation loss 0.018203534185886383 Accuracy 0.81640625\n",
      "Iteration 12160 Training loss 0.017314335331320763 Validation loss 0.01805703155696392 Accuracy 0.8173828125\n",
      "Iteration 12170 Training loss 0.016835542395710945 Validation loss 0.01777225360274315 Accuracy 0.8203125\n",
      "Iteration 12180 Training loss 0.01663338951766491 Validation loss 0.017835799604654312 Accuracy 0.81982421875\n",
      "Iteration 12190 Training loss 0.014664591290056705 Validation loss 0.0176908690482378 Accuracy 0.82080078125\n",
      "Iteration 12200 Training loss 0.016775265336036682 Validation loss 0.017621595412492752 Accuracy 0.822265625\n",
      "Iteration 12210 Training loss 0.015071597881615162 Validation loss 0.017683451995253563 Accuracy 0.8212890625\n",
      "Iteration 12220 Training loss 0.01420163456350565 Validation loss 0.01768454536795616 Accuracy 0.82177734375\n",
      "Iteration 12230 Training loss 0.015734529122710228 Validation loss 0.017869625240564346 Accuracy 0.81982421875\n",
      "Iteration 12240 Training loss 0.014356380328536034 Validation loss 0.01836608536541462 Accuracy 0.81396484375\n",
      "Iteration 12250 Training loss 0.017747843638062477 Validation loss 0.017852777615189552 Accuracy 0.81982421875\n",
      "Iteration 12260 Training loss 0.01412889827042818 Validation loss 0.01780136115849018 Accuracy 0.81982421875\n",
      "Iteration 12270 Training loss 0.01501920260488987 Validation loss 0.01758313551545143 Accuracy 0.82275390625\n",
      "Iteration 12280 Training loss 0.01671873964369297 Validation loss 0.019016021862626076 Accuracy 0.8076171875\n",
      "Iteration 12290 Training loss 0.017012497410178185 Validation loss 0.01839395985007286 Accuracy 0.814453125\n",
      "Iteration 12300 Training loss 0.015459203161299229 Validation loss 0.017417479306459427 Accuracy 0.82421875\n",
      "Iteration 12310 Training loss 0.014973466284573078 Validation loss 0.017777303233742714 Accuracy 0.8203125\n",
      "Iteration 12320 Training loss 0.017062271013855934 Validation loss 0.017688889056444168 Accuracy 0.8212890625\n",
      "Iteration 12330 Training loss 0.01487905066460371 Validation loss 0.017518986016511917 Accuracy 0.82275390625\n",
      "Iteration 12340 Training loss 0.016818681731820107 Validation loss 0.017691390588879585 Accuracy 0.82177734375\n",
      "Iteration 12350 Training loss 0.014021818526089191 Validation loss 0.017448734492063522 Accuracy 0.82421875\n",
      "Iteration 12360 Training loss 0.013467272743582726 Validation loss 0.017501886934041977 Accuracy 0.82275390625\n",
      "Iteration 12370 Training loss 0.016510263085365295 Validation loss 0.017445482313632965 Accuracy 0.82373046875\n",
      "Iteration 12380 Training loss 0.014986556954681873 Validation loss 0.017808593809604645 Accuracy 0.81982421875\n",
      "Iteration 12390 Training loss 0.01825094036757946 Validation loss 0.017830627039074898 Accuracy 0.8203125\n",
      "Iteration 12400 Training loss 0.016174711287021637 Validation loss 0.017736496403813362 Accuracy 0.82080078125\n",
      "Iteration 12410 Training loss 0.01668761484324932 Validation loss 0.01778544671833515 Accuracy 0.8203125\n",
      "Iteration 12420 Training loss 0.016919754445552826 Validation loss 0.01753935217857361 Accuracy 0.82275390625\n",
      "Iteration 12430 Training loss 0.017261667177081108 Validation loss 0.017636770382523537 Accuracy 0.82177734375\n",
      "Iteration 12440 Training loss 0.018328310921788216 Validation loss 0.017779890447854996 Accuracy 0.8203125\n",
      "Iteration 12450 Training loss 0.017309831455349922 Validation loss 0.018039172515273094 Accuracy 0.8173828125\n",
      "Iteration 12460 Training loss 0.016101853922009468 Validation loss 0.017574632540345192 Accuracy 0.822265625\n",
      "Iteration 12470 Training loss 0.016662169247865677 Validation loss 0.018176158890128136 Accuracy 0.81640625\n",
      "Iteration 12480 Training loss 0.017515866085886955 Validation loss 0.018075238913297653 Accuracy 0.8173828125\n",
      "Iteration 12490 Training loss 0.013590656220912933 Validation loss 0.0173905398696661 Accuracy 0.82470703125\n",
      "Iteration 12500 Training loss 0.015650039538741112 Validation loss 0.018409503623843193 Accuracy 0.8134765625\n",
      "Iteration 12510 Training loss 0.016059350222349167 Validation loss 0.018022481352090836 Accuracy 0.81787109375\n",
      "Iteration 12520 Training loss 0.01568629965186119 Validation loss 0.017424877732992172 Accuracy 0.82373046875\n",
      "Iteration 12530 Training loss 0.014464103616774082 Validation loss 0.01780278980731964 Accuracy 0.81982421875\n",
      "Iteration 12540 Training loss 0.017998360097408295 Validation loss 0.01753568835556507 Accuracy 0.82275390625\n",
      "Iteration 12550 Training loss 0.011628844775259495 Validation loss 0.017725255340337753 Accuracy 0.82080078125\n",
      "Iteration 12560 Training loss 0.017376132309436798 Validation loss 0.017822807654738426 Accuracy 0.8203125\n",
      "Iteration 12570 Training loss 0.013846853747963905 Validation loss 0.017835892736911774 Accuracy 0.81982421875\n",
      "Iteration 12580 Training loss 0.01596180535852909 Validation loss 0.018135014921426773 Accuracy 0.81689453125\n",
      "Iteration 12590 Training loss 0.016682177782058716 Validation loss 0.01751861721277237 Accuracy 0.8232421875\n",
      "Iteration 12600 Training loss 0.016616908833384514 Validation loss 0.018415680155158043 Accuracy 0.8134765625\n",
      "Iteration 12610 Training loss 0.019794253632426262 Validation loss 0.017530011013150215 Accuracy 0.82275390625\n",
      "Iteration 12620 Training loss 0.014530746266245842 Validation loss 0.01746019907295704 Accuracy 0.82373046875\n",
      "Iteration 12630 Training loss 0.015656499192118645 Validation loss 0.017587926238775253 Accuracy 0.822265625\n",
      "Iteration 12640 Training loss 0.015116694383323193 Validation loss 0.01794551871716976 Accuracy 0.818359375\n",
      "Iteration 12650 Training loss 0.015407939441502094 Validation loss 0.01773560792207718 Accuracy 0.8203125\n",
      "Iteration 12660 Training loss 0.01875123754143715 Validation loss 0.01935722306370735 Accuracy 0.8056640625\n",
      "Iteration 12670 Training loss 0.013827592134475708 Validation loss 0.01771000400185585 Accuracy 0.8212890625\n",
      "Iteration 12680 Training loss 0.01764184609055519 Validation loss 0.01768755167722702 Accuracy 0.8212890625\n",
      "Iteration 12690 Training loss 0.015193184837698936 Validation loss 0.018031608313322067 Accuracy 0.8173828125\n",
      "Iteration 12700 Training loss 0.014004181139171124 Validation loss 0.017450906336307526 Accuracy 0.82373046875\n",
      "Iteration 12710 Training loss 0.015189417637884617 Validation loss 0.017960434779524803 Accuracy 0.81884765625\n",
      "Iteration 12720 Training loss 0.017058221623301506 Validation loss 0.01817997172474861 Accuracy 0.81640625\n",
      "Iteration 12730 Training loss 0.013146687299013138 Validation loss 0.01762971095740795 Accuracy 0.82177734375\n",
      "Iteration 12740 Training loss 0.015288140624761581 Validation loss 0.017833586782217026 Accuracy 0.81982421875\n",
      "Iteration 12750 Training loss 0.01706433668732643 Validation loss 0.018429772928357124 Accuracy 0.81298828125\n",
      "Iteration 12760 Training loss 0.015713613480329514 Validation loss 0.018367214128375053 Accuracy 0.81396484375\n",
      "Iteration 12770 Training loss 0.016344843432307243 Validation loss 0.018940772861242294 Accuracy 0.80859375\n",
      "Iteration 12780 Training loss 0.015662355348467827 Validation loss 0.01769871823489666 Accuracy 0.8212890625\n",
      "Iteration 12790 Training loss 0.017233308404684067 Validation loss 0.018117370083928108 Accuracy 0.81689453125\n",
      "Iteration 12800 Training loss 0.01557980291545391 Validation loss 0.01749294623732567 Accuracy 0.8232421875\n",
      "Iteration 12810 Training loss 0.017083873972296715 Validation loss 0.017902446910738945 Accuracy 0.8193359375\n",
      "Iteration 12820 Training loss 0.014797857962548733 Validation loss 0.0176096074283123 Accuracy 0.822265625\n",
      "Iteration 12830 Training loss 0.01522346306592226 Validation loss 0.017755303531885147 Accuracy 0.8203125\n",
      "Iteration 12840 Training loss 0.015412397682666779 Validation loss 0.01810343936085701 Accuracy 0.81640625\n",
      "Iteration 12850 Training loss 0.013720860704779625 Validation loss 0.017636999487876892 Accuracy 0.822265625\n",
      "Iteration 12860 Training loss 0.014533748850226402 Validation loss 0.017445622012019157 Accuracy 0.8232421875\n",
      "Iteration 12870 Training loss 0.015053527429699898 Validation loss 0.017544075846672058 Accuracy 0.8232421875\n",
      "Iteration 12880 Training loss 0.016956867650151253 Validation loss 0.017583299428224564 Accuracy 0.822265625\n",
      "Iteration 12890 Training loss 0.012011831626296043 Validation loss 0.017517689615488052 Accuracy 0.8232421875\n",
      "Iteration 12900 Training loss 0.01717408187687397 Validation loss 0.017828665673732758 Accuracy 0.8193359375\n",
      "Iteration 12910 Training loss 0.01728171855211258 Validation loss 0.017540058121085167 Accuracy 0.822265625\n",
      "Iteration 12920 Training loss 0.018786074593663216 Validation loss 0.017504151910543442 Accuracy 0.8232421875\n",
      "Iteration 12930 Training loss 0.017628394067287445 Validation loss 0.017621425911784172 Accuracy 0.8212890625\n",
      "Iteration 12940 Training loss 0.01730051077902317 Validation loss 0.01746392250061035 Accuracy 0.8232421875\n",
      "Iteration 12950 Training loss 0.015090461820363998 Validation loss 0.01750962994992733 Accuracy 0.82275390625\n",
      "Iteration 12960 Training loss 0.015335324220359325 Validation loss 0.017540574073791504 Accuracy 0.82275390625\n",
      "Iteration 12970 Training loss 0.015726760029792786 Validation loss 0.017591159790754318 Accuracy 0.82177734375\n",
      "Iteration 12980 Training loss 0.018031906336545944 Validation loss 0.017437078058719635 Accuracy 0.82373046875\n",
      "Iteration 12990 Training loss 0.01447196863591671 Validation loss 0.017879631370306015 Accuracy 0.8193359375\n",
      "Iteration 13000 Training loss 0.015263920649886131 Validation loss 0.017694814130663872 Accuracy 0.8212890625\n",
      "Iteration 13010 Training loss 0.014784779399633408 Validation loss 0.017430685460567474 Accuracy 0.82373046875\n",
      "Iteration 13020 Training loss 0.015854476019740105 Validation loss 0.017820561304688454 Accuracy 0.81982421875\n",
      "Iteration 13030 Training loss 0.016882801428437233 Validation loss 0.01889508031308651 Accuracy 0.80908203125\n",
      "Iteration 13040 Training loss 0.0176471509039402 Validation loss 0.017538774758577347 Accuracy 0.822265625\n",
      "Iteration 13050 Training loss 0.014450139366090298 Validation loss 0.0181017704308033 Accuracy 0.81689453125\n",
      "Iteration 13060 Training loss 0.013542523607611656 Validation loss 0.017515666782855988 Accuracy 0.82373046875\n",
      "Iteration 13070 Training loss 0.01463850773870945 Validation loss 0.0176736768335104 Accuracy 0.82080078125\n",
      "Iteration 13080 Training loss 0.016458628699183464 Validation loss 0.01812087371945381 Accuracy 0.81689453125\n",
      "Iteration 13090 Training loss 0.016790159046649933 Validation loss 0.017438596114516258 Accuracy 0.82373046875\n",
      "Iteration 13100 Training loss 0.018168658018112183 Validation loss 0.017641527578234673 Accuracy 0.8212890625\n",
      "Iteration 13110 Training loss 0.01765042170882225 Validation loss 0.018116844817996025 Accuracy 0.81640625\n",
      "Iteration 13120 Training loss 0.014181426726281643 Validation loss 0.01793370023369789 Accuracy 0.818359375\n",
      "Iteration 13130 Training loss 0.015582303516566753 Validation loss 0.01803411729633808 Accuracy 0.8173828125\n",
      "Iteration 13140 Training loss 0.016466815024614334 Validation loss 0.01781950518488884 Accuracy 0.81982421875\n",
      "Iteration 13150 Training loss 0.017069894820451736 Validation loss 0.01745733432471752 Accuracy 0.82421875\n",
      "Iteration 13160 Training loss 0.014284100383520126 Validation loss 0.017581624910235405 Accuracy 0.822265625\n",
      "Iteration 13170 Training loss 0.016234351322054863 Validation loss 0.01767747476696968 Accuracy 0.8212890625\n",
      "Iteration 13180 Training loss 0.014606500044465065 Validation loss 0.017438814043998718 Accuracy 0.82373046875\n",
      "Iteration 13190 Training loss 0.014370664022862911 Validation loss 0.017586147412657738 Accuracy 0.822265625\n",
      "Iteration 13200 Training loss 0.01272537000477314 Validation loss 0.017606910318136215 Accuracy 0.82177734375\n",
      "Iteration 13210 Training loss 0.013330193236470222 Validation loss 0.01765122450888157 Accuracy 0.82177734375\n",
      "Iteration 13220 Training loss 0.015046829357743263 Validation loss 0.01745745539665222 Accuracy 0.82373046875\n",
      "Iteration 13230 Training loss 0.01585625484585762 Validation loss 0.017937811091542244 Accuracy 0.81884765625\n",
      "Iteration 13240 Training loss 0.012038005515933037 Validation loss 0.017810577526688576 Accuracy 0.81982421875\n",
      "Iteration 13250 Training loss 0.017804840579628944 Validation loss 0.017637386918067932 Accuracy 0.82177734375\n",
      "Iteration 13260 Training loss 0.016932256519794464 Validation loss 0.01765117235481739 Accuracy 0.8212890625\n",
      "Iteration 13270 Training loss 0.015948893502354622 Validation loss 0.017340976744890213 Accuracy 0.82421875\n",
      "Iteration 13280 Training loss 0.01508038304746151 Validation loss 0.01735980249941349 Accuracy 0.82421875\n",
      "Iteration 13290 Training loss 0.018324188888072968 Validation loss 0.017330190166831017 Accuracy 0.82470703125\n",
      "Iteration 13300 Training loss 0.017067810520529747 Validation loss 0.017508622258901596 Accuracy 0.82275390625\n",
      "Iteration 13310 Training loss 0.016767462715506554 Validation loss 0.018548794090747833 Accuracy 0.81201171875\n",
      "Iteration 13320 Training loss 0.01672106795012951 Validation loss 0.019178351387381554 Accuracy 0.80517578125\n",
      "Iteration 13330 Training loss 0.015302647836506367 Validation loss 0.018089385703206062 Accuracy 0.8173828125\n",
      "Iteration 13340 Training loss 0.014726906083524227 Validation loss 0.01743984781205654 Accuracy 0.82373046875\n",
      "Iteration 13350 Training loss 0.016744626685976982 Validation loss 0.01773855835199356 Accuracy 0.82080078125\n",
      "Iteration 13360 Training loss 0.01814304292201996 Validation loss 0.017329981550574303 Accuracy 0.8251953125\n",
      "Iteration 13370 Training loss 0.01623590849339962 Validation loss 0.01751953922212124 Accuracy 0.822265625\n",
      "Iteration 13380 Training loss 0.0176321379840374 Validation loss 0.017671484500169754 Accuracy 0.8212890625\n",
      "Iteration 13390 Training loss 0.014037912711501122 Validation loss 0.01777266524732113 Accuracy 0.82080078125\n",
      "Iteration 13400 Training loss 0.019139476120471954 Validation loss 0.017880091443657875 Accuracy 0.81884765625\n",
      "Iteration 13410 Training loss 0.015296902507543564 Validation loss 0.017243370413780212 Accuracy 0.82568359375\n",
      "Iteration 13420 Training loss 0.01524569932371378 Validation loss 0.017473846673965454 Accuracy 0.82275390625\n",
      "Iteration 13430 Training loss 0.013281656429171562 Validation loss 0.017688747495412827 Accuracy 0.8212890625\n",
      "Iteration 13440 Training loss 0.0158302653580904 Validation loss 0.018556803464889526 Accuracy 0.81298828125\n",
      "Iteration 13450 Training loss 0.017534367740154266 Validation loss 0.017977871000766754 Accuracy 0.818359375\n",
      "Iteration 13460 Training loss 0.014598185196518898 Validation loss 0.017466342076659203 Accuracy 0.8232421875\n",
      "Iteration 13470 Training loss 0.014439885504543781 Validation loss 0.017447447404265404 Accuracy 0.8232421875\n",
      "Iteration 13480 Training loss 0.015267056412994862 Validation loss 0.018143128603696823 Accuracy 0.81689453125\n",
      "Iteration 13490 Training loss 0.015270957723259926 Validation loss 0.017308581620454788 Accuracy 0.82470703125\n",
      "Iteration 13500 Training loss 0.01204773224890232 Validation loss 0.017569072544574738 Accuracy 0.82177734375\n",
      "Iteration 13510 Training loss 0.017134811729192734 Validation loss 0.01796678826212883 Accuracy 0.818359375\n",
      "Iteration 13520 Training loss 0.016429590061306953 Validation loss 0.01806020922958851 Accuracy 0.81787109375\n",
      "Iteration 13530 Training loss 0.015624077059328556 Validation loss 0.01771588996052742 Accuracy 0.82080078125\n",
      "Iteration 13540 Training loss 0.01647297479212284 Validation loss 0.017688734456896782 Accuracy 0.8212890625\n",
      "Iteration 13550 Training loss 0.015688125044107437 Validation loss 0.01741989515721798 Accuracy 0.82373046875\n",
      "Iteration 13560 Training loss 0.017766494303941727 Validation loss 0.017909714952111244 Accuracy 0.8193359375\n",
      "Iteration 13570 Training loss 0.014184433966875076 Validation loss 0.017191920429468155 Accuracy 0.826171875\n",
      "Iteration 13580 Training loss 0.014333025552332401 Validation loss 0.01742342859506607 Accuracy 0.82421875\n",
      "Iteration 13590 Training loss 0.017384547740221024 Validation loss 0.017597084864974022 Accuracy 0.82177734375\n",
      "Iteration 13600 Training loss 0.013394828885793686 Validation loss 0.01743748039007187 Accuracy 0.8232421875\n",
      "Iteration 13610 Training loss 0.015648361295461655 Validation loss 0.017462540417909622 Accuracy 0.82373046875\n",
      "Iteration 13620 Training loss 0.017204299569129944 Validation loss 0.0172803346067667 Accuracy 0.82568359375\n",
      "Iteration 13630 Training loss 0.015257171355187893 Validation loss 0.01795952022075653 Accuracy 0.818359375\n",
      "Iteration 13640 Training loss 0.015411239117383957 Validation loss 0.017240723595023155 Accuracy 0.82568359375\n",
      "Iteration 13650 Training loss 0.01721196249127388 Validation loss 0.01723497360944748 Accuracy 0.82568359375\n",
      "Iteration 13660 Training loss 0.01638188585639 Validation loss 0.01745629869401455 Accuracy 0.82373046875\n",
      "Iteration 13670 Training loss 0.014789003878831863 Validation loss 0.017331860959529877 Accuracy 0.82470703125\n",
      "Iteration 13680 Training loss 0.014005985110998154 Validation loss 0.017820028588175774 Accuracy 0.8203125\n",
      "Iteration 13690 Training loss 0.015918320044875145 Validation loss 0.017238812521100044 Accuracy 0.82568359375\n",
      "Iteration 13700 Training loss 0.014772091992199421 Validation loss 0.017538735643029213 Accuracy 0.82275390625\n",
      "Iteration 13710 Training loss 0.0155982980504632 Validation loss 0.017421750351786613 Accuracy 0.82373046875\n",
      "Iteration 13720 Training loss 0.01457448210567236 Validation loss 0.017582967877388 Accuracy 0.822265625\n",
      "Iteration 13730 Training loss 0.014494023285806179 Validation loss 0.017304444685578346 Accuracy 0.8251953125\n",
      "Iteration 13740 Training loss 0.013681457377970219 Validation loss 0.017190761864185333 Accuracy 0.826171875\n",
      "Iteration 13750 Training loss 0.015179947949945927 Validation loss 0.017225030809640884 Accuracy 0.82568359375\n",
      "Iteration 13760 Training loss 0.016733935102820396 Validation loss 0.01757780648767948 Accuracy 0.822265625\n",
      "Iteration 13770 Training loss 0.014951410703361034 Validation loss 0.017321975901722908 Accuracy 0.82470703125\n",
      "Iteration 13780 Training loss 0.01243719831109047 Validation loss 0.017366593703627586 Accuracy 0.82421875\n",
      "Iteration 13790 Training loss 0.01602969504892826 Validation loss 0.017126301303505898 Accuracy 0.82666015625\n",
      "Iteration 13800 Training loss 0.01678297482430935 Validation loss 0.01735931634902954 Accuracy 0.8251953125\n",
      "Iteration 13810 Training loss 0.013425744138658047 Validation loss 0.01730176992714405 Accuracy 0.8251953125\n",
      "Iteration 13820 Training loss 0.013895867392420769 Validation loss 0.01725713163614273 Accuracy 0.8251953125\n",
      "Iteration 13830 Training loss 0.01544817816466093 Validation loss 0.01758282631635666 Accuracy 0.822265625\n",
      "Iteration 13840 Training loss 0.015369593165814877 Validation loss 0.017208291217684746 Accuracy 0.82568359375\n",
      "Iteration 13850 Training loss 0.013447649776935577 Validation loss 0.01736830361187458 Accuracy 0.82421875\n",
      "Iteration 13860 Training loss 0.013556581921875477 Validation loss 0.01733355224132538 Accuracy 0.82470703125\n",
      "Iteration 13870 Training loss 0.015296213328838348 Validation loss 0.017790943384170532 Accuracy 0.81982421875\n",
      "Iteration 13880 Training loss 0.016297146677970886 Validation loss 0.017443588003516197 Accuracy 0.82373046875\n",
      "Iteration 13890 Training loss 0.0169011689722538 Validation loss 0.01786859519779682 Accuracy 0.8193359375\n",
      "Iteration 13900 Training loss 0.016949404031038284 Validation loss 0.017945067957043648 Accuracy 0.81787109375\n",
      "Iteration 13910 Training loss 0.015523381531238556 Validation loss 0.017626166343688965 Accuracy 0.82177734375\n",
      "Iteration 13920 Training loss 0.013605089858174324 Validation loss 0.017859090119600296 Accuracy 0.81884765625\n",
      "Iteration 13930 Training loss 0.017879726365208626 Validation loss 0.017690369859337807 Accuracy 0.82080078125\n",
      "Iteration 13940 Training loss 0.015810925513505936 Validation loss 0.018478600308299065 Accuracy 0.81396484375\n",
      "Iteration 13950 Training loss 0.017152871936559677 Validation loss 0.017342276871204376 Accuracy 0.8251953125\n",
      "Iteration 13960 Training loss 0.014587980695068836 Validation loss 0.01781177893280983 Accuracy 0.81982421875\n",
      "Iteration 13970 Training loss 0.01995602622628212 Validation loss 0.01743963360786438 Accuracy 0.82373046875\n",
      "Iteration 13980 Training loss 0.017669271677732468 Validation loss 0.017335759475827217 Accuracy 0.82421875\n",
      "Iteration 13990 Training loss 0.015927180647850037 Validation loss 0.0173212718218565 Accuracy 0.82470703125\n",
      "Iteration 14000 Training loss 0.01917051337659359 Validation loss 0.017515063285827637 Accuracy 0.8232421875\n",
      "Iteration 14010 Training loss 0.014822944067418575 Validation loss 0.017970891669392586 Accuracy 0.818359375\n",
      "Iteration 14020 Training loss 0.01620544120669365 Validation loss 0.018387287855148315 Accuracy 0.814453125\n",
      "Iteration 14030 Training loss 0.01735037751495838 Validation loss 0.01759451813995838 Accuracy 0.822265625\n",
      "Iteration 14040 Training loss 0.014973943121731281 Validation loss 0.017202384769916534 Accuracy 0.826171875\n",
      "Iteration 14050 Training loss 0.015099916607141495 Validation loss 0.017685458064079285 Accuracy 0.8212890625\n",
      "Iteration 14060 Training loss 0.01487622782588005 Validation loss 0.01884419657289982 Accuracy 0.80908203125\n",
      "Iteration 14070 Training loss 0.015411094762384892 Validation loss 0.017616422846913338 Accuracy 0.82177734375\n",
      "Iteration 14080 Training loss 0.018312890082597733 Validation loss 0.0175489354878664 Accuracy 0.822265625\n",
      "Iteration 14090 Training loss 0.014036610722541809 Validation loss 0.017452169209718704 Accuracy 0.8232421875\n",
      "Iteration 14100 Training loss 0.015374209731817245 Validation loss 0.018205750733613968 Accuracy 0.81591796875\n",
      "Iteration 14110 Training loss 0.01468617469072342 Validation loss 0.017300505191087723 Accuracy 0.8251953125\n",
      "Iteration 14120 Training loss 0.01588631235063076 Validation loss 0.017211299389600754 Accuracy 0.82568359375\n",
      "Iteration 14130 Training loss 0.01481134258210659 Validation loss 0.01742037758231163 Accuracy 0.82373046875\n",
      "Iteration 14140 Training loss 0.01498387474566698 Validation loss 0.017553865909576416 Accuracy 0.822265625\n",
      "Iteration 14150 Training loss 0.01657721772789955 Validation loss 0.01717308536171913 Accuracy 0.826171875\n",
      "Iteration 14160 Training loss 0.017190318554639816 Validation loss 0.017224809154868126 Accuracy 0.826171875\n",
      "Iteration 14170 Training loss 0.014819676987826824 Validation loss 0.01769472286105156 Accuracy 0.8203125\n",
      "Iteration 14180 Training loss 0.01605086959898472 Validation loss 0.017633700743317604 Accuracy 0.82177734375\n",
      "Iteration 14190 Training loss 0.016491493210196495 Validation loss 0.017748437821865082 Accuracy 0.82080078125\n",
      "Iteration 14200 Training loss 0.013351721689105034 Validation loss 0.017469311133027077 Accuracy 0.82373046875\n",
      "Iteration 14210 Training loss 0.017000624909996986 Validation loss 0.017657576128840446 Accuracy 0.8212890625\n",
      "Iteration 14220 Training loss 0.015402975492179394 Validation loss 0.01709134504199028 Accuracy 0.82666015625\n",
      "Iteration 14230 Training loss 0.01476614736020565 Validation loss 0.01751467026770115 Accuracy 0.8232421875\n",
      "Iteration 14240 Training loss 0.014199592173099518 Validation loss 0.017346927896142006 Accuracy 0.82470703125\n",
      "Iteration 14250 Training loss 0.013881975784897804 Validation loss 0.017453493550419807 Accuracy 0.8232421875\n",
      "Iteration 14260 Training loss 0.015153234824538231 Validation loss 0.0173971988260746 Accuracy 0.82373046875\n",
      "Iteration 14270 Training loss 0.016324704512953758 Validation loss 0.017157744616270065 Accuracy 0.826171875\n",
      "Iteration 14280 Training loss 0.01745111122727394 Validation loss 0.017339356243610382 Accuracy 0.82470703125\n",
      "Iteration 14290 Training loss 0.014220235869288445 Validation loss 0.01767183654010296 Accuracy 0.82080078125\n",
      "Iteration 14300 Training loss 0.016682665795087814 Validation loss 0.017284853383898735 Accuracy 0.8251953125\n",
      "Iteration 14310 Training loss 0.015331064350903034 Validation loss 0.01731426641345024 Accuracy 0.8251953125\n",
      "Iteration 14320 Training loss 0.014155802316963673 Validation loss 0.017301104962825775 Accuracy 0.82470703125\n",
      "Iteration 14330 Training loss 0.014211728237569332 Validation loss 0.017223911359906197 Accuracy 0.82568359375\n",
      "Iteration 14340 Training loss 0.016582952812314034 Validation loss 0.017567459493875504 Accuracy 0.82177734375\n",
      "Iteration 14350 Training loss 0.017087288200855255 Validation loss 0.017467858269810677 Accuracy 0.82275390625\n",
      "Iteration 14360 Training loss 0.015025108121335506 Validation loss 0.01747850701212883 Accuracy 0.8232421875\n",
      "Iteration 14370 Training loss 0.01335717923939228 Validation loss 0.017198147252202034 Accuracy 0.82568359375\n",
      "Iteration 14380 Training loss 0.0130526227876544 Validation loss 0.017901208251714706 Accuracy 0.818359375\n",
      "Iteration 14390 Training loss 0.014810224063694477 Validation loss 0.01733606867492199 Accuracy 0.82421875\n",
      "Iteration 14400 Training loss 0.01751432940363884 Validation loss 0.01736171916127205 Accuracy 0.82470703125\n",
      "Iteration 14410 Training loss 0.01717245951294899 Validation loss 0.017598513513803482 Accuracy 0.82177734375\n",
      "Iteration 14420 Training loss 0.015398030169308186 Validation loss 0.01748642511665821 Accuracy 0.822265625\n",
      "Iteration 14430 Training loss 0.014909887686371803 Validation loss 0.01738288439810276 Accuracy 0.82421875\n",
      "Iteration 14440 Training loss 0.01768021285533905 Validation loss 0.017324794083833694 Accuracy 0.82421875\n",
      "Iteration 14450 Training loss 0.014215441420674324 Validation loss 0.01759590208530426 Accuracy 0.82177734375\n",
      "Iteration 14460 Training loss 0.014772986061871052 Validation loss 0.017530376091599464 Accuracy 0.8232421875\n",
      "Iteration 14470 Training loss 0.01868663728237152 Validation loss 0.01811465062201023 Accuracy 0.81640625\n",
      "Iteration 14480 Training loss 0.01587701588869095 Validation loss 0.017908349633216858 Accuracy 0.81884765625\n",
      "Iteration 14490 Training loss 0.01560721080750227 Validation loss 0.017287062481045723 Accuracy 0.82470703125\n",
      "Iteration 14500 Training loss 0.015115151181817055 Validation loss 0.018097123131155968 Accuracy 0.81640625\n",
      "Iteration 14510 Training loss 0.015353092923760414 Validation loss 0.01802380010485649 Accuracy 0.81787109375\n",
      "Iteration 14520 Training loss 0.013964259065687656 Validation loss 0.0171575378626585 Accuracy 0.826171875\n",
      "Iteration 14530 Training loss 0.015840018168091774 Validation loss 0.01753145270049572 Accuracy 0.82275390625\n",
      "Iteration 14540 Training loss 0.013726641424000263 Validation loss 0.017293022945523262 Accuracy 0.8251953125\n",
      "Iteration 14550 Training loss 0.015482057817280293 Validation loss 0.018051689490675926 Accuracy 0.8173828125\n",
      "Iteration 14560 Training loss 0.014042009599506855 Validation loss 0.018007537350058556 Accuracy 0.8173828125\n",
      "Iteration 14570 Training loss 0.01221592165529728 Validation loss 0.017733478918671608 Accuracy 0.82080078125\n",
      "Iteration 14580 Training loss 0.01944548450410366 Validation loss 0.01841144450008869 Accuracy 0.8134765625\n",
      "Iteration 14590 Training loss 0.013249484822154045 Validation loss 0.017032776027917862 Accuracy 0.82763671875\n",
      "Iteration 14600 Training loss 0.018500715494155884 Validation loss 0.017943058162927628 Accuracy 0.818359375\n",
      "Iteration 14610 Training loss 0.017636526376008987 Validation loss 0.018424436450004578 Accuracy 0.81396484375\n",
      "Iteration 14620 Training loss 0.01589197851717472 Validation loss 0.017658326774835587 Accuracy 0.8212890625\n",
      "Iteration 14630 Training loss 0.014797711744904518 Validation loss 0.01754506677389145 Accuracy 0.822265625\n",
      "Iteration 14640 Training loss 0.016805782914161682 Validation loss 0.017292018979787827 Accuracy 0.82470703125\n",
      "Iteration 14650 Training loss 0.012116247788071632 Validation loss 0.0176521185785532 Accuracy 0.82177734375\n",
      "Iteration 14660 Training loss 0.014750700443983078 Validation loss 0.017999371513724327 Accuracy 0.81787109375\n",
      "Iteration 14670 Training loss 0.015100548043847084 Validation loss 0.01736118271946907 Accuracy 0.82421875\n",
      "Iteration 14680 Training loss 0.015098151750862598 Validation loss 0.017672404646873474 Accuracy 0.8203125\n",
      "Iteration 14690 Training loss 0.014506751671433449 Validation loss 0.01747957244515419 Accuracy 0.82275390625\n",
      "Iteration 14700 Training loss 0.013141130097210407 Validation loss 0.017530446872115135 Accuracy 0.82275390625\n",
      "Iteration 14710 Training loss 0.017664378508925438 Validation loss 0.01756572537124157 Accuracy 0.822265625\n",
      "Iteration 14720 Training loss 0.01798478700220585 Validation loss 0.017380475997924805 Accuracy 0.82373046875\n",
      "Iteration 14730 Training loss 0.014090673066675663 Validation loss 0.017305858433246613 Accuracy 0.82470703125\n",
      "Iteration 14740 Training loss 0.014939396642148495 Validation loss 0.01715567335486412 Accuracy 0.82666015625\n",
      "Iteration 14750 Training loss 0.01415297668427229 Validation loss 0.0179811492562294 Accuracy 0.8173828125\n",
      "Iteration 14760 Training loss 0.015791155397892 Validation loss 0.017639322206377983 Accuracy 0.82080078125\n",
      "Iteration 14770 Training loss 0.015090737491846085 Validation loss 0.017085734754800797 Accuracy 0.826171875\n",
      "Iteration 14780 Training loss 0.01637294329702854 Validation loss 0.01731106825172901 Accuracy 0.8251953125\n",
      "Iteration 14790 Training loss 0.013568216934800148 Validation loss 0.017048152163624763 Accuracy 0.8271484375\n",
      "Iteration 14800 Training loss 0.014506243169307709 Validation loss 0.016889531165361404 Accuracy 0.8291015625\n",
      "Iteration 14810 Training loss 0.0171920545399189 Validation loss 0.017146626487374306 Accuracy 0.826171875\n",
      "Iteration 14820 Training loss 0.015406928956508636 Validation loss 0.01704980619251728 Accuracy 0.8271484375\n",
      "Iteration 14830 Training loss 0.01429543923586607 Validation loss 0.01737332157790661 Accuracy 0.82373046875\n",
      "Iteration 14840 Training loss 0.01676316373050213 Validation loss 0.017099862918257713 Accuracy 0.82666015625\n",
      "Iteration 14850 Training loss 0.013976344838738441 Validation loss 0.01711791381239891 Accuracy 0.826171875\n",
      "Iteration 14860 Training loss 0.015596876852214336 Validation loss 0.017057616263628006 Accuracy 0.82763671875\n",
      "Iteration 14870 Training loss 0.015418633818626404 Validation loss 0.017087049782276154 Accuracy 0.82763671875\n",
      "Iteration 14880 Training loss 0.014988008886575699 Validation loss 0.017003733664751053 Accuracy 0.828125\n",
      "Iteration 14890 Training loss 0.017586911097168922 Validation loss 0.01746527850627899 Accuracy 0.82275390625\n",
      "Iteration 14900 Training loss 0.014208808541297913 Validation loss 0.016977889463305473 Accuracy 0.828125\n",
      "Iteration 14910 Training loss 0.018162162974476814 Validation loss 0.01718992553651333 Accuracy 0.82568359375\n",
      "Iteration 14920 Training loss 0.014935988932847977 Validation loss 0.017765622586011887 Accuracy 0.81982421875\n",
      "Iteration 14930 Training loss 0.017798002809286118 Validation loss 0.01733478717505932 Accuracy 0.82421875\n",
      "Iteration 14940 Training loss 0.015440430492162704 Validation loss 0.017168812453746796 Accuracy 0.826171875\n",
      "Iteration 14950 Training loss 0.013738229870796204 Validation loss 0.017321869730949402 Accuracy 0.82421875\n",
      "Iteration 14960 Training loss 0.01694820076227188 Validation loss 0.017185477539896965 Accuracy 0.826171875\n",
      "Iteration 14970 Training loss 0.015106353908777237 Validation loss 0.017270971089601517 Accuracy 0.8251953125\n",
      "Iteration 14980 Training loss 0.01222201157361269 Validation loss 0.017542347311973572 Accuracy 0.822265625\n",
      "Iteration 14990 Training loss 0.017030024901032448 Validation loss 0.0178214218467474 Accuracy 0.81884765625\n",
      "Iteration 15000 Training loss 0.01521303504705429 Validation loss 0.01717022992670536 Accuracy 0.82568359375\n",
      "Iteration 15010 Training loss 0.014028407633304596 Validation loss 0.017592329531908035 Accuracy 0.82177734375\n",
      "Iteration 15020 Training loss 0.015719156712293625 Validation loss 0.01760721392929554 Accuracy 0.82177734375\n",
      "Iteration 15030 Training loss 0.01447401661425829 Validation loss 0.017246324568986893 Accuracy 0.82421875\n",
      "Iteration 15040 Training loss 0.016713647171854973 Validation loss 0.017498483881354332 Accuracy 0.82275390625\n",
      "Iteration 15050 Training loss 0.014636867679655552 Validation loss 0.017344670370221138 Accuracy 0.82470703125\n",
      "Iteration 15060 Training loss 0.013272544369101524 Validation loss 0.017458707094192505 Accuracy 0.82373046875\n",
      "Iteration 15070 Training loss 0.015246439725160599 Validation loss 0.017208248376846313 Accuracy 0.826171875\n",
      "Iteration 15080 Training loss 0.014101994223892689 Validation loss 0.01722407341003418 Accuracy 0.82568359375\n",
      "Iteration 15090 Training loss 0.015266087837517262 Validation loss 0.017215508967638016 Accuracy 0.82568359375\n",
      "Iteration 15100 Training loss 0.01379646547138691 Validation loss 0.017368873581290245 Accuracy 0.82421875\n",
      "Iteration 15110 Training loss 0.012503325007855892 Validation loss 0.017360517755150795 Accuracy 0.82421875\n",
      "Iteration 15120 Training loss 0.015734873712062836 Validation loss 0.0171503908932209 Accuracy 0.82666015625\n",
      "Iteration 15130 Training loss 0.016104696318507195 Validation loss 0.017192678526043892 Accuracy 0.826171875\n",
      "Iteration 15140 Training loss 0.012241996824741364 Validation loss 0.01720614917576313 Accuracy 0.8251953125\n",
      "Iteration 15150 Training loss 0.01462168525904417 Validation loss 0.017249086871743202 Accuracy 0.8251953125\n",
      "Iteration 15160 Training loss 0.013984747231006622 Validation loss 0.01718188263475895 Accuracy 0.82568359375\n",
      "Iteration 15170 Training loss 0.015284857712686062 Validation loss 0.01757954992353916 Accuracy 0.82177734375\n",
      "Iteration 15180 Training loss 0.01223932858556509 Validation loss 0.017315873876214027 Accuracy 0.82470703125\n",
      "Iteration 15190 Training loss 0.013682794757187366 Validation loss 0.017408933490514755 Accuracy 0.82373046875\n",
      "Iteration 15200 Training loss 0.014984664507210255 Validation loss 0.01728292927145958 Accuracy 0.82470703125\n",
      "Iteration 15210 Training loss 0.015748269855976105 Validation loss 0.01788945682346821 Accuracy 0.8193359375\n",
      "Iteration 15220 Training loss 0.01571509800851345 Validation loss 0.01730518601834774 Accuracy 0.82470703125\n",
      "Iteration 15230 Training loss 0.018381699919700623 Validation loss 0.018689505755901337 Accuracy 0.81103515625\n",
      "Iteration 15240 Training loss 0.014636443927884102 Validation loss 0.017590943723917007 Accuracy 0.82177734375\n",
      "Iteration 15250 Training loss 0.017838304862380028 Validation loss 0.01760454662144184 Accuracy 0.82177734375\n",
      "Iteration 15260 Training loss 0.017121927812695503 Validation loss 0.01838652975857258 Accuracy 0.81396484375\n",
      "Iteration 15270 Training loss 0.016767743974924088 Validation loss 0.017145654186606407 Accuracy 0.82666015625\n",
      "Iteration 15280 Training loss 0.013595476746559143 Validation loss 0.017198728397488594 Accuracy 0.826171875\n",
      "Iteration 15290 Training loss 0.01668967679142952 Validation loss 0.017140746116638184 Accuracy 0.826171875\n",
      "Iteration 15300 Training loss 0.016240255907177925 Validation loss 0.017689842730760574 Accuracy 0.8212890625\n",
      "Iteration 15310 Training loss 0.015027484856545925 Validation loss 0.01753416284918785 Accuracy 0.82177734375\n",
      "Iteration 15320 Training loss 0.016080938279628754 Validation loss 0.017259415239095688 Accuracy 0.82470703125\n",
      "Iteration 15330 Training loss 0.015856966376304626 Validation loss 0.01725178025662899 Accuracy 0.82470703125\n",
      "Iteration 15340 Training loss 0.0177017729729414 Validation loss 0.0171377994120121 Accuracy 0.82666015625\n",
      "Iteration 15350 Training loss 0.015713302418589592 Validation loss 0.01757383905351162 Accuracy 0.82177734375\n",
      "Iteration 15360 Training loss 0.012297192588448524 Validation loss 0.017327645793557167 Accuracy 0.82470703125\n",
      "Iteration 15370 Training loss 0.017295105382800102 Validation loss 0.017395487055182457 Accuracy 0.82421875\n",
      "Iteration 15380 Training loss 0.01366889663040638 Validation loss 0.017877014353871346 Accuracy 0.8193359375\n",
      "Iteration 15390 Training loss 0.014899514615535736 Validation loss 0.017372895032167435 Accuracy 0.82421875\n",
      "Iteration 15400 Training loss 0.013752980157732964 Validation loss 0.017261913046240807 Accuracy 0.8251953125\n",
      "Iteration 15410 Training loss 0.015465142205357552 Validation loss 0.01745273545384407 Accuracy 0.8232421875\n",
      "Iteration 15420 Training loss 0.017023717984557152 Validation loss 0.017358917742967606 Accuracy 0.82373046875\n",
      "Iteration 15430 Training loss 0.01607932709157467 Validation loss 0.0174215417355299 Accuracy 0.82373046875\n",
      "Iteration 15440 Training loss 0.014665349386632442 Validation loss 0.01785498671233654 Accuracy 0.81884765625\n",
      "Iteration 15450 Training loss 0.01391406450420618 Validation loss 0.017268534749746323 Accuracy 0.82568359375\n",
      "Iteration 15460 Training loss 0.01777050457894802 Validation loss 0.017225878313183784 Accuracy 0.82568359375\n",
      "Iteration 15470 Training loss 0.014340062625706196 Validation loss 0.017311058938503265 Accuracy 0.8251953125\n",
      "Iteration 15480 Training loss 0.013726028613746166 Validation loss 0.01755230873823166 Accuracy 0.822265625\n",
      "Iteration 15490 Training loss 0.014913438819348812 Validation loss 0.017513960599899292 Accuracy 0.82177734375\n",
      "Iteration 15500 Training loss 0.014349199831485748 Validation loss 0.017348717898130417 Accuracy 0.82421875\n",
      "Iteration 15510 Training loss 0.016759252175688744 Validation loss 0.01724705845117569 Accuracy 0.8251953125\n",
      "Iteration 15520 Training loss 0.015442794188857079 Validation loss 0.017493151128292084 Accuracy 0.8232421875\n",
      "Iteration 15530 Training loss 0.017154574394226074 Validation loss 0.017152413725852966 Accuracy 0.82666015625\n",
      "Iteration 15540 Training loss 0.01641816273331642 Validation loss 0.017282014712691307 Accuracy 0.82470703125\n",
      "Iteration 15550 Training loss 0.017578786239027977 Validation loss 0.017168663442134857 Accuracy 0.826171875\n",
      "Iteration 15560 Training loss 0.016543962061405182 Validation loss 0.01723654940724373 Accuracy 0.82568359375\n",
      "Iteration 15570 Training loss 0.01477934792637825 Validation loss 0.017846260219812393 Accuracy 0.81884765625\n",
      "Iteration 15580 Training loss 0.01616637408733368 Validation loss 0.017154410481452942 Accuracy 0.82568359375\n",
      "Iteration 15590 Training loss 0.01724974252283573 Validation loss 0.017529437318444252 Accuracy 0.822265625\n",
      "Iteration 15600 Training loss 0.015040544793009758 Validation loss 0.01727549359202385 Accuracy 0.82470703125\n",
      "Iteration 15610 Training loss 0.01665293425321579 Validation loss 0.017075620591640472 Accuracy 0.8271484375\n",
      "Iteration 15620 Training loss 0.014148587360978127 Validation loss 0.017023177817463875 Accuracy 0.82763671875\n",
      "Iteration 15630 Training loss 0.01587672345340252 Validation loss 0.017092738300561905 Accuracy 0.8271484375\n",
      "Iteration 15640 Training loss 0.01548581663519144 Validation loss 0.017943738028407097 Accuracy 0.81787109375\n",
      "Iteration 15650 Training loss 0.01470070518553257 Validation loss 0.01694532297551632 Accuracy 0.82861328125\n",
      "Iteration 15660 Training loss 0.014927810057997704 Validation loss 0.017129436135292053 Accuracy 0.82666015625\n",
      "Iteration 15670 Training loss 0.016356633976101875 Validation loss 0.017345039173960686 Accuracy 0.8251953125\n",
      "Iteration 15680 Training loss 0.016396719962358475 Validation loss 0.01799856312572956 Accuracy 0.81787109375\n",
      "Iteration 15690 Training loss 0.015036878176033497 Validation loss 0.017084652557969093 Accuracy 0.82666015625\n",
      "Iteration 15700 Training loss 0.016803013160824776 Validation loss 0.01715797558426857 Accuracy 0.826171875\n",
      "Iteration 15710 Training loss 0.012555478140711784 Validation loss 0.017523590475320816 Accuracy 0.82275390625\n",
      "Iteration 15720 Training loss 0.015115895308554173 Validation loss 0.017031008377671242 Accuracy 0.8271484375\n",
      "Iteration 15730 Training loss 0.014409257099032402 Validation loss 0.017393918707966805 Accuracy 0.82421875\n",
      "Iteration 15740 Training loss 0.01614353433251381 Validation loss 0.018420670181512833 Accuracy 0.8134765625\n",
      "Iteration 15750 Training loss 0.01422183122485876 Validation loss 0.017737871035933495 Accuracy 0.8203125\n",
      "Iteration 15760 Training loss 0.017944930121302605 Validation loss 0.01723981462419033 Accuracy 0.82568359375\n",
      "Iteration 15770 Training loss 0.01658438704907894 Validation loss 0.017627917230129242 Accuracy 0.8212890625\n",
      "Iteration 15780 Training loss 0.014306421391665936 Validation loss 0.017597490921616554 Accuracy 0.822265625\n",
      "Iteration 15790 Training loss 0.012893958948552608 Validation loss 0.017369652166962624 Accuracy 0.82421875\n",
      "Iteration 15800 Training loss 0.015409067273139954 Validation loss 0.017922282218933105 Accuracy 0.81884765625\n",
      "Iteration 15810 Training loss 0.013374224305152893 Validation loss 0.017189335078001022 Accuracy 0.82568359375\n",
      "Iteration 15820 Training loss 0.016276154667139053 Validation loss 0.01717003434896469 Accuracy 0.82568359375\n",
      "Iteration 15830 Training loss 0.013858336955308914 Validation loss 0.017739037051796913 Accuracy 0.8203125\n",
      "Iteration 15840 Training loss 0.016371075063943863 Validation loss 0.01719825342297554 Accuracy 0.8251953125\n",
      "Iteration 15850 Training loss 0.01593095064163208 Validation loss 0.017053889110684395 Accuracy 0.8271484375\n",
      "Iteration 15860 Training loss 0.01865401491522789 Validation loss 0.017269229516386986 Accuracy 0.8251953125\n",
      "Iteration 15870 Training loss 0.01533573865890503 Validation loss 0.01701279915869236 Accuracy 0.828125\n",
      "Iteration 15880 Training loss 0.014449616894125938 Validation loss 0.01746995747089386 Accuracy 0.8232421875\n",
      "Iteration 15890 Training loss 0.014608168974518776 Validation loss 0.017045823857188225 Accuracy 0.828125\n",
      "Iteration 15900 Training loss 0.01787581667304039 Validation loss 0.018058238551020622 Accuracy 0.81689453125\n",
      "Iteration 15910 Training loss 0.01810210570693016 Validation loss 0.017339184880256653 Accuracy 0.82421875\n",
      "Iteration 15920 Training loss 0.016021301969885826 Validation loss 0.01718035899102688 Accuracy 0.82568359375\n",
      "Iteration 15930 Training loss 0.016621144488453865 Validation loss 0.017493054270744324 Accuracy 0.82275390625\n",
      "Iteration 15940 Training loss 0.013877621851861477 Validation loss 0.017319483682513237 Accuracy 0.8251953125\n",
      "Iteration 15950 Training loss 0.01539172325283289 Validation loss 0.01696334034204483 Accuracy 0.828125\n",
      "Iteration 15960 Training loss 0.013711439445614815 Validation loss 0.017172276973724365 Accuracy 0.826171875\n",
      "Iteration 15970 Training loss 0.01570126786828041 Validation loss 0.016977189108729362 Accuracy 0.82861328125\n",
      "Iteration 15980 Training loss 0.013052416034042835 Validation loss 0.017319323495030403 Accuracy 0.82470703125\n",
      "Iteration 15990 Training loss 0.015495192259550095 Validation loss 0.017086278647184372 Accuracy 0.82763671875\n",
      "Iteration 16000 Training loss 0.014432723633944988 Validation loss 0.017004145309329033 Accuracy 0.828125\n",
      "Iteration 16010 Training loss 0.014327787794172764 Validation loss 0.01697200909256935 Accuracy 0.828125\n",
      "Iteration 16020 Training loss 0.014573794789612293 Validation loss 0.017494559288024902 Accuracy 0.82275390625\n",
      "Iteration 16030 Training loss 0.012387130409479141 Validation loss 0.01779703050851822 Accuracy 0.8193359375\n",
      "Iteration 16040 Training loss 0.014953169040381908 Validation loss 0.017159540206193924 Accuracy 0.82666015625\n",
      "Iteration 16050 Training loss 0.013366742059588432 Validation loss 0.017261691391468048 Accuracy 0.8251953125\n",
      "Iteration 16060 Training loss 0.012792703695595264 Validation loss 0.016917023807764053 Accuracy 0.82861328125\n",
      "Iteration 16070 Training loss 0.013571061193943024 Validation loss 0.01700550690293312 Accuracy 0.82763671875\n",
      "Iteration 16080 Training loss 0.015460156835615635 Validation loss 0.01719602942466736 Accuracy 0.8251953125\n",
      "Iteration 16090 Training loss 0.013274243101477623 Validation loss 0.017092972993850708 Accuracy 0.82666015625\n",
      "Iteration 16100 Training loss 0.015226353891193867 Validation loss 0.017207155004143715 Accuracy 0.82568359375\n",
      "Iteration 16110 Training loss 0.015079843811690807 Validation loss 0.01822635531425476 Accuracy 0.814453125\n",
      "Iteration 16120 Training loss 0.01742001809179783 Validation loss 0.01698889024555683 Accuracy 0.8271484375\n",
      "Iteration 16130 Training loss 0.012153898365795612 Validation loss 0.016937119886279106 Accuracy 0.82861328125\n",
      "Iteration 16140 Training loss 0.014118902385234833 Validation loss 0.017031315714120865 Accuracy 0.82763671875\n",
      "Iteration 16150 Training loss 0.015510723926126957 Validation loss 0.01835883967578411 Accuracy 0.81298828125\n",
      "Iteration 16160 Training loss 0.016100244596600533 Validation loss 0.017382647842168808 Accuracy 0.82373046875\n",
      "Iteration 16170 Training loss 0.015607316978275776 Validation loss 0.016891609877347946 Accuracy 0.8291015625\n",
      "Iteration 16180 Training loss 0.01668800413608551 Validation loss 0.017491096630692482 Accuracy 0.822265625\n",
      "Iteration 16190 Training loss 0.01680305041372776 Validation loss 0.017307965084910393 Accuracy 0.8251953125\n",
      "Iteration 16200 Training loss 0.015527595765888691 Validation loss 0.017205046489834785 Accuracy 0.82568359375\n",
      "Iteration 16210 Training loss 0.015378435142338276 Validation loss 0.01717054843902588 Accuracy 0.826171875\n",
      "Iteration 16220 Training loss 0.015310419723391533 Validation loss 0.01707838475704193 Accuracy 0.8271484375\n",
      "Iteration 16230 Training loss 0.012332852929830551 Validation loss 0.016991527751088142 Accuracy 0.828125\n",
      "Iteration 16240 Training loss 0.017334548756480217 Validation loss 0.017223935574293137 Accuracy 0.8251953125\n",
      "Iteration 16250 Training loss 0.015174288302659988 Validation loss 0.017031745985150337 Accuracy 0.82763671875\n",
      "Iteration 16260 Training loss 0.015332863666117191 Validation loss 0.017038924619555473 Accuracy 0.82763671875\n",
      "Iteration 16270 Training loss 0.014920785091817379 Validation loss 0.01708933711051941 Accuracy 0.82666015625\n",
      "Iteration 16280 Training loss 0.015501851215958595 Validation loss 0.017198996618390083 Accuracy 0.82568359375\n",
      "Iteration 16290 Training loss 0.014688022434711456 Validation loss 0.017051687464118004 Accuracy 0.82763671875\n",
      "Iteration 16300 Training loss 0.015134924091398716 Validation loss 0.017209669575095177 Accuracy 0.82568359375\n",
      "Iteration 16310 Training loss 0.01569565013051033 Validation loss 0.017043234780430794 Accuracy 0.8271484375\n",
      "Iteration 16320 Training loss 0.016795597970485687 Validation loss 0.017063116654753685 Accuracy 0.82666015625\n",
      "Iteration 16330 Training loss 0.016333989799022675 Validation loss 0.017352307215332985 Accuracy 0.82421875\n",
      "Iteration 16340 Training loss 0.016505464911460876 Validation loss 0.017038507387042046 Accuracy 0.82763671875\n",
      "Iteration 16350 Training loss 0.016509773209691048 Validation loss 0.01751377061009407 Accuracy 0.82275390625\n",
      "Iteration 16360 Training loss 0.014967275783419609 Validation loss 0.01700005866587162 Accuracy 0.82763671875\n",
      "Iteration 16370 Training loss 0.015862800180912018 Validation loss 0.01715225912630558 Accuracy 0.826171875\n",
      "Iteration 16380 Training loss 0.01599201373755932 Validation loss 0.01722032204270363 Accuracy 0.82568359375\n",
      "Iteration 16390 Training loss 0.014068858698010445 Validation loss 0.017017584294080734 Accuracy 0.82763671875\n",
      "Iteration 16400 Training loss 0.016477232798933983 Validation loss 0.017217695713043213 Accuracy 0.82568359375\n",
      "Iteration 16410 Training loss 0.01569460704922676 Validation loss 0.016956044360995293 Accuracy 0.828125\n",
      "Iteration 16420 Training loss 0.016483934596180916 Validation loss 0.017059342935681343 Accuracy 0.8271484375\n",
      "Iteration 16430 Training loss 0.014781041070818901 Validation loss 0.017026808112859726 Accuracy 0.8271484375\n",
      "Iteration 16440 Training loss 0.014967580325901508 Validation loss 0.017042597755789757 Accuracy 0.82763671875\n",
      "Iteration 16450 Training loss 0.013841532170772552 Validation loss 0.016838720068335533 Accuracy 0.82958984375\n",
      "Iteration 16460 Training loss 0.01399968285113573 Validation loss 0.017017995938658714 Accuracy 0.8271484375\n",
      "Iteration 16470 Training loss 0.015182833187282085 Validation loss 0.016929056495428085 Accuracy 0.82861328125\n",
      "Iteration 16480 Training loss 0.01415375154465437 Validation loss 0.01696944795548916 Accuracy 0.828125\n",
      "Iteration 16490 Training loss 0.012895657680928707 Validation loss 0.016984138637781143 Accuracy 0.82763671875\n",
      "Iteration 16500 Training loss 0.014609068632125854 Validation loss 0.016825973987579346 Accuracy 0.82958984375\n",
      "Iteration 16510 Training loss 0.014056835323572159 Validation loss 0.016974985599517822 Accuracy 0.828125\n",
      "Iteration 16520 Training loss 0.015537750907242298 Validation loss 0.01913340948522091 Accuracy 0.80615234375\n",
      "Iteration 16530 Training loss 0.016316816210746765 Validation loss 0.017111066728830338 Accuracy 0.826171875\n",
      "Iteration 16540 Training loss 0.015780720859766006 Validation loss 0.01730562001466751 Accuracy 0.82421875\n",
      "Iteration 16550 Training loss 0.016166506335139275 Validation loss 0.01688011735677719 Accuracy 0.82958984375\n",
      "Iteration 16560 Training loss 0.012003196403384209 Validation loss 0.016829069703817368 Accuracy 0.830078125\n",
      "Iteration 16570 Training loss 0.012419397942721844 Validation loss 0.0169301088899374 Accuracy 0.82861328125\n",
      "Iteration 16580 Training loss 0.016829436644911766 Validation loss 0.017495209351181984 Accuracy 0.82275390625\n",
      "Iteration 16590 Training loss 0.012925878167152405 Validation loss 0.017009295523166656 Accuracy 0.82763671875\n",
      "Iteration 16600 Training loss 0.016950275748968124 Validation loss 0.0168690737336874 Accuracy 0.82861328125\n",
      "Iteration 16610 Training loss 0.015192599967122078 Validation loss 0.01696912944316864 Accuracy 0.82861328125\n",
      "Iteration 16620 Training loss 0.014683348126709461 Validation loss 0.01676574908196926 Accuracy 0.830078125\n",
      "Iteration 16630 Training loss 0.01665947400033474 Validation loss 0.016979137435555458 Accuracy 0.828125\n",
      "Iteration 16640 Training loss 0.014744598418474197 Validation loss 0.017001917585730553 Accuracy 0.8271484375\n",
      "Iteration 16650 Training loss 0.014958656392991543 Validation loss 0.01674342341721058 Accuracy 0.830078125\n",
      "Iteration 16660 Training loss 0.015115746296942234 Validation loss 0.016842206940054893 Accuracy 0.8291015625\n",
      "Iteration 16670 Training loss 0.013230216689407825 Validation loss 0.01687261275947094 Accuracy 0.82861328125\n",
      "Iteration 16680 Training loss 0.013860384933650494 Validation loss 0.01682964526116848 Accuracy 0.8291015625\n",
      "Iteration 16690 Training loss 0.014723584055900574 Validation loss 0.01680700108408928 Accuracy 0.82958984375\n",
      "Iteration 16700 Training loss 0.014535455964505672 Validation loss 0.01701468415558338 Accuracy 0.82763671875\n",
      "Iteration 16710 Training loss 0.015926716849207878 Validation loss 0.017151745036244392 Accuracy 0.826171875\n",
      "Iteration 16720 Training loss 0.01833363249897957 Validation loss 0.017110690474510193 Accuracy 0.826171875\n",
      "Iteration 16730 Training loss 0.01312241330742836 Validation loss 0.017322927713394165 Accuracy 0.82421875\n",
      "Iteration 16740 Training loss 0.0181573536247015 Validation loss 0.017299972474575043 Accuracy 0.82421875\n",
      "Iteration 16750 Training loss 0.015877740457654 Validation loss 0.01753235049545765 Accuracy 0.822265625\n",
      "Iteration 16760 Training loss 0.014953752979636192 Validation loss 0.017406506463885307 Accuracy 0.8232421875\n",
      "Iteration 16770 Training loss 0.017855193465948105 Validation loss 0.01694427989423275 Accuracy 0.828125\n",
      "Iteration 16780 Training loss 0.013665043748915195 Validation loss 0.0170259028673172 Accuracy 0.82763671875\n",
      "Iteration 16790 Training loss 0.016756851226091385 Validation loss 0.017192106693983078 Accuracy 0.82568359375\n",
      "Iteration 16800 Training loss 0.013961737975478172 Validation loss 0.017864219844341278 Accuracy 0.8193359375\n",
      "Iteration 16810 Training loss 0.014614562503993511 Validation loss 0.017181403934955597 Accuracy 0.8251953125\n",
      "Iteration 16820 Training loss 0.013755502179265022 Validation loss 0.01724313013255596 Accuracy 0.8251953125\n",
      "Iteration 16830 Training loss 0.015958989039063454 Validation loss 0.01722414791584015 Accuracy 0.82568359375\n",
      "Iteration 16840 Training loss 0.013568487949669361 Validation loss 0.016890445724129677 Accuracy 0.828125\n",
      "Iteration 16850 Training loss 0.016388604417443275 Validation loss 0.017542706802487373 Accuracy 0.82177734375\n",
      "Iteration 16860 Training loss 0.01552696991711855 Validation loss 0.017285961657762527 Accuracy 0.8251953125\n",
      "Iteration 16870 Training loss 0.018615858629345894 Validation loss 0.019044913351535797 Accuracy 0.80712890625\n",
      "Iteration 16880 Training loss 0.01598699949681759 Validation loss 0.01761835627257824 Accuracy 0.8212890625\n",
      "Iteration 16890 Training loss 0.012572424486279488 Validation loss 0.01718609407544136 Accuracy 0.82568359375\n",
      "Iteration 16900 Training loss 0.012879615649580956 Validation loss 0.017498843371868134 Accuracy 0.822265625\n",
      "Iteration 16910 Training loss 0.012064078822731972 Validation loss 0.017044872045516968 Accuracy 0.8271484375\n",
      "Iteration 16920 Training loss 0.015547536313533783 Validation loss 0.018046727403998375 Accuracy 0.81689453125\n",
      "Iteration 16930 Training loss 0.01497572660446167 Validation loss 0.01715013198554516 Accuracy 0.82568359375\n",
      "Iteration 16940 Training loss 0.016154004260897636 Validation loss 0.01721654087305069 Accuracy 0.82568359375\n",
      "Iteration 16950 Training loss 0.01520472951233387 Validation loss 0.017568090930581093 Accuracy 0.82177734375\n",
      "Iteration 16960 Training loss 0.017410820350050926 Validation loss 0.017424406483769417 Accuracy 0.8232421875\n",
      "Iteration 16970 Training loss 0.01364902127534151 Validation loss 0.017186781391501427 Accuracy 0.82568359375\n",
      "Iteration 16980 Training loss 0.015511799603700638 Validation loss 0.016941463574767113 Accuracy 0.82861328125\n",
      "Iteration 16990 Training loss 0.017489738762378693 Validation loss 0.016957208514213562 Accuracy 0.82763671875\n",
      "Iteration 17000 Training loss 0.015557525679469109 Validation loss 0.016820106655359268 Accuracy 0.82958984375\n",
      "Iteration 17010 Training loss 0.016496019437909126 Validation loss 0.01709592528641224 Accuracy 0.82666015625\n",
      "Iteration 17020 Training loss 0.012547510676085949 Validation loss 0.01699342019855976 Accuracy 0.82763671875\n",
      "Iteration 17030 Training loss 0.016053741797804832 Validation loss 0.017006365582346916 Accuracy 0.82763671875\n",
      "Iteration 17040 Training loss 0.014129044488072395 Validation loss 0.017029857262969017 Accuracy 0.82763671875\n",
      "Iteration 17050 Training loss 0.014089028351008892 Validation loss 0.01697493903338909 Accuracy 0.828125\n",
      "Iteration 17060 Training loss 0.014925890602171421 Validation loss 0.017003556713461876 Accuracy 0.8271484375\n",
      "Iteration 17070 Training loss 0.014320659451186657 Validation loss 0.01694527454674244 Accuracy 0.82763671875\n",
      "Iteration 17080 Training loss 0.01737329177558422 Validation loss 0.017125438898801804 Accuracy 0.826171875\n",
      "Iteration 17090 Training loss 0.015439348295331001 Validation loss 0.016993820667266846 Accuracy 0.82763671875\n",
      "Iteration 17100 Training loss 0.015489961951971054 Validation loss 0.018677208572626114 Accuracy 0.81103515625\n",
      "Iteration 17110 Training loss 0.014749483205378056 Validation loss 0.01688368245959282 Accuracy 0.82861328125\n",
      "Iteration 17120 Training loss 0.014241247437894344 Validation loss 0.017031725496053696 Accuracy 0.8271484375\n",
      "Iteration 17130 Training loss 0.012660023756325245 Validation loss 0.017128413543105125 Accuracy 0.82666015625\n",
      "Iteration 17140 Training loss 0.014194593764841557 Validation loss 0.01727232150733471 Accuracy 0.8251953125\n",
      "Iteration 17150 Training loss 0.01796470582485199 Validation loss 0.017071614041924477 Accuracy 0.82666015625\n",
      "Iteration 17160 Training loss 0.015805309638381004 Validation loss 0.01763971894979477 Accuracy 0.82080078125\n",
      "Iteration 17170 Training loss 0.017109988257288933 Validation loss 0.016953445971012115 Accuracy 0.828125\n",
      "Iteration 17180 Training loss 0.016083326190710068 Validation loss 0.01696515828371048 Accuracy 0.828125\n",
      "Iteration 17190 Training loss 0.014025652781128883 Validation loss 0.01719261147081852 Accuracy 0.8251953125\n",
      "Iteration 17200 Training loss 0.01567092351615429 Validation loss 0.017195260152220726 Accuracy 0.8251953125\n",
      "Iteration 17210 Training loss 0.017282964661717415 Validation loss 0.01756175421178341 Accuracy 0.82177734375\n",
      "Iteration 17220 Training loss 0.01492263562977314 Validation loss 0.017126571387052536 Accuracy 0.82568359375\n",
      "Iteration 17230 Training loss 0.015456588938832283 Validation loss 0.01708381064236164 Accuracy 0.82666015625\n",
      "Iteration 17240 Training loss 0.014591092243790627 Validation loss 0.01763848401606083 Accuracy 0.82080078125\n",
      "Iteration 17250 Training loss 0.013695514760911465 Validation loss 0.01751536875963211 Accuracy 0.82275390625\n",
      "Iteration 17260 Training loss 0.016362637281417847 Validation loss 0.017136475071310997 Accuracy 0.82568359375\n",
      "Iteration 17270 Training loss 0.016436493024230003 Validation loss 0.017228348180651665 Accuracy 0.8251953125\n",
      "Iteration 17280 Training loss 0.012855183333158493 Validation loss 0.017035173252224922 Accuracy 0.82666015625\n",
      "Iteration 17290 Training loss 0.016647260636091232 Validation loss 0.01724189706146717 Accuracy 0.8251953125\n",
      "Iteration 17300 Training loss 0.014050456695258617 Validation loss 0.017331499606370926 Accuracy 0.82373046875\n",
      "Iteration 17310 Training loss 0.01571613736450672 Validation loss 0.017336638644337654 Accuracy 0.82373046875\n",
      "Iteration 17320 Training loss 0.017617588862776756 Validation loss 0.01691804639995098 Accuracy 0.82861328125\n",
      "Iteration 17330 Training loss 0.015886537730693817 Validation loss 0.017428962513804436 Accuracy 0.8232421875\n",
      "Iteration 17340 Training loss 0.015061250887811184 Validation loss 0.01705765351653099 Accuracy 0.82666015625\n",
      "Iteration 17350 Training loss 0.014774278737604618 Validation loss 0.017063995823264122 Accuracy 0.8271484375\n",
      "Iteration 17360 Training loss 0.013927978463470936 Validation loss 0.017432617023587227 Accuracy 0.82373046875\n",
      "Iteration 17370 Training loss 0.01383852120488882 Validation loss 0.017796413972973824 Accuracy 0.8193359375\n",
      "Iteration 17380 Training loss 0.013962307944893837 Validation loss 0.01737140491604805 Accuracy 0.82373046875\n",
      "Iteration 17390 Training loss 0.01366918720304966 Validation loss 0.017140047624707222 Accuracy 0.826171875\n",
      "Iteration 17400 Training loss 0.012315841391682625 Validation loss 0.017076458781957626 Accuracy 0.826171875\n",
      "Iteration 17410 Training loss 0.012756487354636192 Validation loss 0.016870563849806786 Accuracy 0.8291015625\n",
      "Iteration 17420 Training loss 0.015815528109669685 Validation loss 0.01741761714220047 Accuracy 0.8232421875\n",
      "Iteration 17430 Training loss 0.01467113383114338 Validation loss 0.016903460025787354 Accuracy 0.82861328125\n",
      "Iteration 17440 Training loss 0.01646018773317337 Validation loss 0.017061511054635048 Accuracy 0.82666015625\n",
      "Iteration 17450 Training loss 0.012091785669326782 Validation loss 0.016985692083835602 Accuracy 0.82763671875\n",
      "Iteration 17460 Training loss 0.012346414849162102 Validation loss 0.0168717410415411 Accuracy 0.8291015625\n",
      "Iteration 17470 Training loss 0.014889795333147049 Validation loss 0.016941837966442108 Accuracy 0.82861328125\n",
      "Iteration 17480 Training loss 0.015006625093519688 Validation loss 0.01742388680577278 Accuracy 0.8232421875\n",
      "Iteration 17490 Training loss 0.013870368711650372 Validation loss 0.017007775604724884 Accuracy 0.82763671875\n",
      "Iteration 17500 Training loss 0.015832344070076942 Validation loss 0.01716442219913006 Accuracy 0.82568359375\n",
      "Iteration 17510 Training loss 0.015569801442325115 Validation loss 0.016916008666157722 Accuracy 0.828125\n",
      "Iteration 17520 Training loss 0.011981354095041752 Validation loss 0.017286011949181557 Accuracy 0.8251953125\n",
      "Iteration 17530 Training loss 0.01463430654257536 Validation loss 0.017236635088920593 Accuracy 0.8251953125\n",
      "Iteration 17540 Training loss 0.015169784426689148 Validation loss 0.016789620742201805 Accuracy 0.82958984375\n",
      "Iteration 17550 Training loss 0.015300462953746319 Validation loss 0.017167532816529274 Accuracy 0.82568359375\n",
      "Iteration 17560 Training loss 0.01552385650575161 Validation loss 0.0171370692551136 Accuracy 0.826171875\n",
      "Iteration 17570 Training loss 0.015043429099023342 Validation loss 0.017041295766830444 Accuracy 0.8271484375\n",
      "Iteration 17580 Training loss 0.01662970706820488 Validation loss 0.017260583117604256 Accuracy 0.8251953125\n",
      "Iteration 17590 Training loss 0.015692081302404404 Validation loss 0.017112715169787407 Accuracy 0.82666015625\n",
      "Iteration 17600 Training loss 0.01419709250330925 Validation loss 0.01718089170753956 Accuracy 0.826171875\n",
      "Iteration 17610 Training loss 0.01275334320962429 Validation loss 0.0169008020311594 Accuracy 0.82861328125\n",
      "Iteration 17620 Training loss 0.015099369920790195 Validation loss 0.017279082909226418 Accuracy 0.8251953125\n",
      "Iteration 17630 Training loss 0.015273970551788807 Validation loss 0.017072828486561775 Accuracy 0.8271484375\n",
      "Iteration 17640 Training loss 0.01639179140329361 Validation loss 0.017090102657675743 Accuracy 0.82666015625\n",
      "Iteration 17650 Training loss 0.01408003643155098 Validation loss 0.01783883199095726 Accuracy 0.81982421875\n",
      "Iteration 17660 Training loss 0.015185604803264141 Validation loss 0.016811177134513855 Accuracy 0.82958984375\n",
      "Iteration 17670 Training loss 0.014614143408834934 Validation loss 0.017003880813717842 Accuracy 0.82763671875\n",
      "Iteration 17680 Training loss 0.017577817663550377 Validation loss 0.017162928357720375 Accuracy 0.8251953125\n",
      "Iteration 17690 Training loss 0.01579551212489605 Validation loss 0.01716057024896145 Accuracy 0.82568359375\n",
      "Iteration 17700 Training loss 0.014559456147253513 Validation loss 0.016988735646009445 Accuracy 0.828125\n",
      "Iteration 17710 Training loss 0.016220517456531525 Validation loss 0.017289379611611366 Accuracy 0.82470703125\n",
      "Iteration 17720 Training loss 0.013680914416909218 Validation loss 0.016975251957774162 Accuracy 0.828125\n",
      "Iteration 17730 Training loss 0.017394309863448143 Validation loss 0.01691306196153164 Accuracy 0.82861328125\n",
      "Iteration 17740 Training loss 0.015277312137186527 Validation loss 0.017123451456427574 Accuracy 0.826171875\n",
      "Iteration 17750 Training loss 0.018197135999798775 Validation loss 0.01710156351327896 Accuracy 0.826171875\n",
      "Iteration 17760 Training loss 0.015688467770814896 Validation loss 0.016994498670101166 Accuracy 0.82763671875\n",
      "Iteration 17770 Training loss 0.01456701010465622 Validation loss 0.016834434121847153 Accuracy 0.8291015625\n",
      "Iteration 17780 Training loss 0.015587160363793373 Validation loss 0.01719743199646473 Accuracy 0.8251953125\n",
      "Iteration 17790 Training loss 0.015262064523994923 Validation loss 0.0171137023717165 Accuracy 0.826171875\n",
      "Iteration 17800 Training loss 0.013672571629285812 Validation loss 0.017258768901228905 Accuracy 0.82470703125\n",
      "Iteration 17810 Training loss 0.014758018776774406 Validation loss 0.017263835296034813 Accuracy 0.8251953125\n",
      "Iteration 17820 Training loss 0.015624129213392735 Validation loss 0.01710374280810356 Accuracy 0.826171875\n",
      "Iteration 17830 Training loss 0.01511418353766203 Validation loss 0.016964491456747055 Accuracy 0.82763671875\n",
      "Iteration 17840 Training loss 0.016375282779335976 Validation loss 0.01726040057837963 Accuracy 0.82470703125\n",
      "Iteration 17850 Training loss 0.01588466577231884 Validation loss 0.01715809293091297 Accuracy 0.826171875\n",
      "Iteration 17860 Training loss 0.015896234661340714 Validation loss 0.01702798344194889 Accuracy 0.82666015625\n",
      "Iteration 17870 Training loss 0.016106950119137764 Validation loss 0.017394782975316048 Accuracy 0.82373046875\n",
      "Iteration 17880 Training loss 0.01675472967326641 Validation loss 0.017005443572998047 Accuracy 0.8271484375\n",
      "Iteration 17890 Training loss 0.015065844170749187 Validation loss 0.017389416694641113 Accuracy 0.82373046875\n",
      "Iteration 17900 Training loss 0.014121154323220253 Validation loss 0.017694106325507164 Accuracy 0.8203125\n",
      "Iteration 17910 Training loss 0.015504140406847 Validation loss 0.017528563737869263 Accuracy 0.822265625\n",
      "Iteration 17920 Training loss 0.016022436320781708 Validation loss 0.017842795699834824 Accuracy 0.818359375\n",
      "Iteration 17930 Training loss 0.013504633679986 Validation loss 0.01726011000573635 Accuracy 0.8251953125\n",
      "Iteration 17940 Training loss 0.014560429379343987 Validation loss 0.017316896468400955 Accuracy 0.82421875\n",
      "Iteration 17950 Training loss 0.01510097086429596 Validation loss 0.016917560249567032 Accuracy 0.82861328125\n",
      "Iteration 17960 Training loss 0.013200437650084496 Validation loss 0.017041610553860664 Accuracy 0.82666015625\n",
      "Iteration 17970 Training loss 0.017046496272087097 Validation loss 0.017035624012351036 Accuracy 0.82666015625\n",
      "Iteration 17980 Training loss 0.014811915345489979 Validation loss 0.017429228872060776 Accuracy 0.82373046875\n",
      "Iteration 17990 Training loss 0.015407091937959194 Validation loss 0.01691691391170025 Accuracy 0.828125\n",
      "Iteration 18000 Training loss 0.014337362721562386 Validation loss 0.016891103237867355 Accuracy 0.8291015625\n",
      "Iteration 18010 Training loss 0.014670670963823795 Validation loss 0.017155054956674576 Accuracy 0.826171875\n",
      "Iteration 18020 Training loss 0.014824495650827885 Validation loss 0.016860608011484146 Accuracy 0.8291015625\n",
      "Iteration 18030 Training loss 0.015363276936113834 Validation loss 0.016853099688887596 Accuracy 0.8291015625\n",
      "Iteration 18040 Training loss 0.015371425077319145 Validation loss 0.017151644453406334 Accuracy 0.826171875\n",
      "Iteration 18050 Training loss 0.014620221219956875 Validation loss 0.016901757568120956 Accuracy 0.828125\n",
      "Iteration 18060 Training loss 0.014419824816286564 Validation loss 0.016853973269462585 Accuracy 0.8291015625\n",
      "Iteration 18070 Training loss 0.016091376543045044 Validation loss 0.016864053905010223 Accuracy 0.82861328125\n",
      "Iteration 18080 Training loss 0.015732914209365845 Validation loss 0.0171170923858881 Accuracy 0.826171875\n",
      "Iteration 18090 Training loss 0.012092608027160168 Validation loss 0.016929717734456062 Accuracy 0.828125\n",
      "Iteration 18100 Training loss 0.01495624240487814 Validation loss 0.017329437658190727 Accuracy 0.82373046875\n",
      "Iteration 18110 Training loss 0.014757082797586918 Validation loss 0.01682090573012829 Accuracy 0.8291015625\n",
      "Iteration 18120 Training loss 0.011113408021628857 Validation loss 0.016995394602417946 Accuracy 0.828125\n",
      "Iteration 18130 Training loss 0.01686592772603035 Validation loss 0.017148828133940697 Accuracy 0.826171875\n",
      "Iteration 18140 Training loss 0.01567687653005123 Validation loss 0.01717117801308632 Accuracy 0.82568359375\n",
      "Iteration 18150 Training loss 0.014346299692988396 Validation loss 0.01756472885608673 Accuracy 0.82177734375\n",
      "Iteration 18160 Training loss 0.014090286567807198 Validation loss 0.01695971004664898 Accuracy 0.828125\n",
      "Iteration 18170 Training loss 0.012817766517400742 Validation loss 0.01694897748529911 Accuracy 0.828125\n",
      "Iteration 18180 Training loss 0.014273623935878277 Validation loss 0.017216550186276436 Accuracy 0.82568359375\n",
      "Iteration 18190 Training loss 0.014923781156539917 Validation loss 0.016779297962784767 Accuracy 0.83056640625\n",
      "Iteration 18200 Training loss 0.01441209390759468 Validation loss 0.01694456860423088 Accuracy 0.82763671875\n",
      "Iteration 18210 Training loss 0.016254914924502373 Validation loss 0.016932621598243713 Accuracy 0.828125\n",
      "Iteration 18220 Training loss 0.014545122161507607 Validation loss 0.01735774800181389 Accuracy 0.82421875\n",
      "Iteration 18230 Training loss 0.012418553233146667 Validation loss 0.016933633014559746 Accuracy 0.828125\n",
      "Iteration 18240 Training loss 0.01354591641575098 Validation loss 0.017237920314073563 Accuracy 0.8251953125\n",
      "Iteration 18250 Training loss 0.016052383929491043 Validation loss 0.016845444217324257 Accuracy 0.8291015625\n",
      "Iteration 18260 Training loss 0.015920987352728844 Validation loss 0.017071468755602837 Accuracy 0.826171875\n",
      "Iteration 18270 Training loss 0.012527193874120712 Validation loss 0.01693219505250454 Accuracy 0.82861328125\n",
      "Iteration 18280 Training loss 0.015383987687528133 Validation loss 0.017074042931199074 Accuracy 0.82666015625\n",
      "Iteration 18290 Training loss 0.0144916707649827 Validation loss 0.017144711688160896 Accuracy 0.82666015625\n",
      "Iteration 18300 Training loss 0.013712991960346699 Validation loss 0.017036445438861847 Accuracy 0.8271484375\n",
      "Iteration 18310 Training loss 0.016275985166430473 Validation loss 0.017063766717910767 Accuracy 0.82666015625\n",
      "Iteration 18320 Training loss 0.015104382298886776 Validation loss 0.01730221137404442 Accuracy 0.82373046875\n",
      "Iteration 18330 Training loss 0.013047268614172935 Validation loss 0.01701400987803936 Accuracy 0.8271484375\n",
      "Iteration 18340 Training loss 0.01301208883523941 Validation loss 0.01720760390162468 Accuracy 0.8251953125\n",
      "Iteration 18350 Training loss 0.015318854711949825 Validation loss 0.01734989508986473 Accuracy 0.82421875\n",
      "Iteration 18360 Training loss 0.01825799234211445 Validation loss 0.017190098762512207 Accuracy 0.826171875\n",
      "Iteration 18370 Training loss 0.01493010949343443 Validation loss 0.01681463047862053 Accuracy 0.82958984375\n",
      "Iteration 18380 Training loss 0.013150890357792377 Validation loss 0.017026234418153763 Accuracy 0.8271484375\n",
      "Iteration 18390 Training loss 0.01275598630309105 Validation loss 0.016852155327796936 Accuracy 0.828125\n",
      "Iteration 18400 Training loss 0.0160942692309618 Validation loss 0.01700936071574688 Accuracy 0.82763671875\n",
      "Iteration 18410 Training loss 0.014381843619048595 Validation loss 0.016945863142609596 Accuracy 0.828125\n",
      "Iteration 18420 Training loss 0.013345107436180115 Validation loss 0.016995269805192947 Accuracy 0.8271484375\n",
      "Iteration 18430 Training loss 0.014140453189611435 Validation loss 0.01747560314834118 Accuracy 0.822265625\n",
      "Iteration 18440 Training loss 0.01628149300813675 Validation loss 0.017039943486452103 Accuracy 0.82763671875\n",
      "Iteration 18450 Training loss 0.015045374631881714 Validation loss 0.016821173951029778 Accuracy 0.82861328125\n",
      "Iteration 18460 Training loss 0.013534759171307087 Validation loss 0.016690926626324654 Accuracy 0.83056640625\n",
      "Iteration 18470 Training loss 0.01591361127793789 Validation loss 0.017043210566043854 Accuracy 0.82666015625\n",
      "Iteration 18480 Training loss 0.013837706297636032 Validation loss 0.016702836379408836 Accuracy 0.82958984375\n",
      "Iteration 18490 Training loss 0.014364299364387989 Validation loss 0.016753630712628365 Accuracy 0.830078125\n",
      "Iteration 18500 Training loss 0.01256257388740778 Validation loss 0.01703972928225994 Accuracy 0.82666015625\n",
      "Iteration 18510 Training loss 0.014389799907803535 Validation loss 0.016988646239042282 Accuracy 0.82763671875\n",
      "Iteration 18520 Training loss 0.015500812791287899 Validation loss 0.017253780737519264 Accuracy 0.82568359375\n",
      "Iteration 18530 Training loss 0.013852701522409916 Validation loss 0.016966506838798523 Accuracy 0.828125\n",
      "Iteration 18540 Training loss 0.014835179783403873 Validation loss 0.01727956160902977 Accuracy 0.82421875\n",
      "Iteration 18550 Training loss 0.015219731256365776 Validation loss 0.01704508438706398 Accuracy 0.82666015625\n",
      "Iteration 18560 Training loss 0.01616407185792923 Validation loss 0.017012162134051323 Accuracy 0.82763671875\n",
      "Iteration 18570 Training loss 0.019933665171265602 Validation loss 0.01717478781938553 Accuracy 0.82568359375\n",
      "Iteration 18580 Training loss 0.016869571059942245 Validation loss 0.017485762014985085 Accuracy 0.822265625\n",
      "Iteration 18590 Training loss 0.01385924406349659 Validation loss 0.016917759552598 Accuracy 0.82861328125\n",
      "Iteration 18600 Training loss 0.016890335828065872 Validation loss 0.017016161233186722 Accuracy 0.8271484375\n",
      "Iteration 18610 Training loss 0.014755385927855968 Validation loss 0.01703001745045185 Accuracy 0.82666015625\n",
      "Iteration 18620 Training loss 0.01423726137727499 Validation loss 0.01712622307240963 Accuracy 0.826171875\n",
      "Iteration 18630 Training loss 0.014151534996926785 Validation loss 0.017071885988116264 Accuracy 0.826171875\n",
      "Iteration 18640 Training loss 0.013791928067803383 Validation loss 0.016797924414277077 Accuracy 0.82958984375\n",
      "Iteration 18650 Training loss 0.0163740161806345 Validation loss 0.017003923654556274 Accuracy 0.8271484375\n",
      "Iteration 18660 Training loss 0.014315344393253326 Validation loss 0.01672164350748062 Accuracy 0.830078125\n",
      "Iteration 18670 Training loss 0.013878331519663334 Validation loss 0.016878796741366386 Accuracy 0.82861328125\n",
      "Iteration 18680 Training loss 0.014713876880705357 Validation loss 0.016837259754538536 Accuracy 0.82958984375\n",
      "Iteration 18690 Training loss 0.01564217358827591 Validation loss 0.017025357112288475 Accuracy 0.82763671875\n",
      "Iteration 18700 Training loss 0.016320310533046722 Validation loss 0.017168628051877022 Accuracy 0.82568359375\n",
      "Iteration 18710 Training loss 0.014572059735655785 Validation loss 0.016688190400600433 Accuracy 0.830078125\n",
      "Iteration 18720 Training loss 0.011638311669230461 Validation loss 0.016784442588686943 Accuracy 0.8291015625\n",
      "Iteration 18730 Training loss 0.015722252428531647 Validation loss 0.017180170863866806 Accuracy 0.826171875\n",
      "Iteration 18740 Training loss 0.012472361326217651 Validation loss 0.01708337664604187 Accuracy 0.82666015625\n",
      "Iteration 18750 Training loss 0.012964298948645592 Validation loss 0.01677471213042736 Accuracy 0.830078125\n",
      "Iteration 18760 Training loss 0.012825840152800083 Validation loss 0.016850443556904793 Accuracy 0.82958984375\n",
      "Iteration 18770 Training loss 0.01388899888843298 Validation loss 0.017215639352798462 Accuracy 0.8251953125\n",
      "Iteration 18780 Training loss 0.017548667266964912 Validation loss 0.0167363490909338 Accuracy 0.83056640625\n",
      "Iteration 18790 Training loss 0.014354964718222618 Validation loss 0.016928156837821007 Accuracy 0.82861328125\n",
      "Iteration 18800 Training loss 0.013045377098023891 Validation loss 0.01689339615404606 Accuracy 0.82861328125\n",
      "Iteration 18810 Training loss 0.018124178051948547 Validation loss 0.0170837864279747 Accuracy 0.82666015625\n",
      "Iteration 18820 Training loss 0.014304534532129765 Validation loss 0.01701142080128193 Accuracy 0.8271484375\n",
      "Iteration 18830 Training loss 0.012746326625347137 Validation loss 0.016728952527046204 Accuracy 0.830078125\n",
      "Iteration 18840 Training loss 0.01408771239221096 Validation loss 0.017121534794569016 Accuracy 0.82568359375\n",
      "Iteration 18850 Training loss 0.015910590067505836 Validation loss 0.01699899509549141 Accuracy 0.82763671875\n",
      "Iteration 18860 Training loss 0.01608121022582054 Validation loss 0.017088498920202255 Accuracy 0.826171875\n",
      "Iteration 18870 Training loss 0.016172895208001137 Validation loss 0.017010753974318504 Accuracy 0.82763671875\n",
      "Iteration 18880 Training loss 0.014281374402344227 Validation loss 0.016802778467535973 Accuracy 0.8291015625\n",
      "Iteration 18890 Training loss 0.015143373049795628 Validation loss 0.016952430829405785 Accuracy 0.82763671875\n",
      "Iteration 18900 Training loss 0.015485158190131187 Validation loss 0.016713792458176613 Accuracy 0.830078125\n",
      "Iteration 18910 Training loss 0.013476010411977768 Validation loss 0.016948971897363663 Accuracy 0.828125\n",
      "Iteration 18920 Training loss 0.014128385111689568 Validation loss 0.016832813620567322 Accuracy 0.8291015625\n",
      "Iteration 18930 Training loss 0.014504463411867619 Validation loss 0.017051048576831818 Accuracy 0.8271484375\n",
      "Iteration 18940 Training loss 0.01720402203500271 Validation loss 0.016800865530967712 Accuracy 0.8291015625\n",
      "Iteration 18950 Training loss 0.014299837872385979 Validation loss 0.01663823612034321 Accuracy 0.83056640625\n",
      "Iteration 18960 Training loss 0.01254697423428297 Validation loss 0.016619130969047546 Accuracy 0.83154296875\n",
      "Iteration 18970 Training loss 0.012652025558054447 Validation loss 0.0170705895870924 Accuracy 0.82666015625\n",
      "Iteration 18980 Training loss 0.013528487645089626 Validation loss 0.01700296252965927 Accuracy 0.8271484375\n",
      "Iteration 18990 Training loss 0.012213160283863544 Validation loss 0.01693713106215 Accuracy 0.82763671875\n",
      "Iteration 19000 Training loss 0.014972669072449207 Validation loss 0.017237205058336258 Accuracy 0.8251953125\n",
      "Iteration 19010 Training loss 0.014329704456031322 Validation loss 0.016680285334587097 Accuracy 0.83056640625\n",
      "Iteration 19020 Training loss 0.016894791275262833 Validation loss 0.016864117234945297 Accuracy 0.8291015625\n",
      "Iteration 19030 Training loss 0.012972790747880936 Validation loss 0.016748784109950066 Accuracy 0.82958984375\n",
      "Iteration 19040 Training loss 0.012530535459518433 Validation loss 0.016808344051241875 Accuracy 0.830078125\n",
      "Iteration 19050 Training loss 0.014805654995143414 Validation loss 0.016712039709091187 Accuracy 0.830078125\n",
      "Iteration 19060 Training loss 0.017102979123592377 Validation loss 0.01729394681751728 Accuracy 0.82421875\n",
      "Iteration 19070 Training loss 0.014546269550919533 Validation loss 0.0168128851801157 Accuracy 0.82861328125\n",
      "Iteration 19080 Training loss 0.016286566853523254 Validation loss 0.017029443755745888 Accuracy 0.8271484375\n",
      "Iteration 19090 Training loss 0.013683490455150604 Validation loss 0.016631681472063065 Accuracy 0.8310546875\n",
      "Iteration 19100 Training loss 0.01254650391638279 Validation loss 0.016674870625138283 Accuracy 0.83056640625\n",
      "Iteration 19110 Training loss 0.01827334240078926 Validation loss 0.016932738944888115 Accuracy 0.828125\n",
      "Iteration 19120 Training loss 0.013695864006876945 Validation loss 0.016845043748617172 Accuracy 0.82861328125\n",
      "Iteration 19130 Training loss 0.014296992681920528 Validation loss 0.017270220443606377 Accuracy 0.82470703125\n",
      "Iteration 19140 Training loss 0.014751767739653587 Validation loss 0.01719840057194233 Accuracy 0.82568359375\n",
      "Iteration 19150 Training loss 0.016932429745793343 Validation loss 0.016793925315141678 Accuracy 0.8291015625\n",
      "Iteration 19160 Training loss 0.015932852402329445 Validation loss 0.016689902171492577 Accuracy 0.83056640625\n",
      "Iteration 19170 Training loss 0.013082233257591724 Validation loss 0.016407260671257973 Accuracy 0.83349609375\n",
      "Iteration 19180 Training loss 0.01579868234694004 Validation loss 0.016628654673695564 Accuracy 0.8310546875\n",
      "Iteration 19190 Training loss 0.013776244595646858 Validation loss 0.01638602465391159 Accuracy 0.833984375\n",
      "Iteration 19200 Training loss 0.01288018561899662 Validation loss 0.016539441421628 Accuracy 0.83251953125\n",
      "Iteration 19210 Training loss 0.014669367112219334 Validation loss 0.017039785161614418 Accuracy 0.82763671875\n",
      "Iteration 19220 Training loss 0.015126807615160942 Validation loss 0.016662323847413063 Accuracy 0.8310546875\n",
      "Iteration 19230 Training loss 0.01662469282746315 Validation loss 0.017066963016986847 Accuracy 0.8271484375\n",
      "Iteration 19240 Training loss 0.017576146870851517 Validation loss 0.017791079357266426 Accuracy 0.8193359375\n",
      "Iteration 19250 Training loss 0.016652971506118774 Validation loss 0.018219929188489914 Accuracy 0.814453125\n",
      "Iteration 19260 Training loss 0.013429063372313976 Validation loss 0.016683906316757202 Accuracy 0.830078125\n",
      "Iteration 19270 Training loss 0.013749503530561924 Validation loss 0.016581984236836433 Accuracy 0.83203125\n",
      "Iteration 19280 Training loss 0.013645182363688946 Validation loss 0.016744304448366165 Accuracy 0.830078125\n",
      "Iteration 19290 Training loss 0.015647193416953087 Validation loss 0.016688717529177666 Accuracy 0.8310546875\n",
      "Iteration 19300 Training loss 0.014586132019758224 Validation loss 0.01693834736943245 Accuracy 0.828125\n",
      "Iteration 19310 Training loss 0.015854600816965103 Validation loss 0.017147598788142204 Accuracy 0.82568359375\n",
      "Iteration 19320 Training loss 0.01651833765208721 Validation loss 0.016737036406993866 Accuracy 0.830078125\n",
      "Iteration 19330 Training loss 0.014063640497624874 Validation loss 0.016630716621875763 Accuracy 0.8310546875\n",
      "Iteration 19340 Training loss 0.01481575146317482 Validation loss 0.01694457232952118 Accuracy 0.82861328125\n",
      "Iteration 19350 Training loss 0.014905054122209549 Validation loss 0.016555488109588623 Accuracy 0.83251953125\n",
      "Iteration 19360 Training loss 0.015180175192654133 Validation loss 0.01696779578924179 Accuracy 0.82763671875\n",
      "Iteration 19370 Training loss 0.012725451029837132 Validation loss 0.016833825036883354 Accuracy 0.82861328125\n",
      "Iteration 19380 Training loss 0.013941202312707901 Validation loss 0.016574980691075325 Accuracy 0.83154296875\n",
      "Iteration 19390 Training loss 0.014568646438419819 Validation loss 0.01674307882785797 Accuracy 0.82958984375\n",
      "Iteration 19400 Training loss 0.013792048208415508 Validation loss 0.017057884484529495 Accuracy 0.82666015625\n",
      "Iteration 19410 Training loss 0.013427556492388248 Validation loss 0.016606410965323448 Accuracy 0.83203125\n",
      "Iteration 19420 Training loss 0.013746612705290318 Validation loss 0.01706172339618206 Accuracy 0.82666015625\n",
      "Iteration 19430 Training loss 0.014734362252056599 Validation loss 0.01752627082169056 Accuracy 0.82177734375\n",
      "Iteration 19440 Training loss 0.01676117070019245 Validation loss 0.01697572134435177 Accuracy 0.82763671875\n",
      "Iteration 19450 Training loss 0.01536339521408081 Validation loss 0.016806475818157196 Accuracy 0.82861328125\n",
      "Iteration 19460 Training loss 0.015267615206539631 Validation loss 0.016760453581809998 Accuracy 0.82958984375\n",
      "Iteration 19470 Training loss 0.015486539341509342 Validation loss 0.01680118404328823 Accuracy 0.8291015625\n",
      "Iteration 19480 Training loss 0.013824498280882835 Validation loss 0.01687335968017578 Accuracy 0.8291015625\n",
      "Iteration 19490 Training loss 0.014908501878380775 Validation loss 0.017094433307647705 Accuracy 0.826171875\n",
      "Iteration 19500 Training loss 0.014835764653980732 Validation loss 0.01708223670721054 Accuracy 0.82666015625\n",
      "Iteration 19510 Training loss 0.01543053425848484 Validation loss 0.016936488449573517 Accuracy 0.828125\n",
      "Iteration 19520 Training loss 0.015294601209461689 Validation loss 0.01695903018116951 Accuracy 0.828125\n",
      "Iteration 19530 Training loss 0.014160034246742725 Validation loss 0.01684601418673992 Accuracy 0.8291015625\n",
      "Iteration 19540 Training loss 0.013578595593571663 Validation loss 0.016962159425020218 Accuracy 0.828125\n",
      "Iteration 19550 Training loss 0.012761371210217476 Validation loss 0.016755903139710426 Accuracy 0.830078125\n",
      "Iteration 19560 Training loss 0.013997011817991734 Validation loss 0.0166962631046772 Accuracy 0.830078125\n",
      "Iteration 19570 Training loss 0.013059222139418125 Validation loss 0.017014067620038986 Accuracy 0.8271484375\n",
      "Iteration 19580 Training loss 0.015914827585220337 Validation loss 0.016572270542383194 Accuracy 0.83154296875\n",
      "Iteration 19590 Training loss 0.01608734205365181 Validation loss 0.01673928275704384 Accuracy 0.830078125\n",
      "Iteration 19600 Training loss 0.014399276115000248 Validation loss 0.01750071533024311 Accuracy 0.822265625\n",
      "Iteration 19610 Training loss 0.0152349341660738 Validation loss 0.017172660678625107 Accuracy 0.826171875\n",
      "Iteration 19620 Training loss 0.014055910520255566 Validation loss 0.017023127526044846 Accuracy 0.8271484375\n",
      "Iteration 19630 Training loss 0.014804587699472904 Validation loss 0.016883108764886856 Accuracy 0.8271484375\n",
      "Iteration 19640 Training loss 0.01399572379887104 Validation loss 0.016971807926893234 Accuracy 0.82666015625\n",
      "Iteration 19650 Training loss 0.015053467825055122 Validation loss 0.016882942989468575 Accuracy 0.828125\n",
      "Iteration 19660 Training loss 0.01665232889354229 Validation loss 0.016832668334245682 Accuracy 0.8291015625\n",
      "Iteration 19670 Training loss 0.014771031215786934 Validation loss 0.016842514276504517 Accuracy 0.8291015625\n",
      "Iteration 19680 Training loss 0.01520402543246746 Validation loss 0.017084645107388496 Accuracy 0.826171875\n",
      "Iteration 19690 Training loss 0.016574697569012642 Validation loss 0.017527911812067032 Accuracy 0.82177734375\n",
      "Iteration 19700 Training loss 0.015534283593297005 Validation loss 0.01689235493540764 Accuracy 0.82861328125\n",
      "Iteration 19710 Training loss 0.016670212149620056 Validation loss 0.016848918050527573 Accuracy 0.8291015625\n",
      "Iteration 19720 Training loss 0.01300747785717249 Validation loss 0.016821064054965973 Accuracy 0.8291015625\n",
      "Iteration 19730 Training loss 0.014664501883089542 Validation loss 0.01692383736371994 Accuracy 0.82861328125\n",
      "Iteration 19740 Training loss 0.014143005013465881 Validation loss 0.01691117137670517 Accuracy 0.828125\n",
      "Iteration 19750 Training loss 0.015712950378656387 Validation loss 0.01717250794172287 Accuracy 0.82568359375\n",
      "Iteration 19760 Training loss 0.013374005444347858 Validation loss 0.017080046236515045 Accuracy 0.82666015625\n",
      "Iteration 19770 Training loss 0.01219992060214281 Validation loss 0.01666584424674511 Accuracy 0.83056640625\n",
      "Iteration 19780 Training loss 0.015393310226500034 Validation loss 0.01684611104428768 Accuracy 0.82958984375\n",
      "Iteration 19790 Training loss 0.01733488403260708 Validation loss 0.016874821856617928 Accuracy 0.82861328125\n",
      "Iteration 19800 Training loss 0.015288346447050571 Validation loss 0.017522765323519707 Accuracy 0.82080078125\n",
      "Iteration 19810 Training loss 0.013606095686554909 Validation loss 0.01684793084859848 Accuracy 0.82861328125\n",
      "Iteration 19820 Training loss 0.011811144649982452 Validation loss 0.01675032451748848 Accuracy 0.83056640625\n",
      "Iteration 19830 Training loss 0.01595769263803959 Validation loss 0.017053619027137756 Accuracy 0.82666015625\n",
      "Iteration 19840 Training loss 0.01564978063106537 Validation loss 0.016563009470701218 Accuracy 0.83154296875\n",
      "Iteration 19850 Training loss 0.013819904997944832 Validation loss 0.016709567978978157 Accuracy 0.830078125\n",
      "Iteration 19860 Training loss 0.015206271782517433 Validation loss 0.01663118600845337 Accuracy 0.83154296875\n",
      "Iteration 19870 Training loss 0.015877997502684593 Validation loss 0.01666981913149357 Accuracy 0.830078125\n",
      "Iteration 19880 Training loss 0.01368116494268179 Validation loss 0.01688201166689396 Accuracy 0.82861328125\n",
      "Iteration 19890 Training loss 0.014993991702795029 Validation loss 0.01693139411509037 Accuracy 0.82861328125\n",
      "Iteration 19900 Training loss 0.014351480640470982 Validation loss 0.016741810366511345 Accuracy 0.830078125\n",
      "Iteration 19910 Training loss 0.01589275896549225 Validation loss 0.016691772267222404 Accuracy 0.830078125\n",
      "Iteration 19920 Training loss 0.014765662141144276 Validation loss 0.01679241470992565 Accuracy 0.8291015625\n",
      "Iteration 19930 Training loss 0.013890347443521023 Validation loss 0.016639582812786102 Accuracy 0.8310546875\n",
      "Iteration 19940 Training loss 0.01463047880679369 Validation loss 0.01663307659327984 Accuracy 0.8310546875\n",
      "Iteration 19950 Training loss 0.013905469328165054 Validation loss 0.01656670682132244 Accuracy 0.83154296875\n",
      "Iteration 19960 Training loss 0.012804723344743252 Validation loss 0.017062773928046227 Accuracy 0.82666015625\n",
      "Iteration 19970 Training loss 0.013833495788276196 Validation loss 0.016786357387900352 Accuracy 0.82861328125\n",
      "Iteration 19980 Training loss 0.011612234637141228 Validation loss 0.016863366588950157 Accuracy 0.82763671875\n",
      "Iteration 19990 Training loss 0.01649610884487629 Validation loss 0.01663791388273239 Accuracy 0.83056640625\n",
      "Iteration 20000 Training loss 0.014135779812932014 Validation loss 0.016720782965421677 Accuracy 0.82958984375\n",
      "Iteration 20010 Training loss 0.012624012306332588 Validation loss 0.01657397486269474 Accuracy 0.8310546875\n",
      "Iteration 20020 Training loss 0.012483020313084126 Validation loss 0.016577109694480896 Accuracy 0.83056640625\n",
      "Iteration 20030 Training loss 0.01413639821112156 Validation loss 0.01790136657655239 Accuracy 0.81787109375\n",
      "Iteration 20040 Training loss 0.013919955119490623 Validation loss 0.016593612730503082 Accuracy 0.8310546875\n",
      "Iteration 20050 Training loss 0.014584461227059364 Validation loss 0.01682872325181961 Accuracy 0.82861328125\n",
      "Iteration 20060 Training loss 0.015305236913263798 Validation loss 0.016680318862199783 Accuracy 0.83056640625\n",
      "Iteration 20070 Training loss 0.01370547991245985 Validation loss 0.01659580133855343 Accuracy 0.8310546875\n",
      "Iteration 20080 Training loss 0.016729040071368217 Validation loss 0.017087657004594803 Accuracy 0.82666015625\n",
      "Iteration 20090 Training loss 0.013898060657083988 Validation loss 0.017014548182487488 Accuracy 0.82763671875\n",
      "Iteration 20100 Training loss 0.016843346878886223 Validation loss 0.016956128180027008 Accuracy 0.82763671875\n",
      "Iteration 20110 Training loss 0.01454367395490408 Validation loss 0.016865964978933334 Accuracy 0.8291015625\n",
      "Iteration 20120 Training loss 0.014852185733616352 Validation loss 0.016663743183016777 Accuracy 0.830078125\n",
      "Iteration 20130 Training loss 0.014919470064342022 Validation loss 0.0165101308375597 Accuracy 0.83203125\n",
      "Iteration 20140 Training loss 0.013279835693538189 Validation loss 0.016645310446619987 Accuracy 0.8310546875\n",
      "Iteration 20150 Training loss 0.013541587628424168 Validation loss 0.01684907265007496 Accuracy 0.82861328125\n",
      "Iteration 20160 Training loss 0.01635575480759144 Validation loss 0.016726795583963394 Accuracy 0.82958984375\n",
      "Iteration 20170 Training loss 0.014010123908519745 Validation loss 0.016785716637969017 Accuracy 0.82958984375\n",
      "Iteration 20180 Training loss 0.012658714316785336 Validation loss 0.01676461100578308 Accuracy 0.82958984375\n",
      "Iteration 20190 Training loss 0.01611044630408287 Validation loss 0.016838140785694122 Accuracy 0.8291015625\n",
      "Iteration 20200 Training loss 0.013054990209639072 Validation loss 0.01664171740412712 Accuracy 0.8310546875\n",
      "Iteration 20210 Training loss 0.012858428992331028 Validation loss 0.016628963872790337 Accuracy 0.8310546875\n",
      "Iteration 20220 Training loss 0.013070055283606052 Validation loss 0.016679203137755394 Accuracy 0.83056640625\n",
      "Iteration 20230 Training loss 0.01723942905664444 Validation loss 0.01702810823917389 Accuracy 0.82666015625\n",
      "Iteration 20240 Training loss 0.015292111784219742 Validation loss 0.016678636893630028 Accuracy 0.830078125\n",
      "Iteration 20250 Training loss 0.016259759664535522 Validation loss 0.016693556681275368 Accuracy 0.830078125\n",
      "Iteration 20260 Training loss 0.014444323256611824 Validation loss 0.016700221225619316 Accuracy 0.830078125\n",
      "Iteration 20270 Training loss 0.015923693776130676 Validation loss 0.017200713977217674 Accuracy 0.8251953125\n",
      "Iteration 20280 Training loss 0.012638282962143421 Validation loss 0.016808832064270973 Accuracy 0.828125\n",
      "Iteration 20290 Training loss 0.01582682505249977 Validation loss 0.016872474923729897 Accuracy 0.8291015625\n",
      "Iteration 20300 Training loss 0.014551048167049885 Validation loss 0.016944942995905876 Accuracy 0.8271484375\n",
      "Iteration 20310 Training loss 0.015018041245639324 Validation loss 0.01668558269739151 Accuracy 0.83056640625\n",
      "Iteration 20320 Training loss 0.014271239750087261 Validation loss 0.016630427911877632 Accuracy 0.83056640625\n",
      "Iteration 20330 Training loss 0.013945638202130795 Validation loss 0.016998860985040665 Accuracy 0.82763671875\n",
      "Iteration 20340 Training loss 0.017271090298891068 Validation loss 0.017035698518157005 Accuracy 0.82666015625\n",
      "Iteration 20350 Training loss 0.014737141318619251 Validation loss 0.016690026968717575 Accuracy 0.830078125\n",
      "Iteration 20360 Training loss 0.0128287672996521 Validation loss 0.016995124518871307 Accuracy 0.82666015625\n",
      "Iteration 20370 Training loss 0.014370748773217201 Validation loss 0.016673315316438675 Accuracy 0.830078125\n",
      "Iteration 20380 Training loss 0.013214709237217903 Validation loss 0.016703525558114052 Accuracy 0.830078125\n",
      "Iteration 20390 Training loss 0.014331054873764515 Validation loss 0.01672705076634884 Accuracy 0.8291015625\n",
      "Iteration 20400 Training loss 0.01787661947309971 Validation loss 0.016940241679549217 Accuracy 0.82763671875\n",
      "Iteration 20410 Training loss 0.012738713063299656 Validation loss 0.017317909747362137 Accuracy 0.82373046875\n",
      "Iteration 20420 Training loss 0.015137833543121815 Validation loss 0.017284825444221497 Accuracy 0.8232421875\n",
      "Iteration 20430 Training loss 0.016031546518206596 Validation loss 0.017298104241490364 Accuracy 0.82373046875\n",
      "Iteration 20440 Training loss 0.01599823869764805 Validation loss 0.0165420975536108 Accuracy 0.83203125\n",
      "Iteration 20450 Training loss 0.014466111548244953 Validation loss 0.01654018834233284 Accuracy 0.83154296875\n",
      "Iteration 20460 Training loss 0.015754366293549538 Validation loss 0.016990670934319496 Accuracy 0.8271484375\n",
      "Iteration 20470 Training loss 0.012788547202944756 Validation loss 0.016701141372323036 Accuracy 0.830078125\n",
      "Iteration 20480 Training loss 0.015662269666790962 Validation loss 0.017235830426216125 Accuracy 0.82470703125\n",
      "Iteration 20490 Training loss 0.016451211646199226 Validation loss 0.016528399661183357 Accuracy 0.83154296875\n",
      "Iteration 20500 Training loss 0.013270067982375622 Validation loss 0.016608983278274536 Accuracy 0.8310546875\n",
      "Iteration 20510 Training loss 0.014616205357015133 Validation loss 0.017194895073771477 Accuracy 0.8251953125\n",
      "Iteration 20520 Training loss 0.014597218483686447 Validation loss 0.01682579703629017 Accuracy 0.82861328125\n",
      "Iteration 20530 Training loss 0.015867330133914948 Validation loss 0.01674182340502739 Accuracy 0.830078125\n",
      "Iteration 20540 Training loss 0.012428291141986847 Validation loss 0.0168167594820261 Accuracy 0.8291015625\n",
      "Iteration 20550 Training loss 0.01300058513879776 Validation loss 0.016752639785408974 Accuracy 0.830078125\n",
      "Iteration 20560 Training loss 0.015390679240226746 Validation loss 0.016877571120858192 Accuracy 0.82861328125\n",
      "Iteration 20570 Training loss 0.011925043538212776 Validation loss 0.016922570765018463 Accuracy 0.82763671875\n",
      "Iteration 20580 Training loss 0.012982533313333988 Validation loss 0.016815239563584328 Accuracy 0.8291015625\n",
      "Iteration 20590 Training loss 0.01451056357473135 Validation loss 0.017189497128129005 Accuracy 0.8251953125\n",
      "Iteration 20600 Training loss 0.015697363764047623 Validation loss 0.017059961333870888 Accuracy 0.82666015625\n",
      "Iteration 20610 Training loss 0.012780601158738136 Validation loss 0.01684221811592579 Accuracy 0.82861328125\n",
      "Iteration 20620 Training loss 0.015433168970048428 Validation loss 0.016746437177062035 Accuracy 0.8291015625\n",
      "Iteration 20630 Training loss 0.013138915412127972 Validation loss 0.0166261438280344 Accuracy 0.83154296875\n",
      "Iteration 20640 Training loss 0.015348108485341072 Validation loss 0.016518021002411842 Accuracy 0.83154296875\n",
      "Iteration 20650 Training loss 0.012342825531959534 Validation loss 0.016555270180106163 Accuracy 0.83203125\n",
      "Iteration 20660 Training loss 0.015537388622760773 Validation loss 0.016702469438314438 Accuracy 0.82958984375\n",
      "Iteration 20670 Training loss 0.01128934882581234 Validation loss 0.016719594597816467 Accuracy 0.83056640625\n",
      "Iteration 20680 Training loss 0.014431551098823547 Validation loss 0.016801290214061737 Accuracy 0.82861328125\n",
      "Iteration 20690 Training loss 0.016573505476117134 Validation loss 0.01666249893605709 Accuracy 0.83056640625\n",
      "Iteration 20700 Training loss 0.015377526171505451 Validation loss 0.01669125445187092 Accuracy 0.82958984375\n",
      "Iteration 20710 Training loss 0.020044885575771332 Validation loss 0.016813695430755615 Accuracy 0.828125\n",
      "Iteration 20720 Training loss 0.015992267057299614 Validation loss 0.01688835211098194 Accuracy 0.82763671875\n",
      "Iteration 20730 Training loss 0.014928065240383148 Validation loss 0.0164942629635334 Accuracy 0.83251953125\n",
      "Iteration 20740 Training loss 0.013748984783887863 Validation loss 0.016802625730633736 Accuracy 0.82958984375\n",
      "Iteration 20750 Training loss 0.01203212607651949 Validation loss 0.016497496515512466 Accuracy 0.83203125\n",
      "Iteration 20760 Training loss 0.015506386756896973 Validation loss 0.01671256124973297 Accuracy 0.830078125\n",
      "Iteration 20770 Training loss 0.012487247586250305 Validation loss 0.016738945618271828 Accuracy 0.82958984375\n",
      "Iteration 20780 Training loss 0.016598520800471306 Validation loss 0.01724187657237053 Accuracy 0.82421875\n",
      "Iteration 20790 Training loss 0.015273801051080227 Validation loss 0.016847245395183563 Accuracy 0.82861328125\n",
      "Iteration 20800 Training loss 0.014650876633822918 Validation loss 0.01661311276257038 Accuracy 0.83056640625\n",
      "Iteration 20810 Training loss 0.016063448041677475 Validation loss 0.017088714987039566 Accuracy 0.826171875\n",
      "Iteration 20820 Training loss 0.01420595496892929 Validation loss 0.016787288710474968 Accuracy 0.82958984375\n",
      "Iteration 20830 Training loss 0.01476394385099411 Validation loss 0.016908664256334305 Accuracy 0.82861328125\n",
      "Iteration 20840 Training loss 0.01145399734377861 Validation loss 0.01676241122186184 Accuracy 0.8291015625\n",
      "Iteration 20850 Training loss 0.01545371487736702 Validation loss 0.016462335363030434 Accuracy 0.83251953125\n",
      "Iteration 20860 Training loss 0.017618747428059578 Validation loss 0.016286300495266914 Accuracy 0.83447265625\n",
      "Iteration 20870 Training loss 0.014517460949718952 Validation loss 0.01648271456360817 Accuracy 0.83203125\n",
      "Iteration 20880 Training loss 0.01588135026395321 Validation loss 0.016782650724053383 Accuracy 0.828125\n",
      "Iteration 20890 Training loss 0.01373123750090599 Validation loss 0.016922306269407272 Accuracy 0.82763671875\n",
      "Iteration 20900 Training loss 0.015242299064993858 Validation loss 0.017178025096654892 Accuracy 0.82568359375\n",
      "Iteration 20910 Training loss 0.015691831707954407 Validation loss 0.016478661447763443 Accuracy 0.83203125\n",
      "Iteration 20920 Training loss 0.012401191517710686 Validation loss 0.016937481239438057 Accuracy 0.8271484375\n",
      "Iteration 20930 Training loss 0.014112601988017559 Validation loss 0.01671142689883709 Accuracy 0.830078125\n",
      "Iteration 20940 Training loss 0.013375871814787388 Validation loss 0.016820432618260384 Accuracy 0.82861328125\n",
      "Iteration 20950 Training loss 0.0130948256701231 Validation loss 0.016840919852256775 Accuracy 0.828125\n",
      "Iteration 20960 Training loss 0.01444698590785265 Validation loss 0.016525771468877792 Accuracy 0.83154296875\n",
      "Iteration 20970 Training loss 0.01354394294321537 Validation loss 0.01644519902765751 Accuracy 0.83203125\n",
      "Iteration 20980 Training loss 0.013705886900424957 Validation loss 0.016757575795054436 Accuracy 0.8291015625\n",
      "Iteration 20990 Training loss 0.013193152844905853 Validation loss 0.016574056819081306 Accuracy 0.83154296875\n",
      "Iteration 21000 Training loss 0.01235128939151764 Validation loss 0.016662605106830597 Accuracy 0.82958984375\n",
      "Iteration 21010 Training loss 0.012444903142750263 Validation loss 0.01636846363544464 Accuracy 0.83349609375\n",
      "Iteration 21020 Training loss 0.016501188278198242 Validation loss 0.01693054661154747 Accuracy 0.828125\n",
      "Iteration 21030 Training loss 0.012844078242778778 Validation loss 0.016596980392932892 Accuracy 0.830078125\n",
      "Iteration 21040 Training loss 0.015658777207136154 Validation loss 0.016688140109181404 Accuracy 0.830078125\n",
      "Iteration 21050 Training loss 0.01728096231818199 Validation loss 0.016965992748737335 Accuracy 0.8271484375\n",
      "Iteration 21060 Training loss 0.014765736646950245 Validation loss 0.01680172234773636 Accuracy 0.8291015625\n",
      "Iteration 21070 Training loss 0.014611577615141869 Validation loss 0.016691502183675766 Accuracy 0.830078125\n",
      "Iteration 21080 Training loss 0.015136352740228176 Validation loss 0.016733232885599136 Accuracy 0.82958984375\n",
      "Iteration 21090 Training loss 0.013017931021749973 Validation loss 0.01683957874774933 Accuracy 0.82861328125\n",
      "Iteration 21100 Training loss 0.015967626124620438 Validation loss 0.016896212473511696 Accuracy 0.828125\n",
      "Iteration 21110 Training loss 0.015072565525770187 Validation loss 0.01697506755590439 Accuracy 0.82666015625\n",
      "Iteration 21120 Training loss 0.013313857838511467 Validation loss 0.01657727174460888 Accuracy 0.83056640625\n",
      "Iteration 21130 Training loss 0.014461934566497803 Validation loss 0.016604691743850708 Accuracy 0.83056640625\n",
      "Iteration 21140 Training loss 0.013538372702896595 Validation loss 0.016626203432679176 Accuracy 0.83056640625\n",
      "Iteration 21150 Training loss 0.016898497939109802 Validation loss 0.016797224059700966 Accuracy 0.82958984375\n",
      "Iteration 21160 Training loss 0.013195352628827095 Validation loss 0.016845596954226494 Accuracy 0.82861328125\n",
      "Iteration 21170 Training loss 0.01299544982612133 Validation loss 0.01667121984064579 Accuracy 0.830078125\n",
      "Iteration 21180 Training loss 0.014212925918400288 Validation loss 0.016603222116827965 Accuracy 0.8310546875\n",
      "Iteration 21190 Training loss 0.014043658040463924 Validation loss 0.016551615670323372 Accuracy 0.8310546875\n",
      "Iteration 21200 Training loss 0.014556334353983402 Validation loss 0.01681598834693432 Accuracy 0.8291015625\n",
      "Iteration 21210 Training loss 0.013252182863652706 Validation loss 0.016845813021063805 Accuracy 0.82861328125\n",
      "Iteration 21220 Training loss 0.01444330159574747 Validation loss 0.017097828909754753 Accuracy 0.8251953125\n",
      "Iteration 21230 Training loss 0.015264205634593964 Validation loss 0.016889294609427452 Accuracy 0.82861328125\n",
      "Iteration 21240 Training loss 0.013529987074434757 Validation loss 0.016691677272319794 Accuracy 0.8291015625\n",
      "Iteration 21250 Training loss 0.013678204268217087 Validation loss 0.01650550216436386 Accuracy 0.83154296875\n",
      "Iteration 21260 Training loss 0.013712124899029732 Validation loss 0.01681661605834961 Accuracy 0.8291015625\n",
      "Iteration 21270 Training loss 0.01627783663570881 Validation loss 0.016767451539635658 Accuracy 0.82861328125\n",
      "Iteration 21280 Training loss 0.015524187125265598 Validation loss 0.016992729157209396 Accuracy 0.82666015625\n",
      "Iteration 21290 Training loss 0.011619466356933117 Validation loss 0.016907256096601486 Accuracy 0.828125\n",
      "Iteration 21300 Training loss 0.013061245903372765 Validation loss 0.017019107937812805 Accuracy 0.82666015625\n",
      "Iteration 21310 Training loss 0.01253554504364729 Validation loss 0.01684281788766384 Accuracy 0.828125\n",
      "Iteration 21320 Training loss 0.013913113623857498 Validation loss 0.016827689483761787 Accuracy 0.828125\n",
      "Iteration 21330 Training loss 0.014439528807997704 Validation loss 0.016780683770775795 Accuracy 0.82861328125\n",
      "Iteration 21340 Training loss 0.014514520764350891 Validation loss 0.01648402214050293 Accuracy 0.83203125\n",
      "Iteration 21350 Training loss 0.011625390499830246 Validation loss 0.01680029183626175 Accuracy 0.8291015625\n",
      "Iteration 21360 Training loss 0.017564287409186363 Validation loss 0.016687221825122833 Accuracy 0.830078125\n",
      "Iteration 21370 Training loss 0.014068960212171078 Validation loss 0.016687946394085884 Accuracy 0.830078125\n",
      "Iteration 21380 Training loss 0.015041287988424301 Validation loss 0.01697380095720291 Accuracy 0.826171875\n",
      "Iteration 21390 Training loss 0.014286789111793041 Validation loss 0.01647697575390339 Accuracy 0.83203125\n",
      "Iteration 21400 Training loss 0.012388572096824646 Validation loss 0.016889067366719246 Accuracy 0.82666015625\n",
      "Iteration 21410 Training loss 0.012806002981960773 Validation loss 0.016949854791164398 Accuracy 0.82763671875\n",
      "Iteration 21420 Training loss 0.014138824306428432 Validation loss 0.01672266609966755 Accuracy 0.82958984375\n",
      "Iteration 21430 Training loss 0.014111927710473537 Validation loss 0.016559256240725517 Accuracy 0.83203125\n",
      "Iteration 21440 Training loss 0.013980426825582981 Validation loss 0.01661885902285576 Accuracy 0.830078125\n",
      "Iteration 21450 Training loss 0.015609048306941986 Validation loss 0.016943147405982018 Accuracy 0.82666015625\n",
      "Iteration 21460 Training loss 0.014395104721188545 Validation loss 0.016594557091593742 Accuracy 0.83056640625\n",
      "Iteration 21470 Training loss 0.015612921677529812 Validation loss 0.016495531424880028 Accuracy 0.83203125\n",
      "Iteration 21480 Training loss 0.012279589660465717 Validation loss 0.01647314243018627 Accuracy 0.83203125\n",
      "Iteration 21490 Training loss 0.012929092161357403 Validation loss 0.016360681504011154 Accuracy 0.83349609375\n",
      "Iteration 21500 Training loss 0.013707885518670082 Validation loss 0.016503416001796722 Accuracy 0.83154296875\n",
      "Iteration 21510 Training loss 0.01281706988811493 Validation loss 0.016802126541733742 Accuracy 0.82861328125\n",
      "Iteration 21520 Training loss 0.015400156378746033 Validation loss 0.01689678616821766 Accuracy 0.8271484375\n",
      "Iteration 21530 Training loss 0.013741910457611084 Validation loss 0.01648501679301262 Accuracy 0.83154296875\n",
      "Iteration 21540 Training loss 0.0129217728972435 Validation loss 0.01659255288541317 Accuracy 0.83056640625\n",
      "Iteration 21550 Training loss 0.013787223957479 Validation loss 0.016442814841866493 Accuracy 0.83203125\n",
      "Iteration 21560 Training loss 0.015537865459918976 Validation loss 0.016710959374904633 Accuracy 0.8291015625\n",
      "Iteration 21570 Training loss 0.016384124755859375 Validation loss 0.016716264188289642 Accuracy 0.8291015625\n",
      "Iteration 21580 Training loss 0.014982372522354126 Validation loss 0.01691148430109024 Accuracy 0.82763671875\n",
      "Iteration 21590 Training loss 0.01254815049469471 Validation loss 0.01666405238211155 Accuracy 0.830078125\n",
      "Iteration 21600 Training loss 0.015879372134804726 Validation loss 0.01688321679830551 Accuracy 0.8271484375\n",
      "Iteration 21610 Training loss 0.014156732708215714 Validation loss 0.01682058349251747 Accuracy 0.828125\n",
      "Iteration 21620 Training loss 0.011278437450528145 Validation loss 0.01664554886519909 Accuracy 0.83056640625\n",
      "Iteration 21630 Training loss 0.011661241762340069 Validation loss 0.01650933362543583 Accuracy 0.83154296875\n",
      "Iteration 21640 Training loss 0.01694279909133911 Validation loss 0.01669848896563053 Accuracy 0.82958984375\n",
      "Iteration 21650 Training loss 0.015504308044910431 Validation loss 0.01668965257704258 Accuracy 0.83056640625\n",
      "Iteration 21660 Training loss 0.015000601299107075 Validation loss 0.016790257766842842 Accuracy 0.8291015625\n",
      "Iteration 21670 Training loss 0.018476104363799095 Validation loss 0.016913646832108498 Accuracy 0.82763671875\n",
      "Iteration 21680 Training loss 0.013362887315452099 Validation loss 0.016709769144654274 Accuracy 0.8291015625\n",
      "Iteration 21690 Training loss 0.014640693552792072 Validation loss 0.016717683523893356 Accuracy 0.82958984375\n",
      "Iteration 21700 Training loss 0.012468939647078514 Validation loss 0.016801174730062485 Accuracy 0.82861328125\n",
      "Iteration 21710 Training loss 0.01526031456887722 Validation loss 0.016694478690624237 Accuracy 0.830078125\n",
      "Iteration 21720 Training loss 0.013542300090193748 Validation loss 0.01672760397195816 Accuracy 0.830078125\n",
      "Iteration 21730 Training loss 0.013467537239193916 Validation loss 0.016764098778367043 Accuracy 0.82958984375\n",
      "Iteration 21740 Training loss 0.012369999662041664 Validation loss 0.016680236905813217 Accuracy 0.82958984375\n",
      "Iteration 21750 Training loss 0.015400341711938381 Validation loss 0.01687764935195446 Accuracy 0.828125\n",
      "Iteration 21760 Training loss 0.013833081349730492 Validation loss 0.01663217693567276 Accuracy 0.83056640625\n",
      "Iteration 21770 Training loss 0.014304042793810368 Validation loss 0.017056379467248917 Accuracy 0.826171875\n",
      "Iteration 21780 Training loss 0.017662828788161278 Validation loss 0.01752639375627041 Accuracy 0.8203125\n",
      "Iteration 21790 Training loss 0.013252057135105133 Validation loss 0.016635151579976082 Accuracy 0.83056640625\n",
      "Iteration 21800 Training loss 0.011733975261449814 Validation loss 0.01665647327899933 Accuracy 0.82958984375\n",
      "Iteration 21810 Training loss 0.012672086246311665 Validation loss 0.01651291735470295 Accuracy 0.82958984375\n",
      "Iteration 21820 Training loss 0.014105346985161304 Validation loss 0.016568468883633614 Accuracy 0.8291015625\n",
      "Iteration 21830 Training loss 0.015839194878935814 Validation loss 0.01669388636946678 Accuracy 0.8291015625\n",
      "Iteration 21840 Training loss 0.01345133874565363 Validation loss 0.016705017536878586 Accuracy 0.82958984375\n",
      "Iteration 21850 Training loss 0.013601241633296013 Validation loss 0.01656251586973667 Accuracy 0.83056640625\n",
      "Iteration 21860 Training loss 0.013234629295766354 Validation loss 0.016629349440336227 Accuracy 0.830078125\n",
      "Iteration 21870 Training loss 0.012695351615548134 Validation loss 0.01640281267464161 Accuracy 0.83251953125\n",
      "Iteration 21880 Training loss 0.014585763216018677 Validation loss 0.016622034832835197 Accuracy 0.82958984375\n",
      "Iteration 21890 Training loss 0.017181381583213806 Validation loss 0.016419576480984688 Accuracy 0.8330078125\n",
      "Iteration 21900 Training loss 0.013300999999046326 Validation loss 0.016646109521389008 Accuracy 0.83056640625\n",
      "Iteration 21910 Training loss 0.012927891686558723 Validation loss 0.01629025675356388 Accuracy 0.83349609375\n",
      "Iteration 21920 Training loss 0.015923790633678436 Validation loss 0.01640891097486019 Accuracy 0.8330078125\n",
      "Iteration 21930 Training loss 0.01357426680624485 Validation loss 0.016689321026206017 Accuracy 0.8291015625\n",
      "Iteration 21940 Training loss 0.015411592088639736 Validation loss 0.016550427302718163 Accuracy 0.83154296875\n",
      "Iteration 21950 Training loss 0.012944234535098076 Validation loss 0.016339372843503952 Accuracy 0.8330078125\n",
      "Iteration 21960 Training loss 0.014520777389407158 Validation loss 0.016792679205536842 Accuracy 0.8291015625\n",
      "Iteration 21970 Training loss 0.013576759025454521 Validation loss 0.016599299386143684 Accuracy 0.83056640625\n",
      "Iteration 21980 Training loss 0.01518873032182455 Validation loss 0.01657940074801445 Accuracy 0.830078125\n",
      "Iteration 21990 Training loss 0.0128968246281147 Validation loss 0.016598640009760857 Accuracy 0.83056640625\n",
      "Iteration 22000 Training loss 0.015339304693043232 Validation loss 0.016442611813545227 Accuracy 0.83203125\n",
      "Iteration 22010 Training loss 0.01370552834123373 Validation loss 0.016401872038841248 Accuracy 0.83251953125\n",
      "Iteration 22020 Training loss 0.012044603936374187 Validation loss 0.016272295266389847 Accuracy 0.83349609375\n",
      "Iteration 22030 Training loss 0.01355766411870718 Validation loss 0.016306456178426743 Accuracy 0.83349609375\n",
      "Iteration 22040 Training loss 0.014131072908639908 Validation loss 0.01609564572572708 Accuracy 0.83544921875\n",
      "Iteration 22050 Training loss 0.012235702015459538 Validation loss 0.016664663329720497 Accuracy 0.830078125\n",
      "Iteration 22060 Training loss 0.014276769012212753 Validation loss 0.01674691215157509 Accuracy 0.82958984375\n",
      "Iteration 22070 Training loss 0.0123136593028903 Validation loss 0.016848722472786903 Accuracy 0.82763671875\n",
      "Iteration 22080 Training loss 0.014332649298012257 Validation loss 0.01675230823457241 Accuracy 0.828125\n",
      "Iteration 22090 Training loss 0.013650515116751194 Validation loss 0.01631057634949684 Accuracy 0.83349609375\n",
      "Iteration 22100 Training loss 0.014040989801287651 Validation loss 0.01673947647213936 Accuracy 0.8291015625\n",
      "Iteration 22110 Training loss 0.013153592124581337 Validation loss 0.016590021550655365 Accuracy 0.830078125\n",
      "Iteration 22120 Training loss 0.012638084590435028 Validation loss 0.0164662916213274 Accuracy 0.8310546875\n",
      "Iteration 22130 Training loss 0.01473516970872879 Validation loss 0.016330929473042488 Accuracy 0.83349609375\n",
      "Iteration 22140 Training loss 0.013227098621428013 Validation loss 0.016222933307290077 Accuracy 0.83447265625\n",
      "Iteration 22150 Training loss 0.014150640927255154 Validation loss 0.0164082832634449 Accuracy 0.83251953125\n",
      "Iteration 22160 Training loss 0.012289827689528465 Validation loss 0.016501393169164658 Accuracy 0.8310546875\n",
      "Iteration 22170 Training loss 0.012421404011547565 Validation loss 0.01633557863533497 Accuracy 0.8330078125\n",
      "Iteration 22180 Training loss 0.017948908731341362 Validation loss 0.017479661852121353 Accuracy 0.8203125\n",
      "Iteration 22190 Training loss 0.016576137393712997 Validation loss 0.017125075682997704 Accuracy 0.8251953125\n",
      "Iteration 22200 Training loss 0.013787548989057541 Validation loss 0.016084739938378334 Accuracy 0.83544921875\n",
      "Iteration 22210 Training loss 0.01211159024387598 Validation loss 0.016326583921909332 Accuracy 0.83251953125\n",
      "Iteration 22220 Training loss 0.014254460111260414 Validation loss 0.01629837043583393 Accuracy 0.83251953125\n",
      "Iteration 22230 Training loss 0.014507138170301914 Validation loss 0.016244225203990936 Accuracy 0.83349609375\n",
      "Iteration 22240 Training loss 0.01848282665014267 Validation loss 0.016779348254203796 Accuracy 0.82763671875\n",
      "Iteration 22250 Training loss 0.013507837429642677 Validation loss 0.016383029520511627 Accuracy 0.83203125\n",
      "Iteration 22260 Training loss 0.012648460455238819 Validation loss 0.016427822411060333 Accuracy 0.83154296875\n",
      "Iteration 22270 Training loss 0.011995716951787472 Validation loss 0.01615702360868454 Accuracy 0.833984375\n",
      "Iteration 22280 Training loss 0.012991257011890411 Validation loss 0.016016600653529167 Accuracy 0.83642578125\n",
      "Iteration 22290 Training loss 0.01565389335155487 Validation loss 0.016909897327423096 Accuracy 0.8271484375\n",
      "Iteration 22300 Training loss 0.012237233109772205 Validation loss 0.01607395149767399 Accuracy 0.8349609375\n",
      "Iteration 22310 Training loss 0.013955173082649708 Validation loss 0.01642720215022564 Accuracy 0.83203125\n",
      "Iteration 22320 Training loss 0.016792284324765205 Validation loss 0.016597960144281387 Accuracy 0.8310546875\n",
      "Iteration 22330 Training loss 0.01513453759253025 Validation loss 0.0161757729947567 Accuracy 0.8349609375\n",
      "Iteration 22340 Training loss 0.012551089748740196 Validation loss 0.016167165711522102 Accuracy 0.833984375\n",
      "Iteration 22350 Training loss 0.012500917539000511 Validation loss 0.016540372744202614 Accuracy 0.83056640625\n",
      "Iteration 22360 Training loss 0.015345710329711437 Validation loss 0.016391277313232422 Accuracy 0.83154296875\n",
      "Iteration 22370 Training loss 0.014487207867205143 Validation loss 0.01617615297436714 Accuracy 0.833984375\n",
      "Iteration 22380 Training loss 0.014401989988982677 Validation loss 0.01617060787975788 Accuracy 0.83447265625\n",
      "Iteration 22390 Training loss 0.01398110669106245 Validation loss 0.016429224982857704 Accuracy 0.83154296875\n",
      "Iteration 22400 Training loss 0.01373431645333767 Validation loss 0.016467519104480743 Accuracy 0.830078125\n",
      "Iteration 22410 Training loss 0.015608366578817368 Validation loss 0.015987291932106018 Accuracy 0.83642578125\n",
      "Iteration 22420 Training loss 0.012302875518798828 Validation loss 0.016157981008291245 Accuracy 0.83447265625\n",
      "Iteration 22430 Training loss 0.015598085708916187 Validation loss 0.016745800152420998 Accuracy 0.82861328125\n",
      "Iteration 22440 Training loss 0.012065625749528408 Validation loss 0.015960969030857086 Accuracy 0.83642578125\n",
      "Iteration 22450 Training loss 0.013832524418830872 Validation loss 0.016063526272773743 Accuracy 0.83544921875\n",
      "Iteration 22460 Training loss 0.015190125443041325 Validation loss 0.017024312168359756 Accuracy 0.8251953125\n",
      "Iteration 22470 Training loss 0.012002691626548767 Validation loss 0.016324257478117943 Accuracy 0.83251953125\n",
      "Iteration 22480 Training loss 0.012964874505996704 Validation loss 0.01644917018711567 Accuracy 0.83154296875\n",
      "Iteration 22490 Training loss 0.014407218433916569 Validation loss 0.01649511232972145 Accuracy 0.83056640625\n",
      "Iteration 22500 Training loss 0.013650696724653244 Validation loss 0.016027823090553284 Accuracy 0.8349609375\n",
      "Iteration 22510 Training loss 0.013359186239540577 Validation loss 0.015969209372997284 Accuracy 0.8369140625\n",
      "Iteration 22520 Training loss 0.01246881578117609 Validation loss 0.016303809359669685 Accuracy 0.83349609375\n",
      "Iteration 22530 Training loss 0.01589467003941536 Validation loss 0.01645619422197342 Accuracy 0.83154296875\n",
      "Iteration 22540 Training loss 0.014689876697957516 Validation loss 0.01607736200094223 Accuracy 0.8349609375\n",
      "Iteration 22550 Training loss 0.014615170657634735 Validation loss 0.01606900803744793 Accuracy 0.83544921875\n",
      "Iteration 22560 Training loss 0.015670115128159523 Validation loss 0.016330378130078316 Accuracy 0.8330078125\n",
      "Iteration 22570 Training loss 0.016302570700645447 Validation loss 0.016839275136590004 Accuracy 0.82763671875\n",
      "Iteration 22580 Training loss 0.015075772069394588 Validation loss 0.01608959026634693 Accuracy 0.83447265625\n",
      "Iteration 22590 Training loss 0.014826850965619087 Validation loss 0.016414690762758255 Accuracy 0.83056640625\n",
      "Iteration 22600 Training loss 0.015244250185787678 Validation loss 0.016711601987481117 Accuracy 0.82861328125\n",
      "Iteration 22610 Training loss 0.013128044083714485 Validation loss 0.016436340287327766 Accuracy 0.8310546875\n",
      "Iteration 22620 Training loss 0.012964463792741299 Validation loss 0.016270633786916733 Accuracy 0.833984375\n",
      "Iteration 22630 Training loss 0.016099920496344566 Validation loss 0.016287876293063164 Accuracy 0.8330078125\n",
      "Iteration 22640 Training loss 0.012650414370000362 Validation loss 0.01596711575984955 Accuracy 0.8369140625\n",
      "Iteration 22650 Training loss 0.01384732685983181 Validation loss 0.016199307516217232 Accuracy 0.83447265625\n",
      "Iteration 22660 Training loss 0.013922406360507011 Validation loss 0.01640646532177925 Accuracy 0.83349609375\n",
      "Iteration 22670 Training loss 0.014368347823619843 Validation loss 0.016675876453518867 Accuracy 0.82861328125\n",
      "Iteration 22680 Training loss 0.014455052092671394 Validation loss 0.01608901657164097 Accuracy 0.83544921875\n",
      "Iteration 22690 Training loss 0.014796817675232887 Validation loss 0.018147725611925125 Accuracy 0.814453125\n",
      "Iteration 22700 Training loss 0.012375100515782833 Validation loss 0.01638101227581501 Accuracy 0.83154296875\n",
      "Iteration 22710 Training loss 0.01102793961763382 Validation loss 0.01609630323946476 Accuracy 0.83544921875\n",
      "Iteration 22720 Training loss 0.014483604580163956 Validation loss 0.01648440584540367 Accuracy 0.830078125\n",
      "Iteration 22730 Training loss 0.012838874943554401 Validation loss 0.016116918995976448 Accuracy 0.83447265625\n",
      "Iteration 22740 Training loss 0.01622822880744934 Validation loss 0.016339899972081184 Accuracy 0.83203125\n",
      "Iteration 22750 Training loss 0.015467292629182339 Validation loss 0.015937482938170433 Accuracy 0.8369140625\n",
      "Iteration 22760 Training loss 0.016700459644198418 Validation loss 0.019114771857857704 Accuracy 0.8037109375\n",
      "Iteration 22770 Training loss 0.015363924205303192 Validation loss 0.01619045063853264 Accuracy 0.833984375\n",
      "Iteration 22780 Training loss 0.013367106206715107 Validation loss 0.0163982342928648 Accuracy 0.83251953125\n",
      "Iteration 22790 Training loss 0.014980747364461422 Validation loss 0.016206832602620125 Accuracy 0.833984375\n",
      "Iteration 22800 Training loss 0.01508253812789917 Validation loss 0.016698917374014854 Accuracy 0.82861328125\n",
      "Iteration 22810 Training loss 0.011434931308031082 Validation loss 0.016276348382234573 Accuracy 0.83349609375\n",
      "Iteration 22820 Training loss 0.011120633222162724 Validation loss 0.016166439279913902 Accuracy 0.8349609375\n",
      "Iteration 22830 Training loss 0.011969774961471558 Validation loss 0.016249708831310272 Accuracy 0.83251953125\n",
      "Iteration 22840 Training loss 0.013992869295179844 Validation loss 0.016373489052057266 Accuracy 0.83251953125\n",
      "Iteration 22850 Training loss 0.012859055772423744 Validation loss 0.016245998442173004 Accuracy 0.833984375\n",
      "Iteration 22860 Training loss 0.013058746233582497 Validation loss 0.01585407927632332 Accuracy 0.83837890625\n",
      "Iteration 22870 Training loss 0.014866551384329796 Validation loss 0.01596585474908352 Accuracy 0.83544921875\n",
      "Iteration 22880 Training loss 0.01420402992516756 Validation loss 0.015802403911948204 Accuracy 0.837890625\n",
      "Iteration 22890 Training loss 0.012153803370893002 Validation loss 0.016078567132353783 Accuracy 0.83447265625\n",
      "Iteration 22900 Training loss 0.013302240520715714 Validation loss 0.016397465020418167 Accuracy 0.83154296875\n",
      "Iteration 22910 Training loss 0.013733229599893093 Validation loss 0.016135018318891525 Accuracy 0.83447265625\n",
      "Iteration 22920 Training loss 0.012237202376127243 Validation loss 0.015980854630470276 Accuracy 0.8369140625\n",
      "Iteration 22930 Training loss 0.013742552138864994 Validation loss 0.016075758263468742 Accuracy 0.8349609375\n",
      "Iteration 22940 Training loss 0.01490339171141386 Validation loss 0.016519218683242798 Accuracy 0.83056640625\n",
      "Iteration 22950 Training loss 0.014906560070812702 Validation loss 0.016403602436184883 Accuracy 0.83203125\n",
      "Iteration 22960 Training loss 0.011704765260219574 Validation loss 0.015941375866532326 Accuracy 0.83642578125\n",
      "Iteration 22970 Training loss 0.014122522436082363 Validation loss 0.01620371825993061 Accuracy 0.8330078125\n",
      "Iteration 22980 Training loss 0.013653676025569439 Validation loss 0.016803553327918053 Accuracy 0.82666015625\n",
      "Iteration 22990 Training loss 0.015060563571751118 Validation loss 0.015952423214912415 Accuracy 0.83642578125\n",
      "Iteration 23000 Training loss 0.013626886531710625 Validation loss 0.015909740701317787 Accuracy 0.83642578125\n",
      "Iteration 23010 Training loss 0.013902696780860424 Validation loss 0.016466598957777023 Accuracy 0.830078125\n",
      "Iteration 23020 Training loss 0.013048700988292694 Validation loss 0.01607913337647915 Accuracy 0.83349609375\n",
      "Iteration 23030 Training loss 0.013741754926741123 Validation loss 0.016139203682541847 Accuracy 0.83349609375\n",
      "Iteration 23040 Training loss 0.012144923210144043 Validation loss 0.01579664833843708 Accuracy 0.8369140625\n",
      "Iteration 23050 Training loss 0.015421057119965553 Validation loss 0.016115503385663033 Accuracy 0.83349609375\n",
      "Iteration 23060 Training loss 0.011209892109036446 Validation loss 0.015929488465189934 Accuracy 0.83544921875\n",
      "Iteration 23070 Training loss 0.009464097209274769 Validation loss 0.015722401440143585 Accuracy 0.83837890625\n",
      "Iteration 23080 Training loss 0.011367753148078918 Validation loss 0.015950776636600494 Accuracy 0.8349609375\n",
      "Iteration 23090 Training loss 0.01376505009829998 Validation loss 0.015638915821909904 Accuracy 0.83837890625\n",
      "Iteration 23100 Training loss 0.013568613678216934 Validation loss 0.014743068255484104 Accuracy 0.84765625\n",
      "Iteration 23110 Training loss 0.01371515542268753 Validation loss 0.014627750031650066 Accuracy 0.84912109375\n",
      "Iteration 23120 Training loss 0.013592409901320934 Validation loss 0.015542534179985523 Accuracy 0.83984375\n",
      "Iteration 23130 Training loss 0.012440122663974762 Validation loss 0.015464159660041332 Accuracy 0.84033203125\n",
      "Iteration 23140 Training loss 0.01254131831228733 Validation loss 0.015455707907676697 Accuracy 0.841796875\n",
      "Iteration 23150 Training loss 0.012831566855311394 Validation loss 0.0161651112139225 Accuracy 0.83447265625\n",
      "Iteration 23160 Training loss 0.013212655670940876 Validation loss 0.014034504070878029 Accuracy 0.85498046875\n",
      "Iteration 23170 Training loss 0.012142191641032696 Validation loss 0.014634733088314533 Accuracy 0.84912109375\n",
      "Iteration 23180 Training loss 0.011226961389183998 Validation loss 0.013849792070686817 Accuracy 0.857421875\n",
      "Iteration 23190 Training loss 0.012757308781147003 Validation loss 0.014360218308866024 Accuracy 0.8515625\n",
      "Iteration 23200 Training loss 0.012076541781425476 Validation loss 0.014124895446002483 Accuracy 0.8544921875\n",
      "Iteration 23210 Training loss 0.01113818772137165 Validation loss 0.013911298476159573 Accuracy 0.85595703125\n",
      "Iteration 23220 Training loss 0.011941235512495041 Validation loss 0.01462521217763424 Accuracy 0.8486328125\n",
      "Iteration 23230 Training loss 0.01070355623960495 Validation loss 0.014703162014484406 Accuracy 0.8486328125\n",
      "Iteration 23240 Training loss 0.010853569023311138 Validation loss 0.014764754101634026 Accuracy 0.84814453125\n",
      "Iteration 23250 Training loss 0.012456817552447319 Validation loss 0.014173612929880619 Accuracy 0.85400390625\n",
      "Iteration 23260 Training loss 0.01561898272484541 Validation loss 0.016313942149281502 Accuracy 0.83251953125\n",
      "Iteration 23270 Training loss 0.012436523102223873 Validation loss 0.013698073104023933 Accuracy 0.85888671875\n",
      "Iteration 23280 Training loss 0.011769330129027367 Validation loss 0.01329611986875534 Accuracy 0.86279296875\n",
      "Iteration 23290 Training loss 0.012061933055520058 Validation loss 0.013609178364276886 Accuracy 0.859375\n",
      "Iteration 23300 Training loss 0.012477865442633629 Validation loss 0.01467057503759861 Accuracy 0.84765625\n",
      "Iteration 23310 Training loss 0.01180684007704258 Validation loss 0.014666258357465267 Accuracy 0.85009765625\n",
      "Iteration 23320 Training loss 0.010059087537229061 Validation loss 0.013618607074022293 Accuracy 0.859375\n",
      "Iteration 23330 Training loss 0.011427278630435467 Validation loss 0.013520302250981331 Accuracy 0.8603515625\n",
      "Iteration 23340 Training loss 0.009927517734467983 Validation loss 0.013327782042324543 Accuracy 0.86279296875\n",
      "Iteration 23350 Training loss 0.011277422308921814 Validation loss 0.014252275228500366 Accuracy 0.85400390625\n",
      "Iteration 23360 Training loss 0.015211616642773151 Validation loss 0.014783419668674469 Accuracy 0.84765625\n",
      "Iteration 23370 Training loss 0.012032954953610897 Validation loss 0.013317961245775223 Accuracy 0.8623046875\n",
      "Iteration 23380 Training loss 0.011152553372085094 Validation loss 0.013249211013317108 Accuracy 0.86328125\n",
      "Iteration 23390 Training loss 0.010283018462359905 Validation loss 0.013997013680636883 Accuracy 0.85546875\n",
      "Iteration 23400 Training loss 0.010192019864916801 Validation loss 0.013137126341462135 Accuracy 0.86376953125\n",
      "Iteration 23410 Training loss 0.009743383154273033 Validation loss 0.01340730395168066 Accuracy 0.861328125\n",
      "Iteration 23420 Training loss 0.011238502338528633 Validation loss 0.012970950454473495 Accuracy 0.8662109375\n",
      "Iteration 23430 Training loss 0.013215504586696625 Validation loss 0.015571494586765766 Accuracy 0.8388671875\n",
      "Iteration 23440 Training loss 0.010572659783065319 Validation loss 0.013297337107360363 Accuracy 0.86181640625\n",
      "Iteration 23450 Training loss 0.011847224086523056 Validation loss 0.01429011207073927 Accuracy 0.8525390625\n",
      "Iteration 23460 Training loss 0.011027839966118336 Validation loss 0.01358101237565279 Accuracy 0.859375\n",
      "Iteration 23470 Training loss 0.010463690385222435 Validation loss 0.01345477718859911 Accuracy 0.861328125\n",
      "Iteration 23480 Training loss 0.012903694063425064 Validation loss 0.013623957522213459 Accuracy 0.8583984375\n",
      "Iteration 23490 Training loss 0.011332101188600063 Validation loss 0.013267847709357738 Accuracy 0.86279296875\n",
      "Iteration 23500 Training loss 0.009170658886432648 Validation loss 0.013968048617243767 Accuracy 0.85595703125\n",
      "Iteration 23510 Training loss 0.013905448839068413 Validation loss 0.015195365995168686 Accuracy 0.84375\n",
      "Iteration 23520 Training loss 0.01245070993900299 Validation loss 0.013518798165023327 Accuracy 0.86083984375\n",
      "Iteration 23530 Training loss 0.01136334240436554 Validation loss 0.014439171180129051 Accuracy 0.85107421875\n",
      "Iteration 23540 Training loss 0.009452080354094505 Validation loss 0.013559569604694843 Accuracy 0.85986328125\n",
      "Iteration 23550 Training loss 0.009728163480758667 Validation loss 0.012986264191567898 Accuracy 0.865234375\n",
      "Iteration 23560 Training loss 0.010825686156749725 Validation loss 0.014268576167523861 Accuracy 0.85205078125\n",
      "Iteration 23570 Training loss 0.009030663408339024 Validation loss 0.013462645933032036 Accuracy 0.8603515625\n",
      "Iteration 23580 Training loss 0.009458422660827637 Validation loss 0.013665664941072464 Accuracy 0.85888671875\n",
      "Iteration 23590 Training loss 0.009667662903666496 Validation loss 0.013231627643108368 Accuracy 0.86279296875\n",
      "Iteration 23600 Training loss 0.009063498117029667 Validation loss 0.013380690477788448 Accuracy 0.8623046875\n",
      "Iteration 23610 Training loss 0.014608651399612427 Validation loss 0.01416165754199028 Accuracy 0.85302734375\n",
      "Iteration 23620 Training loss 0.011976906098425388 Validation loss 0.013921828009188175 Accuracy 0.8564453125\n",
      "Iteration 23630 Training loss 0.010341357439756393 Validation loss 0.013109501451253891 Accuracy 0.8642578125\n",
      "Iteration 23640 Training loss 0.010310384444892406 Validation loss 0.013000656850636005 Accuracy 0.86572265625\n",
      "Iteration 23650 Training loss 0.011230326257646084 Validation loss 0.01389516331255436 Accuracy 0.857421875\n",
      "Iteration 23660 Training loss 0.009602840058505535 Validation loss 0.013201034627854824 Accuracy 0.86328125\n",
      "Iteration 23670 Training loss 0.01010773703455925 Validation loss 0.013679214753210545 Accuracy 0.8583984375\n",
      "Iteration 23680 Training loss 0.010853145271539688 Validation loss 0.015146921388804913 Accuracy 0.8427734375\n",
      "Iteration 23690 Training loss 0.012700889259576797 Validation loss 0.015118955634534359 Accuracy 0.84375\n",
      "Iteration 23700 Training loss 0.012489656917750835 Validation loss 0.013411671854555607 Accuracy 0.86083984375\n",
      "Iteration 23710 Training loss 0.011139040812849998 Validation loss 0.01323097012937069 Accuracy 0.86376953125\n",
      "Iteration 23720 Training loss 0.008369422517716885 Validation loss 0.012981481850147247 Accuracy 0.865234375\n",
      "Iteration 23730 Training loss 0.011477690190076828 Validation loss 0.013059564866125584 Accuracy 0.865234375\n",
      "Iteration 23740 Training loss 0.010649372823536396 Validation loss 0.013610228896141052 Accuracy 0.85888671875\n",
      "Iteration 23750 Training loss 0.011072003282606602 Validation loss 0.014023268595337868 Accuracy 0.85400390625\n",
      "Iteration 23760 Training loss 0.010298527777194977 Validation loss 0.013048315420746803 Accuracy 0.865234375\n",
      "Iteration 23770 Training loss 0.01360668707638979 Validation loss 0.014402243308722973 Accuracy 0.8505859375\n",
      "Iteration 23780 Training loss 0.009849462658166885 Validation loss 0.013463600538671017 Accuracy 0.86083984375\n",
      "Iteration 23790 Training loss 0.015496811829507351 Validation loss 0.013402976095676422 Accuracy 0.861328125\n",
      "Iteration 23800 Training loss 0.01210859790444374 Validation loss 0.01294306106865406 Accuracy 0.8662109375\n",
      "Iteration 23810 Training loss 0.007915602996945381 Validation loss 0.012868821620941162 Accuracy 0.86669921875\n",
      "Iteration 23820 Training loss 0.011447565630078316 Validation loss 0.013269271701574326 Accuracy 0.8623046875\n",
      "Iteration 23830 Training loss 0.009874843060970306 Validation loss 0.013320156373083591 Accuracy 0.86181640625\n",
      "Iteration 23840 Training loss 0.010630917735397816 Validation loss 0.013040314428508282 Accuracy 0.865234375\n",
      "Iteration 23850 Training loss 0.013270057737827301 Validation loss 0.014059843495488167 Accuracy 0.85546875\n",
      "Iteration 23860 Training loss 0.010630532167851925 Validation loss 0.013325021602213383 Accuracy 0.8623046875\n",
      "Iteration 23870 Training loss 0.011753618717193604 Validation loss 0.013045331463217735 Accuracy 0.86572265625\n",
      "Iteration 23880 Training loss 0.012993349693715572 Validation loss 0.014338038861751556 Accuracy 0.85107421875\n",
      "Iteration 23890 Training loss 0.009871082380414009 Validation loss 0.0130376685410738 Accuracy 0.86474609375\n",
      "Iteration 23900 Training loss 0.01128369476646185 Validation loss 0.012657602317631245 Accuracy 0.86865234375\n",
      "Iteration 23910 Training loss 0.009611338376998901 Validation loss 0.013664234429597855 Accuracy 0.859375\n",
      "Iteration 23920 Training loss 0.010698395781219006 Validation loss 0.01283684279769659 Accuracy 0.86767578125\n",
      "Iteration 23930 Training loss 0.01103672944009304 Validation loss 0.012799330987036228 Accuracy 0.86767578125\n",
      "Iteration 23940 Training loss 0.01023164577782154 Validation loss 0.013772846199572086 Accuracy 0.85791015625\n",
      "Iteration 23950 Training loss 0.010326734744012356 Validation loss 0.012660357169806957 Accuracy 0.86865234375\n",
      "Iteration 23960 Training loss 0.011739923618733883 Validation loss 0.012748842127621174 Accuracy 0.86767578125\n",
      "Iteration 23970 Training loss 0.010036743246018887 Validation loss 0.013085991144180298 Accuracy 0.8642578125\n",
      "Iteration 23980 Training loss 0.00903896801173687 Validation loss 0.013323619961738586 Accuracy 0.86181640625\n",
      "Iteration 23990 Training loss 0.012288851663470268 Validation loss 0.012959858402609825 Accuracy 0.8642578125\n",
      "Iteration 24000 Training loss 0.009725208394229412 Validation loss 0.01297261193394661 Accuracy 0.865234375\n",
      "Iteration 24010 Training loss 0.011375430040061474 Validation loss 0.013479550369083881 Accuracy 0.86083984375\n",
      "Iteration 24020 Training loss 0.009605838917195797 Validation loss 0.012708823196589947 Accuracy 0.86767578125\n",
      "Iteration 24030 Training loss 0.011825536377727985 Validation loss 0.0128291891887784 Accuracy 0.8671875\n",
      "Iteration 24040 Training loss 0.00985872931778431 Validation loss 0.013570817187428474 Accuracy 0.859375\n",
      "Iteration 24050 Training loss 0.009671668522059917 Validation loss 0.013148792088031769 Accuracy 0.86376953125\n",
      "Iteration 24060 Training loss 0.01146899163722992 Validation loss 0.013054700568318367 Accuracy 0.8642578125\n",
      "Iteration 24070 Training loss 0.010296856053173542 Validation loss 0.012609107419848442 Accuracy 0.8701171875\n",
      "Iteration 24080 Training loss 0.00907579343765974 Validation loss 0.014408203773200512 Accuracy 0.85205078125\n",
      "Iteration 24090 Training loss 0.00927451066672802 Validation loss 0.012585393153131008 Accuracy 0.8701171875\n",
      "Iteration 24100 Training loss 0.011402188800275326 Validation loss 0.012919662520289421 Accuracy 0.86572265625\n",
      "Iteration 24110 Training loss 0.010408985428512096 Validation loss 0.012789464555680752 Accuracy 0.86767578125\n",
      "Iteration 24120 Training loss 0.011121068149805069 Validation loss 0.013976652175188065 Accuracy 0.85546875\n",
      "Iteration 24130 Training loss 0.01117856614291668 Validation loss 0.013045558705925941 Accuracy 0.86474609375\n",
      "Iteration 24140 Training loss 0.01049952581524849 Validation loss 0.012890487909317017 Accuracy 0.8662109375\n",
      "Iteration 24150 Training loss 0.0069350674748420715 Validation loss 0.012710153125226498 Accuracy 0.8681640625\n",
      "Iteration 24160 Training loss 0.011377105489373207 Validation loss 0.012307841330766678 Accuracy 0.8720703125\n",
      "Iteration 24170 Training loss 0.009763595648109913 Validation loss 0.012517592869699001 Accuracy 0.869140625\n",
      "Iteration 24180 Training loss 0.009036307223141193 Validation loss 0.013069314882159233 Accuracy 0.865234375\n",
      "Iteration 24190 Training loss 0.009342187084257603 Validation loss 0.01255812868475914 Accuracy 0.869140625\n",
      "Iteration 24200 Training loss 0.010036859661340714 Validation loss 0.013306702487170696 Accuracy 0.86181640625\n",
      "Iteration 24210 Training loss 0.010281546041369438 Validation loss 0.012818141840398312 Accuracy 0.86767578125\n",
      "Iteration 24220 Training loss 0.009410405531525612 Validation loss 0.012861141934990883 Accuracy 0.86669921875\n",
      "Iteration 24230 Training loss 0.013534226454794407 Validation loss 0.015292243100702763 Accuracy 0.8427734375\n",
      "Iteration 24240 Training loss 0.01159681472927332 Validation loss 0.013208068907260895 Accuracy 0.86328125\n",
      "Iteration 24250 Training loss 0.008636667393147945 Validation loss 0.013014252297580242 Accuracy 0.86474609375\n",
      "Iteration 24260 Training loss 0.010804099962115288 Validation loss 0.013489462435245514 Accuracy 0.859375\n",
      "Iteration 24270 Training loss 0.009553353302180767 Validation loss 0.013210002332925797 Accuracy 0.86279296875\n",
      "Iteration 24280 Training loss 0.009379944764077663 Validation loss 0.012976584956049919 Accuracy 0.86572265625\n",
      "Iteration 24290 Training loss 0.00877755880355835 Validation loss 0.012855158187448978 Accuracy 0.8671875\n",
      "Iteration 24300 Training loss 0.009099909104406834 Validation loss 0.013292956165969372 Accuracy 0.86279296875\n",
      "Iteration 24310 Training loss 0.008647948503494263 Validation loss 0.012578844092786312 Accuracy 0.87109375\n",
      "Iteration 24320 Training loss 0.00864963699132204 Validation loss 0.013182603754103184 Accuracy 0.86376953125\n",
      "Iteration 24330 Training loss 0.011884443461894989 Validation loss 0.014130082912743092 Accuracy 0.85302734375\n",
      "Iteration 24340 Training loss 0.008990132249891758 Validation loss 0.013293341733515263 Accuracy 0.86181640625\n",
      "Iteration 24350 Training loss 0.010372898541390896 Validation loss 0.013301528058946133 Accuracy 0.86181640625\n",
      "Iteration 24360 Training loss 0.009146075695753098 Validation loss 0.012864467687904835 Accuracy 0.86669921875\n",
      "Iteration 24370 Training loss 0.01217688713222742 Validation loss 0.012920448556542397 Accuracy 0.8662109375\n",
      "Iteration 24380 Training loss 0.009954337030649185 Validation loss 0.012550202198326588 Accuracy 0.86962890625\n",
      "Iteration 24390 Training loss 0.009480425156652927 Validation loss 0.012846464291214943 Accuracy 0.8671875\n",
      "Iteration 24400 Training loss 0.01118105836212635 Validation loss 0.012750864028930664 Accuracy 0.8681640625\n",
      "Iteration 24410 Training loss 0.008014592342078686 Validation loss 0.012729096226394176 Accuracy 0.8681640625\n",
      "Iteration 24420 Training loss 0.012027997523546219 Validation loss 0.012837152928113937 Accuracy 0.86669921875\n",
      "Iteration 24430 Training loss 0.009922799654304981 Validation loss 0.013726550154387951 Accuracy 0.8583984375\n",
      "Iteration 24440 Training loss 0.00924631953239441 Validation loss 0.012553097680211067 Accuracy 0.86962890625\n",
      "Iteration 24450 Training loss 0.010190477594733238 Validation loss 0.012853151187300682 Accuracy 0.86767578125\n",
      "Iteration 24460 Training loss 0.00873676035553217 Validation loss 0.012923471629619598 Accuracy 0.86669921875\n",
      "Iteration 24470 Training loss 0.011008665896952152 Validation loss 0.012503654696047306 Accuracy 0.8701171875\n",
      "Iteration 24480 Training loss 0.012922110967338085 Validation loss 0.015132572501897812 Accuracy 0.84375\n",
      "Iteration 24490 Training loss 0.008046429604291916 Validation loss 0.012553922832012177 Accuracy 0.87060546875\n",
      "Iteration 24500 Training loss 0.0123340068385005 Validation loss 0.013256439007818699 Accuracy 0.86328125\n",
      "Iteration 24510 Training loss 0.009040584787726402 Validation loss 0.01258136797696352 Accuracy 0.869140625\n",
      "Iteration 24520 Training loss 0.008780169300734997 Validation loss 0.01258886232972145 Accuracy 0.8701171875\n",
      "Iteration 24530 Training loss 0.011431523598730564 Validation loss 0.01270222757011652 Accuracy 0.8681640625\n",
      "Iteration 24540 Training loss 0.012346044182777405 Validation loss 0.014462122693657875 Accuracy 0.8505859375\n",
      "Iteration 24550 Training loss 0.01103817019611597 Validation loss 0.01295298058539629 Accuracy 0.865234375\n",
      "Iteration 24560 Training loss 0.01115944143384695 Validation loss 0.013196849264204502 Accuracy 0.86279296875\n",
      "Iteration 24570 Training loss 0.012026608921587467 Validation loss 0.013930226676166058 Accuracy 0.85595703125\n",
      "Iteration 24580 Training loss 0.010185048915445805 Validation loss 0.01239636167883873 Accuracy 0.87109375\n",
      "Iteration 24590 Training loss 0.009615752846002579 Validation loss 0.012927192263305187 Accuracy 0.86669921875\n",
      "Iteration 24600 Training loss 0.010441808961331844 Validation loss 0.012634980492293835 Accuracy 0.86962890625\n",
      "Iteration 24610 Training loss 0.008904511108994484 Validation loss 0.012801442295312881 Accuracy 0.8671875\n",
      "Iteration 24620 Training loss 0.010962856002151966 Validation loss 0.015233655460178852 Accuracy 0.8427734375\n",
      "Iteration 24630 Training loss 0.0073231291025877 Validation loss 0.01313649583607912 Accuracy 0.8642578125\n",
      "Iteration 24640 Training loss 0.01226798351854086 Validation loss 0.013647519052028656 Accuracy 0.859375\n",
      "Iteration 24650 Training loss 0.010309718549251556 Validation loss 0.012821801006793976 Accuracy 0.8662109375\n",
      "Iteration 24660 Training loss 0.008433363400399685 Validation loss 0.012509221211075783 Accuracy 0.87060546875\n",
      "Iteration 24670 Training loss 0.009122882969677448 Validation loss 0.012646845541894436 Accuracy 0.869140625\n",
      "Iteration 24680 Training loss 0.0091460095718503 Validation loss 0.012397939339280128 Accuracy 0.87109375\n",
      "Iteration 24690 Training loss 0.008615068159997463 Validation loss 0.01231483742594719 Accuracy 0.87109375\n",
      "Iteration 24700 Training loss 0.010320796631276608 Validation loss 0.013106542639434338 Accuracy 0.86376953125\n",
      "Iteration 24710 Training loss 0.010253384709358215 Validation loss 0.012957669794559479 Accuracy 0.8662109375\n",
      "Iteration 24720 Training loss 0.008713342249393463 Validation loss 0.012563875876367092 Accuracy 0.869140625\n",
      "Iteration 24730 Training loss 0.009852969087660313 Validation loss 0.012478056363761425 Accuracy 0.8701171875\n",
      "Iteration 24740 Training loss 0.008719495497643948 Validation loss 0.012405432760715485 Accuracy 0.87109375\n",
      "Iteration 24750 Training loss 0.011676855385303497 Validation loss 0.013092384673655033 Accuracy 0.86376953125\n",
      "Iteration 24760 Training loss 0.00889588799327612 Validation loss 0.01299351267516613 Accuracy 0.865234375\n",
      "Iteration 24770 Training loss 0.0073205456137657166 Validation loss 0.012329822406172752 Accuracy 0.87255859375\n",
      "Iteration 24780 Training loss 0.007119676563888788 Validation loss 0.012408982031047344 Accuracy 0.87060546875\n",
      "Iteration 24790 Training loss 0.010414826683700085 Validation loss 0.01247447356581688 Accuracy 0.87060546875\n",
      "Iteration 24800 Training loss 0.008297430351376534 Validation loss 0.012660812586545944 Accuracy 0.8681640625\n",
      "Iteration 24810 Training loss 0.009689128957688808 Validation loss 0.012543909251689911 Accuracy 0.86962890625\n",
      "Iteration 24820 Training loss 0.009123033843934536 Validation loss 0.013067282736301422 Accuracy 0.8642578125\n",
      "Iteration 24830 Training loss 0.010164512321352959 Validation loss 0.012399787083268166 Accuracy 0.87060546875\n",
      "Iteration 24840 Training loss 0.010006094351410866 Validation loss 0.013178215362131596 Accuracy 0.86328125\n",
      "Iteration 24850 Training loss 0.010260680690407753 Validation loss 0.0130706075578928 Accuracy 0.86376953125\n",
      "Iteration 24860 Training loss 0.00820384081453085 Validation loss 0.012714572250843048 Accuracy 0.86767578125\n",
      "Iteration 24870 Training loss 0.00921868346631527 Validation loss 0.012494399212300777 Accuracy 0.87060546875\n",
      "Iteration 24880 Training loss 0.008945188485085964 Validation loss 0.012630216777324677 Accuracy 0.86865234375\n",
      "Iteration 24890 Training loss 0.009683049283921719 Validation loss 0.012350210919976234 Accuracy 0.87158203125\n",
      "Iteration 24900 Training loss 0.009653199464082718 Validation loss 0.013133101165294647 Accuracy 0.86328125\n",
      "Iteration 24910 Training loss 0.010355754755437374 Validation loss 0.012420034036040306 Accuracy 0.87109375\n",
      "Iteration 24920 Training loss 0.010715843178331852 Validation loss 0.013924937695264816 Accuracy 0.85546875\n",
      "Iteration 24930 Training loss 0.006971632596105337 Validation loss 0.012312962673604488 Accuracy 0.87255859375\n",
      "Iteration 24940 Training loss 0.011616721749305725 Validation loss 0.01326911710202694 Accuracy 0.86279296875\n",
      "Iteration 24950 Training loss 0.008024436421692371 Validation loss 0.01260965783149004 Accuracy 0.8681640625\n",
      "Iteration 24960 Training loss 0.009789782576262951 Validation loss 0.012456257827579975 Accuracy 0.87060546875\n",
      "Iteration 24970 Training loss 0.010813090950250626 Validation loss 0.013159442692995071 Accuracy 0.86376953125\n",
      "Iteration 24980 Training loss 0.00791296549141407 Validation loss 0.012475531548261642 Accuracy 0.87158203125\n",
      "Iteration 24990 Training loss 0.011567442677915096 Validation loss 0.013475692830979824 Accuracy 0.86083984375\n",
      "Iteration 25000 Training loss 0.00933945644646883 Validation loss 0.012981313280761242 Accuracy 0.865234375\n",
      "Iteration 25010 Training loss 0.008946310728788376 Validation loss 0.012502030469477177 Accuracy 0.8701171875\n",
      "Iteration 25020 Training loss 0.011623782105743885 Validation loss 0.01284621749073267 Accuracy 0.8671875\n",
      "Iteration 25030 Training loss 0.009176887571811676 Validation loss 0.012490046210587025 Accuracy 0.8701171875\n",
      "Iteration 25040 Training loss 0.008540963754057884 Validation loss 0.012516831047832966 Accuracy 0.8701171875\n",
      "Iteration 25050 Training loss 0.009492375887930393 Validation loss 0.012919980101287365 Accuracy 0.8662109375\n",
      "Iteration 25060 Training loss 0.009116986766457558 Validation loss 0.012878104113042355 Accuracy 0.8671875\n",
      "Iteration 25070 Training loss 0.008877181448042393 Validation loss 0.012826120480895042 Accuracy 0.8671875\n",
      "Iteration 25080 Training loss 0.007687081582844257 Validation loss 0.012272201478481293 Accuracy 0.8720703125\n",
      "Iteration 25090 Training loss 0.006278998218476772 Validation loss 0.012445350177586079 Accuracy 0.87060546875\n",
      "Iteration 25100 Training loss 0.007787189446389675 Validation loss 0.012427601963281631 Accuracy 0.87109375\n",
      "Iteration 25110 Training loss 0.009170826524496078 Validation loss 0.012765580788254738 Accuracy 0.86767578125\n",
      "Iteration 25120 Training loss 0.008916678838431835 Validation loss 0.013541948050260544 Accuracy 0.85986328125\n",
      "Iteration 25130 Training loss 0.010320139117538929 Validation loss 0.01284767035394907 Accuracy 0.8671875\n",
      "Iteration 25140 Training loss 0.00807857420295477 Validation loss 0.012490207329392433 Accuracy 0.87060546875\n",
      "Iteration 25150 Training loss 0.01134831365197897 Validation loss 0.013050639070570469 Accuracy 0.865234375\n",
      "Iteration 25160 Training loss 0.009284903295338154 Validation loss 0.012905175797641277 Accuracy 0.86572265625\n",
      "Iteration 25170 Training loss 0.010188758373260498 Validation loss 0.012653248384594917 Accuracy 0.86962890625\n",
      "Iteration 25180 Training loss 0.007666624151170254 Validation loss 0.012239353731274605 Accuracy 0.87255859375\n",
      "Iteration 25190 Training loss 0.00917192455381155 Validation loss 0.012663060799241066 Accuracy 0.86865234375\n",
      "Iteration 25200 Training loss 0.008922348730266094 Validation loss 0.012415998615324497 Accuracy 0.8701171875\n",
      "Iteration 25210 Training loss 0.009098764508962631 Validation loss 0.01230014581233263 Accuracy 0.873046875\n",
      "Iteration 25220 Training loss 0.007822580635547638 Validation loss 0.012503127567470074 Accuracy 0.86962890625\n",
      "Iteration 25230 Training loss 0.008689168840646744 Validation loss 0.012692590244114399 Accuracy 0.8671875\n",
      "Iteration 25240 Training loss 0.0074954708106815815 Validation loss 0.01233533676713705 Accuracy 0.8720703125\n",
      "Iteration 25250 Training loss 0.006790568586438894 Validation loss 0.012234840542078018 Accuracy 0.873046875\n",
      "Iteration 25260 Training loss 0.007565063890069723 Validation loss 0.012477604672312737 Accuracy 0.87109375\n",
      "Iteration 25270 Training loss 0.01076409500092268 Validation loss 0.013001682236790657 Accuracy 0.86474609375\n",
      "Iteration 25280 Training loss 0.008486604318022728 Validation loss 0.012226931750774384 Accuracy 0.87353515625\n",
      "Iteration 25290 Training loss 0.010712356306612492 Validation loss 0.012730436399579048 Accuracy 0.8681640625\n",
      "Iteration 25300 Training loss 0.009880621917545795 Validation loss 0.013040823861956596 Accuracy 0.86474609375\n",
      "Iteration 25310 Training loss 0.009571882896125317 Validation loss 0.012293885461986065 Accuracy 0.873046875\n",
      "Iteration 25320 Training loss 0.009266857989132404 Validation loss 0.012148771435022354 Accuracy 0.873046875\n",
      "Iteration 25330 Training loss 0.008564923889935017 Validation loss 0.01248348131775856 Accuracy 0.8701171875\n",
      "Iteration 25340 Training loss 0.007528180256485939 Validation loss 0.012428869493305683 Accuracy 0.8701171875\n",
      "Iteration 25350 Training loss 0.008747292682528496 Validation loss 0.012138549238443375 Accuracy 0.87255859375\n",
      "Iteration 25360 Training loss 0.007540600840002298 Validation loss 0.011839821934700012 Accuracy 0.876953125\n",
      "Iteration 25370 Training loss 0.009464665316045284 Validation loss 0.0124053368344903 Accuracy 0.87109375\n",
      "Iteration 25380 Training loss 0.008560174144804478 Validation loss 0.012582208961248398 Accuracy 0.8701171875\n",
      "Iteration 25390 Training loss 0.010608633980154991 Validation loss 0.012358106672763824 Accuracy 0.87060546875\n",
      "Iteration 25400 Training loss 0.008743108250200748 Validation loss 0.012376813217997551 Accuracy 0.8720703125\n",
      "Iteration 25410 Training loss 0.010020878165960312 Validation loss 0.012116910889744759 Accuracy 0.8740234375\n",
      "Iteration 25420 Training loss 0.009105898439884186 Validation loss 0.012982514686882496 Accuracy 0.865234375\n",
      "Iteration 25430 Training loss 0.008462109602987766 Validation loss 0.012583276256918907 Accuracy 0.86865234375\n",
      "Iteration 25440 Training loss 0.01172474306076765 Validation loss 0.012389316223561764 Accuracy 0.87158203125\n",
      "Iteration 25450 Training loss 0.008464503102004528 Validation loss 0.012077327817678452 Accuracy 0.87451171875\n",
      "Iteration 25460 Training loss 0.0077809980139136314 Validation loss 0.013032143004238605 Accuracy 0.86474609375\n",
      "Iteration 25470 Training loss 0.00854076910763979 Validation loss 0.01302241813391447 Accuracy 0.865234375\n",
      "Iteration 25480 Training loss 0.010310538113117218 Validation loss 0.012154987081885338 Accuracy 0.873046875\n",
      "Iteration 25490 Training loss 0.008060121908783913 Validation loss 0.012050644494593143 Accuracy 0.87451171875\n",
      "Iteration 25500 Training loss 0.010999158024787903 Validation loss 0.013296449556946754 Accuracy 0.8623046875\n",
      "Iteration 25510 Training loss 0.008680632337927818 Validation loss 0.012281706556677818 Accuracy 0.8720703125\n",
      "Iteration 25520 Training loss 0.009761839173734188 Validation loss 0.012150192633271217 Accuracy 0.8740234375\n",
      "Iteration 25530 Training loss 0.007453601341694593 Validation loss 0.012025597505271435 Accuracy 0.875\n",
      "Iteration 25540 Training loss 0.008788962848484516 Validation loss 0.013133948668837547 Accuracy 0.8642578125\n",
      "Iteration 25550 Training loss 0.00849800556898117 Validation loss 0.013223603367805481 Accuracy 0.86376953125\n",
      "Iteration 25560 Training loss 0.009650721214711666 Validation loss 0.012217753566801548 Accuracy 0.87353515625\n",
      "Iteration 25570 Training loss 0.011472855694591999 Validation loss 0.014775553718209267 Accuracy 0.84814453125\n",
      "Iteration 25580 Training loss 0.010640154592692852 Validation loss 0.013200522400438786 Accuracy 0.86376953125\n",
      "Iteration 25590 Training loss 0.008779472671449184 Validation loss 0.011836609803140163 Accuracy 0.87744140625\n",
      "Iteration 25600 Training loss 0.00944334827363491 Validation loss 0.013856603763997555 Accuracy 0.8564453125\n",
      "Iteration 25610 Training loss 0.011049644090235233 Validation loss 0.012073636054992676 Accuracy 0.87451171875\n",
      "Iteration 25620 Training loss 0.009081133641302586 Validation loss 0.01251961849629879 Accuracy 0.8701171875\n",
      "Iteration 25630 Training loss 0.009819882921874523 Validation loss 0.01220516674220562 Accuracy 0.87353515625\n",
      "Iteration 25640 Training loss 0.009936762042343616 Validation loss 0.01230716984719038 Accuracy 0.8720703125\n",
      "Iteration 25650 Training loss 0.009460289031267166 Validation loss 0.012257817201316357 Accuracy 0.8720703125\n",
      "Iteration 25660 Training loss 0.007140385918319225 Validation loss 0.012323244474828243 Accuracy 0.8720703125\n",
      "Iteration 25670 Training loss 0.007376308552920818 Validation loss 0.012110392563045025 Accuracy 0.875\n",
      "Iteration 25680 Training loss 0.007843280211091042 Validation loss 0.01254518423229456 Accuracy 0.86962890625\n",
      "Iteration 25690 Training loss 0.008350641466677189 Validation loss 0.012291010469198227 Accuracy 0.87158203125\n",
      "Iteration 25700 Training loss 0.009917676448822021 Validation loss 0.012344521470367908 Accuracy 0.87158203125\n",
      "Iteration 25710 Training loss 0.00909889955073595 Validation loss 0.012570819817483425 Accuracy 0.86962890625\n",
      "Iteration 25720 Training loss 0.009283074177801609 Validation loss 0.012643381021916866 Accuracy 0.86962890625\n",
      "Iteration 25730 Training loss 0.009525863453745842 Validation loss 0.012020817957818508 Accuracy 0.87548828125\n",
      "Iteration 25740 Training loss 0.008064920082688332 Validation loss 0.012187568470835686 Accuracy 0.87353515625\n",
      "Iteration 25750 Training loss 0.00881898496299982 Validation loss 0.012637278996407986 Accuracy 0.86962890625\n",
      "Iteration 25760 Training loss 0.009214011020958424 Validation loss 0.01290721446275711 Accuracy 0.8662109375\n",
      "Iteration 25770 Training loss 0.012294087558984756 Validation loss 0.012843193486332893 Accuracy 0.8662109375\n",
      "Iteration 25780 Training loss 0.006692951079457998 Validation loss 0.012273984029889107 Accuracy 0.8720703125\n",
      "Iteration 25790 Training loss 0.007962840609252453 Validation loss 0.012128157541155815 Accuracy 0.8740234375\n",
      "Iteration 25800 Training loss 0.008700906299054623 Validation loss 0.011935075744986534 Accuracy 0.8759765625\n",
      "Iteration 25810 Training loss 0.008557654917240143 Validation loss 0.012687626294791698 Accuracy 0.86767578125\n",
      "Iteration 25820 Training loss 0.007999248802661896 Validation loss 0.01197147648781538 Accuracy 0.87548828125\n",
      "Iteration 25830 Training loss 0.010984295047819614 Validation loss 0.012975162826478481 Accuracy 0.86572265625\n",
      "Iteration 25840 Training loss 0.00832232367247343 Validation loss 0.012737268581986427 Accuracy 0.86865234375\n",
      "Iteration 25850 Training loss 0.008249293081462383 Validation loss 0.012137954123318195 Accuracy 0.8740234375\n",
      "Iteration 25860 Training loss 0.007531850133091211 Validation loss 0.012201528064906597 Accuracy 0.8720703125\n",
      "Iteration 25870 Training loss 0.008061373606324196 Validation loss 0.012189900502562523 Accuracy 0.87353515625\n",
      "Iteration 25880 Training loss 0.007218876387923956 Validation loss 0.012021506205201149 Accuracy 0.87451171875\n",
      "Iteration 25890 Training loss 0.00854468997567892 Validation loss 0.012574085034430027 Accuracy 0.86865234375\n",
      "Iteration 25900 Training loss 0.00818928424268961 Validation loss 0.012549655511975288 Accuracy 0.869140625\n",
      "Iteration 25910 Training loss 0.007770848460495472 Validation loss 0.012397421523928642 Accuracy 0.87109375\n",
      "Iteration 25920 Training loss 0.008786020800471306 Validation loss 0.012315981090068817 Accuracy 0.8720703125\n",
      "Iteration 25930 Training loss 0.01092186477035284 Validation loss 0.012842677533626556 Accuracy 0.8671875\n",
      "Iteration 25940 Training loss 0.007792870979756117 Validation loss 0.012461422011256218 Accuracy 0.87060546875\n",
      "Iteration 25950 Training loss 0.010347559116780758 Validation loss 0.012405458837747574 Accuracy 0.8720703125\n",
      "Iteration 25960 Training loss 0.009424308314919472 Validation loss 0.012045011855661869 Accuracy 0.875\n",
      "Iteration 25970 Training loss 0.007950647734105587 Validation loss 0.012095875106751919 Accuracy 0.87353515625\n",
      "Iteration 25980 Training loss 0.011733568273484707 Validation loss 0.013832495547831059 Accuracy 0.85693359375\n",
      "Iteration 25990 Training loss 0.008340707048773766 Validation loss 0.012541020289063454 Accuracy 0.86962890625\n",
      "Iteration 26000 Training loss 0.009460299275815487 Validation loss 0.013099542818963528 Accuracy 0.8642578125\n",
      "Iteration 26010 Training loss 0.008508577942848206 Validation loss 0.012412216514348984 Accuracy 0.87109375\n",
      "Iteration 26020 Training loss 0.007483961526304483 Validation loss 0.012517886236310005 Accuracy 0.8701171875\n",
      "Iteration 26030 Training loss 0.009646140038967133 Validation loss 0.012338578701019287 Accuracy 0.87158203125\n",
      "Iteration 26040 Training loss 0.011412994004786015 Validation loss 0.011961725540459156 Accuracy 0.875\n",
      "Iteration 26050 Training loss 0.010638203471899033 Validation loss 0.012063395231962204 Accuracy 0.87451171875\n",
      "Iteration 26060 Training loss 0.007842827588319778 Validation loss 0.012524831108748913 Accuracy 0.87060546875\n",
      "Iteration 26070 Training loss 0.008114490658044815 Validation loss 0.012024013325572014 Accuracy 0.8759765625\n",
      "Iteration 26080 Training loss 0.008756816387176514 Validation loss 0.01218366902321577 Accuracy 0.87353515625\n",
      "Iteration 26090 Training loss 0.007903443649411201 Validation loss 0.012570563703775406 Accuracy 0.869140625\n",
      "Iteration 26100 Training loss 0.007727142423391342 Validation loss 0.01248034369200468 Accuracy 0.87060546875\n",
      "Iteration 26110 Training loss 0.009024490602314472 Validation loss 0.012039579451084137 Accuracy 0.87451171875\n",
      "Iteration 26120 Training loss 0.010141447186470032 Validation loss 0.012437748722732067 Accuracy 0.8701171875\n",
      "Iteration 26130 Training loss 0.009328296408057213 Validation loss 0.012017817236483097 Accuracy 0.8740234375\n",
      "Iteration 26140 Training loss 0.008086826652288437 Validation loss 0.012509195134043694 Accuracy 0.8701171875\n",
      "Iteration 26150 Training loss 0.008483792655169964 Validation loss 0.011883068829774857 Accuracy 0.87646484375\n",
      "Iteration 26160 Training loss 0.010114897042512894 Validation loss 0.01204812340438366 Accuracy 0.8740234375\n",
      "Iteration 26170 Training loss 0.011334599927067757 Validation loss 0.012475455179810524 Accuracy 0.869140625\n",
      "Iteration 26180 Training loss 0.011177347972989082 Validation loss 0.012531662359833717 Accuracy 0.8701171875\n",
      "Iteration 26190 Training loss 0.008973784744739532 Validation loss 0.012238427065312862 Accuracy 0.87353515625\n",
      "Iteration 26200 Training loss 0.0089629041031003 Validation loss 0.01235685870051384 Accuracy 0.87109375\n",
      "Iteration 26210 Training loss 0.00906755868345499 Validation loss 0.012303684838116169 Accuracy 0.87158203125\n",
      "Iteration 26220 Training loss 0.007904045283794403 Validation loss 0.012374640442430973 Accuracy 0.87158203125\n",
      "Iteration 26230 Training loss 0.008819276466965675 Validation loss 0.012130668386816978 Accuracy 0.87353515625\n",
      "Iteration 26240 Training loss 0.006614255253225565 Validation loss 0.012103190645575523 Accuracy 0.87353515625\n",
      "Iteration 26250 Training loss 0.01033754087984562 Validation loss 0.013289799913764 Accuracy 0.8623046875\n",
      "Iteration 26260 Training loss 0.008153503760695457 Validation loss 0.01324790995568037 Accuracy 0.86279296875\n",
      "Iteration 26270 Training loss 0.010119949467480183 Validation loss 0.012854453176259995 Accuracy 0.86669921875\n",
      "Iteration 26280 Training loss 0.008502975106239319 Validation loss 0.012412844225764275 Accuracy 0.87158203125\n",
      "Iteration 26290 Training loss 0.007216931786388159 Validation loss 0.012259218841791153 Accuracy 0.873046875\n",
      "Iteration 26300 Training loss 0.008853294886648655 Validation loss 0.012204907834529877 Accuracy 0.8720703125\n",
      "Iteration 26310 Training loss 0.007061786483973265 Validation loss 0.011888274922966957 Accuracy 0.87646484375\n",
      "Iteration 26320 Training loss 0.007919440977275372 Validation loss 0.01205942127853632 Accuracy 0.87255859375\n",
      "Iteration 26330 Training loss 0.009368110448122025 Validation loss 0.012087388895452023 Accuracy 0.87353515625\n",
      "Iteration 26340 Training loss 0.01208131480962038 Validation loss 0.014111691154539585 Accuracy 0.853515625\n",
      "Iteration 26350 Training loss 0.00892678927630186 Validation loss 0.012349333614110947 Accuracy 0.87158203125\n",
      "Iteration 26360 Training loss 0.00784861110150814 Validation loss 0.012049977667629719 Accuracy 0.87451171875\n",
      "Iteration 26370 Training loss 0.007918322458863258 Validation loss 0.012226415798068047 Accuracy 0.87255859375\n",
      "Iteration 26380 Training loss 0.007387483958154917 Validation loss 0.012044115923345089 Accuracy 0.875\n",
      "Iteration 26390 Training loss 0.007475132122635841 Validation loss 0.012617651373147964 Accuracy 0.8671875\n",
      "Iteration 26400 Training loss 0.009219889529049397 Validation loss 0.012899743393063545 Accuracy 0.865234375\n",
      "Iteration 26410 Training loss 0.008897189982235432 Validation loss 0.012193500995635986 Accuracy 0.87255859375\n",
      "Iteration 26420 Training loss 0.007850301451981068 Validation loss 0.01200269628316164 Accuracy 0.87548828125\n",
      "Iteration 26430 Training loss 0.007672073319554329 Validation loss 0.012135609984397888 Accuracy 0.8740234375\n",
      "Iteration 26440 Training loss 0.009543233551084995 Validation loss 0.01279063057154417 Accuracy 0.86669921875\n",
      "Iteration 26450 Training loss 0.00894933007657528 Validation loss 0.012475302442908287 Accuracy 0.86962890625\n",
      "Iteration 26460 Training loss 0.007973016239702702 Validation loss 0.01222176756709814 Accuracy 0.87255859375\n",
      "Iteration 26470 Training loss 0.009115883149206638 Validation loss 0.013307157903909683 Accuracy 0.861328125\n",
      "Iteration 26480 Training loss 0.008600750006735325 Validation loss 0.012330533936619759 Accuracy 0.87158203125\n",
      "Iteration 26490 Training loss 0.007871188223361969 Validation loss 0.012135138735175133 Accuracy 0.87353515625\n",
      "Iteration 26500 Training loss 0.008962724357843399 Validation loss 0.012034645304083824 Accuracy 0.8740234375\n",
      "Iteration 26510 Training loss 0.00894597452133894 Validation loss 0.012816853821277618 Accuracy 0.8662109375\n",
      "Iteration 26520 Training loss 0.007407839410007 Validation loss 0.01228482648730278 Accuracy 0.87158203125\n",
      "Iteration 26530 Training loss 0.008177005685865879 Validation loss 0.012540294788777828 Accuracy 0.86962890625\n",
      "Iteration 26540 Training loss 0.00919407606124878 Validation loss 0.012573156505823135 Accuracy 0.86962890625\n",
      "Iteration 26550 Training loss 0.007803328335285187 Validation loss 0.013354754075407982 Accuracy 0.861328125\n",
      "Iteration 26560 Training loss 0.008265440352261066 Validation loss 0.012496544979512691 Accuracy 0.87060546875\n",
      "Iteration 26570 Training loss 0.009038395248353481 Validation loss 0.012997298501431942 Accuracy 0.865234375\n",
      "Iteration 26580 Training loss 0.009389901533722878 Validation loss 0.01200748048722744 Accuracy 0.875\n",
      "Iteration 26590 Training loss 0.006477396469563246 Validation loss 0.01228426769375801 Accuracy 0.87255859375\n",
      "Iteration 26600 Training loss 0.006938885897397995 Validation loss 0.011948795057833195 Accuracy 0.87451171875\n",
      "Iteration 26610 Training loss 0.0075026340782642365 Validation loss 0.012012851424515247 Accuracy 0.8740234375\n",
      "Iteration 26620 Training loss 0.010402984917163849 Validation loss 0.012077203951776028 Accuracy 0.8740234375\n",
      "Iteration 26630 Training loss 0.009508810006082058 Validation loss 0.012185223400592804 Accuracy 0.87255859375\n",
      "Iteration 26640 Training loss 0.008396976627409458 Validation loss 0.011925841681659222 Accuracy 0.875\n",
      "Iteration 26650 Training loss 0.007295187097042799 Validation loss 0.011928912252187729 Accuracy 0.8759765625\n",
      "Iteration 26660 Training loss 0.008437604643404484 Validation loss 0.012713498435914516 Accuracy 0.86669921875\n",
      "Iteration 26670 Training loss 0.007635809481143951 Validation loss 0.01199487503618002 Accuracy 0.875\n",
      "Iteration 26680 Training loss 0.008424307219684124 Validation loss 0.012426401488482952 Accuracy 0.869140625\n",
      "Iteration 26690 Training loss 0.007813169620931149 Validation loss 0.011905672028660774 Accuracy 0.87548828125\n",
      "Iteration 26700 Training loss 0.008234404027462006 Validation loss 0.012013849802315235 Accuracy 0.8740234375\n",
      "Iteration 26710 Training loss 0.009912175126373768 Validation loss 0.012315643951296806 Accuracy 0.87109375\n",
      "Iteration 26720 Training loss 0.006516684778034687 Validation loss 0.012136540375649929 Accuracy 0.873046875\n",
      "Iteration 26730 Training loss 0.0075961994007229805 Validation loss 0.012170433066785336 Accuracy 0.87255859375\n",
      "Iteration 26740 Training loss 0.007591636385768652 Validation loss 0.01201444398611784 Accuracy 0.87451171875\n",
      "Iteration 26750 Training loss 0.007198866456747055 Validation loss 0.012060850858688354 Accuracy 0.87353515625\n",
      "Iteration 26760 Training loss 0.007637469097971916 Validation loss 0.011986663565039635 Accuracy 0.87451171875\n",
      "Iteration 26770 Training loss 0.007439969107508659 Validation loss 0.01196939218789339 Accuracy 0.87451171875\n",
      "Iteration 26780 Training loss 0.009353190660476685 Validation loss 0.012897657230496407 Accuracy 0.865234375\n",
      "Iteration 26790 Training loss 0.008568690158426762 Validation loss 0.01197155099362135 Accuracy 0.87451171875\n",
      "Iteration 26800 Training loss 0.007416048087179661 Validation loss 0.012192963622510433 Accuracy 0.873046875\n",
      "Iteration 26810 Training loss 0.005916633643209934 Validation loss 0.01213938556611538 Accuracy 0.873046875\n",
      "Iteration 26820 Training loss 0.009590279310941696 Validation loss 0.011962899006903172 Accuracy 0.87548828125\n",
      "Iteration 26830 Training loss 0.009356936439871788 Validation loss 0.012314909137785435 Accuracy 0.87158203125\n",
      "Iteration 26840 Training loss 0.00890099536627531 Validation loss 0.012945473194122314 Accuracy 0.86474609375\n",
      "Iteration 26850 Training loss 0.008658085018396378 Validation loss 0.012075419537723064 Accuracy 0.8740234375\n",
      "Iteration 26860 Training loss 0.007613746449351311 Validation loss 0.011892033740878105 Accuracy 0.87548828125\n",
      "Iteration 26870 Training loss 0.008415386080741882 Validation loss 0.01241565402597189 Accuracy 0.87060546875\n",
      "Iteration 26880 Training loss 0.008452784270048141 Validation loss 0.012136813253164291 Accuracy 0.87353515625\n",
      "Iteration 26890 Training loss 0.007810452487319708 Validation loss 0.01182530727237463 Accuracy 0.87744140625\n",
      "Iteration 26900 Training loss 0.007007682230323553 Validation loss 0.0122326435521245 Accuracy 0.8720703125\n",
      "Iteration 26910 Training loss 0.007256948389112949 Validation loss 0.01168699562549591 Accuracy 0.87841796875\n",
      "Iteration 26920 Training loss 0.008713183924555779 Validation loss 0.012597499415278435 Accuracy 0.8681640625\n",
      "Iteration 26930 Training loss 0.00565837137401104 Validation loss 0.011899705044925213 Accuracy 0.8759765625\n",
      "Iteration 26940 Training loss 0.0073652672581374645 Validation loss 0.011785171926021576 Accuracy 0.876953125\n",
      "Iteration 26950 Training loss 0.008319663815200329 Validation loss 0.011923586949706078 Accuracy 0.87548828125\n",
      "Iteration 26960 Training loss 0.010148926638066769 Validation loss 0.012615951709449291 Accuracy 0.86865234375\n",
      "Iteration 26970 Training loss 0.007903787307441235 Validation loss 0.011834115721285343 Accuracy 0.8759765625\n",
      "Iteration 26980 Training loss 0.008631562814116478 Validation loss 0.01223727036267519 Accuracy 0.8720703125\n",
      "Iteration 26990 Training loss 0.009553424082696438 Validation loss 0.011814888566732407 Accuracy 0.87646484375\n",
      "Iteration 27000 Training loss 0.007707531098276377 Validation loss 0.012240869924426079 Accuracy 0.87255859375\n",
      "Iteration 27010 Training loss 0.009118585847318172 Validation loss 0.01176102738827467 Accuracy 0.87646484375\n",
      "Iteration 27020 Training loss 0.007103294134140015 Validation loss 0.012282205745577812 Accuracy 0.87158203125\n",
      "Iteration 27030 Training loss 0.007602397818118334 Validation loss 0.011861243285238743 Accuracy 0.8759765625\n",
      "Iteration 27040 Training loss 0.007311520632356405 Validation loss 0.01199385430663824 Accuracy 0.875\n",
      "Iteration 27050 Training loss 0.006115260999649763 Validation loss 0.012056357227265835 Accuracy 0.875\n",
      "Iteration 27060 Training loss 0.00774487666785717 Validation loss 0.012385768815875053 Accuracy 0.8720703125\n",
      "Iteration 27070 Training loss 0.008991874754428864 Validation loss 0.012400819920003414 Accuracy 0.8701171875\n",
      "Iteration 27080 Training loss 0.00860131997615099 Validation loss 0.011805595830082893 Accuracy 0.876953125\n",
      "Iteration 27090 Training loss 0.009542117826640606 Validation loss 0.011629894375801086 Accuracy 0.87841796875\n",
      "Iteration 27100 Training loss 0.008043692447245121 Validation loss 0.012087726965546608 Accuracy 0.8740234375\n",
      "Iteration 27110 Training loss 0.007381956558674574 Validation loss 0.011943001300096512 Accuracy 0.87548828125\n",
      "Iteration 27120 Training loss 0.008013468235731125 Validation loss 0.011811689473688602 Accuracy 0.87646484375\n",
      "Iteration 27130 Training loss 0.007538420148193836 Validation loss 0.01221220288425684 Accuracy 0.87353515625\n",
      "Iteration 27140 Training loss 0.007607264909893274 Validation loss 0.012523450888693333 Accuracy 0.8681640625\n",
      "Iteration 27150 Training loss 0.009801877662539482 Validation loss 0.012848440557718277 Accuracy 0.86572265625\n",
      "Iteration 27160 Training loss 0.007269041612744331 Validation loss 0.012025680392980576 Accuracy 0.8740234375\n",
      "Iteration 27170 Training loss 0.007112066727131605 Validation loss 0.01231662929058075 Accuracy 0.87109375\n",
      "Iteration 27180 Training loss 0.008274994790554047 Validation loss 0.011915956623852253 Accuracy 0.87646484375\n",
      "Iteration 27190 Training loss 0.008380504325032234 Validation loss 0.012039616703987122 Accuracy 0.87451171875\n",
      "Iteration 27200 Training loss 0.008127039298415184 Validation loss 0.012056640349328518 Accuracy 0.8740234375\n",
      "Iteration 27210 Training loss 0.008995329029858112 Validation loss 0.01206850167363882 Accuracy 0.8740234375\n",
      "Iteration 27220 Training loss 0.006995826028287411 Validation loss 0.012152265757322311 Accuracy 0.8720703125\n",
      "Iteration 27230 Training loss 0.008546951226890087 Validation loss 0.012185227125883102 Accuracy 0.87255859375\n",
      "Iteration 27240 Training loss 0.007503219414502382 Validation loss 0.01226200070232153 Accuracy 0.8720703125\n",
      "Iteration 27250 Training loss 0.007731727324426174 Validation loss 0.012036752887070179 Accuracy 0.8740234375\n",
      "Iteration 27260 Training loss 0.009494684636592865 Validation loss 0.012413254007697105 Accuracy 0.87060546875\n",
      "Iteration 27270 Training loss 0.008144903928041458 Validation loss 0.012149914167821407 Accuracy 0.87353515625\n",
      "Iteration 27280 Training loss 0.006981322076171637 Validation loss 0.011798513121902943 Accuracy 0.876953125\n",
      "Iteration 27290 Training loss 0.008536550216376781 Validation loss 0.012131686322391033 Accuracy 0.873046875\n",
      "Iteration 27300 Training loss 0.007467590272426605 Validation loss 0.012221052311360836 Accuracy 0.87353515625\n",
      "Iteration 27310 Training loss 0.007333347108215094 Validation loss 0.011843542568385601 Accuracy 0.8759765625\n",
      "Iteration 27320 Training loss 0.008279072120785713 Validation loss 0.012262940406799316 Accuracy 0.87255859375\n",
      "Iteration 27330 Training loss 0.008747434243559837 Validation loss 0.012535445392131805 Accuracy 0.86962890625\n",
      "Iteration 27340 Training loss 0.008260590955615044 Validation loss 0.011918844655156136 Accuracy 0.87548828125\n",
      "Iteration 27350 Training loss 0.009264085441827774 Validation loss 0.011988637037575245 Accuracy 0.87548828125\n",
      "Iteration 27360 Training loss 0.00736425444483757 Validation loss 0.01189950667321682 Accuracy 0.87646484375\n",
      "Iteration 27370 Training loss 0.009690562263131142 Validation loss 0.012786193750798702 Accuracy 0.8662109375\n",
      "Iteration 27380 Training loss 0.007230290677398443 Validation loss 0.013091581873595715 Accuracy 0.86376953125\n",
      "Iteration 27390 Training loss 0.009777724742889404 Validation loss 0.012226628139615059 Accuracy 0.87255859375\n",
      "Iteration 27400 Training loss 0.007548240479081869 Validation loss 0.012321708723902702 Accuracy 0.87060546875\n",
      "Iteration 27410 Training loss 0.009475035592913628 Validation loss 0.012715904042124748 Accuracy 0.86669921875\n",
      "Iteration 27420 Training loss 0.00902274064719677 Validation loss 0.01187749020755291 Accuracy 0.8759765625\n",
      "Iteration 27430 Training loss 0.007203333545476198 Validation loss 0.012420226819813251 Accuracy 0.87109375\n",
      "Iteration 27440 Training loss 0.010267699137330055 Validation loss 0.011897295713424683 Accuracy 0.87548828125\n",
      "Iteration 27450 Training loss 0.007315874565392733 Validation loss 0.011988542973995209 Accuracy 0.87451171875\n",
      "Iteration 27460 Training loss 0.006175629794597626 Validation loss 0.012081567198038101 Accuracy 0.8740234375\n",
      "Iteration 27470 Training loss 0.00785187166184187 Validation loss 0.012120542116463184 Accuracy 0.873046875\n",
      "Iteration 27480 Training loss 0.00915579590946436 Validation loss 0.012648508884012699 Accuracy 0.869140625\n",
      "Iteration 27490 Training loss 0.008666946552693844 Validation loss 0.012161779217422009 Accuracy 0.87109375\n",
      "Iteration 27500 Training loss 0.0113311056047678 Validation loss 0.011876552365720272 Accuracy 0.87548828125\n",
      "Iteration 27510 Training loss 0.007659148890525103 Validation loss 0.012435988523066044 Accuracy 0.8701171875\n",
      "Iteration 27520 Training loss 0.010737965814769268 Validation loss 0.012463641352951527 Accuracy 0.8701171875\n",
      "Iteration 27530 Training loss 0.011976459063589573 Validation loss 0.012770484201610088 Accuracy 0.86767578125\n",
      "Iteration 27540 Training loss 0.00861852802336216 Validation loss 0.012075378559529781 Accuracy 0.8740234375\n",
      "Iteration 27550 Training loss 0.009706404991447926 Validation loss 0.011901138350367546 Accuracy 0.87548828125\n",
      "Iteration 27560 Training loss 0.008971702307462692 Validation loss 0.01194775104522705 Accuracy 0.8759765625\n",
      "Iteration 27570 Training loss 0.008687514811754227 Validation loss 0.01189706102013588 Accuracy 0.8759765625\n",
      "Iteration 27580 Training loss 0.007520969491451979 Validation loss 0.01178164966404438 Accuracy 0.8779296875\n",
      "Iteration 27590 Training loss 0.008673150092363358 Validation loss 0.012020622380077839 Accuracy 0.87548828125\n",
      "Iteration 27600 Training loss 0.006757583003491163 Validation loss 0.012181371450424194 Accuracy 0.873046875\n",
      "Iteration 27610 Training loss 0.008614212274551392 Validation loss 0.012004292570054531 Accuracy 0.8740234375\n",
      "Iteration 27620 Training loss 0.009652622975409031 Validation loss 0.01340816356241703 Accuracy 0.8603515625\n",
      "Iteration 27630 Training loss 0.008657928556203842 Validation loss 0.012553798966109753 Accuracy 0.8681640625\n",
      "Iteration 27640 Training loss 0.006138290278613567 Validation loss 0.012106405571103096 Accuracy 0.87353515625\n",
      "Iteration 27650 Training loss 0.00792035274207592 Validation loss 0.012056340463459492 Accuracy 0.875\n",
      "Iteration 27660 Training loss 0.008661012165248394 Validation loss 0.01170412078499794 Accuracy 0.87841796875\n",
      "Iteration 27670 Training loss 0.008327257819473743 Validation loss 0.011686105281114578 Accuracy 0.87744140625\n",
      "Iteration 27680 Training loss 0.007657813839614391 Validation loss 0.012289082631468773 Accuracy 0.8720703125\n",
      "Iteration 27690 Training loss 0.005488371942192316 Validation loss 0.011641820892691612 Accuracy 0.87939453125\n",
      "Iteration 27700 Training loss 0.00752615462988615 Validation loss 0.012048806063830853 Accuracy 0.875\n",
      "Iteration 27710 Training loss 0.008693940937519073 Validation loss 0.011922462843358517 Accuracy 0.8759765625\n",
      "Iteration 27720 Training loss 0.008143694140017033 Validation loss 0.011638901196420193 Accuracy 0.8779296875\n",
      "Iteration 27730 Training loss 0.009865174070000648 Validation loss 0.011954844929277897 Accuracy 0.875\n",
      "Iteration 27740 Training loss 0.00803603045642376 Validation loss 0.012626461684703827 Accuracy 0.86767578125\n",
      "Iteration 27750 Training loss 0.006135066505521536 Validation loss 0.01175222359597683 Accuracy 0.876953125\n",
      "Iteration 27760 Training loss 0.007735795807093382 Validation loss 0.011823012493550777 Accuracy 0.8779296875\n",
      "Iteration 27770 Training loss 0.008364110253751278 Validation loss 0.012191384099423885 Accuracy 0.873046875\n",
      "Iteration 27780 Training loss 0.007790068164467812 Validation loss 0.011867331340909004 Accuracy 0.8759765625\n",
      "Iteration 27790 Training loss 0.007914881221950054 Validation loss 0.011622400023043156 Accuracy 0.87890625\n",
      "Iteration 27800 Training loss 0.009639483876526356 Validation loss 0.011996730230748653 Accuracy 0.875\n",
      "Iteration 27810 Training loss 0.006538126617670059 Validation loss 0.011837325058877468 Accuracy 0.875\n",
      "Iteration 27820 Training loss 0.00722101517021656 Validation loss 0.011910676024854183 Accuracy 0.875\n",
      "Iteration 27830 Training loss 0.007174591533839703 Validation loss 0.01216235663741827 Accuracy 0.873046875\n",
      "Iteration 27840 Training loss 0.009427355602383614 Validation loss 0.011734421364963055 Accuracy 0.8779296875\n",
      "Iteration 27850 Training loss 0.007739648222923279 Validation loss 0.011797456070780754 Accuracy 0.87744140625\n",
      "Iteration 27860 Training loss 0.008321394212543964 Validation loss 0.01215503178536892 Accuracy 0.873046875\n",
      "Iteration 27870 Training loss 0.007340562529861927 Validation loss 0.011596405878663063 Accuracy 0.87841796875\n",
      "Iteration 27880 Training loss 0.006210376508533955 Validation loss 0.0121190520003438 Accuracy 0.87353515625\n",
      "Iteration 27890 Training loss 0.006754279602319002 Validation loss 0.012027931399643421 Accuracy 0.8740234375\n",
      "Iteration 27900 Training loss 0.007972118444740772 Validation loss 0.01203688234090805 Accuracy 0.87451171875\n",
      "Iteration 27910 Training loss 0.008324570953845978 Validation loss 0.011686732992529869 Accuracy 0.87744140625\n",
      "Iteration 27920 Training loss 0.007806430570781231 Validation loss 0.011776108294725418 Accuracy 0.87646484375\n",
      "Iteration 27930 Training loss 0.006579054519534111 Validation loss 0.011917452327907085 Accuracy 0.87548828125\n",
      "Iteration 27940 Training loss 0.005823525134474039 Validation loss 0.011834642849862576 Accuracy 0.8759765625\n",
      "Iteration 27950 Training loss 0.0078183114528656 Validation loss 0.011790216900408268 Accuracy 0.8759765625\n",
      "Iteration 27960 Training loss 0.008393239229917526 Validation loss 0.011684808880090714 Accuracy 0.8779296875\n",
      "Iteration 27970 Training loss 0.007217428646981716 Validation loss 0.012016776017844677 Accuracy 0.875\n",
      "Iteration 27980 Training loss 0.006093183998018503 Validation loss 0.011698751710355282 Accuracy 0.876953125\n",
      "Iteration 27990 Training loss 0.007257089484483004 Validation loss 0.01226949691772461 Accuracy 0.8720703125\n",
      "Iteration 28000 Training loss 0.007036089431494474 Validation loss 0.01159412320703268 Accuracy 0.87841796875\n",
      "Iteration 28010 Training loss 0.006782338488847017 Validation loss 0.011898312717676163 Accuracy 0.87451171875\n",
      "Iteration 28020 Training loss 0.007813377305865288 Validation loss 0.01226307824254036 Accuracy 0.87158203125\n",
      "Iteration 28030 Training loss 0.007364924065768719 Validation loss 0.01172411348670721 Accuracy 0.87744140625\n",
      "Iteration 28040 Training loss 0.007909462787210941 Validation loss 0.012008161284029484 Accuracy 0.8740234375\n",
      "Iteration 28050 Training loss 0.008700714446604252 Validation loss 0.01186104491353035 Accuracy 0.87548828125\n",
      "Iteration 28060 Training loss 0.01027597114443779 Validation loss 0.011756977066397667 Accuracy 0.87646484375\n",
      "Iteration 28070 Training loss 0.007469119969755411 Validation loss 0.011840550228953362 Accuracy 0.87744140625\n",
      "Iteration 28080 Training loss 0.009978980757296085 Validation loss 0.011757723987102509 Accuracy 0.87744140625\n",
      "Iteration 28090 Training loss 0.007380299735814333 Validation loss 0.011966886930167675 Accuracy 0.87548828125\n",
      "Iteration 28100 Training loss 0.006999081000685692 Validation loss 0.01161003764718771 Accuracy 0.87841796875\n",
      "Iteration 28110 Training loss 0.008579635992646217 Validation loss 0.012372247874736786 Accuracy 0.87060546875\n",
      "Iteration 28120 Training loss 0.009545388631522655 Validation loss 0.011946093291044235 Accuracy 0.87548828125\n",
      "Iteration 28130 Training loss 0.007583782076835632 Validation loss 0.012072790414094925 Accuracy 0.87353515625\n",
      "Iteration 28140 Training loss 0.008954240009188652 Validation loss 0.011520723812282085 Accuracy 0.8798828125\n",
      "Iteration 28150 Training loss 0.008747327141463757 Validation loss 0.011851777322590351 Accuracy 0.87548828125\n",
      "Iteration 28160 Training loss 0.009440560825169086 Validation loss 0.013032817281782627 Accuracy 0.86376953125\n",
      "Iteration 28170 Training loss 0.007005890365689993 Validation loss 0.011636543087661266 Accuracy 0.87841796875\n",
      "Iteration 28180 Training loss 0.008342272602021694 Validation loss 0.0119223827496171 Accuracy 0.8740234375\n",
      "Iteration 28190 Training loss 0.00722720380872488 Validation loss 0.011581532657146454 Accuracy 0.87744140625\n",
      "Iteration 28200 Training loss 0.007623474579304457 Validation loss 0.011742842383682728 Accuracy 0.876953125\n",
      "Iteration 28210 Training loss 0.008242147974669933 Validation loss 0.011973617598414421 Accuracy 0.87451171875\n",
      "Iteration 28220 Training loss 0.007128084544092417 Validation loss 0.01180909015238285 Accuracy 0.87548828125\n",
      "Iteration 28230 Training loss 0.00781433004885912 Validation loss 0.011863323859870434 Accuracy 0.87548828125\n",
      "Iteration 28240 Training loss 0.00830643530935049 Validation loss 0.012104235589504242 Accuracy 0.873046875\n",
      "Iteration 28250 Training loss 0.008215717040002346 Validation loss 0.011928979307413101 Accuracy 0.8740234375\n",
      "Iteration 28260 Training loss 0.00836207065731287 Validation loss 0.011819670908153057 Accuracy 0.87646484375\n",
      "Iteration 28270 Training loss 0.008384235203266144 Validation loss 0.01180652342736721 Accuracy 0.8759765625\n",
      "Iteration 28280 Training loss 0.006459343712776899 Validation loss 0.01168214250355959 Accuracy 0.87744140625\n",
      "Iteration 28290 Training loss 0.00820736214518547 Validation loss 0.011647665873169899 Accuracy 0.876953125\n",
      "Iteration 28300 Training loss 0.007637456059455872 Validation loss 0.011741606518626213 Accuracy 0.8759765625\n",
      "Iteration 28310 Training loss 0.007934560999274254 Validation loss 0.011730675585567951 Accuracy 0.8759765625\n",
      "Iteration 28320 Training loss 0.008215402252972126 Validation loss 0.011716511100530624 Accuracy 0.87646484375\n",
      "Iteration 28330 Training loss 0.007005107589066029 Validation loss 0.011678056791424751 Accuracy 0.8779296875\n",
      "Iteration 28340 Training loss 0.008003639057278633 Validation loss 0.011797051876783371 Accuracy 0.875\n",
      "Iteration 28350 Training loss 0.007680942304432392 Validation loss 0.011977077461779118 Accuracy 0.87353515625\n",
      "Iteration 28360 Training loss 0.005875336937606335 Validation loss 0.011476895771920681 Accuracy 0.87890625\n",
      "Iteration 28370 Training loss 0.008449770510196686 Validation loss 0.011498271487653255 Accuracy 0.87939453125\n",
      "Iteration 28380 Training loss 0.007935803383588791 Validation loss 0.01156347244977951 Accuracy 0.87841796875\n",
      "Iteration 28390 Training loss 0.00793205201625824 Validation loss 0.011508078314363956 Accuracy 0.87939453125\n",
      "Iteration 28400 Training loss 0.006709035020321608 Validation loss 0.01183808222413063 Accuracy 0.875\n",
      "Iteration 28410 Training loss 0.009240199811756611 Validation loss 0.011551547795534134 Accuracy 0.87841796875\n",
      "Iteration 28420 Training loss 0.00785096362233162 Validation loss 0.011675548739731312 Accuracy 0.876953125\n",
      "Iteration 28430 Training loss 0.007366660516709089 Validation loss 0.011722191236913204 Accuracy 0.87646484375\n",
      "Iteration 28440 Training loss 0.0064955842681229115 Validation loss 0.011632434092462063 Accuracy 0.8779296875\n",
      "Iteration 28450 Training loss 0.007344002835452557 Validation loss 0.011622766964137554 Accuracy 0.87744140625\n",
      "Iteration 28460 Training loss 0.008183516561985016 Validation loss 0.011903855949640274 Accuracy 0.875\n",
      "Iteration 28470 Training loss 0.007616029586642981 Validation loss 0.012028682976961136 Accuracy 0.873046875\n",
      "Iteration 28480 Training loss 0.007459106855094433 Validation loss 0.01164830382913351 Accuracy 0.876953125\n",
      "Iteration 28490 Training loss 0.00785069540143013 Validation loss 0.011741426773369312 Accuracy 0.876953125\n",
      "Iteration 28500 Training loss 0.007720089051872492 Validation loss 0.012124589644372463 Accuracy 0.87353515625\n",
      "Iteration 28510 Training loss 0.008263619616627693 Validation loss 0.01175711303949356 Accuracy 0.8759765625\n",
      "Iteration 28520 Training loss 0.008301262743771076 Validation loss 0.011694229207932949 Accuracy 0.87646484375\n",
      "Iteration 28530 Training loss 0.009462963789701462 Validation loss 0.011546138674020767 Accuracy 0.87890625\n",
      "Iteration 28540 Training loss 0.007191263604909182 Validation loss 0.011645730584859848 Accuracy 0.8779296875\n",
      "Iteration 28550 Training loss 0.008787734434008598 Validation loss 0.012471684254705906 Accuracy 0.869140625\n",
      "Iteration 28560 Training loss 0.0075080506503582 Validation loss 0.012241258285939693 Accuracy 0.87109375\n",
      "Iteration 28570 Training loss 0.009373405016958714 Validation loss 0.01180389616638422 Accuracy 0.875\n",
      "Iteration 28580 Training loss 0.008437984623014927 Validation loss 0.012080634012818336 Accuracy 0.87353515625\n",
      "Iteration 28590 Training loss 0.006875729188323021 Validation loss 0.01144607923924923 Accuracy 0.8798828125\n",
      "Iteration 28600 Training loss 0.006930139381438494 Validation loss 0.011791342869400978 Accuracy 0.87646484375\n",
      "Iteration 28610 Training loss 0.00606507295742631 Validation loss 0.011563221924006939 Accuracy 0.87890625\n",
      "Iteration 28620 Training loss 0.0074170175939798355 Validation loss 0.011915252543985844 Accuracy 0.875\n",
      "Iteration 28630 Training loss 0.00760864932090044 Validation loss 0.011588232591748238 Accuracy 0.87841796875\n",
      "Iteration 28640 Training loss 0.0057343062944710255 Validation loss 0.011877600103616714 Accuracy 0.87548828125\n",
      "Iteration 28650 Training loss 0.008596181869506836 Validation loss 0.011836964637041092 Accuracy 0.875\n",
      "Iteration 28660 Training loss 0.006850401405245066 Validation loss 0.011979506351053715 Accuracy 0.8720703125\n",
      "Iteration 28670 Training loss 0.00909889955073595 Validation loss 0.011914240196347237 Accuracy 0.87451171875\n",
      "Iteration 28680 Training loss 0.00971619039773941 Validation loss 0.012581568211317062 Accuracy 0.86767578125\n",
      "Iteration 28690 Training loss 0.006926733534783125 Validation loss 0.011693143285810947 Accuracy 0.8759765625\n",
      "Iteration 28700 Training loss 0.008549944497644901 Validation loss 0.011931674554944038 Accuracy 0.87353515625\n",
      "Iteration 28710 Training loss 0.0077823130413889885 Validation loss 0.011517651379108429 Accuracy 0.8779296875\n",
      "Iteration 28720 Training loss 0.006785762030631304 Validation loss 0.011573153547942638 Accuracy 0.87744140625\n",
      "Iteration 28730 Training loss 0.008104716427624226 Validation loss 0.012103875167667866 Accuracy 0.87255859375\n",
      "Iteration 28740 Training loss 0.006149164866656065 Validation loss 0.011513193137943745 Accuracy 0.87841796875\n",
      "Iteration 28750 Training loss 0.008461766876280308 Validation loss 0.011803146451711655 Accuracy 0.87548828125\n",
      "Iteration 28760 Training loss 0.008685644716024399 Validation loss 0.011906764470040798 Accuracy 0.87548828125\n",
      "Iteration 28770 Training loss 0.00803344789892435 Validation loss 0.01226991880685091 Accuracy 0.8701171875\n",
      "Iteration 28780 Training loss 0.0070131937973201275 Validation loss 0.011638574302196503 Accuracy 0.87646484375\n",
      "Iteration 28790 Training loss 0.006284468341618776 Validation loss 0.011869912035763264 Accuracy 0.875\n",
      "Iteration 28800 Training loss 0.00809633731842041 Validation loss 0.011650172993540764 Accuracy 0.87841796875\n",
      "Iteration 28810 Training loss 0.006548220291733742 Validation loss 0.011847897432744503 Accuracy 0.875\n",
      "Iteration 28820 Training loss 0.007642501965165138 Validation loss 0.011849405243992805 Accuracy 0.875\n",
      "Iteration 28830 Training loss 0.00784971285611391 Validation loss 0.011595728807151318 Accuracy 0.8779296875\n",
      "Iteration 28840 Training loss 0.006326471921056509 Validation loss 0.011594928801059723 Accuracy 0.876953125\n",
      "Iteration 28850 Training loss 0.0073667652904987335 Validation loss 0.011604678817093372 Accuracy 0.87744140625\n",
      "Iteration 28860 Training loss 0.011027788743376732 Validation loss 0.012247827835381031 Accuracy 0.87109375\n",
      "Iteration 28870 Training loss 0.005843072198331356 Validation loss 0.012268676422536373 Accuracy 0.87109375\n",
      "Iteration 28880 Training loss 0.010082213208079338 Validation loss 0.012029234319925308 Accuracy 0.87353515625\n",
      "Iteration 28890 Training loss 0.007157936226576567 Validation loss 0.011802885681390762 Accuracy 0.875\n",
      "Iteration 28900 Training loss 0.008787620812654495 Validation loss 0.011728876270353794 Accuracy 0.876953125\n",
      "Iteration 28910 Training loss 0.00642054807394743 Validation loss 0.01200227253139019 Accuracy 0.87353515625\n",
      "Iteration 28920 Training loss 0.0047406828962266445 Validation loss 0.011610009707510471 Accuracy 0.87744140625\n",
      "Iteration 28930 Training loss 0.007527433801442385 Validation loss 0.011729883961379528 Accuracy 0.87744140625\n",
      "Iteration 28940 Training loss 0.008431569673120975 Validation loss 0.011834480799734592 Accuracy 0.8759765625\n",
      "Iteration 28950 Training loss 0.009372752159833908 Validation loss 0.012240167707204819 Accuracy 0.87158203125\n",
      "Iteration 28960 Training loss 0.007464465219527483 Validation loss 0.011912873014807701 Accuracy 0.87451171875\n",
      "Iteration 28970 Training loss 0.007133233826607466 Validation loss 0.01131357904523611 Accuracy 0.88037109375\n",
      "Iteration 28980 Training loss 0.009025568142533302 Validation loss 0.011354230344295502 Accuracy 0.88037109375\n",
      "Iteration 28990 Training loss 0.007667317520827055 Validation loss 0.012948100455105305 Accuracy 0.8642578125\n",
      "Iteration 29000 Training loss 0.009691995568573475 Validation loss 0.012154572643339634 Accuracy 0.87255859375\n",
      "Iteration 29010 Training loss 0.008312353864312172 Validation loss 0.011770224198698997 Accuracy 0.87646484375\n",
      "Iteration 29020 Training loss 0.008319304324686527 Validation loss 0.011682902462780476 Accuracy 0.8759765625\n",
      "Iteration 29030 Training loss 0.007775993086397648 Validation loss 0.011560372076928616 Accuracy 0.87890625\n",
      "Iteration 29040 Training loss 0.006274942308664322 Validation loss 0.01137127261608839 Accuracy 0.88037109375\n",
      "Iteration 29050 Training loss 0.006500554736703634 Validation loss 0.011497384868562222 Accuracy 0.87939453125\n",
      "Iteration 29060 Training loss 0.0055998568423092365 Validation loss 0.011820434592664242 Accuracy 0.875\n",
      "Iteration 29070 Training loss 0.007938789203763008 Validation loss 0.011569462716579437 Accuracy 0.87890625\n",
      "Iteration 29080 Training loss 0.006428725551813841 Validation loss 0.01161816157400608 Accuracy 0.87841796875\n",
      "Iteration 29090 Training loss 0.0056461854837834835 Validation loss 0.01177686732262373 Accuracy 0.8759765625\n",
      "Iteration 29100 Training loss 0.009479908272624016 Validation loss 0.01220209151506424 Accuracy 0.8720703125\n",
      "Iteration 29110 Training loss 0.011350490152835846 Validation loss 0.012314938940107822 Accuracy 0.87060546875\n",
      "Iteration 29120 Training loss 0.007386204786598682 Validation loss 0.012258029542863369 Accuracy 0.87109375\n",
      "Iteration 29130 Training loss 0.008805971592664719 Validation loss 0.01156895887106657 Accuracy 0.87841796875\n",
      "Iteration 29140 Training loss 0.00659548444673419 Validation loss 0.011595758609473705 Accuracy 0.8779296875\n",
      "Iteration 29150 Training loss 0.00704021705314517 Validation loss 0.011448025703430176 Accuracy 0.87841796875\n",
      "Iteration 29160 Training loss 0.008267725817859173 Validation loss 0.011552797630429268 Accuracy 0.87841796875\n",
      "Iteration 29170 Training loss 0.007526073604822159 Validation loss 0.011588630266487598 Accuracy 0.87890625\n",
      "Iteration 29180 Training loss 0.007744623813778162 Validation loss 0.011498232372105122 Accuracy 0.87841796875\n",
      "Iteration 29190 Training loss 0.007167432922869921 Validation loss 0.011653924360871315 Accuracy 0.87744140625\n",
      "Iteration 29200 Training loss 0.006965754088014364 Validation loss 0.011796023696660995 Accuracy 0.87548828125\n",
      "Iteration 29210 Training loss 0.00914719607681036 Validation loss 0.012214794754981995 Accuracy 0.87109375\n",
      "Iteration 29220 Training loss 0.005985599476844072 Validation loss 0.011652694083750248 Accuracy 0.87744140625\n",
      "Iteration 29230 Training loss 0.007067792117595673 Validation loss 0.01135755330324173 Accuracy 0.8798828125\n",
      "Iteration 29240 Training loss 0.006974946241825819 Validation loss 0.011686418205499649 Accuracy 0.87646484375\n",
      "Iteration 29250 Training loss 0.008281572721898556 Validation loss 0.01181998010724783 Accuracy 0.87548828125\n",
      "Iteration 29260 Training loss 0.008416328579187393 Validation loss 0.011763646267354488 Accuracy 0.875\n",
      "Iteration 29270 Training loss 0.007533838972449303 Validation loss 0.011577391065657139 Accuracy 0.8779296875\n",
      "Iteration 29280 Training loss 0.0060797519981861115 Validation loss 0.011474506929516792 Accuracy 0.8779296875\n",
      "Iteration 29290 Training loss 0.007437509950250387 Validation loss 0.011679834686219692 Accuracy 0.87744140625\n",
      "Iteration 29300 Training loss 0.0073678684420883656 Validation loss 0.011861237697303295 Accuracy 0.875\n",
      "Iteration 29310 Training loss 0.006705136504024267 Validation loss 0.011565852910280228 Accuracy 0.87744140625\n",
      "Iteration 29320 Training loss 0.0064809746108949184 Validation loss 0.01152662094682455 Accuracy 0.87890625\n",
      "Iteration 29330 Training loss 0.007426944561302662 Validation loss 0.011619562283158302 Accuracy 0.8779296875\n",
      "Iteration 29340 Training loss 0.006780903320759535 Validation loss 0.011449230834841728 Accuracy 0.87890625\n",
      "Iteration 29350 Training loss 0.008507580496370792 Validation loss 0.011941002681851387 Accuracy 0.8740234375\n",
      "Iteration 29360 Training loss 0.00959145650267601 Validation loss 0.012149100191891193 Accuracy 0.8720703125\n",
      "Iteration 29370 Training loss 0.00867692194879055 Validation loss 0.01184962410479784 Accuracy 0.87646484375\n",
      "Iteration 29380 Training loss 0.007186360191553831 Validation loss 0.011699761264026165 Accuracy 0.8779296875\n",
      "Iteration 29390 Training loss 0.0076568820513784885 Validation loss 0.01209650095552206 Accuracy 0.873046875\n",
      "Iteration 29400 Training loss 0.007533776108175516 Validation loss 0.012073935009539127 Accuracy 0.87255859375\n",
      "Iteration 29410 Training loss 0.007975098676979542 Validation loss 0.011764464899897575 Accuracy 0.876953125\n",
      "Iteration 29420 Training loss 0.007044883444905281 Validation loss 0.0115336449816823 Accuracy 0.8779296875\n",
      "Iteration 29430 Training loss 0.008509304374456406 Validation loss 0.011699296534061432 Accuracy 0.8759765625\n",
      "Iteration 29440 Training loss 0.00684236828237772 Validation loss 0.011422619223594666 Accuracy 0.87939453125\n",
      "Iteration 29450 Training loss 0.00806161854416132 Validation loss 0.011509956791996956 Accuracy 0.87744140625\n",
      "Iteration 29460 Training loss 0.007501149550080299 Validation loss 0.011846054345369339 Accuracy 0.87646484375\n",
      "Iteration 29470 Training loss 0.007874920964241028 Validation loss 0.011654030531644821 Accuracy 0.8779296875\n",
      "Iteration 29480 Training loss 0.0076055810786783695 Validation loss 0.012137901037931442 Accuracy 0.87255859375\n",
      "Iteration 29490 Training loss 0.006193637847900391 Validation loss 0.011549308896064758 Accuracy 0.87841796875\n",
      "Iteration 29500 Training loss 0.007543408777564764 Validation loss 0.01162815373390913 Accuracy 0.876953125\n",
      "Iteration 29510 Training loss 0.006784304045140743 Validation loss 0.011881789192557335 Accuracy 0.875\n",
      "Iteration 29520 Training loss 0.009185289964079857 Validation loss 0.011670217849314213 Accuracy 0.87646484375\n",
      "Iteration 29530 Training loss 0.008006422780454159 Validation loss 0.011619777418673038 Accuracy 0.8779296875\n",
      "Iteration 29540 Training loss 0.006084635388106108 Validation loss 0.01183069683611393 Accuracy 0.87548828125\n",
      "Iteration 29550 Training loss 0.006434381008148193 Validation loss 0.011995278298854828 Accuracy 0.87451171875\n",
      "Iteration 29560 Training loss 0.007338097784668207 Validation loss 0.011782988905906677 Accuracy 0.875\n",
      "Iteration 29570 Training loss 0.007482743356376886 Validation loss 0.011702069081366062 Accuracy 0.87744140625\n",
      "Iteration 29580 Training loss 0.007601690012961626 Validation loss 0.011586995795369148 Accuracy 0.87890625\n",
      "Iteration 29590 Training loss 0.006817697547376156 Validation loss 0.01167033426463604 Accuracy 0.8779296875\n",
      "Iteration 29600 Training loss 0.008799778297543526 Validation loss 0.012072044424712658 Accuracy 0.873046875\n",
      "Iteration 29610 Training loss 0.0071281446143984795 Validation loss 0.011734159663319588 Accuracy 0.87646484375\n",
      "Iteration 29620 Training loss 0.00587235763669014 Validation loss 0.011608109809458256 Accuracy 0.87744140625\n",
      "Iteration 29630 Training loss 0.00768813444301486 Validation loss 0.011471142061054707 Accuracy 0.8779296875\n",
      "Iteration 29640 Training loss 0.006655343808233738 Validation loss 0.011664391495287418 Accuracy 0.876953125\n",
      "Iteration 29650 Training loss 0.0056359851732850075 Validation loss 0.011577474884688854 Accuracy 0.87841796875\n",
      "Iteration 29660 Training loss 0.008825181983411312 Validation loss 0.011934401467442513 Accuracy 0.8740234375\n",
      "Iteration 29670 Training loss 0.004989968612790108 Validation loss 0.011483635753393173 Accuracy 0.87841796875\n",
      "Iteration 29680 Training loss 0.007325365673750639 Validation loss 0.011631420813500881 Accuracy 0.87744140625\n",
      "Iteration 29690 Training loss 0.006545716430991888 Validation loss 0.011541918851435184 Accuracy 0.8779296875\n",
      "Iteration 29700 Training loss 0.007844657637178898 Validation loss 0.011351381428539753 Accuracy 0.880859375\n",
      "Iteration 29710 Training loss 0.008418948389589787 Validation loss 0.012190834619104862 Accuracy 0.87060546875\n",
      "Iteration 29720 Training loss 0.0076545486226677895 Validation loss 0.01162810530513525 Accuracy 0.87744140625\n",
      "Iteration 29730 Training loss 0.009665640071034431 Validation loss 0.012013090774416924 Accuracy 0.87451171875\n",
      "Iteration 29740 Training loss 0.008869007229804993 Validation loss 0.012672814540565014 Accuracy 0.8662109375\n",
      "Iteration 29750 Training loss 0.006043052766472101 Validation loss 0.011842955835163593 Accuracy 0.875\n",
      "Iteration 29760 Training loss 0.008069834671914577 Validation loss 0.011642394587397575 Accuracy 0.8759765625\n",
      "Iteration 29770 Training loss 0.00826241448521614 Validation loss 0.011654086410999298 Accuracy 0.87646484375\n",
      "Iteration 29780 Training loss 0.007931243628263474 Validation loss 0.011962191201746464 Accuracy 0.873046875\n",
      "Iteration 29790 Training loss 0.009380298666656017 Validation loss 0.011608768254518509 Accuracy 0.87646484375\n",
      "Iteration 29800 Training loss 0.007854590192437172 Validation loss 0.011960411444306374 Accuracy 0.87255859375\n",
      "Iteration 29810 Training loss 0.0069343154318630695 Validation loss 0.011574101634323597 Accuracy 0.87841796875\n",
      "Iteration 29820 Training loss 0.006927807349711657 Validation loss 0.012028461322188377 Accuracy 0.87255859375\n",
      "Iteration 29830 Training loss 0.009268650785088539 Validation loss 0.012092038057744503 Accuracy 0.87255859375\n",
      "Iteration 29840 Training loss 0.006385378539562225 Validation loss 0.012208591215312481 Accuracy 0.87109375\n",
      "Iteration 29850 Training loss 0.01059185154736042 Validation loss 0.012063884176313877 Accuracy 0.8720703125\n",
      "Iteration 29860 Training loss 0.006391728762537241 Validation loss 0.011851644143462181 Accuracy 0.87451171875\n",
      "Iteration 29870 Training loss 0.009669671766459942 Validation loss 0.011404096148908138 Accuracy 0.8798828125\n",
      "Iteration 29880 Training loss 0.0072915623895823956 Validation loss 0.011409340426325798 Accuracy 0.87939453125\n",
      "Iteration 29890 Training loss 0.007450420875102282 Validation loss 0.011693412438035011 Accuracy 0.876953125\n",
      "Iteration 29900 Training loss 0.005372568033635616 Validation loss 0.011503860354423523 Accuracy 0.8779296875\n",
      "Iteration 29910 Training loss 0.006695848889648914 Validation loss 0.01161862537264824 Accuracy 0.87744140625\n",
      "Iteration 29920 Training loss 0.007290856447070837 Validation loss 0.011526433750987053 Accuracy 0.87744140625\n",
      "Iteration 29930 Training loss 0.007245967630296946 Validation loss 0.011878280900418758 Accuracy 0.8740234375\n",
      "Iteration 29940 Training loss 0.007169542834162712 Validation loss 0.011761566624045372 Accuracy 0.875\n",
      "Iteration 29950 Training loss 0.006782681215554476 Validation loss 0.01127304695546627 Accuracy 0.88134765625\n",
      "Iteration 29960 Training loss 0.006837773602455854 Validation loss 0.011625709943473339 Accuracy 0.87744140625\n",
      "Iteration 29970 Training loss 0.006976152770221233 Validation loss 0.011588113382458687 Accuracy 0.87646484375\n",
      "Iteration 29980 Training loss 0.006045293062925339 Validation loss 0.011674204841256142 Accuracy 0.8759765625\n",
      "Iteration 29990 Training loss 0.007952505722641945 Validation loss 0.011605022475123405 Accuracy 0.87744140625\n",
      "Iteration 30000 Training loss 0.006320485845208168 Validation loss 0.011645023711025715 Accuracy 0.876953125\n",
      "Iteration 30010 Training loss 0.00654712924733758 Validation loss 0.011405445635318756 Accuracy 0.87841796875\n",
      "Iteration 30020 Training loss 0.006311304867267609 Validation loss 0.011544100940227509 Accuracy 0.87890625\n",
      "Iteration 30030 Training loss 0.006821444723755121 Validation loss 0.011403686366975307 Accuracy 0.87939453125\n",
      "Iteration 30040 Training loss 0.008876199834048748 Validation loss 0.012020300142467022 Accuracy 0.87255859375\n",
      "Iteration 30050 Training loss 0.0072644297033548355 Validation loss 0.011466987431049347 Accuracy 0.87841796875\n",
      "Iteration 30060 Training loss 0.00794339831918478 Validation loss 0.011510550044476986 Accuracy 0.87890625\n",
      "Iteration 30070 Training loss 0.0061071994714438915 Validation loss 0.011365224607288837 Accuracy 0.8798828125\n",
      "Iteration 30080 Training loss 0.0056686317548155785 Validation loss 0.011497137136757374 Accuracy 0.87841796875\n",
      "Iteration 30090 Training loss 0.007283377461135387 Validation loss 0.011418073438107967 Accuracy 0.87939453125\n",
      "Iteration 30100 Training loss 0.0058021447621285915 Validation loss 0.011325769126415253 Accuracy 0.8798828125\n",
      "Iteration 30110 Training loss 0.005559762939810753 Validation loss 0.011872890405356884 Accuracy 0.87451171875\n",
      "Iteration 30120 Training loss 0.007780233398079872 Validation loss 0.01139907818287611 Accuracy 0.8798828125\n",
      "Iteration 30130 Training loss 0.006376921199262142 Validation loss 0.011933556757867336 Accuracy 0.87353515625\n",
      "Iteration 30140 Training loss 0.007162676192820072 Validation loss 0.011702297255396843 Accuracy 0.87646484375\n",
      "Iteration 30150 Training loss 0.006099417340010405 Validation loss 0.011744670569896698 Accuracy 0.8759765625\n",
      "Iteration 30160 Training loss 0.004618639126420021 Validation loss 0.01134496834129095 Accuracy 0.8798828125\n",
      "Iteration 30170 Training loss 0.006389106158167124 Validation loss 0.011659163981676102 Accuracy 0.876953125\n",
      "Iteration 30180 Training loss 0.007250914350152016 Validation loss 0.011176953092217445 Accuracy 0.880859375\n",
      "Iteration 30190 Training loss 0.006504659540951252 Validation loss 0.01138417050242424 Accuracy 0.87939453125\n",
      "Iteration 30200 Training loss 0.005735012702643871 Validation loss 0.0119257103651762 Accuracy 0.8740234375\n",
      "Iteration 30210 Training loss 0.007157765794545412 Validation loss 0.011390642262995243 Accuracy 0.87939453125\n",
      "Iteration 30220 Training loss 0.007401277311146259 Validation loss 0.011566788889467716 Accuracy 0.876953125\n",
      "Iteration 30230 Training loss 0.006264041643589735 Validation loss 0.01137846615165472 Accuracy 0.8798828125\n",
      "Iteration 30240 Training loss 0.006303442642092705 Validation loss 0.011552555486559868 Accuracy 0.87744140625\n",
      "Iteration 30250 Training loss 0.004611768294125795 Validation loss 0.011498291045427322 Accuracy 0.8779296875\n",
      "Iteration 30260 Training loss 0.0063263727352023125 Validation loss 0.011493463069200516 Accuracy 0.8779296875\n",
      "Iteration 30270 Training loss 0.007011446636170149 Validation loss 0.011322922073304653 Accuracy 0.88037109375\n",
      "Iteration 30280 Training loss 0.007073111366480589 Validation loss 0.011159902438521385 Accuracy 0.8818359375\n",
      "Iteration 30290 Training loss 0.005582018289715052 Validation loss 0.011468378826975822 Accuracy 0.87744140625\n",
      "Iteration 30300 Training loss 0.006288452073931694 Validation loss 0.011530361138284206 Accuracy 0.8779296875\n",
      "Iteration 30310 Training loss 0.006208783481270075 Validation loss 0.01146460697054863 Accuracy 0.87841796875\n",
      "Iteration 30320 Training loss 0.008322269655764103 Validation loss 0.011333871632814407 Accuracy 0.8798828125\n",
      "Iteration 30330 Training loss 0.008492800407111645 Validation loss 0.011794650927186012 Accuracy 0.87451171875\n",
      "Iteration 30340 Training loss 0.007968095131218433 Validation loss 0.011554555036127567 Accuracy 0.8779296875\n",
      "Iteration 30350 Training loss 0.006872730795294046 Validation loss 0.011348738335072994 Accuracy 0.87939453125\n",
      "Iteration 30360 Training loss 0.00592793757095933 Validation loss 0.011367558501660824 Accuracy 0.8798828125\n",
      "Iteration 30370 Training loss 0.007965766824781895 Validation loss 0.01127803884446621 Accuracy 0.880859375\n",
      "Iteration 30380 Training loss 0.007681915070861578 Validation loss 0.011774019338190556 Accuracy 0.875\n",
      "Iteration 30390 Training loss 0.006674690172076225 Validation loss 0.011566606350243092 Accuracy 0.87548828125\n",
      "Iteration 30400 Training loss 0.007482477929443121 Validation loss 0.012078342027962208 Accuracy 0.8720703125\n",
      "Iteration 30410 Training loss 0.006008468568325043 Validation loss 0.011420054361224174 Accuracy 0.87890625\n",
      "Iteration 30420 Training loss 0.006931947078555822 Validation loss 0.01155101042240858 Accuracy 0.876953125\n",
      "Iteration 30430 Training loss 0.007682022172957659 Validation loss 0.011521345004439354 Accuracy 0.8779296875\n",
      "Iteration 30440 Training loss 0.005791066214442253 Validation loss 0.011314686387777328 Accuracy 0.8798828125\n",
      "Iteration 30450 Training loss 0.0069931005127727985 Validation loss 0.011604174971580505 Accuracy 0.87646484375\n",
      "Iteration 30460 Training loss 0.006445711478590965 Validation loss 0.011695856228470802 Accuracy 0.8759765625\n",
      "Iteration 30470 Training loss 0.006107133813202381 Validation loss 0.011350839398801327 Accuracy 0.87939453125\n",
      "Iteration 30480 Training loss 0.006730681750923395 Validation loss 0.011488600634038448 Accuracy 0.87841796875\n",
      "Iteration 30490 Training loss 0.0062372866086661816 Validation loss 0.011234290897846222 Accuracy 0.880859375\n",
      "Iteration 30500 Training loss 0.00865839421749115 Validation loss 0.011570945382118225 Accuracy 0.876953125\n",
      "Iteration 30510 Training loss 0.006837805267423391 Validation loss 0.01143072359263897 Accuracy 0.87890625\n",
      "Iteration 30520 Training loss 0.0059654065407812595 Validation loss 0.011074003763496876 Accuracy 0.8828125\n",
      "Iteration 30530 Training loss 0.006026911549270153 Validation loss 0.01126590184867382 Accuracy 0.8818359375\n",
      "Iteration 30540 Training loss 0.00818299874663353 Validation loss 0.011636029928922653 Accuracy 0.87646484375\n",
      "Iteration 30550 Training loss 0.007072826847434044 Validation loss 0.011336669325828552 Accuracy 0.88037109375\n",
      "Iteration 30560 Training loss 0.005263682920485735 Validation loss 0.011292287148535252 Accuracy 0.87939453125\n",
      "Iteration 30570 Training loss 0.007284113671630621 Validation loss 0.011251639574766159 Accuracy 0.8798828125\n",
      "Iteration 30580 Training loss 0.005969533696770668 Validation loss 0.01173490472137928 Accuracy 0.87548828125\n",
      "Iteration 30590 Training loss 0.0067400503903627396 Validation loss 0.01131211407482624 Accuracy 0.88037109375\n",
      "Iteration 30600 Training loss 0.005766026675701141 Validation loss 0.011377491988241673 Accuracy 0.87939453125\n",
      "Iteration 30610 Training loss 0.005587834399193525 Validation loss 0.011359605006873608 Accuracy 0.87890625\n",
      "Iteration 30620 Training loss 0.006956472992897034 Validation loss 0.011613764800131321 Accuracy 0.87646484375\n",
      "Iteration 30630 Training loss 0.008615072816610336 Validation loss 0.01199316419661045 Accuracy 0.8740234375\n",
      "Iteration 30640 Training loss 0.005853231530636549 Validation loss 0.011822115629911423 Accuracy 0.875\n",
      "Iteration 30650 Training loss 0.006833966355770826 Validation loss 0.01148490235209465 Accuracy 0.87890625\n",
      "Iteration 30660 Training loss 0.006115510128438473 Validation loss 0.01157007459551096 Accuracy 0.8779296875\n",
      "Iteration 30670 Training loss 0.007863467559218407 Validation loss 0.011212379671633244 Accuracy 0.880859375\n",
      "Iteration 30680 Training loss 0.005921643693000078 Validation loss 0.011657045222818851 Accuracy 0.87548828125\n",
      "Iteration 30690 Training loss 0.006702142767608166 Validation loss 0.011285195127129555 Accuracy 0.880859375\n",
      "Iteration 30700 Training loss 0.006360635161399841 Validation loss 0.011822494678199291 Accuracy 0.87451171875\n",
      "Iteration 30710 Training loss 0.007450159639120102 Validation loss 0.012079335749149323 Accuracy 0.87109375\n",
      "Iteration 30720 Training loss 0.004823499824851751 Validation loss 0.011513769626617432 Accuracy 0.87939453125\n",
      "Iteration 30730 Training loss 0.00756211718544364 Validation loss 0.011295114643871784 Accuracy 0.8798828125\n",
      "Iteration 30740 Training loss 0.0058893198147416115 Validation loss 0.011357894167304039 Accuracy 0.87939453125\n",
      "Iteration 30750 Training loss 0.006223456002771854 Validation loss 0.011312895454466343 Accuracy 0.8798828125\n",
      "Iteration 30760 Training loss 0.007180692162364721 Validation loss 0.011327408254146576 Accuracy 0.88037109375\n",
      "Iteration 30770 Training loss 0.006474674213677645 Validation loss 0.011528822593390942 Accuracy 0.87744140625\n",
      "Iteration 30780 Training loss 0.008184046484529972 Validation loss 0.011830580420792103 Accuracy 0.875\n",
      "Iteration 30790 Training loss 0.0055742706172168255 Validation loss 0.011489053256809711 Accuracy 0.87744140625\n",
      "Iteration 30800 Training loss 0.007621262688189745 Validation loss 0.011703873053193092 Accuracy 0.87548828125\n",
      "Iteration 30810 Training loss 0.006887189112603664 Validation loss 0.011772827245295048 Accuracy 0.875\n",
      "Iteration 30820 Training loss 0.006074477918446064 Validation loss 0.011181514710187912 Accuracy 0.8818359375\n",
      "Iteration 30830 Training loss 0.00801608245819807 Validation loss 0.01154692005366087 Accuracy 0.8779296875\n",
      "Iteration 30840 Training loss 0.008167123422026634 Validation loss 0.011362283490598202 Accuracy 0.8798828125\n",
      "Iteration 30850 Training loss 0.005624374840408564 Validation loss 0.011255161836743355 Accuracy 0.88037109375\n",
      "Iteration 30860 Training loss 0.005763669963926077 Validation loss 0.011843676678836346 Accuracy 0.8740234375\n",
      "Iteration 30870 Training loss 0.006306475028395653 Validation loss 0.011607742868363857 Accuracy 0.87646484375\n",
      "Iteration 30880 Training loss 0.005465759430080652 Validation loss 0.011336620897054672 Accuracy 0.8798828125\n",
      "Iteration 30890 Training loss 0.006524211261421442 Validation loss 0.011414600536227226 Accuracy 0.87890625\n",
      "Iteration 30900 Training loss 0.006249013356864452 Validation loss 0.011368047446012497 Accuracy 0.8798828125\n",
      "Iteration 30910 Training loss 0.006909803953021765 Validation loss 0.011646590195596218 Accuracy 0.8759765625\n",
      "Iteration 30920 Training loss 0.005905029363930225 Validation loss 0.011480585671961308 Accuracy 0.87841796875\n",
      "Iteration 30930 Training loss 0.007279306184500456 Validation loss 0.011554218828678131 Accuracy 0.876953125\n",
      "Iteration 30940 Training loss 0.006406895350664854 Validation loss 0.011458415538072586 Accuracy 0.8779296875\n",
      "Iteration 30950 Training loss 0.004686365835368633 Validation loss 0.011400390416383743 Accuracy 0.87841796875\n",
      "Iteration 30960 Training loss 0.007463465444743633 Validation loss 0.011138560250401497 Accuracy 0.88330078125\n",
      "Iteration 30970 Training loss 0.005832298658788204 Validation loss 0.011544574983417988 Accuracy 0.8779296875\n",
      "Iteration 30980 Training loss 0.005967637524008751 Validation loss 0.011546511203050613 Accuracy 0.87841796875\n",
      "Iteration 30990 Training loss 0.004418486729264259 Validation loss 0.011307776905596256 Accuracy 0.87939453125\n",
      "Iteration 31000 Training loss 0.006892859004437923 Validation loss 0.011546500958502293 Accuracy 0.876953125\n",
      "Iteration 31010 Training loss 0.00671358359977603 Validation loss 0.011077096685767174 Accuracy 0.88330078125\n",
      "Iteration 31020 Training loss 0.006594575010240078 Validation loss 0.011690285056829453 Accuracy 0.8759765625\n",
      "Iteration 31030 Training loss 0.004634881392121315 Validation loss 0.011700363829731941 Accuracy 0.87744140625\n",
      "Iteration 31040 Training loss 0.008435315452516079 Validation loss 0.012573043815791607 Accuracy 0.86669921875\n",
      "Iteration 31050 Training loss 0.008002035319805145 Validation loss 0.011733652092516422 Accuracy 0.87548828125\n",
      "Iteration 31060 Training loss 0.006755263078957796 Validation loss 0.011546390131115913 Accuracy 0.8779296875\n",
      "Iteration 31070 Training loss 0.006860892754048109 Validation loss 0.011662486009299755 Accuracy 0.8779296875\n",
      "Iteration 31080 Training loss 0.005251283757388592 Validation loss 0.011419357731938362 Accuracy 0.87939453125\n",
      "Iteration 31090 Training loss 0.007562987972050905 Validation loss 0.01163899153470993 Accuracy 0.87744140625\n",
      "Iteration 31100 Training loss 0.006761141587048769 Validation loss 0.011611782014369965 Accuracy 0.8779296875\n",
      "Iteration 31110 Training loss 0.005828452296555042 Validation loss 0.011510202661156654 Accuracy 0.87841796875\n",
      "Iteration 31120 Training loss 0.006256415043026209 Validation loss 0.011398732662200928 Accuracy 0.87939453125\n",
      "Iteration 31130 Training loss 0.0064547243528068066 Validation loss 0.011370510794222355 Accuracy 0.87890625\n",
      "Iteration 31140 Training loss 0.00794876366853714 Validation loss 0.011508412659168243 Accuracy 0.8779296875\n",
      "Iteration 31150 Training loss 0.005794445052742958 Validation loss 0.011370604857802391 Accuracy 0.87890625\n",
      "Iteration 31160 Training loss 0.008427777327597141 Validation loss 0.011336732655763626 Accuracy 0.87890625\n",
      "Iteration 31170 Training loss 0.007432281970977783 Validation loss 0.01158288773149252 Accuracy 0.87646484375\n",
      "Iteration 31180 Training loss 0.006526939105242491 Validation loss 0.011425926350057125 Accuracy 0.87841796875\n",
      "Iteration 31190 Training loss 0.006148590240627527 Validation loss 0.011355796828866005 Accuracy 0.88037109375\n",
      "Iteration 31200 Training loss 0.008400713093578815 Validation loss 0.011745952069759369 Accuracy 0.875\n",
      "Iteration 31210 Training loss 0.00614666473120451 Validation loss 0.011318481527268887 Accuracy 0.87890625\n",
      "Iteration 31220 Training loss 0.006621045526117086 Validation loss 0.01131319347769022 Accuracy 0.87890625\n",
      "Iteration 31230 Training loss 0.005627939943224192 Validation loss 0.011431505903601646 Accuracy 0.8779296875\n",
      "Iteration 31240 Training loss 0.006657647434622049 Validation loss 0.011428875848650932 Accuracy 0.87890625\n",
      "Iteration 31250 Training loss 0.003960699774324894 Validation loss 0.01151049043983221 Accuracy 0.8779296875\n",
      "Iteration 31260 Training loss 0.007570658810436726 Validation loss 0.01155025139451027 Accuracy 0.87646484375\n",
      "Iteration 31270 Training loss 0.008851100690662861 Validation loss 0.01165361050516367 Accuracy 0.876953125\n",
      "Iteration 31280 Training loss 0.006739895325154066 Validation loss 0.01131112314760685 Accuracy 0.8798828125\n",
      "Iteration 31290 Training loss 0.008172345347702503 Validation loss 0.012010597623884678 Accuracy 0.873046875\n",
      "Iteration 31300 Training loss 0.007385765668004751 Validation loss 0.01152070239186287 Accuracy 0.87841796875\n",
      "Iteration 31310 Training loss 0.0073690833523869514 Validation loss 0.011415795423090458 Accuracy 0.87890625\n",
      "Iteration 31320 Training loss 0.005810251925140619 Validation loss 0.01114631537348032 Accuracy 0.8818359375\n",
      "Iteration 31330 Training loss 0.00621394207701087 Validation loss 0.011486254632472992 Accuracy 0.87890625\n",
      "Iteration 31340 Training loss 0.00815499760210514 Validation loss 0.011218996718525887 Accuracy 0.8798828125\n",
      "Iteration 31350 Training loss 0.007855826057493687 Validation loss 0.01148651260882616 Accuracy 0.87841796875\n",
      "Iteration 31360 Training loss 0.006495460402220488 Validation loss 0.011433865875005722 Accuracy 0.87890625\n",
      "Iteration 31370 Training loss 0.0059728967025876045 Validation loss 0.011366954073309898 Accuracy 0.87939453125\n",
      "Iteration 31380 Training loss 0.005083461292088032 Validation loss 0.011052008718252182 Accuracy 0.8828125\n",
      "Iteration 31390 Training loss 0.007524449378252029 Validation loss 0.01118701882660389 Accuracy 0.88134765625\n",
      "Iteration 31400 Training loss 0.00680957967415452 Validation loss 0.01168382540345192 Accuracy 0.87548828125\n",
      "Iteration 31410 Training loss 0.0071913450956344604 Validation loss 0.011489088647067547 Accuracy 0.87744140625\n",
      "Iteration 31420 Training loss 0.006719712633639574 Validation loss 0.011142919771373272 Accuracy 0.880859375\n",
      "Iteration 31430 Training loss 0.005763383582234383 Validation loss 0.011174718849360943 Accuracy 0.880859375\n",
      "Iteration 31440 Training loss 0.0058603063225746155 Validation loss 0.011011255905032158 Accuracy 0.8828125\n",
      "Iteration 31450 Training loss 0.0050453850999474525 Validation loss 0.011381227523088455 Accuracy 0.87939453125\n",
      "Iteration 31460 Training loss 0.00812623742967844 Validation loss 0.011538245715200901 Accuracy 0.8779296875\n",
      "Iteration 31470 Training loss 0.007157976273447275 Validation loss 0.011179555207490921 Accuracy 0.88037109375\n",
      "Iteration 31480 Training loss 0.007016587536782026 Validation loss 0.011066723614931107 Accuracy 0.88134765625\n",
      "Iteration 31490 Training loss 0.006519025657325983 Validation loss 0.01141231507062912 Accuracy 0.87890625\n",
      "Iteration 31500 Training loss 0.0067601329647004604 Validation loss 0.011348193511366844 Accuracy 0.87939453125\n",
      "Iteration 31510 Training loss 0.006124036852270365 Validation loss 0.011502963490784168 Accuracy 0.87646484375\n",
      "Iteration 31520 Training loss 0.006451977416872978 Validation loss 0.011084897443652153 Accuracy 0.88232421875\n",
      "Iteration 31530 Training loss 0.006864383816719055 Validation loss 0.011372088454663754 Accuracy 0.87841796875\n",
      "Iteration 31540 Training loss 0.005893286783248186 Validation loss 0.011266459710896015 Accuracy 0.88037109375\n",
      "Iteration 31550 Training loss 0.0070605771616101265 Validation loss 0.011349118314683437 Accuracy 0.8798828125\n",
      "Iteration 31560 Training loss 0.007135014981031418 Validation loss 0.01131348218768835 Accuracy 0.880859375\n",
      "Iteration 31570 Training loss 0.006554767489433289 Validation loss 0.011182460933923721 Accuracy 0.880859375\n",
      "Iteration 31580 Training loss 0.007676472421735525 Validation loss 0.011483225040137768 Accuracy 0.87744140625\n",
      "Iteration 31590 Training loss 0.005944544915109873 Validation loss 0.011310502886772156 Accuracy 0.87939453125\n",
      "Iteration 31600 Training loss 0.007359717506915331 Validation loss 0.011359701864421368 Accuracy 0.8779296875\n",
      "Iteration 31610 Training loss 0.006802885793149471 Validation loss 0.011484116315841675 Accuracy 0.8779296875\n",
      "Iteration 31620 Training loss 0.006185241509228945 Validation loss 0.011506933718919754 Accuracy 0.87744140625\n",
      "Iteration 31630 Training loss 0.006774481385946274 Validation loss 0.011217162013053894 Accuracy 0.88134765625\n",
      "Iteration 31640 Training loss 0.0060135298408567905 Validation loss 0.011143333278596401 Accuracy 0.88134765625\n",
      "Iteration 31650 Training loss 0.0058058202266693115 Validation loss 0.011408048681914806 Accuracy 0.8779296875\n",
      "Iteration 31660 Training loss 0.006659107748419046 Validation loss 0.011179301887750626 Accuracy 0.88037109375\n",
      "Iteration 31670 Training loss 0.005807135254144669 Validation loss 0.011120337061583996 Accuracy 0.88232421875\n",
      "Iteration 31680 Training loss 0.005990318022668362 Validation loss 0.011156491003930569 Accuracy 0.88134765625\n",
      "Iteration 31690 Training loss 0.005831488408148289 Validation loss 0.011444677598774433 Accuracy 0.8779296875\n",
      "Iteration 31700 Training loss 0.006640064064413309 Validation loss 0.011231325566768646 Accuracy 0.88037109375\n",
      "Iteration 31710 Training loss 0.00585671653971076 Validation loss 0.01155336108058691 Accuracy 0.87646484375\n",
      "Iteration 31720 Training loss 0.0059296125546097755 Validation loss 0.011254173703491688 Accuracy 0.8798828125\n",
      "Iteration 31730 Training loss 0.007100221700966358 Validation loss 0.011084595695137978 Accuracy 0.88232421875\n",
      "Iteration 31740 Training loss 0.00485857343301177 Validation loss 0.011240209452807903 Accuracy 0.87939453125\n",
      "Iteration 31750 Training loss 0.005641834810376167 Validation loss 0.011055625043809414 Accuracy 0.8818359375\n",
      "Iteration 31760 Training loss 0.006896560080349445 Validation loss 0.011265356093645096 Accuracy 0.87939453125\n",
      "Iteration 31770 Training loss 0.005888919346034527 Validation loss 0.011160524562001228 Accuracy 0.880859375\n",
      "Iteration 31780 Training loss 0.006117984652519226 Validation loss 0.011336131021380424 Accuracy 0.87939453125\n",
      "Iteration 31790 Training loss 0.006080157123506069 Validation loss 0.011204167269170284 Accuracy 0.87939453125\n",
      "Iteration 31800 Training loss 0.005285655148327351 Validation loss 0.011249475181102753 Accuracy 0.880859375\n",
      "Iteration 31810 Training loss 0.008147838525474072 Validation loss 0.011412093415856361 Accuracy 0.87841796875\n",
      "Iteration 31820 Training loss 0.007870730943977833 Validation loss 0.01117733120918274 Accuracy 0.880859375\n",
      "Iteration 31830 Training loss 0.005617742892354727 Validation loss 0.011164866387844086 Accuracy 0.8818359375\n",
      "Iteration 31840 Training loss 0.006516637280583382 Validation loss 0.011233162134885788 Accuracy 0.8798828125\n",
      "Iteration 31850 Training loss 0.005380639340728521 Validation loss 0.011128056794404984 Accuracy 0.8828125\n",
      "Iteration 31860 Training loss 0.006204180419445038 Validation loss 0.011296647600829601 Accuracy 0.87890625\n",
      "Iteration 31870 Training loss 0.007310064043849707 Validation loss 0.011242449283599854 Accuracy 0.88037109375\n",
      "Iteration 31880 Training loss 0.005677938461303711 Validation loss 0.011099380441009998 Accuracy 0.8828125\n",
      "Iteration 31890 Training loss 0.006172082852572203 Validation loss 0.01146877370774746 Accuracy 0.87841796875\n",
      "Iteration 31900 Training loss 0.00663214735686779 Validation loss 0.011402328498661518 Accuracy 0.87890625\n",
      "Iteration 31910 Training loss 0.0063080089166760445 Validation loss 0.01103879138827324 Accuracy 0.88232421875\n",
      "Iteration 31920 Training loss 0.005089241079986095 Validation loss 0.011089765466749668 Accuracy 0.8828125\n",
      "Iteration 31930 Training loss 0.005606872495263815 Validation loss 0.011319099925458431 Accuracy 0.87890625\n",
      "Iteration 31940 Training loss 0.007146976888179779 Validation loss 0.011468920856714249 Accuracy 0.87744140625\n",
      "Iteration 31950 Training loss 0.005535517353564501 Validation loss 0.01116957701742649 Accuracy 0.880859375\n",
      "Iteration 31960 Training loss 0.006961475126445293 Validation loss 0.01112690195441246 Accuracy 0.88037109375\n",
      "Iteration 31970 Training loss 0.007591336965560913 Validation loss 0.012392550706863403 Accuracy 0.8681640625\n",
      "Iteration 31980 Training loss 0.004678164608776569 Validation loss 0.011414878070354462 Accuracy 0.87841796875\n",
      "Iteration 31990 Training loss 0.006607383489608765 Validation loss 0.011451706290245056 Accuracy 0.87744140625\n",
      "Iteration 32000 Training loss 0.006043868139386177 Validation loss 0.011440631002187729 Accuracy 0.87744140625\n",
      "Iteration 32010 Training loss 0.005052209831774235 Validation loss 0.011088548228144646 Accuracy 0.88134765625\n",
      "Iteration 32020 Training loss 0.005936037749052048 Validation loss 0.011446586810052395 Accuracy 0.87841796875\n",
      "Iteration 32030 Training loss 0.0048130271025002 Validation loss 0.011110388673841953 Accuracy 0.88134765625\n",
      "Iteration 32040 Training loss 0.006076997611671686 Validation loss 0.011335392482578754 Accuracy 0.87890625\n",
      "Iteration 32050 Training loss 0.005630841478705406 Validation loss 0.011211446486413479 Accuracy 0.88037109375\n",
      "Iteration 32060 Training loss 0.006312255747616291 Validation loss 0.011166835203766823 Accuracy 0.88134765625\n",
      "Iteration 32070 Training loss 0.006910521071404219 Validation loss 0.010959193110466003 Accuracy 0.8828125\n",
      "Iteration 32080 Training loss 0.007310524117201567 Validation loss 0.011272510513663292 Accuracy 0.87890625\n",
      "Iteration 32090 Training loss 0.005777565762400627 Validation loss 0.011175383813679218 Accuracy 0.88134765625\n",
      "Iteration 32100 Training loss 0.006025352515280247 Validation loss 0.011134020984172821 Accuracy 0.880859375\n",
      "Iteration 32110 Training loss 0.006416519172489643 Validation loss 0.011268097907304764 Accuracy 0.8798828125\n",
      "Iteration 32120 Training loss 0.005174259655177593 Validation loss 0.011031895875930786 Accuracy 0.88330078125\n",
      "Iteration 32130 Training loss 0.00613579573109746 Validation loss 0.011404404416680336 Accuracy 0.87939453125\n",
      "Iteration 32140 Training loss 0.004388338886201382 Validation loss 0.011201880872249603 Accuracy 0.88037109375\n",
      "Iteration 32150 Training loss 0.006183243822306395 Validation loss 0.011061491444706917 Accuracy 0.88330078125\n",
      "Iteration 32160 Training loss 0.005777999758720398 Validation loss 0.011062250472605228 Accuracy 0.8818359375\n",
      "Iteration 32170 Training loss 0.00683314586058259 Validation loss 0.01136012189090252 Accuracy 0.8779296875\n",
      "Iteration 32180 Training loss 0.006589523050934076 Validation loss 0.01138758473098278 Accuracy 0.87890625\n",
      "Iteration 32190 Training loss 0.007088576443493366 Validation loss 0.010965137742459774 Accuracy 0.88330078125\n",
      "Iteration 32200 Training loss 0.005318864248692989 Validation loss 0.011028757318854332 Accuracy 0.88134765625\n",
      "Iteration 32210 Training loss 0.007411324884742498 Validation loss 0.011315482668578625 Accuracy 0.8798828125\n",
      "Iteration 32220 Training loss 0.006913422606885433 Validation loss 0.011247135698795319 Accuracy 0.88037109375\n",
      "Iteration 32230 Training loss 0.007097930181771517 Validation loss 0.011470629833638668 Accuracy 0.87646484375\n",
      "Iteration 32240 Training loss 0.0056853145360946655 Validation loss 0.011330881156027317 Accuracy 0.87890625\n",
      "Iteration 32250 Training loss 0.0046750642359256744 Validation loss 0.011206298135221004 Accuracy 0.8798828125\n",
      "Iteration 32260 Training loss 0.007795598357915878 Validation loss 0.011420385912060738 Accuracy 0.8779296875\n",
      "Iteration 32270 Training loss 0.006811072118580341 Validation loss 0.0113152414560318 Accuracy 0.8798828125\n",
      "Iteration 32280 Training loss 0.00517222099006176 Validation loss 0.011437665671110153 Accuracy 0.87890625\n",
      "Iteration 32290 Training loss 0.006062841974198818 Validation loss 0.011190617457032204 Accuracy 0.880859375\n",
      "Iteration 32300 Training loss 0.006304888520389795 Validation loss 0.011379064992070198 Accuracy 0.87841796875\n",
      "Iteration 32310 Training loss 0.006730340886861086 Validation loss 0.011331748217344284 Accuracy 0.87939453125\n",
      "Iteration 32320 Training loss 0.005103130359202623 Validation loss 0.011198078282177448 Accuracy 0.88134765625\n",
      "Iteration 32330 Training loss 0.0058786277659237385 Validation loss 0.011173958890140057 Accuracy 0.880859375\n",
      "Iteration 32340 Training loss 0.005769750103354454 Validation loss 0.011186540126800537 Accuracy 0.88037109375\n",
      "Iteration 32350 Training loss 0.005222922656685114 Validation loss 0.011186666786670685 Accuracy 0.8798828125\n",
      "Iteration 32360 Training loss 0.004542950075119734 Validation loss 0.011427458375692368 Accuracy 0.87841796875\n",
      "Iteration 32370 Training loss 0.005213327240198851 Validation loss 0.011259078979492188 Accuracy 0.880859375\n",
      "Iteration 32380 Training loss 0.00678171357139945 Validation loss 0.01094913575798273 Accuracy 0.88232421875\n",
      "Iteration 32390 Training loss 0.004534040577709675 Validation loss 0.011243944987654686 Accuracy 0.8798828125\n",
      "Iteration 32400 Training loss 0.005894833244383335 Validation loss 0.011235462501645088 Accuracy 0.88134765625\n",
      "Iteration 32410 Training loss 0.006517126224935055 Validation loss 0.0120888976380229 Accuracy 0.87158203125\n",
      "Iteration 32420 Training loss 0.007811353541910648 Validation loss 0.01144343987107277 Accuracy 0.87744140625\n",
      "Iteration 32430 Training loss 0.005486801266670227 Validation loss 0.010889972560107708 Accuracy 0.88330078125\n",
      "Iteration 32440 Training loss 0.005416267551481724 Validation loss 0.011020917445421219 Accuracy 0.88232421875\n",
      "Iteration 32450 Training loss 0.00538030406460166 Validation loss 0.011046559549868107 Accuracy 0.88232421875\n",
      "Iteration 32460 Training loss 0.0063405088149011135 Validation loss 0.011207439005374908 Accuracy 0.8798828125\n",
      "Iteration 32470 Training loss 0.006374238058924675 Validation loss 0.011242998763918877 Accuracy 0.87939453125\n",
      "Iteration 32480 Training loss 0.006465147715061903 Validation loss 0.011129213497042656 Accuracy 0.88037109375\n",
      "Iteration 32490 Training loss 0.005407667253166437 Validation loss 0.011266530491411686 Accuracy 0.87890625\n",
      "Iteration 32500 Training loss 0.006224808748811483 Validation loss 0.01108035258948803 Accuracy 0.8818359375\n",
      "Iteration 32510 Training loss 0.0058698710054159164 Validation loss 0.011022316291928291 Accuracy 0.880859375\n",
      "Iteration 32520 Training loss 0.004886801820248365 Validation loss 0.01131424494087696 Accuracy 0.87890625\n",
      "Iteration 32530 Training loss 0.005628866609185934 Validation loss 0.011311937123537064 Accuracy 0.8798828125\n",
      "Iteration 32540 Training loss 0.006925329566001892 Validation loss 0.011364231817424297 Accuracy 0.87841796875\n",
      "Iteration 32550 Training loss 0.005115578416734934 Validation loss 0.01095704548060894 Accuracy 0.88330078125\n",
      "Iteration 32560 Training loss 0.004764610435813665 Validation loss 0.011250171810388565 Accuracy 0.8798828125\n",
      "Iteration 32570 Training loss 0.005714107304811478 Validation loss 0.011230076663196087 Accuracy 0.87939453125\n",
      "Iteration 32580 Training loss 0.004602549597620964 Validation loss 0.011291556991636753 Accuracy 0.87890625\n",
      "Iteration 32590 Training loss 0.006209971848875284 Validation loss 0.01103617437183857 Accuracy 0.88037109375\n",
      "Iteration 32600 Training loss 0.005945233162492514 Validation loss 0.011113470420241356 Accuracy 0.88134765625\n",
      "Iteration 32610 Training loss 0.005742064211517572 Validation loss 0.011172033846378326 Accuracy 0.880859375\n",
      "Iteration 32620 Training loss 0.005078483372926712 Validation loss 0.011144251562654972 Accuracy 0.880859375\n",
      "Iteration 32630 Training loss 0.0065073915757238865 Validation loss 0.011185494251549244 Accuracy 0.88134765625\n",
      "Iteration 32640 Training loss 0.00734733697026968 Validation loss 0.011305528692901134 Accuracy 0.87841796875\n",
      "Iteration 32650 Training loss 0.003661853726953268 Validation loss 0.011038132011890411 Accuracy 0.88232421875\n",
      "Iteration 32660 Training loss 0.0064261555671691895 Validation loss 0.011106336489319801 Accuracy 0.880859375\n",
      "Iteration 32670 Training loss 0.00541600352153182 Validation loss 0.010845114476978779 Accuracy 0.884765625\n",
      "Iteration 32680 Training loss 0.005439793225377798 Validation loss 0.01112744677811861 Accuracy 0.880859375\n",
      "Iteration 32690 Training loss 0.0064615532755851746 Validation loss 0.011001056991517544 Accuracy 0.88330078125\n",
      "Iteration 32700 Training loss 0.007143276743590832 Validation loss 0.011173753067851067 Accuracy 0.88037109375\n",
      "Iteration 32710 Training loss 0.005788701120764017 Validation loss 0.011056223884224892 Accuracy 0.8818359375\n",
      "Iteration 32720 Training loss 0.00606053089722991 Validation loss 0.01144607923924923 Accuracy 0.87744140625\n",
      "Iteration 32730 Training loss 0.006754925940185785 Validation loss 0.011359351687133312 Accuracy 0.87890625\n",
      "Iteration 32740 Training loss 0.0055391681380569935 Validation loss 0.011078526265919209 Accuracy 0.88232421875\n",
      "Iteration 32750 Training loss 0.006742216181010008 Validation loss 0.0110291363671422 Accuracy 0.88134765625\n",
      "Iteration 32760 Training loss 0.005285071209073067 Validation loss 0.011164629831910133 Accuracy 0.880859375\n",
      "Iteration 32770 Training loss 0.006066729314625263 Validation loss 0.011089322157204151 Accuracy 0.8818359375\n",
      "Iteration 32780 Training loss 0.004873800091445446 Validation loss 0.011289143934845924 Accuracy 0.87939453125\n",
      "Iteration 32790 Training loss 0.006555492989718914 Validation loss 0.011063070967793465 Accuracy 0.880859375\n",
      "Iteration 32800 Training loss 0.0053161997348070145 Validation loss 0.011006442829966545 Accuracy 0.8818359375\n",
      "Iteration 32810 Training loss 0.0070854732766747475 Validation loss 0.011194979771971703 Accuracy 0.88037109375\n",
      "Iteration 32820 Training loss 0.003710194956511259 Validation loss 0.011090941727161407 Accuracy 0.8818359375\n",
      "Iteration 32830 Training loss 0.005472505930811167 Validation loss 0.011186796240508556 Accuracy 0.8798828125\n",
      "Iteration 32840 Training loss 0.006914820056408644 Validation loss 0.011023517698049545 Accuracy 0.88330078125\n",
      "Iteration 32850 Training loss 0.006057742517441511 Validation loss 0.011093984358012676 Accuracy 0.880859375\n",
      "Iteration 32860 Training loss 0.0060669975355267525 Validation loss 0.011073117144405842 Accuracy 0.88232421875\n",
      "Iteration 32870 Training loss 0.006359377410262823 Validation loss 0.011098912917077541 Accuracy 0.880859375\n",
      "Iteration 32880 Training loss 0.004766400437802076 Validation loss 0.011069394648075104 Accuracy 0.88134765625\n",
      "Iteration 32890 Training loss 0.007431465666741133 Validation loss 0.011312881484627724 Accuracy 0.8779296875\n",
      "Iteration 32900 Training loss 0.00499672070145607 Validation loss 0.010981353931128979 Accuracy 0.88134765625\n",
      "Iteration 32910 Training loss 0.007100368849933147 Validation loss 0.011873172596096992 Accuracy 0.873046875\n",
      "Iteration 32920 Training loss 0.005391001235693693 Validation loss 0.011250992305576801 Accuracy 0.87939453125\n",
      "Iteration 32930 Training loss 0.007975034415721893 Validation loss 0.011103508993983269 Accuracy 0.880859375\n",
      "Iteration 32940 Training loss 0.007091119885444641 Validation loss 0.011499923653900623 Accuracy 0.876953125\n",
      "Iteration 32950 Training loss 0.0041091605089604855 Validation loss 0.010772446170449257 Accuracy 0.88525390625\n",
      "Iteration 32960 Training loss 0.005791496019810438 Validation loss 0.011113553307950497 Accuracy 0.88037109375\n",
      "Iteration 32970 Training loss 0.004661709535866976 Validation loss 0.011295083910226822 Accuracy 0.87890625\n",
      "Iteration 32980 Training loss 0.004768460523337126 Validation loss 0.011217458173632622 Accuracy 0.87939453125\n",
      "Iteration 32990 Training loss 0.005323188845068216 Validation loss 0.011148224584758282 Accuracy 0.880859375\n",
      "Iteration 33000 Training loss 0.005822619888931513 Validation loss 0.011090605519711971 Accuracy 0.880859375\n",
      "Iteration 33010 Training loss 0.0052918726578354836 Validation loss 0.0110386423766613 Accuracy 0.88232421875\n",
      "Iteration 33020 Training loss 0.004956148099154234 Validation loss 0.01105357427150011 Accuracy 0.88134765625\n",
      "Iteration 33030 Training loss 0.006127455271780491 Validation loss 0.011407182551920414 Accuracy 0.8779296875\n",
      "Iteration 33040 Training loss 0.006325860042124987 Validation loss 0.010973471216857433 Accuracy 0.8828125\n",
      "Iteration 33050 Training loss 0.007459686603397131 Validation loss 0.011430714279413223 Accuracy 0.87841796875\n",
      "Iteration 33060 Training loss 0.005236974451690912 Validation loss 0.011121918447315693 Accuracy 0.88134765625\n",
      "Iteration 33070 Training loss 0.003881374839693308 Validation loss 0.010915529914200306 Accuracy 0.8828125\n",
      "Iteration 33080 Training loss 0.00471551064401865 Validation loss 0.010822737589478493 Accuracy 0.88427734375\n",
      "Iteration 33090 Training loss 0.004732956178486347 Validation loss 0.010873991064727306 Accuracy 0.8828125\n",
      "Iteration 33100 Training loss 0.00780832814052701 Validation loss 0.01126011647284031 Accuracy 0.8798828125\n",
      "Iteration 33110 Training loss 0.006079086102545261 Validation loss 0.01091078668832779 Accuracy 0.8828125\n",
      "Iteration 33120 Training loss 0.004944067448377609 Validation loss 0.011223575100302696 Accuracy 0.8798828125\n",
      "Iteration 33130 Training loss 0.005759554449468851 Validation loss 0.01135424617677927 Accuracy 0.87890625\n",
      "Iteration 33140 Training loss 0.0043181246146559715 Validation loss 0.011116532608866692 Accuracy 0.880859375\n",
      "Iteration 33150 Training loss 0.007260761223733425 Validation loss 0.011209079064428806 Accuracy 0.880859375\n",
      "Iteration 33160 Training loss 0.0048228297382593155 Validation loss 0.011335575953125954 Accuracy 0.87890625\n",
      "Iteration 33170 Training loss 0.005024743266403675 Validation loss 0.011051638051867485 Accuracy 0.88134765625\n",
      "Iteration 33180 Training loss 0.00554606644436717 Validation loss 0.010971518233418465 Accuracy 0.8828125\n",
      "Iteration 33190 Training loss 0.006011446472257376 Validation loss 0.011498567648231983 Accuracy 0.8759765625\n",
      "Iteration 33200 Training loss 0.00721451872959733 Validation loss 0.011168964207172394 Accuracy 0.8798828125\n",
      "Iteration 33210 Training loss 0.0054899766109883785 Validation loss 0.011002279818058014 Accuracy 0.8818359375\n",
      "Iteration 33220 Training loss 0.005930526647716761 Validation loss 0.011651012115180492 Accuracy 0.87353515625\n",
      "Iteration 33230 Training loss 0.0040464806370437145 Validation loss 0.011218718253076077 Accuracy 0.8779296875\n",
      "Iteration 33240 Training loss 0.005389017518609762 Validation loss 0.011043969541788101 Accuracy 0.88232421875\n",
      "Iteration 33250 Training loss 0.00715686334297061 Validation loss 0.011199657805263996 Accuracy 0.8798828125\n",
      "Iteration 33260 Training loss 0.007078052498400211 Validation loss 0.011253229342401028 Accuracy 0.87939453125\n",
      "Iteration 33270 Training loss 0.0058293151669204235 Validation loss 0.011449843645095825 Accuracy 0.87744140625\n",
      "Iteration 33280 Training loss 0.005939871538430452 Validation loss 0.011273795738816261 Accuracy 0.87890625\n",
      "Iteration 33290 Training loss 0.005754049867391586 Validation loss 0.011064850725233555 Accuracy 0.8818359375\n",
      "Iteration 33300 Training loss 0.00597374839708209 Validation loss 0.011314797215163708 Accuracy 0.87890625\n",
      "Iteration 33310 Training loss 0.0073380013927817345 Validation loss 0.011048360727727413 Accuracy 0.88232421875\n",
      "Iteration 33320 Training loss 0.005542653147131205 Validation loss 0.011031048372387886 Accuracy 0.8818359375\n",
      "Iteration 33330 Training loss 0.006287296302616596 Validation loss 0.011154694482684135 Accuracy 0.88037109375\n",
      "Iteration 33340 Training loss 0.006405770778656006 Validation loss 0.011289018206298351 Accuracy 0.87841796875\n",
      "Iteration 33350 Training loss 0.007323319558054209 Validation loss 0.011119111441075802 Accuracy 0.8798828125\n",
      "Iteration 33360 Training loss 0.006967078894376755 Validation loss 0.011038427241146564 Accuracy 0.88134765625\n",
      "Iteration 33370 Training loss 0.005211099982261658 Validation loss 0.01075092051178217 Accuracy 0.8837890625\n",
      "Iteration 33380 Training loss 0.005780128296464682 Validation loss 0.01088304165750742 Accuracy 0.88232421875\n",
      "Iteration 33390 Training loss 0.005824553780257702 Validation loss 0.010912014171481133 Accuracy 0.88330078125\n",
      "Iteration 33400 Training loss 0.006119498051702976 Validation loss 0.010938097722828388 Accuracy 0.8828125\n",
      "Iteration 33410 Training loss 0.007802218198776245 Validation loss 0.010774060152471066 Accuracy 0.88427734375\n",
      "Iteration 33420 Training loss 0.0059356847777962685 Validation loss 0.010856494307518005 Accuracy 0.88330078125\n",
      "Iteration 33430 Training loss 0.0057453070767223835 Validation loss 0.01131505612283945 Accuracy 0.87890625\n",
      "Iteration 33440 Training loss 0.005020258482545614 Validation loss 0.010928869247436523 Accuracy 0.8828125\n",
      "Iteration 33450 Training loss 0.005837933626025915 Validation loss 0.010775679722428322 Accuracy 0.88427734375\n",
      "Iteration 33460 Training loss 0.005802838131785393 Validation loss 0.010901255533099174 Accuracy 0.88330078125\n",
      "Iteration 33470 Training loss 0.00612132390961051 Validation loss 0.01092152763158083 Accuracy 0.8818359375\n",
      "Iteration 33480 Training loss 0.005717901978641748 Validation loss 0.011285983957350254 Accuracy 0.8779296875\n",
      "Iteration 33490 Training loss 0.004768082872033119 Validation loss 0.010965441353619099 Accuracy 0.88232421875\n",
      "Iteration 33500 Training loss 0.004296719096601009 Validation loss 0.011000524275004864 Accuracy 0.8818359375\n",
      "Iteration 33510 Training loss 0.005506965797394514 Validation loss 0.01113161351531744 Accuracy 0.880859375\n",
      "Iteration 33520 Training loss 0.005960379261523485 Validation loss 0.010901781730353832 Accuracy 0.88232421875\n",
      "Iteration 33530 Training loss 0.007488192990422249 Validation loss 0.010827785357832909 Accuracy 0.88330078125\n",
      "Iteration 33540 Training loss 0.005928617902100086 Validation loss 0.011436927132308483 Accuracy 0.87744140625\n",
      "Iteration 33550 Training loss 0.005280590150505304 Validation loss 0.010887728072702885 Accuracy 0.8837890625\n",
      "Iteration 33560 Training loss 0.004193283151835203 Validation loss 0.010981066152453423 Accuracy 0.880859375\n",
      "Iteration 33570 Training loss 0.005796563811600208 Validation loss 0.010992366820573807 Accuracy 0.8818359375\n",
      "Iteration 33580 Training loss 0.006580467335879803 Validation loss 0.01116297859698534 Accuracy 0.87939453125\n",
      "Iteration 33590 Training loss 0.006861662957817316 Validation loss 0.010969250462949276 Accuracy 0.88330078125\n",
      "Iteration 33600 Training loss 0.006566582713276148 Validation loss 0.011250807903707027 Accuracy 0.87890625\n",
      "Iteration 33610 Training loss 0.004432052839547396 Validation loss 0.011583206243813038 Accuracy 0.876953125\n",
      "Iteration 33620 Training loss 0.006470756605267525 Validation loss 0.011051454581320286 Accuracy 0.88134765625\n",
      "Iteration 33630 Training loss 0.0067951856181025505 Validation loss 0.01123756542801857 Accuracy 0.8798828125\n",
      "Iteration 33640 Training loss 0.006978872697800398 Validation loss 0.011038332246243954 Accuracy 0.8828125\n",
      "Iteration 33650 Training loss 0.004315218422561884 Validation loss 0.01114495936781168 Accuracy 0.8798828125\n",
      "Iteration 33660 Training loss 0.005549706518650055 Validation loss 0.01091818604618311 Accuracy 0.8828125\n",
      "Iteration 33670 Training loss 0.006073350086808205 Validation loss 0.011184741742908955 Accuracy 0.8798828125\n",
      "Iteration 33680 Training loss 0.006844769697636366 Validation loss 0.01104544848203659 Accuracy 0.880859375\n",
      "Iteration 33690 Training loss 0.00712106516584754 Validation loss 0.011179455555975437 Accuracy 0.8798828125\n",
      "Iteration 33700 Training loss 0.007229246199131012 Validation loss 0.011143995448946953 Accuracy 0.8798828125\n",
      "Iteration 33710 Training loss 0.006155109498649836 Validation loss 0.010958385653793812 Accuracy 0.8828125\n",
      "Iteration 33720 Training loss 0.005599426105618477 Validation loss 0.01105405855923891 Accuracy 0.880859375\n",
      "Iteration 33730 Training loss 0.006134703289717436 Validation loss 0.010988683439791203 Accuracy 0.8818359375\n",
      "Iteration 33740 Training loss 0.00606908043846488 Validation loss 0.010978051461279392 Accuracy 0.88232421875\n",
      "Iteration 33750 Training loss 0.004830513149499893 Validation loss 0.011059827171266079 Accuracy 0.88037109375\n",
      "Iteration 33760 Training loss 0.006892392411828041 Validation loss 0.011157114058732986 Accuracy 0.880859375\n",
      "Iteration 33770 Training loss 0.004785783123224974 Validation loss 0.011249958537518978 Accuracy 0.87890625\n",
      "Iteration 33780 Training loss 0.00663499953225255 Validation loss 0.010873151943087578 Accuracy 0.88330078125\n",
      "Iteration 33790 Training loss 0.00582454539835453 Validation loss 0.0111943194642663 Accuracy 0.880859375\n",
      "Iteration 33800 Training loss 0.006190702319145203 Validation loss 0.011300819925963879 Accuracy 0.8798828125\n",
      "Iteration 33810 Training loss 0.00455809012055397 Validation loss 0.011085867881774902 Accuracy 0.88232421875\n",
      "Iteration 33820 Training loss 0.005719364155083895 Validation loss 0.011287249624729156 Accuracy 0.87841796875\n",
      "Iteration 33830 Training loss 0.0051079364493489265 Validation loss 0.011015997268259525 Accuracy 0.8818359375\n",
      "Iteration 33840 Training loss 0.005102388095110655 Validation loss 0.010939457453787327 Accuracy 0.88232421875\n",
      "Iteration 33850 Training loss 0.004392495844513178 Validation loss 0.010962515138089657 Accuracy 0.88134765625\n",
      "Iteration 33860 Training loss 0.0059807333163917065 Validation loss 0.011052832007408142 Accuracy 0.88232421875\n",
      "Iteration 33870 Training loss 0.0057504442520439625 Validation loss 0.010839725844562054 Accuracy 0.8837890625\n",
      "Iteration 33880 Training loss 0.006402602884918451 Validation loss 0.010841064155101776 Accuracy 0.8837890625\n",
      "Iteration 33890 Training loss 0.004303592722862959 Validation loss 0.010955160483717918 Accuracy 0.8818359375\n",
      "Iteration 33900 Training loss 0.004838085267692804 Validation loss 0.011349216103553772 Accuracy 0.87841796875\n",
      "Iteration 33910 Training loss 0.004993305075913668 Validation loss 0.01114443689584732 Accuracy 0.88037109375\n",
      "Iteration 33920 Training loss 0.005978633649647236 Validation loss 0.01123394537717104 Accuracy 0.87890625\n",
      "Iteration 33930 Training loss 0.004312118981033564 Validation loss 0.010958344675600529 Accuracy 0.8818359375\n",
      "Iteration 33940 Training loss 0.006030716467648745 Validation loss 0.01082172617316246 Accuracy 0.88427734375\n",
      "Iteration 33950 Training loss 0.005505686160176992 Validation loss 0.010924316011369228 Accuracy 0.88232421875\n",
      "Iteration 33960 Training loss 0.004142163787037134 Validation loss 0.010935748927295208 Accuracy 0.88232421875\n",
      "Iteration 33970 Training loss 0.00721654063090682 Validation loss 0.010795644484460354 Accuracy 0.88330078125\n",
      "Iteration 33980 Training loss 0.005538804456591606 Validation loss 0.010769921354949474 Accuracy 0.88427734375\n",
      "Iteration 33990 Training loss 0.006359579507261515 Validation loss 0.010929387994110584 Accuracy 0.88232421875\n",
      "Iteration 34000 Training loss 0.005722562316805124 Validation loss 0.010829162783920765 Accuracy 0.88330078125\n",
      "Iteration 34010 Training loss 0.0048515452072024345 Validation loss 0.010836116969585419 Accuracy 0.8837890625\n",
      "Iteration 34020 Training loss 0.00465680006891489 Validation loss 0.010944492183625698 Accuracy 0.8828125\n",
      "Iteration 34030 Training loss 0.006793934851884842 Validation loss 0.01108317356556654 Accuracy 0.88037109375\n",
      "Iteration 34040 Training loss 0.006810344755649567 Validation loss 0.010827918536961079 Accuracy 0.8837890625\n",
      "Iteration 34050 Training loss 0.005022579804062843 Validation loss 0.010926448740065098 Accuracy 0.88232421875\n",
      "Iteration 34060 Training loss 0.006211545784026384 Validation loss 0.010829521343111992 Accuracy 0.88330078125\n",
      "Iteration 34070 Training loss 0.005972073879092932 Validation loss 0.010872323997318745 Accuracy 0.8837890625\n",
      "Iteration 34080 Training loss 0.005382315721362829 Validation loss 0.01111932098865509 Accuracy 0.8798828125\n",
      "Iteration 34090 Training loss 0.00602605938911438 Validation loss 0.010975714772939682 Accuracy 0.8828125\n",
      "Iteration 34100 Training loss 0.0060216281563043594 Validation loss 0.011162346228957176 Accuracy 0.8798828125\n",
      "Iteration 34110 Training loss 0.006470658350735903 Validation loss 0.011048117652535439 Accuracy 0.88037109375\n",
      "Iteration 34120 Training loss 0.0040854839608073235 Validation loss 0.011071178130805492 Accuracy 0.880859375\n",
      "Iteration 34130 Training loss 0.0060341800563037395 Validation loss 0.010986999608576298 Accuracy 0.8828125\n",
      "Iteration 34140 Training loss 0.004287175368517637 Validation loss 0.011313792318105698 Accuracy 0.87841796875\n",
      "Iteration 34150 Training loss 0.005577666684985161 Validation loss 0.011018261313438416 Accuracy 0.88134765625\n",
      "Iteration 34160 Training loss 0.0070123011246323586 Validation loss 0.011951636523008347 Accuracy 0.87060546875\n",
      "Iteration 34170 Training loss 0.006253096275031567 Validation loss 0.011171537451446056 Accuracy 0.88037109375\n",
      "Iteration 34180 Training loss 0.00459196325391531 Validation loss 0.011082375422120094 Accuracy 0.88134765625\n",
      "Iteration 34190 Training loss 0.004986189771443605 Validation loss 0.010937599465250969 Accuracy 0.88232421875\n",
      "Iteration 34200 Training loss 0.005840151570737362 Validation loss 0.011093980632722378 Accuracy 0.880859375\n",
      "Iteration 34210 Training loss 0.0058689420111477375 Validation loss 0.010972518473863602 Accuracy 0.88232421875\n",
      "Iteration 34220 Training loss 0.00527660408988595 Validation loss 0.011029912158846855 Accuracy 0.88134765625\n",
      "Iteration 34230 Training loss 0.00807625986635685 Validation loss 0.011048000305891037 Accuracy 0.88037109375\n",
      "Iteration 34240 Training loss 0.00542535400018096 Validation loss 0.010973246768116951 Accuracy 0.8828125\n",
      "Iteration 34250 Training loss 0.00588562386110425 Validation loss 0.0112401582300663 Accuracy 0.87939453125\n",
      "Iteration 34260 Training loss 0.005355673376470804 Validation loss 0.010888571850955486 Accuracy 0.8828125\n",
      "Iteration 34270 Training loss 0.00532931974157691 Validation loss 0.011016584001481533 Accuracy 0.8818359375\n",
      "Iteration 34280 Training loss 0.005698012188076973 Validation loss 0.010972999967634678 Accuracy 0.880859375\n",
      "Iteration 34290 Training loss 0.005606156773865223 Validation loss 0.010789378546178341 Accuracy 0.88330078125\n",
      "Iteration 34300 Training loss 0.005667287856340408 Validation loss 0.01105121523141861 Accuracy 0.87939453125\n",
      "Iteration 34310 Training loss 0.003926274366676807 Validation loss 0.010911847464740276 Accuracy 0.880859375\n",
      "Iteration 34320 Training loss 0.006159855984151363 Validation loss 0.011012299917638302 Accuracy 0.88037109375\n",
      "Iteration 34330 Training loss 0.004402365535497665 Validation loss 0.011220863088965416 Accuracy 0.87890625\n",
      "Iteration 34340 Training loss 0.0046073393896222115 Validation loss 0.01093856617808342 Accuracy 0.88330078125\n",
      "Iteration 34350 Training loss 0.004468570463359356 Validation loss 0.010810734704136848 Accuracy 0.88427734375\n",
      "Iteration 34360 Training loss 0.006807491648942232 Validation loss 0.010919311083853245 Accuracy 0.8828125\n",
      "Iteration 34370 Training loss 0.006602931302040815 Validation loss 0.011104816570878029 Accuracy 0.880859375\n",
      "Iteration 34380 Training loss 0.0056478409096598625 Validation loss 0.010856575332581997 Accuracy 0.88330078125\n",
      "Iteration 34390 Training loss 0.005555256269872189 Validation loss 0.010997463017702103 Accuracy 0.880859375\n",
      "Iteration 34400 Training loss 0.005151299759745598 Validation loss 0.010972032323479652 Accuracy 0.8828125\n",
      "Iteration 34410 Training loss 0.005362080875784159 Validation loss 0.010917577892541885 Accuracy 0.88232421875\n",
      "Iteration 34420 Training loss 0.004104066640138626 Validation loss 0.011066102422773838 Accuracy 0.880859375\n",
      "Iteration 34430 Training loss 0.007463877089321613 Validation loss 0.01104540005326271 Accuracy 0.88134765625\n",
      "Iteration 34440 Training loss 0.004741338547319174 Validation loss 0.010992560535669327 Accuracy 0.88134765625\n",
      "Iteration 34450 Training loss 0.004252203740179539 Validation loss 0.010787230916321278 Accuracy 0.88427734375\n",
      "Iteration 34460 Training loss 0.005914921872317791 Validation loss 0.011051627807319164 Accuracy 0.8818359375\n",
      "Iteration 34470 Training loss 0.004531474318355322 Validation loss 0.010830232873558998 Accuracy 0.8828125\n",
      "Iteration 34480 Training loss 0.006288690026849508 Validation loss 0.01083419006317854 Accuracy 0.8818359375\n",
      "Iteration 34490 Training loss 0.004698470234870911 Validation loss 0.010797225870192051 Accuracy 0.8828125\n",
      "Iteration 34500 Training loss 0.004417551215738058 Validation loss 0.011183192022144794 Accuracy 0.87744140625\n",
      "Iteration 34510 Training loss 0.006749390624463558 Validation loss 0.010920959524810314 Accuracy 0.8828125\n",
      "Iteration 34520 Training loss 0.005915471818298101 Validation loss 0.010909682139754295 Accuracy 0.88134765625\n",
      "Iteration 34530 Training loss 0.006723895203322172 Validation loss 0.010934511199593544 Accuracy 0.8818359375\n",
      "Iteration 34540 Training loss 0.003299433272331953 Validation loss 0.01083269901573658 Accuracy 0.8828125\n",
      "Iteration 34550 Training loss 0.005609133280813694 Validation loss 0.010865574702620506 Accuracy 0.88232421875\n",
      "Iteration 34560 Training loss 0.003516178345307708 Validation loss 0.01119984220713377 Accuracy 0.87939453125\n",
      "Iteration 34570 Training loss 0.004940441343933344 Validation loss 0.010918752290308475 Accuracy 0.8828125\n",
      "Iteration 34580 Training loss 0.007980382069945335 Validation loss 0.010805168189108372 Accuracy 0.88330078125\n",
      "Iteration 34590 Training loss 0.0065994784235954285 Validation loss 0.01072217058390379 Accuracy 0.88525390625\n",
      "Iteration 34600 Training loss 0.006521256640553474 Validation loss 0.010869338177144527 Accuracy 0.88330078125\n",
      "Iteration 34610 Training loss 0.004015996120870113 Validation loss 0.010820262134075165 Accuracy 0.88232421875\n",
      "Iteration 34620 Training loss 0.004648504313081503 Validation loss 0.01074572280049324 Accuracy 0.8837890625\n",
      "Iteration 34630 Training loss 0.004179863259196281 Validation loss 0.010711297392845154 Accuracy 0.88427734375\n",
      "Iteration 34640 Training loss 0.005839542485773563 Validation loss 0.011082750745117664 Accuracy 0.88037109375\n",
      "Iteration 34650 Training loss 0.004338866099715233 Validation loss 0.010968619957566261 Accuracy 0.880859375\n",
      "Iteration 34660 Training loss 0.0071226246654987335 Validation loss 0.010801498778164387 Accuracy 0.8837890625\n",
      "Iteration 34670 Training loss 0.0044723860919475555 Validation loss 0.010961068794131279 Accuracy 0.88134765625\n",
      "Iteration 34680 Training loss 0.005905398167669773 Validation loss 0.010994933545589447 Accuracy 0.8818359375\n",
      "Iteration 34690 Training loss 0.0063529242761433125 Validation loss 0.011382248252630234 Accuracy 0.8759765625\n",
      "Iteration 34700 Training loss 0.006098308600485325 Validation loss 0.011053104884922504 Accuracy 0.8798828125\n",
      "Iteration 34710 Training loss 0.003992570098489523 Validation loss 0.010926083661615849 Accuracy 0.88232421875\n",
      "Iteration 34720 Training loss 0.003358581569045782 Validation loss 0.01101362518966198 Accuracy 0.8818359375\n",
      "Iteration 34730 Training loss 0.005743080284446478 Validation loss 0.010974595323204994 Accuracy 0.88134765625\n",
      "Iteration 34740 Training loss 0.00538433063775301 Validation loss 0.010808604769408703 Accuracy 0.8837890625\n",
      "Iteration 34750 Training loss 0.005105247255414724 Validation loss 0.011028621345758438 Accuracy 0.88037109375\n",
      "Iteration 34760 Training loss 0.004859441891312599 Validation loss 0.01083703339099884 Accuracy 0.8837890625\n",
      "Iteration 34770 Training loss 0.005262073595076799 Validation loss 0.010890286415815353 Accuracy 0.8837890625\n",
      "Iteration 34780 Training loss 0.004411154426634312 Validation loss 0.010685371235013008 Accuracy 0.8857421875\n",
      "Iteration 34790 Training loss 0.004548525437712669 Validation loss 0.010828735306859016 Accuracy 0.88330078125\n",
      "Iteration 34800 Training loss 0.0054564387537539005 Validation loss 0.010967806912958622 Accuracy 0.88134765625\n",
      "Iteration 34810 Training loss 0.004588638432323933 Validation loss 0.011056018061935902 Accuracy 0.8798828125\n",
      "Iteration 34820 Training loss 0.004419330041855574 Validation loss 0.010957158170640469 Accuracy 0.880859375\n",
      "Iteration 34830 Training loss 0.0055383541621267796 Validation loss 0.011032202281057835 Accuracy 0.880859375\n",
      "Iteration 34840 Training loss 0.005139368586242199 Validation loss 0.011265594512224197 Accuracy 0.87841796875\n",
      "Iteration 34850 Training loss 0.004583647940307856 Validation loss 0.010996355675160885 Accuracy 0.88134765625\n",
      "Iteration 34860 Training loss 0.005780237726867199 Validation loss 0.010877128690481186 Accuracy 0.88232421875\n",
      "Iteration 34870 Training loss 0.004245823249220848 Validation loss 0.011031077243387699 Accuracy 0.8818359375\n",
      "Iteration 34880 Training loss 0.006815675180405378 Validation loss 0.01105334423482418 Accuracy 0.8818359375\n",
      "Iteration 34890 Training loss 0.00622912123799324 Validation loss 0.01100845169275999 Accuracy 0.88134765625\n",
      "Iteration 34900 Training loss 0.006706284824758768 Validation loss 0.011342705227434635 Accuracy 0.8779296875\n",
      "Iteration 34910 Training loss 0.004325876012444496 Validation loss 0.010934623889625072 Accuracy 0.88232421875\n",
      "Iteration 34920 Training loss 0.0038169606123119593 Validation loss 0.010972750373184681 Accuracy 0.88134765625\n",
      "Iteration 34930 Training loss 0.0062723723240196705 Validation loss 0.010924892500042915 Accuracy 0.8818359375\n",
      "Iteration 34940 Training loss 0.0054296148009598255 Validation loss 0.010823840275406837 Accuracy 0.8837890625\n",
      "Iteration 34950 Training loss 0.005869617220014334 Validation loss 0.010855253785848618 Accuracy 0.8818359375\n",
      "Iteration 34960 Training loss 0.0047951252199709415 Validation loss 0.01088684517890215 Accuracy 0.8818359375\n",
      "Iteration 34970 Training loss 0.0030651926063001156 Validation loss 0.010892164893448353 Accuracy 0.88330078125\n",
      "Iteration 34980 Training loss 0.004699092358350754 Validation loss 0.010928686708211899 Accuracy 0.8818359375\n",
      "Iteration 34990 Training loss 0.004958795383572578 Validation loss 0.01090475358068943 Accuracy 0.8818359375\n",
      "Iteration 35000 Training loss 0.00519155990332365 Validation loss 0.010834251530468464 Accuracy 0.88232421875\n",
      "Iteration 35010 Training loss 0.006735845003277063 Validation loss 0.010868352837860584 Accuracy 0.88330078125\n",
      "Iteration 35020 Training loss 0.004224308300763369 Validation loss 0.010854572057723999 Accuracy 0.8828125\n",
      "Iteration 35030 Training loss 0.0055429949425160885 Validation loss 0.010762793011963367 Accuracy 0.8837890625\n",
      "Iteration 35040 Training loss 0.006302955560386181 Validation loss 0.011070231907069683 Accuracy 0.880859375\n",
      "Iteration 35050 Training loss 0.003304277313873172 Validation loss 0.011072342284023762 Accuracy 0.87939453125\n",
      "Iteration 35060 Training loss 0.006071267183870077 Validation loss 0.011230932548642159 Accuracy 0.87890625\n",
      "Iteration 35070 Training loss 0.006204727105796337 Validation loss 0.011499540880322456 Accuracy 0.87646484375\n",
      "Iteration 35080 Training loss 0.005700927693396807 Validation loss 0.011285514570772648 Accuracy 0.87890625\n",
      "Iteration 35090 Training loss 0.006636656355112791 Validation loss 0.011445476673543453 Accuracy 0.87744140625\n",
      "Iteration 35100 Training loss 0.0046415189281105995 Validation loss 0.011027316562831402 Accuracy 0.88037109375\n",
      "Iteration 35110 Training loss 0.006201225332915783 Validation loss 0.011336027644574642 Accuracy 0.8779296875\n",
      "Iteration 35120 Training loss 0.004883858375251293 Validation loss 0.011107360944151878 Accuracy 0.88037109375\n",
      "Iteration 35130 Training loss 0.006433684378862381 Validation loss 0.01083855889737606 Accuracy 0.8828125\n",
      "Iteration 35140 Training loss 0.005523634608834982 Validation loss 0.010827962309122086 Accuracy 0.88232421875\n",
      "Iteration 35150 Training loss 0.0042731924913823605 Validation loss 0.011086725629866123 Accuracy 0.88037109375\n",
      "Iteration 35160 Training loss 0.0034341749269515276 Validation loss 0.010942242108285427 Accuracy 0.880859375\n",
      "Iteration 35170 Training loss 0.004861041903495789 Validation loss 0.010814877226948738 Accuracy 0.8828125\n",
      "Iteration 35180 Training loss 0.005366425961256027 Validation loss 0.010726457461714745 Accuracy 0.88330078125\n",
      "Iteration 35190 Training loss 0.0056046550162136555 Validation loss 0.010879888199269772 Accuracy 0.8818359375\n",
      "Iteration 35200 Training loss 0.0037493056152015924 Validation loss 0.011113828979432583 Accuracy 0.8798828125\n",
      "Iteration 35210 Training loss 0.004489384591579437 Validation loss 0.011081518605351448 Accuracy 0.8798828125\n",
      "Iteration 35220 Training loss 0.006140491459518671 Validation loss 0.011197546496987343 Accuracy 0.87939453125\n",
      "Iteration 35230 Training loss 0.005067737773060799 Validation loss 0.010835234075784683 Accuracy 0.88427734375\n",
      "Iteration 35240 Training loss 0.0049558901228010654 Validation loss 0.011071108281612396 Accuracy 0.8818359375\n",
      "Iteration 35250 Training loss 0.005846313200891018 Validation loss 0.010794658213853836 Accuracy 0.88330078125\n",
      "Iteration 35260 Training loss 0.004254875238984823 Validation loss 0.010799848474562168 Accuracy 0.8837890625\n",
      "Iteration 35270 Training loss 0.006077152211219072 Validation loss 0.010798335075378418 Accuracy 0.88330078125\n",
      "Iteration 35280 Training loss 0.004705902189016342 Validation loss 0.010745488107204437 Accuracy 0.88427734375\n",
      "Iteration 35290 Training loss 0.005937976762652397 Validation loss 0.011061571538448334 Accuracy 0.880859375\n",
      "Iteration 35300 Training loss 0.00657470291480422 Validation loss 0.010594943538308144 Accuracy 0.88623046875\n",
      "Iteration 35310 Training loss 0.005986874923110008 Validation loss 0.011106547899544239 Accuracy 0.8798828125\n",
      "Iteration 35320 Training loss 0.005009589251130819 Validation loss 0.011112352833151817 Accuracy 0.880859375\n",
      "Iteration 35330 Training loss 0.004723375663161278 Validation loss 0.010780438780784607 Accuracy 0.8837890625\n",
      "Iteration 35340 Training loss 0.005400091875344515 Validation loss 0.011069610714912415 Accuracy 0.88134765625\n",
      "Iteration 35350 Training loss 0.004953887313604355 Validation loss 0.010928342118859291 Accuracy 0.88232421875\n",
      "Iteration 35360 Training loss 0.005680931732058525 Validation loss 0.010912184603512287 Accuracy 0.88232421875\n",
      "Iteration 35370 Training loss 0.004883440677076578 Validation loss 0.01092181634157896 Accuracy 0.8818359375\n",
      "Iteration 35380 Training loss 0.007240580394864082 Validation loss 0.010825463570654392 Accuracy 0.8837890625\n",
      "Iteration 35390 Training loss 0.004867620300501585 Validation loss 0.011463964357972145 Accuracy 0.875\n",
      "Iteration 35400 Training loss 0.004835929721593857 Validation loss 0.010911352001130581 Accuracy 0.880859375\n",
      "Iteration 35410 Training loss 0.004081966821104288 Validation loss 0.010978939011693 Accuracy 0.880859375\n",
      "Iteration 35420 Training loss 0.005300124641507864 Validation loss 0.01083411369472742 Accuracy 0.8818359375\n",
      "Iteration 35430 Training loss 0.0038322906475514174 Validation loss 0.010652351193130016 Accuracy 0.8857421875\n",
      "Iteration 35440 Training loss 0.005280085373669863 Validation loss 0.010802894830703735 Accuracy 0.88427734375\n",
      "Iteration 35450 Training loss 0.0037496413569897413 Validation loss 0.010588661767542362 Accuracy 0.884765625\n",
      "Iteration 35460 Training loss 0.006116874516010284 Validation loss 0.010865358635783195 Accuracy 0.88330078125\n",
      "Iteration 35470 Training loss 0.004253764636814594 Validation loss 0.010991860181093216 Accuracy 0.88134765625\n",
      "Iteration 35480 Training loss 0.00470390822738409 Validation loss 0.010735457763075829 Accuracy 0.8837890625\n",
      "Iteration 35490 Training loss 0.005346979945898056 Validation loss 0.01083021704107523 Accuracy 0.88232421875\n",
      "Iteration 35500 Training loss 0.004895744379609823 Validation loss 0.011170086450874805 Accuracy 0.8798828125\n",
      "Iteration 35510 Training loss 0.006203779019415379 Validation loss 0.010875019244849682 Accuracy 0.8818359375\n",
      "Iteration 35520 Training loss 0.0061493185348808765 Validation loss 0.011138500645756721 Accuracy 0.87841796875\n",
      "Iteration 35530 Training loss 0.004750189837068319 Validation loss 0.010793071240186691 Accuracy 0.8837890625\n",
      "Iteration 35540 Training loss 0.004543940536677837 Validation loss 0.01088273897767067 Accuracy 0.88134765625\n",
      "Iteration 35550 Training loss 0.005053723230957985 Validation loss 0.011477995663881302 Accuracy 0.87646484375\n",
      "Iteration 35560 Training loss 0.004601132590323687 Validation loss 0.010774543508887291 Accuracy 0.8828125\n",
      "Iteration 35570 Training loss 0.00643064733594656 Validation loss 0.01096187811344862 Accuracy 0.88232421875\n",
      "Iteration 35580 Training loss 0.004366545006632805 Validation loss 0.010923655703663826 Accuracy 0.880859375\n",
      "Iteration 35590 Training loss 0.0051330444402992725 Validation loss 0.011035649105906487 Accuracy 0.88037109375\n",
      "Iteration 35600 Training loss 0.004819896537810564 Validation loss 0.01093838270753622 Accuracy 0.88232421875\n",
      "Iteration 35610 Training loss 0.005839507561177015 Validation loss 0.010875768959522247 Accuracy 0.88134765625\n",
      "Iteration 35620 Training loss 0.005152310710400343 Validation loss 0.011047855019569397 Accuracy 0.88134765625\n",
      "Iteration 35630 Training loss 0.005360251758247614 Validation loss 0.011012339033186436 Accuracy 0.88037109375\n",
      "Iteration 35640 Training loss 0.004584809765219688 Validation loss 0.010750905610620975 Accuracy 0.88330078125\n",
      "Iteration 35650 Training loss 0.0057207196950912476 Validation loss 0.010808859951794147 Accuracy 0.8837890625\n",
      "Iteration 35660 Training loss 0.003990792669355869 Validation loss 0.010784411802887917 Accuracy 0.88427734375\n",
      "Iteration 35670 Training loss 0.005740655120462179 Validation loss 0.01078211423009634 Accuracy 0.8837890625\n",
      "Iteration 35680 Training loss 0.006177407689392567 Validation loss 0.011951511725783348 Accuracy 0.87109375\n",
      "Iteration 35690 Training loss 0.005885935854166746 Validation loss 0.010795197449624538 Accuracy 0.8818359375\n",
      "Iteration 35700 Training loss 0.006673585157841444 Validation loss 0.010806743055582047 Accuracy 0.88330078125\n",
      "Iteration 35710 Training loss 0.005049930419772863 Validation loss 0.010977272875607014 Accuracy 0.88134765625\n",
      "Iteration 35720 Training loss 0.005437382962554693 Validation loss 0.010799963027238846 Accuracy 0.8837890625\n",
      "Iteration 35730 Training loss 0.0052141123451292515 Validation loss 0.010848012752830982 Accuracy 0.88232421875\n",
      "Iteration 35740 Training loss 0.0056800199672579765 Validation loss 0.010769102722406387 Accuracy 0.88232421875\n",
      "Iteration 35750 Training loss 0.004761065822094679 Validation loss 0.011148226447403431 Accuracy 0.8779296875\n",
      "Iteration 35760 Training loss 0.006096808239817619 Validation loss 0.010949444025754929 Accuracy 0.88134765625\n",
      "Iteration 35770 Training loss 0.003997046034783125 Validation loss 0.010946495458483696 Accuracy 0.880859375\n",
      "Iteration 35780 Training loss 0.005448920652270317 Validation loss 0.010644368827342987 Accuracy 0.88330078125\n",
      "Iteration 35790 Training loss 0.0045711263082921505 Validation loss 0.010790358297526836 Accuracy 0.88330078125\n",
      "Iteration 35800 Training loss 0.005752850789576769 Validation loss 0.010988187976181507 Accuracy 0.880859375\n",
      "Iteration 35810 Training loss 0.005303709767758846 Validation loss 0.010907277464866638 Accuracy 0.8818359375\n",
      "Iteration 35820 Training loss 0.004968616645783186 Validation loss 0.01083427481353283 Accuracy 0.8828125\n",
      "Iteration 35830 Training loss 0.005752285476773977 Validation loss 0.01083788089454174 Accuracy 0.88330078125\n",
      "Iteration 35840 Training loss 0.004760229494422674 Validation loss 0.010910777375102043 Accuracy 0.88134765625\n",
      "Iteration 35850 Training loss 0.00772061524912715 Validation loss 0.010641884058713913 Accuracy 0.884765625\n",
      "Iteration 35860 Training loss 0.005228391848504543 Validation loss 0.010797464288771152 Accuracy 0.8837890625\n",
      "Iteration 35870 Training loss 0.004252160433679819 Validation loss 0.01068130787461996 Accuracy 0.884765625\n",
      "Iteration 35880 Training loss 0.0059740180149674416 Validation loss 0.010666809044778347 Accuracy 0.88427734375\n",
      "Iteration 35890 Training loss 0.004144899547100067 Validation loss 0.010564169846475124 Accuracy 0.8857421875\n",
      "Iteration 35900 Training loss 0.006232064682990313 Validation loss 0.010770800523459911 Accuracy 0.8828125\n",
      "Iteration 35910 Training loss 0.004301398526877165 Validation loss 0.010813457891345024 Accuracy 0.88134765625\n",
      "Iteration 35920 Training loss 0.0038552379701286554 Validation loss 0.010642302222549915 Accuracy 0.884765625\n",
      "Iteration 35930 Training loss 0.00458744028583169 Validation loss 0.010631794109940529 Accuracy 0.884765625\n",
      "Iteration 35940 Training loss 0.0062392037361860275 Validation loss 0.01090913824737072 Accuracy 0.8818359375\n",
      "Iteration 35950 Training loss 0.00383047410286963 Validation loss 0.010715191252529621 Accuracy 0.8837890625\n",
      "Iteration 35960 Training loss 0.006465407554060221 Validation loss 0.010877094231545925 Accuracy 0.8828125\n",
      "Iteration 35970 Training loss 0.00564891891553998 Validation loss 0.011072483845055103 Accuracy 0.8798828125\n",
      "Iteration 35980 Training loss 0.003361091483384371 Validation loss 0.010834337212145329 Accuracy 0.880859375\n",
      "Iteration 35990 Training loss 0.006348785944283009 Validation loss 0.010839025489985943 Accuracy 0.88232421875\n",
      "Iteration 36000 Training loss 0.005995627027004957 Validation loss 0.010874936357140541 Accuracy 0.8818359375\n",
      "Iteration 36010 Training loss 0.005383538547903299 Validation loss 0.010996831580996513 Accuracy 0.88232421875\n",
      "Iteration 36020 Training loss 0.0053169820457696915 Validation loss 0.011238012462854385 Accuracy 0.87744140625\n",
      "Iteration 36030 Training loss 0.0032605361193418503 Validation loss 0.010927785187959671 Accuracy 0.8818359375\n",
      "Iteration 36040 Training loss 0.0056067281402647495 Validation loss 0.010938223451375961 Accuracy 0.8818359375\n",
      "Iteration 36050 Training loss 0.004662691615521908 Validation loss 0.01091537345200777 Accuracy 0.8828125\n",
      "Iteration 36060 Training loss 0.005585019942373037 Validation loss 0.011003666557371616 Accuracy 0.87939453125\n",
      "Iteration 36070 Training loss 0.004707623273134232 Validation loss 0.010671430267393589 Accuracy 0.88427734375\n",
      "Iteration 36080 Training loss 0.004861899651587009 Validation loss 0.010883747600018978 Accuracy 0.8818359375\n",
      "Iteration 36090 Training loss 0.00564256776124239 Validation loss 0.011169124394655228 Accuracy 0.8779296875\n",
      "Iteration 36100 Training loss 0.005960085429251194 Validation loss 0.01086638867855072 Accuracy 0.8828125\n",
      "Iteration 36110 Training loss 0.003861290169879794 Validation loss 0.010836227796971798 Accuracy 0.88427734375\n",
      "Iteration 36120 Training loss 0.0040652574971318245 Validation loss 0.011114201508462429 Accuracy 0.88037109375\n",
      "Iteration 36130 Training loss 0.005039196461439133 Validation loss 0.010774984024465084 Accuracy 0.8828125\n",
      "Iteration 36140 Training loss 0.004622018896043301 Validation loss 0.010964496992528439 Accuracy 0.8818359375\n",
      "Iteration 36150 Training loss 0.005092433653771877 Validation loss 0.010886292904615402 Accuracy 0.88134765625\n",
      "Iteration 36160 Training loss 0.004654785618185997 Validation loss 0.010805336758494377 Accuracy 0.88330078125\n",
      "Iteration 36170 Training loss 0.0053254528902471066 Validation loss 0.010662629269063473 Accuracy 0.88427734375\n",
      "Iteration 36180 Training loss 0.003603694960474968 Validation loss 0.010968564078211784 Accuracy 0.88037109375\n",
      "Iteration 36190 Training loss 0.004834537860006094 Validation loss 0.011046087369322777 Accuracy 0.8798828125\n",
      "Iteration 36200 Training loss 0.00585589837282896 Validation loss 0.010710522532463074 Accuracy 0.8837890625\n",
      "Iteration 36210 Training loss 0.004739528056234121 Validation loss 0.010940218344330788 Accuracy 0.880859375\n",
      "Iteration 36220 Training loss 0.004879385698586702 Validation loss 0.010904491879045963 Accuracy 0.8818359375\n",
      "Iteration 36230 Training loss 0.0047143809497356415 Validation loss 0.010868502780795097 Accuracy 0.8818359375\n",
      "Iteration 36240 Training loss 0.005544369108974934 Validation loss 0.010674665682017803 Accuracy 0.8837890625\n",
      "Iteration 36250 Training loss 0.003581523895263672 Validation loss 0.01079568825662136 Accuracy 0.88232421875\n",
      "Iteration 36260 Training loss 0.003646247088909149 Validation loss 0.010779027827084064 Accuracy 0.8828125\n",
      "Iteration 36270 Training loss 0.004750711843371391 Validation loss 0.010829677805304527 Accuracy 0.8828125\n",
      "Iteration 36280 Training loss 0.005469002760946751 Validation loss 0.0108182979747653 Accuracy 0.88134765625\n",
      "Iteration 36290 Training loss 0.006516037043184042 Validation loss 0.01095650251954794 Accuracy 0.8818359375\n",
      "Iteration 36300 Training loss 0.004985181149095297 Validation loss 0.01096991915255785 Accuracy 0.88134765625\n",
      "Iteration 36310 Training loss 0.004918087273836136 Validation loss 0.010839167051017284 Accuracy 0.8818359375\n",
      "Iteration 36320 Training loss 0.004610057920217514 Validation loss 0.011030277237296104 Accuracy 0.88037109375\n",
      "Iteration 36330 Training loss 0.0032827917020767927 Validation loss 0.01101410947740078 Accuracy 0.880859375\n",
      "Iteration 36340 Training loss 0.0054498156532645226 Validation loss 0.010872368700802326 Accuracy 0.88232421875\n",
      "Iteration 36350 Training loss 0.004585835617035627 Validation loss 0.010940461419522762 Accuracy 0.880859375\n",
      "Iteration 36360 Training loss 0.005370569881051779 Validation loss 0.011098495684564114 Accuracy 0.8798828125\n",
      "Iteration 36370 Training loss 0.004425396677106619 Validation loss 0.010804529301822186 Accuracy 0.88330078125\n",
      "Iteration 36380 Training loss 0.00562274968251586 Validation loss 0.010928268544375896 Accuracy 0.8828125\n",
      "Iteration 36390 Training loss 0.006233348045498133 Validation loss 0.010884919203817844 Accuracy 0.8818359375\n",
      "Iteration 36400 Training loss 0.005513042211532593 Validation loss 0.011004057712852955 Accuracy 0.880859375\n",
      "Iteration 36410 Training loss 0.005056761205196381 Validation loss 0.011001760140061378 Accuracy 0.88134765625\n",
      "Iteration 36420 Training loss 0.004503434989601374 Validation loss 0.010789305903017521 Accuracy 0.88330078125\n",
      "Iteration 36430 Training loss 0.0050795963034033775 Validation loss 0.01075409259647131 Accuracy 0.88427734375\n",
      "Iteration 36440 Training loss 0.004884085152298212 Validation loss 0.010737533681094646 Accuracy 0.8837890625\n",
      "Iteration 36450 Training loss 0.004805978387594223 Validation loss 0.010598366148769855 Accuracy 0.88525390625\n",
      "Iteration 36460 Training loss 0.0038620145060122013 Validation loss 0.010771154426038265 Accuracy 0.8837890625\n",
      "Iteration 36470 Training loss 0.0033863617572933435 Validation loss 0.010791691951453686 Accuracy 0.8828125\n",
      "Iteration 36480 Training loss 0.0049030533991754055 Validation loss 0.010758653283119202 Accuracy 0.8837890625\n",
      "Iteration 36490 Training loss 0.0042475322261452675 Validation loss 0.010735563933849335 Accuracy 0.8837890625\n",
      "Iteration 36500 Training loss 0.005310171749442816 Validation loss 0.011060836724936962 Accuracy 0.8798828125\n",
      "Iteration 36510 Training loss 0.004455948248505592 Validation loss 0.010873142629861832 Accuracy 0.8818359375\n",
      "Iteration 36520 Training loss 0.006276762578636408 Validation loss 0.010712669230997562 Accuracy 0.88330078125\n",
      "Iteration 36530 Training loss 0.004824587609618902 Validation loss 0.010763249360024929 Accuracy 0.8828125\n",
      "Iteration 36540 Training loss 0.004974180366843939 Validation loss 0.010823950171470642 Accuracy 0.88232421875\n",
      "Iteration 36550 Training loss 0.003612320637330413 Validation loss 0.010916030034422874 Accuracy 0.88037109375\n",
      "Iteration 36560 Training loss 0.004867083393037319 Validation loss 0.01076497696340084 Accuracy 0.88330078125\n",
      "Iteration 36570 Training loss 0.0047842999920248985 Validation loss 0.01084841787815094 Accuracy 0.88232421875\n",
      "Iteration 36580 Training loss 0.0039294688031077385 Validation loss 0.010722643695771694 Accuracy 0.8837890625\n",
      "Iteration 36590 Training loss 0.006666115950793028 Validation loss 0.010745341889560223 Accuracy 0.88330078125\n",
      "Iteration 36600 Training loss 0.005757182836532593 Validation loss 0.010785261169075966 Accuracy 0.8818359375\n",
      "Iteration 36610 Training loss 0.0039022034034132957 Validation loss 0.010789182037115097 Accuracy 0.8828125\n",
      "Iteration 36620 Training loss 0.0057323467917740345 Validation loss 0.010729282163083553 Accuracy 0.8828125\n",
      "Iteration 36630 Training loss 0.0038841934874653816 Validation loss 0.010572806000709534 Accuracy 0.8857421875\n",
      "Iteration 36640 Training loss 0.004952897317707539 Validation loss 0.010855003260076046 Accuracy 0.88232421875\n",
      "Iteration 36650 Training loss 0.006716086529195309 Validation loss 0.010962412692606449 Accuracy 0.880859375\n",
      "Iteration 36660 Training loss 0.004829682409763336 Validation loss 0.01064668782055378 Accuracy 0.88427734375\n",
      "Iteration 36670 Training loss 0.004448098596185446 Validation loss 0.010740990750491619 Accuracy 0.8837890625\n",
      "Iteration 36680 Training loss 0.005454560741782188 Validation loss 0.010744694620370865 Accuracy 0.88330078125\n",
      "Iteration 36690 Training loss 0.004939150996506214 Validation loss 0.010591436177492142 Accuracy 0.88525390625\n",
      "Iteration 36700 Training loss 0.00468264427036047 Validation loss 0.010616157203912735 Accuracy 0.88330078125\n",
      "Iteration 36710 Training loss 0.00575623195618391 Validation loss 0.010844147764146328 Accuracy 0.8828125\n",
      "Iteration 36720 Training loss 0.004329555202275515 Validation loss 0.01066660601645708 Accuracy 0.88427734375\n",
      "Iteration 36730 Training loss 0.004379916470497847 Validation loss 0.010856005363166332 Accuracy 0.8828125\n",
      "Iteration 36740 Training loss 0.005347328260540962 Validation loss 0.01105386670678854 Accuracy 0.88037109375\n",
      "Iteration 36750 Training loss 0.005710405297577381 Validation loss 0.010962949134409428 Accuracy 0.880859375\n",
      "Iteration 36760 Training loss 0.005096515640616417 Validation loss 0.01074373908340931 Accuracy 0.88427734375\n",
      "Iteration 36770 Training loss 0.004839751869440079 Validation loss 0.010818029753863811 Accuracy 0.8837890625\n",
      "Iteration 36780 Training loss 0.005508118309080601 Validation loss 0.011158835142850876 Accuracy 0.87939453125\n",
      "Iteration 36790 Training loss 0.004478952381759882 Validation loss 0.010867134667932987 Accuracy 0.88330078125\n",
      "Iteration 36800 Training loss 0.004008192103356123 Validation loss 0.011121513321995735 Accuracy 0.8798828125\n",
      "Iteration 36810 Training loss 0.003508062567561865 Validation loss 0.010634648613631725 Accuracy 0.88427734375\n",
      "Iteration 36820 Training loss 0.0046640317887067795 Validation loss 0.010780970565974712 Accuracy 0.88427734375\n",
      "Iteration 36830 Training loss 0.005682478658854961 Validation loss 0.010770794004201889 Accuracy 0.8828125\n",
      "Iteration 36840 Training loss 0.005017296876758337 Validation loss 0.010740810073912144 Accuracy 0.88427734375\n",
      "Iteration 36850 Training loss 0.0035353698767721653 Validation loss 0.010896697640419006 Accuracy 0.88037109375\n",
      "Iteration 36860 Training loss 0.004836829379200935 Validation loss 0.010809021070599556 Accuracy 0.88134765625\n",
      "Iteration 36870 Training loss 0.006106576882302761 Validation loss 0.010973316617310047 Accuracy 0.8798828125\n",
      "Iteration 36880 Training loss 0.004794553853571415 Validation loss 0.01066884957253933 Accuracy 0.88330078125\n",
      "Iteration 36890 Training loss 0.004925614222884178 Validation loss 0.010797098278999329 Accuracy 0.8828125\n",
      "Iteration 36900 Training loss 0.006135133095085621 Validation loss 0.010882219299674034 Accuracy 0.8818359375\n",
      "Iteration 36910 Training loss 0.004026950336992741 Validation loss 0.010706623084843159 Accuracy 0.88427734375\n",
      "Iteration 36920 Training loss 0.0051614404655992985 Validation loss 0.010661918669939041 Accuracy 0.88330078125\n",
      "Iteration 36930 Training loss 0.002937108976766467 Validation loss 0.010861743241548538 Accuracy 0.88232421875\n",
      "Iteration 36940 Training loss 0.00534228328615427 Validation loss 0.01077212393283844 Accuracy 0.8837890625\n",
      "Iteration 36950 Training loss 0.003181929001584649 Validation loss 0.010632313787937164 Accuracy 0.884765625\n",
      "Iteration 36960 Training loss 0.0037572591099888086 Validation loss 0.010812805034220219 Accuracy 0.8828125\n",
      "Iteration 36970 Training loss 0.0047965385019779205 Validation loss 0.010817162692546844 Accuracy 0.88330078125\n",
      "Iteration 36980 Training loss 0.003217410994693637 Validation loss 0.01080490741878748 Accuracy 0.8818359375\n",
      "Iteration 36990 Training loss 0.005150600802153349 Validation loss 0.010775657370686531 Accuracy 0.88330078125\n",
      "Iteration 37000 Training loss 0.0051017459481954575 Validation loss 0.010998480021953583 Accuracy 0.8798828125\n",
      "Iteration 37010 Training loss 0.005927294958382845 Validation loss 0.010554352775216103 Accuracy 0.88525390625\n",
      "Iteration 37020 Training loss 0.003894876455888152 Validation loss 0.01063032727688551 Accuracy 0.88525390625\n",
      "Iteration 37030 Training loss 0.003940495662391186 Validation loss 0.010784204117953777 Accuracy 0.88330078125\n",
      "Iteration 37040 Training loss 0.005058014299720526 Validation loss 0.011081020347774029 Accuracy 0.88037109375\n",
      "Iteration 37050 Training loss 0.005859794560819864 Validation loss 0.011088770814239979 Accuracy 0.8798828125\n",
      "Iteration 37060 Training loss 0.004703727085143328 Validation loss 0.010867736302316189 Accuracy 0.8828125\n",
      "Iteration 37070 Training loss 0.004609459079802036 Validation loss 0.010758601129055023 Accuracy 0.884765625\n",
      "Iteration 37080 Training loss 0.004130776505917311 Validation loss 0.010617487132549286 Accuracy 0.884765625\n",
      "Iteration 37090 Training loss 0.005420490633696318 Validation loss 0.010747515596449375 Accuracy 0.8828125\n",
      "Iteration 37100 Training loss 0.005298830568790436 Validation loss 0.010631938464939594 Accuracy 0.8837890625\n",
      "Iteration 37110 Training loss 0.003120813285931945 Validation loss 0.010788177140057087 Accuracy 0.8818359375\n",
      "Iteration 37120 Training loss 0.005200050305575132 Validation loss 0.010948613286018372 Accuracy 0.88232421875\n",
      "Iteration 37130 Training loss 0.003627461614087224 Validation loss 0.010720985941588879 Accuracy 0.8837890625\n",
      "Iteration 37140 Training loss 0.005266525782644749 Validation loss 0.010700136423110962 Accuracy 0.8837890625\n",
      "Iteration 37150 Training loss 0.00386174488812685 Validation loss 0.010724414139986038 Accuracy 0.88427734375\n",
      "Iteration 37160 Training loss 0.003405167954042554 Validation loss 0.010728362947702408 Accuracy 0.884765625\n",
      "Iteration 37170 Training loss 0.004180725663900375 Validation loss 0.010844276286661625 Accuracy 0.88330078125\n",
      "Iteration 37180 Training loss 0.004982475657016039 Validation loss 0.010762215591967106 Accuracy 0.88330078125\n",
      "Iteration 37190 Training loss 0.0037653674371540546 Validation loss 0.011041766963899136 Accuracy 0.87890625\n",
      "Iteration 37200 Training loss 0.005135740153491497 Validation loss 0.010673781856894493 Accuracy 0.8857421875\n",
      "Iteration 37210 Training loss 0.003685085801407695 Validation loss 0.010627180337905884 Accuracy 0.884765625\n",
      "Iteration 37220 Training loss 0.0056439731270074844 Validation loss 0.010951767675578594 Accuracy 0.880859375\n",
      "Iteration 37230 Training loss 0.0036005147267132998 Validation loss 0.010789499618113041 Accuracy 0.8828125\n",
      "Iteration 37240 Training loss 0.00471524428576231 Validation loss 0.01080150157213211 Accuracy 0.88330078125\n",
      "Iteration 37250 Training loss 0.0026195624377578497 Validation loss 0.010896398685872555 Accuracy 0.8818359375\n",
      "Iteration 37260 Training loss 0.005228267516940832 Validation loss 0.010674343444406986 Accuracy 0.8837890625\n",
      "Iteration 37270 Training loss 0.004480684641748667 Validation loss 0.011084663681685925 Accuracy 0.8798828125\n",
      "Iteration 37280 Training loss 0.0054839057847857475 Validation loss 0.01094308216124773 Accuracy 0.88134765625\n",
      "Iteration 37290 Training loss 0.005331909749656916 Validation loss 0.010713595896959305 Accuracy 0.8837890625\n",
      "Iteration 37300 Training loss 0.004111263435333967 Validation loss 0.011090639047324657 Accuracy 0.88037109375\n",
      "Iteration 37310 Training loss 0.0047309002839028835 Validation loss 0.01080773863941431 Accuracy 0.8837890625\n",
      "Iteration 37320 Training loss 0.005964725278317928 Validation loss 0.01085049007087946 Accuracy 0.8818359375\n",
      "Iteration 37330 Training loss 0.004724486730992794 Validation loss 0.010580222122371197 Accuracy 0.88525390625\n",
      "Iteration 37340 Training loss 0.0041223862208426 Validation loss 0.010610406287014484 Accuracy 0.88427734375\n",
      "Iteration 37350 Training loss 0.004646022338420153 Validation loss 0.010596955195069313 Accuracy 0.88330078125\n",
      "Iteration 37360 Training loss 0.004118113312870264 Validation loss 0.010824887081980705 Accuracy 0.88330078125\n",
      "Iteration 37370 Training loss 0.0038986268918961287 Validation loss 0.010803427547216415 Accuracy 0.88232421875\n",
      "Iteration 37380 Training loss 0.005309706553816795 Validation loss 0.01094591524451971 Accuracy 0.88232421875\n",
      "Iteration 37390 Training loss 0.0039497194811701775 Validation loss 0.010685180313885212 Accuracy 0.88330078125\n",
      "Iteration 37400 Training loss 0.00598925119265914 Validation loss 0.010756305418908596 Accuracy 0.88330078125\n",
      "Iteration 37410 Training loss 0.004675974603742361 Validation loss 0.010840452276170254 Accuracy 0.88232421875\n",
      "Iteration 37420 Training loss 0.0034502448979765177 Validation loss 0.010505905374884605 Accuracy 0.88525390625\n",
      "Iteration 37430 Training loss 0.004359941463917494 Validation loss 0.010531216859817505 Accuracy 0.88623046875\n",
      "Iteration 37440 Training loss 0.00608350895345211 Validation loss 0.010585209354758263 Accuracy 0.88525390625\n",
      "Iteration 37450 Training loss 0.005232973024249077 Validation loss 0.010725860483944416 Accuracy 0.88330078125\n",
      "Iteration 37460 Training loss 0.004626712296158075 Validation loss 0.010978519916534424 Accuracy 0.8798828125\n",
      "Iteration 37470 Training loss 0.0035411519929766655 Validation loss 0.010714939795434475 Accuracy 0.88330078125\n",
      "Iteration 37480 Training loss 0.005520923528820276 Validation loss 0.0105734346434474 Accuracy 0.8857421875\n",
      "Iteration 37490 Training loss 0.0032379652839154005 Validation loss 0.010678422637283802 Accuracy 0.88427734375\n",
      "Iteration 37500 Training loss 0.005525190383195877 Validation loss 0.010569391772150993 Accuracy 0.88525390625\n",
      "Iteration 37510 Training loss 0.0029689285438507795 Validation loss 0.010672536678612232 Accuracy 0.88427734375\n",
      "Iteration 37520 Training loss 0.005442936439067125 Validation loss 0.010624467395246029 Accuracy 0.88525390625\n",
      "Iteration 37530 Training loss 0.00431768037378788 Validation loss 0.010510459542274475 Accuracy 0.88671875\n",
      "Iteration 37540 Training loss 0.004991254303604364 Validation loss 0.010686840862035751 Accuracy 0.884765625\n",
      "Iteration 37550 Training loss 0.005076379980891943 Validation loss 0.010687995702028275 Accuracy 0.88427734375\n",
      "Iteration 37560 Training loss 0.005096549168229103 Validation loss 0.01072770357131958 Accuracy 0.88232421875\n",
      "Iteration 37570 Training loss 0.004754141438752413 Validation loss 0.01063728891313076 Accuracy 0.88427734375\n",
      "Iteration 37580 Training loss 0.005756422411650419 Validation loss 0.010869817808270454 Accuracy 0.8818359375\n",
      "Iteration 37590 Training loss 0.003630400402471423 Validation loss 0.010975345969200134 Accuracy 0.8818359375\n",
      "Iteration 37600 Training loss 0.0053090122528374195 Validation loss 0.010647024028003216 Accuracy 0.88427734375\n",
      "Iteration 37610 Training loss 0.005479191895574331 Validation loss 0.010657118633389473 Accuracy 0.88330078125\n",
      "Iteration 37620 Training loss 0.006033114157617092 Validation loss 0.010748513974249363 Accuracy 0.8828125\n",
      "Iteration 37630 Training loss 0.004803541582077742 Validation loss 0.010992255993187428 Accuracy 0.88037109375\n",
      "Iteration 37640 Training loss 0.0036748587153851986 Validation loss 0.010837814770638943 Accuracy 0.88232421875\n",
      "Iteration 37650 Training loss 0.004902470391243696 Validation loss 0.010820307768881321 Accuracy 0.8818359375\n",
      "Iteration 37660 Training loss 0.004342240747064352 Validation loss 0.010587278753519058 Accuracy 0.88427734375\n",
      "Iteration 37670 Training loss 0.003847106359899044 Validation loss 0.010785307735204697 Accuracy 0.8828125\n",
      "Iteration 37680 Training loss 0.004092515911906958 Validation loss 0.010986462235450745 Accuracy 0.8798828125\n",
      "Iteration 37690 Training loss 0.0045787980780005455 Validation loss 0.010651522316038609 Accuracy 0.8837890625\n",
      "Iteration 37700 Training loss 0.0030206048395484686 Validation loss 0.010773676447570324 Accuracy 0.88330078125\n",
      "Iteration 37710 Training loss 0.003977978602051735 Validation loss 0.010713964700698853 Accuracy 0.88330078125\n",
      "Iteration 37720 Training loss 0.0034252943005412817 Validation loss 0.010626171715557575 Accuracy 0.88427734375\n",
      "Iteration 37730 Training loss 0.004289072938263416 Validation loss 0.010676427744328976 Accuracy 0.8837890625\n",
      "Iteration 37740 Training loss 0.004524556919932365 Validation loss 0.010617749765515327 Accuracy 0.88525390625\n",
      "Iteration 37750 Training loss 0.004639778286218643 Validation loss 0.010645096190273762 Accuracy 0.8837890625\n",
      "Iteration 37760 Training loss 0.004981304984539747 Validation loss 0.01085812970995903 Accuracy 0.88232421875\n",
      "Iteration 37770 Training loss 0.003725530346855521 Validation loss 0.010501716285943985 Accuracy 0.88623046875\n",
      "Iteration 37780 Training loss 0.004943667445331812 Validation loss 0.01050861831754446 Accuracy 0.88720703125\n",
      "Iteration 37790 Training loss 0.00469533447176218 Validation loss 0.010655845515429974 Accuracy 0.88427734375\n",
      "Iteration 37800 Training loss 0.003304265206679702 Validation loss 0.010588890872895718 Accuracy 0.88525390625\n",
      "Iteration 37810 Training loss 0.00549261923879385 Validation loss 0.010708143934607506 Accuracy 0.88525390625\n",
      "Iteration 37820 Training loss 0.005747760646045208 Validation loss 0.010833678767085075 Accuracy 0.88232421875\n",
      "Iteration 37830 Training loss 0.006010556593537331 Validation loss 0.010770522058010101 Accuracy 0.88330078125\n",
      "Iteration 37840 Training loss 0.00543162552639842 Validation loss 0.010840564034879208 Accuracy 0.8818359375\n",
      "Iteration 37850 Training loss 0.003911050967872143 Validation loss 0.010539577342569828 Accuracy 0.88623046875\n",
      "Iteration 37860 Training loss 0.004783423151820898 Validation loss 0.010600609704852104 Accuracy 0.88623046875\n",
      "Iteration 37870 Training loss 0.00436517084017396 Validation loss 0.010531029663980007 Accuracy 0.88623046875\n",
      "Iteration 37880 Training loss 0.003649249905720353 Validation loss 0.010734586976468563 Accuracy 0.88330078125\n",
      "Iteration 37890 Training loss 0.00454655010253191 Validation loss 0.01067380141466856 Accuracy 0.8837890625\n",
      "Iteration 37900 Training loss 0.003743628738448024 Validation loss 0.010733046568930149 Accuracy 0.8837890625\n",
      "Iteration 37910 Training loss 0.004654671531170607 Validation loss 0.010647375136613846 Accuracy 0.88427734375\n",
      "Iteration 37920 Training loss 0.004864235874265432 Validation loss 0.010601515881717205 Accuracy 0.88427734375\n",
      "Iteration 37930 Training loss 0.005784070119261742 Validation loss 0.010634222999215126 Accuracy 0.88427734375\n",
      "Iteration 37940 Training loss 0.005422648042440414 Validation loss 0.010728280059993267 Accuracy 0.8837890625\n",
      "Iteration 37950 Training loss 0.003748385002836585 Validation loss 0.010658563114702702 Accuracy 0.88525390625\n",
      "Iteration 37960 Training loss 0.004234714899212122 Validation loss 0.01080162450671196 Accuracy 0.88330078125\n",
      "Iteration 37970 Training loss 0.004331181291490793 Validation loss 0.010919020511209965 Accuracy 0.8818359375\n",
      "Iteration 37980 Training loss 0.005181015469133854 Validation loss 0.010920370928943157 Accuracy 0.880859375\n",
      "Iteration 37990 Training loss 0.006750322878360748 Validation loss 0.01072266697883606 Accuracy 0.8837890625\n",
      "Iteration 38000 Training loss 0.004641910083591938 Validation loss 0.01058115903288126 Accuracy 0.88427734375\n",
      "Iteration 38010 Training loss 0.004251854959875345 Validation loss 0.010520500130951405 Accuracy 0.8857421875\n",
      "Iteration 38020 Training loss 0.005741026718169451 Validation loss 0.010598401539027691 Accuracy 0.88427734375\n",
      "Iteration 38030 Training loss 0.005625621881335974 Validation loss 0.010524408891797066 Accuracy 0.8857421875\n",
      "Iteration 38040 Training loss 0.0032945044804364443 Validation loss 0.010593554005026817 Accuracy 0.88525390625\n",
      "Iteration 38050 Training loss 0.005685152020305395 Validation loss 0.011070004664361477 Accuracy 0.87890625\n",
      "Iteration 38060 Training loss 0.004795944318175316 Validation loss 0.010764694772660732 Accuracy 0.88330078125\n",
      "Iteration 38070 Training loss 0.0039059079717844725 Validation loss 0.010834082029759884 Accuracy 0.8828125\n",
      "Iteration 38080 Training loss 0.003836454125121236 Validation loss 0.010677644982933998 Accuracy 0.88525390625\n",
      "Iteration 38090 Training loss 0.005759835708886385 Validation loss 0.010642551816999912 Accuracy 0.884765625\n",
      "Iteration 38100 Training loss 0.003604321042075753 Validation loss 0.01052270270884037 Accuracy 0.8857421875\n",
      "Iteration 38110 Training loss 0.005006025545299053 Validation loss 0.01058964617550373 Accuracy 0.8837890625\n",
      "Iteration 38120 Training loss 0.0037903334014117718 Validation loss 0.01053626835346222 Accuracy 0.884765625\n",
      "Iteration 38130 Training loss 0.0044053103774785995 Validation loss 0.01075579971075058 Accuracy 0.8828125\n",
      "Iteration 38140 Training loss 0.004907591268420219 Validation loss 0.010713267140090466 Accuracy 0.8828125\n",
      "Iteration 38150 Training loss 0.0035882522352039814 Validation loss 0.010698662139475346 Accuracy 0.88330078125\n",
      "Iteration 38160 Training loss 0.005495937075465918 Validation loss 0.010850422084331512 Accuracy 0.88232421875\n",
      "Iteration 38170 Training loss 0.0038999374955892563 Validation loss 0.010605566203594208 Accuracy 0.88427734375\n",
      "Iteration 38180 Training loss 0.004094574600458145 Validation loss 0.01063036359846592 Accuracy 0.88427734375\n",
      "Iteration 38190 Training loss 0.003307588864117861 Validation loss 0.010699440725147724 Accuracy 0.88330078125\n",
      "Iteration 38200 Training loss 0.003332071006298065 Validation loss 0.0105406129732728 Accuracy 0.88525390625\n",
      "Iteration 38210 Training loss 0.004312151111662388 Validation loss 0.010667909868061543 Accuracy 0.8837890625\n",
      "Iteration 38220 Training loss 0.005191261414438486 Validation loss 0.010790329426527023 Accuracy 0.8828125\n",
      "Iteration 38230 Training loss 0.0034165915567427874 Validation loss 0.010828048922121525 Accuracy 0.8828125\n",
      "Iteration 38240 Training loss 0.004012328572571278 Validation loss 0.010725204832851887 Accuracy 0.8828125\n",
      "Iteration 38250 Training loss 0.005389133933931589 Validation loss 0.010752453468739986 Accuracy 0.88232421875\n",
      "Iteration 38260 Training loss 0.005678846966475248 Validation loss 0.010758425109088421 Accuracy 0.88330078125\n",
      "Iteration 38270 Training loss 0.0034369309432804585 Validation loss 0.010658175684511662 Accuracy 0.88427734375\n",
      "Iteration 38280 Training loss 0.0036437357775866985 Validation loss 0.010637765750288963 Accuracy 0.88330078125\n",
      "Iteration 38290 Training loss 0.0035040597431361675 Validation loss 0.01074723619967699 Accuracy 0.88330078125\n",
      "Iteration 38300 Training loss 0.005642360541969538 Validation loss 0.010753419250249863 Accuracy 0.88330078125\n",
      "Iteration 38310 Training loss 0.004357548430562019 Validation loss 0.010605954565107822 Accuracy 0.88525390625\n",
      "Iteration 38320 Training loss 0.004347628448158503 Validation loss 0.01064275112003088 Accuracy 0.884765625\n",
      "Iteration 38330 Training loss 0.004216370638459921 Validation loss 0.010621552355587482 Accuracy 0.884765625\n",
      "Iteration 38340 Training loss 0.004292138386517763 Validation loss 0.011021283455193043 Accuracy 0.880859375\n",
      "Iteration 38350 Training loss 0.004565339535474777 Validation loss 0.010704519227147102 Accuracy 0.884765625\n",
      "Iteration 38360 Training loss 0.0038892815355211496 Validation loss 0.010706446133553982 Accuracy 0.8837890625\n",
      "Iteration 38370 Training loss 0.00635267561301589 Validation loss 0.010703282430768013 Accuracy 0.884765625\n",
      "Iteration 38380 Training loss 0.003779902821406722 Validation loss 0.010697106830775738 Accuracy 0.8818359375\n",
      "Iteration 38390 Training loss 0.005688271950930357 Validation loss 0.010917219333350658 Accuracy 0.88134765625\n",
      "Iteration 38400 Training loss 0.004843258298933506 Validation loss 0.011132504791021347 Accuracy 0.87841796875\n",
      "Iteration 38410 Training loss 0.005434013903141022 Validation loss 0.010616791434586048 Accuracy 0.884765625\n",
      "Iteration 38420 Training loss 0.004675097763538361 Validation loss 0.010713239200413227 Accuracy 0.88330078125\n",
      "Iteration 38430 Training loss 0.0056432741694152355 Validation loss 0.010991904884576797 Accuracy 0.88037109375\n",
      "Iteration 38440 Training loss 0.0038231376092880964 Validation loss 0.010534727945923805 Accuracy 0.88525390625\n",
      "Iteration 38450 Training loss 0.003593884641304612 Validation loss 0.010578571818768978 Accuracy 0.88427734375\n",
      "Iteration 38460 Training loss 0.005442989058792591 Validation loss 0.0105647137388587 Accuracy 0.884765625\n",
      "Iteration 38470 Training loss 0.004384866915643215 Validation loss 0.010638273321092129 Accuracy 0.88427734375\n",
      "Iteration 38480 Training loss 0.003594428999349475 Validation loss 0.010780357755720615 Accuracy 0.88232421875\n",
      "Iteration 38490 Training loss 0.003922039642930031 Validation loss 0.010625100694596767 Accuracy 0.8837890625\n",
      "Iteration 38500 Training loss 0.003350146347656846 Validation loss 0.010688663460314274 Accuracy 0.8837890625\n",
      "Iteration 38510 Training loss 0.003945367410778999 Validation loss 0.010533229447901249 Accuracy 0.88525390625\n",
      "Iteration 38520 Training loss 0.0064283376559615135 Validation loss 0.010649674572050571 Accuracy 0.88427734375\n",
      "Iteration 38530 Training loss 0.003691941499710083 Validation loss 0.010785131715238094 Accuracy 0.8828125\n",
      "Iteration 38540 Training loss 0.0037939390167593956 Validation loss 0.010540375486016273 Accuracy 0.8857421875\n",
      "Iteration 38550 Training loss 0.005619145464152098 Validation loss 0.010580312460660934 Accuracy 0.88427734375\n",
      "Iteration 38560 Training loss 0.003804735140874982 Validation loss 0.010703356936573982 Accuracy 0.88330078125\n",
      "Iteration 38570 Training loss 0.004749269224703312 Validation loss 0.010773586109280586 Accuracy 0.88330078125\n",
      "Iteration 38580 Training loss 0.004289423115551472 Validation loss 0.010766883380711079 Accuracy 0.88134765625\n",
      "Iteration 38590 Training loss 0.004393580369651318 Validation loss 0.010602802038192749 Accuracy 0.88427734375\n",
      "Iteration 38600 Training loss 0.005820530466735363 Validation loss 0.01065948698669672 Accuracy 0.8837890625\n",
      "Iteration 38610 Training loss 0.004715209361165762 Validation loss 0.010473179630935192 Accuracy 0.88623046875\n",
      "Iteration 38620 Training loss 0.003399431938305497 Validation loss 0.010593738406896591 Accuracy 0.884765625\n",
      "Iteration 38630 Training loss 0.004716336727142334 Validation loss 0.010768262669444084 Accuracy 0.88134765625\n",
      "Iteration 38640 Training loss 0.003908529412001371 Validation loss 0.010578489862382412 Accuracy 0.88525390625\n",
      "Iteration 38650 Training loss 0.004183872602880001 Validation loss 0.010702909901738167 Accuracy 0.88427734375\n",
      "Iteration 38660 Training loss 0.004730610642582178 Validation loss 0.010849015787243843 Accuracy 0.8818359375\n",
      "Iteration 38670 Training loss 0.002763092052191496 Validation loss 0.010638861916959286 Accuracy 0.8837890625\n",
      "Iteration 38680 Training loss 0.0037073353305459023 Validation loss 0.010733191855251789 Accuracy 0.88232421875\n",
      "Iteration 38690 Training loss 0.0034336999524384737 Validation loss 0.010651061311364174 Accuracy 0.88427734375\n",
      "Iteration 38700 Training loss 0.004712195135653019 Validation loss 0.010756327770650387 Accuracy 0.88232421875\n",
      "Iteration 38710 Training loss 0.005771634168922901 Validation loss 0.010650282725691795 Accuracy 0.8837890625\n",
      "Iteration 38720 Training loss 0.003847904736176133 Validation loss 0.0105747040361166 Accuracy 0.884765625\n",
      "Iteration 38730 Training loss 0.0039491066709160805 Validation loss 0.010768185369670391 Accuracy 0.88232421875\n",
      "Iteration 38740 Training loss 0.004007610492408276 Validation loss 0.010753034614026546 Accuracy 0.8828125\n",
      "Iteration 38750 Training loss 0.006327593233436346 Validation loss 0.010716856457293034 Accuracy 0.8837890625\n",
      "Iteration 38760 Training loss 0.004023912362754345 Validation loss 0.01077145803719759 Accuracy 0.8828125\n",
      "Iteration 38770 Training loss 0.005455546081066132 Validation loss 0.010943593457341194 Accuracy 0.8798828125\n",
      "Iteration 38780 Training loss 0.004433862864971161 Validation loss 0.010687673464417458 Accuracy 0.8828125\n",
      "Iteration 38790 Training loss 0.004368151538074017 Validation loss 0.0106887212023139 Accuracy 0.8837890625\n",
      "Iteration 38800 Training loss 0.0049568950198590755 Validation loss 0.010640119202435017 Accuracy 0.884765625\n",
      "Iteration 38810 Training loss 0.0043252091854810715 Validation loss 0.01068031694740057 Accuracy 0.8837890625\n",
      "Iteration 38820 Training loss 0.004502746742218733 Validation loss 0.010789579711854458 Accuracy 0.8828125\n",
      "Iteration 38830 Training loss 0.004777236841619015 Validation loss 0.01060124859213829 Accuracy 0.88427734375\n",
      "Iteration 38840 Training loss 0.004762450698763132 Validation loss 0.010557025671005249 Accuracy 0.88525390625\n",
      "Iteration 38850 Training loss 0.0030697074253112078 Validation loss 0.010583325289189816 Accuracy 0.88427734375\n",
      "Iteration 38860 Training loss 0.004014403093606234 Validation loss 0.010761568322777748 Accuracy 0.88330078125\n",
      "Iteration 38870 Training loss 0.004877137485891581 Validation loss 0.010576329194009304 Accuracy 0.88525390625\n",
      "Iteration 38880 Training loss 0.0030453649815171957 Validation loss 0.010495473630726337 Accuracy 0.88671875\n",
      "Iteration 38890 Training loss 0.004595702514052391 Validation loss 0.01055228989571333 Accuracy 0.88427734375\n",
      "Iteration 38900 Training loss 0.005296171642839909 Validation loss 0.010653283447027206 Accuracy 0.884765625\n",
      "Iteration 38910 Training loss 0.003145489376038313 Validation loss 0.010605672374367714 Accuracy 0.884765625\n",
      "Iteration 38920 Training loss 0.003554938593879342 Validation loss 0.01097395271062851 Accuracy 0.880859375\n",
      "Iteration 38930 Training loss 0.0060651423409581184 Validation loss 0.010626479052007198 Accuracy 0.88427734375\n",
      "Iteration 38940 Training loss 0.004013423342257738 Validation loss 0.010612959042191505 Accuracy 0.88427734375\n",
      "Iteration 38950 Training loss 0.003210477065294981 Validation loss 0.010629967786371708 Accuracy 0.8837890625\n",
      "Iteration 38960 Training loss 0.004755923990160227 Validation loss 0.010719472542405128 Accuracy 0.8828125\n",
      "Iteration 38970 Training loss 0.004622420761734247 Validation loss 0.010802516713738441 Accuracy 0.88232421875\n",
      "Iteration 38980 Training loss 0.0021190287079662085 Validation loss 0.01069633848965168 Accuracy 0.88330078125\n",
      "Iteration 38990 Training loss 0.0046179755590856075 Validation loss 0.010928661562502384 Accuracy 0.88232421875\n",
      "Iteration 39000 Training loss 0.004257872235029936 Validation loss 0.010579565539956093 Accuracy 0.88427734375\n",
      "Iteration 39010 Training loss 0.004333168733865023 Validation loss 0.010551320388913155 Accuracy 0.88525390625\n",
      "Iteration 39020 Training loss 0.004592761397361755 Validation loss 0.010645238682627678 Accuracy 0.8837890625\n",
      "Iteration 39030 Training loss 0.005312602501362562 Validation loss 0.010608445852994919 Accuracy 0.884765625\n",
      "Iteration 39040 Training loss 0.005506683140993118 Validation loss 0.010591923259198666 Accuracy 0.884765625\n",
      "Iteration 39050 Training loss 0.004255218431353569 Validation loss 0.010607968084514141 Accuracy 0.88427734375\n",
      "Iteration 39060 Training loss 0.00334594096057117 Validation loss 0.010655876249074936 Accuracy 0.88330078125\n",
      "Iteration 39070 Training loss 0.0036783490795642138 Validation loss 0.010804204270243645 Accuracy 0.8837890625\n",
      "Iteration 39080 Training loss 0.004268414806574583 Validation loss 0.010764580219984055 Accuracy 0.88232421875\n",
      "Iteration 39090 Training loss 0.0051524150185287 Validation loss 0.010640628635883331 Accuracy 0.8837890625\n",
      "Iteration 39100 Training loss 0.003773508360609412 Validation loss 0.010787717066705227 Accuracy 0.88232421875\n",
      "Iteration 39110 Training loss 0.004235872067511082 Validation loss 0.010709128342568874 Accuracy 0.8837890625\n",
      "Iteration 39120 Training loss 0.004657373297959566 Validation loss 0.010623453184962273 Accuracy 0.88427734375\n",
      "Iteration 39130 Training loss 0.0036792277824133635 Validation loss 0.010720574297010899 Accuracy 0.8828125\n",
      "Iteration 39140 Training loss 0.005704274866729975 Validation loss 0.010764560662209988 Accuracy 0.8828125\n",
      "Iteration 39150 Training loss 0.004247247241437435 Validation loss 0.01063929870724678 Accuracy 0.88427734375\n",
      "Iteration 39160 Training loss 0.005278175231069326 Validation loss 0.010911503806710243 Accuracy 0.880859375\n",
      "Iteration 39170 Training loss 0.005251424852758646 Validation loss 0.010776873677968979 Accuracy 0.88232421875\n",
      "Iteration 39180 Training loss 0.003315222915261984 Validation loss 0.010611448436975479 Accuracy 0.8837890625\n",
      "Iteration 39190 Training loss 0.003581554628908634 Validation loss 0.010649642907083035 Accuracy 0.88427734375\n",
      "Iteration 39200 Training loss 0.004706115461885929 Validation loss 0.010650375857949257 Accuracy 0.88330078125\n",
      "Iteration 39210 Training loss 0.0042801774106919765 Validation loss 0.01063098106533289 Accuracy 0.88330078125\n",
      "Iteration 39220 Training loss 0.004955881740897894 Validation loss 0.01059779804199934 Accuracy 0.8837890625\n",
      "Iteration 39230 Training loss 0.004533884581178427 Validation loss 0.010420037433505058 Accuracy 0.88623046875\n",
      "Iteration 39240 Training loss 0.004723507445305586 Validation loss 0.010637265630066395 Accuracy 0.884765625\n",
      "Iteration 39250 Training loss 0.005588486324995756 Validation loss 0.010695390403270721 Accuracy 0.88427734375\n",
      "Iteration 39260 Training loss 0.004147883504629135 Validation loss 0.010789308696985245 Accuracy 0.8828125\n",
      "Iteration 39270 Training loss 0.003750055329874158 Validation loss 0.010704529471695423 Accuracy 0.8837890625\n",
      "Iteration 39280 Training loss 0.0038324189372360706 Validation loss 0.011022946797311306 Accuracy 0.87890625\n",
      "Iteration 39290 Training loss 0.0030984769109636545 Validation loss 0.010814185254275799 Accuracy 0.88232421875\n",
      "Iteration 39300 Training loss 0.004454395268112421 Validation loss 0.01064334437251091 Accuracy 0.88330078125\n",
      "Iteration 39310 Training loss 0.003848180640488863 Validation loss 0.010895011946558952 Accuracy 0.8818359375\n",
      "Iteration 39320 Training loss 0.004410165827721357 Validation loss 0.010614306665956974 Accuracy 0.8837890625\n",
      "Iteration 39330 Training loss 0.004478775430470705 Validation loss 0.010661962442100048 Accuracy 0.88427734375\n",
      "Iteration 39340 Training loss 0.004154769703745842 Validation loss 0.01074258517473936 Accuracy 0.88330078125\n",
      "Iteration 39350 Training loss 0.0034783389419317245 Validation loss 0.010687186382710934 Accuracy 0.8837890625\n",
      "Iteration 39360 Training loss 0.004537888336926699 Validation loss 0.010572451166808605 Accuracy 0.884765625\n",
      "Iteration 39370 Training loss 0.003795691765844822 Validation loss 0.010544579476118088 Accuracy 0.88427734375\n",
      "Iteration 39380 Training loss 0.004330284893512726 Validation loss 0.010537365451455116 Accuracy 0.884765625\n",
      "Iteration 39390 Training loss 0.00476857228204608 Validation loss 0.010672160424292088 Accuracy 0.884765625\n",
      "Iteration 39400 Training loss 0.003422789741307497 Validation loss 0.010512571781873703 Accuracy 0.884765625\n",
      "Iteration 39410 Training loss 0.006095657125115395 Validation loss 0.010601739399135113 Accuracy 0.884765625\n",
      "Iteration 39420 Training loss 0.003856928553432226 Validation loss 0.010513813234865665 Accuracy 0.884765625\n",
      "Iteration 39430 Training loss 0.006919891107827425 Validation loss 0.010609002783894539 Accuracy 0.88427734375\n",
      "Iteration 39440 Training loss 0.004133815877139568 Validation loss 0.010377123951911926 Accuracy 0.88623046875\n",
      "Iteration 39450 Training loss 0.0040296451188623905 Validation loss 0.010422145947813988 Accuracy 0.88671875\n",
      "Iteration 39460 Training loss 0.0036353806499391794 Validation loss 0.010746357031166553 Accuracy 0.88330078125\n",
      "Iteration 39470 Training loss 0.0034371204674243927 Validation loss 0.01059460174292326 Accuracy 0.88427734375\n",
      "Iteration 39480 Training loss 0.004188694059848785 Validation loss 0.010906435549259186 Accuracy 0.880859375\n",
      "Iteration 39490 Training loss 0.004766852594912052 Validation loss 0.01079054456204176 Accuracy 0.88134765625\n",
      "Iteration 39500 Training loss 0.0047638607211411 Validation loss 0.01052729319781065 Accuracy 0.88427734375\n",
      "Iteration 39510 Training loss 0.004060245584696531 Validation loss 0.010686170309782028 Accuracy 0.8837890625\n",
      "Iteration 39520 Training loss 0.0035756975412368774 Validation loss 0.010661632753908634 Accuracy 0.8828125\n",
      "Iteration 39530 Training loss 0.0054933736100792885 Validation loss 0.010777941904962063 Accuracy 0.8828125\n",
      "Iteration 39540 Training loss 0.006764581426978111 Validation loss 0.010655751451849937 Accuracy 0.88330078125\n",
      "Iteration 39550 Training loss 0.004837369080632925 Validation loss 0.010724970139563084 Accuracy 0.88427734375\n",
      "Iteration 39560 Training loss 0.003931492567062378 Validation loss 0.010762067511677742 Accuracy 0.880859375\n",
      "Iteration 39570 Training loss 0.003311619395390153 Validation loss 0.01058886107057333 Accuracy 0.88427734375\n",
      "Iteration 39580 Training loss 0.004345517139881849 Validation loss 0.01049261074513197 Accuracy 0.8857421875\n",
      "Iteration 39590 Training loss 0.005113845691084862 Validation loss 0.010702241212129593 Accuracy 0.88330078125\n",
      "Iteration 39600 Training loss 0.003911677747964859 Validation loss 0.010584003292024136 Accuracy 0.88427734375\n",
      "Iteration 39610 Training loss 0.005498897284269333 Validation loss 0.01065075397491455 Accuracy 0.88330078125\n",
      "Iteration 39620 Training loss 0.005523658823221922 Validation loss 0.010740119032561779 Accuracy 0.88232421875\n",
      "Iteration 39630 Training loss 0.006058956030756235 Validation loss 0.010819683782756329 Accuracy 0.8818359375\n",
      "Iteration 39640 Training loss 0.004554649814963341 Validation loss 0.01068087387830019 Accuracy 0.88330078125\n",
      "Iteration 39650 Training loss 0.004464391618967056 Validation loss 0.010615392588078976 Accuracy 0.8837890625\n",
      "Iteration 39660 Training loss 0.005656907334923744 Validation loss 0.010592160746455193 Accuracy 0.884765625\n",
      "Iteration 39670 Training loss 0.0045265513472259045 Validation loss 0.010741598904132843 Accuracy 0.88330078125\n",
      "Iteration 39680 Training loss 0.005036459304392338 Validation loss 0.010734990239143372 Accuracy 0.8837890625\n",
      "Iteration 39690 Training loss 0.004700683057308197 Validation loss 0.011037081480026245 Accuracy 0.8779296875\n",
      "Iteration 39700 Training loss 0.004419127944856882 Validation loss 0.010798891074955463 Accuracy 0.880859375\n",
      "Iteration 39710 Training loss 0.004719276446849108 Validation loss 0.010881394147872925 Accuracy 0.880859375\n",
      "Iteration 39720 Training loss 0.005160938948392868 Validation loss 0.01059142779558897 Accuracy 0.88427734375\n",
      "Iteration 39730 Training loss 0.00525488518178463 Validation loss 0.010502729564905167 Accuracy 0.88427734375\n",
      "Iteration 39740 Training loss 0.004736660048365593 Validation loss 0.010627318173646927 Accuracy 0.8837890625\n",
      "Iteration 39750 Training loss 0.0036104011815041304 Validation loss 0.010688592679798603 Accuracy 0.88232421875\n",
      "Iteration 39760 Training loss 0.002925051376223564 Validation loss 0.010607927106320858 Accuracy 0.88427734375\n",
      "Iteration 39770 Training loss 0.004249483346939087 Validation loss 0.010530990548431873 Accuracy 0.884765625\n",
      "Iteration 39780 Training loss 0.005038470029830933 Validation loss 0.010539025999605656 Accuracy 0.884765625\n",
      "Iteration 39790 Training loss 0.004963755141943693 Validation loss 0.010543710552155972 Accuracy 0.884765625\n",
      "Iteration 39800 Training loss 0.0037214176263660192 Validation loss 0.010514890775084496 Accuracy 0.88427734375\n",
      "Iteration 39810 Training loss 0.0035762633197009563 Validation loss 0.010631836950778961 Accuracy 0.8828125\n",
      "Iteration 39820 Training loss 0.004189976025372744 Validation loss 0.010554459877312183 Accuracy 0.884765625\n",
      "Iteration 39830 Training loss 0.004417520947754383 Validation loss 0.0107658002525568 Accuracy 0.8828125\n",
      "Iteration 39840 Training loss 0.004169768653810024 Validation loss 0.010738207958638668 Accuracy 0.8828125\n",
      "Iteration 39850 Training loss 0.0036186352372169495 Validation loss 0.010469560511410236 Accuracy 0.8857421875\n",
      "Iteration 39860 Training loss 0.004815122112631798 Validation loss 0.010678918100893497 Accuracy 0.8828125\n",
      "Iteration 39870 Training loss 0.0037204474210739136 Validation loss 0.01065944042056799 Accuracy 0.8828125\n",
      "Iteration 39880 Training loss 0.0031666902359575033 Validation loss 0.01051625981926918 Accuracy 0.88427734375\n",
      "Iteration 39890 Training loss 0.0032610746566206217 Validation loss 0.01057638879865408 Accuracy 0.8837890625\n",
      "Iteration 39900 Training loss 0.0037093213759362698 Validation loss 0.010457631200551987 Accuracy 0.88623046875\n",
      "Iteration 39910 Training loss 0.0040681795217096806 Validation loss 0.010525405406951904 Accuracy 0.88525390625\n",
      "Iteration 39920 Training loss 0.003911406267434359 Validation loss 0.010595150291919708 Accuracy 0.884765625\n",
      "Iteration 39930 Training loss 0.004061183892190456 Validation loss 0.010557865723967552 Accuracy 0.884765625\n",
      "Iteration 39940 Training loss 0.003660487709566951 Validation loss 0.010609857738018036 Accuracy 0.88427734375\n",
      "Iteration 39950 Training loss 0.00399251002818346 Validation loss 0.010515407659113407 Accuracy 0.88525390625\n",
      "Iteration 39960 Training loss 0.004074841272085905 Validation loss 0.010599719360470772 Accuracy 0.88427734375\n",
      "Iteration 39970 Training loss 0.00445729261264205 Validation loss 0.010545725002884865 Accuracy 0.88525390625\n",
      "Iteration 39980 Training loss 0.004715384915471077 Validation loss 0.010538375936448574 Accuracy 0.88525390625\n",
      "Iteration 39990 Training loss 0.00388957466930151 Validation loss 0.01055114809423685 Accuracy 0.88525390625\n",
      "Iteration 40000 Training loss 0.0036841831170022488 Validation loss 0.0105782775208354 Accuracy 0.88525390625\n",
      "Iteration 40010 Training loss 0.0037821540609002113 Validation loss 0.01059537660330534 Accuracy 0.88427734375\n",
      "Iteration 40020 Training loss 0.003713861806318164 Validation loss 0.010581630282104015 Accuracy 0.88427734375\n",
      "Iteration 40030 Training loss 0.006459441967308521 Validation loss 0.01073366217315197 Accuracy 0.8828125\n",
      "Iteration 40040 Training loss 0.004406691994518042 Validation loss 0.010522537864744663 Accuracy 0.8857421875\n",
      "Iteration 40050 Training loss 0.0035649635829031467 Validation loss 0.011305690743029118 Accuracy 0.87548828125\n",
      "Iteration 40060 Training loss 0.0036947738844901323 Validation loss 0.010540236718952656 Accuracy 0.88525390625\n",
      "Iteration 40070 Training loss 0.004556346219033003 Validation loss 0.010516179725527763 Accuracy 0.88623046875\n",
      "Iteration 40080 Training loss 0.003297948744148016 Validation loss 0.010583908297121525 Accuracy 0.8837890625\n",
      "Iteration 40090 Training loss 0.0042344084940850735 Validation loss 0.010654049925506115 Accuracy 0.88330078125\n",
      "Iteration 40100 Training loss 0.003928184509277344 Validation loss 0.010566978715360165 Accuracy 0.884765625\n",
      "Iteration 40110 Training loss 0.003123064059764147 Validation loss 0.01051364466547966 Accuracy 0.884765625\n",
      "Iteration 40120 Training loss 0.005362464115023613 Validation loss 0.010576801374554634 Accuracy 0.8837890625\n",
      "Iteration 40130 Training loss 0.0035032068844884634 Validation loss 0.0106094004586339 Accuracy 0.88427734375\n",
      "Iteration 40140 Training loss 0.004356034565716982 Validation loss 0.01064620353281498 Accuracy 0.8828125\n",
      "Iteration 40150 Training loss 0.0036904376465827227 Validation loss 0.010573068633675575 Accuracy 0.88427734375\n",
      "Iteration 40160 Training loss 0.0036515770480036736 Validation loss 0.010670239105820656 Accuracy 0.8818359375\n",
      "Iteration 40170 Training loss 0.004526267759501934 Validation loss 0.010542621836066246 Accuracy 0.8837890625\n",
      "Iteration 40180 Training loss 0.0031774851959198713 Validation loss 0.010734948329627514 Accuracy 0.8837890625\n",
      "Iteration 40190 Training loss 0.004631189163774252 Validation loss 0.010546333156526089 Accuracy 0.88525390625\n",
      "Iteration 40200 Training loss 0.0048123812302947044 Validation loss 0.010597609914839268 Accuracy 0.88330078125\n",
      "Iteration 40210 Training loss 0.0036256974563002586 Validation loss 0.0105203902348876 Accuracy 0.8857421875\n",
      "Iteration 40220 Training loss 0.004494255408644676 Validation loss 0.010538932867348194 Accuracy 0.884765625\n",
      "Iteration 40230 Training loss 0.005667561665177345 Validation loss 0.010669458657503128 Accuracy 0.8828125\n",
      "Iteration 40240 Training loss 0.004810662474483252 Validation loss 0.010628608986735344 Accuracy 0.8837890625\n",
      "Iteration 40250 Training loss 0.004282111302018166 Validation loss 0.010765119455754757 Accuracy 0.88232421875\n",
      "Iteration 40260 Training loss 0.004161111544817686 Validation loss 0.010665247216820717 Accuracy 0.88330078125\n",
      "Iteration 40270 Training loss 0.0029507791623473167 Validation loss 0.010628246702253819 Accuracy 0.88330078125\n",
      "Iteration 40280 Training loss 0.005228893831372261 Validation loss 0.010622470639646053 Accuracy 0.8837890625\n",
      "Iteration 40290 Training loss 0.005897101480513811 Validation loss 0.010758007876574993 Accuracy 0.88134765625\n",
      "Iteration 40300 Training loss 0.0032433252781629562 Validation loss 0.010838122107088566 Accuracy 0.8818359375\n",
      "Iteration 40310 Training loss 0.003975367173552513 Validation loss 0.010654764249920845 Accuracy 0.8828125\n",
      "Iteration 40320 Training loss 0.004455712158232927 Validation loss 0.011179550550878048 Accuracy 0.8779296875\n",
      "Iteration 40330 Training loss 0.004108286928385496 Validation loss 0.010693829506635666 Accuracy 0.88330078125\n",
      "Iteration 40340 Training loss 0.0044903880916535854 Validation loss 0.010815633460879326 Accuracy 0.8828125\n",
      "Iteration 40350 Training loss 0.003994504921138287 Validation loss 0.010756774805486202 Accuracy 0.8828125\n",
      "Iteration 40360 Training loss 0.0038316892459988594 Validation loss 0.010712111368775368 Accuracy 0.88232421875\n",
      "Iteration 40370 Training loss 0.004126593004912138 Validation loss 0.010852114297449589 Accuracy 0.88134765625\n",
      "Iteration 40380 Training loss 0.003851441666483879 Validation loss 0.010683201253414154 Accuracy 0.88232421875\n",
      "Iteration 40390 Training loss 0.003807829460129142 Validation loss 0.010710193775594234 Accuracy 0.8828125\n",
      "Iteration 40400 Training loss 0.003829010995104909 Validation loss 0.010752920992672443 Accuracy 0.88330078125\n",
      "Iteration 40410 Training loss 0.004884060006588697 Validation loss 0.010725141502916813 Accuracy 0.8828125\n",
      "Iteration 40420 Training loss 0.004833349492400885 Validation loss 0.01075268816202879 Accuracy 0.8828125\n",
      "Iteration 40430 Training loss 0.0035740744788199663 Validation loss 0.01072012074291706 Accuracy 0.8828125\n",
      "Iteration 40440 Training loss 0.0038919257931411266 Validation loss 0.010774455033242702 Accuracy 0.880859375\n",
      "Iteration 40450 Training loss 0.00599195621907711 Validation loss 0.010711070150136948 Accuracy 0.8828125\n",
      "Iteration 40460 Training loss 0.0028160240035504103 Validation loss 0.010597653687000275 Accuracy 0.88427734375\n",
      "Iteration 40470 Training loss 0.003053441410884261 Validation loss 0.010670186951756477 Accuracy 0.8837890625\n",
      "Iteration 40480 Training loss 0.0043598804622888565 Validation loss 0.010593893937766552 Accuracy 0.88525390625\n",
      "Iteration 40490 Training loss 0.0030368443112820387 Validation loss 0.01075947005301714 Accuracy 0.8818359375\n",
      "Iteration 40500 Training loss 0.006393022369593382 Validation loss 0.010666681453585625 Accuracy 0.88330078125\n",
      "Iteration 40510 Training loss 0.004916577134281397 Validation loss 0.010829223319888115 Accuracy 0.880859375\n",
      "Iteration 40520 Training loss 0.0038465899415314198 Validation loss 0.010856416076421738 Accuracy 0.8818359375\n",
      "Iteration 40530 Training loss 0.0041648270562291145 Validation loss 0.010594676248729229 Accuracy 0.8837890625\n",
      "Iteration 40540 Training loss 0.005564094986766577 Validation loss 0.0112330736592412 Accuracy 0.87744140625\n",
      "Iteration 40550 Training loss 0.004149128217250109 Validation loss 0.010589106008410454 Accuracy 0.884765625\n",
      "Iteration 40560 Training loss 0.004844312556087971 Validation loss 0.010586344636976719 Accuracy 0.8837890625\n",
      "Iteration 40570 Training loss 0.005214841105043888 Validation loss 0.010702736675739288 Accuracy 0.8837890625\n",
      "Iteration 40580 Training loss 0.004747110418975353 Validation loss 0.010492898523807526 Accuracy 0.8857421875\n",
      "Iteration 40590 Training loss 0.004632321186363697 Validation loss 0.010503994300961494 Accuracy 0.8857421875\n",
      "Iteration 40600 Training loss 0.0041998024098575115 Validation loss 0.010669312439858913 Accuracy 0.8837890625\n",
      "Iteration 40610 Training loss 0.003863312304019928 Validation loss 0.010861862450838089 Accuracy 0.88134765625\n",
      "Iteration 40620 Training loss 0.0038726311177015305 Validation loss 0.010686006397008896 Accuracy 0.88330078125\n",
      "Iteration 40630 Training loss 0.0059390803799033165 Validation loss 0.01061741542071104 Accuracy 0.8837890625\n",
      "Iteration 40640 Training loss 0.004199751187115908 Validation loss 0.01065751537680626 Accuracy 0.8837890625\n",
      "Iteration 40650 Training loss 0.005324706435203552 Validation loss 0.010636324994266033 Accuracy 0.8837890625\n",
      "Iteration 40660 Training loss 0.004341743420809507 Validation loss 0.010598576627671719 Accuracy 0.8837890625\n",
      "Iteration 40670 Training loss 0.004925774410367012 Validation loss 0.01058078370988369 Accuracy 0.88427734375\n",
      "Iteration 40680 Training loss 0.004221579525619745 Validation loss 0.010522311553359032 Accuracy 0.88427734375\n",
      "Iteration 40690 Training loss 0.002780822804197669 Validation loss 0.010513082146644592 Accuracy 0.88427734375\n",
      "Iteration 40700 Training loss 0.00464183185249567 Validation loss 0.01053999736905098 Accuracy 0.884765625\n",
      "Iteration 40710 Training loss 0.004039193969219923 Validation loss 0.010884763672947884 Accuracy 0.880859375\n",
      "Iteration 40720 Training loss 0.00516018969938159 Validation loss 0.010749369859695435 Accuracy 0.88232421875\n",
      "Iteration 40730 Training loss 0.002409812994301319 Validation loss 0.010556108318269253 Accuracy 0.88330078125\n",
      "Iteration 40740 Training loss 0.004015525337308645 Validation loss 0.0106074633076787 Accuracy 0.88427734375\n",
      "Iteration 40750 Training loss 0.004911715630441904 Validation loss 0.010639485903084278 Accuracy 0.8837890625\n",
      "Iteration 40760 Training loss 0.004169039893895388 Validation loss 0.010647865943610668 Accuracy 0.8828125\n",
      "Iteration 40770 Training loss 0.0040251403115689754 Validation loss 0.010559199377894402 Accuracy 0.88427734375\n",
      "Iteration 40780 Training loss 0.0032809984404593706 Validation loss 0.010641288012266159 Accuracy 0.88330078125\n",
      "Iteration 40790 Training loss 0.0042300778441131115 Validation loss 0.010808410122990608 Accuracy 0.8818359375\n",
      "Iteration 40800 Training loss 0.0025558944325894117 Validation loss 0.010657810606062412 Accuracy 0.88330078125\n",
      "Iteration 40810 Training loss 0.0046351016499102116 Validation loss 0.01091479230672121 Accuracy 0.8818359375\n",
      "Iteration 40820 Training loss 0.003871686989441514 Validation loss 0.010648975148797035 Accuracy 0.8837890625\n",
      "Iteration 40830 Training loss 0.005000201985239983 Validation loss 0.010851813480257988 Accuracy 0.87939453125\n",
      "Iteration 40840 Training loss 0.005803107749670744 Validation loss 0.011103146709501743 Accuracy 0.87841796875\n",
      "Iteration 40850 Training loss 0.0043482412584125996 Validation loss 0.010766281746327877 Accuracy 0.8818359375\n",
      "Iteration 40860 Training loss 0.003977444488555193 Validation loss 0.010666738264262676 Accuracy 0.8837890625\n",
      "Iteration 40870 Training loss 0.0056337094865739346 Validation loss 0.010618808679282665 Accuracy 0.88427734375\n",
      "Iteration 40880 Training loss 0.0037146382965147495 Validation loss 0.010712302289903164 Accuracy 0.88330078125\n",
      "Iteration 40890 Training loss 0.005665259435772896 Validation loss 0.010577436536550522 Accuracy 0.884765625\n",
      "Iteration 40900 Training loss 0.00477595953270793 Validation loss 0.010703986510634422 Accuracy 0.8837890625\n",
      "Iteration 40910 Training loss 0.004540568217635155 Validation loss 0.010636967606842518 Accuracy 0.88525390625\n",
      "Iteration 40920 Training loss 0.0031378609128296375 Validation loss 0.010659257881343365 Accuracy 0.8857421875\n",
      "Iteration 40930 Training loss 0.0054296404123306274 Validation loss 0.010619103908538818 Accuracy 0.8837890625\n",
      "Iteration 40940 Training loss 0.0062735723331570625 Validation loss 0.010751643218100071 Accuracy 0.8828125\n",
      "Iteration 40950 Training loss 0.003933390136808157 Validation loss 0.010780805721879005 Accuracy 0.8828125\n",
      "Iteration 40960 Training loss 0.004157511983066797 Validation loss 0.01076678279787302 Accuracy 0.88232421875\n",
      "Iteration 40970 Training loss 0.004943415056914091 Validation loss 0.010597185231745243 Accuracy 0.88427734375\n",
      "Iteration 40980 Training loss 0.004866198170930147 Validation loss 0.010714400559663773 Accuracy 0.88232421875\n",
      "Iteration 40990 Training loss 0.005518716294318438 Validation loss 0.010705170221626759 Accuracy 0.8828125\n",
      "Iteration 41000 Training loss 0.0045676822774112225 Validation loss 0.010933959856629372 Accuracy 0.880859375\n",
      "Iteration 41010 Training loss 0.0032445755787193775 Validation loss 0.010806520469486713 Accuracy 0.88134765625\n",
      "Iteration 41020 Training loss 0.003268628381192684 Validation loss 0.010711387731134892 Accuracy 0.88330078125\n",
      "Iteration 41030 Training loss 0.005098517052829266 Validation loss 0.010871288366615772 Accuracy 0.88134765625\n",
      "Iteration 41040 Training loss 0.004961447324603796 Validation loss 0.010683688335120678 Accuracy 0.88330078125\n",
      "Iteration 41050 Training loss 0.003739103674888611 Validation loss 0.010661909356713295 Accuracy 0.8837890625\n",
      "Iteration 41060 Training loss 0.005557958036661148 Validation loss 0.010593966580927372 Accuracy 0.88330078125\n",
      "Iteration 41070 Training loss 0.004507163073867559 Validation loss 0.010724293999373913 Accuracy 0.88330078125\n",
      "Iteration 41080 Training loss 0.0036293191369622946 Validation loss 0.010760015808045864 Accuracy 0.8828125\n",
      "Iteration 41090 Training loss 0.004506637342274189 Validation loss 0.010596479289233685 Accuracy 0.88232421875\n",
      "Iteration 41100 Training loss 0.004740568809211254 Validation loss 0.010589223355054855 Accuracy 0.88427734375\n",
      "Iteration 41110 Training loss 0.0059624542482197285 Validation loss 0.01067559514194727 Accuracy 0.8837890625\n",
      "Iteration 41120 Training loss 0.004379835911095142 Validation loss 0.010552714578807354 Accuracy 0.884765625\n",
      "Iteration 41130 Training loss 0.004366687498986721 Validation loss 0.010647309012711048 Accuracy 0.8837890625\n",
      "Iteration 41140 Training loss 0.005314262118190527 Validation loss 0.010695488192141056 Accuracy 0.88330078125\n",
      "Iteration 41150 Training loss 0.003972718957811594 Validation loss 0.010612825863063335 Accuracy 0.88330078125\n",
      "Iteration 41160 Training loss 0.004482068121433258 Validation loss 0.01060808077454567 Accuracy 0.88427734375\n",
      "Iteration 41170 Training loss 0.003371561411768198 Validation loss 0.010617980733513832 Accuracy 0.8837890625\n",
      "Iteration 41180 Training loss 0.003174267476424575 Validation loss 0.010771771892905235 Accuracy 0.88232421875\n",
      "Iteration 41190 Training loss 0.0036051839124411345 Validation loss 0.010602080263197422 Accuracy 0.884765625\n",
      "Iteration 41200 Training loss 0.0030474739614874125 Validation loss 0.010819233022630215 Accuracy 0.88330078125\n",
      "Iteration 41210 Training loss 0.006314991507679224 Validation loss 0.010652152821421623 Accuracy 0.8828125\n",
      "Iteration 41220 Training loss 0.004295333754271269 Validation loss 0.010613002814352512 Accuracy 0.88330078125\n",
      "Iteration 41230 Training loss 0.004572478588670492 Validation loss 0.010572608560323715 Accuracy 0.88427734375\n",
      "Iteration 41240 Training loss 0.0038030489813536406 Validation loss 0.010499422438442707 Accuracy 0.884765625\n",
      "Iteration 41250 Training loss 0.0033303597010672092 Validation loss 0.01061147078871727 Accuracy 0.8837890625\n",
      "Iteration 41260 Training loss 0.0047025298699736595 Validation loss 0.01059736404567957 Accuracy 0.8837890625\n",
      "Iteration 41270 Training loss 0.0026927997823804617 Validation loss 0.010770147666335106 Accuracy 0.88134765625\n",
      "Iteration 41280 Training loss 0.005479188170284033 Validation loss 0.010567199438810349 Accuracy 0.88427734375\n",
      "Iteration 41290 Training loss 0.005572853609919548 Validation loss 0.010592189617455006 Accuracy 0.8837890625\n",
      "Iteration 41300 Training loss 0.004029068164527416 Validation loss 0.010589446872472763 Accuracy 0.88427734375\n",
      "Iteration 41310 Training loss 0.004063465166836977 Validation loss 0.010614308528602123 Accuracy 0.88427734375\n",
      "Iteration 41320 Training loss 0.005511992610991001 Validation loss 0.010778568685054779 Accuracy 0.88134765625\n",
      "Iteration 41330 Training loss 0.0032281430903822184 Validation loss 0.010668384842574596 Accuracy 0.8828125\n",
      "Iteration 41340 Training loss 0.0038612480275332928 Validation loss 0.010704116895794868 Accuracy 0.88232421875\n",
      "Iteration 41350 Training loss 0.0036566562484949827 Validation loss 0.010710719041526318 Accuracy 0.88330078125\n",
      "Iteration 41360 Training loss 0.004216797184199095 Validation loss 0.01055054645985365 Accuracy 0.884765625\n",
      "Iteration 41370 Training loss 0.0037596377078443766 Validation loss 0.010641034692525864 Accuracy 0.8837890625\n",
      "Iteration 41380 Training loss 0.0031393857207149267 Validation loss 0.01063513197004795 Accuracy 0.8828125\n",
      "Iteration 41390 Training loss 0.0034972107969224453 Validation loss 0.010780024342238903 Accuracy 0.88232421875\n",
      "Iteration 41400 Training loss 0.004646006505936384 Validation loss 0.010650050826370716 Accuracy 0.88330078125\n",
      "Iteration 41410 Training loss 0.00416756933555007 Validation loss 0.01069908682256937 Accuracy 0.8828125\n",
      "Iteration 41420 Training loss 0.00409952225163579 Validation loss 0.010718497447669506 Accuracy 0.8828125\n",
      "Iteration 41430 Training loss 0.003934141714125872 Validation loss 0.010657643899321556 Accuracy 0.88330078125\n",
      "Iteration 41440 Training loss 0.005107081960886717 Validation loss 0.010694600641727448 Accuracy 0.8828125\n",
      "Iteration 41450 Training loss 0.005785340443253517 Validation loss 0.010805231519043446 Accuracy 0.88134765625\n",
      "Iteration 41460 Training loss 0.0043381378054618835 Validation loss 0.010543471202254295 Accuracy 0.88525390625\n",
      "Iteration 41470 Training loss 0.004578649997711182 Validation loss 0.010585644282400608 Accuracy 0.8837890625\n",
      "Iteration 41480 Training loss 0.004715454764664173 Validation loss 0.010617265477776527 Accuracy 0.8837890625\n",
      "Iteration 41490 Training loss 0.0056199971586465836 Validation loss 0.01070435717701912 Accuracy 0.88330078125\n",
      "Iteration 41500 Training loss 0.0028427550569176674 Validation loss 0.010867166332900524 Accuracy 0.8818359375\n",
      "Iteration 41510 Training loss 0.003158586099743843 Validation loss 0.010587631724774837 Accuracy 0.884765625\n",
      "Iteration 41520 Training loss 0.004203401040285826 Validation loss 0.010720893740653992 Accuracy 0.8828125\n",
      "Iteration 41530 Training loss 0.00433340622112155 Validation loss 0.010714494623243809 Accuracy 0.88232421875\n",
      "Iteration 41540 Training loss 0.004561657086014748 Validation loss 0.010585824958980083 Accuracy 0.884765625\n",
      "Iteration 41550 Training loss 0.003512165741994977 Validation loss 0.010623482055962086 Accuracy 0.8837890625\n",
      "Iteration 41560 Training loss 0.0033513063099235296 Validation loss 0.010716322809457779 Accuracy 0.88232421875\n",
      "Iteration 41570 Training loss 0.003920918796211481 Validation loss 0.01078816782683134 Accuracy 0.88134765625\n",
      "Iteration 41580 Training loss 0.003850572509691119 Validation loss 0.010699675418436527 Accuracy 0.88232421875\n",
      "Iteration 41590 Training loss 0.003606628393754363 Validation loss 0.010653208009898663 Accuracy 0.8837890625\n",
      "Iteration 41600 Training loss 0.0036076491232961416 Validation loss 0.010721397586166859 Accuracy 0.8837890625\n",
      "Iteration 41610 Training loss 0.004477518144994974 Validation loss 0.01069069467484951 Accuracy 0.8828125\n",
      "Iteration 41620 Training loss 0.004296023864299059 Validation loss 0.010737555101513863 Accuracy 0.88330078125\n",
      "Iteration 41630 Training loss 0.005513833835721016 Validation loss 0.010740802623331547 Accuracy 0.88232421875\n",
      "Iteration 41640 Training loss 0.005585407372564077 Validation loss 0.010762382298707962 Accuracy 0.88134765625\n",
      "Iteration 41650 Training loss 0.0035734716802835464 Validation loss 0.010602395050227642 Accuracy 0.88427734375\n",
      "Iteration 41660 Training loss 0.0032323149498552084 Validation loss 0.01070618350058794 Accuracy 0.8828125\n",
      "Iteration 41670 Training loss 0.004487378057092428 Validation loss 0.010593556798994541 Accuracy 0.884765625\n",
      "Iteration 41680 Training loss 0.0046390327624976635 Validation loss 0.010693288408219814 Accuracy 0.8837890625\n",
      "Iteration 41690 Training loss 0.006280358415096998 Validation loss 0.010790654458105564 Accuracy 0.8828125\n",
      "Iteration 41700 Training loss 0.004882296081632376 Validation loss 0.010732967406511307 Accuracy 0.8818359375\n",
      "Iteration 41710 Training loss 0.0027679188642650843 Validation loss 0.01065759640187025 Accuracy 0.88330078125\n",
      "Iteration 41720 Training loss 0.003736307844519615 Validation loss 0.010985944420099258 Accuracy 0.88037109375\n",
      "Iteration 41730 Training loss 0.0039042236749082804 Validation loss 0.01079853530973196 Accuracy 0.88134765625\n",
      "Iteration 41740 Training loss 0.0032742468174546957 Validation loss 0.010724706575274467 Accuracy 0.88232421875\n",
      "Iteration 41750 Training loss 0.004570730961859226 Validation loss 0.010642394423484802 Accuracy 0.88427734375\n",
      "Iteration 41760 Training loss 0.0034131037537008524 Validation loss 0.010667354799807072 Accuracy 0.8828125\n",
      "Iteration 41770 Training loss 0.004100293852388859 Validation loss 0.010790456086397171 Accuracy 0.880859375\n",
      "Iteration 41780 Training loss 0.004451630637049675 Validation loss 0.01061947364360094 Accuracy 0.88330078125\n",
      "Iteration 41790 Training loss 0.004905063658952713 Validation loss 0.010702932253479958 Accuracy 0.8828125\n",
      "Iteration 41800 Training loss 0.0036158913280814886 Validation loss 0.010657821781933308 Accuracy 0.8837890625\n",
      "Iteration 41810 Training loss 0.005110355094075203 Validation loss 0.010959290899336338 Accuracy 0.87939453125\n",
      "Iteration 41820 Training loss 0.004387283232063055 Validation loss 0.010569478385150433 Accuracy 0.88427734375\n",
      "Iteration 41830 Training loss 0.003773069242015481 Validation loss 0.01060376688838005 Accuracy 0.8837890625\n",
      "Iteration 41840 Training loss 0.0034967234823852777 Validation loss 0.01056433841586113 Accuracy 0.8837890625\n",
      "Iteration 41850 Training loss 0.004136919509619474 Validation loss 0.010602741502225399 Accuracy 0.884765625\n",
      "Iteration 41860 Training loss 0.0026253508403897285 Validation loss 0.010602819733321667 Accuracy 0.8837890625\n",
      "Iteration 41870 Training loss 0.003285973099991679 Validation loss 0.010532140731811523 Accuracy 0.884765625\n",
      "Iteration 41880 Training loss 0.004652874078601599 Validation loss 0.010549481026828289 Accuracy 0.88427734375\n",
      "Iteration 41890 Training loss 0.0031963051296770573 Validation loss 0.010679301805794239 Accuracy 0.8828125\n",
      "Iteration 41900 Training loss 0.003710829420015216 Validation loss 0.010651938617229462 Accuracy 0.884765625\n",
      "Iteration 41910 Training loss 0.0035451792646199465 Validation loss 0.010672022588551044 Accuracy 0.88330078125\n",
      "Iteration 41920 Training loss 0.003615147201344371 Validation loss 0.01059493888169527 Accuracy 0.88427734375\n",
      "Iteration 41930 Training loss 0.0030898943077772856 Validation loss 0.01053636334836483 Accuracy 0.88525390625\n",
      "Iteration 41940 Training loss 0.004163013305515051 Validation loss 0.010572993196547031 Accuracy 0.88427734375\n",
      "Iteration 41950 Training loss 0.004658405669033527 Validation loss 0.010537700727581978 Accuracy 0.8837890625\n",
      "Iteration 41960 Training loss 0.00402194494381547 Validation loss 0.010414536111056805 Accuracy 0.88525390625\n",
      "Iteration 41970 Training loss 0.003578832605853677 Validation loss 0.010415690951049328 Accuracy 0.8857421875\n",
      "Iteration 41980 Training loss 0.004069878254085779 Validation loss 0.010654034093022346 Accuracy 0.8837890625\n",
      "Iteration 41990 Training loss 0.005200882907956839 Validation loss 0.010541105642914772 Accuracy 0.88623046875\n",
      "Iteration 42000 Training loss 0.004831709899008274 Validation loss 0.0106941694393754 Accuracy 0.8828125\n",
      "Iteration 42010 Training loss 0.005165359936654568 Validation loss 0.010611566714942455 Accuracy 0.88427734375\n",
      "Iteration 42020 Training loss 0.0056001897901296616 Validation loss 0.010609671473503113 Accuracy 0.88427734375\n",
      "Iteration 42030 Training loss 0.004939253907650709 Validation loss 0.010538262315094471 Accuracy 0.88427734375\n",
      "Iteration 42040 Training loss 0.0029786396771669388 Validation loss 0.01065019704401493 Accuracy 0.8828125\n",
      "Iteration 42050 Training loss 0.004526454024016857 Validation loss 0.010467680171132088 Accuracy 0.8857421875\n",
      "Iteration 42060 Training loss 0.005023372359573841 Validation loss 0.010538161732256413 Accuracy 0.884765625\n",
      "Iteration 42070 Training loss 0.004840352106839418 Validation loss 0.010648493655025959 Accuracy 0.88330078125\n",
      "Iteration 42080 Training loss 0.004695845767855644 Validation loss 0.010731051675975323 Accuracy 0.88330078125\n",
      "Iteration 42090 Training loss 0.003415892133489251 Validation loss 0.010670448653399944 Accuracy 0.88330078125\n",
      "Iteration 42100 Training loss 0.003517661476507783 Validation loss 0.010730421170592308 Accuracy 0.88232421875\n",
      "Iteration 42110 Training loss 0.004195441957563162 Validation loss 0.010751161724328995 Accuracy 0.8818359375\n",
      "Iteration 42120 Training loss 0.003064650110900402 Validation loss 0.010654292069375515 Accuracy 0.88427734375\n",
      "Iteration 42130 Training loss 0.004756329581141472 Validation loss 0.010560207068920135 Accuracy 0.8837890625\n",
      "Iteration 42140 Training loss 0.0029452871531248093 Validation loss 0.010713496245443821 Accuracy 0.88232421875\n",
      "Iteration 42150 Training loss 0.0038914047181606293 Validation loss 0.010723857209086418 Accuracy 0.8818359375\n",
      "Iteration 42160 Training loss 0.0032698770519346 Validation loss 0.010519084520637989 Accuracy 0.8857421875\n",
      "Iteration 42170 Training loss 0.003945095930248499 Validation loss 0.010633778758347034 Accuracy 0.8837890625\n",
      "Iteration 42180 Training loss 0.00455028610303998 Validation loss 0.010657761245965958 Accuracy 0.8828125\n",
      "Iteration 42190 Training loss 0.0043377564288675785 Validation loss 0.010651735588908195 Accuracy 0.88330078125\n",
      "Iteration 42200 Training loss 0.0035795599687844515 Validation loss 0.010660908184945583 Accuracy 0.88330078125\n",
      "Iteration 42210 Training loss 0.0048516676761209965 Validation loss 0.010599367320537567 Accuracy 0.88427734375\n",
      "Iteration 42220 Training loss 0.005109882913529873 Validation loss 0.010868045501410961 Accuracy 0.88037109375\n",
      "Iteration 42230 Training loss 0.0030580905731767416 Validation loss 0.010626278817653656 Accuracy 0.88330078125\n",
      "Iteration 42240 Training loss 0.00523000955581665 Validation loss 0.010484379716217518 Accuracy 0.8857421875\n",
      "Iteration 42250 Training loss 0.004783958196640015 Validation loss 0.010626076720654964 Accuracy 0.8837890625\n",
      "Iteration 42260 Training loss 0.004813354462385178 Validation loss 0.010797707363963127 Accuracy 0.8828125\n",
      "Iteration 42270 Training loss 0.00509291049093008 Validation loss 0.010717423632740974 Accuracy 0.88232421875\n",
      "Iteration 42280 Training loss 0.0035433333832770586 Validation loss 0.010610261000692844 Accuracy 0.884765625\n",
      "Iteration 42290 Training loss 0.004144694656133652 Validation loss 0.010734020732343197 Accuracy 0.8828125\n",
      "Iteration 42300 Training loss 0.0036507060285657644 Validation loss 0.010743278078734875 Accuracy 0.8818359375\n",
      "Iteration 42310 Training loss 0.003718818072229624 Validation loss 0.010762998834252357 Accuracy 0.88330078125\n",
      "Iteration 42320 Training loss 0.0038892796728760004 Validation loss 0.010707414709031582 Accuracy 0.8837890625\n",
      "Iteration 42330 Training loss 0.0031195348128676414 Validation loss 0.010716971009969711 Accuracy 0.88134765625\n",
      "Iteration 42340 Training loss 0.003930072765797377 Validation loss 0.010654667392373085 Accuracy 0.8828125\n",
      "Iteration 42350 Training loss 0.0042298766784369946 Validation loss 0.010609193705022335 Accuracy 0.88330078125\n",
      "Iteration 42360 Training loss 0.0035488756839185953 Validation loss 0.010509208776056767 Accuracy 0.88427734375\n",
      "Iteration 42370 Training loss 0.004291145130991936 Validation loss 0.010594869963824749 Accuracy 0.8837890625\n",
      "Iteration 42380 Training loss 0.0033115274272859097 Validation loss 0.010446876287460327 Accuracy 0.88623046875\n",
      "Iteration 42390 Training loss 0.0032499865628778934 Validation loss 0.010614014230668545 Accuracy 0.8828125\n",
      "Iteration 42400 Training loss 0.004163943696767092 Validation loss 0.010710752569139004 Accuracy 0.8828125\n",
      "Iteration 42410 Training loss 0.005472397897392511 Validation loss 0.010605189017951488 Accuracy 0.8837890625\n",
      "Iteration 42420 Training loss 0.0040250313468277454 Validation loss 0.0106367077678442 Accuracy 0.8837890625\n",
      "Iteration 42430 Training loss 0.003957030363380909 Validation loss 0.010905124247074127 Accuracy 0.88037109375\n",
      "Iteration 42440 Training loss 0.0033273822627961636 Validation loss 0.010541419498622417 Accuracy 0.88525390625\n",
      "Iteration 42450 Training loss 0.0036913680378347635 Validation loss 0.010559850372374058 Accuracy 0.88525390625\n",
      "Iteration 42460 Training loss 0.003577165538445115 Validation loss 0.010677911341190338 Accuracy 0.8837890625\n",
      "Iteration 42470 Training loss 0.0025475025177001953 Validation loss 0.010552891530096531 Accuracy 0.8857421875\n",
      "Iteration 42480 Training loss 0.003689748467877507 Validation loss 0.010793625377118587 Accuracy 0.8818359375\n",
      "Iteration 42490 Training loss 0.0043186345137655735 Validation loss 0.010759787634015083 Accuracy 0.8828125\n",
      "Iteration 42500 Training loss 0.003991461358964443 Validation loss 0.010625222697854042 Accuracy 0.88427734375\n",
      "Iteration 42510 Training loss 0.004158440977334976 Validation loss 0.010502055287361145 Accuracy 0.88525390625\n",
      "Iteration 42520 Training loss 0.0027470302302390337 Validation loss 0.010512207634747028 Accuracy 0.88525390625\n",
      "Iteration 42530 Training loss 0.004887822549790144 Validation loss 0.010508769191801548 Accuracy 0.88427734375\n",
      "Iteration 42540 Training loss 0.003051475156098604 Validation loss 0.010540513321757317 Accuracy 0.88525390625\n",
      "Iteration 42550 Training loss 0.0028081941418349743 Validation loss 0.010535581037402153 Accuracy 0.88525390625\n",
      "Iteration 42560 Training loss 0.0046476745046675205 Validation loss 0.010558407753705978 Accuracy 0.8837890625\n",
      "Iteration 42570 Training loss 0.004058023449033499 Validation loss 0.010683164931833744 Accuracy 0.88232421875\n",
      "Iteration 42580 Training loss 0.003366628661751747 Validation loss 0.010445522144436836 Accuracy 0.8857421875\n",
      "Iteration 42590 Training loss 0.004019059240818024 Validation loss 0.010644950903952122 Accuracy 0.88427734375\n",
      "Iteration 42600 Training loss 0.0038673137314617634 Validation loss 0.01055412832647562 Accuracy 0.88427734375\n",
      "Iteration 42610 Training loss 0.002869048621505499 Validation loss 0.010478801093995571 Accuracy 0.88623046875\n",
      "Iteration 42620 Training loss 0.0036346956621855497 Validation loss 0.010607114061713219 Accuracy 0.884765625\n",
      "Iteration 42630 Training loss 0.0040349699556827545 Validation loss 0.010500687174499035 Accuracy 0.884765625\n",
      "Iteration 42640 Training loss 0.0033946307376027107 Validation loss 0.010506656020879745 Accuracy 0.88525390625\n",
      "Iteration 42650 Training loss 0.005642608739435673 Validation loss 0.010593444108963013 Accuracy 0.884765625\n",
      "Iteration 42660 Training loss 0.0034117342438548803 Validation loss 0.01063018199056387 Accuracy 0.88330078125\n",
      "Iteration 42670 Training loss 0.004058488178998232 Validation loss 0.010481511242687702 Accuracy 0.884765625\n",
      "Iteration 42680 Training loss 0.0033547887578606606 Validation loss 0.010553818196058273 Accuracy 0.88427734375\n",
      "Iteration 42690 Training loss 0.005815116688609123 Validation loss 0.01086107362061739 Accuracy 0.88037109375\n",
      "Iteration 42700 Training loss 0.004063596483319998 Validation loss 0.01080715749412775 Accuracy 0.880859375\n",
      "Iteration 42710 Training loss 0.003825107589364052 Validation loss 0.010604151524603367 Accuracy 0.88427734375\n",
      "Iteration 42720 Training loss 0.00489305704832077 Validation loss 0.010584782809019089 Accuracy 0.8837890625\n",
      "Iteration 42730 Training loss 0.003216723445802927 Validation loss 0.010634766891598701 Accuracy 0.8828125\n",
      "Iteration 42740 Training loss 0.0033678391482681036 Validation loss 0.010550486855208874 Accuracy 0.884765625\n",
      "Iteration 42750 Training loss 0.004024932160973549 Validation loss 0.010492688044905663 Accuracy 0.88525390625\n",
      "Iteration 42760 Training loss 0.004355865530669689 Validation loss 0.010416540317237377 Accuracy 0.8857421875\n",
      "Iteration 42770 Training loss 0.004596387967467308 Validation loss 0.010586723685264587 Accuracy 0.8828125\n",
      "Iteration 42780 Training loss 0.004089570138603449 Validation loss 0.010436728596687317 Accuracy 0.8857421875\n",
      "Iteration 42790 Training loss 0.0023835517931729555 Validation loss 0.010458815842866898 Accuracy 0.88623046875\n",
      "Iteration 42800 Training loss 0.004798626061528921 Validation loss 0.010417644865810871 Accuracy 0.884765625\n",
      "Iteration 42810 Training loss 0.006226619705557823 Validation loss 0.010460905730724335 Accuracy 0.8837890625\n",
      "Iteration 42820 Training loss 0.0037576663307845592 Validation loss 0.010491302236914635 Accuracy 0.884765625\n",
      "Iteration 42830 Training loss 0.00418603140860796 Validation loss 0.01055740937590599 Accuracy 0.884765625\n",
      "Iteration 42840 Training loss 0.004641061183065176 Validation loss 0.010477307252585888 Accuracy 0.8857421875\n",
      "Iteration 42850 Training loss 0.0023299360182136297 Validation loss 0.010539024136960506 Accuracy 0.884765625\n",
      "Iteration 42860 Training loss 0.004620739724487066 Validation loss 0.010522213764488697 Accuracy 0.884765625\n",
      "Iteration 42870 Training loss 0.0043211812153458595 Validation loss 0.010557463392615318 Accuracy 0.8837890625\n",
      "Iteration 42880 Training loss 0.004295744467526674 Validation loss 0.010502902790904045 Accuracy 0.8857421875\n",
      "Iteration 42890 Training loss 0.003962383139878511 Validation loss 0.010731862857937813 Accuracy 0.88427734375\n",
      "Iteration 42900 Training loss 0.003971129190176725 Validation loss 0.010521689429879189 Accuracy 0.88427734375\n",
      "Iteration 42910 Training loss 0.004333896562457085 Validation loss 0.010422725230455399 Accuracy 0.88671875\n",
      "Iteration 42920 Training loss 0.003851762041449547 Validation loss 0.01046756561845541 Accuracy 0.88623046875\n",
      "Iteration 42930 Training loss 0.0034378375858068466 Validation loss 0.010630407370626926 Accuracy 0.8837890625\n",
      "Iteration 42940 Training loss 0.0044633205980062485 Validation loss 0.010471593588590622 Accuracy 0.88427734375\n",
      "Iteration 42950 Training loss 0.004334855824708939 Validation loss 0.010696405544877052 Accuracy 0.8828125\n",
      "Iteration 42960 Training loss 0.006171856075525284 Validation loss 0.01073395274579525 Accuracy 0.88232421875\n",
      "Iteration 42970 Training loss 0.0034945153165608644 Validation loss 0.010494736954569817 Accuracy 0.88525390625\n",
      "Iteration 42980 Training loss 0.004535998683422804 Validation loss 0.01059463620185852 Accuracy 0.88427734375\n",
      "Iteration 42990 Training loss 0.005290348082780838 Validation loss 0.010488942265510559 Accuracy 0.884765625\n",
      "Iteration 43000 Training loss 0.004074321128427982 Validation loss 0.010555119253695011 Accuracy 0.8837890625\n",
      "Iteration 43010 Training loss 0.004540803376585245 Validation loss 0.010471336543560028 Accuracy 0.88623046875\n",
      "Iteration 43020 Training loss 0.004189009312540293 Validation loss 0.010506387799978256 Accuracy 0.88427734375\n",
      "Iteration 43030 Training loss 0.0034833799581974745 Validation loss 0.01058634277433157 Accuracy 0.88427734375\n",
      "Iteration 43040 Training loss 0.001631816034205258 Validation loss 0.010646933689713478 Accuracy 0.88427734375\n",
      "Iteration 43050 Training loss 0.003289204789325595 Validation loss 0.010600660927593708 Accuracy 0.884765625\n",
      "Iteration 43060 Training loss 0.004148642998188734 Validation loss 0.010591133497655392 Accuracy 0.8857421875\n",
      "Iteration 43070 Training loss 0.002784880343824625 Validation loss 0.010583354160189629 Accuracy 0.884765625\n",
      "Iteration 43080 Training loss 0.0032045068219304085 Validation loss 0.010552012361586094 Accuracy 0.88525390625\n",
      "Iteration 43090 Training loss 0.0034305325243622065 Validation loss 0.010443305596709251 Accuracy 0.88525390625\n",
      "Iteration 43100 Training loss 0.004029822535812855 Validation loss 0.01044869888573885 Accuracy 0.88525390625\n",
      "Iteration 43110 Training loss 0.005129432771354914 Validation loss 0.010514216497540474 Accuracy 0.88525390625\n",
      "Iteration 43120 Training loss 0.005534193478524685 Validation loss 0.010608630254864693 Accuracy 0.88427734375\n",
      "Iteration 43130 Training loss 0.0029155181255191565 Validation loss 0.01055203191936016 Accuracy 0.88427734375\n",
      "Iteration 43140 Training loss 0.0054946159943938255 Validation loss 0.010576658882200718 Accuracy 0.8837890625\n",
      "Iteration 43150 Training loss 0.00595715269446373 Validation loss 0.010619201697409153 Accuracy 0.88330078125\n",
      "Iteration 43160 Training loss 0.0036028798203915358 Validation loss 0.010749220848083496 Accuracy 0.88232421875\n",
      "Iteration 43170 Training loss 0.003400364425033331 Validation loss 0.010465464554727077 Accuracy 0.8857421875\n",
      "Iteration 43180 Training loss 0.0036728803534060717 Validation loss 0.0104218116030097 Accuracy 0.88623046875\n",
      "Iteration 43190 Training loss 0.0035527516156435013 Validation loss 0.010539325885474682 Accuracy 0.884765625\n",
      "Iteration 43200 Training loss 0.003591147717088461 Validation loss 0.010661588981747627 Accuracy 0.88330078125\n",
      "Iteration 43210 Training loss 0.004376570228487253 Validation loss 0.010636802762746811 Accuracy 0.8828125\n",
      "Iteration 43220 Training loss 0.004040402825921774 Validation loss 0.010577292181551456 Accuracy 0.8828125\n",
      "Iteration 43230 Training loss 0.004784110467880964 Validation loss 0.010492893867194653 Accuracy 0.884765625\n",
      "Iteration 43240 Training loss 0.005562040954828262 Validation loss 0.010518512688577175 Accuracy 0.884765625\n",
      "Iteration 43250 Training loss 0.003908295184373856 Validation loss 0.010503645054996014 Accuracy 0.88427734375\n",
      "Iteration 43260 Training loss 0.004591342061758041 Validation loss 0.010517998598515987 Accuracy 0.88427734375\n",
      "Iteration 43270 Training loss 0.004382873419672251 Validation loss 0.010401525534689426 Accuracy 0.88623046875\n",
      "Iteration 43280 Training loss 0.005117103923112154 Validation loss 0.010472826659679413 Accuracy 0.8857421875\n",
      "Iteration 43290 Training loss 0.003060939023271203 Validation loss 0.010465585626661777 Accuracy 0.88427734375\n",
      "Iteration 43300 Training loss 0.003662544535472989 Validation loss 0.010617855936288834 Accuracy 0.88330078125\n",
      "Iteration 43310 Training loss 0.004170915111899376 Validation loss 0.01063328143209219 Accuracy 0.8828125\n",
      "Iteration 43320 Training loss 0.0033097462728619576 Validation loss 0.010549384169280529 Accuracy 0.884765625\n",
      "Iteration 43330 Training loss 0.0035643179435282946 Validation loss 0.010535632260143757 Accuracy 0.88525390625\n",
      "Iteration 43340 Training loss 0.002731343964114785 Validation loss 0.010842970572412014 Accuracy 0.880859375\n",
      "Iteration 43350 Training loss 0.004014153964817524 Validation loss 0.010540546849370003 Accuracy 0.884765625\n",
      "Iteration 43360 Training loss 0.004247615113854408 Validation loss 0.010404541157186031 Accuracy 0.88671875\n",
      "Iteration 43370 Training loss 0.0048668566159904 Validation loss 0.010534667409956455 Accuracy 0.884765625\n",
      "Iteration 43380 Training loss 0.00345028517767787 Validation loss 0.01062226016074419 Accuracy 0.8828125\n",
      "Iteration 43390 Training loss 0.0036318458151072264 Validation loss 0.010536025278270245 Accuracy 0.88427734375\n",
      "Iteration 43400 Training loss 0.0029989241156727076 Validation loss 0.010595463216304779 Accuracy 0.88232421875\n",
      "Iteration 43410 Training loss 0.0029728699009865522 Validation loss 0.010464447550475597 Accuracy 0.8857421875\n",
      "Iteration 43420 Training loss 0.004596141632646322 Validation loss 0.010542870499193668 Accuracy 0.884765625\n",
      "Iteration 43430 Training loss 0.005317287985235453 Validation loss 0.010543796233832836 Accuracy 0.88427734375\n",
      "Iteration 43440 Training loss 0.004074905999004841 Validation loss 0.01053242664784193 Accuracy 0.8857421875\n",
      "Iteration 43450 Training loss 0.004220748320221901 Validation loss 0.010561159811913967 Accuracy 0.8857421875\n",
      "Iteration 43460 Training loss 0.004002738744020462 Validation loss 0.01050572469830513 Accuracy 0.88525390625\n",
      "Iteration 43470 Training loss 0.004274565726518631 Validation loss 0.010581869632005692 Accuracy 0.884765625\n",
      "Iteration 43480 Training loss 0.004031357821077108 Validation loss 0.010523676872253418 Accuracy 0.884765625\n",
      "Iteration 43490 Training loss 0.005041732452809811 Validation loss 0.010590658523142338 Accuracy 0.88427734375\n",
      "Iteration 43500 Training loss 0.0037971711717545986 Validation loss 0.010645618662238121 Accuracy 0.88427734375\n",
      "Iteration 43510 Training loss 0.004686197265982628 Validation loss 0.010636975057423115 Accuracy 0.88330078125\n",
      "Iteration 43520 Training loss 0.003980537876486778 Validation loss 0.010767897590994835 Accuracy 0.88330078125\n",
      "Iteration 43530 Training loss 0.0028202824760228395 Validation loss 0.010489636100828648 Accuracy 0.88525390625\n",
      "Iteration 43540 Training loss 0.0022036167792975903 Validation loss 0.01049790345132351 Accuracy 0.884765625\n",
      "Iteration 43550 Training loss 0.004595757927745581 Validation loss 0.010528902523219585 Accuracy 0.884765625\n",
      "Iteration 43560 Training loss 0.002272811019793153 Validation loss 0.010484814643859863 Accuracy 0.88623046875\n",
      "Iteration 43570 Training loss 0.0038634815718978643 Validation loss 0.010530529543757439 Accuracy 0.88525390625\n",
      "Iteration 43580 Training loss 0.0050338502041995525 Validation loss 0.010505527257919312 Accuracy 0.88525390625\n",
      "Iteration 43590 Training loss 0.00403706356883049 Validation loss 0.010440751910209656 Accuracy 0.88720703125\n",
      "Iteration 43600 Training loss 0.0038268142379820347 Validation loss 0.01061516534537077 Accuracy 0.884765625\n",
      "Iteration 43610 Training loss 0.003513673786073923 Validation loss 0.010591003112494946 Accuracy 0.88427734375\n",
      "Iteration 43620 Training loss 0.004444717429578304 Validation loss 0.01052065659314394 Accuracy 0.8857421875\n",
      "Iteration 43630 Training loss 0.003163732122629881 Validation loss 0.010693284682929516 Accuracy 0.8818359375\n",
      "Iteration 43640 Training loss 0.004383263178169727 Validation loss 0.010571276769042015 Accuracy 0.88525390625\n",
      "Iteration 43650 Training loss 0.00447711069136858 Validation loss 0.01050554309040308 Accuracy 0.8857421875\n",
      "Iteration 43660 Training loss 0.0034569117706269026 Validation loss 0.010567811317741871 Accuracy 0.8837890625\n",
      "Iteration 43670 Training loss 0.004904048517346382 Validation loss 0.010515138506889343 Accuracy 0.88720703125\n",
      "Iteration 43680 Training loss 0.0033226388040930033 Validation loss 0.010596735402941704 Accuracy 0.884765625\n",
      "Iteration 43690 Training loss 0.002960169455036521 Validation loss 0.010466174222528934 Accuracy 0.88623046875\n",
      "Iteration 43700 Training loss 0.0033382419496774673 Validation loss 0.010543720796704292 Accuracy 0.88427734375\n",
      "Iteration 43710 Training loss 0.00409362930804491 Validation loss 0.010469337925314903 Accuracy 0.8857421875\n",
      "Iteration 43720 Training loss 0.004283074755221605 Validation loss 0.010639120824635029 Accuracy 0.88427734375\n",
      "Iteration 43730 Training loss 0.002319875406101346 Validation loss 0.010644958354532719 Accuracy 0.88330078125\n",
      "Iteration 43740 Training loss 0.004151056986302137 Validation loss 0.010577739216387272 Accuracy 0.88427734375\n",
      "Iteration 43750 Training loss 0.0020326462108641863 Validation loss 0.010498402640223503 Accuracy 0.884765625\n",
      "Iteration 43760 Training loss 0.004044805653393269 Validation loss 0.010571557097136974 Accuracy 0.8837890625\n",
      "Iteration 43770 Training loss 0.004236823413521051 Validation loss 0.010574201121926308 Accuracy 0.8857421875\n",
      "Iteration 43780 Training loss 0.005045011639595032 Validation loss 0.010469452477991581 Accuracy 0.88720703125\n",
      "Iteration 43790 Training loss 0.00526663102209568 Validation loss 0.010543041862547398 Accuracy 0.884765625\n",
      "Iteration 43800 Training loss 0.004625456407666206 Validation loss 0.010486375540494919 Accuracy 0.8857421875\n",
      "Iteration 43810 Training loss 0.0031104080844670534 Validation loss 0.010508588515222073 Accuracy 0.88525390625\n",
      "Iteration 43820 Training loss 0.004783633630722761 Validation loss 0.010720261372625828 Accuracy 0.88232421875\n",
      "Iteration 43830 Training loss 0.0027983137406408787 Validation loss 0.01040755957365036 Accuracy 0.8857421875\n",
      "Iteration 43840 Training loss 0.003073797095566988 Validation loss 0.010484876111149788 Accuracy 0.88525390625\n",
      "Iteration 43850 Training loss 0.004940215498209 Validation loss 0.010404227301478386 Accuracy 0.88671875\n",
      "Iteration 43860 Training loss 0.0027098744176328182 Validation loss 0.010568228550255299 Accuracy 0.8837890625\n",
      "Iteration 43870 Training loss 0.005039247218519449 Validation loss 0.01054641604423523 Accuracy 0.8857421875\n",
      "Iteration 43880 Training loss 0.0037297471426427364 Validation loss 0.010434642434120178 Accuracy 0.88623046875\n",
      "Iteration 43890 Training loss 0.00423574261367321 Validation loss 0.010519792325794697 Accuracy 0.88427734375\n",
      "Iteration 43900 Training loss 0.0032852438744157553 Validation loss 0.010554145090281963 Accuracy 0.88427734375\n",
      "Iteration 43910 Training loss 0.0034025204367935658 Validation loss 0.010479132644832134 Accuracy 0.88427734375\n",
      "Iteration 43920 Training loss 0.0035406588576734066 Validation loss 0.01055731438100338 Accuracy 0.884765625\n",
      "Iteration 43930 Training loss 0.003312185872346163 Validation loss 0.010531127452850342 Accuracy 0.884765625\n",
      "Iteration 43940 Training loss 0.0030775838531553745 Validation loss 0.010481983423233032 Accuracy 0.88525390625\n",
      "Iteration 43950 Training loss 0.003930511884391308 Validation loss 0.010450392961502075 Accuracy 0.88623046875\n",
      "Iteration 43960 Training loss 0.004616817459464073 Validation loss 0.010421063750982285 Accuracy 0.88720703125\n",
      "Iteration 43970 Training loss 0.0043929689563810825 Validation loss 0.010744874365627766 Accuracy 0.88232421875\n",
      "Iteration 43980 Training loss 0.002697810996323824 Validation loss 0.010504679754376411 Accuracy 0.88525390625\n",
      "Iteration 43990 Training loss 0.003095156978815794 Validation loss 0.01070641353726387 Accuracy 0.88330078125\n",
      "Iteration 44000 Training loss 0.0032599428668618202 Validation loss 0.010474189184606075 Accuracy 0.88525390625\n",
      "Iteration 44010 Training loss 0.003274142276495695 Validation loss 0.010657580569386482 Accuracy 0.88232421875\n",
      "Iteration 44020 Training loss 0.00245675235055387 Validation loss 0.01060690637677908 Accuracy 0.88427734375\n",
      "Iteration 44030 Training loss 0.0030075269751250744 Validation loss 0.010550621896982193 Accuracy 0.8837890625\n",
      "Iteration 44040 Training loss 0.002429119311273098 Validation loss 0.010496157221496105 Accuracy 0.8857421875\n",
      "Iteration 44050 Training loss 0.004619494546204805 Validation loss 0.010470923967659473 Accuracy 0.88525390625\n",
      "Iteration 44060 Training loss 0.0027843748684972525 Validation loss 0.01044390257447958 Accuracy 0.88671875\n",
      "Iteration 44070 Training loss 0.0015822205459699035 Validation loss 0.01051833014935255 Accuracy 0.88525390625\n",
      "Iteration 44080 Training loss 0.004196095746010542 Validation loss 0.01040884293615818 Accuracy 0.88671875\n",
      "Iteration 44090 Training loss 0.004617076367139816 Validation loss 0.010564456693828106 Accuracy 0.884765625\n",
      "Iteration 44100 Training loss 0.003328014863654971 Validation loss 0.010547606274485588 Accuracy 0.8837890625\n",
      "Iteration 44110 Training loss 0.004442921839654446 Validation loss 0.010508773848414421 Accuracy 0.8857421875\n",
      "Iteration 44120 Training loss 0.0031492544803768396 Validation loss 0.010544005781412125 Accuracy 0.884765625\n",
      "Iteration 44130 Training loss 0.002362676663324237 Validation loss 0.010445545427501202 Accuracy 0.88525390625\n",
      "Iteration 44140 Training loss 0.0030579231679439545 Validation loss 0.010417620651423931 Accuracy 0.88525390625\n",
      "Iteration 44150 Training loss 0.003644935553893447 Validation loss 0.010411307215690613 Accuracy 0.88671875\n",
      "Iteration 44160 Training loss 0.004542316775768995 Validation loss 0.010548303835093975 Accuracy 0.884765625\n",
      "Iteration 44170 Training loss 0.003847719868645072 Validation loss 0.010635794140398502 Accuracy 0.88330078125\n",
      "Iteration 44180 Training loss 0.0031886850483715534 Validation loss 0.01062537357211113 Accuracy 0.88330078125\n",
      "Iteration 44190 Training loss 0.003188040107488632 Validation loss 0.010588240809738636 Accuracy 0.88427734375\n",
      "Iteration 44200 Training loss 0.004083055537194014 Validation loss 0.010654875077307224 Accuracy 0.88330078125\n",
      "Iteration 44210 Training loss 0.0033991695381700993 Validation loss 0.010577966459095478 Accuracy 0.8837890625\n",
      "Iteration 44220 Training loss 0.0038402541540563107 Validation loss 0.010597038082778454 Accuracy 0.8828125\n",
      "Iteration 44230 Training loss 0.003365014912560582 Validation loss 0.010639029555022717 Accuracy 0.88330078125\n",
      "Iteration 44240 Training loss 0.003287957515567541 Validation loss 0.010562432929873466 Accuracy 0.884765625\n",
      "Iteration 44250 Training loss 0.0032903740648180246 Validation loss 0.010743902064859867 Accuracy 0.88330078125\n",
      "Iteration 44260 Training loss 0.0030816751532256603 Validation loss 0.010495133697986603 Accuracy 0.88525390625\n",
      "Iteration 44270 Training loss 0.003762509673833847 Validation loss 0.010662484914064407 Accuracy 0.88330078125\n",
      "Iteration 44280 Training loss 0.004183800891041756 Validation loss 0.010591302998363972 Accuracy 0.88330078125\n",
      "Iteration 44290 Training loss 0.0028429552912712097 Validation loss 0.010639918968081474 Accuracy 0.88330078125\n",
      "Iteration 44300 Training loss 0.002889720955863595 Validation loss 0.010625854134559631 Accuracy 0.88330078125\n",
      "Iteration 44310 Training loss 0.004095694050192833 Validation loss 0.010504444129765034 Accuracy 0.88525390625\n",
      "Iteration 44320 Training loss 0.00304240919649601 Validation loss 0.010465627536177635 Accuracy 0.88623046875\n",
      "Iteration 44330 Training loss 0.004072065465152264 Validation loss 0.010498123243451118 Accuracy 0.88525390625\n",
      "Iteration 44340 Training loss 0.0038348534144461155 Validation loss 0.010541857220232487 Accuracy 0.88427734375\n",
      "Iteration 44350 Training loss 0.003836326766759157 Validation loss 0.010475198738276958 Accuracy 0.88671875\n",
      "Iteration 44360 Training loss 0.0029297543223947287 Validation loss 0.010517698712646961 Accuracy 0.884765625\n",
      "Iteration 44370 Training loss 0.004641999490559101 Validation loss 0.010742935352027416 Accuracy 0.88134765625\n",
      "Iteration 44380 Training loss 0.004931495990604162 Validation loss 0.010615404695272446 Accuracy 0.884765625\n",
      "Iteration 44390 Training loss 0.004069346934556961 Validation loss 0.010406099259853363 Accuracy 0.88623046875\n",
      "Iteration 44400 Training loss 0.005354057066142559 Validation loss 0.010596581734716892 Accuracy 0.8837890625\n",
      "Iteration 44410 Training loss 0.002501482143998146 Validation loss 0.010526733472943306 Accuracy 0.8837890625\n",
      "Iteration 44420 Training loss 0.004285000264644623 Validation loss 0.0106453076004982 Accuracy 0.8837890625\n",
      "Iteration 44430 Training loss 0.0038023001980036497 Validation loss 0.010495148599147797 Accuracy 0.88427734375\n",
      "Iteration 44440 Training loss 0.003922559786587954 Validation loss 0.01049604918807745 Accuracy 0.88525390625\n",
      "Iteration 44450 Training loss 0.004618396051228046 Validation loss 0.010571579448878765 Accuracy 0.88427734375\n",
      "Iteration 44460 Training loss 0.005111371167004108 Validation loss 0.010647550225257874 Accuracy 0.88330078125\n",
      "Iteration 44470 Training loss 0.005287492647767067 Validation loss 0.010598376393318176 Accuracy 0.8837890625\n",
      "Iteration 44480 Training loss 0.0041732401587069035 Validation loss 0.010563754476606846 Accuracy 0.88427734375\n",
      "Iteration 44490 Training loss 0.004149285145103931 Validation loss 0.0107651986181736 Accuracy 0.8818359375\n",
      "Iteration 44500 Training loss 0.0032755269203335047 Validation loss 0.010564472526311874 Accuracy 0.8837890625\n",
      "Iteration 44510 Training loss 0.004089789465069771 Validation loss 0.010656977072358131 Accuracy 0.8828125\n",
      "Iteration 44520 Training loss 0.003732368815690279 Validation loss 0.010627844370901585 Accuracy 0.8828125\n",
      "Iteration 44530 Training loss 0.0037644067779183388 Validation loss 0.010645560920238495 Accuracy 0.88330078125\n",
      "Iteration 44540 Training loss 0.004389527719467878 Validation loss 0.010631082579493523 Accuracy 0.8837890625\n",
      "Iteration 44550 Training loss 0.003572900779545307 Validation loss 0.0106107322499156 Accuracy 0.884765625\n",
      "Iteration 44560 Training loss 0.003514156909659505 Validation loss 0.010545068420469761 Accuracy 0.88525390625\n",
      "Iteration 44570 Training loss 0.003321276977658272 Validation loss 0.010586528107523918 Accuracy 0.88427734375\n",
      "Iteration 44580 Training loss 0.002893689554184675 Validation loss 0.010504096746444702 Accuracy 0.88525390625\n",
      "Iteration 44590 Training loss 0.0027216854505240917 Validation loss 0.010521882213652134 Accuracy 0.88525390625\n",
      "Iteration 44600 Training loss 0.003160238964483142 Validation loss 0.010493665933609009 Accuracy 0.8857421875\n",
      "Iteration 44610 Training loss 0.004237999673932791 Validation loss 0.010630163364112377 Accuracy 0.88330078125\n",
      "Iteration 44620 Training loss 0.0033139877486974 Validation loss 0.010564550757408142 Accuracy 0.884765625\n",
      "Iteration 44630 Training loss 0.003822915256023407 Validation loss 0.010593191720545292 Accuracy 0.88427734375\n",
      "Iteration 44640 Training loss 0.004060933832079172 Validation loss 0.01052002888172865 Accuracy 0.884765625\n",
      "Iteration 44650 Training loss 0.0032471551094204187 Validation loss 0.01046661101281643 Accuracy 0.88525390625\n",
      "Iteration 44660 Training loss 0.0032699883449822664 Validation loss 0.010630839504301548 Accuracy 0.88427734375\n",
      "Iteration 44670 Training loss 0.0035026445984840393 Validation loss 0.010808588936924934 Accuracy 0.88232421875\n",
      "Iteration 44680 Training loss 0.0026578218676149845 Validation loss 0.010800160467624664 Accuracy 0.880859375\n",
      "Iteration 44690 Training loss 0.004376546945422888 Validation loss 0.010680219158530235 Accuracy 0.8837890625\n",
      "Iteration 44700 Training loss 0.002964801387861371 Validation loss 0.01051298063248396 Accuracy 0.88525390625\n",
      "Iteration 44710 Training loss 0.0023950845934450626 Validation loss 0.010517384856939316 Accuracy 0.88427734375\n",
      "Iteration 44720 Training loss 0.0034596140030771494 Validation loss 0.010454838164150715 Accuracy 0.8857421875\n",
      "Iteration 44730 Training loss 0.002672558417543769 Validation loss 0.010528391227126122 Accuracy 0.8857421875\n",
      "Iteration 44740 Training loss 0.003955243155360222 Validation loss 0.010542125441133976 Accuracy 0.88525390625\n",
      "Iteration 44750 Training loss 0.0028968597762286663 Validation loss 0.010574966669082642 Accuracy 0.8837890625\n",
      "Iteration 44760 Training loss 0.004285052418708801 Validation loss 0.010564605705440044 Accuracy 0.8837890625\n",
      "Iteration 44770 Training loss 0.0037607888225466013 Validation loss 0.010561496019363403 Accuracy 0.88330078125\n",
      "Iteration 44780 Training loss 0.003820121055468917 Validation loss 0.010539045557379723 Accuracy 0.884765625\n",
      "Iteration 44790 Training loss 0.003333115251734853 Validation loss 0.010514308698475361 Accuracy 0.8857421875\n",
      "Iteration 44800 Training loss 0.0032530322205275297 Validation loss 0.010557695291936398 Accuracy 0.8837890625\n",
      "Iteration 44810 Training loss 0.004638630896806717 Validation loss 0.010523928329348564 Accuracy 0.8857421875\n",
      "Iteration 44820 Training loss 0.005105248186737299 Validation loss 0.01086717750877142 Accuracy 0.88037109375\n",
      "Iteration 44830 Training loss 0.0035225674510002136 Validation loss 0.010550525039434433 Accuracy 0.884765625\n",
      "Iteration 44840 Training loss 0.004000131040811539 Validation loss 0.01062603946775198 Accuracy 0.8837890625\n",
      "Iteration 44850 Training loss 0.0038887434639036655 Validation loss 0.010610437020659447 Accuracy 0.88330078125\n",
      "Iteration 44860 Training loss 0.0024906957987695932 Validation loss 0.010586855001747608 Accuracy 0.88330078125\n",
      "Iteration 44870 Training loss 0.0026563021820038557 Validation loss 0.010573429986834526 Accuracy 0.8837890625\n",
      "Iteration 44880 Training loss 0.003907757345587015 Validation loss 0.010827566497027874 Accuracy 0.8828125\n",
      "Iteration 44890 Training loss 0.005164096597582102 Validation loss 0.010610410012304783 Accuracy 0.88427734375\n",
      "Iteration 44900 Training loss 0.003860345110297203 Validation loss 0.010572424158453941 Accuracy 0.8837890625\n",
      "Iteration 44910 Training loss 0.004146314691752195 Validation loss 0.01059537660330534 Accuracy 0.88427734375\n",
      "Iteration 44920 Training loss 0.005637226160615683 Validation loss 0.010598810389637947 Accuracy 0.884765625\n",
      "Iteration 44930 Training loss 0.0044548590667545795 Validation loss 0.010661203414201736 Accuracy 0.88232421875\n",
      "Iteration 44940 Training loss 0.0025338216219097376 Validation loss 0.0105505445972085 Accuracy 0.8837890625\n",
      "Iteration 44950 Training loss 0.0030113563407212496 Validation loss 0.010648953728377819 Accuracy 0.88330078125\n",
      "Iteration 44960 Training loss 0.004444741643965244 Validation loss 0.010495507158339024 Accuracy 0.8857421875\n",
      "Iteration 44970 Training loss 0.003208701964467764 Validation loss 0.010542037896811962 Accuracy 0.8857421875\n",
      "Iteration 44980 Training loss 0.002786775818094611 Validation loss 0.010448346845805645 Accuracy 0.88623046875\n",
      "Iteration 44990 Training loss 0.003220374695956707 Validation loss 0.010563707910478115 Accuracy 0.8837890625\n",
      "Iteration 45000 Training loss 0.0035058383364230394 Validation loss 0.010503542609512806 Accuracy 0.88525390625\n",
      "Iteration 45010 Training loss 0.0033982957247644663 Validation loss 0.01054032426327467 Accuracy 0.8857421875\n",
      "Iteration 45020 Training loss 0.003967702388763428 Validation loss 0.010606514289975166 Accuracy 0.8837890625\n",
      "Iteration 45030 Training loss 0.003407838521525264 Validation loss 0.010530668310821056 Accuracy 0.884765625\n",
      "Iteration 45040 Training loss 0.0032982793636620045 Validation loss 0.010544597171247005 Accuracy 0.884765625\n",
      "Iteration 45050 Training loss 0.0021326064597815275 Validation loss 0.01056373119354248 Accuracy 0.88427734375\n",
      "Iteration 45060 Training loss 0.004477722570300102 Validation loss 0.010550926439464092 Accuracy 0.88427734375\n",
      "Iteration 45070 Training loss 0.003123406320810318 Validation loss 0.010543559677898884 Accuracy 0.884765625\n",
      "Iteration 45080 Training loss 0.003310322994366288 Validation loss 0.010535795241594315 Accuracy 0.88330078125\n",
      "Iteration 45090 Training loss 0.003867764724418521 Validation loss 0.010552743449807167 Accuracy 0.8857421875\n",
      "Iteration 45100 Training loss 0.0034279734827578068 Validation loss 0.010534417815506458 Accuracy 0.88525390625\n",
      "Iteration 45110 Training loss 0.002788962097838521 Validation loss 0.010605527088046074 Accuracy 0.88330078125\n",
      "Iteration 45120 Training loss 0.0037640249356627464 Validation loss 0.010656383819878101 Accuracy 0.8828125\n",
      "Iteration 45130 Training loss 0.0030196441803127527 Validation loss 0.010593198239803314 Accuracy 0.8837890625\n",
      "Iteration 45140 Training loss 0.0029020626097917557 Validation loss 0.010553528554737568 Accuracy 0.8837890625\n",
      "Iteration 45150 Training loss 0.003864891594275832 Validation loss 0.01062595471739769 Accuracy 0.88330078125\n",
      "Iteration 45160 Training loss 0.0029833291191607714 Validation loss 0.01066669449210167 Accuracy 0.8837890625\n",
      "Iteration 45170 Training loss 0.0039001668337732553 Validation loss 0.01069293450564146 Accuracy 0.88232421875\n",
      "Iteration 45180 Training loss 0.005127194803208113 Validation loss 0.01056719571352005 Accuracy 0.8818359375\n",
      "Iteration 45190 Training loss 0.0032054241746664047 Validation loss 0.01062854751944542 Accuracy 0.88330078125\n",
      "Iteration 45200 Training loss 0.003766145557165146 Validation loss 0.01068766601383686 Accuracy 0.88427734375\n",
      "Iteration 45210 Training loss 0.0031093501020222902 Validation loss 0.01061042957007885 Accuracy 0.8837890625\n",
      "Iteration 45220 Training loss 0.004346503410488367 Validation loss 0.010617446154356003 Accuracy 0.88427734375\n",
      "Iteration 45230 Training loss 0.00314514571800828 Validation loss 0.01067201979458332 Accuracy 0.88427734375\n",
      "Iteration 45240 Training loss 0.004049466457217932 Validation loss 0.010465150699019432 Accuracy 0.88623046875\n",
      "Iteration 45250 Training loss 0.0030034526716917753 Validation loss 0.01065069530159235 Accuracy 0.88330078125\n",
      "Iteration 45260 Training loss 0.0046117957681417465 Validation loss 0.010914568789303303 Accuracy 0.88037109375\n",
      "Iteration 45270 Training loss 0.0046879686415195465 Validation loss 0.010518542490899563 Accuracy 0.8857421875\n",
      "Iteration 45280 Training loss 0.0038191021885722876 Validation loss 0.010535221546888351 Accuracy 0.884765625\n",
      "Iteration 45290 Training loss 0.0039393603801727295 Validation loss 0.010596944019198418 Accuracy 0.8828125\n",
      "Iteration 45300 Training loss 0.0034830006770789623 Validation loss 0.010534824803471565 Accuracy 0.884765625\n",
      "Iteration 45310 Training loss 0.003356772707775235 Validation loss 0.010540328919887543 Accuracy 0.8837890625\n",
      "Iteration 45320 Training loss 0.004563947208225727 Validation loss 0.01056260708719492 Accuracy 0.88525390625\n",
      "Iteration 45330 Training loss 0.004233994521200657 Validation loss 0.010582615621387959 Accuracy 0.88525390625\n",
      "Iteration 45340 Training loss 0.003955109976232052 Validation loss 0.010566461831331253 Accuracy 0.8837890625\n",
      "Iteration 45350 Training loss 0.002776773413643241 Validation loss 0.010514462366700172 Accuracy 0.88525390625\n",
      "Iteration 45360 Training loss 0.003694792976602912 Validation loss 0.010561015456914902 Accuracy 0.8837890625\n",
      "Iteration 45370 Training loss 0.0024483315646648407 Validation loss 0.010515585541725159 Accuracy 0.88525390625\n",
      "Iteration 45380 Training loss 0.0031768083572387695 Validation loss 0.01045379601418972 Accuracy 0.8857421875\n",
      "Iteration 45390 Training loss 0.002617869060486555 Validation loss 0.010548317804932594 Accuracy 0.884765625\n",
      "Iteration 45400 Training loss 0.003032764419913292 Validation loss 0.010644306428730488 Accuracy 0.8837890625\n",
      "Iteration 45410 Training loss 0.003790088929235935 Validation loss 0.010489042848348618 Accuracy 0.8857421875\n",
      "Iteration 45420 Training loss 0.004441426135599613 Validation loss 0.010508816689252853 Accuracy 0.88427734375\n",
      "Iteration 45430 Training loss 0.0034012775868177414 Validation loss 0.010646829381585121 Accuracy 0.8828125\n",
      "Iteration 45440 Training loss 0.0038159433752298355 Validation loss 0.010453378781676292 Accuracy 0.88525390625\n",
      "Iteration 45450 Training loss 0.003719065338373184 Validation loss 0.01050194539129734 Accuracy 0.8857421875\n",
      "Iteration 45460 Training loss 0.003287774045020342 Validation loss 0.010529575869441032 Accuracy 0.884765625\n",
      "Iteration 45470 Training loss 0.004183095879852772 Validation loss 0.010497946292161942 Accuracy 0.88525390625\n",
      "Iteration 45480 Training loss 0.0032651799265295267 Validation loss 0.010508780367672443 Accuracy 0.88427734375\n",
      "Iteration 45490 Training loss 0.002362895756959915 Validation loss 0.010628812946379185 Accuracy 0.8837890625\n",
      "Iteration 45500 Training loss 0.003413165919482708 Validation loss 0.010562659241259098 Accuracy 0.8837890625\n",
      "Iteration 45510 Training loss 0.004814600571990013 Validation loss 0.01071174256503582 Accuracy 0.8828125\n",
      "Iteration 45520 Training loss 0.0036326819099485874 Validation loss 0.010623429901897907 Accuracy 0.88330078125\n",
      "Iteration 45530 Training loss 0.003092355327680707 Validation loss 0.010499906726181507 Accuracy 0.8837890625\n",
      "Iteration 45540 Training loss 0.004889183212071657 Validation loss 0.010637389495968819 Accuracy 0.8837890625\n",
      "Iteration 45550 Training loss 0.0032856850884854794 Validation loss 0.010661618784070015 Accuracy 0.88427734375\n",
      "Iteration 45560 Training loss 0.0031632385216653347 Validation loss 0.01052467618137598 Accuracy 0.884765625\n",
      "Iteration 45570 Training loss 0.0030640549957752228 Validation loss 0.010546598583459854 Accuracy 0.884765625\n",
      "Iteration 45580 Training loss 0.00316915032453835 Validation loss 0.010566957294940948 Accuracy 0.88427734375\n",
      "Iteration 45590 Training loss 0.0043386537581682205 Validation loss 0.010600082576274872 Accuracy 0.88330078125\n",
      "Iteration 45600 Training loss 0.0031652136240154505 Validation loss 0.01059373002499342 Accuracy 0.8837890625\n",
      "Iteration 45610 Training loss 0.0031657759100198746 Validation loss 0.010508132167160511 Accuracy 0.8857421875\n",
      "Iteration 45620 Training loss 0.004492183681577444 Validation loss 0.010633000172674656 Accuracy 0.88330078125\n",
      "Iteration 45630 Training loss 0.002625055145472288 Validation loss 0.010544771328568459 Accuracy 0.8837890625\n",
      "Iteration 45640 Training loss 0.004356245510280132 Validation loss 0.010664315894246101 Accuracy 0.88232421875\n",
      "Iteration 45650 Training loss 0.002985046012327075 Validation loss 0.010571426711976528 Accuracy 0.88427734375\n",
      "Iteration 45660 Training loss 0.0027260789647698402 Validation loss 0.010600998066365719 Accuracy 0.88427734375\n",
      "Iteration 45670 Training loss 0.003957007545977831 Validation loss 0.010527008213102818 Accuracy 0.8857421875\n",
      "Iteration 45680 Training loss 0.003883767407387495 Validation loss 0.010483244433999062 Accuracy 0.88427734375\n",
      "Iteration 45690 Training loss 0.0027011800557374954 Validation loss 0.010534269735217094 Accuracy 0.884765625\n",
      "Iteration 45700 Training loss 0.0017044031992554665 Validation loss 0.010432763025164604 Accuracy 0.88623046875\n",
      "Iteration 45710 Training loss 0.003842489095404744 Validation loss 0.010379659943282604 Accuracy 0.88671875\n",
      "Iteration 45720 Training loss 0.0039101867005229 Validation loss 0.01041840948164463 Accuracy 0.88623046875\n",
      "Iteration 45730 Training loss 0.004019841551780701 Validation loss 0.010449047200381756 Accuracy 0.88525390625\n",
      "Iteration 45740 Training loss 0.003683464601635933 Validation loss 0.010432809591293335 Accuracy 0.88671875\n",
      "Iteration 45750 Training loss 0.003405228490009904 Validation loss 0.010542360134422779 Accuracy 0.8828125\n",
      "Iteration 45760 Training loss 0.0037896146532148123 Validation loss 0.010720097459852695 Accuracy 0.8818359375\n",
      "Iteration 45770 Training loss 0.004610997624695301 Validation loss 0.010621178895235062 Accuracy 0.88330078125\n",
      "Iteration 45780 Training loss 0.004407957196235657 Validation loss 0.010672399774193764 Accuracy 0.88330078125\n",
      "Iteration 45790 Training loss 0.004233009647578001 Validation loss 0.01046895980834961 Accuracy 0.8857421875\n",
      "Iteration 45800 Training loss 0.004230151418596506 Validation loss 0.01051067840307951 Accuracy 0.884765625\n",
      "Iteration 45810 Training loss 0.004394953139126301 Validation loss 0.010620802640914917 Accuracy 0.8837890625\n",
      "Iteration 45820 Training loss 0.003917749971151352 Validation loss 0.010579630732536316 Accuracy 0.884765625\n",
      "Iteration 45830 Training loss 0.004293951205909252 Validation loss 0.01059603039175272 Accuracy 0.88427734375\n",
      "Iteration 45840 Training loss 0.0033573105465620756 Validation loss 0.010634912177920341 Accuracy 0.88427734375\n",
      "Iteration 45850 Training loss 0.004021196160465479 Validation loss 0.01055439654737711 Accuracy 0.884765625\n",
      "Iteration 45860 Training loss 0.0033977003768086433 Validation loss 0.010559938848018646 Accuracy 0.88427734375\n",
      "Iteration 45870 Training loss 0.004759914241731167 Validation loss 0.010585790500044823 Accuracy 0.8837890625\n",
      "Iteration 45880 Training loss 0.004305256996303797 Validation loss 0.010607887990772724 Accuracy 0.8837890625\n",
      "Iteration 45890 Training loss 0.004673963412642479 Validation loss 0.010686755180358887 Accuracy 0.88232421875\n",
      "Iteration 45900 Training loss 0.004560656379908323 Validation loss 0.010642495937645435 Accuracy 0.88330078125\n",
      "Iteration 45910 Training loss 0.0045947120524942875 Validation loss 0.010599782690405846 Accuracy 0.8837890625\n",
      "Iteration 45920 Training loss 0.0037187275011092424 Validation loss 0.010699630714952946 Accuracy 0.8828125\n",
      "Iteration 45930 Training loss 0.0026056861970573664 Validation loss 0.01066114753484726 Accuracy 0.8828125\n",
      "Iteration 45940 Training loss 0.0029796441085636616 Validation loss 0.010499952360987663 Accuracy 0.884765625\n",
      "Iteration 45950 Training loss 0.004410156048834324 Validation loss 0.010920562781393528 Accuracy 0.87939453125\n",
      "Iteration 45960 Training loss 0.0039547644555568695 Validation loss 0.010502931661903858 Accuracy 0.8857421875\n",
      "Iteration 45970 Training loss 0.004546007607132196 Validation loss 0.010616176761686802 Accuracy 0.8828125\n",
      "Iteration 45980 Training loss 0.002464078599587083 Validation loss 0.010594887658953667 Accuracy 0.8837890625\n",
      "Iteration 45990 Training loss 0.0031847755890339613 Validation loss 0.010512062348425388 Accuracy 0.884765625\n",
      "Iteration 46000 Training loss 0.003955531865358353 Validation loss 0.010629843920469284 Accuracy 0.88330078125\n",
      "Iteration 46010 Training loss 0.003705226816236973 Validation loss 0.01074664480984211 Accuracy 0.88232421875\n",
      "Iteration 46020 Training loss 0.003322020173072815 Validation loss 0.010425670072436333 Accuracy 0.88623046875\n",
      "Iteration 46030 Training loss 0.004477247130125761 Validation loss 0.010438395664095879 Accuracy 0.88623046875\n",
      "Iteration 46040 Training loss 0.00467282859608531 Validation loss 0.010529198683798313 Accuracy 0.884765625\n",
      "Iteration 46050 Training loss 0.004057880491018295 Validation loss 0.01063548307865858 Accuracy 0.8818359375\n",
      "Iteration 46060 Training loss 0.0046834577806293964 Validation loss 0.010500333271920681 Accuracy 0.884765625\n",
      "Iteration 46070 Training loss 0.0036612185649573803 Validation loss 0.010477002710103989 Accuracy 0.8857421875\n",
      "Iteration 46080 Training loss 0.0037663071416318417 Validation loss 0.010408670641481876 Accuracy 0.88720703125\n",
      "Iteration 46090 Training loss 0.0045255268923938274 Validation loss 0.010578306391835213 Accuracy 0.88427734375\n",
      "Iteration 46100 Training loss 0.0037024738267064095 Validation loss 0.010556789115071297 Accuracy 0.88427734375\n",
      "Iteration 46110 Training loss 0.0029853032901883125 Validation loss 0.01050642877817154 Accuracy 0.884765625\n",
      "Iteration 46120 Training loss 0.0033161300234496593 Validation loss 0.010444935411214828 Accuracy 0.88525390625\n",
      "Iteration 46130 Training loss 0.002342482563108206 Validation loss 0.010515440255403519 Accuracy 0.884765625\n",
      "Iteration 46140 Training loss 0.0041634803637862206 Validation loss 0.010508053936064243 Accuracy 0.884765625\n",
      "Iteration 46150 Training loss 0.0034093938302248716 Validation loss 0.010477111674845219 Accuracy 0.8857421875\n",
      "Iteration 46160 Training loss 0.0028936699964106083 Validation loss 0.010447738692164421 Accuracy 0.8857421875\n",
      "Iteration 46170 Training loss 0.003171344753354788 Validation loss 0.010510064661502838 Accuracy 0.884765625\n",
      "Iteration 46180 Training loss 0.003965096548199654 Validation loss 0.010452407412230968 Accuracy 0.88671875\n",
      "Iteration 46190 Training loss 0.004324111621826887 Validation loss 0.01041476335376501 Accuracy 0.88623046875\n",
      "Iteration 46200 Training loss 0.004146083723753691 Validation loss 0.010415640659630299 Accuracy 0.88671875\n",
      "Iteration 46210 Training loss 0.003085031406953931 Validation loss 0.010481519624590874 Accuracy 0.8857421875\n",
      "Iteration 46220 Training loss 0.0030504988972097635 Validation loss 0.010541400872170925 Accuracy 0.884765625\n",
      "Iteration 46230 Training loss 0.0036078928969800472 Validation loss 0.010532553307712078 Accuracy 0.8837890625\n",
      "Iteration 46240 Training loss 0.0030657202005386353 Validation loss 0.010577338747680187 Accuracy 0.884765625\n",
      "Iteration 46250 Training loss 0.0025824308395385742 Validation loss 0.010558057576417923 Accuracy 0.8828125\n",
      "Iteration 46260 Training loss 0.0030141060706228018 Validation loss 0.010555116459727287 Accuracy 0.88427734375\n",
      "Iteration 46270 Training loss 0.002653593895956874 Validation loss 0.010578054003417492 Accuracy 0.884765625\n",
      "Iteration 46280 Training loss 0.0038246954791247845 Validation loss 0.01062730886042118 Accuracy 0.88427734375\n",
      "Iteration 46290 Training loss 0.0027453715447336435 Validation loss 0.010573399253189564 Accuracy 0.8837890625\n",
      "Iteration 46300 Training loss 0.002695505041629076 Validation loss 0.010569435544312 Accuracy 0.8837890625\n",
      "Iteration 46310 Training loss 0.0032056376803666353 Validation loss 0.010588559322059155 Accuracy 0.88427734375\n",
      "Iteration 46320 Training loss 0.0034946142695844173 Validation loss 0.010581974871456623 Accuracy 0.88427734375\n",
      "Iteration 46330 Training loss 0.0025775849353522062 Validation loss 0.010620728135108948 Accuracy 0.88427734375\n",
      "Iteration 46340 Training loss 0.0019964470993727446 Validation loss 0.0105681661516428 Accuracy 0.88427734375\n",
      "Iteration 46350 Training loss 0.00383290508762002 Validation loss 0.010560430586338043 Accuracy 0.88330078125\n",
      "Iteration 46360 Training loss 0.004788821563124657 Validation loss 0.01064160093665123 Accuracy 0.88427734375\n",
      "Iteration 46370 Training loss 0.003481516381725669 Validation loss 0.010478432290256023 Accuracy 0.8857421875\n",
      "Iteration 46380 Training loss 0.003504870692268014 Validation loss 0.010488266125321388 Accuracy 0.8857421875\n",
      "Iteration 46390 Training loss 0.0030516500119119883 Validation loss 0.01054828055202961 Accuracy 0.88427734375\n",
      "Iteration 46400 Training loss 0.0045432159677147865 Validation loss 0.01057980302721262 Accuracy 0.884765625\n",
      "Iteration 46410 Training loss 0.002564690075814724 Validation loss 0.010487547144293785 Accuracy 0.88623046875\n",
      "Iteration 46420 Training loss 0.0035050916485488415 Validation loss 0.010577387176454067 Accuracy 0.88525390625\n",
      "Iteration 46430 Training loss 0.0041191657073795795 Validation loss 0.010448000393807888 Accuracy 0.88623046875\n",
      "Iteration 46440 Training loss 0.003990505822002888 Validation loss 0.010634641163051128 Accuracy 0.8828125\n",
      "Iteration 46450 Training loss 0.004902166780084372 Validation loss 0.01042485423386097 Accuracy 0.88525390625\n",
      "Iteration 46460 Training loss 0.002857137005776167 Validation loss 0.01044150348752737 Accuracy 0.8857421875\n",
      "Iteration 46470 Training loss 0.004091984126716852 Validation loss 0.010483684949576855 Accuracy 0.8857421875\n",
      "Iteration 46480 Training loss 0.0032127045560628176 Validation loss 0.010452684946358204 Accuracy 0.88623046875\n",
      "Iteration 46490 Training loss 0.0029112135525792837 Validation loss 0.010517491959035397 Accuracy 0.88427734375\n",
      "Iteration 46500 Training loss 0.003161884378641844 Validation loss 0.010464469902217388 Accuracy 0.884765625\n",
      "Iteration 46510 Training loss 0.0026205440517514944 Validation loss 0.010575354099273682 Accuracy 0.88427734375\n",
      "Iteration 46520 Training loss 0.003730640048161149 Validation loss 0.010437076911330223 Accuracy 0.88623046875\n",
      "Iteration 46530 Training loss 0.004029261879622936 Validation loss 0.01055863220244646 Accuracy 0.8837890625\n",
      "Iteration 46540 Training loss 0.003994510509073734 Validation loss 0.010570330545306206 Accuracy 0.88427734375\n",
      "Iteration 46550 Training loss 0.00258246879093349 Validation loss 0.010519572533667088 Accuracy 0.88623046875\n",
      "Iteration 46560 Training loss 0.004089856520295143 Validation loss 0.010551296174526215 Accuracy 0.884765625\n",
      "Iteration 46570 Training loss 0.002657361328601837 Validation loss 0.010543683543801308 Accuracy 0.88427734375\n",
      "Iteration 46580 Training loss 0.003892122535035014 Validation loss 0.010502172634005547 Accuracy 0.884765625\n",
      "Iteration 46590 Training loss 0.0032913554459810257 Validation loss 0.010595949366688728 Accuracy 0.8837890625\n",
      "Iteration 46600 Training loss 0.003795723197981715 Validation loss 0.010609443299472332 Accuracy 0.88427734375\n",
      "Iteration 46610 Training loss 0.002441457938402891 Validation loss 0.0105363130569458 Accuracy 0.8857421875\n",
      "Iteration 46620 Training loss 0.0037381043657660484 Validation loss 0.010468672960996628 Accuracy 0.88525390625\n",
      "Iteration 46630 Training loss 0.0031039887107908726 Validation loss 0.010638990439474583 Accuracy 0.8837890625\n",
      "Iteration 46640 Training loss 0.002679766621440649 Validation loss 0.010481076315045357 Accuracy 0.8857421875\n",
      "Iteration 46650 Training loss 0.0025321957655251026 Validation loss 0.010496133007109165 Accuracy 0.88525390625\n",
      "Iteration 46660 Training loss 0.004407411441206932 Validation loss 0.010570771060883999 Accuracy 0.88330078125\n",
      "Iteration 46670 Training loss 0.003152590012177825 Validation loss 0.01081742625683546 Accuracy 0.88134765625\n",
      "Iteration 46680 Training loss 0.003895443631336093 Validation loss 0.010531271807849407 Accuracy 0.8837890625\n",
      "Iteration 46690 Training loss 0.002513444283977151 Validation loss 0.01060716062784195 Accuracy 0.8837890625\n",
      "Iteration 46700 Training loss 0.004636853467673063 Validation loss 0.010575499385595322 Accuracy 0.88427734375\n",
      "Iteration 46710 Training loss 0.004458113107830286 Validation loss 0.010550513863563538 Accuracy 0.88427734375\n",
      "Iteration 46720 Training loss 0.00393408490344882 Validation loss 0.010539584793150425 Accuracy 0.88427734375\n",
      "Iteration 46730 Training loss 0.003910306841135025 Validation loss 0.010442241095006466 Accuracy 0.88623046875\n",
      "Iteration 46740 Training loss 0.003537010168656707 Validation loss 0.010593702085316181 Accuracy 0.8837890625\n",
      "Iteration 46750 Training loss 0.004911472089588642 Validation loss 0.010722377337515354 Accuracy 0.88134765625\n",
      "Iteration 46760 Training loss 0.004226678982377052 Validation loss 0.010607106611132622 Accuracy 0.88330078125\n",
      "Iteration 46770 Training loss 0.0022671776823699474 Validation loss 0.010555165819823742 Accuracy 0.88427734375\n",
      "Iteration 46780 Training loss 0.0034258950036019087 Validation loss 0.010572150349617004 Accuracy 0.88427734375\n",
      "Iteration 46790 Training loss 0.004437325522303581 Validation loss 0.0104402806609869 Accuracy 0.88525390625\n",
      "Iteration 46800 Training loss 0.0027552954852581024 Validation loss 0.010512810200452805 Accuracy 0.884765625\n",
      "Iteration 46810 Training loss 0.0038802064955234528 Validation loss 0.010513256303966045 Accuracy 0.884765625\n",
      "Iteration 46820 Training loss 0.0027136309072375298 Validation loss 0.010671633295714855 Accuracy 0.88330078125\n",
      "Iteration 46830 Training loss 0.004087270237505436 Validation loss 0.010533460415899754 Accuracy 0.884765625\n",
      "Iteration 46840 Training loss 0.004033716395497322 Validation loss 0.010621489025652409 Accuracy 0.8837890625\n",
      "Iteration 46850 Training loss 0.00213615782558918 Validation loss 0.010456789284944534 Accuracy 0.88525390625\n",
      "Iteration 46860 Training loss 0.0028296676464378834 Validation loss 0.010418386198580265 Accuracy 0.88525390625\n",
      "Iteration 46870 Training loss 0.0034555718302726746 Validation loss 0.010498952120542526 Accuracy 0.88525390625\n",
      "Iteration 46880 Training loss 0.003033534623682499 Validation loss 0.010549871250987053 Accuracy 0.88427734375\n",
      "Iteration 46890 Training loss 0.0038026648107916117 Validation loss 0.010482876561582088 Accuracy 0.884765625\n",
      "Iteration 46900 Training loss 0.0038823490031063557 Validation loss 0.010520339012145996 Accuracy 0.88525390625\n",
      "Iteration 46910 Training loss 0.0040305424481630325 Validation loss 0.010554459877312183 Accuracy 0.88427734375\n",
      "Iteration 46920 Training loss 0.0038160388357937336 Validation loss 0.010564056225121021 Accuracy 0.88427734375\n",
      "Iteration 46930 Training loss 0.0029667746275663376 Validation loss 0.010500955395400524 Accuracy 0.88427734375\n",
      "Iteration 46940 Training loss 0.0037321459967643023 Validation loss 0.010642596520483494 Accuracy 0.88232421875\n",
      "Iteration 46950 Training loss 0.0040396638214588165 Validation loss 0.010475203394889832 Accuracy 0.88623046875\n",
      "Iteration 46960 Training loss 0.0036681601777672768 Validation loss 0.010478401556611061 Accuracy 0.88427734375\n",
      "Iteration 46970 Training loss 0.004387068096548319 Validation loss 0.010512747801840305 Accuracy 0.884765625\n",
      "Iteration 46980 Training loss 0.0031750297639518976 Validation loss 0.01048004999756813 Accuracy 0.88525390625\n",
      "Iteration 46990 Training loss 0.0028134924359619617 Validation loss 0.010636373423039913 Accuracy 0.8828125\n",
      "Iteration 47000 Training loss 0.0037920468021184206 Validation loss 0.010553012602031231 Accuracy 0.88330078125\n",
      "Iteration 47010 Training loss 0.002857045503333211 Validation loss 0.010623986832797527 Accuracy 0.8837890625\n",
      "Iteration 47020 Training loss 0.005359203089028597 Validation loss 0.010565430857241154 Accuracy 0.884765625\n",
      "Iteration 47030 Training loss 0.0025147441774606705 Validation loss 0.010540598072111607 Accuracy 0.884765625\n",
      "Iteration 47040 Training loss 0.003983464557677507 Validation loss 0.010543234646320343 Accuracy 0.88427734375\n",
      "Iteration 47050 Training loss 0.003623603144660592 Validation loss 0.010611564852297306 Accuracy 0.88427734375\n",
      "Iteration 47060 Training loss 0.005383232142776251 Validation loss 0.010584788396954536 Accuracy 0.88525390625\n",
      "Iteration 47070 Training loss 0.004833215847611427 Validation loss 0.010502731427550316 Accuracy 0.8857421875\n",
      "Iteration 47080 Training loss 0.0037045020144432783 Validation loss 0.01061662845313549 Accuracy 0.8837890625\n",
      "Iteration 47090 Training loss 0.002830588025972247 Validation loss 0.010544078424572945 Accuracy 0.88427734375\n",
      "Iteration 47100 Training loss 0.004526096861809492 Validation loss 0.010529275052249432 Accuracy 0.884765625\n",
      "Iteration 47110 Training loss 0.005117284599691629 Validation loss 0.010505087673664093 Accuracy 0.88525390625\n",
      "Iteration 47120 Training loss 0.002746107755228877 Validation loss 0.010447145439684391 Accuracy 0.884765625\n",
      "Iteration 47130 Training loss 0.003979431930929422 Validation loss 0.010510362684726715 Accuracy 0.884765625\n",
      "Iteration 47140 Training loss 0.00452042929828167 Validation loss 0.010532653890550137 Accuracy 0.884765625\n",
      "Iteration 47150 Training loss 0.004135061055421829 Validation loss 0.010507465340197086 Accuracy 0.88427734375\n",
      "Iteration 47160 Training loss 0.002297766273841262 Validation loss 0.010592731647193432 Accuracy 0.884765625\n",
      "Iteration 47170 Training loss 0.004466122481971979 Validation loss 0.010657412000000477 Accuracy 0.8837890625\n",
      "Iteration 47180 Training loss 0.004766283091157675 Validation loss 0.010410087183117867 Accuracy 0.88623046875\n",
      "Iteration 47190 Training loss 0.003526736283674836 Validation loss 0.01058374997228384 Accuracy 0.88427734375\n",
      "Iteration 47200 Training loss 0.0027331607416272163 Validation loss 0.010768100619316101 Accuracy 0.88134765625\n",
      "Iteration 47210 Training loss 0.004434953443706036 Validation loss 0.010536029934883118 Accuracy 0.884765625\n",
      "Iteration 47220 Training loss 0.0047264182940125465 Validation loss 0.010523074306547642 Accuracy 0.88525390625\n",
      "Iteration 47230 Training loss 0.00217706267721951 Validation loss 0.010352695360779762 Accuracy 0.8876953125\n",
      "Iteration 47240 Training loss 0.003148153889924288 Validation loss 0.010476501658558846 Accuracy 0.8857421875\n",
      "Iteration 47250 Training loss 0.003561333054676652 Validation loss 0.010339928790926933 Accuracy 0.88671875\n",
      "Iteration 47260 Training loss 0.004397289361804724 Validation loss 0.010365607216954231 Accuracy 0.88671875\n",
      "Iteration 47270 Training loss 0.0021054542157799006 Validation loss 0.010428259149193764 Accuracy 0.88525390625\n",
      "Iteration 47280 Training loss 0.004290164448320866 Validation loss 0.010392429307103157 Accuracy 0.8857421875\n",
      "Iteration 47290 Training loss 0.004286619368940592 Validation loss 0.010392348282039165 Accuracy 0.8857421875\n",
      "Iteration 47300 Training loss 0.004025725647807121 Validation loss 0.010486735962331295 Accuracy 0.8837890625\n",
      "Iteration 47310 Training loss 0.004488817881792784 Validation loss 0.010485795326530933 Accuracy 0.8857421875\n",
      "Iteration 47320 Training loss 0.0038171110209077597 Validation loss 0.01059933565557003 Accuracy 0.8828125\n",
      "Iteration 47330 Training loss 0.004938960075378418 Validation loss 0.01046375185251236 Accuracy 0.88427734375\n",
      "Iteration 47340 Training loss 0.0025526101235300303 Validation loss 0.010493258945643902 Accuracy 0.8857421875\n",
      "Iteration 47350 Training loss 0.002969813533127308 Validation loss 0.010479331016540527 Accuracy 0.88525390625\n",
      "Iteration 47360 Training loss 0.0049047404900193214 Validation loss 0.010589289478957653 Accuracy 0.88330078125\n",
      "Iteration 47370 Training loss 0.003097199834883213 Validation loss 0.010423830710351467 Accuracy 0.88525390625\n",
      "Iteration 47380 Training loss 0.004040752537548542 Validation loss 0.01062809582799673 Accuracy 0.8828125\n",
      "Iteration 47390 Training loss 0.0030575233977288008 Validation loss 0.010585447773337364 Accuracy 0.8837890625\n",
      "Iteration 47400 Training loss 0.004674622323364019 Validation loss 0.010511702857911587 Accuracy 0.88330078125\n",
      "Iteration 47410 Training loss 0.004275583662092686 Validation loss 0.010485516861081123 Accuracy 0.88427734375\n",
      "Iteration 47420 Training loss 0.0031103051733225584 Validation loss 0.010473349131643772 Accuracy 0.884765625\n",
      "Iteration 47430 Training loss 0.003073045052587986 Validation loss 0.010391821153461933 Accuracy 0.88623046875\n",
      "Iteration 47440 Training loss 0.00240899994969368 Validation loss 0.010433743707835674 Accuracy 0.88525390625\n",
      "Iteration 47450 Training loss 0.002208087360486388 Validation loss 0.010429784655570984 Accuracy 0.88525390625\n",
      "Iteration 47460 Training loss 0.002606219844892621 Validation loss 0.010492843575775623 Accuracy 0.88427734375\n",
      "Iteration 47470 Training loss 0.004973213654011488 Validation loss 0.010557127185165882 Accuracy 0.88427734375\n",
      "Iteration 47480 Training loss 0.0029054584447294474 Validation loss 0.010494526475667953 Accuracy 0.884765625\n",
      "Iteration 47490 Training loss 0.00448657525703311 Validation loss 0.010620111599564552 Accuracy 0.88330078125\n",
      "Iteration 47500 Training loss 0.0019422156037762761 Validation loss 0.010500828735530376 Accuracy 0.88525390625\n",
      "Iteration 47510 Training loss 0.0030355663038790226 Validation loss 0.010590908117592335 Accuracy 0.8837890625\n",
      "Iteration 47520 Training loss 0.002968486864119768 Validation loss 0.01056609209626913 Accuracy 0.884765625\n",
      "Iteration 47530 Training loss 0.00374512723647058 Validation loss 0.010460942052304745 Accuracy 0.884765625\n",
      "Iteration 47540 Training loss 0.003312586108222604 Validation loss 0.010414741933345795 Accuracy 0.8857421875\n",
      "Iteration 47550 Training loss 0.001719112740829587 Validation loss 0.010380192659795284 Accuracy 0.88623046875\n",
      "Iteration 47560 Training loss 0.00353095936588943 Validation loss 0.010342096909880638 Accuracy 0.88720703125\n",
      "Iteration 47570 Training loss 0.004106230568140745 Validation loss 0.01034234743565321 Accuracy 0.88671875\n",
      "Iteration 47580 Training loss 0.0035543444100767374 Validation loss 0.010535828769207 Accuracy 0.8837890625\n",
      "Iteration 47590 Training loss 0.003129294142127037 Validation loss 0.010437142103910446 Accuracy 0.88525390625\n",
      "Iteration 47600 Training loss 0.002819676650688052 Validation loss 0.010401292704045773 Accuracy 0.88525390625\n",
      "Iteration 47610 Training loss 0.0029466634150594473 Validation loss 0.010453607887029648 Accuracy 0.8857421875\n",
      "Iteration 47620 Training loss 0.0037099195178598166 Validation loss 0.010414808988571167 Accuracy 0.88623046875\n",
      "Iteration 47630 Training loss 0.0025522431824356318 Validation loss 0.010379007086157799 Accuracy 0.88671875\n",
      "Iteration 47640 Training loss 0.003814568044617772 Validation loss 0.010435864329338074 Accuracy 0.88623046875\n",
      "Iteration 47650 Training loss 0.0040425206534564495 Validation loss 0.010440928861498833 Accuracy 0.8857421875\n",
      "Iteration 47660 Training loss 0.003904039738699794 Validation loss 0.010346266441047192 Accuracy 0.88720703125\n",
      "Iteration 47670 Training loss 0.0037879329174757004 Validation loss 0.01041101012378931 Accuracy 0.88623046875\n",
      "Iteration 47680 Training loss 0.0016771028749644756 Validation loss 0.010436453856527805 Accuracy 0.88525390625\n",
      "Iteration 47690 Training loss 0.0030287220142781734 Validation loss 0.010470593348145485 Accuracy 0.8857421875\n",
      "Iteration 47700 Training loss 0.0032219316344708204 Validation loss 0.010622155852615833 Accuracy 0.88330078125\n",
      "Iteration 47710 Training loss 0.00415578531101346 Validation loss 0.010327687487006187 Accuracy 0.8876953125\n",
      "Iteration 47720 Training loss 0.003779531456530094 Validation loss 0.010451273992657661 Accuracy 0.88623046875\n",
      "Iteration 47730 Training loss 0.003411287674680352 Validation loss 0.01058351993560791 Accuracy 0.8837890625\n",
      "Iteration 47740 Training loss 0.0031535844318568707 Validation loss 0.010528886690735817 Accuracy 0.884765625\n",
      "Iteration 47750 Training loss 0.0036293829325586557 Validation loss 0.01060507446527481 Accuracy 0.8837890625\n",
      "Iteration 47760 Training loss 0.0035456176847219467 Validation loss 0.010487182065844536 Accuracy 0.8857421875\n",
      "Iteration 47770 Training loss 0.002747554099187255 Validation loss 0.01056976430118084 Accuracy 0.88427734375\n",
      "Iteration 47780 Training loss 0.0031497590243816376 Validation loss 0.010442336089909077 Accuracy 0.88671875\n",
      "Iteration 47790 Training loss 0.004100323189049959 Validation loss 0.010553888976573944 Accuracy 0.88427734375\n",
      "Iteration 47800 Training loss 0.0032310541719198227 Validation loss 0.010484262369573116 Accuracy 0.884765625\n",
      "Iteration 47810 Training loss 0.0030374389607459307 Validation loss 0.010629047639667988 Accuracy 0.8837890625\n",
      "Iteration 47820 Training loss 0.0026450552977621555 Validation loss 0.01052059791982174 Accuracy 0.88427734375\n",
      "Iteration 47830 Training loss 0.0027635854203253984 Validation loss 0.010512706823647022 Accuracy 0.88525390625\n",
      "Iteration 47840 Training loss 0.0050482782535254955 Validation loss 0.0104455491527915 Accuracy 0.88525390625\n",
      "Iteration 47850 Training loss 0.003279097145423293 Validation loss 0.010612166486680508 Accuracy 0.8837890625\n",
      "Iteration 47860 Training loss 0.0029556744266301394 Validation loss 0.010438754223287106 Accuracy 0.88671875\n",
      "Iteration 47870 Training loss 0.003408018732443452 Validation loss 0.010618217289447784 Accuracy 0.88427734375\n",
      "Iteration 47880 Training loss 0.0034105151426047087 Validation loss 0.01032580528408289 Accuracy 0.88720703125\n",
      "Iteration 47890 Training loss 0.004203236661851406 Validation loss 0.010456117801368237 Accuracy 0.88525390625\n",
      "Iteration 47900 Training loss 0.004270026460289955 Validation loss 0.01042625866830349 Accuracy 0.88623046875\n",
      "Iteration 47910 Training loss 0.0035964364651590586 Validation loss 0.010561459697782993 Accuracy 0.88427734375\n",
      "Iteration 47920 Training loss 0.003152789082378149 Validation loss 0.010465706698596478 Accuracy 0.8857421875\n",
      "Iteration 47930 Training loss 0.0029040775261819363 Validation loss 0.010347899980843067 Accuracy 0.88671875\n",
      "Iteration 47940 Training loss 0.004687462467700243 Validation loss 0.010418277233839035 Accuracy 0.88720703125\n",
      "Iteration 47950 Training loss 0.0038753566332161427 Validation loss 0.010462953709065914 Accuracy 0.88623046875\n",
      "Iteration 47960 Training loss 0.0028169886209070683 Validation loss 0.010458478704094887 Accuracy 0.88623046875\n",
      "Iteration 47970 Training loss 0.004373032134026289 Validation loss 0.01043002214282751 Accuracy 0.88671875\n",
      "Iteration 47980 Training loss 0.004257144872099161 Validation loss 0.010482292622327805 Accuracy 0.88427734375\n",
      "Iteration 47990 Training loss 0.0026972084306180477 Validation loss 0.010549454018473625 Accuracy 0.88427734375\n",
      "Iteration 48000 Training loss 0.0024530738592147827 Validation loss 0.01043078862130642 Accuracy 0.884765625\n",
      "Iteration 48010 Training loss 0.003904708195477724 Validation loss 0.010460960678756237 Accuracy 0.88427734375\n",
      "Iteration 48020 Training loss 0.0022506751120090485 Validation loss 0.010613463819026947 Accuracy 0.88427734375\n",
      "Iteration 48030 Training loss 0.003829455003142357 Validation loss 0.01046457514166832 Accuracy 0.8837890625\n",
      "Iteration 48040 Training loss 0.003804005915299058 Validation loss 0.01048582885414362 Accuracy 0.88427734375\n",
      "Iteration 48050 Training loss 0.003658510744571686 Validation loss 0.010563261806964874 Accuracy 0.8837890625\n",
      "Iteration 48060 Training loss 0.004182017408311367 Validation loss 0.010597523301839828 Accuracy 0.88330078125\n",
      "Iteration 48070 Training loss 0.004160498734563589 Validation loss 0.010518182069063187 Accuracy 0.88330078125\n",
      "Iteration 48080 Training loss 0.003150038653984666 Validation loss 0.010471122339367867 Accuracy 0.884765625\n",
      "Iteration 48090 Training loss 0.0028732598293572664 Validation loss 0.010531613603234291 Accuracy 0.88427734375\n",
      "Iteration 48100 Training loss 0.0037581450305879116 Validation loss 0.010524751618504524 Accuracy 0.8837890625\n",
      "Iteration 48110 Training loss 0.002417107345536351 Validation loss 0.010457457974553108 Accuracy 0.8857421875\n",
      "Iteration 48120 Training loss 0.0034361814614385366 Validation loss 0.010532709769904613 Accuracy 0.884765625\n",
      "Iteration 48130 Training loss 0.0038646862376481295 Validation loss 0.010444148443639278 Accuracy 0.88623046875\n",
      "Iteration 48140 Training loss 0.004223034717142582 Validation loss 0.010466066189110279 Accuracy 0.88525390625\n",
      "Iteration 48150 Training loss 0.0033226870000362396 Validation loss 0.010545013472437859 Accuracy 0.8837890625\n",
      "Iteration 48160 Training loss 0.0027945886831730604 Validation loss 0.010527191683650017 Accuracy 0.88427734375\n",
      "Iteration 48170 Training loss 0.004167781211435795 Validation loss 0.01066216267645359 Accuracy 0.88232421875\n",
      "Iteration 48180 Training loss 0.0048013185150921345 Validation loss 0.01076118741184473 Accuracy 0.88134765625\n",
      "Iteration 48190 Training loss 0.002866404829546809 Validation loss 0.010635215789079666 Accuracy 0.8828125\n",
      "Iteration 48200 Training loss 0.004189027938991785 Validation loss 0.010553569532930851 Accuracy 0.8837890625\n",
      "Iteration 48210 Training loss 0.003673204453662038 Validation loss 0.010581353679299355 Accuracy 0.8837890625\n",
      "Iteration 48220 Training loss 0.0033568579237908125 Validation loss 0.01045201625674963 Accuracy 0.8857421875\n",
      "Iteration 48230 Training loss 0.004210872575640678 Validation loss 0.010595518164336681 Accuracy 0.884765625\n",
      "Iteration 48240 Training loss 0.0040620435029268265 Validation loss 0.010520563460886478 Accuracy 0.88427734375\n",
      "Iteration 48250 Training loss 0.0021824759896844625 Validation loss 0.010463583283126354 Accuracy 0.88427734375\n",
      "Iteration 48260 Training loss 0.004019719548523426 Validation loss 0.01050879992544651 Accuracy 0.88427734375\n",
      "Iteration 48270 Training loss 0.004292528610676527 Validation loss 0.010431894101202488 Accuracy 0.88525390625\n",
      "Iteration 48280 Training loss 0.0029334169812500477 Validation loss 0.010444428771734238 Accuracy 0.88525390625\n",
      "Iteration 48290 Training loss 0.006760308984667063 Validation loss 0.011066405102610588 Accuracy 0.87744140625\n",
      "Iteration 48300 Training loss 0.003376452485099435 Validation loss 0.010527378879487514 Accuracy 0.88427734375\n",
      "Iteration 48310 Training loss 0.003247038461267948 Validation loss 0.010479328222572803 Accuracy 0.88623046875\n",
      "Iteration 48320 Training loss 0.00279605807736516 Validation loss 0.010507538914680481 Accuracy 0.884765625\n",
      "Iteration 48330 Training loss 0.004431893117725849 Validation loss 0.010576721280813217 Accuracy 0.88330078125\n",
      "Iteration 48340 Training loss 0.003031297354027629 Validation loss 0.010483239777386189 Accuracy 0.88525390625\n",
      "Iteration 48350 Training loss 0.0051032304763793945 Validation loss 0.010574502870440483 Accuracy 0.88427734375\n",
      "Iteration 48360 Training loss 0.003645937889814377 Validation loss 0.01042859721928835 Accuracy 0.88623046875\n",
      "Iteration 48370 Training loss 0.0029447015840560198 Validation loss 0.010577034205198288 Accuracy 0.88525390625\n",
      "Iteration 48380 Training loss 0.003664217656478286 Validation loss 0.010420006699860096 Accuracy 0.8857421875\n",
      "Iteration 48390 Training loss 0.003168753581121564 Validation loss 0.01050198171287775 Accuracy 0.88427734375\n",
      "Iteration 48400 Training loss 0.004224969074130058 Validation loss 0.01045952644199133 Accuracy 0.88525390625\n",
      "Iteration 48410 Training loss 0.0019008597591891885 Validation loss 0.010499724186956882 Accuracy 0.88427734375\n",
      "Iteration 48420 Training loss 0.003440604545176029 Validation loss 0.01063980720937252 Accuracy 0.88330078125\n",
      "Iteration 48430 Training loss 0.0029218895360827446 Validation loss 0.010489738546311855 Accuracy 0.88525390625\n",
      "Iteration 48440 Training loss 0.005088127218186855 Validation loss 0.010469507426023483 Accuracy 0.884765625\n",
      "Iteration 48450 Training loss 0.002773214830085635 Validation loss 0.010505556128919125 Accuracy 0.88525390625\n",
      "Iteration 48460 Training loss 0.0035164738073945045 Validation loss 0.010491922497749329 Accuracy 0.88525390625\n",
      "Iteration 48470 Training loss 0.003280533477663994 Validation loss 0.010481264442205429 Accuracy 0.8857421875\n",
      "Iteration 48480 Training loss 0.0031578291673213243 Validation loss 0.010443267412483692 Accuracy 0.88623046875\n",
      "Iteration 48490 Training loss 0.0028141364455223083 Validation loss 0.010577904991805553 Accuracy 0.884765625\n",
      "Iteration 48500 Training loss 0.0029576919041574 Validation loss 0.010467173531651497 Accuracy 0.88525390625\n",
      "Iteration 48510 Training loss 0.0035379123874008656 Validation loss 0.010402635671198368 Accuracy 0.88720703125\n",
      "Iteration 48520 Training loss 0.0040565673261880875 Validation loss 0.010477668605744839 Accuracy 0.88623046875\n",
      "Iteration 48530 Training loss 0.002827618271112442 Validation loss 0.010597268119454384 Accuracy 0.88427734375\n",
      "Iteration 48540 Training loss 0.0036155933048576117 Validation loss 0.010673383250832558 Accuracy 0.88330078125\n",
      "Iteration 48550 Training loss 0.0034642149694263935 Validation loss 0.01047018263489008 Accuracy 0.8857421875\n",
      "Iteration 48560 Training loss 0.0033755649346858263 Validation loss 0.01052727922797203 Accuracy 0.88623046875\n",
      "Iteration 48570 Training loss 0.003121963469311595 Validation loss 0.010465322062373161 Accuracy 0.88623046875\n",
      "Iteration 48580 Training loss 0.002714460017159581 Validation loss 0.010539643466472626 Accuracy 0.88623046875\n",
      "Iteration 48590 Training loss 0.0044880518689751625 Validation loss 0.010463608428835869 Accuracy 0.88623046875\n",
      "Iteration 48600 Training loss 0.0037658195942640305 Validation loss 0.01060281228274107 Accuracy 0.8837890625\n",
      "Iteration 48610 Training loss 0.003020789474248886 Validation loss 0.010601615533232689 Accuracy 0.884765625\n",
      "Iteration 48620 Training loss 0.003312022890895605 Validation loss 0.010589259676635265 Accuracy 0.88330078125\n",
      "Iteration 48630 Training loss 0.002964275423437357 Validation loss 0.01057843491435051 Accuracy 0.88427734375\n",
      "Iteration 48640 Training loss 0.0031796791590750217 Validation loss 0.010454458184540272 Accuracy 0.8857421875\n",
      "Iteration 48650 Training loss 0.002957539167255163 Validation loss 0.01052870973944664 Accuracy 0.884765625\n",
      "Iteration 48660 Training loss 0.004061939660459757 Validation loss 0.0104757659137249 Accuracy 0.8857421875\n",
      "Iteration 48670 Training loss 0.0031349591445177794 Validation loss 0.010594511404633522 Accuracy 0.88330078125\n",
      "Iteration 48680 Training loss 0.003192868549376726 Validation loss 0.010461235418915749 Accuracy 0.88525390625\n",
      "Iteration 48690 Training loss 0.0029032016173005104 Validation loss 0.010651142336428165 Accuracy 0.8837890625\n",
      "Iteration 48700 Training loss 0.0035357335582375526 Validation loss 0.010628355666995049 Accuracy 0.88330078125\n",
      "Iteration 48710 Training loss 0.003273901529610157 Validation loss 0.010460603050887585 Accuracy 0.884765625\n",
      "Iteration 48720 Training loss 0.002966313622891903 Validation loss 0.010468976572155952 Accuracy 0.8857421875\n",
      "Iteration 48730 Training loss 0.003496701130643487 Validation loss 0.010526242665946484 Accuracy 0.88427734375\n",
      "Iteration 48740 Training loss 0.004156552720814943 Validation loss 0.01048960443586111 Accuracy 0.88427734375\n",
      "Iteration 48750 Training loss 0.0023753310088068247 Validation loss 0.010472487658262253 Accuracy 0.8837890625\n",
      "Iteration 48760 Training loss 0.004246007651090622 Validation loss 0.010400062426924706 Accuracy 0.88671875\n",
      "Iteration 48770 Training loss 0.0035629223566502333 Validation loss 0.01048539113253355 Accuracy 0.8857421875\n",
      "Iteration 48780 Training loss 0.004084184765815735 Validation loss 0.010728788562119007 Accuracy 0.88330078125\n",
      "Iteration 48790 Training loss 0.003330927575007081 Validation loss 0.010548886843025684 Accuracy 0.88330078125\n",
      "Iteration 48800 Training loss 0.0029722878243774176 Validation loss 0.010582253336906433 Accuracy 0.8837890625\n",
      "Iteration 48810 Training loss 0.004354377742856741 Validation loss 0.010455894283950329 Accuracy 0.88525390625\n",
      "Iteration 48820 Training loss 0.004345165099948645 Validation loss 0.01037703175097704 Accuracy 0.88623046875\n",
      "Iteration 48830 Training loss 0.003360994393005967 Validation loss 0.010791270062327385 Accuracy 0.8818359375\n",
      "Iteration 48840 Training loss 0.0030370778404176235 Validation loss 0.010474900715053082 Accuracy 0.88525390625\n",
      "Iteration 48850 Training loss 0.004055231809616089 Validation loss 0.010403540916740894 Accuracy 0.88623046875\n",
      "Iteration 48860 Training loss 0.004723364487290382 Validation loss 0.010547337122261524 Accuracy 0.88525390625\n",
      "Iteration 48870 Training loss 0.00477563077583909 Validation loss 0.010470006614923477 Accuracy 0.884765625\n",
      "Iteration 48880 Training loss 0.003507630666717887 Validation loss 0.010434340685606003 Accuracy 0.88525390625\n",
      "Iteration 48890 Training loss 0.0030906598549336195 Validation loss 0.010322841815650463 Accuracy 0.88720703125\n",
      "Iteration 48900 Training loss 0.004075481556355953 Validation loss 0.010458041913807392 Accuracy 0.88427734375\n",
      "Iteration 48910 Training loss 0.0031121254432946444 Validation loss 0.010600113309919834 Accuracy 0.8837890625\n",
      "Iteration 48920 Training loss 0.004107032436877489 Validation loss 0.010510222055017948 Accuracy 0.884765625\n",
      "Iteration 48930 Training loss 0.004095792304724455 Validation loss 0.010493249632418156 Accuracy 0.88427734375\n",
      "Iteration 48940 Training loss 0.0031290357001125813 Validation loss 0.010599963366985321 Accuracy 0.8837890625\n",
      "Iteration 48950 Training loss 0.0020023686811327934 Validation loss 0.01045672595500946 Accuracy 0.8857421875\n",
      "Iteration 48960 Training loss 0.0022828346118330956 Validation loss 0.010539726354181767 Accuracy 0.884765625\n",
      "Iteration 48970 Training loss 0.004194180481135845 Validation loss 0.010585743933916092 Accuracy 0.8837890625\n",
      "Iteration 48980 Training loss 0.0030840677209198475 Validation loss 0.010549215599894524 Accuracy 0.8837890625\n",
      "Iteration 48990 Training loss 0.004542825743556023 Validation loss 0.010556654073297977 Accuracy 0.88427734375\n",
      "Iteration 49000 Training loss 0.0025832129176706076 Validation loss 0.010567408055067062 Accuracy 0.884765625\n",
      "Iteration 49010 Training loss 0.003154057078063488 Validation loss 0.010665340349078178 Accuracy 0.8837890625\n",
      "Iteration 49020 Training loss 0.004211287014186382 Validation loss 0.010625477880239487 Accuracy 0.8837890625\n",
      "Iteration 49030 Training loss 0.0035646120086312294 Validation loss 0.010518603026866913 Accuracy 0.88427734375\n",
      "Iteration 49040 Training loss 0.002119258278980851 Validation loss 0.010494338348507881 Accuracy 0.884765625\n",
      "Iteration 49050 Training loss 0.002339782426133752 Validation loss 0.010598122142255306 Accuracy 0.8837890625\n",
      "Iteration 49060 Training loss 0.0036114202812314034 Validation loss 0.010528156533837318 Accuracy 0.88427734375\n",
      "Iteration 49070 Training loss 0.003973724320530891 Validation loss 0.010490860790014267 Accuracy 0.88427734375\n",
      "Iteration 49080 Training loss 0.0027487589977681637 Validation loss 0.010501918382942677 Accuracy 0.884765625\n",
      "Iteration 49090 Training loss 0.0029487418942153454 Validation loss 0.010451950132846832 Accuracy 0.8857421875\n",
      "Iteration 49100 Training loss 0.004137295763939619 Validation loss 0.01053825207054615 Accuracy 0.88427734375\n",
      "Iteration 49110 Training loss 0.0038483303505927324 Validation loss 0.010543148964643478 Accuracy 0.8857421875\n",
      "Iteration 49120 Training loss 0.0029854278545826674 Validation loss 0.010638932697474957 Accuracy 0.8837890625\n",
      "Iteration 49130 Training loss 0.003496419172734022 Validation loss 0.010673752054572105 Accuracy 0.88232421875\n",
      "Iteration 49140 Training loss 0.0032231940422207117 Validation loss 0.01050492748618126 Accuracy 0.88427734375\n",
      "Iteration 49150 Training loss 0.004547355696558952 Validation loss 0.01047602016478777 Accuracy 0.8857421875\n",
      "Iteration 49160 Training loss 0.0036965839099138975 Validation loss 0.010528947226703167 Accuracy 0.884765625\n",
      "Iteration 49170 Training loss 0.0037410312797874212 Validation loss 0.010445356369018555 Accuracy 0.8837890625\n",
      "Iteration 49180 Training loss 0.004102491773664951 Validation loss 0.010521458461880684 Accuracy 0.884765625\n",
      "Iteration 49190 Training loss 0.003039780305698514 Validation loss 0.010416877456009388 Accuracy 0.8857421875\n",
      "Iteration 49200 Training loss 0.003284734906628728 Validation loss 0.010529578663408756 Accuracy 0.88427734375\n",
      "Iteration 49210 Training loss 0.004659916274249554 Validation loss 0.010499689728021622 Accuracy 0.8837890625\n",
      "Iteration 49220 Training loss 0.002843532245606184 Validation loss 0.010512902401387691 Accuracy 0.88427734375\n",
      "Iteration 49230 Training loss 0.0037948417011648417 Validation loss 0.010453698225319386 Accuracy 0.8837890625\n",
      "Iteration 49240 Training loss 0.00391743378713727 Validation loss 0.010603200644254684 Accuracy 0.8837890625\n",
      "Iteration 49250 Training loss 0.004957827739417553 Validation loss 0.010570922866463661 Accuracy 0.8837890625\n",
      "Iteration 49260 Training loss 0.0022403886541724205 Validation loss 0.010470900684595108 Accuracy 0.88525390625\n",
      "Iteration 49270 Training loss 0.004477709531784058 Validation loss 0.0105291698127985 Accuracy 0.884765625\n",
      "Iteration 49280 Training loss 0.004157708492130041 Validation loss 0.01052205078303814 Accuracy 0.8837890625\n",
      "Iteration 49290 Training loss 0.004512023646384478 Validation loss 0.010468775406479836 Accuracy 0.88427734375\n",
      "Iteration 49300 Training loss 0.0033980088774114847 Validation loss 0.010497280396521091 Accuracy 0.884765625\n",
      "Iteration 49310 Training loss 0.002654583193361759 Validation loss 0.010568114928901196 Accuracy 0.8837890625\n",
      "Iteration 49320 Training loss 0.0038390846457332373 Validation loss 0.010554883629083633 Accuracy 0.88427734375\n",
      "Iteration 49330 Training loss 0.0034937693271785975 Validation loss 0.010614086873829365 Accuracy 0.884765625\n",
      "Iteration 49340 Training loss 0.003077648114413023 Validation loss 0.010524110868573189 Accuracy 0.8837890625\n",
      "Iteration 49350 Training loss 0.002463442040607333 Validation loss 0.010459301061928272 Accuracy 0.88525390625\n",
      "Iteration 49360 Training loss 0.0024127718061208725 Validation loss 0.010525625199079514 Accuracy 0.88525390625\n",
      "Iteration 49370 Training loss 0.00230771373026073 Validation loss 0.010573222301900387 Accuracy 0.88427734375\n",
      "Iteration 49380 Training loss 0.0029801768250763416 Validation loss 0.010591477155685425 Accuracy 0.88427734375\n",
      "Iteration 49390 Training loss 0.0036311731673777103 Validation loss 0.01063861045986414 Accuracy 0.8828125\n",
      "Iteration 49400 Training loss 0.003690783167257905 Validation loss 0.01050316821783781 Accuracy 0.8837890625\n",
      "Iteration 49410 Training loss 0.0030975171830505133 Validation loss 0.010605581104755402 Accuracy 0.88427734375\n",
      "Iteration 49420 Training loss 0.004261931870132685 Validation loss 0.01055172085762024 Accuracy 0.88427734375\n",
      "Iteration 49430 Training loss 0.002784510375931859 Validation loss 0.010574275627732277 Accuracy 0.88427734375\n",
      "Iteration 49440 Training loss 0.0035539064556360245 Validation loss 0.010593293234705925 Accuracy 0.88427734375\n",
      "Iteration 49450 Training loss 0.002932980889454484 Validation loss 0.010552294552326202 Accuracy 0.8837890625\n",
      "Iteration 49460 Training loss 0.0039347633719444275 Validation loss 0.010617229156196117 Accuracy 0.884765625\n",
      "Iteration 49470 Training loss 0.003557001007720828 Validation loss 0.010557587258517742 Accuracy 0.88427734375\n",
      "Iteration 49480 Training loss 0.004941281396895647 Validation loss 0.010489565320312977 Accuracy 0.88525390625\n",
      "Iteration 49490 Training loss 0.002994278911501169 Validation loss 0.010584686882793903 Accuracy 0.88427734375\n",
      "Iteration 49500 Training loss 0.0025837854482233524 Validation loss 0.010639019310474396 Accuracy 0.88330078125\n",
      "Iteration 49510 Training loss 0.004376713186502457 Validation loss 0.01063615083694458 Accuracy 0.8837890625\n",
      "Iteration 49520 Training loss 0.003169227857142687 Validation loss 0.01047750934958458 Accuracy 0.88623046875\n",
      "Iteration 49530 Training loss 0.002789749065414071 Validation loss 0.010547949001193047 Accuracy 0.884765625\n",
      "Iteration 49540 Training loss 0.0027471857611089945 Validation loss 0.010521954856812954 Accuracy 0.88427734375\n",
      "Iteration 49550 Training loss 0.002617668593302369 Validation loss 0.010418422520160675 Accuracy 0.88623046875\n",
      "Iteration 49560 Training loss 0.004165428224951029 Validation loss 0.010512294247746468 Accuracy 0.8857421875\n",
      "Iteration 49570 Training loss 0.004026902839541435 Validation loss 0.010766112245619297 Accuracy 0.8818359375\n",
      "Iteration 49580 Training loss 0.004735133610665798 Validation loss 0.010593119077384472 Accuracy 0.88427734375\n",
      "Iteration 49590 Training loss 0.004330924712121487 Validation loss 0.010549183934926987 Accuracy 0.884765625\n",
      "Iteration 49600 Training loss 0.0046923356130719185 Validation loss 0.01054120622575283 Accuracy 0.884765625\n",
      "Iteration 49610 Training loss 0.0034053109120577574 Validation loss 0.010620960034430027 Accuracy 0.8837890625\n",
      "Iteration 49620 Training loss 0.0046162838116288185 Validation loss 0.010658969171345234 Accuracy 0.88330078125\n",
      "Iteration 49630 Training loss 0.004579249769449234 Validation loss 0.01050788164138794 Accuracy 0.88623046875\n",
      "Iteration 49640 Training loss 0.0031415068078786135 Validation loss 0.010516424663364887 Accuracy 0.88525390625\n",
      "Iteration 49650 Training loss 0.004086353816092014 Validation loss 0.010529165156185627 Accuracy 0.884765625\n",
      "Iteration 49660 Training loss 0.002412252128124237 Validation loss 0.010598801076412201 Accuracy 0.88427734375\n",
      "Iteration 49670 Training loss 0.003067248733714223 Validation loss 0.010557573288679123 Accuracy 0.88525390625\n",
      "Iteration 49680 Training loss 0.004268404096364975 Validation loss 0.010478168725967407 Accuracy 0.8857421875\n",
      "Iteration 49690 Training loss 0.0032057869248092175 Validation loss 0.01060895249247551 Accuracy 0.88427734375\n",
      "Iteration 49700 Training loss 0.0034566493704915047 Validation loss 0.010545707307755947 Accuracy 0.884765625\n",
      "Iteration 49710 Training loss 0.003677238477393985 Validation loss 0.010494999587535858 Accuracy 0.8857421875\n",
      "Iteration 49720 Training loss 0.004831829108297825 Validation loss 0.010520058684051037 Accuracy 0.88623046875\n",
      "Iteration 49730 Training loss 0.004436536226421595 Validation loss 0.010513490997254848 Accuracy 0.8857421875\n",
      "Iteration 49740 Training loss 0.0037331192288547754 Validation loss 0.010519699193537235 Accuracy 0.884765625\n",
      "Iteration 49750 Training loss 0.003885872196406126 Validation loss 0.010577795095741749 Accuracy 0.8837890625\n",
      "Iteration 49760 Training loss 0.003611810738220811 Validation loss 0.010594204068183899 Accuracy 0.884765625\n",
      "Iteration 49770 Training loss 0.0028145438991487026 Validation loss 0.010581878013908863 Accuracy 0.88427734375\n",
      "Iteration 49780 Training loss 0.004316526930779219 Validation loss 0.010882832109928131 Accuracy 0.880859375\n",
      "Iteration 49790 Training loss 0.0029933147598057985 Validation loss 0.010502254590392113 Accuracy 0.8857421875\n",
      "Iteration 49800 Training loss 0.0036616160068660975 Validation loss 0.01056080311536789 Accuracy 0.88427734375\n",
      "Iteration 49810 Training loss 0.0024928892962634563 Validation loss 0.010483705438673496 Accuracy 0.88623046875\n",
      "Iteration 49820 Training loss 0.0032415485475212336 Validation loss 0.010504411533474922 Accuracy 0.884765625\n",
      "Iteration 49830 Training loss 0.003938318695873022 Validation loss 0.010502943769097328 Accuracy 0.884765625\n",
      "Iteration 49840 Training loss 0.0028264811262488365 Validation loss 0.010479560121893883 Accuracy 0.88525390625\n",
      "Iteration 49850 Training loss 0.0035882212687283754 Validation loss 0.010550341568887234 Accuracy 0.88427734375\n",
      "Iteration 49860 Training loss 0.0030430101323872805 Validation loss 0.010589147917926311 Accuracy 0.88427734375\n",
      "Iteration 49870 Training loss 0.0025478890165686607 Validation loss 0.010647187009453773 Accuracy 0.8828125\n",
      "Iteration 49880 Training loss 0.0030071772634983063 Validation loss 0.010520885698497295 Accuracy 0.8837890625\n",
      "Iteration 49890 Training loss 0.002929431153461337 Validation loss 0.01053661946207285 Accuracy 0.884765625\n",
      "Iteration 49900 Training loss 0.003730497322976589 Validation loss 0.01061005238443613 Accuracy 0.884765625\n",
      "Iteration 49910 Training loss 0.004607867915183306 Validation loss 0.010566308163106441 Accuracy 0.884765625\n",
      "Iteration 49920 Training loss 0.002552253659814596 Validation loss 0.010663495399057865 Accuracy 0.8818359375\n",
      "Iteration 49930 Training loss 0.0036410370375961065 Validation loss 0.010554184205830097 Accuracy 0.88427734375\n",
      "Iteration 49940 Training loss 0.0026545515283942223 Validation loss 0.01058530155569315 Accuracy 0.88330078125\n",
      "Iteration 49950 Training loss 0.002626158995553851 Validation loss 0.010638763196766376 Accuracy 0.88330078125\n",
      "Iteration 49960 Training loss 0.003532005939632654 Validation loss 0.01055237278342247 Accuracy 0.8837890625\n",
      "Iteration 49970 Training loss 0.0037026857025921345 Validation loss 0.010600108653306961 Accuracy 0.88232421875\n",
      "Iteration 49980 Training loss 0.004006069619208574 Validation loss 0.010634350590407848 Accuracy 0.8828125\n",
      "Iteration 49990 Training loss 0.004580450244247913 Validation loss 0.010572138242423534 Accuracy 0.88427734375\n",
      "Iteration 50000 Training loss 0.003699294989928603 Validation loss 0.010588323697447777 Accuracy 0.88427734375\n",
      "Iteration 50010 Training loss 0.004652692936360836 Validation loss 0.010599957779049873 Accuracy 0.88427734375\n",
      "Iteration 50020 Training loss 0.003612064989283681 Validation loss 0.01055159606039524 Accuracy 0.88525390625\n",
      "Iteration 50030 Training loss 0.0035266857594251633 Validation loss 0.010512477718293667 Accuracy 0.8837890625\n",
      "Iteration 50040 Training loss 0.0033858343958854675 Validation loss 0.010580242611467838 Accuracy 0.8837890625\n",
      "Iteration 50050 Training loss 0.004031747113913298 Validation loss 0.010574445128440857 Accuracy 0.88427734375\n",
      "Iteration 50060 Training loss 0.003113354090601206 Validation loss 0.010545740835368633 Accuracy 0.884765625\n",
      "Iteration 50070 Training loss 0.00387983163818717 Validation loss 0.010611900128424168 Accuracy 0.884765625\n",
      "Iteration 50080 Training loss 0.004404490813612938 Validation loss 0.010605111718177795 Accuracy 0.88330078125\n",
      "Iteration 50090 Training loss 0.003695863066241145 Validation loss 0.010627204552292824 Accuracy 0.8837890625\n",
      "Iteration 50100 Training loss 0.004780985880643129 Validation loss 0.010538800619542599 Accuracy 0.884765625\n",
      "Iteration 50110 Training loss 0.002808624180033803 Validation loss 0.010608823038637638 Accuracy 0.88427734375\n",
      "Iteration 50120 Training loss 0.0031780884601175785 Validation loss 0.010513796471059322 Accuracy 0.88525390625\n",
      "Iteration 50130 Training loss 0.004012119024991989 Validation loss 0.010561508126556873 Accuracy 0.884765625\n",
      "Iteration 50140 Training loss 0.0028831935487687588 Validation loss 0.010674955323338509 Accuracy 0.8837890625\n",
      "Iteration 50150 Training loss 0.002972621936351061 Validation loss 0.010548903606832027 Accuracy 0.88427734375\n",
      "Iteration 50160 Training loss 0.0027492763474583626 Validation loss 0.010548478923738003 Accuracy 0.88525390625\n",
      "Iteration 50170 Training loss 0.0033349322620779276 Validation loss 0.010755911469459534 Accuracy 0.8818359375\n",
      "Iteration 50180 Training loss 0.0030831664334982634 Validation loss 0.01054750569164753 Accuracy 0.8837890625\n",
      "Iteration 50190 Training loss 0.0031574585009366274 Validation loss 0.010476156137883663 Accuracy 0.88427734375\n",
      "Iteration 50200 Training loss 0.003965469542890787 Validation loss 0.010560227558016777 Accuracy 0.8837890625\n",
      "Iteration 50210 Training loss 0.003161100437864661 Validation loss 0.010522561147809029 Accuracy 0.884765625\n",
      "Iteration 50220 Training loss 0.004124243278056383 Validation loss 0.01050845067948103 Accuracy 0.884765625\n",
      "Iteration 50230 Training loss 0.0023823524825274944 Validation loss 0.010526140220463276 Accuracy 0.88427734375\n",
      "Iteration 50240 Training loss 0.003189953975379467 Validation loss 0.010553193278610706 Accuracy 0.884765625\n",
      "Iteration 50250 Training loss 0.0035414695739746094 Validation loss 0.010608697310090065 Accuracy 0.8837890625\n",
      "Iteration 50260 Training loss 0.00230246689170599 Validation loss 0.0105279004201293 Accuracy 0.8837890625\n",
      "Iteration 50270 Training loss 0.0023614377714693546 Validation loss 0.010542844422161579 Accuracy 0.884765625\n",
      "Iteration 50280 Training loss 0.0024536841083317995 Validation loss 0.010609721764922142 Accuracy 0.88427734375\n",
      "Iteration 50290 Training loss 0.004864794202148914 Validation loss 0.010694821365177631 Accuracy 0.88330078125\n",
      "Iteration 50300 Training loss 0.003726891241967678 Validation loss 0.010590084828436375 Accuracy 0.88427734375\n",
      "Iteration 50310 Training loss 0.0028116789180785418 Validation loss 0.01069076918065548 Accuracy 0.8828125\n",
      "Iteration 50320 Training loss 0.003203639527782798 Validation loss 0.010663577355444431 Accuracy 0.88232421875\n",
      "Iteration 50330 Training loss 0.004166366066783667 Validation loss 0.01064948458224535 Accuracy 0.88330078125\n",
      "Iteration 50340 Training loss 0.004097992088645697 Validation loss 0.010626127943396568 Accuracy 0.88330078125\n",
      "Iteration 50350 Training loss 0.003640101058408618 Validation loss 0.010597248561680317 Accuracy 0.8828125\n",
      "Iteration 50360 Training loss 0.0029561235569417477 Validation loss 0.01055694930255413 Accuracy 0.8837890625\n",
      "Iteration 50370 Training loss 0.001519882818683982 Validation loss 0.010581707581877708 Accuracy 0.88330078125\n",
      "Iteration 50380 Training loss 0.005351023282855749 Validation loss 0.010548112913966179 Accuracy 0.8837890625\n",
      "Iteration 50390 Training loss 0.0032525600399821997 Validation loss 0.01067076250910759 Accuracy 0.88134765625\n",
      "Iteration 50400 Training loss 0.003184900153428316 Validation loss 0.010525786317884922 Accuracy 0.88427734375\n",
      "Iteration 50410 Training loss 0.0026923855766654015 Validation loss 0.01065879873931408 Accuracy 0.8828125\n",
      "Iteration 50420 Training loss 0.0034643313847482204 Validation loss 0.010604104958474636 Accuracy 0.88427734375\n",
      "Iteration 50430 Training loss 0.0035694516263902187 Validation loss 0.010523512959480286 Accuracy 0.88427734375\n",
      "Iteration 50440 Training loss 0.003376460401341319 Validation loss 0.010619403794407845 Accuracy 0.8828125\n",
      "Iteration 50450 Training loss 0.0026819100603461266 Validation loss 0.010589568875730038 Accuracy 0.8828125\n",
      "Iteration 50460 Training loss 0.0036879233084619045 Validation loss 0.010596656240522861 Accuracy 0.88232421875\n",
      "Iteration 50470 Training loss 0.001658139517530799 Validation loss 0.0106257488951087 Accuracy 0.88330078125\n",
      "Iteration 50480 Training loss 0.00354087189771235 Validation loss 0.010577165521681309 Accuracy 0.8828125\n",
      "Iteration 50490 Training loss 0.0034952659625560045 Validation loss 0.010559305548667908 Accuracy 0.88330078125\n",
      "Iteration 50500 Training loss 0.0027632724959403276 Validation loss 0.010727687738835812 Accuracy 0.880859375\n",
      "Iteration 50510 Training loss 0.003384585026651621 Validation loss 0.010724704712629318 Accuracy 0.88232421875\n",
      "Iteration 50520 Training loss 0.004371327813714743 Validation loss 0.010672111995518208 Accuracy 0.8818359375\n",
      "Iteration 50530 Training loss 0.004396530799567699 Validation loss 0.010652673430740833 Accuracy 0.88330078125\n",
      "Iteration 50540 Training loss 0.003715069964528084 Validation loss 0.010634655132889748 Accuracy 0.88330078125\n",
      "Iteration 50550 Training loss 0.0034655157942324877 Validation loss 0.010583889670670033 Accuracy 0.8837890625\n",
      "Iteration 50560 Training loss 0.004284337162971497 Validation loss 0.010586582124233246 Accuracy 0.8837890625\n",
      "Iteration 50570 Training loss 0.002604519249871373 Validation loss 0.010567386634647846 Accuracy 0.8837890625\n",
      "Iteration 50580 Training loss 0.0037958850152790546 Validation loss 0.010712177492678165 Accuracy 0.88330078125\n",
      "Iteration 50590 Training loss 0.004085075110197067 Validation loss 0.010568845085799694 Accuracy 0.8837890625\n",
      "Iteration 50600 Training loss 0.0035887667909264565 Validation loss 0.010560517199337482 Accuracy 0.88427734375\n",
      "Iteration 50610 Training loss 0.0031120306812226772 Validation loss 0.010555051267147064 Accuracy 0.8837890625\n",
      "Iteration 50620 Training loss 0.003937376197427511 Validation loss 0.010660617612302303 Accuracy 0.8837890625\n",
      "Iteration 50630 Training loss 0.0048689572140574455 Validation loss 0.01109934039413929 Accuracy 0.8779296875\n",
      "Iteration 50640 Training loss 0.0030010780319571495 Validation loss 0.010562189854681492 Accuracy 0.8837890625\n",
      "Iteration 50650 Training loss 0.0034237115178257227 Validation loss 0.010530466213822365 Accuracy 0.88427734375\n",
      "Iteration 50660 Training loss 0.0025837167631834745 Validation loss 0.010581347160041332 Accuracy 0.884765625\n",
      "Iteration 50670 Training loss 0.0038550521712750196 Validation loss 0.010586598888039589 Accuracy 0.8837890625\n",
      "Iteration 50680 Training loss 0.0027207648381590843 Validation loss 0.01059454120695591 Accuracy 0.88330078125\n",
      "Iteration 50690 Training loss 0.003898792201653123 Validation loss 0.0105405542999506 Accuracy 0.884765625\n",
      "Iteration 50700 Training loss 0.004759346134960651 Validation loss 0.010575874708592892 Accuracy 0.884765625\n",
      "Iteration 50710 Training loss 0.003786263521760702 Validation loss 0.010527453385293484 Accuracy 0.884765625\n",
      "Iteration 50720 Training loss 0.0038157023955136538 Validation loss 0.010698407888412476 Accuracy 0.88232421875\n",
      "Iteration 50730 Training loss 0.0031341782305389643 Validation loss 0.010479002259671688 Accuracy 0.88427734375\n",
      "Iteration 50740 Training loss 0.0030187664087861776 Validation loss 0.010482320562005043 Accuracy 0.88525390625\n",
      "Iteration 50750 Training loss 0.0025243540294468403 Validation loss 0.010447469539940357 Accuracy 0.8857421875\n",
      "Iteration 50760 Training loss 0.003553176298737526 Validation loss 0.010467630811035633 Accuracy 0.884765625\n",
      "Iteration 50770 Training loss 0.002355393022298813 Validation loss 0.010623229667544365 Accuracy 0.8828125\n",
      "Iteration 50780 Training loss 0.002980700461193919 Validation loss 0.010496426373720169 Accuracy 0.88427734375\n",
      "Iteration 50790 Training loss 0.0034126504324376583 Validation loss 0.010561259463429451 Accuracy 0.8837890625\n",
      "Iteration 50800 Training loss 0.002400139346718788 Validation loss 0.010620767250657082 Accuracy 0.88330078125\n",
      "Iteration 50810 Training loss 0.0028385440818965435 Validation loss 0.010590917430818081 Accuracy 0.88232421875\n",
      "Iteration 50820 Training loss 0.003499629907310009 Validation loss 0.010421236045658588 Accuracy 0.88525390625\n",
      "Iteration 50830 Training loss 0.003926640842109919 Validation loss 0.010421141982078552 Accuracy 0.88623046875\n",
      "Iteration 50840 Training loss 0.0032755888532847166 Validation loss 0.010506977327167988 Accuracy 0.884765625\n",
      "Iteration 50850 Training loss 0.003561557037755847 Validation loss 0.010541325435042381 Accuracy 0.88427734375\n",
      "Iteration 50860 Training loss 0.004047033842653036 Validation loss 0.01053228136152029 Accuracy 0.88427734375\n",
      "Iteration 50870 Training loss 0.004101823549717665 Validation loss 0.010673739947378635 Accuracy 0.8837890625\n",
      "Iteration 50880 Training loss 0.0038997153751552105 Validation loss 0.010607557371258736 Accuracy 0.8828125\n",
      "Iteration 50890 Training loss 0.004594272002577782 Validation loss 0.010664019733667374 Accuracy 0.8818359375\n",
      "Iteration 50900 Training loss 0.004186857957392931 Validation loss 0.010592675767838955 Accuracy 0.88427734375\n",
      "Iteration 50910 Training loss 0.0018767445581033826 Validation loss 0.01062842644751072 Accuracy 0.88232421875\n",
      "Iteration 50920 Training loss 0.0029220206197351217 Validation loss 0.010555887594819069 Accuracy 0.8837890625\n",
      "Iteration 50930 Training loss 0.0034514369908720255 Validation loss 0.01063616294413805 Accuracy 0.88427734375\n",
      "Iteration 50940 Training loss 0.002893146825954318 Validation loss 0.010495917871594429 Accuracy 0.88525390625\n",
      "Iteration 50950 Training loss 0.0038541676476597786 Validation loss 0.010474461130797863 Accuracy 0.884765625\n",
      "Iteration 50960 Training loss 0.0022423381451517344 Validation loss 0.01049916259944439 Accuracy 0.884765625\n",
      "Iteration 50970 Training loss 0.0024204584769904613 Validation loss 0.010534367524087429 Accuracy 0.88330078125\n",
      "Iteration 50980 Training loss 0.0033797635696828365 Validation loss 0.010531645268201828 Accuracy 0.88427734375\n",
      "Iteration 50990 Training loss 0.004959986545145512 Validation loss 0.010500605218112469 Accuracy 0.88330078125\n",
      "Iteration 51000 Training loss 0.005050321109592915 Validation loss 0.01058841310441494 Accuracy 0.88427734375\n",
      "Iteration 51010 Training loss 0.00393724674358964 Validation loss 0.01065084245055914 Accuracy 0.88330078125\n",
      "Iteration 51020 Training loss 0.0032050437293946743 Validation loss 0.010648036375641823 Accuracy 0.8818359375\n",
      "Iteration 51030 Training loss 0.004296251572668552 Validation loss 0.010588224977254868 Accuracy 0.88330078125\n",
      "Iteration 51040 Training loss 0.003236706368625164 Validation loss 0.01065841969102621 Accuracy 0.8828125\n",
      "Iteration 51050 Training loss 0.0022660414688289165 Validation loss 0.010600348934531212 Accuracy 0.88427734375\n",
      "Iteration 51060 Training loss 0.002161516109481454 Validation loss 0.010569671168923378 Accuracy 0.8837890625\n",
      "Iteration 51070 Training loss 0.0016536866314709187 Validation loss 0.01052201259881258 Accuracy 0.884765625\n",
      "Iteration 51080 Training loss 0.0034957975149154663 Validation loss 0.010477661155164242 Accuracy 0.884765625\n",
      "Iteration 51090 Training loss 0.003433779114857316 Validation loss 0.01060587726533413 Accuracy 0.88427734375\n",
      "Iteration 51100 Training loss 0.002282446250319481 Validation loss 0.010521567426621914 Accuracy 0.88427734375\n",
      "Iteration 51110 Training loss 0.002982097677886486 Validation loss 0.010634343139827251 Accuracy 0.88330078125\n",
      "Iteration 51120 Training loss 0.003732365323230624 Validation loss 0.010594237595796585 Accuracy 0.88330078125\n",
      "Iteration 51130 Training loss 0.0037658310029655695 Validation loss 0.010586591437458992 Accuracy 0.884765625\n",
      "Iteration 51140 Training loss 0.0033751772716641426 Validation loss 0.010596740059554577 Accuracy 0.8837890625\n",
      "Iteration 51150 Training loss 0.002939739730209112 Validation loss 0.010709965601563454 Accuracy 0.88134765625\n",
      "Iteration 51160 Training loss 0.0034148357808589935 Validation loss 0.010501638986170292 Accuracy 0.88427734375\n",
      "Iteration 51170 Training loss 0.0039771683514118195 Validation loss 0.010472182184457779 Accuracy 0.884765625\n",
      "Iteration 51180 Training loss 0.003438885323703289 Validation loss 0.010821807198226452 Accuracy 0.8818359375\n",
      "Iteration 51190 Training loss 0.002799291629344225 Validation loss 0.010590927675366402 Accuracy 0.8837890625\n",
      "Iteration 51200 Training loss 0.00358408666215837 Validation loss 0.010597813874483109 Accuracy 0.88330078125\n",
      "Iteration 51210 Training loss 0.003402632661163807 Validation loss 0.010477898642420769 Accuracy 0.8837890625\n",
      "Iteration 51220 Training loss 0.004520629066973925 Validation loss 0.010553615167737007 Accuracy 0.88427734375\n",
      "Iteration 51230 Training loss 0.003986557479947805 Validation loss 0.010539354756474495 Accuracy 0.88330078125\n",
      "Iteration 51240 Training loss 0.0036622388288378716 Validation loss 0.010519085451960564 Accuracy 0.8837890625\n",
      "Iteration 51250 Training loss 0.003292940091341734 Validation loss 0.010626170784235 Accuracy 0.88330078125\n",
      "Iteration 51260 Training loss 0.0025742482393980026 Validation loss 0.010559389367699623 Accuracy 0.88427734375\n",
      "Iteration 51270 Training loss 0.0034336212556809187 Validation loss 0.01057206280529499 Accuracy 0.88427734375\n",
      "Iteration 51280 Training loss 0.005343096796423197 Validation loss 0.01050183828920126 Accuracy 0.88427734375\n",
      "Iteration 51290 Training loss 0.003014493267983198 Validation loss 0.010500133968889713 Accuracy 0.88330078125\n",
      "Iteration 51300 Training loss 0.0030089644715189934 Validation loss 0.01053337287157774 Accuracy 0.884765625\n",
      "Iteration 51310 Training loss 0.002722532022744417 Validation loss 0.010507064871490002 Accuracy 0.8857421875\n",
      "Iteration 51320 Training loss 0.0037448308430612087 Validation loss 0.01055951975286007 Accuracy 0.88525390625\n",
      "Iteration 51330 Training loss 0.0032205861061811447 Validation loss 0.01056256890296936 Accuracy 0.884765625\n",
      "Iteration 51340 Training loss 0.0031147301197052 Validation loss 0.010527766309678555 Accuracy 0.884765625\n",
      "Iteration 51350 Training loss 0.0038962510880082846 Validation loss 0.010543432086706161 Accuracy 0.88427734375\n",
      "Iteration 51360 Training loss 0.0023221790324896574 Validation loss 0.010456769727170467 Accuracy 0.88623046875\n",
      "Iteration 51370 Training loss 0.004773715976625681 Validation loss 0.010514289140701294 Accuracy 0.8837890625\n",
      "Iteration 51380 Training loss 0.001969355158507824 Validation loss 0.010622143745422363 Accuracy 0.8828125\n",
      "Iteration 51390 Training loss 0.004444020334631205 Validation loss 0.010502024553716183 Accuracy 0.88525390625\n",
      "Iteration 51400 Training loss 0.0028001132886856794 Validation loss 0.010534125380218029 Accuracy 0.88427734375\n",
      "Iteration 51410 Training loss 0.0033870746847242117 Validation loss 0.010470081120729446 Accuracy 0.8857421875\n",
      "Iteration 51420 Training loss 0.0021041210275143385 Validation loss 0.010447571985423565 Accuracy 0.884765625\n",
      "Iteration 51430 Training loss 0.0019850058015435934 Validation loss 0.010503634810447693 Accuracy 0.88525390625\n",
      "Iteration 51440 Training loss 0.002769282553344965 Validation loss 0.010574574582278728 Accuracy 0.88427734375\n",
      "Iteration 51450 Training loss 0.0033078640699386597 Validation loss 0.01056149136275053 Accuracy 0.8837890625\n",
      "Iteration 51460 Training loss 0.003771296702325344 Validation loss 0.010650966316461563 Accuracy 0.88232421875\n",
      "Iteration 51470 Training loss 0.003380455542355776 Validation loss 0.010485924780368805 Accuracy 0.88427734375\n",
      "Iteration 51480 Training loss 0.004243143834173679 Validation loss 0.010533441789448261 Accuracy 0.88427734375\n",
      "Iteration 51490 Training loss 0.00255397567525506 Validation loss 0.010406684130430222 Accuracy 0.88623046875\n",
      "Iteration 51500 Training loss 0.0026622863952070475 Validation loss 0.010485094971954823 Accuracy 0.884765625\n",
      "Iteration 51510 Training loss 0.002625037683174014 Validation loss 0.010507567785680294 Accuracy 0.88427734375\n",
      "Iteration 51520 Training loss 0.002552429214119911 Validation loss 0.010494994930922985 Accuracy 0.8837890625\n",
      "Iteration 51530 Training loss 0.003621087409555912 Validation loss 0.01056587416678667 Accuracy 0.8837890625\n",
      "Iteration 51540 Training loss 0.0026697067078202963 Validation loss 0.01048445887863636 Accuracy 0.8857421875\n",
      "Iteration 51550 Training loss 0.004517171066254377 Validation loss 0.010558058507740498 Accuracy 0.884765625\n",
      "Iteration 51560 Training loss 0.0037085278891026974 Validation loss 0.010460799559950829 Accuracy 0.88525390625\n",
      "Iteration 51570 Training loss 0.0030678578186780214 Validation loss 0.010488798841834068 Accuracy 0.884765625\n",
      "Iteration 51580 Training loss 0.003363472642377019 Validation loss 0.010446116328239441 Accuracy 0.88671875\n",
      "Iteration 51590 Training loss 0.003158092498779297 Validation loss 0.010492466390132904 Accuracy 0.88427734375\n",
      "Iteration 51600 Training loss 0.003954756539314985 Validation loss 0.010488924570381641 Accuracy 0.88427734375\n",
      "Iteration 51610 Training loss 0.0029161740094423294 Validation loss 0.01051107607781887 Accuracy 0.8837890625\n",
      "Iteration 51620 Training loss 0.0031239299569278955 Validation loss 0.010536305606365204 Accuracy 0.88427734375\n",
      "Iteration 51630 Training loss 0.003666542237624526 Validation loss 0.010587377473711967 Accuracy 0.88330078125\n",
      "Iteration 51640 Training loss 0.002431959845125675 Validation loss 0.010560357011854649 Accuracy 0.8837890625\n",
      "Iteration 51650 Training loss 0.003912073094397783 Validation loss 0.010500561445951462 Accuracy 0.88427734375\n",
      "Iteration 51660 Training loss 0.0034330456983298063 Validation loss 0.010596789419651031 Accuracy 0.8837890625\n",
      "Iteration 51670 Training loss 0.002507853088900447 Validation loss 0.010650797747075558 Accuracy 0.8828125\n",
      "Iteration 51680 Training loss 0.003273083595559001 Validation loss 0.010639787651598454 Accuracy 0.88330078125\n",
      "Iteration 51690 Training loss 0.0031753811053931713 Validation loss 0.010539035312831402 Accuracy 0.8837890625\n",
      "Iteration 51700 Training loss 0.003672565333545208 Validation loss 0.010616485029459 Accuracy 0.88330078125\n",
      "Iteration 51710 Training loss 0.002694070804864168 Validation loss 0.010531707666814327 Accuracy 0.8837890625\n",
      "Iteration 51720 Training loss 0.0034465419594198465 Validation loss 0.010685805231332779 Accuracy 0.88232421875\n",
      "Iteration 51730 Training loss 0.004407776985317469 Validation loss 0.010642276145517826 Accuracy 0.8828125\n",
      "Iteration 51740 Training loss 0.0028105087112635374 Validation loss 0.010588088072836399 Accuracy 0.88330078125\n",
      "Iteration 51750 Training loss 0.003376677166670561 Validation loss 0.010605394840240479 Accuracy 0.8837890625\n",
      "Iteration 51760 Training loss 0.0032391827553510666 Validation loss 0.010624104179441929 Accuracy 0.8818359375\n",
      "Iteration 51770 Training loss 0.00419297069311142 Validation loss 0.010654170997440815 Accuracy 0.8828125\n",
      "Iteration 51780 Training loss 0.0038680771831423044 Validation loss 0.01059834472835064 Accuracy 0.8837890625\n",
      "Iteration 51790 Training loss 0.003981614485383034 Validation loss 0.01057500671595335 Accuracy 0.88330078125\n",
      "Iteration 51800 Training loss 0.004544252995401621 Validation loss 0.010541723109781742 Accuracy 0.8837890625\n",
      "Iteration 51810 Training loss 0.0026275075506418943 Validation loss 0.010636613704264164 Accuracy 0.8828125\n",
      "Iteration 51820 Training loss 0.002646729117259383 Validation loss 0.010534374974668026 Accuracy 0.8857421875\n",
      "Iteration 51830 Training loss 0.0033952395897358656 Validation loss 0.010528242215514183 Accuracy 0.8857421875\n",
      "Iteration 51840 Training loss 0.0035698823630809784 Validation loss 0.010563897900283337 Accuracy 0.88525390625\n",
      "Iteration 51850 Training loss 0.0035141506232321262 Validation loss 0.010613384656608105 Accuracy 0.88427734375\n",
      "Iteration 51860 Training loss 0.0034262514673173428 Validation loss 0.010611996985971928 Accuracy 0.88427734375\n",
      "Iteration 51870 Training loss 0.0037867287173867226 Validation loss 0.010683666914701462 Accuracy 0.88427734375\n",
      "Iteration 51880 Training loss 0.0032058327924460173 Validation loss 0.010583886876702309 Accuracy 0.8837890625\n",
      "Iteration 51890 Training loss 0.003710368648171425 Validation loss 0.010596883483231068 Accuracy 0.8837890625\n",
      "Iteration 51900 Training loss 0.0014526993036270142 Validation loss 0.01055498979985714 Accuracy 0.8857421875\n",
      "Iteration 51910 Training loss 0.0021797879599034786 Validation loss 0.010550475679337978 Accuracy 0.8837890625\n",
      "Iteration 51920 Training loss 0.003093574894592166 Validation loss 0.010509304702281952 Accuracy 0.8857421875\n",
      "Iteration 51930 Training loss 0.002217239001765847 Validation loss 0.010497912764549255 Accuracy 0.88525390625\n",
      "Iteration 51940 Training loss 0.0032088086009025574 Validation loss 0.010548782534897327 Accuracy 0.88525390625\n",
      "Iteration 51950 Training loss 0.0036954120732843876 Validation loss 0.010576903820037842 Accuracy 0.88427734375\n",
      "Iteration 51960 Training loss 0.003040350042283535 Validation loss 0.010572802275419235 Accuracy 0.88427734375\n",
      "Iteration 51970 Training loss 0.0023954783100634813 Validation loss 0.010593712329864502 Accuracy 0.88427734375\n",
      "Iteration 51980 Training loss 0.0026881000958383083 Validation loss 0.01060642022639513 Accuracy 0.88330078125\n",
      "Iteration 51990 Training loss 0.004037158098071814 Validation loss 0.010586701333522797 Accuracy 0.88427734375\n",
      "Iteration 52000 Training loss 0.003528136294335127 Validation loss 0.010687869973480701 Accuracy 0.88232421875\n",
      "Iteration 52010 Training loss 0.0036099243443459272 Validation loss 0.010577917098999023 Accuracy 0.88427734375\n",
      "Iteration 52020 Training loss 0.0029484962578862906 Validation loss 0.010626090690493584 Accuracy 0.88330078125\n",
      "Iteration 52030 Training loss 0.0026232933159917593 Validation loss 0.010590716265141964 Accuracy 0.8837890625\n",
      "Iteration 52040 Training loss 0.0021237798500806093 Validation loss 0.010616164654493332 Accuracy 0.8828125\n",
      "Iteration 52050 Training loss 0.002871578559279442 Validation loss 0.010551576502621174 Accuracy 0.884765625\n",
      "Iteration 52060 Training loss 0.0044784811325371265 Validation loss 0.0105216633528471 Accuracy 0.88427734375\n",
      "Iteration 52070 Training loss 0.0024880392011255026 Validation loss 0.010526244528591633 Accuracy 0.884765625\n",
      "Iteration 52080 Training loss 0.0038936410564929247 Validation loss 0.010555987246334553 Accuracy 0.8837890625\n",
      "Iteration 52090 Training loss 0.0030537776183336973 Validation loss 0.010517807677388191 Accuracy 0.88427734375\n",
      "Iteration 52100 Training loss 0.0034912859555333853 Validation loss 0.010492785833775997 Accuracy 0.88525390625\n",
      "Iteration 52110 Training loss 0.0028354041278362274 Validation loss 0.010441643185913563 Accuracy 0.88525390625\n",
      "Iteration 52120 Training loss 0.00395165802910924 Validation loss 0.010517188347876072 Accuracy 0.8837890625\n",
      "Iteration 52130 Training loss 0.003330994164571166 Validation loss 0.010527092032134533 Accuracy 0.88427734375\n",
      "Iteration 52140 Training loss 0.004006952978670597 Validation loss 0.010608743876218796 Accuracy 0.8837890625\n",
      "Iteration 52150 Training loss 0.003255579387769103 Validation loss 0.010595729574561119 Accuracy 0.8828125\n",
      "Iteration 52160 Training loss 0.004064133856445551 Validation loss 0.010537541471421719 Accuracy 0.8857421875\n",
      "Iteration 52170 Training loss 0.0035522873513400555 Validation loss 0.010549977421760559 Accuracy 0.8837890625\n",
      "Iteration 52180 Training loss 0.003157991450279951 Validation loss 0.01061263121664524 Accuracy 0.88330078125\n",
      "Iteration 52190 Training loss 0.0036796526983380318 Validation loss 0.010533337481319904 Accuracy 0.88427734375\n",
      "Iteration 52200 Training loss 0.0027092481032013893 Validation loss 0.010581279173493385 Accuracy 0.88427734375\n",
      "Iteration 52210 Training loss 0.0038020878564566374 Validation loss 0.010591686703264713 Accuracy 0.8837890625\n",
      "Iteration 52220 Training loss 0.0032717350404709578 Validation loss 0.010550415143370628 Accuracy 0.8837890625\n",
      "Iteration 52230 Training loss 0.003429120872169733 Validation loss 0.01055513508617878 Accuracy 0.8837890625\n",
      "Iteration 52240 Training loss 0.004188409075140953 Validation loss 0.010536587797105312 Accuracy 0.88427734375\n",
      "Iteration 52250 Training loss 0.0031895048450678587 Validation loss 0.010486110113561153 Accuracy 0.88525390625\n",
      "Iteration 52260 Training loss 0.0041908398270606995 Validation loss 0.010695700533688068 Accuracy 0.88134765625\n",
      "Iteration 52270 Training loss 0.002499387599527836 Validation loss 0.010786873288452625 Accuracy 0.88134765625\n",
      "Iteration 52280 Training loss 0.00400121184065938 Validation loss 0.010498644784092903 Accuracy 0.884765625\n",
      "Iteration 52290 Training loss 0.0026115563232451677 Validation loss 0.010570301674306393 Accuracy 0.884765625\n",
      "Iteration 52300 Training loss 0.003130232449620962 Validation loss 0.010503115132451057 Accuracy 0.88623046875\n",
      "Iteration 52310 Training loss 0.003815322183072567 Validation loss 0.010455754585564137 Accuracy 0.88623046875\n",
      "Iteration 52320 Training loss 0.003193400101736188 Validation loss 0.010583998635411263 Accuracy 0.8837890625\n",
      "Iteration 52330 Training loss 0.003227277658879757 Validation loss 0.01053821761161089 Accuracy 0.88525390625\n",
      "Iteration 52340 Training loss 0.003093476640060544 Validation loss 0.01053853239864111 Accuracy 0.88427734375\n",
      "Iteration 52350 Training loss 0.00231175241060555 Validation loss 0.010632036253809929 Accuracy 0.8828125\n",
      "Iteration 52360 Training loss 0.0034021567553281784 Validation loss 0.010691528208553791 Accuracy 0.8818359375\n",
      "Iteration 52370 Training loss 0.004897189326584339 Validation loss 0.010495776310563087 Accuracy 0.88427734375\n",
      "Iteration 52380 Training loss 0.0032632071524858475 Validation loss 0.010546071454882622 Accuracy 0.88427734375\n",
      "Iteration 52390 Training loss 0.0034935856238007545 Validation loss 0.010513735003769398 Accuracy 0.8857421875\n",
      "Iteration 52400 Training loss 0.0026426042895764112 Validation loss 0.010588740929961205 Accuracy 0.88427734375\n",
      "Iteration 52410 Training loss 0.0032809083350002766 Validation loss 0.010600248351693153 Accuracy 0.8837890625\n",
      "Iteration 52420 Training loss 0.0032908827997744083 Validation loss 0.010577589273452759 Accuracy 0.88427734375\n",
      "Iteration 52430 Training loss 0.0038951993919909 Validation loss 0.01058969460427761 Accuracy 0.88232421875\n",
      "Iteration 52440 Training loss 0.004053334705531597 Validation loss 0.010529601015150547 Accuracy 0.8837890625\n",
      "Iteration 52450 Training loss 0.0042488472536206245 Validation loss 0.010611619800329208 Accuracy 0.8828125\n",
      "Iteration 52460 Training loss 0.003999167121946812 Validation loss 0.010608295910060406 Accuracy 0.8828125\n",
      "Iteration 52470 Training loss 0.004289424978196621 Validation loss 0.010540901683270931 Accuracy 0.88427734375\n",
      "Iteration 52480 Training loss 0.0027462777215987444 Validation loss 0.010603937320411205 Accuracy 0.8828125\n",
      "Iteration 52490 Training loss 0.0032376288436353207 Validation loss 0.01051476038992405 Accuracy 0.88427734375\n",
      "Iteration 52500 Training loss 0.003433482488617301 Validation loss 0.010547936893999577 Accuracy 0.88427734375\n",
      "Iteration 52510 Training loss 0.0036128046922385693 Validation loss 0.010540849529206753 Accuracy 0.884765625\n",
      "Iteration 52520 Training loss 0.002069884678348899 Validation loss 0.010494186542928219 Accuracy 0.88427734375\n",
      "Iteration 52530 Training loss 0.0037348333280533552 Validation loss 0.010616029612720013 Accuracy 0.8837890625\n",
      "Iteration 52540 Training loss 0.0033509305212646723 Validation loss 0.01054492685943842 Accuracy 0.8837890625\n",
      "Iteration 52550 Training loss 0.0033598304726183414 Validation loss 0.010494111105799675 Accuracy 0.8857421875\n",
      "Iteration 52560 Training loss 0.0024963263422250748 Validation loss 0.010464109480381012 Accuracy 0.88427734375\n",
      "Iteration 52570 Training loss 0.002058342332020402 Validation loss 0.010466838255524635 Accuracy 0.88623046875\n",
      "Iteration 52580 Training loss 0.003154415637254715 Validation loss 0.010471232235431671 Accuracy 0.884765625\n",
      "Iteration 52590 Training loss 0.002680777106434107 Validation loss 0.010451093316078186 Accuracy 0.8857421875\n",
      "Iteration 52600 Training loss 0.0027907942421734333 Validation loss 0.010544335469603539 Accuracy 0.8837890625\n",
      "Iteration 52610 Training loss 0.002978712320327759 Validation loss 0.010549106635153294 Accuracy 0.884765625\n",
      "Iteration 52620 Training loss 0.0031622396782040596 Validation loss 0.010521230287849903 Accuracy 0.88525390625\n",
      "Iteration 52630 Training loss 0.0025500271003693342 Validation loss 0.010525654070079327 Accuracy 0.88525390625\n",
      "Iteration 52640 Training loss 0.004638408310711384 Validation loss 0.010486552491784096 Accuracy 0.88525390625\n",
      "Iteration 52650 Training loss 0.003213099204003811 Validation loss 0.010547514073550701 Accuracy 0.88427734375\n",
      "Iteration 52660 Training loss 0.0026002333033829927 Validation loss 0.010585554875433445 Accuracy 0.88330078125\n",
      "Iteration 52670 Training loss 0.004871133714914322 Validation loss 0.010581676848232746 Accuracy 0.88330078125\n",
      "Iteration 52680 Training loss 0.003932963125407696 Validation loss 0.010660802014172077 Accuracy 0.88134765625\n",
      "Iteration 52690 Training loss 0.003923632204532623 Validation loss 0.010477767325937748 Accuracy 0.8837890625\n",
      "Iteration 52700 Training loss 0.0040189665742218494 Validation loss 0.010610693134367466 Accuracy 0.88232421875\n",
      "Iteration 52710 Training loss 0.00378674129024148 Validation loss 0.010502704419195652 Accuracy 0.8837890625\n",
      "Iteration 52720 Training loss 0.002759984228760004 Validation loss 0.010524973273277283 Accuracy 0.88232421875\n",
      "Iteration 52730 Training loss 0.003343284362927079 Validation loss 0.010557056404650211 Accuracy 0.8828125\n",
      "Iteration 52740 Training loss 0.0027463308069854975 Validation loss 0.010477462783455849 Accuracy 0.884765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1076cb410>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52750 Training loss 0.0025894695427268744 Validation loss 0.010662850923836231 Accuracy 0.8818359375\n",
      "Iteration 52760 Training loss 0.0032494349870830774 Validation loss 0.010695199482142925 Accuracy 0.8818359375\n",
      "Iteration 52770 Training loss 0.0022013294510543346 Validation loss 0.01057723630219698 Accuracy 0.88232421875\n",
      "Iteration 52780 Training loss 0.002767325146123767 Validation loss 0.01067381352186203 Accuracy 0.8818359375\n",
      "Iteration 52790 Training loss 0.003119098022580147 Validation loss 0.010604985058307648 Accuracy 0.8837890625\n",
      "Iteration 52800 Training loss 0.003295122180134058 Validation loss 0.010505369864404202 Accuracy 0.884765625\n",
      "Iteration 52810 Training loss 0.0035483343526721 Validation loss 0.010594431310892105 Accuracy 0.88330078125\n",
      "Iteration 52820 Training loss 0.002788378158584237 Validation loss 0.010640461929142475 Accuracy 0.88330078125\n",
      "Iteration 52830 Training loss 0.0029381532222032547 Validation loss 0.010628159157931805 Accuracy 0.88330078125\n",
      "Iteration 52840 Training loss 0.002728285500779748 Validation loss 0.010466835461556911 Accuracy 0.884765625\n",
      "Iteration 52850 Training loss 0.0035651905927807093 Validation loss 0.010483947582542896 Accuracy 0.8857421875\n",
      "Iteration 52860 Training loss 0.0031982045620679855 Validation loss 0.010510996915400028 Accuracy 0.884765625\n",
      "Iteration 52870 Training loss 0.002768472069874406 Validation loss 0.010612602345645428 Accuracy 0.8828125\n",
      "Iteration 52880 Training loss 0.002927183173596859 Validation loss 0.01053676474839449 Accuracy 0.884765625\n",
      "Iteration 52890 Training loss 0.0022000668104737997 Validation loss 0.01054448913782835 Accuracy 0.8837890625\n",
      "Iteration 52900 Training loss 0.003383659990504384 Validation loss 0.010573131032288074 Accuracy 0.88427734375\n",
      "Iteration 52910 Training loss 0.0035791052505373955 Validation loss 0.010599150322377682 Accuracy 0.8837890625\n",
      "Iteration 52920 Training loss 0.002353723393753171 Validation loss 0.010557221248745918 Accuracy 0.884765625\n",
      "Iteration 52930 Training loss 0.0032759965397417545 Validation loss 0.010668081231415272 Accuracy 0.88232421875\n",
      "Iteration 52940 Training loss 0.0028566003311425447 Validation loss 0.01061930600553751 Accuracy 0.88330078125\n",
      "Iteration 52950 Training loss 0.0021711119916290045 Validation loss 0.010725024156272411 Accuracy 0.8818359375\n",
      "Iteration 52960 Training loss 0.003616190515458584 Validation loss 0.010614655911922455 Accuracy 0.88232421875\n",
      "Iteration 52970 Training loss 0.0029823596123605967 Validation loss 0.010762636549770832 Accuracy 0.8818359375\n",
      "Iteration 52980 Training loss 0.003909616265445948 Validation loss 0.01067126914858818 Accuracy 0.8828125\n",
      "Iteration 52990 Training loss 0.003630965482443571 Validation loss 0.010616812855005264 Accuracy 0.88427734375\n",
      "Iteration 53000 Training loss 0.0037749693728983402 Validation loss 0.010637314058840275 Accuracy 0.88427734375\n",
      "Iteration 53010 Training loss 0.00395519845187664 Validation loss 0.010591965168714523 Accuracy 0.8837890625\n",
      "Iteration 53020 Training loss 0.0036444361321628094 Validation loss 0.010602658614516258 Accuracy 0.8837890625\n",
      "Iteration 53030 Training loss 0.004052089527249336 Validation loss 0.010792321525514126 Accuracy 0.88134765625\n",
      "Iteration 53040 Training loss 0.0035331756807863712 Validation loss 0.01071024127304554 Accuracy 0.88232421875\n",
      "Iteration 53050 Training loss 0.003555749310180545 Validation loss 0.01063634268939495 Accuracy 0.8837890625\n",
      "Iteration 53060 Training loss 0.003416168736293912 Validation loss 0.010631762444972992 Accuracy 0.8828125\n",
      "Iteration 53070 Training loss 0.005585562437772751 Validation loss 0.010716000571846962 Accuracy 0.880859375\n",
      "Iteration 53080 Training loss 0.0029723288025707006 Validation loss 0.010607382282614708 Accuracy 0.88427734375\n",
      "Iteration 53090 Training loss 0.0034764199517667294 Validation loss 0.010657529346644878 Accuracy 0.8828125\n",
      "Iteration 53100 Training loss 0.0036247367970645428 Validation loss 0.010551226325333118 Accuracy 0.8837890625\n",
      "Iteration 53110 Training loss 0.0032343578059226274 Validation loss 0.010503952391445637 Accuracy 0.8857421875\n",
      "Iteration 53120 Training loss 0.0033916181419044733 Validation loss 0.010874579660594463 Accuracy 0.87939453125\n",
      "Iteration 53130 Training loss 0.0024271945003420115 Validation loss 0.010567053221166134 Accuracy 0.8837890625\n",
      "Iteration 53140 Training loss 0.0034827995114028454 Validation loss 0.010661548003554344 Accuracy 0.8828125\n",
      "Iteration 53150 Training loss 0.002752181375399232 Validation loss 0.0105159692466259 Accuracy 0.88427734375\n",
      "Iteration 53160 Training loss 0.003262430429458618 Validation loss 0.01066388376057148 Accuracy 0.88330078125\n",
      "Iteration 53170 Training loss 0.003320393618196249 Validation loss 0.010661513544619083 Accuracy 0.8828125\n",
      "Iteration 53180 Training loss 0.0027384264394640923 Validation loss 0.010655189864337444 Accuracy 0.8828125\n",
      "Iteration 53190 Training loss 0.0032832769211381674 Validation loss 0.010701386258006096 Accuracy 0.88232421875\n",
      "Iteration 53200 Training loss 0.003719841595739126 Validation loss 0.010634150356054306 Accuracy 0.88427734375\n",
      "Iteration 53210 Training loss 0.005397815722972155 Validation loss 0.010544772259891033 Accuracy 0.884765625\n",
      "Iteration 53220 Training loss 0.0035417010076344013 Validation loss 0.010577453300356865 Accuracy 0.884765625\n",
      "Iteration 53230 Training loss 0.00324276858009398 Validation loss 0.010643690824508667 Accuracy 0.8828125\n",
      "Iteration 53240 Training loss 0.003220548387616873 Validation loss 0.010578881949186325 Accuracy 0.88427734375\n",
      "Iteration 53250 Training loss 0.0024649060796946287 Validation loss 0.01058499701321125 Accuracy 0.8837890625\n",
      "Iteration 53260 Training loss 0.003551434725522995 Validation loss 0.01052625011652708 Accuracy 0.88427734375\n",
      "Iteration 53270 Training loss 0.002356769284233451 Validation loss 0.01059730350971222 Accuracy 0.8837890625\n",
      "Iteration 53280 Training loss 0.0027898966800421476 Validation loss 0.010636762715876102 Accuracy 0.8828125\n",
      "Iteration 53290 Training loss 0.0035967521835118532 Validation loss 0.010593632236123085 Accuracy 0.88330078125\n",
      "Iteration 53300 Training loss 0.0025530527345836163 Validation loss 0.010588944889605045 Accuracy 0.8837890625\n",
      "Iteration 53310 Training loss 0.003150569275021553 Validation loss 0.010574077256023884 Accuracy 0.8837890625\n",
      "Iteration 53320 Training loss 0.004766687750816345 Validation loss 0.010571214370429516 Accuracy 0.8828125\n",
      "Iteration 53330 Training loss 0.002242489717900753 Validation loss 0.010582690127193928 Accuracy 0.8837890625\n",
      "Iteration 53340 Training loss 0.004299399442970753 Validation loss 0.010561211034655571 Accuracy 0.8837890625\n",
      "Iteration 53350 Training loss 0.003918703179806471 Validation loss 0.010629001073539257 Accuracy 0.88330078125\n",
      "Iteration 53360 Training loss 0.004806065931916237 Validation loss 0.010570351034402847 Accuracy 0.88427734375\n",
      "Iteration 53370 Training loss 0.0041281599551439285 Validation loss 0.010572054423391819 Accuracy 0.8828125\n",
      "Iteration 53380 Training loss 0.004336182959377766 Validation loss 0.01060598623007536 Accuracy 0.88232421875\n",
      "Iteration 53390 Training loss 0.0027799848467111588 Validation loss 0.010617446154356003 Accuracy 0.88330078125\n",
      "Iteration 53400 Training loss 0.0035049065481871367 Validation loss 0.010602344758808613 Accuracy 0.88232421875\n",
      "Iteration 53410 Training loss 0.0043993182480335236 Validation loss 0.010703349485993385 Accuracy 0.88232421875\n",
      "Iteration 53420 Training loss 0.003843537298962474 Validation loss 0.010699260048568249 Accuracy 0.88232421875\n",
      "Iteration 53430 Training loss 0.003060475690290332 Validation loss 0.010659257881343365 Accuracy 0.88232421875\n",
      "Iteration 53440 Training loss 0.003778671147301793 Validation loss 0.010601924732327461 Accuracy 0.88330078125\n",
      "Iteration 53450 Training loss 0.003303154604509473 Validation loss 0.010678010061383247 Accuracy 0.88232421875\n",
      "Iteration 53460 Training loss 0.002119117183610797 Validation loss 0.010576671920716763 Accuracy 0.8837890625\n",
      "Iteration 53470 Training loss 0.003144994378089905 Validation loss 0.010553678497672081 Accuracy 0.8837890625\n",
      "Iteration 53480 Training loss 0.0027077726554125547 Validation loss 0.010725724510848522 Accuracy 0.8818359375\n",
      "Iteration 53490 Training loss 0.0027315139304846525 Validation loss 0.010726671665906906 Accuracy 0.88232421875\n",
      "Iteration 53500 Training loss 0.0038724711630493402 Validation loss 0.01059864740818739 Accuracy 0.8837890625\n",
      "Iteration 53510 Training loss 0.0029857170302420855 Validation loss 0.010606362484395504 Accuracy 0.8818359375\n",
      "Iteration 53520 Training loss 0.00275930087082088 Validation loss 0.01063038781285286 Accuracy 0.8828125\n",
      "Iteration 53530 Training loss 0.0031279639806598425 Validation loss 0.010665849782526493 Accuracy 0.8828125\n",
      "Iteration 53540 Training loss 0.0016977626364678144 Validation loss 0.010606956668198109 Accuracy 0.88330078125\n",
      "Iteration 53550 Training loss 0.0038358925376087427 Validation loss 0.010612829588353634 Accuracy 0.88330078125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_2_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mtwo_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, reg1, reg2, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\u001b[39;00m\n\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Calcul des gradients\u001b[39;00m\n\u001b[32m    122\u001b[39m grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m grad_z2 = torch.einsum(\u001b[33m'\u001b[39m\u001b[33mnoz, no->nz\u001b[39m\u001b[33m'\u001b[39m, \u001b[43msoftmax_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m, grad_output); grad_z2  = grad_z2.to(dtype) \u001b[38;5;66;03m# shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\u001b[39;00m\n\u001b[32m    124\u001b[39m grad_h1 = torch.mm(grad_z2, \u001b[38;5;28mself\u001b[39m.W2); grad_h1  = grad_h1.to(dtype)  \u001b[38;5;66;03m# shape (n_data, hidden_1_size)\u001b[39;00m\n\u001b[32m    125\u001b[39m grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) \u001b[38;5;66;03m# shape (n_data, hidden_1_size)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36msoftmax_derivative\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):  \u001b[38;5;66;03m# Pour chaque échantillon du batch, on calcule la jacobienne de softmax\u001b[39;00m\n\u001b[32m     24\u001b[39m     si = s[i].unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     jacobians[i] = torch.diagflat(si) - \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msi\u001b[49m\u001b[43m,\u001b[49m\u001b[43msi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jacobians\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c253240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 61465600.0 and the number of iterations is 102442.\n",
      "Iteration 0 Training loss 0.08748721331357956 Validation loss 0.08817725628614426 Accuracy 0.102294921875\n",
      "Iteration 10 Training loss 0.07844120264053345 Validation loss 0.07942960411310196 Accuracy 0.200927734375\n",
      "Iteration 20 Training loss 0.08213479816913605 Validation loss 0.08172647655010223 Accuracy 0.1795654296875\n",
      "Iteration 30 Training loss 0.07555239647626877 Validation loss 0.07497331500053406 Accuracy 0.2467041015625\n",
      "Iteration 40 Training loss 0.07391677051782608 Validation loss 0.07393834739923477 Accuracy 0.257080078125\n",
      "Iteration 50 Training loss 0.07477433979511261 Validation loss 0.07315880805253983 Accuracy 0.265869140625\n",
      "Iteration 60 Training loss 0.0741143673658371 Validation loss 0.0726979598402977 Accuracy 0.2705078125\n",
      "Iteration 70 Training loss 0.06965716183185577 Validation loss 0.07211826741695404 Accuracy 0.273681640625\n",
      "Iteration 80 Training loss 0.07196424156427383 Validation loss 0.0720154345035553 Accuracy 0.275634765625\n",
      "Iteration 90 Training loss 0.07721260190010071 Validation loss 0.07507993280887604 Accuracy 0.246826171875\n",
      "Iteration 100 Training loss 0.0759267807006836 Validation loss 0.07473825663328171 Accuracy 0.2491455078125\n",
      "Iteration 110 Training loss 0.0717630386352539 Validation loss 0.07257895171642303 Accuracy 0.272216796875\n",
      "Iteration 120 Training loss 0.07214990258216858 Validation loss 0.07182168960571289 Accuracy 0.279296875\n",
      "Iteration 130 Training loss 0.07019497454166412 Validation loss 0.07167469710111618 Accuracy 0.281494140625\n",
      "Iteration 140 Training loss 0.07342763245105743 Validation loss 0.07225783169269562 Accuracy 0.27490234375\n",
      "Iteration 150 Training loss 0.07062767446041107 Validation loss 0.07140801101922989 Accuracy 0.283935546875\n",
      "Iteration 160 Training loss 0.0714464783668518 Validation loss 0.07155710458755493 Accuracy 0.28271484375\n",
      "Iteration 170 Training loss 0.06770876795053482 Validation loss 0.07163205742835999 Accuracy 0.280517578125\n",
      "Iteration 180 Training loss 0.07008950412273407 Validation loss 0.07130321860313416 Accuracy 0.284912109375\n",
      "Iteration 190 Training loss 0.07315066456794739 Validation loss 0.07129567116498947 Accuracy 0.2841796875\n",
      "Iteration 200 Training loss 0.07338987290859222 Validation loss 0.07246636599302292 Accuracy 0.272216796875\n",
      "Iteration 210 Training loss 0.07204689085483551 Validation loss 0.07138799130916595 Accuracy 0.283935546875\n",
      "Iteration 220 Training loss 0.07235988229513168 Validation loss 0.0716899186372757 Accuracy 0.280517578125\n",
      "Iteration 230 Training loss 0.06942839920520782 Validation loss 0.07127534598112106 Accuracy 0.285400390625\n",
      "Iteration 240 Training loss 0.07183083146810532 Validation loss 0.07111244648694992 Accuracy 0.286865234375\n",
      "Iteration 250 Training loss 0.07132838666439056 Validation loss 0.07122338563203812 Accuracy 0.285888671875\n",
      "Iteration 260 Training loss 0.07041209191083908 Validation loss 0.07107365131378174 Accuracy 0.287353515625\n",
      "Iteration 270 Training loss 0.07304098457098007 Validation loss 0.07182393223047256 Accuracy 0.2802734375\n",
      "Iteration 280 Training loss 0.07284821569919586 Validation loss 0.07115738838911057 Accuracy 0.287109375\n",
      "Iteration 290 Training loss 0.06719397753477097 Validation loss 0.07122351229190826 Accuracy 0.2861328125\n",
      "Iteration 300 Training loss 0.06866003572940826 Validation loss 0.07116074860095978 Accuracy 0.28662109375\n",
      "Iteration 310 Training loss 0.07362893968820572 Validation loss 0.07191783934831619 Accuracy 0.278564453125\n",
      "Iteration 320 Training loss 0.07060062885284424 Validation loss 0.07154465466737747 Accuracy 0.28271484375\n",
      "Iteration 330 Training loss 0.07253018766641617 Validation loss 0.0712813064455986 Accuracy 0.285400390625\n",
      "Iteration 340 Training loss 0.0675342008471489 Validation loss 0.07087622582912445 Accuracy 0.2890625\n",
      "Iteration 350 Training loss 0.07122082263231277 Validation loss 0.07136025279760361 Accuracy 0.284423828125\n",
      "Iteration 360 Training loss 0.07279221713542938 Validation loss 0.07099083065986633 Accuracy 0.2880859375\n",
      "Iteration 370 Training loss 0.07295078784227371 Validation loss 0.07121231406927109 Accuracy 0.28662109375\n",
      "Iteration 380 Training loss 0.0708935558795929 Validation loss 0.07141188532114029 Accuracy 0.284423828125\n",
      "Iteration 390 Training loss 0.07062819600105286 Validation loss 0.07096616178750992 Accuracy 0.288330078125\n",
      "Iteration 400 Training loss 0.07079897075891495 Validation loss 0.07133126258850098 Accuracy 0.28515625\n",
      "Iteration 410 Training loss 0.0764952301979065 Validation loss 0.07111677527427673 Accuracy 0.286865234375\n",
      "Iteration 420 Training loss 0.07374773919582367 Validation loss 0.0709817185997963 Accuracy 0.288330078125\n",
      "Iteration 430 Training loss 0.069486603140831 Validation loss 0.07096990197896957 Accuracy 0.28857421875\n",
      "Iteration 440 Training loss 0.07080136984586716 Validation loss 0.07135660201311111 Accuracy 0.284912109375\n",
      "Iteration 450 Training loss 0.07041679322719574 Validation loss 0.07168243080377579 Accuracy 0.28173828125\n",
      "Iteration 460 Training loss 0.06918210536241531 Validation loss 0.07151027768850327 Accuracy 0.2841796875\n",
      "Iteration 470 Training loss 0.07236537337303162 Validation loss 0.07149641215801239 Accuracy 0.2841796875\n",
      "Iteration 480 Training loss 0.07356106489896774 Validation loss 0.07383649051189423 Accuracy 0.259033203125\n",
      "Iteration 490 Training loss 0.07197108864784241 Validation loss 0.07144664227962494 Accuracy 0.283935546875\n",
      "Iteration 500 Training loss 0.07025496661663055 Validation loss 0.07100322097539902 Accuracy 0.28857421875\n",
      "Iteration 510 Training loss 0.06805302947759628 Validation loss 0.07116653025150299 Accuracy 0.286865234375\n",
      "Iteration 520 Training loss 0.07254565507173538 Validation loss 0.07102245837450027 Accuracy 0.288818359375\n",
      "Iteration 530 Training loss 0.0699809119105339 Validation loss 0.0709109976887703 Accuracy 0.28955078125\n",
      "Iteration 540 Training loss 0.07113824039697647 Validation loss 0.07132300734519958 Accuracy 0.285400390625\n",
      "Iteration 550 Training loss 0.0713396817445755 Validation loss 0.07096821069717407 Accuracy 0.288818359375\n",
      "Iteration 560 Training loss 0.06839148700237274 Validation loss 0.07093196362257004 Accuracy 0.289306640625\n",
      "Iteration 570 Training loss 0.07086930423974991 Validation loss 0.07089254260063171 Accuracy 0.289794921875\n",
      "Iteration 580 Training loss 0.07126834243535995 Validation loss 0.07098757475614548 Accuracy 0.288330078125\n",
      "Iteration 590 Training loss 0.06976868957281113 Validation loss 0.07094131410121918 Accuracy 0.288818359375\n",
      "Iteration 600 Training loss 0.06865645945072174 Validation loss 0.0709078311920166 Accuracy 0.2890625\n",
      "Iteration 610 Training loss 0.069943867623806 Validation loss 0.07087381929159164 Accuracy 0.2900390625\n",
      "Iteration 620 Training loss 0.07164662331342697 Validation loss 0.07086215913295746 Accuracy 0.289794921875\n",
      "Iteration 630 Training loss 0.07246143370866776 Validation loss 0.07170584052801132 Accuracy 0.281005859375\n",
      "Iteration 640 Training loss 0.07280461490154266 Validation loss 0.07087419927120209 Accuracy 0.289794921875\n",
      "Iteration 650 Training loss 0.07073314487934113 Validation loss 0.0709058940410614 Accuracy 0.28955078125\n",
      "Iteration 660 Training loss 0.0718618780374527 Validation loss 0.07140098512172699 Accuracy 0.284423828125\n",
      "Iteration 670 Training loss 0.06944730132818222 Validation loss 0.07085523754358292 Accuracy 0.290283203125\n",
      "Iteration 680 Training loss 0.06835298240184784 Validation loss 0.070886991918087 Accuracy 0.2900390625\n",
      "Iteration 690 Training loss 0.06835570186376572 Validation loss 0.0709516704082489 Accuracy 0.288818359375\n",
      "Iteration 700 Training loss 0.07111096382141113 Validation loss 0.0710340365767479 Accuracy 0.287841796875\n",
      "Iteration 710 Training loss 0.0693669468164444 Validation loss 0.07090948522090912 Accuracy 0.289306640625\n",
      "Iteration 720 Training loss 0.07367297261953354 Validation loss 0.0709872841835022 Accuracy 0.288818359375\n",
      "Iteration 730 Training loss 0.07023213803768158 Validation loss 0.07189958542585373 Accuracy 0.279541015625\n",
      "Iteration 740 Training loss 0.0705447718501091 Validation loss 0.07095008343458176 Accuracy 0.289306640625\n",
      "Iteration 750 Training loss 0.07101698964834213 Validation loss 0.07118270546197891 Accuracy 0.28662109375\n",
      "Iteration 760 Training loss 0.07190178334712982 Validation loss 0.07092393189668655 Accuracy 0.288818359375\n",
      "Iteration 770 Training loss 0.07203580439090729 Validation loss 0.07095949351787567 Accuracy 0.2890625\n",
      "Iteration 780 Training loss 0.07244843989610672 Validation loss 0.07166728377342224 Accuracy 0.281494140625\n",
      "Iteration 790 Training loss 0.06828044354915619 Validation loss 0.07156912237405777 Accuracy 0.282958984375\n",
      "Iteration 800 Training loss 0.06984912604093552 Validation loss 0.0709584653377533 Accuracy 0.289306640625\n",
      "Iteration 810 Training loss 0.0709235817193985 Validation loss 0.07130976021289825 Accuracy 0.28564453125\n",
      "Iteration 820 Training loss 0.06978222727775574 Validation loss 0.07090403884649277 Accuracy 0.2900390625\n",
      "Iteration 830 Training loss 0.0666196420788765 Validation loss 0.07110550254583359 Accuracy 0.287109375\n",
      "Iteration 840 Training loss 0.07090701162815094 Validation loss 0.07107207924127579 Accuracy 0.28759765625\n",
      "Iteration 850 Training loss 0.0703013613820076 Validation loss 0.07100392878055573 Accuracy 0.287841796875\n",
      "Iteration 860 Training loss 0.06607306748628616 Validation loss 0.07084790617227554 Accuracy 0.290283203125\n",
      "Iteration 870 Training loss 0.06924082338809967 Validation loss 0.070888452231884 Accuracy 0.28955078125\n",
      "Iteration 880 Training loss 0.07218343764543533 Validation loss 0.07095890492200851 Accuracy 0.2890625\n",
      "Iteration 890 Training loss 0.07241138815879822 Validation loss 0.07146570086479187 Accuracy 0.283935546875\n",
      "Iteration 900 Training loss 0.07275329530239105 Validation loss 0.07120218127965927 Accuracy 0.28662109375\n",
      "Iteration 910 Training loss 0.06899648904800415 Validation loss 0.07096637785434723 Accuracy 0.289306640625\n",
      "Iteration 920 Training loss 0.07266399264335632 Validation loss 0.07096680998802185 Accuracy 0.288818359375\n",
      "Iteration 930 Training loss 0.07198655605316162 Validation loss 0.07093052566051483 Accuracy 0.28955078125\n",
      "Iteration 940 Training loss 0.07313224673271179 Validation loss 0.07088159769773483 Accuracy 0.289794921875\n",
      "Iteration 950 Training loss 0.07198300957679749 Validation loss 0.07088141143321991 Accuracy 0.289794921875\n",
      "Iteration 960 Training loss 0.07069901376962662 Validation loss 0.07090603560209274 Accuracy 0.289306640625\n",
      "Iteration 970 Training loss 0.068463996052742 Validation loss 0.07124479115009308 Accuracy 0.28564453125\n",
      "Iteration 980 Training loss 0.07002169638872147 Validation loss 0.07087346166372299 Accuracy 0.2900390625\n",
      "Iteration 990 Training loss 0.07251188158988953 Validation loss 0.07091904431581497 Accuracy 0.288818359375\n",
      "Iteration 1000 Training loss 0.07140665501356125 Validation loss 0.07091684639453888 Accuracy 0.289794921875\n",
      "Iteration 1010 Training loss 0.07076683640480042 Validation loss 0.07089675217866898 Accuracy 0.2900390625\n",
      "Iteration 1020 Training loss 0.07277538627386093 Validation loss 0.0711304098367691 Accuracy 0.28759765625\n",
      "Iteration 1030 Training loss 0.06998053193092346 Validation loss 0.07096902281045914 Accuracy 0.2890625\n",
      "Iteration 1040 Training loss 0.07045979052782059 Validation loss 0.07197359204292297 Accuracy 0.279052734375\n",
      "Iteration 1050 Training loss 0.0690397322177887 Validation loss 0.0712752640247345 Accuracy 0.286376953125\n",
      "Iteration 1060 Training loss 0.06954051554203033 Validation loss 0.0708736777305603 Accuracy 0.2900390625\n",
      "Iteration 1070 Training loss 0.07047631591558456 Validation loss 0.07087226957082748 Accuracy 0.2900390625\n",
      "Iteration 1080 Training loss 0.07157787680625916 Validation loss 0.07090029865503311 Accuracy 0.2900390625\n",
      "Iteration 1090 Training loss 0.07485942542552948 Validation loss 0.07091999053955078 Accuracy 0.2900390625\n",
      "Iteration 1100 Training loss 0.06777343153953552 Validation loss 0.07096996158361435 Accuracy 0.289306640625\n",
      "Iteration 1110 Training loss 0.07133302092552185 Validation loss 0.07100218534469604 Accuracy 0.28857421875\n",
      "Iteration 1120 Training loss 0.07427632808685303 Validation loss 0.07083120197057724 Accuracy 0.290771484375\n",
      "Iteration 1130 Training loss 0.07330437004566193 Validation loss 0.07113571465015411 Accuracy 0.28759765625\n",
      "Iteration 1140 Training loss 0.07542432844638824 Validation loss 0.07089205086231232 Accuracy 0.289794921875\n",
      "Iteration 1150 Training loss 0.07413903623819351 Validation loss 0.07091964036226273 Accuracy 0.2900390625\n",
      "Iteration 1160 Training loss 0.0711735188961029 Validation loss 0.07084916532039642 Accuracy 0.2900390625\n",
      "Iteration 1170 Training loss 0.0750732347369194 Validation loss 0.07110694795846939 Accuracy 0.287841796875\n",
      "Iteration 1180 Training loss 0.0701955258846283 Validation loss 0.07074986398220062 Accuracy 0.291259765625\n",
      "Iteration 1190 Training loss 0.06984959542751312 Validation loss 0.07078535854816437 Accuracy 0.290771484375\n",
      "Iteration 1200 Training loss 0.06958358734846115 Validation loss 0.07079155743122101 Accuracy 0.290771484375\n",
      "Iteration 1210 Training loss 0.06942287087440491 Validation loss 0.07082700729370117 Accuracy 0.290283203125\n",
      "Iteration 1220 Training loss 0.0716485008597374 Validation loss 0.07092670351266861 Accuracy 0.289794921875\n",
      "Iteration 1230 Training loss 0.07133103907108307 Validation loss 0.07110080868005753 Accuracy 0.2880859375\n",
      "Iteration 1240 Training loss 0.07077404856681824 Validation loss 0.07095834612846375 Accuracy 0.28955078125\n",
      "Iteration 1250 Training loss 0.07206308096647263 Validation loss 0.07091616094112396 Accuracy 0.289306640625\n",
      "Iteration 1260 Training loss 0.07053090631961823 Validation loss 0.07075399905443192 Accuracy 0.291015625\n",
      "Iteration 1270 Training loss 0.07068249583244324 Validation loss 0.07090498507022858 Accuracy 0.28857421875\n",
      "Iteration 1280 Training loss 0.0731329470872879 Validation loss 0.07084915786981583 Accuracy 0.2900390625\n",
      "Iteration 1290 Training loss 0.06880169361829758 Validation loss 0.07100631296634674 Accuracy 0.28759765625\n",
      "Iteration 1300 Training loss 0.0708022266626358 Validation loss 0.07068739086389542 Accuracy 0.290771484375\n",
      "Iteration 1310 Training loss 0.07250919193029404 Validation loss 0.07072808593511581 Accuracy 0.2900390625\n",
      "Iteration 1320 Training loss 0.06867685914039612 Validation loss 0.07213373482227325 Accuracy 0.276123046875\n",
      "Iteration 1330 Training loss 0.07291387021541595 Validation loss 0.07077855616807938 Accuracy 0.2900390625\n",
      "Iteration 1340 Training loss 0.073598712682724 Validation loss 0.07077180594205856 Accuracy 0.2900390625\n",
      "Iteration 1350 Training loss 0.07080293446779251 Validation loss 0.07067251950502396 Accuracy 0.290771484375\n",
      "Iteration 1360 Training loss 0.07300635427236557 Validation loss 0.07115969806909561 Accuracy 0.287353515625\n",
      "Iteration 1370 Training loss 0.06955400854349136 Validation loss 0.07073092460632324 Accuracy 0.291259765625\n",
      "Iteration 1380 Training loss 0.07030471414327621 Validation loss 0.07099571079015732 Accuracy 0.288330078125\n",
      "Iteration 1390 Training loss 0.07154569774866104 Validation loss 0.0710015669465065 Accuracy 0.288330078125\n",
      "Iteration 1400 Training loss 0.07042553275823593 Validation loss 0.07072373479604721 Accuracy 0.291748046875\n",
      "Iteration 1410 Training loss 0.07128766179084778 Validation loss 0.07076279819011688 Accuracy 0.290771484375\n",
      "Iteration 1420 Training loss 0.0720587745308876 Validation loss 0.07083477079868317 Accuracy 0.28857421875\n",
      "Iteration 1430 Training loss 0.07040541619062424 Validation loss 0.07071482390165329 Accuracy 0.29052734375\n",
      "Iteration 1440 Training loss 0.07119150459766388 Validation loss 0.07074024528265 Accuracy 0.289794921875\n",
      "Iteration 1450 Training loss 0.06773757189512253 Validation loss 0.07065847516059875 Accuracy 0.2919921875\n",
      "Iteration 1460 Training loss 0.07127724587917328 Validation loss 0.07126982510089874 Accuracy 0.284912109375\n",
      "Iteration 1470 Training loss 0.06687890738248825 Validation loss 0.07065518200397491 Accuracy 0.290771484375\n",
      "Iteration 1480 Training loss 0.07309740781784058 Validation loss 0.07085776329040527 Accuracy 0.2900390625\n",
      "Iteration 1490 Training loss 0.07057816535234451 Validation loss 0.07064719498157501 Accuracy 0.291259765625\n",
      "Iteration 1500 Training loss 0.07122515887022018 Validation loss 0.07073195278644562 Accuracy 0.290283203125\n",
      "Iteration 1510 Training loss 0.07100459188222885 Validation loss 0.07064064592123032 Accuracy 0.2919921875\n",
      "Iteration 1520 Training loss 0.0718037411570549 Validation loss 0.07067393511533737 Accuracy 0.291748046875\n",
      "Iteration 1530 Training loss 0.06797703355550766 Validation loss 0.07073825597763062 Accuracy 0.291015625\n",
      "Iteration 1540 Training loss 0.07115545868873596 Validation loss 0.07071293145418167 Accuracy 0.291015625\n",
      "Iteration 1550 Training loss 0.06913604587316513 Validation loss 0.07074557989835739 Accuracy 0.291259765625\n",
      "Iteration 1560 Training loss 0.07267960906028748 Validation loss 0.07196936011314392 Accuracy 0.278564453125\n",
      "Iteration 1570 Training loss 0.07157102227210999 Validation loss 0.07077410817146301 Accuracy 0.291015625\n",
      "Iteration 1580 Training loss 0.06872744113206863 Validation loss 0.07076103985309601 Accuracy 0.29150390625\n",
      "Iteration 1590 Training loss 0.07011581212282181 Validation loss 0.07077451795339584 Accuracy 0.29150390625\n",
      "Iteration 1600 Training loss 0.07277750223875046 Validation loss 0.07133348286151886 Accuracy 0.285888671875\n",
      "Iteration 1610 Training loss 0.07265085726976395 Validation loss 0.07070106267929077 Accuracy 0.2919921875\n",
      "Iteration 1620 Training loss 0.06966511905193329 Validation loss 0.07074091583490372 Accuracy 0.29150390625\n",
      "Iteration 1630 Training loss 0.06842321902513504 Validation loss 0.07075219601392746 Accuracy 0.29150390625\n",
      "Iteration 1640 Training loss 0.07073908299207687 Validation loss 0.07097158581018448 Accuracy 0.2890625\n",
      "Iteration 1650 Training loss 0.07242466509342194 Validation loss 0.07067615538835526 Accuracy 0.292236328125\n",
      "Iteration 1660 Training loss 0.06931308656930923 Validation loss 0.07064443081617355 Accuracy 0.29150390625\n",
      "Iteration 1670 Training loss 0.07115614414215088 Validation loss 0.07066036760807037 Accuracy 0.291259765625\n",
      "Iteration 1680 Training loss 0.06997215747833252 Validation loss 0.07067224383354187 Accuracy 0.290771484375\n",
      "Iteration 1690 Training loss 0.0707758367061615 Validation loss 0.07077130675315857 Accuracy 0.29150390625\n",
      "Iteration 1700 Training loss 0.06885051727294922 Validation loss 0.07067126035690308 Accuracy 0.291259765625\n",
      "Iteration 1710 Training loss 0.06856928765773773 Validation loss 0.07082043588161469 Accuracy 0.289306640625\n",
      "Iteration 1720 Training loss 0.07103782892227173 Validation loss 0.07075110077857971 Accuracy 0.29150390625\n",
      "Iteration 1730 Training loss 0.06994456797838211 Validation loss 0.07081338763237 Accuracy 0.291015625\n",
      "Iteration 1740 Training loss 0.07197965681552887 Validation loss 0.07089214026927948 Accuracy 0.2900390625\n",
      "Iteration 1750 Training loss 0.07030081003904343 Validation loss 0.07095678895711899 Accuracy 0.289794921875\n",
      "Iteration 1760 Training loss 0.06868743151426315 Validation loss 0.07078579068183899 Accuracy 0.29052734375\n",
      "Iteration 1770 Training loss 0.06928679347038269 Validation loss 0.07105005532503128 Accuracy 0.28857421875\n",
      "Iteration 1780 Training loss 0.06943286955356598 Validation loss 0.07084399461746216 Accuracy 0.29052734375\n",
      "Iteration 1790 Training loss 0.06840281933546066 Validation loss 0.07082416862249374 Accuracy 0.291015625\n",
      "Iteration 1800 Training loss 0.06697490066289902 Validation loss 0.07107087969779968 Accuracy 0.287841796875\n",
      "Iteration 1810 Training loss 0.07013625651597977 Validation loss 0.07086192816495895 Accuracy 0.288818359375\n",
      "Iteration 1820 Training loss 0.07050022482872009 Validation loss 0.0708223432302475 Accuracy 0.289794921875\n",
      "Iteration 1830 Training loss 0.07275490462779999 Validation loss 0.07095272094011307 Accuracy 0.287353515625\n",
      "Iteration 1840 Training loss 0.06845035403966904 Validation loss 0.0706239566206932 Accuracy 0.29150390625\n",
      "Iteration 1850 Training loss 0.06880553066730499 Validation loss 0.07112187892198563 Accuracy 0.28515625\n",
      "Iteration 1860 Training loss 0.06951829791069031 Validation loss 0.07079421728849411 Accuracy 0.2890625\n",
      "Iteration 1870 Training loss 0.0690208151936531 Validation loss 0.07135221362113953 Accuracy 0.28271484375\n",
      "Iteration 1880 Training loss 0.07079115509986877 Validation loss 0.07131745666265488 Accuracy 0.283447265625\n",
      "Iteration 1890 Training loss 0.07010744512081146 Validation loss 0.07114146649837494 Accuracy 0.28564453125\n",
      "Iteration 1900 Training loss 0.0745733305811882 Validation loss 0.07071832567453384 Accuracy 0.28955078125\n",
      "Iteration 1910 Training loss 0.07295206934213638 Validation loss 0.07064103335142136 Accuracy 0.289794921875\n",
      "Iteration 1920 Training loss 0.07357163727283478 Validation loss 0.07076605409383774 Accuracy 0.289306640625\n",
      "Iteration 1930 Training loss 0.07053698599338531 Validation loss 0.07105719298124313 Accuracy 0.28564453125\n",
      "Iteration 1940 Training loss 0.071494922041893 Validation loss 0.07091862708330154 Accuracy 0.2880859375\n",
      "Iteration 1950 Training loss 0.07048382610082626 Validation loss 0.0708765983581543 Accuracy 0.287841796875\n",
      "Iteration 1960 Training loss 0.07481282204389572 Validation loss 0.07083059102296829 Accuracy 0.28857421875\n",
      "Iteration 1970 Training loss 0.07017706334590912 Validation loss 0.07056481391191483 Accuracy 0.29248046875\n",
      "Iteration 1980 Training loss 0.07019739598035812 Validation loss 0.07067269086837769 Accuracy 0.290771484375\n",
      "Iteration 1990 Training loss 0.06763412058353424 Validation loss 0.07069166004657745 Accuracy 0.291015625\n",
      "Iteration 2000 Training loss 0.0715768039226532 Validation loss 0.07068493217229843 Accuracy 0.29150390625\n",
      "Iteration 2010 Training loss 0.0712374597787857 Validation loss 0.07066895067691803 Accuracy 0.29248046875\n",
      "Iteration 2020 Training loss 0.0695357546210289 Validation loss 0.07060419768095016 Accuracy 0.29248046875\n",
      "Iteration 2030 Training loss 0.07241848856210709 Validation loss 0.07063541561365128 Accuracy 0.29248046875\n",
      "Iteration 2040 Training loss 0.07348804920911789 Validation loss 0.07060158252716064 Accuracy 0.29248046875\n",
      "Iteration 2050 Training loss 0.0712938904762268 Validation loss 0.07121356576681137 Accuracy 0.2861328125\n",
      "Iteration 2060 Training loss 0.07210507988929749 Validation loss 0.07067672908306122 Accuracy 0.29150390625\n",
      "Iteration 2070 Training loss 0.06989893317222595 Validation loss 0.07065939903259277 Accuracy 0.29248046875\n",
      "Iteration 2080 Training loss 0.07113947719335556 Validation loss 0.07099207490682602 Accuracy 0.288818359375\n",
      "Iteration 2090 Training loss 0.0723721832036972 Validation loss 0.0707317441701889 Accuracy 0.29150390625\n",
      "Iteration 2100 Training loss 0.06872650980949402 Validation loss 0.0706641748547554 Accuracy 0.29248046875\n",
      "Iteration 2110 Training loss 0.06982405483722687 Validation loss 0.07068432122468948 Accuracy 0.2919921875\n",
      "Iteration 2120 Training loss 0.07035789638757706 Validation loss 0.0706649050116539 Accuracy 0.290771484375\n",
      "Iteration 2130 Training loss 0.07042589038610458 Validation loss 0.07068021595478058 Accuracy 0.291259765625\n",
      "Iteration 2140 Training loss 0.0722566619515419 Validation loss 0.0706423744559288 Accuracy 0.29052734375\n",
      "Iteration 2150 Training loss 0.07101913541555405 Validation loss 0.07063884288072586 Accuracy 0.2919921875\n",
      "Iteration 2160 Training loss 0.06970888376235962 Validation loss 0.07066420465707779 Accuracy 0.2919921875\n",
      "Iteration 2170 Training loss 0.0715966522693634 Validation loss 0.07069651037454605 Accuracy 0.291748046875\n",
      "Iteration 2180 Training loss 0.06930733472108841 Validation loss 0.0706821084022522 Accuracy 0.2919921875\n",
      "Iteration 2190 Training loss 0.0718834325671196 Validation loss 0.07062046229839325 Accuracy 0.292236328125\n",
      "Iteration 2200 Training loss 0.06940855830907822 Validation loss 0.07065702229738235 Accuracy 0.29150390625\n",
      "Iteration 2210 Training loss 0.06970285624265671 Validation loss 0.07123170793056488 Accuracy 0.284423828125\n",
      "Iteration 2220 Training loss 0.06686464697122574 Validation loss 0.07082045823335648 Accuracy 0.289306640625\n",
      "Iteration 2230 Training loss 0.06926864385604858 Validation loss 0.0708613395690918 Accuracy 0.288818359375\n",
      "Iteration 2240 Training loss 0.06798002868890762 Validation loss 0.07064370810985565 Accuracy 0.290771484375\n",
      "Iteration 2250 Training loss 0.0706840232014656 Validation loss 0.07077696174383163 Accuracy 0.2900390625\n",
      "Iteration 2260 Training loss 0.07095770537853241 Validation loss 0.07077760249376297 Accuracy 0.290283203125\n",
      "Iteration 2270 Training loss 0.07245387881994247 Validation loss 0.07065604627132416 Accuracy 0.29150390625\n",
      "Iteration 2280 Training loss 0.07010240852832794 Validation loss 0.07079929113388062 Accuracy 0.2900390625\n",
      "Iteration 2290 Training loss 0.07222370058298111 Validation loss 0.07076942175626755 Accuracy 0.290283203125\n",
      "Iteration 2300 Training loss 0.07244838029146194 Validation loss 0.0712137371301651 Accuracy 0.28564453125\n",
      "Iteration 2310 Training loss 0.06916630268096924 Validation loss 0.07069013267755508 Accuracy 0.291015625\n",
      "Iteration 2320 Training loss 0.07256174832582474 Validation loss 0.07079630345106125 Accuracy 0.2900390625\n",
      "Iteration 2330 Training loss 0.06882895529270172 Validation loss 0.07068304717540741 Accuracy 0.2919921875\n",
      "Iteration 2340 Training loss 0.06989791989326477 Validation loss 0.07068132609128952 Accuracy 0.291015625\n",
      "Iteration 2350 Training loss 0.07419371604919434 Validation loss 0.07086748629808426 Accuracy 0.2900390625\n",
      "Iteration 2360 Training loss 0.07168794423341751 Validation loss 0.07079755514860153 Accuracy 0.290771484375\n",
      "Iteration 2370 Training loss 0.07276266068220139 Validation loss 0.07078531384468079 Accuracy 0.291015625\n",
      "Iteration 2380 Training loss 0.06886181235313416 Validation loss 0.07063354551792145 Accuracy 0.2919921875\n",
      "Iteration 2390 Training loss 0.06901255995035172 Validation loss 0.07060328125953674 Accuracy 0.29150390625\n",
      "Iteration 2400 Training loss 0.07289545238018036 Validation loss 0.07106277346611023 Accuracy 0.287109375\n",
      "Iteration 2410 Training loss 0.0697309672832489 Validation loss 0.07063058018684387 Accuracy 0.2919921875\n",
      "Iteration 2420 Training loss 0.06965501606464386 Validation loss 0.0710323303937912 Accuracy 0.288818359375\n",
      "Iteration 2430 Training loss 0.07050737738609314 Validation loss 0.07064785808324814 Accuracy 0.292236328125\n",
      "Iteration 2440 Training loss 0.06561043113470078 Validation loss 0.07064936310052872 Accuracy 0.29248046875\n",
      "Iteration 2450 Training loss 0.07214679569005966 Validation loss 0.07063011825084686 Accuracy 0.291015625\n",
      "Iteration 2460 Training loss 0.07411666959524155 Validation loss 0.07073312997817993 Accuracy 0.2890625\n",
      "Iteration 2470 Training loss 0.0713767558336258 Validation loss 0.07230707257986069 Accuracy 0.27490234375\n",
      "Iteration 2480 Training loss 0.07014410197734833 Validation loss 0.07052874565124512 Accuracy 0.29150390625\n",
      "Iteration 2490 Training loss 0.07055525481700897 Validation loss 0.07196377962827682 Accuracy 0.278076171875\n",
      "Iteration 2500 Training loss 0.07023564726114273 Validation loss 0.07086565345525742 Accuracy 0.2880859375\n",
      "Iteration 2510 Training loss 0.07208947837352753 Validation loss 0.0707261860370636 Accuracy 0.289306640625\n",
      "Iteration 2520 Training loss 0.07183244824409485 Validation loss 0.06986680626869202 Accuracy 0.298095703125\n",
      "Iteration 2530 Training loss 0.06352957338094711 Validation loss 0.06436977535486221 Accuracy 0.352783203125\n",
      "Iteration 2540 Training loss 0.06372959166765213 Validation loss 0.06253422051668167 Accuracy 0.368896484375\n",
      "Iteration 2550 Training loss 0.06333152204751968 Validation loss 0.06245812028646469 Accuracy 0.37060546875\n",
      "Iteration 2560 Training loss 0.058828845620155334 Validation loss 0.06276752054691315 Accuracy 0.369384765625\n",
      "Iteration 2570 Training loss 0.061252329498529434 Validation loss 0.06163516268134117 Accuracy 0.37744140625\n",
      "Iteration 2580 Training loss 0.059775106608867645 Validation loss 0.06322302669286728 Accuracy 0.36328125\n",
      "Iteration 2590 Training loss 0.06182854250073433 Validation loss 0.06204444169998169 Accuracy 0.377197265625\n",
      "Iteration 2600 Training loss 0.06367304176092148 Validation loss 0.06188160181045532 Accuracy 0.377197265625\n",
      "Iteration 2610 Training loss 0.05975241959095001 Validation loss 0.061960820108652115 Accuracy 0.376220703125\n",
      "Iteration 2620 Training loss 0.0621224045753479 Validation loss 0.06307647377252579 Accuracy 0.3671875\n",
      "Iteration 2630 Training loss 0.06122095510363579 Validation loss 0.061656318604946136 Accuracy 0.381103515625\n",
      "Iteration 2640 Training loss 0.05953725799918175 Validation loss 0.06144162267446518 Accuracy 0.382080078125\n",
      "Iteration 2650 Training loss 0.06492391228675842 Validation loss 0.06219768524169922 Accuracy 0.374267578125\n",
      "Iteration 2660 Training loss 0.06497340649366379 Validation loss 0.061347831040620804 Accuracy 0.3828125\n",
      "Iteration 2670 Training loss 0.06079782545566559 Validation loss 0.062002670019865036 Accuracy 0.375732421875\n",
      "Iteration 2680 Training loss 0.06257936358451843 Validation loss 0.06122655048966408 Accuracy 0.3828125\n",
      "Iteration 2690 Training loss 0.058907561004161835 Validation loss 0.06129096448421478 Accuracy 0.382568359375\n",
      "Iteration 2700 Training loss 0.06277383118867874 Validation loss 0.0619521290063858 Accuracy 0.37548828125\n",
      "Iteration 2710 Training loss 0.06104732304811478 Validation loss 0.06258106976747513 Accuracy 0.370361328125\n",
      "Iteration 2720 Training loss 0.05796518921852112 Validation loss 0.06137348338961601 Accuracy 0.382568359375\n",
      "Iteration 2730 Training loss 0.059293508529663086 Validation loss 0.061555009335279465 Accuracy 0.38037109375\n",
      "Iteration 2740 Training loss 0.061566539108753204 Validation loss 0.061494167894124985 Accuracy 0.382568359375\n",
      "Iteration 2750 Training loss 0.05821249261498451 Validation loss 0.0615902915596962 Accuracy 0.38134765625\n",
      "Iteration 2760 Training loss 0.0614095963537693 Validation loss 0.062975212931633 Accuracy 0.36767578125\n",
      "Iteration 2770 Training loss 0.06117112189531326 Validation loss 0.06176271662116051 Accuracy 0.378662109375\n",
      "Iteration 2780 Training loss 0.05855633318424225 Validation loss 0.0615900419652462 Accuracy 0.38037109375\n",
      "Iteration 2790 Training loss 0.0639413371682167 Validation loss 0.06248094514012337 Accuracy 0.373291015625\n",
      "Iteration 2800 Training loss 0.06260446459054947 Validation loss 0.06253065913915634 Accuracy 0.371337890625\n",
      "Iteration 2810 Training loss 0.06112275272607803 Validation loss 0.06161097064614296 Accuracy 0.3818359375\n",
      "Iteration 2820 Training loss 0.06475833058357239 Validation loss 0.061128851026296616 Accuracy 0.385009765625\n",
      "Iteration 2830 Training loss 0.060731496661901474 Validation loss 0.06122457981109619 Accuracy 0.3857421875\n",
      "Iteration 2840 Training loss 0.06510581076145172 Validation loss 0.06162354350090027 Accuracy 0.38232421875\n",
      "Iteration 2850 Training loss 0.05939977988600731 Validation loss 0.06167266517877579 Accuracy 0.3818359375\n",
      "Iteration 2860 Training loss 0.06048210710287094 Validation loss 0.06147334724664688 Accuracy 0.3837890625\n",
      "Iteration 2870 Training loss 0.05802134796977043 Validation loss 0.06121779978275299 Accuracy 0.3857421875\n",
      "Iteration 2880 Training loss 0.062283292412757874 Validation loss 0.0611722394824028 Accuracy 0.385498046875\n",
      "Iteration 2890 Training loss 0.06415450572967529 Validation loss 0.061778221279382706 Accuracy 0.381103515625\n",
      "Iteration 2900 Training loss 0.058627452701330185 Validation loss 0.06118112802505493 Accuracy 0.384521484375\n",
      "Iteration 2910 Training loss 0.06334862858057022 Validation loss 0.06154220551252365 Accuracy 0.381591796875\n",
      "Iteration 2920 Training loss 0.06068137660622597 Validation loss 0.06144697591662407 Accuracy 0.38330078125\n",
      "Iteration 2930 Training loss 0.06126343831419945 Validation loss 0.06243954598903656 Accuracy 0.372802734375\n",
      "Iteration 2940 Training loss 0.05811047926545143 Validation loss 0.061227887868881226 Accuracy 0.38427734375\n",
      "Iteration 2950 Training loss 0.06203186884522438 Validation loss 0.061082873493433 Accuracy 0.385986328125\n",
      "Iteration 2960 Training loss 0.05871753767132759 Validation loss 0.06127851828932762 Accuracy 0.385009765625\n",
      "Iteration 2970 Training loss 0.06067468225955963 Validation loss 0.06135697662830353 Accuracy 0.3828125\n",
      "Iteration 2980 Training loss 0.06110954284667969 Validation loss 0.061166733503341675 Accuracy 0.385009765625\n",
      "Iteration 2990 Training loss 0.062137421220541 Validation loss 0.061334967613220215 Accuracy 0.3828125\n",
      "Iteration 3000 Training loss 0.06444761902093887 Validation loss 0.06132418289780617 Accuracy 0.38232421875\n",
      "Iteration 3010 Training loss 0.05919363722205162 Validation loss 0.06109168380498886 Accuracy 0.38525390625\n",
      "Iteration 3020 Training loss 0.06118636578321457 Validation loss 0.061317961663007736 Accuracy 0.38330078125\n",
      "Iteration 3030 Training loss 0.06192416325211525 Validation loss 0.06166740134358406 Accuracy 0.380126953125\n",
      "Iteration 3040 Training loss 0.061675842851400375 Validation loss 0.06171690300107002 Accuracy 0.379638671875\n",
      "Iteration 3050 Training loss 0.06495697796344757 Validation loss 0.06126938387751579 Accuracy 0.38427734375\n",
      "Iteration 3060 Training loss 0.06437855213880539 Validation loss 0.06116185337305069 Accuracy 0.385009765625\n",
      "Iteration 3070 Training loss 0.05759882554411888 Validation loss 0.06118816137313843 Accuracy 0.384521484375\n",
      "Iteration 3080 Training loss 0.05655236169695854 Validation loss 0.06118180602788925 Accuracy 0.385009765625\n",
      "Iteration 3090 Training loss 0.06350190192461014 Validation loss 0.06157363951206207 Accuracy 0.382080078125\n",
      "Iteration 3100 Training loss 0.06134757027029991 Validation loss 0.06130237132310867 Accuracy 0.384521484375\n",
      "Iteration 3110 Training loss 0.058159951120615005 Validation loss 0.061371296644210815 Accuracy 0.383056640625\n",
      "Iteration 3120 Training loss 0.0593370720744133 Validation loss 0.06138097494840622 Accuracy 0.384033203125\n",
      "Iteration 3130 Training loss 0.06415412575006485 Validation loss 0.061461396515369415 Accuracy 0.38330078125\n",
      "Iteration 3140 Training loss 0.06351548433303833 Validation loss 0.06124526262283325 Accuracy 0.38525390625\n",
      "Iteration 3150 Training loss 0.06018265709280968 Validation loss 0.06129898875951767 Accuracy 0.385009765625\n",
      "Iteration 3160 Training loss 0.06176214665174484 Validation loss 0.061200857162475586 Accuracy 0.384765625\n",
      "Iteration 3170 Training loss 0.0614425353705883 Validation loss 0.0612839013338089 Accuracy 0.385009765625\n",
      "Iteration 3180 Training loss 0.062528096139431 Validation loss 0.06145220994949341 Accuracy 0.38330078125\n",
      "Iteration 3190 Training loss 0.06122031435370445 Validation loss 0.06169099360704422 Accuracy 0.381591796875\n",
      "Iteration 3200 Training loss 0.06384416669607162 Validation loss 0.06130821257829666 Accuracy 0.3837890625\n",
      "Iteration 3210 Training loss 0.06099637970328331 Validation loss 0.061116356402635574 Accuracy 0.385498046875\n",
      "Iteration 3220 Training loss 0.0570160448551178 Validation loss 0.06128521263599396 Accuracy 0.385009765625\n",
      "Iteration 3230 Training loss 0.06174303963780403 Validation loss 0.06152782961726189 Accuracy 0.382080078125\n",
      "Iteration 3240 Training loss 0.05811203271150589 Validation loss 0.06134258955717087 Accuracy 0.3837890625\n",
      "Iteration 3250 Training loss 0.05781476944684982 Validation loss 0.061189550906419754 Accuracy 0.3857421875\n",
      "Iteration 3260 Training loss 0.0613967590034008 Validation loss 0.06182558834552765 Accuracy 0.378662109375\n",
      "Iteration 3270 Training loss 0.06017503887414932 Validation loss 0.06104778125882149 Accuracy 0.386962890625\n",
      "Iteration 3280 Training loss 0.06051269918680191 Validation loss 0.06146244332194328 Accuracy 0.381103515625\n",
      "Iteration 3290 Training loss 0.059136763215065 Validation loss 0.061065368354320526 Accuracy 0.38671875\n",
      "Iteration 3300 Training loss 0.06211116537451744 Validation loss 0.06130781024694443 Accuracy 0.385009765625\n",
      "Iteration 3310 Training loss 0.06197291612625122 Validation loss 0.061330828815698624 Accuracy 0.383056640625\n",
      "Iteration 3320 Training loss 0.06278657913208008 Validation loss 0.06268342584371567 Accuracy 0.36962890625\n",
      "Iteration 3330 Training loss 0.06093983352184296 Validation loss 0.061167653650045395 Accuracy 0.384765625\n",
      "Iteration 3340 Training loss 0.0646175816655159 Validation loss 0.06167543679475784 Accuracy 0.382080078125\n",
      "Iteration 3350 Training loss 0.0627027302980423 Validation loss 0.061459802091121674 Accuracy 0.383544921875\n",
      "Iteration 3360 Training loss 0.056548237800598145 Validation loss 0.05519186332821846 Accuracy 0.445068359375\n",
      "Iteration 3370 Training loss 0.057088445872068405 Validation loss 0.05489534139633179 Accuracy 0.44873046875\n",
      "Iteration 3380 Training loss 0.050530821084976196 Validation loss 0.054285019636154175 Accuracy 0.45458984375\n",
      "Iteration 3390 Training loss 0.05284414067864418 Validation loss 0.05354375019669533 Accuracy 0.4619140625\n",
      "Iteration 3400 Training loss 0.055678486824035645 Validation loss 0.05345776677131653 Accuracy 0.463623046875\n",
      "Iteration 3410 Training loss 0.05499023199081421 Validation loss 0.0532764308154583 Accuracy 0.464599609375\n",
      "Iteration 3420 Training loss 0.053867045789957047 Validation loss 0.05345506593585014 Accuracy 0.4638671875\n",
      "Iteration 3430 Training loss 0.056030649691820145 Validation loss 0.05341716110706329 Accuracy 0.463134765625\n",
      "Iteration 3440 Training loss 0.05506303161382675 Validation loss 0.05301117151975632 Accuracy 0.467529296875\n",
      "Iteration 3450 Training loss 0.05523405969142914 Validation loss 0.052840203046798706 Accuracy 0.469482421875\n",
      "Iteration 3460 Training loss 0.052478112280368805 Validation loss 0.05403773486614227 Accuracy 0.4580078125\n",
      "Iteration 3470 Training loss 0.052560094743967056 Validation loss 0.05269533395767212 Accuracy 0.470703125\n",
      "Iteration 3480 Training loss 0.05548984557390213 Validation loss 0.05272790789604187 Accuracy 0.469970703125\n",
      "Iteration 3490 Training loss 0.050412822514772415 Validation loss 0.05276774242520332 Accuracy 0.470703125\n",
      "Iteration 3500 Training loss 0.05237458273768425 Validation loss 0.0526011623442173 Accuracy 0.4716796875\n",
      "Iteration 3510 Training loss 0.05185665935277939 Validation loss 0.05291905999183655 Accuracy 0.46923828125\n",
      "Iteration 3520 Training loss 0.050958652049303055 Validation loss 0.053050920367240906 Accuracy 0.46728515625\n",
      "Iteration 3530 Training loss 0.054816700518131256 Validation loss 0.052696920931339264 Accuracy 0.470947265625\n",
      "Iteration 3540 Training loss 0.0566725879907608 Validation loss 0.05424826219677925 Accuracy 0.456298828125\n",
      "Iteration 3550 Training loss 0.051826801151037216 Validation loss 0.05311167240142822 Accuracy 0.466796875\n",
      "Iteration 3560 Training loss 0.05051837116479874 Validation loss 0.05274757370352745 Accuracy 0.47021484375\n",
      "Iteration 3570 Training loss 0.052720438688993454 Validation loss 0.05273052677512169 Accuracy 0.470703125\n",
      "Iteration 3580 Training loss 0.05037475377321243 Validation loss 0.052974577993154526 Accuracy 0.46875\n",
      "Iteration 3590 Training loss 0.051652852445840836 Validation loss 0.0525863841176033 Accuracy 0.471923828125\n",
      "Iteration 3600 Training loss 0.05157936364412308 Validation loss 0.053051162511110306 Accuracy 0.468505859375\n",
      "Iteration 3610 Training loss 0.05120137333869934 Validation loss 0.0534822940826416 Accuracy 0.4619140625\n",
      "Iteration 3620 Training loss 0.050834767520427704 Validation loss 0.05340787023305893 Accuracy 0.4638671875\n",
      "Iteration 3630 Training loss 0.05305740237236023 Validation loss 0.053399767726659775 Accuracy 0.464111328125\n",
      "Iteration 3640 Training loss 0.05327731370925903 Validation loss 0.052648499608039856 Accuracy 0.471435546875\n",
      "Iteration 3650 Training loss 0.05721479281783104 Validation loss 0.05257878825068474 Accuracy 0.47216796875\n",
      "Iteration 3660 Training loss 0.053512927144765854 Validation loss 0.05262576416134834 Accuracy 0.47119140625\n",
      "Iteration 3670 Training loss 0.05068053677678108 Validation loss 0.05350151285529137 Accuracy 0.46337890625\n",
      "Iteration 3680 Training loss 0.05127726122736931 Validation loss 0.052599236369132996 Accuracy 0.471923828125\n",
      "Iteration 3690 Training loss 0.05427267402410507 Validation loss 0.05280442163348198 Accuracy 0.469970703125\n",
      "Iteration 3700 Training loss 0.05497928708791733 Validation loss 0.05351787060499191 Accuracy 0.462890625\n",
      "Iteration 3710 Training loss 0.05343614146113396 Validation loss 0.052809812128543854 Accuracy 0.469970703125\n",
      "Iteration 3720 Training loss 0.048992425203323364 Validation loss 0.05274173244833946 Accuracy 0.470703125\n",
      "Iteration 3730 Training loss 0.05305762216448784 Validation loss 0.05331265553832054 Accuracy 0.464599609375\n",
      "Iteration 3740 Training loss 0.051560670137405396 Validation loss 0.05467524752020836 Accuracy 0.44921875\n",
      "Iteration 3750 Training loss 0.05420340597629547 Validation loss 0.05263923108577728 Accuracy 0.4716796875\n",
      "Iteration 3760 Training loss 0.05111518129706383 Validation loss 0.052976787090301514 Accuracy 0.468505859375\n",
      "Iteration 3770 Training loss 0.05155232176184654 Validation loss 0.05314444378018379 Accuracy 0.46630859375\n",
      "Iteration 3780 Training loss 0.0533936470746994 Validation loss 0.05254673212766647 Accuracy 0.471923828125\n",
      "Iteration 3790 Training loss 0.05198604241013527 Validation loss 0.05253998935222626 Accuracy 0.472900390625\n",
      "Iteration 3800 Training loss 0.052153680473566055 Validation loss 0.05265237018465996 Accuracy 0.4716796875\n",
      "Iteration 3810 Training loss 0.052593130618333817 Validation loss 0.052528899163007736 Accuracy 0.472900390625\n",
      "Iteration 3820 Training loss 0.052614323794841766 Validation loss 0.05250333249568939 Accuracy 0.472900390625\n",
      "Iteration 3830 Training loss 0.04858184605836868 Validation loss 0.05281458795070648 Accuracy 0.46923828125\n",
      "Iteration 3840 Training loss 0.04830420762300491 Validation loss 0.05272859334945679 Accuracy 0.469970703125\n",
      "Iteration 3850 Training loss 0.05227158963680267 Validation loss 0.05295512452721596 Accuracy 0.46923828125\n",
      "Iteration 3860 Training loss 0.05023953318595886 Validation loss 0.05293605476617813 Accuracy 0.46875\n",
      "Iteration 3870 Training loss 0.05184418708086014 Validation loss 0.052608367055654526 Accuracy 0.4716796875\n",
      "Iteration 3880 Training loss 0.0523306168615818 Validation loss 0.05340433493256569 Accuracy 0.464599609375\n",
      "Iteration 3890 Training loss 0.054719533771276474 Validation loss 0.05307382717728615 Accuracy 0.467041015625\n",
      "Iteration 3900 Training loss 0.052548542618751526 Validation loss 0.05296742171049118 Accuracy 0.468017578125\n",
      "Iteration 3910 Training loss 0.052355121821165085 Validation loss 0.052690066397190094 Accuracy 0.47021484375\n",
      "Iteration 3920 Training loss 0.05431956797838211 Validation loss 0.05240446329116821 Accuracy 0.473388671875\n",
      "Iteration 3930 Training loss 0.04939424619078636 Validation loss 0.052569642663002014 Accuracy 0.470947265625\n",
      "Iteration 3940 Training loss 0.05484160780906677 Validation loss 0.05330298840999603 Accuracy 0.464599609375\n",
      "Iteration 3950 Training loss 0.0531114786863327 Validation loss 0.05286020413041115 Accuracy 0.468994140625\n",
      "Iteration 3960 Training loss 0.049692098051309586 Validation loss 0.053208135068416595 Accuracy 0.464599609375\n",
      "Iteration 3970 Training loss 0.05126228928565979 Validation loss 0.05277349427342415 Accuracy 0.469970703125\n",
      "Iteration 3980 Training loss 0.046081166714429855 Validation loss 0.0525723434984684 Accuracy 0.4716796875\n",
      "Iteration 3990 Training loss 0.05033772066235542 Validation loss 0.05284598469734192 Accuracy 0.46875\n",
      "Iteration 4000 Training loss 0.0527382418513298 Validation loss 0.052434902638196945 Accuracy 0.473388671875\n",
      "Iteration 4010 Training loss 0.0532485656440258 Validation loss 0.05239573121070862 Accuracy 0.473388671875\n",
      "Iteration 4020 Training loss 0.05038578808307648 Validation loss 0.05299913138151169 Accuracy 0.468505859375\n",
      "Iteration 4030 Training loss 0.05262576416134834 Validation loss 0.053071923553943634 Accuracy 0.4677734375\n",
      "Iteration 4040 Training loss 0.05268166959285736 Validation loss 0.05263007804751396 Accuracy 0.470703125\n",
      "Iteration 4050 Training loss 0.05395613610744476 Validation loss 0.052913401275873184 Accuracy 0.4677734375\n",
      "Iteration 4060 Training loss 0.05115882307291031 Validation loss 0.052639275789260864 Accuracy 0.470703125\n",
      "Iteration 4070 Training loss 0.05744246393442154 Validation loss 0.05243806168437004 Accuracy 0.472412109375\n",
      "Iteration 4080 Training loss 0.053704190999269485 Validation loss 0.05251167714595795 Accuracy 0.47119140625\n",
      "Iteration 4090 Training loss 0.0471007265150547 Validation loss 0.05266042798757553 Accuracy 0.470458984375\n",
      "Iteration 4100 Training loss 0.052115269005298615 Validation loss 0.053041722625494 Accuracy 0.46630859375\n",
      "Iteration 4110 Training loss 0.0509251207113266 Validation loss 0.05229213833808899 Accuracy 0.473876953125\n",
      "Iteration 4120 Training loss 0.05605437234044075 Validation loss 0.052179910242557526 Accuracy 0.474609375\n",
      "Iteration 4130 Training loss 0.05459189787507057 Validation loss 0.052677225321531296 Accuracy 0.47119140625\n",
      "Iteration 4140 Training loss 0.05179424583911896 Validation loss 0.053095944225788116 Accuracy 0.467041015625\n",
      "Iteration 4150 Training loss 0.05127905681729317 Validation loss 0.05312662944197655 Accuracy 0.465087890625\n",
      "Iteration 4160 Training loss 0.054529231041669846 Validation loss 0.05248827114701271 Accuracy 0.470703125\n",
      "Iteration 4170 Training loss 0.048797424882650375 Validation loss 0.05259234830737114 Accuracy 0.469970703125\n",
      "Iteration 4180 Training loss 0.05837306007742882 Validation loss 0.05242489278316498 Accuracy 0.472412109375\n",
      "Iteration 4190 Training loss 0.054320327937603 Validation loss 0.05234629288315773 Accuracy 0.47216796875\n",
      "Iteration 4200 Training loss 0.0518704317510128 Validation loss 0.052289873361587524 Accuracy 0.472900390625\n",
      "Iteration 4210 Training loss 0.05297376587986946 Validation loss 0.05226224660873413 Accuracy 0.473388671875\n",
      "Iteration 4220 Training loss 0.05505865439772606 Validation loss 0.052348241209983826 Accuracy 0.47216796875\n",
      "Iteration 4230 Training loss 0.05167749151587486 Validation loss 0.05212812498211861 Accuracy 0.474609375\n",
      "Iteration 4240 Training loss 0.057543493807315826 Validation loss 0.05237344279885292 Accuracy 0.47265625\n",
      "Iteration 4250 Training loss 0.050895243883132935 Validation loss 0.05302318185567856 Accuracy 0.466796875\n",
      "Iteration 4260 Training loss 0.054175201803445816 Validation loss 0.05356704443693161 Accuracy 0.461181640625\n",
      "Iteration 4270 Training loss 0.05243002250790596 Validation loss 0.052914608269929886 Accuracy 0.468505859375\n",
      "Iteration 4280 Training loss 0.054698407649993896 Validation loss 0.052845779806375504 Accuracy 0.46923828125\n",
      "Iteration 4290 Training loss 0.04992816224694252 Validation loss 0.052664823830127716 Accuracy 0.470703125\n",
      "Iteration 4300 Training loss 0.052543818950653076 Validation loss 0.052468299865722656 Accuracy 0.471923828125\n",
      "Iteration 4310 Training loss 0.05477188900113106 Validation loss 0.052420180290937424 Accuracy 0.4716796875\n",
      "Iteration 4320 Training loss 0.05128263309597969 Validation loss 0.053399648517370224 Accuracy 0.461181640625\n",
      "Iteration 4330 Training loss 0.05045231059193611 Validation loss 0.052583884447813034 Accuracy 0.469970703125\n",
      "Iteration 4340 Training loss 0.052052877843379974 Validation loss 0.05279266834259033 Accuracy 0.468017578125\n",
      "Iteration 4350 Training loss 0.05218592658638954 Validation loss 0.05271751806139946 Accuracy 0.46923828125\n",
      "Iteration 4360 Training loss 0.04954244941473007 Validation loss 0.05286579951643944 Accuracy 0.468017578125\n",
      "Iteration 4370 Training loss 0.050787150859832764 Validation loss 0.05232495069503784 Accuracy 0.473388671875\n",
      "Iteration 4380 Training loss 0.054525554180145264 Validation loss 0.05230918899178505 Accuracy 0.4736328125\n",
      "Iteration 4390 Training loss 0.055639490485191345 Validation loss 0.05211132764816284 Accuracy 0.47509765625\n",
      "Iteration 4400 Training loss 0.051810137927532196 Validation loss 0.05231517553329468 Accuracy 0.473388671875\n",
      "Iteration 4410 Training loss 0.04859361797571182 Validation loss 0.052168577909469604 Accuracy 0.474853515625\n",
      "Iteration 4420 Training loss 0.05466519668698311 Validation loss 0.05200117453932762 Accuracy 0.47607421875\n",
      "Iteration 4430 Training loss 0.05326671898365021 Validation loss 0.05212608724832535 Accuracy 0.474609375\n",
      "Iteration 4440 Training loss 0.05520514026284218 Validation loss 0.05242707207798958 Accuracy 0.472900390625\n",
      "Iteration 4450 Training loss 0.053288258612155914 Validation loss 0.05315761640667915 Accuracy 0.465087890625\n",
      "Iteration 4460 Training loss 0.054502081125974655 Validation loss 0.053067516535520554 Accuracy 0.46533203125\n",
      "Iteration 4470 Training loss 0.051012735813856125 Validation loss 0.05223380774259567 Accuracy 0.473388671875\n",
      "Iteration 4480 Training loss 0.055893804877996445 Validation loss 0.053449928760528564 Accuracy 0.46240234375\n",
      "Iteration 4490 Training loss 0.05228769779205322 Validation loss 0.0521930456161499 Accuracy 0.474853515625\n",
      "Iteration 4500 Training loss 0.05445140600204468 Validation loss 0.05332063511013985 Accuracy 0.464599609375\n",
      "Iteration 4510 Training loss 0.05013288930058479 Validation loss 0.05228138715028763 Accuracy 0.472412109375\n",
      "Iteration 4520 Training loss 0.054805830121040344 Validation loss 0.05254334211349487 Accuracy 0.47021484375\n",
      "Iteration 4530 Training loss 0.05085491016507149 Validation loss 0.05210375413298607 Accuracy 0.4755859375\n",
      "Iteration 4540 Training loss 0.05327468737959862 Validation loss 0.05227573961019516 Accuracy 0.47412109375\n",
      "Iteration 4550 Training loss 0.05318427458405495 Validation loss 0.052643466740846634 Accuracy 0.470703125\n",
      "Iteration 4560 Training loss 0.05049952119588852 Validation loss 0.05240784212946892 Accuracy 0.47265625\n",
      "Iteration 4570 Training loss 0.052858274430036545 Validation loss 0.05310681089758873 Accuracy 0.465576171875\n",
      "Iteration 4580 Training loss 0.05491029471158981 Validation loss 0.05265739932656288 Accuracy 0.470458984375\n",
      "Iteration 4590 Training loss 0.05096408352255821 Validation loss 0.052212443202733994 Accuracy 0.474365234375\n",
      "Iteration 4600 Training loss 0.05127161741256714 Validation loss 0.0522400364279747 Accuracy 0.47412109375\n",
      "Iteration 4610 Training loss 0.05261845886707306 Validation loss 0.05273928493261337 Accuracy 0.469970703125\n",
      "Iteration 4620 Training loss 0.052480533719062805 Validation loss 0.052174162119627 Accuracy 0.474853515625\n",
      "Iteration 4630 Training loss 0.052191488444805145 Validation loss 0.05318928137421608 Accuracy 0.46630859375\n",
      "Iteration 4640 Training loss 0.05201161652803421 Validation loss 0.05259501934051514 Accuracy 0.471923828125\n",
      "Iteration 4650 Training loss 0.05215970799326897 Validation loss 0.05262136459350586 Accuracy 0.4716796875\n",
      "Iteration 4660 Training loss 0.05208619311451912 Validation loss 0.052606355398893356 Accuracy 0.4716796875\n",
      "Iteration 4670 Training loss 0.051966842263936996 Validation loss 0.05221100524067879 Accuracy 0.474853515625\n",
      "Iteration 4680 Training loss 0.04947218671441078 Validation loss 0.05226042866706848 Accuracy 0.47509765625\n",
      "Iteration 4690 Training loss 0.053752634674310684 Validation loss 0.05233931168913841 Accuracy 0.472900390625\n",
      "Iteration 4700 Training loss 0.052171315997838974 Validation loss 0.052368368953466415 Accuracy 0.472900390625\n",
      "Iteration 4710 Training loss 0.052296243607997894 Validation loss 0.052771247923374176 Accuracy 0.469482421875\n",
      "Iteration 4720 Training loss 0.0528574101626873 Validation loss 0.05294567719101906 Accuracy 0.46630859375\n",
      "Iteration 4730 Training loss 0.04949810355901718 Validation loss 0.05234783887863159 Accuracy 0.4736328125\n",
      "Iteration 4740 Training loss 0.04436717554926872 Validation loss 0.04539083316922188 Accuracy 0.54150390625\n",
      "Iteration 4750 Training loss 0.04086260497570038 Validation loss 0.04555503651499748 Accuracy 0.5419921875\n",
      "Iteration 4760 Training loss 0.046451445668935776 Validation loss 0.04470350593328476 Accuracy 0.54833984375\n",
      "Iteration 4770 Training loss 0.04604858532547951 Validation loss 0.04548073932528496 Accuracy 0.5390625\n",
      "Iteration 4780 Training loss 0.04953023046255112 Validation loss 0.04571780189871788 Accuracy 0.5390625\n",
      "Iteration 4790 Training loss 0.04583849012851715 Validation loss 0.044544901698827744 Accuracy 0.55029296875\n",
      "Iteration 4800 Training loss 0.045205626636743546 Validation loss 0.04455706104636192 Accuracy 0.55126953125\n",
      "Iteration 4810 Training loss 0.041058141738176346 Validation loss 0.04438895359635353 Accuracy 0.55126953125\n",
      "Iteration 4820 Training loss 0.046654753386974335 Validation loss 0.044307079166173935 Accuracy 0.552734375\n",
      "Iteration 4830 Training loss 0.04175594449043274 Validation loss 0.044625137001276016 Accuracy 0.55078125\n",
      "Iteration 4840 Training loss 0.04435684531927109 Validation loss 0.04410974681377411 Accuracy 0.55419921875\n",
      "Iteration 4850 Training loss 0.04125373810529709 Validation loss 0.0454762727022171 Accuracy 0.5419921875\n",
      "Iteration 4860 Training loss 0.048088982701301575 Validation loss 0.04456474632024765 Accuracy 0.55078125\n",
      "Iteration 4870 Training loss 0.04446323961019516 Validation loss 0.044587742537260056 Accuracy 0.55029296875\n",
      "Iteration 4880 Training loss 0.04419764503836632 Validation loss 0.045454081147909164 Accuracy 0.54296875\n",
      "Iteration 4890 Training loss 0.04271752014756203 Validation loss 0.04454898461699486 Accuracy 0.5498046875\n",
      "Iteration 4900 Training loss 0.04475310072302818 Validation loss 0.04466373100876808 Accuracy 0.54931640625\n",
      "Iteration 4910 Training loss 0.04666561260819435 Validation loss 0.04419964551925659 Accuracy 0.55517578125\n",
      "Iteration 4920 Training loss 0.03989559784531593 Validation loss 0.04449980705976486 Accuracy 0.5517578125\n",
      "Iteration 4930 Training loss 0.04630608484148979 Validation loss 0.044227972626686096 Accuracy 0.55419921875\n",
      "Iteration 4940 Training loss 0.044582489877939224 Validation loss 0.04469374567270279 Accuracy 0.548828125\n",
      "Iteration 4950 Training loss 0.04359535127878189 Validation loss 0.044262856245040894 Accuracy 0.55419921875\n",
      "Iteration 4960 Training loss 0.046470507979393005 Validation loss 0.04418860748410225 Accuracy 0.5556640625\n",
      "Iteration 4970 Training loss 0.04023756831884384 Validation loss 0.04438367113471031 Accuracy 0.55224609375\n",
      "Iteration 4980 Training loss 0.04739046096801758 Validation loss 0.044231776148080826 Accuracy 0.55224609375\n",
      "Iteration 4990 Training loss 0.04557628184556961 Validation loss 0.043782565742731094 Accuracy 0.5576171875\n",
      "Iteration 5000 Training loss 0.04471297189593315 Validation loss 0.04453069344162941 Accuracy 0.55126953125\n",
      "Iteration 5010 Training loss 0.04478570073843002 Validation loss 0.043945878744125366 Accuracy 0.55615234375\n",
      "Iteration 5020 Training loss 0.04588751494884491 Validation loss 0.0441003181040287 Accuracy 0.55419921875\n",
      "Iteration 5030 Training loss 0.04764940217137337 Validation loss 0.04525403305888176 Accuracy 0.544921875\n",
      "Iteration 5040 Training loss 0.04361152648925781 Validation loss 0.04372304677963257 Accuracy 0.55908203125\n",
      "Iteration 5050 Training loss 0.04379141330718994 Validation loss 0.04501995071768761 Accuracy 0.54541015625\n",
      "Iteration 5060 Training loss 0.0445907823741436 Validation loss 0.04418480768799782 Accuracy 0.55419921875\n",
      "Iteration 5070 Training loss 0.04253649711608887 Validation loss 0.0442747101187706 Accuracy 0.55322265625\n",
      "Iteration 5080 Training loss 0.048708364367485046 Validation loss 0.04444570094347 Accuracy 0.55224609375\n",
      "Iteration 5090 Training loss 0.04433014988899231 Validation loss 0.044603895395994186 Accuracy 0.54931640625\n",
      "Iteration 5100 Training loss 0.04466957226395607 Validation loss 0.043983057141304016 Accuracy 0.556640625\n",
      "Iteration 5110 Training loss 0.04661884903907776 Validation loss 0.044799432158470154 Accuracy 0.54931640625\n",
      "Iteration 5120 Training loss 0.041263382881879807 Validation loss 0.044520653784275055 Accuracy 0.55078125\n",
      "Iteration 5130 Training loss 0.04116268455982208 Validation loss 0.04378310590982437 Accuracy 0.55859375\n",
      "Iteration 5140 Training loss 0.041770417243242264 Validation loss 0.043895844370126724 Accuracy 0.55810546875\n",
      "Iteration 5150 Training loss 0.04401552304625511 Validation loss 0.04395119845867157 Accuracy 0.556640625\n",
      "Iteration 5160 Training loss 0.04312827065587044 Validation loss 0.043677255511283875 Accuracy 0.55908203125\n",
      "Iteration 5170 Training loss 0.043992094695568085 Validation loss 0.04356935992836952 Accuracy 0.56005859375\n",
      "Iteration 5180 Training loss 0.040891848504543304 Validation loss 0.04421674832701683 Accuracy 0.5546875\n",
      "Iteration 5190 Training loss 0.046239059418439865 Validation loss 0.04352511465549469 Accuracy 0.56005859375\n",
      "Iteration 5200 Training loss 0.0417119637131691 Validation loss 0.04415270313620567 Accuracy 0.556640625\n",
      "Iteration 5210 Training loss 0.04512948542833328 Validation loss 0.043989941477775574 Accuracy 0.5576171875\n",
      "Iteration 5220 Training loss 0.04038636013865471 Validation loss 0.04412702098488808 Accuracy 0.556640625\n",
      "Iteration 5230 Training loss 0.04728583246469498 Validation loss 0.0435740128159523 Accuracy 0.56103515625\n",
      "Iteration 5240 Training loss 0.038041722029447556 Validation loss 0.04369714856147766 Accuracy 0.560546875\n",
      "Iteration 5250 Training loss 0.04405684396624565 Validation loss 0.0437377393245697 Accuracy 0.5595703125\n",
      "Iteration 5260 Training loss 0.044598549604415894 Validation loss 0.04447448253631592 Accuracy 0.5517578125\n",
      "Iteration 5270 Training loss 0.04182332754135132 Validation loss 0.044219374656677246 Accuracy 0.552734375\n",
      "Iteration 5280 Training loss 0.04247516766190529 Validation loss 0.04350894317030907 Accuracy 0.5615234375\n",
      "Iteration 5290 Training loss 0.04334248974919319 Validation loss 0.043865811079740524 Accuracy 0.55712890625\n",
      "Iteration 5300 Training loss 0.040974561125040054 Validation loss 0.04432303458452225 Accuracy 0.55419921875\n",
      "Iteration 5310 Training loss 0.04236410930752754 Validation loss 0.044356297701597214 Accuracy 0.55322265625\n",
      "Iteration 5320 Training loss 0.04407939687371254 Validation loss 0.044364530593156815 Accuracy 0.55322265625\n",
      "Iteration 5330 Training loss 0.04262394830584526 Validation loss 0.043736692517995834 Accuracy 0.5595703125\n",
      "Iteration 5340 Training loss 0.044470809400081635 Validation loss 0.043863240629434586 Accuracy 0.55810546875\n",
      "Iteration 5350 Training loss 0.0431334413588047 Validation loss 0.04346437007188797 Accuracy 0.5625\n",
      "Iteration 5360 Training loss 0.04219779372215271 Validation loss 0.04466306418180466 Accuracy 0.5498046875\n",
      "Iteration 5370 Training loss 0.04603058472275734 Validation loss 0.04488428682088852 Accuracy 0.548828125\n",
      "Iteration 5380 Training loss 0.043323494493961334 Validation loss 0.04412408173084259 Accuracy 0.55517578125\n",
      "Iteration 5390 Training loss 0.04377451539039612 Validation loss 0.043885715305805206 Accuracy 0.5576171875\n",
      "Iteration 5400 Training loss 0.046182312071323395 Validation loss 0.04438774287700653 Accuracy 0.552734375\n",
      "Iteration 5410 Training loss 0.04381490498781204 Validation loss 0.043799128383398056 Accuracy 0.55859375\n",
      "Iteration 5420 Training loss 0.04196873679757118 Validation loss 0.04411875829100609 Accuracy 0.5546875\n",
      "Iteration 5430 Training loss 0.045073602348566055 Validation loss 0.04380885139107704 Accuracy 0.55859375\n",
      "Iteration 5440 Training loss 0.042943622916936874 Validation loss 0.04447697103023529 Accuracy 0.55126953125\n",
      "Iteration 5450 Training loss 0.0413016639649868 Validation loss 0.044040314853191376 Accuracy 0.556640625\n",
      "Iteration 5460 Training loss 0.04333651065826416 Validation loss 0.043805334717035294 Accuracy 0.55810546875\n",
      "Iteration 5470 Training loss 0.04132108390331268 Validation loss 0.043536774814128876 Accuracy 0.56103515625\n",
      "Iteration 5480 Training loss 0.04255499690771103 Validation loss 0.04368407651782036 Accuracy 0.56005859375\n",
      "Iteration 5490 Training loss 0.04265180230140686 Validation loss 0.044084686785936356 Accuracy 0.55712890625\n",
      "Iteration 5500 Training loss 0.043997183442115784 Validation loss 0.04457565397024155 Accuracy 0.5517578125\n",
      "Iteration 5510 Training loss 0.04347763955593109 Validation loss 0.04367905110120773 Accuracy 0.55810546875\n",
      "Iteration 5520 Training loss 0.038476791232824326 Validation loss 0.04368794709444046 Accuracy 0.55859375\n",
      "Iteration 5530 Training loss 0.0482686348259449 Validation loss 0.04360039532184601 Accuracy 0.56005859375\n",
      "Iteration 5540 Training loss 0.04202372208237648 Validation loss 0.0433630608022213 Accuracy 0.5634765625\n",
      "Iteration 5550 Training loss 0.04350965842604637 Validation loss 0.04341650754213333 Accuracy 0.56298828125\n",
      "Iteration 5560 Training loss 0.043888550251722336 Validation loss 0.04325864091515541 Accuracy 0.56396484375\n",
      "Iteration 5570 Training loss 0.04138987883925438 Validation loss 0.04370856657624245 Accuracy 0.560546875\n",
      "Iteration 5580 Training loss 0.04515417292714119 Validation loss 0.043584052473306656 Accuracy 0.56201171875\n",
      "Iteration 5590 Training loss 0.04412772133946419 Validation loss 0.04382222518324852 Accuracy 0.5595703125\n",
      "Iteration 5600 Training loss 0.044861018657684326 Validation loss 0.04460776224732399 Accuracy 0.55078125\n",
      "Iteration 5610 Training loss 0.04132606461644173 Validation loss 0.044145915657281876 Accuracy 0.5556640625\n",
      "Iteration 5620 Training loss 0.04238157719373703 Validation loss 0.04395214840769768 Accuracy 0.55810546875\n",
      "Iteration 5630 Training loss 0.038971614092588425 Validation loss 0.04374897480010986 Accuracy 0.56005859375\n",
      "Iteration 5640 Training loss 0.04401645436882973 Validation loss 0.04381973668932915 Accuracy 0.55810546875\n",
      "Iteration 5650 Training loss 0.0396357923746109 Validation loss 0.043960947543382645 Accuracy 0.55615234375\n",
      "Iteration 5660 Training loss 0.04375302046537399 Validation loss 0.04348530247807503 Accuracy 0.56103515625\n",
      "Iteration 5670 Training loss 0.042186032980680466 Validation loss 0.043570443987846375 Accuracy 0.56201171875\n",
      "Iteration 5680 Training loss 0.04019151255488396 Validation loss 0.043562039732933044 Accuracy 0.56201171875\n",
      "Iteration 5690 Training loss 0.041603896766901016 Validation loss 0.04368690401315689 Accuracy 0.56005859375\n",
      "Iteration 5700 Training loss 0.04172369837760925 Validation loss 0.043451398611068726 Accuracy 0.5615234375\n",
      "Iteration 5710 Training loss 0.044425129890441895 Validation loss 0.04386981204152107 Accuracy 0.55712890625\n",
      "Iteration 5720 Training loss 0.0452781580388546 Validation loss 0.04344608262181282 Accuracy 0.56201171875\n",
      "Iteration 5730 Training loss 0.042325060814619064 Validation loss 0.043849147856235504 Accuracy 0.55810546875\n",
      "Iteration 5740 Training loss 0.043193668127059937 Validation loss 0.043524011969566345 Accuracy 0.56201171875\n",
      "Iteration 5750 Training loss 0.04155987501144409 Validation loss 0.043192069977521896 Accuracy 0.56396484375\n",
      "Iteration 5760 Training loss 0.042884133756160736 Validation loss 0.0433632954955101 Accuracy 0.56201171875\n",
      "Iteration 5770 Training loss 0.04203333333134651 Validation loss 0.04419220983982086 Accuracy 0.55517578125\n",
      "Iteration 5780 Training loss 0.04088945686817169 Validation loss 0.04332950711250305 Accuracy 0.56201171875\n",
      "Iteration 5790 Training loss 0.04360754415392876 Validation loss 0.043744370341300964 Accuracy 0.5595703125\n",
      "Iteration 5800 Training loss 0.04444760084152222 Validation loss 0.04373414069414139 Accuracy 0.5595703125\n",
      "Iteration 5810 Training loss 0.04451167583465576 Validation loss 0.043534960597753525 Accuracy 0.56103515625\n",
      "Iteration 5820 Training loss 0.04220427945256233 Validation loss 0.04331497102975845 Accuracy 0.56201171875\n",
      "Iteration 5830 Training loss 0.04350663349032402 Validation loss 0.043109871447086334 Accuracy 0.564453125\n",
      "Iteration 5840 Training loss 0.04298347607254982 Validation loss 0.043922461569309235 Accuracy 0.55810546875\n",
      "Iteration 5850 Training loss 0.0457274466753006 Validation loss 0.043401919305324554 Accuracy 0.5625\n",
      "Iteration 5860 Training loss 0.042126163840293884 Validation loss 0.043735381215810776 Accuracy 0.55859375\n",
      "Iteration 5870 Training loss 0.03963400796055794 Validation loss 0.043689701706171036 Accuracy 0.56005859375\n",
      "Iteration 5880 Training loss 0.04662974923849106 Validation loss 0.043467726558446884 Accuracy 0.56103515625\n",
      "Iteration 5890 Training loss 0.04625697806477547 Validation loss 0.043412577360868454 Accuracy 0.5615234375\n",
      "Iteration 5900 Training loss 0.04073823615908623 Validation loss 0.043788231909275055 Accuracy 0.5576171875\n",
      "Iteration 5910 Training loss 0.040241602808237076 Validation loss 0.04365519806742668 Accuracy 0.56005859375\n",
      "Iteration 5920 Training loss 0.04236207902431488 Validation loss 0.04401221498847008 Accuracy 0.55712890625\n",
      "Iteration 5930 Training loss 0.04318821802735329 Validation loss 0.04372284188866615 Accuracy 0.55859375\n",
      "Iteration 5940 Training loss 0.04121096432209015 Validation loss 0.043393418192863464 Accuracy 0.5615234375\n",
      "Iteration 5950 Training loss 0.0399416908621788 Validation loss 0.043370071798563004 Accuracy 0.56298828125\n",
      "Iteration 5960 Training loss 0.03984919562935829 Validation loss 0.043442077934741974 Accuracy 0.56201171875\n",
      "Iteration 5970 Training loss 0.0420224592089653 Validation loss 0.04318287968635559 Accuracy 0.56396484375\n",
      "Iteration 5980 Training loss 0.03865104913711548 Validation loss 0.04348977282643318 Accuracy 0.5615234375\n",
      "Iteration 5990 Training loss 0.040319304913282394 Validation loss 0.043897248804569244 Accuracy 0.55712890625\n",
      "Iteration 6000 Training loss 0.041340455412864685 Validation loss 0.043406255543231964 Accuracy 0.56201171875\n",
      "Iteration 6010 Training loss 0.04188816621899605 Validation loss 0.0431048609316349 Accuracy 0.56494140625\n",
      "Iteration 6020 Training loss 0.04473474621772766 Validation loss 0.04347003623843193 Accuracy 0.56103515625\n",
      "Iteration 6030 Training loss 0.04235639423131943 Validation loss 0.043317411094903946 Accuracy 0.5634765625\n",
      "Iteration 6040 Training loss 0.041518017649650574 Validation loss 0.04322236403822899 Accuracy 0.564453125\n",
      "Iteration 6050 Training loss 0.04028113931417465 Validation loss 0.04329052194952965 Accuracy 0.5634765625\n",
      "Iteration 6060 Training loss 0.04361194372177124 Validation loss 0.04336625710129738 Accuracy 0.56201171875\n",
      "Iteration 6070 Training loss 0.04182208701968193 Validation loss 0.04340977594256401 Accuracy 0.5615234375\n",
      "Iteration 6080 Training loss 0.0445362813770771 Validation loss 0.04424098879098892 Accuracy 0.55322265625\n",
      "Iteration 6090 Training loss 0.04441653564572334 Validation loss 0.04387187212705612 Accuracy 0.55810546875\n",
      "Iteration 6100 Training loss 0.041888296604156494 Validation loss 0.0429471954703331 Accuracy 0.56591796875\n",
      "Iteration 6110 Training loss 0.04101664200425148 Validation loss 0.04316054657101631 Accuracy 0.56396484375\n",
      "Iteration 6120 Training loss 0.044852934777736664 Validation loss 0.04452717676758766 Accuracy 0.5517578125\n",
      "Iteration 6130 Training loss 0.042047761380672455 Validation loss 0.0435156375169754 Accuracy 0.56201171875\n",
      "Iteration 6140 Training loss 0.045292530208826065 Validation loss 0.04328888654708862 Accuracy 0.5634765625\n",
      "Iteration 6150 Training loss 0.03871461749076843 Validation loss 0.04314723610877991 Accuracy 0.5634765625\n",
      "Iteration 6160 Training loss 0.04588177427649498 Validation loss 0.0436297170817852 Accuracy 0.560546875\n",
      "Iteration 6170 Training loss 0.04354516416788101 Validation loss 0.04290192946791649 Accuracy 0.56689453125\n",
      "Iteration 6180 Training loss 0.039368774741888046 Validation loss 0.042939167469739914 Accuracy 0.56591796875\n",
      "Iteration 6190 Training loss 0.042626772075891495 Validation loss 0.043153081089258194 Accuracy 0.564453125\n",
      "Iteration 6200 Training loss 0.03933604434132576 Validation loss 0.04323357343673706 Accuracy 0.56494140625\n",
      "Iteration 6210 Training loss 0.04624100774526596 Validation loss 0.04470814764499664 Accuracy 0.548828125\n",
      "Iteration 6220 Training loss 0.04621167108416557 Validation loss 0.04320153221487999 Accuracy 0.56494140625\n",
      "Iteration 6230 Training loss 0.04617409035563469 Validation loss 0.04517027735710144 Accuracy 0.54248046875\n",
      "Iteration 6240 Training loss 0.04167252406477928 Validation loss 0.04288971424102783 Accuracy 0.56689453125\n",
      "Iteration 6250 Training loss 0.04258580505847931 Validation loss 0.04323044419288635 Accuracy 0.56201171875\n",
      "Iteration 6260 Training loss 0.042443446815013885 Validation loss 0.043160490691661835 Accuracy 0.56396484375\n",
      "Iteration 6270 Training loss 0.04317058250308037 Validation loss 0.042869798839092255 Accuracy 0.56689453125\n",
      "Iteration 6280 Training loss 0.04341118782758713 Validation loss 0.04355233907699585 Accuracy 0.560546875\n",
      "Iteration 6290 Training loss 0.038740646094083786 Validation loss 0.043162256479263306 Accuracy 0.56494140625\n",
      "Iteration 6300 Training loss 0.043905749917030334 Validation loss 0.04329195246100426 Accuracy 0.56201171875\n",
      "Iteration 6310 Training loss 0.03743337094783783 Validation loss 0.04349600523710251 Accuracy 0.56103515625\n",
      "Iteration 6320 Training loss 0.04104763641953468 Validation loss 0.04309528321027756 Accuracy 0.56396484375\n",
      "Iteration 6330 Training loss 0.042157094925642014 Validation loss 0.04308914765715599 Accuracy 0.564453125\n",
      "Iteration 6340 Training loss 0.04117630422115326 Validation loss 0.042824968695640564 Accuracy 0.56689453125\n",
      "Iteration 6350 Training loss 0.043375276029109955 Validation loss 0.043041981756687164 Accuracy 0.56494140625\n",
      "Iteration 6360 Training loss 0.0386161133646965 Validation loss 0.04343939945101738 Accuracy 0.5634765625\n",
      "Iteration 6370 Training loss 0.04488645866513252 Validation loss 0.04308917745947838 Accuracy 0.5654296875\n",
      "Iteration 6380 Training loss 0.04212073236703873 Validation loss 0.0432111956179142 Accuracy 0.56396484375\n",
      "Iteration 6390 Training loss 0.039986077696084976 Validation loss 0.043169055134058 Accuracy 0.564453125\n",
      "Iteration 6400 Training loss 0.04170433431863785 Validation loss 0.04295781999826431 Accuracy 0.56640625\n",
      "Iteration 6410 Training loss 0.04411070793867111 Validation loss 0.04367193207144737 Accuracy 0.5595703125\n",
      "Iteration 6420 Training loss 0.04209805279970169 Validation loss 0.04348618537187576 Accuracy 0.56103515625\n",
      "Iteration 6430 Training loss 0.04156726971268654 Validation loss 0.044741082936525345 Accuracy 0.54931640625\n",
      "Iteration 6440 Training loss 0.03967254236340523 Validation loss 0.04291696846485138 Accuracy 0.5654296875\n",
      "Iteration 6450 Training loss 0.042862262576818466 Validation loss 0.0428335964679718 Accuracy 0.56591796875\n",
      "Iteration 6460 Training loss 0.04069620370864868 Validation loss 0.04381635785102844 Accuracy 0.55615234375\n",
      "Iteration 6470 Training loss 0.04430701583623886 Validation loss 0.042863085865974426 Accuracy 0.5654296875\n",
      "Iteration 6480 Training loss 0.04002414271235466 Validation loss 0.04313961789011955 Accuracy 0.56298828125\n",
      "Iteration 6490 Training loss 0.04466401785612106 Validation loss 0.04343390837311745 Accuracy 0.560546875\n",
      "Iteration 6500 Training loss 0.039653435349464417 Validation loss 0.04312979802489281 Accuracy 0.5634765625\n",
      "Iteration 6510 Training loss 0.043340060859918594 Validation loss 0.043732572346925735 Accuracy 0.55859375\n",
      "Iteration 6520 Training loss 0.04110690578818321 Validation loss 0.043522078543901443 Accuracy 0.56103515625\n",
      "Iteration 6530 Training loss 0.04521657153964043 Validation loss 0.043248601257801056 Accuracy 0.5625\n",
      "Iteration 6540 Training loss 0.04300128296017647 Validation loss 0.04315996170043945 Accuracy 0.56396484375\n",
      "Iteration 6550 Training loss 0.041494570672512054 Validation loss 0.04298385605216026 Accuracy 0.5634765625\n",
      "Iteration 6560 Training loss 0.043101489543914795 Validation loss 0.04313408583402634 Accuracy 0.56298828125\n",
      "Iteration 6570 Training loss 0.043951984494924545 Validation loss 0.044341396540403366 Accuracy 0.55224609375\n",
      "Iteration 6580 Training loss 0.04700702801346779 Validation loss 0.044739462435245514 Accuracy 0.54736328125\n",
      "Iteration 6590 Training loss 0.043274667114019394 Validation loss 0.04326137527823448 Accuracy 0.56298828125\n",
      "Iteration 6600 Training loss 0.046821512281894684 Validation loss 0.043103743344545364 Accuracy 0.5634765625\n",
      "Iteration 6610 Training loss 0.04088238999247551 Validation loss 0.04315781965851784 Accuracy 0.5625\n",
      "Iteration 6620 Training loss 0.043268971145153046 Validation loss 0.04301685467362404 Accuracy 0.5634765625\n",
      "Iteration 6630 Training loss 0.040940120816230774 Validation loss 0.043549053370952606 Accuracy 0.5595703125\n",
      "Iteration 6640 Training loss 0.04749494791030884 Validation loss 0.04411771520972252 Accuracy 0.5537109375\n",
      "Iteration 6650 Training loss 0.04341449588537216 Validation loss 0.043597009032964706 Accuracy 0.56005859375\n",
      "Iteration 6660 Training loss 0.03828553855419159 Validation loss 0.04306074231863022 Accuracy 0.56298828125\n",
      "Iteration 6670 Training loss 0.04031846299767494 Validation loss 0.042817845940589905 Accuracy 0.56494140625\n",
      "Iteration 6680 Training loss 0.04305786266922951 Validation loss 0.0429244264960289 Accuracy 0.56298828125\n",
      "Iteration 6690 Training loss 0.04074978828430176 Validation loss 0.04307801276445389 Accuracy 0.56396484375\n",
      "Iteration 6700 Training loss 0.04309828579425812 Validation loss 0.04319782927632332 Accuracy 0.56201171875\n",
      "Iteration 6710 Training loss 0.037474196404218674 Validation loss 0.042943958193063736 Accuracy 0.56396484375\n",
      "Iteration 6720 Training loss 0.04165874049067497 Validation loss 0.0434265173971653 Accuracy 0.5595703125\n",
      "Iteration 6730 Training loss 0.04340570420026779 Validation loss 0.04304700717329979 Accuracy 0.5634765625\n",
      "Iteration 6740 Training loss 0.04501083493232727 Validation loss 0.042778052389621735 Accuracy 0.56689453125\n",
      "Iteration 6750 Training loss 0.04397913068532944 Validation loss 0.0428374707698822 Accuracy 0.564453125\n",
      "Iteration 6760 Training loss 0.04138360545039177 Validation loss 0.043040964752435684 Accuracy 0.56298828125\n",
      "Iteration 6770 Training loss 0.04495478421449661 Validation loss 0.04290890693664551 Accuracy 0.56494140625\n",
      "Iteration 6780 Training loss 0.0415494330227375 Validation loss 0.04283913969993591 Accuracy 0.56494140625\n",
      "Iteration 6790 Training loss 0.042234864085912704 Validation loss 0.04294973611831665 Accuracy 0.564453125\n",
      "Iteration 6800 Training loss 0.043568648397922516 Validation loss 0.04337430000305176 Accuracy 0.56103515625\n",
      "Iteration 6810 Training loss 0.04046300798654556 Validation loss 0.0436185784637928 Accuracy 0.55810546875\n",
      "Iteration 6820 Training loss 0.04413788393139839 Validation loss 0.043768513947725296 Accuracy 0.55859375\n",
      "Iteration 6830 Training loss 0.04211042821407318 Validation loss 0.043202586472034454 Accuracy 0.5634765625\n",
      "Iteration 6840 Training loss 0.04325202479958534 Validation loss 0.04307729750871658 Accuracy 0.56494140625\n",
      "Iteration 6850 Training loss 0.04259369522333145 Validation loss 0.043607622385025024 Accuracy 0.5576171875\n",
      "Iteration 6860 Training loss 0.04247035086154938 Validation loss 0.042739976197481155 Accuracy 0.56591796875\n",
      "Iteration 6870 Training loss 0.03775252401828766 Validation loss 0.04046507552266121 Accuracy 0.58935546875\n",
      "Iteration 6880 Training loss 0.038283221423625946 Validation loss 0.03858131170272827 Accuracy 0.61083984375\n",
      "Iteration 6890 Training loss 0.039179619401693344 Validation loss 0.039352476596832275 Accuracy 0.603515625\n",
      "Iteration 6900 Training loss 0.037432774901390076 Validation loss 0.03835182636976242 Accuracy 0.6123046875\n",
      "Iteration 6910 Training loss 0.036878764629364014 Validation loss 0.038489021360874176 Accuracy 0.61181640625\n",
      "Iteration 6920 Training loss 0.03406573459506035 Validation loss 0.038141585886478424 Accuracy 0.6142578125\n",
      "Iteration 6930 Training loss 0.04093441367149353 Validation loss 0.03878796845674515 Accuracy 0.60791015625\n",
      "Iteration 6940 Training loss 0.03620689734816551 Validation loss 0.03830622881650925 Accuracy 0.6123046875\n",
      "Iteration 6950 Training loss 0.03643743321299553 Validation loss 0.03800332918763161 Accuracy 0.6162109375\n",
      "Iteration 6960 Training loss 0.03385801240801811 Validation loss 0.03847789391875267 Accuracy 0.61181640625\n",
      "Iteration 6970 Training loss 0.03821370378136635 Validation loss 0.03782813251018524 Accuracy 0.619140625\n",
      "Iteration 6980 Training loss 0.037497542798519135 Validation loss 0.03772379830479622 Accuracy 0.6201171875\n",
      "Iteration 6990 Training loss 0.03737882524728775 Validation loss 0.037914153188467026 Accuracy 0.61767578125\n",
      "Iteration 7000 Training loss 0.03569438308477402 Validation loss 0.039185862988233566 Accuracy 0.60400390625\n",
      "Iteration 7010 Training loss 0.034768395125865936 Validation loss 0.038318246603012085 Accuracy 0.61328125\n",
      "Iteration 7020 Training loss 0.03704338148236275 Validation loss 0.03809596970677376 Accuracy 0.615234375\n",
      "Iteration 7030 Training loss 0.03565242141485214 Validation loss 0.037568144500255585 Accuracy 0.6201171875\n",
      "Iteration 7040 Training loss 0.034876152873039246 Validation loss 0.03765837103128433 Accuracy 0.619140625\n",
      "Iteration 7050 Training loss 0.033220816403627396 Validation loss 0.037637535482645035 Accuracy 0.61962890625\n",
      "Iteration 7060 Training loss 0.03813345730304718 Validation loss 0.03821149840950966 Accuracy 0.61376953125\n",
      "Iteration 7070 Training loss 0.03632979094982147 Validation loss 0.038784418255090714 Accuracy 0.60791015625\n",
      "Iteration 7080 Training loss 0.036943767219781876 Validation loss 0.03797085955739021 Accuracy 0.6171875\n",
      "Iteration 7090 Training loss 0.03819011524319649 Validation loss 0.03788083419203758 Accuracy 0.6171875\n",
      "Iteration 7100 Training loss 0.03511688485741615 Validation loss 0.03757105767726898 Accuracy 0.62060546875\n",
      "Iteration 7110 Training loss 0.036379374563694 Validation loss 0.03752145543694496 Accuracy 0.62158203125\n",
      "Iteration 7120 Training loss 0.03373634070158005 Validation loss 0.037289947271347046 Accuracy 0.62255859375\n",
      "Iteration 7130 Training loss 0.035163167864084244 Validation loss 0.03749127686023712 Accuracy 0.62060546875\n",
      "Iteration 7140 Training loss 0.03754248097538948 Validation loss 0.037879716604948044 Accuracy 0.61767578125\n",
      "Iteration 7150 Training loss 0.0378950759768486 Validation loss 0.04067229852080345 Accuracy 0.587890625\n",
      "Iteration 7160 Training loss 0.03952379524707794 Validation loss 0.037682224065065384 Accuracy 0.6181640625\n",
      "Iteration 7170 Training loss 0.035323843359947205 Validation loss 0.03789205104112625 Accuracy 0.6171875\n",
      "Iteration 7180 Training loss 0.036871835589408875 Validation loss 0.03739852085709572 Accuracy 0.6220703125\n",
      "Iteration 7190 Training loss 0.037185292690992355 Validation loss 0.038240544497966766 Accuracy 0.6142578125\n",
      "Iteration 7200 Training loss 0.03856479004025459 Validation loss 0.037697482854127884 Accuracy 0.619140625\n",
      "Iteration 7210 Training loss 0.03295856714248657 Validation loss 0.0374123752117157 Accuracy 0.62158203125\n",
      "Iteration 7220 Training loss 0.03806131333112717 Validation loss 0.03742905706167221 Accuracy 0.62109375\n",
      "Iteration 7230 Training loss 0.03358935937285423 Validation loss 0.03732769936323166 Accuracy 0.62353515625\n",
      "Iteration 7240 Training loss 0.03878827020525932 Validation loss 0.03713660314679146 Accuracy 0.625\n",
      "Iteration 7250 Training loss 0.0379573330283165 Validation loss 0.0375845804810524 Accuracy 0.6201171875\n",
      "Iteration 7260 Training loss 0.03233569115400314 Validation loss 0.037892282009124756 Accuracy 0.6162109375\n",
      "Iteration 7270 Training loss 0.03518793731927872 Validation loss 0.03797009959816933 Accuracy 0.61767578125\n",
      "Iteration 7280 Training loss 0.034458644688129425 Validation loss 0.0383189357817173 Accuracy 0.61083984375\n",
      "Iteration 7290 Training loss 0.03340741619467735 Validation loss 0.03811540827155113 Accuracy 0.61474609375\n",
      "Iteration 7300 Training loss 0.037296194583177567 Validation loss 0.03734392300248146 Accuracy 0.6220703125\n",
      "Iteration 7310 Training loss 0.032555364072322845 Validation loss 0.03732119873166084 Accuracy 0.62255859375\n",
      "Iteration 7320 Training loss 0.032429106533527374 Validation loss 0.037336062639951706 Accuracy 0.62158203125\n",
      "Iteration 7330 Training loss 0.04019375890493393 Validation loss 0.03745678812265396 Accuracy 0.61962890625\n",
      "Iteration 7340 Training loss 0.032286886125802994 Validation loss 0.03687680885195732 Accuracy 0.626953125\n",
      "Iteration 7350 Training loss 0.03360145539045334 Validation loss 0.03712394833564758 Accuracy 0.625\n",
      "Iteration 7360 Training loss 0.03343786671757698 Validation loss 0.03524588420987129 Accuracy 0.64208984375\n",
      "Iteration 7370 Training loss 0.03220345824956894 Validation loss 0.034041840583086014 Accuracy 0.65234375\n",
      "Iteration 7380 Training loss 0.029278788715600967 Validation loss 0.03376345708966255 Accuracy 0.65771484375\n",
      "Iteration 7390 Training loss 0.033759456127882004 Validation loss 0.03464064747095108 Accuracy 0.64892578125\n",
      "Iteration 7400 Training loss 0.03286134824156761 Validation loss 0.033610664308071136 Accuracy 0.65869140625\n",
      "Iteration 7410 Training loss 0.032502010464668274 Validation loss 0.03358250483870506 Accuracy 0.65771484375\n",
      "Iteration 7420 Training loss 0.037126876413822174 Validation loss 0.03318164870142937 Accuracy 0.6630859375\n",
      "Iteration 7430 Training loss 0.033108294010162354 Validation loss 0.03418900817632675 Accuracy 0.65380859375\n",
      "Iteration 7440 Training loss 0.028470413759350777 Validation loss 0.031698863953351974 Accuracy 0.67724609375\n",
      "Iteration 7450 Training loss 0.033968839794397354 Validation loss 0.03330113738775253 Accuracy 0.66064453125\n",
      "Iteration 7460 Training loss 0.028713079169392586 Validation loss 0.03099757805466652 Accuracy 0.68359375\n",
      "Iteration 7470 Training loss 0.029699889943003654 Validation loss 0.031686145812273026 Accuracy 0.67724609375\n",
      "Iteration 7480 Training loss 0.027318140491843224 Validation loss 0.03162771090865135 Accuracy 0.677734375\n",
      "Iteration 7490 Training loss 0.027173316106200218 Validation loss 0.030854957178235054 Accuracy 0.68505859375\n",
      "Iteration 7500 Training loss 0.032075557857751846 Validation loss 0.03326309472322464 Accuracy 0.66259765625\n",
      "Iteration 7510 Training loss 0.030811116099357605 Validation loss 0.031119821593165398 Accuracy 0.68310546875\n",
      "Iteration 7520 Training loss 0.030891217291355133 Validation loss 0.031120875850319862 Accuracy 0.68408203125\n",
      "Iteration 7530 Training loss 0.033219851553440094 Validation loss 0.031463924795389175 Accuracy 0.6796875\n",
      "Iteration 7540 Training loss 0.03144798427820206 Validation loss 0.032331738620996475 Accuracy 0.671875\n",
      "Iteration 7550 Training loss 0.027094395831227303 Validation loss 0.0316031388938427 Accuracy 0.67919921875\n",
      "Iteration 7560 Training loss 0.02940979227423668 Validation loss 0.03203275427222252 Accuracy 0.6748046875\n",
      "Iteration 7570 Training loss 0.029634205624461174 Validation loss 0.03194455802440643 Accuracy 0.67529296875\n",
      "Iteration 7580 Training loss 0.032321497797966 Validation loss 0.03164574131369591 Accuracy 0.6787109375\n",
      "Iteration 7590 Training loss 0.030712706968188286 Validation loss 0.03126341104507446 Accuracy 0.6826171875\n",
      "Iteration 7600 Training loss 0.030713897198438644 Validation loss 0.030526602640748024 Accuracy 0.68896484375\n",
      "Iteration 7610 Training loss 0.028760986402630806 Validation loss 0.031717170029878616 Accuracy 0.677734375\n",
      "Iteration 7620 Training loss 0.033041827380657196 Validation loss 0.03797266632318497 Accuracy 0.61572265625\n",
      "Iteration 7630 Training loss 0.030204974114894867 Validation loss 0.03124387562274933 Accuracy 0.681640625\n",
      "Iteration 7640 Training loss 0.028805037960410118 Validation loss 0.03240704536437988 Accuracy 0.67138671875\n",
      "Iteration 7650 Training loss 0.027818839997053146 Validation loss 0.03213616833090782 Accuracy 0.6728515625\n",
      "Iteration 7660 Training loss 0.0356982946395874 Validation loss 0.035040441900491714 Accuracy 0.64697265625\n",
      "Iteration 7670 Training loss 0.03485509380698204 Validation loss 0.03442079946398735 Accuracy 0.65087890625\n",
      "Iteration 7680 Training loss 0.03019058145582676 Validation loss 0.03266562521457672 Accuracy 0.66796875\n",
      "Iteration 7690 Training loss 0.026003343984484673 Validation loss 0.03064189851284027 Accuracy 0.6875\n",
      "Iteration 7700 Training loss 0.029086997732520103 Validation loss 0.03095114603638649 Accuracy 0.685546875\n",
      "Iteration 7710 Training loss 0.030623784288764 Validation loss 0.03255688026547432 Accuracy 0.669921875\n",
      "Iteration 7720 Training loss 0.030068090185523033 Validation loss 0.03227539360523224 Accuracy 0.6728515625\n",
      "Iteration 7730 Training loss 0.0307490024715662 Validation loss 0.030534248799085617 Accuracy 0.68896484375\n",
      "Iteration 7740 Training loss 0.035712145268917084 Validation loss 0.03445374220609665 Accuracy 0.65087890625\n",
      "Iteration 7750 Training loss 0.030761225149035454 Validation loss 0.030957113951444626 Accuracy 0.6845703125\n",
      "Iteration 7760 Training loss 0.02935541234910488 Validation loss 0.0324554480612278 Accuracy 0.6708984375\n",
      "Iteration 7770 Training loss 0.032240718603134155 Validation loss 0.030582888051867485 Accuracy 0.68798828125\n",
      "Iteration 7780 Training loss 0.03155471384525299 Validation loss 0.0315869115293026 Accuracy 0.6787109375\n",
      "Iteration 7790 Training loss 0.03228555619716644 Validation loss 0.031028812751173973 Accuracy 0.68310546875\n",
      "Iteration 7800 Training loss 0.030906086787581444 Validation loss 0.030719716101884842 Accuracy 0.6875\n",
      "Iteration 7810 Training loss 0.02900327555835247 Validation loss 0.030723944306373596 Accuracy 0.68701171875\n",
      "Iteration 7820 Training loss 0.029982414096593857 Validation loss 0.030575644224882126 Accuracy 0.6884765625\n",
      "Iteration 7830 Training loss 0.032904159277677536 Validation loss 0.031683601438999176 Accuracy 0.67724609375\n",
      "Iteration 7840 Training loss 0.02872474119067192 Validation loss 0.03027024306356907 Accuracy 0.69091796875\n",
      "Iteration 7850 Training loss 0.03148789331316948 Validation loss 0.034235887229442596 Accuracy 0.6533203125\n",
      "Iteration 7860 Training loss 0.030920017510652542 Validation loss 0.030929232016205788 Accuracy 0.68408203125\n",
      "Iteration 7870 Training loss 0.029127538204193115 Validation loss 0.03065752610564232 Accuracy 0.6875\n",
      "Iteration 7880 Training loss 0.033522699028253555 Validation loss 0.03318967670202255 Accuracy 0.66162109375\n",
      "Iteration 7890 Training loss 0.02755344659090042 Validation loss 0.032235004007816315 Accuracy 0.67333984375\n",
      "Iteration 7900 Training loss 0.032539743930101395 Validation loss 0.03307776525616646 Accuracy 0.66357421875\n",
      "Iteration 7910 Training loss 0.03323785960674286 Validation loss 0.03453819826245308 Accuracy 0.6494140625\n",
      "Iteration 7920 Training loss 0.03316228836774826 Validation loss 0.03185245022177696 Accuracy 0.6767578125\n",
      "Iteration 7930 Training loss 0.02726142294704914 Validation loss 0.030723214149475098 Accuracy 0.68701171875\n",
      "Iteration 7940 Training loss 0.03230014443397522 Validation loss 0.031748734414577484 Accuracy 0.6767578125\n",
      "Iteration 7950 Training loss 0.02994055487215519 Validation loss 0.030470507219433784 Accuracy 0.68994140625\n",
      "Iteration 7960 Training loss 0.02857290394604206 Validation loss 0.031623829156160355 Accuracy 0.6767578125\n",
      "Iteration 7970 Training loss 0.028205877169966698 Validation loss 0.031162962317466736 Accuracy 0.68359375\n",
      "Iteration 7980 Training loss 0.031480222940444946 Validation loss 0.03362598642706871 Accuracy 0.6572265625\n",
      "Iteration 7990 Training loss 0.027466878294944763 Validation loss 0.031036188825964928 Accuracy 0.68310546875\n",
      "Iteration 8000 Training loss 0.02760910801589489 Validation loss 0.030939489603042603 Accuracy 0.68505859375\n",
      "Iteration 8010 Training loss 0.029840240254998207 Validation loss 0.03156331554055214 Accuracy 0.67919921875\n",
      "Iteration 8020 Training loss 0.027991607785224915 Validation loss 0.030782576650381088 Accuracy 0.68603515625\n",
      "Iteration 8030 Training loss 0.031075583770871162 Validation loss 0.030914828181266785 Accuracy 0.6865234375\n",
      "Iteration 8040 Training loss 0.03060149773955345 Validation loss 0.0315147303044796 Accuracy 0.67919921875\n",
      "Iteration 8050 Training loss 0.027119887992739677 Validation loss 0.03038814291357994 Accuracy 0.69091796875\n",
      "Iteration 8060 Training loss 0.029621507972478867 Validation loss 0.030599545687437057 Accuracy 0.6865234375\n",
      "Iteration 8070 Training loss 0.030200021341443062 Validation loss 0.030441349372267723 Accuracy 0.69091796875\n",
      "Iteration 8080 Training loss 0.028628524392843246 Validation loss 0.03008476085960865 Accuracy 0.69287109375\n",
      "Iteration 8090 Training loss 0.027430996298789978 Validation loss 0.02993793413043022 Accuracy 0.6943359375\n",
      "Iteration 8100 Training loss 0.02897854521870613 Validation loss 0.031288836151361465 Accuracy 0.6806640625\n",
      "Iteration 8110 Training loss 0.03095744363963604 Validation loss 0.029854169115424156 Accuracy 0.6953125\n",
      "Iteration 8120 Training loss 0.029749775305390358 Validation loss 0.03047586791217327 Accuracy 0.6904296875\n",
      "Iteration 8130 Training loss 0.03345712274312973 Validation loss 0.030806556344032288 Accuracy 0.68603515625\n",
      "Iteration 8140 Training loss 0.03163331001996994 Validation loss 0.0322258397936821 Accuracy 0.67333984375\n",
      "Iteration 8150 Training loss 0.030553199350833893 Validation loss 0.03068581409752369 Accuracy 0.6875\n",
      "Iteration 8160 Training loss 0.029314231127500534 Validation loss 0.030666569247841835 Accuracy 0.6865234375\n",
      "Iteration 8170 Training loss 0.0298884529620409 Validation loss 0.030046286061406136 Accuracy 0.69384765625\n",
      "Iteration 8180 Training loss 0.029464660212397575 Validation loss 0.03117876872420311 Accuracy 0.681640625\n",
      "Iteration 8190 Training loss 0.027353202924132347 Validation loss 0.02998408116400242 Accuracy 0.69482421875\n",
      "Iteration 8200 Training loss 0.03176011145114899 Validation loss 0.0310454573482275 Accuracy 0.68359375\n",
      "Iteration 8210 Training loss 0.028336601331830025 Validation loss 0.02968381531536579 Accuracy 0.6962890625\n",
      "Iteration 8220 Training loss 0.028204889968037605 Validation loss 0.02981581911444664 Accuracy 0.6953125\n",
      "Iteration 8230 Training loss 0.02869001217186451 Validation loss 0.03054213710129261 Accuracy 0.68701171875\n",
      "Iteration 8240 Training loss 0.03293171897530556 Validation loss 0.031052889302372932 Accuracy 0.6826171875\n",
      "Iteration 8250 Training loss 0.030993977561593056 Validation loss 0.03099105879664421 Accuracy 0.68408203125\n",
      "Iteration 8260 Training loss 0.027679352089762688 Validation loss 0.030614973977208138 Accuracy 0.68701171875\n",
      "Iteration 8270 Training loss 0.030518589541316032 Validation loss 0.03088081069290638 Accuracy 0.6845703125\n",
      "Iteration 8280 Training loss 0.028561405837535858 Validation loss 0.03177553787827492 Accuracy 0.67724609375\n",
      "Iteration 8290 Training loss 0.027356678619980812 Validation loss 0.029447108507156372 Accuracy 0.69921875\n",
      "Iteration 8300 Training loss 0.027408065274357796 Validation loss 0.029687367379665375 Accuracy 0.69677734375\n",
      "Iteration 8310 Training loss 0.028598245233297348 Validation loss 0.03004773147404194 Accuracy 0.6943359375\n",
      "Iteration 8320 Training loss 0.03157990053296089 Validation loss 0.029928764328360558 Accuracy 0.6943359375\n",
      "Iteration 8330 Training loss 0.029087601229548454 Validation loss 0.030691862106323242 Accuracy 0.6865234375\n",
      "Iteration 8340 Training loss 0.026144886389374733 Validation loss 0.030074454843997955 Accuracy 0.69482421875\n",
      "Iteration 8350 Training loss 0.03245946392416954 Validation loss 0.030903836712241173 Accuracy 0.6865234375\n",
      "Iteration 8360 Training loss 0.031065212562680244 Validation loss 0.032765842974185944 Accuracy 0.66650390625\n",
      "Iteration 8370 Training loss 0.027054153382778168 Validation loss 0.02982155606150627 Accuracy 0.69580078125\n",
      "Iteration 8380 Training loss 0.026127511635422707 Validation loss 0.030082840472459793 Accuracy 0.69189453125\n",
      "Iteration 8390 Training loss 0.03137262910604477 Validation loss 0.031043803319334984 Accuracy 0.68408203125\n",
      "Iteration 8400 Training loss 0.030089057981967926 Validation loss 0.030587665736675262 Accuracy 0.68701171875\n",
      "Iteration 8410 Training loss 0.033270660787820816 Validation loss 0.031841229647397995 Accuracy 0.6748046875\n",
      "Iteration 8420 Training loss 0.02832123078405857 Validation loss 0.02962687984108925 Accuracy 0.6982421875\n",
      "Iteration 8430 Training loss 0.024137739092111588 Validation loss 0.03009822592139244 Accuracy 0.69384765625\n",
      "Iteration 8440 Training loss 0.030798105522990227 Validation loss 0.03077869489789009 Accuracy 0.68603515625\n",
      "Iteration 8450 Training loss 0.030032318085432053 Validation loss 0.030000779777765274 Accuracy 0.69482421875\n",
      "Iteration 8460 Training loss 0.02776009403169155 Validation loss 0.029673870652914047 Accuracy 0.697265625\n",
      "Iteration 8470 Training loss 0.029149316251277924 Validation loss 0.030021199956536293 Accuracy 0.69384765625\n",
      "Iteration 8480 Training loss 0.028339294716715813 Validation loss 0.03003263846039772 Accuracy 0.69482421875\n",
      "Iteration 8490 Training loss 0.02799369767308235 Validation loss 0.030177753418684006 Accuracy 0.69287109375\n",
      "Iteration 8500 Training loss 0.02711164765059948 Validation loss 0.029717791825532913 Accuracy 0.6982421875\n",
      "Iteration 8510 Training loss 0.030663060024380684 Validation loss 0.029920805245637894 Accuracy 0.69482421875\n",
      "Iteration 8520 Training loss 0.026353389024734497 Validation loss 0.02976119890809059 Accuracy 0.6953125\n",
      "Iteration 8530 Training loss 0.02791457064449787 Validation loss 0.02978183515369892 Accuracy 0.6962890625\n",
      "Iteration 8540 Training loss 0.028641102835536003 Validation loss 0.030048448592424393 Accuracy 0.69140625\n",
      "Iteration 8550 Training loss 0.02722369320690632 Validation loss 0.030396994203329086 Accuracy 0.689453125\n",
      "Iteration 8560 Training loss 0.027523808181285858 Validation loss 0.03181976079940796 Accuracy 0.6767578125\n",
      "Iteration 8570 Training loss 0.028438441455364227 Validation loss 0.029314013198018074 Accuracy 0.701171875\n",
      "Iteration 8580 Training loss 0.02933187037706375 Validation loss 0.030178045853972435 Accuracy 0.6904296875\n",
      "Iteration 8590 Training loss 0.030586516484618187 Validation loss 0.029643623158335686 Accuracy 0.697265625\n",
      "Iteration 8600 Training loss 0.024357527494430542 Validation loss 0.029189366847276688 Accuracy 0.70166015625\n",
      "Iteration 8610 Training loss 0.026375889778137207 Validation loss 0.029680706560611725 Accuracy 0.6962890625\n",
      "Iteration 8620 Training loss 0.030062003061175346 Validation loss 0.02942056581377983 Accuracy 0.69921875\n",
      "Iteration 8630 Training loss 0.025989441201090813 Validation loss 0.02979840710759163 Accuracy 0.6962890625\n",
      "Iteration 8640 Training loss 0.027811802923679352 Validation loss 0.02942643314599991 Accuracy 0.69921875\n",
      "Iteration 8650 Training loss 0.02825791947543621 Validation loss 0.02950294315814972 Accuracy 0.697265625\n",
      "Iteration 8660 Training loss 0.025022059679031372 Validation loss 0.029241686686873436 Accuracy 0.7001953125\n",
      "Iteration 8670 Training loss 0.025942591950297356 Validation loss 0.030022071674466133 Accuracy 0.69189453125\n",
      "Iteration 8680 Training loss 0.02876291424036026 Validation loss 0.02987469732761383 Accuracy 0.6943359375\n",
      "Iteration 8690 Training loss 0.02937520481646061 Validation loss 0.029197128489613533 Accuracy 0.70068359375\n",
      "Iteration 8700 Training loss 0.027515435591340065 Validation loss 0.030047211796045303 Accuracy 0.69189453125\n",
      "Iteration 8710 Training loss 0.02617878094315529 Validation loss 0.028978437185287476 Accuracy 0.70263671875\n",
      "Iteration 8720 Training loss 0.027401473373174667 Validation loss 0.029347263276576996 Accuracy 0.69873046875\n",
      "Iteration 8730 Training loss 0.025678085163235664 Validation loss 0.02925904281437397 Accuracy 0.70068359375\n",
      "Iteration 8740 Training loss 0.026251979172229767 Validation loss 0.029250599443912506 Accuracy 0.70068359375\n",
      "Iteration 8750 Training loss 0.03151731193065643 Validation loss 0.02976394258439541 Accuracy 0.6953125\n",
      "Iteration 8760 Training loss 0.027701212093234062 Validation loss 0.03023066371679306 Accuracy 0.69091796875\n",
      "Iteration 8770 Training loss 0.025969650596380234 Validation loss 0.029061131179332733 Accuracy 0.7021484375\n",
      "Iteration 8780 Training loss 0.03169579803943634 Validation loss 0.03272654861211777 Accuracy 0.66650390625\n",
      "Iteration 8790 Training loss 0.025839243084192276 Validation loss 0.029890624806284904 Accuracy 0.6943359375\n",
      "Iteration 8800 Training loss 0.02646080031991005 Validation loss 0.02950049191713333 Accuracy 0.697265625\n",
      "Iteration 8810 Training loss 0.026502853259444237 Validation loss 0.03037901781499386 Accuracy 0.68798828125\n",
      "Iteration 8820 Training loss 0.02927199751138687 Validation loss 0.0291096493601799 Accuracy 0.70166015625\n",
      "Iteration 8830 Training loss 0.02813032455742359 Validation loss 0.029219970107078552 Accuracy 0.701171875\n",
      "Iteration 8840 Training loss 0.028032083064317703 Validation loss 0.0288620013743639 Accuracy 0.70458984375\n",
      "Iteration 8850 Training loss 0.02814548648893833 Validation loss 0.029667314141988754 Accuracy 0.69482421875\n",
      "Iteration 8860 Training loss 0.025713711977005005 Validation loss 0.02894379384815693 Accuracy 0.7021484375\n",
      "Iteration 8870 Training loss 0.019214250147342682 Validation loss 0.020636746659874916 Accuracy 0.7880859375\n",
      "Iteration 8880 Training loss 0.015699809417128563 Validation loss 0.02158716507256031 Accuracy 0.779296875\n",
      "Iteration 8890 Training loss 0.02006777748465538 Validation loss 0.020622026175260544 Accuracy 0.7890625\n",
      "Iteration 8900 Training loss 0.014146855100989342 Validation loss 0.020985662937164307 Accuracy 0.78515625\n",
      "Iteration 8910 Training loss 0.0213426873087883 Validation loss 0.02188922092318535 Accuracy 0.7763671875\n",
      "Iteration 8920 Training loss 0.02018379233777523 Validation loss 0.02092329040169716 Accuracy 0.7861328125\n",
      "Iteration 8930 Training loss 0.021284906193614006 Validation loss 0.0218126829713583 Accuracy 0.77587890625\n",
      "Iteration 8940 Training loss 0.020098775625228882 Validation loss 0.0228671133518219 Accuracy 0.765625\n",
      "Iteration 8950 Training loss 0.019963782280683517 Validation loss 0.021019956097006798 Accuracy 0.7841796875\n",
      "Iteration 8960 Training loss 0.01951751485466957 Validation loss 0.02054251916706562 Accuracy 0.78955078125\n",
      "Iteration 8970 Training loss 0.021432770416140556 Validation loss 0.022679701447486877 Accuracy 0.76806640625\n",
      "Iteration 8980 Training loss 0.018202079460024834 Validation loss 0.020428303629159927 Accuracy 0.79150390625\n",
      "Iteration 8990 Training loss 0.019375959411263466 Validation loss 0.020913900807499886 Accuracy 0.7861328125\n",
      "Iteration 9000 Training loss 0.020389942452311516 Validation loss 0.02030777372419834 Accuracy 0.7919921875\n",
      "Iteration 9010 Training loss 0.01635976880788803 Validation loss 0.020106585696339607 Accuracy 0.7939453125\n",
      "Iteration 9020 Training loss 0.016196349635720253 Validation loss 0.02015765942633152 Accuracy 0.79296875\n",
      "Iteration 9030 Training loss 0.01375990454107523 Validation loss 0.020162619650363922 Accuracy 0.79296875\n",
      "Iteration 9040 Training loss 0.016564449295401573 Validation loss 0.019780900329351425 Accuracy 0.796875\n",
      "Iteration 9050 Training loss 0.019248150289058685 Validation loss 0.022151855751872063 Accuracy 0.7734375\n",
      "Iteration 9060 Training loss 0.017804928123950958 Validation loss 0.022300785407423973 Accuracy 0.77197265625\n",
      "Iteration 9070 Training loss 0.02157003805041313 Validation loss 0.020584603771567345 Accuracy 0.7890625\n",
      "Iteration 9080 Training loss 0.0162399522960186 Validation loss 0.01999194547533989 Accuracy 0.79443359375\n",
      "Iteration 9090 Training loss 0.01806027628481388 Validation loss 0.02085449919104576 Accuracy 0.7861328125\n",
      "Iteration 9100 Training loss 0.01570982113480568 Validation loss 0.020790379494428635 Accuracy 0.78662109375\n",
      "Iteration 9110 Training loss 0.017088372260332108 Validation loss 0.020760685205459595 Accuracy 0.7880859375\n",
      "Iteration 9120 Training loss 0.017419613897800446 Validation loss 0.020000005140900612 Accuracy 0.79541015625\n",
      "Iteration 9130 Training loss 0.017558086663484573 Validation loss 0.020191827788949013 Accuracy 0.79443359375\n",
      "Iteration 9140 Training loss 0.018046004697680473 Validation loss 0.020005598664283752 Accuracy 0.7958984375\n",
      "Iteration 9150 Training loss 0.020836804062128067 Validation loss 0.020257335156202316 Accuracy 0.79296875\n",
      "Iteration 9160 Training loss 0.020600933581590652 Validation loss 0.01996024139225483 Accuracy 0.7958984375\n",
      "Iteration 9170 Training loss 0.020328020676970482 Validation loss 0.01979345642030239 Accuracy 0.796875\n",
      "Iteration 9180 Training loss 0.018792353570461273 Validation loss 0.020219571888446808 Accuracy 0.79248046875\n",
      "Iteration 9190 Training loss 0.015445144847035408 Validation loss 0.020085565745830536 Accuracy 0.79443359375\n",
      "Iteration 9200 Training loss 0.018876586109399796 Validation loss 0.020236898213624954 Accuracy 0.79248046875\n",
      "Iteration 9210 Training loss 0.01862042397260666 Validation loss 0.019837195053696632 Accuracy 0.796875\n",
      "Iteration 9220 Training loss 0.01693749614059925 Validation loss 0.020405223593115807 Accuracy 0.791015625\n",
      "Iteration 9230 Training loss 0.018039321526885033 Validation loss 0.020466135814785957 Accuracy 0.7900390625\n",
      "Iteration 9240 Training loss 0.01776171661913395 Validation loss 0.02035635896027088 Accuracy 0.79296875\n",
      "Iteration 9250 Training loss 0.01905910298228264 Validation loss 0.01995561458170414 Accuracy 0.7958984375\n",
      "Iteration 9260 Training loss 0.01891482062637806 Validation loss 0.020512495189905167 Accuracy 0.7890625\n",
      "Iteration 9270 Training loss 0.018384814262390137 Validation loss 0.021349556744098663 Accuracy 0.78173828125\n",
      "Iteration 9280 Training loss 0.019415847957134247 Validation loss 0.021009251475334167 Accuracy 0.78564453125\n",
      "Iteration 9290 Training loss 0.018765782937407494 Validation loss 0.02205444686114788 Accuracy 0.7744140625\n",
      "Iteration 9300 Training loss 0.014420815743505955 Validation loss 0.02023608237504959 Accuracy 0.79296875\n",
      "Iteration 9310 Training loss 0.01912388764321804 Validation loss 0.020623158663511276 Accuracy 0.7890625\n",
      "Iteration 9320 Training loss 0.019495347514748573 Validation loss 0.020414847880601883 Accuracy 0.79150390625\n",
      "Iteration 9330 Training loss 0.020493071526288986 Validation loss 0.020773084834218025 Accuracy 0.78662109375\n",
      "Iteration 9340 Training loss 0.015596074052155018 Validation loss 0.020144445821642876 Accuracy 0.79443359375\n",
      "Iteration 9350 Training loss 0.019433697685599327 Validation loss 0.02083783969283104 Accuracy 0.78662109375\n",
      "Iteration 9360 Training loss 0.016844727098941803 Validation loss 0.01997091807425022 Accuracy 0.794921875\n",
      "Iteration 9370 Training loss 0.01865839958190918 Validation loss 0.020519418641924858 Accuracy 0.78955078125\n",
      "Iteration 9380 Training loss 0.01772906817495823 Validation loss 0.01970367506146431 Accuracy 0.7978515625\n",
      "Iteration 9390 Training loss 0.017125368118286133 Validation loss 0.020826486870646477 Accuracy 0.78759765625\n",
      "Iteration 9400 Training loss 0.018425190821290016 Validation loss 0.019831791520118713 Accuracy 0.79736328125\n",
      "Iteration 9410 Training loss 0.017906898632645607 Validation loss 0.020858339965343475 Accuracy 0.78662109375\n",
      "Iteration 9420 Training loss 0.01639486849308014 Validation loss 0.02074403502047062 Accuracy 0.7880859375\n",
      "Iteration 9430 Training loss 0.015937915071845055 Validation loss 0.020885344594717026 Accuracy 0.78662109375\n",
      "Iteration 9440 Training loss 0.020476611331105232 Validation loss 0.019876297563314438 Accuracy 0.79638671875\n",
      "Iteration 9450 Training loss 0.01788383163511753 Validation loss 0.019601253792643547 Accuracy 0.79931640625\n",
      "Iteration 9460 Training loss 0.0184605922549963 Validation loss 0.021680055186152458 Accuracy 0.77880859375\n",
      "Iteration 9470 Training loss 0.017935074865818024 Validation loss 0.019800756126642227 Accuracy 0.79736328125\n",
      "Iteration 9480 Training loss 0.01778441295027733 Validation loss 0.02109544537961483 Accuracy 0.78369140625\n",
      "Iteration 9490 Training loss 0.01742994226515293 Validation loss 0.019674869254231453 Accuracy 0.79833984375\n",
      "Iteration 9500 Training loss 0.018666358664631844 Validation loss 0.019832324236631393 Accuracy 0.796875\n",
      "Iteration 9510 Training loss 0.02220427617430687 Validation loss 0.02217697538435459 Accuracy 0.77392578125\n",
      "Iteration 9520 Training loss 0.01910226419568062 Validation loss 0.021455636247992516 Accuracy 0.78173828125\n",
      "Iteration 9530 Training loss 0.018697915598750114 Validation loss 0.020076416432857513 Accuracy 0.7939453125\n",
      "Iteration 9540 Training loss 0.01780695654451847 Validation loss 0.021741138771176338 Accuracy 0.77783203125\n",
      "Iteration 9550 Training loss 0.020359808579087257 Validation loss 0.02033735252916813 Accuracy 0.791015625\n",
      "Iteration 9560 Training loss 0.01647556945681572 Validation loss 0.01941700652241707 Accuracy 0.80126953125\n",
      "Iteration 9570 Training loss 0.016892826184630394 Validation loss 0.019478194415569305 Accuracy 0.7998046875\n",
      "Iteration 9580 Training loss 0.017929384484887123 Validation loss 0.019910836592316628 Accuracy 0.7958984375\n",
      "Iteration 9590 Training loss 0.018898630514740944 Validation loss 0.020899245515465736 Accuracy 0.78564453125\n",
      "Iteration 9600 Training loss 0.01879434660077095 Validation loss 0.019453490152955055 Accuracy 0.80078125\n",
      "Iteration 9610 Training loss 0.01960163563489914 Validation loss 0.01933608017861843 Accuracy 0.80224609375\n",
      "Iteration 9620 Training loss 0.019744819030165672 Validation loss 0.020466124638915062 Accuracy 0.7900390625\n",
      "Iteration 9630 Training loss 0.019070053473114967 Validation loss 0.020970890298485756 Accuracy 0.78515625\n",
      "Iteration 9640 Training loss 0.017876043915748596 Validation loss 0.01993369869887829 Accuracy 0.79541015625\n",
      "Iteration 9650 Training loss 0.01712799072265625 Validation loss 0.019426686689257622 Accuracy 0.80126953125\n",
      "Iteration 9660 Training loss 0.01691288687288761 Validation loss 0.019894830882549286 Accuracy 0.796875\n",
      "Iteration 9670 Training loss 0.017892617732286453 Validation loss 0.020475640892982483 Accuracy 0.79052734375\n",
      "Iteration 9680 Training loss 0.018314389511942863 Validation loss 0.019494466483592987 Accuracy 0.80078125\n",
      "Iteration 9690 Training loss 0.0177454873919487 Validation loss 0.01928243786096573 Accuracy 0.8017578125\n",
      "Iteration 9700 Training loss 0.01980256848037243 Validation loss 0.019835643470287323 Accuracy 0.796875\n",
      "Iteration 9710 Training loss 0.020435186102986336 Validation loss 0.021783065050840378 Accuracy 0.77783203125\n",
      "Iteration 9720 Training loss 0.016193941235542297 Validation loss 0.01944672130048275 Accuracy 0.80078125\n",
      "Iteration 9730 Training loss 0.017865855246782303 Validation loss 0.01946433074772358 Accuracy 0.80078125\n",
      "Iteration 9740 Training loss 0.015822598710656166 Validation loss 0.019420647993683815 Accuracy 0.80126953125\n",
      "Iteration 9750 Training loss 0.014625894837081432 Validation loss 0.019726421684026718 Accuracy 0.7978515625\n",
      "Iteration 9760 Training loss 0.017617762088775635 Validation loss 0.019357826560735703 Accuracy 0.8017578125\n",
      "Iteration 9770 Training loss 0.017024453729391098 Validation loss 0.01923290267586708 Accuracy 0.80224609375\n",
      "Iteration 9780 Training loss 0.018639422953128815 Validation loss 0.02057894878089428 Accuracy 0.78955078125\n",
      "Iteration 9790 Training loss 0.01724919117987156 Validation loss 0.01975051686167717 Accuracy 0.79736328125\n",
      "Iteration 9800 Training loss 0.018024155870079994 Validation loss 0.019452720880508423 Accuracy 0.7998046875\n",
      "Iteration 9810 Training loss 0.014598206616938114 Validation loss 0.02079935558140278 Accuracy 0.78662109375\n",
      "Iteration 9820 Training loss 0.015976065769791603 Validation loss 0.020123090595006943 Accuracy 0.79345703125\n",
      "Iteration 9830 Training loss 0.01702173426747322 Validation loss 0.019513139501214027 Accuracy 0.7998046875\n",
      "Iteration 9840 Training loss 0.01790929026901722 Validation loss 0.019764602184295654 Accuracy 0.79736328125\n",
      "Iteration 9850 Training loss 0.017543917521834373 Validation loss 0.019498132169246674 Accuracy 0.7998046875\n",
      "Iteration 9860 Training loss 0.018206896260380745 Validation loss 0.01921425200998783 Accuracy 0.802734375\n",
      "Iteration 9870 Training loss 0.017670800909399986 Validation loss 0.019972914829850197 Accuracy 0.7958984375\n",
      "Iteration 9880 Training loss 0.0208203736692667 Validation loss 0.023435750976204872 Accuracy 0.759765625\n",
      "Iteration 9890 Training loss 0.019403720274567604 Validation loss 0.019460024312138557 Accuracy 0.80029296875\n",
      "Iteration 9900 Training loss 0.020212389528751373 Validation loss 0.019460679963231087 Accuracy 0.7998046875\n",
      "Iteration 9910 Training loss 0.015538250096142292 Validation loss 0.020079340785741806 Accuracy 0.79345703125\n",
      "Iteration 9920 Training loss 0.018667254596948624 Validation loss 0.01974157616496086 Accuracy 0.7978515625\n",
      "Iteration 9930 Training loss 0.018532155081629753 Validation loss 0.019408227875828743 Accuracy 0.80029296875\n",
      "Iteration 9940 Training loss 0.019107159227132797 Validation loss 0.019255900755524635 Accuracy 0.802734375\n",
      "Iteration 9950 Training loss 0.019315578043460846 Validation loss 0.020188653841614723 Accuracy 0.79296875\n",
      "Iteration 9960 Training loss 0.015822719782590866 Validation loss 0.0191703662276268 Accuracy 0.8037109375\n",
      "Iteration 9970 Training loss 0.016268381848931313 Validation loss 0.019786197692155838 Accuracy 0.79736328125\n",
      "Iteration 9980 Training loss 0.02072531171143055 Validation loss 0.020063426345586777 Accuracy 0.7939453125\n",
      "Iteration 9990 Training loss 0.016730522736907005 Validation loss 0.019278258085250854 Accuracy 0.80126953125\n",
      "Iteration 10000 Training loss 0.01993531733751297 Validation loss 0.02012518420815468 Accuracy 0.79248046875\n",
      "Iteration 10010 Training loss 0.020878713577985764 Validation loss 0.020316001027822495 Accuracy 0.7919921875\n",
      "Iteration 10020 Training loss 0.018675176426768303 Validation loss 0.019745493307709694 Accuracy 0.7978515625\n",
      "Iteration 10030 Training loss 0.018480196595191956 Validation loss 0.019538938999176025 Accuracy 0.80029296875\n",
      "Iteration 10040 Training loss 0.018736733123660088 Validation loss 0.019408516585826874 Accuracy 0.80078125\n",
      "Iteration 10050 Training loss 0.01774699054658413 Validation loss 0.01993601769208908 Accuracy 0.7958984375\n",
      "Iteration 10060 Training loss 0.019305473193526268 Validation loss 0.01951269432902336 Accuracy 0.80078125\n",
      "Iteration 10070 Training loss 0.018417958170175552 Validation loss 0.019859762862324715 Accuracy 0.79638671875\n",
      "Iteration 10080 Training loss 0.01708434522151947 Validation loss 0.021060211583971977 Accuracy 0.78466796875\n",
      "Iteration 10090 Training loss 0.01717841997742653 Validation loss 0.018970241770148277 Accuracy 0.80517578125\n",
      "Iteration 10100 Training loss 0.0179500263184309 Validation loss 0.019279126077890396 Accuracy 0.80224609375\n",
      "Iteration 10110 Training loss 0.016610272228717804 Validation loss 0.020082036033272743 Accuracy 0.79443359375\n",
      "Iteration 10120 Training loss 0.017145950347185135 Validation loss 0.01947970502078533 Accuracy 0.80029296875\n",
      "Iteration 10130 Training loss 0.018037224188447 Validation loss 0.019358763471245766 Accuracy 0.8017578125\n",
      "Iteration 10140 Training loss 0.02255971170961857 Validation loss 0.0213957317173481 Accuracy 0.78125\n",
      "Iteration 10150 Training loss 0.017608147114515305 Validation loss 0.019034184515476227 Accuracy 0.8046875\n",
      "Iteration 10160 Training loss 0.014996535144746304 Validation loss 0.019295262172818184 Accuracy 0.8017578125\n",
      "Iteration 10170 Training loss 0.01786603033542633 Validation loss 0.01934295892715454 Accuracy 0.80126953125\n",
      "Iteration 10180 Training loss 0.01755378395318985 Validation loss 0.019686300307512283 Accuracy 0.79833984375\n",
      "Iteration 10190 Training loss 0.017384273931384087 Validation loss 0.019118409603834152 Accuracy 0.8037109375\n",
      "Iteration 10200 Training loss 0.016212813556194305 Validation loss 0.01950869895517826 Accuracy 0.79931640625\n",
      "Iteration 10210 Training loss 0.016926856711506844 Validation loss 0.01918199099600315 Accuracy 0.80224609375\n",
      "Iteration 10220 Training loss 0.01799914985895157 Validation loss 0.019573846831917763 Accuracy 0.79931640625\n",
      "Iteration 10230 Training loss 0.019422970712184906 Validation loss 0.019525058567523956 Accuracy 0.79931640625\n",
      "Iteration 10240 Training loss 0.015944629907608032 Validation loss 0.0195353701710701 Accuracy 0.79931640625\n",
      "Iteration 10250 Training loss 0.01756981573998928 Validation loss 0.019539305940270424 Accuracy 0.7998046875\n",
      "Iteration 10260 Training loss 0.018867967650294304 Validation loss 0.01968155801296234 Accuracy 0.79931640625\n",
      "Iteration 10270 Training loss 0.018498245626688004 Validation loss 0.01932128146290779 Accuracy 0.80126953125\n",
      "Iteration 10280 Training loss 0.015109439380466938 Validation loss 0.019675571471452713 Accuracy 0.79736328125\n",
      "Iteration 10290 Training loss 0.024870721623301506 Validation loss 0.0268386397510767 Accuracy 0.72705078125\n",
      "Iteration 10300 Training loss 0.01893800124526024 Validation loss 0.01915230229496956 Accuracy 0.8037109375\n",
      "Iteration 10310 Training loss 0.01670254021883011 Validation loss 0.019324783235788345 Accuracy 0.8017578125\n",
      "Iteration 10320 Training loss 0.018446436151862144 Validation loss 0.020007580518722534 Accuracy 0.794921875\n",
      "Iteration 10330 Training loss 0.015599957667291164 Validation loss 0.019266044721007347 Accuracy 0.80224609375\n",
      "Iteration 10340 Training loss 0.020415741950273514 Validation loss 0.019397128373384476 Accuracy 0.80126953125\n",
      "Iteration 10350 Training loss 0.01861044578254223 Validation loss 0.019884610548615456 Accuracy 0.7958984375\n",
      "Iteration 10360 Training loss 0.015756715089082718 Validation loss 0.020224522799253464 Accuracy 0.79248046875\n",
      "Iteration 10370 Training loss 0.017127973958849907 Validation loss 0.020935770124197006 Accuracy 0.7861328125\n",
      "Iteration 10380 Training loss 0.01911725290119648 Validation loss 0.02146914042532444 Accuracy 0.77978515625\n",
      "Iteration 10390 Training loss 0.017604148015379906 Validation loss 0.018856346607208252 Accuracy 0.806640625\n",
      "Iteration 10400 Training loss 0.018199317157268524 Validation loss 0.01970786042511463 Accuracy 0.79736328125\n",
      "Iteration 10410 Training loss 0.01569247618317604 Validation loss 0.019451148808002472 Accuracy 0.7998046875\n",
      "Iteration 10420 Training loss 0.016818825155496597 Validation loss 0.01952175237238407 Accuracy 0.7998046875\n",
      "Iteration 10430 Training loss 0.01587350107729435 Validation loss 0.018995795398950577 Accuracy 0.8056640625\n",
      "Iteration 10440 Training loss 0.0184959564357996 Validation loss 0.019261792302131653 Accuracy 0.80322265625\n",
      "Iteration 10450 Training loss 0.01663435623049736 Validation loss 0.01920098066329956 Accuracy 0.80322265625\n",
      "Iteration 10460 Training loss 0.015983330085873604 Validation loss 0.019753633067011833 Accuracy 0.79736328125\n",
      "Iteration 10470 Training loss 0.016911586746573448 Validation loss 0.019374441355466843 Accuracy 0.80029296875\n",
      "Iteration 10480 Training loss 0.017192255705595016 Validation loss 0.019185271114110947 Accuracy 0.80322265625\n",
      "Iteration 10490 Training loss 0.01567317172884941 Validation loss 0.01940545253455639 Accuracy 0.80078125\n",
      "Iteration 10500 Training loss 0.017133677378296852 Validation loss 0.019226817414164543 Accuracy 0.80322265625\n",
      "Iteration 10510 Training loss 0.01912142150104046 Validation loss 0.019615471363067627 Accuracy 0.798828125\n",
      "Iteration 10520 Training loss 0.016600847244262695 Validation loss 0.01978563517332077 Accuracy 0.79638671875\n",
      "Iteration 10530 Training loss 0.012696768157184124 Validation loss 0.01953066512942314 Accuracy 0.7998046875\n",
      "Iteration 10540 Training loss 0.015796523541212082 Validation loss 0.01990526355803013 Accuracy 0.79541015625\n",
      "Iteration 10550 Training loss 0.015625767409801483 Validation loss 0.019680766388773918 Accuracy 0.79736328125\n",
      "Iteration 10560 Training loss 0.017381813377141953 Validation loss 0.019811686128377914 Accuracy 0.796875\n",
      "Iteration 10570 Training loss 0.016921836882829666 Validation loss 0.019146161153912544 Accuracy 0.80322265625\n",
      "Iteration 10580 Training loss 0.020052917301654816 Validation loss 0.020775234326720238 Accuracy 0.7880859375\n",
      "Iteration 10590 Training loss 0.01459770929068327 Validation loss 0.020068859681487083 Accuracy 0.7939453125\n",
      "Iteration 10600 Training loss 0.017218811437487602 Validation loss 0.019052058458328247 Accuracy 0.80419921875\n",
      "Iteration 10610 Training loss 0.01865246705710888 Validation loss 0.019544100388884544 Accuracy 0.7998046875\n",
      "Iteration 10620 Training loss 0.017856549471616745 Validation loss 0.019987083971500397 Accuracy 0.79443359375\n",
      "Iteration 10630 Training loss 0.020060617476701736 Validation loss 0.019361378625035286 Accuracy 0.80126953125\n",
      "Iteration 10640 Training loss 0.01777247153222561 Validation loss 0.0203799270093441 Accuracy 0.791015625\n",
      "Iteration 10650 Training loss 0.017489118501544 Validation loss 0.01944335736334324 Accuracy 0.7998046875\n",
      "Iteration 10660 Training loss 0.016936006024479866 Validation loss 0.02042994648218155 Accuracy 0.791015625\n",
      "Iteration 10670 Training loss 0.019688505679368973 Validation loss 0.019700780510902405 Accuracy 0.79736328125\n",
      "Iteration 10680 Training loss 0.018780160695314407 Validation loss 0.019322536885738373 Accuracy 0.8017578125\n",
      "Iteration 10690 Training loss 0.015973486006259918 Validation loss 0.01902632974088192 Accuracy 0.80419921875\n",
      "Iteration 10700 Training loss 0.017027705907821655 Validation loss 0.019408948719501495 Accuracy 0.80078125\n",
      "Iteration 10710 Training loss 0.015592320822179317 Validation loss 0.019580185413360596 Accuracy 0.79931640625\n",
      "Iteration 10720 Training loss 0.014749188907444477 Validation loss 0.019716499373316765 Accuracy 0.79736328125\n",
      "Iteration 10730 Training loss 0.016695888713002205 Validation loss 0.019252153113484383 Accuracy 0.802734375\n",
      "Iteration 10740 Training loss 0.019246093928813934 Validation loss 0.018985988572239876 Accuracy 0.8046875\n",
      "Iteration 10750 Training loss 0.018996916711330414 Validation loss 0.020628366619348526 Accuracy 0.7890625\n",
      "Iteration 10760 Training loss 0.016740664839744568 Validation loss 0.019442329183220863 Accuracy 0.80126953125\n",
      "Iteration 10770 Training loss 0.01755458489060402 Validation loss 0.018955668434500694 Accuracy 0.8046875\n",
      "Iteration 10780 Training loss 0.01777089387178421 Validation loss 0.01988648995757103 Accuracy 0.7958984375\n",
      "Iteration 10790 Training loss 0.017468875274062157 Validation loss 0.019289741292595863 Accuracy 0.80126953125\n",
      "Iteration 10800 Training loss 0.017258981242775917 Validation loss 0.01907425746321678 Accuracy 0.8037109375\n",
      "Iteration 10810 Training loss 0.016080159693956375 Validation loss 0.019236918538808823 Accuracy 0.802734375\n",
      "Iteration 10820 Training loss 0.018485797569155693 Validation loss 0.01995115727186203 Accuracy 0.7958984375\n",
      "Iteration 10830 Training loss 0.019037986174225807 Validation loss 0.01978325843811035 Accuracy 0.79541015625\n",
      "Iteration 10840 Training loss 0.014031333848834038 Validation loss 0.01922329142689705 Accuracy 0.80224609375\n",
      "Iteration 10850 Training loss 0.018455075100064278 Validation loss 0.01920267939567566 Accuracy 0.8017578125\n",
      "Iteration 10860 Training loss 0.01641135849058628 Validation loss 0.019116198644042015 Accuracy 0.80419921875\n",
      "Iteration 10870 Training loss 0.01641050912439823 Validation loss 0.01939520426094532 Accuracy 0.80078125\n",
      "Iteration 10880 Training loss 0.019042156636714935 Validation loss 0.019257333129644394 Accuracy 0.802734375\n",
      "Iteration 10890 Training loss 0.01703675091266632 Validation loss 0.01967458613216877 Accuracy 0.79833984375\n",
      "Iteration 10900 Training loss 0.01405761856585741 Validation loss 0.01949797011911869 Accuracy 0.7998046875\n",
      "Iteration 10910 Training loss 0.017682000994682312 Validation loss 0.019380131736397743 Accuracy 0.80126953125\n",
      "Iteration 10920 Training loss 0.013316141441464424 Validation loss 0.01947961188852787 Accuracy 0.7998046875\n",
      "Iteration 10930 Training loss 0.017075780779123306 Validation loss 0.018788745626807213 Accuracy 0.8076171875\n",
      "Iteration 10940 Training loss 0.01955261453986168 Validation loss 0.0190445389598608 Accuracy 0.8046875\n",
      "Iteration 10950 Training loss 0.01655837520956993 Validation loss 0.01920902542769909 Accuracy 0.80322265625\n",
      "Iteration 10960 Training loss 0.015675140544772148 Validation loss 0.02089403010904789 Accuracy 0.78662109375\n",
      "Iteration 10970 Training loss 0.01752210035920143 Validation loss 0.01931513287127018 Accuracy 0.8017578125\n",
      "Iteration 10980 Training loss 0.018221639096736908 Validation loss 0.0203991886228323 Accuracy 0.7919921875\n",
      "Iteration 10990 Training loss 0.02087813802063465 Validation loss 0.023198995739221573 Accuracy 0.76416015625\n",
      "Iteration 11000 Training loss 0.016288967803120613 Validation loss 0.019312938675284386 Accuracy 0.8017578125\n",
      "Iteration 11010 Training loss 0.01549972128123045 Validation loss 0.019526103511452675 Accuracy 0.80029296875\n",
      "Iteration 11020 Training loss 0.019322330132126808 Validation loss 0.019126281142234802 Accuracy 0.80322265625\n",
      "Iteration 11030 Training loss 0.016084136441349983 Validation loss 0.018998563289642334 Accuracy 0.80615234375\n",
      "Iteration 11040 Training loss 0.01699438877403736 Validation loss 0.019397050142288208 Accuracy 0.80078125\n",
      "Iteration 11050 Training loss 0.018962102010846138 Validation loss 0.019650476053357124 Accuracy 0.7978515625\n",
      "Iteration 11060 Training loss 0.017347341403365135 Validation loss 0.019421042874455452 Accuracy 0.7998046875\n",
      "Iteration 11070 Training loss 0.016548361629247665 Validation loss 0.0193682461977005 Accuracy 0.80078125\n",
      "Iteration 11080 Training loss 0.015867656096816063 Validation loss 0.019568519666790962 Accuracy 0.79833984375\n",
      "Iteration 11090 Training loss 0.01752907782793045 Validation loss 0.019171418622136116 Accuracy 0.80224609375\n",
      "Iteration 11100 Training loss 0.01616874523460865 Validation loss 0.019006889313459396 Accuracy 0.80517578125\n",
      "Iteration 11110 Training loss 0.020415719598531723 Validation loss 0.018669748678803444 Accuracy 0.80810546875\n",
      "Iteration 11120 Training loss 0.019485928118228912 Validation loss 0.019510148093104362 Accuracy 0.7998046875\n",
      "Iteration 11130 Training loss 0.01509378757327795 Validation loss 0.019007869064807892 Accuracy 0.8046875\n",
      "Iteration 11140 Training loss 0.016513461247086525 Validation loss 0.01918088272213936 Accuracy 0.8037109375\n",
      "Iteration 11150 Training loss 0.017033269628882408 Validation loss 0.019935360178351402 Accuracy 0.796875\n",
      "Iteration 11160 Training loss 0.01639353670179844 Validation loss 0.019634855911135674 Accuracy 0.798828125\n",
      "Iteration 11170 Training loss 0.01655144803225994 Validation loss 0.018889667466282845 Accuracy 0.80615234375\n",
      "Iteration 11180 Training loss 0.016590608283877373 Validation loss 0.019422387704253197 Accuracy 0.80029296875\n",
      "Iteration 11190 Training loss 0.02093949168920517 Validation loss 0.019812002778053284 Accuracy 0.79541015625\n",
      "Iteration 11200 Training loss 0.013415832072496414 Validation loss 0.01902903988957405 Accuracy 0.80517578125\n",
      "Iteration 11210 Training loss 0.016369732096791267 Validation loss 0.019137509167194366 Accuracy 0.80419921875\n",
      "Iteration 11220 Training loss 0.01588122732937336 Validation loss 0.019540756940841675 Accuracy 0.7998046875\n",
      "Iteration 11230 Training loss 0.019484924152493477 Validation loss 0.01899307779967785 Accuracy 0.8046875\n",
      "Iteration 11240 Training loss 0.01733790896832943 Validation loss 0.018864983692765236 Accuracy 0.8056640625\n",
      "Iteration 11250 Training loss 0.016177136451005936 Validation loss 0.01877153292298317 Accuracy 0.80712890625\n",
      "Iteration 11260 Training loss 0.013047341257333755 Validation loss 0.01917962357401848 Accuracy 0.802734375\n",
      "Iteration 11270 Training loss 0.015201589092612267 Validation loss 0.01906314119696617 Accuracy 0.80322265625\n",
      "Iteration 11280 Training loss 0.017147734761238098 Validation loss 0.019248835742473602 Accuracy 0.8017578125\n",
      "Iteration 11290 Training loss 0.015617633238434792 Validation loss 0.019156619906425476 Accuracy 0.80322265625\n",
      "Iteration 11300 Training loss 0.01586667262017727 Validation loss 0.019254952669143677 Accuracy 0.8017578125\n",
      "Iteration 11310 Training loss 0.01598214916884899 Validation loss 0.019225645810365677 Accuracy 0.80224609375\n",
      "Iteration 11320 Training loss 0.017189135774970055 Validation loss 0.01900475099682808 Accuracy 0.80419921875\n",
      "Iteration 11330 Training loss 0.018844593316316605 Validation loss 0.019959252327680588 Accuracy 0.79541015625\n",
      "Iteration 11340 Training loss 0.016949234530329704 Validation loss 0.019108669832348824 Accuracy 0.8037109375\n",
      "Iteration 11350 Training loss 0.018481263890862465 Validation loss 0.018702073022723198 Accuracy 0.8076171875\n",
      "Iteration 11360 Training loss 0.02260185219347477 Validation loss 0.020872605964541435 Accuracy 0.78564453125\n",
      "Iteration 11370 Training loss 0.016078991815447807 Validation loss 0.01881127990782261 Accuracy 0.806640625\n",
      "Iteration 11380 Training loss 0.019877588376402855 Validation loss 0.019923407584428787 Accuracy 0.7958984375\n",
      "Iteration 11390 Training loss 0.01847863756120205 Validation loss 0.018884483724832535 Accuracy 0.8056640625\n",
      "Iteration 11400 Training loss 0.014756014570593834 Validation loss 0.018709855154156685 Accuracy 0.80810546875\n",
      "Iteration 11410 Training loss 0.017988057807087898 Validation loss 0.019014891237020493 Accuracy 0.80517578125\n",
      "Iteration 11420 Training loss 0.01672234758734703 Validation loss 0.018890375271439552 Accuracy 0.8056640625\n",
      "Iteration 11430 Training loss 0.01652885414659977 Validation loss 0.018856890499591827 Accuracy 0.8056640625\n",
      "Iteration 11440 Training loss 0.018199708312749863 Validation loss 0.019278710708022118 Accuracy 0.8017578125\n",
      "Iteration 11450 Training loss 0.01668773591518402 Validation loss 0.019611943513154984 Accuracy 0.79833984375\n",
      "Iteration 11460 Training loss 0.018052106723189354 Validation loss 0.018875043839216232 Accuracy 0.80615234375\n",
      "Iteration 11470 Training loss 0.020095720887184143 Validation loss 0.02053925208747387 Accuracy 0.78955078125\n",
      "Iteration 11480 Training loss 0.01625349372625351 Validation loss 0.019033562391996384 Accuracy 0.80419921875\n",
      "Iteration 11490 Training loss 0.018466049805283546 Validation loss 0.020032357424497604 Accuracy 0.79443359375\n",
      "Iteration 11500 Training loss 0.016674522310495377 Validation loss 0.019166437909007072 Accuracy 0.80322265625\n",
      "Iteration 11510 Training loss 0.014070598408579826 Validation loss 0.018930448219180107 Accuracy 0.80517578125\n",
      "Iteration 11520 Training loss 0.01580420881509781 Validation loss 0.018890641629695892 Accuracy 0.80517578125\n",
      "Iteration 11530 Training loss 0.016671955585479736 Validation loss 0.019382033497095108 Accuracy 0.80029296875\n",
      "Iteration 11540 Training loss 0.019578730687499046 Validation loss 0.01986413449048996 Accuracy 0.79541015625\n",
      "Iteration 11550 Training loss 0.0157410129904747 Validation loss 0.019225606694817543 Accuracy 0.80224609375\n",
      "Iteration 11560 Training loss 0.015443655662238598 Validation loss 0.01923125609755516 Accuracy 0.80224609375\n",
      "Iteration 11570 Training loss 0.018458882346749306 Validation loss 0.019878821447491646 Accuracy 0.79638671875\n",
      "Iteration 11580 Training loss 0.016137229278683662 Validation loss 0.019339824095368385 Accuracy 0.80126953125\n",
      "Iteration 11590 Training loss 0.014291430823504925 Validation loss 0.019076241180300713 Accuracy 0.802734375\n",
      "Iteration 11600 Training loss 0.01896396465599537 Validation loss 0.02025717869400978 Accuracy 0.79296875\n",
      "Iteration 11610 Training loss 0.015293828211724758 Validation loss 0.018927814438939095 Accuracy 0.80615234375\n",
      "Iteration 11620 Training loss 0.015051581896841526 Validation loss 0.018959227949380875 Accuracy 0.80615234375\n",
      "Iteration 11630 Training loss 0.016376730054616928 Validation loss 0.019679659977555275 Accuracy 0.79833984375\n",
      "Iteration 11640 Training loss 0.01808128133416176 Validation loss 0.018991859629750252 Accuracy 0.80419921875\n",
      "Iteration 11650 Training loss 0.018884776160120964 Validation loss 0.01945531740784645 Accuracy 0.7998046875\n",
      "Iteration 11660 Training loss 0.01519758440554142 Validation loss 0.01885291188955307 Accuracy 0.8046875\n",
      "Iteration 11670 Training loss 0.01551747415214777 Validation loss 0.02033834718167782 Accuracy 0.79150390625\n",
      "Iteration 11680 Training loss 0.01596435159444809 Validation loss 0.019192537292838097 Accuracy 0.8017578125\n",
      "Iteration 11690 Training loss 0.01692468672990799 Validation loss 0.01995108835399151 Accuracy 0.79443359375\n",
      "Iteration 11700 Training loss 0.01774645783007145 Validation loss 0.019748693332076073 Accuracy 0.796875\n",
      "Iteration 11710 Training loss 0.017093565315008163 Validation loss 0.019321149215102196 Accuracy 0.80126953125\n",
      "Iteration 11720 Training loss 0.018206441774964333 Validation loss 0.019297299906611443 Accuracy 0.802734375\n",
      "Iteration 11730 Training loss 0.016064560040831566 Validation loss 0.01917239837348461 Accuracy 0.80419921875\n",
      "Iteration 11740 Training loss 0.017194753512740135 Validation loss 0.01861945539712906 Accuracy 0.80908203125\n",
      "Iteration 11750 Training loss 0.01554478146135807 Validation loss 0.019328352063894272 Accuracy 0.80029296875\n",
      "Iteration 11760 Training loss 0.015561078675091267 Validation loss 0.019124969840049744 Accuracy 0.80322265625\n",
      "Iteration 11770 Training loss 0.018524976447224617 Validation loss 0.01898115500807762 Accuracy 0.8046875\n",
      "Iteration 11780 Training loss 0.01581263542175293 Validation loss 0.01869904436171055 Accuracy 0.8076171875\n",
      "Iteration 11790 Training loss 0.016401465982198715 Validation loss 0.019352586939930916 Accuracy 0.80078125\n",
      "Iteration 11800 Training loss 0.014739476144313812 Validation loss 0.01927519403398037 Accuracy 0.8017578125\n",
      "Iteration 11810 Training loss 0.0156302098184824 Validation loss 0.01935766451060772 Accuracy 0.80078125\n",
      "Iteration 11820 Training loss 0.01619757153093815 Validation loss 0.018782351166009903 Accuracy 0.806640625\n",
      "Iteration 11830 Training loss 0.017441468313336372 Validation loss 0.019033662974834442 Accuracy 0.80419921875\n",
      "Iteration 11840 Training loss 0.02043784223496914 Validation loss 0.01897379569709301 Accuracy 0.8046875\n",
      "Iteration 11850 Training loss 0.017381705343723297 Validation loss 0.020395753905177116 Accuracy 0.79150390625\n",
      "Iteration 11860 Training loss 0.0181315615773201 Validation loss 0.019037971273064613 Accuracy 0.80419921875\n",
      "Iteration 11870 Training loss 0.016847483813762665 Validation loss 0.01901852898299694 Accuracy 0.80517578125\n",
      "Iteration 11880 Training loss 0.018133552744984627 Validation loss 0.019550535827875137 Accuracy 0.7998046875\n",
      "Iteration 11890 Training loss 0.014912881888449192 Validation loss 0.018987229093909264 Accuracy 0.8037109375\n",
      "Iteration 11900 Training loss 0.014587732963263988 Validation loss 0.019287750124931335 Accuracy 0.8017578125\n",
      "Iteration 11910 Training loss 0.016495641320943832 Validation loss 0.019081346690654755 Accuracy 0.80419921875\n",
      "Iteration 11920 Training loss 0.015503793954849243 Validation loss 0.0200611911714077 Accuracy 0.79296875\n",
      "Iteration 11930 Training loss 0.016712281852960587 Validation loss 0.01921766810119152 Accuracy 0.80126953125\n",
      "Iteration 11940 Training loss 0.015025581233203411 Validation loss 0.018762987107038498 Accuracy 0.80712890625\n",
      "Iteration 11950 Training loss 0.016804512590169907 Validation loss 0.01891457289457321 Accuracy 0.80517578125\n",
      "Iteration 11960 Training loss 0.016472412273287773 Validation loss 0.018889809027314186 Accuracy 0.8056640625\n",
      "Iteration 11970 Training loss 0.018141621723771095 Validation loss 0.01905881054699421 Accuracy 0.80419921875\n",
      "Iteration 11980 Training loss 0.018943417817354202 Validation loss 0.018974894657731056 Accuracy 0.80517578125\n",
      "Iteration 11990 Training loss 0.01830288954079151 Validation loss 0.019246995449066162 Accuracy 0.80322265625\n",
      "Iteration 12000 Training loss 0.017315737903118134 Validation loss 0.019176801666617393 Accuracy 0.8017578125\n",
      "Iteration 12010 Training loss 0.01632680743932724 Validation loss 0.018992243334650993 Accuracy 0.8037109375\n",
      "Iteration 12020 Training loss 0.019277451559901237 Validation loss 0.019661039113998413 Accuracy 0.7978515625\n",
      "Iteration 12030 Training loss 0.014997283928096294 Validation loss 0.01844244822859764 Accuracy 0.81005859375\n",
      "Iteration 12040 Training loss 0.01837490126490593 Validation loss 0.018785642459988594 Accuracy 0.8056640625\n",
      "Iteration 12050 Training loss 0.014767097309231758 Validation loss 0.018734591081738472 Accuracy 0.80712890625\n",
      "Iteration 12060 Training loss 0.015712643042206764 Validation loss 0.018956545740365982 Accuracy 0.80517578125\n",
      "Iteration 12070 Training loss 0.01634032651782036 Validation loss 0.01890505850315094 Accuracy 0.806640625\n",
      "Iteration 12080 Training loss 0.014822385273873806 Validation loss 0.019052905961871147 Accuracy 0.80419921875\n",
      "Iteration 12090 Training loss 0.017878934741020203 Validation loss 0.018968364223837852 Accuracy 0.80517578125\n",
      "Iteration 12100 Training loss 0.01769719459116459 Validation loss 0.01892508566379547 Accuracy 0.8046875\n",
      "Iteration 12110 Training loss 0.015890026465058327 Validation loss 0.019091736525297165 Accuracy 0.802734375\n",
      "Iteration 12120 Training loss 0.012853463180363178 Validation loss 0.01909632608294487 Accuracy 0.80322265625\n",
      "Iteration 12130 Training loss 0.015301528386771679 Validation loss 0.020004916936159134 Accuracy 0.79443359375\n",
      "Iteration 12140 Training loss 0.017393426969647408 Validation loss 0.019626840949058533 Accuracy 0.79833984375\n",
      "Iteration 12150 Training loss 0.014649108983576298 Validation loss 0.01874413713812828 Accuracy 0.80810546875\n",
      "Iteration 12160 Training loss 0.015498371794819832 Validation loss 0.018937626853585243 Accuracy 0.80517578125\n",
      "Iteration 12170 Training loss 0.01759394258260727 Validation loss 0.019528286531567574 Accuracy 0.79931640625\n",
      "Iteration 12180 Training loss 0.017637303099036217 Validation loss 0.018977336585521698 Accuracy 0.8046875\n",
      "Iteration 12190 Training loss 0.01735331118106842 Validation loss 0.018667226657271385 Accuracy 0.80810546875\n",
      "Iteration 12200 Training loss 0.01845472864806652 Validation loss 0.019674818962812424 Accuracy 0.7978515625\n",
      "Iteration 12210 Training loss 0.015749836340546608 Validation loss 0.018853839486837387 Accuracy 0.806640625\n",
      "Iteration 12220 Training loss 0.015326235443353653 Validation loss 0.019137291237711906 Accuracy 0.80322265625\n",
      "Iteration 12230 Training loss 0.015702642500400543 Validation loss 0.018987232819199562 Accuracy 0.8056640625\n",
      "Iteration 12240 Training loss 0.01495231781154871 Validation loss 0.018756940960884094 Accuracy 0.806640625\n",
      "Iteration 12250 Training loss 0.02117742970585823 Validation loss 0.01897892728447914 Accuracy 0.8037109375\n",
      "Iteration 12260 Training loss 0.017022959887981415 Validation loss 0.018710697069764137 Accuracy 0.80712890625\n",
      "Iteration 12270 Training loss 0.017509382218122482 Validation loss 0.019131362438201904 Accuracy 0.802734375\n",
      "Iteration 12280 Training loss 0.01515077706426382 Validation loss 0.01923222839832306 Accuracy 0.80126953125\n",
      "Iteration 12290 Training loss 0.018237732350826263 Validation loss 0.018964603543281555 Accuracy 0.80517578125\n",
      "Iteration 12300 Training loss 0.016856443136930466 Validation loss 0.018771832808852196 Accuracy 0.8076171875\n",
      "Iteration 12310 Training loss 0.018235519528388977 Validation loss 0.019059013575315475 Accuracy 0.80419921875\n",
      "Iteration 12320 Training loss 0.017244936898350716 Validation loss 0.019150981679558754 Accuracy 0.80322265625\n",
      "Iteration 12330 Training loss 0.017191242426633835 Validation loss 0.019018199294805527 Accuracy 0.8046875\n",
      "Iteration 12340 Training loss 0.017366452142596245 Validation loss 0.01893913559615612 Accuracy 0.80517578125\n",
      "Iteration 12350 Training loss 0.014103055000305176 Validation loss 0.01883726939558983 Accuracy 0.806640625\n",
      "Iteration 12360 Training loss 0.016085902228951454 Validation loss 0.019089404493570328 Accuracy 0.8046875\n",
      "Iteration 12370 Training loss 0.014380858279764652 Validation loss 0.019010724499821663 Accuracy 0.8046875\n",
      "Iteration 12380 Training loss 0.01281630527228117 Validation loss 0.019110407680273056 Accuracy 0.8046875\n",
      "Iteration 12390 Training loss 0.015960106626152992 Validation loss 0.020255232229828835 Accuracy 0.79248046875\n",
      "Iteration 12400 Training loss 0.01565876044332981 Validation loss 0.01937939040362835 Accuracy 0.80078125\n",
      "Iteration 12410 Training loss 0.016110029071569443 Validation loss 0.018904339522123337 Accuracy 0.8056640625\n",
      "Iteration 12420 Training loss 0.017377212643623352 Validation loss 0.01929723657667637 Accuracy 0.80224609375\n",
      "Iteration 12430 Training loss 0.01646154187619686 Validation loss 0.01861540786921978 Accuracy 0.80908203125\n",
      "Iteration 12440 Training loss 0.015557807870209217 Validation loss 0.018453752622008324 Accuracy 0.810546875\n",
      "Iteration 12450 Training loss 0.018476752564311028 Validation loss 0.01896478980779648 Accuracy 0.8046875\n",
      "Iteration 12460 Training loss 0.019222361966967583 Validation loss 0.018939141184091568 Accuracy 0.80517578125\n",
      "Iteration 12470 Training loss 0.01450270600616932 Validation loss 0.018958818167448044 Accuracy 0.80419921875\n",
      "Iteration 12480 Training loss 0.01649964042007923 Validation loss 0.018545180559158325 Accuracy 0.8095703125\n",
      "Iteration 12490 Training loss 0.01586175337433815 Validation loss 0.01870667189359665 Accuracy 0.80615234375\n",
      "Iteration 12500 Training loss 0.013331093825399876 Validation loss 0.018975187093019485 Accuracy 0.8046875\n",
      "Iteration 12510 Training loss 0.015091652981936932 Validation loss 0.019690409302711487 Accuracy 0.79833984375\n",
      "Iteration 12520 Training loss 0.016573650762438774 Validation loss 0.01831621490418911 Accuracy 0.81103515625\n",
      "Iteration 12530 Training loss 0.016296232119202614 Validation loss 0.018777789548039436 Accuracy 0.80615234375\n",
      "Iteration 12540 Training loss 0.018388040363788605 Validation loss 0.01848602294921875 Accuracy 0.8095703125\n",
      "Iteration 12550 Training loss 0.016348106786608696 Validation loss 0.018952567130327225 Accuracy 0.80322265625\n",
      "Iteration 12560 Training loss 0.018468813970685005 Validation loss 0.01961517333984375 Accuracy 0.79736328125\n",
      "Iteration 12570 Training loss 0.014949322678148746 Validation loss 0.018743284046649933 Accuracy 0.8056640625\n",
      "Iteration 12580 Training loss 0.01707957126200199 Validation loss 0.019590994343161583 Accuracy 0.79833984375\n",
      "Iteration 12590 Training loss 0.016642460599541664 Validation loss 0.018751440569758415 Accuracy 0.80712890625\n",
      "Iteration 12600 Training loss 0.01666734740138054 Validation loss 0.01885570026934147 Accuracy 0.80712890625\n",
      "Iteration 12610 Training loss 0.014808962121605873 Validation loss 0.018594535067677498 Accuracy 0.8076171875\n",
      "Iteration 12620 Training loss 0.01654234156012535 Validation loss 0.018925746902823448 Accuracy 0.80419921875\n",
      "Iteration 12630 Training loss 0.018335215747356415 Validation loss 0.01843925565481186 Accuracy 0.8095703125\n",
      "Iteration 12640 Training loss 0.018737606704235077 Validation loss 0.019238295033574104 Accuracy 0.8017578125\n",
      "Iteration 12650 Training loss 0.01787818782031536 Validation loss 0.018874621018767357 Accuracy 0.80517578125\n",
      "Iteration 12660 Training loss 0.018294962123036385 Validation loss 0.019266659393906593 Accuracy 0.80126953125\n",
      "Iteration 12670 Training loss 0.0173883568495512 Validation loss 0.019602132961153984 Accuracy 0.7978515625\n",
      "Iteration 12680 Training loss 0.017988627776503563 Validation loss 0.019577516242861748 Accuracy 0.7978515625\n",
      "Iteration 12690 Training loss 0.017427757382392883 Validation loss 0.018841952085494995 Accuracy 0.8056640625\n",
      "Iteration 12700 Training loss 0.01434249710291624 Validation loss 0.018720535561442375 Accuracy 0.8076171875\n",
      "Iteration 12710 Training loss 0.016362905502319336 Validation loss 0.018727390095591545 Accuracy 0.80712890625\n",
      "Iteration 12720 Training loss 0.014394573867321014 Validation loss 0.018584182485938072 Accuracy 0.80908203125\n",
      "Iteration 12730 Training loss 0.01501868199557066 Validation loss 0.018954738974571228 Accuracy 0.80419921875\n",
      "Iteration 12740 Training loss 0.01571366935968399 Validation loss 0.018745746463537216 Accuracy 0.80712890625\n",
      "Iteration 12750 Training loss 0.015107415616512299 Validation loss 0.018793698400259018 Accuracy 0.8056640625\n",
      "Iteration 12760 Training loss 0.013517135754227638 Validation loss 0.018707670271396637 Accuracy 0.80615234375\n",
      "Iteration 12770 Training loss 0.01708320714533329 Validation loss 0.019556129351258278 Accuracy 0.798828125\n",
      "Iteration 12780 Training loss 0.016909008845686913 Validation loss 0.018905578181147575 Accuracy 0.80615234375\n",
      "Iteration 12790 Training loss 0.013690761290490627 Validation loss 0.018905162811279297 Accuracy 0.80615234375\n",
      "Iteration 12800 Training loss 0.01427381206303835 Validation loss 0.01852688379585743 Accuracy 0.8095703125\n",
      "Iteration 12810 Training loss 0.01772174797952175 Validation loss 0.01899479702115059 Accuracy 0.8046875\n",
      "Iteration 12820 Training loss 0.018249517306685448 Validation loss 0.01899007335305214 Accuracy 0.8046875\n",
      "Iteration 12830 Training loss 0.015784693881869316 Validation loss 0.018694883212447166 Accuracy 0.80810546875\n",
      "Iteration 12840 Training loss 0.015676017850637436 Validation loss 0.018431102856993675 Accuracy 0.81005859375\n",
      "Iteration 12850 Training loss 0.01736716739833355 Validation loss 0.01850145123898983 Accuracy 0.80908203125\n",
      "Iteration 12860 Training loss 0.02066030353307724 Validation loss 0.01972450688481331 Accuracy 0.7958984375\n",
      "Iteration 12870 Training loss 0.01631971448659897 Validation loss 0.01899918168783188 Accuracy 0.8037109375\n",
      "Iteration 12880 Training loss 0.015923112630844116 Validation loss 0.019032157957553864 Accuracy 0.80322265625\n",
      "Iteration 12890 Training loss 0.01925630122423172 Validation loss 0.019111357629299164 Accuracy 0.80322265625\n",
      "Iteration 12900 Training loss 0.0163718294352293 Validation loss 0.0188551414757967 Accuracy 0.80517578125\n",
      "Iteration 12910 Training loss 0.01942463591694832 Validation loss 0.019726771861314774 Accuracy 0.79736328125\n",
      "Iteration 12920 Training loss 0.014025884680449963 Validation loss 0.01904181018471718 Accuracy 0.80419921875\n",
      "Iteration 12930 Training loss 0.016672076657414436 Validation loss 0.018560728058218956 Accuracy 0.80810546875\n",
      "Iteration 12940 Training loss 0.01668175496160984 Validation loss 0.01856847107410431 Accuracy 0.80712890625\n",
      "Iteration 12950 Training loss 0.0135949170216918 Validation loss 0.019065529108047485 Accuracy 0.80322265625\n",
      "Iteration 12960 Training loss 0.01374827604740858 Validation loss 0.01889747381210327 Accuracy 0.80419921875\n",
      "Iteration 12970 Training loss 0.017223022878170013 Validation loss 0.01953713409602642 Accuracy 0.7978515625\n",
      "Iteration 12980 Training loss 0.015409957617521286 Validation loss 0.01942463405430317 Accuracy 0.7998046875\n",
      "Iteration 12990 Training loss 0.01568642631173134 Validation loss 0.01887470670044422 Accuracy 0.8046875\n",
      "Iteration 13000 Training loss 0.017520802095532417 Validation loss 0.018838876858353615 Accuracy 0.80517578125\n",
      "Iteration 13010 Training loss 0.014026926830410957 Validation loss 0.018715379759669304 Accuracy 0.806640625\n",
      "Iteration 13020 Training loss 0.014062142930924892 Validation loss 0.018815500661730766 Accuracy 0.80712890625\n",
      "Iteration 13030 Training loss 0.013959714211523533 Validation loss 0.018566595390439034 Accuracy 0.80859375\n",
      "Iteration 13040 Training loss 0.014665690250694752 Validation loss 0.01914331316947937 Accuracy 0.8037109375\n",
      "Iteration 13050 Training loss 0.016405818983912468 Validation loss 0.018680226057767868 Accuracy 0.80810546875\n",
      "Iteration 13060 Training loss 0.014370208606123924 Validation loss 0.018893815577030182 Accuracy 0.80419921875\n",
      "Iteration 13070 Training loss 0.017990674823522568 Validation loss 0.018923116847872734 Accuracy 0.8037109375\n",
      "Iteration 13080 Training loss 0.016379840672016144 Validation loss 0.018537182360887527 Accuracy 0.80859375\n",
      "Iteration 13090 Training loss 0.015004641376435757 Validation loss 0.01868295669555664 Accuracy 0.80810546875\n",
      "Iteration 13100 Training loss 0.015699047595262527 Validation loss 0.018794022500514984 Accuracy 0.8056640625\n",
      "Iteration 13110 Training loss 0.0161670483648777 Validation loss 0.018627025187015533 Accuracy 0.8076171875\n",
      "Iteration 13120 Training loss 0.01745646260678768 Validation loss 0.01879294402897358 Accuracy 0.8056640625\n",
      "Iteration 13130 Training loss 0.013534445315599442 Validation loss 0.018450958654284477 Accuracy 0.80908203125\n",
      "Iteration 13140 Training loss 0.016773724928498268 Validation loss 0.018854239955544472 Accuracy 0.8037109375\n",
      "Iteration 13150 Training loss 0.016500147059559822 Validation loss 0.01869514398276806 Accuracy 0.80712890625\n",
      "Iteration 13160 Training loss 0.017394106835126877 Validation loss 0.018524926155805588 Accuracy 0.80908203125\n",
      "Iteration 13170 Training loss 0.017586389556527138 Validation loss 0.01955721713602543 Accuracy 0.79736328125\n",
      "Iteration 13180 Training loss 0.017683615908026695 Validation loss 0.018828140571713448 Accuracy 0.8056640625\n",
      "Iteration 13190 Training loss 0.015501691028475761 Validation loss 0.01872045174241066 Accuracy 0.8056640625\n",
      "Iteration 13200 Training loss 0.019230496138334274 Validation loss 0.019318940117955208 Accuracy 0.8017578125\n",
      "Iteration 13210 Training loss 0.02018718607723713 Validation loss 0.019442692399024963 Accuracy 0.80029296875\n",
      "Iteration 13220 Training loss 0.016663312911987305 Validation loss 0.019013117998838425 Accuracy 0.80419921875\n",
      "Iteration 13230 Training loss 0.016411345452070236 Validation loss 0.01884370669722557 Accuracy 0.806640625\n",
      "Iteration 13240 Training loss 0.018692374229431152 Validation loss 0.019108224660158157 Accuracy 0.802734375\n",
      "Iteration 13250 Training loss 0.014481175690889359 Validation loss 0.018675094470381737 Accuracy 0.8076171875\n",
      "Iteration 13260 Training loss 0.01925336942076683 Validation loss 0.018425431102514267 Accuracy 0.8095703125\n",
      "Iteration 13270 Training loss 0.019906604662537575 Validation loss 0.019717292860150337 Accuracy 0.7978515625\n",
      "Iteration 13280 Training loss 0.01661224663257599 Validation loss 0.01897607557475567 Accuracy 0.80419921875\n",
      "Iteration 13290 Training loss 0.017111051827669144 Validation loss 0.018437987193465233 Accuracy 0.81005859375\n",
      "Iteration 13300 Training loss 0.016006002202630043 Validation loss 0.018866868689656258 Accuracy 0.80517578125\n",
      "Iteration 13310 Training loss 0.014460115693509579 Validation loss 0.018283652141690254 Accuracy 0.8115234375\n",
      "Iteration 13320 Training loss 0.0150214284658432 Validation loss 0.019117828458547592 Accuracy 0.802734375\n",
      "Iteration 13330 Training loss 0.01668880134820938 Validation loss 0.018868666142225266 Accuracy 0.80419921875\n",
      "Iteration 13340 Training loss 0.015364769846200943 Validation loss 0.018779562786221504 Accuracy 0.80517578125\n",
      "Iteration 13350 Training loss 0.016415180638432503 Validation loss 0.019189627841114998 Accuracy 0.8017578125\n",
      "Iteration 13360 Training loss 0.013906191103160381 Validation loss 0.01864372007548809 Accuracy 0.80810546875\n",
      "Iteration 13370 Training loss 0.014977316372096539 Validation loss 0.018596995621919632 Accuracy 0.8076171875\n",
      "Iteration 13380 Training loss 0.017422489821910858 Validation loss 0.01853039488196373 Accuracy 0.8076171875\n",
      "Iteration 13390 Training loss 0.01441926322877407 Validation loss 0.01827274262905121 Accuracy 0.81103515625\n",
      "Iteration 13400 Training loss 0.01759110577404499 Validation loss 0.018603673204779625 Accuracy 0.8076171875\n",
      "Iteration 13410 Training loss 0.015415745787322521 Validation loss 0.018489260226488113 Accuracy 0.80859375\n",
      "Iteration 13420 Training loss 0.016791507601737976 Validation loss 0.018819527700543404 Accuracy 0.8056640625\n",
      "Iteration 13430 Training loss 0.015316522680222988 Validation loss 0.018946362659335136 Accuracy 0.8046875\n",
      "Iteration 13440 Training loss 0.014647398144006729 Validation loss 0.018584439530968666 Accuracy 0.8076171875\n",
      "Iteration 13450 Training loss 0.017183659598231316 Validation loss 0.019358297809958458 Accuracy 0.798828125\n",
      "Iteration 13460 Training loss 0.01523649599403143 Validation loss 0.01878480426967144 Accuracy 0.80615234375\n",
      "Iteration 13470 Training loss 0.01436314545571804 Validation loss 0.018413253128528595 Accuracy 0.81005859375\n",
      "Iteration 13480 Training loss 0.014857240952551365 Validation loss 0.018621258437633514 Accuracy 0.80712890625\n",
      "Iteration 13490 Training loss 0.012752347625792027 Validation loss 0.018741395324468613 Accuracy 0.8056640625\n",
      "Iteration 13500 Training loss 0.0164723489433527 Validation loss 0.0184037983417511 Accuracy 0.8095703125\n",
      "Iteration 13510 Training loss 0.02000049129128456 Validation loss 0.01891011744737625 Accuracy 0.80322265625\n",
      "Iteration 13520 Training loss 0.015680935233831406 Validation loss 0.01818283647298813 Accuracy 0.81103515625\n",
      "Iteration 13530 Training loss 0.016353605315089226 Validation loss 0.01839986816048622 Accuracy 0.8095703125\n",
      "Iteration 13540 Training loss 0.01362114679068327 Validation loss 0.018698837608098984 Accuracy 0.80615234375\n",
      "Iteration 13550 Training loss 0.012263988144695759 Validation loss 0.018440265208482742 Accuracy 0.810546875\n",
      "Iteration 13560 Training loss 0.016285637393593788 Validation loss 0.018588555976748466 Accuracy 0.8076171875\n",
      "Iteration 13570 Training loss 0.016626780852675438 Validation loss 0.018443433567881584 Accuracy 0.80859375\n",
      "Iteration 13580 Training loss 0.013534177094697952 Validation loss 0.018489720299839973 Accuracy 0.80810546875\n",
      "Iteration 13590 Training loss 0.014129010029137135 Validation loss 0.018194444477558136 Accuracy 0.81103515625\n",
      "Iteration 13600 Training loss 0.01578105427324772 Validation loss 0.01872534118592739 Accuracy 0.80517578125\n",
      "Iteration 13610 Training loss 0.016209792345762253 Validation loss 0.01867685467004776 Accuracy 0.80615234375\n",
      "Iteration 13620 Training loss 0.016713207587599754 Validation loss 0.01831366866827011 Accuracy 0.810546875\n",
      "Iteration 13630 Training loss 0.01654166541993618 Validation loss 0.018834685906767845 Accuracy 0.8037109375\n",
      "Iteration 13640 Training loss 0.01436953991651535 Validation loss 0.01858573593199253 Accuracy 0.80810546875\n",
      "Iteration 13650 Training loss 0.016490301117300987 Validation loss 0.01850230246782303 Accuracy 0.80859375\n",
      "Iteration 13660 Training loss 0.01773110032081604 Validation loss 0.019018063321709633 Accuracy 0.80322265625\n",
      "Iteration 13670 Training loss 0.016465120017528534 Validation loss 0.019403226673603058 Accuracy 0.798828125\n",
      "Iteration 13680 Training loss 0.016803346574306488 Validation loss 0.018636727705597878 Accuracy 0.8076171875\n",
      "Iteration 13690 Training loss 0.015230707824230194 Validation loss 0.018586572259664536 Accuracy 0.80810546875\n",
      "Iteration 13700 Training loss 0.01637698896229267 Validation loss 0.019733134657144547 Accuracy 0.79736328125\n",
      "Iteration 13710 Training loss 0.015869975090026855 Validation loss 0.01852625235915184 Accuracy 0.80859375\n",
      "Iteration 13720 Training loss 0.0187546256929636 Validation loss 0.01992400363087654 Accuracy 0.79443359375\n",
      "Iteration 13730 Training loss 0.017314068973064423 Validation loss 0.018756888806819916 Accuracy 0.8056640625\n",
      "Iteration 13740 Training loss 0.016823623329401016 Validation loss 0.018672989681363106 Accuracy 0.80712890625\n",
      "Iteration 13750 Training loss 0.01613643206655979 Validation loss 0.01847204752266407 Accuracy 0.8095703125\n",
      "Iteration 13760 Training loss 0.017346220090985298 Validation loss 0.018643591552972794 Accuracy 0.80712890625\n",
      "Iteration 13770 Training loss 0.015959784388542175 Validation loss 0.01880570687353611 Accuracy 0.80615234375\n",
      "Iteration 13780 Training loss 0.01779499463737011 Validation loss 0.018670879304409027 Accuracy 0.80615234375\n",
      "Iteration 13790 Training loss 0.014200927689671516 Validation loss 0.01863757334649563 Accuracy 0.806640625\n",
      "Iteration 13800 Training loss 0.015797242522239685 Validation loss 0.018935687839984894 Accuracy 0.80419921875\n",
      "Iteration 13810 Training loss 0.015002377331256866 Validation loss 0.018546951934695244 Accuracy 0.80859375\n",
      "Iteration 13820 Training loss 0.014326670207083225 Validation loss 0.01869778148829937 Accuracy 0.80615234375\n",
      "Iteration 13830 Training loss 0.016470085829496384 Validation loss 0.01842041127383709 Accuracy 0.81005859375\n",
      "Iteration 13840 Training loss 0.015101665630936623 Validation loss 0.018459906801581383 Accuracy 0.80810546875\n",
      "Iteration 13850 Training loss 0.015412088483572006 Validation loss 0.0185107234865427 Accuracy 0.80859375\n",
      "Iteration 13860 Training loss 0.016969729214906693 Validation loss 0.018521703779697418 Accuracy 0.80859375\n",
      "Iteration 13870 Training loss 0.014716732315719128 Validation loss 0.018694385886192322 Accuracy 0.80712890625\n",
      "Iteration 13880 Training loss 0.017758948728442192 Validation loss 0.018740326166152954 Accuracy 0.8056640625\n",
      "Iteration 13890 Training loss 0.015952302142977715 Validation loss 0.018529493361711502 Accuracy 0.80908203125\n",
      "Iteration 13900 Training loss 0.019297994673252106 Validation loss 0.018661413341760635 Accuracy 0.80712890625\n",
      "Iteration 13910 Training loss 0.016652705147862434 Validation loss 0.018378395587205887 Accuracy 0.80908203125\n",
      "Iteration 13920 Training loss 0.016807029023766518 Validation loss 0.01841118186712265 Accuracy 0.81005859375\n",
      "Iteration 13930 Training loss 0.01760607212781906 Validation loss 0.01839854009449482 Accuracy 0.81005859375\n",
      "Iteration 13940 Training loss 0.015362787991762161 Validation loss 0.018360160291194916 Accuracy 0.8095703125\n",
      "Iteration 13950 Training loss 0.013484136201441288 Validation loss 0.01839120127260685 Accuracy 0.8095703125\n",
      "Iteration 13960 Training loss 0.0156979039311409 Validation loss 0.018495140597224236 Accuracy 0.80810546875\n",
      "Iteration 13970 Training loss 0.016927585005760193 Validation loss 0.019358597695827484 Accuracy 0.7998046875\n",
      "Iteration 13980 Training loss 0.014789922162890434 Validation loss 0.019326385110616684 Accuracy 0.798828125\n",
      "Iteration 13990 Training loss 0.018493741750717163 Validation loss 0.019015345722436905 Accuracy 0.802734375\n",
      "Iteration 14000 Training loss 0.017146853730082512 Validation loss 0.019108377397060394 Accuracy 0.8037109375\n",
      "Iteration 14010 Training loss 0.015042525716125965 Validation loss 0.018634630367159843 Accuracy 0.80712890625\n",
      "Iteration 14020 Training loss 0.014638606458902359 Validation loss 0.018689002841711044 Accuracy 0.806640625\n",
      "Iteration 14030 Training loss 0.01599322259426117 Validation loss 0.01834707520902157 Accuracy 0.8095703125\n",
      "Iteration 14040 Training loss 0.015781601890921593 Validation loss 0.019521109759807587 Accuracy 0.79833984375\n",
      "Iteration 14050 Training loss 0.018572745844721794 Validation loss 0.01847890391945839 Accuracy 0.80859375\n",
      "Iteration 14060 Training loss 0.014468975365161896 Validation loss 0.01862333156168461 Accuracy 0.806640625\n",
      "Iteration 14070 Training loss 0.0157243050634861 Validation loss 0.01889663003385067 Accuracy 0.802734375\n",
      "Iteration 14080 Training loss 0.01439580786973238 Validation loss 0.018926825374364853 Accuracy 0.80322265625\n",
      "Iteration 14090 Training loss 0.014562695287168026 Validation loss 0.018510473892092705 Accuracy 0.8076171875\n",
      "Iteration 14100 Training loss 0.016077451407909393 Validation loss 0.018785575404763222 Accuracy 0.8046875\n",
      "Iteration 14110 Training loss 0.013928454369306564 Validation loss 0.018488457426428795 Accuracy 0.80810546875\n",
      "Iteration 14120 Training loss 0.012959431856870651 Validation loss 0.018564406782388687 Accuracy 0.8076171875\n",
      "Iteration 14130 Training loss 0.01547912321984768 Validation loss 0.01863142102956772 Accuracy 0.806640625\n",
      "Iteration 14140 Training loss 0.017700886353850365 Validation loss 0.018652236089110374 Accuracy 0.80615234375\n",
      "Iteration 14150 Training loss 0.013529034331440926 Validation loss 0.018383346498012543 Accuracy 0.80908203125\n",
      "Iteration 14160 Training loss 0.014973591081798077 Validation loss 0.018632791936397552 Accuracy 0.80712890625\n",
      "Iteration 14170 Training loss 0.012576424516737461 Validation loss 0.018486903980374336 Accuracy 0.80908203125\n",
      "Iteration 14180 Training loss 0.012893694452941418 Validation loss 0.018356474116444588 Accuracy 0.8095703125\n",
      "Iteration 14190 Training loss 0.016360612586140633 Validation loss 0.01879573054611683 Accuracy 0.8046875\n",
      "Iteration 14200 Training loss 0.014117174781858921 Validation loss 0.018005361780524254 Accuracy 0.8134765625\n",
      "Iteration 14210 Training loss 0.015215090475976467 Validation loss 0.0184367373585701 Accuracy 0.80810546875\n",
      "Iteration 14220 Training loss 0.01605016365647316 Validation loss 0.019131137058138847 Accuracy 0.8017578125\n",
      "Iteration 14230 Training loss 0.014560192823410034 Validation loss 0.01904805190861225 Accuracy 0.802734375\n",
      "Iteration 14240 Training loss 0.018755266442894936 Validation loss 0.01933952420949936 Accuracy 0.7998046875\n",
      "Iteration 14250 Training loss 0.013271176256239414 Validation loss 0.018447093665599823 Accuracy 0.80859375\n",
      "Iteration 14260 Training loss 0.014421643689274788 Validation loss 0.018482357263565063 Accuracy 0.80810546875\n",
      "Iteration 14270 Training loss 0.01721552386879921 Validation loss 0.018120737746357918 Accuracy 0.8125\n",
      "Iteration 14280 Training loss 0.01563955470919609 Validation loss 0.018666110932826996 Accuracy 0.806640625\n",
      "Iteration 14290 Training loss 0.017333708703517914 Validation loss 0.018345976248383522 Accuracy 0.8095703125\n",
      "Iteration 14300 Training loss 0.016509991139173508 Validation loss 0.018703032284975052 Accuracy 0.80615234375\n",
      "Iteration 14310 Training loss 0.016309354454278946 Validation loss 0.018739525228738785 Accuracy 0.8056640625\n",
      "Iteration 14320 Training loss 0.017002349719405174 Validation loss 0.018160507082939148 Accuracy 0.8125\n",
      "Iteration 14330 Training loss 0.016066523268818855 Validation loss 0.018622487783432007 Accuracy 0.80712890625\n",
      "Iteration 14340 Training loss 0.014841010794043541 Validation loss 0.018285058438777924 Accuracy 0.81103515625\n",
      "Iteration 14350 Training loss 0.01595268025994301 Validation loss 0.018444128334522247 Accuracy 0.80810546875\n",
      "Iteration 14360 Training loss 0.014564508572220802 Validation loss 0.01863083243370056 Accuracy 0.806640625\n",
      "Iteration 14370 Training loss 0.015435920096933842 Validation loss 0.018297895789146423 Accuracy 0.81005859375\n",
      "Iteration 14380 Training loss 0.016655806452035904 Validation loss 0.01811337284743786 Accuracy 0.8115234375\n",
      "Iteration 14390 Training loss 0.014962023124098778 Validation loss 0.018060065805912018 Accuracy 0.8125\n",
      "Iteration 14400 Training loss 0.015105511993169785 Validation loss 0.01826884225010872 Accuracy 0.810546875\n",
      "Iteration 14410 Training loss 0.012575769796967506 Validation loss 0.019060563296079636 Accuracy 0.8017578125\n",
      "Iteration 14420 Training loss 0.014781690202653408 Validation loss 0.018429776653647423 Accuracy 0.81005859375\n",
      "Iteration 14430 Training loss 0.01567974127829075 Validation loss 0.0185557771474123 Accuracy 0.80810546875\n",
      "Iteration 14440 Training loss 0.016713600605726242 Validation loss 0.01868453249335289 Accuracy 0.80712890625\n",
      "Iteration 14450 Training loss 0.018167296424508095 Validation loss 0.018253348767757416 Accuracy 0.81005859375\n",
      "Iteration 14460 Training loss 0.014897636137902737 Validation loss 0.018277497962117195 Accuracy 0.80908203125\n",
      "Iteration 14470 Training loss 0.014433883130550385 Validation loss 0.01860233023762703 Accuracy 0.80712890625\n",
      "Iteration 14480 Training loss 0.016116760671138763 Validation loss 0.01845964603126049 Accuracy 0.80859375\n",
      "Iteration 14490 Training loss 0.018249046057462692 Validation loss 0.01829397864639759 Accuracy 0.81005859375\n",
      "Iteration 14500 Training loss 0.01699681766331196 Validation loss 0.018303420394659042 Accuracy 0.810546875\n",
      "Iteration 14510 Training loss 0.013047505170106888 Validation loss 0.018434759229421616 Accuracy 0.80908203125\n",
      "Iteration 14520 Training loss 0.015212373808026314 Validation loss 0.01861557550728321 Accuracy 0.80712890625\n",
      "Iteration 14530 Training loss 0.01488039642572403 Validation loss 0.01829581893980503 Accuracy 0.81005859375\n",
      "Iteration 14540 Training loss 0.018544290214776993 Validation loss 0.019039997830986977 Accuracy 0.802734375\n",
      "Iteration 14550 Training loss 0.016803249716758728 Validation loss 0.01832122541964054 Accuracy 0.81005859375\n",
      "Iteration 14560 Training loss 0.015242879278957844 Validation loss 0.018409280106425285 Accuracy 0.80859375\n",
      "Iteration 14570 Training loss 0.014840856194496155 Validation loss 0.01831831783056259 Accuracy 0.8095703125\n",
      "Iteration 14580 Training loss 0.016471482813358307 Validation loss 0.018808653578162193 Accuracy 0.80419921875\n",
      "Iteration 14590 Training loss 0.014268750324845314 Validation loss 0.01819480024278164 Accuracy 0.81103515625\n",
      "Iteration 14600 Training loss 0.016110597178339958 Validation loss 0.01844807155430317 Accuracy 0.8076171875\n",
      "Iteration 14610 Training loss 0.018064089119434357 Validation loss 0.020094096660614014 Accuracy 0.79150390625\n",
      "Iteration 14620 Training loss 0.01646418683230877 Validation loss 0.018464788794517517 Accuracy 0.80859375\n",
      "Iteration 14630 Training loss 0.01569165103137493 Validation loss 0.01802739128470421 Accuracy 0.8125\n",
      "Iteration 14640 Training loss 0.010912188328802586 Validation loss 0.014726641587913036 Accuracy 0.845703125\n",
      "Iteration 14650 Training loss 0.010517633520066738 Validation loss 0.014506022445857525 Accuracy 0.84814453125\n",
      "Iteration 14660 Training loss 0.012877007946372032 Validation loss 0.015610819682478905 Accuracy 0.83740234375\n",
      "Iteration 14670 Training loss 0.009510007686913013 Validation loss 0.014740981161594391 Accuracy 0.84619140625\n",
      "Iteration 14680 Training loss 0.012846323661506176 Validation loss 0.014133657328784466 Accuracy 0.8525390625\n",
      "Iteration 14690 Training loss 0.010261187329888344 Validation loss 0.014883366413414478 Accuracy 0.8447265625\n",
      "Iteration 14700 Training loss 0.011169067583978176 Validation loss 0.014142709784209728 Accuracy 0.8525390625\n",
      "Iteration 14710 Training loss 0.008866585791110992 Validation loss 0.01371508277952671 Accuracy 0.8544921875\n",
      "Iteration 14720 Training loss 0.011911292560398579 Validation loss 0.01375108864158392 Accuracy 0.857421875\n",
      "Iteration 14730 Training loss 0.012028200551867485 Validation loss 0.013522477820515633 Accuracy 0.85888671875\n",
      "Iteration 14740 Training loss 0.010760308243334293 Validation loss 0.013510571792721748 Accuracy 0.85888671875\n",
      "Iteration 14750 Training loss 0.01401557121425867 Validation loss 0.015424384735524654 Accuracy 0.83837890625\n",
      "Iteration 14760 Training loss 0.011701817624270916 Validation loss 0.014534229412674904 Accuracy 0.84716796875\n",
      "Iteration 14770 Training loss 0.01020811963826418 Validation loss 0.014247670769691467 Accuracy 0.8515625\n",
      "Iteration 14780 Training loss 0.010956293903291225 Validation loss 0.013388622552156448 Accuracy 0.86083984375\n",
      "Iteration 14790 Training loss 0.009049015119671822 Validation loss 0.013299201615154743 Accuracy 0.861328125\n",
      "Iteration 14800 Training loss 0.009298079647123814 Validation loss 0.013126823119819164 Accuracy 0.86328125\n",
      "Iteration 14810 Training loss 0.011429807171225548 Validation loss 0.014179423451423645 Accuracy 0.8525390625\n",
      "Iteration 14820 Training loss 0.011617721058428288 Validation loss 0.013869289308786392 Accuracy 0.85498046875\n",
      "Iteration 14830 Training loss 0.011694811284542084 Validation loss 0.014235073700547218 Accuracy 0.85107421875\n",
      "Iteration 14840 Training loss 0.011726335622370243 Validation loss 0.014009950682520866 Accuracy 0.853515625\n",
      "Iteration 14850 Training loss 0.01175269391387701 Validation loss 0.013547985814511776 Accuracy 0.8583984375\n",
      "Iteration 14860 Training loss 0.009136194363236427 Validation loss 0.01296147145330906 Accuracy 0.865234375\n",
      "Iteration 14870 Training loss 0.010298590175807476 Validation loss 0.014062987640500069 Accuracy 0.8525390625\n",
      "Iteration 14880 Training loss 0.010818498209118843 Validation loss 0.013688365928828716 Accuracy 0.85693359375\n",
      "Iteration 14890 Training loss 0.01174042746424675 Validation loss 0.01372595690190792 Accuracy 0.857421875\n",
      "Iteration 14900 Training loss 0.011778337880969048 Validation loss 0.014428644441068172 Accuracy 0.849609375\n",
      "Iteration 14910 Training loss 0.010950678028166294 Validation loss 0.013868283480405807 Accuracy 0.85595703125\n",
      "Iteration 14920 Training loss 0.008451879024505615 Validation loss 0.013625108636915684 Accuracy 0.85791015625\n",
      "Iteration 14930 Training loss 0.011644861660897732 Validation loss 0.01356505136936903 Accuracy 0.859375\n",
      "Iteration 14940 Training loss 0.009832755662500858 Validation loss 0.013300632126629353 Accuracy 0.861328125\n",
      "Iteration 14950 Training loss 0.008285013027489185 Validation loss 0.013698039576411247 Accuracy 0.857421875\n",
      "Iteration 14960 Training loss 0.01066703163087368 Validation loss 0.01420533936470747 Accuracy 0.85205078125\n",
      "Iteration 14970 Training loss 0.008930251933634281 Validation loss 0.013482457958161831 Accuracy 0.85986328125\n",
      "Iteration 14980 Training loss 0.010162466205656528 Validation loss 0.013689755462110043 Accuracy 0.85791015625\n",
      "Iteration 14990 Training loss 0.007794716861099005 Validation loss 0.012977750040590763 Accuracy 0.86572265625\n",
      "Iteration 15000 Training loss 0.010708709247410297 Validation loss 0.01311374083161354 Accuracy 0.86376953125\n",
      "Iteration 15010 Training loss 0.009220589883625507 Validation loss 0.01335434801876545 Accuracy 0.86083984375\n",
      "Iteration 15020 Training loss 0.009859371930360794 Validation loss 0.013312953524291515 Accuracy 0.861328125\n",
      "Iteration 15030 Training loss 0.010782492347061634 Validation loss 0.013285180553793907 Accuracy 0.86181640625\n",
      "Iteration 15040 Training loss 0.011038286611437798 Validation loss 0.012864772230386734 Accuracy 0.8662109375\n",
      "Iteration 15050 Training loss 0.009377744980156422 Validation loss 0.012679245322942734 Accuracy 0.8681640625\n",
      "Iteration 15060 Training loss 0.007932690903544426 Validation loss 0.012795394286513329 Accuracy 0.8662109375\n",
      "Iteration 15070 Training loss 0.00888761319220066 Validation loss 0.01301631424576044 Accuracy 0.8642578125\n",
      "Iteration 15080 Training loss 0.009489121846854687 Validation loss 0.013086005114018917 Accuracy 0.8642578125\n",
      "Iteration 15090 Training loss 0.010592013597488403 Validation loss 0.012981530278921127 Accuracy 0.865234375\n",
      "Iteration 15100 Training loss 0.010883376002311707 Validation loss 0.014378041960299015 Accuracy 0.84912109375\n",
      "Iteration 15110 Training loss 0.011893551796674728 Validation loss 0.013234131969511509 Accuracy 0.86279296875\n",
      "Iteration 15120 Training loss 0.013321688398718834 Validation loss 0.017167173326015472 Accuracy 0.822265625\n",
      "Iteration 15130 Training loss 0.012225860729813576 Validation loss 0.01487198006361723 Accuracy 0.84521484375\n",
      "Iteration 15140 Training loss 0.009443646296858788 Validation loss 0.013164984993636608 Accuracy 0.86328125\n",
      "Iteration 15150 Training loss 0.01130435150116682 Validation loss 0.013126139529049397 Accuracy 0.8623046875\n",
      "Iteration 15160 Training loss 0.011256901547312737 Validation loss 0.013051887974143028 Accuracy 0.86376953125\n",
      "Iteration 15170 Training loss 0.011159513145685196 Validation loss 0.014256244525313377 Accuracy 0.8525390625\n",
      "Iteration 15180 Training loss 0.013602552935481071 Validation loss 0.01575990580022335 Accuracy 0.83544921875\n",
      "Iteration 15190 Training loss 0.009647564962506294 Validation loss 0.012965505011379719 Accuracy 0.8642578125\n",
      "Iteration 15200 Training loss 0.009072056040167809 Validation loss 0.01329717505723238 Accuracy 0.86083984375\n",
      "Iteration 15210 Training loss 0.008868159726262093 Validation loss 0.013511992059648037 Accuracy 0.85791015625\n",
      "Iteration 15220 Training loss 0.013794147409498692 Validation loss 0.015714704990386963 Accuracy 0.8369140625\n",
      "Iteration 15230 Training loss 0.009213426150381565 Validation loss 0.013171089813113213 Accuracy 0.86181640625\n",
      "Iteration 15240 Training loss 0.009686785750091076 Validation loss 0.012925783172249794 Accuracy 0.86474609375\n",
      "Iteration 15250 Training loss 0.009710525162518024 Validation loss 0.012887179851531982 Accuracy 0.865234375\n",
      "Iteration 15260 Training loss 0.012509406544268131 Validation loss 0.014966132119297981 Accuracy 0.84326171875\n",
      "Iteration 15270 Training loss 0.009718041867017746 Validation loss 0.013415313325822353 Accuracy 0.85986328125\n",
      "Iteration 15280 Training loss 0.008435082621872425 Validation loss 0.01290966011583805 Accuracy 0.86572265625\n",
      "Iteration 15290 Training loss 0.009978200308978558 Validation loss 0.012890475802123547 Accuracy 0.86572265625\n",
      "Iteration 15300 Training loss 0.00804896280169487 Validation loss 0.012986703775823116 Accuracy 0.8642578125\n",
      "Iteration 15310 Training loss 0.00911190640181303 Validation loss 0.013949920423328876 Accuracy 0.85400390625\n",
      "Iteration 15320 Training loss 0.01074172742664814 Validation loss 0.013224294409155846 Accuracy 0.861328125\n",
      "Iteration 15330 Training loss 0.009128909558057785 Validation loss 0.013699489645659924 Accuracy 0.857421875\n",
      "Iteration 15340 Training loss 0.007241987157613039 Validation loss 0.013157021254301071 Accuracy 0.8623046875\n",
      "Iteration 15350 Training loss 0.009636270813643932 Validation loss 0.012730513699352741 Accuracy 0.8671875\n",
      "Iteration 15360 Training loss 0.00842283759266138 Validation loss 0.012739906087517738 Accuracy 0.8662109375\n",
      "Iteration 15370 Training loss 0.009574447758495808 Validation loss 0.012592155486345291 Accuracy 0.8681640625\n",
      "Iteration 15380 Training loss 0.010986607521772385 Validation loss 0.01345137134194374 Accuracy 0.85986328125\n",
      "Iteration 15390 Training loss 0.008344556204974651 Validation loss 0.013171061873435974 Accuracy 0.86279296875\n",
      "Iteration 15400 Training loss 0.010377734899520874 Validation loss 0.013465077616274357 Accuracy 0.85986328125\n",
      "Iteration 15410 Training loss 0.009664642624557018 Validation loss 0.013357043266296387 Accuracy 0.85986328125\n",
      "Iteration 15420 Training loss 0.009547661989927292 Validation loss 0.012660844251513481 Accuracy 0.8681640625\n",
      "Iteration 15430 Training loss 0.010995735414326191 Validation loss 0.012876858003437519 Accuracy 0.865234375\n",
      "Iteration 15440 Training loss 0.011383275501430035 Validation loss 0.013673617504537106 Accuracy 0.8564453125\n",
      "Iteration 15450 Training loss 0.010420218110084534 Validation loss 0.01327365543693304 Accuracy 0.8623046875\n",
      "Iteration 15460 Training loss 0.010574573650956154 Validation loss 0.01309913583099842 Accuracy 0.86328125\n",
      "Iteration 15470 Training loss 0.007695936597883701 Validation loss 0.012806104496121407 Accuracy 0.865234375\n",
      "Iteration 15480 Training loss 0.009850955568253994 Validation loss 0.012858648784458637 Accuracy 0.865234375\n",
      "Iteration 15490 Training loss 0.007194054313004017 Validation loss 0.012488010339438915 Accuracy 0.86962890625\n",
      "Iteration 15500 Training loss 0.007604797836393118 Validation loss 0.01256110891699791 Accuracy 0.8681640625\n",
      "Iteration 15510 Training loss 0.008996589109301567 Validation loss 0.012991823256015778 Accuracy 0.8642578125\n",
      "Iteration 15520 Training loss 0.009684378281235695 Validation loss 0.013310873880982399 Accuracy 0.8603515625\n",
      "Iteration 15530 Training loss 0.00918505247682333 Validation loss 0.013212003745138645 Accuracy 0.861328125\n",
      "Iteration 15540 Training loss 0.008557593449950218 Validation loss 0.013133120723068714 Accuracy 0.86328125\n",
      "Iteration 15550 Training loss 0.009751803241670132 Validation loss 0.012926936149597168 Accuracy 0.86572265625\n",
      "Iteration 15560 Training loss 0.00824015587568283 Validation loss 0.012577560730278492 Accuracy 0.869140625\n",
      "Iteration 15570 Training loss 0.00950371753424406 Validation loss 0.014023425988852978 Accuracy 0.8544921875\n",
      "Iteration 15580 Training loss 0.008735926821827888 Validation loss 0.012834088876843452 Accuracy 0.8662109375\n",
      "Iteration 15590 Training loss 0.009622820653021336 Validation loss 0.012656408362090588 Accuracy 0.8681640625\n",
      "Iteration 15600 Training loss 0.012811682187020779 Validation loss 0.015666987746953964 Accuracy 0.83642578125\n",
      "Iteration 15610 Training loss 0.009475312195718288 Validation loss 0.013207084499299526 Accuracy 0.86181640625\n",
      "Iteration 15620 Training loss 0.008423499763011932 Validation loss 0.013004770502448082 Accuracy 0.86474609375\n",
      "Iteration 15630 Training loss 0.00759802246466279 Validation loss 0.01283557340502739 Accuracy 0.86572265625\n",
      "Iteration 15640 Training loss 0.008718674071133137 Validation loss 0.013191452249884605 Accuracy 0.86279296875\n",
      "Iteration 15650 Training loss 0.00991471204906702 Validation loss 0.013715161010622978 Accuracy 0.8564453125\n",
      "Iteration 15660 Training loss 0.008391711860895157 Validation loss 0.013452745974063873 Accuracy 0.85986328125\n",
      "Iteration 15670 Training loss 0.008755099959671497 Validation loss 0.012810208834707737 Accuracy 0.8662109375\n",
      "Iteration 15680 Training loss 0.010849326848983765 Validation loss 0.01273118332028389 Accuracy 0.8671875\n",
      "Iteration 15690 Training loss 0.010114212520420551 Validation loss 0.01285307202488184 Accuracy 0.865234375\n",
      "Iteration 15700 Training loss 0.009661003015935421 Validation loss 0.013028646819293499 Accuracy 0.8623046875\n",
      "Iteration 15710 Training loss 0.008906549774110317 Validation loss 0.012913540005683899 Accuracy 0.8642578125\n",
      "Iteration 15720 Training loss 0.012105347588658333 Validation loss 0.01529866922646761 Accuracy 0.83984375\n",
      "Iteration 15730 Training loss 0.00880027562379837 Validation loss 0.012308643199503422 Accuracy 0.87158203125\n",
      "Iteration 15740 Training loss 0.008352442644536495 Validation loss 0.012976394034922123 Accuracy 0.86376953125\n",
      "Iteration 15750 Training loss 0.011778129264712334 Validation loss 0.013670983724296093 Accuracy 0.85693359375\n",
      "Iteration 15760 Training loss 0.00729397451505065 Validation loss 0.012551247142255306 Accuracy 0.86865234375\n",
      "Iteration 15770 Training loss 0.006911237724125385 Validation loss 0.01280741672962904 Accuracy 0.8662109375\n",
      "Iteration 15780 Training loss 0.008304670453071594 Validation loss 0.012840420007705688 Accuracy 0.86572265625\n",
      "Iteration 15790 Training loss 0.006739966105669737 Validation loss 0.013017436489462852 Accuracy 0.8642578125\n",
      "Iteration 15800 Training loss 0.008376228623092175 Validation loss 0.012818015180528164 Accuracy 0.86669921875\n",
      "Iteration 15810 Training loss 0.007938949391245842 Validation loss 0.012120477855205536 Accuracy 0.87353515625\n",
      "Iteration 15820 Training loss 0.008257615379989147 Validation loss 0.01285752933472395 Accuracy 0.865234375\n",
      "Iteration 15830 Training loss 0.008238697424530983 Validation loss 0.01223689690232277 Accuracy 0.8720703125\n",
      "Iteration 15840 Training loss 0.0075705512426793575 Validation loss 0.01340350229293108 Accuracy 0.859375\n",
      "Iteration 15850 Training loss 0.01029575802385807 Validation loss 0.013268161565065384 Accuracy 0.861328125\n",
      "Iteration 15860 Training loss 0.00966846477240324 Validation loss 0.012162513099610806 Accuracy 0.87353515625\n",
      "Iteration 15870 Training loss 0.009936577640473843 Validation loss 0.012546162120997906 Accuracy 0.8681640625\n",
      "Iteration 15880 Training loss 0.007531074341386557 Validation loss 0.012554384768009186 Accuracy 0.8681640625\n",
      "Iteration 15890 Training loss 0.011631464585661888 Validation loss 0.013005864806473255 Accuracy 0.86474609375\n",
      "Iteration 15900 Training loss 0.010156561620533466 Validation loss 0.012288479134440422 Accuracy 0.87109375\n",
      "Iteration 15910 Training loss 0.008197205141186714 Validation loss 0.012582577764987946 Accuracy 0.8681640625\n",
      "Iteration 15920 Training loss 0.007098095957189798 Validation loss 0.012577297165989876 Accuracy 0.869140625\n",
      "Iteration 15930 Training loss 0.008987155742943287 Validation loss 0.012039633467793465 Accuracy 0.875\n",
      "Iteration 15940 Training loss 0.009244582615792751 Validation loss 0.012287970632314682 Accuracy 0.8720703125\n",
      "Iteration 15950 Training loss 0.008584403432905674 Validation loss 0.012977343052625656 Accuracy 0.86474609375\n",
      "Iteration 15960 Training loss 0.010066743940114975 Validation loss 0.012825684621930122 Accuracy 0.865234375\n",
      "Iteration 15970 Training loss 0.010216385126113892 Validation loss 0.01358543150126934 Accuracy 0.85986328125\n",
      "Iteration 15980 Training loss 0.008565082214772701 Validation loss 0.012793922796845436 Accuracy 0.86669921875\n",
      "Iteration 15990 Training loss 0.010324256494641304 Validation loss 0.012443935498595238 Accuracy 0.86962890625\n",
      "Iteration 16000 Training loss 0.008029273711144924 Validation loss 0.012547746300697327 Accuracy 0.86962890625\n",
      "Iteration 16010 Training loss 0.009192746132612228 Validation loss 0.012244799174368382 Accuracy 0.87109375\n",
      "Iteration 16020 Training loss 0.0073545933701097965 Validation loss 0.01274361927062273 Accuracy 0.86865234375\n",
      "Iteration 16030 Training loss 0.010984476655721664 Validation loss 0.013141484931111336 Accuracy 0.8623046875\n",
      "Iteration 16040 Training loss 0.006456204690039158 Validation loss 0.012257883325219154 Accuracy 0.8720703125\n",
      "Iteration 16050 Training loss 0.0092574842274189 Validation loss 0.012928777374327183 Accuracy 0.865234375\n",
      "Iteration 16060 Training loss 0.009759387001395226 Validation loss 0.013192879036068916 Accuracy 0.861328125\n",
      "Iteration 16070 Training loss 0.008007718250155449 Validation loss 0.012605089694261551 Accuracy 0.86669921875\n",
      "Iteration 16080 Training loss 0.0076903486624360085 Validation loss 0.012093329802155495 Accuracy 0.87255859375\n",
      "Iteration 16090 Training loss 0.008426693268120289 Validation loss 0.012036618776619434 Accuracy 0.8740234375\n",
      "Iteration 16100 Training loss 0.007469514850527048 Validation loss 0.012490151450037956 Accuracy 0.869140625\n",
      "Iteration 16110 Training loss 0.009312246926128864 Validation loss 0.013179682195186615 Accuracy 0.861328125\n",
      "Iteration 16120 Training loss 0.007299148943275213 Validation loss 0.013479425571858883 Accuracy 0.8583984375\n",
      "Iteration 16130 Training loss 0.011877697892487049 Validation loss 0.013123217038810253 Accuracy 0.86328125\n",
      "Iteration 16140 Training loss 0.008694848045706749 Validation loss 0.012479116208851337 Accuracy 0.8701171875\n",
      "Iteration 16150 Training loss 0.007498151157051325 Validation loss 0.012544454075396061 Accuracy 0.86865234375\n",
      "Iteration 16160 Training loss 0.0066994475200772285 Validation loss 0.012790456414222717 Accuracy 0.8662109375\n",
      "Iteration 16170 Training loss 0.005987110082060099 Validation loss 0.01246443297713995 Accuracy 0.86962890625\n",
      "Iteration 16180 Training loss 0.010190349072217941 Validation loss 0.01257797610014677 Accuracy 0.869140625\n",
      "Iteration 16190 Training loss 0.009119792841374874 Validation loss 0.012318495661020279 Accuracy 0.87109375\n",
      "Iteration 16200 Training loss 0.007119846064597368 Validation loss 0.01219567097723484 Accuracy 0.8720703125\n",
      "Iteration 16210 Training loss 0.010338831692934036 Validation loss 0.01243955735117197 Accuracy 0.8701171875\n",
      "Iteration 16220 Training loss 0.010989496484398842 Validation loss 0.01328026968985796 Accuracy 0.86083984375\n",
      "Iteration 16230 Training loss 0.00917789340019226 Validation loss 0.013183292001485825 Accuracy 0.861328125\n",
      "Iteration 16240 Training loss 0.008644004352390766 Validation loss 0.012627423740923405 Accuracy 0.86767578125\n",
      "Iteration 16250 Training loss 0.008577785454690456 Validation loss 0.012972009368240833 Accuracy 0.8642578125\n",
      "Iteration 16260 Training loss 0.009298074059188366 Validation loss 0.012552578002214432 Accuracy 0.8681640625\n",
      "Iteration 16270 Training loss 0.008389213122427464 Validation loss 0.012147168628871441 Accuracy 0.8720703125\n",
      "Iteration 16280 Training loss 0.010803950019180775 Validation loss 0.012634553015232086 Accuracy 0.8681640625\n",
      "Iteration 16290 Training loss 0.011316019110381603 Validation loss 0.012863132171332836 Accuracy 0.8662109375\n",
      "Iteration 16300 Training loss 0.007554051000624895 Validation loss 0.012290474958717823 Accuracy 0.87158203125\n",
      "Iteration 16310 Training loss 0.010149591602385044 Validation loss 0.012742002494633198 Accuracy 0.8662109375\n",
      "Iteration 16320 Training loss 0.010271706618368626 Validation loss 0.013129918836057186 Accuracy 0.86181640625\n",
      "Iteration 16330 Training loss 0.007210333365947008 Validation loss 0.01258821040391922 Accuracy 0.86865234375\n",
      "Iteration 16340 Training loss 0.007438191212713718 Validation loss 0.012342612259089947 Accuracy 0.87109375\n",
      "Iteration 16350 Training loss 0.010466252453625202 Validation loss 0.013237688690423965 Accuracy 0.86083984375\n",
      "Iteration 16360 Training loss 0.007485757581889629 Validation loss 0.012406840920448303 Accuracy 0.86962890625\n",
      "Iteration 16370 Training loss 0.00943309348076582 Validation loss 0.013300793245434761 Accuracy 0.861328125\n",
      "Iteration 16380 Training loss 0.009948907420039177 Validation loss 0.012855501845479012 Accuracy 0.86572265625\n",
      "Iteration 16390 Training loss 0.009419601410627365 Validation loss 0.012661966495215893 Accuracy 0.86767578125\n",
      "Iteration 16400 Training loss 0.008154837414622307 Validation loss 0.012276223860681057 Accuracy 0.87158203125\n",
      "Iteration 16410 Training loss 0.007297352887690067 Validation loss 0.012199175544083118 Accuracy 0.87255859375\n",
      "Iteration 16420 Training loss 0.00941611547023058 Validation loss 0.01228010468184948 Accuracy 0.87060546875\n",
      "Iteration 16430 Training loss 0.008527428843080997 Validation loss 0.012334347702562809 Accuracy 0.8701171875\n",
      "Iteration 16440 Training loss 0.010806003585457802 Validation loss 0.012852320447564125 Accuracy 0.86572265625\n",
      "Iteration 16450 Training loss 0.007931315340101719 Validation loss 0.012019484303891659 Accuracy 0.8740234375\n",
      "Iteration 16460 Training loss 0.009325521998107433 Validation loss 0.012388289906084538 Accuracy 0.87060546875\n",
      "Iteration 16470 Training loss 0.009360849857330322 Validation loss 0.012094123288989067 Accuracy 0.87353515625\n",
      "Iteration 16480 Training loss 0.00856911763548851 Validation loss 0.01256994716823101 Accuracy 0.8671875\n",
      "Iteration 16490 Training loss 0.008562438189983368 Validation loss 0.012606486678123474 Accuracy 0.86767578125\n",
      "Iteration 16500 Training loss 0.009840044192969799 Validation loss 0.0124092698097229 Accuracy 0.87060546875\n",
      "Iteration 16510 Training loss 0.011333869770169258 Validation loss 0.0125571945682168 Accuracy 0.869140625\n",
      "Iteration 16520 Training loss 0.008615516126155853 Validation loss 0.012946883216500282 Accuracy 0.86328125\n",
      "Iteration 16530 Training loss 0.00822633970528841 Validation loss 0.01325264573097229 Accuracy 0.86083984375\n",
      "Iteration 16540 Training loss 0.007586903870105743 Validation loss 0.012363173067569733 Accuracy 0.8701171875\n",
      "Iteration 16550 Training loss 0.008038287051022053 Validation loss 0.01214252132922411 Accuracy 0.87353515625\n",
      "Iteration 16560 Training loss 0.012347614392638206 Validation loss 0.014448351226747036 Accuracy 0.84912109375\n",
      "Iteration 16570 Training loss 0.010543017648160458 Validation loss 0.012897985056042671 Accuracy 0.8642578125\n",
      "Iteration 16580 Training loss 0.00900206994265318 Validation loss 0.012048550881445408 Accuracy 0.87353515625\n",
      "Iteration 16590 Training loss 0.010732121765613556 Validation loss 0.012203981168568134 Accuracy 0.8720703125\n",
      "Iteration 16600 Training loss 0.009327670559287071 Validation loss 0.01231563463807106 Accuracy 0.87158203125\n",
      "Iteration 16610 Training loss 0.008072937838733196 Validation loss 0.012235412374138832 Accuracy 0.87158203125\n",
      "Iteration 16620 Training loss 0.009663554839789867 Validation loss 0.01268539298325777 Accuracy 0.86669921875\n",
      "Iteration 16630 Training loss 0.008121966384351254 Validation loss 0.01249082013964653 Accuracy 0.86962890625\n",
      "Iteration 16640 Training loss 0.008841491304337978 Validation loss 0.012287761084735394 Accuracy 0.8720703125\n",
      "Iteration 16650 Training loss 0.008307745680212975 Validation loss 0.012113512493669987 Accuracy 0.87255859375\n",
      "Iteration 16660 Training loss 0.010361053049564362 Validation loss 0.012875927612185478 Accuracy 0.86572265625\n",
      "Iteration 16670 Training loss 0.00785602442920208 Validation loss 0.011963929980993271 Accuracy 0.87451171875\n",
      "Iteration 16680 Training loss 0.008656112477183342 Validation loss 0.012411614879965782 Accuracy 0.87109375\n",
      "Iteration 16690 Training loss 0.007889355532824993 Validation loss 0.01236882247030735 Accuracy 0.86962890625\n",
      "Iteration 16700 Training loss 0.0076001472771167755 Validation loss 0.01233151089400053 Accuracy 0.87060546875\n",
      "Iteration 16710 Training loss 0.006917861755937338 Validation loss 0.01249720435589552 Accuracy 0.869140625\n",
      "Iteration 16720 Training loss 0.007949942722916603 Validation loss 0.012258455157279968 Accuracy 0.87158203125\n",
      "Iteration 16730 Training loss 0.010192982852458954 Validation loss 0.012461886741220951 Accuracy 0.86865234375\n",
      "Iteration 16740 Training loss 0.007943804375827312 Validation loss 0.012453261762857437 Accuracy 0.87060546875\n",
      "Iteration 16750 Training loss 0.009096014313399792 Validation loss 0.01206530723720789 Accuracy 0.87353515625\n",
      "Iteration 16760 Training loss 0.009190039709210396 Validation loss 0.012366468086838722 Accuracy 0.87109375\n",
      "Iteration 16770 Training loss 0.007953677326440811 Validation loss 0.012076755054295063 Accuracy 0.87353515625\n",
      "Iteration 16780 Training loss 0.008548232726752758 Validation loss 0.013360419310629368 Accuracy 0.85986328125\n",
      "Iteration 16790 Training loss 0.006554177962243557 Validation loss 0.012160909362137318 Accuracy 0.8720703125\n",
      "Iteration 16800 Training loss 0.00893909577280283 Validation loss 0.012338008731603622 Accuracy 0.87060546875\n",
      "Iteration 16810 Training loss 0.00938087236136198 Validation loss 0.012193773873150349 Accuracy 0.873046875\n",
      "Iteration 16820 Training loss 0.009047501720488071 Validation loss 0.012031168676912785 Accuracy 0.87451171875\n",
      "Iteration 16830 Training loss 0.008762169629335403 Validation loss 0.012856453657150269 Accuracy 0.86572265625\n",
      "Iteration 16840 Training loss 0.007564541418105364 Validation loss 0.012040780857205391 Accuracy 0.8740234375\n",
      "Iteration 16850 Training loss 0.007029613945633173 Validation loss 0.01204263512045145 Accuracy 0.87353515625\n",
      "Iteration 16860 Training loss 0.007998130284249783 Validation loss 0.011911685578525066 Accuracy 0.87548828125\n",
      "Iteration 16870 Training loss 0.007255412172526121 Validation loss 0.012068038806319237 Accuracy 0.87353515625\n",
      "Iteration 16880 Training loss 0.007389638107270002 Validation loss 0.012060346081852913 Accuracy 0.87353515625\n",
      "Iteration 16890 Training loss 0.00795253086835146 Validation loss 0.012241852469742298 Accuracy 0.8720703125\n",
      "Iteration 16900 Training loss 0.007608878891915083 Validation loss 0.01309204287827015 Accuracy 0.8623046875\n",
      "Iteration 16910 Training loss 0.010184643790125847 Validation loss 0.012109627015888691 Accuracy 0.873046875\n",
      "Iteration 16920 Training loss 0.006308667361736298 Validation loss 0.012795965187251568 Accuracy 0.86669921875\n",
      "Iteration 16930 Training loss 0.008098463527858257 Validation loss 0.01228122878819704 Accuracy 0.87060546875\n",
      "Iteration 16940 Training loss 0.009699725545942783 Validation loss 0.012235491536557674 Accuracy 0.87255859375\n",
      "Iteration 16950 Training loss 0.010709172114729881 Validation loss 0.013162338174879551 Accuracy 0.8623046875\n",
      "Iteration 16960 Training loss 0.007585454266518354 Validation loss 0.011969550512731075 Accuracy 0.875\n",
      "Iteration 16970 Training loss 0.009061826393008232 Validation loss 0.012944230809807777 Accuracy 0.86328125\n",
      "Iteration 16980 Training loss 0.007026189006865025 Validation loss 0.012291367165744305 Accuracy 0.87109375\n",
      "Iteration 16990 Training loss 0.00997874978929758 Validation loss 0.012385230511426926 Accuracy 0.87060546875\n",
      "Iteration 17000 Training loss 0.010898260399699211 Validation loss 0.01292564906179905 Accuracy 0.8642578125\n",
      "Iteration 17010 Training loss 0.008877642452716827 Validation loss 0.013046948239207268 Accuracy 0.86279296875\n",
      "Iteration 17020 Training loss 0.007747103460133076 Validation loss 0.0122302807867527 Accuracy 0.8720703125\n",
      "Iteration 17030 Training loss 0.006897935178130865 Validation loss 0.012427294626832008 Accuracy 0.86962890625\n",
      "Iteration 17040 Training loss 0.007821155712008476 Validation loss 0.012399539351463318 Accuracy 0.86962890625\n",
      "Iteration 17050 Training loss 0.009056692942976952 Validation loss 0.012548825703561306 Accuracy 0.8681640625\n",
      "Iteration 17060 Training loss 0.008017509244382381 Validation loss 0.0124978581443429 Accuracy 0.869140625\n",
      "Iteration 17070 Training loss 0.007689122576266527 Validation loss 0.012217008508741856 Accuracy 0.87158203125\n",
      "Iteration 17080 Training loss 0.007785280235111713 Validation loss 0.012165150605142117 Accuracy 0.87255859375\n",
      "Iteration 17090 Training loss 0.011128829792141914 Validation loss 0.012158242054283619 Accuracy 0.87353515625\n",
      "Iteration 17100 Training loss 0.007611299864947796 Validation loss 0.012189987115561962 Accuracy 0.87255859375\n",
      "Iteration 17110 Training loss 0.009271802380681038 Validation loss 0.012231830507516861 Accuracy 0.87255859375\n",
      "Iteration 17120 Training loss 0.00744756031781435 Validation loss 0.012091065756976604 Accuracy 0.873046875\n",
      "Iteration 17130 Training loss 0.007683014031499624 Validation loss 0.012656640261411667 Accuracy 0.8671875\n",
      "Iteration 17140 Training loss 0.009637040086090565 Validation loss 0.012447553686797619 Accuracy 0.87109375\n",
      "Iteration 17150 Training loss 0.007494731340557337 Validation loss 0.012340519577264786 Accuracy 0.87158203125\n",
      "Iteration 17160 Training loss 0.007916323840618134 Validation loss 0.012227265164256096 Accuracy 0.87158203125\n",
      "Iteration 17170 Training loss 0.009616601280868053 Validation loss 0.012142272666096687 Accuracy 0.873046875\n",
      "Iteration 17180 Training loss 0.006846939213573933 Validation loss 0.012102242559194565 Accuracy 0.87353515625\n",
      "Iteration 17190 Training loss 0.009517182596027851 Validation loss 0.012894738465547562 Accuracy 0.865234375\n",
      "Iteration 17200 Training loss 0.009259947575628757 Validation loss 0.012958308681845665 Accuracy 0.865234375\n",
      "Iteration 17210 Training loss 0.009029882028698921 Validation loss 0.012522551231086254 Accuracy 0.86962890625\n",
      "Iteration 17220 Training loss 0.00862072128802538 Validation loss 0.012855482287704945 Accuracy 0.86669921875\n",
      "Iteration 17230 Training loss 0.01047752145677805 Validation loss 0.01217186264693737 Accuracy 0.8720703125\n",
      "Iteration 17240 Training loss 0.007202975917607546 Validation loss 0.01199689693748951 Accuracy 0.87451171875\n",
      "Iteration 17250 Training loss 0.008606605231761932 Validation loss 0.012060514651238918 Accuracy 0.8740234375\n",
      "Iteration 17260 Training loss 0.010324847884476185 Validation loss 0.013046498410403728 Accuracy 0.86376953125\n",
      "Iteration 17270 Training loss 0.011141130700707436 Validation loss 0.012029238976538181 Accuracy 0.8740234375\n",
      "Iteration 17280 Training loss 0.00932281743735075 Validation loss 0.012042751535773277 Accuracy 0.8740234375\n",
      "Iteration 17290 Training loss 0.008401528932154179 Validation loss 0.011979672126471996 Accuracy 0.87451171875\n",
      "Iteration 17300 Training loss 0.007632842753082514 Validation loss 0.012566948309540749 Accuracy 0.86865234375\n",
      "Iteration 17310 Training loss 0.00956827774643898 Validation loss 0.013531851582229137 Accuracy 0.85888671875\n",
      "Iteration 17320 Training loss 0.009121722541749477 Validation loss 0.012262721545994282 Accuracy 0.8720703125\n",
      "Iteration 17330 Training loss 0.008622588589787483 Validation loss 0.012070422992110252 Accuracy 0.873046875\n",
      "Iteration 17340 Training loss 0.007617984898388386 Validation loss 0.012114237993955612 Accuracy 0.8740234375\n",
      "Iteration 17350 Training loss 0.007224349305033684 Validation loss 0.011822253465652466 Accuracy 0.87548828125\n",
      "Iteration 17360 Training loss 0.00731930136680603 Validation loss 0.012745697051286697 Accuracy 0.86669921875\n",
      "Iteration 17370 Training loss 0.00950920395553112 Validation loss 0.014003981836140156 Accuracy 0.853515625\n",
      "Iteration 17380 Training loss 0.010750571265816689 Validation loss 0.012805228121578693 Accuracy 0.8662109375\n",
      "Iteration 17390 Training loss 0.008360170759260654 Validation loss 0.012629449367523193 Accuracy 0.86865234375\n",
      "Iteration 17400 Training loss 0.007206539157778025 Validation loss 0.012512258253991604 Accuracy 0.8681640625\n",
      "Iteration 17410 Training loss 0.009232074953615665 Validation loss 0.013112055137753487 Accuracy 0.8623046875\n",
      "Iteration 17420 Training loss 0.01075277291238308 Validation loss 0.01254324335604906 Accuracy 0.8681640625\n",
      "Iteration 17430 Training loss 0.00823536328971386 Validation loss 0.012066977098584175 Accuracy 0.8740234375\n",
      "Iteration 17440 Training loss 0.009529651142656803 Validation loss 0.01282102707773447 Accuracy 0.8662109375\n",
      "Iteration 17450 Training loss 0.008008904755115509 Validation loss 0.012566127814352512 Accuracy 0.86767578125\n",
      "Iteration 17460 Training loss 0.007027417421340942 Validation loss 0.012476591393351555 Accuracy 0.8701171875\n",
      "Iteration 17470 Training loss 0.007967226207256317 Validation loss 0.012097069062292576 Accuracy 0.87353515625\n",
      "Iteration 17480 Training loss 0.006999021861702204 Validation loss 0.012679965235292912 Accuracy 0.8671875\n",
      "Iteration 17490 Training loss 0.0078254584223032 Validation loss 0.012403750792145729 Accuracy 0.86962890625\n",
      "Iteration 17500 Training loss 0.00763087859377265 Validation loss 0.012381386011838913 Accuracy 0.8701171875\n",
      "Iteration 17510 Training loss 0.00835593044757843 Validation loss 0.01253406424075365 Accuracy 0.86865234375\n",
      "Iteration 17520 Training loss 0.007481062319129705 Validation loss 0.012172311544418335 Accuracy 0.87255859375\n",
      "Iteration 17530 Training loss 0.00866605993360281 Validation loss 0.012320253998041153 Accuracy 0.87060546875\n",
      "Iteration 17540 Training loss 0.008853031322360039 Validation loss 0.012385976500809193 Accuracy 0.87060546875\n",
      "Iteration 17550 Training loss 0.008403206244111061 Validation loss 0.01196147408336401 Accuracy 0.875\n",
      "Iteration 17560 Training loss 0.009548647329211235 Validation loss 0.012113328091800213 Accuracy 0.8740234375\n",
      "Iteration 17570 Training loss 0.008404407650232315 Validation loss 0.011891700327396393 Accuracy 0.875\n",
      "Iteration 17580 Training loss 0.009005511179566383 Validation loss 0.012313609011471272 Accuracy 0.87158203125\n",
      "Iteration 17590 Training loss 0.010484926402568817 Validation loss 0.012437905184924603 Accuracy 0.8701171875\n",
      "Iteration 17600 Training loss 0.007984085008502007 Validation loss 0.013076890259981155 Accuracy 0.86328125\n",
      "Iteration 17610 Training loss 0.01003616489470005 Validation loss 0.012426985427737236 Accuracy 0.87109375\n",
      "Iteration 17620 Training loss 0.00881833303719759 Validation loss 0.01194668747484684 Accuracy 0.875\n",
      "Iteration 17630 Training loss 0.008400388062000275 Validation loss 0.01217428594827652 Accuracy 0.87158203125\n",
      "Iteration 17640 Training loss 0.008267819881439209 Validation loss 0.012082831934094429 Accuracy 0.87353515625\n",
      "Iteration 17650 Training loss 0.00790893193334341 Validation loss 0.012154902331531048 Accuracy 0.873046875\n",
      "Iteration 17660 Training loss 0.007388266269117594 Validation loss 0.01220572367310524 Accuracy 0.87255859375\n",
      "Iteration 17670 Training loss 0.00929209589958191 Validation loss 0.012491585686802864 Accuracy 0.8701171875\n",
      "Iteration 17680 Training loss 0.007551889400929213 Validation loss 0.012401185929775238 Accuracy 0.87109375\n",
      "Iteration 17690 Training loss 0.008293851278722286 Validation loss 0.012086117640137672 Accuracy 0.87353515625\n",
      "Iteration 17700 Training loss 0.008255751803517342 Validation loss 0.012050453573465347 Accuracy 0.8740234375\n",
      "Iteration 17710 Training loss 0.0073249503038823605 Validation loss 0.011799799278378487 Accuracy 0.876953125\n",
      "Iteration 17720 Training loss 0.00736484257504344 Validation loss 0.012358500622212887 Accuracy 0.87060546875\n",
      "Iteration 17730 Training loss 0.010131895542144775 Validation loss 0.013097197748720646 Accuracy 0.86279296875\n",
      "Iteration 17740 Training loss 0.0075784414075315 Validation loss 0.012318598106503487 Accuracy 0.87109375\n",
      "Iteration 17750 Training loss 0.005978664383292198 Validation loss 0.011770601384341717 Accuracy 0.87646484375\n",
      "Iteration 17760 Training loss 0.007737824693322182 Validation loss 0.011909511871635914 Accuracy 0.87548828125\n",
      "Iteration 17770 Training loss 0.007146931253373623 Validation loss 0.011965488083660603 Accuracy 0.8740234375\n",
      "Iteration 17780 Training loss 0.006724728271365166 Validation loss 0.012135080061852932 Accuracy 0.87158203125\n",
      "Iteration 17790 Training loss 0.010652828961610794 Validation loss 0.012383642606437206 Accuracy 0.86962890625\n",
      "Iteration 17800 Training loss 0.007352277170866728 Validation loss 0.012026410549879074 Accuracy 0.873046875\n",
      "Iteration 17810 Training loss 0.007759288884699345 Validation loss 0.012415889650583267 Accuracy 0.869140625\n",
      "Iteration 17820 Training loss 0.0056677754037082195 Validation loss 0.01220118347555399 Accuracy 0.87255859375\n",
      "Iteration 17830 Training loss 0.009126885794103146 Validation loss 0.01237899623811245 Accuracy 0.8701171875\n",
      "Iteration 17840 Training loss 0.007870843634009361 Validation loss 0.012250326573848724 Accuracy 0.87158203125\n",
      "Iteration 17850 Training loss 0.008034514263272285 Validation loss 0.012483011931180954 Accuracy 0.869140625\n",
      "Iteration 17860 Training loss 0.007146541494876146 Validation loss 0.012022458016872406 Accuracy 0.87451171875\n",
      "Iteration 17870 Training loss 0.009393214248120785 Validation loss 0.012034470215439796 Accuracy 0.8740234375\n",
      "Iteration 17880 Training loss 0.007535846438258886 Validation loss 0.012049969285726547 Accuracy 0.8740234375\n",
      "Iteration 17890 Training loss 0.007788159418851137 Validation loss 0.012374097481369972 Accuracy 0.87109375\n",
      "Iteration 17900 Training loss 0.006119538564234972 Validation loss 0.012482991442084312 Accuracy 0.86962890625\n",
      "Iteration 17910 Training loss 0.008001819252967834 Validation loss 0.012652176432311535 Accuracy 0.8681640625\n",
      "Iteration 17920 Training loss 0.009259996004402637 Validation loss 0.011853749863803387 Accuracy 0.87646484375\n",
      "Iteration 17930 Training loss 0.010046213865280151 Validation loss 0.011818856000900269 Accuracy 0.8759765625\n",
      "Iteration 17940 Training loss 0.006777065806090832 Validation loss 0.012953581288456917 Accuracy 0.865234375\n",
      "Iteration 17950 Training loss 0.006108947563916445 Validation loss 0.012441486120223999 Accuracy 0.8701171875\n",
      "Iteration 17960 Training loss 0.008185870945453644 Validation loss 0.013031239621341228 Accuracy 0.86328125\n",
      "Iteration 17970 Training loss 0.008081172592937946 Validation loss 0.0121424850076437 Accuracy 0.8720703125\n",
      "Iteration 17980 Training loss 0.008401292376220226 Validation loss 0.012194096110761166 Accuracy 0.8720703125\n",
      "Iteration 17990 Training loss 0.008295070379972458 Validation loss 0.011837858706712723 Accuracy 0.87548828125\n",
      "Iteration 18000 Training loss 0.005906211212277412 Validation loss 0.012022646144032478 Accuracy 0.8740234375\n",
      "Iteration 18010 Training loss 0.007015631999820471 Validation loss 0.011866770684719086 Accuracy 0.8759765625\n",
      "Iteration 18020 Training loss 0.009427153505384922 Validation loss 0.011805597692728043 Accuracy 0.87548828125\n",
      "Iteration 18030 Training loss 0.007418048568069935 Validation loss 0.011900250799953938 Accuracy 0.8759765625\n",
      "Iteration 18040 Training loss 0.006197401788085699 Validation loss 0.012838916853070259 Accuracy 0.86474609375\n",
      "Iteration 18050 Training loss 0.008540241979062557 Validation loss 0.01176405232399702 Accuracy 0.8759765625\n",
      "Iteration 18060 Training loss 0.008283022791147232 Validation loss 0.011763494461774826 Accuracy 0.87646484375\n",
      "Iteration 18070 Training loss 0.007220031227916479 Validation loss 0.011692040599882603 Accuracy 0.8759765625\n",
      "Iteration 18080 Training loss 0.007091033738106489 Validation loss 0.012312542647123337 Accuracy 0.87158203125\n",
      "Iteration 18090 Training loss 0.007665902841836214 Validation loss 0.011848466470837593 Accuracy 0.875\n",
      "Iteration 18100 Training loss 0.0082173440605402 Validation loss 0.012023900635540485 Accuracy 0.87353515625\n",
      "Iteration 18110 Training loss 0.008027750998735428 Validation loss 0.012495091184973717 Accuracy 0.869140625\n",
      "Iteration 18120 Training loss 0.008181721903383732 Validation loss 0.012426537461578846 Accuracy 0.86962890625\n",
      "Iteration 18130 Training loss 0.008990374393761158 Validation loss 0.012272033840417862 Accuracy 0.87109375\n",
      "Iteration 18140 Training loss 0.007747245486825705 Validation loss 0.011739542707800865 Accuracy 0.87646484375\n",
      "Iteration 18150 Training loss 0.0079546719789505 Validation loss 0.0117444833740592 Accuracy 0.87646484375\n",
      "Iteration 18160 Training loss 0.006859526038169861 Validation loss 0.012401437386870384 Accuracy 0.8701171875\n",
      "Iteration 18170 Training loss 0.0070600975304841995 Validation loss 0.011886547319591045 Accuracy 0.87548828125\n",
      "Iteration 18180 Training loss 0.007074906025081873 Validation loss 0.012003026902675629 Accuracy 0.87353515625\n",
      "Iteration 18190 Training loss 0.007680376525968313 Validation loss 0.011860900558531284 Accuracy 0.87548828125\n",
      "Iteration 18200 Training loss 0.007672120351344347 Validation loss 0.011768299154937267 Accuracy 0.876953125\n",
      "Iteration 18210 Training loss 0.0066177258267998695 Validation loss 0.01173307839781046 Accuracy 0.87646484375\n",
      "Iteration 18220 Training loss 0.007035407237708569 Validation loss 0.012111213058233261 Accuracy 0.8720703125\n",
      "Iteration 18230 Training loss 0.008169718086719513 Validation loss 0.011752692982554436 Accuracy 0.8759765625\n",
      "Iteration 18240 Training loss 0.0078125 Validation loss 0.011797379702329636 Accuracy 0.8759765625\n",
      "Iteration 18250 Training loss 0.0068857562728226185 Validation loss 0.01194449421018362 Accuracy 0.8740234375\n",
      "Iteration 18260 Training loss 0.006885294336825609 Validation loss 0.012602180242538452 Accuracy 0.8671875\n",
      "Iteration 18270 Training loss 0.00910615362226963 Validation loss 0.013319894671440125 Accuracy 0.86083984375\n",
      "Iteration 18280 Training loss 0.008034538477659225 Validation loss 0.012135075405240059 Accuracy 0.873046875\n",
      "Iteration 18290 Training loss 0.006801027338951826 Validation loss 0.011752571910619736 Accuracy 0.87646484375\n",
      "Iteration 18300 Training loss 0.009568736888468266 Validation loss 0.011796632781624794 Accuracy 0.876953125\n",
      "Iteration 18310 Training loss 0.005910410080105066 Validation loss 0.011609064415097237 Accuracy 0.87841796875\n",
      "Iteration 18320 Training loss 0.006909174378961325 Validation loss 0.012862292118370533 Accuracy 0.8662109375\n",
      "Iteration 18330 Training loss 0.0066212196834385395 Validation loss 0.011631371453404427 Accuracy 0.87890625\n",
      "Iteration 18340 Training loss 0.008220324292778969 Validation loss 0.011935580521821976 Accuracy 0.87451171875\n",
      "Iteration 18350 Training loss 0.006179032381623983 Validation loss 0.011674482375383377 Accuracy 0.87744140625\n",
      "Iteration 18360 Training loss 0.008706141263246536 Validation loss 0.011878417804837227 Accuracy 0.875\n",
      "Iteration 18370 Training loss 0.009014464914798737 Validation loss 0.01196647435426712 Accuracy 0.8740234375\n",
      "Iteration 18380 Training loss 0.008069084025919437 Validation loss 0.01205923780798912 Accuracy 0.87255859375\n",
      "Iteration 18390 Training loss 0.008259657770395279 Validation loss 0.011774053797125816 Accuracy 0.87646484375\n",
      "Iteration 18400 Training loss 0.006334988866001368 Validation loss 0.0117386095225811 Accuracy 0.8759765625\n",
      "Iteration 18410 Training loss 0.010531652718782425 Validation loss 0.012075529433786869 Accuracy 0.87255859375\n",
      "Iteration 18420 Training loss 0.00827227532863617 Validation loss 0.011882644146680832 Accuracy 0.8759765625\n",
      "Iteration 18430 Training loss 0.007283117156475782 Validation loss 0.011871681548655033 Accuracy 0.875\n",
      "Iteration 18440 Training loss 0.009508652612566948 Validation loss 0.012298507615923882 Accuracy 0.8701171875\n",
      "Iteration 18450 Training loss 0.007674511522054672 Validation loss 0.01169058121740818 Accuracy 0.87744140625\n",
      "Iteration 18460 Training loss 0.008678282611072063 Validation loss 0.012228273786604404 Accuracy 0.87158203125\n",
      "Iteration 18470 Training loss 0.007969620637595654 Validation loss 0.0117736104875803 Accuracy 0.876953125\n",
      "Iteration 18480 Training loss 0.0069429995492100716 Validation loss 0.012099777348339558 Accuracy 0.8740234375\n",
      "Iteration 18490 Training loss 0.00835790578275919 Validation loss 0.011981461197137833 Accuracy 0.87451171875\n",
      "Iteration 18500 Training loss 0.008073294535279274 Validation loss 0.012096303515136242 Accuracy 0.873046875\n",
      "Iteration 18510 Training loss 0.007405128329992294 Validation loss 0.011914951726794243 Accuracy 0.875\n",
      "Iteration 18520 Training loss 0.0074620600789785385 Validation loss 0.011729689314961433 Accuracy 0.8779296875\n",
      "Iteration 18530 Training loss 0.008675595745444298 Validation loss 0.01252862997353077 Accuracy 0.86962890625\n",
      "Iteration 18540 Training loss 0.0075951386243104935 Validation loss 0.011644147336483002 Accuracy 0.876953125\n",
      "Iteration 18550 Training loss 0.008378718979656696 Validation loss 0.011945459060370922 Accuracy 0.87451171875\n",
      "Iteration 18560 Training loss 0.0064194099977612495 Validation loss 0.012004769407212734 Accuracy 0.87353515625\n",
      "Iteration 18570 Training loss 0.008063341490924358 Validation loss 0.011722290888428688 Accuracy 0.87646484375\n",
      "Iteration 18580 Training loss 0.006853972561657429 Validation loss 0.011879938654601574 Accuracy 0.87548828125\n",
      "Iteration 18590 Training loss 0.00751544488593936 Validation loss 0.012217424809932709 Accuracy 0.8720703125\n",
      "Iteration 18600 Training loss 0.0068860347382724285 Validation loss 0.012116232886910439 Accuracy 0.87353515625\n",
      "Iteration 18610 Training loss 0.00802088063210249 Validation loss 0.012084435671567917 Accuracy 0.873046875\n",
      "Iteration 18620 Training loss 0.009090578183531761 Validation loss 0.012310079298913479 Accuracy 0.87109375\n",
      "Iteration 18630 Training loss 0.007941159419715405 Validation loss 0.011930477805435658 Accuracy 0.8740234375\n",
      "Iteration 18640 Training loss 0.007355549372732639 Validation loss 0.011724605225026608 Accuracy 0.876953125\n",
      "Iteration 18650 Training loss 0.006484985817223787 Validation loss 0.011800155974924564 Accuracy 0.8759765625\n",
      "Iteration 18660 Training loss 0.009784079156816006 Validation loss 0.0123407281935215 Accuracy 0.87060546875\n",
      "Iteration 18670 Training loss 0.007342070806771517 Validation loss 0.011765590868890285 Accuracy 0.87646484375\n",
      "Iteration 18680 Training loss 0.00796285830438137 Validation loss 0.012524472549557686 Accuracy 0.86865234375\n",
      "Iteration 18690 Training loss 0.010457471013069153 Validation loss 0.012294970452785492 Accuracy 0.87158203125\n",
      "Iteration 18700 Training loss 0.008706298656761646 Validation loss 0.012815441936254501 Accuracy 0.8662109375\n",
      "Iteration 18710 Training loss 0.00903693400323391 Validation loss 0.012075487524271011 Accuracy 0.873046875\n",
      "Iteration 18720 Training loss 0.0068754879757761955 Validation loss 0.011754550039768219 Accuracy 0.8759765625\n",
      "Iteration 18730 Training loss 0.00871837418526411 Validation loss 0.012804626487195492 Accuracy 0.86572265625\n",
      "Iteration 18740 Training loss 0.007221119944006205 Validation loss 0.011492013931274414 Accuracy 0.87890625\n",
      "Iteration 18750 Training loss 0.007065075449645519 Validation loss 0.011574339121580124 Accuracy 0.8779296875\n",
      "Iteration 18760 Training loss 0.008453615941107273 Validation loss 0.011963846161961555 Accuracy 0.87451171875\n",
      "Iteration 18770 Training loss 0.0070107802748680115 Validation loss 0.012177504599094391 Accuracy 0.8720703125\n",
      "Iteration 18780 Training loss 0.006266261916607618 Validation loss 0.012043551541864872 Accuracy 0.87353515625\n",
      "Iteration 18790 Training loss 0.007479474414139986 Validation loss 0.011577568016946316 Accuracy 0.8779296875\n",
      "Iteration 18800 Training loss 0.007389547303318977 Validation loss 0.011568473652005196 Accuracy 0.87890625\n",
      "Iteration 18810 Training loss 0.007608184590935707 Validation loss 0.011806382797658443 Accuracy 0.87646484375\n",
      "Iteration 18820 Training loss 0.009100827388465405 Validation loss 0.011893603019416332 Accuracy 0.8740234375\n",
      "Iteration 18830 Training loss 0.007191701792180538 Validation loss 0.011668730527162552 Accuracy 0.87744140625\n",
      "Iteration 18840 Training loss 0.006246963515877724 Validation loss 0.011748671531677246 Accuracy 0.87744140625\n",
      "Iteration 18850 Training loss 0.0066574434749782085 Validation loss 0.012033598497509956 Accuracy 0.87353515625\n",
      "Iteration 18860 Training loss 0.005308076273649931 Validation loss 0.012037908658385277 Accuracy 0.87353515625\n",
      "Iteration 18870 Training loss 0.00841104332357645 Validation loss 0.01158563420176506 Accuracy 0.87841796875\n",
      "Iteration 18880 Training loss 0.006703255698084831 Validation loss 0.011906718835234642 Accuracy 0.87451171875\n",
      "Iteration 18890 Training loss 0.0066161383874714375 Validation loss 0.012018579989671707 Accuracy 0.87353515625\n",
      "Iteration 18900 Training loss 0.008351899683475494 Validation loss 0.011839966289699078 Accuracy 0.875\n",
      "Iteration 18910 Training loss 0.009633899666368961 Validation loss 0.012416188605129719 Accuracy 0.86865234375\n",
      "Iteration 18920 Training loss 0.004647617693990469 Validation loss 0.012132811360061169 Accuracy 0.873046875\n",
      "Iteration 18930 Training loss 0.007018357049673796 Validation loss 0.011780700646340847 Accuracy 0.8759765625\n",
      "Iteration 18940 Training loss 0.00656375614926219 Validation loss 0.011882215738296509 Accuracy 0.87548828125\n",
      "Iteration 18950 Training loss 0.008174218237400055 Validation loss 0.012903050519526005 Accuracy 0.86474609375\n",
      "Iteration 18960 Training loss 0.0077726515009999275 Validation loss 0.012487977743148804 Accuracy 0.8681640625\n",
      "Iteration 18970 Training loss 0.006332061719149351 Validation loss 0.011886530555784702 Accuracy 0.87451171875\n",
      "Iteration 18980 Training loss 0.0070452382788062096 Validation loss 0.011939474381506443 Accuracy 0.87451171875\n",
      "Iteration 18990 Training loss 0.005691764876246452 Validation loss 0.011927219107747078 Accuracy 0.87451171875\n",
      "Iteration 19000 Training loss 0.00784788466989994 Validation loss 0.012420278042554855 Accuracy 0.8701171875\n",
      "Iteration 19010 Training loss 0.007722118403762579 Validation loss 0.01212526299059391 Accuracy 0.87255859375\n",
      "Iteration 19020 Training loss 0.007969746366143227 Validation loss 0.011951153166592121 Accuracy 0.875\n",
      "Iteration 19030 Training loss 0.006764683406800032 Validation loss 0.011867714114487171 Accuracy 0.875\n",
      "Iteration 19040 Training loss 0.007536106742918491 Validation loss 0.01234267558902502 Accuracy 0.869140625\n",
      "Iteration 19050 Training loss 0.008762981742620468 Validation loss 0.01217910461127758 Accuracy 0.8720703125\n",
      "Iteration 19060 Training loss 0.009591375477612019 Validation loss 0.012453930452466011 Accuracy 0.86865234375\n",
      "Iteration 19070 Training loss 0.009099331684410572 Validation loss 0.012080961838364601 Accuracy 0.873046875\n",
      "Iteration 19080 Training loss 0.008079187013208866 Validation loss 0.012567245401442051 Accuracy 0.86865234375\n",
      "Iteration 19090 Training loss 0.007200323045253754 Validation loss 0.011798180639743805 Accuracy 0.87646484375\n",
      "Iteration 19100 Training loss 0.006351714953780174 Validation loss 0.011950796470046043 Accuracy 0.8740234375\n",
      "Iteration 19110 Training loss 0.006351579446345568 Validation loss 0.012014943175017834 Accuracy 0.8740234375\n",
      "Iteration 19120 Training loss 0.008815468288958073 Validation loss 0.012005140073597431 Accuracy 0.87353515625\n",
      "Iteration 19130 Training loss 0.00851703155785799 Validation loss 0.011773434467613697 Accuracy 0.876953125\n",
      "Iteration 19140 Training loss 0.008383977226912975 Validation loss 0.011916120536625385 Accuracy 0.875\n",
      "Iteration 19150 Training loss 0.007401794195175171 Validation loss 0.011883101426064968 Accuracy 0.87548828125\n",
      "Iteration 19160 Training loss 0.007772975135594606 Validation loss 0.011843200773000717 Accuracy 0.875\n",
      "Iteration 19170 Training loss 0.009675289504230022 Validation loss 0.01203538291156292 Accuracy 0.87353515625\n",
      "Iteration 19180 Training loss 0.008984844200313091 Validation loss 0.01222524419426918 Accuracy 0.8720703125\n",
      "Iteration 19190 Training loss 0.007823798805475235 Validation loss 0.012048301286995411 Accuracy 0.8720703125\n",
      "Iteration 19200 Training loss 0.007891477085649967 Validation loss 0.012080871500074863 Accuracy 0.8720703125\n",
      "Iteration 19210 Training loss 0.005092563573271036 Validation loss 0.011401988565921783 Accuracy 0.8798828125\n",
      "Iteration 19220 Training loss 0.007646540179848671 Validation loss 0.011744786985218525 Accuracy 0.87646484375\n",
      "Iteration 19230 Training loss 0.008512710221111774 Validation loss 0.011913646943867207 Accuracy 0.875\n",
      "Iteration 19240 Training loss 0.007695774082094431 Validation loss 0.011683649383485317 Accuracy 0.8779296875\n",
      "Iteration 19250 Training loss 0.006910438649356365 Validation loss 0.011788374744355679 Accuracy 0.87646484375\n",
      "Iteration 19260 Training loss 0.007820641621947289 Validation loss 0.012193683534860611 Accuracy 0.8720703125\n",
      "Iteration 19270 Training loss 0.007775815203785896 Validation loss 0.012103567831218243 Accuracy 0.87255859375\n",
      "Iteration 19280 Training loss 0.008685164153575897 Validation loss 0.012096766382455826 Accuracy 0.87353515625\n",
      "Iteration 19290 Training loss 0.006349537987262011 Validation loss 0.012505356222391129 Accuracy 0.8681640625\n",
      "Iteration 19300 Training loss 0.004512937273830175 Validation loss 0.012296756729483604 Accuracy 0.87109375\n",
      "Iteration 19310 Training loss 0.006832540035247803 Validation loss 0.01154386904090643 Accuracy 0.87841796875\n",
      "Iteration 19320 Training loss 0.006574404425919056 Validation loss 0.012075236067175865 Accuracy 0.8720703125\n",
      "Iteration 19330 Training loss 0.007648416329175234 Validation loss 0.011720250360667706 Accuracy 0.87646484375\n",
      "Iteration 19340 Training loss 0.006021862383931875 Validation loss 0.011638158932328224 Accuracy 0.8779296875\n",
      "Iteration 19350 Training loss 0.008048615418374538 Validation loss 0.011713996529579163 Accuracy 0.87646484375\n",
      "Iteration 19360 Training loss 0.00750813540071249 Validation loss 0.011676506139338017 Accuracy 0.87646484375\n",
      "Iteration 19370 Training loss 0.005637718830257654 Validation loss 0.011948478408157825 Accuracy 0.8740234375\n",
      "Iteration 19380 Training loss 0.009861159138381481 Validation loss 0.012294615618884563 Accuracy 0.87060546875\n",
      "Iteration 19390 Training loss 0.0068413023836910725 Validation loss 0.011735102161765099 Accuracy 0.87646484375\n",
      "Iteration 19400 Training loss 0.008043664507567883 Validation loss 0.011823190376162529 Accuracy 0.87548828125\n",
      "Iteration 19410 Training loss 0.009220954962074757 Validation loss 0.01276672724634409 Accuracy 0.8662109375\n",
      "Iteration 19420 Training loss 0.006435157265514135 Validation loss 0.012070500291883945 Accuracy 0.87255859375\n",
      "Iteration 19430 Training loss 0.007664923090487719 Validation loss 0.01184156071394682 Accuracy 0.875\n",
      "Iteration 19440 Training loss 0.00689526554197073 Validation loss 0.01192951388657093 Accuracy 0.87451171875\n",
      "Iteration 19450 Training loss 0.008298822678625584 Validation loss 0.011705761775374413 Accuracy 0.87646484375\n",
      "Iteration 19460 Training loss 0.006978444289416075 Validation loss 0.011709809303283691 Accuracy 0.87744140625\n",
      "Iteration 19470 Training loss 0.006688236258924007 Validation loss 0.011955702677369118 Accuracy 0.875\n",
      "Iteration 19480 Training loss 0.008562058210372925 Validation loss 0.011790317483246326 Accuracy 0.875\n",
      "Iteration 19490 Training loss 0.009514672681689262 Validation loss 0.012304704636335373 Accuracy 0.87060546875\n",
      "Iteration 19500 Training loss 0.00872746016830206 Validation loss 0.011751098558306694 Accuracy 0.8759765625\n",
      "Iteration 19510 Training loss 0.006777188740670681 Validation loss 0.012036584317684174 Accuracy 0.8720703125\n",
      "Iteration 19520 Training loss 0.007811355404555798 Validation loss 0.012635739520192146 Accuracy 0.86669921875\n",
      "Iteration 19530 Training loss 0.0055406889878213406 Validation loss 0.011937489733099937 Accuracy 0.8740234375\n",
      "Iteration 19540 Training loss 0.008162273094058037 Validation loss 0.012090831995010376 Accuracy 0.8740234375\n",
      "Iteration 19550 Training loss 0.007501406129449606 Validation loss 0.012089838273823261 Accuracy 0.87255859375\n",
      "Iteration 19560 Training loss 0.006496045272797346 Validation loss 0.011513185687363148 Accuracy 0.87939453125\n",
      "Iteration 19570 Training loss 0.007779345847666264 Validation loss 0.012248864397406578 Accuracy 0.87060546875\n",
      "Iteration 19580 Training loss 0.007253782823681831 Validation loss 0.012166646309196949 Accuracy 0.8720703125\n",
      "Iteration 19590 Training loss 0.006576459389179945 Validation loss 0.011901875026524067 Accuracy 0.875\n",
      "Iteration 19600 Training loss 0.006442081183195114 Validation loss 0.011720802634954453 Accuracy 0.87744140625\n",
      "Iteration 19610 Training loss 0.010024857707321644 Validation loss 0.012746283784508705 Accuracy 0.8662109375\n",
      "Iteration 19620 Training loss 0.006269244942814112 Validation loss 0.011902897618710995 Accuracy 0.87451171875\n",
      "Iteration 19630 Training loss 0.006492486223578453 Validation loss 0.01207934319972992 Accuracy 0.87353515625\n",
      "Iteration 19640 Training loss 0.007371225859969854 Validation loss 0.011886459775269032 Accuracy 0.87451171875\n",
      "Iteration 19650 Training loss 0.006269598379731178 Validation loss 0.01243369746953249 Accuracy 0.8701171875\n",
      "Iteration 19660 Training loss 0.010413940995931625 Validation loss 0.01223849318921566 Accuracy 0.87109375\n",
      "Iteration 19670 Training loss 0.008840561844408512 Validation loss 0.011875711381435394 Accuracy 0.875\n",
      "Iteration 19680 Training loss 0.007725599687546492 Validation loss 0.01192252617329359 Accuracy 0.87548828125\n",
      "Iteration 19690 Training loss 0.005518291611224413 Validation loss 0.011417378671467304 Accuracy 0.8798828125\n",
      "Iteration 19700 Training loss 0.010946174152195454 Validation loss 0.011766435578465462 Accuracy 0.87744140625\n",
      "Iteration 19710 Training loss 0.007583736442029476 Validation loss 0.01159963384270668 Accuracy 0.8779296875\n",
      "Iteration 19720 Training loss 0.008416157215833664 Validation loss 0.011338203214108944 Accuracy 0.88134765625\n",
      "Iteration 19730 Training loss 0.006259313318878412 Validation loss 0.011905219405889511 Accuracy 0.875\n",
      "Iteration 19740 Training loss 0.007983994670212269 Validation loss 0.012357831932604313 Accuracy 0.87060546875\n",
      "Iteration 19750 Training loss 0.005252046510577202 Validation loss 0.011866888031363487 Accuracy 0.875\n",
      "Iteration 19760 Training loss 0.008044615387916565 Validation loss 0.01230328343808651 Accuracy 0.87109375\n",
      "Iteration 19770 Training loss 0.00727566285058856 Validation loss 0.011668302118778229 Accuracy 0.87744140625\n",
      "Iteration 19780 Training loss 0.007453483995050192 Validation loss 0.011805463582277298 Accuracy 0.875\n",
      "Iteration 19790 Training loss 0.006250898819416761 Validation loss 0.011798289604485035 Accuracy 0.87548828125\n",
      "Iteration 19800 Training loss 0.004783414304256439 Validation loss 0.01160119567066431 Accuracy 0.87841796875\n",
      "Iteration 19810 Training loss 0.0066330875270068645 Validation loss 0.011585869826376438 Accuracy 0.87841796875\n",
      "Iteration 19820 Training loss 0.0060520414263010025 Validation loss 0.011819418519735336 Accuracy 0.8759765625\n",
      "Iteration 19830 Training loss 0.005465890280902386 Validation loss 0.011541547253727913 Accuracy 0.87890625\n",
      "Iteration 19840 Training loss 0.00895907822996378 Validation loss 0.011606015264987946 Accuracy 0.87744140625\n",
      "Iteration 19850 Training loss 0.007196292281150818 Validation loss 0.011756069026887417 Accuracy 0.875\n",
      "Iteration 19860 Training loss 0.0070069339126348495 Validation loss 0.011581989005208015 Accuracy 0.87890625\n",
      "Iteration 19870 Training loss 0.009105204604566097 Validation loss 0.012462271377444267 Accuracy 0.86865234375\n",
      "Iteration 19880 Training loss 0.00758237624540925 Validation loss 0.011660898104310036 Accuracy 0.87744140625\n",
      "Iteration 19890 Training loss 0.007116631604731083 Validation loss 0.011980768293142319 Accuracy 0.87353515625\n",
      "Iteration 19900 Training loss 0.0074494159780442715 Validation loss 0.01188630610704422 Accuracy 0.87548828125\n",
      "Iteration 19910 Training loss 0.005803808569908142 Validation loss 0.011528229340910912 Accuracy 0.8779296875\n",
      "Iteration 19920 Training loss 0.006519509479403496 Validation loss 0.011808296665549278 Accuracy 0.875\n",
      "Iteration 19930 Training loss 0.006135799456387758 Validation loss 0.011886940337717533 Accuracy 0.87451171875\n",
      "Iteration 19940 Training loss 0.007838474586606026 Validation loss 0.011574533767998219 Accuracy 0.87841796875\n",
      "Iteration 19950 Training loss 0.0060670520178973675 Validation loss 0.011607257649302483 Accuracy 0.87744140625\n",
      "Iteration 19960 Training loss 0.006034668069332838 Validation loss 0.011533461511135101 Accuracy 0.87939453125\n",
      "Iteration 19970 Training loss 0.007691734004765749 Validation loss 0.011621088720858097 Accuracy 0.8779296875\n",
      "Iteration 19980 Training loss 0.005817682947963476 Validation loss 0.011771904304623604 Accuracy 0.876953125\n",
      "Iteration 19990 Training loss 0.008314733393490314 Validation loss 0.012217792682349682 Accuracy 0.8720703125\n",
      "Iteration 20000 Training loss 0.006352282129228115 Validation loss 0.011400651186704636 Accuracy 0.87939453125\n",
      "Iteration 20010 Training loss 0.005363464821130037 Validation loss 0.012062731198966503 Accuracy 0.87353515625\n",
      "Iteration 20020 Training loss 0.007568892557173967 Validation loss 0.01145137194544077 Accuracy 0.87939453125\n",
      "Iteration 20030 Training loss 0.008243635296821594 Validation loss 0.01169281080365181 Accuracy 0.87646484375\n",
      "Iteration 20040 Training loss 0.007052088156342506 Validation loss 0.011893988586962223 Accuracy 0.87451171875\n",
      "Iteration 20050 Training loss 0.007121838629245758 Validation loss 0.011749987490475178 Accuracy 0.8759765625\n",
      "Iteration 20060 Training loss 0.007856156677007675 Validation loss 0.011401528492569923 Accuracy 0.88037109375\n",
      "Iteration 20070 Training loss 0.005072084255516529 Validation loss 0.011406774632632732 Accuracy 0.87939453125\n",
      "Iteration 20080 Training loss 0.006982236634939909 Validation loss 0.012004205957055092 Accuracy 0.87353515625\n",
      "Iteration 20090 Training loss 0.006063316483050585 Validation loss 0.011508197523653507 Accuracy 0.87890625\n",
      "Iteration 20100 Training loss 0.010139591060578823 Validation loss 0.012349067255854607 Accuracy 0.869140625\n",
      "Iteration 20110 Training loss 0.00888932403177023 Validation loss 0.011512376368045807 Accuracy 0.87890625\n",
      "Iteration 20120 Training loss 0.006830182857811451 Validation loss 0.012074630707502365 Accuracy 0.87255859375\n",
      "Iteration 20130 Training loss 0.005717205815017223 Validation loss 0.011604768224060535 Accuracy 0.87744140625\n",
      "Iteration 20140 Training loss 0.007941663265228271 Validation loss 0.01178357470780611 Accuracy 0.87744140625\n",
      "Iteration 20150 Training loss 0.006918226834386587 Validation loss 0.012080592103302479 Accuracy 0.87353515625\n",
      "Iteration 20160 Training loss 0.007973626255989075 Validation loss 0.011754176579415798 Accuracy 0.87646484375\n",
      "Iteration 20170 Training loss 0.006810352206230164 Validation loss 0.011801783926784992 Accuracy 0.87646484375\n",
      "Iteration 20180 Training loss 0.005919657181948423 Validation loss 0.012067156843841076 Accuracy 0.87353515625\n",
      "Iteration 20190 Training loss 0.008270172402262688 Validation loss 0.012132614850997925 Accuracy 0.873046875\n",
      "Iteration 20200 Training loss 0.004626660142093897 Validation loss 0.011674684472382069 Accuracy 0.87744140625\n",
      "Iteration 20210 Training loss 0.0072489515878260136 Validation loss 0.011887497268617153 Accuracy 0.87451171875\n",
      "Iteration 20220 Training loss 0.006479061674326658 Validation loss 0.012056464329361916 Accuracy 0.8720703125\n",
      "Iteration 20230 Training loss 0.006829905789345503 Validation loss 0.011648600921034813 Accuracy 0.876953125\n",
      "Iteration 20240 Training loss 0.006996419280767441 Validation loss 0.011545702815055847 Accuracy 0.87841796875\n",
      "Iteration 20250 Training loss 0.0060584088787436485 Validation loss 0.01240853127092123 Accuracy 0.86962890625\n",
      "Iteration 20260 Training loss 0.007705168332904577 Validation loss 0.012117165140807629 Accuracy 0.873046875\n",
      "Iteration 20270 Training loss 0.006214364431798458 Validation loss 0.011664699763059616 Accuracy 0.876953125\n",
      "Iteration 20280 Training loss 0.009027284570038319 Validation loss 0.011612013913691044 Accuracy 0.8779296875\n",
      "Iteration 20290 Training loss 0.006399883423000574 Validation loss 0.01171521469950676 Accuracy 0.876953125\n",
      "Iteration 20300 Training loss 0.008812684565782547 Validation loss 0.012079733423888683 Accuracy 0.87255859375\n",
      "Iteration 20310 Training loss 0.00748441182076931 Validation loss 0.012082833796739578 Accuracy 0.873046875\n",
      "Iteration 20320 Training loss 0.005073782987892628 Validation loss 0.011570774018764496 Accuracy 0.87841796875\n",
      "Iteration 20330 Training loss 0.007123475428670645 Validation loss 0.011925782077014446 Accuracy 0.87451171875\n",
      "Iteration 20340 Training loss 0.0054375589825212955 Validation loss 0.011713105253875256 Accuracy 0.87744140625\n",
      "Iteration 20350 Training loss 0.00860959105193615 Validation loss 0.01148940622806549 Accuracy 0.87890625\n",
      "Iteration 20360 Training loss 0.008760361932218075 Validation loss 0.012100549414753914 Accuracy 0.873046875\n",
      "Iteration 20370 Training loss 0.006704936269670725 Validation loss 0.011802866123616695 Accuracy 0.8759765625\n",
      "Iteration 20380 Training loss 0.0076678963378071785 Validation loss 0.011626798659563065 Accuracy 0.8779296875\n",
      "Iteration 20390 Training loss 0.005967423785477877 Validation loss 0.011376645416021347 Accuracy 0.88037109375\n",
      "Iteration 20400 Training loss 0.006379289086908102 Validation loss 0.011609865352511406 Accuracy 0.8779296875\n",
      "Iteration 20410 Training loss 0.00796347577124834 Validation loss 0.011598652228713036 Accuracy 0.87744140625\n",
      "Iteration 20420 Training loss 0.006154251750558615 Validation loss 0.011466510593891144 Accuracy 0.87890625\n",
      "Iteration 20430 Training loss 0.009873430244624615 Validation loss 0.012921166606247425 Accuracy 0.86376953125\n",
      "Iteration 20440 Training loss 0.006806838326156139 Validation loss 0.011430758982896805 Accuracy 0.8798828125\n",
      "Iteration 20450 Training loss 0.010488289408385754 Validation loss 0.012465540319681168 Accuracy 0.869140625\n",
      "Iteration 20460 Training loss 0.007955145090818405 Validation loss 0.01214428711682558 Accuracy 0.873046875\n",
      "Iteration 20470 Training loss 0.005833802279084921 Validation loss 0.012573298066854477 Accuracy 0.8671875\n",
      "Iteration 20480 Training loss 0.007533869240432978 Validation loss 0.011973352171480656 Accuracy 0.873046875\n",
      "Iteration 20490 Training loss 0.007126061711460352 Validation loss 0.011385058052837849 Accuracy 0.87939453125\n",
      "Iteration 20500 Training loss 0.006382415536791086 Validation loss 0.012005601078271866 Accuracy 0.87451171875\n",
      "Iteration 20510 Training loss 0.0075521813705563545 Validation loss 0.011913737282156944 Accuracy 0.875\n",
      "Iteration 20520 Training loss 0.006485556252300739 Validation loss 0.01167260855436325 Accuracy 0.8779296875\n",
      "Iteration 20530 Training loss 0.004726841580122709 Validation loss 0.011610455811023712 Accuracy 0.8779296875\n",
      "Iteration 20540 Training loss 0.006911533419042826 Validation loss 0.011857946403324604 Accuracy 0.8759765625\n",
      "Iteration 20550 Training loss 0.00712865125387907 Validation loss 0.011642700992524624 Accuracy 0.87646484375\n",
      "Iteration 20560 Training loss 0.009192044846713543 Validation loss 0.011784793809056282 Accuracy 0.875\n",
      "Iteration 20570 Training loss 0.008674520999193192 Validation loss 0.012487963773310184 Accuracy 0.869140625\n",
      "Iteration 20580 Training loss 0.006225828547030687 Validation loss 0.01167727168649435 Accuracy 0.87744140625\n",
      "Iteration 20590 Training loss 0.005467962007969618 Validation loss 0.012015873566269875 Accuracy 0.873046875\n",
      "Iteration 20600 Training loss 0.004976952914148569 Validation loss 0.011588489636778831 Accuracy 0.87841796875\n",
      "Iteration 20610 Training loss 0.005116116721183062 Validation loss 0.011591327376663685 Accuracy 0.87841796875\n",
      "Iteration 20620 Training loss 0.008408918976783752 Validation loss 0.01216015126556158 Accuracy 0.8720703125\n",
      "Iteration 20630 Training loss 0.005438619758933783 Validation loss 0.011856178753077984 Accuracy 0.875\n",
      "Iteration 20640 Training loss 0.0070256562903523445 Validation loss 0.011838178150355816 Accuracy 0.87548828125\n",
      "Iteration 20650 Training loss 0.006804533768445253 Validation loss 0.011631252244114876 Accuracy 0.87744140625\n",
      "Iteration 20660 Training loss 0.009836097247898579 Validation loss 0.01186366192996502 Accuracy 0.87548828125\n",
      "Iteration 20670 Training loss 0.00551576679572463 Validation loss 0.011444096453487873 Accuracy 0.8798828125\n",
      "Iteration 20680 Training loss 0.004408481065183878 Validation loss 0.011965048499405384 Accuracy 0.87451171875\n",
      "Iteration 20690 Training loss 0.0075209173373878 Validation loss 0.01202132273465395 Accuracy 0.8720703125\n",
      "Iteration 20700 Training loss 0.005143304355442524 Validation loss 0.011659030802547932 Accuracy 0.87841796875\n",
      "Iteration 20710 Training loss 0.008225549943745136 Validation loss 0.011925749480724335 Accuracy 0.875\n",
      "Iteration 20720 Training loss 0.004957871045917273 Validation loss 0.011811408214271069 Accuracy 0.8759765625\n",
      "Iteration 20730 Training loss 0.006920411251485348 Validation loss 0.0118557158857584 Accuracy 0.875\n",
      "Iteration 20740 Training loss 0.00619810214266181 Validation loss 0.011818023398518562 Accuracy 0.87548828125\n",
      "Iteration 20750 Training loss 0.007119977846741676 Validation loss 0.012181092984974384 Accuracy 0.87158203125\n",
      "Iteration 20760 Training loss 0.007822506129741669 Validation loss 0.012569306418299675 Accuracy 0.86767578125\n",
      "Iteration 20770 Training loss 0.007530753966420889 Validation loss 0.01206537801772356 Accuracy 0.87353515625\n",
      "Iteration 20780 Training loss 0.006740823853760958 Validation loss 0.011646205559372902 Accuracy 0.8779296875\n",
      "Iteration 20790 Training loss 0.008442948572337627 Validation loss 0.012385229580104351 Accuracy 0.86962890625\n",
      "Iteration 20800 Training loss 0.005634988192468882 Validation loss 0.011610081419348717 Accuracy 0.87841796875\n",
      "Iteration 20810 Training loss 0.0071283914148807526 Validation loss 0.011805739253759384 Accuracy 0.8759765625\n",
      "Iteration 20820 Training loss 0.0063687036745250225 Validation loss 0.01178622804582119 Accuracy 0.87646484375\n",
      "Iteration 20830 Training loss 0.006866881158202887 Validation loss 0.01184119563549757 Accuracy 0.8759765625\n",
      "Iteration 20840 Training loss 0.006431824993342161 Validation loss 0.011819939129054546 Accuracy 0.87548828125\n",
      "Iteration 20850 Training loss 0.006137824151664972 Validation loss 0.011289136484265327 Accuracy 0.88134765625\n",
      "Iteration 20860 Training loss 0.006795684807002544 Validation loss 0.011264531873166561 Accuracy 0.880859375\n",
      "Iteration 20870 Training loss 0.006099455524235964 Validation loss 0.011662842705845833 Accuracy 0.87744140625\n",
      "Iteration 20880 Training loss 0.007454556878656149 Validation loss 0.011416415683925152 Accuracy 0.8798828125\n",
      "Iteration 20890 Training loss 0.008346877060830593 Validation loss 0.012279821559786797 Accuracy 0.87060546875\n",
      "Iteration 20900 Training loss 0.004572523757815361 Validation loss 0.011728779412806034 Accuracy 0.8759765625\n",
      "Iteration 20910 Training loss 0.008064857684075832 Validation loss 0.011677619069814682 Accuracy 0.876953125\n",
      "Iteration 20920 Training loss 0.005522776860743761 Validation loss 0.011394632048904896 Accuracy 0.8798828125\n",
      "Iteration 20930 Training loss 0.00529453344643116 Validation loss 0.011206453666090965 Accuracy 0.8818359375\n",
      "Iteration 20940 Training loss 0.00609762966632843 Validation loss 0.011474044993519783 Accuracy 0.87939453125\n",
      "Iteration 20950 Training loss 0.008936544880270958 Validation loss 0.011994938366115093 Accuracy 0.873046875\n",
      "Iteration 20960 Training loss 0.007445622701197863 Validation loss 0.011633068323135376 Accuracy 0.8779296875\n",
      "Iteration 20970 Training loss 0.0063543617725372314 Validation loss 0.011690656654536724 Accuracy 0.87646484375\n",
      "Iteration 20980 Training loss 0.007134765386581421 Validation loss 0.011937580071389675 Accuracy 0.87353515625\n",
      "Iteration 20990 Training loss 0.0070445905439555645 Validation loss 0.011394601315259933 Accuracy 0.8798828125\n",
      "Iteration 21000 Training loss 0.005740034859627485 Validation loss 0.011730806902050972 Accuracy 0.87548828125\n",
      "Iteration 21010 Training loss 0.006967534311115742 Validation loss 0.011381813324987888 Accuracy 0.8798828125\n",
      "Iteration 21020 Training loss 0.005727672949433327 Validation loss 0.012485815212130547 Accuracy 0.8681640625\n",
      "Iteration 21030 Training loss 0.005685076117515564 Validation loss 0.011473161168396473 Accuracy 0.87890625\n",
      "Iteration 21040 Training loss 0.00592609541490674 Validation loss 0.01148407906293869 Accuracy 0.8798828125\n",
      "Iteration 21050 Training loss 0.007339395582675934 Validation loss 0.011627458967268467 Accuracy 0.87744140625\n",
      "Iteration 21060 Training loss 0.009000862017273903 Validation loss 0.011921325698494911 Accuracy 0.875\n",
      "Iteration 21070 Training loss 0.007437131367623806 Validation loss 0.011686638928949833 Accuracy 0.876953125\n",
      "Iteration 21080 Training loss 0.008262427523732185 Validation loss 0.01165554579347372 Accuracy 0.8779296875\n",
      "Iteration 21090 Training loss 0.007801157422363758 Validation loss 0.01160862110555172 Accuracy 0.8779296875\n",
      "Iteration 21100 Training loss 0.005707354750484228 Validation loss 0.011384177021682262 Accuracy 0.88037109375\n",
      "Iteration 21110 Training loss 0.007214664947241545 Validation loss 0.012255944311618805 Accuracy 0.87109375\n",
      "Iteration 21120 Training loss 0.007766688708215952 Validation loss 0.011530219577252865 Accuracy 0.87841796875\n",
      "Iteration 21130 Training loss 0.006327853538095951 Validation loss 0.011719447560608387 Accuracy 0.8759765625\n",
      "Iteration 21140 Training loss 0.0056572542525827885 Validation loss 0.011361398734152317 Accuracy 0.880859375\n",
      "Iteration 21150 Training loss 0.006688543129712343 Validation loss 0.01147528551518917 Accuracy 0.87939453125\n",
      "Iteration 21160 Training loss 0.006722385995090008 Validation loss 0.01124102994799614 Accuracy 0.88134765625\n",
      "Iteration 21170 Training loss 0.007178655359894037 Validation loss 0.011479929089546204 Accuracy 0.8798828125\n",
      "Iteration 21180 Training loss 0.008383908309042454 Validation loss 0.011329533532261848 Accuracy 0.880859375\n",
      "Iteration 21190 Training loss 0.007698180619627237 Validation loss 0.011944944970309734 Accuracy 0.87451171875\n",
      "Iteration 21200 Training loss 0.007838508114218712 Validation loss 0.012314794585108757 Accuracy 0.87060546875\n",
      "Iteration 21210 Training loss 0.007203333545476198 Validation loss 0.011670436710119247 Accuracy 0.876953125\n",
      "Iteration 21220 Training loss 0.008291076868772507 Validation loss 0.012074189260601997 Accuracy 0.87255859375\n",
      "Iteration 21230 Training loss 0.007538847625255585 Validation loss 0.01218628603965044 Accuracy 0.87060546875\n",
      "Iteration 21240 Training loss 0.006944649387151003 Validation loss 0.012081527151167393 Accuracy 0.87255859375\n",
      "Iteration 21250 Training loss 0.005592842120677233 Validation loss 0.011538260616362095 Accuracy 0.87939453125\n",
      "Iteration 21260 Training loss 0.008474904112517834 Validation loss 0.011937406845390797 Accuracy 0.8740234375\n",
      "Iteration 21270 Training loss 0.0066657885909080505 Validation loss 0.011547885835170746 Accuracy 0.87890625\n",
      "Iteration 21280 Training loss 0.0058568790555000305 Validation loss 0.011631330475211143 Accuracy 0.87744140625\n",
      "Iteration 21290 Training loss 0.006352369673550129 Validation loss 0.0112537182867527 Accuracy 0.8818359375\n",
      "Iteration 21300 Training loss 0.0057732099667191505 Validation loss 0.01134719792753458 Accuracy 0.880859375\n",
      "Iteration 21310 Training loss 0.007108307909220457 Validation loss 0.01185719296336174 Accuracy 0.87548828125\n",
      "Iteration 21320 Training loss 0.006698837038129568 Validation loss 0.011733978986740112 Accuracy 0.87646484375\n",
      "Iteration 21330 Training loss 0.006191717926412821 Validation loss 0.01209142804145813 Accuracy 0.873046875\n",
      "Iteration 21340 Training loss 0.005807476118206978 Validation loss 0.011378027498722076 Accuracy 0.8798828125\n",
      "Iteration 21350 Training loss 0.00559152290225029 Validation loss 0.011430710554122925 Accuracy 0.87939453125\n",
      "Iteration 21360 Training loss 0.009190069511532784 Validation loss 0.011940049938857555 Accuracy 0.87353515625\n",
      "Iteration 21370 Training loss 0.0059542362578213215 Validation loss 0.011360879987478256 Accuracy 0.8798828125\n",
      "Iteration 21380 Training loss 0.007296995725482702 Validation loss 0.01200046855956316 Accuracy 0.8740234375\n",
      "Iteration 21390 Training loss 0.008269188925623894 Validation loss 0.011766967363655567 Accuracy 0.87646484375\n",
      "Iteration 21400 Training loss 0.006903535220772028 Validation loss 0.011571744456887245 Accuracy 0.87939453125\n",
      "Iteration 21410 Training loss 0.006398222409188747 Validation loss 0.011590500362217426 Accuracy 0.8779296875\n",
      "Iteration 21420 Training loss 0.0056906635873019695 Validation loss 0.011765656061470509 Accuracy 0.8759765625\n",
      "Iteration 21430 Training loss 0.00477992556989193 Validation loss 0.011624060571193695 Accuracy 0.87744140625\n",
      "Iteration 21440 Training loss 0.007037846837192774 Validation loss 0.011979104951024055 Accuracy 0.875\n",
      "Iteration 21450 Training loss 0.005939450580626726 Validation loss 0.011719190515577793 Accuracy 0.876953125\n",
      "Iteration 21460 Training loss 0.005497191566973925 Validation loss 0.011609903536736965 Accuracy 0.8779296875\n",
      "Iteration 21470 Training loss 0.008904078043997288 Validation loss 0.01274939626455307 Accuracy 0.86572265625\n",
      "Iteration 21480 Training loss 0.005493914242833853 Validation loss 0.011202709749341011 Accuracy 0.88232421875\n",
      "Iteration 21490 Training loss 0.004859123378992081 Validation loss 0.01114602293819189 Accuracy 0.88232421875\n",
      "Iteration 21500 Training loss 0.008249271661043167 Validation loss 0.013582802377641201 Accuracy 0.85693359375\n",
      "Iteration 21510 Training loss 0.004936514422297478 Validation loss 0.0114812720566988 Accuracy 0.8779296875\n",
      "Iteration 21520 Training loss 0.006892061326652765 Validation loss 0.011535807512700558 Accuracy 0.87890625\n",
      "Iteration 21530 Training loss 0.006398672703653574 Validation loss 0.011441564187407494 Accuracy 0.87939453125\n",
      "Iteration 21540 Training loss 0.00628669373691082 Validation loss 0.012311508879065514 Accuracy 0.87158203125\n",
      "Iteration 21550 Training loss 0.006962586659938097 Validation loss 0.011891821399331093 Accuracy 0.87548828125\n",
      "Iteration 21560 Training loss 0.005905600264668465 Validation loss 0.012177892960608006 Accuracy 0.87158203125\n",
      "Iteration 21570 Training loss 0.006108326371759176 Validation loss 0.011433462612330914 Accuracy 0.8798828125\n",
      "Iteration 21580 Training loss 0.006224223878234625 Validation loss 0.011861502192914486 Accuracy 0.875\n",
      "Iteration 21590 Training loss 0.004973383154720068 Validation loss 0.011333060450851917 Accuracy 0.880859375\n",
      "Iteration 21600 Training loss 0.006169362459331751 Validation loss 0.011632567271590233 Accuracy 0.87744140625\n",
      "Iteration 21610 Training loss 0.005914566107094288 Validation loss 0.011227443814277649 Accuracy 0.8818359375\n",
      "Iteration 21620 Training loss 0.007495852652937174 Validation loss 0.011364434845745564 Accuracy 0.88037109375\n",
      "Iteration 21630 Training loss 0.006840101908892393 Validation loss 0.011607302352786064 Accuracy 0.87841796875\n",
      "Iteration 21640 Training loss 0.008340508677065372 Validation loss 0.011332012712955475 Accuracy 0.88037109375\n",
      "Iteration 21650 Training loss 0.008813257329165936 Validation loss 0.011347094550728798 Accuracy 0.8798828125\n",
      "Iteration 21660 Training loss 0.007692382670938969 Validation loss 0.011539997532963753 Accuracy 0.87841796875\n",
      "Iteration 21670 Training loss 0.006021992303431034 Validation loss 0.011526759713888168 Accuracy 0.87890625\n",
      "Iteration 21680 Training loss 0.005285362713038921 Validation loss 0.0113442437723279 Accuracy 0.88037109375\n",
      "Iteration 21690 Training loss 0.006025717128068209 Validation loss 0.01135936751961708 Accuracy 0.88037109375\n",
      "Iteration 21700 Training loss 0.0070494296960532665 Validation loss 0.01139053050428629 Accuracy 0.880859375\n",
      "Iteration 21710 Training loss 0.008107578381896019 Validation loss 0.011833393014967442 Accuracy 0.87451171875\n",
      "Iteration 21720 Training loss 0.006121294107288122 Validation loss 0.011819287203252316 Accuracy 0.875\n",
      "Iteration 21730 Training loss 0.0102214515209198 Validation loss 0.011714433319866657 Accuracy 0.87744140625\n",
      "Iteration 21740 Training loss 0.005816176068037748 Validation loss 0.011501840315759182 Accuracy 0.87890625\n",
      "Iteration 21750 Training loss 0.007188587915152311 Validation loss 0.011669527739286423 Accuracy 0.87646484375\n",
      "Iteration 21760 Training loss 0.007332461420446634 Validation loss 0.011405065655708313 Accuracy 0.87939453125\n",
      "Iteration 21770 Training loss 0.006010090932250023 Validation loss 0.011551725678145885 Accuracy 0.8779296875\n",
      "Iteration 21780 Training loss 0.005773287266492844 Validation loss 0.011566807515919209 Accuracy 0.87890625\n",
      "Iteration 21790 Training loss 0.00789420586079359 Validation loss 0.012297326698899269 Accuracy 0.8701171875\n",
      "Iteration 21800 Training loss 0.007521314080804586 Validation loss 0.0116495992988348 Accuracy 0.87841796875\n",
      "Iteration 21810 Training loss 0.006071270443499088 Validation loss 0.011575681157410145 Accuracy 0.87890625\n",
      "Iteration 21820 Training loss 0.006763548590242863 Validation loss 0.011901666410267353 Accuracy 0.8740234375\n",
      "Iteration 21830 Training loss 0.007111165206879377 Validation loss 0.011635707691311836 Accuracy 0.87744140625\n",
      "Iteration 21840 Training loss 0.007064598146826029 Validation loss 0.011684870347380638 Accuracy 0.87744140625\n",
      "Iteration 21850 Training loss 0.006900899112224579 Validation loss 0.011435167863965034 Accuracy 0.8798828125\n",
      "Iteration 21860 Training loss 0.006984742358326912 Validation loss 0.011311415582895279 Accuracy 0.880859375\n",
      "Iteration 21870 Training loss 0.007495645899325609 Validation loss 0.012120900675654411 Accuracy 0.87255859375\n",
      "Iteration 21880 Training loss 0.006689752917736769 Validation loss 0.01150477398186922 Accuracy 0.87841796875\n",
      "Iteration 21890 Training loss 0.007782142609357834 Validation loss 0.012052162550389767 Accuracy 0.8740234375\n",
      "Iteration 21900 Training loss 0.0060417549684643745 Validation loss 0.011261441744863987 Accuracy 0.88134765625\n",
      "Iteration 21910 Training loss 0.006607873365283012 Validation loss 0.011874807067215443 Accuracy 0.87451171875\n",
      "Iteration 21920 Training loss 0.007052567787468433 Validation loss 0.011614159680902958 Accuracy 0.87841796875\n",
      "Iteration 21930 Training loss 0.00917354691773653 Validation loss 0.012731580063700676 Accuracy 0.86669921875\n",
      "Iteration 21940 Training loss 0.004525294061750174 Validation loss 0.011275977827608585 Accuracy 0.8818359375\n",
      "Iteration 21950 Training loss 0.006018310319632292 Validation loss 0.011506891809403896 Accuracy 0.87890625\n",
      "Iteration 21960 Training loss 0.008158715441823006 Validation loss 0.011697784066200256 Accuracy 0.87646484375\n",
      "Iteration 21970 Training loss 0.006711573339998722 Validation loss 0.012067957781255245 Accuracy 0.87158203125\n",
      "Iteration 21980 Training loss 0.007310068234801292 Validation loss 0.011813928373157978 Accuracy 0.8759765625\n",
      "Iteration 21990 Training loss 0.005067609250545502 Validation loss 0.011575629934668541 Accuracy 0.8779296875\n",
      "Iteration 22000 Training loss 0.007485285401344299 Validation loss 0.01186361163854599 Accuracy 0.8740234375\n",
      "Iteration 22010 Training loss 0.007853452116250992 Validation loss 0.01178718265146017 Accuracy 0.8759765625\n",
      "Iteration 22020 Training loss 0.006173360161483288 Validation loss 0.01170973852276802 Accuracy 0.8759765625\n",
      "Iteration 22030 Training loss 0.006322323344647884 Validation loss 0.01171646174043417 Accuracy 0.875\n",
      "Iteration 22040 Training loss 0.00730010075494647 Validation loss 0.011354584246873856 Accuracy 0.87890625\n",
      "Iteration 22050 Training loss 0.006703244522213936 Validation loss 0.011686627753078938 Accuracy 0.87646484375\n",
      "Iteration 22060 Training loss 0.0071447547525167465 Validation loss 0.011411169543862343 Accuracy 0.8798828125\n",
      "Iteration 22070 Training loss 0.0038456039037555456 Validation loss 0.011142470873892307 Accuracy 0.88232421875\n",
      "Iteration 22080 Training loss 0.00509046483784914 Validation loss 0.011343107558786869 Accuracy 0.88037109375\n",
      "Iteration 22090 Training loss 0.0051116603426635265 Validation loss 0.011781793087720871 Accuracy 0.8759765625\n",
      "Iteration 22100 Training loss 0.008982863277196884 Validation loss 0.011820285581052303 Accuracy 0.87548828125\n",
      "Iteration 22110 Training loss 0.006782062351703644 Validation loss 0.011629397049546242 Accuracy 0.876953125\n",
      "Iteration 22120 Training loss 0.006461183074861765 Validation loss 0.011962086893618107 Accuracy 0.87353515625\n",
      "Iteration 22130 Training loss 0.005151183810085058 Validation loss 0.011531169526278973 Accuracy 0.87890625\n",
      "Iteration 22140 Training loss 0.009500261396169662 Validation loss 0.011932404711842537 Accuracy 0.87451171875\n",
      "Iteration 22150 Training loss 0.007461948320269585 Validation loss 0.011115227825939655 Accuracy 0.88232421875\n",
      "Iteration 22160 Training loss 0.0064300899393856525 Validation loss 0.012319513596594334 Accuracy 0.86962890625\n",
      "Iteration 22170 Training loss 0.005360629875212908 Validation loss 0.011281519196927547 Accuracy 0.880859375\n",
      "Iteration 22180 Training loss 0.008816815912723541 Validation loss 0.012223134748637676 Accuracy 0.87109375\n",
      "Iteration 22190 Training loss 0.006340446416288614 Validation loss 0.01136691588908434 Accuracy 0.8798828125\n",
      "Iteration 22200 Training loss 0.006024359259754419 Validation loss 0.011318706907331944 Accuracy 0.880859375\n",
      "Iteration 22210 Training loss 0.007271686568856239 Validation loss 0.011705739423632622 Accuracy 0.8759765625\n",
      "Iteration 22220 Training loss 0.009577176533639431 Validation loss 0.013255920261144638 Accuracy 0.85986328125\n",
      "Iteration 22230 Training loss 0.006197106558829546 Validation loss 0.011490714736282825 Accuracy 0.87841796875\n",
      "Iteration 22240 Training loss 0.006731685716658831 Validation loss 0.011214599013328552 Accuracy 0.88232421875\n",
      "Iteration 22250 Training loss 0.006036300677806139 Validation loss 0.011538698337972164 Accuracy 0.8779296875\n",
      "Iteration 22260 Training loss 0.007353529334068298 Validation loss 0.01129046268761158 Accuracy 0.88134765625\n",
      "Iteration 22270 Training loss 0.006563976872712374 Validation loss 0.011248906143009663 Accuracy 0.88134765625\n",
      "Iteration 22280 Training loss 0.006198596209287643 Validation loss 0.011362974531948566 Accuracy 0.88037109375\n",
      "Iteration 22290 Training loss 0.007464478723704815 Validation loss 0.011573593132197857 Accuracy 0.8779296875\n",
      "Iteration 22300 Training loss 0.0067467051558196545 Validation loss 0.01142838690429926 Accuracy 0.8798828125\n",
      "Iteration 22310 Training loss 0.0051316265016794205 Validation loss 0.011598212644457817 Accuracy 0.87744140625\n",
      "Iteration 22320 Training loss 0.00551370345056057 Validation loss 0.01141311228275299 Accuracy 0.8798828125\n",
      "Iteration 22330 Training loss 0.006915983743965626 Validation loss 0.011278152465820312 Accuracy 0.88134765625\n",
      "Iteration 22340 Training loss 0.004718095064163208 Validation loss 0.011136362329125404 Accuracy 0.88232421875\n",
      "Iteration 22350 Training loss 0.005332102999091148 Validation loss 0.011324831284582615 Accuracy 0.880859375\n",
      "Iteration 22360 Training loss 0.007257335353642702 Validation loss 0.011408663354814053 Accuracy 0.8798828125\n",
      "Iteration 22370 Training loss 0.007481763605028391 Validation loss 0.011461556889116764 Accuracy 0.87939453125\n",
      "Iteration 22380 Training loss 0.008754092268645763 Validation loss 0.01153438538312912 Accuracy 0.87890625\n",
      "Iteration 22390 Training loss 0.007146343123167753 Validation loss 0.011664772406220436 Accuracy 0.876953125\n",
      "Iteration 22400 Training loss 0.004988780245184898 Validation loss 0.011369766667485237 Accuracy 0.88037109375\n",
      "Iteration 22410 Training loss 0.005092863459140062 Validation loss 0.011277562938630581 Accuracy 0.88037109375\n",
      "Iteration 22420 Training loss 0.006883824709802866 Validation loss 0.01118677482008934 Accuracy 0.88134765625\n",
      "Iteration 22430 Training loss 0.00687751779332757 Validation loss 0.011540091596543789 Accuracy 0.8779296875\n",
      "Iteration 22440 Training loss 0.005839757155627012 Validation loss 0.011335006915032864 Accuracy 0.87939453125\n",
      "Iteration 22450 Training loss 0.005835818592458963 Validation loss 0.011806783266365528 Accuracy 0.87548828125\n",
      "Iteration 22460 Training loss 0.008365200832486153 Validation loss 0.012186619453132153 Accuracy 0.87109375\n",
      "Iteration 22470 Training loss 0.006461500655859709 Validation loss 0.011473177000880241 Accuracy 0.87841796875\n",
      "Iteration 22480 Training loss 0.00695995707064867 Validation loss 0.011601803824305534 Accuracy 0.87890625\n",
      "Iteration 22490 Training loss 0.00519119156524539 Validation loss 0.011561362072825432 Accuracy 0.8779296875\n",
      "Iteration 22500 Training loss 0.006442566402256489 Validation loss 0.011596680618822575 Accuracy 0.8779296875\n",
      "Iteration 22510 Training loss 0.005283366423100233 Validation loss 0.011778061278164387 Accuracy 0.8759765625\n",
      "Iteration 22520 Training loss 0.007112703286111355 Validation loss 0.011455628089606762 Accuracy 0.87939453125\n",
      "Iteration 22530 Training loss 0.00718674948439002 Validation loss 0.011516178958117962 Accuracy 0.87841796875\n",
      "Iteration 22540 Training loss 0.007472315337508917 Validation loss 0.012553123757243156 Accuracy 0.8671875\n",
      "Iteration 22550 Training loss 0.008036904968321323 Validation loss 0.011236213147640228 Accuracy 0.8828125\n",
      "Iteration 22560 Training loss 0.003784920321777463 Validation loss 0.011383556760847569 Accuracy 0.8779296875\n",
      "Iteration 22570 Training loss 0.006931766401976347 Validation loss 0.011308911256492138 Accuracy 0.87939453125\n",
      "Iteration 22580 Training loss 0.006544387899339199 Validation loss 0.011534023098647594 Accuracy 0.87841796875\n",
      "Iteration 22590 Training loss 0.007689665537327528 Validation loss 0.011193196289241314 Accuracy 0.8818359375\n",
      "Iteration 22600 Training loss 0.005699389148503542 Validation loss 0.011508532799780369 Accuracy 0.87841796875\n",
      "Iteration 22610 Training loss 0.0046998788602650166 Validation loss 0.011302340775728226 Accuracy 0.88037109375\n",
      "Iteration 22620 Training loss 0.005305654369294643 Validation loss 0.011278335005044937 Accuracy 0.880859375\n",
      "Iteration 22630 Training loss 0.0060925609432160854 Validation loss 0.011773314327001572 Accuracy 0.8759765625\n",
      "Iteration 22640 Training loss 0.005587513092905283 Validation loss 0.01154824998229742 Accuracy 0.8779296875\n",
      "Iteration 22650 Training loss 0.003790859831497073 Validation loss 0.011328835971653461 Accuracy 0.8798828125\n",
      "Iteration 22660 Training loss 0.0069801090285182 Validation loss 0.011367116123437881 Accuracy 0.88037109375\n",
      "Iteration 22670 Training loss 0.005758407525718212 Validation loss 0.011732607148587704 Accuracy 0.87548828125\n",
      "Iteration 22680 Training loss 0.008139865472912788 Validation loss 0.011408703401684761 Accuracy 0.87939453125\n",
      "Iteration 22690 Training loss 0.009560180827975273 Validation loss 0.014180552214384079 Accuracy 0.8515625\n",
      "Iteration 22700 Training loss 0.00577856320887804 Validation loss 0.011616818606853485 Accuracy 0.8779296875\n",
      "Iteration 22710 Training loss 0.005522649735212326 Validation loss 0.011347794905304909 Accuracy 0.88037109375\n",
      "Iteration 22720 Training loss 0.006405968684703112 Validation loss 0.011438670568168163 Accuracy 0.87939453125\n",
      "Iteration 22730 Training loss 0.005879055708646774 Validation loss 0.011477958410978317 Accuracy 0.87890625\n",
      "Iteration 22740 Training loss 0.006773513741791248 Validation loss 0.01164157409220934 Accuracy 0.87744140625\n",
      "Iteration 22750 Training loss 0.006224830634891987 Validation loss 0.011731859296560287 Accuracy 0.876953125\n",
      "Iteration 22760 Training loss 0.004398013930767775 Validation loss 0.011562065221369267 Accuracy 0.8779296875\n",
      "Iteration 22770 Training loss 0.004872904159128666 Validation loss 0.011762261390686035 Accuracy 0.87548828125\n",
      "Iteration 22780 Training loss 0.006891642697155476 Validation loss 0.011401491239666939 Accuracy 0.87939453125\n",
      "Iteration 22790 Training loss 0.005543590523302555 Validation loss 0.011326666921377182 Accuracy 0.88037109375\n",
      "Iteration 22800 Training loss 0.006571026984602213 Validation loss 0.011340485885739326 Accuracy 0.88037109375\n",
      "Iteration 22810 Training loss 0.006080931052565575 Validation loss 0.011500908061861992 Accuracy 0.87939453125\n",
      "Iteration 22820 Training loss 0.0073065729811787605 Validation loss 0.011315295472741127 Accuracy 0.880859375\n",
      "Iteration 22830 Training loss 0.006641559302806854 Validation loss 0.011746341362595558 Accuracy 0.8759765625\n",
      "Iteration 22840 Training loss 0.004758570343255997 Validation loss 0.011570378206670284 Accuracy 0.87841796875\n",
      "Iteration 22850 Training loss 0.00753673305734992 Validation loss 0.01170413289219141 Accuracy 0.87646484375\n",
      "Iteration 22860 Training loss 0.006437886040657759 Validation loss 0.011352374218404293 Accuracy 0.87939453125\n",
      "Iteration 22870 Training loss 0.006129962392151356 Validation loss 0.011511021293699741 Accuracy 0.87890625\n",
      "Iteration 22880 Training loss 0.005415989551693201 Validation loss 0.011207293719053268 Accuracy 0.8818359375\n",
      "Iteration 22890 Training loss 0.005920459516346455 Validation loss 0.011308925226330757 Accuracy 0.8818359375\n",
      "Iteration 22900 Training loss 0.005421197973191738 Validation loss 0.011631619185209274 Accuracy 0.87744140625\n",
      "Iteration 22910 Training loss 0.004064490087330341 Validation loss 0.01171969249844551 Accuracy 0.87646484375\n",
      "Iteration 22920 Training loss 0.005881635472178459 Validation loss 0.011272462084889412 Accuracy 0.88037109375\n",
      "Iteration 22930 Training loss 0.005036083981394768 Validation loss 0.011475580744445324 Accuracy 0.87939453125\n",
      "Iteration 22940 Training loss 0.005985477473586798 Validation loss 0.011367897503077984 Accuracy 0.87939453125\n",
      "Iteration 22950 Training loss 0.007647308986634016 Validation loss 0.011772875674068928 Accuracy 0.8759765625\n",
      "Iteration 22960 Training loss 0.0063715120777487755 Validation loss 0.011843744665384293 Accuracy 0.87548828125\n",
      "Iteration 22970 Training loss 0.008070160634815693 Validation loss 0.011594901792705059 Accuracy 0.8779296875\n",
      "Iteration 22980 Training loss 0.006006844807416201 Validation loss 0.011520031839609146 Accuracy 0.87744140625\n",
      "Iteration 22990 Training loss 0.008404811844229698 Validation loss 0.011704184114933014 Accuracy 0.8759765625\n",
      "Iteration 23000 Training loss 0.00778901157900691 Validation loss 0.011764715425670147 Accuracy 0.87646484375\n",
      "Iteration 23010 Training loss 0.004564139526337385 Validation loss 0.011437159962952137 Accuracy 0.87939453125\n",
      "Iteration 23020 Training loss 0.006847096607089043 Validation loss 0.011690954677760601 Accuracy 0.87646484375\n",
      "Iteration 23030 Training loss 0.008566939271986485 Validation loss 0.011344840750098228 Accuracy 0.880859375\n",
      "Iteration 23040 Training loss 0.006311634089797735 Validation loss 0.011628687381744385 Accuracy 0.87841796875\n",
      "Iteration 23050 Training loss 0.0059472001157701015 Validation loss 0.01159678678959608 Accuracy 0.87890625\n",
      "Iteration 23060 Training loss 0.0043376656249165535 Validation loss 0.01140008494257927 Accuracy 0.87841796875\n",
      "Iteration 23070 Training loss 0.004712016321718693 Validation loss 0.011600231751799583 Accuracy 0.87548828125\n",
      "Iteration 23080 Training loss 0.006567493546754122 Validation loss 0.011214987374842167 Accuracy 0.8818359375\n",
      "Iteration 23090 Training loss 0.006772542372345924 Validation loss 0.011332379654049873 Accuracy 0.88037109375\n",
      "Iteration 23100 Training loss 0.004667033441364765 Validation loss 0.011153693310916424 Accuracy 0.8818359375\n",
      "Iteration 23110 Training loss 0.007460144814103842 Validation loss 0.011195347644388676 Accuracy 0.8828125\n",
      "Iteration 23120 Training loss 0.006997985299676657 Validation loss 0.011189958080649376 Accuracy 0.88232421875\n",
      "Iteration 23130 Training loss 0.006013388279825449 Validation loss 0.011289757676422596 Accuracy 0.880859375\n",
      "Iteration 23140 Training loss 0.005314499139785767 Validation loss 0.011327018961310387 Accuracy 0.880859375\n",
      "Iteration 23150 Training loss 0.007265964988619089 Validation loss 0.01141269039362669 Accuracy 0.87890625\n",
      "Iteration 23160 Training loss 0.007708193268626928 Validation loss 0.011346716433763504 Accuracy 0.88037109375\n",
      "Iteration 23170 Training loss 0.004403707571327686 Validation loss 0.011246266774833202 Accuracy 0.8818359375\n",
      "Iteration 23180 Training loss 0.006600277964025736 Validation loss 0.01237691380083561 Accuracy 0.86962890625\n",
      "Iteration 23190 Training loss 0.0051328083500266075 Validation loss 0.01166363526135683 Accuracy 0.87646484375\n",
      "Iteration 23200 Training loss 0.00818631425499916 Validation loss 0.012051326222717762 Accuracy 0.87255859375\n",
      "Iteration 23210 Training loss 0.006191923748701811 Validation loss 0.011803208850324154 Accuracy 0.87548828125\n",
      "Iteration 23220 Training loss 0.0060570817440748215 Validation loss 0.011330763809382915 Accuracy 0.88037109375\n",
      "Iteration 23230 Training loss 0.005168805830180645 Validation loss 0.011340558528900146 Accuracy 0.87939453125\n",
      "Iteration 23240 Training loss 0.005250024609267712 Validation loss 0.01154242642223835 Accuracy 0.8779296875\n",
      "Iteration 23250 Training loss 0.005953578744083643 Validation loss 0.011515275575220585 Accuracy 0.87841796875\n",
      "Iteration 23260 Training loss 0.004194584675133228 Validation loss 0.01124765444546938 Accuracy 0.880859375\n",
      "Iteration 23270 Training loss 0.003955502063035965 Validation loss 0.011198215186595917 Accuracy 0.8818359375\n",
      "Iteration 23280 Training loss 0.006012941710650921 Validation loss 0.011666483245790005 Accuracy 0.8759765625\n",
      "Iteration 23290 Training loss 0.0059739635325968266 Validation loss 0.011427517980337143 Accuracy 0.87890625\n",
      "Iteration 23300 Training loss 0.005972014274448156 Validation loss 0.012090911157429218 Accuracy 0.87158203125\n",
      "Iteration 23310 Training loss 0.004844483453780413 Validation loss 0.011553146876394749 Accuracy 0.876953125\n",
      "Iteration 23320 Training loss 0.007928543724119663 Validation loss 0.01144914049655199 Accuracy 0.87890625\n",
      "Iteration 23330 Training loss 0.006347058340907097 Validation loss 0.01122279278934002 Accuracy 0.88134765625\n",
      "Iteration 23340 Training loss 0.003964012023061514 Validation loss 0.011337313801050186 Accuracy 0.87939453125\n",
      "Iteration 23350 Training loss 0.006573000457137823 Validation loss 0.011642209254205227 Accuracy 0.876953125\n",
      "Iteration 23360 Training loss 0.005135662388056517 Validation loss 0.011208991520106792 Accuracy 0.88134765625\n",
      "Iteration 23370 Training loss 0.008536768145859241 Validation loss 0.011725779622793198 Accuracy 0.87548828125\n",
      "Iteration 23380 Training loss 0.007211880758404732 Validation loss 0.01166341919451952 Accuracy 0.876953125\n",
      "Iteration 23390 Training loss 0.0049354941584169865 Validation loss 0.011201185174286366 Accuracy 0.880859375\n",
      "Iteration 23400 Training loss 0.007386564742773771 Validation loss 0.012160449288785458 Accuracy 0.87158203125\n",
      "Iteration 23410 Training loss 0.006168826948851347 Validation loss 0.011344562284648418 Accuracy 0.8798828125\n",
      "Iteration 23420 Training loss 0.004290047101676464 Validation loss 0.011353116482496262 Accuracy 0.87939453125\n",
      "Iteration 23430 Training loss 0.005188977345824242 Validation loss 0.01151090394705534 Accuracy 0.87890625\n",
      "Iteration 23440 Training loss 0.004756886046379805 Validation loss 0.011369041167199612 Accuracy 0.87939453125\n",
      "Iteration 23450 Training loss 0.0058000716380774975 Validation loss 0.011343976482748985 Accuracy 0.880859375\n",
      "Iteration 23460 Training loss 0.005199664272367954 Validation loss 0.011467025615274906 Accuracy 0.8798828125\n",
      "Iteration 23470 Training loss 0.006534601096063852 Validation loss 0.011190755292773247 Accuracy 0.88134765625\n",
      "Iteration 23480 Training loss 0.006754857487976551 Validation loss 0.011778159067034721 Accuracy 0.87548828125\n",
      "Iteration 23490 Training loss 0.005117663647979498 Validation loss 0.011203691363334656 Accuracy 0.8828125\n",
      "Iteration 23500 Training loss 0.0075325132347643375 Validation loss 0.01140100322663784 Accuracy 0.87841796875\n",
      "Iteration 23510 Training loss 0.005691662896424532 Validation loss 0.011482122354209423 Accuracy 0.87841796875\n",
      "Iteration 23520 Training loss 0.0058796461671590805 Validation loss 0.011750828474760056 Accuracy 0.87548828125\n",
      "Iteration 23530 Training loss 0.005703585222363472 Validation loss 0.011203168891370296 Accuracy 0.88134765625\n",
      "Iteration 23540 Training loss 0.007033621426671743 Validation loss 0.011309808120131493 Accuracy 0.88037109375\n",
      "Iteration 23550 Training loss 0.004774114117026329 Validation loss 0.011084910482168198 Accuracy 0.88232421875\n",
      "Iteration 23560 Training loss 0.008030342869460583 Validation loss 0.011959929019212723 Accuracy 0.87451171875\n",
      "Iteration 23570 Training loss 0.005448942072689533 Validation loss 0.011593746952712536 Accuracy 0.8759765625\n",
      "Iteration 23580 Training loss 0.0052994596771895885 Validation loss 0.01115456409752369 Accuracy 0.8818359375\n",
      "Iteration 23590 Training loss 0.004599258303642273 Validation loss 0.011422308161854744 Accuracy 0.87890625\n",
      "Iteration 23600 Training loss 0.007297387346625328 Validation loss 0.012015104293823242 Accuracy 0.873046875\n",
      "Iteration 23610 Training loss 0.007341365329921246 Validation loss 0.01137690432369709 Accuracy 0.87939453125\n",
      "Iteration 23620 Training loss 0.007268806453794241 Validation loss 0.011259239166975021 Accuracy 0.88037109375\n",
      "Iteration 23630 Training loss 0.006423856131732464 Validation loss 0.01135995052754879 Accuracy 0.87939453125\n",
      "Iteration 23640 Training loss 0.004320111125707626 Validation loss 0.011463739909231663 Accuracy 0.87890625\n",
      "Iteration 23650 Training loss 0.00487100426107645 Validation loss 0.011425409466028214 Accuracy 0.8798828125\n",
      "Iteration 23660 Training loss 0.006625285837799311 Validation loss 0.011447597295045853 Accuracy 0.87841796875\n",
      "Iteration 23670 Training loss 0.0062606073915958405 Validation loss 0.011387133039534092 Accuracy 0.87939453125\n",
      "Iteration 23680 Training loss 0.005451090633869171 Validation loss 0.011278693564236164 Accuracy 0.88037109375\n",
      "Iteration 23690 Training loss 0.006203757133334875 Validation loss 0.011793903075158596 Accuracy 0.87548828125\n",
      "Iteration 23700 Training loss 0.005534776020795107 Validation loss 0.011384783312678337 Accuracy 0.8798828125\n",
      "Iteration 23710 Training loss 0.0050302487798035145 Validation loss 0.011410290375351906 Accuracy 0.87939453125\n",
      "Iteration 23720 Training loss 0.005219542887061834 Validation loss 0.011386530473828316 Accuracy 0.8798828125\n",
      "Iteration 23730 Training loss 0.0055654626339674 Validation loss 0.01152213104069233 Accuracy 0.8779296875\n",
      "Iteration 23740 Training loss 0.007366048637777567 Validation loss 0.01175668928772211 Accuracy 0.875\n",
      "Iteration 23750 Training loss 0.006382203195244074 Validation loss 0.011330908164381981 Accuracy 0.8798828125\n",
      "Iteration 23760 Training loss 0.005933166481554508 Validation loss 0.011697525158524513 Accuracy 0.8759765625\n",
      "Iteration 23770 Training loss 0.004572635050863028 Validation loss 0.011859237216413021 Accuracy 0.87451171875\n",
      "Iteration 23780 Training loss 0.0053463527001440525 Validation loss 0.011266598477959633 Accuracy 0.8818359375\n",
      "Iteration 23790 Training loss 0.007053145207464695 Validation loss 0.011848862282931805 Accuracy 0.87451171875\n",
      "Iteration 23800 Training loss 0.004727391991764307 Validation loss 0.011241584084928036 Accuracy 0.88037109375\n",
      "Iteration 23810 Training loss 0.0055258674547076225 Validation loss 0.011493390426039696 Accuracy 0.8779296875\n",
      "Iteration 23820 Training loss 0.003462341846898198 Validation loss 0.011372409760951996 Accuracy 0.880859375\n",
      "Iteration 23830 Training loss 0.006048567593097687 Validation loss 0.011503560468554497 Accuracy 0.87841796875\n",
      "Iteration 23840 Training loss 0.0056546833366155624 Validation loss 0.011398116126656532 Accuracy 0.87939453125\n",
      "Iteration 23850 Training loss 0.006019349209964275 Validation loss 0.011301911436021328 Accuracy 0.880859375\n",
      "Iteration 23860 Training loss 0.004974715411663055 Validation loss 0.011605174280703068 Accuracy 0.8779296875\n",
      "Iteration 23870 Training loss 0.007296114694327116 Validation loss 0.01123534794896841 Accuracy 0.880859375\n",
      "Iteration 23880 Training loss 0.007011507637798786 Validation loss 0.011377601884305477 Accuracy 0.8798828125\n",
      "Iteration 23890 Training loss 0.005189636722207069 Validation loss 0.010970031842589378 Accuracy 0.884765625\n",
      "Iteration 23900 Training loss 0.004924365784972906 Validation loss 0.011244083754718304 Accuracy 0.88134765625\n",
      "Iteration 23910 Training loss 0.004892606753855944 Validation loss 0.01113271713256836 Accuracy 0.88134765625\n",
      "Iteration 23920 Training loss 0.006860149092972279 Validation loss 0.01133290957659483 Accuracy 0.88037109375\n",
      "Iteration 23930 Training loss 0.00637606019154191 Validation loss 0.011268538422882557 Accuracy 0.880859375\n",
      "Iteration 23940 Training loss 0.005752298515290022 Validation loss 0.011028463952243328 Accuracy 0.8828125\n",
      "Iteration 23950 Training loss 0.007160963024944067 Validation loss 0.011098158545792103 Accuracy 0.8828125\n",
      "Iteration 23960 Training loss 0.005478359293192625 Validation loss 0.01132172904908657 Accuracy 0.88037109375\n",
      "Iteration 23970 Training loss 0.006752896588295698 Validation loss 0.011695807799696922 Accuracy 0.87646484375\n",
      "Iteration 23980 Training loss 0.006746791768819094 Validation loss 0.011419896967709064 Accuracy 0.87890625\n",
      "Iteration 23990 Training loss 0.00644800066947937 Validation loss 0.011848402209579945 Accuracy 0.87548828125\n",
      "Iteration 24000 Training loss 0.004600547719746828 Validation loss 0.011591295711696148 Accuracy 0.8779296875\n",
      "Iteration 24010 Training loss 0.005347772501409054 Validation loss 0.011154791340231895 Accuracy 0.8818359375\n",
      "Iteration 24020 Training loss 0.007756727747619152 Validation loss 0.011960526928305626 Accuracy 0.8740234375\n",
      "Iteration 24030 Training loss 0.005816158372908831 Validation loss 0.011452562175691128 Accuracy 0.87890625\n",
      "Iteration 24040 Training loss 0.005318488925695419 Validation loss 0.011430365964770317 Accuracy 0.87939453125\n",
      "Iteration 24050 Training loss 0.006230452563613653 Validation loss 0.011641977354884148 Accuracy 0.876953125\n",
      "Iteration 24060 Training loss 0.00643776124343276 Validation loss 0.01125511433929205 Accuracy 0.88037109375\n",
      "Iteration 24070 Training loss 0.004783631768077612 Validation loss 0.011243323795497417 Accuracy 0.880859375\n",
      "Iteration 24080 Training loss 0.005814108531922102 Validation loss 0.011286857537925243 Accuracy 0.87939453125\n",
      "Iteration 24090 Training loss 0.0073339669033885 Validation loss 0.011347121559083462 Accuracy 0.8798828125\n",
      "Iteration 24100 Training loss 0.005229657981544733 Validation loss 0.011327543295919895 Accuracy 0.8798828125\n",
      "Iteration 24110 Training loss 0.006173672154545784 Validation loss 0.011108212172985077 Accuracy 0.88330078125\n",
      "Iteration 24120 Training loss 0.0052602761425077915 Validation loss 0.010860887356102467 Accuracy 0.88623046875\n",
      "Iteration 24130 Training loss 0.004802832845598459 Validation loss 0.011321193538606167 Accuracy 0.88037109375\n",
      "Iteration 24140 Training loss 0.006070921663194895 Validation loss 0.011300421319901943 Accuracy 0.88037109375\n",
      "Iteration 24150 Training loss 0.00474745174869895 Validation loss 0.011455628089606762 Accuracy 0.87890625\n",
      "Iteration 24160 Training loss 0.005945087876170874 Validation loss 0.011613612063229084 Accuracy 0.8759765625\n",
      "Iteration 24170 Training loss 0.005535897333174944 Validation loss 0.011692618019878864 Accuracy 0.876953125\n",
      "Iteration 24180 Training loss 0.006502246949821711 Validation loss 0.0110784862190485 Accuracy 0.8828125\n",
      "Iteration 24190 Training loss 0.005791733507066965 Validation loss 0.01093988586217165 Accuracy 0.8837890625\n",
      "Iteration 24200 Training loss 0.003597254166379571 Validation loss 0.011180775240063667 Accuracy 0.880859375\n",
      "Iteration 24210 Training loss 0.006683776620775461 Validation loss 0.011190441437065601 Accuracy 0.88134765625\n",
      "Iteration 24220 Training loss 0.0065296851098537445 Validation loss 0.011382775381207466 Accuracy 0.8798828125\n",
      "Iteration 24230 Training loss 0.005770583637058735 Validation loss 0.010939422063529491 Accuracy 0.8837890625\n",
      "Iteration 24240 Training loss 0.00565903028473258 Validation loss 0.011198075488209724 Accuracy 0.88134765625\n",
      "Iteration 24250 Training loss 0.004757055547088385 Validation loss 0.011925515718758106 Accuracy 0.8740234375\n",
      "Iteration 24260 Training loss 0.0059519256465137005 Validation loss 0.011331713758409023 Accuracy 0.88037109375\n",
      "Iteration 24270 Training loss 0.0052511016838252544 Validation loss 0.01155292708426714 Accuracy 0.8779296875\n",
      "Iteration 24280 Training loss 0.005878634750843048 Validation loss 0.011467362754046917 Accuracy 0.87841796875\n",
      "Iteration 24290 Training loss 0.006556034553796053 Validation loss 0.011107392609119415 Accuracy 0.8828125\n",
      "Iteration 24300 Training loss 0.006433095782995224 Validation loss 0.011909694410860538 Accuracy 0.8740234375\n",
      "Iteration 24310 Training loss 0.004876669030636549 Validation loss 0.011190858669579029 Accuracy 0.88232421875\n",
      "Iteration 24320 Training loss 0.006349456496536732 Validation loss 0.011711006984114647 Accuracy 0.87744140625\n",
      "Iteration 24330 Training loss 0.006381376646459103 Validation loss 0.011255266144871712 Accuracy 0.880859375\n",
      "Iteration 24340 Training loss 0.0050158314406871796 Validation loss 0.011337419040501118 Accuracy 0.88037109375\n",
      "Iteration 24350 Training loss 0.006359962280839682 Validation loss 0.011160004884004593 Accuracy 0.8818359375\n",
      "Iteration 24360 Training loss 0.006078408565372229 Validation loss 0.01124454103410244 Accuracy 0.8818359375\n",
      "Iteration 24370 Training loss 0.005143311340361834 Validation loss 0.011175860650837421 Accuracy 0.8818359375\n",
      "Iteration 24380 Training loss 0.004692140966653824 Validation loss 0.010857343673706055 Accuracy 0.88623046875\n",
      "Iteration 24390 Training loss 0.005107086151838303 Validation loss 0.010933826677501202 Accuracy 0.884765625\n",
      "Iteration 24400 Training loss 0.0071876090951263905 Validation loss 0.011418454349040985 Accuracy 0.88037109375\n",
      "Iteration 24410 Training loss 0.005629583727568388 Validation loss 0.01110456045717001 Accuracy 0.880859375\n",
      "Iteration 24420 Training loss 0.0069784140214324 Validation loss 0.011559340171515942 Accuracy 0.8779296875\n",
      "Iteration 24430 Training loss 0.005936130881309509 Validation loss 0.011250772513449192 Accuracy 0.880859375\n",
      "Iteration 24440 Training loss 0.005385276861488819 Validation loss 0.011202231980860233 Accuracy 0.880859375\n",
      "Iteration 24450 Training loss 0.0034586472902446985 Validation loss 0.011250670999288559 Accuracy 0.8818359375\n",
      "Iteration 24460 Training loss 0.006620398722589016 Validation loss 0.011058148927986622 Accuracy 0.8828125\n",
      "Iteration 24470 Training loss 0.006630444899201393 Validation loss 0.011320756748318672 Accuracy 0.88037109375\n",
      "Iteration 24480 Training loss 0.005375719629228115 Validation loss 0.011251899413764477 Accuracy 0.880859375\n",
      "Iteration 24490 Training loss 0.006084328982979059 Validation loss 0.011305754072964191 Accuracy 0.88037109375\n",
      "Iteration 24500 Training loss 0.005001165438443422 Validation loss 0.011503363959491253 Accuracy 0.8779296875\n",
      "Iteration 24510 Training loss 0.004705867730081081 Validation loss 0.011132074519991875 Accuracy 0.8818359375\n",
      "Iteration 24520 Training loss 0.004893324337899685 Validation loss 0.010910222306847572 Accuracy 0.88427734375\n",
      "Iteration 24530 Training loss 0.006098881829530001 Validation loss 0.011230863630771637 Accuracy 0.880859375\n",
      "Iteration 24540 Training loss 0.007306706625968218 Validation loss 0.01096031442284584 Accuracy 0.8837890625\n",
      "Iteration 24550 Training loss 0.007234023418277502 Validation loss 0.010881594382226467 Accuracy 0.8837890625\n",
      "Iteration 24560 Training loss 0.00581017741933465 Validation loss 0.011154109612107277 Accuracy 0.880859375\n",
      "Iteration 24570 Training loss 0.00519912876188755 Validation loss 0.011261220090091228 Accuracy 0.88037109375\n",
      "Iteration 24580 Training loss 0.006934393662959337 Validation loss 0.011216940358281136 Accuracy 0.880859375\n",
      "Iteration 24590 Training loss 0.007538145408034325 Validation loss 0.011973625048995018 Accuracy 0.87255859375\n",
      "Iteration 24600 Training loss 0.005467874929308891 Validation loss 0.011123981326818466 Accuracy 0.8818359375\n",
      "Iteration 24610 Training loss 0.007115209475159645 Validation loss 0.011251840740442276 Accuracy 0.880859375\n",
      "Iteration 24620 Training loss 0.004677434451878071 Validation loss 0.011349259875714779 Accuracy 0.87890625\n",
      "Iteration 24630 Training loss 0.006456897594034672 Validation loss 0.01152073498815298 Accuracy 0.87890625\n",
      "Iteration 24640 Training loss 0.005051294341683388 Validation loss 0.011365729384124279 Accuracy 0.87939453125\n",
      "Iteration 24650 Training loss 0.005621397402137518 Validation loss 0.011439790017902851 Accuracy 0.87890625\n",
      "Iteration 24660 Training loss 0.00570332258939743 Validation loss 0.011264672502875328 Accuracy 0.880859375\n",
      "Iteration 24670 Training loss 0.004387849010527134 Validation loss 0.011243246495723724 Accuracy 0.88134765625\n",
      "Iteration 24680 Training loss 0.004713596310466528 Validation loss 0.011182179674506187 Accuracy 0.8818359375\n",
      "Iteration 24690 Training loss 0.0059768143109977245 Validation loss 0.010827950201928616 Accuracy 0.88525390625\n",
      "Iteration 24700 Training loss 0.006374781019985676 Validation loss 0.011497315019369125 Accuracy 0.87890625\n",
      "Iteration 24710 Training loss 0.005403994116932154 Validation loss 0.010956876911222935 Accuracy 0.884765625\n",
      "Iteration 24720 Training loss 0.0069570583291351795 Validation loss 0.01170971617102623 Accuracy 0.8759765625\n",
      "Iteration 24730 Training loss 0.00513302581384778 Validation loss 0.011206962168216705 Accuracy 0.88134765625\n",
      "Iteration 24740 Training loss 0.006398025434464216 Validation loss 0.011233515106141567 Accuracy 0.88134765625\n",
      "Iteration 24750 Training loss 0.00634120823815465 Validation loss 0.011496801860630512 Accuracy 0.87841796875\n",
      "Iteration 24760 Training loss 0.0066696940921247005 Validation loss 0.011293971911072731 Accuracy 0.88037109375\n",
      "Iteration 24770 Training loss 0.0068624489940702915 Validation loss 0.01143475342541933 Accuracy 0.87890625\n",
      "Iteration 24780 Training loss 0.004690384026616812 Validation loss 0.011169237084686756 Accuracy 0.8818359375\n",
      "Iteration 24790 Training loss 0.004987700842320919 Validation loss 0.011510219424962997 Accuracy 0.8779296875\n",
      "Iteration 24800 Training loss 0.006561496760696173 Validation loss 0.011158288456499577 Accuracy 0.880859375\n",
      "Iteration 24810 Training loss 0.004457561299204826 Validation loss 0.011249467730522156 Accuracy 0.8798828125\n",
      "Iteration 24820 Training loss 0.004281299654394388 Validation loss 0.011469386518001556 Accuracy 0.87744140625\n",
      "Iteration 24830 Training loss 0.005192657001316547 Validation loss 0.011403136886656284 Accuracy 0.87939453125\n",
      "Iteration 24840 Training loss 0.0049584731459617615 Validation loss 0.011020155623555183 Accuracy 0.88330078125\n",
      "Iteration 24850 Training loss 0.007756985258311033 Validation loss 0.01181689091026783 Accuracy 0.87451171875\n",
      "Iteration 24860 Training loss 0.0060426462441682816 Validation loss 0.011345912702381611 Accuracy 0.87939453125\n",
      "Iteration 24870 Training loss 0.0065607032738626 Validation loss 0.011302878148853779 Accuracy 0.8798828125\n",
      "Iteration 24880 Training loss 0.006784564815461636 Validation loss 0.011342334561049938 Accuracy 0.8798828125\n",
      "Iteration 24890 Training loss 0.0064873467199504375 Validation loss 0.012021845206618309 Accuracy 0.87255859375\n",
      "Iteration 24900 Training loss 0.00394819863140583 Validation loss 0.01116734929382801 Accuracy 0.88134765625\n",
      "Iteration 24910 Training loss 0.006282932125031948 Validation loss 0.011067809537053108 Accuracy 0.88330078125\n",
      "Iteration 24920 Training loss 0.005817696917802095 Validation loss 0.01136422622948885 Accuracy 0.87890625\n",
      "Iteration 24930 Training loss 0.005348519422113895 Validation loss 0.011214353144168854 Accuracy 0.88134765625\n",
      "Iteration 24940 Training loss 0.006258323788642883 Validation loss 0.011654943227767944 Accuracy 0.87646484375\n",
      "Iteration 24950 Training loss 0.005790241993963718 Validation loss 0.01099538803100586 Accuracy 0.88232421875\n",
      "Iteration 24960 Training loss 0.0064511289820075035 Validation loss 0.010946615599095821 Accuracy 0.884765625\n",
      "Iteration 24970 Training loss 0.005893938709050417 Validation loss 0.011129393242299557 Accuracy 0.8818359375\n",
      "Iteration 24980 Training loss 0.005687374621629715 Validation loss 0.011199058964848518 Accuracy 0.88134765625\n",
      "Iteration 24990 Training loss 0.007528623566031456 Validation loss 0.011249669827520847 Accuracy 0.880859375\n",
      "Iteration 25000 Training loss 0.006372474599629641 Validation loss 0.011518171988427639 Accuracy 0.87744140625\n",
      "Iteration 25010 Training loss 0.005812598392367363 Validation loss 0.010896340012550354 Accuracy 0.8837890625\n",
      "Iteration 25020 Training loss 0.005201444029808044 Validation loss 0.011020154692232609 Accuracy 0.8828125\n",
      "Iteration 25030 Training loss 0.005161607638001442 Validation loss 0.011217092163860798 Accuracy 0.8798828125\n",
      "Iteration 25040 Training loss 0.00522985216230154 Validation loss 0.01132748182862997 Accuracy 0.8798828125\n",
      "Iteration 25050 Training loss 0.005143207497894764 Validation loss 0.010864474810659885 Accuracy 0.88427734375\n",
      "Iteration 25060 Training loss 0.004263146780431271 Validation loss 0.01152748428285122 Accuracy 0.87744140625\n",
      "Iteration 25070 Training loss 0.005575637798756361 Validation loss 0.011916354298591614 Accuracy 0.8740234375\n",
      "Iteration 25080 Training loss 0.005352507345378399 Validation loss 0.011151948943734169 Accuracy 0.8818359375\n",
      "Iteration 25090 Training loss 0.007237199228256941 Validation loss 0.011078915558755398 Accuracy 0.88232421875\n",
      "Iteration 25100 Training loss 0.006141869816929102 Validation loss 0.011096864007413387 Accuracy 0.88330078125\n",
      "Iteration 25110 Training loss 0.005540969781577587 Validation loss 0.01104117650538683 Accuracy 0.88330078125\n",
      "Iteration 25120 Training loss 0.005363295786082745 Validation loss 0.010961120016872883 Accuracy 0.88330078125\n",
      "Iteration 25130 Training loss 0.0031245702411979437 Validation loss 0.011252478696405888 Accuracy 0.880859375\n",
      "Iteration 25140 Training loss 0.00584949366748333 Validation loss 0.011077575385570526 Accuracy 0.8818359375\n",
      "Iteration 25150 Training loss 0.005403559189289808 Validation loss 0.010997326113283634 Accuracy 0.8837890625\n",
      "Iteration 25160 Training loss 0.005666601937264204 Validation loss 0.0118342200294137 Accuracy 0.87451171875\n",
      "Iteration 25170 Training loss 0.005334780085831881 Validation loss 0.011360883712768555 Accuracy 0.8798828125\n",
      "Iteration 25180 Training loss 0.005386426113545895 Validation loss 0.011340227909386158 Accuracy 0.87939453125\n",
      "Iteration 25190 Training loss 0.0068260664120316505 Validation loss 0.011340086348354816 Accuracy 0.8798828125\n",
      "Iteration 25200 Training loss 0.005912848748266697 Validation loss 0.011365678161382675 Accuracy 0.87939453125\n",
      "Iteration 25210 Training loss 0.004970676265656948 Validation loss 0.01136048510670662 Accuracy 0.87841796875\n",
      "Iteration 25220 Training loss 0.005916281137615442 Validation loss 0.011178278364241123 Accuracy 0.8818359375\n",
      "Iteration 25230 Training loss 0.005204773973673582 Validation loss 0.012113810516893864 Accuracy 0.8720703125\n",
      "Iteration 25240 Training loss 0.004768876358866692 Validation loss 0.01114142406731844 Accuracy 0.88330078125\n",
      "Iteration 25250 Training loss 0.007157471030950546 Validation loss 0.011250726878643036 Accuracy 0.8818359375\n",
      "Iteration 25260 Training loss 0.004990353714674711 Validation loss 0.011426370590925217 Accuracy 0.87939453125\n",
      "Iteration 25270 Training loss 0.006404608488082886 Validation loss 0.011563141830265522 Accuracy 0.8779296875\n",
      "Iteration 25280 Training loss 0.004443135112524033 Validation loss 0.011299023404717445 Accuracy 0.88134765625\n",
      "Iteration 25290 Training loss 0.006757747381925583 Validation loss 0.011300874873995781 Accuracy 0.88037109375\n",
      "Iteration 25300 Training loss 0.004425133112818003 Validation loss 0.011265002191066742 Accuracy 0.88037109375\n",
      "Iteration 25310 Training loss 0.0048441109247505665 Validation loss 0.011090031825006008 Accuracy 0.8828125\n",
      "Iteration 25320 Training loss 0.007293534930795431 Validation loss 0.010879768058657646 Accuracy 0.88525390625\n",
      "Iteration 25330 Training loss 0.00572267547249794 Validation loss 0.011220204643905163 Accuracy 0.8818359375\n",
      "Iteration 25340 Training loss 0.004110709298402071 Validation loss 0.011122814379632473 Accuracy 0.8828125\n",
      "Iteration 25350 Training loss 0.004745895508676767 Validation loss 0.011193771846592426 Accuracy 0.8818359375\n",
      "Iteration 25360 Training loss 0.004625602625310421 Validation loss 0.010953725315630436 Accuracy 0.88427734375\n",
      "Iteration 25370 Training loss 0.005491004791110754 Validation loss 0.011294455267488956 Accuracy 0.880859375\n",
      "Iteration 25380 Training loss 0.005148274824023247 Validation loss 0.01135250460356474 Accuracy 0.8798828125\n",
      "Iteration 25390 Training loss 0.006250485777854919 Validation loss 0.011068466119468212 Accuracy 0.8828125\n",
      "Iteration 25400 Training loss 0.005206177476793528 Validation loss 0.011273227632045746 Accuracy 0.880859375\n",
      "Iteration 25410 Training loss 0.006654779426753521 Validation loss 0.01134247612208128 Accuracy 0.88037109375\n",
      "Iteration 25420 Training loss 0.005071216728538275 Validation loss 0.011544787324965 Accuracy 0.87744140625\n",
      "Iteration 25430 Training loss 0.004198166541755199 Validation loss 0.01134578324854374 Accuracy 0.87939453125\n",
      "Iteration 25440 Training loss 0.004843488801270723 Validation loss 0.011750133708119392 Accuracy 0.87451171875\n",
      "Iteration 25450 Training loss 0.0077122291550040245 Validation loss 0.011586007662117481 Accuracy 0.876953125\n",
      "Iteration 25460 Training loss 0.004576416220515966 Validation loss 0.011207750998437405 Accuracy 0.880859375\n",
      "Iteration 25470 Training loss 0.004688891116529703 Validation loss 0.011004908010363579 Accuracy 0.8828125\n",
      "Iteration 25480 Training loss 0.0047404649667441845 Validation loss 0.010985798202455044 Accuracy 0.8828125\n",
      "Iteration 25490 Training loss 0.005313475616276264 Validation loss 0.01116293203085661 Accuracy 0.88037109375\n",
      "Iteration 25500 Training loss 0.007574823219329119 Validation loss 0.011430444195866585 Accuracy 0.8798828125\n",
      "Iteration 25510 Training loss 0.005887867417186499 Validation loss 0.011178957298398018 Accuracy 0.8818359375\n",
      "Iteration 25520 Training loss 0.005171956494450569 Validation loss 0.010913505218923092 Accuracy 0.88427734375\n",
      "Iteration 25530 Training loss 0.005009529180824757 Validation loss 0.011064340360462666 Accuracy 0.88232421875\n",
      "Iteration 25540 Training loss 0.006971403490751982 Validation loss 0.01147589460015297 Accuracy 0.8779296875\n",
      "Iteration 25550 Training loss 0.005972057115286589 Validation loss 0.011288043111562729 Accuracy 0.8798828125\n",
      "Iteration 25560 Training loss 0.005165891721844673 Validation loss 0.011239930987358093 Accuracy 0.88134765625\n",
      "Iteration 25570 Training loss 0.006078050471842289 Validation loss 0.01115898322314024 Accuracy 0.88134765625\n",
      "Iteration 25580 Training loss 0.005242117680609226 Validation loss 0.01095866784453392 Accuracy 0.8837890625\n",
      "Iteration 25590 Training loss 0.006490897387266159 Validation loss 0.011137858964502811 Accuracy 0.8818359375\n",
      "Iteration 25600 Training loss 0.004758454393595457 Validation loss 0.011024235747754574 Accuracy 0.8818359375\n",
      "Iteration 25610 Training loss 0.006550464779138565 Validation loss 0.011048915795981884 Accuracy 0.88232421875\n",
      "Iteration 25620 Training loss 0.005693239625543356 Validation loss 0.011000175960361958 Accuracy 0.88330078125\n",
      "Iteration 25630 Training loss 0.006360976956784725 Validation loss 0.010835091583430767 Accuracy 0.88427734375\n",
      "Iteration 25640 Training loss 0.005718565545976162 Validation loss 0.011215523816645145 Accuracy 0.8798828125\n",
      "Iteration 25650 Training loss 0.004883980844169855 Validation loss 0.011080966331064701 Accuracy 0.8818359375\n",
      "Iteration 25660 Training loss 0.006239467766135931 Validation loss 0.011225695721805096 Accuracy 0.88037109375\n",
      "Iteration 25670 Training loss 0.005310805048793554 Validation loss 0.010907917283475399 Accuracy 0.8837890625\n",
      "Iteration 25680 Training loss 0.0059577603824436665 Validation loss 0.011557218618690968 Accuracy 0.876953125\n",
      "Iteration 25690 Training loss 0.005767494905740023 Validation loss 0.011411735787987709 Accuracy 0.87841796875\n",
      "Iteration 25700 Training loss 0.008686322718858719 Validation loss 0.011248796246945858 Accuracy 0.880859375\n",
      "Iteration 25710 Training loss 0.0072362045757472515 Validation loss 0.011104545556008816 Accuracy 0.8828125\n",
      "Iteration 25720 Training loss 0.004662193823605776 Validation loss 0.011301998980343342 Accuracy 0.8798828125\n",
      "Iteration 25730 Training loss 0.006207944359630346 Validation loss 0.012193085625767708 Accuracy 0.8701171875\n",
      "Iteration 25740 Training loss 0.005679024383425713 Validation loss 0.011097900569438934 Accuracy 0.8818359375\n",
      "Iteration 25750 Training loss 0.004614075180143118 Validation loss 0.011132831685245037 Accuracy 0.88232421875\n",
      "Iteration 25760 Training loss 0.005683566443622112 Validation loss 0.01104439701884985 Accuracy 0.88330078125\n",
      "Iteration 25770 Training loss 0.007741156034171581 Validation loss 0.011660926043987274 Accuracy 0.876953125\n",
      "Iteration 25780 Training loss 0.005223811138421297 Validation loss 0.01092491764575243 Accuracy 0.88330078125\n",
      "Iteration 25790 Training loss 0.006336522288620472 Validation loss 0.011233210563659668 Accuracy 0.880859375\n",
      "Iteration 25800 Training loss 0.005423230584710836 Validation loss 0.011585223488509655 Accuracy 0.87890625\n",
      "Iteration 25810 Training loss 0.0053069400601089 Validation loss 0.01092914491891861 Accuracy 0.88427734375\n",
      "Iteration 25820 Training loss 0.00593979237601161 Validation loss 0.011164813302457333 Accuracy 0.88134765625\n",
      "Iteration 25830 Training loss 0.006608627270907164 Validation loss 0.011214611120522022 Accuracy 0.88037109375\n",
      "Iteration 25840 Training loss 0.005395283456891775 Validation loss 0.011462738737463951 Accuracy 0.8779296875\n",
      "Iteration 25850 Training loss 0.00550390500575304 Validation loss 0.011051804758608341 Accuracy 0.8818359375\n",
      "Iteration 25860 Training loss 0.0049636103212833405 Validation loss 0.011079523712396622 Accuracy 0.88330078125\n",
      "Iteration 25870 Training loss 0.00479381438344717 Validation loss 0.010934868827462196 Accuracy 0.8828125\n",
      "Iteration 25880 Training loss 0.005202895496040583 Validation loss 0.010883975774049759 Accuracy 0.884765625\n",
      "Iteration 25890 Training loss 0.0049512977711856365 Validation loss 0.010916030965745449 Accuracy 0.88330078125\n",
      "Iteration 25900 Training loss 0.005549461115151644 Validation loss 0.010928820818662643 Accuracy 0.8837890625\n",
      "Iteration 25910 Training loss 0.0046484763734042645 Validation loss 0.010838341899216175 Accuracy 0.88525390625\n",
      "Iteration 25920 Training loss 0.004641344770789146 Validation loss 0.01102191861718893 Accuracy 0.88330078125\n",
      "Iteration 25930 Training loss 0.002886800328269601 Validation loss 0.010960434563457966 Accuracy 0.8837890625\n",
      "Iteration 25940 Training loss 0.004673364106565714 Validation loss 0.011259675025939941 Accuracy 0.8798828125\n",
      "Iteration 25950 Training loss 0.004886422771960497 Validation loss 0.01127680018544197 Accuracy 0.87890625\n",
      "Iteration 25960 Training loss 0.006545765325427055 Validation loss 0.011181039735674858 Accuracy 0.88037109375\n",
      "Iteration 25970 Training loss 0.005239682272076607 Validation loss 0.01093112863600254 Accuracy 0.8837890625\n",
      "Iteration 25980 Training loss 0.004520911257714033 Validation loss 0.010932750068604946 Accuracy 0.8837890625\n",
      "Iteration 25990 Training loss 0.004779407288879156 Validation loss 0.011195268481969833 Accuracy 0.88134765625\n",
      "Iteration 26000 Training loss 0.005349168088287115 Validation loss 0.01106173824518919 Accuracy 0.88232421875\n",
      "Iteration 26010 Training loss 0.006670372094959021 Validation loss 0.012107728980481625 Accuracy 0.87255859375\n",
      "Iteration 26020 Training loss 0.005841778591275215 Validation loss 0.011413670144975185 Accuracy 0.87841796875\n",
      "Iteration 26030 Training loss 0.004814543295651674 Validation loss 0.011244622059166431 Accuracy 0.88037109375\n",
      "Iteration 26040 Training loss 0.0056967418640851974 Validation loss 0.011079628951847553 Accuracy 0.8818359375\n",
      "Iteration 26050 Training loss 0.0052225785329937935 Validation loss 0.011811915785074234 Accuracy 0.87451171875\n",
      "Iteration 26060 Training loss 0.0051148422062397 Validation loss 0.011514213867485523 Accuracy 0.87841796875\n",
      "Iteration 26070 Training loss 0.004699349869042635 Validation loss 0.010955854319036007 Accuracy 0.88330078125\n",
      "Iteration 26080 Training loss 0.006000572349876165 Validation loss 0.01122818049043417 Accuracy 0.880859375\n",
      "Iteration 26090 Training loss 0.005173432175070047 Validation loss 0.011257362551987171 Accuracy 0.880859375\n",
      "Iteration 26100 Training loss 0.004693891387432814 Validation loss 0.011377746239304543 Accuracy 0.87939453125\n",
      "Iteration 26110 Training loss 0.004803462885320187 Validation loss 0.011311247013509274 Accuracy 0.87890625\n",
      "Iteration 26120 Training loss 0.0056985169649124146 Validation loss 0.01113925501704216 Accuracy 0.88232421875\n",
      "Iteration 26130 Training loss 0.003786541521549225 Validation loss 0.011175709776580334 Accuracy 0.88134765625\n",
      "Iteration 26140 Training loss 0.005770216695964336 Validation loss 0.011321243830025196 Accuracy 0.8798828125\n",
      "Iteration 26150 Training loss 0.006366393994539976 Validation loss 0.011412855237722397 Accuracy 0.87939453125\n",
      "Iteration 26160 Training loss 0.006235078908503056 Validation loss 0.011517436243593693 Accuracy 0.87744140625\n",
      "Iteration 26170 Training loss 0.005567519925534725 Validation loss 0.011060371063649654 Accuracy 0.8828125\n",
      "Iteration 26180 Training loss 0.00487801618874073 Validation loss 0.010997067205607891 Accuracy 0.88232421875\n",
      "Iteration 26190 Training loss 0.007212381809949875 Validation loss 0.011869979090988636 Accuracy 0.87255859375\n",
      "Iteration 26200 Training loss 0.005756725091487169 Validation loss 0.011708999052643776 Accuracy 0.875\n",
      "Iteration 26210 Training loss 0.0043949284590780735 Validation loss 0.010839594528079033 Accuracy 0.884765625\n",
      "Iteration 26220 Training loss 0.0035611381754279137 Validation loss 0.01086411066353321 Accuracy 0.8837890625\n",
      "Iteration 26230 Training loss 0.004852428566664457 Validation loss 0.011061306111514568 Accuracy 0.88330078125\n",
      "Iteration 26240 Training loss 0.005589671898633242 Validation loss 0.011142644099891186 Accuracy 0.880859375\n",
      "Iteration 26250 Training loss 0.005856398027390242 Validation loss 0.010986877605319023 Accuracy 0.8837890625\n",
      "Iteration 26260 Training loss 0.004433934576809406 Validation loss 0.011140938848257065 Accuracy 0.880859375\n",
      "Iteration 26270 Training loss 0.006011616438627243 Validation loss 0.010876638814806938 Accuracy 0.88427734375\n",
      "Iteration 26280 Training loss 0.0053406828083097935 Validation loss 0.011171817779541016 Accuracy 0.88134765625\n",
      "Iteration 26290 Training loss 0.003331353422254324 Validation loss 0.011308128014206886 Accuracy 0.87939453125\n",
      "Iteration 26300 Training loss 0.008427945896983147 Validation loss 0.011318236589431763 Accuracy 0.8798828125\n",
      "Iteration 26310 Training loss 0.007620662450790405 Validation loss 0.011141443625092506 Accuracy 0.8818359375\n",
      "Iteration 26320 Training loss 0.006797126494348049 Validation loss 0.011182731948792934 Accuracy 0.88037109375\n",
      "Iteration 26330 Training loss 0.0061798300594091415 Validation loss 0.011286723427474499 Accuracy 0.8798828125\n",
      "Iteration 26340 Training loss 0.0050215995870530605 Validation loss 0.011398936621844769 Accuracy 0.87744140625\n",
      "Iteration 26350 Training loss 0.005323693621903658 Validation loss 0.011195468716323376 Accuracy 0.8798828125\n",
      "Iteration 26360 Training loss 0.004961507394909859 Validation loss 0.011187968775629997 Accuracy 0.880859375\n",
      "Iteration 26370 Training loss 0.004943726118654013 Validation loss 0.011540137231349945 Accuracy 0.876953125\n",
      "Iteration 26380 Training loss 0.006112988572567701 Validation loss 0.011080997996032238 Accuracy 0.88232421875\n",
      "Iteration 26390 Training loss 0.005097391549497843 Validation loss 0.011430683545768261 Accuracy 0.87841796875\n",
      "Iteration 26400 Training loss 0.007065360434353352 Validation loss 0.011772455647587776 Accuracy 0.87548828125\n",
      "Iteration 26410 Training loss 0.007034109905362129 Validation loss 0.01185954175889492 Accuracy 0.87353515625\n",
      "Iteration 26420 Training loss 0.004274987615644932 Validation loss 0.011066201142966747 Accuracy 0.88330078125\n",
      "Iteration 26430 Training loss 0.004919034894555807 Validation loss 0.011898425407707691 Accuracy 0.87353515625\n",
      "Iteration 26440 Training loss 0.00426126504316926 Validation loss 0.011066913604736328 Accuracy 0.8828125\n",
      "Iteration 26450 Training loss 0.0050368355587124825 Validation loss 0.010944782756268978 Accuracy 0.8837890625\n",
      "Iteration 26460 Training loss 0.003908409271389246 Validation loss 0.011206689290702343 Accuracy 0.880859375\n",
      "Iteration 26470 Training loss 0.006517508532851934 Validation loss 0.011129946447908878 Accuracy 0.8818359375\n",
      "Iteration 26480 Training loss 0.005975072272121906 Validation loss 0.011092898435890675 Accuracy 0.88134765625\n",
      "Iteration 26490 Training loss 0.007658636197447777 Validation loss 0.01162275392562151 Accuracy 0.875\n",
      "Iteration 26500 Training loss 0.005411970894783735 Validation loss 0.010874217376112938 Accuracy 0.88525390625\n",
      "Iteration 26510 Training loss 0.006285542622208595 Validation loss 0.011132277548313141 Accuracy 0.88134765625\n",
      "Iteration 26520 Training loss 0.006442971061915159 Validation loss 0.011190295219421387 Accuracy 0.88037109375\n",
      "Iteration 26530 Training loss 0.005426185671240091 Validation loss 0.011123810894787312 Accuracy 0.880859375\n",
      "Iteration 26540 Training loss 0.005539478734135628 Validation loss 0.011117157526314259 Accuracy 0.88037109375\n",
      "Iteration 26550 Training loss 0.006138913333415985 Validation loss 0.011281827464699745 Accuracy 0.87939453125\n",
      "Iteration 26560 Training loss 0.004705038852989674 Validation loss 0.011069275438785553 Accuracy 0.88330078125\n",
      "Iteration 26570 Training loss 0.0055884006433188915 Validation loss 0.011570102535188198 Accuracy 0.8779296875\n",
      "Iteration 26580 Training loss 0.004799301270395517 Validation loss 0.011275463737547398 Accuracy 0.880859375\n",
      "Iteration 26590 Training loss 0.005234428681433201 Validation loss 0.011014132760465145 Accuracy 0.88330078125\n",
      "Iteration 26600 Training loss 0.004423188976943493 Validation loss 0.011148888617753983 Accuracy 0.8818359375\n",
      "Iteration 26610 Training loss 0.0053662522695958614 Validation loss 0.011947043240070343 Accuracy 0.87353515625\n",
      "Iteration 26620 Training loss 0.005244264379143715 Validation loss 0.01114291325211525 Accuracy 0.88134765625\n",
      "Iteration 26630 Training loss 0.005721372086554766 Validation loss 0.011548295617103577 Accuracy 0.87744140625\n",
      "Iteration 26640 Training loss 0.006223181262612343 Validation loss 0.010978435166180134 Accuracy 0.88330078125\n",
      "Iteration 26650 Training loss 0.005184420850127935 Validation loss 0.011186005547642708 Accuracy 0.880859375\n",
      "Iteration 26660 Training loss 0.004155660048127174 Validation loss 0.011230690404772758 Accuracy 0.880859375\n",
      "Iteration 26670 Training loss 0.004797295201569796 Validation loss 0.011290272697806358 Accuracy 0.87939453125\n",
      "Iteration 26680 Training loss 0.004770590458065271 Validation loss 0.011385010555386543 Accuracy 0.87841796875\n",
      "Iteration 26690 Training loss 0.005952535662800074 Validation loss 0.01135337632149458 Accuracy 0.87939453125\n",
      "Iteration 26700 Training loss 0.005431478377431631 Validation loss 0.011691935360431671 Accuracy 0.8759765625\n",
      "Iteration 26710 Training loss 0.003603110322728753 Validation loss 0.011023161001503468 Accuracy 0.88330078125\n",
      "Iteration 26720 Training loss 0.006787446793168783 Validation loss 0.011798310093581676 Accuracy 0.87451171875\n",
      "Iteration 26730 Training loss 0.007615386042743921 Validation loss 0.011051176115870476 Accuracy 0.8828125\n",
      "Iteration 26740 Training loss 0.004404344130307436 Validation loss 0.011212228797376156 Accuracy 0.88037109375\n",
      "Iteration 26750 Training loss 0.004830149933695793 Validation loss 0.01119996514171362 Accuracy 0.88134765625\n",
      "Iteration 26760 Training loss 0.0059267678298056126 Validation loss 0.010880974121391773 Accuracy 0.8837890625\n",
      "Iteration 26770 Training loss 0.004239608068019152 Validation loss 0.011011655442416668 Accuracy 0.88330078125\n",
      "Iteration 26780 Training loss 0.006265608593821526 Validation loss 0.011213457211852074 Accuracy 0.87939453125\n",
      "Iteration 26790 Training loss 0.004752474371343851 Validation loss 0.010767311789095402 Accuracy 0.8857421875\n",
      "Iteration 26800 Training loss 0.004697691183537245 Validation loss 0.011776800267398357 Accuracy 0.87548828125\n",
      "Iteration 26810 Training loss 0.004457315895706415 Validation loss 0.010918735526502132 Accuracy 0.88427734375\n",
      "Iteration 26820 Training loss 0.006168060004711151 Validation loss 0.01109224371612072 Accuracy 0.88232421875\n",
      "Iteration 26830 Training loss 0.00661698542535305 Validation loss 0.010917961597442627 Accuracy 0.88427734375\n",
      "Iteration 26840 Training loss 0.0064738779328763485 Validation loss 0.010850315913558006 Accuracy 0.88525390625\n",
      "Iteration 26850 Training loss 0.005506459623575211 Validation loss 0.010834179818630219 Accuracy 0.88427734375\n",
      "Iteration 26860 Training loss 0.004544908180832863 Validation loss 0.010954523459076881 Accuracy 0.88330078125\n",
      "Iteration 26870 Training loss 0.00404321076348424 Validation loss 0.010952741838991642 Accuracy 0.88427734375\n",
      "Iteration 26880 Training loss 0.005955576431006193 Validation loss 0.01110438909381628 Accuracy 0.8828125\n",
      "Iteration 26890 Training loss 0.005757007747888565 Validation loss 0.010824843309819698 Accuracy 0.8857421875\n",
      "Iteration 26900 Training loss 0.005885799881070852 Validation loss 0.010801387950778008 Accuracy 0.884765625\n",
      "Iteration 26910 Training loss 0.003625662764534354 Validation loss 0.010888608172535896 Accuracy 0.88427734375\n",
      "Iteration 26920 Training loss 0.005448435433208942 Validation loss 0.010702180676162243 Accuracy 0.88623046875\n",
      "Iteration 26930 Training loss 0.004670523572713137 Validation loss 0.011225228197872639 Accuracy 0.880859375\n",
      "Iteration 26940 Training loss 0.00635526655241847 Validation loss 0.011310659348964691 Accuracy 0.8798828125\n",
      "Iteration 26950 Training loss 0.004254254978150129 Validation loss 0.010821818374097347 Accuracy 0.884765625\n",
      "Iteration 26960 Training loss 0.006223613861948252 Validation loss 0.011018476448953152 Accuracy 0.88330078125\n",
      "Iteration 26970 Training loss 0.005465326365083456 Validation loss 0.011132234707474709 Accuracy 0.880859375\n",
      "Iteration 26980 Training loss 0.004806065931916237 Validation loss 0.011069882661104202 Accuracy 0.88232421875\n",
      "Iteration 26990 Training loss 0.006174550857394934 Validation loss 0.011331393383443356 Accuracy 0.8798828125\n",
      "Iteration 27000 Training loss 0.004043081775307655 Validation loss 0.011375895701348782 Accuracy 0.87890625\n",
      "Iteration 27010 Training loss 0.0036227498203516006 Validation loss 0.011232323944568634 Accuracy 0.88134765625\n",
      "Iteration 27020 Training loss 0.007316750939935446 Validation loss 0.011655513197183609 Accuracy 0.87451171875\n",
      "Iteration 27030 Training loss 0.005680416245013475 Validation loss 0.011230438016355038 Accuracy 0.88134765625\n",
      "Iteration 27040 Training loss 0.0054906392470002174 Validation loss 0.011038051918148994 Accuracy 0.88232421875\n",
      "Iteration 27050 Training loss 0.005374914035201073 Validation loss 0.011346293613314629 Accuracy 0.87939453125\n",
      "Iteration 27060 Training loss 0.004872848745435476 Validation loss 0.010876083746552467 Accuracy 0.88427734375\n",
      "Iteration 27070 Training loss 0.006061345338821411 Validation loss 0.010996844619512558 Accuracy 0.8828125\n",
      "Iteration 27080 Training loss 0.005417818669229746 Validation loss 0.011091078631579876 Accuracy 0.88232421875\n",
      "Iteration 27090 Training loss 0.00513048842549324 Validation loss 0.011384009383618832 Accuracy 0.87890625\n",
      "Iteration 27100 Training loss 0.0042753443121910095 Validation loss 0.011224107816815376 Accuracy 0.88037109375\n",
      "Iteration 27110 Training loss 0.0061539942398667336 Validation loss 0.011227519251406193 Accuracy 0.880859375\n",
      "Iteration 27120 Training loss 0.005360075738281012 Validation loss 0.01106232963502407 Accuracy 0.88232421875\n",
      "Iteration 27130 Training loss 0.005106134805828333 Validation loss 0.011134248226881027 Accuracy 0.8818359375\n",
      "Iteration 27140 Training loss 0.007028433494269848 Validation loss 0.012339483015239239 Accuracy 0.86962890625\n",
      "Iteration 27150 Training loss 0.004150213673710823 Validation loss 0.010780123993754387 Accuracy 0.88427734375\n",
      "Iteration 27160 Training loss 0.0056368084624409676 Validation loss 0.01111684087663889 Accuracy 0.8818359375\n",
      "Iteration 27170 Training loss 0.003547891043126583 Validation loss 0.010864026844501495 Accuracy 0.884765625\n",
      "Iteration 27180 Training loss 0.002749307546764612 Validation loss 0.011048378422856331 Accuracy 0.8818359375\n",
      "Iteration 27190 Training loss 0.00540131377056241 Validation loss 0.011053097434341908 Accuracy 0.88232421875\n",
      "Iteration 27200 Training loss 0.0038916263729333878 Validation loss 0.010987850837409496 Accuracy 0.88330078125\n",
      "Iteration 27210 Training loss 0.005352489184588194 Validation loss 0.011209296993911266 Accuracy 0.8798828125\n",
      "Iteration 27220 Training loss 0.005256050731986761 Validation loss 0.011115597561001778 Accuracy 0.88330078125\n",
      "Iteration 27230 Training loss 0.005541648715734482 Validation loss 0.010898921638727188 Accuracy 0.88427734375\n",
      "Iteration 27240 Training loss 0.0046874526888132095 Validation loss 0.011208419688045979 Accuracy 0.88134765625\n",
      "Iteration 27250 Training loss 0.00525695038959384 Validation loss 0.010832848958671093 Accuracy 0.884765625\n",
      "Iteration 27260 Training loss 0.005780231207609177 Validation loss 0.011212844401597977 Accuracy 0.88037109375\n",
      "Iteration 27270 Training loss 0.005729508586227894 Validation loss 0.011359131895005703 Accuracy 0.8798828125\n",
      "Iteration 27280 Training loss 0.005358690395951271 Validation loss 0.010939416475594044 Accuracy 0.88427734375\n",
      "Iteration 27290 Training loss 0.005854861345142126 Validation loss 0.011313016526401043 Accuracy 0.8798828125\n",
      "Iteration 27300 Training loss 0.00635361485183239 Validation loss 0.01208356861025095 Accuracy 0.8720703125\n",
      "Iteration 27310 Training loss 0.004889746196568012 Validation loss 0.011041130870580673 Accuracy 0.88232421875\n",
      "Iteration 27320 Training loss 0.0047821709886193275 Validation loss 0.011360586620867252 Accuracy 0.87939453125\n",
      "Iteration 27330 Training loss 0.004295523278415203 Validation loss 0.01107199676334858 Accuracy 0.88232421875\n",
      "Iteration 27340 Training loss 0.005111014470458031 Validation loss 0.011295346543192863 Accuracy 0.88037109375\n",
      "Iteration 27350 Training loss 0.004597911611199379 Validation loss 0.011341147124767303 Accuracy 0.87939453125\n",
      "Iteration 27360 Training loss 0.00632822047919035 Validation loss 0.01196176279336214 Accuracy 0.8740234375\n",
      "Iteration 27370 Training loss 0.004603330045938492 Validation loss 0.011000003665685654 Accuracy 0.8828125\n",
      "Iteration 27380 Training loss 0.003877770621329546 Validation loss 0.010993754491209984 Accuracy 0.8837890625\n",
      "Iteration 27390 Training loss 0.0051673599518835545 Validation loss 0.01089017279446125 Accuracy 0.884765625\n",
      "Iteration 27400 Training loss 0.0062776184640824795 Validation loss 0.011085270904004574 Accuracy 0.88134765625\n",
      "Iteration 27410 Training loss 0.005701085552573204 Validation loss 0.012014348059892654 Accuracy 0.8720703125\n",
      "Iteration 27420 Training loss 0.0065283579751849174 Validation loss 0.01117488369345665 Accuracy 0.88037109375\n",
      "Iteration 27430 Training loss 0.005520826205611229 Validation loss 0.010920969769358635 Accuracy 0.8837890625\n",
      "Iteration 27440 Training loss 0.004792462568730116 Validation loss 0.01106999721378088 Accuracy 0.8818359375\n",
      "Iteration 27450 Training loss 0.006613409146666527 Validation loss 0.011157792992889881 Accuracy 0.8818359375\n",
      "Iteration 27460 Training loss 0.004457365721464157 Validation loss 0.011219008825719357 Accuracy 0.88037109375\n",
      "Iteration 27470 Training loss 0.004604043439030647 Validation loss 0.010810856707394123 Accuracy 0.88525390625\n",
      "Iteration 27480 Training loss 0.006729340646415949 Validation loss 0.01146544050425291 Accuracy 0.8779296875\n",
      "Iteration 27490 Training loss 0.0044885254465043545 Validation loss 0.010883903130888939 Accuracy 0.884765625\n",
      "Iteration 27500 Training loss 0.004241100512444973 Validation loss 0.010866941884160042 Accuracy 0.88427734375\n",
      "Iteration 27510 Training loss 0.007794239092618227 Validation loss 0.011388509534299374 Accuracy 0.87841796875\n",
      "Iteration 27520 Training loss 0.003915691748261452 Validation loss 0.010976816527545452 Accuracy 0.8837890625\n",
      "Iteration 27530 Training loss 0.005472322925925255 Validation loss 0.011446710675954819 Accuracy 0.8779296875\n",
      "Iteration 27540 Training loss 0.005256186239421368 Validation loss 0.0112908398732543 Accuracy 0.87939453125\n",
      "Iteration 27550 Training loss 0.004151526372879744 Validation loss 0.011005544103682041 Accuracy 0.88232421875\n",
      "Iteration 27560 Training loss 0.005835341289639473 Validation loss 0.011228090152144432 Accuracy 0.88232421875\n",
      "Iteration 27570 Training loss 0.005724437069147825 Validation loss 0.011000090278685093 Accuracy 0.88232421875\n",
      "Iteration 27580 Training loss 0.005856883712112904 Validation loss 0.011162499897181988 Accuracy 0.88037109375\n",
      "Iteration 27590 Training loss 0.004410797730088234 Validation loss 0.011299439705908298 Accuracy 0.87939453125\n",
      "Iteration 27600 Training loss 0.005565624218434095 Validation loss 0.010929304175078869 Accuracy 0.8837890625\n",
      "Iteration 27610 Training loss 0.004124315455555916 Validation loss 0.01089389342814684 Accuracy 0.88330078125\n",
      "Iteration 27620 Training loss 0.005253514740616083 Validation loss 0.011051164008677006 Accuracy 0.88330078125\n",
      "Iteration 27630 Training loss 0.006374063901603222 Validation loss 0.011945605278015137 Accuracy 0.873046875\n",
      "Iteration 27640 Training loss 0.005329711362719536 Validation loss 0.011268211528658867 Accuracy 0.880859375\n",
      "Iteration 27650 Training loss 0.004259663168340921 Validation loss 0.011256217025220394 Accuracy 0.87939453125\n",
      "Iteration 27660 Training loss 0.0037612037267535925 Validation loss 0.010773449204862118 Accuracy 0.88525390625\n",
      "Iteration 27670 Training loss 0.0036457062233239412 Validation loss 0.01072259247303009 Accuracy 0.88525390625\n",
      "Iteration 27680 Training loss 0.006772899534553289 Validation loss 0.011214192025363445 Accuracy 0.88037109375\n",
      "Iteration 27690 Training loss 0.003748226212337613 Validation loss 0.011019157245755196 Accuracy 0.88232421875\n",
      "Iteration 27700 Training loss 0.005580116994678974 Validation loss 0.01149632129818201 Accuracy 0.87841796875\n",
      "Iteration 27710 Training loss 0.003924737684428692 Validation loss 0.011003187857568264 Accuracy 0.8837890625\n",
      "Iteration 27720 Training loss 0.0051092179492115974 Validation loss 0.010987192392349243 Accuracy 0.88232421875\n",
      "Iteration 27730 Training loss 0.005013908259570599 Validation loss 0.011298705823719501 Accuracy 0.87890625\n",
      "Iteration 27740 Training loss 0.004851021803915501 Validation loss 0.010982717387378216 Accuracy 0.8828125\n",
      "Iteration 27750 Training loss 0.005994326435029507 Validation loss 0.011205244809389114 Accuracy 0.8798828125\n",
      "Iteration 27760 Training loss 0.005382436793297529 Validation loss 0.010918458923697472 Accuracy 0.88330078125\n",
      "Iteration 27770 Training loss 0.005357707850635052 Validation loss 0.01154684741050005 Accuracy 0.87646484375\n",
      "Iteration 27780 Training loss 0.005860883742570877 Validation loss 0.011013959534466267 Accuracy 0.88232421875\n",
      "Iteration 27790 Training loss 0.004486911930143833 Validation loss 0.011073002591729164 Accuracy 0.8818359375\n",
      "Iteration 27800 Training loss 0.004481895361095667 Validation loss 0.011067553423345089 Accuracy 0.88232421875\n",
      "Iteration 27810 Training loss 0.004418271128088236 Validation loss 0.011140900664031506 Accuracy 0.8818359375\n",
      "Iteration 27820 Training loss 0.00449117599055171 Validation loss 0.011079687625169754 Accuracy 0.8818359375\n",
      "Iteration 27830 Training loss 0.005535340402275324 Validation loss 0.011278863064944744 Accuracy 0.8798828125\n",
      "Iteration 27840 Training loss 0.005227589979767799 Validation loss 0.010974748060107231 Accuracy 0.8818359375\n",
      "Iteration 27850 Training loss 0.004351702984422445 Validation loss 0.011171088553965092 Accuracy 0.88134765625\n",
      "Iteration 27860 Training loss 0.005266491789370775 Validation loss 0.011005955748260021 Accuracy 0.8828125\n",
      "Iteration 27870 Training loss 0.004911038558930159 Validation loss 0.0113276531919837 Accuracy 0.87939453125\n",
      "Iteration 27880 Training loss 0.004202147945761681 Validation loss 0.010962736792862415 Accuracy 0.8837890625\n",
      "Iteration 27890 Training loss 0.0035464500542730093 Validation loss 0.01094979327172041 Accuracy 0.8828125\n",
      "Iteration 27900 Training loss 0.004510644357651472 Validation loss 0.0109309833496809 Accuracy 0.8837890625\n",
      "Iteration 27910 Training loss 0.004688594024628401 Validation loss 0.01106491219252348 Accuracy 0.88232421875\n",
      "Iteration 27920 Training loss 0.004745951388031244 Validation loss 0.010911569930613041 Accuracy 0.88427734375\n",
      "Iteration 27930 Training loss 0.00429244851693511 Validation loss 0.011031122878193855 Accuracy 0.8828125\n",
      "Iteration 27940 Training loss 0.004964754916727543 Validation loss 0.011195134371519089 Accuracy 0.880859375\n",
      "Iteration 27950 Training loss 0.005853645969182253 Validation loss 0.010790802538394928 Accuracy 0.88525390625\n",
      "Iteration 27960 Training loss 0.004652021918445826 Validation loss 0.010973210446536541 Accuracy 0.88232421875\n",
      "Iteration 27970 Training loss 0.0052507189102470875 Validation loss 0.010758544318377972 Accuracy 0.884765625\n",
      "Iteration 27980 Training loss 0.005317081231623888 Validation loss 0.010765896178781986 Accuracy 0.884765625\n",
      "Iteration 27990 Training loss 0.005901432130485773 Validation loss 0.01106290239840746 Accuracy 0.8828125\n",
      "Iteration 28000 Training loss 0.004947939421981573 Validation loss 0.01110240537673235 Accuracy 0.8818359375\n",
      "Iteration 28010 Training loss 0.005793644115328789 Validation loss 0.011381011456251144 Accuracy 0.8798828125\n",
      "Iteration 28020 Training loss 0.004379419144243002 Validation loss 0.0112595334649086 Accuracy 0.88037109375\n",
      "Iteration 28030 Training loss 0.005813155323266983 Validation loss 0.010902036912739277 Accuracy 0.8837890625\n",
      "Iteration 28040 Training loss 0.004822810646146536 Validation loss 0.011099305003881454 Accuracy 0.88232421875\n",
      "Iteration 28050 Training loss 0.003922098316252232 Validation loss 0.011043607257306576 Accuracy 0.88232421875\n",
      "Iteration 28060 Training loss 0.004898310638964176 Validation loss 0.011005409061908722 Accuracy 0.88330078125\n",
      "Iteration 28070 Training loss 0.00511307455599308 Validation loss 0.011050600558519363 Accuracy 0.88232421875\n",
      "Iteration 28080 Training loss 0.00618847506120801 Validation loss 0.010923256166279316 Accuracy 0.88427734375\n",
      "Iteration 28090 Training loss 0.004431604407727718 Validation loss 0.011022782884538174 Accuracy 0.8828125\n",
      "Iteration 28100 Training loss 0.005681330803781748 Validation loss 0.010950685478746891 Accuracy 0.88232421875\n",
      "Iteration 28110 Training loss 0.004399665631353855 Validation loss 0.011131836101412773 Accuracy 0.880859375\n",
      "Iteration 28120 Training loss 0.005556171294301748 Validation loss 0.011213310062885284 Accuracy 0.88037109375\n",
      "Iteration 28130 Training loss 0.00454375334084034 Validation loss 0.010932880453765392 Accuracy 0.8828125\n",
      "Iteration 28140 Training loss 0.006109870038926601 Validation loss 0.011248082853853703 Accuracy 0.880859375\n",
      "Iteration 28150 Training loss 0.0045554605312645435 Validation loss 0.010897265747189522 Accuracy 0.88427734375\n",
      "Iteration 28160 Training loss 0.003792235627770424 Validation loss 0.010876552201807499 Accuracy 0.8837890625\n",
      "Iteration 28170 Training loss 0.0042882137931883335 Validation loss 0.01110800914466381 Accuracy 0.8818359375\n",
      "Iteration 28180 Training loss 0.005446983501315117 Validation loss 0.01131710410118103 Accuracy 0.87890625\n",
      "Iteration 28190 Training loss 0.004809068515896797 Validation loss 0.010946808382868767 Accuracy 0.88232421875\n",
      "Iteration 28200 Training loss 0.005283923353999853 Validation loss 0.011033608578145504 Accuracy 0.8828125\n",
      "Iteration 28210 Training loss 0.005391646642237902 Validation loss 0.010751032270491123 Accuracy 0.88427734375\n",
      "Iteration 28220 Training loss 0.006467424798756838 Validation loss 0.010868747718632221 Accuracy 0.8837890625\n",
      "Iteration 28230 Training loss 0.004291364457458258 Validation loss 0.010971041396260262 Accuracy 0.88525390625\n",
      "Iteration 28240 Training loss 0.004259697627276182 Validation loss 0.011063042096793652 Accuracy 0.8828125\n",
      "Iteration 28250 Training loss 0.005431088153272867 Validation loss 0.011144401505589485 Accuracy 0.88232421875\n",
      "Iteration 28260 Training loss 0.004618443548679352 Validation loss 0.011346996761858463 Accuracy 0.87939453125\n",
      "Iteration 28270 Training loss 0.005095801781862974 Validation loss 0.011075767688453197 Accuracy 0.88232421875\n",
      "Iteration 28280 Training loss 0.005697701591998339 Validation loss 0.011569526046514511 Accuracy 0.87548828125\n",
      "Iteration 28290 Training loss 0.0041577741503715515 Validation loss 0.011385414749383926 Accuracy 0.87841796875\n",
      "Iteration 28300 Training loss 0.0040849014185369015 Validation loss 0.011097882874310017 Accuracy 0.880859375\n",
      "Iteration 28310 Training loss 0.005493483040481806 Validation loss 0.01126483827829361 Accuracy 0.87939453125\n",
      "Iteration 28320 Training loss 0.004801207687705755 Validation loss 0.011183654889464378 Accuracy 0.88037109375\n",
      "Iteration 28330 Training loss 0.004165909253060818 Validation loss 0.010990846902132034 Accuracy 0.8828125\n",
      "Iteration 28340 Training loss 0.0066460659727454185 Validation loss 0.01104779914021492 Accuracy 0.88232421875\n",
      "Iteration 28350 Training loss 0.004406424704939127 Validation loss 0.011405756697058678 Accuracy 0.87890625\n",
      "Iteration 28360 Training loss 0.005086932796984911 Validation loss 0.011426940560340881 Accuracy 0.87890625\n",
      "Iteration 28370 Training loss 0.005164043977856636 Validation loss 0.010907694697380066 Accuracy 0.88427734375\n",
      "Iteration 28380 Training loss 0.005159869324415922 Validation loss 0.010996767319738865 Accuracy 0.88330078125\n",
      "Iteration 28390 Training loss 0.003983297385275364 Validation loss 0.01123387273401022 Accuracy 0.88037109375\n",
      "Iteration 28400 Training loss 0.005207187030464411 Validation loss 0.01106530986726284 Accuracy 0.88232421875\n",
      "Iteration 28410 Training loss 0.005449590738862753 Validation loss 0.011303380131721497 Accuracy 0.8798828125\n",
      "Iteration 28420 Training loss 0.004256652668118477 Validation loss 0.010951826348900795 Accuracy 0.88330078125\n",
      "Iteration 28430 Training loss 0.00470580393448472 Validation loss 0.010917318053543568 Accuracy 0.88330078125\n",
      "Iteration 28440 Training loss 0.004615080542862415 Validation loss 0.011006530374288559 Accuracy 0.88232421875\n",
      "Iteration 28450 Training loss 0.0051872399635612965 Validation loss 0.01109373476356268 Accuracy 0.880859375\n",
      "Iteration 28460 Training loss 0.0049665640108287334 Validation loss 0.010885195806622505 Accuracy 0.8828125\n",
      "Iteration 28470 Training loss 0.005640088114887476 Validation loss 0.010919333435595036 Accuracy 0.8837890625\n",
      "Iteration 28480 Training loss 0.003942679613828659 Validation loss 0.010907504707574844 Accuracy 0.8837890625\n",
      "Iteration 28490 Training loss 0.005104838404804468 Validation loss 0.01122439093887806 Accuracy 0.88037109375\n",
      "Iteration 28500 Training loss 0.00361830135807395 Validation loss 0.010841213166713715 Accuracy 0.88525390625\n",
      "Iteration 28510 Training loss 0.005151445511728525 Validation loss 0.011667444370687008 Accuracy 0.8759765625\n",
      "Iteration 28520 Training loss 0.004602709319442511 Validation loss 0.011019177734851837 Accuracy 0.8828125\n",
      "Iteration 28530 Training loss 0.006860754452645779 Validation loss 0.010986250825226307 Accuracy 0.8828125\n",
      "Iteration 28540 Training loss 0.005520194303244352 Validation loss 0.010988626629114151 Accuracy 0.8818359375\n",
      "Iteration 28550 Training loss 0.004905024543404579 Validation loss 0.011045871302485466 Accuracy 0.88134765625\n",
      "Iteration 28560 Training loss 0.0035077754873782396 Validation loss 0.011047961190342903 Accuracy 0.8818359375\n",
      "Iteration 28570 Training loss 0.004861171822994947 Validation loss 0.01137405727058649 Accuracy 0.8779296875\n",
      "Iteration 28580 Training loss 0.004120498429983854 Validation loss 0.010854172520339489 Accuracy 0.8837890625\n",
      "Iteration 28590 Training loss 0.00541830575093627 Validation loss 0.010963872075080872 Accuracy 0.88232421875\n",
      "Iteration 28600 Training loss 0.004125514999032021 Validation loss 0.010748371481895447 Accuracy 0.884765625\n",
      "Iteration 28610 Training loss 0.00545946229249239 Validation loss 0.010816274210810661 Accuracy 0.884765625\n",
      "Iteration 28620 Training loss 0.00480540469288826 Validation loss 0.010952609591186047 Accuracy 0.88232421875\n",
      "Iteration 28630 Training loss 0.004811848048120737 Validation loss 0.01089537050575018 Accuracy 0.88330078125\n",
      "Iteration 28640 Training loss 0.004644846543669701 Validation loss 0.01098434254527092 Accuracy 0.88232421875\n",
      "Iteration 28650 Training loss 0.004090689122676849 Validation loss 0.011200145818293095 Accuracy 0.88037109375\n",
      "Iteration 28660 Training loss 0.004809171427041292 Validation loss 0.010865497402846813 Accuracy 0.88427734375\n",
      "Iteration 28670 Training loss 0.005298596806824207 Validation loss 0.0110007980838418 Accuracy 0.8818359375\n",
      "Iteration 28680 Training loss 0.00327684567309916 Validation loss 0.01102948933839798 Accuracy 0.8818359375\n",
      "Iteration 28690 Training loss 0.0057816957123577595 Validation loss 0.011192354373633862 Accuracy 0.880859375\n",
      "Iteration 28700 Training loss 0.0036620839964598417 Validation loss 0.010762382298707962 Accuracy 0.884765625\n",
      "Iteration 28710 Training loss 0.004014180041849613 Validation loss 0.010743862017989159 Accuracy 0.88427734375\n",
      "Iteration 28720 Training loss 0.004961386322975159 Validation loss 0.010765431448817253 Accuracy 0.88525390625\n",
      "Iteration 28730 Training loss 0.006151181645691395 Validation loss 0.011344509199261665 Accuracy 0.87939453125\n",
      "Iteration 28740 Training loss 0.00397305004298687 Validation loss 0.010827829129993916 Accuracy 0.88427734375\n",
      "Iteration 28750 Training loss 0.0036100351717323065 Validation loss 0.011050558649003506 Accuracy 0.88232421875\n",
      "Iteration 28760 Training loss 0.005684088449925184 Validation loss 0.011155177839100361 Accuracy 0.88134765625\n",
      "Iteration 28770 Training loss 0.006331870798021555 Validation loss 0.010996829718351364 Accuracy 0.88232421875\n",
      "Iteration 28780 Training loss 0.0036540997680276632 Validation loss 0.011133622378110886 Accuracy 0.880859375\n",
      "Iteration 28790 Training loss 0.006142125464975834 Validation loss 0.01129031553864479 Accuracy 0.87939453125\n",
      "Iteration 28800 Training loss 0.004134297836571932 Validation loss 0.010892106220126152 Accuracy 0.88330078125\n",
      "Iteration 28810 Training loss 0.003876802511513233 Validation loss 0.011134113185107708 Accuracy 0.880859375\n",
      "Iteration 28820 Training loss 0.004829861223697662 Validation loss 0.011199726723134518 Accuracy 0.88037109375\n",
      "Iteration 28830 Training loss 0.004177686758339405 Validation loss 0.011115841567516327 Accuracy 0.8818359375\n",
      "Iteration 28840 Training loss 0.0041363537311553955 Validation loss 0.011281400918960571 Accuracy 0.87890625\n",
      "Iteration 28850 Training loss 0.005697344895452261 Validation loss 0.011146562173962593 Accuracy 0.8818359375\n",
      "Iteration 28860 Training loss 0.004641234874725342 Validation loss 0.011197532527148724 Accuracy 0.88037109375\n",
      "Iteration 28870 Training loss 0.003934988286346197 Validation loss 0.011141098104417324 Accuracy 0.880859375\n",
      "Iteration 28880 Training loss 0.0068487077951431274 Validation loss 0.011490408331155777 Accuracy 0.8779296875\n",
      "Iteration 28890 Training loss 0.006351771764457226 Validation loss 0.01150039304047823 Accuracy 0.8759765625\n",
      "Iteration 28900 Training loss 0.004801206290721893 Validation loss 0.011047950014472008 Accuracy 0.8818359375\n",
      "Iteration 28910 Training loss 0.006008657161146402 Validation loss 0.0113834822550416 Accuracy 0.87744140625\n",
      "Iteration 28920 Training loss 0.00555424252524972 Validation loss 0.01095940638333559 Accuracy 0.88232421875\n",
      "Iteration 28930 Training loss 0.003813267918303609 Validation loss 0.010777898132801056 Accuracy 0.88427734375\n",
      "Iteration 28940 Training loss 0.003948742989450693 Validation loss 0.011138440109789371 Accuracy 0.880859375\n",
      "Iteration 28950 Training loss 0.004271712154150009 Validation loss 0.011200137436389923 Accuracy 0.8798828125\n",
      "Iteration 28960 Training loss 0.005235549062490463 Validation loss 0.011177253909409046 Accuracy 0.880859375\n",
      "Iteration 28970 Training loss 0.005456211045384407 Validation loss 0.011564718559384346 Accuracy 0.87744140625\n",
      "Iteration 28980 Training loss 0.005672745406627655 Validation loss 0.01091886218637228 Accuracy 0.88330078125\n",
      "Iteration 28990 Training loss 0.0036323030944913626 Validation loss 0.011132352985441685 Accuracy 0.880859375\n",
      "Iteration 29000 Training loss 0.00405650632455945 Validation loss 0.010766101069748402 Accuracy 0.88427734375\n",
      "Iteration 29010 Training loss 0.003314228728413582 Validation loss 0.011183436959981918 Accuracy 0.880859375\n",
      "Iteration 29020 Training loss 0.005099175032228231 Validation loss 0.010903107933700085 Accuracy 0.8837890625\n",
      "Iteration 29030 Training loss 0.00535815441980958 Validation loss 0.011292116716504097 Accuracy 0.87939453125\n",
      "Iteration 29040 Training loss 0.005914564710110426 Validation loss 0.010891476646065712 Accuracy 0.8828125\n",
      "Iteration 29050 Training loss 0.005682336166501045 Validation loss 0.011236955411732197 Accuracy 0.88037109375\n",
      "Iteration 29060 Training loss 0.0036802678368985653 Validation loss 0.011135797016322613 Accuracy 0.8818359375\n",
      "Iteration 29070 Training loss 0.004031247925013304 Validation loss 0.011079326272010803 Accuracy 0.8818359375\n",
      "Iteration 29080 Training loss 0.0025805537588894367 Validation loss 0.011080103926360607 Accuracy 0.8818359375\n",
      "Iteration 29090 Training loss 0.003921968396753073 Validation loss 0.01097810734063387 Accuracy 0.8818359375\n",
      "Iteration 29100 Training loss 0.004872637335211039 Validation loss 0.011127748526632786 Accuracy 0.88134765625\n",
      "Iteration 29110 Training loss 0.0037942505441606045 Validation loss 0.011075041256844997 Accuracy 0.8828125\n",
      "Iteration 29120 Training loss 0.005017537623643875 Validation loss 0.011194157414138317 Accuracy 0.88037109375\n",
      "Iteration 29130 Training loss 0.004899764433503151 Validation loss 0.011385851539671421 Accuracy 0.87890625\n",
      "Iteration 29140 Training loss 0.004166432190686464 Validation loss 0.010790873318910599 Accuracy 0.884765625\n",
      "Iteration 29150 Training loss 0.0037413793615996838 Validation loss 0.011075391434133053 Accuracy 0.8818359375\n",
      "Iteration 29160 Training loss 0.005044138059020042 Validation loss 0.011265900917351246 Accuracy 0.8798828125\n",
      "Iteration 29170 Training loss 0.00462509086355567 Validation loss 0.010872473008930683 Accuracy 0.884765625\n",
      "Iteration 29180 Training loss 0.003961421083658934 Validation loss 0.011081557720899582 Accuracy 0.880859375\n",
      "Iteration 29190 Training loss 0.005435576196759939 Validation loss 0.011047432199120522 Accuracy 0.8818359375\n",
      "Iteration 29200 Training loss 0.0048913415521383286 Validation loss 0.011151987127959728 Accuracy 0.880859375\n",
      "Iteration 29210 Training loss 0.004297333303838968 Validation loss 0.010988808237016201 Accuracy 0.8828125\n",
      "Iteration 29220 Training loss 0.004203586373478174 Validation loss 0.011151708662509918 Accuracy 0.88037109375\n",
      "Iteration 29230 Training loss 0.004407237749546766 Validation loss 0.011103703640401363 Accuracy 0.88232421875\n",
      "Iteration 29240 Training loss 0.004879770800471306 Validation loss 0.010969365015625954 Accuracy 0.88232421875\n",
      "Iteration 29250 Training loss 0.005294289905577898 Validation loss 0.011200662702322006 Accuracy 0.87939453125\n",
      "Iteration 29260 Training loss 0.006267928518354893 Validation loss 0.011595477350056171 Accuracy 0.87548828125\n",
      "Iteration 29270 Training loss 0.005323545541614294 Validation loss 0.010993454605340958 Accuracy 0.8828125\n",
      "Iteration 29280 Training loss 0.003677181201055646 Validation loss 0.011054587550461292 Accuracy 0.88232421875\n",
      "Iteration 29290 Training loss 0.003999378066509962 Validation loss 0.011251210235059261 Accuracy 0.8798828125\n",
      "Iteration 29300 Training loss 0.003266130341216922 Validation loss 0.010731403715908527 Accuracy 0.8857421875\n",
      "Iteration 29310 Training loss 0.0038174211513251066 Validation loss 0.010893022641539574 Accuracy 0.88330078125\n",
      "Iteration 29320 Training loss 0.00438288738951087 Validation loss 0.010947063565254211 Accuracy 0.8828125\n",
      "Iteration 29330 Training loss 0.004472789820283651 Validation loss 0.010976869612932205 Accuracy 0.8828125\n",
      "Iteration 29340 Training loss 0.00496105058118701 Validation loss 0.01067350897938013 Accuracy 0.8857421875\n",
      "Iteration 29350 Training loss 0.004346616566181183 Validation loss 0.010921883396804333 Accuracy 0.8828125\n",
      "Iteration 29360 Training loss 0.0046560377813875675 Validation loss 0.011087138205766678 Accuracy 0.8818359375\n",
      "Iteration 29370 Training loss 0.005729490891098976 Validation loss 0.011270038783550262 Accuracy 0.87939453125\n",
      "Iteration 29380 Training loss 0.005988226737827063 Validation loss 0.011153627187013626 Accuracy 0.8818359375\n",
      "Iteration 29390 Training loss 0.0051888711750507355 Validation loss 0.01084721926599741 Accuracy 0.884765625\n",
      "Iteration 29400 Training loss 0.004870621953159571 Validation loss 0.010841774754226208 Accuracy 0.884765625\n",
      "Iteration 29410 Training loss 0.004820794332772493 Validation loss 0.010760756209492683 Accuracy 0.88525390625\n",
      "Iteration 29420 Training loss 0.0048086936585605145 Validation loss 0.01137343980371952 Accuracy 0.87939453125\n",
      "Iteration 29430 Training loss 0.00460700923576951 Validation loss 0.01082959771156311 Accuracy 0.88427734375\n",
      "Iteration 29440 Training loss 0.004617720376700163 Validation loss 0.010839556343853474 Accuracy 0.884765625\n",
      "Iteration 29450 Training loss 0.006499655544757843 Validation loss 0.010664314031600952 Accuracy 0.88623046875\n",
      "Iteration 29460 Training loss 0.0033231403212994337 Validation loss 0.010824068449437618 Accuracy 0.8837890625\n",
      "Iteration 29470 Training loss 0.0034663076512515545 Validation loss 0.010899035260081291 Accuracy 0.88330078125\n",
      "Iteration 29480 Training loss 0.005978791508823633 Validation loss 0.010706898756325245 Accuracy 0.88720703125\n",
      "Iteration 29490 Training loss 0.005441118963062763 Validation loss 0.0108480928465724 Accuracy 0.8837890625\n",
      "Iteration 29500 Training loss 0.004265658091753721 Validation loss 0.010906178504228592 Accuracy 0.8837890625\n",
      "Iteration 29510 Training loss 0.004104589577764273 Validation loss 0.010968572460114956 Accuracy 0.88330078125\n",
      "Iteration 29520 Training loss 0.004649971146136522 Validation loss 0.010766535066068172 Accuracy 0.884765625\n",
      "Iteration 29530 Training loss 0.005391914863139391 Validation loss 0.010820573195815086 Accuracy 0.884765625\n",
      "Iteration 29540 Training loss 0.00465340306982398 Validation loss 0.011075317859649658 Accuracy 0.8818359375\n",
      "Iteration 29550 Training loss 0.0035346876829862595 Validation loss 0.010991334915161133 Accuracy 0.8818359375\n",
      "Iteration 29560 Training loss 0.0040864297188818455 Validation loss 0.010767140425741673 Accuracy 0.8857421875\n",
      "Iteration 29570 Training loss 0.0035432903096079826 Validation loss 0.01096492912620306 Accuracy 0.88330078125\n",
      "Iteration 29580 Training loss 0.005223878659307957 Validation loss 0.011111181229352951 Accuracy 0.88037109375\n",
      "Iteration 29590 Training loss 0.004685277119278908 Validation loss 0.01076614111661911 Accuracy 0.8857421875\n",
      "Iteration 29600 Training loss 0.0036129786167293787 Validation loss 0.010852540843188763 Accuracy 0.884765625\n",
      "Iteration 29610 Training loss 0.00498345447704196 Validation loss 0.010827071033418179 Accuracy 0.8837890625\n",
      "Iteration 29620 Training loss 0.005173817276954651 Validation loss 0.010839779861271381 Accuracy 0.884765625\n",
      "Iteration 29630 Training loss 0.004763668868690729 Validation loss 0.011029540561139584 Accuracy 0.88330078125\n",
      "Iteration 29640 Training loss 0.006248741876333952 Validation loss 0.011001154780387878 Accuracy 0.88232421875\n",
      "Iteration 29650 Training loss 0.004749090876430273 Validation loss 0.010977184399962425 Accuracy 0.8828125\n",
      "Iteration 29660 Training loss 0.0037389148492366076 Validation loss 0.011077712289988995 Accuracy 0.88134765625\n",
      "Iteration 29670 Training loss 0.00371455866843462 Validation loss 0.011109672486782074 Accuracy 0.8818359375\n",
      "Iteration 29680 Training loss 0.005847801920026541 Validation loss 0.011416954919695854 Accuracy 0.87841796875\n",
      "Iteration 29690 Training loss 0.004723937716335058 Validation loss 0.010961068794131279 Accuracy 0.8837890625\n",
      "Iteration 29700 Training loss 0.004809125792235136 Validation loss 0.011259269900619984 Accuracy 0.880859375\n",
      "Iteration 29710 Training loss 0.0028604702092707157 Validation loss 0.010846535675227642 Accuracy 0.88427734375\n",
      "Iteration 29720 Training loss 0.005786539521068335 Validation loss 0.010702221654355526 Accuracy 0.88623046875\n",
      "Iteration 29730 Training loss 0.00378051376901567 Validation loss 0.010837818495929241 Accuracy 0.88427734375\n",
      "Iteration 29740 Training loss 0.0045422702096402645 Validation loss 0.010702555999159813 Accuracy 0.88623046875\n",
      "Iteration 29750 Training loss 0.003934090957045555 Validation loss 0.011055574752390385 Accuracy 0.88037109375\n",
      "Iteration 29760 Training loss 0.005159594584256411 Validation loss 0.010703378356993198 Accuracy 0.88623046875\n",
      "Iteration 29770 Training loss 0.00542704900726676 Validation loss 0.010880931280553341 Accuracy 0.8837890625\n",
      "Iteration 29780 Training loss 0.0037199973594397306 Validation loss 0.01085752248764038 Accuracy 0.8837890625\n",
      "Iteration 29790 Training loss 0.004398445598781109 Validation loss 0.010981088504195213 Accuracy 0.88232421875\n",
      "Iteration 29800 Training loss 0.005824640393257141 Validation loss 0.011087482795119286 Accuracy 0.88232421875\n",
      "Iteration 29810 Training loss 0.005424892529845238 Validation loss 0.0108941113576293 Accuracy 0.88427734375\n",
      "Iteration 29820 Training loss 0.0056408606469631195 Validation loss 0.011320584453642368 Accuracy 0.88037109375\n",
      "Iteration 29830 Training loss 0.006112379487603903 Validation loss 0.011063388548791409 Accuracy 0.88232421875\n",
      "Iteration 29840 Training loss 0.005272905807942152 Validation loss 0.011249138042330742 Accuracy 0.8798828125\n",
      "Iteration 29850 Training loss 0.004919779486954212 Validation loss 0.010901418514549732 Accuracy 0.8837890625\n",
      "Iteration 29860 Training loss 0.004383999388664961 Validation loss 0.010956364683806896 Accuracy 0.8828125\n",
      "Iteration 29870 Training loss 0.004110144451260567 Validation loss 0.010914365760982037 Accuracy 0.88232421875\n",
      "Iteration 29880 Training loss 0.0033323729876428843 Validation loss 0.010741925798356533 Accuracy 0.8837890625\n",
      "Iteration 29890 Training loss 0.004348321817815304 Validation loss 0.01072611752897501 Accuracy 0.884765625\n",
      "Iteration 29900 Training loss 0.005062439013272524 Validation loss 0.0107295336201787 Accuracy 0.8857421875\n",
      "Iteration 29910 Training loss 0.005188846029341221 Validation loss 0.010807372629642487 Accuracy 0.884765625\n",
      "Iteration 29920 Training loss 0.005294200498610735 Validation loss 0.010649084113538265 Accuracy 0.88623046875\n",
      "Iteration 29930 Training loss 0.004860466346144676 Validation loss 0.010644831694662571 Accuracy 0.88525390625\n",
      "Iteration 29940 Training loss 0.0028164274990558624 Validation loss 0.01081504113972187 Accuracy 0.88427734375\n",
      "Iteration 29950 Training loss 0.004983908962458372 Validation loss 0.011016580276191235 Accuracy 0.88134765625\n",
      "Iteration 29960 Training loss 0.004544044379144907 Validation loss 0.011117368936538696 Accuracy 0.880859375\n",
      "Iteration 29970 Training loss 0.0036456845700740814 Validation loss 0.01147610042244196 Accuracy 0.8779296875\n",
      "Iteration 29980 Training loss 0.003942081239074469 Validation loss 0.01095989253371954 Accuracy 0.8828125\n",
      "Iteration 29990 Training loss 0.005808466579765081 Validation loss 0.011275500059127808 Accuracy 0.87890625\n",
      "Iteration 30000 Training loss 0.0055571370758116245 Validation loss 0.011869830079376698 Accuracy 0.87353515625\n",
      "Iteration 30010 Training loss 0.005714822094887495 Validation loss 0.0111030712723732 Accuracy 0.880859375\n",
      "Iteration 30020 Training loss 0.004373088013380766 Validation loss 0.010882353410124779 Accuracy 0.8837890625\n",
      "Iteration 30030 Training loss 0.004284994211047888 Validation loss 0.010833202861249447 Accuracy 0.8857421875\n",
      "Iteration 30040 Training loss 0.003500918624922633 Validation loss 0.011037897318601608 Accuracy 0.8818359375\n",
      "Iteration 30050 Training loss 0.0030660470947623253 Validation loss 0.01085782703012228 Accuracy 0.8837890625\n",
      "Iteration 30060 Training loss 0.004104133695363998 Validation loss 0.010947726666927338 Accuracy 0.88330078125\n",
      "Iteration 30070 Training loss 0.004852416459470987 Validation loss 0.01101107057183981 Accuracy 0.88232421875\n",
      "Iteration 30080 Training loss 0.003367512719705701 Validation loss 0.010608881711959839 Accuracy 0.88720703125\n",
      "Iteration 30090 Training loss 0.00459663849323988 Validation loss 0.010999553836882114 Accuracy 0.88232421875\n",
      "Iteration 30100 Training loss 0.004271337296813726 Validation loss 0.01069981511682272 Accuracy 0.884765625\n",
      "Iteration 30110 Training loss 0.005261733662337065 Validation loss 0.01085656601935625 Accuracy 0.88427734375\n",
      "Iteration 30120 Training loss 0.0056141954846680164 Validation loss 0.010841130279004574 Accuracy 0.88427734375\n",
      "Iteration 30130 Training loss 0.004444010555744171 Validation loss 0.010769783519208431 Accuracy 0.8857421875\n",
      "Iteration 30140 Training loss 0.007100529968738556 Validation loss 0.011194844730198383 Accuracy 0.880859375\n",
      "Iteration 30150 Training loss 0.004325644578784704 Validation loss 0.010955717414617538 Accuracy 0.8828125\n",
      "Iteration 30160 Training loss 0.005576052237302065 Validation loss 0.01101616770029068 Accuracy 0.8818359375\n",
      "Iteration 30170 Training loss 0.004341470543295145 Validation loss 0.010674107819795609 Accuracy 0.88671875\n",
      "Iteration 30180 Training loss 0.005846785847097635 Validation loss 0.011184696108102798 Accuracy 0.880859375\n",
      "Iteration 30190 Training loss 0.004787237849086523 Validation loss 0.011059770360589027 Accuracy 0.8818359375\n",
      "Iteration 30200 Training loss 0.005385789088904858 Validation loss 0.011527501046657562 Accuracy 0.87646484375\n",
      "Iteration 30210 Training loss 0.004955342970788479 Validation loss 0.01089449692517519 Accuracy 0.8837890625\n",
      "Iteration 30220 Training loss 0.003965455107390881 Validation loss 0.01071945484727621 Accuracy 0.8857421875\n",
      "Iteration 30230 Training loss 0.003954870626330376 Validation loss 0.010996975004673004 Accuracy 0.88330078125\n",
      "Iteration 30240 Training loss 0.003976465202867985 Validation loss 0.010825044475495815 Accuracy 0.88330078125\n",
      "Iteration 30250 Training loss 0.003460913896560669 Validation loss 0.010758018121123314 Accuracy 0.884765625\n",
      "Iteration 30260 Training loss 0.003097300184890628 Validation loss 0.01087441947311163 Accuracy 0.88525390625\n",
      "Iteration 30270 Training loss 0.004805384203791618 Validation loss 0.01066604070365429 Accuracy 0.8857421875\n",
      "Iteration 30280 Training loss 0.004483513999730349 Validation loss 0.010979191400110722 Accuracy 0.88330078125\n",
      "Iteration 30290 Training loss 0.005656997673213482 Validation loss 0.010975339449942112 Accuracy 0.88232421875\n",
      "Iteration 30300 Training loss 0.0047288015484809875 Validation loss 0.010857603512704372 Accuracy 0.884765625\n",
      "Iteration 30310 Training loss 0.005536925513297319 Validation loss 0.010691729374229908 Accuracy 0.88671875\n",
      "Iteration 30320 Training loss 0.005858779884874821 Validation loss 0.0110005559399724 Accuracy 0.88330078125\n",
      "Iteration 30330 Training loss 0.004621861036866903 Validation loss 0.010810361243784428 Accuracy 0.884765625\n",
      "Iteration 30340 Training loss 0.005277101881802082 Validation loss 0.010860146023333073 Accuracy 0.8837890625\n",
      "Iteration 30350 Training loss 0.0055312784388661385 Validation loss 0.010860557667911053 Accuracy 0.88427734375\n",
      "Iteration 30360 Training loss 0.005863516591489315 Validation loss 0.011123303323984146 Accuracy 0.880859375\n",
      "Iteration 30370 Training loss 0.0047205109149217606 Validation loss 0.010982565581798553 Accuracy 0.8828125\n",
      "Iteration 30380 Training loss 0.004967504646629095 Validation loss 0.010915413498878479 Accuracy 0.8837890625\n",
      "Iteration 30390 Training loss 0.004217574838548899 Validation loss 0.01079352293163538 Accuracy 0.88427734375\n",
      "Iteration 30400 Training loss 0.004842076450586319 Validation loss 0.010903866961598396 Accuracy 0.88330078125\n",
      "Iteration 30410 Training loss 0.0041956715285778046 Validation loss 0.010712802410125732 Accuracy 0.8837890625\n",
      "Iteration 30420 Training loss 0.004978125914931297 Validation loss 0.010816018097102642 Accuracy 0.88330078125\n",
      "Iteration 30430 Training loss 0.004496362060308456 Validation loss 0.010592514649033546 Accuracy 0.88671875\n",
      "Iteration 30440 Training loss 0.006684437859803438 Validation loss 0.011133923195302486 Accuracy 0.88134765625\n",
      "Iteration 30450 Training loss 0.004554123617708683 Validation loss 0.011086962185800076 Accuracy 0.880859375\n",
      "Iteration 30460 Training loss 0.003396997693926096 Validation loss 0.010990693233907223 Accuracy 0.8828125\n",
      "Iteration 30470 Training loss 0.004300704225897789 Validation loss 0.010630575940012932 Accuracy 0.8857421875\n",
      "Iteration 30480 Training loss 0.0048478394746780396 Validation loss 0.010890546254813671 Accuracy 0.8837890625\n",
      "Iteration 30490 Training loss 0.004392225760966539 Validation loss 0.010654906742274761 Accuracy 0.88525390625\n",
      "Iteration 30500 Training loss 0.0038435962051153183 Validation loss 0.010886437259614468 Accuracy 0.8837890625\n",
      "Iteration 30510 Training loss 0.005944414529949427 Validation loss 0.010833202861249447 Accuracy 0.8857421875\n",
      "Iteration 30520 Training loss 0.0024626993108540773 Validation loss 0.010895822197198868 Accuracy 0.88330078125\n",
      "Iteration 30530 Training loss 0.0038705470506101847 Validation loss 0.01062572468072176 Accuracy 0.88623046875\n",
      "Iteration 30540 Training loss 0.003690283978357911 Validation loss 0.010708806104958057 Accuracy 0.88671875\n",
      "Iteration 30550 Training loss 0.004745991434901953 Validation loss 0.010683168657124043 Accuracy 0.88623046875\n",
      "Iteration 30560 Training loss 0.00585869699716568 Validation loss 0.010802575387060642 Accuracy 0.884765625\n",
      "Iteration 30570 Training loss 0.00424750242382288 Validation loss 0.011023999191820621 Accuracy 0.8837890625\n",
      "Iteration 30580 Training loss 0.006565842777490616 Validation loss 0.010855128988623619 Accuracy 0.884765625\n",
      "Iteration 30590 Training loss 0.0036851107142865658 Validation loss 0.010819641873240471 Accuracy 0.88330078125\n",
      "Iteration 30600 Training loss 0.005634370259940624 Validation loss 0.011338585987687111 Accuracy 0.8779296875\n",
      "Iteration 30610 Training loss 0.0038047574926167727 Validation loss 0.010940180160105228 Accuracy 0.88330078125\n",
      "Iteration 30620 Training loss 0.003904378740116954 Validation loss 0.01082814671099186 Accuracy 0.88427734375\n",
      "Iteration 30630 Training loss 0.004116291645914316 Validation loss 0.010960829444229603 Accuracy 0.88330078125\n",
      "Iteration 30640 Training loss 0.004044858738780022 Validation loss 0.010911151766777039 Accuracy 0.8837890625\n",
      "Iteration 30650 Training loss 0.004144128877669573 Validation loss 0.010751012712717056 Accuracy 0.884765625\n",
      "Iteration 30660 Training loss 0.005892462562769651 Validation loss 0.011613293550908566 Accuracy 0.876953125\n",
      "Iteration 30670 Training loss 0.0033771656453609467 Validation loss 0.011076798662543297 Accuracy 0.88134765625\n",
      "Iteration 30680 Training loss 0.005098687019199133 Validation loss 0.010712088085711002 Accuracy 0.88623046875\n",
      "Iteration 30690 Training loss 0.0035050385631620884 Validation loss 0.011135891079902649 Accuracy 0.88037109375\n",
      "Iteration 30700 Training loss 0.004170111380517483 Validation loss 0.010837907902896404 Accuracy 0.8837890625\n",
      "Iteration 30710 Training loss 0.005583995021879673 Validation loss 0.010946308262646198 Accuracy 0.88232421875\n",
      "Iteration 30720 Training loss 0.0042596557177603245 Validation loss 0.011245915666222572 Accuracy 0.8798828125\n",
      "Iteration 30730 Training loss 0.004634143318980932 Validation loss 0.011512823402881622 Accuracy 0.87646484375\n",
      "Iteration 30740 Training loss 0.002066435292363167 Validation loss 0.010797392576932907 Accuracy 0.88427734375\n",
      "Iteration 30750 Training loss 0.0051337555050849915 Validation loss 0.01083255372941494 Accuracy 0.8837890625\n",
      "Iteration 30760 Training loss 0.0041565606370568275 Validation loss 0.010752313770353794 Accuracy 0.8857421875\n",
      "Iteration 30770 Training loss 0.0049967519007623196 Validation loss 0.010758311487734318 Accuracy 0.8857421875\n",
      "Iteration 30780 Training loss 0.004947444424033165 Validation loss 0.010804432444274426 Accuracy 0.88330078125\n",
      "Iteration 30790 Training loss 0.0029918975196778774 Validation loss 0.01086256094276905 Accuracy 0.8837890625\n",
      "Iteration 30800 Training loss 0.004376294557005167 Validation loss 0.011281008832156658 Accuracy 0.88037109375\n",
      "Iteration 30810 Training loss 0.0034801498986780643 Validation loss 0.010800573043525219 Accuracy 0.884765625\n",
      "Iteration 30820 Training loss 0.00543979974463582 Validation loss 0.010690995492041111 Accuracy 0.8857421875\n",
      "Iteration 30830 Training loss 0.00448132399469614 Validation loss 0.010658873245120049 Accuracy 0.88623046875\n",
      "Iteration 30840 Training loss 0.0048216562718153 Validation loss 0.010716731660068035 Accuracy 0.88525390625\n",
      "Iteration 30850 Training loss 0.004078918602317572 Validation loss 0.010751930996775627 Accuracy 0.884765625\n",
      "Iteration 30860 Training loss 0.004335327539592981 Validation loss 0.011265846900641918 Accuracy 0.8798828125\n",
      "Iteration 30870 Training loss 0.00432689068838954 Validation loss 0.010787195526063442 Accuracy 0.8837890625\n",
      "Iteration 30880 Training loss 0.004119638353586197 Validation loss 0.010681398212909698 Accuracy 0.88671875\n",
      "Iteration 30890 Training loss 0.004441136494278908 Validation loss 0.01093828584998846 Accuracy 0.88330078125\n",
      "Iteration 30900 Training loss 0.0041491081938147545 Validation loss 0.010844831354916096 Accuracy 0.8837890625\n",
      "Iteration 30910 Training loss 0.004953906871378422 Validation loss 0.011216589249670506 Accuracy 0.8798828125\n",
      "Iteration 30920 Training loss 0.004574904218316078 Validation loss 0.01067846268415451 Accuracy 0.88623046875\n",
      "Iteration 30930 Training loss 0.003986324183642864 Validation loss 0.010831418447196484 Accuracy 0.88427734375\n",
      "Iteration 30940 Training loss 0.003992181736975908 Validation loss 0.010819591581821442 Accuracy 0.88427734375\n",
      "Iteration 30950 Training loss 0.003853201400488615 Validation loss 0.010920140892267227 Accuracy 0.88232421875\n",
      "Iteration 30960 Training loss 0.006228045094758272 Validation loss 0.010808904655277729 Accuracy 0.884765625\n",
      "Iteration 30970 Training loss 0.0036003892309963703 Validation loss 0.011134328320622444 Accuracy 0.88037109375\n",
      "Iteration 30980 Training loss 0.0031995840836316347 Validation loss 0.010926916263997555 Accuracy 0.8828125\n",
      "Iteration 30990 Training loss 0.005498308688402176 Validation loss 0.010971122421324253 Accuracy 0.88232421875\n",
      "Iteration 31000 Training loss 0.0033174988348037004 Validation loss 0.011191142722964287 Accuracy 0.8798828125\n",
      "Iteration 31010 Training loss 0.005351154133677483 Validation loss 0.011119348928332329 Accuracy 0.8818359375\n",
      "Iteration 31020 Training loss 0.005897365976125002 Validation loss 0.01095669623464346 Accuracy 0.8837890625\n",
      "Iteration 31030 Training loss 0.00344702391885221 Validation loss 0.010747460648417473 Accuracy 0.88623046875\n",
      "Iteration 31040 Training loss 0.004480310250073671 Validation loss 0.010975395329296589 Accuracy 0.8828125\n",
      "Iteration 31050 Training loss 0.004880865570157766 Validation loss 0.010812930762767792 Accuracy 0.884765625\n",
      "Iteration 31060 Training loss 0.00512280547991395 Validation loss 0.010911780409514904 Accuracy 0.8837890625\n",
      "Iteration 31070 Training loss 0.004191204905509949 Validation loss 0.010958444327116013 Accuracy 0.88330078125\n",
      "Iteration 31080 Training loss 0.0055156610906124115 Validation loss 0.010645892471075058 Accuracy 0.88623046875\n",
      "Iteration 31090 Training loss 0.0042665572836995125 Validation loss 0.011440690606832504 Accuracy 0.87939453125\n",
      "Iteration 31100 Training loss 0.0053774514235556126 Validation loss 0.01100255362689495 Accuracy 0.88232421875\n",
      "Iteration 31110 Training loss 0.00381491519510746 Validation loss 0.010801617987453938 Accuracy 0.884765625\n",
      "Iteration 31120 Training loss 0.003640042617917061 Validation loss 0.011076935566961765 Accuracy 0.8818359375\n",
      "Iteration 31130 Training loss 0.005033995024859905 Validation loss 0.010526652447879314 Accuracy 0.88720703125\n",
      "Iteration 31140 Training loss 0.00453717028722167 Validation loss 0.01061245333403349 Accuracy 0.88720703125\n",
      "Iteration 31150 Training loss 0.005006662104278803 Validation loss 0.010877924971282482 Accuracy 0.8828125\n",
      "Iteration 31160 Training loss 0.0036060635466128588 Validation loss 0.010730107314884663 Accuracy 0.88525390625\n",
      "Iteration 31170 Training loss 0.003551291534677148 Validation loss 0.011271169409155846 Accuracy 0.87939453125\n",
      "Iteration 31180 Training loss 0.003935503773391247 Validation loss 0.010549189522862434 Accuracy 0.8857421875\n",
      "Iteration 31190 Training loss 0.0044927699491381645 Validation loss 0.011083110235631466 Accuracy 0.88232421875\n",
      "Iteration 31200 Training loss 0.004403355065733194 Validation loss 0.010840934701263905 Accuracy 0.8837890625\n",
      "Iteration 31210 Training loss 0.004344445653259754 Validation loss 0.010786877013742924 Accuracy 0.88427734375\n",
      "Iteration 31220 Training loss 0.005160504020750523 Validation loss 0.010979914106428623 Accuracy 0.88232421875\n",
      "Iteration 31230 Training loss 0.0049318671226501465 Validation loss 0.011100558564066887 Accuracy 0.88134765625\n",
      "Iteration 31240 Training loss 0.004139300901442766 Validation loss 0.010806004516780376 Accuracy 0.88525390625\n",
      "Iteration 31250 Training loss 0.005071793217211962 Validation loss 0.011119483038783073 Accuracy 0.88134765625\n",
      "Iteration 31260 Training loss 0.005756495054811239 Validation loss 0.011098467744886875 Accuracy 0.8818359375\n",
      "Iteration 31270 Training loss 0.003790505463257432 Validation loss 0.010680925101041794 Accuracy 0.884765625\n",
      "Iteration 31280 Training loss 0.0039028720930218697 Validation loss 0.011047233827412128 Accuracy 0.8828125\n",
      "Iteration 31290 Training loss 0.004551181569695473 Validation loss 0.011069483123719692 Accuracy 0.8818359375\n",
      "Iteration 31300 Training loss 0.006047310307621956 Validation loss 0.011002499610185623 Accuracy 0.88232421875\n",
      "Iteration 31310 Training loss 0.006276139989495277 Validation loss 0.010976062156260014 Accuracy 0.8837890625\n",
      "Iteration 31320 Training loss 0.004292989149689674 Validation loss 0.010937334038317204 Accuracy 0.8828125\n",
      "Iteration 31330 Training loss 0.0033729188144207 Validation loss 0.010959445498883724 Accuracy 0.88330078125\n",
      "Iteration 31340 Training loss 0.005148808006197214 Validation loss 0.010937614366412163 Accuracy 0.88232421875\n",
      "Iteration 31350 Training loss 0.004703248851001263 Validation loss 0.011143231764435768 Accuracy 0.88134765625\n",
      "Iteration 31360 Training loss 0.005174565594643354 Validation loss 0.011216484010219574 Accuracy 0.88037109375\n",
      "Iteration 31370 Training loss 0.005361303687095642 Validation loss 0.011249919421970844 Accuracy 0.8798828125\n",
      "Iteration 31380 Training loss 0.004873988218605518 Validation loss 0.01100040040910244 Accuracy 0.8818359375\n",
      "Iteration 31390 Training loss 0.004949276335537434 Validation loss 0.011036536656320095 Accuracy 0.88134765625\n",
      "Iteration 31400 Training loss 0.005103531293570995 Validation loss 0.010822062380611897 Accuracy 0.8837890625\n",
      "Iteration 31410 Training loss 0.004201948177069426 Validation loss 0.010819323360919952 Accuracy 0.88525390625\n",
      "Iteration 31420 Training loss 0.005579747259616852 Validation loss 0.011144174262881279 Accuracy 0.880859375\n",
      "Iteration 31430 Training loss 0.0034258426167070866 Validation loss 0.010769755579531193 Accuracy 0.88671875\n",
      "Iteration 31440 Training loss 0.002971055917441845 Validation loss 0.010963900946080685 Accuracy 0.88330078125\n",
      "Iteration 31450 Training loss 0.006104466505348682 Validation loss 0.011081363074481487 Accuracy 0.8818359375\n",
      "Iteration 31460 Training loss 0.005927270278334618 Validation loss 0.010842588730156422 Accuracy 0.8837890625\n",
      "Iteration 31470 Training loss 0.0046914624981582165 Validation loss 0.011046637780964375 Accuracy 0.8818359375\n",
      "Iteration 31480 Training loss 0.005100517999380827 Validation loss 0.011243682354688644 Accuracy 0.8798828125\n",
      "Iteration 31490 Training loss 0.004168997053056955 Validation loss 0.011024422012269497 Accuracy 0.8828125\n",
      "Iteration 31500 Training loss 0.004140152595937252 Validation loss 0.010946877300739288 Accuracy 0.8828125\n",
      "Iteration 31510 Training loss 0.005562166683375835 Validation loss 0.010935490019619465 Accuracy 0.8837890625\n",
      "Iteration 31520 Training loss 0.004180236253887415 Validation loss 0.010804391466081142 Accuracy 0.88427734375\n",
      "Iteration 31530 Training loss 0.0056005967780947685 Validation loss 0.010894659906625748 Accuracy 0.88330078125\n",
      "Iteration 31540 Training loss 0.004716651979833841 Validation loss 0.010644704103469849 Accuracy 0.88671875\n",
      "Iteration 31550 Training loss 0.005273770075291395 Validation loss 0.01086992397904396 Accuracy 0.8828125\n",
      "Iteration 31560 Training loss 0.0051124864257872105 Validation loss 0.010757612995803356 Accuracy 0.8857421875\n",
      "Iteration 31570 Training loss 0.0037599713541567326 Validation loss 0.01075740810483694 Accuracy 0.8857421875\n",
      "Iteration 31580 Training loss 0.003972606733441353 Validation loss 0.01064188964664936 Accuracy 0.88671875\n",
      "Iteration 31590 Training loss 0.005405975505709648 Validation loss 0.011043749749660492 Accuracy 0.88232421875\n",
      "Iteration 31600 Training loss 0.004232839681208134 Validation loss 0.011001062579452991 Accuracy 0.88232421875\n",
      "Iteration 31610 Training loss 0.005694124381989241 Validation loss 0.010914214886724949 Accuracy 0.8837890625\n",
      "Iteration 31620 Training loss 0.005886517930775881 Validation loss 0.010937709361314774 Accuracy 0.8837890625\n",
      "Iteration 31630 Training loss 0.004115065094083548 Validation loss 0.011238094419240952 Accuracy 0.880859375\n",
      "Iteration 31640 Training loss 0.005061344243586063 Validation loss 0.010696006007492542 Accuracy 0.8857421875\n",
      "Iteration 31650 Training loss 0.00429181195795536 Validation loss 0.010979472659528255 Accuracy 0.8828125\n",
      "Iteration 31660 Training loss 0.002146426821127534 Validation loss 0.010686774738132954 Accuracy 0.88623046875\n",
      "Iteration 31670 Training loss 0.006371852941811085 Validation loss 0.01157389860600233 Accuracy 0.8759765625\n",
      "Iteration 31680 Training loss 0.005767158232629299 Validation loss 0.011585701256990433 Accuracy 0.876953125\n",
      "Iteration 31690 Training loss 0.0047712200321257114 Validation loss 0.010905612260103226 Accuracy 0.88330078125\n",
      "Iteration 31700 Training loss 0.0038234605453908443 Validation loss 0.010984336026012897 Accuracy 0.8837890625\n",
      "Iteration 31710 Training loss 0.003551566507667303 Validation loss 0.011469691060483456 Accuracy 0.87744140625\n",
      "Iteration 31720 Training loss 0.003553427755832672 Validation loss 0.010931521654129028 Accuracy 0.88330078125\n",
      "Iteration 31730 Training loss 0.004531043581664562 Validation loss 0.011172887869179249 Accuracy 0.88037109375\n",
      "Iteration 31740 Training loss 0.0045265681110322475 Validation loss 0.010943254455924034 Accuracy 0.88330078125\n",
      "Iteration 31750 Training loss 0.004396943841129541 Validation loss 0.011124537326395512 Accuracy 0.88232421875\n",
      "Iteration 31760 Training loss 0.00582940923050046 Validation loss 0.010824312455952168 Accuracy 0.884765625\n",
      "Iteration 31770 Training loss 0.005281899590045214 Validation loss 0.010935906320810318 Accuracy 0.88330078125\n",
      "Iteration 31780 Training loss 0.003330484963953495 Validation loss 0.010982739739120007 Accuracy 0.8828125\n",
      "Iteration 31790 Training loss 0.0043899305164813995 Validation loss 0.010623305104672909 Accuracy 0.88623046875\n",
      "Iteration 31800 Training loss 0.0028585351537913084 Validation loss 0.010832448489964008 Accuracy 0.88427734375\n",
      "Iteration 31810 Training loss 0.002778795547783375 Validation loss 0.010834981687366962 Accuracy 0.88427734375\n",
      "Iteration 31820 Training loss 0.004609460011124611 Validation loss 0.011016464792191982 Accuracy 0.8818359375\n",
      "Iteration 31830 Training loss 0.004451681859791279 Validation loss 0.010761188343167305 Accuracy 0.8857421875\n",
      "Iteration 31840 Training loss 0.005183685105293989 Validation loss 0.010913964360952377 Accuracy 0.8837890625\n",
      "Iteration 31850 Training loss 0.0030396778602153063 Validation loss 0.011103067547082901 Accuracy 0.880859375\n",
      "Iteration 31860 Training loss 0.001838846830651164 Validation loss 0.01120965089648962 Accuracy 0.88037109375\n",
      "Iteration 31870 Training loss 0.004107271321117878 Validation loss 0.010671098716557026 Accuracy 0.88525390625\n",
      "Iteration 31880 Training loss 0.004328918643295765 Validation loss 0.010706956498324871 Accuracy 0.88525390625\n",
      "Iteration 31890 Training loss 0.005884375423192978 Validation loss 0.010717513971030712 Accuracy 0.88623046875\n",
      "Iteration 31900 Training loss 0.0038470663130283356 Validation loss 0.011075077578425407 Accuracy 0.88232421875\n",
      "Iteration 31910 Training loss 0.004034124314785004 Validation loss 0.011058583855628967 Accuracy 0.88134765625\n",
      "Iteration 31920 Training loss 0.004646408837288618 Validation loss 0.01088693831115961 Accuracy 0.8837890625\n",
      "Iteration 31930 Training loss 0.004672906827181578 Validation loss 0.010793942958116531 Accuracy 0.88525390625\n",
      "Iteration 31940 Training loss 0.003896568203344941 Validation loss 0.010746069252490997 Accuracy 0.884765625\n",
      "Iteration 31950 Training loss 0.004347424954175949 Validation loss 0.010620578192174435 Accuracy 0.88623046875\n",
      "Iteration 31960 Training loss 0.004475774709135294 Validation loss 0.010604835115373135 Accuracy 0.88623046875\n",
      "Iteration 31970 Training loss 0.005243619438260794 Validation loss 0.010564272291958332 Accuracy 0.88671875\n",
      "Iteration 31980 Training loss 0.004397282842546701 Validation loss 0.011307724751532078 Accuracy 0.87890625\n",
      "Iteration 31990 Training loss 0.004720652941614389 Validation loss 0.011217696592211723 Accuracy 0.88037109375\n",
      "Iteration 32000 Training loss 0.0030243650544434786 Validation loss 0.010939721018075943 Accuracy 0.8828125\n",
      "Iteration 32010 Training loss 0.003833167953416705 Validation loss 0.011027218773961067 Accuracy 0.8818359375\n",
      "Iteration 32020 Training loss 0.0051598562858998775 Validation loss 0.010839283466339111 Accuracy 0.884765625\n",
      "Iteration 32030 Training loss 0.004009635653346777 Validation loss 0.010837763547897339 Accuracy 0.88427734375\n",
      "Iteration 32040 Training loss 0.0032982202246785164 Validation loss 0.0108168451115489 Accuracy 0.88427734375\n",
      "Iteration 32050 Training loss 0.0031099007464945316 Validation loss 0.010658476501703262 Accuracy 0.88671875\n",
      "Iteration 32060 Training loss 0.005254153162240982 Validation loss 0.010820351541042328 Accuracy 0.8837890625\n",
      "Iteration 32070 Training loss 0.00428271247074008 Validation loss 0.010645834729075432 Accuracy 0.88623046875\n",
      "Iteration 32080 Training loss 0.0037310628686100245 Validation loss 0.010670309886336327 Accuracy 0.88671875\n",
      "Iteration 32090 Training loss 0.004065487068146467 Validation loss 0.01103740930557251 Accuracy 0.88232421875\n",
      "Iteration 32100 Training loss 0.0032278571743518114 Validation loss 0.010674002580344677 Accuracy 0.88623046875\n",
      "Iteration 32110 Training loss 0.00591536657884717 Validation loss 0.01060159970074892 Accuracy 0.88623046875\n",
      "Iteration 32120 Training loss 0.004613233730196953 Validation loss 0.010773643851280212 Accuracy 0.884765625\n",
      "Iteration 32130 Training loss 0.004273300990462303 Validation loss 0.011098110117018223 Accuracy 0.88037109375\n",
      "Iteration 32140 Training loss 0.0032055103220045567 Validation loss 0.010970944538712502 Accuracy 0.8828125\n",
      "Iteration 32150 Training loss 0.004330867901444435 Validation loss 0.010842876508831978 Accuracy 0.88427734375\n",
      "Iteration 32160 Training loss 0.005151910707354546 Validation loss 0.011028554290533066 Accuracy 0.8828125\n",
      "Iteration 32170 Training loss 0.0042187818326056 Validation loss 0.010905536822974682 Accuracy 0.88232421875\n",
      "Iteration 32180 Training loss 0.0053675477392971516 Validation loss 0.010987752117216587 Accuracy 0.88232421875\n",
      "Iteration 32190 Training loss 0.0043939449824392796 Validation loss 0.010901431553065777 Accuracy 0.8818359375\n",
      "Iteration 32200 Training loss 0.004317714832723141 Validation loss 0.010707603767514229 Accuracy 0.8857421875\n",
      "Iteration 32210 Training loss 0.004662804771214724 Validation loss 0.010733617469668388 Accuracy 0.884765625\n",
      "Iteration 32220 Training loss 0.005536725278943777 Validation loss 0.010583207942545414 Accuracy 0.88671875\n",
      "Iteration 32230 Training loss 0.004609308205544949 Validation loss 0.010855416767299175 Accuracy 0.88427734375\n",
      "Iteration 32240 Training loss 0.00491061108186841 Validation loss 0.011078180745244026 Accuracy 0.8818359375\n",
      "Iteration 32250 Training loss 0.004225555807352066 Validation loss 0.011333678849041462 Accuracy 0.87890625\n",
      "Iteration 32260 Training loss 0.003920887131243944 Validation loss 0.01071077398955822 Accuracy 0.8857421875\n",
      "Iteration 32270 Training loss 0.004407515749335289 Validation loss 0.010547948069870472 Accuracy 0.8876953125\n",
      "Iteration 32280 Training loss 0.006479048170149326 Validation loss 0.011335753835737705 Accuracy 0.8798828125\n",
      "Iteration 32290 Training loss 0.00478538079187274 Validation loss 0.01086372323334217 Accuracy 0.88427734375\n",
      "Iteration 32300 Training loss 0.006258392706513405 Validation loss 0.011302500031888485 Accuracy 0.87939453125\n",
      "Iteration 32310 Training loss 0.0043335240334272385 Validation loss 0.011119249276816845 Accuracy 0.8818359375\n",
      "Iteration 32320 Training loss 0.0026934235356748104 Validation loss 0.010931984521448612 Accuracy 0.88232421875\n",
      "Iteration 32330 Training loss 0.0046277763321995735 Validation loss 0.010832516476511955 Accuracy 0.8837890625\n",
      "Iteration 32340 Training loss 0.004722808953374624 Validation loss 0.01085522398352623 Accuracy 0.88427734375\n",
      "Iteration 32350 Training loss 0.004049745388329029 Validation loss 0.010831057094037533 Accuracy 0.884765625\n",
      "Iteration 32360 Training loss 0.0038746611680835485 Validation loss 0.01123164501041174 Accuracy 0.87939453125\n",
      "Iteration 32370 Training loss 0.004919133614748716 Validation loss 0.011224939487874508 Accuracy 0.88037109375\n",
      "Iteration 32380 Training loss 0.003970598801970482 Validation loss 0.01081396546214819 Accuracy 0.8837890625\n",
      "Iteration 32390 Training loss 0.005789207294583321 Validation loss 0.010752072557806969 Accuracy 0.884765625\n",
      "Iteration 32400 Training loss 0.004466718528419733 Validation loss 0.011045686900615692 Accuracy 0.88232421875\n",
      "Iteration 32410 Training loss 0.004132927395403385 Validation loss 0.010875711217522621 Accuracy 0.8828125\n",
      "Iteration 32420 Training loss 0.002871833508834243 Validation loss 0.010747932828962803 Accuracy 0.884765625\n",
      "Iteration 32430 Training loss 0.0033283112570643425 Validation loss 0.010711432434618473 Accuracy 0.88525390625\n",
      "Iteration 32440 Training loss 0.00601611565798521 Validation loss 0.011057589203119278 Accuracy 0.8818359375\n",
      "Iteration 32450 Training loss 0.005084620323032141 Validation loss 0.011022454127669334 Accuracy 0.8818359375\n",
      "Iteration 32460 Training loss 0.004398518707603216 Validation loss 0.01092294417321682 Accuracy 0.88330078125\n",
      "Iteration 32470 Training loss 0.004408855922520161 Validation loss 0.01081507746130228 Accuracy 0.88427734375\n",
      "Iteration 32480 Training loss 0.004215228836983442 Validation loss 0.010681762360036373 Accuracy 0.88671875\n",
      "Iteration 32490 Training loss 0.0050824303179979324 Validation loss 0.010984404012560844 Accuracy 0.88330078125\n",
      "Iteration 32500 Training loss 0.003947945777326822 Validation loss 0.01078780461102724 Accuracy 0.884765625\n",
      "Iteration 32510 Training loss 0.00476955808699131 Validation loss 0.010938703082501888 Accuracy 0.8828125\n",
      "Iteration 32520 Training loss 0.004354530479758978 Validation loss 0.011225280351936817 Accuracy 0.880859375\n",
      "Iteration 32530 Training loss 0.0044415900483727455 Validation loss 0.010677271522581577 Accuracy 0.8857421875\n",
      "Iteration 32540 Training loss 0.003996229264885187 Validation loss 0.010909689590334892 Accuracy 0.88427734375\n",
      "Iteration 32550 Training loss 0.003482840722426772 Validation loss 0.010619720444083214 Accuracy 0.88671875\n",
      "Iteration 32560 Training loss 0.00462913466617465 Validation loss 0.0106849679723382 Accuracy 0.88671875\n",
      "Iteration 32570 Training loss 0.004471719264984131 Validation loss 0.010913119651377201 Accuracy 0.88330078125\n",
      "Iteration 32580 Training loss 0.0037855191621929407 Validation loss 0.01047446858137846 Accuracy 0.88916015625\n",
      "Iteration 32590 Training loss 0.004434030503034592 Validation loss 0.010901901870965958 Accuracy 0.88330078125\n",
      "Iteration 32600 Training loss 0.004555427003651857 Validation loss 0.011055037379264832 Accuracy 0.8818359375\n",
      "Iteration 32610 Training loss 0.004556297790259123 Validation loss 0.010918264277279377 Accuracy 0.8828125\n",
      "Iteration 32620 Training loss 0.003432704834267497 Validation loss 0.010929894633591175 Accuracy 0.8828125\n",
      "Iteration 32630 Training loss 0.005237719044089317 Validation loss 0.010951080359518528 Accuracy 0.8828125\n",
      "Iteration 32640 Training loss 0.0061220149509608746 Validation loss 0.010952351614832878 Accuracy 0.8837890625\n",
      "Iteration 32650 Training loss 0.005278280004858971 Validation loss 0.01081344299018383 Accuracy 0.88330078125\n",
      "Iteration 32660 Training loss 0.004018843173980713 Validation loss 0.011071174405515194 Accuracy 0.88134765625\n",
      "Iteration 32670 Training loss 0.0043284217827022076 Validation loss 0.010993212461471558 Accuracy 0.8818359375\n",
      "Iteration 32680 Training loss 0.004761618562042713 Validation loss 0.011094614863395691 Accuracy 0.88232421875\n",
      "Iteration 32690 Training loss 0.004370864015072584 Validation loss 0.010781490243971348 Accuracy 0.884765625\n",
      "Iteration 32700 Training loss 0.003274446353316307 Validation loss 0.011022888123989105 Accuracy 0.8818359375\n",
      "Iteration 32710 Training loss 0.004327783361077309 Validation loss 0.010973017662763596 Accuracy 0.8828125\n",
      "Iteration 32720 Training loss 0.005031533073633909 Validation loss 0.01131435763090849 Accuracy 0.8798828125\n",
      "Iteration 32730 Training loss 0.0034304093569517136 Validation loss 0.010806691832840443 Accuracy 0.884765625\n",
      "Iteration 32740 Training loss 0.005746681243181229 Validation loss 0.01109572034329176 Accuracy 0.8818359375\n",
      "Iteration 32750 Training loss 0.005088745150715113 Validation loss 0.010988202877342701 Accuracy 0.88232421875\n",
      "Iteration 32760 Training loss 0.004674089141190052 Validation loss 0.011225550435483456 Accuracy 0.8798828125\n",
      "Iteration 32770 Training loss 0.00602866243571043 Validation loss 0.011137118563055992 Accuracy 0.880859375\n",
      "Iteration 32780 Training loss 0.0038157710805535316 Validation loss 0.010998928919434547 Accuracy 0.88330078125\n",
      "Iteration 32790 Training loss 0.004067955072969198 Validation loss 0.011027832515537739 Accuracy 0.88232421875\n",
      "Iteration 32800 Training loss 0.0054297661408782005 Validation loss 0.011040979996323586 Accuracy 0.88330078125\n",
      "Iteration 32810 Training loss 0.003435928840190172 Validation loss 0.010787682607769966 Accuracy 0.88427734375\n",
      "Iteration 32820 Training loss 0.0046866838820278645 Validation loss 0.010835767723619938 Accuracy 0.8837890625\n",
      "Iteration 32830 Training loss 0.005198596976697445 Validation loss 0.011068952269852161 Accuracy 0.88232421875\n",
      "Iteration 32840 Training loss 0.0037758664693683386 Validation loss 0.011029308661818504 Accuracy 0.88232421875\n",
      "Iteration 32850 Training loss 0.003656294196844101 Validation loss 0.01070279348641634 Accuracy 0.8857421875\n",
      "Iteration 32860 Training loss 0.003252635942772031 Validation loss 0.011233613826334476 Accuracy 0.87890625\n",
      "Iteration 32870 Training loss 0.003752944292500615 Validation loss 0.01064382866024971 Accuracy 0.88623046875\n",
      "Iteration 32880 Training loss 0.004136998206377029 Validation loss 0.010831503197550774 Accuracy 0.88427734375\n",
      "Iteration 32890 Training loss 0.0038380834739655256 Validation loss 0.010724467225372791 Accuracy 0.88525390625\n",
      "Iteration 32900 Training loss 0.004727063234895468 Validation loss 0.010746709071099758 Accuracy 0.88427734375\n",
      "Iteration 32910 Training loss 0.004604453220963478 Validation loss 0.011143451556563377 Accuracy 0.880859375\n",
      "Iteration 32920 Training loss 0.004631541669368744 Validation loss 0.010793864727020264 Accuracy 0.884765625\n",
      "Iteration 32930 Training loss 0.003627423895522952 Validation loss 0.01108162198215723 Accuracy 0.8818359375\n",
      "Iteration 32940 Training loss 0.003569595282897353 Validation loss 0.010862451046705246 Accuracy 0.8837890625\n",
      "Iteration 32950 Training loss 0.004242564085870981 Validation loss 0.010836588218808174 Accuracy 0.884765625\n",
      "Iteration 32960 Training loss 0.0038186025340110064 Validation loss 0.010680194944143295 Accuracy 0.88525390625\n",
      "Iteration 32970 Training loss 0.003839680226519704 Validation loss 0.010763873346149921 Accuracy 0.88525390625\n",
      "Iteration 32980 Training loss 0.0033206010702997446 Validation loss 0.0107376454398036 Accuracy 0.88623046875\n",
      "Iteration 32990 Training loss 0.003950855694711208 Validation loss 0.010628225281834602 Accuracy 0.88671875\n",
      "Iteration 33000 Training loss 0.0024702849332243204 Validation loss 0.010701566003262997 Accuracy 0.88427734375\n",
      "Iteration 33010 Training loss 0.003848726861178875 Validation loss 0.011030180379748344 Accuracy 0.8837890625\n",
      "Iteration 33020 Training loss 0.00515358243137598 Validation loss 0.010735942982137203 Accuracy 0.88525390625\n",
      "Iteration 33030 Training loss 0.00373170361854136 Validation loss 0.01097936648875475 Accuracy 0.88232421875\n",
      "Iteration 33040 Training loss 0.0046373396180570126 Validation loss 0.011101544834673405 Accuracy 0.880859375\n",
      "Iteration 33050 Training loss 0.005802375730127096 Validation loss 0.011002451181411743 Accuracy 0.88232421875\n",
      "Iteration 33060 Training loss 0.0035446512047201395 Validation loss 0.010946481488645077 Accuracy 0.8828125\n",
      "Iteration 33070 Training loss 0.0039151799865067005 Validation loss 0.010595700703561306 Accuracy 0.88720703125\n",
      "Iteration 33080 Training loss 0.003070816630497575 Validation loss 0.010642502456903458 Accuracy 0.8857421875\n",
      "Iteration 33090 Training loss 0.006220775656402111 Validation loss 0.010798564180731773 Accuracy 0.88427734375\n",
      "Iteration 33100 Training loss 0.0028962958604097366 Validation loss 0.010961931198835373 Accuracy 0.88330078125\n",
      "Iteration 33110 Training loss 0.0028196556959301233 Validation loss 0.010956325568258762 Accuracy 0.88232421875\n",
      "Iteration 33120 Training loss 0.0028297773096710443 Validation loss 0.010607575066387653 Accuracy 0.88623046875\n",
      "Iteration 33130 Training loss 0.003209441900253296 Validation loss 0.010991591960191727 Accuracy 0.8818359375\n",
      "Iteration 33140 Training loss 0.0029535649809986353 Validation loss 0.011105154640972614 Accuracy 0.88134765625\n",
      "Iteration 33150 Training loss 0.0040715476498007774 Validation loss 0.010852055624127388 Accuracy 0.884765625\n",
      "Iteration 33160 Training loss 0.004994231276214123 Validation loss 0.011163346469402313 Accuracy 0.88134765625\n",
      "Iteration 33170 Training loss 0.005246736574918032 Validation loss 0.011387697421014309 Accuracy 0.87890625\n",
      "Iteration 33180 Training loss 0.0037345504388213158 Validation loss 0.01078566163778305 Accuracy 0.884765625\n",
      "Iteration 33190 Training loss 0.0033912425860762596 Validation loss 0.010744764469563961 Accuracy 0.884765625\n",
      "Iteration 33200 Training loss 0.003615268040448427 Validation loss 0.010980380699038506 Accuracy 0.88232421875\n",
      "Iteration 33210 Training loss 0.004025888629257679 Validation loss 0.010692574083805084 Accuracy 0.8857421875\n",
      "Iteration 33220 Training loss 0.00352338794618845 Validation loss 0.010929650627076626 Accuracy 0.8837890625\n",
      "Iteration 33230 Training loss 0.003633905667811632 Validation loss 0.010690322145819664 Accuracy 0.8857421875\n",
      "Iteration 33240 Training loss 0.003989413380622864 Validation loss 0.010916011407971382 Accuracy 0.88330078125\n",
      "Iteration 33250 Training loss 0.0045518456026911736 Validation loss 0.010860924609005451 Accuracy 0.8837890625\n",
      "Iteration 33260 Training loss 0.003612356260418892 Validation loss 0.010630689561367035 Accuracy 0.88720703125\n",
      "Iteration 33270 Training loss 0.002819906920194626 Validation loss 0.01087268814444542 Accuracy 0.8837890625\n",
      "Iteration 33280 Training loss 0.004695784766227007 Validation loss 0.010846245102584362 Accuracy 0.8837890625\n",
      "Iteration 33290 Training loss 0.0039842319674789906 Validation loss 0.01081534381955862 Accuracy 0.8837890625\n",
      "Iteration 33300 Training loss 0.005377980414777994 Validation loss 0.010784359648823738 Accuracy 0.8837890625\n",
      "Iteration 33310 Training loss 0.0042716036550700665 Validation loss 0.011012404225766659 Accuracy 0.8818359375\n",
      "Iteration 33320 Training loss 0.004128015134483576 Validation loss 0.010639316402375698 Accuracy 0.8857421875\n",
      "Iteration 33330 Training loss 0.005490334704518318 Validation loss 0.010733290575444698 Accuracy 0.8857421875\n",
      "Iteration 33340 Training loss 0.004314555320888758 Validation loss 0.010697780176997185 Accuracy 0.884765625\n",
      "Iteration 33350 Training loss 0.003423704532906413 Validation loss 0.010586815886199474 Accuracy 0.88671875\n",
      "Iteration 33360 Training loss 0.0035565050784498453 Validation loss 0.010712162591516972 Accuracy 0.884765625\n",
      "Iteration 33370 Training loss 0.0022400866728276014 Validation loss 0.010749934241175652 Accuracy 0.884765625\n",
      "Iteration 33380 Training loss 0.0033880623523145914 Validation loss 0.010682749561965466 Accuracy 0.8857421875\n",
      "Iteration 33390 Training loss 0.004072363488376141 Validation loss 0.010782182216644287 Accuracy 0.88525390625\n",
      "Iteration 33400 Training loss 0.0034743594005703926 Validation loss 0.010934404097497463 Accuracy 0.8837890625\n",
      "Iteration 33410 Training loss 0.004012517165392637 Validation loss 0.010658390820026398 Accuracy 0.88671875\n",
      "Iteration 33420 Training loss 0.003746998030692339 Validation loss 0.010522580705583096 Accuracy 0.88720703125\n",
      "Iteration 33430 Training loss 0.0034578489139676094 Validation loss 0.010604627430438995 Accuracy 0.88720703125\n",
      "Iteration 33440 Training loss 0.00491076335310936 Validation loss 0.010810215026140213 Accuracy 0.88427734375\n",
      "Iteration 33450 Training loss 0.0031140141654759645 Validation loss 0.011228105053305626 Accuracy 0.87939453125\n",
      "Iteration 33460 Training loss 0.005564397666603327 Validation loss 0.010912366211414337 Accuracy 0.8828125\n",
      "Iteration 33470 Training loss 0.003958546556532383 Validation loss 0.010699967853724957 Accuracy 0.884765625\n",
      "Iteration 33480 Training loss 0.00397328520193696 Validation loss 0.010783437639474869 Accuracy 0.8837890625\n",
      "Iteration 33490 Training loss 0.004665007349103689 Validation loss 0.010900049470365047 Accuracy 0.88330078125\n",
      "Iteration 33500 Training loss 0.0027594028506428003 Validation loss 0.010959292761981487 Accuracy 0.8837890625\n",
      "Iteration 33510 Training loss 0.0031212009489536285 Validation loss 0.010982140898704529 Accuracy 0.88232421875\n",
      "Iteration 33520 Training loss 0.004973439034074545 Validation loss 0.010950585827231407 Accuracy 0.88232421875\n",
      "Iteration 33530 Training loss 0.004183343145996332 Validation loss 0.010821832343935966 Accuracy 0.88330078125\n",
      "Iteration 33540 Training loss 0.00406405096873641 Validation loss 0.010776513256132603 Accuracy 0.884765625\n",
      "Iteration 33550 Training loss 0.0049546463415026665 Validation loss 0.010931716300547123 Accuracy 0.88330078125\n",
      "Iteration 33560 Training loss 0.00440972251817584 Validation loss 0.010842653922736645 Accuracy 0.88427734375\n",
      "Iteration 33570 Training loss 0.00421183230355382 Validation loss 0.010635441169142723 Accuracy 0.88671875\n",
      "Iteration 33580 Training loss 0.005340487230569124 Validation loss 0.01090560108423233 Accuracy 0.8828125\n",
      "Iteration 33590 Training loss 0.003312312066555023 Validation loss 0.010805658996105194 Accuracy 0.88525390625\n",
      "Iteration 33600 Training loss 0.0039109098725020885 Validation loss 0.010908647440373898 Accuracy 0.8837890625\n",
      "Iteration 33610 Training loss 0.003929703030735254 Validation loss 0.010831701569259167 Accuracy 0.8837890625\n",
      "Iteration 33620 Training loss 0.003934184554964304 Validation loss 0.010723410174250603 Accuracy 0.8857421875\n",
      "Iteration 33630 Training loss 0.00501468637958169 Validation loss 0.010816257447004318 Accuracy 0.8837890625\n",
      "Iteration 33640 Training loss 0.005272608250379562 Validation loss 0.01111269649118185 Accuracy 0.880859375\n",
      "Iteration 33650 Training loss 0.004048841539770365 Validation loss 0.010752038098871708 Accuracy 0.88525390625\n",
      "Iteration 33660 Training loss 0.0039907703176140785 Validation loss 0.010730346664786339 Accuracy 0.88525390625\n",
      "Iteration 33670 Training loss 0.004992042668163776 Validation loss 0.011036170646548271 Accuracy 0.8818359375\n",
      "Iteration 33680 Training loss 0.003280091565102339 Validation loss 0.010715551674365997 Accuracy 0.88623046875\n",
      "Iteration 33690 Training loss 0.004729371517896652 Validation loss 0.010923155583441257 Accuracy 0.88330078125\n",
      "Iteration 33700 Training loss 0.0033117153216153383 Validation loss 0.010649400763213634 Accuracy 0.8857421875\n",
      "Iteration 33710 Training loss 0.003945090342313051 Validation loss 0.010694246739149094 Accuracy 0.8857421875\n",
      "Iteration 33720 Training loss 0.003367058001458645 Validation loss 0.010661757551133633 Accuracy 0.88623046875\n",
      "Iteration 33730 Training loss 0.0062856064178049564 Validation loss 0.011172765865921974 Accuracy 0.8818359375\n",
      "Iteration 33740 Training loss 0.0036620476748794317 Validation loss 0.010825887322425842 Accuracy 0.88427734375\n",
      "Iteration 33750 Training loss 0.003660490270704031 Validation loss 0.010839630849659443 Accuracy 0.8837890625\n",
      "Iteration 33760 Training loss 0.004040485247969627 Validation loss 0.0106351962313056 Accuracy 0.88623046875\n",
      "Iteration 33770 Training loss 0.003917057532817125 Validation loss 0.0108854491263628 Accuracy 0.8828125\n",
      "Iteration 33780 Training loss 0.004948843736201525 Validation loss 0.010818704962730408 Accuracy 0.884765625\n",
      "Iteration 33790 Training loss 0.004474584013223648 Validation loss 0.011019296944141388 Accuracy 0.8818359375\n",
      "Iteration 33800 Training loss 0.004848725162446499 Validation loss 0.01060874480754137 Accuracy 0.88818359375\n",
      "Iteration 33810 Training loss 0.004237025510519743 Validation loss 0.01072351448237896 Accuracy 0.8857421875\n",
      "Iteration 33820 Training loss 0.004263861104846001 Validation loss 0.010829446837306023 Accuracy 0.88427734375\n",
      "Iteration 33830 Training loss 0.0051608942449092865 Validation loss 0.01068209670484066 Accuracy 0.8857421875\n",
      "Iteration 33840 Training loss 0.004095019306987524 Validation loss 0.01072494313120842 Accuracy 0.88427734375\n",
      "Iteration 33850 Training loss 0.0028011936228722334 Validation loss 0.010833901353180408 Accuracy 0.88427734375\n",
      "Iteration 33860 Training loss 0.004279653541743755 Validation loss 0.010736840777099133 Accuracy 0.884765625\n",
      "Iteration 33870 Training loss 0.003970600198954344 Validation loss 0.011204764246940613 Accuracy 0.88037109375\n",
      "Iteration 33880 Training loss 0.0027757962234318256 Validation loss 0.010641010478138924 Accuracy 0.8857421875\n",
      "Iteration 33890 Training loss 0.004497618414461613 Validation loss 0.010746045038104057 Accuracy 0.8857421875\n",
      "Iteration 33900 Training loss 0.003988051787018776 Validation loss 0.01073980238288641 Accuracy 0.88525390625\n",
      "Iteration 33910 Training loss 0.002602776512503624 Validation loss 0.010684389621019363 Accuracy 0.88525390625\n",
      "Iteration 33920 Training loss 0.0033453062642365694 Validation loss 0.010843398049473763 Accuracy 0.8837890625\n",
      "Iteration 33930 Training loss 0.00425497954711318 Validation loss 0.011124026030302048 Accuracy 0.880859375\n",
      "Iteration 33940 Training loss 0.004085863009095192 Validation loss 0.010922165587544441 Accuracy 0.88427734375\n",
      "Iteration 33950 Training loss 0.004977871663868427 Validation loss 0.011244389228522778 Accuracy 0.88037109375\n",
      "Iteration 33960 Training loss 0.004596530459821224 Validation loss 0.010808886028826237 Accuracy 0.88427734375\n",
      "Iteration 33970 Training loss 0.003196943551301956 Validation loss 0.010860572569072247 Accuracy 0.88427734375\n",
      "Iteration 33980 Training loss 0.0043840110301971436 Validation loss 0.010746188461780548 Accuracy 0.88525390625\n",
      "Iteration 33990 Training loss 0.004258597269654274 Validation loss 0.011094927787780762 Accuracy 0.88232421875\n",
      "Iteration 34000 Training loss 0.0028519846964627504 Validation loss 0.010766090825200081 Accuracy 0.88525390625\n",
      "Iteration 34010 Training loss 0.0036055666860193014 Validation loss 0.010777665302157402 Accuracy 0.884765625\n",
      "Iteration 34020 Training loss 0.0017296220175921917 Validation loss 0.010636847466230392 Accuracy 0.88623046875\n",
      "Iteration 34030 Training loss 0.00436015147715807 Validation loss 0.010872932150959969 Accuracy 0.8837890625\n",
      "Iteration 34040 Training loss 0.00415085582062602 Validation loss 0.010770458728075027 Accuracy 0.8837890625\n",
      "Iteration 34050 Training loss 0.0037489801179617643 Validation loss 0.011028881184756756 Accuracy 0.88232421875\n",
      "Iteration 34060 Training loss 0.0027568244840949774 Validation loss 0.010524805635213852 Accuracy 0.88818359375\n",
      "Iteration 34070 Training loss 0.003652723506093025 Validation loss 0.01081712357699871 Accuracy 0.88427734375\n",
      "Iteration 34080 Training loss 0.00505864666774869 Validation loss 0.010964835993945599 Accuracy 0.88330078125\n",
      "Iteration 34090 Training loss 0.003291698405519128 Validation loss 0.01071025338023901 Accuracy 0.8857421875\n",
      "Iteration 34100 Training loss 0.0033863417338579893 Validation loss 0.010668774135410786 Accuracy 0.88427734375\n",
      "Iteration 34110 Training loss 0.0031702760607004166 Validation loss 0.010485044680535793 Accuracy 0.88720703125\n",
      "Iteration 34120 Training loss 0.004793370608240366 Validation loss 0.010672868229448795 Accuracy 0.88525390625\n",
      "Iteration 34130 Training loss 0.004195982590317726 Validation loss 0.010607192292809486 Accuracy 0.88671875\n",
      "Iteration 34140 Training loss 0.004721298813819885 Validation loss 0.011131828650832176 Accuracy 0.88037109375\n",
      "Iteration 34150 Training loss 0.003983092028647661 Validation loss 0.010526093654334545 Accuracy 0.88720703125\n",
      "Iteration 34160 Training loss 0.0034550291020423174 Validation loss 0.010598180815577507 Accuracy 0.88720703125\n",
      "Iteration 34170 Training loss 0.004159938544034958 Validation loss 0.010663808323442936 Accuracy 0.88525390625\n",
      "Iteration 34180 Training loss 0.0037573804147541523 Validation loss 0.010661930777132511 Accuracy 0.88671875\n",
      "Iteration 34190 Training loss 0.004542081151157618 Validation loss 0.010516955517232418 Accuracy 0.88720703125\n",
      "Iteration 34200 Training loss 0.001858071656897664 Validation loss 0.01075170747935772 Accuracy 0.88427734375\n",
      "Iteration 34210 Training loss 0.004519709385931492 Validation loss 0.010605229996144772 Accuracy 0.88671875\n",
      "Iteration 34220 Training loss 0.003032634500414133 Validation loss 0.010625412687659264 Accuracy 0.8857421875\n",
      "Iteration 34230 Training loss 0.003985768184065819 Validation loss 0.010761383920907974 Accuracy 0.88525390625\n",
      "Iteration 34240 Training loss 0.004193683620542288 Validation loss 0.010441751219332218 Accuracy 0.88916015625\n",
      "Iteration 34250 Training loss 0.00462412741035223 Validation loss 0.01086872536689043 Accuracy 0.8828125\n",
      "Iteration 34260 Training loss 0.0032750011887401342 Validation loss 0.010779890231788158 Accuracy 0.884765625\n",
      "Iteration 34270 Training loss 0.004586648195981979 Validation loss 0.010794973000884056 Accuracy 0.88427734375\n",
      "Iteration 34280 Training loss 0.003568991320207715 Validation loss 0.010900571011006832 Accuracy 0.88330078125\n",
      "Iteration 34290 Training loss 0.0024436863604933023 Validation loss 0.010609986260533333 Accuracy 0.88623046875\n",
      "Iteration 34300 Training loss 0.0033922067377716303 Validation loss 0.010955603793263435 Accuracy 0.8828125\n",
      "Iteration 34310 Training loss 0.004014408215880394 Validation loss 0.01072665210813284 Accuracy 0.884765625\n",
      "Iteration 34320 Training loss 0.003992513753473759 Validation loss 0.01090798806399107 Accuracy 0.88330078125\n",
      "Iteration 34330 Training loss 0.00444211158901453 Validation loss 0.010740506462752819 Accuracy 0.8857421875\n",
      "Iteration 34340 Training loss 0.0037417407147586346 Validation loss 0.010817743837833405 Accuracy 0.88525390625\n",
      "Iteration 34350 Training loss 0.0027181352488696575 Validation loss 0.011095789261162281 Accuracy 0.8818359375\n",
      "Iteration 34360 Training loss 0.003975201863795519 Validation loss 0.011006074957549572 Accuracy 0.88232421875\n",
      "Iteration 34370 Training loss 0.00428090849891305 Validation loss 0.010656053200364113 Accuracy 0.88720703125\n",
      "Iteration 34380 Training loss 0.0040893107652664185 Validation loss 0.010834538377821445 Accuracy 0.88427734375\n",
      "Iteration 34390 Training loss 0.0050042057409882545 Validation loss 0.010896110907196999 Accuracy 0.8828125\n",
      "Iteration 34400 Training loss 0.0040376088581979275 Validation loss 0.010442324914038181 Accuracy 0.88818359375\n",
      "Iteration 34410 Training loss 0.0038953889161348343 Validation loss 0.01076117530465126 Accuracy 0.8857421875\n",
      "Iteration 34420 Training loss 0.0038531608879566193 Validation loss 0.010653247125446796 Accuracy 0.88671875\n",
      "Iteration 34430 Training loss 0.0032801583874970675 Validation loss 0.010821598581969738 Accuracy 0.884765625\n",
      "Iteration 34440 Training loss 0.0048348503187298775 Validation loss 0.01056670118123293 Accuracy 0.88671875\n",
      "Iteration 34450 Training loss 0.003610792802646756 Validation loss 0.010480890981853008 Accuracy 0.8876953125\n",
      "Iteration 34460 Training loss 0.006085254717618227 Validation loss 0.01067467499524355 Accuracy 0.88671875\n",
      "Iteration 34470 Training loss 0.005653237458318472 Validation loss 0.01096299383789301 Accuracy 0.8837890625\n",
      "Iteration 34480 Training loss 0.003356215776875615 Validation loss 0.01052769459784031 Accuracy 0.88671875\n",
      "Iteration 34490 Training loss 0.004163626115769148 Validation loss 0.010639604181051254 Accuracy 0.88671875\n",
      "Iteration 34500 Training loss 0.001967987045645714 Validation loss 0.010869953781366348 Accuracy 0.88427734375\n",
      "Iteration 34510 Training loss 0.003410096513107419 Validation loss 0.010427983477711678 Accuracy 0.8876953125\n",
      "Iteration 34520 Training loss 0.004115809220820665 Validation loss 0.010609985329210758 Accuracy 0.88623046875\n",
      "Iteration 34530 Training loss 0.004317977465689182 Validation loss 0.010569603182375431 Accuracy 0.88623046875\n",
      "Iteration 34540 Training loss 0.005076994653791189 Validation loss 0.010714193806052208 Accuracy 0.8857421875\n",
      "Iteration 34550 Training loss 0.003717706073075533 Validation loss 0.010690978728234768 Accuracy 0.8857421875\n",
      "Iteration 34560 Training loss 0.005011768080294132 Validation loss 0.010934468358755112 Accuracy 0.8828125\n",
      "Iteration 34570 Training loss 0.004589442629367113 Validation loss 0.01077751163393259 Accuracy 0.88427734375\n",
      "Iteration 34580 Training loss 0.004808942321687937 Validation loss 0.010731556452810764 Accuracy 0.88525390625\n",
      "Iteration 34590 Training loss 0.004740843083709478 Validation loss 0.010504860430955887 Accuracy 0.88720703125\n",
      "Iteration 34600 Training loss 0.00416801730170846 Validation loss 0.010722805745899677 Accuracy 0.8857421875\n",
      "Iteration 34610 Training loss 0.005309318657964468 Validation loss 0.010738274082541466 Accuracy 0.8857421875\n",
      "Iteration 34620 Training loss 0.003459185129031539 Validation loss 0.010609938763082027 Accuracy 0.88671875\n",
      "Iteration 34630 Training loss 0.0034425207413733006 Validation loss 0.010495198890566826 Accuracy 0.88720703125\n",
      "Iteration 34640 Training loss 0.002729948377236724 Validation loss 0.010597281157970428 Accuracy 0.88671875\n",
      "Iteration 34650 Training loss 0.004304047673940659 Validation loss 0.010657568462193012 Accuracy 0.88671875\n",
      "Iteration 34660 Training loss 0.0035735988058149815 Validation loss 0.01092507317662239 Accuracy 0.88427734375\n",
      "Iteration 34670 Training loss 0.004146484192460775 Validation loss 0.01076589897274971 Accuracy 0.884765625\n",
      "Iteration 34680 Training loss 0.004020096268504858 Validation loss 0.010549260303378105 Accuracy 0.88623046875\n",
      "Iteration 34690 Training loss 0.004308838397264481 Validation loss 0.010702072642743587 Accuracy 0.88525390625\n",
      "Iteration 34700 Training loss 0.004919493570923805 Validation loss 0.010620733723044395 Accuracy 0.88623046875\n",
      "Iteration 34710 Training loss 0.004176286514848471 Validation loss 0.010652289725840092 Accuracy 0.8857421875\n",
      "Iteration 34720 Training loss 0.005780432373285294 Validation loss 0.010713648051023483 Accuracy 0.88427734375\n",
      "Iteration 34730 Training loss 0.003936357796192169 Validation loss 0.01089477725327015 Accuracy 0.8837890625\n",
      "Iteration 34740 Training loss 0.003492276882752776 Validation loss 0.010961857624351978 Accuracy 0.88330078125\n",
      "Iteration 34750 Training loss 0.003210330381989479 Validation loss 0.010633722878992558 Accuracy 0.88671875\n",
      "Iteration 34760 Training loss 0.0035892436280846596 Validation loss 0.010918903164565563 Accuracy 0.8828125\n",
      "Iteration 34770 Training loss 0.0035944196861237288 Validation loss 0.010764162056148052 Accuracy 0.884765625\n",
      "Iteration 34780 Training loss 0.004044190514832735 Validation loss 0.01085766963660717 Accuracy 0.88330078125\n",
      "Iteration 34790 Training loss 0.0030972864478826523 Validation loss 0.010578930377960205 Accuracy 0.88720703125\n",
      "Iteration 34800 Training loss 0.0044835819862782955 Validation loss 0.010903632268309593 Accuracy 0.8828125\n",
      "Iteration 34810 Training loss 0.003939968068152666 Validation loss 0.010705915279686451 Accuracy 0.8857421875\n",
      "Iteration 34820 Training loss 0.002311276737600565 Validation loss 0.010807658545672894 Accuracy 0.8837890625\n",
      "Iteration 34830 Training loss 0.004708480555564165 Validation loss 0.010857131332159042 Accuracy 0.8837890625\n",
      "Iteration 34840 Training loss 0.0028771287761628628 Validation loss 0.010608497075736523 Accuracy 0.88671875\n",
      "Iteration 34850 Training loss 0.0038815808948129416 Validation loss 0.010982723906636238 Accuracy 0.8828125\n",
      "Iteration 34860 Training loss 0.0033305182587355375 Validation loss 0.010748068802058697 Accuracy 0.8857421875\n",
      "Iteration 34870 Training loss 0.004664006642997265 Validation loss 0.010744498111307621 Accuracy 0.88525390625\n",
      "Iteration 34880 Training loss 0.0038612298667430878 Validation loss 0.010767120867967606 Accuracy 0.884765625\n",
      "Iteration 34890 Training loss 0.0039131250232458115 Validation loss 0.010638583451509476 Accuracy 0.88525390625\n",
      "Iteration 34900 Training loss 0.005134129896759987 Validation loss 0.010812672786414623 Accuracy 0.8857421875\n",
      "Iteration 34910 Training loss 0.0021646814420819283 Validation loss 0.010512595064938068 Accuracy 0.88818359375\n",
      "Iteration 34920 Training loss 0.003420647932216525 Validation loss 0.010934839025139809 Accuracy 0.88330078125\n",
      "Iteration 34930 Training loss 0.005173624958842993 Validation loss 0.011104047298431396 Accuracy 0.88037109375\n",
      "Iteration 34940 Training loss 0.004802670329809189 Validation loss 0.01128776092082262 Accuracy 0.87939453125\n",
      "Iteration 34950 Training loss 0.0044008283875882626 Validation loss 0.010984046384692192 Accuracy 0.8828125\n",
      "Iteration 34960 Training loss 0.0032494785264134407 Validation loss 0.01063966192305088 Accuracy 0.8857421875\n",
      "Iteration 34970 Training loss 0.004094126168638468 Validation loss 0.01080322265625 Accuracy 0.884765625\n",
      "Iteration 34980 Training loss 0.002725728088989854 Validation loss 0.01106314454227686 Accuracy 0.88232421875\n",
      "Iteration 34990 Training loss 0.003907849546521902 Validation loss 0.010877592489123344 Accuracy 0.88427734375\n",
      "Iteration 35000 Training loss 0.003981495276093483 Validation loss 0.01068701408803463 Accuracy 0.88623046875\n",
      "Iteration 35010 Training loss 0.0036080130375921726 Validation loss 0.010718139819800854 Accuracy 0.88623046875\n",
      "Iteration 35020 Training loss 0.00280218874104321 Validation loss 0.010534345172345638 Accuracy 0.88818359375\n",
      "Iteration 35030 Training loss 0.003842848353087902 Validation loss 0.010842076502740383 Accuracy 0.88427734375\n",
      "Iteration 35040 Training loss 0.00641426257789135 Validation loss 0.011155541986227036 Accuracy 0.88134765625\n",
      "Iteration 35050 Training loss 0.003977867774665356 Validation loss 0.011250981129705906 Accuracy 0.88037109375\n",
      "Iteration 35060 Training loss 0.0031793073285371065 Validation loss 0.010710970498621464 Accuracy 0.884765625\n",
      "Iteration 35070 Training loss 0.0036270199343562126 Validation loss 0.010585074312984943 Accuracy 0.88623046875\n",
      "Iteration 35080 Training loss 0.003745259018614888 Validation loss 0.010890553705394268 Accuracy 0.8828125\n",
      "Iteration 35090 Training loss 0.002470561536028981 Validation loss 0.01086230669170618 Accuracy 0.8837890625\n",
      "Iteration 35100 Training loss 0.003844399005174637 Validation loss 0.01087123341858387 Accuracy 0.8837890625\n",
      "Iteration 35110 Training loss 0.002966483822092414 Validation loss 0.010751388035714626 Accuracy 0.884765625\n",
      "Iteration 35120 Training loss 0.0035536193754523993 Validation loss 0.010607787407934666 Accuracy 0.88720703125\n",
      "Iteration 35130 Training loss 0.00529605895280838 Validation loss 0.010611995123326778 Accuracy 0.88671875\n",
      "Iteration 35140 Training loss 0.004225662909448147 Validation loss 0.010425186716020107 Accuracy 0.888671875\n",
      "Iteration 35150 Training loss 0.004727164749056101 Validation loss 0.010662255808711052 Accuracy 0.88623046875\n",
      "Iteration 35160 Training loss 0.003032383508980274 Validation loss 0.010470635257661343 Accuracy 0.8876953125\n",
      "Iteration 35170 Training loss 0.004634442739188671 Validation loss 0.01043753232806921 Accuracy 0.888671875\n",
      "Iteration 35180 Training loss 0.004464397206902504 Validation loss 0.010544857010245323 Accuracy 0.88623046875\n",
      "Iteration 35190 Training loss 0.0053275274112820625 Validation loss 0.010654345154762268 Accuracy 0.88671875\n",
      "Iteration 35200 Training loss 0.003310216823592782 Validation loss 0.010842653922736645 Accuracy 0.8837890625\n",
      "Iteration 35210 Training loss 0.0031639747321605682 Validation loss 0.01067531667649746 Accuracy 0.88671875\n",
      "Iteration 35220 Training loss 0.003978177439421415 Validation loss 0.010682487860321999 Accuracy 0.88525390625\n",
      "Iteration 35230 Training loss 0.0035616550594568253 Validation loss 0.010660006664693356 Accuracy 0.88623046875\n",
      "Iteration 35240 Training loss 0.003831845475360751 Validation loss 0.01059463620185852 Accuracy 0.88623046875\n",
      "Iteration 35250 Training loss 0.0029408843256533146 Validation loss 0.0105547234416008 Accuracy 0.8876953125\n",
      "Iteration 35260 Training loss 0.0040196385234594345 Validation loss 0.010698825120925903 Accuracy 0.8857421875\n",
      "Iteration 35270 Training loss 0.004745462443679571 Validation loss 0.01077356655150652 Accuracy 0.884765625\n",
      "Iteration 35280 Training loss 0.0028672395274043083 Validation loss 0.01065797172486782 Accuracy 0.88525390625\n",
      "Iteration 35290 Training loss 0.003915665205568075 Validation loss 0.010874169878661633 Accuracy 0.88427734375\n",
      "Iteration 35300 Training loss 0.002381540834903717 Validation loss 0.010798361152410507 Accuracy 0.88427734375\n",
      "Iteration 35310 Training loss 0.003231555223464966 Validation loss 0.01090123038738966 Accuracy 0.8837890625\n",
      "Iteration 35320 Training loss 0.0034168274141848087 Validation loss 0.01069610845297575 Accuracy 0.88623046875\n",
      "Iteration 35330 Training loss 0.004374855197966099 Validation loss 0.010688964277505875 Accuracy 0.8857421875\n",
      "Iteration 35340 Training loss 0.0033879552502185106 Validation loss 0.011181759648025036 Accuracy 0.88037109375\n",
      "Iteration 35350 Training loss 0.0035274671390652657 Validation loss 0.01053603459149599 Accuracy 0.8857421875\n",
      "Iteration 35360 Training loss 0.0034938398748636246 Validation loss 0.010507708415389061 Accuracy 0.88720703125\n",
      "Iteration 35370 Training loss 0.003633936634287238 Validation loss 0.010785551741719246 Accuracy 0.884765625\n",
      "Iteration 35380 Training loss 0.004308589734137058 Validation loss 0.010527340695261955 Accuracy 0.88720703125\n",
      "Iteration 35390 Training loss 0.003659340785816312 Validation loss 0.010855874046683311 Accuracy 0.8837890625\n",
      "Iteration 35400 Training loss 0.004242760129272938 Validation loss 0.010735002346336842 Accuracy 0.88427734375\n",
      "Iteration 35410 Training loss 0.0028201246168464422 Validation loss 0.010495348833501339 Accuracy 0.8876953125\n",
      "Iteration 35420 Training loss 0.0035618040710687637 Validation loss 0.010648906230926514 Accuracy 0.88671875\n",
      "Iteration 35430 Training loss 0.004679340403527021 Validation loss 0.010602914728224277 Accuracy 0.88623046875\n",
      "Iteration 35440 Training loss 0.003342516254633665 Validation loss 0.010743907652795315 Accuracy 0.8857421875\n",
      "Iteration 35450 Training loss 0.004330274648964405 Validation loss 0.010674314573407173 Accuracy 0.88671875\n",
      "Iteration 35460 Training loss 0.003273705020546913 Validation loss 0.010626726783812046 Accuracy 0.88623046875\n",
      "Iteration 35470 Training loss 0.00408944021910429 Validation loss 0.010707796551287174 Accuracy 0.8837890625\n",
      "Iteration 35480 Training loss 0.003012055065482855 Validation loss 0.010656788945198059 Accuracy 0.88623046875\n",
      "Iteration 35490 Training loss 0.004463179036974907 Validation loss 0.010697314515709877 Accuracy 0.884765625\n",
      "Iteration 35500 Training loss 0.0031552205327898264 Validation loss 0.010452832095324993 Accuracy 0.88720703125\n",
      "Iteration 35510 Training loss 0.004248297773301601 Validation loss 0.010600988753139973 Accuracy 0.88623046875\n",
      "Iteration 35520 Training loss 0.003913385793566704 Validation loss 0.010506799444556236 Accuracy 0.88720703125\n",
      "Iteration 35530 Training loss 0.003164831083267927 Validation loss 0.010568317957222462 Accuracy 0.88623046875\n",
      "Iteration 35540 Training loss 0.00512858759611845 Validation loss 0.010705034248530865 Accuracy 0.884765625\n",
      "Iteration 35550 Training loss 0.002965874271467328 Validation loss 0.01047747302800417 Accuracy 0.8876953125\n",
      "Iteration 35560 Training loss 0.0036661310587078333 Validation loss 0.010669810697436333 Accuracy 0.88525390625\n",
      "Iteration 35570 Training loss 0.0030068291816860437 Validation loss 0.010879010893404484 Accuracy 0.88330078125\n",
      "Iteration 35580 Training loss 0.003227442502975464 Validation loss 0.010730606503784657 Accuracy 0.88623046875\n",
      "Iteration 35590 Training loss 0.004942281637340784 Validation loss 0.01073923148214817 Accuracy 0.88525390625\n",
      "Iteration 35600 Training loss 0.0043551078997552395 Validation loss 0.010582407005131245 Accuracy 0.88671875\n",
      "Iteration 35610 Training loss 0.004343243781477213 Validation loss 0.010911609046161175 Accuracy 0.8837890625\n",
      "Iteration 35620 Training loss 0.004098216537386179 Validation loss 0.011025781743228436 Accuracy 0.88232421875\n",
      "Iteration 35630 Training loss 0.003904384560883045 Validation loss 0.010722331702709198 Accuracy 0.884765625\n",
      "Iteration 35640 Training loss 0.003676195628941059 Validation loss 0.0110397320240736 Accuracy 0.8818359375\n",
      "Iteration 35650 Training loss 0.004716707859188318 Validation loss 0.01089246105402708 Accuracy 0.8837890625\n",
      "Iteration 35660 Training loss 0.00428858632221818 Validation loss 0.010914078913629055 Accuracy 0.8837890625\n",
      "Iteration 35670 Training loss 0.003790342016145587 Validation loss 0.010810400359332561 Accuracy 0.8837890625\n",
      "Iteration 35680 Training loss 0.004402414429932833 Validation loss 0.010782519355416298 Accuracy 0.88427734375\n",
      "Iteration 35690 Training loss 0.004254714585840702 Validation loss 0.010564088821411133 Accuracy 0.88623046875\n",
      "Iteration 35700 Training loss 0.004516506567597389 Validation loss 0.010675751604139805 Accuracy 0.88525390625\n",
      "Iteration 35710 Training loss 0.003223998239263892 Validation loss 0.010823879390954971 Accuracy 0.88330078125\n",
      "Iteration 35720 Training loss 0.004135398659855127 Validation loss 0.0107726464048028 Accuracy 0.8837890625\n",
      "Iteration 35730 Training loss 0.0037541782949119806 Validation loss 0.010815690271556377 Accuracy 0.88427734375\n",
      "Iteration 35740 Training loss 0.003243156475946307 Validation loss 0.010712952353060246 Accuracy 0.88525390625\n",
      "Iteration 35750 Training loss 0.0035531367175281048 Validation loss 0.010860729962587357 Accuracy 0.8837890625\n",
      "Iteration 35760 Training loss 0.003567994339391589 Validation loss 0.010809347964823246 Accuracy 0.884765625\n",
      "Iteration 35770 Training loss 0.003509103087708354 Validation loss 0.010738871991634369 Accuracy 0.88525390625\n",
      "Iteration 35780 Training loss 0.002556792227551341 Validation loss 0.010679779574275017 Accuracy 0.88623046875\n",
      "Iteration 35790 Training loss 0.0034927462693303823 Validation loss 0.011097094044089317 Accuracy 0.8818359375\n",
      "Iteration 35800 Training loss 0.00456753745675087 Validation loss 0.010815047658979893 Accuracy 0.8837890625\n",
      "Iteration 35810 Training loss 0.0046477061696350574 Validation loss 0.010682868771255016 Accuracy 0.88525390625\n",
      "Iteration 35820 Training loss 0.0033061103895306587 Validation loss 0.010949073359370232 Accuracy 0.8837890625\n",
      "Iteration 35830 Training loss 0.0036126377526670694 Validation loss 0.010448700748383999 Accuracy 0.88916015625\n",
      "Iteration 35840 Training loss 0.0023154255468398333 Validation loss 0.01053767092525959 Accuracy 0.8876953125\n",
      "Iteration 35850 Training loss 0.0031691938638687134 Validation loss 0.01044089812785387 Accuracy 0.88818359375\n",
      "Iteration 35860 Training loss 0.0027825774159282446 Validation loss 0.010731462389230728 Accuracy 0.88427734375\n",
      "Iteration 35870 Training loss 0.003668418852612376 Validation loss 0.010799895040690899 Accuracy 0.88330078125\n",
      "Iteration 35880 Training loss 0.00515328673645854 Validation loss 0.010698813013732433 Accuracy 0.8857421875\n",
      "Iteration 35890 Training loss 0.002477058907970786 Validation loss 0.010473818518221378 Accuracy 0.88720703125\n",
      "Iteration 35900 Training loss 0.0043205395340919495 Validation loss 0.010753671638667583 Accuracy 0.884765625\n",
      "Iteration 35910 Training loss 0.0032395769376307726 Validation loss 0.010964426212012768 Accuracy 0.88232421875\n",
      "Iteration 35920 Training loss 0.004010110627859831 Validation loss 0.010741855017840862 Accuracy 0.88525390625\n",
      "Iteration 35930 Training loss 0.0024640585761517286 Validation loss 0.010532980784773827 Accuracy 0.88671875\n",
      "Iteration 35940 Training loss 0.002332933945581317 Validation loss 0.010450766421854496 Accuracy 0.88818359375\n",
      "Iteration 35950 Training loss 0.00326215079985559 Validation loss 0.010395695455372334 Accuracy 0.88818359375\n",
      "Iteration 35960 Training loss 0.004498028662055731 Validation loss 0.010735532268881798 Accuracy 0.88525390625\n",
      "Iteration 35970 Training loss 0.003636040957644582 Validation loss 0.010647078976035118 Accuracy 0.88623046875\n",
      "Iteration 35980 Training loss 0.002964960178360343 Validation loss 0.010583302937448025 Accuracy 0.88623046875\n",
      "Iteration 35990 Training loss 0.004759072791785002 Validation loss 0.010514536872506142 Accuracy 0.88818359375\n",
      "Iteration 36000 Training loss 0.0031433654949069023 Validation loss 0.010600373148918152 Accuracy 0.8876953125\n",
      "Iteration 36010 Training loss 0.002913164673373103 Validation loss 0.010679539293050766 Accuracy 0.88525390625\n",
      "Iteration 36020 Training loss 0.002736894879490137 Validation loss 0.01075492613017559 Accuracy 0.88427734375\n",
      "Iteration 36030 Training loss 0.005297939293086529 Validation loss 0.011054903268814087 Accuracy 0.8828125\n",
      "Iteration 36040 Training loss 0.0038161328993737698 Validation loss 0.011012006551027298 Accuracy 0.88134765625\n",
      "Iteration 36050 Training loss 0.00426826998591423 Validation loss 0.010664111003279686 Accuracy 0.88671875\n",
      "Iteration 36060 Training loss 0.004576284438371658 Validation loss 0.010954853147268295 Accuracy 0.88232421875\n",
      "Iteration 36070 Training loss 0.004718192853033543 Validation loss 0.010662108659744263 Accuracy 0.8857421875\n",
      "Iteration 36080 Training loss 0.001706258743070066 Validation loss 0.01060121413320303 Accuracy 0.88720703125\n",
      "Iteration 36090 Training loss 0.004346083849668503 Validation loss 0.010720604099333286 Accuracy 0.8857421875\n",
      "Iteration 36100 Training loss 0.0040605501271784306 Validation loss 0.010774074122309685 Accuracy 0.88427734375\n",
      "Iteration 36110 Training loss 0.004487066064029932 Validation loss 0.01092839241027832 Accuracy 0.8828125\n",
      "Iteration 36120 Training loss 0.004545832052826881 Validation loss 0.010882368311285973 Accuracy 0.8828125\n",
      "Iteration 36130 Training loss 0.003543820697814226 Validation loss 0.01064027938991785 Accuracy 0.884765625\n",
      "Iteration 36140 Training loss 0.00334048829972744 Validation loss 0.010553374886512756 Accuracy 0.88623046875\n",
      "Iteration 36150 Training loss 0.0036991785746067762 Validation loss 0.01085322443395853 Accuracy 0.8837890625\n",
      "Iteration 36160 Training loss 0.002913795178756118 Validation loss 0.010701720602810383 Accuracy 0.8857421875\n",
      "Iteration 36170 Training loss 0.0021013172809034586 Validation loss 0.010498707182705402 Accuracy 0.88818359375\n",
      "Iteration 36180 Training loss 0.003141904715448618 Validation loss 0.010562984272837639 Accuracy 0.8876953125\n",
      "Iteration 36190 Training loss 0.0038991656620055437 Validation loss 0.010567864403128624 Accuracy 0.8876953125\n",
      "Iteration 36200 Training loss 0.002814892213791609 Validation loss 0.01072164811193943 Accuracy 0.8857421875\n",
      "Iteration 36210 Training loss 0.003918673377484083 Validation loss 0.010664449073374271 Accuracy 0.88671875\n",
      "Iteration 36220 Training loss 0.00341430539265275 Validation loss 0.010689537972211838 Accuracy 0.8857421875\n",
      "Iteration 36230 Training loss 0.0042053209617733955 Validation loss 0.010679694823920727 Accuracy 0.8857421875\n",
      "Iteration 36240 Training loss 0.003965457901358604 Validation loss 0.010473400354385376 Accuracy 0.88720703125\n",
      "Iteration 36250 Training loss 0.004900349769741297 Validation loss 0.010853617452085018 Accuracy 0.8837890625\n",
      "Iteration 36260 Training loss 0.0032997499220073223 Validation loss 0.010549607686698437 Accuracy 0.8876953125\n",
      "Iteration 36270 Training loss 0.004997747018933296 Validation loss 0.010669332928955555 Accuracy 0.884765625\n",
      "Iteration 36280 Training loss 0.003384241135790944 Validation loss 0.010688884183764458 Accuracy 0.88671875\n",
      "Iteration 36290 Training loss 0.0052446285262703896 Validation loss 0.010696444660425186 Accuracy 0.88525390625\n",
      "Iteration 36300 Training loss 0.004410683177411556 Validation loss 0.010656972415745258 Accuracy 0.88623046875\n",
      "Iteration 36310 Training loss 0.003532655304297805 Validation loss 0.010616518557071686 Accuracy 0.88671875\n",
      "Iteration 36320 Training loss 0.004749819170683622 Validation loss 0.010588652454316616 Accuracy 0.88720703125\n",
      "Iteration 36330 Training loss 0.0020876999478787184 Validation loss 0.010667634196579456 Accuracy 0.88525390625\n",
      "Iteration 36340 Training loss 0.004750593099743128 Validation loss 0.010665018111467361 Accuracy 0.88623046875\n",
      "Iteration 36350 Training loss 0.0038364154752343893 Validation loss 0.010970763862133026 Accuracy 0.88232421875\n",
      "Iteration 36360 Training loss 0.003897078800946474 Validation loss 0.010768495500087738 Accuracy 0.88525390625\n",
      "Iteration 36370 Training loss 0.002745910082012415 Validation loss 0.010764742270112038 Accuracy 0.8837890625\n",
      "Iteration 36380 Training loss 0.005975375883281231 Validation loss 0.010946597903966904 Accuracy 0.8837890625\n",
      "Iteration 36390 Training loss 0.004983499646186829 Validation loss 0.01054054033011198 Accuracy 0.88818359375\n",
      "Iteration 36400 Training loss 0.0034635858610272408 Validation loss 0.010764660313725471 Accuracy 0.8837890625\n",
      "Iteration 36410 Training loss 0.005143146030604839 Validation loss 0.010645104572176933 Accuracy 0.88671875\n",
      "Iteration 36420 Training loss 0.0028082123026251793 Validation loss 0.010534721426665783 Accuracy 0.88720703125\n",
      "Iteration 36430 Training loss 0.004235481843352318 Validation loss 0.010585547424852848 Accuracy 0.8857421875\n",
      "Iteration 36440 Training loss 0.0032279007136821747 Validation loss 0.010467428714036942 Accuracy 0.88720703125\n",
      "Iteration 36450 Training loss 0.0049466597847640514 Validation loss 0.010805011726915836 Accuracy 0.884765625\n",
      "Iteration 36460 Training loss 0.0033675094600766897 Validation loss 0.010595371015369892 Accuracy 0.88623046875\n",
      "Iteration 36470 Training loss 0.0038405098021030426 Validation loss 0.010674513876438141 Accuracy 0.8857421875\n",
      "Iteration 36480 Training loss 0.003650421043857932 Validation loss 0.010452516376972198 Accuracy 0.88818359375\n",
      "Iteration 36490 Training loss 0.0035870710853487253 Validation loss 0.01053654309362173 Accuracy 0.88671875\n",
      "Iteration 36500 Training loss 0.0029753046110272408 Validation loss 0.01063003670424223 Accuracy 0.88720703125\n",
      "Iteration 36510 Training loss 0.0028548771515488625 Validation loss 0.010499925352633 Accuracy 0.88671875\n",
      "Iteration 36520 Training loss 0.0026308957021683455 Validation loss 0.010522028431296349 Accuracy 0.88818359375\n",
      "Iteration 36530 Training loss 0.0035645095631480217 Validation loss 0.010447812266647816 Accuracy 0.88818359375\n",
      "Iteration 36540 Training loss 0.0025277745444327593 Validation loss 0.010452847927808762 Accuracy 0.8876953125\n",
      "Iteration 36550 Training loss 0.003618543967604637 Validation loss 0.010542514733970165 Accuracy 0.88720703125\n",
      "Iteration 36560 Training loss 0.005518861580640078 Validation loss 0.010784158483147621 Accuracy 0.884765625\n",
      "Iteration 36570 Training loss 0.005025338847190142 Validation loss 0.010609987191855907 Accuracy 0.8876953125\n",
      "Iteration 36580 Training loss 0.004529902711510658 Validation loss 0.01079582516103983 Accuracy 0.88427734375\n",
      "Iteration 36590 Training loss 0.0038801743648946285 Validation loss 0.010475724004209042 Accuracy 0.88720703125\n",
      "Iteration 36600 Training loss 0.003690374316647649 Validation loss 0.0105948681011796 Accuracy 0.88623046875\n",
      "Iteration 36610 Training loss 0.002808949211612344 Validation loss 0.010572419501841068 Accuracy 0.8876953125\n",
      "Iteration 36620 Training loss 0.003008126514032483 Validation loss 0.01048513688147068 Accuracy 0.88720703125\n",
      "Iteration 36630 Training loss 0.004726713988929987 Validation loss 0.010451732203364372 Accuracy 0.88818359375\n",
      "Iteration 36640 Training loss 0.0021559062879532576 Validation loss 0.010587800294160843 Accuracy 0.88671875\n",
      "Iteration 36650 Training loss 0.0031345500610768795 Validation loss 0.010567289777100086 Accuracy 0.88671875\n",
      "Iteration 36660 Training loss 0.003288974752649665 Validation loss 0.010645541362464428 Accuracy 0.88671875\n",
      "Iteration 36670 Training loss 0.0036397892981767654 Validation loss 0.010549964383244514 Accuracy 0.88623046875\n",
      "Iteration 36680 Training loss 0.0030620836187154055 Validation loss 0.010647309012711048 Accuracy 0.88623046875\n",
      "Iteration 36690 Training loss 0.0024291719309985638 Validation loss 0.01060700137168169 Accuracy 0.88525390625\n",
      "Iteration 36700 Training loss 0.004054227378219366 Validation loss 0.01116152759641409 Accuracy 0.880859375\n",
      "Iteration 36710 Training loss 0.0037848185747861862 Validation loss 0.010802402161061764 Accuracy 0.88330078125\n",
      "Iteration 36720 Training loss 0.003934698179364204 Validation loss 0.010786386206746101 Accuracy 0.8837890625\n",
      "Iteration 36730 Training loss 0.0032127241138368845 Validation loss 0.011233046650886536 Accuracy 0.87939453125\n",
      "Iteration 36740 Training loss 0.003515304531902075 Validation loss 0.010683493688702583 Accuracy 0.88525390625\n",
      "Iteration 36750 Training loss 0.0031018052250146866 Validation loss 0.010721257887780666 Accuracy 0.8857421875\n",
      "Iteration 36760 Training loss 0.004504823125898838 Validation loss 0.01087319664657116 Accuracy 0.8837890625\n",
      "Iteration 36770 Training loss 0.0031502211932092905 Validation loss 0.010936353355646133 Accuracy 0.8818359375\n",
      "Iteration 36780 Training loss 0.002643260871991515 Validation loss 0.010778583586215973 Accuracy 0.88330078125\n",
      "Iteration 36790 Training loss 0.0035762032493948936 Validation loss 0.010596193373203278 Accuracy 0.8876953125\n",
      "Iteration 36800 Training loss 0.005096543580293655 Validation loss 0.010439193807542324 Accuracy 0.88720703125\n",
      "Iteration 36810 Training loss 0.0031807583291083574 Validation loss 0.010508796200156212 Accuracy 0.88818359375\n",
      "Iteration 36820 Training loss 0.0036184126511216164 Validation loss 0.010533151216804981 Accuracy 0.88720703125\n",
      "Iteration 36830 Training loss 0.0034766909666359425 Validation loss 0.010723583400249481 Accuracy 0.8857421875\n",
      "Iteration 36840 Training loss 0.0035039717331528664 Validation loss 0.01037876307964325 Accuracy 0.88818359375\n",
      "Iteration 36850 Training loss 0.00441662035882473 Validation loss 0.011686046607792377 Accuracy 0.875\n",
      "Iteration 36860 Training loss 0.0033856555819511414 Validation loss 0.01110110804438591 Accuracy 0.88134765625\n",
      "Iteration 36870 Training loss 0.0046646250411868095 Validation loss 0.010948696173727512 Accuracy 0.88330078125\n",
      "Iteration 36880 Training loss 0.002405497245490551 Validation loss 0.010484604164958 Accuracy 0.88818359375\n",
      "Iteration 36890 Training loss 0.004893817938864231 Validation loss 0.01047083456069231 Accuracy 0.8876953125\n",
      "Iteration 36900 Training loss 0.0040029073134064674 Validation loss 0.010579860769212246 Accuracy 0.8857421875\n",
      "Iteration 36910 Training loss 0.00433652987703681 Validation loss 0.010711058974266052 Accuracy 0.88427734375\n",
      "Iteration 36920 Training loss 0.002956278854981065 Validation loss 0.010560316033661366 Accuracy 0.88623046875\n",
      "Iteration 36930 Training loss 0.0018117781728506088 Validation loss 0.010504266247153282 Accuracy 0.88671875\n",
      "Iteration 36940 Training loss 0.0033471330534666777 Validation loss 0.010594572871923447 Accuracy 0.88671875\n",
      "Iteration 36950 Training loss 0.003323320997878909 Validation loss 0.0105109428986907 Accuracy 0.8876953125\n",
      "Iteration 36960 Training loss 0.003281545592471957 Validation loss 0.010426892898976803 Accuracy 0.88818359375\n",
      "Iteration 36970 Training loss 0.00432689068838954 Validation loss 0.010522001422941685 Accuracy 0.88720703125\n",
      "Iteration 36980 Training loss 0.004034731071442366 Validation loss 0.010490244254469872 Accuracy 0.88818359375\n",
      "Iteration 36990 Training loss 0.003364389296621084 Validation loss 0.010779025964438915 Accuracy 0.88427734375\n",
      "Iteration 37000 Training loss 0.003961008507758379 Validation loss 0.010713068768382072 Accuracy 0.884765625\n",
      "Iteration 37010 Training loss 0.004997149109840393 Validation loss 0.010623627342283726 Accuracy 0.88525390625\n",
      "Iteration 37020 Training loss 0.003535287221893668 Validation loss 0.010609726421535015 Accuracy 0.88623046875\n",
      "Iteration 37030 Training loss 0.003262878395617008 Validation loss 0.010532796382904053 Accuracy 0.8876953125\n",
      "Iteration 37040 Training loss 0.002706727012991905 Validation loss 0.010819421149790287 Accuracy 0.88330078125\n",
      "Iteration 37050 Training loss 0.00424817344173789 Validation loss 0.010573591105639935 Accuracy 0.8876953125\n",
      "Iteration 37060 Training loss 0.003662639996036887 Validation loss 0.010624023154377937 Accuracy 0.88525390625\n",
      "Iteration 37070 Training loss 0.0024082388263195753 Validation loss 0.010544675402343273 Accuracy 0.88720703125\n",
      "Iteration 37080 Training loss 0.0046987091191112995 Validation loss 0.010640563443303108 Accuracy 0.88525390625\n",
      "Iteration 37090 Training loss 0.003791200928390026 Validation loss 0.01094920001924038 Accuracy 0.88232421875\n",
      "Iteration 37100 Training loss 0.003659999230876565 Validation loss 0.01080382987856865 Accuracy 0.88427734375\n",
      "Iteration 37110 Training loss 0.0039057445246726274 Validation loss 0.010600383393466473 Accuracy 0.88525390625\n",
      "Iteration 37120 Training loss 0.0031272820197045803 Validation loss 0.01057686097919941 Accuracy 0.8857421875\n",
      "Iteration 37130 Training loss 0.0036498294211924076 Validation loss 0.010810297913849354 Accuracy 0.88427734375\n",
      "Iteration 37140 Training loss 0.004003135487437248 Validation loss 0.01063950452953577 Accuracy 0.884765625\n",
      "Iteration 37150 Training loss 0.004610702395439148 Validation loss 0.010564287193119526 Accuracy 0.88623046875\n",
      "Iteration 37160 Training loss 0.004232617095112801 Validation loss 0.010647531598806381 Accuracy 0.8857421875\n",
      "Iteration 37170 Training loss 0.004002900328487158 Validation loss 0.010456358082592487 Accuracy 0.88818359375\n",
      "Iteration 37180 Training loss 0.003917937632650137 Validation loss 0.010878002271056175 Accuracy 0.88330078125\n",
      "Iteration 37190 Training loss 0.0051751951687037945 Validation loss 0.010550129227340221 Accuracy 0.88720703125\n",
      "Iteration 37200 Training loss 0.0026402289513498545 Validation loss 0.010836860164999962 Accuracy 0.884765625\n",
      "Iteration 37210 Training loss 0.003246481530368328 Validation loss 0.010548161342740059 Accuracy 0.888671875\n",
      "Iteration 37220 Training loss 0.0032244049943983555 Validation loss 0.010507483035326004 Accuracy 0.88818359375\n",
      "Iteration 37230 Training loss 0.004416974261403084 Validation loss 0.010845121927559376 Accuracy 0.8828125\n",
      "Iteration 37240 Training loss 0.003535239491611719 Validation loss 0.010452455841004848 Accuracy 0.88671875\n",
      "Iteration 37250 Training loss 0.0031541213393211365 Validation loss 0.010538851842284203 Accuracy 0.88720703125\n",
      "Iteration 37260 Training loss 0.003965701442211866 Validation loss 0.010694440454244614 Accuracy 0.884765625\n",
      "Iteration 37270 Training loss 0.004006276372820139 Validation loss 0.010652021504938602 Accuracy 0.88525390625\n",
      "Iteration 37280 Training loss 0.004383698105812073 Validation loss 0.011011259630322456 Accuracy 0.88232421875\n",
      "Iteration 37290 Training loss 0.0043901847675442696 Validation loss 0.010709855705499649 Accuracy 0.8857421875\n",
      "Iteration 37300 Training loss 0.0039505017921328545 Validation loss 0.010536826215684414 Accuracy 0.88671875\n",
      "Iteration 37310 Training loss 0.004203529097139835 Validation loss 0.010738427750766277 Accuracy 0.88525390625\n",
      "Iteration 37320 Training loss 0.0041161589324474335 Validation loss 0.010642286390066147 Accuracy 0.8857421875\n",
      "Iteration 37330 Training loss 0.0037394026294350624 Validation loss 0.010708007961511612 Accuracy 0.88623046875\n",
      "Iteration 37340 Training loss 0.0032404428347945213 Validation loss 0.010768307372927666 Accuracy 0.8837890625\n",
      "Iteration 37350 Training loss 0.0036064572632312775 Validation loss 0.010630921460688114 Accuracy 0.88671875\n",
      "Iteration 37360 Training loss 0.003570991102606058 Validation loss 0.010744232684373856 Accuracy 0.884765625\n",
      "Iteration 37370 Training loss 0.0038987458683550358 Validation loss 0.011299557983875275 Accuracy 0.87890625\n",
      "Iteration 37380 Training loss 0.004890033509582281 Validation loss 0.010729709640145302 Accuracy 0.88427734375\n",
      "Iteration 37390 Training loss 0.0032577801030129194 Validation loss 0.010982939042150974 Accuracy 0.88330078125\n",
      "Iteration 37400 Training loss 0.0037068582605570555 Validation loss 0.010750843212008476 Accuracy 0.884765625\n",
      "Iteration 37410 Training loss 0.0036586569622159004 Validation loss 0.010470363311469555 Accuracy 0.88720703125\n",
      "Iteration 37420 Training loss 0.003995047416538 Validation loss 0.010513920336961746 Accuracy 0.88818359375\n",
      "Iteration 37430 Training loss 0.004433594178408384 Validation loss 0.01068514958024025 Accuracy 0.884765625\n",
      "Iteration 37440 Training loss 0.003727738745510578 Validation loss 0.010722717270255089 Accuracy 0.88525390625\n",
      "Iteration 37450 Training loss 0.0034856058191508055 Validation loss 0.01058153249323368 Accuracy 0.88671875\n",
      "Iteration 37460 Training loss 0.003018500516191125 Validation loss 0.010678612627089024 Accuracy 0.88525390625\n",
      "Iteration 37470 Training loss 0.004217133857309818 Validation loss 0.010907838121056557 Accuracy 0.88232421875\n",
      "Iteration 37480 Training loss 0.0035242519807070494 Validation loss 0.010516567155718803 Accuracy 0.88720703125\n",
      "Iteration 37490 Training loss 0.0029933967161923647 Validation loss 0.010775861330330372 Accuracy 0.8837890625\n",
      "Iteration 37500 Training loss 0.0037544993683695793 Validation loss 0.010646282695233822 Accuracy 0.88671875\n",
      "Iteration 37510 Training loss 0.003993472550064325 Validation loss 0.010481511242687702 Accuracy 0.888671875\n",
      "Iteration 37520 Training loss 0.00447850814089179 Validation loss 0.010606725700199604 Accuracy 0.88671875\n",
      "Iteration 37530 Training loss 0.0056058610789477825 Validation loss 0.010582651011645794 Accuracy 0.8857421875\n",
      "Iteration 37540 Training loss 0.002821657108142972 Validation loss 0.01064027938991785 Accuracy 0.8857421875\n",
      "Iteration 37550 Training loss 0.004122632090002298 Validation loss 0.010801437310874462 Accuracy 0.88330078125\n",
      "Iteration 37560 Training loss 0.00398651510477066 Validation loss 0.010767362080514431 Accuracy 0.88427734375\n",
      "Iteration 37570 Training loss 0.003146166680380702 Validation loss 0.010675412602722645 Accuracy 0.88525390625\n",
      "Iteration 37580 Training loss 0.0038809317629784346 Validation loss 0.010751105844974518 Accuracy 0.88427734375\n",
      "Iteration 37590 Training loss 0.002578369341790676 Validation loss 0.010658244602382183 Accuracy 0.88720703125\n",
      "Iteration 37600 Training loss 0.0032942993566393852 Validation loss 0.010631526820361614 Accuracy 0.8857421875\n",
      "Iteration 37610 Training loss 0.004140867386013269 Validation loss 0.010680011473596096 Accuracy 0.884765625\n",
      "Iteration 37620 Training loss 0.0036600609309971333 Validation loss 0.01060886774212122 Accuracy 0.8857421875\n",
      "Iteration 37630 Training loss 0.003338885260745883 Validation loss 0.010487094521522522 Accuracy 0.88720703125\n",
      "Iteration 37640 Training loss 0.0036303100641816854 Validation loss 0.010422454215586185 Accuracy 0.88720703125\n",
      "Iteration 37650 Training loss 0.004365050233900547 Validation loss 0.010446845553815365 Accuracy 0.88720703125\n",
      "Iteration 37660 Training loss 0.004356499295681715 Validation loss 0.01054504606872797 Accuracy 0.8876953125\n",
      "Iteration 37670 Training loss 0.004602814093232155 Validation loss 0.010723900981247425 Accuracy 0.884765625\n",
      "Iteration 37680 Training loss 0.002125704428181052 Validation loss 0.010815300047397614 Accuracy 0.88427734375\n",
      "Iteration 37690 Training loss 0.0035123019479215145 Validation loss 0.010619091801345348 Accuracy 0.88623046875\n",
      "Iteration 37700 Training loss 0.004286566283553839 Validation loss 0.010620457120239735 Accuracy 0.8857421875\n",
      "Iteration 37710 Training loss 0.0034852097742259502 Validation loss 0.010747582651674747 Accuracy 0.8857421875\n",
      "Iteration 37720 Training loss 0.0038563834968954325 Validation loss 0.010925800539553165 Accuracy 0.88232421875\n",
      "Iteration 37730 Training loss 0.0051120249554514885 Validation loss 0.01070313062518835 Accuracy 0.88623046875\n",
      "Iteration 37740 Training loss 0.0036035366356372833 Validation loss 0.010756725445389748 Accuracy 0.88427734375\n",
      "Iteration 37750 Training loss 0.003705583745613694 Validation loss 0.010596963576972485 Accuracy 0.88623046875\n",
      "Iteration 37760 Training loss 0.003117319429293275 Validation loss 0.01092598121613264 Accuracy 0.88330078125\n",
      "Iteration 37770 Training loss 0.003037021029740572 Validation loss 0.010520190000534058 Accuracy 0.88720703125\n",
      "Iteration 37780 Training loss 0.004702832084149122 Validation loss 0.010448740795254707 Accuracy 0.8876953125\n",
      "Iteration 37790 Training loss 0.0042391205206513405 Validation loss 0.010627382434904575 Accuracy 0.8857421875\n",
      "Iteration 37800 Training loss 0.003088713390752673 Validation loss 0.010625869035720825 Accuracy 0.88720703125\n",
      "Iteration 37810 Training loss 0.0031027754303067923 Validation loss 0.010796216316521168 Accuracy 0.88525390625\n",
      "Iteration 37820 Training loss 0.0031421356834471226 Validation loss 0.01059165969491005 Accuracy 0.88671875\n",
      "Iteration 37830 Training loss 0.004808933939784765 Validation loss 0.010522907599806786 Accuracy 0.888671875\n",
      "Iteration 37840 Training loss 0.0036423015408217907 Validation loss 0.010661057196557522 Accuracy 0.88623046875\n",
      "Iteration 37850 Training loss 0.0045562186278402805 Validation loss 0.011422641575336456 Accuracy 0.8779296875\n",
      "Iteration 37860 Training loss 0.004710226319730282 Validation loss 0.010683572851121426 Accuracy 0.884765625\n",
      "Iteration 37870 Training loss 0.003971883561462164 Validation loss 0.011019467376172543 Accuracy 0.88330078125\n",
      "Iteration 37880 Training loss 0.002300113206729293 Validation loss 0.010607216507196426 Accuracy 0.88671875\n",
      "Iteration 37890 Training loss 0.0032912814058363438 Validation loss 0.010562614537775517 Accuracy 0.8857421875\n",
      "Iteration 37900 Training loss 0.0041782367043197155 Validation loss 0.010669996030628681 Accuracy 0.8857421875\n",
      "Iteration 37910 Training loss 0.004652644041925669 Validation loss 0.011046161875128746 Accuracy 0.88232421875\n",
      "Iteration 37920 Training loss 0.004467313177883625 Validation loss 0.01059491466730833 Accuracy 0.88671875\n",
      "Iteration 37930 Training loss 0.0020586741156876087 Validation loss 0.010609319433569908 Accuracy 0.88671875\n",
      "Iteration 37940 Training loss 0.0020685617346316576 Validation loss 0.010604549199342728 Accuracy 0.88623046875\n",
      "Iteration 37950 Training loss 0.004252976272255182 Validation loss 0.010703889653086662 Accuracy 0.8857421875\n",
      "Iteration 37960 Training loss 0.0030208893585950136 Validation loss 0.010787751525640488 Accuracy 0.88427734375\n",
      "Iteration 37970 Training loss 0.003691550809890032 Validation loss 0.010842448100447655 Accuracy 0.8828125\n",
      "Iteration 37980 Training loss 0.0033977653365582228 Validation loss 0.010632358491420746 Accuracy 0.88671875\n",
      "Iteration 37990 Training loss 0.0033776273485273123 Validation loss 0.010558822192251682 Accuracy 0.88720703125\n",
      "Iteration 38000 Training loss 0.003283450612798333 Validation loss 0.010550686158239841 Accuracy 0.88525390625\n",
      "Iteration 38010 Training loss 0.003166444832459092 Validation loss 0.01064386684447527 Accuracy 0.88525390625\n",
      "Iteration 38020 Training loss 0.0030150546226650476 Validation loss 0.010587207041680813 Accuracy 0.88671875\n",
      "Iteration 38030 Training loss 0.003317242953926325 Validation loss 0.010710072703659534 Accuracy 0.88427734375\n",
      "Iteration 38040 Training loss 0.0025404023472219706 Validation loss 0.010833099484443665 Accuracy 0.884765625\n",
      "Iteration 38050 Training loss 0.0038750043604522943 Validation loss 0.01073953602463007 Accuracy 0.88525390625\n",
      "Iteration 38060 Training loss 0.004087282344698906 Validation loss 0.010910236276686192 Accuracy 0.88330078125\n",
      "Iteration 38070 Training loss 0.002324976958334446 Validation loss 0.010686825960874557 Accuracy 0.8857421875\n",
      "Iteration 38080 Training loss 0.003634524531662464 Validation loss 0.010820417664945126 Accuracy 0.88427734375\n",
      "Iteration 38090 Training loss 0.0035761012695729733 Validation loss 0.011039015837013721 Accuracy 0.88232421875\n",
      "Iteration 38100 Training loss 0.003947321325540543 Validation loss 0.010912487283349037 Accuracy 0.8828125\n",
      "Iteration 38110 Training loss 0.0038529480807483196 Validation loss 0.01060129888355732 Accuracy 0.88671875\n",
      "Iteration 38120 Training loss 0.0036317382473498583 Validation loss 0.010663461871445179 Accuracy 0.8857421875\n",
      "Iteration 38130 Training loss 0.0036366386339068413 Validation loss 0.011160634458065033 Accuracy 0.8798828125\n",
      "Iteration 38140 Training loss 0.002436799695715308 Validation loss 0.010523030534386635 Accuracy 0.88623046875\n",
      "Iteration 38150 Training loss 0.0027501494623720646 Validation loss 0.010603019967675209 Accuracy 0.88671875\n",
      "Iteration 38160 Training loss 0.0026678680442273617 Validation loss 0.010580339469015598 Accuracy 0.88720703125\n",
      "Iteration 38170 Training loss 0.004192854277789593 Validation loss 0.01053448673337698 Accuracy 0.8876953125\n",
      "Iteration 38180 Training loss 0.002716552698984742 Validation loss 0.01053442619740963 Accuracy 0.88720703125\n",
      "Iteration 38190 Training loss 0.003653552383184433 Validation loss 0.01074920129030943 Accuracy 0.88525390625\n",
      "Iteration 38200 Training loss 0.0038314450066536665 Validation loss 0.010472873225808144 Accuracy 0.88818359375\n",
      "Iteration 38210 Training loss 0.003355196909978986 Validation loss 0.010836632922291756 Accuracy 0.88330078125\n",
      "Iteration 38220 Training loss 0.005099277477711439 Validation loss 0.011005991138517857 Accuracy 0.8818359375\n",
      "Iteration 38230 Training loss 0.0018167493399232626 Validation loss 0.010674321092665195 Accuracy 0.88525390625\n",
      "Iteration 38240 Training loss 0.004565126728266478 Validation loss 0.010943290777504444 Accuracy 0.88232421875\n",
      "Iteration 38250 Training loss 0.003393748542293906 Validation loss 0.01074210088700056 Accuracy 0.8837890625\n",
      "Iteration 38260 Training loss 0.0034681595861911774 Validation loss 0.010858545079827309 Accuracy 0.8828125\n",
      "Iteration 38270 Training loss 0.003275701543316245 Validation loss 0.010813955217599869 Accuracy 0.8837890625\n",
      "Iteration 38280 Training loss 0.0030336726922541857 Validation loss 0.01096712239086628 Accuracy 0.88232421875\n",
      "Iteration 38290 Training loss 0.00290341442450881 Validation loss 0.010724847204983234 Accuracy 0.8857421875\n",
      "Iteration 38300 Training loss 0.0040395124815404415 Validation loss 0.010860797949135303 Accuracy 0.8837890625\n",
      "Iteration 38310 Training loss 0.0031119021587073803 Validation loss 0.010670582763850689 Accuracy 0.88525390625\n",
      "Iteration 38320 Training loss 0.004061958286911249 Validation loss 0.010509251616895199 Accuracy 0.88720703125\n",
      "Iteration 38330 Training loss 0.0039876773953437805 Validation loss 0.010517570190131664 Accuracy 0.88818359375\n",
      "Iteration 38340 Training loss 0.003558716969564557 Validation loss 0.01064886711537838 Accuracy 0.88525390625\n",
      "Iteration 38350 Training loss 0.002662437269464135 Validation loss 0.0106767937541008 Accuracy 0.8857421875\n",
      "Iteration 38360 Training loss 0.004326925612986088 Validation loss 0.010579402558505535 Accuracy 0.88671875\n",
      "Iteration 38370 Training loss 0.003725383197888732 Validation loss 0.010482389479875565 Accuracy 0.888671875\n",
      "Iteration 38380 Training loss 0.003114475170150399 Validation loss 0.010791247710585594 Accuracy 0.8857421875\n",
      "Iteration 38390 Training loss 0.003494354197755456 Validation loss 0.010810251347720623 Accuracy 0.88427734375\n",
      "Iteration 38400 Training loss 0.003845602972432971 Validation loss 0.01064449641853571 Accuracy 0.8857421875\n",
      "Iteration 38410 Training loss 0.004366748034954071 Validation loss 0.010467593558132648 Accuracy 0.88720703125\n",
      "Iteration 38420 Training loss 0.0029694270342588425 Validation loss 0.01035957783460617 Accuracy 0.888671875\n",
      "Iteration 38430 Training loss 0.002380231162533164 Validation loss 0.010467697866261005 Accuracy 0.8876953125\n",
      "Iteration 38440 Training loss 0.003615049412474036 Validation loss 0.010628278367221355 Accuracy 0.88623046875\n",
      "Iteration 38450 Training loss 0.0020861581433564425 Validation loss 0.010760807432234287 Accuracy 0.8837890625\n",
      "Iteration 38460 Training loss 0.0046247257851064205 Validation loss 0.01082330197095871 Accuracy 0.8837890625\n",
      "Iteration 38470 Training loss 0.0028113338630646467 Validation loss 0.010721527971327305 Accuracy 0.88427734375\n",
      "Iteration 38480 Training loss 0.005195183213800192 Validation loss 0.01072840578854084 Accuracy 0.884765625\n",
      "Iteration 38490 Training loss 0.0038014843594282866 Validation loss 0.010922367684543133 Accuracy 0.88330078125\n",
      "Iteration 38500 Training loss 0.0027657002210617065 Validation loss 0.010704504325985909 Accuracy 0.88525390625\n",
      "Iteration 38510 Training loss 0.004537297412753105 Validation loss 0.010768935084342957 Accuracy 0.884765625\n",
      "Iteration 38520 Training loss 0.0036546699702739716 Validation loss 0.010759929195046425 Accuracy 0.88427734375\n",
      "Iteration 38530 Training loss 0.004576004110276699 Validation loss 0.010577838867902756 Accuracy 0.88671875\n",
      "Iteration 38540 Training loss 0.003869539825245738 Validation loss 0.010756686329841614 Accuracy 0.884765625\n",
      "Iteration 38550 Training loss 0.00416981428861618 Validation loss 0.010797380469739437 Accuracy 0.8857421875\n",
      "Iteration 38560 Training loss 0.004815931431949139 Validation loss 0.010628764517605305 Accuracy 0.88623046875\n",
      "Iteration 38570 Training loss 0.003984087612479925 Validation loss 0.010605412535369396 Accuracy 0.88671875\n",
      "Iteration 38580 Training loss 0.004251193720847368 Validation loss 0.0105888731777668 Accuracy 0.88720703125\n",
      "Iteration 38590 Training loss 0.003795205382630229 Validation loss 0.010788927786052227 Accuracy 0.8837890625\n",
      "Iteration 38600 Training loss 0.0028551616705954075 Validation loss 0.010858459398150444 Accuracy 0.8837890625\n",
      "Iteration 38610 Training loss 0.002626527799293399 Validation loss 0.010575901716947556 Accuracy 0.88623046875\n",
      "Iteration 38620 Training loss 0.0036638646852225065 Validation loss 0.01075099129229784 Accuracy 0.884765625\n",
      "Iteration 38630 Training loss 0.003492397954687476 Validation loss 0.010620256885886192 Accuracy 0.88671875\n",
      "Iteration 38640 Training loss 0.0030465442687273026 Validation loss 0.010714645497500896 Accuracy 0.884765625\n",
      "Iteration 38650 Training loss 0.0031649747397750616 Validation loss 0.010569647885859013 Accuracy 0.88671875\n",
      "Iteration 38660 Training loss 0.003720587119460106 Validation loss 0.010593099519610405 Accuracy 0.88623046875\n",
      "Iteration 38670 Training loss 0.0037909781094640493 Validation loss 0.011515643447637558 Accuracy 0.876953125\n",
      "Iteration 38680 Training loss 0.0041130841709673405 Validation loss 0.010858435183763504 Accuracy 0.884765625\n",
      "Iteration 38690 Training loss 0.003254098119214177 Validation loss 0.010687775909900665 Accuracy 0.88525390625\n",
      "Iteration 38700 Training loss 0.004475393332540989 Validation loss 0.010535904206335545 Accuracy 0.88720703125\n",
      "Iteration 38710 Training loss 0.0033167852088809013 Validation loss 0.010749168694019318 Accuracy 0.88427734375\n",
      "Iteration 38720 Training loss 0.003888703417032957 Validation loss 0.010743539780378342 Accuracy 0.8837890625\n",
      "Iteration 38730 Training loss 0.004457918927073479 Validation loss 0.010559199377894402 Accuracy 0.8857421875\n",
      "Iteration 38740 Training loss 0.0035443201195448637 Validation loss 0.010799044743180275 Accuracy 0.88330078125\n",
      "Iteration 38750 Training loss 0.0028360493015497923 Validation loss 0.010738986544311047 Accuracy 0.884765625\n",
      "Iteration 38760 Training loss 0.004884601105004549 Validation loss 0.011001838371157646 Accuracy 0.88232421875\n",
      "Iteration 38770 Training loss 0.003336676862090826 Validation loss 0.010644695721566677 Accuracy 0.88525390625\n",
      "Iteration 38780 Training loss 0.002308742143213749 Validation loss 0.010633120313286781 Accuracy 0.8857421875\n",
      "Iteration 38790 Training loss 0.0025168394204229116 Validation loss 0.010489342734217644 Accuracy 0.88818359375\n",
      "Iteration 38800 Training loss 0.0030461056157946587 Validation loss 0.010689465329051018 Accuracy 0.884765625\n",
      "Iteration 38810 Training loss 0.00353904883377254 Validation loss 0.010574016720056534 Accuracy 0.8857421875\n",
      "Iteration 38820 Training loss 0.004069805610924959 Validation loss 0.010915185324847698 Accuracy 0.88232421875\n",
      "Iteration 38830 Training loss 0.0034076543524861336 Validation loss 0.010772996582090855 Accuracy 0.884765625\n",
      "Iteration 38840 Training loss 0.003599137533456087 Validation loss 0.010883285664021969 Accuracy 0.88427734375\n",
      "Iteration 38850 Training loss 0.0033268723636865616 Validation loss 0.010726659558713436 Accuracy 0.884765625\n",
      "Iteration 38860 Training loss 0.0040077636949718 Validation loss 0.01077463012188673 Accuracy 0.8837890625\n",
      "Iteration 38870 Training loss 0.00408592913299799 Validation loss 0.010699206031858921 Accuracy 0.88525390625\n",
      "Iteration 38880 Training loss 0.005125327035784721 Validation loss 0.010648421011865139 Accuracy 0.8857421875\n",
      "Iteration 38890 Training loss 0.0027555800043046474 Validation loss 0.010689843446016312 Accuracy 0.884765625\n",
      "Iteration 38900 Training loss 0.0043998840264976025 Validation loss 0.010327682830393314 Accuracy 0.88916015625\n",
      "Iteration 38910 Training loss 0.0033967820927500725 Validation loss 0.010844760574400425 Accuracy 0.88427734375\n",
      "Iteration 38920 Training loss 0.003041981952264905 Validation loss 0.01042991690337658 Accuracy 0.888671875\n",
      "Iteration 38930 Training loss 0.0031875520944595337 Validation loss 0.010720184072852135 Accuracy 0.88427734375\n",
      "Iteration 38940 Training loss 0.0037135162856429815 Validation loss 0.010945209302008152 Accuracy 0.8818359375\n",
      "Iteration 38950 Training loss 0.003473786637187004 Validation loss 0.01051116082817316 Accuracy 0.88671875\n",
      "Iteration 38960 Training loss 0.00553105166181922 Validation loss 0.010580983944237232 Accuracy 0.8857421875\n",
      "Iteration 38970 Training loss 0.002816767431795597 Validation loss 0.01093541830778122 Accuracy 0.88232421875\n",
      "Iteration 38980 Training loss 0.00343882292509079 Validation loss 0.010486718267202377 Accuracy 0.88720703125\n",
      "Iteration 38990 Training loss 0.003202166873961687 Validation loss 0.010573266074061394 Accuracy 0.8876953125\n",
      "Iteration 39000 Training loss 0.0034555974416434765 Validation loss 0.01072537899017334 Accuracy 0.884765625\n",
      "Iteration 39010 Training loss 0.0027226749807596207 Validation loss 0.010747743770480156 Accuracy 0.88427734375\n",
      "Iteration 39020 Training loss 0.004624071065336466 Validation loss 0.010654793120920658 Accuracy 0.8857421875\n",
      "Iteration 39030 Training loss 0.0036578059662133455 Validation loss 0.010789123363792896 Accuracy 0.88330078125\n",
      "Iteration 39040 Training loss 0.002936403267085552 Validation loss 0.010695637203752995 Accuracy 0.88525390625\n",
      "Iteration 39050 Training loss 0.0029767360538244247 Validation loss 0.010833905078470707 Accuracy 0.8837890625\n",
      "Iteration 39060 Training loss 0.003586571430787444 Validation loss 0.010648217052221298 Accuracy 0.8857421875\n",
      "Iteration 39070 Training loss 0.004543783143162727 Validation loss 0.01056376937776804 Accuracy 0.88623046875\n",
      "Iteration 39080 Training loss 0.004060425329953432 Validation loss 0.01100127398967743 Accuracy 0.8828125\n",
      "Iteration 39090 Training loss 0.0038152243942022324 Validation loss 0.01053739245980978 Accuracy 0.88671875\n",
      "Iteration 39100 Training loss 0.0024856191594153643 Validation loss 0.010464340448379517 Accuracy 0.89013671875\n",
      "Iteration 39110 Training loss 0.0036447092425078154 Validation loss 0.010573217645287514 Accuracy 0.88623046875\n",
      "Iteration 39120 Training loss 0.003793301759287715 Validation loss 0.010586293414235115 Accuracy 0.88720703125\n",
      "Iteration 39130 Training loss 0.0036854168865829706 Validation loss 0.010600639507174492 Accuracy 0.88720703125\n",
      "Iteration 39140 Training loss 0.002745263045653701 Validation loss 0.010778809897601604 Accuracy 0.88427734375\n",
      "Iteration 39150 Training loss 0.004008294548839331 Validation loss 0.010910493321716785 Accuracy 0.8828125\n",
      "Iteration 39160 Training loss 0.003788375062867999 Validation loss 0.010743621736764908 Accuracy 0.884765625\n",
      "Iteration 39170 Training loss 0.00333011825568974 Validation loss 0.01057229470461607 Accuracy 0.88671875\n",
      "Iteration 39180 Training loss 0.0043601891957223415 Validation loss 0.01065811701118946 Accuracy 0.88525390625\n",
      "Iteration 39190 Training loss 0.004928035195916891 Validation loss 0.010635239072144032 Accuracy 0.88623046875\n",
      "Iteration 39200 Training loss 0.003250826383009553 Validation loss 0.010430478490889072 Accuracy 0.8876953125\n",
      "Iteration 39210 Training loss 0.00424566213041544 Validation loss 0.010688056237995625 Accuracy 0.88427734375\n",
      "Iteration 39220 Training loss 0.0038330911193042994 Validation loss 0.010556302033364773 Accuracy 0.88525390625\n",
      "Iteration 39230 Training loss 0.004229987971484661 Validation loss 0.010592155158519745 Accuracy 0.88671875\n",
      "Iteration 39240 Training loss 0.0038112178444862366 Validation loss 0.010655059479176998 Accuracy 0.88623046875\n",
      "Iteration 39250 Training loss 0.0028468307573348284 Validation loss 0.010585475713014603 Accuracy 0.88671875\n",
      "Iteration 39260 Training loss 0.005318204406648874 Validation loss 0.010628288611769676 Accuracy 0.88671875\n",
      "Iteration 39270 Training loss 0.0051565999165177345 Validation loss 0.010605390183627605 Accuracy 0.88720703125\n",
      "Iteration 39280 Training loss 0.003156309947371483 Validation loss 0.010645515285432339 Accuracy 0.88623046875\n",
      "Iteration 39290 Training loss 0.0032648316118866205 Validation loss 0.010638884268701077 Accuracy 0.8857421875\n",
      "Iteration 39300 Training loss 0.0030575015116482973 Validation loss 0.010617957450449467 Accuracy 0.8857421875\n",
      "Iteration 39310 Training loss 0.004032548051327467 Validation loss 0.010721420869231224 Accuracy 0.88427734375\n",
      "Iteration 39320 Training loss 0.002322872867807746 Validation loss 0.010511535219848156 Accuracy 0.88720703125\n",
      "Iteration 39330 Training loss 0.0045610531233251095 Validation loss 0.010931923985481262 Accuracy 0.88330078125\n",
      "Iteration 39340 Training loss 0.0021571884863078594 Validation loss 0.010676494799554348 Accuracy 0.8857421875\n",
      "Iteration 39350 Training loss 0.0030035963281989098 Validation loss 0.010579206049442291 Accuracy 0.88671875\n",
      "Iteration 39360 Training loss 0.0042579518631100655 Validation loss 0.010593881830573082 Accuracy 0.88623046875\n",
      "Iteration 39370 Training loss 0.0028196393977850676 Validation loss 0.0109392199665308 Accuracy 0.8828125\n",
      "Iteration 39380 Training loss 0.0031021670438349247 Validation loss 0.010768632404506207 Accuracy 0.884765625\n",
      "Iteration 39390 Training loss 0.002761944429948926 Validation loss 0.010646602138876915 Accuracy 0.8857421875\n",
      "Iteration 39400 Training loss 0.0038145091384649277 Validation loss 0.010596498847007751 Accuracy 0.88720703125\n",
      "Iteration 39410 Training loss 0.004161476157605648 Validation loss 0.010454026982188225 Accuracy 0.88818359375\n",
      "Iteration 39420 Training loss 0.003258609212934971 Validation loss 0.01051806379109621 Accuracy 0.88671875\n",
      "Iteration 39430 Training loss 0.003719690954312682 Validation loss 0.010469655506312847 Accuracy 0.88720703125\n",
      "Iteration 39440 Training loss 0.0031267786398530006 Validation loss 0.010482761077582836 Accuracy 0.88818359375\n",
      "Iteration 39450 Training loss 0.0036000232212245464 Validation loss 0.010636693798005581 Accuracy 0.88623046875\n",
      "Iteration 39460 Training loss 0.002500948030501604 Validation loss 0.010538190603256226 Accuracy 0.88623046875\n",
      "Iteration 39470 Training loss 0.004010354168713093 Validation loss 0.010540316812694073 Accuracy 0.8857421875\n",
      "Iteration 39480 Training loss 0.0056940470822155476 Validation loss 0.010693151503801346 Accuracy 0.88623046875\n",
      "Iteration 39490 Training loss 0.004072819370776415 Validation loss 0.010725453495979309 Accuracy 0.8837890625\n",
      "Iteration 39500 Training loss 0.003826087573543191 Validation loss 0.01074831560254097 Accuracy 0.884765625\n",
      "Iteration 39510 Training loss 0.0038278503343462944 Validation loss 0.010663913562893867 Accuracy 0.8857421875\n",
      "Iteration 39520 Training loss 0.0033505649771541357 Validation loss 0.010868347249925137 Accuracy 0.88330078125\n",
      "Iteration 39530 Training loss 0.004203429911285639 Validation loss 0.010407939553260803 Accuracy 0.88720703125\n",
      "Iteration 39540 Training loss 0.0013243848225101829 Validation loss 0.0105648348107934 Accuracy 0.88623046875\n",
      "Iteration 39550 Training loss 0.004151068162173033 Validation loss 0.010887271724641323 Accuracy 0.8828125\n",
      "Iteration 39560 Training loss 0.0037252248730510473 Validation loss 0.010571635328233242 Accuracy 0.88720703125\n",
      "Iteration 39570 Training loss 0.0037044486962258816 Validation loss 0.010334601625800133 Accuracy 0.8896484375\n",
      "Iteration 39580 Training loss 0.003945571836084127 Validation loss 0.010916116647422314 Accuracy 0.8828125\n",
      "Iteration 39590 Training loss 0.003862628946080804 Validation loss 0.010462557896971703 Accuracy 0.8876953125\n",
      "Iteration 39600 Training loss 0.004884028807282448 Validation loss 0.010620019398629665 Accuracy 0.88525390625\n",
      "Iteration 39610 Training loss 0.003414260921999812 Validation loss 0.010564906522631645 Accuracy 0.88623046875\n",
      "Iteration 39620 Training loss 0.00335154146887362 Validation loss 0.010655570775270462 Accuracy 0.88525390625\n",
      "Iteration 39630 Training loss 0.0026743023190647364 Validation loss 0.010726832784712315 Accuracy 0.884765625\n",
      "Iteration 39640 Training loss 0.0032402172219008207 Validation loss 0.010504481382668018 Accuracy 0.8876953125\n",
      "Iteration 39650 Training loss 0.003446707036346197 Validation loss 0.010590878315269947 Accuracy 0.88671875\n",
      "Iteration 39660 Training loss 0.0024602890480309725 Validation loss 0.01048998348414898 Accuracy 0.888671875\n",
      "Iteration 39670 Training loss 0.0019973537418991327 Validation loss 0.010550222359597683 Accuracy 0.8857421875\n",
      "Iteration 39680 Training loss 0.0028575139585882425 Validation loss 0.01052919402718544 Accuracy 0.88671875\n",
      "Iteration 39690 Training loss 0.0032278457656502724 Validation loss 0.010519077070057392 Accuracy 0.8876953125\n",
      "Iteration 39700 Training loss 0.004522078670561314 Validation loss 0.010529948398470879 Accuracy 0.88623046875\n",
      "Iteration 39710 Training loss 0.0021544895134866238 Validation loss 0.010518795810639858 Accuracy 0.88720703125\n",
      "Iteration 39720 Training loss 0.003415925893932581 Validation loss 0.01035167183727026 Accuracy 0.888671875\n",
      "Iteration 39730 Training loss 0.003588679013773799 Validation loss 0.010569341480731964 Accuracy 0.88671875\n",
      "Iteration 39740 Training loss 0.0023511273320764303 Validation loss 0.010707632638514042 Accuracy 0.884765625\n",
      "Iteration 39750 Training loss 0.0035697754938155413 Validation loss 0.010547717101871967 Accuracy 0.884765625\n",
      "Iteration 39760 Training loss 0.004126731306314468 Validation loss 0.010524886660277843 Accuracy 0.88623046875\n",
      "Iteration 39770 Training loss 0.003344203345477581 Validation loss 0.010578560642898083 Accuracy 0.8857421875\n",
      "Iteration 39780 Training loss 0.003594173351302743 Validation loss 0.010839146561920643 Accuracy 0.8828125\n",
      "Iteration 39790 Training loss 0.003012138418853283 Validation loss 0.010426503606140614 Accuracy 0.888671875\n",
      "Iteration 39800 Training loss 0.0030063348822295666 Validation loss 0.010553944855928421 Accuracy 0.88623046875\n",
      "Iteration 39810 Training loss 0.004232020117342472 Validation loss 0.010591225698590279 Accuracy 0.8857421875\n",
      "Iteration 39820 Training loss 0.0025132920127362013 Validation loss 0.01079030241817236 Accuracy 0.8837890625\n",
      "Iteration 39830 Training loss 0.0032627484761178493 Validation loss 0.010645166039466858 Accuracy 0.88623046875\n",
      "Iteration 39840 Training loss 0.002373314229771495 Validation loss 0.01079979445785284 Accuracy 0.88427734375\n",
      "Iteration 39850 Training loss 0.003565336111932993 Validation loss 0.0104291420429945 Accuracy 0.88916015625\n",
      "Iteration 39860 Training loss 0.00393351586535573 Validation loss 0.010583817958831787 Accuracy 0.88671875\n",
      "Iteration 39870 Training loss 0.004595127888023853 Validation loss 0.010563519783318043 Accuracy 0.8857421875\n",
      "Iteration 39880 Training loss 0.004059819038957357 Validation loss 0.010610434226691723 Accuracy 0.88623046875\n",
      "Iteration 39890 Training loss 0.0038888179697096348 Validation loss 0.010840597562491894 Accuracy 0.8837890625\n",
      "Iteration 39900 Training loss 0.003775113495066762 Validation loss 0.010638066567480564 Accuracy 0.88623046875\n",
      "Iteration 39910 Training loss 0.005087627563625574 Validation loss 0.010674345307052135 Accuracy 0.8857421875\n",
      "Iteration 39920 Training loss 0.002112498041242361 Validation loss 0.010452472604811192 Accuracy 0.88818359375\n",
      "Iteration 39930 Training loss 0.002792397513985634 Validation loss 0.010592907667160034 Accuracy 0.88623046875\n",
      "Iteration 39940 Training loss 0.002066425746306777 Validation loss 0.01062039379030466 Accuracy 0.88623046875\n",
      "Iteration 39950 Training loss 0.003885383950546384 Validation loss 0.010620763525366783 Accuracy 0.8857421875\n",
      "Iteration 39960 Training loss 0.0032437751069664955 Validation loss 0.010563774034380913 Accuracy 0.88720703125\n",
      "Iteration 39970 Training loss 0.0036304278764873743 Validation loss 0.010619589127600193 Accuracy 0.88720703125\n",
      "Iteration 39980 Training loss 0.003244666149839759 Validation loss 0.010544247925281525 Accuracy 0.88720703125\n",
      "Iteration 39990 Training loss 0.004197100177407265 Validation loss 0.010595775209367275 Accuracy 0.88623046875\n",
      "Iteration 40000 Training loss 0.002836614614352584 Validation loss 0.010639062151312828 Accuracy 0.8857421875\n",
      "Iteration 40010 Training loss 0.00427358178421855 Validation loss 0.010506569407880306 Accuracy 0.8876953125\n",
      "Iteration 40020 Training loss 0.00451639574021101 Validation loss 0.010625092312693596 Accuracy 0.88671875\n",
      "Iteration 40030 Training loss 0.004770142026245594 Validation loss 0.010964804328978062 Accuracy 0.8828125\n",
      "Iteration 40040 Training loss 0.00330475065857172 Validation loss 0.0109883863478899 Accuracy 0.8828125\n",
      "Iteration 40050 Training loss 0.0027185098733752966 Validation loss 0.01071347389370203 Accuracy 0.884765625\n",
      "Iteration 40060 Training loss 0.0045564924366772175 Validation loss 0.010518213734030724 Accuracy 0.88671875\n",
      "Iteration 40070 Training loss 0.0032919333316385746 Validation loss 0.010630810633301735 Accuracy 0.88623046875\n",
      "Iteration 40080 Training loss 0.0028452554251998663 Validation loss 0.010602802038192749 Accuracy 0.88671875\n",
      "Iteration 40090 Training loss 0.003756697988137603 Validation loss 0.010740119032561779 Accuracy 0.88525390625\n",
      "Iteration 40100 Training loss 0.003330649109557271 Validation loss 0.010701139457523823 Accuracy 0.884765625\n",
      "Iteration 40110 Training loss 0.0024682343937456608 Validation loss 0.01065431535243988 Accuracy 0.884765625\n",
      "Iteration 40120 Training loss 0.0035872445441782475 Validation loss 0.010747360996901989 Accuracy 0.88525390625\n",
      "Iteration 40130 Training loss 0.004650440067052841 Validation loss 0.010656159371137619 Accuracy 0.8857421875\n",
      "Iteration 40140 Training loss 0.0035957631189376116 Validation loss 0.01050181407481432 Accuracy 0.88720703125\n",
      "Iteration 40150 Training loss 0.0032265586778521538 Validation loss 0.010835298337042332 Accuracy 0.88427734375\n",
      "Iteration 40160 Training loss 0.002808628138154745 Validation loss 0.0106477877125144 Accuracy 0.88623046875\n",
      "Iteration 40170 Training loss 0.003752220654860139 Validation loss 0.010434753261506557 Accuracy 0.88818359375\n",
      "Iteration 40180 Training loss 0.004699429962784052 Validation loss 0.010758347809314728 Accuracy 0.88525390625\n",
      "Iteration 40190 Training loss 0.0030046964529901743 Validation loss 0.010651188902556896 Accuracy 0.8857421875\n",
      "Iteration 40200 Training loss 0.0028063070494681597 Validation loss 0.01059054397046566 Accuracy 0.88623046875\n",
      "Iteration 40210 Training loss 0.0029611866921186447 Validation loss 0.010655578225851059 Accuracy 0.88525390625\n",
      "Iteration 40220 Training loss 0.0025093546137213707 Validation loss 0.010786342434585094 Accuracy 0.88427734375\n",
      "Iteration 40230 Training loss 0.0029007066041231155 Validation loss 0.010497340932488441 Accuracy 0.88720703125\n",
      "Iteration 40240 Training loss 0.002915454562753439 Validation loss 0.010674619115889072 Accuracy 0.88525390625\n",
      "Iteration 40250 Training loss 0.0028702309355139732 Validation loss 0.010478480719029903 Accuracy 0.8876953125\n",
      "Iteration 40260 Training loss 0.003599221585318446 Validation loss 0.01053929328918457 Accuracy 0.8876953125\n",
      "Iteration 40270 Training loss 0.004830625373870134 Validation loss 0.010530691593885422 Accuracy 0.88720703125\n",
      "Iteration 40280 Training loss 0.004021535161882639 Validation loss 0.010730095207691193 Accuracy 0.8857421875\n",
      "Iteration 40290 Training loss 0.0017114918446168303 Validation loss 0.01077255792915821 Accuracy 0.88427734375\n",
      "Iteration 40300 Training loss 0.002852127654477954 Validation loss 0.010782585479319096 Accuracy 0.884765625\n",
      "Iteration 40310 Training loss 0.0022843715269118547 Validation loss 0.01080272812396288 Accuracy 0.884765625\n",
      "Iteration 40320 Training loss 0.0035613069776445627 Validation loss 0.010700619779527187 Accuracy 0.884765625\n",
      "Iteration 40330 Training loss 0.0028170673176646233 Validation loss 0.010551277548074722 Accuracy 0.88623046875\n",
      "Iteration 40340 Training loss 0.003317885100841522 Validation loss 0.010472746565937996 Accuracy 0.88818359375\n",
      "Iteration 40350 Training loss 0.0037810001522302628 Validation loss 0.010671176947653294 Accuracy 0.88525390625\n",
      "Iteration 40360 Training loss 0.0038341209292411804 Validation loss 0.010502436198294163 Accuracy 0.88720703125\n",
      "Iteration 40370 Training loss 0.0025336304679512978 Validation loss 0.010628681629896164 Accuracy 0.88623046875\n",
      "Iteration 40380 Training loss 0.0032957051880657673 Validation loss 0.01068259496241808 Accuracy 0.8857421875\n",
      "Iteration 40390 Training loss 0.002208341844379902 Validation loss 0.01046714000403881 Accuracy 0.8876953125\n",
      "Iteration 40400 Training loss 0.003410645527765155 Validation loss 0.01042325422167778 Accuracy 0.88671875\n",
      "Iteration 40410 Training loss 0.0035795026924461126 Validation loss 0.010470491833984852 Accuracy 0.8876953125\n",
      "Iteration 40420 Training loss 0.003306854283437133 Validation loss 0.010516691952943802 Accuracy 0.88720703125\n",
      "Iteration 40430 Training loss 0.003495490411296487 Validation loss 0.010598844848573208 Accuracy 0.8857421875\n",
      "Iteration 40440 Training loss 0.0038188821636140347 Validation loss 0.010490795597434044 Accuracy 0.88671875\n",
      "Iteration 40450 Training loss 0.00433260016143322 Validation loss 0.010828261263668537 Accuracy 0.88330078125\n",
      "Iteration 40460 Training loss 0.0035063812974840403 Validation loss 0.010611915960907936 Accuracy 0.88671875\n",
      "Iteration 40470 Training loss 0.0028658227529376745 Validation loss 0.010657164268195629 Accuracy 0.88623046875\n",
      "Iteration 40480 Training loss 0.003019805531948805 Validation loss 0.01059997733682394 Accuracy 0.88671875\n",
      "Iteration 40490 Training loss 0.0030364326667040586 Validation loss 0.010646759532392025 Accuracy 0.88671875\n",
      "Iteration 40500 Training loss 0.0041652568615973 Validation loss 0.010638908483088017 Accuracy 0.88525390625\n",
      "Iteration 40510 Training loss 0.0036488256882876158 Validation loss 0.010752245783805847 Accuracy 0.8837890625\n",
      "Iteration 40520 Training loss 0.00408782996237278 Validation loss 0.010624195449054241 Accuracy 0.8857421875\n",
      "Iteration 40530 Training loss 0.0031436483841389418 Validation loss 0.010763506405055523 Accuracy 0.884765625\n",
      "Iteration 40540 Training loss 0.0029381497297436 Validation loss 0.010803820565342903 Accuracy 0.88330078125\n",
      "Iteration 40550 Training loss 0.0029556022491306067 Validation loss 0.010471496731042862 Accuracy 0.88818359375\n",
      "Iteration 40560 Training loss 0.003646650817245245 Validation loss 0.010818996466696262 Accuracy 0.884765625\n",
      "Iteration 40570 Training loss 0.0034241981338709593 Validation loss 0.010528487153351307 Accuracy 0.88720703125\n",
      "Iteration 40580 Training loss 0.0052858623676002026 Validation loss 0.010693207383155823 Accuracy 0.884765625\n",
      "Iteration 40590 Training loss 0.00457375030964613 Validation loss 0.010643614456057549 Accuracy 0.88525390625\n",
      "Iteration 40600 Training loss 0.00189885008148849 Validation loss 0.010548378340899944 Accuracy 0.88623046875\n",
      "Iteration 40610 Training loss 0.002959931269288063 Validation loss 0.010514108464121819 Accuracy 0.88671875\n",
      "Iteration 40620 Training loss 0.004103674087673426 Validation loss 0.0106569929048419 Accuracy 0.88525390625\n",
      "Iteration 40630 Training loss 0.0030605217907577753 Validation loss 0.010732526890933514 Accuracy 0.88427734375\n",
      "Iteration 40640 Training loss 0.0024204482324421406 Validation loss 0.010652396827936172 Accuracy 0.8857421875\n",
      "Iteration 40650 Training loss 0.003313753055408597 Validation loss 0.010816876776516438 Accuracy 0.884765625\n",
      "Iteration 40660 Training loss 0.0033935876563191414 Validation loss 0.010613051243126392 Accuracy 0.88671875\n",
      "Iteration 40670 Training loss 0.002487583551555872 Validation loss 0.010615305975079536 Accuracy 0.88525390625\n",
      "Iteration 40680 Training loss 0.0024404097348451614 Validation loss 0.010622415691614151 Accuracy 0.88623046875\n",
      "Iteration 40690 Training loss 0.004164768382906914 Validation loss 0.010767382569611073 Accuracy 0.884765625\n",
      "Iteration 40700 Training loss 0.0034513319842517376 Validation loss 0.010797567665576935 Accuracy 0.88427734375\n",
      "Iteration 40710 Training loss 0.003384543815627694 Validation loss 0.010546527802944183 Accuracy 0.88671875\n",
      "Iteration 40720 Training loss 0.004559484776109457 Validation loss 0.010510292835533619 Accuracy 0.88671875\n",
      "Iteration 40730 Training loss 0.004016471095383167 Validation loss 0.010604765266180038 Accuracy 0.8857421875\n",
      "Iteration 40740 Training loss 0.0023098820820450783 Validation loss 0.010671067051589489 Accuracy 0.8857421875\n",
      "Iteration 40750 Training loss 0.003104679984971881 Validation loss 0.010581552051007748 Accuracy 0.88720703125\n",
      "Iteration 40760 Training loss 0.0033429490868002176 Validation loss 0.01096897479146719 Accuracy 0.8828125\n",
      "Iteration 40770 Training loss 0.003799624741077423 Validation loss 0.010695432312786579 Accuracy 0.884765625\n",
      "Iteration 40780 Training loss 0.003093745093792677 Validation loss 0.01084861345589161 Accuracy 0.884765625\n",
      "Iteration 40790 Training loss 0.0042871031910181046 Validation loss 0.01085218321532011 Accuracy 0.8837890625\n",
      "Iteration 40800 Training loss 0.0039395117200911045 Validation loss 0.010848186910152435 Accuracy 0.8837890625\n",
      "Iteration 40810 Training loss 0.0029676761478185654 Validation loss 0.01070837490260601 Accuracy 0.8857421875\n",
      "Iteration 40820 Training loss 0.003394907806068659 Validation loss 0.010784853249788284 Accuracy 0.88427734375\n",
      "Iteration 40830 Training loss 0.0037360338028520346 Validation loss 0.01069994643330574 Accuracy 0.8857421875\n",
      "Iteration 40840 Training loss 0.0021998323500156403 Validation loss 0.01058039627969265 Accuracy 0.88623046875\n",
      "Iteration 40850 Training loss 0.0032771260011941195 Validation loss 0.010606491006910801 Accuracy 0.88671875\n",
      "Iteration 40860 Training loss 0.004446040373295546 Validation loss 0.010472076013684273 Accuracy 0.8876953125\n",
      "Iteration 40870 Training loss 0.002968978602439165 Validation loss 0.01041122991591692 Accuracy 0.8876953125\n",
      "Iteration 40880 Training loss 0.0034339402336627245 Validation loss 0.010599249042570591 Accuracy 0.88671875\n",
      "Iteration 40890 Training loss 0.003742202650755644 Validation loss 0.010711646638810635 Accuracy 0.88525390625\n",
      "Iteration 40900 Training loss 0.0027755647897720337 Validation loss 0.0106497248634696 Accuracy 0.88720703125\n",
      "Iteration 40910 Training loss 0.003404265036806464 Validation loss 0.01059701107442379 Accuracy 0.88623046875\n",
      "Iteration 40920 Training loss 0.0014502271078526974 Validation loss 0.010660332627594471 Accuracy 0.884765625\n",
      "Iteration 40930 Training loss 0.0034897199366241693 Validation loss 0.0107234762981534 Accuracy 0.88525390625\n",
      "Iteration 40940 Training loss 0.0034427859354764223 Validation loss 0.010823322460055351 Accuracy 0.8837890625\n",
      "Iteration 40950 Training loss 0.0032705001067370176 Validation loss 0.010752084665000439 Accuracy 0.88525390625\n",
      "Iteration 40960 Training loss 0.002432272769510746 Validation loss 0.010663686320185661 Accuracy 0.88623046875\n",
      "Iteration 40970 Training loss 0.002293815603479743 Validation loss 0.010454664006829262 Accuracy 0.888671875\n",
      "Iteration 40980 Training loss 0.0037370878271758556 Validation loss 0.010568766854703426 Accuracy 0.88720703125\n",
      "Iteration 40990 Training loss 0.003060116432607174 Validation loss 0.01077550183981657 Accuracy 0.8837890625\n",
      "Iteration 41000 Training loss 0.0037830988876521587 Validation loss 0.010780119337141514 Accuracy 0.8837890625\n",
      "Iteration 41010 Training loss 0.0026359213516116142 Validation loss 0.01089339330792427 Accuracy 0.88330078125\n",
      "Iteration 41020 Training loss 0.0029534136410802603 Validation loss 0.010773235000669956 Accuracy 0.88427734375\n",
      "Iteration 41030 Training loss 0.004293100442737341 Validation loss 0.010685576125979424 Accuracy 0.88525390625\n",
      "Iteration 41040 Training loss 0.0045505003072321415 Validation loss 0.01068231649696827 Accuracy 0.88525390625\n",
      "Iteration 41050 Training loss 0.004169030115008354 Validation loss 0.010887746699154377 Accuracy 0.88330078125\n",
      "Iteration 41060 Training loss 0.003307654056698084 Validation loss 0.010800783522427082 Accuracy 0.884765625\n",
      "Iteration 41070 Training loss 0.0036416491493582726 Validation loss 0.010926386341452599 Accuracy 0.88232421875\n",
      "Iteration 41080 Training loss 0.002980851801112294 Validation loss 0.010659884661436081 Accuracy 0.88525390625\n",
      "Iteration 41090 Training loss 0.0030901883728802204 Validation loss 0.010712158866226673 Accuracy 0.884765625\n",
      "Iteration 41100 Training loss 0.003991662058979273 Validation loss 0.010712251998484135 Accuracy 0.88427734375\n",
      "Iteration 41110 Training loss 0.0035188954789191484 Validation loss 0.010558434762060642 Accuracy 0.88623046875\n",
      "Iteration 41120 Training loss 0.0023097421508282423 Validation loss 0.01072897668927908 Accuracy 0.8837890625\n",
      "Iteration 41130 Training loss 0.002893885364755988 Validation loss 0.010580367408692837 Accuracy 0.88623046875\n",
      "Iteration 41140 Training loss 0.0031289909966289997 Validation loss 0.010923395864665508 Accuracy 0.8837890625\n",
      "Iteration 41150 Training loss 0.002603123662993312 Validation loss 0.010592589154839516 Accuracy 0.88623046875\n",
      "Iteration 41160 Training loss 0.0028299991972744465 Validation loss 0.01059455331414938 Accuracy 0.88623046875\n",
      "Iteration 41170 Training loss 0.0031495278235524893 Validation loss 0.010694452561438084 Accuracy 0.8857421875\n",
      "Iteration 41180 Training loss 0.003720140317454934 Validation loss 0.010648438706994057 Accuracy 0.88623046875\n",
      "Iteration 41190 Training loss 0.003783336840569973 Validation loss 0.010637402534484863 Accuracy 0.88623046875\n",
      "Iteration 41200 Training loss 0.0035959421657025814 Validation loss 0.010751117952167988 Accuracy 0.88525390625\n",
      "Iteration 41210 Training loss 0.0042458935640752316 Validation loss 0.010650381445884705 Accuracy 0.8857421875\n",
      "Iteration 41220 Training loss 0.003421142464503646 Validation loss 0.010698300786316395 Accuracy 0.88525390625\n",
      "Iteration 41230 Training loss 0.0021571265533566475 Validation loss 0.010513232089579105 Accuracy 0.88671875\n",
      "Iteration 41240 Training loss 0.0022339774295687675 Validation loss 0.010744321160018444 Accuracy 0.88525390625\n",
      "Iteration 41250 Training loss 0.0021968712098896503 Validation loss 0.010637934319674969 Accuracy 0.88623046875\n",
      "Iteration 41260 Training loss 0.0031502668280154467 Validation loss 0.01075078547000885 Accuracy 0.88427734375\n",
      "Iteration 41270 Training loss 0.003044659271836281 Validation loss 0.010531813837587833 Accuracy 0.88720703125\n",
      "Iteration 41280 Training loss 0.003308027284219861 Validation loss 0.010831309482455254 Accuracy 0.8837890625\n",
      "Iteration 41290 Training loss 0.0032473027240484953 Validation loss 0.010776343755424023 Accuracy 0.8837890625\n",
      "Iteration 41300 Training loss 0.0029739532619714737 Validation loss 0.010647691786289215 Accuracy 0.88623046875\n",
      "Iteration 41310 Training loss 0.002755463821813464 Validation loss 0.010728497989475727 Accuracy 0.88427734375\n",
      "Iteration 41320 Training loss 0.003055589273571968 Validation loss 0.010764280334115028 Accuracy 0.88427734375\n",
      "Iteration 41330 Training loss 0.0035362732596695423 Validation loss 0.010864883661270142 Accuracy 0.8837890625\n",
      "Iteration 41340 Training loss 0.002687175292521715 Validation loss 0.010921475477516651 Accuracy 0.88232421875\n",
      "Iteration 41350 Training loss 0.0018192599527537823 Validation loss 0.01063476037234068 Accuracy 0.88623046875\n",
      "Iteration 41360 Training loss 0.004210844170302153 Validation loss 0.010941432788968086 Accuracy 0.88232421875\n",
      "Iteration 41370 Training loss 0.0026171961799263954 Validation loss 0.010772615671157837 Accuracy 0.884765625\n",
      "Iteration 41380 Training loss 0.003437890438362956 Validation loss 0.010796642862260342 Accuracy 0.8837890625\n",
      "Iteration 41390 Training loss 0.0031391994562000036 Validation loss 0.010748250409960747 Accuracy 0.88427734375\n",
      "Iteration 41400 Training loss 0.003350300481542945 Validation loss 0.010692162439227104 Accuracy 0.88525390625\n",
      "Iteration 41410 Training loss 0.002339232712984085 Validation loss 0.01061935257166624 Accuracy 0.884765625\n",
      "Iteration 41420 Training loss 0.0024925905745476484 Validation loss 0.010946283116936684 Accuracy 0.88232421875\n",
      "Iteration 41430 Training loss 0.0045610396191477776 Validation loss 0.010832538828253746 Accuracy 0.88427734375\n",
      "Iteration 41440 Training loss 0.0025900413747876883 Validation loss 0.010632151737809181 Accuracy 0.88525390625\n",
      "Iteration 41450 Training loss 0.0036383557599037886 Validation loss 0.010619803331792355 Accuracy 0.88720703125\n",
      "Iteration 41460 Training loss 0.0035364069044589996 Validation loss 0.010859690606594086 Accuracy 0.88427734375\n",
      "Iteration 41470 Training loss 0.003372395411133766 Validation loss 0.010672216303646564 Accuracy 0.88623046875\n",
      "Iteration 41480 Training loss 0.0022353988606482744 Validation loss 0.010494736023247242 Accuracy 0.88720703125\n",
      "Iteration 41490 Training loss 0.0023872521705925465 Validation loss 0.010720238089561462 Accuracy 0.8857421875\n",
      "Iteration 41500 Training loss 0.004415706731379032 Validation loss 0.010591485537588596 Accuracy 0.88623046875\n",
      "Iteration 41510 Training loss 0.0037292626220732927 Validation loss 0.011002550832927227 Accuracy 0.88134765625\n",
      "Iteration 41520 Training loss 0.0027780162636190653 Validation loss 0.010700507089495659 Accuracy 0.884765625\n",
      "Iteration 41530 Training loss 0.003491349518299103 Validation loss 0.010625583119690418 Accuracy 0.88525390625\n",
      "Iteration 41540 Training loss 0.0026023725513368845 Validation loss 0.010608375072479248 Accuracy 0.88671875\n",
      "Iteration 41550 Training loss 0.003959557507187128 Validation loss 0.011061913333833218 Accuracy 0.88232421875\n",
      "Iteration 41560 Training loss 0.003603869117796421 Validation loss 0.010793260298669338 Accuracy 0.8837890625\n",
      "Iteration 41570 Training loss 0.0029386768583208323 Validation loss 0.01096758060157299 Accuracy 0.8828125\n",
      "Iteration 41580 Training loss 0.00352676585316658 Validation loss 0.010614901781082153 Accuracy 0.88671875\n",
      "Iteration 41590 Training loss 0.002261608373373747 Validation loss 0.010671276599168777 Accuracy 0.88623046875\n",
      "Iteration 41600 Training loss 0.0021839295513927937 Validation loss 0.01075784396380186 Accuracy 0.8857421875\n",
      "Iteration 41610 Training loss 0.003814839059486985 Validation loss 0.010620568878948689 Accuracy 0.88671875\n",
      "Iteration 41620 Training loss 0.003440784988924861 Validation loss 0.010634960606694221 Accuracy 0.8857421875\n",
      "Iteration 41630 Training loss 0.0029181893914937973 Validation loss 0.010591474361717701 Accuracy 0.8857421875\n",
      "Iteration 41640 Training loss 0.003942484501749277 Validation loss 0.010630734264850616 Accuracy 0.8857421875\n",
      "Iteration 41650 Training loss 0.002804777817800641 Validation loss 0.010697155259549618 Accuracy 0.88427734375\n",
      "Iteration 41660 Training loss 0.0028932609129697084 Validation loss 0.010635926388204098 Accuracy 0.8857421875\n",
      "Iteration 41670 Training loss 0.002492086496204138 Validation loss 0.0106996214017272 Accuracy 0.884765625\n",
      "Iteration 41680 Training loss 0.0030134429689496756 Validation loss 0.010698098689317703 Accuracy 0.88330078125\n",
      "Iteration 41690 Training loss 0.002395401708781719 Validation loss 0.01069231890141964 Accuracy 0.884765625\n",
      "Iteration 41700 Training loss 0.002591641154140234 Validation loss 0.010476657189428806 Accuracy 0.88671875\n",
      "Iteration 41710 Training loss 0.004524221643805504 Validation loss 0.010640178807079792 Accuracy 0.88671875\n",
      "Iteration 41720 Training loss 0.002788958139717579 Validation loss 0.0106258699670434 Accuracy 0.8857421875\n",
      "Iteration 41730 Training loss 0.0035730688832700253 Validation loss 0.010821767151355743 Accuracy 0.8837890625\n",
      "Iteration 41740 Training loss 0.003704568836838007 Validation loss 0.010769772343337536 Accuracy 0.88525390625\n",
      "Iteration 41750 Training loss 0.002457505324855447 Validation loss 0.011003940366208553 Accuracy 0.8818359375\n",
      "Iteration 41760 Training loss 0.0027509257197380066 Validation loss 0.010853984393179417 Accuracy 0.88330078125\n",
      "Iteration 41770 Training loss 0.004554195795208216 Validation loss 0.011244344525039196 Accuracy 0.87939453125\n",
      "Iteration 41780 Training loss 0.004470594227313995 Validation loss 0.010769047774374485 Accuracy 0.884765625\n",
      "Iteration 41790 Training loss 0.0026572588831186295 Validation loss 0.010826320387423038 Accuracy 0.8837890625\n",
      "Iteration 41800 Training loss 0.0031262950506061316 Validation loss 0.010599174536764622 Accuracy 0.8857421875\n",
      "Iteration 41810 Training loss 0.004190958570688963 Validation loss 0.010910915210843086 Accuracy 0.88330078125\n",
      "Iteration 41820 Training loss 0.003638721536844969 Validation loss 0.01072815153747797 Accuracy 0.88525390625\n",
      "Iteration 41830 Training loss 0.00420098751783371 Validation loss 0.010802744887769222 Accuracy 0.88330078125\n",
      "Iteration 41840 Training loss 0.002288175979629159 Validation loss 0.010678809136152267 Accuracy 0.88525390625\n",
      "Iteration 41850 Training loss 0.0020791208371520042 Validation loss 0.010535912588238716 Accuracy 0.88525390625\n",
      "Iteration 41860 Training loss 0.0029612125363200903 Validation loss 0.010612688027322292 Accuracy 0.8857421875\n",
      "Iteration 41870 Training loss 0.004807450342923403 Validation loss 0.010814514011144638 Accuracy 0.8828125\n",
      "Iteration 41880 Training loss 0.003993681166321039 Validation loss 0.010639284737408161 Accuracy 0.8857421875\n",
      "Iteration 41890 Training loss 0.0032380828633904457 Validation loss 0.011091701686382294 Accuracy 0.88134765625\n",
      "Iteration 41900 Training loss 0.004058287478983402 Validation loss 0.010719245299696922 Accuracy 0.88427734375\n",
      "Iteration 41910 Training loss 0.0030420233961194754 Validation loss 0.010537548922002316 Accuracy 0.88671875\n",
      "Iteration 41920 Training loss 0.0028746582102030516 Validation loss 0.01073567382991314 Accuracy 0.8857421875\n",
      "Iteration 41930 Training loss 0.0026491691824048758 Validation loss 0.010626226663589478 Accuracy 0.8857421875\n",
      "Iteration 41940 Training loss 0.0028626041021198034 Validation loss 0.01063457503914833 Accuracy 0.88623046875\n",
      "Iteration 41950 Training loss 0.003211554139852524 Validation loss 0.01062957476824522 Accuracy 0.8857421875\n",
      "Iteration 41960 Training loss 0.002386959735304117 Validation loss 0.010677204467356205 Accuracy 0.88427734375\n",
      "Iteration 41970 Training loss 0.0036548771895468235 Validation loss 0.010738900862634182 Accuracy 0.88525390625\n",
      "Iteration 41980 Training loss 0.0028545341920107603 Validation loss 0.01066212821751833 Accuracy 0.8857421875\n",
      "Iteration 41990 Training loss 0.0035988858435302973 Validation loss 0.010422115214169025 Accuracy 0.8876953125\n",
      "Iteration 42000 Training loss 0.002936822595074773 Validation loss 0.01069143321365118 Accuracy 0.8857421875\n",
      "Iteration 42010 Training loss 0.004044622182846069 Validation loss 0.0109476363286376 Accuracy 0.8828125\n",
      "Iteration 42020 Training loss 0.00334080308675766 Validation loss 0.010759343393146992 Accuracy 0.88525390625\n",
      "Iteration 42030 Training loss 0.0033393395133316517 Validation loss 0.010883122682571411 Accuracy 0.8837890625\n",
      "Iteration 42040 Training loss 0.003142630448564887 Validation loss 0.010743080638349056 Accuracy 0.884765625\n",
      "Iteration 42050 Training loss 0.0034579080529510975 Validation loss 0.010769009590148926 Accuracy 0.8857421875\n",
      "Iteration 42060 Training loss 0.004460587166249752 Validation loss 0.010579644702374935 Accuracy 0.88623046875\n",
      "Iteration 42070 Training loss 0.004100378602743149 Validation loss 0.010672803968191147 Accuracy 0.88623046875\n",
      "Iteration 42080 Training loss 0.004099609330296516 Validation loss 0.0107363136485219 Accuracy 0.88427734375\n",
      "Iteration 42090 Training loss 0.002262985799461603 Validation loss 0.010580392554402351 Accuracy 0.88671875\n",
      "Iteration 42100 Training loss 0.003747670678421855 Validation loss 0.010652143508195877 Accuracy 0.8857421875\n",
      "Iteration 42110 Training loss 0.004047504160553217 Validation loss 0.010635686106979847 Accuracy 0.88623046875\n",
      "Iteration 42120 Training loss 0.002629151102155447 Validation loss 0.010506141930818558 Accuracy 0.8876953125\n",
      "Iteration 42130 Training loss 0.0030314859468489885 Validation loss 0.010555736720561981 Accuracy 0.88671875\n",
      "Iteration 42140 Training loss 0.0024497625418007374 Validation loss 0.010436005890369415 Accuracy 0.888671875\n",
      "Iteration 42150 Training loss 0.0025334390811622143 Validation loss 0.010473030619323254 Accuracy 0.8876953125\n",
      "Iteration 42160 Training loss 0.004409588873386383 Validation loss 0.010661615990102291 Accuracy 0.8857421875\n",
      "Iteration 42170 Training loss 0.0030782960820943117 Validation loss 0.010742985643446445 Accuracy 0.884765625\n",
      "Iteration 42180 Training loss 0.0030461647547781467 Validation loss 0.011001252569258213 Accuracy 0.880859375\n",
      "Iteration 42190 Training loss 0.0037092934362590313 Validation loss 0.010914742946624756 Accuracy 0.88330078125\n",
      "Iteration 42200 Training loss 0.0028340346179902554 Validation loss 0.010706650093197823 Accuracy 0.8837890625\n",
      "Iteration 42210 Training loss 0.003455501515418291 Validation loss 0.010740404017269611 Accuracy 0.88427734375\n",
      "Iteration 42220 Training loss 0.0025658474769443274 Validation loss 0.010552925989031792 Accuracy 0.88671875\n",
      "Iteration 42230 Training loss 0.003155280603095889 Validation loss 0.010743297636508942 Accuracy 0.88427734375\n",
      "Iteration 42240 Training loss 0.0020761892665177584 Validation loss 0.010705114342272282 Accuracy 0.88427734375\n",
      "Iteration 42250 Training loss 0.002841288223862648 Validation loss 0.010839150287210941 Accuracy 0.8837890625\n",
      "Iteration 42260 Training loss 0.0027945854235440493 Validation loss 0.010883810929954052 Accuracy 0.88330078125\n",
      "Iteration 42270 Training loss 0.0024814873468130827 Validation loss 0.010616803541779518 Accuracy 0.88623046875\n",
      "Iteration 42280 Training loss 0.0032734028063714504 Validation loss 0.010633294470608234 Accuracy 0.88525390625\n",
      "Iteration 42290 Training loss 0.002731210784986615 Validation loss 0.010697778314352036 Accuracy 0.8857421875\n",
      "Iteration 42300 Training loss 0.0035499476362019777 Validation loss 0.010647948831319809 Accuracy 0.88623046875\n",
      "Iteration 42310 Training loss 0.0030114827677607536 Validation loss 0.010818415321409702 Accuracy 0.88330078125\n",
      "Iteration 42320 Training loss 0.002440708689391613 Validation loss 0.010674175806343555 Accuracy 0.88623046875\n",
      "Iteration 42330 Training loss 0.003990080673247576 Validation loss 0.010749008506536484 Accuracy 0.884765625\n",
      "Iteration 42340 Training loss 0.0027719473000615835 Validation loss 0.010771789588034153 Accuracy 0.88427734375\n",
      "Iteration 42350 Training loss 0.0036969135981053114 Validation loss 0.010544662363827229 Accuracy 0.88671875\n",
      "Iteration 42360 Training loss 0.003427717834711075 Validation loss 0.010583911091089249 Accuracy 0.88525390625\n",
      "Iteration 42370 Training loss 0.002784150652587414 Validation loss 0.010592248290777206 Accuracy 0.88623046875\n",
      "Iteration 42380 Training loss 0.0026150369085371494 Validation loss 0.010433424264192581 Accuracy 0.8876953125\n",
      "Iteration 42390 Training loss 0.004155807662755251 Validation loss 0.01078000571578741 Accuracy 0.88427734375\n",
      "Iteration 42400 Training loss 0.0013080304488539696 Validation loss 0.010541405528783798 Accuracy 0.88623046875\n",
      "Iteration 42410 Training loss 0.003493159543722868 Validation loss 0.010928263887763023 Accuracy 0.88330078125\n",
      "Iteration 42420 Training loss 0.003518175333738327 Validation loss 0.011120337061583996 Accuracy 0.8798828125\n",
      "Iteration 42430 Training loss 0.0034513480495661497 Validation loss 0.011004838161170483 Accuracy 0.880859375\n",
      "Iteration 42440 Training loss 0.003785465843975544 Validation loss 0.010544384829699993 Accuracy 0.88671875\n",
      "Iteration 42450 Training loss 0.0029907994903624058 Validation loss 0.010696807876229286 Accuracy 0.88525390625\n",
      "Iteration 42460 Training loss 0.002635152079164982 Validation loss 0.01059782411903143 Accuracy 0.88525390625\n",
      "Iteration 42470 Training loss 0.002654146635904908 Validation loss 0.010662916116416454 Accuracy 0.88525390625\n",
      "Iteration 42480 Training loss 0.003088184632360935 Validation loss 0.01075603999197483 Accuracy 0.884765625\n",
      "Iteration 42490 Training loss 0.0027314515318721533 Validation loss 0.010793562978506088 Accuracy 0.88427734375\n",
      "Iteration 42500 Training loss 0.0034173668827861547 Validation loss 0.010674611665308475 Accuracy 0.88623046875\n",
      "Iteration 42510 Training loss 0.003754282835870981 Validation loss 0.010586857795715332 Accuracy 0.88623046875\n",
      "Iteration 42520 Training loss 0.002787795616313815 Validation loss 0.010804645717144012 Accuracy 0.88330078125\n",
      "Iteration 42530 Training loss 0.002767602214589715 Validation loss 0.010591519065201283 Accuracy 0.8857421875\n",
      "Iteration 42540 Training loss 0.003491208888590336 Validation loss 0.01067805103957653 Accuracy 0.88623046875\n",
      "Iteration 42550 Training loss 0.003669451456516981 Validation loss 0.010637974366545677 Accuracy 0.88720703125\n",
      "Iteration 42560 Training loss 0.003524972591549158 Validation loss 0.01061574462801218 Accuracy 0.88671875\n",
      "Iteration 42570 Training loss 0.0033930556382983923 Validation loss 0.01069639902561903 Accuracy 0.88525390625\n",
      "Iteration 42580 Training loss 0.003189884126186371 Validation loss 0.010728956200182438 Accuracy 0.884765625\n",
      "Iteration 42590 Training loss 0.0025037764571607113 Validation loss 0.010698121972382069 Accuracy 0.884765625\n",
      "Iteration 42600 Training loss 0.003607391845434904 Validation loss 0.010654745623469353 Accuracy 0.88623046875\n",
      "Iteration 42610 Training loss 0.0028226501308381557 Validation loss 0.010737945325672626 Accuracy 0.8857421875\n",
      "Iteration 42620 Training loss 0.002996055642142892 Validation loss 0.011030364781618118 Accuracy 0.880859375\n",
      "Iteration 42630 Training loss 0.0021531269885599613 Validation loss 0.010715317912399769 Accuracy 0.88427734375\n",
      "Iteration 42640 Training loss 0.0026533729396760464 Validation loss 0.011090063489973545 Accuracy 0.88134765625\n",
      "Iteration 42650 Training loss 0.003003116464242339 Validation loss 0.010592186823487282 Accuracy 0.88623046875\n",
      "Iteration 42660 Training loss 0.0027677242178469896 Validation loss 0.010641233995556831 Accuracy 0.88525390625\n",
      "Iteration 42670 Training loss 0.0028964527882635593 Validation loss 0.01073632575571537 Accuracy 0.88525390625\n",
      "Iteration 42680 Training loss 0.003010808490216732 Validation loss 0.010701579041779041 Accuracy 0.88427734375\n",
      "Iteration 42690 Training loss 0.0026590926572680473 Validation loss 0.010786451399326324 Accuracy 0.88232421875\n",
      "Iteration 42700 Training loss 0.003334522247314453 Validation loss 0.010747583582997322 Accuracy 0.8837890625\n",
      "Iteration 42710 Training loss 0.0031206670682877302 Validation loss 0.010584685951471329 Accuracy 0.8857421875\n",
      "Iteration 42720 Training loss 0.004616208840161562 Validation loss 0.0105296541005373 Accuracy 0.88671875\n",
      "Iteration 42730 Training loss 0.004210519138723612 Validation loss 0.010423803701996803 Accuracy 0.88720703125\n",
      "Iteration 42740 Training loss 0.0049119084142148495 Validation loss 0.010648053139448166 Accuracy 0.88623046875\n",
      "Iteration 42750 Training loss 0.002217611065134406 Validation loss 0.0106447022408247 Accuracy 0.8857421875\n",
      "Iteration 42760 Training loss 0.0018104114569723606 Validation loss 0.010841520503163338 Accuracy 0.88427734375\n",
      "Iteration 42770 Training loss 0.0033669264521449804 Validation loss 0.01062835194170475 Accuracy 0.88623046875\n",
      "Iteration 42780 Training loss 0.0016395162092521787 Validation loss 0.01054171472787857 Accuracy 0.88671875\n",
      "Iteration 42790 Training loss 0.0023689132649451494 Validation loss 0.010792125016450882 Accuracy 0.884765625\n",
      "Iteration 42800 Training loss 0.0038198058027774096 Validation loss 0.01072762068361044 Accuracy 0.88525390625\n",
      "Iteration 42810 Training loss 0.0024745273403823376 Validation loss 0.010750995948910713 Accuracy 0.8857421875\n",
      "Iteration 42820 Training loss 0.003002341603860259 Validation loss 0.010734904557466507 Accuracy 0.8857421875\n",
      "Iteration 42830 Training loss 0.0033458583056926727 Validation loss 0.010704242624342442 Accuracy 0.88525390625\n",
      "Iteration 42840 Training loss 0.002028099028393626 Validation loss 0.010544994845986366 Accuracy 0.88720703125\n",
      "Iteration 42850 Training loss 0.0023377889301627874 Validation loss 0.010570197366178036 Accuracy 0.88623046875\n",
      "Iteration 42860 Training loss 0.004820171277970076 Validation loss 0.010546262376010418 Accuracy 0.88671875\n",
      "Iteration 42870 Training loss 0.0030791577883064747 Validation loss 0.010462351143360138 Accuracy 0.88818359375\n",
      "Iteration 42880 Training loss 0.004049114417284727 Validation loss 0.010346520692110062 Accuracy 0.88916015625\n",
      "Iteration 42890 Training loss 0.0017461908282712102 Validation loss 0.01043880358338356 Accuracy 0.888671875\n",
      "Iteration 42900 Training loss 0.003010490443557501 Validation loss 0.010615898296236992 Accuracy 0.8857421875\n",
      "Iteration 42910 Training loss 0.003788809757679701 Validation loss 0.010580925270915031 Accuracy 0.88720703125\n",
      "Iteration 42920 Training loss 0.0021772682666778564 Validation loss 0.010586635209619999 Accuracy 0.88720703125\n",
      "Iteration 42930 Training loss 0.0038592801429331303 Validation loss 0.0107272919267416 Accuracy 0.88427734375\n",
      "Iteration 42940 Training loss 0.003994669299572706 Validation loss 0.010662783868610859 Accuracy 0.8857421875\n",
      "Iteration 42950 Training loss 0.0026426336262375116 Validation loss 0.010681310668587685 Accuracy 0.884765625\n",
      "Iteration 42960 Training loss 0.003272591158747673 Validation loss 0.010518956929445267 Accuracy 0.88671875\n",
      "Iteration 42970 Training loss 0.00347473518922925 Validation loss 0.010634389705955982 Accuracy 0.884765625\n",
      "Iteration 42980 Training loss 0.002759831491857767 Validation loss 0.010630137287080288 Accuracy 0.8857421875\n",
      "Iteration 42990 Training loss 0.002541032386943698 Validation loss 0.010669445618987083 Accuracy 0.88623046875\n",
      "Iteration 43000 Training loss 0.0026532700285315514 Validation loss 0.010747209191322327 Accuracy 0.88427734375\n",
      "Iteration 43010 Training loss 0.0030459905974566936 Validation loss 0.010611024685204029 Accuracy 0.88623046875\n",
      "Iteration 43020 Training loss 0.0028242364060133696 Validation loss 0.010562499985098839 Accuracy 0.88671875\n",
      "Iteration 43030 Training loss 0.002178437076508999 Validation loss 0.010680732317268848 Accuracy 0.884765625\n",
      "Iteration 43040 Training loss 0.0032406558748334646 Validation loss 0.010646463371813297 Accuracy 0.88525390625\n",
      "Iteration 43050 Training loss 0.0021759257651865482 Validation loss 0.01068168692290783 Accuracy 0.88427734375\n",
      "Iteration 43060 Training loss 0.0035285570193082094 Validation loss 0.010835709981620312 Accuracy 0.88427734375\n",
      "Iteration 43070 Training loss 0.002564008114859462 Validation loss 0.010774033144116402 Accuracy 0.88427734375\n",
      "Iteration 43080 Training loss 0.0031943332869559526 Validation loss 0.010539577342569828 Accuracy 0.8876953125\n",
      "Iteration 43090 Training loss 0.0032045592088252306 Validation loss 0.010727724060416222 Accuracy 0.88525390625\n",
      "Iteration 43100 Training loss 0.0038654538802802563 Validation loss 0.010631939396262169 Accuracy 0.8857421875\n",
      "Iteration 43110 Training loss 0.0036169083323329687 Validation loss 0.010697347111999989 Accuracy 0.8857421875\n",
      "Iteration 43120 Training loss 0.0037448813673108816 Validation loss 0.010768145322799683 Accuracy 0.88427734375\n",
      "Iteration 43130 Training loss 0.0027234998997300863 Validation loss 0.010892525315284729 Accuracy 0.8828125\n",
      "Iteration 43140 Training loss 0.0025530795101076365 Validation loss 0.01064242608845234 Accuracy 0.884765625\n",
      "Iteration 43150 Training loss 0.00262086046859622 Validation loss 0.010648391209542751 Accuracy 0.8857421875\n",
      "Iteration 43160 Training loss 0.00379352574236691 Validation loss 0.010776229202747345 Accuracy 0.88525390625\n",
      "Iteration 43170 Training loss 0.0035548838786780834 Validation loss 0.010767641477286816 Accuracy 0.88525390625\n",
      "Iteration 43180 Training loss 0.0030283050145953894 Validation loss 0.010597418062388897 Accuracy 0.8857421875\n",
      "Iteration 43190 Training loss 0.0041106888093054295 Validation loss 0.01063974667340517 Accuracy 0.88623046875\n",
      "Iteration 43200 Training loss 0.0037712426856160164 Validation loss 0.010643655434250832 Accuracy 0.88525390625\n",
      "Iteration 43210 Training loss 0.0031903653871268034 Validation loss 0.010542700067162514 Accuracy 0.88623046875\n",
      "Iteration 43220 Training loss 0.003776431316509843 Validation loss 0.010543324053287506 Accuracy 0.8876953125\n",
      "Iteration 43230 Training loss 0.002426769118756056 Validation loss 0.010701015591621399 Accuracy 0.8857421875\n",
      "Iteration 43240 Training loss 0.0036855051293969154 Validation loss 0.011320219375193119 Accuracy 0.87841796875\n",
      "Iteration 43250 Training loss 0.003759271465241909 Validation loss 0.010636474005877972 Accuracy 0.88671875\n",
      "Iteration 43260 Training loss 0.00259147840552032 Validation loss 0.010594562627375126 Accuracy 0.88525390625\n",
      "Iteration 43270 Training loss 0.0028511567506939173 Validation loss 0.010685000568628311 Accuracy 0.88427734375\n",
      "Iteration 43280 Training loss 0.003783458610996604 Validation loss 0.01057412289083004 Accuracy 0.8857421875\n",
      "Iteration 43290 Training loss 0.003392287530004978 Validation loss 0.010638942942023277 Accuracy 0.88525390625\n",
      "Iteration 43300 Training loss 0.0037529771216213703 Validation loss 0.010472983121871948 Accuracy 0.88720703125\n",
      "Iteration 43310 Training loss 0.0027059824205935 Validation loss 0.010464834049344063 Accuracy 0.88720703125\n",
      "Iteration 43320 Training loss 0.0021741287782788277 Validation loss 0.01071704551577568 Accuracy 0.8857421875\n",
      "Iteration 43330 Training loss 0.003521881066262722 Validation loss 0.010545369237661362 Accuracy 0.88720703125\n",
      "Iteration 43340 Training loss 0.0024445110466331244 Validation loss 0.010745839215815067 Accuracy 0.8857421875\n",
      "Iteration 43350 Training loss 0.0019077910110354424 Validation loss 0.010498955845832825 Accuracy 0.8876953125\n",
      "Iteration 43360 Training loss 0.002843189751729369 Validation loss 0.010414795018732548 Accuracy 0.8876953125\n",
      "Iteration 43370 Training loss 0.0022045571822673082 Validation loss 0.010650199837982655 Accuracy 0.8857421875\n",
      "Iteration 43380 Training loss 0.002639778656885028 Validation loss 0.010718943551182747 Accuracy 0.88427734375\n",
      "Iteration 43390 Training loss 0.0028830391820520163 Validation loss 0.01077308040112257 Accuracy 0.88427734375\n",
      "Iteration 43400 Training loss 0.00339674623683095 Validation loss 0.010667025111615658 Accuracy 0.88525390625\n",
      "Iteration 43410 Training loss 0.0032579945400357246 Validation loss 0.010794579982757568 Accuracy 0.88525390625\n",
      "Iteration 43420 Training loss 0.0026601182762533426 Validation loss 0.01076617743819952 Accuracy 0.88330078125\n",
      "Iteration 43430 Training loss 0.004529840312898159 Validation loss 0.010618251748383045 Accuracy 0.8857421875\n",
      "Iteration 43440 Training loss 0.004071689676493406 Validation loss 0.010673449374735355 Accuracy 0.884765625\n",
      "Iteration 43450 Training loss 0.0034798826090991497 Validation loss 0.010897309519350529 Accuracy 0.8818359375\n",
      "Iteration 43460 Training loss 0.0031682541593909264 Validation loss 0.01081271655857563 Accuracy 0.8828125\n",
      "Iteration 43470 Training loss 0.004456689581274986 Validation loss 0.01067390851676464 Accuracy 0.8857421875\n",
      "Iteration 43480 Training loss 0.0028034579008817673 Validation loss 0.010767862200737 Accuracy 0.88525390625\n",
      "Iteration 43490 Training loss 0.003684673923999071 Validation loss 0.01069034356623888 Accuracy 0.8857421875\n",
      "Iteration 43500 Training loss 0.002853497164323926 Validation loss 0.0106031633913517 Accuracy 0.88623046875\n",
      "Iteration 43510 Training loss 0.0030095456168055534 Validation loss 0.01068070251494646 Accuracy 0.884765625\n",
      "Iteration 43520 Training loss 0.0038725254125893116 Validation loss 0.010524146258831024 Accuracy 0.88720703125\n",
      "Iteration 43530 Training loss 0.0026685886550694704 Validation loss 0.010595392435789108 Accuracy 0.88623046875\n",
      "Iteration 43540 Training loss 0.002577745821326971 Validation loss 0.010759394615888596 Accuracy 0.8837890625\n",
      "Iteration 43550 Training loss 0.0028490759432315826 Validation loss 0.010505951941013336 Accuracy 0.88671875\n",
      "Iteration 43560 Training loss 0.002433009212836623 Validation loss 0.010573063977062702 Accuracy 0.88671875\n",
      "Iteration 43570 Training loss 0.003622195916250348 Validation loss 0.010596295818686485 Accuracy 0.88623046875\n",
      "Iteration 43580 Training loss 0.0034555222373455763 Validation loss 0.01067711878567934 Accuracy 0.8857421875\n",
      "Iteration 43590 Training loss 0.002529617166146636 Validation loss 0.010499111376702785 Accuracy 0.88818359375\n",
      "Iteration 43600 Training loss 0.0026781968772411346 Validation loss 0.010743198916316032 Accuracy 0.8837890625\n",
      "Iteration 43610 Training loss 0.002972551854327321 Validation loss 0.010772584937512875 Accuracy 0.8837890625\n",
      "Iteration 43620 Training loss 0.004266245290637016 Validation loss 0.010623800568282604 Accuracy 0.88623046875\n",
      "Iteration 43630 Training loss 0.0035642993170768023 Validation loss 0.010672258213162422 Accuracy 0.88427734375\n",
      "Iteration 43640 Training loss 0.0026714708656072617 Validation loss 0.010586386546492577 Accuracy 0.88623046875\n",
      "Iteration 43650 Training loss 0.0017719307215884328 Validation loss 0.010686306282877922 Accuracy 0.884765625\n",
      "Iteration 43660 Training loss 0.004111915826797485 Validation loss 0.010520871728658676 Accuracy 0.8876953125\n",
      "Iteration 43670 Training loss 0.00234472774900496 Validation loss 0.010531923733651638 Accuracy 0.88623046875\n",
      "Iteration 43680 Training loss 0.0028314103838056326 Validation loss 0.010699720121920109 Accuracy 0.884765625\n",
      "Iteration 43690 Training loss 0.0035121766850352287 Validation loss 0.010665380395948887 Accuracy 0.88671875\n",
      "Iteration 43700 Training loss 0.004745837301015854 Validation loss 0.010650424286723137 Accuracy 0.88623046875\n",
      "Iteration 43710 Training loss 0.004213080741465092 Validation loss 0.01070246659219265 Accuracy 0.8857421875\n",
      "Iteration 43720 Training loss 0.0038109777960926294 Validation loss 0.010571049526333809 Accuracy 0.88623046875\n",
      "Iteration 43730 Training loss 0.0025257703382521868 Validation loss 0.010692643001675606 Accuracy 0.88623046875\n",
      "Iteration 43740 Training loss 0.003246262203902006 Validation loss 0.010502837598323822 Accuracy 0.88720703125\n",
      "Iteration 43750 Training loss 0.0022925653029233217 Validation loss 0.01058558002114296 Accuracy 0.8857421875\n",
      "Iteration 43760 Training loss 0.0021510536316782236 Validation loss 0.010627989657223225 Accuracy 0.88671875\n",
      "Iteration 43770 Training loss 0.0036628853995352983 Validation loss 0.010687639936804771 Accuracy 0.88427734375\n",
      "Iteration 43780 Training loss 0.0028595682233572006 Validation loss 0.010640610940754414 Accuracy 0.8857421875\n",
      "Iteration 43790 Training loss 0.0032507088035345078 Validation loss 0.010623457841575146 Accuracy 0.884765625\n",
      "Iteration 43800 Training loss 0.0034427775535732508 Validation loss 0.010717760771512985 Accuracy 0.884765625\n",
      "Iteration 43810 Training loss 0.0038962687831372023 Validation loss 0.010745061561465263 Accuracy 0.88427734375\n",
      "Iteration 43820 Training loss 0.0035376872401684523 Validation loss 0.010861112736165524 Accuracy 0.88330078125\n",
      "Iteration 43830 Training loss 0.0037728403694927692 Validation loss 0.010938824154436588 Accuracy 0.8828125\n",
      "Iteration 43840 Training loss 0.002412542700767517 Validation loss 0.010694729164242744 Accuracy 0.88525390625\n",
      "Iteration 43850 Training loss 0.0036924686282873154 Validation loss 0.01066662184894085 Accuracy 0.8857421875\n",
      "Iteration 43860 Training loss 0.002308448078110814 Validation loss 0.010577099397778511 Accuracy 0.8857421875\n",
      "Iteration 43870 Training loss 0.003142639761790633 Validation loss 0.01076273899525404 Accuracy 0.884765625\n",
      "Iteration 43880 Training loss 0.0028451576363295317 Validation loss 0.010791991837322712 Accuracy 0.8837890625\n",
      "Iteration 43890 Training loss 0.002769823418930173 Validation loss 0.010699198581278324 Accuracy 0.88427734375\n",
      "Iteration 43900 Training loss 0.00337530137039721 Validation loss 0.010701064020395279 Accuracy 0.88525390625\n",
      "Iteration 43910 Training loss 0.003933677449822426 Validation loss 0.010757463052868843 Accuracy 0.88330078125\n",
      "Iteration 43920 Training loss 0.0035400851629674435 Validation loss 0.010805265977978706 Accuracy 0.88427734375\n",
      "Iteration 43930 Training loss 0.0021950984373688698 Validation loss 0.010741034522652626 Accuracy 0.884765625\n",
      "Iteration 43940 Training loss 0.003022278193384409 Validation loss 0.01087250467389822 Accuracy 0.88232421875\n",
      "Iteration 43950 Training loss 0.0022665958385914564 Validation loss 0.010695111937820911 Accuracy 0.88525390625\n",
      "Iteration 43960 Training loss 0.004359654616564512 Validation loss 0.010777927935123444 Accuracy 0.88623046875\n",
      "Iteration 43970 Training loss 0.0030180790927261114 Validation loss 0.010852071456611156 Accuracy 0.88330078125\n",
      "Iteration 43980 Training loss 0.002497021108865738 Validation loss 0.01081538014113903 Accuracy 0.88330078125\n",
      "Iteration 43990 Training loss 0.0028980127535760403 Validation loss 0.010693512856960297 Accuracy 0.88525390625\n",
      "Iteration 44000 Training loss 0.002512105042114854 Validation loss 0.010733582079410553 Accuracy 0.88525390625\n",
      "Iteration 44010 Training loss 0.0034256966318935156 Validation loss 0.010709100402891636 Accuracy 0.884765625\n",
      "Iteration 44020 Training loss 0.0036209067329764366 Validation loss 0.010648042894899845 Accuracy 0.88623046875\n",
      "Iteration 44030 Training loss 0.0032602993305772543 Validation loss 0.010763893835246563 Accuracy 0.884765625\n",
      "Iteration 44040 Training loss 0.0037086522206664085 Validation loss 0.010886513628065586 Accuracy 0.88232421875\n",
      "Iteration 44050 Training loss 0.0032856445759534836 Validation loss 0.010790151543915272 Accuracy 0.88427734375\n",
      "Iteration 44060 Training loss 0.003955204505473375 Validation loss 0.010928626172244549 Accuracy 0.8828125\n",
      "Iteration 44070 Training loss 0.0031229699961841106 Validation loss 0.010866200551390648 Accuracy 0.8828125\n",
      "Iteration 44080 Training loss 0.002830593613907695 Validation loss 0.01096384972333908 Accuracy 0.88232421875\n",
      "Iteration 44090 Training loss 0.004313889890909195 Validation loss 0.010949301533401012 Accuracy 0.8818359375\n",
      "Iteration 44100 Training loss 0.0022802178282290697 Validation loss 0.011051452718675137 Accuracy 0.8818359375\n",
      "Iteration 44110 Training loss 0.003654740983620286 Validation loss 0.010871664620935917 Accuracy 0.8828125\n",
      "Iteration 44120 Training loss 0.002663949504494667 Validation loss 0.010853917337954044 Accuracy 0.88330078125\n",
      "Iteration 44130 Training loss 0.003414709120988846 Validation loss 0.010940446518361568 Accuracy 0.8828125\n",
      "Iteration 44140 Training loss 0.0034411961678415537 Validation loss 0.010880221612751484 Accuracy 0.88330078125\n",
      "Iteration 44150 Training loss 0.0035450260620564222 Validation loss 0.010877043008804321 Accuracy 0.8837890625\n",
      "Iteration 44160 Training loss 0.003397765103727579 Validation loss 0.010882665403187275 Accuracy 0.8837890625\n",
      "Iteration 44170 Training loss 0.0035241087898612022 Validation loss 0.010874731466174126 Accuracy 0.8837890625\n",
      "Iteration 44180 Training loss 0.00297885132022202 Validation loss 0.010846978053450584 Accuracy 0.8837890625\n",
      "Iteration 44190 Training loss 0.0028962395153939724 Validation loss 0.011125299148261547 Accuracy 0.880859375\n",
      "Iteration 44200 Training loss 0.0029234387911856174 Validation loss 0.01088419184088707 Accuracy 0.88330078125\n",
      "Iteration 44210 Training loss 0.0035950113087892532 Validation loss 0.010866757482290268 Accuracy 0.8837890625\n",
      "Iteration 44220 Training loss 0.0028011782560497522 Validation loss 0.01082372572273016 Accuracy 0.8837890625\n",
      "Iteration 44230 Training loss 0.003782095853239298 Validation loss 0.010823319666087627 Accuracy 0.88330078125\n",
      "Iteration 44240 Training loss 0.0026539727114140987 Validation loss 0.010724836960434914 Accuracy 0.884765625\n",
      "Iteration 44250 Training loss 0.002717049792408943 Validation loss 0.010818066075444221 Accuracy 0.8837890625\n",
      "Iteration 44260 Training loss 0.002657763659954071 Validation loss 0.010561729781329632 Accuracy 0.88623046875\n",
      "Iteration 44270 Training loss 0.0028130861464887857 Validation loss 0.010643956251442432 Accuracy 0.8857421875\n",
      "Iteration 44280 Training loss 0.0031628322321921587 Validation loss 0.010536924935877323 Accuracy 0.88720703125\n",
      "Iteration 44290 Training loss 0.004038010723888874 Validation loss 0.010809484869241714 Accuracy 0.8837890625\n",
      "Iteration 44300 Training loss 0.0031004950869828463 Validation loss 0.010729154571890831 Accuracy 0.884765625\n",
      "Iteration 44310 Training loss 0.0033865601290017366 Validation loss 0.010621354915201664 Accuracy 0.88623046875\n",
      "Iteration 44320 Training loss 0.002455378184095025 Validation loss 0.010658567771315575 Accuracy 0.88623046875\n",
      "Iteration 44330 Training loss 0.002317163860425353 Validation loss 0.010768922977149487 Accuracy 0.8837890625\n",
      "Iteration 44340 Training loss 0.004080381244421005 Validation loss 0.010721425525844097 Accuracy 0.88427734375\n",
      "Iteration 44350 Training loss 0.003102795220911503 Validation loss 0.010734504088759422 Accuracy 0.8837890625\n",
      "Iteration 44360 Training loss 0.0030501035507768393 Validation loss 0.010931672528386116 Accuracy 0.8818359375\n",
      "Iteration 44370 Training loss 0.004327910020947456 Validation loss 0.010705417022109032 Accuracy 0.88525390625\n",
      "Iteration 44380 Training loss 0.002195813925936818 Validation loss 0.010757277719676495 Accuracy 0.88427734375\n",
      "Iteration 44390 Training loss 0.0025789164938032627 Validation loss 0.010804273188114166 Accuracy 0.8837890625\n",
      "Iteration 44400 Training loss 0.002947867615148425 Validation loss 0.010615619830787182 Accuracy 0.88671875\n",
      "Iteration 44410 Training loss 0.003809420159086585 Validation loss 0.010649366304278374 Accuracy 0.884765625\n",
      "Iteration 44420 Training loss 0.004162325523793697 Validation loss 0.010635319165885448 Accuracy 0.88427734375\n",
      "Iteration 44430 Training loss 0.002702775876969099 Validation loss 0.01056775264441967 Accuracy 0.88623046875\n",
      "Iteration 44440 Training loss 0.002284494461491704 Validation loss 0.01056625321507454 Accuracy 0.88623046875\n",
      "Iteration 44450 Training loss 0.0033547459170222282 Validation loss 0.010759560391306877 Accuracy 0.884765625\n",
      "Iteration 44460 Training loss 0.003619581228122115 Validation loss 0.010731554590165615 Accuracy 0.8837890625\n",
      "Iteration 44470 Training loss 0.0036875600926578045 Validation loss 0.010889487341046333 Accuracy 0.88232421875\n",
      "Iteration 44480 Training loss 0.003240046091377735 Validation loss 0.01062419917434454 Accuracy 0.88623046875\n",
      "Iteration 44490 Training loss 0.002787858946248889 Validation loss 0.01079415250569582 Accuracy 0.88330078125\n",
      "Iteration 44500 Training loss 0.0036897265817970037 Validation loss 0.010766876861453056 Accuracy 0.8837890625\n",
      "Iteration 44510 Training loss 0.002755426801741123 Validation loss 0.01075101736932993 Accuracy 0.88427734375\n",
      "Iteration 44520 Training loss 0.0036651541013270617 Validation loss 0.010650415904819965 Accuracy 0.884765625\n",
      "Iteration 44530 Training loss 0.0044106608256697655 Validation loss 0.01097075641155243 Accuracy 0.88232421875\n",
      "Iteration 44540 Training loss 0.0037321129348129034 Validation loss 0.010707821696996689 Accuracy 0.884765625\n",
      "Iteration 44550 Training loss 0.003033902496099472 Validation loss 0.010684213601052761 Accuracy 0.884765625\n",
      "Iteration 44560 Training loss 0.004189174622297287 Validation loss 0.010703892447054386 Accuracy 0.884765625\n",
      "Iteration 44570 Training loss 0.0024560189340263605 Validation loss 0.010707658715546131 Accuracy 0.88525390625\n",
      "Iteration 44580 Training loss 0.00387491169385612 Validation loss 0.01073025818914175 Accuracy 0.88525390625\n",
      "Iteration 44590 Training loss 0.004191360902041197 Validation loss 0.010605852119624615 Accuracy 0.88623046875\n",
      "Iteration 44600 Training loss 0.0030172092374414206 Validation loss 0.010613112710416317 Accuracy 0.8857421875\n",
      "Iteration 44610 Training loss 0.002283101435750723 Validation loss 0.010639730840921402 Accuracy 0.88525390625\n",
      "Iteration 44620 Training loss 0.0021754922345280647 Validation loss 0.010667752474546432 Accuracy 0.88525390625\n",
      "Iteration 44630 Training loss 0.0024539176374673843 Validation loss 0.010643496178090572 Accuracy 0.884765625\n",
      "Iteration 44640 Training loss 0.004407824017107487 Validation loss 0.010679470375180244 Accuracy 0.884765625\n",
      "Iteration 44650 Training loss 0.0038196074310690165 Validation loss 0.010652459226548672 Accuracy 0.88427734375\n",
      "Iteration 44660 Training loss 0.0016405553324148059 Validation loss 0.010804361663758755 Accuracy 0.88427734375\n",
      "Iteration 44670 Training loss 0.002403398510068655 Validation loss 0.010665043257176876 Accuracy 0.8857421875\n",
      "Iteration 44680 Training loss 0.0037878158036619425 Validation loss 0.010564611293375492 Accuracy 0.88623046875\n",
      "Iteration 44690 Training loss 0.0022808751091361046 Validation loss 0.010521230287849903 Accuracy 0.88720703125\n",
      "Iteration 44700 Training loss 0.003575672395527363 Validation loss 0.010667726397514343 Accuracy 0.88525390625\n",
      "Iteration 44710 Training loss 0.0036804755218327045 Validation loss 0.010742207057774067 Accuracy 0.88427734375\n",
      "Iteration 44720 Training loss 0.003103247843682766 Validation loss 0.010677658021450043 Accuracy 0.88623046875\n",
      "Iteration 44730 Training loss 0.0030299376230686903 Validation loss 0.010848430916666985 Accuracy 0.88232421875\n",
      "Iteration 44740 Training loss 0.0035578873939812183 Validation loss 0.010843581520020962 Accuracy 0.88330078125\n",
      "Iteration 44750 Training loss 0.003666250267997384 Validation loss 0.010864541865885258 Accuracy 0.88330078125\n",
      "Iteration 44760 Training loss 0.0035083817783743143 Validation loss 0.010675141587853432 Accuracy 0.8857421875\n",
      "Iteration 44770 Training loss 0.0028246999718248844 Validation loss 0.0106230853125453 Accuracy 0.88623046875\n",
      "Iteration 44780 Training loss 0.0035392700228840113 Validation loss 0.010686174035072327 Accuracy 0.88427734375\n",
      "Iteration 44790 Training loss 0.0017640263540670276 Validation loss 0.010714098811149597 Accuracy 0.884765625\n",
      "Iteration 44800 Training loss 0.0036121548619121313 Validation loss 0.010608985088765621 Accuracy 0.88671875\n",
      "Iteration 44810 Training loss 0.003244862426072359 Validation loss 0.010765539482235909 Accuracy 0.8837890625\n",
      "Iteration 44820 Training loss 0.0042574149556458 Validation loss 0.010768609121441841 Accuracy 0.884765625\n",
      "Iteration 44830 Training loss 0.002694072900339961 Validation loss 0.010656364262104034 Accuracy 0.8857421875\n",
      "Iteration 44840 Training loss 0.001963603077456355 Validation loss 0.010675644502043724 Accuracy 0.884765625\n",
      "Iteration 44850 Training loss 0.002518025226891041 Validation loss 0.0108222970739007 Accuracy 0.8837890625\n",
      "Iteration 44860 Training loss 0.0034349130000919104 Validation loss 0.010640480555593967 Accuracy 0.88671875\n",
      "Iteration 44870 Training loss 0.003440509084612131 Validation loss 0.010813465341925621 Accuracy 0.8837890625\n",
      "Iteration 44880 Training loss 0.003465634537860751 Validation loss 0.010851128026843071 Accuracy 0.8828125\n",
      "Iteration 44890 Training loss 0.001839072210714221 Validation loss 0.01072730217128992 Accuracy 0.88623046875\n",
      "Iteration 44900 Training loss 0.00318328058347106 Validation loss 0.010649381205439568 Accuracy 0.8857421875\n",
      "Iteration 44910 Training loss 0.003437418257817626 Validation loss 0.010613063350319862 Accuracy 0.88623046875\n",
      "Iteration 44920 Training loss 0.0034428329672664404 Validation loss 0.01070384494960308 Accuracy 0.884765625\n",
      "Iteration 44930 Training loss 0.0033459924161434174 Validation loss 0.010735644027590752 Accuracy 0.8837890625\n",
      "Iteration 44940 Training loss 0.0033011646009981632 Validation loss 0.01065994892269373 Accuracy 0.8857421875\n",
      "Iteration 44950 Training loss 0.0026141004636883736 Validation loss 0.010769728571176529 Accuracy 0.88427734375\n",
      "Iteration 44960 Training loss 0.003050577826797962 Validation loss 0.010837206616997719 Accuracy 0.88427734375\n",
      "Iteration 44970 Training loss 0.002783013042062521 Validation loss 0.010654189623892307 Accuracy 0.8857421875\n",
      "Iteration 44980 Training loss 0.0024897686671465635 Validation loss 0.010667516849935055 Accuracy 0.8857421875\n",
      "Iteration 44990 Training loss 0.004751409869641066 Validation loss 0.010816887021064758 Accuracy 0.8837890625\n",
      "Iteration 45000 Training loss 0.0020667097996920347 Validation loss 0.010791923850774765 Accuracy 0.884765625\n",
      "Iteration 45010 Training loss 0.002640285762026906 Validation loss 0.010835863649845123 Accuracy 0.8837890625\n",
      "Iteration 45020 Training loss 0.004318063147366047 Validation loss 0.010835262015461922 Accuracy 0.88330078125\n",
      "Iteration 45030 Training loss 0.0021837169770151377 Validation loss 0.010721943341195583 Accuracy 0.88427734375\n",
      "Iteration 45040 Training loss 0.0012258682399988174 Validation loss 0.010789615102112293 Accuracy 0.8837890625\n",
      "Iteration 45050 Training loss 0.003808677662163973 Validation loss 0.010636902414262295 Accuracy 0.8857421875\n",
      "Iteration 45060 Training loss 0.0030562984757125378 Validation loss 0.010836580768227577 Accuracy 0.8828125\n",
      "Iteration 45070 Training loss 0.0026778888422995806 Validation loss 0.010679514147341251 Accuracy 0.88525390625\n",
      "Iteration 45080 Training loss 0.002651395509019494 Validation loss 0.010790424421429634 Accuracy 0.8837890625\n",
      "Iteration 45090 Training loss 0.0027414073701947927 Validation loss 0.010990936309099197 Accuracy 0.8818359375\n",
      "Iteration 45100 Training loss 0.004281431902199984 Validation loss 0.010818964801728725 Accuracy 0.88427734375\n",
      "Iteration 45110 Training loss 0.002522441791370511 Validation loss 0.01100603025406599 Accuracy 0.8818359375\n",
      "Iteration 45120 Training loss 0.003300166456028819 Validation loss 0.010769295506179333 Accuracy 0.88427734375\n",
      "Iteration 45130 Training loss 0.00192247424274683 Validation loss 0.01064543891698122 Accuracy 0.88671875\n",
      "Iteration 45140 Training loss 0.0027269073761999607 Validation loss 0.01067738514393568 Accuracy 0.884765625\n",
      "Iteration 45150 Training loss 0.003050445578992367 Validation loss 0.010769503191113472 Accuracy 0.8837890625\n",
      "Iteration 45160 Training loss 0.0024618152529001236 Validation loss 0.010876896791160107 Accuracy 0.88330078125\n",
      "Iteration 45170 Training loss 0.0023879350628703833 Validation loss 0.010808732360601425 Accuracy 0.884765625\n",
      "Iteration 45180 Training loss 0.0032485343981534243 Validation loss 0.010633922182023525 Accuracy 0.88623046875\n",
      "Iteration 45190 Training loss 0.002670773770660162 Validation loss 0.010738571174442768 Accuracy 0.884765625\n",
      "Iteration 45200 Training loss 0.003530242945998907 Validation loss 0.010791592299938202 Accuracy 0.88427734375\n",
      "Iteration 45210 Training loss 0.0038316077552735806 Validation loss 0.01085314154624939 Accuracy 0.88330078125\n",
      "Iteration 45220 Training loss 0.0017229049699380994 Validation loss 0.010705218650400639 Accuracy 0.88427734375\n",
      "Iteration 45230 Training loss 0.003910844214260578 Validation loss 0.010783986188471317 Accuracy 0.88427734375\n",
      "Iteration 45240 Training loss 0.004503238946199417 Validation loss 0.01061108335852623 Accuracy 0.88623046875\n",
      "Iteration 45250 Training loss 0.003059124806895852 Validation loss 0.010742979124188423 Accuracy 0.88427734375\n",
      "Iteration 45260 Training loss 0.002301018452271819 Validation loss 0.010692699812352657 Accuracy 0.884765625\n",
      "Iteration 45270 Training loss 0.0021465958561748266 Validation loss 0.01067445520311594 Accuracy 0.8837890625\n",
      "Iteration 45280 Training loss 0.002993667032569647 Validation loss 0.010560528375208378 Accuracy 0.88671875\n",
      "Iteration 45290 Training loss 0.002916737226769328 Validation loss 0.01057110819965601 Accuracy 0.88623046875\n",
      "Iteration 45300 Training loss 0.003821259131655097 Validation loss 0.010783585719764233 Accuracy 0.88330078125\n",
      "Iteration 45310 Training loss 0.002457896713167429 Validation loss 0.010722039267420769 Accuracy 0.884765625\n",
      "Iteration 45320 Training loss 0.002266732743009925 Validation loss 0.01089642196893692 Accuracy 0.88232421875\n",
      "Iteration 45330 Training loss 0.0033242113422602415 Validation loss 0.010906166397035122 Accuracy 0.88232421875\n",
      "Iteration 45340 Training loss 0.002975670387968421 Validation loss 0.010599552653729916 Accuracy 0.88623046875\n",
      "Iteration 45350 Training loss 0.002264341339468956 Validation loss 0.010766196064651012 Accuracy 0.88427734375\n",
      "Iteration 45360 Training loss 0.0030062971636652946 Validation loss 0.010708678513765335 Accuracy 0.8837890625\n",
      "Iteration 45370 Training loss 0.003424611408263445 Validation loss 0.010667930357158184 Accuracy 0.88525390625\n",
      "Iteration 45380 Training loss 0.003345800330862403 Validation loss 0.010647187009453773 Accuracy 0.88525390625\n",
      "Iteration 45390 Training loss 0.0026587373577058315 Validation loss 0.010679648257791996 Accuracy 0.88330078125\n",
      "Iteration 45400 Training loss 0.0034594624303281307 Validation loss 0.010694297961890697 Accuracy 0.8857421875\n",
      "Iteration 45410 Training loss 0.003319180104881525 Validation loss 0.01060060877352953 Accuracy 0.8857421875\n",
      "Iteration 45420 Training loss 0.00329402438364923 Validation loss 0.010628610849380493 Accuracy 0.8857421875\n",
      "Iteration 45430 Training loss 0.003040400566533208 Validation loss 0.010786280035972595 Accuracy 0.88525390625\n",
      "Iteration 45440 Training loss 0.003648871323093772 Validation loss 0.010644521564245224 Accuracy 0.88623046875\n",
      "Iteration 45450 Training loss 0.0026831740979105234 Validation loss 0.010602721944451332 Accuracy 0.8857421875\n",
      "Iteration 45460 Training loss 0.0025508450344204903 Validation loss 0.010755534283816814 Accuracy 0.88427734375\n",
      "Iteration 45470 Training loss 0.0031792912632226944 Validation loss 0.01081449817866087 Accuracy 0.88427734375\n",
      "Iteration 45480 Training loss 0.0035040511284023523 Validation loss 0.01079173106700182 Accuracy 0.88427734375\n",
      "Iteration 45490 Training loss 0.0024586825165897608 Validation loss 0.010575971566140652 Accuracy 0.88671875\n",
      "Iteration 45500 Training loss 0.002722501754760742 Validation loss 0.010671116411685944 Accuracy 0.88525390625\n",
      "Iteration 45510 Training loss 0.002737971255555749 Validation loss 0.01067936047911644 Accuracy 0.884765625\n",
      "Iteration 45520 Training loss 0.0026229186914861202 Validation loss 0.010706447064876556 Accuracy 0.88427734375\n",
      "Iteration 45530 Training loss 0.003564703045412898 Validation loss 0.010726364329457283 Accuracy 0.884765625\n",
      "Iteration 45540 Training loss 0.0034086285158991814 Validation loss 0.010649469681084156 Accuracy 0.88427734375\n",
      "Iteration 45550 Training loss 0.0026155286468565464 Validation loss 0.010712064802646637 Accuracy 0.8857421875\n",
      "Iteration 45560 Training loss 0.0034470907412469387 Validation loss 0.010632108896970749 Accuracy 0.88623046875\n",
      "Iteration 45570 Training loss 0.004327406641095877 Validation loss 0.01063541229814291 Accuracy 0.8857421875\n",
      "Iteration 45580 Training loss 0.0029190084896981716 Validation loss 0.010932397097349167 Accuracy 0.88330078125\n",
      "Iteration 45590 Training loss 0.002952211769297719 Validation loss 0.010861757211387157 Accuracy 0.8828125\n",
      "Iteration 45600 Training loss 0.0024112078826874495 Validation loss 0.010618966072797775 Accuracy 0.8857421875\n",
      "Iteration 45610 Training loss 0.003372492967173457 Validation loss 0.010660534724593163 Accuracy 0.88671875\n",
      "Iteration 45620 Training loss 0.003841657657176256 Validation loss 0.0106187229976058 Accuracy 0.88623046875\n",
      "Iteration 45630 Training loss 0.003960391506552696 Validation loss 0.010616512969136238 Accuracy 0.8857421875\n",
      "Iteration 45640 Training loss 0.0029963203705847263 Validation loss 0.010677996091544628 Accuracy 0.8857421875\n",
      "Iteration 45650 Training loss 0.002840779023244977 Validation loss 0.010625075548887253 Accuracy 0.88623046875\n",
      "Iteration 45660 Training loss 0.0029201454017311335 Validation loss 0.010648192837834358 Accuracy 0.88623046875\n",
      "Iteration 45670 Training loss 0.0026001017540693283 Validation loss 0.010563777759671211 Accuracy 0.8876953125\n",
      "Iteration 45680 Training loss 0.0031783466693013906 Validation loss 0.010709631256759167 Accuracy 0.884765625\n",
      "Iteration 45690 Training loss 0.0017618306446820498 Validation loss 0.010708294808864594 Accuracy 0.8857421875\n",
      "Iteration 45700 Training loss 0.003973101731389761 Validation loss 0.010741114616394043 Accuracy 0.884765625\n",
      "Iteration 45710 Training loss 0.0036879493854939938 Validation loss 0.010700048878788948 Accuracy 0.884765625\n",
      "Iteration 45720 Training loss 0.002075828844681382 Validation loss 0.01047674659639597 Accuracy 0.8876953125\n",
      "Iteration 45730 Training loss 0.004341174848377705 Validation loss 0.010502099059522152 Accuracy 0.88720703125\n",
      "Iteration 45740 Training loss 0.0026512013282626867 Validation loss 0.010549969971179962 Accuracy 0.8857421875\n",
      "Iteration 45750 Training loss 0.0024588550440967083 Validation loss 0.010497288778424263 Accuracy 0.88671875\n",
      "Iteration 45760 Training loss 0.0018623938085511327 Validation loss 0.010470923967659473 Accuracy 0.88720703125\n",
      "Iteration 45770 Training loss 0.0019345175242051482 Validation loss 0.010656323283910751 Accuracy 0.88427734375\n",
      "Iteration 45780 Training loss 0.003353756619617343 Validation loss 0.010605888441205025 Accuracy 0.88623046875\n",
      "Iteration 45790 Training loss 0.0022538634948432446 Validation loss 0.010692003183066845 Accuracy 0.884765625\n",
      "Iteration 45800 Training loss 0.0025022891350090504 Validation loss 0.01069649402052164 Accuracy 0.884765625\n",
      "Iteration 45810 Training loss 0.0035629933699965477 Validation loss 0.010709199123084545 Accuracy 0.8857421875\n",
      "Iteration 45820 Training loss 0.0030204078648239374 Validation loss 0.010584477335214615 Accuracy 0.88623046875\n",
      "Iteration 45830 Training loss 0.0031884415075182915 Validation loss 0.010548052377998829 Accuracy 0.8857421875\n",
      "Iteration 45840 Training loss 0.004026284907013178 Validation loss 0.01072765700519085 Accuracy 0.88427734375\n",
      "Iteration 45850 Training loss 0.0030915418174117804 Validation loss 0.010641541332006454 Accuracy 0.8857421875\n",
      "Iteration 45860 Training loss 0.0026876700576394796 Validation loss 0.010976763442158699 Accuracy 0.88232421875\n",
      "Iteration 45870 Training loss 0.004040507134050131 Validation loss 0.010662609711289406 Accuracy 0.88525390625\n",
      "Iteration 45880 Training loss 0.002721525263041258 Validation loss 0.01064352411776781 Accuracy 0.8857421875\n",
      "Iteration 45890 Training loss 0.002312122378498316 Validation loss 0.01065151672810316 Accuracy 0.8857421875\n",
      "Iteration 45900 Training loss 0.0032766987569630146 Validation loss 0.010611169040203094 Accuracy 0.8857421875\n",
      "Iteration 45910 Training loss 0.003195713274180889 Validation loss 0.01057208701968193 Accuracy 0.88623046875\n",
      "Iteration 45920 Training loss 0.003126103663817048 Validation loss 0.010563737712800503 Accuracy 0.88671875\n",
      "Iteration 45930 Training loss 0.0032471735030412674 Validation loss 0.010646024718880653 Accuracy 0.88525390625\n",
      "Iteration 45940 Training loss 0.002657700562849641 Validation loss 0.010445191524922848 Accuracy 0.88623046875\n",
      "Iteration 45950 Training loss 0.0033345979172736406 Validation loss 0.010501083917915821 Accuracy 0.8876953125\n",
      "Iteration 45960 Training loss 0.003528304398059845 Validation loss 0.01062462106347084 Accuracy 0.8857421875\n",
      "Iteration 45970 Training loss 0.002732990775257349 Validation loss 0.01065275352448225 Accuracy 0.884765625\n",
      "Iteration 45980 Training loss 0.0033653220161795616 Validation loss 0.01066880114376545 Accuracy 0.88623046875\n",
      "Iteration 45990 Training loss 0.0027103631291538477 Validation loss 0.010662633925676346 Accuracy 0.88623046875\n",
      "Iteration 46000 Training loss 0.003488316433504224 Validation loss 0.010732104070484638 Accuracy 0.884765625\n",
      "Iteration 46010 Training loss 0.002805037423968315 Validation loss 0.010761374607682228 Accuracy 0.8837890625\n",
      "Iteration 46020 Training loss 0.0032350250985473394 Validation loss 0.010639560408890247 Accuracy 0.884765625\n",
      "Iteration 46030 Training loss 0.0035462912637740374 Validation loss 0.01061417069286108 Accuracy 0.88623046875\n",
      "Iteration 46040 Training loss 0.0036669180262833834 Validation loss 0.010603569447994232 Accuracy 0.8857421875\n",
      "Iteration 46050 Training loss 0.002487247809767723 Validation loss 0.010940623469650745 Accuracy 0.88330078125\n",
      "Iteration 46060 Training loss 0.0026321744080632925 Validation loss 0.010749051347374916 Accuracy 0.88525390625\n",
      "Iteration 46070 Training loss 0.0026678936555981636 Validation loss 0.010593929328024387 Accuracy 0.8857421875\n",
      "Iteration 46080 Training loss 0.0033664193470031023 Validation loss 0.010450365953147411 Accuracy 0.88720703125\n",
      "Iteration 46090 Training loss 0.0022549189161509275 Validation loss 0.01055973581969738 Accuracy 0.88623046875\n",
      "Iteration 46100 Training loss 0.0036452794447541237 Validation loss 0.010502348653972149 Accuracy 0.88720703125\n",
      "Iteration 46110 Training loss 0.002260893117636442 Validation loss 0.010546398349106312 Accuracy 0.88720703125\n",
      "Iteration 46120 Training loss 0.0018954552942886949 Validation loss 0.010546904057264328 Accuracy 0.88671875\n",
      "Iteration 46130 Training loss 0.0026301604229956865 Validation loss 0.010535228997468948 Accuracy 0.88623046875\n",
      "Iteration 46140 Training loss 0.002992387395352125 Validation loss 0.011001543141901493 Accuracy 0.88134765625\n",
      "Iteration 46150 Training loss 0.004831735510379076 Validation loss 0.010808691382408142 Accuracy 0.8837890625\n",
      "Iteration 46160 Training loss 0.00388797908090055 Validation loss 0.010725655592978 Accuracy 0.8837890625\n",
      "Iteration 46170 Training loss 0.00253412127494812 Validation loss 0.010698198340833187 Accuracy 0.88427734375\n",
      "Iteration 46180 Training loss 0.003550122957676649 Validation loss 0.010873730294406414 Accuracy 0.88232421875\n",
      "Iteration 46190 Training loss 0.003514606738463044 Validation loss 0.010757924988865852 Accuracy 0.88427734375\n",
      "Iteration 46200 Training loss 0.0038051337469369173 Validation loss 0.010595371015369892 Accuracy 0.88671875\n",
      "Iteration 46210 Training loss 0.003125250805169344 Validation loss 0.010796916671097279 Accuracy 0.88427734375\n",
      "Iteration 46220 Training loss 0.0023483692202717066 Validation loss 0.010684069246053696 Accuracy 0.88427734375\n",
      "Iteration 46230 Training loss 0.0023584100417792797 Validation loss 0.010641832835972309 Accuracy 0.88525390625\n",
      "Iteration 46240 Training loss 0.003966503776609898 Validation loss 0.010748139582574368 Accuracy 0.884765625\n",
      "Iteration 46250 Training loss 0.0032735855784267187 Validation loss 0.010559627786278725 Accuracy 0.88671875\n",
      "Iteration 46260 Training loss 0.002494300249963999 Validation loss 0.010510588064789772 Accuracy 0.88671875\n",
      "Iteration 46270 Training loss 0.0025604753755033016 Validation loss 0.010545508936047554 Accuracy 0.88671875\n",
      "Iteration 46280 Training loss 0.0029629208147525787 Validation loss 0.010622845962643623 Accuracy 0.88623046875\n",
      "Iteration 46290 Training loss 0.0034265187568962574 Validation loss 0.010725614614784718 Accuracy 0.884765625\n",
      "Iteration 46300 Training loss 0.0034205468837171793 Validation loss 0.01069931872189045 Accuracy 0.88427734375\n",
      "Iteration 46310 Training loss 0.003999061416834593 Validation loss 0.010741411708295345 Accuracy 0.88330078125\n",
      "Iteration 46320 Training loss 0.0022876497823745012 Validation loss 0.010516483336687088 Accuracy 0.88720703125\n",
      "Iteration 46330 Training loss 0.0035074881743639708 Validation loss 0.010739143006503582 Accuracy 0.8837890625\n",
      "Iteration 46340 Training loss 0.0032133820932358503 Validation loss 0.010618571192026138 Accuracy 0.8857421875\n",
      "Iteration 46350 Training loss 0.0028420593589544296 Validation loss 0.010545242577791214 Accuracy 0.88671875\n",
      "Iteration 46360 Training loss 0.002946866676211357 Validation loss 0.01053601410239935 Accuracy 0.88671875\n",
      "Iteration 46370 Training loss 0.002447958802804351 Validation loss 0.010616001673042774 Accuracy 0.88525390625\n",
      "Iteration 46380 Training loss 0.003235338255763054 Validation loss 0.01075062993913889 Accuracy 0.8837890625\n",
      "Iteration 46390 Training loss 0.0032484473194926977 Validation loss 0.010660853236913681 Accuracy 0.884765625\n",
      "Iteration 46400 Training loss 0.003626585006713867 Validation loss 0.010681159794330597 Accuracy 0.8857421875\n",
      "Iteration 46410 Training loss 0.002501749899238348 Validation loss 0.010660684667527676 Accuracy 0.88525390625\n",
      "Iteration 46420 Training loss 0.002497608307749033 Validation loss 0.010574137791991234 Accuracy 0.8857421875\n",
      "Iteration 46430 Training loss 0.0024536065757274628 Validation loss 0.010541762225329876 Accuracy 0.8876953125\n",
      "Iteration 46440 Training loss 0.004521807190030813 Validation loss 0.010699589736759663 Accuracy 0.88427734375\n",
      "Iteration 46450 Training loss 0.003609316423535347 Validation loss 0.010648617520928383 Accuracy 0.8857421875\n",
      "Iteration 46460 Training loss 0.0028879616875201464 Validation loss 0.010578102432191372 Accuracy 0.88671875\n",
      "Iteration 46470 Training loss 0.0021162747871130705 Validation loss 0.010565186850726604 Accuracy 0.88623046875\n",
      "Iteration 46480 Training loss 0.002656711498275399 Validation loss 0.010519037023186684 Accuracy 0.88720703125\n",
      "Iteration 46490 Training loss 0.0029971497133374214 Validation loss 0.010586102493107319 Accuracy 0.88623046875\n",
      "Iteration 46500 Training loss 0.003131358651444316 Validation loss 0.010579507797956467 Accuracy 0.88671875\n",
      "Iteration 46510 Training loss 0.0042274403385818005 Validation loss 0.010714779607951641 Accuracy 0.8857421875\n",
      "Iteration 46520 Training loss 0.003472070675343275 Validation loss 0.010704835876822472 Accuracy 0.8857421875\n",
      "Iteration 46530 Training loss 0.003802467370405793 Validation loss 0.01059689186513424 Accuracy 0.88623046875\n",
      "Iteration 46540 Training loss 0.002613379154354334 Validation loss 0.010789713822305202 Accuracy 0.88330078125\n",
      "Iteration 46550 Training loss 0.003062991425395012 Validation loss 0.0105734933167696 Accuracy 0.88623046875\n",
      "Iteration 46560 Training loss 0.003757453989237547 Validation loss 0.010542758740484715 Accuracy 0.88720703125\n",
      "Iteration 46570 Training loss 0.0032337941229343414 Validation loss 0.010488582774996758 Accuracy 0.88720703125\n",
      "Iteration 46580 Training loss 0.0018217687029391527 Validation loss 0.010810773819684982 Accuracy 0.8837890625\n",
      "Iteration 46590 Training loss 0.0035075212363153696 Validation loss 0.010631545446813107 Accuracy 0.8857421875\n",
      "Iteration 46600 Training loss 0.0024983463808894157 Validation loss 0.010663656517863274 Accuracy 0.884765625\n",
      "Iteration 46610 Training loss 0.002731866203248501 Validation loss 0.010554623790085316 Accuracy 0.88671875\n",
      "Iteration 46620 Training loss 0.003799266181886196 Validation loss 0.010565795935690403 Accuracy 0.8876953125\n",
      "Iteration 46630 Training loss 0.0024658117908984423 Validation loss 0.010551664978265762 Accuracy 0.88671875\n",
      "Iteration 46640 Training loss 0.002894399920478463 Validation loss 0.010612624697387218 Accuracy 0.88525390625\n",
      "Iteration 46650 Training loss 0.0013337335549294949 Validation loss 0.010593121871352196 Accuracy 0.8857421875\n",
      "Iteration 46660 Training loss 0.002382742241024971 Validation loss 0.010423600673675537 Accuracy 0.8876953125\n",
      "Iteration 46670 Training loss 0.0038594261277467012 Validation loss 0.010688337497413158 Accuracy 0.884765625\n",
      "Iteration 46680 Training loss 0.002317443024367094 Validation loss 0.01067106518894434 Accuracy 0.88427734375\n",
      "Iteration 46690 Training loss 0.003882434917613864 Validation loss 0.010683317668735981 Accuracy 0.8837890625\n",
      "Iteration 46700 Training loss 0.0043048863299191 Validation loss 0.010675163008272648 Accuracy 0.88525390625\n",
      "Iteration 46710 Training loss 0.0031312720384448767 Validation loss 0.010522919707000256 Accuracy 0.88671875\n",
      "Iteration 46720 Training loss 0.002996098715811968 Validation loss 0.010841473937034607 Accuracy 0.8828125\n",
      "Iteration 46730 Training loss 0.0028756256215274334 Validation loss 0.010534035041928291 Accuracy 0.8857421875\n",
      "Iteration 46740 Training loss 0.0036767113488167524 Validation loss 0.010586259886622429 Accuracy 0.88623046875\n",
      "Iteration 46750 Training loss 0.002315480262041092 Validation loss 0.010580104775726795 Accuracy 0.88720703125\n",
      "Iteration 46760 Training loss 0.0027745275292545557 Validation loss 0.010619941167533398 Accuracy 0.88623046875\n",
      "Iteration 46770 Training loss 0.0035703766625374556 Validation loss 0.010669920593500137 Accuracy 0.8857421875\n",
      "Iteration 46780 Training loss 0.0037643788382411003 Validation loss 0.010603436268866062 Accuracy 0.8857421875\n",
      "Iteration 46790 Training loss 0.002846835181117058 Validation loss 0.01080983504652977 Accuracy 0.88427734375\n",
      "Iteration 46800 Training loss 0.002615779871121049 Validation loss 0.010661994107067585 Accuracy 0.884765625\n",
      "Iteration 46810 Training loss 0.0029149604961276054 Validation loss 0.010542390868067741 Accuracy 0.8876953125\n",
      "Iteration 46820 Training loss 0.0026804995723068714 Validation loss 0.010503539815545082 Accuracy 0.88720703125\n",
      "Iteration 46830 Training loss 0.0026088058948516846 Validation loss 0.010755038820207119 Accuracy 0.8837890625\n",
      "Iteration 46840 Training loss 0.003157292725518346 Validation loss 0.010575531050562859 Accuracy 0.88623046875\n",
      "Iteration 46850 Training loss 0.0028719825204461813 Validation loss 0.010589104145765305 Accuracy 0.8857421875\n",
      "Iteration 46860 Training loss 0.002878683153539896 Validation loss 0.010614853352308273 Accuracy 0.8857421875\n",
      "Iteration 46870 Training loss 0.0020832130685448647 Validation loss 0.010475552640855312 Accuracy 0.88623046875\n",
      "Iteration 46880 Training loss 0.0036197120789438486 Validation loss 0.01058349572122097 Accuracy 0.8857421875\n",
      "Iteration 46890 Training loss 0.0013768603093922138 Validation loss 0.0106499707326293 Accuracy 0.88623046875\n",
      "Iteration 46900 Training loss 0.002905289875343442 Validation loss 0.010617121122777462 Accuracy 0.88623046875\n",
      "Iteration 46910 Training loss 0.0028535390738397837 Validation loss 0.010551905259490013 Accuracy 0.8857421875\n",
      "Iteration 46920 Training loss 0.003590567270293832 Validation loss 0.010659173130989075 Accuracy 0.884765625\n",
      "Iteration 46930 Training loss 0.0027361796237528324 Validation loss 0.010650619864463806 Accuracy 0.88671875\n",
      "Iteration 46940 Training loss 0.003247643820941448 Validation loss 0.010681340470910072 Accuracy 0.8857421875\n",
      "Iteration 46950 Training loss 0.002955281175673008 Validation loss 0.01042894460260868 Accuracy 0.888671875\n",
      "Iteration 46960 Training loss 0.002515546977519989 Validation loss 0.010682700201869011 Accuracy 0.884765625\n",
      "Iteration 46970 Training loss 0.001831109868362546 Validation loss 0.010723095387220383 Accuracy 0.884765625\n",
      "Iteration 46980 Training loss 0.0017731329426169395 Validation loss 0.010595527477562428 Accuracy 0.88623046875\n",
      "Iteration 46990 Training loss 0.004488333128392696 Validation loss 0.010590575635433197 Accuracy 0.88671875\n",
      "Iteration 47000 Training loss 0.00236231810413301 Validation loss 0.01051232311874628 Accuracy 0.88720703125\n",
      "Iteration 47010 Training loss 0.003586034057661891 Validation loss 0.010589852929115295 Accuracy 0.88671875\n",
      "Iteration 47020 Training loss 0.0021386388689279556 Validation loss 0.010585525073111057 Accuracy 0.88623046875\n",
      "Iteration 47030 Training loss 0.0034250481985509396 Validation loss 0.010755037888884544 Accuracy 0.88427734375\n",
      "Iteration 47040 Training loss 0.0013707205653190613 Validation loss 0.010588874109089375 Accuracy 0.88623046875\n",
      "Iteration 47050 Training loss 0.0015000645071268082 Validation loss 0.010700041428208351 Accuracy 0.884765625\n",
      "Iteration 47060 Training loss 0.0023743135388940573 Validation loss 0.010746713727712631 Accuracy 0.88427734375\n",
      "Iteration 47070 Training loss 0.003798243124037981 Validation loss 0.010601615533232689 Accuracy 0.88623046875\n",
      "Iteration 47080 Training loss 0.0027732616290450096 Validation loss 0.01054719090461731 Accuracy 0.88671875\n",
      "Iteration 47090 Training loss 0.0027381298132240772 Validation loss 0.010531044565141201 Accuracy 0.88671875\n",
      "Iteration 47100 Training loss 0.004075719974935055 Validation loss 0.010498714633286 Accuracy 0.88671875\n",
      "Iteration 47110 Training loss 0.003529016859829426 Validation loss 0.010502568446099758 Accuracy 0.88671875\n",
      "Iteration 47120 Training loss 0.003248011227697134 Validation loss 0.010609200224280357 Accuracy 0.8857421875\n",
      "Iteration 47130 Training loss 0.003752522636204958 Validation loss 0.010593860410153866 Accuracy 0.8857421875\n",
      "Iteration 47140 Training loss 0.0018243941012769938 Validation loss 0.010632709600031376 Accuracy 0.88525390625\n",
      "Iteration 47150 Training loss 0.0036640542093664408 Validation loss 0.010595622472465038 Accuracy 0.8857421875\n",
      "Iteration 47160 Training loss 0.003060293849557638 Validation loss 0.010888908058404922 Accuracy 0.8818359375\n",
      "Iteration 47170 Training loss 0.0031003712210804224 Validation loss 0.010546342469751835 Accuracy 0.88623046875\n",
      "Iteration 47180 Training loss 0.003332492895424366 Validation loss 0.010677228681743145 Accuracy 0.884765625\n",
      "Iteration 47190 Training loss 0.0017948998138308525 Validation loss 0.010610253550112247 Accuracy 0.884765625\n",
      "Iteration 47200 Training loss 0.004502652212977409 Validation loss 0.01062515377998352 Accuracy 0.8857421875\n",
      "Iteration 47210 Training loss 0.0024888371117413044 Validation loss 0.01060250960290432 Accuracy 0.8857421875\n",
      "Iteration 47220 Training loss 0.0029907021671533585 Validation loss 0.010545581579208374 Accuracy 0.88671875\n",
      "Iteration 47230 Training loss 0.0023055949714034796 Validation loss 0.010658207349479198 Accuracy 0.88525390625\n",
      "Iteration 47240 Training loss 0.003221237799152732 Validation loss 0.010485460981726646 Accuracy 0.88720703125\n",
      "Iteration 47250 Training loss 0.0032185057643800974 Validation loss 0.01064950879663229 Accuracy 0.8857421875\n",
      "Iteration 47260 Training loss 0.002472875639796257 Validation loss 0.010611738078296185 Accuracy 0.8857421875\n",
      "Iteration 47270 Training loss 0.002640959108248353 Validation loss 0.010513263754546642 Accuracy 0.88671875\n",
      "Iteration 47280 Training loss 0.0029321624897420406 Validation loss 0.010711041279137135 Accuracy 0.884765625\n",
      "Iteration 47290 Training loss 0.0033390820026397705 Validation loss 0.010569443926215172 Accuracy 0.88525390625\n",
      "Iteration 47300 Training loss 0.0031094455625861883 Validation loss 0.010713593102991581 Accuracy 0.88427734375\n",
      "Iteration 47310 Training loss 0.0033776997588574886 Validation loss 0.010710449889302254 Accuracy 0.8857421875\n",
      "Iteration 47320 Training loss 0.003083058400079608 Validation loss 0.010452842339873314 Accuracy 0.8876953125\n",
      "Iteration 47330 Training loss 0.002769644372165203 Validation loss 0.01048262882977724 Accuracy 0.8876953125\n",
      "Iteration 47340 Training loss 0.0022486152593046427 Validation loss 0.010519656352698803 Accuracy 0.88671875\n",
      "Iteration 47350 Training loss 0.0017588261980563402 Validation loss 0.010540680028498173 Accuracy 0.88671875\n",
      "Iteration 47360 Training loss 0.0038271681405603886 Validation loss 0.010619438253343105 Accuracy 0.88623046875\n",
      "Iteration 47370 Training loss 0.002448985818773508 Validation loss 0.010497893206775188 Accuracy 0.88720703125\n",
      "Iteration 47380 Training loss 0.0038184309378266335 Validation loss 0.010568750090897083 Accuracy 0.88720703125\n",
      "Iteration 47390 Training loss 0.002456821035593748 Validation loss 0.010746294632554054 Accuracy 0.88427734375\n",
      "Iteration 47400 Training loss 0.0025982793886214495 Validation loss 0.010677553713321686 Accuracy 0.8857421875\n",
      "Iteration 47410 Training loss 0.0026207375340163708 Validation loss 0.010474154725670815 Accuracy 0.88671875\n",
      "Iteration 47420 Training loss 0.0028007959481328726 Validation loss 0.010634618811309338 Accuracy 0.8857421875\n",
      "Iteration 47430 Training loss 0.0025160349905490875 Validation loss 0.010556429624557495 Accuracy 0.88623046875\n",
      "Iteration 47440 Training loss 0.002462879056110978 Validation loss 0.010707961395382881 Accuracy 0.884765625\n",
      "Iteration 47450 Training loss 0.0015315004857257009 Validation loss 0.010594573803246021 Accuracy 0.88525390625\n",
      "Iteration 47460 Training loss 0.00270366040058434 Validation loss 0.010603837668895721 Accuracy 0.88525390625\n",
      "Iteration 47470 Training loss 0.0030940035358071327 Validation loss 0.010722550563514233 Accuracy 0.88427734375\n",
      "Iteration 47480 Training loss 0.002140115248039365 Validation loss 0.010616361163556576 Accuracy 0.88720703125\n",
      "Iteration 47490 Training loss 0.003799125086516142 Validation loss 0.010561731643974781 Accuracy 0.88671875\n",
      "Iteration 47500 Training loss 0.0028730311896651983 Validation loss 0.010595292784273624 Accuracy 0.88623046875\n",
      "Iteration 47510 Training loss 0.002498872112482786 Validation loss 0.010573217645287514 Accuracy 0.88525390625\n",
      "Iteration 47520 Training loss 0.0020866079721599817 Validation loss 0.010517743416130543 Accuracy 0.88623046875\n",
      "Iteration 47530 Training loss 0.0020903090480715036 Validation loss 0.01045914739370346 Accuracy 0.88671875\n",
      "Iteration 47540 Training loss 0.0017710143001750112 Validation loss 0.01078063901513815 Accuracy 0.8837890625\n",
      "Iteration 47550 Training loss 0.0036219602916389704 Validation loss 0.010706649161875248 Accuracy 0.88427734375\n",
      "Iteration 47560 Training loss 0.0032976798247545958 Validation loss 0.010572842322289944 Accuracy 0.8857421875\n",
      "Iteration 47570 Training loss 0.003107996191829443 Validation loss 0.010650807060301304 Accuracy 0.884765625\n",
      "Iteration 47580 Training loss 0.0014343161601573229 Validation loss 0.010645254515111446 Accuracy 0.88671875\n",
      "Iteration 47590 Training loss 0.0031416791025549173 Validation loss 0.010672138072550297 Accuracy 0.8857421875\n",
      "Iteration 47600 Training loss 0.002444647252559662 Validation loss 0.01054073590785265 Accuracy 0.8876953125\n",
      "Iteration 47610 Training loss 0.0022924768272787333 Validation loss 0.010507592931389809 Accuracy 0.88818359375\n",
      "Iteration 47620 Training loss 0.003298684488981962 Validation loss 0.010686286725103855 Accuracy 0.88427734375\n",
      "Iteration 47630 Training loss 0.0024112656246870756 Validation loss 0.010650224052369595 Accuracy 0.884765625\n",
      "Iteration 47640 Training loss 0.0028595509938895702 Validation loss 0.010594514198601246 Accuracy 0.88671875\n",
      "Iteration 47650 Training loss 0.001999139552935958 Validation loss 0.010557188652455807 Accuracy 0.88720703125\n",
      "Iteration 47660 Training loss 0.003270356683060527 Validation loss 0.01060470100492239 Accuracy 0.88623046875\n",
      "Iteration 47670 Training loss 0.0035857383627444506 Validation loss 0.010805838741362095 Accuracy 0.8837890625\n",
      "Iteration 47680 Training loss 0.0020721061155200005 Validation loss 0.010614927858114243 Accuracy 0.88623046875\n",
      "Iteration 47690 Training loss 0.0036363492254167795 Validation loss 0.010633856989443302 Accuracy 0.88623046875\n",
      "Iteration 47700 Training loss 0.0035347016528248787 Validation loss 0.010666518472135067 Accuracy 0.884765625\n",
      "Iteration 47710 Training loss 0.002303775865584612 Validation loss 0.01067446917295456 Accuracy 0.884765625\n",
      "Iteration 47720 Training loss 0.0035427003167569637 Validation loss 0.010573158040642738 Accuracy 0.88623046875\n",
      "Iteration 47730 Training loss 0.003908445592969656 Validation loss 0.010683475993573666 Accuracy 0.8857421875\n",
      "Iteration 47740 Training loss 0.00277478015050292 Validation loss 0.010782185010612011 Accuracy 0.8828125\n",
      "Iteration 47750 Training loss 0.004121854901313782 Validation loss 0.010564633645117283 Accuracy 0.88623046875\n",
      "Iteration 47760 Training loss 0.0026317767333239317 Validation loss 0.010544904507696629 Accuracy 0.88720703125\n",
      "Iteration 47770 Training loss 0.002240363974124193 Validation loss 0.010538166388869286 Accuracy 0.88623046875\n",
      "Iteration 47780 Training loss 0.0032219234853982925 Validation loss 0.010631591081619263 Accuracy 0.88427734375\n",
      "Iteration 47790 Training loss 0.0037711579352617264 Validation loss 0.01063007302582264 Accuracy 0.88525390625\n",
      "Iteration 47800 Training loss 0.003347602905705571 Validation loss 0.010598713532090187 Accuracy 0.88623046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1076cb410>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47810 Training loss 0.002809042576700449 Validation loss 0.010421793907880783 Accuracy 0.88818359375\n",
      "Iteration 47820 Training loss 0.00301810703240335 Validation loss 0.010476169176399708 Accuracy 0.8876953125\n",
      "Iteration 47830 Training loss 0.002794896485283971 Validation loss 0.010643944144248962 Accuracy 0.8857421875\n",
      "Iteration 47840 Training loss 0.0017396865878254175 Validation loss 0.010600919835269451 Accuracy 0.8857421875\n",
      "Iteration 47850 Training loss 0.0034045397769659758 Validation loss 0.01056494377553463 Accuracy 0.88623046875\n",
      "Iteration 47860 Training loss 0.002637468511238694 Validation loss 0.010590159334242344 Accuracy 0.8857421875\n",
      "Iteration 47870 Training loss 0.002878663595765829 Validation loss 0.010597501881420612 Accuracy 0.88525390625\n",
      "Iteration 47880 Training loss 0.0023843084927648306 Validation loss 0.01059157308191061 Accuracy 0.88623046875\n",
      "Iteration 47890 Training loss 0.0028588958084583282 Validation loss 0.010633093304932117 Accuracy 0.88525390625\n",
      "Iteration 47900 Training loss 0.0021616281010210514 Validation loss 0.010568547993898392 Accuracy 0.88671875\n",
      "Iteration 47910 Training loss 0.002831535879522562 Validation loss 0.010472903959453106 Accuracy 0.88671875\n",
      "Iteration 47920 Training loss 0.0021815267391502857 Validation loss 0.010675854980945587 Accuracy 0.8857421875\n",
      "Iteration 47930 Training loss 0.0013649514876306057 Validation loss 0.010603578761219978 Accuracy 0.88671875\n",
      "Iteration 47940 Training loss 0.002471331274136901 Validation loss 0.010615314356982708 Accuracy 0.88623046875\n",
      "Iteration 47950 Training loss 0.0028619030490517616 Validation loss 0.01047096773982048 Accuracy 0.88818359375\n",
      "Iteration 47960 Training loss 0.0030491482466459274 Validation loss 0.01056408416479826 Accuracy 0.88671875\n",
      "Iteration 47970 Training loss 0.004335252568125725 Validation loss 0.010694711469113827 Accuracy 0.88623046875\n",
      "Iteration 47980 Training loss 0.0032203339505940676 Validation loss 0.010610492900013924 Accuracy 0.88623046875\n",
      "Iteration 47990 Training loss 0.002455121371895075 Validation loss 0.010585837066173553 Accuracy 0.8876953125\n",
      "Iteration 48000 Training loss 0.00315055251121521 Validation loss 0.010545789264142513 Accuracy 0.88720703125\n",
      "Iteration 48010 Training loss 0.002479559276252985 Validation loss 0.010605673305690289 Accuracy 0.8857421875\n",
      "Iteration 48020 Training loss 0.0035923710092902184 Validation loss 0.010660293512046337 Accuracy 0.88525390625\n",
      "Iteration 48030 Training loss 0.0028647335711866617 Validation loss 0.010672379285097122 Accuracy 0.8857421875\n",
      "Iteration 48040 Training loss 0.0032703338656574488 Validation loss 0.010683786123991013 Accuracy 0.88525390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1076cb410>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48050 Training loss 0.002422309946268797 Validation loss 0.010584050789475441 Accuracy 0.88671875\n",
      "Iteration 48060 Training loss 0.0025670321192592382 Validation loss 0.010744591243565083 Accuracy 0.884765625\n",
      "Iteration 48070 Training loss 0.0024934944231063128 Validation loss 0.010549862869083881 Accuracy 0.88818359375\n",
      "Iteration 48080 Training loss 0.004410143010318279 Validation loss 0.010523930191993713 Accuracy 0.88671875\n",
      "Iteration 48090 Training loss 0.0028228263836354017 Validation loss 0.010511358268558979 Accuracy 0.88720703125\n",
      "Iteration 48100 Training loss 0.0037939916364848614 Validation loss 0.010562456212937832 Accuracy 0.88623046875\n",
      "Iteration 48110 Training loss 0.0027133130934089422 Validation loss 0.01059702504426241 Accuracy 0.88623046875\n",
      "Iteration 48120 Training loss 0.002325055655092001 Validation loss 0.010800277814269066 Accuracy 0.88330078125\n",
      "Iteration 48130 Training loss 0.002635604003444314 Validation loss 0.010525689460337162 Accuracy 0.88623046875\n",
      "Iteration 48140 Training loss 0.0025779304560273886 Validation loss 0.010506141930818558 Accuracy 0.88720703125\n",
      "Iteration 48150 Training loss 0.001963868038728833 Validation loss 0.010509156621992588 Accuracy 0.88671875\n",
      "Iteration 48160 Training loss 0.0024368518497794867 Validation loss 0.0105843311175704 Accuracy 0.88623046875\n",
      "Iteration 48170 Training loss 0.003071711864322424 Validation loss 0.010729309171438217 Accuracy 0.88525390625\n",
      "Iteration 48180 Training loss 0.004528329707682133 Validation loss 0.010874398984014988 Accuracy 0.8837890625\n",
      "Iteration 48190 Training loss 0.001718376181088388 Validation loss 0.010500144213438034 Accuracy 0.88818359375\n",
      "Iteration 48200 Training loss 0.0025741676799952984 Validation loss 0.010527810081839561 Accuracy 0.88671875\n",
      "Iteration 48210 Training loss 0.0018373536877334118 Validation loss 0.010510922409594059 Accuracy 0.88818359375\n",
      "Iteration 48220 Training loss 0.002974704373627901 Validation loss 0.01090067345649004 Accuracy 0.88330078125\n",
      "Iteration 48230 Training loss 0.002166483551263809 Validation loss 0.010674023069441319 Accuracy 0.88720703125\n",
      "Iteration 48240 Training loss 0.003933171276003122 Validation loss 0.010626537725329399 Accuracy 0.8857421875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_3_trained_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 258\u001b[39m, in \u001b[36mthree_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, reg1, reg2, reg3, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2, train_layer_3)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\u001b[39;00m\n\u001b[32m    255\u001b[39m \n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Calcul des gradients\u001b[39;00m\n\u001b[32m    257\u001b[39m grad_output = (output - y_minibatch).to(dtype)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m grad_z3 = (torch.einsum(\u001b[33m'\u001b[39m\u001b[33mno,noz->nz\u001b[39m\u001b[33m'\u001b[39m,grad_output,\u001b[43msoftmax_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m)).to(dtype) \u001b[38;5;66;03m# shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\u001b[39;00m\n\u001b[32m    259\u001b[39m grad_h2 = (torch.mm(grad_z3, \u001b[38;5;28mself\u001b[39m.W3)).to(dtype) \u001b[38;5;66;03m# shape (n_data, hidden_2_size)\u001b[39;00m\n\u001b[32m    260\u001b[39m grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) \u001b[38;5;66;03m# shape(n_data, hidden_2_size)         \u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36msoftmax_derivative\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     22\u001b[39m jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) \u001b[38;5;66;03m# Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):  \u001b[38;5;66;03m# Pour chaque échantillon du batch, on calcule la jacobienne de softmax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     si = \u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\u001b[39;00m\n\u001b[32m     25\u001b[39m     jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) \u001b[38;5;66;03m# calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jacobians\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 2048, 2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 61465600.0 and the number of iterations is 102442.\n",
      "Iteration 0 Training loss 0.08693061023950577 Validation loss 0.08905059099197388 Accuracy 0.09869384765625\n",
      "Iteration 10 Training loss 0.07243277132511139 Validation loss 0.07280346751213074 Accuracy 0.263916015625\n",
      "Iteration 20 Training loss 0.06815800070762634 Validation loss 0.06837593764066696 Accuracy 0.3037109375\n",
      "Iteration 30 Training loss 0.06052234768867493 Validation loss 0.060783710330724716 Accuracy 0.38134765625\n",
      "Iteration 40 Training loss 0.06240614876151085 Validation loss 0.06062590703368187 Accuracy 0.385986328125\n",
      "Iteration 50 Training loss 0.060119058936834335 Validation loss 0.05919632688164711 Accuracy 0.401123046875\n",
      "Iteration 60 Training loss 0.05658617615699768 Validation loss 0.05759459733963013 Accuracy 0.416748046875\n",
      "Iteration 70 Training loss 0.05582404509186745 Validation loss 0.05643774941563606 Accuracy 0.4296875\n",
      "Iteration 80 Training loss 0.056503817439079285 Validation loss 0.0576634556055069 Accuracy 0.41796875\n",
      "Iteration 90 Training loss 0.059966713190078735 Validation loss 0.05684490129351616 Accuracy 0.42529296875\n",
      "Iteration 100 Training loss 0.053409114480018616 Validation loss 0.05621764808893204 Accuracy 0.431884765625\n",
      "Iteration 110 Training loss 0.058583877980709076 Validation loss 0.05686316266655922 Accuracy 0.42626953125\n",
      "Iteration 120 Training loss 0.0538860522210598 Validation loss 0.05553059279918671 Accuracy 0.44091796875\n",
      "Iteration 130 Training loss 0.05250126123428345 Validation loss 0.055292341858148575 Accuracy 0.443115234375\n",
      "Iteration 140 Training loss 0.0498942956328392 Validation loss 0.056042224168777466 Accuracy 0.435791015625\n",
      "Iteration 150 Training loss 0.06009785830974579 Validation loss 0.05817083269357681 Accuracy 0.413818359375\n",
      "Iteration 160 Training loss 0.05827498435974121 Validation loss 0.05525842681527138 Accuracy 0.44287109375\n",
      "Iteration 170 Training loss 0.05158396437764168 Validation loss 0.054873041808605194 Accuracy 0.447509765625\n",
      "Iteration 180 Training loss 0.055982839316129684 Validation loss 0.055404528975486755 Accuracy 0.4423828125\n",
      "Iteration 190 Training loss 0.05697507783770561 Validation loss 0.05709540471434593 Accuracy 0.425048828125\n",
      "Iteration 200 Training loss 0.05691593512892723 Validation loss 0.05580965057015419 Accuracy 0.43701171875\n",
      "Iteration 210 Training loss 0.05833029747009277 Validation loss 0.05583688244223595 Accuracy 0.4375\n",
      "Iteration 220 Training loss 0.0544530525803566 Validation loss 0.05454283207654953 Accuracy 0.451171875\n",
      "Iteration 230 Training loss 0.06067050248384476 Validation loss 0.05502963811159134 Accuracy 0.446533203125\n",
      "Iteration 240 Training loss 0.058373261243104935 Validation loss 0.0550675168633461 Accuracy 0.446044921875\n",
      "Iteration 250 Training loss 0.05656743422150612 Validation loss 0.0545990988612175 Accuracy 0.45068359375\n",
      "Iteration 260 Training loss 0.05109228566288948 Validation loss 0.054730214178562164 Accuracy 0.448974609375\n",
      "Iteration 270 Training loss 0.05546232685446739 Validation loss 0.05473393574357033 Accuracy 0.44873046875\n",
      "Iteration 280 Training loss 0.05526656657457352 Validation loss 0.054304640740156174 Accuracy 0.453857421875\n",
      "Iteration 290 Training loss 0.053454793989658356 Validation loss 0.0544632226228714 Accuracy 0.451904296875\n",
      "Iteration 300 Training loss 0.05622014403343201 Validation loss 0.054511576890945435 Accuracy 0.45166015625\n",
      "Iteration 310 Training loss 0.05247711017727852 Validation loss 0.05410699546337128 Accuracy 0.45556640625\n",
      "Iteration 320 Training loss 0.0526147335767746 Validation loss 0.05486622080206871 Accuracy 0.447265625\n",
      "Iteration 330 Training loss 0.05494577810168266 Validation loss 0.05457806587219238 Accuracy 0.450439453125\n",
      "Iteration 340 Training loss 0.052658990025520325 Validation loss 0.05404695123434067 Accuracy 0.456298828125\n",
      "Iteration 350 Training loss 0.05673521012067795 Validation loss 0.05457524210214615 Accuracy 0.45068359375\n",
      "Iteration 360 Training loss 0.05298585072159767 Validation loss 0.05413426086306572 Accuracy 0.45556640625\n",
      "Iteration 370 Training loss 0.05472500994801521 Validation loss 0.05494847521185875 Accuracy 0.44775390625\n",
      "Iteration 380 Training loss 0.05167557671666145 Validation loss 0.0547153502702713 Accuracy 0.44873046875\n",
      "Iteration 390 Training loss 0.051578227430582047 Validation loss 0.05411119386553764 Accuracy 0.456298828125\n",
      "Iteration 400 Training loss 0.05117011442780495 Validation loss 0.05407160893082619 Accuracy 0.456298828125\n",
      "Iteration 410 Training loss 0.050917740911245346 Validation loss 0.05405263602733612 Accuracy 0.45556640625\n",
      "Iteration 420 Training loss 0.05084087699651718 Validation loss 0.05398155003786087 Accuracy 0.456787109375\n",
      "Iteration 430 Training loss 0.05290808901190758 Validation loss 0.05391565337777138 Accuracy 0.457763671875\n",
      "Iteration 440 Training loss 0.05054829269647598 Validation loss 0.054632239043712616 Accuracy 0.45068359375\n",
      "Iteration 450 Training loss 0.05592307448387146 Validation loss 0.05398624762892723 Accuracy 0.45703125\n",
      "Iteration 460 Training loss 0.055096983909606934 Validation loss 0.05397531017661095 Accuracy 0.457275390625\n",
      "Iteration 470 Training loss 0.055507197976112366 Validation loss 0.053988873958587646 Accuracy 0.457275390625\n",
      "Iteration 480 Training loss 0.052996695041656494 Validation loss 0.053988564759492874 Accuracy 0.457275390625\n",
      "Iteration 490 Training loss 0.050506122410297394 Validation loss 0.05387527123093605 Accuracy 0.45849609375\n",
      "Iteration 500 Training loss 0.053853053599596024 Validation loss 0.05397378280758858 Accuracy 0.45751953125\n",
      "Iteration 510 Training loss 0.052962299436330795 Validation loss 0.05432713404297829 Accuracy 0.45263671875\n",
      "Iteration 520 Training loss 0.051921263337135315 Validation loss 0.0548534169793129 Accuracy 0.447265625\n",
      "Iteration 530 Training loss 0.052524931728839874 Validation loss 0.053918007761240005 Accuracy 0.457763671875\n",
      "Iteration 540 Training loss 0.05279041454195976 Validation loss 0.05384373292326927 Accuracy 0.458984375\n",
      "Iteration 550 Training loss 0.0499335378408432 Validation loss 0.053831882774829865 Accuracy 0.458740234375\n",
      "Iteration 560 Training loss 0.05325216054916382 Validation loss 0.05387553200125694 Accuracy 0.45849609375\n",
      "Iteration 570 Training loss 0.05372810736298561 Validation loss 0.053921736776828766 Accuracy 0.45751953125\n",
      "Iteration 580 Training loss 0.0551631823182106 Validation loss 0.054104939103126526 Accuracy 0.455322265625\n",
      "Iteration 590 Training loss 0.055950891226530075 Validation loss 0.05423346534371376 Accuracy 0.454833984375\n",
      "Iteration 600 Training loss 0.054677464067935944 Validation loss 0.05430424585938454 Accuracy 0.45458984375\n",
      "Iteration 610 Training loss 0.05309208109974861 Validation loss 0.05381959676742554 Accuracy 0.45947265625\n",
      "Iteration 620 Training loss 0.05344366282224655 Validation loss 0.05386022478342056 Accuracy 0.45849609375\n",
      "Iteration 630 Training loss 0.051185015588998795 Validation loss 0.05378280207514763 Accuracy 0.459716796875\n",
      "Iteration 640 Training loss 0.05096527189016342 Validation loss 0.05371546745300293 Accuracy 0.460693359375\n",
      "Iteration 650 Training loss 0.05222542956471443 Validation loss 0.05377574637532234 Accuracy 0.459716796875\n",
      "Iteration 660 Training loss 0.05558488890528679 Validation loss 0.0551762618124485 Accuracy 0.445556640625\n",
      "Iteration 670 Training loss 0.054726626724004745 Validation loss 0.053896982222795486 Accuracy 0.458251953125\n",
      "Iteration 680 Training loss 0.053066495805978775 Validation loss 0.053974300622940063 Accuracy 0.457275390625\n",
      "Iteration 690 Training loss 0.05231162905693054 Validation loss 0.053699880838394165 Accuracy 0.460693359375\n",
      "Iteration 700 Training loss 0.056362587958574295 Validation loss 0.05370699614286423 Accuracy 0.4599609375\n",
      "Iteration 710 Training loss 0.05500272661447525 Validation loss 0.05397191271185875 Accuracy 0.45654296875\n",
      "Iteration 720 Training loss 0.05259206146001816 Validation loss 0.053785741329193115 Accuracy 0.458984375\n",
      "Iteration 730 Training loss 0.05477077141404152 Validation loss 0.05386708304286003 Accuracy 0.4580078125\n",
      "Iteration 740 Training loss 0.05313830450177193 Validation loss 0.05382956936955452 Accuracy 0.459228515625\n",
      "Iteration 750 Training loss 0.0533277802169323 Validation loss 0.0536050871014595 Accuracy 0.461181640625\n",
      "Iteration 760 Training loss 0.05074014887213707 Validation loss 0.05376822128891945 Accuracy 0.459228515625\n",
      "Iteration 770 Training loss 0.052626676857471466 Validation loss 0.053548358380794525 Accuracy 0.4619140625\n",
      "Iteration 780 Training loss 0.052173320204019547 Validation loss 0.05377965793013573 Accuracy 0.45947265625\n",
      "Iteration 790 Training loss 0.05101776123046875 Validation loss 0.053557801991701126 Accuracy 0.462158203125\n",
      "Iteration 800 Training loss 0.052854858338832855 Validation loss 0.05363960191607475 Accuracy 0.4609375\n",
      "Iteration 810 Training loss 0.05658920109272003 Validation loss 0.05441320315003395 Accuracy 0.45166015625\n",
      "Iteration 820 Training loss 0.055876150727272034 Validation loss 0.0535455159842968 Accuracy 0.4619140625\n",
      "Iteration 830 Training loss 0.05382890626788139 Validation loss 0.05374075099825859 Accuracy 0.460205078125\n",
      "Iteration 840 Training loss 0.05367175117135048 Validation loss 0.05374020338058472 Accuracy 0.460205078125\n",
      "Iteration 850 Training loss 0.05438601225614548 Validation loss 0.054212331771850586 Accuracy 0.45458984375\n",
      "Iteration 860 Training loss 0.05223210155963898 Validation loss 0.05421711131930351 Accuracy 0.455078125\n",
      "Iteration 870 Training loss 0.051996368914842606 Validation loss 0.05350327119231224 Accuracy 0.46240234375\n",
      "Iteration 880 Training loss 0.05073590576648712 Validation loss 0.053641971200704575 Accuracy 0.460693359375\n",
      "Iteration 890 Training loss 0.052894193679094315 Validation loss 0.0535280741751194 Accuracy 0.461669921875\n",
      "Iteration 900 Training loss 0.055806949734687805 Validation loss 0.053734783083200455 Accuracy 0.459716796875\n",
      "Iteration 910 Training loss 0.052203238010406494 Validation loss 0.05370289087295532 Accuracy 0.46044921875\n",
      "Iteration 920 Training loss 0.05096140503883362 Validation loss 0.053657617419958115 Accuracy 0.460205078125\n",
      "Iteration 930 Training loss 0.04899430274963379 Validation loss 0.053563982248306274 Accuracy 0.461181640625\n",
      "Iteration 940 Training loss 0.05342995747923851 Validation loss 0.053442440927028656 Accuracy 0.46337890625\n",
      "Iteration 950 Training loss 0.05273183807730675 Validation loss 0.05375872924923897 Accuracy 0.459716796875\n",
      "Iteration 960 Training loss 0.049661800265312195 Validation loss 0.053572479635477066 Accuracy 0.461669921875\n",
      "Iteration 970 Training loss 0.05350474640727043 Validation loss 0.0535787008702755 Accuracy 0.46240234375\n",
      "Iteration 980 Training loss 0.04875599220395088 Validation loss 0.05411604419350624 Accuracy 0.4560546875\n",
      "Iteration 990 Training loss 0.05154300481081009 Validation loss 0.05373533070087433 Accuracy 0.4599609375\n",
      "Iteration 1000 Training loss 0.05031268298625946 Validation loss 0.053454212844371796 Accuracy 0.463134765625\n",
      "Iteration 1010 Training loss 0.055864978581666946 Validation loss 0.05363552272319794 Accuracy 0.4609375\n",
      "Iteration 1020 Training loss 0.052962858229875565 Validation loss 0.05343040078878403 Accuracy 0.462646484375\n",
      "Iteration 1030 Training loss 0.05131099373102188 Validation loss 0.053575195372104645 Accuracy 0.461181640625\n",
      "Iteration 1040 Training loss 0.0558355338871479 Validation loss 0.0535716786980629 Accuracy 0.4619140625\n",
      "Iteration 1050 Training loss 0.05653030425310135 Validation loss 0.05489178001880646 Accuracy 0.44775390625\n",
      "Iteration 1060 Training loss 0.05476759746670723 Validation loss 0.05353868007659912 Accuracy 0.46240234375\n",
      "Iteration 1070 Training loss 0.05364881455898285 Validation loss 0.05340338870882988 Accuracy 0.463623046875\n",
      "Iteration 1080 Training loss 0.052644286304712296 Validation loss 0.05346119403839111 Accuracy 0.462646484375\n",
      "Iteration 1090 Training loss 0.04882316663861275 Validation loss 0.05343731492757797 Accuracy 0.462890625\n",
      "Iteration 1100 Training loss 0.052519917488098145 Validation loss 0.05419600009918213 Accuracy 0.454345703125\n",
      "Iteration 1110 Training loss 0.05194628983736038 Validation loss 0.05346310883760452 Accuracy 0.46337890625\n",
      "Iteration 1120 Training loss 0.05184032768011093 Validation loss 0.05332959443330765 Accuracy 0.4638671875\n",
      "Iteration 1130 Training loss 0.05443938821554184 Validation loss 0.05364184454083443 Accuracy 0.460693359375\n",
      "Iteration 1140 Training loss 0.05032498762011528 Validation loss 0.053283773362636566 Accuracy 0.46484375\n",
      "Iteration 1150 Training loss 0.05480281636118889 Validation loss 0.05326021462678909 Accuracy 0.464599609375\n",
      "Iteration 1160 Training loss 0.050774846225976944 Validation loss 0.05411909148097038 Accuracy 0.455078125\n",
      "Iteration 1170 Training loss 0.05363748222589493 Validation loss 0.05511629581451416 Accuracy 0.4453125\n",
      "Iteration 1180 Training loss 0.05407595634460449 Validation loss 0.05349330976605415 Accuracy 0.462158203125\n",
      "Iteration 1190 Training loss 0.05418793857097626 Validation loss 0.05353284999728203 Accuracy 0.4619140625\n",
      "Iteration 1200 Training loss 0.05521729588508606 Validation loss 0.053357772529125214 Accuracy 0.463623046875\n",
      "Iteration 1210 Training loss 0.05437047779560089 Validation loss 0.053513672202825546 Accuracy 0.462890625\n",
      "Iteration 1220 Training loss 0.05091986060142517 Validation loss 0.053429730236530304 Accuracy 0.463134765625\n",
      "Iteration 1230 Training loss 0.05006909742951393 Validation loss 0.05369028076529503 Accuracy 0.46044921875\n",
      "Iteration 1240 Training loss 0.05359295755624771 Validation loss 0.05349905788898468 Accuracy 0.462158203125\n",
      "Iteration 1250 Training loss 0.05297573283314705 Validation loss 0.0535171777009964 Accuracy 0.462890625\n",
      "Iteration 1260 Training loss 0.05211371183395386 Validation loss 0.053381580859422684 Accuracy 0.463623046875\n",
      "Iteration 1270 Training loss 0.053700681775808334 Validation loss 0.053279392421245575 Accuracy 0.46435546875\n",
      "Iteration 1280 Training loss 0.05220352113246918 Validation loss 0.05421395227313042 Accuracy 0.455810546875\n",
      "Iteration 1290 Training loss 0.052521537989377975 Validation loss 0.05380845442414284 Accuracy 0.4599609375\n",
      "Iteration 1300 Training loss 0.052468717098236084 Validation loss 0.05334200710058212 Accuracy 0.46435546875\n",
      "Iteration 1310 Training loss 0.05215200409293175 Validation loss 0.053384773433208466 Accuracy 0.46337890625\n",
      "Iteration 1320 Training loss 0.047127094119787216 Validation loss 0.05339737609028816 Accuracy 0.464111328125\n",
      "Iteration 1330 Training loss 0.052016764879226685 Validation loss 0.05317394435405731 Accuracy 0.466064453125\n",
      "Iteration 1340 Training loss 0.05654222145676613 Validation loss 0.053218308836221695 Accuracy 0.4658203125\n",
      "Iteration 1350 Training loss 0.053736306726932526 Validation loss 0.05352141335606575 Accuracy 0.461669921875\n",
      "Iteration 1360 Training loss 0.049713134765625 Validation loss 0.05324098467826843 Accuracy 0.465576171875\n",
      "Iteration 1370 Training loss 0.05496165528893471 Validation loss 0.053423456847667694 Accuracy 0.463623046875\n",
      "Iteration 1380 Training loss 0.05253519490361214 Validation loss 0.053187839686870575 Accuracy 0.4658203125\n",
      "Iteration 1390 Training loss 0.05371936038136482 Validation loss 0.05309148132801056 Accuracy 0.46630859375\n",
      "Iteration 1400 Training loss 0.05236075818538666 Validation loss 0.053208064287900925 Accuracy 0.465087890625\n",
      "Iteration 1410 Training loss 0.054786186665296555 Validation loss 0.053164709359407425 Accuracy 0.4658203125\n",
      "Iteration 1420 Training loss 0.05491850897669792 Validation loss 0.053360067307949066 Accuracy 0.463623046875\n",
      "Iteration 1430 Training loss 0.05113297328352928 Validation loss 0.053173527121543884 Accuracy 0.465576171875\n",
      "Iteration 1440 Training loss 0.04929886758327484 Validation loss 0.05331163480877876 Accuracy 0.464111328125\n",
      "Iteration 1450 Training loss 0.05324571579694748 Validation loss 0.053359486162662506 Accuracy 0.4638671875\n",
      "Iteration 1460 Training loss 0.05382264032959938 Validation loss 0.05320087820291519 Accuracy 0.46533203125\n",
      "Iteration 1470 Training loss 0.051842108368873596 Validation loss 0.05323244631290436 Accuracy 0.465087890625\n",
      "Iteration 1480 Training loss 0.05323713272809982 Validation loss 0.053327400237321854 Accuracy 0.4638671875\n",
      "Iteration 1490 Training loss 0.055599622428417206 Validation loss 0.05338985100388527 Accuracy 0.46337890625\n",
      "Iteration 1500 Training loss 0.053397756069898605 Validation loss 0.054104696959257126 Accuracy 0.456298828125\n",
      "Iteration 1510 Training loss 0.05320907011628151 Validation loss 0.05310570076107979 Accuracy 0.466552734375\n",
      "Iteration 1520 Training loss 0.05212901160120964 Validation loss 0.05300914868712425 Accuracy 0.46728515625\n",
      "Iteration 1530 Training loss 0.05403968691825867 Validation loss 0.053165849298238754 Accuracy 0.465576171875\n",
      "Iteration 1540 Training loss 0.050644438713788986 Validation loss 0.05310826003551483 Accuracy 0.466796875\n",
      "Iteration 1550 Training loss 0.053571075201034546 Validation loss 0.05324133485555649 Accuracy 0.46533203125\n",
      "Iteration 1560 Training loss 0.05275452136993408 Validation loss 0.053053244948387146 Accuracy 0.46728515625\n",
      "Iteration 1570 Training loss 0.05192498862743378 Validation loss 0.05322103202342987 Accuracy 0.4658203125\n",
      "Iteration 1580 Training loss 0.05574300140142441 Validation loss 0.053300827741622925 Accuracy 0.464599609375\n",
      "Iteration 1590 Training loss 0.0537271611392498 Validation loss 0.05342002213001251 Accuracy 0.463134765625\n",
      "Iteration 1600 Training loss 0.056760065257549286 Validation loss 0.05307011678814888 Accuracy 0.467041015625\n",
      "Iteration 1610 Training loss 0.0541197806596756 Validation loss 0.0531286895275116 Accuracy 0.46630859375\n",
      "Iteration 1620 Training loss 0.04976218566298485 Validation loss 0.05311259627342224 Accuracy 0.466552734375\n",
      "Iteration 1630 Training loss 0.05143704265356064 Validation loss 0.05330413952469826 Accuracy 0.46484375\n",
      "Iteration 1640 Training loss 0.05191725492477417 Validation loss 0.053045134991407394 Accuracy 0.466796875\n",
      "Iteration 1650 Training loss 0.05263630673289299 Validation loss 0.05304848402738571 Accuracy 0.466552734375\n",
      "Iteration 1660 Training loss 0.05458710342645645 Validation loss 0.05345477536320686 Accuracy 0.4619140625\n",
      "Iteration 1670 Training loss 0.0525493361055851 Validation loss 0.053173523396253586 Accuracy 0.465087890625\n",
      "Iteration 1680 Training loss 0.05693603307008743 Validation loss 0.05339100956916809 Accuracy 0.462890625\n",
      "Iteration 1690 Training loss 0.050503771752119064 Validation loss 0.05300864204764366 Accuracy 0.467529296875\n",
      "Iteration 1700 Training loss 0.05193237215280533 Validation loss 0.05317062512040138 Accuracy 0.466064453125\n",
      "Iteration 1710 Training loss 0.054779183119535446 Validation loss 0.05326175317168236 Accuracy 0.465087890625\n",
      "Iteration 1720 Training loss 0.04919891431927681 Validation loss 0.053214170038700104 Accuracy 0.465087890625\n",
      "Iteration 1730 Training loss 0.04864861071109772 Validation loss 0.053137220442295074 Accuracy 0.466064453125\n",
      "Iteration 1740 Training loss 0.052613768726587296 Validation loss 0.05313405022025108 Accuracy 0.465576171875\n",
      "Iteration 1750 Training loss 0.05209730938076973 Validation loss 0.05359858274459839 Accuracy 0.461181640625\n",
      "Iteration 1760 Training loss 0.05372549220919609 Validation loss 0.05293753370642662 Accuracy 0.468017578125\n",
      "Iteration 1770 Training loss 0.05354702100157738 Validation loss 0.05292501673102379 Accuracy 0.468017578125\n",
      "Iteration 1780 Training loss 0.052640847861766815 Validation loss 0.05365331843495369 Accuracy 0.4609375\n",
      "Iteration 1790 Training loss 0.05154237896203995 Validation loss 0.05314989015460014 Accuracy 0.4658203125\n",
      "Iteration 1800 Training loss 0.05387718603014946 Validation loss 0.05303326994180679 Accuracy 0.466796875\n",
      "Iteration 1810 Training loss 0.054308705031871796 Validation loss 0.05322990193963051 Accuracy 0.465087890625\n",
      "Iteration 1820 Training loss 0.05211489647626877 Validation loss 0.05293871834874153 Accuracy 0.4677734375\n",
      "Iteration 1830 Training loss 0.0501500740647316 Validation loss 0.05319118872284889 Accuracy 0.465087890625\n",
      "Iteration 1840 Training loss 0.05439825356006622 Validation loss 0.05388312786817551 Accuracy 0.458740234375\n",
      "Iteration 1850 Training loss 0.05025402829051018 Validation loss 0.053001902997493744 Accuracy 0.4677734375\n",
      "Iteration 1860 Training loss 0.05310758948326111 Validation loss 0.05304589867591858 Accuracy 0.467041015625\n",
      "Iteration 1870 Training loss 0.05312356352806091 Validation loss 0.05309832841157913 Accuracy 0.466796875\n",
      "Iteration 1880 Training loss 0.05289332568645477 Validation loss 0.05301005393266678 Accuracy 0.46728515625\n",
      "Iteration 1890 Training loss 0.05714830383658409 Validation loss 0.05283138528466225 Accuracy 0.46923828125\n",
      "Iteration 1900 Training loss 0.05270233750343323 Validation loss 0.05292070284485817 Accuracy 0.468017578125\n",
      "Iteration 1910 Training loss 0.05386411398649216 Validation loss 0.05302802473306656 Accuracy 0.46728515625\n",
      "Iteration 1920 Training loss 0.05371338874101639 Validation loss 0.05308268591761589 Accuracy 0.466552734375\n",
      "Iteration 1930 Training loss 0.05301028490066528 Validation loss 0.05307554453611374 Accuracy 0.466796875\n",
      "Iteration 1940 Training loss 0.05599121004343033 Validation loss 0.052976589649915695 Accuracy 0.468017578125\n",
      "Iteration 1950 Training loss 0.05524742603302002 Validation loss 0.053045906126499176 Accuracy 0.467041015625\n",
      "Iteration 1960 Training loss 0.051920048892498016 Validation loss 0.05299200117588043 Accuracy 0.467529296875\n",
      "Iteration 1970 Training loss 0.05181209370493889 Validation loss 0.05355814844369888 Accuracy 0.461669921875\n",
      "Iteration 1980 Training loss 0.050569720566272736 Validation loss 0.052855007350444794 Accuracy 0.4677734375\n",
      "Iteration 1990 Training loss 0.05535102263092995 Validation loss 0.053321126848459244 Accuracy 0.463623046875\n",
      "Iteration 2000 Training loss 0.0503520704805851 Validation loss 0.05361897498369217 Accuracy 0.461181640625\n",
      "Iteration 2010 Training loss 0.04884491488337517 Validation loss 0.052885595709085464 Accuracy 0.4677734375\n",
      "Iteration 2020 Training loss 0.05472506582736969 Validation loss 0.05302422121167183 Accuracy 0.466796875\n",
      "Iteration 2030 Training loss 0.048336248844861984 Validation loss 0.05276915431022644 Accuracy 0.46923828125\n",
      "Iteration 2040 Training loss 0.0521373488008976 Validation loss 0.05299295112490654 Accuracy 0.467529296875\n",
      "Iteration 2050 Training loss 0.05442359298467636 Validation loss 0.053040046244859695 Accuracy 0.466552734375\n",
      "Iteration 2060 Training loss 0.05142498388886452 Validation loss 0.053613051772117615 Accuracy 0.46044921875\n",
      "Iteration 2070 Training loss 0.05120427533984184 Validation loss 0.052829284220933914 Accuracy 0.469482421875\n",
      "Iteration 2080 Training loss 0.05466187000274658 Validation loss 0.05281522870063782 Accuracy 0.4697265625\n",
      "Iteration 2090 Training loss 0.051238030195236206 Validation loss 0.05276704952120781 Accuracy 0.469482421875\n",
      "Iteration 2100 Training loss 0.05115619674324989 Validation loss 0.05270756781101227 Accuracy 0.470458984375\n",
      "Iteration 2110 Training loss 0.05556734651327133 Validation loss 0.054198987782001495 Accuracy 0.453369140625\n",
      "Iteration 2120 Training loss 0.054558414965867996 Validation loss 0.05307566747069359 Accuracy 0.466064453125\n",
      "Iteration 2130 Training loss 0.04996097832918167 Validation loss 0.05292566120624542 Accuracy 0.468505859375\n",
      "Iteration 2140 Training loss 0.05234085023403168 Validation loss 0.05322657898068428 Accuracy 0.464599609375\n",
      "Iteration 2150 Training loss 0.052545104175806046 Validation loss 0.052971940487623215 Accuracy 0.467529296875\n",
      "Iteration 2160 Training loss 0.051670581102371216 Validation loss 0.05284029617905617 Accuracy 0.468017578125\n",
      "Iteration 2170 Training loss 0.05145609378814697 Validation loss 0.05295292288064957 Accuracy 0.468017578125\n",
      "Iteration 2180 Training loss 0.05410875380039215 Validation loss 0.05278826504945755 Accuracy 0.469970703125\n",
      "Iteration 2190 Training loss 0.05071915686130524 Validation loss 0.05291523411870003 Accuracy 0.467529296875\n",
      "Iteration 2200 Training loss 0.05266909673810005 Validation loss 0.053017839789390564 Accuracy 0.46728515625\n",
      "Iteration 2210 Training loss 0.05177959054708481 Validation loss 0.05282371863722801 Accuracy 0.46875\n",
      "Iteration 2220 Training loss 0.0529804565012455 Validation loss 0.052962109446525574 Accuracy 0.468017578125\n",
      "Iteration 2230 Training loss 0.0539340116083622 Validation loss 0.0534188374876976 Accuracy 0.46337890625\n",
      "Iteration 2240 Training loss 0.053347278386354446 Validation loss 0.052946776151657104 Accuracy 0.46826171875\n",
      "Iteration 2250 Training loss 0.050054147839546204 Validation loss 0.05286959931254387 Accuracy 0.468505859375\n",
      "Iteration 2260 Training loss 0.04977487027645111 Validation loss 0.05277156084775925 Accuracy 0.4697265625\n",
      "Iteration 2270 Training loss 0.04941040650010109 Validation loss 0.05308857932686806 Accuracy 0.46630859375\n",
      "Iteration 2280 Training loss 0.05584879592061043 Validation loss 0.05302691087126732 Accuracy 0.466552734375\n",
      "Iteration 2290 Training loss 0.05693580210208893 Validation loss 0.05341365188360214 Accuracy 0.46337890625\n",
      "Iteration 2300 Training loss 0.05027672275900841 Validation loss 0.053229883313179016 Accuracy 0.464599609375\n",
      "Iteration 2310 Training loss 0.05394311994314194 Validation loss 0.053238317370414734 Accuracy 0.46435546875\n",
      "Iteration 2320 Training loss 0.05441382899880409 Validation loss 0.05290563032031059 Accuracy 0.468017578125\n",
      "Iteration 2330 Training loss 0.04723268002271652 Validation loss 0.05278437212109566 Accuracy 0.469482421875\n",
      "Iteration 2340 Training loss 0.053403448313474655 Validation loss 0.05404968559741974 Accuracy 0.455810546875\n",
      "Iteration 2350 Training loss 0.05416332930326462 Validation loss 0.05317122861742973 Accuracy 0.46484375\n",
      "Iteration 2360 Training loss 0.05489720031619072 Validation loss 0.052935242652893066 Accuracy 0.46826171875\n",
      "Iteration 2370 Training loss 0.053966570645570755 Validation loss 0.05290241912007332 Accuracy 0.468505859375\n",
      "Iteration 2380 Training loss 0.051727294921875 Validation loss 0.05282239615917206 Accuracy 0.4697265625\n",
      "Iteration 2390 Training loss 0.05465095862746239 Validation loss 0.052857402712106705 Accuracy 0.468994140625\n",
      "Iteration 2400 Training loss 0.048768024891614914 Validation loss 0.05323223024606705 Accuracy 0.46484375\n",
      "Iteration 2410 Training loss 0.052262745797634125 Validation loss 0.05272768437862396 Accuracy 0.47021484375\n",
      "Iteration 2420 Training loss 0.05290094017982483 Validation loss 0.05301736667752266 Accuracy 0.467041015625\n",
      "Iteration 2430 Training loss 0.05269636958837509 Validation loss 0.052684687077999115 Accuracy 0.470458984375\n",
      "Iteration 2440 Training loss 0.05124662071466446 Validation loss 0.05302629619836807 Accuracy 0.467041015625\n",
      "Iteration 2450 Training loss 0.05246933549642563 Validation loss 0.05382099375128746 Accuracy 0.458984375\n",
      "Iteration 2460 Training loss 0.054802510887384415 Validation loss 0.053572844713926315 Accuracy 0.46142578125\n",
      "Iteration 2470 Training loss 0.05220320448279381 Validation loss 0.053729865700006485 Accuracy 0.45849609375\n",
      "Iteration 2480 Training loss 0.05376114696264267 Validation loss 0.052854858338832855 Accuracy 0.46923828125\n",
      "Iteration 2490 Training loss 0.05125727131962776 Validation loss 0.053189121186733246 Accuracy 0.465576171875\n",
      "Iteration 2500 Training loss 0.05475766956806183 Validation loss 0.05335334688425064 Accuracy 0.46337890625\n",
      "Iteration 2510 Training loss 0.053135182708501816 Validation loss 0.052797626703977585 Accuracy 0.46923828125\n",
      "Iteration 2520 Training loss 0.052257098257541656 Validation loss 0.052655868232250214 Accuracy 0.470947265625\n",
      "Iteration 2530 Training loss 0.051430247724056244 Validation loss 0.052657436579465866 Accuracy 0.47119140625\n",
      "Iteration 2540 Training loss 0.04988481104373932 Validation loss 0.052647508680820465 Accuracy 0.47119140625\n",
      "Iteration 2550 Training loss 0.05351287126541138 Validation loss 0.05268353968858719 Accuracy 0.47119140625\n",
      "Iteration 2560 Training loss 0.051382262259721756 Validation loss 0.05266829952597618 Accuracy 0.470947265625\n",
      "Iteration 2570 Training loss 0.052958302199840546 Validation loss 0.05275273695588112 Accuracy 0.4697265625\n",
      "Iteration 2580 Training loss 0.05478041619062424 Validation loss 0.05277319252490997 Accuracy 0.469482421875\n",
      "Iteration 2590 Training loss 0.05068769305944443 Validation loss 0.052738480269908905 Accuracy 0.4697265625\n",
      "Iteration 2600 Training loss 0.05342794209718704 Validation loss 0.05344412103295326 Accuracy 0.462646484375\n",
      "Iteration 2610 Training loss 0.054408080875873566 Validation loss 0.05401398241519928 Accuracy 0.456787109375\n",
      "Iteration 2620 Training loss 0.053435876965522766 Validation loss 0.052757568657398224 Accuracy 0.469482421875\n",
      "Iteration 2630 Training loss 0.05387046933174133 Validation loss 0.05285239964723587 Accuracy 0.468505859375\n",
      "Iteration 2640 Training loss 0.05216722935438156 Validation loss 0.05270986631512642 Accuracy 0.4697265625\n",
      "Iteration 2650 Training loss 0.05506320297718048 Validation loss 0.05337895452976227 Accuracy 0.463134765625\n",
      "Iteration 2660 Training loss 0.056102924048900604 Validation loss 0.05301516503095627 Accuracy 0.46630859375\n",
      "Iteration 2670 Training loss 0.05134507641196251 Validation loss 0.05289122089743614 Accuracy 0.468017578125\n",
      "Iteration 2680 Training loss 0.05513500049710274 Validation loss 0.05268871784210205 Accuracy 0.470458984375\n",
      "Iteration 2690 Training loss 0.0542798675596714 Validation loss 0.05285244807600975 Accuracy 0.468994140625\n",
      "Iteration 2700 Training loss 0.05132480338215828 Validation loss 0.05279951170086861 Accuracy 0.47021484375\n",
      "Iteration 2710 Training loss 0.04996189847588539 Validation loss 0.052996039390563965 Accuracy 0.4677734375\n",
      "Iteration 2720 Training loss 0.05328817665576935 Validation loss 0.0527438223361969 Accuracy 0.469970703125\n",
      "Iteration 2730 Training loss 0.05471845343708992 Validation loss 0.05276544764637947 Accuracy 0.4697265625\n",
      "Iteration 2740 Training loss 0.0545782670378685 Validation loss 0.053106728941202164 Accuracy 0.46630859375\n",
      "Iteration 2750 Training loss 0.052708301693201065 Validation loss 0.05419543758034706 Accuracy 0.4541015625\n",
      "Iteration 2760 Training loss 0.05140242353081703 Validation loss 0.05298352241516113 Accuracy 0.467041015625\n",
      "Iteration 2770 Training loss 0.05175730213522911 Validation loss 0.05458412319421768 Accuracy 0.4501953125\n",
      "Iteration 2780 Training loss 0.05549926683306694 Validation loss 0.05261419713497162 Accuracy 0.47119140625\n",
      "Iteration 2790 Training loss 0.051930561661720276 Validation loss 0.05269908159971237 Accuracy 0.469970703125\n",
      "Iteration 2800 Training loss 0.05224324390292168 Validation loss 0.0527900829911232 Accuracy 0.4697265625\n",
      "Iteration 2810 Training loss 0.04967321828007698 Validation loss 0.052716925740242004 Accuracy 0.47021484375\n",
      "Iteration 2820 Training loss 0.05168396979570389 Validation loss 0.05274459347128868 Accuracy 0.469970703125\n",
      "Iteration 2830 Training loss 0.052771031856536865 Validation loss 0.052854668349027634 Accuracy 0.46923828125\n",
      "Iteration 2840 Training loss 0.05065234750509262 Validation loss 0.0527537576854229 Accuracy 0.470458984375\n",
      "Iteration 2850 Training loss 0.0511174201965332 Validation loss 0.052637528628110886 Accuracy 0.470703125\n",
      "Iteration 2860 Training loss 0.052694812417030334 Validation loss 0.05279281735420227 Accuracy 0.469482421875\n",
      "Iteration 2870 Training loss 0.05143825337290764 Validation loss 0.0527944415807724 Accuracy 0.469970703125\n",
      "Iteration 2880 Training loss 0.055005136877298355 Validation loss 0.053210627287626266 Accuracy 0.465576171875\n",
      "Iteration 2890 Training loss 0.05186699703335762 Validation loss 0.052695851773023605 Accuracy 0.470458984375\n",
      "Iteration 2900 Training loss 0.05509056895971298 Validation loss 0.05278674140572548 Accuracy 0.46923828125\n",
      "Iteration 2910 Training loss 0.04760318994522095 Validation loss 0.05270182341337204 Accuracy 0.470703125\n",
      "Iteration 2920 Training loss 0.052810005843639374 Validation loss 0.05277156084775925 Accuracy 0.4697265625\n",
      "Iteration 2930 Training loss 0.050510477274656296 Validation loss 0.05265580862760544 Accuracy 0.470703125\n",
      "Iteration 2940 Training loss 0.05272595211863518 Validation loss 0.05269258841872215 Accuracy 0.470703125\n",
      "Iteration 2950 Training loss 0.05089796334505081 Validation loss 0.052823495119810104 Accuracy 0.46923828125\n",
      "Iteration 2960 Training loss 0.049787554889917374 Validation loss 0.052656784653663635 Accuracy 0.470947265625\n",
      "Iteration 2970 Training loss 0.05307319760322571 Validation loss 0.05268092826008797 Accuracy 0.470703125\n",
      "Iteration 2980 Training loss 0.053182490170001984 Validation loss 0.05293095484375954 Accuracy 0.468505859375\n",
      "Iteration 2990 Training loss 0.05099175125360489 Validation loss 0.05267558619379997 Accuracy 0.470703125\n",
      "Iteration 3000 Training loss 0.05189492180943489 Validation loss 0.05269467085599899 Accuracy 0.470458984375\n",
      "Iteration 3010 Training loss 0.0533699132502079 Validation loss 0.05273465812206268 Accuracy 0.470458984375\n",
      "Iteration 3020 Training loss 0.05079510062932968 Validation loss 0.052576128393411636 Accuracy 0.471435546875\n",
      "Iteration 3030 Training loss 0.05203975364565849 Validation loss 0.052636854350566864 Accuracy 0.471435546875\n",
      "Iteration 3040 Training loss 0.049787282943725586 Validation loss 0.0525830052793026 Accuracy 0.4716796875\n",
      "Iteration 3050 Training loss 0.049840107560157776 Validation loss 0.05262577906250954 Accuracy 0.4716796875\n",
      "Iteration 3060 Training loss 0.04999629780650139 Validation loss 0.05266765505075455 Accuracy 0.470947265625\n",
      "Iteration 3070 Training loss 0.04920603707432747 Validation loss 0.05278216674923897 Accuracy 0.4697265625\n",
      "Iteration 3080 Training loss 0.05113939195871353 Validation loss 0.052589401602745056 Accuracy 0.471435546875\n",
      "Iteration 3090 Training loss 0.051873352378606796 Validation loss 0.05336432531476021 Accuracy 0.46435546875\n",
      "Iteration 3100 Training loss 0.054902322590351105 Validation loss 0.05292964726686478 Accuracy 0.468017578125\n",
      "Iteration 3110 Training loss 0.05215420946478844 Validation loss 0.052904680371284485 Accuracy 0.46826171875\n",
      "Iteration 3120 Training loss 0.05736595764756203 Validation loss 0.053126052021980286 Accuracy 0.466796875\n",
      "Iteration 3130 Training loss 0.05416562780737877 Validation loss 0.05268657207489014 Accuracy 0.470458984375\n",
      "Iteration 3140 Training loss 0.05207851156592369 Validation loss 0.05257529765367508 Accuracy 0.471435546875\n",
      "Iteration 3150 Training loss 0.05217108502984047 Validation loss 0.05265596881508827 Accuracy 0.470947265625\n",
      "Iteration 3160 Training loss 0.05397374555468559 Validation loss 0.05268850550055504 Accuracy 0.470703125\n",
      "Iteration 3170 Training loss 0.05337345972657204 Validation loss 0.052666228264570236 Accuracy 0.470703125\n",
      "Iteration 3180 Training loss 0.04938441887497902 Validation loss 0.05260585620999336 Accuracy 0.470947265625\n",
      "Iteration 3190 Training loss 0.048838138580322266 Validation loss 0.0526822954416275 Accuracy 0.470947265625\n",
      "Iteration 3200 Training loss 0.05454340949654579 Validation loss 0.05306579917669296 Accuracy 0.46630859375\n",
      "Iteration 3210 Training loss 0.05107467994093895 Validation loss 0.05274162068963051 Accuracy 0.470458984375\n",
      "Iteration 3220 Training loss 0.049630057066679 Validation loss 0.052810247987508774 Accuracy 0.46923828125\n",
      "Iteration 3230 Training loss 0.05336104705929756 Validation loss 0.052807100117206573 Accuracy 0.468994140625\n",
      "Iteration 3240 Training loss 0.04893304407596588 Validation loss 0.052557576447725296 Accuracy 0.4716796875\n",
      "Iteration 3250 Training loss 0.048777904361486435 Validation loss 0.05286245048046112 Accuracy 0.468017578125\n",
      "Iteration 3260 Training loss 0.05126866698265076 Validation loss 0.052889734506607056 Accuracy 0.46923828125\n",
      "Iteration 3270 Training loss 0.0537242628633976 Validation loss 0.05281521752476692 Accuracy 0.468994140625\n",
      "Iteration 3280 Training loss 0.05032077431678772 Validation loss 0.05296856164932251 Accuracy 0.468017578125\n",
      "Iteration 3290 Training loss 0.05034095048904419 Validation loss 0.052633147686719894 Accuracy 0.4716796875\n",
      "Iteration 3300 Training loss 0.051019083708524704 Validation loss 0.05265916883945465 Accuracy 0.470458984375\n",
      "Iteration 3310 Training loss 0.05395425483584404 Validation loss 0.05253465101122856 Accuracy 0.4716796875\n",
      "Iteration 3320 Training loss 0.05070120096206665 Validation loss 0.05258971080183983 Accuracy 0.471435546875\n",
      "Iteration 3330 Training loss 0.05097813904285431 Validation loss 0.05273844674229622 Accuracy 0.469482421875\n",
      "Iteration 3340 Training loss 0.049480803310871124 Validation loss 0.0525544174015522 Accuracy 0.471923828125\n",
      "Iteration 3350 Training loss 0.04926493391394615 Validation loss 0.052536629140377045 Accuracy 0.47216796875\n",
      "Iteration 3360 Training loss 0.05168698728084564 Validation loss 0.05258994922041893 Accuracy 0.4716796875\n",
      "Iteration 3370 Training loss 0.051278360188007355 Validation loss 0.05244366452097893 Accuracy 0.472900390625\n",
      "Iteration 3380 Training loss 0.054750680923461914 Validation loss 0.0527145117521286 Accuracy 0.470703125\n",
      "Iteration 3390 Training loss 0.04857933521270752 Validation loss 0.052609313279390335 Accuracy 0.47119140625\n",
      "Iteration 3400 Training loss 0.05308302864432335 Validation loss 0.05260806530714035 Accuracy 0.47119140625\n",
      "Iteration 3410 Training loss 0.05282570421695709 Validation loss 0.05290656164288521 Accuracy 0.46826171875\n",
      "Iteration 3420 Training loss 0.05029931664466858 Validation loss 0.05264394357800484 Accuracy 0.470703125\n",
      "Iteration 3430 Training loss 0.05465691164135933 Validation loss 0.05270079895853996 Accuracy 0.470703125\n",
      "Iteration 3440 Training loss 0.051800332963466644 Validation loss 0.05284038931131363 Accuracy 0.46875\n",
      "Iteration 3450 Training loss 0.052472684532403946 Validation loss 0.052526943385601044 Accuracy 0.4716796875\n",
      "Iteration 3460 Training loss 0.04967184364795685 Validation loss 0.05266422778367996 Accuracy 0.470458984375\n",
      "Iteration 3470 Training loss 0.0534316822886467 Validation loss 0.052524909377098083 Accuracy 0.4716796875\n",
      "Iteration 3480 Training loss 0.05796854570508003 Validation loss 0.05332408472895622 Accuracy 0.46435546875\n",
      "Iteration 3490 Training loss 0.052469074726104736 Validation loss 0.05324048548936844 Accuracy 0.465576171875\n",
      "Iteration 3500 Training loss 0.049841489642858505 Validation loss 0.053103748708963394 Accuracy 0.46630859375\n",
      "Iteration 3510 Training loss 0.05472121015191078 Validation loss 0.05275660380721092 Accuracy 0.470703125\n",
      "Iteration 3520 Training loss 0.04918282851576805 Validation loss 0.052630290389060974 Accuracy 0.4716796875\n",
      "Iteration 3530 Training loss 0.05382813513278961 Validation loss 0.05261395499110222 Accuracy 0.47119140625\n",
      "Iteration 3540 Training loss 0.04787243530154228 Validation loss 0.052636872977018356 Accuracy 0.470703125\n",
      "Iteration 3550 Training loss 0.05346919596195221 Validation loss 0.0531710721552372 Accuracy 0.4658203125\n",
      "Iteration 3560 Training loss 0.05087432637810707 Validation loss 0.05253978818655014 Accuracy 0.472412109375\n",
      "Iteration 3570 Training loss 0.05295650288462639 Validation loss 0.052566755563020706 Accuracy 0.471435546875\n",
      "Iteration 3580 Training loss 0.04925402253866196 Validation loss 0.0525362491607666 Accuracy 0.4716796875\n",
      "Iteration 3590 Training loss 0.05390330031514168 Validation loss 0.052542343735694885 Accuracy 0.471923828125\n",
      "Iteration 3600 Training loss 0.05513995140790939 Validation loss 0.052702274173498154 Accuracy 0.470947265625\n",
      "Iteration 3610 Training loss 0.056232843548059464 Validation loss 0.05263529345393181 Accuracy 0.470703125\n",
      "Iteration 3620 Training loss 0.05195138603448868 Validation loss 0.05252722650766373 Accuracy 0.4716796875\n",
      "Iteration 3630 Training loss 0.04978828877210617 Validation loss 0.052579157054424286 Accuracy 0.4716796875\n",
      "Iteration 3640 Training loss 0.052640076726675034 Validation loss 0.052840299904346466 Accuracy 0.46826171875\n",
      "Iteration 3650 Training loss 0.050633929669857025 Validation loss 0.05263828858733177 Accuracy 0.470703125\n",
      "Iteration 3660 Training loss 0.05110611766576767 Validation loss 0.0524526946246624 Accuracy 0.47265625\n",
      "Iteration 3670 Training loss 0.053811900317668915 Validation loss 0.052520204335451126 Accuracy 0.47216796875\n",
      "Iteration 3680 Training loss 0.05172479525208473 Validation loss 0.05248894542455673 Accuracy 0.472412109375\n",
      "Iteration 3690 Training loss 0.049496449530124664 Validation loss 0.052640389651060104 Accuracy 0.47119140625\n",
      "Iteration 3700 Training loss 0.05284123867750168 Validation loss 0.052530139684677124 Accuracy 0.4716796875\n",
      "Iteration 3710 Training loss 0.05005546659231186 Validation loss 0.052532851696014404 Accuracy 0.471923828125\n",
      "Iteration 3720 Training loss 0.04959353059530258 Validation loss 0.05289505422115326 Accuracy 0.468017578125\n",
      "Iteration 3730 Training loss 0.055822134017944336 Validation loss 0.05277480185031891 Accuracy 0.46875\n",
      "Iteration 3740 Training loss 0.052058927714824677 Validation loss 0.052601948380470276 Accuracy 0.470947265625\n",
      "Iteration 3750 Training loss 0.051754917949438095 Validation loss 0.05264957994222641 Accuracy 0.471435546875\n",
      "Iteration 3760 Training loss 0.052644748240709305 Validation loss 0.05260522663593292 Accuracy 0.471435546875\n",
      "Iteration 3770 Training loss 0.05154426395893097 Validation loss 0.052554741501808167 Accuracy 0.4716796875\n",
      "Iteration 3780 Training loss 0.05184325948357582 Validation loss 0.05260201916098595 Accuracy 0.4716796875\n",
      "Iteration 3790 Training loss 0.053865883499383926 Validation loss 0.05267278105020523 Accuracy 0.470947265625\n",
      "Iteration 3800 Training loss 0.056731220334768295 Validation loss 0.0525854229927063 Accuracy 0.4716796875\n",
      "Iteration 3810 Training loss 0.05341559648513794 Validation loss 0.05268006771802902 Accuracy 0.470458984375\n",
      "Iteration 3820 Training loss 0.05406434088945389 Validation loss 0.05267621949315071 Accuracy 0.470947265625\n",
      "Iteration 3830 Training loss 0.05178966373205185 Validation loss 0.052593257278203964 Accuracy 0.47119140625\n",
      "Iteration 3840 Training loss 0.05613040551543236 Validation loss 0.05258411914110184 Accuracy 0.470947265625\n",
      "Iteration 3850 Training loss 0.051312681287527084 Validation loss 0.05251533165574074 Accuracy 0.47216796875\n",
      "Iteration 3860 Training loss 0.05292556807398796 Validation loss 0.05256626009941101 Accuracy 0.4716796875\n",
      "Iteration 3870 Training loss 0.04935638979077339 Validation loss 0.05252230167388916 Accuracy 0.471435546875\n",
      "Iteration 3880 Training loss 0.05391758307814598 Validation loss 0.052592333406209946 Accuracy 0.470703125\n",
      "Iteration 3890 Training loss 0.05411115288734436 Validation loss 0.05254749208688736 Accuracy 0.47119140625\n",
      "Iteration 3900 Training loss 0.05228934809565544 Validation loss 0.05246290937066078 Accuracy 0.47265625\n",
      "Iteration 3910 Training loss 0.04891958087682724 Validation loss 0.0524432510137558 Accuracy 0.472900390625\n",
      "Iteration 3920 Training loss 0.05022534728050232 Validation loss 0.052787791937589645 Accuracy 0.46826171875\n",
      "Iteration 3930 Training loss 0.05109335109591484 Validation loss 0.05289231985807419 Accuracy 0.467529296875\n",
      "Iteration 3940 Training loss 0.0514049232006073 Validation loss 0.05316230282187462 Accuracy 0.464599609375\n",
      "Iteration 3950 Training loss 0.054168492555618286 Validation loss 0.05249984562397003 Accuracy 0.47119140625\n",
      "Iteration 3960 Training loss 0.05172491818666458 Validation loss 0.05316365510225296 Accuracy 0.4638671875\n",
      "Iteration 3970 Training loss 0.05135313794016838 Validation loss 0.05251996964216232 Accuracy 0.472412109375\n",
      "Iteration 3980 Training loss 0.048905640840530396 Validation loss 0.05262058600783348 Accuracy 0.470703125\n",
      "Iteration 3990 Training loss 0.05355117842555046 Validation loss 0.053999412804841995 Accuracy 0.45703125\n",
      "Iteration 4000 Training loss 0.054443489760160446 Validation loss 0.05260225012898445 Accuracy 0.470947265625\n",
      "Iteration 4010 Training loss 0.05124406889081001 Validation loss 0.052435752004384995 Accuracy 0.47314453125\n",
      "Iteration 4020 Training loss 0.05071314424276352 Validation loss 0.05383105203509331 Accuracy 0.45947265625\n",
      "Iteration 4030 Training loss 0.05350547283887863 Validation loss 0.052573129534721375 Accuracy 0.470703125\n",
      "Iteration 4040 Training loss 0.05212675407528877 Validation loss 0.05294780805706978 Accuracy 0.468017578125\n",
      "Iteration 4050 Training loss 0.05249371379613876 Validation loss 0.05237137898802757 Accuracy 0.47265625\n",
      "Iteration 4060 Training loss 0.05107032135128975 Validation loss 0.05247625708580017 Accuracy 0.472412109375\n",
      "Iteration 4070 Training loss 0.050488539040088654 Validation loss 0.052581071853637695 Accuracy 0.47021484375\n",
      "Iteration 4080 Training loss 0.05188305675983429 Validation loss 0.05253348499536514 Accuracy 0.4716796875\n",
      "Iteration 4090 Training loss 0.0547347292304039 Validation loss 0.05269753187894821 Accuracy 0.468994140625\n",
      "Iteration 4100 Training loss 0.050558291375637054 Validation loss 0.052583955228328705 Accuracy 0.470458984375\n",
      "Iteration 4110 Training loss 0.05223609507083893 Validation loss 0.05243641510605812 Accuracy 0.47265625\n",
      "Iteration 4120 Training loss 0.05274439603090286 Validation loss 0.052535828202962875 Accuracy 0.472412109375\n",
      "Iteration 4130 Training loss 0.04996606335043907 Validation loss 0.05253249779343605 Accuracy 0.472412109375\n",
      "Iteration 4140 Training loss 0.05207518860697746 Validation loss 0.05243980512022972 Accuracy 0.47265625\n",
      "Iteration 4150 Training loss 0.05105288699269295 Validation loss 0.052496299147605896 Accuracy 0.47265625\n",
      "Iteration 4160 Training loss 0.05159614980220795 Validation loss 0.053257379680871964 Accuracy 0.462890625\n",
      "Iteration 4170 Training loss 0.053773537278175354 Validation loss 0.05243416875600815 Accuracy 0.47314453125\n",
      "Iteration 4180 Training loss 0.05026508495211601 Validation loss 0.05248188227415085 Accuracy 0.472412109375\n",
      "Iteration 4190 Training loss 0.05341419577598572 Validation loss 0.053113631904125214 Accuracy 0.464599609375\n",
      "Iteration 4200 Training loss 0.05154971405863762 Validation loss 0.05282648280262947 Accuracy 0.46826171875\n",
      "Iteration 4210 Training loss 0.0509616881608963 Validation loss 0.05255816876888275 Accuracy 0.4716796875\n",
      "Iteration 4220 Training loss 0.053256724029779434 Validation loss 0.053061943501234055 Accuracy 0.466064453125\n",
      "Iteration 4230 Training loss 0.050969064235687256 Validation loss 0.05248582735657692 Accuracy 0.472412109375\n",
      "Iteration 4240 Training loss 0.052328433841466904 Validation loss 0.052559882402420044 Accuracy 0.471923828125\n",
      "Iteration 4250 Training loss 0.051269132643938065 Validation loss 0.05248695984482765 Accuracy 0.47265625\n",
      "Iteration 4260 Training loss 0.054880209267139435 Validation loss 0.052368275821208954 Accuracy 0.47265625\n",
      "Iteration 4270 Training loss 0.057531580328941345 Validation loss 0.05281796678900719 Accuracy 0.4697265625\n",
      "Iteration 4280 Training loss 0.05090317130088806 Validation loss 0.05245545879006386 Accuracy 0.472900390625\n",
      "Iteration 4290 Training loss 0.04813097417354584 Validation loss 0.05240366980433464 Accuracy 0.47314453125\n",
      "Iteration 4300 Training loss 0.05341849848628044 Validation loss 0.0528082549571991 Accuracy 0.469970703125\n",
      "Iteration 4310 Training loss 0.054469119757413864 Validation loss 0.05286410078406334 Accuracy 0.468994140625\n",
      "Iteration 4320 Training loss 0.0498938225209713 Validation loss 0.05241717770695686 Accuracy 0.473388671875\n",
      "Iteration 4330 Training loss 0.055106211453676224 Validation loss 0.05254721641540527 Accuracy 0.471923828125\n",
      "Iteration 4340 Training loss 0.05498909950256348 Validation loss 0.05244537815451622 Accuracy 0.472412109375\n",
      "Iteration 4350 Training loss 0.05433148145675659 Validation loss 0.052468035370111465 Accuracy 0.47265625\n",
      "Iteration 4360 Training loss 0.051307570189237595 Validation loss 0.05262310430407524 Accuracy 0.471435546875\n",
      "Iteration 4370 Training loss 0.04868258535861969 Validation loss 0.05244021490216255 Accuracy 0.47265625\n",
      "Iteration 4380 Training loss 0.05092290788888931 Validation loss 0.05242713913321495 Accuracy 0.472412109375\n",
      "Iteration 4390 Training loss 0.051348328590393066 Validation loss 0.052382439374923706 Accuracy 0.473876953125\n",
      "Iteration 4400 Training loss 0.05123188719153404 Validation loss 0.05353190749883652 Accuracy 0.46240234375\n",
      "Iteration 4410 Training loss 0.04966726526618004 Validation loss 0.052461542189121246 Accuracy 0.472900390625\n",
      "Iteration 4420 Training loss 0.0503770150244236 Validation loss 0.05252920836210251 Accuracy 0.471923828125\n",
      "Iteration 4430 Training loss 0.05264992639422417 Validation loss 0.05245999991893768 Accuracy 0.47314453125\n",
      "Iteration 4440 Training loss 0.051353819668293 Validation loss 0.052592743188142776 Accuracy 0.471923828125\n",
      "Iteration 4450 Training loss 0.050567638128995895 Validation loss 0.052521731704473495 Accuracy 0.471923828125\n",
      "Iteration 4460 Training loss 0.05343019589781761 Validation loss 0.05264553427696228 Accuracy 0.471435546875\n",
      "Iteration 4470 Training loss 0.04955069348216057 Validation loss 0.05247386172413826 Accuracy 0.472412109375\n",
      "Iteration 4480 Training loss 0.0542263500392437 Validation loss 0.05250290781259537 Accuracy 0.47216796875\n",
      "Iteration 4490 Training loss 0.05527722090482712 Validation loss 0.05234851315617561 Accuracy 0.473388671875\n",
      "Iteration 4500 Training loss 0.05122390761971474 Validation loss 0.052652910351753235 Accuracy 0.470947265625\n",
      "Iteration 4510 Training loss 0.056383464485406876 Validation loss 0.05276503413915634 Accuracy 0.470703125\n",
      "Iteration 4520 Training loss 0.051438577473163605 Validation loss 0.052423056215047836 Accuracy 0.47314453125\n",
      "Iteration 4530 Training loss 0.04882033169269562 Validation loss 0.05272001028060913 Accuracy 0.469970703125\n",
      "Iteration 4540 Training loss 0.049395203590393066 Validation loss 0.05234139412641525 Accuracy 0.47412109375\n",
      "Iteration 4550 Training loss 0.05223558470606804 Validation loss 0.05248938873410225 Accuracy 0.472412109375\n",
      "Iteration 4560 Training loss 0.052430786192417145 Validation loss 0.05252004787325859 Accuracy 0.471923828125\n",
      "Iteration 4570 Training loss 0.052015990018844604 Validation loss 0.05247029289603233 Accuracy 0.472412109375\n",
      "Iteration 4580 Training loss 0.05251390486955643 Validation loss 0.052284467965364456 Accuracy 0.474609375\n",
      "Iteration 4590 Training loss 0.052123792469501495 Validation loss 0.05250057578086853 Accuracy 0.47216796875\n",
      "Iteration 4600 Training loss 0.05702745169401169 Validation loss 0.052454959601163864 Accuracy 0.472412109375\n",
      "Iteration 4610 Training loss 0.05180260166525841 Validation loss 0.05237303674221039 Accuracy 0.473876953125\n",
      "Iteration 4620 Training loss 0.05211514234542847 Validation loss 0.052334852516651154 Accuracy 0.4736328125\n",
      "Iteration 4630 Training loss 0.05066192150115967 Validation loss 0.05252250283956528 Accuracy 0.4716796875\n",
      "Iteration 4640 Training loss 0.05274982750415802 Validation loss 0.05278240516781807 Accuracy 0.469482421875\n",
      "Iteration 4650 Training loss 0.05131329223513603 Validation loss 0.05245010554790497 Accuracy 0.471923828125\n",
      "Iteration 4660 Training loss 0.05332548916339874 Validation loss 0.052586305886507034 Accuracy 0.470703125\n",
      "Iteration 4670 Training loss 0.05296996980905533 Validation loss 0.05244436860084534 Accuracy 0.47216796875\n",
      "Iteration 4680 Training loss 0.055333156138658524 Validation loss 0.052825380116701126 Accuracy 0.46826171875\n",
      "Iteration 4690 Training loss 0.0552741177380085 Validation loss 0.052655309438705444 Accuracy 0.469970703125\n",
      "Iteration 4700 Training loss 0.05396933853626251 Validation loss 0.05253173038363457 Accuracy 0.470947265625\n",
      "Iteration 4710 Training loss 0.0532686710357666 Validation loss 0.05245139077305794 Accuracy 0.472412109375\n",
      "Iteration 4720 Training loss 0.052151329815387726 Validation loss 0.05256771296262741 Accuracy 0.470947265625\n",
      "Iteration 4730 Training loss 0.05444316938519478 Validation loss 0.0524514876306057 Accuracy 0.471923828125\n",
      "Iteration 4740 Training loss 0.050798531621694565 Validation loss 0.0524752140045166 Accuracy 0.471435546875\n",
      "Iteration 4750 Training loss 0.052347756922245026 Validation loss 0.05231683328747749 Accuracy 0.473388671875\n",
      "Iteration 4760 Training loss 0.05061173439025879 Validation loss 0.05288368836045265 Accuracy 0.467041015625\n",
      "Iteration 4770 Training loss 0.04921572655439377 Validation loss 0.05235790088772774 Accuracy 0.47314453125\n",
      "Iteration 4780 Training loss 0.05431909114122391 Validation loss 0.05241122469305992 Accuracy 0.47216796875\n",
      "Iteration 4790 Training loss 0.054569415748119354 Validation loss 0.05233814939856529 Accuracy 0.47412109375\n",
      "Iteration 4800 Training loss 0.050959039479494095 Validation loss 0.05238593742251396 Accuracy 0.47265625\n",
      "Iteration 4810 Training loss 0.05243370682001114 Validation loss 0.052625399082899094 Accuracy 0.47119140625\n",
      "Iteration 4820 Training loss 0.052017997950315475 Validation loss 0.0527774877846241 Accuracy 0.469482421875\n",
      "Iteration 4830 Training loss 0.053895004093647 Validation loss 0.052511997520923615 Accuracy 0.472412109375\n",
      "Iteration 4840 Training loss 0.05252092331647873 Validation loss 0.053112927824258804 Accuracy 0.466552734375\n",
      "Iteration 4850 Training loss 0.052790284156799316 Validation loss 0.052569687366485596 Accuracy 0.471923828125\n",
      "Iteration 4860 Training loss 0.04923753812909126 Validation loss 0.05231810361146927 Accuracy 0.47412109375\n",
      "Iteration 4870 Training loss 0.05083455517888069 Validation loss 0.05237016826868057 Accuracy 0.4736328125\n",
      "Iteration 4880 Training loss 0.05297026410698891 Validation loss 0.05234057456254959 Accuracy 0.473876953125\n",
      "Iteration 4890 Training loss 0.0506717674434185 Validation loss 0.052509959787130356 Accuracy 0.4716796875\n",
      "Iteration 4900 Training loss 0.05270714685320854 Validation loss 0.05255240947008133 Accuracy 0.470947265625\n",
      "Iteration 4910 Training loss 0.05300101637840271 Validation loss 0.05250401049852371 Accuracy 0.472900390625\n",
      "Iteration 4920 Training loss 0.05021336302161217 Validation loss 0.052285339683294296 Accuracy 0.474609375\n",
      "Iteration 4930 Training loss 0.05139998346567154 Validation loss 0.05243697762489319 Accuracy 0.473388671875\n",
      "Iteration 4940 Training loss 0.05161111429333687 Validation loss 0.05242488533258438 Accuracy 0.473388671875\n",
      "Iteration 4950 Training loss 0.0527740940451622 Validation loss 0.05262879654765129 Accuracy 0.47119140625\n",
      "Iteration 4960 Training loss 0.055181849747896194 Validation loss 0.0523960143327713 Accuracy 0.473388671875\n",
      "Iteration 4970 Training loss 0.052882783114910126 Validation loss 0.052391063421964645 Accuracy 0.473388671875\n",
      "Iteration 4980 Training loss 0.05178653821349144 Validation loss 0.05300644412636757 Accuracy 0.4658203125\n",
      "Iteration 4990 Training loss 0.04844239726662636 Validation loss 0.04766597971320152 Accuracy 0.515625\n",
      "Iteration 5000 Training loss 0.044710855931043625 Validation loss 0.047308601438999176 Accuracy 0.51953125\n",
      "Iteration 5010 Training loss 0.04072679951786995 Validation loss 0.045694779604673386 Accuracy 0.5390625\n",
      "Iteration 5020 Training loss 0.041460201144218445 Validation loss 0.04516139626502991 Accuracy 0.54345703125\n",
      "Iteration 5030 Training loss 0.044054534286260605 Validation loss 0.044507719576358795 Accuracy 0.55078125\n",
      "Iteration 5040 Training loss 0.04415867477655411 Validation loss 0.04514337703585625 Accuracy 0.54541015625\n",
      "Iteration 5050 Training loss 0.04530048742890358 Validation loss 0.04460545629262924 Accuracy 0.548828125\n",
      "Iteration 5060 Training loss 0.04416213929653168 Validation loss 0.045885127037763596 Accuracy 0.53515625\n",
      "Iteration 5070 Training loss 0.04244311898946762 Validation loss 0.04415491223335266 Accuracy 0.55419921875\n",
      "Iteration 5080 Training loss 0.03982231020927429 Validation loss 0.04396570101380348 Accuracy 0.556640625\n",
      "Iteration 5090 Training loss 0.043350864201784134 Validation loss 0.04399363324046135 Accuracy 0.556640625\n",
      "Iteration 5100 Training loss 0.04600170999765396 Validation loss 0.04390101134777069 Accuracy 0.55859375\n",
      "Iteration 5110 Training loss 0.0403677336871624 Validation loss 0.04387875273823738 Accuracy 0.55810546875\n",
      "Iteration 5120 Training loss 0.046117544174194336 Validation loss 0.043980203568935394 Accuracy 0.55712890625\n",
      "Iteration 5130 Training loss 0.04110996797680855 Validation loss 0.043966248631477356 Accuracy 0.55810546875\n",
      "Iteration 5140 Training loss 0.04274832084774971 Validation loss 0.04400597885251045 Accuracy 0.556640625\n",
      "Iteration 5150 Training loss 0.04399600252509117 Validation loss 0.04376864805817604 Accuracy 0.55908203125\n",
      "Iteration 5160 Training loss 0.04384913668036461 Validation loss 0.044057946652173996 Accuracy 0.5556640625\n",
      "Iteration 5170 Training loss 0.042486049234867096 Validation loss 0.04397255927324295 Accuracy 0.55810546875\n",
      "Iteration 5180 Training loss 0.04426676779985428 Validation loss 0.04558023810386658 Accuracy 0.5380859375\n",
      "Iteration 5190 Training loss 0.04414256289601326 Validation loss 0.04407002404332161 Accuracy 0.55712890625\n",
      "Iteration 5200 Training loss 0.04211795702576637 Validation loss 0.04404311254620552 Accuracy 0.556640625\n",
      "Iteration 5210 Training loss 0.0439600944519043 Validation loss 0.04418810456991196 Accuracy 0.5546875\n",
      "Iteration 5220 Training loss 0.046508293598890305 Validation loss 0.04385549575090408 Accuracy 0.55810546875\n",
      "Iteration 5230 Training loss 0.04671299085021019 Validation loss 0.04368826374411583 Accuracy 0.56005859375\n",
      "Iteration 5240 Training loss 0.04439910873770714 Validation loss 0.04405426234006882 Accuracy 0.55517578125\n",
      "Iteration 5250 Training loss 0.04494744911789894 Validation loss 0.043921589851379395 Accuracy 0.556640625\n",
      "Iteration 5260 Training loss 0.043426260352134705 Validation loss 0.043815355747938156 Accuracy 0.5595703125\n",
      "Iteration 5270 Training loss 0.043994538486003876 Validation loss 0.04367344081401825 Accuracy 0.560546875\n",
      "Iteration 5280 Training loss 0.041983891278505325 Validation loss 0.044428709894418716 Accuracy 0.5517578125\n",
      "Iteration 5290 Training loss 0.04365644231438637 Validation loss 0.043565887957811356 Accuracy 0.56103515625\n",
      "Iteration 5300 Training loss 0.04343611001968384 Validation loss 0.043570131063461304 Accuracy 0.5615234375\n",
      "Iteration 5310 Training loss 0.0428701713681221 Validation loss 0.04381607845425606 Accuracy 0.5595703125\n",
      "Iteration 5320 Training loss 0.04248448833823204 Validation loss 0.04379108175635338 Accuracy 0.56005859375\n",
      "Iteration 5330 Training loss 0.04109863564372063 Validation loss 0.044460322707891464 Accuracy 0.55126953125\n",
      "Iteration 5340 Training loss 0.04300304129719734 Validation loss 0.04580806940793991 Accuracy 0.53662109375\n",
      "Iteration 5350 Training loss 0.04237945005297661 Validation loss 0.04352601617574692 Accuracy 0.56201171875\n",
      "Iteration 5360 Training loss 0.041755784302949905 Validation loss 0.04457537457346916 Accuracy 0.55126953125\n",
      "Iteration 5370 Training loss 0.04250938817858696 Validation loss 0.04442209005355835 Accuracy 0.5537109375\n",
      "Iteration 5380 Training loss 0.04595620557665825 Validation loss 0.0435420386493206 Accuracy 0.5615234375\n",
      "Iteration 5390 Training loss 0.03629752993583679 Validation loss 0.043769266456365585 Accuracy 0.55908203125\n",
      "Iteration 5400 Training loss 0.04447877034544945 Validation loss 0.04374641552567482 Accuracy 0.56005859375\n",
      "Iteration 5410 Training loss 0.03969963639974594 Validation loss 0.04342934861779213 Accuracy 0.56298828125\n",
      "Iteration 5420 Training loss 0.04531816020607948 Validation loss 0.04430709034204483 Accuracy 0.552734375\n",
      "Iteration 5430 Training loss 0.04206858202815056 Validation loss 0.04405980184674263 Accuracy 0.556640625\n",
      "Iteration 5440 Training loss 0.03950001299381256 Validation loss 0.04359123855829239 Accuracy 0.56201171875\n",
      "Iteration 5450 Training loss 0.04214601591229439 Validation loss 0.04354662820696831 Accuracy 0.5615234375\n",
      "Iteration 5460 Training loss 0.04573218151926994 Validation loss 0.04346306622028351 Accuracy 0.5634765625\n",
      "Iteration 5470 Training loss 0.04848643019795418 Validation loss 0.04370647296309471 Accuracy 0.56005859375\n",
      "Iteration 5480 Training loss 0.0433373749256134 Validation loss 0.04352328181266785 Accuracy 0.56298828125\n",
      "Iteration 5490 Training loss 0.04348205402493477 Validation loss 0.04356042295694351 Accuracy 0.5615234375\n",
      "Iteration 5500 Training loss 0.039848580956459045 Validation loss 0.04350198060274124 Accuracy 0.5625\n",
      "Iteration 5510 Training loss 0.03996029496192932 Validation loss 0.04348871856927872 Accuracy 0.5625\n",
      "Iteration 5520 Training loss 0.04210919141769409 Validation loss 0.043625760823488235 Accuracy 0.5615234375\n",
      "Iteration 5530 Training loss 0.04317976161837578 Validation loss 0.043600067496299744 Accuracy 0.56201171875\n",
      "Iteration 5540 Training loss 0.04047749564051628 Validation loss 0.043661732226610184 Accuracy 0.5615234375\n",
      "Iteration 5550 Training loss 0.04036342725157738 Validation loss 0.04361794888973236 Accuracy 0.56103515625\n",
      "Iteration 5560 Training loss 0.04077937453985214 Validation loss 0.04358057677745819 Accuracy 0.56201171875\n",
      "Iteration 5570 Training loss 0.03962339460849762 Validation loss 0.04381450265645981 Accuracy 0.55859375\n",
      "Iteration 5580 Training loss 0.040431100875139236 Validation loss 0.04351867735385895 Accuracy 0.56201171875\n",
      "Iteration 5590 Training loss 0.045386698096990585 Validation loss 0.044388432055711746 Accuracy 0.55224609375\n",
      "Iteration 5600 Training loss 0.03982017934322357 Validation loss 0.04340006411075592 Accuracy 0.56298828125\n",
      "Iteration 5610 Training loss 0.04228803515434265 Validation loss 0.043441466987133026 Accuracy 0.5634765625\n",
      "Iteration 5620 Training loss 0.03904721513390541 Validation loss 0.04349349066615105 Accuracy 0.56201171875\n",
      "Iteration 5630 Training loss 0.04035332053899765 Validation loss 0.04357776418328285 Accuracy 0.56103515625\n",
      "Iteration 5640 Training loss 0.04261760786175728 Validation loss 0.043342456221580505 Accuracy 0.56396484375\n",
      "Iteration 5650 Training loss 0.042051222175359726 Validation loss 0.044196967035532 Accuracy 0.55517578125\n",
      "Iteration 5660 Training loss 0.04350224882364273 Validation loss 0.04393574222922325 Accuracy 0.556640625\n",
      "Iteration 5670 Training loss 0.04090229421854019 Validation loss 0.04349229484796524 Accuracy 0.5634765625\n",
      "Iteration 5680 Training loss 0.043831489980220795 Validation loss 0.04344968497753143 Accuracy 0.56298828125\n",
      "Iteration 5690 Training loss 0.04381538927555084 Validation loss 0.04333910718560219 Accuracy 0.56396484375\n",
      "Iteration 5700 Training loss 0.04349682480096817 Validation loss 0.04390634596347809 Accuracy 0.55908203125\n",
      "Iteration 5710 Training loss 0.045860406011343 Validation loss 0.04334548860788345 Accuracy 0.564453125\n",
      "Iteration 5720 Training loss 0.04174143075942993 Validation loss 0.04341425001621246 Accuracy 0.5634765625\n",
      "Iteration 5730 Training loss 0.04443731531500816 Validation loss 0.04491812735795975 Accuracy 0.54833984375\n",
      "Iteration 5740 Training loss 0.04159288853406906 Validation loss 0.04340706020593643 Accuracy 0.56298828125\n",
      "Iteration 5750 Training loss 0.045300863683223724 Validation loss 0.043439753353595734 Accuracy 0.5634765625\n",
      "Iteration 5760 Training loss 0.04203008487820625 Validation loss 0.04397987201809883 Accuracy 0.55712890625\n",
      "Iteration 5770 Training loss 0.04205850511789322 Validation loss 0.0433870293200016 Accuracy 0.5634765625\n",
      "Iteration 5780 Training loss 0.039368029683828354 Validation loss 0.043356023728847504 Accuracy 0.564453125\n",
      "Iteration 5790 Training loss 0.04284869506955147 Validation loss 0.04331579804420471 Accuracy 0.56396484375\n",
      "Iteration 5800 Training loss 0.042243555188179016 Validation loss 0.04326847940683365 Accuracy 0.5654296875\n",
      "Iteration 5810 Training loss 0.04319584742188454 Validation loss 0.043315742164850235 Accuracy 0.564453125\n",
      "Iteration 5820 Training loss 0.03950507566332817 Validation loss 0.043336279690265656 Accuracy 0.564453125\n",
      "Iteration 5830 Training loss 0.0419711098074913 Validation loss 0.0436243936419487 Accuracy 0.56005859375\n",
      "Iteration 5840 Training loss 0.04367707669734955 Validation loss 0.043614912778139114 Accuracy 0.5615234375\n",
      "Iteration 5850 Training loss 0.043268945068120956 Validation loss 0.04335936903953552 Accuracy 0.564453125\n",
      "Iteration 5860 Training loss 0.0403420552611351 Validation loss 0.04328138381242752 Accuracy 0.56494140625\n",
      "Iteration 5870 Training loss 0.04515334963798523 Validation loss 0.043412674218416214 Accuracy 0.56298828125\n",
      "Iteration 5880 Training loss 0.04350998252630234 Validation loss 0.04343129321932793 Accuracy 0.5625\n",
      "Iteration 5890 Training loss 0.041630975902080536 Validation loss 0.04327921196818352 Accuracy 0.564453125\n",
      "Iteration 5900 Training loss 0.04233870655298233 Validation loss 0.04332413896918297 Accuracy 0.56396484375\n",
      "Iteration 5910 Training loss 0.04170994088053703 Validation loss 0.043650414794683456 Accuracy 0.5595703125\n",
      "Iteration 5920 Training loss 0.041366949677467346 Validation loss 0.04382035881280899 Accuracy 0.55908203125\n",
      "Iteration 5930 Training loss 0.041858334094285965 Validation loss 0.04331954941153526 Accuracy 0.56396484375\n",
      "Iteration 5940 Training loss 0.041297584772109985 Validation loss 0.0434110127389431 Accuracy 0.56298828125\n",
      "Iteration 5950 Training loss 0.04425641894340515 Validation loss 0.04377485066652298 Accuracy 0.56005859375\n",
      "Iteration 5960 Training loss 0.04690106213092804 Validation loss 0.04446381703019142 Accuracy 0.55322265625\n",
      "Iteration 5970 Training loss 0.04271511733531952 Validation loss 0.043479323387145996 Accuracy 0.56201171875\n",
      "Iteration 5980 Training loss 0.04465605691075325 Validation loss 0.04381953552365303 Accuracy 0.55712890625\n",
      "Iteration 5990 Training loss 0.043032143265008926 Validation loss 0.043586961925029755 Accuracy 0.5615234375\n",
      "Iteration 6000 Training loss 0.043071940541267395 Validation loss 0.04346470907330513 Accuracy 0.56298828125\n",
      "Iteration 6010 Training loss 0.03875007852911949 Validation loss 0.04362710937857628 Accuracy 0.560546875\n",
      "Iteration 6020 Training loss 0.04237191751599312 Validation loss 0.04336767643690109 Accuracy 0.56396484375\n",
      "Iteration 6030 Training loss 0.04199445992708206 Validation loss 0.04330002889037132 Accuracy 0.564453125\n",
      "Iteration 6040 Training loss 0.040908750146627426 Validation loss 0.0433594286441803 Accuracy 0.5625\n",
      "Iteration 6050 Training loss 0.042886342853307724 Validation loss 0.043319687247276306 Accuracy 0.564453125\n",
      "Iteration 6060 Training loss 0.03957967460155487 Validation loss 0.04330778121948242 Accuracy 0.56494140625\n",
      "Iteration 6070 Training loss 0.0440315380692482 Validation loss 0.04332504794001579 Accuracy 0.56396484375\n",
      "Iteration 6080 Training loss 0.04378752037882805 Validation loss 0.04326546937227249 Accuracy 0.5654296875\n",
      "Iteration 6090 Training loss 0.04491494223475456 Validation loss 0.04345685988664627 Accuracy 0.5634765625\n",
      "Iteration 6100 Training loss 0.04650689661502838 Validation loss 0.04331774264574051 Accuracy 0.56494140625\n",
      "Iteration 6110 Training loss 0.041800763458013535 Validation loss 0.043210919946432114 Accuracy 0.56494140625\n",
      "Iteration 6120 Training loss 0.04110492393374443 Validation loss 0.04348713904619217 Accuracy 0.5625\n",
      "Iteration 6130 Training loss 0.04589409381151199 Validation loss 0.04338645935058594 Accuracy 0.5634765625\n",
      "Iteration 6140 Training loss 0.04143902659416199 Validation loss 0.04332464560866356 Accuracy 0.56396484375\n",
      "Iteration 6150 Training loss 0.04332113638520241 Validation loss 0.043478574603796005 Accuracy 0.5634765625\n",
      "Iteration 6160 Training loss 0.042126771062612534 Validation loss 0.04340759292244911 Accuracy 0.5634765625\n",
      "Iteration 6170 Training loss 0.04149184003472328 Validation loss 0.04351471737027168 Accuracy 0.5615234375\n",
      "Iteration 6180 Training loss 0.04578189179301262 Validation loss 0.0435929149389267 Accuracy 0.56201171875\n",
      "Iteration 6190 Training loss 0.043228358030319214 Validation loss 0.04333144798874855 Accuracy 0.56396484375\n",
      "Iteration 6200 Training loss 0.04062171280384064 Validation loss 0.04339803755283356 Accuracy 0.5625\n",
      "Iteration 6210 Training loss 0.0374755822122097 Validation loss 0.04335767403244972 Accuracy 0.5634765625\n",
      "Iteration 6220 Training loss 0.041335709393024445 Validation loss 0.043457645922899246 Accuracy 0.56298828125\n",
      "Iteration 6230 Training loss 0.04147746413946152 Validation loss 0.04327239468693733 Accuracy 0.56396484375\n",
      "Iteration 6240 Training loss 0.04474891722202301 Validation loss 0.04344264417886734 Accuracy 0.5634765625\n",
      "Iteration 6250 Training loss 0.044382091611623764 Validation loss 0.043446965515613556 Accuracy 0.5634765625\n",
      "Iteration 6260 Training loss 0.04309123009443283 Validation loss 0.04333125054836273 Accuracy 0.56396484375\n",
      "Iteration 6270 Training loss 0.041436817497015 Validation loss 0.04429033398628235 Accuracy 0.55517578125\n",
      "Iteration 6280 Training loss 0.04104499891400337 Validation loss 0.04327333718538284 Accuracy 0.56396484375\n",
      "Iteration 6290 Training loss 0.04115069657564163 Validation loss 0.04347117990255356 Accuracy 0.56201171875\n",
      "Iteration 6300 Training loss 0.042286697775125504 Validation loss 0.0433807447552681 Accuracy 0.56396484375\n",
      "Iteration 6310 Training loss 0.04366249591112137 Validation loss 0.04381280019879341 Accuracy 0.55810546875\n",
      "Iteration 6320 Training loss 0.04487567022442818 Validation loss 0.04350712150335312 Accuracy 0.5615234375\n",
      "Iteration 6330 Training loss 0.04328283295035362 Validation loss 0.04347793012857437 Accuracy 0.56201171875\n",
      "Iteration 6340 Training loss 0.039489537477493286 Validation loss 0.04338620975613594 Accuracy 0.56298828125\n",
      "Iteration 6350 Training loss 0.04111037775874138 Validation loss 0.04339480400085449 Accuracy 0.5634765625\n",
      "Iteration 6360 Training loss 0.04246276989579201 Validation loss 0.04346497356891632 Accuracy 0.5625\n",
      "Iteration 6370 Training loss 0.04414201155304909 Validation loss 0.04368877410888672 Accuracy 0.560546875\n",
      "Iteration 6380 Training loss 0.04257145896553993 Validation loss 0.04351432994008064 Accuracy 0.560546875\n",
      "Iteration 6390 Training loss 0.04412739351391792 Validation loss 0.04326803609728813 Accuracy 0.56494140625\n",
      "Iteration 6400 Training loss 0.04073384404182434 Validation loss 0.04338739067316055 Accuracy 0.56396484375\n",
      "Iteration 6410 Training loss 0.03797903284430504 Validation loss 0.043301381170749664 Accuracy 0.564453125\n",
      "Iteration 6420 Training loss 0.04058615118265152 Validation loss 0.043332185596227646 Accuracy 0.56396484375\n",
      "Iteration 6430 Training loss 0.04250851273536682 Validation loss 0.04353959858417511 Accuracy 0.5615234375\n",
      "Iteration 6440 Training loss 0.04707285016775131 Validation loss 0.04353032261133194 Accuracy 0.5615234375\n",
      "Iteration 6450 Training loss 0.042846497148275375 Validation loss 0.04318683594465256 Accuracy 0.5654296875\n",
      "Iteration 6460 Training loss 0.0396871343255043 Validation loss 0.04336375743150711 Accuracy 0.5634765625\n",
      "Iteration 6470 Training loss 0.04359455406665802 Validation loss 0.04334626719355583 Accuracy 0.5634765625\n",
      "Iteration 6480 Training loss 0.04674535244703293 Validation loss 0.0439176931977272 Accuracy 0.55615234375\n",
      "Iteration 6490 Training loss 0.040570374578237534 Validation loss 0.0432807058095932 Accuracy 0.564453125\n",
      "Iteration 6500 Training loss 0.04219048097729683 Validation loss 0.04349386692047119 Accuracy 0.5625\n",
      "Iteration 6510 Training loss 0.04416332021355629 Validation loss 0.04378538951277733 Accuracy 0.5595703125\n",
      "Iteration 6520 Training loss 0.04257936403155327 Validation loss 0.04328371584415436 Accuracy 0.56494140625\n",
      "Iteration 6530 Training loss 0.04269992932677269 Validation loss 0.04343393072485924 Accuracy 0.5634765625\n",
      "Iteration 6540 Training loss 0.045132093131542206 Validation loss 0.043298739939928055 Accuracy 0.56396484375\n",
      "Iteration 6550 Training loss 0.03964397683739662 Validation loss 0.043249789625406265 Accuracy 0.564453125\n",
      "Iteration 6560 Training loss 0.03932492062449455 Validation loss 0.04385657235980034 Accuracy 0.55859375\n",
      "Iteration 6570 Training loss 0.043512724339962006 Validation loss 0.0433238260447979 Accuracy 0.564453125\n",
      "Iteration 6580 Training loss 0.04131733626127243 Validation loss 0.043444663286209106 Accuracy 0.56298828125\n",
      "Iteration 6590 Training loss 0.04101685434579849 Validation loss 0.043251123279333115 Accuracy 0.5654296875\n",
      "Iteration 6600 Training loss 0.0403977632522583 Validation loss 0.04325709491968155 Accuracy 0.564453125\n",
      "Iteration 6610 Training loss 0.04678776115179062 Validation loss 0.04484027251601219 Accuracy 0.54638671875\n",
      "Iteration 6620 Training loss 0.04102420061826706 Validation loss 0.04336600378155708 Accuracy 0.56298828125\n",
      "Iteration 6630 Training loss 0.044677942991256714 Validation loss 0.043444059789180756 Accuracy 0.56298828125\n",
      "Iteration 6640 Training loss 0.04021087661385536 Validation loss 0.043140750378370285 Accuracy 0.56591796875\n",
      "Iteration 6650 Training loss 0.04397616162896156 Validation loss 0.04338927939534187 Accuracy 0.56298828125\n",
      "Iteration 6660 Training loss 0.04211941361427307 Validation loss 0.043279461562633514 Accuracy 0.56494140625\n",
      "Iteration 6670 Training loss 0.04069723188877106 Validation loss 0.043291736394166946 Accuracy 0.56396484375\n",
      "Iteration 6680 Training loss 0.04049192741513252 Validation loss 0.0432584248483181 Accuracy 0.56494140625\n",
      "Iteration 6690 Training loss 0.04088589921593666 Validation loss 0.043315574526786804 Accuracy 0.564453125\n",
      "Iteration 6700 Training loss 0.0404435433447361 Validation loss 0.04329198598861694 Accuracy 0.56396484375\n",
      "Iteration 6710 Training loss 0.043848711997270584 Validation loss 0.04360860213637352 Accuracy 0.56201171875\n",
      "Iteration 6720 Training loss 0.043539345264434814 Validation loss 0.043144043534994125 Accuracy 0.56591796875\n",
      "Iteration 6730 Training loss 0.04265022277832031 Validation loss 0.043233517557382584 Accuracy 0.5654296875\n",
      "Iteration 6740 Training loss 0.04479668289422989 Validation loss 0.043289653956890106 Accuracy 0.56396484375\n",
      "Iteration 6750 Training loss 0.046896208077669144 Validation loss 0.04328397288918495 Accuracy 0.56396484375\n",
      "Iteration 6760 Training loss 0.04333856329321861 Validation loss 0.04321281239390373 Accuracy 0.5654296875\n",
      "Iteration 6770 Training loss 0.03888782113790512 Validation loss 0.04332779347896576 Accuracy 0.564453125\n",
      "Iteration 6780 Training loss 0.04163413122296333 Validation loss 0.043346110731363297 Accuracy 0.564453125\n",
      "Iteration 6790 Training loss 0.03858521580696106 Validation loss 0.043275631964206696 Accuracy 0.56494140625\n",
      "Iteration 6800 Training loss 0.039953336119651794 Validation loss 0.04328605905175209 Accuracy 0.56494140625\n",
      "Iteration 6810 Training loss 0.041643913835287094 Validation loss 0.04337272047996521 Accuracy 0.564453125\n",
      "Iteration 6820 Training loss 0.043192341923713684 Validation loss 0.043240223079919815 Accuracy 0.56494140625\n",
      "Iteration 6830 Training loss 0.040740612894296646 Validation loss 0.04355953633785248 Accuracy 0.56201171875\n",
      "Iteration 6840 Training loss 0.0430842787027359 Validation loss 0.043437495827674866 Accuracy 0.56298828125\n",
      "Iteration 6850 Training loss 0.04361387714743614 Validation loss 0.04323707893490791 Accuracy 0.56494140625\n",
      "Iteration 6860 Training loss 0.0422658696770668 Validation loss 0.04384473338723183 Accuracy 0.5595703125\n",
      "Iteration 6870 Training loss 0.04056677222251892 Validation loss 0.04374660924077034 Accuracy 0.560546875\n",
      "Iteration 6880 Training loss 0.043355897068977356 Validation loss 0.043220486491918564 Accuracy 0.5654296875\n",
      "Iteration 6890 Training loss 0.04407597333192825 Validation loss 0.04336429014801979 Accuracy 0.56396484375\n",
      "Iteration 6900 Training loss 0.042523324489593506 Validation loss 0.04338724538683891 Accuracy 0.56396484375\n",
      "Iteration 6910 Training loss 0.04416464641690254 Validation loss 0.043353110551834106 Accuracy 0.5634765625\n",
      "Iteration 6920 Training loss 0.043853215873241425 Validation loss 0.043222807347774506 Accuracy 0.5654296875\n",
      "Iteration 6930 Training loss 0.041959892958402634 Validation loss 0.04319210723042488 Accuracy 0.56640625\n",
      "Iteration 6940 Training loss 0.043564990162849426 Validation loss 0.04315679147839546 Accuracy 0.5654296875\n",
      "Iteration 6950 Training loss 0.04276739060878754 Validation loss 0.04376290738582611 Accuracy 0.55908203125\n",
      "Iteration 6960 Training loss 0.042916860431432724 Validation loss 0.04314257577061653 Accuracy 0.56689453125\n",
      "Iteration 6970 Training loss 0.04371863231062889 Validation loss 0.04334307089447975 Accuracy 0.564453125\n",
      "Iteration 6980 Training loss 0.042860910296440125 Validation loss 0.04309713840484619 Accuracy 0.56689453125\n",
      "Iteration 6990 Training loss 0.042069654911756516 Validation loss 0.04332416504621506 Accuracy 0.564453125\n",
      "Iteration 7000 Training loss 0.043238889425992966 Validation loss 0.04371804744005203 Accuracy 0.560546875\n",
      "Iteration 7010 Training loss 0.04361599311232567 Validation loss 0.04307008534669876 Accuracy 0.56591796875\n",
      "Iteration 7020 Training loss 0.03956977277994156 Validation loss 0.043512653559446335 Accuracy 0.56298828125\n",
      "Iteration 7030 Training loss 0.0402899831533432 Validation loss 0.043272487819194794 Accuracy 0.56494140625\n",
      "Iteration 7040 Training loss 0.0396568663418293 Validation loss 0.043158821761608124 Accuracy 0.56494140625\n",
      "Iteration 7050 Training loss 0.041540175676345825 Validation loss 0.04387228563427925 Accuracy 0.55908203125\n",
      "Iteration 7060 Training loss 0.04358837753534317 Validation loss 0.04325960949063301 Accuracy 0.56494140625\n",
      "Iteration 7070 Training loss 0.04142390564084053 Validation loss 0.043290652334690094 Accuracy 0.564453125\n",
      "Iteration 7080 Training loss 0.04300752282142639 Validation loss 0.04336153343319893 Accuracy 0.56298828125\n",
      "Iteration 7090 Training loss 0.04201710969209671 Validation loss 0.04318784922361374 Accuracy 0.56640625\n",
      "Iteration 7100 Training loss 0.04108183830976486 Validation loss 0.04408036917448044 Accuracy 0.55517578125\n",
      "Iteration 7110 Training loss 0.04436345398426056 Validation loss 0.04318756237626076 Accuracy 0.56591796875\n",
      "Iteration 7120 Training loss 0.03786690905690193 Validation loss 0.04329534247517586 Accuracy 0.564453125\n",
      "Iteration 7130 Training loss 0.04000284895300865 Validation loss 0.0430821068584919 Accuracy 0.56689453125\n",
      "Iteration 7140 Training loss 0.04424574598670006 Validation loss 0.0432329960167408 Accuracy 0.56591796875\n",
      "Iteration 7150 Training loss 0.044310517609119415 Validation loss 0.043477728962898254 Accuracy 0.56201171875\n",
      "Iteration 7160 Training loss 0.04064960032701492 Validation loss 0.0434543751180172 Accuracy 0.56298828125\n",
      "Iteration 7170 Training loss 0.0394066721200943 Validation loss 0.0431642085313797 Accuracy 0.56591796875\n",
      "Iteration 7180 Training loss 0.043284524232149124 Validation loss 0.0431886725127697 Accuracy 0.56494140625\n",
      "Iteration 7190 Training loss 0.044982872903347015 Validation loss 0.04328560456633568 Accuracy 0.564453125\n",
      "Iteration 7200 Training loss 0.04032652825117111 Validation loss 0.04315822571516037 Accuracy 0.56494140625\n",
      "Iteration 7210 Training loss 0.045394331216812134 Validation loss 0.04384686052799225 Accuracy 0.55908203125\n",
      "Iteration 7220 Training loss 0.04193776473402977 Validation loss 0.04307945817708969 Accuracy 0.56689453125\n",
      "Iteration 7230 Training loss 0.04908491298556328 Validation loss 0.043107517063617706 Accuracy 0.56640625\n",
      "Iteration 7240 Training loss 0.04092545062303543 Validation loss 0.04380403831601143 Accuracy 0.55810546875\n",
      "Iteration 7250 Training loss 0.04229572042822838 Validation loss 0.043194565922021866 Accuracy 0.5654296875\n",
      "Iteration 7260 Training loss 0.040196869522333145 Validation loss 0.043126318603754044 Accuracy 0.56591796875\n",
      "Iteration 7270 Training loss 0.03778409957885742 Validation loss 0.043312713503837585 Accuracy 0.564453125\n",
      "Iteration 7280 Training loss 0.040785688906908035 Validation loss 0.04324563220143318 Accuracy 0.564453125\n",
      "Iteration 7290 Training loss 0.04418798163533211 Validation loss 0.04322313889861107 Accuracy 0.56494140625\n",
      "Iteration 7300 Training loss 0.040572114288806915 Validation loss 0.043484169989824295 Accuracy 0.56298828125\n",
      "Iteration 7310 Training loss 0.0444481298327446 Validation loss 0.04334083944559097 Accuracy 0.564453125\n",
      "Iteration 7320 Training loss 0.04198278859257698 Validation loss 0.043395012617111206 Accuracy 0.56396484375\n",
      "Iteration 7330 Training loss 0.04438719525933266 Validation loss 0.04341266676783562 Accuracy 0.5615234375\n",
      "Iteration 7340 Training loss 0.04336727410554886 Validation loss 0.044072557240724564 Accuracy 0.556640625\n",
      "Iteration 7350 Training loss 0.04744648188352585 Validation loss 0.043337997049093246 Accuracy 0.56396484375\n",
      "Iteration 7360 Training loss 0.04633597657084465 Validation loss 0.0433005653321743 Accuracy 0.56396484375\n",
      "Iteration 7370 Training loss 0.0407690666615963 Validation loss 0.043212439864873886 Accuracy 0.564453125\n",
      "Iteration 7380 Training loss 0.0441393107175827 Validation loss 0.04521939158439636 Accuracy 0.5458984375\n",
      "Iteration 7390 Training loss 0.0408213846385479 Validation loss 0.04331042245030403 Accuracy 0.56494140625\n",
      "Iteration 7400 Training loss 0.04385770112276077 Validation loss 0.04330909997224808 Accuracy 0.56396484375\n",
      "Iteration 7410 Training loss 0.042707398533821106 Validation loss 0.043173182755708694 Accuracy 0.5654296875\n",
      "Iteration 7420 Training loss 0.03905995935201645 Validation loss 0.043176375329494476 Accuracy 0.56591796875\n",
      "Iteration 7430 Training loss 0.04523445665836334 Validation loss 0.043124180287122726 Accuracy 0.56591796875\n",
      "Iteration 7440 Training loss 0.04345695301890373 Validation loss 0.043506741523742676 Accuracy 0.56298828125\n",
      "Iteration 7450 Training loss 0.0435703843832016 Validation loss 0.0432438962161541 Accuracy 0.5654296875\n",
      "Iteration 7460 Training loss 0.04289453104138374 Validation loss 0.043240029364824295 Accuracy 0.564453125\n",
      "Iteration 7470 Training loss 0.04009844362735748 Validation loss 0.043764665722846985 Accuracy 0.5595703125\n",
      "Iteration 7480 Training loss 0.04277695715427399 Validation loss 0.043130580335855484 Accuracy 0.5654296875\n",
      "Iteration 7490 Training loss 0.03926517441868782 Validation loss 0.04324548318982124 Accuracy 0.5654296875\n",
      "Iteration 7500 Training loss 0.04202110320329666 Validation loss 0.04328097403049469 Accuracy 0.5634765625\n",
      "Iteration 7510 Training loss 0.04142477363348007 Validation loss 0.043168846517801285 Accuracy 0.56640625\n",
      "Iteration 7520 Training loss 0.04606826230883598 Validation loss 0.04316011071205139 Accuracy 0.5654296875\n",
      "Iteration 7530 Training loss 0.04556075856089592 Validation loss 0.04332783445715904 Accuracy 0.56396484375\n",
      "Iteration 7540 Training loss 0.040866926312446594 Validation loss 0.04304169863462448 Accuracy 0.5673828125\n",
      "Iteration 7550 Training loss 0.04365493729710579 Validation loss 0.045393962413072586 Accuracy 0.54052734375\n",
      "Iteration 7560 Training loss 0.040575336664915085 Validation loss 0.04329819604754448 Accuracy 0.5654296875\n",
      "Iteration 7570 Training loss 0.04128510504961014 Validation loss 0.0434698723256588 Accuracy 0.5634765625\n",
      "Iteration 7580 Training loss 0.042124420404434204 Validation loss 0.04403916746377945 Accuracy 0.5556640625\n",
      "Iteration 7590 Training loss 0.042582910507917404 Validation loss 0.043384429067373276 Accuracy 0.56396484375\n",
      "Iteration 7600 Training loss 0.044653188437223434 Validation loss 0.0431240014731884 Accuracy 0.56689453125\n",
      "Iteration 7610 Training loss 0.039150435477495193 Validation loss 0.04306132718920708 Accuracy 0.56689453125\n",
      "Iteration 7620 Training loss 0.0409904383122921 Validation loss 0.04349743202328682 Accuracy 0.5625\n",
      "Iteration 7630 Training loss 0.040719445794820786 Validation loss 0.043192774057388306 Accuracy 0.56591796875\n",
      "Iteration 7640 Training loss 0.041621532291173935 Validation loss 0.043188631534576416 Accuracy 0.5654296875\n",
      "Iteration 7650 Training loss 0.044559840112924576 Validation loss 0.043143659830093384 Accuracy 0.56591796875\n",
      "Iteration 7660 Training loss 0.04443100839853287 Validation loss 0.043231043964624405 Accuracy 0.5654296875\n",
      "Iteration 7670 Training loss 0.04314996674656868 Validation loss 0.04324198141694069 Accuracy 0.564453125\n",
      "Iteration 7680 Training loss 0.04084593430161476 Validation loss 0.04317209869623184 Accuracy 0.5654296875\n",
      "Iteration 7690 Training loss 0.04228931665420532 Validation loss 0.043012041598558426 Accuracy 0.5673828125\n",
      "Iteration 7700 Training loss 0.043660130351781845 Validation loss 0.04322425648570061 Accuracy 0.5654296875\n",
      "Iteration 7710 Training loss 0.04413532838225365 Validation loss 0.04313872009515762 Accuracy 0.5654296875\n",
      "Iteration 7720 Training loss 0.04378468915820122 Validation loss 0.0430525466799736 Accuracy 0.56689453125\n",
      "Iteration 7730 Training loss 0.04206835851073265 Validation loss 0.043052781373262405 Accuracy 0.56689453125\n",
      "Iteration 7740 Training loss 0.0429808534681797 Validation loss 0.04328816756606102 Accuracy 0.5654296875\n",
      "Iteration 7750 Training loss 0.042242925614118576 Validation loss 0.043280959129333496 Accuracy 0.56494140625\n",
      "Iteration 7760 Training loss 0.04077478125691414 Validation loss 0.04322265833616257 Accuracy 0.5654296875\n",
      "Iteration 7770 Training loss 0.04195789247751236 Validation loss 0.04366713762283325 Accuracy 0.56005859375\n",
      "Iteration 7780 Training loss 0.04191127419471741 Validation loss 0.04335755109786987 Accuracy 0.5634765625\n",
      "Iteration 7790 Training loss 0.04396376013755798 Validation loss 0.04324539378285408 Accuracy 0.5654296875\n",
      "Iteration 7800 Training loss 0.04173662140965462 Validation loss 0.043213680386543274 Accuracy 0.5654296875\n",
      "Iteration 7810 Training loss 0.04772746190428734 Validation loss 0.043089792132377625 Accuracy 0.56640625\n",
      "Iteration 7820 Training loss 0.04366688430309296 Validation loss 0.04313652589917183 Accuracy 0.56591796875\n",
      "Iteration 7830 Training loss 0.04123613238334656 Validation loss 0.0434856042265892 Accuracy 0.56201171875\n",
      "Iteration 7840 Training loss 0.04318205639719963 Validation loss 0.043187644332647324 Accuracy 0.5654296875\n",
      "Iteration 7850 Training loss 0.04021495580673218 Validation loss 0.043114300817251205 Accuracy 0.56640625\n",
      "Iteration 7860 Training loss 0.04055286943912506 Validation loss 0.043742772191762924 Accuracy 0.560546875\n",
      "Iteration 7870 Training loss 0.04197251424193382 Validation loss 0.04322744905948639 Accuracy 0.5654296875\n",
      "Iteration 7880 Training loss 0.04263540729880333 Validation loss 0.04353820160031319 Accuracy 0.5625\n",
      "Iteration 7890 Training loss 0.044672705233097076 Validation loss 0.04327817261219025 Accuracy 0.5654296875\n",
      "Iteration 7900 Training loss 0.03857012838125229 Validation loss 0.04321373999118805 Accuracy 0.5654296875\n",
      "Iteration 7910 Training loss 0.04530772939324379 Validation loss 0.04363913834095001 Accuracy 0.5615234375\n",
      "Iteration 7920 Training loss 0.04650783911347389 Validation loss 0.043075453490018845 Accuracy 0.5673828125\n",
      "Iteration 7930 Training loss 0.04415283724665642 Validation loss 0.04321960359811783 Accuracy 0.56591796875\n",
      "Iteration 7940 Training loss 0.04061025381088257 Validation loss 0.04303472116589546 Accuracy 0.56689453125\n",
      "Iteration 7950 Training loss 0.04603349789977074 Validation loss 0.043068692088127136 Accuracy 0.56689453125\n",
      "Iteration 7960 Training loss 0.041944291442632675 Validation loss 0.0431927926838398 Accuracy 0.5654296875\n",
      "Iteration 7970 Training loss 0.047642942517995834 Validation loss 0.04310629144310951 Accuracy 0.56689453125\n",
      "Iteration 7980 Training loss 0.04435024410486221 Validation loss 0.04322030767798424 Accuracy 0.5654296875\n",
      "Iteration 7990 Training loss 0.04108356311917305 Validation loss 0.04314037039875984 Accuracy 0.56591796875\n",
      "Iteration 8000 Training loss 0.040963396430015564 Validation loss 0.04323422908782959 Accuracy 0.5654296875\n",
      "Iteration 8010 Training loss 0.04355541989207268 Validation loss 0.043293606489896774 Accuracy 0.56494140625\n",
      "Iteration 8020 Training loss 0.03987034037709236 Validation loss 0.043246228247880936 Accuracy 0.56494140625\n",
      "Iteration 8030 Training loss 0.038285426795482635 Validation loss 0.04322308301925659 Accuracy 0.56396484375\n",
      "Iteration 8040 Training loss 0.04228224232792854 Validation loss 0.043343476951122284 Accuracy 0.56298828125\n",
      "Iteration 8050 Training loss 0.03896481916308403 Validation loss 0.04315388202667236 Accuracy 0.56591796875\n",
      "Iteration 8060 Training loss 0.04181836545467377 Validation loss 0.0430755652487278 Accuracy 0.5654296875\n",
      "Iteration 8070 Training loss 0.04096926376223564 Validation loss 0.04316705837845802 Accuracy 0.56591796875\n",
      "Iteration 8080 Training loss 0.03789253532886505 Validation loss 0.043066833168268204 Accuracy 0.56640625\n",
      "Iteration 8090 Training loss 0.045253507792949677 Validation loss 0.043543606996536255 Accuracy 0.56103515625\n",
      "Iteration 8100 Training loss 0.04624457284808159 Validation loss 0.04310474544763565 Accuracy 0.56689453125\n",
      "Iteration 8110 Training loss 0.042308058589696884 Validation loss 0.043210580945014954 Accuracy 0.56494140625\n",
      "Iteration 8120 Training loss 0.039464619010686874 Validation loss 0.04308808967471123 Accuracy 0.56591796875\n",
      "Iteration 8130 Training loss 0.04198272526264191 Validation loss 0.043015774339437485 Accuracy 0.56689453125\n",
      "Iteration 8140 Training loss 0.04228578135371208 Validation loss 0.04321255907416344 Accuracy 0.56494140625\n",
      "Iteration 8150 Training loss 0.04619087651371956 Validation loss 0.04308825731277466 Accuracy 0.56640625\n",
      "Iteration 8160 Training loss 0.04006558656692505 Validation loss 0.043271444737911224 Accuracy 0.56396484375\n",
      "Iteration 8170 Training loss 0.04542708769440651 Validation loss 0.04300537332892418 Accuracy 0.56689453125\n",
      "Iteration 8180 Training loss 0.04431096464395523 Validation loss 0.044120706617832184 Accuracy 0.5537109375\n",
      "Iteration 8190 Training loss 0.04247013479471207 Validation loss 0.04333854466676712 Accuracy 0.5634765625\n",
      "Iteration 8200 Training loss 0.04527119919657707 Validation loss 0.04307642579078674 Accuracy 0.56689453125\n",
      "Iteration 8210 Training loss 0.040523961186409 Validation loss 0.043003398925065994 Accuracy 0.56640625\n",
      "Iteration 8220 Training loss 0.04175518453121185 Validation loss 0.042752839624881744 Accuracy 0.5693359375\n",
      "Iteration 8230 Training loss 0.04050477594137192 Validation loss 0.039897650480270386 Accuracy 0.59619140625\n",
      "Iteration 8240 Training loss 0.03773167356848717 Validation loss 0.03920770809054375 Accuracy 0.60302734375\n",
      "Iteration 8250 Training loss 0.038143787533044815 Validation loss 0.03834013640880585 Accuracy 0.6123046875\n",
      "Iteration 8260 Training loss 0.03841356188058853 Validation loss 0.03938119858503342 Accuracy 0.6005859375\n",
      "Iteration 8270 Training loss 0.04320228099822998 Validation loss 0.041282009333372116 Accuracy 0.5830078125\n",
      "Iteration 8280 Training loss 0.039601393043994904 Validation loss 0.03805163875222206 Accuracy 0.61572265625\n",
      "Iteration 8290 Training loss 0.038579005748033524 Validation loss 0.03845644369721413 Accuracy 0.611328125\n",
      "Iteration 8300 Training loss 0.03678404912352562 Validation loss 0.03791052848100662 Accuracy 0.6171875\n",
      "Iteration 8310 Training loss 0.033202216029167175 Validation loss 0.03824671730399132 Accuracy 0.61328125\n",
      "Iteration 8320 Training loss 0.03768433257937431 Validation loss 0.03767964616417885 Accuracy 0.6181640625\n",
      "Iteration 8330 Training loss 0.04178687185049057 Validation loss 0.041249725967645645 Accuracy 0.58349609375\n",
      "Iteration 8340 Training loss 0.034416284412145615 Validation loss 0.038171637803316116 Accuracy 0.61376953125\n",
      "Iteration 8350 Training loss 0.039306145161390305 Validation loss 0.03740100562572479 Accuracy 0.6220703125\n",
      "Iteration 8360 Training loss 0.038559578359127045 Validation loss 0.03719455748796463 Accuracy 0.62353515625\n",
      "Iteration 8370 Training loss 0.041599538177251816 Validation loss 0.03739757835865021 Accuracy 0.62158203125\n",
      "Iteration 8380 Training loss 0.033534903079271317 Validation loss 0.03782233968377113 Accuracy 0.61474609375\n",
      "Iteration 8390 Training loss 0.038152750581502914 Validation loss 0.03703189641237259 Accuracy 0.6240234375\n",
      "Iteration 8400 Training loss 0.03341054171323776 Validation loss 0.03741449490189552 Accuracy 0.62158203125\n",
      "Iteration 8410 Training loss 0.04014609754085541 Validation loss 0.03910258039832115 Accuracy 0.60498046875\n",
      "Iteration 8420 Training loss 0.0355740562081337 Validation loss 0.03712834417819977 Accuracy 0.62451171875\n",
      "Iteration 8430 Training loss 0.03766486421227455 Validation loss 0.037107620388269424 Accuracy 0.62353515625\n",
      "Iteration 8440 Training loss 0.03800297901034355 Validation loss 0.038298796862363815 Accuracy 0.61328125\n",
      "Iteration 8450 Training loss 0.03822198882699013 Validation loss 0.03765014186501503 Accuracy 0.619140625\n",
      "Iteration 8460 Training loss 0.030071627348661423 Validation loss 0.03380068764090538 Accuracy 0.6572265625\n",
      "Iteration 8470 Training loss 0.030906908214092255 Validation loss 0.03271043300628662 Accuracy 0.66748046875\n",
      "Iteration 8480 Training loss 0.03528813272714615 Validation loss 0.03406136482954025 Accuracy 0.654296875\n",
      "Iteration 8490 Training loss 0.02987416833639145 Validation loss 0.031443167477846146 Accuracy 0.68115234375\n",
      "Iteration 8500 Training loss 0.03209822624921799 Validation loss 0.03240460529923439 Accuracy 0.67138671875\n",
      "Iteration 8510 Training loss 0.03151990845799446 Validation loss 0.031148666515946388 Accuracy 0.68408203125\n",
      "Iteration 8520 Training loss 0.03198127821087837 Validation loss 0.033566106110811234 Accuracy 0.6611328125\n",
      "Iteration 8530 Training loss 0.03394906967878342 Validation loss 0.03304971009492874 Accuracy 0.6650390625\n",
      "Iteration 8540 Training loss 0.026961244642734528 Validation loss 0.031042026355862617 Accuracy 0.6845703125\n",
      "Iteration 8550 Training loss 0.03058205172419548 Validation loss 0.0317130945622921 Accuracy 0.67919921875\n",
      "Iteration 8560 Training loss 0.025671638548374176 Validation loss 0.030870191752910614 Accuracy 0.6865234375\n",
      "Iteration 8570 Training loss 0.029344908893108368 Validation loss 0.031387750059366226 Accuracy 0.6826171875\n",
      "Iteration 8580 Training loss 0.0302288718521595 Validation loss 0.03089701384305954 Accuracy 0.68701171875\n",
      "Iteration 8590 Training loss 0.02871478535234928 Validation loss 0.031525708734989166 Accuracy 0.6806640625\n",
      "Iteration 8600 Training loss 0.0302534531801939 Validation loss 0.030997060239315033 Accuracy 0.68603515625\n",
      "Iteration 8610 Training loss 0.026771031320095062 Validation loss 0.032440926879644394 Accuracy 0.669921875\n",
      "Iteration 8620 Training loss 0.028577031567692757 Validation loss 0.03081628866493702 Accuracy 0.6875\n",
      "Iteration 8630 Training loss 0.03246412053704262 Validation loss 0.0308830626308918 Accuracy 0.6875\n",
      "Iteration 8640 Training loss 0.02670522965490818 Validation loss 0.03105880506336689 Accuracy 0.685546875\n",
      "Iteration 8650 Training loss 0.032800693064928055 Validation loss 0.03207637369632721 Accuracy 0.67626953125\n",
      "Iteration 8660 Training loss 0.02974938414990902 Validation loss 0.032664839178323746 Accuracy 0.66943359375\n",
      "Iteration 8670 Training loss 0.027017414569854736 Validation loss 0.030864106491208076 Accuracy 0.68701171875\n",
      "Iteration 8680 Training loss 0.0290216114372015 Validation loss 0.03083842247724533 Accuracy 0.68798828125\n",
      "Iteration 8690 Training loss 0.03404190391302109 Validation loss 0.031073803082108498 Accuracy 0.6845703125\n",
      "Iteration 8700 Training loss 0.02975587733089924 Validation loss 0.03089129365980625 Accuracy 0.68701171875\n",
      "Iteration 8710 Training loss 0.030919011682271957 Validation loss 0.03068682551383972 Accuracy 0.68994140625\n",
      "Iteration 8720 Training loss 0.02969144657254219 Validation loss 0.03152291849255562 Accuracy 0.68017578125\n",
      "Iteration 8730 Training loss 0.028010554611682892 Validation loss 0.0317261777818203 Accuracy 0.6787109375\n",
      "Iteration 8740 Training loss 0.030035201460123062 Validation loss 0.03141464665532112 Accuracy 0.68212890625\n",
      "Iteration 8750 Training loss 0.03341350331902504 Validation loss 0.034201547503471375 Accuracy 0.6533203125\n",
      "Iteration 8760 Training loss 0.025813722983002663 Validation loss 0.030939247459173203 Accuracy 0.68701171875\n",
      "Iteration 8770 Training loss 0.030329599976539612 Validation loss 0.031232690438628197 Accuracy 0.68408203125\n",
      "Iteration 8780 Training loss 0.029577407985925674 Validation loss 0.031094595789909363 Accuracy 0.6845703125\n",
      "Iteration 8790 Training loss 0.027458880096673965 Validation loss 0.030725590884685516 Accuracy 0.68994140625\n",
      "Iteration 8800 Training loss 0.030553624033927917 Validation loss 0.03204342722892761 Accuracy 0.67529296875\n",
      "Iteration 8810 Training loss 0.027991322800517082 Validation loss 0.03127129748463631 Accuracy 0.6826171875\n",
      "Iteration 8820 Training loss 0.029203779995441437 Validation loss 0.031279947608709335 Accuracy 0.6826171875\n",
      "Iteration 8830 Training loss 0.029920602217316628 Validation loss 0.03125227987766266 Accuracy 0.6845703125\n",
      "Iteration 8840 Training loss 0.03457140177488327 Validation loss 0.031768009066581726 Accuracy 0.6787109375\n",
      "Iteration 8850 Training loss 0.02693607285618782 Validation loss 0.030350307002663612 Accuracy 0.69287109375\n",
      "Iteration 8860 Training loss 0.028093622997403145 Validation loss 0.03197396546602249 Accuracy 0.6767578125\n",
      "Iteration 8870 Training loss 0.031124765053391457 Validation loss 0.030624516308307648 Accuracy 0.68994140625\n",
      "Iteration 8880 Training loss 0.028466390445828438 Validation loss 0.030645031481981277 Accuracy 0.68994140625\n",
      "Iteration 8890 Training loss 0.0312810055911541 Validation loss 0.03299254924058914 Accuracy 0.66552734375\n",
      "Iteration 8900 Training loss 0.02853880077600479 Validation loss 0.03067472018301487 Accuracy 0.689453125\n",
      "Iteration 8910 Training loss 0.029666511341929436 Validation loss 0.030826104804873466 Accuracy 0.6884765625\n",
      "Iteration 8920 Training loss 0.029731491580605507 Validation loss 0.03061075694859028 Accuracy 0.68994140625\n",
      "Iteration 8930 Training loss 0.03140789642930031 Validation loss 0.030906684696674347 Accuracy 0.6865234375\n",
      "Iteration 8940 Training loss 0.02946893312036991 Validation loss 0.03113183006644249 Accuracy 0.6845703125\n",
      "Iteration 8950 Training loss 0.027554916217923164 Validation loss 0.031256288290023804 Accuracy 0.68408203125\n",
      "Iteration 8960 Training loss 0.026649678125977516 Validation loss 0.030477313324809074 Accuracy 0.69140625\n",
      "Iteration 8970 Training loss 0.029398879036307335 Validation loss 0.030042758211493492 Accuracy 0.6953125\n",
      "Iteration 8980 Training loss 0.024899261072278023 Validation loss 0.030262470245361328 Accuracy 0.69287109375\n",
      "Iteration 8990 Training loss 0.027974344789981842 Validation loss 0.0303341057151556 Accuracy 0.6923828125\n",
      "Iteration 9000 Training loss 0.030281368643045425 Validation loss 0.030665580183267593 Accuracy 0.6884765625\n",
      "Iteration 9010 Training loss 0.032385990023612976 Validation loss 0.03154084458947182 Accuracy 0.68017578125\n",
      "Iteration 9020 Training loss 0.030199239030480385 Validation loss 0.03050048276782036 Accuracy 0.69140625\n",
      "Iteration 9030 Training loss 0.027850208804011345 Validation loss 0.030868861824274063 Accuracy 0.6875\n",
      "Iteration 9040 Training loss 0.0269270371645689 Validation loss 0.030506841838359833 Accuracy 0.69140625\n",
      "Iteration 9050 Training loss 0.029846053570508957 Validation loss 0.0310811810195446 Accuracy 0.68505859375\n",
      "Iteration 9060 Training loss 0.030263585969805717 Validation loss 0.030233129858970642 Accuracy 0.69384765625\n",
      "Iteration 9070 Training loss 0.03140707686543465 Validation loss 0.030015133321285248 Accuracy 0.6962890625\n",
      "Iteration 9080 Training loss 0.02896902896463871 Validation loss 0.030163470655679703 Accuracy 0.6953125\n",
      "Iteration 9090 Training loss 0.030344896018505096 Validation loss 0.030382821336388588 Accuracy 0.6923828125\n",
      "Iteration 9100 Training loss 0.027408357709646225 Validation loss 0.030070966109633446 Accuracy 0.69580078125\n",
      "Iteration 9110 Training loss 0.029618844389915466 Validation loss 0.029910948127508163 Accuracy 0.697265625\n",
      "Iteration 9120 Training loss 0.029272057116031647 Validation loss 0.031233690679073334 Accuracy 0.68310546875\n",
      "Iteration 9130 Training loss 0.030593764036893845 Validation loss 0.029945023357868195 Accuracy 0.69677734375\n",
      "Iteration 9140 Training loss 0.032367266714572906 Validation loss 0.0304417684674263 Accuracy 0.69140625\n",
      "Iteration 9150 Training loss 0.03423001244664192 Validation loss 0.0324026495218277 Accuracy 0.67138671875\n",
      "Iteration 9160 Training loss 0.028066502884030342 Validation loss 0.030301200225949287 Accuracy 0.693359375\n",
      "Iteration 9170 Training loss 0.03015437349677086 Validation loss 0.031605977565050125 Accuracy 0.6806640625\n",
      "Iteration 9180 Training loss 0.03158680349588394 Validation loss 0.030019517987966537 Accuracy 0.6962890625\n",
      "Iteration 9190 Training loss 0.02671239897608757 Validation loss 0.02973683923482895 Accuracy 0.6982421875\n",
      "Iteration 9200 Training loss 0.030168449506163597 Validation loss 0.029730234295129776 Accuracy 0.69873046875\n",
      "Iteration 9210 Training loss 0.027281545102596283 Validation loss 0.029796317219734192 Accuracy 0.6982421875\n",
      "Iteration 9220 Training loss 0.028954429551959038 Validation loss 0.03028632327914238 Accuracy 0.6923828125\n",
      "Iteration 9230 Training loss 0.028463881462812424 Validation loss 0.030447226017713547 Accuracy 0.69140625\n",
      "Iteration 9240 Training loss 0.03019051067531109 Validation loss 0.029783841222524643 Accuracy 0.6982421875\n",
      "Iteration 9250 Training loss 0.02632126584649086 Validation loss 0.02984354831278324 Accuracy 0.69775390625\n",
      "Iteration 9260 Training loss 0.029209082946181297 Validation loss 0.02999621070921421 Accuracy 0.69580078125\n",
      "Iteration 9270 Training loss 0.031089220196008682 Validation loss 0.030377868562936783 Accuracy 0.69189453125\n",
      "Iteration 9280 Training loss 0.03058811090886593 Validation loss 0.029910050332546234 Accuracy 0.697265625\n",
      "Iteration 9290 Training loss 0.028206568211317062 Validation loss 0.030263667926192284 Accuracy 0.69384765625\n",
      "Iteration 9300 Training loss 0.02552441693842411 Validation loss 0.029833776876330376 Accuracy 0.6982421875\n",
      "Iteration 9310 Training loss 0.028994055464863777 Validation loss 0.029928412288427353 Accuracy 0.69677734375\n",
      "Iteration 9320 Training loss 0.028744185343384743 Validation loss 0.029779130592942238 Accuracy 0.69873046875\n",
      "Iteration 9330 Training loss 0.029308868572115898 Validation loss 0.030800526961684227 Accuracy 0.68798828125\n",
      "Iteration 9340 Training loss 0.030633578076958656 Validation loss 0.030335716903209686 Accuracy 0.69287109375\n",
      "Iteration 9350 Training loss 0.029837317764759064 Validation loss 0.030019203200936317 Accuracy 0.69580078125\n",
      "Iteration 9360 Training loss 0.027393193915486336 Validation loss 0.029606759548187256 Accuracy 0.70068359375\n",
      "Iteration 9370 Training loss 0.030379079282283783 Validation loss 0.029860451817512512 Accuracy 0.69775390625\n",
      "Iteration 9380 Training loss 0.027844883501529694 Validation loss 0.030394522473216057 Accuracy 0.6923828125\n",
      "Iteration 9390 Training loss 0.03293190896511078 Validation loss 0.031050067394971848 Accuracy 0.685546875\n",
      "Iteration 9400 Training loss 0.028897305950522423 Validation loss 0.031130753457546234 Accuracy 0.685546875\n",
      "Iteration 9410 Training loss 0.030513977631926537 Validation loss 0.029883431270718575 Accuracy 0.69677734375\n",
      "Iteration 9420 Training loss 0.025816431269049644 Validation loss 0.029900038614869118 Accuracy 0.697265625\n",
      "Iteration 9430 Training loss 0.03164377436041832 Validation loss 0.0296730175614357 Accuracy 0.7001953125\n",
      "Iteration 9440 Training loss 0.03188960254192352 Validation loss 0.03018089197576046 Accuracy 0.6943359375\n",
      "Iteration 9450 Training loss 0.028814740478992462 Validation loss 0.02995886653661728 Accuracy 0.69580078125\n",
      "Iteration 9460 Training loss 0.02650732919573784 Validation loss 0.029828080907464027 Accuracy 0.6982421875\n",
      "Iteration 9470 Training loss 0.03019300289452076 Validation loss 0.03193642571568489 Accuracy 0.6767578125\n",
      "Iteration 9480 Training loss 0.02718847058713436 Validation loss 0.030103769153356552 Accuracy 0.6953125\n",
      "Iteration 9490 Training loss 0.027018271386623383 Validation loss 0.030186444520950317 Accuracy 0.69384765625\n",
      "Iteration 9500 Training loss 0.027387259528040886 Validation loss 0.031113870441913605 Accuracy 0.68603515625\n",
      "Iteration 9510 Training loss 0.02649698033928871 Validation loss 0.02984592877328396 Accuracy 0.6982421875\n",
      "Iteration 9520 Training loss 0.02865319326519966 Validation loss 0.03033660352230072 Accuracy 0.69287109375\n",
      "Iteration 9530 Training loss 0.028030145913362503 Validation loss 0.029819421470165253 Accuracy 0.69873046875\n",
      "Iteration 9540 Training loss 0.02636807970702648 Validation loss 0.029807714745402336 Accuracy 0.69775390625\n",
      "Iteration 9550 Training loss 0.029758689925074577 Validation loss 0.030293909832835197 Accuracy 0.693359375\n",
      "Iteration 9560 Training loss 0.027820607647299767 Validation loss 0.03036259114742279 Accuracy 0.69287109375\n",
      "Iteration 9570 Training loss 0.02882230654358864 Validation loss 0.03071911260485649 Accuracy 0.68896484375\n",
      "Iteration 9580 Training loss 0.03074089251458645 Validation loss 0.02975825034081936 Accuracy 0.69921875\n",
      "Iteration 9590 Training loss 0.02839929610490799 Validation loss 0.030788354575634003 Accuracy 0.68798828125\n",
      "Iteration 9600 Training loss 0.028255511075258255 Validation loss 0.029661759734153748 Accuracy 0.69970703125\n",
      "Iteration 9610 Training loss 0.027140818536281586 Validation loss 0.02974642440676689 Accuracy 0.69873046875\n",
      "Iteration 9620 Training loss 0.028958536684513092 Validation loss 0.030958613380789757 Accuracy 0.6875\n",
      "Iteration 9630 Training loss 0.029114380478858948 Validation loss 0.030597694218158722 Accuracy 0.68994140625\n",
      "Iteration 9640 Training loss 0.03169422224164009 Validation loss 0.030654946342110634 Accuracy 0.6904296875\n",
      "Iteration 9650 Training loss 0.02897092141211033 Validation loss 0.031041370704770088 Accuracy 0.6865234375\n",
      "Iteration 9660 Training loss 0.030046284198760986 Validation loss 0.02995229698717594 Accuracy 0.69677734375\n",
      "Iteration 9670 Training loss 0.029613113030791283 Validation loss 0.02957061491906643 Accuracy 0.701171875\n",
      "Iteration 9680 Training loss 0.026859978213906288 Validation loss 0.031039511784911156 Accuracy 0.68603515625\n",
      "Iteration 9690 Training loss 0.028789332136511803 Validation loss 0.031349681317806244 Accuracy 0.68212890625\n",
      "Iteration 9700 Training loss 0.03157587721943855 Validation loss 0.03173146769404411 Accuracy 0.6796875\n",
      "Iteration 9710 Training loss 0.029793495312333107 Validation loss 0.03129090368747711 Accuracy 0.68408203125\n",
      "Iteration 9720 Training loss 0.029145030304789543 Validation loss 0.029618261381983757 Accuracy 0.7001953125\n",
      "Iteration 9730 Training loss 0.02890178933739662 Validation loss 0.029568763449788094 Accuracy 0.7001953125\n",
      "Iteration 9740 Training loss 0.031538959592580795 Validation loss 0.03116319701075554 Accuracy 0.68505859375\n",
      "Iteration 9750 Training loss 0.030290130525827408 Validation loss 0.03213300555944443 Accuracy 0.6748046875\n",
      "Iteration 9760 Training loss 0.02787897177040577 Validation loss 0.030584637075662613 Accuracy 0.69091796875\n",
      "Iteration 9770 Training loss 0.02619670145213604 Validation loss 0.029575712978839874 Accuracy 0.70068359375\n",
      "Iteration 9780 Training loss 0.027894597500562668 Validation loss 0.031084442511200905 Accuracy 0.68505859375\n",
      "Iteration 9790 Training loss 0.027170775458216667 Validation loss 0.029763534665107727 Accuracy 0.6982421875\n",
      "Iteration 9800 Training loss 0.02764822170138359 Validation loss 0.030421415343880653 Accuracy 0.69189453125\n",
      "Iteration 9810 Training loss 0.02721710130572319 Validation loss 0.029796667397022247 Accuracy 0.6982421875\n",
      "Iteration 9820 Training loss 0.027571294456720352 Validation loss 0.030046606436371803 Accuracy 0.69677734375\n",
      "Iteration 9830 Training loss 0.028785614296793938 Validation loss 0.030241165310144424 Accuracy 0.6943359375\n",
      "Iteration 9840 Training loss 0.025312766432762146 Validation loss 0.029688773676753044 Accuracy 0.69970703125\n",
      "Iteration 9850 Training loss 0.02980971150100231 Validation loss 0.029703373089432716 Accuracy 0.69921875\n",
      "Iteration 9860 Training loss 0.030409589409828186 Validation loss 0.029679544270038605 Accuracy 0.69970703125\n",
      "Iteration 9870 Training loss 0.030720960348844528 Validation loss 0.02955625392496586 Accuracy 0.7001953125\n",
      "Iteration 9880 Training loss 0.028472628444433212 Validation loss 0.02976776286959648 Accuracy 0.69873046875\n",
      "Iteration 9890 Training loss 0.027420638129115105 Validation loss 0.029903434216976166 Accuracy 0.697265625\n",
      "Iteration 9900 Training loss 0.033687490969896317 Validation loss 0.029687801375985146 Accuracy 0.69970703125\n",
      "Iteration 9910 Training loss 0.02646215260028839 Validation loss 0.029913123697042465 Accuracy 0.697265625\n",
      "Iteration 9920 Training loss 0.02631591260433197 Validation loss 0.0299017746001482 Accuracy 0.69775390625\n",
      "Iteration 9930 Training loss 0.02803141437470913 Validation loss 0.030797215178608894 Accuracy 0.68896484375\n",
      "Iteration 9940 Training loss 0.03045240230858326 Validation loss 0.029719941318035126 Accuracy 0.69921875\n",
      "Iteration 9950 Training loss 0.02716982178390026 Validation loss 0.030234046280384064 Accuracy 0.69384765625\n",
      "Iteration 9960 Training loss 0.026356225833296776 Validation loss 0.0304470993578434 Accuracy 0.69189453125\n",
      "Iteration 9970 Training loss 0.029259007424116135 Validation loss 0.029844632372260094 Accuracy 0.6982421875\n",
      "Iteration 9980 Training loss 0.02841801941394806 Validation loss 0.029842397198081017 Accuracy 0.6982421875\n",
      "Iteration 9990 Training loss 0.029880093410611153 Validation loss 0.029283393174409866 Accuracy 0.703125\n",
      "Iteration 10000 Training loss 0.025048047304153442 Validation loss 0.029474971815943718 Accuracy 0.70263671875\n",
      "Iteration 10010 Training loss 0.030471017584204674 Validation loss 0.029956087470054626 Accuracy 0.697265625\n",
      "Iteration 10020 Training loss 0.028826288878917694 Validation loss 0.030111579224467278 Accuracy 0.69580078125\n",
      "Iteration 10030 Training loss 0.02925047278404236 Validation loss 0.030104197561740875 Accuracy 0.6953125\n",
      "Iteration 10040 Training loss 0.028234288096427917 Validation loss 0.0299431923776865 Accuracy 0.697265625\n",
      "Iteration 10050 Training loss 0.027527276426553726 Validation loss 0.029694005846977234 Accuracy 0.69921875\n",
      "Iteration 10060 Training loss 0.028264427557587624 Validation loss 0.0296184029430151 Accuracy 0.7001953125\n",
      "Iteration 10070 Training loss 0.025561202317476273 Validation loss 0.029544208198785782 Accuracy 0.701171875\n",
      "Iteration 10080 Training loss 0.03134270757436752 Validation loss 0.030544627457857132 Accuracy 0.69189453125\n",
      "Iteration 10090 Training loss 0.02976497821509838 Validation loss 0.030178392305970192 Accuracy 0.6943359375\n",
      "Iteration 10100 Training loss 0.027948565781116486 Validation loss 0.029733311384916306 Accuracy 0.6982421875\n",
      "Iteration 10110 Training loss 0.027370182797312737 Validation loss 0.030731141567230225 Accuracy 0.689453125\n",
      "Iteration 10120 Training loss 0.026143338531255722 Validation loss 0.029581723734736443 Accuracy 0.7001953125\n",
      "Iteration 10130 Training loss 0.02829607017338276 Validation loss 0.02985362708568573 Accuracy 0.697265625\n",
      "Iteration 10140 Training loss 0.032046929001808167 Validation loss 0.02985193021595478 Accuracy 0.6982421875\n",
      "Iteration 10150 Training loss 0.03145263344049454 Validation loss 0.02976479008793831 Accuracy 0.69873046875\n",
      "Iteration 10160 Training loss 0.02482360042631626 Validation loss 0.029569609090685844 Accuracy 0.70166015625\n",
      "Iteration 10170 Training loss 0.02950950525701046 Validation loss 0.029496438801288605 Accuracy 0.70068359375\n",
      "Iteration 10180 Training loss 0.027599746361374855 Validation loss 0.0304454043507576 Accuracy 0.6923828125\n",
      "Iteration 10190 Training loss 0.027667442336678505 Validation loss 0.030219830572605133 Accuracy 0.6943359375\n",
      "Iteration 10200 Training loss 0.02590317837893963 Validation loss 0.029568621888756752 Accuracy 0.70068359375\n",
      "Iteration 10210 Training loss 0.025964563712477684 Validation loss 0.03036162443459034 Accuracy 0.69189453125\n",
      "Iteration 10220 Training loss 0.03013809770345688 Validation loss 0.03116588294506073 Accuracy 0.6845703125\n",
      "Iteration 10230 Training loss 0.02759118564426899 Validation loss 0.02966534160077572 Accuracy 0.69970703125\n",
      "Iteration 10240 Training loss 0.02892824448645115 Validation loss 0.031006578356027603 Accuracy 0.68603515625\n",
      "Iteration 10250 Training loss 0.028678791597485542 Validation loss 0.02926911599934101 Accuracy 0.70361328125\n",
      "Iteration 10260 Training loss 0.027199817821383476 Validation loss 0.029623057693243027 Accuracy 0.70068359375\n",
      "Iteration 10270 Training loss 0.026716573163866997 Validation loss 0.02954726479947567 Accuracy 0.7001953125\n",
      "Iteration 10280 Training loss 0.029348623007535934 Validation loss 0.030025817453861237 Accuracy 0.6962890625\n",
      "Iteration 10290 Training loss 0.025661157444119453 Validation loss 0.02932993322610855 Accuracy 0.70361328125\n",
      "Iteration 10300 Training loss 0.03014030121266842 Validation loss 0.029905639588832855 Accuracy 0.697265625\n",
      "Iteration 10310 Training loss 0.027344726026058197 Validation loss 0.029769614338874817 Accuracy 0.69921875\n",
      "Iteration 10320 Training loss 0.030110355466604233 Validation loss 0.029281901195645332 Accuracy 0.70361328125\n",
      "Iteration 10330 Training loss 0.027745231986045837 Validation loss 0.029194457456469536 Accuracy 0.70458984375\n",
      "Iteration 10340 Training loss 0.031220296397805214 Validation loss 0.03207801282405853 Accuracy 0.6767578125\n",
      "Iteration 10350 Training loss 0.02567828819155693 Validation loss 0.02976885624229908 Accuracy 0.6982421875\n",
      "Iteration 10360 Training loss 0.030259642750024796 Validation loss 0.02975679375231266 Accuracy 0.69873046875\n",
      "Iteration 10370 Training loss 0.02995396964251995 Validation loss 0.030043400824069977 Accuracy 0.6962890625\n",
      "Iteration 10380 Training loss 0.02895096316933632 Validation loss 0.03033807873725891 Accuracy 0.6923828125\n",
      "Iteration 10390 Training loss 0.02786148525774479 Validation loss 0.02941797859966755 Accuracy 0.70166015625\n",
      "Iteration 10400 Training loss 0.026084357872605324 Validation loss 0.030382240191102028 Accuracy 0.693359375\n",
      "Iteration 10410 Training loss 0.026549428701400757 Validation loss 0.031438831239938736 Accuracy 0.6826171875\n",
      "Iteration 10420 Training loss 0.028171753510832787 Validation loss 0.029292577877640724 Accuracy 0.70361328125\n",
      "Iteration 10430 Training loss 0.028825072571635246 Validation loss 0.030561894178390503 Accuracy 0.69091796875\n",
      "Iteration 10440 Training loss 0.027355119585990906 Validation loss 0.029314933344721794 Accuracy 0.703125\n",
      "Iteration 10450 Training loss 0.028616590425372124 Validation loss 0.029920177534222603 Accuracy 0.697265625\n",
      "Iteration 10460 Training loss 0.025796955451369286 Validation loss 0.029539085924625397 Accuracy 0.70166015625\n",
      "Iteration 10470 Training loss 0.027228932827711105 Validation loss 0.02951706014573574 Accuracy 0.701171875\n",
      "Iteration 10480 Training loss 0.027332765981554985 Validation loss 0.029131662100553513 Accuracy 0.705078125\n",
      "Iteration 10490 Training loss 0.030080359429121017 Validation loss 0.032892320305109024 Accuracy 0.6669921875\n",
      "Iteration 10500 Training loss 0.030870063230395317 Validation loss 0.03047102503478527 Accuracy 0.69140625\n",
      "Iteration 10510 Training loss 0.02712264657020569 Validation loss 0.02967182546854019 Accuracy 0.69921875\n",
      "Iteration 10520 Training loss 0.023908711969852448 Validation loss 0.02977311983704567 Accuracy 0.69873046875\n",
      "Iteration 10530 Training loss 0.03318466991186142 Validation loss 0.029676640406250954 Accuracy 0.7001953125\n",
      "Iteration 10540 Training loss 0.02600790187716484 Validation loss 0.02924901247024536 Accuracy 0.7041015625\n",
      "Iteration 10550 Training loss 0.03007012978196144 Validation loss 0.029273681342601776 Accuracy 0.703125\n",
      "Iteration 10560 Training loss 0.027905665338039398 Validation loss 0.029773226007819176 Accuracy 0.69921875\n",
      "Iteration 10570 Training loss 0.029987065121531487 Validation loss 0.029415009543299675 Accuracy 0.70263671875\n",
      "Iteration 10580 Training loss 0.030942952260375023 Validation loss 0.030837953090667725 Accuracy 0.6875\n",
      "Iteration 10590 Training loss 0.02702869102358818 Validation loss 0.02963062934577465 Accuracy 0.7001953125\n",
      "Iteration 10600 Training loss 0.02923731878399849 Validation loss 0.029306070879101753 Accuracy 0.703125\n",
      "Iteration 10610 Training loss 0.028719604015350342 Validation loss 0.02957628294825554 Accuracy 0.7001953125\n",
      "Iteration 10620 Training loss 0.025660844519734383 Validation loss 0.030359365046024323 Accuracy 0.69287109375\n",
      "Iteration 10630 Training loss 0.02671281434595585 Validation loss 0.02931751310825348 Accuracy 0.70361328125\n",
      "Iteration 10640 Training loss 0.030090173706412315 Validation loss 0.029328154399991035 Accuracy 0.703125\n",
      "Iteration 10650 Training loss 0.026866286993026733 Validation loss 0.029194608330726624 Accuracy 0.70458984375\n",
      "Iteration 10660 Training loss 0.025407813489437103 Validation loss 0.029490789398550987 Accuracy 0.70166015625\n",
      "Iteration 10670 Training loss 0.028954457491636276 Validation loss 0.030154481530189514 Accuracy 0.6943359375\n",
      "Iteration 10680 Training loss 0.025015713647007942 Validation loss 0.029848231002688408 Accuracy 0.697265625\n",
      "Iteration 10690 Training loss 0.028373098000884056 Validation loss 0.029452484101057053 Accuracy 0.701171875\n",
      "Iteration 10700 Training loss 0.026800496503710747 Validation loss 0.03027684986591339 Accuracy 0.693359375\n",
      "Iteration 10710 Training loss 0.028710482642054558 Validation loss 0.029403403401374817 Accuracy 0.7021484375\n",
      "Iteration 10720 Training loss 0.028675900772213936 Validation loss 0.029448002576828003 Accuracy 0.70166015625\n",
      "Iteration 10730 Training loss 0.02446877956390381 Validation loss 0.02974627912044525 Accuracy 0.69921875\n",
      "Iteration 10740 Training loss 0.030656130984425545 Validation loss 0.029287071898579597 Accuracy 0.70361328125\n",
      "Iteration 10750 Training loss 0.028545858338475227 Validation loss 0.030555177479982376 Accuracy 0.69140625\n",
      "Iteration 10760 Training loss 0.02949896641075611 Validation loss 0.029906708747148514 Accuracy 0.69677734375\n",
      "Iteration 10770 Training loss 0.02986147068440914 Validation loss 0.029143553227186203 Accuracy 0.705078125\n",
      "Iteration 10780 Training loss 0.03127961605787277 Validation loss 0.029162688180804253 Accuracy 0.705078125\n",
      "Iteration 10790 Training loss 0.028555622324347496 Validation loss 0.031348343938589096 Accuracy 0.68310546875\n",
      "Iteration 10800 Training loss 0.029476143419742584 Validation loss 0.029811425134539604 Accuracy 0.6982421875\n",
      "Iteration 10810 Training loss 0.029389800503849983 Validation loss 0.03152154013514519 Accuracy 0.68115234375\n",
      "Iteration 10820 Training loss 0.02980583719909191 Validation loss 0.029714535921812057 Accuracy 0.69921875\n",
      "Iteration 10830 Training loss 0.028635289520025253 Validation loss 0.029491625726222992 Accuracy 0.701171875\n",
      "Iteration 10840 Training loss 0.026691792532801628 Validation loss 0.02915007807314396 Accuracy 0.705078125\n",
      "Iteration 10850 Training loss 0.029916636645793915 Validation loss 0.02997107431292534 Accuracy 0.697265625\n",
      "Iteration 10860 Training loss 0.028233297169208527 Validation loss 0.029250679537653923 Accuracy 0.70361328125\n",
      "Iteration 10870 Training loss 0.02909168042242527 Validation loss 0.02933247946202755 Accuracy 0.70263671875\n",
      "Iteration 10880 Training loss 0.02895548939704895 Validation loss 0.030255110934376717 Accuracy 0.6943359375\n",
      "Iteration 10890 Training loss 0.028772827237844467 Validation loss 0.0292376559227705 Accuracy 0.7041015625\n",
      "Iteration 10900 Training loss 0.02719804085791111 Validation loss 0.029161551967263222 Accuracy 0.70458984375\n",
      "Iteration 10910 Training loss 0.027579588815569878 Validation loss 0.02949473075568676 Accuracy 0.70166015625\n",
      "Iteration 10920 Training loss 0.02781025879085064 Validation loss 0.0294622965157032 Accuracy 0.70166015625\n",
      "Iteration 10930 Training loss 0.024892078712582588 Validation loss 0.02942066639661789 Accuracy 0.7021484375\n",
      "Iteration 10940 Training loss 0.025541046634316444 Validation loss 0.029452703893184662 Accuracy 0.7021484375\n",
      "Iteration 10950 Training loss 0.028352290391921997 Validation loss 0.03005688451230526 Accuracy 0.69580078125\n",
      "Iteration 10960 Training loss 0.02560025081038475 Validation loss 0.029123714193701744 Accuracy 0.70458984375\n",
      "Iteration 10970 Training loss 0.029011039063334465 Validation loss 0.030258480459451675 Accuracy 0.69384765625\n",
      "Iteration 10980 Training loss 0.025353696197271347 Validation loss 0.029472708702087402 Accuracy 0.701171875\n",
      "Iteration 10990 Training loss 0.02906772866845131 Validation loss 0.029748227447271347 Accuracy 0.6982421875\n",
      "Iteration 11000 Training loss 0.027550460770726204 Validation loss 0.029524967074394226 Accuracy 0.70068359375\n",
      "Iteration 11010 Training loss 0.029473574832081795 Validation loss 0.02944828011095524 Accuracy 0.7021484375\n",
      "Iteration 11020 Training loss 0.027271300554275513 Validation loss 0.029698144644498825 Accuracy 0.69921875\n",
      "Iteration 11030 Training loss 0.026094233617186546 Validation loss 0.029006151482462883 Accuracy 0.70654296875\n",
      "Iteration 11040 Training loss 0.027531515806913376 Validation loss 0.029251551255583763 Accuracy 0.7041015625\n",
      "Iteration 11050 Training loss 0.027225224301218987 Validation loss 0.02892526052892208 Accuracy 0.7080078125\n",
      "Iteration 11060 Training loss 0.029607733711600304 Validation loss 0.02912481687963009 Accuracy 0.705078125\n",
      "Iteration 11070 Training loss 0.029111899435520172 Validation loss 0.030097128823399544 Accuracy 0.6962890625\n",
      "Iteration 11080 Training loss 0.029191887006163597 Validation loss 0.029589451849460602 Accuracy 0.7001953125\n",
      "Iteration 11090 Training loss 0.02973145805299282 Validation loss 0.02998659759759903 Accuracy 0.6962890625\n",
      "Iteration 11100 Training loss 0.026323338970541954 Validation loss 0.029896415770053864 Accuracy 0.69775390625\n",
      "Iteration 11110 Training loss 0.02753237448632717 Validation loss 0.029505126178264618 Accuracy 0.70166015625\n",
      "Iteration 11120 Training loss 0.02943357825279236 Validation loss 0.029490754008293152 Accuracy 0.70166015625\n",
      "Iteration 11130 Training loss 0.026380065828561783 Validation loss 0.029477162286639214 Accuracy 0.7021484375\n",
      "Iteration 11140 Training loss 0.029331548139452934 Validation loss 0.029457328841090202 Accuracy 0.7021484375\n",
      "Iteration 11150 Training loss 0.027690252289175987 Validation loss 0.02983783558011055 Accuracy 0.6982421875\n",
      "Iteration 11160 Training loss 0.02602449059486389 Validation loss 0.030407212674617767 Accuracy 0.6923828125\n",
      "Iteration 11170 Training loss 0.030484868213534355 Validation loss 0.02963561937212944 Accuracy 0.7001953125\n",
      "Iteration 11180 Training loss 0.030535629019141197 Validation loss 0.029431933537125587 Accuracy 0.703125\n",
      "Iteration 11190 Training loss 0.028125952929258347 Validation loss 0.029211407527327538 Accuracy 0.7041015625\n",
      "Iteration 11200 Training loss 0.024100473150610924 Validation loss 0.029303735122084618 Accuracy 0.70361328125\n",
      "Iteration 11210 Training loss 0.02527928166091442 Validation loss 0.029268860816955566 Accuracy 0.703125\n",
      "Iteration 11220 Training loss 0.029104840010404587 Validation loss 0.02965770848095417 Accuracy 0.7001953125\n",
      "Iteration 11230 Training loss 0.031056690961122513 Validation loss 0.029598666355013847 Accuracy 0.70068359375\n",
      "Iteration 11240 Training loss 0.029611708596348763 Validation loss 0.030533084645867348 Accuracy 0.69091796875\n",
      "Iteration 11250 Training loss 0.026020027697086334 Validation loss 0.029895296320319176 Accuracy 0.697265625\n",
      "Iteration 11260 Training loss 0.028805838897824287 Validation loss 0.029308278113603592 Accuracy 0.70361328125\n",
      "Iteration 11270 Training loss 0.029200658202171326 Validation loss 0.030147289857268333 Accuracy 0.6953125\n",
      "Iteration 11280 Training loss 0.026428673416376114 Validation loss 0.02997705526649952 Accuracy 0.697265625\n",
      "Iteration 11290 Training loss 0.03006930463016033 Validation loss 0.029334526509046555 Accuracy 0.70361328125\n",
      "Iteration 11300 Training loss 0.026478761807084084 Validation loss 0.029178662225604057 Accuracy 0.70458984375\n",
      "Iteration 11310 Training loss 0.027956655248999596 Validation loss 0.029246000573039055 Accuracy 0.7041015625\n",
      "Iteration 11320 Training loss 0.031649887561798096 Validation loss 0.030439313501119614 Accuracy 0.6923828125\n",
      "Iteration 11330 Training loss 0.03025505505502224 Validation loss 0.029035335406661034 Accuracy 0.7060546875\n",
      "Iteration 11340 Training loss 0.0265504140406847 Validation loss 0.02975730411708355 Accuracy 0.6982421875\n",
      "Iteration 11350 Training loss 0.028695348650217056 Validation loss 0.02928399294614792 Accuracy 0.703125\n",
      "Iteration 11360 Training loss 0.029130924493074417 Validation loss 0.029033703729510307 Accuracy 0.7060546875\n",
      "Iteration 11370 Training loss 0.03642193228006363 Validation loss 0.03270240128040314 Accuracy 0.66943359375\n",
      "Iteration 11380 Training loss 0.033943161368370056 Validation loss 0.030731117352843285 Accuracy 0.68896484375\n",
      "Iteration 11390 Training loss 0.02689497545361519 Validation loss 0.02986820973455906 Accuracy 0.69775390625\n",
      "Iteration 11400 Training loss 0.025702962651848793 Validation loss 0.029435429722070694 Accuracy 0.701171875\n",
      "Iteration 11410 Training loss 0.02948172017931938 Validation loss 0.02939874492585659 Accuracy 0.703125\n",
      "Iteration 11420 Training loss 0.026994002982974052 Validation loss 0.030249446630477905 Accuracy 0.69384765625\n",
      "Iteration 11430 Training loss 0.025051193311810493 Validation loss 0.02931053563952446 Accuracy 0.703125\n",
      "Iteration 11440 Training loss 0.026014678180217743 Validation loss 0.029218723997473717 Accuracy 0.7041015625\n",
      "Iteration 11450 Training loss 0.027990926057100296 Validation loss 0.02958688884973526 Accuracy 0.7001953125\n",
      "Iteration 11460 Training loss 0.028298819437623024 Validation loss 0.02949015609920025 Accuracy 0.701171875\n",
      "Iteration 11470 Training loss 0.027845976874232292 Validation loss 0.02914026938378811 Accuracy 0.705078125\n",
      "Iteration 11480 Training loss 0.027277249842882156 Validation loss 0.029349463060498238 Accuracy 0.70263671875\n",
      "Iteration 11490 Training loss 0.026201661676168442 Validation loss 0.029687821865081787 Accuracy 0.69921875\n",
      "Iteration 11500 Training loss 0.025093546137213707 Validation loss 0.029358696192502975 Accuracy 0.703125\n",
      "Iteration 11510 Training loss 0.028422070667147636 Validation loss 0.029465053230524063 Accuracy 0.70166015625\n",
      "Iteration 11520 Training loss 0.030901437625288963 Validation loss 0.02910718135535717 Accuracy 0.7060546875\n",
      "Iteration 11530 Training loss 0.028132114559412003 Validation loss 0.029288504272699356 Accuracy 0.70361328125\n",
      "Iteration 11540 Training loss 0.029290296137332916 Validation loss 0.02942020446062088 Accuracy 0.701171875\n",
      "Iteration 11550 Training loss 0.024829650297760963 Validation loss 0.029757177457213402 Accuracy 0.69873046875\n",
      "Iteration 11560 Training loss 0.025523915886878967 Validation loss 0.0290945116430521 Accuracy 0.70556640625\n",
      "Iteration 11570 Training loss 0.02961859107017517 Validation loss 0.030126038938760757 Accuracy 0.69580078125\n",
      "Iteration 11580 Training loss 0.02818954922258854 Validation loss 0.029218217357993126 Accuracy 0.70458984375\n",
      "Iteration 11590 Training loss 0.029713759198784828 Validation loss 0.029665743932127953 Accuracy 0.69970703125\n",
      "Iteration 11600 Training loss 0.029825367033481598 Validation loss 0.029021797701716423 Accuracy 0.70654296875\n",
      "Iteration 11610 Training loss 0.027726827189326286 Validation loss 0.030276073142886162 Accuracy 0.69384765625\n",
      "Iteration 11620 Training loss 0.02920759841799736 Validation loss 0.029868295416235924 Accuracy 0.6982421875\n",
      "Iteration 11630 Training loss 0.027240494266152382 Validation loss 0.029048344120383263 Accuracy 0.70556640625\n",
      "Iteration 11640 Training loss 0.027088776230812073 Validation loss 0.029858626425266266 Accuracy 0.69775390625\n",
      "Iteration 11650 Training loss 0.026144545525312424 Validation loss 0.0293161328881979 Accuracy 0.70361328125\n",
      "Iteration 11660 Training loss 0.027429020032286644 Validation loss 0.02959379367530346 Accuracy 0.70068359375\n",
      "Iteration 11670 Training loss 0.03057699464261532 Validation loss 0.03055812604725361 Accuracy 0.69091796875\n",
      "Iteration 11680 Training loss 0.025287464261054993 Validation loss 0.029159869998693466 Accuracy 0.705078125\n",
      "Iteration 11690 Training loss 0.029174163937568665 Validation loss 0.029799822717905045 Accuracy 0.69873046875\n",
      "Iteration 11700 Training loss 0.027778925374150276 Validation loss 0.029658492654561996 Accuracy 0.7001953125\n",
      "Iteration 11710 Training loss 0.028563980013132095 Validation loss 0.029582874849438667 Accuracy 0.70068359375\n",
      "Iteration 11720 Training loss 0.026121504604816437 Validation loss 0.029201984405517578 Accuracy 0.70458984375\n",
      "Iteration 11730 Training loss 0.025974752381443977 Validation loss 0.029284266754984856 Accuracy 0.7041015625\n",
      "Iteration 11740 Training loss 0.023400263860821724 Validation loss 0.0290080439299345 Accuracy 0.70654296875\n",
      "Iteration 11750 Training loss 0.02770182304084301 Validation loss 0.029320087283849716 Accuracy 0.70361328125\n",
      "Iteration 11760 Training loss 0.024846626445651054 Validation loss 0.0296848863363266 Accuracy 0.69921875\n",
      "Iteration 11770 Training loss 0.029483692720532417 Validation loss 0.030123140662908554 Accuracy 0.6953125\n",
      "Iteration 11780 Training loss 0.025425607338547707 Validation loss 0.029429061338305473 Accuracy 0.7021484375\n",
      "Iteration 11790 Training loss 0.02745012193918228 Validation loss 0.029151104390621185 Accuracy 0.705078125\n",
      "Iteration 11800 Training loss 0.028599929064512253 Validation loss 0.031584300100803375 Accuracy 0.68017578125\n",
      "Iteration 11810 Training loss 0.02874794416129589 Validation loss 0.02955802157521248 Accuracy 0.70068359375\n",
      "Iteration 11820 Training loss 0.02954103983938694 Validation loss 0.029053017497062683 Accuracy 0.7060546875\n",
      "Iteration 11830 Training loss 0.02669065073132515 Validation loss 0.029672468081116676 Accuracy 0.7001953125\n",
      "Iteration 11840 Training loss 0.033236902207136154 Validation loss 0.0296280849725008 Accuracy 0.70068359375\n",
      "Iteration 11850 Training loss 0.025192957371473312 Validation loss 0.029526513069868088 Accuracy 0.70068359375\n",
      "Iteration 11860 Training loss 0.026940854266285896 Validation loss 0.029127558693289757 Accuracy 0.705078125\n",
      "Iteration 11870 Training loss 0.02726328931748867 Validation loss 0.029413817450404167 Accuracy 0.70263671875\n",
      "Iteration 11880 Training loss 0.025320833548903465 Validation loss 0.02903355099260807 Accuracy 0.70654296875\n",
      "Iteration 11890 Training loss 0.029687896370887756 Validation loss 0.03005868010222912 Accuracy 0.6953125\n",
      "Iteration 11900 Training loss 0.02502291277050972 Validation loss 0.029099583625793457 Accuracy 0.705078125\n",
      "Iteration 11910 Training loss 0.028340181335806847 Validation loss 0.02951022796332836 Accuracy 0.70166015625\n",
      "Iteration 11920 Training loss 0.028668681159615517 Validation loss 0.029618222266435623 Accuracy 0.69970703125\n",
      "Iteration 11930 Training loss 0.02746560610830784 Validation loss 0.029285365715622902 Accuracy 0.70361328125\n",
      "Iteration 11940 Training loss 0.026537027209997177 Validation loss 0.02933962270617485 Accuracy 0.703125\n",
      "Iteration 11950 Training loss 0.023280909284949303 Validation loss 0.029375774785876274 Accuracy 0.70263671875\n",
      "Iteration 11960 Training loss 0.0302349291741848 Validation loss 0.029165970161557198 Accuracy 0.70556640625\n",
      "Iteration 11970 Training loss 0.03122430108487606 Validation loss 0.02992895618081093 Accuracy 0.697265625\n",
      "Iteration 11980 Training loss 0.028169220313429832 Validation loss 0.029297543689608574 Accuracy 0.70361328125\n",
      "Iteration 11990 Training loss 0.028694583103060722 Validation loss 0.03080124221742153 Accuracy 0.68896484375\n",
      "Iteration 12000 Training loss 0.026802988722920418 Validation loss 0.02946762926876545 Accuracy 0.7021484375\n",
      "Iteration 12010 Training loss 0.02927449345588684 Validation loss 0.029249584302306175 Accuracy 0.7041015625\n",
      "Iteration 12020 Training loss 0.027135202661156654 Validation loss 0.02939828298985958 Accuracy 0.70263671875\n",
      "Iteration 12030 Training loss 0.02590002492070198 Validation loss 0.030146118253469467 Accuracy 0.69482421875\n",
      "Iteration 12040 Training loss 0.03272009640932083 Validation loss 0.0318567231297493 Accuracy 0.67724609375\n",
      "Iteration 12050 Training loss 0.02762967348098755 Validation loss 0.031132109463214874 Accuracy 0.68505859375\n",
      "Iteration 12060 Training loss 0.03259843960404396 Validation loss 0.02918054349720478 Accuracy 0.705078125\n",
      "Iteration 12070 Training loss 0.031210489571094513 Validation loss 0.030011078342795372 Accuracy 0.6962890625\n",
      "Iteration 12080 Training loss 0.024536535143852234 Validation loss 0.028794799000024796 Accuracy 0.70849609375\n",
      "Iteration 12090 Training loss 0.027448730543255806 Validation loss 0.028884848579764366 Accuracy 0.70751953125\n",
      "Iteration 12100 Training loss 0.03180062398314476 Validation loss 0.02934022806584835 Accuracy 0.703125\n",
      "Iteration 12110 Training loss 0.02713368646800518 Validation loss 0.030603768303990364 Accuracy 0.68994140625\n",
      "Iteration 12120 Training loss 0.027083363384008408 Validation loss 0.02935301698744297 Accuracy 0.70263671875\n",
      "Iteration 12130 Training loss 0.027652040123939514 Validation loss 0.029299790039658546 Accuracy 0.703125\n",
      "Iteration 12140 Training loss 0.027882179245352745 Validation loss 0.029220659285783768 Accuracy 0.70458984375\n",
      "Iteration 12150 Training loss 0.026955382898449898 Validation loss 0.030016737058758736 Accuracy 0.69580078125\n",
      "Iteration 12160 Training loss 0.026583371683955193 Validation loss 0.029637088999152184 Accuracy 0.7001953125\n",
      "Iteration 12170 Training loss 0.02764568105340004 Validation loss 0.029430527240037918 Accuracy 0.70263671875\n",
      "Iteration 12180 Training loss 0.028784938156604767 Validation loss 0.029735390096902847 Accuracy 0.69921875\n",
      "Iteration 12190 Training loss 0.026781102642416954 Validation loss 0.030211126431822777 Accuracy 0.69482421875\n",
      "Iteration 12200 Training loss 0.02769009955227375 Validation loss 0.029357992112636566 Accuracy 0.703125\n",
      "Iteration 12210 Training loss 0.0313270166516304 Validation loss 0.030246419832110405 Accuracy 0.6943359375\n",
      "Iteration 12220 Training loss 0.028565429151058197 Validation loss 0.029336264356970787 Accuracy 0.703125\n",
      "Iteration 12230 Training loss 0.027056943625211716 Validation loss 0.02997051179409027 Accuracy 0.697265625\n",
      "Iteration 12240 Training loss 0.024197891354560852 Validation loss 0.03044227883219719 Accuracy 0.69091796875\n",
      "Iteration 12250 Training loss 0.028850596398115158 Validation loss 0.031119531020522118 Accuracy 0.68505859375\n",
      "Iteration 12260 Training loss 0.025424325838685036 Validation loss 0.029287010431289673 Accuracy 0.7041015625\n",
      "Iteration 12270 Training loss 0.025683334097266197 Validation loss 0.02933269739151001 Accuracy 0.703125\n",
      "Iteration 12280 Training loss 0.027137765660881996 Validation loss 0.02910722605884075 Accuracy 0.7060546875\n",
      "Iteration 12290 Training loss 0.029285220429301262 Validation loss 0.029110010713338852 Accuracy 0.70556640625\n",
      "Iteration 12300 Training loss 0.023812387138605118 Validation loss 0.02926873415708542 Accuracy 0.7041015625\n",
      "Iteration 12310 Training loss 0.025381674990057945 Validation loss 0.029408955946564674 Accuracy 0.70263671875\n",
      "Iteration 12320 Training loss 0.023890558630228043 Validation loss 0.029498063027858734 Accuracy 0.70166015625\n",
      "Iteration 12330 Training loss 0.02696417272090912 Validation loss 0.029428398236632347 Accuracy 0.7021484375\n",
      "Iteration 12340 Training loss 0.02793729119002819 Validation loss 0.029169166460633278 Accuracy 0.705078125\n",
      "Iteration 12350 Training loss 0.028625842183828354 Validation loss 0.030225038528442383 Accuracy 0.6953125\n",
      "Iteration 12360 Training loss 0.02731381170451641 Validation loss 0.029207870364189148 Accuracy 0.7041015625\n",
      "Iteration 12370 Training loss 0.02642040140926838 Validation loss 0.029136063531041145 Accuracy 0.705078125\n",
      "Iteration 12380 Training loss 0.026366475969552994 Validation loss 0.028884071856737137 Accuracy 0.70751953125\n",
      "Iteration 12390 Training loss 0.028516540303826332 Validation loss 0.02901962399482727 Accuracy 0.70654296875\n",
      "Iteration 12400 Training loss 0.0279094185680151 Validation loss 0.02931160107254982 Accuracy 0.703125\n",
      "Iteration 12410 Training loss 0.027859460562467575 Validation loss 0.028859157115221024 Accuracy 0.70751953125\n",
      "Iteration 12420 Training loss 0.028130877763032913 Validation loss 0.029912235215306282 Accuracy 0.69677734375\n",
      "Iteration 12430 Training loss 0.025807715952396393 Validation loss 0.029123255982995033 Accuracy 0.705078125\n",
      "Iteration 12440 Training loss 0.026875929906964302 Validation loss 0.029222939163446426 Accuracy 0.70458984375\n",
      "Iteration 12450 Training loss 0.02669927477836609 Validation loss 0.029160209000110626 Accuracy 0.70458984375\n",
      "Iteration 12460 Training loss 0.03053819201886654 Validation loss 0.030404582619667053 Accuracy 0.69189453125\n",
      "Iteration 12470 Training loss 0.02566460147500038 Validation loss 0.028919808566570282 Accuracy 0.7080078125\n",
      "Iteration 12480 Training loss 0.023526284843683243 Validation loss 0.028857054188847542 Accuracy 0.70849609375\n",
      "Iteration 12490 Training loss 0.02570287510752678 Validation loss 0.029298655688762665 Accuracy 0.70361328125\n",
      "Iteration 12500 Training loss 0.030629027634859085 Validation loss 0.029339615255594254 Accuracy 0.70263671875\n",
      "Iteration 12510 Training loss 0.027360480278730392 Validation loss 0.029269827529788017 Accuracy 0.703125\n",
      "Iteration 12520 Training loss 0.028493141755461693 Validation loss 0.03036615066230297 Accuracy 0.69287109375\n",
      "Iteration 12530 Training loss 0.028153415769338608 Validation loss 0.029280496761202812 Accuracy 0.703125\n",
      "Iteration 12540 Training loss 0.027382217347621918 Validation loss 0.029413975775241852 Accuracy 0.70263671875\n",
      "Iteration 12550 Training loss 0.028064921498298645 Validation loss 0.029151825234293938 Accuracy 0.705078125\n",
      "Iteration 12560 Training loss 0.025642242282629013 Validation loss 0.029583441093564034 Accuracy 0.7001953125\n",
      "Iteration 12570 Training loss 0.031092436984181404 Validation loss 0.028931131586432457 Accuracy 0.70751953125\n",
      "Iteration 12580 Training loss 0.028072265908122063 Validation loss 0.02936518006026745 Accuracy 0.70361328125\n",
      "Iteration 12590 Training loss 0.026561925187706947 Validation loss 0.02994522824883461 Accuracy 0.69677734375\n",
      "Iteration 12600 Training loss 0.026588307693600655 Validation loss 0.02887793816626072 Accuracy 0.70751953125\n",
      "Iteration 12610 Training loss 0.025312939658761024 Validation loss 0.02913655713200569 Accuracy 0.70556640625\n",
      "Iteration 12620 Training loss 0.03023623116314411 Validation loss 0.029859580099582672 Accuracy 0.69873046875\n",
      "Iteration 12630 Training loss 0.025889942422509193 Validation loss 0.029985550791025162 Accuracy 0.69677734375\n",
      "Iteration 12640 Training loss 0.025961633771657944 Validation loss 0.029379261657595634 Accuracy 0.703125\n",
      "Iteration 12650 Training loss 0.029918428510427475 Validation loss 0.029183529317378998 Accuracy 0.7041015625\n",
      "Iteration 12660 Training loss 0.026344100013375282 Validation loss 0.028873983770608902 Accuracy 0.7080078125\n",
      "Iteration 12670 Training loss 0.027647901326417923 Validation loss 0.029686767607927322 Accuracy 0.69970703125\n",
      "Iteration 12680 Training loss 0.026110859587788582 Validation loss 0.02944033592939377 Accuracy 0.7021484375\n",
      "Iteration 12690 Training loss 0.02792806178331375 Validation loss 0.029040886089205742 Accuracy 0.70654296875\n",
      "Iteration 12700 Training loss 0.03096577897667885 Validation loss 0.02908068709075451 Accuracy 0.70556640625\n",
      "Iteration 12710 Training loss 0.029058320447802544 Validation loss 0.030247319489717484 Accuracy 0.6943359375\n",
      "Iteration 12720 Training loss 0.029442988336086273 Validation loss 0.030305741354823112 Accuracy 0.69287109375\n",
      "Iteration 12730 Training loss 0.026655858382582664 Validation loss 0.02911985106766224 Accuracy 0.705078125\n",
      "Iteration 12740 Training loss 0.026755401864647865 Validation loss 0.029276356101036072 Accuracy 0.7041015625\n",
      "Iteration 12750 Training loss 0.02818755991756916 Validation loss 0.029900936409831047 Accuracy 0.697265625\n",
      "Iteration 12760 Training loss 0.02552618831396103 Validation loss 0.02921370044350624 Accuracy 0.70458984375\n",
      "Iteration 12770 Training loss 0.024052932858467102 Validation loss 0.029347164556384087 Accuracy 0.703125\n",
      "Iteration 12780 Training loss 0.025752214714884758 Validation loss 0.029042543843388557 Accuracy 0.7060546875\n",
      "Iteration 12790 Training loss 0.028011474758386612 Validation loss 0.028933485969901085 Accuracy 0.70703125\n",
      "Iteration 12800 Training loss 0.027241159230470657 Validation loss 0.029774125665426254 Accuracy 0.69873046875\n",
      "Iteration 12810 Training loss 0.0254930779337883 Validation loss 0.028864702209830284 Accuracy 0.7080078125\n",
      "Iteration 12820 Training loss 0.024763133376836777 Validation loss 0.029223764315247536 Accuracy 0.70361328125\n",
      "Iteration 12830 Training loss 0.025384796783328056 Validation loss 0.0294442567974329 Accuracy 0.701171875\n",
      "Iteration 12840 Training loss 0.027786564081907272 Validation loss 0.029203280806541443 Accuracy 0.70458984375\n",
      "Iteration 12850 Training loss 0.027955906465649605 Validation loss 0.029590237885713577 Accuracy 0.69970703125\n",
      "Iteration 12860 Training loss 0.024905428290367126 Validation loss 0.029029686003923416 Accuracy 0.7060546875\n",
      "Iteration 12870 Training loss 0.027156665921211243 Validation loss 0.029326725751161575 Accuracy 0.70263671875\n",
      "Iteration 12880 Training loss 0.027410736307501793 Validation loss 0.028866738080978394 Accuracy 0.70849609375\n",
      "Iteration 12890 Training loss 0.03171166777610779 Validation loss 0.02936035767197609 Accuracy 0.703125\n",
      "Iteration 12900 Training loss 0.026628900319337845 Validation loss 0.029989974573254585 Accuracy 0.6962890625\n",
      "Iteration 12910 Training loss 0.025439715012907982 Validation loss 0.02932063676416874 Accuracy 0.70263671875\n",
      "Iteration 12920 Training loss 0.029073480516672134 Validation loss 0.028959311544895172 Accuracy 0.70751953125\n",
      "Iteration 12930 Training loss 0.03031483106315136 Validation loss 0.029196742922067642 Accuracy 0.7041015625\n",
      "Iteration 12940 Training loss 0.029086578637361526 Validation loss 0.029923826456069946 Accuracy 0.69677734375\n",
      "Iteration 12950 Training loss 0.028881708160042763 Validation loss 0.029095809906721115 Accuracy 0.70654296875\n",
      "Iteration 12960 Training loss 0.02725144848227501 Validation loss 0.02892867662012577 Accuracy 0.70654296875\n",
      "Iteration 12970 Training loss 0.031538594514131546 Validation loss 0.02927475795149803 Accuracy 0.7041015625\n",
      "Iteration 12980 Training loss 0.02953857183456421 Validation loss 0.030408460646867752 Accuracy 0.69189453125\n",
      "Iteration 12990 Training loss 0.026187896728515625 Validation loss 0.029376469552516937 Accuracy 0.70263671875\n",
      "Iteration 13000 Training loss 0.027914369478821754 Validation loss 0.03002343140542507 Accuracy 0.69580078125\n",
      "Iteration 13010 Training loss 0.02837882936000824 Validation loss 0.02956218831241131 Accuracy 0.701171875\n",
      "Iteration 13020 Training loss 0.027433054521679878 Validation loss 0.0290188230574131 Accuracy 0.7060546875\n",
      "Iteration 13030 Training loss 0.02546832337975502 Validation loss 0.02886909991502762 Accuracy 0.7080078125\n",
      "Iteration 13040 Training loss 0.024610061198472977 Validation loss 0.029793642461299896 Accuracy 0.69873046875\n",
      "Iteration 13050 Training loss 0.024729080498218536 Validation loss 0.028944700956344604 Accuracy 0.70703125\n",
      "Iteration 13060 Training loss 0.024229664355516434 Validation loss 0.02900480479001999 Accuracy 0.70654296875\n",
      "Iteration 13070 Training loss 0.03073013760149479 Validation loss 0.029808344319462776 Accuracy 0.69775390625\n",
      "Iteration 13080 Training loss 0.025751754641532898 Validation loss 0.02894761972129345 Accuracy 0.70654296875\n",
      "Iteration 13090 Training loss 0.027394017204642296 Validation loss 0.02931327559053898 Accuracy 0.7041015625\n",
      "Iteration 13100 Training loss 0.02650384046137333 Validation loss 0.029860228300094604 Accuracy 0.69775390625\n",
      "Iteration 13110 Training loss 0.026474906131625175 Validation loss 0.029037538915872574 Accuracy 0.7060546875\n",
      "Iteration 13120 Training loss 0.02314016781747341 Validation loss 0.028918778523802757 Accuracy 0.70751953125\n",
      "Iteration 13130 Training loss 0.02845160849392414 Validation loss 0.02956238202750683 Accuracy 0.70068359375\n",
      "Iteration 13140 Training loss 0.026748565956950188 Validation loss 0.02886863611638546 Accuracy 0.70849609375\n",
      "Iteration 13150 Training loss 0.025429412722587585 Validation loss 0.02873932383954525 Accuracy 0.70947265625\n",
      "Iteration 13160 Training loss 0.027797365561127663 Validation loss 0.028921153396368027 Accuracy 0.70703125\n",
      "Iteration 13170 Training loss 0.02850489690899849 Validation loss 0.02905324660241604 Accuracy 0.70556640625\n",
      "Iteration 13180 Training loss 0.02770998328924179 Validation loss 0.028903471305966377 Accuracy 0.7080078125\n",
      "Iteration 13190 Training loss 0.028849024325609207 Validation loss 0.029149994254112244 Accuracy 0.70556640625\n",
      "Iteration 13200 Training loss 0.02582479827105999 Validation loss 0.02916443534195423 Accuracy 0.705078125\n",
      "Iteration 13210 Training loss 0.029514892026782036 Validation loss 0.02902904525399208 Accuracy 0.70556640625\n",
      "Iteration 13220 Training loss 0.027695544064044952 Validation loss 0.0289973895996809 Accuracy 0.70654296875\n",
      "Iteration 13230 Training loss 0.025735370814800262 Validation loss 0.028776489198207855 Accuracy 0.708984375\n",
      "Iteration 13240 Training loss 0.026475386694073677 Validation loss 0.02886863984167576 Accuracy 0.7080078125\n",
      "Iteration 13250 Training loss 0.029725901782512665 Validation loss 0.030034076422452927 Accuracy 0.6962890625\n",
      "Iteration 13260 Training loss 0.030592763796448708 Validation loss 0.02912849374115467 Accuracy 0.705078125\n",
      "Iteration 13270 Training loss 0.02873496152460575 Validation loss 0.028990522027015686 Accuracy 0.70654296875\n",
      "Iteration 13280 Training loss 0.028274372220039368 Validation loss 0.02929740585386753 Accuracy 0.7041015625\n",
      "Iteration 13290 Training loss 0.025046594440937042 Validation loss 0.028860589489340782 Accuracy 0.7080078125\n",
      "Iteration 13300 Training loss 0.024923590943217278 Validation loss 0.029118485748767853 Accuracy 0.7060546875\n",
      "Iteration 13310 Training loss 0.026684943586587906 Validation loss 0.028972309082746506 Accuracy 0.70703125\n",
      "Iteration 13320 Training loss 0.028534356504678726 Validation loss 0.028742171823978424 Accuracy 0.70947265625\n",
      "Iteration 13330 Training loss 0.026483997702598572 Validation loss 0.028791459277272224 Accuracy 0.708984375\n",
      "Iteration 13340 Training loss 0.02825744077563286 Validation loss 0.02900552749633789 Accuracy 0.70654296875\n",
      "Iteration 13350 Training loss 0.028159651905298233 Validation loss 0.029357992112636566 Accuracy 0.703125\n",
      "Iteration 13360 Training loss 0.026544691994786263 Validation loss 0.029450519010424614 Accuracy 0.7021484375\n",
      "Iteration 13370 Training loss 0.024097783491015434 Validation loss 0.029181871563196182 Accuracy 0.7041015625\n",
      "Iteration 13380 Training loss 0.028175445273518562 Validation loss 0.02872302196919918 Accuracy 0.70947265625\n",
      "Iteration 13390 Training loss 0.029718881472945213 Validation loss 0.02897079475224018 Accuracy 0.70751953125\n",
      "Iteration 13400 Training loss 0.029873089864850044 Validation loss 0.029185282066464424 Accuracy 0.705078125\n",
      "Iteration 13410 Training loss 0.022473620250821114 Validation loss 0.028747230768203735 Accuracy 0.70947265625\n",
      "Iteration 13420 Training loss 0.028341658413410187 Validation loss 0.029495595023036003 Accuracy 0.701171875\n",
      "Iteration 13430 Training loss 0.0266400296241045 Validation loss 0.028806371614336967 Accuracy 0.70849609375\n",
      "Iteration 13440 Training loss 0.025952240452170372 Validation loss 0.02890363521873951 Accuracy 0.70703125\n",
      "Iteration 13450 Training loss 0.026249948889017105 Validation loss 0.028741730377078056 Accuracy 0.70849609375\n",
      "Iteration 13460 Training loss 0.02877621352672577 Validation loss 0.029209481552243233 Accuracy 0.7041015625\n",
      "Iteration 13470 Training loss 0.024509266018867493 Validation loss 0.029314091429114342 Accuracy 0.70361328125\n",
      "Iteration 13480 Training loss 0.025785598903894424 Validation loss 0.029155714437365532 Accuracy 0.70458984375\n",
      "Iteration 13490 Training loss 0.02590741217136383 Validation loss 0.029088905081152916 Accuracy 0.705078125\n",
      "Iteration 13500 Training loss 0.029067249968647957 Validation loss 0.028965512290596962 Accuracy 0.70703125\n",
      "Iteration 13510 Training loss 0.02863323502242565 Validation loss 0.031346142292022705 Accuracy 0.68212890625\n",
      "Iteration 13520 Training loss 0.029788803309202194 Validation loss 0.028900127857923508 Accuracy 0.70751953125\n",
      "Iteration 13530 Training loss 0.02609139494597912 Validation loss 0.028719007968902588 Accuracy 0.7099609375\n",
      "Iteration 13540 Training loss 0.02545592561364174 Validation loss 0.02880297787487507 Accuracy 0.708984375\n",
      "Iteration 13550 Training loss 0.025320714339613914 Validation loss 0.028968490660190582 Accuracy 0.70703125\n",
      "Iteration 13560 Training loss 0.027022695168852806 Validation loss 0.028682108968496323 Accuracy 0.7099609375\n",
      "Iteration 13570 Training loss 0.026205357164144516 Validation loss 0.02909279055893421 Accuracy 0.70556640625\n",
      "Iteration 13580 Training loss 0.03085450269281864 Validation loss 0.02855987660586834 Accuracy 0.7109375\n",
      "Iteration 13590 Training loss 0.023679371923208237 Validation loss 0.02881370298564434 Accuracy 0.708984375\n",
      "Iteration 13600 Training loss 0.025925416499376297 Validation loss 0.028650829568505287 Accuracy 0.70947265625\n",
      "Iteration 13610 Training loss 0.031086547300219536 Validation loss 0.029376063495874405 Accuracy 0.703125\n",
      "Iteration 13620 Training loss 0.028919782489538193 Validation loss 0.03073756955564022 Accuracy 0.68896484375\n",
      "Iteration 13630 Training loss 0.03168969601392746 Validation loss 0.02940473146736622 Accuracy 0.7021484375\n",
      "Iteration 13640 Training loss 0.02311999537050724 Validation loss 0.028878983110189438 Accuracy 0.7080078125\n",
      "Iteration 13650 Training loss 0.02621590718626976 Validation loss 0.028644300997257233 Accuracy 0.70947265625\n",
      "Iteration 13660 Training loss 0.029233917593955994 Validation loss 0.029181234538555145 Accuracy 0.705078125\n",
      "Iteration 13670 Training loss 0.028250334784388542 Validation loss 0.02904023602604866 Accuracy 0.7060546875\n",
      "Iteration 13680 Training loss 0.02590971812605858 Validation loss 0.028832633048295975 Accuracy 0.7080078125\n",
      "Iteration 13690 Training loss 0.02717217057943344 Validation loss 0.02912151999771595 Accuracy 0.70556640625\n",
      "Iteration 13700 Training loss 0.025282831862568855 Validation loss 0.028813783079385757 Accuracy 0.7080078125\n",
      "Iteration 13710 Training loss 0.02744874730706215 Validation loss 0.02904711849987507 Accuracy 0.70654296875\n",
      "Iteration 13720 Training loss 0.027169276028871536 Validation loss 0.028625482693314552 Accuracy 0.71044921875\n",
      "Iteration 13730 Training loss 0.025748170912265778 Validation loss 0.02879725955426693 Accuracy 0.70849609375\n",
      "Iteration 13740 Training loss 0.0275835320353508 Validation loss 0.030759915709495544 Accuracy 0.68896484375\n",
      "Iteration 13750 Training loss 0.025807470083236694 Validation loss 0.029058558866381645 Accuracy 0.7060546875\n",
      "Iteration 13760 Training loss 0.027568282559514046 Validation loss 0.02896588109433651 Accuracy 0.70751953125\n",
      "Iteration 13770 Training loss 0.02403499372303486 Validation loss 0.028804684057831764 Accuracy 0.7080078125\n",
      "Iteration 13780 Training loss 0.028242889791727066 Validation loss 0.02874109521508217 Accuracy 0.71044921875\n",
      "Iteration 13790 Training loss 0.027015645056962967 Validation loss 0.028830237686634064 Accuracy 0.70849609375\n",
      "Iteration 13800 Training loss 0.026462584733963013 Validation loss 0.029285145923495293 Accuracy 0.7041015625\n",
      "Iteration 13810 Training loss 0.026881825178861618 Validation loss 0.028929725289344788 Accuracy 0.7080078125\n",
      "Iteration 13820 Training loss 0.02474844828248024 Validation loss 0.029081622138619423 Accuracy 0.70654296875\n",
      "Iteration 13830 Training loss 0.026304664090275764 Validation loss 0.028974119573831558 Accuracy 0.70751953125\n",
      "Iteration 13840 Training loss 0.03087710216641426 Validation loss 0.02909948118031025 Accuracy 0.70556640625\n",
      "Iteration 13850 Training loss 0.028803233057260513 Validation loss 0.028672397136688232 Accuracy 0.70947265625\n",
      "Iteration 13860 Training loss 0.026328107342123985 Validation loss 0.028919262811541557 Accuracy 0.70751953125\n",
      "Iteration 13870 Training loss 0.02955310046672821 Validation loss 0.029565345495939255 Accuracy 0.701171875\n",
      "Iteration 13880 Training loss 0.024000564590096474 Validation loss 0.029151175171136856 Accuracy 0.705078125\n",
      "Iteration 13890 Training loss 0.028159460052847862 Validation loss 0.030497387051582336 Accuracy 0.69091796875\n",
      "Iteration 13900 Training loss 0.027605872601270676 Validation loss 0.029819956049323082 Accuracy 0.6982421875\n",
      "Iteration 13910 Training loss 0.02688177116215229 Validation loss 0.029301559552550316 Accuracy 0.70361328125\n",
      "Iteration 13920 Training loss 0.026239588856697083 Validation loss 0.028726553544402122 Accuracy 0.70947265625\n",
      "Iteration 13930 Training loss 0.02829006500542164 Validation loss 0.029062926769256592 Accuracy 0.70654296875\n",
      "Iteration 13940 Training loss 0.02523758076131344 Validation loss 0.029227986931800842 Accuracy 0.705078125\n",
      "Iteration 13950 Training loss 0.02665916457772255 Validation loss 0.028862271457910538 Accuracy 0.70849609375\n",
      "Iteration 13960 Training loss 0.02678522653877735 Validation loss 0.029023997485637665 Accuracy 0.70654296875\n",
      "Iteration 13970 Training loss 0.025421885773539543 Validation loss 0.028903920203447342 Accuracy 0.70751953125\n",
      "Iteration 13980 Training loss 0.0262808408588171 Validation loss 0.02905237302184105 Accuracy 0.70654296875\n",
      "Iteration 13990 Training loss 0.026004608720541 Validation loss 0.028772737830877304 Accuracy 0.708984375\n",
      "Iteration 14000 Training loss 0.026003727689385414 Validation loss 0.02944803610444069 Accuracy 0.701171875\n",
      "Iteration 14010 Training loss 0.029462555423378944 Validation loss 0.030162198469042778 Accuracy 0.69482421875\n",
      "Iteration 14020 Training loss 0.02844475954771042 Validation loss 0.029483690857887268 Accuracy 0.70166015625\n",
      "Iteration 14030 Training loss 0.02785390429198742 Validation loss 0.028788113966584206 Accuracy 0.708984375\n",
      "Iteration 14040 Training loss 0.028371527791023254 Validation loss 0.029232628643512726 Accuracy 0.7041015625\n",
      "Iteration 14050 Training loss 0.029999351128935814 Validation loss 0.02921263873577118 Accuracy 0.70458984375\n",
      "Iteration 14060 Training loss 0.02488730661571026 Validation loss 0.028632791712880135 Accuracy 0.7099609375\n",
      "Iteration 14070 Training loss 0.028643421828746796 Validation loss 0.028626497834920883 Accuracy 0.70947265625\n",
      "Iteration 14080 Training loss 0.027760446071624756 Validation loss 0.028948407620191574 Accuracy 0.70703125\n",
      "Iteration 14090 Training loss 0.029463015496730804 Validation loss 0.028665760532021523 Accuracy 0.7099609375\n",
      "Iteration 14100 Training loss 0.025213520973920822 Validation loss 0.028764134272933006 Accuracy 0.708984375\n",
      "Iteration 14110 Training loss 0.027217350900173187 Validation loss 0.02916353940963745 Accuracy 0.70458984375\n",
      "Iteration 14120 Training loss 0.026264198124408722 Validation loss 0.02919657714664936 Accuracy 0.70458984375\n",
      "Iteration 14130 Training loss 0.02742435224354267 Validation loss 0.028659353032708168 Accuracy 0.7099609375\n",
      "Iteration 14140 Training loss 0.027452489361166954 Validation loss 0.029169168323278427 Accuracy 0.70458984375\n",
      "Iteration 14150 Training loss 0.02550434321165085 Validation loss 0.02971038967370987 Accuracy 0.69873046875\n",
      "Iteration 14160 Training loss 0.02907494269311428 Validation loss 0.029454423114657402 Accuracy 0.70166015625\n",
      "Iteration 14170 Training loss 0.02702406235039234 Validation loss 0.028783224523067474 Accuracy 0.70849609375\n",
      "Iteration 14180 Training loss 0.02895919233560562 Validation loss 0.02877543680369854 Accuracy 0.70849609375\n",
      "Iteration 14190 Training loss 0.025799179449677467 Validation loss 0.029106011614203453 Accuracy 0.70458984375\n",
      "Iteration 14200 Training loss 0.026298915967345238 Validation loss 0.028884394094347954 Accuracy 0.70751953125\n",
      "Iteration 14210 Training loss 0.02766401134431362 Validation loss 0.028523340821266174 Accuracy 0.71142578125\n",
      "Iteration 14220 Training loss 0.027316750958561897 Validation loss 0.02889302372932434 Accuracy 0.7080078125\n",
      "Iteration 14230 Training loss 0.026629190891981125 Validation loss 0.02902214229106903 Accuracy 0.70654296875\n",
      "Iteration 14240 Training loss 0.02772100828588009 Validation loss 0.028914833441376686 Accuracy 0.70751953125\n",
      "Iteration 14250 Training loss 0.027089593932032585 Validation loss 0.028743309900164604 Accuracy 0.70947265625\n",
      "Iteration 14260 Training loss 0.02663353458046913 Validation loss 0.028909703716635704 Accuracy 0.70751953125\n",
      "Iteration 14270 Training loss 0.029397761449217796 Validation loss 0.0297551192343235 Accuracy 0.69873046875\n",
      "Iteration 14280 Training loss 0.026712046936154366 Validation loss 0.02914552390575409 Accuracy 0.70556640625\n",
      "Iteration 14290 Training loss 0.026199044659733772 Validation loss 0.02917228452861309 Accuracy 0.705078125\n",
      "Iteration 14300 Training loss 0.029520314186811447 Validation loss 0.030237482860684395 Accuracy 0.6943359375\n",
      "Iteration 14310 Training loss 0.027246449142694473 Validation loss 0.029849261045455933 Accuracy 0.69775390625\n",
      "Iteration 14320 Training loss 0.02724994346499443 Validation loss 0.028706371784210205 Accuracy 0.7099609375\n",
      "Iteration 14330 Training loss 0.028776779770851135 Validation loss 0.02903614193201065 Accuracy 0.70654296875\n",
      "Iteration 14340 Training loss 0.025165338069200516 Validation loss 0.02877080999314785 Accuracy 0.70947265625\n",
      "Iteration 14350 Training loss 0.02653113752603531 Validation loss 0.028935233131051064 Accuracy 0.70703125\n",
      "Iteration 14360 Training loss 0.025217141956090927 Validation loss 0.028764832764863968 Accuracy 0.708984375\n",
      "Iteration 14370 Training loss 0.025615569204092026 Validation loss 0.029073337092995644 Accuracy 0.70654296875\n",
      "Iteration 14380 Training loss 0.024906693026423454 Validation loss 0.028921332210302353 Accuracy 0.70751953125\n",
      "Iteration 14390 Training loss 0.02536458894610405 Validation loss 0.029012402519583702 Accuracy 0.70703125\n",
      "Iteration 14400 Training loss 0.027000531554222107 Validation loss 0.028670338913798332 Accuracy 0.7099609375\n",
      "Iteration 14410 Training loss 0.02736663818359375 Validation loss 0.02929883450269699 Accuracy 0.70361328125\n",
      "Iteration 14420 Training loss 0.028269851580262184 Validation loss 0.029504872858524323 Accuracy 0.70263671875\n",
      "Iteration 14430 Training loss 0.027110371738672256 Validation loss 0.028842445462942123 Accuracy 0.708984375\n",
      "Iteration 14440 Training loss 0.027175936847925186 Validation loss 0.02950207144021988 Accuracy 0.7021484375\n",
      "Iteration 14450 Training loss 0.02770639769732952 Validation loss 0.029252782464027405 Accuracy 0.703125\n",
      "Iteration 14460 Training loss 0.026077743619680405 Validation loss 0.029162924736738205 Accuracy 0.70458984375\n",
      "Iteration 14470 Training loss 0.024310946464538574 Validation loss 0.029269574210047722 Accuracy 0.70458984375\n",
      "Iteration 14480 Training loss 0.027174314484000206 Validation loss 0.02881457470357418 Accuracy 0.708984375\n",
      "Iteration 14490 Training loss 0.02706768549978733 Validation loss 0.028589600697159767 Accuracy 0.7109375\n",
      "Iteration 14500 Training loss 0.026216771453619003 Validation loss 0.028769711032509804 Accuracy 0.7099609375\n",
      "Iteration 14510 Training loss 0.027964558452367783 Validation loss 0.029554344713687897 Accuracy 0.70166015625\n",
      "Iteration 14520 Training loss 0.029045073315501213 Validation loss 0.028742415830492973 Accuracy 0.708984375\n",
      "Iteration 14530 Training loss 0.025084156543016434 Validation loss 0.02908601053059101 Accuracy 0.705078125\n",
      "Iteration 14540 Training loss 0.028788428753614426 Validation loss 0.029412152245640755 Accuracy 0.701171875\n",
      "Iteration 14550 Training loss 0.02617105282843113 Validation loss 0.028892699629068375 Accuracy 0.7080078125\n",
      "Iteration 14560 Training loss 0.02759387716650963 Validation loss 0.028990594670176506 Accuracy 0.70654296875\n",
      "Iteration 14570 Training loss 0.028447790071368217 Validation loss 0.029012776911258698 Accuracy 0.7060546875\n",
      "Iteration 14580 Training loss 0.025822332128882408 Validation loss 0.028700105845928192 Accuracy 0.7099609375\n",
      "Iteration 14590 Training loss 0.023898083716630936 Validation loss 0.02876446209847927 Accuracy 0.708984375\n",
      "Iteration 14600 Training loss 0.02471744269132614 Validation loss 0.02984912507236004 Accuracy 0.69775390625\n",
      "Iteration 14610 Training loss 0.02416493184864521 Validation loss 0.028782622888684273 Accuracy 0.708984375\n",
      "Iteration 14620 Training loss 0.02533915266394615 Validation loss 0.028766751289367676 Accuracy 0.70849609375\n",
      "Iteration 14630 Training loss 0.029993601143360138 Validation loss 0.030244657769799232 Accuracy 0.6943359375\n",
      "Iteration 14640 Training loss 0.027935903519392014 Validation loss 0.029184037819504738 Accuracy 0.70458984375\n",
      "Iteration 14650 Training loss 0.02585754357278347 Validation loss 0.02892877534031868 Accuracy 0.70751953125\n",
      "Iteration 14660 Training loss 0.028419751673936844 Validation loss 0.029278350993990898 Accuracy 0.70361328125\n",
      "Iteration 14670 Training loss 0.026373764500021935 Validation loss 0.02876352146267891 Accuracy 0.70849609375\n",
      "Iteration 14680 Training loss 0.025770235806703568 Validation loss 0.02865605428814888 Accuracy 0.7099609375\n",
      "Iteration 14690 Training loss 0.026308368891477585 Validation loss 0.029159193858504295 Accuracy 0.70458984375\n",
      "Iteration 14700 Training loss 0.02778627909719944 Validation loss 0.028618669137358665 Accuracy 0.71044921875\n",
      "Iteration 14710 Training loss 0.02851426601409912 Validation loss 0.02969362773001194 Accuracy 0.69970703125\n",
      "Iteration 14720 Training loss 0.027649009600281715 Validation loss 0.028790052980184555 Accuracy 0.70849609375\n",
      "Iteration 14730 Training loss 0.026560358703136444 Validation loss 0.0285797119140625 Accuracy 0.7109375\n",
      "Iteration 14740 Training loss 0.027448810636997223 Validation loss 0.02885892242193222 Accuracy 0.7080078125\n",
      "Iteration 14750 Training loss 0.027957195416092873 Validation loss 0.02899410016834736 Accuracy 0.70703125\n",
      "Iteration 14760 Training loss 0.028159363195300102 Validation loss 0.02861383929848671 Accuracy 0.7109375\n",
      "Iteration 14770 Training loss 0.02399289980530739 Validation loss 0.028762277215719223 Accuracy 0.708984375\n",
      "Iteration 14780 Training loss 0.024357924237847328 Validation loss 0.028777286410331726 Accuracy 0.708984375\n",
      "Iteration 14790 Training loss 0.02465084008872509 Validation loss 0.028910867869853973 Accuracy 0.7080078125\n",
      "Iteration 14800 Training loss 0.025979960337281227 Validation loss 0.02977772057056427 Accuracy 0.69970703125\n",
      "Iteration 14810 Training loss 0.028867090120911598 Validation loss 0.029359940439462662 Accuracy 0.703125\n",
      "Iteration 14820 Training loss 0.027808351442217827 Validation loss 0.028776297345757484 Accuracy 0.70849609375\n",
      "Iteration 14830 Training loss 0.026729557663202286 Validation loss 0.028920497745275497 Accuracy 0.70751953125\n",
      "Iteration 14840 Training loss 0.02579578384757042 Validation loss 0.02891010418534279 Accuracy 0.70703125\n",
      "Iteration 14850 Training loss 0.024135923013091087 Validation loss 0.029217010363936424 Accuracy 0.705078125\n",
      "Iteration 14860 Training loss 0.02687416598200798 Validation loss 0.029424218460917473 Accuracy 0.70263671875\n",
      "Iteration 14870 Training loss 0.031745269894599915 Validation loss 0.02912035584449768 Accuracy 0.705078125\n",
      "Iteration 14880 Training loss 0.024865299463272095 Validation loss 0.028826236724853516 Accuracy 0.7080078125\n",
      "Iteration 14890 Training loss 0.03022211603820324 Validation loss 0.02944287471473217 Accuracy 0.703125\n",
      "Iteration 14900 Training loss 0.028378920629620552 Validation loss 0.028594626113772392 Accuracy 0.71044921875\n",
      "Iteration 14910 Training loss 0.0272519551217556 Validation loss 0.028647571802139282 Accuracy 0.70947265625\n",
      "Iteration 14920 Training loss 0.02854287624359131 Validation loss 0.028814902529120445 Accuracy 0.7080078125\n",
      "Iteration 14930 Training loss 0.02517438307404518 Validation loss 0.02857130765914917 Accuracy 0.7109375\n",
      "Iteration 14940 Training loss 0.02527463808655739 Validation loss 0.02880251593887806 Accuracy 0.70849609375\n",
      "Iteration 14950 Training loss 0.027479004114866257 Validation loss 0.028857363387942314 Accuracy 0.70751953125\n",
      "Iteration 14960 Training loss 0.02738797292113304 Validation loss 0.028552019968628883 Accuracy 0.71142578125\n",
      "Iteration 14970 Training loss 0.024936899542808533 Validation loss 0.028552575036883354 Accuracy 0.7109375\n",
      "Iteration 14980 Training loss 0.029332229867577553 Validation loss 0.02891102060675621 Accuracy 0.70703125\n",
      "Iteration 14990 Training loss 0.028069768100976944 Validation loss 0.029278812929987907 Accuracy 0.7041015625\n",
      "Iteration 15000 Training loss 0.028881840407848358 Validation loss 0.029705557972192764 Accuracy 0.69873046875\n",
      "Iteration 15010 Training loss 0.026405857875943184 Validation loss 0.02884414978325367 Accuracy 0.70751953125\n",
      "Iteration 15020 Training loss 0.02661711536347866 Validation loss 0.028780873864889145 Accuracy 0.70751953125\n",
      "Iteration 15030 Training loss 0.028844153508543968 Validation loss 0.028955336660146713 Accuracy 0.70654296875\n",
      "Iteration 15040 Training loss 0.02784401923418045 Validation loss 0.029704425483942032 Accuracy 0.69970703125\n",
      "Iteration 15050 Training loss 0.027326349169015884 Validation loss 0.028652600944042206 Accuracy 0.71044921875\n",
      "Iteration 15060 Training loss 0.026141244918107986 Validation loss 0.029006462544202805 Accuracy 0.7060546875\n",
      "Iteration 15070 Training loss 0.02431502752006054 Validation loss 0.028698554262518883 Accuracy 0.70947265625\n",
      "Iteration 15080 Training loss 0.026751818135380745 Validation loss 0.02937089093029499 Accuracy 0.7021484375\n",
      "Iteration 15090 Training loss 0.03046022541821003 Validation loss 0.02933681383728981 Accuracy 0.703125\n",
      "Iteration 15100 Training loss 0.028609542176127434 Validation loss 0.029348211362957954 Accuracy 0.70263671875\n",
      "Iteration 15110 Training loss 0.030093805864453316 Validation loss 0.028779227286577225 Accuracy 0.7080078125\n",
      "Iteration 15120 Training loss 0.029022278264164925 Validation loss 0.028668172657489777 Accuracy 0.7099609375\n",
      "Iteration 15130 Training loss 0.024491911754012108 Validation loss 0.02874181792140007 Accuracy 0.708984375\n",
      "Iteration 15140 Training loss 0.026046890765428543 Validation loss 0.028954505920410156 Accuracy 0.70703125\n",
      "Iteration 15150 Training loss 0.025412440299987793 Validation loss 0.029096048325300217 Accuracy 0.70458984375\n",
      "Iteration 15160 Training loss 0.02763514406979084 Validation loss 0.028983257710933685 Accuracy 0.7060546875\n",
      "Iteration 15170 Training loss 0.028158225119113922 Validation loss 0.028671158477663994 Accuracy 0.7099609375\n",
      "Iteration 15180 Training loss 0.025824472308158875 Validation loss 0.02863140031695366 Accuracy 0.71044921875\n",
      "Iteration 15190 Training loss 0.030500678345561028 Validation loss 0.029281744733452797 Accuracy 0.70361328125\n",
      "Iteration 15200 Training loss 0.027460595592856407 Validation loss 0.028818557038903236 Accuracy 0.70849609375\n",
      "Iteration 15210 Training loss 0.028045471757650375 Validation loss 0.028798788785934448 Accuracy 0.7080078125\n",
      "Iteration 15220 Training loss 0.027553711086511612 Validation loss 0.028843510895967484 Accuracy 0.70751953125\n",
      "Iteration 15230 Training loss 0.026832468807697296 Validation loss 0.028862467035651207 Accuracy 0.70849609375\n",
      "Iteration 15240 Training loss 0.027639253064990044 Validation loss 0.02903471328318119 Accuracy 0.70654296875\n",
      "Iteration 15250 Training loss 0.02687174454331398 Validation loss 0.028589842841029167 Accuracy 0.71044921875\n",
      "Iteration 15260 Training loss 0.028672732412815094 Validation loss 0.02892426773905754 Accuracy 0.70703125\n",
      "Iteration 15270 Training loss 0.026654517278075218 Validation loss 0.028734857216477394 Accuracy 0.70849609375\n",
      "Iteration 15280 Training loss 0.027434775605797768 Validation loss 0.02891857922077179 Accuracy 0.70751953125\n",
      "Iteration 15290 Training loss 0.0284833125770092 Validation loss 0.029229970648884773 Accuracy 0.70361328125\n",
      "Iteration 15300 Training loss 0.027418462559580803 Validation loss 0.028624946251511574 Accuracy 0.71044921875\n",
      "Iteration 15310 Training loss 0.02951316349208355 Validation loss 0.02880711853504181 Accuracy 0.7080078125\n",
      "Iteration 15320 Training loss 0.028259973973035812 Validation loss 0.028780153021216393 Accuracy 0.7080078125\n",
      "Iteration 15330 Training loss 0.027530161663889885 Validation loss 0.02869127318263054 Accuracy 0.70947265625\n",
      "Iteration 15340 Training loss 0.026831340044736862 Validation loss 0.02896578051149845 Accuracy 0.70654296875\n",
      "Iteration 15350 Training loss 0.025871146470308304 Validation loss 0.02999257482588291 Accuracy 0.69677734375\n",
      "Iteration 15360 Training loss 0.027851073071360588 Validation loss 0.02904435247182846 Accuracy 0.70654296875\n",
      "Iteration 15370 Training loss 0.02663535624742508 Validation loss 0.02856694720685482 Accuracy 0.71044921875\n",
      "Iteration 15380 Training loss 0.023277562111616135 Validation loss 0.028924433514475822 Accuracy 0.70751953125\n",
      "Iteration 15390 Training loss 0.028854288160800934 Validation loss 0.02852041833102703 Accuracy 0.71142578125\n",
      "Iteration 15400 Training loss 0.027565358206629753 Validation loss 0.029225779697299004 Accuracy 0.7041015625\n",
      "Iteration 15410 Training loss 0.026203829795122147 Validation loss 0.02849626913666725 Accuracy 0.71142578125\n",
      "Iteration 15420 Training loss 0.02798226848244667 Validation loss 0.028530478477478027 Accuracy 0.7109375\n",
      "Iteration 15430 Training loss 0.028207015246152878 Validation loss 0.028830571100115776 Accuracy 0.7080078125\n",
      "Iteration 15440 Training loss 0.02995985373854637 Validation loss 0.028747033327817917 Accuracy 0.708984375\n",
      "Iteration 15450 Training loss 0.029488224536180496 Validation loss 0.02862458862364292 Accuracy 0.7099609375\n",
      "Iteration 15460 Training loss 0.028547795489430428 Validation loss 0.029495514929294586 Accuracy 0.701171875\n",
      "Iteration 15470 Training loss 0.027461839839816093 Validation loss 0.028607172891497612 Accuracy 0.7099609375\n",
      "Iteration 15480 Training loss 0.024769319221377373 Validation loss 0.02890758216381073 Accuracy 0.70751953125\n",
      "Iteration 15490 Training loss 0.023016637191176414 Validation loss 0.029176348820328712 Accuracy 0.705078125\n",
      "Iteration 15500 Training loss 0.026797780767083168 Validation loss 0.028683751821517944 Accuracy 0.7099609375\n",
      "Iteration 15510 Training loss 0.02762707695364952 Validation loss 0.028529999777674675 Accuracy 0.7109375\n",
      "Iteration 15520 Training loss 0.027283543720841408 Validation loss 0.028554296121001244 Accuracy 0.7109375\n",
      "Iteration 15530 Training loss 0.026150893419981003 Validation loss 0.029004577547311783 Accuracy 0.7060546875\n",
      "Iteration 15540 Training loss 0.02851095050573349 Validation loss 0.029079310595989227 Accuracy 0.705078125\n",
      "Iteration 15550 Training loss 0.02688070572912693 Validation loss 0.029138773679733276 Accuracy 0.70458984375\n",
      "Iteration 15560 Training loss 0.02720101736485958 Validation loss 0.028801022097468376 Accuracy 0.70849609375\n",
      "Iteration 15570 Training loss 0.028246134519577026 Validation loss 0.02840920351445675 Accuracy 0.71240234375\n",
      "Iteration 15580 Training loss 0.029442250728607178 Validation loss 0.02867049165070057 Accuracy 0.7099609375\n",
      "Iteration 15590 Training loss 0.026975011453032494 Validation loss 0.028580419719219208 Accuracy 0.7109375\n",
      "Iteration 15600 Training loss 0.028925776481628418 Validation loss 0.028890427201986313 Accuracy 0.70751953125\n",
      "Iteration 15610 Training loss 0.02424812503159046 Validation loss 0.02936588227748871 Accuracy 0.70263671875\n",
      "Iteration 15620 Training loss 0.026513447985053062 Validation loss 0.029065614566206932 Accuracy 0.70654296875\n",
      "Iteration 15630 Training loss 0.027007445693016052 Validation loss 0.029593423008918762 Accuracy 0.7001953125\n",
      "Iteration 15640 Training loss 0.027169177308678627 Validation loss 0.029858244583010674 Accuracy 0.69775390625\n",
      "Iteration 15650 Training loss 0.027015721425414085 Validation loss 0.028376488015055656 Accuracy 0.71240234375\n",
      "Iteration 15660 Training loss 0.026808246970176697 Validation loss 0.0285959392786026 Accuracy 0.71044921875\n",
      "Iteration 15670 Training loss 0.024915648624300957 Validation loss 0.028679510578513145 Accuracy 0.7099609375\n",
      "Iteration 15680 Training loss 0.027676090598106384 Validation loss 0.029075630009174347 Accuracy 0.70556640625\n",
      "Iteration 15690 Training loss 0.025243230164051056 Validation loss 0.028418662026524544 Accuracy 0.7119140625\n",
      "Iteration 15700 Training loss 0.025940347462892532 Validation loss 0.028416968882083893 Accuracy 0.71240234375\n",
      "Iteration 15710 Training loss 0.026327570900321007 Validation loss 0.02886977046728134 Accuracy 0.70751953125\n",
      "Iteration 15720 Training loss 0.02781856805086136 Validation loss 0.02887634187936783 Accuracy 0.7080078125\n",
      "Iteration 15730 Training loss 0.030889037996530533 Validation loss 0.029065879061818123 Accuracy 0.7060546875\n",
      "Iteration 15740 Training loss 0.02683097869157791 Validation loss 0.028889266774058342 Accuracy 0.70751953125\n",
      "Iteration 15750 Training loss 0.026398664340376854 Validation loss 0.028674710541963577 Accuracy 0.7099609375\n",
      "Iteration 15760 Training loss 0.026173153892159462 Validation loss 0.02874951995909214 Accuracy 0.708984375\n",
      "Iteration 15770 Training loss 0.028490131720900536 Validation loss 0.02863367274403572 Accuracy 0.7109375\n",
      "Iteration 15780 Training loss 0.026221882551908493 Validation loss 0.02875054068863392 Accuracy 0.70947265625\n",
      "Iteration 15790 Training loss 0.02648361213505268 Validation loss 0.028756141662597656 Accuracy 0.70849609375\n",
      "Iteration 15800 Training loss 0.029002029448747635 Validation loss 0.029000146314501762 Accuracy 0.70654296875\n",
      "Iteration 15810 Training loss 0.02655169367790222 Validation loss 0.028786586597561836 Accuracy 0.7080078125\n",
      "Iteration 15820 Training loss 0.025943730026483536 Validation loss 0.0286575797945261 Accuracy 0.7099609375\n",
      "Iteration 15830 Training loss 0.026136480271816254 Validation loss 0.02870883233845234 Accuracy 0.708984375\n",
      "Iteration 15840 Training loss 0.02714276686310768 Validation loss 0.028689589351415634 Accuracy 0.70947265625\n",
      "Iteration 15850 Training loss 0.028463546186685562 Validation loss 0.029105169698596 Accuracy 0.70556640625\n",
      "Iteration 15860 Training loss 0.023138681426644325 Validation loss 0.02873217687010765 Accuracy 0.708984375\n",
      "Iteration 15870 Training loss 0.02566872164607048 Validation loss 0.02856181561946869 Accuracy 0.7109375\n",
      "Iteration 15880 Training loss 0.027778934687376022 Validation loss 0.028952917084097862 Accuracy 0.70654296875\n",
      "Iteration 15890 Training loss 0.02540244534611702 Validation loss 0.028578517958521843 Accuracy 0.71044921875\n",
      "Iteration 15900 Training loss 0.02526596374809742 Validation loss 0.029527131468057632 Accuracy 0.70068359375\n",
      "Iteration 15910 Training loss 0.028654716908931732 Validation loss 0.029076287522912025 Accuracy 0.705078125\n",
      "Iteration 15920 Training loss 0.029030969366431236 Validation loss 0.028586337342858315 Accuracy 0.71044921875\n",
      "Iteration 15930 Training loss 0.02641145884990692 Validation loss 0.02916860766708851 Accuracy 0.705078125\n",
      "Iteration 15940 Training loss 0.02944430522620678 Validation loss 0.028600521385669708 Accuracy 0.7099609375\n",
      "Iteration 15950 Training loss 0.027201535180211067 Validation loss 0.028407571837306023 Accuracy 0.71240234375\n",
      "Iteration 15960 Training loss 0.029488954693078995 Validation loss 0.02828778512775898 Accuracy 0.71337890625\n",
      "Iteration 15970 Training loss 0.027521174401044846 Validation loss 0.028342323377728462 Accuracy 0.712890625\n",
      "Iteration 15980 Training loss 0.027995087206363678 Validation loss 0.028898481279611588 Accuracy 0.70751953125\n",
      "Iteration 15990 Training loss 0.02489233948290348 Validation loss 0.028272485360503197 Accuracy 0.71337890625\n",
      "Iteration 16000 Training loss 0.025246210396289825 Validation loss 0.028532830998301506 Accuracy 0.71142578125\n",
      "Iteration 16010 Training loss 0.026118475943803787 Validation loss 0.029841165989637375 Accuracy 0.6982421875\n",
      "Iteration 16020 Training loss 0.02783096209168434 Validation loss 0.029725508764386177 Accuracy 0.69921875\n",
      "Iteration 16030 Training loss 0.025576455518603325 Validation loss 0.028797708451747894 Accuracy 0.708984375\n",
      "Iteration 16040 Training loss 0.029350005090236664 Validation loss 0.02871541492640972 Accuracy 0.70947265625\n",
      "Iteration 16050 Training loss 0.026545079424977303 Validation loss 0.028791949152946472 Accuracy 0.7080078125\n",
      "Iteration 16060 Training loss 0.02736673504114151 Validation loss 0.03004133142530918 Accuracy 0.69580078125\n",
      "Iteration 16070 Training loss 0.027132445946335793 Validation loss 0.028754955157637596 Accuracy 0.70849609375\n",
      "Iteration 16080 Training loss 0.025183385238051414 Validation loss 0.028528522700071335 Accuracy 0.71142578125\n",
      "Iteration 16090 Training loss 0.02427089214324951 Validation loss 0.029019813984632492 Accuracy 0.7060546875\n",
      "Iteration 16100 Training loss 0.024696186184883118 Validation loss 0.02877250872552395 Accuracy 0.70849609375\n",
      "Iteration 16110 Training loss 0.023990876972675323 Validation loss 0.028558533638715744 Accuracy 0.7109375\n",
      "Iteration 16120 Training loss 0.02887626923620701 Validation loss 0.029187416657805443 Accuracy 0.70458984375\n",
      "Iteration 16130 Training loss 0.02733682654798031 Validation loss 0.028716444969177246 Accuracy 0.70947265625\n",
      "Iteration 16140 Training loss 0.024502191692590714 Validation loss 0.028528768569231033 Accuracy 0.7109375\n",
      "Iteration 16150 Training loss 0.026318155229091644 Validation loss 0.028468148782849312 Accuracy 0.71240234375\n",
      "Iteration 16160 Training loss 0.026428140699863434 Validation loss 0.028567353263497353 Accuracy 0.7109375\n",
      "Iteration 16170 Training loss 0.02588110789656639 Validation loss 0.028854364529252052 Accuracy 0.7080078125\n",
      "Iteration 16180 Training loss 0.024941308423876762 Validation loss 0.028617005795240402 Accuracy 0.7109375\n",
      "Iteration 16190 Training loss 0.026356536895036697 Validation loss 0.02861517108976841 Accuracy 0.71044921875\n",
      "Iteration 16200 Training loss 0.027023302391171455 Validation loss 0.028507215902209282 Accuracy 0.71240234375\n",
      "Iteration 16210 Training loss 0.028814323246479034 Validation loss 0.030123380944132805 Accuracy 0.69482421875\n",
      "Iteration 16220 Training loss 0.026764601469039917 Validation loss 0.028406109660863876 Accuracy 0.712890625\n",
      "Iteration 16230 Training loss 0.027945630252361298 Validation loss 0.028445463627576828 Accuracy 0.71240234375\n",
      "Iteration 16240 Training loss 0.029141707345843315 Validation loss 0.028794799000024796 Accuracy 0.70849609375\n",
      "Iteration 16250 Training loss 0.025690123438835144 Validation loss 0.028454627841711044 Accuracy 0.712890625\n",
      "Iteration 16260 Training loss 0.023523051291704178 Validation loss 0.02863691747188568 Accuracy 0.7109375\n",
      "Iteration 16270 Training loss 0.026478691026568413 Validation loss 0.02849128656089306 Accuracy 0.71142578125\n",
      "Iteration 16280 Training loss 0.024613358080387115 Validation loss 0.028527801856398582 Accuracy 0.7119140625\n",
      "Iteration 16290 Training loss 0.028091639280319214 Validation loss 0.028596488758921623 Accuracy 0.7109375\n",
      "Iteration 16300 Training loss 0.029377151280641556 Validation loss 0.030283495783805847 Accuracy 0.693359375\n",
      "Iteration 16310 Training loss 0.027596913278102875 Validation loss 0.028621453791856766 Accuracy 0.7099609375\n",
      "Iteration 16320 Training loss 0.02542787976562977 Validation loss 0.02850646898150444 Accuracy 0.71142578125\n",
      "Iteration 16330 Training loss 0.024037688970565796 Validation loss 0.02848127856850624 Accuracy 0.7119140625\n",
      "Iteration 16340 Training loss 0.027550866827368736 Validation loss 0.029011353850364685 Accuracy 0.7060546875\n",
      "Iteration 16350 Training loss 0.023622803390026093 Validation loss 0.02864144742488861 Accuracy 0.7099609375\n",
      "Iteration 16360 Training loss 0.026520846411585808 Validation loss 0.028528636321425438 Accuracy 0.71142578125\n",
      "Iteration 16370 Training loss 0.024407923221588135 Validation loss 0.028562821447849274 Accuracy 0.7109375\n",
      "Iteration 16380 Training loss 0.027685634791851044 Validation loss 0.02865629270672798 Accuracy 0.7099609375\n",
      "Iteration 16390 Training loss 0.026372665539383888 Validation loss 0.028650887310504913 Accuracy 0.70947265625\n",
      "Iteration 16400 Training loss 0.026801522821187973 Validation loss 0.028566868975758553 Accuracy 0.71044921875\n",
      "Iteration 16410 Training loss 0.024060791358351707 Validation loss 0.028483787551522255 Accuracy 0.71240234375\n",
      "Iteration 16420 Training loss 0.02642103284597397 Validation loss 0.02900794893503189 Accuracy 0.70703125\n",
      "Iteration 16430 Training loss 0.027657384052872658 Validation loss 0.028417132794857025 Accuracy 0.7119140625\n",
      "Iteration 16440 Training loss 0.02704765275120735 Validation loss 0.028469808399677277 Accuracy 0.71142578125\n",
      "Iteration 16450 Training loss 0.0226856991648674 Validation loss 0.02850279211997986 Accuracy 0.71142578125\n",
      "Iteration 16460 Training loss 0.02555757202208042 Validation loss 0.02848171815276146 Accuracy 0.71240234375\n",
      "Iteration 16470 Training loss 0.026895880699157715 Validation loss 0.030102914199233055 Accuracy 0.6953125\n",
      "Iteration 16480 Training loss 0.025704296305775642 Validation loss 0.02919645980000496 Accuracy 0.7041015625\n",
      "Iteration 16490 Training loss 0.02565661631524563 Validation loss 0.028889188542962074 Accuracy 0.70751953125\n",
      "Iteration 16500 Training loss 0.02642880752682686 Validation loss 0.028540803119540215 Accuracy 0.7109375\n",
      "Iteration 16510 Training loss 0.02441651187837124 Validation loss 0.028781777247786522 Accuracy 0.7080078125\n",
      "Iteration 16520 Training loss 0.02630864642560482 Validation loss 0.02858041413128376 Accuracy 0.7109375\n",
      "Iteration 16530 Training loss 0.02832176722586155 Validation loss 0.029700778424739838 Accuracy 0.69921875\n",
      "Iteration 16540 Training loss 0.026938699185848236 Validation loss 0.028798120096325874 Accuracy 0.70849609375\n",
      "Iteration 16550 Training loss 0.027007486671209335 Validation loss 0.029008774086833 Accuracy 0.70654296875\n",
      "Iteration 16560 Training loss 0.025774845853447914 Validation loss 0.028418846428394318 Accuracy 0.71240234375\n",
      "Iteration 16570 Training loss 0.02686811238527298 Validation loss 0.029440516605973244 Accuracy 0.70263671875\n",
      "Iteration 16580 Training loss 0.027274368330836296 Validation loss 0.028920477256178856 Accuracy 0.70751953125\n",
      "Iteration 16590 Training loss 0.026998097077012062 Validation loss 0.028639059513807297 Accuracy 0.7099609375\n",
      "Iteration 16600 Training loss 0.02557702548801899 Validation loss 0.028908979147672653 Accuracy 0.7080078125\n",
      "Iteration 16610 Training loss 0.024584487080574036 Validation loss 0.029077865183353424 Accuracy 0.70556640625\n",
      "Iteration 16620 Training loss 0.027618665248155594 Validation loss 0.02934172749519348 Accuracy 0.703125\n",
      "Iteration 16630 Training loss 0.02657770738005638 Validation loss 0.02865682542324066 Accuracy 0.70947265625\n",
      "Iteration 16640 Training loss 0.024137746542692184 Validation loss 0.02893533930182457 Accuracy 0.70703125\n",
      "Iteration 16650 Training loss 0.02864907868206501 Validation loss 0.028954165056347847 Accuracy 0.70654296875\n",
      "Iteration 16660 Training loss 0.02727871760725975 Validation loss 0.028522372245788574 Accuracy 0.7119140625\n",
      "Iteration 16670 Training loss 0.027574757114052773 Validation loss 0.028725726529955864 Accuracy 0.708984375\n",
      "Iteration 16680 Training loss 0.026949653401970863 Validation loss 0.02860262617468834 Accuracy 0.7109375\n",
      "Iteration 16690 Training loss 0.02622196450829506 Validation loss 0.0287065077573061 Accuracy 0.708984375\n",
      "Iteration 16700 Training loss 0.028196293860673904 Validation loss 0.028742322698235512 Accuracy 0.70849609375\n",
      "Iteration 16710 Training loss 0.028853029012680054 Validation loss 0.028567587956786156 Accuracy 0.7109375\n",
      "Iteration 16720 Training loss 0.02509218081831932 Validation loss 0.028488889336586 Accuracy 0.71142578125\n",
      "Iteration 16730 Training loss 0.0282319076359272 Validation loss 0.02924385666847229 Accuracy 0.7041015625\n",
      "Iteration 16740 Training loss 0.02778027020394802 Validation loss 0.029012037441134453 Accuracy 0.70751953125\n",
      "Iteration 16750 Training loss 0.032153040170669556 Validation loss 0.030992543324828148 Accuracy 0.6865234375\n",
      "Iteration 16760 Training loss 0.023465653881430626 Validation loss 0.029164060950279236 Accuracy 0.7041015625\n",
      "Iteration 16770 Training loss 0.026718884706497192 Validation loss 0.028637459501624107 Accuracy 0.7099609375\n",
      "Iteration 16780 Training loss 0.02708030678331852 Validation loss 0.02937636896967888 Accuracy 0.70361328125\n",
      "Iteration 16790 Training loss 0.024764835834503174 Validation loss 0.02864077128469944 Accuracy 0.71044921875\n",
      "Iteration 16800 Training loss 0.028756335377693176 Validation loss 0.028951434418559074 Accuracy 0.70703125\n",
      "Iteration 16810 Training loss 0.03499799594283104 Validation loss 0.033496417105197906 Accuracy 0.66162109375\n",
      "Iteration 16820 Training loss 0.02718980424106121 Validation loss 0.02858886867761612 Accuracy 0.71044921875\n",
      "Iteration 16830 Training loss 0.02621111273765564 Validation loss 0.02850324846804142 Accuracy 0.71240234375\n",
      "Iteration 16840 Training loss 0.02862147055566311 Validation loss 0.0287700854241848 Accuracy 0.7080078125\n",
      "Iteration 16850 Training loss 0.02638758346438408 Validation loss 0.02928810939192772 Accuracy 0.70361328125\n",
      "Iteration 16860 Training loss 0.026681985706090927 Validation loss 0.028865570202469826 Accuracy 0.7080078125\n",
      "Iteration 16870 Training loss 0.026715753600001335 Validation loss 0.028665848076343536 Accuracy 0.7099609375\n",
      "Iteration 16880 Training loss 0.026664040982723236 Validation loss 0.02832990325987339 Accuracy 0.71337890625\n",
      "Iteration 16890 Training loss 0.028263874351978302 Validation loss 0.02844766527414322 Accuracy 0.71142578125\n",
      "Iteration 16900 Training loss 0.0250670425593853 Validation loss 0.02913886122405529 Accuracy 0.705078125\n",
      "Iteration 16910 Training loss 0.026707220822572708 Validation loss 0.028796613216400146 Accuracy 0.70849609375\n",
      "Iteration 16920 Training loss 0.025670601055026054 Validation loss 0.029293466359376907 Accuracy 0.70361328125\n",
      "Iteration 16930 Training loss 0.026277339085936546 Validation loss 0.02934083715081215 Accuracy 0.70361328125\n",
      "Iteration 16940 Training loss 0.022916991263628006 Validation loss 0.028916332870721817 Accuracy 0.70751953125\n",
      "Iteration 16950 Training loss 0.025640100240707397 Validation loss 0.02891065925359726 Accuracy 0.70751953125\n",
      "Iteration 16960 Training loss 0.0243949256837368 Validation loss 0.029123563319444656 Accuracy 0.705078125\n",
      "Iteration 16970 Training loss 0.028948374092578888 Validation loss 0.02897470071911812 Accuracy 0.70654296875\n",
      "Iteration 16980 Training loss 0.02581036277115345 Validation loss 0.028661396354436874 Accuracy 0.71044921875\n",
      "Iteration 16990 Training loss 0.025912918150424957 Validation loss 0.029925774782896042 Accuracy 0.69775390625\n",
      "Iteration 17000 Training loss 0.027054347097873688 Validation loss 0.028417550027370453 Accuracy 0.7119140625\n",
      "Iteration 17010 Training loss 0.027780653908848763 Validation loss 0.02844090200960636 Accuracy 0.7119140625\n",
      "Iteration 17020 Training loss 0.02511422708630562 Validation loss 0.028457358479499817 Accuracy 0.7119140625\n",
      "Iteration 17030 Training loss 0.027984531596302986 Validation loss 0.028488045558333397 Accuracy 0.7119140625\n",
      "Iteration 17040 Training loss 0.02489607222378254 Validation loss 0.028701771050691605 Accuracy 0.7099609375\n",
      "Iteration 17050 Training loss 0.027425294741988182 Validation loss 0.029180219396948814 Accuracy 0.7041015625\n",
      "Iteration 17060 Training loss 0.029154105111956596 Validation loss 0.028883090242743492 Accuracy 0.7080078125\n",
      "Iteration 17070 Training loss 0.028802307322621346 Validation loss 0.030195636674761772 Accuracy 0.6943359375\n",
      "Iteration 17080 Training loss 0.028921304270625114 Validation loss 0.02841317467391491 Accuracy 0.712890625\n",
      "Iteration 17090 Training loss 0.026462223380804062 Validation loss 0.02882956527173519 Accuracy 0.70849609375\n",
      "Iteration 17100 Training loss 0.029808741062879562 Validation loss 0.028741830959916115 Accuracy 0.70947265625\n",
      "Iteration 17110 Training loss 0.023608626797795296 Validation loss 0.028997918590903282 Accuracy 0.70751953125\n",
      "Iteration 17120 Training loss 0.02571767568588257 Validation loss 0.02856551669538021 Accuracy 0.71044921875\n",
      "Iteration 17130 Training loss 0.02630164474248886 Validation loss 0.028702111914753914 Accuracy 0.70947265625\n",
      "Iteration 17140 Training loss 0.02580440230667591 Validation loss 0.0284001212567091 Accuracy 0.71240234375\n",
      "Iteration 17150 Training loss 0.02547856606543064 Validation loss 0.028503207489848137 Accuracy 0.71142578125\n",
      "Iteration 17160 Training loss 0.02388055995106697 Validation loss 0.02854798175394535 Accuracy 0.7109375\n",
      "Iteration 17170 Training loss 0.02465250715613365 Validation loss 0.028613390401005745 Accuracy 0.7099609375\n",
      "Iteration 17180 Training loss 0.026071369647979736 Validation loss 0.028512919321656227 Accuracy 0.71142578125\n",
      "Iteration 17190 Training loss 0.026720192283391953 Validation loss 0.028804071247577667 Accuracy 0.70849609375\n",
      "Iteration 17200 Training loss 0.02887614630162716 Validation loss 0.02913431078195572 Accuracy 0.70556640625\n",
      "Iteration 17210 Training loss 0.027831487357616425 Validation loss 0.029065653681755066 Accuracy 0.70556640625\n",
      "Iteration 17220 Training loss 0.02450958453118801 Validation loss 0.02833321876823902 Accuracy 0.7119140625\n",
      "Iteration 17230 Training loss 0.024746069684624672 Validation loss 0.030603701248764992 Accuracy 0.6904296875\n",
      "Iteration 17240 Training loss 0.025875698775053024 Validation loss 0.028489012271165848 Accuracy 0.71142578125\n",
      "Iteration 17250 Training loss 0.02502228505909443 Validation loss 0.028774023056030273 Accuracy 0.70849609375\n",
      "Iteration 17260 Training loss 0.027891822159290314 Validation loss 0.028398413211107254 Accuracy 0.712890625\n",
      "Iteration 17270 Training loss 0.0269093569368124 Validation loss 0.029555389657616615 Accuracy 0.70068359375\n",
      "Iteration 17280 Training loss 0.028676124289631844 Validation loss 0.029233872890472412 Accuracy 0.70361328125\n",
      "Iteration 17290 Training loss 0.028506480157375336 Validation loss 0.02871740236878395 Accuracy 0.708984375\n",
      "Iteration 17300 Training loss 0.02450205199420452 Validation loss 0.02838987670838833 Accuracy 0.712890625\n",
      "Iteration 17310 Training loss 0.026112670078873634 Validation loss 0.028536085039377213 Accuracy 0.7109375\n",
      "Iteration 17320 Training loss 0.023329518735408783 Validation loss 0.02866886556148529 Accuracy 0.70849609375\n",
      "Iteration 17330 Training loss 0.028549905866384506 Validation loss 0.03065130114555359 Accuracy 0.68896484375\n",
      "Iteration 17340 Training loss 0.027924982830882072 Validation loss 0.028914902359247208 Accuracy 0.70703125\n",
      "Iteration 17350 Training loss 0.027473904192447662 Validation loss 0.028732363134622574 Accuracy 0.708984375\n",
      "Iteration 17360 Training loss 0.027281975373625755 Validation loss 0.02896513044834137 Accuracy 0.70703125\n",
      "Iteration 17370 Training loss 0.026707392185926437 Validation loss 0.02871537022292614 Accuracy 0.708984375\n",
      "Iteration 17380 Training loss 0.02574436366558075 Validation loss 0.02842199057340622 Accuracy 0.7119140625\n",
      "Iteration 17390 Training loss 0.024192631244659424 Validation loss 0.028723714873194695 Accuracy 0.708984375\n",
      "Iteration 17400 Training loss 0.024793244898319244 Validation loss 0.028972484171390533 Accuracy 0.7060546875\n",
      "Iteration 17410 Training loss 0.025975337252020836 Validation loss 0.028351327404379845 Accuracy 0.712890625\n",
      "Iteration 17420 Training loss 0.02492785081267357 Validation loss 0.028414394706487656 Accuracy 0.7119140625\n",
      "Iteration 17430 Training loss 0.024587932974100113 Validation loss 0.028394026681780815 Accuracy 0.71240234375\n",
      "Iteration 17440 Training loss 0.027296319603919983 Validation loss 0.028345659375190735 Accuracy 0.71337890625\n",
      "Iteration 17450 Training loss 0.030674846842885017 Validation loss 0.02842017449438572 Accuracy 0.71240234375\n",
      "Iteration 17460 Training loss 0.024917010217905045 Validation loss 0.028689973056316376 Accuracy 0.70947265625\n",
      "Iteration 17470 Training loss 0.029741154983639717 Validation loss 0.030154701322317123 Accuracy 0.69482421875\n",
      "Iteration 17480 Training loss 0.022659609094262123 Validation loss 0.02865460328757763 Accuracy 0.70947265625\n",
      "Iteration 17490 Training loss 0.02702486701309681 Validation loss 0.02871851436793804 Accuracy 0.70947265625\n",
      "Iteration 17500 Training loss 0.027222251519560814 Validation loss 0.02881579101085663 Accuracy 0.70849609375\n",
      "Iteration 17510 Training loss 0.023636164143681526 Validation loss 0.028567533940076828 Accuracy 0.71142578125\n",
      "Iteration 17520 Training loss 0.023737838491797447 Validation loss 0.028364045545458794 Accuracy 0.71337890625\n",
      "Iteration 17530 Training loss 0.031915340572595596 Validation loss 0.02848311886191368 Accuracy 0.71142578125\n",
      "Iteration 17540 Training loss 0.025225013494491577 Validation loss 0.028260694816708565 Accuracy 0.71435546875\n",
      "Iteration 17550 Training loss 0.024244623258709908 Validation loss 0.02838272415101528 Accuracy 0.712890625\n",
      "Iteration 17560 Training loss 0.025058550760149956 Validation loss 0.028440948575735092 Accuracy 0.7119140625\n",
      "Iteration 17570 Training loss 0.024075597524642944 Validation loss 0.028391098603606224 Accuracy 0.712890625\n",
      "Iteration 17580 Training loss 0.023918310180306435 Validation loss 0.02851962298154831 Accuracy 0.71142578125\n",
      "Iteration 17590 Training loss 0.02715170569717884 Validation loss 0.028963636606931686 Accuracy 0.70654296875\n",
      "Iteration 17600 Training loss 0.025475816801190376 Validation loss 0.02864791639149189 Accuracy 0.708984375\n",
      "Iteration 17610 Training loss 0.02725013718008995 Validation loss 0.028706829994916916 Accuracy 0.7080078125\n",
      "Iteration 17620 Training loss 0.027732986956834793 Validation loss 0.028543628752231598 Accuracy 0.7099609375\n",
      "Iteration 17630 Training loss 0.02471906691789627 Validation loss 0.028618931770324707 Accuracy 0.71044921875\n",
      "Iteration 17640 Training loss 0.030102243646979332 Validation loss 0.028503788635134697 Accuracy 0.7119140625\n",
      "Iteration 17650 Training loss 0.02564232610166073 Validation loss 0.028506454080343246 Accuracy 0.71142578125\n",
      "Iteration 17660 Training loss 0.026016924530267715 Validation loss 0.028437569737434387 Accuracy 0.71240234375\n",
      "Iteration 17670 Training loss 0.03050568699836731 Validation loss 0.02869396097958088 Accuracy 0.708984375\n",
      "Iteration 17680 Training loss 0.027611160650849342 Validation loss 0.028400687500834465 Accuracy 0.712890625\n",
      "Iteration 17690 Training loss 0.02652481012046337 Validation loss 0.02904331311583519 Accuracy 0.70654296875\n",
      "Iteration 17700 Training loss 0.028531966730952263 Validation loss 0.02865571156144142 Accuracy 0.7099609375\n",
      "Iteration 17710 Training loss 0.023576974868774414 Validation loss 0.028858914971351624 Accuracy 0.7080078125\n",
      "Iteration 17720 Training loss 0.023724552243947983 Validation loss 0.028551019728183746 Accuracy 0.7109375\n",
      "Iteration 17730 Training loss 0.02543480321764946 Validation loss 0.028618693351745605 Accuracy 0.71044921875\n",
      "Iteration 17740 Training loss 0.026065194979310036 Validation loss 0.02902829833328724 Accuracy 0.7060546875\n",
      "Iteration 17750 Training loss 0.02796040289103985 Validation loss 0.03044263832271099 Accuracy 0.69189453125\n",
      "Iteration 17760 Training loss 0.02883050963282585 Validation loss 0.02843594178557396 Accuracy 0.71142578125\n",
      "Iteration 17770 Training loss 0.0269109345972538 Validation loss 0.028545284643769264 Accuracy 0.7109375\n",
      "Iteration 17780 Training loss 0.02796795964241028 Validation loss 0.028767062351107597 Accuracy 0.70849609375\n",
      "Iteration 17790 Training loss 0.02582266554236412 Validation loss 0.02902393974363804 Accuracy 0.70556640625\n",
      "Iteration 17800 Training loss 0.02540646307170391 Validation loss 0.02864047698676586 Accuracy 0.70947265625\n",
      "Iteration 17810 Training loss 0.024922769516706467 Validation loss 0.028626833111047745 Accuracy 0.7099609375\n",
      "Iteration 17820 Training loss 0.030604030936956406 Validation loss 0.029108693823218346 Accuracy 0.70458984375\n",
      "Iteration 17830 Training loss 0.022077152505517006 Validation loss 0.02854020707309246 Accuracy 0.71044921875\n",
      "Iteration 17840 Training loss 0.025641202926635742 Validation loss 0.028515534475445747 Accuracy 0.7109375\n",
      "Iteration 17850 Training loss 0.025553766638040543 Validation loss 0.028530096635222435 Accuracy 0.7109375\n",
      "Iteration 17860 Training loss 0.02779904566705227 Validation loss 0.02871142327785492 Accuracy 0.70947265625\n",
      "Iteration 17870 Training loss 0.025144802406430244 Validation loss 0.028790170326828957 Accuracy 0.7080078125\n",
      "Iteration 17880 Training loss 0.024676427245140076 Validation loss 0.02875322848558426 Accuracy 0.70849609375\n",
      "Iteration 17890 Training loss 0.022392217069864273 Validation loss 0.02885311469435692 Accuracy 0.7080078125\n",
      "Iteration 17900 Training loss 0.028249191120266914 Validation loss 0.02893214114010334 Accuracy 0.70751953125\n",
      "Iteration 17910 Training loss 0.024898884817957878 Validation loss 0.028774308040738106 Accuracy 0.708984375\n",
      "Iteration 17920 Training loss 0.026346620172262192 Validation loss 0.02865593694150448 Accuracy 0.70947265625\n",
      "Iteration 17930 Training loss 0.029074523597955704 Validation loss 0.02850755862891674 Accuracy 0.71142578125\n",
      "Iteration 17940 Training loss 0.02536330372095108 Validation loss 0.02841823734343052 Accuracy 0.7119140625\n",
      "Iteration 17950 Training loss 0.022875070571899414 Validation loss 0.028342559933662415 Accuracy 0.71337890625\n",
      "Iteration 17960 Training loss 0.025926925241947174 Validation loss 0.02833966724574566 Accuracy 0.712890625\n",
      "Iteration 17970 Training loss 0.023907555267214775 Validation loss 0.028320206329226494 Accuracy 0.71337890625\n",
      "Iteration 17980 Training loss 0.025170618668198586 Validation loss 0.028267113491892815 Accuracy 0.71435546875\n",
      "Iteration 17990 Training loss 0.025276077911257744 Validation loss 0.029154282063245773 Accuracy 0.7041015625\n",
      "Iteration 18000 Training loss 0.030292728915810585 Validation loss 0.028423549607396126 Accuracy 0.7119140625\n",
      "Iteration 18010 Training loss 0.027667976915836334 Validation loss 0.02853984758257866 Accuracy 0.71142578125\n",
      "Iteration 18020 Training loss 0.02735256962478161 Validation loss 0.028461016714572906 Accuracy 0.71142578125\n",
      "Iteration 18030 Training loss 0.02814434841275215 Validation loss 0.028804393485188484 Accuracy 0.7080078125\n",
      "Iteration 18040 Training loss 0.023308731615543365 Validation loss 0.028591470792889595 Accuracy 0.7109375\n",
      "Iteration 18050 Training loss 0.027932027354836464 Validation loss 0.028498515486717224 Accuracy 0.71142578125\n",
      "Iteration 18060 Training loss 0.02368907630443573 Validation loss 0.028453823179006577 Accuracy 0.7119140625\n",
      "Iteration 18070 Training loss 0.02387136220932007 Validation loss 0.028498275205492973 Accuracy 0.71142578125\n",
      "Iteration 18080 Training loss 0.02823742851614952 Validation loss 0.028595492243766785 Accuracy 0.7099609375\n",
      "Iteration 18090 Training loss 0.025387180969119072 Validation loss 0.028438348323106766 Accuracy 0.7119140625\n",
      "Iteration 18100 Training loss 0.028405914083123207 Validation loss 0.02904137596487999 Accuracy 0.7060546875\n",
      "Iteration 18110 Training loss 0.027513394132256508 Validation loss 0.028435610234737396 Accuracy 0.7119140625\n",
      "Iteration 18120 Training loss 0.027146978303790092 Validation loss 0.02911710925400257 Accuracy 0.705078125\n",
      "Iteration 18130 Training loss 0.024584416300058365 Validation loss 0.028721751645207405 Accuracy 0.70947265625\n",
      "Iteration 18140 Training loss 0.026755794882774353 Validation loss 0.028739018365740776 Accuracy 0.708984375\n",
      "Iteration 18150 Training loss 0.026858525350689888 Validation loss 0.028886836022138596 Accuracy 0.70654296875\n",
      "Iteration 18160 Training loss 0.02671665884554386 Validation loss 0.028390564024448395 Accuracy 0.712890625\n",
      "Iteration 18170 Training loss 0.024624986574053764 Validation loss 0.028411561623215675 Accuracy 0.7119140625\n",
      "Iteration 18180 Training loss 0.025916997343301773 Validation loss 0.028441132977604866 Accuracy 0.71240234375\n",
      "Iteration 18190 Training loss 0.026311371475458145 Validation loss 0.02860632725059986 Accuracy 0.71044921875\n",
      "Iteration 18200 Training loss 0.02433759719133377 Validation loss 0.028701862320303917 Accuracy 0.7099609375\n",
      "Iteration 18210 Training loss 0.024853413924574852 Validation loss 0.028503378853201866 Accuracy 0.71142578125\n",
      "Iteration 18220 Training loss 0.02513071708381176 Validation loss 0.028528444468975067 Accuracy 0.71142578125\n",
      "Iteration 18230 Training loss 0.022857539355754852 Validation loss 0.028712037950754166 Accuracy 0.708984375\n",
      "Iteration 18240 Training loss 0.026757270097732544 Validation loss 0.02854843996465206 Accuracy 0.7119140625\n",
      "Iteration 18250 Training loss 0.02484804205596447 Validation loss 0.029093554243445396 Accuracy 0.705078125\n",
      "Iteration 18260 Training loss 0.026565633714199066 Validation loss 0.028697000816464424 Accuracy 0.70947265625\n",
      "Iteration 18270 Training loss 0.02528558298945427 Validation loss 0.029060106724500656 Accuracy 0.7060546875\n",
      "Iteration 18280 Training loss 0.025906408205628395 Validation loss 0.02851361781358719 Accuracy 0.7109375\n",
      "Iteration 18290 Training loss 0.025885095819830894 Validation loss 0.0284268818795681 Accuracy 0.71240234375\n",
      "Iteration 18300 Training loss 0.025453943759202957 Validation loss 0.028642745688557625 Accuracy 0.7099609375\n",
      "Iteration 18310 Training loss 0.02659795619547367 Validation loss 0.028585797175765038 Accuracy 0.7109375\n",
      "Iteration 18320 Training loss 0.02596769481897354 Validation loss 0.02874136157333851 Accuracy 0.70849609375\n",
      "Iteration 18330 Training loss 0.02489916980266571 Validation loss 0.028728289529681206 Accuracy 0.70947265625\n",
      "Iteration 18340 Training loss 0.023647353053092957 Validation loss 0.028472570702433586 Accuracy 0.7119140625\n",
      "Iteration 18350 Training loss 0.02710423804819584 Validation loss 0.028411755338311195 Accuracy 0.7119140625\n",
      "Iteration 18360 Training loss 0.026167040690779686 Validation loss 0.028638172894716263 Accuracy 0.7099609375\n",
      "Iteration 18370 Training loss 0.02266639471054077 Validation loss 0.028680531308054924 Accuracy 0.70947265625\n",
      "Iteration 18380 Training loss 0.023084813728928566 Validation loss 0.028685377910733223 Accuracy 0.7099609375\n",
      "Iteration 18390 Training loss 0.02886839210987091 Validation loss 0.028782546520233154 Accuracy 0.708984375\n",
      "Iteration 18400 Training loss 0.024644244462251663 Validation loss 0.028684787452220917 Accuracy 0.7099609375\n",
      "Iteration 18410 Training loss 0.023597385734319687 Validation loss 0.02886710688471794 Accuracy 0.7080078125\n",
      "Iteration 18420 Training loss 0.025115076452493668 Validation loss 0.028526706621050835 Accuracy 0.71142578125\n",
      "Iteration 18430 Training loss 0.024114709347486496 Validation loss 0.028734803199768066 Accuracy 0.70947265625\n",
      "Iteration 18440 Training loss 0.027993174269795418 Validation loss 0.0286607276648283 Accuracy 0.70947265625\n",
      "Iteration 18450 Training loss 0.027323512360453606 Validation loss 0.028537465259432793 Accuracy 0.7109375\n",
      "Iteration 18460 Training loss 0.025735223665833473 Validation loss 0.028767473995685577 Accuracy 0.70849609375\n",
      "Iteration 18470 Training loss 0.025724219158291817 Validation loss 0.028695696964859962 Accuracy 0.70947265625\n",
      "Iteration 18480 Training loss 0.02563435770571232 Validation loss 0.02924519032239914 Accuracy 0.70361328125\n",
      "Iteration 18490 Training loss 0.027572376653552055 Validation loss 0.028831439092755318 Accuracy 0.7080078125\n",
      "Iteration 18500 Training loss 0.02432948350906372 Validation loss 0.028495077043771744 Accuracy 0.7119140625\n",
      "Iteration 18510 Training loss 0.028068842366337776 Validation loss 0.029622452333569527 Accuracy 0.70068359375\n",
      "Iteration 18520 Training loss 0.02646072953939438 Validation loss 0.02869151346385479 Accuracy 0.70947265625\n",
      "Iteration 18530 Training loss 0.02293470688164234 Validation loss 0.028815848752856255 Accuracy 0.708984375\n",
      "Iteration 18540 Training loss 0.02887735888361931 Validation loss 0.02876158617436886 Accuracy 0.708984375\n",
      "Iteration 18550 Training loss 0.024567432701587677 Validation loss 0.028332149609923363 Accuracy 0.712890625\n",
      "Iteration 18560 Training loss 0.02672097273170948 Validation loss 0.02863534539937973 Accuracy 0.7099609375\n",
      "Iteration 18570 Training loss 0.026219487190246582 Validation loss 0.028668789193034172 Accuracy 0.71044921875\n",
      "Iteration 18580 Training loss 0.025794001296162605 Validation loss 0.028606796637177467 Accuracy 0.7109375\n",
      "Iteration 18590 Training loss 0.02459600381553173 Validation loss 0.028421498835086823 Accuracy 0.7119140625\n",
      "Iteration 18600 Training loss 0.02502768114209175 Validation loss 0.028488144278526306 Accuracy 0.7109375\n",
      "Iteration 18610 Training loss 0.026060892269015312 Validation loss 0.028834085911512375 Accuracy 0.708984375\n",
      "Iteration 18620 Training loss 0.025342097505927086 Validation loss 0.02872559055685997 Accuracy 0.708984375\n",
      "Iteration 18630 Training loss 0.02716117911040783 Validation loss 0.028420111164450645 Accuracy 0.71142578125\n",
      "Iteration 18640 Training loss 0.02351410873234272 Validation loss 0.028548656031489372 Accuracy 0.7109375\n",
      "Iteration 18650 Training loss 0.024825068190693855 Validation loss 0.02903500385582447 Accuracy 0.70556640625\n",
      "Iteration 18660 Training loss 0.02542201429605484 Validation loss 0.02876090072095394 Accuracy 0.70947265625\n",
      "Iteration 18670 Training loss 0.02769777737557888 Validation loss 0.02836841717362404 Accuracy 0.712890625\n",
      "Iteration 18680 Training loss 0.0291274581104517 Validation loss 0.028525536879897118 Accuracy 0.7109375\n",
      "Iteration 18690 Training loss 0.02610042318701744 Validation loss 0.02846437133848667 Accuracy 0.71240234375\n",
      "Iteration 18700 Training loss 0.025848153978586197 Validation loss 0.028345683589577675 Accuracy 0.71337890625\n",
      "Iteration 18710 Training loss 0.025824518874287605 Validation loss 0.028439274057745934 Accuracy 0.71142578125\n",
      "Iteration 18720 Training loss 0.02664981596171856 Validation loss 0.028629237785935402 Accuracy 0.70947265625\n",
      "Iteration 18730 Training loss 0.024800654500722885 Validation loss 0.028813155367970467 Accuracy 0.70849609375\n",
      "Iteration 18740 Training loss 0.023200364783406258 Validation loss 0.028349364176392555 Accuracy 0.71240234375\n",
      "Iteration 18750 Training loss 0.025946451351046562 Validation loss 0.0286044180393219 Accuracy 0.70947265625\n",
      "Iteration 18760 Training loss 0.02691800892353058 Validation loss 0.030344100669026375 Accuracy 0.69384765625\n",
      "Iteration 18770 Training loss 0.025072362273931503 Validation loss 0.028629090636968613 Accuracy 0.71044921875\n",
      "Iteration 18780 Training loss 0.025712328031659126 Validation loss 0.028676288202404976 Accuracy 0.7099609375\n",
      "Iteration 18790 Training loss 0.024481775239109993 Validation loss 0.028829701244831085 Accuracy 0.7080078125\n",
      "Iteration 18800 Training loss 0.02574889548122883 Validation loss 0.028574081137776375 Accuracy 0.7109375\n",
      "Iteration 18810 Training loss 0.023782879114151 Validation loss 0.028462085872888565 Accuracy 0.7119140625\n",
      "Iteration 18820 Training loss 0.026442695409059525 Validation loss 0.028577301651239395 Accuracy 0.7109375\n",
      "Iteration 18830 Training loss 0.026058224961161613 Validation loss 0.028402358293533325 Accuracy 0.71240234375\n",
      "Iteration 18840 Training loss 0.02682722732424736 Validation loss 0.02860160544514656 Accuracy 0.7099609375\n",
      "Iteration 18850 Training loss 0.024591030552983284 Validation loss 0.028468644246459007 Accuracy 0.71142578125\n",
      "Iteration 18860 Training loss 0.02578127011656761 Validation loss 0.028875989839434624 Accuracy 0.70751953125\n",
      "Iteration 18870 Training loss 0.02302444912493229 Validation loss 0.02843564935028553 Accuracy 0.7119140625\n",
      "Iteration 18880 Training loss 0.02775365486741066 Validation loss 0.02836069092154503 Accuracy 0.712890625\n",
      "Iteration 18890 Training loss 0.025814618915319443 Validation loss 0.0287584587931633 Accuracy 0.70849609375\n",
      "Iteration 18900 Training loss 0.027168426662683487 Validation loss 0.02872646600008011 Accuracy 0.708984375\n",
      "Iteration 18910 Training loss 0.024932347238063812 Validation loss 0.0285280030220747 Accuracy 0.7109375\n",
      "Iteration 18920 Training loss 0.02607104741036892 Validation loss 0.02857365645468235 Accuracy 0.7099609375\n",
      "Iteration 18930 Training loss 0.024951178580522537 Validation loss 0.02885795757174492 Accuracy 0.7080078125\n",
      "Iteration 18940 Training loss 0.026990605518221855 Validation loss 0.029884586110711098 Accuracy 0.69677734375\n",
      "Iteration 18950 Training loss 0.024446237832307816 Validation loss 0.02869991585612297 Accuracy 0.70849609375\n",
      "Iteration 18960 Training loss 0.027467086911201477 Validation loss 0.02911778725683689 Accuracy 0.705078125\n",
      "Iteration 18970 Training loss 0.024510685354471207 Validation loss 0.02854420430958271 Accuracy 0.7119140625\n",
      "Iteration 18980 Training loss 0.02558719366788864 Validation loss 0.028336329385638237 Accuracy 0.71337890625\n",
      "Iteration 18990 Training loss 0.023978590965270996 Validation loss 0.02857835590839386 Accuracy 0.7109375\n",
      "Iteration 19000 Training loss 0.02696342021226883 Validation loss 0.02910902351140976 Accuracy 0.705078125\n",
      "Iteration 19010 Training loss 0.02698378451168537 Validation loss 0.029081689193844795 Accuracy 0.705078125\n",
      "Iteration 19020 Training loss 0.026925191283226013 Validation loss 0.028818301856517792 Accuracy 0.70751953125\n",
      "Iteration 19030 Training loss 0.025436265394091606 Validation loss 0.028612403199076653 Accuracy 0.7099609375\n",
      "Iteration 19040 Training loss 0.025783857330679893 Validation loss 0.028684936463832855 Accuracy 0.7099609375\n",
      "Iteration 19050 Training loss 0.026680678129196167 Validation loss 0.028380541130900383 Accuracy 0.712890625\n",
      "Iteration 19060 Training loss 0.0266017597168684 Validation loss 0.02851690910756588 Accuracy 0.71142578125\n",
      "Iteration 19070 Training loss 0.02442001737654209 Validation loss 0.028527500107884407 Accuracy 0.71142578125\n",
      "Iteration 19080 Training loss 0.02743767760694027 Validation loss 0.028484147042036057 Accuracy 0.71142578125\n",
      "Iteration 19090 Training loss 0.025556961074471474 Validation loss 0.02831244468688965 Accuracy 0.71337890625\n",
      "Iteration 19100 Training loss 0.02705315686762333 Validation loss 0.028723036870360374 Accuracy 0.708984375\n",
      "Iteration 19110 Training loss 0.024888040497899055 Validation loss 0.02847185917198658 Accuracy 0.71142578125\n",
      "Iteration 19120 Training loss 0.026606500148773193 Validation loss 0.028247684240341187 Accuracy 0.71435546875\n",
      "Iteration 19130 Training loss 0.026284167543053627 Validation loss 0.029880331829190254 Accuracy 0.697265625\n",
      "Iteration 19140 Training loss 0.027536289766430855 Validation loss 0.028594007715582848 Accuracy 0.71044921875\n",
      "Iteration 19150 Training loss 0.02723720856010914 Validation loss 0.028482967987656593 Accuracy 0.71142578125\n",
      "Iteration 19160 Training loss 0.025703394785523415 Validation loss 0.028567025437951088 Accuracy 0.71044921875\n",
      "Iteration 19170 Training loss 0.027508731931447983 Validation loss 0.028510961681604385 Accuracy 0.7119140625\n",
      "Iteration 19180 Training loss 0.023444397374987602 Validation loss 0.028602788224816322 Accuracy 0.71044921875\n",
      "Iteration 19190 Training loss 0.026161199435591698 Validation loss 0.028659364208579063 Accuracy 0.7099609375\n",
      "Iteration 19200 Training loss 0.02961713634431362 Validation loss 0.028507469221949577 Accuracy 0.71142578125\n",
      "Iteration 19210 Training loss 0.025834523141384125 Validation loss 0.028793226927518845 Accuracy 0.708984375\n",
      "Iteration 19220 Training loss 0.029305770993232727 Validation loss 0.028414910659193993 Accuracy 0.71240234375\n",
      "Iteration 19230 Training loss 0.02820054069161415 Validation loss 0.02879810892045498 Accuracy 0.7080078125\n",
      "Iteration 19240 Training loss 0.025472545996308327 Validation loss 0.02876978926360607 Accuracy 0.7080078125\n",
      "Iteration 19250 Training loss 0.026794033125042915 Validation loss 0.028349148109555244 Accuracy 0.71337890625\n",
      "Iteration 19260 Training loss 0.027385788038372993 Validation loss 0.02854289300739765 Accuracy 0.7109375\n",
      "Iteration 19270 Training loss 0.026208968833088875 Validation loss 0.02868531085550785 Accuracy 0.70947265625\n",
      "Iteration 19280 Training loss 0.021291879937052727 Validation loss 0.02834741584956646 Accuracy 0.71240234375\n",
      "Iteration 19290 Training loss 0.02359740249812603 Validation loss 0.028509384021162987 Accuracy 0.7109375\n",
      "Iteration 19300 Training loss 0.02656777948141098 Validation loss 0.028487738221883774 Accuracy 0.71142578125\n",
      "Iteration 19310 Training loss 0.026871632784605026 Validation loss 0.028396980836987495 Accuracy 0.71240234375\n",
      "Iteration 19320 Training loss 0.024762099608778954 Validation loss 0.028611961752176285 Accuracy 0.70947265625\n",
      "Iteration 19330 Training loss 0.023999882861971855 Validation loss 0.028359543532133102 Accuracy 0.712890625\n",
      "Iteration 19340 Training loss 0.025545550510287285 Validation loss 0.02930600568652153 Accuracy 0.703125\n",
      "Iteration 19350 Training loss 0.027683904394507408 Validation loss 0.028319189324975014 Accuracy 0.71337890625\n",
      "Iteration 19360 Training loss 0.0241971667855978 Validation loss 0.02849051170051098 Accuracy 0.7109375\n",
      "Iteration 19370 Training loss 0.02724445052444935 Validation loss 0.028818173334002495 Accuracy 0.70849609375\n",
      "Iteration 19380 Training loss 0.024901285767555237 Validation loss 0.028349725529551506 Accuracy 0.712890625\n",
      "Iteration 19390 Training loss 0.025969956070184708 Validation loss 0.028489811345934868 Accuracy 0.71142578125\n",
      "Iteration 19400 Training loss 0.02450152114033699 Validation loss 0.029079364612698555 Accuracy 0.705078125\n",
      "Iteration 19410 Training loss 0.025636687874794006 Validation loss 0.028422411531209946 Accuracy 0.712890625\n",
      "Iteration 19420 Training loss 0.024903615936636925 Validation loss 0.02891494892537594 Accuracy 0.70703125\n",
      "Iteration 19430 Training loss 0.029914459213614464 Validation loss 0.028611503541469574 Accuracy 0.7099609375\n",
      "Iteration 19440 Training loss 0.024622390046715736 Validation loss 0.0285160094499588 Accuracy 0.71142578125\n",
      "Iteration 19450 Training loss 0.02605944499373436 Validation loss 0.02827434055507183 Accuracy 0.7138671875\n",
      "Iteration 19460 Training loss 0.026170291006565094 Validation loss 0.028756652027368546 Accuracy 0.70947265625\n",
      "Iteration 19470 Training loss 0.024012979120016098 Validation loss 0.028412465006113052 Accuracy 0.712890625\n",
      "Iteration 19480 Training loss 0.025874659419059753 Validation loss 0.02856578305363655 Accuracy 0.7099609375\n",
      "Iteration 19490 Training loss 0.02358761988580227 Validation loss 0.029861561954021454 Accuracy 0.69775390625\n",
      "Iteration 19500 Training loss 0.02611556276679039 Validation loss 0.028468338772654533 Accuracy 0.7119140625\n",
      "Iteration 19510 Training loss 0.026055920869112015 Validation loss 0.028481166809797287 Accuracy 0.71240234375\n",
      "Iteration 19520 Training loss 0.02502855658531189 Validation loss 0.028402680531144142 Accuracy 0.712890625\n",
      "Iteration 19530 Training loss 0.026557164266705513 Validation loss 0.02855147421360016 Accuracy 0.7109375\n",
      "Iteration 19540 Training loss 0.02466956526041031 Validation loss 0.028483392670750618 Accuracy 0.71142578125\n",
      "Iteration 19550 Training loss 0.02648380771279335 Validation loss 0.028378939256072044 Accuracy 0.712890625\n",
      "Iteration 19560 Training loss 0.028991365805268288 Validation loss 0.028812812641263008 Accuracy 0.70849609375\n",
      "Iteration 19570 Training loss 0.02556760050356388 Validation loss 0.028348153457045555 Accuracy 0.71337890625\n",
      "Iteration 19580 Training loss 0.02915019541978836 Validation loss 0.028435712680220604 Accuracy 0.71240234375\n",
      "Iteration 19590 Training loss 0.027635635808110237 Validation loss 0.029076820239424706 Accuracy 0.7060546875\n",
      "Iteration 19600 Training loss 0.028069885447621346 Validation loss 0.028957607224583626 Accuracy 0.70751953125\n",
      "Iteration 19610 Training loss 0.0234241746366024 Validation loss 0.028494378551840782 Accuracy 0.71142578125\n",
      "Iteration 19620 Training loss 0.02863486111164093 Validation loss 0.02865816280245781 Accuracy 0.71044921875\n",
      "Iteration 19630 Training loss 0.02655682899057865 Validation loss 0.028352897614240646 Accuracy 0.71337890625\n",
      "Iteration 19640 Training loss 0.025154797360301018 Validation loss 0.028245963156223297 Accuracy 0.71435546875\n",
      "Iteration 19650 Training loss 0.02774948626756668 Validation loss 0.028368061408400536 Accuracy 0.71240234375\n",
      "Iteration 19660 Training loss 0.025845449417829514 Validation loss 0.02868744730949402 Accuracy 0.708984375\n",
      "Iteration 19670 Training loss 0.026323264464735985 Validation loss 0.028413696214556694 Accuracy 0.71240234375\n",
      "Iteration 19680 Training loss 0.026618851348757744 Validation loss 0.02852674573659897 Accuracy 0.71142578125\n",
      "Iteration 19690 Training loss 0.023343553766608238 Validation loss 0.028406796976923943 Accuracy 0.71240234375\n",
      "Iteration 19700 Training loss 0.02614840678870678 Validation loss 0.02864312380552292 Accuracy 0.70947265625\n",
      "Iteration 19710 Training loss 0.02509286440908909 Validation loss 0.028454139828681946 Accuracy 0.71240234375\n",
      "Iteration 19720 Training loss 0.0241808220744133 Validation loss 0.028343115001916885 Accuracy 0.712890625\n",
      "Iteration 19730 Training loss 0.02545205131173134 Validation loss 0.02824552170932293 Accuracy 0.71435546875\n",
      "Iteration 19740 Training loss 0.025849154219031334 Validation loss 0.02834273874759674 Accuracy 0.712890625\n",
      "Iteration 19750 Training loss 0.02442948706448078 Validation loss 0.02905326336622238 Accuracy 0.7060546875\n",
      "Iteration 19760 Training loss 0.027024442330002785 Validation loss 0.028515975922346115 Accuracy 0.71142578125\n",
      "Iteration 19770 Training loss 0.02861728146672249 Validation loss 0.028271613642573357 Accuracy 0.71435546875\n",
      "Iteration 19780 Training loss 0.027337267994880676 Validation loss 0.028523394837975502 Accuracy 0.71044921875\n",
      "Iteration 19790 Training loss 0.02393288165330887 Validation loss 0.028360866010189056 Accuracy 0.712890625\n",
      "Iteration 19800 Training loss 0.026136226952075958 Validation loss 0.028402531519532204 Accuracy 0.7119140625\n",
      "Iteration 19810 Training loss 0.024919046089053154 Validation loss 0.028370335698127747 Accuracy 0.71240234375\n",
      "Iteration 19820 Training loss 0.026132363826036453 Validation loss 0.028703756630420685 Accuracy 0.70947265625\n",
      "Iteration 19830 Training loss 0.028726300224661827 Validation loss 0.028398709371685982 Accuracy 0.71240234375\n",
      "Iteration 19840 Training loss 0.027150161564350128 Validation loss 0.028619110584259033 Accuracy 0.7099609375\n",
      "Iteration 19850 Training loss 0.02430848591029644 Validation loss 0.02833922579884529 Accuracy 0.71337890625\n",
      "Iteration 19860 Training loss 0.024171389639377594 Validation loss 0.028310295194387436 Accuracy 0.712890625\n",
      "Iteration 19870 Training loss 0.023251965641975403 Validation loss 0.02821172960102558 Accuracy 0.71484375\n",
      "Iteration 19880 Training loss 0.028121769428253174 Validation loss 0.02899610809981823 Accuracy 0.70654296875\n",
      "Iteration 19890 Training loss 0.02698577009141445 Validation loss 0.02844519540667534 Accuracy 0.7119140625\n",
      "Iteration 19900 Training loss 0.028290171176195145 Validation loss 0.028332220390439034 Accuracy 0.712890625\n",
      "Iteration 19910 Training loss 0.029031580314040184 Validation loss 0.028439296409487724 Accuracy 0.71240234375\n",
      "Iteration 19920 Training loss 0.027515379711985588 Validation loss 0.028822841122746468 Accuracy 0.7080078125\n",
      "Iteration 19930 Training loss 0.025115035474300385 Validation loss 0.028545498847961426 Accuracy 0.7109375\n",
      "Iteration 19940 Training loss 0.02378503605723381 Validation loss 0.028130995109677315 Accuracy 0.71533203125\n",
      "Iteration 19950 Training loss 0.02354755811393261 Validation loss 0.028632517904043198 Accuracy 0.71044921875\n",
      "Iteration 19960 Training loss 0.0238634180277586 Validation loss 0.02827475592494011 Accuracy 0.71240234375\n",
      "Iteration 19970 Training loss 0.024379361420869827 Validation loss 0.0288563072681427 Accuracy 0.70849609375\n",
      "Iteration 19980 Training loss 0.02676192671060562 Validation loss 0.028549063950777054 Accuracy 0.7109375\n",
      "Iteration 19990 Training loss 0.02551175095140934 Validation loss 0.028314199298620224 Accuracy 0.712890625\n",
      "Iteration 20000 Training loss 0.027453560382127762 Validation loss 0.029176196083426476 Accuracy 0.70556640625\n",
      "Iteration 20010 Training loss 0.027445312589406967 Validation loss 0.028897015377879143 Accuracy 0.70751953125\n",
      "Iteration 20020 Training loss 0.02599114365875721 Validation loss 0.028478562831878662 Accuracy 0.71142578125\n",
      "Iteration 20030 Training loss 0.025732237845659256 Validation loss 0.02857307344675064 Accuracy 0.71044921875\n",
      "Iteration 20040 Training loss 0.023954574018716812 Validation loss 0.028730155900120735 Accuracy 0.70947265625\n",
      "Iteration 20050 Training loss 0.025923436507582664 Validation loss 0.02833932638168335 Accuracy 0.712890625\n",
      "Iteration 20060 Training loss 0.031058143824338913 Validation loss 0.028683695942163467 Accuracy 0.70849609375\n",
      "Iteration 20070 Training loss 0.027505645528435707 Validation loss 0.028463197872042656 Accuracy 0.7119140625\n",
      "Iteration 20080 Training loss 0.02568935602903366 Validation loss 0.029101543128490448 Accuracy 0.70556640625\n",
      "Iteration 20090 Training loss 0.030236124992370605 Validation loss 0.028468139469623566 Accuracy 0.7119140625\n",
      "Iteration 20100 Training loss 0.025439582765102386 Validation loss 0.028295330703258514 Accuracy 0.71337890625\n",
      "Iteration 20110 Training loss 0.028496455401182175 Validation loss 0.028693079948425293 Accuracy 0.708984375\n",
      "Iteration 20120 Training loss 0.026594679802656174 Validation loss 0.02877947688102722 Accuracy 0.708984375\n",
      "Iteration 20130 Training loss 0.025322509929537773 Validation loss 0.028534017503261566 Accuracy 0.7109375\n",
      "Iteration 20140 Training loss 0.02523529715836048 Validation loss 0.028348034247756004 Accuracy 0.71337890625\n",
      "Iteration 20150 Training loss 0.028430381789803505 Validation loss 0.028296183794736862 Accuracy 0.7138671875\n",
      "Iteration 20160 Training loss 0.025037847459316254 Validation loss 0.028297608718276024 Accuracy 0.71337890625\n",
      "Iteration 20170 Training loss 0.025025632232427597 Validation loss 0.02820543944835663 Accuracy 0.7138671875\n",
      "Iteration 20180 Training loss 0.028527602553367615 Validation loss 0.0286797396838665 Accuracy 0.70947265625\n",
      "Iteration 20190 Training loss 0.02751452848315239 Validation loss 0.02827421762049198 Accuracy 0.7138671875\n",
      "Iteration 20200 Training loss 0.027111200615763664 Validation loss 0.02956782840192318 Accuracy 0.7001953125\n",
      "Iteration 20210 Training loss 0.025171585381031036 Validation loss 0.028310252353549004 Accuracy 0.71337890625\n",
      "Iteration 20220 Training loss 0.025758640840649605 Validation loss 0.02849075384438038 Accuracy 0.71142578125\n",
      "Iteration 20230 Training loss 0.02244306355714798 Validation loss 0.028618749231100082 Accuracy 0.7109375\n",
      "Iteration 20240 Training loss 0.027976099401712418 Validation loss 0.028303099796175957 Accuracy 0.71337890625\n",
      "Iteration 20250 Training loss 0.028805017471313477 Validation loss 0.029437238350510597 Accuracy 0.7021484375\n",
      "Iteration 20260 Training loss 0.02443077601492405 Validation loss 0.02838974818587303 Accuracy 0.71240234375\n",
      "Iteration 20270 Training loss 0.028227269649505615 Validation loss 0.02918912097811699 Accuracy 0.7041015625\n",
      "Iteration 20280 Training loss 0.025694992393255234 Validation loss 0.028425348922610283 Accuracy 0.71240234375\n",
      "Iteration 20290 Training loss 0.024104544892907143 Validation loss 0.028370656073093414 Accuracy 0.712890625\n",
      "Iteration 20300 Training loss 0.025857819244265556 Validation loss 0.02883334830403328 Accuracy 0.70751953125\n",
      "Iteration 20310 Training loss 0.029204919934272766 Validation loss 0.02840457111597061 Accuracy 0.71240234375\n",
      "Iteration 20320 Training loss 0.025081202387809753 Validation loss 0.0285593643784523 Accuracy 0.7099609375\n",
      "Iteration 20330 Training loss 0.024980461224913597 Validation loss 0.02821510098874569 Accuracy 0.71435546875\n",
      "Iteration 20340 Training loss 0.028242167085409164 Validation loss 0.028370853513479233 Accuracy 0.712890625\n",
      "Iteration 20350 Training loss 0.024972980841994286 Validation loss 0.028191864490509033 Accuracy 0.71484375\n",
      "Iteration 20360 Training loss 0.02662874571979046 Validation loss 0.02842520922422409 Accuracy 0.71240234375\n",
      "Iteration 20370 Training loss 0.027429549023509026 Validation loss 0.028274891898036003 Accuracy 0.7138671875\n",
      "Iteration 20380 Training loss 0.027295822277665138 Validation loss 0.02833101898431778 Accuracy 0.71240234375\n",
      "Iteration 20390 Training loss 0.02582694962620735 Validation loss 0.02893512137234211 Accuracy 0.70703125\n",
      "Iteration 20400 Training loss 0.026032622903585434 Validation loss 0.028733110055327415 Accuracy 0.70947265625\n",
      "Iteration 20410 Training loss 0.02596605382859707 Validation loss 0.028298988938331604 Accuracy 0.7138671875\n",
      "Iteration 20420 Training loss 0.025661248713731766 Validation loss 0.02816917560994625 Accuracy 0.71435546875\n",
      "Iteration 20430 Training loss 0.022979987785220146 Validation loss 0.02838742546737194 Accuracy 0.712890625\n",
      "Iteration 20440 Training loss 0.023663995787501335 Validation loss 0.02849523350596428 Accuracy 0.7109375\n",
      "Iteration 20450 Training loss 0.0281378123909235 Validation loss 0.028371557593345642 Accuracy 0.712890625\n",
      "Iteration 20460 Training loss 0.025202244520187378 Validation loss 0.028927186504006386 Accuracy 0.70751953125\n",
      "Iteration 20470 Training loss 0.026695022359490395 Validation loss 0.02888701856136322 Accuracy 0.70751953125\n",
      "Iteration 20480 Training loss 0.024389157071709633 Validation loss 0.02829788066446781 Accuracy 0.71337890625\n",
      "Iteration 20490 Training loss 0.027756907045841217 Validation loss 0.0284953024238348 Accuracy 0.71142578125\n",
      "Iteration 20500 Training loss 0.024474889039993286 Validation loss 0.028408261016011238 Accuracy 0.71240234375\n",
      "Iteration 20510 Training loss 0.027016958221793175 Validation loss 0.028733648359775543 Accuracy 0.70947265625\n",
      "Iteration 20520 Training loss 0.022404126822948456 Validation loss 0.02849939838051796 Accuracy 0.71142578125\n",
      "Iteration 20530 Training loss 0.02603396773338318 Validation loss 0.028896234929561615 Accuracy 0.70751953125\n",
      "Iteration 20540 Training loss 0.0268947072327137 Validation loss 0.028588224202394485 Accuracy 0.7109375\n",
      "Iteration 20550 Training loss 0.026894506067037582 Validation loss 0.028839530423283577 Accuracy 0.7080078125\n",
      "Iteration 20560 Training loss 0.025680020451545715 Validation loss 0.028259828686714172 Accuracy 0.7138671875\n",
      "Iteration 20570 Training loss 0.028599195182323456 Validation loss 0.028841454535722733 Accuracy 0.7080078125\n",
      "Iteration 20580 Training loss 0.028304586187005043 Validation loss 0.028586704283952713 Accuracy 0.71044921875\n",
      "Iteration 20590 Training loss 0.02598232962191105 Validation loss 0.02843465283513069 Accuracy 0.71240234375\n",
      "Iteration 20600 Training loss 0.028845086693763733 Validation loss 0.02831079065799713 Accuracy 0.712890625\n",
      "Iteration 20610 Training loss 0.02854616753757 Validation loss 0.0291562732309103 Accuracy 0.705078125\n",
      "Iteration 20620 Training loss 0.028171854093670845 Validation loss 0.028674989938735962 Accuracy 0.70947265625\n",
      "Iteration 20630 Training loss 0.027028212323784828 Validation loss 0.028381451964378357 Accuracy 0.712890625\n",
      "Iteration 20640 Training loss 0.023964686319231987 Validation loss 0.02848041243851185 Accuracy 0.7119140625\n",
      "Iteration 20650 Training loss 0.029059000313282013 Validation loss 0.02861800231039524 Accuracy 0.71044921875\n",
      "Iteration 20660 Training loss 0.02405012771487236 Validation loss 0.02849576063454151 Accuracy 0.71044921875\n",
      "Iteration 20670 Training loss 0.027236750349402428 Validation loss 0.02849634177982807 Accuracy 0.71142578125\n",
      "Iteration 20680 Training loss 0.022125590592622757 Validation loss 0.028374457731842995 Accuracy 0.71240234375\n",
      "Iteration 20690 Training loss 0.024921877309679985 Validation loss 0.028670301660895348 Accuracy 0.7099609375\n",
      "Iteration 20700 Training loss 0.025716589763760567 Validation loss 0.02893850952386856 Accuracy 0.70703125\n",
      "Iteration 20710 Training loss 0.024498870596289635 Validation loss 0.028248431161046028 Accuracy 0.7138671875\n",
      "Iteration 20720 Training loss 0.024507110938429832 Validation loss 0.028110329061746597 Accuracy 0.71533203125\n",
      "Iteration 20730 Training loss 0.024629665538668633 Validation loss 0.028718251734972 Accuracy 0.70947265625\n",
      "Iteration 20740 Training loss 0.02734445407986641 Validation loss 0.02843247540295124 Accuracy 0.7119140625\n",
      "Iteration 20750 Training loss 0.0260417889803648 Validation loss 0.028653157874941826 Accuracy 0.71044921875\n",
      "Iteration 20760 Training loss 0.027661684900522232 Validation loss 0.02842745929956436 Accuracy 0.71240234375\n",
      "Iteration 20770 Training loss 0.025347765535116196 Validation loss 0.028264688327908516 Accuracy 0.7138671875\n",
      "Iteration 20780 Training loss 0.024689339101314545 Validation loss 0.028236813843250275 Accuracy 0.71484375\n",
      "Iteration 20790 Training loss 0.024293487891554832 Validation loss 0.029645025730133057 Accuracy 0.69921875\n",
      "Iteration 20800 Training loss 0.02448776550590992 Validation loss 0.028295712545514107 Accuracy 0.71337890625\n",
      "Iteration 20810 Training loss 0.024110835045576096 Validation loss 0.028321024030447006 Accuracy 0.71337890625\n",
      "Iteration 20820 Training loss 0.02606506273150444 Validation loss 0.02836764045059681 Accuracy 0.712890625\n",
      "Iteration 20830 Training loss 0.025222213938832283 Validation loss 0.02812338061630726 Accuracy 0.71484375\n",
      "Iteration 20840 Training loss 0.02576884999871254 Validation loss 0.02820591814815998 Accuracy 0.71337890625\n",
      "Iteration 20850 Training loss 0.02913603186607361 Validation loss 0.02822258695960045 Accuracy 0.71435546875\n",
      "Iteration 20860 Training loss 0.025403428822755814 Validation loss 0.028282547369599342 Accuracy 0.71337890625\n",
      "Iteration 20870 Training loss 0.023699389770627022 Validation loss 0.028113309293985367 Accuracy 0.71533203125\n",
      "Iteration 20880 Training loss 0.026412593200802803 Validation loss 0.02833380550146103 Accuracy 0.712890625\n",
      "Iteration 20890 Training loss 0.02604285068809986 Validation loss 0.028372840955853462 Accuracy 0.712890625\n",
      "Iteration 20900 Training loss 0.02617718279361725 Validation loss 0.028225703164935112 Accuracy 0.7138671875\n",
      "Iteration 20910 Training loss 0.02548297867178917 Validation loss 0.0282739344984293 Accuracy 0.7138671875\n",
      "Iteration 20920 Training loss 0.024563351646065712 Validation loss 0.028090521693229675 Accuracy 0.7158203125\n",
      "Iteration 20930 Training loss 0.025503139942884445 Validation loss 0.028106728568673134 Accuracy 0.71484375\n",
      "Iteration 20940 Training loss 0.025937214493751526 Validation loss 0.028648022562265396 Accuracy 0.7099609375\n",
      "Iteration 20950 Training loss 0.02402615360915661 Validation loss 0.028047874569892883 Accuracy 0.7158203125\n",
      "Iteration 20960 Training loss 0.025080498307943344 Validation loss 0.02834261581301689 Accuracy 0.71337890625\n",
      "Iteration 20970 Training loss 0.0250688549131155 Validation loss 0.028867362067103386 Accuracy 0.7080078125\n",
      "Iteration 20980 Training loss 0.028626810759305954 Validation loss 0.028383178636431694 Accuracy 0.712890625\n",
      "Iteration 20990 Training loss 0.023525776341557503 Validation loss 0.028319036588072777 Accuracy 0.71337890625\n",
      "Iteration 21000 Training loss 0.0255556832998991 Validation loss 0.02851841226220131 Accuracy 0.7109375\n",
      "Iteration 21010 Training loss 0.028178850188851357 Validation loss 0.0286007858812809 Accuracy 0.71044921875\n",
      "Iteration 21020 Training loss 0.027792206034064293 Validation loss 0.02821507677435875 Accuracy 0.71435546875\n",
      "Iteration 21030 Training loss 0.026086274534463882 Validation loss 0.028534134849905968 Accuracy 0.71142578125\n",
      "Iteration 21040 Training loss 0.025384606793522835 Validation loss 0.02826811373233795 Accuracy 0.712890625\n",
      "Iteration 21050 Training loss 0.02566590905189514 Validation loss 0.028044264763593674 Accuracy 0.71630859375\n",
      "Iteration 21060 Training loss 0.02503429353237152 Validation loss 0.02821185067296028 Accuracy 0.71435546875\n",
      "Iteration 21070 Training loss 0.027693629264831543 Validation loss 0.02895032800734043 Accuracy 0.70654296875\n",
      "Iteration 21080 Training loss 0.02694293111562729 Validation loss 0.029591640457510948 Accuracy 0.69970703125\n",
      "Iteration 21090 Training loss 0.025988176465034485 Validation loss 0.028348438441753387 Accuracy 0.712890625\n",
      "Iteration 21100 Training loss 0.024264782667160034 Validation loss 0.028182948008179665 Accuracy 0.71435546875\n",
      "Iteration 21110 Training loss 0.023670677095651627 Validation loss 0.02836574614048004 Accuracy 0.71240234375\n",
      "Iteration 21120 Training loss 0.028632421046495438 Validation loss 0.02894970215857029 Accuracy 0.70703125\n",
      "Iteration 21130 Training loss 0.025539860129356384 Validation loss 0.02832743525505066 Accuracy 0.71240234375\n",
      "Iteration 21140 Training loss 0.02471955306828022 Validation loss 0.02822803147137165 Accuracy 0.71435546875\n",
      "Iteration 21150 Training loss 0.02581425942480564 Validation loss 0.02820430137217045 Accuracy 0.7138671875\n",
      "Iteration 21160 Training loss 0.025102024897933006 Validation loss 0.028395140543580055 Accuracy 0.71240234375\n",
      "Iteration 21170 Training loss 0.024558180943131447 Validation loss 0.028523001819849014 Accuracy 0.7109375\n",
      "Iteration 21180 Training loss 0.02642003260552883 Validation loss 0.02807583659887314 Accuracy 0.7158203125\n",
      "Iteration 21190 Training loss 0.026068903505802155 Validation loss 0.02872626483440399 Accuracy 0.708984375\n",
      "Iteration 21200 Training loss 0.025363029912114143 Validation loss 0.028243478387594223 Accuracy 0.7138671875\n",
      "Iteration 21210 Training loss 0.025295857340097427 Validation loss 0.028206126764416695 Accuracy 0.7138671875\n",
      "Iteration 21220 Training loss 0.026664283126592636 Validation loss 0.02854306623339653 Accuracy 0.7109375\n",
      "Iteration 21230 Training loss 0.026540782302618027 Validation loss 0.028918568044900894 Accuracy 0.70751953125\n",
      "Iteration 21240 Training loss 0.025575587525963783 Validation loss 0.028080379590392113 Accuracy 0.7158203125\n",
      "Iteration 21250 Training loss 0.02728072553873062 Validation loss 0.02862582914531231 Accuracy 0.71044921875\n",
      "Iteration 21260 Training loss 0.026180453598499298 Validation loss 0.028467193245887756 Accuracy 0.7119140625\n",
      "Iteration 21270 Training loss 0.02667081356048584 Validation loss 0.028180258348584175 Accuracy 0.7138671875\n",
      "Iteration 21280 Training loss 0.02530498243868351 Validation loss 0.028402021154761314 Accuracy 0.7119140625\n",
      "Iteration 21290 Training loss 0.023377014324069023 Validation loss 0.028287524357438087 Accuracy 0.71337890625\n",
      "Iteration 21300 Training loss 0.027467213571071625 Validation loss 0.02917562611401081 Accuracy 0.7041015625\n",
      "Iteration 21310 Training loss 0.023454543203115463 Validation loss 0.02823038026690483 Accuracy 0.7138671875\n",
      "Iteration 21320 Training loss 0.026025384664535522 Validation loss 0.02836327999830246 Accuracy 0.712890625\n",
      "Iteration 21330 Training loss 0.02719898708164692 Validation loss 0.028315940871834755 Accuracy 0.71240234375\n",
      "Iteration 21340 Training loss 0.023892953991889954 Validation loss 0.028621915727853775 Accuracy 0.71044921875\n",
      "Iteration 21350 Training loss 0.026142053306102753 Validation loss 0.028301918879151344 Accuracy 0.712890625\n",
      "Iteration 21360 Training loss 0.025481602177023888 Validation loss 0.0286306980997324 Accuracy 0.71044921875\n",
      "Iteration 21370 Training loss 0.02418714575469494 Validation loss 0.02831384167075157 Accuracy 0.71337890625\n",
      "Iteration 21380 Training loss 0.02775404416024685 Validation loss 0.028049517422914505 Accuracy 0.71533203125\n",
      "Iteration 21390 Training loss 0.026885101571679115 Validation loss 0.028214775025844574 Accuracy 0.7138671875\n",
      "Iteration 21400 Training loss 0.027071716263890266 Validation loss 0.028258126229047775 Accuracy 0.71337890625\n",
      "Iteration 21410 Training loss 0.02695142664015293 Validation loss 0.028492026031017303 Accuracy 0.7119140625\n",
      "Iteration 21420 Training loss 0.029139818623661995 Validation loss 0.028463298454880714 Accuracy 0.7119140625\n",
      "Iteration 21430 Training loss 0.025637472048401833 Validation loss 0.028387563303112984 Accuracy 0.712890625\n",
      "Iteration 21440 Training loss 0.02754393219947815 Validation loss 0.02844393253326416 Accuracy 0.7109375\n",
      "Iteration 21450 Training loss 0.024842772632837296 Validation loss 0.028685428202152252 Accuracy 0.70947265625\n",
      "Iteration 21460 Training loss 0.026990575715899467 Validation loss 0.0282671470195055 Accuracy 0.71337890625\n",
      "Iteration 21470 Training loss 0.02559526450932026 Validation loss 0.028485940769314766 Accuracy 0.7119140625\n",
      "Iteration 21480 Training loss 0.025173092260956764 Validation loss 0.028171053156256676 Accuracy 0.71484375\n",
      "Iteration 21490 Training loss 0.023091617971658707 Validation loss 0.028272435069084167 Accuracy 0.7138671875\n",
      "Iteration 21500 Training loss 0.02690129540860653 Validation loss 0.028635526075959206 Accuracy 0.7099609375\n",
      "Iteration 21510 Training loss 0.02851221151649952 Validation loss 0.02809099480509758 Accuracy 0.71484375\n",
      "Iteration 21520 Training loss 0.02417815662920475 Validation loss 0.028447171673178673 Accuracy 0.7119140625\n",
      "Iteration 21530 Training loss 0.027781423181295395 Validation loss 0.02834581956267357 Accuracy 0.71240234375\n",
      "Iteration 21540 Training loss 0.026862457394599915 Validation loss 0.02844204008579254 Accuracy 0.7119140625\n",
      "Iteration 21550 Training loss 0.027406521141529083 Validation loss 0.02914072945713997 Accuracy 0.705078125\n",
      "Iteration 21560 Training loss 0.024425584822893143 Validation loss 0.02883346565067768 Accuracy 0.7080078125\n",
      "Iteration 21570 Training loss 0.0252155102789402 Validation loss 0.02875826507806778 Accuracy 0.70849609375\n",
      "Iteration 21580 Training loss 0.023859547451138496 Validation loss 0.028465908020734787 Accuracy 0.71142578125\n",
      "Iteration 21590 Training loss 0.024843815714120865 Validation loss 0.028168153017759323 Accuracy 0.7138671875\n",
      "Iteration 21600 Training loss 0.024164829403162003 Validation loss 0.028562171384692192 Accuracy 0.7099609375\n",
      "Iteration 21610 Training loss 0.029517928138375282 Validation loss 0.028541801497340202 Accuracy 0.7109375\n",
      "Iteration 21620 Training loss 0.02512485161423683 Validation loss 0.028439046815037727 Accuracy 0.7119140625\n",
      "Iteration 21630 Training loss 0.027849776670336723 Validation loss 0.028749672695994377 Accuracy 0.708984375\n",
      "Iteration 21640 Training loss 0.02471046894788742 Validation loss 0.027994094416499138 Accuracy 0.71630859375\n",
      "Iteration 21650 Training loss 0.02596513368189335 Validation loss 0.028471341356635094 Accuracy 0.71142578125\n",
      "Iteration 21660 Training loss 0.02433820255100727 Validation loss 0.02831709384918213 Accuracy 0.71337890625\n",
      "Iteration 21670 Training loss 0.026643997058272362 Validation loss 0.02819427102804184 Accuracy 0.71435546875\n",
      "Iteration 21680 Training loss 0.024199482053518295 Validation loss 0.028713949024677277 Accuracy 0.708984375\n",
      "Iteration 21690 Training loss 0.025375671684741974 Validation loss 0.02818453498184681 Accuracy 0.71435546875\n",
      "Iteration 21700 Training loss 0.024848297238349915 Validation loss 0.02805180847644806 Accuracy 0.71484375\n",
      "Iteration 21710 Training loss 0.026359573006629944 Validation loss 0.02844874933362007 Accuracy 0.7109375\n",
      "Iteration 21720 Training loss 0.02620023488998413 Validation loss 0.028765516355633736 Accuracy 0.708984375\n",
      "Iteration 21730 Training loss 0.023794159293174744 Validation loss 0.0282733216881752 Accuracy 0.71337890625\n",
      "Iteration 21740 Training loss 0.025467142462730408 Validation loss 0.02840745449066162 Accuracy 0.7119140625\n",
      "Iteration 21750 Training loss 0.02407936565577984 Validation loss 0.029066618531942368 Accuracy 0.705078125\n",
      "Iteration 21760 Training loss 0.026508038863539696 Validation loss 0.028259877115488052 Accuracy 0.7138671875\n",
      "Iteration 21770 Training loss 0.025766702368855476 Validation loss 0.02851060964167118 Accuracy 0.71142578125\n",
      "Iteration 21780 Training loss 0.02418738603591919 Validation loss 0.028474818915128708 Accuracy 0.7109375\n",
      "Iteration 21790 Training loss 0.026247400790452957 Validation loss 0.02870711125433445 Accuracy 0.708984375\n",
      "Iteration 21800 Training loss 0.026815643534064293 Validation loss 0.028521796688437462 Accuracy 0.71142578125\n",
      "Iteration 21810 Training loss 0.025289596989750862 Validation loss 0.028391873463988304 Accuracy 0.712890625\n",
      "Iteration 21820 Training loss 0.02435845136642456 Validation loss 0.028333470225334167 Accuracy 0.7119140625\n",
      "Iteration 21830 Training loss 0.025199145078659058 Validation loss 0.02803323231637478 Accuracy 0.7158203125\n",
      "Iteration 21840 Training loss 0.026739954948425293 Validation loss 0.028477348387241364 Accuracy 0.71142578125\n",
      "Iteration 21850 Training loss 0.030894378200173378 Validation loss 0.028988275676965714 Accuracy 0.70654296875\n",
      "Iteration 21860 Training loss 0.02595142275094986 Validation loss 0.028701242059469223 Accuracy 0.708984375\n",
      "Iteration 21870 Training loss 0.02483188360929489 Validation loss 0.02803846076130867 Accuracy 0.71630859375\n",
      "Iteration 21880 Training loss 0.027488870546221733 Validation loss 0.028755731880664825 Accuracy 0.708984375\n",
      "Iteration 21890 Training loss 0.03045397624373436 Validation loss 0.029166795313358307 Accuracy 0.70458984375\n",
      "Iteration 21900 Training loss 0.02463960275053978 Validation loss 0.0281507708132267 Accuracy 0.71435546875\n",
      "Iteration 21910 Training loss 0.01996045559644699 Validation loss 0.028115784749388695 Accuracy 0.71484375\n",
      "Iteration 21920 Training loss 0.02443569339811802 Validation loss 0.028632618486881256 Accuracy 0.7099609375\n",
      "Iteration 21930 Training loss 0.026480188593268394 Validation loss 0.02830011583864689 Accuracy 0.71337890625\n",
      "Iteration 21940 Training loss 0.025131016969680786 Validation loss 0.028053293004631996 Accuracy 0.71533203125\n",
      "Iteration 21950 Training loss 0.02666562795639038 Validation loss 0.028078719973564148 Accuracy 0.71533203125\n",
      "Iteration 21960 Training loss 0.023184239864349365 Validation loss 0.02856357768177986 Accuracy 0.70947265625\n",
      "Iteration 21970 Training loss 0.02651130221784115 Validation loss 0.028225893154740334 Accuracy 0.71337890625\n",
      "Iteration 21980 Training loss 0.028893696144223213 Validation loss 0.029361864551901817 Accuracy 0.70263671875\n",
      "Iteration 21990 Training loss 0.026113418862223625 Validation loss 0.028516268357634544 Accuracy 0.71044921875\n",
      "Iteration 22000 Training loss 0.02485237829387188 Validation loss 0.028092393651604652 Accuracy 0.71484375\n",
      "Iteration 22010 Training loss 0.024441679939627647 Validation loss 0.02815450169146061 Accuracy 0.71435546875\n",
      "Iteration 22020 Training loss 0.02690376713871956 Validation loss 0.028481317684054375 Accuracy 0.7119140625\n",
      "Iteration 22030 Training loss 0.02701607532799244 Validation loss 0.028759554028511047 Accuracy 0.70849609375\n",
      "Iteration 22040 Training loss 0.0238287765532732 Validation loss 0.028409305959939957 Accuracy 0.7119140625\n",
      "Iteration 22050 Training loss 0.026321377605199814 Validation loss 0.02806674689054489 Accuracy 0.71533203125\n",
      "Iteration 22060 Training loss 0.025911478325724602 Validation loss 0.028300922363996506 Accuracy 0.712890625\n",
      "Iteration 22070 Training loss 0.025761118158698082 Validation loss 0.028273625299334526 Accuracy 0.71337890625\n",
      "Iteration 22080 Training loss 0.028505424037575722 Validation loss 0.028281493112444878 Accuracy 0.71337890625\n",
      "Iteration 22090 Training loss 0.026476256549358368 Validation loss 0.028340673074126244 Accuracy 0.7119140625\n",
      "Iteration 22100 Training loss 0.027162699028849602 Validation loss 0.02814973145723343 Accuracy 0.71435546875\n",
      "Iteration 22110 Training loss 0.021904146298766136 Validation loss 0.028202563524246216 Accuracy 0.7138671875\n",
      "Iteration 22120 Training loss 0.026144562289118767 Validation loss 0.02818065695464611 Accuracy 0.7138671875\n",
      "Iteration 22130 Training loss 0.028543345630168915 Validation loss 0.028125768527388573 Accuracy 0.71533203125\n",
      "Iteration 22140 Training loss 0.02685568667948246 Validation loss 0.02845192514359951 Accuracy 0.7119140625\n",
      "Iteration 22150 Training loss 0.026045242324471474 Validation loss 0.028208736330270767 Accuracy 0.7138671875\n",
      "Iteration 22160 Training loss 0.0234642643481493 Validation loss 0.028291400521993637 Accuracy 0.71337890625\n",
      "Iteration 22170 Training loss 0.027265911921858788 Validation loss 0.028665931895375252 Accuracy 0.7099609375\n",
      "Iteration 22180 Training loss 0.026833325624465942 Validation loss 0.028922544792294502 Accuracy 0.70751953125\n",
      "Iteration 22190 Training loss 0.0267294030636549 Validation loss 0.028590399771928787 Accuracy 0.71044921875\n",
      "Iteration 22200 Training loss 0.024660814553499222 Validation loss 0.029151754453778267 Accuracy 0.705078125\n",
      "Iteration 22210 Training loss 0.026960652321577072 Validation loss 0.028369959443807602 Accuracy 0.71240234375\n",
      "Iteration 22220 Training loss 0.025403063744306564 Validation loss 0.02841399796307087 Accuracy 0.71240234375\n",
      "Iteration 22230 Training loss 0.025978321209549904 Validation loss 0.02856643684208393 Accuracy 0.7099609375\n",
      "Iteration 22240 Training loss 0.024153919890522957 Validation loss 0.02849847823381424 Accuracy 0.7109375\n",
      "Iteration 22250 Training loss 0.027502235025167465 Validation loss 0.028143031522631645 Accuracy 0.71435546875\n",
      "Iteration 22260 Training loss 0.025407247245311737 Validation loss 0.028214996680617332 Accuracy 0.7138671875\n",
      "Iteration 22270 Training loss 0.025009892880916595 Validation loss 0.028147581964731216 Accuracy 0.7138671875\n",
      "Iteration 22280 Training loss 0.027196021750569344 Validation loss 0.028251204639673233 Accuracy 0.7138671875\n",
      "Iteration 22290 Training loss 0.022846290841698647 Validation loss 0.02812771499156952 Accuracy 0.71484375\n",
      "Iteration 22300 Training loss 0.02708749659359455 Validation loss 0.028452062979340553 Accuracy 0.71142578125\n",
      "Iteration 22310 Training loss 0.023783141747117043 Validation loss 0.028094183653593063 Accuracy 0.71435546875\n",
      "Iteration 22320 Training loss 0.02676267921924591 Validation loss 0.028140682727098465 Accuracy 0.71435546875\n",
      "Iteration 22330 Training loss 0.024095572531223297 Validation loss 0.028330039232969284 Accuracy 0.71240234375\n",
      "Iteration 22340 Training loss 0.0262029729783535 Validation loss 0.02826489694416523 Accuracy 0.71337890625\n",
      "Iteration 22350 Training loss 0.0273270420730114 Validation loss 0.028180517256259918 Accuracy 0.7138671875\n",
      "Iteration 22360 Training loss 0.02165677398443222 Validation loss 0.028451282531023026 Accuracy 0.7119140625\n",
      "Iteration 22370 Training loss 0.023928580805659294 Validation loss 0.028375009074807167 Accuracy 0.71240234375\n",
      "Iteration 22380 Training loss 0.026761071756482124 Validation loss 0.028197959065437317 Accuracy 0.712890625\n",
      "Iteration 22390 Training loss 0.026216736063361168 Validation loss 0.028162935748696327 Accuracy 0.7138671875\n",
      "Iteration 22400 Training loss 0.024375492706894875 Validation loss 0.028347577899694443 Accuracy 0.71240234375\n",
      "Iteration 22410 Training loss 0.029261525720357895 Validation loss 0.028456326574087143 Accuracy 0.7119140625\n",
      "Iteration 22420 Training loss 0.023660512641072273 Validation loss 0.02806827984750271 Accuracy 0.71533203125\n",
      "Iteration 22430 Training loss 0.026970049366354942 Validation loss 0.028195634484291077 Accuracy 0.71435546875\n",
      "Iteration 22440 Training loss 0.02440645359456539 Validation loss 0.028178280219435692 Accuracy 0.71435546875\n",
      "Iteration 22450 Training loss 0.02524065598845482 Validation loss 0.028407983481884003 Accuracy 0.7119140625\n",
      "Iteration 22460 Training loss 0.025601139292120934 Validation loss 0.028287723660469055 Accuracy 0.71240234375\n",
      "Iteration 22470 Training loss 0.026815932244062424 Validation loss 0.028030702844262123 Accuracy 0.71533203125\n",
      "Iteration 22480 Training loss 0.02778899297118187 Validation loss 0.028220711275935173 Accuracy 0.71337890625\n",
      "Iteration 22490 Training loss 0.02597876451909542 Validation loss 0.02809404768049717 Accuracy 0.71533203125\n",
      "Iteration 22500 Training loss 0.028258226811885834 Validation loss 0.02838405966758728 Accuracy 0.71142578125\n",
      "Iteration 22510 Training loss 0.02321327105164528 Validation loss 0.02823087014257908 Accuracy 0.71337890625\n",
      "Iteration 22520 Training loss 0.024000532925128937 Validation loss 0.02797754481434822 Accuracy 0.71630859375\n",
      "Iteration 22530 Training loss 0.025567039847373962 Validation loss 0.02808774821460247 Accuracy 0.71435546875\n",
      "Iteration 22540 Training loss 0.026265904307365417 Validation loss 0.02803889289498329 Accuracy 0.71484375\n",
      "Iteration 22550 Training loss 0.025262096896767616 Validation loss 0.02807307243347168 Accuracy 0.7158203125\n",
      "Iteration 22560 Training loss 0.025394681841135025 Validation loss 0.02809176594018936 Accuracy 0.71435546875\n",
      "Iteration 22570 Training loss 0.026966923847794533 Validation loss 0.028523432090878487 Accuracy 0.7099609375\n",
      "Iteration 22580 Training loss 0.026800531893968582 Validation loss 0.028485914692282677 Accuracy 0.7099609375\n",
      "Iteration 22590 Training loss 0.027016444131731987 Validation loss 0.028007181361317635 Accuracy 0.7158203125\n",
      "Iteration 22600 Training loss 0.024454189464449883 Validation loss 0.02816336415708065 Accuracy 0.71435546875\n",
      "Iteration 22610 Training loss 0.028574353083968163 Validation loss 0.028152093291282654 Accuracy 0.7138671875\n",
      "Iteration 22620 Training loss 0.02771127223968506 Validation loss 0.028293317183852196 Accuracy 0.71337890625\n",
      "Iteration 22630 Training loss 0.02280570939183235 Validation loss 0.028202073648571968 Accuracy 0.71435546875\n",
      "Iteration 22640 Training loss 0.026539843529462814 Validation loss 0.028429195284843445 Accuracy 0.71240234375\n",
      "Iteration 22650 Training loss 0.02383560687303543 Validation loss 0.028176289051771164 Accuracy 0.71435546875\n",
      "Iteration 22660 Training loss 0.026759950444102287 Validation loss 0.02841031737625599 Accuracy 0.71142578125\n",
      "Iteration 22670 Training loss 0.024875277653336525 Validation loss 0.028360644355416298 Accuracy 0.7119140625\n",
      "Iteration 22680 Training loss 0.02647322602570057 Validation loss 0.028137709945440292 Accuracy 0.71435546875\n",
      "Iteration 22690 Training loss 0.025280043482780457 Validation loss 0.028105322271585464 Accuracy 0.71484375\n",
      "Iteration 22700 Training loss 0.028463445603847504 Validation loss 0.029419243335723877 Accuracy 0.701171875\n",
      "Iteration 22710 Training loss 0.02387051284313202 Validation loss 0.02822800539433956 Accuracy 0.7138671875\n",
      "Iteration 22720 Training loss 0.027601471170783043 Validation loss 0.02822122536599636 Accuracy 0.71337890625\n",
      "Iteration 22730 Training loss 0.026921797543764114 Validation loss 0.02838234044611454 Accuracy 0.7119140625\n",
      "Iteration 22740 Training loss 0.026266664266586304 Validation loss 0.028350943699479103 Accuracy 0.7119140625\n",
      "Iteration 22750 Training loss 0.02502390556037426 Validation loss 0.02848888374865055 Accuracy 0.71044921875\n",
      "Iteration 22760 Training loss 0.02345440350472927 Validation loss 0.028407437726855278 Accuracy 0.7119140625\n",
      "Iteration 22770 Training loss 0.025951573625206947 Validation loss 0.028299294412136078 Accuracy 0.71337890625\n",
      "Iteration 22780 Training loss 0.02466173842549324 Validation loss 0.028266102075576782 Accuracy 0.71337890625\n",
      "Iteration 22790 Training loss 0.024716168642044067 Validation loss 0.028109321370720863 Accuracy 0.71533203125\n",
      "Iteration 22800 Training loss 0.023817965760827065 Validation loss 0.028321625664830208 Accuracy 0.712890625\n",
      "Iteration 22810 Training loss 0.022388091310858727 Validation loss 0.028051991015672684 Accuracy 0.71484375\n",
      "Iteration 22820 Training loss 0.027205858379602432 Validation loss 0.02834249474108219 Accuracy 0.712890625\n",
      "Iteration 22830 Training loss 0.026317276060581207 Validation loss 0.029758388176560402 Accuracy 0.6982421875\n",
      "Iteration 22840 Training loss 0.023280339315533638 Validation loss 0.027974655851721764 Accuracy 0.7158203125\n",
      "Iteration 22850 Training loss 0.023325953632593155 Validation loss 0.02810659073293209 Accuracy 0.71484375\n",
      "Iteration 22860 Training loss 0.025234639644622803 Validation loss 0.0279878918081522 Accuracy 0.71630859375\n",
      "Iteration 22870 Training loss 0.024513285607099533 Validation loss 0.028166571632027626 Accuracy 0.71435546875\n",
      "Iteration 22880 Training loss 0.023795712739229202 Validation loss 0.028081508353352547 Accuracy 0.71484375\n",
      "Iteration 22890 Training loss 0.024050414562225342 Validation loss 0.027973748743534088 Accuracy 0.7158203125\n",
      "Iteration 22900 Training loss 0.03088107518851757 Validation loss 0.028443919494748116 Accuracy 0.71142578125\n",
      "Iteration 22910 Training loss 0.026352619752287865 Validation loss 0.028063258156180382 Accuracy 0.71533203125\n",
      "Iteration 22920 Training loss 0.024288037791848183 Validation loss 0.027973266318440437 Accuracy 0.71630859375\n",
      "Iteration 22930 Training loss 0.030147718265652657 Validation loss 0.028162557631731033 Accuracy 0.7138671875\n",
      "Iteration 22940 Training loss 0.02535611018538475 Validation loss 0.028398659080266953 Accuracy 0.71240234375\n",
      "Iteration 22950 Training loss 0.031156549230217934 Validation loss 0.028570154681801796 Accuracy 0.71044921875\n",
      "Iteration 22960 Training loss 0.024627389386296272 Validation loss 0.028325462713837624 Accuracy 0.712890625\n",
      "Iteration 22970 Training loss 0.027197834104299545 Validation loss 0.028256913647055626 Accuracy 0.71337890625\n",
      "Iteration 22980 Training loss 0.02562139928340912 Validation loss 0.028279734775424004 Accuracy 0.71337890625\n",
      "Iteration 22990 Training loss 0.0229487344622612 Validation loss 0.028533659875392914 Accuracy 0.71142578125\n",
      "Iteration 23000 Training loss 0.026210321113467216 Validation loss 0.028093965724110603 Accuracy 0.71484375\n",
      "Iteration 23010 Training loss 0.020732592791318893 Validation loss 0.028278786689043045 Accuracy 0.712890625\n",
      "Iteration 23020 Training loss 0.022034958004951477 Validation loss 0.0280684195458889 Accuracy 0.7158203125\n",
      "Iteration 23030 Training loss 0.024308618158102036 Validation loss 0.028416112065315247 Accuracy 0.71240234375\n",
      "Iteration 23040 Training loss 0.0238235741853714 Validation loss 0.028139086440205574 Accuracy 0.71533203125\n",
      "Iteration 23050 Training loss 0.023594701662659645 Validation loss 0.0287488903850317 Accuracy 0.70849609375\n",
      "Iteration 23060 Training loss 0.02709437534213066 Validation loss 0.028125526383519173 Accuracy 0.71484375\n",
      "Iteration 23070 Training loss 0.025270218029618263 Validation loss 0.028479544445872307 Accuracy 0.7119140625\n",
      "Iteration 23080 Training loss 0.02385891228914261 Validation loss 0.028082136064767838 Accuracy 0.71533203125\n",
      "Iteration 23090 Training loss 0.026262877508997917 Validation loss 0.028122620657086372 Accuracy 0.71484375\n",
      "Iteration 23100 Training loss 0.025624196976423264 Validation loss 0.027959374710917473 Accuracy 0.71630859375\n",
      "Iteration 23110 Training loss 0.02887040376663208 Validation loss 0.02806682139635086 Accuracy 0.71484375\n",
      "Iteration 23120 Training loss 0.023231694474816322 Validation loss 0.028196997940540314 Accuracy 0.7138671875\n",
      "Iteration 23130 Training loss 0.030992604792118073 Validation loss 0.030153632164001465 Accuracy 0.6943359375\n",
      "Iteration 23140 Training loss 0.02611219696700573 Validation loss 0.028528470546007156 Accuracy 0.71044921875\n",
      "Iteration 23150 Training loss 0.025188451632857323 Validation loss 0.02821996808052063 Accuracy 0.7138671875\n",
      "Iteration 23160 Training loss 0.024063626304268837 Validation loss 0.028494613245129585 Accuracy 0.7109375\n",
      "Iteration 23170 Training loss 0.023877980187535286 Validation loss 0.027946453541517258 Accuracy 0.71630859375\n",
      "Iteration 23180 Training loss 0.027235010638833046 Validation loss 0.028119167312979698 Accuracy 0.71484375\n",
      "Iteration 23190 Training loss 0.025825245305895805 Validation loss 0.02802402712404728 Accuracy 0.71533203125\n",
      "Iteration 23200 Training loss 0.024867406114935875 Validation loss 0.028256654739379883 Accuracy 0.71240234375\n",
      "Iteration 23210 Training loss 0.02517675794661045 Validation loss 0.028143933042883873 Accuracy 0.71435546875\n",
      "Iteration 23220 Training loss 0.021820133551955223 Validation loss 0.028117630630731583 Accuracy 0.71484375\n",
      "Iteration 23230 Training loss 0.022905781865119934 Validation loss 0.028339501470327377 Accuracy 0.712890625\n",
      "Iteration 23240 Training loss 0.024673325940966606 Validation loss 0.0280477162450552 Accuracy 0.71533203125\n",
      "Iteration 23250 Training loss 0.024065429344773293 Validation loss 0.028128808364272118 Accuracy 0.71484375\n",
      "Iteration 23260 Training loss 0.02683175541460514 Validation loss 0.027886204421520233 Accuracy 0.71630859375\n",
      "Iteration 23270 Training loss 0.024557510390877724 Validation loss 0.028339935466647148 Accuracy 0.7119140625\n",
      "Iteration 23280 Training loss 0.025060493499040604 Validation loss 0.02835109271109104 Accuracy 0.71240234375\n",
      "Iteration 23290 Training loss 0.023444289341568947 Validation loss 0.02819584310054779 Accuracy 0.71435546875\n",
      "Iteration 23300 Training loss 0.023592673242092133 Validation loss 0.029460638761520386 Accuracy 0.7001953125\n",
      "Iteration 23310 Training loss 0.025945644825696945 Validation loss 0.028519613668322563 Accuracy 0.7109375\n",
      "Iteration 23320 Training loss 0.02574826590716839 Validation loss 0.02804078347980976 Accuracy 0.71484375\n",
      "Iteration 23330 Training loss 0.02698417566716671 Validation loss 0.028823915868997574 Accuracy 0.70751953125\n",
      "Iteration 23340 Training loss 0.027481267228722572 Validation loss 0.0299084335565567 Accuracy 0.6962890625\n",
      "Iteration 23350 Training loss 0.02462955005466938 Validation loss 0.027926240116357803 Accuracy 0.716796875\n",
      "Iteration 23360 Training loss 0.024065067991614342 Validation loss 0.028230616822838783 Accuracy 0.71337890625\n",
      "Iteration 23370 Training loss 0.02433803863823414 Validation loss 0.0294524896889925 Accuracy 0.70068359375\n",
      "Iteration 23380 Training loss 0.025106200948357582 Validation loss 0.029090456664562225 Accuracy 0.70458984375\n",
      "Iteration 23390 Training loss 0.023994984105229378 Validation loss 0.02804275043308735 Accuracy 0.7158203125\n",
      "Iteration 23400 Training loss 0.023823298513889313 Validation loss 0.02809804119169712 Accuracy 0.71484375\n",
      "Iteration 23410 Training loss 0.0249275304377079 Validation loss 0.028202172368764877 Accuracy 0.7138671875\n",
      "Iteration 23420 Training loss 0.02550685405731201 Validation loss 0.02822580747306347 Accuracy 0.7138671875\n",
      "Iteration 23430 Training loss 0.029080519452691078 Validation loss 0.028091536834836006 Accuracy 0.71533203125\n",
      "Iteration 23440 Training loss 0.023282965645194054 Validation loss 0.02811138518154621 Accuracy 0.71484375\n",
      "Iteration 23450 Training loss 0.025028305128216743 Validation loss 0.02799084410071373 Accuracy 0.7158203125\n",
      "Iteration 23460 Training loss 0.022234702482819557 Validation loss 0.028646517544984818 Accuracy 0.708984375\n",
      "Iteration 23470 Training loss 0.02362900972366333 Validation loss 0.027986839413642883 Accuracy 0.71630859375\n",
      "Iteration 23480 Training loss 0.023722926154732704 Validation loss 0.028205741196870804 Accuracy 0.71337890625\n",
      "Iteration 23490 Training loss 0.0293786209076643 Validation loss 0.028375841677188873 Accuracy 0.71240234375\n",
      "Iteration 23500 Training loss 0.026428058743476868 Validation loss 0.028166893869638443 Accuracy 0.71484375\n",
      "Iteration 23510 Training loss 0.02641899511218071 Validation loss 0.028924940153956413 Accuracy 0.7060546875\n",
      "Iteration 23520 Training loss 0.0238324124366045 Validation loss 0.02804894931614399 Accuracy 0.71533203125\n",
      "Iteration 23530 Training loss 0.021961282938718796 Validation loss 0.02817900851368904 Accuracy 0.71337890625\n",
      "Iteration 23540 Training loss 0.0276983343064785 Validation loss 0.028389744460582733 Accuracy 0.7119140625\n",
      "Iteration 23550 Training loss 0.028978049755096436 Validation loss 0.028685379773378372 Accuracy 0.7080078125\n",
      "Iteration 23560 Training loss 0.027016112580895424 Validation loss 0.028873929753899574 Accuracy 0.70703125\n",
      "Iteration 23570 Training loss 0.02410423755645752 Validation loss 0.028534969314932823 Accuracy 0.7099609375\n",
      "Iteration 23580 Training loss 0.026080317795276642 Validation loss 0.02835577167570591 Accuracy 0.71240234375\n",
      "Iteration 23590 Training loss 0.025717956945300102 Validation loss 0.02834412455558777 Accuracy 0.71240234375\n",
      "Iteration 23600 Training loss 0.02646552585065365 Validation loss 0.028551224619150162 Accuracy 0.71044921875\n",
      "Iteration 23610 Training loss 0.027105184271931648 Validation loss 0.028816137462854385 Accuracy 0.7080078125\n",
      "Iteration 23620 Training loss 0.0256337970495224 Validation loss 0.028110088780522346 Accuracy 0.71435546875\n",
      "Iteration 23630 Training loss 0.02755802869796753 Validation loss 0.028429804369807243 Accuracy 0.71240234375\n",
      "Iteration 23640 Training loss 0.023208124563097954 Validation loss 0.028687557205557823 Accuracy 0.708984375\n",
      "Iteration 23650 Training loss 0.024424150586128235 Validation loss 0.028154056519269943 Accuracy 0.71484375\n",
      "Iteration 23660 Training loss 0.026568293571472168 Validation loss 0.0286355409771204 Accuracy 0.7099609375\n",
      "Iteration 23670 Training loss 0.026725957170128822 Validation loss 0.02838669903576374 Accuracy 0.7119140625\n",
      "Iteration 23680 Training loss 0.027309661731123924 Validation loss 0.027861736714839935 Accuracy 0.71728515625\n",
      "Iteration 23690 Training loss 0.025800442323088646 Validation loss 0.028700534254312515 Accuracy 0.708984375\n",
      "Iteration 23700 Training loss 0.02868124283850193 Validation loss 0.028147313743829727 Accuracy 0.71484375\n",
      "Iteration 23710 Training loss 0.025076987221837044 Validation loss 0.028128046542406082 Accuracy 0.71484375\n",
      "Iteration 23720 Training loss 0.027248069643974304 Validation loss 0.028104908764362335 Accuracy 0.71484375\n",
      "Iteration 23730 Training loss 0.026259897276759148 Validation loss 0.028048377484083176 Accuracy 0.7158203125\n",
      "Iteration 23740 Training loss 0.02431664429605007 Validation loss 0.02812281623482704 Accuracy 0.71484375\n",
      "Iteration 23750 Training loss 0.02330406755208969 Validation loss 0.027928054332733154 Accuracy 0.71630859375\n",
      "Iteration 23760 Training loss 0.02417464181780815 Validation loss 0.027992162853479385 Accuracy 0.71533203125\n",
      "Iteration 23770 Training loss 0.026143986731767654 Validation loss 0.028359100222587585 Accuracy 0.71142578125\n",
      "Iteration 23780 Training loss 0.025875862687826157 Validation loss 0.02880009636282921 Accuracy 0.70751953125\n",
      "Iteration 23790 Training loss 0.024579687044024467 Validation loss 0.028215399011969566 Accuracy 0.712890625\n",
      "Iteration 23800 Training loss 0.02729535847902298 Validation loss 0.027966869994997978 Accuracy 0.716796875\n",
      "Iteration 23810 Training loss 0.02542969398200512 Validation loss 0.028049344196915627 Accuracy 0.71533203125\n",
      "Iteration 23820 Training loss 0.027727331966161728 Validation loss 0.029298989102244377 Accuracy 0.703125\n",
      "Iteration 23830 Training loss 0.02503892406821251 Validation loss 0.028160467743873596 Accuracy 0.7138671875\n",
      "Iteration 23840 Training loss 0.02525310218334198 Validation loss 0.02856295369565487 Accuracy 0.7099609375\n",
      "Iteration 23850 Training loss 0.02358708158135414 Validation loss 0.02835613675415516 Accuracy 0.71240234375\n",
      "Iteration 23860 Training loss 0.025521699339151382 Validation loss 0.02814752794802189 Accuracy 0.7138671875\n",
      "Iteration 23870 Training loss 0.0278833769261837 Validation loss 0.0280551016330719 Accuracy 0.71484375\n",
      "Iteration 23880 Training loss 0.025911929085850716 Validation loss 0.029269989579916 Accuracy 0.70361328125\n",
      "Iteration 23890 Training loss 0.021656421944499016 Validation loss 0.02813328616321087 Accuracy 0.71435546875\n",
      "Iteration 23900 Training loss 0.023053230717778206 Validation loss 0.028131645172834396 Accuracy 0.71435546875\n",
      "Iteration 23910 Training loss 0.024337900802493095 Validation loss 0.0283503457903862 Accuracy 0.71240234375\n",
      "Iteration 23920 Training loss 0.02496950514614582 Validation loss 0.02808445692062378 Accuracy 0.71484375\n",
      "Iteration 23930 Training loss 0.02552257850766182 Validation loss 0.02816118113696575 Accuracy 0.71435546875\n",
      "Iteration 23940 Training loss 0.02610727958381176 Validation loss 0.02806117758154869 Accuracy 0.71533203125\n",
      "Iteration 23950 Training loss 0.017947092652320862 Validation loss 0.028058087453246117 Accuracy 0.71533203125\n",
      "Iteration 23960 Training loss 0.026780107989907265 Validation loss 0.028045233339071274 Accuracy 0.7158203125\n",
      "Iteration 23970 Training loss 0.02622835338115692 Validation loss 0.028518546372652054 Accuracy 0.7109375\n",
      "Iteration 23980 Training loss 0.03016337752342224 Validation loss 0.02876957878470421 Accuracy 0.70849609375\n",
      "Iteration 23990 Training loss 0.02336743101477623 Validation loss 0.028503861278295517 Accuracy 0.7099609375\n",
      "Iteration 24000 Training loss 0.02608316019177437 Validation loss 0.028110623359680176 Accuracy 0.71484375\n",
      "Iteration 24010 Training loss 0.026540331542491913 Validation loss 0.02810927852988243 Accuracy 0.71484375\n",
      "Iteration 24020 Training loss 0.023319315165281296 Validation loss 0.028032371774315834 Accuracy 0.71533203125\n",
      "Iteration 24030 Training loss 0.02568562515079975 Validation loss 0.02805463969707489 Accuracy 0.71484375\n",
      "Iteration 24040 Training loss 0.02501649223268032 Validation loss 0.028163239359855652 Accuracy 0.71435546875\n",
      "Iteration 24050 Training loss 0.02523675747215748 Validation loss 0.028074942529201508 Accuracy 0.7158203125\n",
      "Iteration 24060 Training loss 0.025706084445118904 Validation loss 0.029717303812503815 Accuracy 0.69921875\n",
      "Iteration 24070 Training loss 0.01963553950190544 Validation loss 0.02804657816886902 Accuracy 0.71533203125\n",
      "Iteration 24080 Training loss 0.023102620616555214 Validation loss 0.02801918424665928 Accuracy 0.7158203125\n",
      "Iteration 24090 Training loss 0.027469411492347717 Validation loss 0.028145980089902878 Accuracy 0.71435546875\n",
      "Iteration 24100 Training loss 0.02618470788002014 Validation loss 0.028128450736403465 Accuracy 0.7158203125\n",
      "Iteration 24110 Training loss 0.02520669437944889 Validation loss 0.02827903814613819 Accuracy 0.71337890625\n",
      "Iteration 24120 Training loss 0.0258628036826849 Validation loss 0.028432007879018784 Accuracy 0.71142578125\n",
      "Iteration 24130 Training loss 0.025139983743429184 Validation loss 0.029314052313566208 Accuracy 0.70263671875\n",
      "Iteration 24140 Training loss 0.02353581227362156 Validation loss 0.027926703914999962 Accuracy 0.71728515625\n",
      "Iteration 24150 Training loss 0.02967010997235775 Validation loss 0.027956824749708176 Accuracy 0.71630859375\n",
      "Iteration 24160 Training loss 0.02758079208433628 Validation loss 0.028707031160593033 Accuracy 0.70947265625\n",
      "Iteration 24170 Training loss 0.02193099446594715 Validation loss 0.028088055551052094 Accuracy 0.71484375\n",
      "Iteration 24180 Training loss 0.021720968186855316 Validation loss 0.0281513761729002 Accuracy 0.7138671875\n",
      "Iteration 24190 Training loss 0.0227435864508152 Validation loss 0.028087865561246872 Accuracy 0.71484375\n",
      "Iteration 24200 Training loss 0.02689461223781109 Validation loss 0.028141697868704796 Accuracy 0.71484375\n",
      "Iteration 24210 Training loss 0.026779474690556526 Validation loss 0.028346940875053406 Accuracy 0.7119140625\n",
      "Iteration 24220 Training loss 0.025429846718907356 Validation loss 0.028106581419706345 Accuracy 0.71533203125\n",
      "Iteration 24230 Training loss 0.024960171431303024 Validation loss 0.028133196756243706 Accuracy 0.71484375\n",
      "Iteration 24240 Training loss 0.023830115795135498 Validation loss 0.028047338128089905 Accuracy 0.71533203125\n",
      "Iteration 24250 Training loss 0.027319997549057007 Validation loss 0.0282612107694149 Accuracy 0.712890625\n",
      "Iteration 24260 Training loss 0.027705783024430275 Validation loss 0.028266845270991325 Accuracy 0.712890625\n",
      "Iteration 24270 Training loss 0.02634568326175213 Validation loss 0.028401289135217667 Accuracy 0.7119140625\n",
      "Iteration 24280 Training loss 0.024739261716604233 Validation loss 0.028146876022219658 Accuracy 0.7138671875\n",
      "Iteration 24290 Training loss 0.023689404129981995 Validation loss 0.02826915867626667 Accuracy 0.71240234375\n",
      "Iteration 24300 Training loss 0.025778159499168396 Validation loss 0.02794075571000576 Accuracy 0.7158203125\n",
      "Iteration 24310 Training loss 0.027164829894900322 Validation loss 0.028399977833032608 Accuracy 0.71142578125\n",
      "Iteration 24320 Training loss 0.025497714057564735 Validation loss 0.028360266238451004 Accuracy 0.71240234375\n",
      "Iteration 24330 Training loss 0.024668177589774132 Validation loss 0.028060302138328552 Accuracy 0.71484375\n",
      "Iteration 24340 Training loss 0.026663683354854584 Validation loss 0.028286820277571678 Accuracy 0.71240234375\n",
      "Iteration 24350 Training loss 0.027239283546805382 Validation loss 0.0280087199062109 Accuracy 0.716796875\n",
      "Iteration 24360 Training loss 0.021931162104010582 Validation loss 0.02802269160747528 Accuracy 0.71533203125\n",
      "Iteration 24370 Training loss 0.02261507883667946 Validation loss 0.028003031387925148 Accuracy 0.7158203125\n",
      "Iteration 24380 Training loss 0.028060264885425568 Validation loss 0.028652604669332504 Accuracy 0.7099609375\n",
      "Iteration 24390 Training loss 0.02407936565577984 Validation loss 0.02851145528256893 Accuracy 0.7109375\n",
      "Iteration 24400 Training loss 0.02521161362528801 Validation loss 0.02806834690272808 Accuracy 0.71484375\n",
      "Iteration 24410 Training loss 0.024945175275206566 Validation loss 0.028093697503209114 Accuracy 0.71484375\n",
      "Iteration 24420 Training loss 0.0281758364289999 Validation loss 0.028075823560357094 Accuracy 0.71533203125\n",
      "Iteration 24430 Training loss 0.024119552224874496 Validation loss 0.028188476338982582 Accuracy 0.7138671875\n",
      "Iteration 24440 Training loss 0.029153093695640564 Validation loss 0.028246406465768814 Accuracy 0.712890625\n",
      "Iteration 24450 Training loss 0.023511743173003197 Validation loss 0.028902797028422356 Accuracy 0.70654296875\n",
      "Iteration 24460 Training loss 0.025854341685771942 Validation loss 0.02806965820491314 Accuracy 0.71435546875\n",
      "Iteration 24470 Training loss 0.02463105507194996 Validation loss 0.028096769005060196 Accuracy 0.71435546875\n",
      "Iteration 24480 Training loss 0.025823349133133888 Validation loss 0.028173290193080902 Accuracy 0.71435546875\n",
      "Iteration 24490 Training loss 0.02278820239007473 Validation loss 0.028168655931949615 Accuracy 0.71484375\n",
      "Iteration 24500 Training loss 0.022664425894618034 Validation loss 0.028134673833847046 Accuracy 0.71484375\n",
      "Iteration 24510 Training loss 0.023569533601403236 Validation loss 0.027972416952252388 Accuracy 0.7158203125\n",
      "Iteration 24520 Training loss 0.02834363281726837 Validation loss 0.02801859751343727 Accuracy 0.7158203125\n",
      "Iteration 24530 Training loss 0.025608088821172714 Validation loss 0.02796969935297966 Accuracy 0.71630859375\n",
      "Iteration 24540 Training loss 0.02457924373447895 Validation loss 0.027988119050860405 Accuracy 0.71533203125\n",
      "Iteration 24550 Training loss 0.025863774120807648 Validation loss 0.028270775452256203 Accuracy 0.712890625\n",
      "Iteration 24560 Training loss 0.023984305560588837 Validation loss 0.02819487638771534 Accuracy 0.7138671875\n",
      "Iteration 24570 Training loss 0.023099707439541817 Validation loss 0.028035968542099 Accuracy 0.7158203125\n",
      "Iteration 24580 Training loss 0.023513246327638626 Validation loss 0.028158139437437057 Accuracy 0.7138671875\n",
      "Iteration 24590 Training loss 0.025420093908905983 Validation loss 0.028026996180415154 Accuracy 0.71533203125\n",
      "Iteration 24600 Training loss 0.025303814560174942 Validation loss 0.028149422258138657 Accuracy 0.71435546875\n",
      "Iteration 24610 Training loss 0.02588154561817646 Validation loss 0.028158150613307953 Accuracy 0.7138671875\n",
      "Iteration 24620 Training loss 0.025409435853362083 Validation loss 0.02841171994805336 Accuracy 0.71142578125\n",
      "Iteration 24630 Training loss 0.02745639719069004 Validation loss 0.028985967859625816 Accuracy 0.70556640625\n",
      "Iteration 24640 Training loss 0.023006189614534378 Validation loss 0.028011566027998924 Accuracy 0.71533203125\n",
      "Iteration 24650 Training loss 0.02482035383582115 Validation loss 0.028496429324150085 Accuracy 0.7109375\n",
      "Iteration 24660 Training loss 0.024673420935869217 Validation loss 0.028132088482379913 Accuracy 0.71484375\n",
      "Iteration 24670 Training loss 0.028970496729016304 Validation loss 0.028200622648000717 Accuracy 0.712890625\n",
      "Iteration 24680 Training loss 0.023122768849134445 Validation loss 0.02830190770328045 Accuracy 0.712890625\n",
      "Iteration 24690 Training loss 0.02567411959171295 Validation loss 0.0282405074685812 Accuracy 0.7138671875\n",
      "Iteration 24700 Training loss 0.025763997808098793 Validation loss 0.0287984199821949 Accuracy 0.7080078125\n",
      "Iteration 24710 Training loss 0.024198099970817566 Validation loss 0.028112275525927544 Accuracy 0.71435546875\n",
      "Iteration 24720 Training loss 0.027922164648771286 Validation loss 0.02798371948301792 Accuracy 0.7158203125\n",
      "Iteration 24730 Training loss 0.022475333884358406 Validation loss 0.028090279549360275 Accuracy 0.71533203125\n",
      "Iteration 24740 Training loss 0.02710702456533909 Validation loss 0.028017597272992134 Accuracy 0.71630859375\n",
      "Iteration 24750 Training loss 0.022788381204009056 Validation loss 0.028229333460330963 Accuracy 0.71337890625\n",
      "Iteration 24760 Training loss 0.025806233286857605 Validation loss 0.02798531949520111 Accuracy 0.71533203125\n",
      "Iteration 24770 Training loss 0.02258581481873989 Validation loss 0.02802080474793911 Accuracy 0.71484375\n",
      "Iteration 24780 Training loss 0.02880232222378254 Validation loss 0.02826750837266445 Accuracy 0.7138671875\n",
      "Iteration 24790 Training loss 0.026537379249930382 Validation loss 0.02826070226728916 Accuracy 0.71337890625\n",
      "Iteration 24800 Training loss 0.02235516346991062 Validation loss 0.02813694067299366 Accuracy 0.71435546875\n",
      "Iteration 24810 Training loss 0.024349665269255638 Validation loss 0.028213001787662506 Accuracy 0.71337890625\n",
      "Iteration 24820 Training loss 0.02272091619670391 Validation loss 0.028149671852588654 Accuracy 0.71435546875\n",
      "Iteration 24830 Training loss 0.023607468232512474 Validation loss 0.028058934956789017 Accuracy 0.71630859375\n",
      "Iteration 24840 Training loss 0.026906341314315796 Validation loss 0.028370868414640427 Accuracy 0.712890625\n",
      "Iteration 24850 Training loss 0.025235427543520927 Validation loss 0.0284748338162899 Accuracy 0.7109375\n",
      "Iteration 24860 Training loss 0.026071757078170776 Validation loss 0.02804948017001152 Accuracy 0.7158203125\n",
      "Iteration 24870 Training loss 0.024067377671599388 Validation loss 0.028053143993020058 Accuracy 0.7158203125\n",
      "Iteration 24880 Training loss 0.020411960780620575 Validation loss 0.028208578005433083 Accuracy 0.71337890625\n",
      "Iteration 24890 Training loss 0.027703968808054924 Validation loss 0.028391215950250626 Accuracy 0.71240234375\n",
      "Iteration 24900 Training loss 0.024695487692952156 Validation loss 0.028278155252337456 Accuracy 0.71337890625\n",
      "Iteration 24910 Training loss 0.027697494253516197 Validation loss 0.028648855164647102 Accuracy 0.708984375\n",
      "Iteration 24920 Training loss 0.02476470358669758 Validation loss 0.028253847733139992 Accuracy 0.7138671875\n",
      "Iteration 24930 Training loss 0.025843434035778046 Validation loss 0.028193265199661255 Accuracy 0.71240234375\n",
      "Iteration 24940 Training loss 0.024667419493198395 Validation loss 0.028294242918491364 Accuracy 0.7138671875\n",
      "Iteration 24950 Training loss 0.022708825767040253 Validation loss 0.02784072607755661 Accuracy 0.716796875\n",
      "Iteration 24960 Training loss 0.026128094643354416 Validation loss 0.028826285153627396 Accuracy 0.70703125\n",
      "Iteration 24970 Training loss 0.026558099314570427 Validation loss 0.028045745566487312 Accuracy 0.7158203125\n",
      "Iteration 24980 Training loss 0.02545349858701229 Validation loss 0.028787773102521896 Accuracy 0.70849609375\n",
      "Iteration 24990 Training loss 0.026894746348261833 Validation loss 0.028137782588601112 Accuracy 0.71533203125\n",
      "Iteration 25000 Training loss 0.02529538981616497 Validation loss 0.02831154316663742 Accuracy 0.71337890625\n",
      "Iteration 25010 Training loss 0.0235669557005167 Validation loss 0.028129251673817635 Accuracy 0.71533203125\n",
      "Iteration 25020 Training loss 0.026003992184996605 Validation loss 0.028186557814478874 Accuracy 0.7138671875\n",
      "Iteration 25030 Training loss 0.02544821985065937 Validation loss 0.02825850434601307 Accuracy 0.712890625\n",
      "Iteration 25040 Training loss 0.02519925683736801 Validation loss 0.028069786727428436 Accuracy 0.71533203125\n",
      "Iteration 25050 Training loss 0.023509688675403595 Validation loss 0.027950000017881393 Accuracy 0.71630859375\n",
      "Iteration 25060 Training loss 0.025820119306445122 Validation loss 0.02830594964325428 Accuracy 0.71240234375\n",
      "Iteration 25070 Training loss 0.023924896493554115 Validation loss 0.028061287477612495 Accuracy 0.7158203125\n",
      "Iteration 25080 Training loss 0.026919210329651833 Validation loss 0.02812248468399048 Accuracy 0.71533203125\n",
      "Iteration 25090 Training loss 0.02498394437134266 Validation loss 0.028098944574594498 Accuracy 0.7158203125\n",
      "Iteration 25100 Training loss 0.024879978969693184 Validation loss 0.028006888926029205 Accuracy 0.7158203125\n",
      "Iteration 25110 Training loss 0.025329170748591423 Validation loss 0.02848190814256668 Accuracy 0.7119140625\n",
      "Iteration 25120 Training loss 0.023298827931284904 Validation loss 0.02811732143163681 Accuracy 0.71533203125\n",
      "Iteration 25130 Training loss 0.028268346562981606 Validation loss 0.028260765597224236 Accuracy 0.7138671875\n",
      "Iteration 25140 Training loss 0.02357049472630024 Validation loss 0.02809629775583744 Accuracy 0.71435546875\n",
      "Iteration 25150 Training loss 0.023891199380159378 Validation loss 0.0278942808508873 Accuracy 0.716796875\n",
      "Iteration 25160 Training loss 0.025658216327428818 Validation loss 0.028076235204935074 Accuracy 0.7158203125\n",
      "Iteration 25170 Training loss 0.027326202020049095 Validation loss 0.02813011221587658 Accuracy 0.71435546875\n",
      "Iteration 25180 Training loss 0.027111709117889404 Validation loss 0.028797172009944916 Accuracy 0.7080078125\n",
      "Iteration 25190 Training loss 0.025442814454436302 Validation loss 0.028242314234375954 Accuracy 0.71337890625\n",
      "Iteration 25200 Training loss 0.02387979067862034 Validation loss 0.02812800742685795 Accuracy 0.7138671875\n",
      "Iteration 25210 Training loss 0.02546146884560585 Validation loss 0.02875850535929203 Accuracy 0.70849609375\n",
      "Iteration 25220 Training loss 0.025643397122621536 Validation loss 0.028167422860860825 Accuracy 0.71435546875\n",
      "Iteration 25230 Training loss 0.02470383234322071 Validation loss 0.028222428634762764 Accuracy 0.7138671875\n",
      "Iteration 25240 Training loss 0.023945942521095276 Validation loss 0.02808624878525734 Accuracy 0.71533203125\n",
      "Iteration 25250 Training loss 0.02847805805504322 Validation loss 0.02793062850832939 Accuracy 0.71630859375\n",
      "Iteration 25260 Training loss 0.025708045810461044 Validation loss 0.028013043105602264 Accuracy 0.7158203125\n",
      "Iteration 25270 Training loss 0.024548543617129326 Validation loss 0.028137655928730965 Accuracy 0.71435546875\n",
      "Iteration 25280 Training loss 0.02554132230579853 Validation loss 0.02942950651049614 Accuracy 0.70166015625\n",
      "Iteration 25290 Training loss 0.02091365121304989 Validation loss 0.02803349308669567 Accuracy 0.71630859375\n",
      "Iteration 25300 Training loss 0.02483695186674595 Validation loss 0.028044991195201874 Accuracy 0.71533203125\n",
      "Iteration 25310 Training loss 0.02751205489039421 Validation loss 0.028224671259522438 Accuracy 0.7138671875\n",
      "Iteration 25320 Training loss 0.024947097525000572 Validation loss 0.028183715417981148 Accuracy 0.71435546875\n",
      "Iteration 25330 Training loss 0.02275380864739418 Validation loss 0.028413061052560806 Accuracy 0.7109375\n",
      "Iteration 25340 Training loss 0.024108784273266792 Validation loss 0.028213273733854294 Accuracy 0.712890625\n",
      "Iteration 25350 Training loss 0.02662496455013752 Validation loss 0.029337657615542412 Accuracy 0.7021484375\n",
      "Iteration 25360 Training loss 0.02470509335398674 Validation loss 0.02841123938560486 Accuracy 0.71142578125\n",
      "Iteration 25370 Training loss 0.024837814271450043 Validation loss 0.028076281771063805 Accuracy 0.71484375\n",
      "Iteration 25380 Training loss 0.024961212649941444 Validation loss 0.02814679592847824 Accuracy 0.71435546875\n",
      "Iteration 25390 Training loss 0.020677700638771057 Validation loss 0.02786611206829548 Accuracy 0.716796875\n",
      "Iteration 25400 Training loss 0.024908950552344322 Validation loss 0.028260502964258194 Accuracy 0.712890625\n",
      "Iteration 25410 Training loss 0.02416987158358097 Validation loss 0.028053881600499153 Accuracy 0.71533203125\n",
      "Iteration 25420 Training loss 0.028928786516189575 Validation loss 0.02808009274303913 Accuracy 0.71435546875\n",
      "Iteration 25430 Training loss 0.02365458570420742 Validation loss 0.0280621238052845 Accuracy 0.71533203125\n",
      "Iteration 25440 Training loss 0.026644818484783173 Validation loss 0.028018208220601082 Accuracy 0.71484375\n",
      "Iteration 25450 Training loss 0.02409772388637066 Validation loss 0.02812430076301098 Accuracy 0.71484375\n",
      "Iteration 25460 Training loss 0.026273541152477264 Validation loss 0.028093505650758743 Accuracy 0.71484375\n",
      "Iteration 25470 Training loss 0.024210866540670395 Validation loss 0.02862194925546646 Accuracy 0.70947265625\n",
      "Iteration 25480 Training loss 0.026225047186017036 Validation loss 0.028035176917910576 Accuracy 0.71533203125\n",
      "Iteration 25490 Training loss 0.021102987229824066 Validation loss 0.028111249208450317 Accuracy 0.71484375\n",
      "Iteration 25500 Training loss 0.02480953373014927 Validation loss 0.028087271377444267 Accuracy 0.71484375\n",
      "Iteration 25510 Training loss 0.027592996135354042 Validation loss 0.02841409668326378 Accuracy 0.71142578125\n",
      "Iteration 25520 Training loss 0.02640576660633087 Validation loss 0.027930792421102524 Accuracy 0.7158203125\n",
      "Iteration 25530 Training loss 0.02300640381872654 Validation loss 0.028239566832780838 Accuracy 0.71337890625\n",
      "Iteration 25540 Training loss 0.02516718953847885 Validation loss 0.028197703883051872 Accuracy 0.71435546875\n",
      "Iteration 25550 Training loss 0.026050914078950882 Validation loss 0.028200434520840645 Accuracy 0.7138671875\n",
      "Iteration 25560 Training loss 0.02397836185991764 Validation loss 0.028209161013364792 Accuracy 0.7138671875\n",
      "Iteration 25570 Training loss 0.029112204909324646 Validation loss 0.028035663068294525 Accuracy 0.71533203125\n",
      "Iteration 25580 Training loss 0.02804018184542656 Validation loss 0.028274178504943848 Accuracy 0.7138671875\n",
      "Iteration 25590 Training loss 0.025347361341118813 Validation loss 0.028029974550008774 Accuracy 0.71533203125\n",
      "Iteration 25600 Training loss 0.026348715648055077 Validation loss 0.028238652274012566 Accuracy 0.71337890625\n",
      "Iteration 25610 Training loss 0.027407284826040268 Validation loss 0.028210869058966637 Accuracy 0.71435546875\n",
      "Iteration 25620 Training loss 0.02554798126220703 Validation loss 0.028149940073490143 Accuracy 0.71484375\n",
      "Iteration 25630 Training loss 0.02576131373643875 Validation loss 0.028044475242495537 Accuracy 0.71533203125\n",
      "Iteration 25640 Training loss 0.02665691263973713 Validation loss 0.02827981859445572 Accuracy 0.71240234375\n",
      "Iteration 25650 Training loss 0.023951726034283638 Validation loss 0.028046676889061928 Accuracy 0.71533203125\n",
      "Iteration 25660 Training loss 0.02625773660838604 Validation loss 0.028528695926070213 Accuracy 0.7109375\n",
      "Iteration 25670 Training loss 0.025805745273828506 Validation loss 0.028001923114061356 Accuracy 0.7158203125\n",
      "Iteration 25680 Training loss 0.02524661272764206 Validation loss 0.02797054685652256 Accuracy 0.71533203125\n",
      "Iteration 25690 Training loss 0.023629918694496155 Validation loss 0.02812972292304039 Accuracy 0.71435546875\n",
      "Iteration 25700 Training loss 0.023557225242257118 Validation loss 0.028098655864596367 Accuracy 0.71533203125\n",
      "Iteration 25710 Training loss 0.027138033881783485 Validation loss 0.029001867398619652 Accuracy 0.70458984375\n",
      "Iteration 25720 Training loss 0.0242389477789402 Validation loss 0.02803480066359043 Accuracy 0.7158203125\n",
      "Iteration 25730 Training loss 0.024162862449884415 Validation loss 0.027989676222205162 Accuracy 0.71630859375\n",
      "Iteration 25740 Training loss 0.0250325258821249 Validation loss 0.028070203959941864 Accuracy 0.71484375\n",
      "Iteration 25750 Training loss 0.02688358537852764 Validation loss 0.028093257918953896 Accuracy 0.71484375\n",
      "Iteration 25760 Training loss 0.024613207206130028 Validation loss 0.027949996292591095 Accuracy 0.71630859375\n",
      "Iteration 25770 Training loss 0.028419239446520805 Validation loss 0.02965926192700863 Accuracy 0.69873046875\n",
      "Iteration 25780 Training loss 0.02365589328110218 Validation loss 0.028065524995326996 Accuracy 0.71533203125\n",
      "Iteration 25790 Training loss 0.0235223937779665 Validation loss 0.02798379771411419 Accuracy 0.71630859375\n",
      "Iteration 25800 Training loss 0.02918790653347969 Validation loss 0.028369689360260963 Accuracy 0.712890625\n",
      "Iteration 25810 Training loss 0.025164557620882988 Validation loss 0.028228558599948883 Accuracy 0.71337890625\n",
      "Iteration 25820 Training loss 0.026845192536711693 Validation loss 0.02814543806016445 Accuracy 0.71484375\n",
      "Iteration 25830 Training loss 0.024092882871627808 Validation loss 0.028147436678409576 Accuracy 0.7138671875\n",
      "Iteration 25840 Training loss 0.022142335772514343 Validation loss 0.028072793036699295 Accuracy 0.71533203125\n",
      "Iteration 25850 Training loss 0.024890044704079628 Validation loss 0.02817864716053009 Accuracy 0.7138671875\n",
      "Iteration 25860 Training loss 0.025436630472540855 Validation loss 0.02797926776111126 Accuracy 0.71630859375\n",
      "Iteration 25870 Training loss 0.024989720433950424 Validation loss 0.028984373435378075 Accuracy 0.70654296875\n",
      "Iteration 25880 Training loss 0.0264295507222414 Validation loss 0.02792331390082836 Accuracy 0.71728515625\n",
      "Iteration 25890 Training loss 0.025422848761081696 Validation loss 0.02808309905230999 Accuracy 0.71533203125\n",
      "Iteration 25900 Training loss 0.02514844574034214 Validation loss 0.028095543384552002 Accuracy 0.71484375\n",
      "Iteration 25910 Training loss 0.02587517350912094 Validation loss 0.028276005759835243 Accuracy 0.712890625\n",
      "Iteration 25920 Training loss 0.02441578358411789 Validation loss 0.028105003759264946 Accuracy 0.71435546875\n",
      "Iteration 25930 Training loss 0.023821400478482246 Validation loss 0.028031738474965096 Accuracy 0.7158203125\n",
      "Iteration 25940 Training loss 0.026378408074378967 Validation loss 0.028140699490904808 Accuracy 0.71484375\n",
      "Iteration 25950 Training loss 0.02388378605246544 Validation loss 0.028041038662195206 Accuracy 0.7158203125\n",
      "Iteration 25960 Training loss 0.02255318872630596 Validation loss 0.028162643313407898 Accuracy 0.7138671875\n",
      "Iteration 25970 Training loss 0.02511482499539852 Validation loss 0.02833571471273899 Accuracy 0.7119140625\n",
      "Iteration 25980 Training loss 0.02403400093317032 Validation loss 0.02829909510910511 Accuracy 0.712890625\n",
      "Iteration 25990 Training loss 0.02697635069489479 Validation loss 0.028678832575678825 Accuracy 0.708984375\n",
      "Iteration 26000 Training loss 0.02587384544312954 Validation loss 0.028709961101412773 Accuracy 0.708984375\n",
      "Iteration 26010 Training loss 0.025345703586935997 Validation loss 0.028908878564834595 Accuracy 0.70556640625\n",
      "Iteration 26020 Training loss 0.025981152430176735 Validation loss 0.028422094881534576 Accuracy 0.71142578125\n",
      "Iteration 26030 Training loss 0.02130335383117199 Validation loss 0.028321225196123123 Accuracy 0.712890625\n",
      "Iteration 26040 Training loss 0.025395838543772697 Validation loss 0.028272254392504692 Accuracy 0.712890625\n",
      "Iteration 26050 Training loss 0.025540979579091072 Validation loss 0.028572818264365196 Accuracy 0.71044921875\n",
      "Iteration 26060 Training loss 0.024649079889059067 Validation loss 0.028319403529167175 Accuracy 0.71240234375\n",
      "Iteration 26070 Training loss 0.028379501774907112 Validation loss 0.02837936021387577 Accuracy 0.7119140625\n",
      "Iteration 26080 Training loss 0.021761301904916763 Validation loss 0.02819935977458954 Accuracy 0.71337890625\n",
      "Iteration 26090 Training loss 0.024040287360548973 Validation loss 0.02791636250913143 Accuracy 0.716796875\n",
      "Iteration 26100 Training loss 0.028187265619635582 Validation loss 0.028616713359951973 Accuracy 0.71044921875\n",
      "Iteration 26110 Training loss 0.024106815457344055 Validation loss 0.02845941297709942 Accuracy 0.71142578125\n",
      "Iteration 26120 Training loss 0.024077270179986954 Validation loss 0.028706751763820648 Accuracy 0.70849609375\n",
      "Iteration 26130 Training loss 0.025209622457623482 Validation loss 0.028446512296795845 Accuracy 0.71142578125\n",
      "Iteration 26140 Training loss 0.023281890898942947 Validation loss 0.02841271460056305 Accuracy 0.7119140625\n",
      "Iteration 26150 Training loss 0.028215164318680763 Validation loss 0.02802702598273754 Accuracy 0.71533203125\n",
      "Iteration 26160 Training loss 0.02406836487352848 Validation loss 0.028162069618701935 Accuracy 0.7138671875\n",
      "Iteration 26170 Training loss 0.02121763303875923 Validation loss 0.028160836547613144 Accuracy 0.71337890625\n",
      "Iteration 26180 Training loss 0.026475295424461365 Validation loss 0.028439566493034363 Accuracy 0.71142578125\n",
      "Iteration 26190 Training loss 0.024830522015690804 Validation loss 0.028286591172218323 Accuracy 0.712890625\n",
      "Iteration 26200 Training loss 0.024448957294225693 Validation loss 0.02834189310669899 Accuracy 0.712890625\n",
      "Iteration 26210 Training loss 0.02572089061141014 Validation loss 0.028712011873722076 Accuracy 0.708984375\n",
      "Iteration 26220 Training loss 0.025720393285155296 Validation loss 0.02812829427421093 Accuracy 0.71484375\n",
      "Iteration 26230 Training loss 0.025063766166567802 Validation loss 0.02806350775063038 Accuracy 0.71435546875\n",
      "Iteration 26240 Training loss 0.0260650385171175 Validation loss 0.028030214831233025 Accuracy 0.71533203125\n",
      "Iteration 26250 Training loss 0.02386736497282982 Validation loss 0.028250204399228096 Accuracy 0.71337890625\n",
      "Iteration 26260 Training loss 0.023431340232491493 Validation loss 0.028172362595796585 Accuracy 0.71337890625\n",
      "Iteration 26270 Training loss 0.025373145937919617 Validation loss 0.028394730761647224 Accuracy 0.71142578125\n",
      "Iteration 26280 Training loss 0.02628461644053459 Validation loss 0.02870137058198452 Accuracy 0.70947265625\n",
      "Iteration 26290 Training loss 0.026717528700828552 Validation loss 0.028343863785266876 Accuracy 0.7119140625\n",
      "Iteration 26300 Training loss 0.026441503316164017 Validation loss 0.02820688486099243 Accuracy 0.7138671875\n",
      "Iteration 26310 Training loss 0.028340749442577362 Validation loss 0.028038538992404938 Accuracy 0.7158203125\n",
      "Iteration 26320 Training loss 0.02444901503622532 Validation loss 0.028220050036907196 Accuracy 0.71435546875\n",
      "Iteration 26330 Training loss 0.02607126161456108 Validation loss 0.02804332971572876 Accuracy 0.71484375\n",
      "Iteration 26340 Training loss 0.020190851762890816 Validation loss 0.028088541701436043 Accuracy 0.71533203125\n",
      "Iteration 26350 Training loss 0.023658985272049904 Validation loss 0.02785320207476616 Accuracy 0.71728515625\n",
      "Iteration 26360 Training loss 0.02566339075565338 Validation loss 0.027828915044665337 Accuracy 0.71728515625\n",
      "Iteration 26370 Training loss 0.02265162393450737 Validation loss 0.027958644554018974 Accuracy 0.7158203125\n",
      "Iteration 26380 Training loss 0.025769449770450592 Validation loss 0.028366463258862495 Accuracy 0.7119140625\n",
      "Iteration 26390 Training loss 0.025098780170083046 Validation loss 0.028095481917262077 Accuracy 0.71435546875\n",
      "Iteration 26400 Training loss 0.02324485220015049 Validation loss 0.028656840324401855 Accuracy 0.708984375\n",
      "Iteration 26410 Training loss 0.0257089976221323 Validation loss 0.02785000391304493 Accuracy 0.716796875\n",
      "Iteration 26420 Training loss 0.02612944506108761 Validation loss 0.027971353381872177 Accuracy 0.71630859375\n",
      "Iteration 26430 Training loss 0.025586679577827454 Validation loss 0.028044909238815308 Accuracy 0.71533203125\n",
      "Iteration 26440 Training loss 0.023736149072647095 Validation loss 0.028272109106183052 Accuracy 0.71240234375\n",
      "Iteration 26450 Training loss 0.024875495582818985 Validation loss 0.027929266914725304 Accuracy 0.71630859375\n",
      "Iteration 26460 Training loss 0.02684185281395912 Validation loss 0.02794204093515873 Accuracy 0.71630859375\n",
      "Iteration 26470 Training loss 0.02684197761118412 Validation loss 0.027939550578594208 Accuracy 0.7158203125\n",
      "Iteration 26480 Training loss 0.024193555116653442 Validation loss 0.028071483597159386 Accuracy 0.71484375\n",
      "Iteration 26490 Training loss 0.026503708213567734 Validation loss 0.027927963063120842 Accuracy 0.7158203125\n",
      "Iteration 26500 Training loss 0.02696913108229637 Validation loss 0.027718709781765938 Accuracy 0.71875\n",
      "Iteration 26510 Training loss 0.024472147226333618 Validation loss 0.027820885181427002 Accuracy 0.7177734375\n",
      "Iteration 26520 Training loss 0.026231734082102776 Validation loss 0.028188901022076607 Accuracy 0.7138671875\n",
      "Iteration 26530 Training loss 0.024920053780078888 Validation loss 0.028275545686483383 Accuracy 0.712890625\n",
      "Iteration 26540 Training loss 0.025927135720849037 Validation loss 0.028006581589579582 Accuracy 0.7158203125\n",
      "Iteration 26550 Training loss 0.023774392902851105 Validation loss 0.027961859479546547 Accuracy 0.71484375\n",
      "Iteration 26560 Training loss 0.02699393220245838 Validation loss 0.02816230244934559 Accuracy 0.7138671875\n",
      "Iteration 26570 Training loss 0.02652493678033352 Validation loss 0.0279274620115757 Accuracy 0.716796875\n",
      "Iteration 26580 Training loss 0.02498212829232216 Validation loss 0.028246352449059486 Accuracy 0.712890625\n",
      "Iteration 26590 Training loss 0.029089616611599922 Validation loss 0.028594374656677246 Accuracy 0.70947265625\n",
      "Iteration 26600 Training loss 0.025745509192347527 Validation loss 0.028057323768734932 Accuracy 0.71484375\n",
      "Iteration 26610 Training loss 0.027350855991244316 Validation loss 0.02908601425588131 Accuracy 0.7041015625\n",
      "Iteration 26620 Training loss 0.023625899106264114 Validation loss 0.0281432643532753 Accuracy 0.71435546875\n",
      "Iteration 26630 Training loss 0.022014863789081573 Validation loss 0.027935558930039406 Accuracy 0.7158203125\n",
      "Iteration 26640 Training loss 0.023432347923517227 Validation loss 0.02767479419708252 Accuracy 0.71923828125\n",
      "Iteration 26650 Training loss 0.02648981660604477 Validation loss 0.028224525973200798 Accuracy 0.712890625\n",
      "Iteration 26660 Training loss 0.024826807901263237 Validation loss 0.02785860188305378 Accuracy 0.71728515625\n",
      "Iteration 26670 Training loss 0.025436347350478172 Validation loss 0.02812281623482704 Accuracy 0.71435546875\n",
      "Iteration 26680 Training loss 0.02423093654215336 Validation loss 0.02806001715362072 Accuracy 0.71484375\n",
      "Iteration 26690 Training loss 0.025909891352057457 Validation loss 0.027997655794024467 Accuracy 0.71630859375\n",
      "Iteration 26700 Training loss 0.02294170670211315 Validation loss 0.028024423867464066 Accuracy 0.7158203125\n",
      "Iteration 26710 Training loss 0.026677541434764862 Validation loss 0.028148693963885307 Accuracy 0.71484375\n",
      "Iteration 26720 Training loss 0.023561395704746246 Validation loss 0.028087111189961433 Accuracy 0.71484375\n",
      "Iteration 26730 Training loss 0.025245798751711845 Validation loss 0.027825748547911644 Accuracy 0.716796875\n",
      "Iteration 26740 Training loss 0.024353865534067154 Validation loss 0.027954496443271637 Accuracy 0.71630859375\n",
      "Iteration 26750 Training loss 0.022691581398248672 Validation loss 0.028224872425198555 Accuracy 0.712890625\n",
      "Iteration 26760 Training loss 0.02581166848540306 Validation loss 0.028357619419693947 Accuracy 0.7119140625\n",
      "Iteration 26770 Training loss 0.02505483478307724 Validation loss 0.028212660923600197 Accuracy 0.712890625\n",
      "Iteration 26780 Training loss 0.026726340875029564 Validation loss 0.028260016813874245 Accuracy 0.712890625\n",
      "Iteration 26790 Training loss 0.02367611601948738 Validation loss 0.02778775803744793 Accuracy 0.7177734375\n",
      "Iteration 26800 Training loss 0.02593735232949257 Validation loss 0.028021613135933876 Accuracy 0.7158203125\n",
      "Iteration 26810 Training loss 0.024230245500802994 Validation loss 0.02810087241232395 Accuracy 0.71435546875\n",
      "Iteration 26820 Training loss 0.028344688937067986 Validation loss 0.027994032949209213 Accuracy 0.7158203125\n",
      "Iteration 26830 Training loss 0.02884926274418831 Validation loss 0.028094487264752388 Accuracy 0.71484375\n",
      "Iteration 26840 Training loss 0.026651892811059952 Validation loss 0.02860156260430813 Accuracy 0.7099609375\n",
      "Iteration 26850 Training loss 0.024218617007136345 Validation loss 0.029146412387490273 Accuracy 0.70458984375\n",
      "Iteration 26860 Training loss 0.026082493364810944 Validation loss 0.02797311544418335 Accuracy 0.71533203125\n",
      "Iteration 26870 Training loss 0.026639332994818687 Validation loss 0.028212547302246094 Accuracy 0.71484375\n",
      "Iteration 26880 Training loss 0.02723882347345352 Validation loss 0.028408363461494446 Accuracy 0.7119140625\n",
      "Iteration 26890 Training loss 0.02516951970756054 Validation loss 0.02788764052093029 Accuracy 0.71630859375\n",
      "Iteration 26900 Training loss 0.024826640263199806 Validation loss 0.028147777542471886 Accuracy 0.71337890625\n",
      "Iteration 26910 Training loss 0.02330428548157215 Validation loss 0.028210926800966263 Accuracy 0.71337890625\n",
      "Iteration 26920 Training loss 0.027223924174904823 Validation loss 0.027980126440525055 Accuracy 0.7158203125\n",
      "Iteration 26930 Training loss 0.0254773311316967 Validation loss 0.02796040289103985 Accuracy 0.71533203125\n",
      "Iteration 26940 Training loss 0.02528649941086769 Validation loss 0.027861805632710457 Accuracy 0.716796875\n",
      "Iteration 26950 Training loss 0.026124466210603714 Validation loss 0.028246985748410225 Accuracy 0.712890625\n",
      "Iteration 26960 Training loss 0.02743692696094513 Validation loss 0.027889959514141083 Accuracy 0.71728515625\n",
      "Iteration 26970 Training loss 0.028777727857232094 Validation loss 0.027817200869321823 Accuracy 0.716796875\n",
      "Iteration 26980 Training loss 0.026794537901878357 Validation loss 0.028031092137098312 Accuracy 0.7158203125\n",
      "Iteration 26990 Training loss 0.022382237017154694 Validation loss 0.028903817757964134 Accuracy 0.70703125\n",
      "Iteration 27000 Training loss 0.027989832684397697 Validation loss 0.0279498603194952 Accuracy 0.7158203125\n",
      "Iteration 27010 Training loss 0.023817051202058792 Validation loss 0.028195444494485855 Accuracy 0.7138671875\n",
      "Iteration 27020 Training loss 0.024413345381617546 Validation loss 0.02818644978106022 Accuracy 0.7138671875\n",
      "Iteration 27030 Training loss 0.024435432627797127 Validation loss 0.02813154272735119 Accuracy 0.7138671875\n",
      "Iteration 27040 Training loss 0.024010200053453445 Validation loss 0.028219804167747498 Accuracy 0.71435546875\n",
      "Iteration 27050 Training loss 0.024484898895025253 Validation loss 0.02812887355685234 Accuracy 0.71337890625\n",
      "Iteration 27060 Training loss 0.027051106095314026 Validation loss 0.028149088844656944 Accuracy 0.71337890625\n",
      "Iteration 27070 Training loss 0.02449055388569832 Validation loss 0.028198324143886566 Accuracy 0.7138671875\n",
      "Iteration 27080 Training loss 0.023478027433156967 Validation loss 0.02806328609585762 Accuracy 0.71484375\n",
      "Iteration 27090 Training loss 0.02684548683464527 Validation loss 0.027719609439373016 Accuracy 0.7177734375\n",
      "Iteration 27100 Training loss 0.026454037055373192 Validation loss 0.028445227071642876 Accuracy 0.7109375\n",
      "Iteration 27110 Training loss 0.025168100371956825 Validation loss 0.028354531154036522 Accuracy 0.7119140625\n",
      "Iteration 27120 Training loss 0.02242824248969555 Validation loss 0.028264785185456276 Accuracy 0.7138671875\n",
      "Iteration 27130 Training loss 0.02400183118879795 Validation loss 0.02865397371351719 Accuracy 0.708984375\n",
      "Iteration 27140 Training loss 0.023749032989144325 Validation loss 0.028248757123947144 Accuracy 0.712890625\n",
      "Iteration 27150 Training loss 0.023927144706249237 Validation loss 0.027870047837495804 Accuracy 0.71728515625\n",
      "Iteration 27160 Training loss 0.027899082750082016 Validation loss 0.02813710831105709 Accuracy 0.71484375\n",
      "Iteration 27170 Training loss 0.025227896869182587 Validation loss 0.028177309781312943 Accuracy 0.7138671875\n",
      "Iteration 27180 Training loss 0.02294890396296978 Validation loss 0.027892792597413063 Accuracy 0.71630859375\n",
      "Iteration 27190 Training loss 0.02524438500404358 Validation loss 0.02853354439139366 Accuracy 0.7099609375\n",
      "Iteration 27200 Training loss 0.023961829021573067 Validation loss 0.028053294867277145 Accuracy 0.7158203125\n",
      "Iteration 27210 Training loss 0.02347130887210369 Validation loss 0.028185438364744186 Accuracy 0.7138671875\n",
      "Iteration 27220 Training loss 0.021646423265337944 Validation loss 0.028054647147655487 Accuracy 0.71484375\n",
      "Iteration 27230 Training loss 0.023963604122400284 Validation loss 0.02801046147942543 Accuracy 0.71533203125\n",
      "Iteration 27240 Training loss 0.025148168206214905 Validation loss 0.028203627094626427 Accuracy 0.7138671875\n",
      "Iteration 27250 Training loss 0.02447417937219143 Validation loss 0.02784041501581669 Accuracy 0.7177734375\n",
      "Iteration 27260 Training loss 0.021228089928627014 Validation loss 0.028041517361998558 Accuracy 0.7158203125\n",
      "Iteration 27270 Training loss 0.025709034875035286 Validation loss 0.029007818549871445 Accuracy 0.70556640625\n",
      "Iteration 27280 Training loss 0.026317933574318886 Validation loss 0.02822951041162014 Accuracy 0.71435546875\n",
      "Iteration 27290 Training loss 0.022529004141688347 Validation loss 0.028057606890797615 Accuracy 0.71484375\n",
      "Iteration 27300 Training loss 0.022602513432502747 Validation loss 0.027921922504901886 Accuracy 0.716796875\n",
      "Iteration 27310 Training loss 0.026192540302872658 Validation loss 0.028400184586644173 Accuracy 0.71142578125\n",
      "Iteration 27320 Training loss 0.02663562074303627 Validation loss 0.02836543880403042 Accuracy 0.71142578125\n",
      "Iteration 27330 Training loss 0.024100102484226227 Validation loss 0.028206676244735718 Accuracy 0.71337890625\n",
      "Iteration 27340 Training loss 0.023137537762522697 Validation loss 0.027878742665052414 Accuracy 0.71728515625\n",
      "Iteration 27350 Training loss 0.0251561738550663 Validation loss 0.028160881251096725 Accuracy 0.7138671875\n",
      "Iteration 27360 Training loss 0.0265857744961977 Validation loss 0.02808702364563942 Accuracy 0.71484375\n",
      "Iteration 27370 Training loss 0.025535639375448227 Validation loss 0.02811822108924389 Accuracy 0.71435546875\n",
      "Iteration 27380 Training loss 0.0268215611577034 Validation loss 0.02816774882376194 Accuracy 0.71484375\n",
      "Iteration 27390 Training loss 0.024531232193112373 Validation loss 0.028073681518435478 Accuracy 0.71484375\n",
      "Iteration 27400 Training loss 0.02651786059141159 Validation loss 0.027877245098352432 Accuracy 0.71630859375\n",
      "Iteration 27410 Training loss 0.028389185667037964 Validation loss 0.028178485110402107 Accuracy 0.7138671875\n",
      "Iteration 27420 Training loss 0.02703581191599369 Validation loss 0.02786318212747574 Accuracy 0.71533203125\n",
      "Iteration 27430 Training loss 0.025495899841189384 Validation loss 0.027972154319286346 Accuracy 0.7158203125\n",
      "Iteration 27440 Training loss 0.02354385331273079 Validation loss 0.027959102764725685 Accuracy 0.716796875\n",
      "Iteration 27450 Training loss 0.02647220902144909 Validation loss 0.02830609865486622 Accuracy 0.71337890625\n",
      "Iteration 27460 Training loss 0.023771468549966812 Validation loss 0.028029832988977432 Accuracy 0.7158203125\n",
      "Iteration 27470 Training loss 0.025240279734134674 Validation loss 0.028324423357844353 Accuracy 0.712890625\n",
      "Iteration 27480 Training loss 0.026621444150805473 Validation loss 0.027909496799111366 Accuracy 0.716796875\n",
      "Iteration 27490 Training loss 0.02310837246477604 Validation loss 0.027839822694659233 Accuracy 0.716796875\n",
      "Iteration 27500 Training loss 0.025539455935359 Validation loss 0.027921443805098534 Accuracy 0.716796875\n",
      "Iteration 27510 Training loss 0.026258641853928566 Validation loss 0.028116488829255104 Accuracy 0.71533203125\n",
      "Iteration 27520 Training loss 0.022853942587971687 Validation loss 0.02798987179994583 Accuracy 0.7158203125\n",
      "Iteration 27530 Training loss 0.024489227682352066 Validation loss 0.02805420383810997 Accuracy 0.71533203125\n",
      "Iteration 27540 Training loss 0.025241516530513763 Validation loss 0.027844024822115898 Accuracy 0.7177734375\n",
      "Iteration 27550 Training loss 0.025516927242279053 Validation loss 0.028027577325701714 Accuracy 0.71630859375\n",
      "Iteration 27560 Training loss 0.024345438927412033 Validation loss 0.027952782809734344 Accuracy 0.71630859375\n",
      "Iteration 27570 Training loss 0.02697913348674774 Validation loss 0.02837555669248104 Accuracy 0.7119140625\n",
      "Iteration 27580 Training loss 0.025468667969107628 Validation loss 0.02869619056582451 Accuracy 0.708984375\n",
      "Iteration 27590 Training loss 0.023764682933688164 Validation loss 0.02810210920870304 Accuracy 0.71484375\n",
      "Iteration 27600 Training loss 0.02555346116423607 Validation loss 0.02826550044119358 Accuracy 0.71337890625\n",
      "Iteration 27610 Training loss 0.026405934244394302 Validation loss 0.028337866067886353 Accuracy 0.71240234375\n",
      "Iteration 27620 Training loss 0.024228375405073166 Validation loss 0.028313877061009407 Accuracy 0.712890625\n",
      "Iteration 27630 Training loss 0.027310989797115326 Validation loss 0.028675032779574394 Accuracy 0.70849609375\n",
      "Iteration 27640 Training loss 0.026030197739601135 Validation loss 0.028091149404644966 Accuracy 0.71484375\n",
      "Iteration 27650 Training loss 0.026241054758429527 Validation loss 0.02880541980266571 Accuracy 0.70751953125\n",
      "Iteration 27660 Training loss 0.02485409937798977 Validation loss 0.027799613773822784 Accuracy 0.7177734375\n",
      "Iteration 27670 Training loss 0.02358071692287922 Validation loss 0.028067735955119133 Accuracy 0.71533203125\n",
      "Iteration 27680 Training loss 0.023934997618198395 Validation loss 0.028380438685417175 Accuracy 0.7119140625\n",
      "Iteration 27690 Training loss 0.02749762497842312 Validation loss 0.027903618291020393 Accuracy 0.716796875\n",
      "Iteration 27700 Training loss 0.02373228408396244 Validation loss 0.028112240135669708 Accuracy 0.71484375\n",
      "Iteration 27710 Training loss 0.026070471853017807 Validation loss 0.02812708355486393 Accuracy 0.71484375\n",
      "Iteration 27720 Training loss 0.02721388265490532 Validation loss 0.028618836775422096 Accuracy 0.71044921875\n",
      "Iteration 27730 Training loss 0.02235409989953041 Validation loss 0.027990397065877914 Accuracy 0.7158203125\n",
      "Iteration 27740 Training loss 0.026786375790834427 Validation loss 0.0279182568192482 Accuracy 0.7158203125\n",
      "Iteration 27750 Training loss 0.026066070422530174 Validation loss 0.027798915281891823 Accuracy 0.716796875\n",
      "Iteration 27760 Training loss 0.02530168928205967 Validation loss 0.02806057780981064 Accuracy 0.71435546875\n",
      "Iteration 27770 Training loss 0.02544897422194481 Validation loss 0.028337806463241577 Accuracy 0.71240234375\n",
      "Iteration 27780 Training loss 0.02162684127688408 Validation loss 0.028064779937267303 Accuracy 0.71484375\n",
      "Iteration 27790 Training loss 0.023938333615660667 Validation loss 0.028062094002962112 Accuracy 0.71484375\n",
      "Iteration 27800 Training loss 0.02690093405544758 Validation loss 0.02798415534198284 Accuracy 0.71484375\n",
      "Iteration 27810 Training loss 0.025678440928459167 Validation loss 0.028364231809973717 Accuracy 0.7119140625\n",
      "Iteration 27820 Training loss 0.02450467459857464 Validation loss 0.028063656762242317 Accuracy 0.71484375\n",
      "Iteration 27830 Training loss 0.02563134953379631 Validation loss 0.027861732989549637 Accuracy 0.71630859375\n",
      "Iteration 27840 Training loss 0.022233624011278152 Validation loss 0.027963846921920776 Accuracy 0.71533203125\n",
      "Iteration 27850 Training loss 0.025851452723145485 Validation loss 0.028102552518248558 Accuracy 0.71533203125\n",
      "Iteration 27860 Training loss 0.024802787229418755 Validation loss 0.02774708904325962 Accuracy 0.7177734375\n",
      "Iteration 27870 Training loss 0.02741164155304432 Validation loss 0.028978247195482254 Accuracy 0.7060546875\n",
      "Iteration 27880 Training loss 0.027686629444360733 Validation loss 0.028065087273716927 Accuracy 0.7158203125\n",
      "Iteration 27890 Training loss 0.02044638991355896 Validation loss 0.02783730812370777 Accuracy 0.7177734375\n",
      "Iteration 27900 Training loss 0.02532261796295643 Validation loss 0.027768906205892563 Accuracy 0.7177734375\n",
      "Iteration 27910 Training loss 0.02429145760834217 Validation loss 0.028064511716365814 Accuracy 0.7158203125\n",
      "Iteration 27920 Training loss 0.02488669753074646 Validation loss 0.02855733409523964 Accuracy 0.7109375\n",
      "Iteration 27930 Training loss 0.023203259333968163 Validation loss 0.02823161520063877 Accuracy 0.71435546875\n",
      "Iteration 27940 Training loss 0.024279993027448654 Validation loss 0.02794700115919113 Accuracy 0.716796875\n",
      "Iteration 27950 Training loss 0.023176752030849457 Validation loss 0.027826225385069847 Accuracy 0.7177734375\n",
      "Iteration 27960 Training loss 0.023549994453787804 Validation loss 0.027918189764022827 Accuracy 0.716796875\n",
      "Iteration 27970 Training loss 0.021559519693255424 Validation loss 0.027919694781303406 Accuracy 0.71630859375\n",
      "Iteration 27980 Training loss 0.023066718131303787 Validation loss 0.02773435227572918 Accuracy 0.7177734375\n",
      "Iteration 27990 Training loss 0.02219393476843834 Validation loss 0.02813897840678692 Accuracy 0.71484375\n",
      "Iteration 28000 Training loss 0.023517746478319168 Validation loss 0.027838679030537605 Accuracy 0.716796875\n",
      "Iteration 28010 Training loss 0.02339562401175499 Validation loss 0.02809036150574684 Accuracy 0.71435546875\n",
      "Iteration 28020 Training loss 0.024426890537142754 Validation loss 0.027931571006774902 Accuracy 0.71630859375\n",
      "Iteration 28030 Training loss 0.026178359985351562 Validation loss 0.027963943779468536 Accuracy 0.7158203125\n",
      "Iteration 28040 Training loss 0.024359455332159996 Validation loss 0.028313342481851578 Accuracy 0.71240234375\n",
      "Iteration 28050 Training loss 0.027766799554228783 Validation loss 0.028325416147708893 Accuracy 0.7119140625\n",
      "Iteration 28060 Training loss 0.024257877841591835 Validation loss 0.027791550382971764 Accuracy 0.71728515625\n",
      "Iteration 28070 Training loss 0.02838595025241375 Validation loss 0.02888084203004837 Accuracy 0.7060546875\n",
      "Iteration 28080 Training loss 0.024460021406412125 Validation loss 0.02814830094575882 Accuracy 0.712890625\n",
      "Iteration 28090 Training loss 0.024283014237880707 Validation loss 0.028326118364930153 Accuracy 0.7119140625\n",
      "Iteration 28100 Training loss 0.023491520434617996 Validation loss 0.02795545756816864 Accuracy 0.71533203125\n",
      "Iteration 28110 Training loss 0.02330492064356804 Validation loss 0.028100576251745224 Accuracy 0.71337890625\n",
      "Iteration 28120 Training loss 0.025930093601346016 Validation loss 0.027998339384794235 Accuracy 0.71533203125\n",
      "Iteration 28130 Training loss 0.02670501172542572 Validation loss 0.027963027358055115 Accuracy 0.7158203125\n",
      "Iteration 28140 Training loss 0.024278590455651283 Validation loss 0.028186090290546417 Accuracy 0.71337890625\n",
      "Iteration 28150 Training loss 0.026080939918756485 Validation loss 0.028186170384287834 Accuracy 0.71337890625\n",
      "Iteration 28160 Training loss 0.02503833919763565 Validation loss 0.028144555166363716 Accuracy 0.7138671875\n",
      "Iteration 28170 Training loss 0.024691561236977577 Validation loss 0.028334403410553932 Accuracy 0.71240234375\n",
      "Iteration 28180 Training loss 0.02606421709060669 Validation loss 0.028498727828264236 Accuracy 0.70947265625\n",
      "Iteration 28190 Training loss 0.02776767499744892 Validation loss 0.028108300641179085 Accuracy 0.71435546875\n",
      "Iteration 28200 Training loss 0.02322554588317871 Validation loss 0.027963565662503242 Accuracy 0.71630859375\n",
      "Iteration 28210 Training loss 0.02394605055451393 Validation loss 0.028526410460472107 Accuracy 0.71044921875\n",
      "Iteration 28220 Training loss 0.023103442043066025 Validation loss 0.028052514418959618 Accuracy 0.71484375\n",
      "Iteration 28230 Training loss 0.023052334785461426 Validation loss 0.027848344296216965 Accuracy 0.716796875\n",
      "Iteration 28240 Training loss 0.025385288521647453 Validation loss 0.027963856235146523 Accuracy 0.7158203125\n",
      "Iteration 28250 Training loss 0.02684582769870758 Validation loss 0.028277546167373657 Accuracy 0.71240234375\n",
      "Iteration 28260 Training loss 0.029022447764873505 Validation loss 0.027910033240914345 Accuracy 0.71630859375\n",
      "Iteration 28270 Training loss 0.025121772661805153 Validation loss 0.02821696549654007 Accuracy 0.71435546875\n",
      "Iteration 28280 Training loss 0.02279670163989067 Validation loss 0.027839822694659233 Accuracy 0.71728515625\n",
      "Iteration 28290 Training loss 0.021218743175268173 Validation loss 0.028391527011990547 Accuracy 0.7119140625\n",
      "Iteration 28300 Training loss 0.023863758891820908 Validation loss 0.028521552681922913 Accuracy 0.71044921875\n",
      "Iteration 28310 Training loss 0.026636172086000443 Validation loss 0.02831418439745903 Accuracy 0.7119140625\n",
      "Iteration 28320 Training loss 0.02227943204343319 Validation loss 0.028046000748872757 Accuracy 0.71533203125\n",
      "Iteration 28330 Training loss 0.024012405425310135 Validation loss 0.02784619852900505 Accuracy 0.716796875\n",
      "Iteration 28340 Training loss 0.022391974925994873 Validation loss 0.02786412090063095 Accuracy 0.716796875\n",
      "Iteration 28350 Training loss 0.025096949189901352 Validation loss 0.02806188352406025 Accuracy 0.7158203125\n",
      "Iteration 28360 Training loss 0.022745050489902496 Validation loss 0.02784961275756359 Accuracy 0.71728515625\n",
      "Iteration 28370 Training loss 0.0236553642898798 Validation loss 0.027927562594413757 Accuracy 0.7158203125\n",
      "Iteration 28380 Training loss 0.02353745512664318 Validation loss 0.02819584123790264 Accuracy 0.71337890625\n",
      "Iteration 28390 Training loss 0.02465154230594635 Validation loss 0.028116146102547646 Accuracy 0.71484375\n",
      "Iteration 28400 Training loss 0.022736025974154472 Validation loss 0.02856479398906231 Accuracy 0.70947265625\n",
      "Iteration 28410 Training loss 0.024381304159760475 Validation loss 0.028170576319098473 Accuracy 0.7138671875\n",
      "Iteration 28420 Training loss 0.025900619104504585 Validation loss 0.02852955460548401 Accuracy 0.70947265625\n",
      "Iteration 28430 Training loss 0.026668626815080643 Validation loss 0.027891172096133232 Accuracy 0.71728515625\n",
      "Iteration 28440 Training loss 0.02372867241501808 Validation loss 0.028059126809239388 Accuracy 0.71484375\n",
      "Iteration 28450 Training loss 0.02730661816895008 Validation loss 0.028403108939528465 Accuracy 0.7119140625\n",
      "Iteration 28460 Training loss 0.02582014724612236 Validation loss 0.028109444305300713 Accuracy 0.71435546875\n",
      "Iteration 28470 Training loss 0.02405565418303013 Validation loss 0.027849404141306877 Accuracy 0.71728515625\n",
      "Iteration 28480 Training loss 0.0260861124843359 Validation loss 0.02775541879236698 Accuracy 0.7177734375\n",
      "Iteration 28490 Training loss 0.02459442801773548 Validation loss 0.0278155654668808 Accuracy 0.71728515625\n",
      "Iteration 28500 Training loss 0.025029459968209267 Validation loss 0.02806885726749897 Accuracy 0.71484375\n",
      "Iteration 28510 Training loss 0.02475939318537712 Validation loss 0.028108812868595123 Accuracy 0.71484375\n",
      "Iteration 28520 Training loss 0.025385688990354538 Validation loss 0.028320223093032837 Accuracy 0.7119140625\n",
      "Iteration 28530 Training loss 0.027923593297600746 Validation loss 0.02867000922560692 Accuracy 0.708984375\n",
      "Iteration 28540 Training loss 0.02667982503771782 Validation loss 0.028327127918601036 Accuracy 0.7119140625\n",
      "Iteration 28550 Training loss 0.023837599903345108 Validation loss 0.02784927375614643 Accuracy 0.716796875\n",
      "Iteration 28560 Training loss 0.025979572907090187 Validation loss 0.028184272348880768 Accuracy 0.712890625\n",
      "Iteration 28570 Training loss 0.024980418384075165 Validation loss 0.028050832450389862 Accuracy 0.7138671875\n",
      "Iteration 28580 Training loss 0.02625802345573902 Validation loss 0.027963368222117424 Accuracy 0.7158203125\n",
      "Iteration 28590 Training loss 0.025506509467959404 Validation loss 0.02777840755879879 Accuracy 0.71728515625\n",
      "Iteration 28600 Training loss 0.02243426814675331 Validation loss 0.02823277749121189 Accuracy 0.71337890625\n",
      "Iteration 28610 Training loss 0.02620796114206314 Validation loss 0.027873538434505463 Accuracy 0.71728515625\n",
      "Iteration 28620 Training loss 0.023176409304142 Validation loss 0.02822265774011612 Accuracy 0.71240234375\n",
      "Iteration 28630 Training loss 0.02416370064020157 Validation loss 0.02847307361662388 Accuracy 0.71142578125\n",
      "Iteration 28640 Training loss 0.026984022930264473 Validation loss 0.027908235788345337 Accuracy 0.7158203125\n",
      "Iteration 28650 Training loss 0.02724682353436947 Validation loss 0.02862359769642353 Accuracy 0.70947265625\n",
      "Iteration 28660 Training loss 0.025408655405044556 Validation loss 0.028096625581383705 Accuracy 0.71484375\n",
      "Iteration 28670 Training loss 0.022996604442596436 Validation loss 0.027983084321022034 Accuracy 0.7158203125\n",
      "Iteration 28680 Training loss 0.021968165412545204 Validation loss 0.02854909561574459 Accuracy 0.70947265625\n",
      "Iteration 28690 Training loss 0.024008439853787422 Validation loss 0.027824750170111656 Accuracy 0.71728515625\n",
      "Iteration 28700 Training loss 0.023954184725880623 Validation loss 0.027978720143437386 Accuracy 0.71630859375\n",
      "Iteration 28710 Training loss 0.02523713745176792 Validation loss 0.027796447277069092 Accuracy 0.71728515625\n",
      "Iteration 28720 Training loss 0.024566367268562317 Validation loss 0.027709759771823883 Accuracy 0.71923828125\n",
      "Iteration 28730 Training loss 0.02365395613014698 Validation loss 0.028537757694721222 Accuracy 0.71044921875\n",
      "Iteration 28740 Training loss 0.02720095030963421 Validation loss 0.027910199016332626 Accuracy 0.7158203125\n",
      "Iteration 28750 Training loss 0.02059157006442547 Validation loss 0.027747545391321182 Accuracy 0.7177734375\n",
      "Iteration 28760 Training loss 0.026815546676516533 Validation loss 0.027912931516766548 Accuracy 0.71630859375\n",
      "Iteration 28770 Training loss 0.023499498143792152 Validation loss 0.02819199115037918 Accuracy 0.712890625\n",
      "Iteration 28780 Training loss 0.02581251971423626 Validation loss 0.02820952795445919 Accuracy 0.71337890625\n",
      "Iteration 28790 Training loss 0.026585718616843224 Validation loss 0.02776525728404522 Accuracy 0.7177734375\n",
      "Iteration 28800 Training loss 0.023106297478079796 Validation loss 0.027818040922284126 Accuracy 0.71728515625\n",
      "Iteration 28810 Training loss 0.02517746388912201 Validation loss 0.02785608358681202 Accuracy 0.71728515625\n",
      "Iteration 28820 Training loss 0.024097729474306107 Validation loss 0.028189491480588913 Accuracy 0.71435546875\n",
      "Iteration 28830 Training loss 0.024301201105117798 Validation loss 0.028673024848103523 Accuracy 0.708984375\n",
      "Iteration 28840 Training loss 0.023655002936720848 Validation loss 0.027829276397824287 Accuracy 0.7177734375\n",
      "Iteration 28850 Training loss 0.029585514217615128 Validation loss 0.028941597789525986 Accuracy 0.7060546875\n",
      "Iteration 28860 Training loss 0.02242886833846569 Validation loss 0.02817654050886631 Accuracy 0.71435546875\n",
      "Iteration 28870 Training loss 0.02131064608693123 Validation loss 0.027681922540068626 Accuracy 0.71875\n",
      "Iteration 28880 Training loss 0.022678740322589874 Validation loss 0.027814188972115517 Accuracy 0.71728515625\n",
      "Iteration 28890 Training loss 0.02587811090052128 Validation loss 0.028104525059461594 Accuracy 0.71533203125\n",
      "Iteration 28900 Training loss 0.025889968499541283 Validation loss 0.02805504761636257 Accuracy 0.71533203125\n",
      "Iteration 28910 Training loss 0.02885935641825199 Validation loss 0.028415553271770477 Accuracy 0.71142578125\n",
      "Iteration 28920 Training loss 0.0262136347591877 Validation loss 0.02808902971446514 Accuracy 0.71533203125\n",
      "Iteration 28930 Training loss 0.023304520174860954 Validation loss 0.028141483664512634 Accuracy 0.7138671875\n",
      "Iteration 28940 Training loss 0.023459864780306816 Validation loss 0.027756137773394585 Accuracy 0.71728515625\n",
      "Iteration 28950 Training loss 0.02578384429216385 Validation loss 0.02825004607439041 Accuracy 0.71337890625\n",
      "Iteration 28960 Training loss 0.023983266204595566 Validation loss 0.02869974449276924 Accuracy 0.708984375\n",
      "Iteration 28970 Training loss 0.026277968659996986 Validation loss 0.027776747941970825 Accuracy 0.7177734375\n",
      "Iteration 28980 Training loss 0.024794142693281174 Validation loss 0.027771299704909325 Accuracy 0.7177734375\n",
      "Iteration 28990 Training loss 0.023620467633008957 Validation loss 0.027802538126707077 Accuracy 0.71728515625\n",
      "Iteration 29000 Training loss 0.02701583132147789 Validation loss 0.027725785970687866 Accuracy 0.7177734375\n",
      "Iteration 29010 Training loss 0.025886423885822296 Validation loss 0.027912788093090057 Accuracy 0.716796875\n",
      "Iteration 29020 Training loss 0.02900792844593525 Validation loss 0.030829858034849167 Accuracy 0.6875\n",
      "Iteration 29030 Training loss 0.0241054967045784 Validation loss 0.02786100097000599 Accuracy 0.71728515625\n",
      "Iteration 29040 Training loss 0.023943690583109856 Validation loss 0.027796104550361633 Accuracy 0.7177734375\n",
      "Iteration 29050 Training loss 0.024095332249999046 Validation loss 0.027873778715729713 Accuracy 0.716796875\n",
      "Iteration 29060 Training loss 0.024410750716924667 Validation loss 0.027963798493146896 Accuracy 0.71630859375\n",
      "Iteration 29070 Training loss 0.023253412917256355 Validation loss 0.0280546136200428 Accuracy 0.71435546875\n",
      "Iteration 29080 Training loss 0.025535181164741516 Validation loss 0.028268029913306236 Accuracy 0.712890625\n",
      "Iteration 29090 Training loss 0.02558967098593712 Validation loss 0.02800789475440979 Accuracy 0.7158203125\n",
      "Iteration 29100 Training loss 0.026996174827218056 Validation loss 0.02817480079829693 Accuracy 0.7138671875\n",
      "Iteration 29110 Training loss 0.024111123755574226 Validation loss 0.027876000851392746 Accuracy 0.71533203125\n",
      "Iteration 29120 Training loss 0.02552657015621662 Validation loss 0.02766423486173153 Accuracy 0.71826171875\n",
      "Iteration 29130 Training loss 0.02597469463944435 Validation loss 0.02846871316432953 Accuracy 0.7109375\n",
      "Iteration 29140 Training loss 0.02615882270038128 Validation loss 0.02787378430366516 Accuracy 0.716796875\n",
      "Iteration 29150 Training loss 0.02399411052465439 Validation loss 0.027831414714455605 Accuracy 0.716796875\n",
      "Iteration 29160 Training loss 0.023334337398409843 Validation loss 0.028801947832107544 Accuracy 0.70751953125\n",
      "Iteration 29170 Training loss 0.023847078904509544 Validation loss 0.02778656966984272 Accuracy 0.7177734375\n",
      "Iteration 29180 Training loss 0.025189697742462158 Validation loss 0.02786310762166977 Accuracy 0.71630859375\n",
      "Iteration 29190 Training loss 0.023882107809185982 Validation loss 0.02799975499510765 Accuracy 0.7158203125\n",
      "Iteration 29200 Training loss 0.02641882374882698 Validation loss 0.027743598446249962 Accuracy 0.71875\n",
      "Iteration 29210 Training loss 0.026051385328173637 Validation loss 0.02849976159632206 Accuracy 0.7109375\n",
      "Iteration 29220 Training loss 0.029667137190699577 Validation loss 0.028022553771734238 Accuracy 0.71484375\n",
      "Iteration 29230 Training loss 0.02485998533666134 Validation loss 0.027852771803736687 Accuracy 0.71826171875\n",
      "Iteration 29240 Training loss 0.021949775516986847 Validation loss 0.02798480913043022 Accuracy 0.71630859375\n",
      "Iteration 29250 Training loss 0.025223948061466217 Validation loss 0.02820790745317936 Accuracy 0.71337890625\n",
      "Iteration 29260 Training loss 0.028841715306043625 Validation loss 0.0279579758644104 Accuracy 0.716796875\n",
      "Iteration 29270 Training loss 0.020040450617671013 Validation loss 0.02780330926179886 Accuracy 0.71728515625\n",
      "Iteration 29280 Training loss 0.02473345398902893 Validation loss 0.02794838510453701 Accuracy 0.71630859375\n",
      "Iteration 29290 Training loss 0.026700405403971672 Validation loss 0.028196288272738457 Accuracy 0.7138671875\n",
      "Iteration 29300 Training loss 0.024092860519886017 Validation loss 0.027960654348134995 Accuracy 0.71630859375\n",
      "Iteration 29310 Training loss 0.021691950038075447 Validation loss 0.027643058449029922 Accuracy 0.71875\n",
      "Iteration 29320 Training loss 0.02678696997463703 Validation loss 0.0278093833476305 Accuracy 0.7177734375\n",
      "Iteration 29330 Training loss 0.028160296380519867 Validation loss 0.0281795896589756 Accuracy 0.71435546875\n",
      "Iteration 29340 Training loss 0.02091020904481411 Validation loss 0.02813131920993328 Accuracy 0.71435546875\n",
      "Iteration 29350 Training loss 0.02670561708509922 Validation loss 0.027663419023156166 Accuracy 0.71875\n",
      "Iteration 29360 Training loss 0.020601782947778702 Validation loss 0.027688685804605484 Accuracy 0.71875\n",
      "Iteration 29370 Training loss 0.02171216905117035 Validation loss 0.02777794934809208 Accuracy 0.7177734375\n",
      "Iteration 29380 Training loss 0.022394906729459763 Validation loss 0.027622513473033905 Accuracy 0.71923828125\n",
      "Iteration 29390 Training loss 0.024237262085080147 Validation loss 0.027964599430561066 Accuracy 0.7158203125\n",
      "Iteration 29400 Training loss 0.02632417343556881 Validation loss 0.027815228328108788 Accuracy 0.71728515625\n",
      "Iteration 29410 Training loss 0.025985803455114365 Validation loss 0.027759816497564316 Accuracy 0.71875\n",
      "Iteration 29420 Training loss 0.024077298119664192 Validation loss 0.028511276468634605 Accuracy 0.71142578125\n",
      "Iteration 29430 Training loss 0.024928037077188492 Validation loss 0.029223240911960602 Accuracy 0.70361328125\n",
      "Iteration 29440 Training loss 0.02617180533707142 Validation loss 0.02794771082699299 Accuracy 0.71630859375\n",
      "Iteration 29450 Training loss 0.024532247334718704 Validation loss 0.02861429564654827 Accuracy 0.70947265625\n",
      "Iteration 29460 Training loss 0.02301586978137493 Validation loss 0.02789073996245861 Accuracy 0.716796875\n",
      "Iteration 29470 Training loss 0.02663257345557213 Validation loss 0.02788717672228813 Accuracy 0.716796875\n",
      "Iteration 29480 Training loss 0.025228753685951233 Validation loss 0.027850255370140076 Accuracy 0.71728515625\n",
      "Iteration 29490 Training loss 0.026209019124507904 Validation loss 0.027953695505857468 Accuracy 0.7158203125\n",
      "Iteration 29500 Training loss 0.028272777795791626 Validation loss 0.027998503297567368 Accuracy 0.71630859375\n",
      "Iteration 29510 Training loss 0.02743588760495186 Validation loss 0.027729451656341553 Accuracy 0.71826171875\n",
      "Iteration 29520 Training loss 0.022455040365457535 Validation loss 0.027850065380334854 Accuracy 0.71728515625\n",
      "Iteration 29530 Training loss 0.027711013332009315 Validation loss 0.02859104983508587 Accuracy 0.7099609375\n",
      "Iteration 29540 Training loss 0.025387756526470184 Validation loss 0.02783864177763462 Accuracy 0.71728515625\n",
      "Iteration 29550 Training loss 0.02484460175037384 Validation loss 0.027795536443591118 Accuracy 0.7177734375\n",
      "Iteration 29560 Training loss 0.023424530401825905 Validation loss 0.028149105608463287 Accuracy 0.71435546875\n",
      "Iteration 29570 Training loss 0.02426193468272686 Validation loss 0.027843788266181946 Accuracy 0.716796875\n",
      "Iteration 29580 Training loss 0.024575835093855858 Validation loss 0.02782292850315571 Accuracy 0.71728515625\n",
      "Iteration 29590 Training loss 0.024523422122001648 Validation loss 0.027871469035744667 Accuracy 0.71728515625\n",
      "Iteration 29600 Training loss 0.026595789939165115 Validation loss 0.028415726497769356 Accuracy 0.71044921875\n",
      "Iteration 29610 Training loss 0.027914507314562798 Validation loss 0.027819564566016197 Accuracy 0.716796875\n",
      "Iteration 29620 Training loss 0.02435673587024212 Validation loss 0.02783239260315895 Accuracy 0.71728515625\n",
      "Iteration 29630 Training loss 0.022193796932697296 Validation loss 0.027813967317342758 Accuracy 0.7177734375\n",
      "Iteration 29640 Training loss 0.023992599919438362 Validation loss 0.028437308967113495 Accuracy 0.7109375\n",
      "Iteration 29650 Training loss 0.02392660453915596 Validation loss 0.028211092576384544 Accuracy 0.7138671875\n",
      "Iteration 29660 Training loss 0.02617921493947506 Validation loss 0.027911707758903503 Accuracy 0.71630859375\n",
      "Iteration 29670 Training loss 0.02121168002486229 Validation loss 0.028249213472008705 Accuracy 0.712890625\n",
      "Iteration 29680 Training loss 0.022600723430514336 Validation loss 0.02802162989974022 Accuracy 0.71533203125\n",
      "Iteration 29690 Training loss 0.02424042299389839 Validation loss 0.028160374611616135 Accuracy 0.7138671875\n",
      "Iteration 29700 Training loss 0.022005213424563408 Validation loss 0.02812502346932888 Accuracy 0.7138671875\n",
      "Iteration 29710 Training loss 0.025884822010993958 Validation loss 0.027949193492531776 Accuracy 0.71533203125\n",
      "Iteration 29720 Training loss 0.025054138153791428 Validation loss 0.02809266559779644 Accuracy 0.71435546875\n",
      "Iteration 29730 Training loss 0.02308243326842785 Validation loss 0.028689168393611908 Accuracy 0.70849609375\n",
      "Iteration 29740 Training loss 0.019054817035794258 Validation loss 0.027697362005710602 Accuracy 0.71875\n",
      "Iteration 29750 Training loss 0.024310510605573654 Validation loss 0.027883009985089302 Accuracy 0.7158203125\n",
      "Iteration 29760 Training loss 0.02613305114209652 Validation loss 0.028351396322250366 Accuracy 0.71240234375\n",
      "Iteration 29770 Training loss 0.025508789345622063 Validation loss 0.02858256734907627 Accuracy 0.70947265625\n",
      "Iteration 29780 Training loss 0.02630711905658245 Validation loss 0.028459252789616585 Accuracy 0.71142578125\n",
      "Iteration 29790 Training loss 0.024535581469535828 Validation loss 0.027955085039138794 Accuracy 0.71630859375\n",
      "Iteration 29800 Training loss 0.025531698018312454 Validation loss 0.028583446517586708 Accuracy 0.708984375\n",
      "Iteration 29810 Training loss 0.023041803389787674 Validation loss 0.02810969576239586 Accuracy 0.7138671875\n",
      "Iteration 29820 Training loss 0.02515893615782261 Validation loss 0.027949368581175804 Accuracy 0.71630859375\n",
      "Iteration 29830 Training loss 0.023874588310718536 Validation loss 0.02800498716533184 Accuracy 0.71533203125\n",
      "Iteration 29840 Training loss 0.023919768631458282 Validation loss 0.028005648404359818 Accuracy 0.71484375\n",
      "Iteration 29850 Training loss 0.026621805503964424 Validation loss 0.02816542610526085 Accuracy 0.712890625\n",
      "Iteration 29860 Training loss 0.02374037727713585 Validation loss 0.028020966798067093 Accuracy 0.71533203125\n",
      "Iteration 29870 Training loss 0.024853205308318138 Validation loss 0.028122389689087868 Accuracy 0.71435546875\n",
      "Iteration 29880 Training loss 0.026290882378816605 Validation loss 0.02800006978213787 Accuracy 0.71533203125\n",
      "Iteration 29890 Training loss 0.02709909714758396 Validation loss 0.028110787272453308 Accuracy 0.71435546875\n",
      "Iteration 29900 Training loss 0.02517709881067276 Validation loss 0.027934694662690163 Accuracy 0.71630859375\n",
      "Iteration 29910 Training loss 0.024305732920765877 Validation loss 0.027842171490192413 Accuracy 0.71630859375\n",
      "Iteration 29920 Training loss 0.025025008246302605 Validation loss 0.027757933363318443 Accuracy 0.71728515625\n",
      "Iteration 29930 Training loss 0.024945728480815887 Validation loss 0.02792731299996376 Accuracy 0.71630859375\n",
      "Iteration 29940 Training loss 0.02473430521786213 Validation loss 0.02839503064751625 Accuracy 0.71142578125\n",
      "Iteration 29950 Training loss 0.02558889240026474 Validation loss 0.0283669400960207 Accuracy 0.7119140625\n",
      "Iteration 29960 Training loss 0.028812069445848465 Validation loss 0.028217032551765442 Accuracy 0.71337890625\n",
      "Iteration 29970 Training loss 0.0259238388389349 Validation loss 0.02868208847939968 Accuracy 0.70849609375\n",
      "Iteration 29980 Training loss 0.024072321131825447 Validation loss 0.027919774875044823 Accuracy 0.7158203125\n",
      "Iteration 29990 Training loss 0.022270647808909416 Validation loss 0.028196725994348526 Accuracy 0.7138671875\n",
      "Iteration 30000 Training loss 0.025347940623760223 Validation loss 0.030315695330500603 Accuracy 0.69189453125\n",
      "Iteration 30010 Training loss 0.02823512814939022 Validation loss 0.027802780270576477 Accuracy 0.71728515625\n",
      "Iteration 30020 Training loss 0.024527082219719887 Validation loss 0.027847828343510628 Accuracy 0.716796875\n",
      "Iteration 30030 Training loss 0.0236642025411129 Validation loss 0.027689214795827866 Accuracy 0.71826171875\n",
      "Iteration 30040 Training loss 0.023796014487743378 Validation loss 0.02885225974023342 Accuracy 0.70703125\n",
      "Iteration 30050 Training loss 0.02752991020679474 Validation loss 0.027886822819709778 Accuracy 0.716796875\n",
      "Iteration 30060 Training loss 0.02639363519847393 Validation loss 0.028668919578194618 Accuracy 0.70751953125\n",
      "Iteration 30070 Training loss 0.020480692386627197 Validation loss 0.028278226032853127 Accuracy 0.71240234375\n",
      "Iteration 30080 Training loss 0.02592855878174305 Validation loss 0.028017975389957428 Accuracy 0.71533203125\n",
      "Iteration 30090 Training loss 0.025046344846487045 Validation loss 0.027914635837078094 Accuracy 0.7158203125\n",
      "Iteration 30100 Training loss 0.022731348872184753 Validation loss 0.02775118127465248 Accuracy 0.71826171875\n",
      "Iteration 30110 Training loss 0.027065999805927277 Validation loss 0.02901102416217327 Accuracy 0.70556640625\n",
      "Iteration 30120 Training loss 0.025325346738100052 Validation loss 0.028217030689120293 Accuracy 0.712890625\n",
      "Iteration 30130 Training loss 0.023510055616497993 Validation loss 0.02823687344789505 Accuracy 0.712890625\n",
      "Iteration 30140 Training loss 0.026499068364501 Validation loss 0.02919992059469223 Accuracy 0.70361328125\n",
      "Iteration 30150 Training loss 0.023606114089488983 Validation loss 0.028283128514885902 Accuracy 0.7119140625\n",
      "Iteration 30160 Training loss 0.02535071223974228 Validation loss 0.028324194252490997 Accuracy 0.7119140625\n",
      "Iteration 30170 Training loss 0.026575513184070587 Validation loss 0.027929037809371948 Accuracy 0.71630859375\n",
      "Iteration 30180 Training loss 0.024746017530560493 Validation loss 0.028263090178370476 Accuracy 0.712890625\n",
      "Iteration 30190 Training loss 0.026370050385594368 Validation loss 0.027739500626921654 Accuracy 0.7177734375\n",
      "Iteration 30200 Training loss 0.025215113535523415 Validation loss 0.027913294732570648 Accuracy 0.71630859375\n",
      "Iteration 30210 Training loss 0.02756683900952339 Validation loss 0.02842668816447258 Accuracy 0.7119140625\n",
      "Iteration 30220 Training loss 0.02333851158618927 Validation loss 0.02797207050025463 Accuracy 0.71533203125\n",
      "Iteration 30230 Training loss 0.024425067007541656 Validation loss 0.027835531160235405 Accuracy 0.71728515625\n",
      "Iteration 30240 Training loss 0.023477701470255852 Validation loss 0.027687493711709976 Accuracy 0.71875\n",
      "Iteration 30250 Training loss 0.02434678189456463 Validation loss 0.028279686346650124 Accuracy 0.71337890625\n",
      "Iteration 30260 Training loss 0.02417776919901371 Validation loss 0.028262406587600708 Accuracy 0.71337890625\n",
      "Iteration 30270 Training loss 0.025046376511454582 Validation loss 0.027945345267653465 Accuracy 0.71533203125\n",
      "Iteration 30280 Training loss 0.02411261387169361 Validation loss 0.02775326557457447 Accuracy 0.7177734375\n",
      "Iteration 30290 Training loss 0.023198554292321205 Validation loss 0.0280310045927763 Accuracy 0.71484375\n",
      "Iteration 30300 Training loss 0.02518092840909958 Validation loss 0.02790258638560772 Accuracy 0.716796875\n",
      "Iteration 30310 Training loss 0.02518535777926445 Validation loss 0.028539996594190598 Accuracy 0.7099609375\n",
      "Iteration 30320 Training loss 0.024800119921565056 Validation loss 0.0279662124812603 Accuracy 0.7158203125\n",
      "Iteration 30330 Training loss 0.025226404890418053 Validation loss 0.02808220125734806 Accuracy 0.7138671875\n",
      "Iteration 30340 Training loss 0.026371192187070847 Validation loss 0.028481781482696533 Accuracy 0.7099609375\n",
      "Iteration 30350 Training loss 0.023960478603839874 Validation loss 0.028171991929411888 Accuracy 0.7138671875\n",
      "Iteration 30360 Training loss 0.02648518793284893 Validation loss 0.0283436831086874 Accuracy 0.7119140625\n",
      "Iteration 30370 Training loss 0.025050276890397072 Validation loss 0.02888701669871807 Accuracy 0.70654296875\n",
      "Iteration 30380 Training loss 0.02634250931441784 Validation loss 0.028507260605692863 Accuracy 0.7109375\n",
      "Iteration 30390 Training loss 0.019416196271777153 Validation loss 0.027876757085323334 Accuracy 0.71630859375\n",
      "Iteration 30400 Training loss 0.024273741990327835 Validation loss 0.02830096147954464 Accuracy 0.7119140625\n",
      "Iteration 30410 Training loss 0.020918378606438637 Validation loss 0.027725255116820335 Accuracy 0.71875\n",
      "Iteration 30420 Training loss 0.024283243343234062 Validation loss 0.027698418125510216 Accuracy 0.71875\n",
      "Iteration 30430 Training loss 0.026751646772027016 Validation loss 0.02793280780315399 Accuracy 0.71630859375\n",
      "Iteration 30440 Training loss 0.024695690721273422 Validation loss 0.027958285063505173 Accuracy 0.71630859375\n",
      "Iteration 30450 Training loss 0.021427853032946587 Validation loss 0.027748851105570793 Accuracy 0.71826171875\n",
      "Iteration 30460 Training loss 0.025469128042459488 Validation loss 0.028144195675849915 Accuracy 0.7138671875\n",
      "Iteration 30470 Training loss 0.02583647146821022 Validation loss 0.027928655967116356 Accuracy 0.71630859375\n",
      "Iteration 30480 Training loss 0.02927953563630581 Validation loss 0.029344609007239342 Accuracy 0.701171875\n",
      "Iteration 30490 Training loss 0.022541584447026253 Validation loss 0.02787950076162815 Accuracy 0.716796875\n",
      "Iteration 30500 Training loss 0.023655567318201065 Validation loss 0.027796274051070213 Accuracy 0.7177734375\n",
      "Iteration 30510 Training loss 0.020826201885938644 Validation loss 0.028266923502087593 Accuracy 0.712890625\n",
      "Iteration 30520 Training loss 0.02773214876651764 Validation loss 0.028558725491166115 Accuracy 0.7109375\n",
      "Iteration 30530 Training loss 0.02996225655078888 Validation loss 0.028471553698182106 Accuracy 0.71142578125\n",
      "Iteration 30540 Training loss 0.025570834055542946 Validation loss 0.027684256434440613 Accuracy 0.71923828125\n",
      "Iteration 30550 Training loss 0.023203084245324135 Validation loss 0.0281827412545681 Accuracy 0.71337890625\n",
      "Iteration 30560 Training loss 0.025206655263900757 Validation loss 0.027708549052476883 Accuracy 0.71875\n",
      "Iteration 30570 Training loss 0.028929362073540688 Validation loss 0.02912394516170025 Accuracy 0.7041015625\n",
      "Iteration 30580 Training loss 0.025036374107003212 Validation loss 0.027850527316331863 Accuracy 0.716796875\n",
      "Iteration 30590 Training loss 0.023622024804353714 Validation loss 0.027834974229335785 Accuracy 0.71728515625\n",
      "Iteration 30600 Training loss 0.02422034740447998 Validation loss 0.0279109887778759 Accuracy 0.7158203125\n",
      "Iteration 30610 Training loss 0.021894291043281555 Validation loss 0.027764329686760902 Accuracy 0.7177734375\n",
      "Iteration 30620 Training loss 0.023804519325494766 Validation loss 0.027786586433649063 Accuracy 0.71826171875\n",
      "Iteration 30630 Training loss 0.022642742842435837 Validation loss 0.02788121998310089 Accuracy 0.716796875\n",
      "Iteration 30640 Training loss 0.025060759857296944 Validation loss 0.02772488445043564 Accuracy 0.71875\n",
      "Iteration 30650 Training loss 0.022572170943021774 Validation loss 0.028017215430736542 Accuracy 0.71533203125\n",
      "Iteration 30660 Training loss 0.023080378770828247 Validation loss 0.028036048635840416 Accuracy 0.71533203125\n",
      "Iteration 30670 Training loss 0.023813964799046516 Validation loss 0.027727775275707245 Accuracy 0.71826171875\n",
      "Iteration 30680 Training loss 0.02382337860763073 Validation loss 0.028328582644462585 Accuracy 0.71240234375\n",
      "Iteration 30690 Training loss 0.028700368478894234 Validation loss 0.028299210593104362 Accuracy 0.71240234375\n",
      "Iteration 30700 Training loss 0.023288976401090622 Validation loss 0.02784360572695732 Accuracy 0.7177734375\n",
      "Iteration 30710 Training loss 0.025811564177274704 Validation loss 0.02778295986354351 Accuracy 0.71728515625\n",
      "Iteration 30720 Training loss 0.02559884637594223 Validation loss 0.02807464264333248 Accuracy 0.71484375\n",
      "Iteration 30730 Training loss 0.02177700214087963 Validation loss 0.028833778575062752 Accuracy 0.70751953125\n",
      "Iteration 30740 Training loss 0.026268916204571724 Validation loss 0.028143400326371193 Accuracy 0.7138671875\n",
      "Iteration 30750 Training loss 0.02616272121667862 Validation loss 0.028411947190761566 Accuracy 0.71142578125\n",
      "Iteration 30760 Training loss 0.023009203374385834 Validation loss 0.028249425813555717 Accuracy 0.712890625\n",
      "Iteration 30770 Training loss 0.026577714830636978 Validation loss 0.027842113748192787 Accuracy 0.716796875\n",
      "Iteration 30780 Training loss 0.02433011680841446 Validation loss 0.02779015526175499 Accuracy 0.71630859375\n",
      "Iteration 30790 Training loss 0.02766449935734272 Validation loss 0.028065785765647888 Accuracy 0.71435546875\n",
      "Iteration 30800 Training loss 0.022940510883927345 Validation loss 0.02811492048203945 Accuracy 0.7138671875\n",
      "Iteration 30810 Training loss 0.02521650493144989 Validation loss 0.027894170954823494 Accuracy 0.71630859375\n",
      "Iteration 30820 Training loss 0.021540064364671707 Validation loss 0.027948278933763504 Accuracy 0.7158203125\n",
      "Iteration 30830 Training loss 0.026600733399391174 Validation loss 0.027799883857369423 Accuracy 0.716796875\n",
      "Iteration 30840 Training loss 0.02202356979250908 Validation loss 0.02782326564192772 Accuracy 0.71728515625\n",
      "Iteration 30850 Training loss 0.023396216332912445 Validation loss 0.02788197621703148 Accuracy 0.716796875\n",
      "Iteration 30860 Training loss 0.026347147300839424 Validation loss 0.028318313881754875 Accuracy 0.7119140625\n",
      "Iteration 30870 Training loss 0.022309673950076103 Validation loss 0.027917832136154175 Accuracy 0.71630859375\n",
      "Iteration 30880 Training loss 0.021348584443330765 Validation loss 0.02772342786192894 Accuracy 0.71826171875\n",
      "Iteration 30890 Training loss 0.02256777510046959 Validation loss 0.02813117206096649 Accuracy 0.71533203125\n",
      "Iteration 30900 Training loss 0.022266438230872154 Validation loss 0.02799391932785511 Accuracy 0.71533203125\n",
      "Iteration 30910 Training loss 0.02434161677956581 Validation loss 0.02794625423848629 Accuracy 0.7158203125\n",
      "Iteration 30920 Training loss 0.024024780839681625 Validation loss 0.028129013255238533 Accuracy 0.71435546875\n",
      "Iteration 30930 Training loss 0.023798715323209763 Validation loss 0.027988597750663757 Accuracy 0.7158203125\n",
      "Iteration 30940 Training loss 0.01874871551990509 Validation loss 0.027758551761507988 Accuracy 0.7177734375\n",
      "Iteration 30950 Training loss 0.025393035262823105 Validation loss 0.027953939512372017 Accuracy 0.71630859375\n",
      "Iteration 30960 Training loss 0.023499496281147003 Validation loss 0.027690663933753967 Accuracy 0.71875\n",
      "Iteration 30970 Training loss 0.02345573902130127 Validation loss 0.02796211652457714 Accuracy 0.71630859375\n",
      "Iteration 30980 Training loss 0.024076664820313454 Validation loss 0.0277849193662405 Accuracy 0.7177734375\n",
      "Iteration 30990 Training loss 0.024689150974154472 Validation loss 0.027781276032328606 Accuracy 0.71826171875\n",
      "Iteration 31000 Training loss 0.026652608066797256 Validation loss 0.02800646238029003 Accuracy 0.71533203125\n",
      "Iteration 31010 Training loss 0.026954956352710724 Validation loss 0.02782013639807701 Accuracy 0.71728515625\n",
      "Iteration 31020 Training loss 0.026990981772542 Validation loss 0.02796909585595131 Accuracy 0.71533203125\n",
      "Iteration 31030 Training loss 0.02560434490442276 Validation loss 0.027997557073831558 Accuracy 0.71484375\n",
      "Iteration 31040 Training loss 0.026453640311956406 Validation loss 0.027815023437142372 Accuracy 0.716796875\n",
      "Iteration 31050 Training loss 0.025301482528448105 Validation loss 0.027776608243584633 Accuracy 0.7177734375\n",
      "Iteration 31060 Training loss 0.024981429800391197 Validation loss 0.027938758954405785 Accuracy 0.716796875\n",
      "Iteration 31070 Training loss 0.022487474605441093 Validation loss 0.027885815128684044 Accuracy 0.71630859375\n",
      "Iteration 31080 Training loss 0.022812407463788986 Validation loss 0.028445927426218987 Accuracy 0.7109375\n",
      "Iteration 31090 Training loss 0.02412131242454052 Validation loss 0.027939965948462486 Accuracy 0.7158203125\n",
      "Iteration 31100 Training loss 0.024821775034070015 Validation loss 0.028029151260852814 Accuracy 0.71435546875\n",
      "Iteration 31110 Training loss 0.021467113867402077 Validation loss 0.028135647997260094 Accuracy 0.7138671875\n",
      "Iteration 31120 Training loss 0.026559727266430855 Validation loss 0.027916884049773216 Accuracy 0.71630859375\n",
      "Iteration 31130 Training loss 0.025606809183955193 Validation loss 0.02782618999481201 Accuracy 0.7177734375\n",
      "Iteration 31140 Training loss 0.02185988426208496 Validation loss 0.02805585041642189 Accuracy 0.71484375\n",
      "Iteration 31150 Training loss 0.025213032960891724 Validation loss 0.02809237316250801 Accuracy 0.7138671875\n",
      "Iteration 31160 Training loss 0.024026427417993546 Validation loss 0.02779843658208847 Accuracy 0.716796875\n",
      "Iteration 31170 Training loss 0.02162189409136772 Validation loss 0.02769426628947258 Accuracy 0.7177734375\n",
      "Iteration 31180 Training loss 0.02495739795267582 Validation loss 0.027813607826828957 Accuracy 0.71728515625\n",
      "Iteration 31190 Training loss 0.025898218154907227 Validation loss 0.02785347029566765 Accuracy 0.716796875\n",
      "Iteration 31200 Training loss 0.02550380490720272 Validation loss 0.027876952663064003 Accuracy 0.71630859375\n",
      "Iteration 31210 Training loss 0.0207822285592556 Validation loss 0.0276012122631073 Accuracy 0.71875\n",
      "Iteration 31220 Training loss 0.02611730992794037 Validation loss 0.027807479724287987 Accuracy 0.716796875\n",
      "Iteration 31230 Training loss 0.025728199630975723 Validation loss 0.028038334101438522 Accuracy 0.71435546875\n",
      "Iteration 31240 Training loss 0.022805627435445786 Validation loss 0.02824491448700428 Accuracy 0.712890625\n",
      "Iteration 31250 Training loss 0.025219658389687538 Validation loss 0.027863437309861183 Accuracy 0.716796875\n",
      "Iteration 31260 Training loss 0.02637503482401371 Validation loss 0.027778787538409233 Accuracy 0.71728515625\n",
      "Iteration 31270 Training loss 0.02482510358095169 Validation loss 0.028040437027812004 Accuracy 0.71484375\n",
      "Iteration 31280 Training loss 0.021126708015799522 Validation loss 0.027894511818885803 Accuracy 0.71630859375\n",
      "Iteration 31290 Training loss 0.02628912404179573 Validation loss 0.028243744745850563 Accuracy 0.712890625\n",
      "Iteration 31300 Training loss 0.0245988667011261 Validation loss 0.027813969179987907 Accuracy 0.71728515625\n",
      "Iteration 31310 Training loss 0.025351766496896744 Validation loss 0.02772331051528454 Accuracy 0.71923828125\n",
      "Iteration 31320 Training loss 0.021843018010258675 Validation loss 0.02777688205242157 Accuracy 0.7177734375\n",
      "Iteration 31330 Training loss 0.02532634511590004 Validation loss 0.02823244035243988 Accuracy 0.712890625\n",
      "Iteration 31340 Training loss 0.024826975539326668 Validation loss 0.027795860543847084 Accuracy 0.71826171875\n",
      "Iteration 31350 Training loss 0.026899443939328194 Validation loss 0.028207821771502495 Accuracy 0.712890625\n",
      "Iteration 31360 Training loss 0.025403985753655434 Validation loss 0.028036952018737793 Accuracy 0.71533203125\n",
      "Iteration 31370 Training loss 0.02272327058017254 Validation loss 0.02785986103117466 Accuracy 0.71630859375\n",
      "Iteration 31380 Training loss 0.027980104088783264 Validation loss 0.028627172112464905 Accuracy 0.708984375\n",
      "Iteration 31390 Training loss 0.02468915283679962 Validation loss 0.02781522460281849 Accuracy 0.716796875\n",
      "Iteration 31400 Training loss 0.024099433794617653 Validation loss 0.028008725494146347 Accuracy 0.71484375\n",
      "Iteration 31410 Training loss 0.02358943596482277 Validation loss 0.02816086821258068 Accuracy 0.71337890625\n",
      "Iteration 31420 Training loss 0.020293647423386574 Validation loss 0.02776370756328106 Accuracy 0.71826171875\n",
      "Iteration 31430 Training loss 0.024088002741336823 Validation loss 0.027855942025780678 Accuracy 0.716796875\n",
      "Iteration 31440 Training loss 0.022722752764821053 Validation loss 0.027845898643136024 Accuracy 0.71630859375\n",
      "Iteration 31450 Training loss 0.023588182404637337 Validation loss 0.028301991522312164 Accuracy 0.712890625\n",
      "Iteration 31460 Training loss 0.025148192420601845 Validation loss 0.029211552813649178 Accuracy 0.70361328125\n",
      "Iteration 31470 Training loss 0.021528758108615875 Validation loss 0.028393473476171494 Accuracy 0.7109375\n",
      "Iteration 31480 Training loss 0.0248029176145792 Validation loss 0.028024708852171898 Accuracy 0.71484375\n",
      "Iteration 31490 Training loss 0.0252669844776392 Validation loss 0.02813171222805977 Accuracy 0.71337890625\n",
      "Iteration 31500 Training loss 0.020316649228334427 Validation loss 0.027773646637797356 Accuracy 0.716796875\n",
      "Iteration 31510 Training loss 0.02438916265964508 Validation loss 0.02820173278450966 Accuracy 0.71337890625\n",
      "Iteration 31520 Training loss 0.025504328310489655 Validation loss 0.02767905779182911 Accuracy 0.71875\n",
      "Iteration 31530 Training loss 0.02609454281628132 Validation loss 0.0281209833920002 Accuracy 0.71337890625\n",
      "Iteration 31540 Training loss 0.022240854799747467 Validation loss 0.02775580808520317 Accuracy 0.7177734375\n",
      "Iteration 31550 Training loss 0.026326140388846397 Validation loss 0.028549281880259514 Accuracy 0.7099609375\n",
      "Iteration 31560 Training loss 0.02360447868704796 Validation loss 0.02771407552063465 Accuracy 0.71875\n",
      "Iteration 31570 Training loss 0.026060618460178375 Validation loss 0.027610771358013153 Accuracy 0.71875\n",
      "Iteration 31580 Training loss 0.02615009993314743 Validation loss 0.02795444242656231 Accuracy 0.71630859375\n",
      "Iteration 31590 Training loss 0.02374035306274891 Validation loss 0.027935411781072617 Accuracy 0.71630859375\n",
      "Iteration 31600 Training loss 0.02380802296102047 Validation loss 0.027671966701745987 Accuracy 0.71875\n",
      "Iteration 31610 Training loss 0.025201013311743736 Validation loss 0.027884213253855705 Accuracy 0.716796875\n",
      "Iteration 31620 Training loss 0.024126393720507622 Validation loss 0.027935966849327087 Accuracy 0.71533203125\n",
      "Iteration 31630 Training loss 0.022996973246335983 Validation loss 0.027960192412137985 Accuracy 0.71630859375\n",
      "Iteration 31640 Training loss 0.021505599841475487 Validation loss 0.02790392003953457 Accuracy 0.71533203125\n",
      "Iteration 31650 Training loss 0.028400005772709846 Validation loss 0.02808230370283127 Accuracy 0.71484375\n",
      "Iteration 31660 Training loss 0.023405270650982857 Validation loss 0.027840400114655495 Accuracy 0.71728515625\n",
      "Iteration 31670 Training loss 0.024157512933015823 Validation loss 0.027665136381983757 Accuracy 0.71923828125\n",
      "Iteration 31680 Training loss 0.02609746716916561 Validation loss 0.027695056051015854 Accuracy 0.71875\n",
      "Iteration 31690 Training loss 0.024066278710961342 Validation loss 0.02796088717877865 Accuracy 0.71630859375\n",
      "Iteration 31700 Training loss 0.027201592922210693 Validation loss 0.02798815816640854 Accuracy 0.7158203125\n",
      "Iteration 31710 Training loss 0.02676798403263092 Validation loss 0.02776969224214554 Accuracy 0.716796875\n",
      "Iteration 31720 Training loss 0.023233095183968544 Validation loss 0.0279326643794775 Accuracy 0.7158203125\n",
      "Iteration 31730 Training loss 0.02572326548397541 Validation loss 0.02777191624045372 Accuracy 0.7177734375\n",
      "Iteration 31740 Training loss 0.027928560972213745 Validation loss 0.027823055163025856 Accuracy 0.716796875\n",
      "Iteration 31750 Training loss 0.0246425811201334 Validation loss 0.02758941426873207 Accuracy 0.72021484375\n",
      "Iteration 31760 Training loss 0.025186538696289062 Validation loss 0.027880936861038208 Accuracy 0.71630859375\n",
      "Iteration 31770 Training loss 0.02598738856613636 Validation loss 0.027990488335490227 Accuracy 0.7158203125\n",
      "Iteration 31780 Training loss 0.025910794734954834 Validation loss 0.02782757580280304 Accuracy 0.716796875\n",
      "Iteration 31790 Training loss 0.026631789281964302 Validation loss 0.027582280337810516 Accuracy 0.7197265625\n",
      "Iteration 31800 Training loss 0.02512579783797264 Validation loss 0.027993133291602135 Accuracy 0.7158203125\n",
      "Iteration 31810 Training loss 0.027188783511519432 Validation loss 0.027966415509581566 Accuracy 0.71533203125\n",
      "Iteration 31820 Training loss 0.027597220614552498 Validation loss 0.028583595529198647 Accuracy 0.70947265625\n",
      "Iteration 31830 Training loss 0.025509241968393326 Validation loss 0.027984818443655968 Accuracy 0.71533203125\n",
      "Iteration 31840 Training loss 0.023349979892373085 Validation loss 0.027593981474637985 Accuracy 0.71923828125\n",
      "Iteration 31850 Training loss 0.0251663476228714 Validation loss 0.02798447385430336 Accuracy 0.71533203125\n",
      "Iteration 31860 Training loss 0.027356725186109543 Validation loss 0.028687894344329834 Accuracy 0.708984375\n",
      "Iteration 31870 Training loss 0.02683989889919758 Validation loss 0.027698885649442673 Accuracy 0.71875\n",
      "Iteration 31880 Training loss 0.029643310233950615 Validation loss 0.02979831025004387 Accuracy 0.697265625\n",
      "Iteration 31890 Training loss 0.023807911202311516 Validation loss 0.02790849469602108 Accuracy 0.716796875\n",
      "Iteration 31900 Training loss 0.023333968594670296 Validation loss 0.027727549895644188 Accuracy 0.71826171875\n",
      "Iteration 31910 Training loss 0.027369216084480286 Validation loss 0.027595268562436104 Accuracy 0.7197265625\n",
      "Iteration 31920 Training loss 0.023390701040625572 Validation loss 0.027999330312013626 Accuracy 0.71484375\n",
      "Iteration 31930 Training loss 0.025308288633823395 Validation loss 0.027729462832212448 Accuracy 0.71826171875\n",
      "Iteration 31940 Training loss 0.026343978941440582 Validation loss 0.028157122433185577 Accuracy 0.71337890625\n",
      "Iteration 31950 Training loss 0.026902521029114723 Validation loss 0.027892665937542915 Accuracy 0.716796875\n",
      "Iteration 31960 Training loss 0.02428673580288887 Validation loss 0.02783239260315895 Accuracy 0.7177734375\n",
      "Iteration 31970 Training loss 0.026416435837745667 Validation loss 0.028031673282384872 Accuracy 0.7158203125\n",
      "Iteration 31980 Training loss 0.02564888820052147 Validation loss 0.027839921414852142 Accuracy 0.716796875\n",
      "Iteration 31990 Training loss 0.027424680069088936 Validation loss 0.028737718239426613 Accuracy 0.70751953125\n",
      "Iteration 32000 Training loss 0.02438153512775898 Validation loss 0.027987198904156685 Accuracy 0.71533203125\n",
      "Iteration 32010 Training loss 0.024698158726096153 Validation loss 0.02793247252702713 Accuracy 0.716796875\n",
      "Iteration 32020 Training loss 0.02608645148575306 Validation loss 0.027996810153126717 Accuracy 0.71533203125\n",
      "Iteration 32030 Training loss 0.021934334188699722 Validation loss 0.027731021866202354 Accuracy 0.71826171875\n",
      "Iteration 32040 Training loss 0.02360079251229763 Validation loss 0.028401367366313934 Accuracy 0.71142578125\n",
      "Iteration 32050 Training loss 0.02323184162378311 Validation loss 0.02776310406625271 Accuracy 0.71826171875\n",
      "Iteration 32060 Training loss 0.021284637972712517 Validation loss 0.027750898152589798 Accuracy 0.7177734375\n",
      "Iteration 32070 Training loss 0.026211639866232872 Validation loss 0.027633801102638245 Accuracy 0.7197265625\n",
      "Iteration 32080 Training loss 0.024144360795617104 Validation loss 0.02823033183813095 Accuracy 0.7138671875\n",
      "Iteration 32090 Training loss 0.024154162034392357 Validation loss 0.027826668694615364 Accuracy 0.71728515625\n",
      "Iteration 32100 Training loss 0.025453917682170868 Validation loss 0.02779894508421421 Accuracy 0.71728515625\n",
      "Iteration 32110 Training loss 0.022775748744606972 Validation loss 0.02797011099755764 Accuracy 0.71533203125\n",
      "Iteration 32120 Training loss 0.02312479168176651 Validation loss 0.02801327593624592 Accuracy 0.7158203125\n",
      "Iteration 32130 Training loss 0.026577549055218697 Validation loss 0.02790047787129879 Accuracy 0.71630859375\n",
      "Iteration 32140 Training loss 0.0243363194167614 Validation loss 0.02818331867456436 Accuracy 0.7138671875\n",
      "Iteration 32150 Training loss 0.02459222823381424 Validation loss 0.027848580852150917 Accuracy 0.71728515625\n",
      "Iteration 32160 Training loss 0.02765459194779396 Validation loss 0.027721624821424484 Accuracy 0.71826171875\n",
      "Iteration 32170 Training loss 0.01999032124876976 Validation loss 0.027791280299425125 Accuracy 0.71826171875\n",
      "Iteration 32180 Training loss 0.024648457765579224 Validation loss 0.027788739651441574 Accuracy 0.71728515625\n",
      "Iteration 32190 Training loss 0.026321006938815117 Validation loss 0.02815055288374424 Accuracy 0.7138671875\n",
      "Iteration 32200 Training loss 0.026429301127791405 Validation loss 0.027967318892478943 Accuracy 0.71533203125\n",
      "Iteration 32210 Training loss 0.025043340399861336 Validation loss 0.02797386609017849 Accuracy 0.71630859375\n",
      "Iteration 32220 Training loss 0.026234924793243408 Validation loss 0.028034444898366928 Accuracy 0.7138671875\n",
      "Iteration 32230 Training loss 0.026377705857157707 Validation loss 0.02799418941140175 Accuracy 0.7158203125\n",
      "Iteration 32240 Training loss 0.02381768263876438 Validation loss 0.02799457497894764 Accuracy 0.71533203125\n",
      "Iteration 32250 Training loss 0.022267302498221397 Validation loss 0.027821486815810204 Accuracy 0.71728515625\n",
      "Iteration 32260 Training loss 0.024351881816983223 Validation loss 0.027742812409996986 Accuracy 0.71826171875\n",
      "Iteration 32270 Training loss 0.02299586497247219 Validation loss 0.02808922715485096 Accuracy 0.71484375\n",
      "Iteration 32280 Training loss 0.022898223251104355 Validation loss 0.02807721309363842 Accuracy 0.71484375\n",
      "Iteration 32290 Training loss 0.026240475475788116 Validation loss 0.027926849201321602 Accuracy 0.71630859375\n",
      "Iteration 32300 Training loss 0.024693496525287628 Validation loss 0.027853501960635185 Accuracy 0.716796875\n",
      "Iteration 32310 Training loss 0.022813033312559128 Validation loss 0.027879703789949417 Accuracy 0.71728515625\n",
      "Iteration 32320 Training loss 0.025697484612464905 Validation loss 0.02813483402132988 Accuracy 0.71533203125\n",
      "Iteration 32330 Training loss 0.02435796521604061 Validation loss 0.027982104569673538 Accuracy 0.7158203125\n",
      "Iteration 32340 Training loss 0.02185935713350773 Validation loss 0.02796243317425251 Accuracy 0.71533203125\n",
      "Iteration 32350 Training loss 0.024840278550982475 Validation loss 0.027943871915340424 Accuracy 0.71630859375\n",
      "Iteration 32360 Training loss 0.022553512826561928 Validation loss 0.027925824746489525 Accuracy 0.71630859375\n",
      "Iteration 32370 Training loss 0.024073464795947075 Validation loss 0.027735961601138115 Accuracy 0.71826171875\n",
      "Iteration 32380 Training loss 0.02449440024793148 Validation loss 0.03021279163658619 Accuracy 0.693359375\n",
      "Iteration 32390 Training loss 0.02739495225250721 Validation loss 0.027957094833254814 Accuracy 0.71630859375\n",
      "Iteration 32400 Training loss 0.022906295955181122 Validation loss 0.028295006603002548 Accuracy 0.7119140625\n",
      "Iteration 32410 Training loss 0.0258711539208889 Validation loss 0.027947040274739265 Accuracy 0.71533203125\n",
      "Iteration 32420 Training loss 0.024228950962424278 Validation loss 0.02766110561788082 Accuracy 0.7197265625\n",
      "Iteration 32430 Training loss 0.024564091116189957 Validation loss 0.02797241136431694 Accuracy 0.7158203125\n",
      "Iteration 32440 Training loss 0.025995807722210884 Validation loss 0.027896083891391754 Accuracy 0.71630859375\n",
      "Iteration 32450 Training loss 0.02404807321727276 Validation loss 0.02775486186146736 Accuracy 0.71826171875\n",
      "Iteration 32460 Training loss 0.02386089600622654 Validation loss 0.027742303907871246 Accuracy 0.71826171875\n",
      "Iteration 32470 Training loss 0.02444390021264553 Validation loss 0.02777409367263317 Accuracy 0.716796875\n",
      "Iteration 32480 Training loss 0.023584900423884392 Validation loss 0.027921585366129875 Accuracy 0.71630859375\n",
      "Iteration 32490 Training loss 0.02540488727390766 Validation loss 0.029456378892064095 Accuracy 0.70068359375\n",
      "Iteration 32500 Training loss 0.02346898429095745 Validation loss 0.02784491702914238 Accuracy 0.71728515625\n",
      "Iteration 32510 Training loss 0.025118911638855934 Validation loss 0.027889184653759003 Accuracy 0.71630859375\n",
      "Iteration 32520 Training loss 0.023822829127311707 Validation loss 0.0279676653444767 Accuracy 0.71533203125\n",
      "Iteration 32530 Training loss 0.021968282759189606 Validation loss 0.028231918811798096 Accuracy 0.71337890625\n",
      "Iteration 32540 Training loss 0.023325780406594276 Validation loss 0.02810909040272236 Accuracy 0.71337890625\n",
      "Iteration 32550 Training loss 0.020447593182325363 Validation loss 0.02791227400302887 Accuracy 0.7158203125\n",
      "Iteration 32560 Training loss 0.026790251955389977 Validation loss 0.028007695451378822 Accuracy 0.71484375\n",
      "Iteration 32570 Training loss 0.02326315827667713 Validation loss 0.028032567352056503 Accuracy 0.71484375\n",
      "Iteration 32580 Training loss 0.021943802013993263 Validation loss 0.027664028108119965 Accuracy 0.71875\n",
      "Iteration 32590 Training loss 0.027415165677666664 Validation loss 0.027921762317419052 Accuracy 0.71630859375\n",
      "Iteration 32600 Training loss 0.023227157071232796 Validation loss 0.027931801974773407 Accuracy 0.7158203125\n",
      "Iteration 32610 Training loss 0.023120641708374023 Validation loss 0.029295379295945168 Accuracy 0.70263671875\n",
      "Iteration 32620 Training loss 0.025033844634890556 Validation loss 0.027702147141098976 Accuracy 0.71875\n",
      "Iteration 32630 Training loss 0.023450102657079697 Validation loss 0.027758067473769188 Accuracy 0.71875\n",
      "Iteration 32640 Training loss 0.025494150817394257 Validation loss 0.028101308271288872 Accuracy 0.71533203125\n",
      "Iteration 32650 Training loss 0.02198570780456066 Validation loss 0.027713406831026077 Accuracy 0.71875\n",
      "Iteration 32660 Training loss 0.023251673206686974 Validation loss 0.0279794130474329 Accuracy 0.7158203125\n",
      "Iteration 32670 Training loss 0.024985363706946373 Validation loss 0.02808312140405178 Accuracy 0.7138671875\n",
      "Iteration 32680 Training loss 0.023303842172026634 Validation loss 0.027875438332557678 Accuracy 0.716796875\n",
      "Iteration 32690 Training loss 0.023594466969370842 Validation loss 0.027761230245232582 Accuracy 0.7177734375\n",
      "Iteration 32700 Training loss 0.023327773436903954 Validation loss 0.027908381074666977 Accuracy 0.71484375\n",
      "Iteration 32710 Training loss 0.024599436670541763 Validation loss 0.027881449088454247 Accuracy 0.7158203125\n",
      "Iteration 32720 Training loss 0.0263622235506773 Validation loss 0.02784699574112892 Accuracy 0.7177734375\n",
      "Iteration 32730 Training loss 0.02430891990661621 Validation loss 0.02793629840016365 Accuracy 0.71533203125\n",
      "Iteration 32740 Training loss 0.023797258734703064 Validation loss 0.028141561895608902 Accuracy 0.7138671875\n",
      "Iteration 32750 Training loss 0.024981053546071053 Validation loss 0.028056824579834938 Accuracy 0.71533203125\n",
      "Iteration 32760 Training loss 0.025041788816452026 Validation loss 0.02813693694770336 Accuracy 0.712890625\n",
      "Iteration 32770 Training loss 0.02678065001964569 Validation loss 0.028184881433844566 Accuracy 0.712890625\n",
      "Iteration 32780 Training loss 0.025719303637742996 Validation loss 0.02777748927474022 Accuracy 0.71728515625\n",
      "Iteration 32790 Training loss 0.026710908859968185 Validation loss 0.027995804324746132 Accuracy 0.71533203125\n",
      "Iteration 32800 Training loss 0.025204559788107872 Validation loss 0.027765166014432907 Accuracy 0.71728515625\n",
      "Iteration 32810 Training loss 0.027442600578069687 Validation loss 0.028041554614901543 Accuracy 0.71533203125\n",
      "Iteration 32820 Training loss 0.02398499846458435 Validation loss 0.027937225997447968 Accuracy 0.716796875\n",
      "Iteration 32830 Training loss 0.023703893646597862 Validation loss 0.0279382411390543 Accuracy 0.71728515625\n",
      "Iteration 32840 Training loss 0.02222573384642601 Validation loss 0.027973320335149765 Accuracy 0.716796875\n",
      "Iteration 32850 Training loss 0.02509160339832306 Validation loss 0.027989627793431282 Accuracy 0.7158203125\n",
      "Iteration 32860 Training loss 0.024558577686548233 Validation loss 0.02778678573668003 Accuracy 0.7177734375\n",
      "Iteration 32870 Training loss 0.024798212572932243 Validation loss 0.027668144553899765 Accuracy 0.71875\n",
      "Iteration 32880 Training loss 0.025071611627936363 Validation loss 0.02777015045285225 Accuracy 0.7177734375\n",
      "Iteration 32890 Training loss 0.02280617691576481 Validation loss 0.027854979038238525 Accuracy 0.71728515625\n",
      "Iteration 32900 Training loss 0.022626006975769997 Validation loss 0.02786685898900032 Accuracy 0.71630859375\n",
      "Iteration 32910 Training loss 0.025401685386896133 Validation loss 0.028113607317209244 Accuracy 0.71337890625\n",
      "Iteration 32920 Training loss 0.02705620601773262 Validation loss 0.028210673481225967 Accuracy 0.7138671875\n",
      "Iteration 32930 Training loss 0.02348044142127037 Validation loss 0.028099117800593376 Accuracy 0.71435546875\n",
      "Iteration 32940 Training loss 0.024354182183742523 Validation loss 0.02756287343800068 Accuracy 0.72021484375\n",
      "Iteration 32950 Training loss 0.02287418022751808 Validation loss 0.028224729001522064 Accuracy 0.712890625\n",
      "Iteration 32960 Training loss 0.023297159001231194 Validation loss 0.02769407629966736 Accuracy 0.71826171875\n",
      "Iteration 32970 Training loss 0.02284875139594078 Validation loss 0.028258319944143295 Accuracy 0.71240234375\n",
      "Iteration 32980 Training loss 0.02786104381084442 Validation loss 0.027684466913342476 Accuracy 0.71826171875\n",
      "Iteration 32990 Training loss 0.026226317510008812 Validation loss 0.028266148641705513 Accuracy 0.71240234375\n",
      "Iteration 33000 Training loss 0.025420866906642914 Validation loss 0.027787603437900543 Accuracy 0.716796875\n",
      "Iteration 33010 Training loss 0.022961614653468132 Validation loss 0.027990495786070824 Accuracy 0.71533203125\n",
      "Iteration 33020 Training loss 0.026924090459942818 Validation loss 0.027940213680267334 Accuracy 0.716796875\n",
      "Iteration 33030 Training loss 0.027327531948685646 Validation loss 0.027904130518436432 Accuracy 0.716796875\n",
      "Iteration 33040 Training loss 0.023775214329361916 Validation loss 0.027784395962953568 Accuracy 0.7177734375\n",
      "Iteration 33050 Training loss 0.022999055683612823 Validation loss 0.027772733941674232 Accuracy 0.7177734375\n",
      "Iteration 33060 Training loss 0.0256599523127079 Validation loss 0.028049446642398834 Accuracy 0.71484375\n",
      "Iteration 33070 Training loss 0.025442011654376984 Validation loss 0.02802203968167305 Accuracy 0.7158203125\n",
      "Iteration 33080 Training loss 0.024996062740683556 Validation loss 0.02798151783645153 Accuracy 0.7158203125\n",
      "Iteration 33090 Training loss 0.027849845588207245 Validation loss 0.02852298691868782 Accuracy 0.71044921875\n",
      "Iteration 33100 Training loss 0.024197671562433243 Validation loss 0.02824615128338337 Accuracy 0.7119140625\n",
      "Iteration 33110 Training loss 0.024861199781298637 Validation loss 0.027826400473713875 Accuracy 0.71728515625\n",
      "Iteration 33120 Training loss 0.02577035315334797 Validation loss 0.028039656579494476 Accuracy 0.71484375\n",
      "Iteration 33130 Training loss 0.024052834138274193 Validation loss 0.027915624901652336 Accuracy 0.7158203125\n",
      "Iteration 33140 Training loss 0.02496497891843319 Validation loss 0.02785239927470684 Accuracy 0.716796875\n",
      "Iteration 33150 Training loss 0.0245338287204504 Validation loss 0.028030820190906525 Accuracy 0.71484375\n",
      "Iteration 33160 Training loss 0.026540832594037056 Validation loss 0.027885476127266884 Accuracy 0.7158203125\n",
      "Iteration 33170 Training loss 0.0257425457239151 Validation loss 0.02817845158278942 Accuracy 0.7138671875\n",
      "Iteration 33180 Training loss 0.021529359742999077 Validation loss 0.028028372675180435 Accuracy 0.71533203125\n",
      "Iteration 33190 Training loss 0.02520114928483963 Validation loss 0.027691630646586418 Accuracy 0.71923828125\n",
      "Iteration 33200 Training loss 0.026714319363236427 Validation loss 0.028107626363635063 Accuracy 0.71435546875\n",
      "Iteration 33210 Training loss 0.02602250687777996 Validation loss 0.027852147817611694 Accuracy 0.716796875\n",
      "Iteration 33220 Training loss 0.02279558591544628 Validation loss 0.027918865904211998 Accuracy 0.716796875\n",
      "Iteration 33230 Training loss 0.025861766189336777 Validation loss 0.027862166985869408 Accuracy 0.71728515625\n",
      "Iteration 33240 Training loss 0.02297302708029747 Validation loss 0.027692977339029312 Accuracy 0.71923828125\n",
      "Iteration 33250 Training loss 0.025535989552736282 Validation loss 0.027783168479800224 Accuracy 0.7177734375\n",
      "Iteration 33260 Training loss 0.02466343715786934 Validation loss 0.027744576334953308 Accuracy 0.71826171875\n",
      "Iteration 33270 Training loss 0.023628508672118187 Validation loss 0.02803715690970421 Accuracy 0.71533203125\n",
      "Iteration 33280 Training loss 0.021667759865522385 Validation loss 0.02809392474591732 Accuracy 0.7138671875\n",
      "Iteration 33290 Training loss 0.024404114112257957 Validation loss 0.027884678915143013 Accuracy 0.716796875\n",
      "Iteration 33300 Training loss 0.02379242144525051 Validation loss 0.027772406116127968 Accuracy 0.71728515625\n",
      "Iteration 33310 Training loss 0.02503853291273117 Validation loss 0.027939757332205772 Accuracy 0.7158203125\n",
      "Iteration 33320 Training loss 0.02451372519135475 Validation loss 0.027782153338193893 Accuracy 0.7177734375\n",
      "Iteration 33330 Training loss 0.024201611056923866 Validation loss 0.027588626369833946 Accuracy 0.71923828125\n",
      "Iteration 33340 Training loss 0.023120597004890442 Validation loss 0.027722714468836784 Accuracy 0.71826171875\n",
      "Iteration 33350 Training loss 0.022390030324459076 Validation loss 0.02756655216217041 Accuracy 0.7197265625\n",
      "Iteration 33360 Training loss 0.0246126651763916 Validation loss 0.027976389974355698 Accuracy 0.71630859375\n",
      "Iteration 33370 Training loss 0.027538279071450233 Validation loss 0.02770545333623886 Accuracy 0.71875\n",
      "Iteration 33380 Training loss 0.026523029431700706 Validation loss 0.027823278680443764 Accuracy 0.71728515625\n",
      "Iteration 33390 Training loss 0.02459827996790409 Validation loss 0.027815960347652435 Accuracy 0.71728515625\n",
      "Iteration 33400 Training loss 0.021020935848355293 Validation loss 0.02782745100557804 Accuracy 0.7177734375\n",
      "Iteration 33410 Training loss 0.02259029634296894 Validation loss 0.027923500165343285 Accuracy 0.71630859375\n",
      "Iteration 33420 Training loss 0.023058608174324036 Validation loss 0.027807077392935753 Accuracy 0.716796875\n",
      "Iteration 33430 Training loss 0.024528631940484047 Validation loss 0.027904095128178596 Accuracy 0.7158203125\n",
      "Iteration 33440 Training loss 0.023920638486742973 Validation loss 0.027737382799386978 Accuracy 0.71826171875\n",
      "Iteration 33450 Training loss 0.027111323550343513 Validation loss 0.02782288007438183 Accuracy 0.716796875\n",
      "Iteration 33460 Training loss 0.024967089295387268 Validation loss 0.02811683528125286 Accuracy 0.71435546875\n",
      "Iteration 33470 Training loss 0.02491569146513939 Validation loss 0.02775038592517376 Accuracy 0.7177734375\n",
      "Iteration 33480 Training loss 0.027311988174915314 Validation loss 0.027651991695165634 Accuracy 0.71826171875\n",
      "Iteration 33490 Training loss 0.023599930107593536 Validation loss 0.02818353660404682 Accuracy 0.712890625\n",
      "Iteration 33500 Training loss 0.024106688797473907 Validation loss 0.027592632919549942 Accuracy 0.71923828125\n",
      "Iteration 33510 Training loss 0.023374034091830254 Validation loss 0.027912098914384842 Accuracy 0.7158203125\n",
      "Iteration 33520 Training loss 0.022997168824076653 Validation loss 0.02775985188782215 Accuracy 0.71728515625\n",
      "Iteration 33530 Training loss 0.024151336401700974 Validation loss 0.027783289551734924 Accuracy 0.7177734375\n",
      "Iteration 33540 Training loss 0.025703398510813713 Validation loss 0.02771693281829357 Accuracy 0.71875\n",
      "Iteration 33550 Training loss 0.02222246304154396 Validation loss 0.027597257867455482 Accuracy 0.71875\n",
      "Iteration 33560 Training loss 0.022517038509249687 Validation loss 0.027907511219382286 Accuracy 0.7158203125\n",
      "Iteration 33570 Training loss 0.02628292143344879 Validation loss 0.0263585913926363 Accuracy 0.72998046875\n",
      "Iteration 33580 Training loss 0.019724251702427864 Validation loss 0.027159422636032104 Accuracy 0.7216796875\n",
      "Iteration 33590 Training loss 0.02186894789338112 Validation loss 0.025691132992506027 Accuracy 0.7373046875\n",
      "Iteration 33600 Training loss 0.020805848762392998 Validation loss 0.025708548724651337 Accuracy 0.73681640625\n",
      "Iteration 33610 Training loss 0.025092383846640587 Validation loss 0.02530696801841259 Accuracy 0.7412109375\n",
      "Iteration 33620 Training loss 0.021840915083885193 Validation loss 0.02510436438024044 Accuracy 0.7431640625\n",
      "Iteration 33630 Training loss 0.022380037233233452 Validation loss 0.027206439524888992 Accuracy 0.72216796875\n",
      "Iteration 33640 Training loss 0.02129979245364666 Validation loss 0.02562578022480011 Accuracy 0.7373046875\n",
      "Iteration 33650 Training loss 0.023722870275378227 Validation loss 0.02486421726644039 Accuracy 0.74658203125\n",
      "Iteration 33660 Training loss 0.020736172795295715 Validation loss 0.023949898779392242 Accuracy 0.75537109375\n",
      "Iteration 33670 Training loss 0.02279599756002426 Validation loss 0.024962108582258224 Accuracy 0.7451171875\n",
      "Iteration 33680 Training loss 0.02418229728937149 Validation loss 0.024630550295114517 Accuracy 0.74755859375\n",
      "Iteration 33690 Training loss 0.020169321447610855 Validation loss 0.024850189685821533 Accuracy 0.74560546875\n",
      "Iteration 33700 Training loss 0.01795324869453907 Validation loss 0.023237576708197594 Accuracy 0.76220703125\n",
      "Iteration 33710 Training loss 0.020157117396593094 Validation loss 0.024551548063755035 Accuracy 0.74951171875\n",
      "Iteration 33720 Training loss 0.018275419250130653 Validation loss 0.02352163940668106 Accuracy 0.759765625\n",
      "Iteration 33730 Training loss 0.01806553825736046 Validation loss 0.02348467707633972 Accuracy 0.75927734375\n",
      "Iteration 33740 Training loss 0.02154930867254734 Validation loss 0.024404363706707954 Accuracy 0.75\n",
      "Iteration 33750 Training loss 0.02210686169564724 Validation loss 0.02380409650504589 Accuracy 0.7568359375\n",
      "Iteration 33760 Training loss 0.02335459366440773 Validation loss 0.023271651938557625 Accuracy 0.76171875\n",
      "Iteration 33770 Training loss 0.018311617895960808 Validation loss 0.02329101786017418 Accuracy 0.76171875\n",
      "Iteration 33780 Training loss 0.022181600332260132 Validation loss 0.024601083248853683 Accuracy 0.7490234375\n",
      "Iteration 33790 Training loss 0.021068938076496124 Validation loss 0.023314477875828743 Accuracy 0.76123046875\n",
      "Iteration 33800 Training loss 0.017244629561901093 Validation loss 0.023121897131204605 Accuracy 0.76416015625\n",
      "Iteration 33810 Training loss 0.02043065056204796 Validation loss 0.022889144718647003 Accuracy 0.76513671875\n",
      "Iteration 33820 Training loss 0.02077961526811123 Validation loss 0.022616630420088768 Accuracy 0.76953125\n",
      "Iteration 33830 Training loss 0.01692957617342472 Validation loss 0.022966567426919937 Accuracy 0.765625\n",
      "Iteration 33840 Training loss 0.018816551193594933 Validation loss 0.023016029968857765 Accuracy 0.76513671875\n",
      "Iteration 33850 Training loss 0.017880989238619804 Validation loss 0.02251681499183178 Accuracy 0.76904296875\n",
      "Iteration 33860 Training loss 0.01797964796423912 Validation loss 0.022636014968156815 Accuracy 0.7685546875\n",
      "Iteration 33870 Training loss 0.01662214659154415 Validation loss 0.022864002734422684 Accuracy 0.76611328125\n",
      "Iteration 33880 Training loss 0.01868743635714054 Validation loss 0.022285595536231995 Accuracy 0.77197265625\n",
      "Iteration 33890 Training loss 0.017162561416625977 Validation loss 0.02320900745689869 Accuracy 0.76171875\n",
      "Iteration 33900 Training loss 0.01946347951889038 Validation loss 0.022283438593149185 Accuracy 0.77197265625\n",
      "Iteration 33910 Training loss 0.0210226159542799 Validation loss 0.022713517770171165 Accuracy 0.76708984375\n",
      "Iteration 33920 Training loss 0.02000989019870758 Validation loss 0.0226339939981699 Accuracy 0.767578125\n",
      "Iteration 33930 Training loss 0.02009621635079384 Validation loss 0.022707674652338028 Accuracy 0.767578125\n",
      "Iteration 33940 Training loss 0.017993835732340813 Validation loss 0.022155635058879852 Accuracy 0.77294921875\n",
      "Iteration 33950 Training loss 0.021282542496919632 Validation loss 0.022492000833153725 Accuracy 0.77001953125\n",
      "Iteration 33960 Training loss 0.018668754026293755 Validation loss 0.022476714104413986 Accuracy 0.77001953125\n",
      "Iteration 33970 Training loss 0.019088923931121826 Validation loss 0.02276526391506195 Accuracy 0.76708984375\n",
      "Iteration 33980 Training loss 0.020073208957910538 Validation loss 0.023162325844168663 Accuracy 0.763671875\n",
      "Iteration 33990 Training loss 0.02246522158384323 Validation loss 0.023207787424325943 Accuracy 0.76123046875\n",
      "Iteration 34000 Training loss 0.020064575597643852 Validation loss 0.023840682581067085 Accuracy 0.7548828125\n",
      "Iteration 34010 Training loss 0.01959088444709778 Validation loss 0.022850526496767998 Accuracy 0.7666015625\n",
      "Iteration 34020 Training loss 0.019929297268390656 Validation loss 0.022452156990766525 Accuracy 0.77099609375\n",
      "Iteration 34030 Training loss 0.017587684094905853 Validation loss 0.022389836609363556 Accuracy 0.77099609375\n",
      "Iteration 34040 Training loss 0.019390981644392014 Validation loss 0.022753050550818443 Accuracy 0.767578125\n",
      "Iteration 34050 Training loss 0.020509101450443268 Validation loss 0.02265804633498192 Accuracy 0.767578125\n",
      "Iteration 34060 Training loss 0.020191162824630737 Validation loss 0.02312350459396839 Accuracy 0.76318359375\n",
      "Iteration 34070 Training loss 0.022527923807501793 Validation loss 0.0243084616959095 Accuracy 0.75244140625\n",
      "Iteration 34080 Training loss 0.01661629043519497 Validation loss 0.022101225331425667 Accuracy 0.77392578125\n",
      "Iteration 34090 Training loss 0.022498933598399162 Validation loss 0.02257739193737507 Accuracy 0.7685546875\n",
      "Iteration 34100 Training loss 0.018020907416939735 Validation loss 0.023172898218035698 Accuracy 0.763671875\n",
      "Iteration 34110 Training loss 0.01808641292154789 Validation loss 0.02230392023921013 Accuracy 0.77197265625\n",
      "Iteration 34120 Training loss 0.01639307104051113 Validation loss 0.021840104833245277 Accuracy 0.7763671875\n",
      "Iteration 34130 Training loss 0.01827698014676571 Validation loss 0.022249696776270866 Accuracy 0.7724609375\n",
      "Iteration 34140 Training loss 0.020903436467051506 Validation loss 0.02262621559202671 Accuracy 0.76904296875\n",
      "Iteration 34150 Training loss 0.01970651187002659 Validation loss 0.022337494418025017 Accuracy 0.77197265625\n",
      "Iteration 34160 Training loss 0.02103240229189396 Validation loss 0.022017331793904305 Accuracy 0.77490234375\n",
      "Iteration 34170 Training loss 0.01951485499739647 Validation loss 0.022938281297683716 Accuracy 0.765625\n",
      "Iteration 34180 Training loss 0.021098004654049873 Validation loss 0.022304965183138847 Accuracy 0.771484375\n",
      "Iteration 34190 Training loss 0.020445961505174637 Validation loss 0.02254837192595005 Accuracy 0.7685546875\n",
      "Iteration 34200 Training loss 0.020784590393304825 Validation loss 0.022800615057349205 Accuracy 0.7666015625\n",
      "Iteration 34210 Training loss 0.02043299935758114 Validation loss 0.02209005318582058 Accuracy 0.7744140625\n",
      "Iteration 34220 Training loss 0.023035038262605667 Validation loss 0.02235533483326435 Accuracy 0.77197265625\n",
      "Iteration 34230 Training loss 0.01572713628411293 Validation loss 0.023549286648631096 Accuracy 0.759765625\n",
      "Iteration 34240 Training loss 0.01657879538834095 Validation loss 0.02219557762145996 Accuracy 0.7734375\n",
      "Iteration 34250 Training loss 0.01615295559167862 Validation loss 0.022254357114434242 Accuracy 0.7724609375\n",
      "Iteration 34260 Training loss 0.022898640483617783 Validation loss 0.022322729229927063 Accuracy 0.7724609375\n",
      "Iteration 34270 Training loss 0.019719498232007027 Validation loss 0.02232460305094719 Accuracy 0.771484375\n",
      "Iteration 34280 Training loss 0.019421875476837158 Validation loss 0.0220830999314785 Accuracy 0.7744140625\n",
      "Iteration 34290 Training loss 0.018148403614759445 Validation loss 0.021981997415423393 Accuracy 0.775390625\n",
      "Iteration 34300 Training loss 0.017481984570622444 Validation loss 0.022198304533958435 Accuracy 0.7734375\n",
      "Iteration 34310 Training loss 0.018796581774950027 Validation loss 0.022132324054837227 Accuracy 0.7744140625\n",
      "Iteration 34320 Training loss 0.017072239890694618 Validation loss 0.021923476830124855 Accuracy 0.77685546875\n",
      "Iteration 34330 Training loss 0.01643327996134758 Validation loss 0.022233163937926292 Accuracy 0.77294921875\n",
      "Iteration 34340 Training loss 0.017513765022158623 Validation loss 0.021969985216856003 Accuracy 0.77490234375\n",
      "Iteration 34350 Training loss 0.01728236675262451 Validation loss 0.022083304822444916 Accuracy 0.77490234375\n",
      "Iteration 34360 Training loss 0.014984304085373878 Validation loss 0.022587470710277557 Accuracy 0.7685546875\n",
      "Iteration 34370 Training loss 0.018061993643641472 Validation loss 0.023496756330132484 Accuracy 0.76025390625\n",
      "Iteration 34380 Training loss 0.019343681633472443 Validation loss 0.02230430580675602 Accuracy 0.77197265625\n",
      "Iteration 34390 Training loss 0.021143099293112755 Validation loss 0.022178979590535164 Accuracy 0.77294921875\n",
      "Iteration 34400 Training loss 0.02052922546863556 Validation loss 0.022556791082024574 Accuracy 0.76904296875\n",
      "Iteration 34410 Training loss 0.015978623181581497 Validation loss 0.022150970995426178 Accuracy 0.7734375\n",
      "Iteration 34420 Training loss 0.018814533948898315 Validation loss 0.021752389147877693 Accuracy 0.77783203125\n",
      "Iteration 34430 Training loss 0.01889370009303093 Validation loss 0.022967912256717682 Accuracy 0.76513671875\n",
      "Iteration 34440 Training loss 0.019064968451857567 Validation loss 0.022597752511501312 Accuracy 0.76904296875\n",
      "Iteration 34450 Training loss 0.016115473583340645 Validation loss 0.022442208603024483 Accuracy 0.7705078125\n",
      "Iteration 34460 Training loss 0.021669525653123856 Validation loss 0.022337768226861954 Accuracy 0.771484375\n",
      "Iteration 34470 Training loss 0.018849194049835205 Validation loss 0.022178107872605324 Accuracy 0.7724609375\n",
      "Iteration 34480 Training loss 0.020856162533164024 Validation loss 0.022447191178798676 Accuracy 0.76953125\n",
      "Iteration 34490 Training loss 0.017177598550915718 Validation loss 0.022070150822401047 Accuracy 0.77392578125\n",
      "Iteration 34500 Training loss 0.01786968670785427 Validation loss 0.021875588223338127 Accuracy 0.7763671875\n",
      "Iteration 34510 Training loss 0.01870027929544449 Validation loss 0.022185252979397774 Accuracy 0.77392578125\n",
      "Iteration 34520 Training loss 0.018830975517630577 Validation loss 0.023335356265306473 Accuracy 0.76123046875\n",
      "Iteration 34530 Training loss 0.01769941858947277 Validation loss 0.021898679435253143 Accuracy 0.77587890625\n",
      "Iteration 34540 Training loss 0.015876097604632378 Validation loss 0.022446967661380768 Accuracy 0.7705078125\n",
      "Iteration 34550 Training loss 0.018604328855872154 Validation loss 0.022250205278396606 Accuracy 0.77197265625\n",
      "Iteration 34560 Training loss 0.018938979133963585 Validation loss 0.02208162471652031 Accuracy 0.77392578125\n",
      "Iteration 34570 Training loss 0.019909344613552094 Validation loss 0.022227909415960312 Accuracy 0.7724609375\n",
      "Iteration 34580 Training loss 0.018623651936650276 Validation loss 0.021943671628832817 Accuracy 0.77587890625\n",
      "Iteration 34590 Training loss 0.018902944400906563 Validation loss 0.022868070751428604 Accuracy 0.765625\n",
      "Iteration 34600 Training loss 0.017884936183691025 Validation loss 0.02273602969944477 Accuracy 0.7666015625\n",
      "Iteration 34610 Training loss 0.018490051850676537 Validation loss 0.02222323603928089 Accuracy 0.77197265625\n",
      "Iteration 34620 Training loss 0.019459204748272896 Validation loss 0.021756356582045555 Accuracy 0.77685546875\n",
      "Iteration 34630 Training loss 0.018479999154806137 Validation loss 0.021846650168299675 Accuracy 0.775390625\n",
      "Iteration 34640 Training loss 0.022168604657053947 Validation loss 0.023868165910243988 Accuracy 0.75537109375\n",
      "Iteration 34650 Training loss 0.016663022339344025 Validation loss 0.0218565221875906 Accuracy 0.77685546875\n",
      "Iteration 34660 Training loss 0.016359612345695496 Validation loss 0.02232341468334198 Accuracy 0.771484375\n",
      "Iteration 34670 Training loss 0.015577021054923534 Validation loss 0.021689599379897118 Accuracy 0.77734375\n",
      "Iteration 34680 Training loss 0.019333243370056152 Validation loss 0.02222655899822712 Accuracy 0.7724609375\n",
      "Iteration 34690 Training loss 0.01788456365466118 Validation loss 0.021532410755753517 Accuracy 0.77880859375\n",
      "Iteration 34700 Training loss 0.017153244465589523 Validation loss 0.022030599415302277 Accuracy 0.77392578125\n",
      "Iteration 34710 Training loss 0.018263472244143486 Validation loss 0.02165021002292633 Accuracy 0.779296875\n",
      "Iteration 34720 Training loss 0.0209486186504364 Validation loss 0.022045692428946495 Accuracy 0.77392578125\n",
      "Iteration 34730 Training loss 0.01834225282073021 Validation loss 0.022503476589918137 Accuracy 0.7685546875\n",
      "Iteration 34740 Training loss 0.017849694937467575 Validation loss 0.021724902093410492 Accuracy 0.77783203125\n",
      "Iteration 34750 Training loss 0.016706177964806557 Validation loss 0.021811004728078842 Accuracy 0.77734375\n",
      "Iteration 34760 Training loss 0.017398860305547714 Validation loss 0.0220597255975008 Accuracy 0.77392578125\n",
      "Iteration 34770 Training loss 0.018706679344177246 Validation loss 0.022087348625063896 Accuracy 0.775390625\n",
      "Iteration 34780 Training loss 0.01778663694858551 Validation loss 0.022038795053958893 Accuracy 0.77490234375\n",
      "Iteration 34790 Training loss 0.018046468496322632 Validation loss 0.02208320051431656 Accuracy 0.7744140625\n",
      "Iteration 34800 Training loss 0.018258946016430855 Validation loss 0.0215156190097332 Accuracy 0.77978515625\n",
      "Iteration 34810 Training loss 0.018411172553896904 Validation loss 0.022216178476810455 Accuracy 0.77294921875\n",
      "Iteration 34820 Training loss 0.01871435157954693 Validation loss 0.022187313064932823 Accuracy 0.77099609375\n",
      "Iteration 34830 Training loss 0.0213351771235466 Validation loss 0.022067084908485413 Accuracy 0.7734375\n",
      "Iteration 34840 Training loss 0.016055546700954437 Validation loss 0.022116243839263916 Accuracy 0.7734375\n",
      "Iteration 34850 Training loss 0.019116219133138657 Validation loss 0.022621240466833115 Accuracy 0.7685546875\n",
      "Iteration 34860 Training loss 0.02163236401975155 Validation loss 0.02294783666729927 Accuracy 0.765625\n",
      "Iteration 34870 Training loss 0.018089883029460907 Validation loss 0.021863378584384918 Accuracy 0.77587890625\n",
      "Iteration 34880 Training loss 0.018770482391119003 Validation loss 0.021772131323814392 Accuracy 0.7763671875\n",
      "Iteration 34890 Training loss 0.019461415708065033 Validation loss 0.021701620891690254 Accuracy 0.77783203125\n",
      "Iteration 34900 Training loss 0.016436807811260223 Validation loss 0.02213299088180065 Accuracy 0.7724609375\n",
      "Iteration 34910 Training loss 0.01704060286283493 Validation loss 0.02257884293794632 Accuracy 0.7685546875\n",
      "Iteration 34920 Training loss 0.02049936354160309 Validation loss 0.022330673411488533 Accuracy 0.77099609375\n",
      "Iteration 34930 Training loss 0.021312756463885307 Validation loss 0.022009868174791336 Accuracy 0.7744140625\n",
      "Iteration 34940 Training loss 0.017132863402366638 Validation loss 0.02200460620224476 Accuracy 0.775390625\n",
      "Iteration 34950 Training loss 0.01847689412534237 Validation loss 0.02216210402548313 Accuracy 0.7734375\n",
      "Iteration 34960 Training loss 0.016920777037739754 Validation loss 0.021730465814471245 Accuracy 0.7783203125\n",
      "Iteration 34970 Training loss 0.018234921619296074 Validation loss 0.021864313632249832 Accuracy 0.77734375\n",
      "Iteration 34980 Training loss 0.017099987715482712 Validation loss 0.021384965628385544 Accuracy 0.7822265625\n",
      "Iteration 34990 Training loss 0.021774185821413994 Validation loss 0.02208327129483223 Accuracy 0.7744140625\n",
      "Iteration 35000 Training loss 0.019124682992696762 Validation loss 0.022138886153697968 Accuracy 0.77294921875\n",
      "Iteration 35010 Training loss 0.018939221277832985 Validation loss 0.02198714390397072 Accuracy 0.77490234375\n",
      "Iteration 35020 Training loss 0.01845952682197094 Validation loss 0.021886469796299934 Accuracy 0.7763671875\n",
      "Iteration 35030 Training loss 0.01942029781639576 Validation loss 0.02190449833869934 Accuracy 0.7763671875\n",
      "Iteration 35040 Training loss 0.017055507749319077 Validation loss 0.021802641451358795 Accuracy 0.77783203125\n",
      "Iteration 35050 Training loss 0.018743492662906647 Validation loss 0.022469254210591316 Accuracy 0.77001953125\n",
      "Iteration 35060 Training loss 0.015693936496973038 Validation loss 0.021695220842957497 Accuracy 0.77783203125\n",
      "Iteration 35070 Training loss 0.018702397122979164 Validation loss 0.021675584837794304 Accuracy 0.7783203125\n",
      "Iteration 35080 Training loss 0.019420962780714035 Validation loss 0.022121833637356758 Accuracy 0.77392578125\n",
      "Iteration 35090 Training loss 0.01845286600291729 Validation loss 0.022377870976924896 Accuracy 0.7705078125\n",
      "Iteration 35100 Training loss 0.01743193529546261 Validation loss 0.02266870252788067 Accuracy 0.76806640625\n",
      "Iteration 35110 Training loss 0.020402034744620323 Validation loss 0.022078579291701317 Accuracy 0.77392578125\n",
      "Iteration 35120 Training loss 0.018171587958931923 Validation loss 0.021893875673413277 Accuracy 0.7763671875\n",
      "Iteration 35130 Training loss 0.02028603106737137 Validation loss 0.02175251394510269 Accuracy 0.77783203125\n",
      "Iteration 35140 Training loss 0.017430050298571587 Validation loss 0.02174660749733448 Accuracy 0.77734375\n",
      "Iteration 35150 Training loss 0.020335756242275238 Validation loss 0.022093407809734344 Accuracy 0.7734375\n",
      "Iteration 35160 Training loss 0.015978515148162842 Validation loss 0.021897049620747566 Accuracy 0.77490234375\n",
      "Iteration 35170 Training loss 0.017976989969611168 Validation loss 0.023128746077418327 Accuracy 0.7626953125\n",
      "Iteration 35180 Training loss 0.018103985115885735 Validation loss 0.022320985794067383 Accuracy 0.771484375\n",
      "Iteration 35190 Training loss 0.017056835815310478 Validation loss 0.02173350565135479 Accuracy 0.77734375\n",
      "Iteration 35200 Training loss 0.018252339214086533 Validation loss 0.023102005943655968 Accuracy 0.76318359375\n",
      "Iteration 35210 Training loss 0.019661227241158485 Validation loss 0.022353682667016983 Accuracy 0.771484375\n",
      "Iteration 35220 Training loss 0.018895767629146576 Validation loss 0.02164340764284134 Accuracy 0.77734375\n",
      "Iteration 35230 Training loss 0.01928248628973961 Validation loss 0.02184637077152729 Accuracy 0.775390625\n",
      "Iteration 35240 Training loss 0.018474742770195007 Validation loss 0.021760134026408195 Accuracy 0.77685546875\n",
      "Iteration 35250 Training loss 0.017025090754032135 Validation loss 0.021794315427541733 Accuracy 0.77685546875\n",
      "Iteration 35260 Training loss 0.016323547810316086 Validation loss 0.021923517808318138 Accuracy 0.77587890625\n",
      "Iteration 35270 Training loss 0.017376694828271866 Validation loss 0.021834643557667732 Accuracy 0.77734375\n",
      "Iteration 35280 Training loss 0.01768338307738304 Validation loss 0.021846942603588104 Accuracy 0.77734375\n",
      "Iteration 35290 Training loss 0.02021052874624729 Validation loss 0.022138040512800217 Accuracy 0.7744140625\n",
      "Iteration 35300 Training loss 0.016787389293313026 Validation loss 0.021722707897424698 Accuracy 0.77734375\n",
      "Iteration 35310 Training loss 0.017610488459467888 Validation loss 0.021622758358716965 Accuracy 0.779296875\n",
      "Iteration 35320 Training loss 0.017405539751052856 Validation loss 0.021828515455126762 Accuracy 0.77685546875\n",
      "Iteration 35330 Training loss 0.021958770230412483 Validation loss 0.02207956090569496 Accuracy 0.77392578125\n",
      "Iteration 35340 Training loss 0.014773045666515827 Validation loss 0.022265374660491943 Accuracy 0.771484375\n",
      "Iteration 35350 Training loss 0.021336736157536507 Validation loss 0.023542119190096855 Accuracy 0.75732421875\n",
      "Iteration 35360 Training loss 0.020870910957455635 Validation loss 0.023333635181188583 Accuracy 0.76220703125\n",
      "Iteration 35370 Training loss 0.018770400434732437 Validation loss 0.021812669932842255 Accuracy 0.77685546875\n",
      "Iteration 35380 Training loss 0.017043039202690125 Validation loss 0.021909847855567932 Accuracy 0.7763671875\n",
      "Iteration 35390 Training loss 0.017446234822273254 Validation loss 0.021530404686927795 Accuracy 0.7802734375\n",
      "Iteration 35400 Training loss 0.016218259930610657 Validation loss 0.022116737440228462 Accuracy 0.77392578125\n",
      "Iteration 35410 Training loss 0.01772667095065117 Validation loss 0.02174399420619011 Accuracy 0.77783203125\n",
      "Iteration 35420 Training loss 0.01938355155289173 Validation loss 0.02195178158581257 Accuracy 0.775390625\n",
      "Iteration 35430 Training loss 0.021427856758236885 Validation loss 0.023945661261677742 Accuracy 0.75439453125\n",
      "Iteration 35440 Training loss 0.018915390595793724 Validation loss 0.0215370524674654 Accuracy 0.7802734375\n",
      "Iteration 35450 Training loss 0.019004182890057564 Validation loss 0.021666986867785454 Accuracy 0.77734375\n",
      "Iteration 35460 Training loss 0.01770167611539364 Validation loss 0.022838270291686058 Accuracy 0.76611328125\n",
      "Iteration 35470 Training loss 0.021205609664320946 Validation loss 0.022052645683288574 Accuracy 0.77490234375\n",
      "Iteration 35480 Training loss 0.01853807643055916 Validation loss 0.02197682298719883 Accuracy 0.77490234375\n",
      "Iteration 35490 Training loss 0.019981445744633675 Validation loss 0.02200787514448166 Accuracy 0.7744140625\n",
      "Iteration 35500 Training loss 0.017996449023485184 Validation loss 0.022863922640681267 Accuracy 0.76513671875\n",
      "Iteration 35510 Training loss 0.015358998440206051 Validation loss 0.02210882678627968 Accuracy 0.77294921875\n",
      "Iteration 35520 Training loss 0.019299816340208054 Validation loss 0.021624209359288216 Accuracy 0.779296875\n",
      "Iteration 35530 Training loss 0.01748591475188732 Validation loss 0.022031359374523163 Accuracy 0.7744140625\n",
      "Iteration 35540 Training loss 0.016267763450741768 Validation loss 0.02189539559185505 Accuracy 0.775390625\n",
      "Iteration 35550 Training loss 0.017208142206072807 Validation loss 0.02181023173034191 Accuracy 0.7763671875\n",
      "Iteration 35560 Training loss 0.018571261316537857 Validation loss 0.02242407202720642 Accuracy 0.7705078125\n",
      "Iteration 35570 Training loss 0.017863299697637558 Validation loss 0.022303173318505287 Accuracy 0.771484375\n",
      "Iteration 35580 Training loss 0.017715569585561752 Validation loss 0.02215856872498989 Accuracy 0.77294921875\n",
      "Iteration 35590 Training loss 0.02007703110575676 Validation loss 0.02222885750234127 Accuracy 0.77197265625\n",
      "Iteration 35600 Training loss 0.018441271036863327 Validation loss 0.021649884060025215 Accuracy 0.77783203125\n",
      "Iteration 35610 Training loss 0.01720566116273403 Validation loss 0.021592367440462112 Accuracy 0.7783203125\n",
      "Iteration 35620 Training loss 0.0172844547778368 Validation loss 0.02201165445148945 Accuracy 0.7744140625\n",
      "Iteration 35630 Training loss 0.020415015518665314 Validation loss 0.021817274391651154 Accuracy 0.77685546875\n",
      "Iteration 35640 Training loss 0.019088072702288628 Validation loss 0.021803831681609154 Accuracy 0.77783203125\n",
      "Iteration 35650 Training loss 0.016422713175415993 Validation loss 0.021730801090598106 Accuracy 0.77734375\n",
      "Iteration 35660 Training loss 0.01760999485850334 Validation loss 0.021926274523139 Accuracy 0.7744140625\n",
      "Iteration 35670 Training loss 0.01824873499572277 Validation loss 0.022081494331359863 Accuracy 0.7734375\n",
      "Iteration 35680 Training loss 0.016300179064273834 Validation loss 0.021978072822093964 Accuracy 0.77587890625\n",
      "Iteration 35690 Training loss 0.02186017856001854 Validation loss 0.022109510377049446 Accuracy 0.77392578125\n",
      "Iteration 35700 Training loss 0.018417587503790855 Validation loss 0.021578319370746613 Accuracy 0.77978515625\n",
      "Iteration 35710 Training loss 0.017125342041254044 Validation loss 0.022234130650758743 Accuracy 0.7724609375\n",
      "Iteration 35720 Training loss 0.018629631027579308 Validation loss 0.022028494626283646 Accuracy 0.7744140625\n",
      "Iteration 35730 Training loss 0.017993733286857605 Validation loss 0.021650224924087524 Accuracy 0.77880859375\n",
      "Iteration 35740 Training loss 0.017187178134918213 Validation loss 0.023198675364255905 Accuracy 0.76220703125\n",
      "Iteration 35750 Training loss 0.01627367176115513 Validation loss 0.021604834124445915 Accuracy 0.77880859375\n",
      "Iteration 35760 Training loss 0.015281051397323608 Validation loss 0.02194947749376297 Accuracy 0.775390625\n",
      "Iteration 35770 Training loss 0.018673080950975418 Validation loss 0.021768715232610703 Accuracy 0.77783203125\n",
      "Iteration 35780 Training loss 0.016327446326613426 Validation loss 0.02178180031478405 Accuracy 0.77734375\n",
      "Iteration 35790 Training loss 0.017303336411714554 Validation loss 0.02215585671365261 Accuracy 0.77197265625\n",
      "Iteration 35800 Training loss 0.015761660411953926 Validation loss 0.021740037947893143 Accuracy 0.7763671875\n",
      "Iteration 35810 Training loss 0.017595278099179268 Validation loss 0.02177620120346546 Accuracy 0.77734375\n",
      "Iteration 35820 Training loss 0.019245360046625137 Validation loss 0.022208837792277336 Accuracy 0.7724609375\n",
      "Iteration 35830 Training loss 0.016745541244745255 Validation loss 0.021709339693188667 Accuracy 0.77734375\n",
      "Iteration 35840 Training loss 0.01651422306895256 Validation loss 0.021542176604270935 Accuracy 0.7783203125\n",
      "Iteration 35850 Training loss 0.019547881558537483 Validation loss 0.023408303037285805 Accuracy 0.7607421875\n",
      "Iteration 35860 Training loss 0.01830950379371643 Validation loss 0.021555233746767044 Accuracy 0.77978515625\n",
      "Iteration 35870 Training loss 0.01700740121304989 Validation loss 0.021755553781986237 Accuracy 0.7763671875\n",
      "Iteration 35880 Training loss 0.0178530290722847 Validation loss 0.021657031029462814 Accuracy 0.77783203125\n",
      "Iteration 35890 Training loss 0.017815129831433296 Validation loss 0.02230459451675415 Accuracy 0.771484375\n",
      "Iteration 35900 Training loss 0.014148064889013767 Validation loss 0.022954242303967476 Accuracy 0.7646484375\n",
      "Iteration 35910 Training loss 0.020762905478477478 Validation loss 0.02196146920323372 Accuracy 0.775390625\n",
      "Iteration 35920 Training loss 0.016101032495498657 Validation loss 0.021678665652871132 Accuracy 0.77880859375\n",
      "Iteration 35930 Training loss 0.01884819008409977 Validation loss 0.022853832691907883 Accuracy 0.76611328125\n",
      "Iteration 35940 Training loss 0.017245089635252953 Validation loss 0.022511860355734825 Accuracy 0.77001953125\n",
      "Iteration 35950 Training loss 0.019064178690314293 Validation loss 0.021860145032405853 Accuracy 0.77587890625\n",
      "Iteration 35960 Training loss 0.017336690798401833 Validation loss 0.021783383563160896 Accuracy 0.77734375\n",
      "Iteration 35970 Training loss 0.01991298981010914 Validation loss 0.0222800113260746 Accuracy 0.77197265625\n",
      "Iteration 35980 Training loss 0.022176319733262062 Validation loss 0.021557284519076347 Accuracy 0.7802734375\n",
      "Iteration 35990 Training loss 0.016910554841160774 Validation loss 0.02216489426791668 Accuracy 0.7724609375\n",
      "Iteration 36000 Training loss 0.01597026363015175 Validation loss 0.021610619500279427 Accuracy 0.7783203125\n",
      "Iteration 36010 Training loss 0.01998443901538849 Validation loss 0.02189592644572258 Accuracy 0.77685546875\n",
      "Iteration 36020 Training loss 0.016652248799800873 Validation loss 0.021574711427092552 Accuracy 0.779296875\n",
      "Iteration 36030 Training loss 0.019066715613007545 Validation loss 0.02170521393418312 Accuracy 0.77783203125\n",
      "Iteration 36040 Training loss 0.018608825281262398 Validation loss 0.022297129034996033 Accuracy 0.77197265625\n",
      "Iteration 36050 Training loss 0.01800946705043316 Validation loss 0.021848583593964577 Accuracy 0.7763671875\n",
      "Iteration 36060 Training loss 0.01700728014111519 Validation loss 0.021901996806263924 Accuracy 0.77587890625\n",
      "Iteration 36070 Training loss 0.01832181215286255 Validation loss 0.022639157250523567 Accuracy 0.76904296875\n",
      "Iteration 36080 Training loss 0.018888406455516815 Validation loss 0.022176507860422134 Accuracy 0.7734375\n",
      "Iteration 36090 Training loss 0.018291903659701347 Validation loss 0.021674267947673798 Accuracy 0.7783203125\n",
      "Iteration 36100 Training loss 0.019365841522812843 Validation loss 0.022392578423023224 Accuracy 0.771484375\n",
      "Iteration 36110 Training loss 0.01699838414788246 Validation loss 0.02187836915254593 Accuracy 0.7763671875\n",
      "Iteration 36120 Training loss 0.021469442173838615 Validation loss 0.02177906595170498 Accuracy 0.77734375\n",
      "Iteration 36130 Training loss 0.015038810670375824 Validation loss 0.022233838215470314 Accuracy 0.7724609375\n",
      "Iteration 36140 Training loss 0.015694033354520798 Validation loss 0.02165522240102291 Accuracy 0.7783203125\n",
      "Iteration 36150 Training loss 0.01989436149597168 Validation loss 0.02154817059636116 Accuracy 0.7802734375\n",
      "Iteration 36160 Training loss 0.016622381284832954 Validation loss 0.021543236449360847 Accuracy 0.779296875\n",
      "Iteration 36170 Training loss 0.018685517832636833 Validation loss 0.02194887399673462 Accuracy 0.77587890625\n",
      "Iteration 36180 Training loss 0.0171563308686018 Validation loss 0.02176753804087639 Accuracy 0.77685546875\n",
      "Iteration 36190 Training loss 0.019403764978051186 Validation loss 0.021662145853042603 Accuracy 0.77880859375\n",
      "Iteration 36200 Training loss 0.015898237004876137 Validation loss 0.02205638401210308 Accuracy 0.775390625\n",
      "Iteration 36210 Training loss 0.01955622248351574 Validation loss 0.02238198183476925 Accuracy 0.77099609375\n",
      "Iteration 36220 Training loss 0.018730631098151207 Validation loss 0.02210399881005287 Accuracy 0.77392578125\n",
      "Iteration 36230 Training loss 0.013983351178467274 Validation loss 0.02167939953505993 Accuracy 0.779296875\n",
      "Iteration 36240 Training loss 0.015942895784974098 Validation loss 0.022063875570893288 Accuracy 0.7744140625\n",
      "Iteration 36250 Training loss 0.016184614971280098 Validation loss 0.02178419940173626 Accuracy 0.77685546875\n",
      "Iteration 36260 Training loss 0.019081421196460724 Validation loss 0.023764606565237045 Accuracy 0.75634765625\n",
      "Iteration 36270 Training loss 0.014997906982898712 Validation loss 0.021949633955955505 Accuracy 0.77587890625\n",
      "Iteration 36280 Training loss 0.02064407244324684 Validation loss 0.02195928618311882 Accuracy 0.775390625\n",
      "Iteration 36290 Training loss 0.018522387370467186 Validation loss 0.022463427856564522 Accuracy 0.76904296875\n",
      "Iteration 36300 Training loss 0.017037276178598404 Validation loss 0.021584371104836464 Accuracy 0.779296875\n",
      "Iteration 36310 Training loss 0.018747203052043915 Validation loss 0.02238943986594677 Accuracy 0.771484375\n",
      "Iteration 36320 Training loss 0.015683960169553757 Validation loss 0.021557174623012543 Accuracy 0.77880859375\n",
      "Iteration 36330 Training loss 0.01779312826693058 Validation loss 0.02193561941385269 Accuracy 0.77587890625\n",
      "Iteration 36340 Training loss 0.02100672945380211 Validation loss 0.022161968052387238 Accuracy 0.77294921875\n",
      "Iteration 36350 Training loss 0.018636494874954224 Validation loss 0.021558022126555443 Accuracy 0.77978515625\n",
      "Iteration 36360 Training loss 0.015578076243400574 Validation loss 0.022240666672587395 Accuracy 0.7734375\n",
      "Iteration 36370 Training loss 0.019201340153813362 Validation loss 0.022114725783467293 Accuracy 0.77392578125\n",
      "Iteration 36380 Training loss 0.01959390752017498 Validation loss 0.021594077348709106 Accuracy 0.77880859375\n",
      "Iteration 36390 Training loss 0.01735381782054901 Validation loss 0.022112825885415077 Accuracy 0.77392578125\n",
      "Iteration 36400 Training loss 0.018125062808394432 Validation loss 0.02237025834619999 Accuracy 0.77197265625\n",
      "Iteration 36410 Training loss 0.01923491433262825 Validation loss 0.02161509171128273 Accuracy 0.77880859375\n",
      "Iteration 36420 Training loss 0.018034735694527626 Validation loss 0.021593783050775528 Accuracy 0.779296875\n",
      "Iteration 36430 Training loss 0.018062865361571312 Validation loss 0.02205243520438671 Accuracy 0.77490234375\n",
      "Iteration 36440 Training loss 0.01969568431377411 Validation loss 0.022233014926314354 Accuracy 0.77197265625\n",
      "Iteration 36450 Training loss 0.016893092542886734 Validation loss 0.021709471940994263 Accuracy 0.77783203125\n",
      "Iteration 36460 Training loss 0.017666809260845184 Validation loss 0.022345365956425667 Accuracy 0.77197265625\n",
      "Iteration 36470 Training loss 0.01901102624833584 Validation loss 0.023119285702705383 Accuracy 0.763671875\n",
      "Iteration 36480 Training loss 0.017780249938368797 Validation loss 0.02159472554922104 Accuracy 0.779296875\n",
      "Iteration 36490 Training loss 0.018329542130231857 Validation loss 0.02319389581680298 Accuracy 0.7626953125\n",
      "Iteration 36500 Training loss 0.018810274079442024 Validation loss 0.021981926634907722 Accuracy 0.775390625\n",
      "Iteration 36510 Training loss 0.014567817561328411 Validation loss 0.021842272952198982 Accuracy 0.77685546875\n",
      "Iteration 36520 Training loss 0.02010691538453102 Validation loss 0.021654672920703888 Accuracy 0.7783203125\n",
      "Iteration 36530 Training loss 0.019174637272953987 Validation loss 0.022272584959864616 Accuracy 0.7724609375\n",
      "Iteration 36540 Training loss 0.01823216676712036 Validation loss 0.021556884050369263 Accuracy 0.77978515625\n",
      "Iteration 36550 Training loss 0.015610475093126297 Validation loss 0.021417275071144104 Accuracy 0.78125\n",
      "Iteration 36560 Training loss 0.01887962408363819 Validation loss 0.02236645668745041 Accuracy 0.771484375\n",
      "Iteration 36570 Training loss 0.018379470333456993 Validation loss 0.0220860056579113 Accuracy 0.77294921875\n",
      "Iteration 36580 Training loss 0.017457265406847 Validation loss 0.022669177502393723 Accuracy 0.76708984375\n",
      "Iteration 36590 Training loss 0.01489374227821827 Validation loss 0.021484335884451866 Accuracy 0.77978515625\n",
      "Iteration 36600 Training loss 0.016738271340727806 Validation loss 0.02153242565691471 Accuracy 0.77978515625\n",
      "Iteration 36610 Training loss 0.016159962862730026 Validation loss 0.02168530784547329 Accuracy 0.7783203125\n",
      "Iteration 36620 Training loss 0.017128247767686844 Validation loss 0.021829791367053986 Accuracy 0.77685546875\n",
      "Iteration 36630 Training loss 0.017682787030935287 Validation loss 0.0217168889939785 Accuracy 0.77734375\n",
      "Iteration 36640 Training loss 0.01832430064678192 Validation loss 0.021728040650486946 Accuracy 0.77783203125\n",
      "Iteration 36650 Training loss 0.017510492354631424 Validation loss 0.02165094204246998 Accuracy 0.77880859375\n",
      "Iteration 36660 Training loss 0.017142094671726227 Validation loss 0.021719688549637794 Accuracy 0.77783203125\n",
      "Iteration 36670 Training loss 0.018075525760650635 Validation loss 0.021734101697802544 Accuracy 0.77734375\n",
      "Iteration 36680 Training loss 0.019453490152955055 Validation loss 0.02173660136759281 Accuracy 0.77734375\n",
      "Iteration 36690 Training loss 0.019873248413205147 Validation loss 0.021980643272399902 Accuracy 0.775390625\n",
      "Iteration 36700 Training loss 0.017248142510652542 Validation loss 0.02180509641766548 Accuracy 0.77734375\n",
      "Iteration 36710 Training loss 0.01694890484213829 Validation loss 0.02163003198802471 Accuracy 0.7783203125\n",
      "Iteration 36720 Training loss 0.018014736473560333 Validation loss 0.021640872582793236 Accuracy 0.77880859375\n",
      "Iteration 36730 Training loss 0.019178278744220734 Validation loss 0.022423097863793373 Accuracy 0.77099609375\n",
      "Iteration 36740 Training loss 0.015169146470725536 Validation loss 0.022099120542407036 Accuracy 0.77490234375\n",
      "Iteration 36750 Training loss 0.01738310419023037 Validation loss 0.021861499175429344 Accuracy 0.77734375\n",
      "Iteration 36760 Training loss 0.018509849905967712 Validation loss 0.021434105932712555 Accuracy 0.78076171875\n",
      "Iteration 36770 Training loss 0.016620421782135963 Validation loss 0.021805904805660248 Accuracy 0.77734375\n",
      "Iteration 36780 Training loss 0.01876172050833702 Validation loss 0.02176492102444172 Accuracy 0.77734375\n",
      "Iteration 36790 Training loss 0.0186903215944767 Validation loss 0.021882977336645126 Accuracy 0.7763671875\n",
      "Iteration 36800 Training loss 0.017243294045329094 Validation loss 0.022078964859247208 Accuracy 0.77490234375\n",
      "Iteration 36810 Training loss 0.018138885498046875 Validation loss 0.02218291535973549 Accuracy 0.771484375\n",
      "Iteration 36820 Training loss 0.01785830408334732 Validation loss 0.021669801324605942 Accuracy 0.77880859375\n",
      "Iteration 36830 Training loss 0.01911475695669651 Validation loss 0.021827245131134987 Accuracy 0.77587890625\n",
      "Iteration 36840 Training loss 0.019373180344700813 Validation loss 0.021587202325463295 Accuracy 0.77880859375\n",
      "Iteration 36850 Training loss 0.016889818012714386 Validation loss 0.02168416604399681 Accuracy 0.77783203125\n",
      "Iteration 36860 Training loss 0.017237838357686996 Validation loss 0.021458832547068596 Accuracy 0.78076171875\n",
      "Iteration 36870 Training loss 0.019348157569766045 Validation loss 0.021641883999109268 Accuracy 0.77880859375\n",
      "Iteration 36880 Training loss 0.01725388877093792 Validation loss 0.021483607590198517 Accuracy 0.78125\n",
      "Iteration 36890 Training loss 0.019401323050260544 Validation loss 0.021332046017050743 Accuracy 0.78076171875\n",
      "Iteration 36900 Training loss 0.014920978806912899 Validation loss 0.021905535832047462 Accuracy 0.77685546875\n",
      "Iteration 36910 Training loss 0.016694162040948868 Validation loss 0.021490829065442085 Accuracy 0.78076171875\n",
      "Iteration 36920 Training loss 0.01868143118917942 Validation loss 0.021480485796928406 Accuracy 0.77978515625\n",
      "Iteration 36930 Training loss 0.018341077491641045 Validation loss 0.021948417648673058 Accuracy 0.77587890625\n",
      "Iteration 36940 Training loss 0.018626894801855087 Validation loss 0.02217249944806099 Accuracy 0.77197265625\n",
      "Iteration 36950 Training loss 0.017009589821100235 Validation loss 0.0215165838599205 Accuracy 0.7802734375\n",
      "Iteration 36960 Training loss 0.017693497240543365 Validation loss 0.021525876596570015 Accuracy 0.78076171875\n",
      "Iteration 36970 Training loss 0.018962504342198372 Validation loss 0.02252882532775402 Accuracy 0.76904296875\n",
      "Iteration 36980 Training loss 0.018659356981515884 Validation loss 0.021883051842451096 Accuracy 0.7763671875\n",
      "Iteration 36990 Training loss 0.017243746668100357 Validation loss 0.02156129851937294 Accuracy 0.779296875\n",
      "Iteration 37000 Training loss 0.01803547330200672 Validation loss 0.02170601300895214 Accuracy 0.7783203125\n",
      "Iteration 37010 Training loss 0.017767494544386864 Validation loss 0.02133537456393242 Accuracy 0.78125\n",
      "Iteration 37020 Training loss 0.01808844693005085 Validation loss 0.021307550370693207 Accuracy 0.78125\n",
      "Iteration 37030 Training loss 0.017531031742691994 Validation loss 0.021780766546726227 Accuracy 0.7763671875\n",
      "Iteration 37040 Training loss 0.019490383565425873 Validation loss 0.021785058081150055 Accuracy 0.77685546875\n",
      "Iteration 37050 Training loss 0.01780574955046177 Validation loss 0.021472927182912827 Accuracy 0.78076171875\n",
      "Iteration 37060 Training loss 0.015666354447603226 Validation loss 0.02154199592769146 Accuracy 0.7802734375\n",
      "Iteration 37070 Training loss 0.016542475670576096 Validation loss 0.021645165979862213 Accuracy 0.779296875\n",
      "Iteration 37080 Training loss 0.014795857481658459 Validation loss 0.021879997104406357 Accuracy 0.7763671875\n",
      "Iteration 37090 Training loss 0.019581114873290062 Validation loss 0.021876605227589607 Accuracy 0.7763671875\n",
      "Iteration 37100 Training loss 0.019852938130497932 Validation loss 0.0214022696018219 Accuracy 0.7802734375\n",
      "Iteration 37110 Training loss 0.018028613179922104 Validation loss 0.021688075736165047 Accuracy 0.77783203125\n",
      "Iteration 37120 Training loss 0.015827352181077003 Validation loss 0.021512147039175034 Accuracy 0.7802734375\n",
      "Iteration 37130 Training loss 0.016682012006640434 Validation loss 0.021266717463731766 Accuracy 0.7822265625\n",
      "Iteration 37140 Training loss 0.017556916922330856 Validation loss 0.02192079834640026 Accuracy 0.775390625\n",
      "Iteration 37150 Training loss 0.016544833779335022 Validation loss 0.021399827674031258 Accuracy 0.78076171875\n",
      "Iteration 37160 Training loss 0.016950640827417374 Validation loss 0.021513551473617554 Accuracy 0.779296875\n",
      "Iteration 37170 Training loss 0.01673952303826809 Validation loss 0.0218916404992342 Accuracy 0.775390625\n",
      "Iteration 37180 Training loss 0.017803799360990524 Validation loss 0.02157253585755825 Accuracy 0.779296875\n",
      "Iteration 37190 Training loss 0.01665063202381134 Validation loss 0.02155136875808239 Accuracy 0.77978515625\n",
      "Iteration 37200 Training loss 0.01747576892375946 Validation loss 0.021820249035954475 Accuracy 0.7763671875\n",
      "Iteration 37210 Training loss 0.018013454973697662 Validation loss 0.022614935413002968 Accuracy 0.7685546875\n",
      "Iteration 37220 Training loss 0.01948581077158451 Validation loss 0.021407626569271088 Accuracy 0.78125\n",
      "Iteration 37230 Training loss 0.017475008964538574 Validation loss 0.021627143025398254 Accuracy 0.7783203125\n",
      "Iteration 37240 Training loss 0.0192556232213974 Validation loss 0.02324187196791172 Accuracy 0.76220703125\n",
      "Iteration 37250 Training loss 0.017413752153515816 Validation loss 0.02142389677464962 Accuracy 0.78076171875\n",
      "Iteration 37260 Training loss 0.01904795877635479 Validation loss 0.022098831832408905 Accuracy 0.7734375\n",
      "Iteration 37270 Training loss 0.018139423802495003 Validation loss 0.021534455940127373 Accuracy 0.7802734375\n",
      "Iteration 37280 Training loss 0.016025103628635406 Validation loss 0.022420644760131836 Accuracy 0.76953125\n",
      "Iteration 37290 Training loss 0.013484390452504158 Validation loss 0.02148337848484516 Accuracy 0.7802734375\n",
      "Iteration 37300 Training loss 0.01963004842400551 Validation loss 0.021478978917002678 Accuracy 0.779296875\n",
      "Iteration 37310 Training loss 0.017015811055898666 Validation loss 0.021974535658955574 Accuracy 0.775390625\n",
      "Iteration 37320 Training loss 0.01780513860285282 Validation loss 0.021838651970028877 Accuracy 0.7763671875\n",
      "Iteration 37330 Training loss 0.02029379829764366 Validation loss 0.022141283378005028 Accuracy 0.7744140625\n",
      "Iteration 37340 Training loss 0.01639586128294468 Validation loss 0.02178720384836197 Accuracy 0.77734375\n",
      "Iteration 37350 Training loss 0.017984988167881966 Validation loss 0.02147880382835865 Accuracy 0.77978515625\n",
      "Iteration 37360 Training loss 0.01947762444615364 Validation loss 0.02189699187874794 Accuracy 0.77490234375\n",
      "Iteration 37370 Training loss 0.01847602240741253 Validation loss 0.021603282541036606 Accuracy 0.779296875\n",
      "Iteration 37380 Training loss 0.015816884115338326 Validation loss 0.02136559970676899 Accuracy 0.78271484375\n",
      "Iteration 37390 Training loss 0.020794136449694633 Validation loss 0.022509677335619926 Accuracy 0.7685546875\n",
      "Iteration 37400 Training loss 0.014833585359156132 Validation loss 0.021811047568917274 Accuracy 0.77685546875\n",
      "Iteration 37410 Training loss 0.01747255213558674 Validation loss 0.021620653569698334 Accuracy 0.77880859375\n",
      "Iteration 37420 Training loss 0.016293909400701523 Validation loss 0.021597811952233315 Accuracy 0.779296875\n",
      "Iteration 37430 Training loss 0.01909862831234932 Validation loss 0.02142421156167984 Accuracy 0.7802734375\n",
      "Iteration 37440 Training loss 0.015059188939630985 Validation loss 0.021645473316311836 Accuracy 0.77978515625\n",
      "Iteration 37450 Training loss 0.019056273624300957 Validation loss 0.021693876013159752 Accuracy 0.7783203125\n",
      "Iteration 37460 Training loss 0.01854828931391239 Validation loss 0.0213371142745018 Accuracy 0.7822265625\n",
      "Iteration 37470 Training loss 0.0173951406031847 Validation loss 0.021518390625715256 Accuracy 0.779296875\n",
      "Iteration 37480 Training loss 0.01682092994451523 Validation loss 0.021998925134539604 Accuracy 0.7744140625\n",
      "Iteration 37490 Training loss 0.01812760718166828 Validation loss 0.022777216508984566 Accuracy 0.7666015625\n",
      "Iteration 37500 Training loss 0.01751006580889225 Validation loss 0.021764740347862244 Accuracy 0.77783203125\n",
      "Iteration 37510 Training loss 0.0189815741032362 Validation loss 0.022199442610144615 Accuracy 0.77294921875\n",
      "Iteration 37520 Training loss 0.016046468168497086 Validation loss 0.021348917856812477 Accuracy 0.78125\n",
      "Iteration 37530 Training loss 0.018969204276800156 Validation loss 0.021634308621287346 Accuracy 0.7783203125\n",
      "Iteration 37540 Training loss 0.019203806295990944 Validation loss 0.021934842690825462 Accuracy 0.775390625\n",
      "Iteration 37550 Training loss 0.01571030355989933 Validation loss 0.02153117209672928 Accuracy 0.779296875\n",
      "Iteration 37560 Training loss 0.01958124339580536 Validation loss 0.02168298326432705 Accuracy 0.77783203125\n",
      "Iteration 37570 Training loss 0.018445584923028946 Validation loss 0.021557824686169624 Accuracy 0.7783203125\n",
      "Iteration 37580 Training loss 0.013911648653447628 Validation loss 0.021789349615573883 Accuracy 0.7763671875\n",
      "Iteration 37590 Training loss 0.01859419420361519 Validation loss 0.021624673157930374 Accuracy 0.779296875\n",
      "Iteration 37600 Training loss 0.01880807988345623 Validation loss 0.021981047466397285 Accuracy 0.77490234375\n",
      "Iteration 37610 Training loss 0.018195632845163345 Validation loss 0.02321929857134819 Accuracy 0.76171875\n",
      "Iteration 37620 Training loss 0.014918462373316288 Validation loss 0.021526562049984932 Accuracy 0.77880859375\n",
      "Iteration 37630 Training loss 0.0203405674546957 Validation loss 0.022169915959239006 Accuracy 0.771484375\n",
      "Iteration 37640 Training loss 0.01850087195634842 Validation loss 0.021461062133312225 Accuracy 0.78076171875\n",
      "Iteration 37650 Training loss 0.01801414042711258 Validation loss 0.021615413948893547 Accuracy 0.77880859375\n",
      "Iteration 37660 Training loss 0.015186142176389694 Validation loss 0.021547360345721245 Accuracy 0.77880859375\n",
      "Iteration 37670 Training loss 0.01590392366051674 Validation loss 0.021675528958439827 Accuracy 0.77783203125\n",
      "Iteration 37680 Training loss 0.01638915203511715 Validation loss 0.02153286524116993 Accuracy 0.77880859375\n",
      "Iteration 37690 Training loss 0.01938461698591709 Validation loss 0.021805614233016968 Accuracy 0.77734375\n",
      "Iteration 37700 Training loss 0.02040802873671055 Validation loss 0.02164783701300621 Accuracy 0.7783203125\n",
      "Iteration 37710 Training loss 0.018200743943452835 Validation loss 0.02144656702876091 Accuracy 0.7802734375\n",
      "Iteration 37720 Training loss 0.018725264817476273 Validation loss 0.02152007259428501 Accuracy 0.77880859375\n",
      "Iteration 37730 Training loss 0.017086466774344444 Validation loss 0.021664796397089958 Accuracy 0.779296875\n",
      "Iteration 37740 Training loss 0.018366053700447083 Validation loss 0.02148710936307907 Accuracy 0.7783203125\n",
      "Iteration 37750 Training loss 0.017795244231820107 Validation loss 0.02167057991027832 Accuracy 0.77783203125\n",
      "Iteration 37760 Training loss 0.014631184749305248 Validation loss 0.021966584026813507 Accuracy 0.77490234375\n",
      "Iteration 37770 Training loss 0.016805006191134453 Validation loss 0.021914519369602203 Accuracy 0.775390625\n",
      "Iteration 37780 Training loss 0.0182084608823061 Validation loss 0.02199692837893963 Accuracy 0.7744140625\n",
      "Iteration 37790 Training loss 0.01674812100827694 Validation loss 0.02182704769074917 Accuracy 0.7763671875\n",
      "Iteration 37800 Training loss 0.01690230704843998 Validation loss 0.021960346028208733 Accuracy 0.775390625\n",
      "Iteration 37810 Training loss 0.019161023199558258 Validation loss 0.021353330463171005 Accuracy 0.78076171875\n",
      "Iteration 37820 Training loss 0.018295010551810265 Validation loss 0.021376904100179672 Accuracy 0.78076171875\n",
      "Iteration 37830 Training loss 0.018460338935256004 Validation loss 0.02172688953578472 Accuracy 0.77783203125\n",
      "Iteration 37840 Training loss 0.017939845100045204 Validation loss 0.02192271687090397 Accuracy 0.7744140625\n",
      "Iteration 37850 Training loss 0.015879370272159576 Validation loss 0.021324777975678444 Accuracy 0.78125\n",
      "Iteration 37860 Training loss 0.013570976443588734 Validation loss 0.02160421758890152 Accuracy 0.77880859375\n",
      "Iteration 37870 Training loss 0.017674267292022705 Validation loss 0.02198915183544159 Accuracy 0.77490234375\n",
      "Iteration 37880 Training loss 0.018293194472789764 Validation loss 0.021858831867575645 Accuracy 0.77587890625\n",
      "Iteration 37890 Training loss 0.018282538279891014 Validation loss 0.02148769050836563 Accuracy 0.7802734375\n",
      "Iteration 37900 Training loss 0.016351450234651566 Validation loss 0.02182171307504177 Accuracy 0.7763671875\n",
      "Iteration 37910 Training loss 0.016815250739455223 Validation loss 0.021157938987016678 Accuracy 0.78369140625\n",
      "Iteration 37920 Training loss 0.014944151043891907 Validation loss 0.02156807854771614 Accuracy 0.77734375\n",
      "Iteration 37930 Training loss 0.020009972155094147 Validation loss 0.021707845851778984 Accuracy 0.77783203125\n",
      "Iteration 37940 Training loss 0.01761469617486 Validation loss 0.021573446691036224 Accuracy 0.77978515625\n",
      "Iteration 37950 Training loss 0.01662401854991913 Validation loss 0.021371254697442055 Accuracy 0.78173828125\n",
      "Iteration 37960 Training loss 0.015096929855644703 Validation loss 0.021971315145492554 Accuracy 0.77490234375\n",
      "Iteration 37970 Training loss 0.019014956429600716 Validation loss 0.021505631506443024 Accuracy 0.77978515625\n",
      "Iteration 37980 Training loss 0.017786411568522453 Validation loss 0.021991610527038574 Accuracy 0.775390625\n",
      "Iteration 37990 Training loss 0.015823477879166603 Validation loss 0.021598495543003082 Accuracy 0.77880859375\n",
      "Iteration 38000 Training loss 0.017418423667550087 Validation loss 0.021502280607819557 Accuracy 0.7802734375\n",
      "Iteration 38010 Training loss 0.017244279384613037 Validation loss 0.021672386676073074 Accuracy 0.7783203125\n",
      "Iteration 38020 Training loss 0.018722018226981163 Validation loss 0.021413465961813927 Accuracy 0.78125\n",
      "Iteration 38030 Training loss 0.014866354875266552 Validation loss 0.021850071847438812 Accuracy 0.7763671875\n",
      "Iteration 38040 Training loss 0.016759926453232765 Validation loss 0.021337958052754402 Accuracy 0.78173828125\n",
      "Iteration 38050 Training loss 0.01534002274274826 Validation loss 0.0213675107806921 Accuracy 0.78125\n",
      "Iteration 38060 Training loss 0.012974303215742111 Validation loss 0.021741371601819992 Accuracy 0.77734375\n",
      "Iteration 38070 Training loss 0.017956523224711418 Validation loss 0.02149815298616886 Accuracy 0.77978515625\n",
      "Iteration 38080 Training loss 0.017402369529008865 Validation loss 0.021410131826996803 Accuracy 0.78125\n",
      "Iteration 38090 Training loss 0.01634124293923378 Validation loss 0.021668143570423126 Accuracy 0.77880859375\n",
      "Iteration 38100 Training loss 0.016868149861693382 Validation loss 0.021233242005109787 Accuracy 0.7822265625\n",
      "Iteration 38110 Training loss 0.014555166475474834 Validation loss 0.02136756107211113 Accuracy 0.78173828125\n",
      "Iteration 38120 Training loss 0.01648709923028946 Validation loss 0.02161732316017151 Accuracy 0.779296875\n",
      "Iteration 38130 Training loss 0.01825537346303463 Validation loss 0.021412992849946022 Accuracy 0.78076171875\n",
      "Iteration 38140 Training loss 0.018438110128045082 Validation loss 0.02138681150972843 Accuracy 0.78173828125\n",
      "Iteration 38150 Training loss 0.01480481494218111 Validation loss 0.021423758938908577 Accuracy 0.78076171875\n",
      "Iteration 38160 Training loss 0.013694709166884422 Validation loss 0.0214384738355875 Accuracy 0.78125\n",
      "Iteration 38170 Training loss 0.01415302138775587 Validation loss 0.02180640399456024 Accuracy 0.7763671875\n",
      "Iteration 38180 Training loss 0.018873989582061768 Validation loss 0.022368980571627617 Accuracy 0.771484375\n",
      "Iteration 38190 Training loss 0.015708716586232185 Validation loss 0.02212609350681305 Accuracy 0.77392578125\n",
      "Iteration 38200 Training loss 0.017178181558847427 Validation loss 0.022032681852579117 Accuracy 0.77392578125\n",
      "Iteration 38210 Training loss 0.015945779159665108 Validation loss 0.02182817831635475 Accuracy 0.77587890625\n",
      "Iteration 38220 Training loss 0.017757035791873932 Validation loss 0.02172328531742096 Accuracy 0.77783203125\n",
      "Iteration 38230 Training loss 0.016203632578253746 Validation loss 0.021478580310940742 Accuracy 0.7802734375\n",
      "Iteration 38240 Training loss 0.01637696847319603 Validation loss 0.021761925891041756 Accuracy 0.77783203125\n",
      "Iteration 38250 Training loss 0.017114844173192978 Validation loss 0.02150394394993782 Accuracy 0.77978515625\n",
      "Iteration 38260 Training loss 0.0209614560008049 Validation loss 0.02245315909385681 Accuracy 0.7705078125\n",
      "Iteration 38270 Training loss 0.020014818757772446 Validation loss 0.02140014059841633 Accuracy 0.78076171875\n",
      "Iteration 38280 Training loss 0.017067963257431984 Validation loss 0.021604198962450027 Accuracy 0.77880859375\n",
      "Iteration 38290 Training loss 0.017380449920892715 Validation loss 0.021899105980992317 Accuracy 0.7763671875\n",
      "Iteration 38300 Training loss 0.017818741500377655 Validation loss 0.021487079560756683 Accuracy 0.7802734375\n",
      "Iteration 38310 Training loss 0.017447007820010185 Validation loss 0.021566499024629593 Accuracy 0.779296875\n",
      "Iteration 38320 Training loss 0.018778931349515915 Validation loss 0.021601947024464607 Accuracy 0.77978515625\n",
      "Iteration 38330 Training loss 0.018357500433921814 Validation loss 0.022903451696038246 Accuracy 0.76513671875\n",
      "Iteration 38340 Training loss 0.01724967546761036 Validation loss 0.021724320948123932 Accuracy 0.7763671875\n",
      "Iteration 38350 Training loss 0.016627192497253418 Validation loss 0.02145550027489662 Accuracy 0.7802734375\n",
      "Iteration 38360 Training loss 0.016435766592621803 Validation loss 0.021561501547694206 Accuracy 0.779296875\n",
      "Iteration 38370 Training loss 0.015070728957653046 Validation loss 0.02147524431347847 Accuracy 0.7802734375\n",
      "Iteration 38380 Training loss 0.0154651440680027 Validation loss 0.02145165205001831 Accuracy 0.7802734375\n",
      "Iteration 38390 Training loss 0.01798475906252861 Validation loss 0.021726835519075394 Accuracy 0.77783203125\n",
      "Iteration 38400 Training loss 0.01771933026611805 Validation loss 0.02153255045413971 Accuracy 0.779296875\n",
      "Iteration 38410 Training loss 0.017458520829677582 Validation loss 0.021601518616080284 Accuracy 0.77880859375\n",
      "Iteration 38420 Training loss 0.017431315034627914 Validation loss 0.021571649238467216 Accuracy 0.779296875\n",
      "Iteration 38430 Training loss 0.0151795269921422 Validation loss 0.021494537591934204 Accuracy 0.7802734375\n",
      "Iteration 38440 Training loss 0.01685204729437828 Validation loss 0.021214105188846588 Accuracy 0.7822265625\n",
      "Iteration 38450 Training loss 0.01766924187541008 Validation loss 0.02184336446225643 Accuracy 0.77587890625\n",
      "Iteration 38460 Training loss 0.014302288182079792 Validation loss 0.021783681586384773 Accuracy 0.77734375\n",
      "Iteration 38470 Training loss 0.018324552103877068 Validation loss 0.021587666124105453 Accuracy 0.77783203125\n",
      "Iteration 38480 Training loss 0.014837930910289288 Validation loss 0.021683914586901665 Accuracy 0.77783203125\n",
      "Iteration 38490 Training loss 0.017127079889178276 Validation loss 0.02149103954434395 Accuracy 0.77880859375\n",
      "Iteration 38500 Training loss 0.018788591027259827 Validation loss 0.02189674787223339 Accuracy 0.77587890625\n",
      "Iteration 38510 Training loss 0.014379373751580715 Validation loss 0.021361393854022026 Accuracy 0.78125\n",
      "Iteration 38520 Training loss 0.01761367730796337 Validation loss 0.021745488047599792 Accuracy 0.7763671875\n",
      "Iteration 38530 Training loss 0.016649898141622543 Validation loss 0.023280825465917587 Accuracy 0.76123046875\n",
      "Iteration 38540 Training loss 0.015984797850251198 Validation loss 0.021337606012821198 Accuracy 0.7802734375\n",
      "Iteration 38550 Training loss 0.01581275835633278 Validation loss 0.021486593410372734 Accuracy 0.77880859375\n",
      "Iteration 38560 Training loss 0.01934809796512127 Validation loss 0.021436495706439018 Accuracy 0.7802734375\n",
      "Iteration 38570 Training loss 0.01940291002392769 Validation loss 0.022624174132943153 Accuracy 0.76806640625\n",
      "Iteration 38580 Training loss 0.014513123780488968 Validation loss 0.0215970017015934 Accuracy 0.77880859375\n",
      "Iteration 38590 Training loss 0.01905856467783451 Validation loss 0.02168135717511177 Accuracy 0.77734375\n",
      "Iteration 38600 Training loss 0.016972914338111877 Validation loss 0.021221598610281944 Accuracy 0.78271484375\n",
      "Iteration 38610 Training loss 0.017798492684960365 Validation loss 0.021524569019675255 Accuracy 0.779296875\n",
      "Iteration 38620 Training loss 0.015732862055301666 Validation loss 0.021448403596878052 Accuracy 0.7802734375\n",
      "Iteration 38630 Training loss 0.018901903182268143 Validation loss 0.02159726992249489 Accuracy 0.7783203125\n",
      "Iteration 38640 Training loss 0.01564994640648365 Validation loss 0.021713070571422577 Accuracy 0.77734375\n",
      "Iteration 38650 Training loss 0.014842037111520767 Validation loss 0.022147782146930695 Accuracy 0.7734375\n",
      "Iteration 38660 Training loss 0.01565592736005783 Validation loss 0.021380316466093063 Accuracy 0.7802734375\n",
      "Iteration 38670 Training loss 0.014045074582099915 Validation loss 0.021628808230161667 Accuracy 0.7783203125\n",
      "Iteration 38680 Training loss 0.01684681698679924 Validation loss 0.021333876997232437 Accuracy 0.78076171875\n",
      "Iteration 38690 Training loss 0.015549538657069206 Validation loss 0.021458234637975693 Accuracy 0.78076171875\n",
      "Iteration 38700 Training loss 0.01774001494050026 Validation loss 0.022668534889817238 Accuracy 0.767578125\n",
      "Iteration 38710 Training loss 0.018438715487718582 Validation loss 0.02138476073741913 Accuracy 0.78076171875\n",
      "Iteration 38720 Training loss 0.01728520728647709 Validation loss 0.021463792771100998 Accuracy 0.7802734375\n",
      "Iteration 38730 Training loss 0.018588224425911903 Validation loss 0.021409252658486366 Accuracy 0.78076171875\n",
      "Iteration 38740 Training loss 0.017944935709238052 Validation loss 0.021781815215945244 Accuracy 0.7763671875\n",
      "Iteration 38750 Training loss 0.015000061132013798 Validation loss 0.0213340912014246 Accuracy 0.78076171875\n",
      "Iteration 38760 Training loss 0.013211153447628021 Validation loss 0.022041702643036842 Accuracy 0.77392578125\n",
      "Iteration 38770 Training loss 0.017296595498919487 Validation loss 0.021572645753622055 Accuracy 0.779296875\n",
      "Iteration 38780 Training loss 0.015093821100890636 Validation loss 0.021747469902038574 Accuracy 0.77685546875\n",
      "Iteration 38790 Training loss 0.016960404813289642 Validation loss 0.021477369591593742 Accuracy 0.7802734375\n",
      "Iteration 38800 Training loss 0.016593480482697487 Validation loss 0.02142239175736904 Accuracy 0.77978515625\n",
      "Iteration 38810 Training loss 0.016825605183839798 Validation loss 0.021500831469893456 Accuracy 0.77978515625\n",
      "Iteration 38820 Training loss 0.018373852595686913 Validation loss 0.02157019078731537 Accuracy 0.7783203125\n",
      "Iteration 38830 Training loss 0.014334889128804207 Validation loss 0.02142438478767872 Accuracy 0.7802734375\n",
      "Iteration 38840 Training loss 0.019095497205853462 Validation loss 0.021412352100014687 Accuracy 0.78076171875\n",
      "Iteration 38850 Training loss 0.016073819249868393 Validation loss 0.022025341168045998 Accuracy 0.77392578125\n",
      "Iteration 38860 Training loss 0.021838534623384476 Validation loss 0.023601366207003593 Accuracy 0.7587890625\n",
      "Iteration 38870 Training loss 0.01780051924288273 Validation loss 0.021473390981554985 Accuracy 0.779296875\n",
      "Iteration 38880 Training loss 0.015817459672689438 Validation loss 0.021542346104979515 Accuracy 0.77978515625\n",
      "Iteration 38890 Training loss 0.013721222057938576 Validation loss 0.021479733288288116 Accuracy 0.77978515625\n",
      "Iteration 38900 Training loss 0.016572220250964165 Validation loss 0.02169984020292759 Accuracy 0.77734375\n",
      "Iteration 38910 Training loss 0.01788831315934658 Validation loss 0.021814843639731407 Accuracy 0.7763671875\n",
      "Iteration 38920 Training loss 0.01624525710940361 Validation loss 0.021613556891679764 Accuracy 0.7783203125\n",
      "Iteration 38930 Training loss 0.018409108743071556 Validation loss 0.021389784291386604 Accuracy 0.78076171875\n",
      "Iteration 38940 Training loss 0.018780825659632683 Validation loss 0.021555505692958832 Accuracy 0.77978515625\n",
      "Iteration 38950 Training loss 0.013800641521811485 Validation loss 0.021436745300889015 Accuracy 0.78125\n",
      "Iteration 38960 Training loss 0.019241008907556534 Validation loss 0.021435927599668503 Accuracy 0.7802734375\n",
      "Iteration 38970 Training loss 0.01769007369875908 Validation loss 0.021089306101202965 Accuracy 0.7841796875\n",
      "Iteration 38980 Training loss 0.017243588343262672 Validation loss 0.021206362172961235 Accuracy 0.78271484375\n",
      "Iteration 38990 Training loss 0.01758607290685177 Validation loss 0.0214398130774498 Accuracy 0.7802734375\n",
      "Iteration 39000 Training loss 0.015346333384513855 Validation loss 0.021666215732693672 Accuracy 0.779296875\n",
      "Iteration 39010 Training loss 0.014943870715796947 Validation loss 0.021517131477594376 Accuracy 0.77978515625\n",
      "Iteration 39020 Training loss 0.017833301797509193 Validation loss 0.02142481878399849 Accuracy 0.7802734375\n",
      "Iteration 39030 Training loss 0.01747720316052437 Validation loss 0.021540043875575066 Accuracy 0.77978515625\n",
      "Iteration 39040 Training loss 0.017548946663737297 Validation loss 0.02167372964322567 Accuracy 0.77783203125\n",
      "Iteration 39050 Training loss 0.01646026223897934 Validation loss 0.021190742030739784 Accuracy 0.783203125\n",
      "Iteration 39060 Training loss 0.01768365502357483 Validation loss 0.021297099068760872 Accuracy 0.7822265625\n",
      "Iteration 39070 Training loss 0.016973327845335007 Validation loss 0.021865706890821457 Accuracy 0.77490234375\n",
      "Iteration 39080 Training loss 0.016658304259181023 Validation loss 0.02123766578733921 Accuracy 0.78271484375\n",
      "Iteration 39090 Training loss 0.018902335315942764 Validation loss 0.021420089527964592 Accuracy 0.78173828125\n",
      "Iteration 39100 Training loss 0.015296345576643944 Validation loss 0.02139844372868538 Accuracy 0.77978515625\n",
      "Iteration 39110 Training loss 0.01874655671417713 Validation loss 0.021599071100354195 Accuracy 0.7783203125\n",
      "Iteration 39120 Training loss 0.016698617488145828 Validation loss 0.0214602779597044 Accuracy 0.7802734375\n",
      "Iteration 39130 Training loss 0.017321739345788956 Validation loss 0.021558117121458054 Accuracy 0.779296875\n",
      "Iteration 39140 Training loss 0.01866050809621811 Validation loss 0.021866118535399437 Accuracy 0.77685546875\n",
      "Iteration 39150 Training loss 0.014203079044818878 Validation loss 0.021444877609610558 Accuracy 0.77978515625\n",
      "Iteration 39160 Training loss 0.0156859140843153 Validation loss 0.022001923993229866 Accuracy 0.7744140625\n",
      "Iteration 39170 Training loss 0.01605735719203949 Validation loss 0.02183431386947632 Accuracy 0.7763671875\n",
      "Iteration 39180 Training loss 0.014868760481476784 Validation loss 0.021514330059289932 Accuracy 0.77978515625\n",
      "Iteration 39190 Training loss 0.016978062689304352 Validation loss 0.02129705436527729 Accuracy 0.78173828125\n",
      "Iteration 39200 Training loss 0.016514765098690987 Validation loss 0.0215272456407547 Accuracy 0.77978515625\n",
      "Iteration 39210 Training loss 0.016121869906783104 Validation loss 0.02133883722126484 Accuracy 0.78076171875\n",
      "Iteration 39220 Training loss 0.016876066103577614 Validation loss 0.02134612761437893 Accuracy 0.78076171875\n",
      "Iteration 39230 Training loss 0.01815028302371502 Validation loss 0.021302374079823494 Accuracy 0.78173828125\n",
      "Iteration 39240 Training loss 0.015237808227539062 Validation loss 0.021351905539631844 Accuracy 0.78125\n",
      "Iteration 39250 Training loss 0.01557093020528555 Validation loss 0.021709060296416283 Accuracy 0.77783203125\n",
      "Iteration 39260 Training loss 0.01503031887114048 Validation loss 0.021191058680415154 Accuracy 0.78271484375\n",
      "Iteration 39270 Training loss 0.018066924065351486 Validation loss 0.021369965746998787 Accuracy 0.78076171875\n",
      "Iteration 39280 Training loss 0.01589645817875862 Validation loss 0.02125641703605652 Accuracy 0.7822265625\n",
      "Iteration 39290 Training loss 0.01738075166940689 Validation loss 0.021463563665747643 Accuracy 0.779296875\n",
      "Iteration 39300 Training loss 0.014945528469979763 Validation loss 0.021328158676624298 Accuracy 0.78076171875\n",
      "Iteration 39310 Training loss 0.018078235909342766 Validation loss 0.021638402715325356 Accuracy 0.77734375\n",
      "Iteration 39320 Training loss 0.015935774892568588 Validation loss 0.02156062424182892 Accuracy 0.77880859375\n",
      "Iteration 39330 Training loss 0.018082939088344574 Validation loss 0.02240620367228985 Accuracy 0.76953125\n",
      "Iteration 39340 Training loss 0.01603773795068264 Validation loss 0.02174718677997589 Accuracy 0.77783203125\n",
      "Iteration 39350 Training loss 0.019541701301932335 Validation loss 0.021892717108130455 Accuracy 0.77587890625\n",
      "Iteration 39360 Training loss 0.01735764369368553 Validation loss 0.02156997099518776 Accuracy 0.779296875\n",
      "Iteration 39370 Training loss 0.01752118207514286 Validation loss 0.021287992596626282 Accuracy 0.78173828125\n",
      "Iteration 39380 Training loss 0.016052518039941788 Validation loss 0.021873822435736656 Accuracy 0.7763671875\n",
      "Iteration 39390 Training loss 0.01830163598060608 Validation loss 0.02173519879579544 Accuracy 0.77734375\n",
      "Iteration 39400 Training loss 0.01732412539422512 Validation loss 0.02151978388428688 Accuracy 0.7802734375\n",
      "Iteration 39410 Training loss 0.0193878673017025 Validation loss 0.021241456270217896 Accuracy 0.7822265625\n",
      "Iteration 39420 Training loss 0.01880761608481407 Validation loss 0.021658871322870255 Accuracy 0.77783203125\n",
      "Iteration 39430 Training loss 0.015359211713075638 Validation loss 0.021363765001296997 Accuracy 0.78076171875\n",
      "Iteration 39440 Training loss 0.016965489834547043 Validation loss 0.022100575268268585 Accuracy 0.77294921875\n",
      "Iteration 39450 Training loss 0.01731848157942295 Validation loss 0.02158385142683983 Accuracy 0.779296875\n",
      "Iteration 39460 Training loss 0.015434036031365395 Validation loss 0.021436307579278946 Accuracy 0.78076171875\n",
      "Iteration 39470 Training loss 0.019136730581521988 Validation loss 0.021391533315181732 Accuracy 0.78076171875\n",
      "Iteration 39480 Training loss 0.01431405358016491 Validation loss 0.021754570305347443 Accuracy 0.77685546875\n",
      "Iteration 39490 Training loss 0.017595605924725533 Validation loss 0.021190615370869637 Accuracy 0.783203125\n",
      "Iteration 39500 Training loss 0.0188994649797678 Validation loss 0.02128356322646141 Accuracy 0.78173828125\n",
      "Iteration 39510 Training loss 0.018238615244627 Validation loss 0.021582279354333878 Accuracy 0.7783203125\n",
      "Iteration 39520 Training loss 0.020499644801020622 Validation loss 0.022287648171186447 Accuracy 0.77197265625\n",
      "Iteration 39530 Training loss 0.015001822263002396 Validation loss 0.021204805001616478 Accuracy 0.78369140625\n",
      "Iteration 39540 Training loss 0.016617992892861366 Validation loss 0.021298304200172424 Accuracy 0.78125\n",
      "Iteration 39550 Training loss 0.01696348376572132 Validation loss 0.021440956741571426 Accuracy 0.78076171875\n",
      "Iteration 39560 Training loss 0.016744814813137054 Validation loss 0.02115967497229576 Accuracy 0.783203125\n",
      "Iteration 39570 Training loss 0.016346072778105736 Validation loss 0.021633876487612724 Accuracy 0.7783203125\n",
      "Iteration 39580 Training loss 0.01854906976222992 Validation loss 0.021258536726236343 Accuracy 0.78271484375\n",
      "Iteration 39590 Training loss 0.013929611071944237 Validation loss 0.021606162190437317 Accuracy 0.77880859375\n",
      "Iteration 39600 Training loss 0.017911475151777267 Validation loss 0.022210413590073586 Accuracy 0.771484375\n",
      "Iteration 39610 Training loss 0.016464130952954292 Validation loss 0.02147013135254383 Accuracy 0.78125\n",
      "Iteration 39620 Training loss 0.017131445929408073 Validation loss 0.021498234942555428 Accuracy 0.77978515625\n",
      "Iteration 39630 Training loss 0.01751667447388172 Validation loss 0.021352561190724373 Accuracy 0.78173828125\n",
      "Iteration 39640 Training loss 0.016762427985668182 Validation loss 0.021433431655168533 Accuracy 0.78076171875\n",
      "Iteration 39650 Training loss 0.017660226672887802 Validation loss 0.021617861464619637 Accuracy 0.779296875\n",
      "Iteration 39660 Training loss 0.017445823177695274 Validation loss 0.021390385925769806 Accuracy 0.78125\n",
      "Iteration 39670 Training loss 0.018674246966838837 Validation loss 0.02184499055147171 Accuracy 0.77734375\n",
      "Iteration 39680 Training loss 0.015421202406287193 Validation loss 0.021988680586218834 Accuracy 0.77587890625\n",
      "Iteration 39690 Training loss 0.017923854291439056 Validation loss 0.02205561473965645 Accuracy 0.77392578125\n",
      "Iteration 39700 Training loss 0.01788800209760666 Validation loss 0.023104552179574966 Accuracy 0.7626953125\n",
      "Iteration 39710 Training loss 0.017439819872379303 Validation loss 0.02154955081641674 Accuracy 0.7783203125\n",
      "Iteration 39720 Training loss 0.014072153717279434 Validation loss 0.021677490323781967 Accuracy 0.77783203125\n",
      "Iteration 39730 Training loss 0.02205188013613224 Validation loss 0.023854609578847885 Accuracy 0.75537109375\n",
      "Iteration 39740 Training loss 0.017657823860645294 Validation loss 0.021773120388388634 Accuracy 0.7763671875\n",
      "Iteration 39750 Training loss 0.014861556701362133 Validation loss 0.02120315656065941 Accuracy 0.78173828125\n",
      "Iteration 39760 Training loss 0.016988474875688553 Validation loss 0.021927475929260254 Accuracy 0.775390625\n",
      "Iteration 39770 Training loss 0.01538976188749075 Validation loss 0.021280240267515182 Accuracy 0.7822265625\n",
      "Iteration 39780 Training loss 0.01699735037982464 Validation loss 0.0224329624325037 Accuracy 0.76953125\n",
      "Iteration 39790 Training loss 0.017851898446679115 Validation loss 0.02202998474240303 Accuracy 0.7734375\n",
      "Iteration 39800 Training loss 0.016320347785949707 Validation loss 0.02128147892653942 Accuracy 0.7822265625\n",
      "Iteration 39810 Training loss 0.016216805204749107 Validation loss 0.021976053714752197 Accuracy 0.7744140625\n",
      "Iteration 39820 Training loss 0.02094206213951111 Validation loss 0.021324271336197853 Accuracy 0.78076171875\n",
      "Iteration 39830 Training loss 0.019127395004034042 Validation loss 0.0223387498408556 Accuracy 0.7705078125\n",
      "Iteration 39840 Training loss 0.016258420422673225 Validation loss 0.02126174233853817 Accuracy 0.783203125\n",
      "Iteration 39850 Training loss 0.015398808754980564 Validation loss 0.021603334695100784 Accuracy 0.7783203125\n",
      "Iteration 39860 Training loss 0.014886547811329365 Validation loss 0.02120596170425415 Accuracy 0.78271484375\n",
      "Iteration 39870 Training loss 0.019166823476552963 Validation loss 0.02155918814241886 Accuracy 0.77880859375\n",
      "Iteration 39880 Training loss 0.019777826964855194 Validation loss 0.02190079540014267 Accuracy 0.775390625\n",
      "Iteration 39890 Training loss 0.01766909286379814 Validation loss 0.021311596035957336 Accuracy 0.7822265625\n",
      "Iteration 39900 Training loss 0.01779000833630562 Validation loss 0.02187112532556057 Accuracy 0.7763671875\n",
      "Iteration 39910 Training loss 0.017196794971823692 Validation loss 0.021201759576797485 Accuracy 0.78271484375\n",
      "Iteration 39920 Training loss 0.01781177520751953 Validation loss 0.021503562107682228 Accuracy 0.78076171875\n",
      "Iteration 39930 Training loss 0.016566690057516098 Validation loss 0.02136239781975746 Accuracy 0.78076171875\n",
      "Iteration 39940 Training loss 0.016693633049726486 Validation loss 0.021334389224648476 Accuracy 0.78173828125\n",
      "Iteration 39950 Training loss 0.017760613933205605 Validation loss 0.02130872942507267 Accuracy 0.78173828125\n",
      "Iteration 39960 Training loss 0.017229484394192696 Validation loss 0.02176971361041069 Accuracy 0.77685546875\n",
      "Iteration 39970 Training loss 0.016352249309420586 Validation loss 0.021991508081555367 Accuracy 0.7744140625\n",
      "Iteration 39980 Training loss 0.016197364777326584 Validation loss 0.02168772742152214 Accuracy 0.77734375\n",
      "Iteration 39990 Training loss 0.016724824905395508 Validation loss 0.02155105397105217 Accuracy 0.77880859375\n",
      "Iteration 40000 Training loss 0.014514973387122154 Validation loss 0.021574897691607475 Accuracy 0.77783203125\n",
      "Iteration 40010 Training loss 0.01562635600566864 Validation loss 0.021195081993937492 Accuracy 0.783203125\n",
      "Iteration 40020 Training loss 0.014330923557281494 Validation loss 0.021097587421536446 Accuracy 0.78466796875\n",
      "Iteration 40030 Training loss 0.015538045205175877 Validation loss 0.021145625039935112 Accuracy 0.78271484375\n",
      "Iteration 40040 Training loss 0.018015364184975624 Validation loss 0.021414853632450104 Accuracy 0.78076171875\n",
      "Iteration 40050 Training loss 0.013934669084846973 Validation loss 0.02114495262503624 Accuracy 0.78369140625\n",
      "Iteration 40060 Training loss 0.016068780794739723 Validation loss 0.021820107474923134 Accuracy 0.775390625\n",
      "Iteration 40070 Training loss 0.016027553007006645 Validation loss 0.022363200783729553 Accuracy 0.77099609375\n",
      "Iteration 40080 Training loss 0.018932990729808807 Validation loss 0.02176889404654503 Accuracy 0.7763671875\n",
      "Iteration 40090 Training loss 0.0180758535861969 Validation loss 0.022025389596819878 Accuracy 0.77392578125\n",
      "Iteration 40100 Training loss 0.01724058948457241 Validation loss 0.021434061229228973 Accuracy 0.77978515625\n",
      "Iteration 40110 Training loss 0.01821609027683735 Validation loss 0.021435337141156197 Accuracy 0.78125\n",
      "Iteration 40120 Training loss 0.015975188463926315 Validation loss 0.021186694502830505 Accuracy 0.783203125\n",
      "Iteration 40130 Training loss 0.016052888706326485 Validation loss 0.02185911126434803 Accuracy 0.775390625\n",
      "Iteration 40140 Training loss 0.0166818518191576 Validation loss 0.021702690050005913 Accuracy 0.77685546875\n",
      "Iteration 40150 Training loss 0.015630418434739113 Validation loss 0.022966252639889717 Accuracy 0.763671875\n",
      "Iteration 40160 Training loss 0.018210411071777344 Validation loss 0.021488100290298462 Accuracy 0.779296875\n",
      "Iteration 40170 Training loss 0.018946945667266846 Validation loss 0.021316418424248695 Accuracy 0.78173828125\n",
      "Iteration 40180 Training loss 0.016295459121465683 Validation loss 0.02170073240995407 Accuracy 0.77734375\n",
      "Iteration 40190 Training loss 0.014386354945600033 Validation loss 0.021326130256056786 Accuracy 0.78125\n",
      "Iteration 40200 Training loss 0.01721891574561596 Validation loss 0.02165624499320984 Accuracy 0.77783203125\n",
      "Iteration 40210 Training loss 0.0163265448063612 Validation loss 0.02165081538259983 Accuracy 0.7783203125\n",
      "Iteration 40220 Training loss 0.01715613342821598 Validation loss 0.02124456688761711 Accuracy 0.78271484375\n",
      "Iteration 40230 Training loss 0.01564248837530613 Validation loss 0.021091561764478683 Accuracy 0.783203125\n",
      "Iteration 40240 Training loss 0.017143990844488144 Validation loss 0.021111391484737396 Accuracy 0.78369140625\n",
      "Iteration 40250 Training loss 0.017200130969285965 Validation loss 0.021720824763178825 Accuracy 0.77734375\n",
      "Iteration 40260 Training loss 0.017365725710988045 Validation loss 0.021290678530931473 Accuracy 0.78173828125\n",
      "Iteration 40270 Training loss 0.015477527864277363 Validation loss 0.021535448729991913 Accuracy 0.77880859375\n",
      "Iteration 40280 Training loss 0.016302336007356644 Validation loss 0.021456239745020866 Accuracy 0.779296875\n",
      "Iteration 40290 Training loss 0.01689193770289421 Validation loss 0.021241236478090286 Accuracy 0.7822265625\n",
      "Iteration 40300 Training loss 0.01668187603354454 Validation loss 0.021635418757796288 Accuracy 0.7783203125\n",
      "Iteration 40310 Training loss 0.015229592099785805 Validation loss 0.021356744691729546 Accuracy 0.78173828125\n",
      "Iteration 40320 Training loss 0.017068639397621155 Validation loss 0.02111649326980114 Accuracy 0.78466796875\n",
      "Iteration 40330 Training loss 0.016458269208669662 Validation loss 0.021199168637394905 Accuracy 0.78173828125\n",
      "Iteration 40340 Training loss 0.018136590719223022 Validation loss 0.02154497243463993 Accuracy 0.7783203125\n",
      "Iteration 40350 Training loss 0.016582369804382324 Validation loss 0.021649213507771492 Accuracy 0.77685546875\n",
      "Iteration 40360 Training loss 0.019031474366784096 Validation loss 0.02158798836171627 Accuracy 0.77978515625\n",
      "Iteration 40370 Training loss 0.016648054122924805 Validation loss 0.021163079887628555 Accuracy 0.78369140625\n",
      "Iteration 40380 Training loss 0.017076289281249046 Validation loss 0.02137259766459465 Accuracy 0.78125\n",
      "Iteration 40390 Training loss 0.01581876538693905 Validation loss 0.02123027853667736 Accuracy 0.78173828125\n",
      "Iteration 40400 Training loss 0.014871111139655113 Validation loss 0.021088408306241035 Accuracy 0.78369140625\n",
      "Iteration 40410 Training loss 0.016872551292181015 Validation loss 0.021556679159402847 Accuracy 0.779296875\n",
      "Iteration 40420 Training loss 0.013640953227877617 Validation loss 0.021071435883641243 Accuracy 0.7841796875\n",
      "Iteration 40430 Training loss 0.017104940488934517 Validation loss 0.021253924816846848 Accuracy 0.78173828125\n",
      "Iteration 40440 Training loss 0.018310468643903732 Validation loss 0.0223093181848526 Accuracy 0.7724609375\n",
      "Iteration 40450 Training loss 0.014941578730940819 Validation loss 0.02119632251560688 Accuracy 0.783203125\n",
      "Iteration 40460 Training loss 0.014586173929274082 Validation loss 0.02153393067419529 Accuracy 0.77978515625\n",
      "Iteration 40470 Training loss 0.016809191554784775 Validation loss 0.021238476037979126 Accuracy 0.78271484375\n",
      "Iteration 40480 Training loss 0.016369590535759926 Validation loss 0.021387198939919472 Accuracy 0.78076171875\n",
      "Iteration 40490 Training loss 0.018040474504232407 Validation loss 0.02203710936009884 Accuracy 0.77392578125\n",
      "Iteration 40500 Training loss 0.0132355447858572 Validation loss 0.02103472873568535 Accuracy 0.7841796875\n",
      "Iteration 40510 Training loss 0.015140402130782604 Validation loss 0.02120325155556202 Accuracy 0.78369140625\n",
      "Iteration 40520 Training loss 0.016381163150072098 Validation loss 0.02110285498201847 Accuracy 0.78369140625\n",
      "Iteration 40530 Training loss 0.01576908864080906 Validation loss 0.021355316042900085 Accuracy 0.7822265625\n",
      "Iteration 40540 Training loss 0.01503707841038704 Validation loss 0.021136371418833733 Accuracy 0.7841796875\n",
      "Iteration 40550 Training loss 0.017762862145900726 Validation loss 0.021445611491799355 Accuracy 0.77978515625\n",
      "Iteration 40560 Training loss 0.018170537427067757 Validation loss 0.021641796454787254 Accuracy 0.77783203125\n",
      "Iteration 40570 Training loss 0.013292002491652966 Validation loss 0.02106531709432602 Accuracy 0.7841796875\n",
      "Iteration 40580 Training loss 0.016166796907782555 Validation loss 0.02115662582218647 Accuracy 0.7822265625\n",
      "Iteration 40590 Training loss 0.014742551371455193 Validation loss 0.02125653810799122 Accuracy 0.78271484375\n",
      "Iteration 40600 Training loss 0.02081739716231823 Validation loss 0.0225736815482378 Accuracy 0.767578125\n",
      "Iteration 40610 Training loss 0.015600194223225117 Validation loss 0.021127374842762947 Accuracy 0.783203125\n",
      "Iteration 40620 Training loss 0.016236815601587296 Validation loss 0.02168290689587593 Accuracy 0.77734375\n",
      "Iteration 40630 Training loss 0.014908608980476856 Validation loss 0.02171943336725235 Accuracy 0.77783203125\n",
      "Iteration 40640 Training loss 0.015015017241239548 Validation loss 0.021117357537150383 Accuracy 0.78271484375\n",
      "Iteration 40650 Training loss 0.014951171353459358 Validation loss 0.021219180896878242 Accuracy 0.78369140625\n",
      "Iteration 40660 Training loss 0.015595931559801102 Validation loss 0.021243395283818245 Accuracy 0.783203125\n",
      "Iteration 40670 Training loss 0.016940388828516006 Validation loss 0.021224183961749077 Accuracy 0.7822265625\n",
      "Iteration 40680 Training loss 0.015543652698397636 Validation loss 0.021286677569150925 Accuracy 0.7822265625\n",
      "Iteration 40690 Training loss 0.018399352207779884 Validation loss 0.02141077071428299 Accuracy 0.78173828125\n",
      "Iteration 40700 Training loss 0.01737726666033268 Validation loss 0.021328797563910484 Accuracy 0.78076171875\n",
      "Iteration 40710 Training loss 0.016487956047058105 Validation loss 0.021634027361869812 Accuracy 0.77880859375\n",
      "Iteration 40720 Training loss 0.014141138643026352 Validation loss 0.02126883529126644 Accuracy 0.783203125\n",
      "Iteration 40730 Training loss 0.018449505791068077 Validation loss 0.02120191417634487 Accuracy 0.7822265625\n",
      "Iteration 40740 Training loss 0.016554681584239006 Validation loss 0.02153298817574978 Accuracy 0.77880859375\n",
      "Iteration 40750 Training loss 0.020598093047738075 Validation loss 0.022694237530231476 Accuracy 0.7685546875\n",
      "Iteration 40760 Training loss 0.019922995939850807 Validation loss 0.021095652133226395 Accuracy 0.7841796875\n",
      "Iteration 40770 Training loss 0.016321448609232903 Validation loss 0.021608207374811172 Accuracy 0.7783203125\n",
      "Iteration 40780 Training loss 0.019516460597515106 Validation loss 0.023356517776846886 Accuracy 0.7607421875\n",
      "Iteration 40790 Training loss 0.018290460109710693 Validation loss 0.021505557000637054 Accuracy 0.77978515625\n",
      "Iteration 40800 Training loss 0.013769823126494884 Validation loss 0.02140011452138424 Accuracy 0.7802734375\n",
      "Iteration 40810 Training loss 0.015100094489753246 Validation loss 0.021696344017982483 Accuracy 0.77734375\n",
      "Iteration 40820 Training loss 0.017827650532126427 Validation loss 0.02139580063521862 Accuracy 0.77978515625\n",
      "Iteration 40830 Training loss 0.015399690717458725 Validation loss 0.021620651707053185 Accuracy 0.77783203125\n",
      "Iteration 40840 Training loss 0.01609491929411888 Validation loss 0.021964535117149353 Accuracy 0.77392578125\n",
      "Iteration 40850 Training loss 0.01597437635064125 Validation loss 0.02108541689813137 Accuracy 0.78369140625\n",
      "Iteration 40860 Training loss 0.0177456047385931 Validation loss 0.02177911065518856 Accuracy 0.77734375\n",
      "Iteration 40870 Training loss 0.01801248826086521 Validation loss 0.021363675594329834 Accuracy 0.78125\n",
      "Iteration 40880 Training loss 0.018347958102822304 Validation loss 0.02135232836008072 Accuracy 0.78076171875\n",
      "Iteration 40890 Training loss 0.01627901755273342 Validation loss 0.021163146942853928 Accuracy 0.783203125\n",
      "Iteration 40900 Training loss 0.016360076144337654 Validation loss 0.021350251510739326 Accuracy 0.78076171875\n",
      "Iteration 40910 Training loss 0.01428291853517294 Validation loss 0.021324049681425095 Accuracy 0.78076171875\n",
      "Iteration 40920 Training loss 0.01716916263103485 Validation loss 0.021823152899742126 Accuracy 0.7763671875\n",
      "Iteration 40930 Training loss 0.015487901866436005 Validation loss 0.02113812416791916 Accuracy 0.7841796875\n",
      "Iteration 40940 Training loss 0.015452415682375431 Validation loss 0.02142796851694584 Accuracy 0.7802734375\n",
      "Iteration 40950 Training loss 0.0169216375797987 Validation loss 0.021349340677261353 Accuracy 0.7802734375\n",
      "Iteration 40960 Training loss 0.017063850536942482 Validation loss 0.021205678582191467 Accuracy 0.7822265625\n",
      "Iteration 40970 Training loss 0.01601932756602764 Validation loss 0.021687114611268044 Accuracy 0.77685546875\n",
      "Iteration 40980 Training loss 0.015390535816550255 Validation loss 0.02097134292125702 Accuracy 0.78515625\n",
      "Iteration 40990 Training loss 0.014526313170790672 Validation loss 0.021509619429707527 Accuracy 0.7783203125\n",
      "Iteration 41000 Training loss 0.012958882376551628 Validation loss 0.021278278902173042 Accuracy 0.78173828125\n",
      "Iteration 41010 Training loss 0.017298616468906403 Validation loss 0.021164100617170334 Accuracy 0.7822265625\n",
      "Iteration 41020 Training loss 0.017516998574137688 Validation loss 0.02183879353106022 Accuracy 0.7763671875\n",
      "Iteration 41030 Training loss 0.017810843884944916 Validation loss 0.021269427612423897 Accuracy 0.7822265625\n",
      "Iteration 41040 Training loss 0.017866387963294983 Validation loss 0.02139708399772644 Accuracy 0.7802734375\n",
      "Iteration 41050 Training loss 0.016706427559256554 Validation loss 0.02130049280822277 Accuracy 0.78076171875\n",
      "Iteration 41060 Training loss 0.01416301354765892 Validation loss 0.021549316123127937 Accuracy 0.77783203125\n",
      "Iteration 41070 Training loss 0.016900192946195602 Validation loss 0.0216213446110487 Accuracy 0.77783203125\n",
      "Iteration 41080 Training loss 0.016300054267048836 Validation loss 0.021183988079428673 Accuracy 0.78271484375\n",
      "Iteration 41090 Training loss 0.015304777771234512 Validation loss 0.021529940888285637 Accuracy 0.779296875\n",
      "Iteration 41100 Training loss 0.017708903178572655 Validation loss 0.022077709436416626 Accuracy 0.77392578125\n",
      "Iteration 41110 Training loss 0.017181430011987686 Validation loss 0.021570267155766487 Accuracy 0.77880859375\n",
      "Iteration 41120 Training loss 0.017670484259724617 Validation loss 0.021653281524777412 Accuracy 0.77783203125\n",
      "Iteration 41130 Training loss 0.015017425641417503 Validation loss 0.021149853244423866 Accuracy 0.7822265625\n",
      "Iteration 41140 Training loss 0.016311749815940857 Validation loss 0.021529916673898697 Accuracy 0.77978515625\n",
      "Iteration 41150 Training loss 0.014769631437957287 Validation loss 0.021060844883322716 Accuracy 0.7841796875\n",
      "Iteration 41160 Training loss 0.013436853885650635 Validation loss 0.02148938737809658 Accuracy 0.7783203125\n",
      "Iteration 41170 Training loss 0.017504816874861717 Validation loss 0.021039893850684166 Accuracy 0.78466796875\n",
      "Iteration 41180 Training loss 0.017718728631734848 Validation loss 0.021349014714360237 Accuracy 0.78125\n",
      "Iteration 41190 Training loss 0.015501871705055237 Validation loss 0.021711602807044983 Accuracy 0.77734375\n",
      "Iteration 41200 Training loss 0.01710006780922413 Validation loss 0.02115989662706852 Accuracy 0.7822265625\n",
      "Iteration 41210 Training loss 0.01805255375802517 Validation loss 0.021129360422492027 Accuracy 0.78369140625\n",
      "Iteration 41220 Training loss 0.018112940713763237 Validation loss 0.021375278010964394 Accuracy 0.77978515625\n",
      "Iteration 41230 Training loss 0.017135411500930786 Validation loss 0.02144612744450569 Accuracy 0.77880859375\n",
      "Iteration 41240 Training loss 0.0171660203486681 Validation loss 0.02133684605360031 Accuracy 0.78173828125\n",
      "Iteration 41250 Training loss 0.016872582957148552 Validation loss 0.021512610837817192 Accuracy 0.7802734375\n",
      "Iteration 41260 Training loss 0.015537231229245663 Validation loss 0.021443691104650497 Accuracy 0.77978515625\n",
      "Iteration 41270 Training loss 0.014425466768443584 Validation loss 0.021411659196019173 Accuracy 0.78125\n",
      "Iteration 41280 Training loss 0.02025829255580902 Validation loss 0.02188761718571186 Accuracy 0.775390625\n",
      "Iteration 41290 Training loss 0.01748930662870407 Validation loss 0.021606728434562683 Accuracy 0.77880859375\n",
      "Iteration 41300 Training loss 0.016040021553635597 Validation loss 0.021496303379535675 Accuracy 0.77880859375\n",
      "Iteration 41310 Training loss 0.01498395949602127 Validation loss 0.020995574072003365 Accuracy 0.78466796875\n",
      "Iteration 41320 Training loss 0.017062220722436905 Validation loss 0.02107839845120907 Accuracy 0.7841796875\n",
      "Iteration 41330 Training loss 0.01682719960808754 Validation loss 0.021717309951782227 Accuracy 0.77783203125\n",
      "Iteration 41340 Training loss 0.018949465826153755 Validation loss 0.021445399150252342 Accuracy 0.77978515625\n",
      "Iteration 41350 Training loss 0.014595224522054195 Validation loss 0.021070336923003197 Accuracy 0.783203125\n",
      "Iteration 41360 Training loss 0.017030345275998116 Validation loss 0.02109844610095024 Accuracy 0.78369140625\n",
      "Iteration 41370 Training loss 0.017794622108340263 Validation loss 0.021785592660307884 Accuracy 0.77587890625\n",
      "Iteration 41380 Training loss 0.012635519728064537 Validation loss 0.021145420148968697 Accuracy 0.783203125\n",
      "Iteration 41390 Training loss 0.013003801926970482 Validation loss 0.021408187225461006 Accuracy 0.77978515625\n",
      "Iteration 41400 Training loss 0.01482484582811594 Validation loss 0.02128876931965351 Accuracy 0.78076171875\n",
      "Iteration 41410 Training loss 0.016534971073269844 Validation loss 0.02159515582025051 Accuracy 0.7783203125\n",
      "Iteration 41420 Training loss 0.013341233134269714 Validation loss 0.021260028705000877 Accuracy 0.78125\n",
      "Iteration 41430 Training loss 0.020051632076501846 Validation loss 0.021182894706726074 Accuracy 0.78271484375\n",
      "Iteration 41440 Training loss 0.01660161279141903 Validation loss 0.02182319201529026 Accuracy 0.77587890625\n",
      "Iteration 41450 Training loss 0.016596006229519844 Validation loss 0.02155395783483982 Accuracy 0.779296875\n",
      "Iteration 41460 Training loss 0.016370542347431183 Validation loss 0.021227827295660973 Accuracy 0.78271484375\n",
      "Iteration 41470 Training loss 0.016198376193642616 Validation loss 0.02129111811518669 Accuracy 0.7822265625\n",
      "Iteration 41480 Training loss 0.01733817346394062 Validation loss 0.021544741466641426 Accuracy 0.779296875\n",
      "Iteration 41490 Training loss 0.013276242651045322 Validation loss 0.021168004721403122 Accuracy 0.78369140625\n",
      "Iteration 41500 Training loss 0.018137089908123016 Validation loss 0.021689359098672867 Accuracy 0.77734375\n",
      "Iteration 41510 Training loss 0.015757769346237183 Validation loss 0.021041225641965866 Accuracy 0.78466796875\n",
      "Iteration 41520 Training loss 0.016840238124132156 Validation loss 0.021506762132048607 Accuracy 0.779296875\n",
      "Iteration 41530 Training loss 0.015185531228780746 Validation loss 0.021100984886288643 Accuracy 0.78369140625\n",
      "Iteration 41540 Training loss 0.01561623066663742 Validation loss 0.021010765805840492 Accuracy 0.7841796875\n",
      "Iteration 41550 Training loss 0.01701783947646618 Validation loss 0.02155069261789322 Accuracy 0.77978515625\n",
      "Iteration 41560 Training loss 0.016093038022518158 Validation loss 0.02118999883532524 Accuracy 0.78173828125\n",
      "Iteration 41570 Training loss 0.016123972833156586 Validation loss 0.021405722945928574 Accuracy 0.77978515625\n",
      "Iteration 41580 Training loss 0.01731361821293831 Validation loss 0.022616466507315636 Accuracy 0.7685546875\n",
      "Iteration 41590 Training loss 0.01691594161093235 Validation loss 0.022495843470096588 Accuracy 0.7685546875\n",
      "Iteration 41600 Training loss 0.015140886418521404 Validation loss 0.021332480013370514 Accuracy 0.78125\n",
      "Iteration 41610 Training loss 0.014662004075944424 Validation loss 0.021724559366703033 Accuracy 0.77685546875\n",
      "Iteration 41620 Training loss 0.017696868628263474 Validation loss 0.0210720207542181 Accuracy 0.78369140625\n",
      "Iteration 41630 Training loss 0.01642512157559395 Validation loss 0.021552674472332 Accuracy 0.77880859375\n",
      "Iteration 41640 Training loss 0.017568189650774002 Validation loss 0.02167748473584652 Accuracy 0.77783203125\n",
      "Iteration 41650 Training loss 0.017526857554912567 Validation loss 0.0214778333902359 Accuracy 0.77880859375\n",
      "Iteration 41660 Training loss 0.014493792317807674 Validation loss 0.021542316302657127 Accuracy 0.7783203125\n",
      "Iteration 41670 Training loss 0.01688263937830925 Validation loss 0.021504763513803482 Accuracy 0.7783203125\n",
      "Iteration 41680 Training loss 0.016002561897039413 Validation loss 0.021311594173312187 Accuracy 0.78076171875\n",
      "Iteration 41690 Training loss 0.01864481158554554 Validation loss 0.021181464195251465 Accuracy 0.7822265625\n",
      "Iteration 41700 Training loss 0.01460447907447815 Validation loss 0.021612411364912987 Accuracy 0.779296875\n",
      "Iteration 41710 Training loss 0.01317578461021185 Validation loss 0.02119368687272072 Accuracy 0.78271484375\n",
      "Iteration 41720 Training loss 0.013196166604757309 Validation loss 0.02136077545583248 Accuracy 0.78076171875\n",
      "Iteration 41730 Training loss 0.015619782730937004 Validation loss 0.021820444613695145 Accuracy 0.77685546875\n",
      "Iteration 41740 Training loss 0.01641993224620819 Validation loss 0.02121908590197563 Accuracy 0.78076171875\n",
      "Iteration 41750 Training loss 0.018340248614549637 Validation loss 0.02142670378088951 Accuracy 0.7802734375\n",
      "Iteration 41760 Training loss 0.01600109413266182 Validation loss 0.02118026837706566 Accuracy 0.7822265625\n",
      "Iteration 41770 Training loss 0.015765933319926262 Validation loss 0.021154720336198807 Accuracy 0.78271484375\n",
      "Iteration 41780 Training loss 0.014222259633243084 Validation loss 0.021123042330145836 Accuracy 0.783203125\n",
      "Iteration 41790 Training loss 0.016006596386432648 Validation loss 0.02216082252562046 Accuracy 0.7734375\n",
      "Iteration 41800 Training loss 0.016171079128980637 Validation loss 0.021116115152835846 Accuracy 0.78369140625\n",
      "Iteration 41810 Training loss 0.015155601315200329 Validation loss 0.021477801725268364 Accuracy 0.7783203125\n",
      "Iteration 41820 Training loss 0.015026361681520939 Validation loss 0.02098638191819191 Accuracy 0.78466796875\n",
      "Iteration 41830 Training loss 0.01341311540454626 Validation loss 0.021540626883506775 Accuracy 0.77880859375\n",
      "Iteration 41840 Training loss 0.016680028289556503 Validation loss 0.021039027720689774 Accuracy 0.78466796875\n",
      "Iteration 41850 Training loss 0.01644929312169552 Validation loss 0.021127885207533836 Accuracy 0.783203125\n",
      "Iteration 41860 Training loss 0.015970496460795403 Validation loss 0.02130935899913311 Accuracy 0.78076171875\n",
      "Iteration 41870 Training loss 0.01637939363718033 Validation loss 0.021299073472619057 Accuracy 0.78125\n",
      "Iteration 41880 Training loss 0.017610307782888412 Validation loss 0.021489188075065613 Accuracy 0.77978515625\n",
      "Iteration 41890 Training loss 0.018223220482468605 Validation loss 0.02119326777756214 Accuracy 0.78271484375\n",
      "Iteration 41900 Training loss 0.015842201188206673 Validation loss 0.021269047632813454 Accuracy 0.78173828125\n",
      "Iteration 41910 Training loss 0.015907417982816696 Validation loss 0.0209178663790226 Accuracy 0.78515625\n",
      "Iteration 41920 Training loss 0.018451591953635216 Validation loss 0.021870555356144905 Accuracy 0.7763671875\n",
      "Iteration 41930 Training loss 0.01512121595442295 Validation loss 0.02164611965417862 Accuracy 0.7783203125\n",
      "Iteration 41940 Training loss 0.019193967804312706 Validation loss 0.02212236449122429 Accuracy 0.77392578125\n",
      "Iteration 41950 Training loss 0.01814265176653862 Validation loss 0.021195895969867706 Accuracy 0.7822265625\n",
      "Iteration 41960 Training loss 0.016009621322155 Validation loss 0.02130676619708538 Accuracy 0.78125\n",
      "Iteration 41970 Training loss 0.014419925399124622 Validation loss 0.021451516076922417 Accuracy 0.78076171875\n",
      "Iteration 41980 Training loss 0.014626673422753811 Validation loss 0.0213970597833395 Accuracy 0.78125\n",
      "Iteration 41990 Training loss 0.015289797447621822 Validation loss 0.021362990140914917 Accuracy 0.78173828125\n",
      "Iteration 42000 Training loss 0.014829307794570923 Validation loss 0.021623535081744194 Accuracy 0.779296875\n",
      "Iteration 42010 Training loss 0.01754552125930786 Validation loss 0.021689461544156075 Accuracy 0.77783203125\n",
      "Iteration 42020 Training loss 0.016230689361691475 Validation loss 0.021246908232569695 Accuracy 0.7822265625\n",
      "Iteration 42030 Training loss 0.015576861798763275 Validation loss 0.021804749965667725 Accuracy 0.7763671875\n",
      "Iteration 42040 Training loss 0.01585371047258377 Validation loss 0.021344508975744247 Accuracy 0.78076171875\n",
      "Iteration 42050 Training loss 0.017463602125644684 Validation loss 0.02100943587720394 Accuracy 0.78515625\n",
      "Iteration 42060 Training loss 0.0172426737844944 Validation loss 0.022113030776381493 Accuracy 0.7734375\n",
      "Iteration 42070 Training loss 0.016512712463736534 Validation loss 0.02141043357551098 Accuracy 0.77978515625\n",
      "Iteration 42080 Training loss 0.014703087508678436 Validation loss 0.021648485213518143 Accuracy 0.7783203125\n",
      "Iteration 42090 Training loss 0.016182800754904747 Validation loss 0.02116205543279648 Accuracy 0.78271484375\n",
      "Iteration 42100 Training loss 0.01873898133635521 Validation loss 0.021152377128601074 Accuracy 0.78271484375\n",
      "Iteration 42110 Training loss 0.016865871846675873 Validation loss 0.02121099643409252 Accuracy 0.7822265625\n",
      "Iteration 42120 Training loss 0.014866260811686516 Validation loss 0.021199041977524757 Accuracy 0.783203125\n",
      "Iteration 42130 Training loss 0.019054410979151726 Validation loss 0.021346597000956535 Accuracy 0.78076171875\n",
      "Iteration 42140 Training loss 0.01274795550853014 Validation loss 0.021074047312140465 Accuracy 0.78369140625\n",
      "Iteration 42150 Training loss 0.017332427203655243 Validation loss 0.021348416805267334 Accuracy 0.78173828125\n",
      "Iteration 42160 Training loss 0.018162503838539124 Validation loss 0.021359223872423172 Accuracy 0.78125\n",
      "Iteration 42170 Training loss 0.015357933938503265 Validation loss 0.02126508578658104 Accuracy 0.7822265625\n",
      "Iteration 42180 Training loss 0.013881511986255646 Validation loss 0.021143322810530663 Accuracy 0.783203125\n",
      "Iteration 42190 Training loss 0.016035189852118492 Validation loss 0.02147895283997059 Accuracy 0.77978515625\n",
      "Iteration 42200 Training loss 0.012600911781191826 Validation loss 0.021149367094039917 Accuracy 0.78271484375\n",
      "Iteration 42210 Training loss 0.017421351745724678 Validation loss 0.021539969369769096 Accuracy 0.7802734375\n",
      "Iteration 42220 Training loss 0.01775236614048481 Validation loss 0.02149907499551773 Accuracy 0.77880859375\n",
      "Iteration 42230 Training loss 0.01605425961315632 Validation loss 0.02160721831023693 Accuracy 0.7783203125\n",
      "Iteration 42240 Training loss 0.015623990446329117 Validation loss 0.02128686010837555 Accuracy 0.78125\n",
      "Iteration 42250 Training loss 0.016423258930444717 Validation loss 0.021021362394094467 Accuracy 0.78466796875\n",
      "Iteration 42260 Training loss 0.01802416890859604 Validation loss 0.021154019981622696 Accuracy 0.78369140625\n",
      "Iteration 42270 Training loss 0.014847389422357082 Validation loss 0.021303284913301468 Accuracy 0.78125\n",
      "Iteration 42280 Training loss 0.015747319906949997 Validation loss 0.021311821416020393 Accuracy 0.78076171875\n",
      "Iteration 42290 Training loss 0.018718158826231956 Validation loss 0.02175283618271351 Accuracy 0.77734375\n",
      "Iteration 42300 Training loss 0.014320868998765945 Validation loss 0.021313466131687164 Accuracy 0.78125\n",
      "Iteration 42310 Training loss 0.016706041991710663 Validation loss 0.02137201651930809 Accuracy 0.78076171875\n",
      "Iteration 42320 Training loss 0.015875982120633125 Validation loss 0.021061915904283524 Accuracy 0.78466796875\n",
      "Iteration 42330 Training loss 0.011503435671329498 Validation loss 0.0213431715965271 Accuracy 0.78076171875\n",
      "Iteration 42340 Training loss 0.020174812525510788 Validation loss 0.022517098113894463 Accuracy 0.76904296875\n",
      "Iteration 42350 Training loss 0.017557568848133087 Validation loss 0.02112237736582756 Accuracy 0.78369140625\n",
      "Iteration 42360 Training loss 0.01697058416903019 Validation loss 0.021243680268526077 Accuracy 0.78271484375\n",
      "Iteration 42370 Training loss 0.018706483766436577 Validation loss 0.021370677277445793 Accuracy 0.78173828125\n",
      "Iteration 42380 Training loss 0.01959829032421112 Validation loss 0.02112756296992302 Accuracy 0.7841796875\n",
      "Iteration 42390 Training loss 0.015358230099081993 Validation loss 0.021360427141189575 Accuracy 0.78173828125\n",
      "Iteration 42400 Training loss 0.0155023830011487 Validation loss 0.021645212545990944 Accuracy 0.7783203125\n",
      "Iteration 42410 Training loss 0.019535575062036514 Validation loss 0.0211334228515625 Accuracy 0.783203125\n",
      "Iteration 42420 Training loss 0.018520431593060493 Validation loss 0.02165886200964451 Accuracy 0.7783203125\n",
      "Iteration 42430 Training loss 0.018177930265665054 Validation loss 0.021147387102246284 Accuracy 0.78271484375\n",
      "Iteration 42440 Training loss 0.016536934301257133 Validation loss 0.02118385210633278 Accuracy 0.783203125\n",
      "Iteration 42450 Training loss 0.018239876255393028 Validation loss 0.02188597060739994 Accuracy 0.77490234375\n",
      "Iteration 42460 Training loss 0.014873514883220196 Validation loss 0.021123265847563744 Accuracy 0.783203125\n",
      "Iteration 42470 Training loss 0.017365651205182076 Validation loss 0.021647494286298752 Accuracy 0.77880859375\n",
      "Iteration 42480 Training loss 0.017469419166445732 Validation loss 0.021370675414800644 Accuracy 0.78173828125\n",
      "Iteration 42490 Training loss 0.016490835696458817 Validation loss 0.021861014887690544 Accuracy 0.77587890625\n",
      "Iteration 42500 Training loss 0.015916401520371437 Validation loss 0.021255094558000565 Accuracy 0.78173828125\n",
      "Iteration 42510 Training loss 0.017079470679163933 Validation loss 0.021224437281489372 Accuracy 0.78173828125\n",
      "Iteration 42520 Training loss 0.015740087255835533 Validation loss 0.021696388721466064 Accuracy 0.77685546875\n",
      "Iteration 42530 Training loss 0.018445024266839027 Validation loss 0.021578175947070122 Accuracy 0.77783203125\n",
      "Iteration 42540 Training loss 0.018682893365621567 Validation loss 0.0212494395673275 Accuracy 0.78125\n",
      "Iteration 42550 Training loss 0.01534214522689581 Validation loss 0.021068422123789787 Accuracy 0.7841796875\n",
      "Iteration 42560 Training loss 0.017555121332406998 Validation loss 0.02175760269165039 Accuracy 0.77685546875\n",
      "Iteration 42570 Training loss 0.014889147132635117 Validation loss 0.021087978035211563 Accuracy 0.7841796875\n",
      "Iteration 42580 Training loss 0.015500581823289394 Validation loss 0.021241219714283943 Accuracy 0.7822265625\n",
      "Iteration 42590 Training loss 0.016652941703796387 Validation loss 0.021159319207072258 Accuracy 0.78271484375\n",
      "Iteration 42600 Training loss 0.01364695094525814 Validation loss 0.02137770876288414 Accuracy 0.78076171875\n",
      "Iteration 42610 Training loss 0.017033005133271217 Validation loss 0.021138407289981842 Accuracy 0.78271484375\n",
      "Iteration 42620 Training loss 0.016712402924895287 Validation loss 0.021137723699212074 Accuracy 0.78369140625\n",
      "Iteration 42630 Training loss 0.015729425475001335 Validation loss 0.02174864150583744 Accuracy 0.775390625\n",
      "Iteration 42640 Training loss 0.017269650474190712 Validation loss 0.021406030282378197 Accuracy 0.78076171875\n",
      "Iteration 42650 Training loss 0.01700466126203537 Validation loss 0.02155744843184948 Accuracy 0.77880859375\n",
      "Iteration 42660 Training loss 0.01538823638111353 Validation loss 0.021296318620443344 Accuracy 0.7822265625\n",
      "Iteration 42670 Training loss 0.01699909381568432 Validation loss 0.021355463191866875 Accuracy 0.78076171875\n",
      "Iteration 42680 Training loss 0.017845431342720985 Validation loss 0.020995058119297028 Accuracy 0.78466796875\n",
      "Iteration 42690 Training loss 0.015880201011896133 Validation loss 0.021230168640613556 Accuracy 0.7822265625\n",
      "Iteration 42700 Training loss 0.017337564378976822 Validation loss 0.021318664774298668 Accuracy 0.78125\n",
      "Iteration 42710 Training loss 0.014783988706767559 Validation loss 0.021109331399202347 Accuracy 0.78369140625\n",
      "Iteration 42720 Training loss 0.015508586540818214 Validation loss 0.021122630685567856 Accuracy 0.78271484375\n",
      "Iteration 42730 Training loss 0.015732264146208763 Validation loss 0.021163886412978172 Accuracy 0.783203125\n",
      "Iteration 42740 Training loss 0.01777942292392254 Validation loss 0.021047601476311684 Accuracy 0.78369140625\n",
      "Iteration 42750 Training loss 0.014686751179397106 Validation loss 0.021372279152274132 Accuracy 0.78076171875\n",
      "Iteration 42760 Training loss 0.015229313634335995 Validation loss 0.021465659141540527 Accuracy 0.7802734375\n",
      "Iteration 42770 Training loss 0.013326787389814854 Validation loss 0.020956432446837425 Accuracy 0.78515625\n",
      "Iteration 42780 Training loss 0.016986118629574776 Validation loss 0.021074673160910606 Accuracy 0.78369140625\n",
      "Iteration 42790 Training loss 0.01777912862598896 Validation loss 0.021261043846607208 Accuracy 0.78173828125\n",
      "Iteration 42800 Training loss 0.01628260314464569 Validation loss 0.02111729420721531 Accuracy 0.783203125\n",
      "Iteration 42810 Training loss 0.017759142443537712 Validation loss 0.02116926945745945 Accuracy 0.783203125\n",
      "Iteration 42820 Training loss 0.016655972227454185 Validation loss 0.021235931664705276 Accuracy 0.7822265625\n",
      "Iteration 42830 Training loss 0.017867563292384148 Validation loss 0.021157441660761833 Accuracy 0.78369140625\n",
      "Iteration 42840 Training loss 0.015633856877684593 Validation loss 0.021165942773222923 Accuracy 0.78369140625\n",
      "Iteration 42850 Training loss 0.018912026658654213 Validation loss 0.021644223481416702 Accuracy 0.7783203125\n",
      "Iteration 42860 Training loss 0.015627633780241013 Validation loss 0.02154758758842945 Accuracy 0.77880859375\n",
      "Iteration 42870 Training loss 0.017875580117106438 Validation loss 0.021954195573925972 Accuracy 0.77392578125\n",
      "Iteration 42880 Training loss 0.01536627672612667 Validation loss 0.021820468828082085 Accuracy 0.77587890625\n",
      "Iteration 42890 Training loss 0.01728726178407669 Validation loss 0.02141357772052288 Accuracy 0.77978515625\n",
      "Iteration 42900 Training loss 0.016427339985966682 Validation loss 0.021246591582894325 Accuracy 0.78271484375\n",
      "Iteration 42910 Training loss 0.018832450732588768 Validation loss 0.021368127316236496 Accuracy 0.7802734375\n",
      "Iteration 42920 Training loss 0.01536018867045641 Validation loss 0.021090468391776085 Accuracy 0.78466796875\n",
      "Iteration 42930 Training loss 0.013779428787529469 Validation loss 0.021054983139038086 Accuracy 0.7841796875\n",
      "Iteration 42940 Training loss 0.015599641017615795 Validation loss 0.02124081552028656 Accuracy 0.7822265625\n",
      "Iteration 42950 Training loss 0.016958318650722504 Validation loss 0.020925233140587807 Accuracy 0.78466796875\n",
      "Iteration 42960 Training loss 0.019498461857438087 Validation loss 0.021057402715086937 Accuracy 0.78369140625\n",
      "Iteration 42970 Training loss 0.020843034610152245 Validation loss 0.021163618192076683 Accuracy 0.78369140625\n",
      "Iteration 42980 Training loss 0.016129298135638237 Validation loss 0.021093202754855156 Accuracy 0.783203125\n",
      "Iteration 42990 Training loss 0.01548832654953003 Validation loss 0.021309858188033104 Accuracy 0.78125\n",
      "Iteration 43000 Training loss 0.01512722298502922 Validation loss 0.02117270976305008 Accuracy 0.783203125\n",
      "Iteration 43010 Training loss 0.0184056144207716 Validation loss 0.0219491645693779 Accuracy 0.7744140625\n",
      "Iteration 43020 Training loss 0.018772533163428307 Validation loss 0.021268507465720177 Accuracy 0.78173828125\n",
      "Iteration 43030 Training loss 0.01624223217368126 Validation loss 0.021887585520744324 Accuracy 0.775390625\n",
      "Iteration 43040 Training loss 0.015937859192490578 Validation loss 0.021114954724907875 Accuracy 0.783203125\n",
      "Iteration 43050 Training loss 0.015463459305465221 Validation loss 0.02102973498404026 Accuracy 0.78515625\n",
      "Iteration 43060 Training loss 0.015220241621136665 Validation loss 0.021640438586473465 Accuracy 0.77734375\n",
      "Iteration 43070 Training loss 0.015097455121576786 Validation loss 0.021280229091644287 Accuracy 0.7822265625\n",
      "Iteration 43080 Training loss 0.016446899622678757 Validation loss 0.021345585584640503 Accuracy 0.78076171875\n",
      "Iteration 43090 Training loss 0.014513702131807804 Validation loss 0.021419964730739594 Accuracy 0.78076171875\n",
      "Iteration 43100 Training loss 0.01813613250851631 Validation loss 0.021441098302602768 Accuracy 0.77978515625\n",
      "Iteration 43110 Training loss 0.014337373897433281 Validation loss 0.021059105172753334 Accuracy 0.78369140625\n",
      "Iteration 43120 Training loss 0.01617549918591976 Validation loss 0.02122345194220543 Accuracy 0.7822265625\n",
      "Iteration 43130 Training loss 0.015380515716969967 Validation loss 0.021036166697740555 Accuracy 0.783203125\n",
      "Iteration 43140 Training loss 0.018503254279494286 Validation loss 0.0210538599640131 Accuracy 0.78369140625\n",
      "Iteration 43150 Training loss 0.015130029991269112 Validation loss 0.021258866414427757 Accuracy 0.78173828125\n",
      "Iteration 43160 Training loss 0.013876748271286488 Validation loss 0.022076603025197983 Accuracy 0.7724609375\n",
      "Iteration 43170 Training loss 0.018489789217710495 Validation loss 0.021824559196829796 Accuracy 0.7763671875\n",
      "Iteration 43180 Training loss 0.016003374010324478 Validation loss 0.02122359722852707 Accuracy 0.78173828125\n",
      "Iteration 43190 Training loss 0.0151571249589324 Validation loss 0.021045692265033722 Accuracy 0.78466796875\n",
      "Iteration 43200 Training loss 0.015958450734615326 Validation loss 0.021028641611337662 Accuracy 0.78466796875\n",
      "Iteration 43210 Training loss 0.01748983934521675 Validation loss 0.021605435758829117 Accuracy 0.77880859375\n",
      "Iteration 43220 Training loss 0.019412770867347717 Validation loss 0.02134345844388008 Accuracy 0.78076171875\n",
      "Iteration 43230 Training loss 0.01568938046693802 Validation loss 0.02128240093588829 Accuracy 0.78173828125\n",
      "Iteration 43240 Training loss 0.016642171889543533 Validation loss 0.02171141840517521 Accuracy 0.7783203125\n",
      "Iteration 43250 Training loss 0.017337346449494362 Validation loss 0.022036099806427956 Accuracy 0.77294921875\n",
      "Iteration 43260 Training loss 0.014615235850214958 Validation loss 0.021058881655335426 Accuracy 0.78466796875\n",
      "Iteration 43270 Training loss 0.015897095203399658 Validation loss 0.021137528121471405 Accuracy 0.7841796875\n",
      "Iteration 43280 Training loss 0.016811449080705643 Validation loss 0.021392345428466797 Accuracy 0.7802734375\n",
      "Iteration 43290 Training loss 0.014873087406158447 Validation loss 0.021048760041594505 Accuracy 0.78369140625\n",
      "Iteration 43300 Training loss 0.016901390627026558 Validation loss 0.02118644490838051 Accuracy 0.78271484375\n",
      "Iteration 43310 Training loss 0.016692567616701126 Validation loss 0.021665185689926147 Accuracy 0.77734375\n",
      "Iteration 43320 Training loss 0.017024710774421692 Validation loss 0.021648680791258812 Accuracy 0.77880859375\n",
      "Iteration 43330 Training loss 0.014520853757858276 Validation loss 0.0209267009049654 Accuracy 0.78515625\n",
      "Iteration 43340 Training loss 0.015291829593479633 Validation loss 0.022450333461165428 Accuracy 0.76953125\n",
      "Iteration 43350 Training loss 0.016366245225071907 Validation loss 0.02153637446463108 Accuracy 0.77734375\n",
      "Iteration 43360 Training loss 0.014480880461633205 Validation loss 0.02116471156477928 Accuracy 0.78271484375\n",
      "Iteration 43370 Training loss 0.012778975069522858 Validation loss 0.021108118817210197 Accuracy 0.7841796875\n",
      "Iteration 43380 Training loss 0.01564639061689377 Validation loss 0.021296150982379913 Accuracy 0.78125\n",
      "Iteration 43390 Training loss 0.0168263241648674 Validation loss 0.020845048129558563 Accuracy 0.7861328125\n",
      "Iteration 43400 Training loss 0.015357607044279575 Validation loss 0.02090853452682495 Accuracy 0.78466796875\n",
      "Iteration 43410 Training loss 0.01717379316687584 Validation loss 0.020916972309350967 Accuracy 0.78564453125\n",
      "Iteration 43420 Training loss 0.018025260418653488 Validation loss 0.0226520337164402 Accuracy 0.76708984375\n",
      "Iteration 43430 Training loss 0.016135793179273605 Validation loss 0.021494342014193535 Accuracy 0.7783203125\n",
      "Iteration 43440 Training loss 0.015004938468337059 Validation loss 0.021277587860822678 Accuracy 0.78173828125\n",
      "Iteration 43450 Training loss 0.01610933057963848 Validation loss 0.021992065012454987 Accuracy 0.775390625\n",
      "Iteration 43460 Training loss 0.017029231414198875 Validation loss 0.021130327135324478 Accuracy 0.783203125\n",
      "Iteration 43470 Training loss 0.01524000521749258 Validation loss 0.021119657903909683 Accuracy 0.78369140625\n",
      "Iteration 43480 Training loss 0.018161648884415627 Validation loss 0.021046027541160583 Accuracy 0.78466796875\n",
      "Iteration 43490 Training loss 0.013279571197926998 Validation loss 0.02099565416574478 Accuracy 0.78466796875\n",
      "Iteration 43500 Training loss 0.01630041003227234 Validation loss 0.02136596292257309 Accuracy 0.78173828125\n",
      "Iteration 43510 Training loss 0.015493682585656643 Validation loss 0.02126437984406948 Accuracy 0.78173828125\n",
      "Iteration 43520 Training loss 0.018746180459856987 Validation loss 0.02132526785135269 Accuracy 0.78076171875\n",
      "Iteration 43530 Training loss 0.014996709302067757 Validation loss 0.02120289020240307 Accuracy 0.78271484375\n",
      "Iteration 43540 Training loss 0.016385843977332115 Validation loss 0.02159091830253601 Accuracy 0.7783203125\n",
      "Iteration 43550 Training loss 0.014888547360897064 Validation loss 0.02153603732585907 Accuracy 0.779296875\n",
      "Iteration 43560 Training loss 0.016744928434491158 Validation loss 0.021191054955124855 Accuracy 0.783203125\n",
      "Iteration 43570 Training loss 0.014643071219325066 Validation loss 0.02125641144812107 Accuracy 0.7822265625\n",
      "Iteration 43580 Training loss 0.01490839384496212 Validation loss 0.021233055740594864 Accuracy 0.7822265625\n",
      "Iteration 43590 Training loss 0.019247787073254585 Validation loss 0.022138351574540138 Accuracy 0.7724609375\n",
      "Iteration 43600 Training loss 0.01700911670923233 Validation loss 0.021199993789196014 Accuracy 0.7822265625\n",
      "Iteration 43610 Training loss 0.01685144193470478 Validation loss 0.021454550325870514 Accuracy 0.77978515625\n",
      "Iteration 43620 Training loss 0.019036587327718735 Validation loss 0.02162846550345421 Accuracy 0.77783203125\n",
      "Iteration 43630 Training loss 0.015943702310323715 Validation loss 0.02173672243952751 Accuracy 0.77587890625\n",
      "Iteration 43640 Training loss 0.01679694652557373 Validation loss 0.02157560922205448 Accuracy 0.779296875\n",
      "Iteration 43650 Training loss 0.017574602738022804 Validation loss 0.02212609350681305 Accuracy 0.7734375\n",
      "Iteration 43660 Training loss 0.01706903986632824 Validation loss 0.021313685923814774 Accuracy 0.78125\n",
      "Iteration 43670 Training loss 0.012702921405434608 Validation loss 0.021199535578489304 Accuracy 0.78125\n",
      "Iteration 43680 Training loss 0.018254410475492477 Validation loss 0.021848872303962708 Accuracy 0.7763671875\n",
      "Iteration 43690 Training loss 0.016206538304686546 Validation loss 0.021579129621386528 Accuracy 0.77978515625\n",
      "Iteration 43700 Training loss 0.01363292708992958 Validation loss 0.02124612405896187 Accuracy 0.783203125\n",
      "Iteration 43710 Training loss 0.015121121890842915 Validation loss 0.021425744518637657 Accuracy 0.77978515625\n",
      "Iteration 43720 Training loss 0.015295126475393772 Validation loss 0.021390434354543686 Accuracy 0.7802734375\n",
      "Iteration 43730 Training loss 0.018516236916184425 Validation loss 0.021231772378087044 Accuracy 0.78271484375\n",
      "Iteration 43740 Training loss 0.01825878582894802 Validation loss 0.021557020023465157 Accuracy 0.77880859375\n",
      "Iteration 43750 Training loss 0.01831970177590847 Validation loss 0.021956542506814003 Accuracy 0.7744140625\n",
      "Iteration 43760 Training loss 0.01674519293010235 Validation loss 0.02097790502011776 Accuracy 0.78515625\n",
      "Iteration 43770 Training loss 0.013140164315700531 Validation loss 0.021035971119999886 Accuracy 0.78564453125\n",
      "Iteration 43780 Training loss 0.015786804258823395 Validation loss 0.021583490073680878 Accuracy 0.779296875\n",
      "Iteration 43790 Training loss 0.01693851500749588 Validation loss 0.02118341252207756 Accuracy 0.78271484375\n",
      "Iteration 43800 Training loss 0.018641289323568344 Validation loss 0.021370312198996544 Accuracy 0.78173828125\n",
      "Iteration 43810 Training loss 0.017097851261496544 Validation loss 0.021038154140114784 Accuracy 0.7841796875\n",
      "Iteration 43820 Training loss 0.017874034121632576 Validation loss 0.021735267713665962 Accuracy 0.77734375\n",
      "Iteration 43830 Training loss 0.01513652317225933 Validation loss 0.021404515951871872 Accuracy 0.7802734375\n",
      "Iteration 43840 Training loss 0.016770748421549797 Validation loss 0.02103996090590954 Accuracy 0.7841796875\n",
      "Iteration 43850 Training loss 0.016082219779491425 Validation loss 0.021023407578468323 Accuracy 0.7841796875\n",
      "Iteration 43860 Training loss 0.01771928369998932 Validation loss 0.021579580381512642 Accuracy 0.77783203125\n",
      "Iteration 43870 Training loss 0.016685035079717636 Validation loss 0.022203143686056137 Accuracy 0.77197265625\n",
      "Iteration 43880 Training loss 0.016458341851830482 Validation loss 0.021276136860251427 Accuracy 0.7822265625\n",
      "Iteration 43890 Training loss 0.013976679183542728 Validation loss 0.021470483392477036 Accuracy 0.779296875\n",
      "Iteration 43900 Training loss 0.015897855162620544 Validation loss 0.021192526444792747 Accuracy 0.7822265625\n",
      "Iteration 43910 Training loss 0.014170591719448566 Validation loss 0.021510755643248558 Accuracy 0.779296875\n",
      "Iteration 43920 Training loss 0.016112005338072777 Validation loss 0.021247543394565582 Accuracy 0.78125\n",
      "Iteration 43930 Training loss 0.013879010453820229 Validation loss 0.02119377814233303 Accuracy 0.7822265625\n",
      "Iteration 43940 Training loss 0.018636370077729225 Validation loss 0.021143227815628052 Accuracy 0.78369140625\n",
      "Iteration 43950 Training loss 0.015951890498399734 Validation loss 0.02106647938489914 Accuracy 0.78466796875\n",
      "Iteration 43960 Training loss 0.015640556812286377 Validation loss 0.021003270521759987 Accuracy 0.7841796875\n",
      "Iteration 43970 Training loss 0.014774085953831673 Validation loss 0.021023720502853394 Accuracy 0.78515625\n",
      "Iteration 43980 Training loss 0.016534890979528427 Validation loss 0.021151268854737282 Accuracy 0.78271484375\n",
      "Iteration 43990 Training loss 0.01619386486709118 Validation loss 0.02135404199361801 Accuracy 0.78076171875\n",
      "Iteration 44000 Training loss 0.014884239993989468 Validation loss 0.021096162497997284 Accuracy 0.783203125\n",
      "Iteration 44010 Training loss 0.014564459212124348 Validation loss 0.021076221019029617 Accuracy 0.78369140625\n",
      "Iteration 44020 Training loss 0.016331827268004417 Validation loss 0.02152274362742901 Accuracy 0.77880859375\n",
      "Iteration 44030 Training loss 0.016247693449258804 Validation loss 0.02111326903104782 Accuracy 0.78369140625\n",
      "Iteration 44040 Training loss 0.017348527908325195 Validation loss 0.021322792395949364 Accuracy 0.78125\n",
      "Iteration 44050 Training loss 0.01891431212425232 Validation loss 0.022487953305244446 Accuracy 0.77001953125\n",
      "Iteration 44060 Training loss 0.014052186161279678 Validation loss 0.021003788337111473 Accuracy 0.78466796875\n",
      "Iteration 44070 Training loss 0.018681727349758148 Validation loss 0.02111009694635868 Accuracy 0.78271484375\n",
      "Iteration 44080 Training loss 0.019271891564130783 Validation loss 0.02222442254424095 Accuracy 0.77197265625\n",
      "Iteration 44090 Training loss 0.01659301295876503 Validation loss 0.021204547956585884 Accuracy 0.7822265625\n",
      "Iteration 44100 Training loss 0.014962688088417053 Validation loss 0.021280141547322273 Accuracy 0.78173828125\n",
      "Iteration 44110 Training loss 0.014922447502613068 Validation loss 0.021068312227725983 Accuracy 0.78369140625\n",
      "Iteration 44120 Training loss 0.013695264235138893 Validation loss 0.0211414135992527 Accuracy 0.78369140625\n",
      "Iteration 44130 Training loss 0.014671191573143005 Validation loss 0.02109682373702526 Accuracy 0.78271484375\n",
      "Iteration 44140 Training loss 0.016405997797846794 Validation loss 0.021242868155241013 Accuracy 0.78173828125\n",
      "Iteration 44150 Training loss 0.015843721106648445 Validation loss 0.02133631333708763 Accuracy 0.78173828125\n",
      "Iteration 44160 Training loss 0.01903017982840538 Validation loss 0.021005233749747276 Accuracy 0.7841796875\n",
      "Iteration 44170 Training loss 0.013058949261903763 Validation loss 0.021157624199986458 Accuracy 0.78271484375\n",
      "Iteration 44180 Training loss 0.014253388158977032 Validation loss 0.021457722410559654 Accuracy 0.78076171875\n",
      "Iteration 44190 Training loss 0.017077164724469185 Validation loss 0.02160150371491909 Accuracy 0.77783203125\n",
      "Iteration 44200 Training loss 0.013169611804187298 Validation loss 0.02091597206890583 Accuracy 0.78515625\n",
      "Iteration 44210 Training loss 0.014753060415387154 Validation loss 0.021403154358267784 Accuracy 0.78076171875\n",
      "Iteration 44220 Training loss 0.013511900790035725 Validation loss 0.02098812162876129 Accuracy 0.7861328125\n",
      "Iteration 44230 Training loss 0.017122047021985054 Validation loss 0.021018177270889282 Accuracy 0.78466796875\n",
      "Iteration 44240 Training loss 0.01741558127105236 Validation loss 0.02079157717525959 Accuracy 0.78662109375\n",
      "Iteration 44250 Training loss 0.015447870828211308 Validation loss 0.021223481744527817 Accuracy 0.78271484375\n",
      "Iteration 44260 Training loss 0.013409784995019436 Validation loss 0.02104122005403042 Accuracy 0.78466796875\n",
      "Iteration 44270 Training loss 0.014782583341002464 Validation loss 0.02124837413430214 Accuracy 0.78271484375\n",
      "Iteration 44280 Training loss 0.01824438013136387 Validation loss 0.02111302874982357 Accuracy 0.7841796875\n",
      "Iteration 44290 Training loss 0.015495272353291512 Validation loss 0.021022450178861618 Accuracy 0.78515625\n",
      "Iteration 44300 Training loss 0.015914015471935272 Validation loss 0.021491767838597298 Accuracy 0.77880859375\n",
      "Iteration 44310 Training loss 0.013814033009111881 Validation loss 0.02103007212281227 Accuracy 0.78369140625\n",
      "Iteration 44320 Training loss 0.016863223165273666 Validation loss 0.021219896152615547 Accuracy 0.783203125\n",
      "Iteration 44330 Training loss 0.014044455252587795 Validation loss 0.020967179909348488 Accuracy 0.78515625\n",
      "Iteration 44340 Training loss 0.01554512232542038 Validation loss 0.021234426647424698 Accuracy 0.7822265625\n",
      "Iteration 44350 Training loss 0.014121065847575665 Validation loss 0.02098683826625347 Accuracy 0.78466796875\n",
      "Iteration 44360 Training loss 0.015452341176569462 Validation loss 0.020951254293322563 Accuracy 0.78466796875\n",
      "Iteration 44370 Training loss 0.014317645691335201 Validation loss 0.02126440778374672 Accuracy 0.7822265625\n",
      "Iteration 44380 Training loss 0.016917599365115166 Validation loss 0.021882284432649612 Accuracy 0.77587890625\n",
      "Iteration 44390 Training loss 0.013244765810668468 Validation loss 0.021024756133556366 Accuracy 0.78369140625\n",
      "Iteration 44400 Training loss 0.016462555155158043 Validation loss 0.0231822207570076 Accuracy 0.7626953125\n",
      "Iteration 44410 Training loss 0.016822488978505135 Validation loss 0.021265849471092224 Accuracy 0.78173828125\n",
      "Iteration 44420 Training loss 0.015895191580057144 Validation loss 0.02151433937251568 Accuracy 0.7783203125\n",
      "Iteration 44430 Training loss 0.01831558719277382 Validation loss 0.021032948046922684 Accuracy 0.78515625\n",
      "Iteration 44440 Training loss 0.014386149123311043 Validation loss 0.021099694073200226 Accuracy 0.78369140625\n",
      "Iteration 44450 Training loss 0.01467579510062933 Validation loss 0.020990895107388496 Accuracy 0.78564453125\n",
      "Iteration 44460 Training loss 0.016135092824697495 Validation loss 0.02117587812244892 Accuracy 0.783203125\n",
      "Iteration 44470 Training loss 0.016410989686846733 Validation loss 0.021227113902568817 Accuracy 0.783203125\n",
      "Iteration 44480 Training loss 0.015502825379371643 Validation loss 0.021047843620181084 Accuracy 0.78515625\n",
      "Iteration 44490 Training loss 0.017915887758135796 Validation loss 0.02107767015695572 Accuracy 0.78369140625\n",
      "Iteration 44500 Training loss 0.018159538507461548 Validation loss 0.021124524995684624 Accuracy 0.7841796875\n",
      "Iteration 44510 Training loss 0.015359611250460148 Validation loss 0.021145688369870186 Accuracy 0.783203125\n",
      "Iteration 44520 Training loss 0.01767292246222496 Validation loss 0.021315352991223335 Accuracy 0.78125\n",
      "Iteration 44530 Training loss 0.016313742846250534 Validation loss 0.02100817859172821 Accuracy 0.78515625\n",
      "Iteration 44540 Training loss 0.017908092588186264 Validation loss 0.021686270833015442 Accuracy 0.77880859375\n",
      "Iteration 44550 Training loss 0.017457209527492523 Validation loss 0.021696098148822784 Accuracy 0.77783203125\n",
      "Iteration 44560 Training loss 0.016507891938090324 Validation loss 0.02163633331656456 Accuracy 0.77734375\n",
      "Iteration 44570 Training loss 0.015884948894381523 Validation loss 0.021215854212641716 Accuracy 0.78271484375\n",
      "Iteration 44580 Training loss 0.014933614991605282 Validation loss 0.021373944357037544 Accuracy 0.78076171875\n",
      "Iteration 44590 Training loss 0.015236284583806992 Validation loss 0.021764000877738 Accuracy 0.77685546875\n",
      "Iteration 44600 Training loss 0.01475504506379366 Validation loss 0.021463431417942047 Accuracy 0.77978515625\n",
      "Iteration 44610 Training loss 0.016996869817376137 Validation loss 0.0211852565407753 Accuracy 0.78369140625\n",
      "Iteration 44620 Training loss 0.017791306599974632 Validation loss 0.021165505051612854 Accuracy 0.783203125\n",
      "Iteration 44630 Training loss 0.015826387330889702 Validation loss 0.021388353779911995 Accuracy 0.78125\n",
      "Iteration 44640 Training loss 0.01479186862707138 Validation loss 0.021351385861635208 Accuracy 0.78125\n",
      "Iteration 44650 Training loss 0.01455153152346611 Validation loss 0.021290253847837448 Accuracy 0.78125\n",
      "Iteration 44660 Training loss 0.01629035174846649 Validation loss 0.020952001214027405 Accuracy 0.78466796875\n",
      "Iteration 44670 Training loss 0.014769764617085457 Validation loss 0.02118138037621975 Accuracy 0.7822265625\n",
      "Iteration 44680 Training loss 0.01655065454542637 Validation loss 0.02096172608435154 Accuracy 0.78369140625\n",
      "Iteration 44690 Training loss 0.01623290218412876 Validation loss 0.02169010601937771 Accuracy 0.77734375\n",
      "Iteration 44700 Training loss 0.015728464350104332 Validation loss 0.021624481305480003 Accuracy 0.77783203125\n",
      "Iteration 44710 Training loss 0.017117420211434364 Validation loss 0.021479489281773567 Accuracy 0.77978515625\n",
      "Iteration 44720 Training loss 0.01789875701069832 Validation loss 0.021651117131114006 Accuracy 0.77783203125\n",
      "Iteration 44730 Training loss 0.016375383362174034 Validation loss 0.021226409822702408 Accuracy 0.78271484375\n",
      "Iteration 44740 Training loss 0.017390944063663483 Validation loss 0.02127692848443985 Accuracy 0.78271484375\n",
      "Iteration 44750 Training loss 0.016143662855029106 Validation loss 0.02121155522763729 Accuracy 0.7822265625\n",
      "Iteration 44760 Training loss 0.01714537851512432 Validation loss 0.021073531359434128 Accuracy 0.78369140625\n",
      "Iteration 44770 Training loss 0.017105014994740486 Validation loss 0.02109002321958542 Accuracy 0.783203125\n",
      "Iteration 44780 Training loss 0.015233607031404972 Validation loss 0.02131841704249382 Accuracy 0.78076171875\n",
      "Iteration 44790 Training loss 0.014493162743747234 Validation loss 0.02110958844423294 Accuracy 0.783203125\n",
      "Iteration 44800 Training loss 0.015996569767594337 Validation loss 0.021367674693465233 Accuracy 0.7802734375\n",
      "Iteration 44810 Training loss 0.017458941787481308 Validation loss 0.021003397181630135 Accuracy 0.78466796875\n",
      "Iteration 44820 Training loss 0.0157280582934618 Validation loss 0.021254196763038635 Accuracy 0.78173828125\n",
      "Iteration 44830 Training loss 0.017516158521175385 Validation loss 0.02124829590320587 Accuracy 0.7822265625\n",
      "Iteration 44840 Training loss 0.017489755526185036 Validation loss 0.021073998883366585 Accuracy 0.78271484375\n",
      "Iteration 44850 Training loss 0.018444903194904327 Validation loss 0.021234946325421333 Accuracy 0.78173828125\n",
      "Iteration 44860 Training loss 0.018354397267103195 Validation loss 0.02127775549888611 Accuracy 0.78125\n",
      "Iteration 44870 Training loss 0.016194550320506096 Validation loss 0.021364200860261917 Accuracy 0.7802734375\n",
      "Iteration 44880 Training loss 0.0162610225379467 Validation loss 0.021182045340538025 Accuracy 0.7822265625\n",
      "Iteration 44890 Training loss 0.016322679817676544 Validation loss 0.021431054919958115 Accuracy 0.77978515625\n",
      "Iteration 44900 Training loss 0.016372745856642723 Validation loss 0.021586287766695023 Accuracy 0.7783203125\n",
      "Iteration 44910 Training loss 0.015213790349662304 Validation loss 0.021322235465049744 Accuracy 0.7802734375\n",
      "Iteration 44920 Training loss 0.015952568501234055 Validation loss 0.021057313308119774 Accuracy 0.78369140625\n",
      "Iteration 44930 Training loss 0.017667138949036598 Validation loss 0.02135459892451763 Accuracy 0.78076171875\n",
      "Iteration 44940 Training loss 0.014885018579661846 Validation loss 0.021219076588749886 Accuracy 0.7822265625\n",
      "Iteration 44950 Training loss 0.014809496700763702 Validation loss 0.021053310483694077 Accuracy 0.78369140625\n",
      "Iteration 44960 Training loss 0.012315562926232815 Validation loss 0.021090727299451828 Accuracy 0.78369140625\n",
      "Iteration 44970 Training loss 0.016140101477503777 Validation loss 0.021317215636372566 Accuracy 0.77978515625\n",
      "Iteration 44980 Training loss 0.013930966146290302 Validation loss 0.020987533032894135 Accuracy 0.78466796875\n",
      "Iteration 44990 Training loss 0.015005098655819893 Validation loss 0.0214332714676857 Accuracy 0.779296875\n",
      "Iteration 45000 Training loss 0.01568506844341755 Validation loss 0.021545253694057465 Accuracy 0.779296875\n",
      "Iteration 45010 Training loss 0.016523847356438637 Validation loss 0.021259471774101257 Accuracy 0.78125\n",
      "Iteration 45020 Training loss 0.013722414150834084 Validation loss 0.02102893777191639 Accuracy 0.78369140625\n",
      "Iteration 45030 Training loss 0.013197540305554867 Validation loss 0.021011166274547577 Accuracy 0.7841796875\n",
      "Iteration 45040 Training loss 0.016030404716730118 Validation loss 0.021217811852693558 Accuracy 0.78173828125\n",
      "Iteration 45050 Training loss 0.018218273296952248 Validation loss 0.02109304443001747 Accuracy 0.78466796875\n",
      "Iteration 45060 Training loss 0.013658217154443264 Validation loss 0.022031152620911598 Accuracy 0.7744140625\n",
      "Iteration 45070 Training loss 0.01609826646745205 Validation loss 0.02142062596976757 Accuracy 0.779296875\n",
      "Iteration 45080 Training loss 0.016728784888982773 Validation loss 0.02235163003206253 Accuracy 0.771484375\n",
      "Iteration 45090 Training loss 0.016225431114435196 Validation loss 0.021219953894615173 Accuracy 0.78125\n",
      "Iteration 45100 Training loss 0.013306645676493645 Validation loss 0.021424569189548492 Accuracy 0.77978515625\n",
      "Iteration 45110 Training loss 0.013796309940516949 Validation loss 0.021082527935504913 Accuracy 0.78369140625\n",
      "Iteration 45120 Training loss 0.015393177978694439 Validation loss 0.020962005481123924 Accuracy 0.78515625\n",
      "Iteration 45130 Training loss 0.015205595642328262 Validation loss 0.021068625152111053 Accuracy 0.783203125\n",
      "Iteration 45140 Training loss 0.017980776727199554 Validation loss 0.021149927750229836 Accuracy 0.78369140625\n",
      "Iteration 45150 Training loss 0.015112653374671936 Validation loss 0.021097436547279358 Accuracy 0.783203125\n",
      "Iteration 45160 Training loss 0.018537944182753563 Validation loss 0.021506596356630325 Accuracy 0.77880859375\n",
      "Iteration 45170 Training loss 0.01669696718454361 Validation loss 0.021142080426216125 Accuracy 0.78271484375\n",
      "Iteration 45180 Training loss 0.016194095835089684 Validation loss 0.021329181268811226 Accuracy 0.78076171875\n",
      "Iteration 45190 Training loss 0.015087702311575413 Validation loss 0.02128339372575283 Accuracy 0.78125\n",
      "Iteration 45200 Training loss 0.01608087494969368 Validation loss 0.021509887650609016 Accuracy 0.77880859375\n",
      "Iteration 45210 Training loss 0.013229248113930225 Validation loss 0.021216168999671936 Accuracy 0.78173828125\n",
      "Iteration 45220 Training loss 0.014548596926033497 Validation loss 0.02113552764058113 Accuracy 0.7822265625\n",
      "Iteration 45230 Training loss 0.015519393607974052 Validation loss 0.02099938504397869 Accuracy 0.783203125\n",
      "Iteration 45240 Training loss 0.01761348359286785 Validation loss 0.02184501476585865 Accuracy 0.7744140625\n",
      "Iteration 45250 Training loss 0.01639125496149063 Validation loss 0.021074604243040085 Accuracy 0.783203125\n",
      "Iteration 45260 Training loss 0.01755373366177082 Validation loss 0.021599818021059036 Accuracy 0.779296875\n",
      "Iteration 45270 Training loss 0.016044296324253082 Validation loss 0.021425258368253708 Accuracy 0.7802734375\n",
      "Iteration 45280 Training loss 0.013715021312236786 Validation loss 0.021115342155098915 Accuracy 0.78271484375\n",
      "Iteration 45290 Training loss 0.014542524702847004 Validation loss 0.021058300510048866 Accuracy 0.78369140625\n",
      "Iteration 45300 Training loss 0.016826672479510307 Validation loss 0.02127178944647312 Accuracy 0.7802734375\n",
      "Iteration 45310 Training loss 0.014482477679848671 Validation loss 0.020791733637452126 Accuracy 0.7861328125\n",
      "Iteration 45320 Training loss 0.01803434267640114 Validation loss 0.021098852157592773 Accuracy 0.783203125\n",
      "Iteration 45330 Training loss 0.014000482857227325 Validation loss 0.021064596250653267 Accuracy 0.78369140625\n",
      "Iteration 45340 Training loss 0.018199125304818153 Validation loss 0.02218415029346943 Accuracy 0.77197265625\n",
      "Iteration 45350 Training loss 0.01683788001537323 Validation loss 0.021126141771674156 Accuracy 0.783203125\n",
      "Iteration 45360 Training loss 0.017510732635855675 Validation loss 0.021192563697695732 Accuracy 0.7822265625\n",
      "Iteration 45370 Training loss 0.01547474879771471 Validation loss 0.021249065175652504 Accuracy 0.7822265625\n",
      "Iteration 45380 Training loss 0.016858069226145744 Validation loss 0.021550454199314117 Accuracy 0.7783203125\n",
      "Iteration 45390 Training loss 0.01718616671860218 Validation loss 0.020896146073937416 Accuracy 0.78515625\n",
      "Iteration 45400 Training loss 0.02007254585623741 Validation loss 0.02098819799721241 Accuracy 0.7841796875\n",
      "Iteration 45410 Training loss 0.016395041719079018 Validation loss 0.021017782390117645 Accuracy 0.78369140625\n",
      "Iteration 45420 Training loss 0.015490674413740635 Validation loss 0.021179847419261932 Accuracy 0.78271484375\n",
      "Iteration 45430 Training loss 0.014264601282775402 Validation loss 0.02101576142013073 Accuracy 0.7841796875\n",
      "Iteration 45440 Training loss 0.01479335781186819 Validation loss 0.02105107344686985 Accuracy 0.7841796875\n",
      "Iteration 45450 Training loss 0.014320150949060917 Validation loss 0.021117277443408966 Accuracy 0.78271484375\n",
      "Iteration 45460 Training loss 0.017306767404079437 Validation loss 0.02089388109743595 Accuracy 0.78662109375\n",
      "Iteration 45470 Training loss 0.015914054587483406 Validation loss 0.02145019918680191 Accuracy 0.77978515625\n",
      "Iteration 45480 Training loss 0.01578255370259285 Validation loss 0.02109212800860405 Accuracy 0.783203125\n",
      "Iteration 45490 Training loss 0.016406171023845673 Validation loss 0.020896660163998604 Accuracy 0.78515625\n",
      "Iteration 45500 Training loss 0.015581751242280006 Validation loss 0.02164519391953945 Accuracy 0.77783203125\n",
      "Iteration 45510 Training loss 0.01472040917724371 Validation loss 0.021434562280774117 Accuracy 0.779296875\n",
      "Iteration 45520 Training loss 0.017736393958330154 Validation loss 0.021352367475628853 Accuracy 0.78076171875\n",
      "Iteration 45530 Training loss 0.01773514226078987 Validation loss 0.021311655640602112 Accuracy 0.78076171875\n",
      "Iteration 45540 Training loss 0.016974501311779022 Validation loss 0.021441180258989334 Accuracy 0.77978515625\n",
      "Iteration 45550 Training loss 0.01726163923740387 Validation loss 0.02121240645647049 Accuracy 0.78173828125\n",
      "Iteration 45560 Training loss 0.016152989119291306 Validation loss 0.020853588357567787 Accuracy 0.78564453125\n",
      "Iteration 45570 Training loss 0.016729634255170822 Validation loss 0.021179281175136566 Accuracy 0.78271484375\n",
      "Iteration 45580 Training loss 0.014843324199318886 Validation loss 0.021551523357629776 Accuracy 0.77880859375\n",
      "Iteration 45590 Training loss 0.018033625558018684 Validation loss 0.02100187912583351 Accuracy 0.78369140625\n",
      "Iteration 45600 Training loss 0.014996438287198544 Validation loss 0.021067896857857704 Accuracy 0.78369140625\n",
      "Iteration 45610 Training loss 0.01776733249425888 Validation loss 0.02090621553361416 Accuracy 0.78515625\n",
      "Iteration 45620 Training loss 0.012538053095340729 Validation loss 0.020943477749824524 Accuracy 0.78466796875\n",
      "Iteration 45630 Training loss 0.015607945621013641 Validation loss 0.020986782386898994 Accuracy 0.78466796875\n",
      "Iteration 45640 Training loss 0.015691449865698814 Validation loss 0.020969577133655548 Accuracy 0.78466796875\n",
      "Iteration 45650 Training loss 0.016978267580270767 Validation loss 0.02149178460240364 Accuracy 0.77880859375\n",
      "Iteration 45660 Training loss 0.015645373612642288 Validation loss 0.021354880183935165 Accuracy 0.78076171875\n",
      "Iteration 45670 Training loss 0.014135713689029217 Validation loss 0.02132582478225231 Accuracy 0.7802734375\n",
      "Iteration 45680 Training loss 0.017162993550300598 Validation loss 0.022883128374814987 Accuracy 0.76513671875\n",
      "Iteration 45690 Training loss 0.0160849466919899 Validation loss 0.021481122821569443 Accuracy 0.77880859375\n",
      "Iteration 45700 Training loss 0.017879024147987366 Validation loss 0.021832337602972984 Accuracy 0.77490234375\n",
      "Iteration 45710 Training loss 0.017310256138443947 Validation loss 0.021070875227451324 Accuracy 0.78369140625\n",
      "Iteration 45720 Training loss 0.01780596561729908 Validation loss 0.021233046427369118 Accuracy 0.78271484375\n",
      "Iteration 45730 Training loss 0.015215779654681683 Validation loss 0.021316280588507652 Accuracy 0.7802734375\n",
      "Iteration 45740 Training loss 0.01574188657104969 Validation loss 0.02087242156267166 Accuracy 0.78564453125\n",
      "Iteration 45750 Training loss 0.0156719833612442 Validation loss 0.021328739821910858 Accuracy 0.77978515625\n",
      "Iteration 45760 Training loss 0.016783757135272026 Validation loss 0.021227167919278145 Accuracy 0.7822265625\n",
      "Iteration 45770 Training loss 0.015257814899086952 Validation loss 0.020927075296640396 Accuracy 0.78564453125\n",
      "Iteration 45780 Training loss 0.015173250809311867 Validation loss 0.021396305412054062 Accuracy 0.78076171875\n",
      "Iteration 45790 Training loss 0.01455127913504839 Validation loss 0.02161354385316372 Accuracy 0.77783203125\n",
      "Iteration 45800 Training loss 0.016629105433821678 Validation loss 0.020973797887563705 Accuracy 0.78515625\n",
      "Iteration 45810 Training loss 0.01726161688566208 Validation loss 0.021056722849607468 Accuracy 0.78369140625\n",
      "Iteration 45820 Training loss 0.012231030501425266 Validation loss 0.020942186936736107 Accuracy 0.7841796875\n",
      "Iteration 45830 Training loss 0.014651256613433361 Validation loss 0.021090980619192123 Accuracy 0.7841796875\n",
      "Iteration 45840 Training loss 0.017134688794612885 Validation loss 0.021222049370408058 Accuracy 0.7822265625\n",
      "Iteration 45850 Training loss 0.01508191879838705 Validation loss 0.02095206454396248 Accuracy 0.78369140625\n",
      "Iteration 45860 Training loss 0.016750842332839966 Validation loss 0.021090250462293625 Accuracy 0.78271484375\n",
      "Iteration 45870 Training loss 0.01661025919020176 Validation loss 0.021006815135478973 Accuracy 0.78466796875\n",
      "Iteration 45880 Training loss 0.014779528602957726 Validation loss 0.021339252591133118 Accuracy 0.78076171875\n",
      "Iteration 45890 Training loss 0.013876909390091896 Validation loss 0.02108592540025711 Accuracy 0.78369140625\n",
      "Iteration 45900 Training loss 0.015771374106407166 Validation loss 0.022690868005156517 Accuracy 0.767578125\n",
      "Iteration 45910 Training loss 0.014450912363827229 Validation loss 0.021135924383997917 Accuracy 0.783203125\n",
      "Iteration 45920 Training loss 0.017687778919935226 Validation loss 0.021086985245347023 Accuracy 0.78369140625\n",
      "Iteration 45930 Training loss 0.018420664593577385 Validation loss 0.02131137065589428 Accuracy 0.78173828125\n",
      "Iteration 45940 Training loss 0.018447794020175934 Validation loss 0.021478353068232536 Accuracy 0.779296875\n",
      "Iteration 45950 Training loss 0.017775272950530052 Validation loss 0.02134011685848236 Accuracy 0.78125\n",
      "Iteration 45960 Training loss 0.016262980177998543 Validation loss 0.021245956420898438 Accuracy 0.78076171875\n",
      "Iteration 45970 Training loss 0.013851839117705822 Validation loss 0.021362049505114555 Accuracy 0.78076171875\n",
      "Iteration 45980 Training loss 0.01851005293428898 Validation loss 0.021539105102419853 Accuracy 0.7783203125\n",
      "Iteration 45990 Training loss 0.016190283000469208 Validation loss 0.02114979922771454 Accuracy 0.78271484375\n",
      "Iteration 46000 Training loss 0.015847409144043922 Validation loss 0.020986074581742287 Accuracy 0.7841796875\n",
      "Iteration 46010 Training loss 0.016302067786455154 Validation loss 0.02125953510403633 Accuracy 0.78173828125\n",
      "Iteration 46020 Training loss 0.016451677307486534 Validation loss 0.021676495671272278 Accuracy 0.77880859375\n",
      "Iteration 46030 Training loss 0.015382865443825722 Validation loss 0.02112734131515026 Accuracy 0.783203125\n",
      "Iteration 46040 Training loss 0.01565626822412014 Validation loss 0.020866606384515762 Accuracy 0.7861328125\n",
      "Iteration 46050 Training loss 0.015498935244977474 Validation loss 0.02133120596408844 Accuracy 0.7802734375\n",
      "Iteration 46060 Training loss 0.014425713568925858 Validation loss 0.02123894914984703 Accuracy 0.78173828125\n",
      "Iteration 46070 Training loss 0.017606794834136963 Validation loss 0.021671395748853683 Accuracy 0.7763671875\n",
      "Iteration 46080 Training loss 0.015144052915275097 Validation loss 0.02101382240653038 Accuracy 0.7841796875\n",
      "Iteration 46090 Training loss 0.015139161609113216 Validation loss 0.020884374156594276 Accuracy 0.78466796875\n",
      "Iteration 46100 Training loss 0.016443658620119095 Validation loss 0.02127188630402088 Accuracy 0.78076171875\n",
      "Iteration 46110 Training loss 0.016636211425065994 Validation loss 0.021230898797512054 Accuracy 0.78173828125\n",
      "Iteration 46120 Training loss 0.01708700880408287 Validation loss 0.02111748233437538 Accuracy 0.783203125\n",
      "Iteration 46130 Training loss 0.017204836010932922 Validation loss 0.021655680611729622 Accuracy 0.77783203125\n",
      "Iteration 46140 Training loss 0.014867947436869144 Validation loss 0.02139335870742798 Accuracy 0.77978515625\n",
      "Iteration 46150 Training loss 0.015441668219864368 Validation loss 0.021160781383514404 Accuracy 0.783203125\n",
      "Iteration 46160 Training loss 0.017833754420280457 Validation loss 0.021152516826987267 Accuracy 0.783203125\n",
      "Iteration 46170 Training loss 0.016950204968452454 Validation loss 0.02114279195666313 Accuracy 0.78173828125\n",
      "Iteration 46180 Training loss 0.019398091360926628 Validation loss 0.022046875208616257 Accuracy 0.77294921875\n",
      "Iteration 46190 Training loss 0.013409562408924103 Validation loss 0.021015752106904984 Accuracy 0.7841796875\n",
      "Iteration 46200 Training loss 0.014197101816534996 Validation loss 0.020963070914149284 Accuracy 0.78369140625\n",
      "Iteration 46210 Training loss 0.015883034095168114 Validation loss 0.02114293724298477 Accuracy 0.7822265625\n",
      "Iteration 46220 Training loss 0.015108422376215458 Validation loss 0.02110130339860916 Accuracy 0.783203125\n",
      "Iteration 46230 Training loss 0.014584409072995186 Validation loss 0.021368978545069695 Accuracy 0.78125\n",
      "Iteration 46240 Training loss 0.015114082023501396 Validation loss 0.021134305745363235 Accuracy 0.783203125\n",
      "Iteration 46250 Training loss 0.015340801328420639 Validation loss 0.02113187126815319 Accuracy 0.7822265625\n",
      "Iteration 46260 Training loss 0.01333034411072731 Validation loss 0.021075505763292313 Accuracy 0.783203125\n",
      "Iteration 46270 Training loss 0.015309776179492474 Validation loss 0.02088431641459465 Accuracy 0.7841796875\n",
      "Iteration 46280 Training loss 0.01572762429714203 Validation loss 0.020848315209150314 Accuracy 0.78564453125\n",
      "Iteration 46290 Training loss 0.014425837434828281 Validation loss 0.02130376361310482 Accuracy 0.78125\n",
      "Iteration 46300 Training loss 0.017602134495973587 Validation loss 0.02096809446811676 Accuracy 0.78515625\n",
      "Iteration 46310 Training loss 0.014643663540482521 Validation loss 0.020974721759557724 Accuracy 0.7841796875\n",
      "Iteration 46320 Training loss 0.01574755273759365 Validation loss 0.021156180649995804 Accuracy 0.78271484375\n",
      "Iteration 46330 Training loss 0.015739230439066887 Validation loss 0.020992739126086235 Accuracy 0.78369140625\n",
      "Iteration 46340 Training loss 0.015445996075868607 Validation loss 0.022440185770392418 Accuracy 0.77001953125\n",
      "Iteration 46350 Training loss 0.01544369850307703 Validation loss 0.02095259726047516 Accuracy 0.7841796875\n",
      "Iteration 46360 Training loss 0.016343234106898308 Validation loss 0.021177874878048897 Accuracy 0.78173828125\n",
      "Iteration 46370 Training loss 0.01580939255654812 Validation loss 0.020913945510983467 Accuracy 0.78369140625\n",
      "Iteration 46380 Training loss 0.014496710151433945 Validation loss 0.020954828709363937 Accuracy 0.7841796875\n",
      "Iteration 46390 Training loss 0.015418464317917824 Validation loss 0.021387513726949692 Accuracy 0.78076171875\n",
      "Iteration 46400 Training loss 0.017342641949653625 Validation loss 0.021179186180233955 Accuracy 0.78271484375\n",
      "Iteration 46410 Training loss 0.01321558840572834 Validation loss 0.021045878529548645 Accuracy 0.7841796875\n",
      "Iteration 46420 Training loss 0.01667468249797821 Validation loss 0.021641340106725693 Accuracy 0.77783203125\n",
      "Iteration 46430 Training loss 0.016547823324799538 Validation loss 0.021636877208948135 Accuracy 0.7783203125\n",
      "Iteration 46440 Training loss 0.01564161106944084 Validation loss 0.02119259722530842 Accuracy 0.7822265625\n",
      "Iteration 46450 Training loss 0.013962586410343647 Validation loss 0.021522650495171547 Accuracy 0.77880859375\n",
      "Iteration 46460 Training loss 0.014706281013786793 Validation loss 0.021296614781022072 Accuracy 0.78173828125\n",
      "Iteration 46470 Training loss 0.020108487457036972 Validation loss 0.02166007272899151 Accuracy 0.77783203125\n",
      "Iteration 46480 Training loss 0.0122480234131217 Validation loss 0.020942872390151024 Accuracy 0.78564453125\n",
      "Iteration 46490 Training loss 0.013829872012138367 Validation loss 0.021240830421447754 Accuracy 0.78173828125\n",
      "Iteration 46500 Training loss 0.015362309291958809 Validation loss 0.021209705621004105 Accuracy 0.78173828125\n",
      "Iteration 46510 Training loss 0.015221193432807922 Validation loss 0.02141956053674221 Accuracy 0.77978515625\n",
      "Iteration 46520 Training loss 0.015570519492030144 Validation loss 0.02154046855866909 Accuracy 0.77783203125\n",
      "Iteration 46530 Training loss 0.016499541699886322 Validation loss 0.021046089008450508 Accuracy 0.78369140625\n",
      "Iteration 46540 Training loss 0.013838119804859161 Validation loss 0.02100566029548645 Accuracy 0.7841796875\n",
      "Iteration 46550 Training loss 0.01736517623066902 Validation loss 0.021186117082834244 Accuracy 0.78125\n",
      "Iteration 46560 Training loss 0.014727864414453506 Validation loss 0.02111927978694439 Accuracy 0.783203125\n",
      "Iteration 46570 Training loss 0.015401652082800865 Validation loss 0.021441418677568436 Accuracy 0.7802734375\n",
      "Iteration 46580 Training loss 0.012826657854020596 Validation loss 0.021052414551377296 Accuracy 0.78369140625\n",
      "Iteration 46590 Training loss 0.014299072325229645 Validation loss 0.02108820714056492 Accuracy 0.783203125\n",
      "Iteration 46600 Training loss 0.01433179248124361 Validation loss 0.02097983844578266 Accuracy 0.78466796875\n",
      "Iteration 46610 Training loss 0.016938375309109688 Validation loss 0.02161017246544361 Accuracy 0.7783203125\n",
      "Iteration 46620 Training loss 0.015690023079514503 Validation loss 0.020959259942173958 Accuracy 0.7861328125\n",
      "Iteration 46630 Training loss 0.01678740419447422 Validation loss 0.021226005628705025 Accuracy 0.78173828125\n",
      "Iteration 46640 Training loss 0.015532700344920158 Validation loss 0.02116275019943714 Accuracy 0.78271484375\n",
      "Iteration 46650 Training loss 0.01732316054403782 Validation loss 0.02152913436293602 Accuracy 0.77880859375\n",
      "Iteration 46660 Training loss 0.016422288492321968 Validation loss 0.021107392385601997 Accuracy 0.78271484375\n",
      "Iteration 46670 Training loss 0.015901682898402214 Validation loss 0.02117420919239521 Accuracy 0.78271484375\n",
      "Iteration 46680 Training loss 0.01671634428203106 Validation loss 0.020893964916467667 Accuracy 0.7861328125\n",
      "Iteration 46690 Training loss 0.015382965095341206 Validation loss 0.021075375378131866 Accuracy 0.78369140625\n",
      "Iteration 46700 Training loss 0.014497380703687668 Validation loss 0.02111397124826908 Accuracy 0.78271484375\n",
      "Iteration 46710 Training loss 0.01959977112710476 Validation loss 0.02134852483868599 Accuracy 0.7802734375\n",
      "Iteration 46720 Training loss 0.0167813953012228 Validation loss 0.021412324160337448 Accuracy 0.77978515625\n",
      "Iteration 46730 Training loss 0.016602806746959686 Validation loss 0.021243510767817497 Accuracy 0.78173828125\n",
      "Iteration 46740 Training loss 0.017598722130060196 Validation loss 0.02174398861825466 Accuracy 0.77734375\n",
      "Iteration 46750 Training loss 0.016716258600354195 Validation loss 0.021441444754600525 Accuracy 0.77880859375\n",
      "Iteration 46760 Training loss 0.016948962584137917 Validation loss 0.02182692103087902 Accuracy 0.775390625\n",
      "Iteration 46770 Training loss 0.01806425116956234 Validation loss 0.022691693156957626 Accuracy 0.767578125\n",
      "Iteration 46780 Training loss 0.015278114005923271 Validation loss 0.02127830497920513 Accuracy 0.78125\n",
      "Iteration 46790 Training loss 0.015015510842204094 Validation loss 0.02157445251941681 Accuracy 0.7783203125\n",
      "Iteration 46800 Training loss 0.016903487965464592 Validation loss 0.021001258864998817 Accuracy 0.7841796875\n",
      "Iteration 46810 Training loss 0.013453671708703041 Validation loss 0.021089423447847366 Accuracy 0.78271484375\n",
      "Iteration 46820 Training loss 0.013326870277523994 Validation loss 0.021335426717996597 Accuracy 0.7802734375\n",
      "Iteration 46830 Training loss 0.014394808560609818 Validation loss 0.021115746349096298 Accuracy 0.783203125\n",
      "Iteration 46840 Training loss 0.015439155511558056 Validation loss 0.020953582599759102 Accuracy 0.7841796875\n",
      "Iteration 46850 Training loss 0.017247792333364487 Validation loss 0.020981550216674805 Accuracy 0.78466796875\n",
      "Iteration 46860 Training loss 0.015499616041779518 Validation loss 0.021130209788680077 Accuracy 0.783203125\n",
      "Iteration 46870 Training loss 0.01731674186885357 Validation loss 0.022510407492518425 Accuracy 0.76953125\n",
      "Iteration 46880 Training loss 0.01783166266977787 Validation loss 0.022867470979690552 Accuracy 0.76611328125\n",
      "Iteration 46890 Training loss 0.01701766438782215 Validation loss 0.021922148764133453 Accuracy 0.7744140625\n",
      "Iteration 46900 Training loss 0.015593457035720348 Validation loss 0.020895810797810555 Accuracy 0.7861328125\n",
      "Iteration 46910 Training loss 0.0156415905803442 Validation loss 0.021085921674966812 Accuracy 0.7841796875\n",
      "Iteration 46920 Training loss 0.01393253542482853 Validation loss 0.021343275904655457 Accuracy 0.78125\n",
      "Iteration 46930 Training loss 0.015064338222146034 Validation loss 0.021138640120625496 Accuracy 0.78369140625\n",
      "Iteration 46940 Training loss 0.015409344807267189 Validation loss 0.021022699773311615 Accuracy 0.78515625\n",
      "Iteration 46950 Training loss 0.015187282115221024 Validation loss 0.021050497889518738 Accuracy 0.7841796875\n",
      "Iteration 46960 Training loss 0.014164893887937069 Validation loss 0.021340740844607353 Accuracy 0.78076171875\n",
      "Iteration 46970 Training loss 0.017516957595944405 Validation loss 0.021175814792513847 Accuracy 0.78271484375\n",
      "Iteration 46980 Training loss 0.018617266789078712 Validation loss 0.02095002308487892 Accuracy 0.78369140625\n",
      "Iteration 46990 Training loss 0.016576386988162994 Validation loss 0.02112807333469391 Accuracy 0.783203125\n",
      "Iteration 47000 Training loss 0.015788841992616653 Validation loss 0.02136799320578575 Accuracy 0.78173828125\n",
      "Iteration 47010 Training loss 0.014677591621875763 Validation loss 0.021401645615696907 Accuracy 0.78076171875\n",
      "Iteration 47020 Training loss 0.015645813196897507 Validation loss 0.021238673478364944 Accuracy 0.7822265625\n",
      "Iteration 47030 Training loss 0.01610082946717739 Validation loss 0.020870698615908623 Accuracy 0.78466796875\n",
      "Iteration 47040 Training loss 0.014105519279837608 Validation loss 0.021241649985313416 Accuracy 0.7822265625\n",
      "Iteration 47050 Training loss 0.015928754583001137 Validation loss 0.02101057581603527 Accuracy 0.78466796875\n",
      "Iteration 47060 Training loss 0.01743577979505062 Validation loss 0.02120826207101345 Accuracy 0.7822265625\n",
      "Iteration 47070 Training loss 0.01577875018119812 Validation loss 0.02128690853714943 Accuracy 0.78173828125\n",
      "Iteration 47080 Training loss 0.014897312968969345 Validation loss 0.021256566047668457 Accuracy 0.78173828125\n",
      "Iteration 47090 Training loss 0.019624896347522736 Validation loss 0.021764973178505898 Accuracy 0.77685546875\n",
      "Iteration 47100 Training loss 0.013527763076126575 Validation loss 0.02099098637700081 Accuracy 0.7841796875\n",
      "Iteration 47110 Training loss 0.018814176321029663 Validation loss 0.021879740059375763 Accuracy 0.77490234375\n",
      "Iteration 47120 Training loss 0.014297259040176868 Validation loss 0.021531907841563225 Accuracy 0.77734375\n",
      "Iteration 47130 Training loss 0.017222918570041656 Validation loss 0.021019868552684784 Accuracy 0.7841796875\n",
      "Iteration 47140 Training loss 0.015398068353533745 Validation loss 0.021056517958641052 Accuracy 0.78369140625\n",
      "Iteration 47150 Training loss 0.01864137500524521 Validation loss 0.021476278081536293 Accuracy 0.779296875\n",
      "Iteration 47160 Training loss 0.01598481833934784 Validation loss 0.02111850120127201 Accuracy 0.78369140625\n",
      "Iteration 47170 Training loss 0.017346778884530067 Validation loss 0.021093299612402916 Accuracy 0.78369140625\n",
      "Iteration 47180 Training loss 0.01622956432402134 Validation loss 0.020949747413396835 Accuracy 0.78466796875\n",
      "Iteration 47190 Training loss 0.0153310876339674 Validation loss 0.02111661061644554 Accuracy 0.7822265625\n",
      "Iteration 47200 Training loss 0.017778074368834496 Validation loss 0.02100207470357418 Accuracy 0.78369140625\n",
      "Iteration 47210 Training loss 0.014778295531868935 Validation loss 0.021280106157064438 Accuracy 0.78125\n",
      "Iteration 47220 Training loss 0.01809055730700493 Validation loss 0.021382784470915794 Accuracy 0.77978515625\n",
      "Iteration 47230 Training loss 0.015453478321433067 Validation loss 0.021695883944630623 Accuracy 0.7763671875\n",
      "Iteration 47240 Training loss 0.01635567657649517 Validation loss 0.022514134645462036 Accuracy 0.7685546875\n",
      "Iteration 47250 Training loss 0.01603587530553341 Validation loss 0.02113722451031208 Accuracy 0.78271484375\n",
      "Iteration 47260 Training loss 0.012815682217478752 Validation loss 0.021136805415153503 Accuracy 0.78173828125\n",
      "Iteration 47270 Training loss 0.015506991185247898 Validation loss 0.021498847752809525 Accuracy 0.77880859375\n",
      "Iteration 47280 Training loss 0.014883333817124367 Validation loss 0.02112353965640068 Accuracy 0.783203125\n",
      "Iteration 47290 Training loss 0.01577373780310154 Validation loss 0.021351849660277367 Accuracy 0.77978515625\n",
      "Iteration 47300 Training loss 0.013060948811471462 Validation loss 0.02123233862221241 Accuracy 0.78173828125\n",
      "Iteration 47310 Training loss 0.013091039843857288 Validation loss 0.02113388106226921 Accuracy 0.78271484375\n",
      "Iteration 47320 Training loss 0.01592405140399933 Validation loss 0.02116171270608902 Accuracy 0.783203125\n",
      "Iteration 47330 Training loss 0.01564852148294449 Validation loss 0.021163728088140488 Accuracy 0.7822265625\n",
      "Iteration 47340 Training loss 0.014772459864616394 Validation loss 0.021041469648480415 Accuracy 0.78271484375\n",
      "Iteration 47350 Training loss 0.018647123128175735 Validation loss 0.021253272891044617 Accuracy 0.78173828125\n",
      "Iteration 47360 Training loss 0.017362093552947044 Validation loss 0.0211153756827116 Accuracy 0.78271484375\n",
      "Iteration 47370 Training loss 0.013766003772616386 Validation loss 0.02166862227022648 Accuracy 0.77880859375\n",
      "Iteration 47380 Training loss 0.01592600718140602 Validation loss 0.02128507010638714 Accuracy 0.78173828125\n",
      "Iteration 47390 Training loss 0.015001283958554268 Validation loss 0.02141732908785343 Accuracy 0.78076171875\n",
      "Iteration 47400 Training loss 0.015859119594097137 Validation loss 0.021342143416404724 Accuracy 0.78076171875\n",
      "Iteration 47410 Training loss 0.018672723323106766 Validation loss 0.0212246160954237 Accuracy 0.78173828125\n",
      "Iteration 47420 Training loss 0.014297551475465298 Validation loss 0.0213763527572155 Accuracy 0.779296875\n",
      "Iteration 47430 Training loss 0.015867162495851517 Validation loss 0.02156279794871807 Accuracy 0.7783203125\n",
      "Iteration 47440 Training loss 0.01840599626302719 Validation loss 0.021177317947149277 Accuracy 0.78271484375\n",
      "Iteration 47450 Training loss 0.014423667453229427 Validation loss 0.021280385553836823 Accuracy 0.78125\n",
      "Iteration 47460 Training loss 0.014127050526440144 Validation loss 0.020832939073443413 Accuracy 0.78564453125\n",
      "Iteration 47470 Training loss 0.01569528877735138 Validation loss 0.021115483716130257 Accuracy 0.78369140625\n",
      "Iteration 47480 Training loss 0.018730327486991882 Validation loss 0.022359328344464302 Accuracy 0.771484375\n",
      "Iteration 47490 Training loss 0.014528455212712288 Validation loss 0.02094958908855915 Accuracy 0.7841796875\n",
      "Iteration 47500 Training loss 0.01750359497964382 Validation loss 0.021039331331849098 Accuracy 0.783203125\n",
      "Iteration 47510 Training loss 0.016336491331458092 Validation loss 0.021240081638097763 Accuracy 0.7822265625\n",
      "Iteration 47520 Training loss 0.014767030254006386 Validation loss 0.020965537056326866 Accuracy 0.78466796875\n",
      "Iteration 47530 Training loss 0.016054874286055565 Validation loss 0.021346479654312134 Accuracy 0.7802734375\n",
      "Iteration 47540 Training loss 0.015325011685490608 Validation loss 0.021133676171302795 Accuracy 0.783203125\n",
      "Iteration 47550 Training loss 0.01594551093876362 Validation loss 0.021015658974647522 Accuracy 0.783203125\n",
      "Iteration 47560 Training loss 0.018182525411248207 Validation loss 0.02187119610607624 Accuracy 0.77490234375\n",
      "Iteration 47570 Training loss 0.017390014603734016 Validation loss 0.021291688084602356 Accuracy 0.7802734375\n",
      "Iteration 47580 Training loss 0.014672464691102505 Validation loss 0.021049197763204575 Accuracy 0.78271484375\n",
      "Iteration 47590 Training loss 0.01469678245484829 Validation loss 0.02118164487183094 Accuracy 0.78369140625\n",
      "Iteration 47600 Training loss 0.015903746709227562 Validation loss 0.021028269082307816 Accuracy 0.78466796875\n",
      "Iteration 47610 Training loss 0.01414448767900467 Validation loss 0.020846595987677574 Accuracy 0.78564453125\n",
      "Iteration 47620 Training loss 0.015813950449228287 Validation loss 0.02129201591014862 Accuracy 0.78173828125\n",
      "Iteration 47630 Training loss 0.0159061960875988 Validation loss 0.02109566330909729 Accuracy 0.78271484375\n",
      "Iteration 47640 Training loss 0.014103970490396023 Validation loss 0.021481383591890335 Accuracy 0.779296875\n",
      "Iteration 47650 Training loss 0.015230249613523483 Validation loss 0.021016810089349747 Accuracy 0.78369140625\n",
      "Iteration 47660 Training loss 0.014829530380666256 Validation loss 0.02141745761036873 Accuracy 0.779296875\n",
      "Iteration 47670 Training loss 0.01614704169332981 Validation loss 0.02120879851281643 Accuracy 0.78271484375\n",
      "Iteration 47680 Training loss 0.018553761765360832 Validation loss 0.02114059589803219 Accuracy 0.783203125\n",
      "Iteration 47690 Training loss 0.016445284709334373 Validation loss 0.02144669182598591 Accuracy 0.7802734375\n",
      "Iteration 47700 Training loss 0.016915539279580116 Validation loss 0.02143591083586216 Accuracy 0.77978515625\n",
      "Iteration 47710 Training loss 0.014755242504179478 Validation loss 0.02112211287021637 Accuracy 0.78271484375\n",
      "Iteration 47720 Training loss 0.01575884409248829 Validation loss 0.02094365656375885 Accuracy 0.78466796875\n",
      "Iteration 47730 Training loss 0.016772251576185226 Validation loss 0.021078016608953476 Accuracy 0.783203125\n",
      "Iteration 47740 Training loss 0.01687173917889595 Validation loss 0.02110113762319088 Accuracy 0.78369140625\n",
      "Iteration 47750 Training loss 0.016037609428167343 Validation loss 0.021111086010932922 Accuracy 0.783203125\n",
      "Iteration 47760 Training loss 0.017391402274370193 Validation loss 0.021713130176067352 Accuracy 0.77685546875\n",
      "Iteration 47770 Training loss 0.01469096727669239 Validation loss 0.02097932994365692 Accuracy 0.78466796875\n",
      "Iteration 47780 Training loss 0.013645695522427559 Validation loss 0.02117854356765747 Accuracy 0.78271484375\n",
      "Iteration 47790 Training loss 0.01522514596581459 Validation loss 0.021295985206961632 Accuracy 0.78076171875\n",
      "Iteration 47800 Training loss 0.013721576891839504 Validation loss 0.0210697203874588 Accuracy 0.78369140625\n",
      "Iteration 47810 Training loss 0.01603158563375473 Validation loss 0.02150430530309677 Accuracy 0.77880859375\n",
      "Iteration 47820 Training loss 0.01239396445453167 Validation loss 0.0210478026419878 Accuracy 0.7841796875\n",
      "Iteration 47830 Training loss 0.014445065520703793 Validation loss 0.021023212000727654 Accuracy 0.783203125\n",
      "Iteration 47840 Training loss 0.012020637281239033 Validation loss 0.021169310435652733 Accuracy 0.78173828125\n",
      "Iteration 47850 Training loss 0.016190137714147568 Validation loss 0.02120492048561573 Accuracy 0.78271484375\n",
      "Iteration 47860 Training loss 0.012856905348598957 Validation loss 0.021404486149549484 Accuracy 0.78125\n",
      "Iteration 47870 Training loss 0.01883002370595932 Validation loss 0.021482501178979874 Accuracy 0.779296875\n",
      "Iteration 47880 Training loss 0.014749009162187576 Validation loss 0.021177858114242554 Accuracy 0.7841796875\n",
      "Iteration 47890 Training loss 0.01502340566366911 Validation loss 0.021047309041023254 Accuracy 0.78369140625\n",
      "Iteration 47900 Training loss 0.017548471689224243 Validation loss 0.021121153607964516 Accuracy 0.78271484375\n",
      "Iteration 47910 Training loss 0.017167069017887115 Validation loss 0.021514713764190674 Accuracy 0.7783203125\n",
      "Iteration 47920 Training loss 0.015101964585483074 Validation loss 0.021277694031596184 Accuracy 0.78173828125\n",
      "Iteration 47930 Training loss 0.015233693644404411 Validation loss 0.020981652662158012 Accuracy 0.78466796875\n",
      "Iteration 47940 Training loss 0.01479255873709917 Validation loss 0.021069660782814026 Accuracy 0.783203125\n",
      "Iteration 47950 Training loss 0.014950950630009174 Validation loss 0.02129868045449257 Accuracy 0.7802734375\n",
      "Iteration 47960 Training loss 0.013263508677482605 Validation loss 0.021090609952807426 Accuracy 0.78369140625\n",
      "Iteration 47970 Training loss 0.014384688809514046 Validation loss 0.02141568809747696 Accuracy 0.7802734375\n",
      "Iteration 47980 Training loss 0.013951069675385952 Validation loss 0.02088424563407898 Accuracy 0.78564453125\n",
      "Iteration 47990 Training loss 0.014615535736083984 Validation loss 0.020970003679394722 Accuracy 0.7841796875\n",
      "Iteration 48000 Training loss 0.015053167939186096 Validation loss 0.021026235073804855 Accuracy 0.78369140625\n",
      "Iteration 48010 Training loss 0.0181003138422966 Validation loss 0.02127416618168354 Accuracy 0.78076171875\n",
      "Iteration 48020 Training loss 0.013236206024885178 Validation loss 0.021038079634308815 Accuracy 0.78271484375\n",
      "Iteration 48030 Training loss 0.01639431156218052 Validation loss 0.02083713375031948 Accuracy 0.78515625\n",
      "Iteration 48040 Training loss 0.012143823318183422 Validation loss 0.02095090225338936 Accuracy 0.78466796875\n",
      "Iteration 48050 Training loss 0.016575628891587257 Validation loss 0.021369827911257744 Accuracy 0.7802734375\n",
      "Iteration 48060 Training loss 0.01775396429002285 Validation loss 0.02086367830634117 Accuracy 0.78515625\n",
      "Iteration 48070 Training loss 0.018722224980592728 Validation loss 0.022460663691163063 Accuracy 0.7705078125\n",
      "Iteration 48080 Training loss 0.017118923366069794 Validation loss 0.022385595366358757 Accuracy 0.76953125\n",
      "Iteration 48090 Training loss 0.01602383330464363 Validation loss 0.02102319337427616 Accuracy 0.7841796875\n",
      "Iteration 48100 Training loss 0.016086751595139503 Validation loss 0.02116360142827034 Accuracy 0.78173828125\n",
      "Iteration 48110 Training loss 0.015351430512964725 Validation loss 0.021760541945695877 Accuracy 0.77587890625\n",
      "Iteration 48120 Training loss 0.016344962641596794 Validation loss 0.021082362160086632 Accuracy 0.78369140625\n",
      "Iteration 48130 Training loss 0.011989232152700424 Validation loss 0.021058769896626472 Accuracy 0.78369140625\n",
      "Iteration 48140 Training loss 0.01594608835875988 Validation loss 0.021098876371979713 Accuracy 0.78369140625\n",
      "Iteration 48150 Training loss 0.014784011989831924 Validation loss 0.021300775930285454 Accuracy 0.78125\n",
      "Iteration 48160 Training loss 0.013887058943510056 Validation loss 0.021147651597857475 Accuracy 0.78271484375\n",
      "Iteration 48170 Training loss 0.014791811816394329 Validation loss 0.020968588069081306 Accuracy 0.78515625\n",
      "Iteration 48180 Training loss 0.01467805728316307 Validation loss 0.021387986838817596 Accuracy 0.78173828125\n",
      "Iteration 48190 Training loss 0.01584135927259922 Validation loss 0.021550297737121582 Accuracy 0.779296875\n",
      "Iteration 48200 Training loss 0.013488801196217537 Validation loss 0.020977959036827087 Accuracy 0.7841796875\n",
      "Iteration 48210 Training loss 0.01601850427687168 Validation loss 0.0211054477840662 Accuracy 0.783203125\n",
      "Iteration 48220 Training loss 0.013044893741607666 Validation loss 0.021128123626112938 Accuracy 0.783203125\n",
      "Iteration 48230 Training loss 0.014899848960340023 Validation loss 0.0210890956223011 Accuracy 0.78369140625\n",
      "Iteration 48240 Training loss 0.015721334144473076 Validation loss 0.020998729392886162 Accuracy 0.78369140625\n",
      "Iteration 48250 Training loss 0.013804912567138672 Validation loss 0.020872825756669044 Accuracy 0.78515625\n",
      "Iteration 48260 Training loss 0.015218202956020832 Validation loss 0.021378938108682632 Accuracy 0.77978515625\n",
      "Iteration 48270 Training loss 0.01650470308959484 Validation loss 0.020859017968177795 Accuracy 0.78564453125\n",
      "Iteration 48280 Training loss 0.017080137506127357 Validation loss 0.021392839029431343 Accuracy 0.77978515625\n",
      "Iteration 48290 Training loss 0.013936541974544525 Validation loss 0.021009579300880432 Accuracy 0.78369140625\n",
      "Iteration 48300 Training loss 0.016254620626568794 Validation loss 0.022091131657361984 Accuracy 0.77197265625\n",
      "Iteration 48310 Training loss 0.014544283039867878 Validation loss 0.021357055753469467 Accuracy 0.78125\n",
      "Iteration 48320 Training loss 0.013171677477657795 Validation loss 0.02129008062183857 Accuracy 0.78125\n",
      "Iteration 48330 Training loss 0.017434794455766678 Validation loss 0.02150469832122326 Accuracy 0.779296875\n",
      "Iteration 48340 Training loss 0.014399202540516853 Validation loss 0.021131493151187897 Accuracy 0.783203125\n",
      "Iteration 48350 Training loss 0.01930268108844757 Validation loss 0.021016689017415047 Accuracy 0.783203125\n",
      "Iteration 48360 Training loss 0.016518959775567055 Validation loss 0.020901722833514214 Accuracy 0.78515625\n",
      "Iteration 48370 Training loss 0.016069000586867332 Validation loss 0.021253090351819992 Accuracy 0.78076171875\n",
      "Iteration 48380 Training loss 0.01636696048080921 Validation loss 0.021673517301678658 Accuracy 0.77783203125\n",
      "Iteration 48390 Training loss 0.01833750307559967 Validation loss 0.0230408962816 Accuracy 0.763671875\n",
      "Iteration 48400 Training loss 0.014226277358829975 Validation loss 0.02115141600370407 Accuracy 0.78271484375\n",
      "Iteration 48410 Training loss 0.017444131895899773 Validation loss 0.021239150315523148 Accuracy 0.78125\n",
      "Iteration 48420 Training loss 0.015185259282588959 Validation loss 0.020931562408804893 Accuracy 0.78466796875\n",
      "Iteration 48430 Training loss 0.017581280320882797 Validation loss 0.021597426384687424 Accuracy 0.77734375\n",
      "Iteration 48440 Training loss 0.01330199372023344 Validation loss 0.021173914894461632 Accuracy 0.78271484375\n",
      "Iteration 48450 Training loss 0.016171978786587715 Validation loss 0.021133163943886757 Accuracy 0.78271484375\n",
      "Iteration 48460 Training loss 0.01403315830975771 Validation loss 0.021285515278577805 Accuracy 0.78076171875\n",
      "Iteration 48470 Training loss 0.014346756972372532 Validation loss 0.021143421530723572 Accuracy 0.7822265625\n",
      "Iteration 48480 Training loss 0.01578022725880146 Validation loss 0.021704833954572678 Accuracy 0.77734375\n",
      "Iteration 48490 Training loss 0.014979577623307705 Validation loss 0.021469557657837868 Accuracy 0.77783203125\n",
      "Iteration 48500 Training loss 0.016783395782113075 Validation loss 0.021274320781230927 Accuracy 0.78125\n",
      "Iteration 48510 Training loss 0.015180646441876888 Validation loss 0.0211653895676136 Accuracy 0.7822265625\n",
      "Iteration 48520 Training loss 0.01604962721467018 Validation loss 0.021436244249343872 Accuracy 0.779296875\n",
      "Iteration 48530 Training loss 0.015979258343577385 Validation loss 0.02134428173303604 Accuracy 0.78076171875\n",
      "Iteration 48540 Training loss 0.016473839059472084 Validation loss 0.022267308086156845 Accuracy 0.77001953125\n",
      "Iteration 48550 Training loss 0.014936632476747036 Validation loss 0.023256951943039894 Accuracy 0.76123046875\n",
      "Iteration 48560 Training loss 0.014036036096513271 Validation loss 0.02095143496990204 Accuracy 0.7841796875\n",
      "Iteration 48570 Training loss 0.016865739598870277 Validation loss 0.021197430789470673 Accuracy 0.7822265625\n",
      "Iteration 48580 Training loss 0.013510530814528465 Validation loss 0.021079175174236298 Accuracy 0.78369140625\n",
      "Iteration 48590 Training loss 0.014746785163879395 Validation loss 0.020942626520991325 Accuracy 0.7841796875\n",
      "Iteration 48600 Training loss 0.01572800986468792 Validation loss 0.021134760230779648 Accuracy 0.7822265625\n",
      "Iteration 48610 Training loss 0.015024729073047638 Validation loss 0.02099751867353916 Accuracy 0.78466796875\n",
      "Iteration 48620 Training loss 0.01745697483420372 Validation loss 0.021039359271526337 Accuracy 0.783203125\n",
      "Iteration 48630 Training loss 0.0175058301538229 Validation loss 0.021156873553991318 Accuracy 0.7822265625\n",
      "Iteration 48640 Training loss 0.016491858288645744 Validation loss 0.0212556142359972 Accuracy 0.78076171875\n",
      "Iteration 48650 Training loss 0.01262779999524355 Validation loss 0.02100072242319584 Accuracy 0.7841796875\n",
      "Iteration 48660 Training loss 0.014355169609189034 Validation loss 0.020935045555233955 Accuracy 0.78369140625\n",
      "Iteration 48670 Training loss 0.013928054831922054 Validation loss 0.02101796120405197 Accuracy 0.7841796875\n",
      "Iteration 48680 Training loss 0.015588666312396526 Validation loss 0.020936401560902596 Accuracy 0.7841796875\n",
      "Iteration 48690 Training loss 0.018366113305091858 Validation loss 0.021565981209278107 Accuracy 0.77880859375\n",
      "Iteration 48700 Training loss 0.013995069079101086 Validation loss 0.0210068691521883 Accuracy 0.7841796875\n",
      "Iteration 48710 Training loss 0.013794958591461182 Validation loss 0.020874785259366035 Accuracy 0.78466796875\n",
      "Iteration 48720 Training loss 0.017584392800927162 Validation loss 0.021204132586717606 Accuracy 0.78173828125\n",
      "Iteration 48730 Training loss 0.01689118705689907 Validation loss 0.02098245732486248 Accuracy 0.78466796875\n",
      "Iteration 48740 Training loss 0.013666007667779922 Validation loss 0.020936422049999237 Accuracy 0.78515625\n",
      "Iteration 48750 Training loss 0.014383971691131592 Validation loss 0.021048037335276604 Accuracy 0.78271484375\n",
      "Iteration 48760 Training loss 0.015027269721031189 Validation loss 0.02113908715546131 Accuracy 0.783203125\n",
      "Iteration 48770 Training loss 0.015294509939849377 Validation loss 0.021028514951467514 Accuracy 0.7841796875\n",
      "Iteration 48780 Training loss 0.015561609528958797 Validation loss 0.020891517400741577 Accuracy 0.78515625\n",
      "Iteration 48790 Training loss 0.012263472191989422 Validation loss 0.021015195176005363 Accuracy 0.78466796875\n",
      "Iteration 48800 Training loss 0.01795266941189766 Validation loss 0.02095656283199787 Accuracy 0.78466796875\n",
      "Iteration 48810 Training loss 0.015959780663251877 Validation loss 0.021193569526076317 Accuracy 0.78271484375\n",
      "Iteration 48820 Training loss 0.013399958610534668 Validation loss 0.02115338295698166 Accuracy 0.78271484375\n",
      "Iteration 48830 Training loss 0.01564706861972809 Validation loss 0.021013310179114342 Accuracy 0.783203125\n",
      "Iteration 48840 Training loss 0.014806139282882214 Validation loss 0.02106146514415741 Accuracy 0.78466796875\n",
      "Iteration 48850 Training loss 0.016496894881129265 Validation loss 0.020887451246380806 Accuracy 0.78564453125\n",
      "Iteration 48860 Training loss 0.016145851463079453 Validation loss 0.021032197400927544 Accuracy 0.78466796875\n",
      "Iteration 48870 Training loss 0.016491834074258804 Validation loss 0.021252689883112907 Accuracy 0.78173828125\n",
      "Iteration 48880 Training loss 0.014276341535151005 Validation loss 0.020839298143982887 Accuracy 0.7861328125\n",
      "Iteration 48890 Training loss 0.012076113373041153 Validation loss 0.020953141152858734 Accuracy 0.78515625\n",
      "Iteration 48900 Training loss 0.015834232792258263 Validation loss 0.0210671778768301 Accuracy 0.783203125\n",
      "Iteration 48910 Training loss 0.01681448332965374 Validation loss 0.021357174962759018 Accuracy 0.7802734375\n",
      "Iteration 48920 Training loss 0.014878606423735619 Validation loss 0.02152436412870884 Accuracy 0.7783203125\n",
      "Iteration 48930 Training loss 0.016435004770755768 Validation loss 0.02097082883119583 Accuracy 0.78369140625\n",
      "Iteration 48940 Training loss 0.017387356609106064 Validation loss 0.021470636129379272 Accuracy 0.779296875\n",
      "Iteration 48950 Training loss 0.01497343648225069 Validation loss 0.021324144676327705 Accuracy 0.77978515625\n",
      "Iteration 48960 Training loss 0.016618967056274414 Validation loss 0.021093852818012238 Accuracy 0.78271484375\n",
      "Iteration 48970 Training loss 0.016421493142843246 Validation loss 0.021467391401529312 Accuracy 0.77880859375\n",
      "Iteration 48980 Training loss 0.015601951628923416 Validation loss 0.02148989960551262 Accuracy 0.779296875\n",
      "Iteration 48990 Training loss 0.015013499185442924 Validation loss 0.02139570191502571 Accuracy 0.77978515625\n",
      "Iteration 49000 Training loss 0.013894914649426937 Validation loss 0.020978987216949463 Accuracy 0.7841796875\n",
      "Iteration 49010 Training loss 0.015495737083256245 Validation loss 0.021147722378373146 Accuracy 0.78173828125\n",
      "Iteration 49020 Training loss 0.013130162842571735 Validation loss 0.02096579596400261 Accuracy 0.7841796875\n",
      "Iteration 49030 Training loss 0.013865157961845398 Validation loss 0.021285220980644226 Accuracy 0.78125\n",
      "Iteration 49040 Training loss 0.016398046165704727 Validation loss 0.02091647870838642 Accuracy 0.78515625\n",
      "Iteration 49050 Training loss 0.01678706891834736 Validation loss 0.022176850587129593 Accuracy 0.77197265625\n",
      "Iteration 49060 Training loss 0.016895608976483345 Validation loss 0.021382231265306473 Accuracy 0.779296875\n",
      "Iteration 49070 Training loss 0.013493563048541546 Validation loss 0.02133142575621605 Accuracy 0.7802734375\n",
      "Iteration 49080 Training loss 0.013373996131122112 Validation loss 0.02120363712310791 Accuracy 0.78173828125\n",
      "Iteration 49090 Training loss 0.015395662747323513 Validation loss 0.020885057747364044 Accuracy 0.78369140625\n",
      "Iteration 49100 Training loss 0.01752450317144394 Validation loss 0.0212162546813488 Accuracy 0.78173828125\n",
      "Iteration 49110 Training loss 0.01613847352564335 Validation loss 0.021149979904294014 Accuracy 0.7822265625\n",
      "Iteration 49120 Training loss 0.015112700872123241 Validation loss 0.02107340842485428 Accuracy 0.783203125\n",
      "Iteration 49130 Training loss 0.017768071964383125 Validation loss 0.021132200956344604 Accuracy 0.7822265625\n",
      "Iteration 49140 Training loss 0.015123071148991585 Validation loss 0.02132696658372879 Accuracy 0.77880859375\n",
      "Iteration 49150 Training loss 0.015584494918584824 Validation loss 0.020966079086065292 Accuracy 0.78466796875\n",
      "Iteration 49160 Training loss 0.016100116074085236 Validation loss 0.02117421105504036 Accuracy 0.7822265625\n",
      "Iteration 49170 Training loss 0.017447177320718765 Validation loss 0.021156102418899536 Accuracy 0.78173828125\n",
      "Iteration 49180 Training loss 0.015073684975504875 Validation loss 0.02129463292658329 Accuracy 0.78076171875\n",
      "Iteration 49190 Training loss 0.013794458471238613 Validation loss 0.021109389141201973 Accuracy 0.78271484375\n",
      "Iteration 49200 Training loss 0.015804432332515717 Validation loss 0.021051928400993347 Accuracy 0.783203125\n",
      "Iteration 49210 Training loss 0.015264905989170074 Validation loss 0.021274030208587646 Accuracy 0.78125\n",
      "Iteration 49220 Training loss 0.015491006895899773 Validation loss 0.021002763882279396 Accuracy 0.78369140625\n",
      "Iteration 49230 Training loss 0.014977442100644112 Validation loss 0.021027572453022003 Accuracy 0.7841796875\n",
      "Iteration 49240 Training loss 0.014783797785639763 Validation loss 0.02133508212864399 Accuracy 0.78076171875\n",
      "Iteration 49250 Training loss 0.012951896525919437 Validation loss 0.02096829004585743 Accuracy 0.7841796875\n",
      "Iteration 49260 Training loss 0.014098762534558773 Validation loss 0.020830847322940826 Accuracy 0.78662109375\n",
      "Iteration 49270 Training loss 0.0154840387403965 Validation loss 0.020956749096512794 Accuracy 0.78466796875\n",
      "Iteration 49280 Training loss 0.014161600731313229 Validation loss 0.021011555567383766 Accuracy 0.78369140625\n",
      "Iteration 49290 Training loss 0.01575987972319126 Validation loss 0.02098650299012661 Accuracy 0.78515625\n",
      "Iteration 49300 Training loss 0.015550783835351467 Validation loss 0.02119876816868782 Accuracy 0.78271484375\n",
      "Iteration 49310 Training loss 0.016457511112093925 Validation loss 0.02093588374555111 Accuracy 0.7841796875\n",
      "Iteration 49320 Training loss 0.014412150718271732 Validation loss 0.022143790498375893 Accuracy 0.7724609375\n",
      "Iteration 49330 Training loss 0.013337815180420876 Validation loss 0.021449774503707886 Accuracy 0.7783203125\n",
      "Iteration 49340 Training loss 0.014239778742194176 Validation loss 0.02109382301568985 Accuracy 0.783203125\n",
      "Iteration 49350 Training loss 0.016686173155903816 Validation loss 0.021073676645755768 Accuracy 0.78369140625\n",
      "Iteration 49360 Training loss 0.014346234500408173 Validation loss 0.020837198942899704 Accuracy 0.7861328125\n",
      "Iteration 49370 Training loss 0.016492832452058792 Validation loss 0.02091839723289013 Accuracy 0.78564453125\n",
      "Iteration 49380 Training loss 0.01620461419224739 Validation loss 0.021085835993289948 Accuracy 0.78271484375\n",
      "Iteration 49390 Training loss 0.016121726483106613 Validation loss 0.022201823070645332 Accuracy 0.77197265625\n",
      "Iteration 49400 Training loss 0.016010282561182976 Validation loss 0.02114005759358406 Accuracy 0.78173828125\n",
      "Iteration 49410 Training loss 0.01526583731174469 Validation loss 0.021028945222496986 Accuracy 0.78271484375\n",
      "Iteration 49420 Training loss 0.016172735020518303 Validation loss 0.021120784804224968 Accuracy 0.783203125\n",
      "Iteration 49430 Training loss 0.016615217551589012 Validation loss 0.021057719364762306 Accuracy 0.783203125\n",
      "Iteration 49440 Training loss 0.014228973537683487 Validation loss 0.021198656409978867 Accuracy 0.78173828125\n",
      "Iteration 49450 Training loss 0.015564722940325737 Validation loss 0.021725459024310112 Accuracy 0.7763671875\n",
      "Iteration 49460 Training loss 0.013711122795939445 Validation loss 0.02113981917500496 Accuracy 0.7822265625\n",
      "Iteration 49470 Training loss 0.01602144166827202 Validation loss 0.021221376955509186 Accuracy 0.78271484375\n",
      "Iteration 49480 Training loss 0.015328832902014256 Validation loss 0.02100287564098835 Accuracy 0.7841796875\n",
      "Iteration 49490 Training loss 0.01467215921729803 Validation loss 0.02157033234834671 Accuracy 0.7783203125\n",
      "Iteration 49500 Training loss 0.015407274477183819 Validation loss 0.02135271020233631 Accuracy 0.77978515625\n",
      "Iteration 49510 Training loss 0.011952056549489498 Validation loss 0.02099176123738289 Accuracy 0.78369140625\n",
      "Iteration 49520 Training loss 0.01640051230788231 Validation loss 0.021155716851353645 Accuracy 0.78271484375\n",
      "Iteration 49530 Training loss 0.013576778583228588 Validation loss 0.02113005518913269 Accuracy 0.78369140625\n",
      "Iteration 49540 Training loss 0.016154393553733826 Validation loss 0.02106735296547413 Accuracy 0.7841796875\n",
      "Iteration 49550 Training loss 0.016740402206778526 Validation loss 0.02082694321870804 Accuracy 0.78564453125\n",
      "Iteration 49560 Training loss 0.0131354546174407 Validation loss 0.021627157926559448 Accuracy 0.77685546875\n",
      "Iteration 49570 Training loss 0.016423266381025314 Validation loss 0.021023612469434738 Accuracy 0.78271484375\n",
      "Iteration 49580 Training loss 0.014663442969322205 Validation loss 0.02133971080183983 Accuracy 0.7802734375\n",
      "Iteration 49590 Training loss 0.015680555254220963 Validation loss 0.021205417811870575 Accuracy 0.78173828125\n",
      "Iteration 49600 Training loss 0.018254945054650307 Validation loss 0.021436795592308044 Accuracy 0.779296875\n",
      "Iteration 49610 Training loss 0.0165475532412529 Validation loss 0.021416040137410164 Accuracy 0.77978515625\n",
      "Iteration 49620 Training loss 0.012881174683570862 Validation loss 0.021089235320687294 Accuracy 0.78271484375\n",
      "Iteration 49630 Training loss 0.01586228236556053 Validation loss 0.02110772393643856 Accuracy 0.783203125\n",
      "Iteration 49640 Training loss 0.013663973659276962 Validation loss 0.020930597558617592 Accuracy 0.78466796875\n",
      "Iteration 49650 Training loss 0.015711139887571335 Validation loss 0.021319355815649033 Accuracy 0.78173828125\n",
      "Iteration 49660 Training loss 0.01695394329726696 Validation loss 0.021060334518551826 Accuracy 0.783203125\n",
      "Iteration 49670 Training loss 0.016230769455432892 Validation loss 0.02181311696767807 Accuracy 0.77587890625\n",
      "Iteration 49680 Training loss 0.013568571768701077 Validation loss 0.02116313949227333 Accuracy 0.78173828125\n",
      "Iteration 49690 Training loss 0.014160960912704468 Validation loss 0.021118473261594772 Accuracy 0.78271484375\n",
      "Iteration 49700 Training loss 0.017306823283433914 Validation loss 0.02155761606991291 Accuracy 0.7783203125\n",
      "Iteration 49710 Training loss 0.013288403861224651 Validation loss 0.021103069186210632 Accuracy 0.78271484375\n",
      "Iteration 49720 Training loss 0.013999677263200283 Validation loss 0.021140189841389656 Accuracy 0.78271484375\n",
      "Iteration 49730 Training loss 0.0168763380497694 Validation loss 0.021407682448625565 Accuracy 0.779296875\n",
      "Iteration 49740 Training loss 0.01410504151135683 Validation loss 0.021060848608613014 Accuracy 0.7841796875\n",
      "Iteration 49750 Training loss 0.01672469638288021 Validation loss 0.021281549707055092 Accuracy 0.78125\n",
      "Iteration 49760 Training loss 0.0180727019906044 Validation loss 0.02102445811033249 Accuracy 0.78369140625\n",
      "Iteration 49770 Training loss 0.014418648555874825 Validation loss 0.020914865657687187 Accuracy 0.78515625\n",
      "Iteration 49780 Training loss 0.012752147391438484 Validation loss 0.020747918635606766 Accuracy 0.78662109375\n",
      "Iteration 49790 Training loss 0.014929019846022129 Validation loss 0.021278368309140205 Accuracy 0.78271484375\n",
      "Iteration 49800 Training loss 0.014226676896214485 Validation loss 0.02157304435968399 Accuracy 0.7783203125\n",
      "Iteration 49810 Training loss 0.015521346591413021 Validation loss 0.021276943385601044 Accuracy 0.78173828125\n",
      "Iteration 49820 Training loss 0.014409426599740982 Validation loss 0.021350253373384476 Accuracy 0.78076171875\n",
      "Iteration 49830 Training loss 0.015044341795146465 Validation loss 0.020719023421406746 Accuracy 0.78759765625\n",
      "Iteration 49840 Training loss 0.01776001788675785 Validation loss 0.02100156620144844 Accuracy 0.78369140625\n",
      "Iteration 49850 Training loss 0.014617952518165112 Validation loss 0.02072303183376789 Accuracy 0.7861328125\n",
      "Iteration 49860 Training loss 0.013422194868326187 Validation loss 0.021067319437861443 Accuracy 0.783203125\n",
      "Iteration 49870 Training loss 0.014083865098655224 Validation loss 0.020941073074936867 Accuracy 0.78466796875\n",
      "Iteration 49880 Training loss 0.01302677858620882 Validation loss 0.020824862644076347 Accuracy 0.78466796875\n",
      "Iteration 49890 Training loss 0.012497593648731709 Validation loss 0.021129703149199486 Accuracy 0.7822265625\n",
      "Iteration 49900 Training loss 0.013243010267615318 Validation loss 0.021012982353568077 Accuracy 0.7841796875\n",
      "Iteration 49910 Training loss 0.013479880057275295 Validation loss 0.02076498046517372 Accuracy 0.7861328125\n",
      "Iteration 49920 Training loss 0.01409967616200447 Validation loss 0.021036630496382713 Accuracy 0.78271484375\n",
      "Iteration 49930 Training loss 0.012639859691262245 Validation loss 0.021035345271229744 Accuracy 0.78369140625\n",
      "Iteration 49940 Training loss 0.01377759501338005 Validation loss 0.021112997084856033 Accuracy 0.78271484375\n",
      "Iteration 49950 Training loss 0.014873631298542023 Validation loss 0.020860638469457626 Accuracy 0.7861328125\n",
      "Iteration 49960 Training loss 0.015605343505740166 Validation loss 0.020736709237098694 Accuracy 0.7861328125\n",
      "Iteration 49970 Training loss 0.014008107595145702 Validation loss 0.0213808324187994 Accuracy 0.7802734375\n",
      "Iteration 49980 Training loss 0.017286516726017 Validation loss 0.02117784693837166 Accuracy 0.7822265625\n",
      "Iteration 49990 Training loss 0.016676682978868484 Validation loss 0.021024437621235847 Accuracy 0.7841796875\n",
      "Iteration 50000 Training loss 0.01572338491678238 Validation loss 0.02098284661769867 Accuracy 0.78369140625\n",
      "Iteration 50010 Training loss 0.017220141366124153 Validation loss 0.021546993404626846 Accuracy 0.779296875\n",
      "Iteration 50020 Training loss 0.015895675867795944 Validation loss 0.020861586555838585 Accuracy 0.78515625\n",
      "Iteration 50030 Training loss 0.01327994279563427 Validation loss 0.020922748371958733 Accuracy 0.78466796875\n",
      "Iteration 50040 Training loss 0.014225353486835957 Validation loss 0.02093195728957653 Accuracy 0.7841796875\n",
      "Iteration 50050 Training loss 0.016868527978658676 Validation loss 0.021182019263505936 Accuracy 0.78173828125\n",
      "Iteration 50060 Training loss 0.014886067248880863 Validation loss 0.02083604410290718 Accuracy 0.78564453125\n",
      "Iteration 50070 Training loss 0.012632749043405056 Validation loss 0.021068627014756203 Accuracy 0.78369140625\n",
      "Iteration 50080 Training loss 0.014155428856611252 Validation loss 0.020837927237153053 Accuracy 0.78564453125\n",
      "Iteration 50090 Training loss 0.014345125295221806 Validation loss 0.021022938191890717 Accuracy 0.7841796875\n",
      "Iteration 50100 Training loss 0.014553701505064964 Validation loss 0.021365439519286156 Accuracy 0.7802734375\n",
      "Iteration 50110 Training loss 0.013743527233600616 Validation loss 0.020980708301067352 Accuracy 0.78515625\n",
      "Iteration 50120 Training loss 0.01859540492296219 Validation loss 0.021517060697078705 Accuracy 0.77783203125\n",
      "Iteration 50130 Training loss 0.01447698101401329 Validation loss 0.021012460812926292 Accuracy 0.7841796875\n",
      "Iteration 50140 Training loss 0.015502975322306156 Validation loss 0.02114618569612503 Accuracy 0.783203125\n",
      "Iteration 50150 Training loss 0.014079493470489979 Validation loss 0.02143646962940693 Accuracy 0.779296875\n",
      "Iteration 50160 Training loss 0.015034645795822144 Validation loss 0.02096417360007763 Accuracy 0.7841796875\n",
      "Iteration 50170 Training loss 0.014631032012403011 Validation loss 0.0212009996175766 Accuracy 0.7822265625\n",
      "Iteration 50180 Training loss 0.018306011334061623 Validation loss 0.020954735577106476 Accuracy 0.78369140625\n",
      "Iteration 50190 Training loss 0.014850466512143612 Validation loss 0.02073029801249504 Accuracy 0.78662109375\n",
      "Iteration 50200 Training loss 0.017429569736123085 Validation loss 0.02099514566361904 Accuracy 0.78466796875\n",
      "Iteration 50210 Training loss 0.015726398676633835 Validation loss 0.020828470587730408 Accuracy 0.7861328125\n",
      "Iteration 50220 Training loss 0.013815785758197308 Validation loss 0.02090631239116192 Accuracy 0.7861328125\n",
      "Iteration 50230 Training loss 0.016359591856598854 Validation loss 0.02091454528272152 Accuracy 0.7861328125\n",
      "Iteration 50240 Training loss 0.017229273915290833 Validation loss 0.02099216729402542 Accuracy 0.7841796875\n",
      "Iteration 50250 Training loss 0.018516305834054947 Validation loss 0.021015817299485207 Accuracy 0.7841796875\n",
      "Iteration 50260 Training loss 0.013594301417469978 Validation loss 0.021040305495262146 Accuracy 0.783203125\n",
      "Iteration 50270 Training loss 0.015354039147496223 Validation loss 0.021710718050599098 Accuracy 0.775390625\n",
      "Iteration 50280 Training loss 0.01657983846962452 Validation loss 0.021362606436014175 Accuracy 0.7802734375\n",
      "Iteration 50290 Training loss 0.01454255636781454 Validation loss 0.020961806178092957 Accuracy 0.7841796875\n",
      "Iteration 50300 Training loss 0.013591515831649303 Validation loss 0.021034618839621544 Accuracy 0.78369140625\n",
      "Iteration 50310 Training loss 0.016062738373875618 Validation loss 0.021614503115415573 Accuracy 0.77734375\n",
      "Iteration 50320 Training loss 0.018911181017756462 Validation loss 0.021037964150309563 Accuracy 0.78369140625\n",
      "Iteration 50330 Training loss 0.013706621713936329 Validation loss 0.020893117412924767 Accuracy 0.7841796875\n",
      "Iteration 50340 Training loss 0.015031312592327595 Validation loss 0.02110939472913742 Accuracy 0.78369140625\n",
      "Iteration 50350 Training loss 0.01634412258863449 Validation loss 0.021309196949005127 Accuracy 0.78125\n",
      "Iteration 50360 Training loss 0.014424626715481281 Validation loss 0.020839663222432137 Accuracy 0.78515625\n",
      "Iteration 50370 Training loss 0.01648038811981678 Validation loss 0.020902523770928383 Accuracy 0.78466796875\n",
      "Iteration 50380 Training loss 0.014516250230371952 Validation loss 0.021583519876003265 Accuracy 0.77783203125\n",
      "Iteration 50390 Training loss 0.014950280077755451 Validation loss 0.02106732316315174 Accuracy 0.7841796875\n",
      "Iteration 50400 Training loss 0.016973473131656647 Validation loss 0.02154245786368847 Accuracy 0.77783203125\n",
      "Iteration 50410 Training loss 0.01464269869029522 Validation loss 0.02096058614552021 Accuracy 0.7841796875\n",
      "Iteration 50420 Training loss 0.016115006059408188 Validation loss 0.020923161879181862 Accuracy 0.7841796875\n",
      "Iteration 50430 Training loss 0.015719808638095856 Validation loss 0.02075948193669319 Accuracy 0.7861328125\n",
      "Iteration 50440 Training loss 0.013853317126631737 Validation loss 0.02122650295495987 Accuracy 0.7822265625\n",
      "Iteration 50450 Training loss 0.0151877012103796 Validation loss 0.021865760907530785 Accuracy 0.77490234375\n",
      "Iteration 50460 Training loss 0.014474052004516125 Validation loss 0.021108295768499374 Accuracy 0.783203125\n",
      "Iteration 50470 Training loss 0.011975713074207306 Validation loss 0.0209409948438406 Accuracy 0.783203125\n",
      "Iteration 50480 Training loss 0.01628325693309307 Validation loss 0.020955169573426247 Accuracy 0.78369140625\n",
      "Iteration 50490 Training loss 0.01599634811282158 Validation loss 0.020978344604372978 Accuracy 0.783203125\n",
      "Iteration 50500 Training loss 0.017055567353963852 Validation loss 0.020906537771224976 Accuracy 0.7841796875\n",
      "Iteration 50510 Training loss 0.013617325574159622 Validation loss 0.021149316802620888 Accuracy 0.7822265625\n",
      "Iteration 50520 Training loss 0.01531667448580265 Validation loss 0.020908651873469353 Accuracy 0.78466796875\n",
      "Iteration 50530 Training loss 0.015461396425962448 Validation loss 0.021016499027609825 Accuracy 0.783203125\n",
      "Iteration 50540 Training loss 0.01664896309375763 Validation loss 0.021257242187857628 Accuracy 0.78125\n",
      "Iteration 50550 Training loss 0.013156314380466938 Validation loss 0.020982155576348305 Accuracy 0.78466796875\n",
      "Iteration 50560 Training loss 0.014724735170602798 Validation loss 0.020777035504579544 Accuracy 0.78564453125\n",
      "Iteration 50570 Training loss 0.015201855450868607 Validation loss 0.021161938086152077 Accuracy 0.78271484375\n",
      "Iteration 50580 Training loss 0.01767238788306713 Validation loss 0.02087016962468624 Accuracy 0.78515625\n",
      "Iteration 50590 Training loss 0.013559762388467789 Validation loss 0.02082374505698681 Accuracy 0.78515625\n",
      "Iteration 50600 Training loss 0.014401324093341827 Validation loss 0.0210904348641634 Accuracy 0.783203125\n",
      "Iteration 50610 Training loss 0.013775338418781757 Validation loss 0.02081194333732128 Accuracy 0.78564453125\n",
      "Iteration 50620 Training loss 0.016260558739304543 Validation loss 0.021042855456471443 Accuracy 0.7822265625\n",
      "Iteration 50630 Training loss 0.013806953094899654 Validation loss 0.020825320854783058 Accuracy 0.78564453125\n",
      "Iteration 50640 Training loss 0.018152499571442604 Validation loss 0.020872026681900024 Accuracy 0.78564453125\n",
      "Iteration 50650 Training loss 0.01544330082833767 Validation loss 0.020904384553432465 Accuracy 0.78515625\n",
      "Iteration 50660 Training loss 0.012958311475813389 Validation loss 0.020731691271066666 Accuracy 0.78662109375\n",
      "Iteration 50670 Training loss 0.01554009597748518 Validation loss 0.021335819736123085 Accuracy 0.78076171875\n",
      "Iteration 50680 Training loss 0.015830425545573235 Validation loss 0.021657651290297508 Accuracy 0.77734375\n",
      "Iteration 50690 Training loss 0.018592217937111855 Validation loss 0.02092061936855316 Accuracy 0.78466796875\n",
      "Iteration 50700 Training loss 0.017462510615587234 Validation loss 0.02079133689403534 Accuracy 0.7861328125\n",
      "Iteration 50710 Training loss 0.016326667740941048 Validation loss 0.02078755386173725 Accuracy 0.78662109375\n",
      "Iteration 50720 Training loss 0.016155796125531197 Validation loss 0.02085164561867714 Accuracy 0.78515625\n",
      "Iteration 50730 Training loss 0.016332212835550308 Validation loss 0.021184466779232025 Accuracy 0.78125\n",
      "Iteration 50740 Training loss 0.017833594232797623 Validation loss 0.02089064195752144 Accuracy 0.78515625\n",
      "Iteration 50750 Training loss 0.01454366184771061 Validation loss 0.020883891731500626 Accuracy 0.78564453125\n",
      "Iteration 50760 Training loss 0.015255553647875786 Validation loss 0.021031320095062256 Accuracy 0.7841796875\n",
      "Iteration 50770 Training loss 0.016587737947702408 Validation loss 0.02080872468650341 Accuracy 0.78515625\n",
      "Iteration 50780 Training loss 0.015460565686225891 Validation loss 0.02107657864689827 Accuracy 0.78369140625\n",
      "Iteration 50790 Training loss 0.01690547727048397 Validation loss 0.021206121891736984 Accuracy 0.78125\n",
      "Iteration 50800 Training loss 0.014262872748076916 Validation loss 0.020922567695379257 Accuracy 0.78466796875\n",
      "Iteration 50810 Training loss 0.0169404037296772 Validation loss 0.021203698590397835 Accuracy 0.78271484375\n",
      "Iteration 50820 Training loss 0.015348769724369049 Validation loss 0.020863015204668045 Accuracy 0.78564453125\n",
      "Iteration 50830 Training loss 0.015522572211921215 Validation loss 0.020676353946328163 Accuracy 0.78759765625\n",
      "Iteration 50840 Training loss 0.017411580309271812 Validation loss 0.021115893498063087 Accuracy 0.78271484375\n",
      "Iteration 50850 Training loss 0.014782966114580631 Validation loss 0.020635951310396194 Accuracy 0.787109375\n",
      "Iteration 50860 Training loss 0.011754185892641544 Validation loss 0.020589059218764305 Accuracy 0.78759765625\n",
      "Iteration 50870 Training loss 0.01340196467936039 Validation loss 0.02098618634045124 Accuracy 0.78369140625\n",
      "Iteration 50880 Training loss 0.015145927667617798 Validation loss 0.02072160877287388 Accuracy 0.78662109375\n",
      "Iteration 50890 Training loss 0.017042534425854683 Validation loss 0.02124950848519802 Accuracy 0.7822265625\n",
      "Iteration 50900 Training loss 0.014999519102275372 Validation loss 0.020809562876820564 Accuracy 0.78564453125\n",
      "Iteration 50910 Training loss 0.013414915651082993 Validation loss 0.020960381254553795 Accuracy 0.78369140625\n",
      "Iteration 50920 Training loss 0.016686324030160904 Validation loss 0.02094339393079281 Accuracy 0.7841796875\n",
      "Iteration 50930 Training loss 0.013632937334477901 Validation loss 0.020828554406762123 Accuracy 0.78564453125\n",
      "Iteration 50940 Training loss 0.017216455191373825 Validation loss 0.021329188719391823 Accuracy 0.78076171875\n",
      "Iteration 50950 Training loss 0.017355196177959442 Validation loss 0.021230224519968033 Accuracy 0.78173828125\n",
      "Iteration 50960 Training loss 0.012360499240458012 Validation loss 0.02092428132891655 Accuracy 0.78515625\n",
      "Iteration 50970 Training loss 0.014713232405483723 Validation loss 0.020946219563484192 Accuracy 0.7841796875\n",
      "Iteration 50980 Training loss 0.019784778356552124 Validation loss 0.021215226501226425 Accuracy 0.7822265625\n",
      "Iteration 50990 Training loss 0.014176368713378906 Validation loss 0.020871471613645554 Accuracy 0.7841796875\n",
      "Iteration 51000 Training loss 0.015571569092571735 Validation loss 0.021405044943094254 Accuracy 0.77978515625\n",
      "Iteration 51010 Training loss 0.015107196755707264 Validation loss 0.02081802487373352 Accuracy 0.7861328125\n",
      "Iteration 51020 Training loss 0.015357356518507004 Validation loss 0.020830439403653145 Accuracy 0.7861328125\n",
      "Iteration 51030 Training loss 0.016461903229355812 Validation loss 0.021149376407265663 Accuracy 0.7822265625\n",
      "Iteration 51040 Training loss 0.01686861552298069 Validation loss 0.02139003574848175 Accuracy 0.7802734375\n",
      "Iteration 51050 Training loss 0.01733950711786747 Validation loss 0.021786781027913094 Accuracy 0.77685546875\n",
      "Iteration 51060 Training loss 0.014540229924023151 Validation loss 0.021140560507774353 Accuracy 0.7822265625\n",
      "Iteration 51070 Training loss 0.014026130549609661 Validation loss 0.020973876118659973 Accuracy 0.7841796875\n",
      "Iteration 51080 Training loss 0.016580574214458466 Validation loss 0.020837198942899704 Accuracy 0.78466796875\n",
      "Iteration 51090 Training loss 0.015411837957799435 Validation loss 0.020986177027225494 Accuracy 0.78369140625\n",
      "Iteration 51100 Training loss 0.018376968801021576 Validation loss 0.021317580714821815 Accuracy 0.7802734375\n",
      "Iteration 51110 Training loss 0.013761993497610092 Validation loss 0.02077716961503029 Accuracy 0.78515625\n",
      "Iteration 51120 Training loss 0.016496988013386726 Validation loss 0.02123292349278927 Accuracy 0.78173828125\n",
      "Iteration 51130 Training loss 0.016440384089946747 Validation loss 0.020781874656677246 Accuracy 0.7861328125\n",
      "Iteration 51140 Training loss 0.01249905489385128 Validation loss 0.020841849967837334 Accuracy 0.78564453125\n",
      "Iteration 51150 Training loss 0.0165583286434412 Validation loss 0.021154286339879036 Accuracy 0.78271484375\n",
      "Iteration 51160 Training loss 0.01235800702124834 Validation loss 0.020887933671474457 Accuracy 0.78564453125\n",
      "Iteration 51170 Training loss 0.01483787503093481 Validation loss 0.02085452899336815 Accuracy 0.78515625\n",
      "Iteration 51180 Training loss 0.01660335063934326 Validation loss 0.020830074325203896 Accuracy 0.78662109375\n",
      "Iteration 51190 Training loss 0.014248639345169067 Validation loss 0.02072186954319477 Accuracy 0.78662109375\n",
      "Iteration 51200 Training loss 0.014953328296542168 Validation loss 0.02101242169737816 Accuracy 0.7841796875\n",
      "Iteration 51210 Training loss 0.014435592107474804 Validation loss 0.020910359919071198 Accuracy 0.78515625\n",
      "Iteration 51220 Training loss 0.015692491084337234 Validation loss 0.02063686028122902 Accuracy 0.78759765625\n",
      "Iteration 51230 Training loss 0.014871079474687576 Validation loss 0.020947301760315895 Accuracy 0.7841796875\n",
      "Iteration 51240 Training loss 0.01465421263128519 Validation loss 0.020827766507864 Accuracy 0.7861328125\n",
      "Iteration 51250 Training loss 0.013027157634496689 Validation loss 0.02094264142215252 Accuracy 0.78515625\n",
      "Iteration 51260 Training loss 0.015446563251316547 Validation loss 0.020723573863506317 Accuracy 0.787109375\n",
      "Iteration 51270 Training loss 0.01559413131326437 Validation loss 0.0216065663844347 Accuracy 0.77783203125\n",
      "Iteration 51280 Training loss 0.015037192031741142 Validation loss 0.02128700539469719 Accuracy 0.78125\n",
      "Iteration 51290 Training loss 0.013839301653206348 Validation loss 0.020936772227287292 Accuracy 0.7841796875\n",
      "Iteration 51300 Training loss 0.0156417116522789 Validation loss 0.020731156691908836 Accuracy 0.7880859375\n",
      "Iteration 51310 Training loss 0.01589534804224968 Validation loss 0.02121696248650551 Accuracy 0.7822265625\n",
      "Iteration 51320 Training loss 0.015624269843101501 Validation loss 0.021120328456163406 Accuracy 0.78271484375\n",
      "Iteration 51330 Training loss 0.016335781663656235 Validation loss 0.02125813439488411 Accuracy 0.78173828125\n",
      "Iteration 51340 Training loss 0.014211438596248627 Validation loss 0.021472755819559097 Accuracy 0.77978515625\n",
      "Iteration 51350 Training loss 0.014532304368913174 Validation loss 0.020937323570251465 Accuracy 0.7841796875\n",
      "Iteration 51360 Training loss 0.015203389339148998 Validation loss 0.021285565569996834 Accuracy 0.78173828125\n",
      "Iteration 51370 Training loss 0.016559552401304245 Validation loss 0.020960776135325432 Accuracy 0.78564453125\n",
      "Iteration 51380 Training loss 0.016196083277463913 Validation loss 0.021523749455809593 Accuracy 0.77783203125\n",
      "Iteration 51390 Training loss 0.014435664750635624 Validation loss 0.02102532982826233 Accuracy 0.78369140625\n",
      "Iteration 51400 Training loss 0.01519493106752634 Validation loss 0.02079584449529648 Accuracy 0.7861328125\n",
      "Iteration 51410 Training loss 0.014109339565038681 Validation loss 0.020779399201273918 Accuracy 0.7861328125\n",
      "Iteration 51420 Training loss 0.01612391509115696 Validation loss 0.020937640219926834 Accuracy 0.78466796875\n",
      "Iteration 51430 Training loss 0.01498495601117611 Validation loss 0.021104564890265465 Accuracy 0.7822265625\n",
      "Iteration 51440 Training loss 0.017888197675347328 Validation loss 0.02199028804898262 Accuracy 0.77392578125\n",
      "Iteration 51450 Training loss 0.014567806385457516 Validation loss 0.021032623946666718 Accuracy 0.78271484375\n",
      "Iteration 51460 Training loss 0.018311694264411926 Validation loss 0.020908324047923088 Accuracy 0.78369140625\n",
      "Iteration 51470 Training loss 0.015367168001830578 Validation loss 0.020902596414089203 Accuracy 0.7841796875\n",
      "Iteration 51480 Training loss 0.018361834809184074 Validation loss 0.020732266828417778 Accuracy 0.7861328125\n",
      "Iteration 51490 Training loss 0.01479603536427021 Validation loss 0.020943017676472664 Accuracy 0.78466796875\n",
      "Iteration 51500 Training loss 0.014113489538431168 Validation loss 0.020979149267077446 Accuracy 0.78369140625\n",
      "Iteration 51510 Training loss 0.016421053558588028 Validation loss 0.021032074466347694 Accuracy 0.78271484375\n",
      "Iteration 51520 Training loss 0.018235163763165474 Validation loss 0.021473504602909088 Accuracy 0.7783203125\n",
      "Iteration 51530 Training loss 0.014090510085225105 Validation loss 0.02093283087015152 Accuracy 0.78369140625\n",
      "Iteration 51540 Training loss 0.01603209227323532 Validation loss 0.02101590298116207 Accuracy 0.783203125\n",
      "Iteration 51550 Training loss 0.016291052103042603 Validation loss 0.021262187510728836 Accuracy 0.7822265625\n",
      "Iteration 51560 Training loss 0.0129553796723485 Validation loss 0.0210114773362875 Accuracy 0.7841796875\n",
      "Iteration 51570 Training loss 0.014462885446846485 Validation loss 0.020977793261408806 Accuracy 0.78466796875\n",
      "Iteration 51580 Training loss 0.014650237746536732 Validation loss 0.021100668236613274 Accuracy 0.783203125\n",
      "Iteration 51590 Training loss 0.01656924933195114 Validation loss 0.021072950214147568 Accuracy 0.78271484375\n",
      "Iteration 51600 Training loss 0.016120819374918938 Validation loss 0.020751623436808586 Accuracy 0.7861328125\n",
      "Iteration 51610 Training loss 0.014969482086598873 Validation loss 0.02099562995135784 Accuracy 0.78369140625\n",
      "Iteration 51620 Training loss 0.015252631157636642 Validation loss 0.021086836233735085 Accuracy 0.783203125\n",
      "Iteration 51630 Training loss 0.012510888278484344 Validation loss 0.020770670846104622 Accuracy 0.78564453125\n",
      "Iteration 51640 Training loss 0.016040122136473656 Validation loss 0.021425288170576096 Accuracy 0.77978515625\n",
      "Iteration 51650 Training loss 0.016563018783926964 Validation loss 0.020815301686525345 Accuracy 0.78564453125\n",
      "Iteration 51660 Training loss 0.014247515238821507 Validation loss 0.020974066108465195 Accuracy 0.78466796875\n",
      "Iteration 51670 Training loss 0.015861520543694496 Validation loss 0.021360991522669792 Accuracy 0.779296875\n",
      "Iteration 51680 Training loss 0.0137136559933424 Validation loss 0.02097906731069088 Accuracy 0.78369140625\n",
      "Iteration 51690 Training loss 0.013746795244514942 Validation loss 0.020800387486815453 Accuracy 0.78564453125\n",
      "Iteration 51700 Training loss 0.014891873113811016 Validation loss 0.02055349014699459 Accuracy 0.7880859375\n",
      "Iteration 51710 Training loss 0.01563754677772522 Validation loss 0.020909691229462624 Accuracy 0.7861328125\n",
      "Iteration 51720 Training loss 0.01344618946313858 Validation loss 0.020711787045001984 Accuracy 0.7880859375\n",
      "Iteration 51730 Training loss 0.017870206385850906 Validation loss 0.020984802395105362 Accuracy 0.78369140625\n",
      "Iteration 51740 Training loss 0.012765992432832718 Validation loss 0.020726390182971954 Accuracy 0.787109375\n",
      "Iteration 51750 Training loss 0.01456062775105238 Validation loss 0.021293964236974716 Accuracy 0.7802734375\n",
      "Iteration 51760 Training loss 0.014303743839263916 Validation loss 0.021362992003560066 Accuracy 0.7802734375\n",
      "Iteration 51770 Training loss 0.01554195862263441 Validation loss 0.020885692909359932 Accuracy 0.78564453125\n",
      "Iteration 51780 Training loss 0.01409925427287817 Validation loss 0.021501833572983742 Accuracy 0.77880859375\n",
      "Iteration 51790 Training loss 0.0161576084792614 Validation loss 0.020931439474225044 Accuracy 0.78466796875\n",
      "Iteration 51800 Training loss 0.013555784709751606 Validation loss 0.020856380462646484 Accuracy 0.78564453125\n",
      "Iteration 51810 Training loss 0.014961274340748787 Validation loss 0.020999465137720108 Accuracy 0.78466796875\n",
      "Iteration 51820 Training loss 0.014967511408030987 Validation loss 0.021322699263691902 Accuracy 0.78076171875\n",
      "Iteration 51830 Training loss 0.01286394614726305 Validation loss 0.021082421764731407 Accuracy 0.78271484375\n",
      "Iteration 51840 Training loss 0.015186584554612637 Validation loss 0.020928187295794487 Accuracy 0.7841796875\n",
      "Iteration 51850 Training loss 0.015546147711575031 Validation loss 0.02101580984890461 Accuracy 0.783203125\n",
      "Iteration 51860 Training loss 0.014874346554279327 Validation loss 0.020600127056241035 Accuracy 0.78759765625\n",
      "Iteration 51870 Training loss 0.017584241926670074 Validation loss 0.02109456993639469 Accuracy 0.7822265625\n",
      "Iteration 51880 Training loss 0.01495443470776081 Validation loss 0.020662611350417137 Accuracy 0.787109375\n",
      "Iteration 51890 Training loss 0.014030477963387966 Validation loss 0.02091946266591549 Accuracy 0.783203125\n",
      "Iteration 51900 Training loss 0.013457353226840496 Validation loss 0.020745960995554924 Accuracy 0.78564453125\n",
      "Iteration 51910 Training loss 0.014017285779118538 Validation loss 0.02087293192744255 Accuracy 0.78515625\n",
      "Iteration 51920 Training loss 0.015201334841549397 Validation loss 0.020845763385295868 Accuracy 0.7841796875\n",
      "Iteration 51930 Training loss 0.016138343140482903 Validation loss 0.020711615681648254 Accuracy 0.787109375\n",
      "Iteration 51940 Training loss 0.01494376827031374 Validation loss 0.020694324746727943 Accuracy 0.78759765625\n",
      "Iteration 51950 Training loss 0.017487023025751114 Validation loss 0.0212431438267231 Accuracy 0.7822265625\n",
      "Iteration 51960 Training loss 0.016588732600212097 Validation loss 0.02115844190120697 Accuracy 0.78271484375\n",
      "Iteration 51970 Training loss 0.015386845916509628 Validation loss 0.020979169756174088 Accuracy 0.78466796875\n",
      "Iteration 51980 Training loss 0.015687135979533195 Validation loss 0.02109157107770443 Accuracy 0.78271484375\n",
      "Iteration 51990 Training loss 0.014816222712397575 Validation loss 0.02063659392297268 Accuracy 0.78857421875\n",
      "Iteration 52000 Training loss 0.01441754400730133 Validation loss 0.021578630432486534 Accuracy 0.77734375\n",
      "Iteration 52010 Training loss 0.015753045678138733 Validation loss 0.020420286804437637 Accuracy 0.7900390625\n",
      "Iteration 52020 Training loss 0.016351591795682907 Validation loss 0.021471478044986725 Accuracy 0.779296875\n",
      "Iteration 52030 Training loss 0.014369524084031582 Validation loss 0.021208537742495537 Accuracy 0.78076171875\n",
      "Iteration 52040 Training loss 0.01384319644421339 Validation loss 0.020705534145236015 Accuracy 0.78662109375\n",
      "Iteration 52050 Training loss 0.016262134537100792 Validation loss 0.02090020477771759 Accuracy 0.78564453125\n",
      "Iteration 52060 Training loss 0.01339848805218935 Validation loss 0.021277936175465584 Accuracy 0.7802734375\n",
      "Iteration 52070 Training loss 0.01552141085267067 Validation loss 0.02094578556716442 Accuracy 0.7841796875\n",
      "Iteration 52080 Training loss 0.01311128493398428 Validation loss 0.021363845095038414 Accuracy 0.7802734375\n",
      "Iteration 52090 Training loss 0.015427461825311184 Validation loss 0.02104623056948185 Accuracy 0.7841796875\n",
      "Iteration 52100 Training loss 0.014279251918196678 Validation loss 0.020891712978482246 Accuracy 0.78564453125\n",
      "Iteration 52110 Training loss 0.014455001801252365 Validation loss 0.020833168178796768 Accuracy 0.78515625\n",
      "Iteration 52120 Training loss 0.011522656306624413 Validation loss 0.02070835418999195 Accuracy 0.787109375\n",
      "Iteration 52130 Training loss 0.014168483205139637 Validation loss 0.02168651856482029 Accuracy 0.77685546875\n",
      "Iteration 52140 Training loss 0.01680028811097145 Validation loss 0.02087210863828659 Accuracy 0.78515625\n",
      "Iteration 52150 Training loss 0.016824958845973015 Validation loss 0.021227402612566948 Accuracy 0.7822265625\n",
      "Iteration 52160 Training loss 0.016258561983704567 Validation loss 0.020887466147542 Accuracy 0.7861328125\n",
      "Iteration 52170 Training loss 0.014583976939320564 Validation loss 0.021007834002375603 Accuracy 0.78369140625\n",
      "Iteration 52180 Training loss 0.018206682056188583 Validation loss 0.021143820136785507 Accuracy 0.78369140625\n",
      "Iteration 52190 Training loss 0.01734909601509571 Validation loss 0.021034635603427887 Accuracy 0.78369140625\n",
      "Iteration 52200 Training loss 0.014909191988408566 Validation loss 0.021017197519540787 Accuracy 0.78369140625\n",
      "Iteration 52210 Training loss 0.014978259801864624 Validation loss 0.021279292181134224 Accuracy 0.78076171875\n",
      "Iteration 52220 Training loss 0.015436709858477116 Validation loss 0.021097352728247643 Accuracy 0.783203125\n",
      "Iteration 52230 Training loss 0.018260449171066284 Validation loss 0.02113424427807331 Accuracy 0.78173828125\n",
      "Iteration 52240 Training loss 0.014944111928343773 Validation loss 0.020937899127602577 Accuracy 0.783203125\n",
      "Iteration 52250 Training loss 0.01625128835439682 Validation loss 0.020921049639582634 Accuracy 0.78369140625\n",
      "Iteration 52260 Training loss 0.014321488328278065 Validation loss 0.020770058035850525 Accuracy 0.7861328125\n",
      "Iteration 52270 Training loss 0.013232325203716755 Validation loss 0.0214571300894022 Accuracy 0.7783203125\n",
      "Iteration 52280 Training loss 0.015455062501132488 Validation loss 0.02085377462208271 Accuracy 0.78564453125\n",
      "Iteration 52290 Training loss 0.01492492388933897 Validation loss 0.020910272374749184 Accuracy 0.78515625\n",
      "Iteration 52300 Training loss 0.015314146876335144 Validation loss 0.02110297791659832 Accuracy 0.7822265625\n",
      "Iteration 52310 Training loss 0.01403891947120428 Validation loss 0.021389514207839966 Accuracy 0.7802734375\n",
      "Iteration 52320 Training loss 0.01527436263859272 Validation loss 0.02148074470460415 Accuracy 0.779296875\n",
      "Iteration 52330 Training loss 0.01415125373750925 Validation loss 0.021014153957366943 Accuracy 0.7841796875\n",
      "Iteration 52340 Training loss 0.012863202020525932 Validation loss 0.020997771993279457 Accuracy 0.7841796875\n",
      "Iteration 52350 Training loss 0.015599365346133709 Validation loss 0.02072613686323166 Accuracy 0.78759765625\n",
      "Iteration 52360 Training loss 0.012629050761461258 Validation loss 0.020934291183948517 Accuracy 0.78466796875\n",
      "Iteration 52370 Training loss 0.01767435111105442 Validation loss 0.020987365394830704 Accuracy 0.78369140625\n",
      "Iteration 52380 Training loss 0.011949644424021244 Validation loss 0.020800098776817322 Accuracy 0.78564453125\n",
      "Iteration 52390 Training loss 0.018199509009718895 Validation loss 0.0208310354501009 Accuracy 0.78466796875\n",
      "Iteration 52400 Training loss 0.015861961990594864 Validation loss 0.020911242812871933 Accuracy 0.78466796875\n",
      "Iteration 52410 Training loss 0.012223365716636181 Validation loss 0.020774275064468384 Accuracy 0.78515625\n",
      "Iteration 52420 Training loss 0.014430707320570946 Validation loss 0.020700253546237946 Accuracy 0.7861328125\n",
      "Iteration 52430 Training loss 0.014327972196042538 Validation loss 0.020917806774377823 Accuracy 0.7841796875\n",
      "Iteration 52440 Training loss 0.016112713143229485 Validation loss 0.020768573507666588 Accuracy 0.78564453125\n",
      "Iteration 52450 Training loss 0.015867268666625023 Validation loss 0.02091953344643116 Accuracy 0.78466796875\n",
      "Iteration 52460 Training loss 0.017572572454810143 Validation loss 0.021339545026421547 Accuracy 0.77978515625\n",
      "Iteration 52470 Training loss 0.015499599277973175 Validation loss 0.020957782864570618 Accuracy 0.78466796875\n",
      "Iteration 52480 Training loss 0.015567595139145851 Validation loss 0.02087727002799511 Accuracy 0.78466796875\n",
      "Iteration 52490 Training loss 0.015880364924669266 Validation loss 0.021090926602482796 Accuracy 0.783203125\n",
      "Iteration 52500 Training loss 0.015048001892864704 Validation loss 0.021036220714449883 Accuracy 0.783203125\n",
      "Iteration 52510 Training loss 0.017739901319146156 Validation loss 0.02098524570465088 Accuracy 0.7841796875\n",
      "Iteration 52520 Training loss 0.015399240888655186 Validation loss 0.02108660154044628 Accuracy 0.78271484375\n",
      "Iteration 52530 Training loss 0.01615402288734913 Validation loss 0.020875971764326096 Accuracy 0.78515625\n",
      "Iteration 52540 Training loss 0.0165493693202734 Validation loss 0.021292762830853462 Accuracy 0.7802734375\n",
      "Iteration 52550 Training loss 0.012891245074570179 Validation loss 0.02169632352888584 Accuracy 0.7783203125\n",
      "Iteration 52560 Training loss 0.015523750334978104 Validation loss 0.021179744973778725 Accuracy 0.78271484375\n",
      "Iteration 52570 Training loss 0.016254743561148643 Validation loss 0.02134792134165764 Accuracy 0.78076171875\n",
      "Iteration 52580 Training loss 0.015078471973538399 Validation loss 0.02095157653093338 Accuracy 0.78466796875\n",
      "Iteration 52590 Training loss 0.013711804524064064 Validation loss 0.020992498844861984 Accuracy 0.78466796875\n",
      "Iteration 52600 Training loss 0.01724887639284134 Validation loss 0.021443624049425125 Accuracy 0.7802734375\n",
      "Iteration 52610 Training loss 0.017310503870248795 Validation loss 0.021107668057084084 Accuracy 0.783203125\n",
      "Iteration 52620 Training loss 0.014019553549587727 Validation loss 0.021201593801379204 Accuracy 0.78271484375\n",
      "Iteration 52630 Training loss 0.014874214306473732 Validation loss 0.02118617668747902 Accuracy 0.78076171875\n",
      "Iteration 52640 Training loss 0.013241910375654697 Validation loss 0.02099287323653698 Accuracy 0.783203125\n",
      "Iteration 52650 Training loss 0.018994741141796112 Validation loss 0.02187039516866207 Accuracy 0.77490234375\n",
      "Iteration 52660 Training loss 0.014011097140610218 Validation loss 0.02129048854112625 Accuracy 0.78125\n",
      "Iteration 52670 Training loss 0.0156953614205122 Validation loss 0.021094832569360733 Accuracy 0.783203125\n",
      "Iteration 52680 Training loss 0.015806622803211212 Validation loss 0.020876776427030563 Accuracy 0.78369140625\n",
      "Iteration 52690 Training loss 0.01232383493334055 Validation loss 0.021327929571270943 Accuracy 0.78125\n",
      "Iteration 52700 Training loss 0.013705103658139706 Validation loss 0.021017994731664658 Accuracy 0.783203125\n",
      "Iteration 52710 Training loss 0.014324060641229153 Validation loss 0.021129218861460686 Accuracy 0.783203125\n",
      "Iteration 52720 Training loss 0.012927228584885597 Validation loss 0.02115032635629177 Accuracy 0.7822265625\n",
      "Iteration 52730 Training loss 0.013871408067643642 Validation loss 0.021296456456184387 Accuracy 0.7802734375\n",
      "Iteration 52740 Training loss 0.014772748574614525 Validation loss 0.02092893421649933 Accuracy 0.78369140625\n",
      "Iteration 52750 Training loss 0.015873311087489128 Validation loss 0.021067829802632332 Accuracy 0.783203125\n",
      "Iteration 52760 Training loss 0.014186261221766472 Validation loss 0.0208753515034914 Accuracy 0.78515625\n",
      "Iteration 52770 Training loss 0.01815495267510414 Validation loss 0.022770216688513756 Accuracy 0.76611328125\n",
      "Iteration 52780 Training loss 0.015369531698524952 Validation loss 0.021065710112452507 Accuracy 0.7822265625\n",
      "Iteration 52790 Training loss 0.014641105197370052 Validation loss 0.021268576383590698 Accuracy 0.7822265625\n",
      "Iteration 52800 Training loss 0.01600685529410839 Validation loss 0.020687486976385117 Accuracy 0.78662109375\n",
      "Iteration 52810 Training loss 0.014942053705453873 Validation loss 0.020943457260727882 Accuracy 0.783203125\n",
      "Iteration 52820 Training loss 0.01486208289861679 Validation loss 0.02108646370470524 Accuracy 0.78271484375\n",
      "Iteration 52830 Training loss 0.015310337767004967 Validation loss 0.021094970405101776 Accuracy 0.7822265625\n",
      "Iteration 52840 Training loss 0.016898073256015778 Validation loss 0.021378254517912865 Accuracy 0.77978515625\n",
      "Iteration 52850 Training loss 0.014719158411026001 Validation loss 0.021236946806311607 Accuracy 0.78173828125\n",
      "Iteration 52860 Training loss 0.017893031239509583 Validation loss 0.02127895876765251 Accuracy 0.78125\n",
      "Iteration 52870 Training loss 0.01639891229569912 Validation loss 0.021059375256299973 Accuracy 0.78271484375\n",
      "Iteration 52880 Training loss 0.013970939442515373 Validation loss 0.020831409841775894 Accuracy 0.78515625\n",
      "Iteration 52890 Training loss 0.016464632004499435 Validation loss 0.020925095304846764 Accuracy 0.7841796875\n",
      "Iteration 52900 Training loss 0.01809345930814743 Validation loss 0.021445482969284058 Accuracy 0.77978515625\n",
      "Iteration 52910 Training loss 0.01430425513535738 Validation loss 0.02105814404785633 Accuracy 0.783203125\n",
      "Iteration 52920 Training loss 0.016803530976176262 Validation loss 0.020922550931572914 Accuracy 0.78369140625\n",
      "Iteration 52930 Training loss 0.016476908698678017 Validation loss 0.021270643919706345 Accuracy 0.7802734375\n",
      "Iteration 52940 Training loss 0.014447870664298534 Validation loss 0.020866919308900833 Accuracy 0.78564453125\n",
      "Iteration 52950 Training loss 0.013740100897848606 Validation loss 0.020886601880192757 Accuracy 0.78515625\n",
      "Iteration 52960 Training loss 0.015543347224593163 Validation loss 0.021134240552783012 Accuracy 0.78173828125\n",
      "Iteration 52970 Training loss 0.01554798148572445 Validation loss 0.020882314071059227 Accuracy 0.78515625\n",
      "Iteration 52980 Training loss 0.015615968964993954 Validation loss 0.02079574577510357 Accuracy 0.78564453125\n",
      "Iteration 52990 Training loss 0.015382378362119198 Validation loss 0.020873215049505234 Accuracy 0.78515625\n",
      "Iteration 53000 Training loss 0.018255291506648064 Validation loss 0.020795824006199837 Accuracy 0.78564453125\n",
      "Iteration 53010 Training loss 0.013715926557779312 Validation loss 0.021036718040704727 Accuracy 0.783203125\n",
      "Iteration 53020 Training loss 0.013716326095163822 Validation loss 0.020970769226551056 Accuracy 0.78466796875\n",
      "Iteration 53030 Training loss 0.012119028717279434 Validation loss 0.021161528304219246 Accuracy 0.78271484375\n",
      "Iteration 53040 Training loss 0.017480339854955673 Validation loss 0.020894130691885948 Accuracy 0.7841796875\n",
      "Iteration 53050 Training loss 0.01630682684481144 Validation loss 0.021526750177145004 Accuracy 0.779296875\n",
      "Iteration 53060 Training loss 0.013316432945430279 Validation loss 0.020927373319864273 Accuracy 0.78369140625\n",
      "Iteration 53070 Training loss 0.01824844814836979 Validation loss 0.021868588402867317 Accuracy 0.77587890625\n",
      "Iteration 53080 Training loss 0.0158334169536829 Validation loss 0.021600214764475822 Accuracy 0.77880859375\n",
      "Iteration 53090 Training loss 0.01613856479525566 Validation loss 0.0211265180259943 Accuracy 0.7822265625\n",
      "Iteration 53100 Training loss 0.013365763239562511 Validation loss 0.021021397784352303 Accuracy 0.78271484375\n",
      "Iteration 53110 Training loss 0.014402111992239952 Validation loss 0.021065084263682365 Accuracy 0.78369140625\n",
      "Iteration 53120 Training loss 0.016325155273079872 Validation loss 0.02137717790901661 Accuracy 0.779296875\n",
      "Iteration 53130 Training loss 0.01667902059853077 Validation loss 0.021191051229834557 Accuracy 0.78173828125\n",
      "Iteration 53140 Training loss 0.016009876504540443 Validation loss 0.021129371598362923 Accuracy 0.78271484375\n",
      "Iteration 53150 Training loss 0.014649535529315472 Validation loss 0.02086104452610016 Accuracy 0.78466796875\n",
      "Iteration 53160 Training loss 0.0162504855543375 Validation loss 0.021070700138807297 Accuracy 0.78271484375\n",
      "Iteration 53170 Training loss 0.015559297986328602 Validation loss 0.02071000449359417 Accuracy 0.7861328125\n",
      "Iteration 53180 Training loss 0.016937345266342163 Validation loss 0.021323617547750473 Accuracy 0.77978515625\n",
      "Iteration 53190 Training loss 0.012991617433726788 Validation loss 0.021079257130622864 Accuracy 0.7822265625\n",
      "Iteration 53200 Training loss 0.014103284105658531 Validation loss 0.02144475094974041 Accuracy 0.7783203125\n",
      "Iteration 53210 Training loss 0.012882004491984844 Validation loss 0.020938510075211525 Accuracy 0.78564453125\n",
      "Iteration 53220 Training loss 0.015763895586133003 Validation loss 0.020949091762304306 Accuracy 0.78369140625\n",
      "Iteration 53230 Training loss 0.016475364565849304 Validation loss 0.021099984645843506 Accuracy 0.783203125\n",
      "Iteration 53240 Training loss 0.016172129660844803 Validation loss 0.021156396716833115 Accuracy 0.78125\n",
      "Iteration 53250 Training loss 0.014504335820674896 Validation loss 0.021257197484374046 Accuracy 0.7802734375\n",
      "Iteration 53260 Training loss 0.01653994992375374 Validation loss 0.02134658955037594 Accuracy 0.78076171875\n",
      "Iteration 53270 Training loss 0.012723107822239399 Validation loss 0.020859578624367714 Accuracy 0.7841796875\n",
      "Iteration 53280 Training loss 0.014184137806296349 Validation loss 0.020788168534636497 Accuracy 0.78466796875\n",
      "Iteration 53290 Training loss 0.016625557094812393 Validation loss 0.021785130724310875 Accuracy 0.77587890625\n",
      "Iteration 53300 Training loss 0.016894826665520668 Validation loss 0.021182294934988022 Accuracy 0.78173828125\n",
      "Iteration 53310 Training loss 0.01503019966185093 Validation loss 0.020925017073750496 Accuracy 0.78466796875\n",
      "Iteration 53320 Training loss 0.013057845644652843 Validation loss 0.021339813247323036 Accuracy 0.77978515625\n",
      "Iteration 53330 Training loss 0.01564723812043667 Validation loss 0.020927047356963158 Accuracy 0.78369140625\n",
      "Iteration 53340 Training loss 0.01426009088754654 Validation loss 0.020764101296663284 Accuracy 0.78662109375\n",
      "Iteration 53350 Training loss 0.016885602846741676 Validation loss 0.020904427394270897 Accuracy 0.78466796875\n",
      "Iteration 53360 Training loss 0.012264015153050423 Validation loss 0.02100917510688305 Accuracy 0.783203125\n",
      "Iteration 53370 Training loss 0.0154555793851614 Validation loss 0.020925672724843025 Accuracy 0.7841796875\n",
      "Iteration 53380 Training loss 0.015121584758162498 Validation loss 0.02093582972884178 Accuracy 0.78466796875\n",
      "Iteration 53390 Training loss 0.015519625507295132 Validation loss 0.02102828398346901 Accuracy 0.783203125\n",
      "Iteration 53400 Training loss 0.013034108094871044 Validation loss 0.021171286702156067 Accuracy 0.78173828125\n",
      "Iteration 53410 Training loss 0.016285408288240433 Validation loss 0.021821744740009308 Accuracy 0.77490234375\n",
      "Iteration 53420 Training loss 0.013514233753085136 Validation loss 0.02095426432788372 Accuracy 0.78369140625\n",
      "Iteration 53430 Training loss 0.013866735622286797 Validation loss 0.021114345639944077 Accuracy 0.78173828125\n",
      "Iteration 53440 Training loss 0.014198684133589268 Validation loss 0.021413588896393776 Accuracy 0.7802734375\n",
      "Iteration 53450 Training loss 0.014032597653567791 Validation loss 0.021104415878653526 Accuracy 0.7822265625\n",
      "Iteration 53460 Training loss 0.015126528218388557 Validation loss 0.020783305168151855 Accuracy 0.78564453125\n",
      "Iteration 53470 Training loss 0.013790123164653778 Validation loss 0.021073035895824432 Accuracy 0.7822265625\n",
      "Iteration 53480 Training loss 0.014914900064468384 Validation loss 0.020816948264837265 Accuracy 0.7861328125\n",
      "Iteration 53490 Training loss 0.01575779914855957 Validation loss 0.02079116366803646 Accuracy 0.7861328125\n",
      "Iteration 53500 Training loss 0.015329353511333466 Validation loss 0.020842943340539932 Accuracy 0.78564453125\n",
      "Iteration 53510 Training loss 0.014169102534651756 Validation loss 0.02121908590197563 Accuracy 0.78076171875\n",
      "Iteration 53520 Training loss 0.015086578205227852 Validation loss 0.02105650305747986 Accuracy 0.783203125\n",
      "Iteration 53530 Training loss 0.013062694109976292 Validation loss 0.021616889163851738 Accuracy 0.7783203125\n",
      "Iteration 53540 Training loss 0.016544977203011513 Validation loss 0.021234164014458656 Accuracy 0.78076171875\n",
      "Iteration 53550 Training loss 0.013393859378993511 Validation loss 0.020888861268758774 Accuracy 0.78466796875\n",
      "Iteration 53560 Training loss 0.013272351585328579 Validation loss 0.021111762151122093 Accuracy 0.78076171875\n",
      "Iteration 53570 Training loss 0.016118401661515236 Validation loss 0.02117936871945858 Accuracy 0.7822265625\n",
      "Iteration 53580 Training loss 0.014332903549075127 Validation loss 0.021300360560417175 Accuracy 0.78173828125\n",
      "Iteration 53590 Training loss 0.01584070548415184 Validation loss 0.02126791886985302 Accuracy 0.7822265625\n",
      "Iteration 53600 Training loss 0.015072540380060673 Validation loss 0.021185625344514847 Accuracy 0.78271484375\n",
      "Iteration 53610 Training loss 0.014410410076379776 Validation loss 0.021253999322652817 Accuracy 0.78076171875\n",
      "Iteration 53620 Training loss 0.015120628289878368 Validation loss 0.021356230601668358 Accuracy 0.7802734375\n",
      "Iteration 53630 Training loss 0.015220691449940205 Validation loss 0.02121969684958458 Accuracy 0.78173828125\n",
      "Iteration 53640 Training loss 0.0129644013941288 Validation loss 0.021198129281401634 Accuracy 0.7822265625\n",
      "Iteration 53650 Training loss 0.015680402517318726 Validation loss 0.0210677832365036 Accuracy 0.78271484375\n",
      "Iteration 53660 Training loss 0.01935753785073757 Validation loss 0.022478945553302765 Accuracy 0.76904296875\n",
      "Iteration 53670 Training loss 0.014228573068976402 Validation loss 0.020713305100798607 Accuracy 0.78564453125\n",
      "Iteration 53680 Training loss 0.014188231900334358 Validation loss 0.0212077759206295 Accuracy 0.78173828125\n",
      "Iteration 53690 Training loss 0.014958877116441727 Validation loss 0.02099207043647766 Accuracy 0.78369140625\n",
      "Iteration 53700 Training loss 0.013515048660337925 Validation loss 0.021032173186540604 Accuracy 0.783203125\n",
      "Iteration 53710 Training loss 0.015033798292279243 Validation loss 0.020887674763798714 Accuracy 0.7841796875\n",
      "Iteration 53720 Training loss 0.013763567432761192 Validation loss 0.020858915522694588 Accuracy 0.78466796875\n",
      "Iteration 53730 Training loss 0.015907304361462593 Validation loss 0.020841537043452263 Accuracy 0.78515625\n",
      "Iteration 53740 Training loss 0.014841606840491295 Validation loss 0.02080867625772953 Accuracy 0.78564453125\n",
      "Iteration 53750 Training loss 0.015653610229492188 Validation loss 0.02121911011636257 Accuracy 0.7822265625\n",
      "Iteration 53760 Training loss 0.01722976751625538 Validation loss 0.021133147180080414 Accuracy 0.7822265625\n",
      "Iteration 53770 Training loss 0.01644299365580082 Validation loss 0.02131936512887478 Accuracy 0.779296875\n",
      "Iteration 53780 Training loss 0.013804920017719269 Validation loss 0.020811770111322403 Accuracy 0.78564453125\n",
      "Iteration 53790 Training loss 0.01383266318589449 Validation loss 0.021117499098181725 Accuracy 0.78173828125\n",
      "Iteration 53800 Training loss 0.0165436789393425 Validation loss 0.020894315093755722 Accuracy 0.78466796875\n",
      "Iteration 53810 Training loss 0.015295056626200676 Validation loss 0.021308759227395058 Accuracy 0.7802734375\n",
      "Iteration 53820 Training loss 0.016735699027776718 Validation loss 0.021784722805023193 Accuracy 0.7744140625\n",
      "Iteration 53830 Training loss 0.01836097612977028 Validation loss 0.02140909619629383 Accuracy 0.7802734375\n",
      "Iteration 53840 Training loss 0.018737714737653732 Validation loss 0.022476455196738243 Accuracy 0.767578125\n",
      "Iteration 53850 Training loss 0.015551063232123852 Validation loss 0.021414335817098618 Accuracy 0.779296875\n",
      "Iteration 53860 Training loss 0.0161313246935606 Validation loss 0.02118522860109806 Accuracy 0.78076171875\n",
      "Iteration 53870 Training loss 0.013537672348320484 Validation loss 0.020856613293290138 Accuracy 0.78515625\n",
      "Iteration 53880 Training loss 0.01347357127815485 Validation loss 0.020938856527209282 Accuracy 0.7841796875\n",
      "Iteration 53890 Training loss 0.015985559672117233 Validation loss 0.020943593233823776 Accuracy 0.78466796875\n",
      "Iteration 53900 Training loss 0.013953812420368195 Validation loss 0.021471822634339333 Accuracy 0.77880859375\n",
      "Iteration 53910 Training loss 0.015284625813364983 Validation loss 0.020970962941646576 Accuracy 0.78369140625\n",
      "Iteration 53920 Training loss 0.014820439741015434 Validation loss 0.022099023684859276 Accuracy 0.7724609375\n",
      "Iteration 53930 Training loss 0.017582641914486885 Validation loss 0.021745488047599792 Accuracy 0.7763671875\n",
      "Iteration 53940 Training loss 0.014457682147622108 Validation loss 0.020852725952863693 Accuracy 0.78564453125\n",
      "Iteration 53950 Training loss 0.016263166442513466 Validation loss 0.02108120545744896 Accuracy 0.78271484375\n",
      "Iteration 53960 Training loss 0.01650429703295231 Validation loss 0.021159447729587555 Accuracy 0.7822265625\n",
      "Iteration 53970 Training loss 0.017356613650918007 Validation loss 0.02136952057480812 Accuracy 0.779296875\n",
      "Iteration 53980 Training loss 0.013662104494869709 Validation loss 0.020898304879665375 Accuracy 0.78466796875\n",
      "Iteration 53990 Training loss 0.015698624774813652 Validation loss 0.020853117108345032 Accuracy 0.78564453125\n",
      "Iteration 54000 Training loss 0.01713315024971962 Validation loss 0.020975282415747643 Accuracy 0.7841796875\n",
      "Iteration 54010 Training loss 0.016293879598379135 Validation loss 0.0216244924813509 Accuracy 0.77734375\n",
      "Iteration 54020 Training loss 0.016234122216701508 Validation loss 0.021402698010206223 Accuracy 0.77978515625\n",
      "Iteration 54030 Training loss 0.014953401871025562 Validation loss 0.02093028835952282 Accuracy 0.7841796875\n",
      "Iteration 54040 Training loss 0.016447193920612335 Validation loss 0.020968236029148102 Accuracy 0.78369140625\n",
      "Iteration 54050 Training loss 0.018041418865323067 Validation loss 0.021033188328146935 Accuracy 0.7822265625\n",
      "Iteration 54060 Training loss 0.012820577248930931 Validation loss 0.02093711867928505 Accuracy 0.78369140625\n",
      "Iteration 54070 Training loss 0.013043937273323536 Validation loss 0.020857401192188263 Accuracy 0.78515625\n",
      "Iteration 54080 Training loss 0.018756169825792313 Validation loss 0.021806467324495316 Accuracy 0.77587890625\n",
      "Iteration 54090 Training loss 0.016214977949857712 Validation loss 0.020823366940021515 Accuracy 0.78564453125\n",
      "Iteration 54100 Training loss 0.014312037266790867 Validation loss 0.021210042759776115 Accuracy 0.78076171875\n",
      "Iteration 54110 Training loss 0.016434432938694954 Validation loss 0.020894179120659828 Accuracy 0.7841796875\n",
      "Iteration 54120 Training loss 0.014097119681537151 Validation loss 0.020988576114177704 Accuracy 0.78369140625\n",
      "Iteration 54130 Training loss 0.015615344978868961 Validation loss 0.020900709554553032 Accuracy 0.7841796875\n",
      "Iteration 54140 Training loss 0.016526227816939354 Validation loss 0.02084026299417019 Accuracy 0.78515625\n",
      "Iteration 54150 Training loss 0.01326307188719511 Validation loss 0.020821180194616318 Accuracy 0.78466796875\n",
      "Iteration 54160 Training loss 0.016823405399918556 Validation loss 0.021789535880088806 Accuracy 0.7744140625\n",
      "Iteration 54170 Training loss 0.01091037504374981 Validation loss 0.02088838815689087 Accuracy 0.78369140625\n",
      "Iteration 54180 Training loss 0.014537693932652473 Validation loss 0.021638251841068268 Accuracy 0.77734375\n",
      "Iteration 54190 Training loss 0.015106030739843845 Validation loss 0.021173568442463875 Accuracy 0.78173828125\n",
      "Iteration 54200 Training loss 0.015542159788310528 Validation loss 0.02105030044913292 Accuracy 0.783203125\n",
      "Iteration 54210 Training loss 0.014131372794508934 Validation loss 0.02079833671450615 Accuracy 0.7861328125\n",
      "Iteration 54220 Training loss 0.02021740935742855 Validation loss 0.020983871072530746 Accuracy 0.7841796875\n",
      "Iteration 54230 Training loss 0.015706654638051987 Validation loss 0.021303635090589523 Accuracy 0.78076171875\n",
      "Iteration 54240 Training loss 0.01498242188245058 Validation loss 0.021067917346954346 Accuracy 0.78271484375\n",
      "Iteration 54250 Training loss 0.012987220659852028 Validation loss 0.02091817744076252 Accuracy 0.78515625\n",
      "Iteration 54260 Training loss 0.014633175916969776 Validation loss 0.02085566334426403 Accuracy 0.78564453125\n",
      "Iteration 54270 Training loss 0.016307957470417023 Validation loss 0.02107575163245201 Accuracy 0.78271484375\n",
      "Iteration 54280 Training loss 0.01538370456546545 Validation loss 0.0208441149443388 Accuracy 0.7841796875\n",
      "Iteration 54290 Training loss 0.015207056887447834 Validation loss 0.021573476493358612 Accuracy 0.7783203125\n",
      "Iteration 54300 Training loss 0.015213904902338982 Validation loss 0.020965229719877243 Accuracy 0.7841796875\n",
      "Iteration 54310 Training loss 0.016117068007588387 Validation loss 0.020925065502524376 Accuracy 0.78466796875\n",
      "Iteration 54320 Training loss 0.017407430335879326 Validation loss 0.020893817767500877 Accuracy 0.78515625\n",
      "Iteration 54330 Training loss 0.015057314187288284 Validation loss 0.021447425708174706 Accuracy 0.779296875\n",
      "Iteration 54340 Training loss 0.01482983399182558 Validation loss 0.02090953104197979 Accuracy 0.78466796875\n",
      "Iteration 54350 Training loss 0.017262214794754982 Validation loss 0.021682851016521454 Accuracy 0.77685546875\n",
      "Iteration 54360 Training loss 0.012018661014735699 Validation loss 0.020772067829966545 Accuracy 0.7861328125\n",
      "Iteration 54370 Training loss 0.013344649225473404 Validation loss 0.020717129111289978 Accuracy 0.78564453125\n",
      "Iteration 54380 Training loss 0.014091460965573788 Validation loss 0.02151716686785221 Accuracy 0.77783203125\n",
      "Iteration 54390 Training loss 0.016459353268146515 Validation loss 0.02103150263428688 Accuracy 0.783203125\n",
      "Iteration 54400 Training loss 0.014706730842590332 Validation loss 0.021048307418823242 Accuracy 0.78173828125\n",
      "Iteration 54410 Training loss 0.01721741259098053 Validation loss 0.02124137245118618 Accuracy 0.78125\n",
      "Iteration 54420 Training loss 0.015206214971840382 Validation loss 0.02085087075829506 Accuracy 0.78515625\n",
      "Iteration 54430 Training loss 0.01617121882736683 Validation loss 0.020964831113815308 Accuracy 0.7841796875\n",
      "Iteration 54440 Training loss 0.014349753968417645 Validation loss 0.021041283383965492 Accuracy 0.78369140625\n",
      "Iteration 54450 Training loss 0.01491808332502842 Validation loss 0.020872123539447784 Accuracy 0.78369140625\n",
      "Iteration 54460 Training loss 0.016221780329942703 Validation loss 0.021760158240795135 Accuracy 0.77734375\n",
      "Iteration 54470 Training loss 0.01595146395266056 Validation loss 0.02100563421845436 Accuracy 0.783203125\n",
      "Iteration 54480 Training loss 0.020409053191542625 Validation loss 0.02220616489648819 Accuracy 0.77197265625\n",
      "Iteration 54490 Training loss 0.015072745271027088 Validation loss 0.02122359722852707 Accuracy 0.78173828125\n",
      "Iteration 54500 Training loss 0.015063783153891563 Validation loss 0.020935341715812683 Accuracy 0.78369140625\n",
      "Iteration 54510 Training loss 0.01515162643045187 Validation loss 0.021167680621147156 Accuracy 0.78173828125\n",
      "Iteration 54520 Training loss 0.014883458614349365 Validation loss 0.021012142300605774 Accuracy 0.783203125\n",
      "Iteration 54530 Training loss 0.0118333725258708 Validation loss 0.021021835505962372 Accuracy 0.7822265625\n",
      "Iteration 54540 Training loss 0.014397368766367435 Validation loss 0.021187443286180496 Accuracy 0.78173828125\n",
      "Iteration 54550 Training loss 0.017005344852805138 Validation loss 0.020843930542469025 Accuracy 0.78466796875\n",
      "Iteration 54560 Training loss 0.013672810979187489 Validation loss 0.021150795742869377 Accuracy 0.78173828125\n",
      "Iteration 54570 Training loss 0.013597129844129086 Validation loss 0.020924365147948265 Accuracy 0.78466796875\n",
      "Iteration 54580 Training loss 0.014477997086942196 Validation loss 0.0212403554469347 Accuracy 0.78173828125\n",
      "Iteration 54590 Training loss 0.013604840263724327 Validation loss 0.02096025086939335 Accuracy 0.7841796875\n",
      "Iteration 54600 Training loss 0.01387288048863411 Validation loss 0.021170614287257195 Accuracy 0.7822265625\n",
      "Iteration 54610 Training loss 0.014132113195955753 Validation loss 0.02116292342543602 Accuracy 0.7822265625\n",
      "Iteration 54620 Training loss 0.015221567824482918 Validation loss 0.02081688866019249 Accuracy 0.78564453125\n",
      "Iteration 54630 Training loss 0.014177225530147552 Validation loss 0.02097378857433796 Accuracy 0.78369140625\n",
      "Iteration 54640 Training loss 0.013479309156537056 Validation loss 0.020914433524012566 Accuracy 0.78466796875\n",
      "Iteration 54650 Training loss 0.016034964472055435 Validation loss 0.021085092797875404 Accuracy 0.78173828125\n",
      "Iteration 54660 Training loss 0.014817115850746632 Validation loss 0.02078811265528202 Accuracy 0.78564453125\n",
      "Iteration 54670 Training loss 0.013752554543316364 Validation loss 0.020942123606801033 Accuracy 0.78369140625\n",
      "Iteration 54680 Training loss 0.018242675811052322 Validation loss 0.022540383040905 Accuracy 0.76806640625\n",
      "Iteration 54690 Training loss 0.015800822526216507 Validation loss 0.021002382040023804 Accuracy 0.783203125\n",
      "Iteration 54700 Training loss 0.015181739814579487 Validation loss 0.02124655619263649 Accuracy 0.7822265625\n",
      "Iteration 54710 Training loss 0.015110286884009838 Validation loss 0.02106849104166031 Accuracy 0.783203125\n",
      "Iteration 54720 Training loss 0.016034046187996864 Validation loss 0.021166380494832993 Accuracy 0.78271484375\n",
      "Iteration 54730 Training loss 0.013815268874168396 Validation loss 0.020771127194166183 Accuracy 0.7861328125\n",
      "Iteration 54740 Training loss 0.016173433512449265 Validation loss 0.020765667781233788 Accuracy 0.78662109375\n",
      "Iteration 54750 Training loss 0.014117655344307423 Validation loss 0.02082797884941101 Accuracy 0.78515625\n",
      "Iteration 54760 Training loss 0.015687022358179092 Validation loss 0.02078862302005291 Accuracy 0.78662109375\n",
      "Iteration 54770 Training loss 0.011704706586897373 Validation loss 0.020786596462130547 Accuracy 0.78515625\n",
      "Iteration 54780 Training loss 0.015267965383827686 Validation loss 0.020801696926355362 Accuracy 0.78515625\n",
      "Iteration 54790 Training loss 0.01620221510529518 Validation loss 0.02095441147685051 Accuracy 0.7841796875\n",
      "Iteration 54800 Training loss 0.012619473971426487 Validation loss 0.02089507505297661 Accuracy 0.78466796875\n",
      "Iteration 54810 Training loss 0.01302232127636671 Validation loss 0.02100784331560135 Accuracy 0.78369140625\n",
      "Iteration 54820 Training loss 0.014959197491407394 Validation loss 0.02171538956463337 Accuracy 0.77783203125\n",
      "Iteration 54830 Training loss 0.014631013385951519 Validation loss 0.02111554704606533 Accuracy 0.78271484375\n",
      "Iteration 54840 Training loss 0.019971506670117378 Validation loss 0.02278219908475876 Accuracy 0.76416015625\n",
      "Iteration 54850 Training loss 0.011498074978590012 Validation loss 0.020973393693566322 Accuracy 0.78369140625\n",
      "Iteration 54860 Training loss 0.015074984170496464 Validation loss 0.02124730683863163 Accuracy 0.78125\n",
      "Iteration 54870 Training loss 0.013429030776023865 Validation loss 0.020958861336112022 Accuracy 0.78369140625\n",
      "Iteration 54880 Training loss 0.016357187181711197 Validation loss 0.02104778029024601 Accuracy 0.7841796875\n",
      "Iteration 54890 Training loss 0.013952885754406452 Validation loss 0.020998744294047356 Accuracy 0.7841796875\n",
      "Iteration 54900 Training loss 0.015212939120829105 Validation loss 0.020848939195275307 Accuracy 0.78466796875\n",
      "Iteration 54910 Training loss 0.014845435507595539 Validation loss 0.02075848914682865 Accuracy 0.78515625\n",
      "Iteration 54920 Training loss 0.016782086342573166 Validation loss 0.02107199653983116 Accuracy 0.78173828125\n",
      "Iteration 54930 Training loss 0.014672767370939255 Validation loss 0.021020663902163506 Accuracy 0.78369140625\n",
      "Iteration 54940 Training loss 0.014402159489691257 Validation loss 0.020932894200086594 Accuracy 0.7841796875\n",
      "Iteration 54950 Training loss 0.016786644235253334 Validation loss 0.020839549601078033 Accuracy 0.78466796875\n",
      "Iteration 54960 Training loss 0.015010859817266464 Validation loss 0.021148793399333954 Accuracy 0.78125\n",
      "Iteration 54970 Training loss 0.013667169958353043 Validation loss 0.021121881902217865 Accuracy 0.78173828125\n",
      "Iteration 54980 Training loss 0.01617133617401123 Validation loss 0.02083313651382923 Accuracy 0.78466796875\n",
      "Iteration 54990 Training loss 0.014460721053183079 Validation loss 0.021169867366552353 Accuracy 0.78125\n",
      "Iteration 55000 Training loss 0.016982244327664375 Validation loss 0.021095722913742065 Accuracy 0.783203125\n",
      "Iteration 55010 Training loss 0.012965241447091103 Validation loss 0.022158121690154076 Accuracy 0.77099609375\n",
      "Iteration 55020 Training loss 0.014337745495140553 Validation loss 0.02096704952418804 Accuracy 0.78466796875\n",
      "Iteration 55030 Training loss 0.01224346924573183 Validation loss 0.021074824035167694 Accuracy 0.78271484375\n",
      "Iteration 55040 Training loss 0.015548585914075375 Validation loss 0.021275745704770088 Accuracy 0.78125\n",
      "Iteration 55050 Training loss 0.013773309998214245 Validation loss 0.020969366654753685 Accuracy 0.7841796875\n",
      "Iteration 55060 Training loss 0.012609130702912807 Validation loss 0.02118889056146145 Accuracy 0.783203125\n",
      "Iteration 55070 Training loss 0.014008859172463417 Validation loss 0.021318158134818077 Accuracy 0.78125\n",
      "Iteration 55080 Training loss 0.013984777964651585 Validation loss 0.02123906835913658 Accuracy 0.78173828125\n",
      "Iteration 55090 Training loss 0.01640530303120613 Validation loss 0.020863525569438934 Accuracy 0.78466796875\n",
      "Iteration 55100 Training loss 0.015186268836259842 Validation loss 0.021361632272601128 Accuracy 0.7802734375\n",
      "Iteration 55110 Training loss 0.0142386294901371 Validation loss 0.020945148542523384 Accuracy 0.78466796875\n",
      "Iteration 55120 Training loss 0.014167515560984612 Validation loss 0.0209064744412899 Accuracy 0.7841796875\n",
      "Iteration 55130 Training loss 0.011741576716303825 Validation loss 0.02098431997001171 Accuracy 0.7841796875\n",
      "Iteration 55140 Training loss 0.014505032449960709 Validation loss 0.020706547424197197 Accuracy 0.7861328125\n",
      "Iteration 55150 Training loss 0.014048566110432148 Validation loss 0.021227914839982986 Accuracy 0.78076171875\n",
      "Iteration 55160 Training loss 0.01316408533602953 Validation loss 0.021211808547377586 Accuracy 0.78076171875\n",
      "Iteration 55170 Training loss 0.015547937713563442 Validation loss 0.021112235262989998 Accuracy 0.78125\n",
      "Iteration 55180 Training loss 0.014116058126091957 Validation loss 0.021104682236909866 Accuracy 0.783203125\n",
      "Iteration 55190 Training loss 0.01433119922876358 Validation loss 0.020830966532230377 Accuracy 0.78515625\n",
      "Iteration 55200 Training loss 0.013433934189379215 Validation loss 0.020944656804203987 Accuracy 0.7841796875\n",
      "Iteration 55210 Training loss 0.015189153142273426 Validation loss 0.020787006244063377 Accuracy 0.78564453125\n",
      "Iteration 55220 Training loss 0.012698444537818432 Validation loss 0.021135590970516205 Accuracy 0.7822265625\n",
      "Iteration 55230 Training loss 0.014054611325263977 Validation loss 0.02078920044004917 Accuracy 0.78515625\n",
      "Iteration 55240 Training loss 0.013239359483122826 Validation loss 0.020784815773367882 Accuracy 0.78564453125\n",
      "Iteration 55250 Training loss 0.01442246325314045 Validation loss 0.020923856645822525 Accuracy 0.78369140625\n",
      "Iteration 55260 Training loss 0.013299385085701942 Validation loss 0.02068556658923626 Accuracy 0.7861328125\n",
      "Iteration 55270 Training loss 0.015159941278398037 Validation loss 0.021571841090917587 Accuracy 0.7783203125\n",
      "Iteration 55280 Training loss 0.014584402553737164 Validation loss 0.02076416276395321 Accuracy 0.78515625\n",
      "Iteration 55290 Training loss 0.013376239687204361 Validation loss 0.02099859155714512 Accuracy 0.78466796875\n",
      "Iteration 55300 Training loss 0.014934333972632885 Validation loss 0.021205931901931763 Accuracy 0.78173828125\n",
      "Iteration 55310 Training loss 0.01318021398037672 Validation loss 0.021051889285445213 Accuracy 0.783203125\n",
      "Iteration 55320 Training loss 0.015399055555462837 Validation loss 0.020870111882686615 Accuracy 0.78466796875\n",
      "Iteration 55330 Training loss 0.014536972157657146 Validation loss 0.021183354780077934 Accuracy 0.78125\n",
      "Iteration 55340 Training loss 0.016335196793079376 Validation loss 0.021191410720348358 Accuracy 0.7822265625\n",
      "Iteration 55350 Training loss 0.013959812931716442 Validation loss 0.021098734810948372 Accuracy 0.78271484375\n",
      "Iteration 55360 Training loss 0.015002972446382046 Validation loss 0.021025143563747406 Accuracy 0.7822265625\n",
      "Iteration 55370 Training loss 0.012448109686374664 Validation loss 0.021006355062127113 Accuracy 0.78271484375\n",
      "Iteration 55380 Training loss 0.01599087566137314 Validation loss 0.020628970116376877 Accuracy 0.787109375\n",
      "Iteration 55390 Training loss 0.015595382079482079 Validation loss 0.020625384524464607 Accuracy 0.78759765625\n",
      "Iteration 55400 Training loss 0.015380797907710075 Validation loss 0.020747223868966103 Accuracy 0.7861328125\n",
      "Iteration 55410 Training loss 0.01557900756597519 Validation loss 0.021232834085822105 Accuracy 0.78125\n",
      "Iteration 55420 Training loss 0.014077986590564251 Validation loss 0.02092195674777031 Accuracy 0.78369140625\n",
      "Iteration 55430 Training loss 0.015979023650288582 Validation loss 0.021286288276314735 Accuracy 0.78076171875\n",
      "Iteration 55440 Training loss 0.013424515724182129 Validation loss 0.021016737446188927 Accuracy 0.78369140625\n",
      "Iteration 55450 Training loss 0.013790277764201164 Validation loss 0.02155178226530552 Accuracy 0.7783203125\n",
      "Iteration 55460 Training loss 0.01707579754292965 Validation loss 0.021044619381427765 Accuracy 0.783203125\n",
      "Iteration 55470 Training loss 0.012122136540710926 Validation loss 0.020900428295135498 Accuracy 0.78466796875\n",
      "Iteration 55480 Training loss 0.016822218894958496 Validation loss 0.021039199084043503 Accuracy 0.783203125\n",
      "Iteration 55490 Training loss 0.014607287012040615 Validation loss 0.02136250026524067 Accuracy 0.77880859375\n",
      "Iteration 55500 Training loss 0.013873809948563576 Validation loss 0.02115345560014248 Accuracy 0.78173828125\n",
      "Iteration 55510 Training loss 0.0157270859926939 Validation loss 0.020608384162187576 Accuracy 0.787109375\n",
      "Iteration 55520 Training loss 0.01378294825553894 Validation loss 0.021180104464292526 Accuracy 0.7822265625\n",
      "Iteration 55530 Training loss 0.014167055487632751 Validation loss 0.02115611359477043 Accuracy 0.78125\n",
      "Iteration 55540 Training loss 0.013187507167458534 Validation loss 0.021111557260155678 Accuracy 0.78369140625\n",
      "Iteration 55550 Training loss 0.012283923104405403 Validation loss 0.02070094645023346 Accuracy 0.78564453125\n",
      "Iteration 55560 Training loss 0.01731737330555916 Validation loss 0.02064387872815132 Accuracy 0.78662109375\n",
      "Iteration 55570 Training loss 0.017855897545814514 Validation loss 0.02077011577785015 Accuracy 0.78564453125\n",
      "Iteration 55580 Training loss 0.014262979850172997 Validation loss 0.020902123302221298 Accuracy 0.7841796875\n",
      "Iteration 55590 Training loss 0.013400089927017689 Validation loss 0.021318748593330383 Accuracy 0.78076171875\n",
      "Iteration 55600 Training loss 0.015121355652809143 Validation loss 0.020774830132722855 Accuracy 0.78564453125\n",
      "Iteration 55610 Training loss 0.015637274831533432 Validation loss 0.021083012223243713 Accuracy 0.7822265625\n",
      "Iteration 55620 Training loss 0.013568353839218616 Validation loss 0.02150898054242134 Accuracy 0.77783203125\n",
      "Iteration 55630 Training loss 0.013406176120042801 Validation loss 0.02079937607049942 Accuracy 0.78515625\n",
      "Iteration 55640 Training loss 0.015215400606393814 Validation loss 0.020877845585346222 Accuracy 0.78466796875\n",
      "Iteration 55650 Training loss 0.015096118673682213 Validation loss 0.020732039585709572 Accuracy 0.78662109375\n",
      "Iteration 55660 Training loss 0.016580304130911827 Validation loss 0.020964061841368675 Accuracy 0.78466796875\n",
      "Iteration 55670 Training loss 0.014842224307358265 Validation loss 0.020857036113739014 Accuracy 0.78466796875\n",
      "Iteration 55680 Training loss 0.015352054499089718 Validation loss 0.0207695160061121 Accuracy 0.78515625\n",
      "Iteration 55690 Training loss 0.015976782888174057 Validation loss 0.020828742533922195 Accuracy 0.78515625\n",
      "Iteration 55700 Training loss 0.017029741778969765 Validation loss 0.02092360518872738 Accuracy 0.783203125\n",
      "Iteration 55710 Training loss 0.013879000209271908 Validation loss 0.020875737071037292 Accuracy 0.78564453125\n",
      "Iteration 55720 Training loss 0.016093341633677483 Validation loss 0.020893895998597145 Accuracy 0.7841796875\n",
      "Iteration 55730 Training loss 0.013502798974514008 Validation loss 0.021174000576138496 Accuracy 0.7822265625\n",
      "Iteration 55740 Training loss 0.013186869211494923 Validation loss 0.021016361191868782 Accuracy 0.783203125\n",
      "Iteration 55750 Training loss 0.017504308372735977 Validation loss 0.02107866108417511 Accuracy 0.783203125\n",
      "Iteration 55760 Training loss 0.013650491833686829 Validation loss 0.02106589265167713 Accuracy 0.783203125\n",
      "Iteration 55770 Training loss 0.015112195163965225 Validation loss 0.020634077489376068 Accuracy 0.78662109375\n",
      "Iteration 55780 Training loss 0.012861416675150394 Validation loss 0.020870797336101532 Accuracy 0.78515625\n",
      "Iteration 55790 Training loss 0.013585093431174755 Validation loss 0.021222379058599472 Accuracy 0.78076171875\n",
      "Iteration 55800 Training loss 0.015487389639019966 Validation loss 0.02162899821996689 Accuracy 0.77734375\n",
      "Iteration 55810 Training loss 0.014146247878670692 Validation loss 0.0213122870773077 Accuracy 0.78076171875\n",
      "Iteration 55820 Training loss 0.016185635700821877 Validation loss 0.02102731354534626 Accuracy 0.7841796875\n",
      "Iteration 55830 Training loss 0.013988555409014225 Validation loss 0.021218957379460335 Accuracy 0.7802734375\n",
      "Iteration 55840 Training loss 0.012440299615263939 Validation loss 0.020938530564308167 Accuracy 0.78369140625\n",
      "Iteration 55850 Training loss 0.014356286264955997 Validation loss 0.02082771621644497 Accuracy 0.78466796875\n",
      "Iteration 55860 Training loss 0.016475161537528038 Validation loss 0.02096165530383587 Accuracy 0.7841796875\n",
      "Iteration 55870 Training loss 0.016377951949834824 Validation loss 0.02076234295964241 Accuracy 0.7861328125\n",
      "Iteration 55880 Training loss 0.016095105558633804 Validation loss 0.020816149190068245 Accuracy 0.78564453125\n",
      "Iteration 55890 Training loss 0.014112647622823715 Validation loss 0.02075466327369213 Accuracy 0.78564453125\n",
      "Iteration 55900 Training loss 0.015329083427786827 Validation loss 0.021190565079450607 Accuracy 0.78271484375\n",
      "Iteration 55910 Training loss 0.0158417709171772 Validation loss 0.021196864545345306 Accuracy 0.78173828125\n",
      "Iteration 55920 Training loss 0.012892555445432663 Validation loss 0.02117009647190571 Accuracy 0.78125\n",
      "Iteration 55930 Training loss 0.015871522948145866 Validation loss 0.02096090279519558 Accuracy 0.7822265625\n",
      "Iteration 55940 Training loss 0.014629612676799297 Validation loss 0.02097400277853012 Accuracy 0.78466796875\n",
      "Iteration 55950 Training loss 0.014176960103213787 Validation loss 0.021312512457370758 Accuracy 0.78125\n",
      "Iteration 55960 Training loss 0.015673933550715446 Validation loss 0.021338017657399178 Accuracy 0.7802734375\n",
      "Iteration 55970 Training loss 0.013321352191269398 Validation loss 0.020936233922839165 Accuracy 0.7841796875\n",
      "Iteration 55980 Training loss 0.014636287465691566 Validation loss 0.021376075223088264 Accuracy 0.78076171875\n",
      "Iteration 55990 Training loss 0.015745600685477257 Validation loss 0.020898880437016487 Accuracy 0.7841796875\n",
      "Iteration 56000 Training loss 0.012427576817572117 Validation loss 0.02141771838068962 Accuracy 0.779296875\n",
      "Iteration 56010 Training loss 0.016432926058769226 Validation loss 0.02093661017715931 Accuracy 0.7841796875\n",
      "Iteration 56020 Training loss 0.01893051154911518 Validation loss 0.021111678332090378 Accuracy 0.78271484375\n",
      "Iteration 56030 Training loss 0.016703639179468155 Validation loss 0.020787987858057022 Accuracy 0.78564453125\n",
      "Iteration 56040 Training loss 0.014222712256014347 Validation loss 0.020821649581193924 Accuracy 0.78515625\n",
      "Iteration 56050 Training loss 0.01573139615356922 Validation loss 0.020750069990754128 Accuracy 0.7861328125\n",
      "Iteration 56060 Training loss 0.013726875185966492 Validation loss 0.021046200767159462 Accuracy 0.78271484375\n",
      "Iteration 56070 Training loss 0.01380774937570095 Validation loss 0.02120448835194111 Accuracy 0.78125\n",
      "Iteration 56080 Training loss 0.012720515951514244 Validation loss 0.02085617184638977 Accuracy 0.78515625\n",
      "Iteration 56090 Training loss 0.014326321892440319 Validation loss 0.020779700949788094 Accuracy 0.7861328125\n",
      "Iteration 56100 Training loss 0.01543632335960865 Validation loss 0.021223798394203186 Accuracy 0.78271484375\n",
      "Iteration 56110 Training loss 0.013657221570611 Validation loss 0.021727919578552246 Accuracy 0.77685546875\n",
      "Iteration 56120 Training loss 0.012566810473799706 Validation loss 0.021004673093557358 Accuracy 0.783203125\n",
      "Iteration 56130 Training loss 0.012781130149960518 Validation loss 0.020997226238250732 Accuracy 0.78271484375\n",
      "Iteration 56140 Training loss 0.01466647069901228 Validation loss 0.02094866707921028 Accuracy 0.78466796875\n",
      "Iteration 56150 Training loss 0.015635203570127487 Validation loss 0.02106146514415741 Accuracy 0.7822265625\n",
      "Iteration 56160 Training loss 0.015294775366783142 Validation loss 0.021089188754558563 Accuracy 0.78271484375\n",
      "Iteration 56170 Training loss 0.014088797383010387 Validation loss 0.02123216725885868 Accuracy 0.78076171875\n",
      "Iteration 56180 Training loss 0.011114791966974735 Validation loss 0.020948875695466995 Accuracy 0.78369140625\n",
      "Iteration 56190 Training loss 0.014669659547507763 Validation loss 0.021580826491117477 Accuracy 0.77734375\n",
      "Iteration 56200 Training loss 0.014233099296689034 Validation loss 0.02129640057682991 Accuracy 0.7802734375\n",
      "Iteration 56210 Training loss 0.018815021961927414 Validation loss 0.021009569987654686 Accuracy 0.78271484375\n",
      "Iteration 56220 Training loss 0.01591104082763195 Validation loss 0.021034585312008858 Accuracy 0.78271484375\n",
      "Iteration 56230 Training loss 0.012825200334191322 Validation loss 0.02107885479927063 Accuracy 0.78271484375\n",
      "Iteration 56240 Training loss 0.01414412073791027 Validation loss 0.020964378491044044 Accuracy 0.783203125\n",
      "Iteration 56250 Training loss 0.014305240474641323 Validation loss 0.021126732230186462 Accuracy 0.78173828125\n",
      "Iteration 56260 Training loss 0.012314941734075546 Validation loss 0.020877592265605927 Accuracy 0.7841796875\n",
      "Iteration 56270 Training loss 0.015728004276752472 Validation loss 0.021147239953279495 Accuracy 0.78173828125\n",
      "Iteration 56280 Training loss 0.015005935914814472 Validation loss 0.020989572629332542 Accuracy 0.78271484375\n",
      "Iteration 56290 Training loss 0.01493759173899889 Validation loss 0.020998042076826096 Accuracy 0.78271484375\n",
      "Iteration 56300 Training loss 0.013203910551965237 Validation loss 0.021049438044428825 Accuracy 0.78271484375\n",
      "Iteration 56310 Training loss 0.013158414512872696 Validation loss 0.021074658259749413 Accuracy 0.78271484375\n",
      "Iteration 56320 Training loss 0.01308754738420248 Validation loss 0.020878765732049942 Accuracy 0.7841796875\n",
      "Iteration 56330 Training loss 0.014204018749296665 Validation loss 0.020824555307626724 Accuracy 0.78515625\n",
      "Iteration 56340 Training loss 0.01691611483693123 Validation loss 0.020937679335474968 Accuracy 0.78369140625\n",
      "Iteration 56350 Training loss 0.015546468086540699 Validation loss 0.020693670958280563 Accuracy 0.78662109375\n",
      "Iteration 56360 Training loss 0.01301575731486082 Validation loss 0.02097892202436924 Accuracy 0.78466796875\n",
      "Iteration 56370 Training loss 0.014224477112293243 Validation loss 0.02085672877728939 Accuracy 0.78515625\n",
      "Iteration 56380 Training loss 0.0137045718729496 Validation loss 0.021504072472453117 Accuracy 0.7783203125\n",
      "Iteration 56390 Training loss 0.015634411945939064 Validation loss 0.021162040531635284 Accuracy 0.78125\n",
      "Iteration 56400 Training loss 0.014568048529326916 Validation loss 0.020995892584323883 Accuracy 0.78369140625\n",
      "Iteration 56410 Training loss 0.012389298528432846 Validation loss 0.02097345143556595 Accuracy 0.78466796875\n",
      "Iteration 56420 Training loss 0.012380467727780342 Validation loss 0.020912596955895424 Accuracy 0.78466796875\n",
      "Iteration 56430 Training loss 0.018227845430374146 Validation loss 0.021114135161042213 Accuracy 0.7822265625\n",
      "Iteration 56440 Training loss 0.0156719833612442 Validation loss 0.02087554708123207 Accuracy 0.78466796875\n",
      "Iteration 56450 Training loss 0.013581293635070324 Validation loss 0.020845511928200722 Accuracy 0.7841796875\n",
      "Iteration 56460 Training loss 0.017123399302363396 Validation loss 0.021012919023633003 Accuracy 0.7822265625\n",
      "Iteration 56470 Training loss 0.012354524806141853 Validation loss 0.0209497157484293 Accuracy 0.78271484375\n",
      "Iteration 56480 Training loss 0.015876559540629387 Validation loss 0.02127349004149437 Accuracy 0.7802734375\n",
      "Iteration 56490 Training loss 0.014574280939996243 Validation loss 0.021286772564053535 Accuracy 0.7802734375\n",
      "Iteration 56500 Training loss 0.01668287068605423 Validation loss 0.020991483703255653 Accuracy 0.78271484375\n",
      "Iteration 56510 Training loss 0.010414807125926018 Validation loss 0.02126806043088436 Accuracy 0.78076171875\n",
      "Iteration 56520 Training loss 0.014962203800678253 Validation loss 0.0210019089281559 Accuracy 0.783203125\n",
      "Iteration 56530 Training loss 0.01296637486666441 Validation loss 0.02100503444671631 Accuracy 0.783203125\n",
      "Iteration 56540 Training loss 0.01332857459783554 Validation loss 0.021127596497535706 Accuracy 0.78271484375\n",
      "Iteration 56550 Training loss 0.011501803062856197 Validation loss 0.020768530666828156 Accuracy 0.78515625\n",
      "Iteration 56560 Training loss 0.011354087851941586 Validation loss 0.020916219800710678 Accuracy 0.78369140625\n",
      "Iteration 56570 Training loss 0.012234102934598923 Validation loss 0.021342098712921143 Accuracy 0.7802734375\n",
      "Iteration 56580 Training loss 0.013310193084180355 Validation loss 0.020848631858825684 Accuracy 0.78564453125\n",
      "Iteration 56590 Training loss 0.01270014513283968 Validation loss 0.021464284509420395 Accuracy 0.779296875\n",
      "Iteration 56600 Training loss 0.01374861877411604 Validation loss 0.020975248888134956 Accuracy 0.7841796875\n",
      "Iteration 56610 Training loss 0.01417239848524332 Validation loss 0.0211345162242651 Accuracy 0.78271484375\n",
      "Iteration 56620 Training loss 0.016637861728668213 Validation loss 0.021030329167842865 Accuracy 0.78271484375\n",
      "Iteration 56630 Training loss 0.01529930904507637 Validation loss 0.021092409268021584 Accuracy 0.783203125\n",
      "Iteration 56640 Training loss 0.013965008780360222 Validation loss 0.021019594743847847 Accuracy 0.783203125\n",
      "Iteration 56650 Training loss 0.018098771572113037 Validation loss 0.02141949161887169 Accuracy 0.77783203125\n",
      "Iteration 56660 Training loss 0.01648050546646118 Validation loss 0.021209686994552612 Accuracy 0.7802734375\n",
      "Iteration 56670 Training loss 0.016824916005134583 Validation loss 0.021315718069672585 Accuracy 0.77978515625\n",
      "Iteration 56680 Training loss 0.013998204842209816 Validation loss 0.021381240338087082 Accuracy 0.77880859375\n",
      "Iteration 56690 Training loss 0.011993642896413803 Validation loss 0.02098005637526512 Accuracy 0.78369140625\n",
      "Iteration 56700 Training loss 0.01399060059338808 Validation loss 0.02090645395219326 Accuracy 0.78369140625\n",
      "Iteration 56710 Training loss 0.014427894726395607 Validation loss 0.02092057280242443 Accuracy 0.7841796875\n",
      "Iteration 56720 Training loss 0.012279622256755829 Validation loss 0.02108093723654747 Accuracy 0.78173828125\n",
      "Iteration 56730 Training loss 0.016192303970456123 Validation loss 0.02093684673309326 Accuracy 0.78369140625\n",
      "Iteration 56740 Training loss 0.014678108505904675 Validation loss 0.021102551370859146 Accuracy 0.78173828125\n",
      "Iteration 56750 Training loss 0.014254223555326462 Validation loss 0.02101902849972248 Accuracy 0.783203125\n",
      "Iteration 56760 Training loss 0.014053130522370338 Validation loss 0.021141935139894485 Accuracy 0.7822265625\n",
      "Iteration 56770 Training loss 0.013137679547071457 Validation loss 0.020930808037519455 Accuracy 0.78369140625\n",
      "Iteration 56780 Training loss 0.013279612176120281 Validation loss 0.020888719707727432 Accuracy 0.78515625\n",
      "Iteration 56790 Training loss 0.015817610546946526 Validation loss 0.020884163677692413 Accuracy 0.7841796875\n",
      "Iteration 56800 Training loss 0.01589171402156353 Validation loss 0.02109261229634285 Accuracy 0.78173828125\n",
      "Iteration 56810 Training loss 0.014893299899995327 Validation loss 0.021040180698037148 Accuracy 0.78271484375\n",
      "Iteration 56820 Training loss 0.012927121482789516 Validation loss 0.020862026140093803 Accuracy 0.78515625\n",
      "Iteration 56830 Training loss 0.015523177571594715 Validation loss 0.021413864567875862 Accuracy 0.77880859375\n",
      "Iteration 56840 Training loss 0.014862572774291039 Validation loss 0.020991789177060127 Accuracy 0.783203125\n",
      "Iteration 56850 Training loss 0.0169014073908329 Validation loss 0.021228041499853134 Accuracy 0.78125\n",
      "Iteration 56860 Training loss 0.014045332558453083 Validation loss 0.02121128886938095 Accuracy 0.78173828125\n",
      "Iteration 56870 Training loss 0.01139016356319189 Validation loss 0.021128717809915543 Accuracy 0.78173828125\n",
      "Iteration 56880 Training loss 0.01507316529750824 Validation loss 0.021247517317533493 Accuracy 0.7802734375\n",
      "Iteration 56890 Training loss 0.015010768547654152 Validation loss 0.02114611677825451 Accuracy 0.78173828125\n",
      "Iteration 56900 Training loss 0.015382468700408936 Validation loss 0.021062368527054787 Accuracy 0.783203125\n",
      "Iteration 56910 Training loss 0.014020537957549095 Validation loss 0.021365584805607796 Accuracy 0.78076171875\n",
      "Iteration 56920 Training loss 0.014301611110568047 Validation loss 0.020999057218432426 Accuracy 0.78271484375\n",
      "Iteration 56930 Training loss 0.01271753665059805 Validation loss 0.02083219401538372 Accuracy 0.78466796875\n",
      "Iteration 56940 Training loss 0.01588236168026924 Validation loss 0.02086266130208969 Accuracy 0.78466796875\n",
      "Iteration 56950 Training loss 0.014954004436731339 Validation loss 0.021149970591068268 Accuracy 0.7822265625\n",
      "Iteration 56960 Training loss 0.013180458918213844 Validation loss 0.02102113701403141 Accuracy 0.78271484375\n",
      "Iteration 56970 Training loss 0.015212961472570896 Validation loss 0.02151917666196823 Accuracy 0.7783203125\n",
      "Iteration 56980 Training loss 0.014314918778836727 Validation loss 0.0207977294921875 Accuracy 0.78515625\n",
      "Iteration 56990 Training loss 0.01664135791361332 Validation loss 0.02195233292877674 Accuracy 0.7734375\n",
      "Iteration 57000 Training loss 0.015033802948892117 Validation loss 0.020879287272691727 Accuracy 0.78466796875\n",
      "Iteration 57010 Training loss 0.01482448447495699 Validation loss 0.021106312051415443 Accuracy 0.78173828125\n",
      "Iteration 57020 Training loss 0.012201628647744656 Validation loss 0.020827874541282654 Accuracy 0.78466796875\n",
      "Iteration 57030 Training loss 0.014529820531606674 Validation loss 0.0210172887891531 Accuracy 0.78271484375\n",
      "Iteration 57040 Training loss 0.017335128039121628 Validation loss 0.02085890620946884 Accuracy 0.7841796875\n",
      "Iteration 57050 Training loss 0.014945854432880878 Validation loss 0.020906493067741394 Accuracy 0.7841796875\n",
      "Iteration 57060 Training loss 0.01580909825861454 Validation loss 0.02091299742460251 Accuracy 0.7841796875\n",
      "Iteration 57070 Training loss 0.015687787905335426 Validation loss 0.020820870995521545 Accuracy 0.78515625\n",
      "Iteration 57080 Training loss 0.0128223467618227 Validation loss 0.02082536555826664 Accuracy 0.78466796875\n",
      "Iteration 57090 Training loss 0.018858205527067184 Validation loss 0.021112186834216118 Accuracy 0.78271484375\n",
      "Iteration 57100 Training loss 0.014458208344876766 Validation loss 0.020910851657390594 Accuracy 0.7841796875\n",
      "Iteration 57110 Training loss 0.017473647370934486 Validation loss 0.020860936492681503 Accuracy 0.78369140625\n",
      "Iteration 57120 Training loss 0.013652404770255089 Validation loss 0.02101919800043106 Accuracy 0.783203125\n",
      "Iteration 57130 Training loss 0.015035110525786877 Validation loss 0.020913075655698776 Accuracy 0.78466796875\n",
      "Iteration 57140 Training loss 0.01551044825464487 Validation loss 0.021212389692664146 Accuracy 0.77978515625\n",
      "Iteration 57150 Training loss 0.016730278730392456 Validation loss 0.020931541919708252 Accuracy 0.78271484375\n",
      "Iteration 57160 Training loss 0.013763628900051117 Validation loss 0.020743051543831825 Accuracy 0.7861328125\n",
      "Iteration 57170 Training loss 0.015900634229183197 Validation loss 0.020974690094590187 Accuracy 0.783203125\n",
      "Iteration 57180 Training loss 0.015153683722019196 Validation loss 0.021123798564076424 Accuracy 0.7822265625\n",
      "Iteration 57190 Training loss 0.013993274420499802 Validation loss 0.020933447405695915 Accuracy 0.78369140625\n",
      "Iteration 57200 Training loss 0.01143548171967268 Validation loss 0.020848412066698074 Accuracy 0.7841796875\n",
      "Iteration 57210 Training loss 0.013875704258680344 Validation loss 0.020906224846839905 Accuracy 0.78369140625\n",
      "Iteration 57220 Training loss 0.016369296237826347 Validation loss 0.021219003945589066 Accuracy 0.78076171875\n",
      "Iteration 57230 Training loss 0.014106635935604572 Validation loss 0.020890338346362114 Accuracy 0.78515625\n",
      "Iteration 57240 Training loss 0.014318348839879036 Validation loss 0.021204935386776924 Accuracy 0.78125\n",
      "Iteration 57250 Training loss 0.014978928491473198 Validation loss 0.021189114078879356 Accuracy 0.78125\n",
      "Iteration 57260 Training loss 0.017806051298975945 Validation loss 0.022186068817973137 Accuracy 0.77099609375\n",
      "Iteration 57270 Training loss 0.014171109534800053 Validation loss 0.021153587847948074 Accuracy 0.78125\n",
      "Iteration 57280 Training loss 0.015000119805335999 Validation loss 0.02110847644507885 Accuracy 0.78173828125\n",
      "Iteration 57290 Training loss 0.015262972563505173 Validation loss 0.02106621488928795 Accuracy 0.78271484375\n",
      "Iteration 57300 Training loss 0.015254326164722443 Validation loss 0.02087436430156231 Accuracy 0.78515625\n",
      "Iteration 57310 Training loss 0.01594882272183895 Validation loss 0.02244717627763748 Accuracy 0.77001953125\n",
      "Iteration 57320 Training loss 0.014199480414390564 Validation loss 0.020965920761227608 Accuracy 0.78271484375\n",
      "Iteration 57330 Training loss 0.01575217954814434 Validation loss 0.020835286006331444 Accuracy 0.7841796875\n",
      "Iteration 57340 Training loss 0.015165041200816631 Validation loss 0.02080230601131916 Accuracy 0.7841796875\n",
      "Iteration 57350 Training loss 0.01575128361582756 Validation loss 0.02098042145371437 Accuracy 0.783203125\n",
      "Iteration 57360 Training loss 0.01781488209962845 Validation loss 0.020786486566066742 Accuracy 0.7841796875\n",
      "Iteration 57370 Training loss 0.014756005257368088 Validation loss 0.021012436598539352 Accuracy 0.783203125\n",
      "Iteration 57380 Training loss 0.015980005264282227 Validation loss 0.02101100981235504 Accuracy 0.78125\n",
      "Iteration 57390 Training loss 0.013825626112520695 Validation loss 0.02100895717740059 Accuracy 0.7822265625\n",
      "Iteration 57400 Training loss 0.012115946039557457 Validation loss 0.020967021584510803 Accuracy 0.783203125\n",
      "Iteration 57410 Training loss 0.014729238115251064 Validation loss 0.02085319347679615 Accuracy 0.7841796875\n",
      "Iteration 57420 Training loss 0.012743660248816013 Validation loss 0.020965900272130966 Accuracy 0.78369140625\n",
      "Iteration 57430 Training loss 0.016049014404416084 Validation loss 0.020801015198230743 Accuracy 0.78564453125\n",
      "Iteration 57440 Training loss 0.016267824918031693 Validation loss 0.02095847576856613 Accuracy 0.78369140625\n",
      "Iteration 57450 Training loss 0.01545646134763956 Validation loss 0.020912621170282364 Accuracy 0.78466796875\n",
      "Iteration 57460 Training loss 0.01585981249809265 Validation loss 0.021083006635308266 Accuracy 0.783203125\n",
      "Iteration 57470 Training loss 0.017616119235754013 Validation loss 0.020740006119012833 Accuracy 0.7861328125\n",
      "Iteration 57480 Training loss 0.015523828566074371 Validation loss 0.021030206233263016 Accuracy 0.78369140625\n",
      "Iteration 57490 Training loss 0.012149943970143795 Validation loss 0.020674636587500572 Accuracy 0.7861328125\n",
      "Iteration 57500 Training loss 0.015200362540781498 Validation loss 0.020963676273822784 Accuracy 0.78369140625\n",
      "Iteration 57510 Training loss 0.01396895106881857 Validation loss 0.020823754370212555 Accuracy 0.78515625\n",
      "Iteration 57520 Training loss 0.012830281630158424 Validation loss 0.020850474014878273 Accuracy 0.78564453125\n",
      "Iteration 57530 Training loss 0.013768639415502548 Validation loss 0.02126404270529747 Accuracy 0.78076171875\n",
      "Iteration 57540 Training loss 0.015140876173973083 Validation loss 0.02087566629052162 Accuracy 0.7841796875\n",
      "Iteration 57550 Training loss 0.01406027190387249 Validation loss 0.0209144726395607 Accuracy 0.7841796875\n",
      "Iteration 57560 Training loss 0.013253876939415932 Validation loss 0.02087024599313736 Accuracy 0.78564453125\n",
      "Iteration 57570 Training loss 0.00961571279913187 Validation loss 0.02084980346262455 Accuracy 0.78515625\n",
      "Iteration 57580 Training loss 0.014131609350442886 Validation loss 0.02096276730298996 Accuracy 0.78369140625\n",
      "Iteration 57590 Training loss 0.012399674393236637 Validation loss 0.020933236926794052 Accuracy 0.783203125\n",
      "Iteration 57600 Training loss 0.013455658219754696 Validation loss 0.020801367238163948 Accuracy 0.78466796875\n",
      "Iteration 57610 Training loss 0.01683914102613926 Validation loss 0.021078452467918396 Accuracy 0.7822265625\n",
      "Iteration 57620 Training loss 0.014258477836847305 Validation loss 0.020803309977054596 Accuracy 0.78564453125\n",
      "Iteration 57630 Training loss 0.015430916100740433 Validation loss 0.020784681662917137 Accuracy 0.78466796875\n",
      "Iteration 57640 Training loss 0.014872945845127106 Validation loss 0.02100680209696293 Accuracy 0.78369140625\n",
      "Iteration 57650 Training loss 0.014665265567600727 Validation loss 0.020804299041628838 Accuracy 0.78466796875\n",
      "Iteration 57660 Training loss 0.016263872385025024 Validation loss 0.02140146866440773 Accuracy 0.7783203125\n",
      "Iteration 57670 Training loss 0.01588945835828781 Validation loss 0.020817957818508148 Accuracy 0.78466796875\n",
      "Iteration 57680 Training loss 0.01680152863264084 Validation loss 0.02168891951441765 Accuracy 0.7763671875\n",
      "Iteration 57690 Training loss 0.016764285042881966 Validation loss 0.021103285253047943 Accuracy 0.7822265625\n",
      "Iteration 57700 Training loss 0.012283861637115479 Validation loss 0.02124420367181301 Accuracy 0.78173828125\n",
      "Iteration 57710 Training loss 0.013112812303006649 Validation loss 0.02087590843439102 Accuracy 0.7841796875\n",
      "Iteration 57720 Training loss 0.015551537275314331 Validation loss 0.0207644235342741 Accuracy 0.78662109375\n",
      "Iteration 57730 Training loss 0.014379575848579407 Validation loss 0.02129109762609005 Accuracy 0.78076171875\n",
      "Iteration 57740 Training loss 0.01382461003959179 Validation loss 0.021045267581939697 Accuracy 0.783203125\n",
      "Iteration 57750 Training loss 0.011894923634827137 Validation loss 0.020912861451506615 Accuracy 0.7841796875\n",
      "Iteration 57760 Training loss 0.014451181516051292 Validation loss 0.021132268011569977 Accuracy 0.78173828125\n",
      "Iteration 57770 Training loss 0.013367232866585255 Validation loss 0.020801816135644913 Accuracy 0.78369140625\n",
      "Iteration 57780 Training loss 0.017776815220713615 Validation loss 0.020913129672408104 Accuracy 0.783203125\n",
      "Iteration 57790 Training loss 0.01702071912586689 Validation loss 0.021142929792404175 Accuracy 0.78173828125\n",
      "Iteration 57800 Training loss 0.014796634204685688 Validation loss 0.021668603643774986 Accuracy 0.77783203125\n",
      "Iteration 57810 Training loss 0.01406562514603138 Validation loss 0.020993947982788086 Accuracy 0.783203125\n",
      "Iteration 57820 Training loss 0.014751570299267769 Validation loss 0.02111118659377098 Accuracy 0.7822265625\n",
      "Iteration 57830 Training loss 0.014081787317991257 Validation loss 0.020851949229836464 Accuracy 0.78369140625\n",
      "Iteration 57840 Training loss 0.012523439712822437 Validation loss 0.020882919430732727 Accuracy 0.7841796875\n",
      "Iteration 57850 Training loss 0.015049724839627743 Validation loss 0.021214162930846214 Accuracy 0.78271484375\n",
      "Iteration 57860 Training loss 0.014578260481357574 Validation loss 0.020772147923707962 Accuracy 0.78515625\n",
      "Iteration 57870 Training loss 0.013662111945450306 Validation loss 0.02062978409230709 Accuracy 0.787109375\n",
      "Iteration 57880 Training loss 0.01293894648551941 Validation loss 0.021454809233546257 Accuracy 0.7783203125\n",
      "Iteration 57890 Training loss 0.015342730097472668 Validation loss 0.02107921801507473 Accuracy 0.78173828125\n",
      "Iteration 57900 Training loss 0.015455379150807858 Validation loss 0.02127211168408394 Accuracy 0.78076171875\n",
      "Iteration 57910 Training loss 0.012259667739272118 Validation loss 0.02112673968076706 Accuracy 0.7822265625\n",
      "Iteration 57920 Training loss 0.012896224856376648 Validation loss 0.021115999668836594 Accuracy 0.78173828125\n",
      "Iteration 57930 Training loss 0.015136608853936195 Validation loss 0.021501772105693817 Accuracy 0.7783203125\n",
      "Iteration 57940 Training loss 0.013083597645163536 Validation loss 0.020846938714385033 Accuracy 0.7841796875\n",
      "Iteration 57950 Training loss 0.013620282523334026 Validation loss 0.020790282636880875 Accuracy 0.78564453125\n",
      "Iteration 57960 Training loss 0.016450311988592148 Validation loss 0.021355008706450462 Accuracy 0.779296875\n",
      "Iteration 57970 Training loss 0.012774957343935966 Validation loss 0.021126098930835724 Accuracy 0.7822265625\n",
      "Iteration 57980 Training loss 0.011877926997840405 Validation loss 0.02106962911784649 Accuracy 0.7822265625\n",
      "Iteration 57990 Training loss 0.014064354822039604 Validation loss 0.02080940455198288 Accuracy 0.78515625\n",
      "Iteration 58000 Training loss 0.01530876662582159 Validation loss 0.020879926159977913 Accuracy 0.78466796875\n",
      "Iteration 58010 Training loss 0.013949891552329063 Validation loss 0.020940160378813744 Accuracy 0.78466796875\n",
      "Iteration 58020 Training loss 0.014801022596657276 Validation loss 0.021141406148672104 Accuracy 0.78271484375\n",
      "Iteration 58030 Training loss 0.014252524822950363 Validation loss 0.021149026229977608 Accuracy 0.78173828125\n",
      "Iteration 58040 Training loss 0.0147800138220191 Validation loss 0.020756449550390244 Accuracy 0.78564453125\n",
      "Iteration 58050 Training loss 0.014713396318256855 Validation loss 0.020839596167206764 Accuracy 0.78369140625\n",
      "Iteration 58060 Training loss 0.014871548861265182 Validation loss 0.0213957317173481 Accuracy 0.7783203125\n",
      "Iteration 58070 Training loss 0.01612701825797558 Validation loss 0.021244261413812637 Accuracy 0.7802734375\n",
      "Iteration 58080 Training loss 0.012154108844697475 Validation loss 0.021001899614930153 Accuracy 0.783203125\n",
      "Iteration 58090 Training loss 0.014867829158902168 Validation loss 0.020828887820243835 Accuracy 0.78515625\n",
      "Iteration 58100 Training loss 0.012241405434906483 Validation loss 0.020913636311888695 Accuracy 0.78369140625\n",
      "Iteration 58110 Training loss 0.01248144544661045 Validation loss 0.02089991234242916 Accuracy 0.78466796875\n",
      "Iteration 58120 Training loss 0.017464427277445793 Validation loss 0.020900927484035492 Accuracy 0.7841796875\n",
      "Iteration 58130 Training loss 0.016126025468111038 Validation loss 0.021448295563459396 Accuracy 0.77880859375\n",
      "Iteration 58140 Training loss 0.016794513911008835 Validation loss 0.020879007875919342 Accuracy 0.78466796875\n",
      "Iteration 58150 Training loss 0.015627706423401833 Validation loss 0.020831452682614326 Accuracy 0.78515625\n",
      "Iteration 58160 Training loss 0.015702517703175545 Validation loss 0.021286524832248688 Accuracy 0.7802734375\n",
      "Iteration 58170 Training loss 0.012323456816375256 Validation loss 0.020853763446211815 Accuracy 0.78369140625\n",
      "Iteration 58180 Training loss 0.014992143958806992 Validation loss 0.02110462635755539 Accuracy 0.78125\n",
      "Iteration 58190 Training loss 0.013461325317621231 Validation loss 0.021087907254695892 Accuracy 0.78271484375\n",
      "Iteration 58200 Training loss 0.014129605144262314 Validation loss 0.020973065868020058 Accuracy 0.78369140625\n",
      "Iteration 58210 Training loss 0.015459359623491764 Validation loss 0.020912187173962593 Accuracy 0.78466796875\n",
      "Iteration 58220 Training loss 0.012011885643005371 Validation loss 0.020839326083660126 Accuracy 0.78369140625\n",
      "Iteration 58230 Training loss 0.014252392575144768 Validation loss 0.020802773535251617 Accuracy 0.78466796875\n",
      "Iteration 58240 Training loss 0.014793790876865387 Validation loss 0.020923012867569923 Accuracy 0.7841796875\n",
      "Iteration 58250 Training loss 0.011643827892839909 Validation loss 0.020865799859166145 Accuracy 0.7841796875\n",
      "Iteration 58260 Training loss 0.01830991357564926 Validation loss 0.02143731899559498 Accuracy 0.77783203125\n",
      "Iteration 58270 Training loss 0.01358996331691742 Validation loss 0.020829958841204643 Accuracy 0.78515625\n",
      "Iteration 58280 Training loss 0.016512447968125343 Validation loss 0.02139207534492016 Accuracy 0.779296875\n",
      "Iteration 58290 Training loss 0.015004891902208328 Validation loss 0.02083156257867813 Accuracy 0.78515625\n",
      "Iteration 58300 Training loss 0.013854343444108963 Validation loss 0.020830556750297546 Accuracy 0.78515625\n",
      "Iteration 58310 Training loss 0.014309524558484554 Validation loss 0.020778510719537735 Accuracy 0.7861328125\n",
      "Iteration 58320 Training loss 0.016286145895719528 Validation loss 0.020954711362719536 Accuracy 0.7841796875\n",
      "Iteration 58330 Training loss 0.015263243578374386 Validation loss 0.021783115342259407 Accuracy 0.7744140625\n",
      "Iteration 58340 Training loss 0.014134916476905346 Validation loss 0.02092629112303257 Accuracy 0.78515625\n",
      "Iteration 58350 Training loss 0.014362627640366554 Validation loss 0.021153351292014122 Accuracy 0.78125\n",
      "Iteration 58360 Training loss 0.01341313123703003 Validation loss 0.02088802680373192 Accuracy 0.78466796875\n",
      "Iteration 58370 Training loss 0.014290130697190762 Validation loss 0.021088387817144394 Accuracy 0.7822265625\n",
      "Iteration 58380 Training loss 0.01516075897961855 Validation loss 0.020958764478564262 Accuracy 0.78369140625\n",
      "Iteration 58390 Training loss 0.016174957156181335 Validation loss 0.021173691377043724 Accuracy 0.78271484375\n",
      "Iteration 58400 Training loss 0.017203716561198235 Validation loss 0.020808756351470947 Accuracy 0.78515625\n",
      "Iteration 58410 Training loss 0.01208592765033245 Validation loss 0.02069820836186409 Accuracy 0.78515625\n",
      "Iteration 58420 Training loss 0.012564411386847496 Validation loss 0.02060854434967041 Accuracy 0.787109375\n",
      "Iteration 58430 Training loss 0.01636585406959057 Validation loss 0.020936135202646255 Accuracy 0.78369140625\n",
      "Iteration 58440 Training loss 0.015200271271169186 Validation loss 0.020941002294421196 Accuracy 0.783203125\n",
      "Iteration 58450 Training loss 0.015918172895908356 Validation loss 0.020903436467051506 Accuracy 0.7841796875\n",
      "Iteration 58460 Training loss 0.013302082195878029 Validation loss 0.02130819484591484 Accuracy 0.78076171875\n",
      "Iteration 58470 Training loss 0.013584339991211891 Validation loss 0.02089311182498932 Accuracy 0.78369140625\n",
      "Iteration 58480 Training loss 0.015269492752850056 Validation loss 0.020831020548939705 Accuracy 0.7841796875\n",
      "Iteration 58490 Training loss 0.012090062722563744 Validation loss 0.020802993327379227 Accuracy 0.78515625\n",
      "Iteration 58500 Training loss 0.01516268402338028 Validation loss 0.02080412767827511 Accuracy 0.78515625\n",
      "Iteration 58510 Training loss 0.014972851611673832 Validation loss 0.02108687534928322 Accuracy 0.78173828125\n",
      "Iteration 58520 Training loss 0.01566229574382305 Validation loss 0.020885709673166275 Accuracy 0.7841796875\n",
      "Iteration 58530 Training loss 0.013394388370215893 Validation loss 0.02082509733736515 Accuracy 0.78515625\n",
      "Iteration 58540 Training loss 0.01606222428381443 Validation loss 0.021201245486736298 Accuracy 0.7802734375\n",
      "Iteration 58550 Training loss 0.011998419649899006 Validation loss 0.020927365869283676 Accuracy 0.7841796875\n",
      "Iteration 58560 Training loss 0.013907799497246742 Validation loss 0.020912855863571167 Accuracy 0.7841796875\n",
      "Iteration 58570 Training loss 0.01719287410378456 Validation loss 0.020828714594244957 Accuracy 0.78466796875\n",
      "Iteration 58580 Training loss 0.015749221667647362 Validation loss 0.021211830899119377 Accuracy 0.78125\n",
      "Iteration 58590 Training loss 0.011356262490153313 Validation loss 0.02091926522552967 Accuracy 0.7841796875\n",
      "Iteration 58600 Training loss 0.014910043217241764 Validation loss 0.02109077014029026 Accuracy 0.78369140625\n",
      "Iteration 58610 Training loss 0.01564478874206543 Validation loss 0.02187964878976345 Accuracy 0.7744140625\n",
      "Iteration 58620 Training loss 0.015873566269874573 Validation loss 0.021295001730322838 Accuracy 0.78076171875\n",
      "Iteration 58630 Training loss 0.014194843359291553 Validation loss 0.02123010717332363 Accuracy 0.78076171875\n",
      "Iteration 58640 Training loss 0.011412028223276138 Validation loss 0.021249862387776375 Accuracy 0.7822265625\n",
      "Iteration 58650 Training loss 0.015030906535685062 Validation loss 0.02111358381807804 Accuracy 0.78271484375\n",
      "Iteration 58660 Training loss 0.017496256157755852 Validation loss 0.02180762216448784 Accuracy 0.7744140625\n",
      "Iteration 58670 Training loss 0.015038876794278622 Validation loss 0.020858781412243843 Accuracy 0.7841796875\n",
      "Iteration 58680 Training loss 0.014184203930199146 Validation loss 0.020890405401587486 Accuracy 0.78369140625\n",
      "Iteration 58690 Training loss 0.015320749953389168 Validation loss 0.020697763189673424 Accuracy 0.78515625\n",
      "Iteration 58700 Training loss 0.014862812124192715 Validation loss 0.02094891294836998 Accuracy 0.7841796875\n",
      "Iteration 58710 Training loss 0.013903958722949028 Validation loss 0.021040163934230804 Accuracy 0.78271484375\n",
      "Iteration 58720 Training loss 0.014181012287735939 Validation loss 0.020945917814970016 Accuracy 0.78271484375\n",
      "Iteration 58730 Training loss 0.012383079156279564 Validation loss 0.020768629387021065 Accuracy 0.78564453125\n",
      "Iteration 58740 Training loss 0.015470817685127258 Validation loss 0.021873168647289276 Accuracy 0.77392578125\n",
      "Iteration 58750 Training loss 0.015306536108255386 Validation loss 0.02104262448847294 Accuracy 0.78271484375\n",
      "Iteration 58760 Training loss 0.016836067661643028 Validation loss 0.02102404274046421 Accuracy 0.78271484375\n",
      "Iteration 58770 Training loss 0.01465541310608387 Validation loss 0.021372796967625618 Accuracy 0.779296875\n",
      "Iteration 58780 Training loss 0.014962471090257168 Validation loss 0.020873673260211945 Accuracy 0.78466796875\n",
      "Iteration 58790 Training loss 0.013862251304090023 Validation loss 0.02085946314036846 Accuracy 0.78564453125\n",
      "Iteration 58800 Training loss 0.014008065685629845 Validation loss 0.02096390537917614 Accuracy 0.783203125\n",
      "Iteration 58810 Training loss 0.015456362627446651 Validation loss 0.020948808640241623 Accuracy 0.78466796875\n",
      "Iteration 58820 Training loss 0.012959427200257778 Validation loss 0.02105122059583664 Accuracy 0.783203125\n",
      "Iteration 58830 Training loss 0.014069585129618645 Validation loss 0.02134208008646965 Accuracy 0.77978515625\n",
      "Iteration 58840 Training loss 0.013924079947173595 Validation loss 0.021107235923409462 Accuracy 0.7822265625\n",
      "Iteration 58850 Training loss 0.01673535816371441 Validation loss 0.02103232778608799 Accuracy 0.783203125\n",
      "Iteration 58860 Training loss 0.01485251821577549 Validation loss 0.02093217708170414 Accuracy 0.78369140625\n",
      "Iteration 58870 Training loss 0.014902882277965546 Validation loss 0.02094562165439129 Accuracy 0.783203125\n",
      "Iteration 58880 Training loss 0.014207271859049797 Validation loss 0.020869575440883636 Accuracy 0.78369140625\n",
      "Iteration 58890 Training loss 0.01346582267433405 Validation loss 0.021108446642756462 Accuracy 0.7822265625\n",
      "Iteration 58900 Training loss 0.014056464657187462 Validation loss 0.02156490460038185 Accuracy 0.7783203125\n",
      "Iteration 58910 Training loss 0.018770238384604454 Validation loss 0.02096051163971424 Accuracy 0.78369140625\n",
      "Iteration 58920 Training loss 0.012981826439499855 Validation loss 0.020890122279524803 Accuracy 0.7841796875\n",
      "Iteration 58930 Training loss 0.01517239585518837 Validation loss 0.020892348140478134 Accuracy 0.78466796875\n",
      "Iteration 58940 Training loss 0.013537900522351265 Validation loss 0.020848460495471954 Accuracy 0.78515625\n",
      "Iteration 58950 Training loss 0.015943417325615883 Validation loss 0.022273175418376923 Accuracy 0.771484375\n",
      "Iteration 58960 Training loss 0.013311716727912426 Validation loss 0.02087424322962761 Accuracy 0.78369140625\n",
      "Iteration 58970 Training loss 0.017047306522727013 Validation loss 0.021195512264966965 Accuracy 0.78076171875\n",
      "Iteration 58980 Training loss 0.013686922378838062 Validation loss 0.020883610472083092 Accuracy 0.7841796875\n",
      "Iteration 58990 Training loss 0.014771412126719952 Validation loss 0.021112248301506042 Accuracy 0.78173828125\n",
      "Iteration 59000 Training loss 0.013136306777596474 Validation loss 0.021032102406024933 Accuracy 0.78271484375\n",
      "Iteration 59010 Training loss 0.014691837131977081 Validation loss 0.020783480256795883 Accuracy 0.78515625\n",
      "Iteration 59020 Training loss 0.014554232358932495 Validation loss 0.021250981837511063 Accuracy 0.7802734375\n",
      "Iteration 59030 Training loss 0.016570910811424255 Validation loss 0.021702304482460022 Accuracy 0.775390625\n",
      "Iteration 59040 Training loss 0.01393866166472435 Validation loss 0.02143915556371212 Accuracy 0.77880859375\n",
      "Iteration 59050 Training loss 0.013339119963347912 Validation loss 0.020763788372278214 Accuracy 0.78515625\n",
      "Iteration 59060 Training loss 0.015755338594317436 Validation loss 0.021149400621652603 Accuracy 0.78125\n",
      "Iteration 59070 Training loss 0.013636049814522266 Validation loss 0.021162167191505432 Accuracy 0.78173828125\n",
      "Iteration 59080 Training loss 0.013629061169922352 Validation loss 0.0209084153175354 Accuracy 0.78466796875\n",
      "Iteration 59090 Training loss 0.013720388524234295 Validation loss 0.021132169291377068 Accuracy 0.78125\n",
      "Iteration 59100 Training loss 0.014911781065165997 Validation loss 0.02101210318505764 Accuracy 0.78271484375\n",
      "Iteration 59110 Training loss 0.016225211322307587 Validation loss 0.020943038165569305 Accuracy 0.78369140625\n",
      "Iteration 59120 Training loss 0.015179651789367199 Validation loss 0.02128724381327629 Accuracy 0.77978515625\n",
      "Iteration 59130 Training loss 0.01357671432197094 Validation loss 0.021012485027313232 Accuracy 0.78271484375\n",
      "Iteration 59140 Training loss 0.013916697353124619 Validation loss 0.021104106679558754 Accuracy 0.7822265625\n",
      "Iteration 59150 Training loss 0.015429485589265823 Validation loss 0.020834708586335182 Accuracy 0.78466796875\n",
      "Iteration 59160 Training loss 0.014070103876292706 Validation loss 0.021097641438245773 Accuracy 0.7822265625\n",
      "Iteration 59170 Training loss 0.013963199220597744 Validation loss 0.02104998007416725 Accuracy 0.783203125\n",
      "Iteration 59180 Training loss 0.011982424184679985 Validation loss 0.021039443090558052 Accuracy 0.7822265625\n",
      "Iteration 59190 Training loss 0.016404014080762863 Validation loss 0.020985888317227364 Accuracy 0.78369140625\n",
      "Iteration 59200 Training loss 0.012213044799864292 Validation loss 0.02099788188934326 Accuracy 0.78369140625\n",
      "Iteration 59210 Training loss 0.014202387072145939 Validation loss 0.020752938464283943 Accuracy 0.78564453125\n",
      "Iteration 59220 Training loss 0.015535286627709866 Validation loss 0.020761622115969658 Accuracy 0.78564453125\n",
      "Iteration 59230 Training loss 0.01628168672323227 Validation loss 0.02098444476723671 Accuracy 0.7841796875\n",
      "Iteration 59240 Training loss 0.013282704167068005 Validation loss 0.020900283008813858 Accuracy 0.7841796875\n",
      "Iteration 59250 Training loss 0.014258753508329391 Validation loss 0.020939232781529427 Accuracy 0.78369140625\n",
      "Iteration 59260 Training loss 0.018586328253149986 Validation loss 0.021690750494599342 Accuracy 0.77685546875\n",
      "Iteration 59270 Training loss 0.014503064565360546 Validation loss 0.020790424197912216 Accuracy 0.78466796875\n",
      "Iteration 59280 Training loss 0.01483427919447422 Validation loss 0.020854799076914787 Accuracy 0.78564453125\n",
      "Iteration 59290 Training loss 0.015061184763908386 Validation loss 0.02108851820230484 Accuracy 0.78271484375\n",
      "Iteration 59300 Training loss 0.013817372731864452 Validation loss 0.020934028550982475 Accuracy 0.78271484375\n",
      "Iteration 59310 Training loss 0.017445961013436317 Validation loss 0.020868675783276558 Accuracy 0.78466796875\n",
      "Iteration 59320 Training loss 0.01567091792821884 Validation loss 0.020935270935297012 Accuracy 0.78369140625\n",
      "Iteration 59330 Training loss 0.012570217251777649 Validation loss 0.020983172580599785 Accuracy 0.783203125\n",
      "Iteration 59340 Training loss 0.013991640880703926 Validation loss 0.02086048759520054 Accuracy 0.78466796875\n",
      "Iteration 59350 Training loss 0.013549882918596268 Validation loss 0.020802250131964684 Accuracy 0.78515625\n",
      "Iteration 59360 Training loss 0.01312694326043129 Validation loss 0.02104124054312706 Accuracy 0.7822265625\n",
      "Iteration 59370 Training loss 0.01386438962072134 Validation loss 0.020956052467226982 Accuracy 0.78466796875\n",
      "Iteration 59380 Training loss 0.015495930798351765 Validation loss 0.022377127781510353 Accuracy 0.77001953125\n",
      "Iteration 59390 Training loss 0.01576213911175728 Validation loss 0.0207645446062088 Accuracy 0.78564453125\n",
      "Iteration 59400 Training loss 0.013775868341326714 Validation loss 0.02110726572573185 Accuracy 0.7822265625\n",
      "Iteration 59410 Training loss 0.01665148138999939 Validation loss 0.02109365165233612 Accuracy 0.78173828125\n",
      "Iteration 59420 Training loss 0.01372463721781969 Validation loss 0.021004745736718178 Accuracy 0.78271484375\n",
      "Iteration 59430 Training loss 0.0145433209836483 Validation loss 0.020961971953511238 Accuracy 0.78369140625\n",
      "Iteration 59440 Training loss 0.016267364844679832 Validation loss 0.021013446152210236 Accuracy 0.783203125\n",
      "Iteration 59450 Training loss 0.015556338243186474 Validation loss 0.021062307059764862 Accuracy 0.78271484375\n",
      "Iteration 59460 Training loss 0.013256337493658066 Validation loss 0.02075420320034027 Accuracy 0.7861328125\n",
      "Iteration 59470 Training loss 0.014377688989043236 Validation loss 0.02083117701113224 Accuracy 0.78515625\n",
      "Iteration 59480 Training loss 0.012402490712702274 Validation loss 0.021123910322785378 Accuracy 0.78271484375\n",
      "Iteration 59490 Training loss 0.015197818167507648 Validation loss 0.02094070240855217 Accuracy 0.7841796875\n",
      "Iteration 59500 Training loss 0.014383580535650253 Validation loss 0.02101929299533367 Accuracy 0.783203125\n",
      "Iteration 59510 Training loss 0.015466428361833096 Validation loss 0.021353386342525482 Accuracy 0.77978515625\n",
      "Iteration 59520 Training loss 0.013492710888385773 Validation loss 0.020940154790878296 Accuracy 0.78466796875\n",
      "Iteration 59530 Training loss 0.011889995075762272 Validation loss 0.021193422377109528 Accuracy 0.78173828125\n",
      "Iteration 59540 Training loss 0.01664930395781994 Validation loss 0.021079618483781815 Accuracy 0.78271484375\n",
      "Iteration 59550 Training loss 0.013879460282623768 Validation loss 0.02096729166805744 Accuracy 0.7841796875\n",
      "Iteration 59560 Training loss 0.014109771698713303 Validation loss 0.020958006381988525 Accuracy 0.783203125\n",
      "Iteration 59570 Training loss 0.012324152514338493 Validation loss 0.021140046417713165 Accuracy 0.78125\n",
      "Iteration 59580 Training loss 0.013605846092104912 Validation loss 0.021780846640467644 Accuracy 0.77490234375\n",
      "Iteration 59590 Training loss 0.01847989112138748 Validation loss 0.0211854949593544 Accuracy 0.7822265625\n",
      "Iteration 59600 Training loss 0.014480847865343094 Validation loss 0.02137358859181404 Accuracy 0.7802734375\n",
      "Iteration 59610 Training loss 0.013917765580117702 Validation loss 0.021196717396378517 Accuracy 0.78125\n",
      "Iteration 59620 Training loss 0.014798659831285477 Validation loss 0.021116796880960464 Accuracy 0.7822265625\n",
      "Iteration 59630 Training loss 0.013529523275792599 Validation loss 0.020876599475741386 Accuracy 0.7841796875\n",
      "Iteration 59640 Training loss 0.01536348182708025 Validation loss 0.020595965906977654 Accuracy 0.787109375\n",
      "Iteration 59650 Training loss 0.015783783048391342 Validation loss 0.020872078835964203 Accuracy 0.78466796875\n",
      "Iteration 59660 Training loss 0.014964503236114979 Validation loss 0.020669808611273766 Accuracy 0.78662109375\n",
      "Iteration 59670 Training loss 0.015543258748948574 Validation loss 0.020708592608571053 Accuracy 0.78564453125\n",
      "Iteration 59680 Training loss 0.014816622249782085 Validation loss 0.021186647936701775 Accuracy 0.78125\n",
      "Iteration 59690 Training loss 0.0123612554743886 Validation loss 0.02084212377667427 Accuracy 0.78515625\n",
      "Iteration 59700 Training loss 0.01363114919513464 Validation loss 0.020891956984996796 Accuracy 0.78564453125\n",
      "Iteration 59710 Training loss 0.016769535839557648 Validation loss 0.021457413211464882 Accuracy 0.7783203125\n",
      "Iteration 59720 Training loss 0.014677469618618488 Validation loss 0.0207749605178833 Accuracy 0.78515625\n",
      "Iteration 59730 Training loss 0.01430186815559864 Validation loss 0.021046200767159462 Accuracy 0.783203125\n",
      "Iteration 59740 Training loss 0.016501840204000473 Validation loss 0.021317368373274803 Accuracy 0.78076171875\n",
      "Iteration 59750 Training loss 0.016052106395363808 Validation loss 0.02108686789870262 Accuracy 0.78173828125\n",
      "Iteration 59760 Training loss 0.016112076118588448 Validation loss 0.021130794659256935 Accuracy 0.78173828125\n",
      "Iteration 59770 Training loss 0.013138450682163239 Validation loss 0.02096828632056713 Accuracy 0.78369140625\n",
      "Iteration 59780 Training loss 0.013572020456194878 Validation loss 0.020930888131260872 Accuracy 0.78271484375\n",
      "Iteration 59790 Training loss 0.01348303072154522 Validation loss 0.021207014098763466 Accuracy 0.78125\n",
      "Iteration 59800 Training loss 0.012384551577270031 Validation loss 0.02088315039873123 Accuracy 0.7841796875\n",
      "Iteration 59810 Training loss 0.012558220885694027 Validation loss 0.02074596658349037 Accuracy 0.78564453125\n",
      "Iteration 59820 Training loss 0.013480354100465775 Validation loss 0.02145267464220524 Accuracy 0.779296875\n",
      "Iteration 59830 Training loss 0.012746323831379414 Validation loss 0.020875805988907814 Accuracy 0.78564453125\n",
      "Iteration 59840 Training loss 0.014824967831373215 Validation loss 0.02081916481256485 Accuracy 0.7841796875\n",
      "Iteration 59850 Training loss 0.014547936618328094 Validation loss 0.021098241209983826 Accuracy 0.78271484375\n",
      "Iteration 59860 Training loss 0.013205128721892834 Validation loss 0.02070770412683487 Accuracy 0.7861328125\n",
      "Iteration 59870 Training loss 0.014691959135234356 Validation loss 0.021286996081471443 Accuracy 0.7802734375\n",
      "Iteration 59880 Training loss 0.015890423208475113 Validation loss 0.021221088245511055 Accuracy 0.78076171875\n",
      "Iteration 59890 Training loss 0.015558593906462193 Validation loss 0.020750392228364944 Accuracy 0.78515625\n",
      "Iteration 59900 Training loss 0.013359745033085346 Validation loss 0.02103978395462036 Accuracy 0.783203125\n",
      "Iteration 59910 Training loss 0.01209973357617855 Validation loss 0.02119901403784752 Accuracy 0.78076171875\n",
      "Iteration 59920 Training loss 0.016101285815238953 Validation loss 0.02079019881784916 Accuracy 0.7841796875\n",
      "Iteration 59930 Training loss 0.014030799269676208 Validation loss 0.020765092223882675 Accuracy 0.7841796875\n",
      "Iteration 59940 Training loss 0.01573546789586544 Validation loss 0.02086230181157589 Accuracy 0.7841796875\n",
      "Iteration 59950 Training loss 0.01300150528550148 Validation loss 0.020887460559606552 Accuracy 0.78466796875\n",
      "Iteration 59960 Training loss 0.01569478027522564 Validation loss 0.020932823419570923 Accuracy 0.78369140625\n",
      "Iteration 59970 Training loss 0.014981772750616074 Validation loss 0.02085212431848049 Accuracy 0.78466796875\n",
      "Iteration 59980 Training loss 0.013579769060015678 Validation loss 0.02095632813870907 Accuracy 0.78369140625\n",
      "Iteration 59990 Training loss 0.0148647865280509 Validation loss 0.020898403599858284 Accuracy 0.783203125\n",
      "Iteration 60000 Training loss 0.013563250191509724 Validation loss 0.020973743870854378 Accuracy 0.78271484375\n",
      "Iteration 60010 Training loss 0.012747197411954403 Validation loss 0.020929833874106407 Accuracy 0.78369140625\n",
      "Iteration 60020 Training loss 0.015148166567087173 Validation loss 0.020917698740959167 Accuracy 0.783203125\n",
      "Iteration 60030 Training loss 0.015969056636095047 Validation loss 0.02139231190085411 Accuracy 0.779296875\n",
      "Iteration 60040 Training loss 0.010824035853147507 Validation loss 0.02089826576411724 Accuracy 0.78369140625\n",
      "Iteration 60050 Training loss 0.014335409738123417 Validation loss 0.02128705568611622 Accuracy 0.7802734375\n",
      "Iteration 60060 Training loss 0.013893508352339268 Validation loss 0.02094585821032524 Accuracy 0.783203125\n",
      "Iteration 60070 Training loss 0.017368514090776443 Validation loss 0.021141722798347473 Accuracy 0.78173828125\n",
      "Iteration 60080 Training loss 0.014698135666549206 Validation loss 0.021188823506236076 Accuracy 0.78125\n",
      "Iteration 60090 Training loss 0.014978383667767048 Validation loss 0.021067485213279724 Accuracy 0.7822265625\n",
      "Iteration 60100 Training loss 0.013867052271962166 Validation loss 0.021137503907084465 Accuracy 0.78125\n",
      "Iteration 60110 Training loss 0.011876345612108707 Validation loss 0.021167494356632233 Accuracy 0.78076171875\n",
      "Iteration 60120 Training loss 0.015834268182516098 Validation loss 0.021507587283849716 Accuracy 0.77783203125\n",
      "Iteration 60130 Training loss 0.014666067436337471 Validation loss 0.02090568281710148 Accuracy 0.78369140625\n",
      "Iteration 60140 Training loss 0.015393165871500969 Validation loss 0.020862920209765434 Accuracy 0.78369140625\n",
      "Iteration 60150 Training loss 0.014420781284570694 Validation loss 0.02100050263106823 Accuracy 0.783203125\n",
      "Iteration 60160 Training loss 0.012210268527269363 Validation loss 0.02089652232825756 Accuracy 0.78369140625\n",
      "Iteration 60170 Training loss 0.013791384175419807 Validation loss 0.02139110304415226 Accuracy 0.77978515625\n",
      "Iteration 60180 Training loss 0.014043952338397503 Validation loss 0.02108425460755825 Accuracy 0.78271484375\n",
      "Iteration 60190 Training loss 0.015855181962251663 Validation loss 0.021246301010251045 Accuracy 0.78076171875\n",
      "Iteration 60200 Training loss 0.011829358525574207 Validation loss 0.02075871080160141 Accuracy 0.78662109375\n",
      "Iteration 60210 Training loss 0.013977358117699623 Validation loss 0.021023722365498543 Accuracy 0.7822265625\n",
      "Iteration 60220 Training loss 0.015040350146591663 Validation loss 0.021108463406562805 Accuracy 0.78173828125\n",
      "Iteration 60230 Training loss 0.013972132466733456 Validation loss 0.02123657613992691 Accuracy 0.779296875\n",
      "Iteration 60240 Training loss 0.011496029794216156 Validation loss 0.0208901260048151 Accuracy 0.78369140625\n",
      "Iteration 60250 Training loss 0.014975916594266891 Validation loss 0.020914016291499138 Accuracy 0.78369140625\n",
      "Iteration 60260 Training loss 0.014327995479106903 Validation loss 0.02130218781530857 Accuracy 0.7802734375\n",
      "Iteration 60270 Training loss 0.014268065802752972 Validation loss 0.021037787199020386 Accuracy 0.78271484375\n",
      "Iteration 60280 Training loss 0.016510922461748123 Validation loss 0.021177401766180992 Accuracy 0.78125\n",
      "Iteration 60290 Training loss 0.014250177890062332 Validation loss 0.020756276324391365 Accuracy 0.78515625\n",
      "Iteration 60300 Training loss 0.01572921685874462 Validation loss 0.02083149366080761 Accuracy 0.78466796875\n",
      "Iteration 60310 Training loss 0.015166337601840496 Validation loss 0.021063536405563354 Accuracy 0.783203125\n",
      "Iteration 60320 Training loss 0.015896642580628395 Validation loss 0.020803092047572136 Accuracy 0.78515625\n",
      "Iteration 60330 Training loss 0.01637931354343891 Validation loss 0.021396860480308533 Accuracy 0.779296875\n",
      "Iteration 60340 Training loss 0.012872723862528801 Validation loss 0.020824164152145386 Accuracy 0.78466796875\n",
      "Iteration 60350 Training loss 0.013709972612559795 Validation loss 0.02096676267683506 Accuracy 0.78271484375\n",
      "Iteration 60360 Training loss 0.012709395959973335 Validation loss 0.02093438431620598 Accuracy 0.7841796875\n",
      "Iteration 60370 Training loss 0.014036379754543304 Validation loss 0.021143393591046333 Accuracy 0.78173828125\n",
      "Iteration 60380 Training loss 0.014098027721047401 Validation loss 0.020984912291169167 Accuracy 0.7841796875\n",
      "Iteration 60390 Training loss 0.014169328846037388 Validation loss 0.021010849624872208 Accuracy 0.78271484375\n",
      "Iteration 60400 Training loss 0.012899166904389858 Validation loss 0.021433470770716667 Accuracy 0.779296875\n",
      "Iteration 60410 Training loss 0.014395812526345253 Validation loss 0.020992426201701164 Accuracy 0.783203125\n",
      "Iteration 60420 Training loss 0.015122881159186363 Validation loss 0.02111147902905941 Accuracy 0.78173828125\n",
      "Iteration 60430 Training loss 0.01474789623171091 Validation loss 0.021209999918937683 Accuracy 0.78125\n",
      "Iteration 60440 Training loss 0.012737059034407139 Validation loss 0.02121107093989849 Accuracy 0.78125\n",
      "Iteration 60450 Training loss 0.015698665753006935 Validation loss 0.020786575973033905 Accuracy 0.78515625\n",
      "Iteration 60460 Training loss 0.01460349466651678 Validation loss 0.021518783643841743 Accuracy 0.77734375\n",
      "Iteration 60470 Training loss 0.01427328959107399 Validation loss 0.020776014775037766 Accuracy 0.78466796875\n",
      "Iteration 60480 Training loss 0.01362697035074234 Validation loss 0.020835040137171745 Accuracy 0.78466796875\n",
      "Iteration 60490 Training loss 0.015830764546990395 Validation loss 0.021034281700849533 Accuracy 0.783203125\n",
      "Iteration 60500 Training loss 0.01117403618991375 Validation loss 0.0208999365568161 Accuracy 0.78466796875\n",
      "Iteration 60510 Training loss 0.012459542602300644 Validation loss 0.021030453965067863 Accuracy 0.783203125\n",
      "Iteration 60520 Training loss 0.014538021758198738 Validation loss 0.02099345251917839 Accuracy 0.78369140625\n",
      "Iteration 60530 Training loss 0.016435323283076286 Validation loss 0.020849594846367836 Accuracy 0.78466796875\n",
      "Iteration 60540 Training loss 0.012719729915261269 Validation loss 0.021016264334321022 Accuracy 0.7822265625\n",
      "Iteration 60550 Training loss 0.01717539317905903 Validation loss 0.02121312916278839 Accuracy 0.7802734375\n",
      "Iteration 60560 Training loss 0.014062365517020226 Validation loss 0.020781679078936577 Accuracy 0.78564453125\n",
      "Iteration 60570 Training loss 0.012212657369673252 Validation loss 0.020998166874051094 Accuracy 0.783203125\n",
      "Iteration 60580 Training loss 0.014093521051108837 Validation loss 0.02149771712720394 Accuracy 0.77880859375\n",
      "Iteration 60590 Training loss 0.013813815079629421 Validation loss 0.02107829973101616 Accuracy 0.7822265625\n",
      "Iteration 60600 Training loss 0.01618335023522377 Validation loss 0.02111181616783142 Accuracy 0.7822265625\n",
      "Iteration 60610 Training loss 0.014525745064020157 Validation loss 0.020843051373958588 Accuracy 0.7841796875\n",
      "Iteration 60620 Training loss 0.013785967603325844 Validation loss 0.021523119881749153 Accuracy 0.77783203125\n",
      "Iteration 60630 Training loss 0.0136259151622653 Validation loss 0.021097412332892418 Accuracy 0.7822265625\n",
      "Iteration 60640 Training loss 0.0166343841701746 Validation loss 0.02092813141644001 Accuracy 0.78369140625\n",
      "Iteration 60650 Training loss 0.015227963216602802 Validation loss 0.020787540823221207 Accuracy 0.78515625\n",
      "Iteration 60660 Training loss 0.012608872726559639 Validation loss 0.02082216739654541 Accuracy 0.78466796875\n",
      "Iteration 60670 Training loss 0.01511958334594965 Validation loss 0.020900780335068703 Accuracy 0.78369140625\n",
      "Iteration 60680 Training loss 0.01678326167166233 Validation loss 0.02157018519937992 Accuracy 0.77734375\n",
      "Iteration 60690 Training loss 0.014178033918142319 Validation loss 0.021229486912488937 Accuracy 0.78173828125\n",
      "Iteration 60700 Training loss 0.015570860356092453 Validation loss 0.021120112389326096 Accuracy 0.78125\n",
      "Iteration 60710 Training loss 0.014572741463780403 Validation loss 0.020909853279590607 Accuracy 0.78466796875\n",
      "Iteration 60720 Training loss 0.012412043288350105 Validation loss 0.02127288468182087 Accuracy 0.78076171875\n",
      "Iteration 60730 Training loss 0.013486738316714764 Validation loss 0.02098565362393856 Accuracy 0.78271484375\n",
      "Iteration 60740 Training loss 0.014233092777431011 Validation loss 0.021484864875674248 Accuracy 0.77880859375\n",
      "Iteration 60750 Training loss 0.015066070482134819 Validation loss 0.021474184468388557 Accuracy 0.77685546875\n",
      "Iteration 60760 Training loss 0.014575272798538208 Validation loss 0.020720751956105232 Accuracy 0.78564453125\n",
      "Iteration 60770 Training loss 0.013283762149512768 Validation loss 0.02081952430307865 Accuracy 0.78369140625\n",
      "Iteration 60780 Training loss 0.01336422748863697 Validation loss 0.021184656769037247 Accuracy 0.78173828125\n",
      "Iteration 60790 Training loss 0.01281794998794794 Validation loss 0.021390004083514214 Accuracy 0.779296875\n",
      "Iteration 60800 Training loss 0.01641460880637169 Validation loss 0.02127731218934059 Accuracy 0.78076171875\n",
      "Iteration 60810 Training loss 0.015013478696346283 Validation loss 0.021302606910467148 Accuracy 0.78076171875\n",
      "Iteration 60820 Training loss 0.013693802058696747 Validation loss 0.020782016217708588 Accuracy 0.78515625\n",
      "Iteration 60830 Training loss 0.013777127489447594 Validation loss 0.021017536520957947 Accuracy 0.7822265625\n",
      "Iteration 60840 Training loss 0.012720867991447449 Validation loss 0.021013012155890465 Accuracy 0.78271484375\n",
      "Iteration 60850 Training loss 0.015350486151874065 Validation loss 0.02105499431490898 Accuracy 0.7822265625\n",
      "Iteration 60860 Training loss 0.014779196120798588 Validation loss 0.02135424315929413 Accuracy 0.7802734375\n",
      "Iteration 60870 Training loss 0.01443515159189701 Validation loss 0.021120551973581314 Accuracy 0.78271484375\n",
      "Iteration 60880 Training loss 0.015026799403131008 Validation loss 0.021189594641327858 Accuracy 0.78173828125\n",
      "Iteration 60890 Training loss 0.013986397534608841 Validation loss 0.020761601626873016 Accuracy 0.78515625\n",
      "Iteration 60900 Training loss 0.0154567314311862 Validation loss 0.021165255457162857 Accuracy 0.7802734375\n",
      "Iteration 60910 Training loss 0.013262636959552765 Validation loss 0.02083372324705124 Accuracy 0.78466796875\n",
      "Iteration 60920 Training loss 0.013320108875632286 Validation loss 0.02108783647418022 Accuracy 0.7822265625\n",
      "Iteration 60930 Training loss 0.01370153296738863 Validation loss 0.02105512097477913 Accuracy 0.7822265625\n",
      "Iteration 60940 Training loss 0.012617622502148151 Validation loss 0.02184392511844635 Accuracy 0.7744140625\n",
      "Iteration 60950 Training loss 0.011288383044302464 Validation loss 0.020765632390975952 Accuracy 0.78515625\n",
      "Iteration 60960 Training loss 0.015000437386333942 Validation loss 0.02154531516134739 Accuracy 0.77734375\n",
      "Iteration 60970 Training loss 0.01758480630815029 Validation loss 0.02102227509021759 Accuracy 0.78271484375\n",
      "Iteration 60980 Training loss 0.013558782637119293 Validation loss 0.020947465673089027 Accuracy 0.78173828125\n",
      "Iteration 60990 Training loss 0.014126583933830261 Validation loss 0.021051449701189995 Accuracy 0.78271484375\n",
      "Iteration 61000 Training loss 0.01673533208668232 Validation loss 0.021659862250089645 Accuracy 0.77685546875\n",
      "Iteration 61010 Training loss 0.014342819340527058 Validation loss 0.020856034010648727 Accuracy 0.78369140625\n",
      "Iteration 61020 Training loss 0.014677791856229305 Validation loss 0.021099213510751724 Accuracy 0.7822265625\n",
      "Iteration 61030 Training loss 0.013999054208397865 Validation loss 0.02099038101732731 Accuracy 0.7822265625\n",
      "Iteration 61040 Training loss 0.015216914936900139 Validation loss 0.021116025745868683 Accuracy 0.78125\n",
      "Iteration 61050 Training loss 0.017025545239448547 Validation loss 0.021116938441991806 Accuracy 0.78125\n",
      "Iteration 61060 Training loss 0.012144983746111393 Validation loss 0.020946327596902847 Accuracy 0.783203125\n",
      "Iteration 61070 Training loss 0.015038111247122288 Validation loss 0.02114918828010559 Accuracy 0.78173828125\n",
      "Iteration 61080 Training loss 0.014191914349794388 Validation loss 0.020982934162020683 Accuracy 0.78369140625\n",
      "Iteration 61090 Training loss 0.014045135118067265 Validation loss 0.020990055054426193 Accuracy 0.7822265625\n",
      "Iteration 61100 Training loss 0.012240329757332802 Validation loss 0.020912690088152885 Accuracy 0.78369140625\n",
      "Iteration 61110 Training loss 0.010941182263195515 Validation loss 0.02090483345091343 Accuracy 0.7841796875\n",
      "Iteration 61120 Training loss 0.014631974510848522 Validation loss 0.020910631865262985 Accuracy 0.7841796875\n",
      "Iteration 61130 Training loss 0.012216724455356598 Validation loss 0.02072506956756115 Accuracy 0.7861328125\n",
      "Iteration 61140 Training loss 0.01771022379398346 Validation loss 0.02112508937716484 Accuracy 0.7822265625\n",
      "Iteration 61150 Training loss 0.01249845139682293 Validation loss 0.021000487729907036 Accuracy 0.783203125\n",
      "Iteration 61160 Training loss 0.01348897535353899 Validation loss 0.021144572645425797 Accuracy 0.78125\n",
      "Iteration 61170 Training loss 0.016791095957159996 Validation loss 0.021594569087028503 Accuracy 0.77685546875\n",
      "Iteration 61180 Training loss 0.015285913832485676 Validation loss 0.020928988233208656 Accuracy 0.7841796875\n",
      "Iteration 61190 Training loss 0.012300621718168259 Validation loss 0.021590562537312508 Accuracy 0.7763671875\n",
      "Iteration 61200 Training loss 0.015402084216475487 Validation loss 0.021247953176498413 Accuracy 0.78076171875\n",
      "Iteration 61210 Training loss 0.015096486546099186 Validation loss 0.021340424194931984 Accuracy 0.7802734375\n",
      "Iteration 61220 Training loss 0.012418495491147041 Validation loss 0.02121719717979431 Accuracy 0.78076171875\n",
      "Iteration 61230 Training loss 0.012147228233516216 Validation loss 0.02129058539867401 Accuracy 0.77978515625\n",
      "Iteration 61240 Training loss 0.01654844358563423 Validation loss 0.021045971661806107 Accuracy 0.78271484375\n",
      "Iteration 61250 Training loss 0.014140890911221504 Validation loss 0.02114712819457054 Accuracy 0.7822265625\n",
      "Iteration 61260 Training loss 0.015228169970214367 Validation loss 0.02082725614309311 Accuracy 0.78466796875\n",
      "Iteration 61270 Training loss 0.01384451799094677 Validation loss 0.021124476566910744 Accuracy 0.78173828125\n",
      "Iteration 61280 Training loss 0.0157933346927166 Validation loss 0.02188422717154026 Accuracy 0.775390625\n",
      "Iteration 61290 Training loss 0.014939386397600174 Validation loss 0.021158957853913307 Accuracy 0.78271484375\n",
      "Iteration 61300 Training loss 0.012955727986991405 Validation loss 0.02097245864570141 Accuracy 0.78369140625\n",
      "Iteration 61310 Training loss 0.014788101427257061 Validation loss 0.021066930145025253 Accuracy 0.7822265625\n",
      "Iteration 61320 Training loss 0.012934865429997444 Validation loss 0.0209298525005579 Accuracy 0.78369140625\n",
      "Iteration 61330 Training loss 0.015331720933318138 Validation loss 0.021129323169589043 Accuracy 0.7822265625\n",
      "Iteration 61340 Training loss 0.014313510619103909 Validation loss 0.020997539162635803 Accuracy 0.78271484375\n",
      "Iteration 61350 Training loss 0.015059199184179306 Validation loss 0.02079625427722931 Accuracy 0.78466796875\n",
      "Iteration 61360 Training loss 0.01777147687971592 Validation loss 0.021042995154857635 Accuracy 0.78125\n",
      "Iteration 61370 Training loss 0.014486335217952728 Validation loss 0.02085479348897934 Accuracy 0.78466796875\n",
      "Iteration 61380 Training loss 0.012823237106204033 Validation loss 0.02110917866230011 Accuracy 0.78173828125\n",
      "Iteration 61390 Training loss 0.014479226432740688 Validation loss 0.02090151607990265 Accuracy 0.7841796875\n",
      "Iteration 61400 Training loss 0.012899103574454784 Validation loss 0.02099253423511982 Accuracy 0.78271484375\n",
      "Iteration 61410 Training loss 0.014548379927873611 Validation loss 0.02104208618402481 Accuracy 0.78369140625\n",
      "Iteration 61420 Training loss 0.013653954491019249 Validation loss 0.020889608189463615 Accuracy 0.7841796875\n",
      "Iteration 61430 Training loss 0.014392714016139507 Validation loss 0.021424852311611176 Accuracy 0.779296875\n",
      "Iteration 61440 Training loss 0.016169220209121704 Validation loss 0.020983926951885223 Accuracy 0.78369140625\n",
      "Iteration 61450 Training loss 0.01425243355333805 Validation loss 0.021076444536447525 Accuracy 0.78271484375\n",
      "Iteration 61460 Training loss 0.015360081568360329 Validation loss 0.02106226049363613 Accuracy 0.78271484375\n",
      "Iteration 61470 Training loss 0.014096786268055439 Validation loss 0.020936239510774612 Accuracy 0.78466796875\n",
      "Iteration 61480 Training loss 0.014205604791641235 Validation loss 0.02162797749042511 Accuracy 0.77783203125\n",
      "Iteration 61490 Training loss 0.015930060297250748 Validation loss 0.02119450643658638 Accuracy 0.78076171875\n",
      "Iteration 61500 Training loss 0.016874656081199646 Validation loss 0.021338459104299545 Accuracy 0.77978515625\n",
      "Iteration 61510 Training loss 0.015457992441952229 Validation loss 0.02100568823516369 Accuracy 0.7822265625\n",
      "Iteration 61520 Training loss 0.01502248365432024 Validation loss 0.021045664325356483 Accuracy 0.78369140625\n",
      "Iteration 61530 Training loss 0.013632725924253464 Validation loss 0.02119956910610199 Accuracy 0.78173828125\n",
      "Iteration 61540 Training loss 0.01506953127682209 Validation loss 0.02102886699140072 Accuracy 0.783203125\n",
      "Iteration 61550 Training loss 0.015288327820599079 Validation loss 0.020792342722415924 Accuracy 0.7841796875\n",
      "Iteration 61560 Training loss 0.0146714486181736 Validation loss 0.020868618041276932 Accuracy 0.78369140625\n",
      "Iteration 61570 Training loss 0.017018288373947144 Validation loss 0.020829448476433754 Accuracy 0.7841796875\n",
      "Iteration 61580 Training loss 0.014402552507817745 Validation loss 0.020763812586665154 Accuracy 0.78515625\n",
      "Iteration 61590 Training loss 0.01436157338321209 Validation loss 0.020961688831448555 Accuracy 0.78369140625\n",
      "Iteration 61600 Training loss 0.01615913398563862 Validation loss 0.021010439842939377 Accuracy 0.78369140625\n",
      "Iteration 61610 Training loss 0.013239370658993721 Validation loss 0.021101301535964012 Accuracy 0.78271484375\n",
      "Iteration 61620 Training loss 0.014964372850954533 Validation loss 0.021207502111792564 Accuracy 0.78076171875\n",
      "Iteration 61630 Training loss 0.016498839482665062 Validation loss 0.020744819194078445 Accuracy 0.7861328125\n",
      "Iteration 61640 Training loss 0.01429816149175167 Validation loss 0.020955035462975502 Accuracy 0.783203125\n",
      "Iteration 61650 Training loss 0.013173899613320827 Validation loss 0.020777452737092972 Accuracy 0.78564453125\n",
      "Iteration 61660 Training loss 0.014907533302903175 Validation loss 0.021384628489613533 Accuracy 0.7802734375\n",
      "Iteration 61670 Training loss 0.015881357714533806 Validation loss 0.021357005462050438 Accuracy 0.78076171875\n",
      "Iteration 61680 Training loss 0.018684806302189827 Validation loss 0.022100698202848434 Accuracy 0.77294921875\n",
      "Iteration 61690 Training loss 0.017472149804234505 Validation loss 0.02175348997116089 Accuracy 0.7763671875\n",
      "Iteration 61700 Training loss 0.01604665070772171 Validation loss 0.021246103569865227 Accuracy 0.77978515625\n",
      "Iteration 61710 Training loss 0.013990629464387894 Validation loss 0.020951751619577408 Accuracy 0.7841796875\n",
      "Iteration 61720 Training loss 0.0125794168561697 Validation loss 0.021157270297408104 Accuracy 0.7822265625\n",
      "Iteration 61730 Training loss 0.014918411150574684 Validation loss 0.020936571061611176 Accuracy 0.7841796875\n",
      "Iteration 61740 Training loss 0.014227546751499176 Validation loss 0.02108677662909031 Accuracy 0.7822265625\n",
      "Iteration 61750 Training loss 0.013293064199388027 Validation loss 0.021070966497063637 Accuracy 0.78173828125\n",
      "Iteration 61760 Training loss 0.016651790589094162 Validation loss 0.021060790866613388 Accuracy 0.7822265625\n",
      "Iteration 61770 Training loss 0.011585789732635021 Validation loss 0.02115885727107525 Accuracy 0.7822265625\n",
      "Iteration 61780 Training loss 0.01598186418414116 Validation loss 0.02088659629225731 Accuracy 0.7841796875\n",
      "Iteration 61790 Training loss 0.013391266576945782 Validation loss 0.021043559536337852 Accuracy 0.783203125\n",
      "Iteration 61800 Training loss 0.016816522926092148 Validation loss 0.02123667486011982 Accuracy 0.78125\n",
      "Iteration 61810 Training loss 0.014726447872817516 Validation loss 0.02122710458934307 Accuracy 0.7802734375\n",
      "Iteration 61820 Training loss 0.016372477635741234 Validation loss 0.020906517282128334 Accuracy 0.78466796875\n",
      "Iteration 61830 Training loss 0.014009215869009495 Validation loss 0.021171998232603073 Accuracy 0.78173828125\n",
      "Iteration 61840 Training loss 0.017225027084350586 Validation loss 0.021056532859802246 Accuracy 0.78369140625\n",
      "Iteration 61850 Training loss 0.013249079696834087 Validation loss 0.02104310318827629 Accuracy 0.78271484375\n",
      "Iteration 61860 Training loss 0.013252421282231808 Validation loss 0.021167714148759842 Accuracy 0.7822265625\n",
      "Iteration 61870 Training loss 0.016080912202596664 Validation loss 0.020884627476334572 Accuracy 0.7841796875\n",
      "Iteration 61880 Training loss 0.018308375030755997 Validation loss 0.021169302985072136 Accuracy 0.78173828125\n",
      "Iteration 61890 Training loss 0.014887109398841858 Validation loss 0.02141864039003849 Accuracy 0.77880859375\n",
      "Iteration 61900 Training loss 0.01515988353639841 Validation loss 0.021045101806521416 Accuracy 0.78369140625\n",
      "Iteration 61910 Training loss 0.014860010705888271 Validation loss 0.021025434136390686 Accuracy 0.783203125\n",
      "Iteration 61920 Training loss 0.015580740757286549 Validation loss 0.02133532240986824 Accuracy 0.78173828125\n",
      "Iteration 61930 Training loss 0.012196797877550125 Validation loss 0.020750805735588074 Accuracy 0.7861328125\n",
      "Iteration 61940 Training loss 0.014666958712041378 Validation loss 0.020885538309812546 Accuracy 0.78369140625\n",
      "Iteration 61950 Training loss 0.01438823901116848 Validation loss 0.020776424556970596 Accuracy 0.78564453125\n",
      "Iteration 61960 Training loss 0.012081513181328773 Validation loss 0.02121468260884285 Accuracy 0.78125\n",
      "Iteration 61970 Training loss 0.01640501618385315 Validation loss 0.02104855142533779 Accuracy 0.7822265625\n",
      "Iteration 61980 Training loss 0.014502603560686111 Validation loss 0.02110622078180313 Accuracy 0.78173828125\n",
      "Iteration 61990 Training loss 0.012772432528436184 Validation loss 0.0214309711009264 Accuracy 0.7783203125\n",
      "Iteration 62000 Training loss 0.012602361850440502 Validation loss 0.020884664729237556 Accuracy 0.78369140625\n",
      "Iteration 62010 Training loss 0.012866170145571232 Validation loss 0.02090846374630928 Accuracy 0.78369140625\n",
      "Iteration 62020 Training loss 0.012611028738319874 Validation loss 0.021090470254421234 Accuracy 0.78173828125\n",
      "Iteration 62030 Training loss 0.014157608151435852 Validation loss 0.020998109132051468 Accuracy 0.78369140625\n",
      "Iteration 62040 Training loss 0.014733043499290943 Validation loss 0.020792609080672264 Accuracy 0.78466796875\n",
      "Iteration 62050 Training loss 0.013913665898144245 Validation loss 0.021163344383239746 Accuracy 0.78173828125\n",
      "Iteration 62060 Training loss 0.015417470596730709 Validation loss 0.021878376603126526 Accuracy 0.77392578125\n",
      "Iteration 62070 Training loss 0.01365990936756134 Validation loss 0.020836517214775085 Accuracy 0.7841796875\n",
      "Iteration 62080 Training loss 0.015031348913908005 Validation loss 0.02133849635720253 Accuracy 0.77783203125\n",
      "Iteration 62090 Training loss 0.01625780016183853 Validation loss 0.021145381033420563 Accuracy 0.7822265625\n",
      "Iteration 62100 Training loss 0.013288512825965881 Validation loss 0.02181510627269745 Accuracy 0.775390625\n",
      "Iteration 62110 Training loss 0.013963713310658932 Validation loss 0.020808733999729156 Accuracy 0.78466796875\n",
      "Iteration 62120 Training loss 0.01417430117726326 Validation loss 0.020876482129096985 Accuracy 0.7841796875\n",
      "Iteration 62130 Training loss 0.014512944035232067 Validation loss 0.020842844620347023 Accuracy 0.78515625\n",
      "Iteration 62140 Training loss 0.014226982370018959 Validation loss 0.021030792966485023 Accuracy 0.78271484375\n",
      "Iteration 62150 Training loss 0.014428260736167431 Validation loss 0.020860793069005013 Accuracy 0.78369140625\n",
      "Iteration 62160 Training loss 0.013719551265239716 Validation loss 0.021028665825724602 Accuracy 0.78271484375\n",
      "Iteration 62170 Training loss 0.016694094985723495 Validation loss 0.02083493024110794 Accuracy 0.78515625\n",
      "Iteration 62180 Training loss 0.01371513307094574 Validation loss 0.02084886096417904 Accuracy 0.78515625\n",
      "Iteration 62190 Training loss 0.015188677236437798 Validation loss 0.021041065454483032 Accuracy 0.78271484375\n",
      "Iteration 62200 Training loss 0.013318926095962524 Validation loss 0.0208659078925848 Accuracy 0.78466796875\n",
      "Iteration 62210 Training loss 0.013757489621639252 Validation loss 0.0208902508020401 Accuracy 0.78466796875\n",
      "Iteration 62220 Training loss 0.012933703139424324 Validation loss 0.021244728937745094 Accuracy 0.78173828125\n",
      "Iteration 62230 Training loss 0.014688787050545216 Validation loss 0.020914452150464058 Accuracy 0.783203125\n",
      "Iteration 62240 Training loss 0.015312811359763145 Validation loss 0.02079436555504799 Accuracy 0.78515625\n",
      "Iteration 62250 Training loss 0.01345043070614338 Validation loss 0.02116473577916622 Accuracy 0.78076171875\n",
      "Iteration 62260 Training loss 0.016335178166627884 Validation loss 0.02085808478295803 Accuracy 0.78369140625\n",
      "Iteration 62270 Training loss 0.014032110571861267 Validation loss 0.021200593560934067 Accuracy 0.77978515625\n",
      "Iteration 62280 Training loss 0.011883776634931564 Validation loss 0.02094094268977642 Accuracy 0.783203125\n",
      "Iteration 62290 Training loss 0.011832206510007381 Validation loss 0.020977254956960678 Accuracy 0.78369140625\n",
      "Iteration 62300 Training loss 0.013266703113913536 Validation loss 0.020763905718922615 Accuracy 0.78466796875\n",
      "Iteration 62310 Training loss 0.015475061722099781 Validation loss 0.021100031211972237 Accuracy 0.78173828125\n",
      "Iteration 62320 Training loss 0.013928358443081379 Validation loss 0.021233465522527695 Accuracy 0.78173828125\n",
      "Iteration 62330 Training loss 0.014205881394445896 Validation loss 0.020717667415738106 Accuracy 0.78564453125\n",
      "Iteration 62340 Training loss 0.015174341388046741 Validation loss 0.02158074826002121 Accuracy 0.775390625\n",
      "Iteration 62350 Training loss 0.014819788746535778 Validation loss 0.02081536315381527 Accuracy 0.78466796875\n",
      "Iteration 62360 Training loss 0.013157347217202187 Validation loss 0.020923398435115814 Accuracy 0.78369140625\n",
      "Iteration 62370 Training loss 0.011601271107792854 Validation loss 0.02098139561712742 Accuracy 0.783203125\n",
      "Iteration 62380 Training loss 0.014165906235575676 Validation loss 0.020980877801775932 Accuracy 0.78271484375\n",
      "Iteration 62390 Training loss 0.014875924214720726 Validation loss 0.02102443389594555 Accuracy 0.7822265625\n",
      "Iteration 62400 Training loss 0.014242962934076786 Validation loss 0.02095954492688179 Accuracy 0.7822265625\n",
      "Iteration 62410 Training loss 0.012993224896490574 Validation loss 0.020897021517157555 Accuracy 0.7841796875\n",
      "Iteration 62420 Training loss 0.01667424663901329 Validation loss 0.02085941843688488 Accuracy 0.7841796875\n",
      "Iteration 62430 Training loss 0.014287068508565426 Validation loss 0.020819956436753273 Accuracy 0.78466796875\n",
      "Iteration 62440 Training loss 0.014674024656414986 Validation loss 0.021328894421458244 Accuracy 0.78125\n",
      "Iteration 62450 Training loss 0.01609117165207863 Validation loss 0.020875414833426476 Accuracy 0.78369140625\n",
      "Iteration 62460 Training loss 0.015222233720123768 Validation loss 0.021217726171016693 Accuracy 0.78125\n",
      "Iteration 62470 Training loss 0.016815390437841415 Validation loss 0.021348964422941208 Accuracy 0.77880859375\n",
      "Iteration 62480 Training loss 0.015535886399447918 Validation loss 0.021057216450572014 Accuracy 0.7822265625\n",
      "Iteration 62490 Training loss 0.013008341193199158 Validation loss 0.02105458453297615 Accuracy 0.783203125\n",
      "Iteration 62500 Training loss 0.017741918563842773 Validation loss 0.02144249528646469 Accuracy 0.77783203125\n",
      "Iteration 62510 Training loss 0.01569778099656105 Validation loss 0.02094196528196335 Accuracy 0.783203125\n",
      "Iteration 62520 Training loss 0.013206705451011658 Validation loss 0.021026285365223885 Accuracy 0.78271484375\n",
      "Iteration 62530 Training loss 0.014789578504860401 Validation loss 0.02149108052253723 Accuracy 0.77734375\n",
      "Iteration 62540 Training loss 0.012120919302105904 Validation loss 0.021024193614721298 Accuracy 0.78271484375\n",
      "Iteration 62550 Training loss 0.01327500119805336 Validation loss 0.020856346935033798 Accuracy 0.783203125\n",
      "Iteration 62560 Training loss 0.013755498453974724 Validation loss 0.020850766450166702 Accuracy 0.78369140625\n",
      "Iteration 62570 Training loss 0.014863710850477219 Validation loss 0.020884335041046143 Accuracy 0.78369140625\n",
      "Iteration 62580 Training loss 0.013569884933531284 Validation loss 0.021181831136345863 Accuracy 0.78173828125\n",
      "Iteration 62590 Training loss 0.011995396576821804 Validation loss 0.021283142268657684 Accuracy 0.78076171875\n",
      "Iteration 62600 Training loss 0.014652094803750515 Validation loss 0.021098868921399117 Accuracy 0.78271484375\n",
      "Iteration 62610 Training loss 0.014296521432697773 Validation loss 0.021058866754174232 Accuracy 0.78271484375\n",
      "Iteration 62620 Training loss 0.012216380797326565 Validation loss 0.0208169873803854 Accuracy 0.78466796875\n",
      "Iteration 62630 Training loss 0.014536157250404358 Validation loss 0.021514924243092537 Accuracy 0.77783203125\n",
      "Iteration 62640 Training loss 0.01570156030356884 Validation loss 0.02100222557783127 Accuracy 0.783203125\n",
      "Iteration 62650 Training loss 0.015489269979298115 Validation loss 0.020840756595134735 Accuracy 0.7841796875\n",
      "Iteration 62660 Training loss 0.01280570961534977 Validation loss 0.02099907584488392 Accuracy 0.783203125\n",
      "Iteration 62670 Training loss 0.017080577090382576 Validation loss 0.02096867375075817 Accuracy 0.78271484375\n",
      "Iteration 62680 Training loss 0.01683564856648445 Validation loss 0.020896151661872864 Accuracy 0.7822265625\n",
      "Iteration 62690 Training loss 0.015956537798047066 Validation loss 0.02108163945376873 Accuracy 0.78271484375\n",
      "Iteration 62700 Training loss 0.014480305835604668 Validation loss 0.020862260833382607 Accuracy 0.78466796875\n",
      "Iteration 62710 Training loss 0.012720156461000443 Validation loss 0.020875724032521248 Accuracy 0.7841796875\n",
      "Iteration 62720 Training loss 0.015962466597557068 Validation loss 0.02102767489850521 Accuracy 0.78271484375\n",
      "Iteration 62730 Training loss 0.014243380166590214 Validation loss 0.021109648048877716 Accuracy 0.78173828125\n",
      "Iteration 62740 Training loss 0.01434358675032854 Validation loss 0.021173905581235886 Accuracy 0.78173828125\n",
      "Iteration 62750 Training loss 0.015428313054144382 Validation loss 0.021286064758896828 Accuracy 0.78125\n",
      "Iteration 62760 Training loss 0.013182487338781357 Validation loss 0.021137241274118423 Accuracy 0.78271484375\n",
      "Iteration 62770 Training loss 0.010997177101671696 Validation loss 0.021222488954663277 Accuracy 0.78173828125\n",
      "Iteration 62780 Training loss 0.014450334943830967 Validation loss 0.020929550752043724 Accuracy 0.7841796875\n",
      "Iteration 62790 Training loss 0.013252487406134605 Validation loss 0.02096971683204174 Accuracy 0.78369140625\n",
      "Iteration 62800 Training loss 0.013110951520502567 Validation loss 0.02106725238263607 Accuracy 0.78173828125\n",
      "Iteration 62810 Training loss 0.015976455062627792 Validation loss 0.020914696156978607 Accuracy 0.78466796875\n",
      "Iteration 62820 Training loss 0.014179191552102566 Validation loss 0.02145293727517128 Accuracy 0.77880859375\n",
      "Iteration 62830 Training loss 0.01493794471025467 Validation loss 0.020960869267582893 Accuracy 0.7841796875\n",
      "Iteration 62840 Training loss 0.01657647266983986 Validation loss 0.021275591105222702 Accuracy 0.78076171875\n",
      "Iteration 62850 Training loss 0.014212967827916145 Validation loss 0.020889418199658394 Accuracy 0.7841796875\n",
      "Iteration 62860 Training loss 0.010618374682962894 Validation loss 0.021156318485736847 Accuracy 0.78076171875\n",
      "Iteration 62870 Training loss 0.014929208904504776 Validation loss 0.020720375701785088 Accuracy 0.78515625\n",
      "Iteration 62880 Training loss 0.017678240314126015 Validation loss 0.020860804244875908 Accuracy 0.7841796875\n",
      "Iteration 62890 Training loss 0.01358624268323183 Validation loss 0.021312931552529335 Accuracy 0.7802734375\n",
      "Iteration 62900 Training loss 0.01335059106349945 Validation loss 0.021197358146309853 Accuracy 0.78125\n",
      "Iteration 62910 Training loss 0.01568947359919548 Validation loss 0.021217817440629005 Accuracy 0.77978515625\n",
      "Iteration 62920 Training loss 0.015709450468420982 Validation loss 0.021323882043361664 Accuracy 0.7802734375\n",
      "Iteration 62930 Training loss 0.011114844121038914 Validation loss 0.02088373899459839 Accuracy 0.7841796875\n",
      "Iteration 62940 Training loss 0.012862122617661953 Validation loss 0.020902670919895172 Accuracy 0.78271484375\n",
      "Iteration 62950 Training loss 0.014544219709932804 Validation loss 0.0208588894456625 Accuracy 0.78466796875\n",
      "Iteration 62960 Training loss 0.014447741210460663 Validation loss 0.020805194973945618 Accuracy 0.78466796875\n",
      "Iteration 62970 Training loss 0.01302762608975172 Validation loss 0.021226916462183 Accuracy 0.78076171875\n",
      "Iteration 62980 Training loss 0.014516419731080532 Validation loss 0.02106229029595852 Accuracy 0.7822265625\n",
      "Iteration 62990 Training loss 0.016463294625282288 Validation loss 0.021102817729115486 Accuracy 0.78271484375\n",
      "Iteration 63000 Training loss 0.012000922113656998 Validation loss 0.020958561450242996 Accuracy 0.783203125\n",
      "Iteration 63010 Training loss 0.01486713532358408 Validation loss 0.02120301127433777 Accuracy 0.78173828125\n",
      "Iteration 63020 Training loss 0.013972227461636066 Validation loss 0.0211429875344038 Accuracy 0.783203125\n",
      "Iteration 63030 Training loss 0.01582416705787182 Validation loss 0.021078381687402725 Accuracy 0.78125\n",
      "Iteration 63040 Training loss 0.013586897403001785 Validation loss 0.021157603710889816 Accuracy 0.78271484375\n",
      "Iteration 63050 Training loss 0.01485198363661766 Validation loss 0.021308470517396927 Accuracy 0.78076171875\n",
      "Iteration 63060 Training loss 0.01395154558122158 Validation loss 0.021837186068296432 Accuracy 0.7744140625\n",
      "Iteration 63070 Training loss 0.012394185177981853 Validation loss 0.02100461535155773 Accuracy 0.783203125\n",
      "Iteration 63080 Training loss 0.01466547790914774 Validation loss 0.02075469121336937 Accuracy 0.78515625\n",
      "Iteration 63090 Training loss 0.013119004666805267 Validation loss 0.02089211344718933 Accuracy 0.78369140625\n",
      "Iteration 63100 Training loss 0.013146916404366493 Validation loss 0.021107187494635582 Accuracy 0.78173828125\n",
      "Iteration 63110 Training loss 0.014105260372161865 Validation loss 0.021351533010601997 Accuracy 0.7802734375\n",
      "Iteration 63120 Training loss 0.013486291281878948 Validation loss 0.021525384858250618 Accuracy 0.7783203125\n",
      "Iteration 63130 Training loss 0.013868733309209347 Validation loss 0.021058043465018272 Accuracy 0.78271484375\n",
      "Iteration 63140 Training loss 0.014871866442263126 Validation loss 0.020883627235889435 Accuracy 0.78466796875\n",
      "Iteration 63150 Training loss 0.012618308886885643 Validation loss 0.020990636199712753 Accuracy 0.78271484375\n",
      "Iteration 63160 Training loss 0.014142808504402637 Validation loss 0.020872050896286964 Accuracy 0.7841796875\n",
      "Iteration 63170 Training loss 0.015210503712296486 Validation loss 0.020908832550048828 Accuracy 0.783203125\n",
      "Iteration 63180 Training loss 0.012690380215644836 Validation loss 0.0206980612128973 Accuracy 0.78515625\n",
      "Iteration 63190 Training loss 0.013912209309637547 Validation loss 0.020759785547852516 Accuracy 0.78466796875\n",
      "Iteration 63200 Training loss 0.014644055627286434 Validation loss 0.02097425051033497 Accuracy 0.7841796875\n",
      "Iteration 63210 Training loss 0.014431769959628582 Validation loss 0.02100847288966179 Accuracy 0.783203125\n",
      "Iteration 63220 Training loss 0.014688300900161266 Validation loss 0.021139709278941154 Accuracy 0.78173828125\n",
      "Iteration 63230 Training loss 0.011421002447605133 Validation loss 0.021033432334661484 Accuracy 0.78125\n",
      "Iteration 63240 Training loss 0.017397060990333557 Validation loss 0.02115837298333645 Accuracy 0.78173828125\n",
      "Iteration 63250 Training loss 0.016456611454486847 Validation loss 0.02098880149424076 Accuracy 0.78173828125\n",
      "Iteration 63260 Training loss 0.012560986913740635 Validation loss 0.021173041313886642 Accuracy 0.78125\n",
      "Iteration 63270 Training loss 0.015227851457893848 Validation loss 0.021097933873534203 Accuracy 0.78271484375\n",
      "Iteration 63280 Training loss 0.01485267374664545 Validation loss 0.02150973118841648 Accuracy 0.7783203125\n",
      "Iteration 63290 Training loss 0.01427894365042448 Validation loss 0.021092377603054047 Accuracy 0.78271484375\n",
      "Iteration 63300 Training loss 0.01479878555983305 Validation loss 0.021049218252301216 Accuracy 0.78173828125\n",
      "Iteration 63310 Training loss 0.012041411362588406 Validation loss 0.02098063752055168 Accuracy 0.78271484375\n",
      "Iteration 63320 Training loss 0.014582223258912563 Validation loss 0.02083081193268299 Accuracy 0.78369140625\n",
      "Iteration 63330 Training loss 0.01134552899748087 Validation loss 0.02143540233373642 Accuracy 0.7783203125\n",
      "Iteration 63340 Training loss 0.014768910594284534 Validation loss 0.021293846890330315 Accuracy 0.7802734375\n",
      "Iteration 63350 Training loss 0.017492976039648056 Validation loss 0.021286528557538986 Accuracy 0.77978515625\n",
      "Iteration 63360 Training loss 0.013655693270266056 Validation loss 0.021034305915236473 Accuracy 0.78173828125\n",
      "Iteration 63370 Training loss 0.012010213918983936 Validation loss 0.020751068368554115 Accuracy 0.78369140625\n",
      "Iteration 63380 Training loss 0.015618682838976383 Validation loss 0.0210630651563406 Accuracy 0.78173828125\n",
      "Iteration 63390 Training loss 0.016378164291381836 Validation loss 0.021234627813100815 Accuracy 0.78173828125\n",
      "Iteration 63400 Training loss 0.013919100165367126 Validation loss 0.021074678748846054 Accuracy 0.78125\n",
      "Iteration 63410 Training loss 0.014429008588194847 Validation loss 0.02105146460235119 Accuracy 0.783203125\n",
      "Iteration 63420 Training loss 0.015391170047223568 Validation loss 0.021172264590859413 Accuracy 0.78173828125\n",
      "Iteration 63430 Training loss 0.014642425812780857 Validation loss 0.0209623072296381 Accuracy 0.7841796875\n",
      "Iteration 63440 Training loss 0.015740156173706055 Validation loss 0.02085672877728939 Accuracy 0.78369140625\n",
      "Iteration 63450 Training loss 0.011519864201545715 Validation loss 0.020829012617468834 Accuracy 0.78466796875\n",
      "Iteration 63460 Training loss 0.014835772104561329 Validation loss 0.020832305774092674 Accuracy 0.7841796875\n",
      "Iteration 63470 Training loss 0.013616771437227726 Validation loss 0.02090461365878582 Accuracy 0.78369140625\n",
      "Iteration 63480 Training loss 0.012746202759444714 Validation loss 0.021096646785736084 Accuracy 0.7822265625\n",
      "Iteration 63490 Training loss 0.012500505894422531 Validation loss 0.021070050075650215 Accuracy 0.7822265625\n",
      "Iteration 63500 Training loss 0.012855135835707188 Validation loss 0.021093981340527534 Accuracy 0.78173828125\n",
      "Iteration 63510 Training loss 0.013625330291688442 Validation loss 0.021035922691226006 Accuracy 0.78369140625\n",
      "Iteration 63520 Training loss 0.01579057238996029 Validation loss 0.020873254165053368 Accuracy 0.78271484375\n",
      "Iteration 63530 Training loss 0.016848580911755562 Validation loss 0.021068934351205826 Accuracy 0.78271484375\n",
      "Iteration 63540 Training loss 0.014431511983275414 Validation loss 0.021069489419460297 Accuracy 0.7822265625\n",
      "Iteration 63550 Training loss 0.013810895383358002 Validation loss 0.020919518545269966 Accuracy 0.783203125\n",
      "Iteration 63560 Training loss 0.01377373468130827 Validation loss 0.020905373618006706 Accuracy 0.78369140625\n",
      "Iteration 63570 Training loss 0.015105494298040867 Validation loss 0.02081303671002388 Accuracy 0.7841796875\n",
      "Iteration 63580 Training loss 0.0159192755818367 Validation loss 0.02110103890299797 Accuracy 0.7822265625\n",
      "Iteration 63590 Training loss 0.012845395132899284 Validation loss 0.02093822881579399 Accuracy 0.783203125\n",
      "Iteration 63600 Training loss 0.015918195247650146 Validation loss 0.021664569154381752 Accuracy 0.7763671875\n",
      "Iteration 63610 Training loss 0.015444507822394371 Validation loss 0.021150998771190643 Accuracy 0.78125\n",
      "Iteration 63620 Training loss 0.014848841354250908 Validation loss 0.020985687151551247 Accuracy 0.78271484375\n",
      "Iteration 63630 Training loss 0.012401415035128593 Validation loss 0.02086174301803112 Accuracy 0.78466796875\n",
      "Iteration 63640 Training loss 0.015054559335112572 Validation loss 0.02116266079246998 Accuracy 0.78125\n",
      "Iteration 63650 Training loss 0.014706510119140148 Validation loss 0.021131793037056923 Accuracy 0.78271484375\n",
      "Iteration 63660 Training loss 0.0154535798355937 Validation loss 0.02193811908364296 Accuracy 0.7744140625\n",
      "Iteration 63670 Training loss 0.015986936166882515 Validation loss 0.020926129072904587 Accuracy 0.78271484375\n",
      "Iteration 63680 Training loss 0.011353058740496635 Validation loss 0.02124211937189102 Accuracy 0.78125\n",
      "Iteration 63690 Training loss 0.014065474271774292 Validation loss 0.021037088707089424 Accuracy 0.7841796875\n",
      "Iteration 63700 Training loss 0.014836245216429234 Validation loss 0.021123047918081284 Accuracy 0.7822265625\n",
      "Iteration 63710 Training loss 0.012386774644255638 Validation loss 0.020985513925552368 Accuracy 0.78369140625\n",
      "Iteration 63720 Training loss 0.012756853364408016 Validation loss 0.020793825387954712 Accuracy 0.78515625\n",
      "Iteration 63730 Training loss 0.013465571217238903 Validation loss 0.021004466339945793 Accuracy 0.78271484375\n",
      "Iteration 63740 Training loss 0.01359200943261385 Validation loss 0.021219713613390923 Accuracy 0.78076171875\n",
      "Iteration 63750 Training loss 0.011665334925055504 Validation loss 0.021012306213378906 Accuracy 0.7822265625\n",
      "Iteration 63760 Training loss 0.013094192370772362 Validation loss 0.021008118987083435 Accuracy 0.78369140625\n",
      "Iteration 63770 Training loss 0.015351702459156513 Validation loss 0.020989984273910522 Accuracy 0.783203125\n",
      "Iteration 63780 Training loss 0.016561148688197136 Validation loss 0.021036066114902496 Accuracy 0.78271484375\n",
      "Iteration 63790 Training loss 0.014302955009043217 Validation loss 0.02108626812696457 Accuracy 0.78173828125\n",
      "Iteration 63800 Training loss 0.01312197744846344 Validation loss 0.02102484740316868 Accuracy 0.783203125\n",
      "Iteration 63810 Training loss 0.014383899979293346 Validation loss 0.021163664758205414 Accuracy 0.78076171875\n",
      "Iteration 63820 Training loss 0.012350259348750114 Validation loss 0.02113186940550804 Accuracy 0.7822265625\n",
      "Iteration 63830 Training loss 0.012878506444394588 Validation loss 0.021034061908721924 Accuracy 0.783203125\n",
      "Iteration 63840 Training loss 0.013380298390984535 Validation loss 0.021669739857316017 Accuracy 0.7763671875\n",
      "Iteration 63850 Training loss 0.01449554692953825 Validation loss 0.021096304059028625 Accuracy 0.7822265625\n",
      "Iteration 63860 Training loss 0.014919477514922619 Validation loss 0.021740026772022247 Accuracy 0.775390625\n",
      "Iteration 63870 Training loss 0.014804122969508171 Validation loss 0.021017983555793762 Accuracy 0.783203125\n",
      "Iteration 63880 Training loss 0.014827143400907516 Validation loss 0.021304722875356674 Accuracy 0.7802734375\n",
      "Iteration 63890 Training loss 0.014350038953125477 Validation loss 0.020979149267077446 Accuracy 0.783203125\n",
      "Iteration 63900 Training loss 0.014311917126178741 Validation loss 0.021345965564250946 Accuracy 0.7802734375\n",
      "Iteration 63910 Training loss 0.01398614514619112 Validation loss 0.021374380216002464 Accuracy 0.7802734375\n",
      "Iteration 63920 Training loss 0.014266119338572025 Validation loss 0.021543696522712708 Accuracy 0.77734375\n",
      "Iteration 63930 Training loss 0.01443399116396904 Validation loss 0.020910095423460007 Accuracy 0.78369140625\n",
      "Iteration 63940 Training loss 0.01272086426615715 Validation loss 0.02086333930492401 Accuracy 0.78369140625\n",
      "Iteration 63950 Training loss 0.013904069550335407 Validation loss 0.020769750699400902 Accuracy 0.78466796875\n",
      "Iteration 63960 Training loss 0.013767627999186516 Validation loss 0.021115383133292198 Accuracy 0.78173828125\n",
      "Iteration 63970 Training loss 0.014256101101636887 Validation loss 0.02084067277610302 Accuracy 0.78515625\n",
      "Iteration 63980 Training loss 0.015348277986049652 Validation loss 0.021048149093985558 Accuracy 0.78173828125\n",
      "Iteration 63990 Training loss 0.01325211487710476 Validation loss 0.0212369654327631 Accuracy 0.7802734375\n",
      "Iteration 64000 Training loss 0.015259571373462677 Validation loss 0.021080678328871727 Accuracy 0.783203125\n",
      "Iteration 64010 Training loss 0.014973615296185017 Validation loss 0.021144866943359375 Accuracy 0.78271484375\n",
      "Iteration 64020 Training loss 0.013840393163263798 Validation loss 0.020937202498316765 Accuracy 0.783203125\n",
      "Iteration 64030 Training loss 0.013496968895196915 Validation loss 0.02093343250453472 Accuracy 0.78369140625\n",
      "Iteration 64040 Training loss 0.011738130822777748 Validation loss 0.02110259234905243 Accuracy 0.7822265625\n",
      "Iteration 64050 Training loss 0.012770315632224083 Validation loss 0.021328164264559746 Accuracy 0.7802734375\n",
      "Iteration 64060 Training loss 0.01647823117673397 Validation loss 0.020991886034607887 Accuracy 0.783203125\n",
      "Iteration 64070 Training loss 0.012812664732336998 Validation loss 0.021232709288597107 Accuracy 0.78173828125\n",
      "Iteration 64080 Training loss 0.012504177168011665 Validation loss 0.020795803517103195 Accuracy 0.7841796875\n",
      "Iteration 64090 Training loss 0.014741738326847553 Validation loss 0.02082659676671028 Accuracy 0.78515625\n",
      "Iteration 64100 Training loss 0.013632583431899548 Validation loss 0.020929818972945213 Accuracy 0.78369140625\n",
      "Iteration 64110 Training loss 0.015476870350539684 Validation loss 0.020908139646053314 Accuracy 0.78369140625\n",
      "Iteration 64120 Training loss 0.013139032758772373 Validation loss 0.020938461646437645 Accuracy 0.78369140625\n",
      "Iteration 64130 Training loss 0.01199907436966896 Validation loss 0.021067289635539055 Accuracy 0.7822265625\n",
      "Iteration 64140 Training loss 0.012363254092633724 Validation loss 0.020935483276844025 Accuracy 0.783203125\n",
      "Iteration 64150 Training loss 0.01435205340385437 Validation loss 0.021296147257089615 Accuracy 0.77978515625\n",
      "Iteration 64160 Training loss 0.012699262239038944 Validation loss 0.02103850059211254 Accuracy 0.783203125\n",
      "Iteration 64170 Training loss 0.014274830929934978 Validation loss 0.021366849541664124 Accuracy 0.77978515625\n",
      "Iteration 64180 Training loss 0.01449224166572094 Validation loss 0.021094437688589096 Accuracy 0.78271484375\n",
      "Iteration 64190 Training loss 0.013398201204836369 Validation loss 0.020946279168128967 Accuracy 0.78369140625\n",
      "Iteration 64200 Training loss 0.01538009848445654 Validation loss 0.02105584181845188 Accuracy 0.78271484375\n",
      "Iteration 64210 Training loss 0.014445149339735508 Validation loss 0.0208592526614666 Accuracy 0.78369140625\n",
      "Iteration 64220 Training loss 0.014528317376971245 Validation loss 0.0209281537681818 Accuracy 0.78271484375\n",
      "Iteration 64230 Training loss 0.014504996128380299 Validation loss 0.021280130371451378 Accuracy 0.78076171875\n",
      "Iteration 64240 Training loss 0.0157348420470953 Validation loss 0.02099667489528656 Accuracy 0.78369140625\n",
      "Iteration 64250 Training loss 0.014666564762592316 Validation loss 0.02109583280980587 Accuracy 0.78173828125\n",
      "Iteration 64260 Training loss 0.013896804302930832 Validation loss 0.020940765738487244 Accuracy 0.78369140625\n",
      "Iteration 64270 Training loss 0.01343220379203558 Validation loss 0.02105448767542839 Accuracy 0.78271484375\n",
      "Iteration 64280 Training loss 0.015516933053731918 Validation loss 0.021173851564526558 Accuracy 0.78173828125\n",
      "Iteration 64290 Training loss 0.014679498970508575 Validation loss 0.021092567592859268 Accuracy 0.78271484375\n",
      "Iteration 64300 Training loss 0.011976336129009724 Validation loss 0.021032610908150673 Accuracy 0.7822265625\n",
      "Iteration 64310 Training loss 0.01609751582145691 Validation loss 0.021399013698101044 Accuracy 0.7783203125\n",
      "Iteration 64320 Training loss 0.013270052149891853 Validation loss 0.021143663674592972 Accuracy 0.7822265625\n",
      "Iteration 64330 Training loss 0.015434153378009796 Validation loss 0.021083449944853783 Accuracy 0.78173828125\n",
      "Iteration 64340 Training loss 0.01581418886780739 Validation loss 0.02321561984717846 Accuracy 0.759765625\n",
      "Iteration 64350 Training loss 0.01366859208792448 Validation loss 0.021298695355653763 Accuracy 0.78125\n",
      "Iteration 64360 Training loss 0.013118671253323555 Validation loss 0.020945370197296143 Accuracy 0.78271484375\n",
      "Iteration 64370 Training loss 0.013235615566372871 Validation loss 0.020937195047736168 Accuracy 0.78369140625\n",
      "Iteration 64380 Training loss 0.016705647110939026 Validation loss 0.021005036309361458 Accuracy 0.78271484375\n",
      "Iteration 64390 Training loss 0.012880958616733551 Validation loss 0.021157797425985336 Accuracy 0.78076171875\n",
      "Iteration 64400 Training loss 0.015706820413470268 Validation loss 0.021114349365234375 Accuracy 0.78125\n",
      "Iteration 64410 Training loss 0.015335199423134327 Validation loss 0.02090458758175373 Accuracy 0.783203125\n",
      "Iteration 64420 Training loss 0.013819582760334015 Validation loss 0.021021615713834763 Accuracy 0.783203125\n",
      "Iteration 64430 Training loss 0.014524421654641628 Validation loss 0.020777937024831772 Accuracy 0.78466796875\n",
      "Iteration 64440 Training loss 0.014449557289481163 Validation loss 0.021131128072738647 Accuracy 0.78173828125\n",
      "Iteration 64450 Training loss 0.014663326554000378 Validation loss 0.020983301103115082 Accuracy 0.783203125\n",
      "Iteration 64460 Training loss 0.013814532198011875 Validation loss 0.021038008853793144 Accuracy 0.78369140625\n",
      "Iteration 64470 Training loss 0.01532948762178421 Validation loss 0.022556476294994354 Accuracy 0.767578125\n",
      "Iteration 64480 Training loss 0.014855555258691311 Validation loss 0.0210360549390316 Accuracy 0.7822265625\n",
      "Iteration 64490 Training loss 0.013159824535250664 Validation loss 0.021038109436631203 Accuracy 0.78369140625\n",
      "Iteration 64500 Training loss 0.012696782127022743 Validation loss 0.021006543189287186 Accuracy 0.783203125\n",
      "Iteration 64510 Training loss 0.014270516112446785 Validation loss 0.021382873877882957 Accuracy 0.7783203125\n",
      "Iteration 64520 Training loss 0.01534056942909956 Validation loss 0.0212581604719162 Accuracy 0.7802734375\n",
      "Iteration 64530 Training loss 0.013455263338983059 Validation loss 0.020871933549642563 Accuracy 0.783203125\n",
      "Iteration 64540 Training loss 0.01426518615335226 Validation loss 0.021011753007769585 Accuracy 0.78271484375\n",
      "Iteration 64550 Training loss 0.016287872567772865 Validation loss 0.020885441452264786 Accuracy 0.78466796875\n",
      "Iteration 64560 Training loss 0.011761021800339222 Validation loss 0.021024929359555244 Accuracy 0.7822265625\n",
      "Iteration 64570 Training loss 0.017537659034132957 Validation loss 0.02094826102256775 Accuracy 0.78271484375\n",
      "Iteration 64580 Training loss 0.014866294339299202 Validation loss 0.021130045875906944 Accuracy 0.78173828125\n",
      "Iteration 64590 Training loss 0.014678959734737873 Validation loss 0.020974257960915565 Accuracy 0.783203125\n",
      "Iteration 64600 Training loss 0.015163937583565712 Validation loss 0.021132709458470345 Accuracy 0.78076171875\n",
      "Iteration 64610 Training loss 0.013501412235200405 Validation loss 0.020891554653644562 Accuracy 0.78369140625\n",
      "Iteration 64620 Training loss 0.01510407030582428 Validation loss 0.02080460824072361 Accuracy 0.78466796875\n",
      "Iteration 64630 Training loss 0.013769248500466347 Validation loss 0.021027909591794014 Accuracy 0.783203125\n",
      "Iteration 64640 Training loss 0.011895542033016682 Validation loss 0.02093779668211937 Accuracy 0.78369140625\n",
      "Iteration 64650 Training loss 0.013251185417175293 Validation loss 0.02113543450832367 Accuracy 0.78173828125\n",
      "Iteration 64660 Training loss 0.013878468424081802 Validation loss 0.020978502929210663 Accuracy 0.783203125\n",
      "Iteration 64670 Training loss 0.012239438481628895 Validation loss 0.02120761200785637 Accuracy 0.7802734375\n",
      "Iteration 64680 Training loss 0.015343856997787952 Validation loss 0.02100050076842308 Accuracy 0.78271484375\n",
      "Iteration 64690 Training loss 0.014895271509885788 Validation loss 0.02085799351334572 Accuracy 0.78369140625\n",
      "Iteration 64700 Training loss 0.014383266679942608 Validation loss 0.021048344671726227 Accuracy 0.7822265625\n",
      "Iteration 64710 Training loss 0.015274795703589916 Validation loss 0.0210907980799675 Accuracy 0.78076171875\n",
      "Iteration 64720 Training loss 0.016274694353342056 Validation loss 0.021682802587747574 Accuracy 0.77685546875\n",
      "Iteration 64730 Training loss 0.013840697705745697 Validation loss 0.021373119205236435 Accuracy 0.77880859375\n",
      "Iteration 64740 Training loss 0.012612900696694851 Validation loss 0.021103596314787865 Accuracy 0.78173828125\n",
      "Iteration 64750 Training loss 0.013654222711920738 Validation loss 0.0209422018378973 Accuracy 0.78271484375\n",
      "Iteration 64760 Training loss 0.014626444317400455 Validation loss 0.021226532757282257 Accuracy 0.78076171875\n",
      "Iteration 64770 Training loss 0.013064160943031311 Validation loss 0.020966582000255585 Accuracy 0.783203125\n",
      "Iteration 64780 Training loss 0.012958245351910591 Validation loss 0.020846053957939148 Accuracy 0.78369140625\n",
      "Iteration 64790 Training loss 0.012456065975129604 Validation loss 0.021059604361653328 Accuracy 0.7822265625\n",
      "Iteration 64800 Training loss 0.015318077988922596 Validation loss 0.020860493183135986 Accuracy 0.7841796875\n",
      "Iteration 64810 Training loss 0.011657726019620895 Validation loss 0.020906252786517143 Accuracy 0.78369140625\n",
      "Iteration 64820 Training loss 0.01563739962875843 Validation loss 0.020987391471862793 Accuracy 0.783203125\n",
      "Iteration 64830 Training loss 0.01104410644620657 Validation loss 0.020926721394062042 Accuracy 0.78271484375\n",
      "Iteration 64840 Training loss 0.017575055360794067 Validation loss 0.021829837933182716 Accuracy 0.77490234375\n",
      "Iteration 64850 Training loss 0.016385335475206375 Validation loss 0.020931581035256386 Accuracy 0.78271484375\n",
      "Iteration 64860 Training loss 0.013352179899811745 Validation loss 0.021170057356357574 Accuracy 0.78076171875\n",
      "Iteration 64870 Training loss 0.014842955395579338 Validation loss 0.021158840507268906 Accuracy 0.7822265625\n",
      "Iteration 64880 Training loss 0.014881785027682781 Validation loss 0.021115204319357872 Accuracy 0.78125\n",
      "Iteration 64890 Training loss 0.016853321343660355 Validation loss 0.02102705091238022 Accuracy 0.783203125\n",
      "Iteration 64900 Training loss 0.01235941518098116 Validation loss 0.021241210401058197 Accuracy 0.77978515625\n",
      "Iteration 64910 Training loss 0.017928585410118103 Validation loss 0.02131308801472187 Accuracy 0.77978515625\n",
      "Iteration 64920 Training loss 0.011404858902096748 Validation loss 0.0207992997020483 Accuracy 0.78466796875\n",
      "Iteration 64930 Training loss 0.013477180153131485 Validation loss 0.0209061149507761 Accuracy 0.78271484375\n",
      "Iteration 64940 Training loss 0.014832358807325363 Validation loss 0.020879218354821205 Accuracy 0.7841796875\n",
      "Iteration 64950 Training loss 0.012983833439648151 Validation loss 0.021207183599472046 Accuracy 0.78173828125\n",
      "Iteration 64960 Training loss 0.014564646407961845 Validation loss 0.02093597687780857 Accuracy 0.78369140625\n",
      "Iteration 64970 Training loss 0.014361295849084854 Validation loss 0.02138395980000496 Accuracy 0.7783203125\n",
      "Iteration 64980 Training loss 0.015473765321075916 Validation loss 0.021269718185067177 Accuracy 0.7802734375\n",
      "Iteration 64990 Training loss 0.013610047288239002 Validation loss 0.02104038931429386 Accuracy 0.78173828125\n",
      "Iteration 65000 Training loss 0.014475386589765549 Validation loss 0.020895937457680702 Accuracy 0.7841796875\n",
      "Iteration 65010 Training loss 0.01400761678814888 Validation loss 0.02127188630402088 Accuracy 0.77978515625\n",
      "Iteration 65020 Training loss 0.014417748898267746 Validation loss 0.021170834079384804 Accuracy 0.78173828125\n",
      "Iteration 65030 Training loss 0.013576618395745754 Validation loss 0.02104509249329567 Accuracy 0.7822265625\n",
      "Iteration 65040 Training loss 0.01353586558252573 Validation loss 0.020980605855584145 Accuracy 0.78271484375\n",
      "Iteration 65050 Training loss 0.01458747498691082 Validation loss 0.020899515599012375 Accuracy 0.783203125\n",
      "Iteration 65060 Training loss 0.014385703951120377 Validation loss 0.02108449675142765 Accuracy 0.78271484375\n",
      "Iteration 65070 Training loss 0.015387642197310925 Validation loss 0.021089550107717514 Accuracy 0.7822265625\n",
      "Iteration 65080 Training loss 0.01543286070227623 Validation loss 0.02138218656182289 Accuracy 0.7802734375\n",
      "Iteration 65090 Training loss 0.01242156233638525 Validation loss 0.02109331451356411 Accuracy 0.7822265625\n",
      "Iteration 65100 Training loss 0.015019225887954235 Validation loss 0.0210078414529562 Accuracy 0.78271484375\n",
      "Iteration 65110 Training loss 0.013981078751385212 Validation loss 0.02114846557378769 Accuracy 0.783203125\n",
      "Iteration 65120 Training loss 0.015414802357554436 Validation loss 0.02108946442604065 Accuracy 0.783203125\n",
      "Iteration 65130 Training loss 0.013919857330620289 Validation loss 0.02095133252441883 Accuracy 0.783203125\n",
      "Iteration 65140 Training loss 0.012549392879009247 Validation loss 0.020879365503787994 Accuracy 0.7841796875\n",
      "Iteration 65150 Training loss 0.016477229073643684 Validation loss 0.020947908982634544 Accuracy 0.78466796875\n",
      "Iteration 65160 Training loss 0.014911089092493057 Validation loss 0.020835531875491142 Accuracy 0.78515625\n",
      "Iteration 65170 Training loss 0.013696663081645966 Validation loss 0.021083572879433632 Accuracy 0.783203125\n",
      "Iteration 65180 Training loss 0.015643073245882988 Validation loss 0.021386317908763885 Accuracy 0.779296875\n",
      "Iteration 65190 Training loss 0.013027328066527843 Validation loss 0.021152447909116745 Accuracy 0.78125\n",
      "Iteration 65200 Training loss 0.013952692970633507 Validation loss 0.020916366949677467 Accuracy 0.783203125\n",
      "Iteration 65210 Training loss 0.01092719379812479 Validation loss 0.02088884636759758 Accuracy 0.78466796875\n",
      "Iteration 65220 Training loss 0.012168345041573048 Validation loss 0.02102469839155674 Accuracy 0.78173828125\n",
      "Iteration 65230 Training loss 0.014417546801269054 Validation loss 0.020887093618512154 Accuracy 0.78369140625\n",
      "Iteration 65240 Training loss 0.013656885363161564 Validation loss 0.020781775936484337 Accuracy 0.7841796875\n",
      "Iteration 65250 Training loss 0.012392758391797543 Validation loss 0.021228035911917686 Accuracy 0.78076171875\n",
      "Iteration 65260 Training loss 0.013671251945197582 Validation loss 0.021115507930517197 Accuracy 0.78125\n",
      "Iteration 65270 Training loss 0.014771247282624245 Validation loss 0.020860327407717705 Accuracy 0.7841796875\n",
      "Iteration 65280 Training loss 0.015537739731371403 Validation loss 0.02087447978556156 Accuracy 0.7841796875\n",
      "Iteration 65290 Training loss 0.01399332657456398 Validation loss 0.021019628271460533 Accuracy 0.78271484375\n",
      "Iteration 65300 Training loss 0.015151667408645153 Validation loss 0.021170902997255325 Accuracy 0.78076171875\n",
      "Iteration 65310 Training loss 0.014941342175006866 Validation loss 0.020999867469072342 Accuracy 0.7822265625\n",
      "Iteration 65320 Training loss 0.01285365130752325 Validation loss 0.020977254956960678 Accuracy 0.78271484375\n",
      "Iteration 65330 Training loss 0.013921717181801796 Validation loss 0.020801739767193794 Accuracy 0.78369140625\n",
      "Iteration 65340 Training loss 0.012940065003931522 Validation loss 0.02084936760365963 Accuracy 0.78466796875\n",
      "Iteration 65350 Training loss 0.013181038200855255 Validation loss 0.020858366042375565 Accuracy 0.78466796875\n",
      "Iteration 65360 Training loss 0.01477002166211605 Validation loss 0.02091895416378975 Accuracy 0.7841796875\n",
      "Iteration 65370 Training loss 0.017214030027389526 Validation loss 0.02104528248310089 Accuracy 0.7841796875\n",
      "Iteration 65380 Training loss 0.013775760307908058 Validation loss 0.02083650417625904 Accuracy 0.78515625\n",
      "Iteration 65390 Training loss 0.018124882131814957 Validation loss 0.022977767512202263 Accuracy 0.76171875\n",
      "Iteration 65400 Training loss 0.015580768696963787 Validation loss 0.021250424906611443 Accuracy 0.77978515625\n",
      "Iteration 65410 Training loss 0.013451242819428444 Validation loss 0.021060604602098465 Accuracy 0.78271484375\n",
      "Iteration 65420 Training loss 0.015910502523183823 Validation loss 0.021118922159075737 Accuracy 0.7822265625\n",
      "Iteration 65430 Training loss 0.015397392213344574 Validation loss 0.020959816873073578 Accuracy 0.78271484375\n",
      "Iteration 65440 Training loss 0.01453685574233532 Validation loss 0.021350208669900894 Accuracy 0.77978515625\n",
      "Iteration 65450 Training loss 0.012984905391931534 Validation loss 0.021044660359621048 Accuracy 0.78271484375\n",
      "Iteration 65460 Training loss 0.014113438315689564 Validation loss 0.021652190014719963 Accuracy 0.7763671875\n",
      "Iteration 65470 Training loss 0.01257947739213705 Validation loss 0.02096492052078247 Accuracy 0.78271484375\n",
      "Iteration 65480 Training loss 0.012003383599221706 Validation loss 0.02113931067287922 Accuracy 0.78173828125\n",
      "Iteration 65490 Training loss 0.012659702450037003 Validation loss 0.020894451066851616 Accuracy 0.78466796875\n",
      "Iteration 65500 Training loss 0.01308844331651926 Validation loss 0.020816894248127937 Accuracy 0.78515625\n",
      "Iteration 65510 Training loss 0.01582520268857479 Validation loss 0.02082499861717224 Accuracy 0.78466796875\n",
      "Iteration 65520 Training loss 0.012917930260300636 Validation loss 0.02084868587553501 Accuracy 0.78466796875\n",
      "Iteration 65530 Training loss 0.014042570255696774 Validation loss 0.021024098619818687 Accuracy 0.78271484375\n",
      "Iteration 65540 Training loss 0.01783645525574684 Validation loss 0.021063685417175293 Accuracy 0.7822265625\n",
      "Iteration 65550 Training loss 0.012622683309018612 Validation loss 0.020929833874106407 Accuracy 0.7841796875\n",
      "Iteration 65560 Training loss 0.01452877838164568 Validation loss 0.02136465720832348 Accuracy 0.78076171875\n",
      "Iteration 65570 Training loss 0.013703512959182262 Validation loss 0.021011019125580788 Accuracy 0.78271484375\n",
      "Iteration 65580 Training loss 0.014242309145629406 Validation loss 0.021047282963991165 Accuracy 0.78271484375\n",
      "Iteration 65590 Training loss 0.010891936719417572 Validation loss 0.02099570818245411 Accuracy 0.78369140625\n",
      "Iteration 65600 Training loss 0.013985422439873219 Validation loss 0.021148068830370903 Accuracy 0.78125\n",
      "Iteration 65610 Training loss 0.016596214845776558 Validation loss 0.02110961824655533 Accuracy 0.78173828125\n",
      "Iteration 65620 Training loss 0.01225966215133667 Validation loss 0.020748496055603027 Accuracy 0.7861328125\n",
      "Iteration 65630 Training loss 0.014076746068894863 Validation loss 0.02134690433740616 Accuracy 0.7802734375\n",
      "Iteration 65640 Training loss 0.01250552199780941 Validation loss 0.02080460451543331 Accuracy 0.78564453125\n",
      "Iteration 65650 Training loss 0.011989504098892212 Validation loss 0.020793773233890533 Accuracy 0.7841796875\n",
      "Iteration 65660 Training loss 0.012769444845616817 Validation loss 0.02080012671649456 Accuracy 0.78564453125\n",
      "Iteration 65670 Training loss 0.0129943722859025 Validation loss 0.020963449031114578 Accuracy 0.78271484375\n",
      "Iteration 65680 Training loss 0.012388094328343868 Validation loss 0.02090698666870594 Accuracy 0.78369140625\n",
      "Iteration 65690 Training loss 0.013377293944358826 Validation loss 0.02096293494105339 Accuracy 0.78271484375\n",
      "Iteration 65700 Training loss 0.014748862013220787 Validation loss 0.02091989666223526 Accuracy 0.7841796875\n",
      "Iteration 65710 Training loss 0.013969652354717255 Validation loss 0.02080790512263775 Accuracy 0.78515625\n",
      "Iteration 65720 Training loss 0.012912793084979057 Validation loss 0.02079133316874504 Accuracy 0.78466796875\n",
      "Iteration 65730 Training loss 0.013882908970117569 Validation loss 0.02087877318263054 Accuracy 0.78466796875\n",
      "Iteration 65740 Training loss 0.011322032660245895 Validation loss 0.020934944972395897 Accuracy 0.78369140625\n",
      "Iteration 65750 Training loss 0.014897597022354603 Validation loss 0.021428244188427925 Accuracy 0.77783203125\n",
      "Iteration 65760 Training loss 0.012907380238175392 Validation loss 0.020898081362247467 Accuracy 0.7841796875\n",
      "Iteration 65770 Training loss 0.013686459511518478 Validation loss 0.02087709680199623 Accuracy 0.78369140625\n",
      "Iteration 65780 Training loss 0.012484459206461906 Validation loss 0.02112407609820366 Accuracy 0.78076171875\n",
      "Iteration 65790 Training loss 0.013465874828398228 Validation loss 0.021227603778243065 Accuracy 0.78125\n",
      "Iteration 65800 Training loss 0.012401762418448925 Validation loss 0.021412430331110954 Accuracy 0.77880859375\n",
      "Iteration 65810 Training loss 0.013942843303084373 Validation loss 0.02087347023189068 Accuracy 0.783203125\n",
      "Iteration 65820 Training loss 0.012905157171189785 Validation loss 0.02113647200167179 Accuracy 0.78271484375\n",
      "Iteration 65830 Training loss 0.011821458116173744 Validation loss 0.02127220667898655 Accuracy 0.77978515625\n",
      "Iteration 65840 Training loss 0.014215324074029922 Validation loss 0.020940354093909264 Accuracy 0.783203125\n",
      "Iteration 65850 Training loss 0.014226146973669529 Validation loss 0.021153710782527924 Accuracy 0.78125\n",
      "Iteration 65860 Training loss 0.014774369075894356 Validation loss 0.021370649337768555 Accuracy 0.78076171875\n",
      "Iteration 65870 Training loss 0.013592376373708248 Validation loss 0.021397944539785385 Accuracy 0.77978515625\n",
      "Iteration 65880 Training loss 0.013344215229153633 Validation loss 0.020953573286533356 Accuracy 0.78466796875\n",
      "Iteration 65890 Training loss 0.015811949968338013 Validation loss 0.021066268905997276 Accuracy 0.78173828125\n",
      "Iteration 65900 Training loss 0.014290321618318558 Validation loss 0.021081997081637383 Accuracy 0.7822265625\n",
      "Iteration 65910 Training loss 0.013383106328547001 Validation loss 0.021166667342185974 Accuracy 0.7822265625\n",
      "Iteration 65920 Training loss 0.0132936155423522 Validation loss 0.020860999822616577 Accuracy 0.78466796875\n",
      "Iteration 65930 Training loss 0.013956940732896328 Validation loss 0.02094632014632225 Accuracy 0.78271484375\n",
      "Iteration 65940 Training loss 0.013838227838277817 Validation loss 0.020958945155143738 Accuracy 0.78271484375\n",
      "Iteration 65950 Training loss 0.01366151962429285 Validation loss 0.02108280174434185 Accuracy 0.7822265625\n",
      "Iteration 65960 Training loss 0.014460103586316109 Validation loss 0.021048126742243767 Accuracy 0.783203125\n",
      "Iteration 65970 Training loss 0.015624740161001682 Validation loss 0.020774735137820244 Accuracy 0.78564453125\n",
      "Iteration 65980 Training loss 0.01442020758986473 Validation loss 0.021031584590673447 Accuracy 0.78173828125\n",
      "Iteration 65990 Training loss 0.011663040146231651 Validation loss 0.02117537520825863 Accuracy 0.78271484375\n",
      "Iteration 66000 Training loss 0.014496661722660065 Validation loss 0.02087332494556904 Accuracy 0.78369140625\n",
      "Iteration 66010 Training loss 0.012869404628872871 Validation loss 0.021185195073485374 Accuracy 0.7822265625\n",
      "Iteration 66020 Training loss 0.013217629864811897 Validation loss 0.020808881148695946 Accuracy 0.78466796875\n",
      "Iteration 66030 Training loss 0.015081795863807201 Validation loss 0.020917577669024467 Accuracy 0.783203125\n",
      "Iteration 66040 Training loss 0.015840748324990273 Validation loss 0.021207256242632866 Accuracy 0.78173828125\n",
      "Iteration 66050 Training loss 0.014182143844664097 Validation loss 0.020750809460878372 Accuracy 0.78564453125\n",
      "Iteration 66060 Training loss 0.013256528414785862 Validation loss 0.020731311291456223 Accuracy 0.78515625\n",
      "Iteration 66070 Training loss 0.01331951655447483 Validation loss 0.020864339545369148 Accuracy 0.78466796875\n",
      "Iteration 66080 Training loss 0.012451043352484703 Validation loss 0.020861171185970306 Accuracy 0.7841796875\n",
      "Iteration 66090 Training loss 0.012633793987333775 Validation loss 0.020916186273097992 Accuracy 0.7841796875\n",
      "Iteration 66100 Training loss 0.011806508526206017 Validation loss 0.020829040557146072 Accuracy 0.783203125\n",
      "Iteration 66110 Training loss 0.013585800305008888 Validation loss 0.02082143910229206 Accuracy 0.78466796875\n",
      "Iteration 66120 Training loss 0.01327640749514103 Validation loss 0.02074371464550495 Accuracy 0.78515625\n",
      "Iteration 66130 Training loss 0.013989580795168877 Validation loss 0.02087852731347084 Accuracy 0.78369140625\n",
      "Iteration 66140 Training loss 0.011429407633841038 Validation loss 0.020892519503831863 Accuracy 0.783203125\n",
      "Iteration 66150 Training loss 0.013126404955983162 Validation loss 0.021220866590738297 Accuracy 0.77978515625\n",
      "Iteration 66160 Training loss 0.015384775586426258 Validation loss 0.021090956404805183 Accuracy 0.78271484375\n",
      "Iteration 66170 Training loss 0.011494431644678116 Validation loss 0.020869368687272072 Accuracy 0.78369140625\n",
      "Iteration 66180 Training loss 0.01254223845899105 Validation loss 0.020818961784243584 Accuracy 0.78515625\n",
      "Iteration 66190 Training loss 0.017104828730225563 Validation loss 0.021578403189778328 Accuracy 0.77734375\n",
      "Iteration 66200 Training loss 0.017400071024894714 Validation loss 0.020861966535449028 Accuracy 0.7841796875\n",
      "Iteration 66210 Training loss 0.01529788225889206 Validation loss 0.02179792895913124 Accuracy 0.7744140625\n",
      "Iteration 66220 Training loss 0.014097373932600021 Validation loss 0.021262889727950096 Accuracy 0.78076171875\n",
      "Iteration 66230 Training loss 0.016187088564038277 Validation loss 0.021311558783054352 Accuracy 0.7802734375\n",
      "Iteration 66240 Training loss 0.01562945730984211 Validation loss 0.02096855826675892 Accuracy 0.78271484375\n",
      "Iteration 66250 Training loss 0.015072105452418327 Validation loss 0.021412059664726257 Accuracy 0.7802734375\n",
      "Iteration 66260 Training loss 0.013441871851682663 Validation loss 0.020926455035805702 Accuracy 0.783203125\n",
      "Iteration 66270 Training loss 0.011440479196608067 Validation loss 0.021254731342196465 Accuracy 0.77978515625\n",
      "Iteration 66280 Training loss 0.012279083020985126 Validation loss 0.020789651200175285 Accuracy 0.78515625\n",
      "Iteration 66290 Training loss 0.01380500104278326 Validation loss 0.021014826372265816 Accuracy 0.7822265625\n",
      "Iteration 66300 Training loss 0.011546358466148376 Validation loss 0.020964428782463074 Accuracy 0.7841796875\n",
      "Iteration 66310 Training loss 0.013620843179523945 Validation loss 0.020845526829361916 Accuracy 0.7841796875\n",
      "Iteration 66320 Training loss 0.01316145434975624 Validation loss 0.020890844985842705 Accuracy 0.78466796875\n",
      "Iteration 66330 Training loss 0.013323572464287281 Validation loss 0.020950675010681152 Accuracy 0.783203125\n",
      "Iteration 66340 Training loss 0.01311108935624361 Validation loss 0.021197890862822533 Accuracy 0.78173828125\n",
      "Iteration 66350 Training loss 0.013249177485704422 Validation loss 0.020887263119220734 Accuracy 0.78369140625\n",
      "Iteration 66360 Training loss 0.01261606439948082 Validation loss 0.020905008539557457 Accuracy 0.78466796875\n",
      "Iteration 66370 Training loss 0.013433168642222881 Validation loss 0.020763667300343513 Accuracy 0.78515625\n",
      "Iteration 66380 Training loss 0.012110275216400623 Validation loss 0.020733924582600594 Accuracy 0.78564453125\n",
      "Iteration 66390 Training loss 0.012316710315644741 Validation loss 0.020690390840172768 Accuracy 0.78466796875\n",
      "Iteration 66400 Training loss 0.0148242749273777 Validation loss 0.020924247801303864 Accuracy 0.78369140625\n",
      "Iteration 66410 Training loss 0.01384944561868906 Validation loss 0.021407855674624443 Accuracy 0.77880859375\n",
      "Iteration 66420 Training loss 0.013236410915851593 Validation loss 0.02076936513185501 Accuracy 0.78466796875\n",
      "Iteration 66430 Training loss 0.013616451993584633 Validation loss 0.020932646468281746 Accuracy 0.78369140625\n",
      "Iteration 66440 Training loss 0.015248894691467285 Validation loss 0.02076507732272148 Accuracy 0.78466796875\n",
      "Iteration 66450 Training loss 0.01713324524462223 Validation loss 0.02083522453904152 Accuracy 0.7841796875\n",
      "Iteration 66460 Training loss 0.015780683606863022 Validation loss 0.020909424871206284 Accuracy 0.783203125\n",
      "Iteration 66470 Training loss 0.01575813814997673 Validation loss 0.021104369312524796 Accuracy 0.78173828125\n",
      "Iteration 66480 Training loss 0.015201522968709469 Validation loss 0.020936479791998863 Accuracy 0.78564453125\n",
      "Iteration 66490 Training loss 0.016246983781456947 Validation loss 0.021482782438397408 Accuracy 0.779296875\n",
      "Iteration 66500 Training loss 0.015978818759322166 Validation loss 0.021374471485614777 Accuracy 0.77978515625\n",
      "Iteration 66510 Training loss 0.01327489037066698 Validation loss 0.021102499216794968 Accuracy 0.78271484375\n",
      "Iteration 66520 Training loss 0.01369357667863369 Validation loss 0.02110520377755165 Accuracy 0.78271484375\n",
      "Iteration 66530 Training loss 0.016774624586105347 Validation loss 0.021063700318336487 Accuracy 0.783203125\n",
      "Iteration 66540 Training loss 0.01653633452951908 Validation loss 0.0208805613219738 Accuracy 0.7841796875\n",
      "Iteration 66550 Training loss 0.014530912041664124 Validation loss 0.021224943920969963 Accuracy 0.78125\n",
      "Iteration 66560 Training loss 0.013658596202731133 Validation loss 0.020864298567175865 Accuracy 0.7841796875\n",
      "Iteration 66570 Training loss 0.012767918407917023 Validation loss 0.021054072305560112 Accuracy 0.78271484375\n",
      "Iteration 66580 Training loss 0.014830583706498146 Validation loss 0.021149471402168274 Accuracy 0.78173828125\n",
      "Iteration 66590 Training loss 0.014442049898207188 Validation loss 0.02090613916516304 Accuracy 0.78369140625\n",
      "Iteration 66600 Training loss 0.012939028441905975 Validation loss 0.021075433120131493 Accuracy 0.783203125\n",
      "Iteration 66610 Training loss 0.016428016126155853 Validation loss 0.020942499861121178 Accuracy 0.7841796875\n",
      "Iteration 66620 Training loss 0.013533630408346653 Validation loss 0.02074476331472397 Accuracy 0.78564453125\n",
      "Iteration 66630 Training loss 0.015048239380121231 Validation loss 0.020871898159384727 Accuracy 0.78369140625\n",
      "Iteration 66640 Training loss 0.014324747957289219 Validation loss 0.021015677601099014 Accuracy 0.783203125\n",
      "Iteration 66650 Training loss 0.015082919038832188 Validation loss 0.020865067839622498 Accuracy 0.7841796875\n",
      "Iteration 66660 Training loss 0.0150500712916255 Validation loss 0.021158479154109955 Accuracy 0.7822265625\n",
      "Iteration 66670 Training loss 0.013950997963547707 Validation loss 0.0209070835262537 Accuracy 0.7841796875\n",
      "Iteration 66680 Training loss 0.012993662618100643 Validation loss 0.020694952458143234 Accuracy 0.78564453125\n",
      "Iteration 66690 Training loss 0.012924069538712502 Validation loss 0.02140921913087368 Accuracy 0.7783203125\n",
      "Iteration 66700 Training loss 0.014508504420518875 Validation loss 0.02109803818166256 Accuracy 0.7822265625\n",
      "Iteration 66710 Training loss 0.014774113893508911 Validation loss 0.021103929728269577 Accuracy 0.78173828125\n",
      "Iteration 66720 Training loss 0.012809475883841515 Validation loss 0.020776405930519104 Accuracy 0.78466796875\n",
      "Iteration 66730 Training loss 0.01269852090626955 Validation loss 0.020863009616732597 Accuracy 0.78515625\n",
      "Iteration 66740 Training loss 0.010738838464021683 Validation loss 0.020884446799755096 Accuracy 0.78369140625\n",
      "Iteration 66750 Training loss 0.014810074120759964 Validation loss 0.02103528194129467 Accuracy 0.78271484375\n",
      "Iteration 66760 Training loss 0.013535677455365658 Validation loss 0.020736217498779297 Accuracy 0.78369140625\n",
      "Iteration 66770 Training loss 0.016539888456463814 Validation loss 0.02124430611729622 Accuracy 0.779296875\n",
      "Iteration 66780 Training loss 0.014936556108295918 Validation loss 0.02098723128437996 Accuracy 0.7841796875\n",
      "Iteration 66790 Training loss 0.013295402750372887 Validation loss 0.020913273096084595 Accuracy 0.78271484375\n",
      "Iteration 66800 Training loss 0.015368596650660038 Validation loss 0.021342357620596886 Accuracy 0.78076171875\n",
      "Iteration 66810 Training loss 0.01261220034211874 Validation loss 0.02092452347278595 Accuracy 0.783203125\n",
      "Iteration 66820 Training loss 0.013974464498460293 Validation loss 0.020796366035938263 Accuracy 0.7841796875\n",
      "Iteration 66830 Training loss 0.01745079644024372 Validation loss 0.020932717248797417 Accuracy 0.783203125\n",
      "Iteration 66840 Training loss 0.013852833770215511 Validation loss 0.0208208579570055 Accuracy 0.78369140625\n",
      "Iteration 66850 Training loss 0.014122861437499523 Validation loss 0.0209051463752985 Accuracy 0.7841796875\n",
      "Iteration 66860 Training loss 0.013056444935500622 Validation loss 0.02119126357138157 Accuracy 0.78076171875\n",
      "Iteration 66870 Training loss 0.012987833470106125 Validation loss 0.020990345627069473 Accuracy 0.78369140625\n",
      "Iteration 66880 Training loss 0.014890246093273163 Validation loss 0.02111097052693367 Accuracy 0.7822265625\n",
      "Iteration 66890 Training loss 0.014632153324782848 Validation loss 0.02095678634941578 Accuracy 0.783203125\n",
      "Iteration 66900 Training loss 0.01381633896380663 Validation loss 0.020820854231715202 Accuracy 0.7841796875\n",
      "Iteration 66910 Training loss 0.01371667068451643 Validation loss 0.020868433639407158 Accuracy 0.78369140625\n",
      "Iteration 66920 Training loss 0.014064980670809746 Validation loss 0.020835498347878456 Accuracy 0.7841796875\n",
      "Iteration 66930 Training loss 0.012932746671140194 Validation loss 0.020995868369936943 Accuracy 0.78271484375\n",
      "Iteration 66940 Training loss 0.012969174422323704 Validation loss 0.021052362397313118 Accuracy 0.7822265625\n",
      "Iteration 66950 Training loss 0.011694568209350109 Validation loss 0.021274052560329437 Accuracy 0.7802734375\n",
      "Iteration 66960 Training loss 0.012439711950719357 Validation loss 0.020859135314822197 Accuracy 0.7841796875\n",
      "Iteration 66970 Training loss 0.01373987551778555 Validation loss 0.020746923983097076 Accuracy 0.78515625\n",
      "Iteration 66980 Training loss 0.011727709323167801 Validation loss 0.021242687478661537 Accuracy 0.78125\n",
      "Iteration 66990 Training loss 0.014154299162328243 Validation loss 0.02111046388745308 Accuracy 0.7822265625\n",
      "Iteration 67000 Training loss 0.013138305395841599 Validation loss 0.020917855203151703 Accuracy 0.78466796875\n",
      "Iteration 67010 Training loss 0.013806378468871117 Validation loss 0.02081189677119255 Accuracy 0.78466796875\n",
      "Iteration 67020 Training loss 0.01319133397191763 Validation loss 0.020721258595585823 Accuracy 0.78466796875\n",
      "Iteration 67030 Training loss 0.013697958551347256 Validation loss 0.02089719846844673 Accuracy 0.7841796875\n",
      "Iteration 67040 Training loss 0.01369994506239891 Validation loss 0.021065043285489082 Accuracy 0.783203125\n",
      "Iteration 67050 Training loss 0.016784973442554474 Validation loss 0.0212025735527277 Accuracy 0.78173828125\n",
      "Iteration 67060 Training loss 0.01293499767780304 Validation loss 0.02092340774834156 Accuracy 0.78271484375\n",
      "Iteration 67070 Training loss 0.01299265492707491 Validation loss 0.02091912366449833 Accuracy 0.78369140625\n",
      "Iteration 67080 Training loss 0.013795453123748302 Validation loss 0.021443650126457214 Accuracy 0.77880859375\n",
      "Iteration 67090 Training loss 0.014312740415334702 Validation loss 0.020820189267396927 Accuracy 0.78369140625\n",
      "Iteration 67100 Training loss 0.014192840084433556 Validation loss 0.02085156738758087 Accuracy 0.78369140625\n",
      "Iteration 67110 Training loss 0.015609006397426128 Validation loss 0.020943831652402878 Accuracy 0.783203125\n",
      "Iteration 67120 Training loss 0.013049290515482426 Validation loss 0.02096370980143547 Accuracy 0.78271484375\n",
      "Iteration 67130 Training loss 0.013299176469445229 Validation loss 0.020870106294751167 Accuracy 0.78369140625\n",
      "Iteration 67140 Training loss 0.011483610607683659 Validation loss 0.021108778193593025 Accuracy 0.78173828125\n",
      "Iteration 67150 Training loss 0.014125026762485504 Validation loss 0.0211790781468153 Accuracy 0.78173828125\n",
      "Iteration 67160 Training loss 0.015021947212517262 Validation loss 0.020944220945239067 Accuracy 0.78369140625\n",
      "Iteration 67170 Training loss 0.014953638426959515 Validation loss 0.02094697207212448 Accuracy 0.783203125\n",
      "Iteration 67180 Training loss 0.014275262132287025 Validation loss 0.020773379132151604 Accuracy 0.7841796875\n",
      "Iteration 67190 Training loss 0.01282384991645813 Validation loss 0.02105950191617012 Accuracy 0.7822265625\n",
      "Iteration 67200 Training loss 0.01454363577067852 Validation loss 0.020980609580874443 Accuracy 0.78369140625\n",
      "Iteration 67210 Training loss 0.011891793459653854 Validation loss 0.02092307060956955 Accuracy 0.7841796875\n",
      "Iteration 67220 Training loss 0.011147389188408852 Validation loss 0.02092798240482807 Accuracy 0.78369140625\n",
      "Iteration 67230 Training loss 0.011811777949333191 Validation loss 0.021088197827339172 Accuracy 0.783203125\n",
      "Iteration 67240 Training loss 0.014469670131802559 Validation loss 0.020815595984458923 Accuracy 0.78466796875\n",
      "Iteration 67250 Training loss 0.014367090538144112 Validation loss 0.020850546658039093 Accuracy 0.7841796875\n",
      "Iteration 67260 Training loss 0.013354110531508923 Validation loss 0.02096862718462944 Accuracy 0.783203125\n",
      "Iteration 67270 Training loss 0.015874948352575302 Validation loss 0.021196288987994194 Accuracy 0.78125\n",
      "Iteration 67280 Training loss 0.011685490608215332 Validation loss 0.02118617668747902 Accuracy 0.78173828125\n",
      "Iteration 67290 Training loss 0.013931338675320148 Validation loss 0.020831000059843063 Accuracy 0.7841796875\n",
      "Iteration 67300 Training loss 0.015550022944808006 Validation loss 0.020837340503931046 Accuracy 0.78369140625\n",
      "Iteration 67310 Training loss 0.01656455546617508 Validation loss 0.02093007229268551 Accuracy 0.783203125\n",
      "Iteration 67320 Training loss 0.012006220407783985 Validation loss 0.020791321992874146 Accuracy 0.78466796875\n",
      "Iteration 67330 Training loss 0.013925372622907162 Validation loss 0.02123800292611122 Accuracy 0.7822265625\n",
      "Iteration 67340 Training loss 0.01432338822633028 Validation loss 0.020998632535338402 Accuracy 0.783203125\n",
      "Iteration 67350 Training loss 0.011377234943211079 Validation loss 0.021084891632199287 Accuracy 0.7822265625\n",
      "Iteration 67360 Training loss 0.013315153308212757 Validation loss 0.021049901843070984 Accuracy 0.78173828125\n",
      "Iteration 67370 Training loss 0.011588653549551964 Validation loss 0.02082853950560093 Accuracy 0.78466796875\n",
      "Iteration 67380 Training loss 0.012203597463667393 Validation loss 0.0209286417812109 Accuracy 0.783203125\n",
      "Iteration 67390 Training loss 0.013824651017785072 Validation loss 0.020900219678878784 Accuracy 0.783203125\n",
      "Iteration 67400 Training loss 0.012339511886239052 Validation loss 0.0209769569337368 Accuracy 0.7841796875\n",
      "Iteration 67410 Training loss 0.013711265288293362 Validation loss 0.021196406334638596 Accuracy 0.78125\n",
      "Iteration 67420 Training loss 0.013825688511133194 Validation loss 0.020845629274845123 Accuracy 0.78369140625\n",
      "Iteration 67430 Training loss 0.01449892669916153 Validation loss 0.021098868921399117 Accuracy 0.78271484375\n",
      "Iteration 67440 Training loss 0.014421132393181324 Validation loss 0.021176422014832497 Accuracy 0.78076171875\n",
      "Iteration 67450 Training loss 0.01466661598533392 Validation loss 0.021024316549301147 Accuracy 0.78369140625\n",
      "Iteration 67460 Training loss 0.0128513528034091 Validation loss 0.020878348499536514 Accuracy 0.78466796875\n",
      "Iteration 67470 Training loss 0.015691561624407768 Validation loss 0.02117416448891163 Accuracy 0.78076171875\n",
      "Iteration 67480 Training loss 0.012801038101315498 Validation loss 0.020972929894924164 Accuracy 0.783203125\n",
      "Iteration 67490 Training loss 0.013675854541361332 Validation loss 0.02105618268251419 Accuracy 0.78173828125\n",
      "Iteration 67500 Training loss 0.009956925176084042 Validation loss 0.020983625203371048 Accuracy 0.78271484375\n",
      "Iteration 67510 Training loss 0.01336027029901743 Validation loss 0.020900502800941467 Accuracy 0.78369140625\n",
      "Iteration 67520 Training loss 0.013737996108829975 Validation loss 0.020886162295937538 Accuracy 0.783203125\n",
      "Iteration 67530 Training loss 0.014864501543343067 Validation loss 0.021254150196909904 Accuracy 0.77978515625\n",
      "Iteration 67540 Training loss 0.014624061062932014 Validation loss 0.021318865939974785 Accuracy 0.78076171875\n",
      "Iteration 67550 Training loss 0.016445882618427277 Validation loss 0.021158065646886826 Accuracy 0.7822265625\n",
      "Iteration 67560 Training loss 0.015183359384536743 Validation loss 0.020970409736037254 Accuracy 0.78369140625\n",
      "Iteration 67570 Training loss 0.012183371931314468 Validation loss 0.02102055586874485 Accuracy 0.78271484375\n",
      "Iteration 67580 Training loss 0.014653804711997509 Validation loss 0.02093149907886982 Accuracy 0.78466796875\n",
      "Iteration 67590 Training loss 0.0167123693972826 Validation loss 0.020824948325753212 Accuracy 0.78369140625\n",
      "Iteration 67600 Training loss 0.014340700581669807 Validation loss 0.02087174542248249 Accuracy 0.783203125\n",
      "Iteration 67610 Training loss 0.013388331979513168 Validation loss 0.021070921793580055 Accuracy 0.78173828125\n",
      "Iteration 67620 Training loss 0.014471453614532948 Validation loss 0.020921405404806137 Accuracy 0.78369140625\n",
      "Iteration 67630 Training loss 0.013293236494064331 Validation loss 0.021068625152111053 Accuracy 0.7822265625\n",
      "Iteration 67640 Training loss 0.013302202336490154 Validation loss 0.021036913618445396 Accuracy 0.78173828125\n",
      "Iteration 67650 Training loss 0.013765527866780758 Validation loss 0.020911943167448044 Accuracy 0.78369140625\n",
      "Iteration 67660 Training loss 0.014082025736570358 Validation loss 0.02086593769490719 Accuracy 0.7841796875\n",
      "Iteration 67670 Training loss 0.014437269419431686 Validation loss 0.02106117643415928 Accuracy 0.78173828125\n",
      "Iteration 67680 Training loss 0.012254834175109863 Validation loss 0.02108706906437874 Accuracy 0.7822265625\n",
      "Iteration 67690 Training loss 0.015743272379040718 Validation loss 0.02090124413371086 Accuracy 0.7841796875\n",
      "Iteration 67700 Training loss 0.014769126661121845 Validation loss 0.020963050425052643 Accuracy 0.78173828125\n",
      "Iteration 67710 Training loss 0.01469937339425087 Validation loss 0.020865121856331825 Accuracy 0.78369140625\n",
      "Iteration 67720 Training loss 0.012745797634124756 Validation loss 0.021001480519771576 Accuracy 0.78271484375\n",
      "Iteration 67730 Training loss 0.012308657169342041 Validation loss 0.02136293239891529 Accuracy 0.779296875\n",
      "Iteration 67740 Training loss 0.014961633831262589 Validation loss 0.020831981673836708 Accuracy 0.78369140625\n",
      "Iteration 67750 Training loss 0.011850019916892052 Validation loss 0.020989613607525826 Accuracy 0.7841796875\n",
      "Iteration 67760 Training loss 0.012563830241560936 Validation loss 0.020861925557255745 Accuracy 0.78515625\n",
      "Iteration 67770 Training loss 0.013099201954901218 Validation loss 0.020942240953445435 Accuracy 0.783203125\n",
      "Iteration 67780 Training loss 0.013580628670752048 Validation loss 0.021063348278403282 Accuracy 0.78271484375\n",
      "Iteration 67790 Training loss 0.014467134140431881 Validation loss 0.021161586046218872 Accuracy 0.78173828125\n",
      "Iteration 67800 Training loss 0.013726430013775826 Validation loss 0.021245736628770828 Accuracy 0.7822265625\n",
      "Iteration 67810 Training loss 0.012996419332921505 Validation loss 0.021067338064312935 Accuracy 0.7822265625\n",
      "Iteration 67820 Training loss 0.013882912695407867 Validation loss 0.02076535113155842 Accuracy 0.78369140625\n",
      "Iteration 67830 Training loss 0.014101753942668438 Validation loss 0.02086407132446766 Accuracy 0.78466796875\n",
      "Iteration 67840 Training loss 0.012736150994896889 Validation loss 0.020968250930309296 Accuracy 0.78369140625\n",
      "Iteration 67850 Training loss 0.013934389688074589 Validation loss 0.021061569452285767 Accuracy 0.783203125\n",
      "Iteration 67860 Training loss 0.01295163482427597 Validation loss 0.021032560616731644 Accuracy 0.78271484375\n",
      "Iteration 67870 Training loss 0.01467022392898798 Validation loss 0.021191811189055443 Accuracy 0.78173828125\n",
      "Iteration 67880 Training loss 0.01052950695157051 Validation loss 0.020946776494383812 Accuracy 0.783203125\n",
      "Iteration 67890 Training loss 0.013430612161755562 Validation loss 0.020810246467590332 Accuracy 0.78515625\n",
      "Iteration 67900 Training loss 0.01593894325196743 Validation loss 0.020965520292520523 Accuracy 0.783203125\n",
      "Iteration 67910 Training loss 0.013839359395205975 Validation loss 0.02065422758460045 Accuracy 0.78564453125\n",
      "Iteration 67920 Training loss 0.012229230254888535 Validation loss 0.020711814984679222 Accuracy 0.78564453125\n",
      "Iteration 67930 Training loss 0.016102587804198265 Validation loss 0.020847810432314873 Accuracy 0.78369140625\n",
      "Iteration 67940 Training loss 0.013524265959858894 Validation loss 0.02074437215924263 Accuracy 0.78515625\n",
      "Iteration 67950 Training loss 0.013591018505394459 Validation loss 0.02110418863594532 Accuracy 0.78173828125\n",
      "Iteration 67960 Training loss 0.013730189763009548 Validation loss 0.021301312372088432 Accuracy 0.7802734375\n",
      "Iteration 67970 Training loss 0.01298647839576006 Validation loss 0.02073686383664608 Accuracy 0.78515625\n",
      "Iteration 67980 Training loss 0.013424739241600037 Validation loss 0.02076331153512001 Accuracy 0.78466796875\n",
      "Iteration 67990 Training loss 0.016166692599654198 Validation loss 0.02082916535437107 Accuracy 0.78369140625\n",
      "Iteration 68000 Training loss 0.015103110112249851 Validation loss 0.020953280851244926 Accuracy 0.783203125\n",
      "Iteration 68010 Training loss 0.014697931706905365 Validation loss 0.02082044817507267 Accuracy 0.7841796875\n",
      "Iteration 68020 Training loss 0.013900618068873882 Validation loss 0.021062616258859634 Accuracy 0.78271484375\n",
      "Iteration 68030 Training loss 0.013341791927814484 Validation loss 0.020807236433029175 Accuracy 0.78466796875\n",
      "Iteration 68040 Training loss 0.011195547878742218 Validation loss 0.020920023322105408 Accuracy 0.78369140625\n",
      "Iteration 68050 Training loss 0.01663064770400524 Validation loss 0.02104882337152958 Accuracy 0.783203125\n",
      "Iteration 68060 Training loss 0.014961094595491886 Validation loss 0.020858831703662872 Accuracy 0.78466796875\n",
      "Iteration 68070 Training loss 0.014112659730017185 Validation loss 0.0207742378115654 Accuracy 0.7861328125\n",
      "Iteration 68080 Training loss 0.015720168128609657 Validation loss 0.020813196897506714 Accuracy 0.7841796875\n",
      "Iteration 68090 Training loss 0.01321339886635542 Validation loss 0.020808890461921692 Accuracy 0.7841796875\n",
      "Iteration 68100 Training loss 0.016139168292284012 Validation loss 0.021056903526186943 Accuracy 0.78173828125\n",
      "Iteration 68110 Training loss 0.013291485607624054 Validation loss 0.020712904632091522 Accuracy 0.78515625\n",
      "Iteration 68120 Training loss 0.012529167346656322 Validation loss 0.021101655438542366 Accuracy 0.7822265625\n",
      "Iteration 68130 Training loss 0.013994559645652771 Validation loss 0.020913168787956238 Accuracy 0.783203125\n",
      "Iteration 68140 Training loss 0.014580869115889072 Validation loss 0.0208754800260067 Accuracy 0.78515625\n",
      "Iteration 68150 Training loss 0.013814636506140232 Validation loss 0.02094266563653946 Accuracy 0.78271484375\n",
      "Iteration 68160 Training loss 0.013955223374068737 Validation loss 0.02092103473842144 Accuracy 0.78369140625\n",
      "Iteration 68170 Training loss 0.012339264154434204 Validation loss 0.021229544654488564 Accuracy 0.78076171875\n",
      "Iteration 68180 Training loss 0.014048452489078045 Validation loss 0.02099217288196087 Accuracy 0.783203125\n",
      "Iteration 68190 Training loss 0.013698020949959755 Validation loss 0.020801933482289314 Accuracy 0.7841796875\n",
      "Iteration 68200 Training loss 0.013949014246463776 Validation loss 0.020899146795272827 Accuracy 0.7841796875\n",
      "Iteration 68210 Training loss 0.01338960137218237 Validation loss 0.02083118073642254 Accuracy 0.7841796875\n",
      "Iteration 68220 Training loss 0.013074290007352829 Validation loss 0.02095242589712143 Accuracy 0.78271484375\n",
      "Iteration 68230 Training loss 0.014141357503831387 Validation loss 0.0209670290350914 Accuracy 0.783203125\n",
      "Iteration 68240 Training loss 0.013467979617416859 Validation loss 0.020756496116518974 Accuracy 0.78466796875\n",
      "Iteration 68250 Training loss 0.013471857644617558 Validation loss 0.020820289850234985 Accuracy 0.7841796875\n",
      "Iteration 68260 Training loss 0.014104428701102734 Validation loss 0.02093333937227726 Accuracy 0.783203125\n",
      "Iteration 68270 Training loss 0.014193838462233543 Validation loss 0.020884979516267776 Accuracy 0.78369140625\n",
      "Iteration 68280 Training loss 0.016562126576900482 Validation loss 0.020875047892332077 Accuracy 0.7841796875\n",
      "Iteration 68290 Training loss 0.016986887902021408 Validation loss 0.02081330493092537 Accuracy 0.7841796875\n",
      "Iteration 68300 Training loss 0.01494890172034502 Validation loss 0.021004455164074898 Accuracy 0.78271484375\n",
      "Iteration 68310 Training loss 0.014236715622246265 Validation loss 0.020979540422558784 Accuracy 0.783203125\n",
      "Iteration 68320 Training loss 0.013164183124899864 Validation loss 0.02103373222053051 Accuracy 0.78271484375\n",
      "Iteration 68330 Training loss 0.012507998384535313 Validation loss 0.020705638453364372 Accuracy 0.78466796875\n",
      "Iteration 68340 Training loss 0.012491434812545776 Validation loss 0.02087347023189068 Accuracy 0.783203125\n",
      "Iteration 68350 Training loss 0.013615668751299381 Validation loss 0.021207567304372787 Accuracy 0.78076171875\n",
      "Iteration 68360 Training loss 0.014481956139206886 Validation loss 0.02094905450940132 Accuracy 0.78271484375\n",
      "Iteration 68370 Training loss 0.013201644644141197 Validation loss 0.021006621420383453 Accuracy 0.78271484375\n",
      "Iteration 68380 Training loss 0.012828296981751919 Validation loss 0.021067025139927864 Accuracy 0.78173828125\n",
      "Iteration 68390 Training loss 0.013278147205710411 Validation loss 0.02104182541370392 Accuracy 0.7822265625\n",
      "Iteration 68400 Training loss 0.011930595152080059 Validation loss 0.021042030304670334 Accuracy 0.78125\n",
      "Iteration 68410 Training loss 0.013120104558765888 Validation loss 0.021019676700234413 Accuracy 0.7822265625\n",
      "Iteration 68420 Training loss 0.01269789319485426 Validation loss 0.020885096862912178 Accuracy 0.783203125\n",
      "Iteration 68430 Training loss 0.015607836656272411 Validation loss 0.02115676924586296 Accuracy 0.78271484375\n",
      "Iteration 68440 Training loss 0.013468221761286259 Validation loss 0.020967645570635796 Accuracy 0.783203125\n",
      "Iteration 68450 Training loss 0.013688084669411182 Validation loss 0.020838700234889984 Accuracy 0.7841796875\n",
      "Iteration 68460 Training loss 0.01171738188713789 Validation loss 0.021016011014580727 Accuracy 0.783203125\n",
      "Iteration 68470 Training loss 0.012309150770306587 Validation loss 0.021241798996925354 Accuracy 0.78125\n",
      "Iteration 68480 Training loss 0.01178154069930315 Validation loss 0.021035099402070045 Accuracy 0.78271484375\n",
      "Iteration 68490 Training loss 0.015112697146832943 Validation loss 0.020838193595409393 Accuracy 0.7841796875\n",
      "Iteration 68500 Training loss 0.014121122658252716 Validation loss 0.021038122475147247 Accuracy 0.78173828125\n",
      "Iteration 68510 Training loss 0.01333809643983841 Validation loss 0.021354738622903824 Accuracy 0.7802734375\n",
      "Iteration 68520 Training loss 0.014545559883117676 Validation loss 0.021105049178004265 Accuracy 0.78271484375\n",
      "Iteration 68530 Training loss 0.012468279339373112 Validation loss 0.020940028131008148 Accuracy 0.78271484375\n",
      "Iteration 68540 Training loss 0.014832244254648685 Validation loss 0.021050211042165756 Accuracy 0.7822265625\n",
      "Iteration 68550 Training loss 0.01611095666885376 Validation loss 0.020823972299695015 Accuracy 0.7841796875\n",
      "Iteration 68560 Training loss 0.016000108793377876 Validation loss 0.02105490118265152 Accuracy 0.7822265625\n",
      "Iteration 68570 Training loss 0.01416955329477787 Validation loss 0.02103789709508419 Accuracy 0.78271484375\n",
      "Iteration 68580 Training loss 0.014346687123179436 Validation loss 0.021096033975481987 Accuracy 0.78173828125\n",
      "Iteration 68590 Training loss 0.013312217779457569 Validation loss 0.021122068166732788 Accuracy 0.783203125\n",
      "Iteration 68600 Training loss 0.013607921078801155 Validation loss 0.0208283681422472 Accuracy 0.78369140625\n",
      "Iteration 68610 Training loss 0.014045271091163158 Validation loss 0.0210759025067091 Accuracy 0.7822265625\n",
      "Iteration 68620 Training loss 0.01231392752379179 Validation loss 0.020868932828307152 Accuracy 0.78369140625\n",
      "Iteration 68630 Training loss 0.012194051407277584 Validation loss 0.021082928404211998 Accuracy 0.78173828125\n",
      "Iteration 68640 Training loss 0.016425376757979393 Validation loss 0.021569203585386276 Accuracy 0.77587890625\n",
      "Iteration 68650 Training loss 0.01604873314499855 Validation loss 0.021149657666683197 Accuracy 0.7802734375\n",
      "Iteration 68660 Training loss 0.014289410784840584 Validation loss 0.020962342619895935 Accuracy 0.783203125\n",
      "Iteration 68670 Training loss 0.015045811422169209 Validation loss 0.02086622454226017 Accuracy 0.78466796875\n",
      "Iteration 68680 Training loss 0.011817431077361107 Validation loss 0.02073298767209053 Accuracy 0.78515625\n",
      "Iteration 68690 Training loss 0.013578292913734913 Validation loss 0.0209870133548975 Accuracy 0.78369140625\n",
      "Iteration 68700 Training loss 0.014515748247504234 Validation loss 0.021208595484495163 Accuracy 0.7802734375\n",
      "Iteration 68710 Training loss 0.014373166486620903 Validation loss 0.020776337012648582 Accuracy 0.78466796875\n",
      "Iteration 68720 Training loss 0.01341989915817976 Validation loss 0.020890288054943085 Accuracy 0.78369140625\n",
      "Iteration 68730 Training loss 0.013561423867940903 Validation loss 0.020810863003134727 Accuracy 0.78515625\n",
      "Iteration 68740 Training loss 0.014818612486124039 Validation loss 0.020802047103643417 Accuracy 0.78369140625\n",
      "Iteration 68750 Training loss 0.015331522561609745 Validation loss 0.02079884149134159 Accuracy 0.78369140625\n",
      "Iteration 68760 Training loss 0.0169509407132864 Validation loss 0.021378930658102036 Accuracy 0.77880859375\n",
      "Iteration 68770 Training loss 0.01313238125294447 Validation loss 0.020819855853915215 Accuracy 0.78515625\n",
      "Iteration 68780 Training loss 0.015781832858920097 Validation loss 0.020905667915940285 Accuracy 0.78515625\n",
      "Iteration 68790 Training loss 0.015059269964694977 Validation loss 0.020906757563352585 Accuracy 0.78369140625\n",
      "Iteration 68800 Training loss 0.01655932143330574 Validation loss 0.020921649411320686 Accuracy 0.78369140625\n",
      "Iteration 68810 Training loss 0.012270277366042137 Validation loss 0.020875848829746246 Accuracy 0.7841796875\n",
      "Iteration 68820 Training loss 0.0159925427287817 Validation loss 0.02120361477136612 Accuracy 0.78076171875\n",
      "Iteration 68830 Training loss 0.010988348163664341 Validation loss 0.02095789462327957 Accuracy 0.783203125\n",
      "Iteration 68840 Training loss 0.015908559784293175 Validation loss 0.021359549835324287 Accuracy 0.779296875\n",
      "Iteration 68850 Training loss 0.015248570591211319 Validation loss 0.021388759836554527 Accuracy 0.77880859375\n",
      "Iteration 68860 Training loss 0.014408434741199017 Validation loss 0.02099708467721939 Accuracy 0.7822265625\n",
      "Iteration 68870 Training loss 0.015268274582922459 Validation loss 0.020878560841083527 Accuracy 0.78369140625\n",
      "Iteration 68880 Training loss 0.012749566696584225 Validation loss 0.020875291898846626 Accuracy 0.78369140625\n",
      "Iteration 68890 Training loss 0.015733545646071434 Validation loss 0.020792173221707344 Accuracy 0.78515625\n",
      "Iteration 68900 Training loss 0.013982029631733894 Validation loss 0.020947543904185295 Accuracy 0.78271484375\n",
      "Iteration 68910 Training loss 0.013648134656250477 Validation loss 0.02077382430434227 Accuracy 0.78515625\n",
      "Iteration 68920 Training loss 0.011448986828327179 Validation loss 0.021081537008285522 Accuracy 0.78125\n",
      "Iteration 68930 Training loss 0.013533965684473515 Validation loss 0.020642340183258057 Accuracy 0.7861328125\n",
      "Iteration 68940 Training loss 0.01257394254207611 Validation loss 0.0207517109811306 Accuracy 0.7841796875\n",
      "Iteration 68950 Training loss 0.014984549954533577 Validation loss 0.020951738581061363 Accuracy 0.78271484375\n",
      "Iteration 68960 Training loss 0.01425839588046074 Validation loss 0.02082282118499279 Accuracy 0.7841796875\n",
      "Iteration 68970 Training loss 0.013829303905367851 Validation loss 0.0212191604077816 Accuracy 0.77978515625\n",
      "Iteration 68980 Training loss 0.010938100516796112 Validation loss 0.02124263532459736 Accuracy 0.7802734375\n",
      "Iteration 68990 Training loss 0.013010978698730469 Validation loss 0.020994309335947037 Accuracy 0.78173828125\n",
      "Iteration 69000 Training loss 0.014407329261302948 Validation loss 0.020889325067400932 Accuracy 0.7841796875\n",
      "Iteration 69010 Training loss 0.012885445728898048 Validation loss 0.020949365571141243 Accuracy 0.783203125\n",
      "Iteration 69020 Training loss 0.013355130329728127 Validation loss 0.021002328023314476 Accuracy 0.783203125\n",
      "Iteration 69030 Training loss 0.010503132827579975 Validation loss 0.021083347499370575 Accuracy 0.78173828125\n",
      "Iteration 69040 Training loss 0.011287114582955837 Validation loss 0.020971376448869705 Accuracy 0.783203125\n",
      "Iteration 69050 Training loss 0.014918330125510693 Validation loss 0.021008504554629326 Accuracy 0.7841796875\n",
      "Iteration 69060 Training loss 0.012315159663558006 Validation loss 0.02083536610007286 Accuracy 0.78369140625\n",
      "Iteration 69070 Training loss 0.011226775124669075 Validation loss 0.020751766860485077 Accuracy 0.78466796875\n",
      "Iteration 69080 Training loss 0.014111467637121677 Validation loss 0.02081265114247799 Accuracy 0.7841796875\n",
      "Iteration 69090 Training loss 0.012666510418057442 Validation loss 0.021104535087943077 Accuracy 0.7822265625\n",
      "Iteration 69100 Training loss 0.015523236244916916 Validation loss 0.02096315287053585 Accuracy 0.7841796875\n",
      "Iteration 69110 Training loss 0.012268460355699062 Validation loss 0.021028177812695503 Accuracy 0.7841796875\n",
      "Iteration 69120 Training loss 0.012325774878263474 Validation loss 0.020946089178323746 Accuracy 0.783203125\n",
      "Iteration 69130 Training loss 0.014925001189112663 Validation loss 0.02090328559279442 Accuracy 0.7841796875\n",
      "Iteration 69140 Training loss 0.013485404662787914 Validation loss 0.02102619595825672 Accuracy 0.78271484375\n",
      "Iteration 69150 Training loss 0.014734501019120216 Validation loss 0.02089756727218628 Accuracy 0.78369140625\n",
      "Iteration 69160 Training loss 0.011370501480996609 Validation loss 0.020921459421515465 Accuracy 0.78271484375\n",
      "Iteration 69170 Training loss 0.015203440561890602 Validation loss 0.021078143268823624 Accuracy 0.78173828125\n",
      "Iteration 69180 Training loss 0.01528574526309967 Validation loss 0.020827990025281906 Accuracy 0.78369140625\n",
      "Iteration 69190 Training loss 0.013097915798425674 Validation loss 0.021123774349689484 Accuracy 0.78125\n",
      "Iteration 69200 Training loss 0.012897773645818233 Validation loss 0.020987465977668762 Accuracy 0.78173828125\n",
      "Iteration 69210 Training loss 0.011622443795204163 Validation loss 0.02097405306994915 Accuracy 0.78173828125\n",
      "Iteration 69220 Training loss 0.014913330785930157 Validation loss 0.021124310791492462 Accuracy 0.7822265625\n",
      "Iteration 69230 Training loss 0.013361353427171707 Validation loss 0.020976992323994637 Accuracy 0.7822265625\n",
      "Iteration 69240 Training loss 0.013139094226062298 Validation loss 0.021101538091897964 Accuracy 0.7802734375\n",
      "Iteration 69250 Training loss 0.012469040229916573 Validation loss 0.02090797759592533 Accuracy 0.78271484375\n",
      "Iteration 69260 Training loss 0.013321912847459316 Validation loss 0.020955078303813934 Accuracy 0.783203125\n",
      "Iteration 69270 Training loss 0.013844466768205166 Validation loss 0.021060138940811157 Accuracy 0.78173828125\n",
      "Iteration 69280 Training loss 0.01509245578199625 Validation loss 0.021163566038012505 Accuracy 0.78173828125\n",
      "Iteration 69290 Training loss 0.014715634286403656 Validation loss 0.02086147852241993 Accuracy 0.7841796875\n",
      "Iteration 69300 Training loss 0.013408508151769638 Validation loss 0.020965004339814186 Accuracy 0.7822265625\n",
      "Iteration 69310 Training loss 0.016716351732611656 Validation loss 0.02077428810298443 Accuracy 0.78564453125\n",
      "Iteration 69320 Training loss 0.01150541566312313 Validation loss 0.021026572212576866 Accuracy 0.7822265625\n",
      "Iteration 69330 Training loss 0.013980994001030922 Validation loss 0.020941611379384995 Accuracy 0.78271484375\n",
      "Iteration 69340 Training loss 0.012491307221353054 Validation loss 0.021282698959112167 Accuracy 0.779296875\n",
      "Iteration 69350 Training loss 0.014038791880011559 Validation loss 0.020774472504854202 Accuracy 0.7841796875\n",
      "Iteration 69360 Training loss 0.012323722243309021 Validation loss 0.020878281444311142 Accuracy 0.7841796875\n",
      "Iteration 69370 Training loss 0.012248360551893711 Validation loss 0.02088048867881298 Accuracy 0.7841796875\n",
      "Iteration 69380 Training loss 0.014297882094979286 Validation loss 0.020841479301452637 Accuracy 0.78466796875\n",
      "Iteration 69390 Training loss 0.014192513190209866 Validation loss 0.021139206364750862 Accuracy 0.78076171875\n",
      "Iteration 69400 Training loss 0.013236558996140957 Validation loss 0.020983027294278145 Accuracy 0.78369140625\n",
      "Iteration 69410 Training loss 0.011361399665474892 Validation loss 0.021040000021457672 Accuracy 0.78173828125\n",
      "Iteration 69420 Training loss 0.014228709042072296 Validation loss 0.02089766040444374 Accuracy 0.7841796875\n",
      "Iteration 69430 Training loss 0.013195723295211792 Validation loss 0.02154690772294998 Accuracy 0.7783203125\n",
      "Iteration 69440 Training loss 0.013542027212679386 Validation loss 0.021320974454283714 Accuracy 0.78076171875\n",
      "Iteration 69450 Training loss 0.011624221689999104 Validation loss 0.02093050442636013 Accuracy 0.783203125\n",
      "Iteration 69460 Training loss 0.01401682011783123 Validation loss 0.02127179689705372 Accuracy 0.7802734375\n",
      "Iteration 69470 Training loss 0.015145165845751762 Validation loss 0.021272070705890656 Accuracy 0.78173828125\n",
      "Iteration 69480 Training loss 0.014647741802036762 Validation loss 0.020877083763480186 Accuracy 0.78369140625\n",
      "Iteration 69490 Training loss 0.014961451292037964 Validation loss 0.02132156677544117 Accuracy 0.77880859375\n",
      "Iteration 69500 Training loss 0.01695478893816471 Validation loss 0.020862914621829987 Accuracy 0.78466796875\n",
      "Iteration 69510 Training loss 0.014725960791110992 Validation loss 0.02092677727341652 Accuracy 0.78271484375\n",
      "Iteration 69520 Training loss 0.013877197168767452 Validation loss 0.02096034586429596 Accuracy 0.78271484375\n",
      "Iteration 69530 Training loss 0.015502505004405975 Validation loss 0.02092946693301201 Accuracy 0.783203125\n",
      "Iteration 69540 Training loss 0.009982060641050339 Validation loss 0.0208803191781044 Accuracy 0.78369140625\n",
      "Iteration 69550 Training loss 0.016337625682353973 Validation loss 0.0210417527705431 Accuracy 0.78271484375\n",
      "Iteration 69560 Training loss 0.013737319968640804 Validation loss 0.020931031554937363 Accuracy 0.7841796875\n",
      "Iteration 69570 Training loss 0.012582211755216122 Validation loss 0.020951012149453163 Accuracy 0.78369140625\n",
      "Iteration 69580 Training loss 0.013637705706059933 Validation loss 0.020902372896671295 Accuracy 0.78369140625\n",
      "Iteration 69590 Training loss 0.014444401487708092 Validation loss 0.02100186049938202 Accuracy 0.78271484375\n",
      "Iteration 69600 Training loss 0.01219959370791912 Validation loss 0.021146638318896294 Accuracy 0.78173828125\n",
      "Iteration 69610 Training loss 0.014760126359760761 Validation loss 0.020967241376638412 Accuracy 0.78271484375\n",
      "Iteration 69620 Training loss 0.013706774450838566 Validation loss 0.020873695611953735 Accuracy 0.783203125\n",
      "Iteration 69630 Training loss 0.014003124088048935 Validation loss 0.02111210487782955 Accuracy 0.7822265625\n",
      "Iteration 69640 Training loss 0.014805961400270462 Validation loss 0.021058103069663048 Accuracy 0.78125\n",
      "Iteration 69650 Training loss 0.014006647281348705 Validation loss 0.02137680910527706 Accuracy 0.779296875\n",
      "Iteration 69660 Training loss 0.01397224422544241 Validation loss 0.02091989852488041 Accuracy 0.78369140625\n",
      "Iteration 69670 Training loss 0.012279294431209564 Validation loss 0.021029802039265633 Accuracy 0.78125\n",
      "Iteration 69680 Training loss 0.01289457455277443 Validation loss 0.021070346236228943 Accuracy 0.78173828125\n",
      "Iteration 69690 Training loss 0.013036748394370079 Validation loss 0.02129240706562996 Accuracy 0.78076171875\n",
      "Iteration 69700 Training loss 0.01269365381449461 Validation loss 0.020871944725513458 Accuracy 0.7841796875\n",
      "Iteration 69710 Training loss 0.012099914252758026 Validation loss 0.020896699279546738 Accuracy 0.78369140625\n",
      "Iteration 69720 Training loss 0.012850186787545681 Validation loss 0.020835507661104202 Accuracy 0.783203125\n",
      "Iteration 69730 Training loss 0.015841441228985786 Validation loss 0.022125916555523872 Accuracy 0.771484375\n",
      "Iteration 69740 Training loss 0.013872946612536907 Validation loss 0.021099207922816277 Accuracy 0.78173828125\n",
      "Iteration 69750 Training loss 0.01550020556896925 Validation loss 0.021103844046592712 Accuracy 0.78076171875\n",
      "Iteration 69760 Training loss 0.016901809722185135 Validation loss 0.02161797322332859 Accuracy 0.775390625\n",
      "Iteration 69770 Training loss 0.014964304864406586 Validation loss 0.020933499559760094 Accuracy 0.78173828125\n",
      "Iteration 69780 Training loss 0.013267447240650654 Validation loss 0.020796366035938263 Accuracy 0.78369140625\n",
      "Iteration 69790 Training loss 0.0136905862018466 Validation loss 0.020963823422789574 Accuracy 0.783203125\n",
      "Iteration 69800 Training loss 0.014331820420920849 Validation loss 0.02087603136897087 Accuracy 0.78369140625\n",
      "Iteration 69810 Training loss 0.012108910828828812 Validation loss 0.020974867045879364 Accuracy 0.783203125\n",
      "Iteration 69820 Training loss 0.01465232577174902 Validation loss 0.02078704535961151 Accuracy 0.78515625\n",
      "Iteration 69830 Training loss 0.014690455049276352 Validation loss 0.020894957706332207 Accuracy 0.7841796875\n",
      "Iteration 69840 Training loss 0.016193101182579994 Validation loss 0.020931558683514595 Accuracy 0.783203125\n",
      "Iteration 69850 Training loss 0.012251649051904678 Validation loss 0.02088351547718048 Accuracy 0.78369140625\n",
      "Iteration 69860 Training loss 0.01607983186841011 Validation loss 0.02173307165503502 Accuracy 0.775390625\n",
      "Iteration 69870 Training loss 0.012437041848897934 Validation loss 0.021086396649479866 Accuracy 0.78173828125\n",
      "Iteration 69880 Training loss 0.016616541892290115 Validation loss 0.020901912823319435 Accuracy 0.78369140625\n",
      "Iteration 69890 Training loss 0.013224761001765728 Validation loss 0.021122651174664497 Accuracy 0.78076171875\n",
      "Iteration 69900 Training loss 0.01354395505040884 Validation loss 0.02123977802693844 Accuracy 0.77978515625\n",
      "Iteration 69910 Training loss 0.01262413989752531 Validation loss 0.021269960328936577 Accuracy 0.779296875\n",
      "Iteration 69920 Training loss 0.01671750657260418 Validation loss 0.020948275923728943 Accuracy 0.783203125\n",
      "Iteration 69930 Training loss 0.014318299479782581 Validation loss 0.021216288208961487 Accuracy 0.7802734375\n",
      "Iteration 69940 Training loss 0.012918525375425816 Validation loss 0.0211297869682312 Accuracy 0.78125\n",
      "Iteration 69950 Training loss 0.014184186235070229 Validation loss 0.0210113525390625 Accuracy 0.78173828125\n",
      "Iteration 69960 Training loss 0.015628229826688766 Validation loss 0.02108434960246086 Accuracy 0.7822265625\n",
      "Iteration 69970 Training loss 0.012273264117538929 Validation loss 0.02090221643447876 Accuracy 0.783203125\n",
      "Iteration 69980 Training loss 0.013493232429027557 Validation loss 0.02102973498404026 Accuracy 0.78173828125\n",
      "Iteration 69990 Training loss 0.015828745439648628 Validation loss 0.022420814260840416 Accuracy 0.767578125\n",
      "Iteration 70000 Training loss 0.01593414507806301 Validation loss 0.02101498655974865 Accuracy 0.78173828125\n",
      "Iteration 70010 Training loss 0.01409640721976757 Validation loss 0.021208804100751877 Accuracy 0.78076171875\n",
      "Iteration 70020 Training loss 0.014163202606141567 Validation loss 0.020985370501875877 Accuracy 0.78173828125\n",
      "Iteration 70030 Training loss 0.013590983115136623 Validation loss 0.02087830938398838 Accuracy 0.7841796875\n",
      "Iteration 70040 Training loss 0.013858930207788944 Validation loss 0.0212237648665905 Accuracy 0.77978515625\n",
      "Iteration 70050 Training loss 0.014846506528556347 Validation loss 0.020985739305615425 Accuracy 0.7822265625\n",
      "Iteration 70060 Training loss 0.014179147779941559 Validation loss 0.02106054686009884 Accuracy 0.78173828125\n",
      "Iteration 70070 Training loss 0.013717018067836761 Validation loss 0.020956123247742653 Accuracy 0.78369140625\n",
      "Iteration 70080 Training loss 0.013519604690372944 Validation loss 0.020908992737531662 Accuracy 0.78369140625\n",
      "Iteration 70090 Training loss 0.014279985800385475 Validation loss 0.02112794667482376 Accuracy 0.78271484375\n",
      "Iteration 70100 Training loss 0.011965682730078697 Validation loss 0.02086169458925724 Accuracy 0.78369140625\n",
      "Iteration 70110 Training loss 0.014560526236891747 Validation loss 0.02151382714509964 Accuracy 0.7783203125\n",
      "Iteration 70120 Training loss 0.01173031609505415 Validation loss 0.02080228552222252 Accuracy 0.78466796875\n",
      "Iteration 70130 Training loss 0.014474647119641304 Validation loss 0.02098086103796959 Accuracy 0.7822265625\n",
      "Iteration 70140 Training loss 0.016965603455901146 Validation loss 0.02086261846125126 Accuracy 0.78369140625\n",
      "Iteration 70150 Training loss 0.011629580520093441 Validation loss 0.02098388411104679 Accuracy 0.783203125\n",
      "Iteration 70160 Training loss 0.017420070245862007 Validation loss 0.02093755267560482 Accuracy 0.78271484375\n",
      "Iteration 70170 Training loss 0.01269288919866085 Validation loss 0.021194154396653175 Accuracy 0.78076171875\n",
      "Iteration 70180 Training loss 0.015012700110673904 Validation loss 0.02074938826262951 Accuracy 0.78466796875\n",
      "Iteration 70190 Training loss 0.012667047791182995 Validation loss 0.02094835415482521 Accuracy 0.78271484375\n",
      "Iteration 70200 Training loss 0.01440712995827198 Validation loss 0.02110271342098713 Accuracy 0.78271484375\n",
      "Iteration 70210 Training loss 0.015211262740194798 Validation loss 0.0208450797945261 Accuracy 0.7841796875\n",
      "Iteration 70220 Training loss 0.016000742092728615 Validation loss 0.02082601562142372 Accuracy 0.7841796875\n",
      "Iteration 70230 Training loss 0.015649953857064247 Validation loss 0.02087170071899891 Accuracy 0.783203125\n",
      "Iteration 70240 Training loss 0.01635240949690342 Validation loss 0.020768696442246437 Accuracy 0.78564453125\n",
      "Iteration 70250 Training loss 0.012926393188536167 Validation loss 0.021009501069784164 Accuracy 0.7822265625\n",
      "Iteration 70260 Training loss 0.014599738642573357 Validation loss 0.0208734218031168 Accuracy 0.7841796875\n",
      "Iteration 70270 Training loss 0.014962058514356613 Validation loss 0.02076403982937336 Accuracy 0.78466796875\n",
      "Iteration 70280 Training loss 0.015642819926142693 Validation loss 0.020899569615721703 Accuracy 0.7841796875\n",
      "Iteration 70290 Training loss 0.014317961409687996 Validation loss 0.020926205441355705 Accuracy 0.78369140625\n",
      "Iteration 70300 Training loss 0.012958861887454987 Validation loss 0.020839404314756393 Accuracy 0.78466796875\n",
      "Iteration 70310 Training loss 0.014572935178875923 Validation loss 0.02082221210002899 Accuracy 0.7841796875\n",
      "Iteration 70320 Training loss 0.015472350642085075 Validation loss 0.02079690434038639 Accuracy 0.7841796875\n",
      "Iteration 70330 Training loss 0.012358258478343487 Validation loss 0.020814096555113792 Accuracy 0.78369140625\n",
      "Iteration 70340 Training loss 0.016936352476477623 Validation loss 0.021011056378483772 Accuracy 0.7822265625\n",
      "Iteration 70350 Training loss 0.012196899391710758 Validation loss 0.02096695825457573 Accuracy 0.78369140625\n",
      "Iteration 70360 Training loss 0.012468630447983742 Validation loss 0.02113150991499424 Accuracy 0.78125\n",
      "Iteration 70370 Training loss 0.011497249826788902 Validation loss 0.020904384553432465 Accuracy 0.783203125\n",
      "Iteration 70380 Training loss 0.01328989677131176 Validation loss 0.020828666165471077 Accuracy 0.78271484375\n",
      "Iteration 70390 Training loss 0.012910478748381138 Validation loss 0.0209980271756649 Accuracy 0.7822265625\n",
      "Iteration 70400 Training loss 0.013299452140927315 Validation loss 0.020994462072849274 Accuracy 0.7822265625\n",
      "Iteration 70410 Training loss 0.015181144699454308 Validation loss 0.021322377026081085 Accuracy 0.7783203125\n",
      "Iteration 70420 Training loss 0.012506731785833836 Validation loss 0.021354354918003082 Accuracy 0.77978515625\n",
      "Iteration 70430 Training loss 0.015152234584093094 Validation loss 0.02100330963730812 Accuracy 0.7822265625\n",
      "Iteration 70440 Training loss 0.016050444915890694 Validation loss 0.021145233884453773 Accuracy 0.78125\n",
      "Iteration 70450 Training loss 0.012008030898869038 Validation loss 0.021109238266944885 Accuracy 0.78271484375\n",
      "Iteration 70460 Training loss 0.010008550249040127 Validation loss 0.020985282957553864 Accuracy 0.783203125\n",
      "Iteration 70470 Training loss 0.01876128278672695 Validation loss 0.022756723687052727 Accuracy 0.76513671875\n",
      "Iteration 70480 Training loss 0.013845420442521572 Validation loss 0.020928561687469482 Accuracy 0.78271484375\n",
      "Iteration 70490 Training loss 0.014402272179722786 Validation loss 0.020946450531482697 Accuracy 0.7841796875\n",
      "Iteration 70500 Training loss 0.012736798264086246 Validation loss 0.0209762342274189 Accuracy 0.78271484375\n",
      "Iteration 70510 Training loss 0.009299667552113533 Validation loss 0.021011969074606895 Accuracy 0.783203125\n",
      "Iteration 70520 Training loss 0.012224250473082066 Validation loss 0.021016372367739677 Accuracy 0.78271484375\n",
      "Iteration 70530 Training loss 0.010988268069922924 Validation loss 0.021129587665200233 Accuracy 0.78173828125\n",
      "Iteration 70540 Training loss 0.016511501744389534 Validation loss 0.020998544991016388 Accuracy 0.783203125\n",
      "Iteration 70550 Training loss 0.015813875943422318 Validation loss 0.02100285328924656 Accuracy 0.783203125\n",
      "Iteration 70560 Training loss 0.01462885644286871 Validation loss 0.02111753076314926 Accuracy 0.7822265625\n",
      "Iteration 70570 Training loss 0.013063383288681507 Validation loss 0.020893631502985954 Accuracy 0.7841796875\n",
      "Iteration 70580 Training loss 0.016841046512126923 Validation loss 0.022061049938201904 Accuracy 0.771484375\n",
      "Iteration 70590 Training loss 0.013491865247488022 Validation loss 0.02108502946794033 Accuracy 0.7822265625\n",
      "Iteration 70600 Training loss 0.014171677641570568 Validation loss 0.02105405181646347 Accuracy 0.78076171875\n",
      "Iteration 70610 Training loss 0.013379066251218319 Validation loss 0.021004904061555862 Accuracy 0.78271484375\n",
      "Iteration 70620 Training loss 0.013722315430641174 Validation loss 0.0210561566054821 Accuracy 0.7822265625\n",
      "Iteration 70630 Training loss 0.013724970631301403 Validation loss 0.020974066108465195 Accuracy 0.783203125\n",
      "Iteration 70640 Training loss 0.011986518278717995 Validation loss 0.021242618560791016 Accuracy 0.78076171875\n",
      "Iteration 70650 Training loss 0.01392679288983345 Validation loss 0.021062221378087997 Accuracy 0.7822265625\n",
      "Iteration 70660 Training loss 0.013414247892796993 Validation loss 0.020858924835920334 Accuracy 0.7841796875\n",
      "Iteration 70670 Training loss 0.013558484613895416 Validation loss 0.0215611569583416 Accuracy 0.77783203125\n",
      "Iteration 70680 Training loss 0.0139103252440691 Validation loss 0.021907398477196693 Accuracy 0.77392578125\n",
      "Iteration 70690 Training loss 0.012946730479598045 Validation loss 0.020944220945239067 Accuracy 0.78369140625\n",
      "Iteration 70700 Training loss 0.01516664493829012 Validation loss 0.02106708474457264 Accuracy 0.783203125\n",
      "Iteration 70710 Training loss 0.012584156356751919 Validation loss 0.020931463688611984 Accuracy 0.78271484375\n",
      "Iteration 70720 Training loss 0.014367522671818733 Validation loss 0.02101179212331772 Accuracy 0.78271484375\n",
      "Iteration 70730 Training loss 0.016108950600028038 Validation loss 0.02087109163403511 Accuracy 0.78369140625\n",
      "Iteration 70740 Training loss 0.014016344211995602 Validation loss 0.02104044146835804 Accuracy 0.7822265625\n",
      "Iteration 70750 Training loss 0.013287256471812725 Validation loss 0.021051740273833275 Accuracy 0.7822265625\n",
      "Iteration 70760 Training loss 0.012621388770639896 Validation loss 0.020907601341605186 Accuracy 0.78369140625\n",
      "Iteration 70770 Training loss 0.01250332873314619 Validation loss 0.020907269790768623 Accuracy 0.783203125\n",
      "Iteration 70780 Training loss 0.013645673170685768 Validation loss 0.02108720690011978 Accuracy 0.78173828125\n",
      "Iteration 70790 Training loss 0.013500785455107689 Validation loss 0.02092040702700615 Accuracy 0.78369140625\n",
      "Iteration 70800 Training loss 0.011147227138280869 Validation loss 0.020961971953511238 Accuracy 0.78271484375\n",
      "Iteration 70810 Training loss 0.013027957640588284 Validation loss 0.021067680791020393 Accuracy 0.7822265625\n",
      "Iteration 70820 Training loss 0.01538213063031435 Validation loss 0.0209415964782238 Accuracy 0.78271484375\n",
      "Iteration 70830 Training loss 0.014538791961967945 Validation loss 0.021073773503303528 Accuracy 0.78271484375\n",
      "Iteration 70840 Training loss 0.011460714042186737 Validation loss 0.020881423726677895 Accuracy 0.783203125\n",
      "Iteration 70850 Training loss 0.01450579334050417 Validation loss 0.020906437188386917 Accuracy 0.783203125\n",
      "Iteration 70860 Training loss 0.014399724081158638 Validation loss 0.02091461792588234 Accuracy 0.78369140625\n",
      "Iteration 70870 Training loss 0.01285609696060419 Validation loss 0.02092350460588932 Accuracy 0.783203125\n",
      "Iteration 70880 Training loss 0.015427767299115658 Validation loss 0.02100568823516369 Accuracy 0.7822265625\n",
      "Iteration 70890 Training loss 0.013497255742549896 Validation loss 0.02101239189505577 Accuracy 0.7822265625\n",
      "Iteration 70900 Training loss 0.01175842247903347 Validation loss 0.021051554009318352 Accuracy 0.78076171875\n",
      "Iteration 70910 Training loss 0.012463393621146679 Validation loss 0.02091396413743496 Accuracy 0.783203125\n",
      "Iteration 70920 Training loss 0.015253018587827682 Validation loss 0.021053317934274673 Accuracy 0.7822265625\n",
      "Iteration 70930 Training loss 0.01443461887538433 Validation loss 0.02103978767991066 Accuracy 0.78271484375\n",
      "Iteration 70940 Training loss 0.015297302976250648 Validation loss 0.021206310018897057 Accuracy 0.78076171875\n",
      "Iteration 70950 Training loss 0.015238751657307148 Validation loss 0.021005423739552498 Accuracy 0.783203125\n",
      "Iteration 70960 Training loss 0.012660710141062737 Validation loss 0.021030882373452187 Accuracy 0.7822265625\n",
      "Iteration 70970 Training loss 0.012722885236144066 Validation loss 0.021097220480442047 Accuracy 0.78173828125\n",
      "Iteration 70980 Training loss 0.01755748875439167 Validation loss 0.02128106728196144 Accuracy 0.779296875\n",
      "Iteration 70990 Training loss 0.013222027570009232 Validation loss 0.02115335315465927 Accuracy 0.7822265625\n",
      "Iteration 71000 Training loss 0.010989115573465824 Validation loss 0.020948393270373344 Accuracy 0.78271484375\n",
      "Iteration 71010 Training loss 0.012914352118968964 Validation loss 0.021032657474279404 Accuracy 0.78271484375\n",
      "Iteration 71020 Training loss 0.01792253740131855 Validation loss 0.021066876128315926 Accuracy 0.78125\n",
      "Iteration 71030 Training loss 0.012631040066480637 Validation loss 0.021047787740826607 Accuracy 0.783203125\n",
      "Iteration 71040 Training loss 0.01367274485528469 Validation loss 0.02100960910320282 Accuracy 0.78173828125\n",
      "Iteration 71050 Training loss 0.01656128466129303 Validation loss 0.021353481337428093 Accuracy 0.779296875\n",
      "Iteration 71060 Training loss 0.01640322431921959 Validation loss 0.021256878972053528 Accuracy 0.77978515625\n",
      "Iteration 71070 Training loss 0.015589426271617413 Validation loss 0.02094203606247902 Accuracy 0.78271484375\n",
      "Iteration 71080 Training loss 0.013307230547070503 Validation loss 0.020856086164712906 Accuracy 0.78466796875\n",
      "Iteration 71090 Training loss 0.015096853487193584 Validation loss 0.02105085924267769 Accuracy 0.78173828125\n",
      "Iteration 71100 Training loss 0.011653906665742397 Validation loss 0.021423406898975372 Accuracy 0.77783203125\n",
      "Iteration 71110 Training loss 0.014304224401712418 Validation loss 0.021037597209215164 Accuracy 0.7822265625\n",
      "Iteration 71120 Training loss 0.011688517406582832 Validation loss 0.020976539701223373 Accuracy 0.783203125\n",
      "Iteration 71130 Training loss 0.012935086153447628 Validation loss 0.02147136628627777 Accuracy 0.77783203125\n",
      "Iteration 71140 Training loss 0.012871251441538334 Validation loss 0.02103087306022644 Accuracy 0.7822265625\n",
      "Iteration 71150 Training loss 0.0144940335303545 Validation loss 0.02218923345208168 Accuracy 0.77001953125\n",
      "Iteration 71160 Training loss 0.018371418118476868 Validation loss 0.02111073210835457 Accuracy 0.783203125\n",
      "Iteration 71170 Training loss 0.01319610420614481 Validation loss 0.021140361204743385 Accuracy 0.78173828125\n",
      "Iteration 71180 Training loss 0.014794031158089638 Validation loss 0.02098977006971836 Accuracy 0.78271484375\n",
      "Iteration 71190 Training loss 0.013664618134498596 Validation loss 0.021070731803774834 Accuracy 0.78173828125\n",
      "Iteration 71200 Training loss 0.012740119360387325 Validation loss 0.021053094416856766 Accuracy 0.78271484375\n",
      "Iteration 71210 Training loss 0.01263342797756195 Validation loss 0.020936332643032074 Accuracy 0.783203125\n",
      "Iteration 71220 Training loss 0.015355946496129036 Validation loss 0.020979464054107666 Accuracy 0.78271484375\n",
      "Iteration 71230 Training loss 0.012987054884433746 Validation loss 0.021327441558241844 Accuracy 0.77978515625\n",
      "Iteration 71240 Training loss 0.012779449112713337 Validation loss 0.021085049957036972 Accuracy 0.78173828125\n",
      "Iteration 71250 Training loss 0.014229347929358482 Validation loss 0.02109961397945881 Accuracy 0.78125\n",
      "Iteration 71260 Training loss 0.013486506417393684 Validation loss 0.021305639296770096 Accuracy 0.77978515625\n",
      "Iteration 71270 Training loss 0.012902434915304184 Validation loss 0.02103579230606556 Accuracy 0.7822265625\n",
      "Iteration 71280 Training loss 0.012983474880456924 Validation loss 0.02093414217233658 Accuracy 0.7822265625\n",
      "Iteration 71290 Training loss 0.014184321276843548 Validation loss 0.02090577408671379 Accuracy 0.783203125\n",
      "Iteration 71300 Training loss 0.013958684168756008 Validation loss 0.021113049238920212 Accuracy 0.78125\n",
      "Iteration 71310 Training loss 0.012965342029929161 Validation loss 0.021138587966561317 Accuracy 0.78125\n",
      "Iteration 71320 Training loss 0.012568769045174122 Validation loss 0.021106166765093803 Accuracy 0.7802734375\n",
      "Iteration 71330 Training loss 0.012769744731485844 Validation loss 0.020907960832118988 Accuracy 0.78271484375\n",
      "Iteration 71340 Training loss 0.013680243864655495 Validation loss 0.020947402343153954 Accuracy 0.783203125\n",
      "Iteration 71350 Training loss 0.013720067217946053 Validation loss 0.020961588248610497 Accuracy 0.78271484375\n",
      "Iteration 71360 Training loss 0.01272681262344122 Validation loss 0.021079281345009804 Accuracy 0.78173828125\n",
      "Iteration 71370 Training loss 0.013946235179901123 Validation loss 0.021088039502501488 Accuracy 0.78125\n",
      "Iteration 71380 Training loss 0.014189554378390312 Validation loss 0.020881302654743195 Accuracy 0.78369140625\n",
      "Iteration 71390 Training loss 0.012898910790681839 Validation loss 0.021262286230921745 Accuracy 0.77978515625\n",
      "Iteration 71400 Training loss 0.012894042767584324 Validation loss 0.02105918899178505 Accuracy 0.7822265625\n",
      "Iteration 71410 Training loss 0.013223245739936829 Validation loss 0.02112511731684208 Accuracy 0.78125\n",
      "Iteration 71420 Training loss 0.01595836691558361 Validation loss 0.020947201177477837 Accuracy 0.78271484375\n",
      "Iteration 71430 Training loss 0.016114698722958565 Validation loss 0.021214190870523453 Accuracy 0.78076171875\n",
      "Iteration 71440 Training loss 0.013548904098570347 Validation loss 0.021241215988993645 Accuracy 0.77978515625\n",
      "Iteration 71450 Training loss 0.01522836834192276 Validation loss 0.02128807082772255 Accuracy 0.7802734375\n",
      "Iteration 71460 Training loss 0.013474002480506897 Validation loss 0.020900754258036613 Accuracy 0.783203125\n",
      "Iteration 71470 Training loss 0.015292856842279434 Validation loss 0.020876824855804443 Accuracy 0.783203125\n",
      "Iteration 71480 Training loss 0.011916317977011204 Validation loss 0.020957015454769135 Accuracy 0.783203125\n",
      "Iteration 71490 Training loss 0.011986750178039074 Validation loss 0.021111784502863884 Accuracy 0.78076171875\n",
      "Iteration 71500 Training loss 0.01265703421086073 Validation loss 0.020850248634815216 Accuracy 0.78369140625\n",
      "Iteration 71510 Training loss 0.0139906732365489 Validation loss 0.02108706720173359 Accuracy 0.78076171875\n",
      "Iteration 71520 Training loss 0.013873329386115074 Validation loss 0.021214120090007782 Accuracy 0.78125\n",
      "Iteration 71530 Training loss 0.013759629800915718 Validation loss 0.020962215960025787 Accuracy 0.78271484375\n",
      "Iteration 71540 Training loss 0.013559105806052685 Validation loss 0.02078370563685894 Accuracy 0.783203125\n",
      "Iteration 71550 Training loss 0.014040264301002026 Validation loss 0.022054199129343033 Accuracy 0.7724609375\n",
      "Iteration 71560 Training loss 0.013889000751078129 Validation loss 0.020939208567142487 Accuracy 0.78271484375\n",
      "Iteration 71570 Training loss 0.012456838972866535 Validation loss 0.02142973802983761 Accuracy 0.77783203125\n",
      "Iteration 71580 Training loss 0.0103183314204216 Validation loss 0.020872334018349648 Accuracy 0.7841796875\n",
      "Iteration 71590 Training loss 0.013231740333139896 Validation loss 0.021026846021413803 Accuracy 0.7822265625\n",
      "Iteration 71600 Training loss 0.012241276912391186 Validation loss 0.020944686606526375 Accuracy 0.78271484375\n",
      "Iteration 71610 Training loss 0.014624273404479027 Validation loss 0.02097916416823864 Accuracy 0.7822265625\n",
      "Iteration 71620 Training loss 0.012486963532865047 Validation loss 0.02101491205394268 Accuracy 0.78369140625\n",
      "Iteration 71630 Training loss 0.01407677959650755 Validation loss 0.0210046898573637 Accuracy 0.783203125\n",
      "Iteration 71640 Training loss 0.011868744157254696 Validation loss 0.021147675812244415 Accuracy 0.7802734375\n",
      "Iteration 71650 Training loss 0.01294081099331379 Validation loss 0.021114401519298553 Accuracy 0.78125\n",
      "Iteration 71660 Training loss 0.019104287028312683 Validation loss 0.02189713902771473 Accuracy 0.7724609375\n",
      "Iteration 71670 Training loss 0.012947837822139263 Validation loss 0.021145008504390717 Accuracy 0.78076171875\n",
      "Iteration 71680 Training loss 0.013861920684576035 Validation loss 0.020935019478201866 Accuracy 0.78369140625\n",
      "Iteration 71690 Training loss 0.011793828569352627 Validation loss 0.021252280101180077 Accuracy 0.77880859375\n",
      "Iteration 71700 Training loss 0.012170231901109219 Validation loss 0.020914118736982346 Accuracy 0.78369140625\n",
      "Iteration 71710 Training loss 0.012150991708040237 Validation loss 0.020968899130821228 Accuracy 0.7822265625\n",
      "Iteration 71720 Training loss 0.015019219368696213 Validation loss 0.021015623584389687 Accuracy 0.7822265625\n",
      "Iteration 71730 Training loss 0.013790743425488472 Validation loss 0.02108871005475521 Accuracy 0.78173828125\n",
      "Iteration 71740 Training loss 0.016035839915275574 Validation loss 0.021113386377692223 Accuracy 0.78125\n",
      "Iteration 71750 Training loss 0.013712506741285324 Validation loss 0.020957572385668755 Accuracy 0.7822265625\n",
      "Iteration 71760 Training loss 0.014194742776453495 Validation loss 0.021099135279655457 Accuracy 0.78173828125\n",
      "Iteration 71770 Training loss 0.016649385914206505 Validation loss 0.02126128412783146 Accuracy 0.78125\n",
      "Iteration 71780 Training loss 0.012714267708361149 Validation loss 0.021195536479353905 Accuracy 0.7802734375\n",
      "Iteration 71790 Training loss 0.012757990509271622 Validation loss 0.02108621597290039 Accuracy 0.78076171875\n",
      "Iteration 71800 Training loss 0.012964786030352116 Validation loss 0.020937683060765266 Accuracy 0.78271484375\n",
      "Iteration 71810 Training loss 0.01582004874944687 Validation loss 0.02106582000851631 Accuracy 0.7822265625\n",
      "Iteration 71820 Training loss 0.015416745096445084 Validation loss 0.021132757887244225 Accuracy 0.78125\n",
      "Iteration 71830 Training loss 0.012162347324192524 Validation loss 0.02104170061647892 Accuracy 0.7822265625\n",
      "Iteration 71840 Training loss 0.009767776355147362 Validation loss 0.021071327850222588 Accuracy 0.78173828125\n",
      "Iteration 71850 Training loss 0.014609143137931824 Validation loss 0.020914774388074875 Accuracy 0.78369140625\n",
      "Iteration 71860 Training loss 0.012311180122196674 Validation loss 0.021674521267414093 Accuracy 0.77587890625\n",
      "Iteration 71870 Training loss 0.010908479802310467 Validation loss 0.021071158349514008 Accuracy 0.78173828125\n",
      "Iteration 71880 Training loss 0.010871164500713348 Validation loss 0.020820165053009987 Accuracy 0.78369140625\n",
      "Iteration 71890 Training loss 0.012214715592563152 Validation loss 0.02112755738198757 Accuracy 0.78173828125\n",
      "Iteration 71900 Training loss 0.011408990249037743 Validation loss 0.020927343517541885 Accuracy 0.78271484375\n",
      "Iteration 71910 Training loss 0.014778726734220982 Validation loss 0.02103809267282486 Accuracy 0.7822265625\n",
      "Iteration 71920 Training loss 0.011826474219560623 Validation loss 0.020916100591421127 Accuracy 0.783203125\n",
      "Iteration 71930 Training loss 0.01401916891336441 Validation loss 0.020988963544368744 Accuracy 0.78173828125\n",
      "Iteration 71940 Training loss 0.014923712238669395 Validation loss 0.021583016961812973 Accuracy 0.77734375\n",
      "Iteration 71950 Training loss 0.013035064563155174 Validation loss 0.02103450708091259 Accuracy 0.7822265625\n",
      "Iteration 71960 Training loss 0.012885134667158127 Validation loss 0.02092267945408821 Accuracy 0.78271484375\n",
      "Iteration 71970 Training loss 0.012663744390010834 Validation loss 0.021009793505072594 Accuracy 0.78173828125\n",
      "Iteration 71980 Training loss 0.012051520869135857 Validation loss 0.020979585126042366 Accuracy 0.7822265625\n",
      "Iteration 71990 Training loss 0.012621131725609303 Validation loss 0.02123173512518406 Accuracy 0.7802734375\n",
      "Iteration 72000 Training loss 0.01415698230266571 Validation loss 0.021055370569229126 Accuracy 0.7822265625\n",
      "Iteration 72010 Training loss 0.015814155340194702 Validation loss 0.021053295582532883 Accuracy 0.78125\n",
      "Iteration 72020 Training loss 0.01448160782456398 Validation loss 0.021019862964749336 Accuracy 0.7822265625\n",
      "Iteration 72030 Training loss 0.013532924465835094 Validation loss 0.02104475535452366 Accuracy 0.7822265625\n",
      "Iteration 72040 Training loss 0.014081714674830437 Validation loss 0.021050428971648216 Accuracy 0.7822265625\n",
      "Iteration 72050 Training loss 0.013762555085122585 Validation loss 0.020980140194296837 Accuracy 0.78173828125\n",
      "Iteration 72060 Training loss 0.014241681434214115 Validation loss 0.02102029323577881 Accuracy 0.7822265625\n",
      "Iteration 72070 Training loss 0.01701400615274906 Validation loss 0.0211392380297184 Accuracy 0.78125\n",
      "Iteration 72080 Training loss 0.014341961592435837 Validation loss 0.021316396072506905 Accuracy 0.78076171875\n",
      "Iteration 72090 Training loss 0.016476450487971306 Validation loss 0.021046552807092667 Accuracy 0.78271484375\n",
      "Iteration 72100 Training loss 0.014200683683156967 Validation loss 0.021198216825723648 Accuracy 0.78076171875\n",
      "Iteration 72110 Training loss 0.013624448329210281 Validation loss 0.02105037495493889 Accuracy 0.7822265625\n",
      "Iteration 72120 Training loss 0.014931813813745975 Validation loss 0.020893128588795662 Accuracy 0.78271484375\n",
      "Iteration 72130 Training loss 0.01572946086525917 Validation loss 0.021166276186704636 Accuracy 0.78125\n",
      "Iteration 72140 Training loss 0.015630342066287994 Validation loss 0.02103421278297901 Accuracy 0.7822265625\n",
      "Iteration 72150 Training loss 0.015573229640722275 Validation loss 0.021679330617189407 Accuracy 0.77587890625\n",
      "Iteration 72160 Training loss 0.01412674318999052 Validation loss 0.020972736179828644 Accuracy 0.78369140625\n",
      "Iteration 72170 Training loss 0.011897646822035313 Validation loss 0.02114170789718628 Accuracy 0.7822265625\n",
      "Iteration 72180 Training loss 0.010829771868884563 Validation loss 0.021187378093600273 Accuracy 0.78125\n",
      "Iteration 72190 Training loss 0.014674858190119267 Validation loss 0.02090325579047203 Accuracy 0.783203125\n",
      "Iteration 72200 Training loss 0.010979875922203064 Validation loss 0.02102036029100418 Accuracy 0.7822265625\n",
      "Iteration 72210 Training loss 0.01340724527835846 Validation loss 0.020860634744167328 Accuracy 0.783203125\n",
      "Iteration 72220 Training loss 0.014664052985608578 Validation loss 0.02099207416176796 Accuracy 0.78271484375\n",
      "Iteration 72230 Training loss 0.014907468110322952 Validation loss 0.021562527865171432 Accuracy 0.77734375\n",
      "Iteration 72240 Training loss 0.012166078202426434 Validation loss 0.020948661491274834 Accuracy 0.78271484375\n",
      "Iteration 72250 Training loss 0.012512151151895523 Validation loss 0.021034320816397667 Accuracy 0.78271484375\n",
      "Iteration 72260 Training loss 0.011226491071283817 Validation loss 0.021139292046427727 Accuracy 0.779296875\n",
      "Iteration 72270 Training loss 0.013594966381788254 Validation loss 0.021040456369519234 Accuracy 0.78173828125\n",
      "Iteration 72280 Training loss 0.013664321973919868 Validation loss 0.02087620086967945 Accuracy 0.78369140625\n",
      "Iteration 72290 Training loss 0.01429569348692894 Validation loss 0.021087776869535446 Accuracy 0.7822265625\n",
      "Iteration 72300 Training loss 0.012778800912201405 Validation loss 0.020972304046154022 Accuracy 0.783203125\n",
      "Iteration 72310 Training loss 0.013219851069152355 Validation loss 0.02073262631893158 Accuracy 0.78466796875\n",
      "Iteration 72320 Training loss 0.014339695684611797 Validation loss 0.020850688219070435 Accuracy 0.7841796875\n",
      "Iteration 72330 Training loss 0.015444843098521233 Validation loss 0.020843010395765305 Accuracy 0.78466796875\n",
      "Iteration 72340 Training loss 0.015401460230350494 Validation loss 0.020874682813882828 Accuracy 0.78369140625\n",
      "Iteration 72350 Training loss 0.013674840331077576 Validation loss 0.02121063508093357 Accuracy 0.779296875\n",
      "Iteration 72360 Training loss 0.015384653583168983 Validation loss 0.021189814433455467 Accuracy 0.78173828125\n",
      "Iteration 72370 Training loss 0.01504864264279604 Validation loss 0.020884374156594276 Accuracy 0.7841796875\n",
      "Iteration 72380 Training loss 0.013168247416615486 Validation loss 0.020756565034389496 Accuracy 0.78515625\n",
      "Iteration 72390 Training loss 0.01346659567207098 Validation loss 0.021000804379582405 Accuracy 0.7822265625\n",
      "Iteration 72400 Training loss 0.01353383157402277 Validation loss 0.02094847522675991 Accuracy 0.783203125\n",
      "Iteration 72410 Training loss 0.01275323610752821 Validation loss 0.020902348682284355 Accuracy 0.783203125\n",
      "Iteration 72420 Training loss 0.01385832205414772 Validation loss 0.02099783904850483 Accuracy 0.7822265625\n",
      "Iteration 72430 Training loss 0.014785588718950748 Validation loss 0.020822031423449516 Accuracy 0.78466796875\n",
      "Iteration 72440 Training loss 0.015007220208644867 Validation loss 0.020701494067907333 Accuracy 0.78564453125\n",
      "Iteration 72450 Training loss 0.01562145259231329 Validation loss 0.0210826825350523 Accuracy 0.78271484375\n",
      "Iteration 72460 Training loss 0.014494805596768856 Validation loss 0.020840145647525787 Accuracy 0.78369140625\n",
      "Iteration 72470 Training loss 0.015135938301682472 Validation loss 0.020801756531000137 Accuracy 0.7841796875\n",
      "Iteration 72480 Training loss 0.01388497930020094 Validation loss 0.02094944193959236 Accuracy 0.78369140625\n",
      "Iteration 72490 Training loss 0.015494165942072868 Validation loss 0.020944051444530487 Accuracy 0.7841796875\n",
      "Iteration 72500 Training loss 0.013982321135699749 Validation loss 0.020924458280205727 Accuracy 0.78369140625\n",
      "Iteration 72510 Training loss 0.01424868032336235 Validation loss 0.020840618759393692 Accuracy 0.7841796875\n",
      "Iteration 72520 Training loss 0.012450270354747772 Validation loss 0.02126278355717659 Accuracy 0.78125\n",
      "Iteration 72530 Training loss 0.015739694237709045 Validation loss 0.020795978605747223 Accuracy 0.783203125\n",
      "Iteration 72540 Training loss 0.012886695563793182 Validation loss 0.020812468603253365 Accuracy 0.783203125\n",
      "Iteration 72550 Training loss 0.013419145718216896 Validation loss 0.02110864967107773 Accuracy 0.7822265625\n",
      "Iteration 72560 Training loss 0.015581654384732246 Validation loss 0.020974846556782722 Accuracy 0.783203125\n",
      "Iteration 72570 Training loss 0.01467793807387352 Validation loss 0.020982399582862854 Accuracy 0.783203125\n",
      "Iteration 72580 Training loss 0.011859285645186901 Validation loss 0.020819002762436867 Accuracy 0.78515625\n",
      "Iteration 72590 Training loss 0.014907650649547577 Validation loss 0.021280737593770027 Accuracy 0.78076171875\n",
      "Iteration 72600 Training loss 0.012345364317297935 Validation loss 0.020908299833536148 Accuracy 0.78369140625\n",
      "Iteration 72610 Training loss 0.010834205895662308 Validation loss 0.020987996831536293 Accuracy 0.78369140625\n",
      "Iteration 72620 Training loss 0.01431504637002945 Validation loss 0.021163037046790123 Accuracy 0.78173828125\n",
      "Iteration 72630 Training loss 0.014556707814335823 Validation loss 0.020941419526934624 Accuracy 0.7841796875\n",
      "Iteration 72640 Training loss 0.016439514234662056 Validation loss 0.02121252566576004 Accuracy 0.7822265625\n",
      "Iteration 72650 Training loss 0.014225765131413937 Validation loss 0.020924411714076996 Accuracy 0.78466796875\n",
      "Iteration 72660 Training loss 0.01240994967520237 Validation loss 0.021069467067718506 Accuracy 0.783203125\n",
      "Iteration 72670 Training loss 0.01270093023777008 Validation loss 0.02104366198182106 Accuracy 0.783203125\n",
      "Iteration 72680 Training loss 0.013602877967059612 Validation loss 0.021114785224199295 Accuracy 0.7822265625\n",
      "Iteration 72690 Training loss 0.015827612951397896 Validation loss 0.02103809453547001 Accuracy 0.78271484375\n",
      "Iteration 72700 Training loss 0.016676193103194237 Validation loss 0.021722257137298584 Accuracy 0.775390625\n",
      "Iteration 72710 Training loss 0.014017394743859768 Validation loss 0.0209774412214756 Accuracy 0.7841796875\n",
      "Iteration 72720 Training loss 0.01229680236428976 Validation loss 0.02096535824239254 Accuracy 0.7841796875\n",
      "Iteration 72730 Training loss 0.011647074483335018 Validation loss 0.020829707384109497 Accuracy 0.78369140625\n",
      "Iteration 72740 Training loss 0.014909237623214722 Validation loss 0.020915541797876358 Accuracy 0.783203125\n",
      "Iteration 72750 Training loss 0.012837497517466545 Validation loss 0.020831752568483353 Accuracy 0.78466796875\n",
      "Iteration 72760 Training loss 0.010537070222198963 Validation loss 0.020937353372573853 Accuracy 0.7822265625\n",
      "Iteration 72770 Training loss 0.013233177363872528 Validation loss 0.021046005189418793 Accuracy 0.7822265625\n",
      "Iteration 72780 Training loss 0.013902689330279827 Validation loss 0.020946908742189407 Accuracy 0.7841796875\n",
      "Iteration 72790 Training loss 0.012287424877285957 Validation loss 0.020935654640197754 Accuracy 0.7841796875\n",
      "Iteration 72800 Training loss 0.011213503777980804 Validation loss 0.020914200693368912 Accuracy 0.7841796875\n",
      "Iteration 72810 Training loss 0.012483357451856136 Validation loss 0.02077474631369114 Accuracy 0.78515625\n",
      "Iteration 72820 Training loss 0.01430570986121893 Validation loss 0.020837731659412384 Accuracy 0.783203125\n",
      "Iteration 72830 Training loss 0.013671226799488068 Validation loss 0.02096583880484104 Accuracy 0.78369140625\n",
      "Iteration 72840 Training loss 0.015123800374567509 Validation loss 0.020932894200086594 Accuracy 0.783203125\n",
      "Iteration 72850 Training loss 0.01325466949492693 Validation loss 0.020748497918248177 Accuracy 0.78515625\n",
      "Iteration 72860 Training loss 0.015857119113206863 Validation loss 0.020700747147202492 Accuracy 0.78515625\n",
      "Iteration 72870 Training loss 0.014498809352517128 Validation loss 0.02160264551639557 Accuracy 0.77685546875\n",
      "Iteration 72880 Training loss 0.016590362414717674 Validation loss 0.021130291745066643 Accuracy 0.78173828125\n",
      "Iteration 72890 Training loss 0.01497872918844223 Validation loss 0.020991111174225807 Accuracy 0.7822265625\n",
      "Iteration 72900 Training loss 0.014221057295799255 Validation loss 0.020771315321326256 Accuracy 0.78466796875\n",
      "Iteration 72910 Training loss 0.012072382494807243 Validation loss 0.020858705043792725 Accuracy 0.783203125\n",
      "Iteration 72920 Training loss 0.014182167127728462 Validation loss 0.021156271919608116 Accuracy 0.7822265625\n",
      "Iteration 72930 Training loss 0.01347736082971096 Validation loss 0.021229630336165428 Accuracy 0.78076171875\n",
      "Iteration 72940 Training loss 0.013429834507405758 Validation loss 0.020995335653424263 Accuracy 0.78369140625\n",
      "Iteration 72950 Training loss 0.014156574383378029 Validation loss 0.02107085846364498 Accuracy 0.78173828125\n",
      "Iteration 72960 Training loss 0.013472147285938263 Validation loss 0.020991472527384758 Accuracy 0.783203125\n",
      "Iteration 72970 Training loss 0.011969605460762978 Validation loss 0.021006595343351364 Accuracy 0.78271484375\n",
      "Iteration 72980 Training loss 0.01210717298090458 Validation loss 0.020941343158483505 Accuracy 0.7841796875\n",
      "Iteration 72990 Training loss 0.014269665814936161 Validation loss 0.020780697464942932 Accuracy 0.7841796875\n",
      "Iteration 73000 Training loss 0.012879402376711369 Validation loss 0.021066151559352875 Accuracy 0.78173828125\n",
      "Iteration 73010 Training loss 0.01149396225810051 Validation loss 0.02108479104936123 Accuracy 0.78125\n",
      "Iteration 73020 Training loss 0.012995848432183266 Validation loss 0.020903930068016052 Accuracy 0.78369140625\n",
      "Iteration 73030 Training loss 0.015805469825863838 Validation loss 0.02096090465784073 Accuracy 0.78271484375\n",
      "Iteration 73040 Training loss 0.01281091570854187 Validation loss 0.021296899765729904 Accuracy 0.77978515625\n",
      "Iteration 73050 Training loss 0.01435445249080658 Validation loss 0.020935382694005966 Accuracy 0.783203125\n",
      "Iteration 73060 Training loss 0.013866274617612362 Validation loss 0.020742669701576233 Accuracy 0.78466796875\n",
      "Iteration 73070 Training loss 0.01636548526585102 Validation loss 0.02084250934422016 Accuracy 0.78369140625\n",
      "Iteration 73080 Training loss 0.01260098721832037 Validation loss 0.021033398807048798 Accuracy 0.78076171875\n",
      "Iteration 73090 Training loss 0.014611342921853065 Validation loss 0.020820677280426025 Accuracy 0.783203125\n",
      "Iteration 73100 Training loss 0.013481046073138714 Validation loss 0.021018989384174347 Accuracy 0.78271484375\n",
      "Iteration 73110 Training loss 0.013138155452907085 Validation loss 0.02112683467566967 Accuracy 0.78173828125\n",
      "Iteration 73120 Training loss 0.012246825732290745 Validation loss 0.021393893286585808 Accuracy 0.779296875\n",
      "Iteration 73130 Training loss 0.014569936320185661 Validation loss 0.02090352028608322 Accuracy 0.783203125\n",
      "Iteration 73140 Training loss 0.014132455922663212 Validation loss 0.020983580499887466 Accuracy 0.78271484375\n",
      "Iteration 73150 Training loss 0.012156737968325615 Validation loss 0.02141263149678707 Accuracy 0.779296875\n",
      "Iteration 73160 Training loss 0.013887902721762657 Validation loss 0.02080562897026539 Accuracy 0.78369140625\n",
      "Iteration 73170 Training loss 0.012795908376574516 Validation loss 0.021154191344976425 Accuracy 0.78125\n",
      "Iteration 73180 Training loss 0.012147514149546623 Validation loss 0.020835155621170998 Accuracy 0.78369140625\n",
      "Iteration 73190 Training loss 0.015238175168633461 Validation loss 0.020909257233142853 Accuracy 0.78271484375\n",
      "Iteration 73200 Training loss 0.012362075969576836 Validation loss 0.020830849185585976 Accuracy 0.78369140625\n",
      "Iteration 73210 Training loss 0.013329756446182728 Validation loss 0.020864540711045265 Accuracy 0.78369140625\n",
      "Iteration 73220 Training loss 0.016499897465109825 Validation loss 0.020732101052999496 Accuracy 0.78515625\n",
      "Iteration 73230 Training loss 0.013486719690263271 Validation loss 0.020856551826000214 Accuracy 0.78271484375\n",
      "Iteration 73240 Training loss 0.013232776895165443 Validation loss 0.020886024460196495 Accuracy 0.78369140625\n",
      "Iteration 73250 Training loss 0.014721505343914032 Validation loss 0.021315941587090492 Accuracy 0.77880859375\n",
      "Iteration 73260 Training loss 0.015790222212672234 Validation loss 0.020997488871216774 Accuracy 0.7822265625\n",
      "Iteration 73270 Training loss 0.015562723390758038 Validation loss 0.020820345729589462 Accuracy 0.7841796875\n",
      "Iteration 73280 Training loss 0.012919326312839985 Validation loss 0.02078746072947979 Accuracy 0.7841796875\n",
      "Iteration 73290 Training loss 0.014115030877292156 Validation loss 0.020973587408661842 Accuracy 0.78271484375\n",
      "Iteration 73300 Training loss 0.014999029226601124 Validation loss 0.020804857835173607 Accuracy 0.7841796875\n",
      "Iteration 73310 Training loss 0.012325780466198921 Validation loss 0.020869970321655273 Accuracy 0.783203125\n",
      "Iteration 73320 Training loss 0.014820110984146595 Validation loss 0.021214956417679787 Accuracy 0.78076171875\n",
      "Iteration 73330 Training loss 0.011428690515458584 Validation loss 0.02075471356511116 Accuracy 0.78564453125\n",
      "Iteration 73340 Training loss 0.01202104426920414 Validation loss 0.020942680537700653 Accuracy 0.78369140625\n",
      "Iteration 73350 Training loss 0.014379452913999557 Validation loss 0.020827889442443848 Accuracy 0.7841796875\n",
      "Iteration 73360 Training loss 0.01409542839974165 Validation loss 0.021282454952597618 Accuracy 0.779296875\n",
      "Iteration 73370 Training loss 0.016609733924269676 Validation loss 0.020873915404081345 Accuracy 0.783203125\n",
      "Iteration 73380 Training loss 0.01327807828783989 Validation loss 0.02092193067073822 Accuracy 0.783203125\n",
      "Iteration 73390 Training loss 0.01654672995209694 Validation loss 0.020890457555651665 Accuracy 0.783203125\n",
      "Iteration 73400 Training loss 0.011247921735048294 Validation loss 0.020766111090779305 Accuracy 0.7841796875\n",
      "Iteration 73410 Training loss 0.01211570855230093 Validation loss 0.02076435089111328 Accuracy 0.7841796875\n",
      "Iteration 73420 Training loss 0.013177305459976196 Validation loss 0.02075955457985401 Accuracy 0.7841796875\n",
      "Iteration 73430 Training loss 0.012576253153383732 Validation loss 0.020981967449188232 Accuracy 0.7822265625\n",
      "Iteration 73440 Training loss 0.011909902095794678 Validation loss 0.0209445022046566 Accuracy 0.78271484375\n",
      "Iteration 73450 Training loss 0.012156517244875431 Validation loss 0.02090921811759472 Accuracy 0.7822265625\n",
      "Iteration 73460 Training loss 0.0137509535998106 Validation loss 0.020872123539447784 Accuracy 0.783203125\n",
      "Iteration 73470 Training loss 0.015035503543913364 Validation loss 0.021165961399674416 Accuracy 0.78125\n",
      "Iteration 73480 Training loss 0.012637644074857235 Validation loss 0.021254058927297592 Accuracy 0.7802734375\n",
      "Iteration 73490 Training loss 0.015190457925200462 Validation loss 0.021485986188054085 Accuracy 0.77734375\n",
      "Iteration 73500 Training loss 0.013075497932732105 Validation loss 0.020775096490979195 Accuracy 0.7841796875\n",
      "Iteration 73510 Training loss 0.015273443423211575 Validation loss 0.0210279431194067 Accuracy 0.783203125\n",
      "Iteration 73520 Training loss 0.013196399435400963 Validation loss 0.020635658875107765 Accuracy 0.78662109375\n",
      "Iteration 73530 Training loss 0.013454534113407135 Validation loss 0.0207718163728714 Accuracy 0.78369140625\n",
      "Iteration 73540 Training loss 0.013460428453981876 Validation loss 0.020764712244272232 Accuracy 0.78466796875\n",
      "Iteration 73550 Training loss 0.012791076675057411 Validation loss 0.020850691944360733 Accuracy 0.78271484375\n",
      "Iteration 73560 Training loss 0.01248712558299303 Validation loss 0.02079104445874691 Accuracy 0.78564453125\n",
      "Iteration 73570 Training loss 0.015662623569369316 Validation loss 0.020812463015317917 Accuracy 0.78466796875\n",
      "Iteration 73580 Training loss 0.011884636245667934 Validation loss 0.020779456943273544 Accuracy 0.78515625\n",
      "Iteration 73590 Training loss 0.015567309223115444 Validation loss 0.02072158083319664 Accuracy 0.78466796875\n",
      "Iteration 73600 Training loss 0.01516678836196661 Validation loss 0.02073947712779045 Accuracy 0.78466796875\n",
      "Iteration 73610 Training loss 0.014959834516048431 Validation loss 0.020636068657040596 Accuracy 0.78662109375\n",
      "Iteration 73620 Training loss 0.0159450750797987 Validation loss 0.020839663222432137 Accuracy 0.78369140625\n",
      "Iteration 73630 Training loss 0.014031773433089256 Validation loss 0.02096504531800747 Accuracy 0.78271484375\n",
      "Iteration 73640 Training loss 0.013067134656012058 Validation loss 0.02085220254957676 Accuracy 0.783203125\n",
      "Iteration 73650 Training loss 0.013632532209157944 Validation loss 0.02082173526287079 Accuracy 0.78466796875\n",
      "Iteration 73660 Training loss 0.015295669436454773 Validation loss 0.020874490961432457 Accuracy 0.7841796875\n",
      "Iteration 73670 Training loss 0.014993369579315186 Validation loss 0.020785555243492126 Accuracy 0.78515625\n",
      "Iteration 73680 Training loss 0.014056965708732605 Validation loss 0.020873481407761574 Accuracy 0.7841796875\n",
      "Iteration 73690 Training loss 0.012439376674592495 Validation loss 0.020856289193034172 Accuracy 0.78369140625\n",
      "Iteration 73700 Training loss 0.013932250440120697 Validation loss 0.02094784565269947 Accuracy 0.7841796875\n",
      "Iteration 73710 Training loss 0.010662451386451721 Validation loss 0.02067701146006584 Accuracy 0.78515625\n",
      "Iteration 73720 Training loss 0.013604006730020046 Validation loss 0.020719017833471298 Accuracy 0.78515625\n",
      "Iteration 73730 Training loss 0.013772450387477875 Validation loss 0.020675834268331528 Accuracy 0.78564453125\n",
      "Iteration 73740 Training loss 0.013657105155289173 Validation loss 0.02056550234556198 Accuracy 0.78662109375\n",
      "Iteration 73750 Training loss 0.013842211104929447 Validation loss 0.020737405866384506 Accuracy 0.78515625\n",
      "Iteration 73760 Training loss 0.012813692912459373 Validation loss 0.020710842683911324 Accuracy 0.78515625\n",
      "Iteration 73770 Training loss 0.011936411261558533 Validation loss 0.020844677463173866 Accuracy 0.78369140625\n",
      "Iteration 73780 Training loss 0.015229040756821632 Validation loss 0.020720848813652992 Accuracy 0.78515625\n",
      "Iteration 73790 Training loss 0.014823660254478455 Validation loss 0.021106183528900146 Accuracy 0.7822265625\n",
      "Iteration 73800 Training loss 0.011721990071237087 Validation loss 0.021210692822933197 Accuracy 0.7802734375\n",
      "Iteration 73810 Training loss 0.012264132499694824 Validation loss 0.020842919126152992 Accuracy 0.78369140625\n",
      "Iteration 73820 Training loss 0.013178077526390553 Validation loss 0.02086704969406128 Accuracy 0.783203125\n",
      "Iteration 73830 Training loss 0.013707589358091354 Validation loss 0.02095610834658146 Accuracy 0.78271484375\n",
      "Iteration 73840 Training loss 0.012799089774489403 Validation loss 0.020868007093667984 Accuracy 0.78466796875\n",
      "Iteration 73850 Training loss 0.012515829876065254 Validation loss 0.021033037453889847 Accuracy 0.7822265625\n",
      "Iteration 73860 Training loss 0.01312008686363697 Validation loss 0.021190013736486435 Accuracy 0.78173828125\n",
      "Iteration 73870 Training loss 0.013371952809393406 Validation loss 0.020673422142863274 Accuracy 0.7861328125\n",
      "Iteration 73880 Training loss 0.011224973015487194 Validation loss 0.02084619738161564 Accuracy 0.78369140625\n",
      "Iteration 73890 Training loss 0.011586506851017475 Validation loss 0.021195197477936745 Accuracy 0.78076171875\n",
      "Iteration 73900 Training loss 0.01366081740707159 Validation loss 0.020888280123472214 Accuracy 0.78369140625\n",
      "Iteration 73910 Training loss 0.013162166811525822 Validation loss 0.021003596484661102 Accuracy 0.78271484375\n",
      "Iteration 73920 Training loss 0.012546160258352757 Validation loss 0.020761463791131973 Accuracy 0.78466796875\n",
      "Iteration 73930 Training loss 0.013130842708051205 Validation loss 0.02107963338494301 Accuracy 0.7822265625\n",
      "Iteration 73940 Training loss 0.013127213343977928 Validation loss 0.020940665155649185 Accuracy 0.783203125\n",
      "Iteration 73950 Training loss 0.013183181174099445 Validation loss 0.020747171714901924 Accuracy 0.78466796875\n",
      "Iteration 73960 Training loss 0.013517328538000584 Validation loss 0.02081218734383583 Accuracy 0.783203125\n",
      "Iteration 73970 Training loss 0.012790395878255367 Validation loss 0.020793965086340904 Accuracy 0.7841796875\n",
      "Iteration 73980 Training loss 0.0131136579439044 Validation loss 0.0207600686699152 Accuracy 0.7841796875\n",
      "Iteration 73990 Training loss 0.013075296767055988 Validation loss 0.021214723587036133 Accuracy 0.78173828125\n",
      "Iteration 74000 Training loss 0.01564471609890461 Validation loss 0.02094440348446369 Accuracy 0.78369140625\n",
      "Iteration 74010 Training loss 0.013059706427156925 Validation loss 0.021177835762500763 Accuracy 0.78125\n",
      "Iteration 74020 Training loss 0.016511138528585434 Validation loss 0.021112175658345222 Accuracy 0.78271484375\n",
      "Iteration 74030 Training loss 0.014161773957312107 Validation loss 0.020734596997499466 Accuracy 0.78466796875\n",
      "Iteration 74040 Training loss 0.012214322574436665 Validation loss 0.020781757310032845 Accuracy 0.78466796875\n",
      "Iteration 74050 Training loss 0.011265140026807785 Validation loss 0.020713109523057938 Accuracy 0.78515625\n",
      "Iteration 74060 Training loss 0.011460005305707455 Validation loss 0.020790094509720802 Accuracy 0.78466796875\n",
      "Iteration 74070 Training loss 0.013554837554693222 Validation loss 0.020759526640176773 Accuracy 0.78515625\n",
      "Iteration 74080 Training loss 0.014530524611473083 Validation loss 0.020981431007385254 Accuracy 0.78271484375\n",
      "Iteration 74090 Training loss 0.01496751420199871 Validation loss 0.020950717851519585 Accuracy 0.783203125\n",
      "Iteration 74100 Training loss 0.014184468425810337 Validation loss 0.020972782745957375 Accuracy 0.783203125\n",
      "Iteration 74110 Training loss 0.015518393367528915 Validation loss 0.021403901278972626 Accuracy 0.7783203125\n",
      "Iteration 74120 Training loss 0.012336933985352516 Validation loss 0.021000046283006668 Accuracy 0.783203125\n",
      "Iteration 74130 Training loss 0.014589309692382812 Validation loss 0.020587416365742683 Accuracy 0.78564453125\n",
      "Iteration 74140 Training loss 0.014966283924877644 Validation loss 0.02083183266222477 Accuracy 0.7841796875\n",
      "Iteration 74150 Training loss 0.013499466702342033 Validation loss 0.02073018066585064 Accuracy 0.78466796875\n",
      "Iteration 74160 Training loss 0.013494178652763367 Validation loss 0.021026259288191795 Accuracy 0.7822265625\n",
      "Iteration 74170 Training loss 0.013802128843963146 Validation loss 0.020747140049934387 Accuracy 0.78466796875\n",
      "Iteration 74180 Training loss 0.014194194227457047 Validation loss 0.020930001512169838 Accuracy 0.78271484375\n",
      "Iteration 74190 Training loss 0.014448900707066059 Validation loss 0.020896464586257935 Accuracy 0.78173828125\n",
      "Iteration 74200 Training loss 0.015593335032463074 Validation loss 0.020840350538492203 Accuracy 0.78369140625\n",
      "Iteration 74210 Training loss 0.014102527871727943 Validation loss 0.021181823685765266 Accuracy 0.78173828125\n",
      "Iteration 74220 Training loss 0.014179824851453304 Validation loss 0.02086709998548031 Accuracy 0.78369140625\n",
      "Iteration 74230 Training loss 0.013381063006818295 Validation loss 0.020917396992444992 Accuracy 0.78271484375\n",
      "Iteration 74240 Training loss 0.014112588949501514 Validation loss 0.020884735509753227 Accuracy 0.78466796875\n",
      "Iteration 74250 Training loss 0.013937428593635559 Validation loss 0.020774034783244133 Accuracy 0.7841796875\n",
      "Iteration 74260 Training loss 0.013238155283033848 Validation loss 0.020889058709144592 Accuracy 0.78369140625\n",
      "Iteration 74270 Training loss 0.015003008767962456 Validation loss 0.02071843296289444 Accuracy 0.78564453125\n",
      "Iteration 74280 Training loss 0.013536603190004826 Validation loss 0.021158460527658463 Accuracy 0.78076171875\n",
      "Iteration 74290 Training loss 0.013526362366974354 Validation loss 0.020830584689974785 Accuracy 0.7822265625\n",
      "Iteration 74300 Training loss 0.012709221802651882 Validation loss 0.02096226066350937 Accuracy 0.78271484375\n",
      "Iteration 74310 Training loss 0.014122487045824528 Validation loss 0.0207566786557436 Accuracy 0.78466796875\n",
      "Iteration 74320 Training loss 0.011961515061557293 Validation loss 0.020938651636242867 Accuracy 0.78271484375\n",
      "Iteration 74330 Training loss 0.01230601966381073 Validation loss 0.02083371765911579 Accuracy 0.7822265625\n",
      "Iteration 74340 Training loss 0.014339122921228409 Validation loss 0.02093258686363697 Accuracy 0.78173828125\n",
      "Iteration 74350 Training loss 0.01378678996115923 Validation loss 0.020931484177708626 Accuracy 0.78173828125\n",
      "Iteration 74360 Training loss 0.016295332461595535 Validation loss 0.02089056186378002 Accuracy 0.7841796875\n",
      "Iteration 74370 Training loss 0.015852252021431923 Validation loss 0.021009476855397224 Accuracy 0.78173828125\n",
      "Iteration 74380 Training loss 0.013161974027752876 Validation loss 0.020722804591059685 Accuracy 0.78515625\n",
      "Iteration 74390 Training loss 0.013108337298035622 Validation loss 0.020800882950425148 Accuracy 0.7841796875\n",
      "Iteration 74400 Training loss 0.01230960339307785 Validation loss 0.020703373476862907 Accuracy 0.78515625\n",
      "Iteration 74410 Training loss 0.014368824660778046 Validation loss 0.020918382331728935 Accuracy 0.783203125\n",
      "Iteration 74420 Training loss 0.012513263151049614 Validation loss 0.020838258787989616 Accuracy 0.78369140625\n",
      "Iteration 74430 Training loss 0.013932889327406883 Validation loss 0.021060258150100708 Accuracy 0.783203125\n",
      "Iteration 74440 Training loss 0.012215673923492432 Validation loss 0.02085874415934086 Accuracy 0.7841796875\n",
      "Iteration 74450 Training loss 0.014048057608306408 Validation loss 0.02092028222978115 Accuracy 0.7841796875\n",
      "Iteration 74460 Training loss 0.013612535782158375 Validation loss 0.021235590800642967 Accuracy 0.779296875\n",
      "Iteration 74470 Training loss 0.01261893194168806 Validation loss 0.021050751209259033 Accuracy 0.78076171875\n",
      "Iteration 74480 Training loss 0.013241851702332497 Validation loss 0.020845524966716766 Accuracy 0.783203125\n",
      "Iteration 74490 Training loss 0.013970445841550827 Validation loss 0.020926879718899727 Accuracy 0.78369140625\n",
      "Iteration 74500 Training loss 0.01429331861436367 Validation loss 0.02082894928753376 Accuracy 0.78369140625\n",
      "Iteration 74510 Training loss 0.013172035105526447 Validation loss 0.02080634795129299 Accuracy 0.78369140625\n",
      "Iteration 74520 Training loss 0.013122893869876862 Validation loss 0.02081516571342945 Accuracy 0.78369140625\n",
      "Iteration 74530 Training loss 0.012883801944553852 Validation loss 0.02103522978723049 Accuracy 0.78271484375\n",
      "Iteration 74540 Training loss 0.01230723038315773 Validation loss 0.021056823432445526 Accuracy 0.7802734375\n",
      "Iteration 74550 Training loss 0.012937851250171661 Validation loss 0.020916912704706192 Accuracy 0.78271484375\n",
      "Iteration 74560 Training loss 0.014509355649352074 Validation loss 0.022343559190630913 Accuracy 0.76806640625\n",
      "Iteration 74570 Training loss 0.014070595614612103 Validation loss 0.021031342446804047 Accuracy 0.7822265625\n",
      "Iteration 74580 Training loss 0.013750326819717884 Validation loss 0.021528957411646843 Accuracy 0.77734375\n",
      "Iteration 74590 Training loss 0.014888686127960682 Validation loss 0.021212294697761536 Accuracy 0.779296875\n",
      "Iteration 74600 Training loss 0.014645316638052464 Validation loss 0.02133510261774063 Accuracy 0.779296875\n",
      "Iteration 74610 Training loss 0.012404730543494225 Validation loss 0.02092692255973816 Accuracy 0.78271484375\n",
      "Iteration 74620 Training loss 0.01555351261049509 Validation loss 0.02104397490620613 Accuracy 0.7822265625\n",
      "Iteration 74630 Training loss 0.012890858575701714 Validation loss 0.021105775609612465 Accuracy 0.78125\n",
      "Iteration 74640 Training loss 0.011620470322668552 Validation loss 0.02085546776652336 Accuracy 0.78369140625\n",
      "Iteration 74650 Training loss 0.013192850165069103 Validation loss 0.02093472145497799 Accuracy 0.783203125\n",
      "Iteration 74660 Training loss 0.014716976322233677 Validation loss 0.02082100696861744 Accuracy 0.78369140625\n",
      "Iteration 74670 Training loss 0.012997875921428204 Validation loss 0.020753858610987663 Accuracy 0.78466796875\n",
      "Iteration 74680 Training loss 0.015583803877234459 Validation loss 0.02097601443529129 Accuracy 0.78271484375\n",
      "Iteration 74690 Training loss 0.014140418730676174 Validation loss 0.02111935429275036 Accuracy 0.7802734375\n",
      "Iteration 74700 Training loss 0.012454606592655182 Validation loss 0.02084478549659252 Accuracy 0.78271484375\n",
      "Iteration 74710 Training loss 0.01563076861202717 Validation loss 0.02105054073035717 Accuracy 0.78125\n",
      "Iteration 74720 Training loss 0.014749526977539062 Validation loss 0.022165736183524132 Accuracy 0.771484375\n",
      "Iteration 74730 Training loss 0.014997942373156548 Validation loss 0.021165411919355392 Accuracy 0.78076171875\n",
      "Iteration 74740 Training loss 0.013704762794077396 Validation loss 0.021271785721182823 Accuracy 0.779296875\n",
      "Iteration 74750 Training loss 0.014316736720502377 Validation loss 0.021260913461446762 Accuracy 0.78125\n",
      "Iteration 74760 Training loss 0.012874992564320564 Validation loss 0.02118915505707264 Accuracy 0.77978515625\n",
      "Iteration 74770 Training loss 0.011930998414754868 Validation loss 0.02106935903429985 Accuracy 0.7822265625\n",
      "Iteration 74780 Training loss 0.014064772054553032 Validation loss 0.020982347428798676 Accuracy 0.7822265625\n",
      "Iteration 74790 Training loss 0.015757910907268524 Validation loss 0.021154915913939476 Accuracy 0.78076171875\n",
      "Iteration 74800 Training loss 0.012827720493078232 Validation loss 0.020723866298794746 Accuracy 0.78466796875\n",
      "Iteration 74810 Training loss 0.00997663103044033 Validation loss 0.02083432488143444 Accuracy 0.7841796875\n",
      "Iteration 74820 Training loss 0.014084652997553349 Validation loss 0.02082708477973938 Accuracy 0.7841796875\n",
      "Iteration 74830 Training loss 0.012912099249660969 Validation loss 0.020960839465260506 Accuracy 0.78173828125\n",
      "Iteration 74840 Training loss 0.014214422553777695 Validation loss 0.021211516112089157 Accuracy 0.78125\n",
      "Iteration 74850 Training loss 0.013273441232740879 Validation loss 0.021068602800369263 Accuracy 0.78271484375\n",
      "Iteration 74860 Training loss 0.012836996465921402 Validation loss 0.02095736190676689 Accuracy 0.783203125\n",
      "Iteration 74870 Training loss 0.012328294105827808 Validation loss 0.021053945645689964 Accuracy 0.78173828125\n",
      "Iteration 74880 Training loss 0.013667181134223938 Validation loss 0.0209750197827816 Accuracy 0.7822265625\n",
      "Iteration 74890 Training loss 0.013319206424057484 Validation loss 0.021019887179136276 Accuracy 0.78125\n",
      "Iteration 74900 Training loss 0.01741384156048298 Validation loss 0.02109988033771515 Accuracy 0.78173828125\n",
      "Iteration 74910 Training loss 0.013956723734736443 Validation loss 0.020800219848752022 Accuracy 0.78369140625\n",
      "Iteration 74920 Training loss 0.013869932852685452 Validation loss 0.020845331251621246 Accuracy 0.783203125\n",
      "Iteration 74930 Training loss 0.015573501586914062 Validation loss 0.02099340781569481 Accuracy 0.78271484375\n",
      "Iteration 74940 Training loss 0.012958819977939129 Validation loss 0.020825941115617752 Accuracy 0.78466796875\n",
      "Iteration 74950 Training loss 0.011946719139814377 Validation loss 0.021184056997299194 Accuracy 0.78076171875\n",
      "Iteration 74960 Training loss 0.01262104045599699 Validation loss 0.021142132580280304 Accuracy 0.78076171875\n",
      "Iteration 74970 Training loss 0.0142612149938941 Validation loss 0.020808693021535873 Accuracy 0.78466796875\n",
      "Iteration 74980 Training loss 0.013699684292078018 Validation loss 0.021079406142234802 Accuracy 0.78076171875\n",
      "Iteration 74990 Training loss 0.014758951961994171 Validation loss 0.020987391471862793 Accuracy 0.783203125\n",
      "Iteration 75000 Training loss 0.01476610079407692 Validation loss 0.020852558314800262 Accuracy 0.78271484375\n",
      "Iteration 75010 Training loss 0.013788426294922829 Validation loss 0.021129049360752106 Accuracy 0.78125\n",
      "Iteration 75020 Training loss 0.012843950651586056 Validation loss 0.021367806941270828 Accuracy 0.77880859375\n",
      "Iteration 75030 Training loss 0.013861386105418205 Validation loss 0.02092546597123146 Accuracy 0.78173828125\n",
      "Iteration 75040 Training loss 0.013384245336055756 Validation loss 0.02096000872552395 Accuracy 0.78173828125\n",
      "Iteration 75050 Training loss 0.013425623066723347 Validation loss 0.020786337554454803 Accuracy 0.78369140625\n",
      "Iteration 75060 Training loss 0.015367947518825531 Validation loss 0.021416641771793365 Accuracy 0.77880859375\n",
      "Iteration 75070 Training loss 0.013205286115407944 Validation loss 0.02106291428208351 Accuracy 0.78173828125\n",
      "Iteration 75080 Training loss 0.013735699467360973 Validation loss 0.020785780623555183 Accuracy 0.7841796875\n",
      "Iteration 75090 Training loss 0.011444653384387493 Validation loss 0.020961718633770943 Accuracy 0.78173828125\n",
      "Iteration 75100 Training loss 0.012833920307457447 Validation loss 0.02096659131348133 Accuracy 0.78271484375\n",
      "Iteration 75110 Training loss 0.013391463086009026 Validation loss 0.020853275433182716 Accuracy 0.783203125\n",
      "Iteration 75120 Training loss 0.01610884629189968 Validation loss 0.021463803946971893 Accuracy 0.7783203125\n",
      "Iteration 75130 Training loss 0.012839175760746002 Validation loss 0.021236151456832886 Accuracy 0.7802734375\n",
      "Iteration 75140 Training loss 0.01526694092899561 Validation loss 0.02113758772611618 Accuracy 0.78125\n",
      "Iteration 75150 Training loss 0.015231725759804249 Validation loss 0.020930668339133263 Accuracy 0.7841796875\n",
      "Iteration 75160 Training loss 0.012206761166453362 Validation loss 0.020737551152706146 Accuracy 0.78515625\n",
      "Iteration 75170 Training loss 0.015294389799237251 Validation loss 0.021397342905402184 Accuracy 0.7783203125\n",
      "Iteration 75180 Training loss 0.01369996927678585 Validation loss 0.020743459463119507 Accuracy 0.78466796875\n",
      "Iteration 75190 Training loss 0.013661311008036137 Validation loss 0.020808998495340347 Accuracy 0.78466796875\n",
      "Iteration 75200 Training loss 0.01300211064517498 Validation loss 0.020906178280711174 Accuracy 0.783203125\n",
      "Iteration 75210 Training loss 0.01489657536149025 Validation loss 0.020821232348680496 Accuracy 0.78369140625\n",
      "Iteration 75220 Training loss 0.014770573936402798 Validation loss 0.02074773795902729 Accuracy 0.78369140625\n",
      "Iteration 75230 Training loss 0.010650178417563438 Validation loss 0.020826153457164764 Accuracy 0.783203125\n",
      "Iteration 75240 Training loss 0.013025392778217793 Validation loss 0.020809903740882874 Accuracy 0.783203125\n",
      "Iteration 75250 Training loss 0.014687497168779373 Validation loss 0.021035069599747658 Accuracy 0.78173828125\n",
      "Iteration 75260 Training loss 0.017288081347942352 Validation loss 0.02096163108944893 Accuracy 0.7822265625\n",
      "Iteration 75270 Training loss 0.013176383450627327 Validation loss 0.020912738516926765 Accuracy 0.783203125\n",
      "Iteration 75280 Training loss 0.014295553788542747 Validation loss 0.020848482847213745 Accuracy 0.783203125\n",
      "Iteration 75290 Training loss 0.011525457724928856 Validation loss 0.02090468257665634 Accuracy 0.783203125\n",
      "Iteration 75300 Training loss 0.0158834308385849 Validation loss 0.020824823528528214 Accuracy 0.78271484375\n",
      "Iteration 75310 Training loss 0.013603166677057743 Validation loss 0.020769288763403893 Accuracy 0.78466796875\n",
      "Iteration 75320 Training loss 0.01389382779598236 Validation loss 0.02110336348414421 Accuracy 0.7802734375\n",
      "Iteration 75330 Training loss 0.011293770745396614 Validation loss 0.020810067653656006 Accuracy 0.78271484375\n",
      "Iteration 75340 Training loss 0.01533501036465168 Validation loss 0.02079564891755581 Accuracy 0.7841796875\n",
      "Iteration 75350 Training loss 0.01393555011600256 Validation loss 0.020998874679207802 Accuracy 0.783203125\n",
      "Iteration 75360 Training loss 0.011562354862689972 Validation loss 0.021180247887969017 Accuracy 0.78076171875\n",
      "Iteration 75370 Training loss 0.013129150494933128 Validation loss 0.0210743248462677 Accuracy 0.783203125\n",
      "Iteration 75380 Training loss 0.013215544633567333 Validation loss 0.021012548357248306 Accuracy 0.78271484375\n",
      "Iteration 75390 Training loss 0.015289300121366978 Validation loss 0.020890915766358376 Accuracy 0.78271484375\n",
      "Iteration 75400 Training loss 0.014398209750652313 Validation loss 0.020979251712560654 Accuracy 0.783203125\n",
      "Iteration 75410 Training loss 0.012626655399799347 Validation loss 0.021027136594057083 Accuracy 0.78173828125\n",
      "Iteration 75420 Training loss 0.012264198623597622 Validation loss 0.021208403632044792 Accuracy 0.779296875\n",
      "Iteration 75430 Training loss 0.013290229253470898 Validation loss 0.0210819523781538 Accuracy 0.78076171875\n",
      "Iteration 75440 Training loss 0.01153195183724165 Validation loss 0.020866181701421738 Accuracy 0.783203125\n",
      "Iteration 75450 Training loss 0.013927001506090164 Validation loss 0.020859967917203903 Accuracy 0.78271484375\n",
      "Iteration 75460 Training loss 0.014270253479480743 Validation loss 0.020921064540743828 Accuracy 0.78369140625\n",
      "Iteration 75470 Training loss 0.01443916279822588 Validation loss 0.020847929641604424 Accuracy 0.78369140625\n",
      "Iteration 75480 Training loss 0.014633900485932827 Validation loss 0.021064676344394684 Accuracy 0.783203125\n",
      "Iteration 75490 Training loss 0.014246216043829918 Validation loss 0.020884964615106583 Accuracy 0.7841796875\n",
      "Iteration 75500 Training loss 0.012838716618716717 Validation loss 0.021066443994641304 Accuracy 0.78173828125\n",
      "Iteration 75510 Training loss 0.01431016530841589 Validation loss 0.021435633301734924 Accuracy 0.77685546875\n",
      "Iteration 75520 Training loss 0.014603828079998493 Validation loss 0.020861154422163963 Accuracy 0.78369140625\n",
      "Iteration 75530 Training loss 0.01317287515848875 Validation loss 0.02086406946182251 Accuracy 0.78369140625\n",
      "Iteration 75540 Training loss 0.013430673629045486 Validation loss 0.020984452217817307 Accuracy 0.783203125\n",
      "Iteration 75550 Training loss 0.014738761819899082 Validation loss 0.020896030589938164 Accuracy 0.783203125\n",
      "Iteration 75560 Training loss 0.013087285682559013 Validation loss 0.02110600285232067 Accuracy 0.78125\n",
      "Iteration 75570 Training loss 0.012257909402251244 Validation loss 0.020874734967947006 Accuracy 0.78369140625\n",
      "Iteration 75580 Training loss 0.012394579127430916 Validation loss 0.020753299817442894 Accuracy 0.7841796875\n",
      "Iteration 75590 Training loss 0.01230707112699747 Validation loss 0.020956913009285927 Accuracy 0.78369140625\n",
      "Iteration 75600 Training loss 0.012411316856741905 Validation loss 0.020887382328510284 Accuracy 0.78271484375\n",
      "Iteration 75610 Training loss 0.013477472588419914 Validation loss 0.02074289508163929 Accuracy 0.7841796875\n",
      "Iteration 75620 Training loss 0.01417629700154066 Validation loss 0.020822402089834213 Accuracy 0.783203125\n",
      "Iteration 75630 Training loss 0.013385793194174767 Validation loss 0.020717352628707886 Accuracy 0.7841796875\n",
      "Iteration 75640 Training loss 0.011913619935512543 Validation loss 0.021207816898822784 Accuracy 0.78076171875\n",
      "Iteration 75650 Training loss 0.01422810833901167 Validation loss 0.020896054804325104 Accuracy 0.7841796875\n",
      "Iteration 75660 Training loss 0.017008021473884583 Validation loss 0.020834581926465034 Accuracy 0.78466796875\n",
      "Iteration 75670 Training loss 0.012978523038327694 Validation loss 0.020862499251961708 Accuracy 0.7841796875\n",
      "Iteration 75680 Training loss 0.012596244923770428 Validation loss 0.02071714773774147 Accuracy 0.78369140625\n",
      "Iteration 75690 Training loss 0.015769658610224724 Validation loss 0.021007245406508446 Accuracy 0.7822265625\n",
      "Iteration 75700 Training loss 0.012665224261581898 Validation loss 0.02074490115046501 Accuracy 0.78466796875\n",
      "Iteration 75710 Training loss 0.01345799770206213 Validation loss 0.020779086276888847 Accuracy 0.78515625\n",
      "Iteration 75720 Training loss 0.012412801384925842 Validation loss 0.021006513386964798 Accuracy 0.78173828125\n",
      "Iteration 75730 Training loss 0.01571972668170929 Validation loss 0.020823009312152863 Accuracy 0.78369140625\n",
      "Iteration 75740 Training loss 0.012312761507928371 Validation loss 0.02066970244050026 Accuracy 0.78515625\n",
      "Iteration 75750 Training loss 0.015343357808887959 Validation loss 0.020659858360886574 Accuracy 0.7861328125\n",
      "Iteration 75760 Training loss 0.016499366611242294 Validation loss 0.02095785364508629 Accuracy 0.783203125\n",
      "Iteration 75770 Training loss 0.014285391196608543 Validation loss 0.020848348736763 Accuracy 0.78369140625\n",
      "Iteration 75780 Training loss 0.015783239156007767 Validation loss 0.020810602232813835 Accuracy 0.78369140625\n",
      "Iteration 75790 Training loss 0.011234547942876816 Validation loss 0.020978953689336777 Accuracy 0.78271484375\n",
      "Iteration 75800 Training loss 0.011722764000296593 Validation loss 0.02094353549182415 Accuracy 0.78369140625\n",
      "Iteration 75810 Training loss 0.013311280868947506 Validation loss 0.020980888977646828 Accuracy 0.78173828125\n",
      "Iteration 75820 Training loss 0.01147440169006586 Validation loss 0.02097325585782528 Accuracy 0.78271484375\n",
      "Iteration 75830 Training loss 0.01650092378258705 Validation loss 0.02097693271934986 Accuracy 0.78173828125\n",
      "Iteration 75840 Training loss 0.012441687285900116 Validation loss 0.02077801153063774 Accuracy 0.78466796875\n",
      "Iteration 75850 Training loss 0.014259540475904942 Validation loss 0.02096579596400261 Accuracy 0.78173828125\n",
      "Iteration 75860 Training loss 0.011275437660515308 Validation loss 0.020961161702871323 Accuracy 0.7822265625\n",
      "Iteration 75870 Training loss 0.01463439129292965 Validation loss 0.020898232236504555 Accuracy 0.78271484375\n",
      "Iteration 75880 Training loss 0.01246421318501234 Validation loss 0.021165162324905396 Accuracy 0.78173828125\n",
      "Iteration 75890 Training loss 0.01703709363937378 Validation loss 0.020811451599001884 Accuracy 0.7841796875\n",
      "Iteration 75900 Training loss 0.013288894668221474 Validation loss 0.02081107906997204 Accuracy 0.7841796875\n",
      "Iteration 75910 Training loss 0.01150741707533598 Validation loss 0.02087705209851265 Accuracy 0.783203125\n",
      "Iteration 75920 Training loss 0.011752822436392307 Validation loss 0.020758427679538727 Accuracy 0.78515625\n",
      "Iteration 75930 Training loss 0.013228815980255604 Validation loss 0.021067598834633827 Accuracy 0.78076171875\n",
      "Iteration 75940 Training loss 0.014398801140487194 Validation loss 0.02116306871175766 Accuracy 0.78076171875\n",
      "Iteration 75950 Training loss 0.013420606963336468 Validation loss 0.020912330597639084 Accuracy 0.783203125\n",
      "Iteration 75960 Training loss 0.012331807054579258 Validation loss 0.020958255976438522 Accuracy 0.78271484375\n",
      "Iteration 75970 Training loss 0.015059791505336761 Validation loss 0.021096620708703995 Accuracy 0.78125\n",
      "Iteration 75980 Training loss 0.014657465741038322 Validation loss 0.02102385275065899 Accuracy 0.78271484375\n",
      "Iteration 75990 Training loss 0.012976346537470818 Validation loss 0.02093631774187088 Accuracy 0.78271484375\n",
      "Iteration 76000 Training loss 0.01405587512999773 Validation loss 0.021049801260232925 Accuracy 0.78173828125\n",
      "Iteration 76010 Training loss 0.014086813665926456 Validation loss 0.020852450281381607 Accuracy 0.7822265625\n",
      "Iteration 76020 Training loss 0.013087663799524307 Validation loss 0.020781001076102257 Accuracy 0.78369140625\n",
      "Iteration 76030 Training loss 0.011413000524044037 Validation loss 0.020988499745726585 Accuracy 0.783203125\n",
      "Iteration 76040 Training loss 0.013078811578452587 Validation loss 0.020936699584126472 Accuracy 0.783203125\n",
      "Iteration 76050 Training loss 0.013851006515324116 Validation loss 0.020960181951522827 Accuracy 0.78271484375\n",
      "Iteration 76060 Training loss 0.015395527705550194 Validation loss 0.02089923992753029 Accuracy 0.783203125\n",
      "Iteration 76070 Training loss 0.013864231295883656 Validation loss 0.02104012854397297 Accuracy 0.78125\n",
      "Iteration 76080 Training loss 0.015614970587193966 Validation loss 0.021208306774497032 Accuracy 0.78076171875\n",
      "Iteration 76090 Training loss 0.014197392389178276 Validation loss 0.020813364535570145 Accuracy 0.783203125\n",
      "Iteration 76100 Training loss 0.013223887421190739 Validation loss 0.02088010124862194 Accuracy 0.78369140625\n",
      "Iteration 76110 Training loss 0.01385705266147852 Validation loss 0.020831367000937462 Accuracy 0.7841796875\n",
      "Iteration 76120 Training loss 0.012551635503768921 Validation loss 0.021002335473895073 Accuracy 0.78271484375\n",
      "Iteration 76130 Training loss 0.011862380430102348 Validation loss 0.02075866237282753 Accuracy 0.78515625\n",
      "Iteration 76140 Training loss 0.01459223497658968 Validation loss 0.020954575389623642 Accuracy 0.7841796875\n",
      "Iteration 76150 Training loss 0.014910626225173473 Validation loss 0.020840590819716454 Accuracy 0.78515625\n",
      "Iteration 76160 Training loss 0.01354699395596981 Validation loss 0.021038778126239777 Accuracy 0.78173828125\n",
      "Iteration 76170 Training loss 0.011943744495511055 Validation loss 0.020945580676198006 Accuracy 0.78369140625\n",
      "Iteration 76180 Training loss 0.013818375766277313 Validation loss 0.02084377221763134 Accuracy 0.7841796875\n",
      "Iteration 76190 Training loss 0.01671535149216652 Validation loss 0.02079707942903042 Accuracy 0.78515625\n",
      "Iteration 76200 Training loss 0.01384744979441166 Validation loss 0.02155822142958641 Accuracy 0.77783203125\n",
      "Iteration 76210 Training loss 0.014317011460661888 Validation loss 0.020771268755197525 Accuracy 0.78369140625\n",
      "Iteration 76220 Training loss 0.012377499602735043 Validation loss 0.021084176376461983 Accuracy 0.7822265625\n",
      "Iteration 76230 Training loss 0.014038121327757835 Validation loss 0.02095876820385456 Accuracy 0.783203125\n",
      "Iteration 76240 Training loss 0.01275496557354927 Validation loss 0.020890189334750175 Accuracy 0.783203125\n",
      "Iteration 76250 Training loss 0.015339594334363937 Validation loss 0.020855868235230446 Accuracy 0.7822265625\n",
      "Iteration 76260 Training loss 0.01408934872597456 Validation loss 0.020826254040002823 Accuracy 0.78369140625\n",
      "Iteration 76270 Training loss 0.016591766849160194 Validation loss 0.02094539999961853 Accuracy 0.7822265625\n",
      "Iteration 76280 Training loss 0.01310757640749216 Validation loss 0.020864004269242287 Accuracy 0.78369140625\n",
      "Iteration 76290 Training loss 0.013450326398015022 Validation loss 0.02085874415934086 Accuracy 0.78271484375\n",
      "Iteration 76300 Training loss 0.012634117156267166 Validation loss 0.021124208346009254 Accuracy 0.78125\n",
      "Iteration 76310 Training loss 0.010711658746004105 Validation loss 0.02112627401947975 Accuracy 0.78125\n",
      "Iteration 76320 Training loss 0.014596633613109589 Validation loss 0.020747922360897064 Accuracy 0.78369140625\n",
      "Iteration 76330 Training loss 0.015183915384113789 Validation loss 0.020816264674067497 Accuracy 0.7841796875\n",
      "Iteration 76340 Training loss 0.012776455841958523 Validation loss 0.020823808386921883 Accuracy 0.78515625\n",
      "Iteration 76350 Training loss 0.012426743283867836 Validation loss 0.0209769569337368 Accuracy 0.78173828125\n",
      "Iteration 76360 Training loss 0.01299257017672062 Validation loss 0.020798105746507645 Accuracy 0.7841796875\n",
      "Iteration 76370 Training loss 0.01417271513491869 Validation loss 0.020917924121022224 Accuracy 0.78369140625\n",
      "Iteration 76380 Training loss 0.012267167679965496 Validation loss 0.020829051733016968 Accuracy 0.78369140625\n",
      "Iteration 76390 Training loss 0.016027528792619705 Validation loss 0.020967721939086914 Accuracy 0.78271484375\n",
      "Iteration 76400 Training loss 0.015498227439820766 Validation loss 0.02113465964794159 Accuracy 0.78173828125\n",
      "Iteration 76410 Training loss 0.013156411238014698 Validation loss 0.021034564822912216 Accuracy 0.78271484375\n",
      "Iteration 76420 Training loss 0.012608629651367664 Validation loss 0.02092696726322174 Accuracy 0.783203125\n",
      "Iteration 76430 Training loss 0.012865351513028145 Validation loss 0.02089953050017357 Accuracy 0.7822265625\n",
      "Iteration 76440 Training loss 0.013740264810621738 Validation loss 0.020963434129953384 Accuracy 0.78173828125\n",
      "Iteration 76450 Training loss 0.014297365210950375 Validation loss 0.02092580497264862 Accuracy 0.78173828125\n",
      "Iteration 76460 Training loss 0.011600647121667862 Validation loss 0.02071123942732811 Accuracy 0.7841796875\n",
      "Iteration 76470 Training loss 0.013158508576452732 Validation loss 0.020881107077002525 Accuracy 0.78369140625\n",
      "Iteration 76480 Training loss 0.01233767531812191 Validation loss 0.020836779847741127 Accuracy 0.783203125\n",
      "Iteration 76490 Training loss 0.01361119281500578 Validation loss 0.020771430805325508 Accuracy 0.78466796875\n",
      "Iteration 76500 Training loss 0.014380866661667824 Validation loss 0.02077414095401764 Accuracy 0.78369140625\n",
      "Iteration 76510 Training loss 0.014082725159823895 Validation loss 0.020858032628893852 Accuracy 0.78369140625\n",
      "Iteration 76520 Training loss 0.012523004785180092 Validation loss 0.020886098966002464 Accuracy 0.783203125\n",
      "Iteration 76530 Training loss 0.01395172718912363 Validation loss 0.02093694359064102 Accuracy 0.78369140625\n",
      "Iteration 76540 Training loss 0.014486287720501423 Validation loss 0.02101016975939274 Accuracy 0.7822265625\n",
      "Iteration 76550 Training loss 0.009861934930086136 Validation loss 0.020684901624917984 Accuracy 0.78515625\n",
      "Iteration 76560 Training loss 0.011922108940780163 Validation loss 0.02077627182006836 Accuracy 0.78369140625\n",
      "Iteration 76570 Training loss 0.014383961446583271 Validation loss 0.020950263366103172 Accuracy 0.78271484375\n",
      "Iteration 76580 Training loss 0.014294597320258617 Validation loss 0.02089221589267254 Accuracy 0.78369140625\n",
      "Iteration 76590 Training loss 0.012154722586274147 Validation loss 0.020722802728414536 Accuracy 0.78466796875\n",
      "Iteration 76600 Training loss 0.0160902701318264 Validation loss 0.021817518398165703 Accuracy 0.7734375\n",
      "Iteration 76610 Training loss 0.013357448391616344 Validation loss 0.020879477262496948 Accuracy 0.783203125\n",
      "Iteration 76620 Training loss 0.014580258168280125 Validation loss 0.020862143486738205 Accuracy 0.78271484375\n",
      "Iteration 76630 Training loss 0.014665680006146431 Validation loss 0.02075330540537834 Accuracy 0.78369140625\n",
      "Iteration 76640 Training loss 0.0123789357021451 Validation loss 0.02090863697230816 Accuracy 0.7841796875\n",
      "Iteration 76650 Training loss 0.012750583700835705 Validation loss 0.020850688219070435 Accuracy 0.78369140625\n",
      "Iteration 76660 Training loss 0.014565304853022099 Validation loss 0.02093995362520218 Accuracy 0.7822265625\n",
      "Iteration 76670 Training loss 0.014351953752338886 Validation loss 0.021030880510807037 Accuracy 0.7822265625\n",
      "Iteration 76680 Training loss 0.012932314537465572 Validation loss 0.020811527967453003 Accuracy 0.78369140625\n",
      "Iteration 76690 Training loss 0.015232670120894909 Validation loss 0.020889589563012123 Accuracy 0.783203125\n",
      "Iteration 76700 Training loss 0.013405817560851574 Validation loss 0.021209513768553734 Accuracy 0.77978515625\n",
      "Iteration 76710 Training loss 0.011847611516714096 Validation loss 0.020979922264814377 Accuracy 0.7822265625\n",
      "Iteration 76720 Training loss 0.012537695467472076 Validation loss 0.02088252641260624 Accuracy 0.7822265625\n",
      "Iteration 76730 Training loss 0.013515817932784557 Validation loss 0.020878436043858528 Accuracy 0.78173828125\n",
      "Iteration 76740 Training loss 0.011630191467702389 Validation loss 0.020983632653951645 Accuracy 0.783203125\n",
      "Iteration 76750 Training loss 0.014899021945893764 Validation loss 0.020761538296937943 Accuracy 0.7841796875\n",
      "Iteration 76760 Training loss 0.012627225369215012 Validation loss 0.020817596465349197 Accuracy 0.78466796875\n",
      "Iteration 76770 Training loss 0.016092374920845032 Validation loss 0.021022440865635872 Accuracy 0.78173828125\n",
      "Iteration 76780 Training loss 0.011937814764678478 Validation loss 0.02083398960530758 Accuracy 0.78369140625\n",
      "Iteration 76790 Training loss 0.012142420746386051 Validation loss 0.020700067281723022 Accuracy 0.78515625\n",
      "Iteration 76800 Training loss 0.012052278965711594 Validation loss 0.02135552652180195 Accuracy 0.77783203125\n",
      "Iteration 76810 Training loss 0.011355254799127579 Validation loss 0.020976973697543144 Accuracy 0.7822265625\n",
      "Iteration 76820 Training loss 0.014063992537558079 Validation loss 0.021153882145881653 Accuracy 0.78173828125\n",
      "Iteration 76830 Training loss 0.012803826481103897 Validation loss 0.021064935252070427 Accuracy 0.7822265625\n",
      "Iteration 76840 Training loss 0.01256273128092289 Validation loss 0.020792817696928978 Accuracy 0.7841796875\n",
      "Iteration 76850 Training loss 0.013233927078545094 Validation loss 0.021173959597945213 Accuracy 0.7822265625\n",
      "Iteration 76860 Training loss 0.012906283140182495 Validation loss 0.020927609875798225 Accuracy 0.78271484375\n",
      "Iteration 76870 Training loss 0.016275016590952873 Validation loss 0.020958106964826584 Accuracy 0.78271484375\n",
      "Iteration 76880 Training loss 0.011425257660448551 Validation loss 0.020858123898506165 Accuracy 0.783203125\n",
      "Iteration 76890 Training loss 0.011577745899558067 Validation loss 0.020855868235230446 Accuracy 0.7841796875\n",
      "Iteration 76900 Training loss 0.015303489752113819 Validation loss 0.02109703980386257 Accuracy 0.78173828125\n",
      "Iteration 76910 Training loss 0.013027595356106758 Validation loss 0.02081698179244995 Accuracy 0.7841796875\n",
      "Iteration 76920 Training loss 0.010799773968756199 Validation loss 0.020962128415703773 Accuracy 0.7822265625\n",
      "Iteration 76930 Training loss 0.012966150417923927 Validation loss 0.02084062620997429 Accuracy 0.7841796875\n",
      "Iteration 76940 Training loss 0.01339737419039011 Validation loss 0.020965229719877243 Accuracy 0.78271484375\n",
      "Iteration 76950 Training loss 0.014815101400017738 Validation loss 0.021043686196208 Accuracy 0.78173828125\n",
      "Iteration 76960 Training loss 0.012309099547564983 Validation loss 0.02073950693011284 Accuracy 0.783203125\n",
      "Iteration 76970 Training loss 0.011996109038591385 Validation loss 0.020804008468985558 Accuracy 0.783203125\n",
      "Iteration 76980 Training loss 0.01689312420785427 Validation loss 0.02164137177169323 Accuracy 0.7763671875\n",
      "Iteration 76990 Training loss 0.015739193186163902 Validation loss 0.020843159407377243 Accuracy 0.783203125\n",
      "Iteration 77000 Training loss 0.015121588483452797 Validation loss 0.020849965512752533 Accuracy 0.7841796875\n",
      "Iteration 77010 Training loss 0.013270197436213493 Validation loss 0.020738553255796432 Accuracy 0.78466796875\n",
      "Iteration 77020 Training loss 0.01368076540529728 Validation loss 0.0208030603826046 Accuracy 0.78369140625\n",
      "Iteration 77030 Training loss 0.013165377080440521 Validation loss 0.020776351913809776 Accuracy 0.7841796875\n",
      "Iteration 77040 Training loss 0.012393665499985218 Validation loss 0.02070458233356476 Accuracy 0.78466796875\n",
      "Iteration 77050 Training loss 0.01253529079258442 Validation loss 0.02074771374464035 Accuracy 0.7861328125\n",
      "Iteration 77060 Training loss 0.0137358158826828 Validation loss 0.02070694975554943 Accuracy 0.78466796875\n",
      "Iteration 77070 Training loss 0.01615203730762005 Validation loss 0.020725438371300697 Accuracy 0.7841796875\n",
      "Iteration 77080 Training loss 0.014325696043670177 Validation loss 0.020806502550840378 Accuracy 0.783203125\n",
      "Iteration 77090 Training loss 0.0144658787176013 Validation loss 0.02071141079068184 Accuracy 0.78466796875\n",
      "Iteration 77100 Training loss 0.012600578367710114 Validation loss 0.020803114399313927 Accuracy 0.78466796875\n",
      "Iteration 77110 Training loss 0.011672205291688442 Validation loss 0.020816689357161522 Accuracy 0.7841796875\n",
      "Iteration 77120 Training loss 0.010903239250183105 Validation loss 0.020799217745661736 Accuracy 0.783203125\n",
      "Iteration 77130 Training loss 0.013960333541035652 Validation loss 0.020947225391864777 Accuracy 0.78369140625\n",
      "Iteration 77140 Training loss 0.014207498170435429 Validation loss 0.02077346295118332 Accuracy 0.78515625\n",
      "Iteration 77150 Training loss 0.016244638711214066 Validation loss 0.021248336881399155 Accuracy 0.77978515625\n",
      "Iteration 77160 Training loss 0.011875386349856853 Validation loss 0.020993633195757866 Accuracy 0.7822265625\n",
      "Iteration 77170 Training loss 0.01494233962148428 Validation loss 0.020846931263804436 Accuracy 0.7841796875\n",
      "Iteration 77180 Training loss 0.014356425032019615 Validation loss 0.02095787040889263 Accuracy 0.78369140625\n",
      "Iteration 77190 Training loss 0.014324241317808628 Validation loss 0.020944181829690933 Accuracy 0.7841796875\n",
      "Iteration 77200 Training loss 0.010786077007651329 Validation loss 0.020757272839546204 Accuracy 0.78515625\n",
      "Iteration 77210 Training loss 0.013005701825022697 Validation loss 0.021043436601758003 Accuracy 0.78173828125\n",
      "Iteration 77220 Training loss 0.013742752373218536 Validation loss 0.020850757136940956 Accuracy 0.7841796875\n",
      "Iteration 77230 Training loss 0.01377175748348236 Validation loss 0.020846186205744743 Accuracy 0.7841796875\n",
      "Iteration 77240 Training loss 0.013642205856740475 Validation loss 0.02082943730056286 Accuracy 0.7841796875\n",
      "Iteration 77250 Training loss 0.010648288764059544 Validation loss 0.02082746848464012 Accuracy 0.7841796875\n",
      "Iteration 77260 Training loss 0.009909736923873425 Validation loss 0.020896008238196373 Accuracy 0.783203125\n",
      "Iteration 77270 Training loss 0.013148088939487934 Validation loss 0.0208870992064476 Accuracy 0.78369140625\n",
      "Iteration 77280 Training loss 0.013071983121335506 Validation loss 0.020842665806412697 Accuracy 0.783203125\n",
      "Iteration 77290 Training loss 0.013993946835398674 Validation loss 0.0207686610519886 Accuracy 0.78369140625\n",
      "Iteration 77300 Training loss 0.013824262656271458 Validation loss 0.02078728936612606 Accuracy 0.783203125\n",
      "Iteration 77310 Training loss 0.014481399208307266 Validation loss 0.02079220488667488 Accuracy 0.78369140625\n",
      "Iteration 77320 Training loss 0.013481554575264454 Validation loss 0.021127933636307716 Accuracy 0.78173828125\n",
      "Iteration 77330 Training loss 0.013083704747259617 Validation loss 0.020976591855287552 Accuracy 0.7822265625\n",
      "Iteration 77340 Training loss 0.01319047249853611 Validation loss 0.020990679040551186 Accuracy 0.7822265625\n",
      "Iteration 77350 Training loss 0.013996860012412071 Validation loss 0.02092706225812435 Accuracy 0.783203125\n",
      "Iteration 77360 Training loss 0.014132222160696983 Validation loss 0.02083542011678219 Accuracy 0.78369140625\n",
      "Iteration 77370 Training loss 0.013221613131463528 Validation loss 0.02098052203655243 Accuracy 0.78369140625\n",
      "Iteration 77380 Training loss 0.017026778310537338 Validation loss 0.020835869014263153 Accuracy 0.7841796875\n",
      "Iteration 77390 Training loss 0.011600598692893982 Validation loss 0.020892931148409843 Accuracy 0.78369140625\n",
      "Iteration 77400 Training loss 0.015976866707205772 Validation loss 0.021274147555232048 Accuracy 0.78173828125\n",
      "Iteration 77410 Training loss 0.015429219231009483 Validation loss 0.020849261432886124 Accuracy 0.78271484375\n",
      "Iteration 77420 Training loss 0.014890911057591438 Validation loss 0.02089182287454605 Accuracy 0.78369140625\n",
      "Iteration 77430 Training loss 0.013712461106479168 Validation loss 0.02089007757604122 Accuracy 0.78271484375\n",
      "Iteration 77440 Training loss 0.013554557226598263 Validation loss 0.02092600055038929 Accuracy 0.78271484375\n",
      "Iteration 77450 Training loss 0.013157344423234463 Validation loss 0.020890189334750175 Accuracy 0.78466796875\n",
      "Iteration 77460 Training loss 0.0111473947763443 Validation loss 0.02101166918873787 Accuracy 0.7822265625\n",
      "Iteration 77470 Training loss 0.016262581571936607 Validation loss 0.020904960110783577 Accuracy 0.783203125\n",
      "Iteration 77480 Training loss 0.013357238844037056 Validation loss 0.02079921029508114 Accuracy 0.78515625\n",
      "Iteration 77490 Training loss 0.011631179600954056 Validation loss 0.020929774269461632 Accuracy 0.783203125\n",
      "Iteration 77500 Training loss 0.01305734645575285 Validation loss 0.02088254876434803 Accuracy 0.7841796875\n",
      "Iteration 77510 Training loss 0.014179008081555367 Validation loss 0.021137680858373642 Accuracy 0.78125\n",
      "Iteration 77520 Training loss 0.011804014444351196 Validation loss 0.020987143740057945 Accuracy 0.783203125\n",
      "Iteration 77530 Training loss 0.014030680060386658 Validation loss 0.0208599753677845 Accuracy 0.7841796875\n",
      "Iteration 77540 Training loss 0.013367068022489548 Validation loss 0.02102150395512581 Accuracy 0.78271484375\n",
      "Iteration 77550 Training loss 0.013507921248674393 Validation loss 0.02086903527379036 Accuracy 0.783203125\n",
      "Iteration 77560 Training loss 0.01566196046769619 Validation loss 0.020896805450320244 Accuracy 0.7841796875\n",
      "Iteration 77570 Training loss 0.014971952885389328 Validation loss 0.020916180685162544 Accuracy 0.7841796875\n",
      "Iteration 77580 Training loss 0.013598088175058365 Validation loss 0.02095024101436138 Accuracy 0.78271484375\n",
      "Iteration 77590 Training loss 0.013436419889330864 Validation loss 0.020907239988446236 Accuracy 0.78466796875\n",
      "Iteration 77600 Training loss 0.01390792615711689 Validation loss 0.021007562056183815 Accuracy 0.7841796875\n",
      "Iteration 77610 Training loss 0.011324654333293438 Validation loss 0.021123725920915604 Accuracy 0.78173828125\n",
      "Iteration 77620 Training loss 0.014255345799028873 Validation loss 0.02095584198832512 Accuracy 0.78369140625\n",
      "Iteration 77630 Training loss 0.012404668144881725 Validation loss 0.02106395922601223 Accuracy 0.7822265625\n",
      "Iteration 77640 Training loss 0.014187458902597427 Validation loss 0.020880453288555145 Accuracy 0.78369140625\n",
      "Iteration 77650 Training loss 0.011069418862462044 Validation loss 0.021168677136301994 Accuracy 0.7802734375\n",
      "Iteration 77660 Training loss 0.014359471388161182 Validation loss 0.02095501683652401 Accuracy 0.783203125\n",
      "Iteration 77670 Training loss 0.012189604341983795 Validation loss 0.02079589106142521 Accuracy 0.7841796875\n",
      "Iteration 77680 Training loss 0.014062019065022469 Validation loss 0.020827163010835648 Accuracy 0.78369140625\n",
      "Iteration 77690 Training loss 0.013860460370779037 Validation loss 0.020925110206007957 Accuracy 0.78271484375\n",
      "Iteration 77700 Training loss 0.011065684258937836 Validation loss 0.020723722875118256 Accuracy 0.78466796875\n",
      "Iteration 77710 Training loss 0.013238235376775265 Validation loss 0.020726919174194336 Accuracy 0.78466796875\n",
      "Iteration 77720 Training loss 0.014063539914786816 Validation loss 0.0208713598549366 Accuracy 0.7841796875\n",
      "Iteration 77730 Training loss 0.015166266821324825 Validation loss 0.020792260766029358 Accuracy 0.78369140625\n",
      "Iteration 77740 Training loss 0.014494627714157104 Validation loss 0.02113313414156437 Accuracy 0.7822265625\n",
      "Iteration 77750 Training loss 0.014667695388197899 Validation loss 0.02102091535925865 Accuracy 0.7822265625\n",
      "Iteration 77760 Training loss 0.013471976853907108 Validation loss 0.020793912932276726 Accuracy 0.78466796875\n",
      "Iteration 77770 Training loss 0.012316153384745121 Validation loss 0.020939210429787636 Accuracy 0.783203125\n",
      "Iteration 77780 Training loss 0.013466588221490383 Validation loss 0.020874828100204468 Accuracy 0.7841796875\n",
      "Iteration 77790 Training loss 0.01445100735872984 Validation loss 0.021182138472795486 Accuracy 0.78173828125\n",
      "Iteration 77800 Training loss 0.014054682105779648 Validation loss 0.02115449495613575 Accuracy 0.7822265625\n",
      "Iteration 77810 Training loss 0.013683285564184189 Validation loss 0.020718513056635857 Accuracy 0.78466796875\n",
      "Iteration 77820 Training loss 0.015849800780415535 Validation loss 0.021139563992619514 Accuracy 0.78173828125\n",
      "Iteration 77830 Training loss 0.01333294901996851 Validation loss 0.020709306001663208 Accuracy 0.78466796875\n",
      "Iteration 77840 Training loss 0.01712455227971077 Validation loss 0.020844649523496628 Accuracy 0.78369140625\n",
      "Iteration 77850 Training loss 0.013394729234278202 Validation loss 0.02097463794052601 Accuracy 0.7822265625\n",
      "Iteration 77860 Training loss 0.012052522972226143 Validation loss 0.02084455080330372 Accuracy 0.78515625\n",
      "Iteration 77870 Training loss 0.012805997394025326 Validation loss 0.02076791413128376 Accuracy 0.78466796875\n",
      "Iteration 77880 Training loss 0.013639764860272408 Validation loss 0.020858028903603554 Accuracy 0.7841796875\n",
      "Iteration 77890 Training loss 0.014392981305718422 Validation loss 0.02085055597126484 Accuracy 0.7841796875\n",
      "Iteration 77900 Training loss 0.015722811222076416 Validation loss 0.02100757509469986 Accuracy 0.7822265625\n",
      "Iteration 77910 Training loss 0.014863144606351852 Validation loss 0.02109690196812153 Accuracy 0.7802734375\n",
      "Iteration 77920 Training loss 0.014589869417250156 Validation loss 0.020762596279382706 Accuracy 0.78515625\n",
      "Iteration 77930 Training loss 0.012523772194981575 Validation loss 0.020899496972560883 Accuracy 0.7841796875\n",
      "Iteration 77940 Training loss 0.01321770716458559 Validation loss 0.020880399271845818 Accuracy 0.7841796875\n",
      "Iteration 77950 Training loss 0.013740867376327515 Validation loss 0.02084164507687092 Accuracy 0.783203125\n",
      "Iteration 77960 Training loss 0.012145087122917175 Validation loss 0.021092137321829796 Accuracy 0.78173828125\n",
      "Iteration 77970 Training loss 0.013612492941319942 Validation loss 0.02078119106590748 Accuracy 0.78515625\n",
      "Iteration 77980 Training loss 0.014559394679963589 Validation loss 0.020811710506677628 Accuracy 0.7841796875\n",
      "Iteration 77990 Training loss 0.011703643947839737 Validation loss 0.02075808309018612 Accuracy 0.78369140625\n",
      "Iteration 78000 Training loss 0.012585352174937725 Validation loss 0.020824138075113297 Accuracy 0.7841796875\n",
      "Iteration 78010 Training loss 0.015110407024621964 Validation loss 0.020897990092635155 Accuracy 0.78173828125\n",
      "Iteration 78020 Training loss 0.014570380561053753 Validation loss 0.020921386778354645 Accuracy 0.78271484375\n",
      "Iteration 78030 Training loss 0.012808355502784252 Validation loss 0.02083360217511654 Accuracy 0.78271484375\n",
      "Iteration 78040 Training loss 0.016600772738456726 Validation loss 0.02080165408551693 Accuracy 0.78369140625\n",
      "Iteration 78050 Training loss 0.013734519481658936 Validation loss 0.021187303587794304 Accuracy 0.78076171875\n",
      "Iteration 78060 Training loss 0.012292833998799324 Validation loss 0.020812762901186943 Accuracy 0.7841796875\n",
      "Iteration 78070 Training loss 0.012244363315403461 Validation loss 0.020878972485661507 Accuracy 0.78271484375\n",
      "Iteration 78080 Training loss 0.015352949500083923 Validation loss 0.02085730992257595 Accuracy 0.78369140625\n",
      "Iteration 78090 Training loss 0.014983695931732655 Validation loss 0.020981991663575172 Accuracy 0.783203125\n",
      "Iteration 78100 Training loss 0.014098486863076687 Validation loss 0.02077695168554783 Accuracy 0.78466796875\n",
      "Iteration 78110 Training loss 0.012464391067624092 Validation loss 0.020746981725096703 Accuracy 0.78369140625\n",
      "Iteration 78120 Training loss 0.012610262259840965 Validation loss 0.020818568766117096 Accuracy 0.783203125\n",
      "Iteration 78130 Training loss 0.013834331184625626 Validation loss 0.02087549678981304 Accuracy 0.7822265625\n",
      "Iteration 78140 Training loss 0.014378760941326618 Validation loss 0.020779047161340714 Accuracy 0.7841796875\n",
      "Iteration 78150 Training loss 0.011617609299719334 Validation loss 0.020845403894782066 Accuracy 0.78369140625\n",
      "Iteration 78160 Training loss 0.015032323077321053 Validation loss 0.020988617092370987 Accuracy 0.7822265625\n",
      "Iteration 78170 Training loss 0.015146340243518353 Validation loss 0.021009869873523712 Accuracy 0.7822265625\n",
      "Iteration 78180 Training loss 0.012580836191773415 Validation loss 0.021053286269307137 Accuracy 0.78076171875\n",
      "Iteration 78190 Training loss 0.016161661595106125 Validation loss 0.02096915990114212 Accuracy 0.7822265625\n",
      "Iteration 78200 Training loss 0.014593275263905525 Validation loss 0.020941780880093575 Accuracy 0.78271484375\n",
      "Iteration 78210 Training loss 0.01242284756153822 Validation loss 0.020943421870470047 Accuracy 0.78271484375\n",
      "Iteration 78220 Training loss 0.014414133504033089 Validation loss 0.02107042446732521 Accuracy 0.78173828125\n",
      "Iteration 78230 Training loss 0.01269284076988697 Validation loss 0.020959975197911263 Accuracy 0.78369140625\n",
      "Iteration 78240 Training loss 0.012355181388556957 Validation loss 0.02099951170384884 Accuracy 0.783203125\n",
      "Iteration 78250 Training loss 0.012264137156307697 Validation loss 0.020974276587367058 Accuracy 0.7822265625\n",
      "Iteration 78260 Training loss 0.014081418514251709 Validation loss 0.020887257531285286 Accuracy 0.783203125\n",
      "Iteration 78270 Training loss 0.011928827501833439 Validation loss 0.020893346518278122 Accuracy 0.7822265625\n",
      "Iteration 78280 Training loss 0.011745798401534557 Validation loss 0.02109311707317829 Accuracy 0.7802734375\n",
      "Iteration 78290 Training loss 0.01462058536708355 Validation loss 0.020846350118517876 Accuracy 0.78466796875\n",
      "Iteration 78300 Training loss 0.014495058916509151 Validation loss 0.02100001834332943 Accuracy 0.78271484375\n",
      "Iteration 78310 Training loss 0.0132986381649971 Validation loss 0.020878475159406662 Accuracy 0.783203125\n",
      "Iteration 78320 Training loss 0.014020627364516258 Validation loss 0.020980229601264 Accuracy 0.78173828125\n",
      "Iteration 78330 Training loss 0.012819160707294941 Validation loss 0.021084513515233994 Accuracy 0.78125\n",
      "Iteration 78340 Training loss 0.012040041387081146 Validation loss 0.02110329084098339 Accuracy 0.78271484375\n",
      "Iteration 78350 Training loss 0.012627691961824894 Validation loss 0.021069837734103203 Accuracy 0.78076171875\n",
      "Iteration 78360 Training loss 0.015458175912499428 Validation loss 0.020909838378429413 Accuracy 0.78173828125\n",
      "Iteration 78370 Training loss 0.013575672172009945 Validation loss 0.021206820383667946 Accuracy 0.78125\n",
      "Iteration 78380 Training loss 0.012887449935078621 Validation loss 0.020900258794426918 Accuracy 0.78369140625\n",
      "Iteration 78390 Training loss 0.012892858125269413 Validation loss 0.02095029316842556 Accuracy 0.783203125\n",
      "Iteration 78400 Training loss 0.01273294072598219 Validation loss 0.020834743976593018 Accuracy 0.78466796875\n",
      "Iteration 78410 Training loss 0.012437229044735432 Validation loss 0.021012309938669205 Accuracy 0.783203125\n",
      "Iteration 78420 Training loss 0.017118612304329872 Validation loss 0.020785488188266754 Accuracy 0.78369140625\n",
      "Iteration 78430 Training loss 0.013909045606851578 Validation loss 0.020808618515729904 Accuracy 0.78369140625\n",
      "Iteration 78440 Training loss 0.013426559045910835 Validation loss 0.02079862169921398 Accuracy 0.78369140625\n",
      "Iteration 78450 Training loss 0.012472844682633877 Validation loss 0.02119872346520424 Accuracy 0.78125\n",
      "Iteration 78460 Training loss 0.014667445793747902 Validation loss 0.020976878702640533 Accuracy 0.78271484375\n",
      "Iteration 78470 Training loss 0.01474504079669714 Validation loss 0.02088712900876999 Accuracy 0.78271484375\n",
      "Iteration 78480 Training loss 0.013703133910894394 Validation loss 0.021004242822527885 Accuracy 0.78173828125\n",
      "Iteration 78490 Training loss 0.014340902678668499 Validation loss 0.021071864292025566 Accuracy 0.7822265625\n",
      "Iteration 78500 Training loss 0.013161586597561836 Validation loss 0.02077041007578373 Accuracy 0.783203125\n",
      "Iteration 78510 Training loss 0.014535591006278992 Validation loss 0.021298043429851532 Accuracy 0.7802734375\n",
      "Iteration 78520 Training loss 0.0131197115406394 Validation loss 0.020900234580039978 Accuracy 0.78173828125\n",
      "Iteration 78530 Training loss 0.013872039504349232 Validation loss 0.020919105038046837 Accuracy 0.78271484375\n",
      "Iteration 78540 Training loss 0.014077987521886826 Validation loss 0.020772233605384827 Accuracy 0.7841796875\n",
      "Iteration 78550 Training loss 0.013399430550634861 Validation loss 0.020915605127811432 Accuracy 0.783203125\n",
      "Iteration 78560 Training loss 0.010929581709206104 Validation loss 0.020832760259509087 Accuracy 0.7841796875\n",
      "Iteration 78570 Training loss 0.014599761925637722 Validation loss 0.021053336560726166 Accuracy 0.78076171875\n",
      "Iteration 78580 Training loss 0.014253944158554077 Validation loss 0.021025924012064934 Accuracy 0.7822265625\n",
      "Iteration 78590 Training loss 0.01364679355174303 Validation loss 0.020840736106038094 Accuracy 0.7822265625\n",
      "Iteration 78600 Training loss 0.012996301986277103 Validation loss 0.02111387439072132 Accuracy 0.78125\n",
      "Iteration 78610 Training loss 0.015210423618555069 Validation loss 0.02116170898079872 Accuracy 0.77978515625\n",
      "Iteration 78620 Training loss 0.014094808138906956 Validation loss 0.02100912109017372 Accuracy 0.78125\n",
      "Iteration 78630 Training loss 0.013373821042478085 Validation loss 0.021034453064203262 Accuracy 0.78271484375\n",
      "Iteration 78640 Training loss 0.013062188401818275 Validation loss 0.021014340221881866 Accuracy 0.78271484375\n",
      "Iteration 78650 Training loss 0.009731887839734554 Validation loss 0.020927177742123604 Accuracy 0.78173828125\n",
      "Iteration 78660 Training loss 0.014698758721351624 Validation loss 0.021569054573774338 Accuracy 0.7763671875\n",
      "Iteration 78670 Training loss 0.015746314078569412 Validation loss 0.021023249253630638 Accuracy 0.7822265625\n",
      "Iteration 78680 Training loss 0.014467610977590084 Validation loss 0.020882846787571907 Accuracy 0.783203125\n",
      "Iteration 78690 Training loss 0.01331093069165945 Validation loss 0.020840192213654518 Accuracy 0.783203125\n",
      "Iteration 78700 Training loss 0.015483848750591278 Validation loss 0.02088293433189392 Accuracy 0.783203125\n",
      "Iteration 78710 Training loss 0.013343214988708496 Validation loss 0.020748067647218704 Accuracy 0.7841796875\n",
      "Iteration 78720 Training loss 0.011785723268985748 Validation loss 0.02075616642832756 Accuracy 0.78466796875\n",
      "Iteration 78730 Training loss 0.012095586396753788 Validation loss 0.020945213735103607 Accuracy 0.78271484375\n",
      "Iteration 78740 Training loss 0.016352839767932892 Validation loss 0.02084541879594326 Accuracy 0.7841796875\n",
      "Iteration 78750 Training loss 0.013747585006058216 Validation loss 0.02080675959587097 Accuracy 0.78369140625\n",
      "Iteration 78760 Training loss 0.012233936227858067 Validation loss 0.020708149299025536 Accuracy 0.7841796875\n",
      "Iteration 78770 Training loss 0.01368836872279644 Validation loss 0.020758885890245438 Accuracy 0.78369140625\n",
      "Iteration 78780 Training loss 0.014607777819037437 Validation loss 0.02077609859406948 Accuracy 0.7841796875\n",
      "Iteration 78790 Training loss 0.013951086439192295 Validation loss 0.020963139832019806 Accuracy 0.78271484375\n",
      "Iteration 78800 Training loss 0.011663892306387424 Validation loss 0.020929811522364616 Accuracy 0.78271484375\n",
      "Iteration 78810 Training loss 0.015121814794838428 Validation loss 0.020775677636265755 Accuracy 0.7841796875\n",
      "Iteration 78820 Training loss 0.01379061583429575 Validation loss 0.020786771550774574 Accuracy 0.7841796875\n",
      "Iteration 78830 Training loss 0.014538158662617207 Validation loss 0.02081773802638054 Accuracy 0.783203125\n",
      "Iteration 78840 Training loss 0.013436797074973583 Validation loss 0.02085576392710209 Accuracy 0.7822265625\n",
      "Iteration 78850 Training loss 0.0155002037063241 Validation loss 0.020798197016119957 Accuracy 0.78369140625\n",
      "Iteration 78860 Training loss 0.01030033640563488 Validation loss 0.020813725888729095 Accuracy 0.78369140625\n",
      "Iteration 78870 Training loss 0.014765613712370396 Validation loss 0.020800698548555374 Accuracy 0.78466796875\n",
      "Iteration 78880 Training loss 0.014368451200425625 Validation loss 0.020782137289643288 Accuracy 0.7841796875\n",
      "Iteration 78890 Training loss 0.011896474286913872 Validation loss 0.020830407738685608 Accuracy 0.7841796875\n",
      "Iteration 78900 Training loss 0.013873254880309105 Validation loss 0.02104613184928894 Accuracy 0.78076171875\n",
      "Iteration 78910 Training loss 0.014232030138373375 Validation loss 0.020929735153913498 Accuracy 0.7822265625\n",
      "Iteration 78920 Training loss 0.014734182506799698 Validation loss 0.020905805751681328 Accuracy 0.78173828125\n",
      "Iteration 78930 Training loss 0.01332557387650013 Validation loss 0.021094394847750664 Accuracy 0.78125\n",
      "Iteration 78940 Training loss 0.011031994596123695 Validation loss 0.020761767402291298 Accuracy 0.783203125\n",
      "Iteration 78950 Training loss 0.013582704588770866 Validation loss 0.020839886739850044 Accuracy 0.783203125\n",
      "Iteration 78960 Training loss 0.014854353852570057 Validation loss 0.02089529298245907 Accuracy 0.783203125\n",
      "Iteration 78970 Training loss 0.013013206422328949 Validation loss 0.020969515666365623 Accuracy 0.78271484375\n",
      "Iteration 78980 Training loss 0.015330581925809383 Validation loss 0.02110058069229126 Accuracy 0.78173828125\n",
      "Iteration 78990 Training loss 0.014114576391875744 Validation loss 0.020964236930012703 Accuracy 0.783203125\n",
      "Iteration 79000 Training loss 0.013687499798834324 Validation loss 0.020908033475279808 Accuracy 0.783203125\n",
      "Iteration 79010 Training loss 0.013131263665854931 Validation loss 0.02083524316549301 Accuracy 0.783203125\n",
      "Iteration 79020 Training loss 0.01552178431302309 Validation loss 0.020902026444673538 Accuracy 0.78369140625\n",
      "Iteration 79030 Training loss 0.012898175045847893 Validation loss 0.02088707685470581 Accuracy 0.783203125\n",
      "Iteration 79040 Training loss 0.01318377535790205 Validation loss 0.020967308431863785 Accuracy 0.7822265625\n",
      "Iteration 79050 Training loss 0.013905963860452175 Validation loss 0.020809784531593323 Accuracy 0.78369140625\n",
      "Iteration 79060 Training loss 0.014112640172243118 Validation loss 0.021017709746956825 Accuracy 0.78271484375\n",
      "Iteration 79070 Training loss 0.013927680440247059 Validation loss 0.020924506708979607 Accuracy 0.78271484375\n",
      "Iteration 79080 Training loss 0.013565893284976482 Validation loss 0.02098359726369381 Accuracy 0.7822265625\n",
      "Iteration 79090 Training loss 0.012994159944355488 Validation loss 0.020788289606571198 Accuracy 0.78271484375\n",
      "Iteration 79100 Training loss 0.012042282149195671 Validation loss 0.02078728936612606 Accuracy 0.7841796875\n",
      "Iteration 79110 Training loss 0.013397491537034512 Validation loss 0.020725049078464508 Accuracy 0.78466796875\n",
      "Iteration 79120 Training loss 0.013528065755963326 Validation loss 0.020857997238636017 Accuracy 0.78271484375\n",
      "Iteration 79130 Training loss 0.012568498961627483 Validation loss 0.02090882696211338 Accuracy 0.783203125\n",
      "Iteration 79140 Training loss 0.012255069799721241 Validation loss 0.021014105528593063 Accuracy 0.7802734375\n",
      "Iteration 79150 Training loss 0.013539331965148449 Validation loss 0.02095041424036026 Accuracy 0.78271484375\n",
      "Iteration 79160 Training loss 0.014598200097680092 Validation loss 0.020726801827549934 Accuracy 0.7841796875\n",
      "Iteration 79170 Training loss 0.017189383506774902 Validation loss 0.02093249373137951 Accuracy 0.78369140625\n",
      "Iteration 79180 Training loss 0.014341270551085472 Validation loss 0.02077053114771843 Accuracy 0.78369140625\n",
      "Iteration 79190 Training loss 0.015563947148621082 Validation loss 0.0211328137665987 Accuracy 0.78076171875\n",
      "Iteration 79200 Training loss 0.013422276824712753 Validation loss 0.020977625623345375 Accuracy 0.78173828125\n",
      "Iteration 79210 Training loss 0.01226989645510912 Validation loss 0.020804129540920258 Accuracy 0.78271484375\n",
      "Iteration 79220 Training loss 0.012259227223694324 Validation loss 0.02111257053911686 Accuracy 0.78173828125\n",
      "Iteration 79230 Training loss 0.013110185973346233 Validation loss 0.020840857177972794 Accuracy 0.78271484375\n",
      "Iteration 79240 Training loss 0.013367482461035252 Validation loss 0.020963851362466812 Accuracy 0.78271484375\n",
      "Iteration 79250 Training loss 0.01493107434362173 Validation loss 0.020877055823802948 Accuracy 0.78369140625\n",
      "Iteration 79260 Training loss 0.012595750391483307 Validation loss 0.020760223269462585 Accuracy 0.78515625\n",
      "Iteration 79270 Training loss 0.012596211396157742 Validation loss 0.02079141139984131 Accuracy 0.783203125\n",
      "Iteration 79280 Training loss 0.012041029520332813 Validation loss 0.020850172266364098 Accuracy 0.7841796875\n",
      "Iteration 79290 Training loss 0.010975786484777927 Validation loss 0.02072504349052906 Accuracy 0.7841796875\n",
      "Iteration 79300 Training loss 0.014860516414046288 Validation loss 0.021002592518925667 Accuracy 0.7822265625\n",
      "Iteration 79310 Training loss 0.012670033611357212 Validation loss 0.021166455000638962 Accuracy 0.77978515625\n",
      "Iteration 79320 Training loss 0.010564561933279037 Validation loss 0.021030761301517487 Accuracy 0.78173828125\n",
      "Iteration 79330 Training loss 0.013810094445943832 Validation loss 0.02100619487464428 Accuracy 0.7822265625\n",
      "Iteration 79340 Training loss 0.012737155891954899 Validation loss 0.02084319479763508 Accuracy 0.78271484375\n",
      "Iteration 79350 Training loss 0.013437670655548573 Validation loss 0.02076592668890953 Accuracy 0.78466796875\n",
      "Iteration 79360 Training loss 0.012520693242549896 Validation loss 0.021116863936185837 Accuracy 0.78173828125\n",
      "Iteration 79370 Training loss 0.010842610150575638 Validation loss 0.021306922659277916 Accuracy 0.77880859375\n",
      "Iteration 79380 Training loss 0.015049189329147339 Validation loss 0.021030405536293983 Accuracy 0.78271484375\n",
      "Iteration 79390 Training loss 0.013164792209863663 Validation loss 0.020823083817958832 Accuracy 0.78369140625\n",
      "Iteration 79400 Training loss 0.013150769285857677 Validation loss 0.02099967934191227 Accuracy 0.78173828125\n",
      "Iteration 79410 Training loss 0.011787795461714268 Validation loss 0.02086685411632061 Accuracy 0.783203125\n",
      "Iteration 79420 Training loss 0.010943089611828327 Validation loss 0.020727548748254776 Accuracy 0.78515625\n",
      "Iteration 79430 Training loss 0.014647143892943859 Validation loss 0.021405180916190147 Accuracy 0.779296875\n",
      "Iteration 79440 Training loss 0.01359026413410902 Validation loss 0.0208919458091259 Accuracy 0.78369140625\n",
      "Iteration 79450 Training loss 0.01357512641698122 Validation loss 0.02080007828772068 Accuracy 0.78515625\n",
      "Iteration 79460 Training loss 0.010867279022932053 Validation loss 0.02093374729156494 Accuracy 0.783203125\n",
      "Iteration 79470 Training loss 0.013975642621517181 Validation loss 0.020718641579151154 Accuracy 0.78515625\n",
      "Iteration 79480 Training loss 0.014021153561770916 Validation loss 0.0210783239454031 Accuracy 0.78076171875\n",
      "Iteration 79490 Training loss 0.01112558227032423 Validation loss 0.021031495183706284 Accuracy 0.7822265625\n",
      "Iteration 79500 Training loss 0.011621931567788124 Validation loss 0.02078862488269806 Accuracy 0.783203125\n",
      "Iteration 79510 Training loss 0.013966384343802929 Validation loss 0.0209710244089365 Accuracy 0.78173828125\n",
      "Iteration 79520 Training loss 0.013013889081776142 Validation loss 0.0210355706512928 Accuracy 0.78173828125\n",
      "Iteration 79530 Training loss 0.013644523918628693 Validation loss 0.02103302627801895 Accuracy 0.78173828125\n",
      "Iteration 79540 Training loss 0.013227013871073723 Validation loss 0.020875979214906693 Accuracy 0.78271484375\n",
      "Iteration 79550 Training loss 0.012883426621556282 Validation loss 0.020831292495131493 Accuracy 0.78271484375\n",
      "Iteration 79560 Training loss 0.015250567346811295 Validation loss 0.020814992487430573 Accuracy 0.7841796875\n",
      "Iteration 79570 Training loss 0.013549691066145897 Validation loss 0.021099833771586418 Accuracy 0.78076171875\n",
      "Iteration 79580 Training loss 0.01367316022515297 Validation loss 0.02077639102935791 Accuracy 0.78369140625\n",
      "Iteration 79590 Training loss 0.01301528513431549 Validation loss 0.02079751342535019 Accuracy 0.78369140625\n",
      "Iteration 79600 Training loss 0.013101992197334766 Validation loss 0.021065937355160713 Accuracy 0.78173828125\n",
      "Iteration 79610 Training loss 0.013917757198214531 Validation loss 0.020860478281974792 Accuracy 0.78369140625\n",
      "Iteration 79620 Training loss 0.013957562856376171 Validation loss 0.020907923579216003 Accuracy 0.783203125\n",
      "Iteration 79630 Training loss 0.012716861441731453 Validation loss 0.020868197083473206 Accuracy 0.7841796875\n",
      "Iteration 79640 Training loss 0.014478016644716263 Validation loss 0.02107870951294899 Accuracy 0.78173828125\n",
      "Iteration 79650 Training loss 0.013985206373035908 Validation loss 0.0209469273686409 Accuracy 0.7841796875\n",
      "Iteration 79660 Training loss 0.013001604937016964 Validation loss 0.020952453836798668 Accuracy 0.7841796875\n",
      "Iteration 79670 Training loss 0.016290852800011635 Validation loss 0.02102503925561905 Accuracy 0.78173828125\n",
      "Iteration 79680 Training loss 0.01324382983148098 Validation loss 0.020808927714824677 Accuracy 0.78369140625\n",
      "Iteration 79690 Training loss 0.012669149786233902 Validation loss 0.02096346579492092 Accuracy 0.783203125\n",
      "Iteration 79700 Training loss 0.014746439643204212 Validation loss 0.020971596240997314 Accuracy 0.78271484375\n",
      "Iteration 79710 Training loss 0.012823307886719704 Validation loss 0.021209103986620903 Accuracy 0.77978515625\n",
      "Iteration 79720 Training loss 0.011819222941994667 Validation loss 0.02104353904724121 Accuracy 0.78271484375\n",
      "Iteration 79730 Training loss 0.013122458010911942 Validation loss 0.021176869049668312 Accuracy 0.78125\n",
      "Iteration 79740 Training loss 0.01440636906772852 Validation loss 0.021019430831074715 Accuracy 0.7822265625\n",
      "Iteration 79750 Training loss 0.01232869178056717 Validation loss 0.020946549251675606 Accuracy 0.78271484375\n",
      "Iteration 79760 Training loss 0.01207165140658617 Validation loss 0.020845385268330574 Accuracy 0.78369140625\n",
      "Iteration 79770 Training loss 0.019146520644426346 Validation loss 0.022081326693296432 Accuracy 0.771484375\n",
      "Iteration 79780 Training loss 0.015465588308870792 Validation loss 0.02112107165157795 Accuracy 0.78125\n",
      "Iteration 79790 Training loss 0.012127125635743141 Validation loss 0.020777303725481033 Accuracy 0.7841796875\n",
      "Iteration 79800 Training loss 0.01469956710934639 Validation loss 0.021077480167150497 Accuracy 0.7822265625\n",
      "Iteration 79810 Training loss 0.01396136824041605 Validation loss 0.02122274413704872 Accuracy 0.77978515625\n",
      "Iteration 79820 Training loss 0.014222618192434311 Validation loss 0.021059464663267136 Accuracy 0.78271484375\n",
      "Iteration 79830 Training loss 0.014928031712770462 Validation loss 0.020961325615644455 Accuracy 0.78271484375\n",
      "Iteration 79840 Training loss 0.011816620826721191 Validation loss 0.021077293902635574 Accuracy 0.7822265625\n",
      "Iteration 79850 Training loss 0.014312271028757095 Validation loss 0.02091999724507332 Accuracy 0.783203125\n",
      "Iteration 79860 Training loss 0.013886570930480957 Validation loss 0.0210206750780344 Accuracy 0.78271484375\n",
      "Iteration 79870 Training loss 0.013652755878865719 Validation loss 0.02076694369316101 Accuracy 0.78466796875\n",
      "Iteration 79880 Training loss 0.012593439780175686 Validation loss 0.020854178816080093 Accuracy 0.78466796875\n",
      "Iteration 79890 Training loss 0.012326972559094429 Validation loss 0.021002022549510002 Accuracy 0.783203125\n",
      "Iteration 79900 Training loss 0.013616648502647877 Validation loss 0.021054327487945557 Accuracy 0.7822265625\n",
      "Iteration 79910 Training loss 0.012980922125279903 Validation loss 0.02090776525437832 Accuracy 0.783203125\n",
      "Iteration 79920 Training loss 0.010519711300730705 Validation loss 0.02096700109541416 Accuracy 0.783203125\n",
      "Iteration 79930 Training loss 0.010166390798985958 Validation loss 0.020936215296387672 Accuracy 0.78271484375\n",
      "Iteration 79940 Training loss 0.01201616320759058 Validation loss 0.020894447341561317 Accuracy 0.783203125\n",
      "Iteration 79950 Training loss 0.012647608295083046 Validation loss 0.02081897482275963 Accuracy 0.78466796875\n",
      "Iteration 79960 Training loss 0.013963132165372372 Validation loss 0.020741160959005356 Accuracy 0.78466796875\n",
      "Iteration 79970 Training loss 0.013908935710787773 Validation loss 0.020988846197724342 Accuracy 0.78173828125\n",
      "Iteration 79980 Training loss 0.013722619041800499 Validation loss 0.020857373252511024 Accuracy 0.783203125\n",
      "Iteration 79990 Training loss 0.014043853618204594 Validation loss 0.02109692245721817 Accuracy 0.78125\n",
      "Iteration 80000 Training loss 0.013064666651189327 Validation loss 0.020827485248446465 Accuracy 0.78369140625\n",
      "Iteration 80010 Training loss 0.015191360376775265 Validation loss 0.021106097847223282 Accuracy 0.78076171875\n",
      "Iteration 80020 Training loss 0.0127387261018157 Validation loss 0.02096085622906685 Accuracy 0.78271484375\n",
      "Iteration 80030 Training loss 0.013046730309724808 Validation loss 0.020998740568757057 Accuracy 0.7822265625\n",
      "Iteration 80040 Training loss 0.014445321634411812 Validation loss 0.020821042358875275 Accuracy 0.78369140625\n",
      "Iteration 80050 Training loss 0.012151677161455154 Validation loss 0.021170808002352715 Accuracy 0.78076171875\n",
      "Iteration 80060 Training loss 0.014428820461034775 Validation loss 0.021101264283061028 Accuracy 0.78173828125\n",
      "Iteration 80070 Training loss 0.012968804687261581 Validation loss 0.020948313176631927 Accuracy 0.7822265625\n",
      "Iteration 80080 Training loss 0.014642204158008099 Validation loss 0.021253302693367004 Accuracy 0.77880859375\n",
      "Iteration 80090 Training loss 0.015511985868215561 Validation loss 0.02103150635957718 Accuracy 0.7822265625\n",
      "Iteration 80100 Training loss 0.011703206226229668 Validation loss 0.021129999309778214 Accuracy 0.78125\n",
      "Iteration 80110 Training loss 0.016351526603102684 Validation loss 0.021453646942973137 Accuracy 0.779296875\n",
      "Iteration 80120 Training loss 0.014752297662198544 Validation loss 0.02110964246094227 Accuracy 0.7822265625\n",
      "Iteration 80130 Training loss 0.01777331531047821 Validation loss 0.022286774590611458 Accuracy 0.76953125\n",
      "Iteration 80140 Training loss 0.01383098866790533 Validation loss 0.02099716290831566 Accuracy 0.7822265625\n",
      "Iteration 80150 Training loss 0.013938482850790024 Validation loss 0.020916059613227844 Accuracy 0.78271484375\n",
      "Iteration 80160 Training loss 0.016203906387090683 Validation loss 0.021242639049887657 Accuracy 0.779296875\n",
      "Iteration 80170 Training loss 0.014812953770160675 Validation loss 0.021004965528845787 Accuracy 0.78125\n",
      "Iteration 80180 Training loss 0.011946284212172031 Validation loss 0.021113676950335503 Accuracy 0.78076171875\n",
      "Iteration 80190 Training loss 0.015083981677889824 Validation loss 0.02099519968032837 Accuracy 0.7822265625\n",
      "Iteration 80200 Training loss 0.01390956062823534 Validation loss 0.020923735573887825 Accuracy 0.783203125\n",
      "Iteration 80210 Training loss 0.01422239188104868 Validation loss 0.020828843116760254 Accuracy 0.783203125\n",
      "Iteration 80220 Training loss 0.010848218575119972 Validation loss 0.021103519946336746 Accuracy 0.78125\n",
      "Iteration 80230 Training loss 0.014957216568291187 Validation loss 0.020923784002661705 Accuracy 0.783203125\n",
      "Iteration 80240 Training loss 0.013057675212621689 Validation loss 0.020955126732587814 Accuracy 0.78271484375\n",
      "Iteration 80250 Training loss 0.013333638198673725 Validation loss 0.020880717784166336 Accuracy 0.783203125\n",
      "Iteration 80260 Training loss 0.012962859123945236 Validation loss 0.02091425284743309 Accuracy 0.78271484375\n",
      "Iteration 80270 Training loss 0.01420531328767538 Validation loss 0.02089322730898857 Accuracy 0.783203125\n",
      "Iteration 80280 Training loss 0.014024314470589161 Validation loss 0.020953642204403877 Accuracy 0.7822265625\n",
      "Iteration 80290 Training loss 0.014205824583768845 Validation loss 0.0210240688174963 Accuracy 0.7822265625\n",
      "Iteration 80300 Training loss 0.01644197851419449 Validation loss 0.02170494571328163 Accuracy 0.775390625\n",
      "Iteration 80310 Training loss 0.013689661398530006 Validation loss 0.021104393526911736 Accuracy 0.7822265625\n",
      "Iteration 80320 Training loss 0.013078996911644936 Validation loss 0.020929142832756042 Accuracy 0.7841796875\n",
      "Iteration 80330 Training loss 0.01471326407045126 Validation loss 0.02091732807457447 Accuracy 0.783203125\n",
      "Iteration 80340 Training loss 0.010033832862973213 Validation loss 0.02081642486155033 Accuracy 0.78271484375\n",
      "Iteration 80350 Training loss 0.014592450112104416 Validation loss 0.020857220515608788 Accuracy 0.783203125\n",
      "Iteration 80360 Training loss 0.013457885943353176 Validation loss 0.02111778035759926 Accuracy 0.7802734375\n",
      "Iteration 80370 Training loss 0.012194647453725338 Validation loss 0.021221278235316277 Accuracy 0.7802734375\n",
      "Iteration 80380 Training loss 0.01260943803936243 Validation loss 0.020882971584796906 Accuracy 0.7841796875\n",
      "Iteration 80390 Training loss 0.012994487769901752 Validation loss 0.020732484757900238 Accuracy 0.78564453125\n",
      "Iteration 80400 Training loss 0.010855463333427906 Validation loss 0.02081833966076374 Accuracy 0.7841796875\n",
      "Iteration 80410 Training loss 0.011664941906929016 Validation loss 0.021110795438289642 Accuracy 0.78076171875\n",
      "Iteration 80420 Training loss 0.014217127114534378 Validation loss 0.02105502225458622 Accuracy 0.78076171875\n",
      "Iteration 80430 Training loss 0.014977103099226952 Validation loss 0.021106010302901268 Accuracy 0.78125\n",
      "Iteration 80440 Training loss 0.013395446352660656 Validation loss 0.020945416763424873 Accuracy 0.78173828125\n",
      "Iteration 80450 Training loss 0.013998891226947308 Validation loss 0.021033229306340218 Accuracy 0.7822265625\n",
      "Iteration 80460 Training loss 0.013355061411857605 Validation loss 0.020937660709023476 Accuracy 0.783203125\n",
      "Iteration 80470 Training loss 0.014639380387961864 Validation loss 0.02101575955748558 Accuracy 0.78271484375\n",
      "Iteration 80480 Training loss 0.012964032590389252 Validation loss 0.02087356150150299 Accuracy 0.7841796875\n",
      "Iteration 80490 Training loss 0.014956334605813026 Validation loss 0.020875882357358932 Accuracy 0.78369140625\n",
      "Iteration 80500 Training loss 0.01120894867926836 Validation loss 0.02079867385327816 Accuracy 0.78369140625\n",
      "Iteration 80510 Training loss 0.015736263245344162 Validation loss 0.021327640861272812 Accuracy 0.77978515625\n",
      "Iteration 80520 Training loss 0.012954422272741795 Validation loss 0.021003656089305878 Accuracy 0.78271484375\n",
      "Iteration 80530 Training loss 0.012204146943986416 Validation loss 0.021042289212346077 Accuracy 0.78076171875\n",
      "Iteration 80540 Training loss 0.013888976536691189 Validation loss 0.021007472649216652 Accuracy 0.78173828125\n",
      "Iteration 80550 Training loss 0.013481282629072666 Validation loss 0.020938904955983162 Accuracy 0.783203125\n",
      "Iteration 80560 Training loss 0.01192589569836855 Validation loss 0.021161137148737907 Accuracy 0.78125\n",
      "Iteration 80570 Training loss 0.011069043539464474 Validation loss 0.021224237978458405 Accuracy 0.779296875\n",
      "Iteration 80580 Training loss 0.013061225414276123 Validation loss 0.020900439471006393 Accuracy 0.7822265625\n",
      "Iteration 80590 Training loss 0.014044425450265408 Validation loss 0.021003279834985733 Accuracy 0.7822265625\n",
      "Iteration 80600 Training loss 0.011065118946135044 Validation loss 0.020949747413396835 Accuracy 0.78271484375\n",
      "Iteration 80610 Training loss 0.014561118558049202 Validation loss 0.021184340119361877 Accuracy 0.7802734375\n",
      "Iteration 80620 Training loss 0.013115177862346172 Validation loss 0.020887132734060287 Accuracy 0.783203125\n",
      "Iteration 80630 Training loss 0.012992395088076591 Validation loss 0.0209233146160841 Accuracy 0.7822265625\n",
      "Iteration 80640 Training loss 0.01289455033838749 Validation loss 0.020775213837623596 Accuracy 0.78466796875\n",
      "Iteration 80650 Training loss 0.012686899863183498 Validation loss 0.02099912241101265 Accuracy 0.783203125\n",
      "Iteration 80660 Training loss 0.012256438843905926 Validation loss 0.021165495738387108 Accuracy 0.78076171875\n",
      "Iteration 80670 Training loss 0.012200279161334038 Validation loss 0.021061602979898453 Accuracy 0.78125\n",
      "Iteration 80680 Training loss 0.014150494709610939 Validation loss 0.020875677466392517 Accuracy 0.78369140625\n",
      "Iteration 80690 Training loss 0.014321422204375267 Validation loss 0.020920610055327415 Accuracy 0.7822265625\n",
      "Iteration 80700 Training loss 0.014860954135656357 Validation loss 0.021083295345306396 Accuracy 0.78125\n",
      "Iteration 80710 Training loss 0.013456159271299839 Validation loss 0.02098172716796398 Accuracy 0.7822265625\n",
      "Iteration 80720 Training loss 0.015384012833237648 Validation loss 0.020967306569218636 Accuracy 0.78173828125\n",
      "Iteration 80730 Training loss 0.014804951846599579 Validation loss 0.021039577201008797 Accuracy 0.78125\n",
      "Iteration 80740 Training loss 0.013784757815301418 Validation loss 0.02102934569120407 Accuracy 0.7802734375\n",
      "Iteration 80750 Training loss 0.013376174494624138 Validation loss 0.021043622866272926 Accuracy 0.78173828125\n",
      "Iteration 80760 Training loss 0.010589775629341602 Validation loss 0.021062951534986496 Accuracy 0.78271484375\n",
      "Iteration 80770 Training loss 0.014028958044946194 Validation loss 0.021114584058523178 Accuracy 0.78125\n",
      "Iteration 80780 Training loss 0.013388247229158878 Validation loss 0.021071448922157288 Accuracy 0.78173828125\n",
      "Iteration 80790 Training loss 0.014591814950108528 Validation loss 0.020978456363081932 Accuracy 0.7822265625\n",
      "Iteration 80800 Training loss 0.014132983051240444 Validation loss 0.021095924079418182 Accuracy 0.78076171875\n",
      "Iteration 80810 Training loss 0.014211316592991352 Validation loss 0.020910922437906265 Accuracy 0.78369140625\n",
      "Iteration 80820 Training loss 0.012645985931158066 Validation loss 0.020743466913700104 Accuracy 0.7841796875\n",
      "Iteration 80830 Training loss 0.01508395653218031 Validation loss 0.021025096997618675 Accuracy 0.78125\n",
      "Iteration 80840 Training loss 0.01348451990634203 Validation loss 0.02116851694881916 Accuracy 0.78173828125\n",
      "Iteration 80850 Training loss 0.014551580883562565 Validation loss 0.02092953957617283 Accuracy 0.78369140625\n",
      "Iteration 80860 Training loss 0.012047172524034977 Validation loss 0.020826371386647224 Accuracy 0.783203125\n",
      "Iteration 80870 Training loss 0.012476556934416294 Validation loss 0.02089909091591835 Accuracy 0.78369140625\n",
      "Iteration 80880 Training loss 0.012944427318871021 Validation loss 0.021232645958662033 Accuracy 0.77978515625\n",
      "Iteration 80890 Training loss 0.0122167207300663 Validation loss 0.021179847419261932 Accuracy 0.78076171875\n",
      "Iteration 80900 Training loss 0.012017461471259594 Validation loss 0.02076069451868534 Accuracy 0.7841796875\n",
      "Iteration 80910 Training loss 0.013173292391002178 Validation loss 0.020867055281996727 Accuracy 0.78369140625\n",
      "Iteration 80920 Training loss 0.012450288981199265 Validation loss 0.02096063457429409 Accuracy 0.78173828125\n",
      "Iteration 80930 Training loss 0.013236773200333118 Validation loss 0.021032480522990227 Accuracy 0.78271484375\n",
      "Iteration 80940 Training loss 0.013999709859490395 Validation loss 0.020906422287225723 Accuracy 0.78271484375\n",
      "Iteration 80950 Training loss 0.015904052183032036 Validation loss 0.020877663046121597 Accuracy 0.783203125\n",
      "Iteration 80960 Training loss 0.01321987435221672 Validation loss 0.020875077694654465 Accuracy 0.78369140625\n",
      "Iteration 80970 Training loss 0.01316409558057785 Validation loss 0.02084430679678917 Accuracy 0.78369140625\n",
      "Iteration 80980 Training loss 0.01311598438769579 Validation loss 0.020757991820573807 Accuracy 0.78466796875\n",
      "Iteration 80990 Training loss 0.013721946626901627 Validation loss 0.020907694473862648 Accuracy 0.7841796875\n",
      "Iteration 81000 Training loss 0.014291046187281609 Validation loss 0.020817318931221962 Accuracy 0.78369140625\n",
      "Iteration 81010 Training loss 0.016703026369214058 Validation loss 0.02096588723361492 Accuracy 0.78271484375\n",
      "Iteration 81020 Training loss 0.010588293895125389 Validation loss 0.021018221974372864 Accuracy 0.783203125\n",
      "Iteration 81030 Training loss 0.012719492428004742 Validation loss 0.021020829677581787 Accuracy 0.7822265625\n",
      "Iteration 81040 Training loss 0.013337480835616589 Validation loss 0.021064169704914093 Accuracy 0.7822265625\n",
      "Iteration 81050 Training loss 0.012592099606990814 Validation loss 0.020857250317931175 Accuracy 0.7841796875\n",
      "Iteration 81060 Training loss 0.013602427206933498 Validation loss 0.021078839898109436 Accuracy 0.78076171875\n",
      "Iteration 81070 Training loss 0.01340483222156763 Validation loss 0.021094027906656265 Accuracy 0.78076171875\n",
      "Iteration 81080 Training loss 0.013149897567927837 Validation loss 0.021220041438937187 Accuracy 0.78125\n",
      "Iteration 81090 Training loss 0.014793390408158302 Validation loss 0.020811811089515686 Accuracy 0.7841796875\n",
      "Iteration 81100 Training loss 0.011517479084432125 Validation loss 0.021163955330848694 Accuracy 0.78173828125\n",
      "Iteration 81110 Training loss 0.01575230434536934 Validation loss 0.021067440509796143 Accuracy 0.78173828125\n",
      "Iteration 81120 Training loss 0.01283288560807705 Validation loss 0.02103131264448166 Accuracy 0.78271484375\n",
      "Iteration 81130 Training loss 0.013370162807404995 Validation loss 0.020991574972867966 Accuracy 0.7822265625\n",
      "Iteration 81140 Training loss 0.013273756951093674 Validation loss 0.020916501060128212 Accuracy 0.783203125\n",
      "Iteration 81150 Training loss 0.013941572979092598 Validation loss 0.020869916304945946 Accuracy 0.783203125\n",
      "Iteration 81160 Training loss 0.012489364482462406 Validation loss 0.020769938826560974 Accuracy 0.78369140625\n",
      "Iteration 81170 Training loss 0.01601259596645832 Validation loss 0.021261272951960564 Accuracy 0.77880859375\n",
      "Iteration 81180 Training loss 0.012539262883365154 Validation loss 0.021033992990851402 Accuracy 0.78076171875\n",
      "Iteration 81190 Training loss 0.01226989645510912 Validation loss 0.021116916090250015 Accuracy 0.78173828125\n",
      "Iteration 81200 Training loss 0.014826917089521885 Validation loss 0.020958270877599716 Accuracy 0.7822265625\n",
      "Iteration 81210 Training loss 0.013250552117824554 Validation loss 0.020944608375430107 Accuracy 0.78369140625\n",
      "Iteration 81220 Training loss 0.010504630394279957 Validation loss 0.020865295082330704 Accuracy 0.7822265625\n",
      "Iteration 81230 Training loss 0.012835399247705936 Validation loss 0.021172624081373215 Accuracy 0.78125\n",
      "Iteration 81240 Training loss 0.013586160726845264 Validation loss 0.020870134234428406 Accuracy 0.783203125\n",
      "Iteration 81250 Training loss 0.013774305582046509 Validation loss 0.02104916051030159 Accuracy 0.78173828125\n",
      "Iteration 81260 Training loss 0.012836522422730923 Validation loss 0.021119877696037292 Accuracy 0.78125\n",
      "Iteration 81270 Training loss 0.01363079808652401 Validation loss 0.021164545789361 Accuracy 0.78125\n",
      "Iteration 81280 Training loss 0.012264602817595005 Validation loss 0.02091541700065136 Accuracy 0.78271484375\n",
      "Iteration 81290 Training loss 0.013856453821063042 Validation loss 0.02113116718828678 Accuracy 0.78076171875\n",
      "Iteration 81300 Training loss 0.01340480800718069 Validation loss 0.02087627351284027 Accuracy 0.7822265625\n",
      "Iteration 81310 Training loss 0.014041696675121784 Validation loss 0.020926635712385178 Accuracy 0.78271484375\n",
      "Iteration 81320 Training loss 0.011548731476068497 Validation loss 0.02101520635187626 Accuracy 0.78076171875\n",
      "Iteration 81330 Training loss 0.012110180221498013 Validation loss 0.021023724228143692 Accuracy 0.78125\n",
      "Iteration 81340 Training loss 0.012240779586136341 Validation loss 0.02086806856095791 Accuracy 0.783203125\n",
      "Iteration 81350 Training loss 0.011160524562001228 Validation loss 0.020958716049790382 Accuracy 0.78271484375\n",
      "Iteration 81360 Training loss 0.013704289682209492 Validation loss 0.02107570692896843 Accuracy 0.78271484375\n",
      "Iteration 81370 Training loss 0.015118508599698544 Validation loss 0.020916808396577835 Accuracy 0.78369140625\n",
      "Iteration 81380 Training loss 0.01265182625502348 Validation loss 0.020860634744167328 Accuracy 0.78369140625\n",
      "Iteration 81390 Training loss 0.013989581726491451 Validation loss 0.020862402394413948 Accuracy 0.7822265625\n",
      "Iteration 81400 Training loss 0.013696210458874702 Validation loss 0.02095854841172695 Accuracy 0.7822265625\n",
      "Iteration 81410 Training loss 0.014116485603153706 Validation loss 0.02097640559077263 Accuracy 0.7822265625\n",
      "Iteration 81420 Training loss 0.012983993627130985 Validation loss 0.02089647948741913 Accuracy 0.78271484375\n",
      "Iteration 81430 Training loss 0.012632288038730621 Validation loss 0.02102089300751686 Accuracy 0.78076171875\n",
      "Iteration 81440 Training loss 0.013000181876122952 Validation loss 0.02131820283830166 Accuracy 0.77734375\n",
      "Iteration 81450 Training loss 0.014860603027045727 Validation loss 0.020802199840545654 Accuracy 0.7841796875\n",
      "Iteration 81460 Training loss 0.013902362436056137 Validation loss 0.020978109911084175 Accuracy 0.783203125\n",
      "Iteration 81470 Training loss 0.01336083747446537 Validation loss 0.020848345011472702 Accuracy 0.7841796875\n",
      "Iteration 81480 Training loss 0.011866685934364796 Validation loss 0.021113881841301918 Accuracy 0.78076171875\n",
      "Iteration 81490 Training loss 0.013627003878355026 Validation loss 0.021157583221793175 Accuracy 0.7802734375\n",
      "Iteration 81500 Training loss 0.012869611382484436 Validation loss 0.02082255482673645 Accuracy 0.78369140625\n",
      "Iteration 81510 Training loss 0.014358596876263618 Validation loss 0.02108609490096569 Accuracy 0.7822265625\n",
      "Iteration 81520 Training loss 0.011850796639919281 Validation loss 0.020826078951358795 Accuracy 0.78369140625\n",
      "Iteration 81530 Training loss 0.01347323041409254 Validation loss 0.02132275328040123 Accuracy 0.779296875\n",
      "Iteration 81540 Training loss 0.014002692885696888 Validation loss 0.020812787115573883 Accuracy 0.78466796875\n",
      "Iteration 81550 Training loss 0.015479530207812786 Validation loss 0.020813100039958954 Accuracy 0.78369140625\n",
      "Iteration 81560 Training loss 0.010507616214454174 Validation loss 0.021098213270306587 Accuracy 0.7802734375\n",
      "Iteration 81570 Training loss 0.012176208198070526 Validation loss 0.021096406504511833 Accuracy 0.78125\n",
      "Iteration 81580 Training loss 0.016569724306464195 Validation loss 0.021588295698165894 Accuracy 0.77587890625\n",
      "Iteration 81590 Training loss 0.011844060383737087 Validation loss 0.02087317779660225 Accuracy 0.783203125\n",
      "Iteration 81600 Training loss 0.014671997167170048 Validation loss 0.020734619349241257 Accuracy 0.7841796875\n",
      "Iteration 81610 Training loss 0.014920986257493496 Validation loss 0.020903749391436577 Accuracy 0.78271484375\n",
      "Iteration 81620 Training loss 0.01392407063394785 Validation loss 0.0208713561296463 Accuracy 0.78271484375\n",
      "Iteration 81630 Training loss 0.014536283910274506 Validation loss 0.020888829603791237 Accuracy 0.78369140625\n",
      "Iteration 81640 Training loss 0.014489449560642242 Validation loss 0.020779386162757874 Accuracy 0.783203125\n",
      "Iteration 81650 Training loss 0.012814326211810112 Validation loss 0.02089862711727619 Accuracy 0.78369140625\n",
      "Iteration 81660 Training loss 0.014767084270715714 Validation loss 0.020885691046714783 Accuracy 0.783203125\n",
      "Iteration 81670 Training loss 0.014753038063645363 Validation loss 0.021779021248221397 Accuracy 0.77587890625\n",
      "Iteration 81680 Training loss 0.011685594916343689 Validation loss 0.02099108137190342 Accuracy 0.78271484375\n",
      "Iteration 81690 Training loss 0.013777447864413261 Validation loss 0.02092805691063404 Accuracy 0.7822265625\n",
      "Iteration 81700 Training loss 0.013183880597352982 Validation loss 0.02101220190525055 Accuracy 0.78125\n",
      "Iteration 81710 Training loss 0.012617253698408604 Validation loss 0.02086552232503891 Accuracy 0.783203125\n",
      "Iteration 81720 Training loss 0.013546369038522243 Validation loss 0.02079787477850914 Accuracy 0.78369140625\n",
      "Iteration 81730 Training loss 0.015375603921711445 Validation loss 0.02100829966366291 Accuracy 0.7822265625\n",
      "Iteration 81740 Training loss 0.012259562499821186 Validation loss 0.021075362339615822 Accuracy 0.78076171875\n",
      "Iteration 81750 Training loss 0.012828510254621506 Validation loss 0.02099810540676117 Accuracy 0.783203125\n",
      "Iteration 81760 Training loss 0.01305678766220808 Validation loss 0.02081095241010189 Accuracy 0.7841796875\n",
      "Iteration 81770 Training loss 0.011927250772714615 Validation loss 0.020772797986865044 Accuracy 0.7841796875\n",
      "Iteration 81780 Training loss 0.017261860892176628 Validation loss 0.021067382767796516 Accuracy 0.7822265625\n",
      "Iteration 81790 Training loss 0.014142008498311043 Validation loss 0.021038822829723358 Accuracy 0.78125\n",
      "Iteration 81800 Training loss 0.012012593448162079 Validation loss 0.020884737372398376 Accuracy 0.7822265625\n",
      "Iteration 81810 Training loss 0.013617151416838169 Validation loss 0.02089964784681797 Accuracy 0.78271484375\n",
      "Iteration 81820 Training loss 0.01427406445145607 Validation loss 0.020872600376605988 Accuracy 0.78271484375\n",
      "Iteration 81830 Training loss 0.011541137471795082 Validation loss 0.020865848287940025 Accuracy 0.78369140625\n",
      "Iteration 81840 Training loss 0.01381224300712347 Validation loss 0.021082650870084763 Accuracy 0.78271484375\n",
      "Iteration 81850 Training loss 0.012483029626309872 Validation loss 0.020829325541853905 Accuracy 0.783203125\n",
      "Iteration 81860 Training loss 0.013650408014655113 Validation loss 0.02083834819495678 Accuracy 0.78173828125\n",
      "Iteration 81870 Training loss 0.013277224265038967 Validation loss 0.020829476416110992 Accuracy 0.7841796875\n",
      "Iteration 81880 Training loss 0.012389165349304676 Validation loss 0.020837245509028435 Accuracy 0.78271484375\n",
      "Iteration 81890 Training loss 0.01135833840817213 Validation loss 0.020859360694885254 Accuracy 0.7822265625\n",
      "Iteration 81900 Training loss 0.012204938568174839 Validation loss 0.021040204912424088 Accuracy 0.78173828125\n",
      "Iteration 81910 Training loss 0.012703878805041313 Validation loss 0.020850161090493202 Accuracy 0.783203125\n",
      "Iteration 81920 Training loss 0.012898247689008713 Validation loss 0.02080487459897995 Accuracy 0.783203125\n",
      "Iteration 81930 Training loss 0.012640975415706635 Validation loss 0.02084162086248398 Accuracy 0.78369140625\n",
      "Iteration 81940 Training loss 0.010203063488006592 Validation loss 0.02088121697306633 Accuracy 0.783203125\n",
      "Iteration 81950 Training loss 0.012767023406922817 Validation loss 0.020948756486177444 Accuracy 0.7822265625\n",
      "Iteration 81960 Training loss 0.012390258722007275 Validation loss 0.02094379812479019 Accuracy 0.78271484375\n",
      "Iteration 81970 Training loss 0.01328772958368063 Validation loss 0.020850306376814842 Accuracy 0.7841796875\n",
      "Iteration 81980 Training loss 0.014268764294683933 Validation loss 0.020782914012670517 Accuracy 0.7841796875\n",
      "Iteration 81990 Training loss 0.010946983471512794 Validation loss 0.02104191668331623 Accuracy 0.78271484375\n",
      "Iteration 82000 Training loss 0.013314735144376755 Validation loss 0.020846737548708916 Accuracy 0.783203125\n",
      "Iteration 82010 Training loss 0.011881585232913494 Validation loss 0.020850295200943947 Accuracy 0.783203125\n",
      "Iteration 82020 Training loss 0.013169570825994015 Validation loss 0.02076462283730507 Accuracy 0.7841796875\n",
      "Iteration 82030 Training loss 0.012349044904112816 Validation loss 0.021372953429818153 Accuracy 0.77978515625\n",
      "Iteration 82040 Training loss 0.013598212972283363 Validation loss 0.02108312025666237 Accuracy 0.78125\n",
      "Iteration 82050 Training loss 0.014009099453687668 Validation loss 0.021000614389777184 Accuracy 0.7822265625\n",
      "Iteration 82060 Training loss 0.013711445964872837 Validation loss 0.0208535548299551 Accuracy 0.78369140625\n",
      "Iteration 82070 Training loss 0.013740300200879574 Validation loss 0.020739933475852013 Accuracy 0.78466796875\n",
      "Iteration 82080 Training loss 0.01385659258812666 Validation loss 0.020813534036278725 Accuracy 0.78466796875\n",
      "Iteration 82090 Training loss 0.01439415942877531 Validation loss 0.020979946479201317 Accuracy 0.7822265625\n",
      "Iteration 82100 Training loss 0.013938991352915764 Validation loss 0.02102712169289589 Accuracy 0.78173828125\n",
      "Iteration 82110 Training loss 0.014779241755604744 Validation loss 0.020955612882971764 Accuracy 0.78271484375\n",
      "Iteration 82120 Training loss 0.013724258169531822 Validation loss 0.020886166021227837 Accuracy 0.78271484375\n",
      "Iteration 82130 Training loss 0.015506700612604618 Validation loss 0.021186502650380135 Accuracy 0.7802734375\n",
      "Iteration 82140 Training loss 0.01354700606316328 Validation loss 0.02089722640812397 Accuracy 0.78369140625\n",
      "Iteration 82150 Training loss 0.013903653249144554 Validation loss 0.02086671255528927 Accuracy 0.78369140625\n",
      "Iteration 82160 Training loss 0.013752797618508339 Validation loss 0.021013343706727028 Accuracy 0.78271484375\n",
      "Iteration 82170 Training loss 0.01220039650797844 Validation loss 0.02113657258450985 Accuracy 0.78076171875\n",
      "Iteration 82180 Training loss 0.014596831984817982 Validation loss 0.020951181650161743 Accuracy 0.78173828125\n",
      "Iteration 82190 Training loss 0.012727838940918446 Validation loss 0.02092244103550911 Accuracy 0.7822265625\n",
      "Iteration 82200 Training loss 0.015094953589141369 Validation loss 0.021139906719326973 Accuracy 0.7802734375\n",
      "Iteration 82210 Training loss 0.012018187902867794 Validation loss 0.02105972357094288 Accuracy 0.78076171875\n",
      "Iteration 82220 Training loss 0.012681620195508003 Validation loss 0.020994648337364197 Accuracy 0.7822265625\n",
      "Iteration 82230 Training loss 0.01608322001993656 Validation loss 0.02121703512966633 Accuracy 0.78076171875\n",
      "Iteration 82240 Training loss 0.014107144437730312 Validation loss 0.02113293670117855 Accuracy 0.78125\n",
      "Iteration 82250 Training loss 0.012866097502410412 Validation loss 0.0209438968449831 Accuracy 0.78271484375\n",
      "Iteration 82260 Training loss 0.013537625782191753 Validation loss 0.020817836746573448 Accuracy 0.78271484375\n",
      "Iteration 82270 Training loss 0.01408456638455391 Validation loss 0.020862402394413948 Accuracy 0.78271484375\n",
      "Iteration 82280 Training loss 0.012533971108496189 Validation loss 0.020716756582260132 Accuracy 0.78466796875\n",
      "Iteration 82290 Training loss 0.015504084527492523 Validation loss 0.020769918337464333 Accuracy 0.7841796875\n",
      "Iteration 82300 Training loss 0.013707606121897697 Validation loss 0.02093905210494995 Accuracy 0.7822265625\n",
      "Iteration 82310 Training loss 0.014195317402482033 Validation loss 0.02081947959959507 Accuracy 0.78271484375\n",
      "Iteration 82320 Training loss 0.013989654369652271 Validation loss 0.021158648654818535 Accuracy 0.78125\n",
      "Iteration 82330 Training loss 0.014281349256634712 Validation loss 0.02076871320605278 Accuracy 0.7841796875\n",
      "Iteration 82340 Training loss 0.013424532487988472 Validation loss 0.020992817357182503 Accuracy 0.7822265625\n",
      "Iteration 82350 Training loss 0.012085264548659325 Validation loss 0.020895585417747498 Accuracy 0.78271484375\n",
      "Iteration 82360 Training loss 0.014211485162377357 Validation loss 0.02085287496447563 Accuracy 0.7841796875\n",
      "Iteration 82370 Training loss 0.011929272674024105 Validation loss 0.020851194858551025 Accuracy 0.78271484375\n",
      "Iteration 82380 Training loss 0.012932687997817993 Validation loss 0.020760217681527138 Accuracy 0.7841796875\n",
      "Iteration 82390 Training loss 0.013444970361888409 Validation loss 0.020924970507621765 Accuracy 0.783203125\n",
      "Iteration 82400 Training loss 0.011967061087489128 Validation loss 0.020815957337617874 Accuracy 0.78466796875\n",
      "Iteration 82410 Training loss 0.01342692133039236 Validation loss 0.020810607820749283 Accuracy 0.7841796875\n",
      "Iteration 82420 Training loss 0.012926515191793442 Validation loss 0.021088017150759697 Accuracy 0.78173828125\n",
      "Iteration 82430 Training loss 0.013987747021019459 Validation loss 0.020882053300738335 Accuracy 0.78369140625\n",
      "Iteration 82440 Training loss 0.015287241898477077 Validation loss 0.021030232310295105 Accuracy 0.78173828125\n",
      "Iteration 82450 Training loss 0.011977937072515488 Validation loss 0.020881270989775658 Accuracy 0.783203125\n",
      "Iteration 82460 Training loss 0.014776214025914669 Validation loss 0.020788541063666344 Accuracy 0.78369140625\n",
      "Iteration 82470 Training loss 0.013827836140990257 Validation loss 0.021219633519649506 Accuracy 0.77978515625\n",
      "Iteration 82480 Training loss 0.012837893329560757 Validation loss 0.02102837897837162 Accuracy 0.78271484375\n",
      "Iteration 82490 Training loss 0.012176276184618473 Validation loss 0.021461253985762596 Accuracy 0.7763671875\n",
      "Iteration 82500 Training loss 0.013534522615373135 Validation loss 0.020990639925003052 Accuracy 0.7822265625\n",
      "Iteration 82510 Training loss 0.012296684086322784 Validation loss 0.020809542387723923 Accuracy 0.78369140625\n",
      "Iteration 82520 Training loss 0.014902041293680668 Validation loss 0.021197479218244553 Accuracy 0.7802734375\n",
      "Iteration 82530 Training loss 0.01660451851785183 Validation loss 0.021436650305986404 Accuracy 0.77783203125\n",
      "Iteration 82540 Training loss 0.015806330367922783 Validation loss 0.021039536222815514 Accuracy 0.7822265625\n",
      "Iteration 82550 Training loss 0.014513758011162281 Validation loss 0.021013740450143814 Accuracy 0.78173828125\n",
      "Iteration 82560 Training loss 0.013851065188646317 Validation loss 0.02091114968061447 Accuracy 0.783203125\n",
      "Iteration 82570 Training loss 0.016460580751299858 Validation loss 0.020884687080979347 Accuracy 0.783203125\n",
      "Iteration 82580 Training loss 0.013867358677089214 Validation loss 0.020868094637989998 Accuracy 0.78271484375\n",
      "Iteration 82590 Training loss 0.010998791083693504 Validation loss 0.02085844986140728 Accuracy 0.783203125\n",
      "Iteration 82600 Training loss 0.0158471018075943 Validation loss 0.020989123731851578 Accuracy 0.78173828125\n",
      "Iteration 82610 Training loss 0.01149655506014824 Validation loss 0.020846044644713402 Accuracy 0.783203125\n",
      "Iteration 82620 Training loss 0.010300875641405582 Validation loss 0.02113703265786171 Accuracy 0.78173828125\n",
      "Iteration 82630 Training loss 0.012045860290527344 Validation loss 0.02089015021920204 Accuracy 0.7822265625\n",
      "Iteration 82640 Training loss 0.01518198661506176 Validation loss 0.021023739129304886 Accuracy 0.7822265625\n",
      "Iteration 82650 Training loss 0.015141728334128857 Validation loss 0.021517856046557426 Accuracy 0.7783203125\n",
      "Iteration 82660 Training loss 0.012009254656732082 Validation loss 0.02088147960603237 Accuracy 0.78271484375\n",
      "Iteration 82670 Training loss 0.013006450608372688 Validation loss 0.02087906002998352 Accuracy 0.78271484375\n",
      "Iteration 82680 Training loss 0.013240940868854523 Validation loss 0.0208436269313097 Accuracy 0.78369140625\n",
      "Iteration 82690 Training loss 0.01172972097992897 Validation loss 0.02092725969851017 Accuracy 0.783203125\n",
      "Iteration 82700 Training loss 0.01078749168664217 Validation loss 0.02083553746342659 Accuracy 0.783203125\n",
      "Iteration 82710 Training loss 0.012873836793005466 Validation loss 0.020934296771883965 Accuracy 0.78271484375\n",
      "Iteration 82720 Training loss 0.013752156868577003 Validation loss 0.020909925922751427 Accuracy 0.78271484375\n",
      "Iteration 82730 Training loss 0.012847037054598331 Validation loss 0.020898301154375076 Accuracy 0.783203125\n",
      "Iteration 82740 Training loss 0.013186168856918812 Validation loss 0.021150633692741394 Accuracy 0.7802734375\n",
      "Iteration 82750 Training loss 0.012762438505887985 Validation loss 0.020869696512818336 Accuracy 0.78173828125\n",
      "Iteration 82760 Training loss 0.01123045850545168 Validation loss 0.02097674272954464 Accuracy 0.78271484375\n",
      "Iteration 82770 Training loss 0.01176286581903696 Validation loss 0.02079140394926071 Accuracy 0.783203125\n",
      "Iteration 82780 Training loss 0.01539935264736414 Validation loss 0.020977573469281197 Accuracy 0.7822265625\n",
      "Iteration 82790 Training loss 0.012464934028685093 Validation loss 0.02085932157933712 Accuracy 0.78369140625\n",
      "Iteration 82800 Training loss 0.012029359117150307 Validation loss 0.021108224987983704 Accuracy 0.78076171875\n",
      "Iteration 82810 Training loss 0.01223737746477127 Validation loss 0.02087903767824173 Accuracy 0.78369140625\n",
      "Iteration 82820 Training loss 0.013318361714482307 Validation loss 0.02095692791044712 Accuracy 0.7822265625\n",
      "Iteration 82830 Training loss 0.01248500682413578 Validation loss 0.02099679224193096 Accuracy 0.783203125\n",
      "Iteration 82840 Training loss 0.012163551524281502 Validation loss 0.02103411965072155 Accuracy 0.7822265625\n",
      "Iteration 82850 Training loss 0.013682808727025986 Validation loss 0.020903510972857475 Accuracy 0.78271484375\n",
      "Iteration 82860 Training loss 0.01395783107727766 Validation loss 0.020963497459888458 Accuracy 0.7822265625\n",
      "Iteration 82870 Training loss 0.013098683208227158 Validation loss 0.020970897749066353 Accuracy 0.78173828125\n",
      "Iteration 82880 Training loss 0.01373097114264965 Validation loss 0.020757775753736496 Accuracy 0.783203125\n",
      "Iteration 82890 Training loss 0.013762237504124641 Validation loss 0.020943814888596535 Accuracy 0.7822265625\n",
      "Iteration 82900 Training loss 0.015703849494457245 Validation loss 0.021295292302966118 Accuracy 0.7802734375\n",
      "Iteration 82910 Training loss 0.014298450201749802 Validation loss 0.02078920416533947 Accuracy 0.783203125\n",
      "Iteration 82920 Training loss 0.012535384856164455 Validation loss 0.02087198942899704 Accuracy 0.7841796875\n",
      "Iteration 82930 Training loss 0.01256517507135868 Validation loss 0.020877119153738022 Accuracy 0.78369140625\n",
      "Iteration 82940 Training loss 0.013704621233046055 Validation loss 0.02088613249361515 Accuracy 0.78271484375\n",
      "Iteration 82950 Training loss 0.014090130105614662 Validation loss 0.021110931411385536 Accuracy 0.78076171875\n",
      "Iteration 82960 Training loss 0.011985286138951778 Validation loss 0.02095223031938076 Accuracy 0.78271484375\n",
      "Iteration 82970 Training loss 0.014012070372700691 Validation loss 0.020969051867723465 Accuracy 0.7822265625\n",
      "Iteration 82980 Training loss 0.01563146896660328 Validation loss 0.02097419463098049 Accuracy 0.78173828125\n",
      "Iteration 82990 Training loss 0.011183331720530987 Validation loss 0.020797351375222206 Accuracy 0.78369140625\n",
      "Iteration 83000 Training loss 0.013144513592123985 Validation loss 0.02092793956398964 Accuracy 0.78173828125\n",
      "Iteration 83010 Training loss 0.016143640503287315 Validation loss 0.020833812654018402 Accuracy 0.78369140625\n",
      "Iteration 83020 Training loss 0.012253074906766415 Validation loss 0.02082674764096737 Accuracy 0.78369140625\n",
      "Iteration 83030 Training loss 0.012401783838868141 Validation loss 0.021115966141223907 Accuracy 0.7802734375\n",
      "Iteration 83040 Training loss 0.0150099266320467 Validation loss 0.020833002403378487 Accuracy 0.78369140625\n",
      "Iteration 83050 Training loss 0.013192827813327312 Validation loss 0.02100895158946514 Accuracy 0.78125\n",
      "Iteration 83060 Training loss 0.01184570137411356 Validation loss 0.020958472043275833 Accuracy 0.78173828125\n",
      "Iteration 83070 Training loss 0.014751080423593521 Validation loss 0.020826278254389763 Accuracy 0.783203125\n",
      "Iteration 83080 Training loss 0.014657091349363327 Validation loss 0.02089676260948181 Accuracy 0.783203125\n",
      "Iteration 83090 Training loss 0.013947243802249432 Validation loss 0.020764898508787155 Accuracy 0.7841796875\n",
      "Iteration 83100 Training loss 0.012506279163062572 Validation loss 0.0210056584328413 Accuracy 0.78125\n",
      "Iteration 83110 Training loss 0.013144047930836678 Validation loss 0.020858846604824066 Accuracy 0.78271484375\n",
      "Iteration 83120 Training loss 0.013633196242153645 Validation loss 0.020762305706739426 Accuracy 0.7841796875\n",
      "Iteration 83130 Training loss 0.0128184724599123 Validation loss 0.02085437998175621 Accuracy 0.78271484375\n",
      "Iteration 83140 Training loss 0.013902926817536354 Validation loss 0.02076512947678566 Accuracy 0.78369140625\n",
      "Iteration 83150 Training loss 0.012802448123693466 Validation loss 0.02065720409154892 Accuracy 0.78515625\n",
      "Iteration 83160 Training loss 0.014080515131354332 Validation loss 0.020838627591729164 Accuracy 0.78369140625\n",
      "Iteration 83170 Training loss 0.015497754327952862 Validation loss 0.020754961296916008 Accuracy 0.78271484375\n",
      "Iteration 83180 Training loss 0.0128104193136096 Validation loss 0.020883304998278618 Accuracy 0.78173828125\n",
      "Iteration 83190 Training loss 0.015340397134423256 Validation loss 0.020706908777356148 Accuracy 0.78466796875\n",
      "Iteration 83200 Training loss 0.014973819255828857 Validation loss 0.020913343876600266 Accuracy 0.78271484375\n",
      "Iteration 83210 Training loss 0.01327954325824976 Validation loss 0.02085372805595398 Accuracy 0.78369140625\n",
      "Iteration 83220 Training loss 0.014060918241739273 Validation loss 0.02092932164669037 Accuracy 0.783203125\n",
      "Iteration 83230 Training loss 0.013810805976390839 Validation loss 0.02124498412013054 Accuracy 0.7802734375\n",
      "Iteration 83240 Training loss 0.01512149814516306 Validation loss 0.020817508921027184 Accuracy 0.7841796875\n",
      "Iteration 83250 Training loss 0.015273645520210266 Validation loss 0.021154191344976425 Accuracy 0.78173828125\n",
      "Iteration 83260 Training loss 0.012132398784160614 Validation loss 0.021006623283028603 Accuracy 0.7822265625\n",
      "Iteration 83270 Training loss 0.01535231526941061 Validation loss 0.020868539810180664 Accuracy 0.78271484375\n",
      "Iteration 83280 Training loss 0.011198948137462139 Validation loss 0.02123848907649517 Accuracy 0.78076171875\n",
      "Iteration 83290 Training loss 0.011718785390257835 Validation loss 0.02084188163280487 Accuracy 0.78369140625\n",
      "Iteration 83300 Training loss 0.01600346900522709 Validation loss 0.020945604890584946 Accuracy 0.7822265625\n",
      "Iteration 83310 Training loss 0.015957728028297424 Validation loss 0.020796621218323708 Accuracy 0.78369140625\n",
      "Iteration 83320 Training loss 0.014728362672030926 Validation loss 0.02083875611424446 Accuracy 0.78369140625\n",
      "Iteration 83330 Training loss 0.014377939514815807 Validation loss 0.02104276604950428 Accuracy 0.78125\n",
      "Iteration 83340 Training loss 0.012303591705858707 Validation loss 0.020814932882785797 Accuracy 0.78271484375\n",
      "Iteration 83350 Training loss 0.011943974532186985 Validation loss 0.02111169882118702 Accuracy 0.78125\n",
      "Iteration 83360 Training loss 0.012260892428457737 Validation loss 0.020924042910337448 Accuracy 0.783203125\n",
      "Iteration 83370 Training loss 0.014617308974266052 Validation loss 0.02123349718749523 Accuracy 0.7802734375\n",
      "Iteration 83380 Training loss 0.013062477111816406 Validation loss 0.02073313109576702 Accuracy 0.78466796875\n",
      "Iteration 83390 Training loss 0.013013000600039959 Validation loss 0.02089102193713188 Accuracy 0.78271484375\n",
      "Iteration 83400 Training loss 0.015365357510745525 Validation loss 0.021009910851716995 Accuracy 0.78173828125\n",
      "Iteration 83410 Training loss 0.014801841229200363 Validation loss 0.021199462935328484 Accuracy 0.7802734375\n",
      "Iteration 83420 Training loss 0.014361109584569931 Validation loss 0.02089604176580906 Accuracy 0.7841796875\n",
      "Iteration 83430 Training loss 0.012228098697960377 Validation loss 0.020947914570569992 Accuracy 0.78173828125\n",
      "Iteration 83440 Training loss 0.009147604927420616 Validation loss 0.020950859412550926 Accuracy 0.7822265625\n",
      "Iteration 83450 Training loss 0.013880521059036255 Validation loss 0.021237986162304878 Accuracy 0.77978515625\n",
      "Iteration 83460 Training loss 0.013078177347779274 Validation loss 0.021211406216025352 Accuracy 0.77978515625\n",
      "Iteration 83470 Training loss 0.011885515414178371 Validation loss 0.020988764241337776 Accuracy 0.78271484375\n",
      "Iteration 83480 Training loss 0.014934287406504154 Validation loss 0.020892983302474022 Accuracy 0.78271484375\n",
      "Iteration 83490 Training loss 0.013542503118515015 Validation loss 0.02115809917449951 Accuracy 0.78125\n",
      "Iteration 83500 Training loss 0.013770163990557194 Validation loss 0.021050671115517616 Accuracy 0.7822265625\n",
      "Iteration 83510 Training loss 0.012112746946513653 Validation loss 0.020911263301968575 Accuracy 0.78271484375\n",
      "Iteration 83520 Training loss 0.010816323570907116 Validation loss 0.020656149834394455 Accuracy 0.78564453125\n",
      "Iteration 83530 Training loss 0.012261937372386456 Validation loss 0.020848553627729416 Accuracy 0.78369140625\n",
      "Iteration 83540 Training loss 0.011566365137696266 Validation loss 0.020840873941779137 Accuracy 0.78369140625\n",
      "Iteration 83550 Training loss 0.013349349610507488 Validation loss 0.020970337092876434 Accuracy 0.783203125\n",
      "Iteration 83560 Training loss 0.012355835177004337 Validation loss 0.02078932523727417 Accuracy 0.78369140625\n",
      "Iteration 83570 Training loss 0.01266608014702797 Validation loss 0.020826643332839012 Accuracy 0.78369140625\n",
      "Iteration 83580 Training loss 0.010889689438045025 Validation loss 0.020956214517354965 Accuracy 0.78271484375\n",
      "Iteration 83590 Training loss 0.013085361570119858 Validation loss 0.02090642973780632 Accuracy 0.783203125\n",
      "Iteration 83600 Training loss 0.011724508367478848 Validation loss 0.02079862728714943 Accuracy 0.783203125\n",
      "Iteration 83610 Training loss 0.012813224457204342 Validation loss 0.020718663930892944 Accuracy 0.7841796875\n",
      "Iteration 83620 Training loss 0.013769025914371014 Validation loss 0.020904595032334328 Accuracy 0.78271484375\n",
      "Iteration 83630 Training loss 0.012214330956339836 Validation loss 0.02087314799427986 Accuracy 0.783203125\n",
      "Iteration 83640 Training loss 0.011842708103358746 Validation loss 0.02095409668982029 Accuracy 0.78173828125\n",
      "Iteration 83650 Training loss 0.012655219994485378 Validation loss 0.020691053941845894 Accuracy 0.78466796875\n",
      "Iteration 83660 Training loss 0.013677960261702538 Validation loss 0.020909305661916733 Accuracy 0.7822265625\n",
      "Iteration 83670 Training loss 0.014484715647995472 Validation loss 0.020861832424998283 Accuracy 0.783203125\n",
      "Iteration 83680 Training loss 0.012714340351521969 Validation loss 0.020832665264606476 Accuracy 0.783203125\n",
      "Iteration 83690 Training loss 0.014772924594581127 Validation loss 0.021146493032574654 Accuracy 0.78173828125\n",
      "Iteration 83700 Training loss 0.013103710487484932 Validation loss 0.02078850567340851 Accuracy 0.7841796875\n",
      "Iteration 83710 Training loss 0.013106361031532288 Validation loss 0.020780419930815697 Accuracy 0.78369140625\n",
      "Iteration 83720 Training loss 0.014277370646595955 Validation loss 0.020723124966025352 Accuracy 0.78564453125\n",
      "Iteration 83730 Training loss 0.013384704478085041 Validation loss 0.020999984815716743 Accuracy 0.78173828125\n",
      "Iteration 83740 Training loss 0.013566560111939907 Validation loss 0.021002836525440216 Accuracy 0.7822265625\n",
      "Iteration 83750 Training loss 0.01625826023519039 Validation loss 0.020692594349384308 Accuracy 0.78564453125\n",
      "Iteration 83760 Training loss 0.01089886762201786 Validation loss 0.0207015722990036 Accuracy 0.78515625\n",
      "Iteration 83770 Training loss 0.014405954629182816 Validation loss 0.020668882876634598 Accuracy 0.78564453125\n",
      "Iteration 83780 Training loss 0.011805746704339981 Validation loss 0.021022716537117958 Accuracy 0.78369140625\n",
      "Iteration 83790 Training loss 0.014449113979935646 Validation loss 0.02097930572926998 Accuracy 0.78271484375\n",
      "Iteration 83800 Training loss 0.01479105744510889 Validation loss 0.021060485392808914 Accuracy 0.78173828125\n",
      "Iteration 83810 Training loss 0.013400188647210598 Validation loss 0.02086464874446392 Accuracy 0.7841796875\n",
      "Iteration 83820 Training loss 0.013764414004981518 Validation loss 0.020778538659214973 Accuracy 0.78369140625\n",
      "Iteration 83830 Training loss 0.016974372789263725 Validation loss 0.020830439403653145 Accuracy 0.78271484375\n",
      "Iteration 83840 Training loss 0.013699047267436981 Validation loss 0.020801877602934837 Accuracy 0.78369140625\n",
      "Iteration 83850 Training loss 0.012897154316306114 Validation loss 0.02092115208506584 Accuracy 0.783203125\n",
      "Iteration 83860 Training loss 0.013932415284216404 Validation loss 0.020878944545984268 Accuracy 0.78271484375\n",
      "Iteration 83870 Training loss 0.01143200695514679 Validation loss 0.02078678086400032 Accuracy 0.78369140625\n",
      "Iteration 83880 Training loss 0.01438357774168253 Validation loss 0.020878514274954796 Accuracy 0.78369140625\n",
      "Iteration 83890 Training loss 0.013495760038495064 Validation loss 0.021078649908304214 Accuracy 0.78125\n",
      "Iteration 83900 Training loss 0.011943403631448746 Validation loss 0.020689887925982475 Accuracy 0.78515625\n",
      "Iteration 83910 Training loss 0.014464202336966991 Validation loss 0.020863249897956848 Accuracy 0.78369140625\n",
      "Iteration 83920 Training loss 0.01458018459379673 Validation loss 0.02087920345366001 Accuracy 0.783203125\n",
      "Iteration 83930 Training loss 0.01135264802724123 Validation loss 0.020834503695368767 Accuracy 0.7841796875\n",
      "Iteration 83940 Training loss 0.0125153549015522 Validation loss 0.02074890397489071 Accuracy 0.7841796875\n",
      "Iteration 83950 Training loss 0.010757185518741608 Validation loss 0.02106737531721592 Accuracy 0.7822265625\n",
      "Iteration 83960 Training loss 0.01569138653576374 Validation loss 0.02077464386820793 Accuracy 0.78369140625\n",
      "Iteration 83970 Training loss 0.01269424706697464 Validation loss 0.02096630074083805 Accuracy 0.7822265625\n",
      "Iteration 83980 Training loss 0.019777247682213783 Validation loss 0.023201527073979378 Accuracy 0.75927734375\n",
      "Iteration 83990 Training loss 0.012637963518500328 Validation loss 0.020886151120066643 Accuracy 0.78369140625\n",
      "Iteration 84000 Training loss 0.01543966680765152 Validation loss 0.020921994000673294 Accuracy 0.7822265625\n",
      "Iteration 84010 Training loss 0.014047002419829369 Validation loss 0.02083985134959221 Accuracy 0.78369140625\n",
      "Iteration 84020 Training loss 0.01158722210675478 Validation loss 0.020861290395259857 Accuracy 0.78466796875\n",
      "Iteration 84030 Training loss 0.014691337011754513 Validation loss 0.020575590431690216 Accuracy 0.78662109375\n",
      "Iteration 84040 Training loss 0.013143873773515224 Validation loss 0.020729994401335716 Accuracy 0.78515625\n",
      "Iteration 84050 Training loss 0.013853968121111393 Validation loss 0.020741142332553864 Accuracy 0.78369140625\n",
      "Iteration 84060 Training loss 0.014522613026201725 Validation loss 0.02091478370130062 Accuracy 0.78369140625\n",
      "Iteration 84070 Training loss 0.012430086731910706 Validation loss 0.02099432609975338 Accuracy 0.78173828125\n",
      "Iteration 84080 Training loss 0.014891576021909714 Validation loss 0.02103814110159874 Accuracy 0.7822265625\n",
      "Iteration 84090 Training loss 0.012344454415142536 Validation loss 0.020775439217686653 Accuracy 0.78369140625\n",
      "Iteration 84100 Training loss 0.012616049498319626 Validation loss 0.020829681307077408 Accuracy 0.78369140625\n",
      "Iteration 84110 Training loss 0.014426377601921558 Validation loss 0.02085014618933201 Accuracy 0.78271484375\n",
      "Iteration 84120 Training loss 0.015219828113913536 Validation loss 0.02080780267715454 Accuracy 0.7841796875\n",
      "Iteration 84130 Training loss 0.012139237485826015 Validation loss 0.020906396210193634 Accuracy 0.783203125\n",
      "Iteration 84140 Training loss 0.013049020431935787 Validation loss 0.020959055051207542 Accuracy 0.78271484375\n",
      "Iteration 84150 Training loss 0.014429349452257156 Validation loss 0.020969733595848083 Accuracy 0.7822265625\n",
      "Iteration 84160 Training loss 0.01447264850139618 Validation loss 0.020894329994916916 Accuracy 0.7822265625\n",
      "Iteration 84170 Training loss 0.012969368137419224 Validation loss 0.020803935825824738 Accuracy 0.783203125\n",
      "Iteration 84180 Training loss 0.013490770943462849 Validation loss 0.020868368446826935 Accuracy 0.7822265625\n",
      "Iteration 84190 Training loss 0.010581263341009617 Validation loss 0.020879942923784256 Accuracy 0.78173828125\n",
      "Iteration 84200 Training loss 0.012345421127974987 Validation loss 0.021071797236800194 Accuracy 0.78173828125\n",
      "Iteration 84210 Training loss 0.01287120208144188 Validation loss 0.020886395126581192 Accuracy 0.783203125\n",
      "Iteration 84220 Training loss 0.01277912873774767 Validation loss 0.021146349608898163 Accuracy 0.7802734375\n",
      "Iteration 84230 Training loss 0.014747700653970242 Validation loss 0.020926369354128838 Accuracy 0.783203125\n",
      "Iteration 84240 Training loss 0.012579668313264847 Validation loss 0.020934870466589928 Accuracy 0.783203125\n",
      "Iteration 84250 Training loss 0.011766774579882622 Validation loss 0.02080199122428894 Accuracy 0.783203125\n",
      "Iteration 84260 Training loss 0.013767814263701439 Validation loss 0.021020852029323578 Accuracy 0.78076171875\n",
      "Iteration 84270 Training loss 0.013351415283977985 Validation loss 0.020856346935033798 Accuracy 0.7822265625\n",
      "Iteration 84280 Training loss 0.013944389298558235 Validation loss 0.020785078406333923 Accuracy 0.783203125\n",
      "Iteration 84290 Training loss 0.012208343483507633 Validation loss 0.020786918699741364 Accuracy 0.783203125\n",
      "Iteration 84300 Training loss 0.012749656103551388 Validation loss 0.020991653203964233 Accuracy 0.7802734375\n",
      "Iteration 84310 Training loss 0.012546994723379612 Validation loss 0.02088993787765503 Accuracy 0.7822265625\n",
      "Iteration 84320 Training loss 0.013899772427976131 Validation loss 0.02088538557291031 Accuracy 0.783203125\n",
      "Iteration 84330 Training loss 0.01434269454330206 Validation loss 0.020741287618875504 Accuracy 0.78369140625\n",
      "Iteration 84340 Training loss 0.013718333095312119 Validation loss 0.020771728828549385 Accuracy 0.7841796875\n",
      "Iteration 84350 Training loss 0.0143529511988163 Validation loss 0.020915592089295387 Accuracy 0.78173828125\n",
      "Iteration 84360 Training loss 0.013910556212067604 Validation loss 0.020812850445508957 Accuracy 0.78369140625\n",
      "Iteration 84370 Training loss 0.013282297179102898 Validation loss 0.020790856331586838 Accuracy 0.78369140625\n",
      "Iteration 84380 Training loss 0.01272920984774828 Validation loss 0.02086419053375721 Accuracy 0.78271484375\n",
      "Iteration 84390 Training loss 0.013707851059734821 Validation loss 0.02086472511291504 Accuracy 0.78369140625\n",
      "Iteration 84400 Training loss 0.014614474028348923 Validation loss 0.02082236297428608 Accuracy 0.78271484375\n",
      "Iteration 84410 Training loss 0.014367570169270039 Validation loss 0.02112620323896408 Accuracy 0.78173828125\n",
      "Iteration 84420 Training loss 0.017386043444275856 Validation loss 0.020998632535338402 Accuracy 0.783203125\n",
      "Iteration 84430 Training loss 0.013664771802723408 Validation loss 0.020756369456648827 Accuracy 0.78369140625\n",
      "Iteration 84440 Training loss 0.01250429730862379 Validation loss 0.021267548203468323 Accuracy 0.77978515625\n",
      "Iteration 84450 Training loss 0.013868527486920357 Validation loss 0.0207517109811306 Accuracy 0.78369140625\n",
      "Iteration 84460 Training loss 0.013579757884144783 Validation loss 0.02080981433391571 Accuracy 0.783203125\n",
      "Iteration 84470 Training loss 0.01252774242311716 Validation loss 0.020961180329322815 Accuracy 0.78271484375\n",
      "Iteration 84480 Training loss 0.014187975786626339 Validation loss 0.020850658416748047 Accuracy 0.78369140625\n",
      "Iteration 84490 Training loss 0.012996088713407516 Validation loss 0.020940160378813744 Accuracy 0.783203125\n",
      "Iteration 84500 Training loss 0.014454256743192673 Validation loss 0.020976319909095764 Accuracy 0.78173828125\n",
      "Iteration 84510 Training loss 0.013959955424070358 Validation loss 0.020824730396270752 Accuracy 0.78466796875\n",
      "Iteration 84520 Training loss 0.014352357015013695 Validation loss 0.02087332494556904 Accuracy 0.78271484375\n",
      "Iteration 84530 Training loss 0.012470858171582222 Validation loss 0.021094903349876404 Accuracy 0.77978515625\n",
      "Iteration 84540 Training loss 0.01211267989128828 Validation loss 0.02093055658042431 Accuracy 0.78271484375\n",
      "Iteration 84550 Training loss 0.014322339557111263 Validation loss 0.02095140516757965 Accuracy 0.7822265625\n",
      "Iteration 84560 Training loss 0.012985270470380783 Validation loss 0.020897772163152695 Accuracy 0.78271484375\n",
      "Iteration 84570 Training loss 0.013563702814280987 Validation loss 0.021034998819231987 Accuracy 0.78125\n",
      "Iteration 84580 Training loss 0.0112118199467659 Validation loss 0.02097010239958763 Accuracy 0.78271484375\n",
      "Iteration 84590 Training loss 0.01038987934589386 Validation loss 0.02089741639792919 Accuracy 0.783203125\n",
      "Iteration 84600 Training loss 0.0102616548538208 Validation loss 0.021328717470169067 Accuracy 0.779296875\n",
      "Iteration 84610 Training loss 0.012491609901189804 Validation loss 0.021011341363191605 Accuracy 0.7822265625\n",
      "Iteration 84620 Training loss 0.012327583506703377 Validation loss 0.020880073308944702 Accuracy 0.783203125\n",
      "Iteration 84630 Training loss 0.013870653696358204 Validation loss 0.020888999104499817 Accuracy 0.783203125\n",
      "Iteration 84640 Training loss 0.013037637807428837 Validation loss 0.021013682708144188 Accuracy 0.78173828125\n",
      "Iteration 84650 Training loss 0.01214215625077486 Validation loss 0.02087405137717724 Accuracy 0.783203125\n",
      "Iteration 84660 Training loss 0.01333567127585411 Validation loss 0.021115131676197052 Accuracy 0.78173828125\n",
      "Iteration 84670 Training loss 0.013130824081599712 Validation loss 0.020988373085856438 Accuracy 0.7822265625\n",
      "Iteration 84680 Training loss 0.012101694941520691 Validation loss 0.02075689472258091 Accuracy 0.7841796875\n",
      "Iteration 84690 Training loss 0.014202218502759933 Validation loss 0.02089841477572918 Accuracy 0.78271484375\n",
      "Iteration 84700 Training loss 0.01562766544520855 Validation loss 0.02087397687137127 Accuracy 0.7822265625\n",
      "Iteration 84710 Training loss 0.014724784530699253 Validation loss 0.02103627473115921 Accuracy 0.78125\n",
      "Iteration 84720 Training loss 0.012047304771840572 Validation loss 0.020940151065587997 Accuracy 0.78125\n",
      "Iteration 84730 Training loss 0.012606557458639145 Validation loss 0.02103934809565544 Accuracy 0.7802734375\n",
      "Iteration 84740 Training loss 0.013449953868985176 Validation loss 0.021157607436180115 Accuracy 0.78076171875\n",
      "Iteration 84750 Training loss 0.014219959266483784 Validation loss 0.02095206268131733 Accuracy 0.78271484375\n",
      "Iteration 84760 Training loss 0.012963859364390373 Validation loss 0.020906155928969383 Accuracy 0.78271484375\n",
      "Iteration 84770 Training loss 0.01348431408405304 Validation loss 0.020809784531593323 Accuracy 0.78369140625\n",
      "Iteration 84780 Training loss 0.010553586296737194 Validation loss 0.020909398794174194 Accuracy 0.7822265625\n",
      "Iteration 84790 Training loss 0.015139531344175339 Validation loss 0.02106446772813797 Accuracy 0.78173828125\n",
      "Iteration 84800 Training loss 0.01401172112673521 Validation loss 0.020892899483442307 Accuracy 0.7822265625\n",
      "Iteration 84810 Training loss 0.014928355813026428 Validation loss 0.020925074815750122 Accuracy 0.78271484375\n",
      "Iteration 84820 Training loss 0.014155839569866657 Validation loss 0.02090543322265148 Accuracy 0.78271484375\n",
      "Iteration 84830 Training loss 0.014606865122914314 Validation loss 0.02086596190929413 Accuracy 0.78369140625\n",
      "Iteration 84840 Training loss 0.015032131224870682 Validation loss 0.020840127021074295 Accuracy 0.78369140625\n",
      "Iteration 84850 Training loss 0.012581028044223785 Validation loss 0.020798034965991974 Accuracy 0.783203125\n",
      "Iteration 84860 Training loss 0.013665692880749702 Validation loss 0.020939340814948082 Accuracy 0.78271484375\n",
      "Iteration 84870 Training loss 0.014611487276852131 Validation loss 0.02094394899904728 Accuracy 0.78271484375\n",
      "Iteration 84880 Training loss 0.014977649785578251 Validation loss 0.020913375541567802 Accuracy 0.78173828125\n",
      "Iteration 84890 Training loss 0.014162775129079819 Validation loss 0.02088642120361328 Accuracy 0.78271484375\n",
      "Iteration 84900 Training loss 0.015444905497133732 Validation loss 0.021004071459174156 Accuracy 0.7822265625\n",
      "Iteration 84910 Training loss 0.012588681653141975 Validation loss 0.020981211215257645 Accuracy 0.78173828125\n",
      "Iteration 84920 Training loss 0.011723424308001995 Validation loss 0.021023280918598175 Accuracy 0.78125\n",
      "Iteration 84930 Training loss 0.013370055705308914 Validation loss 0.02096981555223465 Accuracy 0.78271484375\n",
      "Iteration 84940 Training loss 0.013316207565367222 Validation loss 0.021089963614940643 Accuracy 0.78271484375\n",
      "Iteration 84950 Training loss 0.011908533982932568 Validation loss 0.021129626780748367 Accuracy 0.78125\n",
      "Iteration 84960 Training loss 0.014173964969813824 Validation loss 0.021086927503347397 Accuracy 0.78173828125\n",
      "Iteration 84970 Training loss 0.014654605649411678 Validation loss 0.021106723695993423 Accuracy 0.78173828125\n",
      "Iteration 84980 Training loss 0.01142116542905569 Validation loss 0.0209396630525589 Accuracy 0.78271484375\n",
      "Iteration 84990 Training loss 0.013178952038288116 Validation loss 0.021138545125722885 Accuracy 0.78076171875\n",
      "Iteration 85000 Training loss 0.012116891331970692 Validation loss 0.020918646827340126 Accuracy 0.7822265625\n",
      "Iteration 85010 Training loss 0.013082657009363174 Validation loss 0.02085309475660324 Accuracy 0.783203125\n",
      "Iteration 85020 Training loss 0.014161200262606144 Validation loss 0.02077038586139679 Accuracy 0.78369140625\n",
      "Iteration 85030 Training loss 0.014658845029771328 Validation loss 0.02093825675547123 Accuracy 0.7822265625\n",
      "Iteration 85040 Training loss 0.015061678364872932 Validation loss 0.02084771916270256 Accuracy 0.78271484375\n",
      "Iteration 85050 Training loss 0.012450159527361393 Validation loss 0.021190766245126724 Accuracy 0.77978515625\n",
      "Iteration 85060 Training loss 0.015221908688545227 Validation loss 0.021090034395456314 Accuracy 0.78125\n",
      "Iteration 85070 Training loss 0.013534950092434883 Validation loss 0.02097632735967636 Accuracy 0.7822265625\n",
      "Iteration 85080 Training loss 0.011655339039862156 Validation loss 0.0208541601896286 Accuracy 0.78271484375\n",
      "Iteration 85090 Training loss 0.01135715190321207 Validation loss 0.0207877978682518 Accuracy 0.7841796875\n",
      "Iteration 85100 Training loss 0.01570105366408825 Validation loss 0.020847748965024948 Accuracy 0.7841796875\n",
      "Iteration 85110 Training loss 0.013509778305888176 Validation loss 0.020821308717131615 Accuracy 0.78369140625\n",
      "Iteration 85120 Training loss 0.012787195853888988 Validation loss 0.020910488441586494 Accuracy 0.7822265625\n",
      "Iteration 85130 Training loss 0.011225828900933266 Validation loss 0.020787587389349937 Accuracy 0.783203125\n",
      "Iteration 85140 Training loss 0.013450599275529385 Validation loss 0.020836511626839638 Accuracy 0.78271484375\n",
      "Iteration 85150 Training loss 0.015065065585076809 Validation loss 0.02080973982810974 Accuracy 0.7822265625\n",
      "Iteration 85160 Training loss 0.014120398089289665 Validation loss 0.020805902779102325 Accuracy 0.78369140625\n",
      "Iteration 85170 Training loss 0.01320977509021759 Validation loss 0.020928367972373962 Accuracy 0.78076171875\n",
      "Iteration 85180 Training loss 0.014595718123018742 Validation loss 0.020972387865185738 Accuracy 0.7822265625\n",
      "Iteration 85190 Training loss 0.014099095948040485 Validation loss 0.020779628306627274 Accuracy 0.783203125\n",
      "Iteration 85200 Training loss 0.009801208972930908 Validation loss 0.02113673835992813 Accuracy 0.78125\n",
      "Iteration 85210 Training loss 0.014440556988120079 Validation loss 0.02096857875585556 Accuracy 0.78173828125\n",
      "Iteration 85220 Training loss 0.013421788811683655 Validation loss 0.020944632589817047 Accuracy 0.7822265625\n",
      "Iteration 85230 Training loss 0.014158617705106735 Validation loss 0.02068236470222473 Accuracy 0.78466796875\n",
      "Iteration 85240 Training loss 0.012997610494494438 Validation loss 0.020924469456076622 Accuracy 0.7822265625\n",
      "Iteration 85250 Training loss 0.013349238783121109 Validation loss 0.020854152739048004 Accuracy 0.78271484375\n",
      "Iteration 85260 Training loss 0.012057151645421982 Validation loss 0.020866913720965385 Accuracy 0.78369140625\n",
      "Iteration 85270 Training loss 0.01202378049492836 Validation loss 0.020764628425240517 Accuracy 0.78271484375\n",
      "Iteration 85280 Training loss 0.012037033215165138 Validation loss 0.02074313908815384 Accuracy 0.7841796875\n",
      "Iteration 85290 Training loss 0.013796674087643623 Validation loss 0.020755335688591003 Accuracy 0.78466796875\n",
      "Iteration 85300 Training loss 0.013286810368299484 Validation loss 0.020898934453725815 Accuracy 0.78369140625\n",
      "Iteration 85310 Training loss 0.013447219505906105 Validation loss 0.02064904198050499 Accuracy 0.78466796875\n",
      "Iteration 85320 Training loss 0.013221260160207748 Validation loss 0.02103198692202568 Accuracy 0.78271484375\n",
      "Iteration 85330 Training loss 0.013919747434556484 Validation loss 0.02079329453408718 Accuracy 0.7841796875\n",
      "Iteration 85340 Training loss 0.011106248944997787 Validation loss 0.021098734810948372 Accuracy 0.78173828125\n",
      "Iteration 85350 Training loss 0.014812898822128773 Validation loss 0.020902294665575027 Accuracy 0.7822265625\n",
      "Iteration 85360 Training loss 0.013127077370882034 Validation loss 0.021003322675824165 Accuracy 0.78271484375\n",
      "Iteration 85370 Training loss 0.010616821236908436 Validation loss 0.020731355994939804 Accuracy 0.7841796875\n",
      "Iteration 85380 Training loss 0.013402790762484074 Validation loss 0.020908724516630173 Accuracy 0.783203125\n",
      "Iteration 85390 Training loss 0.011467736214399338 Validation loss 0.02078423835337162 Accuracy 0.78369140625\n",
      "Iteration 85400 Training loss 0.013406926766037941 Validation loss 0.020890463143587112 Accuracy 0.78369140625\n",
      "Iteration 85410 Training loss 0.014754298143088818 Validation loss 0.02091284655034542 Accuracy 0.78369140625\n",
      "Iteration 85420 Training loss 0.012247586622834206 Validation loss 0.020909130573272705 Accuracy 0.7822265625\n",
      "Iteration 85430 Training loss 0.01146197970956564 Validation loss 0.020862475037574768 Accuracy 0.78271484375\n",
      "Iteration 85440 Training loss 0.011783350259065628 Validation loss 0.02082926221191883 Accuracy 0.783203125\n",
      "Iteration 85450 Training loss 0.012858662754297256 Validation loss 0.02115779183804989 Accuracy 0.78125\n",
      "Iteration 85460 Training loss 0.013865559361875057 Validation loss 0.020757131278514862 Accuracy 0.783203125\n",
      "Iteration 85470 Training loss 0.015893593430519104 Validation loss 0.020695360377430916 Accuracy 0.78466796875\n",
      "Iteration 85480 Training loss 0.01577327959239483 Validation loss 0.020900294184684753 Accuracy 0.78271484375\n",
      "Iteration 85490 Training loss 0.014044677838683128 Validation loss 0.020971478894352913 Accuracy 0.78173828125\n",
      "Iteration 85500 Training loss 0.01425650529563427 Validation loss 0.020766878500580788 Accuracy 0.78369140625\n",
      "Iteration 85510 Training loss 0.01410799939185381 Validation loss 0.021106278523802757 Accuracy 0.78076171875\n",
      "Iteration 85520 Training loss 0.012129000388085842 Validation loss 0.020841479301452637 Accuracy 0.783203125\n",
      "Iteration 85530 Training loss 0.012678016908466816 Validation loss 0.020992645993828773 Accuracy 0.7822265625\n",
      "Iteration 85540 Training loss 0.013773709535598755 Validation loss 0.020750511437654495 Accuracy 0.78369140625\n",
      "Iteration 85550 Training loss 0.015260453335940838 Validation loss 0.020890047773718834 Accuracy 0.783203125\n",
      "Iteration 85560 Training loss 0.014487681910395622 Validation loss 0.021031592041254044 Accuracy 0.78173828125\n",
      "Iteration 85570 Training loss 0.014997472055256367 Validation loss 0.020805224776268005 Accuracy 0.783203125\n",
      "Iteration 85580 Training loss 0.013603419065475464 Validation loss 0.021134762093424797 Accuracy 0.7802734375\n",
      "Iteration 85590 Training loss 0.01357966847717762 Validation loss 0.020977314561605453 Accuracy 0.78271484375\n",
      "Iteration 85600 Training loss 0.01280520111322403 Validation loss 0.020790331065654755 Accuracy 0.78271484375\n",
      "Iteration 85610 Training loss 0.01026223972439766 Validation loss 0.020796416327357292 Accuracy 0.7822265625\n",
      "Iteration 85620 Training loss 0.013598418794572353 Validation loss 0.020788012072443962 Accuracy 0.783203125\n",
      "Iteration 85630 Training loss 0.010688153095543385 Validation loss 0.02071397192776203 Accuracy 0.78515625\n",
      "Iteration 85640 Training loss 0.014895503409206867 Validation loss 0.020704593509435654 Accuracy 0.78466796875\n",
      "Iteration 85650 Training loss 0.010464678518474102 Validation loss 0.020829442888498306 Accuracy 0.78369140625\n",
      "Iteration 85660 Training loss 0.013729510828852654 Validation loss 0.020874647423624992 Accuracy 0.7841796875\n",
      "Iteration 85670 Training loss 0.011849545873701572 Validation loss 0.02075781673192978 Accuracy 0.7841796875\n",
      "Iteration 85680 Training loss 0.01239734422415495 Validation loss 0.0208482276648283 Accuracy 0.78466796875\n",
      "Iteration 85690 Training loss 0.013153745792806149 Validation loss 0.02091558277606964 Accuracy 0.783203125\n",
      "Iteration 85700 Training loss 0.012470649555325508 Validation loss 0.02093333750963211 Accuracy 0.783203125\n",
      "Iteration 85710 Training loss 0.012525779195129871 Validation loss 0.02078591100871563 Accuracy 0.7841796875\n",
      "Iteration 85720 Training loss 0.015164695680141449 Validation loss 0.020651737228035927 Accuracy 0.7841796875\n",
      "Iteration 85730 Training loss 0.013100474141538143 Validation loss 0.020801033824682236 Accuracy 0.78369140625\n",
      "Iteration 85740 Training loss 0.012208566069602966 Validation loss 0.02074550651013851 Accuracy 0.78369140625\n",
      "Iteration 85750 Training loss 0.011541241779923439 Validation loss 0.020919838920235634 Accuracy 0.78271484375\n",
      "Iteration 85760 Training loss 0.012928897514939308 Validation loss 0.020755112171173096 Accuracy 0.78466796875\n",
      "Iteration 85770 Training loss 0.01394234225153923 Validation loss 0.021116577088832855 Accuracy 0.78125\n",
      "Iteration 85780 Training loss 0.014040789566934109 Validation loss 0.020729925483465195 Accuracy 0.783203125\n",
      "Iteration 85790 Training loss 0.012365084141492844 Validation loss 0.02075672335922718 Accuracy 0.78466796875\n",
      "Iteration 85800 Training loss 0.01377846859395504 Validation loss 0.02082372084259987 Accuracy 0.78369140625\n",
      "Iteration 85810 Training loss 0.014463248662650585 Validation loss 0.021006044000387192 Accuracy 0.78271484375\n",
      "Iteration 85820 Training loss 0.011698384769260883 Validation loss 0.02103775553405285 Accuracy 0.78271484375\n",
      "Iteration 85830 Training loss 0.013921041041612625 Validation loss 0.020787060260772705 Accuracy 0.78369140625\n",
      "Iteration 85840 Training loss 0.013192570768296719 Validation loss 0.020687630400061607 Accuracy 0.7841796875\n",
      "Iteration 85850 Training loss 0.013225407339632511 Validation loss 0.02081691287457943 Accuracy 0.783203125\n",
      "Iteration 85860 Training loss 0.013786680996418 Validation loss 0.020854316651821136 Accuracy 0.78369140625\n",
      "Iteration 85870 Training loss 0.012753410264849663 Validation loss 0.020885365083813667 Accuracy 0.7841796875\n",
      "Iteration 85880 Training loss 0.011488545686006546 Validation loss 0.020842982456088066 Accuracy 0.7841796875\n",
      "Iteration 85890 Training loss 0.0113869933411479 Validation loss 0.020931381732225418 Accuracy 0.783203125\n",
      "Iteration 85900 Training loss 0.013425441458821297 Validation loss 0.020884446799755096 Accuracy 0.7822265625\n",
      "Iteration 85910 Training loss 0.013454421423375607 Validation loss 0.020723598077893257 Accuracy 0.78466796875\n",
      "Iteration 85920 Training loss 0.012595240958034992 Validation loss 0.021046863868832588 Accuracy 0.78271484375\n",
      "Iteration 85930 Training loss 0.013530281372368336 Validation loss 0.020801730453968048 Accuracy 0.78369140625\n",
      "Iteration 85940 Training loss 0.014090323820710182 Validation loss 0.020771147683262825 Accuracy 0.78369140625\n",
      "Iteration 85950 Training loss 0.012991177849471569 Validation loss 0.02119522914290428 Accuracy 0.78125\n",
      "Iteration 85960 Training loss 0.013678171671926975 Validation loss 0.02092275582253933 Accuracy 0.7841796875\n",
      "Iteration 85970 Training loss 0.012197812087833881 Validation loss 0.020716268569231033 Accuracy 0.7841796875\n",
      "Iteration 85980 Training loss 0.01579253375530243 Validation loss 0.020748071372509003 Accuracy 0.78369140625\n",
      "Iteration 85990 Training loss 0.01465639192610979 Validation loss 0.02080242708325386 Accuracy 0.78271484375\n",
      "Iteration 86000 Training loss 0.014596379362046719 Validation loss 0.02077529765665531 Accuracy 0.783203125\n",
      "Iteration 86010 Training loss 0.012908946722745895 Validation loss 0.020628446713089943 Accuracy 0.78564453125\n",
      "Iteration 86020 Training loss 0.013525040820240974 Validation loss 0.02074115164577961 Accuracy 0.78515625\n",
      "Iteration 86030 Training loss 0.012600346468389034 Validation loss 0.020770205184817314 Accuracy 0.783203125\n",
      "Iteration 86040 Training loss 0.012941314838826656 Validation loss 0.02063603885471821 Accuracy 0.7861328125\n",
      "Iteration 86050 Training loss 0.012822373770177364 Validation loss 0.02071511745452881 Accuracy 0.78466796875\n",
      "Iteration 86060 Training loss 0.012713623233139515 Validation loss 0.021056000143289566 Accuracy 0.78076171875\n",
      "Iteration 86070 Training loss 0.011881524696946144 Validation loss 0.02102796733379364 Accuracy 0.78271484375\n",
      "Iteration 86080 Training loss 0.013644362799823284 Validation loss 0.020846860483288765 Accuracy 0.783203125\n",
      "Iteration 86090 Training loss 0.013498916290700436 Validation loss 0.02083459682762623 Accuracy 0.7841796875\n",
      "Iteration 86100 Training loss 0.012001622468233109 Validation loss 0.020935796201229095 Accuracy 0.78173828125\n",
      "Iteration 86110 Training loss 0.012203755788505077 Validation loss 0.02098199725151062 Accuracy 0.78173828125\n",
      "Iteration 86120 Training loss 0.013310648500919342 Validation loss 0.020884105935692787 Accuracy 0.78271484375\n",
      "Iteration 86130 Training loss 0.010532059706747532 Validation loss 0.021005989983677864 Accuracy 0.78271484375\n",
      "Iteration 86140 Training loss 0.012373674660921097 Validation loss 0.020876141265034676 Accuracy 0.78369140625\n",
      "Iteration 86150 Training loss 0.011704111471772194 Validation loss 0.02091236226260662 Accuracy 0.78271484375\n",
      "Iteration 86160 Training loss 0.014776874333620071 Validation loss 0.020902901887893677 Accuracy 0.7841796875\n",
      "Iteration 86170 Training loss 0.010352415032684803 Validation loss 0.02076856791973114 Accuracy 0.78515625\n",
      "Iteration 86180 Training loss 0.014699142426252365 Validation loss 0.02110508270561695 Accuracy 0.78173828125\n",
      "Iteration 86190 Training loss 0.01113882940262556 Validation loss 0.02077150158584118 Accuracy 0.7841796875\n",
      "Iteration 86200 Training loss 0.014005227014422417 Validation loss 0.020798353478312492 Accuracy 0.78564453125\n",
      "Iteration 86210 Training loss 0.012840580195188522 Validation loss 0.021177388727664948 Accuracy 0.77978515625\n",
      "Iteration 86220 Training loss 0.015052319504320621 Validation loss 0.020925860852003098 Accuracy 0.7841796875\n",
      "Iteration 86230 Training loss 0.014380200766026974 Validation loss 0.020819203928112984 Accuracy 0.78369140625\n",
      "Iteration 86240 Training loss 0.013300666585564613 Validation loss 0.020841779187321663 Accuracy 0.78466796875\n",
      "Iteration 86250 Training loss 0.013519263826310635 Validation loss 0.020784247666597366 Accuracy 0.78515625\n",
      "Iteration 86260 Training loss 0.010950242169201374 Validation loss 0.02087487280368805 Accuracy 0.7822265625\n",
      "Iteration 86270 Training loss 0.01326428446918726 Validation loss 0.020788075402379036 Accuracy 0.7841796875\n",
      "Iteration 86280 Training loss 0.012286718003451824 Validation loss 0.020695360377430916 Accuracy 0.78564453125\n",
      "Iteration 86290 Training loss 0.012991035357117653 Validation loss 0.020907087251544 Accuracy 0.78466796875\n",
      "Iteration 86300 Training loss 0.01364903710782528 Validation loss 0.020722083747386932 Accuracy 0.7841796875\n",
      "Iteration 86310 Training loss 0.014191973023116589 Validation loss 0.02085282653570175 Accuracy 0.78466796875\n",
      "Iteration 86320 Training loss 0.014652363955974579 Validation loss 0.020768798887729645 Accuracy 0.7841796875\n",
      "Iteration 86330 Training loss 0.012755725532770157 Validation loss 0.020602447912096977 Accuracy 0.78564453125\n",
      "Iteration 86340 Training loss 0.013874907977879047 Validation loss 0.020695604383945465 Accuracy 0.78466796875\n",
      "Iteration 86350 Training loss 0.01262195035815239 Validation loss 0.020667126402258873 Accuracy 0.78564453125\n",
      "Iteration 86360 Training loss 0.013939597643911839 Validation loss 0.02096446044743061 Accuracy 0.78125\n",
      "Iteration 86370 Training loss 0.012745366431772709 Validation loss 0.020831098780035973 Accuracy 0.783203125\n",
      "Iteration 86380 Training loss 0.012991630472242832 Validation loss 0.02083958126604557 Accuracy 0.78369140625\n",
      "Iteration 86390 Training loss 0.014546853490173817 Validation loss 0.020849311724305153 Accuracy 0.78271484375\n",
      "Iteration 86400 Training loss 0.015146993100643158 Validation loss 0.02090306766331196 Accuracy 0.783203125\n",
      "Iteration 86410 Training loss 0.012527977116405964 Validation loss 0.021104006096720695 Accuracy 0.78173828125\n",
      "Iteration 86420 Training loss 0.009816142730414867 Validation loss 0.020742226392030716 Accuracy 0.78466796875\n",
      "Iteration 86430 Training loss 0.012395134195685387 Validation loss 0.020685702562332153 Accuracy 0.78564453125\n",
      "Iteration 86440 Training loss 0.014864743687212467 Validation loss 0.020831076428294182 Accuracy 0.7841796875\n",
      "Iteration 86450 Training loss 0.013592584058642387 Validation loss 0.020903417840600014 Accuracy 0.78271484375\n",
      "Iteration 86460 Training loss 0.013947602361440659 Validation loss 0.020843733102083206 Accuracy 0.783203125\n",
      "Iteration 86470 Training loss 0.011302716098725796 Validation loss 0.020872246474027634 Accuracy 0.7841796875\n",
      "Iteration 86480 Training loss 0.013066306710243225 Validation loss 0.020686659961938858 Accuracy 0.78466796875\n",
      "Iteration 86490 Training loss 0.012745802290737629 Validation loss 0.020705072209239006 Accuracy 0.7841796875\n",
      "Iteration 86500 Training loss 0.01314752921462059 Validation loss 0.020654093474149704 Accuracy 0.78515625\n",
      "Iteration 86510 Training loss 0.012101897969841957 Validation loss 0.02096121571958065 Accuracy 0.783203125\n",
      "Iteration 86520 Training loss 0.012660924345254898 Validation loss 0.020685946568846703 Accuracy 0.78515625\n",
      "Iteration 86530 Training loss 0.012886705808341503 Validation loss 0.020654946565628052 Accuracy 0.78515625\n",
      "Iteration 86540 Training loss 0.013536285609006882 Validation loss 0.020640874281525612 Accuracy 0.78466796875\n",
      "Iteration 86550 Training loss 0.014585377648472786 Validation loss 0.0206796545535326 Accuracy 0.7841796875\n",
      "Iteration 86560 Training loss 0.012967906892299652 Validation loss 0.02082676626741886 Accuracy 0.7841796875\n",
      "Iteration 86570 Training loss 0.012549756094813347 Validation loss 0.020673193037509918 Accuracy 0.78466796875\n",
      "Iteration 86580 Training loss 0.011424294672906399 Validation loss 0.020717408508062363 Accuracy 0.78466796875\n",
      "Iteration 86590 Training loss 0.015239790081977844 Validation loss 0.020969629287719727 Accuracy 0.78369140625\n",
      "Iteration 86600 Training loss 0.012379970401525497 Validation loss 0.020736420527100563 Accuracy 0.78466796875\n",
      "Iteration 86610 Training loss 0.017195383086800575 Validation loss 0.021045178174972534 Accuracy 0.78173828125\n",
      "Iteration 86620 Training loss 0.011772416532039642 Validation loss 0.020667294040322304 Accuracy 0.78515625\n",
      "Iteration 86630 Training loss 0.01142881903797388 Validation loss 0.021087029948830605 Accuracy 0.78125\n",
      "Iteration 86640 Training loss 0.012950883246958256 Validation loss 0.020857373252511024 Accuracy 0.7822265625\n",
      "Iteration 86650 Training loss 0.012869314290583134 Validation loss 0.020810004323720932 Accuracy 0.7841796875\n",
      "Iteration 86660 Training loss 0.013676567003130913 Validation loss 0.020995067432522774 Accuracy 0.7822265625\n",
      "Iteration 86670 Training loss 0.01416612509638071 Validation loss 0.020948447287082672 Accuracy 0.783203125\n",
      "Iteration 86680 Training loss 0.012298398651182652 Validation loss 0.020733755081892014 Accuracy 0.7841796875\n",
      "Iteration 86690 Training loss 0.014865286648273468 Validation loss 0.020836949348449707 Accuracy 0.78271484375\n",
      "Iteration 86700 Training loss 0.012524939142167568 Validation loss 0.020823465660214424 Accuracy 0.78271484375\n",
      "Iteration 86710 Training loss 0.012335371226072311 Validation loss 0.02090761624276638 Accuracy 0.783203125\n",
      "Iteration 86720 Training loss 0.013266745954751968 Validation loss 0.02088271453976631 Accuracy 0.78271484375\n",
      "Iteration 86730 Training loss 0.014991152100265026 Validation loss 0.020952308550477028 Accuracy 0.783203125\n",
      "Iteration 86740 Training loss 0.012289360165596008 Validation loss 0.020765500143170357 Accuracy 0.78369140625\n",
      "Iteration 86750 Training loss 0.012663653120398521 Validation loss 0.02100251615047455 Accuracy 0.78076171875\n",
      "Iteration 86760 Training loss 0.01167477760463953 Validation loss 0.02085079625248909 Accuracy 0.783203125\n",
      "Iteration 86770 Training loss 0.012982998043298721 Validation loss 0.020946288481354713 Accuracy 0.7822265625\n",
      "Iteration 86780 Training loss 0.013372793793678284 Validation loss 0.020785842090845108 Accuracy 0.78369140625\n",
      "Iteration 86790 Training loss 0.012086049653589725 Validation loss 0.02082827500998974 Accuracy 0.78271484375\n",
      "Iteration 86800 Training loss 0.014236707240343094 Validation loss 0.020784515887498856 Accuracy 0.7822265625\n",
      "Iteration 86810 Training loss 0.01308510359376669 Validation loss 0.021082201972603798 Accuracy 0.78125\n",
      "Iteration 86820 Training loss 0.012867340818047523 Validation loss 0.020930951461195946 Accuracy 0.78271484375\n",
      "Iteration 86830 Training loss 0.00943287555128336 Validation loss 0.02065812237560749 Accuracy 0.78466796875\n",
      "Iteration 86840 Training loss 0.011228477582335472 Validation loss 0.020819613710045815 Accuracy 0.78271484375\n",
      "Iteration 86850 Training loss 0.013925503008067608 Validation loss 0.020884092897176743 Accuracy 0.78271484375\n",
      "Iteration 86860 Training loss 0.014689071103930473 Validation loss 0.02077595889568329 Accuracy 0.78271484375\n",
      "Iteration 86870 Training loss 0.014514407142996788 Validation loss 0.02112753316760063 Accuracy 0.78173828125\n",
      "Iteration 86880 Training loss 0.014579767361283302 Validation loss 0.02090778574347496 Accuracy 0.783203125\n",
      "Iteration 86890 Training loss 0.01158064417541027 Validation loss 0.020814822986721992 Accuracy 0.78369140625\n",
      "Iteration 86900 Training loss 0.011424277909100056 Validation loss 0.020786168053746223 Accuracy 0.7841796875\n",
      "Iteration 86910 Training loss 0.013145477510988712 Validation loss 0.020791225135326385 Accuracy 0.783203125\n",
      "Iteration 86920 Training loss 0.013110648840665817 Validation loss 0.020690809935331345 Accuracy 0.78369140625\n",
      "Iteration 86930 Training loss 0.011941979639232159 Validation loss 0.020853307098150253 Accuracy 0.7822265625\n",
      "Iteration 86940 Training loss 0.010036819614470005 Validation loss 0.021020546555519104 Accuracy 0.78173828125\n",
      "Iteration 86950 Training loss 0.014728114008903503 Validation loss 0.02119291014969349 Accuracy 0.78076171875\n",
      "Iteration 86960 Training loss 0.014542859047651291 Validation loss 0.021314281970262527 Accuracy 0.7802734375\n",
      "Iteration 86970 Training loss 0.015485391020774841 Validation loss 0.02097879908978939 Accuracy 0.7822265625\n",
      "Iteration 86980 Training loss 0.012892160564661026 Validation loss 0.02084425650537014 Accuracy 0.78271484375\n",
      "Iteration 86990 Training loss 0.013424267992377281 Validation loss 0.020792968571186066 Accuracy 0.78369140625\n",
      "Iteration 87000 Training loss 0.013407313264906406 Validation loss 0.020865067839622498 Accuracy 0.78369140625\n",
      "Iteration 87010 Training loss 0.01028674840927124 Validation loss 0.0207783542573452 Accuracy 0.7841796875\n",
      "Iteration 87020 Training loss 0.0163523331284523 Validation loss 0.02089495025575161 Accuracy 0.7841796875\n",
      "Iteration 87030 Training loss 0.012853759340941906 Validation loss 0.020721914246678352 Accuracy 0.78369140625\n",
      "Iteration 87040 Training loss 0.013322467915713787 Validation loss 0.020867446437478065 Accuracy 0.783203125\n",
      "Iteration 87050 Training loss 0.012347294948995113 Validation loss 0.020886503159999847 Accuracy 0.78271484375\n",
      "Iteration 87060 Training loss 0.013086723163723946 Validation loss 0.020894719287753105 Accuracy 0.783203125\n",
      "Iteration 87070 Training loss 0.014940544962882996 Validation loss 0.02099757082760334 Accuracy 0.783203125\n",
      "Iteration 87080 Training loss 0.013013181276619434 Validation loss 0.021116167306900024 Accuracy 0.78076171875\n",
      "Iteration 87090 Training loss 0.01531987078487873 Validation loss 0.02083754539489746 Accuracy 0.7841796875\n",
      "Iteration 87100 Training loss 0.012656702660024166 Validation loss 0.020879752933979034 Accuracy 0.7822265625\n",
      "Iteration 87110 Training loss 0.012927553616464138 Validation loss 0.020751334726810455 Accuracy 0.7841796875\n",
      "Iteration 87120 Training loss 0.011574138887226582 Validation loss 0.020891303196549416 Accuracy 0.78173828125\n",
      "Iteration 87130 Training loss 0.011561472900211811 Validation loss 0.02074441872537136 Accuracy 0.783203125\n",
      "Iteration 87140 Training loss 0.01558225229382515 Validation loss 0.02104557491838932 Accuracy 0.7822265625\n",
      "Iteration 87150 Training loss 0.014309720136225224 Validation loss 0.021022489294409752 Accuracy 0.7822265625\n",
      "Iteration 87160 Training loss 0.01185289490967989 Validation loss 0.020664282143115997 Accuracy 0.78466796875\n",
      "Iteration 87170 Training loss 0.014086990617215633 Validation loss 0.021253367885947227 Accuracy 0.77978515625\n",
      "Iteration 87180 Training loss 0.013067389838397503 Validation loss 0.020801855251193047 Accuracy 0.783203125\n",
      "Iteration 87190 Training loss 0.010516791604459286 Validation loss 0.020759601145982742 Accuracy 0.783203125\n",
      "Iteration 87200 Training loss 0.012394795194268227 Validation loss 0.02089713141322136 Accuracy 0.783203125\n",
      "Iteration 87210 Training loss 0.011082726530730724 Validation loss 0.020775126293301582 Accuracy 0.783203125\n",
      "Iteration 87220 Training loss 0.012179593555629253 Validation loss 0.020919620990753174 Accuracy 0.78173828125\n",
      "Iteration 87230 Training loss 0.014519968070089817 Validation loss 0.020811889320611954 Accuracy 0.783203125\n",
      "Iteration 87240 Training loss 0.011805620975792408 Validation loss 0.02072613313794136 Accuracy 0.7841796875\n",
      "Iteration 87250 Training loss 0.013758114539086819 Validation loss 0.02087649516761303 Accuracy 0.78369140625\n",
      "Iteration 87260 Training loss 0.014314984902739525 Validation loss 0.0211339108645916 Accuracy 0.78076171875\n",
      "Iteration 87270 Training loss 0.01594056747853756 Validation loss 0.020967938005924225 Accuracy 0.78271484375\n",
      "Iteration 87280 Training loss 0.01641988754272461 Validation loss 0.02097533456981182 Accuracy 0.7822265625\n",
      "Iteration 87290 Training loss 0.0140557074919343 Validation loss 0.0207792017608881 Accuracy 0.783203125\n",
      "Iteration 87300 Training loss 0.011828952468931675 Validation loss 0.020819423720240593 Accuracy 0.783203125\n",
      "Iteration 87310 Training loss 0.013433100655674934 Validation loss 0.020784221589565277 Accuracy 0.783203125\n",
      "Iteration 87320 Training loss 0.012683025561273098 Validation loss 0.020706625655293465 Accuracy 0.78466796875\n",
      "Iteration 87330 Training loss 0.012906781397759914 Validation loss 0.02078004740178585 Accuracy 0.78271484375\n",
      "Iteration 87340 Training loss 0.013670309446752071 Validation loss 0.020702265202999115 Accuracy 0.7841796875\n",
      "Iteration 87350 Training loss 0.01455945335328579 Validation loss 0.02078457921743393 Accuracy 0.78369140625\n",
      "Iteration 87360 Training loss 0.013685968704521656 Validation loss 0.020927850157022476 Accuracy 0.78271484375\n",
      "Iteration 87370 Training loss 0.012336728163063526 Validation loss 0.020955568179488182 Accuracy 0.78271484375\n",
      "Iteration 87380 Training loss 0.013642738573253155 Validation loss 0.020904725417494774 Accuracy 0.78271484375\n",
      "Iteration 87390 Training loss 0.015849187970161438 Validation loss 0.021058592945337296 Accuracy 0.78173828125\n",
      "Iteration 87400 Training loss 0.017126033082604408 Validation loss 0.020708708092570305 Accuracy 0.78466796875\n",
      "Iteration 87410 Training loss 0.011758323758840561 Validation loss 0.020920800045132637 Accuracy 0.783203125\n",
      "Iteration 87420 Training loss 0.012877506203949451 Validation loss 0.02078956924378872 Accuracy 0.7822265625\n",
      "Iteration 87430 Training loss 0.014479744248092175 Validation loss 0.020822832360863686 Accuracy 0.78369140625\n",
      "Iteration 87440 Training loss 0.015253659337759018 Validation loss 0.020941205322742462 Accuracy 0.78173828125\n",
      "Iteration 87450 Training loss 0.014415448531508446 Validation loss 0.02089102938771248 Accuracy 0.783203125\n",
      "Iteration 87460 Training loss 0.011098532006144524 Validation loss 0.02078970894217491 Accuracy 0.7841796875\n",
      "Iteration 87470 Training loss 0.014155471697449684 Validation loss 0.02149646356701851 Accuracy 0.77587890625\n",
      "Iteration 87480 Training loss 0.013984107412397861 Validation loss 0.021012747660279274 Accuracy 0.78173828125\n",
      "Iteration 87490 Training loss 0.011462407186627388 Validation loss 0.02104264684021473 Accuracy 0.78076171875\n",
      "Iteration 87500 Training loss 0.013673532754182816 Validation loss 0.020851559937000275 Accuracy 0.78271484375\n",
      "Iteration 87510 Training loss 0.012333404272794724 Validation loss 0.020787475630640984 Accuracy 0.78369140625\n",
      "Iteration 87520 Training loss 0.014718755148351192 Validation loss 0.020900307223200798 Accuracy 0.783203125\n",
      "Iteration 87530 Training loss 0.013490972109138966 Validation loss 0.02060595154762268 Accuracy 0.78466796875\n",
      "Iteration 87540 Training loss 0.011920379474759102 Validation loss 0.020743750035762787 Accuracy 0.7841796875\n",
      "Iteration 87550 Training loss 0.01293105073273182 Validation loss 0.02079758793115616 Accuracy 0.78173828125\n",
      "Iteration 87560 Training loss 0.011947416700422764 Validation loss 0.02103688195347786 Accuracy 0.78125\n",
      "Iteration 87570 Training loss 0.014032180421054363 Validation loss 0.02099638059735298 Accuracy 0.78173828125\n",
      "Iteration 87580 Training loss 0.011373812332749367 Validation loss 0.020874449983239174 Accuracy 0.78173828125\n",
      "Iteration 87590 Training loss 0.011673972941935062 Validation loss 0.02089741639792919 Accuracy 0.7822265625\n",
      "Iteration 87600 Training loss 0.012183234095573425 Validation loss 0.02076406218111515 Accuracy 0.78271484375\n",
      "Iteration 87610 Training loss 0.01565471664071083 Validation loss 0.020797304809093475 Accuracy 0.78271484375\n",
      "Iteration 87620 Training loss 0.013262748718261719 Validation loss 0.02077796868979931 Accuracy 0.783203125\n",
      "Iteration 87630 Training loss 0.012783686630427837 Validation loss 0.02072802744805813 Accuracy 0.7841796875\n",
      "Iteration 87640 Training loss 0.013126037083566189 Validation loss 0.02097100019454956 Accuracy 0.78271484375\n",
      "Iteration 87650 Training loss 0.01255545299500227 Validation loss 0.020972561091184616 Accuracy 0.78125\n",
      "Iteration 87660 Training loss 0.014923861250281334 Validation loss 0.02082175761461258 Accuracy 0.78271484375\n",
      "Iteration 87670 Training loss 0.013450532220304012 Validation loss 0.020908905193209648 Accuracy 0.7822265625\n",
      "Iteration 87680 Training loss 0.015636224299669266 Validation loss 0.02070007100701332 Accuracy 0.7841796875\n",
      "Iteration 87690 Training loss 0.013035029172897339 Validation loss 0.020735453814268112 Accuracy 0.7841796875\n",
      "Iteration 87700 Training loss 0.015567554160952568 Validation loss 0.020866790786385536 Accuracy 0.78173828125\n",
      "Iteration 87710 Training loss 0.013354068621993065 Validation loss 0.020735349506139755 Accuracy 0.78369140625\n",
      "Iteration 87720 Training loss 0.014299950562417507 Validation loss 0.020678119733929634 Accuracy 0.78369140625\n",
      "Iteration 87730 Training loss 0.014276606030762196 Validation loss 0.020736217498779297 Accuracy 0.78369140625\n",
      "Iteration 87740 Training loss 0.013662198558449745 Validation loss 0.020719239488244057 Accuracy 0.7841796875\n",
      "Iteration 87750 Training loss 0.01232918631285429 Validation loss 0.020968973636627197 Accuracy 0.78173828125\n",
      "Iteration 87760 Training loss 0.015927614644169807 Validation loss 0.02081795409321785 Accuracy 0.78369140625\n",
      "Iteration 87770 Training loss 0.015861621126532555 Validation loss 0.020916156470775604 Accuracy 0.78125\n",
      "Iteration 87780 Training loss 0.013815907761454582 Validation loss 0.02113492041826248 Accuracy 0.7802734375\n",
      "Iteration 87790 Training loss 0.01286013051867485 Validation loss 0.020779266953468323 Accuracy 0.7822265625\n",
      "Iteration 87800 Training loss 0.012390762567520142 Validation loss 0.021059716120362282 Accuracy 0.78173828125\n",
      "Iteration 87810 Training loss 0.011174898594617844 Validation loss 0.02087627910077572 Accuracy 0.78271484375\n",
      "Iteration 87820 Training loss 0.012542426586151123 Validation loss 0.020763175562024117 Accuracy 0.783203125\n",
      "Iteration 87830 Training loss 0.011501466855406761 Validation loss 0.02079092711210251 Accuracy 0.783203125\n",
      "Iteration 87840 Training loss 0.01506796758621931 Validation loss 0.021031729876995087 Accuracy 0.78125\n",
      "Iteration 87850 Training loss 0.012102123349905014 Validation loss 0.020895488560199738 Accuracy 0.78271484375\n",
      "Iteration 87860 Training loss 0.010515902191400528 Validation loss 0.020663950592279434 Accuracy 0.7841796875\n",
      "Iteration 87870 Training loss 0.01174619048833847 Validation loss 0.0207937303930521 Accuracy 0.78369140625\n",
      "Iteration 87880 Training loss 0.01219850778579712 Validation loss 0.02074093371629715 Accuracy 0.78369140625\n",
      "Iteration 87890 Training loss 0.013984589837491512 Validation loss 0.021107371896505356 Accuracy 0.7802734375\n",
      "Iteration 87900 Training loss 0.012434816919267178 Validation loss 0.021035023033618927 Accuracy 0.78076171875\n",
      "Iteration 87910 Training loss 0.013471323065459728 Validation loss 0.02071116678416729 Accuracy 0.78369140625\n",
      "Iteration 87920 Training loss 0.017246443778276443 Validation loss 0.0209656972438097 Accuracy 0.78271484375\n",
      "Iteration 87930 Training loss 0.011494120582938194 Validation loss 0.020708667114377022 Accuracy 0.783203125\n",
      "Iteration 87940 Training loss 0.013359866105020046 Validation loss 0.020955173298716545 Accuracy 0.7822265625\n",
      "Iteration 87950 Training loss 0.0143230389803648 Validation loss 0.020644061267375946 Accuracy 0.78369140625\n",
      "Iteration 87960 Training loss 0.01332077942788601 Validation loss 0.020949918776750565 Accuracy 0.7822265625\n",
      "Iteration 87970 Training loss 0.013387329876422882 Validation loss 0.02067467011511326 Accuracy 0.7841796875\n",
      "Iteration 87980 Training loss 0.015311770141124725 Validation loss 0.02096875011920929 Accuracy 0.7822265625\n",
      "Iteration 87990 Training loss 0.013407979160547256 Validation loss 0.020818088203668594 Accuracy 0.78271484375\n",
      "Iteration 88000 Training loss 0.013031333684921265 Validation loss 0.020898304879665375 Accuracy 0.78125\n",
      "Iteration 88010 Training loss 0.012326564639806747 Validation loss 0.02094997838139534 Accuracy 0.78173828125\n",
      "Iteration 88020 Training loss 0.01360574085265398 Validation loss 0.020674899220466614 Accuracy 0.78369140625\n",
      "Iteration 88030 Training loss 0.013995310291647911 Validation loss 0.020774653181433678 Accuracy 0.783203125\n",
      "Iteration 88040 Training loss 0.012673851102590561 Validation loss 0.0208382960408926 Accuracy 0.78271484375\n",
      "Iteration 88050 Training loss 0.0159732848405838 Validation loss 0.020912956446409225 Accuracy 0.78369140625\n",
      "Iteration 88060 Training loss 0.012805702164769173 Validation loss 0.020816370844841003 Accuracy 0.7822265625\n",
      "Iteration 88070 Training loss 0.011439038440585136 Validation loss 0.021117130294442177 Accuracy 0.77978515625\n",
      "Iteration 88080 Training loss 0.012418583035469055 Validation loss 0.020817363634705544 Accuracy 0.78271484375\n",
      "Iteration 88090 Training loss 0.013278055936098099 Validation loss 0.021079497411847115 Accuracy 0.7802734375\n",
      "Iteration 88100 Training loss 0.013318793848156929 Validation loss 0.020800558850169182 Accuracy 0.783203125\n",
      "Iteration 88110 Training loss 0.011020530946552753 Validation loss 0.020781921222805977 Accuracy 0.7841796875\n",
      "Iteration 88120 Training loss 0.011541439220309258 Validation loss 0.0209368746727705 Accuracy 0.783203125\n",
      "Iteration 88130 Training loss 0.014529353938996792 Validation loss 0.02062060683965683 Accuracy 0.78466796875\n",
      "Iteration 88140 Training loss 0.011091882362961769 Validation loss 0.020686691626906395 Accuracy 0.78515625\n",
      "Iteration 88150 Training loss 0.012071711011230946 Validation loss 0.020836923271417618 Accuracy 0.7841796875\n",
      "Iteration 88160 Training loss 0.0115843890234828 Validation loss 0.020899701863527298 Accuracy 0.78271484375\n",
      "Iteration 88170 Training loss 0.012961641885340214 Validation loss 0.020893217995762825 Accuracy 0.7822265625\n",
      "Iteration 88180 Training loss 0.013808337040245533 Validation loss 0.021225126460194588 Accuracy 0.779296875\n",
      "Iteration 88190 Training loss 0.013720087707042694 Validation loss 0.020717745646834373 Accuracy 0.783203125\n",
      "Iteration 88200 Training loss 0.012600153684616089 Validation loss 0.020723393186926842 Accuracy 0.7841796875\n",
      "Iteration 88210 Training loss 0.011958125047385693 Validation loss 0.020868904888629913 Accuracy 0.78271484375\n",
      "Iteration 88220 Training loss 0.012665234506130219 Validation loss 0.020707299932837486 Accuracy 0.7841796875\n",
      "Iteration 88230 Training loss 0.014618747867643833 Validation loss 0.020627567544579506 Accuracy 0.78515625\n",
      "Iteration 88240 Training loss 0.013633104972541332 Validation loss 0.02068917639553547 Accuracy 0.78564453125\n",
      "Iteration 88250 Training loss 0.014797231182456017 Validation loss 0.020739931613206863 Accuracy 0.783203125\n",
      "Iteration 88260 Training loss 0.014628386124968529 Validation loss 0.020794877782464027 Accuracy 0.7841796875\n",
      "Iteration 88270 Training loss 0.013194533996284008 Validation loss 0.02084958739578724 Accuracy 0.783203125\n",
      "Iteration 88280 Training loss 0.010917897336184978 Validation loss 0.020736418664455414 Accuracy 0.7841796875\n",
      "Iteration 88290 Training loss 0.011536194011569023 Validation loss 0.020844558253884315 Accuracy 0.78369140625\n",
      "Iteration 88300 Training loss 0.015000900253653526 Validation loss 0.02079956978559494 Accuracy 0.78466796875\n",
      "Iteration 88310 Training loss 0.01119602657854557 Validation loss 0.020645201206207275 Accuracy 0.78466796875\n",
      "Iteration 88320 Training loss 0.013737103901803493 Validation loss 0.020739281550049782 Accuracy 0.7841796875\n",
      "Iteration 88330 Training loss 0.013479146175086498 Validation loss 0.021034862846136093 Accuracy 0.78173828125\n",
      "Iteration 88340 Training loss 0.011763796210289001 Validation loss 0.020762035623192787 Accuracy 0.783203125\n",
      "Iteration 88350 Training loss 0.015828143805265427 Validation loss 0.02074681594967842 Accuracy 0.783203125\n",
      "Iteration 88360 Training loss 0.012927974574267864 Validation loss 0.020768778398633003 Accuracy 0.78369140625\n",
      "Iteration 88370 Training loss 0.013010022230446339 Validation loss 0.0208200141787529 Accuracy 0.78271484375\n",
      "Iteration 88380 Training loss 0.013646471314132214 Validation loss 0.0209317859262228 Accuracy 0.783203125\n",
      "Iteration 88390 Training loss 0.012309211306273937 Validation loss 0.020721137523651123 Accuracy 0.783203125\n",
      "Iteration 88400 Training loss 0.014387398026883602 Validation loss 0.021037762984633446 Accuracy 0.78125\n",
      "Iteration 88410 Training loss 0.012416287325322628 Validation loss 0.02082156389951706 Accuracy 0.78271484375\n",
      "Iteration 88420 Training loss 0.011204887181520462 Validation loss 0.020949212834239006 Accuracy 0.7822265625\n",
      "Iteration 88430 Training loss 0.015127301216125488 Validation loss 0.02066948637366295 Accuracy 0.78466796875\n",
      "Iteration 88440 Training loss 0.01436382532119751 Validation loss 0.020873013883829117 Accuracy 0.78369140625\n",
      "Iteration 88450 Training loss 0.013557183556258678 Validation loss 0.02116735465824604 Accuracy 0.77978515625\n",
      "Iteration 88460 Training loss 0.01451481319963932 Validation loss 0.020814845338463783 Accuracy 0.78369140625\n",
      "Iteration 88470 Training loss 0.013268397189676762 Validation loss 0.020789306610822678 Accuracy 0.7841796875\n",
      "Iteration 88480 Training loss 0.014295964501798153 Validation loss 0.020763911306858063 Accuracy 0.78515625\n",
      "Iteration 88490 Training loss 0.014301590621471405 Validation loss 0.02102316915988922 Accuracy 0.7822265625\n",
      "Iteration 88500 Training loss 0.014061369001865387 Validation loss 0.020781660452485085 Accuracy 0.78466796875\n",
      "Iteration 88510 Training loss 0.012260152027010918 Validation loss 0.02070208638906479 Accuracy 0.7841796875\n",
      "Iteration 88520 Training loss 0.012781184166669846 Validation loss 0.020712142810225487 Accuracy 0.7841796875\n",
      "Iteration 88530 Training loss 0.01388794556260109 Validation loss 0.020763486623764038 Accuracy 0.7841796875\n",
      "Iteration 88540 Training loss 0.014415585435926914 Validation loss 0.020840326324105263 Accuracy 0.78271484375\n",
      "Iteration 88550 Training loss 0.01379366684705019 Validation loss 0.02085190825164318 Accuracy 0.78271484375\n",
      "Iteration 88560 Training loss 0.012859636917710304 Validation loss 0.020918341353535652 Accuracy 0.78173828125\n",
      "Iteration 88570 Training loss 0.012727364897727966 Validation loss 0.020716356113553047 Accuracy 0.78466796875\n",
      "Iteration 88580 Training loss 0.01439391914755106 Validation loss 0.020682604983448982 Accuracy 0.7841796875\n",
      "Iteration 88590 Training loss 0.014529436826705933 Validation loss 0.0207197405397892 Accuracy 0.7841796875\n",
      "Iteration 88600 Training loss 0.012609049677848816 Validation loss 0.02076861262321472 Accuracy 0.78515625\n",
      "Iteration 88610 Training loss 0.015114705078303814 Validation loss 0.02068244107067585 Accuracy 0.78466796875\n",
      "Iteration 88620 Training loss 0.013964497484266758 Validation loss 0.02078963629901409 Accuracy 0.783203125\n",
      "Iteration 88630 Training loss 0.012020720168948174 Validation loss 0.02088330313563347 Accuracy 0.78271484375\n",
      "Iteration 88640 Training loss 0.012027498334646225 Validation loss 0.02065989188849926 Accuracy 0.78466796875\n",
      "Iteration 88650 Training loss 0.013566916808485985 Validation loss 0.02097734808921814 Accuracy 0.7822265625\n",
      "Iteration 88660 Training loss 0.011091131716966629 Validation loss 0.020914951339364052 Accuracy 0.78125\n",
      "Iteration 88670 Training loss 0.012009480968117714 Validation loss 0.020928548648953438 Accuracy 0.78125\n",
      "Iteration 88680 Training loss 0.013692664913833141 Validation loss 0.020860595628619194 Accuracy 0.78271484375\n",
      "Iteration 88690 Training loss 0.014377507381141186 Validation loss 0.0208340622484684 Accuracy 0.78369140625\n",
      "Iteration 88700 Training loss 0.01400201115757227 Validation loss 0.020873453468084335 Accuracy 0.78271484375\n",
      "Iteration 88710 Training loss 0.013155652210116386 Validation loss 0.020777694880962372 Accuracy 0.78369140625\n",
      "Iteration 88720 Training loss 0.014817586168646812 Validation loss 0.020709911361336708 Accuracy 0.78369140625\n",
      "Iteration 88730 Training loss 0.010384558700025082 Validation loss 0.020944667980074883 Accuracy 0.7822265625\n",
      "Iteration 88740 Training loss 0.013293316587805748 Validation loss 0.020958339795470238 Accuracy 0.7822265625\n",
      "Iteration 88750 Training loss 0.012156433425843716 Validation loss 0.020610522478818893 Accuracy 0.78564453125\n",
      "Iteration 88760 Training loss 0.013300687074661255 Validation loss 0.020752541720867157 Accuracy 0.7841796875\n",
      "Iteration 88770 Training loss 0.013319043442606926 Validation loss 0.021062111482024193 Accuracy 0.78125\n",
      "Iteration 88780 Training loss 0.01551714725792408 Validation loss 0.02063705585896969 Accuracy 0.78369140625\n",
      "Iteration 88790 Training loss 0.012886034324765205 Validation loss 0.02066447213292122 Accuracy 0.783203125\n",
      "Iteration 88800 Training loss 0.012553594075143337 Validation loss 0.020944753661751747 Accuracy 0.7822265625\n",
      "Iteration 88810 Training loss 0.013575782999396324 Validation loss 0.020796433091163635 Accuracy 0.78369140625\n",
      "Iteration 88820 Training loss 0.013456365093588829 Validation loss 0.02083115093410015 Accuracy 0.78271484375\n",
      "Iteration 88830 Training loss 0.013252084143459797 Validation loss 0.020858310163021088 Accuracy 0.7822265625\n",
      "Iteration 88840 Training loss 0.012492639943957329 Validation loss 0.020701825618743896 Accuracy 0.7841796875\n",
      "Iteration 88850 Training loss 0.011770645156502724 Validation loss 0.02087482437491417 Accuracy 0.783203125\n",
      "Iteration 88860 Training loss 0.013512255623936653 Validation loss 0.020720604807138443 Accuracy 0.7841796875\n",
      "Iteration 88870 Training loss 0.013459543697535992 Validation loss 0.020717963576316833 Accuracy 0.78515625\n",
      "Iteration 88880 Training loss 0.01103669498115778 Validation loss 0.02084333263337612 Accuracy 0.78369140625\n",
      "Iteration 88890 Training loss 0.011539613828063011 Validation loss 0.020810702815651894 Accuracy 0.783203125\n",
      "Iteration 88900 Training loss 0.01050527859479189 Validation loss 0.020882951095700264 Accuracy 0.78271484375\n",
      "Iteration 88910 Training loss 0.015160497277975082 Validation loss 0.020743481814861298 Accuracy 0.78369140625\n",
      "Iteration 88920 Training loss 0.012364606373012066 Validation loss 0.020704668015241623 Accuracy 0.78515625\n",
      "Iteration 88930 Training loss 0.010092917829751968 Validation loss 0.020696571096777916 Accuracy 0.7841796875\n",
      "Iteration 88940 Training loss 0.012220855802297592 Validation loss 0.020841090008616447 Accuracy 0.78271484375\n",
      "Iteration 88950 Training loss 0.01464924868196249 Validation loss 0.02066119760274887 Accuracy 0.78369140625\n",
      "Iteration 88960 Training loss 0.013979087583720684 Validation loss 0.020798563957214355 Accuracy 0.7822265625\n",
      "Iteration 88970 Training loss 0.011660273186862469 Validation loss 0.02083532325923443 Accuracy 0.783203125\n",
      "Iteration 88980 Training loss 0.013800948858261108 Validation loss 0.020799340680241585 Accuracy 0.78271484375\n",
      "Iteration 88990 Training loss 0.013226115144789219 Validation loss 0.020606908947229385 Accuracy 0.7841796875\n",
      "Iteration 89000 Training loss 0.015432894229888916 Validation loss 0.021190010011196136 Accuracy 0.78076171875\n",
      "Iteration 89010 Training loss 0.013518564403057098 Validation loss 0.020908702164888382 Accuracy 0.78173828125\n",
      "Iteration 89020 Training loss 0.011881319805979729 Validation loss 0.02063712291419506 Accuracy 0.7841796875\n",
      "Iteration 89030 Training loss 0.012129982002079487 Validation loss 0.020629500970244408 Accuracy 0.7841796875\n",
      "Iteration 89040 Training loss 0.013397210277616978 Validation loss 0.021369481459259987 Accuracy 0.77880859375\n",
      "Iteration 89050 Training loss 0.013630314730107784 Validation loss 0.020640099421143532 Accuracy 0.78515625\n",
      "Iteration 89060 Training loss 0.011598639190196991 Validation loss 0.02095630392432213 Accuracy 0.7822265625\n",
      "Iteration 89070 Training loss 0.012849866412580013 Validation loss 0.020850228145718575 Accuracy 0.783203125\n",
      "Iteration 89080 Training loss 0.010598016902804375 Validation loss 0.020835770294070244 Accuracy 0.78271484375\n",
      "Iteration 89090 Training loss 0.010718036442995071 Validation loss 0.020607855170965195 Accuracy 0.78466796875\n",
      "Iteration 89100 Training loss 0.013316940516233444 Validation loss 0.020922265946865082 Accuracy 0.783203125\n",
      "Iteration 89110 Training loss 0.012082920409739017 Validation loss 0.02051888220012188 Accuracy 0.78466796875\n",
      "Iteration 89120 Training loss 0.013339439406991005 Validation loss 0.021337436512112617 Accuracy 0.7802734375\n",
      "Iteration 89130 Training loss 0.01373446173965931 Validation loss 0.020720412954688072 Accuracy 0.7841796875\n",
      "Iteration 89140 Training loss 0.011725847609341145 Validation loss 0.02069154754281044 Accuracy 0.783203125\n",
      "Iteration 89150 Training loss 0.013619808480143547 Validation loss 0.02080894075334072 Accuracy 0.78271484375\n",
      "Iteration 89160 Training loss 0.014024109579622746 Validation loss 0.020659897476434708 Accuracy 0.78369140625\n",
      "Iteration 89170 Training loss 0.013722577132284641 Validation loss 0.020693473517894745 Accuracy 0.78369140625\n",
      "Iteration 89180 Training loss 0.013207623735070229 Validation loss 0.02080160565674305 Accuracy 0.783203125\n",
      "Iteration 89190 Training loss 0.012824608013033867 Validation loss 0.020656010136008263 Accuracy 0.783203125\n",
      "Iteration 89200 Training loss 0.011703534051775932 Validation loss 0.020693134516477585 Accuracy 0.78466796875\n",
      "Iteration 89210 Training loss 0.012833045795559883 Validation loss 0.02073284611105919 Accuracy 0.78466796875\n",
      "Iteration 89220 Training loss 0.013356197625398636 Validation loss 0.020805206149816513 Accuracy 0.7822265625\n",
      "Iteration 89230 Training loss 0.013739402405917645 Validation loss 0.020894311368465424 Accuracy 0.7822265625\n",
      "Iteration 89240 Training loss 0.012354795821011066 Validation loss 0.02083642967045307 Accuracy 0.783203125\n",
      "Iteration 89250 Training loss 0.013449223712086678 Validation loss 0.02105230651795864 Accuracy 0.78173828125\n",
      "Iteration 89260 Training loss 0.014620359055697918 Validation loss 0.02070542983710766 Accuracy 0.78369140625\n",
      "Iteration 89270 Training loss 0.01456470973789692 Validation loss 0.02083704061806202 Accuracy 0.78271484375\n",
      "Iteration 89280 Training loss 0.013835418969392776 Validation loss 0.020841581746935844 Accuracy 0.78271484375\n",
      "Iteration 89290 Training loss 0.014177970588207245 Validation loss 0.020754359662532806 Accuracy 0.78466796875\n",
      "Iteration 89300 Training loss 0.012721654959022999 Validation loss 0.02090276964008808 Accuracy 0.783203125\n",
      "Iteration 89310 Training loss 0.013239496387541294 Validation loss 0.020770974457263947 Accuracy 0.78271484375\n",
      "Iteration 89320 Training loss 0.017610806971788406 Validation loss 0.020769154652953148 Accuracy 0.783203125\n",
      "Iteration 89330 Training loss 0.012431850656867027 Validation loss 0.020833905786275864 Accuracy 0.783203125\n",
      "Iteration 89340 Training loss 0.013039348646998405 Validation loss 0.020807137712836266 Accuracy 0.78369140625\n",
      "Iteration 89350 Training loss 0.014198939315974712 Validation loss 0.021092666313052177 Accuracy 0.7802734375\n",
      "Iteration 89360 Training loss 0.013711775653064251 Validation loss 0.020815109834074974 Accuracy 0.78369140625\n",
      "Iteration 89370 Training loss 0.012479061260819435 Validation loss 0.020972318947315216 Accuracy 0.78173828125\n",
      "Iteration 89380 Training loss 0.01305797416716814 Validation loss 0.020669203251600266 Accuracy 0.7841796875\n",
      "Iteration 89390 Training loss 0.013906246051192284 Validation loss 0.020942000672221184 Accuracy 0.78369140625\n",
      "Iteration 89400 Training loss 0.01232509221881628 Validation loss 0.020814912393689156 Accuracy 0.78271484375\n",
      "Iteration 89410 Training loss 0.01273256167769432 Validation loss 0.020850399509072304 Accuracy 0.7822265625\n",
      "Iteration 89420 Training loss 0.014415581710636616 Validation loss 0.020764712244272232 Accuracy 0.78369140625\n",
      "Iteration 89430 Training loss 0.013450783677399158 Validation loss 0.020891893655061722 Accuracy 0.78369140625\n",
      "Iteration 89440 Training loss 0.013850619085133076 Validation loss 0.020768187940120697 Accuracy 0.783203125\n",
      "Iteration 89450 Training loss 0.014752598479390144 Validation loss 0.02074790932238102 Accuracy 0.783203125\n",
      "Iteration 89460 Training loss 0.011732771061360836 Validation loss 0.020733065903186798 Accuracy 0.78369140625\n",
      "Iteration 89470 Training loss 0.008447585627436638 Validation loss 0.020791886374354362 Accuracy 0.783203125\n",
      "Iteration 89480 Training loss 0.013821189291775227 Validation loss 0.020924847573041916 Accuracy 0.7822265625\n",
      "Iteration 89490 Training loss 0.01409891527146101 Validation loss 0.020753873512148857 Accuracy 0.78271484375\n",
      "Iteration 89500 Training loss 0.014496801421046257 Validation loss 0.020893188193440437 Accuracy 0.78125\n",
      "Iteration 89510 Training loss 0.012862083502113819 Validation loss 0.020910579711198807 Accuracy 0.78173828125\n",
      "Iteration 89520 Training loss 0.012352709658443928 Validation loss 0.02093883790075779 Accuracy 0.78076171875\n",
      "Iteration 89530 Training loss 0.01307134609669447 Validation loss 0.020825326442718506 Accuracy 0.78173828125\n",
      "Iteration 89540 Training loss 0.01426728069782257 Validation loss 0.020971253514289856 Accuracy 0.78125\n",
      "Iteration 89550 Training loss 0.011046668514609337 Validation loss 0.0209987573325634 Accuracy 0.78125\n",
      "Iteration 89560 Training loss 0.013287564739584923 Validation loss 0.020752986893057823 Accuracy 0.78369140625\n",
      "Iteration 89570 Training loss 0.013890225440263748 Validation loss 0.021102817729115486 Accuracy 0.78076171875\n",
      "Iteration 89580 Training loss 0.012624984607100487 Validation loss 0.020873665809631348 Accuracy 0.78271484375\n",
      "Iteration 89590 Training loss 0.012578797526657581 Validation loss 0.020795371383428574 Accuracy 0.78173828125\n",
      "Iteration 89600 Training loss 0.011910146102309227 Validation loss 0.020864754915237427 Accuracy 0.783203125\n",
      "Iteration 89610 Training loss 0.013344320468604565 Validation loss 0.02087477222084999 Accuracy 0.78271484375\n",
      "Iteration 89620 Training loss 0.01479529682546854 Validation loss 0.020825577899813652 Accuracy 0.7822265625\n",
      "Iteration 89630 Training loss 0.012835792265832424 Validation loss 0.020819447934627533 Accuracy 0.78271484375\n",
      "Iteration 89640 Training loss 0.014195309951901436 Validation loss 0.020674610510468483 Accuracy 0.7841796875\n",
      "Iteration 89650 Training loss 0.01312063354998827 Validation loss 0.020839931443333626 Accuracy 0.7822265625\n",
      "Iteration 89660 Training loss 0.01481835637241602 Validation loss 0.02100277505815029 Accuracy 0.78173828125\n",
      "Iteration 89670 Training loss 0.01374124176800251 Validation loss 0.020864838734269142 Accuracy 0.78271484375\n",
      "Iteration 89680 Training loss 0.015104533173143864 Validation loss 0.020738057792186737 Accuracy 0.7841796875\n",
      "Iteration 89690 Training loss 0.014228719286620617 Validation loss 0.021116942167282104 Accuracy 0.78076171875\n",
      "Iteration 89700 Training loss 0.011807973496615887 Validation loss 0.020833482965826988 Accuracy 0.78369140625\n",
      "Iteration 89710 Training loss 0.015517634339630604 Validation loss 0.021335452795028687 Accuracy 0.78076171875\n",
      "Iteration 89720 Training loss 0.013683889992535114 Validation loss 0.020931782200932503 Accuracy 0.78271484375\n",
      "Iteration 89730 Training loss 0.01158097106963396 Validation loss 0.02084263414144516 Accuracy 0.78271484375\n",
      "Iteration 89740 Training loss 0.014428522437810898 Validation loss 0.020915599539875984 Accuracy 0.7822265625\n",
      "Iteration 89750 Training loss 0.014994300901889801 Validation loss 0.021177690476179123 Accuracy 0.77880859375\n",
      "Iteration 89760 Training loss 0.012168403714895248 Validation loss 0.020996274426579475 Accuracy 0.78173828125\n",
      "Iteration 89770 Training loss 0.015400020405650139 Validation loss 0.021003324538469315 Accuracy 0.7802734375\n",
      "Iteration 89780 Training loss 0.014754883013665676 Validation loss 0.02104637585580349 Accuracy 0.779296875\n",
      "Iteration 89790 Training loss 0.009844598360359669 Validation loss 0.020784961059689522 Accuracy 0.78271484375\n",
      "Iteration 89800 Training loss 0.010985389351844788 Validation loss 0.02081570215523243 Accuracy 0.7822265625\n",
      "Iteration 89810 Training loss 0.012298875488340855 Validation loss 0.02096155658364296 Accuracy 0.78271484375\n",
      "Iteration 89820 Training loss 0.01468908041715622 Validation loss 0.02096346579492092 Accuracy 0.7822265625\n",
      "Iteration 89830 Training loss 0.011913229711353779 Validation loss 0.02101590856909752 Accuracy 0.78125\n",
      "Iteration 89840 Training loss 0.011861874721944332 Validation loss 0.02079743891954422 Accuracy 0.7822265625\n",
      "Iteration 89850 Training loss 0.011812545359134674 Validation loss 0.020767703652381897 Accuracy 0.78369140625\n",
      "Iteration 89860 Training loss 0.014984535053372383 Validation loss 0.020967645570635796 Accuracy 0.78271484375\n",
      "Iteration 89870 Training loss 0.014266015030443668 Validation loss 0.020746180787682533 Accuracy 0.783203125\n",
      "Iteration 89880 Training loss 0.012826714664697647 Validation loss 0.020908154547214508 Accuracy 0.78125\n",
      "Iteration 89890 Training loss 0.016134588047862053 Validation loss 0.021174076944589615 Accuracy 0.78125\n",
      "Iteration 89900 Training loss 0.01360144279897213 Validation loss 0.02085217274725437 Accuracy 0.7841796875\n",
      "Iteration 89910 Training loss 0.01397445797920227 Validation loss 0.021382713690400124 Accuracy 0.77880859375\n",
      "Iteration 89920 Training loss 0.013915699906647205 Validation loss 0.020867636427283287 Accuracy 0.7822265625\n",
      "Iteration 89930 Training loss 0.015237199142575264 Validation loss 0.02117057517170906 Accuracy 0.77978515625\n",
      "Iteration 89940 Training loss 0.013165763579308987 Validation loss 0.020957035943865776 Accuracy 0.78271484375\n",
      "Iteration 89950 Training loss 0.014100813306868076 Validation loss 0.020810140296816826 Accuracy 0.7822265625\n",
      "Iteration 89960 Training loss 0.013937090523540974 Validation loss 0.020656971260905266 Accuracy 0.7841796875\n",
      "Iteration 89970 Training loss 0.014648175798356533 Validation loss 0.020864004269242287 Accuracy 0.7822265625\n",
      "Iteration 89980 Training loss 0.011496209539473057 Validation loss 0.02082890085875988 Accuracy 0.7822265625\n",
      "Iteration 89990 Training loss 0.014198125340044498 Validation loss 0.020896777510643005 Accuracy 0.78271484375\n",
      "Iteration 90000 Training loss 0.013549829833209515 Validation loss 0.02075584977865219 Accuracy 0.78369140625\n",
      "Iteration 90010 Training loss 0.013683151453733444 Validation loss 0.020979028195142746 Accuracy 0.78271484375\n",
      "Iteration 90020 Training loss 0.012386984191834927 Validation loss 0.020852187648415565 Accuracy 0.78271484375\n",
      "Iteration 90030 Training loss 0.014941596426069736 Validation loss 0.020762663334608078 Accuracy 0.7841796875\n",
      "Iteration 90040 Training loss 0.011184577830135822 Validation loss 0.02079789526760578 Accuracy 0.7822265625\n",
      "Iteration 90050 Training loss 0.01399234775453806 Validation loss 0.02075793966650963 Accuracy 0.7822265625\n",
      "Iteration 90060 Training loss 0.012924357317388058 Validation loss 0.020891863852739334 Accuracy 0.78125\n",
      "Iteration 90070 Training loss 0.014491836540400982 Validation loss 0.020817656069993973 Accuracy 0.78271484375\n",
      "Iteration 90080 Training loss 0.01361037790775299 Validation loss 0.020681709051132202 Accuracy 0.78369140625\n",
      "Iteration 90090 Training loss 0.012864489108324051 Validation loss 0.020693520084023476 Accuracy 0.78369140625\n",
      "Iteration 90100 Training loss 0.012201045639812946 Validation loss 0.020633045583963394 Accuracy 0.78369140625\n",
      "Iteration 90110 Training loss 0.01102312933653593 Validation loss 0.02092921733856201 Accuracy 0.78125\n",
      "Iteration 90120 Training loss 0.014314960688352585 Validation loss 0.020852550864219666 Accuracy 0.78271484375\n",
      "Iteration 90130 Training loss 0.011663067154586315 Validation loss 0.020897163078188896 Accuracy 0.78271484375\n",
      "Iteration 90140 Training loss 0.014060878194868565 Validation loss 0.020962050184607506 Accuracy 0.78125\n",
      "Iteration 90150 Training loss 0.014984511770308018 Validation loss 0.02128920890390873 Accuracy 0.77978515625\n",
      "Iteration 90160 Training loss 0.013984899036586285 Validation loss 0.020831024274230003 Accuracy 0.78271484375\n",
      "Iteration 90170 Training loss 0.011327235959470272 Validation loss 0.021042317152023315 Accuracy 0.78173828125\n",
      "Iteration 90180 Training loss 0.014410057105123997 Validation loss 0.020905619487166405 Accuracy 0.783203125\n",
      "Iteration 90190 Training loss 0.011564915999770164 Validation loss 0.0208049938082695 Accuracy 0.783203125\n",
      "Iteration 90200 Training loss 0.012483561411499977 Validation loss 0.02081877924501896 Accuracy 0.7822265625\n",
      "Iteration 90210 Training loss 0.010170325636863708 Validation loss 0.02074526809155941 Accuracy 0.783203125\n",
      "Iteration 90220 Training loss 0.013330509886145592 Validation loss 0.0207341518253088 Accuracy 0.78369140625\n",
      "Iteration 90230 Training loss 0.012758663855493069 Validation loss 0.02076772414147854 Accuracy 0.783203125\n",
      "Iteration 90240 Training loss 0.01253716740757227 Validation loss 0.02073790691792965 Accuracy 0.78369140625\n",
      "Iteration 90250 Training loss 0.013406883925199509 Validation loss 0.020707348361611366 Accuracy 0.7841796875\n",
      "Iteration 90260 Training loss 0.013230410404503345 Validation loss 0.021093226969242096 Accuracy 0.78076171875\n",
      "Iteration 90270 Training loss 0.013477603904902935 Validation loss 0.020909296348690987 Accuracy 0.78173828125\n",
      "Iteration 90280 Training loss 0.012006543576717377 Validation loss 0.0207597017288208 Accuracy 0.78369140625\n",
      "Iteration 90290 Training loss 0.013852625153958797 Validation loss 0.020774345844984055 Accuracy 0.783203125\n",
      "Iteration 90300 Training loss 0.015959622338414192 Validation loss 0.020938675850629807 Accuracy 0.7822265625\n",
      "Iteration 90310 Training loss 0.013908350840210915 Validation loss 0.020979927852749825 Accuracy 0.7822265625\n",
      "Iteration 90320 Training loss 0.012484829872846603 Validation loss 0.02075502835214138 Accuracy 0.783203125\n",
      "Iteration 90330 Training loss 0.012182023376226425 Validation loss 0.020867686718702316 Accuracy 0.7822265625\n",
      "Iteration 90340 Training loss 0.012557173147797585 Validation loss 0.02086338773369789 Accuracy 0.78173828125\n",
      "Iteration 90350 Training loss 0.013021382503211498 Validation loss 0.020819637924432755 Accuracy 0.7841796875\n",
      "Iteration 90360 Training loss 0.01253837812691927 Validation loss 0.020697426050901413 Accuracy 0.78369140625\n",
      "Iteration 90370 Training loss 0.012309291400015354 Validation loss 0.02097487449645996 Accuracy 0.78173828125\n",
      "Iteration 90380 Training loss 0.013267908245325089 Validation loss 0.020832806825637817 Accuracy 0.783203125\n",
      "Iteration 90390 Training loss 0.011952867731451988 Validation loss 0.020714106038212776 Accuracy 0.78369140625\n",
      "Iteration 90400 Training loss 0.013483448885381222 Validation loss 0.020783061161637306 Accuracy 0.7841796875\n",
      "Iteration 90410 Training loss 0.012169569730758667 Validation loss 0.020696943625807762 Accuracy 0.78369140625\n",
      "Iteration 90420 Training loss 0.012979329563677311 Validation loss 0.02072310447692871 Accuracy 0.783203125\n",
      "Iteration 90430 Training loss 0.013061795383691788 Validation loss 0.02074386179447174 Accuracy 0.7841796875\n",
      "Iteration 90440 Training loss 0.012122627347707748 Validation loss 0.02091362327337265 Accuracy 0.78271484375\n",
      "Iteration 90450 Training loss 0.013587345369160175 Validation loss 0.020743506029248238 Accuracy 0.783203125\n",
      "Iteration 90460 Training loss 0.01264245342463255 Validation loss 0.020639920607209206 Accuracy 0.78369140625\n",
      "Iteration 90470 Training loss 0.013034427538514137 Validation loss 0.020843949168920517 Accuracy 0.7841796875\n",
      "Iteration 90480 Training loss 0.012999013066291809 Validation loss 0.02088453248143196 Accuracy 0.78369140625\n",
      "Iteration 90490 Training loss 0.014551959000527859 Validation loss 0.020761771127581596 Accuracy 0.78466796875\n",
      "Iteration 90500 Training loss 0.01507109310477972 Validation loss 0.020855404436588287 Accuracy 0.783203125\n",
      "Iteration 90510 Training loss 0.013427169993519783 Validation loss 0.020878611132502556 Accuracy 0.783203125\n",
      "Iteration 90520 Training loss 0.013589153997600079 Validation loss 0.020953642204403877 Accuracy 0.78076171875\n",
      "Iteration 90530 Training loss 0.012333273887634277 Validation loss 0.020693065598607063 Accuracy 0.7841796875\n",
      "Iteration 90540 Training loss 0.014718208461999893 Validation loss 0.020837239921092987 Accuracy 0.783203125\n",
      "Iteration 90550 Training loss 0.012612276710569859 Validation loss 0.020808175206184387 Accuracy 0.78369140625\n",
      "Iteration 90560 Training loss 0.01337833609431982 Validation loss 0.020779622718691826 Accuracy 0.78369140625\n",
      "Iteration 90570 Training loss 0.012634184211492538 Validation loss 0.0211752038449049 Accuracy 0.78173828125\n",
      "Iteration 90580 Training loss 0.011343419551849365 Validation loss 0.020856501534581184 Accuracy 0.78173828125\n",
      "Iteration 90590 Training loss 0.013207568787038326 Validation loss 0.02087436057627201 Accuracy 0.783203125\n",
      "Iteration 90600 Training loss 0.011118358932435513 Validation loss 0.020947983488440514 Accuracy 0.7822265625\n",
      "Iteration 90610 Training loss 0.014138071797788143 Validation loss 0.02105013094842434 Accuracy 0.78173828125\n",
      "Iteration 90620 Training loss 0.015536963939666748 Validation loss 0.02085660584270954 Accuracy 0.78369140625\n",
      "Iteration 90630 Training loss 0.012488709762692451 Validation loss 0.020735252648591995 Accuracy 0.7841796875\n",
      "Iteration 90640 Training loss 0.012021194212138653 Validation loss 0.02085716836154461 Accuracy 0.78369140625\n",
      "Iteration 90650 Training loss 0.013988839462399483 Validation loss 0.02085869386792183 Accuracy 0.78173828125\n",
      "Iteration 90660 Training loss 0.014463167637586594 Validation loss 0.020788807421922684 Accuracy 0.7841796875\n",
      "Iteration 90670 Training loss 0.011519823223352432 Validation loss 0.0206390880048275 Accuracy 0.78515625\n",
      "Iteration 90680 Training loss 0.013672525994479656 Validation loss 0.021254420280456543 Accuracy 0.7802734375\n",
      "Iteration 90690 Training loss 0.012745470739901066 Validation loss 0.020655062049627304 Accuracy 0.7841796875\n",
      "Iteration 90700 Training loss 0.014004317112267017 Validation loss 0.020540466532111168 Accuracy 0.78466796875\n",
      "Iteration 90710 Training loss 0.015955200418829918 Validation loss 0.020797211676836014 Accuracy 0.78369140625\n",
      "Iteration 90720 Training loss 0.012660511769354343 Validation loss 0.020934905856847763 Accuracy 0.7822265625\n",
      "Iteration 90730 Training loss 0.013745727017521858 Validation loss 0.020575614646077156 Accuracy 0.78466796875\n",
      "Iteration 90740 Training loss 0.013200992718338966 Validation loss 0.02095160260796547 Accuracy 0.78173828125\n",
      "Iteration 90750 Training loss 0.012439107522368431 Validation loss 0.020818138495087624 Accuracy 0.78369140625\n",
      "Iteration 90760 Training loss 0.012322545051574707 Validation loss 0.020723309367895126 Accuracy 0.78369140625\n",
      "Iteration 90770 Training loss 0.012589426711201668 Validation loss 0.02069825679063797 Accuracy 0.78369140625\n",
      "Iteration 90780 Training loss 0.013559697195887566 Validation loss 0.020559823140501976 Accuracy 0.78515625\n",
      "Iteration 90790 Training loss 0.012219508178532124 Validation loss 0.02073688991367817 Accuracy 0.78515625\n",
      "Iteration 90800 Training loss 0.013691937550902367 Validation loss 0.021216796711087227 Accuracy 0.78125\n",
      "Iteration 90810 Training loss 0.013394704088568687 Validation loss 0.020765285938978195 Accuracy 0.783203125\n",
      "Iteration 90820 Training loss 0.014159788377583027 Validation loss 0.02079036645591259 Accuracy 0.78271484375\n",
      "Iteration 90830 Training loss 0.016163567081093788 Validation loss 0.020748034119606018 Accuracy 0.7841796875\n",
      "Iteration 90840 Training loss 0.012735051102936268 Validation loss 0.020625371485948563 Accuracy 0.78515625\n",
      "Iteration 90850 Training loss 0.013388467952609062 Validation loss 0.020722119137644768 Accuracy 0.7841796875\n",
      "Iteration 90860 Training loss 0.01201398205012083 Validation loss 0.02061142772436142 Accuracy 0.78515625\n",
      "Iteration 90870 Training loss 0.010087350383400917 Validation loss 0.020708847790956497 Accuracy 0.7841796875\n",
      "Iteration 90880 Training loss 0.01306887250393629 Validation loss 0.020984262228012085 Accuracy 0.7822265625\n",
      "Iteration 90890 Training loss 0.011491846293210983 Validation loss 0.020669367164373398 Accuracy 0.7841796875\n",
      "Iteration 90900 Training loss 0.014575307257473469 Validation loss 0.02069919928908348 Accuracy 0.78564453125\n",
      "Iteration 90910 Training loss 0.012461718171834946 Validation loss 0.020805248990654945 Accuracy 0.78369140625\n",
      "Iteration 90920 Training loss 0.012015864253044128 Validation loss 0.02072344906628132 Accuracy 0.78369140625\n",
      "Iteration 90930 Training loss 0.016239063814282417 Validation loss 0.02079860121011734 Accuracy 0.78466796875\n",
      "Iteration 90940 Training loss 0.01149227935820818 Validation loss 0.020824117586016655 Accuracy 0.78271484375\n",
      "Iteration 90950 Training loss 0.013990729115903378 Validation loss 0.02090117149055004 Accuracy 0.78173828125\n",
      "Iteration 90960 Training loss 0.014221244491636753 Validation loss 0.02079491689801216 Accuracy 0.78369140625\n",
      "Iteration 90970 Training loss 0.011075054295361042 Validation loss 0.020677214488387108 Accuracy 0.7822265625\n",
      "Iteration 90980 Training loss 0.013487271033227444 Validation loss 0.020846955478191376 Accuracy 0.783203125\n",
      "Iteration 90990 Training loss 0.01245157141238451 Validation loss 0.020952777937054634 Accuracy 0.78173828125\n",
      "Iteration 91000 Training loss 0.013268770650029182 Validation loss 0.020817207172513008 Accuracy 0.783203125\n",
      "Iteration 91010 Training loss 0.012197853066027164 Validation loss 0.020587284117937088 Accuracy 0.7841796875\n",
      "Iteration 91020 Training loss 0.014389348216354847 Validation loss 0.020741339772939682 Accuracy 0.783203125\n",
      "Iteration 91030 Training loss 0.011872980743646622 Validation loss 0.020691046491265297 Accuracy 0.78369140625\n",
      "Iteration 91040 Training loss 0.013423725962638855 Validation loss 0.02091403305530548 Accuracy 0.78271484375\n",
      "Iteration 91050 Training loss 0.013149216771125793 Validation loss 0.020704159513115883 Accuracy 0.783203125\n",
      "Iteration 91060 Training loss 0.011862078681588173 Validation loss 0.020724641159176826 Accuracy 0.783203125\n",
      "Iteration 91070 Training loss 0.01328895054757595 Validation loss 0.0208125002682209 Accuracy 0.783203125\n",
      "Iteration 91080 Training loss 0.013633175753057003 Validation loss 0.020929565653204918 Accuracy 0.78173828125\n",
      "Iteration 91090 Training loss 0.01292815338820219 Validation loss 0.020733224228024483 Accuracy 0.78466796875\n",
      "Iteration 91100 Training loss 0.0134800486266613 Validation loss 0.02089441381394863 Accuracy 0.78271484375\n",
      "Iteration 91110 Training loss 0.013632488436996937 Validation loss 0.02085018903017044 Accuracy 0.78466796875\n",
      "Iteration 91120 Training loss 0.013266662135720253 Validation loss 0.020831650123000145 Accuracy 0.78271484375\n",
      "Iteration 91130 Training loss 0.013261969201266766 Validation loss 0.020785853266716003 Accuracy 0.7841796875\n",
      "Iteration 91140 Training loss 0.012928934767842293 Validation loss 0.020926568657159805 Accuracy 0.7841796875\n",
      "Iteration 91150 Training loss 0.013532690703868866 Validation loss 0.02065884694457054 Accuracy 0.78466796875\n",
      "Iteration 91160 Training loss 0.013494951650500298 Validation loss 0.02101953886449337 Accuracy 0.7822265625\n",
      "Iteration 91170 Training loss 0.013705103658139706 Validation loss 0.020939521491527557 Accuracy 0.78271484375\n",
      "Iteration 91180 Training loss 0.012307114899158478 Validation loss 0.020934393629431725 Accuracy 0.78271484375\n",
      "Iteration 91190 Training loss 0.01502526830881834 Validation loss 0.020612118765711784 Accuracy 0.78466796875\n",
      "Iteration 91200 Training loss 0.014204566366970539 Validation loss 0.02085130289196968 Accuracy 0.783203125\n",
      "Iteration 91210 Training loss 0.013452362269163132 Validation loss 0.02087540179491043 Accuracy 0.7841796875\n",
      "Iteration 91220 Training loss 0.01478295773267746 Validation loss 0.02059469185769558 Accuracy 0.7861328125\n",
      "Iteration 91230 Training loss 0.013880493119359016 Validation loss 0.02071393094956875 Accuracy 0.7841796875\n",
      "Iteration 91240 Training loss 0.014811215922236443 Validation loss 0.020768731832504272 Accuracy 0.78466796875\n",
      "Iteration 91250 Training loss 0.012985341250896454 Validation loss 0.0208653025329113 Accuracy 0.78173828125\n",
      "Iteration 91260 Training loss 0.01353143434971571 Validation loss 0.020658813416957855 Accuracy 0.7841796875\n",
      "Iteration 91270 Training loss 0.014452606439590454 Validation loss 0.020813768729567528 Accuracy 0.78271484375\n",
      "Iteration 91280 Training loss 0.013272946700453758 Validation loss 0.020743241533637047 Accuracy 0.78466796875\n",
      "Iteration 91290 Training loss 0.012341736815869808 Validation loss 0.020690307021141052 Accuracy 0.78369140625\n",
      "Iteration 91300 Training loss 0.016048990190029144 Validation loss 0.020985273644328117 Accuracy 0.78125\n",
      "Iteration 91310 Training loss 0.014422712847590446 Validation loss 0.02068118192255497 Accuracy 0.78466796875\n",
      "Iteration 91320 Training loss 0.012715784832835197 Validation loss 0.020940903574228287 Accuracy 0.7822265625\n",
      "Iteration 91330 Training loss 0.012636289931833744 Validation loss 0.020625561475753784 Accuracy 0.78564453125\n",
      "Iteration 91340 Training loss 0.012614592909812927 Validation loss 0.020713377743959427 Accuracy 0.78466796875\n",
      "Iteration 91350 Training loss 0.014050988480448723 Validation loss 0.02084503136575222 Accuracy 0.7822265625\n",
      "Iteration 91360 Training loss 0.014482907019555569 Validation loss 0.020899586379528046 Accuracy 0.78271484375\n",
      "Iteration 91370 Training loss 0.01436094380915165 Validation loss 0.020790323615074158 Accuracy 0.78271484375\n",
      "Iteration 91380 Training loss 0.013327282853424549 Validation loss 0.02069091796875 Accuracy 0.7841796875\n",
      "Iteration 91390 Training loss 0.014775012619793415 Validation loss 0.020905831828713417 Accuracy 0.78173828125\n",
      "Iteration 91400 Training loss 0.01148388534784317 Validation loss 0.020855648443102837 Accuracy 0.78369140625\n",
      "Iteration 91410 Training loss 0.012928931042551994 Validation loss 0.020886925980448723 Accuracy 0.7822265625\n",
      "Iteration 91420 Training loss 0.012494060210883617 Validation loss 0.020873215049505234 Accuracy 0.783203125\n",
      "Iteration 91430 Training loss 0.01302630826830864 Validation loss 0.020730411633849144 Accuracy 0.78369140625\n",
      "Iteration 91440 Training loss 0.01294406782835722 Validation loss 0.020737770944833755 Accuracy 0.7841796875\n",
      "Iteration 91450 Training loss 0.01452119555324316 Validation loss 0.02072148397564888 Accuracy 0.783203125\n",
      "Iteration 91460 Training loss 0.013556044548749924 Validation loss 0.020644977688789368 Accuracy 0.78466796875\n",
      "Iteration 91470 Training loss 0.01325004268437624 Validation loss 0.02111492119729519 Accuracy 0.7822265625\n",
      "Iteration 91480 Training loss 0.01243438571691513 Validation loss 0.020721595734357834 Accuracy 0.7841796875\n",
      "Iteration 91490 Training loss 0.013501814566552639 Validation loss 0.020726727321743965 Accuracy 0.783203125\n",
      "Iteration 91500 Training loss 0.014681125059723854 Validation loss 0.020905038341879845 Accuracy 0.78271484375\n",
      "Iteration 91510 Training loss 0.015170547179877758 Validation loss 0.020912889391183853 Accuracy 0.78173828125\n",
      "Iteration 91520 Training loss 0.013978353701531887 Validation loss 0.020941434428095818 Accuracy 0.78271484375\n",
      "Iteration 91530 Training loss 0.011788942851126194 Validation loss 0.020894506946206093 Accuracy 0.783203125\n",
      "Iteration 91540 Training loss 0.011875800788402557 Validation loss 0.021107157692313194 Accuracy 0.78125\n",
      "Iteration 91550 Training loss 0.012838994152843952 Validation loss 0.020705990493297577 Accuracy 0.783203125\n",
      "Iteration 91560 Training loss 0.013044364750385284 Validation loss 0.020858509466052055 Accuracy 0.783203125\n",
      "Iteration 91570 Training loss 0.012877182103693485 Validation loss 0.020716827362775803 Accuracy 0.78369140625\n",
      "Iteration 91580 Training loss 0.014665771275758743 Validation loss 0.02078411541879177 Accuracy 0.7841796875\n",
      "Iteration 91590 Training loss 0.013368673622608185 Validation loss 0.020982829853892326 Accuracy 0.78125\n",
      "Iteration 91600 Training loss 0.01215402688831091 Validation loss 0.020651420578360558 Accuracy 0.78466796875\n",
      "Iteration 91610 Training loss 0.012541048228740692 Validation loss 0.02062595635652542 Accuracy 0.78466796875\n",
      "Iteration 91620 Training loss 0.014420879073441029 Validation loss 0.020530788227915764 Accuracy 0.78515625\n",
      "Iteration 91630 Training loss 0.01390852965414524 Validation loss 0.021412713453173637 Accuracy 0.77783203125\n",
      "Iteration 91640 Training loss 0.011429153382778168 Validation loss 0.02073274366557598 Accuracy 0.783203125\n",
      "Iteration 91650 Training loss 0.013225728645920753 Validation loss 0.020677508786320686 Accuracy 0.78369140625\n",
      "Iteration 91660 Training loss 0.015362566336989403 Validation loss 0.020939160138368607 Accuracy 0.78076171875\n",
      "Iteration 91670 Training loss 0.014318244531750679 Validation loss 0.02085896208882332 Accuracy 0.783203125\n",
      "Iteration 91680 Training loss 0.01508135162293911 Validation loss 0.021190384402871132 Accuracy 0.7783203125\n",
      "Iteration 91690 Training loss 0.015371880494058132 Validation loss 0.02075827121734619 Accuracy 0.783203125\n",
      "Iteration 91700 Training loss 0.013183465227484703 Validation loss 0.020777590572834015 Accuracy 0.7822265625\n",
      "Iteration 91710 Training loss 0.01241002045571804 Validation loss 0.020881159231066704 Accuracy 0.7822265625\n",
      "Iteration 91720 Training loss 0.014771893620491028 Validation loss 0.020709417760372162 Accuracy 0.78369140625\n",
      "Iteration 91730 Training loss 0.012559915892779827 Validation loss 0.020944952964782715 Accuracy 0.7822265625\n",
      "Iteration 91740 Training loss 0.014015641063451767 Validation loss 0.02074737660586834 Accuracy 0.783203125\n",
      "Iteration 91750 Training loss 0.01258155982941389 Validation loss 0.02097063511610031 Accuracy 0.78125\n",
      "Iteration 91760 Training loss 0.015302696265280247 Validation loss 0.020926810801029205 Accuracy 0.78173828125\n",
      "Iteration 91770 Training loss 0.015077896416187286 Validation loss 0.02069389633834362 Accuracy 0.7841796875\n",
      "Iteration 91780 Training loss 0.014949914999306202 Validation loss 0.020920008420944214 Accuracy 0.78125\n",
      "Iteration 91790 Training loss 0.01139372494071722 Validation loss 0.020718222483992577 Accuracy 0.783203125\n",
      "Iteration 91800 Training loss 0.011497083120048046 Validation loss 0.020556718111038208 Accuracy 0.78564453125\n",
      "Iteration 91810 Training loss 0.012084515765309334 Validation loss 0.020947866141796112 Accuracy 0.7822265625\n",
      "Iteration 91820 Training loss 0.012927178293466568 Validation loss 0.021066997200250626 Accuracy 0.78125\n",
      "Iteration 91830 Training loss 0.01336650364100933 Validation loss 0.020739011466503143 Accuracy 0.783203125\n",
      "Iteration 91840 Training loss 0.012165853753685951 Validation loss 0.021076049655675888 Accuracy 0.78076171875\n",
      "Iteration 91850 Training loss 0.011776954866945744 Validation loss 0.020749598741531372 Accuracy 0.78271484375\n",
      "Iteration 91860 Training loss 0.014987451955676079 Validation loss 0.020856883376836777 Accuracy 0.7822265625\n",
      "Iteration 91870 Training loss 0.013557514175772667 Validation loss 0.020866449922323227 Accuracy 0.783203125\n",
      "Iteration 91880 Training loss 0.011884518899023533 Validation loss 0.02078801766037941 Accuracy 0.783203125\n",
      "Iteration 91890 Training loss 0.01736637018620968 Validation loss 0.02163109742105007 Accuracy 0.775390625\n",
      "Iteration 91900 Training loss 0.011119097471237183 Validation loss 0.020851820707321167 Accuracy 0.7841796875\n",
      "Iteration 91910 Training loss 0.013541456311941147 Validation loss 0.020934173837304115 Accuracy 0.78369140625\n",
      "Iteration 91920 Training loss 0.0131316427141428 Validation loss 0.020761987194418907 Accuracy 0.7841796875\n",
      "Iteration 91930 Training loss 0.011720672249794006 Validation loss 0.02074453793466091 Accuracy 0.78466796875\n",
      "Iteration 91940 Training loss 0.013999730348587036 Validation loss 0.020933138206601143 Accuracy 0.78076171875\n",
      "Iteration 91950 Training loss 0.012755581177771091 Validation loss 0.020826557651162148 Accuracy 0.7822265625\n",
      "Iteration 91960 Training loss 0.012181894853711128 Validation loss 0.020832866430282593 Accuracy 0.78271484375\n",
      "Iteration 91970 Training loss 0.01246315985918045 Validation loss 0.020672425627708435 Accuracy 0.78369140625\n",
      "Iteration 91980 Training loss 0.012364290654659271 Validation loss 0.020765986293554306 Accuracy 0.7841796875\n",
      "Iteration 91990 Training loss 0.011920089833438396 Validation loss 0.020558321848511696 Accuracy 0.78564453125\n",
      "Iteration 92000 Training loss 0.013719920068979263 Validation loss 0.020668864250183105 Accuracy 0.78515625\n",
      "Iteration 92010 Training loss 0.014832197688519955 Validation loss 0.020678363740444183 Accuracy 0.78466796875\n",
      "Iteration 92020 Training loss 0.0125726368278265 Validation loss 0.020799702033400536 Accuracy 0.78271484375\n",
      "Iteration 92030 Training loss 0.011032957583665848 Validation loss 0.020908702164888382 Accuracy 0.7822265625\n",
      "Iteration 92040 Training loss 0.011992618441581726 Validation loss 0.020795997232198715 Accuracy 0.78271484375\n",
      "Iteration 92050 Training loss 0.01142879482358694 Validation loss 0.020675942301750183 Accuracy 0.7841796875\n",
      "Iteration 92060 Training loss 0.013019151985645294 Validation loss 0.02076536975800991 Accuracy 0.7841796875\n",
      "Iteration 92070 Training loss 0.013846103101968765 Validation loss 0.02074679173529148 Accuracy 0.7822265625\n",
      "Iteration 92080 Training loss 0.014834494329988956 Validation loss 0.021134227514266968 Accuracy 0.78173828125\n",
      "Iteration 92090 Training loss 0.011387470178306103 Validation loss 0.02091815695166588 Accuracy 0.7822265625\n",
      "Iteration 92100 Training loss 0.011461842805147171 Validation loss 0.020910555496811867 Accuracy 0.78271484375\n",
      "Iteration 92110 Training loss 0.013368982821702957 Validation loss 0.021416686475276947 Accuracy 0.779296875\n",
      "Iteration 92120 Training loss 0.01188351958990097 Validation loss 0.020847568288445473 Accuracy 0.7822265625\n",
      "Iteration 92130 Training loss 0.013291946612298489 Validation loss 0.020694833248853683 Accuracy 0.78466796875\n",
      "Iteration 92140 Training loss 0.013253618963062763 Validation loss 0.020606959238648415 Accuracy 0.78466796875\n",
      "Iteration 92150 Training loss 0.011536107398569584 Validation loss 0.020749643445014954 Accuracy 0.783203125\n",
      "Iteration 92160 Training loss 0.012058359570801258 Validation loss 0.020617248490452766 Accuracy 0.78515625\n",
      "Iteration 92170 Training loss 0.014778882265090942 Validation loss 0.020701253786683083 Accuracy 0.7841796875\n",
      "Iteration 92180 Training loss 0.0113933514803648 Validation loss 0.020642757415771484 Accuracy 0.78369140625\n",
      "Iteration 92190 Training loss 0.015784675255417824 Validation loss 0.02091791480779648 Accuracy 0.78271484375\n",
      "Iteration 92200 Training loss 0.01355141680687666 Validation loss 0.02074732817709446 Accuracy 0.78466796875\n",
      "Iteration 92210 Training loss 0.014135854318737984 Validation loss 0.020652810111641884 Accuracy 0.7841796875\n",
      "Iteration 92220 Training loss 0.014663293957710266 Validation loss 0.02065037377178669 Accuracy 0.7841796875\n",
      "Iteration 92230 Training loss 0.013731775805354118 Validation loss 0.020781483501195908 Accuracy 0.7822265625\n",
      "Iteration 92240 Training loss 0.014945325441658497 Validation loss 0.02072177641093731 Accuracy 0.7822265625\n",
      "Iteration 92250 Training loss 0.015364434570074081 Validation loss 0.020708221942186356 Accuracy 0.78369140625\n",
      "Iteration 92260 Training loss 0.013349550776183605 Validation loss 0.020691314712166786 Accuracy 0.78369140625\n",
      "Iteration 92270 Training loss 0.011686948128044605 Validation loss 0.02066398784518242 Accuracy 0.78515625\n",
      "Iteration 92280 Training loss 0.011847008019685745 Validation loss 0.020675960928201675 Accuracy 0.7841796875\n",
      "Iteration 92290 Training loss 0.013531145639717579 Validation loss 0.020807631313800812 Accuracy 0.7841796875\n",
      "Iteration 92300 Training loss 0.015443201176822186 Validation loss 0.020711731165647507 Accuracy 0.783203125\n",
      "Iteration 92310 Training loss 0.01323231216520071 Validation loss 0.02101558819413185 Accuracy 0.78173828125\n",
      "Iteration 92320 Training loss 0.012527957558631897 Validation loss 0.0205571036785841 Accuracy 0.78466796875\n",
      "Iteration 92330 Training loss 0.015582618303596973 Validation loss 0.020624574273824692 Accuracy 0.78369140625\n",
      "Iteration 92340 Training loss 0.012748206965625286 Validation loss 0.020693151280283928 Accuracy 0.78369140625\n",
      "Iteration 92350 Training loss 0.011758839711546898 Validation loss 0.020914852619171143 Accuracy 0.7822265625\n",
      "Iteration 92360 Training loss 0.011113528162240982 Validation loss 0.020698755979537964 Accuracy 0.7841796875\n",
      "Iteration 92370 Training loss 0.014328223653137684 Validation loss 0.020752258598804474 Accuracy 0.783203125\n",
      "Iteration 92380 Training loss 0.012240685522556305 Validation loss 0.02078276313841343 Accuracy 0.78369140625\n",
      "Iteration 92390 Training loss 0.011501842178404331 Validation loss 0.020687727257609367 Accuracy 0.78466796875\n",
      "Iteration 92400 Training loss 0.014375912956893444 Validation loss 0.020803434774279594 Accuracy 0.78271484375\n",
      "Iteration 92410 Training loss 0.01262898463755846 Validation loss 0.020683666691184044 Accuracy 0.78369140625\n",
      "Iteration 92420 Training loss 0.01345392968505621 Validation loss 0.02061617560684681 Accuracy 0.7841796875\n",
      "Iteration 92430 Training loss 0.011709826067090034 Validation loss 0.020744552835822105 Accuracy 0.783203125\n",
      "Iteration 92440 Training loss 0.013371537439525127 Validation loss 0.02090376988053322 Accuracy 0.78271484375\n",
      "Iteration 92450 Training loss 0.013205462135374546 Validation loss 0.02083151787519455 Accuracy 0.78271484375\n",
      "Iteration 92460 Training loss 0.015057377517223358 Validation loss 0.020720116794109344 Accuracy 0.78466796875\n",
      "Iteration 92470 Training loss 0.013524821028113365 Validation loss 0.020712368190288544 Accuracy 0.7841796875\n",
      "Iteration 92480 Training loss 0.012243643403053284 Validation loss 0.02099708467721939 Accuracy 0.78076171875\n",
      "Iteration 92490 Training loss 0.014647038653492928 Validation loss 0.020856322720646858 Accuracy 0.78271484375\n",
      "Iteration 92500 Training loss 0.011700261384248734 Validation loss 0.020716235041618347 Accuracy 0.78466796875\n",
      "Iteration 92510 Training loss 0.013959783129394054 Validation loss 0.020677531138062477 Accuracy 0.78466796875\n",
      "Iteration 92520 Training loss 0.01356169581413269 Validation loss 0.020659858360886574 Accuracy 0.78466796875\n",
      "Iteration 92530 Training loss 0.013048802502453327 Validation loss 0.020939575508236885 Accuracy 0.78173828125\n",
      "Iteration 92540 Training loss 0.010805008001625538 Validation loss 0.020722681656479836 Accuracy 0.78369140625\n",
      "Iteration 92550 Training loss 0.013896810822188854 Validation loss 0.020667757838964462 Accuracy 0.78369140625\n",
      "Iteration 92560 Training loss 0.012691406533122063 Validation loss 0.0208564642816782 Accuracy 0.7822265625\n",
      "Iteration 92570 Training loss 0.01507605705410242 Validation loss 0.020962806418538094 Accuracy 0.78076171875\n",
      "Iteration 92580 Training loss 0.010729296132922173 Validation loss 0.020843900740146637 Accuracy 0.78173828125\n",
      "Iteration 92590 Training loss 0.0133481090888381 Validation loss 0.02070942334830761 Accuracy 0.78369140625\n",
      "Iteration 92600 Training loss 0.013352996669709682 Validation loss 0.02077089250087738 Accuracy 0.78369140625\n",
      "Iteration 92610 Training loss 0.013818643987178802 Validation loss 0.020787645131349564 Accuracy 0.78271484375\n",
      "Iteration 92620 Training loss 0.013978930190205574 Validation loss 0.021009067073464394 Accuracy 0.78173828125\n",
      "Iteration 92630 Training loss 0.013190251775085926 Validation loss 0.02083154208958149 Accuracy 0.78271484375\n",
      "Iteration 92640 Training loss 0.011016067117452621 Validation loss 0.020724449306726456 Accuracy 0.78369140625\n",
      "Iteration 92650 Training loss 0.013900653459131718 Validation loss 0.020719049498438835 Accuracy 0.7841796875\n",
      "Iteration 92660 Training loss 0.016259673982858658 Validation loss 0.0208636075258255 Accuracy 0.783203125\n",
      "Iteration 92670 Training loss 0.012529662810266018 Validation loss 0.021095868200063705 Accuracy 0.78076171875\n",
      "Iteration 92680 Training loss 0.013241175562143326 Validation loss 0.020643314346671104 Accuracy 0.7841796875\n",
      "Iteration 92690 Training loss 0.013836940750479698 Validation loss 0.02103678323328495 Accuracy 0.78076171875\n",
      "Iteration 92700 Training loss 0.013701891526579857 Validation loss 0.020764222368597984 Accuracy 0.7822265625\n",
      "Iteration 92710 Training loss 0.01296781376004219 Validation loss 0.021169431507587433 Accuracy 0.7802734375\n",
      "Iteration 92720 Training loss 0.011715962551534176 Validation loss 0.020760932937264442 Accuracy 0.783203125\n",
      "Iteration 92730 Training loss 0.011667554266750813 Validation loss 0.021024106070399284 Accuracy 0.78173828125\n",
      "Iteration 92740 Training loss 0.012812263332307339 Validation loss 0.020683299750089645 Accuracy 0.78369140625\n",
      "Iteration 92750 Training loss 0.014231648296117783 Validation loss 0.020738236606121063 Accuracy 0.783203125\n",
      "Iteration 92760 Training loss 0.014747105538845062 Validation loss 0.0211215578019619 Accuracy 0.78173828125\n",
      "Iteration 92770 Training loss 0.012002159841358662 Validation loss 0.020760653540492058 Accuracy 0.78369140625\n",
      "Iteration 92780 Training loss 0.009804108180105686 Validation loss 0.020714202895760536 Accuracy 0.78369140625\n",
      "Iteration 92790 Training loss 0.01297302357852459 Validation loss 0.021022077649831772 Accuracy 0.78173828125\n",
      "Iteration 92800 Training loss 0.014463561587035656 Validation loss 0.020723773166537285 Accuracy 0.7841796875\n",
      "Iteration 92810 Training loss 0.012455887161195278 Validation loss 0.02069546841084957 Accuracy 0.7841796875\n",
      "Iteration 92820 Training loss 0.012966041453182697 Validation loss 0.020685849711298943 Accuracy 0.78466796875\n",
      "Iteration 92830 Training loss 0.01349570695310831 Validation loss 0.020629681646823883 Accuracy 0.78369140625\n",
      "Iteration 92840 Training loss 0.013696292415261269 Validation loss 0.020830361172556877 Accuracy 0.783203125\n",
      "Iteration 92850 Training loss 0.012941482476890087 Validation loss 0.021033432334661484 Accuracy 0.78173828125\n",
      "Iteration 92860 Training loss 0.012402277439832687 Validation loss 0.02076401747763157 Accuracy 0.783203125\n",
      "Iteration 92870 Training loss 0.014424318447709084 Validation loss 0.020786762237548828 Accuracy 0.78515625\n",
      "Iteration 92880 Training loss 0.012235760688781738 Validation loss 0.02055441401898861 Accuracy 0.78466796875\n",
      "Iteration 92890 Training loss 0.014675005339086056 Validation loss 0.021366124972701073 Accuracy 0.77685546875\n",
      "Iteration 92900 Training loss 0.011893142946064472 Validation loss 0.02080460824072361 Accuracy 0.78369140625\n",
      "Iteration 92910 Training loss 0.012652113102376461 Validation loss 0.020643027499318123 Accuracy 0.78564453125\n",
      "Iteration 92920 Training loss 0.012973703444004059 Validation loss 0.02074359357357025 Accuracy 0.78466796875\n",
      "Iteration 92930 Training loss 0.014216219075024128 Validation loss 0.021077977493405342 Accuracy 0.779296875\n",
      "Iteration 92940 Training loss 0.011679756455123425 Validation loss 0.020656263455748558 Accuracy 0.78466796875\n",
      "Iteration 92950 Training loss 0.014369224198162556 Validation loss 0.020775560289621353 Accuracy 0.78369140625\n",
      "Iteration 92960 Training loss 0.011716730892658234 Validation loss 0.020771479234099388 Accuracy 0.783203125\n",
      "Iteration 92970 Training loss 0.01635139435529709 Validation loss 0.02125244028866291 Accuracy 0.779296875\n",
      "Iteration 92980 Training loss 0.014492741785943508 Validation loss 0.02091163955628872 Accuracy 0.783203125\n",
      "Iteration 92990 Training loss 0.01159870345145464 Validation loss 0.02069721929728985 Accuracy 0.78369140625\n",
      "Iteration 93000 Training loss 0.01367703452706337 Validation loss 0.021201342344284058 Accuracy 0.7783203125\n",
      "Iteration 93010 Training loss 0.013993586413562298 Validation loss 0.020769979804754257 Accuracy 0.783203125\n",
      "Iteration 93020 Training loss 0.011724520474672318 Validation loss 0.020765092223882675 Accuracy 0.7841796875\n",
      "Iteration 93030 Training loss 0.011746751144528389 Validation loss 0.020691048353910446 Accuracy 0.78369140625\n",
      "Iteration 93040 Training loss 0.011601366102695465 Validation loss 0.020997826009988785 Accuracy 0.78125\n",
      "Iteration 93050 Training loss 0.013071691617369652 Validation loss 0.020818646997213364 Accuracy 0.7822265625\n",
      "Iteration 93060 Training loss 0.014649583958089352 Validation loss 0.020721176639199257 Accuracy 0.78466796875\n",
      "Iteration 93070 Training loss 0.015466433018445969 Validation loss 0.02119605243206024 Accuracy 0.77978515625\n",
      "Iteration 93080 Training loss 0.014742597006261349 Validation loss 0.020942596718668938 Accuracy 0.78271484375\n",
      "Iteration 93090 Training loss 0.012764589861035347 Validation loss 0.02094954624772072 Accuracy 0.78173828125\n",
      "Iteration 93100 Training loss 0.015354405157268047 Validation loss 0.020981380715966225 Accuracy 0.78076171875\n",
      "Iteration 93110 Training loss 0.014437914825975895 Validation loss 0.020720258355140686 Accuracy 0.783203125\n",
      "Iteration 93120 Training loss 0.013928445987403393 Validation loss 0.021290423348546028 Accuracy 0.7783203125\n",
      "Iteration 93130 Training loss 0.014721245504915714 Validation loss 0.020758798345923424 Accuracy 0.78173828125\n",
      "Iteration 93140 Training loss 0.013732131570577621 Validation loss 0.020757142454385757 Accuracy 0.783203125\n",
      "Iteration 93150 Training loss 0.013895095326006413 Validation loss 0.02071157656610012 Accuracy 0.783203125\n",
      "Iteration 93160 Training loss 0.012662501074373722 Validation loss 0.02078445814549923 Accuracy 0.78271484375\n",
      "Iteration 93170 Training loss 0.013309639878571033 Validation loss 0.020775815472006798 Accuracy 0.783203125\n",
      "Iteration 93180 Training loss 0.017190750688314438 Validation loss 0.02079450711607933 Accuracy 0.78173828125\n",
      "Iteration 93190 Training loss 0.01194733940064907 Validation loss 0.020724965259432793 Accuracy 0.7822265625\n",
      "Iteration 93200 Training loss 0.013058659620583057 Validation loss 0.020834989845752716 Accuracy 0.7822265625\n",
      "Iteration 93210 Training loss 0.014345687814056873 Validation loss 0.02104833908379078 Accuracy 0.78173828125\n",
      "Iteration 93220 Training loss 0.013914108276367188 Validation loss 0.020977230742573738 Accuracy 0.78125\n",
      "Iteration 93230 Training loss 0.013462003320455551 Validation loss 0.0208198893815279 Accuracy 0.7822265625\n",
      "Iteration 93240 Training loss 0.0136109609156847 Validation loss 0.021021826192736626 Accuracy 0.7802734375\n",
      "Iteration 93250 Training loss 0.015300235711038113 Validation loss 0.020937558263540268 Accuracy 0.78173828125\n",
      "Iteration 93260 Training loss 0.011950243264436722 Validation loss 0.020674122497439384 Accuracy 0.783203125\n",
      "Iteration 93270 Training loss 0.012301371432840824 Validation loss 0.02073846384882927 Accuracy 0.78369140625\n",
      "Iteration 93280 Training loss 0.013503346592187881 Validation loss 0.02076009288430214 Accuracy 0.78271484375\n",
      "Iteration 93290 Training loss 0.010704810731112957 Validation loss 0.020821135491132736 Accuracy 0.783203125\n",
      "Iteration 93300 Training loss 0.01406775787472725 Validation loss 0.02097703516483307 Accuracy 0.78076171875\n",
      "Iteration 93310 Training loss 0.01326266210526228 Validation loss 0.02078346163034439 Accuracy 0.7822265625\n",
      "Iteration 93320 Training loss 0.01442843396216631 Validation loss 0.020826896652579308 Accuracy 0.7822265625\n",
      "Iteration 93330 Training loss 0.014410029165446758 Validation loss 0.020684491842985153 Accuracy 0.78369140625\n",
      "Iteration 93340 Training loss 0.010306897573173046 Validation loss 0.020728319883346558 Accuracy 0.78271484375\n",
      "Iteration 93350 Training loss 0.011450788006186485 Validation loss 0.02085430547595024 Accuracy 0.78271484375\n",
      "Iteration 93360 Training loss 0.012588953599333763 Validation loss 0.020899886265397072 Accuracy 0.7822265625\n",
      "Iteration 93370 Training loss 0.01253641489893198 Validation loss 0.020664343610405922 Accuracy 0.783203125\n",
      "Iteration 93380 Training loss 0.015886111184954643 Validation loss 0.02114061452448368 Accuracy 0.77978515625\n",
      "Iteration 93390 Training loss 0.010865152813494205 Validation loss 0.02073596604168415 Accuracy 0.78271484375\n",
      "Iteration 93400 Training loss 0.013162370771169662 Validation loss 0.02105625905096531 Accuracy 0.78076171875\n",
      "Iteration 93410 Training loss 0.01315116137266159 Validation loss 0.021031251177191734 Accuracy 0.7822265625\n",
      "Iteration 93420 Training loss 0.014044202864170074 Validation loss 0.021347250789403915 Accuracy 0.77880859375\n",
      "Iteration 93430 Training loss 0.011293080635368824 Validation loss 0.02085656113922596 Accuracy 0.7822265625\n",
      "Iteration 93440 Training loss 0.014208502136170864 Validation loss 0.020665502175688744 Accuracy 0.783203125\n",
      "Iteration 93450 Training loss 0.011351637542247772 Validation loss 0.021034445613622665 Accuracy 0.78173828125\n",
      "Iteration 93460 Training loss 0.011352055706083775 Validation loss 0.020750531926751137 Accuracy 0.78271484375\n",
      "Iteration 93470 Training loss 0.012807393446564674 Validation loss 0.020830394700169563 Accuracy 0.7822265625\n",
      "Iteration 93480 Training loss 0.01488084252923727 Validation loss 0.02075406163930893 Accuracy 0.78271484375\n",
      "Iteration 93490 Training loss 0.01217349711805582 Validation loss 0.0208185613155365 Accuracy 0.783203125\n",
      "Iteration 93500 Training loss 0.015089139342308044 Validation loss 0.020893890410661697 Accuracy 0.78271484375\n",
      "Iteration 93510 Training loss 0.0129943136125803 Validation loss 0.020720062777400017 Accuracy 0.783203125\n",
      "Iteration 93520 Training loss 0.012031130492687225 Validation loss 0.020780976861715317 Accuracy 0.78369140625\n",
      "Iteration 93530 Training loss 0.012199075892567635 Validation loss 0.020836202427744865 Accuracy 0.783203125\n",
      "Iteration 93540 Training loss 0.01529852394014597 Validation loss 0.02130579575896263 Accuracy 0.77978515625\n",
      "Iteration 93550 Training loss 0.013476981781423092 Validation loss 0.02066298946738243 Accuracy 0.78466796875\n",
      "Iteration 93560 Training loss 0.011719886213541031 Validation loss 0.02094440534710884 Accuracy 0.78271484375\n",
      "Iteration 93570 Training loss 0.013032603077590466 Validation loss 0.02103077806532383 Accuracy 0.78125\n",
      "Iteration 93580 Training loss 0.012920309789478779 Validation loss 0.02071942389011383 Accuracy 0.783203125\n",
      "Iteration 93590 Training loss 0.011773123405873775 Validation loss 0.020823979750275612 Accuracy 0.78173828125\n",
      "Iteration 93600 Training loss 0.012732213363051414 Validation loss 0.020805982872843742 Accuracy 0.78369140625\n",
      "Iteration 93610 Training loss 0.012047243304550648 Validation loss 0.020951781421899796 Accuracy 0.783203125\n",
      "Iteration 93620 Training loss 0.012019536457955837 Validation loss 0.02078419364988804 Accuracy 0.783203125\n",
      "Iteration 93630 Training loss 0.014606546610593796 Validation loss 0.02074759267270565 Accuracy 0.78271484375\n",
      "Iteration 93640 Training loss 0.013059047982096672 Validation loss 0.021161576732993126 Accuracy 0.78076171875\n",
      "Iteration 93650 Training loss 0.01265241764485836 Validation loss 0.020758334547281265 Accuracy 0.78369140625\n",
      "Iteration 93660 Training loss 0.0140426866710186 Validation loss 0.020545735955238342 Accuracy 0.78466796875\n",
      "Iteration 93670 Training loss 0.012980058789253235 Validation loss 0.020714443176984787 Accuracy 0.78369140625\n",
      "Iteration 93680 Training loss 0.013806256465613842 Validation loss 0.020827172324061394 Accuracy 0.7822265625\n",
      "Iteration 93690 Training loss 0.011995109729468822 Validation loss 0.020701579749584198 Accuracy 0.7841796875\n",
      "Iteration 93700 Training loss 0.012982149608433247 Validation loss 0.02086859755218029 Accuracy 0.78271484375\n",
      "Iteration 93710 Training loss 0.011541767977178097 Validation loss 0.020850397646427155 Accuracy 0.7822265625\n",
      "Iteration 93720 Training loss 0.013025329448282719 Validation loss 0.02083328366279602 Accuracy 0.78271484375\n",
      "Iteration 93730 Training loss 0.012507891282439232 Validation loss 0.02062523178756237 Accuracy 0.7841796875\n",
      "Iteration 93740 Training loss 0.014786727726459503 Validation loss 0.02128586173057556 Accuracy 0.77978515625\n",
      "Iteration 93750 Training loss 0.012621840462088585 Validation loss 0.020741049200296402 Accuracy 0.7841796875\n",
      "Iteration 93760 Training loss 0.013010410591959953 Validation loss 0.02097272500395775 Accuracy 0.78173828125\n",
      "Iteration 93770 Training loss 0.012719663791358471 Validation loss 0.02063373476266861 Accuracy 0.783203125\n",
      "Iteration 93780 Training loss 0.013389541767537594 Validation loss 0.020670436322689056 Accuracy 0.783203125\n",
      "Iteration 93790 Training loss 0.012595031410455704 Validation loss 0.020705536007881165 Accuracy 0.78369140625\n",
      "Iteration 93800 Training loss 0.012251823209226131 Validation loss 0.020787816494703293 Accuracy 0.783203125\n",
      "Iteration 93810 Training loss 0.013979016803205013 Validation loss 0.021033676341176033 Accuracy 0.78173828125\n",
      "Iteration 93820 Training loss 0.012533643282949924 Validation loss 0.020822355523705482 Accuracy 0.783203125\n",
      "Iteration 93830 Training loss 0.01226218231022358 Validation loss 0.020713774487376213 Accuracy 0.78369140625\n",
      "Iteration 93840 Training loss 0.012918805703520775 Validation loss 0.020678188651800156 Accuracy 0.78369140625\n",
      "Iteration 93850 Training loss 0.013503644615411758 Validation loss 0.0209573432803154 Accuracy 0.783203125\n",
      "Iteration 93860 Training loss 0.014685294590890408 Validation loss 0.0207260400056839 Accuracy 0.783203125\n",
      "Iteration 93870 Training loss 0.015339553356170654 Validation loss 0.02077472023665905 Accuracy 0.78369140625\n",
      "Iteration 93880 Training loss 0.013153381645679474 Validation loss 0.020950859412550926 Accuracy 0.783203125\n",
      "Iteration 93890 Training loss 0.012588728219270706 Validation loss 0.02071685902774334 Accuracy 0.78369140625\n",
      "Iteration 93900 Training loss 0.014604788273572922 Validation loss 0.020949115976691246 Accuracy 0.78369140625\n",
      "Iteration 93910 Training loss 0.012637601234018803 Validation loss 0.020746802911162376 Accuracy 0.783203125\n",
      "Iteration 93920 Training loss 0.012604211457073689 Validation loss 0.020765995606780052 Accuracy 0.78173828125\n",
      "Iteration 93930 Training loss 0.01258774008601904 Validation loss 0.020978476852178574 Accuracy 0.78271484375\n",
      "Iteration 93940 Training loss 0.01394497137516737 Validation loss 0.02083272486925125 Accuracy 0.78369140625\n",
      "Iteration 93950 Training loss 0.01151958666741848 Validation loss 0.02062740921974182 Accuracy 0.78369140625\n",
      "Iteration 93960 Training loss 0.014199396595358849 Validation loss 0.020797105506062508 Accuracy 0.78271484375\n",
      "Iteration 93970 Training loss 0.012177558615803719 Validation loss 0.020799851045012474 Accuracy 0.783203125\n",
      "Iteration 93980 Training loss 0.013377398252487183 Validation loss 0.020642010495066643 Accuracy 0.78369140625\n",
      "Iteration 93990 Training loss 0.014429445378482342 Validation loss 0.02095336839556694 Accuracy 0.78125\n",
      "Iteration 94000 Training loss 0.012399201281368732 Validation loss 0.0207859817892313 Accuracy 0.78271484375\n",
      "Iteration 94010 Training loss 0.01481559220701456 Validation loss 0.020948676392436028 Accuracy 0.78271484375\n",
      "Iteration 94020 Training loss 0.011536849662661552 Validation loss 0.020878199487924576 Accuracy 0.78125\n",
      "Iteration 94030 Training loss 0.011439139023423195 Validation loss 0.020678440108895302 Accuracy 0.78271484375\n",
      "Iteration 94040 Training loss 0.012748472392559052 Validation loss 0.02076965756714344 Accuracy 0.783203125\n",
      "Iteration 94050 Training loss 0.011477144435048103 Validation loss 0.020719116553664207 Accuracy 0.783203125\n",
      "Iteration 94060 Training loss 0.013242942281067371 Validation loss 0.020708631724119186 Accuracy 0.78369140625\n",
      "Iteration 94070 Training loss 0.013612287119030952 Validation loss 0.02072996459901333 Accuracy 0.7841796875\n",
      "Iteration 94080 Training loss 0.015471626073122025 Validation loss 0.020736783742904663 Accuracy 0.78369140625\n",
      "Iteration 94090 Training loss 0.013816065154969692 Validation loss 0.020666811615228653 Accuracy 0.7841796875\n",
      "Iteration 94100 Training loss 0.013351202942430973 Validation loss 0.020805004984140396 Accuracy 0.78125\n",
      "Iteration 94110 Training loss 0.013000638224184513 Validation loss 0.020618367940187454 Accuracy 0.78369140625\n",
      "Iteration 94120 Training loss 0.014348619617521763 Validation loss 0.02064821682870388 Accuracy 0.78369140625\n",
      "Iteration 94130 Training loss 0.014572442509233952 Validation loss 0.020969940349459648 Accuracy 0.7822265625\n",
      "Iteration 94140 Training loss 0.011010866612195969 Validation loss 0.020631635561585426 Accuracy 0.78466796875\n",
      "Iteration 94150 Training loss 0.014756040647625923 Validation loss 0.020677829161286354 Accuracy 0.783203125\n",
      "Iteration 94160 Training loss 0.010560600087046623 Validation loss 0.020683787763118744 Accuracy 0.783203125\n",
      "Iteration 94170 Training loss 0.012981363572180271 Validation loss 0.02070661075413227 Accuracy 0.78369140625\n",
      "Iteration 94180 Training loss 0.012433434836566448 Validation loss 0.020453035831451416 Accuracy 0.78564453125\n",
      "Iteration 94190 Training loss 0.01370721124112606 Validation loss 0.02081637643277645 Accuracy 0.78369140625\n",
      "Iteration 94200 Training loss 0.013848946429789066 Validation loss 0.020684007555246353 Accuracy 0.78271484375\n",
      "Iteration 94210 Training loss 0.013368314132094383 Validation loss 0.020812949165701866 Accuracy 0.78271484375\n",
      "Iteration 94220 Training loss 0.013746721670031548 Validation loss 0.02072461135685444 Accuracy 0.78271484375\n",
      "Iteration 94230 Training loss 0.011293603107333183 Validation loss 0.020697912201285362 Accuracy 0.78466796875\n",
      "Iteration 94240 Training loss 0.011738026514649391 Validation loss 0.020674224942922592 Accuracy 0.783203125\n",
      "Iteration 94250 Training loss 0.011643306352198124 Validation loss 0.02067335695028305 Accuracy 0.783203125\n",
      "Iteration 94260 Training loss 0.012083359993994236 Validation loss 0.020595083013176918 Accuracy 0.78466796875\n",
      "Iteration 94270 Training loss 0.010784859769046307 Validation loss 0.020630978047847748 Accuracy 0.78466796875\n",
      "Iteration 94280 Training loss 0.014106059446930885 Validation loss 0.020614486187696457 Accuracy 0.7841796875\n",
      "Iteration 94290 Training loss 0.012243708595633507 Validation loss 0.020846448838710785 Accuracy 0.78271484375\n",
      "Iteration 94300 Training loss 0.012946292757987976 Validation loss 0.020989131182432175 Accuracy 0.78271484375\n",
      "Iteration 94310 Training loss 0.012535972520709038 Validation loss 0.020562445744872093 Accuracy 0.7841796875\n",
      "Iteration 94320 Training loss 0.015520990826189518 Validation loss 0.020479753613471985 Accuracy 0.78564453125\n",
      "Iteration 94330 Training loss 0.01300483476370573 Validation loss 0.020665595307946205 Accuracy 0.783203125\n",
      "Iteration 94340 Training loss 0.012781181372702122 Validation loss 0.020722324028611183 Accuracy 0.78271484375\n",
      "Iteration 94350 Training loss 0.013823609799146652 Validation loss 0.02062332257628441 Accuracy 0.78515625\n",
      "Iteration 94360 Training loss 0.014425534754991531 Validation loss 0.020695459097623825 Accuracy 0.78369140625\n",
      "Iteration 94370 Training loss 0.013106328435242176 Validation loss 0.020532390102744102 Accuracy 0.78466796875\n",
      "Iteration 94380 Training loss 0.009572183713316917 Validation loss 0.020672447979450226 Accuracy 0.78466796875\n",
      "Iteration 94390 Training loss 0.011861625127494335 Validation loss 0.02067369595170021 Accuracy 0.78369140625\n",
      "Iteration 94400 Training loss 0.010905709117650986 Validation loss 0.021282006055116653 Accuracy 0.77978515625\n",
      "Iteration 94410 Training loss 0.012938493862748146 Validation loss 0.020600998774170876 Accuracy 0.78466796875\n",
      "Iteration 94420 Training loss 0.010995002463459969 Validation loss 0.020945264026522636 Accuracy 0.78076171875\n",
      "Iteration 94430 Training loss 0.014325852505862713 Validation loss 0.020691635087132454 Accuracy 0.7841796875\n",
      "Iteration 94440 Training loss 0.014218531548976898 Validation loss 0.020691651850938797 Accuracy 0.78466796875\n",
      "Iteration 94450 Training loss 0.012669292278587818 Validation loss 0.02092689462006092 Accuracy 0.78271484375\n",
      "Iteration 94460 Training loss 0.013703040778636932 Validation loss 0.0208135973662138 Accuracy 0.7841796875\n",
      "Iteration 94470 Training loss 0.011571443639695644 Validation loss 0.020602885633707047 Accuracy 0.78369140625\n",
      "Iteration 94480 Training loss 0.013321001082658768 Validation loss 0.020764602348208427 Accuracy 0.78466796875\n",
      "Iteration 94490 Training loss 0.013665051199495792 Validation loss 0.020771801471710205 Accuracy 0.7841796875\n",
      "Iteration 94500 Training loss 0.012238468043506145 Validation loss 0.020675957202911377 Accuracy 0.78271484375\n",
      "Iteration 94510 Training loss 0.01337182056158781 Validation loss 0.02061876468360424 Accuracy 0.78466796875\n",
      "Iteration 94520 Training loss 0.01357880886644125 Validation loss 0.020885853096842766 Accuracy 0.78271484375\n",
      "Iteration 94530 Training loss 0.010056781582534313 Validation loss 0.020635731518268585 Accuracy 0.78515625\n",
      "Iteration 94540 Training loss 0.014629103243350983 Validation loss 0.02110084891319275 Accuracy 0.78173828125\n",
      "Iteration 94550 Training loss 0.012188420630991459 Validation loss 0.020916404202580452 Accuracy 0.783203125\n",
      "Iteration 94560 Training loss 0.015642285346984863 Validation loss 0.02067779190838337 Accuracy 0.78466796875\n",
      "Iteration 94570 Training loss 0.012170110829174519 Validation loss 0.02053889073431492 Accuracy 0.7841796875\n",
      "Iteration 94580 Training loss 0.012838106602430344 Validation loss 0.02112576551735401 Accuracy 0.78173828125\n",
      "Iteration 94590 Training loss 0.013179229572415352 Validation loss 0.020795704796910286 Accuracy 0.78271484375\n",
      "Iteration 94600 Training loss 0.011917078867554665 Validation loss 0.020708486437797546 Accuracy 0.78369140625\n",
      "Iteration 94610 Training loss 0.015213708393275738 Validation loss 0.02066844142973423 Accuracy 0.783203125\n",
      "Iteration 94620 Training loss 0.012114280834794044 Validation loss 0.021051567047834396 Accuracy 0.7822265625\n",
      "Iteration 94630 Training loss 0.013235070742666721 Validation loss 0.020724330097436905 Accuracy 0.78466796875\n",
      "Iteration 94640 Training loss 0.011245383881032467 Validation loss 0.02094065211713314 Accuracy 0.7822265625\n",
      "Iteration 94650 Training loss 0.014686685986816883 Validation loss 0.0207753274589777 Accuracy 0.783203125\n",
      "Iteration 94660 Training loss 0.013241416774690151 Validation loss 0.02065751887857914 Accuracy 0.7841796875\n",
      "Iteration 94670 Training loss 0.013303077779710293 Validation loss 0.020838426426053047 Accuracy 0.78369140625\n",
      "Iteration 94680 Training loss 0.01048816367983818 Validation loss 0.02073834463953972 Accuracy 0.78271484375\n",
      "Iteration 94690 Training loss 0.013518630526959896 Validation loss 0.02061675302684307 Accuracy 0.7861328125\n",
      "Iteration 94700 Training loss 0.014131657779216766 Validation loss 0.02060815319418907 Accuracy 0.7841796875\n",
      "Iteration 94710 Training loss 0.01314974669367075 Validation loss 0.02056330442428589 Accuracy 0.7841796875\n",
      "Iteration 94720 Training loss 0.013100297190248966 Validation loss 0.020588526502251625 Accuracy 0.78564453125\n",
      "Iteration 94730 Training loss 0.012832569889724255 Validation loss 0.020944148302078247 Accuracy 0.78369140625\n",
      "Iteration 94740 Training loss 0.013638186268508434 Validation loss 0.02084849588572979 Accuracy 0.783203125\n",
      "Iteration 94750 Training loss 0.013416541740298271 Validation loss 0.020763389766216278 Accuracy 0.7822265625\n",
      "Iteration 94760 Training loss 0.01209942251443863 Validation loss 0.020674457773566246 Accuracy 0.78369140625\n",
      "Iteration 94770 Training loss 0.01484368834644556 Validation loss 0.020847391337156296 Accuracy 0.783203125\n",
      "Iteration 94780 Training loss 0.010575375519692898 Validation loss 0.020611293613910675 Accuracy 0.78515625\n",
      "Iteration 94790 Training loss 0.01277140248566866 Validation loss 0.020728271454572678 Accuracy 0.783203125\n",
      "Iteration 94800 Training loss 0.013335947878658772 Validation loss 0.02054581232368946 Accuracy 0.78369140625\n",
      "Iteration 94810 Training loss 0.014889243990182877 Validation loss 0.020688891410827637 Accuracy 0.78369140625\n",
      "Iteration 94820 Training loss 0.010101469233632088 Validation loss 0.020719090476632118 Accuracy 0.78369140625\n",
      "Iteration 94830 Training loss 0.011554604396224022 Validation loss 0.020723329856991768 Accuracy 0.78271484375\n",
      "Iteration 94840 Training loss 0.014313298277556896 Validation loss 0.020722953602671623 Accuracy 0.78369140625\n",
      "Iteration 94850 Training loss 0.015023074112832546 Validation loss 0.020926693454384804 Accuracy 0.78173828125\n",
      "Iteration 94860 Training loss 0.013035007752478123 Validation loss 0.02127927727997303 Accuracy 0.77978515625\n",
      "Iteration 94870 Training loss 0.013323669321835041 Validation loss 0.02068944275379181 Accuracy 0.78369140625\n",
      "Iteration 94880 Training loss 0.014038016088306904 Validation loss 0.02063925191760063 Accuracy 0.7841796875\n",
      "Iteration 94890 Training loss 0.013022094964981079 Validation loss 0.020748939365148544 Accuracy 0.783203125\n",
      "Iteration 94900 Training loss 0.011523201130330563 Validation loss 0.02075163461267948 Accuracy 0.7822265625\n",
      "Iteration 94910 Training loss 0.014087979681789875 Validation loss 0.020811302587389946 Accuracy 0.7822265625\n",
      "Iteration 94920 Training loss 0.013311397284269333 Validation loss 0.02056301012635231 Accuracy 0.78466796875\n",
      "Iteration 94930 Training loss 0.0129960672929883 Validation loss 0.02085898630321026 Accuracy 0.783203125\n",
      "Iteration 94940 Training loss 0.01322834100574255 Validation loss 0.020709926262497902 Accuracy 0.783203125\n",
      "Iteration 94950 Training loss 0.012511583976447582 Validation loss 0.020822960883378983 Accuracy 0.78173828125\n",
      "Iteration 94960 Training loss 0.01369987428188324 Validation loss 0.020937906578183174 Accuracy 0.78271484375\n",
      "Iteration 94970 Training loss 0.014539394527673721 Validation loss 0.02085014432668686 Accuracy 0.7822265625\n",
      "Iteration 94980 Training loss 0.013036582618951797 Validation loss 0.02071371115744114 Accuracy 0.783203125\n",
      "Iteration 94990 Training loss 0.013420244678854942 Validation loss 0.020626947283744812 Accuracy 0.78564453125\n",
      "Iteration 95000 Training loss 0.010992580093443394 Validation loss 0.020871439948678017 Accuracy 0.7822265625\n",
      "Iteration 95010 Training loss 0.012275167740881443 Validation loss 0.020755406469106674 Accuracy 0.78271484375\n",
      "Iteration 95020 Training loss 0.016066286712884903 Validation loss 0.020869232714176178 Accuracy 0.78173828125\n",
      "Iteration 95030 Training loss 0.016815664246678352 Validation loss 0.02190803922712803 Accuracy 0.771484375\n",
      "Iteration 95040 Training loss 0.012327170930802822 Validation loss 0.020795272663235664 Accuracy 0.7822265625\n",
      "Iteration 95050 Training loss 0.015427802689373493 Validation loss 0.020716547966003418 Accuracy 0.783203125\n",
      "Iteration 95060 Training loss 0.015062116086483002 Validation loss 0.020685819908976555 Accuracy 0.78271484375\n",
      "Iteration 95070 Training loss 0.016202690079808235 Validation loss 0.02057294361293316 Accuracy 0.78466796875\n",
      "Iteration 95080 Training loss 0.01343235932290554 Validation loss 0.02076191082596779 Accuracy 0.783203125\n",
      "Iteration 95090 Training loss 0.008488690480589867 Validation loss 0.020662249997258186 Accuracy 0.7841796875\n",
      "Iteration 95100 Training loss 0.009925688616931438 Validation loss 0.02071559987962246 Accuracy 0.78271484375\n",
      "Iteration 95110 Training loss 0.013668052852153778 Validation loss 0.020916877314448357 Accuracy 0.783203125\n",
      "Iteration 95120 Training loss 0.015177963301539421 Validation loss 0.02062738686800003 Accuracy 0.7841796875\n",
      "Iteration 95130 Training loss 0.01237538643181324 Validation loss 0.02069227024912834 Accuracy 0.7841796875\n",
      "Iteration 95140 Training loss 0.012570563703775406 Validation loss 0.020838646218180656 Accuracy 0.78271484375\n",
      "Iteration 95150 Training loss 0.010863961651921272 Validation loss 0.020598500967025757 Accuracy 0.78369140625\n",
      "Iteration 95160 Training loss 0.012452476657927036 Validation loss 0.020786873996257782 Accuracy 0.7841796875\n",
      "Iteration 95170 Training loss 0.015162934549152851 Validation loss 0.020813550800085068 Accuracy 0.78271484375\n",
      "Iteration 95180 Training loss 0.0135061489418149 Validation loss 0.020768005400896072 Accuracy 0.783203125\n",
      "Iteration 95190 Training loss 0.0129160201177001 Validation loss 0.02074919082224369 Accuracy 0.7841796875\n",
      "Iteration 95200 Training loss 0.013462645933032036 Validation loss 0.020615356042981148 Accuracy 0.78466796875\n",
      "Iteration 95210 Training loss 0.01267192978411913 Validation loss 0.02057626098394394 Accuracy 0.78466796875\n",
      "Iteration 95220 Training loss 0.011955741792917252 Validation loss 0.020701296627521515 Accuracy 0.78369140625\n",
      "Iteration 95230 Training loss 0.009115307591855526 Validation loss 0.020693881437182426 Accuracy 0.78369140625\n",
      "Iteration 95240 Training loss 0.014366389252245426 Validation loss 0.020832089707255363 Accuracy 0.78271484375\n",
      "Iteration 95250 Training loss 0.012180875055491924 Validation loss 0.020653173327445984 Accuracy 0.78515625\n",
      "Iteration 95260 Training loss 0.013276048935949802 Validation loss 0.02056942880153656 Accuracy 0.78466796875\n",
      "Iteration 95270 Training loss 0.015157310292124748 Validation loss 0.02079242654144764 Accuracy 0.78271484375\n",
      "Iteration 95280 Training loss 0.011472738347947598 Validation loss 0.020766807720065117 Accuracy 0.783203125\n",
      "Iteration 95290 Training loss 0.011719360016286373 Validation loss 0.02067708410322666 Accuracy 0.78466796875\n",
      "Iteration 95300 Training loss 0.01339046936482191 Validation loss 0.020680218935012817 Accuracy 0.783203125\n",
      "Iteration 95310 Training loss 0.013561681844294071 Validation loss 0.020620575174689293 Accuracy 0.783203125\n",
      "Iteration 95320 Training loss 0.013787791132926941 Validation loss 0.02055501379072666 Accuracy 0.7841796875\n",
      "Iteration 95330 Training loss 0.013741823844611645 Validation loss 0.020697157829999924 Accuracy 0.78271484375\n",
      "Iteration 95340 Training loss 0.013316663913428783 Validation loss 0.020805001258850098 Accuracy 0.783203125\n",
      "Iteration 95350 Training loss 0.011752637103199959 Validation loss 0.020743750035762787 Accuracy 0.78369140625\n",
      "Iteration 95360 Training loss 0.010309306904673576 Validation loss 0.02095807157456875 Accuracy 0.78173828125\n",
      "Iteration 95370 Training loss 0.01101868599653244 Validation loss 0.021017197519540787 Accuracy 0.7802734375\n",
      "Iteration 95380 Training loss 0.015931611880660057 Validation loss 0.020953942090272903 Accuracy 0.78173828125\n",
      "Iteration 95390 Training loss 0.014062348753213882 Validation loss 0.02071824111044407 Accuracy 0.78369140625\n",
      "Iteration 95400 Training loss 0.015049932524561882 Validation loss 0.020644374191761017 Accuracy 0.7841796875\n",
      "Iteration 95410 Training loss 0.012084669433534145 Validation loss 0.02077692747116089 Accuracy 0.78271484375\n",
      "Iteration 95420 Training loss 0.01323796808719635 Validation loss 0.020628510043025017 Accuracy 0.783203125\n",
      "Iteration 95430 Training loss 0.011087203398346901 Validation loss 0.020861830562353134 Accuracy 0.7822265625\n",
      "Iteration 95440 Training loss 0.01151953637599945 Validation loss 0.020835110917687416 Accuracy 0.7822265625\n",
      "Iteration 95450 Training loss 0.012447654269635677 Validation loss 0.02104923501610756 Accuracy 0.78076171875\n",
      "Iteration 95460 Training loss 0.013565127737820148 Validation loss 0.021078338846564293 Accuracy 0.78173828125\n",
      "Iteration 95470 Training loss 0.014636700972914696 Validation loss 0.020860694348812103 Accuracy 0.78173828125\n",
      "Iteration 95480 Training loss 0.014173061586916447 Validation loss 0.02069709822535515 Accuracy 0.78466796875\n",
      "Iteration 95490 Training loss 0.012941030785441399 Validation loss 0.020720604807138443 Accuracy 0.78369140625\n",
      "Iteration 95500 Training loss 0.011959976516664028 Validation loss 0.02079826407134533 Accuracy 0.783203125\n",
      "Iteration 95510 Training loss 0.01446470245718956 Validation loss 0.02076392062008381 Accuracy 0.78271484375\n",
      "Iteration 95520 Training loss 0.016158271580934525 Validation loss 0.02085791900753975 Accuracy 0.783203125\n",
      "Iteration 95530 Training loss 0.012878237292170525 Validation loss 0.020832745358347893 Accuracy 0.7841796875\n",
      "Iteration 95540 Training loss 0.01286657340824604 Validation loss 0.020712314173579216 Accuracy 0.7841796875\n",
      "Iteration 95550 Training loss 0.013665435835719109 Validation loss 0.020705346018075943 Accuracy 0.78369140625\n",
      "Iteration 95560 Training loss 0.012034022249281406 Validation loss 0.020731141790747643 Accuracy 0.78271484375\n",
      "Iteration 95570 Training loss 0.013868738897144794 Validation loss 0.020639808848500252 Accuracy 0.78515625\n",
      "Iteration 95580 Training loss 0.012913660146296024 Validation loss 0.02061237208545208 Accuracy 0.78369140625\n",
      "Iteration 95590 Training loss 0.0147468326613307 Validation loss 0.02099604159593582 Accuracy 0.779296875\n",
      "Iteration 95600 Training loss 0.012542447075247765 Validation loss 0.020703651010990143 Accuracy 0.78271484375\n",
      "Iteration 95610 Training loss 0.013043354265391827 Validation loss 0.020696068182587624 Accuracy 0.783203125\n",
      "Iteration 95620 Training loss 0.012501955963671207 Validation loss 0.020748255774378777 Accuracy 0.7822265625\n",
      "Iteration 95630 Training loss 0.01390869915485382 Validation loss 0.020717652514576912 Accuracy 0.78271484375\n",
      "Iteration 95640 Training loss 0.014119967818260193 Validation loss 0.02095554582774639 Accuracy 0.78076171875\n",
      "Iteration 95650 Training loss 0.014291307888925076 Validation loss 0.020755585283041 Accuracy 0.7822265625\n",
      "Iteration 95660 Training loss 0.014183757826685905 Validation loss 0.02083359844982624 Accuracy 0.783203125\n",
      "Iteration 95670 Training loss 0.011476332321763039 Validation loss 0.02059735730290413 Accuracy 0.78466796875\n",
      "Iteration 95680 Training loss 0.013611773028969765 Validation loss 0.02090063877403736 Accuracy 0.783203125\n",
      "Iteration 95690 Training loss 0.014490271918475628 Validation loss 0.020564168691635132 Accuracy 0.78515625\n",
      "Iteration 95700 Training loss 0.010679311119019985 Validation loss 0.02068580687046051 Accuracy 0.783203125\n",
      "Iteration 95710 Training loss 0.013371787965297699 Validation loss 0.02079492062330246 Accuracy 0.7841796875\n",
      "Iteration 95720 Training loss 0.012666752561926842 Validation loss 0.02081410028040409 Accuracy 0.78369140625\n",
      "Iteration 95730 Training loss 0.011462869122624397 Validation loss 0.020640522241592407 Accuracy 0.78466796875\n",
      "Iteration 95740 Training loss 0.010981706902384758 Validation loss 0.020738402381539345 Accuracy 0.783203125\n",
      "Iteration 95750 Training loss 0.012702765874564648 Validation loss 0.02089407481253147 Accuracy 0.7822265625\n",
      "Iteration 95760 Training loss 0.016043292358517647 Validation loss 0.020902106538414955 Accuracy 0.783203125\n",
      "Iteration 95770 Training loss 0.012484366074204445 Validation loss 0.0207056924700737 Accuracy 0.78271484375\n",
      "Iteration 95780 Training loss 0.013014371506869793 Validation loss 0.02074148692190647 Accuracy 0.783203125\n",
      "Iteration 95790 Training loss 0.01031855121254921 Validation loss 0.020791050046682358 Accuracy 0.7822265625\n",
      "Iteration 95800 Training loss 0.015508362092077732 Validation loss 0.021216673776507378 Accuracy 0.7783203125\n",
      "Iteration 95810 Training loss 0.011835929937660694 Validation loss 0.020843571051955223 Accuracy 0.78271484375\n",
      "Iteration 95820 Training loss 0.012705894187092781 Validation loss 0.02083476260304451 Accuracy 0.78271484375\n",
      "Iteration 95830 Training loss 0.013126383535563946 Validation loss 0.020733114331960678 Accuracy 0.78369140625\n",
      "Iteration 95840 Training loss 0.01129801757633686 Validation loss 0.020628398284316063 Accuracy 0.78515625\n",
      "Iteration 95850 Training loss 0.01219287607818842 Validation loss 0.020767400041222572 Accuracy 0.78271484375\n",
      "Iteration 95860 Training loss 0.012057200074195862 Validation loss 0.020630287006497383 Accuracy 0.7841796875\n",
      "Iteration 95870 Training loss 0.014103970490396023 Validation loss 0.020917074754834175 Accuracy 0.78076171875\n",
      "Iteration 95880 Training loss 0.011188814416527748 Validation loss 0.02063501812517643 Accuracy 0.78369140625\n",
      "Iteration 95890 Training loss 0.014954129233956337 Validation loss 0.020704379305243492 Accuracy 0.78369140625\n",
      "Iteration 95900 Training loss 0.012710059992969036 Validation loss 0.020714154466986656 Accuracy 0.783203125\n",
      "Iteration 95910 Training loss 0.013435005210340023 Validation loss 0.020673410966992378 Accuracy 0.78466796875\n",
      "Iteration 95920 Training loss 0.010663959197700024 Validation loss 0.02060365118086338 Accuracy 0.78466796875\n",
      "Iteration 95930 Training loss 0.012659888714551926 Validation loss 0.020674433559179306 Accuracy 0.78369140625\n",
      "Iteration 95940 Training loss 0.015077543444931507 Validation loss 0.020722851157188416 Accuracy 0.7822265625\n",
      "Iteration 95950 Training loss 0.013453698717057705 Validation loss 0.020806176587939262 Accuracy 0.7822265625\n",
      "Iteration 95960 Training loss 0.01079285703599453 Validation loss 0.02061082422733307 Accuracy 0.78466796875\n",
      "Iteration 95970 Training loss 0.015369984321296215 Validation loss 0.02058984339237213 Accuracy 0.783203125\n",
      "Iteration 95980 Training loss 0.013019036501646042 Validation loss 0.020627282559871674 Accuracy 0.7841796875\n",
      "Iteration 95990 Training loss 0.012231399305164814 Validation loss 0.020681478083133698 Accuracy 0.7822265625\n",
      "Iteration 96000 Training loss 0.012824936769902706 Validation loss 0.02088373899459839 Accuracy 0.78271484375\n",
      "Iteration 96010 Training loss 0.011952693574130535 Validation loss 0.020735453814268112 Accuracy 0.783203125\n",
      "Iteration 96020 Training loss 0.014511149376630783 Validation loss 0.020605269819498062 Accuracy 0.78369140625\n",
      "Iteration 96030 Training loss 0.01448362972587347 Validation loss 0.020637327805161476 Accuracy 0.7841796875\n",
      "Iteration 96040 Training loss 0.013321316801011562 Validation loss 0.02086377702653408 Accuracy 0.7841796875\n",
      "Iteration 96050 Training loss 0.017016984522342682 Validation loss 0.020930863916873932 Accuracy 0.78271484375\n",
      "Iteration 96060 Training loss 0.010680385865271091 Validation loss 0.020495766773819923 Accuracy 0.78515625\n",
      "Iteration 96070 Training loss 0.013087868690490723 Validation loss 0.02073177881538868 Accuracy 0.78271484375\n",
      "Iteration 96080 Training loss 0.012091809883713722 Validation loss 0.020493008196353912 Accuracy 0.78466796875\n",
      "Iteration 96090 Training loss 0.014998923987150192 Validation loss 0.020753290504217148 Accuracy 0.78369140625\n",
      "Iteration 96100 Training loss 0.014323174953460693 Validation loss 0.0206980612128973 Accuracy 0.78369140625\n",
      "Iteration 96110 Training loss 0.01349011342972517 Validation loss 0.020534418523311615 Accuracy 0.7841796875\n",
      "Iteration 96120 Training loss 0.01241403166204691 Validation loss 0.020545467734336853 Accuracy 0.7841796875\n",
      "Iteration 96130 Training loss 0.01262118760496378 Validation loss 0.020805951207876205 Accuracy 0.78271484375\n",
      "Iteration 96140 Training loss 0.011665135622024536 Validation loss 0.020631415769457817 Accuracy 0.78466796875\n",
      "Iteration 96150 Training loss 0.013848154805600643 Validation loss 0.020881855860352516 Accuracy 0.78271484375\n",
      "Iteration 96160 Training loss 0.014014975167810917 Validation loss 0.02063080482184887 Accuracy 0.78466796875\n",
      "Iteration 96170 Training loss 0.014194831252098083 Validation loss 0.020788783207535744 Accuracy 0.78369140625\n",
      "Iteration 96180 Training loss 0.013057423755526543 Validation loss 0.02051766961812973 Accuracy 0.7841796875\n",
      "Iteration 96190 Training loss 0.011591298505663872 Validation loss 0.02064729854464531 Accuracy 0.783203125\n",
      "Iteration 96200 Training loss 0.012419675476849079 Validation loss 0.020571617409586906 Accuracy 0.78466796875\n",
      "Iteration 96210 Training loss 0.01262073777616024 Validation loss 0.020666103810071945 Accuracy 0.78369140625\n",
      "Iteration 96220 Training loss 0.01410454511642456 Validation loss 0.02107403799891472 Accuracy 0.78173828125\n",
      "Iteration 96230 Training loss 0.012619060464203358 Validation loss 0.020953910425305367 Accuracy 0.78076171875\n",
      "Iteration 96240 Training loss 0.012918807566165924 Validation loss 0.020739315077662468 Accuracy 0.78271484375\n",
      "Iteration 96250 Training loss 0.013957155868411064 Validation loss 0.020838618278503418 Accuracy 0.78271484375\n",
      "Iteration 96260 Training loss 0.014520138502120972 Validation loss 0.021257171407341957 Accuracy 0.78076171875\n",
      "Iteration 96270 Training loss 0.014171319082379341 Validation loss 0.02101043425500393 Accuracy 0.78271484375\n",
      "Iteration 96280 Training loss 0.011395053938031197 Validation loss 0.0204787477850914 Accuracy 0.78564453125\n",
      "Iteration 96290 Training loss 0.012166459113359451 Validation loss 0.020740386098623276 Accuracy 0.78271484375\n",
      "Iteration 96300 Training loss 0.013391194865107536 Validation loss 0.020955750718712807 Accuracy 0.7822265625\n",
      "Iteration 96310 Training loss 0.011372198350727558 Validation loss 0.020634129643440247 Accuracy 0.7841796875\n",
      "Iteration 96320 Training loss 0.01198438461869955 Validation loss 0.020819393917918205 Accuracy 0.7841796875\n",
      "Iteration 96330 Training loss 0.012047391384840012 Validation loss 0.02065816894173622 Accuracy 0.7841796875\n",
      "Iteration 96340 Training loss 0.01211567036807537 Validation loss 0.020621122792363167 Accuracy 0.78369140625\n",
      "Iteration 96350 Training loss 0.013742485083639622 Validation loss 0.020673636347055435 Accuracy 0.7841796875\n",
      "Iteration 96360 Training loss 0.012197119183838367 Validation loss 0.02065453492105007 Accuracy 0.783203125\n",
      "Iteration 96370 Training loss 0.012850288301706314 Validation loss 0.02081398107111454 Accuracy 0.78369140625\n",
      "Iteration 96380 Training loss 0.013746115379035473 Validation loss 0.020765263587236404 Accuracy 0.78271484375\n",
      "Iteration 96390 Training loss 0.011967913247644901 Validation loss 0.020711539313197136 Accuracy 0.783203125\n",
      "Iteration 96400 Training loss 0.014061475172638893 Validation loss 0.02061389572918415 Accuracy 0.78466796875\n",
      "Iteration 96410 Training loss 0.013284273445606232 Validation loss 0.020794447511434555 Accuracy 0.78369140625\n",
      "Iteration 96420 Training loss 0.010722069069743156 Validation loss 0.020653503015637398 Accuracy 0.78466796875\n",
      "Iteration 96430 Training loss 0.014562878757715225 Validation loss 0.02067512460052967 Accuracy 0.78466796875\n",
      "Iteration 96440 Training loss 0.010814695619046688 Validation loss 0.020737143233418465 Accuracy 0.78271484375\n",
      "Iteration 96450 Training loss 0.014428469352424145 Validation loss 0.021101556718349457 Accuracy 0.78076171875\n",
      "Iteration 96460 Training loss 0.01323875691741705 Validation loss 0.02067248523235321 Accuracy 0.78369140625\n",
      "Iteration 96470 Training loss 0.014530466869473457 Validation loss 0.020793957635760307 Accuracy 0.78369140625\n",
      "Iteration 96480 Training loss 0.012504770420491695 Validation loss 0.020822491496801376 Accuracy 0.78271484375\n",
      "Iteration 96490 Training loss 0.012561907060444355 Validation loss 0.020662521943449974 Accuracy 0.7841796875\n",
      "Iteration 96500 Training loss 0.012243106961250305 Validation loss 0.020821429789066315 Accuracy 0.78271484375\n",
      "Iteration 96510 Training loss 0.014977836050093174 Validation loss 0.020746689289808273 Accuracy 0.78369140625\n",
      "Iteration 96520 Training loss 0.015092483721673489 Validation loss 0.021040350198745728 Accuracy 0.78173828125\n",
      "Iteration 96530 Training loss 0.014199643395841122 Validation loss 0.02078338898718357 Accuracy 0.78271484375\n",
      "Iteration 96540 Training loss 0.011853245086967945 Validation loss 0.020646831020712852 Accuracy 0.7841796875\n",
      "Iteration 96550 Training loss 0.013291620649397373 Validation loss 0.020699163898825645 Accuracy 0.78369140625\n",
      "Iteration 96560 Training loss 0.01112548541277647 Validation loss 0.020818056538701057 Accuracy 0.7841796875\n",
      "Iteration 96570 Training loss 0.014113625511527061 Validation loss 0.020647766068577766 Accuracy 0.78369140625\n",
      "Iteration 96580 Training loss 0.016044629737734795 Validation loss 0.020635517314076424 Accuracy 0.783203125\n",
      "Iteration 96590 Training loss 0.01395573653280735 Validation loss 0.020626429468393326 Accuracy 0.783203125\n",
      "Iteration 96600 Training loss 0.011388913728296757 Validation loss 0.02062162011861801 Accuracy 0.7841796875\n",
      "Iteration 96610 Training loss 0.013268425129354 Validation loss 0.02060657925903797 Accuracy 0.78466796875\n",
      "Iteration 96620 Training loss 0.013238649815320969 Validation loss 0.020691271871328354 Accuracy 0.78369140625\n",
      "Iteration 96630 Training loss 0.013233762234449387 Validation loss 0.020611733198165894 Accuracy 0.7841796875\n",
      "Iteration 96640 Training loss 0.011028707027435303 Validation loss 0.020807303488254547 Accuracy 0.7822265625\n",
      "Iteration 96650 Training loss 0.013307810761034489 Validation loss 0.02061331644654274 Accuracy 0.7841796875\n",
      "Iteration 96660 Training loss 0.012445896863937378 Validation loss 0.020601535215973854 Accuracy 0.783203125\n",
      "Iteration 96670 Training loss 0.012850912287831306 Validation loss 0.020927194505929947 Accuracy 0.78076171875\n",
      "Iteration 96680 Training loss 0.013691247440874577 Validation loss 0.02112261950969696 Accuracy 0.78076171875\n",
      "Iteration 96690 Training loss 0.01464950293302536 Validation loss 0.020763574168086052 Accuracy 0.78271484375\n",
      "Iteration 96700 Training loss 0.015433047898113728 Validation loss 0.020669203251600266 Accuracy 0.78369140625\n",
      "Iteration 96710 Training loss 0.014103022404015064 Validation loss 0.020638026297092438 Accuracy 0.7841796875\n",
      "Iteration 96720 Training loss 0.015924636274576187 Validation loss 0.020851513370871544 Accuracy 0.78271484375\n",
      "Iteration 96730 Training loss 0.013394642621278763 Validation loss 0.02074921317398548 Accuracy 0.783203125\n",
      "Iteration 96740 Training loss 0.012634942308068275 Validation loss 0.020710790529847145 Accuracy 0.78271484375\n",
      "Iteration 96750 Training loss 0.012622085399925709 Validation loss 0.020969178527593613 Accuracy 0.78173828125\n",
      "Iteration 96760 Training loss 0.012807880528271198 Validation loss 0.020596742630004883 Accuracy 0.7841796875\n",
      "Iteration 96770 Training loss 0.011658629402518272 Validation loss 0.020491547882556915 Accuracy 0.783203125\n",
      "Iteration 96780 Training loss 0.01146379578858614 Validation loss 0.02079910598695278 Accuracy 0.783203125\n",
      "Iteration 96790 Training loss 0.015237168408930302 Validation loss 0.020801957696676254 Accuracy 0.78369140625\n",
      "Iteration 96800 Training loss 0.013685565441846848 Validation loss 0.02068915031850338 Accuracy 0.78369140625\n",
      "Iteration 96810 Training loss 0.013920728117227554 Validation loss 0.020566171035170555 Accuracy 0.78466796875\n",
      "Iteration 96820 Training loss 0.013642323203384876 Validation loss 0.02072150819003582 Accuracy 0.78369140625\n",
      "Iteration 96830 Training loss 0.013971159234642982 Validation loss 0.020682699978351593 Accuracy 0.78369140625\n",
      "Iteration 96840 Training loss 0.01396811194717884 Validation loss 0.020862430334091187 Accuracy 0.78369140625\n",
      "Iteration 96850 Training loss 0.01213203463703394 Validation loss 0.02062925323843956 Accuracy 0.7841796875\n",
      "Iteration 96860 Training loss 0.012964416295289993 Validation loss 0.020642364397644997 Accuracy 0.7841796875\n",
      "Iteration 96870 Training loss 0.011691824533045292 Validation loss 0.0206209197640419 Accuracy 0.78369140625\n",
      "Iteration 96880 Training loss 0.011057360097765923 Validation loss 0.02086033672094345 Accuracy 0.78271484375\n",
      "Iteration 96890 Training loss 0.013760754838585854 Validation loss 0.020852988585829735 Accuracy 0.78173828125\n",
      "Iteration 96900 Training loss 0.012930239550769329 Validation loss 0.020794488489627838 Accuracy 0.78271484375\n",
      "Iteration 96910 Training loss 0.015255691483616829 Validation loss 0.02096741646528244 Accuracy 0.7822265625\n",
      "Iteration 96920 Training loss 0.012114115059375763 Validation loss 0.02074657380580902 Accuracy 0.783203125\n",
      "Iteration 96930 Training loss 0.014504809863865376 Validation loss 0.02062923274934292 Accuracy 0.7841796875\n",
      "Iteration 96940 Training loss 0.013551151379942894 Validation loss 0.02050863951444626 Accuracy 0.7861328125\n",
      "Iteration 96950 Training loss 0.013786409981548786 Validation loss 0.021168258041143417 Accuracy 0.779296875\n",
      "Iteration 96960 Training loss 0.01250178087502718 Validation loss 0.02082766965031624 Accuracy 0.78173828125\n",
      "Iteration 96970 Training loss 0.011084981262683868 Validation loss 0.020745517686009407 Accuracy 0.78369140625\n",
      "Iteration 96980 Training loss 0.014167233370244503 Validation loss 0.020641379058361053 Accuracy 0.7841796875\n",
      "Iteration 96990 Training loss 0.015082234516739845 Validation loss 0.020754750818014145 Accuracy 0.7841796875\n",
      "Iteration 97000 Training loss 0.014542111195623875 Validation loss 0.020796755328774452 Accuracy 0.78466796875\n",
      "Iteration 97010 Training loss 0.015052705071866512 Validation loss 0.020769702270627022 Accuracy 0.78369140625\n",
      "Iteration 97020 Training loss 0.011931177228689194 Validation loss 0.020615696907043457 Accuracy 0.7841796875\n",
      "Iteration 97030 Training loss 0.014003705233335495 Validation loss 0.02069513313472271 Accuracy 0.78271484375\n",
      "Iteration 97040 Training loss 0.010578619316220284 Validation loss 0.02069755271077156 Accuracy 0.783203125\n",
      "Iteration 97050 Training loss 0.015231344848871231 Validation loss 0.020707987248897552 Accuracy 0.783203125\n",
      "Iteration 97060 Training loss 0.010192414745688438 Validation loss 0.020596645772457123 Accuracy 0.7822265625\n",
      "Iteration 97070 Training loss 0.013008267618715763 Validation loss 0.020782988518476486 Accuracy 0.7822265625\n",
      "Iteration 97080 Training loss 0.011767499148845673 Validation loss 0.021091163158416748 Accuracy 0.78076171875\n",
      "Iteration 97090 Training loss 0.014004155062139034 Validation loss 0.0209735669195652 Accuracy 0.78173828125\n",
      "Iteration 97100 Training loss 0.014836753718554974 Validation loss 0.020680421963334084 Accuracy 0.783203125\n",
      "Iteration 97110 Training loss 0.010748382657766342 Validation loss 0.020514726638793945 Accuracy 0.7841796875\n",
      "Iteration 97120 Training loss 0.011449305340647697 Validation loss 0.020569046959280968 Accuracy 0.78369140625\n",
      "Iteration 97130 Training loss 0.013630582019686699 Validation loss 0.02050483040511608 Accuracy 0.78466796875\n",
      "Iteration 97140 Training loss 0.011373985558748245 Validation loss 0.020698288455605507 Accuracy 0.78369140625\n",
      "Iteration 97150 Training loss 0.013588683679699898 Validation loss 0.020899256691336632 Accuracy 0.7822265625\n",
      "Iteration 97160 Training loss 0.012010282836854458 Validation loss 0.020647509023547173 Accuracy 0.78369140625\n",
      "Iteration 97170 Training loss 0.011747169308364391 Validation loss 0.020707882940769196 Accuracy 0.783203125\n",
      "Iteration 97180 Training loss 0.012478998862206936 Validation loss 0.020519409328699112 Accuracy 0.78466796875\n",
      "Iteration 97190 Training loss 0.011873474344611168 Validation loss 0.020513122901320457 Accuracy 0.7841796875\n",
      "Iteration 97200 Training loss 0.015271694399416447 Validation loss 0.022435832768678665 Accuracy 0.76611328125\n",
      "Iteration 97210 Training loss 0.012484717182815075 Validation loss 0.020648475736379623 Accuracy 0.7841796875\n",
      "Iteration 97220 Training loss 0.014155438169836998 Validation loss 0.020735807716846466 Accuracy 0.78271484375\n",
      "Iteration 97230 Training loss 0.012110522948205471 Validation loss 0.02077706716954708 Accuracy 0.783203125\n",
      "Iteration 97240 Training loss 0.014418331906199455 Validation loss 0.02074296399950981 Accuracy 0.7822265625\n",
      "Iteration 97250 Training loss 0.012528610415756702 Validation loss 0.020794551819562912 Accuracy 0.783203125\n",
      "Iteration 97260 Training loss 0.01342822890728712 Validation loss 0.020503247156739235 Accuracy 0.78515625\n",
      "Iteration 97270 Training loss 0.01272517628967762 Validation loss 0.020789554342627525 Accuracy 0.78369140625\n",
      "Iteration 97280 Training loss 0.015196260996162891 Validation loss 0.02091679722070694 Accuracy 0.78125\n",
      "Iteration 97290 Training loss 0.01611330732703209 Validation loss 0.02125234343111515 Accuracy 0.7783203125\n",
      "Iteration 97300 Training loss 0.013039355166256428 Validation loss 0.020710207521915436 Accuracy 0.783203125\n",
      "Iteration 97310 Training loss 0.012487342581152916 Validation loss 0.02101348526775837 Accuracy 0.78173828125\n",
      "Iteration 97320 Training loss 0.014579718932509422 Validation loss 0.020962221547961235 Accuracy 0.7822265625\n",
      "Iteration 97330 Training loss 0.013963154517114162 Validation loss 0.020760048180818558 Accuracy 0.78271484375\n",
      "Iteration 97340 Training loss 0.012904991395771503 Validation loss 0.02073218673467636 Accuracy 0.783203125\n",
      "Iteration 97350 Training loss 0.012763536535203457 Validation loss 0.020828286185860634 Accuracy 0.7841796875\n",
      "Iteration 97360 Training loss 0.010702725499868393 Validation loss 0.020681679248809814 Accuracy 0.78369140625\n",
      "Iteration 97370 Training loss 0.013023247011005878 Validation loss 0.021060122177004814 Accuracy 0.7802734375\n",
      "Iteration 97380 Training loss 0.012943265028297901 Validation loss 0.02074328064918518 Accuracy 0.78369140625\n",
      "Iteration 97390 Training loss 0.011067412793636322 Validation loss 0.02058517374098301 Accuracy 0.78369140625\n",
      "Iteration 97400 Training loss 0.012998131103813648 Validation loss 0.020841732621192932 Accuracy 0.78173828125\n",
      "Iteration 97410 Training loss 0.01178465224802494 Validation loss 0.020754076540470123 Accuracy 0.7822265625\n",
      "Iteration 97420 Training loss 0.01291433535516262 Validation loss 0.020721781998872757 Accuracy 0.78369140625\n",
      "Iteration 97430 Training loss 0.0107363760471344 Validation loss 0.020636282861232758 Accuracy 0.7841796875\n",
      "Iteration 97440 Training loss 0.012931271456182003 Validation loss 0.020538318902254105 Accuracy 0.7841796875\n",
      "Iteration 97450 Training loss 0.016737598925828934 Validation loss 0.02085781842470169 Accuracy 0.78076171875\n",
      "Iteration 97460 Training loss 0.014222399331629276 Validation loss 0.02075725980103016 Accuracy 0.783203125\n",
      "Iteration 97470 Training loss 0.014065662398934364 Validation loss 0.02090163715183735 Accuracy 0.7822265625\n",
      "Iteration 97480 Training loss 0.013548304326832294 Validation loss 0.02072003297507763 Accuracy 0.783203125\n",
      "Iteration 97490 Training loss 0.011898668482899666 Validation loss 0.020671894773840904 Accuracy 0.783203125\n",
      "Iteration 97500 Training loss 0.010764441452920437 Validation loss 0.020692147314548492 Accuracy 0.78271484375\n",
      "Iteration 97510 Training loss 0.011413480155169964 Validation loss 0.02070087380707264 Accuracy 0.78369140625\n",
      "Iteration 97520 Training loss 0.014334159903228283 Validation loss 0.02067233994603157 Accuracy 0.78369140625\n",
      "Iteration 97530 Training loss 0.014063018374145031 Validation loss 0.020705536007881165 Accuracy 0.78369140625\n",
      "Iteration 97540 Training loss 0.012423070147633553 Validation loss 0.02092704549431801 Accuracy 0.78076171875\n",
      "Iteration 97550 Training loss 0.012157399207353592 Validation loss 0.02119409665465355 Accuracy 0.77783203125\n",
      "Iteration 97560 Training loss 0.0126129649579525 Validation loss 0.020701825618743896 Accuracy 0.7822265625\n",
      "Iteration 97570 Training loss 0.012969853356480598 Validation loss 0.021014630794525146 Accuracy 0.7802734375\n",
      "Iteration 97580 Training loss 0.015237707644701004 Validation loss 0.020721038803458214 Accuracy 0.78369140625\n",
      "Iteration 97590 Training loss 0.014121700078248978 Validation loss 0.02068745531141758 Accuracy 0.78466796875\n",
      "Iteration 97600 Training loss 0.018115481361746788 Validation loss 0.021737875416874886 Accuracy 0.77197265625\n",
      "Iteration 97610 Training loss 0.013026488944888115 Validation loss 0.020504672080278397 Accuracy 0.7841796875\n",
      "Iteration 97620 Training loss 0.011137455701828003 Validation loss 0.020627392455935478 Accuracy 0.78369140625\n",
      "Iteration 97630 Training loss 0.01144543569535017 Validation loss 0.02061064913868904 Accuracy 0.78369140625\n",
      "Iteration 97640 Training loss 0.014225202612578869 Validation loss 0.020669851452112198 Accuracy 0.7841796875\n",
      "Iteration 97650 Training loss 0.012074766680598259 Validation loss 0.02095056138932705 Accuracy 0.7822265625\n",
      "Iteration 97660 Training loss 0.014581757597625256 Validation loss 0.02066357061266899 Accuracy 0.7841796875\n",
      "Iteration 97670 Training loss 0.012028385885059834 Validation loss 0.02068989910185337 Accuracy 0.78369140625\n",
      "Iteration 97680 Training loss 0.011976528912782669 Validation loss 0.020850293338298798 Accuracy 0.7822265625\n",
      "Iteration 97690 Training loss 0.012286440469324589 Validation loss 0.020888498052954674 Accuracy 0.783203125\n",
      "Iteration 97700 Training loss 0.013460924848914146 Validation loss 0.020631741732358932 Accuracy 0.78369140625\n",
      "Iteration 97710 Training loss 0.013343621045351028 Validation loss 0.02104373462498188 Accuracy 0.78076171875\n",
      "Iteration 97720 Training loss 0.013384639285504818 Validation loss 0.020637914538383484 Accuracy 0.78466796875\n",
      "Iteration 97730 Training loss 0.011997418478131294 Validation loss 0.020621083676815033 Accuracy 0.7841796875\n",
      "Iteration 97740 Training loss 0.011618462391197681 Validation loss 0.021045366302132607 Accuracy 0.7802734375\n",
      "Iteration 97750 Training loss 0.011619656346738338 Validation loss 0.020512208342552185 Accuracy 0.78466796875\n",
      "Iteration 97760 Training loss 0.014534306712448597 Validation loss 0.021829163655638695 Accuracy 0.77197265625\n",
      "Iteration 97770 Training loss 0.012396316044032574 Validation loss 0.020617326721549034 Accuracy 0.78369140625\n",
      "Iteration 97780 Training loss 0.01580098271369934 Validation loss 0.020518261939287186 Accuracy 0.7841796875\n",
      "Iteration 97790 Training loss 0.01447626855224371 Validation loss 0.020427191630005836 Accuracy 0.78564453125\n",
      "Iteration 97800 Training loss 0.013703691773116589 Validation loss 0.02054341323673725 Accuracy 0.78564453125\n",
      "Iteration 97810 Training loss 0.012331636622548103 Validation loss 0.02047332189977169 Accuracy 0.78564453125\n",
      "Iteration 97820 Training loss 0.014333994127810001 Validation loss 0.020580336451530457 Accuracy 0.78369140625\n",
      "Iteration 97830 Training loss 0.012218737974762917 Validation loss 0.02081070840358734 Accuracy 0.7822265625\n",
      "Iteration 97840 Training loss 0.011036556214094162 Validation loss 0.020660962909460068 Accuracy 0.78369140625\n",
      "Iteration 97850 Training loss 0.015441087074577808 Validation loss 0.020650124177336693 Accuracy 0.78369140625\n",
      "Iteration 97860 Training loss 0.009883510880172253 Validation loss 0.02067101188004017 Accuracy 0.78369140625\n",
      "Iteration 97870 Training loss 0.01134447567164898 Validation loss 0.020625706762075424 Accuracy 0.78466796875\n",
      "Iteration 97880 Training loss 0.011385141871869564 Validation loss 0.02078344114124775 Accuracy 0.78271484375\n",
      "Iteration 97890 Training loss 0.012423697859048843 Validation loss 0.020824091508984566 Accuracy 0.783203125\n",
      "Iteration 97900 Training loss 0.013835741207003593 Validation loss 0.02054455503821373 Accuracy 0.78515625\n",
      "Iteration 97910 Training loss 0.012703360989689827 Validation loss 0.02061172015964985 Accuracy 0.783203125\n",
      "Iteration 97920 Training loss 0.012914090417325497 Validation loss 0.020641585811972618 Accuracy 0.78466796875\n",
      "Iteration 97930 Training loss 0.011676882393658161 Validation loss 0.02121756598353386 Accuracy 0.78076171875\n",
      "Iteration 97940 Training loss 0.012912986800074577 Validation loss 0.0209126565605402 Accuracy 0.78125\n",
      "Iteration 97950 Training loss 0.0130202891305089 Validation loss 0.020587747916579247 Accuracy 0.7841796875\n",
      "Iteration 97960 Training loss 0.011411166749894619 Validation loss 0.020720480009913445 Accuracy 0.7841796875\n",
      "Iteration 97970 Training loss 0.01303066499531269 Validation loss 0.020666124299168587 Accuracy 0.78271484375\n",
      "Iteration 97980 Training loss 0.012602229602634907 Validation loss 0.020802553743124008 Accuracy 0.783203125\n",
      "Iteration 97990 Training loss 0.01544239278882742 Validation loss 0.020682845264673233 Accuracy 0.78369140625\n",
      "Iteration 98000 Training loss 0.010100703686475754 Validation loss 0.020640810951590538 Accuracy 0.78466796875\n",
      "Iteration 98010 Training loss 0.012143775820732117 Validation loss 0.02071363665163517 Accuracy 0.78369140625\n",
      "Iteration 98020 Training loss 0.011778272688388824 Validation loss 0.02116422727704048 Accuracy 0.77880859375\n",
      "Iteration 98030 Training loss 0.014276443049311638 Validation loss 0.020594246685504913 Accuracy 0.78369140625\n",
      "Iteration 98040 Training loss 0.013979939743876457 Validation loss 0.02077816054224968 Accuracy 0.78271484375\n",
      "Iteration 98050 Training loss 0.015423782169818878 Validation loss 0.020637797191739082 Accuracy 0.78515625\n",
      "Iteration 98060 Training loss 0.014801095239818096 Validation loss 0.020583076402544975 Accuracy 0.7841796875\n",
      "Iteration 98070 Training loss 0.013133873231709003 Validation loss 0.0205437783151865 Accuracy 0.7841796875\n",
      "Iteration 98080 Training loss 0.01220763474702835 Validation loss 0.020690903067588806 Accuracy 0.7841796875\n",
      "Iteration 98090 Training loss 0.013565683737397194 Validation loss 0.020977459847927094 Accuracy 0.7822265625\n",
      "Iteration 98100 Training loss 0.012687207199633121 Validation loss 0.020624345168471336 Accuracy 0.78466796875\n",
      "Iteration 98110 Training loss 0.014337965287268162 Validation loss 0.020972885191440582 Accuracy 0.78125\n",
      "Iteration 98120 Training loss 0.012001754716038704 Validation loss 0.020620213821530342 Accuracy 0.78369140625\n",
      "Iteration 98130 Training loss 0.014220621436834335 Validation loss 0.02065294049680233 Accuracy 0.78515625\n",
      "Iteration 98140 Training loss 0.012672703713178635 Validation loss 0.020712513476610184 Accuracy 0.783203125\n",
      "Iteration 98150 Training loss 0.013939869590103626 Validation loss 0.020718710497021675 Accuracy 0.78271484375\n",
      "Iteration 98160 Training loss 0.012593406252563 Validation loss 0.020756054669618607 Accuracy 0.78369140625\n",
      "Iteration 98170 Training loss 0.014312143437564373 Validation loss 0.0208500437438488 Accuracy 0.783203125\n",
      "Iteration 98180 Training loss 0.013915655203163624 Validation loss 0.020630594342947006 Accuracy 0.78369140625\n",
      "Iteration 98190 Training loss 0.013600781559944153 Validation loss 0.020729174837470055 Accuracy 0.78271484375\n",
      "Iteration 98200 Training loss 0.012449045665562153 Validation loss 0.02065180614590645 Accuracy 0.7841796875\n",
      "Iteration 98210 Training loss 0.014422869309782982 Validation loss 0.02065587416291237 Accuracy 0.7841796875\n",
      "Iteration 98220 Training loss 0.014735378324985504 Validation loss 0.021021859720349312 Accuracy 0.7822265625\n",
      "Iteration 98230 Training loss 0.016608672216534615 Validation loss 0.02090604603290558 Accuracy 0.783203125\n",
      "Iteration 98240 Training loss 0.013148386031389236 Validation loss 0.020607490092515945 Accuracy 0.78466796875\n",
      "Iteration 98250 Training loss 0.01200004294514656 Validation loss 0.020659811794757843 Accuracy 0.78271484375\n",
      "Iteration 98260 Training loss 0.01361733116209507 Validation loss 0.020577477291226387 Accuracy 0.78515625\n",
      "Iteration 98270 Training loss 0.01060579065233469 Validation loss 0.020606933161616325 Accuracy 0.78564453125\n",
      "Iteration 98280 Training loss 0.01213923655450344 Validation loss 0.020580755546689034 Accuracy 0.78466796875\n",
      "Iteration 98290 Training loss 0.013969643972814083 Validation loss 0.02076011709868908 Accuracy 0.78369140625\n",
      "Iteration 98300 Training loss 0.01394414622336626 Validation loss 0.020740995183587074 Accuracy 0.783203125\n",
      "Iteration 98310 Training loss 0.01291746273636818 Validation loss 0.02100657857954502 Accuracy 0.78125\n",
      "Iteration 98320 Training loss 0.012575383298099041 Validation loss 0.020854124799370766 Accuracy 0.783203125\n",
      "Iteration 98330 Training loss 0.01105312630534172 Validation loss 0.020848505198955536 Accuracy 0.783203125\n",
      "Iteration 98340 Training loss 0.01310504600405693 Validation loss 0.020679347217082977 Accuracy 0.78369140625\n",
      "Iteration 98350 Training loss 0.014119559898972511 Validation loss 0.020695414394140244 Accuracy 0.783203125\n",
      "Iteration 98360 Training loss 0.013470489531755447 Validation loss 0.020622728392481804 Accuracy 0.78515625\n",
      "Iteration 98370 Training loss 0.013999305665493011 Validation loss 0.020610399544239044 Accuracy 0.7841796875\n",
      "Iteration 98380 Training loss 0.01181979849934578 Validation loss 0.02068881317973137 Accuracy 0.783203125\n",
      "Iteration 98390 Training loss 0.01477005798369646 Validation loss 0.02062978409230709 Accuracy 0.7841796875\n",
      "Iteration 98400 Training loss 0.014449816197156906 Validation loss 0.02083749696612358 Accuracy 0.78271484375\n",
      "Iteration 98410 Training loss 0.014454632066190243 Validation loss 0.020695770159363747 Accuracy 0.783203125\n",
      "Iteration 98420 Training loss 0.01435998547822237 Validation loss 0.020636390894651413 Accuracy 0.78369140625\n",
      "Iteration 98430 Training loss 0.013152379542589188 Validation loss 0.020794758573174477 Accuracy 0.78173828125\n",
      "Iteration 98440 Training loss 0.012362596578896046 Validation loss 0.02059243805706501 Accuracy 0.78369140625\n",
      "Iteration 98450 Training loss 0.015159164555370808 Validation loss 0.020672274753451347 Accuracy 0.78369140625\n",
      "Iteration 98460 Training loss 0.013216744177043438 Validation loss 0.020614249631762505 Accuracy 0.78466796875\n",
      "Iteration 98470 Training loss 0.011877539567649364 Validation loss 0.020642494782805443 Accuracy 0.783203125\n",
      "Iteration 98480 Training loss 0.013081992976367474 Validation loss 0.020916763693094254 Accuracy 0.78173828125\n",
      "Iteration 98490 Training loss 0.01335351075977087 Validation loss 0.02074647881090641 Accuracy 0.78466796875\n",
      "Iteration 98500 Training loss 0.013694479130208492 Validation loss 0.020726270973682404 Accuracy 0.78271484375\n",
      "Iteration 98510 Training loss 0.012726943008601665 Validation loss 0.020633425563573837 Accuracy 0.78369140625\n",
      "Iteration 98520 Training loss 0.013238867744803429 Validation loss 0.020608240738511086 Accuracy 0.7841796875\n",
      "Iteration 98530 Training loss 0.0131582822650671 Validation loss 0.020723918452858925 Accuracy 0.7841796875\n",
      "Iteration 98540 Training loss 0.013396309688687325 Validation loss 0.020872702822089195 Accuracy 0.7822265625\n",
      "Iteration 98550 Training loss 0.0123786935582757 Validation loss 0.020759230479598045 Accuracy 0.783203125\n",
      "Iteration 98560 Training loss 0.013167141936719418 Validation loss 0.021022619679570198 Accuracy 0.78125\n",
      "Iteration 98570 Training loss 0.013025694526731968 Validation loss 0.020729929208755493 Accuracy 0.78369140625\n",
      "Iteration 98580 Training loss 0.013804135844111443 Validation loss 0.02084979973733425 Accuracy 0.78076171875\n",
      "Iteration 98590 Training loss 0.01367982942610979 Validation loss 0.020805304870009422 Accuracy 0.78271484375\n",
      "Iteration 98600 Training loss 0.010701468214392662 Validation loss 0.020706571638584137 Accuracy 0.783203125\n",
      "Iteration 98610 Training loss 0.013781133107841015 Validation loss 0.021026745438575745 Accuracy 0.78173828125\n",
      "Iteration 98620 Training loss 0.01326330192387104 Validation loss 0.020762933418154716 Accuracy 0.7841796875\n",
      "Iteration 98630 Training loss 0.012252393178641796 Validation loss 0.020811118185520172 Accuracy 0.783203125\n",
      "Iteration 98640 Training loss 0.012497196905314922 Validation loss 0.02111065946519375 Accuracy 0.78125\n",
      "Iteration 98650 Training loss 0.012562529183924198 Validation loss 0.02062796801328659 Accuracy 0.7841796875\n",
      "Iteration 98660 Training loss 0.01701950840651989 Validation loss 0.020837465301156044 Accuracy 0.78125\n",
      "Iteration 98670 Training loss 0.012398317456245422 Validation loss 0.020658651366829872 Accuracy 0.78271484375\n",
      "Iteration 98680 Training loss 0.0135581586509943 Validation loss 0.020807866007089615 Accuracy 0.783203125\n",
      "Iteration 98690 Training loss 0.013349884189665318 Validation loss 0.0206749327480793 Accuracy 0.78369140625\n",
      "Iteration 98700 Training loss 0.013751340098679066 Validation loss 0.020878328010439873 Accuracy 0.78271484375\n",
      "Iteration 98710 Training loss 0.014066577889025211 Validation loss 0.02073468454182148 Accuracy 0.7841796875\n",
      "Iteration 98720 Training loss 0.014260918833315372 Validation loss 0.02063019946217537 Accuracy 0.78466796875\n",
      "Iteration 98730 Training loss 0.013603252358734608 Validation loss 0.020649034529924393 Accuracy 0.783203125\n",
      "Iteration 98740 Training loss 0.012561558745801449 Validation loss 0.020676618441939354 Accuracy 0.78369140625\n",
      "Iteration 98750 Training loss 0.013904825784265995 Validation loss 0.02071833424270153 Accuracy 0.783203125\n",
      "Iteration 98760 Training loss 0.013871279545128345 Validation loss 0.020598798990249634 Accuracy 0.78369140625\n",
      "Iteration 98770 Training loss 0.015038184821605682 Validation loss 0.02063116617500782 Accuracy 0.783203125\n",
      "Iteration 98780 Training loss 0.014232628047466278 Validation loss 0.020579127594828606 Accuracy 0.7841796875\n",
      "Iteration 98790 Training loss 0.01500160712748766 Validation loss 0.020680582150816917 Accuracy 0.7841796875\n",
      "Iteration 98800 Training loss 0.014405225403606892 Validation loss 0.020620668306946754 Accuracy 0.78369140625\n",
      "Iteration 98810 Training loss 0.013660194352269173 Validation loss 0.020640453323721886 Accuracy 0.78466796875\n",
      "Iteration 98820 Training loss 0.01365644857287407 Validation loss 0.020782873034477234 Accuracy 0.78271484375\n",
      "Iteration 98830 Training loss 0.012256374582648277 Validation loss 0.021041439846158028 Accuracy 0.78173828125\n",
      "Iteration 98840 Training loss 0.012080433778464794 Validation loss 0.0207185298204422 Accuracy 0.7841796875\n",
      "Iteration 98850 Training loss 0.012476080097258091 Validation loss 0.020558329299092293 Accuracy 0.78369140625\n",
      "Iteration 98860 Training loss 0.012594906613230705 Validation loss 0.020662151277065277 Accuracy 0.7841796875\n",
      "Iteration 98870 Training loss 0.012100224383175373 Validation loss 0.020706679672002792 Accuracy 0.78271484375\n",
      "Iteration 98880 Training loss 0.012538082897663116 Validation loss 0.020716331899166107 Accuracy 0.78369140625\n",
      "Iteration 98890 Training loss 0.014266128651797771 Validation loss 0.020570561289787292 Accuracy 0.7841796875\n",
      "Iteration 98900 Training loss 0.013331372290849686 Validation loss 0.020811481401324272 Accuracy 0.78271484375\n",
      "Iteration 98910 Training loss 0.014369084499776363 Validation loss 0.02081652730703354 Accuracy 0.7822265625\n",
      "Iteration 98920 Training loss 0.012749457731842995 Validation loss 0.020858002826571465 Accuracy 0.78173828125\n",
      "Iteration 98930 Training loss 0.013455289416015148 Validation loss 0.02089777961373329 Accuracy 0.78125\n",
      "Iteration 98940 Training loss 0.010846437886357307 Validation loss 0.021081149578094482 Accuracy 0.7822265625\n",
      "Iteration 98950 Training loss 0.013642117381095886 Validation loss 0.020606836304068565 Accuracy 0.78466796875\n",
      "Iteration 98960 Training loss 0.011752761900424957 Validation loss 0.020741542801260948 Accuracy 0.783203125\n",
      "Iteration 98970 Training loss 0.01302417740225792 Validation loss 0.020608017221093178 Accuracy 0.78466796875\n",
      "Iteration 98980 Training loss 0.016601720824837685 Validation loss 0.020694369450211525 Accuracy 0.7841796875\n",
      "Iteration 98990 Training loss 0.011062001809477806 Validation loss 0.02058582380414009 Accuracy 0.78466796875\n",
      "Iteration 99000 Training loss 0.013112837448716164 Validation loss 0.020692402496933937 Accuracy 0.78271484375\n",
      "Iteration 99010 Training loss 0.014896959997713566 Validation loss 0.02070939727127552 Accuracy 0.783203125\n",
      "Iteration 99020 Training loss 0.010599512606859207 Validation loss 0.020597748458385468 Accuracy 0.7841796875\n",
      "Iteration 99030 Training loss 0.013162282295525074 Validation loss 0.02062544785439968 Accuracy 0.78466796875\n",
      "Iteration 99040 Training loss 0.012080655433237553 Validation loss 0.020696338266134262 Accuracy 0.7841796875\n",
      "Iteration 99050 Training loss 0.013363740406930447 Validation loss 0.02092648297548294 Accuracy 0.78271484375\n",
      "Iteration 99060 Training loss 0.011148433201014996 Validation loss 0.02084067277610302 Accuracy 0.78173828125\n",
      "Iteration 99070 Training loss 0.012428919784724712 Validation loss 0.020923806354403496 Accuracy 0.78125\n",
      "Iteration 99080 Training loss 0.013079129159450531 Validation loss 0.021307723596692085 Accuracy 0.77734375\n",
      "Iteration 99090 Training loss 0.013463864102959633 Validation loss 0.020903637632727623 Accuracy 0.78173828125\n",
      "Iteration 99100 Training loss 0.011189008131623268 Validation loss 0.02088640257716179 Accuracy 0.78271484375\n",
      "Iteration 99110 Training loss 0.011987769976258278 Validation loss 0.020666800439357758 Accuracy 0.78369140625\n",
      "Iteration 99120 Training loss 0.015026677399873734 Validation loss 0.020933832973241806 Accuracy 0.78125\n",
      "Iteration 99130 Training loss 0.012793955393135548 Validation loss 0.0208013616502285 Accuracy 0.78271484375\n",
      "Iteration 99140 Training loss 0.01374367531388998 Validation loss 0.02104741334915161 Accuracy 0.77978515625\n",
      "Iteration 99150 Training loss 0.014695659279823303 Validation loss 0.020812686532735825 Accuracy 0.78271484375\n",
      "Iteration 99160 Training loss 0.01079263910651207 Validation loss 0.020747309550642967 Accuracy 0.783203125\n",
      "Iteration 99170 Training loss 0.011670411564409733 Validation loss 0.020632781088352203 Accuracy 0.78369140625\n",
      "Iteration 99180 Training loss 0.0112919295206666 Validation loss 0.02051790989935398 Accuracy 0.7841796875\n",
      "Iteration 99190 Training loss 0.011623689904808998 Validation loss 0.020743798464536667 Accuracy 0.783203125\n",
      "Iteration 99200 Training loss 0.01200864277780056 Validation loss 0.020722631365060806 Accuracy 0.783203125\n",
      "Iteration 99210 Training loss 0.011993498541414738 Validation loss 0.02059900015592575 Accuracy 0.78369140625\n",
      "Iteration 99220 Training loss 0.014823280274868011 Validation loss 0.020775752142071724 Accuracy 0.783203125\n",
      "Iteration 99230 Training loss 0.012629657983779907 Validation loss 0.02082822285592556 Accuracy 0.78271484375\n",
      "Iteration 99240 Training loss 0.016295578330755234 Validation loss 0.020796742290258408 Accuracy 0.783203125\n",
      "Iteration 99250 Training loss 0.01490760501474142 Validation loss 0.02083582431077957 Accuracy 0.78369140625\n",
      "Iteration 99260 Training loss 0.01307318452745676 Validation loss 0.02072164975106716 Accuracy 0.783203125\n",
      "Iteration 99270 Training loss 0.010920378379523754 Validation loss 0.02071528695523739 Accuracy 0.783203125\n",
      "Iteration 99280 Training loss 0.013092710636556149 Validation loss 0.020923618227243423 Accuracy 0.78125\n",
      "Iteration 99290 Training loss 0.01366417296230793 Validation loss 0.020715370774269104 Accuracy 0.7841796875\n",
      "Iteration 99300 Training loss 0.014796470291912556 Validation loss 0.020893298089504242 Accuracy 0.78173828125\n",
      "Iteration 99310 Training loss 0.012147203087806702 Validation loss 0.02073082886636257 Accuracy 0.78369140625\n",
      "Iteration 99320 Training loss 0.013332638889551163 Validation loss 0.02055279165506363 Accuracy 0.7841796875\n",
      "Iteration 99330 Training loss 0.011429280042648315 Validation loss 0.02061586081981659 Accuracy 0.78564453125\n",
      "Iteration 99340 Training loss 0.01569328084588051 Validation loss 0.020731039345264435 Accuracy 0.783203125\n",
      "Iteration 99350 Training loss 0.014166615903377533 Validation loss 0.02075011096894741 Accuracy 0.7822265625\n",
      "Iteration 99360 Training loss 0.014791387133300304 Validation loss 0.020864184945821762 Accuracy 0.783203125\n",
      "Iteration 99370 Training loss 0.011056127026677132 Validation loss 0.02062865160405636 Accuracy 0.78515625\n",
      "Iteration 99380 Training loss 0.01284059602767229 Validation loss 0.0206876453012228 Accuracy 0.7841796875\n",
      "Iteration 99390 Training loss 0.013839870691299438 Validation loss 0.020603710785508156 Accuracy 0.78466796875\n",
      "Iteration 99400 Training loss 0.012856756336987019 Validation loss 0.020683392882347107 Accuracy 0.7822265625\n",
      "Iteration 99410 Training loss 0.011766504496335983 Validation loss 0.020498646423220634 Accuracy 0.78515625\n",
      "Iteration 99420 Training loss 0.012587074190378189 Validation loss 0.020495926961302757 Accuracy 0.78515625\n",
      "Iteration 99430 Training loss 0.014266891404986382 Validation loss 0.020847080275416374 Accuracy 0.78271484375\n",
      "Iteration 99440 Training loss 0.012948798947036266 Validation loss 0.0206264890730381 Accuracy 0.78515625\n",
      "Iteration 99450 Training loss 0.014523002319037914 Validation loss 0.020916171371936798 Accuracy 0.783203125\n",
      "Iteration 99460 Training loss 0.012089232914149761 Validation loss 0.020750736817717552 Accuracy 0.78369140625\n",
      "Iteration 99470 Training loss 0.014929882250726223 Validation loss 0.02079043909907341 Accuracy 0.7822265625\n",
      "Iteration 99480 Training loss 0.011930578388273716 Validation loss 0.020498126745224 Accuracy 0.7841796875\n",
      "Iteration 99490 Training loss 0.012974703684449196 Validation loss 0.02081878110766411 Accuracy 0.783203125\n",
      "Iteration 99500 Training loss 0.014003116637468338 Validation loss 0.020644769072532654 Accuracy 0.78466796875\n",
      "Iteration 99510 Training loss 0.011611277237534523 Validation loss 0.02079659141600132 Accuracy 0.7822265625\n",
      "Iteration 99520 Training loss 0.014649439603090286 Validation loss 0.020729009062051773 Accuracy 0.7841796875\n",
      "Iteration 99530 Training loss 0.012509032152593136 Validation loss 0.020843159407377243 Accuracy 0.78271484375\n",
      "Iteration 99540 Training loss 0.012738308869302273 Validation loss 0.020549023523926735 Accuracy 0.78466796875\n",
      "Iteration 99550 Training loss 0.013176292181015015 Validation loss 0.020820526406168938 Accuracy 0.783203125\n",
      "Iteration 99560 Training loss 0.011425073258578777 Validation loss 0.02075647935271263 Accuracy 0.7841796875\n",
      "Iteration 99570 Training loss 0.01261009182780981 Validation loss 0.020951688289642334 Accuracy 0.78173828125\n",
      "Iteration 99580 Training loss 0.011357736773788929 Validation loss 0.02052191272377968 Accuracy 0.78466796875\n",
      "Iteration 99590 Training loss 0.012809102423489094 Validation loss 0.020777661353349686 Accuracy 0.783203125\n",
      "Iteration 99600 Training loss 0.012791982851922512 Validation loss 0.020747147500514984 Accuracy 0.78369140625\n",
      "Iteration 99610 Training loss 0.01329534687101841 Validation loss 0.020851697772741318 Accuracy 0.78271484375\n",
      "Iteration 99620 Training loss 0.01333690620958805 Validation loss 0.020644433796405792 Accuracy 0.78369140625\n",
      "Iteration 99630 Training loss 0.013434620574116707 Validation loss 0.020729195326566696 Accuracy 0.78271484375\n",
      "Iteration 99640 Training loss 0.015529230237007141 Validation loss 0.020602714270353317 Accuracy 0.7841796875\n",
      "Iteration 99650 Training loss 0.01295112818479538 Validation loss 0.020887434482574463 Accuracy 0.78125\n",
      "Iteration 99660 Training loss 0.014213197864592075 Validation loss 0.020701825618743896 Accuracy 0.7841796875\n",
      "Iteration 99670 Training loss 0.013896954245865345 Validation loss 0.020687231793999672 Accuracy 0.783203125\n",
      "Iteration 99680 Training loss 0.011698915623128414 Validation loss 0.020679092034697533 Accuracy 0.7841796875\n",
      "Iteration 99690 Training loss 0.013600628823041916 Validation loss 0.02071433700621128 Accuracy 0.78271484375\n",
      "Iteration 99700 Training loss 0.012064527720212936 Validation loss 0.020761923864483833 Accuracy 0.7841796875\n",
      "Iteration 99710 Training loss 0.012145563960075378 Validation loss 0.020603232085704803 Accuracy 0.7841796875\n",
      "Iteration 99720 Training loss 0.011751529760658741 Validation loss 0.02073361538350582 Accuracy 0.7841796875\n",
      "Iteration 99730 Training loss 0.01344729121774435 Validation loss 0.020651744678616524 Accuracy 0.78369140625\n",
      "Iteration 99740 Training loss 0.015978071838617325 Validation loss 0.021091347560286522 Accuracy 0.7802734375\n",
      "Iteration 99750 Training loss 0.0138860447332263 Validation loss 0.020971927791833878 Accuracy 0.78125\n",
      "Iteration 99760 Training loss 0.0164246316999197 Validation loss 0.020819969475269318 Accuracy 0.7822265625\n",
      "Iteration 99770 Training loss 0.012355835177004337 Validation loss 0.020698774605989456 Accuracy 0.7822265625\n",
      "Iteration 99780 Training loss 0.013829813338816166 Validation loss 0.0207586120814085 Accuracy 0.7822265625\n",
      "Iteration 99790 Training loss 0.013232390396296978 Validation loss 0.020735202357172966 Accuracy 0.78369140625\n",
      "Iteration 99800 Training loss 0.012798530049622059 Validation loss 0.020756494253873825 Accuracy 0.78369140625\n",
      "Iteration 99810 Training loss 0.012680300511419773 Validation loss 0.020646406337618828 Accuracy 0.783203125\n",
      "Iteration 99820 Training loss 0.01100117340683937 Validation loss 0.020724870264530182 Accuracy 0.78369140625\n",
      "Iteration 99830 Training loss 0.012895657680928707 Validation loss 0.020836425945162773 Accuracy 0.78173828125\n",
      "Iteration 99840 Training loss 0.014006294310092926 Validation loss 0.020933620631694794 Accuracy 0.78076171875\n",
      "Iteration 99850 Training loss 0.015179518610239029 Validation loss 0.020652201026678085 Accuracy 0.7822265625\n",
      "Iteration 99860 Training loss 0.011622579768300056 Validation loss 0.020778490230441093 Accuracy 0.78173828125\n",
      "Iteration 99870 Training loss 0.012515729293227196 Validation loss 0.020816626027226448 Accuracy 0.78271484375\n",
      "Iteration 99880 Training loss 0.01483504381030798 Validation loss 0.02105676755309105 Accuracy 0.77734375\n",
      "Iteration 99890 Training loss 0.011084682308137417 Validation loss 0.0208422914147377 Accuracy 0.7822265625\n",
      "Iteration 99900 Training loss 0.012889401987195015 Validation loss 0.02082964777946472 Accuracy 0.78271484375\n",
      "Iteration 99910 Training loss 0.011176098138093948 Validation loss 0.020720282569527626 Accuracy 0.78271484375\n",
      "Iteration 99920 Training loss 0.013070294633507729 Validation loss 0.020655741915106773 Accuracy 0.783203125\n",
      "Iteration 99930 Training loss 0.015675269067287445 Validation loss 0.020721977576613426 Accuracy 0.783203125\n",
      "Iteration 99940 Training loss 0.011735707521438599 Validation loss 0.02067415975034237 Accuracy 0.783203125\n",
      "Iteration 99950 Training loss 0.01290964987128973 Validation loss 0.020721005275845528 Accuracy 0.7841796875\n",
      "Iteration 99960 Training loss 0.013535033911466599 Validation loss 0.020536739379167557 Accuracy 0.7841796875\n",
      "Iteration 99970 Training loss 0.013465438969433308 Validation loss 0.0206284336745739 Accuracy 0.7841796875\n",
      "Iteration 99980 Training loss 0.011432494036853313 Validation loss 0.02072002924978733 Accuracy 0.78271484375\n",
      "Iteration 99990 Training loss 0.013701451011002064 Validation loss 0.021998314186930656 Accuracy 0.77099609375\n",
      "Iteration 100000 Training loss 0.011855398304760456 Validation loss 0.0207450482994318 Accuracy 0.7822265625\n",
      "Iteration 100010 Training loss 0.01244698278605938 Validation loss 0.020852290093898773 Accuracy 0.7822265625\n",
      "Iteration 100020 Training loss 0.011654495261609554 Validation loss 0.020860424265265465 Accuracy 0.783203125\n",
      "Iteration 100030 Training loss 0.013699254021048546 Validation loss 0.02067555859684944 Accuracy 0.78271484375\n",
      "Iteration 100040 Training loss 0.010707052424550056 Validation loss 0.020619213581085205 Accuracy 0.78369140625\n",
      "Iteration 100050 Training loss 0.013463934883475304 Validation loss 0.020720062777400017 Accuracy 0.783203125\n",
      "Iteration 100060 Training loss 0.015759969130158424 Validation loss 0.021313035860657692 Accuracy 0.7783203125\n",
      "Iteration 100070 Training loss 0.01361570693552494 Validation loss 0.02066565304994583 Accuracy 0.78271484375\n",
      "Iteration 100080 Training loss 0.012120269238948822 Validation loss 0.02067900076508522 Accuracy 0.7841796875\n",
      "Iteration 100090 Training loss 0.011212999932467937 Validation loss 0.020758256316184998 Accuracy 0.7822265625\n",
      "Iteration 100100 Training loss 0.011606822721660137 Validation loss 0.020825788378715515 Accuracy 0.78271484375\n",
      "Iteration 100110 Training loss 0.013230673037469387 Validation loss 0.020934879779815674 Accuracy 0.78173828125\n",
      "Iteration 100120 Training loss 0.012155265547335148 Validation loss 0.02093466743826866 Accuracy 0.78076171875\n",
      "Iteration 100130 Training loss 0.014133679680526257 Validation loss 0.021133851259946823 Accuracy 0.779296875\n",
      "Iteration 100140 Training loss 0.014134286902844906 Validation loss 0.020916152745485306 Accuracy 0.77978515625\n",
      "Iteration 100150 Training loss 0.015257175080478191 Validation loss 0.021047309041023254 Accuracy 0.78076171875\n",
      "Iteration 100160 Training loss 0.01317894458770752 Validation loss 0.020735204219818115 Accuracy 0.7822265625\n",
      "Iteration 100170 Training loss 0.012612683698534966 Validation loss 0.021274171769618988 Accuracy 0.77734375\n",
      "Iteration 100180 Training loss 0.014329739846289158 Validation loss 0.02071855403482914 Accuracy 0.78173828125\n",
      "Iteration 100190 Training loss 0.01412520557641983 Validation loss 0.02066762186586857 Accuracy 0.78369140625\n",
      "Iteration 100200 Training loss 0.015301679261028767 Validation loss 0.020911699160933495 Accuracy 0.7802734375\n",
      "Iteration 100210 Training loss 0.01245537307113409 Validation loss 0.020853614434599876 Accuracy 0.7802734375\n",
      "Iteration 100220 Training loss 0.011485064402222633 Validation loss 0.02079981565475464 Accuracy 0.783203125\n",
      "Iteration 100230 Training loss 0.013441584073007107 Validation loss 0.02079135738313198 Accuracy 0.78076171875\n",
      "Iteration 100240 Training loss 0.011886052787303925 Validation loss 0.020689386874437332 Accuracy 0.783203125\n",
      "Iteration 100250 Training loss 0.013544880785048008 Validation loss 0.020872851833701134 Accuracy 0.78125\n",
      "Iteration 100260 Training loss 0.01298621203750372 Validation loss 0.020779447630047798 Accuracy 0.78173828125\n",
      "Iteration 100270 Training loss 0.012807697989046574 Validation loss 0.02085140533745289 Accuracy 0.78173828125\n",
      "Iteration 100280 Training loss 0.014415337704122066 Validation loss 0.020613636821508408 Accuracy 0.78271484375\n",
      "Iteration 100290 Training loss 0.012327305972576141 Validation loss 0.020692043006420135 Accuracy 0.783203125\n",
      "Iteration 100300 Training loss 0.01214523520320654 Validation loss 0.02073022350668907 Accuracy 0.783203125\n",
      "Iteration 100310 Training loss 0.014114725403487682 Validation loss 0.02076573856174946 Accuracy 0.783203125\n",
      "Iteration 100320 Training loss 0.014142444357275963 Validation loss 0.0207087192684412 Accuracy 0.7822265625\n",
      "Iteration 100330 Training loss 0.015328262001276016 Validation loss 0.020691415295004845 Accuracy 0.78271484375\n",
      "Iteration 100340 Training loss 0.011098653078079224 Validation loss 0.02074987068772316 Accuracy 0.78271484375\n",
      "Iteration 100350 Training loss 0.014330151490867138 Validation loss 0.020683975890278816 Accuracy 0.78271484375\n",
      "Iteration 100360 Training loss 0.014532431960105896 Validation loss 0.021157853305339813 Accuracy 0.779296875\n",
      "Iteration 100370 Training loss 0.012793555855751038 Validation loss 0.020870424807071686 Accuracy 0.78173828125\n",
      "Iteration 100380 Training loss 0.013803990557789803 Validation loss 0.020951570942997932 Accuracy 0.7802734375\n",
      "Iteration 100390 Training loss 0.013790259137749672 Validation loss 0.02084718458354473 Accuracy 0.78173828125\n",
      "Iteration 100400 Training loss 0.013129297643899918 Validation loss 0.020792778581380844 Accuracy 0.783203125\n",
      "Iteration 100410 Training loss 0.014864364638924599 Validation loss 0.020611613988876343 Accuracy 0.7841796875\n",
      "Iteration 100420 Training loss 0.013885283842682838 Validation loss 0.02088209241628647 Accuracy 0.7822265625\n",
      "Iteration 100430 Training loss 0.012244952842593193 Validation loss 0.020694807171821594 Accuracy 0.78369140625\n",
      "Iteration 100440 Training loss 0.010291040875017643 Validation loss 0.020511332899332047 Accuracy 0.78466796875\n",
      "Iteration 100450 Training loss 0.011968120001256466 Validation loss 0.02056555077433586 Accuracy 0.7841796875\n",
      "Iteration 100460 Training loss 0.012362921610474586 Validation loss 0.020636430010199547 Accuracy 0.783203125\n",
      "Iteration 100470 Training loss 0.012895983643829823 Validation loss 0.020890839397907257 Accuracy 0.78125\n",
      "Iteration 100480 Training loss 0.013179889880120754 Validation loss 0.020712802186608315 Accuracy 0.78271484375\n",
      "Iteration 100490 Training loss 0.014673305675387383 Validation loss 0.020767489448189735 Accuracy 0.78271484375\n",
      "Iteration 100500 Training loss 0.012158142402768135 Validation loss 0.02101670391857624 Accuracy 0.7822265625\n",
      "Iteration 100510 Training loss 0.013186243362724781 Validation loss 0.02095920778810978 Accuracy 0.78076171875\n",
      "Iteration 100520 Training loss 0.012464415282011032 Validation loss 0.020848354324698448 Accuracy 0.78271484375\n",
      "Iteration 100530 Training loss 0.01264688465744257 Validation loss 0.020616434514522552 Accuracy 0.78369140625\n",
      "Iteration 100540 Training loss 0.014093256555497646 Validation loss 0.020673345774412155 Accuracy 0.783203125\n",
      "Iteration 100550 Training loss 0.013669253326952457 Validation loss 0.020806869491934776 Accuracy 0.783203125\n",
      "Iteration 100560 Training loss 0.014567453414201736 Validation loss 0.020726686343550682 Accuracy 0.78369140625\n",
      "Iteration 100570 Training loss 0.012483764439821243 Validation loss 0.020639071241021156 Accuracy 0.78369140625\n",
      "Iteration 100580 Training loss 0.0154571533203125 Validation loss 0.020683329552412033 Accuracy 0.78369140625\n",
      "Iteration 100590 Training loss 0.011651871725916862 Validation loss 0.02064705081284046 Accuracy 0.78466796875\n",
      "Iteration 100600 Training loss 0.013473935425281525 Validation loss 0.02069861814379692 Accuracy 0.78369140625\n",
      "Iteration 100610 Training loss 0.01336672157049179 Validation loss 0.02087344415485859 Accuracy 0.78125\n",
      "Iteration 100620 Training loss 0.015246963128447533 Validation loss 0.02067822776734829 Accuracy 0.78369140625\n",
      "Iteration 100630 Training loss 0.01452589314430952 Validation loss 0.020727181807160378 Accuracy 0.7822265625\n",
      "Iteration 100640 Training loss 0.010951858013868332 Validation loss 0.020709818229079247 Accuracy 0.78369140625\n",
      "Iteration 100650 Training loss 0.012714996002614498 Validation loss 0.020865177735686302 Accuracy 0.78173828125\n",
      "Iteration 100660 Training loss 0.01399579830467701 Validation loss 0.020821115002036095 Accuracy 0.783203125\n",
      "Iteration 100670 Training loss 0.014533363282680511 Validation loss 0.02059762366116047 Accuracy 0.78369140625\n",
      "Iteration 100680 Training loss 0.014425917528569698 Validation loss 0.020738858729600906 Accuracy 0.78369140625\n",
      "Iteration 100690 Training loss 0.011438366957008839 Validation loss 0.020820653066039085 Accuracy 0.78369140625\n",
      "Iteration 100700 Training loss 0.012260869145393372 Validation loss 0.02069699577987194 Accuracy 0.78369140625\n",
      "Iteration 100710 Training loss 0.014241820201277733 Validation loss 0.020690716803073883 Accuracy 0.78369140625\n",
      "Iteration 100720 Training loss 0.011406389996409416 Validation loss 0.020547164604067802 Accuracy 0.78466796875\n",
      "Iteration 100730 Training loss 0.013356668874621391 Validation loss 0.020743636414408684 Accuracy 0.78369140625\n",
      "Iteration 100740 Training loss 0.012722300365567207 Validation loss 0.020607054233551025 Accuracy 0.7841796875\n",
      "Iteration 100750 Training loss 0.010656729340553284 Validation loss 0.020822253078222275 Accuracy 0.78271484375\n",
      "Iteration 100760 Training loss 0.011006602086126804 Validation loss 0.020687760785222054 Accuracy 0.78271484375\n",
      "Iteration 100770 Training loss 0.013468421995639801 Validation loss 0.02072056755423546 Accuracy 0.78271484375\n",
      "Iteration 100780 Training loss 0.012991798110306263 Validation loss 0.02079017087817192 Accuracy 0.783203125\n",
      "Iteration 100790 Training loss 0.013685926795005798 Validation loss 0.020894935354590416 Accuracy 0.78369140625\n",
      "Iteration 100800 Training loss 0.012689284980297089 Validation loss 0.020568080246448517 Accuracy 0.7841796875\n",
      "Iteration 100810 Training loss 0.013824543915688992 Validation loss 0.02065056748688221 Accuracy 0.783203125\n",
      "Iteration 100820 Training loss 0.014065053313970566 Validation loss 0.02072053588926792 Accuracy 0.78271484375\n",
      "Iteration 100830 Training loss 0.013058912940323353 Validation loss 0.020731328055262566 Accuracy 0.7822265625\n",
      "Iteration 100840 Training loss 0.01380724087357521 Validation loss 0.020895449444651604 Accuracy 0.78173828125\n",
      "Iteration 100850 Training loss 0.014142694883048534 Validation loss 0.02067645452916622 Accuracy 0.78369140625\n",
      "Iteration 100860 Training loss 0.014368956908583641 Validation loss 0.020642371848225594 Accuracy 0.7841796875\n",
      "Iteration 100870 Training loss 0.011877588927745819 Validation loss 0.020938729867339134 Accuracy 0.78076171875\n",
      "Iteration 100880 Training loss 0.01359524019062519 Validation loss 0.02101963572204113 Accuracy 0.78173828125\n",
      "Iteration 100890 Training loss 0.011345797218382359 Validation loss 0.020645463839173317 Accuracy 0.7841796875\n",
      "Iteration 100900 Training loss 0.014409785158932209 Validation loss 0.021260453388094902 Accuracy 0.77978515625\n",
      "Iteration 100910 Training loss 0.011921579949557781 Validation loss 0.020717846229672432 Accuracy 0.78271484375\n",
      "Iteration 100920 Training loss 0.011103651486337185 Validation loss 0.02085004188120365 Accuracy 0.78173828125\n",
      "Iteration 100930 Training loss 0.011594075709581375 Validation loss 0.020823322236537933 Accuracy 0.78173828125\n",
      "Iteration 100940 Training loss 0.014206760562956333 Validation loss 0.02068311907351017 Accuracy 0.783203125\n",
      "Iteration 100950 Training loss 0.01295579131692648 Validation loss 0.020816126838326454 Accuracy 0.78173828125\n",
      "Iteration 100960 Training loss 0.014922513626515865 Validation loss 0.020770380273461342 Accuracy 0.78369140625\n",
      "Iteration 100970 Training loss 0.012244855985045433 Validation loss 0.020617367699742317 Accuracy 0.78271484375\n",
      "Iteration 100980 Training loss 0.012887506745755672 Validation loss 0.020970653742551804 Accuracy 0.78125\n",
      "Iteration 100990 Training loss 0.011716697365045547 Validation loss 0.020617173984646797 Accuracy 0.7841796875\n",
      "Iteration 101000 Training loss 0.012778541073203087 Validation loss 0.020588796585798264 Accuracy 0.78369140625\n",
      "Iteration 101010 Training loss 0.015673112124204636 Validation loss 0.021308958530426025 Accuracy 0.7783203125\n",
      "Iteration 101020 Training loss 0.011053589172661304 Validation loss 0.020706987008452415 Accuracy 0.783203125\n",
      "Iteration 101030 Training loss 0.013779563829302788 Validation loss 0.020791035145521164 Accuracy 0.7822265625\n",
      "Iteration 101040 Training loss 0.01299970131367445 Validation loss 0.020881405100226402 Accuracy 0.78173828125\n",
      "Iteration 101050 Training loss 0.012409764342010021 Validation loss 0.02060781419277191 Accuracy 0.78369140625\n",
      "Iteration 101060 Training loss 0.013522986322641373 Validation loss 0.020643429830670357 Accuracy 0.783203125\n",
      "Iteration 101070 Training loss 0.012140936218202114 Validation loss 0.02088501863181591 Accuracy 0.78271484375\n",
      "Iteration 101080 Training loss 0.01084831077605486 Validation loss 0.020669210702180862 Accuracy 0.78369140625\n",
      "Iteration 101090 Training loss 0.012244628742337227 Validation loss 0.020809901878237724 Accuracy 0.78271484375\n",
      "Iteration 101100 Training loss 0.012882942333817482 Validation loss 0.02071264758706093 Accuracy 0.78271484375\n",
      "Iteration 101110 Training loss 0.011665292084217072 Validation loss 0.020627127960324287 Accuracy 0.783203125\n",
      "Iteration 101120 Training loss 0.015173145569860935 Validation loss 0.020666033029556274 Accuracy 0.7822265625\n",
      "Iteration 101130 Training loss 0.014907684177160263 Validation loss 0.020789561793208122 Accuracy 0.783203125\n",
      "Iteration 101140 Training loss 0.015283065848052502 Validation loss 0.020698031410574913 Accuracy 0.783203125\n",
      "Iteration 101150 Training loss 0.010990660637617111 Validation loss 0.02064099721610546 Accuracy 0.78369140625\n",
      "Iteration 101160 Training loss 0.01270668301731348 Validation loss 0.0208272747695446 Accuracy 0.78173828125\n",
      "Iteration 101170 Training loss 0.014361124485731125 Validation loss 0.020642787218093872 Accuracy 0.783203125\n",
      "Iteration 101180 Training loss 0.012965342961251736 Validation loss 0.02053089253604412 Accuracy 0.78369140625\n",
      "Iteration 101190 Training loss 0.012257207185029984 Validation loss 0.020567387342453003 Accuracy 0.78466796875\n",
      "Iteration 101200 Training loss 0.011536003090441227 Validation loss 0.02083260379731655 Accuracy 0.7822265625\n",
      "Iteration 101210 Training loss 0.011324264109134674 Validation loss 0.020642181858420372 Accuracy 0.78369140625\n",
      "Iteration 101220 Training loss 0.010447644628584385 Validation loss 0.020615268498659134 Accuracy 0.783203125\n",
      "Iteration 101230 Training loss 0.01250761654227972 Validation loss 0.020662298426032066 Accuracy 0.783203125\n",
      "Iteration 101240 Training loss 0.011852072551846504 Validation loss 0.020672334358096123 Accuracy 0.783203125\n",
      "Iteration 101250 Training loss 0.014509552158415318 Validation loss 0.020601613447070122 Accuracy 0.783203125\n",
      "Iteration 101260 Training loss 0.010313665494322777 Validation loss 0.02064422331750393 Accuracy 0.783203125\n",
      "Iteration 101270 Training loss 0.01383244339376688 Validation loss 0.02088553085923195 Accuracy 0.78173828125\n",
      "Iteration 101280 Training loss 0.012905399315059185 Validation loss 0.02093677967786789 Accuracy 0.78173828125\n",
      "Iteration 101290 Training loss 0.014025568030774593 Validation loss 0.020629985257983208 Accuracy 0.783203125\n",
      "Iteration 101300 Training loss 0.013895979151129723 Validation loss 0.020803002640604973 Accuracy 0.7822265625\n",
      "Iteration 101310 Training loss 0.014860269613564014 Validation loss 0.020613016560673714 Accuracy 0.783203125\n",
      "Iteration 101320 Training loss 0.011335116811096668 Validation loss 0.020834345370531082 Accuracy 0.78271484375\n",
      "Iteration 101330 Training loss 0.011528195813298225 Validation loss 0.020464032888412476 Accuracy 0.78515625\n",
      "Iteration 101340 Training loss 0.013243746012449265 Validation loss 0.02061324380338192 Accuracy 0.783203125\n",
      "Iteration 101350 Training loss 0.013901753351092339 Validation loss 0.020606694743037224 Accuracy 0.78369140625\n",
      "Iteration 101360 Training loss 0.013093513436615467 Validation loss 0.021593598648905754 Accuracy 0.7744140625\n",
      "Iteration 101370 Training loss 0.012948663905262947 Validation loss 0.02082025073468685 Accuracy 0.783203125\n",
      "Iteration 101380 Training loss 0.01502272393554449 Validation loss 0.02086908183991909 Accuracy 0.78173828125\n",
      "Iteration 101390 Training loss 0.01236776728183031 Validation loss 0.02074502594769001 Accuracy 0.78271484375\n",
      "Iteration 101400 Training loss 0.012535021640360355 Validation loss 0.020697368308901787 Accuracy 0.78173828125\n",
      "Iteration 101410 Training loss 0.012762979604303837 Validation loss 0.020627569407224655 Accuracy 0.78369140625\n",
      "Iteration 101420 Training loss 0.012317921966314316 Validation loss 0.02055208757519722 Accuracy 0.78515625\n",
      "Iteration 101430 Training loss 0.010533259250223637 Validation loss 0.02074059285223484 Accuracy 0.783203125\n",
      "Iteration 101440 Training loss 0.013703963719308376 Validation loss 0.020581217482686043 Accuracy 0.78369140625\n",
      "Iteration 101450 Training loss 0.013767952099442482 Validation loss 0.020562687888741493 Accuracy 0.7841796875\n",
      "Iteration 101460 Training loss 0.013559804297983646 Validation loss 0.020717568695545197 Accuracy 0.783203125\n",
      "Iteration 101470 Training loss 0.012359566055238247 Validation loss 0.020651115104556084 Accuracy 0.783203125\n",
      "Iteration 101480 Training loss 0.01127657014876604 Validation loss 0.020658304914832115 Accuracy 0.7841796875\n",
      "Iteration 101490 Training loss 0.012460451573133469 Validation loss 0.020595664158463478 Accuracy 0.78369140625\n",
      "Iteration 101500 Training loss 0.01378956064581871 Validation loss 0.020676840096712112 Accuracy 0.78466796875\n",
      "Iteration 101510 Training loss 0.013913711532950401 Validation loss 0.020763788372278214 Accuracy 0.78271484375\n",
      "Iteration 101520 Training loss 0.015654994174838066 Validation loss 0.020918836817145348 Accuracy 0.78076171875\n",
      "Iteration 101530 Training loss 0.014355666004121304 Validation loss 0.02063043601810932 Accuracy 0.78369140625\n",
      "Iteration 101540 Training loss 0.011930493637919426 Validation loss 0.02077014371752739 Accuracy 0.78271484375\n",
      "Iteration 101550 Training loss 0.010108529590070248 Validation loss 0.020547574386000633 Accuracy 0.78466796875\n",
      "Iteration 101560 Training loss 0.013790968805551529 Validation loss 0.02072192169725895 Accuracy 0.7822265625\n",
      "Iteration 101570 Training loss 0.013715912587940693 Validation loss 0.02061874233186245 Accuracy 0.7841796875\n",
      "Iteration 101580 Training loss 0.013416669331490993 Validation loss 0.02066906914114952 Accuracy 0.7822265625\n",
      "Iteration 101590 Training loss 0.013434488326311111 Validation loss 0.020971957594156265 Accuracy 0.78076171875\n",
      "Iteration 101600 Training loss 0.013852018862962723 Validation loss 0.020850060507655144 Accuracy 0.78125\n",
      "Iteration 101610 Training loss 0.012888031080365181 Validation loss 0.020780501887202263 Accuracy 0.78173828125\n",
      "Iteration 101620 Training loss 0.015499131754040718 Validation loss 0.020680511370301247 Accuracy 0.78271484375\n",
      "Iteration 101630 Training loss 0.012992264702916145 Validation loss 0.020887479186058044 Accuracy 0.78173828125\n",
      "Iteration 101640 Training loss 0.013446243479847908 Validation loss 0.0207919180393219 Accuracy 0.78173828125\n",
      "Iteration 101650 Training loss 0.013087519444525242 Validation loss 0.02079804427921772 Accuracy 0.78271484375\n",
      "Iteration 101660 Training loss 0.012589246034622192 Validation loss 0.02071060985326767 Accuracy 0.78271484375\n",
      "Iteration 101670 Training loss 0.012575034983456135 Validation loss 0.020809000357985497 Accuracy 0.78076171875\n",
      "Iteration 101680 Training loss 0.014320969581604004 Validation loss 0.02070164494216442 Accuracy 0.78271484375\n",
      "Iteration 101690 Training loss 0.011273648589849472 Validation loss 0.02086622826755047 Accuracy 0.78173828125\n",
      "Iteration 101700 Training loss 0.012316868640482426 Validation loss 0.020823953673243523 Accuracy 0.78125\n",
      "Iteration 101710 Training loss 0.013444757089018822 Validation loss 0.020646415650844574 Accuracy 0.78369140625\n",
      "Iteration 101720 Training loss 0.011396718211472034 Validation loss 0.020682692527770996 Accuracy 0.78271484375\n",
      "Iteration 101730 Training loss 0.01265723817050457 Validation loss 0.02084318734705448 Accuracy 0.78076171875\n",
      "Iteration 101740 Training loss 0.016233136877417564 Validation loss 0.020567236468195915 Accuracy 0.78369140625\n",
      "Iteration 101750 Training loss 0.01308567076921463 Validation loss 0.020841116085648537 Accuracy 0.78271484375\n",
      "Iteration 101760 Training loss 0.014043651521205902 Validation loss 0.02074173279106617 Accuracy 0.78125\n",
      "Iteration 101770 Training loss 0.010785802267491817 Validation loss 0.02060716226696968 Accuracy 0.7822265625\n",
      "Iteration 101780 Training loss 0.014110124669969082 Validation loss 0.020660459995269775 Accuracy 0.783203125\n",
      "Iteration 101790 Training loss 0.011861004866659641 Validation loss 0.021061168983578682 Accuracy 0.78076171875\n",
      "Iteration 101800 Training loss 0.013402564451098442 Validation loss 0.020997539162635803 Accuracy 0.7802734375\n",
      "Iteration 101810 Training loss 0.010322873480618 Validation loss 0.02097020298242569 Accuracy 0.78173828125\n",
      "Iteration 101820 Training loss 0.010194385424256325 Validation loss 0.02067360281944275 Accuracy 0.7822265625\n",
      "Iteration 101830 Training loss 0.012927275151014328 Validation loss 0.020599769428372383 Accuracy 0.7841796875\n",
      "Iteration 101840 Training loss 0.011463676579296589 Validation loss 0.020689262077212334 Accuracy 0.7822265625\n",
      "Iteration 101850 Training loss 0.013427057303488255 Validation loss 0.02070256695151329 Accuracy 0.7822265625\n",
      "Iteration 101860 Training loss 0.012573907151818275 Validation loss 0.02136443369090557 Accuracy 0.77880859375\n",
      "Iteration 101870 Training loss 0.013501248322427273 Validation loss 0.020894255489110947 Accuracy 0.78076171875\n",
      "Iteration 101880 Training loss 0.011850472539663315 Validation loss 0.020528966560959816 Accuracy 0.78369140625\n",
      "Iteration 101890 Training loss 0.010889933444559574 Validation loss 0.0206801388412714 Accuracy 0.783203125\n",
      "Iteration 101900 Training loss 0.011784668080508709 Validation loss 0.0208530705422163 Accuracy 0.7802734375\n",
      "Iteration 101910 Training loss 0.01157345436513424 Validation loss 0.020663782954216003 Accuracy 0.78271484375\n",
      "Iteration 101920 Training loss 0.012486281804740429 Validation loss 0.020639410242438316 Accuracy 0.78271484375\n",
      "Iteration 101930 Training loss 0.014563041739165783 Validation loss 0.02064318023622036 Accuracy 0.78271484375\n",
      "Iteration 101940 Training loss 0.012311459518969059 Validation loss 0.020857855677604675 Accuracy 0.78125\n",
      "Iteration 101950 Training loss 0.011720878072082996 Validation loss 0.020679039880633354 Accuracy 0.783203125\n",
      "Iteration 101960 Training loss 0.01365711446851492 Validation loss 0.02096703089773655 Accuracy 0.78125\n",
      "Iteration 101970 Training loss 0.012787855230271816 Validation loss 0.020662030205130577 Accuracy 0.78271484375\n",
      "Iteration 101980 Training loss 0.013665512204170227 Validation loss 0.0205110851675272 Accuracy 0.78369140625\n",
      "Iteration 101990 Training loss 0.012022738344967365 Validation loss 0.02088036574423313 Accuracy 0.78076171875\n",
      "Iteration 102000 Training loss 0.013242986053228378 Validation loss 0.021105846390128136 Accuracy 0.779296875\n",
      "Iteration 102010 Training loss 0.012291908264160156 Validation loss 0.020596586167812347 Accuracy 0.78369140625\n",
      "Iteration 102020 Training loss 0.013334566727280617 Validation loss 0.020709868520498276 Accuracy 0.78369140625\n",
      "Iteration 102030 Training loss 0.012569407932460308 Validation loss 0.02076011151075363 Accuracy 0.78271484375\n",
      "Iteration 102040 Training loss 0.012491769157350063 Validation loss 0.020660828799009323 Accuracy 0.78271484375\n",
      "Iteration 102050 Training loss 0.012983693741261959 Validation loss 0.020666800439357758 Accuracy 0.783203125\n",
      "Iteration 102060 Training loss 0.013583417050540447 Validation loss 0.020810028538107872 Accuracy 0.78076171875\n",
      "Iteration 102070 Training loss 0.01451150979846716 Validation loss 0.020711874589323997 Accuracy 0.7822265625\n",
      "Iteration 102080 Training loss 0.013504913076758385 Validation loss 0.020768558606505394 Accuracy 0.78125\n",
      "Iteration 102090 Training loss 0.014104482717812061 Validation loss 0.020908555015921593 Accuracy 0.78076171875\n",
      "Iteration 102100 Training loss 0.014001627452671528 Validation loss 0.020645584911108017 Accuracy 0.783203125\n",
      "Iteration 102110 Training loss 0.01412286702543497 Validation loss 0.020644614472985268 Accuracy 0.783203125\n",
      "Iteration 102120 Training loss 0.014095578342676163 Validation loss 0.02079053223133087 Accuracy 0.78271484375\n",
      "Iteration 102130 Training loss 0.01238352432847023 Validation loss 0.02077782154083252 Accuracy 0.7822265625\n",
      "Iteration 102140 Training loss 0.011091750115156174 Validation loss 0.02068869024515152 Accuracy 0.783203125\n",
      "Iteration 102150 Training loss 0.01325164269655943 Validation loss 0.020639697089791298 Accuracy 0.783203125\n",
      "Iteration 102160 Training loss 0.013806872069835663 Validation loss 0.02083779312670231 Accuracy 0.78125\n",
      "Iteration 102170 Training loss 0.014748490415513515 Validation loss 0.02079269476234913 Accuracy 0.7822265625\n",
      "Iteration 102180 Training loss 0.01170238945633173 Validation loss 0.020556967705488205 Accuracy 0.783203125\n",
      "Iteration 102190 Training loss 0.014086849987506866 Validation loss 0.02081950567662716 Accuracy 0.78173828125\n",
      "Iteration 102200 Training loss 0.012677759863436222 Validation loss 0.02078264020383358 Accuracy 0.78173828125\n",
      "Iteration 102210 Training loss 0.015114369802176952 Validation loss 0.02065512165427208 Accuracy 0.78271484375\n",
      "Iteration 102220 Training loss 0.012016682885587215 Validation loss 0.020761288702487946 Accuracy 0.78271484375\n",
      "Iteration 102230 Training loss 0.010429536923766136 Validation loss 0.020629266276955605 Accuracy 0.78271484375\n",
      "Iteration 102240 Training loss 0.013286476023495197 Validation loss 0.02090284787118435 Accuracy 0.78125\n",
      "Iteration 102250 Training loss 0.011591305956244469 Validation loss 0.021012170240283012 Accuracy 0.78125\n",
      "Iteration 102260 Training loss 0.013169906102120876 Validation loss 0.020694095641374588 Accuracy 0.78271484375\n",
      "Iteration 102270 Training loss 0.012286477722227573 Validation loss 0.02073749527335167 Accuracy 0.78271484375\n",
      "Iteration 102280 Training loss 0.014751538634300232 Validation loss 0.020646147429943085 Accuracy 0.783203125\n",
      "Iteration 102290 Training loss 0.014204233884811401 Validation loss 0.02082121931016445 Accuracy 0.783203125\n",
      "Iteration 102300 Training loss 0.013134750537574291 Validation loss 0.020775996148586273 Accuracy 0.783203125\n",
      "Iteration 102310 Training loss 0.012654106132686138 Validation loss 0.020982753485441208 Accuracy 0.7822265625\n",
      "Iteration 102320 Training loss 0.013304724358022213 Validation loss 0.020563311874866486 Accuracy 0.7841796875\n",
      "Iteration 102330 Training loss 0.012109729461371899 Validation loss 0.020729748532176018 Accuracy 0.78271484375\n",
      "Iteration 102340 Training loss 0.011456640437245369 Validation loss 0.020989783108234406 Accuracy 0.78125\n",
      "Iteration 102350 Training loss 0.01162588782608509 Validation loss 0.020647021010518074 Accuracy 0.78369140625\n",
      "Iteration 102360 Training loss 0.013231421820819378 Validation loss 0.020897038280963898 Accuracy 0.7822265625\n",
      "Iteration 102370 Training loss 0.013035077601671219 Validation loss 0.020700011402368546 Accuracy 0.783203125\n",
      "Iteration 102380 Training loss 0.011473513208329678 Validation loss 0.02070002630352974 Accuracy 0.7841796875\n",
      "Iteration 102390 Training loss 0.0135513199493289 Validation loss 0.020684408023953438 Accuracy 0.783203125\n",
      "Iteration 102400 Training loss 0.015075677074491978 Validation loss 0.020733851939439774 Accuracy 0.78466796875\n",
      "Iteration 102410 Training loss 0.012324610725045204 Validation loss 0.02056315168738365 Accuracy 0.78369140625\n",
      "Iteration 102420 Training loss 0.011538631282746792 Validation loss 0.020562145859003067 Accuracy 0.7841796875\n",
      "Iteration 102430 Training loss 0.011649058200418949 Validation loss 0.020639194175601006 Accuracy 0.783203125\n",
      "Iteration 102440 Training loss 0.01434602215886116 Validation loss 0.02060951665043831 Accuracy 0.783203125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-4, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = True\n",
    "date = \"29_04_25_\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = model_3_layer_1_untrained\n",
    "    model_name = \"model_(784+2048-2048+10)\"\n",
    "    save_path = \"Classifiers/\" + date + model_name + \"/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"datas\\models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "tensor(0.8692)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "model_name = \"model_2_layer_trained_save_1\"\n",
    "assessed_model = torch.load(\"datas/models/model_2_layer_save_1.pt\", weights_only=False)\n",
    "\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plots of performances\n",
    "accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(accuracy)\n",
    "kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_accuracy)\n",
    "loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(loss)\n",
    "kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_loss)\n",
    "plt.show()\n",
    "\n",
    "# Import datas\n",
    "accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).to(dtype)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "\n",
    "\n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, esp_init = 1, fraction_batch=0.01):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = esp_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = esp_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # # hidden neurons layer 2\n",
    "        z3 = torch.mm(self.W3, h2.t() + self.b3)\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, coef_iter = 1, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=0.2, fraction_batch=1e-2, train_layer1=True, train_layer2=True, train_layer3=True):\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        kappa_max = 1 + coef_iter\n",
    "        max_iter = self.input_dimension**(kappa_max)\n",
    "        print(\"max_iter\", max_iter)\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_minibatches = int(max_iter / self.fraction_batch) # Nombre de minibatches utilisés pour l'apprentissage de la première couche\n",
    "        for i in range(N_minibatches):\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            # Suivi de l'apprentissage\n",
    "            if i % 100 == 0:\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2, dim=0) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                overall_training_loss = torch.mean(training_loss,dim=0)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2, dim=0)\n",
    "                overall_validation_loss = torch.mean(validation_loss,dim=0)\n",
    "                self.training_loss_trajectory.append(overall_training_loss.item())\n",
    "                self.validation_loss_trajectory.append(overall_validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", overall_training_loss.item(), \"Validation loss\", overall_validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0] # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = torch.mean(grad_z1, dim=0).unsqueeze(1) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0] # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = torch.mean(grad_z2, dim=0).unsqueeze(1)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            self.W1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/self.eps_init**2 + self.reg1*self.W1) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "            self.b1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/self.eps_init**2 + self.reg1*self.b1)\n",
    "            self.W2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/self.eps_init**2 +self.reg2*self.W2)\n",
    "            self.b2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/self.eps_init**2 + self.reg2*self.b2)\n",
    "        return \"Training done\"\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
